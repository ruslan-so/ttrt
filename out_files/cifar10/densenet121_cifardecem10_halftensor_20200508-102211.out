# Model: densenet121
# Dataset: cifardecem
# Batch size: 128
# Freezeout: False
# Low precision: True
# Epochs: 90
# Initial learning rate: 0.1
# module_name: models_cifardecem.densenet
<function densenet121 at 0x7f714179af28>
# model requested: 'densenet121'
# printing out the model
DenseNet(
  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (features): Sequential(
    (dense_block_layer_0): Sequential(
      (bottle_neck_layer_0): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_1): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_2): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_3): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_4): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_5): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
    )
    (transition_layer_0): Transition(
      (down_sample): Sequential(
        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): AvgPool2d(kernel_size=2, stride=2, padding=0)
      )
    )
    (dense_block_layer_1): Sequential(
      (bottle_neck_layer_0): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_1): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_2): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_3): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_4): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_5): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_6): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_7): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_8): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_9): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_10): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_11): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
    )
    (transition_layer_1): Transition(
      (down_sample): Sequential(
        (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): AvgPool2d(kernel_size=2, stride=2, padding=0)
      )
    )
    (dense_block_layer_2): Sequential(
      (bottle_neck_layer_0): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_1): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_2): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_3): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_4): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_5): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_6): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_7): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_8): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_9): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_10): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_11): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_12): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_13): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_14): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_15): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_16): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_17): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_18): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_19): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_20): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_21): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_22): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_23): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
    )
    (transition_layer_2): Transition(
      (down_sample): Sequential(
        (0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): AvgPool2d(kernel_size=2, stride=2, padding=0)
      )
    )
    (dense_block3): Sequential(
      (bottle_neck_layer_0): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_1): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_2): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_3): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_4): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_5): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_6): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_7): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_8): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_9): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_10): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_11): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_12): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_13): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_14): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_15): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
    )
    (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (linear): Linear(in_features=1024, out_features=10, bias=True)
)
# model is low precision
# Model: densenet121
# Dataset: cifardecem
# Freezeout: False
# Low precision: True
# Initial learning rate: 0.1
# Criterion: CrossEntropyLoss()
# Optimizer: SGD
#	lr 0.1
#	momentum 0.9
#	dampening 0
#	weight_decay 0.0001
#	nesterov False
CIFAR10 loading and normalization
==> Preparing data..
# CIFAR10 / low precision
Files already downloaded and verified
Files already downloaded and verified
#--> Training data: <--#
#-->>
EPOCH 0
# current learning rate: 0.1
# Switched to train mode...
Epoch: [0][  0/391]	Time  4.641 ( 4.641)	Data  0.106 ( 0.106)	Loss 2.3750e+00 (2.3750e+00)	Acc@1  10.16 ( 10.16)	Acc@5  50.00 ( 50.00)
Epoch: [0][ 10/391]	Time  0.117 ( 0.532)	Data  0.001 ( 0.010)	Loss 2.6953e+00 (2.4416e+00)	Acc@1  11.72 ( 17.33)	Acc@5  73.44 ( 66.41)
Epoch: [0][ 20/391]	Time  0.120 ( 0.336)	Data  0.001 ( 0.006)	Loss 1.9258e+00 (2.3207e+00)	Acc@1  32.81 ( 20.54)	Acc@5  78.12 ( 70.98)
Epoch: [0][ 30/391]	Time  0.121 ( 0.266)	Data  0.001 ( 0.004)	Loss 1.9551e+00 (2.2373e+00)	Acc@1  18.75 ( 21.19)	Acc@5  85.94 ( 74.19)
Epoch: [0][ 40/391]	Time  0.117 ( 0.230)	Data  0.001 ( 0.004)	Loss 2.0488e+00 (2.1865e+00)	Acc@1  32.81 ( 22.58)	Acc@5  82.81 ( 75.90)
Epoch: [0][ 50/391]	Time  0.121 ( 0.208)	Data  0.001 ( 0.003)	Loss 2.0195e+00 (2.1368e+00)	Acc@1  23.44 ( 24.10)	Acc@5  82.81 ( 77.48)
Epoch: [0][ 60/391]	Time  0.117 ( 0.193)	Data  0.001 ( 0.003)	Loss 1.8447e+00 (2.0957e+00)	Acc@1  31.25 ( 24.96)	Acc@5  82.81 ( 78.48)
Epoch: [0][ 70/391]	Time  0.116 ( 0.183)	Data  0.001 ( 0.002)	Loss 1.8457e+00 (2.0564e+00)	Acc@1  25.78 ( 25.80)	Acc@5  86.72 ( 79.48)
Epoch: [0][ 80/391]	Time  0.117 ( 0.175)	Data  0.001 ( 0.002)	Loss 1.6113e+00 (2.0214e+00)	Acc@1  35.16 ( 26.69)	Acc@5  89.06 ( 80.36)
Epoch: [0][ 90/391]	Time  0.128 ( 0.169)	Data  0.001 ( 0.002)	Loss 1.7256e+00 (1.9910e+00)	Acc@1  36.72 ( 27.46)	Acc@5  85.16 ( 81.09)
Epoch: [0][100/391]	Time  0.118 ( 0.164)	Data  0.001 ( 0.002)	Loss 1.7100e+00 (1.9639e+00)	Acc@1  39.84 ( 28.32)	Acc@5  86.72 ( 81.79)
Epoch: [0][110/391]	Time  0.120 ( 0.160)	Data  0.001 ( 0.002)	Loss 1.5664e+00 (1.9376e+00)	Acc@1  41.41 ( 29.08)	Acc@5  92.97 ( 82.54)
Epoch: [0][120/391]	Time  0.115 ( 0.156)	Data  0.001 ( 0.002)	Loss 1.6826e+00 (1.9182e+00)	Acc@1  36.72 ( 29.60)	Acc@5  87.50 ( 83.08)
Epoch: [0][130/391]	Time  0.117 ( 0.153)	Data  0.001 ( 0.002)	Loss 1.7227e+00 (1.9030e+00)	Acc@1  36.72 ( 29.96)	Acc@5  84.38 ( 83.46)
Epoch: [0][140/391]	Time  0.116 ( 0.151)	Data  0.001 ( 0.002)	Loss 1.7754e+00 (1.8900e+00)	Acc@1  40.62 ( 30.35)	Acc@5  82.81 ( 83.76)
Epoch: [0][150/391]	Time  0.117 ( 0.149)	Data  0.001 ( 0.002)	Loss 1.6514e+00 (1.8718e+00)	Acc@1  39.84 ( 30.99)	Acc@5  86.72 ( 84.14)
Epoch: [0][160/391]	Time  0.116 ( 0.147)	Data  0.001 ( 0.002)	Loss 1.6299e+00 (1.8560e+00)	Acc@1  41.41 ( 31.58)	Acc@5  90.62 ( 84.44)
Epoch: [0][170/391]	Time  0.117 ( 0.145)	Data  0.001 ( 0.002)	Loss 1.5342e+00 (1.8405e+00)	Acc@1  42.19 ( 32.11)	Acc@5  93.75 ( 84.80)
Epoch: [0][180/391]	Time  0.117 ( 0.144)	Data  0.001 ( 0.002)	Loss 1.4922e+00 (1.8299e+00)	Acc@1  48.44 ( 32.59)	Acc@5  89.06 ( 85.06)
Epoch: [0][190/391]	Time  0.118 ( 0.143)	Data  0.001 ( 0.002)	Loss 1.6270e+00 (1.8157e+00)	Acc@1  39.84 ( 33.07)	Acc@5  91.41 ( 85.38)
Epoch: [0][200/391]	Time  0.125 ( 0.141)	Data  0.001 ( 0.002)	Loss 1.6758e+00 (1.8007e+00)	Acc@1  40.62 ( 33.68)	Acc@5  89.06 ( 85.70)
Epoch: [0][210/391]	Time  0.116 ( 0.140)	Data  0.001 ( 0.002)	Loss 1.4658e+00 (1.7892e+00)	Acc@1  46.09 ( 34.15)	Acc@5  94.53 ( 85.97)
Epoch: [0][220/391]	Time  0.125 ( 0.139)	Data  0.001 ( 0.001)	Loss 1.4717e+00 (1.7770e+00)	Acc@1  50.00 ( 34.66)	Acc@5  89.84 ( 86.14)
Epoch: [0][230/391]	Time  0.116 ( 0.138)	Data  0.001 ( 0.001)	Loss 1.4355e+00 (1.7649e+00)	Acc@1  48.44 ( 35.13)	Acc@5  93.75 ( 86.38)
Epoch: [0][240/391]	Time  0.117 ( 0.138)	Data  0.001 ( 0.001)	Loss 1.5771e+00 (1.7536e+00)	Acc@1  44.53 ( 35.53)	Acc@5  89.06 ( 86.60)
Epoch: [0][250/391]	Time  0.127 ( 0.137)	Data  0.001 ( 0.001)	Loss 1.4824e+00 (1.7430e+00)	Acc@1  41.41 ( 35.93)	Acc@5  92.19 ( 86.79)
Epoch: [0][260/391]	Time  0.115 ( 0.136)	Data  0.001 ( 0.001)	Loss 1.2578e+00 (1.7318e+00)	Acc@1  51.56 ( 36.32)	Acc@5  97.66 ( 87.00)
Epoch: [0][270/391]	Time  0.117 ( 0.136)	Data  0.001 ( 0.001)	Loss 1.4551e+00 (1.7218e+00)	Acc@1  46.09 ( 36.66)	Acc@5  92.19 ( 87.17)
Epoch: [0][280/391]	Time  0.118 ( 0.135)	Data  0.001 ( 0.001)	Loss 1.5332e+00 (1.7127e+00)	Acc@1  43.75 ( 37.02)	Acc@5  93.75 ( 87.39)
Epoch: [0][290/391]	Time  0.119 ( 0.134)	Data  0.001 ( 0.001)	Loss 1.4551e+00 (1.7049e+00)	Acc@1  54.69 ( 37.35)	Acc@5  89.84 ( 87.53)
Epoch: [0][300/391]	Time  0.117 ( 0.134)	Data  0.001 ( 0.001)	Loss 1.4014e+00 (1.6942e+00)	Acc@1  52.34 ( 37.72)	Acc@5  90.62 ( 87.72)
Epoch: [0][310/391]	Time  0.117 ( 0.133)	Data  0.001 ( 0.001)	Loss 1.3984e+00 (1.6862e+00)	Acc@1  46.88 ( 38.08)	Acc@5  92.97 ( 87.84)
Epoch: [0][320/391]	Time  0.117 ( 0.133)	Data  0.001 ( 0.001)	Loss 1.4932e+00 (1.6784e+00)	Acc@1  42.97 ( 38.37)	Acc@5  92.97 ( 88.03)
Epoch: [0][330/391]	Time  0.122 ( 0.133)	Data  0.001 ( 0.001)	Loss 1.6113e+00 (1.6723e+00)	Acc@1  42.97 ( 38.61)	Acc@5  87.50 ( 88.14)
Epoch: [0][340/391]	Time  0.117 ( 0.132)	Data  0.001 ( 0.001)	Loss 1.2949e+00 (1.6629e+00)	Acc@1  57.81 ( 38.93)	Acc@5  94.53 ( 88.31)
Epoch: [0][350/391]	Time  0.116 ( 0.132)	Data  0.001 ( 0.001)	Loss 1.4238e+00 (1.6547e+00)	Acc@1  42.19 ( 39.17)	Acc@5  92.97 ( 88.46)
Epoch: [0][360/391]	Time  0.126 ( 0.131)	Data  0.001 ( 0.001)	Loss 1.3115e+00 (1.6474e+00)	Acc@1  50.78 ( 39.39)	Acc@5  97.66 ( 88.59)
Epoch: [0][370/391]	Time  0.118 ( 0.131)	Data  0.001 ( 0.001)	Loss 1.4541e+00 (1.6396e+00)	Acc@1  47.66 ( 39.69)	Acc@5  91.41 ( 88.70)
Epoch: [0][380/391]	Time  0.119 ( 0.131)	Data  0.001 ( 0.001)	Loss 1.3643e+00 (1.6326e+00)	Acc@1  50.00 ( 39.92)	Acc@5  92.19 ( 88.83)
Epoch: [0][390/391]	Time  1.444 ( 0.134)	Data  0.001 ( 0.001)	Loss 1.4443e+00 (1.6254e+00)	Acc@1  43.75 ( 40.17)	Acc@5  90.00 ( 88.94)
## e[0] optimizer.zero_grad (sum) time: 0.6894104480743408
## e[0]       loss.backward (sum) time: 16.065802097320557
## e[0]      optimizer.step (sum) time: 8.049598693847656
## epoch[0] training(only) time: 52.38722896575928
# Switched to evaluate mode...
Test: [  0/100]	Time  0.714 ( 0.714)	Loss 1.1992e+00 (1.1992e+00)	Acc@1  56.00 ( 56.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.045 ( 0.108)	Loss 1.1982e+00 (1.2755e+00)	Acc@1  60.00 ( 54.82)	Acc@5  95.00 ( 94.91)
Test: [ 20/100]	Time  0.045 ( 0.079)	Loss 1.1846e+00 (1.2786e+00)	Acc@1  50.00 ( 53.62)	Acc@5  94.00 ( 95.00)
Test: [ 30/100]	Time  0.048 ( 0.068)	Loss 1.2100e+00 (1.2906e+00)	Acc@1  56.00 ( 53.48)	Acc@5  96.00 ( 94.84)
Test: [ 40/100]	Time  0.045 ( 0.063)	Loss 1.3008e+00 (1.2950e+00)	Acc@1  48.00 ( 52.78)	Acc@5  95.00 ( 94.68)
Test: [ 50/100]	Time  0.045 ( 0.060)	Loss 1.2266e+00 (1.2871e+00)	Acc@1  51.00 ( 52.94)	Acc@5  96.00 ( 94.84)
Test: [ 60/100]	Time  0.046 ( 0.058)	Loss 1.3340e+00 (1.2840e+00)	Acc@1  49.00 ( 52.85)	Acc@5  94.00 ( 94.77)
Test: [ 70/100]	Time  0.050 ( 0.056)	Loss 1.2197e+00 (1.2876e+00)	Acc@1  56.00 ( 52.52)	Acc@5  94.00 ( 94.82)
Test: [ 80/100]	Time  0.045 ( 0.055)	Loss 1.0371e+00 (1.2828e+00)	Acc@1  61.00 ( 52.77)	Acc@5  96.00 ( 94.89)
Test: [ 90/100]	Time  0.045 ( 0.054)	Loss 1.2256e+00 (1.2878e+00)	Acc@1  48.00 ( 52.52)	Acc@5 100.00 ( 94.88)
 * Acc@1 52.440 Acc@5 94.830
### epoch[0] execution time: 57.784332036972046
EPOCH 1
# current learning rate: 0.1
# Switched to train mode...
Epoch: [1][  0/391]	Time  0.323 ( 0.323)	Data  0.144 ( 0.144)	Loss 1.3115e+00 (1.3115e+00)	Acc@1  50.00 ( 50.00)	Acc@5  95.31 ( 95.31)
Epoch: [1][ 10/391]	Time  0.117 ( 0.138)	Data  0.001 ( 0.014)	Loss 1.3369e+00 (1.3124e+00)	Acc@1  50.00 ( 52.27)	Acc@5  96.09 ( 94.39)
Epoch: [1][ 20/391]	Time  0.116 ( 0.128)	Data  0.001 ( 0.008)	Loss 1.3643e+00 (1.3340e+00)	Acc@1  43.75 ( 50.52)	Acc@5  93.75 ( 94.20)
Epoch: [1][ 30/391]	Time  0.120 ( 0.126)	Data  0.001 ( 0.006)	Loss 1.4229e+00 (1.3429e+00)	Acc@1  44.53 ( 50.78)	Acc@5  92.19 ( 94.08)
Epoch: [1][ 40/391]	Time  0.116 ( 0.124)	Data  0.001 ( 0.004)	Loss 1.3506e+00 (1.3323e+00)	Acc@1  49.22 ( 51.03)	Acc@5  95.31 ( 94.25)
Epoch: [1][ 50/391]	Time  0.122 ( 0.123)	Data  0.001 ( 0.004)	Loss 1.2705e+00 (1.3148e+00)	Acc@1  58.59 ( 52.13)	Acc@5  92.19 ( 94.18)
Epoch: [1][ 60/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.2383e+00 (1.3054e+00)	Acc@1  54.69 ( 52.63)	Acc@5  96.88 ( 94.24)
Epoch: [1][ 70/391]	Time  0.116 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.2979e+00 (1.2982e+00)	Acc@1  56.25 ( 53.08)	Acc@5  95.31 ( 94.33)
Epoch: [1][ 80/391]	Time  0.126 ( 0.122)	Data  0.001 ( 0.003)	Loss 9.9121e-01 (1.2874e+00)	Acc@1  67.97 ( 53.42)	Acc@5  95.31 ( 94.47)
Epoch: [1][ 90/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.003)	Loss 1.2148e+00 (1.2822e+00)	Acc@1  50.78 ( 53.56)	Acc@5  96.09 ( 94.49)
Epoch: [1][100/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.2266e+00 (1.2790e+00)	Acc@1  54.69 ( 53.65)	Acc@5  93.75 ( 94.51)
Epoch: [1][110/391]	Time  0.126 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.2168e+00 (1.2719e+00)	Acc@1  50.78 ( 53.89)	Acc@5  97.66 ( 94.62)
Epoch: [1][120/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.2354e+00 (1.2656e+00)	Acc@1  53.91 ( 54.15)	Acc@5  92.97 ( 94.65)
Epoch: [1][130/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.1016e+00 (1.2618e+00)	Acc@1  58.59 ( 54.33)	Acc@5  94.53 ( 94.67)
Epoch: [1][140/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.1035e+00 (1.2582e+00)	Acc@1  64.06 ( 54.53)	Acc@5  96.09 ( 94.69)
Epoch: [1][150/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 9.9854e-01 (1.2525e+00)	Acc@1  61.72 ( 54.73)	Acc@5  97.66 ( 94.70)
Epoch: [1][160/391]	Time  0.116 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.2627e+00 (1.2476e+00)	Acc@1  50.78 ( 54.91)	Acc@5  96.09 ( 94.74)
Epoch: [1][170/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.1904e+00 (1.2401e+00)	Acc@1  55.47 ( 55.23)	Acc@5  96.09 ( 94.81)
Epoch: [1][180/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.2227e+00 (1.2325e+00)	Acc@1  57.03 ( 55.47)	Acc@5  96.09 ( 94.91)
Epoch: [1][190/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.1426e+00 (1.2255e+00)	Acc@1  60.16 ( 55.74)	Acc@5  94.53 ( 94.96)
Epoch: [1][200/391]	Time  0.125 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.0566e+00 (1.2178e+00)	Acc@1  54.69 ( 56.01)	Acc@5  96.88 ( 95.04)
Epoch: [1][210/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.0996e+00 (1.2128e+00)	Acc@1  61.72 ( 56.22)	Acc@5  93.75 ( 95.06)
Epoch: [1][220/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.0684e+00 (1.2092e+00)	Acc@1  64.06 ( 56.41)	Acc@5  96.09 ( 95.09)
Epoch: [1][230/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.0654e+00 (1.2020e+00)	Acc@1  60.16 ( 56.69)	Acc@5  96.88 ( 95.17)
Epoch: [1][240/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.1025e+00 (1.2007e+00)	Acc@1  62.50 ( 56.77)	Acc@5  96.88 ( 95.20)
Epoch: [1][250/391]	Time  0.116 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.0459e+00 (1.1992e+00)	Acc@1  59.38 ( 56.93)	Acc@5  96.88 ( 95.21)
Epoch: [1][260/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.1416e+00 (1.1935e+00)	Acc@1  66.41 ( 57.11)	Acc@5  93.75 ( 95.25)
Epoch: [1][270/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.1279e+00 (1.1902e+00)	Acc@1  62.50 ( 57.27)	Acc@5  93.75 ( 95.28)
Epoch: [1][280/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.1826e+00 (1.1880e+00)	Acc@1  58.59 ( 57.31)	Acc@5  96.88 ( 95.28)
Epoch: [1][290/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 9.1602e-01 (1.1852e+00)	Acc@1  64.84 ( 57.40)	Acc@5  95.31 ( 95.29)
Epoch: [1][300/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.0049e+00 (1.1815e+00)	Acc@1  64.84 ( 57.54)	Acc@5  95.31 ( 95.30)
Epoch: [1][310/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 9.6387e-01 (1.1763e+00)	Acc@1  69.53 ( 57.75)	Acc@5  95.31 ( 95.34)
Epoch: [1][320/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 8.9355e-01 (1.1714e+00)	Acc@1  66.41 ( 57.90)	Acc@5  98.44 ( 95.39)
Epoch: [1][330/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 9.4434e-01 (1.1672e+00)	Acc@1  69.53 ( 58.07)	Acc@5  96.09 ( 95.41)
Epoch: [1][340/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 9.9414e-01 (1.1613e+00)	Acc@1  64.06 ( 58.31)	Acc@5  97.66 ( 95.47)
Epoch: [1][350/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 9.9854e-01 (1.1578e+00)	Acc@1  66.41 ( 58.48)	Acc@5  96.09 ( 95.49)
Epoch: [1][360/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.0469e+00 (1.1537e+00)	Acc@1  66.41 ( 58.63)	Acc@5  94.53 ( 95.51)
Epoch: [1][370/391]	Time  0.116 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.1885e+00 (1.1494e+00)	Acc@1  56.25 ( 58.76)	Acc@5  96.88 ( 95.55)
Epoch: [1][380/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 9.7021e-01 (1.1444e+00)	Acc@1  60.16 ( 58.96)	Acc@5  95.31 ( 95.58)
Epoch: [1][390/391]	Time  0.112 ( 0.120)	Data  0.001 ( 0.001)	Loss 9.2627e-01 (1.1414e+00)	Acc@1  71.25 ( 59.09)	Acc@5  95.00 ( 95.59)
## e[1] optimizer.zero_grad (sum) time: 0.6845099925994873
## e[1]       loss.backward (sum) time: 14.274154424667358
## e[1]      optimizer.step (sum) time: 8.101885795593262
## epoch[1] training(only) time: 46.85393786430359
# Switched to evaluate mode...
Test: [  0/100]	Time  0.179 ( 0.179)	Loss 1.1611e+00 (1.1611e+00)	Acc@1  64.00 ( 64.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.045 ( 0.059)	Loss 9.2334e-01 (1.0714e+00)	Acc@1  66.00 ( 63.45)	Acc@5  99.00 ( 96.09)
Test: [ 20/100]	Time  0.049 ( 0.053)	Loss 9.8877e-01 (1.0983e+00)	Acc@1  57.00 ( 62.43)	Acc@5  98.00 ( 96.14)
Test: [ 30/100]	Time  0.047 ( 0.051)	Loss 1.0439e+00 (1.1038e+00)	Acc@1  69.00 ( 62.42)	Acc@5  96.00 ( 96.32)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 1.0420e+00 (1.1145e+00)	Acc@1  62.00 ( 62.24)	Acc@5  97.00 ( 96.20)
Test: [ 50/100]	Time  0.050 ( 0.050)	Loss 1.0625e+00 (1.0968e+00)	Acc@1  63.00 ( 62.96)	Acc@5  93.00 ( 96.27)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 9.9609e-01 (1.0915e+00)	Acc@1  59.00 ( 62.61)	Acc@5  97.00 ( 96.36)
Test: [ 70/100]	Time  0.048 ( 0.049)	Loss 9.3262e-01 (1.0944e+00)	Acc@1  66.00 ( 62.56)	Acc@5 100.00 ( 96.44)
Test: [ 80/100]	Time  0.045 ( 0.048)	Loss 1.1133e+00 (1.0949e+00)	Acc@1  63.00 ( 62.60)	Acc@5  93.00 ( 96.51)
Test: [ 90/100]	Time  0.045 ( 0.048)	Loss 1.0000e+00 (1.0896e+00)	Acc@1  65.00 ( 62.58)	Acc@5 100.00 ( 96.52)
 * Acc@1 62.640 Acc@5 96.450
### epoch[1] execution time: 51.72794032096863
EPOCH 2
# current learning rate: 0.1
# Switched to train mode...
Epoch: [2][  0/391]	Time  0.274 ( 0.274)	Data  0.142 ( 0.142)	Loss 1.0303e+00 (1.0303e+00)	Acc@1  61.72 ( 61.72)	Acc@5  96.88 ( 96.88)
Epoch: [2][ 10/391]	Time  0.119 ( 0.132)	Data  0.001 ( 0.014)	Loss 9.2969e-01 (1.0154e+00)	Acc@1  64.06 ( 63.71)	Acc@5  96.88 ( 96.45)
Epoch: [2][ 20/391]	Time  0.117 ( 0.126)	Data  0.001 ( 0.008)	Loss 9.7510e-01 (1.0127e+00)	Acc@1  60.94 ( 63.62)	Acc@5  96.88 ( 96.65)
Epoch: [2][ 30/391]	Time  0.120 ( 0.124)	Data  0.001 ( 0.006)	Loss 1.0684e+00 (9.9820e-01)	Acc@1  64.84 ( 64.06)	Acc@5  94.53 ( 96.77)
Epoch: [2][ 40/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.004)	Loss 8.7451e-01 (9.7937e-01)	Acc@1  73.44 ( 64.62)	Acc@5  99.22 ( 97.01)
Epoch: [2][ 50/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.004)	Loss 8.0176e-01 (9.6860e-01)	Acc@1  74.22 ( 65.23)	Acc@5  96.88 ( 96.97)
Epoch: [2][ 60/391]	Time  0.116 ( 0.121)	Data  0.001 ( 0.003)	Loss 1.1631e+00 (9.7482e-01)	Acc@1  60.94 ( 65.20)	Acc@5  92.97 ( 96.84)
Epoch: [2][ 70/391]	Time  0.115 ( 0.121)	Data  0.001 ( 0.003)	Loss 1.0205e+00 (9.7700e-01)	Acc@1  65.62 ( 65.37)	Acc@5  97.66 ( 96.73)
Epoch: [2][ 80/391]	Time  0.119 ( 0.121)	Data  0.002 ( 0.003)	Loss 9.9756e-01 (9.7188e-01)	Acc@1  59.38 ( 65.60)	Acc@5  98.44 ( 96.74)
Epoch: [2][ 90/391]	Time  0.115 ( 0.121)	Data  0.001 ( 0.003)	Loss 9.1504e-01 (9.7015e-01)	Acc@1  67.19 ( 65.69)	Acc@5  96.88 ( 96.76)
Epoch: [2][100/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 8.1055e-01 (9.6611e-01)	Acc@1  70.31 ( 65.80)	Acc@5  98.44 ( 96.87)
Epoch: [2][110/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.0684e+00 (9.6607e-01)	Acc@1  63.28 ( 65.80)	Acc@5  95.31 ( 96.90)
Epoch: [2][120/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 8.6475e-01 (9.6749e-01)	Acc@1  68.75 ( 65.72)	Acc@5  98.44 ( 96.89)
Epoch: [2][130/391]	Time  0.122 ( 0.120)	Data  0.001 ( 0.002)	Loss 8.3740e-01 (9.6552e-01)	Acc@1  71.88 ( 65.77)	Acc@5  98.44 ( 96.92)
Epoch: [2][140/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 9.3750e-01 (9.6156e-01)	Acc@1  64.84 ( 65.90)	Acc@5  97.66 ( 96.93)
Epoch: [2][150/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.9082e-01 (9.5432e-01)	Acc@1  75.78 ( 66.21)	Acc@5  99.22 ( 96.99)
Epoch: [2][160/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 9.1992e-01 (9.5194e-01)	Acc@1  65.62 ( 66.25)	Acc@5  98.44 ( 96.98)
Epoch: [2][170/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.0146e+00 (9.5231e-01)	Acc@1  66.41 ( 66.29)	Acc@5  96.09 ( 96.97)
Epoch: [2][180/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.0527e+00 (9.5031e-01)	Acc@1  64.84 ( 66.37)	Acc@5  96.09 ( 97.00)
Epoch: [2][190/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 8.7695e-01 (9.4630e-01)	Acc@1  71.88 ( 66.57)	Acc@5  96.09 ( 97.03)
Epoch: [2][200/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 8.0469e-01 (9.4338e-01)	Acc@1  71.09 ( 66.66)	Acc@5  99.22 ( 97.05)
Epoch: [2][210/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 9.3750e-01 (9.4045e-01)	Acc@1  68.75 ( 66.78)	Acc@5  98.44 ( 97.08)
Epoch: [2][220/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 9.1797e-01 (9.3933e-01)	Acc@1  69.53 ( 66.84)	Acc@5  98.44 ( 97.08)
Epoch: [2][230/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 9.9609e-01 (9.3698e-01)	Acc@1  64.84 ( 66.96)	Acc@5  94.53 ( 97.09)
Epoch: [2][240/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 8.9746e-01 (9.3555e-01)	Acc@1  68.75 ( 66.95)	Acc@5  97.66 ( 97.09)
Epoch: [2][250/391]	Time  0.135 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.0449e+00 (9.3334e-01)	Acc@1  64.06 ( 67.06)	Acc@5  95.31 ( 97.10)
Epoch: [2][260/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 8.1738e-01 (9.2946e-01)	Acc@1  73.44 ( 67.21)	Acc@5  93.75 ( 97.11)
Epoch: [2][270/391]	Time  0.115 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.2305e+00 (9.2725e-01)	Acc@1  57.03 ( 67.31)	Acc@5  95.31 ( 97.12)
Epoch: [2][280/391]	Time  0.128 ( 0.120)	Data  0.001 ( 0.002)	Loss 7.2803e-01 (9.2488e-01)	Acc@1  72.66 ( 67.43)	Acc@5 100.00 ( 97.15)
Epoch: [2][290/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 7.7686e-01 (9.2470e-01)	Acc@1  70.31 ( 67.38)	Acc@5  98.44 ( 97.16)
Epoch: [2][300/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 8.1738e-01 (9.2356e-01)	Acc@1  67.97 ( 67.38)	Acc@5  99.22 ( 97.19)
Epoch: [2][310/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 7.8760e-01 (9.2116e-01)	Acc@1  74.22 ( 67.51)	Acc@5  99.22 ( 97.21)
Epoch: [2][320/391]	Time  0.116 ( 0.120)	Data  0.001 ( 0.002)	Loss 8.2129e-01 (9.1941e-01)	Acc@1  76.56 ( 67.57)	Acc@5  99.22 ( 97.24)
Epoch: [2][330/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 8.5645e-01 (9.1797e-01)	Acc@1  67.97 ( 67.59)	Acc@5  96.88 ( 97.25)
Epoch: [2][340/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 9.5361e-01 (9.1633e-01)	Acc@1  68.75 ( 67.64)	Acc@5  97.66 ( 97.25)
Epoch: [2][350/391]	Time  0.116 ( 0.120)	Data  0.001 ( 0.001)	Loss 9.8047e-01 (9.1374e-01)	Acc@1  68.75 ( 67.74)	Acc@5  95.31 ( 97.27)
Epoch: [2][360/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 7.5879e-01 (9.1090e-01)	Acc@1  75.78 ( 67.87)	Acc@5  97.66 ( 97.28)
Epoch: [2][370/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.9922e-01 (9.0691e-01)	Acc@1  72.66 ( 68.03)	Acc@5 100.00 ( 97.31)
Epoch: [2][380/391]	Time  0.116 ( 0.120)	Data  0.001 ( 0.001)	Loss 8.5889e-01 (9.0577e-01)	Acc@1  65.62 ( 68.06)	Acc@5  98.44 ( 97.32)
Epoch: [2][390/391]	Time  0.105 ( 0.119)	Data  0.001 ( 0.001)	Loss 5.8057e-01 (9.0366e-01)	Acc@1  82.50 ( 68.13)	Acc@5 100.00 ( 97.35)
## e[2] optimizer.zero_grad (sum) time: 0.6878530979156494
## e[2]       loss.backward (sum) time: 14.171966791152954
## e[2]      optimizer.step (sum) time: 8.128744125366211
## epoch[2] training(only) time: 46.776965856552124
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 7.2949e-01 (7.2949e-01)	Acc@1  74.00 ( 74.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 7.0459e-01 (8.2795e-01)	Acc@1  77.00 ( 70.82)	Acc@5  98.00 ( 98.00)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 8.7451e-01 (8.5547e-01)	Acc@1  65.00 ( 69.95)	Acc@5  99.00 ( 98.14)
Test: [ 30/100]	Time  0.045 ( 0.052)	Loss 8.0371e-01 (8.5457e-01)	Acc@1  73.00 ( 70.61)	Acc@5  98.00 ( 98.03)
Test: [ 40/100]	Time  0.045 ( 0.051)	Loss 9.7510e-01 (8.6149e-01)	Acc@1  70.00 ( 70.46)	Acc@5  98.00 ( 98.00)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 7.8955e-01 (8.5558e-01)	Acc@1  72.00 ( 70.55)	Acc@5  97.00 ( 97.98)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 6.6992e-01 (8.5516e-01)	Acc@1  68.00 ( 70.28)	Acc@5 100.00 ( 97.97)
Test: [ 70/100]	Time  0.045 ( 0.049)	Loss 8.8916e-01 (8.5647e-01)	Acc@1  71.00 ( 70.32)	Acc@5  96.00 ( 97.99)
Test: [ 80/100]	Time  0.049 ( 0.049)	Loss 8.5596e-01 (8.5662e-01)	Acc@1  71.00 ( 70.21)	Acc@5  99.00 ( 97.98)
Test: [ 90/100]	Time  0.046 ( 0.048)	Loss 7.4316e-01 (8.5528e-01)	Acc@1  73.00 ( 70.20)	Acc@5 100.00 ( 97.99)
 * Acc@1 70.090 Acc@5 97.980
### epoch[2] execution time: 51.6731276512146
EPOCH 3
# current learning rate: 0.1
# Switched to train mode...
Epoch: [3][  0/391]	Time  0.290 ( 0.290)	Data  0.159 ( 0.159)	Loss 8.5156e-01 (8.5156e-01)	Acc@1  69.53 ( 69.53)	Acc@5  98.44 ( 98.44)
Epoch: [3][ 10/391]	Time  0.117 ( 0.134)	Data  0.001 ( 0.015)	Loss 7.2217e-01 (8.2497e-01)	Acc@1  67.97 ( 68.39)	Acc@5  98.44 ( 98.01)
Epoch: [3][ 20/391]	Time  0.118 ( 0.127)	Data  0.001 ( 0.008)	Loss 7.9248e-01 (8.1017e-01)	Acc@1  71.88 ( 70.01)	Acc@5  95.31 ( 97.84)
Epoch: [3][ 30/391]	Time  0.117 ( 0.124)	Data  0.001 ( 0.006)	Loss 7.4463e-01 (8.0800e-01)	Acc@1  71.88 ( 70.59)	Acc@5  99.22 ( 98.06)
Epoch: [3][ 40/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.005)	Loss 7.9639e-01 (8.0401e-01)	Acc@1  70.31 ( 70.81)	Acc@5  96.09 ( 98.04)
Epoch: [3][ 50/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.004)	Loss 8.9014e-01 (8.0654e-01)	Acc@1  66.41 ( 70.91)	Acc@5  96.88 ( 98.10)
Epoch: [3][ 60/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.004)	Loss 7.7246e-01 (8.0585e-01)	Acc@1  75.78 ( 71.16)	Acc@5  98.44 ( 98.08)
Epoch: [3][ 70/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.003)	Loss 6.8359e-01 (8.0228e-01)	Acc@1  74.22 ( 71.29)	Acc@5 100.00 ( 98.12)
Epoch: [3][ 80/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.003)	Loss 7.0361e-01 (8.0038e-01)	Acc@1  73.44 ( 71.27)	Acc@5  97.66 ( 98.10)
Epoch: [3][ 90/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.003)	Loss 6.8213e-01 (7.9664e-01)	Acc@1  75.78 ( 71.58)	Acc@5  99.22 ( 98.07)
Epoch: [3][100/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.003)	Loss 9.6973e-01 (8.0224e-01)	Acc@1  70.31 ( 71.46)	Acc@5  96.88 ( 98.04)
Epoch: [3][110/391]	Time  0.126 ( 0.120)	Data  0.001 ( 0.002)	Loss 7.1143e-01 (7.9905e-01)	Acc@1  74.22 ( 71.56)	Acc@5  98.44 ( 98.01)
Epoch: [3][120/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 7.0996e-01 (7.9567e-01)	Acc@1  75.00 ( 71.71)	Acc@5  98.44 ( 98.06)
Epoch: [3][130/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 7.7344e-01 (8.0014e-01)	Acc@1  67.19 ( 71.48)	Acc@5  99.22 ( 98.01)
Epoch: [3][140/391]	Time  0.126 ( 0.120)	Data  0.001 ( 0.002)	Loss 8.6572e-01 (7.9804e-01)	Acc@1  69.53 ( 71.63)	Acc@5  98.44 ( 98.02)
Epoch: [3][150/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 8.2617e-01 (7.9725e-01)	Acc@1  72.66 ( 71.72)	Acc@5  96.88 ( 98.01)
Epoch: [3][160/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 8.2812e-01 (7.9701e-01)	Acc@1  72.66 ( 71.70)	Acc@5  99.22 ( 98.05)
Epoch: [3][170/391]	Time  0.116 ( 0.120)	Data  0.001 ( 0.002)	Loss 9.0430e-01 (7.9646e-01)	Acc@1  72.66 ( 71.83)	Acc@5  98.44 ( 98.07)
Epoch: [3][180/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 9.0967e-01 (7.9815e-01)	Acc@1  68.75 ( 71.76)	Acc@5  94.53 ( 98.07)
Epoch: [3][190/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 8.2812e-01 (7.9799e-01)	Acc@1  69.53 ( 71.76)	Acc@5  99.22 ( 98.08)
Epoch: [3][200/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.2549e-01 (7.9846e-01)	Acc@1  78.91 ( 71.75)	Acc@5  97.66 ( 98.07)
Epoch: [3][210/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 8.6621e-01 (7.9742e-01)	Acc@1  71.09 ( 71.83)	Acc@5  96.88 ( 98.07)
Epoch: [3][220/391]	Time  0.116 ( 0.119)	Data  0.001 ( 0.002)	Loss 5.6348e-01 (7.9448e-01)	Acc@1  78.91 ( 71.95)	Acc@5 100.00 ( 98.08)
Epoch: [3][230/391]	Time  0.119 ( 0.119)	Data  0.001 ( 0.002)	Loss 8.6230e-01 (7.9345e-01)	Acc@1  71.09 ( 72.00)	Acc@5  97.66 ( 98.08)
Epoch: [3][240/391]	Time  0.116 ( 0.119)	Data  0.001 ( 0.002)	Loss 7.5244e-01 (7.9401e-01)	Acc@1  71.88 ( 72.03)	Acc@5  96.88 ( 98.06)
Epoch: [3][250/391]	Time  0.118 ( 0.119)	Data  0.001 ( 0.002)	Loss 8.2861e-01 (7.9179e-01)	Acc@1  70.31 ( 72.13)	Acc@5  99.22 ( 98.09)
Epoch: [3][260/391]	Time  0.120 ( 0.119)	Data  0.001 ( 0.002)	Loss 7.6221e-01 (7.9157e-01)	Acc@1  71.88 ( 72.11)	Acc@5  99.22 ( 98.08)
Epoch: [3][270/391]	Time  0.117 ( 0.119)	Data  0.001 ( 0.002)	Loss 7.5098e-01 (7.9047e-01)	Acc@1  75.78 ( 72.19)	Acc@5  98.44 ( 98.08)
Epoch: [3][280/391]	Time  0.123 ( 0.119)	Data  0.001 ( 0.002)	Loss 8.2422e-01 (7.9026e-01)	Acc@1  70.31 ( 72.21)	Acc@5  96.88 ( 98.10)
Epoch: [3][290/391]	Time  0.117 ( 0.119)	Data  0.001 ( 0.002)	Loss 6.8750e-01 (7.8792e-01)	Acc@1  75.78 ( 72.31)	Acc@5  99.22 ( 98.10)
Epoch: [3][300/391]	Time  0.118 ( 0.119)	Data  0.001 ( 0.002)	Loss 7.0264e-01 (7.8662e-01)	Acc@1  74.22 ( 72.36)	Acc@5  97.66 ( 98.11)
Epoch: [3][310/391]	Time  0.119 ( 0.119)	Data  0.001 ( 0.002)	Loss 7.0898e-01 (7.8517e-01)	Acc@1  70.31 ( 72.43)	Acc@5  99.22 ( 98.11)
Epoch: [3][320/391]	Time  0.128 ( 0.119)	Data  0.001 ( 0.001)	Loss 5.5273e-01 (7.8234e-01)	Acc@1  82.03 ( 72.52)	Acc@5  99.22 ( 98.13)
Epoch: [3][330/391]	Time  0.122 ( 0.119)	Data  0.001 ( 0.001)	Loss 7.3389e-01 (7.8122e-01)	Acc@1  75.00 ( 72.57)	Acc@5  99.22 ( 98.13)
Epoch: [3][340/391]	Time  0.118 ( 0.119)	Data  0.001 ( 0.001)	Loss 8.7646e-01 (7.8051e-01)	Acc@1  67.19 ( 72.61)	Acc@5  95.31 ( 98.12)
Epoch: [3][350/391]	Time  0.116 ( 0.119)	Data  0.001 ( 0.001)	Loss 8.5010e-01 (7.7962e-01)	Acc@1  68.75 ( 72.62)	Acc@5  98.44 ( 98.12)
Epoch: [3][360/391]	Time  0.118 ( 0.119)	Data  0.001 ( 0.001)	Loss 6.6553e-01 (7.7872e-01)	Acc@1  78.91 ( 72.66)	Acc@5  97.66 ( 98.12)
Epoch: [3][370/391]	Time  0.133 ( 0.119)	Data  0.001 ( 0.001)	Loss 9.3994e-01 (7.7836e-01)	Acc@1  66.41 ( 72.66)	Acc@5 100.00 ( 98.12)
Epoch: [3][380/391]	Time  0.118 ( 0.119)	Data  0.001 ( 0.001)	Loss 7.2168e-01 (7.7922e-01)	Acc@1  75.00 ( 72.62)	Acc@5  98.44 ( 98.12)
Epoch: [3][390/391]	Time  0.101 ( 0.119)	Data  0.001 ( 0.001)	Loss 7.5684e-01 (7.7824e-01)	Acc@1  71.25 ( 72.67)	Acc@5 100.00 ( 98.14)
## e[3] optimizer.zero_grad (sum) time: 0.6827282905578613
## e[3]       loss.backward (sum) time: 14.191165924072266
## e[3]      optimizer.step (sum) time: 8.058692216873169
## epoch[3] training(only) time: 46.718358755111694
# Switched to evaluate mode...
Test: [  0/100]	Time  0.192 ( 0.192)	Loss 6.2402e-01 (6.2402e-01)	Acc@1  80.00 ( 80.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.045 ( 0.059)	Loss 7.6416e-01 (7.1808e-01)	Acc@1  70.00 ( 74.64)	Acc@5  98.00 ( 98.82)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 7.5928e-01 (7.1082e-01)	Acc@1  74.00 ( 75.05)	Acc@5  98.00 ( 98.81)
Test: [ 30/100]	Time  0.049 ( 0.052)	Loss 6.6748e-01 (7.1343e-01)	Acc@1  78.00 ( 75.19)	Acc@5  98.00 ( 98.77)
Test: [ 40/100]	Time  0.048 ( 0.051)	Loss 7.4805e-01 (7.1941e-01)	Acc@1  75.00 ( 74.93)	Acc@5  99.00 ( 98.78)
Test: [ 50/100]	Time  0.053 ( 0.050)	Loss 6.7188e-01 (7.1575e-01)	Acc@1  77.00 ( 75.29)	Acc@5  98.00 ( 98.63)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 6.1035e-01 (7.1231e-01)	Acc@1  78.00 ( 75.31)	Acc@5  99.00 ( 98.69)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 7.7734e-01 (7.1760e-01)	Acc@1  75.00 ( 75.10)	Acc@5  98.00 ( 98.69)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 6.1377e-01 (7.1573e-01)	Acc@1  79.00 ( 75.14)	Acc@5  98.00 ( 98.67)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 5.6641e-01 (7.1599e-01)	Acc@1  82.00 ( 75.26)	Acc@5 100.00 ( 98.66)
 * Acc@1 75.240 Acc@5 98.650
### epoch[3] execution time: 51.62502956390381
EPOCH 4
# current learning rate: 0.1
# Switched to train mode...
Epoch: [4][  0/391]	Time  0.288 ( 0.288)	Data  0.149 ( 0.149)	Loss 6.8994e-01 (6.8994e-01)	Acc@1  71.09 ( 71.09)	Acc@5  98.44 ( 98.44)
Epoch: [4][ 10/391]	Time  0.116 ( 0.133)	Data  0.001 ( 0.015)	Loss 6.9971e-01 (6.8102e-01)	Acc@1  74.22 ( 75.64)	Acc@5  98.44 ( 98.30)
Epoch: [4][ 20/391]	Time  0.117 ( 0.126)	Data  0.001 ( 0.008)	Loss 5.6250e-01 (6.9022e-01)	Acc@1  83.59 ( 75.89)	Acc@5 100.00 ( 98.55)
Epoch: [4][ 30/391]	Time  0.117 ( 0.124)	Data  0.001 ( 0.006)	Loss 6.8018e-01 (6.8252e-01)	Acc@1  77.34 ( 75.93)	Acc@5  96.88 ( 98.46)
Epoch: [4][ 40/391]	Time  0.116 ( 0.123)	Data  0.001 ( 0.005)	Loss 8.4863e-01 (6.9162e-01)	Acc@1  68.75 ( 75.51)	Acc@5  98.44 ( 98.57)
Epoch: [4][ 50/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.004)	Loss 7.6758e-01 (7.0559e-01)	Acc@1  72.66 ( 75.15)	Acc@5  99.22 ( 98.47)
Epoch: [4][ 60/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.004)	Loss 6.1328e-01 (7.0052e-01)	Acc@1  76.56 ( 75.68)	Acc@5  98.44 ( 98.51)
Epoch: [4][ 70/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.003)	Loss 8.2568e-01 (7.0253e-01)	Acc@1  71.09 ( 75.36)	Acc@5  98.44 ( 98.51)
Epoch: [4][ 80/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.003)	Loss 6.1963e-01 (6.9759e-01)	Acc@1  75.78 ( 75.60)	Acc@5  99.22 ( 98.55)
Epoch: [4][ 90/391]	Time  0.116 ( 0.121)	Data  0.001 ( 0.003)	Loss 7.5977e-01 (6.9380e-01)	Acc@1  75.78 ( 75.72)	Acc@5  99.22 ( 98.57)
Epoch: [4][100/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.003)	Loss 7.4463e-01 (6.9774e-01)	Acc@1  69.53 ( 75.63)	Acc@5  97.66 ( 98.53)
Epoch: [4][110/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.3682e-01 (6.9784e-01)	Acc@1  75.00 ( 75.70)	Acc@5 100.00 ( 98.53)
Epoch: [4][120/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.8945e-01 (6.9560e-01)	Acc@1  77.34 ( 75.71)	Acc@5  98.44 ( 98.57)
Epoch: [4][130/391]	Time  0.116 ( 0.120)	Data  0.001 ( 0.002)	Loss 8.3398e-01 (6.9558e-01)	Acc@1  70.31 ( 75.75)	Acc@5  96.88 ( 98.57)
Epoch: [4][140/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 7.3584e-01 (6.9677e-01)	Acc@1  74.22 ( 75.71)	Acc@5  99.22 ( 98.58)
Epoch: [4][150/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 7.1875e-01 (6.9620e-01)	Acc@1  77.34 ( 75.70)	Acc@5  98.44 ( 98.57)
Epoch: [4][160/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.0059e-01 (6.9478e-01)	Acc@1  79.69 ( 75.80)	Acc@5  99.22 ( 98.57)
Epoch: [4][170/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 7.1191e-01 (6.9186e-01)	Acc@1  75.00 ( 75.90)	Acc@5  99.22 ( 98.62)
Epoch: [4][180/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 7.2070e-01 (6.9243e-01)	Acc@1  76.56 ( 75.92)	Acc@5  98.44 ( 98.63)
Epoch: [4][190/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.3379e-01 (6.9163e-01)	Acc@1  80.47 ( 75.92)	Acc@5 100.00 ( 98.65)
Epoch: [4][200/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 7.5391e-01 (6.9108e-01)	Acc@1  72.66 ( 75.89)	Acc@5 100.00 ( 98.67)
Epoch: [4][210/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.7139e-01 (6.9070e-01)	Acc@1  72.66 ( 75.84)	Acc@5  98.44 ( 98.68)
Epoch: [4][220/391]	Time  0.122 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.6211e-01 (6.8854e-01)	Acc@1  75.78 ( 75.93)	Acc@5  99.22 ( 98.67)
Epoch: [4][230/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.9424e-01 (6.8746e-01)	Acc@1  81.25 ( 75.96)	Acc@5  96.09 ( 98.65)
Epoch: [4][240/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 7.8906e-01 (6.8590e-01)	Acc@1  69.53 ( 76.00)	Acc@5  98.44 ( 98.65)
Epoch: [4][250/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.9131e-01 (6.8574e-01)	Acc@1  78.12 ( 76.00)	Acc@5 100.00 ( 98.65)
Epoch: [4][260/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.2393e-01 (6.8242e-01)	Acc@1  80.47 ( 76.13)	Acc@5 100.00 ( 98.65)
Epoch: [4][270/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 7.4219e-01 (6.8215e-01)	Acc@1  78.91 ( 76.12)	Acc@5  97.66 ( 98.64)
Epoch: [4][280/391]	Time  0.123 ( 0.120)	Data  0.001 ( 0.002)	Loss 7.6416e-01 (6.8256e-01)	Acc@1  71.09 ( 76.10)	Acc@5  96.88 ( 98.64)
Epoch: [4][290/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 7.4756e-01 (6.8201e-01)	Acc@1  77.34 ( 76.19)	Acc@5  98.44 ( 98.63)
Epoch: [4][300/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.6606e-01 (6.7911e-01)	Acc@1  85.16 ( 76.28)	Acc@5 100.00 ( 98.66)
Epoch: [4][310/391]	Time  0.127 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.9463e-01 (6.7947e-01)	Acc@1  82.81 ( 76.29)	Acc@5 100.00 ( 98.65)
Epoch: [4][320/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.9258e-01 (6.7680e-01)	Acc@1  88.28 ( 76.37)	Acc@5 100.00 ( 98.66)
Epoch: [4][330/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.1758e-01 (6.7585e-01)	Acc@1  83.59 ( 76.43)	Acc@5 100.00 ( 98.67)
Epoch: [4][340/391]	Time  0.133 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.5625e-01 (6.7660e-01)	Acc@1  77.34 ( 76.45)	Acc@5  98.44 ( 98.66)
Epoch: [4][350/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.4111e-01 (6.7694e-01)	Acc@1  76.56 ( 76.44)	Acc@5  99.22 ( 98.67)
Epoch: [4][360/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 7.4658e-01 (6.7573e-01)	Acc@1  73.44 ( 76.51)	Acc@5  98.44 ( 98.67)
Epoch: [4][370/391]	Time  0.116 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.2012e-01 (6.7400e-01)	Acc@1  77.34 ( 76.54)	Acc@5  97.66 ( 98.67)
Epoch: [4][380/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.3320e-01 (6.7338e-01)	Acc@1  80.47 ( 76.56)	Acc@5  99.22 ( 98.67)
Epoch: [4][390/391]	Time  0.107 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.7041e-01 (6.7224e-01)	Acc@1  77.50 ( 76.61)	Acc@5  95.00 ( 98.68)
## e[4] optimizer.zero_grad (sum) time: 0.6847305297851562
## e[4]       loss.backward (sum) time: 14.239804983139038
## e[4]      optimizer.step (sum) time: 8.079856872558594
## epoch[4] training(only) time: 46.85694646835327
# Switched to evaluate mode...
Test: [  0/100]	Time  0.179 ( 0.179)	Loss 6.5576e-01 (6.5576e-01)	Acc@1  79.00 ( 79.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.045 ( 0.059)	Loss 5.4346e-01 (7.1369e-01)	Acc@1  83.00 ( 76.82)	Acc@5  99.00 ( 98.55)
Test: [ 20/100]	Time  0.045 ( 0.053)	Loss 8.5254e-01 (7.0851e-01)	Acc@1  72.00 ( 76.00)	Acc@5  98.00 ( 98.43)
Test: [ 30/100]	Time  0.049 ( 0.051)	Loss 6.8701e-01 (7.0693e-01)	Acc@1  76.00 ( 75.97)	Acc@5  98.00 ( 98.48)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 7.2461e-01 (7.1338e-01)	Acc@1  74.00 ( 75.78)	Acc@5  99.00 ( 98.37)
Test: [ 50/100]	Time  0.049 ( 0.050)	Loss 5.8203e-01 (7.0450e-01)	Acc@1  80.00 ( 76.22)	Acc@5 100.00 ( 98.47)
Test: [ 60/100]	Time  0.049 ( 0.049)	Loss 6.0400e-01 (7.0373e-01)	Acc@1  79.00 ( 76.08)	Acc@5  99.00 ( 98.52)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 8.8379e-01 (7.0342e-01)	Acc@1  71.00 ( 76.20)	Acc@5  98.00 ( 98.52)
Test: [ 80/100]	Time  0.045 ( 0.049)	Loss 6.5771e-01 (7.0236e-01)	Acc@1  75.00 ( 76.28)	Acc@5  99.00 ( 98.56)
Test: [ 90/100]	Time  0.048 ( 0.048)	Loss 4.6924e-01 (7.0138e-01)	Acc@1  80.00 ( 76.33)	Acc@5 100.00 ( 98.59)
 * Acc@1 76.420 Acc@5 98.610
### epoch[4] execution time: 51.75854539871216
EPOCH 5
# current learning rate: 0.1
# Switched to train mode...
Epoch: [5][  0/391]	Time  0.286 ( 0.286)	Data  0.155 ( 0.155)	Loss 6.2988e-01 (6.2988e-01)	Acc@1  75.78 ( 75.78)	Acc@5  98.44 ( 98.44)
Epoch: [5][ 10/391]	Time  0.117 ( 0.134)	Data  0.001 ( 0.015)	Loss 4.5190e-01 (5.9337e-01)	Acc@1  85.94 ( 79.05)	Acc@5  99.22 ( 99.01)
Epoch: [5][ 20/391]	Time  0.118 ( 0.127)	Data  0.001 ( 0.008)	Loss 6.0791e-01 (6.1101e-01)	Acc@1  80.47 ( 79.17)	Acc@5  98.44 ( 98.88)
Epoch: [5][ 30/391]	Time  0.118 ( 0.124)	Data  0.001 ( 0.006)	Loss 5.4980e-01 (6.0503e-01)	Acc@1  80.47 ( 79.59)	Acc@5 100.00 ( 98.87)
Epoch: [5][ 40/391]	Time  0.116 ( 0.123)	Data  0.001 ( 0.005)	Loss 5.8691e-01 (6.0566e-01)	Acc@1  83.59 ( 79.57)	Acc@5 100.00 ( 98.82)
Epoch: [5][ 50/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.004)	Loss 5.2637e-01 (6.0393e-01)	Acc@1  83.59 ( 79.40)	Acc@5  99.22 ( 98.81)
Epoch: [5][ 60/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 7.1338e-01 (6.0948e-01)	Acc@1  72.66 ( 79.16)	Acc@5 100.00 ( 98.81)
Epoch: [5][ 70/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.003)	Loss 5.2539e-01 (6.0697e-01)	Acc@1  80.47 ( 79.17)	Acc@5  99.22 ( 98.84)
Epoch: [5][ 80/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.003)	Loss 7.4756e-01 (6.1182e-01)	Acc@1  72.66 ( 79.04)	Acc@5  97.66 ( 98.85)
Epoch: [5][ 90/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.003)	Loss 5.5176e-01 (6.0776e-01)	Acc@1  79.69 ( 79.12)	Acc@5  99.22 ( 98.88)
Epoch: [5][100/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.003)	Loss 5.0293e-01 (6.0477e-01)	Acc@1  85.94 ( 79.26)	Acc@5  99.22 ( 98.87)
Epoch: [5][110/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.9668e-01 (6.0314e-01)	Acc@1  75.00 ( 79.24)	Acc@5  99.22 ( 98.86)
Epoch: [5][120/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.7144e-01 (6.0255e-01)	Acc@1  84.38 ( 79.31)	Acc@5  99.22 ( 98.85)
Epoch: [5][130/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.4404e-01 (6.0091e-01)	Acc@1  76.56 ( 79.33)	Acc@5 100.00 ( 98.88)
Epoch: [5][140/391]	Time  0.127 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.7139e-01 (6.0170e-01)	Acc@1  75.00 ( 79.27)	Acc@5  99.22 ( 98.86)
Epoch: [5][150/391]	Time  0.116 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.4941e-01 (6.0262e-01)	Acc@1  78.91 ( 79.23)	Acc@5  98.44 ( 98.84)
Epoch: [5][160/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 7.6514e-01 (6.0101e-01)	Acc@1  73.44 ( 79.26)	Acc@5  98.44 ( 98.85)
Epoch: [5][170/391]	Time  0.125 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.2456e-01 (5.9577e-01)	Acc@1  85.16 ( 79.42)	Acc@5  99.22 ( 98.89)
Epoch: [5][180/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.4834e-01 (5.9378e-01)	Acc@1  80.47 ( 79.49)	Acc@5  99.22 ( 98.90)
Epoch: [5][190/391]	Time  0.122 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.1084e-01 (5.9545e-01)	Acc@1  77.34 ( 79.41)	Acc@5  99.22 ( 98.88)
Epoch: [5][200/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.8486e-01 (5.9649e-01)	Acc@1  83.59 ( 79.39)	Acc@5  99.22 ( 98.88)
Epoch: [5][210/391]	Time  0.122 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.1138e-01 (5.9227e-01)	Acc@1  84.38 ( 79.51)	Acc@5 100.00 ( 98.92)
Epoch: [5][220/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.9097e-01 (5.9251e-01)	Acc@1  80.47 ( 79.49)	Acc@5 100.00 ( 98.93)
Epoch: [5][230/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.6021e-01 (5.9182e-01)	Acc@1  82.03 ( 79.53)	Acc@5 100.00 ( 98.93)
Epoch: [5][240/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.6445e-01 (5.9169e-01)	Acc@1  82.03 ( 79.56)	Acc@5  98.44 ( 98.93)
Epoch: [5][250/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.6899e-01 (5.8984e-01)	Acc@1  82.81 ( 79.64)	Acc@5 100.00 ( 98.94)
Epoch: [5][260/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.1123e-01 (5.8888e-01)	Acc@1  78.91 ( 79.66)	Acc@5  99.22 ( 98.94)
Epoch: [5][270/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.6055e-01 (5.8879e-01)	Acc@1  83.59 ( 79.64)	Acc@5  98.44 ( 98.94)
Epoch: [5][280/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.4160e-01 (5.8791e-01)	Acc@1  77.34 ( 79.68)	Acc@5  99.22 ( 98.95)
Epoch: [5][290/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.9976e-01 (5.8794e-01)	Acc@1  82.81 ( 79.68)	Acc@5 100.00 ( 98.94)
Epoch: [5][300/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.9951e-01 (5.8612e-01)	Acc@1  83.59 ( 79.72)	Acc@5  99.22 ( 98.96)
Epoch: [5][310/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.9727e-01 (5.8840e-01)	Acc@1  75.78 ( 79.63)	Acc@5  99.22 ( 98.95)
Epoch: [5][320/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.8799e-01 (5.8606e-01)	Acc@1  76.56 ( 79.66)	Acc@5 100.00 ( 98.96)
Epoch: [5][330/391]	Time  0.122 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.0195e-01 (5.8494e-01)	Acc@1  79.69 ( 79.64)	Acc@5 100.00 ( 98.98)
Epoch: [5][340/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 7.7197e-01 (5.8440e-01)	Acc@1  75.00 ( 79.66)	Acc@5  96.88 ( 98.98)
Epoch: [5][350/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.2734e-01 (5.8485e-01)	Acc@1  80.47 ( 79.62)	Acc@5 100.00 ( 98.98)
Epoch: [5][360/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.6504e-01 (5.8388e-01)	Acc@1  75.78 ( 79.64)	Acc@5  97.66 ( 98.99)
Epoch: [5][370/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.7485e-01 (5.8227e-01)	Acc@1  82.81 ( 79.74)	Acc@5 100.00 ( 98.99)
Epoch: [5][380/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.0830e-01 (5.8156e-01)	Acc@1  82.03 ( 79.74)	Acc@5  97.66 ( 98.99)
Epoch: [5][390/391]	Time  0.108 ( 0.119)	Data  0.001 ( 0.001)	Loss 5.6006e-01 (5.8050e-01)	Acc@1  82.50 ( 79.78)	Acc@5  98.75 ( 99.00)
## e[5] optimizer.zero_grad (sum) time: 0.6771726608276367
## e[5]       loss.backward (sum) time: 14.218527793884277
## e[5]      optimizer.step (sum) time: 8.073309421539307
## epoch[5] training(only) time: 46.79409956932068
# Switched to evaluate mode...
Test: [  0/100]	Time  0.182 ( 0.182)	Loss 5.2344e-01 (5.2344e-01)	Acc@1  78.00 ( 78.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 6.4648e-01 (6.2784e-01)	Acc@1  76.00 ( 77.64)	Acc@5  96.00 ( 98.55)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 5.9229e-01 (6.1593e-01)	Acc@1  76.00 ( 78.48)	Acc@5 100.00 ( 98.67)
Test: [ 30/100]	Time  0.045 ( 0.051)	Loss 6.1914e-01 (6.2234e-01)	Acc@1  84.00 ( 78.52)	Acc@5  98.00 ( 98.58)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 6.2793e-01 (6.3104e-01)	Acc@1  80.00 ( 78.17)	Acc@5  99.00 ( 98.66)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 5.4736e-01 (6.2203e-01)	Acc@1  80.00 ( 78.41)	Acc@5  99.00 ( 98.73)
Test: [ 60/100]	Time  0.047 ( 0.049)	Loss 5.2246e-01 (6.2019e-01)	Acc@1  79.00 ( 78.70)	Acc@5  99.00 ( 98.80)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 6.9385e-01 (6.2214e-01)	Acc@1  80.00 ( 78.61)	Acc@5  99.00 ( 98.87)
Test: [ 80/100]	Time  0.045 ( 0.048)	Loss 6.4014e-01 (6.2101e-01)	Acc@1  79.00 ( 78.68)	Acc@5  97.00 ( 98.89)
Test: [ 90/100]	Time  0.045 ( 0.048)	Loss 4.3628e-01 (6.2256e-01)	Acc@1  84.00 ( 78.57)	Acc@5 100.00 ( 98.88)
 * Acc@1 78.580 Acc@5 98.930
### epoch[5] execution time: 51.67362380027771
EPOCH 6
# current learning rate: 0.1
# Switched to train mode...
Epoch: [6][  0/391]	Time  0.293 ( 0.293)	Data  0.148 ( 0.148)	Loss 5.3809e-01 (5.3809e-01)	Acc@1  83.59 ( 83.59)	Acc@5  99.22 ( 99.22)
Epoch: [6][ 10/391]	Time  0.117 ( 0.134)	Data  0.001 ( 0.014)	Loss 5.3027e-01 (5.2097e-01)	Acc@1  77.34 ( 80.47)	Acc@5 100.00 ( 99.29)
Epoch: [6][ 20/391]	Time  0.117 ( 0.127)	Data  0.001 ( 0.008)	Loss 5.6641e-01 (5.3769e-01)	Acc@1  77.34 ( 81.21)	Acc@5  98.44 ( 99.18)
Epoch: [6][ 30/391]	Time  0.130 ( 0.125)	Data  0.001 ( 0.006)	Loss 4.4312e-01 (5.1900e-01)	Acc@1  82.03 ( 81.80)	Acc@5 100.00 ( 99.22)
Epoch: [6][ 40/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.005)	Loss 5.2637e-01 (5.1077e-01)	Acc@1  87.50 ( 82.05)	Acc@5  98.44 ( 99.28)
Epoch: [6][ 50/391]	Time  0.117 ( 0.123)	Data  0.001 ( 0.004)	Loss 4.7852e-01 (5.1364e-01)	Acc@1  84.38 ( 82.14)	Acc@5  99.22 ( 99.16)
Epoch: [6][ 60/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.8120e-01 (5.1813e-01)	Acc@1  81.25 ( 81.92)	Acc@5 100.00 ( 99.17)
Epoch: [6][ 70/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.003)	Loss 4.8560e-01 (5.1835e-01)	Acc@1  81.25 ( 81.90)	Acc@5  99.22 ( 99.10)
Epoch: [6][ 80/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.003)	Loss 5.5127e-01 (5.1809e-01)	Acc@1  82.03 ( 82.08)	Acc@5 100.00 ( 99.13)
Epoch: [6][ 90/391]	Time  0.125 ( 0.121)	Data  0.001 ( 0.003)	Loss 4.9854e-01 (5.1860e-01)	Acc@1  81.25 ( 82.00)	Acc@5  99.22 ( 99.13)
Epoch: [6][100/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.7422e-01 (5.1804e-01)	Acc@1  79.69 ( 82.09)	Acc@5  99.22 ( 99.14)
Epoch: [6][110/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.1992e-01 (5.1738e-01)	Acc@1  83.59 ( 82.05)	Acc@5 100.00 ( 99.18)
Epoch: [6][120/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.9033e-01 (5.1761e-01)	Acc@1  80.47 ( 82.00)	Acc@5  99.22 ( 99.22)
Epoch: [6][130/391]	Time  0.116 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.5420e-01 (5.1711e-01)	Acc@1  81.25 ( 81.98)	Acc@5  96.88 ( 99.22)
Epoch: [6][140/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.7046e-01 (5.1985e-01)	Acc@1  85.94 ( 81.80)	Acc@5 100.00 ( 99.22)
Epoch: [6][150/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.8936e-01 (5.2097e-01)	Acc@1  76.56 ( 81.78)	Acc@5  99.22 ( 99.23)
Epoch: [6][160/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.2588e-01 (5.2189e-01)	Acc@1  80.47 ( 81.71)	Acc@5 100.00 ( 99.25)
Epoch: [6][170/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.7964e-01 (5.2011e-01)	Acc@1  87.50 ( 81.77)	Acc@5  99.22 ( 99.26)
Epoch: [6][180/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.0879e-01 (5.1838e-01)	Acc@1  81.25 ( 81.85)	Acc@5 100.00 ( 99.28)
Epoch: [6][190/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.3613e-01 (5.1957e-01)	Acc@1  82.81 ( 81.76)	Acc@5  98.44 ( 99.27)
Epoch: [6][200/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.4297e-01 (5.1829e-01)	Acc@1  77.34 ( 81.77)	Acc@5  99.22 ( 99.25)
Epoch: [6][210/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.0254e-01 (5.1864e-01)	Acc@1  78.12 ( 81.76)	Acc@5  99.22 ( 99.26)
Epoch: [6][220/391]	Time  0.116 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.8350e-01 (5.1893e-01)	Acc@1  82.81 ( 81.81)	Acc@5  98.44 ( 99.24)
Epoch: [6][230/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.0303e-01 (5.1725e-01)	Acc@1  77.34 ( 81.90)	Acc@5  99.22 ( 99.24)
Epoch: [6][240/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.0986e-01 (5.1571e-01)	Acc@1  78.12 ( 81.97)	Acc@5  98.44 ( 99.24)
Epoch: [6][250/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.6509e-01 (5.1414e-01)	Acc@1  84.38 ( 82.01)	Acc@5 100.00 ( 99.23)
Epoch: [6][260/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.7095e-01 (5.1217e-01)	Acc@1  82.81 ( 82.05)	Acc@5 100.00 ( 99.25)
Epoch: [6][270/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.5117e-01 (5.1394e-01)	Acc@1  86.72 ( 82.02)	Acc@5 100.00 ( 99.24)
Epoch: [6][280/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.6309e-01 (5.1461e-01)	Acc@1  75.00 ( 81.95)	Acc@5  99.22 ( 99.25)
Epoch: [6][290/391]	Time  0.116 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.2246e-01 (5.1391e-01)	Acc@1  82.03 ( 82.00)	Acc@5 100.00 ( 99.25)
Epoch: [6][300/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.2402e-01 (5.1388e-01)	Acc@1  79.69 ( 81.98)	Acc@5  97.66 ( 99.25)
Epoch: [6][310/391]	Time  0.122 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.9229e-01 (5.1570e-01)	Acc@1  78.12 ( 81.90)	Acc@5  99.22 ( 99.24)
Epoch: [6][320/391]	Time  0.116 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.2500e-01 (5.1575e-01)	Acc@1  76.56 ( 81.88)	Acc@5  97.66 ( 99.24)
Epoch: [6][330/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.0254e-01 (5.1600e-01)	Acc@1  78.12 ( 81.89)	Acc@5  99.22 ( 99.24)
Epoch: [6][340/391]	Time  0.126 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.6533e-01 (5.1569e-01)	Acc@1  85.16 ( 81.91)	Acc@5 100.00 ( 99.25)
Epoch: [6][350/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.2939e-01 (5.1533e-01)	Acc@1  77.34 ( 81.94)	Acc@5  98.44 ( 99.24)
Epoch: [6][360/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.4482e-01 (5.1561e-01)	Acc@1  83.59 ( 81.94)	Acc@5  99.22 ( 99.24)
Epoch: [6][370/391]	Time  0.125 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.6060e-01 (5.1496e-01)	Acc@1  87.50 ( 81.97)	Acc@5 100.00 ( 99.25)
Epoch: [6][380/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.4409e-01 (5.1349e-01)	Acc@1  82.81 ( 82.01)	Acc@5 100.00 ( 99.26)
Epoch: [6][390/391]	Time  0.105 ( 0.119)	Data  0.001 ( 0.001)	Loss 5.1172e-01 (5.1378e-01)	Acc@1  82.50 ( 82.00)	Acc@5  96.25 ( 99.24)
## e[6] optimizer.zero_grad (sum) time: 0.6782321929931641
## e[6]       loss.backward (sum) time: 14.258431673049927
## e[6]      optimizer.step (sum) time: 8.048351049423218
## epoch[6] training(only) time: 46.81339764595032
# Switched to evaluate mode...
Test: [  0/100]	Time  0.181 ( 0.181)	Loss 3.5791e-01 (3.5791e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.049 ( 0.061)	Loss 5.4980e-01 (5.2686e-01)	Acc@1  82.00 ( 82.55)	Acc@5  99.00 ( 99.55)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 5.9424e-01 (5.4853e-01)	Acc@1  73.00 ( 81.24)	Acc@5 100.00 ( 99.29)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 4.6729e-01 (5.5207e-01)	Acc@1  82.00 ( 81.23)	Acc@5 100.00 ( 99.23)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 5.2637e-01 (5.5030e-01)	Acc@1  82.00 ( 81.10)	Acc@5  99.00 ( 99.17)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 5.1465e-01 (5.4692e-01)	Acc@1  80.00 ( 81.35)	Acc@5  99.00 ( 99.16)
Test: [ 60/100]	Time  0.045 ( 0.050)	Loss 4.1602e-01 (5.4363e-01)	Acc@1  84.00 ( 81.46)	Acc@5 100.00 ( 99.21)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 3.9844e-01 (5.3919e-01)	Acc@1  87.00 ( 81.48)	Acc@5 100.00 ( 99.25)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 6.8311e-01 (5.4333e-01)	Acc@1  80.00 ( 81.40)	Acc@5  97.00 ( 99.22)
Test: [ 90/100]	Time  0.045 ( 0.049)	Loss 5.1758e-01 (5.4719e-01)	Acc@1  79.00 ( 81.14)	Acc@5 100.00 ( 99.23)
 * Acc@1 81.350 Acc@5 99.280
### epoch[6] execution time: 51.811668157577515
EPOCH 7
# current learning rate: 0.1
# Switched to train mode...
Epoch: [7][  0/391]	Time  0.277 ( 0.277)	Data  0.141 ( 0.141)	Loss 4.5776e-01 (4.5776e-01)	Acc@1  85.16 ( 85.16)	Acc@5  99.22 ( 99.22)
Epoch: [7][ 10/391]	Time  0.117 ( 0.134)	Data  0.001 ( 0.014)	Loss 4.4189e-01 (4.1999e-01)	Acc@1  84.38 ( 85.58)	Acc@5  99.22 ( 99.64)
Epoch: [7][ 20/391]	Time  0.120 ( 0.126)	Data  0.001 ( 0.008)	Loss 5.2686e-01 (4.5708e-01)	Acc@1  81.25 ( 84.19)	Acc@5  99.22 ( 99.52)
Epoch: [7][ 30/391]	Time  0.119 ( 0.125)	Data  0.001 ( 0.005)	Loss 4.5801e-01 (4.6732e-01)	Acc@1  84.38 ( 83.59)	Acc@5 100.00 ( 99.47)
Epoch: [7][ 40/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.004)	Loss 3.4448e-01 (4.6770e-01)	Acc@1  88.28 ( 83.48)	Acc@5  99.22 ( 99.50)
Epoch: [7][ 50/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.004)	Loss 4.4531e-01 (4.6129e-01)	Acc@1  83.59 ( 83.76)	Acc@5 100.00 ( 99.51)
Epoch: [7][ 60/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.0698e-01 (4.6042e-01)	Acc@1  85.94 ( 83.89)	Acc@5  98.44 ( 99.46)
Epoch: [7][ 70/391]	Time  0.116 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.0625e-01 (4.6397e-01)	Acc@1  85.16 ( 83.69)	Acc@5  99.22 ( 99.48)
Epoch: [7][ 80/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.003)	Loss 4.0356e-01 (4.6161e-01)	Acc@1  88.28 ( 83.95)	Acc@5  99.22 ( 99.44)
Epoch: [7][ 90/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.003)	Loss 3.5229e-01 (4.5593e-01)	Acc@1  87.50 ( 84.15)	Acc@5  99.22 ( 99.42)
Epoch: [7][100/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.3516e-01 (4.5531e-01)	Acc@1  82.81 ( 84.12)	Acc@5  99.22 ( 99.45)
Epoch: [7][110/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.8096e-01 (4.5472e-01)	Acc@1  85.16 ( 84.13)	Acc@5  99.22 ( 99.42)
Epoch: [7][120/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.2930e-01 (4.6073e-01)	Acc@1  82.03 ( 83.97)	Acc@5  99.22 ( 99.37)
Epoch: [7][130/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.9707e-01 (4.6243e-01)	Acc@1  83.59 ( 83.87)	Acc@5  99.22 ( 99.36)
Epoch: [7][140/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.7681e-01 (4.6163e-01)	Acc@1  79.69 ( 83.91)	Acc@5  99.22 ( 99.37)
Epoch: [7][150/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.5117e-01 (4.6035e-01)	Acc@1  85.94 ( 83.96)	Acc@5  99.22 ( 99.37)
Epoch: [7][160/391]	Time  0.116 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.5107e-01 (4.6125e-01)	Acc@1  87.50 ( 83.93)	Acc@5  99.22 ( 99.38)
Epoch: [7][170/391]	Time  0.123 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.0732e-01 (4.6212e-01)	Acc@1  83.59 ( 83.96)	Acc@5  97.66 ( 99.35)
Epoch: [7][180/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.0400e-01 (4.6267e-01)	Acc@1  81.25 ( 83.96)	Acc@5  96.09 ( 99.33)
Epoch: [7][190/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.7119e-01 (4.6173e-01)	Acc@1  82.03 ( 83.97)	Acc@5  98.44 ( 99.33)
Epoch: [7][200/391]	Time  0.126 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.1084e-01 (4.6259e-01)	Acc@1  82.81 ( 83.96)	Acc@5  99.22 ( 99.32)
Epoch: [7][210/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.2549e-01 (4.6336e-01)	Acc@1  78.12 ( 83.95)	Acc@5  98.44 ( 99.33)
Epoch: [7][220/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.1904e-01 (4.6568e-01)	Acc@1  82.81 ( 83.88)	Acc@5 100.00 ( 99.33)
Epoch: [7][230/391]	Time  0.126 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.0635e-01 (4.6716e-01)	Acc@1  81.25 ( 83.79)	Acc@5 100.00 ( 99.32)
Epoch: [7][240/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.2295e-01 (4.6557e-01)	Acc@1  83.59 ( 83.91)	Acc@5  96.88 ( 99.32)
Epoch: [7][250/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.4004e-01 (4.6852e-01)	Acc@1  81.25 ( 83.80)	Acc@5  99.22 ( 99.31)
Epoch: [7][260/391]	Time  0.116 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.3569e-01 (4.6806e-01)	Acc@1  89.84 ( 83.79)	Acc@5 100.00 ( 99.31)
Epoch: [7][270/391]	Time  0.122 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.7354e-01 (4.6839e-01)	Acc@1  85.94 ( 83.74)	Acc@5  99.22 ( 99.31)
Epoch: [7][280/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.8745e-01 (4.6868e-01)	Acc@1  89.06 ( 83.70)	Acc@5 100.00 ( 99.31)
Epoch: [7][290/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.6543e-01 (4.6762e-01)	Acc@1  82.81 ( 83.76)	Acc@5 100.00 ( 99.32)
Epoch: [7][300/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.8721e-01 (4.6787e-01)	Acc@1  87.50 ( 83.78)	Acc@5  98.44 ( 99.29)
Epoch: [7][310/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.9243e-01 (4.6855e-01)	Acc@1  79.69 ( 83.76)	Acc@5 100.00 ( 99.28)
Epoch: [7][320/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.8340e-01 (4.6852e-01)	Acc@1  85.94 ( 83.77)	Acc@5  97.66 ( 99.28)
Epoch: [7][330/391]	Time  0.122 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.7812e-01 (4.6914e-01)	Acc@1  81.25 ( 83.75)	Acc@5  99.22 ( 99.27)
Epoch: [7][340/391]	Time  0.116 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.9966e-01 (4.6920e-01)	Acc@1  88.28 ( 83.76)	Acc@5  99.22 ( 99.28)
Epoch: [7][350/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.8721e-01 (4.6788e-01)	Acc@1  85.94 ( 83.84)	Acc@5 100.00 ( 99.29)
Epoch: [7][360/391]	Time  0.122 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.4165e-01 (4.6828e-01)	Acc@1  85.94 ( 83.84)	Acc@5 100.00 ( 99.29)
Epoch: [7][370/391]	Time  0.132 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.8403e-01 (4.6688e-01)	Acc@1  86.72 ( 83.90)	Acc@5  99.22 ( 99.30)
Epoch: [7][380/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.4541e-01 (4.6656e-01)	Acc@1  80.47 ( 83.90)	Acc@5  99.22 ( 99.30)
Epoch: [7][390/391]	Time  0.104 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.5767e-01 (4.6670e-01)	Acc@1  86.25 ( 83.89)	Acc@5 100.00 ( 99.30)
## e[7] optimizer.zero_grad (sum) time: 0.6783580780029297
## e[7]       loss.backward (sum) time: 14.26463270187378
## e[7]      optimizer.step (sum) time: 8.046872615814209
## epoch[7] training(only) time: 46.86569881439209
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 5.4150e-01 (5.4150e-01)	Acc@1  78.00 ( 78.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 6.1621e-01 (5.5855e-01)	Acc@1  79.00 ( 80.73)	Acc@5  98.00 ( 99.18)
Test: [ 20/100]	Time  0.047 ( 0.054)	Loss 5.2539e-01 (5.7266e-01)	Acc@1  80.00 ( 80.71)	Acc@5  99.00 ( 99.00)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 5.5713e-01 (5.7264e-01)	Acc@1  79.00 ( 80.58)	Acc@5  96.00 ( 99.00)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 5.6543e-01 (5.7288e-01)	Acc@1  79.00 ( 80.44)	Acc@5  99.00 ( 98.93)
Test: [ 50/100]	Time  0.045 ( 0.050)	Loss 5.0781e-01 (5.6864e-01)	Acc@1  83.00 ( 81.00)	Acc@5  99.00 ( 98.98)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 4.9219e-01 (5.7221e-01)	Acc@1  84.00 ( 81.02)	Acc@5  99.00 ( 98.98)
Test: [ 70/100]	Time  0.045 ( 0.049)	Loss 7.4414e-01 (5.7880e-01)	Acc@1  82.00 ( 80.76)	Acc@5  99.00 ( 99.03)
Test: [ 80/100]	Time  0.045 ( 0.049)	Loss 5.4492e-01 (5.8063e-01)	Acc@1  80.00 ( 80.70)	Acc@5 100.00 ( 99.02)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 4.2114e-01 (5.8008e-01)	Acc@1  85.00 ( 80.75)	Acc@5  99.00 ( 99.02)
 * Acc@1 80.750 Acc@5 99.070
### epoch[7] execution time: 51.81123995780945
EPOCH 8
# current learning rate: 0.1
# Switched to train mode...
Epoch: [8][  0/391]	Time  0.294 ( 0.294)	Data  0.161 ( 0.161)	Loss 4.5557e-01 (4.5557e-01)	Acc@1  81.25 ( 81.25)	Acc@5  99.22 ( 99.22)
Epoch: [8][ 10/391]	Time  0.117 ( 0.135)	Data  0.001 ( 0.016)	Loss 4.2261e-01 (4.1324e-01)	Acc@1  86.72 ( 85.01)	Acc@5  99.22 ( 99.64)
Epoch: [8][ 20/391]	Time  0.120 ( 0.127)	Data  0.001 ( 0.009)	Loss 4.9976e-01 (4.1507e-01)	Acc@1  82.81 ( 85.31)	Acc@5  98.44 ( 99.55)
Epoch: [8][ 30/391]	Time  0.126 ( 0.125)	Data  0.001 ( 0.006)	Loss 4.7021e-01 (4.3351e-01)	Acc@1  85.94 ( 84.75)	Acc@5 100.00 ( 99.40)
Epoch: [8][ 40/391]	Time  0.117 ( 0.123)	Data  0.001 ( 0.005)	Loss 4.6021e-01 (4.3371e-01)	Acc@1  85.94 ( 84.74)	Acc@5 100.00 ( 99.47)
Epoch: [8][ 50/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.004)	Loss 4.6729e-01 (4.2938e-01)	Acc@1  85.94 ( 85.17)	Acc@5 100.00 ( 99.49)
Epoch: [8][ 60/391]	Time  0.127 ( 0.122)	Data  0.001 ( 0.004)	Loss 4.6167e-01 (4.2832e-01)	Acc@1  82.81 ( 84.99)	Acc@5  99.22 ( 99.51)
Epoch: [8][ 70/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.003)	Loss 4.3066e-01 (4.2680e-01)	Acc@1  82.81 ( 85.11)	Acc@5 100.00 ( 99.44)
Epoch: [8][ 80/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.003)	Loss 4.4751e-01 (4.3112e-01)	Acc@1  84.38 ( 84.96)	Acc@5  99.22 ( 99.38)
Epoch: [8][ 90/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.003)	Loss 4.3994e-01 (4.3144e-01)	Acc@1  84.38 ( 85.11)	Acc@5  99.22 ( 99.36)
Epoch: [8][100/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.003)	Loss 4.0698e-01 (4.3003e-01)	Acc@1  87.50 ( 85.16)	Acc@5 100.00 ( 99.38)
Epoch: [8][110/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.0225e-01 (4.2785e-01)	Acc@1  89.06 ( 85.23)	Acc@5 100.00 ( 99.38)
Epoch: [8][120/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.5303e-01 (4.2708e-01)	Acc@1  83.59 ( 85.17)	Acc@5  99.22 ( 99.39)
Epoch: [8][130/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.0205e-01 (4.2872e-01)	Acc@1  82.03 ( 85.08)	Acc@5  98.44 ( 99.41)
Epoch: [8][140/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.4590e-01 (4.3016e-01)	Acc@1  82.81 ( 85.08)	Acc@5 100.00 ( 99.41)
Epoch: [8][150/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.7500e-01 (4.3363e-01)	Acc@1  85.94 ( 84.92)	Acc@5  99.22 ( 99.40)
Epoch: [8][160/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.3154e-01 (4.3384e-01)	Acc@1  89.06 ( 84.90)	Acc@5  99.22 ( 99.40)
Epoch: [8][170/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.1074e-01 (4.3348e-01)	Acc@1  85.16 ( 84.92)	Acc@5  97.66 ( 99.39)
Epoch: [8][180/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.5312e-01 (4.3354e-01)	Acc@1  83.59 ( 84.92)	Acc@5 100.00 ( 99.40)
Epoch: [8][190/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.1514e-01 (4.3479e-01)	Acc@1  81.25 ( 84.84)	Acc@5  98.44 ( 99.39)
Epoch: [8][200/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.4570e-01 (4.3430e-01)	Acc@1  87.50 ( 84.89)	Acc@5 100.00 ( 99.40)
Epoch: [8][210/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.5654e-01 (4.3604e-01)	Acc@1  83.59 ( 84.85)	Acc@5 100.00 ( 99.39)
Epoch: [8][220/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.8545e-01 (4.3684e-01)	Acc@1  81.25 ( 84.85)	Acc@5  99.22 ( 99.38)
Epoch: [8][230/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.2837e-01 (4.3533e-01)	Acc@1  86.72 ( 84.85)	Acc@5 100.00 ( 99.38)
Epoch: [8][240/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.1333e-01 (4.3475e-01)	Acc@1  83.59 ( 84.85)	Acc@5  99.22 ( 99.37)
Epoch: [8][250/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.8745e-01 (4.3666e-01)	Acc@1  88.28 ( 84.82)	Acc@5 100.00 ( 99.37)
Epoch: [8][260/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.6460e-01 (4.3641e-01)	Acc@1  83.59 ( 84.85)	Acc@5 100.00 ( 99.38)
Epoch: [8][270/391]	Time  0.124 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.2456e-01 (4.3671e-01)	Acc@1  87.50 ( 84.84)	Acc@5 100.00 ( 99.36)
Epoch: [8][280/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.6250e-01 (4.3546e-01)	Acc@1  83.59 ( 84.91)	Acc@5  99.22 ( 99.37)
Epoch: [8][290/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.6558e-01 (4.3510e-01)	Acc@1  85.16 ( 84.94)	Acc@5  97.66 ( 99.37)
Epoch: [8][300/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.7461e-01 (4.3633e-01)	Acc@1  83.59 ( 84.92)	Acc@5  97.66 ( 99.36)
Epoch: [8][310/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.6914e-01 (4.3622e-01)	Acc@1  85.94 ( 84.91)	Acc@5  99.22 ( 99.37)
Epoch: [8][320/391]	Time  0.116 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.9087e-01 (4.3503e-01)	Acc@1  85.16 ( 84.95)	Acc@5 100.00 ( 99.38)
Epoch: [8][330/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.6343e-01 (4.3343e-01)	Acc@1  91.41 ( 84.99)	Acc@5 100.00 ( 99.39)
Epoch: [8][340/391]	Time  0.116 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.8564e-01 (4.3207e-01)	Acc@1  91.41 ( 85.02)	Acc@5  99.22 ( 99.39)
Epoch: [8][350/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.3628e-01 (4.3155e-01)	Acc@1  83.59 ( 85.01)	Acc@5  99.22 ( 99.39)
Epoch: [8][360/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.5762e-01 (4.3146e-01)	Acc@1  82.81 ( 85.04)	Acc@5  98.44 ( 99.39)
Epoch: [8][370/391]	Time  0.128 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.5742e-01 (4.3082e-01)	Acc@1  86.72 ( 85.05)	Acc@5 100.00 ( 99.40)
Epoch: [8][380/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.1030e-01 (4.3068e-01)	Acc@1  89.06 ( 85.03)	Acc@5 100.00 ( 99.40)
Epoch: [8][390/391]	Time  0.109 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.3130e-01 (4.3057e-01)	Acc@1  87.50 ( 85.03)	Acc@5 100.00 ( 99.40)
## e[8] optimizer.zero_grad (sum) time: 0.6850075721740723
## e[8]       loss.backward (sum) time: 14.252503395080566
## e[8]      optimizer.step (sum) time: 8.09903621673584
## epoch[8] training(only) time: 46.91857838630676
# Switched to evaluate mode...
Test: [  0/100]	Time  0.180 ( 0.180)	Loss 4.9854e-01 (4.9854e-01)	Acc@1  82.00 ( 82.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.056 ( 0.059)	Loss 5.0000e-01 (4.2665e-01)	Acc@1  84.00 ( 85.36)	Acc@5  99.00 ( 99.36)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 5.9424e-01 (4.5046e-01)	Acc@1  79.00 ( 84.62)	Acc@5  99.00 ( 99.19)
Test: [ 30/100]	Time  0.048 ( 0.052)	Loss 5.6055e-01 (4.6669e-01)	Acc@1  85.00 ( 84.45)	Acc@5  99.00 ( 99.16)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 3.9526e-01 (4.6553e-01)	Acc@1  84.00 ( 84.34)	Acc@5 100.00 ( 99.17)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 3.1665e-01 (4.6561e-01)	Acc@1  90.00 ( 84.55)	Acc@5 100.00 ( 99.20)
Test: [ 60/100]	Time  0.053 ( 0.049)	Loss 4.2505e-01 (4.6335e-01)	Acc@1  80.00 ( 84.48)	Acc@5 100.00 ( 99.28)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 4.1138e-01 (4.6490e-01)	Acc@1  83.00 ( 84.34)	Acc@5 100.00 ( 99.31)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 4.4312e-01 (4.6902e-01)	Acc@1  84.00 ( 84.19)	Acc@5 100.00 ( 99.35)
Test: [ 90/100]	Time  0.050 ( 0.048)	Loss 4.1919e-01 (4.6795e-01)	Acc@1  87.00 ( 84.25)	Acc@5 100.00 ( 99.38)
 * Acc@1 84.310 Acc@5 99.410
### epoch[8] execution time: 51.85083198547363
EPOCH 9
# current learning rate: 0.1
# Switched to train mode...
Epoch: [9][  0/391]	Time  0.285 ( 0.285)	Data  0.138 ( 0.138)	Loss 3.6182e-01 (3.6182e-01)	Acc@1  87.50 ( 87.50)	Acc@5 100.00 (100.00)
Epoch: [9][ 10/391]	Time  0.120 ( 0.134)	Data  0.001 ( 0.013)	Loss 5.2979e-01 (4.0040e-01)	Acc@1  83.59 ( 86.22)	Acc@5  99.22 ( 99.72)
Epoch: [9][ 20/391]	Time  0.117 ( 0.127)	Data  0.001 ( 0.007)	Loss 4.5703e-01 (4.0978e-01)	Acc@1  81.25 ( 85.49)	Acc@5 100.00 ( 99.70)
Epoch: [9][ 30/391]	Time  0.118 ( 0.124)	Data  0.001 ( 0.005)	Loss 4.7852e-01 (4.0485e-01)	Acc@1  84.38 ( 85.99)	Acc@5  99.22 ( 99.75)
Epoch: [9][ 40/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.004)	Loss 2.7271e-01 (4.0756e-01)	Acc@1  89.06 ( 85.82)	Acc@5 100.00 ( 99.66)
Epoch: [9][ 50/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.004)	Loss 4.0503e-01 (4.0141e-01)	Acc@1  86.72 ( 86.14)	Acc@5 100.00 ( 99.60)
Epoch: [9][ 60/391]	Time  0.116 ( 0.122)	Data  0.001 ( 0.003)	Loss 2.4463e-01 (4.0156e-01)	Acc@1  92.19 ( 86.22)	Acc@5 100.00 ( 99.55)
Epoch: [9][ 70/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 5.5420e-01 (4.0660e-01)	Acc@1  81.25 ( 86.10)	Acc@5  98.44 ( 99.48)
Epoch: [9][ 80/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.003)	Loss 3.8477e-01 (4.0185e-01)	Acc@1  84.38 ( 86.34)	Acc@5 100.00 ( 99.49)
Epoch: [9][ 90/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.003)	Loss 4.7583e-01 (4.0383e-01)	Acc@1  84.38 ( 86.30)	Acc@5  98.44 ( 99.50)
Epoch: [9][100/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.7949e-01 (4.0620e-01)	Acc@1  87.50 ( 86.32)	Acc@5  97.66 ( 99.45)
Epoch: [9][110/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.4912e-01 (4.0279e-01)	Acc@1  85.94 ( 86.47)	Acc@5 100.00 ( 99.46)
Epoch: [9][120/391]	Time  0.124 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.6338e-01 (4.0059e-01)	Acc@1  81.25 ( 86.51)	Acc@5  99.22 ( 99.46)
Epoch: [9][130/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.4155e-01 (3.9753e-01)	Acc@1  89.84 ( 86.60)	Acc@5  99.22 ( 99.47)
Epoch: [9][140/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.2627e-01 (3.9742e-01)	Acc@1  84.38 ( 86.61)	Acc@5 100.00 ( 99.47)
Epoch: [9][150/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.3335e-01 (3.9792e-01)	Acc@1  85.94 ( 86.55)	Acc@5  98.44 ( 99.48)
Epoch: [9][160/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.6304e-01 (3.9934e-01)	Acc@1  89.06 ( 86.47)	Acc@5  98.44 ( 99.46)
Epoch: [9][170/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.0161e-01 (3.9998e-01)	Acc@1  85.94 ( 86.41)	Acc@5  97.66 ( 99.47)
Epoch: [9][180/391]	Time  0.125 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.9526e-01 (4.0079e-01)	Acc@1  85.94 ( 86.29)	Acc@5 100.00 ( 99.48)
Epoch: [9][190/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.5435e-01 (4.0297e-01)	Acc@1  82.03 ( 86.18)	Acc@5 100.00 ( 99.49)
Epoch: [9][200/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.3369e-01 (4.0387e-01)	Acc@1  84.38 ( 86.09)	Acc@5 100.00 ( 99.51)
Epoch: [9][210/391]	Time  0.122 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.7910e-01 (4.0505e-01)	Acc@1  82.03 ( 86.14)	Acc@5 100.00 ( 99.49)
Epoch: [9][220/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.7915e-01 (4.0650e-01)	Acc@1  88.28 ( 86.11)	Acc@5 100.00 ( 99.49)
Epoch: [9][230/391]	Time  0.130 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.4009e-01 (4.0529e-01)	Acc@1  86.72 ( 86.15)	Acc@5 100.00 ( 99.50)
Epoch: [9][240/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.9097e-01 (4.0351e-01)	Acc@1  85.16 ( 86.21)	Acc@5  98.44 ( 99.50)
Epoch: [9][250/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.1455e-01 (4.0293e-01)	Acc@1  88.28 ( 86.23)	Acc@5 100.00 ( 99.51)
Epoch: [9][260/391]	Time  0.126 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.3774e-01 (4.0270e-01)	Acc@1  84.38 ( 86.25)	Acc@5 100.00 ( 99.51)
Epoch: [9][270/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.0322e-01 (4.0117e-01)	Acc@1  85.16 ( 86.27)	Acc@5 100.00 ( 99.50)
Epoch: [9][280/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.0664e-01 (4.0138e-01)	Acc@1  88.28 ( 86.27)	Acc@5  99.22 ( 99.51)
Epoch: [9][290/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.4238e-01 (4.0247e-01)	Acc@1  85.94 ( 86.21)	Acc@5 100.00 ( 99.51)
Epoch: [9][300/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.7354e-01 (4.0154e-01)	Acc@1  86.72 ( 86.23)	Acc@5  99.22 ( 99.53)
Epoch: [9][310/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.7583e-01 (4.0177e-01)	Acc@1  84.38 ( 86.21)	Acc@5 100.00 ( 99.52)
Epoch: [9][320/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.3301e-01 (4.0242e-01)	Acc@1  87.50 ( 86.17)	Acc@5 100.00 ( 99.51)
Epoch: [9][330/391]	Time  0.130 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.1323e-01 (4.0239e-01)	Acc@1  88.28 ( 86.17)	Acc@5  99.22 ( 99.51)
Epoch: [9][340/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.6782e-01 (4.0195e-01)	Acc@1  89.84 ( 86.17)	Acc@5  99.22 ( 99.51)
Epoch: [9][350/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.2637e-01 (4.0268e-01)	Acc@1  85.94 ( 86.16)	Acc@5  99.22 ( 99.51)
Epoch: [9][360/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.7500e-01 (4.0214e-01)	Acc@1  88.28 ( 86.16)	Acc@5  99.22 ( 99.51)
Epoch: [9][370/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.2642e-01 (4.0032e-01)	Acc@1  87.50 ( 86.22)	Acc@5  99.22 ( 99.51)
Epoch: [9][380/391]	Time  0.116 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.1494e-01 (4.0074e-01)	Acc@1  89.06 ( 86.18)	Acc@5 100.00 ( 99.52)
Epoch: [9][390/391]	Time  0.106 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.8838e-01 (4.0077e-01)	Acc@1  78.75 ( 86.17)	Acc@5  96.25 ( 99.51)
## e[9] optimizer.zero_grad (sum) time: 0.68094801902771
## e[9]       loss.backward (sum) time: 14.250606298446655
## e[9]      optimizer.step (sum) time: 8.01632285118103
## epoch[9] training(only) time: 46.854594469070435
# Switched to evaluate mode...
Test: [  0/100]	Time  0.180 ( 0.180)	Loss 3.3374e-01 (3.3374e-01)	Acc@1  86.00 ( 86.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.060)	Loss 4.9878e-01 (4.7084e-01)	Acc@1  88.00 ( 84.64)	Acc@5  98.00 ( 99.45)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 5.1855e-01 (4.8553e-01)	Acc@1  83.00 ( 84.24)	Acc@5  98.00 ( 98.90)
Test: [ 30/100]	Time  0.048 ( 0.051)	Loss 4.8389e-01 (4.9412e-01)	Acc@1  84.00 ( 84.00)	Acc@5  98.00 ( 99.03)
Test: [ 40/100]	Time  0.047 ( 0.050)	Loss 4.7559e-01 (4.9330e-01)	Acc@1  84.00 ( 83.80)	Acc@5 100.00 ( 99.02)
Test: [ 50/100]	Time  0.046 ( 0.049)	Loss 3.6572e-01 (4.8751e-01)	Acc@1  88.00 ( 84.12)	Acc@5  99.00 ( 99.12)
Test: [ 60/100]	Time  0.045 ( 0.049)	Loss 4.0674e-01 (4.8268e-01)	Acc@1  85.00 ( 84.25)	Acc@5 100.00 ( 99.20)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 5.4590e-01 (4.7958e-01)	Acc@1  84.00 ( 84.21)	Acc@5  99.00 ( 99.20)
Test: [ 80/100]	Time  0.046 ( 0.048)	Loss 4.2456e-01 (4.8223e-01)	Acc@1  82.00 ( 84.02)	Acc@5 100.00 ( 99.21)
Test: [ 90/100]	Time  0.047 ( 0.048)	Loss 4.3164e-01 (4.8399e-01)	Acc@1  84.00 ( 83.88)	Acc@5 100.00 ( 99.23)
 * Acc@1 83.880 Acc@5 99.240
### epoch[9] execution time: 51.73877739906311
EPOCH 10
# current learning rate: 0.1
# Switched to train mode...
Epoch: [10][  0/391]	Time  0.288 ( 0.288)	Data  0.159 ( 0.159)	Loss 4.1577e-01 (4.1577e-01)	Acc@1  82.03 ( 82.03)	Acc@5 100.00 (100.00)
Epoch: [10][ 10/391]	Time  0.123 ( 0.135)	Data  0.001 ( 0.015)	Loss 5.2197e-01 (3.9491e-01)	Acc@1  80.47 ( 86.15)	Acc@5  98.44 ( 99.50)
Epoch: [10][ 20/391]	Time  0.120 ( 0.127)	Data  0.001 ( 0.009)	Loss 4.2114e-01 (3.8168e-01)	Acc@1  84.38 ( 86.94)	Acc@5 100.00 ( 99.52)
Epoch: [10][ 30/391]	Time  0.117 ( 0.125)	Data  0.001 ( 0.006)	Loss 4.5288e-01 (3.7765e-01)	Acc@1  82.81 ( 86.74)	Acc@5  99.22 ( 99.55)
Epoch: [10][ 40/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.005)	Loss 3.8721e-01 (3.6590e-01)	Acc@1  85.94 ( 86.87)	Acc@5  99.22 ( 99.58)
Epoch: [10][ 50/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.004)	Loss 2.5220e-01 (3.6102e-01)	Acc@1  85.94 ( 87.06)	Acc@5 100.00 ( 99.62)
Epoch: [10][ 60/391]	Time  0.125 ( 0.122)	Data  0.001 ( 0.004)	Loss 4.3481e-01 (3.6213e-01)	Acc@1  85.94 ( 87.17)	Acc@5  99.22 ( 99.63)
Epoch: [10][ 70/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 3.5327e-01 (3.6139e-01)	Acc@1  86.72 ( 87.08)	Acc@5 100.00 ( 99.67)
Epoch: [10][ 80/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 2.0361e-01 (3.5845e-01)	Acc@1  92.19 ( 87.15)	Acc@5 100.00 ( 99.67)
Epoch: [10][ 90/391]	Time  0.126 ( 0.121)	Data  0.001 ( 0.003)	Loss 4.2090e-01 (3.5914e-01)	Acc@1  85.94 ( 87.22)	Acc@5 100.00 ( 99.67)
Epoch: [10][100/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.003)	Loss 4.7876e-01 (3.6172e-01)	Acc@1  82.03 ( 87.13)	Acc@5  98.44 ( 99.65)
Epoch: [10][110/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.9004e-01 (3.6284e-01)	Acc@1  87.50 ( 87.03)	Acc@5 100.00 ( 99.67)
Epoch: [10][120/391]	Time  0.129 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.7109e-01 (3.6359e-01)	Acc@1  87.50 ( 87.01)	Acc@5 100.00 ( 99.67)
Epoch: [10][130/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.6304e-01 (3.6364e-01)	Acc@1  86.72 ( 87.04)	Acc@5 100.00 ( 99.66)
Epoch: [10][140/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.9209e-01 (3.6518e-01)	Acc@1  85.94 ( 86.93)	Acc@5 100.00 ( 99.67)
Epoch: [10][150/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.8853e-01 (3.6473e-01)	Acc@1  85.16 ( 87.00)	Acc@5 100.00 ( 99.68)
Epoch: [10][160/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.6450e-01 (3.6582e-01)	Acc@1  89.06 ( 86.99)	Acc@5  99.22 ( 99.67)
Epoch: [10][170/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.2529e-01 (3.6484e-01)	Acc@1  88.28 ( 87.09)	Acc@5  99.22 ( 99.68)
Epoch: [10][180/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.1104e-01 (3.6326e-01)	Acc@1  87.50 ( 87.18)	Acc@5 100.00 ( 99.68)
Epoch: [10][190/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.4971e-01 (3.6317e-01)	Acc@1  84.38 ( 87.22)	Acc@5  98.44 ( 99.68)
Epoch: [10][200/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.6514e-01 (3.6258e-01)	Acc@1  89.84 ( 87.23)	Acc@5 100.00 ( 99.68)
Epoch: [10][210/391]	Time  0.123 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.1399e-01 (3.6387e-01)	Acc@1  92.19 ( 87.17)	Acc@5 100.00 ( 99.66)
Epoch: [10][220/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.5947e-01 (3.6512e-01)	Acc@1  85.16 ( 87.15)	Acc@5  98.44 ( 99.64)
Epoch: [10][230/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.8257e-01 (3.6820e-01)	Acc@1  86.72 ( 87.03)	Acc@5 100.00 ( 99.64)
Epoch: [10][240/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.6411e-01 (3.6987e-01)	Acc@1  85.94 ( 86.97)	Acc@5 100.00 ( 99.64)
Epoch: [10][250/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.4629e-01 (3.6864e-01)	Acc@1  86.72 ( 86.99)	Acc@5  97.66 ( 99.64)
Epoch: [10][260/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.2861e-01 (3.6933e-01)	Acc@1  87.50 ( 87.01)	Acc@5 100.00 ( 99.63)
Epoch: [10][270/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.0762e-01 (3.6809e-01)	Acc@1  90.62 ( 87.05)	Acc@5 100.00 ( 99.63)
Epoch: [10][280/391]	Time  0.126 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.7783e-01 (3.6766e-01)	Acc@1  89.84 ( 87.09)	Acc@5 100.00 ( 99.62)
Epoch: [10][290/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.6025e-01 (3.6775e-01)	Acc@1  90.62 ( 87.11)	Acc@5  99.22 ( 99.62)
Epoch: [10][300/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.9492e-01 (3.6802e-01)	Acc@1  90.62 ( 87.11)	Acc@5  98.44 ( 99.61)
Epoch: [10][310/391]	Time  0.124 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.7393e-01 (3.6736e-01)	Acc@1  89.84 ( 87.11)	Acc@5 100.00 ( 99.62)
Epoch: [10][320/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.9160e-01 (3.6734e-01)	Acc@1  86.72 ( 87.10)	Acc@5 100.00 ( 99.62)
Epoch: [10][330/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.2163e-01 (3.6694e-01)	Acc@1  85.94 ( 87.11)	Acc@5  99.22 ( 99.61)
Epoch: [10][340/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.8599e-01 (3.6719e-01)	Acc@1  85.16 ( 87.11)	Acc@5  99.22 ( 99.60)
Epoch: [10][350/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.1616e-01 (3.6733e-01)	Acc@1  86.72 ( 87.13)	Acc@5 100.00 ( 99.60)
Epoch: [10][360/391]	Time  0.130 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.7939e-01 (3.6753e-01)	Acc@1  89.06 ( 87.14)	Acc@5  98.44 ( 99.60)
Epoch: [10][370/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.8662e-01 (3.6659e-01)	Acc@1  90.62 ( 87.19)	Acc@5 100.00 ( 99.61)
Epoch: [10][380/391]	Time  0.125 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.5938e-01 (3.6565e-01)	Acc@1  85.94 ( 87.22)	Acc@5 100.00 ( 99.60)
Epoch: [10][390/391]	Time  0.106 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.3691e-01 (3.6505e-01)	Acc@1  87.50 ( 87.26)	Acc@5  98.75 ( 99.61)
## e[10] optimizer.zero_grad (sum) time: 0.676044225692749
## e[10]       loss.backward (sum) time: 14.235692262649536
## e[10]      optimizer.step (sum) time: 8.055354833602905
## epoch[10] training(only) time: 46.89798069000244
# Switched to evaluate mode...
Test: [  0/100]	Time  0.179 ( 0.179)	Loss 4.8535e-01 (4.8535e-01)	Acc@1  83.00 ( 83.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.049 ( 0.059)	Loss 5.2441e-01 (5.0190e-01)	Acc@1  86.00 ( 83.45)	Acc@5  99.00 ( 99.55)
Test: [ 20/100]	Time  0.049 ( 0.054)	Loss 6.3525e-01 (5.1219e-01)	Acc@1  84.00 ( 83.48)	Acc@5 100.00 ( 99.33)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 5.8740e-01 (5.2014e-01)	Acc@1  82.00 ( 83.35)	Acc@5 100.00 ( 99.26)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 5.7129e-01 (5.2293e-01)	Acc@1  83.00 ( 83.41)	Acc@5 100.00 ( 99.24)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 4.6240e-01 (5.2160e-01)	Acc@1  83.00 ( 83.59)	Acc@5 100.00 ( 99.29)
Test: [ 60/100]	Time  0.056 ( 0.049)	Loss 4.9683e-01 (5.2657e-01)	Acc@1  87.00 ( 83.46)	Acc@5 100.00 ( 99.31)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 5.2441e-01 (5.2908e-01)	Acc@1  88.00 ( 83.31)	Acc@5 100.00 ( 99.34)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 3.6572e-01 (5.2390e-01)	Acc@1  85.00 ( 83.33)	Acc@5 100.00 ( 99.38)
Test: [ 90/100]	Time  0.049 ( 0.049)	Loss 2.2595e-01 (5.2592e-01)	Acc@1  91.00 ( 83.20)	Acc@5 100.00 ( 99.38)
 * Acc@1 83.360 Acc@5 99.400
### epoch[10] execution time: 51.83974552154541
EPOCH 11
# current learning rate: 0.1
# Switched to train mode...
Epoch: [11][  0/391]	Time  0.276 ( 0.276)	Data  0.147 ( 0.147)	Loss 3.5596e-01 (3.5596e-01)	Acc@1  86.72 ( 86.72)	Acc@5 100.00 (100.00)
Epoch: [11][ 10/391]	Time  0.116 ( 0.134)	Data  0.001 ( 0.014)	Loss 3.4253e-01 (3.3751e-01)	Acc@1  89.84 ( 87.78)	Acc@5 100.00 ( 99.79)
Epoch: [11][ 20/391]	Time  0.118 ( 0.126)	Data  0.001 ( 0.008)	Loss 3.5815e-01 (3.3637e-01)	Acc@1  85.94 ( 87.80)	Acc@5  99.22 ( 99.70)
Epoch: [11][ 30/391]	Time  0.117 ( 0.124)	Data  0.001 ( 0.006)	Loss 2.6074e-01 (3.2904e-01)	Acc@1  89.84 ( 88.28)	Acc@5 100.00 ( 99.70)
Epoch: [11][ 40/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.005)	Loss 3.6719e-01 (3.2900e-01)	Acc@1  86.72 ( 88.43)	Acc@5  99.22 ( 99.68)
Epoch: [11][ 50/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.004)	Loss 3.9087e-01 (3.2672e-01)	Acc@1  88.28 ( 88.53)	Acc@5 100.00 ( 99.68)
Epoch: [11][ 60/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.2456e-01 (3.2966e-01)	Acc@1  86.72 ( 88.23)	Acc@5 100.00 ( 99.73)
Epoch: [11][ 70/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 3.4277e-01 (3.3195e-01)	Acc@1  90.62 ( 88.27)	Acc@5  99.22 ( 99.70)
Epoch: [11][ 80/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.003)	Loss 3.8110e-01 (3.3136e-01)	Acc@1  85.94 ( 88.25)	Acc@5 100.00 ( 99.69)
Epoch: [11][ 90/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.003)	Loss 2.4915e-01 (3.3329e-01)	Acc@1  89.06 ( 88.08)	Acc@5 100.00 ( 99.73)
Epoch: [11][100/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.9268e-01 (3.3797e-01)	Acc@1  82.81 ( 87.96)	Acc@5  99.22 ( 99.72)
Epoch: [11][110/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.3813e-01 (3.3753e-01)	Acc@1  87.50 ( 87.94)	Acc@5 100.00 ( 99.73)
Epoch: [11][120/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.7710e-01 (3.3697e-01)	Acc@1  89.06 ( 88.02)	Acc@5 100.00 ( 99.71)
Epoch: [11][130/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.6816e-01 (3.3807e-01)	Acc@1  88.28 ( 88.03)	Acc@5  99.22 ( 99.70)
Epoch: [11][140/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.9614e-01 (3.3788e-01)	Acc@1  90.62 ( 88.05)	Acc@5 100.00 ( 99.70)
Epoch: [11][150/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.4268e-01 (3.3751e-01)	Acc@1  89.84 ( 88.12)	Acc@5 100.00 ( 99.67)
Epoch: [11][160/391]	Time  0.122 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.1680e-01 (3.3928e-01)	Acc@1  92.19 ( 88.04)	Acc@5 100.00 ( 99.66)
Epoch: [11][170/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.2422e-01 (3.4035e-01)	Acc@1  86.72 ( 88.03)	Acc@5 100.00 ( 99.66)
Epoch: [11][180/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.5288e-01 (3.4224e-01)	Acc@1  85.16 ( 88.00)	Acc@5  97.66 ( 99.65)
Epoch: [11][190/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.4951e-01 (3.4126e-01)	Acc@1  89.84 ( 88.03)	Acc@5 100.00 ( 99.64)
Epoch: [11][200/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.7134e-01 (3.4087e-01)	Acc@1  89.06 ( 88.05)	Acc@5  99.22 ( 99.64)
Epoch: [11][210/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.2861e-01 (3.3948e-01)	Acc@1  90.62 ( 88.10)	Acc@5  98.44 ( 99.64)
Epoch: [11][220/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.2227e-01 (3.4128e-01)	Acc@1  89.06 ( 88.08)	Acc@5  99.22 ( 99.64)
Epoch: [11][230/391]	Time  0.115 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.8125e-01 (3.4121e-01)	Acc@1  89.84 ( 88.08)	Acc@5  99.22 ( 99.62)
Epoch: [11][240/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.1128e-01 (3.4210e-01)	Acc@1  89.06 ( 88.04)	Acc@5 100.00 ( 99.62)
Epoch: [11][250/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.1284e-01 (3.4134e-01)	Acc@1  85.16 ( 88.08)	Acc@5 100.00 ( 99.63)
Epoch: [11][260/391]	Time  0.126 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.4766e-01 (3.4201e-01)	Acc@1  88.28 ( 88.03)	Acc@5 100.00 ( 99.63)
Epoch: [11][270/391]	Time  0.116 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.3276e-01 (3.4191e-01)	Acc@1  89.84 ( 88.07)	Acc@5 100.00 ( 99.63)
Epoch: [11][280/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.5815e-01 (3.4223e-01)	Acc@1  89.06 ( 88.08)	Acc@5 100.00 ( 99.63)
Epoch: [11][290/391]	Time  0.125 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.3726e-01 (3.4284e-01)	Acc@1  85.16 ( 88.06)	Acc@5 100.00 ( 99.63)
Epoch: [11][300/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.2986e-01 (3.4267e-01)	Acc@1  91.41 ( 88.05)	Acc@5 100.00 ( 99.63)
Epoch: [11][310/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.7539e-01 (3.4339e-01)	Acc@1  92.19 ( 88.06)	Acc@5 100.00 ( 99.64)
Epoch: [11][320/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.0371e-01 (3.4365e-01)	Acc@1  91.41 ( 88.06)	Acc@5 100.00 ( 99.64)
Epoch: [11][330/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.5474e-01 (3.4482e-01)	Acc@1  89.06 ( 88.04)	Acc@5 100.00 ( 99.64)
Epoch: [11][340/391]	Time  0.131 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.8296e-01 (3.4575e-01)	Acc@1  88.28 ( 88.01)	Acc@5  99.22 ( 99.63)
Epoch: [11][350/391]	Time  0.124 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.4619e-01 (3.4586e-01)	Acc@1  89.06 ( 88.01)	Acc@5 100.00 ( 99.63)
Epoch: [11][360/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.7930e-01 (3.4571e-01)	Acc@1  91.41 ( 88.02)	Acc@5 100.00 ( 99.64)
Epoch: [11][370/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.3384e-01 (3.4627e-01)	Acc@1  85.16 ( 88.01)	Acc@5 100.00 ( 99.63)
Epoch: [11][380/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.5557e-01 (3.4670e-01)	Acc@1  81.25 ( 87.99)	Acc@5 100.00 ( 99.63)
Epoch: [11][390/391]	Time  0.109 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.5708e-01 (3.4648e-01)	Acc@1  90.00 ( 88.00)	Acc@5 100.00 ( 99.64)
## e[11] optimizer.zero_grad (sum) time: 0.675666332244873
## e[11]       loss.backward (sum) time: 14.26742959022522
## e[11]      optimizer.step (sum) time: 8.037057399749756
## epoch[11] training(only) time: 46.93048167228699
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 3.5815e-01 (3.5815e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 4.4922e-01 (3.9967e-01)	Acc@1  83.00 ( 86.27)	Acc@5  99.00 ( 99.55)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 5.2197e-01 (4.4076e-01)	Acc@1  81.00 ( 85.00)	Acc@5 100.00 ( 99.38)
Test: [ 30/100]	Time  0.049 ( 0.052)	Loss 5.3027e-01 (4.5388e-01)	Acc@1  81.00 ( 84.58)	Acc@5  98.00 ( 99.29)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 5.2344e-01 (4.5158e-01)	Acc@1  81.00 ( 84.66)	Acc@5 100.00 ( 99.32)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 2.8613e-01 (4.5460e-01)	Acc@1  89.00 ( 84.80)	Acc@5 100.00 ( 99.33)
Test: [ 60/100]	Time  0.045 ( 0.049)	Loss 4.2114e-01 (4.5690e-01)	Acc@1  84.00 ( 84.92)	Acc@5 100.00 ( 99.38)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 5.0244e-01 (4.5346e-01)	Acc@1  84.00 ( 84.85)	Acc@5  99.00 ( 99.38)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 3.0713e-01 (4.5477e-01)	Acc@1  88.00 ( 84.81)	Acc@5  99.00 ( 99.42)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 3.2715e-01 (4.5384e-01)	Acc@1  88.00 ( 84.88)	Acc@5 100.00 ( 99.45)
 * Acc@1 85.040 Acc@5 99.470
### epoch[11] execution time: 51.83945965766907
EPOCH 12
# current learning rate: 0.1
# Switched to train mode...
Epoch: [12][  0/391]	Time  0.272 ( 0.272)	Data  0.145 ( 0.145)	Loss 3.9258e-01 (3.9258e-01)	Acc@1  87.50 ( 87.50)	Acc@5 100.00 (100.00)
Epoch: [12][ 10/391]	Time  0.120 ( 0.133)	Data  0.001 ( 0.014)	Loss 3.9062e-01 (3.4899e-01)	Acc@1  85.16 ( 88.57)	Acc@5  99.22 ( 99.57)
Epoch: [12][ 20/391]	Time  0.120 ( 0.126)	Data  0.001 ( 0.008)	Loss 3.7744e-01 (3.2320e-01)	Acc@1  89.84 ( 89.25)	Acc@5 100.00 ( 99.52)
Epoch: [12][ 30/391]	Time  0.118 ( 0.124)	Data  0.001 ( 0.006)	Loss 2.6904e-01 (3.2001e-01)	Acc@1  89.84 ( 89.14)	Acc@5  99.22 ( 99.55)
Epoch: [12][ 40/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.004)	Loss 2.6562e-01 (3.0970e-01)	Acc@1  89.84 ( 89.27)	Acc@5  99.22 ( 99.62)
Epoch: [12][ 50/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.004)	Loss 2.1594e-01 (3.0497e-01)	Acc@1  93.75 ( 89.52)	Acc@5 100.00 ( 99.63)
Epoch: [12][ 60/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.003)	Loss 2.7563e-01 (3.0504e-01)	Acc@1  89.84 ( 89.43)	Acc@5  99.22 ( 99.62)
Epoch: [12][ 70/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.003)	Loss 3.2080e-01 (3.1050e-01)	Acc@1  93.75 ( 89.43)	Acc@5  99.22 ( 99.63)
Epoch: [12][ 80/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.003)	Loss 3.6011e-01 (3.1191e-01)	Acc@1  85.94 ( 89.32)	Acc@5 100.00 ( 99.67)
Epoch: [12][ 90/391]	Time  0.116 ( 0.121)	Data  0.001 ( 0.003)	Loss 4.0234e-01 (3.0966e-01)	Acc@1  85.94 ( 89.35)	Acc@5 100.00 ( 99.69)
Epoch: [12][100/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.0615e-01 (3.1036e-01)	Acc@1  84.38 ( 89.31)	Acc@5  99.22 ( 99.65)
Epoch: [12][110/391]	Time  0.116 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.6245e-01 (3.1233e-01)	Acc@1  90.62 ( 89.25)	Acc@5 100.00 ( 99.66)
Epoch: [12][120/391]	Time  0.127 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.8120e-01 (3.1508e-01)	Acc@1  86.72 ( 89.13)	Acc@5  98.44 ( 99.66)
Epoch: [12][130/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.0825e-01 (3.1444e-01)	Acc@1  91.41 ( 89.07)	Acc@5 100.00 ( 99.68)
Epoch: [12][140/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.6245e-01 (3.1428e-01)	Acc@1  92.97 ( 89.08)	Acc@5 100.00 ( 99.70)
Epoch: [12][150/391]	Time  0.128 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.4546e-01 (3.1809e-01)	Acc@1  89.84 ( 88.97)	Acc@5  99.22 ( 99.67)
Epoch: [12][160/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.2153e-01 (3.1931e-01)	Acc@1  89.06 ( 88.94)	Acc@5  99.22 ( 99.66)
Epoch: [12][170/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.8125e-01 (3.1972e-01)	Acc@1  92.19 ( 88.93)	Acc@5  99.22 ( 99.65)
Epoch: [12][180/391]	Time  0.123 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.0542e-01 (3.2236e-01)	Acc@1  92.19 ( 88.83)	Acc@5  99.22 ( 99.65)
Epoch: [12][190/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.5962e-01 (3.2419e-01)	Acc@1  88.28 ( 88.75)	Acc@5  99.22 ( 99.64)
Epoch: [12][200/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.2202e-01 (3.2353e-01)	Acc@1  89.06 ( 88.74)	Acc@5  99.22 ( 99.64)
Epoch: [12][210/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.1128e-01 (3.2531e-01)	Acc@1  89.06 ( 88.72)	Acc@5 100.00 ( 99.63)
Epoch: [12][220/391]	Time  0.124 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.1543e-01 (3.2456e-01)	Acc@1  90.62 ( 88.76)	Acc@5 100.00 ( 99.65)
Epoch: [12][230/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.9272e-01 (3.2368e-01)	Acc@1  89.06 ( 88.82)	Acc@5  99.22 ( 99.65)
Epoch: [12][240/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.8784e-01 (3.2315e-01)	Acc@1  87.50 ( 88.76)	Acc@5 100.00 ( 99.66)
Epoch: [12][250/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.6802e-01 (3.2503e-01)	Acc@1  83.59 ( 88.69)	Acc@5 100.00 ( 99.65)
Epoch: [12][260/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.1270e-01 (3.2653e-01)	Acc@1  84.38 ( 88.57)	Acc@5  99.22 ( 99.65)
Epoch: [12][270/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.1592e-01 (3.2702e-01)	Acc@1  89.06 ( 88.53)	Acc@5  99.22 ( 99.65)
Epoch: [12][280/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.0112e-01 (3.2780e-01)	Acc@1  86.72 ( 88.48)	Acc@5 100.00 ( 99.66)
Epoch: [12][290/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.8281e-01 (3.2798e-01)	Acc@1  89.84 ( 88.48)	Acc@5  98.44 ( 99.66)
Epoch: [12][300/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.8052e-01 (3.2799e-01)	Acc@1  89.84 ( 88.49)	Acc@5 100.00 ( 99.65)
Epoch: [12][310/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.9224e-01 (3.2776e-01)	Acc@1  88.28 ( 88.50)	Acc@5 100.00 ( 99.66)
Epoch: [12][320/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.5303e-01 (3.2726e-01)	Acc@1  89.84 ( 88.51)	Acc@5 100.00 ( 99.66)
Epoch: [12][330/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.8003e-01 (3.2874e-01)	Acc@1  90.62 ( 88.50)	Acc@5 100.00 ( 99.66)
Epoch: [12][340/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.0259e-01 (3.2886e-01)	Acc@1  87.50 ( 88.53)	Acc@5  98.44 ( 99.66)
Epoch: [12][350/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.9600e-01 (3.2843e-01)	Acc@1  85.16 ( 88.54)	Acc@5  99.22 ( 99.65)
Epoch: [12][360/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.1641e-01 (3.2852e-01)	Acc@1  89.06 ( 88.55)	Acc@5 100.00 ( 99.65)
Epoch: [12][370/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.0713e-01 (3.2840e-01)	Acc@1  89.06 ( 88.54)	Acc@5 100.00 ( 99.66)
Epoch: [12][380/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.5293e-01 (3.2768e-01)	Acc@1  92.19 ( 88.58)	Acc@5 100.00 ( 99.65)
Epoch: [12][390/391]	Time  0.101 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.3694e-01 (3.2721e-01)	Acc@1  90.00 ( 88.63)	Acc@5 100.00 ( 99.64)
## e[12] optimizer.zero_grad (sum) time: 0.6831247806549072
## e[12]       loss.backward (sum) time: 14.235909938812256
## e[12]      optimizer.step (sum) time: 8.0399329662323
## epoch[12] training(only) time: 46.937421560287476
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 3.3667e-01 (3.3667e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.045 ( 0.059)	Loss 3.5400e-01 (3.7156e-01)	Acc@1  86.00 ( 87.73)	Acc@5  99.00 ( 99.45)
Test: [ 20/100]	Time  0.046 ( 0.053)	Loss 4.5142e-01 (3.9004e-01)	Acc@1  88.00 ( 87.38)	Acc@5 100.00 ( 99.48)
Test: [ 30/100]	Time  0.046 ( 0.051)	Loss 4.6289e-01 (4.0676e-01)	Acc@1  84.00 ( 87.00)	Acc@5  99.00 ( 99.42)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 3.6011e-01 (4.0575e-01)	Acc@1  90.00 ( 86.85)	Acc@5  99.00 ( 99.39)
Test: [ 50/100]	Time  0.047 ( 0.049)	Loss 3.9087e-01 (4.0955e-01)	Acc@1  86.00 ( 86.84)	Acc@5  98.00 ( 99.43)
Test: [ 60/100]	Time  0.047 ( 0.049)	Loss 3.0835e-01 (4.0769e-01)	Acc@1  89.00 ( 86.84)	Acc@5 100.00 ( 99.49)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 4.6167e-01 (4.1025e-01)	Acc@1  84.00 ( 86.68)	Acc@5 100.00 ( 99.54)
Test: [ 80/100]	Time  0.050 ( 0.049)	Loss 3.8501e-01 (4.0991e-01)	Acc@1  88.00 ( 86.63)	Acc@5 100.00 ( 99.58)
Test: [ 90/100]	Time  0.046 ( 0.048)	Loss 2.6172e-01 (4.1262e-01)	Acc@1  87.00 ( 86.43)	Acc@5 100.00 ( 99.59)
 * Acc@1 86.500 Acc@5 99.610
### epoch[12] execution time: 51.859110832214355
EPOCH 13
# current learning rate: 0.1
# Switched to train mode...
Epoch: [13][  0/391]	Time  0.283 ( 0.283)	Data  0.146 ( 0.146)	Loss 1.8799e-01 (1.8799e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
Epoch: [13][ 10/391]	Time  0.133 ( 0.136)	Data  0.001 ( 0.014)	Loss 2.2351e-01 (2.7931e-01)	Acc@1  92.19 ( 91.19)	Acc@5 100.00 ( 99.86)
Epoch: [13][ 20/391]	Time  0.117 ( 0.128)	Data  0.001 ( 0.008)	Loss 3.0688e-01 (2.9260e-01)	Acc@1  90.62 ( 90.40)	Acc@5  99.22 ( 99.63)
Epoch: [13][ 30/391]	Time  0.117 ( 0.125)	Data  0.001 ( 0.006)	Loss 2.9370e-01 (2.9224e-01)	Acc@1  89.06 ( 89.82)	Acc@5 100.00 ( 99.70)
Epoch: [13][ 40/391]	Time  0.116 ( 0.124)	Data  0.001 ( 0.004)	Loss 2.8247e-01 (2.9024e-01)	Acc@1  89.84 ( 89.77)	Acc@5  99.22 ( 99.71)
Epoch: [13][ 50/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.004)	Loss 3.1055e-01 (2.9049e-01)	Acc@1  89.84 ( 89.80)	Acc@5 100.00 ( 99.74)
Epoch: [13][ 60/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 3.0713e-01 (2.8731e-01)	Acc@1  90.62 ( 90.05)	Acc@5 100.00 ( 99.77)
Epoch: [13][ 70/391]	Time  0.124 ( 0.122)	Data  0.001 ( 0.003)	Loss 3.1860e-01 (2.9064e-01)	Acc@1  89.84 ( 89.99)	Acc@5  99.22 ( 99.77)
Epoch: [13][ 80/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.003)	Loss 3.2935e-01 (2.9128e-01)	Acc@1  85.16 ( 90.01)	Acc@5 100.00 ( 99.78)
Epoch: [13][ 90/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.003)	Loss 2.4573e-01 (2.8881e-01)	Acc@1  91.41 ( 90.14)	Acc@5 100.00 ( 99.79)
Epoch: [13][100/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.6123e-01 (2.8626e-01)	Acc@1  88.28 ( 90.15)	Acc@5 100.00 ( 99.78)
Epoch: [13][110/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.4033e-01 (2.8814e-01)	Acc@1  88.28 ( 90.03)	Acc@5  99.22 ( 99.73)
Epoch: [13][120/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.5977e-01 (2.8951e-01)	Acc@1  91.41 ( 89.99)	Acc@5 100.00 ( 99.73)
Epoch: [13][130/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.9858e-01 (2.9073e-01)	Acc@1  90.62 ( 89.98)	Acc@5 100.00 ( 99.72)
Epoch: [13][140/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.1592e-01 (2.9263e-01)	Acc@1  92.19 ( 89.94)	Acc@5 100.00 ( 99.72)
Epoch: [13][150/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.6440e-01 (2.9278e-01)	Acc@1  91.41 ( 89.93)	Acc@5 100.00 ( 99.72)
Epoch: [13][160/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.1616e-01 (2.9534e-01)	Acc@1  89.06 ( 89.79)	Acc@5 100.00 ( 99.72)
Epoch: [13][170/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.7295e-01 (2.9584e-01)	Acc@1  91.41 ( 89.70)	Acc@5 100.00 ( 99.73)
Epoch: [13][180/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.1631e-01 (2.9626e-01)	Acc@1  90.62 ( 89.67)	Acc@5 100.00 ( 99.73)
Epoch: [13][190/391]	Time  0.122 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.4839e-01 (2.9794e-01)	Acc@1  88.28 ( 89.60)	Acc@5  99.22 ( 99.74)
Epoch: [13][200/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.7686e-01 (2.9902e-01)	Acc@1  90.62 ( 89.58)	Acc@5 100.00 ( 99.75)
Epoch: [13][210/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.7222e-01 (2.9947e-01)	Acc@1  86.72 ( 89.53)	Acc@5  99.22 ( 99.75)
Epoch: [13][220/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.8931e-01 (3.0042e-01)	Acc@1  88.28 ( 89.50)	Acc@5 100.00 ( 99.75)
Epoch: [13][230/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.9966e-01 (3.0054e-01)	Acc@1  86.72 ( 89.49)	Acc@5  99.22 ( 99.76)
Epoch: [13][240/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.3267e-01 (3.0076e-01)	Acc@1  91.41 ( 89.51)	Acc@5 100.00 ( 99.75)
Epoch: [13][250/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.9663e-01 (3.0079e-01)	Acc@1  88.28 ( 89.49)	Acc@5  99.22 ( 99.76)
Epoch: [13][260/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.2422e-01 (3.0072e-01)	Acc@1  85.16 ( 89.49)	Acc@5 100.00 ( 99.76)
Epoch: [13][270/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.5718e-01 (3.0004e-01)	Acc@1  88.28 ( 89.54)	Acc@5 100.00 ( 99.77)
Epoch: [13][280/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.8315e-01 (3.0072e-01)	Acc@1  83.59 ( 89.49)	Acc@5 100.00 ( 99.77)
Epoch: [13][290/391]	Time  0.130 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.5098e-01 (3.0029e-01)	Acc@1  89.06 ( 89.45)	Acc@5 100.00 ( 99.76)
Epoch: [13][300/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.3569e-01 (3.0074e-01)	Acc@1  86.72 ( 89.43)	Acc@5  99.22 ( 99.76)
Epoch: [13][310/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.8638e-01 (3.0128e-01)	Acc@1  92.19 ( 89.38)	Acc@5 100.00 ( 99.77)
Epoch: [13][320/391]	Time  0.125 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.6353e-01 (3.0090e-01)	Acc@1  90.62 ( 89.41)	Acc@5  98.44 ( 99.77)
Epoch: [13][330/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.9688e-01 (3.0125e-01)	Acc@1  89.84 ( 89.40)	Acc@5  99.22 ( 99.77)
Epoch: [13][340/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.4482e-01 (3.0144e-01)	Acc@1  82.03 ( 89.41)	Acc@5 100.00 ( 99.77)
Epoch: [13][350/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.3645e-01 (3.0147e-01)	Acc@1  90.62 ( 89.42)	Acc@5 100.00 ( 99.77)
Epoch: [13][360/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.7539e-01 (3.0233e-01)	Acc@1  89.84 ( 89.39)	Acc@5  99.22 ( 99.77)
Epoch: [13][370/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.8835e-01 (3.0295e-01)	Acc@1  92.19 ( 89.37)	Acc@5 100.00 ( 99.77)
Epoch: [13][380/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.4629e-01 (3.0392e-01)	Acc@1  81.25 ( 89.34)	Acc@5 100.00 ( 99.76)
Epoch: [13][390/391]	Time  0.108 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.8184e-01 (3.0533e-01)	Acc@1  83.75 ( 89.31)	Acc@5 100.00 ( 99.76)
## e[13] optimizer.zero_grad (sum) time: 0.6774108409881592
## e[13]       loss.backward (sum) time: 14.226148128509521
## e[13]      optimizer.step (sum) time: 8.11732029914856
## epoch[13] training(only) time: 46.947551250457764
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 3.0713e-01 (3.0713e-01)	Acc@1  91.00 ( 91.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.047 ( 0.060)	Loss 4.3774e-01 (3.6935e-01)	Acc@1  87.00 ( 88.09)	Acc@5 100.00 ( 99.55)
Test: [ 20/100]	Time  0.057 ( 0.054)	Loss 5.3857e-01 (3.9569e-01)	Acc@1  82.00 ( 87.43)	Acc@5  99.00 ( 99.52)
Test: [ 30/100]	Time  0.048 ( 0.052)	Loss 4.0723e-01 (4.0309e-01)	Acc@1  85.00 ( 87.10)	Acc@5 100.00 ( 99.52)
Test: [ 40/100]	Time  0.060 ( 0.051)	Loss 3.8794e-01 (3.9592e-01)	Acc@1  87.00 ( 87.00)	Acc@5  99.00 ( 99.46)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 4.1260e-01 (3.9680e-01)	Acc@1  88.00 ( 86.90)	Acc@5  99.00 ( 99.45)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 2.3730e-01 (3.9256e-01)	Acc@1  92.00 ( 86.98)	Acc@5 100.00 ( 99.51)
Test: [ 70/100]	Time  0.058 ( 0.050)	Loss 4.7778e-01 (3.9155e-01)	Acc@1  84.00 ( 86.97)	Acc@5 100.00 ( 99.54)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 3.1812e-01 (3.9129e-01)	Acc@1  89.00 ( 86.80)	Acc@5 100.00 ( 99.58)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 3.0981e-01 (3.9564e-01)	Acc@1  92.00 ( 86.69)	Acc@5 100.00 ( 99.60)
 * Acc@1 86.720 Acc@5 99.610
### epoch[13] execution time: 51.91768479347229
EPOCH 14
# current learning rate: 0.1
# Switched to train mode...
Epoch: [14][  0/391]	Time  0.274 ( 0.274)	Data  0.138 ( 0.138)	Loss 2.1228e-01 (2.1228e-01)	Acc@1  90.62 ( 90.62)	Acc@5 100.00 (100.00)
Epoch: [14][ 10/391]	Time  0.119 ( 0.133)	Data  0.001 ( 0.013)	Loss 1.7810e-01 (2.5789e-01)	Acc@1  94.53 ( 90.70)	Acc@5 100.00 ( 99.72)
Epoch: [14][ 20/391]	Time  0.117 ( 0.127)	Data  0.001 ( 0.008)	Loss 2.6782e-01 (2.6461e-01)	Acc@1  89.84 ( 90.51)	Acc@5 100.00 ( 99.78)
Epoch: [14][ 30/391]	Time  0.120 ( 0.124)	Data  0.001 ( 0.005)	Loss 2.1399e-01 (2.7299e-01)	Acc@1  92.97 ( 90.25)	Acc@5 100.00 ( 99.80)
Epoch: [14][ 40/391]	Time  0.117 ( 0.123)	Data  0.001 ( 0.004)	Loss 2.2351e-01 (2.7107e-01)	Acc@1  92.19 ( 90.43)	Acc@5 100.00 ( 99.81)
Epoch: [14][ 50/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.004)	Loss 3.6035e-01 (2.6620e-01)	Acc@1  85.94 ( 90.58)	Acc@5  99.22 ( 99.82)
Epoch: [14][ 60/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 2.6099e-01 (2.6924e-01)	Acc@1  92.97 ( 90.46)	Acc@5 100.00 ( 99.81)
Epoch: [14][ 70/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.003)	Loss 2.7173e-01 (2.7147e-01)	Acc@1  88.28 ( 90.43)	Acc@5  99.22 ( 99.77)
Epoch: [14][ 80/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.003)	Loss 3.0615e-01 (2.7706e-01)	Acc@1  90.62 ( 90.17)	Acc@5 100.00 ( 99.79)
Epoch: [14][ 90/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.7344e-01 (2.8022e-01)	Acc@1  91.41 ( 90.13)	Acc@5 100.00 ( 99.78)
Epoch: [14][100/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.0322e-01 (2.8221e-01)	Acc@1  89.84 ( 90.12)	Acc@5 100.00 ( 99.76)
Epoch: [14][110/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.8916e-01 (2.8188e-01)	Acc@1  88.28 ( 90.13)	Acc@5  99.22 ( 99.77)
Epoch: [14][120/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.5425e-01 (2.8315e-01)	Acc@1  86.72 ( 90.01)	Acc@5 100.00 ( 99.76)
Epoch: [14][130/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.3130e-01 (2.8281e-01)	Acc@1  85.94 ( 90.02)	Acc@5 100.00 ( 99.75)
Epoch: [14][140/391]	Time  0.116 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.2742e-01 (2.8296e-01)	Acc@1  92.97 ( 90.05)	Acc@5 100.00 ( 99.76)
Epoch: [14][150/391]	Time  0.129 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.7134e-01 (2.8297e-01)	Acc@1  87.50 ( 90.02)	Acc@5 100.00 ( 99.76)
Epoch: [14][160/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.2571e-01 (2.8370e-01)	Acc@1  91.41 ( 89.97)	Acc@5 100.00 ( 99.77)
Epoch: [14][170/391]	Time  0.123 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.7012e-01 (2.8348e-01)	Acc@1  89.06 ( 89.96)	Acc@5 100.00 ( 99.77)
Epoch: [14][180/391]	Time  0.127 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.4351e-01 (2.8180e-01)	Acc@1  86.72 ( 89.99)	Acc@5 100.00 ( 99.78)
Epoch: [14][190/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.8877e-01 (2.8237e-01)	Acc@1  79.69 ( 89.94)	Acc@5  98.44 ( 99.78)
Epoch: [14][200/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.1836e-01 (2.8382e-01)	Acc@1  88.28 ( 89.88)	Acc@5 100.00 ( 99.77)
Epoch: [14][210/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.4741e-01 (2.8476e-01)	Acc@1  89.84 ( 89.85)	Acc@5 100.00 ( 99.78)
Epoch: [14][220/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.4531e-01 (2.8613e-01)	Acc@1  81.25 ( 89.78)	Acc@5  99.22 ( 99.78)
Epoch: [14][230/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.1494e-01 (2.8848e-01)	Acc@1  89.84 ( 89.72)	Acc@5  99.22 ( 99.77)
Epoch: [14][240/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.6987e-01 (2.8933e-01)	Acc@1  90.62 ( 89.67)	Acc@5  98.44 ( 99.76)
Epoch: [14][250/391]	Time  0.123 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.6440e-01 (2.9120e-01)	Acc@1  89.84 ( 89.59)	Acc@5  99.22 ( 99.77)
Epoch: [14][260/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.2168e-01 (2.9137e-01)	Acc@1  92.19 ( 89.62)	Acc@5 100.00 ( 99.76)
Epoch: [14][270/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.2642e-01 (2.9218e-01)	Acc@1  86.72 ( 89.59)	Acc@5 100.00 ( 99.77)
Epoch: [14][280/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.2397e-01 (2.9423e-01)	Acc@1  88.28 ( 89.52)	Acc@5 100.00 ( 99.77)
Epoch: [14][290/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.1958e-01 (2.9348e-01)	Acc@1  89.84 ( 89.56)	Acc@5  99.22 ( 99.77)
Epoch: [14][300/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.0444e-01 (2.9398e-01)	Acc@1  90.62 ( 89.56)	Acc@5  99.22 ( 99.76)
Epoch: [14][310/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.1846e-01 (2.9508e-01)	Acc@1  82.81 ( 89.54)	Acc@5 100.00 ( 99.77)
Epoch: [14][320/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.4023e-01 (2.9362e-01)	Acc@1  92.19 ( 89.60)	Acc@5 100.00 ( 99.76)
Epoch: [14][330/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.5759e-01 (2.9174e-01)	Acc@1  94.53 ( 89.65)	Acc@5 100.00 ( 99.77)
Epoch: [14][340/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.4146e-01 (2.9112e-01)	Acc@1  89.06 ( 89.67)	Acc@5 100.00 ( 99.77)
Epoch: [14][350/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.2837e-01 (2.9032e-01)	Acc@1  87.50 ( 89.69)	Acc@5  99.22 ( 99.77)
Epoch: [14][360/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.6370e-01 (2.8929e-01)	Acc@1  93.75 ( 89.73)	Acc@5 100.00 ( 99.77)
Epoch: [14][370/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.0225e-01 (2.8879e-01)	Acc@1  89.84 ( 89.76)	Acc@5 100.00 ( 99.77)
Epoch: [14][380/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.2935e-01 (2.8977e-01)	Acc@1  87.50 ( 89.74)	Acc@5  99.22 ( 99.77)
Epoch: [14][390/391]	Time  0.105 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.3984e-01 (2.8897e-01)	Acc@1  88.75 ( 89.75)	Acc@5 100.00 ( 99.77)
## e[14] optimizer.zero_grad (sum) time: 0.6888177394866943
## e[14]       loss.backward (sum) time: 14.172446727752686
## e[14]      optimizer.step (sum) time: 8.083266496658325
## epoch[14] training(only) time: 46.893067598342896
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 3.0151e-01 (3.0151e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.059)	Loss 4.4067e-01 (3.6845e-01)	Acc@1  87.00 ( 87.09)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 5.6689e-01 (3.9846e-01)	Acc@1  75.00 ( 86.24)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 4.1968e-01 (3.8881e-01)	Acc@1  84.00 ( 86.65)	Acc@5  99.00 ( 99.55)
Test: [ 40/100]	Time  0.048 ( 0.051)	Loss 3.4277e-01 (3.8662e-01)	Acc@1  89.00 ( 86.85)	Acc@5 100.00 ( 99.51)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 2.3584e-01 (3.9466e-01)	Acc@1  90.00 ( 86.90)	Acc@5 100.00 ( 99.55)
Test: [ 60/100]	Time  0.048 ( 0.050)	Loss 3.9746e-01 (3.9874e-01)	Acc@1  84.00 ( 86.75)	Acc@5 100.00 ( 99.59)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 3.7524e-01 (3.9501e-01)	Acc@1  87.00 ( 86.83)	Acc@5 100.00 ( 99.62)
Test: [ 80/100]	Time  0.045 ( 0.049)	Loss 3.5303e-01 (3.9776e-01)	Acc@1  83.00 ( 86.67)	Acc@5 100.00 ( 99.62)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 2.5293e-01 (3.9704e-01)	Acc@1  91.00 ( 86.69)	Acc@5  99.00 ( 99.60)
 * Acc@1 86.940 Acc@5 99.610
### epoch[14] execution time: 51.854296922683716
EPOCH 15
# current learning rate: 0.1
# Switched to train mode...
Epoch: [15][  0/391]	Time  0.283 ( 0.283)	Data  0.153 ( 0.153)	Loss 2.8564e-01 (2.8564e-01)	Acc@1  86.72 ( 86.72)	Acc@5 100.00 (100.00)
Epoch: [15][ 10/391]	Time  0.133 ( 0.134)	Data  0.001 ( 0.015)	Loss 2.7393e-01 (2.2980e-01)	Acc@1  89.06 ( 91.55)	Acc@5  99.22 ( 99.86)
Epoch: [15][ 20/391]	Time  0.119 ( 0.127)	Data  0.001 ( 0.008)	Loss 2.0190e-01 (2.3193e-01)	Acc@1  94.53 ( 92.19)	Acc@5 100.00 ( 99.85)
Epoch: [15][ 30/391]	Time  0.120 ( 0.124)	Data  0.001 ( 0.006)	Loss 3.5645e-01 (2.3074e-01)	Acc@1  85.94 ( 91.94)	Acc@5 100.00 ( 99.90)
Epoch: [15][ 40/391]	Time  0.126 ( 0.123)	Data  0.001 ( 0.005)	Loss 2.6440e-01 (2.3358e-01)	Acc@1  89.84 ( 91.86)	Acc@5 100.00 ( 99.89)
Epoch: [15][ 50/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.004)	Loss 1.9519e-01 (2.4491e-01)	Acc@1  92.19 ( 91.48)	Acc@5 100.00 ( 99.86)
Epoch: [15][ 60/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 2.4329e-01 (2.4587e-01)	Acc@1  91.41 ( 91.42)	Acc@5  99.22 ( 99.85)
Epoch: [15][ 70/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.003)	Loss 2.6587e-01 (2.4844e-01)	Acc@1  90.62 ( 91.22)	Acc@5 100.00 ( 99.82)
Epoch: [15][ 80/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.003)	Loss 2.9541e-01 (2.5097e-01)	Acc@1  90.62 ( 91.12)	Acc@5 100.00 ( 99.83)
Epoch: [15][ 90/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.003)	Loss 4.2456e-01 (2.5609e-01)	Acc@1  89.06 ( 90.97)	Acc@5 100.00 ( 99.83)
Epoch: [15][100/391]	Time  0.116 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.5708e-01 (2.5864e-01)	Acc@1  90.62 ( 90.90)	Acc@5 100.00 ( 99.85)
Epoch: [15][110/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.4780e-01 (2.6050e-01)	Acc@1  89.84 ( 90.77)	Acc@5 100.00 ( 99.85)
Epoch: [15][120/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.8491e-01 (2.6144e-01)	Acc@1  90.62 ( 90.82)	Acc@5 100.00 ( 99.85)
Epoch: [15][130/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.5181e-01 (2.6267e-01)	Acc@1  88.28 ( 90.76)	Acc@5 100.00 ( 99.85)
Epoch: [15][140/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.0203e-01 (2.6127e-01)	Acc@1  94.53 ( 90.82)	Acc@5 100.00 ( 99.85)
Epoch: [15][150/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.3767e-01 (2.6117e-01)	Acc@1  90.62 ( 90.88)	Acc@5 100.00 ( 99.84)
Epoch: [15][160/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.9934e-01 (2.5910e-01)	Acc@1  93.75 ( 90.95)	Acc@5 100.00 ( 99.85)
Epoch: [15][170/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.4500e-01 (2.6038e-01)	Acc@1  90.62 ( 90.92)	Acc@5  99.22 ( 99.83)
Epoch: [15][180/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.9189e-01 (2.6149e-01)	Acc@1  92.97 ( 90.84)	Acc@5 100.00 ( 99.82)
Epoch: [15][190/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.3252e-01 (2.6173e-01)	Acc@1  89.84 ( 90.80)	Acc@5 100.00 ( 99.83)
Epoch: [15][200/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.3413e-01 (2.6262e-01)	Acc@1  88.28 ( 90.77)	Acc@5 100.00 ( 99.83)
Epoch: [15][210/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.4951e-01 (2.6176e-01)	Acc@1  92.19 ( 90.84)	Acc@5 100.00 ( 99.83)
Epoch: [15][220/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.3853e-01 (2.6076e-01)	Acc@1  89.84 ( 90.88)	Acc@5 100.00 ( 99.82)
Epoch: [15][230/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.5488e-01 (2.6128e-01)	Acc@1  89.84 ( 90.87)	Acc@5 100.00 ( 99.83)
Epoch: [15][240/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.2495e-01 (2.6236e-01)	Acc@1  91.41 ( 90.85)	Acc@5 100.00 ( 99.82)
Epoch: [15][250/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.4121e-01 (2.6421e-01)	Acc@1  89.84 ( 90.79)	Acc@5 100.00 ( 99.82)
Epoch: [15][260/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.7222e-01 (2.6496e-01)	Acc@1  92.19 ( 90.75)	Acc@5 100.00 ( 99.81)
Epoch: [15][270/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.1201e-01 (2.6696e-01)	Acc@1  92.19 ( 90.67)	Acc@5  99.22 ( 99.80)
Epoch: [15][280/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.4939e-01 (2.6728e-01)	Acc@1  89.84 ( 90.64)	Acc@5  99.22 ( 99.80)
Epoch: [15][290/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.6572e-01 (2.6992e-01)	Acc@1  87.50 ( 90.59)	Acc@5 100.00 ( 99.80)
Epoch: [15][300/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.0835e-01 (2.7083e-01)	Acc@1  87.50 ( 90.57)	Acc@5 100.00 ( 99.80)
Epoch: [15][310/391]	Time  0.122 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.1519e-01 (2.7022e-01)	Acc@1  89.06 ( 90.57)	Acc@5 100.00 ( 99.80)
Epoch: [15][320/391]	Time  0.124 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.3911e-01 (2.7052e-01)	Acc@1  89.06 ( 90.57)	Acc@5  99.22 ( 99.80)
Epoch: [15][330/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.0337e-01 (2.7036e-01)	Acc@1  91.41 ( 90.60)	Acc@5  98.44 ( 99.79)
Epoch: [15][340/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.7344e-01 (2.7092e-01)	Acc@1  91.41 ( 90.57)	Acc@5  99.22 ( 99.78)
Epoch: [15][350/391]	Time  0.130 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.6670e-01 (2.7244e-01)	Acc@1  85.94 ( 90.52)	Acc@5 100.00 ( 99.78)
Epoch: [15][360/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.5464e-01 (2.7287e-01)	Acc@1  92.97 ( 90.51)	Acc@5 100.00 ( 99.79)
Epoch: [15][370/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.6138e-01 (2.7306e-01)	Acc@1  95.31 ( 90.51)	Acc@5 100.00 ( 99.79)
Epoch: [15][380/391]	Time  0.130 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.6248e-01 (2.7244e-01)	Acc@1  95.31 ( 90.52)	Acc@5 100.00 ( 99.79)
Epoch: [15][390/391]	Time  0.106 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.5156e-01 (2.7305e-01)	Acc@1  85.00 ( 90.50)	Acc@5 100.00 ( 99.78)
## e[15] optimizer.zero_grad (sum) time: 0.6824765205383301
## e[15]       loss.backward (sum) time: 14.178839921951294
## e[15]      optimizer.step (sum) time: 8.04867696762085
## epoch[15] training(only) time: 46.851176023483276
# Switched to evaluate mode...
Test: [  0/100]	Time  0.176 ( 0.176)	Loss 3.0688e-01 (3.0688e-01)	Acc@1  86.00 ( 86.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.049 ( 0.060)	Loss 3.5425e-01 (3.4042e-01)	Acc@1  90.00 ( 87.64)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.057 ( 0.054)	Loss 4.8950e-01 (3.7221e-01)	Acc@1  81.00 ( 87.19)	Acc@5  99.00 ( 99.62)
Test: [ 30/100]	Time  0.045 ( 0.052)	Loss 3.9355e-01 (3.7405e-01)	Acc@1  87.00 ( 87.29)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 4.2407e-01 (3.7318e-01)	Acc@1  87.00 ( 87.63)	Acc@5  99.00 ( 99.61)
Test: [ 50/100]	Time  0.048 ( 0.050)	Loss 3.7573e-01 (3.7557e-01)	Acc@1  83.00 ( 87.35)	Acc@5  99.00 ( 99.59)
Test: [ 60/100]	Time  0.045 ( 0.050)	Loss 2.8271e-01 (3.7504e-01)	Acc@1  94.00 ( 87.56)	Acc@5 100.00 ( 99.64)
Test: [ 70/100]	Time  0.057 ( 0.049)	Loss 5.5566e-01 (3.7627e-01)	Acc@1  83.00 ( 87.42)	Acc@5 100.00 ( 99.63)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 2.9248e-01 (3.7686e-01)	Acc@1  88.00 ( 87.43)	Acc@5 100.00 ( 99.63)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 2.1973e-01 (3.7696e-01)	Acc@1  88.00 ( 87.42)	Acc@5 100.00 ( 99.65)
 * Acc@1 87.520 Acc@5 99.640
### epoch[15] execution time: 51.80560088157654
EPOCH 16
# current learning rate: 0.1
# Switched to train mode...
Epoch: [16][  0/391]	Time  0.284 ( 0.284)	Data  0.151 ( 0.151)	Loss 1.7725e-01 (1.7725e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [16][ 10/391]	Time  0.118 ( 0.135)	Data  0.001 ( 0.015)	Loss 2.5293e-01 (2.6404e-01)	Acc@1  90.62 ( 90.55)	Acc@5 100.00 ( 99.72)
Epoch: [16][ 20/391]	Time  0.116 ( 0.128)	Data  0.001 ( 0.008)	Loss 2.5366e-01 (2.6168e-01)	Acc@1  88.28 ( 90.36)	Acc@5 100.00 ( 99.85)
Epoch: [16][ 30/391]	Time  0.120 ( 0.125)	Data  0.001 ( 0.006)	Loss 2.6294e-01 (2.5683e-01)	Acc@1  91.41 ( 90.62)	Acc@5 100.00 ( 99.85)
Epoch: [16][ 40/391]	Time  0.117 ( 0.124)	Data  0.001 ( 0.005)	Loss 2.1765e-01 (2.5572e-01)	Acc@1  92.19 ( 90.83)	Acc@5 100.00 ( 99.87)
Epoch: [16][ 50/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.004)	Loss 3.8501e-01 (2.6436e-01)	Acc@1  87.50 ( 90.49)	Acc@5 100.00 ( 99.88)
Epoch: [16][ 60/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.003)	Loss 3.3618e-01 (2.6341e-01)	Acc@1  86.72 ( 90.42)	Acc@5  99.22 ( 99.86)
Epoch: [16][ 70/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 3.3350e-01 (2.6377e-01)	Acc@1  88.28 ( 90.43)	Acc@5  99.22 ( 99.86)
Epoch: [16][ 80/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.003)	Loss 2.7905e-01 (2.6997e-01)	Acc@1  90.62 ( 90.27)	Acc@5 100.00 ( 99.85)
Epoch: [16][ 90/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.9653e-01 (2.6377e-01)	Acc@1  94.53 ( 90.56)	Acc@5  99.22 ( 99.85)
Epoch: [16][100/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.0688e-01 (2.6339e-01)	Acc@1  89.06 ( 90.57)	Acc@5  99.22 ( 99.85)
Epoch: [16][110/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.1350e-01 (2.6290e-01)	Acc@1  90.62 ( 90.62)	Acc@5 100.00 ( 99.85)
Epoch: [16][120/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.9565e-01 (2.6647e-01)	Acc@1  89.06 ( 90.52)	Acc@5  99.22 ( 99.85)
Epoch: [16][130/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.6392e-01 (2.6790e-01)	Acc@1  89.06 ( 90.54)	Acc@5  99.22 ( 99.86)
Epoch: [16][140/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.2449e-01 (2.6741e-01)	Acc@1  90.62 ( 90.55)	Acc@5 100.00 ( 99.86)
Epoch: [16][150/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.8760e-01 (2.6512e-01)	Acc@1  92.19 ( 90.69)	Acc@5 100.00 ( 99.85)
Epoch: [16][160/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.7979e-01 (2.6543e-01)	Acc@1  90.62 ( 90.71)	Acc@5 100.00 ( 99.84)
Epoch: [16][170/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.5747e-01 (2.6606e-01)	Acc@1  92.97 ( 90.68)	Acc@5 100.00 ( 99.84)
Epoch: [16][180/391]	Time  0.130 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.5146e-01 (2.6546e-01)	Acc@1  90.62 ( 90.74)	Acc@5 100.00 ( 99.84)
Epoch: [16][190/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.8188e-01 (2.6315e-01)	Acc@1  95.31 ( 90.82)	Acc@5 100.00 ( 99.84)
Epoch: [16][200/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.6147e-01 (2.6060e-01)	Acc@1  89.84 ( 90.85)	Acc@5 100.00 ( 99.84)
Epoch: [16][210/391]	Time  0.129 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.0981e-01 (2.6086e-01)	Acc@1  90.62 ( 90.81)	Acc@5  99.22 ( 99.84)
Epoch: [16][220/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.7881e-01 (2.6153e-01)	Acc@1  88.28 ( 90.80)	Acc@5 100.00 ( 99.84)
Epoch: [16][230/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.9751e-01 (2.6095e-01)	Acc@1  91.41 ( 90.81)	Acc@5 100.00 ( 99.84)
Epoch: [16][240/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.7197e-01 (2.6080e-01)	Acc@1  91.41 ( 90.85)	Acc@5 100.00 ( 99.83)
Epoch: [16][250/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.9038e-01 (2.6196e-01)	Acc@1  86.72 ( 90.81)	Acc@5  99.22 ( 99.82)
Epoch: [16][260/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.3169e-01 (2.6169e-01)	Acc@1  90.62 ( 90.83)	Acc@5 100.00 ( 99.83)
Epoch: [16][270/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.6074e-01 (2.6118e-01)	Acc@1  90.62 ( 90.84)	Acc@5 100.00 ( 99.83)
Epoch: [16][280/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.7979e-01 (2.6069e-01)	Acc@1  91.41 ( 90.86)	Acc@5 100.00 ( 99.84)
Epoch: [16][290/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.8237e-01 (2.5983e-01)	Acc@1  93.75 ( 90.89)	Acc@5 100.00 ( 99.84)
Epoch: [16][300/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.4597e-01 (2.6095e-01)	Acc@1  92.97 ( 90.85)	Acc@5 100.00 ( 99.84)
Epoch: [16][310/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.0203e-01 (2.6039e-01)	Acc@1  92.19 ( 90.87)	Acc@5 100.00 ( 99.84)
Epoch: [16][320/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.8811e-01 (2.5950e-01)	Acc@1  94.53 ( 90.90)	Acc@5 100.00 ( 99.84)
Epoch: [16][330/391]	Time  0.132 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.9910e-01 (2.6001e-01)	Acc@1  94.53 ( 90.90)	Acc@5 100.00 ( 99.84)
Epoch: [16][340/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.0688e-01 (2.6089e-01)	Acc@1  89.06 ( 90.87)	Acc@5 100.00 ( 99.83)
Epoch: [16][350/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.9614e-01 (2.6091e-01)	Acc@1  89.06 ( 90.87)	Acc@5 100.00 ( 99.83)
Epoch: [16][360/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.6587e-01 (2.6194e-01)	Acc@1  92.19 ( 90.85)	Acc@5 100.00 ( 99.83)
Epoch: [16][370/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.5366e-01 (2.6247e-01)	Acc@1  90.62 ( 90.85)	Acc@5 100.00 ( 99.83)
Epoch: [16][380/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.7246e-01 (2.6282e-01)	Acc@1  89.84 ( 90.85)	Acc@5  99.22 ( 99.83)
Epoch: [16][390/391]	Time  0.103 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.5898e-01 (2.6310e-01)	Acc@1  80.00 ( 90.82)	Acc@5 100.00 ( 99.82)
## e[16] optimizer.zero_grad (sum) time: 0.6803281307220459
## e[16]       loss.backward (sum) time: 14.302786827087402
## e[16]      optimizer.step (sum) time: 8.028430938720703
## epoch[16] training(only) time: 47.014442682266235
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 3.0566e-01 (3.0566e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.045 ( 0.059)	Loss 3.0396e-01 (3.7431e-01)	Acc@1  91.00 ( 87.73)	Acc@5  99.00 ( 99.55)
Test: [ 20/100]	Time  0.047 ( 0.054)	Loss 5.6592e-01 (4.0476e-01)	Acc@1  83.00 ( 87.48)	Acc@5 100.00 ( 99.24)
Test: [ 30/100]	Time  0.045 ( 0.053)	Loss 5.2881e-01 (4.2226e-01)	Acc@1  83.00 ( 87.10)	Acc@5 100.00 ( 99.23)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 4.4458e-01 (4.2037e-01)	Acc@1  87.00 ( 87.12)	Acc@5 100.00 ( 99.24)
Test: [ 50/100]	Time  0.049 ( 0.050)	Loss 3.7305e-01 (4.2387e-01)	Acc@1  87.00 ( 86.88)	Acc@5  98.00 ( 99.20)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 3.6670e-01 (4.2787e-01)	Acc@1  87.00 ( 86.51)	Acc@5 100.00 ( 99.30)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 3.7646e-01 (4.2064e-01)	Acc@1  87.00 ( 86.62)	Acc@5 100.00 ( 99.38)
Test: [ 80/100]	Time  0.050 ( 0.049)	Loss 3.1836e-01 (4.1936e-01)	Acc@1  93.00 ( 86.63)	Acc@5  99.00 ( 99.41)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 3.1641e-01 (4.1911e-01)	Acc@1  86.00 ( 86.56)	Acc@5 100.00 ( 99.41)
 * Acc@1 86.700 Acc@5 99.420
### epoch[16] execution time: 51.975852966308594
EPOCH 17
# current learning rate: 0.1
# Switched to train mode...
Epoch: [17][  0/391]	Time  0.287 ( 0.287)	Data  0.150 ( 0.150)	Loss 3.7891e-01 (3.7891e-01)	Acc@1  87.50 ( 87.50)	Acc@5  99.22 ( 99.22)
Epoch: [17][ 10/391]	Time  0.117 ( 0.134)	Data  0.001 ( 0.014)	Loss 2.1057e-01 (2.3830e-01)	Acc@1  92.19 ( 91.90)	Acc@5 100.00 ( 99.86)
Epoch: [17][ 20/391]	Time  0.122 ( 0.127)	Data  0.001 ( 0.008)	Loss 1.4209e-01 (2.3966e-01)	Acc@1  95.31 ( 91.74)	Acc@5 100.00 ( 99.85)
Epoch: [17][ 30/391]	Time  0.118 ( 0.125)	Data  0.001 ( 0.006)	Loss 2.4707e-01 (2.4647e-01)	Acc@1  92.19 ( 91.66)	Acc@5 100.00 ( 99.82)
Epoch: [17][ 40/391]	Time  0.133 ( 0.124)	Data  0.001 ( 0.005)	Loss 3.0469e-01 (2.5174e-01)	Acc@1  89.84 ( 91.33)	Acc@5 100.00 ( 99.83)
Epoch: [17][ 50/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.004)	Loss 2.6733e-01 (2.4713e-01)	Acc@1  86.72 ( 91.44)	Acc@5 100.00 ( 99.85)
Epoch: [17][ 60/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.003)	Loss 2.1155e-01 (2.4708e-01)	Acc@1  92.19 ( 91.47)	Acc@5 100.00 ( 99.85)
Epoch: [17][ 70/391]	Time  0.128 ( 0.122)	Data  0.001 ( 0.003)	Loss 2.7759e-01 (2.5053e-01)	Acc@1  92.19 ( 91.41)	Acc@5  99.22 ( 99.85)
Epoch: [17][ 80/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 2.2546e-01 (2.4853e-01)	Acc@1  92.97 ( 91.43)	Acc@5 100.00 ( 99.86)
Epoch: [17][ 90/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.003)	Loss 3.0518e-01 (2.5514e-01)	Acc@1  87.50 ( 91.14)	Acc@5 100.00 ( 99.85)
Epoch: [17][100/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.003)	Loss 2.3450e-01 (2.6007e-01)	Acc@1  91.41 ( 90.87)	Acc@5 100.00 ( 99.84)
Epoch: [17][110/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.7354e-01 (2.5972e-01)	Acc@1  85.94 ( 90.89)	Acc@5  99.22 ( 99.81)
Epoch: [17][120/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.6123e-01 (2.5851e-01)	Acc@1  90.62 ( 90.97)	Acc@5 100.00 ( 99.83)
Epoch: [17][130/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.6223e-01 (2.5874e-01)	Acc@1  93.75 ( 90.92)	Acc@5 100.00 ( 99.82)
Epoch: [17][140/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.8262e-01 (2.5616e-01)	Acc@1  93.75 ( 91.05)	Acc@5 100.00 ( 99.81)
Epoch: [17][150/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.6333e-01 (2.5292e-01)	Acc@1  94.53 ( 91.20)	Acc@5 100.00 ( 99.82)
Epoch: [17][160/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.9446e-01 (2.5208e-01)	Acc@1  93.75 ( 91.24)	Acc@5 100.00 ( 99.83)
Epoch: [17][170/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.0410e-01 (2.5114e-01)	Acc@1  92.97 ( 91.28)	Acc@5 100.00 ( 99.83)
Epoch: [17][180/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.9446e-01 (2.5107e-01)	Acc@1  93.75 ( 91.27)	Acc@5 100.00 ( 99.82)
Epoch: [17][190/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.1641e-01 (2.5173e-01)	Acc@1  91.41 ( 91.26)	Acc@5 100.00 ( 99.82)
Epoch: [17][200/391]	Time  0.122 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.4951e-01 (2.5158e-01)	Acc@1  88.28 ( 91.26)	Acc@5 100.00 ( 99.83)
Epoch: [17][210/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.8076e-01 (2.5250e-01)	Acc@1  90.62 ( 91.25)	Acc@5  99.22 ( 99.82)
Epoch: [17][220/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.6792e-01 (2.5264e-01)	Acc@1  87.50 ( 91.22)	Acc@5 100.00 ( 99.83)
Epoch: [17][230/391]	Time  0.116 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.4475e-01 (2.5235e-01)	Acc@1  90.62 ( 91.22)	Acc@5 100.00 ( 99.83)
Epoch: [17][240/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.5122e-01 (2.5270e-01)	Acc@1  90.62 ( 91.18)	Acc@5 100.00 ( 99.83)
Epoch: [17][250/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.7905e-01 (2.5360e-01)	Acc@1  90.62 ( 91.13)	Acc@5  99.22 ( 99.83)
Epoch: [17][260/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.6221e-01 (2.5331e-01)	Acc@1  88.28 ( 91.12)	Acc@5 100.00 ( 99.83)
Epoch: [17][270/391]	Time  0.129 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.1079e-01 (2.5411e-01)	Acc@1  89.84 ( 91.08)	Acc@5  98.44 ( 99.82)
Epoch: [17][280/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.1216e-01 (2.5380e-01)	Acc@1  90.62 ( 91.11)	Acc@5 100.00 ( 99.82)
Epoch: [17][290/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.2693e-01 (2.5428e-01)	Acc@1  92.97 ( 91.11)	Acc@5 100.00 ( 99.82)
Epoch: [17][300/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.0947e-01 (2.5365e-01)	Acc@1  94.53 ( 91.15)	Acc@5 100.00 ( 99.82)
Epoch: [17][310/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.7686e-01 (2.5349e-01)	Acc@1  89.84 ( 91.14)	Acc@5 100.00 ( 99.82)
Epoch: [17][320/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.8787e-01 (2.5398e-01)	Acc@1  93.75 ( 91.15)	Acc@5 100.00 ( 99.82)
Epoch: [17][330/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.7725e-01 (2.5276e-01)	Acc@1  93.75 ( 91.19)	Acc@5 100.00 ( 99.82)
Epoch: [17][340/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.2705e-01 (2.5249e-01)	Acc@1  92.19 ( 91.19)	Acc@5  99.22 ( 99.82)
Epoch: [17][350/391]	Time  0.123 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.5000e-01 (2.5296e-01)	Acc@1  90.62 ( 91.17)	Acc@5 100.00 ( 99.82)
Epoch: [17][360/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.9727e-01 (2.5347e-01)	Acc@1  94.53 ( 91.17)	Acc@5 100.00 ( 99.81)
Epoch: [17][370/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.5757e-01 (2.5365e-01)	Acc@1  88.28 ( 91.15)	Acc@5  99.22 ( 99.80)
Epoch: [17][380/391]	Time  0.127 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.3462e-01 (2.5375e-01)	Acc@1  89.84 ( 91.15)	Acc@5 100.00 ( 99.81)
Epoch: [17][390/391]	Time  0.107 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.6538e-01 (2.5362e-01)	Acc@1  90.00 ( 91.16)	Acc@5 100.00 ( 99.80)
## e[17] optimizer.zero_grad (sum) time: 0.6807239055633545
## e[17]       loss.backward (sum) time: 14.204565048217773
## e[17]      optimizer.step (sum) time: 8.076574087142944
## epoch[17] training(only) time: 46.926210165023804
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 3.6328e-01 (3.6328e-01)	Acc@1  88.00 ( 88.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 4.5386e-01 (3.7093e-01)	Acc@1  87.00 ( 88.00)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.049 ( 0.053)	Loss 4.8145e-01 (3.8478e-01)	Acc@1  84.00 ( 87.33)	Acc@5  99.00 ( 99.62)
Test: [ 30/100]	Time  0.046 ( 0.051)	Loss 4.8633e-01 (3.9564e-01)	Acc@1  87.00 ( 87.68)	Acc@5  99.00 ( 99.52)
Test: [ 40/100]	Time  0.050 ( 0.050)	Loss 4.6899e-01 (3.9230e-01)	Acc@1  85.00 ( 87.73)	Acc@5  99.00 ( 99.51)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 2.7612e-01 (3.9475e-01)	Acc@1  90.00 ( 87.75)	Acc@5 100.00 ( 99.53)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 2.0569e-01 (3.9189e-01)	Acc@1  93.00 ( 87.84)	Acc@5 100.00 ( 99.54)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 5.7080e-01 (3.9290e-01)	Acc@1  85.00 ( 87.68)	Acc@5 100.00 ( 99.55)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 3.3008e-01 (3.9793e-01)	Acc@1  92.00 ( 87.56)	Acc@5 100.00 ( 99.53)
Test: [ 90/100]	Time  0.045 ( 0.048)	Loss 2.9932e-01 (3.9845e-01)	Acc@1  93.00 ( 87.62)	Acc@5 100.00 ( 99.52)
 * Acc@1 87.580 Acc@5 99.540
### epoch[17] execution time: 51.85985279083252
EPOCH 18
# current learning rate: 0.1
# Switched to train mode...
Epoch: [18][  0/391]	Time  0.266 ( 0.266)	Data  0.138 ( 0.138)	Loss 2.0190e-01 (2.0190e-01)	Acc@1  92.97 ( 92.97)	Acc@5  99.22 ( 99.22)
Epoch: [18][ 10/391]	Time  0.120 ( 0.133)	Data  0.001 ( 0.013)	Loss 2.7295e-01 (2.1394e-01)	Acc@1  85.94 ( 92.19)	Acc@5 100.00 ( 99.79)
Epoch: [18][ 20/391]	Time  0.120 ( 0.127)	Data  0.001 ( 0.007)	Loss 3.4888e-01 (2.1183e-01)	Acc@1  86.72 ( 92.30)	Acc@5 100.00 ( 99.81)
Epoch: [18][ 30/391]	Time  0.117 ( 0.124)	Data  0.001 ( 0.005)	Loss 2.7344e-01 (2.2129e-01)	Acc@1  89.06 ( 92.24)	Acc@5 100.00 ( 99.85)
Epoch: [18][ 40/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.004)	Loss 2.5049e-01 (2.2610e-01)	Acc@1  89.06 ( 92.05)	Acc@5 100.00 ( 99.85)
Epoch: [18][ 50/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.004)	Loss 2.3157e-01 (2.2565e-01)	Acc@1  89.84 ( 92.06)	Acc@5 100.00 ( 99.85)
Epoch: [18][ 60/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 2.2400e-01 (2.2707e-01)	Acc@1  91.41 ( 92.12)	Acc@5 100.00 ( 99.85)
Epoch: [18][ 70/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 2.0325e-01 (2.2324e-01)	Acc@1  92.19 ( 92.31)	Acc@5  99.22 ( 99.82)
Epoch: [18][ 80/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.003)	Loss 2.3145e-01 (2.2220e-01)	Acc@1  92.97 ( 92.49)	Acc@5 100.00 ( 99.84)
Epoch: [18][ 90/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.8320e-01 (2.2229e-01)	Acc@1  93.75 ( 92.53)	Acc@5 100.00 ( 99.85)
Epoch: [18][100/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.2766e-01 (2.2300e-01)	Acc@1  92.19 ( 92.52)	Acc@5 100.00 ( 99.85)
Epoch: [18][110/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.2644e-01 (2.2216e-01)	Acc@1  92.19 ( 92.55)	Acc@5  98.44 ( 99.85)
Epoch: [18][120/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.9885e-01 (2.2306e-01)	Acc@1  91.41 ( 92.51)	Acc@5 100.00 ( 99.85)
Epoch: [18][130/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.0151e-01 (2.2548e-01)	Acc@1  89.06 ( 92.40)	Acc@5  99.22 ( 99.84)
Epoch: [18][140/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.8823e-01 (2.2666e-01)	Acc@1  94.53 ( 92.33)	Acc@5 100.00 ( 99.84)
Epoch: [18][150/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.4204e-01 (2.2915e-01)	Acc@1  89.06 ( 92.23)	Acc@5 100.00 ( 99.83)
Epoch: [18][160/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.4355e-01 (2.2993e-01)	Acc@1  95.31 ( 92.16)	Acc@5 100.00 ( 99.84)
Epoch: [18][170/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.0537e-01 (2.3371e-01)	Acc@1  86.72 ( 92.08)	Acc@5  99.22 ( 99.84)
Epoch: [18][180/391]	Time  0.127 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.9507e-01 (2.3516e-01)	Acc@1  92.97 ( 92.07)	Acc@5 100.00 ( 99.83)
Epoch: [18][190/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.8047e-01 (2.3689e-01)	Acc@1  83.59 ( 92.02)	Acc@5  98.44 ( 99.82)
Epoch: [18][200/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.4768e-01 (2.3721e-01)	Acc@1  92.19 ( 91.97)	Acc@5 100.00 ( 99.83)
Epoch: [18][210/391]	Time  0.125 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.5906e-01 (2.3653e-01)	Acc@1  92.97 ( 91.96)	Acc@5 100.00 ( 99.84)
Epoch: [18][220/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.8347e-01 (2.3641e-01)	Acc@1  93.75 ( 91.97)	Acc@5 100.00 ( 99.83)
Epoch: [18][230/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.7319e-01 (2.3668e-01)	Acc@1  90.62 ( 91.96)	Acc@5  99.22 ( 99.83)
Epoch: [18][240/391]	Time  0.127 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.6660e-01 (2.3631e-01)	Acc@1  90.62 ( 91.96)	Acc@5 100.00 ( 99.83)
Epoch: [18][250/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.0654e-01 (2.3572e-01)	Acc@1  91.41 ( 91.96)	Acc@5 100.00 ( 99.84)
Epoch: [18][260/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.6831e-01 (2.3645e-01)	Acc@1  89.84 ( 91.94)	Acc@5  99.22 ( 99.84)
Epoch: [18][270/391]	Time  0.125 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.9324e-01 (2.3570e-01)	Acc@1  94.53 ( 91.97)	Acc@5 100.00 ( 99.84)
Epoch: [18][280/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.1350e-01 (2.3530e-01)	Acc@1  90.62 ( 91.97)	Acc@5 100.00 ( 99.84)
Epoch: [18][290/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.4795e-01 (2.3610e-01)	Acc@1  94.53 ( 91.93)	Acc@5 100.00 ( 99.84)
Epoch: [18][300/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.9932e-01 (2.3658e-01)	Acc@1  89.84 ( 91.91)	Acc@5 100.00 ( 99.84)
Epoch: [18][310/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.1543e-01 (2.3732e-01)	Acc@1  86.72 ( 91.89)	Acc@5  99.22 ( 99.84)
Epoch: [18][320/391]	Time  0.116 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.3767e-01 (2.3841e-01)	Acc@1  89.06 ( 91.83)	Acc@5 100.00 ( 99.84)
Epoch: [18][330/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.7041e-01 (2.3860e-01)	Acc@1  92.19 ( 91.81)	Acc@5 100.00 ( 99.83)
Epoch: [18][340/391]	Time  0.128 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.2544e-01 (2.3860e-01)	Acc@1  90.62 ( 91.81)	Acc@5 100.00 ( 99.84)
Epoch: [18][350/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.4644e-01 (2.3869e-01)	Acc@1  85.94 ( 91.82)	Acc@5 100.00 ( 99.84)
Epoch: [18][360/391]	Time  0.125 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.7622e-01 (2.3950e-01)	Acc@1  85.94 ( 91.77)	Acc@5 100.00 ( 99.84)
Epoch: [18][370/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.6343e-01 (2.4085e-01)	Acc@1  90.62 ( 91.71)	Acc@5 100.00 ( 99.84)
Epoch: [18][380/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.0699e-01 (2.4109e-01)	Acc@1  96.88 ( 91.72)	Acc@5 100.00 ( 99.84)
Epoch: [18][390/391]	Time  0.110 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.6367e-01 (2.4036e-01)	Acc@1  92.50 ( 91.75)	Acc@5  98.75 ( 99.84)
## e[18] optimizer.zero_grad (sum) time: 0.6835079193115234
## e[18]       loss.backward (sum) time: 14.237000465393066
## e[18]      optimizer.step (sum) time: 8.072962999343872
## epoch[18] training(only) time: 46.95644569396973
# Switched to evaluate mode...
Test: [  0/100]	Time  0.179 ( 0.179)	Loss 3.0640e-01 (3.0640e-01)	Acc@1  91.00 ( 91.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.046 ( 0.059)	Loss 3.4448e-01 (3.0164e-01)	Acc@1  90.00 ( 88.82)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.047 ( 0.054)	Loss 4.5312e-01 (3.2789e-01)	Acc@1  85.00 ( 88.29)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.056 ( 0.052)	Loss 4.6631e-01 (3.4429e-01)	Acc@1  82.00 ( 88.29)	Acc@5  98.00 ( 99.42)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 3.1982e-01 (3.4157e-01)	Acc@1  87.00 ( 88.66)	Acc@5  99.00 ( 99.37)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 2.3694e-01 (3.4094e-01)	Acc@1  94.00 ( 88.86)	Acc@5  99.00 ( 99.37)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 2.2534e-01 (3.4173e-01)	Acc@1  94.00 ( 88.87)	Acc@5 100.00 ( 99.41)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 3.3740e-01 (3.4491e-01)	Acc@1  90.00 ( 88.76)	Acc@5 100.00 ( 99.45)
Test: [ 80/100]	Time  0.054 ( 0.049)	Loss 2.4365e-01 (3.4178e-01)	Acc@1  91.00 ( 88.85)	Acc@5 100.00 ( 99.47)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 2.5098e-01 (3.4017e-01)	Acc@1  90.00 ( 88.78)	Acc@5 100.00 ( 99.52)
 * Acc@1 88.880 Acc@5 99.530
### epoch[18] execution time: 51.90486812591553
EPOCH 19
# current learning rate: 0.1
# Switched to train mode...
Epoch: [19][  0/391]	Time  0.270 ( 0.270)	Data  0.137 ( 0.137)	Loss 1.7139e-01 (1.7139e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
Epoch: [19][ 10/391]	Time  0.118 ( 0.132)	Data  0.001 ( 0.013)	Loss 2.3413e-01 (1.9828e-01)	Acc@1  93.75 ( 92.97)	Acc@5 100.00 ( 99.86)
Epoch: [19][ 20/391]	Time  0.117 ( 0.126)	Data  0.001 ( 0.007)	Loss 1.7249e-01 (1.9495e-01)	Acc@1  92.97 ( 93.23)	Acc@5 100.00 ( 99.93)
Epoch: [19][ 30/391]	Time  0.118 ( 0.124)	Data  0.001 ( 0.005)	Loss 2.1765e-01 (1.9855e-01)	Acc@1  92.97 ( 93.09)	Acc@5 100.00 ( 99.92)
Epoch: [19][ 40/391]	Time  0.116 ( 0.123)	Data  0.001 ( 0.004)	Loss 2.4829e-01 (1.9954e-01)	Acc@1  92.19 ( 93.08)	Acc@5 100.00 ( 99.92)
Epoch: [19][ 50/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.004)	Loss 1.3049e-01 (2.0205e-01)	Acc@1  96.88 ( 93.01)	Acc@5 100.00 ( 99.91)
Epoch: [19][ 60/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.1676e-01 (2.0259e-01)	Acc@1  96.88 ( 92.98)	Acc@5 100.00 ( 99.90)
Epoch: [19][ 70/391]	Time  0.127 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.8164e-01 (2.0614e-01)	Acc@1  93.75 ( 92.94)	Acc@5 100.00 ( 99.89)
Epoch: [19][ 80/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.003)	Loss 2.6392e-01 (2.0540e-01)	Acc@1  90.62 ( 92.95)	Acc@5  99.22 ( 99.87)
Epoch: [19][ 90/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.1106e-01 (2.0370e-01)	Acc@1  92.97 ( 93.03)	Acc@5 100.00 ( 99.88)
Epoch: [19][100/391]	Time  0.125 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.7004e-01 (2.0583e-01)	Acc@1  92.97 ( 92.88)	Acc@5 100.00 ( 99.88)
Epoch: [19][110/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.5674e-01 (2.0661e-01)	Acc@1  95.31 ( 92.82)	Acc@5 100.00 ( 99.88)
Epoch: [19][120/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.5918e-01 (2.0856e-01)	Acc@1  94.53 ( 92.73)	Acc@5 100.00 ( 99.87)
Epoch: [19][130/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.5928e-01 (2.1331e-01)	Acc@1  90.62 ( 92.53)	Acc@5 100.00 ( 99.87)
Epoch: [19][140/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.3926e-01 (2.1494e-01)	Acc@1  92.19 ( 92.50)	Acc@5 100.00 ( 99.87)
Epoch: [19][150/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.1704e-01 (2.1522e-01)	Acc@1  91.41 ( 92.50)	Acc@5 100.00 ( 99.87)
Epoch: [19][160/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.2788e-01 (2.2002e-01)	Acc@1  88.28 ( 92.35)	Acc@5 100.00 ( 99.86)
Epoch: [19][170/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.6370e-01 (2.2089e-01)	Acc@1  95.31 ( 92.29)	Acc@5 100.00 ( 99.87)
Epoch: [19][180/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.0850e-01 (2.2038e-01)	Acc@1  92.19 ( 92.31)	Acc@5  99.22 ( 99.86)
Epoch: [19][190/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.9028e-01 (2.2120e-01)	Acc@1  89.06 ( 92.27)	Acc@5  99.22 ( 99.86)
Epoch: [19][200/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.9614e-01 (2.2173e-01)	Acc@1  85.94 ( 92.24)	Acc@5 100.00 ( 99.86)
Epoch: [19][210/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.8428e-01 (2.2296e-01)	Acc@1  86.72 ( 92.20)	Acc@5  99.22 ( 99.86)
Epoch: [19][220/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.1179e-01 (2.2317e-01)	Acc@1  95.31 ( 92.16)	Acc@5 100.00 ( 99.87)
Epoch: [19][230/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.9507e-01 (2.2427e-01)	Acc@1  93.75 ( 92.13)	Acc@5 100.00 ( 99.86)
Epoch: [19][240/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.5669e-01 (2.2573e-01)	Acc@1  88.28 ( 92.06)	Acc@5 100.00 ( 99.86)
Epoch: [19][250/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.8774e-01 (2.2707e-01)	Acc@1  92.97 ( 92.03)	Acc@5 100.00 ( 99.86)
Epoch: [19][260/391]	Time  0.129 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.6099e-01 (2.2732e-01)	Acc@1  92.19 ( 92.04)	Acc@5 100.00 ( 99.86)
Epoch: [19][270/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.8271e-01 (2.2749e-01)	Acc@1  87.50 ( 92.05)	Acc@5 100.00 ( 99.86)
Epoch: [19][280/391]	Time  0.125 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.1350e-01 (2.2796e-01)	Acc@1  93.75 ( 92.03)	Acc@5 100.00 ( 99.86)
Epoch: [19][290/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.5415e-01 (2.2716e-01)	Acc@1  89.84 ( 92.06)	Acc@5  99.22 ( 99.86)
Epoch: [19][300/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.1958e-01 (2.2833e-01)	Acc@1  88.28 ( 92.01)	Acc@5 100.00 ( 99.85)
Epoch: [19][310/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.6296e-01 (2.2836e-01)	Acc@1  95.31 ( 91.99)	Acc@5 100.00 ( 99.85)
Epoch: [19][320/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.5342e-01 (2.2860e-01)	Acc@1  90.62 ( 92.00)	Acc@5 100.00 ( 99.85)
Epoch: [19][330/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.7524e-01 (2.2943e-01)	Acc@1  87.50 ( 91.97)	Acc@5  98.44 ( 99.85)
Epoch: [19][340/391]	Time  0.116 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.8857e-01 (2.2868e-01)	Acc@1  88.28 ( 92.01)	Acc@5 100.00 ( 99.86)
Epoch: [19][350/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.4438e-01 (2.2938e-01)	Acc@1  89.06 ( 92.00)	Acc@5 100.00 ( 99.85)
Epoch: [19][360/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.1079e-01 (2.3000e-01)	Acc@1  89.84 ( 91.99)	Acc@5 100.00 ( 99.85)
Epoch: [19][370/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.0691e-01 (2.3060e-01)	Acc@1  92.97 ( 91.95)	Acc@5 100.00 ( 99.85)
Epoch: [19][380/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.2140e-01 (2.3001e-01)	Acc@1  95.31 ( 91.96)	Acc@5 100.00 ( 99.85)
Epoch: [19][390/391]	Time  0.108 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.2839e-01 (2.3028e-01)	Acc@1  93.75 ( 91.97)	Acc@5 100.00 ( 99.85)
## e[19] optimizer.zero_grad (sum) time: 0.6823930740356445
## e[19]       loss.backward (sum) time: 14.205716371536255
## e[19]      optimizer.step (sum) time: 8.103654146194458
## epoch[19] training(only) time: 46.95958232879639
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 2.5122e-01 (2.5122e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 3.5376e-01 (3.2819e-01)	Acc@1  89.00 ( 88.55)	Acc@5  99.00 ( 99.73)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 4.2310e-01 (3.4072e-01)	Acc@1  84.00 ( 88.10)	Acc@5  99.00 ( 99.67)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 3.8623e-01 (3.5283e-01)	Acc@1  88.00 ( 88.19)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 3.3984e-01 (3.5682e-01)	Acc@1  83.00 ( 87.90)	Acc@5 100.00 ( 99.61)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 2.5928e-01 (3.4992e-01)	Acc@1  90.00 ( 88.27)	Acc@5 100.00 ( 99.65)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 2.1997e-01 (3.5078e-01)	Acc@1  94.00 ( 88.34)	Acc@5 100.00 ( 99.62)
Test: [ 70/100]	Time  0.048 ( 0.049)	Loss 5.4492e-01 (3.5310e-01)	Acc@1  85.00 ( 88.27)	Acc@5 100.00 ( 99.63)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 2.7588e-01 (3.5387e-01)	Acc@1  91.00 ( 88.25)	Acc@5 100.00 ( 99.58)
Test: [ 90/100]	Time  0.049 ( 0.049)	Loss 2.8101e-01 (3.5625e-01)	Acc@1  88.00 ( 88.04)	Acc@5 100.00 ( 99.63)
 * Acc@1 88.220 Acc@5 99.660
### epoch[19] execution time: 51.9052996635437
EPOCH 20
# current learning rate: 0.1
# Switched to train mode...
Epoch: [20][  0/391]	Time  0.283 ( 0.283)	Data  0.150 ( 0.150)	Loss 4.0894e-01 (4.0894e-01)	Acc@1  85.16 ( 85.16)	Acc@5 100.00 (100.00)
Epoch: [20][ 10/391]	Time  0.121 ( 0.136)	Data  0.001 ( 0.014)	Loss 1.8652e-01 (2.0576e-01)	Acc@1  92.97 ( 92.97)	Acc@5 100.00 (100.00)
Epoch: [20][ 20/391]	Time  0.117 ( 0.128)	Data  0.001 ( 0.008)	Loss 2.5195e-01 (2.1211e-01)	Acc@1  89.84 ( 92.52)	Acc@5 100.00 (100.00)
Epoch: [20][ 30/391]	Time  0.120 ( 0.125)	Data  0.001 ( 0.006)	Loss 1.7200e-01 (2.0640e-01)	Acc@1  94.53 ( 92.64)	Acc@5  99.22 ( 99.92)
Epoch: [20][ 40/391]	Time  0.117 ( 0.124)	Data  0.001 ( 0.005)	Loss 2.0215e-01 (2.0413e-01)	Acc@1  93.75 ( 92.84)	Acc@5 100.00 ( 99.92)
Epoch: [20][ 50/391]	Time  0.117 ( 0.123)	Data  0.001 ( 0.004)	Loss 2.2388e-01 (2.0168e-01)	Acc@1  90.62 ( 92.82)	Acc@5 100.00 ( 99.92)
Epoch: [20][ 60/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.003)	Loss 2.3816e-01 (2.0589e-01)	Acc@1  90.62 ( 92.64)	Acc@5 100.00 ( 99.90)
Epoch: [20][ 70/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 2.5635e-01 (2.1037e-01)	Acc@1  89.84 ( 92.47)	Acc@5 100.00 ( 99.88)
Epoch: [20][ 80/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.003)	Loss 2.3401e-01 (2.1075e-01)	Acc@1  91.41 ( 92.50)	Acc@5 100.00 ( 99.88)
Epoch: [20][ 90/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.003)	Loss 2.0862e-01 (2.1054e-01)	Acc@1  94.53 ( 92.49)	Acc@5 100.00 ( 99.88)
Epoch: [20][100/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.8542e-01 (2.1339e-01)	Acc@1  92.97 ( 92.33)	Acc@5 100.00 ( 99.87)
Epoch: [20][110/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.5833e-01 (2.1480e-01)	Acc@1  94.53 ( 92.37)	Acc@5 100.00 ( 99.87)
Epoch: [20][120/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.8955e-01 (2.1434e-01)	Acc@1  88.28 ( 92.38)	Acc@5 100.00 ( 99.86)
Epoch: [20][130/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.5293e-01 (2.1462e-01)	Acc@1  92.19 ( 92.40)	Acc@5 100.00 ( 99.87)
Epoch: [20][140/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.9785e-01 (2.1815e-01)	Acc@1  90.62 ( 92.35)	Acc@5 100.00 ( 99.87)
Epoch: [20][150/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.1069e-01 (2.1958e-01)	Acc@1  91.41 ( 92.29)	Acc@5 100.00 ( 99.86)
Epoch: [20][160/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.3110e-01 (2.1935e-01)	Acc@1  95.31 ( 92.26)	Acc@5 100.00 ( 99.87)
Epoch: [20][170/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.2229e-01 (2.1827e-01)	Acc@1  91.41 ( 92.33)	Acc@5  99.22 ( 99.86)
Epoch: [20][180/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.4634e-01 (2.1925e-01)	Acc@1  89.84 ( 92.30)	Acc@5 100.00 ( 99.86)
Epoch: [20][190/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.9175e-01 (2.1966e-01)	Acc@1  89.84 ( 92.32)	Acc@5 100.00 ( 99.86)
Epoch: [20][200/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.5049e-01 (2.2072e-01)	Acc@1  92.19 ( 92.29)	Acc@5  99.22 ( 99.86)
Epoch: [20][210/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.9614e-01 (2.2205e-01)	Acc@1  89.84 ( 92.23)	Acc@5 100.00 ( 99.86)
Epoch: [20][220/391]	Time  0.122 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.4634e-01 (2.2281e-01)	Acc@1  91.41 ( 92.21)	Acc@5 100.00 ( 99.86)
Epoch: [20][230/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.9502e-01 (2.2410e-01)	Acc@1  85.94 ( 92.17)	Acc@5  99.22 ( 99.86)
Epoch: [20][240/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.6833e-01 (2.2401e-01)	Acc@1  94.53 ( 92.17)	Acc@5 100.00 ( 99.86)
Epoch: [20][250/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.1201e-01 (2.2501e-01)	Acc@1  89.84 ( 92.14)	Acc@5 100.00 ( 99.85)
Epoch: [20][260/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.2070e-01 (2.2494e-01)	Acc@1  92.97 ( 92.18)	Acc@5 100.00 ( 99.85)
Epoch: [20][270/391]	Time  0.133 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.3938e-01 (2.2421e-01)	Acc@1  92.97 ( 92.21)	Acc@5 100.00 ( 99.86)
Epoch: [20][280/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.8638e-01 (2.2552e-01)	Acc@1  88.28 ( 92.18)	Acc@5 100.00 ( 99.86)
Epoch: [20][290/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.6833e-01 (2.2636e-01)	Acc@1  96.88 ( 92.14)	Acc@5 100.00 ( 99.86)
Epoch: [20][300/391]	Time  0.127 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.2952e-01 (2.2574e-01)	Acc@1  95.31 ( 92.18)	Acc@5 100.00 ( 99.87)
Epoch: [20][310/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.0908e-01 (2.2581e-01)	Acc@1  87.50 ( 92.16)	Acc@5 100.00 ( 99.86)
Epoch: [20][320/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.2876e-01 (2.2578e-01)	Acc@1  90.62 ( 92.16)	Acc@5 100.00 ( 99.86)
Epoch: [20][330/391]	Time  0.122 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.4670e-01 (2.2620e-01)	Acc@1  91.41 ( 92.16)	Acc@5 100.00 ( 99.86)
Epoch: [20][340/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.9910e-01 (2.2630e-01)	Acc@1  92.19 ( 92.13)	Acc@5 100.00 ( 99.86)
Epoch: [20][350/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.4182e-01 (2.2637e-01)	Acc@1  89.84 ( 92.09)	Acc@5 100.00 ( 99.87)
Epoch: [20][360/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.6782e-01 (2.2789e-01)	Acc@1  92.19 ( 92.05)	Acc@5 100.00 ( 99.86)
Epoch: [20][370/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.0396e-01 (2.2858e-01)	Acc@1  89.06 ( 92.03)	Acc@5 100.00 ( 99.86)
Epoch: [20][380/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.5146e-01 (2.2885e-01)	Acc@1  92.97 ( 92.02)	Acc@5 100.00 ( 99.86)
Epoch: [20][390/391]	Time  0.108 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.5454e-01 (2.2936e-01)	Acc@1  92.50 ( 91.98)	Acc@5 100.00 ( 99.86)
## e[20] optimizer.zero_grad (sum) time: 0.6771504878997803
## e[20]       loss.backward (sum) time: 14.295496463775635
## e[20]      optimizer.step (sum) time: 8.047511100769043
## epoch[20] training(only) time: 47.042768239974976
# Switched to evaluate mode...
Test: [  0/100]	Time  0.179 ( 0.179)	Loss 2.3584e-01 (2.3584e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.058)	Loss 4.3579e-01 (4.0251e-01)	Acc@1  86.00 ( 86.91)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.045 ( 0.053)	Loss 5.1855e-01 (4.1548e-01)	Acc@1  80.00 ( 85.71)	Acc@5  99.00 ( 99.48)
Test: [ 30/100]	Time  0.055 ( 0.051)	Loss 5.0635e-01 (4.2617e-01)	Acc@1  84.00 ( 85.81)	Acc@5  99.00 ( 99.48)
Test: [ 40/100]	Time  0.045 ( 0.050)	Loss 4.6729e-01 (4.2160e-01)	Acc@1  86.00 ( 86.07)	Acc@5  99.00 ( 99.49)
Test: [ 50/100]	Time  0.046 ( 0.049)	Loss 3.0054e-01 (4.1696e-01)	Acc@1  90.00 ( 86.29)	Acc@5 100.00 ( 99.47)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 4.2334e-01 (4.1425e-01)	Acc@1  87.00 ( 86.51)	Acc@5 100.00 ( 99.52)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 4.4727e-01 (4.1533e-01)	Acc@1  85.00 ( 86.51)	Acc@5 100.00 ( 99.54)
Test: [ 80/100]	Time  0.054 ( 0.048)	Loss 4.0381e-01 (4.1414e-01)	Acc@1  86.00 ( 86.42)	Acc@5  99.00 ( 99.57)
Test: [ 90/100]	Time  0.046 ( 0.048)	Loss 3.1299e-01 (4.2193e-01)	Acc@1  90.00 ( 86.20)	Acc@5 100.00 ( 99.56)
 * Acc@1 86.330 Acc@5 99.600
### epoch[20] execution time: 51.9540855884552
EPOCH 21
# current learning rate: 0.1
# Switched to train mode...
Epoch: [21][  0/391]	Time  0.275 ( 0.275)	Data  0.143 ( 0.143)	Loss 1.7371e-01 (1.7371e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
Epoch: [21][ 10/391]	Time  0.118 ( 0.133)	Data  0.001 ( 0.014)	Loss 2.4463e-01 (2.0841e-01)	Acc@1  92.19 ( 93.04)	Acc@5 100.00 ( 99.86)
Epoch: [21][ 20/391]	Time  0.117 ( 0.126)	Data  0.001 ( 0.008)	Loss 1.7627e-01 (2.0289e-01)	Acc@1  92.19 ( 92.75)	Acc@5 100.00 ( 99.89)
Epoch: [21][ 30/391]	Time  0.120 ( 0.124)	Data  0.001 ( 0.006)	Loss 2.2205e-01 (2.0693e-01)	Acc@1  92.19 ( 92.62)	Acc@5  99.22 ( 99.90)
Epoch: [21][ 40/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.004)	Loss 1.7407e-01 (2.0075e-01)	Acc@1  92.97 ( 92.89)	Acc@5 100.00 ( 99.92)
Epoch: [21][ 50/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.004)	Loss 1.7688e-01 (1.9348e-01)	Acc@1  94.53 ( 93.24)	Acc@5 100.00 ( 99.94)
Epoch: [21][ 60/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.5735e-01 (1.9037e-01)	Acc@1  93.75 ( 93.22)	Acc@5 100.00 ( 99.95)
Epoch: [21][ 70/391]	Time  0.116 ( 0.121)	Data  0.001 ( 0.003)	Loss 2.1912e-01 (1.9192e-01)	Acc@1  92.19 ( 93.07)	Acc@5 100.00 ( 99.94)
Epoch: [21][ 80/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.003)	Loss 2.1057e-01 (1.9215e-01)	Acc@1  92.97 ( 93.02)	Acc@5 100.00 ( 99.95)
Epoch: [21][ 90/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.003)	Loss 1.4880e-01 (1.9278e-01)	Acc@1  96.88 ( 93.01)	Acc@5 100.00 ( 99.96)
Epoch: [21][100/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.5430e-01 (1.9302e-01)	Acc@1  94.53 ( 92.97)	Acc@5 100.00 ( 99.95)
Epoch: [21][110/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.6650e-01 (1.9451e-01)	Acc@1  94.53 ( 92.90)	Acc@5 100.00 ( 99.93)
Epoch: [21][120/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.5659e-01 (1.9687e-01)	Acc@1  92.19 ( 92.87)	Acc@5 100.00 ( 99.94)
Epoch: [21][130/391]	Time  0.133 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.0178e-01 (1.9950e-01)	Acc@1  94.53 ( 92.79)	Acc@5  99.22 ( 99.92)
Epoch: [21][140/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.4734e-01 (1.9947e-01)	Acc@1  94.53 ( 92.79)	Acc@5 100.00 ( 99.92)
Epoch: [21][150/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.5806e-01 (2.0237e-01)	Acc@1  92.97 ( 92.69)	Acc@5 100.00 ( 99.92)
Epoch: [21][160/391]	Time  0.128 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.0447e-01 (2.0435e-01)	Acc@1  92.19 ( 92.62)	Acc@5  99.22 ( 99.91)
Epoch: [21][170/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.3911e-01 (2.0874e-01)	Acc@1  85.94 ( 92.45)	Acc@5 100.00 ( 99.91)
Epoch: [21][180/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.0496e-01 (2.0900e-01)	Acc@1  92.97 ( 92.43)	Acc@5 100.00 ( 99.90)
Epoch: [21][190/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.6880e-01 (2.0995e-01)	Acc@1  89.84 ( 92.42)	Acc@5 100.00 ( 99.91)
Epoch: [21][200/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.0715e-01 (2.1164e-01)	Acc@1  92.97 ( 92.40)	Acc@5 100.00 ( 99.90)
Epoch: [21][210/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.9248e-01 (2.1191e-01)	Acc@1  89.84 ( 92.37)	Acc@5 100.00 ( 99.90)
Epoch: [21][220/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.3525e-01 (2.1233e-01)	Acc@1  96.88 ( 92.39)	Acc@5 100.00 ( 99.91)
Epoch: [21][230/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.1958e-01 (2.1317e-01)	Acc@1  86.72 ( 92.37)	Acc@5 100.00 ( 99.91)
Epoch: [21][240/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.2180e-01 (2.1385e-01)	Acc@1  94.53 ( 92.38)	Acc@5 100.00 ( 99.91)
Epoch: [21][250/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.9592e-01 (2.1453e-01)	Acc@1  93.75 ( 92.36)	Acc@5  99.22 ( 99.91)
Epoch: [21][260/391]	Time  0.127 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.4878e-01 (2.1402e-01)	Acc@1  91.41 ( 92.40)	Acc@5 100.00 ( 99.90)
Epoch: [21][270/391]	Time  0.122 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.7041e-01 (2.1392e-01)	Acc@1  93.75 ( 92.39)	Acc@5 100.00 ( 99.90)
Epoch: [21][280/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.8296e-01 (2.1392e-01)	Acc@1  90.62 ( 92.40)	Acc@5 100.00 ( 99.91)
Epoch: [21][290/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.2388e-01 (2.1402e-01)	Acc@1  92.97 ( 92.39)	Acc@5  99.22 ( 99.91)
Epoch: [21][300/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.9263e-01 (2.1450e-01)	Acc@1  89.84 ( 92.37)	Acc@5 100.00 ( 99.91)
Epoch: [21][310/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.9971e-01 (2.1582e-01)	Acc@1  89.84 ( 92.32)	Acc@5 100.00 ( 99.91)
Epoch: [21][320/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.1560e-01 (2.1627e-01)	Acc@1  96.88 ( 92.32)	Acc@5 100.00 ( 99.91)
Epoch: [21][330/391]	Time  0.125 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.4844e-01 (2.1663e-01)	Acc@1  94.53 ( 92.31)	Acc@5 100.00 ( 99.91)
Epoch: [21][340/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.0760e-01 (2.1743e-01)	Acc@1  96.88 ( 92.29)	Acc@5 100.00 ( 99.90)
Epoch: [21][350/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.1423e-01 (2.1754e-01)	Acc@1  93.75 ( 92.29)	Acc@5 100.00 ( 99.90)
Epoch: [21][360/391]	Time  0.122 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.8652e-01 (2.1745e-01)	Acc@1  92.19 ( 92.28)	Acc@5 100.00 ( 99.90)
Epoch: [21][370/391]	Time  0.124 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.8762e-01 (2.1767e-01)	Acc@1  92.97 ( 92.25)	Acc@5 100.00 ( 99.89)
Epoch: [21][380/391]	Time  0.133 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.8445e-01 (2.1783e-01)	Acc@1  93.75 ( 92.27)	Acc@5 100.00 ( 99.90)
Epoch: [21][390/391]	Time  0.106 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.5991e-01 (2.1810e-01)	Acc@1  93.75 ( 92.25)	Acc@5 100.00 ( 99.90)
## e[21] optimizer.zero_grad (sum) time: 0.6781632900238037
## e[21]       loss.backward (sum) time: 14.2600257396698
## e[21]      optimizer.step (sum) time: 8.052351474761963
## epoch[21] training(only) time: 46.9890353679657
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 4.2700e-01 (4.2700e-01)	Acc@1  86.00 ( 86.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 3.5107e-01 (3.6037e-01)	Acc@1  87.00 ( 88.09)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 4.2773e-01 (3.6746e-01)	Acc@1  86.00 ( 87.43)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 4.3066e-01 (3.8043e-01)	Acc@1  89.00 ( 87.55)	Acc@5 100.00 ( 99.55)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 3.8892e-01 (3.7415e-01)	Acc@1  86.00 ( 87.88)	Acc@5  99.00 ( 99.54)
Test: [ 50/100]	Time  0.049 ( 0.050)	Loss 3.0688e-01 (3.7588e-01)	Acc@1  90.00 ( 88.06)	Acc@5  99.00 ( 99.53)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 2.0081e-01 (3.6983e-01)	Acc@1  91.00 ( 88.21)	Acc@5 100.00 ( 99.59)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 4.5410e-01 (3.6472e-01)	Acc@1  87.00 ( 88.42)	Acc@5 100.00 ( 99.59)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 3.0322e-01 (3.6745e-01)	Acc@1  91.00 ( 88.25)	Acc@5 100.00 ( 99.59)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 3.6670e-01 (3.6938e-01)	Acc@1  87.00 ( 88.15)	Acc@5 100.00 ( 99.57)
 * Acc@1 88.180 Acc@5 99.580
### epoch[21] execution time: 51.928020000457764
EPOCH 22
# current learning rate: 0.1
# Switched to train mode...
Epoch: [22][  0/391]	Time  0.281 ( 0.281)	Data  0.145 ( 0.145)	Loss 2.3682e-01 (2.3682e-01)	Acc@1  92.19 ( 92.19)	Acc@5 100.00 (100.00)
Epoch: [22][ 10/391]	Time  0.117 ( 0.134)	Data  0.001 ( 0.014)	Loss 1.6431e-01 (1.8980e-01)	Acc@1  94.53 ( 93.39)	Acc@5 100.00 ( 99.93)
Epoch: [22][ 20/391]	Time  0.119 ( 0.127)	Data  0.001 ( 0.008)	Loss 1.4929e-01 (1.9252e-01)	Acc@1  95.31 ( 93.04)	Acc@5 100.00 ( 99.96)
Epoch: [22][ 30/391]	Time  0.122 ( 0.125)	Data  0.001 ( 0.006)	Loss 1.4648e-01 (1.9058e-01)	Acc@1  95.31 ( 93.30)	Acc@5 100.00 ( 99.95)
Epoch: [22][ 40/391]	Time  0.119 ( 0.124)	Data  0.001 ( 0.004)	Loss 1.8274e-01 (1.8307e-01)	Acc@1  92.19 ( 93.46)	Acc@5 100.00 ( 99.96)
Epoch: [22][ 50/391]	Time  0.117 ( 0.123)	Data  0.001 ( 0.004)	Loss 1.3416e-01 (1.8301e-01)	Acc@1  96.09 ( 93.49)	Acc@5 100.00 ( 99.94)
Epoch: [22][ 60/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 2.5977e-01 (1.8486e-01)	Acc@1  92.19 ( 93.49)	Acc@5  99.22 ( 99.94)
Epoch: [22][ 70/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.7273e-01 (1.8680e-01)	Acc@1  95.31 ( 93.45)	Acc@5 100.00 ( 99.94)
Epoch: [22][ 80/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.8298e-01 (1.8730e-01)	Acc@1  96.09 ( 93.45)	Acc@5  99.22 ( 99.94)
Epoch: [22][ 90/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.003)	Loss 2.7466e-01 (1.9109e-01)	Acc@1  89.06 ( 93.29)	Acc@5 100.00 ( 99.92)
Epoch: [22][100/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.0483e-01 (1.9184e-01)	Acc@1  90.62 ( 93.33)	Acc@5 100.00 ( 99.92)
Epoch: [22][110/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.4893e-01 (1.9269e-01)	Acc@1  94.53 ( 93.30)	Acc@5 100.00 ( 99.92)
Epoch: [22][120/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.6235e-01 (1.9078e-01)	Acc@1  93.75 ( 93.38)	Acc@5 100.00 ( 99.91)
Epoch: [22][130/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.7957e-01 (1.9158e-01)	Acc@1  94.53 ( 93.33)	Acc@5 100.00 ( 99.90)
Epoch: [22][140/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.9224e-01 (1.9204e-01)	Acc@1  90.62 ( 93.35)	Acc@5 100.00 ( 99.89)
Epoch: [22][150/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.3147e-01 (1.9325e-01)	Acc@1  96.09 ( 93.31)	Acc@5 100.00 ( 99.88)
Epoch: [22][160/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.2520e-01 (1.9377e-01)	Acc@1  87.50 ( 93.27)	Acc@5 100.00 ( 99.89)
Epoch: [22][170/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.5491e-01 (1.9815e-01)	Acc@1  93.75 ( 93.11)	Acc@5 100.00 ( 99.89)
Epoch: [22][180/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.1689e-01 (2.0087e-01)	Acc@1  89.06 ( 93.00)	Acc@5 100.00 ( 99.88)
Epoch: [22][190/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.9531e-01 (2.0256e-01)	Acc@1  92.97 ( 92.96)	Acc@5 100.00 ( 99.89)
Epoch: [22][200/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.9834e-01 (2.0468e-01)	Acc@1  90.62 ( 92.89)	Acc@5 100.00 ( 99.88)
Epoch: [22][210/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.7344e-01 (2.0470e-01)	Acc@1  89.06 ( 92.91)	Acc@5 100.00 ( 99.88)
Epoch: [22][220/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.4917e-01 (2.0500e-01)	Acc@1  93.75 ( 92.89)	Acc@5 100.00 ( 99.88)
Epoch: [22][230/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.8311e-01 (2.0617e-01)	Acc@1  93.75 ( 92.84)	Acc@5  99.22 ( 99.87)
Epoch: [22][240/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.2729e-01 (2.0542e-01)	Acc@1  92.19 ( 92.87)	Acc@5 100.00 ( 99.88)
Epoch: [22][250/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.6248e-01 (2.0499e-01)	Acc@1  95.31 ( 92.87)	Acc@5 100.00 ( 99.88)
Epoch: [22][260/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.5757e-01 (2.0445e-01)	Acc@1  88.28 ( 92.84)	Acc@5 100.00 ( 99.89)
Epoch: [22][270/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.7993e-01 (2.0485e-01)	Acc@1  94.53 ( 92.83)	Acc@5 100.00 ( 99.88)
Epoch: [22][280/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.3291e-01 (2.0531e-01)	Acc@1  90.62 ( 92.82)	Acc@5 100.00 ( 99.88)
Epoch: [22][290/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.4814e-01 (2.0762e-01)	Acc@1  89.06 ( 92.74)	Acc@5 100.00 ( 99.88)
Epoch: [22][300/391]	Time  0.129 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.3340e-01 (2.0758e-01)	Acc@1  91.41 ( 92.73)	Acc@5 100.00 ( 99.89)
Epoch: [22][310/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.4282e-01 (2.0789e-01)	Acc@1  96.09 ( 92.74)	Acc@5  99.22 ( 99.88)
Epoch: [22][320/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.6709e-01 (2.0776e-01)	Acc@1  88.28 ( 92.75)	Acc@5 100.00 ( 99.88)
Epoch: [22][330/391]	Time  0.126 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.2715e-01 (2.0775e-01)	Acc@1  89.06 ( 92.74)	Acc@5 100.00 ( 99.88)
Epoch: [22][340/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.0215e-01 (2.0851e-01)	Acc@1  91.41 ( 92.71)	Acc@5 100.00 ( 99.88)
Epoch: [22][350/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.4060e-01 (2.0788e-01)	Acc@1  92.19 ( 92.73)	Acc@5 100.00 ( 99.88)
Epoch: [22][360/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.4097e-01 (2.0815e-01)	Acc@1  90.62 ( 92.72)	Acc@5 100.00 ( 99.88)
Epoch: [22][370/391]	Time  0.124 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.6211e-01 (2.0766e-01)	Acc@1  95.31 ( 92.73)	Acc@5 100.00 ( 99.88)
Epoch: [22][380/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.3962e-01 (2.0749e-01)	Acc@1  89.84 ( 92.72)	Acc@5 100.00 ( 99.88)
Epoch: [22][390/391]	Time  0.106 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.9492e-01 (2.0710e-01)	Acc@1  90.00 ( 92.75)	Acc@5 100.00 ( 99.88)
## e[22] optimizer.zero_grad (sum) time: 0.6856770515441895
## e[22]       loss.backward (sum) time: 14.232129573822021
## e[22]      optimizer.step (sum) time: 8.0267493724823
## epoch[22] training(only) time: 46.953962326049805
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 3.3691e-01 (3.3691e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.059)	Loss 3.3594e-01 (3.3388e-01)	Acc@1  91.00 ( 89.73)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.048 ( 0.053)	Loss 4.8901e-01 (3.5287e-01)	Acc@1  84.00 ( 89.38)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.049 ( 0.051)	Loss 5.5176e-01 (3.7225e-01)	Acc@1  82.00 ( 88.68)	Acc@5  98.00 ( 99.61)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 3.9380e-01 (3.6549e-01)	Acc@1  86.00 ( 88.68)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.045 ( 0.050)	Loss 2.5488e-01 (3.6458e-01)	Acc@1  92.00 ( 88.94)	Acc@5  99.00 ( 99.69)
Test: [ 60/100]	Time  0.045 ( 0.049)	Loss 2.9883e-01 (3.6184e-01)	Acc@1  88.00 ( 88.89)	Acc@5 100.00 ( 99.67)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 5.3320e-01 (3.6563e-01)	Acc@1  85.00 ( 88.86)	Acc@5  99.00 ( 99.65)
Test: [ 80/100]	Time  0.046 ( 0.048)	Loss 1.5063e-01 (3.6668e-01)	Acc@1  95.00 ( 88.91)	Acc@5 100.00 ( 99.65)
Test: [ 90/100]	Time  0.045 ( 0.048)	Loss 2.4500e-01 (3.6951e-01)	Acc@1  91.00 ( 88.87)	Acc@5 100.00 ( 99.64)
 * Acc@1 88.870 Acc@5 99.670
### epoch[22] execution time: 51.86364197731018
EPOCH 23
# current learning rate: 0.1
# Switched to train mode...
Epoch: [23][  0/391]	Time  0.275 ( 0.275)	Data  0.140 ( 0.140)	Loss 1.1011e-01 (1.1011e-01)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [23][ 10/391]	Time  0.120 ( 0.134)	Data  0.001 ( 0.014)	Loss 1.8311e-01 (1.9831e-01)	Acc@1  95.31 ( 93.32)	Acc@5 100.00 ( 99.86)
Epoch: [23][ 20/391]	Time  0.122 ( 0.127)	Data  0.001 ( 0.008)	Loss 1.6125e-01 (2.0426e-01)	Acc@1  95.31 ( 93.15)	Acc@5 100.00 ( 99.93)
Epoch: [23][ 30/391]	Time  0.121 ( 0.125)	Data  0.001 ( 0.006)	Loss 2.6147e-01 (2.0038e-01)	Acc@1  89.06 ( 92.87)	Acc@5 100.00 ( 99.90)
Epoch: [23][ 40/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.004)	Loss 1.6943e-01 (2.0012e-01)	Acc@1  91.41 ( 92.76)	Acc@5 100.00 ( 99.89)
Epoch: [23][ 50/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.004)	Loss 2.0642e-01 (1.9414e-01)	Acc@1  94.53 ( 93.11)	Acc@5 100.00 ( 99.91)
Epoch: [23][ 60/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.5222e-01 (1.8808e-01)	Acc@1  95.31 ( 93.31)	Acc@5  99.22 ( 99.91)
Epoch: [23][ 70/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 8.8562e-02 (1.8486e-01)	Acc@1  98.44 ( 93.45)	Acc@5 100.00 ( 99.91)
Epoch: [23][ 80/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.003)	Loss 2.0679e-01 (1.8253e-01)	Acc@1  94.53 ( 93.61)	Acc@5 100.00 ( 99.90)
Epoch: [23][ 90/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.003)	Loss 1.4050e-01 (1.8062e-01)	Acc@1  96.09 ( 93.69)	Acc@5  99.22 ( 99.91)
Epoch: [23][100/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.2925e-01 (1.8011e-01)	Acc@1  89.84 ( 93.65)	Acc@5 100.00 ( 99.91)
Epoch: [23][110/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.2164e-01 (1.7903e-01)	Acc@1  96.09 ( 93.69)	Acc@5 100.00 ( 99.92)
Epoch: [23][120/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.5342e-01 (1.8276e-01)	Acc@1  92.19 ( 93.56)	Acc@5  99.22 ( 99.92)
Epoch: [23][130/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.2485e-01 (1.8321e-01)	Acc@1  92.97 ( 93.52)	Acc@5 100.00 ( 99.92)
Epoch: [23][140/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.5708e-01 (1.8419e-01)	Acc@1  92.97 ( 93.50)	Acc@5  99.22 ( 99.91)
Epoch: [23][150/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.7102e-01 (1.8364e-01)	Acc@1  94.53 ( 93.49)	Acc@5 100.00 ( 99.91)
Epoch: [23][160/391]	Time  0.127 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.8420e-01 (1.8469e-01)	Acc@1  93.75 ( 93.42)	Acc@5  99.22 ( 99.90)
Epoch: [23][170/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.2107e-01 (1.8555e-01)	Acc@1  90.62 ( 93.38)	Acc@5 100.00 ( 99.90)
Epoch: [23][180/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.6418e-01 (1.8622e-01)	Acc@1  94.53 ( 93.33)	Acc@5 100.00 ( 99.91)
Epoch: [23][190/391]	Time  0.124 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.4954e-01 (1.8764e-01)	Acc@1  95.31 ( 93.27)	Acc@5 100.00 ( 99.91)
Epoch: [23][200/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.7944e-01 (1.8804e-01)	Acc@1  94.53 ( 93.28)	Acc@5 100.00 ( 99.91)
Epoch: [23][210/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.8784e-01 (1.8801e-01)	Acc@1  87.50 ( 93.30)	Acc@5 100.00 ( 99.91)
Epoch: [23][220/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.1145e-01 (1.8924e-01)	Acc@1  96.88 ( 93.25)	Acc@5 100.00 ( 99.90)
Epoch: [23][230/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.0811e-01 (1.9041e-01)	Acc@1  90.62 ( 93.23)	Acc@5  99.22 ( 99.90)
Epoch: [23][240/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.8335e-01 (1.9072e-01)	Acc@1  95.31 ( 93.23)	Acc@5 100.00 ( 99.90)
Epoch: [23][250/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.2278e-01 (1.9227e-01)	Acc@1  92.97 ( 93.20)	Acc@5 100.00 ( 99.90)
Epoch: [23][260/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.3477e-01 (1.9433e-01)	Acc@1  94.53 ( 93.14)	Acc@5 100.00 ( 99.90)
Epoch: [23][270/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.0786e-01 (1.9598e-01)	Acc@1  89.06 ( 93.08)	Acc@5  98.44 ( 99.90)
Epoch: [23][280/391]	Time  0.116 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.9861e-01 (1.9619e-01)	Acc@1  90.62 ( 93.07)	Acc@5 100.00 ( 99.90)
Epoch: [23][290/391]	Time  0.122 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.7810e-01 (1.9628e-01)	Acc@1  92.97 ( 93.08)	Acc@5 100.00 ( 99.90)
Epoch: [23][300/391]	Time  0.122 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.0654e-01 (1.9673e-01)	Acc@1  92.19 ( 93.04)	Acc@5 100.00 ( 99.90)
Epoch: [23][310/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.4868e-01 (1.9640e-01)	Acc@1  96.09 ( 93.05)	Acc@5 100.00 ( 99.90)
Epoch: [23][320/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.5039e-01 (1.9584e-01)	Acc@1  94.53 ( 93.06)	Acc@5 100.00 ( 99.90)
Epoch: [23][330/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.4316e-01 (1.9596e-01)	Acc@1  89.84 ( 93.05)	Acc@5 100.00 ( 99.90)
Epoch: [23][340/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.4595e-01 (1.9669e-01)	Acc@1  85.94 ( 93.04)	Acc@5 100.00 ( 99.90)
Epoch: [23][350/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.2058e-01 (1.9724e-01)	Acc@1  91.41 ( 93.02)	Acc@5 100.00 ( 99.90)
Epoch: [23][360/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.5000e-01 (1.9789e-01)	Acc@1  91.41 ( 93.00)	Acc@5 100.00 ( 99.90)
Epoch: [23][370/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.8225e-01 (1.9780e-01)	Acc@1  93.75 ( 93.00)	Acc@5 100.00 ( 99.90)
Epoch: [23][380/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.5122e-01 (1.9860e-01)	Acc@1  90.62 ( 92.99)	Acc@5 100.00 ( 99.90)
Epoch: [23][390/391]	Time  0.107 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.2378e-01 (1.9848e-01)	Acc@1  97.50 ( 93.01)	Acc@5 100.00 ( 99.90)
## e[23] optimizer.zero_grad (sum) time: 0.6887073516845703
## e[23]       loss.backward (sum) time: 14.24484395980835
## e[23]      optimizer.step (sum) time: 8.06081509590149
## epoch[23] training(only) time: 46.97439408302307
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 2.6294e-01 (2.6294e-01)	Acc@1  88.00 ( 88.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 4.0527e-01 (3.7791e-01)	Acc@1  87.00 ( 88.45)	Acc@5 100.00 ( 99.55)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 3.9209e-01 (3.6719e-01)	Acc@1  87.00 ( 88.57)	Acc@5 100.00 ( 99.48)
Test: [ 30/100]	Time  0.045 ( 0.052)	Loss 4.3872e-01 (3.7131e-01)	Acc@1  82.00 ( 88.45)	Acc@5  99.00 ( 99.48)
Test: [ 40/100]	Time  0.056 ( 0.051)	Loss 3.3936e-01 (3.7278e-01)	Acc@1  91.00 ( 88.54)	Acc@5  98.00 ( 99.44)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 2.5000e-01 (3.7077e-01)	Acc@1  91.00 ( 88.67)	Acc@5 100.00 ( 99.47)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 2.8174e-01 (3.6181e-01)	Acc@1  91.00 ( 88.77)	Acc@5 100.00 ( 99.52)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 3.6255e-01 (3.6183e-01)	Acc@1  90.00 ( 88.73)	Acc@5 100.00 ( 99.55)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 2.0679e-01 (3.5744e-01)	Acc@1  90.00 ( 88.81)	Acc@5 100.00 ( 99.59)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 3.1006e-01 (3.5358e-01)	Acc@1  90.00 ( 88.92)	Acc@5 100.00 ( 99.62)
 * Acc@1 89.070 Acc@5 99.630
### epoch[23] execution time: 51.92436909675598
EPOCH 24
# current learning rate: 0.1
# Switched to train mode...
Epoch: [24][  0/391]	Time  0.270 ( 0.270)	Data  0.139 ( 0.139)	Loss 2.1411e-01 (2.1411e-01)	Acc@1  92.97 ( 92.97)	Acc@5  99.22 ( 99.22)
Epoch: [24][ 10/391]	Time  0.119 ( 0.134)	Data  0.001 ( 0.013)	Loss 1.4478e-01 (1.6669e-01)	Acc@1  95.31 ( 93.96)	Acc@5 100.00 ( 99.86)
Epoch: [24][ 20/391]	Time  0.129 ( 0.128)	Data  0.001 ( 0.008)	Loss 1.7236e-01 (1.7013e-01)	Acc@1  90.62 ( 93.60)	Acc@5 100.00 ( 99.93)
Epoch: [24][ 30/391]	Time  0.117 ( 0.126)	Data  0.001 ( 0.005)	Loss 1.9165e-01 (1.6471e-01)	Acc@1  92.97 ( 94.08)	Acc@5  99.22 ( 99.92)
Epoch: [24][ 40/391]	Time  0.119 ( 0.124)	Data  0.001 ( 0.004)	Loss 1.5002e-01 (1.7160e-01)	Acc@1  94.53 ( 93.69)	Acc@5 100.00 ( 99.92)
Epoch: [24][ 50/391]	Time  0.132 ( 0.124)	Data  0.001 ( 0.004)	Loss 1.4099e-01 (1.7045e-01)	Acc@1  95.31 ( 93.78)	Acc@5 100.00 ( 99.92)
Epoch: [24][ 60/391]	Time  0.121 ( 0.123)	Data  0.001 ( 0.003)	Loss 1.1469e-01 (1.6854e-01)	Acc@1  96.88 ( 93.92)	Acc@5 100.00 ( 99.92)
Epoch: [24][ 70/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.003)	Loss 1.4832e-01 (1.6977e-01)	Acc@1  95.31 ( 93.99)	Acc@5 100.00 ( 99.92)
Epoch: [24][ 80/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 2.2144e-01 (1.7186e-01)	Acc@1  89.84 ( 93.90)	Acc@5 100.00 ( 99.91)
Epoch: [24][ 90/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.8152e-01 (1.7299e-01)	Acc@1  94.53 ( 93.94)	Acc@5 100.00 ( 99.90)
Epoch: [24][100/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.9458e-01 (1.7366e-01)	Acc@1  94.53 ( 93.94)	Acc@5 100.00 ( 99.89)
Epoch: [24][110/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.7041e-01 (1.7426e-01)	Acc@1  96.09 ( 93.90)	Acc@5 100.00 ( 99.88)
Epoch: [24][120/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.7236e-01 (1.7502e-01)	Acc@1  94.53 ( 93.93)	Acc@5 100.00 ( 99.87)
Epoch: [24][130/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.3989e-01 (1.7578e-01)	Acc@1  94.53 ( 93.89)	Acc@5 100.00 ( 99.87)
Epoch: [24][140/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.5002e-01 (1.7669e-01)	Acc@1  93.75 ( 93.83)	Acc@5 100.00 ( 99.87)
Epoch: [24][150/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.1078e-01 (1.7854e-01)	Acc@1  96.88 ( 93.77)	Acc@5 100.00 ( 99.88)
Epoch: [24][160/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.5610e-01 (1.8026e-01)	Acc@1  91.41 ( 93.74)	Acc@5 100.00 ( 99.87)
Epoch: [24][170/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.5171e-01 (1.8001e-01)	Acc@1  92.19 ( 93.75)	Acc@5 100.00 ( 99.88)
Epoch: [24][180/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.1448e-01 (1.8111e-01)	Acc@1  94.53 ( 93.66)	Acc@5 100.00 ( 99.89)
Epoch: [24][190/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.1326e-01 (1.8273e-01)	Acc@1  92.19 ( 93.62)	Acc@5 100.00 ( 99.89)
Epoch: [24][200/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.3364e-01 (1.8310e-01)	Acc@1  93.75 ( 93.61)	Acc@5 100.00 ( 99.90)
Epoch: [24][210/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.2225e-01 (1.8312e-01)	Acc@1  94.53 ( 93.60)	Acc@5 100.00 ( 99.90)
Epoch: [24][220/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.5381e-01 (1.8319e-01)	Acc@1  95.31 ( 93.62)	Acc@5 100.00 ( 99.90)
Epoch: [24][230/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.3438e-01 (1.8391e-01)	Acc@1  92.19 ( 93.58)	Acc@5 100.00 ( 99.91)
Epoch: [24][240/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.0227e-01 (1.8610e-01)	Acc@1  92.97 ( 93.50)	Acc@5 100.00 ( 99.90)
Epoch: [24][250/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.4111e-01 (1.8857e-01)	Acc@1  96.09 ( 93.42)	Acc@5 100.00 ( 99.90)
Epoch: [24][260/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.2717e-01 (1.8940e-01)	Acc@1  90.62 ( 93.41)	Acc@5 100.00 ( 99.90)
Epoch: [24][270/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.2412e-01 (1.8939e-01)	Acc@1  92.97 ( 93.40)	Acc@5 100.00 ( 99.90)
Epoch: [24][280/391]	Time  0.123 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.3257e-01 (1.9053e-01)	Acc@1  96.88 ( 93.35)	Acc@5 100.00 ( 99.90)
Epoch: [24][290/391]	Time  0.125 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.4819e-01 (1.8996e-01)	Acc@1  95.31 ( 93.38)	Acc@5 100.00 ( 99.91)
Epoch: [24][300/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.3767e-01 (1.9143e-01)	Acc@1  92.19 ( 93.33)	Acc@5  99.22 ( 99.90)
Epoch: [24][310/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.5122e-01 (1.9345e-01)	Acc@1  88.28 ( 93.24)	Acc@5 100.00 ( 99.90)
Epoch: [24][320/391]	Time  0.134 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.1875e-01 (1.9421e-01)	Acc@1  92.19 ( 93.22)	Acc@5 100.00 ( 99.91)
Epoch: [24][330/391]	Time  0.126 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.4893e-01 (1.9371e-01)	Acc@1  96.09 ( 93.26)	Acc@5 100.00 ( 99.91)
Epoch: [24][340/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.5234e-01 (1.9372e-01)	Acc@1  93.75 ( 93.23)	Acc@5 100.00 ( 99.91)
Epoch: [24][350/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.6077e-01 (1.9293e-01)	Acc@1  94.53 ( 93.28)	Acc@5 100.00 ( 99.91)
Epoch: [24][360/391]	Time  0.127 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.8247e-01 (1.9280e-01)	Acc@1  91.41 ( 93.30)	Acc@5  99.22 ( 99.91)
Epoch: [24][370/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.1179e-01 (1.9298e-01)	Acc@1  92.97 ( 93.29)	Acc@5  99.22 ( 99.91)
Epoch: [24][380/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.3022e-01 (1.9364e-01)	Acc@1  92.19 ( 93.26)	Acc@5 100.00 ( 99.91)
Epoch: [24][390/391]	Time  0.104 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.9849e-01 (1.9410e-01)	Acc@1  91.25 ( 93.24)	Acc@5 100.00 ( 99.91)
## e[24] optimizer.zero_grad (sum) time: 0.6899220943450928
## e[24]       loss.backward (sum) time: 14.25694489479065
## e[24]      optimizer.step (sum) time: 8.124907970428467
## epoch[24] training(only) time: 47.044952392578125
# Switched to evaluate mode...
Test: [  0/100]	Time  0.181 ( 0.181)	Loss 2.6001e-01 (2.6001e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.048 ( 0.059)	Loss 3.5376e-01 (3.4383e-01)	Acc@1  90.00 ( 88.18)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.047 ( 0.054)	Loss 3.6768e-01 (3.5652e-01)	Acc@1  89.00 ( 88.05)	Acc@5  99.00 ( 99.52)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 4.9390e-01 (3.8347e-01)	Acc@1  87.00 ( 87.61)	Acc@5  99.00 ( 99.58)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 3.9624e-01 (3.8693e-01)	Acc@1  85.00 ( 87.59)	Acc@5  99.00 ( 99.59)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 3.2495e-01 (3.8566e-01)	Acc@1  88.00 ( 87.55)	Acc@5  99.00 ( 99.57)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 3.0200e-01 (3.8154e-01)	Acc@1  90.00 ( 87.80)	Acc@5 100.00 ( 99.59)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 5.3223e-01 (3.7483e-01)	Acc@1  88.00 ( 88.06)	Acc@5 100.00 ( 99.61)
Test: [ 80/100]	Time  0.048 ( 0.048)	Loss 3.6353e-01 (3.7590e-01)	Acc@1  87.00 ( 87.94)	Acc@5  99.00 ( 99.60)
Test: [ 90/100]	Time  0.049 ( 0.048)	Loss 2.8027e-01 (3.7657e-01)	Acc@1  91.00 ( 87.90)	Acc@5 100.00 ( 99.62)
 * Acc@1 88.090 Acc@5 99.640
### epoch[24] execution time: 51.95075583457947
EPOCH 25
# current learning rate: 0.1
# Switched to train mode...
Epoch: [25][  0/391]	Time  0.268 ( 0.268)	Data  0.140 ( 0.140)	Loss 1.9653e-01 (1.9653e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [25][ 10/391]	Time  0.117 ( 0.132)	Data  0.001 ( 0.014)	Loss 1.2671e-01 (1.7357e-01)	Acc@1  95.31 ( 94.32)	Acc@5 100.00 (100.00)
Epoch: [25][ 20/391]	Time  0.119 ( 0.126)	Data  0.001 ( 0.008)	Loss 1.8579e-01 (1.7504e-01)	Acc@1  93.75 ( 94.16)	Acc@5 100.00 (100.00)
Epoch: [25][ 30/391]	Time  0.117 ( 0.124)	Data  0.001 ( 0.005)	Loss 9.6863e-02 (1.7485e-01)	Acc@1  96.88 ( 94.15)	Acc@5 100.00 ( 99.97)
Epoch: [25][ 40/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.004)	Loss 1.3977e-01 (1.6942e-01)	Acc@1  95.31 ( 94.28)	Acc@5 100.00 ( 99.98)
Epoch: [25][ 50/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.004)	Loss 1.4124e-01 (1.6503e-01)	Acc@1  96.09 ( 94.39)	Acc@5 100.00 ( 99.94)
Epoch: [25][ 60/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.3647e-01 (1.6679e-01)	Acc@1  96.09 ( 94.28)	Acc@5 100.00 ( 99.94)
Epoch: [25][ 70/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.003)	Loss 1.0809e-01 (1.6177e-01)	Acc@1  96.09 ( 94.41)	Acc@5 100.00 ( 99.94)
Epoch: [25][ 80/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.003)	Loss 1.6870e-01 (1.5985e-01)	Acc@1  95.31 ( 94.46)	Acc@5 100.00 ( 99.95)
Epoch: [25][ 90/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.003)	Loss 1.6846e-01 (1.5963e-01)	Acc@1  93.75 ( 94.43)	Acc@5  99.22 ( 99.95)
Epoch: [25][100/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.2754e-01 (1.5854e-01)	Acc@1  92.19 ( 94.46)	Acc@5 100.00 ( 99.95)
Epoch: [25][110/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.5000e-01 (1.6058e-01)	Acc@1  91.41 ( 94.34)	Acc@5  99.22 ( 99.94)
Epoch: [25][120/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.4429e-01 (1.6402e-01)	Acc@1  94.53 ( 94.21)	Acc@5 100.00 ( 99.95)
Epoch: [25][130/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.5464e-01 (1.6598e-01)	Acc@1  90.62 ( 94.18)	Acc@5 100.00 ( 99.94)
Epoch: [25][140/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.4331e-01 (1.6736e-01)	Acc@1  94.53 ( 94.14)	Acc@5 100.00 ( 99.94)
Epoch: [25][150/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.8481e-01 (1.6942e-01)	Acc@1  93.75 ( 94.07)	Acc@5 100.00 ( 99.94)
Epoch: [25][160/391]	Time  0.123 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.1863e-01 (1.7142e-01)	Acc@1  95.31 ( 94.03)	Acc@5 100.00 ( 99.94)
Epoch: [25][170/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.7566e-01 (1.7185e-01)	Acc@1  93.75 ( 94.03)	Acc@5 100.00 ( 99.93)
Epoch: [25][180/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.3391e-01 (1.7257e-01)	Acc@1  96.88 ( 93.97)	Acc@5 100.00 ( 99.93)
Epoch: [25][190/391]	Time  0.132 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.6223e-01 (1.7260e-01)	Acc@1  92.19 ( 93.95)	Acc@5 100.00 ( 99.93)
Epoch: [25][200/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.3865e-01 (1.7239e-01)	Acc@1  90.62 ( 93.95)	Acc@5 100.00 ( 99.94)
Epoch: [25][210/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.2402e-01 (1.7297e-01)	Acc@1  95.31 ( 93.90)	Acc@5 100.00 ( 99.94)
Epoch: [25][220/391]	Time  0.126 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.1692e-01 (1.7409e-01)	Acc@1  92.97 ( 93.86)	Acc@5 100.00 ( 99.94)
Epoch: [25][230/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.2153e-01 (1.7623e-01)	Acc@1  90.62 ( 93.80)	Acc@5 100.00 ( 99.93)
Epoch: [25][240/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.0264e-01 (1.7853e-01)	Acc@1  92.97 ( 93.73)	Acc@5 100.00 ( 99.93)
Epoch: [25][250/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.0020e-01 (1.8006e-01)	Acc@1  90.62 ( 93.66)	Acc@5 100.00 ( 99.93)
Epoch: [25][260/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.4587e-01 (1.7983e-01)	Acc@1  95.31 ( 93.67)	Acc@5 100.00 ( 99.93)
Epoch: [25][270/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.7615e-01 (1.8009e-01)	Acc@1  95.31 ( 93.70)	Acc@5 100.00 ( 99.93)
Epoch: [25][280/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.6870e-01 (1.8022e-01)	Acc@1  94.53 ( 93.69)	Acc@5 100.00 ( 99.93)
Epoch: [25][290/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.5366e-01 (1.8068e-01)	Acc@1  91.41 ( 93.67)	Acc@5 100.00 ( 99.93)
Epoch: [25][300/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.7441e-01 (1.8165e-01)	Acc@1  89.84 ( 93.66)	Acc@5 100.00 ( 99.93)
Epoch: [25][310/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.2839e-01 (1.8138e-01)	Acc@1  93.75 ( 93.67)	Acc@5  99.22 ( 99.92)
Epoch: [25][320/391]	Time  0.115 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.6382e-01 (1.8195e-01)	Acc@1  95.31 ( 93.64)	Acc@5 100.00 ( 99.92)
Epoch: [25][330/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.6968e-01 (1.8224e-01)	Acc@1  93.75 ( 93.63)	Acc@5 100.00 ( 99.92)
Epoch: [25][340/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.4783e-01 (1.8242e-01)	Acc@1  95.31 ( 93.62)	Acc@5 100.00 ( 99.91)
Epoch: [25][350/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.7249e-01 (1.8230e-01)	Acc@1  93.75 ( 93.61)	Acc@5 100.00 ( 99.91)
Epoch: [25][360/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.1731e-01 (1.8199e-01)	Acc@1  95.31 ( 93.62)	Acc@5 100.00 ( 99.91)
Epoch: [25][370/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.9409e-01 (1.8288e-01)	Acc@1  90.62 ( 93.58)	Acc@5 100.00 ( 99.91)
Epoch: [25][380/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.1157e-01 (1.8425e-01)	Acc@1  96.88 ( 93.54)	Acc@5 100.00 ( 99.91)
Epoch: [25][390/391]	Time  0.106 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.1030e-01 (1.8485e-01)	Acc@1  86.25 ( 93.52)	Acc@5 100.00 ( 99.91)
## e[25] optimizer.zero_grad (sum) time: 0.6821742057800293
## e[25]       loss.backward (sum) time: 14.266101837158203
## e[25]      optimizer.step (sum) time: 8.0396409034729
## epoch[25] training(only) time: 46.98772215843201
# Switched to evaluate mode...
Test: [  0/100]	Time  0.181 ( 0.181)	Loss 3.0249e-01 (3.0249e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 3.6548e-01 (3.2876e-01)	Acc@1  90.00 ( 89.55)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 4.3506e-01 (3.5182e-01)	Acc@1  85.00 ( 89.14)	Acc@5 100.00 ( 99.52)
Test: [ 30/100]	Time  0.045 ( 0.051)	Loss 4.8853e-01 (3.5993e-01)	Acc@1  85.00 ( 89.03)	Acc@5  97.00 ( 99.39)
Test: [ 40/100]	Time  0.055 ( 0.051)	Loss 2.6904e-01 (3.5279e-01)	Acc@1  91.00 ( 89.24)	Acc@5 100.00 ( 99.44)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.9739e-01 (3.5467e-01)	Acc@1  93.00 ( 89.33)	Acc@5 100.00 ( 99.45)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 3.2959e-01 (3.5213e-01)	Acc@1  90.00 ( 89.11)	Acc@5  99.00 ( 99.49)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 5.3418e-01 (3.5814e-01)	Acc@1  90.00 ( 89.03)	Acc@5 100.00 ( 99.49)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 2.7930e-01 (3.6088e-01)	Acc@1  94.00 ( 89.04)	Acc@5 100.00 ( 99.53)
Test: [ 90/100]	Time  0.054 ( 0.049)	Loss 2.4561e-01 (3.6482e-01)	Acc@1  93.00 ( 88.89)	Acc@5 100.00 ( 99.54)
 * Acc@1 88.900 Acc@5 99.560
### epoch[25] execution time: 51.91657638549805
EPOCH 26
# current learning rate: 0.1
# Switched to train mode...
Epoch: [26][  0/391]	Time  0.267 ( 0.267)	Data  0.133 ( 0.133)	Loss 2.2668e-01 (2.2668e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
Epoch: [26][ 10/391]	Time  0.117 ( 0.132)	Data  0.001 ( 0.013)	Loss 1.5198e-01 (1.6325e-01)	Acc@1  96.09 ( 94.74)	Acc@5 100.00 ( 99.93)
Epoch: [26][ 20/391]	Time  0.119 ( 0.126)	Data  0.001 ( 0.007)	Loss 2.0776e-01 (1.5801e-01)	Acc@1  91.41 ( 94.57)	Acc@5 100.00 ( 99.93)
Epoch: [26][ 30/391]	Time  0.117 ( 0.124)	Data  0.001 ( 0.005)	Loss 1.6296e-01 (1.6599e-01)	Acc@1  94.53 ( 93.93)	Acc@5 100.00 ( 99.90)
Epoch: [26][ 40/391]	Time  0.117 ( 0.123)	Data  0.001 ( 0.004)	Loss 1.1090e-01 (1.6266e-01)	Acc@1  95.31 ( 94.26)	Acc@5 100.00 ( 99.90)
Epoch: [26][ 50/391]	Time  0.128 ( 0.122)	Data  0.001 ( 0.004)	Loss 1.7480e-01 (1.7108e-01)	Acc@1  93.75 ( 94.15)	Acc@5 100.00 ( 99.91)
Epoch: [26][ 60/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 8.0322e-02 (1.7093e-01)	Acc@1  97.66 ( 94.04)	Acc@5 100.00 ( 99.92)
Epoch: [26][ 70/391]	Time  0.116 ( 0.121)	Data  0.001 ( 0.003)	Loss 1.6895e-01 (1.7240e-01)	Acc@1  94.53 ( 93.93)	Acc@5 100.00 ( 99.92)
Epoch: [26][ 80/391]	Time  0.127 ( 0.121)	Data  0.001 ( 0.003)	Loss 1.7590e-01 (1.7506e-01)	Acc@1  92.97 ( 93.91)	Acc@5 100.00 ( 99.93)
Epoch: [26][ 90/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.6394e-01 (1.7237e-01)	Acc@1  94.53 ( 93.95)	Acc@5 100.00 ( 99.93)
Epoch: [26][100/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.1909e-01 (1.7506e-01)	Acc@1  90.62 ( 93.89)	Acc@5  99.22 ( 99.93)
Epoch: [26][110/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.9812e-01 (1.7656e-01)	Acc@1  92.19 ( 93.88)	Acc@5 100.00 ( 99.93)
Epoch: [26][120/391]	Time  0.124 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.8882e-01 (1.7987e-01)	Acc@1  87.50 ( 93.71)	Acc@5 100.00 ( 99.92)
Epoch: [26][130/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.6882e-01 (1.8012e-01)	Acc@1  93.75 ( 93.73)	Acc@5  99.22 ( 99.92)
Epoch: [26][140/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.8164e-01 (1.7978e-01)	Acc@1  92.19 ( 93.72)	Acc@5 100.00 ( 99.92)
Epoch: [26][150/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.8506e-01 (1.8116e-01)	Acc@1  95.31 ( 93.71)	Acc@5 100.00 ( 99.92)
Epoch: [26][160/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.5112e-01 (1.8151e-01)	Acc@1  92.97 ( 93.70)	Acc@5 100.00 ( 99.92)
Epoch: [26][170/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.5479e-01 (1.8017e-01)	Acc@1  95.31 ( 93.75)	Acc@5 100.00 ( 99.93)
Epoch: [26][180/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.4099e-01 (1.7938e-01)	Acc@1  95.31 ( 93.79)	Acc@5 100.00 ( 99.92)
Epoch: [26][190/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.1436e-01 (1.7937e-01)	Acc@1  89.84 ( 93.77)	Acc@5 100.00 ( 99.91)
Epoch: [26][200/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.6418e-01 (1.8044e-01)	Acc@1  94.53 ( 93.75)	Acc@5 100.00 ( 99.91)
Epoch: [26][210/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.6736e-01 (1.8072e-01)	Acc@1  93.75 ( 93.74)	Acc@5 100.00 ( 99.92)
Epoch: [26][220/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.1252e-01 (1.8344e-01)	Acc@1  91.41 ( 93.64)	Acc@5 100.00 ( 99.92)
Epoch: [26][230/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.7517e-01 (1.8299e-01)	Acc@1  92.97 ( 93.65)	Acc@5 100.00 ( 99.91)
Epoch: [26][240/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.5002e-01 (1.8324e-01)	Acc@1  93.75 ( 93.63)	Acc@5 100.00 ( 99.91)
Epoch: [26][250/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.0925e-01 (1.8224e-01)	Acc@1  96.09 ( 93.65)	Acc@5 100.00 ( 99.92)
Epoch: [26][260/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.7017e-01 (1.8199e-01)	Acc@1  95.31 ( 93.68)	Acc@5 100.00 ( 99.92)
Epoch: [26][270/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.4685e-01 (1.8284e-01)	Acc@1  94.53 ( 93.66)	Acc@5 100.00 ( 99.92)
Epoch: [26][280/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.0984e-01 (1.8288e-01)	Acc@1  91.41 ( 93.65)	Acc@5 100.00 ( 99.92)
Epoch: [26][290/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.6272e-01 (1.8368e-01)	Acc@1  94.53 ( 93.61)	Acc@5  99.22 ( 99.92)
Epoch: [26][300/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.2473e-01 (1.8477e-01)	Acc@1  92.19 ( 93.58)	Acc@5 100.00 ( 99.91)
Epoch: [26][310/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.6211e-01 (1.8477e-01)	Acc@1  94.53 ( 93.58)	Acc@5 100.00 ( 99.92)
Epoch: [26][320/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.2079e-01 (1.8461e-01)	Acc@1  95.31 ( 93.56)	Acc@5  99.22 ( 99.92)
Epoch: [26][330/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.4670e-01 (1.8449e-01)	Acc@1  93.75 ( 93.58)	Acc@5 100.00 ( 99.92)
Epoch: [26][340/391]	Time  0.123 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.7249e-01 (1.8438e-01)	Acc@1  92.19 ( 93.59)	Acc@5 100.00 ( 99.92)
Epoch: [26][350/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.4988e-01 (1.8384e-01)	Acc@1  91.41 ( 93.61)	Acc@5  99.22 ( 99.92)
Epoch: [26][360/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.3743e-01 (1.8422e-01)	Acc@1  92.97 ( 93.59)	Acc@5 100.00 ( 99.92)
Epoch: [26][370/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.6638e-01 (1.8475e-01)	Acc@1  94.53 ( 93.55)	Acc@5 100.00 ( 99.91)
Epoch: [26][380/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.5000e-01 (1.8513e-01)	Acc@1  93.75 ( 93.54)	Acc@5 100.00 ( 99.90)
Epoch: [26][390/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.1118e-01 (1.8425e-01)	Acc@1  92.50 ( 93.57)	Acc@5 100.00 ( 99.91)
## e[26] optimizer.zero_grad (sum) time: 0.6773991584777832
## e[26]       loss.backward (sum) time: 14.258413076400757
## e[26]      optimizer.step (sum) time: 7.99767541885376
## epoch[26] training(only) time: 46.9478554725647
# Switched to evaluate mode...
Test: [  0/100]	Time  0.178 ( 0.178)	Loss 2.3889e-01 (2.3889e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.059)	Loss 3.9990e-01 (3.2251e-01)	Acc@1  89.00 ( 89.45)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.048 ( 0.053)	Loss 4.0503e-01 (3.5263e-01)	Acc@1  87.00 ( 88.81)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.045 ( 0.051)	Loss 4.5459e-01 (3.6034e-01)	Acc@1  84.00 ( 88.71)	Acc@5 100.00 ( 99.77)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 3.5034e-01 (3.5885e-01)	Acc@1  89.00 ( 88.80)	Acc@5  99.00 ( 99.68)
Test: [ 50/100]	Time  0.051 ( 0.050)	Loss 1.7834e-01 (3.5372e-01)	Acc@1  94.00 ( 88.88)	Acc@5 100.00 ( 99.65)
Test: [ 60/100]	Time  0.045 ( 0.049)	Loss 2.9077e-01 (3.5111e-01)	Acc@1  90.00 ( 88.93)	Acc@5 100.00 ( 99.66)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 4.8511e-01 (3.4306e-01)	Acc@1  88.00 ( 89.11)	Acc@5 100.00 ( 99.69)
Test: [ 80/100]	Time  0.045 ( 0.049)	Loss 2.8564e-01 (3.4447e-01)	Acc@1  87.00 ( 89.04)	Acc@5  99.00 ( 99.69)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 2.2314e-01 (3.4442e-01)	Acc@1  92.00 ( 88.98)	Acc@5 100.00 ( 99.71)
 * Acc@1 89.120 Acc@5 99.730
### epoch[26] execution time: 51.887864112854004
EPOCH 27
# current learning rate: 0.1
# Switched to train mode...
Epoch: [27][  0/391]	Time  0.277 ( 0.277)	Data  0.142 ( 0.142)	Loss 8.2642e-02 (8.2642e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [27][ 10/391]	Time  0.117 ( 0.132)	Data  0.001 ( 0.014)	Loss 1.3538e-01 (1.5118e-01)	Acc@1  95.31 ( 94.60)	Acc@5 100.00 (100.00)
Epoch: [27][ 20/391]	Time  0.118 ( 0.126)	Data  0.001 ( 0.008)	Loss 2.0496e-01 (1.5449e-01)	Acc@1  91.41 ( 94.35)	Acc@5 100.00 (100.00)
Epoch: [27][ 30/391]	Time  0.118 ( 0.124)	Data  0.001 ( 0.006)	Loss 1.7139e-01 (1.5617e-01)	Acc@1  92.97 ( 94.48)	Acc@5 100.00 (100.00)
Epoch: [27][ 40/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.004)	Loss 1.4917e-01 (1.5679e-01)	Acc@1  95.31 ( 94.32)	Acc@5 100.00 (100.00)
Epoch: [27][ 50/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.004)	Loss 2.3157e-01 (1.6069e-01)	Acc@1  92.19 ( 94.30)	Acc@5  99.22 ( 99.98)
Epoch: [27][ 60/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.0309e-01 (1.5493e-01)	Acc@1  96.88 ( 94.48)	Acc@5 100.00 ( 99.99)
Epoch: [27][ 70/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.003)	Loss 1.2421e-01 (1.5457e-01)	Acc@1  96.09 ( 94.59)	Acc@5 100.00 ( 99.98)
Epoch: [27][ 80/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.003)	Loss 1.2372e-01 (1.5334e-01)	Acc@1  96.09 ( 94.64)	Acc@5 100.00 ( 99.98)
Epoch: [27][ 90/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.003)	Loss 1.0138e-01 (1.5337e-01)	Acc@1  96.88 ( 94.56)	Acc@5 100.00 ( 99.98)
Epoch: [27][100/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.4563e-01 (1.5400e-01)	Acc@1  95.31 ( 94.59)	Acc@5 100.00 ( 99.97)
Epoch: [27][110/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.7236e-01 (1.5535e-01)	Acc@1  93.75 ( 94.55)	Acc@5 100.00 ( 99.97)
Epoch: [27][120/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.1053e-01 (1.5724e-01)	Acc@1  96.09 ( 94.45)	Acc@5 100.00 ( 99.97)
Epoch: [27][130/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.5684e-01 (1.6085e-01)	Acc@1  92.19 ( 94.33)	Acc@5 100.00 ( 99.96)
Epoch: [27][140/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.2485e-01 (1.6227e-01)	Acc@1  92.19 ( 94.27)	Acc@5 100.00 ( 99.97)
Epoch: [27][150/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.5808e-01 (1.6333e-01)	Acc@1  93.75 ( 94.25)	Acc@5 100.00 ( 99.97)
Epoch: [27][160/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.3254e-01 (1.6629e-01)	Acc@1  92.97 ( 94.19)	Acc@5 100.00 ( 99.96)
Epoch: [27][170/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.9714e-01 (1.6757e-01)	Acc@1  93.75 ( 94.10)	Acc@5 100.00 ( 99.96)
Epoch: [27][180/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.8335e-01 (1.6856e-01)	Acc@1  92.97 ( 94.06)	Acc@5 100.00 ( 99.97)
Epoch: [27][190/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.3354e-01 (1.6944e-01)	Acc@1  96.09 ( 94.04)	Acc@5 100.00 ( 99.97)
Epoch: [27][200/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.6272e-01 (1.6921e-01)	Acc@1  95.31 ( 94.05)	Acc@5 100.00 ( 99.97)
Epoch: [27][210/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.2031e-01 (1.6967e-01)	Acc@1  90.62 ( 94.04)	Acc@5 100.00 ( 99.97)
Epoch: [27][220/391]	Time  0.128 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.1057e-01 (1.7036e-01)	Acc@1  92.97 ( 94.03)	Acc@5 100.00 ( 99.96)
Epoch: [27][230/391]	Time  0.123 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.2009e-01 (1.7106e-01)	Acc@1  90.62 ( 93.97)	Acc@5 100.00 ( 99.96)
Epoch: [27][240/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.9739e-01 (1.7132e-01)	Acc@1  90.62 ( 93.94)	Acc@5 100.00 ( 99.96)
Epoch: [27][250/391]	Time  0.126 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.1375e-01 (1.7153e-01)	Acc@1  90.62 ( 93.93)	Acc@5  99.22 ( 99.96)
Epoch: [27][260/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.2671e-01 (1.7232e-01)	Acc@1  94.53 ( 93.92)	Acc@5 100.00 ( 99.96)
Epoch: [27][270/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.2109e-01 (1.7166e-01)	Acc@1  96.88 ( 93.94)	Acc@5 100.00 ( 99.96)
Epoch: [27][280/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.4890e-01 (1.7205e-01)	Acc@1  91.41 ( 93.94)	Acc@5 100.00 ( 99.95)
Epoch: [27][290/391]	Time  0.124 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.3667e-01 (1.7292e-01)	Acc@1  89.84 ( 93.91)	Acc@5  99.22 ( 99.95)
Epoch: [27][300/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.3171e-01 (1.7292e-01)	Acc@1  95.31 ( 93.90)	Acc@5 100.00 ( 99.95)
Epoch: [27][310/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.3511e-01 (1.7330e-01)	Acc@1  92.19 ( 93.88)	Acc@5  99.22 ( 99.95)
Epoch: [27][320/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.6577e-01 (1.7350e-01)	Acc@1  92.97 ( 93.88)	Acc@5 100.00 ( 99.95)
Epoch: [27][330/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.0443e-01 (1.7361e-01)	Acc@1  96.09 ( 93.85)	Acc@5 100.00 ( 99.95)
Epoch: [27][340/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.1826e-01 (1.7404e-01)	Acc@1  92.19 ( 93.82)	Acc@5 100.00 ( 99.95)
Epoch: [27][350/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.5293e-01 (1.7499e-01)	Acc@1  89.84 ( 93.78)	Acc@5 100.00 ( 99.94)
Epoch: [27][360/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.4819e-01 (1.7527e-01)	Acc@1  95.31 ( 93.78)	Acc@5 100.00 ( 99.94)
Epoch: [27][370/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.8149e-01 (1.7621e-01)	Acc@1  89.84 ( 93.74)	Acc@5 100.00 ( 99.94)
Epoch: [27][380/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.5234e-01 (1.7653e-01)	Acc@1  93.75 ( 93.72)	Acc@5 100.00 ( 99.94)
Epoch: [27][390/391]	Time  0.101 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.2593e-01 (1.7736e-01)	Acc@1  92.50 ( 93.70)	Acc@5 100.00 ( 99.94)
## e[27] optimizer.zero_grad (sum) time: 0.6864748001098633
## e[27]       loss.backward (sum) time: 14.207916259765625
## e[27]      optimizer.step (sum) time: 8.078730344772339
## epoch[27] training(only) time: 46.964643478393555
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 2.8442e-01 (2.8442e-01)	Acc@1  91.00 ( 91.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.050 ( 0.060)	Loss 4.4873e-01 (4.2720e-01)	Acc@1  86.00 ( 87.64)	Acc@5 100.00 ( 99.45)
Test: [ 20/100]	Time  0.048 ( 0.054)	Loss 5.7764e-01 (4.2060e-01)	Acc@1  79.00 ( 87.57)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 3.9844e-01 (4.2644e-01)	Acc@1  87.00 ( 87.55)	Acc@5 100.00 ( 99.61)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 5.0977e-01 (4.3680e-01)	Acc@1  86.00 ( 87.39)	Acc@5  99.00 ( 99.56)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 3.0127e-01 (4.3536e-01)	Acc@1  92.00 ( 87.27)	Acc@5  99.00 ( 99.57)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 2.8247e-01 (4.3657e-01)	Acc@1  91.00 ( 87.03)	Acc@5 100.00 ( 99.62)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 3.8281e-01 (4.3010e-01)	Acc@1  86.00 ( 87.14)	Acc@5 100.00 ( 99.61)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 3.2349e-01 (4.2672e-01)	Acc@1  93.00 ( 87.32)	Acc@5 100.00 ( 99.63)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 2.8662e-01 (4.3639e-01)	Acc@1  89.00 ( 87.09)	Acc@5 100.00 ( 99.63)
 * Acc@1 87.000 Acc@5 99.640
### epoch[27] execution time: 51.89126396179199
EPOCH 28
# current learning rate: 0.1
# Switched to train mode...
Epoch: [28][  0/391]	Time  0.290 ( 0.290)	Data  0.148 ( 0.148)	Loss 2.8906e-01 (2.8906e-01)	Acc@1  89.06 ( 89.06)	Acc@5  99.22 ( 99.22)
Epoch: [28][ 10/391]	Time  0.116 ( 0.135)	Data  0.001 ( 0.014)	Loss 1.9641e-01 (1.9814e-01)	Acc@1  92.97 ( 92.47)	Acc@5 100.00 ( 99.93)
Epoch: [28][ 20/391]	Time  0.119 ( 0.127)	Data  0.001 ( 0.008)	Loss 1.9995e-01 (1.8909e-01)	Acc@1  94.53 ( 93.27)	Acc@5 100.00 ( 99.85)
Epoch: [28][ 30/391]	Time  0.117 ( 0.125)	Data  0.001 ( 0.006)	Loss 1.4355e-01 (1.7782e-01)	Acc@1  94.53 ( 93.60)	Acc@5 100.00 ( 99.90)
Epoch: [28][ 40/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.005)	Loss 1.8933e-01 (1.7257e-01)	Acc@1  93.75 ( 93.96)	Acc@5 100.00 ( 99.92)
Epoch: [28][ 50/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.004)	Loss 2.2473e-01 (1.7001e-01)	Acc@1  92.19 ( 94.07)	Acc@5 100.00 ( 99.94)
Epoch: [28][ 60/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.3806e-01 (1.6773e-01)	Acc@1  94.53 ( 94.13)	Acc@5 100.00 ( 99.95)
Epoch: [28][ 70/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.003)	Loss 9.1919e-02 (1.6554e-01)	Acc@1  97.66 ( 94.22)	Acc@5 100.00 ( 99.96)
Epoch: [28][ 80/391]	Time  0.131 ( 0.121)	Data  0.001 ( 0.003)	Loss 2.2522e-01 (1.6326e-01)	Acc@1  92.19 ( 94.33)	Acc@5 100.00 ( 99.96)
Epoch: [28][ 90/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.003)	Loss 1.9128e-01 (1.6426e-01)	Acc@1  92.97 ( 94.23)	Acc@5 100.00 ( 99.94)
Epoch: [28][100/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.2253e-01 (1.6198e-01)	Acc@1  90.62 ( 94.31)	Acc@5 100.00 ( 99.94)
Epoch: [28][110/391]	Time  0.133 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0925e-01 (1.6060e-01)	Acc@1  95.31 ( 94.35)	Acc@5 100.00 ( 99.94)
Epoch: [28][120/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.2351e-01 (1.6288e-01)	Acc@1  92.19 ( 94.24)	Acc@5  99.22 ( 99.94)
Epoch: [28][130/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.5295e-01 (1.6424e-01)	Acc@1  92.19 ( 94.18)	Acc@5 100.00 ( 99.94)
Epoch: [28][140/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.3950e-01 (1.6496e-01)	Acc@1  92.19 ( 94.16)	Acc@5 100.00 ( 99.94)
Epoch: [28][150/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.8613e-01 (1.6602e-01)	Acc@1  90.62 ( 94.15)	Acc@5 100.00 ( 99.94)
Epoch: [28][160/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 8.4778e-02 (1.6618e-01)	Acc@1  96.88 ( 94.18)	Acc@5 100.00 ( 99.93)
Epoch: [28][170/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.6321e-01 (1.6575e-01)	Acc@1  95.31 ( 94.18)	Acc@5 100.00 ( 99.94)
Epoch: [28][180/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.5552e-01 (1.6582e-01)	Acc@1  93.75 ( 94.21)	Acc@5 100.00 ( 99.93)
Epoch: [28][190/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.5686e-01 (1.6592e-01)	Acc@1  94.53 ( 94.19)	Acc@5  99.22 ( 99.92)
Epoch: [28][200/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.7493e-01 (1.6562e-01)	Acc@1  93.75 ( 94.20)	Acc@5 100.00 ( 99.92)
Epoch: [28][210/391]	Time  0.122 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.7441e-01 (1.6691e-01)	Acc@1  92.19 ( 94.16)	Acc@5  99.22 ( 99.90)
Epoch: [28][220/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.7310e-01 (1.6837e-01)	Acc@1  92.19 ( 94.10)	Acc@5 100.00 ( 99.90)
Epoch: [28][230/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.7883e-01 (1.6751e-01)	Acc@1  94.53 ( 94.13)	Acc@5 100.00 ( 99.90)
Epoch: [28][240/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.0630e-01 (1.6789e-01)	Acc@1  92.19 ( 94.11)	Acc@5 100.00 ( 99.90)
Epoch: [28][250/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.3608e-01 (1.6783e-01)	Acc@1  92.19 ( 94.09)	Acc@5 100.00 ( 99.90)
Epoch: [28][260/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.6602e-01 (1.6708e-01)	Acc@1  94.53 ( 94.14)	Acc@5 100.00 ( 99.90)
Epoch: [28][270/391]	Time  0.127 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.4771e-01 (1.6768e-01)	Acc@1  95.31 ( 94.11)	Acc@5 100.00 ( 99.90)
Epoch: [28][280/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.0361e-01 (1.6837e-01)	Acc@1  93.75 ( 94.09)	Acc@5  99.22 ( 99.90)
Epoch: [28][290/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.5808e-01 (1.6868e-01)	Acc@1  95.31 ( 94.08)	Acc@5 100.00 ( 99.90)
Epoch: [28][300/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.5586e-01 (1.7026e-01)	Acc@1  91.41 ( 94.01)	Acc@5  99.22 ( 99.90)
Epoch: [28][310/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.8445e-01 (1.7013e-01)	Acc@1  92.97 ( 94.01)	Acc@5 100.00 ( 99.90)
Epoch: [28][320/391]	Time  0.123 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.2976e-01 (1.7031e-01)	Acc@1  93.75 ( 94.00)	Acc@5 100.00 ( 99.90)
Epoch: [28][330/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.0374e-01 (1.7090e-01)	Acc@1  92.97 ( 93.98)	Acc@5 100.00 ( 99.90)
Epoch: [28][340/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.5732e-01 (1.7228e-01)	Acc@1  91.41 ( 93.94)	Acc@5 100.00 ( 99.89)
Epoch: [28][350/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.8274e-01 (1.7188e-01)	Acc@1  92.19 ( 93.95)	Acc@5 100.00 ( 99.89)
Epoch: [28][360/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 8.2764e-02 (1.7280e-01)	Acc@1  96.88 ( 93.93)	Acc@5 100.00 ( 99.89)
Epoch: [28][370/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.0337e-01 (1.7332e-01)	Acc@1  91.41 ( 93.91)	Acc@5 100.00 ( 99.90)
Epoch: [28][380/391]	Time  0.136 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.6663e-01 (1.7364e-01)	Acc@1  92.97 ( 93.89)	Acc@5 100.00 ( 99.90)
Epoch: [28][390/391]	Time  0.104 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.3960e-01 (1.7365e-01)	Acc@1  91.25 ( 93.90)	Acc@5 100.00 ( 99.90)
## e[28] optimizer.zero_grad (sum) time: 0.6820766925811768
## e[28]       loss.backward (sum) time: 14.227567672729492
## e[28]      optimizer.step (sum) time: 8.071723461151123
## epoch[28] training(only) time: 47.01818108558655
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 4.0479e-01 (4.0479e-01)	Acc@1  87.00 ( 87.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.049 ( 0.061)	Loss 4.8486e-01 (3.7928e-01)	Acc@1  87.00 ( 87.91)	Acc@5 100.00 ( 99.55)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 4.2920e-01 (3.9387e-01)	Acc@1  88.00 ( 88.10)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 4.9829e-01 (3.8323e-01)	Acc@1  87.00 ( 88.35)	Acc@5  98.00 ( 99.58)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 4.7754e-01 (3.8990e-01)	Acc@1  84.00 ( 88.12)	Acc@5 100.00 ( 99.56)
Test: [ 50/100]	Time  0.049 ( 0.050)	Loss 1.7773e-01 (3.8580e-01)	Acc@1  93.00 ( 88.27)	Acc@5  99.00 ( 99.61)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 2.6099e-01 (3.8392e-01)	Acc@1  93.00 ( 88.15)	Acc@5 100.00 ( 99.64)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 5.8350e-01 (3.8869e-01)	Acc@1  83.00 ( 88.06)	Acc@5 100.00 ( 99.66)
Test: [ 80/100]	Time  0.048 ( 0.049)	Loss 4.1528e-01 (3.8683e-01)	Acc@1  92.00 ( 88.19)	Acc@5 100.00 ( 99.70)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 1.7883e-01 (3.8378e-01)	Acc@1  92.00 ( 88.30)	Acc@5 100.00 ( 99.71)
 * Acc@1 88.280 Acc@5 99.730
### epoch[28] execution time: 52.01905393600464
EPOCH 29
# current learning rate: 0.1
# Switched to train mode...
Epoch: [29][  0/391]	Time  0.279 ( 0.279)	Data  0.148 ( 0.148)	Loss 9.4666e-02 (9.4666e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [29][ 10/391]	Time  0.117 ( 0.134)	Data  0.001 ( 0.014)	Loss 1.4001e-01 (1.5307e-01)	Acc@1  94.53 ( 94.60)	Acc@5  99.22 ( 99.93)
Epoch: [29][ 20/391]	Time  0.116 ( 0.127)	Data  0.001 ( 0.008)	Loss 1.4050e-01 (1.5481e-01)	Acc@1  92.97 ( 94.42)	Acc@5 100.00 ( 99.93)
Epoch: [29][ 30/391]	Time  0.117 ( 0.125)	Data  0.001 ( 0.006)	Loss 1.6785e-01 (1.5013e-01)	Acc@1  94.53 ( 94.81)	Acc@5 100.00 ( 99.92)
Epoch: [29][ 40/391]	Time  0.125 ( 0.123)	Data  0.001 ( 0.005)	Loss 2.3132e-01 (1.5326e-01)	Acc@1  93.75 ( 94.65)	Acc@5 100.00 ( 99.92)
Epoch: [29][ 50/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.004)	Loss 2.0789e-01 (1.5088e-01)	Acc@1  92.19 ( 94.73)	Acc@5 100.00 ( 99.94)
Epoch: [29][ 60/391]	Time  0.125 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.8811e-01 (1.5194e-01)	Acc@1  95.31 ( 94.80)	Acc@5 100.00 ( 99.95)
Epoch: [29][ 70/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 7.3730e-02 (1.4928e-01)	Acc@1  97.66 ( 94.89)	Acc@5 100.00 ( 99.96)
Epoch: [29][ 80/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.7090e-01 (1.5063e-01)	Acc@1  93.75 ( 94.78)	Acc@5 100.00 ( 99.96)
Epoch: [29][ 90/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.003)	Loss 1.8811e-01 (1.5066e-01)	Acc@1  94.53 ( 94.81)	Acc@5 100.00 ( 99.97)
Epoch: [29][100/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.4087e-01 (1.4927e-01)	Acc@1  94.53 ( 94.83)	Acc@5 100.00 ( 99.97)
Epoch: [29][110/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.4050e-01 (1.4894e-01)	Acc@1  94.53 ( 94.82)	Acc@5 100.00 ( 99.96)
Epoch: [29][120/391]	Time  0.116 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.3989e-01 (1.4728e-01)	Acc@1  96.88 ( 94.82)	Acc@5 100.00 ( 99.97)
Epoch: [29][130/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.8762e-01 (1.4808e-01)	Acc@1  92.97 ( 94.77)	Acc@5 100.00 ( 99.97)
Epoch: [29][140/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.5051e-01 (1.5160e-01)	Acc@1  94.53 ( 94.65)	Acc@5 100.00 ( 99.97)
Epoch: [29][150/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.1761e-01 (1.5161e-01)	Acc@1  96.09 ( 94.65)	Acc@5 100.00 ( 99.96)
Epoch: [29][160/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.5771e-01 (1.5386e-01)	Acc@1  93.75 ( 94.53)	Acc@5 100.00 ( 99.95)
Epoch: [29][170/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.0715e-01 (1.5557e-01)	Acc@1  93.75 ( 94.51)	Acc@5 100.00 ( 99.95)
Epoch: [29][180/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.1945e-01 (1.5539e-01)	Acc@1  93.75 ( 94.51)	Acc@5 100.00 ( 99.95)
Epoch: [29][190/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.5659e-01 (1.5570e-01)	Acc@1  91.41 ( 94.51)	Acc@5 100.00 ( 99.96)
Epoch: [29][200/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.6929e-01 (1.5722e-01)	Acc@1  89.06 ( 94.45)	Acc@5 100.00 ( 99.96)
Epoch: [29][210/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.9346e-01 (1.5848e-01)	Acc@1  89.06 ( 94.42)	Acc@5  99.22 ( 99.95)
Epoch: [29][220/391]	Time  0.123 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.1731e-01 (1.5847e-01)	Acc@1  96.09 ( 94.40)	Acc@5  99.22 ( 99.95)
Epoch: [29][230/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.2671e-01 (1.5973e-01)	Acc@1  95.31 ( 94.36)	Acc@5 100.00 ( 99.95)
Epoch: [29][240/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.9727e-01 (1.5982e-01)	Acc@1  93.75 ( 94.36)	Acc@5 100.00 ( 99.95)
Epoch: [29][250/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.2683e-01 (1.6117e-01)	Acc@1  95.31 ( 94.33)	Acc@5 100.00 ( 99.95)
Epoch: [29][260/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 7.5500e-02 (1.6049e-01)	Acc@1  97.66 ( 94.36)	Acc@5 100.00 ( 99.95)
Epoch: [29][270/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.0315e-01 (1.6023e-01)	Acc@1  96.88 ( 94.36)	Acc@5 100.00 ( 99.95)
Epoch: [29][280/391]	Time  0.127 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.5674e-01 (1.6030e-01)	Acc@1  93.75 ( 94.37)	Acc@5 100.00 ( 99.95)
Epoch: [29][290/391]	Time  0.124 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.9995e-01 (1.6013e-01)	Acc@1  92.97 ( 94.36)	Acc@5 100.00 ( 99.95)
Epoch: [29][300/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.9092e-01 (1.6080e-01)	Acc@1  92.19 ( 94.34)	Acc@5 100.00 ( 99.95)
Epoch: [29][310/391]	Time  0.125 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.5784e-01 (1.6055e-01)	Acc@1  92.97 ( 94.36)	Acc@5 100.00 ( 99.95)
Epoch: [29][320/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.9666e-01 (1.6103e-01)	Acc@1  96.09 ( 94.34)	Acc@5 100.00 ( 99.95)
Epoch: [29][330/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.3062e-01 (1.6234e-01)	Acc@1  96.09 ( 94.32)	Acc@5 100.00 ( 99.95)
Epoch: [29][340/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.6077e-01 (1.6302e-01)	Acc@1  93.75 ( 94.30)	Acc@5 100.00 ( 99.94)
Epoch: [29][350/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.2607e-01 (1.6334e-01)	Acc@1  93.75 ( 94.32)	Acc@5  99.22 ( 99.94)
Epoch: [29][360/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 9.1003e-02 (1.6317e-01)	Acc@1  96.88 ( 94.31)	Acc@5 100.00 ( 99.94)
Epoch: [29][370/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.6956e-01 (1.6445e-01)	Acc@1  92.19 ( 94.26)	Acc@5 100.00 ( 99.94)
Epoch: [29][380/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.4062e-01 (1.6458e-01)	Acc@1  95.31 ( 94.24)	Acc@5 100.00 ( 99.94)
Epoch: [29][390/391]	Time  0.103 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.9995e-01 (1.6522e-01)	Acc@1  92.50 ( 94.21)	Acc@5 100.00 ( 99.94)
## e[29] optimizer.zero_grad (sum) time: 0.6847050189971924
## e[29]       loss.backward (sum) time: 14.191565752029419
## e[29]      optimizer.step (sum) time: 8.090760946273804
## epoch[29] training(only) time: 46.93958330154419
# Switched to evaluate mode...
Test: [  0/100]	Time  0.181 ( 0.181)	Loss 3.2886e-01 (3.2886e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.059)	Loss 3.1445e-01 (3.6487e-01)	Acc@1  90.00 ( 88.55)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.046 ( 0.053)	Loss 5.9180e-01 (3.6610e-01)	Acc@1  84.00 ( 88.57)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.046 ( 0.051)	Loss 4.5190e-01 (3.6914e-01)	Acc@1  89.00 ( 89.00)	Acc@5  99.00 ( 99.65)
Test: [ 40/100]	Time  0.045 ( 0.050)	Loss 3.7646e-01 (3.7268e-01)	Acc@1  86.00 ( 88.83)	Acc@5 100.00 ( 99.59)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 3.9819e-01 (3.6583e-01)	Acc@1  90.00 ( 89.00)	Acc@5 100.00 ( 99.59)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 3.2471e-01 (3.6392e-01)	Acc@1  88.00 ( 89.07)	Acc@5 100.00 ( 99.59)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 3.6230e-01 (3.5917e-01)	Acc@1  90.00 ( 88.97)	Acc@5 100.00 ( 99.61)
Test: [ 80/100]	Time  0.045 ( 0.049)	Loss 2.8125e-01 (3.5607e-01)	Acc@1  89.00 ( 89.12)	Acc@5 100.00 ( 99.63)
Test: [ 90/100]	Time  0.047 ( 0.048)	Loss 4.0527e-01 (3.5778e-01)	Acc@1  86.00 ( 89.04)	Acc@5 100.00 ( 99.60)
 * Acc@1 89.150 Acc@5 99.630
### epoch[29] execution time: 51.84405541419983
EPOCH 30
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [30][  0/391]	Time  0.288 ( 0.288)	Data  0.152 ( 0.152)	Loss 9.6191e-02 (9.6191e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [30][ 10/391]	Time  0.120 ( 0.135)	Data  0.001 ( 0.015)	Loss 1.3135e-01 (1.4970e-01)	Acc@1  95.31 ( 94.53)	Acc@5 100.00 ( 99.93)
Epoch: [30][ 20/391]	Time  0.119 ( 0.128)	Data  0.001 ( 0.008)	Loss 8.0383e-02 (1.3737e-01)	Acc@1  96.88 ( 95.35)	Acc@5 100.00 ( 99.96)
Epoch: [30][ 30/391]	Time  0.118 ( 0.125)	Data  0.001 ( 0.006)	Loss 9.7778e-02 (1.2782e-01)	Acc@1  96.88 ( 95.59)	Acc@5 100.00 ( 99.97)
Epoch: [30][ 40/391]	Time  0.117 ( 0.124)	Data  0.001 ( 0.005)	Loss 7.8857e-02 (1.2025e-01)	Acc@1  97.66 ( 95.90)	Acc@5 100.00 ( 99.98)
Epoch: [30][ 50/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.004)	Loss 5.2856e-02 (1.1688e-01)	Acc@1  98.44 ( 96.05)	Acc@5 100.00 ( 99.98)
Epoch: [30][ 60/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.004)	Loss 7.5073e-02 (1.1187e-01)	Acc@1  99.22 ( 96.29)	Acc@5 100.00 ( 99.99)
Epoch: [30][ 70/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.3855e-01 (1.1034e-01)	Acc@1  93.75 ( 96.30)	Acc@5 100.00 ( 99.99)
Epoch: [30][ 80/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.003)	Loss 1.1115e-01 (1.0575e-01)	Acc@1  96.88 ( 96.48)	Acc@5 100.00 ( 99.99)
Epoch: [30][ 90/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.003)	Loss 8.2642e-02 (1.0550e-01)	Acc@1  96.88 ( 96.49)	Acc@5 100.00 ( 99.99)
Epoch: [30][100/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.003)	Loss 6.4575e-02 (1.0445e-01)	Acc@1  99.22 ( 96.57)	Acc@5 100.00 ( 99.98)
Epoch: [30][110/391]	Time  0.132 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.6497e-02 (1.0365e-01)	Acc@1  96.88 ( 96.59)	Acc@5 100.00 ( 99.99)
Epoch: [30][120/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.7585e-02 (1.0334e-01)	Acc@1  97.66 ( 96.55)	Acc@5 100.00 ( 99.99)
Epoch: [30][130/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.1970e-02 (1.0194e-01)	Acc@1  98.44 ( 96.62)	Acc@5 100.00 ( 99.99)
Epoch: [30][140/391]	Time  0.130 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.8694e-02 (1.0151e-01)	Acc@1  96.88 ( 96.62)	Acc@5 100.00 ( 99.99)
Epoch: [30][150/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.8136e-02 (9.9794e-02)	Acc@1  99.22 ( 96.68)	Acc@5 100.00 ( 99.99)
Epoch: [30][160/391]	Time  0.126 ( 0.120)	Data  0.001 ( 0.002)	Loss 8.0627e-02 (9.8659e-02)	Acc@1  96.88 ( 96.73)	Acc@5 100.00 ( 99.99)
Epoch: [30][170/391]	Time  0.124 ( 0.120)	Data  0.001 ( 0.002)	Loss 8.7341e-02 (9.7538e-02)	Acc@1  97.66 ( 96.77)	Acc@5 100.00 ( 99.99)
Epoch: [30][180/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 8.2214e-02 (9.6453e-02)	Acc@1  98.44 ( 96.82)	Acc@5 100.00 ( 99.99)
Epoch: [30][190/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.1420e-01 (9.6025e-02)	Acc@1  96.88 ( 96.83)	Acc@5 100.00 ( 99.99)
Epoch: [30][200/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.3112e-02 (9.4238e-02)	Acc@1 100.00 ( 96.91)	Acc@5 100.00 ( 99.99)
Epoch: [30][210/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 8.9661e-02 (9.3496e-02)	Acc@1  97.66 ( 96.95)	Acc@5 100.00 ( 99.99)
Epoch: [30][220/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 8.5327e-02 (9.3200e-02)	Acc@1  97.66 ( 96.96)	Acc@5 100.00 ( 99.99)
Epoch: [30][230/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 7.2205e-02 (9.3127e-02)	Acc@1  96.88 ( 96.96)	Acc@5 100.00 ( 99.99)
Epoch: [30][240/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.0187e-01 (9.2200e-02)	Acc@1  95.31 ( 96.97)	Acc@5 100.00 ( 99.99)
Epoch: [30][250/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.0779e-01 (9.1402e-02)	Acc@1  96.09 ( 97.01)	Acc@5 100.00 ( 99.99)
Epoch: [30][260/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 8.8501e-02 (9.0363e-02)	Acc@1  96.09 ( 97.03)	Acc@5 100.00 ( 99.99)
Epoch: [30][270/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.7465e-02 (8.9444e-02)	Acc@1  98.44 ( 97.06)	Acc@5 100.00 ( 99.99)
Epoch: [30][280/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.9683e-02 (8.8767e-02)	Acc@1  97.66 ( 97.09)	Acc@5 100.00 ( 99.99)
Epoch: [30][290/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 9.1125e-02 (8.9020e-02)	Acc@1  98.44 ( 97.09)	Acc@5  99.22 ( 99.99)
Epoch: [30][300/391]	Time  0.123 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.2927e-02 (8.8647e-02)	Acc@1  97.66 ( 97.11)	Acc@5 100.00 ( 99.99)
Epoch: [30][310/391]	Time  0.116 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.2755e-02 (8.7901e-02)	Acc@1  98.44 ( 97.13)	Acc@5 100.00 ( 99.99)
Epoch: [30][320/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.0938e-01 (8.7218e-02)	Acc@1  96.09 ( 97.15)	Acc@5 100.00 ( 99.99)
Epoch: [30][330/391]	Time  0.129 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.9500e-02 (8.7106e-02)	Acc@1  98.44 ( 97.16)	Acc@5 100.00 ( 99.99)
Epoch: [30][340/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.3721e-02 (8.6468e-02)	Acc@1  97.66 ( 97.18)	Acc@5 100.00 ( 99.99)
Epoch: [30][350/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.6478e-02 (8.6387e-02)	Acc@1  99.22 ( 97.18)	Acc@5 100.00 ( 99.99)
Epoch: [30][360/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.1053e-01 (8.6208e-02)	Acc@1  95.31 ( 97.19)	Acc@5 100.00 ( 99.99)
Epoch: [30][370/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 9.1309e-02 (8.5745e-02)	Acc@1  94.53 ( 97.20)	Acc@5 100.00 ( 99.99)
Epoch: [30][380/391]	Time  0.131 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.7598e-02 (8.5278e-02)	Acc@1  99.22 ( 97.21)	Acc@5 100.00 ( 99.99)
Epoch: [30][390/391]	Time  0.111 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.5247e-01 (8.4797e-02)	Acc@1  96.25 ( 97.23)	Acc@5 100.00 ( 99.99)
## e[30] optimizer.zero_grad (sum) time: 0.6822004318237305
## e[30]       loss.backward (sum) time: 14.251618385314941
## e[30]      optimizer.step (sum) time: 8.068347930908203
## epoch[30] training(only) time: 47.009971141815186
# Switched to evaluate mode...
Test: [  0/100]	Time  0.204 ( 0.204)	Loss 1.5918e-01 (1.5918e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 1.9836e-01 (2.0982e-01)	Acc@1  95.00 ( 93.27)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 3.3740e-01 (2.3664e-01)	Acc@1  87.00 ( 92.38)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 2.5806e-01 (2.3751e-01)	Acc@1  93.00 ( 92.71)	Acc@5  99.00 ( 99.71)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 2.8613e-01 (2.4159e-01)	Acc@1  89.00 ( 92.46)	Acc@5 100.00 ( 99.78)
Test: [ 50/100]	Time  0.058 ( 0.050)	Loss 1.5674e-01 (2.4180e-01)	Acc@1  94.00 ( 92.35)	Acc@5 100.00 ( 99.80)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.8933e-01 (2.3676e-01)	Acc@1  95.00 ( 92.54)	Acc@5 100.00 ( 99.82)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 3.3350e-01 (2.3497e-01)	Acc@1  90.00 ( 92.46)	Acc@5 100.00 ( 99.82)
Test: [ 80/100]	Time  0.048 ( 0.049)	Loss 1.6260e-01 (2.3685e-01)	Acc@1  95.00 ( 92.51)	Acc@5 100.00 ( 99.83)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 1.5967e-01 (2.3560e-01)	Acc@1  93.00 ( 92.49)	Acc@5 100.00 ( 99.82)
 * Acc@1 92.510 Acc@5 99.840
### epoch[30] execution time: 51.95689654350281
EPOCH 31
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [31][  0/391]	Time  0.294 ( 0.294)	Data  0.150 ( 0.150)	Loss 2.5238e-02 (2.5238e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [31][ 10/391]	Time  0.120 ( 0.134)	Data  0.001 ( 0.014)	Loss 5.2399e-02 (5.6383e-02)	Acc@1  99.22 ( 98.51)	Acc@5 100.00 (100.00)
Epoch: [31][ 20/391]	Time  0.117 ( 0.127)	Data  0.001 ( 0.008)	Loss 5.1544e-02 (5.7706e-02)	Acc@1  98.44 ( 98.47)	Acc@5 100.00 (100.00)
Epoch: [31][ 30/391]	Time  0.120 ( 0.125)	Data  0.001 ( 0.006)	Loss 5.9448e-02 (6.0767e-02)	Acc@1  97.66 ( 98.26)	Acc@5 100.00 (100.00)
Epoch: [31][ 40/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.005)	Loss 8.5693e-02 (6.0082e-02)	Acc@1  97.66 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [31][ 50/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.004)	Loss 5.5328e-02 (5.9871e-02)	Acc@1  98.44 ( 98.25)	Acc@5 100.00 (100.00)
Epoch: [31][ 60/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 5.3772e-02 (6.1808e-02)	Acc@1  99.22 ( 98.18)	Acc@5 100.00 (100.00)
Epoch: [31][ 70/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 2.7222e-02 (6.0352e-02)	Acc@1 100.00 ( 98.20)	Acc@5 100.00 (100.00)
Epoch: [31][ 80/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.6478e-02 (5.9729e-02)	Acc@1  99.22 ( 98.21)	Acc@5 100.00 (100.00)
Epoch: [31][ 90/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.003)	Loss 4.3030e-02 (5.9595e-02)	Acc@1  99.22 ( 98.24)	Acc@5 100.00 (100.00)
Epoch: [31][100/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.5217e-02 (5.9422e-02)	Acc@1  99.22 ( 98.25)	Acc@5 100.00 (100.00)
Epoch: [31][110/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.0505e-02 (5.9089e-02)	Acc@1  97.66 ( 98.25)	Acc@5 100.00 (100.00)
Epoch: [31][120/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.2397e-02 (5.8975e-02)	Acc@1  97.66 ( 98.24)	Acc@5 100.00 (100.00)
Epoch: [31][130/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.1025e-02 (5.8761e-02)	Acc@1  99.22 ( 98.24)	Acc@5 100.00 (100.00)
Epoch: [31][140/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.7107e-02 (5.8151e-02)	Acc@1  97.66 ( 98.28)	Acc@5 100.00 (100.00)
Epoch: [31][150/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.5522e-02 (5.7924e-02)	Acc@1 100.00 ( 98.29)	Acc@5 100.00 (100.00)
Epoch: [31][160/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.4524e-02 (5.8149e-02)	Acc@1  98.44 ( 98.28)	Acc@5 100.00 (100.00)
Epoch: [31][170/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.9702e-02 (5.7809e-02)	Acc@1  98.44 ( 98.31)	Acc@5 100.00 (100.00)
Epoch: [31][180/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.3905e-02 (5.8069e-02)	Acc@1 100.00 ( 98.31)	Acc@5 100.00 (100.00)
Epoch: [31][190/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.5847e-02 (5.8204e-02)	Acc@1  98.44 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [31][200/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.6489e-02 (5.7829e-02)	Acc@1 100.00 ( 98.31)	Acc@5 100.00 (100.00)
Epoch: [31][210/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.0590e-01 (5.7909e-02)	Acc@1  96.09 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [31][220/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.7390e-02 (5.7952e-02)	Acc@1  99.22 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [31][230/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 8.2275e-02 (5.7828e-02)	Acc@1  96.88 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [31][240/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.6976e-02 (5.7857e-02)	Acc@1  98.44 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [31][250/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 7.3486e-02 (5.7874e-02)	Acc@1  97.66 ( 98.29)	Acc@5 100.00 (100.00)
Epoch: [31][260/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.5675e-02 (5.7783e-02)	Acc@1  99.22 ( 98.29)	Acc@5 100.00 (100.00)
Epoch: [31][270/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.1636e-02 (5.7730e-02)	Acc@1  97.66 ( 98.28)	Acc@5 100.00 (100.00)
Epoch: [31][280/391]	Time  0.116 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.3232e-02 (5.7940e-02)	Acc@1  98.44 ( 98.28)	Acc@5 100.00 (100.00)
Epoch: [31][290/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.7699e-02 (5.7817e-02)	Acc@1  99.22 ( 98.28)	Acc@5 100.00 (100.00)
Epoch: [31][300/391]	Time  0.122 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.0874e-02 (5.7933e-02)	Acc@1 100.00 ( 98.28)	Acc@5 100.00 (100.00)
Epoch: [31][310/391]	Time  0.126 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.8889e-02 (5.7891e-02)	Acc@1  99.22 ( 98.28)	Acc@5 100.00 (100.00)
Epoch: [31][320/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.2389e-02 (5.7664e-02)	Acc@1  99.22 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [31][330/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.9408e-02 (5.7460e-02)	Acc@1  98.44 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [31][340/391]	Time  0.129 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.4891e-02 (5.7231e-02)	Acc@1  99.22 ( 98.32)	Acc@5 100.00 (100.00)
Epoch: [31][350/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.9831e-02 (5.6860e-02)	Acc@1 100.00 ( 98.33)	Acc@5 100.00 (100.00)
Epoch: [31][360/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 7.4951e-02 (5.7053e-02)	Acc@1  97.66 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [31][370/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.9835e-02 (5.7181e-02)	Acc@1  99.22 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [31][380/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.5491e-02 (5.7201e-02)	Acc@1  97.66 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [31][390/391]	Time  0.104 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.7516e-02 (5.7013e-02)	Acc@1  97.50 ( 98.31)	Acc@5 100.00 (100.00)
## e[31] optimizer.zero_grad (sum) time: 0.6831681728363037
## e[31]       loss.backward (sum) time: 14.23122501373291
## e[31]      optimizer.step (sum) time: 8.03523564338684
## epoch[31] training(only) time: 46.95119547843933
# Switched to evaluate mode...
Test: [  0/100]	Time  0.179 ( 0.179)	Loss 1.5283e-01 (1.5283e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 2.0740e-01 (2.0542e-01)	Acc@1  95.00 ( 93.18)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 3.4741e-01 (2.3034e-01)	Acc@1  88.00 ( 92.48)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.049 ( 0.052)	Loss 2.3401e-01 (2.3084e-01)	Acc@1  92.00 ( 92.71)	Acc@5  99.00 ( 99.74)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 2.8979e-01 (2.3417e-01)	Acc@1  88.00 ( 92.49)	Acc@5 100.00 ( 99.76)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.5234e-01 (2.3411e-01)	Acc@1  94.00 ( 92.51)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.9214e-01 (2.2942e-01)	Acc@1  97.00 ( 92.64)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.045 ( 0.049)	Loss 3.1763e-01 (2.2758e-01)	Acc@1  93.00 ( 92.80)	Acc@5 100.00 ( 99.82)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 1.5808e-01 (2.2871e-01)	Acc@1  95.00 ( 92.86)	Acc@5 100.00 ( 99.81)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 1.7932e-01 (2.2799e-01)	Acc@1  92.00 ( 92.79)	Acc@5 100.00 ( 99.82)
 * Acc@1 92.880 Acc@5 99.840
### epoch[31] execution time: 51.9123055934906
EPOCH 32
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [32][  0/391]	Time  0.285 ( 0.285)	Data  0.152 ( 0.152)	Loss 6.7078e-02 (6.7078e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [32][ 10/391]	Time  0.117 ( 0.135)	Data  0.001 ( 0.015)	Loss 4.5654e-02 (5.8691e-02)	Acc@1  98.44 ( 98.08)	Acc@5 100.00 (100.00)
Epoch: [32][ 20/391]	Time  0.117 ( 0.127)	Data  0.001 ( 0.008)	Loss 6.2805e-02 (5.4160e-02)	Acc@1  98.44 ( 98.25)	Acc@5 100.00 (100.00)
Epoch: [32][ 30/391]	Time  0.120 ( 0.125)	Data  0.001 ( 0.006)	Loss 6.7627e-02 (5.4664e-02)	Acc@1  97.66 ( 98.41)	Acc@5 100.00 ( 99.97)
Epoch: [32][ 40/391]	Time  0.120 ( 0.124)	Data  0.001 ( 0.005)	Loss 1.5640e-02 (5.2313e-02)	Acc@1 100.00 ( 98.59)	Acc@5 100.00 ( 99.98)
Epoch: [32][ 50/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.004)	Loss 3.3051e-02 (5.2180e-02)	Acc@1  99.22 ( 98.54)	Acc@5 100.00 ( 99.98)
Epoch: [32][ 60/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.6112e-02 (5.0891e-02)	Acc@1  98.44 ( 98.57)	Acc@5 100.00 ( 99.99)
Epoch: [32][ 70/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 6.1310e-02 (5.0490e-02)	Acc@1  97.66 ( 98.58)	Acc@5 100.00 ( 99.99)
Epoch: [32][ 80/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.003)	Loss 6.2622e-02 (5.1443e-02)	Acc@1  96.88 ( 98.55)	Acc@5 100.00 ( 99.99)
Epoch: [32][ 90/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.003)	Loss 2.8336e-02 (5.2009e-02)	Acc@1 100.00 ( 98.48)	Acc@5 100.00 ( 99.99)
Epoch: [32][100/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.003)	Loss 2.7756e-02 (5.1030e-02)	Acc@1 100.00 ( 98.55)	Acc@5 100.00 ( 99.99)
Epoch: [32][110/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.8848e-02 (5.0750e-02)	Acc@1  96.88 ( 98.54)	Acc@5 100.00 ( 99.99)
Epoch: [32][120/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.2288e-02 (5.0381e-02)	Acc@1 100.00 ( 98.54)	Acc@5 100.00 ( 99.99)
Epoch: [32][130/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.9089e-02 (4.9496e-02)	Acc@1 100.00 ( 98.59)	Acc@5 100.00 ( 99.99)
Epoch: [32][140/391]	Time  0.126 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.1676e-02 (4.9093e-02)	Acc@1  97.66 ( 98.61)	Acc@5 100.00 ( 99.99)
Epoch: [32][150/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.1483e-02 (4.9178e-02)	Acc@1  98.44 ( 98.61)	Acc@5 100.00 ( 99.99)
Epoch: [32][160/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.7903e-02 (4.9492e-02)	Acc@1  99.22 ( 98.59)	Acc@5 100.00 (100.00)
Epoch: [32][170/391]	Time  0.130 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.3284e-02 (4.9231e-02)	Acc@1  96.88 ( 98.57)	Acc@5 100.00 (100.00)
Epoch: [32][180/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.8798e-02 (4.8570e-02)	Acc@1  99.22 ( 98.61)	Acc@5 100.00 (100.00)
Epoch: [32][190/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.5593e-02 (4.8988e-02)	Acc@1  98.44 ( 98.58)	Acc@5 100.00 (100.00)
Epoch: [32][200/391]	Time  0.127 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.1779e-02 (4.8672e-02)	Acc@1  99.22 ( 98.59)	Acc@5 100.00 (100.00)
Epoch: [32][210/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.9570e-02 (4.8505e-02)	Acc@1  98.44 ( 98.60)	Acc@5 100.00 (100.00)
Epoch: [32][220/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.4525e-02 (4.8339e-02)	Acc@1  97.66 ( 98.60)	Acc@5 100.00 (100.00)
Epoch: [32][230/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.1046e-02 (4.8294e-02)	Acc@1  98.44 ( 98.61)	Acc@5 100.00 (100.00)
Epoch: [32][240/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.2891e-01 (4.8575e-02)	Acc@1  96.88 ( 98.61)	Acc@5 100.00 ( 99.99)
Epoch: [32][250/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.9336e-02 (4.8726e-02)	Acc@1  97.66 ( 98.61)	Acc@5 100.00 ( 99.99)
Epoch: [32][260/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.9297e-02 (4.8503e-02)	Acc@1 100.00 ( 98.62)	Acc@5 100.00 ( 99.99)
Epoch: [32][270/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 7.5562e-02 (4.8440e-02)	Acc@1  97.66 ( 98.62)	Acc@5 100.00 ( 99.99)
Epoch: [32][280/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.1412e-02 (4.8787e-02)	Acc@1  99.22 ( 98.62)	Acc@5 100.00 ( 99.99)
Epoch: [32][290/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 8.2642e-02 (4.9205e-02)	Acc@1  96.88 ( 98.60)	Acc@5 100.00 ( 99.99)
Epoch: [32][300/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.7699e-02 (4.9103e-02)	Acc@1  98.44 ( 98.60)	Acc@5 100.00 ( 99.99)
Epoch: [32][310/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 7.6538e-02 (4.9065e-02)	Acc@1  97.66 ( 98.59)	Acc@5 100.00 ( 99.99)
Epoch: [32][320/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.3569e-02 (4.9256e-02)	Acc@1  99.22 ( 98.57)	Acc@5 100.00 (100.00)
Epoch: [32][330/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.8065e-02 (4.9407e-02)	Acc@1  99.22 ( 98.56)	Acc@5 100.00 (100.00)
Epoch: [32][340/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 8.4534e-02 (4.9363e-02)	Acc@1  96.88 ( 98.56)	Acc@5 100.00 ( 99.99)
Epoch: [32][350/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.5900e-02 (4.9069e-02)	Acc@1 100.00 ( 98.57)	Acc@5 100.00 ( 99.99)
Epoch: [32][360/391]	Time  0.116 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.9937e-02 (4.9092e-02)	Acc@1  97.66 ( 98.57)	Acc@5 100.00 ( 99.99)
Epoch: [32][370/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.5431e-02 (4.8922e-02)	Acc@1  99.22 ( 98.59)	Acc@5 100.00 ( 99.99)
Epoch: [32][380/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.0018e-02 (4.9240e-02)	Acc@1  98.44 ( 98.58)	Acc@5 100.00 ( 99.99)
Epoch: [32][390/391]	Time  0.110 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.8350e-02 (4.8911e-02)	Acc@1  98.75 ( 98.60)	Acc@5 100.00 ( 99.99)
## e[32] optimizer.zero_grad (sum) time: 0.6764192581176758
## e[32]       loss.backward (sum) time: 14.281368970870972
## e[32]      optimizer.step (sum) time: 8.045756578445435
## epoch[32] training(only) time: 47.00541877746582
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 1.7151e-01 (1.7151e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.049 ( 0.061)	Loss 1.9202e-01 (2.0046e-01)	Acc@1  96.00 ( 92.91)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 3.6987e-01 (2.2530e-01)	Acc@1  88.00 ( 92.48)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 2.5098e-01 (2.2828e-01)	Acc@1  92.00 ( 92.77)	Acc@5  99.00 ( 99.68)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 2.6904e-01 (2.3152e-01)	Acc@1  88.00 ( 92.51)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.045 ( 0.050)	Loss 1.6223e-01 (2.3281e-01)	Acc@1  94.00 ( 92.59)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.7480e-01 (2.2801e-01)	Acc@1  98.00 ( 92.75)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 3.2275e-01 (2.2624e-01)	Acc@1  91.00 ( 92.79)	Acc@5 100.00 ( 99.80)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.5942e-01 (2.2761e-01)	Acc@1  94.00 ( 92.74)	Acc@5 100.00 ( 99.81)
Test: [ 90/100]	Time  0.045 ( 0.049)	Loss 1.7896e-01 (2.2612e-01)	Acc@1  94.00 ( 92.73)	Acc@5 100.00 ( 99.82)
 * Acc@1 92.790 Acc@5 99.840
### epoch[32] execution time: 51.973074436187744
EPOCH 33
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [33][  0/391]	Time  0.293 ( 0.293)	Data  0.132 ( 0.132)	Loss 2.3056e-02 (2.3056e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [33][ 10/391]	Time  0.118 ( 0.135)	Data  0.001 ( 0.013)	Loss 3.7445e-02 (3.2930e-02)	Acc@1  98.44 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [33][ 20/391]	Time  0.117 ( 0.127)	Data  0.001 ( 0.007)	Loss 4.8523e-02 (3.7139e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [33][ 30/391]	Time  0.127 ( 0.125)	Data  0.001 ( 0.005)	Loss 3.9490e-02 (3.9827e-02)	Acc@1  99.22 ( 98.92)	Acc@5 100.00 (100.00)
Epoch: [33][ 40/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.004)	Loss 4.5624e-02 (3.9827e-02)	Acc@1  98.44 ( 98.89)	Acc@5 100.00 (100.00)
Epoch: [33][ 50/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 5.1147e-02 (4.0081e-02)	Acc@1  98.44 ( 98.90)	Acc@5 100.00 (100.00)
Epoch: [33][ 60/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.8219e-02 (4.0539e-02)	Acc@1  99.22 ( 98.89)	Acc@5 100.00 (100.00)
Epoch: [33][ 70/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.003)	Loss 4.9500e-02 (4.1682e-02)	Acc@1  97.66 ( 98.84)	Acc@5 100.00 (100.00)
Epoch: [33][ 80/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.003)	Loss 2.1729e-02 (4.3161e-02)	Acc@1 100.00 ( 98.80)	Acc@5 100.00 (100.00)
Epoch: [33][ 90/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.7323e-02 (4.2700e-02)	Acc@1  99.22 ( 98.82)	Acc@5 100.00 (100.00)
Epoch: [33][100/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.7176e-02 (4.2470e-02)	Acc@1 100.00 ( 98.80)	Acc@5 100.00 (100.00)
Epoch: [33][110/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.4271e-02 (4.3084e-02)	Acc@1 100.00 ( 98.79)	Acc@5 100.00 (100.00)
Epoch: [33][120/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.4241e-02 (4.2904e-02)	Acc@1  99.22 ( 98.81)	Acc@5 100.00 (100.00)
Epoch: [33][130/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.6580e-02 (4.3303e-02)	Acc@1  96.88 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [33][140/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.3599e-02 (4.3127e-02)	Acc@1  96.88 ( 98.78)	Acc@5 100.00 (100.00)
Epoch: [33][150/391]	Time  0.124 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.8055e-02 (4.3357e-02)	Acc@1  99.22 ( 98.79)	Acc@5 100.00 (100.00)
Epoch: [33][160/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.6840e-02 (4.3326e-02)	Acc@1 100.00 ( 98.79)	Acc@5 100.00 (100.00)
Epoch: [33][170/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.1118e-02 (4.3331e-02)	Acc@1 100.00 ( 98.80)	Acc@5 100.00 (100.00)
Epoch: [33][180/391]	Time  0.130 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.3549e-02 (4.3410e-02)	Acc@1  99.22 ( 98.78)	Acc@5 100.00 (100.00)
Epoch: [33][190/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.3356e-02 (4.3386e-02)	Acc@1  98.44 ( 98.76)	Acc@5 100.00 (100.00)
Epoch: [33][200/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.9429e-02 (4.3550e-02)	Acc@1  98.44 ( 98.75)	Acc@5 100.00 (100.00)
Epoch: [33][210/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.3422e-02 (4.3860e-02)	Acc@1  99.22 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [33][220/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.0883e-02 (4.3840e-02)	Acc@1  96.88 ( 98.71)	Acc@5 100.00 (100.00)
Epoch: [33][230/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.5162e-02 (4.3885e-02)	Acc@1 100.00 ( 98.70)	Acc@5 100.00 (100.00)
Epoch: [33][240/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.5370e-02 (4.3752e-02)	Acc@1  99.22 ( 98.71)	Acc@5 100.00 (100.00)
Epoch: [33][250/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.3392e-02 (4.3857e-02)	Acc@1 100.00 ( 98.71)	Acc@5 100.00 (100.00)
Epoch: [33][260/391]	Time  0.123 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.3499e-02 (4.3863e-02)	Acc@1 100.00 ( 98.70)	Acc@5 100.00 (100.00)
Epoch: [33][270/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.4037e-02 (4.3801e-02)	Acc@1  99.22 ( 98.71)	Acc@5 100.00 (100.00)
Epoch: [33][280/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.1281e-02 (4.3829e-02)	Acc@1 100.00 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [33][290/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.5247e-02 (4.4195e-02)	Acc@1  98.44 ( 98.71)	Acc@5 100.00 (100.00)
Epoch: [33][300/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.1799e-02 (4.4193e-02)	Acc@1 100.00 ( 98.71)	Acc@5 100.00 (100.00)
Epoch: [33][310/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 8.1055e-02 (4.4461e-02)	Acc@1  97.66 ( 98.69)	Acc@5 100.00 (100.00)
Epoch: [33][320/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.0016e-01 (4.4644e-02)	Acc@1  95.31 ( 98.68)	Acc@5 100.00 (100.00)
Epoch: [33][330/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.7659e-02 (4.4525e-02)	Acc@1  98.44 ( 98.69)	Acc@5 100.00 (100.00)
Epoch: [33][340/391]	Time  0.130 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.8295e-02 (4.4405e-02)	Acc@1 100.00 ( 98.70)	Acc@5 100.00 (100.00)
Epoch: [33][350/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.5253e-02 (4.4064e-02)	Acc@1 100.00 ( 98.71)	Acc@5 100.00 (100.00)
Epoch: [33][360/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.5095e-02 (4.3926e-02)	Acc@1  98.44 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [33][370/391]	Time  0.126 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.6844e-02 (4.3776e-02)	Acc@1  98.44 ( 98.73)	Acc@5 100.00 (100.00)
Epoch: [33][380/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.8818e-02 (4.3940e-02)	Acc@1 100.00 ( 98.73)	Acc@5 100.00 (100.00)
Epoch: [33][390/391]	Time  0.107 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.9998e-02 (4.4143e-02)	Acc@1  97.50 ( 98.71)	Acc@5 100.00 (100.00)
## e[33] optimizer.zero_grad (sum) time: 0.6730101108551025
## e[33]       loss.backward (sum) time: 14.284108638763428
## e[33]      optimizer.step (sum) time: 8.03657579421997
## epoch[33] training(only) time: 46.94579076766968
# Switched to evaluate mode...
Test: [  0/100]	Time  0.176 ( 0.176)	Loss 1.5430e-01 (1.5430e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 2.0679e-01 (1.9558e-01)	Acc@1  96.00 ( 93.09)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.046 ( 0.053)	Loss 3.9307e-01 (2.2841e-01)	Acc@1  88.00 ( 92.43)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.046 ( 0.051)	Loss 2.2498e-01 (2.3222e-01)	Acc@1  93.00 ( 92.65)	Acc@5  99.00 ( 99.74)
Test: [ 40/100]	Time  0.049 ( 0.051)	Loss 2.5610e-01 (2.3223e-01)	Acc@1  89.00 ( 92.46)	Acc@5 100.00 ( 99.78)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.5503e-01 (2.3526e-01)	Acc@1  94.00 ( 92.51)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 2.0447e-01 (2.3072e-01)	Acc@1  96.00 ( 92.72)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 3.5327e-01 (2.2856e-01)	Acc@1  91.00 ( 92.80)	Acc@5 100.00 ( 99.82)
Test: [ 80/100]	Time  0.062 ( 0.049)	Loss 1.3196e-01 (2.3072e-01)	Acc@1  96.00 ( 92.79)	Acc@5 100.00 ( 99.83)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.7456e-01 (2.2989e-01)	Acc@1  94.00 ( 92.85)	Acc@5 100.00 ( 99.84)
 * Acc@1 92.950 Acc@5 99.840
### epoch[33] execution time: 51.90718197822571
EPOCH 34
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [34][  0/391]	Time  0.273 ( 0.273)	Data  0.145 ( 0.145)	Loss 5.5267e-02 (5.5267e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [34][ 10/391]	Time  0.118 ( 0.133)	Data  0.001 ( 0.014)	Loss 3.4180e-02 (4.9162e-02)	Acc@1  98.44 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [34][ 20/391]	Time  0.122 ( 0.126)	Data  0.001 ( 0.008)	Loss 6.8542e-02 (4.4322e-02)	Acc@1  98.44 ( 98.62)	Acc@5 100.00 (100.00)
Epoch: [34][ 30/391]	Time  0.118 ( 0.124)	Data  0.001 ( 0.006)	Loss 3.3112e-02 (4.3133e-02)	Acc@1  98.44 ( 98.64)	Acc@5 100.00 (100.00)
Epoch: [34][ 40/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.004)	Loss 3.7079e-02 (4.2018e-02)	Acc@1  98.44 ( 98.65)	Acc@5 100.00 (100.00)
Epoch: [34][ 50/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.004)	Loss 5.3711e-02 (4.1222e-02)	Acc@1  98.44 ( 98.71)	Acc@5 100.00 (100.00)
Epoch: [34][ 60/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 3.4546e-02 (4.1438e-02)	Acc@1  99.22 ( 98.71)	Acc@5 100.00 ( 99.99)
Epoch: [34][ 70/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.003)	Loss 4.4830e-02 (4.0632e-02)	Acc@1  98.44 ( 98.77)	Acc@5 100.00 ( 99.99)
Epoch: [34][ 80/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.003)	Loss 2.7115e-02 (4.0478e-02)	Acc@1 100.00 ( 98.80)	Acc@5 100.00 ( 99.99)
Epoch: [34][ 90/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.003)	Loss 2.5711e-02 (4.0023e-02)	Acc@1  99.22 ( 98.82)	Acc@5 100.00 ( 99.99)
Epoch: [34][100/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.0638e-02 (4.0824e-02)	Acc@1  97.66 ( 98.78)	Acc@5 100.00 ( 99.99)
Epoch: [34][110/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.6102e-02 (4.0980e-02)	Acc@1  99.22 ( 98.80)	Acc@5 100.00 ( 99.99)
Epoch: [34][120/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.9469e-02 (4.0494e-02)	Acc@1  99.22 ( 98.84)	Acc@5 100.00 ( 99.99)
Epoch: [34][130/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.3955e-02 (4.0259e-02)	Acc@1  99.22 ( 98.88)	Acc@5 100.00 ( 99.99)
Epoch: [34][140/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.1737e-02 (3.9551e-02)	Acc@1  97.66 ( 98.90)	Acc@5 100.00 ( 99.99)
Epoch: [34][150/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.0859e-02 (3.9297e-02)	Acc@1 100.00 ( 98.88)	Acc@5 100.00 ( 99.99)
Epoch: [34][160/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.2797e-02 (3.9627e-02)	Acc@1 100.00 ( 98.88)	Acc@5 100.00 (100.00)
Epoch: [34][170/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.2419e-02 (3.9354e-02)	Acc@1  98.44 ( 98.87)	Acc@5 100.00 (100.00)
Epoch: [34][180/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.4576e-02 (3.9444e-02)	Acc@1  99.22 ( 98.85)	Acc@5 100.00 (100.00)
Epoch: [34][190/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.3223e-02 (3.9908e-02)	Acc@1  98.44 ( 98.83)	Acc@5 100.00 (100.00)
Epoch: [34][200/391]	Time  0.128 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.9153e-02 (3.9996e-02)	Acc@1  99.22 ( 98.85)	Acc@5 100.00 (100.00)
Epoch: [34][210/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.4481e-02 (3.9898e-02)	Acc@1 100.00 ( 98.85)	Acc@5 100.00 (100.00)
Epoch: [34][220/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.6265e-02 (4.0185e-02)	Acc@1  97.66 ( 98.84)	Acc@5 100.00 (100.00)
Epoch: [34][230/391]	Time  0.133 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.1423e-02 (4.0317e-02)	Acc@1  99.22 ( 98.83)	Acc@5 100.00 (100.00)
Epoch: [34][240/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.5839e-02 (4.0300e-02)	Acc@1 100.00 ( 98.83)	Acc@5 100.00 (100.00)
Epoch: [34][250/391]	Time  0.122 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.0466e-02 (4.0174e-02)	Acc@1  99.22 ( 98.83)	Acc@5 100.00 (100.00)
Epoch: [34][260/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.5796e-02 (4.0304e-02)	Acc@1  97.66 ( 98.84)	Acc@5 100.00 (100.00)
Epoch: [34][270/391]	Time  0.123 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.0649e-02 (4.0175e-02)	Acc@1  98.44 ( 98.84)	Acc@5 100.00 (100.00)
Epoch: [34][280/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.2053e-02 (4.0516e-02)	Acc@1  98.44 ( 98.82)	Acc@5 100.00 (100.00)
Epoch: [34][290/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.0283e-02 (4.0747e-02)	Acc@1  98.44 ( 98.82)	Acc@5 100.00 (100.00)
Epoch: [34][300/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.4342e-02 (4.0583e-02)	Acc@1  99.22 ( 98.82)	Acc@5 100.00 (100.00)
Epoch: [34][310/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.5685e-02 (4.0211e-02)	Acc@1  98.44 ( 98.84)	Acc@5 100.00 (100.00)
Epoch: [34][320/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.4220e-02 (4.0143e-02)	Acc@1  99.22 ( 98.84)	Acc@5 100.00 (100.00)
Epoch: [34][330/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.4973e-02 (3.9922e-02)	Acc@1  99.22 ( 98.85)	Acc@5 100.00 (100.00)
Epoch: [34][340/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.4668e-02 (3.9668e-02)	Acc@1  98.44 ( 98.86)	Acc@5 100.00 (100.00)
Epoch: [34][350/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.8757e-02 (3.9807e-02)	Acc@1  99.22 ( 98.86)	Acc@5 100.00 (100.00)
Epoch: [34][360/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.0222e-02 (4.0001e-02)	Acc@1 100.00 ( 98.85)	Acc@5 100.00 (100.00)
Epoch: [34][370/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.2079e-02 (3.9903e-02)	Acc@1  99.22 ( 98.86)	Acc@5 100.00 (100.00)
Epoch: [34][380/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.3575e-02 (3.9612e-02)	Acc@1  99.22 ( 98.87)	Acc@5 100.00 (100.00)
Epoch: [34][390/391]	Time  0.104 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.9580e-02 (3.9545e-02)	Acc@1  97.50 ( 98.88)	Acc@5 100.00 (100.00)
## e[34] optimizer.zero_grad (sum) time: 0.6824407577514648
## e[34]       loss.backward (sum) time: 14.280768394470215
## e[34]      optimizer.step (sum) time: 7.979510545730591
## epoch[34] training(only) time: 46.902315855026245
# Switched to evaluate mode...
Test: [  0/100]	Time  0.178 ( 0.178)	Loss 1.6736e-01 (1.6736e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.048 ( 0.060)	Loss 1.8542e-01 (2.0321e-01)	Acc@1  96.00 ( 93.18)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.045 ( 0.054)	Loss 3.9771e-01 (2.3169e-01)	Acc@1  90.00 ( 92.57)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.048 ( 0.052)	Loss 2.6538e-01 (2.3598e-01)	Acc@1  93.00 ( 92.84)	Acc@5  99.00 ( 99.68)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 2.7954e-01 (2.3860e-01)	Acc@1  89.00 ( 92.56)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.7163e-01 (2.3960e-01)	Acc@1  93.00 ( 92.57)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.048 ( 0.050)	Loss 2.0166e-01 (2.3439e-01)	Acc@1  97.00 ( 92.75)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 3.1250e-01 (2.3116e-01)	Acc@1  93.00 ( 92.89)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4673e-01 (2.3169e-01)	Acc@1  95.00 ( 92.98)	Acc@5 100.00 ( 99.81)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 2.0703e-01 (2.3005e-01)	Acc@1  94.00 ( 93.00)	Acc@5 100.00 ( 99.82)
 * Acc@1 93.100 Acc@5 99.840
### epoch[34] execution time: 51.83984351158142
EPOCH 35
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [35][  0/391]	Time  0.279 ( 0.279)	Data  0.144 ( 0.144)	Loss 7.1838e-02 (7.1838e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [35][ 10/391]	Time  0.122 ( 0.134)	Data  0.001 ( 0.014)	Loss 2.0950e-02 (3.9010e-02)	Acc@1 100.00 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [35][ 20/391]	Time  0.118 ( 0.127)	Data  0.001 ( 0.008)	Loss 3.8269e-02 (4.2889e-02)	Acc@1  99.22 ( 98.74)	Acc@5 100.00 (100.00)
Epoch: [35][ 30/391]	Time  0.122 ( 0.125)	Data  0.001 ( 0.006)	Loss 2.7481e-02 (3.9980e-02)	Acc@1  99.22 ( 98.94)	Acc@5 100.00 (100.00)
Epoch: [35][ 40/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.004)	Loss 3.4241e-02 (3.7440e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [35][ 50/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.004)	Loss 3.5461e-02 (3.8118e-02)	Acc@1  98.44 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [35][ 60/391]	Time  0.126 ( 0.122)	Data  0.001 ( 0.003)	Loss 5.0629e-02 (3.7900e-02)	Acc@1  99.22 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [35][ 70/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.003)	Loss 5.4382e-02 (3.7160e-02)	Acc@1  97.66 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [35][ 80/391]	Time  0.127 ( 0.121)	Data  0.001 ( 0.003)	Loss 4.5746e-02 (3.6659e-02)	Acc@1  99.22 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [35][ 90/391]	Time  0.128 ( 0.121)	Data  0.001 ( 0.003)	Loss 1.9791e-02 (3.6717e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [35][100/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.4435e-02 (3.6545e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [35][110/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.5238e-02 (3.6717e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [35][120/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.4414e-02 (3.6861e-02)	Acc@1 100.00 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [35][130/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.3955e-02 (3.6505e-02)	Acc@1  96.88 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [35][140/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.0599e-02 (3.6338e-02)	Acc@1 100.00 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [35][150/391]	Time  0.125 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.9225e-02 (3.6195e-02)	Acc@1  98.44 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [35][160/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.9800e-02 (3.6363e-02)	Acc@1  99.22 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [35][170/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.6152e-02 (3.6200e-02)	Acc@1  98.44 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [35][180/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.1118e-02 (3.6438e-02)	Acc@1  99.22 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [35][190/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.4016e-02 (3.6954e-02)	Acc@1  96.88 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [35][200/391]	Time  0.126 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.3691e-02 (3.6837e-02)	Acc@1  98.44 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [35][210/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.1982e-02 (3.6900e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [35][220/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.6230e-02 (3.6477e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [35][230/391]	Time  0.116 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.9449e-02 (3.6985e-02)	Acc@1  99.22 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [35][240/391]	Time  0.122 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.9469e-02 (3.6898e-02)	Acc@1  99.22 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [35][250/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.3538e-02 (3.6812e-02)	Acc@1  97.66 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [35][260/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.2833e-02 (3.7132e-02)	Acc@1 100.00 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [35][270/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.1250e-02 (3.7291e-02)	Acc@1  98.44 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [35][280/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.2074e-02 (3.7105e-02)	Acc@1  98.44 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [35][290/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.6718e-02 (3.6936e-02)	Acc@1  99.22 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [35][300/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.0502e-02 (3.6929e-02)	Acc@1  99.22 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [35][310/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.8610e-02 (3.6912e-02)	Acc@1 100.00 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [35][320/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.9500e-02 (3.6711e-02)	Acc@1  99.22 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [35][330/391]	Time  0.132 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.0334e-02 (3.6577e-02)	Acc@1 100.00 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [35][340/391]	Time  0.116 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.7720e-02 (3.6487e-02)	Acc@1  98.44 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [35][350/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.1138e-02 (3.6495e-02)	Acc@1  99.22 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [35][360/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.9438e-02 (3.6641e-02)	Acc@1  98.44 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [35][370/391]	Time  0.127 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.0751e-02 (3.6473e-02)	Acc@1  99.22 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [35][380/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.9194e-02 (3.6289e-02)	Acc@1  98.44 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [35][390/391]	Time  0.107 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.3817e-02 (3.6355e-02)	Acc@1 100.00 ( 98.99)	Acc@5 100.00 (100.00)
## e[35] optimizer.zero_grad (sum) time: 0.6763391494750977
## e[35]       loss.backward (sum) time: 14.270897626876831
## e[35]      optimizer.step (sum) time: 8.027374982833862
## epoch[35] training(only) time: 46.91343331336975
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 1.7151e-01 (1.7151e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.062 ( 0.061)	Loss 2.1130e-01 (1.9536e-01)	Acc@1  95.00 ( 93.36)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.045 ( 0.054)	Loss 3.6060e-01 (2.2298e-01)	Acc@1  87.00 ( 92.62)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.045 ( 0.052)	Loss 2.7026e-01 (2.3150e-01)	Acc@1  92.00 ( 92.81)	Acc@5  99.00 ( 99.68)
Test: [ 40/100]	Time  0.048 ( 0.051)	Loss 2.5610e-01 (2.3394e-01)	Acc@1  90.00 ( 92.66)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.050 ( 0.050)	Loss 1.6174e-01 (2.3560e-01)	Acc@1  95.00 ( 92.67)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.054 ( 0.050)	Loss 1.9202e-01 (2.3053e-01)	Acc@1  98.00 ( 92.87)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 3.4888e-01 (2.2830e-01)	Acc@1  92.00 ( 92.97)	Acc@5 100.00 ( 99.80)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 1.6235e-01 (2.2983e-01)	Acc@1  94.00 ( 92.99)	Acc@5 100.00 ( 99.81)
Test: [ 90/100]	Time  0.045 ( 0.049)	Loss 2.1863e-01 (2.2819e-01)	Acc@1  94.00 ( 93.03)	Acc@5 100.00 ( 99.82)
 * Acc@1 93.090 Acc@5 99.840
### epoch[35] execution time: 51.87361717224121
EPOCH 36
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [36][  0/391]	Time  0.276 ( 0.276)	Data  0.138 ( 0.138)	Loss 5.0446e-02 (5.0446e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [36][ 10/391]	Time  0.117 ( 0.135)	Data  0.001 ( 0.013)	Loss 6.1279e-02 (2.9413e-02)	Acc@1  98.44 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [36][ 20/391]	Time  0.117 ( 0.127)	Data  0.001 ( 0.007)	Loss 2.5711e-02 (2.7652e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [36][ 30/391]	Time  0.117 ( 0.125)	Data  0.001 ( 0.005)	Loss 2.3697e-02 (2.9100e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [36][ 40/391]	Time  0.118 ( 0.124)	Data  0.001 ( 0.004)	Loss 1.7914e-02 (2.7661e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [36][ 50/391]	Time  0.122 ( 0.123)	Data  0.001 ( 0.004)	Loss 3.2471e-02 (3.0412e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [36][ 60/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.003)	Loss 2.9297e-02 (3.0079e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [36][ 70/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.003)	Loss 2.8687e-02 (3.0311e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [36][ 80/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.4342e-02 (3.0618e-02)	Acc@1  97.66 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [36][ 90/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.5778e-02 (2.9385e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [36][100/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.5549e-02 (3.0005e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [36][110/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.6133e-02 (3.0046e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [36][120/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.9821e-02 (3.0244e-02)	Acc@1  99.22 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [36][130/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.5085e-02 (3.0251e-02)	Acc@1  99.22 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [36][140/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.3477e-02 (3.1336e-02)	Acc@1  98.44 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [36][150/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.1921e-02 (3.1568e-02)	Acc@1  98.44 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [36][160/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.3224e-02 (3.1503e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [36][170/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.2725e-02 (3.2406e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [36][180/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.0344e-02 (3.2704e-02)	Acc@1  97.66 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [36][190/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.9755e-02 (3.2738e-02)	Acc@1  98.44 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [36][200/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.8265e-02 (3.2173e-02)	Acc@1 100.00 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [36][210/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.3365e-02 (3.2141e-02)	Acc@1  99.22 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [36][220/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.2733e-02 (3.2248e-02)	Acc@1 100.00 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [36][230/391]	Time  0.130 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.5369e-02 (3.2790e-02)	Acc@1  97.66 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [36][240/391]	Time  0.123 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.4332e-02 (3.2543e-02)	Acc@1  99.22 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [36][250/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.3254e-02 (3.2796e-02)	Acc@1  99.22 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [36][260/391]	Time  0.127 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.8448e-02 (3.2988e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [36][270/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.7180e-02 (3.3371e-02)	Acc@1  97.66 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [36][280/391]	Time  0.126 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.5345e-02 (3.3566e-02)	Acc@1  99.22 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [36][290/391]	Time  0.124 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.5481e-02 (3.3599e-02)	Acc@1  98.44 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [36][300/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.1595e-02 (3.3774e-02)	Acc@1  97.66 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [36][310/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.1392e-02 (3.3737e-02)	Acc@1  98.44 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [36][320/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.1027e-02 (3.3681e-02)	Acc@1 100.00 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [36][330/391]	Time  0.123 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.3361e-02 (3.3684e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [36][340/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.1392e-02 (3.4119e-02)	Acc@1  97.66 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [36][350/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.8295e-02 (3.4133e-02)	Acc@1 100.00 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [36][360/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.5650e-02 (3.3943e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [36][370/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.7603e-02 (3.3949e-02)	Acc@1  98.44 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [36][380/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.8116e-02 (3.3717e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [36][390/391]	Time  0.108 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.0497e-02 (3.3742e-02)	Acc@1  98.75 ( 99.02)	Acc@5 100.00 (100.00)
## e[36] optimizer.zero_grad (sum) time: 0.6845128536224365
## e[36]       loss.backward (sum) time: 14.221333742141724
## e[36]      optimizer.step (sum) time: 8.089189767837524
## epoch[36] training(only) time: 46.99923014640808
# Switched to evaluate mode...
Test: [  0/100]	Time  0.181 ( 0.181)	Loss 1.7493e-01 (1.7493e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.059)	Loss 2.0691e-01 (2.0198e-01)	Acc@1  96.00 ( 93.82)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.049 ( 0.054)	Loss 4.3042e-01 (2.3361e-01)	Acc@1  87.00 ( 92.95)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.046 ( 0.051)	Loss 2.5537e-01 (2.3795e-01)	Acc@1  93.00 ( 93.03)	Acc@5  99.00 ( 99.71)
Test: [ 40/100]	Time  0.048 ( 0.050)	Loss 3.0298e-01 (2.4101e-01)	Acc@1  89.00 ( 92.93)	Acc@5 100.00 ( 99.78)
Test: [ 50/100]	Time  0.049 ( 0.050)	Loss 1.5503e-01 (2.4168e-01)	Acc@1  94.00 ( 92.86)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 2.0117e-01 (2.3658e-01)	Acc@1  96.00 ( 92.98)	Acc@5 100.00 ( 99.82)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 3.3569e-01 (2.3471e-01)	Acc@1  92.00 ( 93.06)	Acc@5 100.00 ( 99.83)
Test: [ 80/100]	Time  0.049 ( 0.049)	Loss 1.5125e-01 (2.3648e-01)	Acc@1  93.00 ( 93.02)	Acc@5 100.00 ( 99.85)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 1.9482e-01 (2.3517e-01)	Acc@1  94.00 ( 93.03)	Acc@5 100.00 ( 99.87)
 * Acc@1 93.080 Acc@5 99.880
### epoch[36] execution time: 51.93435192108154
EPOCH 37
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [37][  0/391]	Time  0.293 ( 0.293)	Data  0.158 ( 0.158)	Loss 3.4760e-02 (3.4760e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [37][ 10/391]	Time  0.117 ( 0.135)	Data  0.001 ( 0.015)	Loss 3.2654e-02 (2.8712e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [37][ 20/391]	Time  0.120 ( 0.128)	Data  0.001 ( 0.008)	Loss 2.3056e-02 (2.9589e-02)	Acc@1  99.22 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [37][ 30/391]	Time  0.117 ( 0.125)	Data  0.001 ( 0.006)	Loss 2.8793e-02 (3.0272e-02)	Acc@1  98.44 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [37][ 40/391]	Time  0.121 ( 0.123)	Data  0.001 ( 0.005)	Loss 1.6754e-02 (3.1814e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [37][ 50/391]	Time  0.117 ( 0.123)	Data  0.001 ( 0.004)	Loss 3.5278e-02 (3.0198e-02)	Acc@1  99.22 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [37][ 60/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.004)	Loss 1.9043e-02 (2.9109e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [37][ 70/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 3.0487e-02 (2.8944e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [37][ 80/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.003)	Loss 2.4323e-02 (2.9117e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [37][ 90/391]	Time  0.127 ( 0.121)	Data  0.001 ( 0.003)	Loss 2.2934e-02 (2.8939e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [37][100/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.003)	Loss 4.7180e-02 (2.9795e-02)	Acc@1  98.44 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [37][110/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.4320e-02 (3.0063e-02)	Acc@1 100.00 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [37][120/391]	Time  0.127 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.8147e-02 (3.0449e-02)	Acc@1  98.44 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [37][130/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.4292e-02 (3.0905e-02)	Acc@1  99.22 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [37][140/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.8422e-02 (3.0789e-02)	Acc@1  98.44 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [37][150/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.4229e-02 (3.0955e-02)	Acc@1 100.00 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [37][160/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.6754e-02 (3.0370e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [37][170/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.1494e-02 (2.9998e-02)	Acc@1  97.66 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [37][180/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.6733e-02 (2.9743e-02)	Acc@1 100.00 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [37][190/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.2440e-02 (3.0067e-02)	Acc@1  98.44 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [37][200/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.6244e-02 (3.0446e-02)	Acc@1  97.66 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [37][210/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.7075e-02 (3.0695e-02)	Acc@1 100.00 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [37][220/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.9312e-02 (3.0589e-02)	Acc@1 100.00 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [37][230/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.5442e-02 (3.0458e-02)	Acc@1 100.00 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [37][240/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.7624e-02 (3.0234e-02)	Acc@1 100.00 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [37][250/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.1597e-02 (3.0177e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [37][260/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.2745e-02 (3.0235e-02)	Acc@1  98.44 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [37][270/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.6983e-02 (3.0159e-02)	Acc@1 100.00 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [37][280/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.6281e-02 (2.9966e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [37][290/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.9580e-03 (2.9786e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [37][300/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.9622e-02 (2.9759e-02)	Acc@1  98.44 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [37][310/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 7.0305e-03 (2.9672e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [37][320/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 7.1068e-03 (2.9713e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [37][330/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.9438e-02 (2.9859e-02)	Acc@1  98.44 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [37][340/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.5431e-02 (2.9823e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [37][350/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.8173e-02 (2.9936e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [37][360/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.1149e-02 (2.9925e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [37][370/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.5681e-02 (2.9854e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [37][380/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.5101e-02 (2.9793e-02)	Acc@1  99.22 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [37][390/391]	Time  0.110 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.1240e-02 (2.9842e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 (100.00)
## e[37] optimizer.zero_grad (sum) time: 0.6848912239074707
## e[37]       loss.backward (sum) time: 14.216386795043945
## e[37]      optimizer.step (sum) time: 8.031545400619507
## epoch[37] training(only) time: 46.89256715774536
# Switched to evaluate mode...
Test: [  0/100]	Time  0.173 ( 0.173)	Loss 1.6772e-01 (1.6772e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.059)	Loss 2.0923e-01 (1.9771e-01)	Acc@1  95.00 ( 93.64)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.060 ( 0.054)	Loss 3.5522e-01 (2.3034e-01)	Acc@1  87.00 ( 92.81)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 2.4048e-01 (2.4107e-01)	Acc@1  92.00 ( 92.90)	Acc@5  99.00 ( 99.68)
Test: [ 40/100]	Time  0.049 ( 0.051)	Loss 2.9541e-01 (2.4229e-01)	Acc@1  90.00 ( 92.80)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.5540e-01 (2.4239e-01)	Acc@1  95.00 ( 92.82)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.8750e-01 (2.3752e-01)	Acc@1  97.00 ( 92.92)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 3.7866e-01 (2.3481e-01)	Acc@1  90.00 ( 92.99)	Acc@5 100.00 ( 99.80)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 1.3721e-01 (2.3697e-01)	Acc@1  95.00 ( 93.02)	Acc@5 100.00 ( 99.81)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.9958e-01 (2.3521e-01)	Acc@1  94.00 ( 93.07)	Acc@5 100.00 ( 99.82)
 * Acc@1 93.120 Acc@5 99.830
### epoch[37] execution time: 51.84247946739197
EPOCH 38
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [38][  0/391]	Time  0.283 ( 0.283)	Data  0.148 ( 0.148)	Loss 3.0838e-02 (3.0838e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [38][ 10/391]	Time  0.119 ( 0.136)	Data  0.001 ( 0.014)	Loss 2.0859e-02 (2.4279e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [38][ 20/391]	Time  0.117 ( 0.127)	Data  0.001 ( 0.008)	Loss 1.8692e-02 (2.7374e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [38][ 30/391]	Time  0.117 ( 0.125)	Data  0.001 ( 0.006)	Loss 1.4374e-02 (2.9278e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [38][ 40/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.004)	Loss 2.9343e-02 (2.7845e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [38][ 50/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.004)	Loss 7.1945e-03 (2.7293e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [38][ 60/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.4626e-02 (2.6387e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [38][ 70/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.003)	Loss 2.3788e-02 (2.6379e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [38][ 80/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.003)	Loss 5.7861e-02 (2.6879e-02)	Acc@1  97.66 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [38][ 90/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.003)	Loss 2.5162e-02 (2.7933e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [38][100/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.3010e-02 (2.8645e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [38][110/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.6321e-02 (2.8398e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [38][120/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.5925e-02 (2.8689e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [38][130/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.1391e-02 (2.8466e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [38][140/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.0436e-02 (2.7706e-02)	Acc@1  98.44 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [38][150/391]	Time  0.115 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.6962e-02 (2.7663e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [38][160/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.8366e-02 (2.7682e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [38][170/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.9226e-02 (2.8334e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [38][180/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.9943e-02 (2.8447e-02)	Acc@1  99.22 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [38][190/391]	Time  0.125 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.8580e-02 (2.8521e-02)	Acc@1  99.22 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [38][200/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.9204e-02 (2.9063e-02)	Acc@1  98.44 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [38][210/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.8600e-02 (2.9148e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [38][220/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.3071e-02 (2.8966e-02)	Acc@1  99.22 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [38][230/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.6962e-02 (2.8734e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [38][240/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.5869e-02 (2.8432e-02)	Acc@1  99.22 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [38][250/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.9835e-02 (2.8345e-02)	Acc@1  98.44 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [38][260/391]	Time  0.128 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.2385e-02 (2.8457e-02)	Acc@1 100.00 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [38][270/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.2598e-02 (2.8394e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [38][280/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.0624e-02 (2.8510e-02)	Acc@1  98.44 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [38][290/391]	Time  0.128 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.8152e-02 (2.8677e-02)	Acc@1  98.44 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [38][300/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.3557e-02 (2.8499e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [38][310/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.0670e-02 (2.8649e-02)	Acc@1  98.44 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [38][320/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.9877e-02 (2.8858e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [38][330/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.0361e-02 (2.9049e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [38][340/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.9058e-02 (2.8954e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [38][350/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.6804e-02 (2.8958e-02)	Acc@1  98.44 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [38][360/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.9734e-02 (2.9060e-02)	Acc@1  98.44 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [38][370/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.3213e-02 (2.8974e-02)	Acc@1  97.66 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [38][380/391]	Time  0.116 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.3947e-02 (2.8915e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [38][390/391]	Time  0.107 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.1194e-02 (2.8850e-02)	Acc@1  98.75 ( 99.18)	Acc@5 100.00 (100.00)
## e[38] optimizer.zero_grad (sum) time: 0.6805360317230225
## e[38]       loss.backward (sum) time: 14.218748569488525
## e[38]      optimizer.step (sum) time: 8.016664505004883
## epoch[38] training(only) time: 46.85719847679138
# Switched to evaluate mode...
Test: [  0/100]	Time  0.180 ( 0.180)	Loss 1.6797e-01 (1.6797e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.060)	Loss 2.1399e-01 (2.1052e-01)	Acc@1  96.00 ( 93.73)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 4.4067e-01 (2.4003e-01)	Acc@1  89.00 ( 92.76)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 2.3889e-01 (2.4350e-01)	Acc@1  92.00 ( 92.81)	Acc@5  99.00 ( 99.71)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 2.7466e-01 (2.4172e-01)	Acc@1  89.00 ( 92.71)	Acc@5 100.00 ( 99.78)
Test: [ 50/100]	Time  0.045 ( 0.050)	Loss 1.4502e-01 (2.4364e-01)	Acc@1  93.00 ( 92.67)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.9629e-01 (2.3897e-01)	Acc@1  95.00 ( 92.85)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 3.4253e-01 (2.3612e-01)	Acc@1  91.00 ( 93.00)	Acc@5 100.00 ( 99.80)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.6821e-01 (2.3766e-01)	Acc@1  95.00 ( 93.04)	Acc@5 100.00 ( 99.81)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 2.5732e-01 (2.3622e-01)	Acc@1  94.00 ( 93.05)	Acc@5 100.00 ( 99.82)
 * Acc@1 93.120 Acc@5 99.840
### epoch[38] execution time: 51.79185104370117
EPOCH 39
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [39][  0/391]	Time  0.282 ( 0.282)	Data  0.149 ( 0.149)	Loss 3.8849e-02 (3.8849e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [39][ 10/391]	Time  0.117 ( 0.135)	Data  0.001 ( 0.014)	Loss 2.8458e-02 (2.4234e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [39][ 20/391]	Time  0.119 ( 0.128)	Data  0.001 ( 0.008)	Loss 2.4323e-02 (2.4027e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [39][ 30/391]	Time  0.118 ( 0.125)	Data  0.001 ( 0.006)	Loss 3.6133e-02 (2.6872e-02)	Acc@1  98.44 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [39][ 40/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.005)	Loss 4.7546e-02 (2.5470e-02)	Acc@1  97.66 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [39][ 50/391]	Time  0.117 ( 0.123)	Data  0.001 ( 0.004)	Loss 4.5288e-02 (2.4981e-02)	Acc@1  98.44 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [39][ 60/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.6403e-02 (2.4718e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [39][ 70/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 2.1271e-02 (2.4713e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [39][ 80/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.003)	Loss 3.6957e-02 (2.4878e-02)	Acc@1  98.44 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [39][ 90/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.003)	Loss 7.1106e-03 (2.5127e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [39][100/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.3987e-02 (2.5094e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [39][110/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.3084e-03 (2.5966e-02)	Acc@1 100.00 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [39][120/391]	Time  0.129 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.7527e-02 (2.5960e-02)	Acc@1  98.44 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [39][130/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.2379e-02 (2.5956e-02)	Acc@1  98.44 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [39][140/391]	Time  0.126 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.8387e-02 (2.6129e-02)	Acc@1  99.22 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [39][150/391]	Time  0.128 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.6093e-02 (2.5890e-02)	Acc@1  99.22 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [39][160/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.3661e-02 (2.5558e-02)	Acc@1  98.44 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [39][170/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.2267e-02 (2.5846e-02)	Acc@1  98.44 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [39][180/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.9042e-02 (2.6008e-02)	Acc@1  96.88 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [39][190/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.8208e-02 (2.6005e-02)	Acc@1  98.44 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [39][200/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.3743e-02 (2.5941e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [39][210/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.2755e-02 (2.5692e-02)	Acc@1  97.66 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [39][220/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.8951e-02 (2.5663e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [39][230/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.5217e-02 (2.5530e-02)	Acc@1  98.44 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [39][240/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.4545e-03 (2.5300e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [39][250/391]	Time  0.123 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.7546e-02 (2.5523e-02)	Acc@1  98.44 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [39][260/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.0624e-02 (2.5640e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [39][270/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.2430e-02 (2.5759e-02)	Acc@1  98.44 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [39][280/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.8311e-02 (2.5711e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [39][290/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.1982e-02 (2.5594e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [39][300/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.3016e-02 (2.5404e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [39][310/391]	Time  0.116 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.3634e-02 (2.5433e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [39][320/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.6367e-02 (2.5478e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [39][330/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.4271e-02 (2.5416e-02)	Acc@1  97.66 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [39][340/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.7863e-02 (2.5508e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [39][350/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.3092e-02 (2.5698e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [39][360/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.2776e-02 (2.5838e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [39][370/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.4282e-02 (2.5869e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [39][380/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.0574e-02 (2.5832e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [39][390/391]	Time  0.105 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.8208e-02 (2.5847e-02)	Acc@1  98.75 ( 99.30)	Acc@5 100.00 (100.00)
## e[39] optimizer.zero_grad (sum) time: 0.6828868389129639
## e[39]       loss.backward (sum) time: 14.28056025505066
## e[39]      optimizer.step (sum) time: 8.042053937911987
## epoch[39] training(only) time: 47.01278901100159
# Switched to evaluate mode...
Test: [  0/100]	Time  0.181 ( 0.181)	Loss 1.6516e-01 (1.6516e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.059)	Loss 2.2266e-01 (1.9546e-01)	Acc@1  96.00 ( 94.18)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.048 ( 0.054)	Loss 3.6401e-01 (2.3004e-01)	Acc@1  88.00 ( 93.14)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.045 ( 0.052)	Loss 2.5708e-01 (2.3857e-01)	Acc@1  93.00 ( 93.16)	Acc@5  99.00 ( 99.68)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 2.8857e-01 (2.3987e-01)	Acc@1  88.00 ( 93.10)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.045 ( 0.050)	Loss 1.3306e-01 (2.4052e-01)	Acc@1  96.00 ( 93.04)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.045 ( 0.049)	Loss 1.8103e-01 (2.3660e-01)	Acc@1  97.00 ( 93.21)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 3.8379e-01 (2.3542e-01)	Acc@1  91.00 ( 93.20)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.5466e-01 (2.3898e-01)	Acc@1  93.00 ( 93.15)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 2.3364e-01 (2.3701e-01)	Acc@1  95.00 ( 93.22)	Acc@5 100.00 ( 99.79)
 * Acc@1 93.250 Acc@5 99.800
### epoch[39] execution time: 51.98399305343628
EPOCH 40
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [40][  0/391]	Time  0.275 ( 0.275)	Data  0.147 ( 0.147)	Loss 2.0081e-02 (2.0081e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [40][ 10/391]	Time  0.131 ( 0.134)	Data  0.001 ( 0.014)	Loss 1.3374e-02 (2.1579e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [40][ 20/391]	Time  0.120 ( 0.127)	Data  0.001 ( 0.008)	Loss 3.2562e-02 (2.2832e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [40][ 30/391]	Time  0.121 ( 0.124)	Data  0.001 ( 0.006)	Loss 4.0070e-02 (2.3249e-02)	Acc@1  98.44 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [40][ 40/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.005)	Loss 8.8959e-03 (2.5360e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [40][ 50/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.004)	Loss 4.0314e-02 (2.5316e-02)	Acc@1  98.44 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [40][ 60/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 9.8648e-03 (2.4579e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [40][ 70/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 3.1982e-02 (2.5859e-02)	Acc@1  99.22 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [40][ 80/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.003)	Loss 2.2919e-02 (2.5568e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [40][ 90/391]	Time  0.126 ( 0.121)	Data  0.001 ( 0.003)	Loss 2.0340e-02 (2.5094e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [40][100/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.2054e-02 (2.4979e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [40][110/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.6800e-02 (2.4948e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [40][120/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.4576e-02 (2.5272e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [40][130/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.6205e-02 (2.5137e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [40][140/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.2562e-02 (2.4687e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [40][150/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.2797e-02 (2.4518e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [40][160/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.5839e-02 (2.4802e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [40][170/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.3161e-02 (2.4806e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [40][180/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.2683e-02 (2.5025e-02)	Acc@1  98.44 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [40][190/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.1856e-02 (2.5388e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [40][200/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.0508e-02 (2.5239e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [40][210/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.1382e-02 (2.5300e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [40][220/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.8868e-02 (2.5385e-02)	Acc@1  97.66 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [40][230/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.8417e-02 (2.5291e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [40][240/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.4216e-02 (2.5008e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [40][250/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.6215e-02 (2.5072e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [40][260/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.6133e-02 (2.5001e-02)	Acc@1  98.44 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [40][270/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.9531e-02 (2.5358e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [40][280/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.1330e-02 (2.5132e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [40][290/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.2714e-03 (2.5172e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [40][300/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.4180e-02 (2.5236e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [40][310/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.4436e-03 (2.5053e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [40][320/391]	Time  0.125 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.1948e-02 (2.4928e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [40][330/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.2703e-02 (2.4885e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [40][340/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.4719e-02 (2.4898e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [40][350/391]	Time  0.125 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.8478e-02 (2.4916e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [40][360/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.6800e-02 (2.4927e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [40][370/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.1002e-02 (2.5223e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [40][380/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.8976e-02 (2.5172e-02)	Acc@1  98.44 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [40][390/391]	Time  0.103 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.5976e-02 (2.5149e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
## e[40] optimizer.zero_grad (sum) time: 0.6846113204956055
## e[40]       loss.backward (sum) time: 14.191059350967407
## e[40]      optimizer.step (sum) time: 8.02141284942627
## epoch[40] training(only) time: 46.895283222198486
# Switched to evaluate mode...
Test: [  0/100]	Time  0.180 ( 0.180)	Loss 1.7834e-01 (1.7834e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.059)	Loss 2.1179e-01 (2.0158e-01)	Acc@1  97.00 ( 93.73)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.058 ( 0.054)	Loss 3.7793e-01 (2.3459e-01)	Acc@1  87.00 ( 93.05)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 2.7612e-01 (2.4520e-01)	Acc@1  93.00 ( 93.10)	Acc@5  99.00 ( 99.71)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 2.7344e-01 (2.4535e-01)	Acc@1  88.00 ( 92.88)	Acc@5 100.00 ( 99.76)
Test: [ 50/100]	Time  0.049 ( 0.050)	Loss 1.3403e-01 (2.4629e-01)	Acc@1  95.00 ( 92.86)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.9299e-01 (2.4182e-01)	Acc@1  98.00 ( 93.05)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.054 ( 0.049)	Loss 3.3618e-01 (2.3760e-01)	Acc@1  92.00 ( 93.13)	Acc@5 100.00 ( 99.80)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 1.7615e-01 (2.4068e-01)	Acc@1  94.00 ( 93.12)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 2.6123e-01 (2.3720e-01)	Acc@1  95.00 ( 93.30)	Acc@5 100.00 ( 99.81)
 * Acc@1 93.320 Acc@5 99.830
### epoch[40] execution time: 51.80867886543274
EPOCH 41
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [41][  0/391]	Time  0.287 ( 0.287)	Data  0.148 ( 0.148)	Loss 4.3060e-02 (4.3060e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [41][ 10/391]	Time  0.121 ( 0.135)	Data  0.001 ( 0.014)	Loss 2.4567e-02 (2.3263e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [41][ 20/391]	Time  0.117 ( 0.128)	Data  0.001 ( 0.008)	Loss 2.0798e-02 (2.1699e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [41][ 30/391]	Time  0.117 ( 0.125)	Data  0.001 ( 0.006)	Loss 1.3107e-02 (2.1328e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [41][ 40/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.005)	Loss 9.2316e-03 (2.0805e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [41][ 50/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.004)	Loss 2.5284e-02 (2.3130e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [41][ 60/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.004)	Loss 1.6098e-02 (2.3303e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [41][ 70/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.6922e-02 (2.2098e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [41][ 80/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.003)	Loss 3.1494e-02 (2.2326e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [41][ 90/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.003)	Loss 3.4760e-02 (2.2960e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [41][100/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.003)	Loss 1.7090e-02 (2.2916e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [41][110/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.7323e-02 (2.3181e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [41][120/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.8295e-02 (2.3455e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [41][130/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.2787e-02 (2.3324e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [41][140/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.0386e-02 (2.3172e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [41][150/391]	Time  0.131 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.7054e-02 (2.3055e-02)	Acc@1  98.44 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [41][160/391]	Time  0.123 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.1992e-02 (2.3012e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [41][170/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.1088e-02 (2.3132e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [41][180/391]	Time  0.133 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.0782e-02 (2.3197e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [41][190/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.0994e-02 (2.2892e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [41][200/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.3325e-02 (2.3256e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [41][210/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.3458e-02 (2.3087e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [41][220/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.9846e-02 (2.3486e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [41][230/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.1421e-02 (2.3570e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [41][240/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 9.8953e-03 (2.3489e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [41][250/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.3435e-02 (2.3365e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [41][260/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.8290e-02 (2.3187e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [41][270/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.6708e-02 (2.3070e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [41][280/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.9073e-02 (2.3163e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [41][290/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.5574e-02 (2.3218e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [41][300/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.2551e-02 (2.3463e-02)	Acc@1  97.66 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [41][310/391]	Time  0.116 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.0025e-02 (2.3362e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [41][320/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.6785e-02 (2.3588e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [41][330/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.0354e-02 (2.3516e-02)	Acc@1  97.66 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [41][340/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.9135e-02 (2.3699e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [41][350/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.2943e-03 (2.3659e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [41][360/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 9.7198e-03 (2.3715e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [41][370/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.4404e-02 (2.3432e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [41][380/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.0048e-02 (2.3562e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [41][390/391]	Time  0.104 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.2978e-02 (2.3456e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
## e[41] optimizer.zero_grad (sum) time: 0.6886327266693115
## e[41]       loss.backward (sum) time: 14.19656229019165
## e[41]      optimizer.step (sum) time: 8.033600807189941
## epoch[41] training(only) time: 46.971956729888916
# Switched to evaluate mode...
Test: [  0/100]	Time  0.181 ( 0.181)	Loss 1.4661e-01 (1.4661e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.058)	Loss 2.1973e-01 (1.9866e-01)	Acc@1  97.00 ( 94.36)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.045 ( 0.053)	Loss 3.8232e-01 (2.3112e-01)	Acc@1  89.00 ( 93.29)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.046 ( 0.051)	Loss 2.6929e-01 (2.4322e-01)	Acc@1  93.00 ( 93.13)	Acc@5  99.00 ( 99.71)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 2.6660e-01 (2.4158e-01)	Acc@1  89.00 ( 92.95)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.5381e-01 (2.4537e-01)	Acc@1  94.00 ( 92.86)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.9128e-01 (2.4062e-01)	Acc@1  97.00 ( 92.98)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.045 ( 0.049)	Loss 3.7939e-01 (2.3694e-01)	Acc@1  91.00 ( 93.14)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.7224e-01 (2.4094e-01)	Acc@1  93.00 ( 93.07)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.050 ( 0.049)	Loss 2.1753e-01 (2.3830e-01)	Acc@1  95.00 ( 93.15)	Acc@5 100.00 ( 99.81)
 * Acc@1 93.210 Acc@5 99.830
### epoch[41] execution time: 51.94177484512329
EPOCH 42
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [42][  0/391]	Time  0.267 ( 0.267)	Data  0.139 ( 0.139)	Loss 2.7573e-02 (2.7573e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [42][ 10/391]	Time  0.145 ( 0.134)	Data  0.001 ( 0.013)	Loss 1.1635e-02 (2.0398e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [42][ 20/391]	Time  0.118 ( 0.127)	Data  0.001 ( 0.007)	Loss 1.7944e-02 (1.9120e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [42][ 30/391]	Time  0.117 ( 0.124)	Data  0.001 ( 0.005)	Loss 2.2125e-02 (2.0298e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [42][ 40/391]	Time  0.125 ( 0.123)	Data  0.001 ( 0.004)	Loss 1.5091e-02 (1.9858e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [42][ 50/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.004)	Loss 2.8397e-02 (2.1210e-02)	Acc@1  98.44 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [42][ 60/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.2283e-02 (2.1218e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [42][ 70/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 3.3234e-02 (2.0959e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [42][ 80/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.003)	Loss 1.6388e-02 (2.0420e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [42][ 90/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.003)	Loss 3.9101e-03 (1.9781e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [42][100/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.8143e-02 (1.9730e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [42][110/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.6154e-02 (1.9383e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [42][120/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.8610e-02 (1.9894e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [42][130/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0254e-02 (1.9814e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [42][140/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.6321e-02 (1.9709e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [42][150/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.0839e-03 (1.9888e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [42][160/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.7477e-03 (1.9985e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [42][170/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.5915e-02 (2.0025e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [42][180/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.1208e-02 (1.9827e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [42][190/391]	Time  0.123 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.1168e-02 (2.0182e-02)	Acc@1  98.44 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [42][200/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.3427e-02 (2.0610e-02)	Acc@1  97.66 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [42][210/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.4918e-02 (2.0648e-02)	Acc@1  98.44 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [42][220/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.3234e-02 (2.0631e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [42][230/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.0551e-02 (2.0553e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [42][240/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.3794e-02 (2.0688e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [42][250/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.8555e-02 (2.0642e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [42][260/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 8.6670e-03 (2.0630e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [42][270/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.2962e-02 (2.0521e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [42][280/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.7603e-02 (2.0708e-02)	Acc@1  98.44 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [42][290/391]	Time  0.122 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.0902e-02 (2.0704e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [42][300/391]	Time  0.125 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.0016e-03 (2.0780e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [42][310/391]	Time  0.122 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.0039e-02 (2.0775e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [42][320/391]	Time  0.116 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.7985e-02 (2.0745e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [42][330/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.6479e-02 (2.0847e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [42][340/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.2375e-02 (2.0921e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [42][350/391]	Time  0.129 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.7100e-02 (2.1105e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [42][360/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.3763e-02 (2.1154e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [42][370/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.2023e-02 (2.1150e-02)	Acc@1  98.44 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [42][380/391]	Time  0.126 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.0278e-03 (2.1086e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [42][390/391]	Time  0.112 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.8524e-02 (2.1160e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
## e[42] optimizer.zero_grad (sum) time: 0.6821815967559814
## e[42]       loss.backward (sum) time: 14.247630596160889
## e[42]      optimizer.step (sum) time: 8.111274242401123
## epoch[42] training(only) time: 47.03294849395752
# Switched to evaluate mode...
Test: [  0/100]	Time  0.177 ( 0.177)	Loss 1.6016e-01 (1.6016e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.045 ( 0.060)	Loss 2.1606e-01 (2.0581e-01)	Acc@1  97.00 ( 93.55)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.049 ( 0.054)	Loss 4.1138e-01 (2.3840e-01)	Acc@1  88.00 ( 92.81)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.047 ( 0.051)	Loss 2.7905e-01 (2.4600e-01)	Acc@1  91.00 ( 92.81)	Acc@5  99.00 ( 99.68)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 2.9028e-01 (2.4592e-01)	Acc@1  88.00 ( 92.80)	Acc@5 100.00 ( 99.76)
Test: [ 50/100]	Time  0.049 ( 0.050)	Loss 1.4539e-01 (2.4883e-01)	Acc@1  93.00 ( 92.76)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.045 ( 0.049)	Loss 1.9214e-01 (2.4355e-01)	Acc@1  98.00 ( 92.98)	Acc@5 100.00 ( 99.82)
Test: [ 70/100]	Time  0.060 ( 0.049)	Loss 3.9600e-01 (2.4167e-01)	Acc@1  92.00 ( 93.04)	Acc@5 100.00 ( 99.83)
Test: [ 80/100]	Time  0.049 ( 0.049)	Loss 1.3354e-01 (2.4500e-01)	Acc@1  94.00 ( 93.00)	Acc@5 100.00 ( 99.83)
Test: [ 90/100]	Time  0.048 ( 0.049)	Loss 2.2888e-01 (2.4312e-01)	Acc@1  94.00 ( 93.11)	Acc@5 100.00 ( 99.84)
 * Acc@1 93.170 Acc@5 99.850
### epoch[42] execution time: 51.939377784729004
EPOCH 43
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [43][  0/391]	Time  0.281 ( 0.281)	Data  0.151 ( 0.151)	Loss 3.5980e-02 (3.5980e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [43][ 10/391]	Time  0.118 ( 0.134)	Data  0.001 ( 0.015)	Loss 7.7858e-03 (1.8470e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [43][ 20/391]	Time  0.120 ( 0.127)	Data  0.001 ( 0.008)	Loss 3.3600e-02 (2.0047e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [43][ 30/391]	Time  0.119 ( 0.125)	Data  0.001 ( 0.006)	Loss 3.3234e-02 (2.0646e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [43][ 40/391]	Time  0.120 ( 0.124)	Data  0.001 ( 0.005)	Loss 2.1957e-02 (2.1384e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [43][ 50/391]	Time  0.116 ( 0.123)	Data  0.001 ( 0.004)	Loss 3.0121e-02 (2.2084e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [43][ 60/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.3145e-02 (2.0973e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [43][ 70/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.0208e-02 (2.1096e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [43][ 80/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 2.3270e-02 (2.2019e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [43][ 90/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.003)	Loss 3.3836e-03 (2.1595e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [43][100/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.9653e-02 (2.1346e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [43][110/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.9419e-02 (2.1896e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [43][120/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.2980e-02 (2.1508e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [43][130/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0139e-02 (2.1796e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [43][140/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.1408e-02 (2.1461e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [43][150/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.8417e-02 (2.1097e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [43][160/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.6739e-02 (2.0907e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [43][170/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.1215e-02 (2.0527e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [43][180/391]	Time  0.127 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.1131e-02 (2.0480e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [43][190/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.7191e-02 (2.0507e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [43][200/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.5651e-03 (2.0238e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [43][210/391]	Time  0.125 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.9046e-03 (2.0131e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [43][220/391]	Time  0.123 ( 0.120)	Data  0.001 ( 0.002)	Loss 7.4005e-03 (1.9893e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [43][230/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.7563e-02 (1.9809e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [43][240/391]	Time  0.131 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.2062e-02 (1.9663e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [43][250/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.5360e-02 (1.9538e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [43][260/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.1713e-02 (1.9397e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [43][270/391]	Time  0.116 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.9084e-03 (1.9329e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [43][280/391]	Time  0.116 ( 0.120)	Data  0.001 ( 0.002)	Loss 7.3929e-03 (1.9466e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [43][290/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.4454e-02 (1.9455e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [43][300/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.2512e-02 (1.9453e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [43][310/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 8.7128e-03 (1.9417e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [43][320/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.8951e-02 (1.9363e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [43][330/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.7776e-02 (1.9288e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [43][340/391]	Time  0.132 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.3579e-02 (1.9408e-02)	Acc@1  98.44 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [43][350/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.1851e-02 (1.9309e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [43][360/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.0432e-02 (1.9275e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [43][370/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.6434e-02 (1.9240e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [43][380/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.1088e-02 (1.9350e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [43][390/391]	Time  0.108 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.0243e-02 (1.9473e-02)	Acc@1  98.75 ( 99.52)	Acc@5 100.00 (100.00)
## e[43] optimizer.zero_grad (sum) time: 0.6797049045562744
## e[43]       loss.backward (sum) time: 14.267150640487671
## e[43]      optimizer.step (sum) time: 8.020586013793945
## epoch[43] training(only) time: 47.03620767593384
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 1.9543e-01 (1.9543e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.050 ( 0.060)	Loss 2.1521e-01 (2.1296e-01)	Acc@1  96.00 ( 93.36)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 3.5547e-01 (2.3967e-01)	Acc@1  89.00 ( 92.67)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 2.6245e-01 (2.4624e-01)	Acc@1  93.00 ( 92.71)	Acc@5  99.00 ( 99.71)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 3.1616e-01 (2.4903e-01)	Acc@1  88.00 ( 92.66)	Acc@5 100.00 ( 99.78)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.5027e-01 (2.5054e-01)	Acc@1  95.00 ( 92.65)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.9226e-01 (2.4582e-01)	Acc@1  97.00 ( 92.85)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 3.6816e-01 (2.4309e-01)	Acc@1  92.00 ( 92.90)	Acc@5 100.00 ( 99.82)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.5552e-01 (2.4706e-01)	Acc@1  94.00 ( 92.90)	Acc@5 100.00 ( 99.81)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 2.4939e-01 (2.4507e-01)	Acc@1  94.00 ( 92.97)	Acc@5 100.00 ( 99.81)
 * Acc@1 93.050 Acc@5 99.830
### epoch[43] execution time: 51.98226714134216
EPOCH 44
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [44][  0/391]	Time  0.285 ( 0.285)	Data  0.156 ( 0.156)	Loss 4.0741e-02 (4.0741e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [44][ 10/391]	Time  0.121 ( 0.134)	Data  0.001 ( 0.015)	Loss 1.4328e-02 (1.9525e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [44][ 20/391]	Time  0.120 ( 0.127)	Data  0.001 ( 0.008)	Loss 1.4763e-02 (2.1872e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [44][ 30/391]	Time  0.118 ( 0.125)	Data  0.001 ( 0.006)	Loss 5.6458e-03 (2.0832e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [44][ 40/391]	Time  0.128 ( 0.124)	Data  0.001 ( 0.005)	Loss 6.7482e-03 (2.0063e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [44][ 50/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.004)	Loss 2.2583e-02 (2.0734e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [44][ 60/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.004)	Loss 5.5733e-03 (1.9495e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [44][ 70/391]	Time  0.139 ( 0.122)	Data  0.001 ( 0.003)	Loss 2.9251e-02 (1.9636e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [44][ 80/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.1589e-02 (1.9126e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [44][ 90/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 2.2446e-02 (1.9009e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [44][100/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.003)	Loss 9.4299e-03 (1.9105e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [44][110/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.7227e-02 (1.9081e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [44][120/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.2471e-02 (1.9387e-02)	Acc@1  98.44 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [44][130/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.3783e-02 (1.9508e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [44][140/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.1971e-02 (1.9781e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [44][150/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.5900e-02 (1.9684e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [44][160/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.2076e-03 (1.9489e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [44][170/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.7811e-02 (1.9909e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [44][180/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0246e-02 (1.9656e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [44][190/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.3977e-02 (1.9623e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [44][200/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.8188e-02 (1.9595e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [44][210/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.5282e-02 (1.9487e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [44][220/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.7878e-02 (1.9777e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [44][230/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.4114e-02 (1.9950e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [44][240/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.1982e-02 (2.0003e-02)	Acc@1  98.44 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [44][250/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.0497e-02 (1.9926e-02)	Acc@1  98.44 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [44][260/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.7960e-02 (1.9908e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [44][270/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.5594e-02 (2.0106e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [44][280/391]	Time  0.122 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.0353e-02 (1.9851e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [44][290/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.7252e-02 (1.9718e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [44][300/391]	Time  0.122 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.2288e-02 (1.9698e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [44][310/391]	Time  0.122 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.4236e-02 (1.9796e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [44][320/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 8.3160e-03 (1.9902e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [44][330/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.8359e-03 (1.9787e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [44][340/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.2459e-02 (1.9845e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [44][350/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.5175e-02 (1.9858e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [44][360/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 7.7362e-03 (2.0046e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [44][370/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 7.6408e-03 (2.0069e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [44][380/391]	Time  0.130 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.4201e-03 (1.9929e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [44][390/391]	Time  0.110 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.8890e-02 (1.9840e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
## e[44] optimizer.zero_grad (sum) time: 0.688068151473999
## e[44]       loss.backward (sum) time: 14.252746343612671
## e[44]      optimizer.step (sum) time: 8.084816694259644
## epoch[44] training(only) time: 47.10689830780029
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 1.9055e-01 (1.9055e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 2.2803e-01 (2.0755e-01)	Acc@1  97.00 ( 94.45)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.045 ( 0.054)	Loss 3.7988e-01 (2.3827e-01)	Acc@1  88.00 ( 93.52)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 2.8271e-01 (2.4613e-01)	Acc@1  92.00 ( 93.45)	Acc@5  99.00 ( 99.71)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 3.1421e-01 (2.4830e-01)	Acc@1  88.00 ( 93.27)	Acc@5 100.00 ( 99.76)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.5845e-01 (2.5079e-01)	Acc@1  95.00 ( 93.10)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.8140e-01 (2.4637e-01)	Acc@1  97.00 ( 93.15)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 3.9795e-01 (2.4412e-01)	Acc@1  92.00 ( 93.18)	Acc@5 100.00 ( 99.82)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.6943e-01 (2.4781e-01)	Acc@1  94.00 ( 93.17)	Acc@5 100.00 ( 99.83)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 2.2083e-01 (2.4555e-01)	Acc@1  94.00 ( 93.22)	Acc@5 100.00 ( 99.82)
 * Acc@1 93.310 Acc@5 99.840
### epoch[44] execution time: 52.05746650695801
EPOCH 45
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [45][  0/391]	Time  0.276 ( 0.276)	Data  0.144 ( 0.144)	Loss 1.4633e-02 (1.4633e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [45][ 10/391]	Time  0.119 ( 0.134)	Data  0.001 ( 0.014)	Loss 8.2703e-03 (1.6129e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [45][ 20/391]	Time  0.117 ( 0.127)	Data  0.001 ( 0.008)	Loss 8.9951e-03 (1.7361e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [45][ 30/391]	Time  0.119 ( 0.124)	Data  0.001 ( 0.006)	Loss 1.7578e-02 (1.8636e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [45][ 40/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.005)	Loss 5.7144e-03 (1.8557e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [45][ 50/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.004)	Loss 9.6893e-03 (1.8119e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [45][ 60/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.3641e-02 (1.8259e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [45][ 70/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.003)	Loss 1.5610e-02 (1.8318e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [45][ 80/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.003)	Loss 1.2741e-02 (1.7930e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [45][ 90/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.003)	Loss 9.9564e-03 (1.7716e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [45][100/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.3611e-02 (1.7993e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [45][110/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.4000e-03 (1.7803e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [45][120/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0048e-02 (1.7511e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [45][130/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.9188e-03 (1.7273e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [45][140/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.7029e-02 (1.7091e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [45][150/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.8951e-02 (1.7121e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [45][160/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.8234e-02 (1.7275e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [45][170/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.1070e-02 (1.7468e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [45][180/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.4658e-02 (1.7770e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [45][190/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.7384e-03 (1.8101e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [45][200/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.8982e-02 (1.8633e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [45][210/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.4023e-02 (1.8953e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [45][220/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.2003e-02 (1.9023e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [45][230/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.4912e-02 (1.9022e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [45][240/391]	Time  0.126 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.4992e-02 (1.8965e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [45][250/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.2430e-02 (1.8896e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [45][260/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.1347e-02 (1.8968e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [45][270/391]	Time  0.127 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.0935e-02 (1.8999e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [45][280/391]	Time  0.123 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.2878e-02 (1.8827e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [45][290/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.2566e-02 (1.8834e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [45][300/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.6083e-02 (1.8753e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [45][310/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.0170e-02 (1.8551e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [45][320/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.9129e-02 (1.8420e-02)	Acc@1  98.44 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [45][330/391]	Time  0.134 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.3956e-02 (1.8378e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [45][340/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.4474e-02 (1.8477e-02)	Acc@1  98.44 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [45][350/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.5280e-03 (1.8497e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [45][360/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.8889e-02 (1.8620e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [45][370/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 8.1711e-03 (1.8470e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [45][380/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.5427e-02 (1.8473e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [45][390/391]	Time  0.111 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.3386e-02 (1.8476e-02)	Acc@1  98.75 ( 99.55)	Acc@5 100.00 (100.00)
## e[45] optimizer.zero_grad (sum) time: 0.682819128036499
## e[45]       loss.backward (sum) time: 14.246021270751953
## e[45]      optimizer.step (sum) time: 8.02816915512085
## epoch[45] training(only) time: 47.05215525627136
# Switched to evaluate mode...
Test: [  0/100]	Time  0.190 ( 0.190)	Loss 1.6003e-01 (1.6003e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.059)	Loss 2.4475e-01 (2.0435e-01)	Acc@1  95.00 ( 93.82)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 3.7769e-01 (2.3796e-01)	Acc@1  88.00 ( 92.95)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.057 ( 0.052)	Loss 2.7075e-01 (2.4710e-01)	Acc@1  92.00 ( 92.97)	Acc@5  99.00 ( 99.71)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 2.7441e-01 (2.4657e-01)	Acc@1  89.00 ( 92.88)	Acc@5 100.00 ( 99.76)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.4062e-01 (2.4983e-01)	Acc@1  97.00 ( 92.88)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.8164e-01 (2.4508e-01)	Acc@1  97.00 ( 93.05)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.049 ( 0.049)	Loss 4.1821e-01 (2.4338e-01)	Acc@1  91.00 ( 93.08)	Acc@5 100.00 ( 99.82)
Test: [ 80/100]	Time  0.055 ( 0.049)	Loss 1.3782e-01 (2.4571e-01)	Acc@1  94.00 ( 93.09)	Acc@5 100.00 ( 99.84)
Test: [ 90/100]	Time  0.050 ( 0.049)	Loss 2.3926e-01 (2.4373e-01)	Acc@1  95.00 ( 93.14)	Acc@5 100.00 ( 99.84)
 * Acc@1 93.210 Acc@5 99.850
### epoch[45] execution time: 52.01306509971619
EPOCH 46
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [46][  0/391]	Time  0.272 ( 0.272)	Data  0.133 ( 0.133)	Loss 1.1009e-02 (1.1009e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [46][ 10/391]	Time  0.120 ( 0.133)	Data  0.001 ( 0.013)	Loss 8.1711e-03 (1.8492e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [46][ 20/391]	Time  0.118 ( 0.127)	Data  0.001 ( 0.007)	Loss 1.5160e-02 (1.8606e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [46][ 30/391]	Time  0.117 ( 0.124)	Data  0.001 ( 0.005)	Loss 4.7607e-03 (1.6934e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [46][ 40/391]	Time  0.117 ( 0.123)	Data  0.001 ( 0.004)	Loss 1.7700e-02 (1.6291e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [46][ 50/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.004)	Loss 1.9409e-02 (1.6163e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [46][ 60/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 3.8300e-02 (1.6399e-02)	Acc@1  98.44 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [46][ 70/391]	Time  0.127 ( 0.122)	Data  0.001 ( 0.003)	Loss 7.7782e-03 (1.6102e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [46][ 80/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.003)	Loss 1.0071e-02 (1.6200e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [46][ 90/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0414e-02 (1.6289e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [46][100/391]	Time  0.134 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.0185e-03 (1.6082e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [46][110/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.2842e-02 (1.5965e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [46][120/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.3832e-02 (1.5840e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [46][130/391]	Time  0.127 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.1068e-03 (1.5703e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [46][140/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.6722e-02 (1.6159e-02)	Acc@1  98.44 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [46][150/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.1391e-02 (1.6482e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [46][160/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.0190e-03 (1.6471e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [46][170/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.5152e-02 (1.6286e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [46][180/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.2772e-02 (1.6527e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [46][190/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 9.4757e-03 (1.6522e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [46][200/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 8.8959e-03 (1.6267e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [46][210/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.1003e-03 (1.6376e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [46][220/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.3911e-02 (1.6296e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [46][230/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 9.8419e-03 (1.6425e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [46][240/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.9398e-02 (1.6630e-02)	Acc@1  98.44 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [46][250/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.7466e-02 (1.6504e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [46][260/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.9073e-02 (1.6599e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [46][270/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.2368e-02 (1.6756e-02)	Acc@1  98.44 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [46][280/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.0391e-02 (1.6760e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [46][290/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.3849e-02 (1.6768e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [46][300/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.9046e-03 (1.6497e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [46][310/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.6937e-02 (1.6705e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [46][320/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 8.8043e-03 (1.6729e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [46][330/391]	Time  0.116 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.1169e-02 (1.6830e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [46][340/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.5747e-02 (1.6842e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [46][350/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.8585e-02 (1.6814e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [46][360/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.2293e-02 (1.6792e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [46][370/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 9.1171e-03 (1.6755e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [46][380/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.8845e-02 (1.6677e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [46][390/391]	Time  0.104 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.9978e-03 (1.6631e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
## e[46] optimizer.zero_grad (sum) time: 0.6813993453979492
## e[46]       loss.backward (sum) time: 14.21135663986206
## e[46]      optimizer.step (sum) time: 7.998960494995117
## epoch[46] training(only) time: 47.00912523269653
# Switched to evaluate mode...
Test: [  0/100]	Time  0.181 ( 0.181)	Loss 1.9104e-01 (1.9104e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 2.4756e-01 (2.1952e-01)	Acc@1  97.00 ( 93.82)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 3.9697e-01 (2.4459e-01)	Acc@1  89.00 ( 93.00)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.049 ( 0.052)	Loss 2.1155e-01 (2.4890e-01)	Acc@1  94.00 ( 93.13)	Acc@5  99.00 ( 99.68)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 3.2275e-01 (2.4912e-01)	Acc@1  90.00 ( 93.05)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.050 ( 0.050)	Loss 1.5857e-01 (2.5171e-01)	Acc@1  95.00 ( 92.98)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.8408e-01 (2.4614e-01)	Acc@1  98.00 ( 93.11)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 3.8623e-01 (2.4410e-01)	Acc@1  92.00 ( 93.20)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 1.6602e-01 (2.4717e-01)	Acc@1  94.00 ( 93.16)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 2.6440e-01 (2.4519e-01)	Acc@1  95.00 ( 93.23)	Acc@5 100.00 ( 99.80)
 * Acc@1 93.300 Acc@5 99.820
### epoch[46] execution time: 51.97926330566406
EPOCH 47
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [47][  0/391]	Time  0.283 ( 0.283)	Data  0.152 ( 0.152)	Loss 5.2910e-03 (5.2910e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [47][ 10/391]	Time  0.118 ( 0.134)	Data  0.001 ( 0.015)	Loss 2.7893e-02 (1.8245e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [47][ 20/391]	Time  0.117 ( 0.127)	Data  0.001 ( 0.008)	Loss 3.0518e-02 (1.7413e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [47][ 30/391]	Time  0.120 ( 0.124)	Data  0.001 ( 0.006)	Loss 2.1713e-02 (1.7466e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [47][ 40/391]	Time  0.121 ( 0.124)	Data  0.001 ( 0.005)	Loss 8.3466e-03 (1.8258e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [47][ 50/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.004)	Loss 5.2795e-03 (1.7360e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [47][ 60/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.5572e-02 (1.7265e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [47][ 70/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 5.0392e-03 (1.6767e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [47][ 80/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 3.3905e-02 (1.6621e-02)	Acc@1  98.44 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [47][ 90/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.003)	Loss 8.6594e-03 (1.6411e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [47][100/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.4610e-03 (1.6214e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [47][110/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.0446e-02 (1.6689e-02)	Acc@1  97.66 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [47][120/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0147e-02 (1.6824e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [47][130/391]	Time  0.127 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.4244e-02 (1.6991e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [47][140/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.2370e-03 (1.6817e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [47][150/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.6637e-03 (1.6831e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [47][160/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.4307e-02 (1.6661e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [47][170/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.1398e-02 (1.6930e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [47][180/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.6693e-02 (1.6712e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [47][190/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.6296e-02 (1.6573e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [47][200/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.4605e-03 (1.6473e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [47][210/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.8890e-02 (1.6476e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [47][220/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.1869e-03 (1.6354e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [47][230/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.9602e-02 (1.6329e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [47][240/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.0139e-02 (1.6339e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [47][250/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.0635e-02 (1.6272e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [47][260/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.8463e-02 (1.6157e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [47][270/391]	Time  0.132 ( 0.120)	Data  0.001 ( 0.002)	Loss 7.0114e-03 (1.6182e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [47][280/391]	Time  0.123 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.5564e-02 (1.6135e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [47][290/391]	Time  0.122 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.0475e-02 (1.5972e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [47][300/391]	Time  0.128 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.8696e-02 (1.6172e-02)	Acc@1  98.44 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [47][310/391]	Time  0.122 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.3544e-02 (1.6248e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [47][320/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.6296e-02 (1.6268e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [47][330/391]	Time  0.133 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.7349e-02 (1.6426e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [47][340/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.2043e-02 (1.6493e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [47][350/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.7602e-03 (1.6490e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [47][360/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.3331e-02 (1.6493e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [47][370/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 7.4768e-03 (1.6646e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [47][380/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.4857e-02 (1.6763e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [47][390/391]	Time  0.107 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.0798e-02 (1.6722e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
## e[47] optimizer.zero_grad (sum) time: 0.6845204830169678
## e[47]       loss.backward (sum) time: 14.281534671783447
## e[47]      optimizer.step (sum) time: 7.988878011703491
## epoch[47] training(only) time: 47.08120846748352
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 1.7578e-01 (1.7578e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 2.4072e-01 (2.2079e-01)	Acc@1  97.00 ( 93.73)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.049 ( 0.054)	Loss 4.4507e-01 (2.5159e-01)	Acc@1  88.00 ( 93.05)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.057 ( 0.052)	Loss 2.5854e-01 (2.5498e-01)	Acc@1  93.00 ( 93.00)	Acc@5  99.00 ( 99.68)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 3.7671e-01 (2.5646e-01)	Acc@1  89.00 ( 92.88)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.3647e-01 (2.5652e-01)	Acc@1  98.00 ( 92.92)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.048 ( 0.050)	Loss 1.7761e-01 (2.5119e-01)	Acc@1  99.00 ( 93.13)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 4.1211e-01 (2.4813e-01)	Acc@1  91.00 ( 93.17)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.056 ( 0.049)	Loss 1.3062e-01 (2.4990e-01)	Acc@1  95.00 ( 93.19)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.050 ( 0.049)	Loss 2.5342e-01 (2.4781e-01)	Acc@1  95.00 ( 93.20)	Acc@5 100.00 ( 99.80)
 * Acc@1 93.280 Acc@5 99.820
### epoch[47] execution time: 52.06136226654053
EPOCH 48
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [48][  0/391]	Time  0.282 ( 0.282)	Data  0.151 ( 0.151)	Loss 1.8707e-02 (1.8707e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [48][ 10/391]	Time  0.119 ( 0.134)	Data  0.001 ( 0.015)	Loss 7.4615e-03 (1.4094e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [48][ 20/391]	Time  0.118 ( 0.127)	Data  0.001 ( 0.008)	Loss 7.6561e-03 (1.3543e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [48][ 30/391]	Time  0.118 ( 0.125)	Data  0.001 ( 0.006)	Loss 1.0521e-02 (1.4279e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [48][ 40/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.005)	Loss 5.0087e-03 (1.4119e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [48][ 50/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.004)	Loss 4.4060e-03 (1.3729e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [48][ 60/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.1497e-02 (1.3925e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [48][ 70/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.003)	Loss 1.9287e-02 (1.4088e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [48][ 80/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.003)	Loss 3.8071e-03 (1.4537e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [48][ 90/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.003)	Loss 7.9880e-03 (1.4364e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [48][100/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.1345e-02 (1.4607e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [48][110/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.1411e-03 (1.4537e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [48][120/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.1722e-03 (1.4947e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [48][130/391]	Time  0.127 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.1378e-03 (1.5100e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [48][140/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.0111e-02 (1.5210e-02)	Acc@1  98.44 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [48][150/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.0109e-02 (1.5193e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [48][160/391]	Time  0.131 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.0501e-03 (1.5215e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [48][170/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.1956e-03 (1.5264e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [48][180/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.7975e-02 (1.5158e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [48][190/391]	Time  0.122 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.3031e-02 (1.4946e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [48][200/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.8022e-03 (1.4762e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [48][210/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.1133e-02 (1.4793e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [48][220/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.3069e-02 (1.4703e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [48][230/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.1270e-02 (1.4867e-02)	Acc@1  98.44 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [48][240/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.5083e-02 (1.4836e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [48][250/391]	Time  0.125 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.1565e-02 (1.4977e-02)	Acc@1  98.44 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [48][260/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 8.2474e-03 (1.5098e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [48][270/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.5919e-02 (1.5211e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [48][280/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.8625e-02 (1.5187e-02)	Acc@1  98.44 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [48][290/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.7471e-02 (1.5180e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [48][300/391]	Time  0.123 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.0048e-02 (1.5152e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [48][310/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.5954e-03 (1.5103e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [48][320/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.1231e-03 (1.5172e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [48][330/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.0731e-02 (1.5145e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [48][340/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 9.3384e-03 (1.5141e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [48][350/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 9.0027e-03 (1.5226e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [48][360/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.3987e-02 (1.5369e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [48][370/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.1149e-02 (1.5305e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [48][380/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.6382e-02 (1.5295e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [48][390/391]	Time  0.108 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.2489e-02 (1.5288e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
## e[48] optimizer.zero_grad (sum) time: 0.6870708465576172
## e[48]       loss.backward (sum) time: 14.22122049331665
## e[48]      optimizer.step (sum) time: 8.014929294586182
## epoch[48] training(only) time: 47.0254921913147
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 1.6406e-01 (1.6406e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 2.3389e-01 (2.1455e-01)	Acc@1  96.00 ( 94.00)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.047 ( 0.054)	Loss 4.2236e-01 (2.4454e-01)	Acc@1  88.00 ( 93.24)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.048 ( 0.052)	Loss 2.4194e-01 (2.5080e-01)	Acc@1  91.00 ( 93.10)	Acc@5  99.00 ( 99.68)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 2.9761e-01 (2.5281e-01)	Acc@1  87.00 ( 93.05)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.049 ( 0.050)	Loss 1.5015e-01 (2.5463e-01)	Acc@1  96.00 ( 93.02)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.045 ( 0.050)	Loss 1.9336e-01 (2.5092e-01)	Acc@1  98.00 ( 93.00)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 3.8550e-01 (2.4881e-01)	Acc@1  91.00 ( 93.04)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 1.3916e-01 (2.5177e-01)	Acc@1  93.00 ( 93.04)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.049 ( 0.049)	Loss 2.6587e-01 (2.5007e-01)	Acc@1  95.00 ( 93.10)	Acc@5 100.00 ( 99.81)
 * Acc@1 93.200 Acc@5 99.830
### epoch[48] execution time: 51.98990821838379
EPOCH 49
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [49][  0/391]	Time  0.279 ( 0.279)	Data  0.148 ( 0.148)	Loss 8.8882e-03 (8.8882e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [49][ 10/391]	Time  0.118 ( 0.135)	Data  0.001 ( 0.014)	Loss 1.0727e-02 (1.7849e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [49][ 20/391]	Time  0.130 ( 0.128)	Data  0.001 ( 0.008)	Loss 1.0941e-02 (1.5433e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [49][ 30/391]	Time  0.120 ( 0.125)	Data  0.001 ( 0.006)	Loss 3.3092e-03 (1.5512e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [49][ 40/391]	Time  0.119 ( 0.124)	Data  0.001 ( 0.005)	Loss 6.8970e-03 (1.5363e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [49][ 50/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.004)	Loss 1.9363e-02 (1.4728e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [49][ 60/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.0742e-02 (1.5239e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [49][ 70/391]	Time  0.126 ( 0.122)	Data  0.001 ( 0.003)	Loss 8.8882e-03 (1.4919e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [49][ 80/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.003)	Loss 9.7275e-03 (1.5417e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [49][ 90/391]	Time  0.128 ( 0.122)	Data  0.001 ( 0.003)	Loss 2.5711e-02 (1.5213e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [49][100/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.002)	Loss 7.9651e-03 (1.4651e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [49][110/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.5289e-02 (1.4357e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [49][120/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.6244e-02 (1.4759e-02)	Acc@1  97.66 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [49][130/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.6817e-03 (1.4935e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [49][140/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0033e-02 (1.4997e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [49][150/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.3607e-03 (1.5106e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [49][160/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.2561e-03 (1.4928e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [49][170/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.4420e-02 (1.4751e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [49][180/391]	Time  0.124 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.3602e-03 (1.4383e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [49][190/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.3191e-02 (1.4571e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [49][200/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.0643e-03 (1.4510e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [49][210/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.3586e-03 (1.4442e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [49][220/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0345e-02 (1.4522e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [49][230/391]	Time  0.124 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.9699e-02 (1.4581e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [49][240/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.4240e-03 (1.4627e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [49][250/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.7176e-02 (1.4863e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [49][260/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.7298e-02 (1.4913e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [49][270/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.2659e-02 (1.4918e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [49][280/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.4778e-02 (1.4849e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [49][290/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 7.4387e-03 (1.4681e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [49][300/391]	Time  0.136 ( 0.120)	Data  0.001 ( 0.001)	Loss 7.6561e-03 (1.4783e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [49][310/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 7.4615e-03 (1.4688e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [49][320/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 8.0948e-03 (1.4656e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [49][330/391]	Time  0.132 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.4160e-02 (1.4628e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [49][340/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.4313e-02 (1.4614e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [49][350/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 9.4604e-03 (1.4782e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [49][360/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.4626e-02 (1.4886e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [49][370/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.4915e-02 (1.4859e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [49][380/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.0048e-02 (1.4803e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [49][390/391]	Time  0.104 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.4915e-02 (1.4888e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
## e[49] optimizer.zero_grad (sum) time: 0.6821885108947754
## e[49]       loss.backward (sum) time: 14.357058048248291
## e[49]      optimizer.step (sum) time: 8.00070571899414
## epoch[49] training(only) time: 47.1451461315155
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 1.6455e-01 (1.6455e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.059)	Loss 2.0325e-01 (2.1079e-01)	Acc@1  96.00 ( 93.55)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.046 ( 0.053)	Loss 4.5215e-01 (2.4497e-01)	Acc@1  89.00 ( 93.00)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.046 ( 0.051)	Loss 2.7393e-01 (2.5107e-01)	Acc@1  91.00 ( 92.90)	Acc@5  99.00 ( 99.71)
Test: [ 40/100]	Time  0.047 ( 0.050)	Loss 3.1738e-01 (2.5291e-01)	Acc@1  88.00 ( 92.93)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.3037e-01 (2.5663e-01)	Acc@1  94.00 ( 92.96)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.047 ( 0.049)	Loss 1.9690e-01 (2.5300e-01)	Acc@1  97.00 ( 93.07)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 3.6621e-01 (2.4989e-01)	Acc@1  92.00 ( 93.14)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.2018e-01 (2.5143e-01)	Acc@1  95.00 ( 93.22)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.046 ( 0.048)	Loss 2.4097e-01 (2.5025e-01)	Acc@1  95.00 ( 93.29)	Acc@5 100.00 ( 99.81)
 * Acc@1 93.370 Acc@5 99.830
### epoch[49] execution time: 52.07920455932617
EPOCH 50
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [50][  0/391]	Time  0.279 ( 0.279)	Data  0.152 ( 0.152)	Loss 6.6109e-03 (6.6109e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [50][ 10/391]	Time  0.119 ( 0.134)	Data  0.001 ( 0.015)	Loss 2.1347e-02 (1.3234e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [50][ 20/391]	Time  0.118 ( 0.127)	Data  0.001 ( 0.008)	Loss 1.7868e-02 (1.2639e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [50][ 30/391]	Time  0.121 ( 0.125)	Data  0.001 ( 0.006)	Loss 2.7039e-02 (1.3230e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [50][ 40/391]	Time  0.116 ( 0.123)	Data  0.001 ( 0.005)	Loss 8.3618e-03 (1.3470e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [50][ 50/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.004)	Loss 6.4430e-03 (1.3949e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [50][ 60/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 7.2517e-03 (1.3311e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [50][ 70/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.0559e-02 (1.3344e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [50][ 80/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.003)	Loss 8.7051e-03 (1.3552e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [50][ 90/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.003)	Loss 6.9351e-03 (1.3256e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [50][100/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.9199e-03 (1.3141e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [50][110/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.0781e-02 (1.3668e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [50][120/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.6136e-03 (1.3997e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [50][130/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.2016e-03 (1.3674e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [50][140/391]	Time  0.124 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.9089e-02 (1.3940e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [50][150/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.2330e-03 (1.3770e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [50][160/391]	Time  0.127 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.6647e-02 (1.4132e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [50][170/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.2894e-02 (1.4053e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [50][180/391]	Time  0.124 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.5015e-02 (1.4011e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [50][190/391]	Time  0.133 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.8005e-02 (1.4014e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [50][200/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.5274e-02 (1.3955e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [50][210/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.4409e-03 (1.4059e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [50][220/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.7802e-02 (1.4095e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [50][230/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.6922e-02 (1.4091e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [50][240/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.3295e-02 (1.4645e-02)	Acc@1  98.44 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [50][250/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.2867e-02 (1.4640e-02)	Acc@1  97.66 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [50][260/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 8.0414e-03 (1.4659e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [50][270/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.8524e-02 (1.4659e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [50][280/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.7334e-02 (1.4725e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [50][290/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.1681e-02 (1.4809e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [50][300/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 7.5912e-03 (1.4659e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [50][310/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.8319e-02 (1.4708e-02)	Acc@1  97.66 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [50][320/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.0132e-02 (1.4730e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [50][330/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.4536e-02 (1.4717e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [50][340/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.4252e-02 (1.4627e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [50][350/391]	Time  0.125 ( 0.120)	Data  0.001 ( 0.001)	Loss 9.5444e-03 (1.4506e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [50][360/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.2154e-02 (1.4580e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [50][370/391]	Time  0.133 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.1408e-02 (1.4627e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [50][380/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.2238e-02 (1.4705e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [50][390/391]	Time  0.106 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.4392e-03 (1.4770e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
## e[50] optimizer.zero_grad (sum) time: 0.6891322135925293
## e[50]       loss.backward (sum) time: 14.2721107006073
## e[50]      optimizer.step (sum) time: 7.9850380420684814
## epoch[50] training(only) time: 47.10941481590271
# Switched to evaluate mode...
Test: [  0/100]	Time  0.181 ( 0.181)	Loss 1.5051e-01 (1.5051e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 2.2705e-01 (2.1126e-01)	Acc@1  96.00 ( 93.82)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 4.1724e-01 (2.4952e-01)	Acc@1  87.00 ( 92.95)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 2.6758e-01 (2.5913e-01)	Acc@1  92.00 ( 92.90)	Acc@5  99.00 ( 99.71)
Test: [ 40/100]	Time  0.056 ( 0.051)	Loss 2.9517e-01 (2.5783e-01)	Acc@1  89.00 ( 92.90)	Acc@5 100.00 ( 99.76)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.2646e-01 (2.6016e-01)	Acc@1  97.00 ( 92.92)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.9165e-01 (2.5501e-01)	Acc@1  97.00 ( 93.03)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 3.5693e-01 (2.5113e-01)	Acc@1  92.00 ( 93.08)	Acc@5 100.00 ( 99.80)
Test: [ 80/100]	Time  0.050 ( 0.049)	Loss 1.3696e-01 (2.5384e-01)	Acc@1  96.00 ( 93.07)	Acc@5 100.00 ( 99.83)
Test: [ 90/100]	Time  0.054 ( 0.049)	Loss 2.5806e-01 (2.5229e-01)	Acc@1  95.00 ( 93.19)	Acc@5 100.00 ( 99.82)
 * Acc@1 93.250 Acc@5 99.840
### epoch[50] execution time: 52.086063385009766
EPOCH 51
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [51][  0/391]	Time  0.274 ( 0.274)	Data  0.139 ( 0.139)	Loss 5.6076e-03 (5.6076e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [51][ 10/391]	Time  0.125 ( 0.136)	Data  0.001 ( 0.013)	Loss 1.8234e-02 (1.6483e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [51][ 20/391]	Time  0.133 ( 0.128)	Data  0.001 ( 0.007)	Loss 1.1993e-02 (1.7266e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [51][ 30/391]	Time  0.122 ( 0.125)	Data  0.001 ( 0.005)	Loss 6.7635e-03 (1.6781e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [51][ 40/391]	Time  0.119 ( 0.124)	Data  0.001 ( 0.004)	Loss 1.2611e-02 (1.6329e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [51][ 50/391]	Time  0.129 ( 0.123)	Data  0.001 ( 0.004)	Loss 3.8528e-03 (1.5180e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [51][ 60/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.003)	Loss 4.1885e-03 (1.4663e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [51][ 70/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 3.1586e-03 (1.4031e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [51][ 80/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 3.7727e-03 (1.4029e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [51][ 90/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.3176e-02 (1.3850e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [51][100/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.1955e-02 (1.4170e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [51][110/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.6392e-03 (1.3762e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [51][120/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.2779e-03 (1.3445e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [51][130/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.3313e-02 (1.3572e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [51][140/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.6436e-03 (1.3528e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [51][150/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.7308e-03 (1.3474e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [51][160/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.2550e-03 (1.3676e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [51][170/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.2349e-02 (1.3958e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [51][180/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.4673e-02 (1.4034e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [51][190/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.2147e-03 (1.4167e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [51][200/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.1948e-02 (1.4011e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [51][210/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.5989e-03 (1.4022e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [51][220/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.1626e-02 (1.4373e-02)	Acc@1  97.66 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [51][230/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.1787e-02 (1.4654e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [51][240/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.5139e-03 (1.4496e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [51][250/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.9099e-02 (1.4370e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [51][260/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.3268e-02 (1.4285e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [51][270/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.1248e-03 (1.4370e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [51][280/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.1860e-02 (1.4281e-02)	Acc@1  98.44 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [51][290/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.6754e-02 (1.4290e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [51][300/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.7199e-03 (1.4181e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [51][310/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.7618e-02 (1.4187e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [51][320/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.4136e-03 (1.4105e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [51][330/391]	Time  0.126 ( 0.121)	Data  0.001 ( 0.001)	Loss 9.7656e-03 (1.4124e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [51][340/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.9771e-03 (1.4069e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [51][350/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.5076e-02 (1.4072e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [51][360/391]	Time  0.130 ( 0.121)	Data  0.001 ( 0.001)	Loss 8.4152e-03 (1.4004e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [51][370/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.2650e-02 (1.3956e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [51][380/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.6377e-02 (1.4056e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [51][390/391]	Time  0.116 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.1497e-02 (1.4082e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
## e[51] optimizer.zero_grad (sum) time: 0.6845495700836182
## e[51]       loss.backward (sum) time: 14.268560647964478
## e[51]      optimizer.step (sum) time: 7.996637344360352
## epoch[51] training(only) time: 47.159812927246094
# Switched to evaluate mode...
Test: [  0/100]	Time  0.190 ( 0.190)	Loss 1.7261e-01 (1.7261e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.059)	Loss 2.3132e-01 (2.2129e-01)	Acc@1  96.00 ( 93.64)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 4.2822e-01 (2.4910e-01)	Acc@1  88.00 ( 93.10)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.046 ( 0.051)	Loss 2.5073e-01 (2.5792e-01)	Acc@1  93.00 ( 93.23)	Acc@5  99.00 ( 99.68)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 3.0640e-01 (2.5930e-01)	Acc@1  90.00 ( 93.32)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.4868e-01 (2.6142e-01)	Acc@1  96.00 ( 93.24)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.9287e-01 (2.5627e-01)	Acc@1  98.00 ( 93.30)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.049 ( 0.049)	Loss 3.6499e-01 (2.5276e-01)	Acc@1  92.00 ( 93.31)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.6370e-01 (2.5536e-01)	Acc@1  93.00 ( 93.23)	Acc@5 100.00 ( 99.81)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 2.8442e-01 (2.5387e-01)	Acc@1  92.00 ( 93.26)	Acc@5 100.00 ( 99.81)
 * Acc@1 93.340 Acc@5 99.830
### epoch[51] execution time: 52.11565017700195
EPOCH 52
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [52][  0/391]	Time  0.279 ( 0.279)	Data  0.143 ( 0.143)	Loss 9.0714e-03 (9.0714e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [52][ 10/391]	Time  0.121 ( 0.134)	Data  0.001 ( 0.014)	Loss 1.3275e-02 (9.5837e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [52][ 20/391]	Time  0.119 ( 0.127)	Data  0.001 ( 0.008)	Loss 3.3894e-03 (9.7286e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [52][ 30/391]	Time  0.121 ( 0.125)	Data  0.001 ( 0.006)	Loss 7.4463e-03 (9.2081e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [52][ 40/391]	Time  0.120 ( 0.124)	Data  0.001 ( 0.004)	Loss 2.0844e-02 (9.4705e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [52][ 50/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.004)	Loss 2.2064e-02 (1.0102e-02)	Acc@1  98.44 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [52][ 60/391]	Time  0.122 ( 0.123)	Data  0.001 ( 0.003)	Loss 3.0880e-03 (1.0557e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [52][ 70/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 7.1602e-03 (1.0650e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [52][ 80/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.1284e-02 (1.0541e-02)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [52][ 90/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.4114e-02 (1.1479e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [52][100/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.1330e-02 (1.1944e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [52][110/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.5706e-03 (1.1575e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [52][120/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.3553e-03 (1.1513e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [52][130/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.5818e-02 (1.1610e-02)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [52][140/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.4763e-03 (1.1730e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [52][150/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.3384e-03 (1.1779e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [52][160/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.6948e-03 (1.1792e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [52][170/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.8071e-03 (1.1961e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [52][180/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.5695e-03 (1.2361e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [52][190/391]	Time  0.130 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.9500e-02 (1.2477e-02)	Acc@1  97.66 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [52][200/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.4244e-02 (1.2687e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [52][210/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.1993e-02 (1.2709e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [52][220/391]	Time  0.129 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.7273e-02 (1.2782e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [52][230/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.4060e-03 (1.2829e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [52][240/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.9111e-03 (1.2953e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [52][250/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.1471e-03 (1.2946e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [52][260/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.0545e-03 (1.2893e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [52][270/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.2024e-02 (1.2812e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [52][280/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.4000e-03 (1.2720e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [52][290/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 9.0332e-03 (1.2764e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [52][300/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.0621e-03 (1.2688e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [52][310/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.0788e-02 (1.2838e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [52][320/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.6260e-02 (1.2885e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [52][330/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.5251e-02 (1.2864e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [52][340/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.0496e-03 (1.2802e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [52][350/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 8.5907e-03 (1.2795e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [52][360/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.0124e-02 (1.2909e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [52][370/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.3252e-02 (1.2872e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [52][380/391]	Time  0.133 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.4809e-02 (1.2819e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [52][390/391]	Time  0.105 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.4210e-02 (1.2909e-02)	Acc@1  98.75 ( 99.72)	Acc@5 100.00 (100.00)
## e[52] optimizer.zero_grad (sum) time: 0.6818563938140869
## e[52]       loss.backward (sum) time: 14.323909759521484
## e[52]      optimizer.step (sum) time: 8.015868186950684
## epoch[52] training(only) time: 47.21856927871704
# Switched to evaluate mode...
Test: [  0/100]	Time  0.181 ( 0.181)	Loss 1.6846e-01 (1.6846e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 2.4670e-01 (2.1833e-01)	Acc@1  97.00 ( 93.73)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 3.8721e-01 (2.4377e-01)	Acc@1  89.00 ( 93.14)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 2.6367e-01 (2.5314e-01)	Acc@1  92.00 ( 93.13)	Acc@5  99.00 ( 99.71)
Test: [ 40/100]	Time  0.055 ( 0.051)	Loss 2.8345e-01 (2.5543e-01)	Acc@1  88.00 ( 93.12)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.048 ( 0.050)	Loss 1.6565e-01 (2.6137e-01)	Acc@1  94.00 ( 93.08)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.9507e-01 (2.5750e-01)	Acc@1  96.00 ( 93.07)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 3.7744e-01 (2.5452e-01)	Acc@1  92.00 ( 93.11)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.050 ( 0.049)	Loss 1.5967e-01 (2.5730e-01)	Acc@1  94.00 ( 93.07)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.056 ( 0.049)	Loss 2.8296e-01 (2.5681e-01)	Acc@1  94.00 ( 93.11)	Acc@5 100.00 ( 99.81)
 * Acc@1 93.180 Acc@5 99.830
### epoch[52] execution time: 52.20660161972046
EPOCH 53
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [53][  0/391]	Time  0.284 ( 0.284)	Data  0.147 ( 0.147)	Loss 1.0239e-02 (1.0239e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [53][ 10/391]	Time  0.121 ( 0.136)	Data  0.001 ( 0.014)	Loss 2.3918e-03 (1.3276e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [53][ 20/391]	Time  0.119 ( 0.128)	Data  0.001 ( 0.008)	Loss 2.5284e-02 (1.2700e-02)	Acc@1  98.44 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [53][ 30/391]	Time  0.121 ( 0.125)	Data  0.001 ( 0.006)	Loss 8.2550e-03 (1.1948e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [53][ 40/391]	Time  0.117 ( 0.124)	Data  0.001 ( 0.004)	Loss 9.6893e-03 (1.1645e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [53][ 50/391]	Time  0.132 ( 0.123)	Data  0.001 ( 0.004)	Loss 6.6757e-03 (1.1421e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [53][ 60/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 7.2021e-03 (1.1168e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [53][ 70/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.1620e-02 (1.0667e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [53][ 80/391]	Time  0.128 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.0414e-02 (1.0916e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [53][ 90/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.003)	Loss 3.7537e-03 (1.1090e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [53][100/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.9417e-03 (1.1318e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [53][110/391]	Time  0.128 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.0114e-03 (1.1419e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [53][120/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.9008e-03 (1.1030e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [53][130/391]	Time  0.126 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.1466e-03 (1.0934e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [53][140/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.9400e-03 (1.0723e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [53][150/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.3878e-02 (1.0894e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [53][160/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.3008e-03 (1.1094e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [53][170/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.5989e-03 (1.0863e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [53][180/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0025e-02 (1.1013e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [53][190/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.5122e-03 (1.0892e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [53][200/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.2964e-02 (1.1033e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [53][210/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.1484e-02 (1.1225e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [53][220/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.0172e-02 (1.1169e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [53][230/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.6414e-03 (1.1224e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [53][240/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.8214e-02 (1.1252e-02)	Acc@1  98.44 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [53][250/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.3515e-03 (1.1309e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [53][260/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 8.5297e-03 (1.1347e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [53][270/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.0040e-02 (1.1418e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [53][280/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.4244e-02 (1.1468e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [53][290/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 8.0185e-03 (1.1472e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [53][300/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.2733e-02 (1.1351e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [53][310/391]	Time  0.131 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.7569e-03 (1.1269e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [53][320/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 9.0485e-03 (1.1322e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [53][330/391]	Time  0.134 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.6510e-02 (1.1312e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [53][340/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.6901e-02 (1.1399e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [53][350/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.8519e-02 (1.1500e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [53][360/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.3972e-03 (1.1439e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [53][370/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.0645e-02 (1.1748e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [53][380/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.0811e-02 (1.1810e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [53][390/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.8076e-02 (1.1877e-02)	Acc@1  98.75 ( 99.73)	Acc@5 100.00 (100.00)
## e[53] optimizer.zero_grad (sum) time: 0.6783506870269775
## e[53]       loss.backward (sum) time: 14.299928903579712
## e[53]      optimizer.step (sum) time: 8.049153327941895
## epoch[53] training(only) time: 47.18169546127319
# Switched to evaluate mode...
Test: [  0/100]	Time  0.182 ( 0.182)	Loss 1.9202e-01 (1.9202e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 2.3376e-01 (2.2385e-01)	Acc@1  96.00 ( 93.91)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 3.4131e-01 (2.4645e-01)	Acc@1  89.00 ( 93.24)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 2.4707e-01 (2.5736e-01)	Acc@1  95.00 ( 93.16)	Acc@5  99.00 ( 99.65)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 2.8198e-01 (2.5934e-01)	Acc@1  87.00 ( 93.05)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.5833e-01 (2.6262e-01)	Acc@1  97.00 ( 93.20)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.7920e-01 (2.5720e-01)	Acc@1  98.00 ( 93.23)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.048 ( 0.049)	Loss 4.0601e-01 (2.5498e-01)	Acc@1  92.00 ( 93.31)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.049 ( 0.049)	Loss 1.2659e-01 (2.5801e-01)	Acc@1  94.00 ( 93.23)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.049 ( 0.049)	Loss 3.3643e-01 (2.5764e-01)	Acc@1  93.00 ( 93.27)	Acc@5 100.00 ( 99.77)
 * Acc@1 93.300 Acc@5 99.790
### epoch[53] execution time: 52.16048002243042
EPOCH 54
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [54][  0/391]	Time  0.290 ( 0.290)	Data  0.152 ( 0.152)	Loss 7.0648e-03 (7.0648e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [54][ 10/391]	Time  0.118 ( 0.134)	Data  0.001 ( 0.015)	Loss 1.1299e-02 (1.1916e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [54][ 20/391]	Time  0.118 ( 0.127)	Data  0.001 ( 0.008)	Loss 4.5547e-03 (1.1146e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [54][ 30/391]	Time  0.117 ( 0.125)	Data  0.001 ( 0.006)	Loss 1.2512e-02 (1.1458e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [54][ 40/391]	Time  0.118 ( 0.124)	Data  0.001 ( 0.005)	Loss 1.0567e-02 (1.1615e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [54][ 50/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.004)	Loss 6.7940e-03 (1.2309e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [54][ 60/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.003)	Loss 8.8501e-03 (1.1552e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [54][ 70/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.8631e-02 (1.1948e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [54][ 80/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.003)	Loss 2.0096e-02 (1.1671e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [54][ 90/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.4938e-02 (1.1441e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [54][100/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.0544e-02 (1.2064e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [54][110/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.6725e-03 (1.1853e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [54][120/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.7181e-02 (1.1810e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [54][130/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0651e-02 (1.1940e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [54][140/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.7553e-03 (1.1789e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [54][150/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.2230e-02 (1.1813e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [54][160/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.4577e-03 (1.1918e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [54][170/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.5286e-03 (1.1842e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [54][180/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.4932e-03 (1.1760e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [54][190/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.0613e-03 (1.1804e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [54][200/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.6708e-02 (1.1806e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [54][210/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.8114e-03 (1.2004e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [54][220/391]	Time  0.128 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.4628e-02 (1.2046e-02)	Acc@1  98.44 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [54][230/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.3077e-02 (1.1940e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [54][240/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.6877e-03 (1.1873e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [54][250/391]	Time  0.128 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.4323e-02 (1.1991e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [54][260/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.6310e-03 (1.1899e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [54][270/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.8661e-02 (1.1900e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [54][280/391]	Time  0.132 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.8708e-03 (1.1805e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [54][290/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.2299e-02 (1.1801e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [54][300/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.1818e-02 (1.1824e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [54][310/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 8.2932e-03 (1.1888e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [54][320/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.0384e-02 (1.1853e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [54][330/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.2497e-02 (1.1826e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [54][340/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.1866e-02 (1.1806e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [54][350/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.1734e-02 (1.1851e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [54][360/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.8890e-02 (1.1735e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [54][370/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.1711e-02 (1.1702e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [54][380/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.1414e-02 (1.1707e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [54][390/391]	Time  0.109 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.8779e-03 (1.1710e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
## e[54] optimizer.zero_grad (sum) time: 0.6784288883209229
## e[54]       loss.backward (sum) time: 14.377201795578003
## e[54]      optimizer.step (sum) time: 7.935218811035156
## epoch[54] training(only) time: 47.195475816726685
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 1.6797e-01 (1.6797e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 2.4121e-01 (2.1758e-01)	Acc@1  97.00 ( 94.64)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.045 ( 0.054)	Loss 4.2017e-01 (2.4496e-01)	Acc@1  87.00 ( 93.57)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 2.4915e-01 (2.5440e-01)	Acc@1  93.00 ( 93.42)	Acc@5  99.00 ( 99.68)
Test: [ 40/100]	Time  0.049 ( 0.051)	Loss 3.1958e-01 (2.5568e-01)	Acc@1  90.00 ( 93.34)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.048 ( 0.050)	Loss 1.6516e-01 (2.6045e-01)	Acc@1  94.00 ( 93.25)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.8494e-01 (2.5728e-01)	Acc@1  97.00 ( 93.30)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 3.9502e-01 (2.5436e-01)	Acc@1  92.00 ( 93.30)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.049 ( 0.049)	Loss 1.5161e-01 (2.5727e-01)	Acc@1  94.00 ( 93.27)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 3.1348e-01 (2.5574e-01)	Acc@1  94.00 ( 93.32)	Acc@5 100.00 ( 99.78)
 * Acc@1 93.430 Acc@5 99.790
### epoch[54] execution time: 52.15183186531067
EPOCH 55
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [55][  0/391]	Time  0.280 ( 0.280)	Data  0.149 ( 0.149)	Loss 5.5656e-03 (5.5656e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [55][ 10/391]	Time  0.118 ( 0.134)	Data  0.001 ( 0.014)	Loss 2.7885e-03 (1.4857e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [55][ 20/391]	Time  0.119 ( 0.127)	Data  0.001 ( 0.008)	Loss 5.3139e-03 (1.3015e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [55][ 30/391]	Time  0.118 ( 0.125)	Data  0.001 ( 0.006)	Loss 1.5976e-02 (1.2674e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [55][ 40/391]	Time  0.119 ( 0.124)	Data  0.001 ( 0.005)	Loss 1.1368e-02 (1.2242e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [55][ 50/391]	Time  0.123 ( 0.123)	Data  0.001 ( 0.004)	Loss 5.0507e-03 (1.1770e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [55][ 60/391]	Time  0.125 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.6785e-02 (1.1302e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [55][ 70/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.7654e-02 (1.1075e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [55][ 80/391]	Time  0.127 ( 0.122)	Data  0.001 ( 0.003)	Loss 5.0774e-03 (1.0761e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [55][ 90/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.3716e-03 (1.0998e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [55][100/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.2964e-02 (1.1202e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [55][110/391]	Time  0.130 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.9013e-03 (1.1719e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [55][120/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.1793e-03 (1.1799e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [55][130/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.5854e-02 (1.2254e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [55][140/391]	Time  0.124 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.2289e-03 (1.2352e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [55][150/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.3586e-03 (1.2299e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [55][160/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.1391e-02 (1.2523e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [55][170/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.6337e-02 (1.2486e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [55][180/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.3943e-03 (1.2404e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [55][190/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.4823e-03 (1.2353e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [55][200/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.8707e-02 (1.2342e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [55][210/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.3804e-02 (1.2247e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [55][220/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.3449e-03 (1.2288e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [55][230/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.0719e-03 (1.2271e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [55][240/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.2245e-02 (1.2110e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [55][250/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.3237e-02 (1.2063e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [55][260/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.1429e-02 (1.2049e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [55][270/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0239e-02 (1.1927e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [55][280/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.9814e-03 (1.1790e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [55][290/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.6657e-02 (1.1830e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [55][300/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.0483e-02 (1.1799e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [55][310/391]	Time  0.134 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.1681e-02 (1.1895e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [55][320/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.8485e-03 (1.2125e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [55][330/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.0523e-03 (1.2009e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [55][340/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.0648e-03 (1.1894e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [55][350/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.2261e-03 (1.1864e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [55][360/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.9541e-02 (1.1878e-02)	Acc@1  98.44 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [55][370/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.1345e-02 (1.1892e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [55][380/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.3509e-03 (1.1878e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [55][390/391]	Time  0.108 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.1927e-02 (1.1894e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
## e[55] optimizer.zero_grad (sum) time: 0.6806602478027344
## e[55]       loss.backward (sum) time: 14.290987968444824
## e[55]      optimizer.step (sum) time: 8.022677898406982
## epoch[55] training(only) time: 47.20411419868469
# Switched to evaluate mode...
Test: [  0/100]	Time  0.198 ( 0.198)	Loss 2.0764e-01 (2.0764e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 2.5342e-01 (2.2724e-01)	Acc@1  96.00 ( 94.36)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 3.5913e-01 (2.5204e-01)	Acc@1  88.00 ( 93.38)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 2.6587e-01 (2.6243e-01)	Acc@1  93.00 ( 93.29)	Acc@5  99.00 ( 99.61)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 3.1030e-01 (2.6362e-01)	Acc@1  88.00 ( 93.10)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.057 ( 0.050)	Loss 1.5552e-01 (2.6641e-01)	Acc@1  94.00 ( 92.92)	Acc@5 100.00 ( 99.69)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.6846e-01 (2.6147e-01)	Acc@1  97.00 ( 93.02)	Acc@5 100.00 ( 99.72)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 4.3164e-01 (2.6049e-01)	Acc@1  91.00 ( 93.04)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.7676e-01 (2.6480e-01)	Acc@1  94.00 ( 93.05)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.048 ( 0.049)	Loss 3.0469e-01 (2.6210e-01)	Acc@1  93.00 ( 93.15)	Acc@5 100.00 ( 99.76)
 * Acc@1 93.260 Acc@5 99.780
### epoch[55] execution time: 52.17699956893921
EPOCH 56
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [56][  0/391]	Time  0.287 ( 0.287)	Data  0.151 ( 0.151)	Loss 5.0568e-02 (5.0568e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [56][ 10/391]	Time  0.119 ( 0.134)	Data  0.001 ( 0.015)	Loss 3.1525e-02 (1.8826e-02)	Acc@1  98.44 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [56][ 20/391]	Time  0.119 ( 0.128)	Data  0.001 ( 0.008)	Loss 3.4866e-03 (1.4178e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [56][ 30/391]	Time  0.118 ( 0.126)	Data  0.001 ( 0.006)	Loss 2.7008e-02 (1.4012e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [56][ 40/391]	Time  0.118 ( 0.124)	Data  0.001 ( 0.005)	Loss 6.3324e-03 (1.4373e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [56][ 50/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.004)	Loss 5.5923e-03 (1.3450e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [56][ 60/391]	Time  0.117 ( 0.123)	Data  0.001 ( 0.003)	Loss 8.3313e-03 (1.2534e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [56][ 70/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.6891e-02 (1.2592e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [56][ 80/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.7059e-02 (1.1886e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [56][ 90/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.2915e-03 (1.1585e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [56][100/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.2332e-03 (1.1586e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [56][110/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.9580e-03 (1.1359e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [56][120/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.1072e-02 (1.1286e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [56][130/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.3605e-02 (1.1619e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [56][140/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.2349e-02 (1.1682e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [56][150/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.6403e-02 (1.1493e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [56][160/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.0267e-03 (1.1495e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [56][170/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.4094e-02 (1.1517e-02)	Acc@1  98.44 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [56][180/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.1429e-02 (1.1642e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [56][190/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.5013e-03 (1.1556e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [56][200/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.8180e-03 (1.1766e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [56][210/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.1220e-03 (1.2033e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [56][220/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.4550e-03 (1.1930e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [56][230/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.4832e-02 (1.1735e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [56][240/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.0499e-03 (1.1560e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [56][250/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.2452e-03 (1.1637e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [56][260/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.1826e-02 (1.1509e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [56][270/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.7532e-02 (1.1549e-02)	Acc@1  98.44 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [56][280/391]	Time  0.128 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.6297e-03 (1.1483e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [56][290/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.8436e-03 (1.1578e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [56][300/391]	Time  0.125 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.6174e-02 (1.1639e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [56][310/391]	Time  0.130 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.5128e-03 (1.1719e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [56][320/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 9.4681e-03 (1.1635e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [56][330/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.8365e-03 (1.1654e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [56][340/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.8950e-02 (1.1866e-02)	Acc@1  97.66 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [56][350/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.2894e-02 (1.1842e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [56][360/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 8.2626e-03 (1.1819e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [56][370/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.0368e-02 (1.1835e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [56][380/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.4641e-02 (1.1753e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [56][390/391]	Time  0.107 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.1469e-02 (1.1785e-02)	Acc@1  98.75 ( 99.73)	Acc@5 100.00 (100.00)
## e[56] optimizer.zero_grad (sum) time: 0.6752927303314209
## e[56]       loss.backward (sum) time: 14.310744524002075
## e[56]      optimizer.step (sum) time: 7.94756555557251
## epoch[56] training(only) time: 47.10306978225708
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 1.6711e-01 (1.6711e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.060)	Loss 2.2632e-01 (2.1646e-01)	Acc@1  97.00 ( 94.18)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 4.1602e-01 (2.4969e-01)	Acc@1  87.00 ( 93.14)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 2.2644e-01 (2.5668e-01)	Acc@1  93.00 ( 93.13)	Acc@5  99.00 ( 99.65)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 3.3594e-01 (2.5817e-01)	Acc@1  89.00 ( 93.05)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.048 ( 0.050)	Loss 1.5344e-01 (2.6273e-01)	Acc@1  95.00 ( 92.94)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.8579e-01 (2.5764e-01)	Acc@1  97.00 ( 93.05)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.051 ( 0.049)	Loss 4.0430e-01 (2.5536e-01)	Acc@1  93.00 ( 93.14)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.048 ( 0.049)	Loss 1.4661e-01 (2.5961e-01)	Acc@1  94.00 ( 93.14)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 2.6489e-01 (2.5748e-01)	Acc@1  93.00 ( 93.20)	Acc@5 100.00 ( 99.77)
 * Acc@1 93.320 Acc@5 99.790
### epoch[56] execution time: 52.094090938568115
EPOCH 57
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [57][  0/391]	Time  0.288 ( 0.288)	Data  0.145 ( 0.145)	Loss 1.4732e-02 (1.4732e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [57][ 10/391]	Time  0.120 ( 0.135)	Data  0.001 ( 0.014)	Loss 1.0902e-02 (1.0246e-02)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [57][ 20/391]	Time  0.119 ( 0.127)	Data  0.001 ( 0.008)	Loss 6.0310e-03 (9.6261e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [57][ 30/391]	Time  0.118 ( 0.125)	Data  0.001 ( 0.006)	Loss 3.8528e-03 (1.0293e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [57][ 40/391]	Time  0.119 ( 0.124)	Data  0.001 ( 0.004)	Loss 1.9512e-03 (1.0381e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [57][ 50/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.004)	Loss 5.2032e-03 (1.0205e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [57][ 60/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 2.7504e-03 (1.0275e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [57][ 70/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.003)	Loss 5.4512e-03 (1.0896e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [57][ 80/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.003)	Loss 2.5120e-03 (1.1428e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [57][ 90/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.003)	Loss 1.0880e-02 (1.1321e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [57][100/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.3474e-02 (1.1257e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [57][110/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.3283e-02 (1.1019e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [57][120/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.8550e-03 (1.0914e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [57][130/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.1988e-02 (1.1154e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [57][140/391]	Time  0.131 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.9722e-03 (1.0958e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [57][150/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.6342e-02 (1.1010e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [57][160/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.5825e-03 (1.0957e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [57][170/391]	Time  0.127 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.6561e-03 (1.0975e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [57][180/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.3923e-03 (1.0768e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [57][190/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.3939e-02 (1.0671e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [57][200/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.7532e-02 (1.0614e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [57][210/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.2537e-02 (1.0705e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [57][220/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.5994e-03 (1.0682e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [57][230/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.3787e-03 (1.0632e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [57][240/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.9417e-03 (1.0609e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [57][250/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.2440e-02 (1.0555e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [57][260/391]	Time  0.132 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.5319e-03 (1.0486e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [57][270/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.4124e-02 (1.0583e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [57][280/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.4973e-02 (1.0692e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [57][290/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.3275e-02 (1.0629e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [57][300/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.8572e-03 (1.0550e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [57][310/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.9651e-03 (1.0651e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [57][320/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 8.1253e-03 (1.0599e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [57][330/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.4017e-02 (1.0563e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [57][340/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.7924e-03 (1.0556e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [57][350/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 8.7509e-03 (1.0635e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [57][360/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.0071e-02 (1.0565e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [57][370/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.2634e-02 (1.0570e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [57][380/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.2909e-02 (1.0621e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [57][390/391]	Time  0.107 ( 0.120)	Data  0.001 ( 0.001)	Loss 9.6054e-03 (1.0687e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
## e[57] optimizer.zero_grad (sum) time: 0.6892364025115967
## e[57]       loss.backward (sum) time: 14.279232263565063
## e[57]      optimizer.step (sum) time: 8.057154178619385
## epoch[57] training(only) time: 47.19747591018677
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 1.4697e-01 (1.4697e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.063)	Loss 2.4890e-01 (2.1751e-01)	Acc@1  97.00 ( 94.09)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.047 ( 0.056)	Loss 4.4116e-01 (2.5510e-01)	Acc@1  87.00 ( 93.24)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 2.3865e-01 (2.6259e-01)	Acc@1  94.00 ( 93.19)	Acc@5  99.00 ( 99.68)
Test: [ 40/100]	Time  0.050 ( 0.052)	Loss 3.2251e-01 (2.6388e-01)	Acc@1  89.00 ( 93.17)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.056 ( 0.051)	Loss 1.4758e-01 (2.6919e-01)	Acc@1  95.00 ( 93.12)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.045 ( 0.050)	Loss 1.9812e-01 (2.6301e-01)	Acc@1  96.00 ( 93.18)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 4.1040e-01 (2.6204e-01)	Acc@1  93.00 ( 93.23)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.047 ( 0.050)	Loss 1.5784e-01 (2.6605e-01)	Acc@1  94.00 ( 93.22)	Acc@5 100.00 ( 99.81)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 2.6855e-01 (2.6306e-01)	Acc@1  93.00 ( 93.27)	Acc@5 100.00 ( 99.81)
 * Acc@1 93.340 Acc@5 99.830
### epoch[57] execution time: 52.19801712036133
EPOCH 58
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [58][  0/391]	Time  0.287 ( 0.287)	Data  0.146 ( 0.146)	Loss 1.7136e-02 (1.7136e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [58][ 10/391]	Time  0.120 ( 0.135)	Data  0.001 ( 0.014)	Loss 6.3896e-03 (8.9229e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [58][ 20/391]	Time  0.118 ( 0.128)	Data  0.001 ( 0.008)	Loss 2.5742e-02 (9.7115e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [58][ 30/391]	Time  0.120 ( 0.125)	Data  0.001 ( 0.006)	Loss 1.5602e-02 (1.0190e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [58][ 40/391]	Time  0.120 ( 0.124)	Data  0.001 ( 0.005)	Loss 9.2087e-03 (1.0094e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [58][ 50/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.004)	Loss 4.4746e-03 (9.7827e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [58][ 60/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.003)	Loss 1.1757e-02 (9.6837e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [58][ 70/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 5.3482e-03 (9.3246e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [58][ 80/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.1559e-02 (9.2222e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [58][ 90/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 2.4704e-02 (9.3777e-03)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [58][100/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.4888e-03 (9.3241e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [58][110/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.9793e-03 (9.2565e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [58][120/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.4468e-03 (9.0865e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [58][130/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.5940e-02 (9.3808e-03)	Acc@1  98.44 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [58][140/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.7885e-03 (9.6025e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [58][150/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0368e-02 (9.9252e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [58][160/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.7954e-02 (9.9782e-03)	Acc@1  98.44 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [58][170/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.7760e-03 (1.0035e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [58][180/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.4994e-02 (1.0337e-02)	Acc@1  98.44 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [58][190/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.2899e-03 (1.0566e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [58][200/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.4679e-02 (1.0538e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [58][210/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.9716e-03 (1.0622e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [58][220/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.8311e-02 (1.0808e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [58][230/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.4588e-03 (1.0701e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [58][240/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.1772e-02 (1.0641e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [58][250/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.0071e-02 (1.0872e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [58][260/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.3483e-02 (1.0915e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [58][270/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.0985e-02 (1.1012e-02)	Acc@1  97.66 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [58][280/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.2978e-02 (1.0984e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [58][290/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 8.6060e-03 (1.1057e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [58][300/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.6883e-03 (1.1027e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [58][310/391]	Time  0.135 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.7913e-03 (1.1014e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [58][320/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 7.5340e-03 (1.0979e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [58][330/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.6281e-02 (1.0926e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [58][340/391]	Time  0.134 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.8143e-02 (1.0854e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [58][350/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.6520e-02 (1.0874e-02)	Acc@1  98.44 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [58][360/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.4850e-03 (1.0788e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [58][370/391]	Time  0.123 ( 0.120)	Data  0.001 ( 0.001)	Loss 7.3204e-03 (1.0749e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [58][380/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.9089e-02 (1.0857e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [58][390/391]	Time  0.110 ( 0.120)	Data  0.001 ( 0.001)	Loss 7.5569e-03 (1.0754e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
## e[58] optimizer.zero_grad (sum) time: 0.6831390857696533
## e[58]       loss.backward (sum) time: 14.262550115585327
## e[58]      optimizer.step (sum) time: 8.004411220550537
## epoch[58] training(only) time: 47.129148960113525
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 1.7554e-01 (1.7554e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 2.4390e-01 (2.2551e-01)	Acc@1  97.00 ( 93.91)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.050 ( 0.054)	Loss 4.6460e-01 (2.6131e-01)	Acc@1  86.00 ( 93.10)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 2.6099e-01 (2.6508e-01)	Acc@1  91.00 ( 93.00)	Acc@5  99.00 ( 99.61)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 3.9771e-01 (2.6881e-01)	Acc@1  87.00 ( 93.05)	Acc@5 100.00 ( 99.66)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.6138e-01 (2.7195e-01)	Acc@1  95.00 ( 92.96)	Acc@5 100.00 ( 99.67)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.9568e-01 (2.6487e-01)	Acc@1  97.00 ( 93.15)	Acc@5 100.00 ( 99.70)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 3.9233e-01 (2.6472e-01)	Acc@1  93.00 ( 93.20)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.8823e-01 (2.6620e-01)	Acc@1  92.00 ( 93.15)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 2.5195e-01 (2.6396e-01)	Acc@1  92.00 ( 93.20)	Acc@5 100.00 ( 99.76)
 * Acc@1 93.260 Acc@5 99.780
### epoch[58] execution time: 52.08706736564636
EPOCH 59
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [59][  0/391]	Time  0.282 ( 0.282)	Data  0.150 ( 0.150)	Loss 7.0992e-03 (7.0992e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [59][ 10/391]	Time  0.118 ( 0.135)	Data  0.001 ( 0.014)	Loss 5.3062e-03 (6.9188e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [59][ 20/391]	Time  0.117 ( 0.127)	Data  0.001 ( 0.008)	Loss 4.2610e-03 (8.1991e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [59][ 30/391]	Time  0.118 ( 0.125)	Data  0.001 ( 0.006)	Loss 3.0613e-03 (9.8563e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [59][ 40/391]	Time  0.120 ( 0.124)	Data  0.001 ( 0.005)	Loss 6.5346e-03 (9.0624e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [59][ 50/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.004)	Loss 1.4610e-02 (8.8380e-03)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [59][ 60/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 2.6321e-02 (9.2863e-03)	Acc@1  98.44 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [59][ 70/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.003)	Loss 2.1515e-03 (8.9292e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [59][ 80/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.2306e-02 (9.0265e-03)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [59][ 90/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.003)	Loss 3.5725e-03 (8.7095e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [59][100/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.2070e-02 (9.6038e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [59][110/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.7593e-02 (9.3374e-03)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [59][120/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.1505e-02 (9.4363e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [59][130/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.9498e-03 (9.6056e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [59][140/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.9803e-03 (9.5943e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [59][150/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.9787e-03 (9.5115e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [59][160/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.9469e-03 (9.4109e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [59][170/391]	Time  0.128 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.6871e-02 (9.6110e-03)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [59][180/391]	Time  0.124 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.3079e-03 (9.6045e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [59][190/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0757e-02 (9.6566e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [59][200/391]	Time  0.125 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.5002e-03 (9.7657e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [59][210/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.8967e-03 (9.7429e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [59][220/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.4000e-02 (9.9818e-03)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [59][230/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.7853e-02 (1.0100e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [59][240/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.0742e-02 (1.0132e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [59][250/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.0894e-03 (1.0327e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [59][260/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.6839e-03 (1.0194e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [59][270/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.4098e-03 (1.0180e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [59][280/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.9547e-02 (1.0239e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [59][290/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.0132e-02 (1.0357e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [59][300/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.5199e-03 (1.0335e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [59][310/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.9171e-03 (1.0256e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [59][320/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.6686e-03 (1.0201e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [59][330/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.0506e-02 (1.0172e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [59][340/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.0430e-03 (1.0081e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [59][350/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 8.1100e-03 (1.0022e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [59][360/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 9.7809e-03 (1.0067e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [59][370/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.3560e-02 (1.0180e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [59][380/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.7166e-02 (1.0151e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [59][390/391]	Time  0.105 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.2887e-02 (1.0195e-02)	Acc@1  97.50 ( 99.77)	Acc@5 100.00 (100.00)
## e[59] optimizer.zero_grad (sum) time: 0.6845879554748535
## e[59]       loss.backward (sum) time: 14.261414051055908
## e[59]      optimizer.step (sum) time: 8.005749940872192
## epoch[59] training(only) time: 47.078030586242676
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 1.9080e-01 (1.9080e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 2.3352e-01 (2.3191e-01)	Acc@1  96.00 ( 94.00)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 4.1479e-01 (2.6380e-01)	Acc@1  88.00 ( 93.00)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.050 ( 0.052)	Loss 3.2324e-01 (2.7048e-01)	Acc@1  91.00 ( 93.03)	Acc@5  99.00 ( 99.61)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 3.5864e-01 (2.6843e-01)	Acc@1  88.00 ( 93.10)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.6199e-01 (2.7232e-01)	Acc@1  94.00 ( 92.98)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.9336e-01 (2.6519e-01)	Acc@1  98.00 ( 93.05)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 4.0356e-01 (2.6487e-01)	Acc@1  92.00 ( 93.11)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4368e-01 (2.6719e-01)	Acc@1  94.00 ( 93.11)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 2.9419e-01 (2.6660e-01)	Acc@1  94.00 ( 93.15)	Acc@5 100.00 ( 99.78)
 * Acc@1 93.190 Acc@5 99.800
### epoch[59] execution time: 52.03963327407837
EPOCH 60
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [60][  0/391]	Time  0.285 ( 0.285)	Data  0.149 ( 0.149)	Loss 6.4850e-03 (6.4850e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [60][ 10/391]	Time  0.117 ( 0.133)	Data  0.001 ( 0.014)	Loss 3.3234e-02 (1.0342e-02)	Acc@1  98.44 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [60][ 20/391]	Time  0.119 ( 0.127)	Data  0.001 ( 0.008)	Loss 4.5166e-03 (1.0628e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [60][ 30/391]	Time  0.128 ( 0.125)	Data  0.001 ( 0.006)	Loss 3.8967e-03 (1.0528e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [60][ 40/391]	Time  0.122 ( 0.123)	Data  0.001 ( 0.005)	Loss 2.8915e-02 (1.0171e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [60][ 50/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.004)	Loss 1.6113e-02 (1.0507e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [60][ 60/391]	Time  0.126 ( 0.122)	Data  0.001 ( 0.003)	Loss 2.4433e-03 (1.0303e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [60][ 70/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 3.6354e-03 (1.0288e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [60][ 80/391]	Time  0.124 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.2381e-03 (1.0067e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [60][ 90/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 8.0872e-03 (9.8589e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [60][100/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.2239e-03 (9.5348e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [60][110/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.1555e-02 (9.7414e-03)	Acc@1  98.44 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [60][120/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.6130e-03 (9.6102e-03)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [60][130/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.2507e-03 (9.4380e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [60][140/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0124e-02 (9.5266e-03)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [60][150/391]	Time  0.124 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.5930e-02 (9.5291e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [60][160/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.0493e-02 (9.5539e-03)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [60][170/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.8141e-03 (9.5878e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [60][180/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.0229e-03 (9.5334e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [60][190/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.4821e-02 (9.5736e-03)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [60][200/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.1452e-03 (9.4459e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [60][210/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.8087e-03 (9.3616e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [60][220/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.4850e-03 (9.3294e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [60][230/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.0120e-03 (9.5651e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [60][240/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.0321e-03 (9.5525e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [60][250/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 9.5596e-03 (9.5864e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [60][260/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.4557e-02 (9.6529e-03)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [60][270/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.1871e-02 (9.5949e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [60][280/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.5349e-03 (9.4806e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [60][290/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.3133e-03 (9.4343e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [60][300/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.5083e-02 (9.3473e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [60][310/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.7831e-03 (9.3632e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [60][320/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.1780e-02 (9.3505e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [60][330/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.5421e-02 (9.3625e-03)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [60][340/391]	Time  0.128 ( 0.120)	Data  0.001 ( 0.001)	Loss 8.0948e-03 (9.3502e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [60][350/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.7406e-03 (9.3827e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [60][360/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 9.2850e-03 (9.4178e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [60][370/391]	Time  0.130 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.4212e-03 (9.4851e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [60][380/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 7.8087e-03 (9.4048e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [60][390/391]	Time  0.108 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.2466e-02 (9.4510e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
## e[60] optimizer.zero_grad (sum) time: 0.6919252872467041
## e[60]       loss.backward (sum) time: 14.228473663330078
## e[60]      optimizer.step (sum) time: 8.074893951416016
## epoch[60] training(only) time: 47.12196636199951
# Switched to evaluate mode...
Test: [  0/100]	Time  0.180 ( 0.180)	Loss 2.0361e-01 (2.0361e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.056 ( 0.059)	Loss 2.3059e-01 (2.3356e-01)	Acc@1  97.00 ( 94.18)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.047 ( 0.053)	Loss 4.0283e-01 (2.6326e-01)	Acc@1  88.00 ( 93.33)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 3.1763e-01 (2.7045e-01)	Acc@1  91.00 ( 93.23)	Acc@5  99.00 ( 99.61)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 3.2324e-01 (2.6731e-01)	Acc@1  89.00 ( 93.17)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.6199e-01 (2.7105e-01)	Acc@1  94.00 ( 93.04)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 1.8445e-01 (2.6429e-01)	Acc@1  99.00 ( 93.11)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 4.1357e-01 (2.6327e-01)	Acc@1  92.00 ( 93.15)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.6040e-01 (2.6649e-01)	Acc@1  94.00 ( 93.12)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 3.0859e-01 (2.6547e-01)	Acc@1  94.00 ( 93.18)	Acc@5 100.00 ( 99.78)
 * Acc@1 93.270 Acc@5 99.800
### epoch[60] execution time: 52.06834673881531
EPOCH 61
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [61][  0/391]	Time  0.272 ( 0.272)	Data  0.144 ( 0.144)	Loss 1.2741e-02 (1.2741e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [61][ 10/391]	Time  0.118 ( 0.134)	Data  0.001 ( 0.014)	Loss 3.3779e-03 (9.5747e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [61][ 20/391]	Time  0.121 ( 0.128)	Data  0.001 ( 0.008)	Loss 5.7640e-03 (8.7942e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [61][ 30/391]	Time  0.117 ( 0.125)	Data  0.001 ( 0.006)	Loss 5.1270e-03 (8.6445e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [61][ 40/391]	Time  0.120 ( 0.124)	Data  0.001 ( 0.004)	Loss 5.7297e-03 (8.6127e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [61][ 50/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.004)	Loss 1.8661e-02 (8.5263e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [61][ 60/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 8.9798e-03 (8.5340e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [61][ 70/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 2.2278e-03 (9.2105e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [61][ 80/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 2.4490e-02 (9.3417e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [61][ 90/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 7.9117e-03 (9.5270e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [61][100/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.0817e-03 (9.5536e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [61][110/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.9897e-02 (9.5059e-03)	Acc@1  98.44 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [61][120/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.6068e-02 (9.7875e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [61][130/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.0621e-03 (9.4987e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [61][140/391]	Time  0.124 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.4654e-03 (9.7360e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [61][150/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.1711e-03 (9.6629e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [61][160/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.0049e-03 (9.7048e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [61][170/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.7344e-02 (9.6764e-03)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [61][180/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.5152e-02 (9.7251e-03)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [61][190/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.0325e-02 (9.7851e-03)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [61][200/391]	Time  0.128 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.2092e-03 (9.7279e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [61][210/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.2368e-03 (9.5551e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [61][220/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.5226e-03 (9.5868e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [61][230/391]	Time  0.128 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.4854e-02 (9.5196e-03)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [61][240/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 7.3586e-03 (9.3976e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [61][250/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.5238e-02 (9.3316e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [61][260/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.8409e-03 (9.3438e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [61][270/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.7384e-02 (9.4324e-03)	Acc@1  98.44 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [61][280/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.9166e-03 (9.3541e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [61][290/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.7918e-03 (9.3396e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [61][300/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.1803e-02 (9.3782e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [61][310/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.2947e-02 (9.3240e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [61][320/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.9507e-03 (9.3020e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [61][330/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.0536e-02 (9.2647e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [61][340/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.4708e-03 (9.2002e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [61][350/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.6505e-02 (9.2398e-03)	Acc@1  98.44 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [61][360/391]	Time  0.122 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.0501e-03 (9.1772e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [61][370/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.5191e-03 (9.2327e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [61][380/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.4136e-03 (9.2376e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [61][390/391]	Time  0.108 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.8357e-03 (9.2383e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
## e[61] optimizer.zero_grad (sum) time: 0.6857001781463623
## e[61]       loss.backward (sum) time: 14.283003091812134
## e[61]      optimizer.step (sum) time: 8.00651478767395
## epoch[61] training(only) time: 47.152074337005615
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 1.8689e-01 (1.8689e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 2.2937e-01 (2.3180e-01)	Acc@1  96.00 ( 94.36)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 4.2944e-01 (2.6475e-01)	Acc@1  88.00 ( 93.38)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 3.2520e-01 (2.7322e-01)	Acc@1  91.00 ( 93.23)	Acc@5  99.00 ( 99.61)
Test: [ 40/100]	Time  0.045 ( 0.051)	Loss 3.3618e-01 (2.7100e-01)	Acc@1  88.00 ( 93.32)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.6467e-01 (2.7481e-01)	Acc@1  95.00 ( 93.22)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.049 ( 0.050)	Loss 1.9653e-01 (2.6785e-01)	Acc@1  98.00 ( 93.23)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.050 ( 0.050)	Loss 3.9453e-01 (2.6746e-01)	Acc@1  92.00 ( 93.25)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4380e-01 (2.7018e-01)	Acc@1  95.00 ( 93.23)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 3.0518e-01 (2.6913e-01)	Acc@1  94.00 ( 93.26)	Acc@5 100.00 ( 99.76)
 * Acc@1 93.320 Acc@5 99.780
### epoch[61] execution time: 52.09646439552307
EPOCH 62
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [62][  0/391]	Time  0.283 ( 0.283)	Data  0.148 ( 0.148)	Loss 2.2621e-03 (2.2621e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [62][ 10/391]	Time  0.120 ( 0.135)	Data  0.001 ( 0.014)	Loss 3.4122e-03 (6.7931e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [62][ 20/391]	Time  0.118 ( 0.128)	Data  0.001 ( 0.008)	Loss 1.0681e-02 (7.4254e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [62][ 30/391]	Time  0.119 ( 0.125)	Data  0.001 ( 0.006)	Loss 2.9316e-03 (7.1449e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [62][ 40/391]	Time  0.118 ( 0.124)	Data  0.001 ( 0.004)	Loss 3.7727e-03 (7.1398e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [62][ 50/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.004)	Loss 1.2352e-02 (7.7849e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [62][ 60/391]	Time  0.129 ( 0.123)	Data  0.001 ( 0.003)	Loss 1.0933e-02 (7.8061e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [62][ 70/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 7.9498e-03 (8.0215e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [62][ 80/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.0025e-02 (8.2664e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [62][ 90/391]	Time  0.128 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.9745e-02 (8.4667e-03)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [62][100/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.1842e-03 (8.9010e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [62][110/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.5579e-02 (8.7830e-03)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [62][120/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.8415e-03 (8.7826e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [62][130/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.8735e-03 (9.1491e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [62][140/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.9166e-03 (9.1900e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [62][150/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.2084e-02 (9.2996e-03)	Acc@1  98.44 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [62][160/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.3403e-03 (9.1930e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [62][170/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.4278e-03 (9.3948e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [62][180/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.3191e-02 (9.3455e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [62][190/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.9967e-03 (9.4249e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [62][200/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.8899e-03 (9.4212e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [62][210/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.4403e-03 (9.4008e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [62][220/391]	Time  0.116 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.0959e-03 (9.3335e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [62][230/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.8823e-03 (9.2553e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [62][240/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.0921e-03 (9.4993e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [62][250/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.3588e-02 (9.4095e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [62][260/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.8120e-03 (9.3468e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [62][270/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.3297e-03 (9.2419e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [62][280/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.5228e-02 (9.1943e-03)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [62][290/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.6886e-02 (9.3077e-03)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [62][300/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 8.5144e-03 (9.2888e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [62][310/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.1161e-03 (9.2579e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [62][320/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.9809e-03 (9.3069e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [62][330/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.8207e-03 (9.4486e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [62][340/391]	Time  0.129 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.8128e-03 (9.4212e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [62][350/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.0499e-03 (9.4078e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [62][360/391]	Time  0.123 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.6463e-03 (9.4934e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [62][370/391]	Time  0.124 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.6812e-03 (9.4473e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [62][380/391]	Time  0.116 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.8523e-03 (9.4823e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [62][390/391]	Time  0.105 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.5515e-03 (9.4456e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
## e[62] optimizer.zero_grad (sum) time: 0.6810197830200195
## e[62]       loss.backward (sum) time: 14.30435299873352
## e[62]      optimizer.step (sum) time: 8.003398656845093
## epoch[62] training(only) time: 47.10724449157715
# Switched to evaluate mode...
Test: [  0/100]	Time  0.193 ( 0.193)	Loss 2.0898e-01 (2.0898e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.057 ( 0.061)	Loss 2.2913e-01 (2.3346e-01)	Acc@1  96.00 ( 94.27)	Acc@5 100.00 ( 99.55)
Test: [ 20/100]	Time  0.047 ( 0.054)	Loss 3.9160e-01 (2.6012e-01)	Acc@1  89.00 ( 93.62)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 2.8223e-01 (2.6722e-01)	Acc@1  91.00 ( 93.45)	Acc@5  99.00 ( 99.55)
Test: [ 40/100]	Time  0.049 ( 0.051)	Loss 3.3838e-01 (2.6577e-01)	Acc@1  88.00 ( 93.27)	Acc@5 100.00 ( 99.63)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.6541e-01 (2.6971e-01)	Acc@1  96.00 ( 93.14)	Acc@5 100.00 ( 99.67)
Test: [ 60/100]	Time  0.056 ( 0.050)	Loss 1.8286e-01 (2.6197e-01)	Acc@1  99.00 ( 93.21)	Acc@5 100.00 ( 99.72)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 4.2188e-01 (2.6217e-01)	Acc@1  92.00 ( 93.25)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4954e-01 (2.6582e-01)	Acc@1  94.00 ( 93.21)	Acc@5 100.00 ( 99.73)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 3.0371e-01 (2.6467e-01)	Acc@1  94.00 ( 93.27)	Acc@5 100.00 ( 99.75)
 * Acc@1 93.290 Acc@5 99.770
### epoch[62] execution time: 52.04080581665039
EPOCH 63
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [63][  0/391]	Time  0.276 ( 0.276)	Data  0.140 ( 0.140)	Loss 4.6654e-03 (4.6654e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [63][ 10/391]	Time  0.119 ( 0.134)	Data  0.001 ( 0.014)	Loss 3.6316e-03 (8.1624e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [63][ 20/391]	Time  0.118 ( 0.127)	Data  0.001 ( 0.008)	Loss 3.0197e-02 (1.1117e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [63][ 30/391]	Time  0.117 ( 0.124)	Data  0.001 ( 0.005)	Loss 2.2259e-03 (1.0485e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [63][ 40/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.004)	Loss 4.7913e-03 (9.9601e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [63][ 50/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.004)	Loss 4.6005e-03 (9.8406e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [63][ 60/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 5.5122e-03 (9.3569e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [63][ 70/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.003)	Loss 9.9792e-03 (9.4592e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [63][ 80/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.003)	Loss 7.4158e-03 (9.5957e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [63][ 90/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.3613e-03 (9.1475e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [63][100/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.7008e-03 (9.3092e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [63][110/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0498e-02 (9.4389e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [63][120/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.0763e-03 (9.3836e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [63][130/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.4817e-03 (9.3685e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [63][140/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.1223e-02 (9.4350e-03)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [63][150/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.1498e-03 (9.5774e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [63][160/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.3373e-03 (9.5513e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [63][170/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.7914e-02 (9.5808e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [63][180/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.6692e-03 (9.5206e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [63][190/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.2719e-03 (9.3848e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [63][200/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 9.8877e-03 (9.2729e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [63][210/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.5008e-03 (9.2020e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [63][220/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.7635e-03 (9.0701e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [63][230/391]	Time  0.128 ( 0.120)	Data  0.001 ( 0.002)	Loss 7.9422e-03 (9.0706e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [63][240/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.3245e-02 (9.1389e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [63][250/391]	Time  0.122 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.1580e-03 (9.1897e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [63][260/391]	Time  0.139 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.2873e-02 (9.2018e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [63][270/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.5978e-03 (9.2773e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [63][280/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.1978e-02 (9.2216e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [63][290/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.2781e-02 (9.2369e-03)	Acc@1  98.44 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [63][300/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 7.1678e-03 (9.3765e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [63][310/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 7.0686e-03 (9.2815e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [63][320/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.4163e-03 (9.2975e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [63][330/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.9553e-03 (9.2321e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [63][340/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.3313e-02 (9.2793e-03)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [63][350/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 9.3231e-03 (9.2462e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [63][360/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.5004e-02 (9.3867e-03)	Acc@1  98.44 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [63][370/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.7166e-02 (9.4998e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [63][380/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.6310e-03 (9.5067e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [63][390/391]	Time  0.106 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.7684e-03 (9.5838e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
## e[63] optimizer.zero_grad (sum) time: 0.684859037399292
## e[63]       loss.backward (sum) time: 14.238844156265259
## e[63]      optimizer.step (sum) time: 7.986174821853638
## epoch[63] training(only) time: 47.06887626647949
# Switched to evaluate mode...
Test: [  0/100]	Time  0.191 ( 0.191)	Loss 1.7273e-01 (1.7273e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 2.3450e-01 (2.2711e-01)	Acc@1  97.00 ( 94.18)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 4.1602e-01 (2.5809e-01)	Acc@1  89.00 ( 93.48)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 2.8687e-01 (2.6325e-01)	Acc@1  92.00 ( 93.39)	Acc@5  99.00 ( 99.68)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 3.4131e-01 (2.6321e-01)	Acc@1  88.00 ( 93.20)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.6980e-01 (2.6799e-01)	Acc@1  94.00 ( 93.06)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.8860e-01 (2.6120e-01)	Acc@1  97.00 ( 93.07)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 4.0088e-01 (2.6150e-01)	Acc@1  92.00 ( 93.11)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.3513e-01 (2.6335e-01)	Acc@1  94.00 ( 93.11)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 2.8735e-01 (2.6269e-01)	Acc@1  94.00 ( 93.18)	Acc@5 100.00 ( 99.81)
 * Acc@1 93.230 Acc@5 99.830
### epoch[63] execution time: 52.02027940750122
EPOCH 64
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [64][  0/391]	Time  0.275 ( 0.275)	Data  0.140 ( 0.140)	Loss 6.2447e-03 (6.2447e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [64][ 10/391]	Time  0.118 ( 0.135)	Data  0.001 ( 0.014)	Loss 6.0005e-03 (8.2228e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [64][ 20/391]	Time  0.119 ( 0.128)	Data  0.001 ( 0.008)	Loss 3.9787e-03 (1.2127e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [64][ 30/391]	Time  0.117 ( 0.125)	Data  0.001 ( 0.005)	Loss 5.6152e-03 (1.1641e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [64][ 40/391]	Time  0.117 ( 0.124)	Data  0.001 ( 0.004)	Loss 9.1858e-03 (1.0636e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [64][ 50/391]	Time  0.122 ( 0.123)	Data  0.001 ( 0.004)	Loss 1.9318e-02 (1.0689e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [64][ 60/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.003)	Loss 5.0316e-03 (1.0788e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [64][ 70/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 3.0701e-02 (1.0953e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [64][ 80/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 2.1606e-02 (1.0787e-02)	Acc@1  98.44 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [64][ 90/391]	Time  0.127 ( 0.122)	Data  0.001 ( 0.003)	Loss 6.2256e-03 (1.0463e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [64][100/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.6692e-03 (1.0790e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [64][110/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.2597e-03 (1.0490e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [64][120/391]	Time  0.130 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.1171e-03 (1.0417e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [64][130/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.3782e-03 (1.0308e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [64][140/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.7128e-03 (1.0216e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [64][150/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.9163e-03 (9.8424e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [64][160/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.6464e-02 (9.8530e-03)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [64][170/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.4376e-03 (9.7713e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [64][180/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.6921e-03 (9.8524e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [64][190/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.0847e-03 (9.8522e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [64][200/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.8305e-03 (9.6883e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [64][210/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.6114e-03 (9.5916e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [64][220/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.2997e-03 (9.5426e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [64][230/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.4061e-02 (9.5170e-03)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [64][240/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.2016e-03 (9.3450e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [64][250/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.7502e-02 (9.3177e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [64][260/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.6659e-03 (9.2749e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [64][270/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.3479e-03 (9.1337e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [64][280/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 8.7662e-03 (9.2386e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [64][290/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 7.8125e-03 (9.1859e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [64][300/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.2932e-03 (9.0970e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [64][310/391]	Time  0.128 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.7479e-03 (9.0531e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [64][320/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.2375e-02 (9.0633e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [64][330/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.6143e-02 (9.1791e-03)	Acc@1  97.66 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [64][340/391]	Time  0.123 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.9550e-03 (9.1204e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [64][350/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.5815e-03 (9.1355e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [64][360/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 7.9956e-03 (9.0447e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [64][370/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 8.4457e-03 (9.0400e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [64][380/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.1337e-02 (9.0532e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [64][390/391]	Time  0.109 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.3895e-02 (9.0780e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
## e[64] optimizer.zero_grad (sum) time: 0.685199499130249
## e[64]       loss.backward (sum) time: 14.27812647819519
## e[64]      optimizer.step (sum) time: 8.019545555114746
## epoch[64] training(only) time: 47.11598467826843
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 1.9775e-01 (1.9775e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 2.2412e-01 (2.3232e-01)	Acc@1  97.00 ( 94.27)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 3.9380e-01 (2.6152e-01)	Acc@1  89.00 ( 93.48)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 2.9834e-01 (2.6784e-01)	Acc@1  92.00 ( 93.42)	Acc@5  99.00 ( 99.68)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 3.3496e-01 (2.6738e-01)	Acc@1  89.00 ( 93.32)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.6235e-01 (2.7041e-01)	Acc@1  94.00 ( 93.18)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.048 ( 0.049)	Loss 1.8591e-01 (2.6439e-01)	Acc@1  98.00 ( 93.23)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 3.9307e-01 (2.6336e-01)	Acc@1  92.00 ( 93.28)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.6541e-01 (2.6580e-01)	Acc@1  93.00 ( 93.25)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 2.8906e-01 (2.6431e-01)	Acc@1  94.00 ( 93.32)	Acc@5 100.00 ( 99.80)
 * Acc@1 93.350 Acc@5 99.820
### epoch[64] execution time: 52.05464506149292
EPOCH 65
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [65][  0/391]	Time  0.276 ( 0.276)	Data  0.142 ( 0.142)	Loss 2.4776e-03 (2.4776e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [65][ 10/391]	Time  0.117 ( 0.134)	Data  0.001 ( 0.014)	Loss 8.5449e-03 (7.7005e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [65][ 20/391]	Time  0.120 ( 0.126)	Data  0.001 ( 0.008)	Loss 9.9030e-03 (1.0747e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [65][ 30/391]	Time  0.118 ( 0.124)	Data  0.001 ( 0.005)	Loss 5.9662e-03 (1.2050e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [65][ 40/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.004)	Loss 1.7731e-02 (1.1984e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [65][ 50/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.004)	Loss 7.7019e-03 (1.1947e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [65][ 60/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.003)	Loss 9.4604e-03 (1.2191e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [65][ 70/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.0436e-03 (1.2207e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [65][ 80/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.003)	Loss 1.5335e-02 (1.1769e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [65][ 90/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.7441e-03 (1.1472e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [65][100/391]	Time  0.130 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.2093e-02 (1.1126e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [65][110/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.4557e-02 (1.1010e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [65][120/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.5231e-03 (1.0945e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [65][130/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.0804e-03 (1.0834e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [65][140/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.4420e-02 (1.0591e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [65][150/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.2230e-02 (1.0746e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [65][160/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.0785e-03 (1.0470e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [65][170/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.0387e-03 (1.0392e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [65][180/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.8289e-03 (1.0232e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [65][190/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0605e-02 (1.0107e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [65][200/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.1575e-03 (9.9986e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [65][210/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.2560e-03 (9.8517e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [65][220/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.9084e-03 (9.9375e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [65][230/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.5472e-02 (9.8024e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [65][240/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.0507e-03 (9.7976e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [65][250/391]	Time  0.122 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.1307e-02 (9.8218e-03)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [65][260/391]	Time  0.125 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.3297e-03 (9.7764e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [65][270/391]	Time  0.127 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.7902e-03 (9.7563e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [65][280/391]	Time  0.122 ( 0.120)	Data  0.001 ( 0.001)	Loss 7.2632e-03 (9.7096e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [65][290/391]	Time  0.126 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.1040e-02 (9.6170e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [65][300/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.1122e-03 (9.5308e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [65][310/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.4567e-03 (9.5961e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [65][320/391]	Time  0.127 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.2992e-03 (9.5320e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [65][330/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.2131e-02 (9.5520e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [65][340/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 8.4000e-03 (9.5898e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [65][350/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.6451e-03 (9.5358e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [65][360/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.8779e-03 (9.6338e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [65][370/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.3092e-03 (9.5331e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [65][380/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 7.2899e-03 (9.4848e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [65][390/391]	Time  0.108 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.4474e-03 (9.3936e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
## e[65] optimizer.zero_grad (sum) time: 0.6792242527008057
## e[65]       loss.backward (sum) time: 14.300513744354248
## e[65]      optimizer.step (sum) time: 7.956253290176392
## epoch[65] training(only) time: 47.05784225463867
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 1.9006e-01 (1.9006e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 2.3804e-01 (2.2946e-01)	Acc@1  96.00 ( 94.00)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 3.9966e-01 (2.5891e-01)	Acc@1  88.00 ( 93.33)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 2.7881e-01 (2.6548e-01)	Acc@1  93.00 ( 93.35)	Acc@5  99.00 ( 99.61)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 3.3936e-01 (2.6399e-01)	Acc@1  88.00 ( 93.24)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.048 ( 0.050)	Loss 1.6028e-01 (2.6862e-01)	Acc@1  95.00 ( 93.10)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.8530e-01 (2.6125e-01)	Acc@1  99.00 ( 93.18)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 4.1455e-01 (2.6108e-01)	Acc@1  92.00 ( 93.20)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4429e-01 (2.6390e-01)	Acc@1  93.00 ( 93.15)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 2.9224e-01 (2.6293e-01)	Acc@1  94.00 ( 93.21)	Acc@5 100.00 ( 99.77)
 * Acc@1 93.270 Acc@5 99.790
### epoch[65] execution time: 52.01195287704468
EPOCH 66
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [66][  0/391]	Time  0.285 ( 0.285)	Data  0.142 ( 0.142)	Loss 4.1046e-03 (4.1046e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [66][ 10/391]	Time  0.116 ( 0.136)	Data  0.001 ( 0.014)	Loss 7.9803e-03 (9.5513e-03)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [66][ 20/391]	Time  0.120 ( 0.128)	Data  0.001 ( 0.008)	Loss 5.9624e-03 (8.7053e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [66][ 30/391]	Time  0.120 ( 0.125)	Data  0.001 ( 0.006)	Loss 5.4817e-03 (8.1617e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [66][ 40/391]	Time  0.120 ( 0.124)	Data  0.001 ( 0.004)	Loss 1.1665e-02 (8.4966e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [66][ 50/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.004)	Loss 4.7379e-03 (8.4356e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [66][ 60/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 3.5667e-03 (8.2819e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [66][ 70/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 5.1727e-03 (8.0973e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [66][ 80/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 5.8975e-03 (8.1373e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [66][ 90/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.003)	Loss 1.7385e-03 (8.1429e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [66][100/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.0599e-03 (8.2925e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [66][110/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.7406e-03 (8.3480e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [66][120/391]	Time  0.133 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.0959e-03 (8.3274e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [66][130/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.5275e-03 (8.5353e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [66][140/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.7814e-03 (8.9035e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [66][150/391]	Time  0.126 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.1629e-03 (9.3789e-03)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [66][160/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.6907e-03 (9.6392e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [66][170/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.1079e-03 (9.5137e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [66][180/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.6932e-02 (9.6548e-03)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [66][190/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.9258e-03 (9.5462e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [66][200/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.4023e-02 (9.5042e-03)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [66][210/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.7226e-03 (9.5279e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [66][220/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.8381e-03 (9.4598e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [66][230/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.3443e-02 (9.4258e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [66][240/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.2834e-03 (9.3871e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [66][250/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0529e-02 (9.3442e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [66][260/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.1957e-02 (9.3294e-03)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [66][270/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.4098e-03 (9.2723e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [66][280/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 8.6441e-03 (9.2261e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [66][290/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.4979e-02 (9.1997e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [66][300/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 9.0103e-03 (9.2151e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [66][310/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.6068e-02 (9.2128e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [66][320/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 9.8953e-03 (9.3171e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [66][330/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.5114e-03 (9.4449e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [66][340/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 8.6594e-03 (9.4026e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [66][350/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 8.6517e-03 (9.4311e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [66][360/391]	Time  0.122 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.1749e-03 (9.3402e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [66][370/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.0316e-03 (9.3651e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [66][380/391]	Time  0.131 ( 0.120)	Data  0.001 ( 0.001)	Loss 9.2926e-03 (9.4084e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [66][390/391]	Time  0.108 ( 0.120)	Data  0.001 ( 0.001)	Loss 9.7580e-03 (9.4181e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
## e[66] optimizer.zero_grad (sum) time: 0.6829385757446289
## e[66]       loss.backward (sum) time: 14.29607343673706
## e[66]      optimizer.step (sum) time: 8.002747774124146
## epoch[66] training(only) time: 47.13767719268799
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 1.9080e-01 (1.9080e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.059)	Loss 2.3328e-01 (2.3113e-01)	Acc@1  97.00 ( 94.27)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.045 ( 0.054)	Loss 3.9795e-01 (2.6136e-01)	Acc@1  88.00 ( 93.33)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 2.9614e-01 (2.6825e-01)	Acc@1  91.00 ( 93.26)	Acc@5  99.00 ( 99.58)
Test: [ 40/100]	Time  0.049 ( 0.050)	Loss 3.4326e-01 (2.6645e-01)	Acc@1  88.00 ( 93.17)	Acc@5 100.00 ( 99.63)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 1.6187e-01 (2.7040e-01)	Acc@1  95.00 ( 93.00)	Acc@5 100.00 ( 99.67)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.8835e-01 (2.6370e-01)	Acc@1  98.00 ( 93.08)	Acc@5 100.00 ( 99.72)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 4.0771e-01 (2.6279e-01)	Acc@1  92.00 ( 93.14)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4661e-01 (2.6576e-01)	Acc@1  95.00 ( 93.11)	Acc@5 100.00 ( 99.73)
Test: [ 90/100]	Time  0.046 ( 0.048)	Loss 3.1299e-01 (2.6466e-01)	Acc@1  94.00 ( 93.20)	Acc@5 100.00 ( 99.75)
 * Acc@1 93.260 Acc@5 99.770
### epoch[66] execution time: 52.0481436252594
EPOCH 67
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [67][  0/391]	Time  0.283 ( 0.283)	Data  0.148 ( 0.148)	Loss 3.0746e-02 (3.0746e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [67][ 10/391]	Time  0.129 ( 0.135)	Data  0.001 ( 0.014)	Loss 9.2316e-03 (1.2544e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [67][ 20/391]	Time  0.118 ( 0.127)	Data  0.001 ( 0.008)	Loss 7.6790e-03 (1.0918e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [67][ 30/391]	Time  0.117 ( 0.125)	Data  0.001 ( 0.006)	Loss 8.9340e-03 (1.0591e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [67][ 40/391]	Time  0.117 ( 0.123)	Data  0.001 ( 0.005)	Loss 8.3694e-03 (1.0055e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [67][ 50/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.004)	Loss 3.7270e-03 (1.0321e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [67][ 60/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 7.5340e-03 (1.0729e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [67][ 70/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.8585e-02 (1.0313e-02)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [67][ 80/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.003)	Loss 1.2291e-02 (1.0614e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [67][ 90/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.003)	Loss 2.9068e-03 (1.0266e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [67][100/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.7182e-03 (1.0354e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [67][110/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.3646e-03 (1.0170e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [67][120/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.6365e-03 (9.9941e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [67][130/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.3204e-03 (1.0002e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [67][140/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.9880e-03 (9.8706e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [67][150/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0590e-02 (9.7389e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [67][160/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.7334e-02 (9.6410e-03)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [67][170/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.0477e-02 (9.6391e-03)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [67][180/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.2239e-03 (9.7096e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [67][190/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0017e-02 (9.5079e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [67][200/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.3741e-02 (9.3176e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [67][210/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.2801e-03 (9.2926e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [67][220/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.8212e-03 (9.1984e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [67][230/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.2419e-03 (9.2624e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [67][240/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.7695e-03 (9.1946e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [67][250/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.2261e-03 (9.1795e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [67][260/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.1604e-02 (9.2323e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [67][270/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.0392e-03 (9.1501e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [67][280/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.3329e-02 (9.1827e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [67][290/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.6760e-03 (9.1698e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [67][300/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.5640e-02 (9.1829e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [67][310/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.2106e-03 (9.1176e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [67][320/391]	Time  0.130 ( 0.120)	Data  0.001 ( 0.001)	Loss 8.5068e-03 (9.1374e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [67][330/391]	Time  0.122 ( 0.120)	Data  0.001 ( 0.001)	Loss 7.6714e-03 (9.0349e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [67][340/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.0518e-03 (8.9231e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [67][350/391]	Time  0.127 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.1459e-02 (8.9372e-03)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [67][360/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.5618e-03 (8.9754e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [67][370/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.4374e-02 (8.9055e-03)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [67][380/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 7.0190e-03 (8.9259e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [67][390/391]	Time  0.106 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.4862e-02 (8.8628e-03)	Acc@1  98.75 ( 99.84)	Acc@5 100.00 (100.00)
## e[67] optimizer.zero_grad (sum) time: 0.6826701164245605
## e[67]       loss.backward (sum) time: 14.255833625793457
## e[67]      optimizer.step (sum) time: 8.005917072296143
## epoch[67] training(only) time: 47.05639600753784
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 2.0862e-01 (2.0862e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 2.4646e-01 (2.3288e-01)	Acc@1  96.00 ( 94.18)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.055 ( 0.054)	Loss 4.0967e-01 (2.6089e-01)	Acc@1  88.00 ( 93.38)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.048 ( 0.052)	Loss 3.0029e-01 (2.6677e-01)	Acc@1  92.00 ( 93.42)	Acc@5  99.00 ( 99.68)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 3.4155e-01 (2.6553e-01)	Acc@1  89.00 ( 93.39)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.048 ( 0.050)	Loss 1.7554e-01 (2.6917e-01)	Acc@1  94.00 ( 93.18)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.7749e-01 (2.6293e-01)	Acc@1  98.00 ( 93.23)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.055 ( 0.050)	Loss 4.1870e-01 (2.6293e-01)	Acc@1  92.00 ( 93.31)	Acc@5 100.00 ( 99.80)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.6113e-01 (2.6554e-01)	Acc@1  94.00 ( 93.27)	Acc@5 100.00 ( 99.81)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 3.1055e-01 (2.6420e-01)	Acc@1  94.00 ( 93.34)	Acc@5 100.00 ( 99.82)
 * Acc@1 93.380 Acc@5 99.840
### epoch[67] execution time: 52.01985955238342
EPOCH 68
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [68][  0/391]	Time  0.283 ( 0.283)	Data  0.151 ( 0.151)	Loss 1.0963e-02 (1.0963e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [68][ 10/391]	Time  0.117 ( 0.134)	Data  0.001 ( 0.015)	Loss 1.0796e-02 (9.7877e-03)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [68][ 20/391]	Time  0.117 ( 0.127)	Data  0.001 ( 0.008)	Loss 1.5821e-03 (9.7584e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [68][ 30/391]	Time  0.118 ( 0.125)	Data  0.001 ( 0.006)	Loss 8.9035e-03 (8.6874e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [68][ 40/391]	Time  0.120 ( 0.124)	Data  0.001 ( 0.005)	Loss 1.4542e-02 (9.1395e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [68][ 50/391]	Time  0.125 ( 0.123)	Data  0.001 ( 0.004)	Loss 9.8343e-03 (9.2131e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [68][ 60/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.003)	Loss 5.9052e-03 (9.1019e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [68][ 70/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 8.7204e-03 (9.1058e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [68][ 80/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 7.7324e-03 (8.8540e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [68][ 90/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 6.5765e-03 (9.0886e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [68][100/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.3798e-03 (8.9760e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [68][110/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.3189e-03 (8.8410e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [68][120/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.5845e-03 (8.7520e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [68][130/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.0142e-02 (8.7270e-03)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [68][140/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.2222e-02 (8.6077e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [68][150/391]	Time  0.124 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.1877e-03 (8.5252e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [68][160/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0529e-02 (8.5075e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [68][170/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0139e-02 (8.4437e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [68][180/391]	Time  0.126 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.5678e-03 (8.3460e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [68][190/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.1940e-03 (8.3811e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [68][200/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.0826e-02 (8.3680e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [68][210/391]	Time  0.133 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.0735e-03 (8.2776e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [68][220/391]	Time  0.123 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.2232e-02 (8.3128e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [68][230/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.9248e-03 (8.3556e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [68][240/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 7.6180e-03 (8.6493e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [68][250/391]	Time  0.122 ( 0.120)	Data  0.001 ( 0.002)	Loss 9.8648e-03 (8.7767e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [68][260/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.4011e-03 (8.8382e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [68][270/391]	Time  0.125 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.9182e-03 (8.7796e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [68][280/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.7542e-02 (8.8335e-03)	Acc@1  98.44 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [68][290/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.3422e-02 (8.8850e-03)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [68][300/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.4412e-02 (9.0425e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [68][310/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.7823e-03 (9.0064e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [68][320/391]	Time  0.122 ( 0.120)	Data  0.001 ( 0.001)	Loss 7.5531e-03 (8.9834e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [68][330/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.4447e-03 (8.9095e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [68][340/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.9635e-03 (8.8709e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [68][350/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.8605e-03 (8.8671e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [68][360/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.1836e-03 (8.8874e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [68][370/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.0594e-02 (8.9625e-03)	Acc@1  98.44 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [68][380/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.0131e-03 (8.9390e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [68][390/391]	Time  0.109 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.3596e-03 (8.9224e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
## e[68] optimizer.zero_grad (sum) time: 0.6840288639068604
## e[68]       loss.backward (sum) time: 14.278756380081177
## e[68]      optimizer.step (sum) time: 8.002581596374512
## epoch[68] training(only) time: 47.04992198944092
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 1.8054e-01 (1.8054e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.060)	Loss 2.2766e-01 (2.2953e-01)	Acc@1  97.00 ( 94.18)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.047 ( 0.054)	Loss 4.2188e-01 (2.5976e-01)	Acc@1  88.00 ( 93.48)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 2.6685e-01 (2.6436e-01)	Acc@1  91.00 ( 93.39)	Acc@5  99.00 ( 99.65)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 3.3447e-01 (2.6412e-01)	Acc@1  89.00 ( 93.24)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.5820e-01 (2.6823e-01)	Acc@1  95.00 ( 93.12)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.9897e-01 (2.6169e-01)	Acc@1  97.00 ( 93.15)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 3.9746e-01 (2.6141e-01)	Acc@1  92.00 ( 93.18)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4453e-01 (2.6329e-01)	Acc@1  94.00 ( 93.19)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 3.1030e-01 (2.6251e-01)	Acc@1  94.00 ( 93.25)	Acc@5 100.00 ( 99.81)
 * Acc@1 93.300 Acc@5 99.830
### epoch[68] execution time: 51.99307918548584
EPOCH 69
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [69][  0/391]	Time  0.268 ( 0.268)	Data  0.141 ( 0.141)	Loss 1.9363e-02 (1.9363e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [69][ 10/391]	Time  0.134 ( 0.134)	Data  0.001 ( 0.014)	Loss 1.1879e-02 (9.9366e-03)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [69][ 20/391]	Time  0.120 ( 0.127)	Data  0.001 ( 0.008)	Loss 2.8061e-02 (1.1380e-02)	Acc@1  98.44 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [69][ 30/391]	Time  0.119 ( 0.125)	Data  0.001 ( 0.005)	Loss 5.3673e-03 (9.9770e-03)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [69][ 40/391]	Time  0.130 ( 0.124)	Data  0.001 ( 0.004)	Loss 2.5463e-03 (8.8975e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [69][ 50/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.004)	Loss 3.5820e-03 (8.2923e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [69][ 60/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.003)	Loss 3.5000e-03 (8.0517e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [69][ 70/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.1046e-03 (8.1636e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [69][ 80/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.003)	Loss 1.3596e-02 (8.3430e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [69][ 90/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.003)	Loss 5.4932e-03 (8.1459e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [69][100/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.1034e-03 (8.1097e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [69][110/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.1711e-02 (8.1007e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [69][120/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.6569e-03 (8.2492e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [69][130/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.2223e-03 (8.2497e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [69][140/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.2390e-02 (8.3255e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [69][150/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.4937e-03 (8.2870e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [69][160/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.7379e-03 (8.1371e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [69][170/391]	Time  0.123 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.2726e-02 (8.3527e-03)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [69][180/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 7.8430e-03 (8.1882e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [69][190/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 7.7095e-03 (8.2194e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [69][200/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.3526e-03 (8.4900e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [69][210/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 8.0032e-03 (8.4554e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [69][220/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.5389e-03 (8.5427e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [69][230/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 7.8125e-03 (8.5836e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [69][240/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 7.8735e-03 (8.4568e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [69][250/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 9.5749e-03 (8.3859e-03)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [69][260/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.5242e-03 (8.3044e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [69][270/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.5396e-02 (8.4318e-03)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [69][280/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.7212e-03 (8.3903e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [69][290/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.4163e-03 (8.5492e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [69][300/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.5635e-03 (8.4840e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [69][310/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.3771e-02 (8.4881e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [69][320/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.2376e-03 (8.5377e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [69][330/391]	Time  0.122 ( 0.120)	Data  0.001 ( 0.001)	Loss 7.5722e-03 (8.5271e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [69][340/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.0321e-03 (8.5827e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [69][350/391]	Time  0.128 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.9640e-03 (8.5639e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [69][360/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.4676e-03 (8.5355e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [69][370/391]	Time  0.131 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.3041e-03 (8.5815e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [69][380/391]	Time  0.127 ( 0.120)	Data  0.001 ( 0.001)	Loss 7.1564e-03 (8.5876e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [69][390/391]	Time  0.102 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.3756e-02 (8.6339e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
## e[69] optimizer.zero_grad (sum) time: 0.6916329860687256
## e[69]       loss.backward (sum) time: 14.198310852050781
## e[69]      optimizer.step (sum) time: 8.011820077896118
## epoch[69] training(only) time: 47.00979995727539
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 2.0886e-01 (2.0886e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 2.2156e-01 (2.3486e-01)	Acc@1  96.00 ( 94.45)	Acc@5 100.00 ( 99.55)
Test: [ 20/100]	Time  0.046 ( 0.053)	Loss 3.9941e-01 (2.6599e-01)	Acc@1  89.00 ( 93.52)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 3.0078e-01 (2.7219e-01)	Acc@1  91.00 ( 93.16)	Acc@5  99.00 ( 99.58)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 3.4424e-01 (2.6932e-01)	Acc@1  88.00 ( 93.15)	Acc@5 100.00 ( 99.66)
Test: [ 50/100]	Time  0.065 ( 0.050)	Loss 1.5515e-01 (2.7288e-01)	Acc@1  96.00 ( 93.04)	Acc@5 100.00 ( 99.69)
Test: [ 60/100]	Time  0.049 ( 0.050)	Loss 1.8823e-01 (2.6588e-01)	Acc@1  98.00 ( 93.13)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 4.0796e-01 (2.6456e-01)	Acc@1  92.00 ( 93.21)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 1.6370e-01 (2.6843e-01)	Acc@1  94.00 ( 93.19)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 3.1934e-01 (2.6713e-01)	Acc@1  94.00 ( 93.24)	Acc@5 100.00 ( 99.75)
 * Acc@1 93.290 Acc@5 99.770
### epoch[69] execution time: 51.977853298187256
EPOCH 70
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [70][  0/391]	Time  0.284 ( 0.284)	Data  0.150 ( 0.150)	Loss 5.3940e-03 (5.3940e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [70][ 10/391]	Time  0.117 ( 0.135)	Data  0.001 ( 0.015)	Loss 5.3749e-03 (8.7970e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [70][ 20/391]	Time  0.121 ( 0.128)	Data  0.001 ( 0.008)	Loss 4.4403e-03 (1.0478e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [70][ 30/391]	Time  0.120 ( 0.125)	Data  0.001 ( 0.006)	Loss 2.5864e-03 (9.5838e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [70][ 40/391]	Time  0.119 ( 0.124)	Data  0.001 ( 0.005)	Loss 5.1346e-03 (9.5635e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [70][ 50/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.004)	Loss 3.2776e-02 (9.7271e-03)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [70][ 60/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.003)	Loss 5.0468e-03 (9.8803e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [70][ 70/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.6403e-02 (9.8413e-03)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [70][ 80/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.003)	Loss 5.4855e-03 (9.8132e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [70][ 90/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.003)	Loss 7.4081e-03 (9.8558e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [70][100/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.003)	Loss 1.2665e-02 (9.7052e-03)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [70][110/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.0959e-03 (9.9554e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [70][120/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.7324e-03 (9.6455e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [70][130/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.8283e-03 (9.8417e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [70][140/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.5825e-03 (9.5849e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [70][150/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.9175e-02 (9.6686e-03)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [70][160/391]	Time  0.116 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.4550e-03 (9.5555e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [70][170/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0811e-02 (9.4935e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [70][180/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.9869e-03 (9.2789e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [70][190/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.1765e-03 (9.1433e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [70][200/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.6989e-03 (9.0931e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [70][210/391]	Time  0.128 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.6846e-02 (9.0659e-03)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [70][220/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.5617e-02 (9.1902e-03)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [70][230/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.0894e-03 (9.1328e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [70][240/391]	Time  0.129 ( 0.120)	Data  0.001 ( 0.002)	Loss 9.7351e-03 (9.0859e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [70][250/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.5614e-02 (9.2443e-03)	Acc@1  97.66 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [70][260/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.5237e-03 (9.1310e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [70][270/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 9.6970e-03 (9.1470e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [70][280/391]	Time  0.123 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.3379e-03 (9.2018e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [70][290/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.7084e-02 (9.2941e-03)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [70][300/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.9275e-03 (9.4090e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [70][310/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.4518e-03 (9.5453e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [70][320/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.6541e-02 (9.5010e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [70][330/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.3588e-02 (9.4962e-03)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [70][340/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.4519e-02 (9.5661e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [70][350/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.1586e-03 (9.4530e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [70][360/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.4970e-03 (9.4830e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [70][370/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.1887e-02 (9.4212e-03)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [70][380/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 9.5520e-03 (9.4906e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [70][390/391]	Time  0.102 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.1389e-03 (9.3939e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
## e[70] optimizer.zero_grad (sum) time: 0.692176103591919
## e[70]       loss.backward (sum) time: 14.234089136123657
## e[70]      optimizer.step (sum) time: 8.070132970809937
## epoch[70] training(only) time: 47.10525560379028
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 1.8774e-01 (1.8774e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.045 ( 0.059)	Loss 2.2510e-01 (2.2793e-01)	Acc@1  96.00 ( 94.00)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.046 ( 0.053)	Loss 4.0283e-01 (2.5741e-01)	Acc@1  89.00 ( 93.10)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.046 ( 0.051)	Loss 2.6782e-01 (2.6357e-01)	Acc@1  92.00 ( 93.16)	Acc@5  99.00 ( 99.61)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 3.5376e-01 (2.6288e-01)	Acc@1  89.00 ( 93.12)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.6199e-01 (2.6712e-01)	Acc@1  95.00 ( 92.98)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.9116e-01 (2.5951e-01)	Acc@1  99.00 ( 93.10)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 4.1284e-01 (2.5939e-01)	Acc@1  92.00 ( 93.15)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 1.4209e-01 (2.6178e-01)	Acc@1  93.00 ( 93.15)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.046 ( 0.048)	Loss 2.9956e-01 (2.6074e-01)	Acc@1  94.00 ( 93.21)	Acc@5 100.00 ( 99.78)
 * Acc@1 93.270 Acc@5 99.800
### epoch[70] execution time: 52.04064464569092
EPOCH 71
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [71][  0/391]	Time  0.294 ( 0.294)	Data  0.137 ( 0.137)	Loss 5.2567e-03 (5.2567e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [71][ 10/391]	Time  0.119 ( 0.135)	Data  0.001 ( 0.013)	Loss 5.6114e-03 (7.0785e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [71][ 20/391]	Time  0.118 ( 0.128)	Data  0.001 ( 0.007)	Loss 4.0665e-03 (6.6375e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [71][ 30/391]	Time  0.117 ( 0.125)	Data  0.001 ( 0.005)	Loss 2.3746e-03 (6.8098e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [71][ 40/391]	Time  0.122 ( 0.124)	Data  0.001 ( 0.004)	Loss 7.1068e-03 (7.4218e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [71][ 50/391]	Time  0.121 ( 0.123)	Data  0.001 ( 0.004)	Loss 7.0686e-03 (7.3381e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [71][ 60/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 7.5760e-03 (7.4740e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [71][ 70/391]	Time  0.126 ( 0.122)	Data  0.001 ( 0.003)	Loss 9.9335e-03 (7.7418e-03)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [71][ 80/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.5967e-03 (7.4075e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [71][ 90/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.4507e-03 (7.4178e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [71][100/391]	Time  0.125 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.5215e-03 (7.6519e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [71][110/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.8553e-03 (7.4835e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [71][120/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.7120e-02 (7.6135e-03)	Acc@1  98.44 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [71][130/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.3847e-03 (7.7273e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [71][140/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.0942e-03 (7.8302e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [71][150/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.0828e-02 (8.1984e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [71][160/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.1199e-03 (8.3238e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [71][170/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.8921e-03 (8.3390e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [71][180/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.5387e-03 (8.4504e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [71][190/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.1255e-02 (8.5811e-03)	Acc@1  98.44 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [71][200/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.0245e-03 (8.5218e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [71][210/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 7.1449e-03 (8.6266e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [71][220/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.3636e-02 (8.7505e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [71][230/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.9330e-03 (8.7452e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [71][240/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.1836e-03 (8.6875e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [71][250/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.1500e-02 (8.6974e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [71][260/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.0577e-03 (8.6469e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [71][270/391]	Time  0.122 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.8981e-03 (8.6167e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [71][280/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.0437e-02 (8.6067e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [71][290/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 9.4528e-03 (8.5913e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [71][300/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.1078e-02 (8.5823e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [71][310/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.2360e-02 (8.5549e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [71][320/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.7556e-03 (8.5976e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [71][330/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.3858e-03 (8.5911e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [71][340/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.0178e-02 (8.6417e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [71][350/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 7.4844e-03 (8.5754e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [71][360/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.5665e-02 (8.6059e-03)	Acc@1  98.44 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [71][370/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.7233e-03 (8.5672e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [71][380/391]	Time  0.129 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.2623e-02 (8.7452e-03)	Acc@1  98.44 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [71][390/391]	Time  0.105 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.2680e-02 (8.7182e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
## e[71] optimizer.zero_grad (sum) time: 0.6842505931854248
## e[71]       loss.backward (sum) time: 14.231872320175171
## e[71]      optimizer.step (sum) time: 8.02802300453186
## epoch[71] training(only) time: 47.02965784072876
# Switched to evaluate mode...
Test: [  0/100]	Time  0.181 ( 0.181)	Loss 1.6809e-01 (1.6809e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.049 ( 0.061)	Loss 2.3535e-01 (2.2594e-01)	Acc@1  96.00 ( 94.09)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 4.2432e-01 (2.5977e-01)	Acc@1  88.00 ( 93.38)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 2.7979e-01 (2.6661e-01)	Acc@1  92.00 ( 93.29)	Acc@5  99.00 ( 99.61)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 3.3740e-01 (2.6502e-01)	Acc@1  88.00 ( 93.24)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.049 ( 0.050)	Loss 1.6516e-01 (2.6968e-01)	Acc@1  94.00 ( 93.06)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 2.0068e-01 (2.6273e-01)	Acc@1  96.00 ( 93.11)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 4.2285e-01 (2.6287e-01)	Acc@1  91.00 ( 93.18)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4294e-01 (2.6471e-01)	Acc@1  94.00 ( 93.17)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 2.9590e-01 (2.6378e-01)	Acc@1  94.00 ( 93.19)	Acc@5 100.00 ( 99.78)
 * Acc@1 93.240 Acc@5 99.800
### epoch[71] execution time: 51.98670554161072
EPOCH 72
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [72][  0/391]	Time  0.297 ( 0.297)	Data  0.159 ( 0.159)	Loss 9.4986e-03 (9.4986e-03)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [72][ 10/391]	Time  0.118 ( 0.137)	Data  0.001 ( 0.015)	Loss 5.6496e-03 (7.1803e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [72][ 20/391]	Time  0.120 ( 0.129)	Data  0.001 ( 0.008)	Loss 6.6681e-03 (8.2209e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [72][ 30/391]	Time  0.121 ( 0.126)	Data  0.001 ( 0.006)	Loss 2.3823e-03 (7.5043e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [72][ 40/391]	Time  0.118 ( 0.124)	Data  0.001 ( 0.005)	Loss 3.2711e-03 (7.8594e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [72][ 50/391]	Time  0.119 ( 0.124)	Data  0.001 ( 0.004)	Loss 3.2692e-03 (7.9486e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [72][ 60/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.004)	Loss 2.8214e-02 (8.4492e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [72][ 70/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.003)	Loss 1.7405e-03 (8.0902e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [72][ 80/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.7776e-03 (7.8478e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [72][ 90/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 2.9175e-02 (8.1195e-03)	Acc@1  98.44 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [72][100/391]	Time  0.116 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.4136e-03 (8.1289e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [72][110/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.8665e-03 (8.2557e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [72][120/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.3169e-03 (8.5031e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [72][130/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.9005e-03 (8.3217e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [72][140/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0201e-02 (8.5326e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [72][150/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.8637e-03 (8.2829e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [72][160/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.3793e-03 (8.3741e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [72][170/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.4275e-03 (8.5050e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [72][180/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.6321e-02 (8.7944e-03)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [72][190/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.5213e-02 (8.7600e-03)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [72][200/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.2229e-03 (8.7521e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [72][210/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.1086e-02 (8.8767e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [72][220/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.5700e-03 (8.8285e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [72][230/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.7673e-03 (8.7608e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [72][240/391]	Time  0.130 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.4290e-02 (8.7920e-03)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [72][250/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.6469e-03 (8.7155e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [72][260/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.0353e-02 (8.6556e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [72][270/391]	Time  0.126 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.1017e-02 (8.6475e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [72][280/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.5556e-02 (8.5884e-03)	Acc@1  98.44 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [72][290/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.2077e-02 (8.5593e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [72][300/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.4163e-03 (8.5960e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [72][310/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.2003e-02 (8.5654e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [72][320/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.3550e-03 (8.4950e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [72][330/391]	Time  0.116 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.2817e-02 (8.4725e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [72][340/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.9019e-03 (8.5950e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [72][350/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.4278e-03 (8.6614e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [72][360/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.7294e-03 (8.6439e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [72][370/391]	Time  0.116 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.8890e-02 (8.6274e-03)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [72][380/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.0353e-02 (8.5677e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [72][390/391]	Time  0.112 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.0054e-03 (8.5307e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
## e[72] optimizer.zero_grad (sum) time: 0.690054178237915
## e[72]       loss.backward (sum) time: 14.233086347579956
## e[72]      optimizer.step (sum) time: 7.982622146606445
## epoch[72] training(only) time: 47.085628032684326
# Switched to evaluate mode...
Test: [  0/100]	Time  0.175 ( 0.175)	Loss 1.7480e-01 (1.7480e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.049 ( 0.058)	Loss 2.3926e-01 (2.2580e-01)	Acc@1  96.00 ( 94.27)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.046 ( 0.053)	Loss 4.1235e-01 (2.5695e-01)	Acc@1  88.00 ( 93.38)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.059 ( 0.052)	Loss 2.8149e-01 (2.6424e-01)	Acc@1  92.00 ( 93.29)	Acc@5  99.00 ( 99.61)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 3.3765e-01 (2.6267e-01)	Acc@1  88.00 ( 93.24)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.6174e-01 (2.6729e-01)	Acc@1  95.00 ( 93.08)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.049 ( 0.049)	Loss 1.8970e-01 (2.6019e-01)	Acc@1  99.00 ( 93.20)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 4.1821e-01 (2.6001e-01)	Acc@1  92.00 ( 93.27)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.070 ( 0.049)	Loss 1.4453e-01 (2.6267e-01)	Acc@1  94.00 ( 93.25)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.049 ( 0.049)	Loss 3.0396e-01 (2.6154e-01)	Acc@1  94.00 ( 93.31)	Acc@5 100.00 ( 99.78)
 * Acc@1 93.350 Acc@5 99.800
### epoch[72] execution time: 52.04700231552124
EPOCH 73
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [73][  0/391]	Time  0.295 ( 0.295)	Data  0.158 ( 0.158)	Loss 1.1887e-02 (1.1887e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [73][ 10/391]	Time  0.122 ( 0.136)	Data  0.001 ( 0.015)	Loss 1.1627e-02 (8.7722e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [73][ 20/391]	Time  0.117 ( 0.128)	Data  0.001 ( 0.008)	Loss 2.3727e-03 (8.2639e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [73][ 30/391]	Time  0.121 ( 0.126)	Data  0.001 ( 0.006)	Loss 2.0096e-02 (8.2171e-03)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [73][ 40/391]	Time  0.119 ( 0.124)	Data  0.001 ( 0.005)	Loss 2.5848e-02 (8.0176e-03)	Acc@1  98.44 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [73][ 50/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.004)	Loss 9.5444e-03 (8.6530e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [73][ 60/391]	Time  0.121 ( 0.123)	Data  0.001 ( 0.004)	Loss 5.5466e-03 (8.1021e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [73][ 70/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.5495e-02 (8.1707e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [73][ 80/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.0109e-02 (7.9386e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [73][ 90/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 6.2027e-03 (7.9386e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [73][100/391]	Time  0.134 ( 0.122)	Data  0.001 ( 0.003)	Loss 9.4681e-03 (8.1188e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [73][110/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.8888e-03 (8.1078e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [73][120/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.7438e-03 (8.6014e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [73][130/391]	Time  0.127 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.5450e-03 (8.5080e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [73][140/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.8180e-03 (8.5818e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [73][150/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.5242e-03 (8.5121e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [73][160/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.8700e-03 (8.4867e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [73][170/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.9640e-03 (8.5394e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [73][180/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.6403e-02 (8.4078e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [73][190/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.5038e-03 (8.3691e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [73][200/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.3253e-03 (8.5788e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [73][210/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.1270e-03 (8.4410e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [73][220/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.4512e-03 (8.5283e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [73][230/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.7120e-02 (8.5341e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [73][240/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.5749e-03 (8.5536e-03)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [73][250/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.4088e-02 (8.7205e-03)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [73][260/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.1324e-03 (8.6560e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [73][270/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.5319e-03 (8.5594e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [73][280/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.7496e-02 (8.5862e-03)	Acc@1  98.44 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [73][290/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 9.0256e-03 (8.6934e-03)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [73][300/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.6617e-02 (8.6922e-03)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [73][310/391]	Time  0.123 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.6496e-03 (8.7107e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [73][320/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 8.5068e-03 (8.6943e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [73][330/391]	Time  0.123 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.8790e-03 (8.6594e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [73][340/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.0278e-03 (8.6546e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [73][350/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.4468e-03 (8.6872e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [73][360/391]	Time  0.131 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.2650e-02 (8.7100e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [73][370/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.8826e-03 (8.7539e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [73][380/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 7.4387e-03 (8.7268e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [73][390/391]	Time  0.104 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.2599e-03 (8.7479e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
## e[73] optimizer.zero_grad (sum) time: 0.6781904697418213
## e[73]       loss.backward (sum) time: 14.30186152458191
## e[73]      optimizer.step (sum) time: 8.007820129394531
## epoch[73] training(only) time: 47.135711908340454
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 1.7786e-01 (1.7786e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 2.3584e-01 (2.2672e-01)	Acc@1  96.00 ( 94.27)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.049 ( 0.054)	Loss 4.1333e-01 (2.5881e-01)	Acc@1  88.00 ( 93.33)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 2.8613e-01 (2.6477e-01)	Acc@1  91.00 ( 93.16)	Acc@5  99.00 ( 99.68)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 3.5083e-01 (2.6345e-01)	Acc@1  88.00 ( 93.12)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.6211e-01 (2.6808e-01)	Acc@1  94.00 ( 93.02)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.9714e-01 (2.6144e-01)	Acc@1  97.00 ( 93.07)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 4.0552e-01 (2.6103e-01)	Acc@1  92.00 ( 93.13)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4758e-01 (2.6331e-01)	Acc@1  94.00 ( 93.16)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.050 ( 0.049)	Loss 3.0029e-01 (2.6244e-01)	Acc@1  94.00 ( 93.20)	Acc@5 100.00 ( 99.80)
 * Acc@1 93.250 Acc@5 99.820
### epoch[73] execution time: 52.096354722976685
EPOCH 74
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [74][  0/391]	Time  0.279 ( 0.279)	Data  0.148 ( 0.148)	Loss 5.9547e-03 (5.9547e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [74][ 10/391]	Time  0.120 ( 0.135)	Data  0.001 ( 0.014)	Loss 2.6321e-02 (8.9854e-03)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [74][ 20/391]	Time  0.118 ( 0.128)	Data  0.001 ( 0.008)	Loss 3.7079e-02 (9.7115e-03)	Acc@1  98.44 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [74][ 30/391]	Time  0.120 ( 0.125)	Data  0.001 ( 0.006)	Loss 8.2932e-03 (9.0965e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [74][ 40/391]	Time  0.121 ( 0.124)	Data  0.001 ( 0.005)	Loss 4.7874e-03 (8.0742e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [74][ 50/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.004)	Loss 9.0942e-03 (7.7246e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [74][ 60/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.003)	Loss 3.6297e-03 (7.7558e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [74][ 70/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.003)	Loss 2.5970e-02 (8.0070e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [74][ 80/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 2.5063e-03 (8.0891e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [74][ 90/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.003)	Loss 1.1635e-02 (8.0129e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [74][100/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.8706e-03 (8.3506e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [74][110/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.2136e-03 (8.1234e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [74][120/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.5617e-02 (8.0815e-03)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [74][130/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.2065e-03 (7.9641e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [74][140/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.0103e-03 (7.8723e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [74][150/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.2385e-02 (8.0788e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [74][160/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.4910e-03 (8.1347e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [74][170/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.8877e-03 (8.2278e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [74][180/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.2043e-03 (8.0656e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [74][190/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.2611e-02 (8.0003e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [74][200/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.6229e-03 (7.9470e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [74][210/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.5128e-03 (8.0428e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [74][220/391]	Time  0.123 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.9400e-03 (8.0880e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [74][230/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.9095e-03 (7.9795e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [74][240/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.4250e-03 (8.0133e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [74][250/391]	Time  0.123 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.3453e-02 (8.2778e-03)	Acc@1  98.44 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [74][260/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.0163e-03 (8.1899e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [74][270/391]	Time  0.134 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.0283e-03 (8.3271e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [74][280/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.1057e-03 (8.3489e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [74][290/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.6530e-02 (8.5093e-03)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [74][300/391]	Time  0.128 ( 0.120)	Data  0.001 ( 0.002)	Loss 7.6599e-03 (8.4921e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [74][310/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 8.7967e-03 (8.5643e-03)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [74][320/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.7065e-03 (8.4984e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [74][330/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.6076e-03 (8.5395e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [74][340/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.5542e-03 (8.5166e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [74][350/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.9695e-03 (8.5515e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [74][360/391]	Time  0.123 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.5804e-03 (8.6143e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [74][370/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.6379e-03 (8.6050e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [74][380/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.9591e-03 (8.6089e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [74][390/391]	Time  0.107 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.1684e-03 (8.5697e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
## e[74] optimizer.zero_grad (sum) time: 0.6901929378509521
## e[74]       loss.backward (sum) time: 14.224029302597046
## e[74]      optimizer.step (sum) time: 8.011985540390015
## epoch[74] training(only) time: 47.07548809051514
# Switched to evaluate mode...
Test: [  0/100]	Time  0.181 ( 0.181)	Loss 1.8103e-01 (1.8103e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.045 ( 0.059)	Loss 2.4561e-01 (2.2517e-01)	Acc@1  96.00 ( 94.18)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.046 ( 0.053)	Loss 4.0308e-01 (2.5545e-01)	Acc@1  88.00 ( 93.33)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.046 ( 0.051)	Loss 2.7222e-01 (2.6254e-01)	Acc@1  92.00 ( 93.23)	Acc@5  99.00 ( 99.65)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 3.2031e-01 (2.6173e-01)	Acc@1  89.00 ( 93.15)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.6479e-01 (2.6559e-01)	Acc@1  95.00 ( 93.02)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.7712e-01 (2.5881e-01)	Acc@1  99.00 ( 93.10)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 4.0332e-01 (2.5833e-01)	Acc@1  92.00 ( 93.14)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.056 ( 0.049)	Loss 1.5576e-01 (2.6109e-01)	Acc@1  94.00 ( 93.12)	Acc@5 100.00 ( 99.81)
Test: [ 90/100]	Time  0.046 ( 0.048)	Loss 2.8247e-01 (2.5953e-01)	Acc@1  94.00 ( 93.18)	Acc@5 100.00 ( 99.81)
 * Acc@1 93.210 Acc@5 99.830
### epoch[74] execution time: 52.018914222717285
EPOCH 75
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [75][  0/391]	Time  0.275 ( 0.275)	Data  0.141 ( 0.141)	Loss 5.0926e-03 (5.0926e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [75][ 10/391]	Time  0.117 ( 0.133)	Data  0.001 ( 0.014)	Loss 1.8021e-02 (8.9285e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [75][ 20/391]	Time  0.120 ( 0.127)	Data  0.001 ( 0.008)	Loss 4.2305e-03 (8.9015e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [75][ 30/391]	Time  0.120 ( 0.124)	Data  0.001 ( 0.006)	Loss 3.5610e-03 (7.9273e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [75][ 40/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.004)	Loss 9.8724e-03 (8.2409e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [75][ 50/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.004)	Loss 5.1498e-03 (8.1545e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [75][ 60/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.5045e-02 (8.1472e-03)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [75][ 70/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 9.8877e-03 (8.1859e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [75][ 80/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 2.0981e-02 (8.5282e-03)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [75][ 90/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.6436e-03 (8.5378e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [75][100/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.7822e-02 (8.6396e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [75][110/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.1971e-02 (8.6333e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [75][120/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.9280e-03 (8.7551e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [75][130/391]	Time  0.133 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.2383e-02 (8.8106e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [75][140/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.7215e-03 (8.7503e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [75][150/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.1400e-03 (8.7147e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [75][160/391]	Time  0.130 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.0518e-02 (8.7286e-03)	Acc@1  98.44 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [75][170/391]	Time  0.116 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.5477e-03 (8.8020e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [75][180/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.1487e-03 (8.5351e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [75][190/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.2390e-02 (8.4329e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [75][200/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.9697e-03 (8.2736e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [75][210/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.5419e-03 (8.1966e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [75][220/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.3766e-03 (8.1996e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [75][230/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.2834e-03 (8.1998e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [75][240/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.7766e-03 (8.3751e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [75][250/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.3215e-03 (8.4944e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [75][260/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.7271e-03 (8.6207e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [75][270/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.1362e-03 (8.8154e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [75][280/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.3896e-03 (8.8597e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [75][290/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.8060e-03 (8.8566e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [75][300/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 8.3618e-03 (8.8585e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [75][310/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 7.4692e-03 (8.8227e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [75][320/391]	Time  0.127 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.4429e-02 (8.8424e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [75][330/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.3646e-03 (8.7499e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [75][340/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.9764e-02 (8.8455e-03)	Acc@1  98.44 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [75][350/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.0493e-02 (8.9081e-03)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [75][360/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.7098e-03 (8.8720e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [75][370/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.1284e-02 (8.8053e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [75][380/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.3840e-02 (8.9123e-03)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [75][390/391]	Time  0.101 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.6490e-03 (8.8824e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
## e[75] optimizer.zero_grad (sum) time: 0.6821765899658203
## e[75]       loss.backward (sum) time: 14.25981593132019
## e[75]      optimizer.step (sum) time: 8.029396295547485
## epoch[75] training(only) time: 47.09471368789673
# Switched to evaluate mode...
Test: [  0/100]	Time  0.190 ( 0.190)	Loss 1.8457e-01 (1.8457e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.060)	Loss 2.3022e-01 (2.3218e-01)	Acc@1  97.00 ( 94.55)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.047 ( 0.054)	Loss 4.3188e-01 (2.6414e-01)	Acc@1  88.00 ( 93.43)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.045 ( 0.052)	Loss 2.8638e-01 (2.6938e-01)	Acc@1  91.00 ( 93.39)	Acc@5  99.00 ( 99.65)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 3.3032e-01 (2.6880e-01)	Acc@1  88.00 ( 93.27)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.050 ( 0.050)	Loss 1.5234e-01 (2.7203e-01)	Acc@1  95.00 ( 93.14)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.8958e-01 (2.6513e-01)	Acc@1  98.00 ( 93.20)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 3.7671e-01 (2.6408e-01)	Acc@1  92.00 ( 93.27)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4978e-01 (2.6660e-01)	Acc@1  95.00 ( 93.27)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 3.1592e-01 (2.6528e-01)	Acc@1  94.00 ( 93.33)	Acc@5 100.00 ( 99.79)
 * Acc@1 93.390 Acc@5 99.810
### epoch[75] execution time: 52.04057240486145
EPOCH 76
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [76][  0/391]	Time  0.273 ( 0.273)	Data  0.139 ( 0.139)	Loss 7.1602e-03 (7.1602e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [76][ 10/391]	Time  0.119 ( 0.134)	Data  0.001 ( 0.013)	Loss 4.9706e-03 (5.4340e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [76][ 20/391]	Time  0.130 ( 0.127)	Data  0.001 ( 0.007)	Loss 9.6512e-03 (6.0388e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [76][ 30/391]	Time  0.120 ( 0.124)	Data  0.001 ( 0.005)	Loss 2.7943e-03 (6.5238e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [76][ 40/391]	Time  0.117 ( 0.123)	Data  0.001 ( 0.004)	Loss 1.2390e-02 (7.0970e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [76][ 50/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.004)	Loss 1.4900e-02 (7.7297e-03)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [76][ 60/391]	Time  0.126 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.0696e-02 (7.7739e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [76][ 70/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.2978e-02 (7.8066e-03)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [76][ 80/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.003)	Loss 3.1796e-03 (7.5097e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [76][ 90/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.3725e-02 (7.4940e-03)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [76][100/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.0283e-03 (7.6854e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [76][110/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.0997e-03 (8.0732e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [76][120/391]	Time  0.126 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.6419e-03 (7.9209e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [76][130/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.7237e-03 (7.8545e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [76][140/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.5350e-02 (7.8888e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [76][150/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.4981e-03 (7.8290e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [76][160/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 7.1602e-03 (7.7198e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [76][170/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.2924e-02 (7.6908e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [76][180/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 7.6294e-03 (7.6904e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [76][190/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.3989e-03 (7.8934e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [76][200/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.9210e-03 (7.9501e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [76][210/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.2146e-02 (8.0774e-03)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [76][220/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.6305e-03 (8.0152e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [76][230/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.5787e-03 (7.9724e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [76][240/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.7989e-03 (7.9204e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [76][250/391]	Time  0.129 ( 0.120)	Data  0.001 ( 0.001)	Loss 9.2850e-03 (7.9334e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [76][260/391]	Time  0.126 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.2686e-03 (7.9504e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [76][270/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.4708e-03 (7.9465e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [76][280/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.4125e-03 (8.0025e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [76][290/391]	Time  0.128 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.8624e-03 (7.9936e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [76][300/391]	Time  0.128 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.2038e-03 (7.9448e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [76][310/391]	Time  0.125 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.9738e-03 (8.0058e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [76][320/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.7798e-03 (7.9823e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [76][330/391]	Time  0.128 ( 0.120)	Data  0.001 ( 0.001)	Loss 8.2855e-03 (7.9877e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [76][340/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.1620e-02 (7.9298e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [76][350/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.1656e-03 (8.0336e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [76][360/391]	Time  0.126 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.9591e-03 (8.0161e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [76][370/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.8305e-03 (7.9781e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [76][380/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 8.8196e-03 (8.0187e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [76][390/391]	Time  0.103 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.4008e-03 (8.1643e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
## e[76] optimizer.zero_grad (sum) time: 0.6839694976806641
## e[76]       loss.backward (sum) time: 14.267720937728882
## e[76]      optimizer.step (sum) time: 7.968422889709473
## epoch[76] training(only) time: 47.01229429244995
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 1.7883e-01 (1.7883e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.059)	Loss 2.4133e-01 (2.2860e-01)	Acc@1  96.00 ( 94.36)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 4.1113e-01 (2.6032e-01)	Acc@1  88.00 ( 93.29)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.046 ( 0.051)	Loss 2.9492e-01 (2.6888e-01)	Acc@1  91.00 ( 93.19)	Acc@5  99.00 ( 99.61)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 3.1641e-01 (2.6674e-01)	Acc@1  88.00 ( 93.17)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.045 ( 0.050)	Loss 1.5613e-01 (2.6983e-01)	Acc@1  94.00 ( 93.08)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.8604e-01 (2.6342e-01)	Acc@1  98.00 ( 93.16)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 3.9307e-01 (2.6224e-01)	Acc@1  92.00 ( 93.23)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4856e-01 (2.6497e-01)	Acc@1  95.00 ( 93.21)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 2.9834e-01 (2.6333e-01)	Acc@1  94.00 ( 93.29)	Acc@5 100.00 ( 99.77)
 * Acc@1 93.360 Acc@5 99.790
### epoch[76] execution time: 51.975834608078
EPOCH 77
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [77][  0/391]	Time  0.272 ( 0.272)	Data  0.134 ( 0.134)	Loss 8.4991e-03 (8.4991e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [77][ 10/391]	Time  0.118 ( 0.133)	Data  0.001 ( 0.013)	Loss 4.0894e-03 (1.1026e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [77][ 20/391]	Time  0.119 ( 0.127)	Data  0.001 ( 0.007)	Loss 7.9269e-03 (1.0821e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [77][ 30/391]	Time  0.119 ( 0.125)	Data  0.001 ( 0.005)	Loss 5.0850e-03 (9.4665e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [77][ 40/391]	Time  0.121 ( 0.123)	Data  0.001 ( 0.004)	Loss 5.1041e-03 (9.0465e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [77][ 50/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.004)	Loss 2.4223e-03 (9.2038e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [77][ 60/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 5.0964e-03 (9.5760e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [77][ 70/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.8218e-03 (9.2934e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [77][ 80/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 8.1482e-03 (9.1988e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [77][ 90/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.003)	Loss 1.2177e-02 (8.9889e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [77][100/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.9716e-03 (8.7268e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [77][110/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.9645e-03 (8.8819e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [77][120/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.9095e-03 (8.6960e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [77][130/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.7793e-03 (8.5693e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [77][140/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.5900e-02 (8.6791e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [77][150/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.8120e-03 (8.7605e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [77][160/391]	Time  0.132 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.4147e-03 (8.8386e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [77][170/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.1139e-02 (8.8662e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [77][180/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.4027e-03 (8.7288e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [77][190/391]	Time  0.126 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.1297e-03 (8.6238e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [77][200/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.5076e-03 (8.3891e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [77][210/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.1651e-03 (8.3490e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [77][220/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 7.5569e-03 (8.3920e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [77][230/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.3864e-03 (8.4702e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [77][240/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.2024e-02 (8.4351e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [77][250/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 9.1248e-03 (8.4000e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [77][260/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.6068e-02 (8.5074e-03)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [77][270/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.8458e-03 (8.3621e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [77][280/391]	Time  0.124 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.9464e-03 (8.3015e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [77][290/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.0392e-03 (8.2074e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [77][300/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 8.4381e-03 (8.1569e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [77][310/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.0370e-03 (8.2090e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [77][320/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 8.3160e-03 (8.2432e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [77][330/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.1531e-03 (8.1767e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [77][340/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.6267e-03 (8.1526e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [77][350/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.0997e-03 (8.1801e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [77][360/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 8.4839e-03 (8.2205e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [77][370/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 7.2556e-03 (8.2547e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [77][380/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.7994e-03 (8.2637e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [77][390/391]	Time  0.108 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.1057e-03 (8.2753e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
## e[77] optimizer.zero_grad (sum) time: 0.6835544109344482
## e[77]       loss.backward (sum) time: 14.25635814666748
## e[77]      optimizer.step (sum) time: 7.986399412155151
## epoch[77] training(only) time: 47.074373722076416
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 1.8079e-01 (1.8079e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.045 ( 0.060)	Loss 2.3596e-01 (2.2751e-01)	Acc@1  97.00 ( 94.36)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.046 ( 0.053)	Loss 4.1382e-01 (2.5665e-01)	Acc@1  88.00 ( 93.57)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.046 ( 0.051)	Loss 2.6855e-01 (2.6390e-01)	Acc@1  92.00 ( 93.42)	Acc@5  99.00 ( 99.61)
Test: [ 40/100]	Time  0.055 ( 0.050)	Loss 3.3936e-01 (2.6340e-01)	Acc@1  89.00 ( 93.34)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.048 ( 0.049)	Loss 1.6064e-01 (2.6698e-01)	Acc@1  95.00 ( 93.18)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.8530e-01 (2.6047e-01)	Acc@1  99.00 ( 93.23)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 4.0601e-01 (2.6006e-01)	Acc@1  92.00 ( 93.27)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4563e-01 (2.6227e-01)	Acc@1  94.00 ( 93.25)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.059 ( 0.049)	Loss 2.9468e-01 (2.6093e-01)	Acc@1  94.00 ( 93.33)	Acc@5 100.00 ( 99.79)
 * Acc@1 93.380 Acc@5 99.810
### epoch[77] execution time: 52.01172161102295
EPOCH 78
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [78][  0/391]	Time  0.285 ( 0.285)	Data  0.151 ( 0.151)	Loss 5.5656e-03 (5.5656e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [78][ 10/391]	Time  0.121 ( 0.135)	Data  0.001 ( 0.015)	Loss 1.7944e-02 (6.8463e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [78][ 20/391]	Time  0.131 ( 0.128)	Data  0.001 ( 0.008)	Loss 5.7869e-03 (9.1617e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [78][ 30/391]	Time  0.118 ( 0.125)	Data  0.001 ( 0.006)	Loss 3.9673e-03 (9.3712e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [78][ 40/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.005)	Loss 2.3849e-02 (1.0184e-02)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [78][ 50/391]	Time  0.126 ( 0.123)	Data  0.001 ( 0.004)	Loss 1.0586e-03 (9.7504e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [78][ 60/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.1757e-02 (1.0343e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [78][ 70/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 7.9041e-03 (9.9960e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [78][ 80/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.3687e-02 (9.6451e-03)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [78][ 90/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.003)	Loss 5.4588e-03 (9.5439e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [78][100/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.003)	Loss 9.2697e-03 (9.3922e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [78][110/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.3678e-03 (9.5251e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [78][120/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.6875e-03 (9.2320e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [78][130/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.6283e-03 (9.2912e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [78][140/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.1467e-02 (9.2187e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [78][150/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0498e-02 (9.1555e-03)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [78][160/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.3131e-03 (8.9466e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [78][170/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.1063e-02 (9.0174e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [78][180/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.0490e-03 (9.0326e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [78][190/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.2376e-03 (8.9393e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [78][200/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.9062e-03 (9.0278e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [78][210/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.2643e-03 (9.0985e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [78][220/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.2848e-02 (9.1002e-03)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [78][230/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.3514e-02 (9.0564e-03)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [78][240/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.2825e-02 (9.0477e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [78][250/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.2635e-03 (9.0694e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [78][260/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.3444e-03 (8.9654e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [78][270/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 8.3084e-03 (8.8841e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [78][280/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.1055e-02 (8.9060e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [78][290/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.6617e-02 (8.8694e-03)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [78][300/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.2114e-03 (8.7757e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [78][310/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.5823e-02 (8.8511e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [78][320/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.5346e-03 (8.7945e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [78][330/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.4545e-03 (8.8933e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [78][340/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 7.7248e-03 (8.9325e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [78][350/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.6932e-03 (9.0449e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [78][360/391]	Time  0.128 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.6150e-03 (9.0187e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [78][370/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.4572e-02 (8.9929e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [78][380/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.7302e-03 (8.9488e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [78][390/391]	Time  0.111 ( 0.120)	Data  0.001 ( 0.001)	Loss 8.8043e-03 (9.0092e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
## e[78] optimizer.zero_grad (sum) time: 0.6793315410614014
## e[78]       loss.backward (sum) time: 14.279080629348755
## e[78]      optimizer.step (sum) time: 8.011266469955444
## epoch[78] training(only) time: 47.13959503173828
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 1.9031e-01 (1.9031e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 2.4414e-01 (2.2793e-01)	Acc@1  96.00 ( 94.18)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.045 ( 0.054)	Loss 4.1357e-01 (2.5762e-01)	Acc@1  88.00 ( 93.29)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 2.7124e-01 (2.6387e-01)	Acc@1  91.00 ( 93.29)	Acc@5  99.00 ( 99.61)
Test: [ 40/100]	Time  0.049 ( 0.051)	Loss 3.4277e-01 (2.6415e-01)	Acc@1  88.00 ( 93.20)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.6187e-01 (2.6844e-01)	Acc@1  95.00 ( 93.10)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.048 ( 0.050)	Loss 1.8359e-01 (2.6157e-01)	Acc@1  99.00 ( 93.23)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.048 ( 0.049)	Loss 3.9990e-01 (2.6145e-01)	Acc@1  92.00 ( 93.28)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 1.4709e-01 (2.6416e-01)	Acc@1  95.00 ( 93.26)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 2.9565e-01 (2.6258e-01)	Acc@1  94.00 ( 93.32)	Acc@5 100.00 ( 99.79)
 * Acc@1 93.350 Acc@5 99.810
### epoch[78] execution time: 52.11202430725098
EPOCH 79
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [79][  0/391]	Time  0.276 ( 0.276)	Data  0.142 ( 0.142)	Loss 7.1220e-03 (7.1220e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [79][ 10/391]	Time  0.118 ( 0.135)	Data  0.001 ( 0.014)	Loss 1.3695e-02 (1.0471e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [79][ 20/391]	Time  0.121 ( 0.128)	Data  0.001 ( 0.008)	Loss 9.6817e-03 (9.8372e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [79][ 30/391]	Time  0.123 ( 0.126)	Data  0.001 ( 0.005)	Loss 1.1963e-02 (9.4631e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [79][ 40/391]	Time  0.117 ( 0.124)	Data  0.001 ( 0.004)	Loss 5.4703e-03 (8.2750e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [79][ 50/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.004)	Loss 2.4185e-03 (7.8529e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [79][ 60/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.003)	Loss 4.9072e-02 (8.5031e-03)	Acc@1  97.66 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [79][ 70/391]	Time  0.117 ( 0.123)	Data  0.001 ( 0.003)	Loss 1.1032e-02 (8.9506e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [79][ 80/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 9.4833e-03 (9.2154e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [79][ 90/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.6163e-02 (9.3000e-03)	Acc@1  98.44 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [79][100/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 7.0343e-03 (9.0019e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [79][110/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.1269e-02 (9.1803e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [79][120/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.9714e-02 (9.2340e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [79][130/391]	Time  0.124 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.7961e-03 (9.0543e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [79][140/391]	Time  0.125 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.2421e-02 (8.9378e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [79][150/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.3286e-03 (9.1026e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [79][160/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.5776e-03 (8.9784e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [79][170/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.9335e-03 (8.9770e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [79][180/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.6496e-03 (9.2367e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [79][190/391]	Time  0.124 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.2032e-02 (9.0499e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [79][200/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.0126e-02 (9.1066e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [79][210/391]	Time  0.126 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.3259e-03 (9.1646e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [79][220/391]	Time  0.131 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.3412e-02 (9.1638e-03)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [79][230/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.2643e-03 (9.0565e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [79][240/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.7182e-03 (8.9393e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [79][250/391]	Time  0.129 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.0441e-03 (8.7281e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [79][260/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.4594e-03 (8.7969e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [79][270/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.6373e-02 (8.7800e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [79][280/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.5384e-03 (8.9451e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [79][290/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.0201e-02 (8.8889e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [79][300/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.7725e-02 (9.0041e-03)	Acc@1  98.44 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [79][310/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.1421e-02 (8.9823e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [79][320/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 9.6283e-03 (8.9407e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [79][330/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.3802e-02 (8.9298e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [79][340/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.6876e-02 (8.8741e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [79][350/391]	Time  0.125 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.1396e-03 (8.8648e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [79][360/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.8599e-03 (8.8279e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [79][370/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.1528e-03 (8.7967e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [79][380/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.3779e-02 (8.8365e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [79][390/391]	Time  0.107 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.5466e-03 (8.8260e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
## e[79] optimizer.zero_grad (sum) time: 0.6814792156219482
## e[79]       loss.backward (sum) time: 14.335161924362183
## e[79]      optimizer.step (sum) time: 8.055158138275146
## epoch[79] training(only) time: 47.219865798950195
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 1.7065e-01 (1.7065e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.059)	Loss 2.3804e-01 (2.3105e-01)	Acc@1  97.00 ( 94.18)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.045 ( 0.053)	Loss 4.2236e-01 (2.6300e-01)	Acc@1  88.00 ( 93.14)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.046 ( 0.051)	Loss 2.7051e-01 (2.6872e-01)	Acc@1  91.00 ( 93.06)	Acc@5  99.00 ( 99.61)
Test: [ 40/100]	Time  0.055 ( 0.050)	Loss 3.5254e-01 (2.6797e-01)	Acc@1  88.00 ( 93.07)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.046 ( 0.049)	Loss 1.4990e-01 (2.7205e-01)	Acc@1  95.00 ( 92.94)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.049 ( 0.049)	Loss 1.9470e-01 (2.6456e-01)	Acc@1  98.00 ( 93.03)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 4.0112e-01 (2.6392e-01)	Acc@1  93.00 ( 93.10)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.046 ( 0.048)	Loss 1.4001e-01 (2.6661e-01)	Acc@1  94.00 ( 93.10)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.061 ( 0.048)	Loss 2.9834e-01 (2.6587e-01)	Acc@1  95.00 ( 93.15)	Acc@5 100.00 ( 99.77)
 * Acc@1 93.210 Acc@5 99.790
### epoch[79] execution time: 52.12042832374573
EPOCH 80
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [80][  0/391]	Time  0.279 ( 0.279)	Data  0.151 ( 0.151)	Loss 2.0771e-03 (2.0771e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [80][ 10/391]	Time  0.119 ( 0.135)	Data  0.001 ( 0.015)	Loss 1.1238e-02 (7.0730e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [80][ 20/391]	Time  0.117 ( 0.127)	Data  0.001 ( 0.008)	Loss 1.0925e-02 (9.3223e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [80][ 30/391]	Time  0.120 ( 0.125)	Data  0.001 ( 0.006)	Loss 2.1572e-03 (9.2424e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [80][ 40/391]	Time  0.117 ( 0.123)	Data  0.001 ( 0.005)	Loss 9.6512e-03 (8.5511e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [80][ 50/391]	Time  0.130 ( 0.123)	Data  0.001 ( 0.004)	Loss 1.1963e-02 (8.6198e-03)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [80][ 60/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 2.0828e-03 (8.9952e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [80][ 70/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.003)	Loss 5.8289e-03 (8.7005e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [80][ 80/391]	Time  0.127 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.1253e-02 (8.7786e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [80][ 90/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.003)	Loss 4.7722e-03 (8.7045e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [80][100/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.6106e-03 (8.7818e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [80][110/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.9041e-03 (9.0760e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [80][120/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.0577e-03 (9.0003e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [80][130/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.2474e-03 (8.8157e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [80][140/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.1879e-02 (8.7911e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [80][150/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.4964e-03 (8.6949e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [80][160/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.5613e-03 (8.9379e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [80][170/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.6459e-02 (9.1547e-03)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [80][180/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.1871e-02 (9.0706e-03)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [80][190/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.8065e-03 (9.0549e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [80][200/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.9368e-03 (9.0460e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [80][210/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 9.3460e-03 (9.0465e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [80][220/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.5504e-03 (9.1327e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [80][230/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.5771e-03 (9.1446e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [80][240/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.7978e-03 (8.9856e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [80][250/391]	Time  0.122 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.0544e-02 (8.8668e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [80][260/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 8.4000e-03 (8.8305e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [80][270/391]	Time  0.124 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.3657e-02 (8.7781e-03)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [80][280/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 7.2975e-03 (8.7482e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [80][290/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.4849e-03 (8.7231e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [80][300/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 8.5373e-03 (8.7360e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [80][310/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 7.5798e-03 (8.6461e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [80][320/391]	Time  0.122 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.6354e-03 (8.6307e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [80][330/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 9.3307e-03 (8.6488e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [80][340/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 8.1100e-03 (8.6054e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [80][350/391]	Time  0.125 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.5013e-03 (8.5546e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [80][360/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.1162e-02 (8.5765e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [80][370/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.1749e-02 (8.5326e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [80][380/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 9.5291e-03 (8.5596e-03)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [80][390/391]	Time  0.113 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.2776e-02 (8.5000e-03)	Acc@1  98.75 ( 99.84)	Acc@5 100.00 (100.00)
## e[80] optimizer.zero_grad (sum) time: 0.6816482543945312
## e[80]       loss.backward (sum) time: 14.28933072090149
## e[80]      optimizer.step (sum) time: 8.004238367080688
## epoch[80] training(only) time: 47.08397722244263
# Switched to evaluate mode...
Test: [  0/100]	Time  0.191 ( 0.191)	Loss 1.8298e-01 (1.8298e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 2.4146e-01 (2.2326e-01)	Acc@1  96.00 ( 94.27)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.048 ( 0.055)	Loss 4.0918e-01 (2.5476e-01)	Acc@1  88.00 ( 93.52)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 2.7930e-01 (2.6092e-01)	Acc@1  92.00 ( 93.39)	Acc@5  99.00 ( 99.65)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 3.1763e-01 (2.6171e-01)	Acc@1  88.00 ( 93.27)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.5808e-01 (2.6572e-01)	Acc@1  95.00 ( 93.16)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.8005e-01 (2.5942e-01)	Acc@1  98.00 ( 93.25)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 3.9600e-01 (2.5923e-01)	Acc@1  92.00 ( 93.30)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4990e-01 (2.6144e-01)	Acc@1  94.00 ( 93.23)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 2.8247e-01 (2.5988e-01)	Acc@1  94.00 ( 93.29)	Acc@5 100.00 ( 99.80)
 * Acc@1 93.330 Acc@5 99.820
### epoch[80] execution time: 52.0176727771759
EPOCH 81
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [81][  0/391]	Time  0.287 ( 0.287)	Data  0.158 ( 0.158)	Loss 3.6377e-02 (3.6377e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [81][ 10/391]	Time  0.117 ( 0.133)	Data  0.001 ( 0.015)	Loss 6.4926e-03 (1.2678e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [81][ 20/391]	Time  0.116 ( 0.127)	Data  0.001 ( 0.008)	Loss 6.1493e-03 (1.1376e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [81][ 30/391]	Time  0.119 ( 0.125)	Data  0.001 ( 0.006)	Loss 3.9902e-03 (1.0581e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [81][ 40/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.005)	Loss 7.4654e-03 (9.9505e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [81][ 50/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.004)	Loss 7.6065e-03 (1.0597e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [81][ 60/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.004)	Loss 5.8899e-03 (1.0268e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [81][ 70/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 3.9291e-03 (9.7388e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [81][ 80/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.003)	Loss 5.2681e-03 (9.5094e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [81][ 90/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.003)	Loss 9.0637e-03 (9.4877e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [81][100/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.003)	Loss 3.5191e-03 (9.4252e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [81][110/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.0839e-03 (9.3693e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [81][120/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.1940e-03 (9.4741e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [81][130/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.6109e-03 (9.4772e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [81][140/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.3635e-03 (9.3216e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [81][150/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.2245e-02 (9.1458e-03)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [81][160/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.9782e-03 (9.0537e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [81][170/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.1150e-03 (8.8902e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [81][180/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.0801e-03 (8.7388e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [81][190/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.0537e-03 (8.7064e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [81][200/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.5367e-03 (8.6691e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [81][210/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.3139e-03 (8.8930e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [81][220/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.0196e-03 (8.8282e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [81][230/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.2021e-03 (8.7608e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [81][240/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.4839e-03 (8.7850e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [81][250/391]	Time  0.126 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.5167e-02 (8.7784e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [81][260/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.2940e-03 (8.7108e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [81][270/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.5651e-03 (8.7976e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [81][280/391]	Time  0.134 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.5366e-02 (8.8438e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [81][290/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.9368e-03 (8.8537e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [81][300/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.5667e-03 (8.7794e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [81][310/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.0277e-02 (8.7813e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [81][320/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.6261e-03 (8.7697e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [81][330/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.3412e-02 (8.8553e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [81][340/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.0468e-02 (8.8856e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [81][350/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 7.3547e-03 (8.8229e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [81][360/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.0445e-02 (8.7743e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [81][370/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.0269e-02 (8.8272e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [81][380/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 8.2397e-03 (8.8282e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [81][390/391]	Time  0.113 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.0251e-03 (8.7962e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
## e[81] optimizer.zero_grad (sum) time: 0.6887838840484619
## e[81]       loss.backward (sum) time: 14.277404308319092
## e[81]      optimizer.step (sum) time: 8.038933038711548
## epoch[81] training(only) time: 47.1512393951416
# Switched to evaluate mode...
Test: [  0/100]	Time  0.181 ( 0.181)	Loss 2.1582e-01 (2.1582e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.048 ( 0.060)	Loss 2.3999e-01 (2.3345e-01)	Acc@1  96.00 ( 94.45)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.046 ( 0.053)	Loss 4.0576e-01 (2.6264e-01)	Acc@1  88.00 ( 93.52)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 2.9004e-01 (2.6959e-01)	Acc@1  92.00 ( 93.48)	Acc@5  99.00 ( 99.65)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 3.2397e-01 (2.6890e-01)	Acc@1  88.00 ( 93.41)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 1.6174e-01 (2.7232e-01)	Acc@1  96.00 ( 93.25)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.7676e-01 (2.6585e-01)	Acc@1  98.00 ( 93.34)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 4.0820e-01 (2.6463e-01)	Acc@1  92.00 ( 93.39)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.7444e-01 (2.6882e-01)	Acc@1  94.00 ( 93.35)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 3.0835e-01 (2.6682e-01)	Acc@1  94.00 ( 93.36)	Acc@5 100.00 ( 99.79)
 * Acc@1 93.380 Acc@5 99.810
### epoch[81] execution time: 52.094109535217285
EPOCH 82
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [82][  0/391]	Time  0.281 ( 0.281)	Data  0.153 ( 0.153)	Loss 4.5853e-03 (4.5853e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [82][ 10/391]	Time  0.117 ( 0.135)	Data  0.001 ( 0.015)	Loss 1.4687e-02 (8.8823e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [82][ 20/391]	Time  0.120 ( 0.127)	Data  0.001 ( 0.008)	Loss 9.5901e-03 (7.4675e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [82][ 30/391]	Time  0.119 ( 0.125)	Data  0.001 ( 0.006)	Loss 1.0010e-02 (7.9837e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [82][ 40/391]	Time  0.121 ( 0.124)	Data  0.001 ( 0.005)	Loss 3.0212e-03 (7.6418e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [82][ 50/391]	Time  0.117 ( 0.123)	Data  0.001 ( 0.004)	Loss 7.0229e-03 (8.4793e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [82][ 60/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 8.0872e-03 (8.3482e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [82][ 70/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 6.9313e-03 (8.0571e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [82][ 80/391]	Time  0.125 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.5282e-02 (8.1089e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [82][ 90/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.003)	Loss 1.1238e-02 (8.0512e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [82][100/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.5343e-02 (7.9841e-03)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [82][110/391]	Time  0.128 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.6968e-02 (7.9451e-03)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [82][120/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.0245e-03 (8.1324e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [82][130/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.2098e-03 (8.2103e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [82][140/391]	Time  0.126 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.3291e-03 (8.1422e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [82][150/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.1612e-02 (8.3420e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [82][160/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.9495e-02 (8.5287e-03)	Acc@1  98.44 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [82][170/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.8348e-03 (8.6395e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [82][180/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.8433e-03 (8.5658e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [82][190/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0635e-02 (8.6976e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [82][200/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.1230e-02 (8.6266e-03)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [82][210/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 7.4081e-03 (8.5861e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [82][220/391]	Time  0.126 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.2872e-03 (8.5140e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [82][230/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.9948e-02 (8.6831e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [82][240/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.8172e-03 (8.6185e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [82][250/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.3390e-02 (8.6166e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [82][260/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 9.8648e-03 (8.5780e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [82][270/391]	Time  0.123 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.7564e-03 (8.5640e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [82][280/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.5967e-03 (8.5784e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [82][290/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.1281e-03 (8.5008e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [82][300/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.0577e-03 (8.4079e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [82][310/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.0804e-03 (8.4473e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [82][320/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.3940e-03 (8.4867e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [82][330/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.8904e-03 (8.4712e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [82][340/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.4174e-03 (8.4988e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [82][350/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.3972e-02 (8.5099e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [82][360/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.0590e-02 (8.5148e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [82][370/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.9624e-03 (8.4885e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [82][380/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.0697e-03 (8.5262e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [82][390/391]	Time  0.108 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.4534e-02 (8.5099e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
## e[82] optimizer.zero_grad (sum) time: 0.6784029006958008
## e[82]       loss.backward (sum) time: 14.305500268936157
## e[82]      optimizer.step (sum) time: 7.99325966835022
## epoch[82] training(only) time: 47.096439838409424
# Switched to evaluate mode...
Test: [  0/100]	Time  0.199 ( 0.199)	Loss 1.8188e-01 (1.8188e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.050 ( 0.061)	Loss 2.3584e-01 (2.2450e-01)	Acc@1  97.00 ( 94.36)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 3.9966e-01 (2.5539e-01)	Acc@1  87.00 ( 93.48)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.046 ( 0.051)	Loss 2.6978e-01 (2.6464e-01)	Acc@1  92.00 ( 93.35)	Acc@5  99.00 ( 99.61)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 3.3276e-01 (2.6329e-01)	Acc@1  90.00 ( 93.32)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.056 ( 0.050)	Loss 1.6003e-01 (2.6684e-01)	Acc@1  95.00 ( 93.14)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.049 ( 0.049)	Loss 1.8909e-01 (2.6053e-01)	Acc@1  98.00 ( 93.20)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 4.2773e-01 (2.6016e-01)	Acc@1  91.00 ( 93.21)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.5576e-01 (2.6254e-01)	Acc@1  93.00 ( 93.21)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.046 ( 0.048)	Loss 2.9370e-01 (2.6043e-01)	Acc@1  94.00 ( 93.31)	Acc@5 100.00 ( 99.78)
 * Acc@1 93.360 Acc@5 99.800
### epoch[82] execution time: 52.055774211883545
EPOCH 83
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [83][  0/391]	Time  0.275 ( 0.275)	Data  0.145 ( 0.145)	Loss 1.3533e-03 (1.3533e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [83][ 10/391]	Time  0.118 ( 0.134)	Data  0.001 ( 0.014)	Loss 2.5806e-03 (6.0415e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [83][ 20/391]	Time  0.119 ( 0.127)	Data  0.001 ( 0.008)	Loss 3.1757e-03 (8.4057e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [83][ 30/391]	Time  0.121 ( 0.125)	Data  0.001 ( 0.006)	Loss 6.1417e-03 (8.4538e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [83][ 40/391]	Time  0.122 ( 0.124)	Data  0.001 ( 0.005)	Loss 3.6983e-03 (8.2859e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [83][ 50/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.004)	Loss 9.2621e-03 (7.9391e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [83][ 60/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 5.4283e-03 (7.9291e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [83][ 70/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 7.6828e-03 (8.3547e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [83][ 80/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 7.1754e-03 (8.1498e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [83][ 90/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.3359e-02 (8.0712e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [83][100/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.3193e-02 (8.4784e-03)	Acc@1  98.44 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [83][110/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.3139e-03 (8.4528e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [83][120/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0796e-02 (8.5124e-03)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [83][130/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.7945e-03 (8.4352e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [83][140/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.9640e-03 (8.5859e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [83][150/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.6678e-03 (8.7916e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [83][160/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.5915e-02 (8.7831e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [83][170/391]	Time  0.116 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.1238e-02 (8.7488e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [83][180/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.7479e-03 (8.7029e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [83][190/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.4313e-03 (8.5793e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [83][200/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.5137e-02 (8.6290e-03)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [83][210/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0002e-02 (8.5146e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [83][220/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.3392e-02 (8.4458e-03)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [83][230/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.3923e-03 (8.5714e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [83][240/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.5320e-02 (8.6322e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [83][250/391]	Time  0.123 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.2986e-03 (8.7612e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [83][260/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 8.4534e-03 (8.7059e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [83][270/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.1009e-02 (8.6526e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [83][280/391]	Time  0.126 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.6434e-02 (8.5925e-03)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [83][290/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.1033e-03 (8.4636e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [83][300/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.0757e-02 (8.4626e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [83][310/391]	Time  0.131 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.7302e-03 (8.3573e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [83][320/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.6432e-03 (8.4272e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [83][330/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.6795e-03 (8.4528e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [83][340/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 8.5144e-03 (8.5145e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [83][350/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.5635e-03 (8.4388e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [83][360/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 7.5836e-03 (8.3898e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [83][370/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.0474e-03 (8.3656e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [83][380/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.6692e-03 (8.3766e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [83][390/391]	Time  0.107 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.6011e-03 (8.3034e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
## e[83] optimizer.zero_grad (sum) time: 0.6856420040130615
## e[83]       loss.backward (sum) time: 14.270456552505493
## e[83]      optimizer.step (sum) time: 8.007955551147461
## epoch[83] training(only) time: 47.11250305175781
# Switched to evaluate mode...
Test: [  0/100]	Time  0.182 ( 0.182)	Loss 1.6394e-01 (1.6394e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 2.4243e-01 (2.2471e-01)	Acc@1  97.00 ( 94.36)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.046 ( 0.053)	Loss 4.0356e-01 (2.5679e-01)	Acc@1  88.00 ( 93.48)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.046 ( 0.051)	Loss 2.5830e-01 (2.6373e-01)	Acc@1  91.00 ( 93.32)	Acc@5  99.00 ( 99.65)
Test: [ 40/100]	Time  0.045 ( 0.050)	Loss 3.1982e-01 (2.6272e-01)	Acc@1  88.00 ( 93.20)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.5930e-01 (2.6742e-01)	Acc@1  95.00 ( 93.08)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.8286e-01 (2.6065e-01)	Acc@1  98.00 ( 93.16)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.050 ( 0.049)	Loss 4.1284e-01 (2.6065e-01)	Acc@1  92.00 ( 93.24)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.045 ( 0.049)	Loss 1.4453e-01 (2.6394e-01)	Acc@1  94.00 ( 93.20)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.050 ( 0.048)	Loss 2.9004e-01 (2.6265e-01)	Acc@1  94.00 ( 93.25)	Acc@5 100.00 ( 99.79)
 * Acc@1 93.310 Acc@5 99.810
### epoch[83] execution time: 52.0425910949707
EPOCH 84
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [84][  0/391]	Time  0.297 ( 0.297)	Data  0.160 ( 0.160)	Loss 1.8631e-02 (1.8631e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [84][ 10/391]	Time  0.123 ( 0.137)	Data  0.001 ( 0.015)	Loss 9.7656e-03 (8.5545e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [84][ 20/391]	Time  0.120 ( 0.129)	Data  0.001 ( 0.009)	Loss 1.8143e-02 (8.2295e-03)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [84][ 30/391]	Time  0.119 ( 0.126)	Data  0.001 ( 0.006)	Loss 3.0842e-03 (7.9020e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [84][ 40/391]	Time  0.123 ( 0.124)	Data  0.001 ( 0.005)	Loss 3.5877e-03 (8.5955e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [84][ 50/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.004)	Loss 6.3667e-03 (8.5952e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [84][ 60/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.004)	Loss 7.7438e-03 (8.5803e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [84][ 70/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.2381e-03 (8.6678e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [84][ 80/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 5.8212e-03 (8.8334e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [84][ 90/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 9.7809e-03 (8.5137e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [84][100/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 9.9564e-03 (8.5519e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [84][110/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.1793e-03 (8.4265e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [84][120/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.1391e-02 (8.6857e-03)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [84][130/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.0207e-03 (8.8256e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [84][140/391]	Time  0.128 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.1406e-03 (8.6837e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [84][150/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.3618e-03 (8.7178e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [84][160/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.3504e-02 (8.8467e-03)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [84][170/391]	Time  0.127 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.8000e-03 (8.7485e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [84][180/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.1403e-02 (9.0765e-03)	Acc@1  97.66 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [84][190/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.0201e-03 (9.0772e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [84][200/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.2267e-03 (8.9772e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [84][210/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.0801e-03 (8.9686e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [84][220/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.9874e-03 (8.8348e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [84][230/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.8409e-03 (8.7362e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [84][240/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.0498e-02 (8.6290e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [84][250/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.1108e-02 (8.7011e-03)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [84][260/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.9771e-03 (8.6609e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [84][270/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.5166e-03 (8.6010e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [84][280/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.2230e-02 (8.5518e-03)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [84][290/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.5381e-03 (8.4303e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [84][300/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.8305e-03 (8.3613e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [84][310/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.1063e-02 (8.4685e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [84][320/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.4796e-03 (8.4047e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [84][330/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 9.2545e-03 (8.3999e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [84][340/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.4504e-03 (8.4097e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [84][350/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.1826e-02 (8.3601e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [84][360/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.3744e-03 (8.3515e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [84][370/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 7.2632e-03 (8.3574e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [84][380/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.6831e-03 (8.2972e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [84][390/391]	Time  0.104 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.3478e-02 (8.4076e-03)	Acc@1  98.75 ( 99.82)	Acc@5 100.00 (100.00)
## e[84] optimizer.zero_grad (sum) time: 0.6931612491607666
## e[84]       loss.backward (sum) time: 14.192254304885864
## e[84]      optimizer.step (sum) time: 8.063247919082642
## epoch[84] training(only) time: 47.04050898551941
# Switched to evaluate mode...
Test: [  0/100]	Time  0.208 ( 0.208)	Loss 1.8884e-01 (1.8884e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.048 ( 0.062)	Loss 2.3633e-01 (2.2766e-01)	Acc@1  96.00 ( 94.45)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.049 ( 0.055)	Loss 4.0503e-01 (2.5828e-01)	Acc@1  89.00 ( 93.52)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 2.6782e-01 (2.6565e-01)	Acc@1  91.00 ( 93.29)	Acc@5  99.00 ( 99.61)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 3.3057e-01 (2.6496e-01)	Acc@1  88.00 ( 93.22)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.061 ( 0.051)	Loss 1.5527e-01 (2.6893e-01)	Acc@1  96.00 ( 93.12)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.8103e-01 (2.6184e-01)	Acc@1  99.00 ( 93.26)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 4.0796e-01 (2.6085e-01)	Acc@1  92.00 ( 93.31)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.048 ( 0.049)	Loss 1.5308e-01 (2.6427e-01)	Acc@1  94.00 ( 93.30)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 2.9004e-01 (2.6257e-01)	Acc@1  94.00 ( 93.34)	Acc@5 100.00 ( 99.77)
 * Acc@1 93.390 Acc@5 99.790
### epoch[84] execution time: 52.03331780433655
EPOCH 85
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [85][  0/391]	Time  0.280 ( 0.280)	Data  0.139 ( 0.139)	Loss 4.7951e-03 (4.7951e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [85][ 10/391]	Time  0.117 ( 0.134)	Data  0.001 ( 0.014)	Loss 1.3474e-02 (7.6442e-03)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [85][ 20/391]	Time  0.117 ( 0.127)	Data  0.001 ( 0.008)	Loss 4.1199e-03 (6.5716e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [85][ 30/391]	Time  0.117 ( 0.125)	Data  0.001 ( 0.005)	Loss 1.7746e-02 (7.9099e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [85][ 40/391]	Time  0.121 ( 0.123)	Data  0.001 ( 0.004)	Loss 4.7493e-03 (8.0359e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [85][ 50/391]	Time  0.117 ( 0.123)	Data  0.001 ( 0.004)	Loss 7.1297e-03 (8.4718e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [85][ 60/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 6.8016e-03 (8.3917e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [85][ 70/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.2305e-03 (8.3725e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [85][ 80/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.003)	Loss 2.7943e-03 (8.5609e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [85][ 90/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.003)	Loss 1.0788e-02 (8.4186e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [85][100/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.6617e-02 (8.3821e-03)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [85][110/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.0125e-03 (8.7167e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [85][120/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.1711e-02 (8.5907e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [85][130/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.2507e-03 (8.4139e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [85][140/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.9330e-03 (8.5688e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [85][150/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.4556e-03 (8.4402e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [85][160/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.3346e-02 (8.6882e-03)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [85][170/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.0782e-02 (8.8219e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [85][180/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.7349e-02 (8.8006e-03)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [85][190/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.6757e-03 (8.7183e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [85][200/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.2305e-03 (8.5930e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [85][210/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.6577e-03 (8.4636e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [85][220/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.8676e-03 (8.4472e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [85][230/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.5128e-03 (8.6159e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [85][240/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.1613e-03 (8.6300e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [85][250/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.1345e-02 (8.6723e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [85][260/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.4365e-03 (8.6433e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [85][270/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.1393e-02 (8.6718e-03)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [85][280/391]	Time  0.124 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.8585e-02 (8.6761e-03)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [85][290/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.5205e-02 (8.8126e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [85][300/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.0721e-02 (8.8015e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [85][310/391]	Time  0.130 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.3864e-03 (8.7444e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [85][320/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.0757e-02 (8.7401e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [85][330/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.5299e-02 (8.7787e-03)	Acc@1  98.44 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [85][340/391]	Time  0.128 ( 0.120)	Data  0.001 ( 0.001)	Loss 8.5602e-03 (8.8697e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [85][350/391]	Time  0.132 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.4779e-03 (8.8228e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [85][360/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.2087e-03 (8.8306e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [85][370/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 7.6485e-03 (8.8094e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [85][380/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.7700e-02 (8.7918e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [85][390/391]	Time  0.106 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.1475e-02 (8.7812e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
## e[85] optimizer.zero_grad (sum) time: 0.6780877113342285
## e[85]       loss.backward (sum) time: 14.26027226448059
## e[85]      optimizer.step (sum) time: 7.969785928726196
## epoch[85] training(only) time: 47.02767014503479
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 1.8872e-01 (1.8872e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 2.4097e-01 (2.2782e-01)	Acc@1  97.00 ( 94.18)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 3.9185e-01 (2.5646e-01)	Acc@1  88.00 ( 93.57)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.050 ( 0.052)	Loss 2.6489e-01 (2.6307e-01)	Acc@1  91.00 ( 93.39)	Acc@5  99.00 ( 99.58)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 3.6060e-01 (2.6246e-01)	Acc@1  88.00 ( 93.29)	Acc@5 100.00 ( 99.63)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.5356e-01 (2.6693e-01)	Acc@1  96.00 ( 93.12)	Acc@5 100.00 ( 99.67)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.8762e-01 (2.5940e-01)	Acc@1  99.00 ( 93.20)	Acc@5 100.00 ( 99.70)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 4.3213e-01 (2.5956e-01)	Acc@1  92.00 ( 93.27)	Acc@5 100.00 ( 99.72)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4880e-01 (2.6226e-01)	Acc@1  93.00 ( 93.21)	Acc@5 100.00 ( 99.73)
Test: [ 90/100]	Time  0.045 ( 0.048)	Loss 3.0029e-01 (2.6116e-01)	Acc@1  94.00 ( 93.25)	Acc@5 100.00 ( 99.75)
 * Acc@1 93.300 Acc@5 99.770
### epoch[85] execution time: 51.975446701049805
EPOCH 86
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [86][  0/391]	Time  0.278 ( 0.278)	Data  0.143 ( 0.143)	Loss 4.7379e-03 (4.7379e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [86][ 10/391]	Time  0.120 ( 0.135)	Data  0.001 ( 0.014)	Loss 4.8943e-03 (9.0270e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [86][ 20/391]	Time  0.120 ( 0.127)	Data  0.001 ( 0.008)	Loss 4.1580e-03 (9.7684e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [86][ 30/391]	Time  0.122 ( 0.125)	Data  0.001 ( 0.006)	Loss 2.5463e-03 (8.8500e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [86][ 40/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.004)	Loss 4.5853e-03 (9.6655e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [86][ 50/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.004)	Loss 3.4847e-03 (9.4784e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [86][ 60/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.9196e-02 (9.3650e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [86][ 70/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 5.9052e-03 (8.9071e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [86][ 80/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.003)	Loss 6.0272e-03 (9.0572e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [86][ 90/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.003)	Loss 2.8477e-03 (8.7586e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [86][100/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0323e-02 (8.8433e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [86][110/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.5580e-03 (9.2260e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [86][120/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.8198e-02 (9.2470e-03)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [86][130/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.6855e-03 (9.1629e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [86][140/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.6523e-03 (9.0742e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [86][150/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.6283e-03 (9.0262e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [86][160/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.0359e-03 (8.8754e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [86][170/391]	Time  0.128 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.9340e-03 (8.7708e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [86][180/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.5236e-02 (8.7199e-03)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [86][190/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.6068e-03 (8.6258e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [86][200/391]	Time  0.130 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.0588e-03 (8.5165e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [86][210/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.4078e-02 (8.6114e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [86][220/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.7537e-03 (8.6997e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [86][230/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.4567e-02 (8.7691e-03)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [86][240/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 7.1335e-03 (8.7921e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [86][250/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.1988e-02 (8.7743e-03)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [86][260/391]	Time  0.135 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.2100e-02 (8.7599e-03)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [86][270/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.1109e-03 (8.7519e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [86][280/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.7302e-03 (8.6788e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [86][290/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.5574e-02 (8.8708e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [86][300/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.8409e-03 (8.7593e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [86][310/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 8.0109e-03 (8.6997e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [86][320/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.3635e-03 (8.6622e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [86][330/391]	Time  0.123 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.2534e-03 (8.6455e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [86][340/391]	Time  0.122 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.0582e-02 (8.6874e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [86][350/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.4790e-03 (8.5788e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [86][360/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 9.1782e-03 (8.5920e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [86][370/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.8332e-03 (8.4922e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [86][380/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.0071e-02 (8.5481e-03)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [86][390/391]	Time  0.108 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.7022e-03 (8.5814e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
## e[86] optimizer.zero_grad (sum) time: 0.6805720329284668
## e[86]       loss.backward (sum) time: 14.259529113769531
## e[86]      optimizer.step (sum) time: 7.979915618896484
## epoch[86] training(only) time: 47.0460729598999
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 1.8604e-01 (1.8604e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 2.4280e-01 (2.3182e-01)	Acc@1  97.00 ( 94.09)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.045 ( 0.054)	Loss 4.4116e-01 (2.6275e-01)	Acc@1  88.00 ( 93.24)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 2.8931e-01 (2.6871e-01)	Acc@1  91.00 ( 93.13)	Acc@5  99.00 ( 99.68)
Test: [ 40/100]	Time  0.048 ( 0.051)	Loss 3.3105e-01 (2.6767e-01)	Acc@1  88.00 ( 93.12)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.5894e-01 (2.7144e-01)	Acc@1  95.00 ( 93.02)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 1.8713e-01 (2.6508e-01)	Acc@1  99.00 ( 93.13)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 3.8770e-01 (2.6376e-01)	Acc@1  92.00 ( 93.23)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.049 ( 0.049)	Loss 1.5161e-01 (2.6659e-01)	Acc@1  95.00 ( 93.16)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.045 ( 0.049)	Loss 3.1445e-01 (2.6511e-01)	Acc@1  94.00 ( 93.25)	Acc@5 100.00 ( 99.80)
 * Acc@1 93.310 Acc@5 99.820
### epoch[86] execution time: 52.01668858528137
EPOCH 87
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [87][  0/391]	Time  0.296 ( 0.296)	Data  0.148 ( 0.148)	Loss 8.5602e-03 (8.5602e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [87][ 10/391]	Time  0.119 ( 0.136)	Data  0.001 ( 0.014)	Loss 3.9520e-03 (6.3163e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [87][ 20/391]	Time  0.118 ( 0.129)	Data  0.001 ( 0.008)	Loss 9.1095e-03 (6.5846e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [87][ 30/391]	Time  0.127 ( 0.126)	Data  0.001 ( 0.006)	Loss 7.9193e-03 (7.2514e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [87][ 40/391]	Time  0.120 ( 0.125)	Data  0.001 ( 0.005)	Loss 2.4681e-03 (8.4203e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [87][ 50/391]	Time  0.123 ( 0.124)	Data  0.001 ( 0.004)	Loss 7.6065e-03 (8.4423e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [87][ 60/391]	Time  0.128 ( 0.123)	Data  0.001 ( 0.003)	Loss 1.4915e-02 (8.9356e-03)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [87][ 70/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.003)	Loss 1.6266e-02 (9.1267e-03)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [87][ 80/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 7.8011e-03 (8.8055e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [87][ 90/391]	Time  0.116 ( 0.122)	Data  0.001 ( 0.003)	Loss 2.3544e-02 (9.3592e-03)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [87][100/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.4475e-02 (9.7360e-03)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [87][110/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.0390e-03 (9.3408e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [87][120/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 7.9651e-03 (9.2735e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [87][130/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.3474e-03 (9.3028e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [87][140/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.5547e-03 (9.3598e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [87][150/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.7417e-03 (9.2645e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [87][160/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.1439e-03 (9.2756e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [87][170/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.9531e-02 (9.2094e-03)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [87][180/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.4670e-03 (9.1498e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [87][190/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.1509e-03 (9.0800e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [87][200/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.9836e-04 (9.0055e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [87][210/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.9400e-03 (9.0156e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [87][220/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.0648e-03 (9.1567e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [87][230/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.5309e-02 (9.1828e-03)	Acc@1  98.44 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [87][240/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.9449e-03 (9.2724e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [87][250/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.0964e-03 (9.1547e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [87][260/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.7902e-03 (9.2045e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [87][270/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.4436e-03 (9.2086e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [87][280/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.9597e-03 (9.1631e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [87][290/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.1543e-02 (9.1351e-03)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [87][300/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.9248e-03 (9.0300e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [87][310/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.7035e-03 (8.9743e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [87][320/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.1765e-02 (8.9323e-03)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [87][330/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.7755e-03 (8.9466e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [87][340/391]	Time  0.132 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.7548e-02 (9.0196e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [87][350/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.4771e-03 (8.9042e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [87][360/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.0703e-03 (8.8890e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [87][370/391]	Time  0.126 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.7815e-03 (8.8803e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [87][380/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.2939e-02 (8.9024e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [87][390/391]	Time  0.101 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.7130e-02 (8.9629e-03)	Acc@1  98.75 ( 99.82)	Acc@5 100.00 (100.00)
## e[87] optimizer.zero_grad (sum) time: 0.688305139541626
## e[87]       loss.backward (sum) time: 14.281331062316895
## e[87]      optimizer.step (sum) time: 8.037510395050049
## epoch[87] training(only) time: 47.13070273399353
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 1.9250e-01 (1.9250e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.057 ( 0.060)	Loss 2.2925e-01 (2.2595e-01)	Acc@1  97.00 ( 94.55)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.048 ( 0.054)	Loss 3.8135e-01 (2.5643e-01)	Acc@1  88.00 ( 93.57)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 2.7271e-01 (2.6566e-01)	Acc@1  92.00 ( 93.32)	Acc@5  99.00 ( 99.61)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 3.1982e-01 (2.6459e-01)	Acc@1  90.00 ( 93.27)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.048 ( 0.050)	Loss 1.5479e-01 (2.6796e-01)	Acc@1  96.00 ( 93.12)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.8054e-01 (2.6181e-01)	Acc@1  98.00 ( 93.23)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 3.9648e-01 (2.6016e-01)	Acc@1  92.00 ( 93.30)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.7688e-01 (2.6371e-01)	Acc@1  94.00 ( 93.27)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.046 ( 0.048)	Loss 2.9565e-01 (2.6137e-01)	Acc@1  94.00 ( 93.35)	Acc@5 100.00 ( 99.79)
 * Acc@1 93.430 Acc@5 99.810
### epoch[87] execution time: 52.02634644508362
EPOCH 88
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [88][  0/391]	Time  0.295 ( 0.295)	Data  0.148 ( 0.148)	Loss 5.1842e-03 (5.1842e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [88][ 10/391]	Time  0.118 ( 0.136)	Data  0.001 ( 0.014)	Loss 2.2202e-03 (1.2295e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [88][ 20/391]	Time  0.118 ( 0.128)	Data  0.001 ( 0.008)	Loss 6.4812e-03 (1.0354e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [88][ 30/391]	Time  0.118 ( 0.125)	Data  0.001 ( 0.006)	Loss 7.3280e-03 (1.0109e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [88][ 40/391]	Time  0.119 ( 0.124)	Data  0.001 ( 0.005)	Loss 5.3520e-03 (1.0026e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [88][ 50/391]	Time  0.121 ( 0.123)	Data  0.001 ( 0.004)	Loss 7.7858e-03 (9.7533e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [88][ 60/391]	Time  0.117 ( 0.123)	Data  0.001 ( 0.003)	Loss 8.6594e-03 (1.0072e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [88][ 70/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.1292e-02 (9.7344e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [88][ 80/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 5.3635e-03 (9.4057e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [88][ 90/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 3.6583e-03 (8.9340e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [88][100/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.1193e-03 (8.6256e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [88][110/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.8509e-03 (8.3834e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [88][120/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.2158e-03 (8.2362e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [88][130/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.3678e-03 (8.0342e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [88][140/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.0724e-03 (8.0219e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [88][150/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.5624e-03 (7.8945e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [88][160/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.1564e-03 (7.7629e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [88][170/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.7866e-03 (7.8320e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [88][180/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.7426e-02 (8.1775e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [88][190/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.4908e-02 (8.1621e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [88][200/391]	Time  0.128 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.2316e-03 (8.1595e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [88][210/391]	Time  0.124 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.6218e-03 (8.1636e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [88][220/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.1384e-03 (8.1725e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [88][230/391]	Time  0.126 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.6430e-03 (8.3757e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [88][240/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.3084e-02 (8.4929e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [88][250/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.2370e-03 (8.3983e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [88][260/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.7520e-03 (8.3851e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [88][270/391]	Time  0.134 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.2708e-03 (8.4298e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [88][280/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.9111e-03 (8.4788e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [88][290/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.1798e-03 (8.5953e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [88][300/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 9.0561e-03 (8.5697e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [88][310/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.0310e-03 (8.5795e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [88][320/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 7.7667e-03 (8.5871e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [88][330/391]	Time  0.122 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.1246e-02 (8.6165e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [88][340/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.8172e-03 (8.5616e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [88][350/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.4624e-03 (8.5446e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [88][360/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.6011e-03 (8.5270e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [88][370/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.4442e-02 (8.5211e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [88][380/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.9629e-03 (8.5279e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [88][390/391]	Time  0.110 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.4970e-03 (8.4677e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
## e[88] optimizer.zero_grad (sum) time: 0.6835393905639648
## e[88]       loss.backward (sum) time: 14.293907642364502
## e[88]      optimizer.step (sum) time: 7.976994276046753
## epoch[88] training(only) time: 47.1024215221405
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 1.7090e-01 (1.7090e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.045 ( 0.060)	Loss 2.5073e-01 (2.2494e-01)	Acc@1  96.00 ( 94.09)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 4.1699e-01 (2.5555e-01)	Acc@1  88.00 ( 93.33)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 2.7393e-01 (2.6293e-01)	Acc@1  91.00 ( 93.19)	Acc@5  99.00 ( 99.61)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 3.1543e-01 (2.6289e-01)	Acc@1  88.00 ( 93.07)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.5796e-01 (2.6705e-01)	Acc@1  95.00 ( 92.96)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 1.8652e-01 (2.6077e-01)	Acc@1  98.00 ( 93.08)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 3.9282e-01 (2.6036e-01)	Acc@1  92.00 ( 93.11)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.3599e-01 (2.6272e-01)	Acc@1  94.00 ( 93.11)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 3.0396e-01 (2.6117e-01)	Acc@1  94.00 ( 93.20)	Acc@5 100.00 ( 99.78)
 * Acc@1 93.270 Acc@5 99.800
### epoch[88] execution time: 52.05366325378418
EPOCH 89
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [89][  0/391]	Time  0.273 ( 0.273)	Data  0.139 ( 0.139)	Loss 2.3041e-03 (2.3041e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [89][ 10/391]	Time  0.118 ( 0.134)	Data  0.001 ( 0.013)	Loss 8.4000e-03 (6.4692e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [89][ 20/391]	Time  0.120 ( 0.127)	Data  0.001 ( 0.008)	Loss 1.3359e-02 (8.5419e-03)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [89][ 30/391]	Time  0.118 ( 0.125)	Data  0.001 ( 0.005)	Loss 2.6817e-03 (7.5686e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [89][ 40/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.004)	Loss 4.8676e-03 (7.7399e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [89][ 50/391]	Time  0.117 ( 0.123)	Data  0.001 ( 0.004)	Loss 1.2146e-02 (7.7047e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [89][ 60/391]	Time  0.130 ( 0.122)	Data  0.001 ( 0.003)	Loss 8.6365e-03 (7.9209e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [89][ 70/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 5.2757e-03 (8.0072e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [89][ 80/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 2.4094e-02 (8.3084e-03)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [89][ 90/391]	Time  0.125 ( 0.121)	Data  0.001 ( 0.003)	Loss 4.6310e-03 (8.3580e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [89][100/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.5419e-03 (8.3612e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [89][110/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.9417e-03 (8.2086e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [89][120/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.8586e-03 (8.2011e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [89][130/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.0779e-03 (8.0826e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [89][140/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.9569e-03 (8.6329e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [89][150/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.5738e-03 (8.5467e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [89][160/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.5869e-02 (8.5519e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [89][170/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.0027e-03 (8.3678e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [89][180/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.9275e-03 (8.5082e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [89][190/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.9177e-03 (8.3863e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [89][200/391]	Time  0.116 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.3940e-03 (8.4959e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [89][210/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.0850e-03 (8.6209e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [89][220/391]	Time  0.123 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.8511e-03 (8.5102e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [89][230/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.9144e-03 (8.5714e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [89][240/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 7.1983e-03 (8.6712e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [89][250/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.8692e-02 (8.7440e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [89][260/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 1.7212e-02 (8.7417e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [89][270/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.0092e-03 (8.6730e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [89][280/391]	Time  0.125 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.5060e-02 (8.5979e-03)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [89][290/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.7678e-03 (8.4883e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [89][300/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.1757e-02 (8.4403e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [89][310/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.3864e-03 (8.4210e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [89][320/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.9144e-03 (8.4847e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [89][330/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.2755e-03 (8.5110e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [89][340/391]	Time  0.122 ( 0.120)	Data  0.001 ( 0.001)	Loss 9.6283e-03 (8.5045e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [89][350/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.1662e-03 (8.4582e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [89][360/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 2.9602e-03 (8.4045e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [89][370/391]	Time  0.125 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.1803e-02 (8.5317e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [89][380/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.9471e-03 (8.4568e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [89][390/391]	Time  0.105 ( 0.120)	Data  0.001 ( 0.001)	Loss 8.9455e-04 (8.4723e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
## e[89] optimizer.zero_grad (sum) time: 0.6840884685516357
## e[89]       loss.backward (sum) time: 14.29032588005066
## e[89]      optimizer.step (sum) time: 8.009150981903076
## epoch[89] training(only) time: 47.06629657745361
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 1.8713e-01 (1.8713e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.060 ( 0.059)	Loss 2.3267e-01 (2.3102e-01)	Acc@1  97.00 ( 94.27)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 4.1748e-01 (2.5934e-01)	Acc@1  88.00 ( 93.67)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 2.7759e-01 (2.6549e-01)	Acc@1  91.00 ( 93.52)	Acc@5  99.00 ( 99.61)
Test: [ 40/100]	Time  0.049 ( 0.051)	Loss 3.5181e-01 (2.6594e-01)	Acc@1  88.00 ( 93.37)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.5222e-01 (2.6944e-01)	Acc@1  95.00 ( 93.24)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.055 ( 0.050)	Loss 1.9470e-01 (2.6268e-01)	Acc@1  98.00 ( 93.30)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 3.9478e-01 (2.6167e-01)	Acc@1  92.00 ( 93.32)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.045 ( 0.049)	Loss 1.4636e-01 (2.6360e-01)	Acc@1  94.00 ( 93.28)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 3.1738e-01 (2.6246e-01)	Acc@1  93.00 ( 93.32)	Acc@5 100.00 ( 99.78)
 * Acc@1 93.350 Acc@5 99.800
### epoch[89] execution time: 52.0326669216156
### Training complete:
#### total training(only) time: 4237.148603916168
##### Total run time: 4686.670519351959
