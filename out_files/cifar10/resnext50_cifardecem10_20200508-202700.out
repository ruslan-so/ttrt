# Model: resnext50
# Dataset: cifardecem
# Batch size: 128
# Freezeout: False
# Low precision: False
# Epochs: 90
# Initial learning rate: 0.1
# module_name: models_cifardecem.resnext
<function resnext50 at 0x7f83a0845f28>
# model requested: 'resnext50'
# printing out the model
ResNext(
  (conv1): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (conv2): Sequential(
    (0): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
    (2): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
  )
  (conv3): Sequential(
    (0): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
    (2): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
    (3): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
  )
  (conv4): Sequential(
    (0): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
    (2): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
    (3): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
    (4): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
    (5): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
  )
  (conv5): Sequential(
    (0): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
    (2): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
  )
  (avg): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=2048, out_features=10, bias=True)
)
# model is full precision
# Model: resnext50
# Dataset: cifardecem
# Freezeout: False
# Low precision: False
# Initial learning rate: 0.1
# Criterion: CrossEntropyLoss()
# Optimizer: SGD
#	lr 0.1
#	momentum 0.9
#	dampening 0
#	weight_decay 0.0001
#	nesterov False
CIFAR10 loading and normalization
==> Preparing data..
# CIFAR10 / full precision
Files already downloaded and verified
Files already downloaded and verified
#--> Training data: <--#
#-->>
EPOCH 0
# current learning rate: 0.1
# Switched to train mode...
Epoch: [0][  0/391]	Time  4.426 ( 4.426)	Data  0.116 ( 0.116)	Loss 2.3801e+00 (2.3801e+00)	Acc@1   9.38 (  9.38)	Acc@5  60.94 ( 60.94)
Epoch: [0][ 10/391]	Time  0.127 ( 0.519)	Data  0.001 ( 0.012)	Loss 1.5989e+01 (2.5670e+01)	Acc@1   6.25 ( 10.23)	Acc@5  52.34 ( 50.14)
Epoch: [0][ 20/391]	Time  0.126 ( 0.333)	Data  0.001 ( 0.008)	Loss 1.0881e+01 (1.7258e+01)	Acc@1   7.81 ( 10.01)	Acc@5  46.88 ( 49.85)
Epoch: [0][ 30/391]	Time  0.126 ( 0.267)	Data  0.001 ( 0.007)	Loss 2.7806e+00 (1.3293e+01)	Acc@1   9.38 ( 10.01)	Acc@5  50.00 ( 50.08)
Epoch: [0][ 40/391]	Time  0.126 ( 0.233)	Data  0.001 ( 0.006)	Loss 2.9221e+00 (1.0879e+01)	Acc@1   7.03 ( 10.14)	Acc@5  46.88 ( 50.36)
Epoch: [0][ 50/391]	Time  0.123 ( 0.213)	Data  0.001 ( 0.006)	Loss 2.5187e+00 (9.3954e+00)	Acc@1   7.03 (  9.97)	Acc@5  53.91 ( 50.70)
Epoch: [0][ 60/391]	Time  0.126 ( 0.199)	Data  0.001 ( 0.005)	Loss 2.4800e+00 (8.3643e+00)	Acc@1  11.72 (  9.99)	Acc@5  48.44 ( 50.92)
Epoch: [0][ 70/391]	Time  0.127 ( 0.189)	Data  0.001 ( 0.005)	Loss 2.3091e+00 (7.5176e+00)	Acc@1  14.84 ( 10.11)	Acc@5  58.59 ( 51.21)
Epoch: [0][ 80/391]	Time  0.128 ( 0.182)	Data  0.001 ( 0.005)	Loss 2.2900e+00 (6.9009e+00)	Acc@1   9.38 ( 10.11)	Acc@5  48.44 ( 51.13)
Epoch: [0][ 90/391]	Time  0.127 ( 0.176)	Data  0.001 ( 0.005)	Loss 2.3403e+00 (6.4030e+00)	Acc@1  11.72 ( 10.22)	Acc@5  53.91 ( 51.31)
Epoch: [0][100/391]	Time  0.135 ( 0.172)	Data  0.001 ( 0.005)	Loss 2.3221e+00 (5.9982e+00)	Acc@1  11.72 ( 10.31)	Acc@5  53.91 ( 51.34)
Epoch: [0][110/391]	Time  0.126 ( 0.168)	Data  0.001 ( 0.005)	Loss 2.2680e+00 (5.6657e+00)	Acc@1  11.72 ( 10.40)	Acc@5  54.69 ( 51.34)
Epoch: [0][120/391]	Time  0.129 ( 0.165)	Data  0.001 ( 0.005)	Loss 2.2568e+00 (5.3868e+00)	Acc@1  17.97 ( 10.58)	Acc@5  55.47 ( 51.69)
Epoch: [0][130/391]	Time  0.128 ( 0.162)	Data  0.001 ( 0.005)	Loss 2.3644e+00 (5.1506e+00)	Acc@1   8.59 ( 10.68)	Acc@5  54.69 ( 52.05)
Epoch: [0][140/391]	Time  0.134 ( 0.160)	Data  0.001 ( 0.005)	Loss 2.3134e+00 (4.9492e+00)	Acc@1  11.72 ( 10.77)	Acc@5  54.69 ( 52.55)
Epoch: [0][150/391]	Time  0.129 ( 0.158)	Data  0.001 ( 0.005)	Loss 2.2943e+00 (4.7761e+00)	Acc@1  10.16 ( 10.89)	Acc@5  53.12 ( 52.91)
Epoch: [0][160/391]	Time  0.127 ( 0.156)	Data  0.001 ( 0.004)	Loss 2.2439e+00 (4.6218e+00)	Acc@1  11.72 ( 10.98)	Acc@5  62.50 ( 53.21)
Epoch: [0][170/391]	Time  0.134 ( 0.155)	Data  0.001 ( 0.004)	Loss 2.2761e+00 (4.4842e+00)	Acc@1  12.50 ( 11.06)	Acc@5  64.06 ( 53.50)
Epoch: [0][180/391]	Time  0.133 ( 0.154)	Data  0.001 ( 0.004)	Loss 2.2810e+00 (4.3639e+00)	Acc@1   7.81 ( 11.08)	Acc@5  53.91 ( 53.60)
Epoch: [0][190/391]	Time  0.135 ( 0.153)	Data  0.001 ( 0.004)	Loss 2.2948e+00 (4.2564e+00)	Acc@1   6.25 ( 11.15)	Acc@5  50.00 ( 53.64)
Epoch: [0][200/391]	Time  0.130 ( 0.152)	Data  0.001 ( 0.004)	Loss 2.2934e+00 (4.1572e+00)	Acc@1   7.81 ( 11.27)	Acc@5  46.88 ( 53.91)
Epoch: [0][210/391]	Time  0.128 ( 0.151)	Data  0.001 ( 0.004)	Loss 2.2646e+00 (4.0676e+00)	Acc@1  12.50 ( 11.46)	Acc@5  59.38 ( 54.08)
Epoch: [0][220/391]	Time  0.133 ( 0.150)	Data  0.001 ( 0.004)	Loss 2.2446e+00 (3.9864e+00)	Acc@1  17.97 ( 11.60)	Acc@5  58.59 ( 54.26)
Epoch: [0][230/391]	Time  0.128 ( 0.149)	Data  0.001 ( 0.004)	Loss 2.2740e+00 (3.9125e+00)	Acc@1  12.50 ( 11.67)	Acc@5  52.34 ( 54.27)
Epoch: [0][240/391]	Time  0.134 ( 0.149)	Data  0.001 ( 0.004)	Loss 2.2579e+00 (3.8438e+00)	Acc@1  20.31 ( 11.85)	Acc@5  67.19 ( 54.64)
Epoch: [0][250/391]	Time  0.133 ( 0.148)	Data  0.001 ( 0.004)	Loss 2.1985e+00 (3.7806e+00)	Acc@1  21.88 ( 12.00)	Acc@5  65.62 ( 54.97)
Epoch: [0][260/391]	Time  0.129 ( 0.147)	Data  0.001 ( 0.004)	Loss 2.2418e+00 (3.7211e+00)	Acc@1  18.75 ( 12.21)	Acc@5  60.94 ( 55.32)
Epoch: [0][270/391]	Time  0.127 ( 0.147)	Data  0.001 ( 0.004)	Loss 2.2112e+00 (3.6657e+00)	Acc@1  22.66 ( 12.42)	Acc@5  66.41 ( 55.71)
Epoch: [0][280/391]	Time  0.128 ( 0.146)	Data  0.001 ( 0.004)	Loss 2.2554e+00 (3.6145e+00)	Acc@1  17.97 ( 12.57)	Acc@5  65.62 ( 56.05)
Epoch: [0][290/391]	Time  0.136 ( 0.146)	Data  0.001 ( 0.004)	Loss 2.2136e+00 (3.5667e+00)	Acc@1  15.62 ( 12.67)	Acc@5  72.66 ( 56.39)
Epoch: [0][300/391]	Time  0.133 ( 0.146)	Data  0.001 ( 0.004)	Loss 2.1456e+00 (3.5211e+00)	Acc@1  13.28 ( 12.81)	Acc@5  71.09 ( 56.78)
Epoch: [0][310/391]	Time  0.133 ( 0.145)	Data  0.001 ( 0.004)	Loss 2.1827e+00 (3.4777e+00)	Acc@1  13.28 ( 12.92)	Acc@5  66.41 ( 57.17)
Epoch: [0][320/391]	Time  0.129 ( 0.145)	Data  0.001 ( 0.004)	Loss 2.1725e+00 (3.4374e+00)	Acc@1  17.19 ( 13.05)	Acc@5  65.62 ( 57.42)
Epoch: [0][330/391]	Time  0.136 ( 0.145)	Data  0.001 ( 0.004)	Loss 2.1816e+00 (3.3991e+00)	Acc@1  25.78 ( 13.23)	Acc@5  64.06 ( 57.79)
Epoch: [0][340/391]	Time  0.132 ( 0.144)	Data  0.001 ( 0.004)	Loss 2.2202e+00 (3.3629e+00)	Acc@1  17.19 ( 13.33)	Acc@5  70.31 ( 58.20)
Epoch: [0][350/391]	Time  0.134 ( 0.144)	Data  0.001 ( 0.004)	Loss 2.1572e+00 (3.3271e+00)	Acc@1  18.75 ( 13.47)	Acc@5  71.09 ( 58.66)
Epoch: [0][360/391]	Time  0.135 ( 0.144)	Data  0.001 ( 0.004)	Loss 2.1971e+00 (3.2931e+00)	Acc@1  16.41 ( 13.67)	Acc@5  71.88 ( 59.09)
Epoch: [0][370/391]	Time  0.135 ( 0.143)	Data  0.001 ( 0.004)	Loss 2.0351e+00 (3.2606e+00)	Acc@1  21.09 ( 13.83)	Acc@5  74.22 ( 59.51)
Epoch: [0][380/391]	Time  0.134 ( 0.143)	Data  0.001 ( 0.004)	Loss 2.0681e+00 (3.2299e+00)	Acc@1  17.97 ( 14.00)	Acc@5  79.69 ( 59.91)
Epoch: [0][390/391]	Time  1.117 ( 0.146)	Data  0.001 ( 0.004)	Loss 2.0793e+00 (3.2007e+00)	Acc@1  18.75 ( 14.19)	Acc@5  77.50 ( 60.33)
## e[0] optimizer.zero_grad (sum) time: 0.2962822914123535
## e[0]       loss.backward (sum) time: 19.75422739982605
## e[0]      optimizer.step (sum) time: 2.8200979232788086
## epoch[0] training(only) time: 56.9577956199646
# Switched to evaluate mode...
Test: [  0/100]	Time  0.435 ( 0.435)	Loss 2.0351e+00 (2.0351e+00)	Acc@1  27.00 ( 27.00)	Acc@5  80.00 ( 80.00)
Test: [ 10/100]	Time  0.044 ( 0.085)	Loss 2.0373e+00 (2.0514e+00)	Acc@1  28.00 ( 23.09)	Acc@5  72.00 ( 77.36)
Test: [ 20/100]	Time  0.042 ( 0.066)	Loss 2.0436e+00 (2.0483e+00)	Acc@1  20.00 ( 22.67)	Acc@5  72.00 ( 76.71)
Test: [ 30/100]	Time  0.044 ( 0.058)	Loss 1.9893e+00 (2.0446e+00)	Acc@1  30.00 ( 22.84)	Acc@5  82.00 ( 76.61)
Test: [ 40/100]	Time  0.043 ( 0.055)	Loss 2.0682e+00 (2.0502e+00)	Acc@1  24.00 ( 22.41)	Acc@5  81.00 ( 76.98)
Test: [ 50/100]	Time  0.042 ( 0.052)	Loss 2.0778e+00 (2.0471e+00)	Acc@1  21.00 ( 22.12)	Acc@5  77.00 ( 77.18)
Test: [ 60/100]	Time  0.042 ( 0.051)	Loss 2.0850e+00 (2.0531e+00)	Acc@1  18.00 ( 21.85)	Acc@5  70.00 ( 77.03)
Test: [ 70/100]	Time  0.042 ( 0.050)	Loss 2.1699e+00 (2.0495e+00)	Acc@1  23.00 ( 21.97)	Acc@5  71.00 ( 76.97)
Test: [ 80/100]	Time  0.045 ( 0.049)	Loss 2.0406e+00 (2.0480e+00)	Acc@1  25.00 ( 22.01)	Acc@5  81.00 ( 77.15)
Test: [ 90/100]	Time  0.044 ( 0.049)	Loss 2.2150e+00 (2.0502e+00)	Acc@1  22.00 ( 22.24)	Acc@5  78.00 ( 77.13)
 * Acc@1 21.980 Acc@5 77.310
### epoch[0] execution time: 61.876028537750244
EPOCH 1
# current learning rate: 0.1
# Switched to train mode...
Epoch: [1][  0/391]	Time  0.314 ( 0.314)	Data  0.172 ( 0.172)	Loss 2.0751e+00 (2.0751e+00)	Acc@1  17.97 ( 17.97)	Acc@5  75.00 ( 75.00)
Epoch: [1][ 10/391]	Time  0.135 ( 0.149)	Data  0.001 ( 0.019)	Loss 2.0845e+00 (2.0690e+00)	Acc@1  22.66 ( 21.38)	Acc@5  71.09 ( 74.64)
Epoch: [1][ 20/391]	Time  0.131 ( 0.142)	Data  0.001 ( 0.012)	Loss 2.1238e+00 (2.0558e+00)	Acc@1  19.53 ( 22.43)	Acc@5  78.12 ( 75.74)
Epoch: [1][ 30/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.009)	Loss 2.0597e+00 (2.0523e+00)	Acc@1  14.84 ( 21.95)	Acc@5  76.56 ( 75.78)
Epoch: [1][ 40/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.008)	Loss 1.9599e+00 (2.0530e+00)	Acc@1  27.34 ( 22.14)	Acc@5  76.56 ( 75.99)
Epoch: [1][ 50/391]	Time  0.130 ( 0.137)	Data  0.001 ( 0.007)	Loss 1.9483e+00 (2.0407e+00)	Acc@1  22.66 ( 21.95)	Acc@5  77.34 ( 76.46)
Epoch: [1][ 60/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.007)	Loss 2.0254e+00 (2.0423e+00)	Acc@1  22.66 ( 22.05)	Acc@5  78.12 ( 76.51)
Epoch: [1][ 70/391]	Time  0.131 ( 0.136)	Data  0.001 ( 0.007)	Loss 2.1041e+00 (2.0382e+00)	Acc@1  21.09 ( 22.06)	Acc@5  80.47 ( 76.88)
Epoch: [1][ 80/391]	Time  0.134 ( 0.136)	Data  0.001 ( 0.006)	Loss 1.9225e+00 (2.0320e+00)	Acc@1  19.53 ( 22.10)	Acc@5  85.94 ( 77.35)
Epoch: [1][ 90/391]	Time  0.135 ( 0.136)	Data  0.001 ( 0.006)	Loss 1.9751e+00 (2.0221e+00)	Acc@1  27.34 ( 22.44)	Acc@5  81.25 ( 77.77)
Epoch: [1][100/391]	Time  0.132 ( 0.136)	Data  0.001 ( 0.006)	Loss 1.9729e+00 (2.0157e+00)	Acc@1  15.62 ( 22.45)	Acc@5  77.34 ( 77.92)
Epoch: [1][110/391]	Time  0.135 ( 0.136)	Data  0.001 ( 0.006)	Loss 1.9393e+00 (2.0094e+00)	Acc@1  22.66 ( 22.68)	Acc@5  80.47 ( 78.20)
Epoch: [1][120/391]	Time  0.137 ( 0.136)	Data  0.001 ( 0.006)	Loss 1.8911e+00 (2.0010e+00)	Acc@1  25.00 ( 22.80)	Acc@5  82.81 ( 78.59)
Epoch: [1][130/391]	Time  0.134 ( 0.136)	Data  0.001 ( 0.006)	Loss 1.9569e+00 (1.9926e+00)	Acc@1  27.34 ( 23.03)	Acc@5  79.69 ( 78.89)
Epoch: [1][140/391]	Time  0.132 ( 0.136)	Data  0.002 ( 0.005)	Loss 2.0155e+00 (1.9877e+00)	Acc@1  19.53 ( 23.15)	Acc@5  77.34 ( 79.11)
Epoch: [1][150/391]	Time  0.134 ( 0.136)	Data  0.001 ( 0.005)	Loss 1.9296e+00 (1.9814e+00)	Acc@1  18.75 ( 23.33)	Acc@5  80.47 ( 79.34)
Epoch: [1][160/391]	Time  0.140 ( 0.136)	Data  0.001 ( 0.005)	Loss 1.8792e+00 (1.9751e+00)	Acc@1  30.47 ( 23.68)	Acc@5  79.69 ( 79.66)
Epoch: [1][170/391]	Time  0.134 ( 0.136)	Data  0.001 ( 0.005)	Loss 1.8359e+00 (1.9693e+00)	Acc@1  31.25 ( 23.98)	Acc@5  84.38 ( 79.85)
Epoch: [1][180/391]	Time  0.131 ( 0.136)	Data  0.001 ( 0.005)	Loss 2.0337e+00 (1.9642e+00)	Acc@1  21.09 ( 24.12)	Acc@5  75.00 ( 80.06)
Epoch: [1][190/391]	Time  0.132 ( 0.136)	Data  0.001 ( 0.005)	Loss 1.8969e+00 (1.9575e+00)	Acc@1  21.88 ( 24.27)	Acc@5  82.03 ( 80.31)
Epoch: [1][200/391]	Time  0.129 ( 0.136)	Data  0.001 ( 0.005)	Loss 1.9235e+00 (1.9541e+00)	Acc@1  25.00 ( 24.31)	Acc@5  85.94 ( 80.52)
Epoch: [1][210/391]	Time  0.134 ( 0.136)	Data  0.001 ( 0.005)	Loss 1.8970e+00 (1.9510e+00)	Acc@1  25.00 ( 24.38)	Acc@5  87.50 ( 80.67)
Epoch: [1][220/391]	Time  0.133 ( 0.136)	Data  0.001 ( 0.005)	Loss 1.9200e+00 (1.9461e+00)	Acc@1  22.66 ( 24.54)	Acc@5  85.94 ( 80.85)
Epoch: [1][230/391]	Time  0.135 ( 0.136)	Data  0.001 ( 0.005)	Loss 1.7340e+00 (1.9409e+00)	Acc@1  36.72 ( 24.73)	Acc@5  85.94 ( 80.97)
Epoch: [1][240/391]	Time  0.133 ( 0.136)	Data  0.001 ( 0.005)	Loss 1.7963e+00 (1.9358e+00)	Acc@1  36.72 ( 24.92)	Acc@5  84.38 ( 81.16)
Epoch: [1][250/391]	Time  0.137 ( 0.136)	Data  0.001 ( 0.005)	Loss 1.8409e+00 (1.9312e+00)	Acc@1  28.12 ( 25.15)	Acc@5  88.28 ( 81.29)
Epoch: [1][260/391]	Time  0.134 ( 0.136)	Data  0.001 ( 0.005)	Loss 1.7839e+00 (1.9274e+00)	Acc@1  28.12 ( 25.34)	Acc@5  86.72 ( 81.40)
Epoch: [1][270/391]	Time  0.128 ( 0.136)	Data  0.001 ( 0.005)	Loss 1.7829e+00 (1.9237e+00)	Acc@1  31.25 ( 25.52)	Acc@5  82.03 ( 81.45)
Epoch: [1][280/391]	Time  0.134 ( 0.136)	Data  0.001 ( 0.005)	Loss 1.6492e+00 (1.9184e+00)	Acc@1  33.59 ( 25.75)	Acc@5  92.97 ( 81.60)
Epoch: [1][290/391]	Time  0.134 ( 0.136)	Data  0.001 ( 0.005)	Loss 1.7915e+00 (1.9150e+00)	Acc@1  30.47 ( 25.85)	Acc@5  85.16 ( 81.72)
Epoch: [1][300/391]	Time  0.135 ( 0.136)	Data  0.001 ( 0.005)	Loss 1.8474e+00 (1.9125e+00)	Acc@1  25.78 ( 25.97)	Acc@5  82.03 ( 81.79)
Epoch: [1][310/391]	Time  0.134 ( 0.136)	Data  0.001 ( 0.005)	Loss 1.8997e+00 (1.9097e+00)	Acc@1  30.47 ( 26.10)	Acc@5  82.81 ( 81.90)
Epoch: [1][320/391]	Time  0.132 ( 0.136)	Data  0.001 ( 0.005)	Loss 1.7060e+00 (1.9061e+00)	Acc@1  32.03 ( 26.27)	Acc@5  89.84 ( 82.07)
Epoch: [1][330/391]	Time  0.138 ( 0.136)	Data  0.001 ( 0.005)	Loss 1.9550e+00 (1.9035e+00)	Acc@1  23.44 ( 26.34)	Acc@5  82.81 ( 82.18)
Epoch: [1][340/391]	Time  0.132 ( 0.136)	Data  0.001 ( 0.005)	Loss 1.7897e+00 (1.9011e+00)	Acc@1  32.03 ( 26.46)	Acc@5  83.59 ( 82.25)
Epoch: [1][350/391]	Time  0.135 ( 0.136)	Data  0.001 ( 0.005)	Loss 1.7606e+00 (1.8982e+00)	Acc@1  32.03 ( 26.58)	Acc@5  88.28 ( 82.35)
Epoch: [1][360/391]	Time  0.135 ( 0.136)	Data  0.001 ( 0.005)	Loss 1.6752e+00 (1.8941e+00)	Acc@1  33.59 ( 26.80)	Acc@5  91.41 ( 82.46)
Epoch: [1][370/391]	Time  0.133 ( 0.136)	Data  0.001 ( 0.005)	Loss 1.8279e+00 (1.8907e+00)	Acc@1  35.94 ( 27.01)	Acc@5  83.59 ( 82.56)
Epoch: [1][380/391]	Time  0.136 ( 0.136)	Data  0.001 ( 0.005)	Loss 1.8217e+00 (1.8868e+00)	Acc@1  28.91 ( 27.20)	Acc@5  85.94 ( 82.69)
Epoch: [1][390/391]	Time  0.128 ( 0.136)	Data  0.001 ( 0.005)	Loss 1.8454e+00 (1.8835e+00)	Acc@1  23.75 ( 27.26)	Acc@5  86.25 ( 82.81)
## e[1] optimizer.zero_grad (sum) time: 0.29221391677856445
## e[1]       loss.backward (sum) time: 18.49725580215454
## e[1]      optimizer.step (sum) time: 2.877225399017334
## epoch[1] training(only) time: 53.16232347488403
# Switched to evaluate mode...
Test: [  0/100]	Time  0.191 ( 0.191)	Loss 1.6224e+00 (1.6224e+00)	Acc@1  37.00 ( 37.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.044 ( 0.057)	Loss 1.7223e+00 (1.6968e+00)	Acc@1  40.00 ( 35.82)	Acc@5  87.00 ( 88.55)
Test: [ 20/100]	Time  0.042 ( 0.050)	Loss 1.6827e+00 (1.7045e+00)	Acc@1  40.00 ( 35.33)	Acc@5  88.00 ( 89.00)
Test: [ 30/100]	Time  0.045 ( 0.048)	Loss 1.5211e+00 (1.7106e+00)	Acc@1  43.00 ( 35.39)	Acc@5  95.00 ( 88.61)
Test: [ 40/100]	Time  0.041 ( 0.047)	Loss 1.8055e+00 (1.7151e+00)	Acc@1  36.00 ( 35.68)	Acc@5  85.00 ( 88.07)
Test: [ 50/100]	Time  0.044 ( 0.047)	Loss 1.7147e+00 (1.7076e+00)	Acc@1  29.00 ( 36.04)	Acc@5  89.00 ( 88.12)
Test: [ 60/100]	Time  0.045 ( 0.046)	Loss 1.6764e+00 (1.7137e+00)	Acc@1  34.00 ( 35.46)	Acc@5  87.00 ( 88.08)
Test: [ 70/100]	Time  0.043 ( 0.046)	Loss 1.8050e+00 (1.7137e+00)	Acc@1  39.00 ( 35.20)	Acc@5  85.00 ( 87.92)
Test: [ 80/100]	Time  0.046 ( 0.045)	Loss 1.6979e+00 (1.7120e+00)	Acc@1  41.00 ( 35.36)	Acc@5  84.00 ( 88.00)
Test: [ 90/100]	Time  0.043 ( 0.045)	Loss 1.7441e+00 (1.7185e+00)	Acc@1  31.00 ( 34.92)	Acc@5  91.00 ( 87.96)
 * Acc@1 34.840 Acc@5 87.770
### epoch[1] execution time: 57.78033113479614
EPOCH 2
# current learning rate: 0.1
# Switched to train mode...
Epoch: [2][  0/391]	Time  0.297 ( 0.297)	Data  0.181 ( 0.181)	Loss 1.6521e+00 (1.6521e+00)	Acc@1  39.84 ( 39.84)	Acc@5  87.50 ( 87.50)
Epoch: [2][ 10/391]	Time  0.131 ( 0.149)	Data  0.001 ( 0.020)	Loss 1.6559e+00 (1.7350e+00)	Acc@1  42.19 ( 35.09)	Acc@5  91.41 ( 87.78)
Epoch: [2][ 20/391]	Time  0.130 ( 0.142)	Data  0.001 ( 0.013)	Loss 1.6887e+00 (1.7367e+00)	Acc@1  43.75 ( 34.90)	Acc@5  85.94 ( 87.20)
Epoch: [2][ 30/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.010)	Loss 1.7876e+00 (1.7385e+00)	Acc@1  32.03 ( 33.82)	Acc@5  80.47 ( 86.77)
Epoch: [2][ 40/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.009)	Loss 1.5903e+00 (1.7308e+00)	Acc@1  40.62 ( 34.43)	Acc@5  91.41 ( 86.97)
Epoch: [2][ 50/391]	Time  0.130 ( 0.138)	Data  0.001 ( 0.008)	Loss 1.6681e+00 (1.7269e+00)	Acc@1  40.62 ( 34.44)	Acc@5  89.06 ( 86.90)
Epoch: [2][ 60/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.007)	Loss 1.6531e+00 (1.7241e+00)	Acc@1  39.84 ( 34.75)	Acc@5  89.06 ( 86.97)
Epoch: [2][ 70/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.007)	Loss 1.6436e+00 (1.7266e+00)	Acc@1  41.41 ( 34.60)	Acc@5  86.72 ( 86.98)
Epoch: [2][ 80/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.006)	Loss 1.6608e+00 (1.7313e+00)	Acc@1  32.81 ( 34.22)	Acc@5  89.06 ( 86.94)
Epoch: [2][ 90/391]	Time  0.141 ( 0.137)	Data  0.001 ( 0.006)	Loss 1.6264e+00 (1.7258e+00)	Acc@1  34.38 ( 34.40)	Acc@5  92.97 ( 87.08)
Epoch: [2][100/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.006)	Loss 1.6579e+00 (1.7216e+00)	Acc@1  34.38 ( 34.44)	Acc@5  89.06 ( 87.11)
Epoch: [2][110/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.006)	Loss 1.7404e+00 (1.7203e+00)	Acc@1  34.38 ( 34.48)	Acc@5  89.06 ( 87.13)
Epoch: [2][120/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.006)	Loss 1.6971e+00 (1.7166e+00)	Acc@1  40.62 ( 34.71)	Acc@5  88.28 ( 87.15)
Epoch: [2][130/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.006)	Loss 1.9145e+00 (1.7144e+00)	Acc@1  27.34 ( 34.84)	Acc@5  82.81 ( 87.21)
Epoch: [2][140/391]	Time  0.129 ( 0.137)	Data  0.001 ( 0.006)	Loss 1.6513e+00 (1.7102e+00)	Acc@1  39.06 ( 35.16)	Acc@5  88.28 ( 87.24)
Epoch: [2][150/391]	Time  0.136 ( 0.137)	Data  0.002 ( 0.006)	Loss 1.5576e+00 (1.7046e+00)	Acc@1  34.38 ( 35.36)	Acc@5  92.19 ( 87.32)
Epoch: [2][160/391]	Time  0.131 ( 0.136)	Data  0.001 ( 0.005)	Loss 1.6290e+00 (1.7024e+00)	Acc@1  39.06 ( 35.53)	Acc@5  89.84 ( 87.42)
Epoch: [2][170/391]	Time  0.136 ( 0.136)	Data  0.001 ( 0.005)	Loss 1.5519e+00 (1.6973e+00)	Acc@1  40.62 ( 35.74)	Acc@5  89.84 ( 87.48)
Epoch: [2][180/391]	Time  0.135 ( 0.136)	Data  0.001 ( 0.005)	Loss 1.6751e+00 (1.6931e+00)	Acc@1  29.69 ( 35.85)	Acc@5  87.50 ( 87.61)
Epoch: [2][190/391]	Time  0.131 ( 0.136)	Data  0.001 ( 0.005)	Loss 1.7675e+00 (1.6896e+00)	Acc@1  32.81 ( 35.95)	Acc@5  82.03 ( 87.73)
Epoch: [2][200/391]	Time  0.135 ( 0.136)	Data  0.001 ( 0.005)	Loss 1.7899e+00 (1.6887e+00)	Acc@1  35.16 ( 36.07)	Acc@5  89.06 ( 87.75)
Epoch: [2][210/391]	Time  0.130 ( 0.136)	Data  0.001 ( 0.005)	Loss 1.6474e+00 (1.6864e+00)	Acc@1  34.38 ( 36.13)	Acc@5  86.72 ( 87.80)
Epoch: [2][220/391]	Time  0.138 ( 0.136)	Data  0.001 ( 0.005)	Loss 1.5674e+00 (1.6843e+00)	Acc@1  42.97 ( 36.26)	Acc@5  89.06 ( 87.84)
Epoch: [2][230/391]	Time  0.133 ( 0.136)	Data  0.001 ( 0.005)	Loss 1.8970e+00 (1.6838e+00)	Acc@1  28.91 ( 36.25)	Acc@5  80.47 ( 87.88)
Epoch: [2][240/391]	Time  0.136 ( 0.136)	Data  0.001 ( 0.005)	Loss 1.4882e+00 (1.6818e+00)	Acc@1  42.97 ( 36.34)	Acc@5  92.97 ( 87.88)
Epoch: [2][250/391]	Time  0.130 ( 0.136)	Data  0.001 ( 0.005)	Loss 1.7404e+00 (1.6792e+00)	Acc@1  34.38 ( 36.44)	Acc@5  86.72 ( 87.98)
Epoch: [2][260/391]	Time  0.135 ( 0.136)	Data  0.001 ( 0.005)	Loss 1.6438e+00 (1.6767e+00)	Acc@1  35.16 ( 36.58)	Acc@5  89.06 ( 88.00)
Epoch: [2][270/391]	Time  0.136 ( 0.136)	Data  0.001 ( 0.005)	Loss 1.6648e+00 (1.6738e+00)	Acc@1  40.62 ( 36.70)	Acc@5  87.50 ( 88.03)
Epoch: [2][280/391]	Time  0.135 ( 0.136)	Data  0.001 ( 0.005)	Loss 1.6417e+00 (1.6717e+00)	Acc@1  32.81 ( 36.82)	Acc@5  89.84 ( 88.10)
Epoch: [2][290/391]	Time  0.136 ( 0.136)	Data  0.001 ( 0.005)	Loss 1.4837e+00 (1.6702e+00)	Acc@1  37.50 ( 36.85)	Acc@5  96.09 ( 88.14)
Epoch: [2][300/391]	Time  0.134 ( 0.136)	Data  0.001 ( 0.005)	Loss 1.5839e+00 (1.6683e+00)	Acc@1  37.50 ( 36.94)	Acc@5  90.62 ( 88.19)
Epoch: [2][310/391]	Time  0.135 ( 0.136)	Data  0.001 ( 0.005)	Loss 1.6870e+00 (1.6661e+00)	Acc@1  37.50 ( 37.03)	Acc@5  89.84 ( 88.27)
Epoch: [2][320/391]	Time  0.132 ( 0.136)	Data  0.001 ( 0.005)	Loss 1.5506e+00 (1.6623e+00)	Acc@1  39.84 ( 37.19)	Acc@5  92.19 ( 88.34)
Epoch: [2][330/391]	Time  0.135 ( 0.136)	Data  0.001 ( 0.005)	Loss 1.5599e+00 (1.6574e+00)	Acc@1  42.97 ( 37.37)	Acc@5  89.84 ( 88.46)
Epoch: [2][340/391]	Time  0.136 ( 0.136)	Data  0.001 ( 0.005)	Loss 1.5648e+00 (1.6538e+00)	Acc@1  41.41 ( 37.56)	Acc@5  90.62 ( 88.52)
Epoch: [2][350/391]	Time  0.138 ( 0.136)	Data  0.001 ( 0.005)	Loss 1.4793e+00 (1.6515e+00)	Acc@1  46.09 ( 37.64)	Acc@5  91.41 ( 88.55)
Epoch: [2][360/391]	Time  0.136 ( 0.136)	Data  0.001 ( 0.005)	Loss 1.6841e+00 (1.6494e+00)	Acc@1  35.94 ( 37.70)	Acc@5  89.06 ( 88.61)
Epoch: [2][370/391]	Time  0.139 ( 0.136)	Data  0.001 ( 0.005)	Loss 1.3845e+00 (1.6469e+00)	Acc@1  45.31 ( 37.82)	Acc@5  94.53 ( 88.66)
Epoch: [2][380/391]	Time  0.138 ( 0.136)	Data  0.001 ( 0.005)	Loss 1.5236e+00 (1.6437e+00)	Acc@1  45.31 ( 37.94)	Acc@5  89.84 ( 88.74)
Epoch: [2][390/391]	Time  0.127 ( 0.136)	Data  0.001 ( 0.005)	Loss 1.7054e+00 (1.6423e+00)	Acc@1  35.00 ( 38.00)	Acc@5  91.25 ( 88.77)
## e[2] optimizer.zero_grad (sum) time: 0.2891695499420166
## e[2]       loss.backward (sum) time: 18.445873022079468
## e[2]      optimizer.step (sum) time: 2.9206323623657227
## epoch[2] training(only) time: 53.410072565078735
# Switched to evaluate mode...
Test: [  0/100]	Time  0.208 ( 0.208)	Loss 1.4506e+00 (1.4506e+00)	Acc@1  44.00 ( 44.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.044 ( 0.059)	Loss 1.4293e+00 (1.5626e+00)	Acc@1  45.00 ( 41.36)	Acc@5  90.00 ( 90.45)
Test: [ 20/100]	Time  0.041 ( 0.051)	Loss 1.3756e+00 (1.5461e+00)	Acc@1  50.00 ( 40.95)	Acc@5  90.00 ( 91.00)
Test: [ 30/100]	Time  0.042 ( 0.049)	Loss 1.3442e+00 (1.5620e+00)	Acc@1  49.00 ( 40.58)	Acc@5  95.00 ( 90.68)
Test: [ 40/100]	Time  0.044 ( 0.048)	Loss 1.6922e+00 (1.5702e+00)	Acc@1  41.00 ( 40.95)	Acc@5  91.00 ( 90.71)
Test: [ 50/100]	Time  0.043 ( 0.047)	Loss 1.4659e+00 (1.5519e+00)	Acc@1  44.00 ( 41.69)	Acc@5  94.00 ( 91.16)
Test: [ 60/100]	Time  0.044 ( 0.046)	Loss 1.5293e+00 (1.5515e+00)	Acc@1  42.00 ( 42.05)	Acc@5  90.00 ( 91.18)
Test: [ 70/100]	Time  0.043 ( 0.046)	Loss 1.7578e+00 (1.5578e+00)	Acc@1  39.00 ( 41.89)	Acc@5  91.00 ( 91.08)
Test: [ 80/100]	Time  0.044 ( 0.045)	Loss 1.3611e+00 (1.5509e+00)	Acc@1  52.00 ( 42.37)	Acc@5  94.00 ( 91.27)
Test: [ 90/100]	Time  0.044 ( 0.045)	Loss 1.7938e+00 (1.5663e+00)	Acc@1  38.00 ( 41.93)	Acc@5  95.00 ( 91.12)
 * Acc@1 42.040 Acc@5 91.130
### epoch[2] execution time: 58.026976108551025
EPOCH 3
# current learning rate: 0.1
# Switched to train mode...
Epoch: [3][  0/391]	Time  0.289 ( 0.289)	Data  0.174 ( 0.174)	Loss 1.6220e+00 (1.6220e+00)	Acc@1  41.41 ( 41.41)	Acc@5  88.28 ( 88.28)
Epoch: [3][ 10/391]	Time  0.130 ( 0.148)	Data  0.001 ( 0.019)	Loss 1.4496e+00 (1.5661e+00)	Acc@1  41.41 ( 41.34)	Acc@5  94.53 ( 90.77)
Epoch: [3][ 20/391]	Time  0.137 ( 0.142)	Data  0.001 ( 0.012)	Loss 1.2772e+00 (1.5393e+00)	Acc@1  58.59 ( 42.15)	Acc@5  93.75 ( 91.18)
Epoch: [3][ 30/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.010)	Loss 1.5587e+00 (1.5290e+00)	Acc@1  37.50 ( 42.09)	Acc@5  92.19 ( 91.10)
Epoch: [3][ 40/391]	Time  0.137 ( 0.139)	Data  0.002 ( 0.008)	Loss 1.6861e+00 (1.5296e+00)	Acc@1  36.72 ( 42.28)	Acc@5  92.19 ( 91.06)
Epoch: [3][ 50/391]	Time  0.132 ( 0.139)	Data  0.001 ( 0.008)	Loss 1.3529e+00 (1.5255e+00)	Acc@1  50.00 ( 42.59)	Acc@5  94.53 ( 91.25)
Epoch: [3][ 60/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.007)	Loss 1.3493e+00 (1.5208e+00)	Acc@1  47.66 ( 42.70)	Acc@5  96.09 ( 91.34)
Epoch: [3][ 70/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.007)	Loss 1.4881e+00 (1.5234e+00)	Acc@1  42.97 ( 42.45)	Acc@5  92.19 ( 91.19)
Epoch: [3][ 80/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.4887e+00 (1.5165e+00)	Acc@1  43.75 ( 42.78)	Acc@5  91.41 ( 91.29)
Epoch: [3][ 90/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.4037e+00 (1.5163e+00)	Acc@1  44.53 ( 42.89)	Acc@5  94.53 ( 91.42)
Epoch: [3][100/391]	Time  0.129 ( 0.137)	Data  0.001 ( 0.006)	Loss 1.4643e+00 (1.5173e+00)	Acc@1  46.09 ( 43.02)	Acc@5  92.97 ( 91.40)
Epoch: [3][110/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.006)	Loss 1.4346e+00 (1.5136e+00)	Acc@1  43.75 ( 43.35)	Acc@5  92.97 ( 91.37)
Epoch: [3][120/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.006)	Loss 1.5192e+00 (1.5159e+00)	Acc@1  46.09 ( 43.43)	Acc@5  90.62 ( 91.29)
Epoch: [3][130/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.006)	Loss 1.4991e+00 (1.5108e+00)	Acc@1  48.44 ( 43.73)	Acc@5  90.62 ( 91.29)
Epoch: [3][140/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.006)	Loss 1.4881e+00 (1.5056e+00)	Acc@1  39.84 ( 43.96)	Acc@5  92.19 ( 91.32)
Epoch: [3][150/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.006)	Loss 1.5839e+00 (1.5006e+00)	Acc@1  42.97 ( 44.18)	Acc@5  92.97 ( 91.40)
Epoch: [3][160/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.6897e+00 (1.4995e+00)	Acc@1  43.75 ( 44.26)	Acc@5  87.50 ( 91.44)
Epoch: [3][170/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.6421e+00 (1.4964e+00)	Acc@1  43.75 ( 44.36)	Acc@5  88.28 ( 91.50)
Epoch: [3][180/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.4358e+00 (1.4926e+00)	Acc@1  47.66 ( 44.56)	Acc@5  93.75 ( 91.51)
Epoch: [3][190/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3678e+00 (1.4925e+00)	Acc@1  50.78 ( 44.61)	Acc@5  92.19 ( 91.41)
Epoch: [3][200/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.5680e+00 (1.4884e+00)	Acc@1  46.09 ( 44.78)	Acc@5  88.28 ( 91.46)
Epoch: [3][210/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.4467e+00 (1.4840e+00)	Acc@1  47.66 ( 45.01)	Acc@5  92.97 ( 91.52)
Epoch: [3][220/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1670e+00 (1.4801e+00)	Acc@1  60.94 ( 45.16)	Acc@5  94.53 ( 91.59)
Epoch: [3][230/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.4390e+00 (1.4776e+00)	Acc@1  43.75 ( 45.17)	Acc@5  91.41 ( 91.63)
Epoch: [3][240/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3638e+00 (1.4742e+00)	Acc@1  43.75 ( 45.33)	Acc@5  95.31 ( 91.65)
Epoch: [3][250/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.6000e+00 (1.4717e+00)	Acc@1  45.31 ( 45.46)	Acc@5  91.41 ( 91.70)
Epoch: [3][260/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3109e+00 (1.4688e+00)	Acc@1  46.09 ( 45.53)	Acc@5  96.09 ( 91.77)
Epoch: [3][270/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3405e+00 (1.4663e+00)	Acc@1  50.00 ( 45.60)	Acc@5  91.41 ( 91.80)
Epoch: [3][280/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.4839e+00 (1.4649e+00)	Acc@1  44.53 ( 45.65)	Acc@5  89.84 ( 91.80)
Epoch: [3][290/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.4552e+00 (1.4634e+00)	Acc@1  52.34 ( 45.75)	Acc@5  87.50 ( 91.83)
Epoch: [3][300/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.2458e+00 (1.4596e+00)	Acc@1  51.56 ( 45.90)	Acc@5  92.97 ( 91.87)
Epoch: [3][310/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.4934e+00 (1.4567e+00)	Acc@1  40.62 ( 46.01)	Acc@5  91.41 ( 91.92)
Epoch: [3][320/391]	Time  0.132 ( 0.137)	Data  0.002 ( 0.005)	Loss 1.2341e+00 (1.4538e+00)	Acc@1  53.91 ( 46.13)	Acc@5  95.31 ( 91.93)
Epoch: [3][330/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3421e+00 (1.4523e+00)	Acc@1  50.00 ( 46.22)	Acc@5  94.53 ( 91.98)
Epoch: [3][340/391]	Time  0.141 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.2699e+00 (1.4504e+00)	Acc@1  52.34 ( 46.28)	Acc@5  96.09 ( 92.01)
Epoch: [3][350/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3858e+00 (1.4491e+00)	Acc@1  50.00 ( 46.37)	Acc@5  93.75 ( 92.07)
Epoch: [3][360/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.4458e+00 (1.4473e+00)	Acc@1  42.97 ( 46.44)	Acc@5  92.97 ( 92.10)
Epoch: [3][370/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3121e+00 (1.4434e+00)	Acc@1  53.12 ( 46.63)	Acc@5  91.41 ( 92.12)
Epoch: [3][380/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3088e+00 (1.4412e+00)	Acc@1  53.12 ( 46.75)	Acc@5  92.19 ( 92.14)
Epoch: [3][390/391]	Time  0.126 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3801e+00 (1.4388e+00)	Acc@1  45.00 ( 46.83)	Acc@5  92.50 ( 92.18)
## e[3] optimizer.zero_grad (sum) time: 0.286116361618042
## e[3]       loss.backward (sum) time: 18.496700763702393
## e[3]      optimizer.step (sum) time: 2.8969948291778564
## epoch[3] training(only) time: 53.60383105278015
# Switched to evaluate mode...
Test: [  0/100]	Time  0.199 ( 0.199)	Loss 1.5386e+00 (1.5386e+00)	Acc@1  42.00 ( 42.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.043 ( 0.058)	Loss 1.3592e+00 (1.4499e+00)	Acc@1  52.00 ( 46.64)	Acc@5  93.00 ( 93.36)
Test: [ 20/100]	Time  0.042 ( 0.051)	Loss 1.4141e+00 (1.4424e+00)	Acc@1  49.00 ( 46.57)	Acc@5  94.00 ( 93.38)
Test: [ 30/100]	Time  0.041 ( 0.048)	Loss 1.3168e+00 (1.4525e+00)	Acc@1  47.00 ( 46.32)	Acc@5  94.00 ( 93.13)
Test: [ 40/100]	Time  0.044 ( 0.048)	Loss 1.6326e+00 (1.4606e+00)	Acc@1  52.00 ( 46.41)	Acc@5  93.00 ( 92.98)
Test: [ 50/100]	Time  0.044 ( 0.047)	Loss 1.5493e+00 (1.4547e+00)	Acc@1  41.00 ( 46.33)	Acc@5  90.00 ( 93.10)
Test: [ 60/100]	Time  0.043 ( 0.046)	Loss 1.4949e+00 (1.4582e+00)	Acc@1  41.00 ( 46.28)	Acc@5  95.00 ( 93.21)
Test: [ 70/100]	Time  0.042 ( 0.046)	Loss 1.3744e+00 (1.4630e+00)	Acc@1  50.00 ( 45.94)	Acc@5  93.00 ( 93.30)
Test: [ 80/100]	Time  0.045 ( 0.046)	Loss 1.3432e+00 (1.4612e+00)	Acc@1  46.00 ( 46.20)	Acc@5  93.00 ( 93.32)
Test: [ 90/100]	Time  0.044 ( 0.046)	Loss 1.3935e+00 (1.4660e+00)	Acc@1  43.00 ( 45.98)	Acc@5  95.00 ( 93.18)
 * Acc@1 46.070 Acc@5 93.170
### epoch[3] execution time: 58.24967622756958
EPOCH 4
# current learning rate: 0.1
# Switched to train mode...
Epoch: [4][  0/391]	Time  0.299 ( 0.299)	Data  0.164 ( 0.164)	Loss 1.4676e+00 (1.4676e+00)	Acc@1  50.78 ( 50.78)	Acc@5  89.84 ( 89.84)
Epoch: [4][ 10/391]	Time  0.136 ( 0.151)	Data  0.001 ( 0.018)	Loss 1.3199e+00 (1.3979e+00)	Acc@1  54.69 ( 48.65)	Acc@5  92.97 ( 92.12)
Epoch: [4][ 20/391]	Time  0.128 ( 0.143)	Data  0.001 ( 0.012)	Loss 1.3741e+00 (1.3612e+00)	Acc@1  52.34 ( 50.04)	Acc@5  92.19 ( 92.45)
Epoch: [4][ 30/391]	Time  0.136 ( 0.141)	Data  0.001 ( 0.009)	Loss 1.3554e+00 (1.3385e+00)	Acc@1  48.44 ( 50.58)	Acc@5  92.97 ( 92.99)
Epoch: [4][ 40/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.008)	Loss 1.1918e+00 (1.3420e+00)	Acc@1  55.47 ( 50.00)	Acc@5  95.31 ( 93.10)
Epoch: [4][ 50/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.007)	Loss 1.2999e+00 (1.3318e+00)	Acc@1  54.69 ( 50.55)	Acc@5  96.09 ( 93.41)
Epoch: [4][ 60/391]	Time  0.132 ( 0.139)	Data  0.001 ( 0.007)	Loss 1.3907e+00 (1.3279e+00)	Acc@1  46.88 ( 50.87)	Acc@5  92.97 ( 93.42)
Epoch: [4][ 70/391]	Time  0.131 ( 0.138)	Data  0.001 ( 0.007)	Loss 1.2750e+00 (1.3255e+00)	Acc@1  57.03 ( 51.01)	Acc@5  94.53 ( 93.51)
Epoch: [4][ 80/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.4597e+00 (1.3269e+00)	Acc@1  41.41 ( 50.98)	Acc@5  92.19 ( 93.48)
Epoch: [4][ 90/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.2930e+00 (1.3245e+00)	Acc@1  52.34 ( 51.12)	Acc@5  93.75 ( 93.49)
Epoch: [4][100/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.2608e+00 (1.3271e+00)	Acc@1  56.25 ( 50.96)	Acc@5  95.31 ( 93.49)
Epoch: [4][110/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.006)	Loss 1.2535e+00 (1.3298e+00)	Acc@1  51.56 ( 50.91)	Acc@5  96.09 ( 93.47)
Epoch: [4][120/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.006)	Loss 1.3732e+00 (1.3266e+00)	Acc@1  50.00 ( 51.01)	Acc@5  92.19 ( 93.54)
Epoch: [4][130/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.006)	Loss 1.2814e+00 (1.3232e+00)	Acc@1  53.91 ( 51.04)	Acc@5  89.84 ( 93.66)
Epoch: [4][140/391]	Time  0.141 ( 0.137)	Data  0.001 ( 0.006)	Loss 1.3398e+00 (1.3191e+00)	Acc@1  47.66 ( 51.27)	Acc@5  94.53 ( 93.68)
Epoch: [4][150/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3049e+00 (1.3200e+00)	Acc@1  56.25 ( 51.38)	Acc@5  93.75 ( 93.65)
Epoch: [4][160/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3530e+00 (1.3168e+00)	Acc@1  50.00 ( 51.44)	Acc@5  96.09 ( 93.74)
Epoch: [4][170/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.2257e+00 (1.3154e+00)	Acc@1  53.12 ( 51.59)	Acc@5  93.75 ( 93.77)
Epoch: [4][180/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1402e+00 (1.3112e+00)	Acc@1  54.69 ( 51.78)	Acc@5  95.31 ( 93.80)
Epoch: [4][190/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.2817e+00 (1.3131e+00)	Acc@1  47.66 ( 51.70)	Acc@5  92.19 ( 93.70)
Epoch: [4][200/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3441e+00 (1.3120e+00)	Acc@1  51.56 ( 51.82)	Acc@5  90.62 ( 93.70)
Epoch: [4][210/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1124e+00 (1.3059e+00)	Acc@1  59.38 ( 52.09)	Acc@5  94.53 ( 93.76)
Epoch: [4][220/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3768e+00 (1.3012e+00)	Acc@1  47.66 ( 52.31)	Acc@5  91.41 ( 93.80)
Epoch: [4][230/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.2830e+00 (1.2980e+00)	Acc@1  50.78 ( 52.44)	Acc@5  93.75 ( 93.82)
Epoch: [4][240/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0780e+00 (1.2964e+00)	Acc@1  65.62 ( 52.49)	Acc@5  95.31 ( 93.85)
Epoch: [4][250/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1197e+00 (1.2943e+00)	Acc@1  57.03 ( 52.58)	Acc@5  96.09 ( 93.87)
Epoch: [4][260/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1849e+00 (1.2909e+00)	Acc@1  52.34 ( 52.71)	Acc@5  93.75 ( 93.91)
Epoch: [4][270/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1757e+00 (1.2880e+00)	Acc@1  55.47 ( 52.85)	Acc@5  99.22 ( 93.98)
Epoch: [4][280/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.2940e+00 (1.2851e+00)	Acc@1  50.00 ( 52.93)	Acc@5  93.75 ( 94.06)
Epoch: [4][290/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.2648e+00 (1.2852e+00)	Acc@1  51.56 ( 52.93)	Acc@5  95.31 ( 94.07)
Epoch: [4][300/391]	Time  0.134 ( 0.137)	Data  0.002 ( 0.005)	Loss 1.1392e+00 (1.2816e+00)	Acc@1  60.16 ( 53.07)	Acc@5  92.97 ( 94.10)
Epoch: [4][310/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.4381e+00 (1.2819e+00)	Acc@1  52.34 ( 53.09)	Acc@5  92.97 ( 94.10)
Epoch: [4][320/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1451e+00 (1.2797e+00)	Acc@1  57.81 ( 53.18)	Acc@5  94.53 ( 94.10)
Epoch: [4][330/391]	Time  0.128 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.2535e+00 (1.2761e+00)	Acc@1  53.91 ( 53.34)	Acc@5  92.97 ( 94.14)
Epoch: [4][340/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.2530e+00 (1.2721e+00)	Acc@1  63.28 ( 53.54)	Acc@5  92.97 ( 94.16)
Epoch: [4][350/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0580e+00 (1.2703e+00)	Acc@1  64.06 ( 53.64)	Acc@5  93.75 ( 94.18)
Epoch: [4][360/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.2898e+00 (1.2708e+00)	Acc@1  50.78 ( 53.61)	Acc@5  94.53 ( 94.19)
Epoch: [4][370/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.2088e+00 (1.2707e+00)	Acc@1  56.25 ( 53.65)	Acc@5  92.97 ( 94.18)
Epoch: [4][380/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.2365e+00 (1.2699e+00)	Acc@1  54.69 ( 53.71)	Acc@5  92.97 ( 94.17)
Epoch: [4][390/391]	Time  0.119 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3063e+00 (1.2689e+00)	Acc@1  66.25 ( 53.74)	Acc@5  91.25 ( 94.17)
## e[4] optimizer.zero_grad (sum) time: 0.28504037857055664
## e[4]       loss.backward (sum) time: 18.549591064453125
## e[4]      optimizer.step (sum) time: 2.9587957859039307
## epoch[4] training(only) time: 53.71771264076233
# Switched to evaluate mode...
Test: [  0/100]	Time  0.206 ( 0.206)	Loss 1.2040e+00 (1.2040e+00)	Acc@1  55.00 ( 55.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.044 ( 0.059)	Loss 1.0461e+00 (1.1833e+00)	Acc@1  59.00 ( 56.91)	Acc@5  96.00 ( 96.00)
Test: [ 20/100]	Time  0.042 ( 0.051)	Loss 1.1109e+00 (1.1854e+00)	Acc@1  64.00 ( 56.90)	Acc@5  95.00 ( 95.95)
Test: [ 30/100]	Time  0.044 ( 0.049)	Loss 1.0076e+00 (1.2002e+00)	Acc@1  65.00 ( 56.23)	Acc@5  96.00 ( 95.71)
Test: [ 40/100]	Time  0.042 ( 0.047)	Loss 1.4475e+00 (1.2159e+00)	Acc@1  55.00 ( 55.85)	Acc@5  96.00 ( 95.54)
Test: [ 50/100]	Time  0.041 ( 0.046)	Loss 1.2658e+00 (1.2016e+00)	Acc@1  61.00 ( 56.73)	Acc@5  95.00 ( 95.71)
Test: [ 60/100]	Time  0.043 ( 0.046)	Loss 1.2401e+00 (1.2051e+00)	Acc@1  57.00 ( 56.54)	Acc@5  97.00 ( 95.64)
Test: [ 70/100]	Time  0.042 ( 0.046)	Loss 1.2780e+00 (1.2125e+00)	Acc@1  55.00 ( 56.25)	Acc@5  96.00 ( 95.76)
Test: [ 80/100]	Time  0.044 ( 0.045)	Loss 1.1805e+00 (1.2076e+00)	Acc@1  54.00 ( 56.16)	Acc@5  95.00 ( 95.85)
Test: [ 90/100]	Time  0.044 ( 0.045)	Loss 1.1292e+00 (1.2051e+00)	Acc@1  58.00 ( 56.25)	Acc@5  99.00 ( 95.85)
 * Acc@1 56.150 Acc@5 95.750
### epoch[4] execution time: 58.3385968208313
EPOCH 5
# current learning rate: 0.1
# Switched to train mode...
Epoch: [5][  0/391]	Time  0.294 ( 0.294)	Data  0.172 ( 0.172)	Loss 1.1034e+00 (1.1034e+00)	Acc@1  57.03 ( 57.03)	Acc@5  96.09 ( 96.09)
Epoch: [5][ 10/391]	Time  0.129 ( 0.149)	Data  0.001 ( 0.019)	Loss 1.0460e+00 (1.1645e+00)	Acc@1  59.38 ( 58.17)	Acc@5  95.31 ( 95.53)
Epoch: [5][ 20/391]	Time  0.130 ( 0.143)	Data  0.001 ( 0.012)	Loss 1.0386e+00 (1.1502e+00)	Acc@1  57.81 ( 58.30)	Acc@5  97.66 ( 95.76)
Epoch: [5][ 30/391]	Time  0.136 ( 0.141)	Data  0.001 ( 0.010)	Loss 1.2580e+00 (1.1456e+00)	Acc@1  57.03 ( 58.47)	Acc@5  93.75 ( 95.87)
Epoch: [5][ 40/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.008)	Loss 1.2027e+00 (1.1586e+00)	Acc@1  57.81 ( 57.87)	Acc@5  96.88 ( 95.64)
Epoch: [5][ 50/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.008)	Loss 1.2383e+00 (1.1732e+00)	Acc@1  53.91 ( 57.29)	Acc@5  94.53 ( 95.28)
Epoch: [5][ 60/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.007)	Loss 1.1407e+00 (1.1684e+00)	Acc@1  57.81 ( 57.44)	Acc@5  95.31 ( 95.22)
Epoch: [5][ 70/391]	Time  0.136 ( 0.138)	Data  0.002 ( 0.007)	Loss 1.1445e+00 (1.1677e+00)	Acc@1  64.06 ( 57.43)	Acc@5  96.09 ( 95.21)
Epoch: [5][ 80/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.007)	Loss 1.2389e+00 (1.1671e+00)	Acc@1  56.25 ( 57.41)	Acc@5  91.41 ( 95.15)
Epoch: [5][ 90/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.3565e+00 (1.1736e+00)	Acc@1  52.34 ( 57.22)	Acc@5  92.97 ( 95.13)
Epoch: [5][100/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.0978e+00 (1.1705e+00)	Acc@1  57.81 ( 57.30)	Acc@5  96.88 ( 95.22)
Epoch: [5][110/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.2647e+00 (1.1699e+00)	Acc@1  57.81 ( 57.42)	Acc@5  95.31 ( 95.22)
Epoch: [5][120/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.0834e+00 (1.1670e+00)	Acc@1  59.38 ( 57.46)	Acc@5  97.66 ( 95.29)
Epoch: [5][130/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.1287e+00 (1.1640e+00)	Acc@1  58.59 ( 57.62)	Acc@5  98.44 ( 95.37)
Epoch: [5][140/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.0252e+00 (1.1625e+00)	Acc@1  62.50 ( 57.71)	Acc@5  96.09 ( 95.32)
Epoch: [5][150/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.0019e+00 (1.1628e+00)	Acc@1  62.50 ( 57.70)	Acc@5  96.09 ( 95.25)
Epoch: [5][160/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.006)	Loss 1.2318e+00 (1.1587e+00)	Acc@1  59.38 ( 57.92)	Acc@5  93.75 ( 95.27)
Epoch: [5][170/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1261e+00 (1.1581e+00)	Acc@1  57.81 ( 57.96)	Acc@5  96.09 ( 95.29)
Epoch: [5][180/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.2404e+00 (1.1604e+00)	Acc@1  50.78 ( 57.89)	Acc@5  95.31 ( 95.28)
Epoch: [5][190/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1044e+00 (1.1609e+00)	Acc@1  58.59 ( 57.86)	Acc@5  96.09 ( 95.26)
Epoch: [5][200/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1326e+00 (1.1609e+00)	Acc@1  60.94 ( 57.87)	Acc@5  94.53 ( 95.25)
Epoch: [5][210/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.9072e-01 (1.1568e+00)	Acc@1  61.72 ( 57.96)	Acc@5  96.09 ( 95.28)
Epoch: [5][220/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0216e+00 (1.1545e+00)	Acc@1  64.06 ( 58.03)	Acc@5  99.22 ( 95.30)
Epoch: [5][230/391]	Time  0.130 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1143e+00 (1.1526e+00)	Acc@1  59.38 ( 58.14)	Acc@5  96.88 ( 95.32)
Epoch: [5][240/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0659e+00 (1.1509e+00)	Acc@1  62.50 ( 58.20)	Acc@5  94.53 ( 95.34)
Epoch: [5][250/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.5612e-01 (1.1498e+00)	Acc@1  68.75 ( 58.24)	Acc@5  97.66 ( 95.36)
Epoch: [5][260/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0370e+00 (1.1470e+00)	Acc@1  60.16 ( 58.35)	Acc@5  96.09 ( 95.41)
Epoch: [5][270/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1360e+00 (1.1457e+00)	Acc@1  58.59 ( 58.40)	Acc@5  96.09 ( 95.44)
Epoch: [5][280/391]	Time  0.134 ( 0.137)	Data  0.004 ( 0.005)	Loss 1.0447e+00 (1.1427e+00)	Acc@1  64.06 ( 58.54)	Acc@5  95.31 ( 95.46)
Epoch: [5][290/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.8164e-01 (1.1410e+00)	Acc@1  66.41 ( 58.62)	Acc@5  96.88 ( 95.48)
Epoch: [5][300/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1342e+00 (1.1396e+00)	Acc@1  57.81 ( 58.65)	Acc@5  92.97 ( 95.48)
Epoch: [5][310/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0841e+00 (1.1382e+00)	Acc@1  64.06 ( 58.74)	Acc@5  95.31 ( 95.48)
Epoch: [5][320/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.2488e-01 (1.1361e+00)	Acc@1  67.97 ( 58.86)	Acc@5  97.66 ( 95.50)
Epoch: [5][330/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0831e+00 (1.1341e+00)	Acc@1  57.81 ( 58.92)	Acc@5  96.88 ( 95.53)
Epoch: [5][340/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1961e+00 (1.1330e+00)	Acc@1  57.03 ( 58.94)	Acc@5  94.53 ( 95.55)
Epoch: [5][350/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1442e+00 (1.1315e+00)	Acc@1  57.81 ( 59.00)	Acc@5  92.97 ( 95.57)
Epoch: [5][360/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0092e+00 (1.1290e+00)	Acc@1  62.50 ( 59.13)	Acc@5  97.66 ( 95.59)
Epoch: [5][370/391]	Time  0.137 ( 0.137)	Data  0.002 ( 0.005)	Loss 1.0595e+00 (1.1255e+00)	Acc@1  67.19 ( 59.25)	Acc@5  96.09 ( 95.63)
Epoch: [5][380/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.5146e-01 (1.1236e+00)	Acc@1  60.94 ( 59.33)	Acc@5  96.09 ( 95.63)
Epoch: [5][390/391]	Time  0.126 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0556e+00 (1.1220e+00)	Acc@1  58.75 ( 59.39)	Acc@5  97.50 ( 95.66)
## e[5] optimizer.zero_grad (sum) time: 0.2877840995788574
## e[5]       loss.backward (sum) time: 18.5357186794281
## e[5]      optimizer.step (sum) time: 2.873551368713379
## epoch[5] training(only) time: 53.68763327598572
# Switched to evaluate mode...
Test: [  0/100]	Time  0.208 ( 0.208)	Loss 1.0946e+00 (1.0946e+00)	Acc@1  65.00 ( 65.00)	Acc@5  97.00 ( 97.00)
Test: [ 10/100]	Time  0.045 ( 0.059)	Loss 1.0142e+00 (1.0856e+00)	Acc@1  59.00 ( 61.64)	Acc@5  97.00 ( 95.27)
Test: [ 20/100]	Time  0.042 ( 0.052)	Loss 1.1326e+00 (1.0894e+00)	Acc@1  63.00 ( 61.00)	Acc@5  95.00 ( 95.90)
Test: [ 30/100]	Time  0.043 ( 0.049)	Loss 9.5519e-01 (1.1092e+00)	Acc@1  67.00 ( 60.10)	Acc@5  96.00 ( 95.68)
Test: [ 40/100]	Time  0.042 ( 0.047)	Loss 1.2428e+00 (1.1141e+00)	Acc@1  57.00 ( 59.76)	Acc@5  92.00 ( 95.39)
Test: [ 50/100]	Time  0.042 ( 0.046)	Loss 1.0308e+00 (1.0952e+00)	Acc@1  64.00 ( 60.33)	Acc@5  98.00 ( 95.82)
Test: [ 60/100]	Time  0.044 ( 0.046)	Loss 1.0725e+00 (1.0974e+00)	Acc@1  62.00 ( 60.11)	Acc@5  97.00 ( 95.79)
Test: [ 70/100]	Time  0.046 ( 0.046)	Loss 9.5822e-01 (1.1034e+00)	Acc@1  64.00 ( 59.99)	Acc@5  95.00 ( 95.79)
Test: [ 80/100]	Time  0.041 ( 0.045)	Loss 1.0902e+00 (1.1057e+00)	Acc@1  60.00 ( 59.95)	Acc@5  96.00 ( 95.75)
Test: [ 90/100]	Time  0.042 ( 0.045)	Loss 9.9161e-01 (1.1137e+00)	Acc@1  63.00 ( 59.62)	Acc@5  99.00 ( 95.67)
 * Acc@1 59.700 Acc@5 95.570
### epoch[5] execution time: 58.30182147026062
EPOCH 6
# current learning rate: 0.1
# Switched to train mode...
Epoch: [6][  0/391]	Time  0.292 ( 0.292)	Data  0.177 ( 0.177)	Loss 9.3122e-01 (9.3122e-01)	Acc@1  64.84 ( 64.84)	Acc@5  98.44 ( 98.44)
Epoch: [6][ 10/391]	Time  0.135 ( 0.150)	Data  0.001 ( 0.020)	Loss 9.8565e-01 (1.0659e+00)	Acc@1  61.72 ( 60.09)	Acc@5  96.09 ( 96.31)
Epoch: [6][ 20/391]	Time  0.130 ( 0.143)	Data  0.001 ( 0.013)	Loss 9.7476e-01 (1.0565e+00)	Acc@1  60.94 ( 61.31)	Acc@5  98.44 ( 96.32)
Epoch: [6][ 30/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.010)	Loss 1.0169e+00 (1.0688e+00)	Acc@1  64.06 ( 61.24)	Acc@5  94.53 ( 96.17)
Epoch: [6][ 40/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.009)	Loss 9.3636e-01 (1.0530e+00)	Acc@1  66.41 ( 61.85)	Acc@5  96.09 ( 96.13)
Epoch: [6][ 50/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.008)	Loss 1.0332e+00 (1.0529e+00)	Acc@1  63.28 ( 62.13)	Acc@5  99.22 ( 96.11)
Epoch: [6][ 60/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.007)	Loss 1.2724e+00 (1.0578e+00)	Acc@1  55.47 ( 61.86)	Acc@5  95.31 ( 96.03)
Epoch: [6][ 70/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.007)	Loss 9.7213e-01 (1.0479e+00)	Acc@1  66.41 ( 62.24)	Acc@5  98.44 ( 96.17)
Epoch: [6][ 80/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.007)	Loss 9.6089e-01 (1.0367e+00)	Acc@1  64.06 ( 62.77)	Acc@5  98.44 ( 96.27)
Epoch: [6][ 90/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 9.2793e-01 (1.0401e+00)	Acc@1  68.75 ( 62.73)	Acc@5  96.88 ( 96.22)
Epoch: [6][100/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 9.1364e-01 (1.0364e+00)	Acc@1  66.41 ( 62.87)	Acc@5  97.66 ( 96.23)
Epoch: [6][110/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 8.2209e-01 (1.0294e+00)	Acc@1  72.66 ( 63.18)	Acc@5  98.44 ( 96.28)
Epoch: [6][120/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 9.0687e-01 (1.0313e+00)	Acc@1  62.50 ( 62.98)	Acc@5  96.88 ( 96.33)
Epoch: [6][130/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 9.5502e-01 (1.0315e+00)	Acc@1  64.06 ( 62.98)	Acc@5  98.44 ( 96.31)
Epoch: [6][140/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.0030e+00 (1.0331e+00)	Acc@1  64.06 ( 62.94)	Acc@5  94.53 ( 96.32)
Epoch: [6][150/391]	Time  0.129 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.0453e+00 (1.0319e+00)	Acc@1  64.06 ( 62.97)	Acc@5  96.88 ( 96.35)
Epoch: [6][160/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.0344e+00 (1.0316e+00)	Acc@1  62.50 ( 63.00)	Acc@5  97.66 ( 96.34)
Epoch: [6][170/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.0387e+00 (1.0322e+00)	Acc@1  61.72 ( 62.95)	Acc@5  97.66 ( 96.39)
Epoch: [6][180/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.6121e-01 (1.0327e+00)	Acc@1  64.84 ( 62.97)	Acc@5  99.22 ( 96.38)
Epoch: [6][190/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1021e+00 (1.0322e+00)	Acc@1  57.81 ( 62.97)	Acc@5  96.09 ( 96.38)
Epoch: [6][200/391]	Time  0.142 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.6479e-01 (1.0298e+00)	Acc@1  66.41 ( 63.07)	Acc@5  96.88 ( 96.39)
Epoch: [6][210/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.4628e-01 (1.0255e+00)	Acc@1  65.62 ( 63.23)	Acc@5  99.22 ( 96.43)
Epoch: [6][220/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.3024e-01 (1.0223e+00)	Acc@1  67.97 ( 63.38)	Acc@5  96.88 ( 96.46)
Epoch: [6][230/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0198e+00 (1.0232e+00)	Acc@1  64.06 ( 63.37)	Acc@5  95.31 ( 96.48)
Epoch: [6][240/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.0020e-01 (1.0211e+00)	Acc@1  70.31 ( 63.48)	Acc@5  96.09 ( 96.46)
Epoch: [6][250/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.2557e+00 (1.0202e+00)	Acc@1  50.78 ( 63.56)	Acc@5  92.19 ( 96.44)
Epoch: [6][260/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0187e+00 (1.0185e+00)	Acc@1  67.19 ( 63.66)	Acc@5  98.44 ( 96.47)
Epoch: [6][270/391]	Time  0.130 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0378e+00 (1.0172e+00)	Acc@1  63.28 ( 63.71)	Acc@5  96.88 ( 96.49)
Epoch: [6][280/391]	Time  0.130 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0287e+00 (1.0142e+00)	Acc@1  61.72 ( 63.76)	Acc@5  96.88 ( 96.52)
Epoch: [6][290/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0231e+00 (1.0142e+00)	Acc@1  62.50 ( 63.74)	Acc@5  95.31 ( 96.55)
Epoch: [6][300/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.7966e-01 (1.0127e+00)	Acc@1  71.88 ( 63.82)	Acc@5  98.44 ( 96.56)
Epoch: [6][310/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0475e+00 (1.0114e+00)	Acc@1  62.50 ( 63.87)	Acc@5  97.66 ( 96.57)
Epoch: [6][320/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.7486e-01 (1.0122e+00)	Acc@1  66.41 ( 63.89)	Acc@5  96.88 ( 96.53)
Epoch: [6][330/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1693e+00 (1.0125e+00)	Acc@1  60.94 ( 63.93)	Acc@5  91.41 ( 96.52)
Epoch: [6][340/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1270e+00 (1.0122e+00)	Acc@1  58.59 ( 63.96)	Acc@5  95.31 ( 96.53)
Epoch: [6][350/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.8089e-01 (1.0104e+00)	Acc@1  67.19 ( 64.02)	Acc@5  98.44 ( 96.55)
Epoch: [6][360/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.2250e-01 (1.0095e+00)	Acc@1  65.62 ( 64.06)	Acc@5  97.66 ( 96.54)
Epoch: [6][370/391]	Time  0.128 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.7387e-01 (1.0070e+00)	Acc@1  69.53 ( 64.18)	Acc@5  98.44 ( 96.54)
Epoch: [6][380/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0124e+00 (1.0047e+00)	Acc@1  67.19 ( 64.29)	Acc@5  96.09 ( 96.55)
Epoch: [6][390/391]	Time  0.129 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.0351e-01 (1.0041e+00)	Acc@1  71.25 ( 64.29)	Acc@5  97.50 ( 96.57)
## e[6] optimizer.zero_grad (sum) time: 0.2906534671783447
## e[6]       loss.backward (sum) time: 18.523905277252197
## e[6]      optimizer.step (sum) time: 2.892284631729126
## epoch[6] training(only) time: 53.709738969802856
# Switched to evaluate mode...
Test: [  0/100]	Time  0.203 ( 0.203)	Loss 8.5690e-01 (8.5690e-01)	Acc@1  68.00 ( 68.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.044 ( 0.058)	Loss 9.1668e-01 (8.9900e-01)	Acc@1  67.00 ( 68.18)	Acc@5  96.00 ( 97.91)
Test: [ 20/100]	Time  0.042 ( 0.051)	Loss 8.4090e-01 (9.0720e-01)	Acc@1  69.00 ( 68.10)	Acc@5  97.00 ( 97.33)
Test: [ 30/100]	Time  0.046 ( 0.049)	Loss 9.0150e-01 (9.2403e-01)	Acc@1  67.00 ( 67.26)	Acc@5  99.00 ( 97.19)
Test: [ 40/100]	Time  0.046 ( 0.047)	Loss 1.1459e+00 (9.4147e-01)	Acc@1  63.00 ( 66.68)	Acc@5  96.00 ( 97.10)
Test: [ 50/100]	Time  0.043 ( 0.047)	Loss 8.6339e-01 (9.2753e-01)	Acc@1  67.00 ( 67.22)	Acc@5  97.00 ( 97.33)
Test: [ 60/100]	Time  0.045 ( 0.046)	Loss 1.0131e+00 (9.3759e-01)	Acc@1  64.00 ( 66.87)	Acc@5  98.00 ( 97.31)
Test: [ 70/100]	Time  0.042 ( 0.046)	Loss 9.5537e-01 (9.3935e-01)	Acc@1  67.00 ( 66.99)	Acc@5  97.00 ( 97.42)
Test: [ 80/100]	Time  0.045 ( 0.045)	Loss 8.4794e-01 (9.3820e-01)	Acc@1  73.00 ( 66.99)	Acc@5  98.00 ( 97.46)
Test: [ 90/100]	Time  0.043 ( 0.045)	Loss 7.5650e-01 (9.3758e-01)	Acc@1  71.00 ( 66.96)	Acc@5 100.00 ( 97.44)
 * Acc@1 66.990 Acc@5 97.420
### epoch[6] execution time: 58.32807540893555
EPOCH 7
# current learning rate: 0.1
# Switched to train mode...
Epoch: [7][  0/391]	Time  0.299 ( 0.299)	Data  0.176 ( 0.176)	Loss 9.1123e-01 (9.1123e-01)	Acc@1  67.19 ( 67.19)	Acc@5  97.66 ( 97.66)
Epoch: [7][ 10/391]	Time  0.134 ( 0.151)	Data  0.001 ( 0.020)	Loss 1.1430e+00 (9.6670e-01)	Acc@1  52.34 ( 64.84)	Acc@5  98.44 ( 96.80)
Epoch: [7][ 20/391]	Time  0.133 ( 0.144)	Data  0.001 ( 0.012)	Loss 7.6198e-01 (9.5916e-01)	Acc@1  73.44 ( 65.96)	Acc@5  97.66 ( 96.80)
Epoch: [7][ 30/391]	Time  0.136 ( 0.141)	Data  0.001 ( 0.010)	Loss 8.1697e-01 (9.4915e-01)	Acc@1  71.88 ( 66.66)	Acc@5  98.44 ( 96.80)
Epoch: [7][ 40/391]	Time  0.140 ( 0.140)	Data  0.002 ( 0.009)	Loss 9.1201e-01 (9.2599e-01)	Acc@1  66.41 ( 67.09)	Acc@5  97.66 ( 96.99)
Epoch: [7][ 50/391]	Time  0.132 ( 0.140)	Data  0.001 ( 0.008)	Loss 1.0609e+00 (9.2779e-01)	Acc@1  67.19 ( 66.96)	Acc@5  96.09 ( 97.03)
Epoch: [7][ 60/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.007)	Loss 8.5702e-01 (9.2568e-01)	Acc@1  68.75 ( 67.29)	Acc@5  95.31 ( 97.02)
Epoch: [7][ 70/391]	Time  0.129 ( 0.139)	Data  0.001 ( 0.007)	Loss 8.7455e-01 (9.3793e-01)	Acc@1  67.97 ( 66.82)	Acc@5  97.66 ( 96.92)
Epoch: [7][ 80/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.007)	Loss 8.8908e-01 (9.4047e-01)	Acc@1  67.97 ( 66.82)	Acc@5  97.66 ( 96.97)
Epoch: [7][ 90/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 9.9441e-01 (9.4335e-01)	Acc@1  59.38 ( 66.53)	Acc@5  99.22 ( 97.04)
Epoch: [7][100/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 9.1851e-01 (9.4513e-01)	Acc@1  62.50 ( 66.40)	Acc@5  98.44 ( 97.09)
Epoch: [7][110/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 8.9670e-01 (9.4268e-01)	Acc@1  67.19 ( 66.34)	Acc@5  97.66 ( 97.07)
Epoch: [7][120/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 8.6544e-01 (9.3828e-01)	Acc@1  75.00 ( 66.52)	Acc@5  95.31 ( 97.06)
Epoch: [7][130/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 6.6221e-01 (9.3854e-01)	Acc@1  79.69 ( 66.61)	Acc@5  98.44 ( 97.02)
Epoch: [7][140/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.006)	Loss 7.9745e-01 (9.3873e-01)	Acc@1  75.00 ( 66.63)	Acc@5  96.88 ( 96.97)
Epoch: [7][150/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.006)	Loss 8.5111e-01 (9.3581e-01)	Acc@1  74.22 ( 66.79)	Acc@5  96.88 ( 97.01)
Epoch: [7][160/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.8704e-01 (9.3498e-01)	Acc@1  62.50 ( 66.72)	Acc@5  98.44 ( 97.01)
Epoch: [7][170/391]	Time  0.134 ( 0.137)	Data  0.002 ( 0.005)	Loss 9.3744e-01 (9.3407e-01)	Acc@1  67.19 ( 66.75)	Acc@5  94.53 ( 96.99)
Epoch: [7][180/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.5449e-01 (9.3478e-01)	Acc@1  66.41 ( 66.73)	Acc@5  96.88 ( 96.99)
Epoch: [7][190/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.0035e-01 (9.3310e-01)	Acc@1  62.50 ( 66.77)	Acc@5  99.22 ( 97.01)
Epoch: [7][200/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0018e+00 (9.3297e-01)	Acc@1  67.19 ( 66.79)	Acc@5  97.66 ( 97.00)
Epoch: [7][210/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.3087e-01 (9.3278e-01)	Acc@1  68.75 ( 66.82)	Acc@5  97.66 ( 97.00)
Epoch: [7][220/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.1708e-01 (9.2881e-01)	Acc@1  67.97 ( 67.04)	Acc@5  98.44 ( 97.02)
Epoch: [7][230/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.8478e-01 (9.2914e-01)	Acc@1  72.66 ( 67.02)	Acc@5  98.44 ( 97.04)
Epoch: [7][240/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.2682e-01 (9.2928e-01)	Acc@1  63.28 ( 67.03)	Acc@5  97.66 ( 97.01)
Epoch: [7][250/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0345e+00 (9.2788e-01)	Acc@1  63.28 ( 67.07)	Acc@5  95.31 ( 97.03)
Epoch: [7][260/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0185e+00 (9.3006e-01)	Acc@1  62.50 ( 67.02)	Acc@5  97.66 ( 97.03)
Epoch: [7][270/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0634e+00 (9.2965e-01)	Acc@1  60.94 ( 67.01)	Acc@5  96.88 ( 97.05)
Epoch: [7][280/391]	Time  0.138 ( 0.137)	Data  0.002 ( 0.005)	Loss 9.2272e-01 (9.2846e-01)	Acc@1  69.53 ( 67.06)	Acc@5  96.88 ( 97.08)
Epoch: [7][290/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.9656e-01 (9.2735e-01)	Acc@1  67.19 ( 67.10)	Acc@5  96.88 ( 97.10)
Epoch: [7][300/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.2339e+00 (9.2724e-01)	Acc@1  64.84 ( 67.12)	Acc@5  96.88 ( 97.11)
Epoch: [7][310/391]	Time  0.130 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.1328e-01 (9.2619e-01)	Acc@1  68.75 ( 67.13)	Acc@5  99.22 ( 97.14)
Epoch: [7][320/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.0606e-01 (9.2444e-01)	Acc@1  64.84 ( 67.19)	Acc@5  98.44 ( 97.16)
Epoch: [7][330/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.1276e-01 (9.2396e-01)	Acc@1  73.44 ( 67.23)	Acc@5  97.66 ( 97.16)
Epoch: [7][340/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1489e+00 (9.2350e-01)	Acc@1  63.28 ( 67.26)	Acc@5  93.75 ( 97.15)
Epoch: [7][350/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.8419e-01 (9.2305e-01)	Acc@1  66.41 ( 67.25)	Acc@5  96.09 ( 97.14)
Epoch: [7][360/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.0407e-01 (9.2335e-01)	Acc@1  71.88 ( 67.23)	Acc@5  99.22 ( 97.13)
Epoch: [7][370/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.3122e-01 (9.2210e-01)	Acc@1  64.06 ( 67.29)	Acc@5  98.44 ( 97.14)
Epoch: [7][380/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.1788e-01 (9.2059e-01)	Acc@1  78.91 ( 67.37)	Acc@5  97.66 ( 97.15)
Epoch: [7][390/391]	Time  0.124 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.1899e-01 (9.1885e-01)	Acc@1  71.25 ( 67.42)	Acc@5  98.75 ( 97.17)
## e[7] optimizer.zero_grad (sum) time: 0.28682661056518555
## e[7]       loss.backward (sum) time: 18.54367971420288
## e[7]      optimizer.step (sum) time: 2.923525333404541
## epoch[7] training(only) time: 53.80508613586426
# Switched to evaluate mode...
Test: [  0/100]	Time  0.205 ( 0.205)	Loss 7.6134e-01 (7.6134e-01)	Acc@1  74.00 ( 74.00)	Acc@5  97.00 ( 97.00)
Test: [ 10/100]	Time  0.042 ( 0.058)	Loss 8.1158e-01 (8.3231e-01)	Acc@1  68.00 ( 70.55)	Acc@5 100.00 ( 97.64)
Test: [ 20/100]	Time  0.044 ( 0.051)	Loss 8.7682e-01 (8.4311e-01)	Acc@1  66.00 ( 70.33)	Acc@5  97.00 ( 97.81)
Test: [ 30/100]	Time  0.044 ( 0.048)	Loss 8.8992e-01 (8.6540e-01)	Acc@1  72.00 ( 69.87)	Acc@5  96.00 ( 97.58)
Test: [ 40/100]	Time  0.044 ( 0.047)	Loss 1.4187e+00 (8.9540e-01)	Acc@1  70.00 ( 69.73)	Acc@5  98.00 ( 97.46)
Test: [ 50/100]	Time  0.043 ( 0.046)	Loss 6.6389e-01 (8.7383e-01)	Acc@1  75.00 ( 70.31)	Acc@5  98.00 ( 97.63)
Test: [ 60/100]	Time  0.043 ( 0.046)	Loss 8.0209e-01 (8.7273e-01)	Acc@1  64.00 ( 69.89)	Acc@5  99.00 ( 97.70)
Test: [ 70/100]	Time  0.043 ( 0.046)	Loss 8.2778e-01 (8.7215e-01)	Acc@1  75.00 ( 70.00)	Acc@5  96.00 ( 97.73)
Test: [ 80/100]	Time  0.045 ( 0.045)	Loss 7.5045e-01 (8.6928e-01)	Acc@1  76.00 ( 70.22)	Acc@5  99.00 ( 97.81)
Test: [ 90/100]	Time  0.044 ( 0.045)	Loss 7.0344e-01 (8.6767e-01)	Acc@1  73.00 ( 70.15)	Acc@5 100.00 ( 97.82)
 * Acc@1 70.110 Acc@5 97.850
### epoch[7] execution time: 58.4239547252655
EPOCH 8
# current learning rate: 0.1
# Switched to train mode...
Epoch: [8][  0/391]	Time  0.313 ( 0.313)	Data  0.178 ( 0.178)	Loss 8.5448e-01 (8.5448e-01)	Acc@1  71.09 ( 71.09)	Acc@5  94.53 ( 94.53)
Epoch: [8][ 10/391]	Time  0.137 ( 0.152)	Data  0.001 ( 0.020)	Loss 7.6043e-01 (8.2305e-01)	Acc@1  73.44 ( 70.10)	Acc@5  98.44 ( 97.66)
Epoch: [8][ 20/391]	Time  0.134 ( 0.144)	Data  0.001 ( 0.013)	Loss 1.0158e+00 (8.4067e-01)	Acc@1  67.19 ( 70.31)	Acc@5  93.75 ( 97.36)
Epoch: [8][ 30/391]	Time  0.136 ( 0.142)	Data  0.001 ( 0.010)	Loss 7.3555e-01 (8.3437e-01)	Acc@1  73.44 ( 70.14)	Acc@5 100.00 ( 97.58)
Epoch: [8][ 40/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.009)	Loss 9.4190e-01 (8.5437e-01)	Acc@1  67.19 ( 69.36)	Acc@5  93.75 ( 97.31)
Epoch: [8][ 50/391]	Time  0.135 ( 0.140)	Data  0.002 ( 0.008)	Loss 7.5506e-01 (8.4226e-01)	Acc@1  71.88 ( 69.76)	Acc@5  99.22 ( 97.52)
Epoch: [8][ 60/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.007)	Loss 7.6831e-01 (8.4507e-01)	Acc@1  76.56 ( 69.75)	Acc@5  99.22 ( 97.50)
Epoch: [8][ 70/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.007)	Loss 8.9537e-01 (8.4837e-01)	Acc@1  65.62 ( 69.56)	Acc@5 100.00 ( 97.62)
Epoch: [8][ 80/391]	Time  0.131 ( 0.139)	Data  0.001 ( 0.007)	Loss 8.2064e-01 (8.5180e-01)	Acc@1  71.09 ( 69.40)	Acc@5  97.66 ( 97.60)
Epoch: [8][ 90/391]	Time  0.130 ( 0.138)	Data  0.001 ( 0.006)	Loss 6.5981e-01 (8.4924e-01)	Acc@1  77.34 ( 69.61)	Acc@5  99.22 ( 97.60)
Epoch: [8][100/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 7.9185e-01 (8.5370e-01)	Acc@1  71.88 ( 69.48)	Acc@5  97.66 ( 97.64)
Epoch: [8][110/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 8.2561e-01 (8.5338e-01)	Acc@1  74.22 ( 69.66)	Acc@5  96.88 ( 97.64)
Epoch: [8][120/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 6.4388e-01 (8.5246e-01)	Acc@1  76.56 ( 69.74)	Acc@5  99.22 ( 97.60)
Epoch: [8][130/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 8.1121e-01 (8.5230e-01)	Acc@1  69.53 ( 69.85)	Acc@5  95.31 ( 97.54)
Epoch: [8][140/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 7.3516e-01 (8.5199e-01)	Acc@1  79.69 ( 69.90)	Acc@5  98.44 ( 97.52)
Epoch: [8][150/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.0291e+00 (8.5373e-01)	Acc@1  64.84 ( 69.84)	Acc@5  94.53 ( 97.48)
Epoch: [8][160/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.006)	Loss 6.7202e-01 (8.5109e-01)	Acc@1  75.78 ( 69.89)	Acc@5  99.22 ( 97.52)
Epoch: [8][170/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 8.4755e-01 (8.4955e-01)	Acc@1  67.19 ( 69.87)	Acc@5  95.31 ( 97.53)
Epoch: [8][180/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.005)	Loss 8.5075e-01 (8.4997e-01)	Acc@1  67.97 ( 69.79)	Acc@5 100.00 ( 97.57)
Epoch: [8][190/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.005)	Loss 7.4204e-01 (8.4934e-01)	Acc@1  68.75 ( 69.79)	Acc@5  99.22 ( 97.57)
Epoch: [8][200/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.005)	Loss 8.9928e-01 (8.4970e-01)	Acc@1  71.09 ( 69.81)	Acc@5  94.53 ( 97.56)
Epoch: [8][210/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.6762e-01 (8.4877e-01)	Acc@1  71.09 ( 69.79)	Acc@5  96.88 ( 97.57)
Epoch: [8][220/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.9599e-01 (8.4752e-01)	Acc@1  75.00 ( 69.81)	Acc@5  98.44 ( 97.57)
Epoch: [8][230/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.2160e-01 (8.4559e-01)	Acc@1  71.88 ( 69.85)	Acc@5  97.66 ( 97.56)
Epoch: [8][240/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.4216e-01 (8.4538e-01)	Acc@1  71.09 ( 69.92)	Acc@5  96.88 ( 97.56)
Epoch: [8][250/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.5507e-01 (8.4512e-01)	Acc@1  73.44 ( 69.93)	Acc@5  99.22 ( 97.56)
Epoch: [8][260/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.9917e-01 (8.4585e-01)	Acc@1  67.19 ( 69.91)	Acc@5  96.88 ( 97.56)
Epoch: [8][270/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.1265e-01 (8.4383e-01)	Acc@1  74.22 ( 70.00)	Acc@5  96.09 ( 97.57)
Epoch: [8][280/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.8583e-01 (8.4319e-01)	Acc@1  72.66 ( 70.05)	Acc@5  99.22 ( 97.55)
Epoch: [8][290/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.8623e-01 (8.4273e-01)	Acc@1  72.66 ( 70.07)	Acc@5  98.44 ( 97.55)
Epoch: [8][300/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.5701e-01 (8.4308e-01)	Acc@1  78.12 ( 70.09)	Acc@5  98.44 ( 97.55)
Epoch: [8][310/391]	Time  0.130 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.2387e-01 (8.4508e-01)	Acc@1  69.53 ( 70.06)	Acc@5  99.22 ( 97.54)
Epoch: [8][320/391]	Time  0.138 ( 0.137)	Data  0.002 ( 0.005)	Loss 8.5866e-01 (8.4402e-01)	Acc@1  68.75 ( 70.08)	Acc@5  99.22 ( 97.56)
Epoch: [8][330/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.9682e-01 (8.4281e-01)	Acc@1  67.19 ( 70.11)	Acc@5  95.31 ( 97.57)
Epoch: [8][340/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.6257e-01 (8.4315e-01)	Acc@1  67.19 ( 70.07)	Acc@5  96.88 ( 97.58)
Epoch: [8][350/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.5322e-01 (8.4316e-01)	Acc@1  71.88 ( 70.06)	Acc@5  98.44 ( 97.61)
Epoch: [8][360/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.9490e-01 (8.4221e-01)	Acc@1  71.88 ( 70.13)	Acc@5  96.88 ( 97.60)
Epoch: [8][370/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.0241e-01 (8.4028e-01)	Acc@1  73.44 ( 70.21)	Acc@5  98.44 ( 97.62)
Epoch: [8][380/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.3802e-01 (8.3924e-01)	Acc@1  68.75 ( 70.27)	Acc@5  94.53 ( 97.63)
Epoch: [8][390/391]	Time  0.127 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.7150e-01 (8.3986e-01)	Acc@1  67.50 ( 70.26)	Acc@5  97.50 ( 97.62)
## e[8] optimizer.zero_grad (sum) time: 0.285764217376709
## e[8]       loss.backward (sum) time: 18.540697813034058
## e[8]      optimizer.step (sum) time: 2.8997864723205566
## epoch[8] training(only) time: 53.7797155380249
# Switched to evaluate mode...
Test: [  0/100]	Time  0.200 ( 0.200)	Loss 8.3912e-01 (8.3912e-01)	Acc@1  72.00 ( 72.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.045 ( 0.058)	Loss 7.2989e-01 (8.4279e-01)	Acc@1  73.00 ( 70.09)	Acc@5  97.00 ( 97.55)
Test: [ 20/100]	Time  0.042 ( 0.051)	Loss 8.9837e-01 (8.1901e-01)	Acc@1  65.00 ( 71.48)	Acc@5  98.00 ( 97.76)
Test: [ 30/100]	Time  0.042 ( 0.049)	Loss 8.2792e-01 (8.3079e-01)	Acc@1  71.00 ( 71.06)	Acc@5  99.00 ( 97.71)
Test: [ 40/100]	Time  0.046 ( 0.047)	Loss 1.1701e+00 (8.3915e-01)	Acc@1  69.00 ( 71.49)	Acc@5  95.00 ( 97.41)
Test: [ 50/100]	Time  0.043 ( 0.047)	Loss 6.9115e-01 (8.2571e-01)	Acc@1  80.00 ( 72.33)	Acc@5  97.00 ( 97.59)
Test: [ 60/100]	Time  0.045 ( 0.046)	Loss 8.5528e-01 (8.3012e-01)	Acc@1  68.00 ( 71.87)	Acc@5 100.00 ( 97.66)
Test: [ 70/100]	Time  0.041 ( 0.046)	Loss 7.6404e-01 (8.3519e-01)	Acc@1  72.00 ( 71.49)	Acc@5 100.00 ( 97.75)
Test: [ 80/100]	Time  0.043 ( 0.045)	Loss 7.3869e-01 (8.3990e-01)	Acc@1  80.00 ( 71.52)	Acc@5  96.00 ( 97.65)
Test: [ 90/100]	Time  0.043 ( 0.045)	Loss 7.1043e-01 (8.4109e-01)	Acc@1  74.00 ( 71.40)	Acc@5  99.00 ( 97.67)
 * Acc@1 71.370 Acc@5 97.690
### epoch[8] execution time: 58.37910342216492
EPOCH 9
# current learning rate: 0.1
# Switched to train mode...
Epoch: [9][  0/391]	Time  0.299 ( 0.299)	Data  0.177 ( 0.177)	Loss 9.3790e-01 (9.3790e-01)	Acc@1  71.09 ( 71.09)	Acc@5  96.09 ( 96.09)
Epoch: [9][ 10/391]	Time  0.133 ( 0.151)	Data  0.001 ( 0.020)	Loss 1.0239e+00 (8.0828e-01)	Acc@1  64.06 ( 72.94)	Acc@5  96.88 ( 97.30)
Epoch: [9][ 20/391]	Time  0.132 ( 0.143)	Data  0.001 ( 0.012)	Loss 7.7146e-01 (8.1050e-01)	Acc@1  67.97 ( 71.95)	Acc@5  96.09 ( 97.43)
Epoch: [9][ 30/391]	Time  0.132 ( 0.141)	Data  0.001 ( 0.010)	Loss 8.5935e-01 (7.8522e-01)	Acc@1  70.31 ( 72.51)	Acc@5  97.66 ( 97.66)
Epoch: [9][ 40/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.009)	Loss 7.8333e-01 (8.0396e-01)	Acc@1  76.56 ( 71.93)	Acc@5  98.44 ( 97.56)
Epoch: [9][ 50/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.008)	Loss 7.5417e-01 (8.0247e-01)	Acc@1  71.88 ( 71.86)	Acc@5 100.00 ( 97.59)
Epoch: [9][ 60/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.007)	Loss 6.8852e-01 (7.9683e-01)	Acc@1  71.88 ( 71.75)	Acc@5  98.44 ( 97.59)
Epoch: [9][ 70/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.007)	Loss 8.2450e-01 (7.9360e-01)	Acc@1  67.19 ( 71.90)	Acc@5  97.66 ( 97.73)
Epoch: [9][ 80/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.007)	Loss 8.9635e-01 (7.9421e-01)	Acc@1  70.31 ( 71.96)	Acc@5  96.09 ( 97.79)
Epoch: [9][ 90/391]	Time  0.131 ( 0.138)	Data  0.001 ( 0.006)	Loss 7.0684e-01 (7.9534e-01)	Acc@1  75.00 ( 71.94)	Acc@5 100.00 ( 97.83)
Epoch: [9][100/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 8.6986e-01 (7.9364e-01)	Acc@1  66.41 ( 71.94)	Acc@5  93.75 ( 97.80)
Epoch: [9][110/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 6.4864e-01 (7.9190e-01)	Acc@1  76.56 ( 71.97)	Acc@5 100.00 ( 97.83)
Epoch: [9][120/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 7.3721e-01 (7.9313e-01)	Acc@1  78.91 ( 72.03)	Acc@5  99.22 ( 97.86)
Epoch: [9][130/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 6.4558e-01 (7.9456e-01)	Acc@1  77.34 ( 71.98)	Acc@5  99.22 ( 97.85)
Epoch: [9][140/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 8.3568e-01 (7.9063e-01)	Acc@1  67.97 ( 72.08)	Acc@5  99.22 ( 97.91)
Epoch: [9][150/391]	Time  0.138 ( 0.138)	Data  0.002 ( 0.006)	Loss 8.1814e-01 (7.8899e-01)	Acc@1  71.09 ( 72.27)	Acc@5  97.66 ( 97.92)
Epoch: [9][160/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 8.8036e-01 (7.9076e-01)	Acc@1  69.53 ( 72.28)	Acc@5  96.88 ( 97.88)
Epoch: [9][170/391]	Time  0.142 ( 0.138)	Data  0.002 ( 0.005)	Loss 7.8843e-01 (7.9200e-01)	Acc@1  71.09 ( 72.12)	Acc@5  96.88 ( 97.90)
Epoch: [9][180/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0068e+00 (7.9266e-01)	Acc@1  63.28 ( 72.01)	Acc@5  96.88 ( 97.91)
Epoch: [9][190/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 8.0002e-01 (7.9114e-01)	Acc@1  72.66 ( 72.09)	Acc@5  98.44 ( 97.94)
Epoch: [9][200/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.005)	Loss 6.4959e-01 (7.8906e-01)	Acc@1  78.12 ( 72.15)	Acc@5  99.22 ( 97.95)
Epoch: [9][210/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.005)	Loss 7.4445e-01 (7.8721e-01)	Acc@1  68.75 ( 72.21)	Acc@5  96.88 ( 97.96)
Epoch: [9][220/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.005)	Loss 8.8176e-01 (7.8646e-01)	Acc@1  71.88 ( 72.32)	Acc@5  97.66 ( 97.93)
Epoch: [9][230/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.005)	Loss 5.2764e-01 (7.8410e-01)	Acc@1  82.03 ( 72.43)	Acc@5  99.22 ( 97.93)
Epoch: [9][240/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.7521e-01 (7.8365e-01)	Acc@1  79.69 ( 72.41)	Acc@5  99.22 ( 97.94)
Epoch: [9][250/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.9996e-01 (7.8224e-01)	Acc@1  71.88 ( 72.46)	Acc@5  97.66 ( 97.96)
Epoch: [9][260/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.6629e-01 (7.8290e-01)	Acc@1  75.00 ( 72.42)	Acc@5  96.88 ( 97.96)
Epoch: [9][270/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.8681e-01 (7.8141e-01)	Acc@1  69.53 ( 72.45)	Acc@5  99.22 ( 97.99)
Epoch: [9][280/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.7966e-01 (7.8078e-01)	Acc@1  72.66 ( 72.41)	Acc@5 100.00 ( 98.00)
Epoch: [9][290/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.9104e-01 (7.8062e-01)	Acc@1  76.56 ( 72.44)	Acc@5  96.88 ( 97.98)
Epoch: [9][300/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.8239e-01 (7.7923e-01)	Acc@1  75.78 ( 72.46)	Acc@5  99.22 ( 97.99)
Epoch: [9][310/391]	Time  0.141 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.3904e-01 (7.7765e-01)	Acc@1  75.78 ( 72.51)	Acc@5  98.44 ( 97.99)
Epoch: [9][320/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.1889e-01 (7.7605e-01)	Acc@1  67.97 ( 72.60)	Acc@5  98.44 ( 97.99)
Epoch: [9][330/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.0168e-01 (7.7619e-01)	Acc@1  75.78 ( 72.62)	Acc@5  96.09 ( 97.99)
Epoch: [9][340/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.3000e-01 (7.7595e-01)	Acc@1  77.34 ( 72.64)	Acc@5  98.44 ( 98.01)
Epoch: [9][350/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.7712e-01 (7.7547e-01)	Acc@1  78.12 ( 72.66)	Acc@5 100.00 ( 98.01)
Epoch: [9][360/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.9958e-01 (7.7441e-01)	Acc@1  79.69 ( 72.69)	Acc@5  96.09 ( 98.01)
Epoch: [9][370/391]	Time  0.132 ( 0.137)	Data  0.002 ( 0.005)	Loss 7.7086e-01 (7.7437e-01)	Acc@1  69.53 ( 72.69)	Acc@5  99.22 ( 98.01)
Epoch: [9][380/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.4092e-01 (7.7503e-01)	Acc@1  73.44 ( 72.67)	Acc@5  97.66 ( 98.01)
Epoch: [9][390/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.1454e-01 (7.7423e-01)	Acc@1  73.75 ( 72.72)	Acc@5  98.75 ( 98.01)
## e[9] optimizer.zero_grad (sum) time: 0.2849442958831787
## e[9]       loss.backward (sum) time: 18.610353231430054
## e[9]      optimizer.step (sum) time: 2.945021152496338
## epoch[9] training(only) time: 53.877718448638916
# Switched to evaluate mode...
Test: [  0/100]	Time  0.200 ( 0.200)	Loss 7.7636e-01 (7.7636e-01)	Acc@1  72.00 ( 72.00)	Acc@5  97.00 ( 97.00)
Test: [ 10/100]	Time  0.044 ( 0.059)	Loss 8.5623e-01 (1.0272e+00)	Acc@1  72.00 ( 68.64)	Acc@5  93.00 ( 97.00)
Test: [ 20/100]	Time  0.043 ( 0.051)	Loss 9.2877e-01 (1.1463e+00)	Acc@1  63.00 ( 68.29)	Acc@5  99.00 ( 97.33)
Test: [ 30/100]	Time  0.043 ( 0.049)	Loss 8.1298e-01 (1.0809e+00)	Acc@1  71.00 ( 68.74)	Acc@5  96.00 ( 97.39)
Test: [ 40/100]	Time  0.045 ( 0.048)	Loss 8.3948e-01 (1.0443e+00)	Acc@1  75.00 ( 68.90)	Acc@5  98.00 ( 97.29)
Test: [ 50/100]	Time  0.044 ( 0.047)	Loss 7.5364e-01 (1.0669e+00)	Acc@1  76.00 ( 69.37)	Acc@5  98.00 ( 97.37)
Test: [ 60/100]	Time  0.043 ( 0.046)	Loss 9.0019e-01 (1.0730e+00)	Acc@1  75.00 ( 69.52)	Acc@5  96.00 ( 97.41)
Test: [ 70/100]	Time  0.043 ( 0.046)	Loss 1.0280e+00 (1.0564e+00)	Acc@1  68.00 ( 69.30)	Acc@5  97.00 ( 97.39)
Test: [ 80/100]	Time  0.046 ( 0.046)	Loss 8.9553e-01 (1.0425e+00)	Acc@1  69.00 ( 69.43)	Acc@5  95.00 ( 97.43)
Test: [ 90/100]	Time  0.043 ( 0.046)	Loss 9.6113e-01 (1.0514e+00)	Acc@1  76.00 ( 69.27)	Acc@5 100.00 ( 97.44)
 * Acc@1 69.060 Acc@5 97.370
### epoch[9] execution time: 58.55449199676514
EPOCH 10
# current learning rate: 0.1
# Switched to train mode...
Epoch: [10][  0/391]	Time  0.291 ( 0.291)	Data  0.172 ( 0.172)	Loss 7.5541e-01 (7.5541e-01)	Acc@1  72.66 ( 72.66)	Acc@5  97.66 ( 97.66)
Epoch: [10][ 10/391]	Time  0.136 ( 0.150)	Data  0.001 ( 0.020)	Loss 7.4557e-01 (7.6069e-01)	Acc@1  73.44 ( 70.88)	Acc@5  98.44 ( 97.94)
Epoch: [10][ 20/391]	Time  0.134 ( 0.143)	Data  0.001 ( 0.012)	Loss 7.2185e-01 (7.2981e-01)	Acc@1  75.78 ( 73.36)	Acc@5  99.22 ( 98.03)
Epoch: [10][ 30/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.010)	Loss 7.9345e-01 (7.2173e-01)	Acc@1  75.00 ( 74.04)	Acc@5  98.44 ( 98.01)
Epoch: [10][ 40/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.008)	Loss 6.4118e-01 (7.1949e-01)	Acc@1  76.56 ( 74.14)	Acc@5 100.00 ( 98.09)
Epoch: [10][ 50/391]	Time  0.132 ( 0.139)	Data  0.001 ( 0.008)	Loss 6.9239e-01 (7.2459e-01)	Acc@1  72.66 ( 73.91)	Acc@5  99.22 ( 98.09)
Epoch: [10][ 60/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.007)	Loss 6.2515e-01 (7.1724e-01)	Acc@1  75.78 ( 74.13)	Acc@5 100.00 ( 98.22)
Epoch: [10][ 70/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.007)	Loss 6.1845e-01 (7.1932e-01)	Acc@1  82.03 ( 74.17)	Acc@5  96.88 ( 98.22)
Epoch: [10][ 80/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 9.0135e-01 (7.1990e-01)	Acc@1  65.62 ( 74.01)	Acc@5  98.44 ( 98.27)
Epoch: [10][ 90/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 7.0617e-01 (7.2028e-01)	Acc@1  75.78 ( 74.16)	Acc@5  97.66 ( 98.19)
Epoch: [10][100/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 6.5245e-01 (7.1822e-01)	Acc@1  77.34 ( 74.28)	Acc@5  99.22 ( 98.21)
Epoch: [10][110/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 6.4105e-01 (7.1739e-01)	Acc@1  76.56 ( 74.51)	Acc@5  98.44 ( 98.18)
Epoch: [10][120/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 6.8359e-01 (7.1896e-01)	Acc@1  79.69 ( 74.52)	Acc@5  97.66 ( 98.17)
Epoch: [10][130/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 7.1243e-01 (7.1588e-01)	Acc@1  73.44 ( 74.63)	Acc@5 100.00 ( 98.20)
Epoch: [10][140/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.006)	Loss 6.7535e-01 (7.1869e-01)	Acc@1  76.56 ( 74.51)	Acc@5  98.44 ( 98.20)
Epoch: [10][150/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.006)	Loss 6.7014e-01 (7.1566e-01)	Acc@1  76.56 ( 74.57)	Acc@5 100.00 ( 98.20)
Epoch: [10][160/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.005)	Loss 8.7546e-01 (7.1716e-01)	Acc@1  70.31 ( 74.60)	Acc@5  98.44 ( 98.20)
Epoch: [10][170/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.005)	Loss 7.3861e-01 (7.1873e-01)	Acc@1  71.09 ( 74.56)	Acc@5  99.22 ( 98.25)
Epoch: [10][180/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 8.2596e-01 (7.2074e-01)	Acc@1  68.75 ( 74.46)	Acc@5  97.66 ( 98.22)
Epoch: [10][190/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.005)	Loss 8.7111e-01 (7.2138e-01)	Acc@1  71.88 ( 74.47)	Acc@5  98.44 ( 98.22)
Epoch: [10][200/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 5.6341e-01 (7.1953e-01)	Acc@1  79.69 ( 74.51)	Acc@5 100.00 ( 98.25)
Epoch: [10][210/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.005)	Loss 8.5523e-01 (7.2237e-01)	Acc@1  70.31 ( 74.44)	Acc@5  96.88 ( 98.24)
Epoch: [10][220/391]	Time  0.130 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.5949e-01 (7.2255e-01)	Acc@1  73.44 ( 74.45)	Acc@5  98.44 ( 98.25)
Epoch: [10][230/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.6178e-01 (7.2204e-01)	Acc@1  73.44 ( 74.42)	Acc@5  97.66 ( 98.28)
Epoch: [10][240/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.2166e-01 (7.2492e-01)	Acc@1  77.34 ( 74.34)	Acc@5  98.44 ( 98.28)
Epoch: [10][250/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.7915e-01 (7.2457e-01)	Acc@1  72.66 ( 74.33)	Acc@5  98.44 ( 98.28)
Epoch: [10][260/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.2180e-01 (7.2471e-01)	Acc@1  70.31 ( 74.33)	Acc@5  96.09 ( 98.28)
Epoch: [10][270/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.7871e-01 (7.2342e-01)	Acc@1  76.56 ( 74.34)	Acc@5  99.22 ( 98.30)
Epoch: [10][280/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.4959e-01 (7.2201e-01)	Acc@1  75.00 ( 74.37)	Acc@5 100.00 ( 98.32)
Epoch: [10][290/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.1169e-01 (7.2085e-01)	Acc@1  77.34 ( 74.42)	Acc@5  99.22 ( 98.32)
Epoch: [10][300/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.0371e-01 (7.2052e-01)	Acc@1  71.09 ( 74.44)	Acc@5  97.66 ( 98.34)
Epoch: [10][310/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.1711e-01 (7.1903e-01)	Acc@1  75.78 ( 74.50)	Acc@5  99.22 ( 98.35)
Epoch: [10][320/391]	Time  0.138 ( 0.137)	Data  0.002 ( 0.005)	Loss 7.0743e-01 (7.2040e-01)	Acc@1  78.91 ( 74.50)	Acc@5  98.44 ( 98.34)
Epoch: [10][330/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.0861e-01 (7.1961e-01)	Acc@1  68.75 ( 74.55)	Acc@5  96.09 ( 98.35)
Epoch: [10][340/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.6659e-01 (7.1855e-01)	Acc@1  79.69 ( 74.56)	Acc@5 100.00 ( 98.36)
Epoch: [10][350/391]	Time  0.142 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.3785e-01 (7.1810e-01)	Acc@1  72.66 ( 74.57)	Acc@5  95.31 ( 98.35)
Epoch: [10][360/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.3040e-01 (7.1715e-01)	Acc@1  75.78 ( 74.60)	Acc@5 100.00 ( 98.35)
Epoch: [10][370/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.3292e-01 (7.1651e-01)	Acc@1  71.88 ( 74.61)	Acc@5  98.44 ( 98.35)
Epoch: [10][380/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.1574e-01 (7.1745e-01)	Acc@1  68.75 ( 74.61)	Acc@5  98.44 ( 98.34)
Epoch: [10][390/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.8533e-01 (7.1678e-01)	Acc@1  78.75 ( 74.65)	Acc@5 100.00 ( 98.35)
## e[10] optimizer.zero_grad (sum) time: 0.289017915725708
## e[10]       loss.backward (sum) time: 18.54509925842285
## e[10]      optimizer.step (sum) time: 2.9518752098083496
## epoch[10] training(only) time: 53.849379539489746
# Switched to evaluate mode...
Test: [  0/100]	Time  0.205 ( 0.205)	Loss 6.0019e-01 (6.0019e-01)	Acc@1  80.00 ( 80.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.045 ( 0.059)	Loss 6.3011e-01 (6.3411e-01)	Acc@1  74.00 ( 78.18)	Acc@5  98.00 ( 98.91)
Test: [ 20/100]	Time  0.044 ( 0.052)	Loss 6.4647e-01 (6.4372e-01)	Acc@1  77.00 ( 77.67)	Acc@5 100.00 ( 98.95)
Test: [ 30/100]	Time  0.043 ( 0.049)	Loss 6.3740e-01 (6.4005e-01)	Acc@1  78.00 ( 77.74)	Acc@5  98.00 ( 98.65)
Test: [ 40/100]	Time  0.046 ( 0.048)	Loss 6.1568e-01 (6.3985e-01)	Acc@1  76.00 ( 77.80)	Acc@5  98.00 ( 98.56)
Test: [ 50/100]	Time  0.041 ( 0.047)	Loss 5.5796e-01 (6.3237e-01)	Acc@1  80.00 ( 78.10)	Acc@5  98.00 ( 98.59)
Test: [ 60/100]	Time  0.046 ( 0.046)	Loss 6.4784e-01 (6.3324e-01)	Acc@1  78.00 ( 78.03)	Acc@5  99.00 ( 98.67)
Test: [ 70/100]	Time  0.041 ( 0.046)	Loss 6.1860e-01 (6.3774e-01)	Acc@1  80.00 ( 77.96)	Acc@5  99.00 ( 98.69)
Test: [ 80/100]	Time  0.045 ( 0.046)	Loss 6.4979e-01 (6.4018e-01)	Acc@1  73.00 ( 77.84)	Acc@5  99.00 ( 98.73)
Test: [ 90/100]	Time  0.044 ( 0.045)	Loss 5.3350e-01 (6.3768e-01)	Acc@1  80.00 ( 77.77)	Acc@5  99.00 ( 98.74)
 * Acc@1 77.610 Acc@5 98.760
### epoch[10] execution time: 58.48249673843384
EPOCH 11
# current learning rate: 0.1
# Switched to train mode...
Epoch: [11][  0/391]	Time  0.290 ( 0.290)	Data  0.174 ( 0.174)	Loss 6.4330e-01 (6.4330e-01)	Acc@1  79.69 ( 79.69)	Acc@5  98.44 ( 98.44)
Epoch: [11][ 10/391]	Time  0.139 ( 0.150)	Data  0.001 ( 0.019)	Loss 7.3765e-01 (6.4645e-01)	Acc@1  73.44 ( 77.84)	Acc@5  98.44 ( 98.72)
Epoch: [11][ 20/391]	Time  0.140 ( 0.143)	Data  0.001 ( 0.012)	Loss 5.0578e-01 (6.4167e-01)	Acc@1  82.03 ( 77.68)	Acc@5 100.00 ( 98.66)
Epoch: [11][ 30/391]	Time  0.132 ( 0.141)	Data  0.001 ( 0.010)	Loss 7.8149e-01 (6.7279e-01)	Acc@1  75.00 ( 76.74)	Acc@5  97.66 ( 98.44)
Epoch: [11][ 40/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.008)	Loss 6.0622e-01 (6.6800e-01)	Acc@1  82.03 ( 76.87)	Acc@5  98.44 ( 98.53)
Epoch: [11][ 50/391]	Time  0.130 ( 0.139)	Data  0.001 ( 0.008)	Loss 6.6179e-01 (6.6940e-01)	Acc@1  81.25 ( 76.61)	Acc@5  96.88 ( 98.56)
Epoch: [11][ 60/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.007)	Loss 6.9069e-01 (6.6705e-01)	Acc@1  72.66 ( 76.59)	Acc@5  99.22 ( 98.54)
Epoch: [11][ 70/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.007)	Loss 6.1889e-01 (6.6268e-01)	Acc@1  76.56 ( 76.65)	Acc@5  98.44 ( 98.57)
Epoch: [11][ 80/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 7.2629e-01 (6.6358e-01)	Acc@1  74.22 ( 76.76)	Acc@5  99.22 ( 98.61)
Epoch: [11][ 90/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 5.6018e-01 (6.6144e-01)	Acc@1  78.91 ( 76.84)	Acc@5 100.00 ( 98.61)
Epoch: [11][100/391]	Time  0.143 ( 0.138)	Data  0.001 ( 0.006)	Loss 6.2313e-01 (6.6884e-01)	Acc@1  75.00 ( 76.58)	Acc@5  99.22 ( 98.59)
Epoch: [11][110/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 6.5009e-01 (6.7407e-01)	Acc@1  78.91 ( 76.39)	Acc@5  96.88 ( 98.51)
Epoch: [11][120/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 7.8936e-01 (6.7863e-01)	Acc@1  76.56 ( 76.16)	Acc@5  98.44 ( 98.51)
Epoch: [11][130/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.006)	Loss 7.3682e-01 (6.8309e-01)	Acc@1  75.78 ( 76.00)	Acc@5  97.66 ( 98.49)
Epoch: [11][140/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 7.6019e-01 (6.8645e-01)	Acc@1  75.00 ( 75.88)	Acc@5  96.88 ( 98.45)
Epoch: [11][150/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.005)	Loss 6.2136e-01 (6.8058e-01)	Acc@1  82.03 ( 76.09)	Acc@5  97.66 ( 98.47)
Epoch: [11][160/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.005)	Loss 7.2551e-01 (6.8114e-01)	Acc@1  74.22 ( 76.13)	Acc@5  99.22 ( 98.50)
Epoch: [11][170/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 5.5798e-01 (6.8056e-01)	Acc@1  75.00 ( 76.11)	Acc@5  99.22 ( 98.49)
Epoch: [11][180/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.005)	Loss 5.6866e-01 (6.8060e-01)	Acc@1  76.56 ( 76.09)	Acc@5 100.00 ( 98.55)
Epoch: [11][190/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.005)	Loss 7.7807e-01 (6.8112e-01)	Acc@1  68.75 ( 76.05)	Acc@5  97.66 ( 98.56)
Epoch: [11][200/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.005)	Loss 6.5069e-01 (6.8223e-01)	Acc@1  74.22 ( 76.03)	Acc@5  98.44 ( 98.57)
Epoch: [11][210/391]	Time  0.131 ( 0.138)	Data  0.001 ( 0.005)	Loss 6.3528e-01 (6.8187e-01)	Acc@1  76.56 ( 76.01)	Acc@5  98.44 ( 98.55)
Epoch: [11][220/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.005)	Loss 6.3187e-01 (6.8420e-01)	Acc@1  76.56 ( 75.98)	Acc@5 100.00 ( 98.52)
Epoch: [11][230/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.005)	Loss 7.1123e-01 (6.8417e-01)	Acc@1  72.66 ( 75.99)	Acc@5  96.88 ( 98.49)
Epoch: [11][240/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.005)	Loss 6.2682e-01 (6.8341e-01)	Acc@1  78.12 ( 75.99)	Acc@5  98.44 ( 98.50)
Epoch: [11][250/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.005)	Loss 6.2303e-01 (6.8063e-01)	Acc@1  78.91 ( 76.10)	Acc@5  99.22 ( 98.51)
Epoch: [11][260/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.0417e-01 (6.8292e-01)	Acc@1  82.81 ( 76.05)	Acc@5  99.22 ( 98.50)
Epoch: [11][270/391]	Time  0.140 ( 0.138)	Data  0.002 ( 0.005)	Loss 6.8703e-01 (6.8092e-01)	Acc@1  75.78 ( 76.12)	Acc@5  97.66 ( 98.51)
Epoch: [11][280/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.2287e-01 (6.7996e-01)	Acc@1  81.25 ( 76.23)	Acc@5  98.44 ( 98.51)
Epoch: [11][290/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.9781e-01 (6.7872e-01)	Acc@1  76.56 ( 76.27)	Acc@5  98.44 ( 98.50)
Epoch: [11][300/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.5799e-01 (6.7788e-01)	Acc@1  82.03 ( 76.32)	Acc@5  96.88 ( 98.50)
Epoch: [11][310/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.2332e-01 (6.7697e-01)	Acc@1  78.91 ( 76.31)	Acc@5  99.22 ( 98.51)
Epoch: [11][320/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.5547e-01 (6.7648e-01)	Acc@1  76.56 ( 76.32)	Acc@5  99.22 ( 98.51)
Epoch: [11][330/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.3521e-01 (6.7642e-01)	Acc@1  75.00 ( 76.34)	Acc@5  98.44 ( 98.50)
Epoch: [11][340/391]	Time  0.136 ( 0.137)	Data  0.002 ( 0.005)	Loss 5.9692e-01 (6.7631e-01)	Acc@1  76.56 ( 76.33)	Acc@5  97.66 ( 98.51)
Epoch: [11][350/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.6382e-01 (6.7568e-01)	Acc@1  73.44 ( 76.37)	Acc@5 100.00 ( 98.52)
Epoch: [11][360/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.0428e-01 (6.7565e-01)	Acc@1  72.66 ( 76.37)	Acc@5  99.22 ( 98.52)
Epoch: [11][370/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.9804e-01 (6.7475e-01)	Acc@1  75.78 ( 76.37)	Acc@5  98.44 ( 98.54)
Epoch: [11][380/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.3863e-01 (6.7423e-01)	Acc@1  78.12 ( 76.38)	Acc@5  99.22 ( 98.54)
Epoch: [11][390/391]	Time  0.124 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.7820e-01 (6.7429e-01)	Acc@1  78.75 ( 76.37)	Acc@5  97.50 ( 98.52)
## e[11] optimizer.zero_grad (sum) time: 0.2869853973388672
## e[11]       loss.backward (sum) time: 18.588348150253296
## e[11]      optimizer.step (sum) time: 2.921299457550049
## epoch[11] training(only) time: 53.835901498794556
# Switched to evaluate mode...
Test: [  0/100]	Time  0.200 ( 0.200)	Loss 6.8611e-01 (6.8611e-01)	Acc@1  75.00 ( 75.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.043 ( 0.058)	Loss 7.0139e-01 (6.9410e-01)	Acc@1  75.00 ( 75.00)	Acc@5  98.00 ( 98.36)
Test: [ 20/100]	Time  0.043 ( 0.051)	Loss 7.9827e-01 (6.9472e-01)	Acc@1  69.00 ( 75.38)	Acc@5 100.00 ( 98.48)
Test: [ 30/100]	Time  0.041 ( 0.048)	Loss 5.9238e-01 (6.8691e-01)	Acc@1  79.00 ( 75.81)	Acc@5  98.00 ( 98.39)
Test: [ 40/100]	Time  0.042 ( 0.047)	Loss 7.3119e-01 (6.9343e-01)	Acc@1  79.00 ( 75.88)	Acc@5  97.00 ( 98.32)
Test: [ 50/100]	Time  0.041 ( 0.046)	Loss 6.2694e-01 (6.8906e-01)	Acc@1  74.00 ( 75.96)	Acc@5  98.00 ( 98.39)
Test: [ 60/100]	Time  0.042 ( 0.046)	Loss 6.8008e-01 (6.8938e-01)	Acc@1  75.00 ( 75.82)	Acc@5  99.00 ( 98.49)
Test: [ 70/100]	Time  0.043 ( 0.045)	Loss 7.6443e-01 (6.9402e-01)	Acc@1  79.00 ( 75.90)	Acc@5  99.00 ( 98.51)
Test: [ 80/100]	Time  0.043 ( 0.045)	Loss 6.4181e-01 (6.9519e-01)	Acc@1  75.00 ( 75.75)	Acc@5  98.00 ( 98.56)
Test: [ 90/100]	Time  0.041 ( 0.045)	Loss 5.4406e-01 (6.9641e-01)	Acc@1  82.00 ( 75.64)	Acc@5 100.00 ( 98.57)
 * Acc@1 75.870 Acc@5 98.550
### epoch[11] execution time: 58.4215989112854
EPOCH 12
# current learning rate: 0.1
# Switched to train mode...
Epoch: [12][  0/391]	Time  0.304 ( 0.304)	Data  0.179 ( 0.179)	Loss 4.3738e-01 (4.3738e-01)	Acc@1  86.72 ( 86.72)	Acc@5  98.44 ( 98.44)
Epoch: [12][ 10/391]	Time  0.136 ( 0.151)	Data  0.001 ( 0.020)	Loss 5.7483e-01 (5.5721e-01)	Acc@1  81.25 ( 80.82)	Acc@5  99.22 ( 98.93)
Epoch: [12][ 20/391]	Time  0.134 ( 0.144)	Data  0.001 ( 0.013)	Loss 6.1208e-01 (5.9046e-01)	Acc@1  78.12 ( 80.06)	Acc@5 100.00 ( 98.70)
Epoch: [12][ 30/391]	Time  0.132 ( 0.141)	Data  0.001 ( 0.010)	Loss 7.3426e-01 (5.8356e-01)	Acc@1  73.44 ( 79.91)	Acc@5 100.00 ( 98.82)
Epoch: [12][ 40/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.009)	Loss 6.5047e-01 (5.9635e-01)	Acc@1  76.56 ( 79.63)	Acc@5  99.22 ( 98.88)
Epoch: [12][ 50/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.008)	Loss 4.9368e-01 (6.0395e-01)	Acc@1  84.38 ( 79.23)	Acc@5  99.22 ( 98.88)
Epoch: [12][ 60/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.007)	Loss 7.5667e-01 (6.0964e-01)	Acc@1  75.78 ( 79.07)	Acc@5  97.66 ( 98.85)
Epoch: [12][ 70/391]	Time  0.129 ( 0.139)	Data  0.001 ( 0.007)	Loss 5.3812e-01 (6.0816e-01)	Acc@1  78.12 ( 79.03)	Acc@5 100.00 ( 98.83)
Epoch: [12][ 80/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.007)	Loss 6.2172e-01 (6.0733e-01)	Acc@1  84.38 ( 79.18)	Acc@5  98.44 ( 98.83)
Epoch: [12][ 90/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 6.7241e-01 (6.1219e-01)	Acc@1  80.47 ( 79.10)	Acc@5  99.22 ( 98.83)
Epoch: [12][100/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 6.3697e-01 (6.1137e-01)	Acc@1  80.47 ( 79.13)	Acc@5  97.66 ( 98.82)
Epoch: [12][110/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 6.2012e-01 (6.1252e-01)	Acc@1  75.78 ( 79.00)	Acc@5  99.22 ( 98.85)
Epoch: [12][120/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.006)	Loss 6.2454e-01 (6.1529e-01)	Acc@1  76.56 ( 78.75)	Acc@5 100.00 ( 98.86)
Epoch: [12][130/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 6.6035e-01 (6.1742e-01)	Acc@1  77.34 ( 78.63)	Acc@5  98.44 ( 98.83)
Epoch: [12][140/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.006)	Loss 5.5009e-01 (6.1655e-01)	Acc@1  82.81 ( 78.60)	Acc@5 100.00 ( 98.81)
Epoch: [12][150/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.006)	Loss 6.2085e-01 (6.1590e-01)	Acc@1  83.59 ( 78.61)	Acc@5  99.22 ( 98.83)
Epoch: [12][160/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.006)	Loss 5.5779e-01 (6.1334e-01)	Acc@1  79.69 ( 78.67)	Acc@5  98.44 ( 98.83)
Epoch: [12][170/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.005)	Loss 5.2882e-01 (6.1589e-01)	Acc@1  82.03 ( 78.60)	Acc@5  99.22 ( 98.79)
Epoch: [12][180/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.005)	Loss 7.0500e-01 (6.1584e-01)	Acc@1  76.56 ( 78.54)	Acc@5  96.88 ( 98.78)
Epoch: [12][190/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.005)	Loss 5.2972e-01 (6.1318e-01)	Acc@1  80.47 ( 78.61)	Acc@5 100.00 ( 98.79)
Epoch: [12][200/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.005)	Loss 7.0966e-01 (6.1319e-01)	Acc@1  73.44 ( 78.58)	Acc@5  97.66 ( 98.80)
Epoch: [12][210/391]	Time  0.137 ( 0.138)	Data  0.002 ( 0.005)	Loss 4.5828e-01 (6.1085e-01)	Acc@1  84.38 ( 78.67)	Acc@5  99.22 ( 98.82)
Epoch: [12][220/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.8064e-01 (6.1195e-01)	Acc@1  71.88 ( 78.62)	Acc@5  98.44 ( 98.82)
Epoch: [12][230/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.7063e-01 (6.1046e-01)	Acc@1  82.03 ( 78.70)	Acc@5  98.44 ( 98.82)
Epoch: [12][240/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.3488e-01 (6.0998e-01)	Acc@1  75.78 ( 78.76)	Acc@5  99.22 ( 98.82)
Epoch: [12][250/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.1235e-01 (6.1376e-01)	Acc@1  77.34 ( 78.59)	Acc@5  99.22 ( 98.80)
Epoch: [12][260/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.2127e-01 (6.1509e-01)	Acc@1  77.34 ( 78.54)	Acc@5  98.44 ( 98.80)
Epoch: [12][270/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.9157e-01 (6.1492e-01)	Acc@1  75.78 ( 78.52)	Acc@5  99.22 ( 98.81)
Epoch: [12][280/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.2872e-01 (6.1420e-01)	Acc@1  73.44 ( 78.52)	Acc@5  96.88 ( 98.81)
Epoch: [12][290/391]	Time  0.130 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.4296e-01 (6.1458e-01)	Acc@1  80.47 ( 78.52)	Acc@5  97.66 ( 98.81)
Epoch: [12][300/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.8086e-01 (6.1300e-01)	Acc@1  85.94 ( 78.58)	Acc@5  98.44 ( 98.82)
Epoch: [12][310/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.8286e-01 (6.1251e-01)	Acc@1  76.56 ( 78.57)	Acc@5  98.44 ( 98.81)
Epoch: [12][320/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.2301e-01 (6.1137e-01)	Acc@1  71.88 ( 78.61)	Acc@5  99.22 ( 98.82)
Epoch: [12][330/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.6140e-01 (6.1278e-01)	Acc@1  75.00 ( 78.55)	Acc@5  97.66 ( 98.83)
Epoch: [12][340/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.2073e-01 (6.1411e-01)	Acc@1  78.12 ( 78.52)	Acc@5  99.22 ( 98.82)
Epoch: [12][350/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.2566e-01 (6.1405e-01)	Acc@1  80.47 ( 78.53)	Acc@5  99.22 ( 98.83)
Epoch: [12][360/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.6034e-01 (6.1308e-01)	Acc@1  83.59 ( 78.58)	Acc@5 100.00 ( 98.85)
Epoch: [12][370/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.9184e-01 (6.1234e-01)	Acc@1  79.69 ( 78.58)	Acc@5  97.66 ( 98.85)
Epoch: [12][380/391]	Time  0.142 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.4930e-01 (6.1388e-01)	Acc@1  78.12 ( 78.53)	Acc@5  97.66 ( 98.84)
Epoch: [12][390/391]	Time  0.129 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.1832e-01 (6.1362e-01)	Acc@1  78.75 ( 78.56)	Acc@5  96.25 ( 98.82)
## e[12] optimizer.zero_grad (sum) time: 0.28662800788879395
## e[12]       loss.backward (sum) time: 18.56482720375061
## e[12]      optimizer.step (sum) time: 2.858234167098999
## epoch[12] training(only) time: 53.882548332214355
# Switched to evaluate mode...
Test: [  0/100]	Time  0.208 ( 0.208)	Loss 5.4757e-01 (5.4757e-01)	Acc@1  84.00 ( 84.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.046 ( 0.059)	Loss 5.8496e-01 (6.0183e-01)	Acc@1  74.00 ( 78.09)	Acc@5  99.00 ( 99.09)
Test: [ 20/100]	Time  0.044 ( 0.052)	Loss 6.1845e-01 (6.1349e-01)	Acc@1  77.00 ( 77.43)	Acc@5 100.00 ( 99.10)
Test: [ 30/100]	Time  0.045 ( 0.049)	Loss 5.7396e-01 (6.2123e-01)	Acc@1  81.00 ( 77.55)	Acc@5  98.00 ( 98.84)
Test: [ 40/100]	Time  0.044 ( 0.048)	Loss 6.4277e-01 (6.3213e-01)	Acc@1  79.00 ( 77.39)	Acc@5  99.00 ( 98.71)
Test: [ 50/100]	Time  0.041 ( 0.047)	Loss 5.1319e-01 (6.3066e-01)	Acc@1  78.00 ( 77.57)	Acc@5 100.00 ( 98.82)
Test: [ 60/100]	Time  0.042 ( 0.047)	Loss 5.4268e-01 (6.2570e-01)	Acc@1  77.00 ( 77.75)	Acc@5 100.00 ( 98.82)
Test: [ 70/100]	Time  0.043 ( 0.046)	Loss 6.8867e-01 (6.2389e-01)	Acc@1  78.00 ( 77.89)	Acc@5 100.00 ( 98.87)
Test: [ 80/100]	Time  0.044 ( 0.046)	Loss 6.6354e-01 (6.2721e-01)	Acc@1  77.00 ( 77.88)	Acc@5  98.00 ( 98.83)
Test: [ 90/100]	Time  0.043 ( 0.046)	Loss 4.4037e-01 (6.2496e-01)	Acc@1  82.00 ( 78.03)	Acc@5 100.00 ( 98.86)
 * Acc@1 78.140 Acc@5 98.900
### epoch[12] execution time: 58.55957627296448
EPOCH 13
# current learning rate: 0.1
# Switched to train mode...
Epoch: [13][  0/391]	Time  0.282 ( 0.282)	Data  0.165 ( 0.165)	Loss 6.2985e-01 (6.2985e-01)	Acc@1  81.25 ( 81.25)	Acc@5  99.22 ( 99.22)
Epoch: [13][ 10/391]	Time  0.136 ( 0.149)	Data  0.001 ( 0.019)	Loss 5.5627e-01 (5.5942e-01)	Acc@1  80.47 ( 80.61)	Acc@5  99.22 ( 99.08)
Epoch: [13][ 20/391]	Time  0.135 ( 0.143)	Data  0.001 ( 0.012)	Loss 3.9361e-01 (5.3216e-01)	Acc@1  85.94 ( 81.32)	Acc@5 100.00 ( 99.14)
Epoch: [13][ 30/391]	Time  0.139 ( 0.141)	Data  0.001 ( 0.010)	Loss 5.6238e-01 (5.3381e-01)	Acc@1  81.25 ( 81.38)	Acc@5 100.00 ( 99.24)
Epoch: [13][ 40/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.008)	Loss 6.4861e-01 (5.3563e-01)	Acc@1  77.34 ( 81.17)	Acc@5 100.00 ( 99.26)
Epoch: [13][ 50/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.008)	Loss 7.1432e-01 (5.4674e-01)	Acc@1  73.44 ( 80.56)	Acc@5  97.66 ( 99.22)
Epoch: [13][ 60/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.007)	Loss 5.5248e-01 (5.5885e-01)	Acc@1  81.25 ( 80.34)	Acc@5  99.22 ( 99.12)
Epoch: [13][ 70/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.007)	Loss 5.7329e-01 (5.6539e-01)	Acc@1  82.81 ( 80.18)	Acc@5  98.44 ( 99.06)
Epoch: [13][ 80/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.6299e-01 (5.7048e-01)	Acc@1  85.94 ( 80.07)	Acc@5  99.22 ( 99.08)
Epoch: [13][ 90/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.0010e-01 (5.7098e-01)	Acc@1  85.94 ( 80.01)	Acc@5 100.00 ( 99.12)
Epoch: [13][100/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.006)	Loss 6.4966e-01 (5.7337e-01)	Acc@1  76.56 ( 79.97)	Acc@5  98.44 ( 99.10)
Epoch: [13][110/391]	Time  0.131 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.8092e-01 (5.7128e-01)	Acc@1  85.16 ( 80.03)	Acc@5  99.22 ( 99.06)
Epoch: [13][120/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 6.4256e-01 (5.7110e-01)	Acc@1  77.34 ( 80.07)	Acc@5 100.00 ( 99.06)
Epoch: [13][130/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 6.4476e-01 (5.6782e-01)	Acc@1  78.12 ( 80.25)	Acc@5  99.22 ( 99.06)
Epoch: [13][140/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.006)	Loss 5.6678e-01 (5.6789e-01)	Acc@1  81.25 ( 80.27)	Acc@5  99.22 ( 99.05)
Epoch: [13][150/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 5.8963e-01 (5.6713e-01)	Acc@1  78.91 ( 80.26)	Acc@5  98.44 ( 99.05)
Epoch: [13][160/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.005)	Loss 5.3760e-01 (5.6957e-01)	Acc@1  82.03 ( 80.23)	Acc@5  97.66 ( 99.03)
Epoch: [13][170/391]	Time  0.133 ( 0.138)	Data  0.002 ( 0.005)	Loss 7.1888e-01 (5.7361e-01)	Acc@1  69.53 ( 80.08)	Acc@5  99.22 ( 99.01)
Epoch: [13][180/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.005)	Loss 5.9142e-01 (5.7437e-01)	Acc@1  77.34 ( 79.99)	Acc@5  99.22 ( 98.99)
Epoch: [13][190/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.3273e-01 (5.7562e-01)	Acc@1  72.66 ( 79.96)	Acc@5  99.22 ( 99.00)
Epoch: [13][200/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.5607e-01 (5.7562e-01)	Acc@1  73.44 ( 79.90)	Acc@5  97.66 ( 99.00)
Epoch: [13][210/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.8869e-01 (5.7899e-01)	Acc@1  80.47 ( 79.72)	Acc@5  97.66 ( 98.99)
Epoch: [13][220/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.8267e-01 (5.7864e-01)	Acc@1  72.66 ( 79.69)	Acc@5  99.22 ( 98.99)
Epoch: [13][230/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.2082e-01 (5.7887e-01)	Acc@1  78.91 ( 79.73)	Acc@5  98.44 ( 99.00)
Epoch: [13][240/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.7019e-01 (5.8101e-01)	Acc@1  81.25 ( 79.70)	Acc@5  99.22 ( 98.99)
Epoch: [13][250/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.8030e-01 (5.8131e-01)	Acc@1  72.66 ( 79.69)	Acc@5  99.22 ( 98.98)
Epoch: [13][260/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.2529e-01 (5.8190e-01)	Acc@1  84.38 ( 79.67)	Acc@5 100.00 ( 98.98)
Epoch: [13][270/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.6551e-01 (5.7965e-01)	Acc@1  79.69 ( 79.77)	Acc@5 100.00 ( 98.99)
Epoch: [13][280/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.6883e-01 (5.7909e-01)	Acc@1  81.25 ( 79.80)	Acc@5 100.00 ( 99.00)
Epoch: [13][290/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.3156e-01 (5.7884e-01)	Acc@1  82.03 ( 79.82)	Acc@5  99.22 ( 99.00)
Epoch: [13][300/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.3947e-01 (5.7879e-01)	Acc@1  84.38 ( 79.84)	Acc@5  99.22 ( 99.01)
Epoch: [13][310/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.5674e-01 (5.7569e-01)	Acc@1  78.12 ( 79.93)	Acc@5 100.00 ( 99.04)
Epoch: [13][320/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.5898e-01 (5.7641e-01)	Acc@1  71.09 ( 79.93)	Acc@5  99.22 ( 99.02)
Epoch: [13][330/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.1189e-01 (5.7671e-01)	Acc@1  82.81 ( 79.87)	Acc@5  99.22 ( 99.04)
Epoch: [13][340/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.4637e-01 (5.7679e-01)	Acc@1  83.59 ( 79.88)	Acc@5  99.22 ( 99.02)
Epoch: [13][350/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.0429e-01 (5.7798e-01)	Acc@1  81.25 ( 79.84)	Acc@5  98.44 ( 99.02)
Epoch: [13][360/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.2167e-01 (5.7802e-01)	Acc@1  84.38 ( 79.86)	Acc@5  99.22 ( 99.01)
Epoch: [13][370/391]	Time  0.130 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.4948e-01 (5.7666e-01)	Acc@1  80.47 ( 79.92)	Acc@5 100.00 ( 99.02)
Epoch: [13][380/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.8995e-01 (5.7599e-01)	Acc@1  81.25 ( 79.96)	Acc@5  98.44 ( 99.01)
Epoch: [13][390/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.4555e-01 (5.7719e-01)	Acc@1  76.25 ( 79.90)	Acc@5  98.75 ( 98.99)
## e[13] optimizer.zero_grad (sum) time: 0.28596997261047363
## e[13]       loss.backward (sum) time: 18.55853033065796
## e[13]      optimizer.step (sum) time: 2.9687905311584473
## epoch[13] training(only) time: 53.73547053337097
# Switched to evaluate mode...
Test: [  0/100]	Time  0.203 ( 0.203)	Loss 6.0251e-01 (6.0251e-01)	Acc@1  79.00 ( 79.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.043 ( 0.058)	Loss 6.7998e-01 (6.5211e-01)	Acc@1  76.00 ( 77.00)	Acc@5  99.00 ( 99.09)
Test: [ 20/100]	Time  0.043 ( 0.051)	Loss 7.1742e-01 (6.3176e-01)	Acc@1  72.00 ( 77.52)	Acc@5 100.00 ( 99.14)
Test: [ 30/100]	Time  0.043 ( 0.049)	Loss 6.4028e-01 (6.3679e-01)	Acc@1  81.00 ( 77.84)	Acc@5  98.00 ( 98.97)
Test: [ 40/100]	Time  0.043 ( 0.047)	Loss 6.4214e-01 (6.3874e-01)	Acc@1  78.00 ( 77.76)	Acc@5  99.00 ( 98.83)
Test: [ 50/100]	Time  0.043 ( 0.046)	Loss 5.6339e-01 (6.3449e-01)	Acc@1  80.00 ( 78.29)	Acc@5  99.00 ( 98.88)
Test: [ 60/100]	Time  0.044 ( 0.046)	Loss 6.5708e-01 (6.3116e-01)	Acc@1  77.00 ( 78.44)	Acc@5  99.00 ( 98.90)
Test: [ 70/100]	Time  0.043 ( 0.046)	Loss 6.8304e-01 (6.3968e-01)	Acc@1  78.00 ( 78.21)	Acc@5  99.00 ( 98.93)
Test: [ 80/100]	Time  0.041 ( 0.045)	Loss 5.8313e-01 (6.4174e-01)	Acc@1  79.00 ( 78.10)	Acc@5 100.00 ( 98.95)
Test: [ 90/100]	Time  0.042 ( 0.045)	Loss 4.6985e-01 (6.4309e-01)	Acc@1  84.00 ( 78.04)	Acc@5 100.00 ( 98.92)
 * Acc@1 77.880 Acc@5 98.970
### epoch[13] execution time: 58.33792757987976
EPOCH 14
# current learning rate: 0.1
# Switched to train mode...
Epoch: [14][  0/391]	Time  0.292 ( 0.292)	Data  0.176 ( 0.176)	Loss 4.4382e-01 (4.4382e-01)	Acc@1  82.81 ( 82.81)	Acc@5 100.00 (100.00)
Epoch: [14][ 10/391]	Time  0.139 ( 0.150)	Data  0.001 ( 0.020)	Loss 5.3934e-01 (5.4067e-01)	Acc@1  82.81 ( 81.61)	Acc@5  99.22 ( 99.57)
Epoch: [14][ 20/391]	Time  0.137 ( 0.144)	Data  0.001 ( 0.012)	Loss 5.0021e-01 (5.1686e-01)	Acc@1  83.59 ( 82.22)	Acc@5  99.22 ( 99.52)
Epoch: [14][ 30/391]	Time  0.133 ( 0.141)	Data  0.001 ( 0.010)	Loss 5.1989e-01 (5.0690e-01)	Acc@1  83.59 ( 82.59)	Acc@5  99.22 ( 99.32)
Epoch: [14][ 40/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.008)	Loss 6.2290e-01 (5.1941e-01)	Acc@1  78.91 ( 82.05)	Acc@5  98.44 ( 99.12)
Epoch: [14][ 50/391]	Time  0.140 ( 0.140)	Data  0.001 ( 0.008)	Loss 6.6864e-01 (5.1322e-01)	Acc@1  76.56 ( 82.26)	Acc@5  99.22 ( 99.17)
Epoch: [14][ 60/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.007)	Loss 5.4947e-01 (5.1465e-01)	Acc@1  81.25 ( 82.07)	Acc@5  99.22 ( 99.13)
Epoch: [14][ 70/391]	Time  0.130 ( 0.139)	Data  0.001 ( 0.007)	Loss 4.4746e-01 (5.1514e-01)	Acc@1  82.03 ( 82.08)	Acc@5 100.00 ( 99.19)
Epoch: [14][ 80/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.006)	Loss 4.0454e-01 (5.1889e-01)	Acc@1  85.16 ( 81.97)	Acc@5  99.22 ( 99.15)
Epoch: [14][ 90/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.006)	Loss 5.5794e-01 (5.2026e-01)	Acc@1  79.69 ( 81.99)	Acc@5 100.00 ( 99.17)
Epoch: [14][100/391]	Time  0.133 ( 0.138)	Data  0.002 ( 0.006)	Loss 6.4246e-01 (5.1908e-01)	Acc@1  76.56 ( 82.12)	Acc@5  98.44 ( 99.16)
Epoch: [14][110/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.4703e-01 (5.1897e-01)	Acc@1  84.38 ( 82.12)	Acc@5  99.22 ( 99.13)
Epoch: [14][120/391]	Time  0.138 ( 0.138)	Data  0.002 ( 0.006)	Loss 4.6657e-01 (5.1448e-01)	Acc@1  83.59 ( 82.29)	Acc@5  97.66 ( 99.13)
Epoch: [14][130/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 5.8329e-01 (5.1478e-01)	Acc@1  78.91 ( 82.24)	Acc@5  97.66 ( 99.10)
Epoch: [14][140/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.5013e-01 (5.1664e-01)	Acc@1  84.38 ( 82.18)	Acc@5  99.22 ( 99.09)
Epoch: [14][150/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.6839e-01 (5.1633e-01)	Acc@1  82.03 ( 82.10)	Acc@5  99.22 ( 99.08)
Epoch: [14][160/391]	Time  0.131 ( 0.138)	Data  0.001 ( 0.005)	Loss 6.0172e-01 (5.1800e-01)	Acc@1  78.12 ( 82.01)	Acc@5  99.22 ( 99.08)
Epoch: [14][170/391]	Time  0.131 ( 0.138)	Data  0.001 ( 0.005)	Loss 6.2034e-01 (5.2251e-01)	Acc@1  79.69 ( 81.83)	Acc@5  98.44 ( 99.08)
Epoch: [14][180/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.005)	Loss 4.1812e-01 (5.1944e-01)	Acc@1  80.47 ( 81.89)	Acc@5 100.00 ( 99.11)
Epoch: [14][190/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.005)	Loss 5.1949e-01 (5.1941e-01)	Acc@1  79.69 ( 81.89)	Acc@5  99.22 ( 99.10)
Epoch: [14][200/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.005)	Loss 5.7476e-01 (5.2003e-01)	Acc@1  81.25 ( 81.85)	Acc@5  99.22 ( 99.10)
Epoch: [14][210/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.005)	Loss 5.3479e-01 (5.1911e-01)	Acc@1  78.91 ( 81.84)	Acc@5 100.00 ( 99.12)
Epoch: [14][220/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.4172e-01 (5.1757e-01)	Acc@1  88.28 ( 81.84)	Acc@5  99.22 ( 99.13)
Epoch: [14][230/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.7038e-01 (5.1821e-01)	Acc@1  84.38 ( 81.81)	Acc@5 100.00 ( 99.13)
Epoch: [14][240/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.5167e-01 (5.1837e-01)	Acc@1  77.34 ( 81.85)	Acc@5 100.00 ( 99.14)
Epoch: [14][250/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.3143e-01 (5.1932e-01)	Acc@1  78.12 ( 81.78)	Acc@5  99.22 ( 99.15)
Epoch: [14][260/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 4.6929e-01 (5.1929e-01)	Acc@1  82.81 ( 81.77)	Acc@5 100.00 ( 99.16)
Epoch: [14][270/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.1467e-01 (5.1964e-01)	Acc@1  78.12 ( 81.80)	Acc@5  97.66 ( 99.16)
Epoch: [14][280/391]	Time  0.141 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.1599e-01 (5.2163e-01)	Acc@1  73.44 ( 81.73)	Acc@5  98.44 ( 99.15)
Epoch: [14][290/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.9615e-01 (5.2241e-01)	Acc@1  79.69 ( 81.71)	Acc@5 100.00 ( 99.15)
Epoch: [14][300/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.1739e-01 (5.2294e-01)	Acc@1  79.69 ( 81.72)	Acc@5  97.66 ( 99.13)
Epoch: [14][310/391]	Time  0.135 ( 0.137)	Data  0.002 ( 0.005)	Loss 5.5957e-01 (5.2337e-01)	Acc@1  82.81 ( 81.74)	Acc@5  99.22 ( 99.13)
Epoch: [14][320/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.3866e-01 (5.2426e-01)	Acc@1  78.12 ( 81.68)	Acc@5  99.22 ( 99.13)
Epoch: [14][330/391]	Time  0.135 ( 0.137)	Data  0.002 ( 0.005)	Loss 5.0200e-01 (5.2382e-01)	Acc@1  81.25 ( 81.68)	Acc@5 100.00 ( 99.14)
Epoch: [14][340/391]	Time  0.142 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.3243e-01 (5.2327e-01)	Acc@1  80.47 ( 81.71)	Acc@5  99.22 ( 99.13)
Epoch: [14][350/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.9598e-01 (5.2275e-01)	Acc@1  86.72 ( 81.74)	Acc@5 100.00 ( 99.15)
Epoch: [14][360/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.0295e-01 (5.2265e-01)	Acc@1  86.72 ( 81.77)	Acc@5  98.44 ( 99.14)
Epoch: [14][370/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.9994e-01 (5.2356e-01)	Acc@1  82.81 ( 81.74)	Acc@5  99.22 ( 99.15)
Epoch: [14][380/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.4300e-01 (5.2324e-01)	Acc@1  82.03 ( 81.77)	Acc@5  98.44 ( 99.14)
Epoch: [14][390/391]	Time  0.128 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.8130e-01 (5.2295e-01)	Acc@1  70.00 ( 81.75)	Acc@5 100.00 ( 99.15)
## e[14] optimizer.zero_grad (sum) time: 0.287900447845459
## e[14]       loss.backward (sum) time: 18.603662967681885
## e[14]      optimizer.step (sum) time: 2.9027907848358154
## epoch[14] training(only) time: 53.860817670822144
# Switched to evaluate mode...
Test: [  0/100]	Time  0.197 ( 0.197)	Loss 4.8587e-01 (4.8587e-01)	Acc@1  86.00 ( 86.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.042 ( 0.057)	Loss 5.0768e-01 (5.0773e-01)	Acc@1  82.00 ( 81.45)	Acc@5  99.00 ( 99.55)
Test: [ 20/100]	Time  0.041 ( 0.050)	Loss 5.6916e-01 (5.1383e-01)	Acc@1  82.00 ( 81.33)	Acc@5  99.00 ( 99.19)
Test: [ 30/100]	Time  0.043 ( 0.049)	Loss 6.2251e-01 (5.2824e-01)	Acc@1  78.00 ( 81.16)	Acc@5  98.00 ( 99.06)
Test: [ 40/100]	Time  0.044 ( 0.047)	Loss 5.6784e-01 (5.3095e-01)	Acc@1  83.00 ( 81.29)	Acc@5  98.00 ( 99.07)
Test: [ 50/100]	Time  0.041 ( 0.046)	Loss 4.4815e-01 (5.3265e-01)	Acc@1  80.00 ( 81.24)	Acc@5  98.00 ( 99.12)
Test: [ 60/100]	Time  0.044 ( 0.046)	Loss 5.7600e-01 (5.3151e-01)	Acc@1  81.00 ( 81.44)	Acc@5 100.00 ( 99.18)
Test: [ 70/100]	Time  0.045 ( 0.046)	Loss 6.6053e-01 (5.3679e-01)	Acc@1  82.00 ( 81.35)	Acc@5  99.00 ( 99.17)
Test: [ 80/100]	Time  0.043 ( 0.045)	Loss 5.0082e-01 (5.4025e-01)	Acc@1  82.00 ( 81.31)	Acc@5  99.00 ( 99.15)
Test: [ 90/100]	Time  0.045 ( 0.045)	Loss 3.7922e-01 (5.4162e-01)	Acc@1  88.00 ( 81.31)	Acc@5 100.00 ( 99.19)
 * Acc@1 81.390 Acc@5 99.240
### epoch[14] execution time: 58.48987936973572
EPOCH 15
# current learning rate: 0.1
# Switched to train mode...
Epoch: [15][  0/391]	Time  0.299 ( 0.299)	Data  0.178 ( 0.178)	Loss 5.1522e-01 (5.1522e-01)	Acc@1  81.25 ( 81.25)	Acc@5  99.22 ( 99.22)
Epoch: [15][ 10/391]	Time  0.134 ( 0.151)	Data  0.001 ( 0.020)	Loss 6.3612e-01 (5.0341e-01)	Acc@1  77.34 ( 81.82)	Acc@5  98.44 ( 99.22)
Epoch: [15][ 20/391]	Time  0.133 ( 0.144)	Data  0.001 ( 0.012)	Loss 4.4068e-01 (4.9116e-01)	Acc@1  82.03 ( 82.89)	Acc@5  99.22 ( 99.00)
Epoch: [15][ 30/391]	Time  0.138 ( 0.141)	Data  0.001 ( 0.010)	Loss 5.1404e-01 (4.9670e-01)	Acc@1  81.25 ( 82.99)	Acc@5  98.44 ( 98.92)
Epoch: [15][ 40/391]	Time  0.132 ( 0.140)	Data  0.001 ( 0.008)	Loss 5.5019e-01 (4.9589e-01)	Acc@1  79.69 ( 82.93)	Acc@5  99.22 ( 99.03)
Epoch: [15][ 50/391]	Time  0.135 ( 0.139)	Data  0.002 ( 0.008)	Loss 3.2803e-01 (4.7881e-01)	Acc@1  90.62 ( 83.64)	Acc@5  98.44 ( 99.11)
Epoch: [15][ 60/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.007)	Loss 6.1192e-01 (4.7946e-01)	Acc@1  78.91 ( 83.50)	Acc@5  99.22 ( 99.10)
Epoch: [15][ 70/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.007)	Loss 5.0074e-01 (4.8532e-01)	Acc@1  81.25 ( 83.05)	Acc@5 100.00 ( 99.17)
Epoch: [15][ 80/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.006)	Loss 5.3767e-01 (4.8969e-01)	Acc@1  81.25 ( 82.84)	Acc@5 100.00 ( 99.16)
Epoch: [15][ 90/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.3427e-01 (4.9239e-01)	Acc@1  80.47 ( 82.69)	Acc@5 100.00 ( 99.19)
Epoch: [15][100/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 6.8794e-01 (4.9686e-01)	Acc@1  76.56 ( 82.67)	Acc@5  96.88 ( 99.19)
Epoch: [15][110/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.9680e-01 (4.9385e-01)	Acc@1  91.41 ( 82.85)	Acc@5  99.22 ( 99.20)
Epoch: [15][120/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.3023e-01 (4.9232e-01)	Acc@1  83.59 ( 82.92)	Acc@5 100.00 ( 99.20)
Epoch: [15][130/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 5.2759e-01 (4.9371e-01)	Acc@1  78.91 ( 82.88)	Acc@5  99.22 ( 99.20)
Epoch: [15][140/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 6.3487e-01 (4.9627e-01)	Acc@1  72.66 ( 82.65)	Acc@5 100.00 ( 99.22)
Epoch: [15][150/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 6.1619e-01 (4.9801e-01)	Acc@1  75.78 ( 82.58)	Acc@5  99.22 ( 99.22)
Epoch: [15][160/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.9390e-01 (4.9655e-01)	Acc@1  84.38 ( 82.70)	Acc@5  99.22 ( 99.24)
Epoch: [15][170/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.005)	Loss 4.2912e-01 (4.9586e-01)	Acc@1  85.94 ( 82.78)	Acc@5 100.00 ( 99.23)
Epoch: [15][180/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.005)	Loss 4.6458e-01 (4.9515e-01)	Acc@1  82.81 ( 82.85)	Acc@5 100.00 ( 99.23)
Epoch: [15][190/391]	Time  0.128 ( 0.138)	Data  0.002 ( 0.005)	Loss 4.9878e-01 (4.9470e-01)	Acc@1  83.59 ( 82.93)	Acc@5  97.66 ( 99.21)
Epoch: [15][200/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.005)	Loss 4.5432e-01 (4.9539e-01)	Acc@1  82.03 ( 82.89)	Acc@5  99.22 ( 99.21)
Epoch: [15][210/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.005)	Loss 5.5565e-01 (4.9476e-01)	Acc@1  78.91 ( 82.92)	Acc@5  99.22 ( 99.24)
Epoch: [15][220/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.005)	Loss 5.2466e-01 (4.9410e-01)	Acc@1  80.47 ( 82.90)	Acc@5  98.44 ( 99.23)
Epoch: [15][230/391]	Time  0.129 ( 0.138)	Data  0.002 ( 0.005)	Loss 3.3455e-01 (4.9377e-01)	Acc@1  89.06 ( 82.90)	Acc@5  97.66 ( 99.24)
Epoch: [15][240/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.005)	Loss 4.5619e-01 (4.9368e-01)	Acc@1  82.81 ( 82.93)	Acc@5  99.22 ( 99.23)
Epoch: [15][250/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 4.1328e-01 (4.9375e-01)	Acc@1  85.16 ( 82.91)	Acc@5 100.00 ( 99.23)
Epoch: [15][260/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.3945e-01 (4.9302e-01)	Acc@1  88.28 ( 82.92)	Acc@5 100.00 ( 99.25)
Epoch: [15][270/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.005)	Loss 5.1812e-01 (4.9417e-01)	Acc@1  80.47 ( 82.90)	Acc@5  98.44 ( 99.24)
Epoch: [15][280/391]	Time  0.130 ( 0.137)	Data  0.002 ( 0.005)	Loss 6.0047e-01 (4.9459e-01)	Acc@1  78.91 ( 82.90)	Acc@5  99.22 ( 99.25)
Epoch: [15][290/391]	Time  0.130 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.6141e-01 (4.9498e-01)	Acc@1  85.16 ( 82.91)	Acc@5  98.44 ( 99.24)
Epoch: [15][300/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.9073e-01 (4.9520e-01)	Acc@1  82.81 ( 82.88)	Acc@5  99.22 ( 99.24)
Epoch: [15][310/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.6811e-01 (4.9544e-01)	Acc@1  85.94 ( 82.86)	Acc@5 100.00 ( 99.25)
Epoch: [15][320/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.2817e-01 (4.9452e-01)	Acc@1  82.03 ( 82.87)	Acc@5 100.00 ( 99.25)
Epoch: [15][330/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.7276e-01 (4.9416e-01)	Acc@1  89.06 ( 82.89)	Acc@5  99.22 ( 99.25)
Epoch: [15][340/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.7308e-01 (4.9449e-01)	Acc@1  88.28 ( 82.90)	Acc@5  99.22 ( 99.23)
Epoch: [15][350/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.6196e-01 (4.9269e-01)	Acc@1  85.16 ( 83.00)	Acc@5 100.00 ( 99.24)
Epoch: [15][360/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.7283e-01 (4.9268e-01)	Acc@1  82.81 ( 83.00)	Acc@5 100.00 ( 99.23)
Epoch: [15][370/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.8538e-01 (4.9153e-01)	Acc@1  89.06 ( 83.03)	Acc@5 100.00 ( 99.24)
Epoch: [15][380/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.9479e-01 (4.9055e-01)	Acc@1  89.06 ( 83.03)	Acc@5 100.00 ( 99.25)
Epoch: [15][390/391]	Time  0.125 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.4230e-01 (4.9049e-01)	Acc@1  81.25 ( 83.02)	Acc@5  98.75 ( 99.25)
## e[15] optimizer.zero_grad (sum) time: 0.2817399501800537
## e[15]       loss.backward (sum) time: 18.52096962928772
## e[15]      optimizer.step (sum) time: 2.9089162349700928
## epoch[15] training(only) time: 53.87965202331543
# Switched to evaluate mode...
Test: [  0/100]	Time  0.196 ( 0.196)	Loss 4.7218e-01 (4.7218e-01)	Acc@1  86.00 ( 86.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.043 ( 0.058)	Loss 5.1667e-01 (4.9915e-01)	Acc@1  85.00 ( 82.36)	Acc@5  98.00 ( 99.09)
Test: [ 20/100]	Time  0.043 ( 0.051)	Loss 7.1092e-01 (5.2639e-01)	Acc@1  75.00 ( 81.81)	Acc@5 100.00 ( 99.00)
Test: [ 30/100]	Time  0.044 ( 0.049)	Loss 5.4269e-01 (5.2474e-01)	Acc@1  83.00 ( 82.16)	Acc@5  98.00 ( 98.94)
Test: [ 40/100]	Time  0.042 ( 0.048)	Loss 5.0239e-01 (5.3111e-01)	Acc@1  81.00 ( 81.95)	Acc@5  98.00 ( 98.90)
Test: [ 50/100]	Time  0.043 ( 0.047)	Loss 4.7016e-01 (5.2655e-01)	Acc@1  84.00 ( 82.22)	Acc@5  99.00 ( 98.90)
Test: [ 60/100]	Time  0.043 ( 0.046)	Loss 5.9538e-01 (5.3173e-01)	Acc@1  84.00 ( 82.15)	Acc@5 100.00 ( 98.89)
Test: [ 70/100]	Time  0.041 ( 0.046)	Loss 7.0155e-01 (5.3329e-01)	Acc@1  81.00 ( 82.18)	Acc@5  98.00 ( 98.90)
Test: [ 80/100]	Time  0.042 ( 0.046)	Loss 3.6647e-01 (5.3358e-01)	Acc@1  87.00 ( 82.30)	Acc@5 100.00 ( 98.90)
Test: [ 90/100]	Time  0.046 ( 0.045)	Loss 3.5260e-01 (5.3086e-01)	Acc@1  84.00 ( 82.27)	Acc@5 100.00 ( 98.96)
 * Acc@1 82.290 Acc@5 98.970
### epoch[15] execution time: 58.51787805557251
EPOCH 16
# current learning rate: 0.1
# Switched to train mode...
Epoch: [16][  0/391]	Time  0.305 ( 0.305)	Data  0.175 ( 0.175)	Loss 3.2813e-01 (3.2813e-01)	Acc@1  87.50 ( 87.50)	Acc@5 100.00 (100.00)
Epoch: [16][ 10/391]	Time  0.136 ( 0.151)	Data  0.001 ( 0.020)	Loss 3.5725e-01 (4.1488e-01)	Acc@1  89.06 ( 85.37)	Acc@5  99.22 ( 99.72)
Epoch: [16][ 20/391]	Time  0.137 ( 0.144)	Data  0.001 ( 0.012)	Loss 5.0383e-01 (4.3527e-01)	Acc@1  80.47 ( 84.64)	Acc@5 100.00 ( 99.74)
Epoch: [16][ 30/391]	Time  0.133 ( 0.141)	Data  0.001 ( 0.010)	Loss 6.3295e-01 (4.4256e-01)	Acc@1  78.91 ( 84.43)	Acc@5  98.44 ( 99.67)
Epoch: [16][ 40/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.008)	Loss 4.1255e-01 (4.4840e-01)	Acc@1  85.94 ( 84.22)	Acc@5 100.00 ( 99.64)
Epoch: [16][ 50/391]	Time  0.130 ( 0.140)	Data  0.001 ( 0.008)	Loss 4.6807e-01 (4.5077e-01)	Acc@1  83.59 ( 84.19)	Acc@5 100.00 ( 99.62)
Epoch: [16][ 60/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.007)	Loss 4.0630e-01 (4.5314e-01)	Acc@1  85.16 ( 84.05)	Acc@5 100.00 ( 99.56)
Epoch: [16][ 70/391]	Time  0.132 ( 0.139)	Data  0.001 ( 0.007)	Loss 5.1274e-01 (4.5614e-01)	Acc@1  83.59 ( 84.03)	Acc@5  98.44 ( 99.50)
Epoch: [16][ 80/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.006)	Loss 3.0687e-01 (4.5401e-01)	Acc@1  86.72 ( 84.10)	Acc@5 100.00 ( 99.45)
Epoch: [16][ 90/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.3299e-01 (4.5297e-01)	Acc@1  85.94 ( 84.15)	Acc@5 100.00 ( 99.42)
Epoch: [16][100/391]	Time  0.133 ( 0.138)	Data  0.002 ( 0.006)	Loss 4.9396e-01 (4.5972e-01)	Acc@1  80.47 ( 83.77)	Acc@5  99.22 ( 99.41)
Epoch: [16][110/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.0764e-01 (4.6199e-01)	Acc@1  88.28 ( 83.71)	Acc@5  99.22 ( 99.40)
Epoch: [16][120/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.006)	Loss 5.4028e-01 (4.6616e-01)	Acc@1  78.91 ( 83.62)	Acc@5  98.44 ( 99.35)
Epoch: [16][130/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.1532e-01 (4.6711e-01)	Acc@1  83.59 ( 83.55)	Acc@5 100.00 ( 99.34)
Epoch: [16][140/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.9495e-01 (4.6514e-01)	Acc@1  84.38 ( 83.65)	Acc@5 100.00 ( 99.38)
Epoch: [16][150/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.006)	Loss 6.6479e-01 (4.6535e-01)	Acc@1  74.22 ( 83.60)	Acc@5  97.66 ( 99.36)
Epoch: [16][160/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.005)	Loss 4.5103e-01 (4.6513e-01)	Acc@1  84.38 ( 83.63)	Acc@5 100.00 ( 99.39)
Epoch: [16][170/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.005)	Loss 4.2012e-01 (4.6573e-01)	Acc@1  87.50 ( 83.66)	Acc@5 100.00 ( 99.36)
Epoch: [16][180/391]	Time  0.141 ( 0.137)	Data  0.002 ( 0.005)	Loss 4.8611e-01 (4.6431e-01)	Acc@1  82.81 ( 83.81)	Acc@5 100.00 ( 99.36)
Epoch: [16][190/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.3567e-01 (4.6349e-01)	Acc@1  85.94 ( 83.87)	Acc@5 100.00 ( 99.35)
Epoch: [16][200/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.2364e-01 (4.6155e-01)	Acc@1  89.06 ( 83.94)	Acc@5 100.00 ( 99.37)
Epoch: [16][210/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.7341e-01 (4.6122e-01)	Acc@1  79.69 ( 83.98)	Acc@5 100.00 ( 99.39)
Epoch: [16][220/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.2727e-01 (4.6025e-01)	Acc@1  86.72 ( 84.05)	Acc@5 100.00 ( 99.37)
Epoch: [16][230/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.1289e-01 (4.6037e-01)	Acc@1  87.50 ( 84.07)	Acc@5  98.44 ( 99.37)
Epoch: [16][240/391]	Time  0.134 ( 0.137)	Data  0.002 ( 0.005)	Loss 5.0957e-01 (4.5951e-01)	Acc@1  84.38 ( 84.13)	Acc@5  97.66 ( 99.36)
Epoch: [16][250/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.5135e-01 (4.5896e-01)	Acc@1  81.25 ( 84.12)	Acc@5  99.22 ( 99.36)
Epoch: [16][260/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.6158e-01 (4.5778e-01)	Acc@1  84.38 ( 84.16)	Acc@5 100.00 ( 99.36)
Epoch: [16][270/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.9174e-01 (4.5613e-01)	Acc@1  81.25 ( 84.20)	Acc@5  98.44 ( 99.37)
Epoch: [16][280/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.4147e-01 (4.5584e-01)	Acc@1  87.50 ( 84.18)	Acc@5  99.22 ( 99.36)
Epoch: [16][290/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.9671e-01 (4.5622e-01)	Acc@1  85.94 ( 84.19)	Acc@5 100.00 ( 99.35)
Epoch: [16][300/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.6028e-01 (4.5457e-01)	Acc@1  90.62 ( 84.27)	Acc@5 100.00 ( 99.36)
Epoch: [16][310/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.3459e-01 (4.5507e-01)	Acc@1  86.72 ( 84.27)	Acc@5 100.00 ( 99.37)
Epoch: [16][320/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.8363e-01 (4.5619e-01)	Acc@1  80.47 ( 84.23)	Acc@5  97.66 ( 99.36)
Epoch: [16][330/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.5255e-01 (4.5638e-01)	Acc@1  89.06 ( 84.26)	Acc@5  98.44 ( 99.36)
Epoch: [16][340/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.5933e-01 (4.5699e-01)	Acc@1  78.91 ( 84.24)	Acc@5  98.44 ( 99.35)
Epoch: [16][350/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.4045e-01 (4.5589e-01)	Acc@1  79.69 ( 84.28)	Acc@5 100.00 ( 99.35)
Epoch: [16][360/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.9587e-01 (4.5556e-01)	Acc@1  80.47 ( 84.28)	Acc@5  99.22 ( 99.36)
Epoch: [16][370/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.7279e-01 (4.5612e-01)	Acc@1  88.28 ( 84.26)	Acc@5  99.22 ( 99.36)
Epoch: [16][380/391]	Time  0.134 ( 0.137)	Data  0.002 ( 0.005)	Loss 5.9800e-01 (4.5570e-01)	Acc@1  83.59 ( 84.29)	Acc@5  96.88 ( 99.36)
Epoch: [16][390/391]	Time  0.125 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.8387e-01 (4.5609e-01)	Acc@1  87.50 ( 84.28)	Acc@5  98.75 ( 99.36)
## e[16] optimizer.zero_grad (sum) time: 0.28496408462524414
## e[16]       loss.backward (sum) time: 18.56050419807434
## e[16]      optimizer.step (sum) time: 2.8548896312713623
## epoch[16] training(only) time: 53.82624340057373
# Switched to evaluate mode...
Test: [  0/100]	Time  0.204 ( 0.204)	Loss 4.2966e-01 (4.2966e-01)	Acc@1  86.00 ( 86.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.052 ( 0.059)	Loss 4.5831e-01 (5.0938e-01)	Acc@1  87.00 ( 81.91)	Acc@5 100.00 ( 99.36)
Test: [ 20/100]	Time  0.045 ( 0.052)	Loss 6.5488e-01 (5.3381e-01)	Acc@1  79.00 ( 81.48)	Acc@5  98.00 ( 99.14)
Test: [ 30/100]	Time  0.042 ( 0.049)	Loss 6.3518e-01 (5.4678e-01)	Acc@1  80.00 ( 81.42)	Acc@5  98.00 ( 98.97)
Test: [ 40/100]	Time  0.044 ( 0.048)	Loss 5.6412e-01 (5.5024e-01)	Acc@1  81.00 ( 81.49)	Acc@5  97.00 ( 98.95)
Test: [ 50/100]	Time  0.043 ( 0.047)	Loss 3.9555e-01 (5.4901e-01)	Acc@1  86.00 ( 81.53)	Acc@5  99.00 ( 98.92)
Test: [ 60/100]	Time  0.043 ( 0.046)	Loss 6.1343e-01 (5.4890e-01)	Acc@1  78.00 ( 81.38)	Acc@5 100.00 ( 98.98)
Test: [ 70/100]	Time  0.044 ( 0.046)	Loss 6.3869e-01 (5.4677e-01)	Acc@1  82.00 ( 81.41)	Acc@5 100.00 ( 99.06)
Test: [ 80/100]	Time  0.044 ( 0.046)	Loss 5.7373e-01 (5.4787e-01)	Acc@1  79.00 ( 81.30)	Acc@5 100.00 ( 99.09)
Test: [ 90/100]	Time  0.044 ( 0.045)	Loss 5.0219e-01 (5.5096e-01)	Acc@1  78.00 ( 81.16)	Acc@5 100.00 ( 99.08)
 * Acc@1 81.280 Acc@5 99.070
### epoch[16] execution time: 58.46617937088013
EPOCH 17
# current learning rate: 0.1
# Switched to train mode...
Epoch: [17][  0/391]	Time  0.299 ( 0.299)	Data  0.179 ( 0.179)	Loss 2.4166e-01 (2.4166e-01)	Acc@1  92.19 ( 92.19)	Acc@5 100.00 (100.00)
Epoch: [17][ 10/391]	Time  0.138 ( 0.151)	Data  0.001 ( 0.020)	Loss 4.7436e-01 (4.1107e-01)	Acc@1  85.16 ( 86.01)	Acc@5  99.22 ( 99.72)
Epoch: [17][ 20/391]	Time  0.134 ( 0.144)	Data  0.001 ( 0.013)	Loss 4.3244e-01 (4.1829e-01)	Acc@1  84.38 ( 85.31)	Acc@5  99.22 ( 99.59)
Epoch: [17][ 30/391]	Time  0.135 ( 0.141)	Data  0.001 ( 0.010)	Loss 5.1892e-01 (4.3232e-01)	Acc@1  81.25 ( 85.01)	Acc@5  98.44 ( 99.50)
Epoch: [17][ 40/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.009)	Loss 5.1402e-01 (4.2902e-01)	Acc@1  78.91 ( 85.02)	Acc@5  99.22 ( 99.41)
Epoch: [17][ 50/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.008)	Loss 4.1189e-01 (4.2666e-01)	Acc@1  86.72 ( 85.16)	Acc@5  99.22 ( 99.45)
Epoch: [17][ 60/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.007)	Loss 5.0060e-01 (4.2574e-01)	Acc@1  85.16 ( 85.17)	Acc@5 100.00 ( 99.50)
Epoch: [17][ 70/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.007)	Loss 3.9245e-01 (4.2246e-01)	Acc@1  85.94 ( 85.32)	Acc@5 100.00 ( 99.49)
Epoch: [17][ 80/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.007)	Loss 3.6943e-01 (4.1982e-01)	Acc@1  85.16 ( 85.36)	Acc@5  99.22 ( 99.49)
Epoch: [17][ 90/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 5.0827e-01 (4.2421e-01)	Acc@1  82.81 ( 85.30)	Acc@5  98.44 ( 99.41)
Epoch: [17][100/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.9390e-01 (4.2166e-01)	Acc@1  89.84 ( 85.45)	Acc@5  99.22 ( 99.41)
Epoch: [17][110/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.9404e-01 (4.2031e-01)	Acc@1  85.94 ( 85.50)	Acc@5  99.22 ( 99.42)
Epoch: [17][120/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 5.1168e-01 (4.2255e-01)	Acc@1  83.59 ( 85.51)	Acc@5  99.22 ( 99.43)
Epoch: [17][130/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 5.6779e-01 (4.2109e-01)	Acc@1  83.59 ( 85.53)	Acc@5  98.44 ( 99.45)
Epoch: [17][140/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.3244e-01 (4.1865e-01)	Acc@1  92.97 ( 85.57)	Acc@5 100.00 ( 99.47)
Epoch: [17][150/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.4088e-01 (4.1776e-01)	Acc@1  87.50 ( 85.53)	Acc@5  98.44 ( 99.47)
Epoch: [17][160/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 5.2879e-01 (4.1926e-01)	Acc@1  79.69 ( 85.46)	Acc@5 100.00 ( 99.47)
Epoch: [17][170/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.005)	Loss 4.8383e-01 (4.1913e-01)	Acc@1  83.59 ( 85.44)	Acc@5  99.22 ( 99.47)
Epoch: [17][180/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.005)	Loss 5.9614e-01 (4.2203e-01)	Acc@1  79.69 ( 85.32)	Acc@5  99.22 ( 99.45)
Epoch: [17][190/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.005)	Loss 4.1310e-01 (4.2310e-01)	Acc@1  86.72 ( 85.30)	Acc@5 100.00 ( 99.45)
Epoch: [17][200/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.005)	Loss 4.5739e-01 (4.2453e-01)	Acc@1  88.28 ( 85.30)	Acc@5  99.22 ( 99.45)
Epoch: [17][210/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.005)	Loss 4.4447e-01 (4.2693e-01)	Acc@1  83.59 ( 85.22)	Acc@5 100.00 ( 99.44)
Epoch: [17][220/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.005)	Loss 4.3675e-01 (4.2750e-01)	Acc@1  83.59 ( 85.18)	Acc@5 100.00 ( 99.44)
Epoch: [17][230/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.005)	Loss 4.2779e-01 (4.2889e-01)	Acc@1  87.50 ( 85.16)	Acc@5 100.00 ( 99.44)
Epoch: [17][240/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.5318e-01 (4.2983e-01)	Acc@1  89.06 ( 85.19)	Acc@5  99.22 ( 99.43)
Epoch: [17][250/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.005)	Loss 5.4025e-01 (4.2870e-01)	Acc@1  79.69 ( 85.22)	Acc@5  99.22 ( 99.43)
Epoch: [17][260/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.005)	Loss 4.8930e-01 (4.2909e-01)	Acc@1  83.59 ( 85.17)	Acc@5  98.44 ( 99.44)
Epoch: [17][270/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.8043e-01 (4.2801e-01)	Acc@1  88.28 ( 85.21)	Acc@5  98.44 ( 99.44)
Epoch: [17][280/391]	Time  0.128 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.8120e-01 (4.2571e-01)	Acc@1  86.72 ( 85.31)	Acc@5 100.00 ( 99.44)
Epoch: [17][290/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.0101e-01 (4.2546e-01)	Acc@1  84.38 ( 85.29)	Acc@5 100.00 ( 99.43)
Epoch: [17][300/391]	Time  0.141 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.8526e-01 (4.2445e-01)	Acc@1  90.62 ( 85.31)	Acc@5 100.00 ( 99.44)
Epoch: [17][310/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.6596e-01 (4.2592e-01)	Acc@1  82.03 ( 85.26)	Acc@5  99.22 ( 99.43)
Epoch: [17][320/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.9578e-01 (4.2718e-01)	Acc@1  81.25 ( 85.21)	Acc@5  99.22 ( 99.43)
Epoch: [17][330/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.8038e-01 (4.2767e-01)	Acc@1  87.50 ( 85.19)	Acc@5  99.22 ( 99.42)
Epoch: [17][340/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.6132e-01 (4.2746e-01)	Acc@1  89.84 ( 85.22)	Acc@5 100.00 ( 99.42)
Epoch: [17][350/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.7757e-01 (4.2661e-01)	Acc@1  82.81 ( 85.23)	Acc@5  98.44 ( 99.42)
Epoch: [17][360/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.9070e-01 (4.2627e-01)	Acc@1  87.50 ( 85.27)	Acc@5  99.22 ( 99.42)
Epoch: [17][370/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.1568e-01 (4.2700e-01)	Acc@1  85.16 ( 85.22)	Acc@5  99.22 ( 99.42)
Epoch: [17][380/391]	Time  0.135 ( 0.137)	Data  0.002 ( 0.005)	Loss 3.3684e-01 (4.2758e-01)	Acc@1  88.28 ( 85.17)	Acc@5  99.22 ( 99.42)
Epoch: [17][390/391]	Time  0.127 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.6868e-01 (4.2722e-01)	Acc@1  83.75 ( 85.19)	Acc@5  98.75 ( 99.42)
## e[17] optimizer.zero_grad (sum) time: 0.2857704162597656
## e[17]       loss.backward (sum) time: 18.520589590072632
## e[17]      optimizer.step (sum) time: 2.853656768798828
## epoch[17] training(only) time: 53.859092235565186
# Switched to evaluate mode...
Test: [  0/100]	Time  0.199 ( 0.199)	Loss 3.9411e-01 (3.9411e-01)	Acc@1  87.00 ( 87.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.044 ( 0.059)	Loss 3.7537e-01 (4.5264e-01)	Acc@1  85.00 ( 83.73)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.043 ( 0.051)	Loss 5.8776e-01 (4.8109e-01)	Acc@1  78.00 ( 83.19)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.041 ( 0.049)	Loss 4.9725e-01 (4.8130e-01)	Acc@1  88.00 ( 83.81)	Acc@5  99.00 ( 99.52)
Test: [ 40/100]	Time  0.044 ( 0.048)	Loss 5.2806e-01 (4.8243e-01)	Acc@1  82.00 ( 83.76)	Acc@5 100.00 ( 99.44)
Test: [ 50/100]	Time  0.044 ( 0.047)	Loss 4.4102e-01 (4.8131e-01)	Acc@1  83.00 ( 83.92)	Acc@5  99.00 ( 99.39)
Test: [ 60/100]	Time  0.044 ( 0.046)	Loss 4.3104e-01 (4.8817e-01)	Acc@1  86.00 ( 83.61)	Acc@5 100.00 ( 99.33)
Test: [ 70/100]	Time  0.043 ( 0.046)	Loss 4.6021e-01 (4.8884e-01)	Acc@1  82.00 ( 83.52)	Acc@5  99.00 ( 99.31)
Test: [ 80/100]	Time  0.041 ( 0.046)	Loss 3.9440e-01 (4.8707e-01)	Acc@1  88.00 ( 83.52)	Acc@5 100.00 ( 99.32)
Test: [ 90/100]	Time  0.045 ( 0.045)	Loss 3.0345e-01 (4.8767e-01)	Acc@1  88.00 ( 83.42)	Acc@5 100.00 ( 99.36)
 * Acc@1 83.480 Acc@5 99.390
### epoch[17] execution time: 58.496699810028076
EPOCH 18
# current learning rate: 0.1
# Switched to train mode...
Epoch: [18][  0/391]	Time  0.287 ( 0.287)	Data  0.174 ( 0.174)	Loss 3.7089e-01 (3.7089e-01)	Acc@1  87.50 ( 87.50)	Acc@5  99.22 ( 99.22)
Epoch: [18][ 10/391]	Time  0.136 ( 0.150)	Data  0.001 ( 0.020)	Loss 4.6471e-01 (3.7288e-01)	Acc@1  81.25 ( 86.65)	Acc@5  97.66 ( 99.57)
Epoch: [18][ 20/391]	Time  0.132 ( 0.143)	Data  0.001 ( 0.012)	Loss 3.7368e-01 (3.8523e-01)	Acc@1  83.59 ( 86.01)	Acc@5 100.00 ( 99.52)
Epoch: [18][ 30/391]	Time  0.131 ( 0.141)	Data  0.001 ( 0.010)	Loss 4.9354e-01 (3.8464e-01)	Acc@1  81.25 ( 86.04)	Acc@5  98.44 ( 99.52)
Epoch: [18][ 40/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.009)	Loss 3.7518e-01 (3.8514e-01)	Acc@1  87.50 ( 86.05)	Acc@5 100.00 ( 99.49)
Epoch: [18][ 50/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.008)	Loss 2.3776e-01 (3.8279e-01)	Acc@1  92.19 ( 86.35)	Acc@5 100.00 ( 99.54)
Epoch: [18][ 60/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.007)	Loss 3.0397e-01 (3.8494e-01)	Acc@1  89.06 ( 86.28)	Acc@5  99.22 ( 99.55)
Epoch: [18][ 70/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.007)	Loss 4.9254e-01 (3.9346e-01)	Acc@1  83.59 ( 86.07)	Acc@5  99.22 ( 99.56)
Epoch: [18][ 80/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.006)	Loss 5.9923e-01 (3.9428e-01)	Acc@1  78.91 ( 86.07)	Acc@5 100.00 ( 99.57)
Epoch: [18][ 90/391]	Time  0.130 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.9039e-01 (3.9418e-01)	Acc@1  87.50 ( 86.15)	Acc@5 100.00 ( 99.58)
Epoch: [18][100/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.3019e-01 (3.9189e-01)	Acc@1  89.06 ( 86.27)	Acc@5 100.00 ( 99.56)
Epoch: [18][110/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.9175e-01 (3.8927e-01)	Acc@1  84.38 ( 86.40)	Acc@5 100.00 ( 99.56)
Epoch: [18][120/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.8513e-01 (3.9218e-01)	Acc@1  91.41 ( 86.34)	Acc@5 100.00 ( 99.58)
Epoch: [18][130/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.8978e-01 (3.9443e-01)	Acc@1  87.50 ( 86.22)	Acc@5 100.00 ( 99.56)
Epoch: [18][140/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.4838e-01 (3.9498e-01)	Acc@1  85.16 ( 86.13)	Acc@5 100.00 ( 99.57)
Epoch: [18][150/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.4023e-01 (3.9572e-01)	Acc@1  85.16 ( 86.07)	Acc@5  99.22 ( 99.57)
Epoch: [18][160/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.005)	Loss 5.6022e-01 (3.9725e-01)	Acc@1  79.69 ( 86.03)	Acc@5  98.44 ( 99.55)
Epoch: [18][170/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.005)	Loss 4.9863e-01 (3.9721e-01)	Acc@1  80.47 ( 86.02)	Acc@5  99.22 ( 99.56)
Epoch: [18][180/391]	Time  0.131 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.7781e-01 (3.9858e-01)	Acc@1  85.94 ( 85.96)	Acc@5 100.00 ( 99.56)
Epoch: [18][190/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.6899e-01 (3.9808e-01)	Acc@1  86.72 ( 85.99)	Acc@5 100.00 ( 99.57)
Epoch: [18][200/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.6601e-01 (3.9673e-01)	Acc@1  91.41 ( 86.03)	Acc@5 100.00 ( 99.59)
Epoch: [18][210/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.005)	Loss 4.0840e-01 (3.9665e-01)	Acc@1  83.59 ( 86.02)	Acc@5  99.22 ( 99.58)
Epoch: [18][220/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.005)	Loss 4.8732e-01 (3.9636e-01)	Acc@1  84.38 ( 86.05)	Acc@5  97.66 ( 99.57)
Epoch: [18][230/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.4139e-01 (3.9602e-01)	Acc@1  89.06 ( 86.10)	Acc@5 100.00 ( 99.56)
Epoch: [18][240/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.005)	Loss 5.1905e-01 (3.9688e-01)	Acc@1  81.25 ( 86.12)	Acc@5  99.22 ( 99.55)
Epoch: [18][250/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.9932e-01 (3.9525e-01)	Acc@1  84.38 ( 86.18)	Acc@5  99.22 ( 99.56)
Epoch: [18][260/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.0192e-01 (3.9355e-01)	Acc@1  90.62 ( 86.20)	Acc@5  99.22 ( 99.56)
Epoch: [18][270/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.4008e-01 (3.9285e-01)	Acc@1  85.94 ( 86.21)	Acc@5 100.00 ( 99.56)
Epoch: [18][280/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.005)	Loss 4.6231e-01 (3.9409e-01)	Acc@1  79.69 ( 86.19)	Acc@5  99.22 ( 99.56)
Epoch: [18][290/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.9363e-01 (3.9649e-01)	Acc@1  89.84 ( 86.11)	Acc@5 100.00 ( 99.55)
Epoch: [18][300/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.8575e-01 (3.9713e-01)	Acc@1  85.16 ( 86.11)	Acc@5  97.66 ( 99.54)
Epoch: [18][310/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.2613e-01 (3.9558e-01)	Acc@1  85.16 ( 86.21)	Acc@5 100.00 ( 99.54)
Epoch: [18][320/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.0793e-01 (3.9551e-01)	Acc@1  86.72 ( 86.24)	Acc@5  98.44 ( 99.53)
Epoch: [18][330/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.6319e-01 (3.9684e-01)	Acc@1  77.34 ( 86.21)	Acc@5 100.00 ( 99.53)
Epoch: [18][340/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.0377e-01 (3.9653e-01)	Acc@1  89.06 ( 86.23)	Acc@5 100.00 ( 99.54)
Epoch: [18][350/391]	Time  0.142 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.0002e-01 (3.9708e-01)	Acc@1  86.72 ( 86.23)	Acc@5 100.00 ( 99.54)
Epoch: [18][360/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.7760e-01 (4.0038e-01)	Acc@1  87.50 ( 86.13)	Acc@5 100.00 ( 99.53)
Epoch: [18][370/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.8049e-01 (4.0142e-01)	Acc@1  81.25 ( 86.12)	Acc@5  98.44 ( 99.52)
Epoch: [18][380/391]	Time  0.137 ( 0.137)	Data  0.002 ( 0.005)	Loss 3.2829e-01 (4.0083e-01)	Acc@1  89.06 ( 86.13)	Acc@5  99.22 ( 99.52)
Epoch: [18][390/391]	Time  0.128 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.5537e-01 (4.0021e-01)	Acc@1  90.00 ( 86.17)	Acc@5 100.00 ( 99.52)
## e[18] optimizer.zero_grad (sum) time: 0.2867867946624756
## e[18]       loss.backward (sum) time: 18.5639431476593
## e[18]      optimizer.step (sum) time: 2.8848876953125
## epoch[18] training(only) time: 53.92360591888428
# Switched to evaluate mode...
Test: [  0/100]	Time  0.198 ( 0.198)	Loss 4.4976e-01 (4.4976e-01)	Acc@1  83.00 ( 83.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.050 ( 0.058)	Loss 4.9370e-01 (4.3104e-01)	Acc@1  84.00 ( 85.45)	Acc@5  99.00 ( 99.45)
Test: [ 20/100]	Time  0.043 ( 0.051)	Loss 5.1586e-01 (4.6202e-01)	Acc@1  84.00 ( 84.57)	Acc@5  99.00 ( 99.14)
Test: [ 30/100]	Time  0.043 ( 0.049)	Loss 4.9065e-01 (4.7670e-01)	Acc@1  84.00 ( 84.61)	Acc@5  98.00 ( 98.90)
Test: [ 40/100]	Time  0.045 ( 0.047)	Loss 4.2857e-01 (4.7854e-01)	Acc@1  86.00 ( 84.32)	Acc@5  99.00 ( 99.02)
Test: [ 50/100]	Time  0.043 ( 0.047)	Loss 4.3433e-01 (4.6886e-01)	Acc@1  86.00 ( 84.76)	Acc@5  98.00 ( 99.10)
Test: [ 60/100]	Time  0.043 ( 0.046)	Loss 4.3644e-01 (4.6929e-01)	Acc@1  85.00 ( 84.59)	Acc@5 100.00 ( 99.15)
Test: [ 70/100]	Time  0.041 ( 0.045)	Loss 5.2153e-01 (4.6754e-01)	Acc@1  85.00 ( 84.76)	Acc@5  99.00 ( 99.18)
Test: [ 80/100]	Time  0.044 ( 0.045)	Loss 4.1687e-01 (4.6774e-01)	Acc@1  85.00 ( 84.79)	Acc@5  99.00 ( 99.22)
Test: [ 90/100]	Time  0.045 ( 0.045)	Loss 4.3705e-01 (4.6930e-01)	Acc@1  86.00 ( 84.75)	Acc@5 100.00 ( 99.23)
 * Acc@1 84.630 Acc@5 99.290
### epoch[18] execution time: 58.51975989341736
EPOCH 19
# current learning rate: 0.1
# Switched to train mode...
Epoch: [19][  0/391]	Time  0.279 ( 0.279)	Data  0.167 ( 0.167)	Loss 2.3873e-01 (2.3873e-01)	Acc@1  92.19 ( 92.19)	Acc@5 100.00 (100.00)
Epoch: [19][ 10/391]	Time  0.131 ( 0.148)	Data  0.001 ( 0.019)	Loss 3.2360e-01 (3.3089e-01)	Acc@1  87.50 ( 88.57)	Acc@5 100.00 ( 99.79)
Epoch: [19][ 20/391]	Time  0.137 ( 0.142)	Data  0.001 ( 0.012)	Loss 3.9987e-01 (3.4537e-01)	Acc@1  86.72 ( 88.02)	Acc@5  99.22 ( 99.78)
Epoch: [19][ 30/391]	Time  0.133 ( 0.140)	Data  0.001 ( 0.010)	Loss 4.1510e-01 (3.4692e-01)	Acc@1  86.72 ( 87.78)	Acc@5  99.22 ( 99.77)
Epoch: [19][ 40/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.008)	Loss 4.3794e-01 (3.5706e-01)	Acc@1  85.16 ( 87.52)	Acc@5  98.44 ( 99.73)
Epoch: [19][ 50/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.008)	Loss 5.2409e-01 (3.6280e-01)	Acc@1  83.59 ( 87.39)	Acc@5  99.22 ( 99.69)
Epoch: [19][ 60/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.007)	Loss 4.6036e-01 (3.6639e-01)	Acc@1  83.59 ( 87.31)	Acc@5  99.22 ( 99.68)
Epoch: [19][ 70/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.007)	Loss 5.3189e-01 (3.7052e-01)	Acc@1  85.16 ( 87.42)	Acc@5  98.44 ( 99.61)
Epoch: [19][ 80/391]	Time  0.128 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.8444e-01 (3.6994e-01)	Acc@1  85.16 ( 87.39)	Acc@5 100.00 ( 99.63)
Epoch: [19][ 90/391]	Time  0.133 ( 0.138)	Data  0.002 ( 0.006)	Loss 3.1745e-01 (3.6803e-01)	Acc@1  90.62 ( 87.48)	Acc@5  98.44 ( 99.61)
Epoch: [19][100/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.1486e-01 (3.6795e-01)	Acc@1  84.38 ( 87.44)	Acc@5  99.22 ( 99.60)
Epoch: [19][110/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.5869e-01 (3.7085e-01)	Acc@1  84.38 ( 87.32)	Acc@5  99.22 ( 99.58)
Epoch: [19][120/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.1231e-01 (3.7218e-01)	Acc@1  88.28 ( 87.29)	Acc@5  99.22 ( 99.59)
Epoch: [19][130/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.006)	Loss 3.5963e-01 (3.7478e-01)	Acc@1  86.72 ( 87.15)	Acc@5 100.00 ( 99.61)
Epoch: [19][140/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.006)	Loss 4.7953e-01 (3.7625e-01)	Acc@1  83.59 ( 87.13)	Acc@5  99.22 ( 99.59)
Epoch: [19][150/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.006)	Loss 3.4431e-01 (3.7730e-01)	Acc@1  87.50 ( 87.11)	Acc@5 100.00 ( 99.58)
Epoch: [19][160/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.8608e-01 (3.7709e-01)	Acc@1  82.81 ( 87.07)	Acc@5  98.44 ( 99.56)
Epoch: [19][170/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.8752e-01 (3.7780e-01)	Acc@1  78.12 ( 86.98)	Acc@5 100.00 ( 99.55)
Epoch: [19][180/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.4395e-01 (3.7890e-01)	Acc@1  78.12 ( 86.96)	Acc@5  99.22 ( 99.56)
Epoch: [19][190/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.3072e-01 (3.7903e-01)	Acc@1  82.81 ( 86.95)	Acc@5 100.00 ( 99.56)
Epoch: [19][200/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.0470e-01 (3.8089e-01)	Acc@1  83.59 ( 86.88)	Acc@5 100.00 ( 99.57)
Epoch: [19][210/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.7019e-01 (3.8114e-01)	Acc@1  87.50 ( 86.92)	Acc@5  98.44 ( 99.56)
Epoch: [19][220/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.9831e-01 (3.8130e-01)	Acc@1  87.50 ( 86.89)	Acc@5 100.00 ( 99.54)
Epoch: [19][230/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.7752e-01 (3.8228e-01)	Acc@1  86.72 ( 86.87)	Acc@5  99.22 ( 99.53)
Epoch: [19][240/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.4476e-01 (3.8084e-01)	Acc@1  87.50 ( 86.94)	Acc@5 100.00 ( 99.54)
Epoch: [19][250/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.4865e-01 (3.8265e-01)	Acc@1  82.03 ( 86.85)	Acc@5 100.00 ( 99.54)
Epoch: [19][260/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.3463e-01 (3.8142e-01)	Acc@1  89.06 ( 86.90)	Acc@5 100.00 ( 99.55)
Epoch: [19][270/391]	Time  0.129 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.4221e-01 (3.8132e-01)	Acc@1  90.62 ( 86.86)	Acc@5 100.00 ( 99.54)
Epoch: [19][280/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.8781e-01 (3.8185e-01)	Acc@1  82.03 ( 86.82)	Acc@5 100.00 ( 99.54)
Epoch: [19][290/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.2862e-01 (3.8219e-01)	Acc@1  85.94 ( 86.79)	Acc@5 100.00 ( 99.54)
Epoch: [19][300/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.5494e-01 (3.8101e-01)	Acc@1  86.72 ( 86.83)	Acc@5  99.22 ( 99.55)
Epoch: [19][310/391]	Time  0.130 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.7021e-01 (3.8230e-01)	Acc@1  93.75 ( 86.81)	Acc@5 100.00 ( 99.54)
Epoch: [19][320/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.4642e-01 (3.8080e-01)	Acc@1  87.50 ( 86.85)	Acc@5 100.00 ( 99.54)
Epoch: [19][330/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.7107e-01 (3.8055e-01)	Acc@1  85.94 ( 86.87)	Acc@5  97.66 ( 99.54)
Epoch: [19][340/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.6968e-01 (3.8007e-01)	Acc@1  87.50 ( 86.89)	Acc@5  99.22 ( 99.54)
Epoch: [19][350/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.5675e-01 (3.8043e-01)	Acc@1  88.28 ( 86.88)	Acc@5 100.00 ( 99.54)
Epoch: [19][360/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.7577e-01 (3.8053e-01)	Acc@1  80.47 ( 86.87)	Acc@5  99.22 ( 99.54)
Epoch: [19][370/391]	Time  0.134 ( 0.137)	Data  0.002 ( 0.005)	Loss 4.9518e-01 (3.8150e-01)	Acc@1  85.94 ( 86.85)	Acc@5  97.66 ( 99.54)
Epoch: [19][380/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.0274e-01 (3.8204e-01)	Acc@1  85.16 ( 86.82)	Acc@5  99.22 ( 99.54)
Epoch: [19][390/391]	Time  0.126 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.1359e-01 (3.8232e-01)	Acc@1  86.25 ( 86.80)	Acc@5 100.00 ( 99.54)
## e[19] optimizer.zero_grad (sum) time: 0.28624796867370605
## e[19]       loss.backward (sum) time: 18.584169149398804
## e[19]      optimizer.step (sum) time: 2.871722936630249
## epoch[19] training(only) time: 53.69585633277893
# Switched to evaluate mode...
Test: [  0/100]	Time  0.197 ( 0.197)	Loss 3.3271e-01 (3.3271e-01)	Acc@1  90.00 ( 90.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.046 ( 0.058)	Loss 4.8777e-01 (4.4473e-01)	Acc@1  86.00 ( 85.18)	Acc@5 100.00 ( 99.55)
Test: [ 20/100]	Time  0.044 ( 0.051)	Loss 5.6155e-01 (4.6389e-01)	Acc@1  81.00 ( 83.62)	Acc@5  99.00 ( 99.48)
Test: [ 30/100]	Time  0.043 ( 0.049)	Loss 4.3255e-01 (4.6858e-01)	Acc@1  81.00 ( 83.65)	Acc@5  99.00 ( 99.32)
Test: [ 40/100]	Time  0.043 ( 0.048)	Loss 5.7789e-01 (4.7615e-01)	Acc@1  80.00 ( 83.61)	Acc@5  98.00 ( 99.22)
Test: [ 50/100]	Time  0.045 ( 0.047)	Loss 3.9361e-01 (4.9063e-01)	Acc@1  89.00 ( 84.06)	Acc@5 100.00 ( 99.25)
Test: [ 60/100]	Time  0.042 ( 0.046)	Loss 5.2706e-01 (4.8921e-01)	Acc@1  80.00 ( 83.87)	Acc@5 100.00 ( 99.33)
Test: [ 70/100]	Time  0.042 ( 0.046)	Loss 4.8102e-01 (4.8364e-01)	Acc@1  86.00 ( 84.04)	Acc@5 100.00 ( 99.37)
Test: [ 80/100]	Time  0.043 ( 0.045)	Loss 4.7985e-01 (4.8070e-01)	Acc@1  82.00 ( 83.98)	Acc@5  99.00 ( 99.41)
Test: [ 90/100]	Time  0.043 ( 0.045)	Loss 4.0519e-01 (4.8250e-01)	Acc@1  82.00 ( 83.85)	Acc@5 100.00 ( 99.41)
 * Acc@1 83.990 Acc@5 99.400
### epoch[19] execution time: 58.350082874298096
EPOCH 20
# current learning rate: 0.1
# Switched to train mode...
Epoch: [20][  0/391]	Time  0.298 ( 0.298)	Data  0.182 ( 0.182)	Loss 2.5822e-01 (2.5822e-01)	Acc@1  89.06 ( 89.06)	Acc@5 100.00 (100.00)
Epoch: [20][ 10/391]	Time  0.125 ( 0.149)	Data  0.001 ( 0.020)	Loss 2.9967e-01 (3.4129e-01)	Acc@1  89.84 ( 87.29)	Acc@5 100.00 ( 99.93)
Epoch: [20][ 20/391]	Time  0.134 ( 0.143)	Data  0.001 ( 0.013)	Loss 4.6451e-01 (3.5306e-01)	Acc@1  83.59 ( 86.83)	Acc@5  98.44 ( 99.70)
Epoch: [20][ 30/391]	Time  0.134 ( 0.141)	Data  0.001 ( 0.010)	Loss 4.4173e-01 (3.4799e-01)	Acc@1  85.94 ( 87.47)	Acc@5  98.44 ( 99.70)
Epoch: [20][ 40/391]	Time  0.138 ( 0.140)	Data  0.002 ( 0.009)	Loss 4.2754e-01 (3.4804e-01)	Acc@1  85.16 ( 87.67)	Acc@5 100.00 ( 99.62)
Epoch: [20][ 50/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.008)	Loss 2.5729e-01 (3.5068e-01)	Acc@1  93.75 ( 87.62)	Acc@5 100.00 ( 99.60)
Epoch: [20][ 60/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.007)	Loss 2.8680e-01 (3.5663e-01)	Acc@1  91.41 ( 87.53)	Acc@5 100.00 ( 99.62)
Epoch: [20][ 70/391]	Time  0.137 ( 0.139)	Data  0.002 ( 0.007)	Loss 3.0765e-01 (3.5017e-01)	Acc@1  89.84 ( 87.76)	Acc@5  99.22 ( 99.60)
Epoch: [20][ 80/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.007)	Loss 3.5335e-01 (3.4586e-01)	Acc@1  88.28 ( 87.72)	Acc@5  99.22 ( 99.62)
Epoch: [20][ 90/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.2469e-01 (3.4465e-01)	Acc@1  85.16 ( 87.82)	Acc@5 100.00 ( 99.61)
Epoch: [20][100/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.6387e-01 (3.4616e-01)	Acc@1  86.72 ( 87.80)	Acc@5 100.00 ( 99.61)
Epoch: [20][110/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.9685e-01 (3.5164e-01)	Acc@1  79.69 ( 87.69)	Acc@5 100.00 ( 99.61)
Epoch: [20][120/391]	Time  0.130 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.8035e-01 (3.5279e-01)	Acc@1  86.72 ( 87.57)	Acc@5 100.00 ( 99.63)
Epoch: [20][130/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.9040e-01 (3.5257e-01)	Acc@1  92.19 ( 87.58)	Acc@5 100.00 ( 99.65)
Epoch: [20][140/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.6869e-01 (3.5254e-01)	Acc@1  86.72 ( 87.55)	Acc@5 100.00 ( 99.65)
Epoch: [20][150/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.0196e-01 (3.5265e-01)	Acc@1  88.28 ( 87.59)	Acc@5 100.00 ( 99.66)
Epoch: [20][160/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.4876e-01 (3.5397e-01)	Acc@1  83.59 ( 87.59)	Acc@5  99.22 ( 99.65)
Epoch: [20][170/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.005)	Loss 4.3921e-01 (3.5734e-01)	Acc@1  86.72 ( 87.48)	Acc@5  99.22 ( 99.64)
Epoch: [20][180/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.9160e-01 (3.5793e-01)	Acc@1  88.28 ( 87.44)	Acc@5  99.22 ( 99.65)
Epoch: [20][190/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.005)	Loss 4.0573e-01 (3.5929e-01)	Acc@1  86.72 ( 87.39)	Acc@5  99.22 ( 99.63)
Epoch: [20][200/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.005)	Loss 4.3172e-01 (3.5941e-01)	Acc@1  85.94 ( 87.42)	Acc@5 100.00 ( 99.64)
Epoch: [20][210/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.7668e-01 (3.6213e-01)	Acc@1  86.72 ( 87.33)	Acc@5 100.00 ( 99.64)
Epoch: [20][220/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.005)	Loss 4.4935e-01 (3.6411e-01)	Acc@1  87.50 ( 87.25)	Acc@5  99.22 ( 99.64)
Epoch: [20][230/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 4.5989e-01 (3.6596e-01)	Acc@1  82.03 ( 87.21)	Acc@5  99.22 ( 99.63)
Epoch: [20][240/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.7320e-01 (3.6793e-01)	Acc@1  79.69 ( 87.07)	Acc@5 100.00 ( 99.63)
Epoch: [20][250/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.0447e-01 (3.6809e-01)	Acc@1  82.81 ( 87.06)	Acc@5  99.22 ( 99.63)
Epoch: [20][260/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.0459e-01 (3.6841e-01)	Acc@1  83.59 ( 87.02)	Acc@5  99.22 ( 99.63)
Epoch: [20][270/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.4408e-01 (3.6708e-01)	Acc@1  82.03 ( 87.04)	Acc@5  98.44 ( 99.63)
Epoch: [20][280/391]	Time  0.139 ( 0.137)	Data  0.002 ( 0.005)	Loss 3.7171e-01 (3.6624e-01)	Acc@1  86.72 ( 87.10)	Acc@5  99.22 ( 99.63)
Epoch: [20][290/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.9281e-01 (3.6748e-01)	Acc@1  83.59 ( 87.04)	Acc@5  98.44 ( 99.62)
Epoch: [20][300/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.5476e-01 (3.6736e-01)	Acc@1  84.38 ( 87.06)	Acc@5  99.22 ( 99.62)
Epoch: [20][310/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.9986e-01 (3.6681e-01)	Acc@1  87.50 ( 87.09)	Acc@5  97.66 ( 99.62)
Epoch: [20][320/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.8321e-01 (3.6666e-01)	Acc@1  87.50 ( 87.12)	Acc@5  98.44 ( 99.62)
Epoch: [20][330/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.3193e-01 (3.6623e-01)	Acc@1  87.50 ( 87.15)	Acc@5  99.22 ( 99.61)
Epoch: [20][340/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.6020e-01 (3.6702e-01)	Acc@1  82.03 ( 87.13)	Acc@5  99.22 ( 99.61)
Epoch: [20][350/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.5219e-01 (3.6795e-01)	Acc@1  89.06 ( 87.13)	Acc@5 100.00 ( 99.60)
Epoch: [20][360/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.7337e-01 (3.6790e-01)	Acc@1  82.81 ( 87.16)	Acc@5  99.22 ( 99.61)
Epoch: [20][370/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.6427e-01 (3.6742e-01)	Acc@1  82.03 ( 87.17)	Acc@5  99.22 ( 99.61)
Epoch: [20][380/391]	Time  0.135 ( 0.137)	Data  0.002 ( 0.005)	Loss 3.2041e-01 (3.6713e-01)	Acc@1  89.84 ( 87.17)	Acc@5  99.22 ( 99.61)
Epoch: [20][390/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.1697e-01 (3.6732e-01)	Acc@1  87.50 ( 87.18)	Acc@5 100.00 ( 99.61)
## e[20] optimizer.zero_grad (sum) time: 0.2874729633331299
## e[20]       loss.backward (sum) time: 18.554399967193604
## e[20]      optimizer.step (sum) time: 2.9135079383850098
## epoch[20] training(only) time: 53.889018297195435
# Switched to evaluate mode...
Test: [  0/100]	Time  0.204 ( 0.204)	Loss 3.2200e-01 (3.2200e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.042 ( 0.058)	Loss 4.4090e-01 (4.1903e-01)	Acc@1  87.00 ( 86.73)	Acc@5  99.00 ( 99.45)
Test: [ 20/100]	Time  0.042 ( 0.051)	Loss 5.3084e-01 (4.4928e-01)	Acc@1  79.00 ( 85.14)	Acc@5 100.00 ( 99.48)
Test: [ 30/100]	Time  0.041 ( 0.048)	Loss 4.9477e-01 (4.5583e-01)	Acc@1  80.00 ( 85.03)	Acc@5  99.00 ( 99.45)
Test: [ 40/100]	Time  0.042 ( 0.047)	Loss 5.0866e-01 (4.6538e-01)	Acc@1  83.00 ( 84.88)	Acc@5  98.00 ( 99.37)
Test: [ 50/100]	Time  0.045 ( 0.046)	Loss 4.7191e-01 (4.6647e-01)	Acc@1  83.00 ( 84.88)	Acc@5  98.00 ( 99.33)
Test: [ 60/100]	Time  0.046 ( 0.046)	Loss 4.7452e-01 (4.7182e-01)	Acc@1  84.00 ( 84.66)	Acc@5 100.00 ( 99.31)
Test: [ 70/100]	Time  0.045 ( 0.045)	Loss 6.6566e-01 (4.7295e-01)	Acc@1  84.00 ( 84.68)	Acc@5  98.00 ( 99.25)
Test: [ 80/100]	Time  0.045 ( 0.045)	Loss 4.9402e-01 (4.7044e-01)	Acc@1  85.00 ( 84.69)	Acc@5  99.00 ( 99.27)
Test: [ 90/100]	Time  0.044 ( 0.045)	Loss 3.9180e-01 (4.6587e-01)	Acc@1  86.00 ( 84.73)	Acc@5 100.00 ( 99.31)
 * Acc@1 84.930 Acc@5 99.330
### epoch[20] execution time: 58.478617429733276
EPOCH 21
# current learning rate: 0.1
# Switched to train mode...
Epoch: [21][  0/391]	Time  0.302 ( 0.302)	Data  0.179 ( 0.179)	Loss 4.0292e-01 (4.0292e-01)	Acc@1  85.94 ( 85.94)	Acc@5 100.00 (100.00)
Epoch: [21][ 10/391]	Time  0.136 ( 0.152)	Data  0.001 ( 0.020)	Loss 3.1491e-01 (3.3941e-01)	Acc@1  85.94 ( 89.06)	Acc@5 100.00 ( 99.50)
Epoch: [21][ 20/391]	Time  0.131 ( 0.144)	Data  0.001 ( 0.012)	Loss 2.4001e-01 (3.3878e-01)	Acc@1  93.75 ( 89.03)	Acc@5 100.00 ( 99.55)
Epoch: [21][ 30/391]	Time  0.133 ( 0.142)	Data  0.001 ( 0.010)	Loss 2.4643e-01 (3.4041e-01)	Acc@1  89.06 ( 88.84)	Acc@5 100.00 ( 99.57)
Epoch: [21][ 40/391]	Time  0.134 ( 0.140)	Data  0.002 ( 0.009)	Loss 3.5322e-01 (3.4462e-01)	Acc@1  89.84 ( 88.59)	Acc@5 100.00 ( 99.58)
Epoch: [21][ 50/391]	Time  0.133 ( 0.140)	Data  0.001 ( 0.008)	Loss 3.8494e-01 (3.4692e-01)	Acc@1  85.94 ( 88.42)	Acc@5 100.00 ( 99.59)
Epoch: [21][ 60/391]	Time  0.133 ( 0.139)	Data  0.002 ( 0.007)	Loss 3.3460e-01 (3.4494e-01)	Acc@1  90.62 ( 88.31)	Acc@5  99.22 ( 99.59)
Epoch: [21][ 70/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.007)	Loss 4.4669e-01 (3.4590e-01)	Acc@1  83.59 ( 88.29)	Acc@5  99.22 ( 99.57)
Epoch: [21][ 80/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.006)	Loss 4.1339e-01 (3.4219e-01)	Acc@1  88.28 ( 88.42)	Acc@5  98.44 ( 99.57)
Epoch: [21][ 90/391]	Time  0.130 ( 0.138)	Data  0.002 ( 0.006)	Loss 2.7741e-01 (3.4805e-01)	Acc@1  89.84 ( 88.16)	Acc@5 100.00 ( 99.56)
Epoch: [21][100/391]	Time  0.130 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.6544e-01 (3.4949e-01)	Acc@1  88.28 ( 88.04)	Acc@5 100.00 ( 99.57)
Epoch: [21][110/391]	Time  0.136 ( 0.138)	Data  0.002 ( 0.006)	Loss 3.5333e-01 (3.4920e-01)	Acc@1  87.50 ( 88.02)	Acc@5 100.00 ( 99.58)
Epoch: [21][120/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.1414e-01 (3.4866e-01)	Acc@1  92.97 ( 87.99)	Acc@5 100.00 ( 99.59)
Epoch: [21][130/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.2252e-01 (3.5014e-01)	Acc@1  88.28 ( 87.91)	Acc@5 100.00 ( 99.58)
Epoch: [21][140/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.3992e-01 (3.4998e-01)	Acc@1  88.28 ( 87.90)	Acc@5 100.00 ( 99.57)
Epoch: [21][150/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.9637e-01 (3.4985e-01)	Acc@1  86.72 ( 87.90)	Acc@5 100.00 ( 99.59)
Epoch: [21][160/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.6259e-01 (3.4950e-01)	Acc@1  92.19 ( 87.96)	Acc@5 100.00 ( 99.60)
Epoch: [21][170/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.1495e-01 (3.5055e-01)	Acc@1  92.97 ( 87.92)	Acc@5 100.00 ( 99.59)
Epoch: [21][180/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.3331e-01 (3.4961e-01)	Acc@1  87.50 ( 87.96)	Acc@5 100.00 ( 99.60)
Epoch: [21][190/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.6888e-01 (3.4691e-01)	Acc@1  92.19 ( 88.08)	Acc@5  99.22 ( 99.61)
Epoch: [21][200/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.005)	Loss 4.6361e-01 (3.4521e-01)	Acc@1  83.59 ( 88.16)	Acc@5 100.00 ( 99.62)
Epoch: [21][210/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.9877e-01 (3.4553e-01)	Acc@1  87.50 ( 88.13)	Acc@5 100.00 ( 99.61)
Epoch: [21][220/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.0210e-01 (3.4452e-01)	Acc@1  89.84 ( 88.15)	Acc@5 100.00 ( 99.63)
Epoch: [21][230/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.2833e-01 (3.4523e-01)	Acc@1  87.50 ( 88.11)	Acc@5 100.00 ( 99.63)
Epoch: [21][240/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.8863e-01 (3.4531e-01)	Acc@1  88.28 ( 88.08)	Acc@5 100.00 ( 99.62)
Epoch: [21][250/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.3303e-01 (3.4527e-01)	Acc@1  90.62 ( 88.10)	Acc@5  99.22 ( 99.62)
Epoch: [21][260/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.8474e-01 (3.4585e-01)	Acc@1  85.16 ( 88.04)	Acc@5 100.00 ( 99.62)
Epoch: [21][270/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.5605e-01 (3.4724e-01)	Acc@1  85.94 ( 87.98)	Acc@5 100.00 ( 99.61)
Epoch: [21][280/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.005)	Loss 4.0028e-01 (3.4830e-01)	Acc@1  88.28 ( 87.93)	Acc@5  99.22 ( 99.61)
Epoch: [21][290/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.7177e-01 (3.4818e-01)	Acc@1  88.28 ( 87.91)	Acc@5  99.22 ( 99.61)
Epoch: [21][300/391]	Time  0.129 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.7832e-01 (3.4681e-01)	Acc@1  87.50 ( 87.96)	Acc@5 100.00 ( 99.62)
Epoch: [21][310/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.5671e-01 (3.4613e-01)	Acc@1  89.06 ( 88.00)	Acc@5  99.22 ( 99.62)
Epoch: [21][320/391]	Time  0.131 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.3028e-01 (3.4562e-01)	Acc@1  89.84 ( 88.03)	Acc@5  99.22 ( 99.61)
Epoch: [21][330/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.6501e-01 (3.4593e-01)	Acc@1  89.06 ( 88.02)	Acc@5  99.22 ( 99.61)
Epoch: [21][340/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.005)	Loss 4.1070e-01 (3.4602e-01)	Acc@1  83.59 ( 88.00)	Acc@5  98.44 ( 99.61)
Epoch: [21][350/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.5331e-01 (3.4571e-01)	Acc@1  85.16 ( 88.01)	Acc@5  99.22 ( 99.61)
Epoch: [21][360/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.7055e-01 (3.4508e-01)	Acc@1  89.06 ( 88.04)	Acc@5 100.00 ( 99.61)
Epoch: [21][370/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.005)	Loss 4.2534e-01 (3.4556e-01)	Acc@1  86.72 ( 88.03)	Acc@5  98.44 ( 99.61)
Epoch: [21][380/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.005)	Loss 4.0510e-01 (3.4665e-01)	Acc@1  85.16 ( 88.01)	Acc@5 100.00 ( 99.61)
Epoch: [21][390/391]	Time  0.126 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.9267e-01 (3.4614e-01)	Acc@1  87.50 ( 88.02)	Acc@5 100.00 ( 99.61)
## e[21] optimizer.zero_grad (sum) time: 0.28844451904296875
## e[21]       loss.backward (sum) time: 18.576305389404297
## e[21]      optimizer.step (sum) time: 2.910099506378174
## epoch[21] training(only) time: 53.972325563430786
# Switched to evaluate mode...
Test: [  0/100]	Time  0.208 ( 0.208)	Loss 3.3980e-01 (3.3980e-01)	Acc@1  88.00 ( 88.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.042 ( 0.059)	Loss 3.8063e-01 (4.1066e-01)	Acc@1  87.00 ( 85.73)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.044 ( 0.052)	Loss 6.0432e-01 (4.3680e-01)	Acc@1  78.00 ( 84.95)	Acc@5  99.00 ( 99.62)
Test: [ 30/100]	Time  0.042 ( 0.049)	Loss 3.6382e-01 (4.4764e-01)	Acc@1  87.00 ( 85.16)	Acc@5  99.00 ( 99.42)
Test: [ 40/100]	Time  0.045 ( 0.048)	Loss 4.8837e-01 (4.5324e-01)	Acc@1  88.00 ( 84.78)	Acc@5  99.00 ( 99.37)
Test: [ 50/100]	Time  0.044 ( 0.047)	Loss 4.4712e-01 (4.5044e-01)	Acc@1  85.00 ( 84.96)	Acc@5  98.00 ( 99.37)
Test: [ 60/100]	Time  0.045 ( 0.047)	Loss 4.4794e-01 (4.5504e-01)	Acc@1  82.00 ( 84.74)	Acc@5 100.00 ( 99.39)
Test: [ 70/100]	Time  0.043 ( 0.046)	Loss 6.4830e-01 (4.5498e-01)	Acc@1  84.00 ( 84.97)	Acc@5  98.00 ( 99.39)
Test: [ 80/100]	Time  0.042 ( 0.046)	Loss 3.6623e-01 (4.5393e-01)	Acc@1  88.00 ( 84.94)	Acc@5 100.00 ( 99.41)
Test: [ 90/100]	Time  0.041 ( 0.045)	Loss 2.8378e-01 (4.5098e-01)	Acc@1  85.00 ( 85.02)	Acc@5 100.00 ( 99.38)
 * Acc@1 85.250 Acc@5 99.400
### epoch[21] execution time: 58.61064028739929
EPOCH 22
# current learning rate: 0.1
# Switched to train mode...
Epoch: [22][  0/391]	Time  0.297 ( 0.297)	Data  0.178 ( 0.178)	Loss 2.9456e-01 (2.9456e-01)	Acc@1  87.50 ( 87.50)	Acc@5 100.00 (100.00)
Epoch: [22][ 10/391]	Time  0.137 ( 0.150)	Data  0.001 ( 0.020)	Loss 2.2539e-01 (3.1688e-01)	Acc@1  92.19 ( 88.71)	Acc@5 100.00 (100.00)
Epoch: [22][ 20/391]	Time  0.134 ( 0.143)	Data  0.001 ( 0.012)	Loss 3.0451e-01 (3.1229e-01)	Acc@1  88.28 ( 88.91)	Acc@5 100.00 ( 99.78)
Epoch: [22][ 30/391]	Time  0.139 ( 0.141)	Data  0.001 ( 0.010)	Loss 3.1623e-01 (2.9794e-01)	Acc@1  86.72 ( 89.57)	Acc@5 100.00 ( 99.85)
Epoch: [22][ 40/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.008)	Loss 2.6657e-01 (3.0663e-01)	Acc@1  89.84 ( 89.42)	Acc@5 100.00 ( 99.81)
Epoch: [22][ 50/391]	Time  0.133 ( 0.140)	Data  0.001 ( 0.008)	Loss 3.3395e-01 (3.0931e-01)	Acc@1  87.50 ( 89.31)	Acc@5 100.00 ( 99.80)
Epoch: [22][ 60/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.007)	Loss 3.6202e-01 (3.1570e-01)	Acc@1  87.50 ( 89.01)	Acc@5 100.00 ( 99.80)
Epoch: [22][ 70/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.007)	Loss 2.5492e-01 (3.1264e-01)	Acc@1  91.41 ( 89.12)	Acc@5 100.00 ( 99.80)
Epoch: [22][ 80/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.006)	Loss 1.8973e-01 (3.1134e-01)	Acc@1  92.19 ( 89.09)	Acc@5 100.00 ( 99.82)
Epoch: [22][ 90/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.006)	Loss 2.9604e-01 (3.1282e-01)	Acc@1  89.84 ( 89.04)	Acc@5 100.00 ( 99.78)
Epoch: [22][100/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.006)	Loss 2.9602e-01 (3.1958e-01)	Acc@1  87.50 ( 88.88)	Acc@5 100.00 ( 99.75)
Epoch: [22][110/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.1576e-01 (3.2036e-01)	Acc@1  90.62 ( 88.81)	Acc@5  99.22 ( 99.70)
Epoch: [22][120/391]	Time  0.126 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.0363e-01 (3.2274e-01)	Acc@1  91.41 ( 88.73)	Acc@5 100.00 ( 99.69)
Epoch: [22][130/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.8490e-01 (3.2379e-01)	Acc@1  90.62 ( 88.69)	Acc@5 100.00 ( 99.68)
Epoch: [22][140/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.5142e-01 (3.2162e-01)	Acc@1  92.19 ( 88.84)	Acc@5 100.00 ( 99.67)
Epoch: [22][150/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.7693e-01 (3.2228e-01)	Acc@1  88.28 ( 88.82)	Acc@5  98.44 ( 99.67)
Epoch: [22][160/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.005)	Loss 4.4512e-01 (3.2364e-01)	Acc@1  85.94 ( 88.80)	Acc@5  98.44 ( 99.67)
Epoch: [22][170/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.3948e-01 (3.2546e-01)	Acc@1  87.50 ( 88.69)	Acc@5 100.00 ( 99.68)
Epoch: [22][180/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.9180e-01 (3.2635e-01)	Acc@1  84.38 ( 88.57)	Acc@5 100.00 ( 99.68)
Epoch: [22][190/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.4917e-01 (3.2321e-01)	Acc@1  90.62 ( 88.69)	Acc@5 100.00 ( 99.69)
Epoch: [22][200/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.1964e-01 (3.2392e-01)	Acc@1  90.62 ( 88.65)	Acc@5 100.00 ( 99.70)
Epoch: [22][210/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.2719e-01 (3.2310e-01)	Acc@1  91.41 ( 88.70)	Acc@5  99.22 ( 99.70)
Epoch: [22][220/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.0015e-01 (3.2270e-01)	Acc@1  88.28 ( 88.66)	Acc@5 100.00 ( 99.70)
Epoch: [22][230/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.6040e-01 (3.2360e-01)	Acc@1  91.41 ( 88.65)	Acc@5 100.00 ( 99.70)
Epoch: [22][240/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.3660e-01 (3.2408e-01)	Acc@1  90.62 ( 88.64)	Acc@5 100.00 ( 99.71)
Epoch: [22][250/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.7534e-01 (3.2358e-01)	Acc@1  88.28 ( 88.66)	Acc@5  99.22 ( 99.70)
Epoch: [22][260/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.7756e-01 (3.2588e-01)	Acc@1  83.59 ( 88.55)	Acc@5  96.88 ( 99.69)
Epoch: [22][270/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.6577e-01 (3.2618e-01)	Acc@1  89.06 ( 88.55)	Acc@5 100.00 ( 99.69)
Epoch: [22][280/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.4570e-01 (3.2665e-01)	Acc@1  82.81 ( 88.54)	Acc@5  99.22 ( 99.69)
Epoch: [22][290/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.6809e-01 (3.2724e-01)	Acc@1  90.62 ( 88.56)	Acc@5 100.00 ( 99.70)
Epoch: [22][300/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.0229e-01 (3.2727e-01)	Acc@1  88.28 ( 88.59)	Acc@5 100.00 ( 99.69)
Epoch: [22][310/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.0827e-01 (3.2799e-01)	Acc@1  88.28 ( 88.58)	Acc@5  99.22 ( 99.69)
Epoch: [22][320/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.4477e-01 (3.2915e-01)	Acc@1  86.72 ( 88.53)	Acc@5 100.00 ( 99.68)
Epoch: [22][330/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.3458e-01 (3.2931e-01)	Acc@1  89.84 ( 88.54)	Acc@5 100.00 ( 99.69)
Epoch: [22][340/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.9870e-01 (3.2953e-01)	Acc@1  86.72 ( 88.55)	Acc@5  99.22 ( 99.68)
Epoch: [22][350/391]	Time  0.134 ( 0.137)	Data  0.002 ( 0.005)	Loss 3.0763e-01 (3.2953e-01)	Acc@1  88.28 ( 88.54)	Acc@5 100.00 ( 99.68)
Epoch: [22][360/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.7288e-01 (3.3037e-01)	Acc@1  86.72 ( 88.50)	Acc@5 100.00 ( 99.68)
Epoch: [22][370/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.2079e-01 (3.3239e-01)	Acc@1  85.94 ( 88.43)	Acc@5  99.22 ( 99.67)
Epoch: [22][380/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.3438e-01 (3.3411e-01)	Acc@1  82.03 ( 88.37)	Acc@5 100.00 ( 99.67)
Epoch: [22][390/391]	Time  0.126 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.2187e-01 (3.3428e-01)	Acc@1  87.50 ( 88.35)	Acc@5 100.00 ( 99.67)
## e[22] optimizer.zero_grad (sum) time: 0.2836732864379883
## e[22]       loss.backward (sum) time: 18.5292010307312
## e[22]      optimizer.step (sum) time: 2.925109624862671
## epoch[22] training(only) time: 53.80276823043823
# Switched to evaluate mode...
Test: [  0/100]	Time  0.207 ( 0.207)	Loss 3.6883e-01 (3.6883e-01)	Acc@1  88.00 ( 88.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.043 ( 0.058)	Loss 4.4666e-01 (4.1027e-01)	Acc@1  83.00 ( 85.45)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.044 ( 0.051)	Loss 4.9475e-01 (4.1673e-01)	Acc@1  82.00 ( 85.57)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.042 ( 0.048)	Loss 3.7556e-01 (4.2684e-01)	Acc@1  85.00 ( 85.61)	Acc@5  99.00 ( 99.48)
Test: [ 40/100]	Time  0.043 ( 0.047)	Loss 4.4912e-01 (4.2965e-01)	Acc@1  83.00 ( 85.56)	Acc@5  99.00 ( 99.44)
Test: [ 50/100]	Time  0.043 ( 0.046)	Loss 3.6479e-01 (4.3072e-01)	Acc@1  88.00 ( 85.61)	Acc@5  97.00 ( 99.39)
Test: [ 60/100]	Time  0.044 ( 0.046)	Loss 4.2571e-01 (4.3045e-01)	Acc@1  83.00 ( 85.49)	Acc@5 100.00 ( 99.41)
Test: [ 70/100]	Time  0.043 ( 0.045)	Loss 4.8135e-01 (4.3226e-01)	Acc@1  85.00 ( 85.49)	Acc@5  98.00 ( 99.42)
Test: [ 80/100]	Time  0.043 ( 0.045)	Loss 3.5544e-01 (4.3195e-01)	Acc@1  86.00 ( 85.53)	Acc@5 100.00 ( 99.44)
Test: [ 90/100]	Time  0.044 ( 0.045)	Loss 3.0834e-01 (4.3301e-01)	Acc@1  90.00 ( 85.47)	Acc@5 100.00 ( 99.44)
 * Acc@1 85.580 Acc@5 99.470
### epoch[22] execution time: 58.40223836898804
EPOCH 23
# current learning rate: 0.1
# Switched to train mode...
Epoch: [23][  0/391]	Time  0.288 ( 0.288)	Data  0.176 ( 0.176)	Loss 3.7424e-01 (3.7424e-01)	Acc@1  87.50 ( 87.50)	Acc@5 100.00 (100.00)
Epoch: [23][ 10/391]	Time  0.136 ( 0.150)	Data  0.001 ( 0.020)	Loss 1.7648e-01 (2.8855e-01)	Acc@1  96.09 ( 90.62)	Acc@5 100.00 ( 99.79)
Epoch: [23][ 20/391]	Time  0.135 ( 0.143)	Data  0.001 ( 0.012)	Loss 3.1783e-01 (2.9042e-01)	Acc@1  87.50 ( 90.03)	Acc@5  99.22 ( 99.63)
Epoch: [23][ 30/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.010)	Loss 3.0105e-01 (2.8954e-01)	Acc@1  91.41 ( 90.40)	Acc@5 100.00 ( 99.65)
Epoch: [23][ 40/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.008)	Loss 2.4059e-01 (2.8292e-01)	Acc@1  92.97 ( 90.66)	Acc@5 100.00 ( 99.70)
Epoch: [23][ 50/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.008)	Loss 3.3368e-01 (2.8840e-01)	Acc@1  89.84 ( 90.52)	Acc@5  99.22 ( 99.71)
Epoch: [23][ 60/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.007)	Loss 2.4890e-01 (2.9442e-01)	Acc@1  89.84 ( 90.14)	Acc@5 100.00 ( 99.71)
Epoch: [23][ 70/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.007)	Loss 3.3846e-01 (2.9167e-01)	Acc@1  87.50 ( 90.10)	Acc@5 100.00 ( 99.71)
Epoch: [23][ 80/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.8714e-01 (2.9855e-01)	Acc@1  87.50 ( 89.83)	Acc@5  99.22 ( 99.65)
Epoch: [23][ 90/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.8731e-01 (2.9625e-01)	Acc@1  88.28 ( 89.79)	Acc@5 100.00 ( 99.68)
Epoch: [23][100/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.1283e-01 (2.9872e-01)	Acc@1  89.06 ( 89.77)	Acc@5  99.22 ( 99.67)
Epoch: [23][110/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.7804e-01 (3.0088e-01)	Acc@1  91.41 ( 89.72)	Acc@5 100.00 ( 99.66)
Epoch: [23][120/391]	Time  0.134 ( 0.138)	Data  0.002 ( 0.006)	Loss 4.3122e-01 (3.0603e-01)	Acc@1  85.16 ( 89.45)	Acc@5  99.22 ( 99.64)
Epoch: [23][130/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.3328e-01 (3.0633e-01)	Acc@1  92.97 ( 89.40)	Acc@5 100.00 ( 99.65)
Epoch: [23][140/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.6248e-01 (3.0941e-01)	Acc@1  85.16 ( 89.30)	Acc@5 100.00 ( 99.64)
Epoch: [23][150/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.4208e-01 (3.0934e-01)	Acc@1  92.19 ( 89.35)	Acc@5 100.00 ( 99.63)
Epoch: [23][160/391]	Time  0.129 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.7513e-01 (3.0986e-01)	Acc@1  88.28 ( 89.31)	Acc@5 100.00 ( 99.64)
Epoch: [23][170/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.8048e-01 (3.1024e-01)	Acc@1  84.38 ( 89.23)	Acc@5 100.00 ( 99.65)
Epoch: [23][180/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.1300e-01 (3.1161e-01)	Acc@1  92.19 ( 89.16)	Acc@5 100.00 ( 99.66)
Epoch: [23][190/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.7223e-01 (3.1114e-01)	Acc@1  83.59 ( 89.10)	Acc@5  99.22 ( 99.65)
Epoch: [23][200/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.1797e-01 (3.1055e-01)	Acc@1  88.28 ( 89.11)	Acc@5  99.22 ( 99.65)
Epoch: [23][210/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.4317e-01 (3.1084e-01)	Acc@1  89.06 ( 89.10)	Acc@5  98.44 ( 99.64)
Epoch: [23][220/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.9844e-01 (3.1154e-01)	Acc@1  85.94 ( 89.05)	Acc@5  98.44 ( 99.64)
Epoch: [23][230/391]	Time  0.136 ( 0.137)	Data  0.002 ( 0.005)	Loss 4.8624e-01 (3.1350e-01)	Acc@1  82.81 ( 88.98)	Acc@5 100.00 ( 99.65)
Epoch: [23][240/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.5855e-01 (3.1458e-01)	Acc@1  84.38 ( 88.95)	Acc@5 100.00 ( 99.65)
Epoch: [23][250/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.0315e-01 (3.1513e-01)	Acc@1  89.84 ( 88.93)	Acc@5 100.00 ( 99.65)
Epoch: [23][260/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.6028e-01 (3.1638e-01)	Acc@1  89.84 ( 88.88)	Acc@5 100.00 ( 99.66)
Epoch: [23][270/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.2751e-01 (3.1559e-01)	Acc@1  92.97 ( 88.92)	Acc@5 100.00 ( 99.66)
Epoch: [23][280/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.6150e-01 (3.1671e-01)	Acc@1  85.94 ( 88.88)	Acc@5 100.00 ( 99.66)
Epoch: [23][290/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.1211e-01 (3.1725e-01)	Acc@1  84.38 ( 88.87)	Acc@5  98.44 ( 99.66)
Epoch: [23][300/391]	Time  0.139 ( 0.137)	Data  0.002 ( 0.005)	Loss 3.0899e-01 (3.1645e-01)	Acc@1  89.84 ( 88.91)	Acc@5 100.00 ( 99.66)
Epoch: [23][310/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.7693e-01 (3.1784e-01)	Acc@1  81.25 ( 88.87)	Acc@5  98.44 ( 99.66)
Epoch: [23][320/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.9055e-01 (3.1843e-01)	Acc@1  89.84 ( 88.86)	Acc@5  99.22 ( 99.65)
Epoch: [23][330/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.5648e-01 (3.1850e-01)	Acc@1  88.28 ( 88.88)	Acc@5  99.22 ( 99.65)
Epoch: [23][340/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.1153e-01 (3.1867e-01)	Acc@1  86.72 ( 88.88)	Acc@5 100.00 ( 99.65)
Epoch: [23][350/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.6879e-01 (3.1827e-01)	Acc@1  89.06 ( 88.91)	Acc@5 100.00 ( 99.65)
Epoch: [23][360/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.5437e-01 (3.1823e-01)	Acc@1  90.62 ( 88.92)	Acc@5 100.00 ( 99.65)
Epoch: [23][370/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.4411e-01 (3.1838e-01)	Acc@1  91.41 ( 88.90)	Acc@5 100.00 ( 99.65)
Epoch: [23][380/391]	Time  0.138 ( 0.137)	Data  0.002 ( 0.005)	Loss 3.8277e-01 (3.2026e-01)	Acc@1  83.59 ( 88.82)	Acc@5 100.00 ( 99.65)
Epoch: [23][390/391]	Time  0.127 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.1069e-01 (3.2136e-01)	Acc@1  88.75 ( 88.82)	Acc@5  98.75 ( 99.65)
## e[23] optimizer.zero_grad (sum) time: 0.28534460067749023
## e[23]       loss.backward (sum) time: 18.575286626815796
## e[23]      optimizer.step (sum) time: 2.907709836959839
## epoch[23] training(only) time: 53.88243317604065
# Switched to evaluate mode...
Test: [  0/100]	Time  0.202 ( 0.202)	Loss 4.4185e-01 (4.4185e-01)	Acc@1  86.00 ( 86.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.043 ( 0.059)	Loss 4.4402e-01 (4.4313e-01)	Acc@1  87.00 ( 84.45)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.045 ( 0.052)	Loss 6.4873e-01 (4.6824e-01)	Acc@1  79.00 ( 84.00)	Acc@5 100.00 ( 99.52)
Test: [ 30/100]	Time  0.042 ( 0.049)	Loss 3.9895e-01 (4.6500e-01)	Acc@1  83.00 ( 84.35)	Acc@5 100.00 ( 99.55)
Test: [ 40/100]	Time  0.042 ( 0.048)	Loss 6.0208e-01 (4.6747e-01)	Acc@1  79.00 ( 84.27)	Acc@5  99.00 ( 99.44)
Test: [ 50/100]	Time  0.043 ( 0.047)	Loss 3.3288e-01 (4.5985e-01)	Acc@1  88.00 ( 84.67)	Acc@5  99.00 ( 99.39)
Test: [ 60/100]	Time  0.045 ( 0.046)	Loss 4.1224e-01 (4.6675e-01)	Acc@1  85.00 ( 84.49)	Acc@5 100.00 ( 99.39)
Test: [ 70/100]	Time  0.041 ( 0.046)	Loss 7.1928e-01 (4.7550e-01)	Acc@1  80.00 ( 84.42)	Acc@5 100.00 ( 99.34)
Test: [ 80/100]	Time  0.045 ( 0.046)	Loss 3.7496e-01 (4.7469e-01)	Acc@1  86.00 ( 84.33)	Acc@5  99.00 ( 99.31)
Test: [ 90/100]	Time  0.044 ( 0.045)	Loss 3.9871e-01 (4.7490e-01)	Acc@1  83.00 ( 84.29)	Acc@5 100.00 ( 99.32)
 * Acc@1 84.530 Acc@5 99.360
### epoch[23] execution time: 58.51911282539368
EPOCH 24
# current learning rate: 0.1
# Switched to train mode...
Epoch: [24][  0/391]	Time  0.299 ( 0.299)	Data  0.181 ( 0.181)	Loss 2.0426e-01 (2.0426e-01)	Acc@1  92.97 ( 92.97)	Acc@5  99.22 ( 99.22)
Epoch: [24][ 10/391]	Time  0.133 ( 0.152)	Data  0.001 ( 0.022)	Loss 2.5436e-01 (2.7483e-01)	Acc@1  92.19 ( 90.62)	Acc@5 100.00 ( 99.72)
Epoch: [24][ 20/391]	Time  0.138 ( 0.145)	Data  0.001 ( 0.013)	Loss 2.8164e-01 (2.7224e-01)	Acc@1  92.19 ( 90.59)	Acc@5  99.22 ( 99.78)
Epoch: [24][ 30/391]	Time  0.135 ( 0.142)	Data  0.001 ( 0.010)	Loss 3.3063e-01 (2.7468e-01)	Acc@1  89.06 ( 90.55)	Acc@5  99.22 ( 99.77)
Epoch: [24][ 40/391]	Time  0.136 ( 0.141)	Data  0.001 ( 0.009)	Loss 3.7344e-01 (2.8648e-01)	Acc@1  85.94 ( 90.24)	Acc@5 100.00 ( 99.81)
Epoch: [24][ 50/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.008)	Loss 2.4996e-01 (2.9119e-01)	Acc@1  90.62 ( 89.95)	Acc@5 100.00 ( 99.80)
Epoch: [24][ 60/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.008)	Loss 2.2002e-01 (2.8532e-01)	Acc@1  93.75 ( 90.11)	Acc@5 100.00 ( 99.77)
Epoch: [24][ 70/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.007)	Loss 2.2670e-01 (2.8219e-01)	Acc@1  92.97 ( 90.11)	Acc@5 100.00 ( 99.77)
Epoch: [24][ 80/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.007)	Loss 2.6279e-01 (2.8335e-01)	Acc@1  92.97 ( 90.09)	Acc@5  99.22 ( 99.75)
Epoch: [24][ 90/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.007)	Loss 2.1392e-01 (2.8834e-01)	Acc@1  90.62 ( 89.92)	Acc@5 100.00 ( 99.73)
Epoch: [24][100/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.006)	Loss 4.4086e-01 (2.8919e-01)	Acc@1  82.81 ( 89.84)	Acc@5 100.00 ( 99.74)
Epoch: [24][110/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.7331e-01 (2.9071e-01)	Acc@1  90.62 ( 89.82)	Acc@5 100.00 ( 99.75)
Epoch: [24][120/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.9919e-01 (2.8976e-01)	Acc@1  91.41 ( 89.83)	Acc@5  98.44 ( 99.75)
Epoch: [24][130/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.4082e-01 (2.8906e-01)	Acc@1  86.72 ( 89.81)	Acc@5 100.00 ( 99.77)
Epoch: [24][140/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.3776e-01 (2.9011e-01)	Acc@1  89.06 ( 89.79)	Acc@5 100.00 ( 99.78)
Epoch: [24][150/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.1859e-01 (2.9089e-01)	Acc@1  92.97 ( 89.70)	Acc@5 100.00 ( 99.78)
Epoch: [24][160/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.2834e-01 (2.9335e-01)	Acc@1  91.41 ( 89.66)	Acc@5 100.00 ( 99.78)
Epoch: [24][170/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.4709e-01 (2.9501e-01)	Acc@1  82.81 ( 89.62)	Acc@5  99.22 ( 99.78)
Epoch: [24][180/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.9101e-01 (2.9768e-01)	Acc@1  89.84 ( 89.57)	Acc@5 100.00 ( 99.76)
Epoch: [24][190/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.005)	Loss 4.1952e-01 (2.9956e-01)	Acc@1  85.16 ( 89.52)	Acc@5 100.00 ( 99.76)
Epoch: [24][200/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.0591e-01 (2.9994e-01)	Acc@1  89.84 ( 89.51)	Acc@5 100.00 ( 99.76)
Epoch: [24][210/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.4438e-01 (3.0099e-01)	Acc@1  87.50 ( 89.45)	Acc@5 100.00 ( 99.77)
Epoch: [24][220/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.0861e-01 (3.0130e-01)	Acc@1  87.50 ( 89.42)	Acc@5 100.00 ( 99.77)
Epoch: [24][230/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 4.3595e-01 (3.0294e-01)	Acc@1  84.38 ( 89.43)	Acc@5  99.22 ( 99.77)
Epoch: [24][240/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.3558e-01 (3.0330e-01)	Acc@1  88.28 ( 89.41)	Acc@5 100.00 ( 99.77)
Epoch: [24][250/391]	Time  0.139 ( 0.138)	Data  0.002 ( 0.005)	Loss 2.8296e-01 (3.0401e-01)	Acc@1  89.84 ( 89.40)	Acc@5 100.00 ( 99.77)
Epoch: [24][260/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.9816e-01 (3.0401e-01)	Acc@1  87.50 ( 89.41)	Acc@5  98.44 ( 99.76)
Epoch: [24][270/391]	Time  0.136 ( 0.137)	Data  0.002 ( 0.005)	Loss 2.9810e-01 (3.0554e-01)	Acc@1  92.97 ( 89.32)	Acc@5 100.00 ( 99.75)
Epoch: [24][280/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.7355e-01 (3.0586e-01)	Acc@1  91.41 ( 89.34)	Acc@5 100.00 ( 99.76)
Epoch: [24][290/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.3195e-01 (3.0594e-01)	Acc@1  89.06 ( 89.34)	Acc@5 100.00 ( 99.75)
Epoch: [24][300/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.3409e-01 (3.0567e-01)	Acc@1  91.41 ( 89.31)	Acc@5 100.00 ( 99.75)
Epoch: [24][310/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.1328e-01 (3.0592e-01)	Acc@1  93.75 ( 89.32)	Acc@5 100.00 ( 99.74)
Epoch: [24][320/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.2871e-01 (3.0661e-01)	Acc@1  92.97 ( 89.31)	Acc@5 100.00 ( 99.74)
Epoch: [24][330/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.9420e-01 (3.0585e-01)	Acc@1  87.50 ( 89.32)	Acc@5 100.00 ( 99.73)
Epoch: [24][340/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.7834e-01 (3.0581e-01)	Acc@1  89.06 ( 89.32)	Acc@5  97.66 ( 99.73)
Epoch: [24][350/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.7164e-01 (3.0774e-01)	Acc@1  88.28 ( 89.26)	Acc@5  99.22 ( 99.73)
Epoch: [24][360/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.7854e-01 (3.0811e-01)	Acc@1  89.06 ( 89.26)	Acc@5 100.00 ( 99.73)
Epoch: [24][370/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.4321e-01 (3.0857e-01)	Acc@1  89.06 ( 89.26)	Acc@5  99.22 ( 99.72)
Epoch: [24][380/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.2233e-01 (3.0786e-01)	Acc@1  88.28 ( 89.28)	Acc@5 100.00 ( 99.72)
Epoch: [24][390/391]	Time  0.128 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.0090e-01 (3.0823e-01)	Acc@1  82.50 ( 89.27)	Acc@5 100.00 ( 99.72)
## e[24] optimizer.zero_grad (sum) time: 0.28533077239990234
## e[24]       loss.backward (sum) time: 18.540423154830933
## e[24]      optimizer.step (sum) time: 2.8956985473632812
## epoch[24] training(only) time: 53.833789110183716
# Switched to evaluate mode...
Test: [  0/100]	Time  0.202 ( 0.202)	Loss 3.2578e-01 (3.2578e-01)	Acc@1  91.00 ( 91.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.046 ( 0.059)	Loss 4.4252e-01 (3.6775e-01)	Acc@1  83.00 ( 87.91)	Acc@5 100.00 ( 99.45)
Test: [ 20/100]	Time  0.044 ( 0.052)	Loss 4.1401e-01 (3.7818e-01)	Acc@1  83.00 ( 87.38)	Acc@5 100.00 ( 99.52)
Test: [ 30/100]	Time  0.043 ( 0.049)	Loss 5.8589e-01 (3.9119e-01)	Acc@1  83.00 ( 87.32)	Acc@5 100.00 ( 99.48)
Test: [ 40/100]	Time  0.041 ( 0.048)	Loss 5.0854e-01 (3.9827e-01)	Acc@1  83.00 ( 87.20)	Acc@5  98.00 ( 99.37)
Test: [ 50/100]	Time  0.042 ( 0.047)	Loss 4.5474e-01 (3.9356e-01)	Acc@1  86.00 ( 87.29)	Acc@5  98.00 ( 99.33)
Test: [ 60/100]	Time  0.049 ( 0.047)	Loss 3.6797e-01 (3.9774e-01)	Acc@1  88.00 ( 87.20)	Acc@5 100.00 ( 99.36)
Test: [ 70/100]	Time  0.045 ( 0.046)	Loss 7.1886e-01 (4.0015e-01)	Acc@1  80.00 ( 87.04)	Acc@5  99.00 ( 99.41)
Test: [ 80/100]	Time  0.046 ( 0.046)	Loss 3.5605e-01 (3.9632e-01)	Acc@1  89.00 ( 87.12)	Acc@5 100.00 ( 99.44)
Test: [ 90/100]	Time  0.043 ( 0.046)	Loss 3.3692e-01 (3.9486e-01)	Acc@1  89.00 ( 87.21)	Acc@5 100.00 ( 99.48)
 * Acc@1 87.190 Acc@5 99.530
### epoch[24] execution time: 58.4714081287384
EPOCH 25
# current learning rate: 0.1
# Switched to train mode...
Epoch: [25][  0/391]	Time  0.297 ( 0.297)	Data  0.177 ( 0.177)	Loss 3.2054e-01 (3.2054e-01)	Acc@1  86.72 ( 86.72)	Acc@5 100.00 (100.00)
Epoch: [25][ 10/391]	Time  0.132 ( 0.151)	Data  0.001 ( 0.020)	Loss 3.4520e-01 (3.1508e-01)	Acc@1  84.38 ( 88.57)	Acc@5  99.22 ( 99.86)
Epoch: [25][ 20/391]	Time  0.133 ( 0.144)	Data  0.001 ( 0.013)	Loss 2.8706e-01 (2.9605e-01)	Acc@1  92.97 ( 89.62)	Acc@5  99.22 ( 99.78)
Epoch: [25][ 30/391]	Time  0.135 ( 0.141)	Data  0.001 ( 0.010)	Loss 2.0709e-01 (2.8848e-01)	Acc@1  95.31 ( 89.89)	Acc@5 100.00 ( 99.80)
Epoch: [25][ 40/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.009)	Loss 2.9923e-01 (2.8705e-01)	Acc@1  88.28 ( 89.69)	Acc@5 100.00 ( 99.79)
Epoch: [25][ 50/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.008)	Loss 1.7938e-01 (2.7799e-01)	Acc@1  93.75 ( 89.94)	Acc@5 100.00 ( 99.80)
Epoch: [25][ 60/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.007)	Loss 2.5034e-01 (2.8360e-01)	Acc@1  89.06 ( 89.70)	Acc@5 100.00 ( 99.77)
Epoch: [25][ 70/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.007)	Loss 2.4243e-01 (2.8189e-01)	Acc@1  93.75 ( 89.78)	Acc@5 100.00 ( 99.79)
Epoch: [25][ 80/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.007)	Loss 3.1677e-01 (2.8356e-01)	Acc@1  91.41 ( 89.79)	Acc@5  99.22 ( 99.77)
Epoch: [25][ 90/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.1797e-01 (2.8555e-01)	Acc@1  94.53 ( 89.68)	Acc@5 100.00 ( 99.77)
Epoch: [25][100/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.0267e-01 (2.8451e-01)	Acc@1  86.72 ( 89.83)	Acc@5  99.22 ( 99.77)
Epoch: [25][110/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.2343e-01 (2.8631e-01)	Acc@1  95.31 ( 89.84)	Acc@5 100.00 ( 99.77)
Epoch: [25][120/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.5243e-01 (2.8593e-01)	Acc@1  89.84 ( 89.97)	Acc@5 100.00 ( 99.76)
Epoch: [25][130/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.3472e-01 (2.8525e-01)	Acc@1  88.28 ( 89.97)	Acc@5 100.00 ( 99.77)
Epoch: [25][140/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.5185e-01 (2.8277e-01)	Acc@1  90.62 ( 90.05)	Acc@5 100.00 ( 99.78)
Epoch: [25][150/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.3219e-01 (2.8347e-01)	Acc@1  89.06 ( 90.10)	Acc@5 100.00 ( 99.78)
Epoch: [25][160/391]	Time  0.134 ( 0.138)	Data  0.002 ( 0.006)	Loss 2.9068e-01 (2.8422e-01)	Acc@1  87.50 ( 90.01)	Acc@5 100.00 ( 99.79)
Epoch: [25][170/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.2798e-01 (2.8511e-01)	Acc@1  89.06 ( 90.02)	Acc@5  99.22 ( 99.79)
Epoch: [25][180/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.3839e-01 (2.8489e-01)	Acc@1  92.19 ( 90.04)	Acc@5 100.00 ( 99.78)
Epoch: [25][190/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.3431e-01 (2.8490e-01)	Acc@1  92.97 ( 90.02)	Acc@5 100.00 ( 99.79)
Epoch: [25][200/391]	Time  0.140 ( 0.137)	Data  0.002 ( 0.005)	Loss 1.6378e-01 (2.8514e-01)	Acc@1  92.97 ( 90.04)	Acc@5 100.00 ( 99.78)
Epoch: [25][210/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.8230e-01 (2.8776e-01)	Acc@1  85.16 ( 89.95)	Acc@5 100.00 ( 99.79)
Epoch: [25][220/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.0518e-01 (2.8795e-01)	Acc@1  89.06 ( 89.94)	Acc@5  99.22 ( 99.79)
Epoch: [25][230/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.8837e-01 (2.8867e-01)	Acc@1  86.72 ( 89.89)	Acc@5 100.00 ( 99.79)
Epoch: [25][240/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.3137e-01 (2.9018e-01)	Acc@1  92.97 ( 89.86)	Acc@5 100.00 ( 99.79)
Epoch: [25][250/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.2787e-01 (2.9101e-01)	Acc@1  89.06 ( 89.84)	Acc@5 100.00 ( 99.78)
Epoch: [25][260/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.2210e-01 (2.9149e-01)	Acc@1  92.19 ( 89.84)	Acc@5 100.00 ( 99.78)
Epoch: [25][270/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.8688e-01 (2.9154e-01)	Acc@1  89.06 ( 89.82)	Acc@5 100.00 ( 99.78)
Epoch: [25][280/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.1088e-01 (2.9221e-01)	Acc@1  89.84 ( 89.79)	Acc@5 100.00 ( 99.78)
Epoch: [25][290/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.6911e-01 (2.9210e-01)	Acc@1  90.62 ( 89.76)	Acc@5 100.00 ( 99.79)
Epoch: [25][300/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.4513e-01 (2.9237e-01)	Acc@1  89.06 ( 89.75)	Acc@5 100.00 ( 99.79)
Epoch: [25][310/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.0261e-01 (2.9273e-01)	Acc@1  90.62 ( 89.75)	Acc@5 100.00 ( 99.79)
Epoch: [25][320/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.5895e-01 (2.9326e-01)	Acc@1  89.84 ( 89.74)	Acc@5  98.44 ( 99.79)
Epoch: [25][330/391]	Time  0.127 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.3424e-01 (2.9420e-01)	Acc@1  89.06 ( 89.70)	Acc@5  99.22 ( 99.78)
Epoch: [25][340/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.0448e-01 (2.9402e-01)	Acc@1  85.16 ( 89.70)	Acc@5 100.00 ( 99.78)
Epoch: [25][350/391]	Time  0.141 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.2534e-01 (2.9478e-01)	Acc@1  85.16 ( 89.67)	Acc@5 100.00 ( 99.78)
Epoch: [25][360/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.4754e-01 (2.9510e-01)	Acc@1  91.41 ( 89.68)	Acc@5 100.00 ( 99.78)
Epoch: [25][370/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.0936e-01 (2.9589e-01)	Acc@1  90.62 ( 89.65)	Acc@5 100.00 ( 99.78)
Epoch: [25][380/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.6731e-01 (2.9686e-01)	Acc@1  90.62 ( 89.61)	Acc@5 100.00 ( 99.78)
Epoch: [25][390/391]	Time  0.127 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.6339e-01 (2.9667e-01)	Acc@1  91.25 ( 89.60)	Acc@5 100.00 ( 99.78)
## e[25] optimizer.zero_grad (sum) time: 0.28820300102233887
## e[25]       loss.backward (sum) time: 18.567455768585205
## e[25]      optimizer.step (sum) time: 2.8605031967163086
## epoch[25] training(only) time: 53.81764793395996
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 3.6760e-01 (3.6760e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.045 ( 0.058)	Loss 5.3168e-01 (4.0511e-01)	Acc@1  84.00 ( 87.55)	Acc@5  99.00 ( 99.45)
Test: [ 20/100]	Time  0.043 ( 0.051)	Loss 5.3967e-01 (4.1502e-01)	Acc@1  83.00 ( 86.90)	Acc@5  99.00 ( 99.33)
Test: [ 30/100]	Time  0.041 ( 0.049)	Loss 4.0342e-01 (4.2557e-01)	Acc@1  85.00 ( 86.58)	Acc@5  99.00 ( 99.29)
Test: [ 40/100]	Time  0.042 ( 0.047)	Loss 5.1734e-01 (4.2700e-01)	Acc@1  86.00 ( 86.41)	Acc@5  99.00 ( 99.22)
Test: [ 50/100]	Time  0.043 ( 0.047)	Loss 2.2786e-01 (4.1918e-01)	Acc@1  92.00 ( 86.71)	Acc@5  99.00 ( 99.29)
Test: [ 60/100]	Time  0.044 ( 0.046)	Loss 3.3876e-01 (4.1835e-01)	Acc@1  88.00 ( 86.61)	Acc@5 100.00 ( 99.36)
Test: [ 70/100]	Time  0.046 ( 0.046)	Loss 4.5015e-01 (4.1958e-01)	Acc@1  90.00 ( 86.58)	Acc@5 100.00 ( 99.37)
Test: [ 80/100]	Time  0.043 ( 0.046)	Loss 2.5491e-01 (4.1800e-01)	Acc@1  92.00 ( 86.62)	Acc@5 100.00 ( 99.36)
Test: [ 90/100]	Time  0.041 ( 0.045)	Loss 4.1258e-01 (4.1682e-01)	Acc@1  87.00 ( 86.62)	Acc@5 100.00 ( 99.41)
 * Acc@1 86.760 Acc@5 99.440
### epoch[25] execution time: 58.44820308685303
EPOCH 26
# current learning rate: 0.1
# Switched to train mode...
Epoch: [26][  0/391]	Time  0.298 ( 0.298)	Data  0.176 ( 0.176)	Loss 3.8592e-01 (3.8592e-01)	Acc@1  84.38 ( 84.38)	Acc@5 100.00 (100.00)
Epoch: [26][ 10/391]	Time  0.133 ( 0.150)	Data  0.001 ( 0.020)	Loss 2.3602e-01 (2.3959e-01)	Acc@1  93.75 ( 91.69)	Acc@5 100.00 ( 99.93)
Epoch: [26][ 20/391]	Time  0.137 ( 0.144)	Data  0.001 ( 0.013)	Loss 3.1371e-01 (2.4217e-01)	Acc@1  88.28 ( 91.85)	Acc@5 100.00 ( 99.93)
Epoch: [26][ 30/391]	Time  0.135 ( 0.141)	Data  0.001 ( 0.010)	Loss 3.7933e-01 (2.4301e-01)	Acc@1  86.72 ( 91.56)	Acc@5  98.44 ( 99.85)
Epoch: [26][ 40/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.009)	Loss 2.0404e-01 (2.6071e-01)	Acc@1  92.97 ( 90.89)	Acc@5 100.00 ( 99.79)
Epoch: [26][ 50/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.008)	Loss 3.7239e-01 (2.7274e-01)	Acc@1  85.94 ( 90.36)	Acc@5 100.00 ( 99.77)
Epoch: [26][ 60/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.007)	Loss 2.9940e-01 (2.7781e-01)	Acc@1  89.84 ( 90.24)	Acc@5  99.22 ( 99.80)
Epoch: [26][ 70/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.007)	Loss 2.4160e-01 (2.7927e-01)	Acc@1  89.84 ( 90.13)	Acc@5 100.00 ( 99.77)
Epoch: [26][ 80/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.006)	Loss 3.1445e-01 (2.7928e-01)	Acc@1  90.62 ( 90.27)	Acc@5  99.22 ( 99.77)
Epoch: [26][ 90/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.006)	Loss 3.7323e-01 (2.7804e-01)	Acc@1  88.28 ( 90.34)	Acc@5 100.00 ( 99.78)
Epoch: [26][100/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.8360e-01 (2.7724e-01)	Acc@1  90.62 ( 90.41)	Acc@5 100.00 ( 99.76)
Epoch: [26][110/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.8508e-01 (2.7532e-01)	Acc@1  94.53 ( 90.44)	Acc@5 100.00 ( 99.77)
Epoch: [26][120/391]	Time  0.136 ( 0.138)	Data  0.002 ( 0.006)	Loss 2.6307e-01 (2.7549e-01)	Acc@1  90.62 ( 90.44)	Acc@5  99.22 ( 99.76)
Epoch: [26][130/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.8060e-01 (2.7309e-01)	Acc@1  94.53 ( 90.52)	Acc@5 100.00 ( 99.77)
Epoch: [26][140/391]	Time  0.133 ( 0.138)	Data  0.002 ( 0.006)	Loss 2.2141e-01 (2.7174e-01)	Acc@1  91.41 ( 90.52)	Acc@5  99.22 ( 99.77)
Epoch: [26][150/391]	Time  0.136 ( 0.138)	Data  0.002 ( 0.006)	Loss 2.8806e-01 (2.7332e-01)	Acc@1  91.41 ( 90.42)	Acc@5  99.22 ( 99.77)
Epoch: [26][160/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.9014e-01 (2.7305e-01)	Acc@1  93.75 ( 90.43)	Acc@5 100.00 ( 99.78)
Epoch: [26][170/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.6162e-01 (2.7427e-01)	Acc@1  92.19 ( 90.42)	Acc@5  99.22 ( 99.79)
Epoch: [26][180/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.3648e-01 (2.7487e-01)	Acc@1  89.84 ( 90.44)	Acc@5  98.44 ( 99.77)
Epoch: [26][190/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.3821e-01 (2.7510e-01)	Acc@1  87.50 ( 90.41)	Acc@5  99.22 ( 99.76)
Epoch: [26][200/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.4800e-01 (2.7309e-01)	Acc@1  92.97 ( 90.51)	Acc@5 100.00 ( 99.77)
Epoch: [26][210/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.0857e-01 (2.7363e-01)	Acc@1  93.75 ( 90.48)	Acc@5  99.22 ( 99.76)
Epoch: [26][220/391]	Time  0.134 ( 0.138)	Data  0.002 ( 0.005)	Loss 3.1731e-01 (2.7263e-01)	Acc@1  89.84 ( 90.55)	Acc@5  98.44 ( 99.75)
Epoch: [26][230/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.7196e-01 (2.7236e-01)	Acc@1  89.06 ( 90.56)	Acc@5  99.22 ( 99.75)
Epoch: [26][240/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.0836e-01 (2.7302e-01)	Acc@1  90.62 ( 90.52)	Acc@5 100.00 ( 99.75)
Epoch: [26][250/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.5534e-01 (2.7275e-01)	Acc@1  92.19 ( 90.53)	Acc@5 100.00 ( 99.74)
Epoch: [26][260/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.9685e-01 (2.7296e-01)	Acc@1  88.28 ( 90.53)	Acc@5 100.00 ( 99.74)
Epoch: [26][270/391]	Time  0.133 ( 0.138)	Data  0.002 ( 0.005)	Loss 3.7172e-01 (2.7484e-01)	Acc@1  89.84 ( 90.47)	Acc@5 100.00 ( 99.74)
Epoch: [26][280/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.5608e-01 (2.7614e-01)	Acc@1  89.06 ( 90.43)	Acc@5 100.00 ( 99.73)
Epoch: [26][290/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.5976e-01 (2.7674e-01)	Acc@1  89.06 ( 90.40)	Acc@5  99.22 ( 99.74)
Epoch: [26][300/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.6675e-01 (2.7632e-01)	Acc@1  89.84 ( 90.43)	Acc@5 100.00 ( 99.74)
Epoch: [26][310/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.2051e-01 (2.7633e-01)	Acc@1  89.06 ( 90.42)	Acc@5 100.00 ( 99.75)
Epoch: [26][320/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.6808e-01 (2.7692e-01)	Acc@1  90.62 ( 90.39)	Acc@5 100.00 ( 99.75)
Epoch: [26][330/391]	Time  0.135 ( 0.137)	Data  0.002 ( 0.005)	Loss 3.4530e-01 (2.7817e-01)	Acc@1  87.50 ( 90.35)	Acc@5 100.00 ( 99.75)
Epoch: [26][340/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.1686e-01 (2.7746e-01)	Acc@1  92.19 ( 90.38)	Acc@5  99.22 ( 99.75)
Epoch: [26][350/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.7831e-01 (2.7865e-01)	Acc@1  92.97 ( 90.33)	Acc@5 100.00 ( 99.75)
Epoch: [26][360/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.4735e-01 (2.7924e-01)	Acc@1  89.84 ( 90.30)	Acc@5 100.00 ( 99.75)
Epoch: [26][370/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.8150e-01 (2.7942e-01)	Acc@1  89.84 ( 90.31)	Acc@5 100.00 ( 99.75)
Epoch: [26][380/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.5026e-01 (2.7995e-01)	Acc@1  89.84 ( 90.31)	Acc@5  97.66 ( 99.75)
Epoch: [26][390/391]	Time  0.125 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.1168e-01 (2.8131e-01)	Acc@1  81.25 ( 90.24)	Acc@5  98.75 ( 99.75)
## e[26] optimizer.zero_grad (sum) time: 0.28566527366638184
## e[26]       loss.backward (sum) time: 18.551987409591675
## e[26]      optimizer.step (sum) time: 2.925436496734619
## epoch[26] training(only) time: 53.840808391571045
# Switched to evaluate mode...
Test: [  0/100]	Time  0.200 ( 0.200)	Loss 3.1483e-01 (3.1483e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.041 ( 0.057)	Loss 4.4139e-01 (3.8447e-01)	Acc@1  85.00 ( 87.00)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.043 ( 0.051)	Loss 4.6681e-01 (3.8764e-01)	Acc@1  84.00 ( 87.38)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.042 ( 0.048)	Loss 4.0384e-01 (3.9652e-01)	Acc@1  85.00 ( 87.16)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.046 ( 0.047)	Loss 4.5090e-01 (4.0015e-01)	Acc@1  84.00 ( 87.22)	Acc@5  99.00 ( 99.54)
Test: [ 50/100]	Time  0.044 ( 0.046)	Loss 1.6716e-01 (3.9028e-01)	Acc@1  95.00 ( 87.45)	Acc@5 100.00 ( 99.53)
Test: [ 60/100]	Time  0.052 ( 0.046)	Loss 3.6683e-01 (3.8741e-01)	Acc@1  91.00 ( 87.33)	Acc@5 100.00 ( 99.56)
Test: [ 70/100]	Time  0.045 ( 0.046)	Loss 5.0747e-01 (3.8650e-01)	Acc@1  84.00 ( 87.41)	Acc@5 100.00 ( 99.61)
Test: [ 80/100]	Time  0.043 ( 0.045)	Loss 2.8707e-01 (3.8333e-01)	Acc@1  90.00 ( 87.37)	Acc@5 100.00 ( 99.64)
Test: [ 90/100]	Time  0.043 ( 0.045)	Loss 2.4553e-01 (3.7961e-01)	Acc@1  91.00 ( 87.41)	Acc@5 100.00 ( 99.67)
 * Acc@1 87.480 Acc@5 99.680
### epoch[26] execution time: 58.470503091812134
EPOCH 27
# current learning rate: 0.1
# Switched to train mode...
Epoch: [27][  0/391]	Time  0.290 ( 0.290)	Data  0.171 ( 0.171)	Loss 2.2083e-01 (2.2083e-01)	Acc@1  91.41 ( 91.41)	Acc@5  99.22 ( 99.22)
Epoch: [27][ 10/391]	Time  0.133 ( 0.150)	Data  0.001 ( 0.020)	Loss 2.3678e-01 (2.5470e-01)	Acc@1  93.75 ( 90.84)	Acc@5 100.00 ( 99.79)
Epoch: [27][ 20/391]	Time  0.132 ( 0.143)	Data  0.001 ( 0.012)	Loss 2.1433e-01 (2.5455e-01)	Acc@1  92.19 ( 91.22)	Acc@5 100.00 ( 99.78)
Epoch: [27][ 30/391]	Time  0.136 ( 0.141)	Data  0.001 ( 0.010)	Loss 1.9960e-01 (2.5316e-01)	Acc@1  91.41 ( 91.28)	Acc@5 100.00 ( 99.82)
Epoch: [27][ 40/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.009)	Loss 4.1591e-01 (2.6195e-01)	Acc@1  83.59 ( 90.91)	Acc@5 100.00 ( 99.81)
Epoch: [27][ 50/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.008)	Loss 2.3441e-01 (2.5957e-01)	Acc@1  93.75 ( 91.10)	Acc@5 100.00 ( 99.85)
Epoch: [27][ 60/391]	Time  0.132 ( 0.139)	Data  0.001 ( 0.007)	Loss 2.5994e-01 (2.5747e-01)	Acc@1  91.41 ( 91.19)	Acc@5  99.22 ( 99.82)
Epoch: [27][ 70/391]	Time  0.133 ( 0.139)	Data  0.002 ( 0.007)	Loss 3.8268e-01 (2.6181e-01)	Acc@1  88.28 ( 91.10)	Acc@5 100.00 ( 99.81)
Epoch: [27][ 80/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.007)	Loss 2.2688e-01 (2.6449e-01)	Acc@1  90.62 ( 90.99)	Acc@5 100.00 ( 99.79)
Epoch: [27][ 90/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.006)	Loss 3.0936e-01 (2.6264e-01)	Acc@1  88.28 ( 91.12)	Acc@5 100.00 ( 99.79)
Epoch: [27][100/391]	Time  0.133 ( 0.138)	Data  0.002 ( 0.006)	Loss 2.4237e-01 (2.6350e-01)	Acc@1  93.75 ( 91.07)	Acc@5  99.22 ( 99.80)
Epoch: [27][110/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.2836e-01 (2.6517e-01)	Acc@1  92.97 ( 91.06)	Acc@5 100.00 ( 99.78)
Epoch: [27][120/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.8413e-01 (2.6529e-01)	Acc@1  85.94 ( 90.98)	Acc@5 100.00 ( 99.77)
Epoch: [27][130/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.9839e-01 (2.6610e-01)	Acc@1  89.84 ( 90.88)	Acc@5 100.00 ( 99.78)
Epoch: [27][140/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.5761e-01 (2.6729e-01)	Acc@1  88.28 ( 90.80)	Acc@5 100.00 ( 99.78)
Epoch: [27][150/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.2115e-01 (2.6808e-01)	Acc@1  85.94 ( 90.75)	Acc@5 100.00 ( 99.78)
Epoch: [27][160/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.0503e-01 (2.6923e-01)	Acc@1  85.94 ( 90.71)	Acc@5 100.00 ( 99.78)
Epoch: [27][170/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.4236e-01 (2.6754e-01)	Acc@1  90.62 ( 90.77)	Acc@5 100.00 ( 99.79)
Epoch: [27][180/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.2372e-01 (2.6737e-01)	Acc@1  90.62 ( 90.78)	Acc@5 100.00 ( 99.78)
Epoch: [27][190/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.4120e-01 (2.6736e-01)	Acc@1  91.41 ( 90.79)	Acc@5 100.00 ( 99.80)
Epoch: [27][200/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.3050e-01 (2.6925e-01)	Acc@1  89.84 ( 90.71)	Acc@5 100.00 ( 99.80)
Epoch: [27][210/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.0944e-01 (2.7043e-01)	Acc@1  90.62 ( 90.69)	Acc@5  99.22 ( 99.80)
Epoch: [27][220/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.6452e-01 (2.7145e-01)	Acc@1  88.28 ( 90.68)	Acc@5 100.00 ( 99.80)
Epoch: [27][230/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.5265e-01 (2.7097e-01)	Acc@1  90.62 ( 90.71)	Acc@5 100.00 ( 99.80)
Epoch: [27][240/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.1499e-01 (2.7063e-01)	Acc@1  92.19 ( 90.70)	Acc@5  99.22 ( 99.80)
Epoch: [27][250/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.9919e-01 (2.7074e-01)	Acc@1  92.97 ( 90.71)	Acc@5 100.00 ( 99.79)
Epoch: [27][260/391]	Time  0.129 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.6535e-01 (2.7137e-01)	Acc@1  89.84 ( 90.68)	Acc@5 100.00 ( 99.78)
Epoch: [27][270/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.6934e-01 (2.7302e-01)	Acc@1  86.72 ( 90.64)	Acc@5  99.22 ( 99.77)
Epoch: [27][280/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.6175e-01 (2.7315e-01)	Acc@1  90.62 ( 90.64)	Acc@5 100.00 ( 99.77)
Epoch: [27][290/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.2188e-01 (2.7441e-01)	Acc@1  90.62 ( 90.60)	Acc@5 100.00 ( 99.77)
Epoch: [27][300/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.6710e-01 (2.7509e-01)	Acc@1  89.84 ( 90.55)	Acc@5 100.00 ( 99.78)
Epoch: [27][310/391]	Time  0.129 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.8698e-01 (2.7494e-01)	Acc@1  94.53 ( 90.53)	Acc@5  99.22 ( 99.77)
Epoch: [27][320/391]	Time  0.129 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.4838e-01 (2.7465e-01)	Acc@1  88.28 ( 90.54)	Acc@5 100.00 ( 99.78)
Epoch: [27][330/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.1517e-01 (2.7478e-01)	Acc@1  92.97 ( 90.54)	Acc@5 100.00 ( 99.78)
Epoch: [27][340/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.005)	Loss 5.0454e-01 (2.7592e-01)	Acc@1  83.59 ( 90.49)	Acc@5  98.44 ( 99.78)
Epoch: [27][350/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.5163e-01 (2.7677e-01)	Acc@1  88.28 ( 90.46)	Acc@5 100.00 ( 99.78)
Epoch: [27][360/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.8497e-01 (2.7764e-01)	Acc@1  92.19 ( 90.43)	Acc@5  99.22 ( 99.77)
Epoch: [27][370/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.5168e-01 (2.7705e-01)	Acc@1  93.75 ( 90.47)	Acc@5 100.00 ( 99.78)
Epoch: [27][380/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.9829e-01 (2.7769e-01)	Acc@1  92.19 ( 90.44)	Acc@5 100.00 ( 99.78)
Epoch: [27][390/391]	Time  0.126 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.1816e-01 (2.7692e-01)	Acc@1  90.00 ( 90.47)	Acc@5 100.00 ( 99.78)
## e[27] optimizer.zero_grad (sum) time: 0.288362979888916
## e[27]       loss.backward (sum) time: 18.587177991867065
## e[27]      optimizer.step (sum) time: 2.934361696243286
## epoch[27] training(only) time: 53.92418885231018
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 3.2798e-01 (3.2798e-01)	Acc@1  87.00 ( 87.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.042 ( 0.056)	Loss 4.7382e-01 (3.9442e-01)	Acc@1  88.00 ( 88.27)	Acc@5  99.00 ( 99.27)
Test: [ 20/100]	Time  0.041 ( 0.050)	Loss 4.3591e-01 (4.1581e-01)	Acc@1  84.00 ( 87.05)	Acc@5 100.00 ( 99.43)
Test: [ 30/100]	Time  0.045 ( 0.048)	Loss 5.4273e-01 (4.1282e-01)	Acc@1  79.00 ( 86.87)	Acc@5 100.00 ( 99.42)
Test: [ 40/100]	Time  0.043 ( 0.047)	Loss 6.1422e-01 (4.1648e-01)	Acc@1  82.00 ( 86.95)	Acc@5  98.00 ( 99.34)
Test: [ 50/100]	Time  0.043 ( 0.046)	Loss 3.0553e-01 (4.0793e-01)	Acc@1  89.00 ( 87.12)	Acc@5  99.00 ( 99.37)
Test: [ 60/100]	Time  0.043 ( 0.046)	Loss 3.7492e-01 (4.0430e-01)	Acc@1  89.00 ( 87.26)	Acc@5  99.00 ( 99.43)
Test: [ 70/100]	Time  0.044 ( 0.045)	Loss 4.0334e-01 (4.0162e-01)	Acc@1  88.00 ( 87.31)	Acc@5 100.00 ( 99.44)
Test: [ 80/100]	Time  0.041 ( 0.045)	Loss 3.1176e-01 (3.9944e-01)	Acc@1  89.00 ( 87.36)	Acc@5 100.00 ( 99.42)
Test: [ 90/100]	Time  0.045 ( 0.045)	Loss 3.0762e-01 (3.9646e-01)	Acc@1  88.00 ( 87.36)	Acc@5 100.00 ( 99.45)
 * Acc@1 87.550 Acc@5 99.500
### epoch[27] execution time: 58.51924133300781
EPOCH 28
# current learning rate: 0.1
# Switched to train mode...
Epoch: [28][  0/391]	Time  0.290 ( 0.290)	Data  0.170 ( 0.170)	Loss 4.1090e-01 (4.1090e-01)	Acc@1  82.03 ( 82.03)	Acc@5 100.00 (100.00)
Epoch: [28][ 10/391]	Time  0.138 ( 0.150)	Data  0.001 ( 0.019)	Loss 3.1939e-01 (2.6571e-01)	Acc@1  89.84 ( 91.19)	Acc@5 100.00 ( 99.93)
Epoch: [28][ 20/391]	Time  0.134 ( 0.143)	Data  0.001 ( 0.012)	Loss 3.1594e-01 (2.6854e-01)	Acc@1  90.62 ( 90.96)	Acc@5 100.00 ( 99.81)
Epoch: [28][ 30/391]	Time  0.131 ( 0.141)	Data  0.002 ( 0.010)	Loss 2.1393e-01 (2.6909e-01)	Acc@1  93.75 ( 90.85)	Acc@5 100.00 ( 99.87)
Epoch: [28][ 40/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.008)	Loss 2.4565e-01 (2.5510e-01)	Acc@1  92.97 ( 91.41)	Acc@5  99.22 ( 99.87)
Epoch: [28][ 50/391]	Time  0.135 ( 0.139)	Data  0.002 ( 0.008)	Loss 1.8063e-01 (2.4970e-01)	Acc@1  92.97 ( 91.50)	Acc@5  99.22 ( 99.83)
Epoch: [28][ 60/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.007)	Loss 2.3151e-01 (2.4348e-01)	Acc@1  92.19 ( 91.66)	Acc@5 100.00 ( 99.86)
Epoch: [28][ 70/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.007)	Loss 2.9242e-01 (2.4699e-01)	Acc@1  89.06 ( 91.57)	Acc@5  99.22 ( 99.85)
Epoch: [28][ 80/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.0765e-01 (2.4877e-01)	Acc@1  90.62 ( 91.48)	Acc@5  99.22 ( 99.80)
Epoch: [28][ 90/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.2203e-01 (2.4906e-01)	Acc@1  89.84 ( 91.41)	Acc@5  99.22 ( 99.80)
Epoch: [28][100/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.6734e-01 (2.5111e-01)	Acc@1  87.50 ( 91.24)	Acc@5  98.44 ( 99.78)
Epoch: [28][110/391]	Time  0.131 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.2845e-01 (2.4836e-01)	Acc@1  90.62 ( 91.33)	Acc@5  99.22 ( 99.79)
Epoch: [28][120/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.2761e-01 (2.5256e-01)	Acc@1  91.41 ( 91.20)	Acc@5 100.00 ( 99.80)
Epoch: [28][130/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.5707e-01 (2.5536e-01)	Acc@1  89.84 ( 91.12)	Acc@5 100.00 ( 99.80)
Epoch: [28][140/391]	Time  0.130 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.2882e-01 (2.5460e-01)	Acc@1  92.19 ( 91.11)	Acc@5  99.22 ( 99.78)
Epoch: [28][150/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.7151e-01 (2.5626e-01)	Acc@1  89.84 ( 91.02)	Acc@5 100.00 ( 99.77)
Epoch: [28][160/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.7535e-01 (2.5438e-01)	Acc@1  90.62 ( 91.13)	Acc@5  99.22 ( 99.78)
Epoch: [28][170/391]	Time  0.131 ( 0.138)	Data  0.001 ( 0.005)	Loss 4.0363e-01 (2.5533e-01)	Acc@1  86.72 ( 91.15)	Acc@5  99.22 ( 99.77)
Epoch: [28][180/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.4314e-01 (2.5479e-01)	Acc@1  96.09 ( 91.13)	Acc@5 100.00 ( 99.77)
Epoch: [28][190/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.2929e-01 (2.5397e-01)	Acc@1  92.19 ( 91.12)	Acc@5 100.00 ( 99.77)
Epoch: [28][200/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.2896e-01 (2.5340e-01)	Acc@1  88.28 ( 91.14)	Acc@5 100.00 ( 99.77)
Epoch: [28][210/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.7290e-01 (2.5257e-01)	Acc@1  92.97 ( 91.18)	Acc@5 100.00 ( 99.78)
Epoch: [28][220/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.9249e-01 (2.5239e-01)	Acc@1  88.28 ( 91.21)	Acc@5 100.00 ( 99.78)
Epoch: [28][230/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.9727e-01 (2.5378e-01)	Acc@1  91.41 ( 91.16)	Acc@5 100.00 ( 99.79)
Epoch: [28][240/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.3282e-01 (2.5383e-01)	Acc@1  93.75 ( 91.18)	Acc@5 100.00 ( 99.79)
Epoch: [28][250/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.1977e-01 (2.5229e-01)	Acc@1  92.19 ( 91.23)	Acc@5 100.00 ( 99.80)
Epoch: [28][260/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.3407e-01 (2.5224e-01)	Acc@1  90.62 ( 91.19)	Acc@5 100.00 ( 99.80)
Epoch: [28][270/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.2665e-01 (2.5417e-01)	Acc@1  92.19 ( 91.14)	Acc@5 100.00 ( 99.80)
Epoch: [28][280/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.1815e-01 (2.5377e-01)	Acc@1  92.97 ( 91.15)	Acc@5 100.00 ( 99.81)
Epoch: [28][290/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.0355e-01 (2.5429e-01)	Acc@1  92.97 ( 91.13)	Acc@5 100.00 ( 99.81)
Epoch: [28][300/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.8117e-01 (2.5489e-01)	Acc@1  94.53 ( 91.11)	Acc@5 100.00 ( 99.81)
Epoch: [28][310/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.8168e-01 (2.5579e-01)	Acc@1  85.94 ( 91.06)	Acc@5 100.00 ( 99.81)
Epoch: [28][320/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.8295e-01 (2.5587e-01)	Acc@1  89.84 ( 91.07)	Acc@5 100.00 ( 99.82)
Epoch: [28][330/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.3626e-01 (2.5688e-01)	Acc@1  91.41 ( 91.05)	Acc@5  99.22 ( 99.81)
Epoch: [28][340/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.8746e-01 (2.5838e-01)	Acc@1  84.38 ( 91.00)	Acc@5 100.00 ( 99.81)
Epoch: [28][350/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.3381e-01 (2.6027e-01)	Acc@1  87.50 ( 90.93)	Acc@5  99.22 ( 99.81)
Epoch: [28][360/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.7885e-01 (2.6087e-01)	Acc@1  88.28 ( 90.94)	Acc@5 100.00 ( 99.81)
Epoch: [28][370/391]	Time  0.128 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.4964e-01 (2.6172e-01)	Acc@1  82.03 ( 90.90)	Acc@5  99.22 ( 99.81)
Epoch: [28][380/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.3517e-01 (2.6158e-01)	Acc@1  94.53 ( 90.90)	Acc@5 100.00 ( 99.81)
Epoch: [28][390/391]	Time  0.130 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.2892e-01 (2.6120e-01)	Acc@1  92.50 ( 90.91)	Acc@5  98.75 ( 99.80)
## e[28] optimizer.zero_grad (sum) time: 0.28684020042419434
## e[28]       loss.backward (sum) time: 18.557037115097046
## e[28]      optimizer.step (sum) time: 2.8987746238708496
## epoch[28] training(only) time: 53.86011362075806
# Switched to evaluate mode...
Test: [  0/100]	Time  0.206 ( 0.206)	Loss 2.6389e-01 (2.6389e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.042 ( 0.058)	Loss 3.2720e-01 (3.5740e-01)	Acc@1  90.00 ( 88.27)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.042 ( 0.051)	Loss 4.6462e-01 (3.8116e-01)	Acc@1  87.00 ( 87.62)	Acc@5  98.00 ( 99.57)
Test: [ 30/100]	Time  0.044 ( 0.049)	Loss 4.7874e-01 (4.0385e-01)	Acc@1  84.00 ( 87.39)	Acc@5  98.00 ( 99.45)
Test: [ 40/100]	Time  0.051 ( 0.048)	Loss 5.2673e-01 (4.0475e-01)	Acc@1  83.00 ( 87.32)	Acc@5  99.00 ( 99.46)
Test: [ 50/100]	Time  0.045 ( 0.047)	Loss 2.3179e-01 (4.1090e-01)	Acc@1  90.00 ( 87.20)	Acc@5  99.00 ( 99.43)
Test: [ 60/100]	Time  0.044 ( 0.046)	Loss 4.1477e-01 (4.0980e-01)	Acc@1  87.00 ( 87.10)	Acc@5 100.00 ( 99.48)
Test: [ 70/100]	Time  0.043 ( 0.046)	Loss 4.8763e-01 (4.1139e-01)	Acc@1  89.00 ( 87.07)	Acc@5 100.00 ( 99.52)
Test: [ 80/100]	Time  0.043 ( 0.045)	Loss 3.4724e-01 (4.1175e-01)	Acc@1  88.00 ( 87.01)	Acc@5 100.00 ( 99.52)
Test: [ 90/100]	Time  0.048 ( 0.045)	Loss 2.1050e-01 (4.1094e-01)	Acc@1  90.00 ( 86.99)	Acc@5 100.00 ( 99.53)
 * Acc@1 86.960 Acc@5 99.530
### epoch[28] execution time: 58.4787712097168
EPOCH 29
# current learning rate: 0.1
# Switched to train mode...
Epoch: [29][  0/391]	Time  0.300 ( 0.300)	Data  0.182 ( 0.182)	Loss 1.6896e-01 (1.6896e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [29][ 10/391]	Time  0.134 ( 0.150)	Data  0.001 ( 0.020)	Loss 2.1846e-01 (2.1003e-01)	Acc@1  93.75 ( 92.97)	Acc@5 100.00 ( 99.93)
Epoch: [29][ 20/391]	Time  0.134 ( 0.143)	Data  0.001 ( 0.013)	Loss 3.5868e-01 (2.2660e-01)	Acc@1  86.72 ( 91.96)	Acc@5 100.00 ( 99.96)
Epoch: [29][ 30/391]	Time  0.135 ( 0.141)	Data  0.001 ( 0.010)	Loss 2.9519e-01 (2.3298e-01)	Acc@1  91.41 ( 91.78)	Acc@5  99.22 ( 99.87)
Epoch: [29][ 40/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.009)	Loss 1.8201e-01 (2.3472e-01)	Acc@1  92.19 ( 91.73)	Acc@5 100.00 ( 99.85)
Epoch: [29][ 50/391]	Time  0.132 ( 0.139)	Data  0.001 ( 0.008)	Loss 2.6998e-01 (2.3649e-01)	Acc@1  87.50 ( 91.56)	Acc@5 100.00 ( 99.83)
Epoch: [29][ 60/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.007)	Loss 3.7023e-01 (2.4252e-01)	Acc@1  89.06 ( 91.42)	Acc@5 100.00 ( 99.82)
Epoch: [29][ 70/391]	Time  0.131 ( 0.139)	Data  0.001 ( 0.007)	Loss 1.5765e-01 (2.4159e-01)	Acc@1  93.75 ( 91.38)	Acc@5 100.00 ( 99.82)
Epoch: [29][ 80/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.007)	Loss 2.2886e-01 (2.4048e-01)	Acc@1  93.75 ( 91.53)	Acc@5  99.22 ( 99.82)
Epoch: [29][ 90/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.3967e-01 (2.3882e-01)	Acc@1  92.97 ( 91.60)	Acc@5 100.00 ( 99.84)
Epoch: [29][100/391]	Time  0.129 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.4876e-01 (2.3496e-01)	Acc@1  92.97 ( 91.75)	Acc@5 100.00 ( 99.85)
Epoch: [29][110/391]	Time  0.129 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.4089e-01 (2.3413e-01)	Acc@1  92.97 ( 91.81)	Acc@5 100.00 ( 99.85)
Epoch: [29][120/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.1057e-01 (2.3418e-01)	Acc@1  90.62 ( 91.81)	Acc@5  99.22 ( 99.85)
Epoch: [29][130/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.5714e-01 (2.3915e-01)	Acc@1  89.84 ( 91.64)	Acc@5 100.00 ( 99.83)
Epoch: [29][140/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.2212e-01 (2.4147e-01)	Acc@1  86.72 ( 91.57)	Acc@5 100.00 ( 99.84)
Epoch: [29][150/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.3159e-01 (2.4530e-01)	Acc@1  86.72 ( 91.45)	Acc@5 100.00 ( 99.83)
Epoch: [29][160/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.9797e-01 (2.4641e-01)	Acc@1  93.75 ( 91.46)	Acc@5 100.00 ( 99.83)
Epoch: [29][170/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.4155e-01 (2.4625e-01)	Acc@1  88.28 ( 91.53)	Acc@5 100.00 ( 99.84)
Epoch: [29][180/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.2017e-01 (2.4583e-01)	Acc@1  88.28 ( 91.55)	Acc@5 100.00 ( 99.83)
Epoch: [29][190/391]	Time  0.137 ( 0.138)	Data  0.002 ( 0.005)	Loss 2.8917e-01 (2.4743e-01)	Acc@1  89.06 ( 91.46)	Acc@5 100.00 ( 99.84)
Epoch: [29][200/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.8634e-01 (2.4749e-01)	Acc@1  92.97 ( 91.44)	Acc@5  99.22 ( 99.83)
Epoch: [29][210/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.2213e-01 (2.4720e-01)	Acc@1  90.62 ( 91.45)	Acc@5 100.00 ( 99.83)
Epoch: [29][220/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.9231e-01 (2.4822e-01)	Acc@1  93.75 ( 91.43)	Acc@5 100.00 ( 99.82)
Epoch: [29][230/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.7099e-01 (2.4792e-01)	Acc@1  89.06 ( 91.43)	Acc@5 100.00 ( 99.81)
Epoch: [29][240/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.3374e-01 (2.4865e-01)	Acc@1  87.50 ( 91.40)	Acc@5 100.00 ( 99.82)
Epoch: [29][250/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.6805e-01 (2.5032e-01)	Acc@1  89.84 ( 91.33)	Acc@5 100.00 ( 99.82)
Epoch: [29][260/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.6109e-01 (2.5016e-01)	Acc@1  89.84 ( 91.33)	Acc@5 100.00 ( 99.81)
Epoch: [29][270/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.0333e-01 (2.4939e-01)	Acc@1  91.41 ( 91.36)	Acc@5  99.22 ( 99.82)
Epoch: [29][280/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.6703e-01 (2.4907e-01)	Acc@1  94.53 ( 91.37)	Acc@5 100.00 ( 99.82)
Epoch: [29][290/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.2046e-01 (2.4955e-01)	Acc@1  90.62 ( 91.36)	Acc@5  97.66 ( 99.81)
Epoch: [29][300/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.1773e-01 (2.5321e-01)	Acc@1  90.62 ( 91.26)	Acc@5 100.00 ( 99.81)
Epoch: [29][310/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.7637e-01 (2.5262e-01)	Acc@1  91.41 ( 91.28)	Acc@5 100.00 ( 99.81)
Epoch: [29][320/391]	Time  0.136 ( 0.137)	Data  0.002 ( 0.005)	Loss 2.1876e-01 (2.5299e-01)	Acc@1  92.19 ( 91.27)	Acc@5 100.00 ( 99.81)
Epoch: [29][330/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.4790e-01 (2.5301e-01)	Acc@1  91.41 ( 91.26)	Acc@5 100.00 ( 99.81)
Epoch: [29][340/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.2272e-01 (2.5266e-01)	Acc@1  93.75 ( 91.27)	Acc@5 100.00 ( 99.81)
Epoch: [29][350/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.3059e-01 (2.5402e-01)	Acc@1  86.72 ( 91.23)	Acc@5 100.00 ( 99.81)
Epoch: [29][360/391]	Time  0.132 ( 0.137)	Data  0.002 ( 0.005)	Loss 2.0425e-01 (2.5396e-01)	Acc@1  92.19 ( 91.22)	Acc@5 100.00 ( 99.81)
Epoch: [29][370/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.4832e-01 (2.5485e-01)	Acc@1  90.62 ( 91.17)	Acc@5  99.22 ( 99.81)
Epoch: [29][380/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.6339e-01 (2.5518e-01)	Acc@1  92.97 ( 91.18)	Acc@5 100.00 ( 99.81)
Epoch: [29][390/391]	Time  0.127 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.6645e-01 (2.5475e-01)	Acc@1  92.50 ( 91.20)	Acc@5 100.00 ( 99.81)
## e[29] optimizer.zero_grad (sum) time: 0.2850790023803711
## e[29]       loss.backward (sum) time: 18.503222465515137
## e[29]      optimizer.step (sum) time: 2.860809803009033
## epoch[29] training(only) time: 53.84420895576477
# Switched to evaluate mode...
Test: [  0/100]	Time  0.196 ( 0.196)	Loss 2.6913e-01 (2.6913e-01)	Acc@1  91.00 ( 91.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.042 ( 0.058)	Loss 4.2232e-01 (3.7599e-01)	Acc@1  89.00 ( 88.18)	Acc@5  99.00 ( 99.45)
Test: [ 20/100]	Time  0.042 ( 0.051)	Loss 5.5549e-01 (3.9685e-01)	Acc@1  82.00 ( 87.43)	Acc@5  99.00 ( 99.33)
Test: [ 30/100]	Time  0.041 ( 0.048)	Loss 3.7172e-01 (4.0598e-01)	Acc@1  87.00 ( 87.35)	Acc@5 100.00 ( 99.32)
Test: [ 40/100]	Time  0.045 ( 0.047)	Loss 3.8902e-01 (4.0163e-01)	Acc@1  87.00 ( 87.39)	Acc@5 100.00 ( 99.34)
Test: [ 50/100]	Time  0.041 ( 0.046)	Loss 3.5775e-01 (4.0411e-01)	Acc@1  84.00 ( 87.35)	Acc@5 100.00 ( 99.41)
Test: [ 60/100]	Time  0.042 ( 0.046)	Loss 4.4802e-01 (4.0823e-01)	Acc@1  90.00 ( 87.34)	Acc@5 100.00 ( 99.43)
Test: [ 70/100]	Time  0.043 ( 0.045)	Loss 5.8170e-01 (4.0940e-01)	Acc@1  84.00 ( 87.18)	Acc@5  99.00 ( 99.45)
Test: [ 80/100]	Time  0.043 ( 0.045)	Loss 2.5333e-01 (4.0367e-01)	Acc@1  92.00 ( 87.28)	Acc@5 100.00 ( 99.47)
Test: [ 90/100]	Time  0.046 ( 0.045)	Loss 2.2267e-01 (4.0533e-01)	Acc@1  92.00 ( 87.21)	Acc@5 100.00 ( 99.51)
 * Acc@1 87.290 Acc@5 99.530
### epoch[29] execution time: 58.502501249313354
EPOCH 30
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [30][  0/391]	Time  0.301 ( 0.301)	Data  0.183 ( 0.183)	Loss 2.6071e-01 (2.6071e-01)	Acc@1  87.50 ( 87.50)	Acc@5 100.00 (100.00)
Epoch: [30][ 10/391]	Time  0.135 ( 0.151)	Data  0.001 ( 0.021)	Loss 1.5037e-01 (2.4325e-01)	Acc@1  94.53 ( 90.77)	Acc@5 100.00 ( 99.93)
Epoch: [30][ 20/391]	Time  0.135 ( 0.144)	Data  0.001 ( 0.013)	Loss 1.8777e-01 (2.2501e-01)	Acc@1  94.53 ( 91.67)	Acc@5 100.00 ( 99.96)
Epoch: [30][ 30/391]	Time  0.138 ( 0.141)	Data  0.001 ( 0.010)	Loss 1.8219e-01 (2.0405e-01)	Acc@1  93.75 ( 92.52)	Acc@5 100.00 ( 99.97)
Epoch: [30][ 40/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.009)	Loss 9.9361e-02 (2.0644e-01)	Acc@1  96.09 ( 92.42)	Acc@5 100.00 ( 99.94)
Epoch: [30][ 50/391]	Time  0.132 ( 0.140)	Data  0.001 ( 0.008)	Loss 1.0559e-01 (1.9884e-01)	Acc@1  96.88 ( 92.74)	Acc@5 100.00 ( 99.94)
Epoch: [30][ 60/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.007)	Loss 1.1144e-01 (1.9279e-01)	Acc@1  96.09 ( 93.05)	Acc@5 100.00 ( 99.91)
Epoch: [30][ 70/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.007)	Loss 1.6114e-01 (1.8856e-01)	Acc@1  93.75 ( 93.16)	Acc@5 100.00 ( 99.92)
Epoch: [30][ 80/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.007)	Loss 1.6136e-01 (1.8536e-01)	Acc@1  95.31 ( 93.31)	Acc@5 100.00 ( 99.93)
Epoch: [30][ 90/391]	Time  0.132 ( 0.139)	Data  0.001 ( 0.006)	Loss 1.3214e-01 (1.8504e-01)	Acc@1  96.09 ( 93.35)	Acc@5 100.00 ( 99.93)
Epoch: [30][100/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.9026e-01 (1.8422e-01)	Acc@1  93.75 ( 93.47)	Acc@5 100.00 ( 99.93)
Epoch: [30][110/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.2439e-01 (1.8320e-01)	Acc@1  89.06 ( 93.48)	Acc@5 100.00 ( 99.93)
Epoch: [30][120/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.7274e-01 (1.8222e-01)	Acc@1  94.53 ( 93.53)	Acc@5  99.22 ( 99.92)
Epoch: [30][130/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.6829e-01 (1.8101e-01)	Acc@1  92.19 ( 93.57)	Acc@5 100.00 ( 99.91)
Epoch: [30][140/391]	Time  0.133 ( 0.138)	Data  0.002 ( 0.006)	Loss 1.2661e-01 (1.7975e-01)	Acc@1  96.88 ( 93.59)	Acc@5 100.00 ( 99.92)
Epoch: [30][150/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.3027e-01 (1.7812e-01)	Acc@1  96.09 ( 93.68)	Acc@5 100.00 ( 99.92)
Epoch: [30][160/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.1643e-01 (1.7616e-01)	Acc@1  96.88 ( 93.77)	Acc@5 100.00 ( 99.92)
Epoch: [30][170/391]	Time  0.128 ( 0.138)	Data  0.001 ( 0.005)	Loss 9.5416e-02 (1.7483e-01)	Acc@1  98.44 ( 93.80)	Acc@5 100.00 ( 99.92)
Epoch: [30][180/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.005)	Loss 6.4912e-02 (1.7238e-01)	Acc@1  98.44 ( 93.88)	Acc@5 100.00 ( 99.91)
Epoch: [30][190/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.005)	Loss 9.0063e-02 (1.7045e-01)	Acc@1  97.66 ( 93.97)	Acc@5 100.00 ( 99.92)
Epoch: [30][200/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.005)	Loss 8.0720e-02 (1.6849e-01)	Acc@1  97.66 ( 94.05)	Acc@5 100.00 ( 99.91)
Epoch: [30][210/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.7569e-01 (1.6726e-01)	Acc@1  93.75 ( 94.09)	Acc@5 100.00 ( 99.91)
Epoch: [30][220/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.4813e-01 (1.6578e-01)	Acc@1  93.75 ( 94.15)	Acc@5 100.00 ( 99.92)
Epoch: [30][230/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.6016e-01 (1.6468e-01)	Acc@1  95.31 ( 94.21)	Acc@5  99.22 ( 99.92)
Epoch: [30][240/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.6072e-01 (1.6406e-01)	Acc@1  95.31 ( 94.23)	Acc@5 100.00 ( 99.92)
Epoch: [30][250/391]	Time  0.131 ( 0.138)	Data  0.002 ( 0.005)	Loss 1.5617e-01 (1.6362e-01)	Acc@1  95.31 ( 94.28)	Acc@5 100.00 ( 99.92)
Epoch: [30][260/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.3274e-01 (1.6242e-01)	Acc@1  96.09 ( 94.35)	Acc@5 100.00 ( 99.92)
Epoch: [30][270/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.5544e-01 (1.6160e-01)	Acc@1  93.75 ( 94.38)	Acc@5  99.22 ( 99.91)
Epoch: [30][280/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.6003e-01 (1.6007e-01)	Acc@1  93.75 ( 94.43)	Acc@5 100.00 ( 99.92)
Epoch: [30][290/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.4242e-02 (1.5898e-01)	Acc@1  96.88 ( 94.46)	Acc@5 100.00 ( 99.91)
Epoch: [30][300/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.4261e-01 (1.5869e-01)	Acc@1  95.31 ( 94.47)	Acc@5 100.00 ( 99.91)
Epoch: [30][310/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.5014e-01 (1.5855e-01)	Acc@1  93.75 ( 94.48)	Acc@5 100.00 ( 99.91)
Epoch: [30][320/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0030e-01 (1.5766e-01)	Acc@1  96.88 ( 94.50)	Acc@5 100.00 ( 99.91)
Epoch: [30][330/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.2793e-01 (1.5696e-01)	Acc@1  94.53 ( 94.52)	Acc@5 100.00 ( 99.92)
Epoch: [30][340/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.8983e-01 (1.5697e-01)	Acc@1  92.19 ( 94.53)	Acc@5 100.00 ( 99.91)
Epoch: [30][350/391]	Time  0.139 ( 0.137)	Data  0.002 ( 0.005)	Loss 1.3062e-01 (1.5648e-01)	Acc@1  96.09 ( 94.55)	Acc@5 100.00 ( 99.91)
Epoch: [30][360/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.4760e-02 (1.5580e-01)	Acc@1  96.09 ( 94.58)	Acc@5 100.00 ( 99.91)
Epoch: [30][370/391]	Time  0.141 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.0912e-01 (1.5487e-01)	Acc@1  93.75 ( 94.62)	Acc@5  99.22 ( 99.91)
Epoch: [30][380/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.2054e-02 (1.5396e-01)	Acc@1  96.88 ( 94.66)	Acc@5 100.00 ( 99.91)
Epoch: [30][390/391]	Time  0.127 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.3345e-02 (1.5367e-01)	Acc@1  98.75 ( 94.67)	Acc@5 100.00 ( 99.92)
## e[30] optimizer.zero_grad (sum) time: 0.28737521171569824
## e[30]       loss.backward (sum) time: 18.594455242156982
## e[30]      optimizer.step (sum) time: 2.9471657276153564
## epoch[30] training(only) time: 53.83966946601868
# Switched to evaluate mode...
Test: [  0/100]	Time  0.201 ( 0.201)	Loss 1.6908e-01 (1.6908e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.043 ( 0.058)	Loss 3.2544e-01 (2.5389e-01)	Acc@1  86.00 ( 91.73)	Acc@5  99.00 ( 99.45)
Test: [ 20/100]	Time  0.041 ( 0.051)	Loss 3.4917e-01 (2.7334e-01)	Acc@1  89.00 ( 91.29)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.044 ( 0.049)	Loss 2.1400e-01 (2.7730e-01)	Acc@1  90.00 ( 91.42)	Acc@5 100.00 ( 99.61)
Test: [ 40/100]	Time  0.045 ( 0.047)	Loss 1.8931e-01 (2.7582e-01)	Acc@1  89.00 ( 91.24)	Acc@5 100.00 ( 99.66)
Test: [ 50/100]	Time  0.042 ( 0.047)	Loss 1.7851e-01 (2.7893e-01)	Acc@1  93.00 ( 91.22)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.042 ( 0.046)	Loss 2.4014e-01 (2.7733e-01)	Acc@1  94.00 ( 91.25)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.041 ( 0.046)	Loss 4.2012e-01 (2.7626e-01)	Acc@1  87.00 ( 91.21)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.043 ( 0.045)	Loss 2.3178e-01 (2.7637e-01)	Acc@1  92.00 ( 91.26)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.043 ( 0.045)	Loss 1.4530e-01 (2.7410e-01)	Acc@1  94.00 ( 91.26)	Acc@5 100.00 ( 99.76)
 * Acc@1 91.340 Acc@5 99.770
### epoch[30] execution time: 58.44951152801514
EPOCH 31
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [31][  0/391]	Time  0.301 ( 0.301)	Data  0.182 ( 0.182)	Loss 1.1723e-01 (1.1723e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [31][ 10/391]	Time  0.133 ( 0.151)	Data  0.001 ( 0.020)	Loss 7.2397e-02 (1.1620e-01)	Acc@1  98.44 ( 95.88)	Acc@5 100.00 (100.00)
Epoch: [31][ 20/391]	Time  0.136 ( 0.144)	Data  0.001 ( 0.013)	Loss 8.5963e-02 (1.2562e-01)	Acc@1  96.88 ( 95.57)	Acc@5 100.00 ( 99.96)
Epoch: [31][ 30/391]	Time  0.136 ( 0.141)	Data  0.001 ( 0.010)	Loss 1.9273e-01 (1.2902e-01)	Acc@1  94.53 ( 95.44)	Acc@5 100.00 ( 99.95)
Epoch: [31][ 40/391]	Time  0.135 ( 0.140)	Data  0.002 ( 0.009)	Loss 1.1171e-01 (1.3057e-01)	Acc@1  96.09 ( 95.39)	Acc@5 100.00 ( 99.96)
Epoch: [31][ 50/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.008)	Loss 9.7262e-02 (1.2372e-01)	Acc@1  96.09 ( 95.68)	Acc@5 100.00 ( 99.95)
Epoch: [31][ 60/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.007)	Loss 1.8873e-01 (1.2661e-01)	Acc@1  96.09 ( 95.65)	Acc@5 100.00 ( 99.96)
Epoch: [31][ 70/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.007)	Loss 1.0742e-01 (1.2406e-01)	Acc@1  96.09 ( 95.76)	Acc@5 100.00 ( 99.96)
Epoch: [31][ 80/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.007)	Loss 1.5380e-01 (1.2298e-01)	Acc@1  95.31 ( 95.85)	Acc@5 100.00 ( 99.95)
Epoch: [31][ 90/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.006)	Loss 1.1484e-01 (1.2220e-01)	Acc@1  94.53 ( 95.87)	Acc@5 100.00 ( 99.95)
Epoch: [31][100/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.3744e-01 (1.2239e-01)	Acc@1  94.53 ( 95.90)	Acc@5 100.00 ( 99.95)
Epoch: [31][110/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.1607e-01 (1.2359e-01)	Acc@1  97.66 ( 95.88)	Acc@5 100.00 ( 99.96)
Epoch: [31][120/391]	Time  0.136 ( 0.138)	Data  0.002 ( 0.006)	Loss 9.8753e-02 (1.2245e-01)	Acc@1  96.88 ( 95.95)	Acc@5 100.00 ( 99.96)
Epoch: [31][130/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.4557e-01 (1.2345e-01)	Acc@1  94.53 ( 95.93)	Acc@5 100.00 ( 99.96)
Epoch: [31][140/391]	Time  0.130 ( 0.138)	Data  0.001 ( 0.006)	Loss 6.3414e-02 (1.2227e-01)	Acc@1  97.66 ( 95.96)	Acc@5 100.00 ( 99.96)
Epoch: [31][150/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.4680e-02 (1.2192e-01)	Acc@1 100.00 ( 95.97)	Acc@5 100.00 ( 99.96)
Epoch: [31][160/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.1959e-01 (1.2121e-01)	Acc@1  93.75 ( 96.02)	Acc@5 100.00 ( 99.97)
Epoch: [31][170/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.1265e-01 (1.2114e-01)	Acc@1  96.09 ( 96.01)	Acc@5 100.00 ( 99.97)
Epoch: [31][180/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.8899e-02 (1.2040e-01)	Acc@1  96.88 ( 96.01)	Acc@5 100.00 ( 99.97)
Epoch: [31][190/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.2646e-01 (1.2092e-01)	Acc@1  95.31 ( 96.00)	Acc@5 100.00 ( 99.96)
Epoch: [31][200/391]	Time  0.129 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.6860e-02 (1.1965e-01)	Acc@1  96.09 ( 96.04)	Acc@5 100.00 ( 99.96)
Epoch: [31][210/391]	Time  0.130 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3140e-01 (1.1929e-01)	Acc@1  95.31 ( 96.02)	Acc@5 100.00 ( 99.96)
Epoch: [31][220/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.2685e-02 (1.1835e-01)	Acc@1  96.09 ( 96.04)	Acc@5 100.00 ( 99.96)
Epoch: [31][230/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.4492e-02 (1.1811e-01)	Acc@1  96.88 ( 96.03)	Acc@5 100.00 ( 99.97)
Epoch: [31][240/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0716e-01 (1.1877e-01)	Acc@1  95.31 ( 95.97)	Acc@5 100.00 ( 99.96)
Epoch: [31][250/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3630e-01 (1.1857e-01)	Acc@1  92.97 ( 95.97)	Acc@5 100.00 ( 99.96)
Epoch: [31][260/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.7337e-01 (1.1783e-01)	Acc@1  94.53 ( 95.98)	Acc@5 100.00 ( 99.96)
Epoch: [31][270/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.5508e-02 (1.1769e-01)	Acc@1  99.22 ( 95.99)	Acc@5  99.22 ( 99.96)
Epoch: [31][280/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.9831e-02 (1.1753e-01)	Acc@1  97.66 ( 95.98)	Acc@5 100.00 ( 99.96)
Epoch: [31][290/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0844e-01 (1.1696e-01)	Acc@1  95.31 ( 95.98)	Acc@5 100.00 ( 99.96)
Epoch: [31][300/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3235e-01 (1.1657e-01)	Acc@1  95.31 ( 96.01)	Acc@5 100.00 ( 99.96)
Epoch: [31][310/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3492e-01 (1.1623e-01)	Acc@1  94.53 ( 96.01)	Acc@5 100.00 ( 99.96)
Epoch: [31][320/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.8365e-02 (1.1688e-01)	Acc@1  98.44 ( 95.99)	Acc@5 100.00 ( 99.96)
Epoch: [31][330/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.5083e-02 (1.1671e-01)	Acc@1  96.09 ( 96.00)	Acc@5 100.00 ( 99.96)
Epoch: [31][340/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.6489e-02 (1.1638e-01)	Acc@1  96.09 ( 96.00)	Acc@5 100.00 ( 99.97)
Epoch: [31][350/391]	Time  0.132 ( 0.137)	Data  0.002 ( 0.005)	Loss 9.7510e-02 (1.1665e-01)	Acc@1  96.09 ( 95.98)	Acc@5 100.00 ( 99.96)
Epoch: [31][360/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0593e-01 (1.1639e-01)	Acc@1  96.88 ( 96.00)	Acc@5 100.00 ( 99.97)
Epoch: [31][370/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.7910e-02 (1.1603e-01)	Acc@1  95.31 ( 96.00)	Acc@5 100.00 ( 99.96)
Epoch: [31][380/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.0617e-02 (1.1549e-01)	Acc@1  97.66 ( 96.02)	Acc@5 100.00 ( 99.97)
Epoch: [31][390/391]	Time  0.124 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.9703e-01 (1.1575e-01)	Acc@1  95.00 ( 96.02)	Acc@5  98.75 ( 99.96)
## e[31] optimizer.zero_grad (sum) time: 0.2873971462249756
## e[31]       loss.backward (sum) time: 18.594245195388794
## e[31]      optimizer.step (sum) time: 2.8932511806488037
## epoch[31] training(only) time: 53.85157489776611
# Switched to evaluate mode...
Test: [  0/100]	Time  0.199 ( 0.199)	Loss 1.4621e-01 (1.4621e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.043 ( 0.058)	Loss 4.0427e-01 (2.5843e-01)	Acc@1  88.00 ( 91.82)	Acc@5  99.00 ( 99.55)
Test: [ 20/100]	Time  0.045 ( 0.051)	Loss 3.8181e-01 (2.8095e-01)	Acc@1  87.00 ( 91.10)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.044 ( 0.049)	Loss 2.1084e-01 (2.9087e-01)	Acc@1  92.00 ( 91.13)	Acc@5 100.00 ( 99.61)
Test: [ 40/100]	Time  0.044 ( 0.048)	Loss 2.1758e-01 (2.8786e-01)	Acc@1  93.00 ( 91.24)	Acc@5 100.00 ( 99.59)
Test: [ 50/100]	Time  0.044 ( 0.047)	Loss 2.0798e-01 (2.8973e-01)	Acc@1  91.00 ( 91.12)	Acc@5 100.00 ( 99.63)
Test: [ 60/100]	Time  0.042 ( 0.047)	Loss 2.2722e-01 (2.8729e-01)	Acc@1  91.00 ( 91.18)	Acc@5 100.00 ( 99.67)
Test: [ 70/100]	Time  0.046 ( 0.046)	Loss 4.4831e-01 (2.8834e-01)	Acc@1  86.00 ( 91.13)	Acc@5 100.00 ( 99.70)
Test: [ 80/100]	Time  0.046 ( 0.046)	Loss 1.9934e-01 (2.8776e-01)	Acc@1  93.00 ( 91.16)	Acc@5 100.00 ( 99.69)
Test: [ 90/100]	Time  0.046 ( 0.046)	Loss 1.4223e-01 (2.8367e-01)	Acc@1  94.00 ( 91.27)	Acc@5 100.00 ( 99.73)
 * Acc@1 91.360 Acc@5 99.740
### epoch[31] execution time: 58.5092077255249
EPOCH 32
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [32][  0/391]	Time  0.292 ( 0.292)	Data  0.176 ( 0.176)	Loss 1.3113e-01 (1.3113e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
Epoch: [32][ 10/391]	Time  0.135 ( 0.150)	Data  0.001 ( 0.020)	Loss 8.2637e-02 (9.9151e-02)	Acc@1  96.88 ( 96.02)	Acc@5 100.00 (100.00)
Epoch: [32][ 20/391]	Time  0.133 ( 0.143)	Data  0.001 ( 0.012)	Loss 1.1577e-01 (1.0118e-01)	Acc@1  96.09 ( 96.17)	Acc@5 100.00 (100.00)
Epoch: [32][ 30/391]	Time  0.135 ( 0.141)	Data  0.001 ( 0.010)	Loss 6.8196e-02 (9.8228e-02)	Acc@1  96.88 ( 96.52)	Acc@5 100.00 (100.00)
Epoch: [32][ 40/391]	Time  0.137 ( 0.140)	Data  0.002 ( 0.008)	Loss 1.1839e-01 (9.9141e-02)	Acc@1  95.31 ( 96.44)	Acc@5 100.00 (100.00)
Epoch: [32][ 50/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.008)	Loss 9.0249e-02 (1.0056e-01)	Acc@1  96.88 ( 96.29)	Acc@5 100.00 (100.00)
Epoch: [32][ 60/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.007)	Loss 7.4768e-02 (1.0069e-01)	Acc@1  96.88 ( 96.38)	Acc@5 100.00 (100.00)
Epoch: [32][ 70/391]	Time  0.135 ( 0.139)	Data  0.002 ( 0.007)	Loss 1.4472e-01 (1.0176e-01)	Acc@1  92.97 ( 96.30)	Acc@5 100.00 (100.00)
Epoch: [32][ 80/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 7.0198e-02 (1.0031e-01)	Acc@1  98.44 ( 96.42)	Acc@5 100.00 (100.00)
Epoch: [32][ 90/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.4770e-01 (9.9624e-02)	Acc@1  94.53 ( 96.45)	Acc@5 100.00 ( 99.99)
Epoch: [32][100/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 9.8788e-02 (1.0096e-01)	Acc@1  97.66 ( 96.40)	Acc@5  99.22 ( 99.98)
Epoch: [32][110/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 8.4543e-02 (1.0095e-01)	Acc@1  97.66 ( 96.38)	Acc@5 100.00 ( 99.99)
Epoch: [32][120/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 9.1670e-02 (1.0024e-01)	Acc@1  96.09 ( 96.42)	Acc@5 100.00 ( 99.99)
Epoch: [32][130/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.1243e-01 (9.9301e-02)	Acc@1  94.53 ( 96.46)	Acc@5 100.00 ( 99.99)
Epoch: [32][140/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.0164e-01 (9.9750e-02)	Acc@1  97.66 ( 96.48)	Acc@5 100.00 ( 99.99)
Epoch: [32][150/391]	Time  0.127 ( 0.138)	Data  0.002 ( 0.006)	Loss 9.2842e-02 (9.9307e-02)	Acc@1  96.88 ( 96.51)	Acc@5 100.00 ( 99.98)
Epoch: [32][160/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.6655e-01 (9.9274e-02)	Acc@1  95.31 ( 96.53)	Acc@5  99.22 ( 99.97)
Epoch: [32][170/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.0845e-01 (9.9235e-02)	Acc@1  96.09 ( 96.54)	Acc@5 100.00 ( 99.97)
Epoch: [32][180/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.005)	Loss 8.9091e-02 (1.0028e-01)	Acc@1  97.66 ( 96.51)	Acc@5 100.00 ( 99.97)
Epoch: [32][190/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.005)	Loss 8.3748e-02 (9.9576e-02)	Acc@1  97.66 ( 96.54)	Acc@5 100.00 ( 99.96)
Epoch: [32][200/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.1555e-01 (9.9481e-02)	Acc@1  96.88 ( 96.55)	Acc@5 100.00 ( 99.97)
Epoch: [32][210/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 9.3541e-02 (1.0016e-01)	Acc@1  95.31 ( 96.54)	Acc@5 100.00 ( 99.96)
Epoch: [32][220/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.2290e-02 (1.0024e-01)	Acc@1  98.44 ( 96.54)	Acc@5 100.00 ( 99.96)
Epoch: [32][230/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.7739e-02 (1.0136e-01)	Acc@1  96.09 ( 96.50)	Acc@5 100.00 ( 99.96)
Epoch: [32][240/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3163e-01 (1.0209e-01)	Acc@1  96.88 ( 96.50)	Acc@5  99.22 ( 99.96)
Epoch: [32][250/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0239e-01 (1.0184e-01)	Acc@1  96.09 ( 96.51)	Acc@5 100.00 ( 99.96)
Epoch: [32][260/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0740e-01 (1.0168e-01)	Acc@1  96.88 ( 96.50)	Acc@5 100.00 ( 99.96)
Epoch: [32][270/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.6964e-02 (1.0145e-01)	Acc@1  96.09 ( 96.52)	Acc@5 100.00 ( 99.97)
Epoch: [32][280/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.5226e-01 (1.0175e-01)	Acc@1  94.53 ( 96.52)	Acc@5 100.00 ( 99.96)
Epoch: [32][290/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0380e-01 (1.0162e-01)	Acc@1  96.88 ( 96.53)	Acc@5 100.00 ( 99.96)
Epoch: [32][300/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.9992e-02 (1.0157e-01)	Acc@1  96.88 ( 96.53)	Acc@5 100.00 ( 99.96)
Epoch: [32][310/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.4490e-02 (1.0137e-01)	Acc@1  95.31 ( 96.52)	Acc@5 100.00 ( 99.96)
Epoch: [32][320/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3324e-01 (1.0155e-01)	Acc@1  96.88 ( 96.52)	Acc@5 100.00 ( 99.96)
Epoch: [32][330/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1026e-01 (1.0125e-01)	Acc@1  96.88 ( 96.53)	Acc@5 100.00 ( 99.96)
Epoch: [32][340/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1516e-01 (1.0109e-01)	Acc@1  95.31 ( 96.54)	Acc@5 100.00 ( 99.96)
Epoch: [32][350/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.0717e-02 (1.0115e-01)	Acc@1  96.09 ( 96.54)	Acc@5 100.00 ( 99.96)
Epoch: [32][360/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0602e-01 (1.0116e-01)	Acc@1  95.31 ( 96.54)	Acc@5 100.00 ( 99.96)
Epoch: [32][370/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.0768e-02 (1.0161e-01)	Acc@1  97.66 ( 96.53)	Acc@5 100.00 ( 99.96)
Epoch: [32][380/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.9596e-02 (1.0183e-01)	Acc@1  98.44 ( 96.53)	Acc@5 100.00 ( 99.96)
Epoch: [32][390/391]	Time  0.126 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1635e-01 (1.0129e-01)	Acc@1  95.00 ( 96.56)	Acc@5 100.00 ( 99.96)
## e[32] optimizer.zero_grad (sum) time: 0.2895987033843994
## e[32]       loss.backward (sum) time: 18.57625436782837
## e[32]      optimizer.step (sum) time: 2.9150209426879883
## epoch[32] training(only) time: 53.80624008178711
# Switched to evaluate mode...
Test: [  0/100]	Time  0.204 ( 0.204)	Loss 1.2601e-01 (1.2601e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.042 ( 0.057)	Loss 3.5722e-01 (2.4702e-01)	Acc@1  87.00 ( 92.18)	Acc@5  99.00 ( 99.55)
Test: [ 20/100]	Time  0.043 ( 0.051)	Loss 3.2893e-01 (2.6565e-01)	Acc@1  88.00 ( 91.76)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.044 ( 0.049)	Loss 1.7618e-01 (2.7534e-01)	Acc@1  92.00 ( 91.71)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.041 ( 0.048)	Loss 2.4559e-01 (2.7919e-01)	Acc@1  92.00 ( 91.61)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.042 ( 0.047)	Loss 1.5369e-01 (2.8289e-01)	Acc@1  95.00 ( 91.61)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.044 ( 0.046)	Loss 2.3985e-01 (2.8086e-01)	Acc@1  93.00 ( 91.66)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.041 ( 0.046)	Loss 5.0526e-01 (2.8339e-01)	Acc@1  87.00 ( 91.49)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.044 ( 0.046)	Loss 1.8540e-01 (2.8472e-01)	Acc@1  95.00 ( 91.59)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.041 ( 0.045)	Loss 1.1369e-01 (2.8159e-01)	Acc@1  94.00 ( 91.66)	Acc@5 100.00 ( 99.78)
 * Acc@1 91.690 Acc@5 99.790
### epoch[32] execution time: 58.432759523391724
EPOCH 33
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [33][  0/391]	Time  0.303 ( 0.303)	Data  0.175 ( 0.175)	Loss 7.4835e-02 (7.4835e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [33][ 10/391]	Time  0.129 ( 0.150)	Data  0.001 ( 0.020)	Loss 8.2632e-02 (7.6672e-02)	Acc@1  97.66 ( 97.44)	Acc@5 100.00 (100.00)
Epoch: [33][ 20/391]	Time  0.132 ( 0.143)	Data  0.001 ( 0.012)	Loss 5.7919e-02 (7.5984e-02)	Acc@1  98.44 ( 97.43)	Acc@5 100.00 ( 99.96)
Epoch: [33][ 30/391]	Time  0.130 ( 0.141)	Data  0.001 ( 0.010)	Loss 5.9190e-02 (7.3571e-02)	Acc@1  97.66 ( 97.51)	Acc@5 100.00 ( 99.97)
Epoch: [33][ 40/391]	Time  0.133 ( 0.140)	Data  0.001 ( 0.009)	Loss 8.9597e-02 (7.8325e-02)	Acc@1  98.44 ( 97.28)	Acc@5 100.00 ( 99.98)
Epoch: [33][ 50/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.008)	Loss 1.3827e-01 (7.9733e-02)	Acc@1  95.31 ( 97.15)	Acc@5 100.00 ( 99.98)
Epoch: [33][ 60/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.007)	Loss 8.5666e-02 (8.2460e-02)	Acc@1  96.09 ( 97.00)	Acc@5 100.00 ( 99.99)
Epoch: [33][ 70/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.007)	Loss 8.7810e-02 (8.4228e-02)	Acc@1  98.44 ( 96.97)	Acc@5 100.00 ( 99.99)
Epoch: [33][ 80/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.007)	Loss 9.9794e-02 (8.8940e-02)	Acc@1  96.09 ( 96.79)	Acc@5 100.00 ( 99.99)
Epoch: [33][ 90/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.5624e-02 (8.9302e-02)	Acc@1  98.44 ( 96.75)	Acc@5 100.00 ( 99.98)
Epoch: [33][100/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.006)	Loss 9.3691e-02 (9.0644e-02)	Acc@1  97.66 ( 96.71)	Acc@5 100.00 ( 99.98)
Epoch: [33][110/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.006)	Loss 5.6004e-02 (9.0404e-02)	Acc@1  97.66 ( 96.71)	Acc@5 100.00 ( 99.99)
Epoch: [33][120/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.006)	Loss 1.3887e-01 (9.1630e-02)	Acc@1  94.53 ( 96.70)	Acc@5 100.00 ( 99.99)
Epoch: [33][130/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.006)	Loss 8.4740e-02 (9.2025e-02)	Acc@1  96.88 ( 96.70)	Acc@5 100.00 ( 99.98)
Epoch: [33][140/391]	Time  0.136 ( 0.137)	Data  0.002 ( 0.006)	Loss 4.5883e-02 (9.2012e-02)	Acc@1  98.44 ( 96.73)	Acc@5 100.00 ( 99.98)
Epoch: [33][150/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.006)	Loss 7.8721e-02 (9.2211e-02)	Acc@1  96.88 ( 96.72)	Acc@5 100.00 ( 99.98)
Epoch: [33][160/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.006)	Loss 1.0517e-01 (9.2670e-02)	Acc@1  96.88 ( 96.71)	Acc@5 100.00 ( 99.98)
Epoch: [33][170/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.2319e-02 (9.2968e-02)	Acc@1  97.66 ( 96.72)	Acc@5 100.00 ( 99.98)
Epoch: [33][180/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3010e-01 (9.2436e-02)	Acc@1  94.53 ( 96.75)	Acc@5 100.00 ( 99.98)
Epoch: [33][190/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1327e-01 (9.3024e-02)	Acc@1  96.88 ( 96.75)	Acc@5 100.00 ( 99.98)
Epoch: [33][200/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.5428e-01 (9.3602e-02)	Acc@1  94.53 ( 96.74)	Acc@5 100.00 ( 99.98)
Epoch: [33][210/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.8929e-02 (9.4473e-02)	Acc@1  96.09 ( 96.72)	Acc@5 100.00 ( 99.98)
Epoch: [33][220/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3963e-01 (9.4164e-02)	Acc@1  96.09 ( 96.73)	Acc@5 100.00 ( 99.98)
Epoch: [33][230/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3193e-01 (9.3975e-02)	Acc@1  94.53 ( 96.73)	Acc@5 100.00 ( 99.98)
Epoch: [33][240/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.0586e-02 (9.3384e-02)	Acc@1 100.00 ( 96.74)	Acc@5 100.00 ( 99.98)
Epoch: [33][250/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.2403e-01 (9.3733e-02)	Acc@1  94.53 ( 96.73)	Acc@5 100.00 ( 99.98)
Epoch: [33][260/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.6980e-01 (9.3792e-02)	Acc@1  95.31 ( 96.74)	Acc@5 100.00 ( 99.99)
Epoch: [33][270/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.2439e-02 (9.3096e-02)	Acc@1  98.44 ( 96.77)	Acc@5 100.00 ( 99.98)
Epoch: [33][280/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.9300e-02 (9.3093e-02)	Acc@1  96.09 ( 96.79)	Acc@5 100.00 ( 99.98)
Epoch: [33][290/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.6716e-02 (9.2961e-02)	Acc@1  97.66 ( 96.80)	Acc@5 100.00 ( 99.98)
Epoch: [33][300/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.9851e-02 (9.2510e-02)	Acc@1  97.66 ( 96.82)	Acc@5 100.00 ( 99.98)
Epoch: [33][310/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.8112e-02 (9.2743e-02)	Acc@1  95.31 ( 96.78)	Acc@5 100.00 ( 99.98)
Epoch: [33][320/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.2764e-01 (9.2743e-02)	Acc@1  95.31 ( 96.76)	Acc@5 100.00 ( 99.98)
Epoch: [33][330/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.9657e-02 (9.2138e-02)	Acc@1  97.66 ( 96.78)	Acc@5 100.00 ( 99.98)
Epoch: [33][340/391]	Time  0.134 ( 0.137)	Data  0.002 ( 0.005)	Loss 7.9687e-02 (9.1970e-02)	Acc@1  96.88 ( 96.80)	Acc@5 100.00 ( 99.98)
Epoch: [33][350/391]	Time  0.141 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.6393e-02 (9.1602e-02)	Acc@1  98.44 ( 96.81)	Acc@5 100.00 ( 99.98)
Epoch: [33][360/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.5148e-01 (9.1529e-02)	Acc@1  95.31 ( 96.81)	Acc@5 100.00 ( 99.98)
Epoch: [33][370/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.2709e-02 (9.2203e-02)	Acc@1  96.09 ( 96.79)	Acc@5 100.00 ( 99.98)
Epoch: [33][380/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3063e-01 (9.2289e-02)	Acc@1  95.31 ( 96.78)	Acc@5 100.00 ( 99.98)
Epoch: [33][390/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.5610e-01 (9.2261e-02)	Acc@1  93.75 ( 96.79)	Acc@5 100.00 ( 99.98)
## e[33] optimizer.zero_grad (sum) time: 0.28844451904296875
## e[33]       loss.backward (sum) time: 18.589827299118042
## e[33]      optimizer.step (sum) time: 2.90518856048584
## epoch[33] training(only) time: 53.774662017822266
# Switched to evaluate mode...
Test: [  0/100]	Time  0.204 ( 0.204)	Loss 1.5489e-01 (1.5489e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.042 ( 0.059)	Loss 4.3936e-01 (2.5056e-01)	Acc@1  90.00 ( 93.36)	Acc@5  99.00 ( 99.45)
Test: [ 20/100]	Time  0.042 ( 0.051)	Loss 3.8511e-01 (2.7849e-01)	Acc@1  88.00 ( 92.43)	Acc@5 100.00 ( 99.48)
Test: [ 30/100]	Time  0.043 ( 0.049)	Loss 1.8388e-01 (2.9366e-01)	Acc@1  91.00 ( 92.23)	Acc@5 100.00 ( 99.55)
Test: [ 40/100]	Time  0.045 ( 0.048)	Loss 2.4135e-01 (2.9646e-01)	Acc@1  93.00 ( 91.98)	Acc@5 100.00 ( 99.61)
Test: [ 50/100]	Time  0.043 ( 0.047)	Loss 1.5315e-01 (2.9574e-01)	Acc@1  95.00 ( 92.02)	Acc@5 100.00 ( 99.67)
Test: [ 60/100]	Time  0.041 ( 0.046)	Loss 2.3259e-01 (2.8909e-01)	Acc@1  95.00 ( 92.07)	Acc@5 100.00 ( 99.70)
Test: [ 70/100]	Time  0.042 ( 0.046)	Loss 4.2841e-01 (2.8655e-01)	Acc@1  91.00 ( 92.03)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.041 ( 0.046)	Loss 2.3633e-01 (2.8663e-01)	Acc@1  95.00 ( 92.01)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.043 ( 0.045)	Loss 1.7404e-01 (2.8252e-01)	Acc@1  92.00 ( 91.96)	Acc@5 100.00 ( 99.77)
 * Acc@1 91.990 Acc@5 99.770
### epoch[33] execution time: 58.417959451675415
EPOCH 34
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [34][  0/391]	Time  0.297 ( 0.297)	Data  0.180 ( 0.180)	Loss 9.0340e-02 (9.0340e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [34][ 10/391]	Time  0.135 ( 0.151)	Data  0.001 ( 0.020)	Loss 2.7097e-02 (8.5069e-02)	Acc@1 100.00 ( 96.88)	Acc@5 100.00 ( 99.93)
Epoch: [34][ 20/391]	Time  0.133 ( 0.143)	Data  0.001 ( 0.013)	Loss 7.1210e-02 (8.3999e-02)	Acc@1  98.44 ( 96.99)	Acc@5 100.00 ( 99.96)
Epoch: [34][ 30/391]	Time  0.133 ( 0.141)	Data  0.001 ( 0.010)	Loss 7.9675e-02 (7.9294e-02)	Acc@1  97.66 ( 97.25)	Acc@5 100.00 ( 99.97)
Epoch: [34][ 40/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.009)	Loss 8.5299e-02 (8.0531e-02)	Acc@1  96.88 ( 97.20)	Acc@5 100.00 ( 99.98)
Epoch: [34][ 50/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.008)	Loss 4.1947e-02 (7.8470e-02)	Acc@1  98.44 ( 97.29)	Acc@5 100.00 ( 99.97)
Epoch: [34][ 60/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.007)	Loss 7.8962e-02 (8.0624e-02)	Acc@1  96.09 ( 97.16)	Acc@5 100.00 ( 99.96)
Epoch: [34][ 70/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.007)	Loss 6.4106e-02 (8.2141e-02)	Acc@1  98.44 ( 97.25)	Acc@5 100.00 ( 99.97)
Epoch: [34][ 80/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.007)	Loss 9.9007e-02 (8.0179e-02)	Acc@1  96.09 ( 97.32)	Acc@5 100.00 ( 99.97)
Epoch: [34][ 90/391]	Time  0.135 ( 0.138)	Data  0.002 ( 0.006)	Loss 1.0466e-01 (7.9757e-02)	Acc@1  96.09 ( 97.33)	Acc@5 100.00 ( 99.97)
Epoch: [34][100/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.3121e-01 (8.0785e-02)	Acc@1  97.66 ( 97.30)	Acc@5 100.00 ( 99.98)
Epoch: [34][110/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 7.8836e-02 (8.0802e-02)	Acc@1  96.88 ( 97.28)	Acc@5 100.00 ( 99.98)
Epoch: [34][120/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.006)	Loss 4.1595e-02 (8.0653e-02)	Acc@1  98.44 ( 97.28)	Acc@5 100.00 ( 99.97)
Epoch: [34][130/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.006)	Loss 1.1054e-01 (7.9681e-02)	Acc@1  95.31 ( 97.29)	Acc@5 100.00 ( 99.97)
Epoch: [34][140/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.006)	Loss 8.5185e-02 (8.1049e-02)	Acc@1  95.31 ( 97.20)	Acc@5 100.00 ( 99.97)
Epoch: [34][150/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.006)	Loss 1.5436e-01 (8.0524e-02)	Acc@1  94.53 ( 97.19)	Acc@5 100.00 ( 99.97)
Epoch: [34][160/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.006)	Loss 7.6391e-02 (8.0249e-02)	Acc@1  97.66 ( 97.17)	Acc@5 100.00 ( 99.98)
Epoch: [34][170/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.4040e-01 (8.0723e-02)	Acc@1  94.53 ( 97.14)	Acc@5 100.00 ( 99.98)
Epoch: [34][180/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.6200e-02 (8.0602e-02)	Acc@1  96.09 ( 97.15)	Acc@5 100.00 ( 99.98)
Epoch: [34][190/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.1280e-02 (8.1466e-02)	Acc@1  96.09 ( 97.13)	Acc@5 100.00 ( 99.98)
Epoch: [34][200/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.2331e-01 (8.0918e-02)	Acc@1  95.31 ( 97.16)	Acc@5 100.00 ( 99.98)
Epoch: [34][210/391]	Time  0.131 ( 0.137)	Data  0.002 ( 0.005)	Loss 6.7019e-02 (8.1696e-02)	Acc@1  97.66 ( 97.14)	Acc@5 100.00 ( 99.98)
Epoch: [34][220/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.6153e-02 (8.1976e-02)	Acc@1  99.22 ( 97.15)	Acc@5 100.00 ( 99.98)
Epoch: [34][230/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1494e-01 (8.1917e-02)	Acc@1  96.09 ( 97.17)	Acc@5 100.00 ( 99.98)
Epoch: [34][240/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.5165e-01 (8.1710e-02)	Acc@1  93.75 ( 97.17)	Acc@5 100.00 ( 99.98)
Epoch: [34][250/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.3758e-02 (8.2361e-02)	Acc@1  96.88 ( 97.12)	Acc@5 100.00 ( 99.98)
Epoch: [34][260/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.2362e-02 (8.2122e-02)	Acc@1  97.66 ( 97.11)	Acc@5 100.00 ( 99.98)
Epoch: [34][270/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.2699e-02 (8.2296e-02)	Acc@1  95.31 ( 97.11)	Acc@5 100.00 ( 99.98)
Epoch: [34][280/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.8570e-02 (8.2682e-02)	Acc@1  98.44 ( 97.10)	Acc@5 100.00 ( 99.98)
Epoch: [34][290/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.6415e-02 (8.2763e-02)	Acc@1  98.44 ( 97.10)	Acc@5 100.00 ( 99.98)
Epoch: [34][300/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.5739e-02 (8.3086e-02)	Acc@1  96.88 ( 97.10)	Acc@5 100.00 ( 99.98)
Epoch: [34][310/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.5910e-02 (8.2911e-02)	Acc@1  97.66 ( 97.10)	Acc@5 100.00 ( 99.98)
Epoch: [34][320/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.5851e-02 (8.3014e-02)	Acc@1  97.66 ( 97.09)	Acc@5 100.00 ( 99.98)
Epoch: [34][330/391]	Time  0.130 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.6890e-02 (8.3312e-02)	Acc@1  98.44 ( 97.08)	Acc@5 100.00 ( 99.98)
Epoch: [34][340/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.8648e-02 (8.3480e-02)	Acc@1  96.88 ( 97.08)	Acc@5 100.00 ( 99.98)
Epoch: [34][350/391]	Time  0.136 ( 0.137)	Data  0.002 ( 0.005)	Loss 7.8611e-02 (8.2996e-02)	Acc@1  98.44 ( 97.10)	Acc@5 100.00 ( 99.98)
Epoch: [34][360/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.8376e-02 (8.2899e-02)	Acc@1  96.88 ( 97.12)	Acc@5 100.00 ( 99.98)
Epoch: [34][370/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.4824e-02 (8.3176e-02)	Acc@1  96.88 ( 97.12)	Acc@5  99.22 ( 99.98)
Epoch: [34][380/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.4655e-01 (8.3886e-02)	Acc@1  90.62 ( 97.08)	Acc@5 100.00 ( 99.98)
Epoch: [34][390/391]	Time  0.130 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.9867e-02 (8.3984e-02)	Acc@1  96.25 ( 97.08)	Acc@5 100.00 ( 99.98)
## e[34] optimizer.zero_grad (sum) time: 0.28616762161254883
## e[34]       loss.backward (sum) time: 18.5105721950531
## e[34]      optimizer.step (sum) time: 2.92787504196167
## epoch[34] training(only) time: 53.68794846534729
# Switched to evaluate mode...
Test: [  0/100]	Time  0.206 ( 0.206)	Loss 1.2691e-01 (1.2691e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.044 ( 0.059)	Loss 4.4616e-01 (2.5346e-01)	Acc@1  90.00 ( 92.82)	Acc@5  99.00 ( 99.45)
Test: [ 20/100]	Time  0.047 ( 0.052)	Loss 3.4059e-01 (2.7878e-01)	Acc@1  87.00 ( 91.95)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.041 ( 0.049)	Loss 1.8381e-01 (2.9069e-01)	Acc@1  91.00 ( 91.97)	Acc@5 100.00 ( 99.58)
Test: [ 40/100]	Time  0.045 ( 0.048)	Loss 2.6341e-01 (2.9331e-01)	Acc@1  87.00 ( 91.66)	Acc@5  99.00 ( 99.61)
Test: [ 50/100]	Time  0.045 ( 0.047)	Loss 1.5660e-01 (2.9697e-01)	Acc@1  95.00 ( 91.57)	Acc@5 100.00 ( 99.67)
Test: [ 60/100]	Time  0.042 ( 0.047)	Loss 2.5357e-01 (2.9002e-01)	Acc@1  93.00 ( 91.74)	Acc@5 100.00 ( 99.70)
Test: [ 70/100]	Time  0.043 ( 0.046)	Loss 4.4783e-01 (2.8765e-01)	Acc@1  87.00 ( 91.66)	Acc@5 100.00 ( 99.72)
Test: [ 80/100]	Time  0.044 ( 0.046)	Loss 2.1926e-01 (2.8853e-01)	Acc@1  94.00 ( 91.72)	Acc@5 100.00 ( 99.72)
Test: [ 90/100]	Time  0.045 ( 0.046)	Loss 1.0563e-01 (2.8657e-01)	Acc@1  94.00 ( 91.74)	Acc@5 100.00 ( 99.75)
 * Acc@1 91.780 Acc@5 99.760
### epoch[34] execution time: 58.36981558799744
EPOCH 35
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [35][  0/391]	Time  0.308 ( 0.308)	Data  0.184 ( 0.184)	Loss 1.1254e-01 (1.1254e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
Epoch: [35][ 10/391]	Time  0.127 ( 0.151)	Data  0.001 ( 0.021)	Loss 4.0101e-02 (6.5697e-02)	Acc@1  99.22 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [35][ 20/391]	Time  0.130 ( 0.144)	Data  0.001 ( 0.013)	Loss 5.4761e-02 (7.3260e-02)	Acc@1  99.22 ( 97.58)	Acc@5 100.00 ( 99.96)
Epoch: [35][ 30/391]	Time  0.133 ( 0.142)	Data  0.001 ( 0.010)	Loss 8.4693e-02 (7.1282e-02)	Acc@1  96.88 ( 97.58)	Acc@5 100.00 ( 99.97)
Epoch: [35][ 40/391]	Time  0.138 ( 0.140)	Data  0.002 ( 0.009)	Loss 7.4592e-02 (7.2880e-02)	Acc@1  96.88 ( 97.50)	Acc@5 100.00 ( 99.98)
Epoch: [35][ 50/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.008)	Loss 5.2196e-02 (7.3787e-02)	Acc@1  99.22 ( 97.43)	Acc@5  99.22 ( 99.97)
Epoch: [35][ 60/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.007)	Loss 8.7390e-02 (7.4050e-02)	Acc@1  96.88 ( 97.43)	Acc@5 100.00 ( 99.97)
Epoch: [35][ 70/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.007)	Loss 1.0980e-01 (7.4927e-02)	Acc@1  96.88 ( 97.38)	Acc@5  99.22 ( 99.97)
Epoch: [35][ 80/391]	Time  0.132 ( 0.139)	Data  0.001 ( 0.007)	Loss 1.3267e-01 (7.4347e-02)	Acc@1  96.88 ( 97.40)	Acc@5 100.00 ( 99.97)
Epoch: [35][ 90/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.006)	Loss 2.8469e-02 (7.3197e-02)	Acc@1  99.22 ( 97.42)	Acc@5 100.00 ( 99.97)
Epoch: [35][100/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 7.1504e-02 (7.2596e-02)	Acc@1  97.66 ( 97.44)	Acc@5 100.00 ( 99.97)
Epoch: [35][110/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 5.0391e-02 (7.4038e-02)	Acc@1  98.44 ( 97.39)	Acc@5 100.00 ( 99.97)
Epoch: [35][120/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.006)	Loss 6.3542e-02 (7.3987e-02)	Acc@1  98.44 ( 97.37)	Acc@5 100.00 ( 99.97)
Epoch: [35][130/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.6753e-02 (7.4490e-02)	Acc@1  99.22 ( 97.38)	Acc@5 100.00 ( 99.98)
Epoch: [35][140/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.1974e-02 (7.4155e-02)	Acc@1  99.22 ( 97.43)	Acc@5 100.00 ( 99.97)
Epoch: [35][150/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.006)	Loss 6.1159e-02 (7.4083e-02)	Acc@1  97.66 ( 97.45)	Acc@5 100.00 ( 99.97)
Epoch: [35][160/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.0106e-02 (7.3740e-02)	Acc@1 100.00 ( 97.46)	Acc@5 100.00 ( 99.98)
Epoch: [35][170/391]	Time  0.130 ( 0.138)	Data  0.001 ( 0.005)	Loss 8.2871e-02 (7.3578e-02)	Acc@1  96.09 ( 97.47)	Acc@5 100.00 ( 99.98)
Epoch: [35][180/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 6.0260e-02 (7.4315e-02)	Acc@1  97.66 ( 97.41)	Acc@5 100.00 ( 99.98)
Epoch: [35][190/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.005)	Loss 4.5075e-02 (7.4092e-02)	Acc@1  98.44 ( 97.42)	Acc@5 100.00 ( 99.98)
Epoch: [35][200/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.6990e-02 (7.4062e-02)	Acc@1  99.22 ( 97.43)	Acc@5 100.00 ( 99.98)
Epoch: [35][210/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.005)	Loss 9.2179e-02 (7.4174e-02)	Acc@1  97.66 ( 97.42)	Acc@5 100.00 ( 99.98)
Epoch: [35][220/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.005)	Loss 5.0901e-02 (7.4128e-02)	Acc@1  97.66 ( 97.43)	Acc@5 100.00 ( 99.98)
Epoch: [35][230/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.8308e-02 (7.3728e-02)	Acc@1  99.22 ( 97.46)	Acc@5 100.00 ( 99.98)
Epoch: [35][240/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.005)	Loss 8.5554e-02 (7.3554e-02)	Acc@1  96.88 ( 97.46)	Acc@5 100.00 ( 99.98)
Epoch: [35][250/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1087e-01 (7.4021e-02)	Acc@1  96.88 ( 97.45)	Acc@5 100.00 ( 99.98)
Epoch: [35][260/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.3539e-02 (7.3697e-02)	Acc@1  96.88 ( 97.47)	Acc@5 100.00 ( 99.98)
Epoch: [35][270/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.8127e-02 (7.4143e-02)	Acc@1  98.44 ( 97.46)	Acc@5 100.00 ( 99.98)
Epoch: [35][280/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.7730e-02 (7.4354e-02)	Acc@1 100.00 ( 97.43)	Acc@5 100.00 ( 99.98)
Epoch: [35][290/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.2807e-02 (7.4462e-02)	Acc@1  96.88 ( 97.41)	Acc@5 100.00 ( 99.98)
Epoch: [35][300/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.7337e-02 (7.4460e-02)	Acc@1  98.44 ( 97.41)	Acc@5 100.00 ( 99.98)
Epoch: [35][310/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.6012e-02 (7.4336e-02)	Acc@1  95.31 ( 97.42)	Acc@5 100.00 ( 99.98)
Epoch: [35][320/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.1910e-02 (7.4342e-02)	Acc@1  96.88 ( 97.42)	Acc@5 100.00 ( 99.99)
Epoch: [35][330/391]	Time  0.138 ( 0.137)	Data  0.002 ( 0.005)	Loss 7.0607e-02 (7.4699e-02)	Acc@1  96.88 ( 97.40)	Acc@5 100.00 ( 99.99)
Epoch: [35][340/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.7260e-02 (7.3886e-02)	Acc@1  98.44 ( 97.43)	Acc@5 100.00 ( 99.99)
Epoch: [35][350/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.0718e-02 (7.3871e-02)	Acc@1  99.22 ( 97.42)	Acc@5 100.00 ( 99.99)
Epoch: [35][360/391]	Time  0.125 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0295e-01 (7.3957e-02)	Acc@1  97.66 ( 97.42)	Acc@5 100.00 ( 99.99)
Epoch: [35][370/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.2185e-01 (7.4309e-02)	Acc@1  94.53 ( 97.40)	Acc@5 100.00 ( 99.99)
Epoch: [35][380/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.8335e-02 (7.4088e-02)	Acc@1  95.31 ( 97.40)	Acc@5 100.00 ( 99.99)
Epoch: [35][390/391]	Time  0.124 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.2803e-02 (7.3980e-02)	Acc@1  97.50 ( 97.39)	Acc@5 100.00 ( 99.99)
## e[35] optimizer.zero_grad (sum) time: 0.2848381996154785
## e[35]       loss.backward (sum) time: 18.603046417236328
## e[35]      optimizer.step (sum) time: 2.911722421646118
## epoch[35] training(only) time: 53.869720220565796
# Switched to evaluate mode...
Test: [  0/100]	Time  0.193 ( 0.193)	Loss 1.5813e-01 (1.5813e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.043 ( 0.057)	Loss 4.4454e-01 (2.6677e-01)	Acc@1  89.00 ( 93.27)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.043 ( 0.051)	Loss 3.6071e-01 (2.9717e-01)	Acc@1  88.00 ( 92.00)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.045 ( 0.048)	Loss 2.1344e-01 (3.1327e-01)	Acc@1  90.00 ( 91.58)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.042 ( 0.047)	Loss 2.6635e-01 (3.1223e-01)	Acc@1  88.00 ( 91.39)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.043 ( 0.046)	Loss 1.9112e-01 (3.1149e-01)	Acc@1  96.00 ( 91.45)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.043 ( 0.046)	Loss 2.4163e-01 (3.0430e-01)	Acc@1  95.00 ( 91.54)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.044 ( 0.045)	Loss 4.7347e-01 (3.0194e-01)	Acc@1  87.00 ( 91.52)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.041 ( 0.045)	Loss 2.1807e-01 (3.0060e-01)	Acc@1  94.00 ( 91.54)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.042 ( 0.045)	Loss 1.7469e-01 (2.9686e-01)	Acc@1  93.00 ( 91.58)	Acc@5 100.00 ( 99.79)
 * Acc@1 91.640 Acc@5 99.800
### epoch[35] execution time: 58.4588360786438
EPOCH 36
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [36][  0/391]	Time  0.292 ( 0.292)	Data  0.173 ( 0.173)	Loss 5.8500e-02 (5.8500e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [36][ 10/391]	Time  0.133 ( 0.150)	Data  0.001 ( 0.019)	Loss 8.3508e-02 (5.7706e-02)	Acc@1  96.88 ( 97.94)	Acc@5 100.00 (100.00)
Epoch: [36][ 20/391]	Time  0.130 ( 0.143)	Data  0.001 ( 0.012)	Loss 2.7747e-02 (5.9887e-02)	Acc@1 100.00 ( 97.81)	Acc@5 100.00 (100.00)
Epoch: [36][ 30/391]	Time  0.136 ( 0.141)	Data  0.001 ( 0.010)	Loss 6.1370e-02 (6.0202e-02)	Acc@1  97.66 ( 97.83)	Acc@5 100.00 (100.00)
Epoch: [36][ 40/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.008)	Loss 7.5011e-02 (6.1310e-02)	Acc@1  97.66 ( 97.85)	Acc@5 100.00 (100.00)
Epoch: [36][ 50/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.008)	Loss 9.0629e-02 (6.2036e-02)	Acc@1  95.31 ( 97.78)	Acc@5 100.00 (100.00)
Epoch: [36][ 60/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.007)	Loss 7.8239e-02 (6.1068e-02)	Acc@1  96.88 ( 97.82)	Acc@5 100.00 ( 99.99)
Epoch: [36][ 70/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.007)	Loss 7.4723e-02 (6.1090e-02)	Acc@1  96.09 ( 97.82)	Acc@5 100.00 ( 99.99)
Epoch: [36][ 80/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 8.7807e-02 (6.2266e-02)	Acc@1  96.09 ( 97.81)	Acc@5 100.00 ( 99.99)
Epoch: [36][ 90/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.006)	Loss 9.4622e-02 (6.2203e-02)	Acc@1  97.66 ( 97.85)	Acc@5 100.00 ( 99.99)
Epoch: [36][100/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.3484e-02 (6.2177e-02)	Acc@1  99.22 ( 97.82)	Acc@5 100.00 ( 99.99)
Epoch: [36][110/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 8.2610e-02 (6.3250e-02)	Acc@1  96.88 ( 97.77)	Acc@5 100.00 ( 99.99)
Epoch: [36][120/391]	Time  0.138 ( 0.138)	Data  0.002 ( 0.006)	Loss 7.5762e-02 (6.2758e-02)	Acc@1  98.44 ( 97.79)	Acc@5 100.00 ( 99.99)
Epoch: [36][130/391]	Time  0.131 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.4866e-02 (6.2846e-02)	Acc@1  98.44 ( 97.79)	Acc@5 100.00 ( 99.99)
Epoch: [36][140/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.9736e-02 (6.2443e-02)	Acc@1  99.22 ( 97.79)	Acc@5 100.00 ( 99.99)
Epoch: [36][150/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.005)	Loss 4.1646e-02 (6.3519e-02)	Acc@1  98.44 ( 97.75)	Acc@5 100.00 ( 99.99)
Epoch: [36][160/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 4.1292e-02 (6.3350e-02)	Acc@1  97.66 ( 97.74)	Acc@5 100.00 (100.00)
Epoch: [36][170/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.7605e-02 (6.3224e-02)	Acc@1  96.88 ( 97.74)	Acc@5 100.00 (100.00)
Epoch: [36][180/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.1577e-02 (6.3931e-02)	Acc@1  99.22 ( 97.74)	Acc@5  99.22 ( 99.99)
Epoch: [36][190/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.8673e-02 (6.4771e-02)	Acc@1  97.66 ( 97.70)	Acc@5 100.00 ( 99.99)
Epoch: [36][200/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.7346e-02 (6.5258e-02)	Acc@1  97.66 ( 97.68)	Acc@5 100.00 ( 99.99)
Epoch: [36][210/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.3501e-02 (6.5583e-02)	Acc@1  97.66 ( 97.67)	Acc@5 100.00 ( 99.99)
Epoch: [36][220/391]	Time  0.128 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.1502e-02 (6.6277e-02)	Acc@1  96.88 ( 97.65)	Acc@5 100.00 ( 99.99)
Epoch: [36][230/391]	Time  0.130 ( 0.137)	Data  0.002 ( 0.005)	Loss 1.0112e-01 (6.6455e-02)	Acc@1  97.66 ( 97.65)	Acc@5 100.00 ( 99.99)
Epoch: [36][240/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.2143e-02 (6.6550e-02)	Acc@1  99.22 ( 97.65)	Acc@5 100.00 ( 99.99)
Epoch: [36][250/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.5125e-02 (6.6868e-02)	Acc@1  97.66 ( 97.64)	Acc@5 100.00 ( 99.99)
Epoch: [36][260/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.9083e-02 (6.7505e-02)	Acc@1  96.88 ( 97.62)	Acc@5 100.00 ( 99.99)
Epoch: [36][270/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1021e-01 (6.7706e-02)	Acc@1  96.09 ( 97.60)	Acc@5 100.00 ( 99.99)
Epoch: [36][280/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.8998e-02 (6.8097e-02)	Acc@1  98.44 ( 97.60)	Acc@5 100.00 ( 99.99)
Epoch: [36][290/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.8985e-02 (6.7991e-02)	Acc@1  97.66 ( 97.61)	Acc@5 100.00 ( 99.99)
Epoch: [36][300/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.5242e-02 (6.8336e-02)	Acc@1  97.66 ( 97.59)	Acc@5 100.00 ( 99.99)
Epoch: [36][310/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0704e-01 (6.9180e-02)	Acc@1  96.09 ( 97.59)	Acc@5 100.00 ( 99.99)
Epoch: [36][320/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.3736e-02 (6.8591e-02)	Acc@1  98.44 ( 97.61)	Acc@5 100.00 ( 99.99)
Epoch: [36][330/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.3788e-02 (6.8655e-02)	Acc@1  97.66 ( 97.62)	Acc@5 100.00 ( 99.99)
Epoch: [36][340/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.8767e-02 (6.8184e-02)	Acc@1 100.00 ( 97.64)	Acc@5 100.00 ( 99.99)
Epoch: [36][350/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.8121e-02 (6.8326e-02)	Acc@1  97.66 ( 97.62)	Acc@5 100.00 ( 99.99)
Epoch: [36][360/391]	Time  0.141 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.3914e-02 (6.8918e-02)	Acc@1  98.44 ( 97.60)	Acc@5 100.00 ( 99.99)
Epoch: [36][370/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.0117e-02 (6.9264e-02)	Acc@1  96.88 ( 97.59)	Acc@5 100.00 ( 99.99)
Epoch: [36][380/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0997e-01 (6.9177e-02)	Acc@1  96.09 ( 97.58)	Acc@5 100.00 ( 99.99)
Epoch: [36][390/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.3602e-02 (6.9176e-02)	Acc@1  97.50 ( 97.58)	Acc@5 100.00 ( 99.99)
## e[36] optimizer.zero_grad (sum) time: 0.28854990005493164
## e[36]       loss.backward (sum) time: 18.597211837768555
## e[36]      optimizer.step (sum) time: 2.973550796508789
## epoch[36] training(only) time: 53.757155656814575
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 1.4803e-01 (1.4803e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.044 ( 0.057)	Loss 4.2164e-01 (2.6123e-01)	Acc@1  91.00 ( 92.82)	Acc@5  99.00 ( 99.73)
Test: [ 20/100]	Time  0.045 ( 0.051)	Loss 3.6250e-01 (2.9340e-01)	Acc@1  88.00 ( 91.81)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.044 ( 0.049)	Loss 1.9100e-01 (3.0655e-01)	Acc@1  91.00 ( 91.61)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.045 ( 0.047)	Loss 2.4752e-01 (3.1663e-01)	Acc@1  88.00 ( 91.32)	Acc@5  99.00 ( 99.73)
Test: [ 50/100]	Time  0.042 ( 0.047)	Loss 1.5985e-01 (3.1450e-01)	Acc@1  96.00 ( 91.45)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.041 ( 0.046)	Loss 2.7065e-01 (3.0645e-01)	Acc@1  93.00 ( 91.67)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.040 ( 0.046)	Loss 4.8765e-01 (3.0541e-01)	Acc@1  90.00 ( 91.65)	Acc@5 100.00 ( 99.82)
Test: [ 80/100]	Time  0.043 ( 0.045)	Loss 1.9298e-01 (3.0561e-01)	Acc@1  93.00 ( 91.65)	Acc@5 100.00 ( 99.81)
Test: [ 90/100]	Time  0.050 ( 0.045)	Loss 1.3137e-01 (3.0288e-01)	Acc@1  95.00 ( 91.76)	Acc@5 100.00 ( 99.84)
 * Acc@1 91.830 Acc@5 99.830
### epoch[36] execution time: 58.37060856819153
EPOCH 37
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [37][  0/391]	Time  0.286 ( 0.286)	Data  0.167 ( 0.167)	Loss 8.6512e-02 (8.6512e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [37][ 10/391]	Time  0.138 ( 0.150)	Data  0.001 ( 0.019)	Loss 6.3395e-02 (6.5060e-02)	Acc@1  97.66 ( 97.51)	Acc@5 100.00 (100.00)
Epoch: [37][ 20/391]	Time  0.135 ( 0.143)	Data  0.001 ( 0.012)	Loss 7.0249e-02 (6.3466e-02)	Acc@1  97.66 ( 97.58)	Acc@5 100.00 (100.00)
Epoch: [37][ 30/391]	Time  0.134 ( 0.141)	Data  0.001 ( 0.010)	Loss 1.0443e-01 (6.6736e-02)	Acc@1  95.31 ( 97.56)	Acc@5 100.00 (100.00)
Epoch: [37][ 40/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.008)	Loss 4.2324e-02 (6.7392e-02)	Acc@1  98.44 ( 97.45)	Acc@5 100.00 (100.00)
Epoch: [37][ 50/391]	Time  0.130 ( 0.139)	Data  0.001 ( 0.008)	Loss 2.9984e-02 (6.3190e-02)	Acc@1  99.22 ( 97.63)	Acc@5 100.00 (100.00)
Epoch: [37][ 60/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.007)	Loss 2.1986e-02 (6.1752e-02)	Acc@1 100.00 ( 97.71)	Acc@5 100.00 (100.00)
Epoch: [37][ 70/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.007)	Loss 4.8897e-02 (6.1616e-02)	Acc@1  97.66 ( 97.72)	Acc@5 100.00 (100.00)
Epoch: [37][ 80/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 5.0281e-02 (6.1350e-02)	Acc@1  97.66 ( 97.71)	Acc@5 100.00 ( 99.99)
Epoch: [37][ 90/391]	Time  0.130 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.0251e-02 (6.2012e-02)	Acc@1  98.44 ( 97.72)	Acc@5 100.00 ( 99.99)
Epoch: [37][100/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 6.8338e-02 (6.1219e-02)	Acc@1  98.44 ( 97.78)	Acc@5 100.00 ( 99.99)
Epoch: [37][110/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.9837e-02 (5.9920e-02)	Acc@1  99.22 ( 97.86)	Acc@5 100.00 ( 99.99)
Epoch: [37][120/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.4427e-02 (5.9691e-02)	Acc@1  99.22 ( 97.90)	Acc@5 100.00 ( 99.99)
Epoch: [37][130/391]	Time  0.137 ( 0.138)	Data  0.002 ( 0.006)	Loss 4.2463e-02 (5.8384e-02)	Acc@1  98.44 ( 97.92)	Acc@5 100.00 ( 99.99)
Epoch: [37][140/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.6328e-02 (5.8777e-02)	Acc@1  97.66 ( 97.89)	Acc@5 100.00 ( 99.99)
Epoch: [37][150/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.3058e-02 (5.8711e-02)	Acc@1 100.00 ( 97.90)	Acc@5 100.00 ( 99.99)
Epoch: [37][160/391]	Time  0.134 ( 0.138)	Data  0.002 ( 0.005)	Loss 5.0560e-02 (5.8537e-02)	Acc@1  98.44 ( 97.91)	Acc@5 100.00 (100.00)
Epoch: [37][170/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.005)	Loss 5.9256e-02 (5.9011e-02)	Acc@1  97.66 ( 97.88)	Acc@5 100.00 (100.00)
Epoch: [37][180/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.3588e-02 (5.9271e-02)	Acc@1  97.66 ( 97.88)	Acc@5 100.00 (100.00)
Epoch: [37][190/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.7517e-02 (5.9627e-02)	Acc@1  96.09 ( 97.87)	Acc@5 100.00 (100.00)
Epoch: [37][200/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.9042e-02 (6.0391e-02)	Acc@1  97.66 ( 97.85)	Acc@5 100.00 (100.00)
Epoch: [37][210/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.3345e-02 (6.0487e-02)	Acc@1  96.88 ( 97.85)	Acc@5 100.00 ( 99.99)
Epoch: [37][220/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.1907e-02 (6.0376e-02)	Acc@1  99.22 ( 97.83)	Acc@5 100.00 ( 99.99)
Epoch: [37][230/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1329e-01 (6.0730e-02)	Acc@1  96.09 ( 97.81)	Acc@5 100.00 ( 99.99)
Epoch: [37][240/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.7167e-02 (6.1153e-02)	Acc@1  97.66 ( 97.79)	Acc@5 100.00 ( 99.99)
Epoch: [37][250/391]	Time  0.130 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.3383e-02 (6.1314e-02)	Acc@1  97.66 ( 97.77)	Acc@5 100.00 ( 99.99)
Epoch: [37][260/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.6885e-02 (6.1980e-02)	Acc@1  96.88 ( 97.77)	Acc@5 100.00 ( 99.99)
Epoch: [37][270/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.7150e-02 (6.2194e-02)	Acc@1  99.22 ( 97.77)	Acc@5 100.00 ( 99.99)
Epoch: [37][280/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.8467e-02 (6.2344e-02)	Acc@1  98.44 ( 97.78)	Acc@5 100.00 ( 99.99)
Epoch: [37][290/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.5223e-02 (6.1936e-02)	Acc@1  98.44 ( 97.81)	Acc@5 100.00 ( 99.99)
Epoch: [37][300/391]	Time  0.136 ( 0.137)	Data  0.002 ( 0.005)	Loss 3.7330e-02 (6.2054e-02)	Acc@1  98.44 ( 97.81)	Acc@5 100.00 ( 99.99)
Epoch: [37][310/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.1289e-02 (6.2351e-02)	Acc@1 100.00 ( 97.79)	Acc@5 100.00 ( 99.99)
Epoch: [37][320/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0548e-01 (6.2470e-02)	Acc@1  96.09 ( 97.79)	Acc@5 100.00 ( 99.99)
Epoch: [37][330/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.7644e-02 (6.2006e-02)	Acc@1  97.66 ( 97.82)	Acc@5 100.00 ( 99.99)
Epoch: [37][340/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1839e-01 (6.2155e-02)	Acc@1  95.31 ( 97.81)	Acc@5 100.00 ( 99.99)
Epoch: [37][350/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.2013e-02 (6.2139e-02)	Acc@1  96.88 ( 97.82)	Acc@5 100.00 ( 99.99)
Epoch: [37][360/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.6648e-02 (6.2039e-02)	Acc@1  97.66 ( 97.83)	Acc@5 100.00 ( 99.99)
Epoch: [37][370/391]	Time  0.135 ( 0.137)	Data  0.002 ( 0.005)	Loss 3.7241e-02 (6.2101e-02)	Acc@1  99.22 ( 97.83)	Acc@5 100.00 ( 99.99)
Epoch: [37][380/391]	Time  0.130 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.3440e-02 (6.1685e-02)	Acc@1  99.22 ( 97.85)	Acc@5 100.00 ( 99.99)
Epoch: [37][390/391]	Time  0.128 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.8448e-02 (6.1808e-02)	Acc@1  97.50 ( 97.84)	Acc@5 100.00 ( 99.99)
## e[37] optimizer.zero_grad (sum) time: 0.28618502616882324
## e[37]       loss.backward (sum) time: 18.55080533027649
## e[37]      optimizer.step (sum) time: 2.8932676315307617
## epoch[37] training(only) time: 53.76025652885437
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 1.6606e-01 (1.6606e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.043 ( 0.057)	Loss 4.2613e-01 (2.7567e-01)	Acc@1  91.00 ( 92.91)	Acc@5  99.00 ( 99.55)
Test: [ 20/100]	Time  0.042 ( 0.051)	Loss 3.2994e-01 (3.1461e-01)	Acc@1  88.00 ( 91.62)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.043 ( 0.048)	Loss 1.6737e-01 (3.2248e-01)	Acc@1  93.00 ( 91.65)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.043 ( 0.047)	Loss 2.5776e-01 (3.2892e-01)	Acc@1  89.00 ( 91.54)	Acc@5  99.00 ( 99.66)
Test: [ 50/100]	Time  0.042 ( 0.046)	Loss 2.0820e-01 (3.2183e-01)	Acc@1  94.00 ( 91.57)	Acc@5  99.00 ( 99.71)
Test: [ 60/100]	Time  0.046 ( 0.046)	Loss 2.7960e-01 (3.1468e-01)	Acc@1  93.00 ( 91.75)	Acc@5 100.00 ( 99.70)
Test: [ 70/100]	Time  0.043 ( 0.046)	Loss 4.3191e-01 (3.1087e-01)	Acc@1  86.00 ( 91.75)	Acc@5 100.00 ( 99.72)
Test: [ 80/100]	Time  0.045 ( 0.045)	Loss 2.0643e-01 (3.0940e-01)	Acc@1  96.00 ( 91.91)	Acc@5 100.00 ( 99.69)
Test: [ 90/100]	Time  0.045 ( 0.045)	Loss 1.8872e-01 (3.0389e-01)	Acc@1  93.00 ( 91.97)	Acc@5 100.00 ( 99.73)
 * Acc@1 92.010 Acc@5 99.730
### epoch[37] execution time: 58.37095594406128
EPOCH 38
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [38][  0/391]	Time  0.293 ( 0.293)	Data  0.177 ( 0.177)	Loss 1.1262e-01 (1.1262e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [38][ 10/391]	Time  0.130 ( 0.150)	Data  0.001 ( 0.020)	Loss 2.5214e-02 (4.7875e-02)	Acc@1  99.22 ( 98.08)	Acc@5 100.00 (100.00)
Epoch: [38][ 20/391]	Time  0.133 ( 0.144)	Data  0.001 ( 0.013)	Loss 8.4278e-02 (5.4740e-02)	Acc@1  96.88 ( 98.03)	Acc@5 100.00 (100.00)
Epoch: [38][ 30/391]	Time  0.139 ( 0.141)	Data  0.001 ( 0.010)	Loss 1.3113e-01 (6.1660e-02)	Acc@1  96.09 ( 97.78)	Acc@5 100.00 ( 99.97)
Epoch: [38][ 40/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.009)	Loss 5.7786e-02 (6.0143e-02)	Acc@1  98.44 ( 97.81)	Acc@5 100.00 ( 99.98)
Epoch: [38][ 50/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.008)	Loss 8.6873e-02 (6.1362e-02)	Acc@1  96.09 ( 97.81)	Acc@5 100.00 ( 99.98)
Epoch: [38][ 60/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.007)	Loss 6.5330e-02 (5.9570e-02)	Acc@1  97.66 ( 97.87)	Acc@5 100.00 ( 99.99)
Epoch: [38][ 70/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.007)	Loss 8.3075e-02 (6.0077e-02)	Acc@1  96.88 ( 97.89)	Acc@5 100.00 ( 99.99)
Epoch: [38][ 80/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.007)	Loss 2.8448e-02 (5.9623e-02)	Acc@1 100.00 ( 97.93)	Acc@5 100.00 ( 99.99)
Epoch: [38][ 90/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.6856e-02 (5.8236e-02)	Acc@1 100.00 ( 98.00)	Acc@5 100.00 ( 99.99)
Epoch: [38][100/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.9370e-02 (5.9338e-02)	Acc@1  99.22 ( 97.97)	Acc@5 100.00 ( 99.99)
Epoch: [38][110/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 5.3370e-02 (5.9388e-02)	Acc@1  97.66 ( 97.95)	Acc@5 100.00 ( 99.99)
Epoch: [38][120/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.006)	Loss 6.8548e-02 (5.9575e-02)	Acc@1  96.88 ( 97.92)	Acc@5 100.00 ( 99.99)
Epoch: [38][130/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.006)	Loss 7.9648e-02 (5.9962e-02)	Acc@1  96.88 ( 97.91)	Acc@5 100.00 ( 99.99)
Epoch: [38][140/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.0646e-01 (6.0072e-02)	Acc@1  96.88 ( 97.92)	Acc@5 100.00 ( 99.99)
Epoch: [38][150/391]	Time  0.134 ( 0.138)	Data  0.002 ( 0.005)	Loss 6.3914e-02 (5.9864e-02)	Acc@1  97.66 ( 97.93)	Acc@5 100.00 ( 99.99)
Epoch: [38][160/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.005)	Loss 5.1912e-02 (5.9131e-02)	Acc@1  99.22 ( 97.96)	Acc@5 100.00 ( 99.99)
Epoch: [38][170/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.005)	Loss 6.7863e-02 (5.9490e-02)	Acc@1  98.44 ( 97.97)	Acc@5 100.00 ( 99.99)
Epoch: [38][180/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.005)	Loss 5.6860e-02 (6.0445e-02)	Acc@1  98.44 ( 97.95)	Acc@5 100.00 ( 99.99)
Epoch: [38][190/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.005)	Loss 5.8869e-02 (6.0722e-02)	Acc@1  98.44 ( 97.93)	Acc@5 100.00 ( 99.99)
Epoch: [38][200/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.005)	Loss 5.9581e-02 (6.0348e-02)	Acc@1  97.66 ( 97.94)	Acc@5 100.00 ( 99.99)
Epoch: [38][210/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.005)	Loss 5.8735e-02 (5.9573e-02)	Acc@1  97.66 ( 97.96)	Acc@5 100.00 ( 99.99)
Epoch: [38][220/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.6686e-02 (5.9327e-02)	Acc@1  97.66 ( 97.96)	Acc@5 100.00 ( 99.99)
Epoch: [38][230/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.8535e-02 (5.9132e-02)	Acc@1  96.88 ( 97.97)	Acc@5 100.00 ( 99.99)
Epoch: [38][240/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.3349e-02 (5.8633e-02)	Acc@1  99.22 ( 97.99)	Acc@5 100.00 ( 99.99)
Epoch: [38][250/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.2468e-02 (5.8470e-02)	Acc@1  99.22 ( 97.99)	Acc@5 100.00 ( 99.99)
Epoch: [38][260/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.1488e-02 (5.9098e-02)	Acc@1  98.44 ( 97.96)	Acc@5 100.00 ( 99.99)
Epoch: [38][270/391]	Time  0.142 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.6656e-02 (5.9164e-02)	Acc@1  98.44 ( 97.95)	Acc@5 100.00 ( 99.99)
Epoch: [38][280/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1089e-01 (5.9816e-02)	Acc@1  95.31 ( 97.94)	Acc@5 100.00 ( 99.99)
Epoch: [38][290/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.5314e-02 (5.9712e-02)	Acc@1  96.88 ( 97.92)	Acc@5 100.00 ( 99.99)
Epoch: [38][300/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.4093e-02 (5.9743e-02)	Acc@1 100.00 ( 97.92)	Acc@5 100.00 ( 99.99)
Epoch: [38][310/391]	Time  0.134 ( 0.137)	Data  0.002 ( 0.005)	Loss 4.7859e-02 (5.9267e-02)	Acc@1  97.66 ( 97.94)	Acc@5 100.00 ( 99.99)
Epoch: [38][320/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.0606e-02 (5.8710e-02)	Acc@1  99.22 ( 97.97)	Acc@5 100.00 ( 99.99)
Epoch: [38][330/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.8938e-02 (5.8180e-02)	Acc@1  97.66 ( 97.99)	Acc@5 100.00 ( 99.99)
Epoch: [38][340/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.7008e-02 (5.8154e-02)	Acc@1  96.88 ( 97.99)	Acc@5 100.00 ( 99.99)
Epoch: [38][350/391]	Time  0.130 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.0116e-02 (5.8143e-02)	Acc@1  98.44 ( 97.99)	Acc@5 100.00 ( 99.99)
Epoch: [38][360/391]	Time  0.134 ( 0.137)	Data  0.002 ( 0.005)	Loss 4.7727e-02 (5.8146e-02)	Acc@1  98.44 ( 97.98)	Acc@5 100.00 ( 99.99)
Epoch: [38][370/391]	Time  0.140 ( 0.137)	Data  0.002 ( 0.005)	Loss 4.5622e-02 (5.8214e-02)	Acc@1  99.22 ( 97.98)	Acc@5 100.00 ( 99.99)
Epoch: [38][380/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.3546e-02 (5.7882e-02)	Acc@1  99.22 ( 97.99)	Acc@5 100.00 ( 99.99)
Epoch: [38][390/391]	Time  0.130 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.8035e-02 (5.7651e-02)	Acc@1 100.00 ( 98.00)	Acc@5 100.00 ( 99.99)
## e[38] optimizer.zero_grad (sum) time: 0.2839503288269043
## e[38]       loss.backward (sum) time: 18.560786962509155
## e[38]      optimizer.step (sum) time: 2.946669816970825
## epoch[38] training(only) time: 53.87496590614319
# Switched to evaluate mode...
Test: [  0/100]	Time  0.207 ( 0.207)	Loss 1.2827e-01 (1.2827e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.043 ( 0.059)	Loss 4.3387e-01 (2.8120e-01)	Acc@1  91.00 ( 92.73)	Acc@5  99.00 ( 99.36)
Test: [ 20/100]	Time  0.043 ( 0.052)	Loss 3.5516e-01 (3.1072e-01)	Acc@1  90.00 ( 91.86)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.043 ( 0.049)	Loss 1.9091e-01 (3.2294e-01)	Acc@1  90.00 ( 91.68)	Acc@5 100.00 ( 99.58)
Test: [ 40/100]	Time  0.046 ( 0.048)	Loss 2.6422e-01 (3.3275e-01)	Acc@1  89.00 ( 91.51)	Acc@5  99.00 ( 99.54)
Test: [ 50/100]	Time  0.046 ( 0.047)	Loss 1.9689e-01 (3.3030e-01)	Acc@1  96.00 ( 91.61)	Acc@5 100.00 ( 99.59)
Test: [ 60/100]	Time  0.044 ( 0.047)	Loss 2.7760e-01 (3.2160e-01)	Acc@1  95.00 ( 91.80)	Acc@5 100.00 ( 99.66)
Test: [ 70/100]	Time  0.041 ( 0.046)	Loss 5.2306e-01 (3.1982e-01)	Acc@1  88.00 ( 91.82)	Acc@5 100.00 ( 99.68)
Test: [ 80/100]	Time  0.044 ( 0.046)	Loss 1.9002e-01 (3.1972e-01)	Acc@1  96.00 ( 91.98)	Acc@5 100.00 ( 99.68)
Test: [ 90/100]	Time  0.046 ( 0.046)	Loss 1.6261e-01 (3.1513e-01)	Acc@1  96.00 ( 92.02)	Acc@5 100.00 ( 99.71)
 * Acc@1 92.000 Acc@5 99.710
### epoch[38] execution time: 58.54040312767029
EPOCH 39
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [39][  0/391]	Time  0.302 ( 0.302)	Data  0.182 ( 0.182)	Loss 4.1545e-02 (4.1545e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [39][ 10/391]	Time  0.135 ( 0.151)	Data  0.001 ( 0.020)	Loss 3.0283e-02 (3.8820e-02)	Acc@1  99.22 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [39][ 20/391]	Time  0.139 ( 0.144)	Data  0.001 ( 0.013)	Loss 6.7770e-02 (4.4203e-02)	Acc@1  97.66 ( 98.40)	Acc@5 100.00 (100.00)
Epoch: [39][ 30/391]	Time  0.135 ( 0.142)	Data  0.001 ( 0.010)	Loss 5.8393e-02 (4.6265e-02)	Acc@1  98.44 ( 98.41)	Acc@5 100.00 (100.00)
Epoch: [39][ 40/391]	Time  0.141 ( 0.140)	Data  0.001 ( 0.009)	Loss 6.5743e-02 (4.9216e-02)	Acc@1  97.66 ( 98.29)	Acc@5 100.00 (100.00)
Epoch: [39][ 50/391]	Time  0.136 ( 0.139)	Data  0.002 ( 0.008)	Loss 2.9671e-02 (4.9438e-02)	Acc@1 100.00 ( 98.28)	Acc@5 100.00 ( 99.98)
Epoch: [39][ 60/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.007)	Loss 4.9869e-02 (5.2524e-02)	Acc@1  99.22 ( 98.13)	Acc@5 100.00 ( 99.99)
Epoch: [39][ 70/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.007)	Loss 5.2204e-02 (5.2617e-02)	Acc@1  98.44 ( 98.17)	Acc@5 100.00 ( 99.99)
Epoch: [39][ 80/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.007)	Loss 8.1188e-02 (5.4225e-02)	Acc@1  96.88 ( 98.16)	Acc@5 100.00 ( 99.98)
Epoch: [39][ 90/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.006)	Loss 9.8540e-02 (5.4380e-02)	Acc@1  97.66 ( 98.15)	Acc@5  99.22 ( 99.97)
Epoch: [39][100/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.2029e-01 (5.5024e-02)	Acc@1  94.53 ( 98.09)	Acc@5 100.00 ( 99.98)
Epoch: [39][110/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.8880e-02 (5.4809e-02)	Acc@1  97.66 ( 98.06)	Acc@5 100.00 ( 99.98)
Epoch: [39][120/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 5.8809e-02 (5.5610e-02)	Acc@1  96.88 ( 98.00)	Acc@5 100.00 ( 99.98)
Epoch: [39][130/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.2130e-02 (5.4374e-02)	Acc@1 100.00 ( 98.06)	Acc@5 100.00 ( 99.98)
Epoch: [39][140/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 9.0026e-02 (5.4702e-02)	Acc@1  94.53 ( 98.03)	Acc@5 100.00 ( 99.98)
Epoch: [39][150/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.2872e-02 (5.3731e-02)	Acc@1  99.22 ( 98.07)	Acc@5 100.00 ( 99.98)
Epoch: [39][160/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.8806e-02 (5.3315e-02)	Acc@1  99.22 ( 98.13)	Acc@5 100.00 ( 99.98)
Epoch: [39][170/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.6446e-02 (5.3146e-02)	Acc@1  97.66 ( 98.15)	Acc@5 100.00 ( 99.98)
Epoch: [39][180/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.2734e-02 (5.2717e-02)	Acc@1  99.22 ( 98.17)	Acc@5 100.00 ( 99.98)
Epoch: [39][190/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.7161e-02 (5.2390e-02)	Acc@1  96.88 ( 98.17)	Acc@5 100.00 ( 99.98)
Epoch: [39][200/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.4669e-02 (5.2024e-02)	Acc@1  99.22 ( 98.18)	Acc@5 100.00 ( 99.98)
Epoch: [39][210/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.6243e-02 (5.1689e-02)	Acc@1  99.22 ( 98.19)	Acc@5 100.00 ( 99.99)
Epoch: [39][220/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.6796e-02 (5.1994e-02)	Acc@1  98.44 ( 98.18)	Acc@5 100.00 ( 99.99)
Epoch: [39][230/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.1514e-02 (5.2672e-02)	Acc@1  96.88 ( 98.17)	Acc@5 100.00 ( 99.98)
Epoch: [39][240/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.3303e-02 (5.2820e-02)	Acc@1 100.00 ( 98.15)	Acc@5 100.00 ( 99.98)
Epoch: [39][250/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.4196e-02 (5.2631e-02)	Acc@1  98.44 ( 98.16)	Acc@5 100.00 ( 99.98)
Epoch: [39][260/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.0768e-02 (5.3115e-02)	Acc@1  96.88 ( 98.14)	Acc@5 100.00 ( 99.99)
Epoch: [39][270/391]	Time  0.141 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.8076e-02 (5.2730e-02)	Acc@1  98.44 ( 98.17)	Acc@5 100.00 ( 99.99)
Epoch: [39][280/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.7391e-02 (5.2685e-02)	Acc@1  96.88 ( 98.17)	Acc@5 100.00 ( 99.99)
Epoch: [39][290/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1338e-01 (5.3356e-02)	Acc@1  96.88 ( 98.15)	Acc@5 100.00 ( 99.99)
Epoch: [39][300/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.0090e-02 (5.3150e-02)	Acc@1  98.44 ( 98.16)	Acc@5 100.00 ( 99.99)
Epoch: [39][310/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.6142e-02 (5.3086e-02)	Acc@1  98.44 ( 98.15)	Acc@5 100.00 ( 99.99)
Epoch: [39][320/391]	Time  0.134 ( 0.137)	Data  0.002 ( 0.005)	Loss 1.0777e-01 (5.3650e-02)	Acc@1  96.09 ( 98.13)	Acc@5 100.00 ( 99.99)
Epoch: [39][330/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.3634e-02 (5.3538e-02)	Acc@1  99.22 ( 98.13)	Acc@5 100.00 ( 99.99)
Epoch: [39][340/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.5772e-02 (5.3325e-02)	Acc@1  99.22 ( 98.14)	Acc@5 100.00 ( 99.99)
Epoch: [39][350/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0025e-01 (5.3497e-02)	Acc@1  96.09 ( 98.13)	Acc@5 100.00 ( 99.99)
Epoch: [39][360/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.2563e-02 (5.3734e-02)	Acc@1  97.66 ( 98.13)	Acc@5 100.00 ( 99.99)
Epoch: [39][370/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.5890e-02 (5.3784e-02)	Acc@1  99.22 ( 98.13)	Acc@5 100.00 ( 99.99)
Epoch: [39][380/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.6288e-02 (5.3947e-02)	Acc@1  97.66 ( 98.12)	Acc@5 100.00 ( 99.99)
Epoch: [39][390/391]	Time  0.125 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.0758e-02 (5.3913e-02)	Acc@1  98.75 ( 98.12)	Acc@5 100.00 ( 99.99)
## e[39] optimizer.zero_grad (sum) time: 0.2841494083404541
## e[39]       loss.backward (sum) time: 18.488014698028564
## e[39]      optimizer.step (sum) time: 2.8624422550201416
## epoch[39] training(only) time: 53.72688674926758
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 1.5575e-01 (1.5575e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.042 ( 0.057)	Loss 4.9681e-01 (2.9005e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.043 ( 0.051)	Loss 3.5394e-01 (3.1961e-01)	Acc@1  86.00 ( 91.38)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.043 ( 0.049)	Loss 2.0194e-01 (3.3406e-01)	Acc@1  94.00 ( 91.65)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.042 ( 0.048)	Loss 2.9971e-01 (3.4155e-01)	Acc@1  90.00 ( 91.51)	Acc@5  99.00 ( 99.59)
Test: [ 50/100]	Time  0.042 ( 0.047)	Loss 2.5695e-01 (3.3851e-01)	Acc@1  95.00 ( 91.61)	Acc@5  99.00 ( 99.63)
Test: [ 60/100]	Time  0.041 ( 0.046)	Loss 3.0574e-01 (3.3401e-01)	Acc@1  93.00 ( 91.67)	Acc@5 100.00 ( 99.67)
Test: [ 70/100]	Time  0.044 ( 0.046)	Loss 5.2264e-01 (3.3199e-01)	Acc@1  88.00 ( 91.72)	Acc@5 100.00 ( 99.69)
Test: [ 80/100]	Time  0.042 ( 0.045)	Loss 2.8314e-01 (3.3015e-01)	Acc@1  92.00 ( 91.80)	Acc@5 100.00 ( 99.69)
Test: [ 90/100]	Time  0.043 ( 0.045)	Loss 1.9513e-01 (3.2444e-01)	Acc@1  95.00 ( 91.88)	Acc@5 100.00 ( 99.73)
 * Acc@1 91.950 Acc@5 99.730
### epoch[39] execution time: 58.32915925979614
EPOCH 40
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [40][  0/391]	Time  0.308 ( 0.308)	Data  0.186 ( 0.186)	Loss 5.2818e-02 (5.2818e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [40][ 10/391]	Time  0.132 ( 0.151)	Data  0.001 ( 0.021)	Loss 1.7994e-02 (6.3477e-02)	Acc@1 100.00 ( 98.01)	Acc@5 100.00 ( 99.93)
Epoch: [40][ 20/391]	Time  0.132 ( 0.144)	Data  0.001 ( 0.013)	Loss 4.4056e-02 (5.6723e-02)	Acc@1  98.44 ( 98.29)	Acc@5 100.00 ( 99.96)
Epoch: [40][ 30/391]	Time  0.134 ( 0.141)	Data  0.001 ( 0.010)	Loss 3.2675e-02 (5.2635e-02)	Acc@1  99.22 ( 98.36)	Acc@5 100.00 ( 99.97)
Epoch: [40][ 40/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.009)	Loss 3.4685e-02 (5.2265e-02)	Acc@1  99.22 ( 98.29)	Acc@5 100.00 ( 99.98)
Epoch: [40][ 50/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.008)	Loss 7.0923e-02 (5.3444e-02)	Acc@1  96.09 ( 98.21)	Acc@5 100.00 ( 99.98)
Epoch: [40][ 60/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.007)	Loss 2.8566e-02 (5.2216e-02)	Acc@1  98.44 ( 98.22)	Acc@5 100.00 ( 99.99)
Epoch: [40][ 70/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.007)	Loss 8.3939e-02 (5.2360e-02)	Acc@1  97.66 ( 98.22)	Acc@5 100.00 ( 99.99)
Epoch: [40][ 80/391]	Time  0.130 ( 0.138)	Data  0.001 ( 0.007)	Loss 2.3054e-02 (5.0178e-02)	Acc@1 100.00 ( 98.30)	Acc@5 100.00 ( 99.99)
Epoch: [40][ 90/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.006)	Loss 6.0967e-02 (5.0392e-02)	Acc@1  98.44 ( 98.32)	Acc@5 100.00 ( 99.99)
Epoch: [40][100/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.7035e-02 (5.0204e-02)	Acc@1  98.44 ( 98.31)	Acc@5 100.00 ( 99.98)
Epoch: [40][110/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.8776e-02 (4.9529e-02)	Acc@1 100.00 ( 98.33)	Acc@5 100.00 ( 99.99)
Epoch: [40][120/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 6.2680e-02 (4.9428e-02)	Acc@1  97.66 ( 98.31)	Acc@5 100.00 ( 99.99)
Epoch: [40][130/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.006)	Loss 9.0031e-02 (4.9773e-02)	Acc@1  99.22 ( 98.31)	Acc@5 100.00 ( 99.99)
Epoch: [40][140/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 5.6612e-02 (5.0032e-02)	Acc@1  97.66 ( 98.29)	Acc@5 100.00 ( 99.99)
Epoch: [40][150/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 6.2865e-02 (5.0396e-02)	Acc@1  97.66 ( 98.28)	Acc@5 100.00 ( 99.99)
Epoch: [40][160/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.006)	Loss 9.2581e-02 (5.0913e-02)	Acc@1  96.88 ( 98.28)	Acc@5 100.00 ( 99.99)
Epoch: [40][170/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.2949e-02 (5.0699e-02)	Acc@1  98.44 ( 98.30)	Acc@5 100.00 ( 99.99)
Epoch: [40][180/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.3783e-02 (5.0503e-02)	Acc@1  99.22 ( 98.30)	Acc@5 100.00 ( 99.99)
Epoch: [40][190/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.4513e-02 (5.0571e-02)	Acc@1  98.44 ( 98.30)	Acc@5 100.00 ( 99.99)
Epoch: [40][200/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.7871e-02 (5.0142e-02)	Acc@1  98.44 ( 98.33)	Acc@5 100.00 ( 99.99)
Epoch: [40][210/391]	Time  0.130 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.7547e-02 (4.9723e-02)	Acc@1  96.88 ( 98.34)	Acc@5 100.00 ( 99.99)
Epoch: [40][220/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.0716e-02 (5.0714e-02)	Acc@1  99.22 ( 98.29)	Acc@5 100.00 ( 99.99)
Epoch: [40][230/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.3273e-02 (5.0468e-02)	Acc@1  97.66 ( 98.31)	Acc@5 100.00 ( 99.99)
Epoch: [40][240/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.3291e-02 (5.1003e-02)	Acc@1  98.44 ( 98.29)	Acc@5 100.00 ( 99.99)
Epoch: [40][250/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.2281e-02 (5.0962e-02)	Acc@1  97.66 ( 98.29)	Acc@5 100.00 ( 99.99)
Epoch: [40][260/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.1180e-02 (5.1017e-02)	Acc@1  99.22 ( 98.29)	Acc@5 100.00 ( 99.99)
Epoch: [40][270/391]	Time  0.135 ( 0.137)	Data  0.002 ( 0.005)	Loss 2.5265e-02 (5.1202e-02)	Acc@1  99.22 ( 98.30)	Acc@5 100.00 ( 99.99)
Epoch: [40][280/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1487e-01 (5.1577e-02)	Acc@1  94.53 ( 98.27)	Acc@5 100.00 ( 99.99)
Epoch: [40][290/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.2512e-02 (5.1495e-02)	Acc@1  96.88 ( 98.26)	Acc@5 100.00 ( 99.99)
Epoch: [40][300/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.1479e-02 (5.1661e-02)	Acc@1  97.66 ( 98.26)	Acc@5 100.00 ( 99.99)
Epoch: [40][310/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1342e-01 (5.1955e-02)	Acc@1  96.09 ( 98.25)	Acc@5 100.00 ( 99.99)
Epoch: [40][320/391]	Time  0.138 ( 0.137)	Data  0.002 ( 0.005)	Loss 3.7957e-02 (5.1452e-02)	Acc@1  98.44 ( 98.26)	Acc@5 100.00 ( 99.99)
Epoch: [40][330/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.3625e-02 (5.1228e-02)	Acc@1  99.22 ( 98.26)	Acc@5 100.00 ( 99.99)
Epoch: [40][340/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.1342e-02 (5.1342e-02)	Acc@1  97.66 ( 98.25)	Acc@5 100.00 ( 99.99)
Epoch: [40][350/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.9047e-02 (5.1505e-02)	Acc@1  98.44 ( 98.25)	Acc@5 100.00 ( 99.99)
Epoch: [40][360/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.9543e-02 (5.0946e-02)	Acc@1 100.00 ( 98.27)	Acc@5 100.00 ( 99.99)
Epoch: [40][370/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.8931e-02 (5.1022e-02)	Acc@1  96.88 ( 98.26)	Acc@5 100.00 ( 99.99)
Epoch: [40][380/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.6748e-02 (5.0715e-02)	Acc@1  99.22 ( 98.27)	Acc@5 100.00 ( 99.99)
Epoch: [40][390/391]	Time  0.129 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.0912e-02 (5.0923e-02)	Acc@1  98.75 ( 98.26)	Acc@5 100.00 ( 99.99)
## e[40] optimizer.zero_grad (sum) time: 0.2855088710784912
## e[40]       loss.backward (sum) time: 18.52273726463318
## e[40]      optimizer.step (sum) time: 2.9014532566070557
## epoch[40] training(only) time: 53.817198753356934
# Switched to evaluate mode...
Test: [  0/100]	Time  0.190 ( 0.190)	Loss 2.0428e-01 (2.0428e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.043 ( 0.057)	Loss 4.9402e-01 (2.8777e-01)	Acc@1  90.00 ( 92.09)	Acc@5  99.00 ( 99.82)
Test: [ 20/100]	Time  0.042 ( 0.050)	Loss 3.7021e-01 (3.2680e-01)	Acc@1  88.00 ( 91.24)	Acc@5 100.00 ( 99.90)
Test: [ 30/100]	Time  0.044 ( 0.048)	Loss 2.2207e-01 (3.4241e-01)	Acc@1  91.00 ( 91.42)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.045 ( 0.047)	Loss 3.4746e-01 (3.5115e-01)	Acc@1  88.00 ( 91.27)	Acc@5  98.00 ( 99.66)
Test: [ 50/100]	Time  0.041 ( 0.046)	Loss 2.2328e-01 (3.4849e-01)	Acc@1  94.00 ( 91.27)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.045 ( 0.046)	Loss 3.2261e-01 (3.4201e-01)	Acc@1  92.00 ( 91.46)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.044 ( 0.046)	Loss 5.2105e-01 (3.3723e-01)	Acc@1  89.00 ( 91.49)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.044 ( 0.046)	Loss 2.1072e-01 (3.3875e-01)	Acc@1  95.00 ( 91.59)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.043 ( 0.045)	Loss 1.4354e-01 (3.2963e-01)	Acc@1  97.00 ( 91.76)	Acc@5 100.00 ( 99.78)
 * Acc@1 91.810 Acc@5 99.770
### epoch[40] execution time: 58.44604754447937
EPOCH 41
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [41][  0/391]	Time  0.304 ( 0.304)	Data  0.178 ( 0.178)	Loss 6.1940e-02 (6.1940e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [41][ 10/391]	Time  0.137 ( 0.151)	Data  0.001 ( 0.020)	Loss 2.6187e-02 (5.6595e-02)	Acc@1  98.44 ( 98.01)	Acc@5 100.00 (100.00)
Epoch: [41][ 20/391]	Time  0.134 ( 0.143)	Data  0.001 ( 0.012)	Loss 8.0652e-02 (5.4968e-02)	Acc@1  96.09 ( 98.07)	Acc@5 100.00 (100.00)
Epoch: [41][ 30/391]	Time  0.133 ( 0.141)	Data  0.001 ( 0.010)	Loss 6.4025e-02 (5.4371e-02)	Acc@1  98.44 ( 98.03)	Acc@5 100.00 (100.00)
Epoch: [41][ 40/391]	Time  0.132 ( 0.139)	Data  0.002 ( 0.008)	Loss 3.0396e-02 (5.0886e-02)	Acc@1  99.22 ( 98.17)	Acc@5 100.00 (100.00)
Epoch: [41][ 50/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.008)	Loss 6.3602e-02 (5.1245e-02)	Acc@1  97.66 ( 98.21)	Acc@5 100.00 (100.00)
Epoch: [41][ 60/391]	Time  0.130 ( 0.138)	Data  0.001 ( 0.007)	Loss 4.2151e-02 (4.9962e-02)	Acc@1  99.22 ( 98.23)	Acc@5 100.00 (100.00)
Epoch: [41][ 70/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.007)	Loss 4.4739e-02 (4.8733e-02)	Acc@1  99.22 ( 98.29)	Acc@5 100.00 (100.00)
Epoch: [41][ 80/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 8.0627e-02 (4.9277e-02)	Acc@1  96.09 ( 98.27)	Acc@5 100.00 (100.00)
Epoch: [41][ 90/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.5083e-02 (4.7255e-02)	Acc@1 100.00 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [41][100/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.006)	Loss 8.3719e-02 (4.8499e-02)	Acc@1  96.88 ( 98.33)	Acc@5 100.00 (100.00)
Epoch: [41][110/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.006)	Loss 4.5035e-02 (4.6556e-02)	Acc@1  97.66 ( 98.39)	Acc@5 100.00 (100.00)
Epoch: [41][120/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.006)	Loss 3.5903e-02 (4.7764e-02)	Acc@1  99.22 ( 98.34)	Acc@5 100.00 (100.00)
Epoch: [41][130/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.006)	Loss 4.9807e-02 (4.6855e-02)	Acc@1  99.22 ( 98.40)	Acc@5 100.00 (100.00)
Epoch: [41][140/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.006)	Loss 4.8834e-02 (4.7114e-02)	Acc@1  99.22 ( 98.41)	Acc@5 100.00 (100.00)
Epoch: [41][150/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.006)	Loss 7.6508e-02 (4.6993e-02)	Acc@1  96.88 ( 98.43)	Acc@5 100.00 (100.00)
Epoch: [41][160/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.0160e-02 (4.6574e-02)	Acc@1  99.22 ( 98.45)	Acc@5 100.00 (100.00)
Epoch: [41][170/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.7198e-02 (4.6482e-02)	Acc@1  97.66 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [41][180/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.8556e-02 (4.6205e-02)	Acc@1  98.44 ( 98.43)	Acc@5 100.00 (100.00)
Epoch: [41][190/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.6027e-02 (4.5985e-02)	Acc@1  96.09 ( 98.42)	Acc@5 100.00 (100.00)
Epoch: [41][200/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.2026e-02 (4.6163e-02)	Acc@1  96.88 ( 98.40)	Acc@5 100.00 (100.00)
Epoch: [41][210/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.6075e-02 (4.6415e-02)	Acc@1  96.09 ( 98.39)	Acc@5 100.00 (100.00)
Epoch: [41][220/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.3816e-02 (4.7093e-02)	Acc@1  97.66 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [41][230/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.4883e-02 (4.6952e-02)	Acc@1  98.44 ( 98.38)	Acc@5 100.00 (100.00)
Epoch: [41][240/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.4455e-02 (4.7460e-02)	Acc@1  97.66 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [41][250/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.9917e-02 (4.7595e-02)	Acc@1  99.22 ( 98.38)	Acc@5 100.00 (100.00)
Epoch: [41][260/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.6252e-02 (4.7454e-02)	Acc@1 100.00 ( 98.38)	Acc@5 100.00 (100.00)
Epoch: [41][270/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.4150e-02 (4.7605e-02)	Acc@1  96.88 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [41][280/391]	Time  0.135 ( 0.137)	Data  0.002 ( 0.005)	Loss 5.0361e-02 (4.7678e-02)	Acc@1  97.66 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [41][290/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.5847e-02 (4.7787e-02)	Acc@1  99.22 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [41][300/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.5513e-02 (4.7732e-02)	Acc@1  98.44 ( 98.38)	Acc@5 100.00 (100.00)
Epoch: [41][310/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.8775e-02 (4.7667e-02)	Acc@1  97.66 ( 98.38)	Acc@5 100.00 (100.00)
Epoch: [41][320/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.5531e-02 (4.7467e-02)	Acc@1 100.00 ( 98.38)	Acc@5 100.00 (100.00)
Epoch: [41][330/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.4929e-02 (4.7296e-02)	Acc@1  98.44 ( 98.39)	Acc@5 100.00 (100.00)
Epoch: [41][340/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.4263e-02 (4.7662e-02)	Acc@1  98.44 ( 98.38)	Acc@5 100.00 (100.00)
Epoch: [41][350/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.8132e-02 (4.7440e-02)	Acc@1  98.44 ( 98.39)	Acc@5 100.00 (100.00)
Epoch: [41][360/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.5718e-02 (4.7425e-02)	Acc@1 100.00 ( 98.39)	Acc@5 100.00 (100.00)
Epoch: [41][370/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.9020e-02 (4.7643e-02)	Acc@1  97.66 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [41][380/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.3146e-02 (4.7688e-02)	Acc@1  98.44 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [41][390/391]	Time  0.129 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.6268e-02 (4.7837e-02)	Acc@1  98.75 ( 98.37)	Acc@5 100.00 (100.00)
## e[41] optimizer.zero_grad (sum) time: 0.2860116958618164
## e[41]       loss.backward (sum) time: 18.515374183654785
## e[41]      optimizer.step (sum) time: 2.8770525455474854
## epoch[41] training(only) time: 53.73841404914856
# Switched to evaluate mode...
Test: [  0/100]	Time  0.206 ( 0.206)	Loss 2.1065e-01 (2.1065e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.041 ( 0.057)	Loss 4.4528e-01 (3.0012e-01)	Acc@1  91.00 ( 91.91)	Acc@5  99.00 ( 99.73)
Test: [ 20/100]	Time  0.043 ( 0.051)	Loss 4.5140e-01 (3.3124e-01)	Acc@1  88.00 ( 91.57)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.044 ( 0.048)	Loss 2.3029e-01 (3.4782e-01)	Acc@1  92.00 ( 91.52)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.042 ( 0.047)	Loss 4.2172e-01 (3.5968e-01)	Acc@1  89.00 ( 91.29)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.041 ( 0.046)	Loss 2.5689e-01 (3.5326e-01)	Acc@1  95.00 ( 91.45)	Acc@5  99.00 ( 99.75)
Test: [ 60/100]	Time  0.044 ( 0.046)	Loss 3.0742e-01 (3.4758e-01)	Acc@1  94.00 ( 91.48)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.043 ( 0.046)	Loss 4.5340e-01 (3.4234e-01)	Acc@1  90.00 ( 91.49)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.043 ( 0.045)	Loss 1.9920e-01 (3.3853e-01)	Acc@1  94.00 ( 91.64)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.042 ( 0.045)	Loss 2.1159e-01 (3.3156e-01)	Acc@1  93.00 ( 91.71)	Acc@5 100.00 ( 99.81)
 * Acc@1 91.780 Acc@5 99.820
### epoch[41] execution time: 58.33965826034546
EPOCH 42
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [42][  0/391]	Time  0.306 ( 0.306)	Data  0.174 ( 0.174)	Loss 6.1596e-02 (6.1596e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [42][ 10/391]	Time  0.133 ( 0.151)	Data  0.001 ( 0.020)	Loss 2.6938e-02 (4.7186e-02)	Acc@1  99.22 ( 98.22)	Acc@5 100.00 (100.00)
Epoch: [42][ 20/391]	Time  0.134 ( 0.144)	Data  0.001 ( 0.012)	Loss 1.1555e-02 (4.4663e-02)	Acc@1 100.00 ( 98.33)	Acc@5 100.00 (100.00)
Epoch: [42][ 30/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.010)	Loss 8.1965e-02 (4.3020e-02)	Acc@1  96.09 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [42][ 40/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.008)	Loss 4.7761e-02 (4.1872e-02)	Acc@1  98.44 ( 98.46)	Acc@5 100.00 (100.00)
Epoch: [42][ 50/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.008)	Loss 6.2936e-02 (4.2602e-02)	Acc@1  98.44 ( 98.50)	Acc@5 100.00 (100.00)
Epoch: [42][ 60/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.007)	Loss 2.0237e-02 (4.1471e-02)	Acc@1  99.22 ( 98.62)	Acc@5 100.00 (100.00)
Epoch: [42][ 70/391]	Time  0.131 ( 0.139)	Data  0.001 ( 0.007)	Loss 5.3578e-02 (4.1512e-02)	Acc@1  98.44 ( 98.62)	Acc@5 100.00 (100.00)
Epoch: [42][ 80/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.006)	Loss 9.1515e-02 (4.3467e-02)	Acc@1  96.09 ( 98.51)	Acc@5 100.00 (100.00)
Epoch: [42][ 90/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.4235e-02 (4.2075e-02)	Acc@1 100.00 ( 98.56)	Acc@5 100.00 (100.00)
Epoch: [42][100/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.4899e-02 (4.1919e-02)	Acc@1 100.00 ( 98.57)	Acc@5 100.00 (100.00)
Epoch: [42][110/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.006)	Loss 6.3872e-02 (4.1246e-02)	Acc@1  97.66 ( 98.59)	Acc@5 100.00 ( 99.99)
Epoch: [42][120/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.7459e-02 (4.0834e-02)	Acc@1  99.22 ( 98.62)	Acc@5 100.00 ( 99.99)
Epoch: [42][130/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.2662e-02 (4.0312e-02)	Acc@1  98.44 ( 98.64)	Acc@5 100.00 ( 99.99)
Epoch: [42][140/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.8015e-02 (4.1021e-02)	Acc@1  99.22 ( 98.59)	Acc@5 100.00 ( 99.99)
Epoch: [42][150/391]	Time  0.138 ( 0.138)	Data  0.002 ( 0.006)	Loss 2.7368e-02 (4.1645e-02)	Acc@1  98.44 ( 98.57)	Acc@5 100.00 ( 99.99)
Epoch: [42][160/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.0656e-02 (4.1541e-02)	Acc@1  99.22 ( 98.56)	Acc@5 100.00 (100.00)
Epoch: [42][170/391]	Time  0.132 ( 0.138)	Data  0.002 ( 0.005)	Loss 5.2714e-02 (4.2462e-02)	Acc@1  97.66 ( 98.52)	Acc@5 100.00 (100.00)
Epoch: [42][180/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.2721e-02 (4.2167e-02)	Acc@1  99.22 ( 98.54)	Acc@5 100.00 (100.00)
Epoch: [42][190/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.6016e-02 (4.2092e-02)	Acc@1  98.44 ( 98.52)	Acc@5 100.00 (100.00)
Epoch: [42][200/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.7840e-02 (4.2383e-02)	Acc@1 100.00 ( 98.51)	Acc@5 100.00 (100.00)
Epoch: [42][210/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 4.8425e-02 (4.2652e-02)	Acc@1  98.44 ( 98.51)	Acc@5 100.00 (100.00)
Epoch: [42][220/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 5.6779e-02 (4.2974e-02)	Acc@1  97.66 ( 98.51)	Acc@5 100.00 (100.00)
Epoch: [42][230/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.005)	Loss 5.7124e-02 (4.3133e-02)	Acc@1  98.44 ( 98.50)	Acc@5 100.00 (100.00)
Epoch: [42][240/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.005)	Loss 4.1891e-02 (4.3439e-02)	Acc@1  97.66 ( 98.49)	Acc@5 100.00 (100.00)
Epoch: [42][250/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.5612e-02 (4.3432e-02)	Acc@1 100.00 ( 98.49)	Acc@5 100.00 (100.00)
Epoch: [42][260/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.4855e-02 (4.3221e-02)	Acc@1  99.22 ( 98.49)	Acc@5 100.00 (100.00)
Epoch: [42][270/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.0693e-02 (4.3613e-02)	Acc@1  98.44 ( 98.47)	Acc@5 100.00 (100.00)
Epoch: [42][280/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.8014e-02 (4.3830e-02)	Acc@1  97.66 ( 98.47)	Acc@5 100.00 (100.00)
Epoch: [42][290/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.6575e-02 (4.3538e-02)	Acc@1  97.66 ( 98.48)	Acc@5 100.00 (100.00)
Epoch: [42][300/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.3335e-02 (4.3982e-02)	Acc@1  98.44 ( 98.47)	Acc@5 100.00 (100.00)
Epoch: [42][310/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.5911e-02 (4.4049e-02)	Acc@1  97.66 ( 98.46)	Acc@5 100.00 (100.00)
Epoch: [42][320/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.9320e-02 (4.4007e-02)	Acc@1  98.44 ( 98.48)	Acc@5 100.00 (100.00)
Epoch: [42][330/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.0439e-02 (4.4017e-02)	Acc@1  97.66 ( 98.48)	Acc@5 100.00 (100.00)
Epoch: [42][340/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.6253e-02 (4.3709e-02)	Acc@1  98.44 ( 98.50)	Acc@5 100.00 (100.00)
Epoch: [42][350/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.3861e-02 (4.3450e-02)	Acc@1  99.22 ( 98.50)	Acc@5 100.00 (100.00)
Epoch: [42][360/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1543e-02 (4.3465e-02)	Acc@1 100.00 ( 98.50)	Acc@5 100.00 (100.00)
Epoch: [42][370/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.4243e-02 (4.3742e-02)	Acc@1  99.22 ( 98.49)	Acc@5 100.00 (100.00)
Epoch: [42][380/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.0414e-02 (4.3437e-02)	Acc@1  99.22 ( 98.51)	Acc@5 100.00 (100.00)
Epoch: [42][390/391]	Time  0.128 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.8143e-02 (4.3507e-02)	Acc@1  97.50 ( 98.50)	Acc@5 100.00 (100.00)
## e[42] optimizer.zero_grad (sum) time: 0.2870631217956543
## e[42]       loss.backward (sum) time: 18.532007694244385
## e[42]      optimizer.step (sum) time: 2.905783176422119
## epoch[42] training(only) time: 53.81583595275879
# Switched to evaluate mode...
Test: [  0/100]	Time  0.209 ( 0.209)	Loss 2.0555e-01 (2.0555e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.043 ( 0.060)	Loss 4.4287e-01 (3.0764e-01)	Acc@1  89.00 ( 91.91)	Acc@5  99.00 ( 99.73)
Test: [ 20/100]	Time  0.042 ( 0.052)	Loss 3.7642e-01 (3.3575e-01)	Acc@1  88.00 ( 91.43)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.045 ( 0.049)	Loss 2.0259e-01 (3.5214e-01)	Acc@1  92.00 ( 91.52)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.046 ( 0.048)	Loss 3.9009e-01 (3.6187e-01)	Acc@1  88.00 ( 91.15)	Acc@5  99.00 ( 99.71)
Test: [ 50/100]	Time  0.042 ( 0.047)	Loss 1.7435e-01 (3.5645e-01)	Acc@1  95.00 ( 91.27)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.043 ( 0.046)	Loss 3.1336e-01 (3.5010e-01)	Acc@1  95.00 ( 91.39)	Acc@5  99.00 ( 99.74)
Test: [ 70/100]	Time  0.042 ( 0.046)	Loss 4.8310e-01 (3.4631e-01)	Acc@1  89.00 ( 91.39)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.043 ( 0.046)	Loss 2.0397e-01 (3.4625e-01)	Acc@1  94.00 ( 91.43)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.043 ( 0.045)	Loss 1.4061e-01 (3.3999e-01)	Acc@1  96.00 ( 91.59)	Acc@5 100.00 ( 99.77)
 * Acc@1 91.650 Acc@5 99.770
### epoch[42] execution time: 58.45233869552612
EPOCH 43
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [43][  0/391]	Time  0.305 ( 0.305)	Data  0.180 ( 0.180)	Loss 2.3945e-02 (2.3945e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [43][ 10/391]	Time  0.136 ( 0.152)	Data  0.001 ( 0.020)	Loss 2.0259e-02 (4.0224e-02)	Acc@1  99.22 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [43][ 20/391]	Time  0.134 ( 0.144)	Data  0.001 ( 0.013)	Loss 1.2665e-02 (3.4741e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [43][ 30/391]	Time  0.135 ( 0.142)	Data  0.001 ( 0.010)	Loss 5.6779e-02 (3.4463e-02)	Acc@1  98.44 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [43][ 40/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.009)	Loss 4.0359e-02 (3.6458e-02)	Acc@1  98.44 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [43][ 50/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.008)	Loss 7.0838e-02 (3.6606e-02)	Acc@1  99.22 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [43][ 60/391]	Time  0.128 ( 0.139)	Data  0.001 ( 0.007)	Loss 7.2943e-02 (3.6816e-02)	Acc@1  96.88 ( 98.87)	Acc@5 100.00 (100.00)
Epoch: [43][ 70/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.007)	Loss 3.0600e-02 (3.5864e-02)	Acc@1  99.22 ( 98.90)	Acc@5 100.00 (100.00)
Epoch: [43][ 80/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.007)	Loss 3.7904e-02 (3.6742e-02)	Acc@1  98.44 ( 98.81)	Acc@5 100.00 (100.00)
Epoch: [43][ 90/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.006)	Loss 3.4803e-02 (3.7284e-02)	Acc@1  98.44 ( 98.81)	Acc@5 100.00 (100.00)
Epoch: [43][100/391]	Time  0.137 ( 0.138)	Data  0.002 ( 0.006)	Loss 3.4682e-02 (3.7904e-02)	Acc@1  98.44 ( 98.79)	Acc@5 100.00 (100.00)
Epoch: [43][110/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.5223e-02 (3.8026e-02)	Acc@1 100.00 ( 98.78)	Acc@5 100.00 (100.00)
Epoch: [43][120/391]	Time  0.133 ( 0.138)	Data  0.002 ( 0.006)	Loss 8.1814e-02 (3.8371e-02)	Acc@1  97.66 ( 98.76)	Acc@5 100.00 (100.00)
Epoch: [43][130/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.7413e-02 (3.7864e-02)	Acc@1  98.44 ( 98.75)	Acc@5 100.00 (100.00)
Epoch: [43][140/391]	Time  0.130 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.0552e-02 (3.8360e-02)	Acc@1  99.22 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [43][150/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.9490e-02 (3.8767e-02)	Acc@1  99.22 ( 98.70)	Acc@5 100.00 (100.00)
Epoch: [43][160/391]	Time  0.131 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.9626e-02 (3.8532e-02)	Acc@1 100.00 ( 98.70)	Acc@5 100.00 (100.00)
Epoch: [43][170/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.005)	Loss 8.8423e-02 (3.9545e-02)	Acc@1  96.09 ( 98.63)	Acc@5 100.00 (100.00)
Epoch: [43][180/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.005)	Loss 9.6147e-03 (3.9858e-02)	Acc@1 100.00 ( 98.64)	Acc@5 100.00 (100.00)
Epoch: [43][190/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.005)	Loss 5.1278e-02 (4.0701e-02)	Acc@1  98.44 ( 98.59)	Acc@5 100.00 (100.00)
Epoch: [43][200/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.9651e-02 (4.0823e-02)	Acc@1  97.66 ( 98.57)	Acc@5 100.00 (100.00)
Epoch: [43][210/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.8045e-02 (4.0847e-02)	Acc@1  99.22 ( 98.58)	Acc@5 100.00 (100.00)
Epoch: [43][220/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.2898e-02 (4.0894e-02)	Acc@1 100.00 ( 98.59)	Acc@5 100.00 (100.00)
Epoch: [43][230/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.2381e-02 (4.1356e-02)	Acc@1  98.44 ( 98.58)	Acc@5 100.00 (100.00)
Epoch: [43][240/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.5009e-02 (4.1476e-02)	Acc@1  97.66 ( 98.57)	Acc@5 100.00 (100.00)
Epoch: [43][250/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.1132e-02 (4.1411e-02)	Acc@1  99.22 ( 98.58)	Acc@5 100.00 (100.00)
Epoch: [43][260/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.9478e-02 (4.1211e-02)	Acc@1  97.66 ( 98.59)	Acc@5  99.22 (100.00)
Epoch: [43][270/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.6385e-02 (4.1171e-02)	Acc@1 100.00 ( 98.60)	Acc@5 100.00 (100.00)
Epoch: [43][280/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.4774e-02 (4.1328e-02)	Acc@1  97.66 ( 98.59)	Acc@5 100.00 (100.00)
Epoch: [43][290/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.4539e-02 (4.1518e-02)	Acc@1  98.44 ( 98.58)	Acc@5 100.00 (100.00)
Epoch: [43][300/391]	Time  0.135 ( 0.137)	Data  0.002 ( 0.005)	Loss 3.8797e-02 (4.1485e-02)	Acc@1  99.22 ( 98.58)	Acc@5 100.00 (100.00)
Epoch: [43][310/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.6697e-02 (4.1592e-02)	Acc@1  98.44 ( 98.58)	Acc@5 100.00 (100.00)
Epoch: [43][320/391]	Time  0.142 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.5624e-02 (4.1570e-02)	Acc@1 100.00 ( 98.58)	Acc@5 100.00 (100.00)
Epoch: [43][330/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1484e-01 (4.1761e-02)	Acc@1  96.09 ( 98.58)	Acc@5 100.00 (100.00)
Epoch: [43][340/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.1254e-02 (4.1656e-02)	Acc@1 100.00 ( 98.59)	Acc@5 100.00 (100.00)
Epoch: [43][350/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.1396e-02 (4.1803e-02)	Acc@1  99.22 ( 98.58)	Acc@5 100.00 (100.00)
Epoch: [43][360/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.5389e-02 (4.1584e-02)	Acc@1  99.22 ( 98.59)	Acc@5 100.00 (100.00)
Epoch: [43][370/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.5703e-02 (4.1490e-02)	Acc@1 100.00 ( 98.60)	Acc@5 100.00 (100.00)
Epoch: [43][380/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.1189e-02 (4.1417e-02)	Acc@1  98.44 ( 98.60)	Acc@5 100.00 (100.00)
Epoch: [43][390/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.2719e-02 (4.1248e-02)	Acc@1  98.75 ( 98.60)	Acc@5 100.00 (100.00)
## e[43] optimizer.zero_grad (sum) time: 0.2864830493927002
## e[43]       loss.backward (sum) time: 18.560307025909424
## e[43]      optimizer.step (sum) time: 2.918558359146118
## epoch[43] training(only) time: 53.85261821746826
# Switched to evaluate mode...
Test: [  0/100]	Time  0.197 ( 0.197)	Loss 1.4214e-01 (1.4214e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.042 ( 0.058)	Loss 4.7633e-01 (2.9108e-01)	Acc@1  90.00 ( 92.09)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.043 ( 0.051)	Loss 4.4250e-01 (3.3648e-01)	Acc@1  87.00 ( 91.00)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.044 ( 0.049)	Loss 1.8057e-01 (3.4521e-01)	Acc@1  92.00 ( 91.65)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.042 ( 0.047)	Loss 3.8047e-01 (3.5681e-01)	Acc@1  88.00 ( 91.41)	Acc@5  99.00 ( 99.63)
Test: [ 50/100]	Time  0.042 ( 0.047)	Loss 1.7697e-01 (3.5225e-01)	Acc@1  97.00 ( 91.65)	Acc@5  99.00 ( 99.67)
Test: [ 60/100]	Time  0.043 ( 0.046)	Loss 3.4289e-01 (3.4985e-01)	Acc@1  93.00 ( 91.75)	Acc@5 100.00 ( 99.70)
Test: [ 70/100]	Time  0.042 ( 0.046)	Loss 5.1261e-01 (3.4617e-01)	Acc@1  88.00 ( 91.72)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.046 ( 0.046)	Loss 1.7127e-01 (3.4830e-01)	Acc@1  95.00 ( 91.77)	Acc@5 100.00 ( 99.73)
Test: [ 90/100]	Time  0.045 ( 0.045)	Loss 2.7359e-01 (3.4193e-01)	Acc@1  92.00 ( 91.80)	Acc@5 100.00 ( 99.76)
 * Acc@1 91.860 Acc@5 99.750
### epoch[43] execution time: 58.470701932907104
EPOCH 44
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [44][  0/391]	Time  0.296 ( 0.296)	Data  0.179 ( 0.179)	Loss 3.3624e-02 (3.3624e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [44][ 10/391]	Time  0.137 ( 0.152)	Data  0.001 ( 0.020)	Loss 2.8215e-02 (4.6374e-02)	Acc@1  97.66 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [44][ 20/391]	Time  0.132 ( 0.144)	Data  0.001 ( 0.013)	Loss 1.0846e-01 (4.3208e-02)	Acc@1  96.09 ( 98.51)	Acc@5 100.00 (100.00)
Epoch: [44][ 30/391]	Time  0.133 ( 0.142)	Data  0.001 ( 0.010)	Loss 1.3372e-02 (4.2629e-02)	Acc@1 100.00 ( 98.66)	Acc@5 100.00 (100.00)
Epoch: [44][ 40/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.009)	Loss 3.2724e-02 (4.2820e-02)	Acc@1  98.44 ( 98.67)	Acc@5 100.00 ( 99.98)
Epoch: [44][ 50/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.008)	Loss 6.5061e-02 (4.0256e-02)	Acc@1  96.88 ( 98.71)	Acc@5 100.00 ( 99.98)
Epoch: [44][ 60/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.007)	Loss 2.5226e-02 (3.9971e-02)	Acc@1  99.22 ( 98.71)	Acc@5 100.00 ( 99.99)
Epoch: [44][ 70/391]	Time  0.143 ( 0.139)	Data  0.001 ( 0.007)	Loss 3.1188e-02 (3.9481e-02)	Acc@1  99.22 ( 98.71)	Acc@5 100.00 ( 99.99)
Epoch: [44][ 80/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.007)	Loss 1.4686e-02 (4.0027e-02)	Acc@1 100.00 ( 98.73)	Acc@5 100.00 ( 99.99)
Epoch: [44][ 90/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.006)	Loss 2.7536e-02 (3.9086e-02)	Acc@1 100.00 ( 98.76)	Acc@5 100.00 ( 99.99)
Epoch: [44][100/391]	Time  0.135 ( 0.138)	Data  0.002 ( 0.006)	Loss 3.5668e-02 (3.7978e-02)	Acc@1  98.44 ( 98.82)	Acc@5 100.00 ( 99.99)
Epoch: [44][110/391]	Time  0.135 ( 0.138)	Data  0.002 ( 0.006)	Loss 7.0109e-02 (3.7600e-02)	Acc@1  96.09 ( 98.80)	Acc@5 100.00 ( 99.99)
Epoch: [44][120/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.5632e-02 (3.7253e-02)	Acc@1  99.22 ( 98.81)	Acc@5 100.00 ( 99.99)
Epoch: [44][130/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.0168e-01 (3.7779e-02)	Acc@1  96.09 ( 98.80)	Acc@5 100.00 ( 99.99)
Epoch: [44][140/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 5.1081e-02 (3.7798e-02)	Acc@1  97.66 ( 98.78)	Acc@5 100.00 ( 99.99)
Epoch: [44][150/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.3749e-02 (3.7355e-02)	Acc@1  98.44 ( 98.78)	Acc@5 100.00 ( 99.99)
Epoch: [44][160/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.005)	Loss 5.5629e-02 (3.7320e-02)	Acc@1  96.88 ( 98.80)	Acc@5 100.00 ( 99.99)
Epoch: [44][170/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.005)	Loss 5.1472e-02 (3.6905e-02)	Acc@1  97.66 ( 98.80)	Acc@5 100.00 ( 99.99)
Epoch: [44][180/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.1472e-02 (3.7056e-02)	Acc@1  99.22 ( 98.80)	Acc@5 100.00 ( 99.99)
Epoch: [44][190/391]	Time  0.129 ( 0.138)	Data  0.001 ( 0.005)	Loss 5.1302e-02 (3.7025e-02)	Acc@1  97.66 ( 98.80)	Acc@5 100.00 ( 99.99)
Epoch: [44][200/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.005)	Loss 5.8677e-02 (3.6812e-02)	Acc@1  97.66 ( 98.80)	Acc@5 100.00 ( 99.99)
Epoch: [44][210/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.0044e-01 (3.6794e-02)	Acc@1  98.44 ( 98.82)	Acc@5 100.00 ( 99.99)
Epoch: [44][220/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 4.6292e-02 (3.6817e-02)	Acc@1  97.66 ( 98.82)	Acc@5 100.00 ( 99.99)
Epoch: [44][230/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.1857e-02 (3.6523e-02)	Acc@1 100.00 ( 98.82)	Acc@5 100.00 ( 99.99)
Epoch: [44][240/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.5068e-02 (3.6339e-02)	Acc@1  98.44 ( 98.82)	Acc@5 100.00 ( 99.99)
Epoch: [44][250/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.9750e-02 (3.6150e-02)	Acc@1  99.22 ( 98.82)	Acc@5 100.00 ( 99.99)
Epoch: [44][260/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.0339e-02 (3.5929e-02)	Acc@1 100.00 ( 98.84)	Acc@5 100.00 ( 99.99)
Epoch: [44][270/391]	Time  0.128 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.8815e-02 (3.5707e-02)	Acc@1  99.22 ( 98.85)	Acc@5 100.00 ( 99.99)
Epoch: [44][280/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.0771e-02 (3.5446e-02)	Acc@1  99.22 ( 98.86)	Acc@5 100.00 ( 99.99)
Epoch: [44][290/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.3340e-02 (3.5715e-02)	Acc@1  98.44 ( 98.85)	Acc@5 100.00 ( 99.99)
Epoch: [44][300/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.0391e-02 (3.5724e-02)	Acc@1  99.22 ( 98.85)	Acc@5 100.00 ( 99.99)
Epoch: [44][310/391]	Time  0.130 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.5187e-02 (3.5758e-02)	Acc@1  98.44 ( 98.84)	Acc@5 100.00 ( 99.99)
Epoch: [44][320/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.1467e-02 (3.5782e-02)	Acc@1  99.22 ( 98.83)	Acc@5 100.00 (100.00)
Epoch: [44][330/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.5858e-02 (3.5799e-02)	Acc@1  98.44 ( 98.83)	Acc@5 100.00 (100.00)
Epoch: [44][340/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.9679e-02 (3.5501e-02)	Acc@1  97.66 ( 98.83)	Acc@5 100.00 (100.00)
Epoch: [44][350/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.3965e-02 (3.5429e-02)	Acc@1  98.44 ( 98.84)	Acc@5  99.22 ( 99.99)
Epoch: [44][360/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.6111e-02 (3.5432e-02)	Acc@1  97.66 ( 98.83)	Acc@5 100.00 ( 99.99)
Epoch: [44][370/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.9570e-02 (3.5390e-02)	Acc@1  96.88 ( 98.84)	Acc@5 100.00 ( 99.99)
Epoch: [44][380/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.4784e-02 (3.5323e-02)	Acc@1  98.44 ( 98.84)	Acc@5 100.00 ( 99.99)
Epoch: [44][390/391]	Time  0.127 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.7946e-02 (3.5797e-02)	Acc@1  97.50 ( 98.82)	Acc@5 100.00 ( 99.99)
## e[44] optimizer.zero_grad (sum) time: 0.28800106048583984
## e[44]       loss.backward (sum) time: 18.604023933410645
## e[44]      optimizer.step (sum) time: 2.90057373046875
## epoch[44] training(only) time: 53.82643127441406
# Switched to evaluate mode...
Test: [  0/100]	Time  0.194 ( 0.194)	Loss 1.9356e-01 (1.9356e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.043 ( 0.057)	Loss 4.9056e-01 (2.9510e-01)	Acc@1  89.00 ( 92.91)	Acc@5  99.00 ( 99.82)
Test: [ 20/100]	Time  0.043 ( 0.050)	Loss 3.7614e-01 (3.4564e-01)	Acc@1  87.00 ( 91.81)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.044 ( 0.048)	Loss 1.9833e-01 (3.6254e-01)	Acc@1  94.00 ( 92.06)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.041 ( 0.047)	Loss 3.3807e-01 (3.7746e-01)	Acc@1  90.00 ( 91.66)	Acc@5  99.00 ( 99.63)
Test: [ 50/100]	Time  0.045 ( 0.046)	Loss 2.3215e-01 (3.6857e-01)	Acc@1  94.00 ( 91.76)	Acc@5  99.00 ( 99.67)
Test: [ 60/100]	Time  0.045 ( 0.046)	Loss 3.3536e-01 (3.6736e-01)	Acc@1  92.00 ( 91.67)	Acc@5 100.00 ( 99.69)
Test: [ 70/100]	Time  0.042 ( 0.045)	Loss 5.3674e-01 (3.6117e-01)	Acc@1  89.00 ( 91.70)	Acc@5 100.00 ( 99.72)
Test: [ 80/100]	Time  0.046 ( 0.045)	Loss 2.4951e-01 (3.6228e-01)	Acc@1  94.00 ( 91.78)	Acc@5 100.00 ( 99.69)
Test: [ 90/100]	Time  0.044 ( 0.045)	Loss 2.0672e-01 (3.5404e-01)	Acc@1  95.00 ( 91.84)	Acc@5 100.00 ( 99.73)
 * Acc@1 91.910 Acc@5 99.730
### epoch[44] execution time: 58.46830153465271
EPOCH 45
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [45][  0/391]	Time  0.298 ( 0.298)	Data  0.176 ( 0.176)	Loss 3.3309e-02 (3.3309e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [45][ 10/391]	Time  0.133 ( 0.152)	Data  0.001 ( 0.021)	Loss 8.5051e-02 (3.7587e-02)	Acc@1  96.88 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [45][ 20/391]	Time  0.138 ( 0.145)	Data  0.001 ( 0.013)	Loss 1.6447e-02 (3.5819e-02)	Acc@1 100.00 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [45][ 30/391]	Time  0.135 ( 0.142)	Data  0.001 ( 0.010)	Loss 7.0174e-02 (3.1363e-02)	Acc@1  97.66 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [45][ 40/391]	Time  0.131 ( 0.141)	Data  0.001 ( 0.009)	Loss 2.4044e-02 (3.2935e-02)	Acc@1  99.22 ( 98.91)	Acc@5 100.00 (100.00)
Epoch: [45][ 50/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.008)	Loss 1.6441e-01 (3.5079e-02)	Acc@1  97.66 ( 98.90)	Acc@5  98.44 ( 99.97)
Epoch: [45][ 60/391]	Time  0.136 ( 0.140)	Data  0.002 ( 0.007)	Loss 2.5840e-02 (3.5869e-02)	Acc@1  98.44 ( 98.81)	Acc@5 100.00 ( 99.97)
Epoch: [45][ 70/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.007)	Loss 1.3040e-02 (3.5167e-02)	Acc@1  99.22 ( 98.87)	Acc@5 100.00 ( 99.98)
Epoch: [45][ 80/391]	Time  0.134 ( 0.139)	Data  0.002 ( 0.007)	Loss 1.8256e-02 (3.5344e-02)	Acc@1  99.22 ( 98.84)	Acc@5 100.00 ( 99.98)
Epoch: [45][ 90/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.007)	Loss 5.2013e-02 (3.5247e-02)	Acc@1  98.44 ( 98.83)	Acc@5 100.00 ( 99.98)
Epoch: [45][100/391]	Time  0.137 ( 0.139)	Data  0.002 ( 0.006)	Loss 2.8337e-02 (3.4818e-02)	Acc@1  99.22 ( 98.82)	Acc@5 100.00 ( 99.98)
Epoch: [45][110/391]	Time  0.132 ( 0.139)	Data  0.001 ( 0.006)	Loss 8.7425e-03 (3.3775e-02)	Acc@1 100.00 ( 98.86)	Acc@5 100.00 ( 99.99)
Epoch: [45][120/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.5661e-02 (3.3824e-02)	Acc@1  98.44 ( 98.88)	Acc@5 100.00 ( 99.99)
Epoch: [45][130/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.3788e-02 (3.3578e-02)	Acc@1 100.00 ( 98.89)	Acc@5 100.00 ( 99.99)
Epoch: [45][140/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.7576e-02 (3.4679e-02)	Acc@1  98.44 ( 98.85)	Acc@5 100.00 ( 99.99)
Epoch: [45][150/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.7162e-02 (3.4998e-02)	Acc@1  99.22 ( 98.83)	Acc@5 100.00 ( 99.99)
Epoch: [45][160/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 6.5275e-02 (3.6704e-02)	Acc@1  97.66 ( 98.76)	Acc@5 100.00 ( 99.99)
Epoch: [45][170/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 5.8438e-02 (3.6736e-02)	Acc@1  98.44 ( 98.76)	Acc@5 100.00 ( 99.99)
Epoch: [45][180/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 5.1955e-03 (3.6201e-02)	Acc@1 100.00 ( 98.78)	Acc@5 100.00 ( 99.99)
Epoch: [45][190/391]	Time  0.131 ( 0.138)	Data  0.001 ( 0.005)	Loss 4.9454e-02 (3.6833e-02)	Acc@1  97.66 ( 98.75)	Acc@5 100.00 ( 99.99)
Epoch: [45][200/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.0992e-02 (3.6524e-02)	Acc@1  99.22 ( 98.76)	Acc@5 100.00 ( 99.99)
Epoch: [45][210/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.8840e-02 (3.6079e-02)	Acc@1 100.00 ( 98.77)	Acc@5 100.00 ( 99.99)
Epoch: [45][220/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 5.3421e-02 (3.6243e-02)	Acc@1  97.66 ( 98.76)	Acc@5 100.00 ( 99.99)
Epoch: [45][230/391]	Time  0.131 ( 0.138)	Data  0.001 ( 0.005)	Loss 6.7052e-02 (3.6494e-02)	Acc@1  97.66 ( 98.76)	Acc@5 100.00 ( 99.99)
Epoch: [45][240/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.005)	Loss 5.4824e-02 (3.6407e-02)	Acc@1  98.44 ( 98.76)	Acc@5 100.00 ( 99.99)
Epoch: [45][250/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.005)	Loss 4.0195e-02 (3.6404e-02)	Acc@1  97.66 ( 98.74)	Acc@5 100.00 ( 99.99)
Epoch: [45][260/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.005)	Loss 6.6023e-02 (3.6468e-02)	Acc@1  97.66 ( 98.73)	Acc@5 100.00 ( 99.99)
Epoch: [45][270/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.005)	Loss 4.3966e-02 (3.6955e-02)	Acc@1  98.44 ( 98.72)	Acc@5 100.00 ( 99.99)
Epoch: [45][280/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.5440e-02 (3.6738e-02)	Acc@1  98.44 ( 98.73)	Acc@5 100.00 ( 99.99)
Epoch: [45][290/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.005)	Loss 9.4668e-03 (3.6806e-02)	Acc@1 100.00 ( 98.72)	Acc@5 100.00 ( 99.99)
Epoch: [45][300/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.9430e-02 (3.7097e-02)	Acc@1  98.44 ( 98.70)	Acc@5 100.00 ( 99.99)
Epoch: [45][310/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.005)	Loss 6.0208e-02 (3.6915e-02)	Acc@1  96.88 ( 98.70)	Acc@5 100.00 ( 99.99)
Epoch: [45][320/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 6.2170e-02 (3.7504e-02)	Acc@1  96.88 ( 98.67)	Acc@5 100.00 (100.00)
Epoch: [45][330/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.8605e-02 (3.7404e-02)	Acc@1 100.00 ( 98.67)	Acc@5 100.00 (100.00)
Epoch: [45][340/391]	Time  0.136 ( 0.138)	Data  0.002 ( 0.005)	Loss 3.1731e-02 (3.7521e-02)	Acc@1  99.22 ( 98.68)	Acc@5 100.00 (100.00)
Epoch: [45][350/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.005)	Loss 4.6063e-02 (3.7615e-02)	Acc@1  98.44 ( 98.68)	Acc@5 100.00 (100.00)
Epoch: [45][360/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.8548e-02 (3.7542e-02)	Acc@1  99.22 ( 98.68)	Acc@5 100.00 (100.00)
Epoch: [45][370/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.2614e-02 (3.7243e-02)	Acc@1  99.22 ( 98.70)	Acc@5 100.00 (100.00)
Epoch: [45][380/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.0090e-02 (3.7179e-02)	Acc@1  98.44 ( 98.70)	Acc@5 100.00 (100.00)
Epoch: [45][390/391]	Time  0.131 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.0144e-01 (3.7111e-02)	Acc@1  96.25 ( 98.71)	Acc@5 100.00 (100.00)
## e[45] optimizer.zero_grad (sum) time: 0.2846822738647461
## e[45]       loss.backward (sum) time: 18.598379373550415
## e[45]      optimizer.step (sum) time: 2.8966000080108643
## epoch[45] training(only) time: 53.9239296913147
# Switched to evaluate mode...
Test: [  0/100]	Time  0.202 ( 0.202)	Loss 2.2864e-01 (2.2864e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.043 ( 0.059)	Loss 4.5343e-01 (3.2352e-01)	Acc@1  90.00 ( 92.18)	Acc@5  99.00 ( 99.55)
Test: [ 20/100]	Time  0.044 ( 0.052)	Loss 3.4946e-01 (3.5855e-01)	Acc@1  90.00 ( 91.62)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.042 ( 0.049)	Loss 2.1590e-01 (3.6611e-01)	Acc@1  92.00 ( 91.61)	Acc@5 100.00 ( 99.61)
Test: [ 40/100]	Time  0.046 ( 0.048)	Loss 4.1616e-01 (3.7382e-01)	Acc@1  87.00 ( 91.34)	Acc@5  99.00 ( 99.61)
Test: [ 50/100]	Time  0.045 ( 0.047)	Loss 2.0718e-01 (3.6468e-01)	Acc@1  95.00 ( 91.53)	Acc@5 100.00 ( 99.67)
Test: [ 60/100]	Time  0.044 ( 0.046)	Loss 3.2573e-01 (3.6020e-01)	Acc@1  96.00 ( 91.64)	Acc@5  99.00 ( 99.69)
Test: [ 70/100]	Time  0.042 ( 0.046)	Loss 5.1399e-01 (3.5340e-01)	Acc@1  89.00 ( 91.63)	Acc@5 100.00 ( 99.70)
Test: [ 80/100]	Time  0.044 ( 0.046)	Loss 2.5036e-01 (3.5669e-01)	Acc@1  93.00 ( 91.65)	Acc@5 100.00 ( 99.69)
Test: [ 90/100]	Time  0.043 ( 0.045)	Loss 1.9941e-01 (3.5055e-01)	Acc@1  93.00 ( 91.75)	Acc@5 100.00 ( 99.71)
 * Acc@1 91.850 Acc@5 99.710
### epoch[45] execution time: 58.56062579154968
EPOCH 46
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [46][  0/391]	Time  0.296 ( 0.296)	Data  0.169 ( 0.169)	Loss 2.5058e-02 (2.5058e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [46][ 10/391]	Time  0.135 ( 0.149)	Data  0.001 ( 0.019)	Loss 1.4118e-02 (2.9606e-02)	Acc@1 100.00 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [46][ 20/391]	Time  0.138 ( 0.143)	Data  0.001 ( 0.012)	Loss 2.3766e-03 (3.1341e-02)	Acc@1 100.00 ( 98.92)	Acc@5 100.00 ( 99.96)
Epoch: [46][ 30/391]	Time  0.131 ( 0.140)	Data  0.001 ( 0.010)	Loss 5.8338e-02 (3.1338e-02)	Acc@1  98.44 ( 98.92)	Acc@5 100.00 ( 99.97)
Epoch: [46][ 40/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.008)	Loss 1.2777e-02 (3.1432e-02)	Acc@1 100.00 ( 98.95)	Acc@5 100.00 ( 99.98)
Epoch: [46][ 50/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.008)	Loss 4.7317e-02 (3.3155e-02)	Acc@1  97.66 ( 98.97)	Acc@5 100.00 ( 99.97)
Epoch: [46][ 60/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.007)	Loss 2.3478e-02 (3.2793e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 ( 99.97)
Epoch: [46][ 70/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.007)	Loss 5.7358e-02 (3.1906e-02)	Acc@1  98.44 ( 99.04)	Acc@5 100.00 ( 99.98)
Epoch: [46][ 80/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.9454e-02 (3.0978e-02)	Acc@1  99.22 ( 99.08)	Acc@5 100.00 ( 99.98)
Epoch: [46][ 90/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.4093e-02 (3.0520e-02)	Acc@1  99.22 ( 99.12)	Acc@5 100.00 ( 99.97)
Epoch: [46][100/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.5582e-02 (3.1074e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 ( 99.98)
Epoch: [46][110/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.006)	Loss 1.7640e-02 (3.1049e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 ( 99.98)
Epoch: [46][120/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.006)	Loss 2.7110e-02 (3.1908e-02)	Acc@1  98.44 ( 99.02)	Acc@5 100.00 ( 99.98)
Epoch: [46][130/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.006)	Loss 7.7970e-03 (3.1307e-02)	Acc@1 100.00 ( 99.03)	Acc@5 100.00 ( 99.98)
Epoch: [46][140/391]	Time  0.130 ( 0.137)	Data  0.001 ( 0.006)	Loss 6.4373e-02 (3.1814e-02)	Acc@1  96.88 ( 98.98)	Acc@5 100.00 ( 99.98)
Epoch: [46][150/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.006)	Loss 2.8349e-02 (3.1920e-02)	Acc@1  99.22 ( 98.97)	Acc@5 100.00 ( 99.98)
Epoch: [46][160/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.3796e-02 (3.1278e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [46][170/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.8227e-02 (3.1303e-02)	Acc@1  97.66 ( 98.98)	Acc@5 100.00 ( 99.99)
Epoch: [46][180/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.2083e-02 (3.0658e-02)	Acc@1 100.00 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [46][190/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.4513e-02 (3.0835e-02)	Acc@1 100.00 ( 98.99)	Acc@5 100.00 ( 99.99)
Epoch: [46][200/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.0402e-02 (3.1142e-02)	Acc@1  99.22 ( 98.97)	Acc@5 100.00 ( 99.99)
Epoch: [46][210/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.9037e-02 (3.1510e-02)	Acc@1 100.00 ( 98.94)	Acc@5 100.00 ( 99.99)
Epoch: [46][220/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.2777e-02 (3.1591e-02)	Acc@1  99.22 ( 98.94)	Acc@5 100.00 ( 99.99)
Epoch: [46][230/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.4744e-02 (3.1926e-02)	Acc@1  98.44 ( 98.93)	Acc@5 100.00 ( 99.99)
Epoch: [46][240/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.6180e-02 (3.1926e-02)	Acc@1  99.22 ( 98.92)	Acc@5 100.00 ( 99.99)
Epoch: [46][250/391]	Time  0.134 ( 0.137)	Data  0.002 ( 0.005)	Loss 1.8164e-02 (3.2062e-02)	Acc@1 100.00 ( 98.93)	Acc@5 100.00 ( 99.99)
Epoch: [46][260/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.1235e-02 (3.1884e-02)	Acc@1  99.22 ( 98.93)	Acc@5 100.00 ( 99.99)
Epoch: [46][270/391]	Time  0.129 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.0536e-02 (3.1863e-02)	Acc@1  99.22 ( 98.92)	Acc@5 100.00 ( 99.99)
Epoch: [46][280/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.3999e-02 (3.1806e-02)	Acc@1  99.22 ( 98.93)	Acc@5 100.00 ( 99.99)
Epoch: [46][290/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.8516e-02 (3.2019e-02)	Acc@1  99.22 ( 98.92)	Acc@5 100.00 ( 99.99)
Epoch: [46][300/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.1813e-02 (3.2301e-02)	Acc@1  96.88 ( 98.90)	Acc@5 100.00 ( 99.99)
Epoch: [46][310/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.0266e-02 (3.2600e-02)	Acc@1  98.44 ( 98.90)	Acc@5 100.00 ( 99.99)
Epoch: [46][320/391]	Time  0.143 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.9617e-02 (3.2663e-02)	Acc@1  99.22 ( 98.90)	Acc@5 100.00 ( 99.99)
Epoch: [46][330/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.4386e-02 (3.2536e-02)	Acc@1  98.44 ( 98.91)	Acc@5 100.00 ( 99.99)
Epoch: [46][340/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.2695e-02 (3.2301e-02)	Acc@1  98.44 ( 98.92)	Acc@5 100.00 ( 99.99)
Epoch: [46][350/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.1091e-02 (3.2618e-02)	Acc@1  99.22 ( 98.90)	Acc@5 100.00 ( 99.99)
Epoch: [46][360/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.1416e-02 (3.2586e-02)	Acc@1  96.88 ( 98.90)	Acc@5 100.00 ( 99.99)
Epoch: [46][370/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.2134e-02 (3.2668e-02)	Acc@1  98.44 ( 98.90)	Acc@5 100.00 ( 99.99)
Epoch: [46][380/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.5587e-02 (3.2655e-02)	Acc@1  98.44 ( 98.91)	Acc@5 100.00 ( 99.99)
Epoch: [46][390/391]	Time  0.130 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.8630e-02 (3.3071e-02)	Acc@1  97.50 ( 98.89)	Acc@5 100.00 ( 99.99)
## e[46] optimizer.zero_grad (sum) time: 0.28618502616882324
## e[46]       loss.backward (sum) time: 18.539419412612915
## e[46]      optimizer.step (sum) time: 2.87790584564209
## epoch[46] training(only) time: 53.63804483413696
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 2.4944e-01 (2.4944e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.044 ( 0.057)	Loss 5.5358e-01 (3.1286e-01)	Acc@1  89.00 ( 92.55)	Acc@5  99.00 ( 99.55)
Test: [ 20/100]	Time  0.044 ( 0.051)	Loss 4.0012e-01 (3.5373e-01)	Acc@1  86.00 ( 91.62)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.044 ( 0.049)	Loss 2.1166e-01 (3.6397e-01)	Acc@1  93.00 ( 91.81)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.045 ( 0.048)	Loss 4.2175e-01 (3.7483e-01)	Acc@1  89.00 ( 91.56)	Acc@5  99.00 ( 99.73)
Test: [ 50/100]	Time  0.040 ( 0.047)	Loss 2.1526e-01 (3.7229e-01)	Acc@1  91.00 ( 91.67)	Acc@5  99.00 ( 99.75)
Test: [ 60/100]	Time  0.045 ( 0.046)	Loss 4.4682e-01 (3.6716e-01)	Acc@1  93.00 ( 91.85)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.041 ( 0.046)	Loss 5.4309e-01 (3.6353e-01)	Acc@1  87.00 ( 91.76)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.041 ( 0.045)	Loss 2.0403e-01 (3.6716e-01)	Acc@1  96.00 ( 91.78)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.043 ( 0.045)	Loss 2.7922e-01 (3.6145e-01)	Acc@1  92.00 ( 91.77)	Acc@5 100.00 ( 99.79)
 * Acc@1 91.850 Acc@5 99.780
### epoch[46] execution time: 58.29188084602356
EPOCH 47
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [47][  0/391]	Time  0.304 ( 0.304)	Data  0.184 ( 0.184)	Loss 4.8417e-02 (4.8417e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [47][ 10/391]	Time  0.136 ( 0.151)	Data  0.001 ( 0.021)	Loss 1.5281e-02 (2.8092e-02)	Acc@1 100.00 ( 98.65)	Acc@5 100.00 (100.00)
Epoch: [47][ 20/391]	Time  0.133 ( 0.144)	Data  0.001 ( 0.013)	Loss 2.7602e-02 (2.6009e-02)	Acc@1  99.22 ( 98.81)	Acc@5 100.00 (100.00)
Epoch: [47][ 30/391]	Time  0.136 ( 0.141)	Data  0.001 ( 0.010)	Loss 2.9223e-02 (2.9078e-02)	Acc@1  99.22 ( 98.79)	Acc@5 100.00 (100.00)
Epoch: [47][ 40/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.009)	Loss 7.1865e-03 (2.8929e-02)	Acc@1 100.00 ( 98.82)	Acc@5 100.00 (100.00)
Epoch: [47][ 50/391]	Time  0.130 ( 0.139)	Data  0.001 ( 0.008)	Loss 4.0705e-02 (2.8639e-02)	Acc@1  98.44 ( 98.90)	Acc@5 100.00 (100.00)
Epoch: [47][ 60/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.007)	Loss 2.2011e-02 (2.9353e-02)	Acc@1  99.22 ( 98.91)	Acc@5 100.00 ( 99.99)
Epoch: [47][ 70/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.007)	Loss 8.9009e-02 (3.0342e-02)	Acc@1  96.88 ( 98.90)	Acc@5 100.00 ( 99.99)
Epoch: [47][ 80/391]	Time  0.137 ( 0.139)	Data  0.002 ( 0.007)	Loss 6.8658e-02 (3.1001e-02)	Acc@1  97.66 ( 98.89)	Acc@5 100.00 ( 99.99)
Epoch: [47][ 90/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.8751e-02 (3.1636e-02)	Acc@1  99.22 ( 98.88)	Acc@5 100.00 ( 99.99)
Epoch: [47][100/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.9744e-02 (3.1753e-02)	Acc@1  99.22 ( 98.89)	Acc@5 100.00 ( 99.99)
Epoch: [47][110/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.2723e-02 (3.1129e-02)	Acc@1 100.00 ( 98.94)	Acc@5 100.00 ( 99.99)
Epoch: [47][120/391]	Time  0.131 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.6183e-02 (3.1321e-02)	Acc@1 100.00 ( 98.93)	Acc@5 100.00 ( 99.99)
Epoch: [47][130/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.6925e-02 (3.1370e-02)	Acc@1  96.09 ( 98.93)	Acc@5 100.00 ( 99.99)
Epoch: [47][140/391]	Time  0.129 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.2313e-02 (3.1178e-02)	Acc@1  99.22 ( 98.94)	Acc@5 100.00 ( 99.99)
Epoch: [47][150/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.006)	Loss 5.9532e-02 (3.1761e-02)	Acc@1  98.44 ( 98.92)	Acc@5 100.00 ( 99.99)
Epoch: [47][160/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.8240e-02 (3.1922e-02)	Acc@1  98.44 ( 98.91)	Acc@5 100.00 (100.00)
Epoch: [47][170/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 6.5555e-02 (3.2403e-02)	Acc@1  97.66 ( 98.89)	Acc@5 100.00 (100.00)
Epoch: [47][180/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.005)	Loss 9.6997e-03 (3.2577e-02)	Acc@1 100.00 ( 98.89)	Acc@5 100.00 (100.00)
Epoch: [47][190/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.005)	Loss 4.2245e-02 (3.2907e-02)	Acc@1  99.22 ( 98.87)	Acc@5 100.00 (100.00)
Epoch: [47][200/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.2777e-02 (3.3303e-02)	Acc@1 100.00 ( 98.84)	Acc@5 100.00 (100.00)
Epoch: [47][210/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.0192e-02 (3.3277e-02)	Acc@1 100.00 ( 98.84)	Acc@5 100.00 (100.00)
Epoch: [47][220/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.9445e-02 (3.3311e-02)	Acc@1 100.00 ( 98.84)	Acc@5 100.00 (100.00)
Epoch: [47][230/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.0326e-02 (3.3234e-02)	Acc@1  99.22 ( 98.85)	Acc@5 100.00 (100.00)
Epoch: [47][240/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.005)	Loss 6.9222e-03 (3.2962e-02)	Acc@1 100.00 ( 98.87)	Acc@5 100.00 (100.00)
Epoch: [47][250/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.005)	Loss 7.1827e-02 (3.3299e-02)	Acc@1  98.44 ( 98.87)	Acc@5 100.00 (100.00)
Epoch: [47][260/391]	Time  0.130 ( 0.138)	Data  0.001 ( 0.005)	Loss 6.5240e-02 (3.3216e-02)	Acc@1  97.66 ( 98.87)	Acc@5 100.00 (100.00)
Epoch: [47][270/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.2603e-02 (3.3376e-02)	Acc@1  99.22 ( 98.87)	Acc@5 100.00 (100.00)
Epoch: [47][280/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.005)	Loss 8.6438e-03 (3.3517e-02)	Acc@1 100.00 ( 98.87)	Acc@5 100.00 (100.00)
Epoch: [47][290/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.7831e-02 (3.3530e-02)	Acc@1  99.22 ( 98.88)	Acc@5 100.00 (100.00)
Epoch: [47][300/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.3052e-02 (3.3448e-02)	Acc@1  99.22 ( 98.87)	Acc@5 100.00 (100.00)
Epoch: [47][310/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.3150e-02 (3.3385e-02)	Acc@1  96.09 ( 98.87)	Acc@5 100.00 (100.00)
Epoch: [47][320/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.3262e-02 (3.3308e-02)	Acc@1  99.22 ( 98.87)	Acc@5 100.00 (100.00)
Epoch: [47][330/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.4953e-02 (3.3441e-02)	Acc@1  99.22 ( 98.87)	Acc@5 100.00 (100.00)
Epoch: [47][340/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.8003e-02 (3.3109e-02)	Acc@1 100.00 ( 98.89)	Acc@5 100.00 (100.00)
Epoch: [47][350/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.4430e-02 (3.3344e-02)	Acc@1  99.22 ( 98.88)	Acc@5 100.00 (100.00)
Epoch: [47][360/391]	Time  0.136 ( 0.137)	Data  0.002 ( 0.005)	Loss 4.4694e-02 (3.3372e-02)	Acc@1  99.22 ( 98.87)	Acc@5 100.00 (100.00)
Epoch: [47][370/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.8937e-02 (3.3374e-02)	Acc@1  96.88 ( 98.87)	Acc@5 100.00 (100.00)
Epoch: [47][380/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.2591e-02 (3.3185e-02)	Acc@1 100.00 ( 98.88)	Acc@5 100.00 (100.00)
Epoch: [47][390/391]	Time  0.127 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.1253e-03 (3.3121e-02)	Acc@1 100.00 ( 98.88)	Acc@5 100.00 (100.00)
## e[47] optimizer.zero_grad (sum) time: 0.2855038642883301
## e[47]       loss.backward (sum) time: 18.580812692642212
## e[47]      optimizer.step (sum) time: 2.9039881229400635
## epoch[47] training(only) time: 53.817283391952515
# Switched to evaluate mode...
Test: [  0/100]	Time  0.212 ( 0.212)	Loss 2.4735e-01 (2.4735e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.059)	Loss 4.9983e-01 (3.2157e-01)	Acc@1  88.00 ( 91.91)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.045 ( 0.052)	Loss 3.9129e-01 (3.6998e-01)	Acc@1  89.00 ( 91.48)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.043 ( 0.049)	Loss 2.4863e-01 (3.8440e-01)	Acc@1  92.00 ( 91.58)	Acc@5 100.00 ( 99.58)
Test: [ 40/100]	Time  0.042 ( 0.048)	Loss 4.4209e-01 (3.9539e-01)	Acc@1  89.00 ( 91.15)	Acc@5  99.00 ( 99.56)
Test: [ 50/100]	Time  0.041 ( 0.047)	Loss 1.6517e-01 (3.8805e-01)	Acc@1  94.00 ( 91.24)	Acc@5 100.00 ( 99.63)
Test: [ 60/100]	Time  0.042 ( 0.047)	Loss 3.6368e-01 (3.7742e-01)	Acc@1  94.00 ( 91.46)	Acc@5 100.00 ( 99.67)
Test: [ 70/100]	Time  0.045 ( 0.046)	Loss 5.1486e-01 (3.7070e-01)	Acc@1  89.00 ( 91.51)	Acc@5 100.00 ( 99.70)
Test: [ 80/100]	Time  0.044 ( 0.046)	Loss 2.9402e-01 (3.7710e-01)	Acc@1  90.00 ( 91.46)	Acc@5 100.00 ( 99.69)
Test: [ 90/100]	Time  0.043 ( 0.046)	Loss 2.0135e-01 (3.7056e-01)	Acc@1  95.00 ( 91.55)	Acc@5 100.00 ( 99.73)
 * Acc@1 91.600 Acc@5 99.720
### epoch[47] execution time: 58.476083278656006
EPOCH 48
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [48][  0/391]	Time  0.296 ( 0.296)	Data  0.178 ( 0.178)	Loss 3.0501e-02 (3.0501e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [48][ 10/391]	Time  0.135 ( 0.150)	Data  0.001 ( 0.020)	Loss 5.1571e-03 (2.3486e-02)	Acc@1 100.00 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [48][ 20/391]	Time  0.133 ( 0.143)	Data  0.001 ( 0.013)	Loss 2.2495e-02 (2.5292e-02)	Acc@1  98.44 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [48][ 30/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.010)	Loss 1.4976e-02 (2.5833e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [48][ 40/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.009)	Loss 2.9332e-02 (2.6600e-02)	Acc@1  99.22 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [48][ 50/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.008)	Loss 3.0830e-02 (2.5944e-02)	Acc@1  99.22 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [48][ 60/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.007)	Loss 1.2747e-02 (2.4199e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [48][ 70/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.007)	Loss 3.4923e-02 (2.4232e-02)	Acc@1  98.44 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [48][ 80/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.007)	Loss 2.1517e-02 (2.3988e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [48][ 90/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.4165e-02 (2.5208e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [48][100/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.6790e-02 (2.5745e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [48][110/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.3070e-02 (2.5419e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [48][120/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.006)	Loss 9.6560e-03 (2.5430e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [48][130/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.006)	Loss 3.9327e-03 (2.5267e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [48][140/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.006)	Loss 2.4418e-02 (2.5417e-02)	Acc@1  98.44 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [48][150/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.006)	Loss 1.3341e-02 (2.5483e-02)	Acc@1 100.00 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [48][160/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.006)	Loss 2.7741e-02 (2.6095e-02)	Acc@1  99.22 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [48][170/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.2294e-02 (2.6292e-02)	Acc@1  97.66 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [48][180/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.3376e-02 (2.6493e-02)	Acc@1  98.44 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [48][190/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.6577e-02 (2.6463e-02)	Acc@1  99.22 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [48][200/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1887e-02 (2.6560e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [48][210/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.4250e-02 (2.6462e-02)	Acc@1  97.66 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [48][220/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.3030e-02 (2.6628e-02)	Acc@1  97.66 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [48][230/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.6254e-02 (2.6517e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [48][240/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.7696e-02 (2.6763e-02)	Acc@1  98.44 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [48][250/391]	Time  0.127 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.9125e-02 (2.7341e-02)	Acc@1  98.44 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [48][260/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.2821e-02 (2.7573e-02)	Acc@1  99.22 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [48][270/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.9698e-02 (2.7800e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [48][280/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.0588e-02 (2.7693e-02)	Acc@1  99.22 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [48][290/391]	Time  0.139 ( 0.137)	Data  0.002 ( 0.005)	Loss 3.2219e-02 (2.7981e-02)	Acc@1  99.22 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [48][300/391]	Time  0.134 ( 0.137)	Data  0.002 ( 0.005)	Loss 9.7030e-03 (2.8097e-02)	Acc@1 100.00 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [48][310/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.8723e-02 (2.8102e-02)	Acc@1  98.44 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [48][320/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.5899e-03 (2.8138e-02)	Acc@1 100.00 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [48][330/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.8147e-02 (2.8551e-02)	Acc@1  97.66 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [48][340/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.2004e-02 (2.8776e-02)	Acc@1  97.66 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [48][350/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.8214e-03 (2.8675e-02)	Acc@1 100.00 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [48][360/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.0367e-02 (2.9157e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [48][370/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.4328e-02 (2.9139e-02)	Acc@1  99.22 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [48][380/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.0334e-02 (2.9114e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [48][390/391]	Time  0.129 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.4176e-03 (2.9106e-02)	Acc@1 100.00 ( 98.99)	Acc@5 100.00 (100.00)
## e[48] optimizer.zero_grad (sum) time: 0.2860846519470215
## e[48]       loss.backward (sum) time: 18.540480613708496
## e[48]      optimizer.step (sum) time: 2.886888265609741
## epoch[48] training(only) time: 53.686542987823486
# Switched to evaluate mode...
Test: [  0/100]	Time  0.205 ( 0.205)	Loss 1.6789e-01 (1.6789e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.045 ( 0.059)	Loss 4.8214e-01 (2.9934e-01)	Acc@1  90.00 ( 92.00)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.044 ( 0.052)	Loss 4.4626e-01 (3.6032e-01)	Acc@1  89.00 ( 91.48)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.041 ( 0.049)	Loss 2.3254e-01 (3.8224e-01)	Acc@1  94.00 ( 91.77)	Acc@5  99.00 ( 99.48)
Test: [ 40/100]	Time  0.043 ( 0.048)	Loss 3.6539e-01 (3.9101e-01)	Acc@1  90.00 ( 91.51)	Acc@5  99.00 ( 99.49)
Test: [ 50/100]	Time  0.043 ( 0.047)	Loss 2.0429e-01 (3.8235e-01)	Acc@1  95.00 ( 91.67)	Acc@5 100.00 ( 99.57)
Test: [ 60/100]	Time  0.044 ( 0.047)	Loss 3.7604e-01 (3.7990e-01)	Acc@1  92.00 ( 91.75)	Acc@5 100.00 ( 99.62)
Test: [ 70/100]	Time  0.043 ( 0.046)	Loss 5.5496e-01 (3.7526e-01)	Acc@1  89.00 ( 91.80)	Acc@5 100.00 ( 99.62)
Test: [ 80/100]	Time  0.043 ( 0.046)	Loss 2.4298e-01 (3.7720e-01)	Acc@1  93.00 ( 91.79)	Acc@5 100.00 ( 99.62)
Test: [ 90/100]	Time  0.043 ( 0.045)	Loss 2.3222e-01 (3.7021e-01)	Acc@1  93.00 ( 91.84)	Acc@5 100.00 ( 99.66)
 * Acc@1 91.950 Acc@5 99.650
### epoch[48] execution time: 58.32523036003113
EPOCH 49
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [49][  0/391]	Time  0.291 ( 0.291)	Data  0.173 ( 0.173)	Loss 8.9421e-03 (8.9421e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [49][ 10/391]	Time  0.132 ( 0.150)	Data  0.001 ( 0.020)	Loss 8.9932e-03 (2.4884e-02)	Acc@1 100.00 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [49][ 20/391]	Time  0.129 ( 0.143)	Data  0.001 ( 0.013)	Loss 3.2090e-02 (2.9815e-02)	Acc@1  99.22 ( 98.96)	Acc@5 100.00 ( 99.96)
Epoch: [49][ 30/391]	Time  0.133 ( 0.140)	Data  0.001 ( 0.010)	Loss 1.5710e-02 (2.7395e-02)	Acc@1 100.00 ( 99.09)	Acc@5 100.00 ( 99.97)
Epoch: [49][ 40/391]	Time  0.132 ( 0.139)	Data  0.002 ( 0.009)	Loss 2.0707e-02 (2.6735e-02)	Acc@1  99.22 ( 99.10)	Acc@5 100.00 ( 99.98)
Epoch: [49][ 50/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.008)	Loss 4.4829e-03 (2.5633e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 ( 99.98)
Epoch: [49][ 60/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.007)	Loss 6.1108e-02 (2.5695e-02)	Acc@1  97.66 ( 99.17)	Acc@5 100.00 ( 99.99)
Epoch: [49][ 70/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.007)	Loss 2.0234e-02 (2.6535e-02)	Acc@1  98.44 ( 99.10)	Acc@5 100.00 ( 99.99)
Epoch: [49][ 80/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.007)	Loss 3.2908e-02 (2.6450e-02)	Acc@1  99.22 ( 99.10)	Acc@5 100.00 ( 99.99)
Epoch: [49][ 90/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.2595e-02 (2.7801e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [49][100/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.2474e-02 (2.8224e-02)	Acc@1  97.66 ( 99.05)	Acc@5 100.00 ( 99.99)
Epoch: [49][110/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.2056e-02 (2.8345e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [49][120/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.8645e-02 (2.7988e-02)	Acc@1  96.88 ( 99.04)	Acc@5 100.00 ( 99.99)
Epoch: [49][130/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 5.1790e-02 (2.7700e-02)	Acc@1  96.88 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [49][140/391]	Time  0.127 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.0272e-03 (2.8128e-02)	Acc@1 100.00 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [49][150/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.006)	Loss 2.1918e-02 (2.7515e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 ( 99.99)
Epoch: [49][160/391]	Time  0.130 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.5449e-02 (2.7523e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [49][170/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.8361e-02 (2.7277e-02)	Acc@1 100.00 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [49][180/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.1699e-02 (2.7504e-02)	Acc@1  97.66 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [49][190/391]	Time  0.130 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3063e-02 (2.7397e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [49][200/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.2394e-02 (2.7212e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [49][210/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.4464e-02 (2.7131e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [49][220/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1417e-02 (2.7170e-02)	Acc@1 100.00 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [49][230/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.9036e-02 (2.6834e-02)	Acc@1  98.44 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [49][240/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.2109e-02 (2.6751e-02)	Acc@1 100.00 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [49][250/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.4573e-02 (2.6981e-02)	Acc@1  98.44 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [49][260/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.9872e-02 (2.7175e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [49][270/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.7488e-02 (2.7061e-02)	Acc@1  98.44 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [49][280/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.1035e-02 (2.6738e-02)	Acc@1 100.00 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [49][290/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.3917e-02 (2.7027e-02)	Acc@1  98.44 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [49][300/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.6653e-02 (2.7354e-02)	Acc@1  97.66 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [49][310/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.6270e-02 (2.7585e-02)	Acc@1  96.88 ( 99.04)	Acc@5 100.00 ( 99.99)
Epoch: [49][320/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.0036e-02 (2.7549e-02)	Acc@1  97.66 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [49][330/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.0411e-03 (2.7626e-02)	Acc@1 100.00 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [49][340/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.0166e-02 (2.7625e-02)	Acc@1  99.22 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [49][350/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.7740e-02 (2.7934e-02)	Acc@1  98.44 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [49][360/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.0070e-02 (2.8234e-02)	Acc@1 100.00 ( 99.00)	Acc@5 100.00 ( 99.99)
Epoch: [49][370/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.2127e-02 (2.8164e-02)	Acc@1  98.44 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [49][380/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.5146e-02 (2.8209e-02)	Acc@1  98.44 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [49][390/391]	Time  0.124 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.3104e-02 (2.8390e-02)	Acc@1  97.50 ( 99.00)	Acc@5 100.00 ( 99.99)
## e[49] optimizer.zero_grad (sum) time: 0.287992000579834
## e[49]       loss.backward (sum) time: 18.56807541847229
## e[49]      optimizer.step (sum) time: 2.9311463832855225
## epoch[49] training(only) time: 53.71488881111145
# Switched to evaluate mode...
Test: [  0/100]	Time  0.199 ( 0.199)	Loss 1.2037e-01 (1.2037e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.045 ( 0.059)	Loss 3.8357e-01 (2.9372e-01)	Acc@1  90.00 ( 92.82)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.046 ( 0.052)	Loss 3.5962e-01 (3.5819e-01)	Acc@1  87.00 ( 91.95)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.043 ( 0.049)	Loss 1.9325e-01 (3.7380e-01)	Acc@1  93.00 ( 91.71)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.045 ( 0.048)	Loss 4.7542e-01 (3.8829e-01)	Acc@1  89.00 ( 91.41)	Acc@5 100.00 ( 99.80)
Test: [ 50/100]	Time  0.041 ( 0.047)	Loss 1.7702e-01 (3.7686e-01)	Acc@1  92.00 ( 91.49)	Acc@5 100.00 ( 99.84)
Test: [ 60/100]	Time  0.044 ( 0.046)	Loss 4.0966e-01 (3.7300e-01)	Acc@1  93.00 ( 91.56)	Acc@5  99.00 ( 99.85)
Test: [ 70/100]	Time  0.043 ( 0.046)	Loss 4.5718e-01 (3.6571e-01)	Acc@1  89.00 ( 91.59)	Acc@5 100.00 ( 99.86)
Test: [ 80/100]	Time  0.044 ( 0.045)	Loss 1.9818e-01 (3.6609e-01)	Acc@1  94.00 ( 91.70)	Acc@5 100.00 ( 99.83)
Test: [ 90/100]	Time  0.042 ( 0.045)	Loss 1.7068e-01 (3.6105e-01)	Acc@1  94.00 ( 91.76)	Acc@5 100.00 ( 99.84)
 * Acc@1 91.840 Acc@5 99.830
### epoch[49] execution time: 58.342888832092285
EPOCH 50
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [50][  0/391]	Time  0.288 ( 0.288)	Data  0.168 ( 0.168)	Loss 1.2297e-02 (1.2297e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [50][ 10/391]	Time  0.132 ( 0.149)	Data  0.002 ( 0.019)	Loss 2.3599e-03 (2.7371e-02)	Acc@1 100.00 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [50][ 20/391]	Time  0.135 ( 0.144)	Data  0.001 ( 0.012)	Loss 1.0722e-02 (2.3911e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [50][ 30/391]	Time  0.138 ( 0.141)	Data  0.001 ( 0.010)	Loss 2.4097e-02 (2.7328e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [50][ 40/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.008)	Loss 3.8735e-02 (3.1682e-02)	Acc@1  98.44 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [50][ 50/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.008)	Loss 5.5268e-02 (3.1126e-02)	Acc@1  97.66 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [50][ 60/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.007)	Loss 1.5225e-02 (2.8993e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [50][ 70/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.007)	Loss 2.3625e-02 (2.7444e-02)	Acc@1  99.22 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [50][ 80/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.1332e-02 (2.6492e-02)	Acc@1  98.44 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [50][ 90/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.5259e-02 (2.6594e-02)	Acc@1  99.22 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [50][100/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.3042e-02 (2.6097e-02)	Acc@1  98.44 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [50][110/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.0970e-02 (2.6298e-02)	Acc@1  97.66 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [50][120/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.4045e-02 (2.7768e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [50][130/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.6228e-02 (2.7637e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [50][140/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.0306e-02 (2.7276e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [50][150/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.4448e-02 (2.6937e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [50][160/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.0519e-02 (2.7049e-02)	Acc@1  97.66 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [50][170/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.3907e-02 (2.6776e-02)	Acc@1  97.66 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [50][180/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.7839e-02 (2.6998e-02)	Acc@1  99.22 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [50][190/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.1561e-02 (2.6918e-02)	Acc@1  99.22 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [50][200/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.2124e-02 (2.6948e-02)	Acc@1  98.44 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [50][210/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.0754e-03 (2.7081e-02)	Acc@1 100.00 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [50][220/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.8467e-02 (2.6961e-02)	Acc@1  97.66 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [50][230/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.3619e-02 (2.7867e-02)	Acc@1  98.44 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [50][240/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.2661e-02 (2.7851e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [50][250/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1658e-02 (2.8054e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [50][260/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1724e-02 (2.8238e-02)	Acc@1 100.00 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [50][270/391]	Time  0.142 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.6485e-03 (2.7964e-02)	Acc@1 100.00 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [50][280/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1154e-02 (2.8076e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [50][290/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.0564e-02 (2.8386e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [50][300/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.4164e-03 (2.8385e-02)	Acc@1 100.00 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [50][310/391]	Time  0.130 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.0390e-02 (2.8395e-02)	Acc@1  97.66 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [50][320/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0421e-02 (2.8178e-02)	Acc@1 100.00 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [50][330/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.3760e-02 (2.8419e-02)	Acc@1  98.44 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [50][340/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.8005e-03 (2.8324e-02)	Acc@1 100.00 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [50][350/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.7510e-02 (2.8489e-02)	Acc@1  96.88 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [50][360/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.0910e-02 (2.8433e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [50][370/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.8630e-02 (2.8611e-02)	Acc@1 100.00 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [50][380/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0607e-02 (2.8542e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [50][390/391]	Time  0.124 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.7682e-02 (2.8441e-02)	Acc@1  96.25 ( 99.03)	Acc@5 100.00 (100.00)
## e[50] optimizer.zero_grad (sum) time: 0.28768062591552734
## e[50]       loss.backward (sum) time: 18.58945369720459
## e[50]      optimizer.step (sum) time: 2.891596555709839
## epoch[50] training(only) time: 53.738266468048096
# Switched to evaluate mode...
Test: [  0/100]	Time  0.218 ( 0.218)	Loss 1.3575e-01 (1.3575e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.045 ( 0.060)	Loss 3.7942e-01 (3.2912e-01)	Acc@1  89.00 ( 93.00)	Acc@5 100.00 ( 99.55)
Test: [ 20/100]	Time  0.043 ( 0.052)	Loss 4.4144e-01 (3.6694e-01)	Acc@1  88.00 ( 92.19)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.043 ( 0.049)	Loss 2.3871e-01 (3.8579e-01)	Acc@1  92.00 ( 92.06)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.046 ( 0.048)	Loss 3.7432e-01 (3.9648e-01)	Acc@1  88.00 ( 91.78)	Acc@5  99.00 ( 99.66)
Test: [ 50/100]	Time  0.045 ( 0.047)	Loss 1.7128e-01 (3.8943e-01)	Acc@1  94.00 ( 91.94)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.044 ( 0.047)	Loss 3.5106e-01 (3.8348e-01)	Acc@1  95.00 ( 92.00)	Acc@5  99.00 ( 99.72)
Test: [ 70/100]	Time  0.043 ( 0.046)	Loss 5.2293e-01 (3.7900e-01)	Acc@1  88.00 ( 91.96)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.043 ( 0.046)	Loss 2.3459e-01 (3.8354e-01)	Acc@1  95.00 ( 91.93)	Acc@5 100.00 ( 99.73)
Test: [ 90/100]	Time  0.042 ( 0.046)	Loss 2.0819e-01 (3.7718e-01)	Acc@1  95.00 ( 92.09)	Acc@5 100.00 ( 99.76)
 * Acc@1 92.190 Acc@5 99.760
### epoch[50] execution time: 58.40132761001587
EPOCH 51
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [51][  0/391]	Time  0.291 ( 0.291)	Data  0.177 ( 0.177)	Loss 1.6148e-02 (1.6148e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [51][ 10/391]	Time  0.137 ( 0.150)	Data  0.001 ( 0.020)	Loss 1.3756e-02 (1.9118e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [51][ 20/391]	Time  0.130 ( 0.143)	Data  0.001 ( 0.012)	Loss 4.2200e-03 (2.3522e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [51][ 30/391]	Time  0.136 ( 0.141)	Data  0.001 ( 0.010)	Loss 2.6047e-02 (2.2552e-02)	Acc@1  98.44 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [51][ 40/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.009)	Loss 5.9313e-02 (2.1915e-02)	Acc@1  97.66 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [51][ 50/391]	Time  0.132 ( 0.139)	Data  0.001 ( 0.008)	Loss 2.1932e-02 (2.2967e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [51][ 60/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.007)	Loss 2.1201e-02 (2.3423e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [51][ 70/391]	Time  0.138 ( 0.138)	Data  0.002 ( 0.007)	Loss 1.2631e-02 (2.3123e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [51][ 80/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.007)	Loss 1.7736e-02 (2.3464e-02)	Acc@1  99.22 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [51][ 90/391]	Time  0.130 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.8179e-02 (2.4137e-02)	Acc@1  99.22 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [51][100/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.9534e-02 (2.5230e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [51][110/391]	Time  0.129 ( 0.138)	Data  0.001 ( 0.006)	Loss 5.8932e-02 (2.6265e-02)	Acc@1  98.44 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [51][120/391]	Time  0.136 ( 0.138)	Data  0.002 ( 0.006)	Loss 1.9947e-02 (2.6052e-02)	Acc@1  99.22 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [51][130/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.006)	Loss 2.7736e-02 (2.6253e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [51][140/391]	Time  0.129 ( 0.137)	Data  0.001 ( 0.006)	Loss 3.0717e-02 (2.5860e-02)	Acc@1  98.44 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [51][150/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.006)	Loss 3.7557e-02 (2.6407e-02)	Acc@1  97.66 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [51][160/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.6418e-02 (2.6529e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [51][170/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.0325e-02 (2.7130e-02)	Acc@1  99.22 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [51][180/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0367e-02 (2.7118e-02)	Acc@1 100.00 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [51][190/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.5921e-02 (2.7285e-02)	Acc@1  98.44 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [51][200/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0523e-02 (2.7319e-02)	Acc@1 100.00 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [51][210/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.5730e-02 (2.6866e-02)	Acc@1  99.22 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [51][220/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.3701e-02 (2.6923e-02)	Acc@1  99.22 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [51][230/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.2596e-02 (2.6936e-02)	Acc@1 100.00 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [51][240/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.5347e-03 (2.6545e-02)	Acc@1 100.00 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [51][250/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.9560e-02 (2.7000e-02)	Acc@1  97.66 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [51][260/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.6777e-03 (2.6772e-02)	Acc@1 100.00 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [51][270/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.6003e-02 (2.6906e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [51][280/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.1765e-02 (2.6845e-02)	Acc@1  98.44 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [51][290/391]	Time  0.133 ( 0.137)	Data  0.002 ( 0.005)	Loss 4.0416e-02 (2.7037e-02)	Acc@1  97.66 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [51][300/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.1850e-02 (2.7254e-02)	Acc@1  99.22 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [51][310/391]	Time  0.143 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.8624e-02 (2.7086e-02)	Acc@1  99.22 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [51][320/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.1014e-02 (2.6909e-02)	Acc@1  99.22 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [51][330/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.2650e-02 (2.6998e-02)	Acc@1 100.00 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [51][340/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.9980e-02 (2.7525e-02)	Acc@1  98.44 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [51][350/391]	Time  0.134 ( 0.137)	Data  0.002 ( 0.005)	Loss 1.7372e-02 (2.7836e-02)	Acc@1 100.00 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [51][360/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.4934e-03 (2.7681e-02)	Acc@1 100.00 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [51][370/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.0608e-03 (2.7505e-02)	Acc@1 100.00 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [51][380/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.4215e-02 (2.7348e-02)	Acc@1  99.22 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [51][390/391]	Time  0.125 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.2129e-02 (2.7238e-02)	Acc@1  97.50 ( 99.11)	Acc@5 100.00 (100.00)
## e[51] optimizer.zero_grad (sum) time: 0.2886161804199219
## e[51]       loss.backward (sum) time: 18.54756212234497
## e[51]      optimizer.step (sum) time: 2.9432613849639893
## epoch[51] training(only) time: 53.68516826629639
# Switched to evaluate mode...
Test: [  0/100]	Time  0.190 ( 0.190)	Loss 1.2290e-01 (1.2290e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.044 ( 0.058)	Loss 5.1430e-01 (2.9794e-01)	Acc@1  89.00 ( 91.82)	Acc@5  99.00 ( 99.82)
Test: [ 20/100]	Time  0.046 ( 0.051)	Loss 4.4661e-01 (3.6782e-01)	Acc@1  89.00 ( 91.57)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.042 ( 0.049)	Loss 2.1656e-01 (3.8721e-01)	Acc@1  93.00 ( 91.71)	Acc@5 100.00 ( 99.81)
Test: [ 40/100]	Time  0.041 ( 0.047)	Loss 3.5005e-01 (3.9567e-01)	Acc@1  90.00 ( 91.73)	Acc@5  99.00 ( 99.78)
Test: [ 50/100]	Time  0.045 ( 0.047)	Loss 1.8253e-01 (3.8937e-01)	Acc@1  97.00 ( 91.90)	Acc@5 100.00 ( 99.80)
Test: [ 60/100]	Time  0.043 ( 0.046)	Loss 4.4144e-01 (3.8821e-01)	Acc@1  93.00 ( 91.93)	Acc@5  99.00 ( 99.80)
Test: [ 70/100]	Time  0.041 ( 0.046)	Loss 6.5053e-01 (3.8363e-01)	Acc@1  88.00 ( 91.89)	Acc@5 100.00 ( 99.80)
Test: [ 80/100]	Time  0.047 ( 0.045)	Loss 2.8605e-01 (3.8575e-01)	Acc@1  95.00 ( 91.91)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.040 ( 0.045)	Loss 2.4379e-01 (3.7806e-01)	Acc@1  95.00 ( 92.02)	Acc@5 100.00 ( 99.81)
 * Acc@1 92.130 Acc@5 99.810
### epoch[51] execution time: 58.29381060600281
EPOCH 52
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [52][  0/391]	Time  0.287 ( 0.287)	Data  0.166 ( 0.166)	Loss 6.4160e-03 (6.4160e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [52][ 10/391]	Time  0.134 ( 0.149)	Data  0.001 ( 0.019)	Loss 1.7918e-02 (2.2612e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [52][ 20/391]	Time  0.131 ( 0.142)	Data  0.001 ( 0.012)	Loss 1.8215e-02 (2.8855e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [52][ 30/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.010)	Loss 1.4573e-02 (2.7839e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [52][ 40/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.008)	Loss 9.4275e-03 (2.6585e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [52][ 50/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.008)	Loss 1.1506e-02 (2.5289e-02)	Acc@1 100.00 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [52][ 60/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.007)	Loss 3.7365e-02 (2.4141e-02)	Acc@1  97.66 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [52][ 70/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.007)	Loss 7.5303e-03 (2.5050e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [52][ 80/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.8679e-02 (2.5122e-02)	Acc@1  97.66 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [52][ 90/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.0566e-02 (2.4511e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [52][100/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.4977e-02 (2.4270e-02)	Acc@1  97.66 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [52][110/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.1064e-02 (2.4079e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [52][120/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.7432e-02 (2.3719e-02)	Acc@1 100.00 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [52][130/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.006)	Loss 2.2274e-02 (2.3772e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [52][140/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.006)	Loss 2.6108e-03 (2.3583e-02)	Acc@1 100.00 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [52][150/391]	Time  0.132 ( 0.137)	Data  0.002 ( 0.006)	Loss 4.9637e-03 (2.3788e-02)	Acc@1 100.00 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [52][160/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.4126e-02 (2.3523e-02)	Acc@1  99.22 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [52][170/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.8064e-02 (2.3270e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [52][180/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.5337e-03 (2.3527e-02)	Acc@1 100.00 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [52][190/391]	Time  0.133 ( 0.137)	Data  0.002 ( 0.005)	Loss 5.6260e-02 (2.3586e-02)	Acc@1  98.44 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [52][200/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.6262e-02 (2.3907e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [52][210/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.2863e-02 (2.3964e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [52][220/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.4041e-02 (2.4024e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [52][230/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.7606e-02 (2.3874e-02)	Acc@1  98.44 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [52][240/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.0302e-03 (2.4318e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [52][250/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.4391e-02 (2.4541e-02)	Acc@1  98.44 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [52][260/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.4219e-03 (2.4195e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [52][270/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.1131e-03 (2.4333e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [52][280/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.6597e-02 (2.4085e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [52][290/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.9494e-02 (2.4489e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [52][300/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.3216e-02 (2.4433e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [52][310/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.9221e-03 (2.4402e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [52][320/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.2239e-02 (2.4427e-02)	Acc@1  98.44 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [52][330/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.9422e-02 (2.4605e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [52][340/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.6892e-02 (2.4409e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [52][350/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.1710e-03 (2.4613e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [52][360/391]	Time  0.137 ( 0.137)	Data  0.002 ( 0.005)	Loss 1.0589e-02 (2.4476e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [52][370/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0718e-02 (2.4270e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [52][380/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.4440e-02 (2.4361e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [52][390/391]	Time  0.128 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1456e-01 (2.4560e-02)	Acc@1  96.25 ( 99.19)	Acc@5 100.00 (100.00)
## e[52] optimizer.zero_grad (sum) time: 0.2875638008117676
## e[52]       loss.backward (sum) time: 18.56077742576599
## e[52]      optimizer.step (sum) time: 2.858771800994873
## epoch[52] training(only) time: 53.76081299781799
# Switched to evaluate mode...
Test: [  0/100]	Time  0.201 ( 0.201)	Loss 1.3867e-01 (1.3867e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.058)	Loss 4.6481e-01 (3.0548e-01)	Acc@1  91.00 ( 92.73)	Acc@5  99.00 ( 99.73)
Test: [ 20/100]	Time  0.044 ( 0.051)	Loss 3.7385e-01 (3.6204e-01)	Acc@1  90.00 ( 91.81)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.042 ( 0.049)	Loss 1.8961e-01 (3.8950e-01)	Acc@1  95.00 ( 91.81)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.046 ( 0.047)	Loss 4.1394e-01 (4.0341e-01)	Acc@1  89.00 ( 91.51)	Acc@5  99.00 ( 99.73)
Test: [ 50/100]	Time  0.042 ( 0.047)	Loss 2.1903e-01 (4.0187e-01)	Acc@1  93.00 ( 91.49)	Acc@5  99.00 ( 99.75)
Test: [ 60/100]	Time  0.044 ( 0.046)	Loss 3.8969e-01 (3.9124e-01)	Acc@1  94.00 ( 91.67)	Acc@5  99.00 ( 99.75)
Test: [ 70/100]	Time  0.045 ( 0.046)	Loss 4.8312e-01 (3.8497e-01)	Acc@1  89.00 ( 91.70)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.042 ( 0.046)	Loss 2.2796e-01 (3.9009e-01)	Acc@1  94.00 ( 91.75)	Acc@5 100.00 ( 99.73)
Test: [ 90/100]	Time  0.044 ( 0.045)	Loss 2.2612e-01 (3.8443e-01)	Acc@1  95.00 ( 91.84)	Acc@5 100.00 ( 99.76)
 * Acc@1 91.910 Acc@5 99.760
### epoch[52] execution time: 58.38432860374451
EPOCH 53
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [53][  0/391]	Time  0.306 ( 0.306)	Data  0.187 ( 0.187)	Loss 8.9469e-03 (8.9469e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [53][ 10/391]	Time  0.131 ( 0.151)	Data  0.001 ( 0.021)	Loss 1.0464e-02 (1.5908e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [53][ 20/391]	Time  0.134 ( 0.143)	Data  0.001 ( 0.013)	Loss 2.2051e-02 (1.6450e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [53][ 30/391]	Time  0.138 ( 0.141)	Data  0.001 ( 0.010)	Loss 1.7439e-02 (1.7350e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [53][ 40/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.009)	Loss 1.8195e-02 (1.7221e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [53][ 50/391]	Time  0.133 ( 0.140)	Data  0.001 ( 0.008)	Loss 7.9535e-03 (1.7442e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [53][ 60/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.007)	Loss 2.2147e-02 (1.8664e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [53][ 70/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.007)	Loss 7.8023e-03 (1.8363e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [53][ 80/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.007)	Loss 4.0997e-02 (1.8855e-02)	Acc@1  98.44 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [53][ 90/391]	Time  0.131 ( 0.138)	Data  0.001 ( 0.007)	Loss 3.6189e-02 (2.0236e-02)	Acc@1  98.44 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [53][100/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.4521e-02 (2.0662e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [53][110/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 7.9343e-03 (2.1293e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [53][120/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.3614e-02 (2.1608e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [53][130/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.9319e-02 (2.1736e-02)	Acc@1  98.44 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [53][140/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.006)	Loss 8.1639e-03 (2.1699e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [53][150/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.6354e-02 (2.1668e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [53][160/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.8815e-02 (2.1672e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [53][170/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.2084e-02 (2.1741e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [53][180/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.1220e-02 (2.2160e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [53][190/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.2322e-02 (2.2328e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [53][200/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.9372e-02 (2.2317e-02)	Acc@1  97.66 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [53][210/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.6388e-02 (2.2487e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [53][220/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.7712e-02 (2.2559e-02)	Acc@1  98.44 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [53][230/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3461e-02 (2.2443e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [53][240/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.6667e-02 (2.2657e-02)	Acc@1  96.09 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [53][250/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.6571e-02 (2.3011e-02)	Acc@1  99.22 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [53][260/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.6888e-02 (2.3026e-02)	Acc@1  98.44 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [53][270/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.5353e-02 (2.3114e-02)	Acc@1  98.44 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [53][280/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.0357e-02 (2.3040e-02)	Acc@1  99.22 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [53][290/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.7030e-02 (2.3073e-02)	Acc@1  97.66 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [53][300/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.5427e-03 (2.3258e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [53][310/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.1662e-02 (2.3134e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [53][320/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.7945e-02 (2.3319e-02)	Acc@1  98.44 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [53][330/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.8549e-02 (2.3317e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [53][340/391]	Time  0.135 ( 0.137)	Data  0.002 ( 0.005)	Loss 9.3973e-03 (2.3059e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [53][350/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.4005e-02 (2.3238e-02)	Acc@1  97.66 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [53][360/391]	Time  0.141 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.4942e-02 (2.3226e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [53][370/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.4661e-02 (2.3147e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [53][380/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.9008e-03 (2.2968e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [53][390/391]	Time  0.128 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.6849e-02 (2.3409e-02)	Acc@1  97.50 ( 99.18)	Acc@5 100.00 (100.00)
## e[53] optimizer.zero_grad (sum) time: 0.28472280502319336
## e[53]       loss.backward (sum) time: 18.534944534301758
## e[53]      optimizer.step (sum) time: 2.883824110031128
## epoch[53] training(only) time: 53.79544544219971
# Switched to evaluate mode...
Test: [  0/100]	Time  0.202 ( 0.202)	Loss 2.2340e-01 (2.2340e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.042 ( 0.058)	Loss 4.0552e-01 (3.1120e-01)	Acc@1  89.00 ( 92.45)	Acc@5  99.00 ( 99.82)
Test: [ 20/100]	Time  0.046 ( 0.051)	Loss 3.6563e-01 (3.7019e-01)	Acc@1  91.00 ( 91.90)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.045 ( 0.049)	Loss 3.0411e-01 (3.9615e-01)	Acc@1  93.00 ( 91.61)	Acc@5 100.00 ( 99.77)
Test: [ 40/100]	Time  0.043 ( 0.047)	Loss 3.7983e-01 (4.0433e-01)	Acc@1  90.00 ( 91.63)	Acc@5 100.00 ( 99.83)
Test: [ 50/100]	Time  0.050 ( 0.047)	Loss 2.5588e-01 (4.0340e-01)	Acc@1  95.00 ( 91.63)	Acc@5  99.00 ( 99.82)
Test: [ 60/100]	Time  0.043 ( 0.046)	Loss 3.4145e-01 (3.9283e-01)	Acc@1  95.00 ( 91.85)	Acc@5 100.00 ( 99.84)
Test: [ 70/100]	Time  0.044 ( 0.046)	Loss 5.2253e-01 (3.8208e-01)	Acc@1  88.00 ( 91.94)	Acc@5 100.00 ( 99.82)
Test: [ 80/100]	Time  0.042 ( 0.045)	Loss 2.4847e-01 (3.8898e-01)	Acc@1  92.00 ( 91.98)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.043 ( 0.045)	Loss 2.2379e-01 (3.8226e-01)	Acc@1  94.00 ( 92.08)	Acc@5 100.00 ( 99.81)
 * Acc@1 92.160 Acc@5 99.820
### epoch[53] execution time: 58.41004729270935
EPOCH 54
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [54][  0/391]	Time  0.297 ( 0.297)	Data  0.181 ( 0.181)	Loss 1.0439e-02 (1.0439e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [54][ 10/391]	Time  0.136 ( 0.151)	Data  0.006 ( 0.021)	Loss 3.0410e-02 (2.4225e-02)	Acc@1  98.44 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [54][ 20/391]	Time  0.134 ( 0.144)	Data  0.001 ( 0.013)	Loss 1.2586e-02 (2.0586e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [54][ 30/391]	Time  0.136 ( 0.141)	Data  0.001 ( 0.010)	Loss 2.9823e-02 (1.9139e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [54][ 40/391]	Time  0.132 ( 0.140)	Data  0.001 ( 0.009)	Loss 2.0126e-02 (1.8429e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [54][ 50/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.008)	Loss 7.1642e-03 (1.7510e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [54][ 60/391]	Time  0.131 ( 0.139)	Data  0.001 ( 0.007)	Loss 1.7896e-02 (1.8326e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [54][ 70/391]	Time  0.133 ( 0.139)	Data  0.002 ( 0.007)	Loss 5.6808e-02 (1.7875e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [54][ 80/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.007)	Loss 8.5487e-03 (1.9251e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [54][ 90/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.006)	Loss 2.5367e-02 (1.9317e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [54][100/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 5.5083e-02 (2.0496e-02)	Acc@1  97.66 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [54][110/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.6217e-02 (2.0347e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [54][120/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.3514e-02 (2.0551e-02)	Acc@1  98.44 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [54][130/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 7.6828e-03 (2.0629e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [54][140/391]	Time  0.129 ( 0.138)	Data  0.001 ( 0.006)	Loss 5.8520e-02 (2.1359e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [54][150/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.006)	Loss 5.0017e-02 (2.1279e-02)	Acc@1  98.44 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [54][160/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.0030e-02 (2.1891e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [54][170/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.1227e-02 (2.1577e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [54][180/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.1572e-02 (2.1443e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [54][190/391]	Time  0.131 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.6702e-03 (2.1516e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [54][200/391]	Time  0.132 ( 0.138)	Data  0.002 ( 0.005)	Loss 1.1630e-02 (2.1039e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [54][210/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.005)	Loss 4.3053e-02 (2.1159e-02)	Acc@1  97.66 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [54][220/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.3854e-02 (2.1556e-02)	Acc@1  97.66 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [54][230/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.9076e-03 (2.1517e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [54][240/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.7988e-02 (2.1480e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [54][250/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.5157e-02 (2.1618e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [54][260/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.4512e-02 (2.1678e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [54][270/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.9481e-02 (2.1879e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [54][280/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.0263e-02 (2.1972e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [54][290/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.7463e-02 (2.2110e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [54][300/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.4622e-02 (2.2259e-02)	Acc@1  98.44 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [54][310/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.7857e-02 (2.2271e-02)	Acc@1  97.66 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [54][320/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.3291e-02 (2.2577e-02)	Acc@1  98.44 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [54][330/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.7601e-02 (2.2571e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [54][340/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1242e-02 (2.2634e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [54][350/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.4677e-02 (2.2754e-02)	Acc@1  98.44 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [54][360/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.6287e-02 (2.3057e-02)	Acc@1  96.88 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [54][370/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.8007e-02 (2.3041e-02)	Acc@1  98.44 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [54][380/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.1981e-02 (2.3142e-02)	Acc@1  98.44 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [54][390/391]	Time  0.126 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.7667e-02 (2.3196e-02)	Acc@1  98.75 ( 99.25)	Acc@5 100.00 (100.00)
## e[54] optimizer.zero_grad (sum) time: 0.28713202476501465
## e[54]       loss.backward (sum) time: 18.530444622039795
## e[54]      optimizer.step (sum) time: 2.8692071437835693
## epoch[54] training(only) time: 53.83166456222534
# Switched to evaluate mode...
Test: [  0/100]	Time  0.200 ( 0.200)	Loss 1.7905e-01 (1.7905e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.042 ( 0.057)	Loss 4.2968e-01 (3.1237e-01)	Acc@1  90.00 ( 93.36)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.043 ( 0.051)	Loss 4.4702e-01 (3.8684e-01)	Acc@1  89.00 ( 92.29)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.044 ( 0.049)	Loss 1.9916e-01 (3.9667e-01)	Acc@1  94.00 ( 92.26)	Acc@5 100.00 ( 99.81)
Test: [ 40/100]	Time  0.042 ( 0.047)	Loss 4.1232e-01 (4.1475e-01)	Acc@1  91.00 ( 91.83)	Acc@5  99.00 ( 99.78)
Test: [ 50/100]	Time  0.043 ( 0.046)	Loss 1.4900e-01 (4.1201e-01)	Acc@1  94.00 ( 91.78)	Acc@5 100.00 ( 99.80)
Test: [ 60/100]	Time  0.046 ( 0.046)	Loss 3.9654e-01 (3.9877e-01)	Acc@1  92.00 ( 91.92)	Acc@5  99.00 ( 99.80)
Test: [ 70/100]	Time  0.044 ( 0.046)	Loss 5.6742e-01 (3.9558e-01)	Acc@1  88.00 ( 91.92)	Acc@5 100.00 ( 99.80)
Test: [ 80/100]	Time  0.046 ( 0.045)	Loss 2.3897e-01 (3.9901e-01)	Acc@1  95.00 ( 91.98)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.045 ( 0.045)	Loss 2.9909e-01 (3.9158e-01)	Acc@1  94.00 ( 92.05)	Acc@5 100.00 ( 99.81)
 * Acc@1 92.100 Acc@5 99.810
### epoch[54] execution time: 58.438621044158936
EPOCH 55
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [55][  0/391]	Time  0.299 ( 0.299)	Data  0.177 ( 0.177)	Loss 3.7026e-02 (3.7026e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [55][ 10/391]	Time  0.127 ( 0.151)	Data  0.001 ( 0.020)	Loss 1.3035e-02 (2.7803e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [55][ 20/391]	Time  0.131 ( 0.143)	Data  0.001 ( 0.013)	Loss 7.4444e-03 (2.4755e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 ( 99.96)
Epoch: [55][ 30/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.010)	Loss 1.6666e-02 (2.3345e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 ( 99.97)
Epoch: [55][ 40/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.009)	Loss 2.0690e-02 (2.3028e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 ( 99.98)
Epoch: [55][ 50/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.008)	Loss 3.9691e-03 (2.2946e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 ( 99.98)
Epoch: [55][ 60/391]	Time  0.132 ( 0.139)	Data  0.001 ( 0.007)	Loss 1.8795e-02 (2.4379e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 ( 99.99)
Epoch: [55][ 70/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.007)	Loss 1.7517e-02 (2.4523e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 ( 99.99)
Epoch: [55][ 80/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 9.8135e-03 (2.2934e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 ( 99.99)
Epoch: [55][ 90/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.7838e-02 (2.2778e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 ( 99.99)
Epoch: [55][100/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.4430e-03 (2.2407e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 ( 99.99)
Epoch: [55][110/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.1567e-02 (2.2625e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 ( 99.99)
Epoch: [55][120/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.006)	Loss 1.6030e-02 (2.2366e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 ( 99.99)
Epoch: [55][130/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.006)	Loss 1.8580e-02 (2.2132e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 ( 99.99)
Epoch: [55][140/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.006)	Loss 2.4499e-02 (2.2051e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 ( 99.99)
Epoch: [55][150/391]	Time  0.138 ( 0.137)	Data  0.002 ( 0.006)	Loss 1.8004e-02 (2.2606e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 ( 99.99)
Epoch: [55][160/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.4312e-02 (2.2131e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [55][170/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.5070e-02 (2.2339e-02)	Acc@1  98.44 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [55][180/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.2025e-02 (2.2295e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [55][190/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0612e-02 (2.1845e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [55][200/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.1507e-02 (2.2023e-02)	Acc@1  98.44 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [55][210/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.1974e-03 (2.1674e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [55][220/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.1823e-03 (2.1540e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [55][230/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.4859e-03 (2.1455e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [55][240/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.0189e-02 (2.1332e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [55][250/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.5311e-02 (2.1324e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [55][260/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0214e-02 (2.1287e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [55][270/391]	Time  0.130 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.9919e-02 (2.1562e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [55][280/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.2795e-02 (2.1393e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [55][290/391]	Time  0.132 ( 0.137)	Data  0.002 ( 0.005)	Loss 9.3638e-03 (2.1543e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [55][300/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.6098e-02 (2.1556e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [55][310/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.1563e-02 (2.1524e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [55][320/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.7229e-02 (2.1679e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [55][330/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.7647e-02 (2.1536e-02)	Acc@1  98.44 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [55][340/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.4298e-03 (2.1695e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [55][350/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1583e-02 (2.1673e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [55][360/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.7309e-02 (2.1857e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [55][370/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.2749e-02 (2.1981e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [55][380/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.5196e-03 (2.1962e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [55][390/391]	Time  0.128 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.4118e-02 (2.2073e-02)	Acc@1  98.75 ( 99.26)	Acc@5 100.00 (100.00)
## e[55] optimizer.zero_grad (sum) time: 0.2863037586212158
## e[55]       loss.backward (sum) time: 18.507456064224243
## e[55]      optimizer.step (sum) time: 2.8893697261810303
## epoch[55] training(only) time: 53.75771498680115
# Switched to evaluate mode...
Test: [  0/100]	Time  0.208 ( 0.208)	Loss 1.8144e-01 (1.8144e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.044 ( 0.059)	Loss 4.3234e-01 (3.1443e-01)	Acc@1  93.00 ( 92.64)	Acc@5  98.00 ( 99.55)
Test: [ 20/100]	Time  0.043 ( 0.051)	Loss 4.6843e-01 (3.8691e-01)	Acc@1  89.00 ( 91.71)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.045 ( 0.049)	Loss 3.0300e-01 (4.0820e-01)	Acc@1  91.00 ( 92.03)	Acc@5 100.00 ( 99.77)
Test: [ 40/100]	Time  0.046 ( 0.048)	Loss 4.6582e-01 (4.2416e-01)	Acc@1  90.00 ( 91.63)	Acc@5 100.00 ( 99.78)
Test: [ 50/100]	Time  0.051 ( 0.047)	Loss 1.6865e-01 (4.1707e-01)	Acc@1  95.00 ( 91.59)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.042 ( 0.046)	Loss 3.6482e-01 (3.9995e-01)	Acc@1  94.00 ( 91.84)	Acc@5 100.00 ( 99.82)
Test: [ 70/100]	Time  0.045 ( 0.046)	Loss 4.9924e-01 (3.9088e-01)	Acc@1  89.00 ( 91.94)	Acc@5 100.00 ( 99.83)
Test: [ 80/100]	Time  0.046 ( 0.045)	Loss 2.5801e-01 (3.9499e-01)	Acc@1  93.00 ( 91.99)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.044 ( 0.045)	Loss 2.3811e-01 (3.8504e-01)	Acc@1  93.00 ( 92.10)	Acc@5 100.00 ( 99.82)
 * Acc@1 92.170 Acc@5 99.800
### epoch[55] execution time: 58.38238739967346
EPOCH 56
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [56][  0/391]	Time  0.301 ( 0.301)	Data  0.179 ( 0.179)	Loss 1.6751e-02 (1.6751e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [56][ 10/391]	Time  0.136 ( 0.151)	Data  0.001 ( 0.020)	Loss 1.0787e-02 (2.1932e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [56][ 20/391]	Time  0.139 ( 0.144)	Data  0.001 ( 0.013)	Loss 2.1557e-02 (2.0836e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [56][ 30/391]	Time  0.133 ( 0.141)	Data  0.001 ( 0.010)	Loss 1.5010e-02 (2.0924e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [56][ 40/391]	Time  0.131 ( 0.140)	Data  0.001 ( 0.009)	Loss 1.2761e-02 (2.0740e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [56][ 50/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.008)	Loss 1.5592e-02 (1.9809e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [56][ 60/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.007)	Loss 1.2064e-02 (1.8990e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [56][ 70/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.007)	Loss 2.7403e-02 (1.8641e-02)	Acc@1  98.44 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [56][ 80/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.007)	Loss 7.0632e-03 (1.8499e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [56][ 90/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.8001e-02 (1.8555e-02)	Acc@1  98.44 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [56][100/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.8767e-02 (1.9179e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [56][110/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.0104e-02 (1.9755e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [56][120/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.2219e-02 (1.9806e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [56][130/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.5425e-02 (2.0093e-02)	Acc@1  98.44 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [56][140/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.3848e-02 (2.0126e-02)	Acc@1  97.66 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [56][150/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.006)	Loss 7.2095e-03 (2.0794e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [56][160/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.0524e-02 (2.0627e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [56][170/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.5509e-02 (2.0728e-02)	Acc@1  98.44 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [56][180/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.6179e-03 (2.0537e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [56][190/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.005)	Loss 4.2339e-03 (2.0408e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [56][200/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.2549e-02 (2.0477e-02)	Acc@1  98.44 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [56][210/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.3703e-02 (2.0681e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [56][220/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.7166e-02 (2.0320e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [56][230/391]	Time  0.130 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.9732e-03 (2.0406e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [56][240/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.8231e-02 (2.0278e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [56][250/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.9127e-03 (2.0630e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [56][260/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.1630e-02 (2.0461e-02)	Acc@1  98.44 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [56][270/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.6078e-02 (2.0693e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [56][280/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.4576e-03 (2.0763e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [56][290/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.3694e-03 (2.1128e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [56][300/391]	Time  0.130 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.2163e-02 (2.1616e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [56][310/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.6808e-03 (2.1679e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [56][320/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.6281e-02 (2.1637e-02)	Acc@1  98.44 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [56][330/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.0133e-02 (2.1774e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [56][340/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.0269e-02 (2.1703e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [56][350/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.0852e-02 (2.2080e-02)	Acc@1  98.44 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [56][360/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.5983e-02 (2.1987e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [56][370/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1515e-02 (2.1897e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [56][380/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.4345e-02 (2.1822e-02)	Acc@1  98.44 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [56][390/391]	Time  0.125 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.6190e-03 (2.2073e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
## e[56] optimizer.zero_grad (sum) time: 0.28392457962036133
## e[56]       loss.backward (sum) time: 18.503321409225464
## e[56]      optimizer.step (sum) time: 2.8985822200775146
## epoch[56] training(only) time: 53.678056955337524
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 1.4967e-01 (1.4967e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.044 ( 0.056)	Loss 4.1431e-01 (3.1129e-01)	Acc@1  92.00 ( 92.45)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.042 ( 0.050)	Loss 3.7057e-01 (3.6864e-01)	Acc@1  90.00 ( 91.52)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.044 ( 0.048)	Loss 3.2783e-01 (3.9932e-01)	Acc@1  90.00 ( 91.55)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.042 ( 0.047)	Loss 4.9688e-01 (4.0656e-01)	Acc@1  90.00 ( 91.41)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.045 ( 0.046)	Loss 1.8329e-01 (4.0356e-01)	Acc@1  94.00 ( 91.43)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.044 ( 0.046)	Loss 3.3469e-01 (3.9550e-01)	Acc@1  96.00 ( 91.69)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.043 ( 0.046)	Loss 8.9564e-01 (3.9426e-01)	Acc@1  88.00 ( 91.65)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.044 ( 0.045)	Loss 1.8476e-01 (3.9366e-01)	Acc@1  95.00 ( 91.70)	Acc@5 100.00 ( 99.69)
Test: [ 90/100]	Time  0.042 ( 0.045)	Loss 2.1533e-01 (3.8722e-01)	Acc@1  95.00 ( 91.88)	Acc@5 100.00 ( 99.73)
 * Acc@1 92.000 Acc@5 99.710
### epoch[56] execution time: 58.27744507789612
EPOCH 57
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [57][  0/391]	Time  0.297 ( 0.297)	Data  0.179 ( 0.179)	Loss 3.1468e-03 (3.1468e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [57][ 10/391]	Time  0.134 ( 0.149)	Data  0.001 ( 0.020)	Loss 3.4498e-02 (1.3727e-02)	Acc@1  98.44 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [57][ 20/391]	Time  0.134 ( 0.143)	Data  0.001 ( 0.013)	Loss 7.1563e-03 (1.7091e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [57][ 30/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.010)	Loss 2.1603e-02 (2.1230e-02)	Acc@1  97.66 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [57][ 40/391]	Time  0.132 ( 0.139)	Data  0.002 ( 0.009)	Loss 4.2117e-03 (2.1059e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [57][ 50/391]	Time  0.137 ( 0.139)	Data  0.002 ( 0.008)	Loss 2.0082e-02 (2.0290e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [57][ 60/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.007)	Loss 2.3498e-02 (2.1592e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [57][ 70/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.007)	Loss 4.5812e-03 (2.0187e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [57][ 80/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.007)	Loss 7.2611e-03 (1.9561e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [57][ 90/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 8.2350e-03 (1.8681e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [57][100/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.0273e-02 (1.8247e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [57][110/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.3405e-02 (1.7986e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [57][120/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.5568e-02 (1.7663e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [57][130/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.006)	Loss 9.4079e-03 (1.7804e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [57][140/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.006)	Loss 1.8739e-02 (1.7584e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [57][150/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.006)	Loss 9.8880e-03 (1.7367e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [57][160/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.006)	Loss 3.0574e-02 (1.7071e-02)	Acc@1  98.44 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [57][170/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.006)	Loss 2.8551e-03 (1.7017e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [57][180/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.1014e-02 (1.7719e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [57][190/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.0341e-02 (1.7922e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [57][200/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.0277e-03 (1.8580e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [57][210/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3734e-02 (1.8765e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [57][220/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.2953e-02 (1.9239e-02)	Acc@1  97.66 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [57][230/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.4449e-02 (1.9207e-02)	Acc@1  98.44 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [57][240/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1555e-02 (1.9085e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [57][250/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.3179e-02 (1.9494e-02)	Acc@1  98.44 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [57][260/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3881e-02 (1.9645e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [57][270/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.4500e-02 (1.9827e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [57][280/391]	Time  0.135 ( 0.137)	Data  0.002 ( 0.005)	Loss 5.0365e-03 (1.9857e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [57][290/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.2552e-02 (1.9704e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [57][300/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.2772e-02 (1.9830e-02)	Acc@1  98.44 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [57][310/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.9973e-02 (1.9821e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [57][320/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.5685e-02 (2.0017e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [57][330/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.0410e-03 (2.0085e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [57][340/391]	Time  0.129 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.4591e-02 (2.0082e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [57][350/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.4221e-02 (2.0017e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [57][360/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.8742e-03 (2.0056e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [57][370/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.2123e-02 (2.0105e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [57][380/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.5213e-02 (2.0335e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [57][390/391]	Time  0.128 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.5522e-03 (2.0456e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
## e[57] optimizer.zero_grad (sum) time: 0.28803086280822754
## e[57]       loss.backward (sum) time: 18.524221897125244
## e[57]      optimizer.step (sum) time: 2.9230477809906006
## epoch[57] training(only) time: 53.76605677604675
# Switched to evaluate mode...
Test: [  0/100]	Time  0.202 ( 0.202)	Loss 1.9070e-01 (1.9070e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.045 ( 0.058)	Loss 5.5132e-01 (3.2987e-01)	Acc@1  87.00 ( 92.64)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.045 ( 0.051)	Loss 5.2184e-01 (3.9510e-01)	Acc@1  88.00 ( 91.71)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.054 ( 0.049)	Loss 3.0918e-01 (4.1266e-01)	Acc@1  92.00 ( 91.77)	Acc@5 100.00 ( 99.81)
Test: [ 40/100]	Time  0.043 ( 0.048)	Loss 6.0803e-01 (4.2487e-01)	Acc@1  88.00 ( 91.54)	Acc@5  99.00 ( 99.83)
Test: [ 50/100]	Time  0.043 ( 0.047)	Loss 2.8131e-01 (4.2382e-01)	Acc@1  94.00 ( 91.61)	Acc@5 100.00 ( 99.82)
Test: [ 60/100]	Time  0.047 ( 0.046)	Loss 3.4459e-01 (4.0989e-01)	Acc@1  94.00 ( 91.75)	Acc@5  99.00 ( 99.82)
Test: [ 70/100]	Time  0.043 ( 0.046)	Loss 8.5582e-01 (4.0495e-01)	Acc@1  87.00 ( 91.76)	Acc@5  99.00 ( 99.80)
Test: [ 80/100]	Time  0.045 ( 0.046)	Loss 2.2259e-01 (4.0700e-01)	Acc@1  94.00 ( 91.75)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.045 ( 0.046)	Loss 1.6825e-01 (3.9617e-01)	Acc@1  95.00 ( 91.86)	Acc@5 100.00 ( 99.77)
 * Acc@1 91.950 Acc@5 99.760
### epoch[57] execution time: 58.42515563964844
EPOCH 58
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [58][  0/391]	Time  0.302 ( 0.302)	Data  0.175 ( 0.175)	Loss 3.3043e-02 (3.3043e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [58][ 10/391]	Time  0.128 ( 0.151)	Data  0.001 ( 0.020)	Loss 9.4875e-03 (1.9437e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [58][ 20/391]	Time  0.134 ( 0.144)	Data  0.001 ( 0.013)	Loss 3.5584e-03 (1.9921e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [58][ 30/391]	Time  0.135 ( 0.141)	Data  0.001 ( 0.010)	Loss 5.3443e-02 (2.2221e-02)	Acc@1  98.44 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [58][ 40/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.009)	Loss 1.6910e-02 (2.1270e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [58][ 50/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.008)	Loss 9.4204e-03 (1.9805e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [58][ 60/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.007)	Loss 6.2864e-03 (1.9222e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [58][ 70/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.007)	Loss 2.0877e-02 (1.9270e-02)	Acc@1  98.44 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [58][ 80/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.007)	Loss 9.4255e-03 (1.8998e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [58][ 90/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.9743e-02 (1.8391e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [58][100/391]	Time  0.133 ( 0.138)	Data  0.002 ( 0.006)	Loss 9.4409e-03 (1.8446e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [58][110/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.0824e-03 (1.8912e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [58][120/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.3904e-03 (1.8777e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [58][130/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.2821e-02 (1.8695e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [58][140/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.6105e-02 (1.8744e-02)	Acc@1  98.44 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [58][150/391]	Time  0.131 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.0929e-02 (1.9414e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [58][160/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.006)	Loss 4.1332e-03 (1.9209e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [58][170/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.8558e-02 (1.9630e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [58][180/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0230e-02 (1.9834e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [58][190/391]	Time  0.133 ( 0.137)	Data  0.002 ( 0.005)	Loss 6.0121e-02 (2.0311e-02)	Acc@1  97.66 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [58][200/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.0474e-02 (2.0035e-02)	Acc@1  98.44 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [58][210/391]	Time  0.132 ( 0.137)	Data  0.002 ( 0.005)	Loss 4.3119e-03 (1.9843e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [58][220/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.7543e-02 (1.9796e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [58][230/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.1463e-03 (1.9599e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [58][240/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.9891e-03 (1.9926e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [58][250/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.9454e-03 (1.9866e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [58][260/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.6658e-03 (1.9768e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [58][270/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.1806e-02 (1.9778e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [58][280/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1503e-02 (1.9858e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [58][290/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.5845e-03 (1.9704e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [58][300/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.6242e-02 (1.9602e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [58][310/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.7374e-02 (1.9715e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [58][320/391]	Time  0.129 ( 0.137)	Data  0.002 ( 0.005)	Loss 1.0254e-02 (2.0166e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [58][330/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.5691e-03 (2.0138e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [58][340/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.1099e-03 (2.0207e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [58][350/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.7983e-02 (2.0275e-02)	Acc@1  98.44 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [58][360/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.4904e-02 (2.0098e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [58][370/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1448e-02 (2.0066e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [58][380/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.9840e-03 (2.0055e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [58][390/391]	Time  0.130 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.1268e-02 (2.0334e-02)	Acc@1  96.25 ( 99.35)	Acc@5 100.00 (100.00)
## e[58] optimizer.zero_grad (sum) time: 0.287703275680542
## e[58]       loss.backward (sum) time: 18.500854969024658
## e[58]      optimizer.step (sum) time: 2.8580896854400635
## epoch[58] training(only) time: 53.72664475440979
# Switched to evaluate mode...
Test: [  0/100]	Time  0.210 ( 0.210)	Loss 1.8225e-01 (1.8225e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.044 ( 0.059)	Loss 4.4735e-01 (3.2835e-01)	Acc@1  89.00 ( 92.18)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.044 ( 0.052)	Loss 5.1469e-01 (3.9962e-01)	Acc@1  87.00 ( 91.29)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.043 ( 0.049)	Loss 3.6483e-01 (4.0920e-01)	Acc@1  91.00 ( 91.58)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.044 ( 0.048)	Loss 5.1095e-01 (4.1769e-01)	Acc@1  89.00 ( 91.46)	Acc@5 100.00 ( 99.78)
Test: [ 50/100]	Time  0.046 ( 0.047)	Loss 1.8531e-01 (4.1246e-01)	Acc@1  95.00 ( 91.41)	Acc@5 100.00 ( 99.82)
Test: [ 60/100]	Time  0.042 ( 0.047)	Loss 4.2061e-01 (3.9716e-01)	Acc@1  92.00 ( 91.57)	Acc@5 100.00 ( 99.85)
Test: [ 70/100]	Time  0.042 ( 0.046)	Loss 6.0539e-01 (3.9234e-01)	Acc@1  87.00 ( 91.73)	Acc@5 100.00 ( 99.85)
Test: [ 80/100]	Time  0.045 ( 0.046)	Loss 1.8776e-01 (3.9318e-01)	Acc@1  95.00 ( 91.83)	Acc@5 100.00 ( 99.83)
Test: [ 90/100]	Time  0.041 ( 0.046)	Loss 2.1807e-01 (3.8527e-01)	Acc@1  95.00 ( 91.88)	Acc@5 100.00 ( 99.85)
 * Acc@1 91.950 Acc@5 99.840
### epoch[58] execution time: 58.395686626434326
EPOCH 59
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [59][  0/391]	Time  0.291 ( 0.291)	Data  0.177 ( 0.177)	Loss 3.0843e-02 (3.0843e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [59][ 10/391]	Time  0.133 ( 0.150)	Data  0.001 ( 0.020)	Loss 1.0589e-02 (1.9130e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [59][ 20/391]	Time  0.130 ( 0.143)	Data  0.001 ( 0.013)	Loss 5.1370e-02 (1.8592e-02)	Acc@1  97.66 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [59][ 30/391]	Time  0.139 ( 0.141)	Data  0.001 ( 0.010)	Loss 1.2988e-02 (1.7599e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [59][ 40/391]	Time  0.131 ( 0.140)	Data  0.001 ( 0.009)	Loss 4.4007e-03 (1.8147e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [59][ 50/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.008)	Loss 4.2581e-03 (1.8664e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [59][ 60/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.007)	Loss 2.1741e-02 (1.9462e-02)	Acc@1  98.44 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [59][ 70/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.007)	Loss 1.0225e-02 (1.9712e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [59][ 80/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.007)	Loss 1.4910e-02 (2.0609e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [59][ 90/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.0894e-02 (2.1316e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [59][100/391]	Time  0.135 ( 0.138)	Data  0.002 ( 0.006)	Loss 5.5024e-03 (2.0801e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [59][110/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 8.4424e-03 (2.0163e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [59][120/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.1151e-02 (2.0193e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 ( 99.99)
Epoch: [59][130/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.8031e-02 (1.9748e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 ( 99.99)
Epoch: [59][140/391]	Time  0.138 ( 0.138)	Data  0.002 ( 0.006)	Loss 1.2474e-02 (1.9442e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 ( 99.99)
Epoch: [59][150/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.6631e-02 (1.9085e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 ( 99.99)
Epoch: [59][160/391]	Time  0.131 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.0330e-02 (1.9039e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [59][170/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.3067e-03 (1.8686e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [59][180/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.2460e-02 (1.9154e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [59][190/391]	Time  0.131 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.5108e-02 (1.9081e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [59][200/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.9820e-03 (1.8978e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [59][210/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.1332e-02 (1.9204e-02)	Acc@1  97.66 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [59][220/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0177e-02 (1.8906e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [59][230/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.2123e-02 (1.9153e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [59][240/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.1599e-02 (1.9126e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [59][250/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.0389e-02 (1.9101e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [59][260/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.0677e-02 (1.9291e-02)	Acc@1  98.44 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [59][270/391]	Time  0.129 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.1454e-03 (1.9358e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [59][280/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.7779e-02 (1.9491e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [59][290/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.1642e-02 (1.9645e-02)	Acc@1  98.44 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [59][300/391]	Time  0.141 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.7901e-03 (1.9561e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [59][310/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.9552e-02 (1.9757e-02)	Acc@1  96.88 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [59][320/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.5872e-02 (1.9828e-02)	Acc@1  98.44 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [59][330/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1590e-02 (1.9852e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [59][340/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.7188e-02 (2.0104e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [59][350/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.9795e-03 (2.0103e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [59][360/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.6276e-02 (2.0041e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [59][370/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.0180e-02 (2.0158e-02)	Acc@1  96.88 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [59][380/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.4136e-02 (2.0020e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [59][390/391]	Time  0.122 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.1515e-01 (2.0262e-02)	Acc@1  92.50 ( 99.28)	Acc@5 100.00 (100.00)
## e[59] optimizer.zero_grad (sum) time: 0.28402018547058105
## e[59]       loss.backward (sum) time: 18.55021834373474
## e[59]      optimizer.step (sum) time: 2.931906223297119
## epoch[59] training(only) time: 53.796082496643066
# Switched to evaluate mode...
Test: [  0/100]	Time  0.199 ( 0.199)	Loss 1.5296e-01 (1.5296e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.044 ( 0.058)	Loss 5.0417e-01 (3.1195e-01)	Acc@1  89.00 ( 92.82)	Acc@5  99.00 ( 99.73)
Test: [ 20/100]	Time  0.046 ( 0.052)	Loss 4.5082e-01 (3.8062e-01)	Acc@1  88.00 ( 91.38)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.043 ( 0.049)	Loss 3.4160e-01 (4.0271e-01)	Acc@1  91.00 ( 91.74)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.041 ( 0.048)	Loss 4.6650e-01 (4.0968e-01)	Acc@1  89.00 ( 91.63)	Acc@5  99.00 ( 99.71)
Test: [ 50/100]	Time  0.045 ( 0.047)	Loss 1.4977e-01 (4.0783e-01)	Acc@1  95.00 ( 91.69)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.043 ( 0.046)	Loss 3.5524e-01 (3.9264e-01)	Acc@1  94.00 ( 91.90)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.043 ( 0.046)	Loss 4.8687e-01 (3.8943e-01)	Acc@1  90.00 ( 91.93)	Acc@5 100.00 ( 99.80)
Test: [ 80/100]	Time  0.044 ( 0.046)	Loss 1.6493e-01 (3.9362e-01)	Acc@1  95.00 ( 91.91)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.041 ( 0.045)	Loss 1.9167e-01 (3.8716e-01)	Acc@1  96.00 ( 91.96)	Acc@5 100.00 ( 99.80)
 * Acc@1 92.010 Acc@5 99.790
### epoch[59] execution time: 58.44026494026184
EPOCH 60
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [60][  0/391]	Time  0.292 ( 0.292)	Data  0.177 ( 0.177)	Loss 1.1805e-02 (1.1805e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [60][ 10/391]	Time  0.133 ( 0.150)	Data  0.001 ( 0.020)	Loss 1.8562e-02 (1.1965e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [60][ 20/391]	Time  0.134 ( 0.143)	Data  0.001 ( 0.013)	Loss 1.5842e-02 (1.2566e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [60][ 30/391]	Time  0.131 ( 0.140)	Data  0.001 ( 0.010)	Loss 3.2591e-03 (1.2132e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [60][ 40/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.009)	Loss 2.0265e-02 (1.3855e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [60][ 50/391]	Time  0.133 ( 0.139)	Data  0.002 ( 0.008)	Loss 2.5148e-03 (1.4655e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [60][ 60/391]	Time  0.142 ( 0.139)	Data  0.001 ( 0.007)	Loss 3.1836e-02 (1.5179e-02)	Acc@1  97.66 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [60][ 70/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.007)	Loss 8.1705e-03 (1.5053e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [60][ 80/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.007)	Loss 7.3746e-03 (1.4692e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [60][ 90/391]	Time  0.133 ( 0.138)	Data  0.002 ( 0.006)	Loss 2.8380e-02 (1.5030e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [60][100/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.9739e-02 (1.5194e-02)	Acc@1  98.44 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [60][110/391]	Time  0.136 ( 0.138)	Data  0.002 ( 0.006)	Loss 2.3937e-02 (1.5914e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [60][120/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.2210e-02 (1.5633e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [60][130/391]	Time  0.136 ( 0.138)	Data  0.002 ( 0.006)	Loss 1.9920e-02 (1.5763e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [60][140/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.3382e-02 (1.5169e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [60][150/391]	Time  0.136 ( 0.138)	Data  0.002 ( 0.006)	Loss 1.0192e-02 (1.5153e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [60][160/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.8137e-02 (1.4864e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [60][170/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.7619e-02 (1.5140e-02)	Acc@1  98.44 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [60][180/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.4442e-03 (1.5024e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [60][190/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.8871e-03 (1.4943e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [60][200/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.5666e-02 (1.4985e-02)	Acc@1  96.88 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [60][210/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.0240e-03 (1.4877e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [60][220/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.3535e-03 (1.4631e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [60][230/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.2854e-03 (1.4679e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [60][240/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.6021e-02 (1.4849e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [60][250/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.5850e-03 (1.4584e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [60][260/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.8666e-03 (1.4391e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [60][270/391]	Time  0.136 ( 0.137)	Data  0.002 ( 0.005)	Loss 1.8108e-02 (1.4369e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [60][280/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.7203e-03 (1.4804e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [60][290/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.5578e-02 (1.4925e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [60][300/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.0755e-03 (1.4888e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [60][310/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.3804e-03 (1.4936e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [60][320/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.2473e-02 (1.5171e-02)	Acc@1  98.44 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [60][330/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.0235e-02 (1.5100e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [60][340/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.9227e-03 (1.5008e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [60][350/391]	Time  0.135 ( 0.137)	Data  0.002 ( 0.005)	Loss 5.8637e-03 (1.5011e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [60][360/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.6852e-02 (1.4953e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [60][370/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0425e-02 (1.5073e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [60][380/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.5600e-03 (1.4923e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [60][390/391]	Time  0.126 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.7937e-03 (1.5089e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
## e[60] optimizer.zero_grad (sum) time: 0.2827417850494385
## e[60]       loss.backward (sum) time: 18.51732873916626
## e[60]      optimizer.step (sum) time: 2.869629144668579
## epoch[60] training(only) time: 53.781707763671875
# Switched to evaluate mode...
Test: [  0/100]	Time  0.197 ( 0.197)	Loss 8.9327e-02 (8.9327e-02)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.045 ( 0.057)	Loss 4.4637e-01 (2.8632e-01)	Acc@1  90.00 ( 93.27)	Acc@5  99.00 ( 99.73)
Test: [ 20/100]	Time  0.042 ( 0.051)	Loss 4.5136e-01 (3.6615e-01)	Acc@1  91.00 ( 91.95)	Acc@5  99.00 ( 99.71)
Test: [ 30/100]	Time  0.040 ( 0.048)	Loss 2.8378e-01 (3.8792e-01)	Acc@1  93.00 ( 92.06)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.045 ( 0.047)	Loss 5.1251e-01 (3.9809e-01)	Acc@1  89.00 ( 91.90)	Acc@5  99.00 ( 99.73)
Test: [ 50/100]	Time  0.043 ( 0.046)	Loss 1.5406e-01 (3.9765e-01)	Acc@1  95.00 ( 91.90)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.043 ( 0.046)	Loss 3.5994e-01 (3.8328e-01)	Acc@1  94.00 ( 92.07)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.042 ( 0.045)	Loss 5.4696e-01 (3.7983e-01)	Acc@1  90.00 ( 92.11)	Acc@5 100.00 ( 99.82)
Test: [ 80/100]	Time  0.046 ( 0.045)	Loss 1.6887e-01 (3.8315e-01)	Acc@1  95.00 ( 92.14)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.043 ( 0.045)	Loss 1.9661e-01 (3.7590e-01)	Acc@1  95.00 ( 92.21)	Acc@5 100.00 ( 99.82)
 * Acc@1 92.210 Acc@5 99.810
### epoch[60] execution time: 58.40081524848938
EPOCH 61
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [61][  0/391]	Time  0.288 ( 0.288)	Data  0.169 ( 0.169)	Loss 3.5951e-03 (3.5951e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [61][ 10/391]	Time  0.136 ( 0.150)	Data  0.001 ( 0.019)	Loss 1.8217e-03 (1.0670e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [61][ 20/391]	Time  0.138 ( 0.143)	Data  0.001 ( 0.012)	Loss 6.5355e-03 (1.1191e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [61][ 30/391]	Time  0.140 ( 0.141)	Data  0.001 ( 0.010)	Loss 1.8963e-02 (1.3292e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [61][ 40/391]	Time  0.140 ( 0.140)	Data  0.001 ( 0.008)	Loss 4.7776e-03 (1.3189e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [61][ 50/391]	Time  0.133 ( 0.139)	Data  0.002 ( 0.007)	Loss 7.1590e-03 (1.2606e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [61][ 60/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.007)	Loss 1.2645e-02 (1.2730e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [61][ 70/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.007)	Loss 1.6172e-02 (1.2945e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [61][ 80/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 8.9803e-03 (1.2781e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [61][ 90/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.9880e-02 (1.3298e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [61][100/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 7.4193e-04 (1.2907e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [61][110/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.8014e-02 (1.3415e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [61][120/391]	Time  0.131 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.3911e-03 (1.3161e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [61][130/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.6774e-03 (1.3286e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [61][140/391]	Time  0.130 ( 0.138)	Data  0.001 ( 0.005)	Loss 9.1286e-03 (1.3140e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [61][150/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.2850e-02 (1.3120e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [61][160/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.005)	Loss 9.4301e-03 (1.3009e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [61][170/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.4793e-02 (1.3385e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [61][180/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.2159e-02 (1.3129e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [61][190/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.0331e-03 (1.3102e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [61][200/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.2180e-02 (1.3161e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [61][210/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.8251e-02 (1.3118e-02)	Acc@1  98.44 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [61][220/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.6169e-02 (1.3068e-02)	Acc@1  98.44 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [61][230/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.3905e-03 (1.3136e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [61][240/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0725e-02 (1.3115e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [61][250/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.5838e-02 (1.3104e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [61][260/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3936e-02 (1.3026e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [61][270/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.5799e-03 (1.2911e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [61][280/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.6413e-03 (1.2735e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [61][290/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.7193e-03 (1.2729e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [61][300/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.9643e-03 (1.2975e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [61][310/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.2402e-03 (1.3125e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [61][320/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.4844e-03 (1.3063e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [61][330/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.6899e-03 (1.3034e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [61][340/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.4370e-03 (1.2971e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [61][350/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.7555e-03 (1.3228e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [61][360/391]	Time  0.130 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.3561e-02 (1.3268e-02)	Acc@1  98.44 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [61][370/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.8396e-02 (1.3341e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [61][380/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.9587e-03 (1.3346e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [61][390/391]	Time  0.127 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.7076e-03 (1.3243e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
## e[61] optimizer.zero_grad (sum) time: 0.28691792488098145
## e[61]       loss.backward (sum) time: 18.523869037628174
## e[61]      optimizer.step (sum) time: 2.8867697715759277
## epoch[61] training(only) time: 53.76732778549194
# Switched to evaluate mode...
Test: [  0/100]	Time  0.199 ( 0.199)	Loss 1.0131e-01 (1.0131e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.059)	Loss 4.2164e-01 (2.9064e-01)	Acc@1  91.00 ( 93.64)	Acc@5  99.00 ( 99.73)
Test: [ 20/100]	Time  0.042 ( 0.052)	Loss 4.6871e-01 (3.7557e-01)	Acc@1  90.00 ( 92.24)	Acc@5  99.00 ( 99.76)
Test: [ 30/100]	Time  0.041 ( 0.049)	Loss 2.7269e-01 (3.9188e-01)	Acc@1  93.00 ( 92.35)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.044 ( 0.047)	Loss 4.6343e-01 (3.9936e-01)	Acc@1  90.00 ( 92.10)	Acc@5  99.00 ( 99.73)
Test: [ 50/100]	Time  0.046 ( 0.047)	Loss 1.3600e-01 (3.9557e-01)	Acc@1  96.00 ( 92.18)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.042 ( 0.046)	Loss 3.7571e-01 (3.8193e-01)	Acc@1  95.00 ( 92.28)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.043 ( 0.046)	Loss 5.1949e-01 (3.7728e-01)	Acc@1  89.00 ( 92.37)	Acc@5 100.00 ( 99.80)
Test: [ 80/100]	Time  0.046 ( 0.046)	Loss 1.8138e-01 (3.8029e-01)	Acc@1  95.00 ( 92.41)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.044 ( 0.045)	Loss 2.1424e-01 (3.7288e-01)	Acc@1  95.00 ( 92.47)	Acc@5 100.00 ( 99.80)
 * Acc@1 92.490 Acc@5 99.790
### epoch[61] execution time: 58.391748666763306
EPOCH 62
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [62][  0/391]	Time  0.303 ( 0.303)	Data  0.175 ( 0.175)	Loss 3.6038e-03 (3.6038e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [62][ 10/391]	Time  0.135 ( 0.151)	Data  0.001 ( 0.020)	Loss 8.7178e-03 (8.1872e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [62][ 20/391]	Time  0.135 ( 0.143)	Data  0.001 ( 0.012)	Loss 3.9897e-03 (1.3287e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [62][ 30/391]	Time  0.135 ( 0.141)	Data  0.002 ( 0.010)	Loss 2.2982e-03 (1.3242e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [62][ 40/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.008)	Loss 1.2308e-02 (1.4934e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [62][ 50/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.008)	Loss 8.9642e-03 (1.3710e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [62][ 60/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.007)	Loss 1.0881e-02 (1.3966e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [62][ 70/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.007)	Loss 1.5019e-02 (1.4522e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [62][ 80/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.007)	Loss 2.2342e-02 (1.4547e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [62][ 90/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 5.9805e-03 (1.3698e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [62][100/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.4132e-03 (1.3662e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [62][110/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.1903e-03 (1.4025e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [62][120/391]	Time  0.128 ( 0.138)	Data  0.002 ( 0.006)	Loss 4.4122e-03 (1.3482e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [62][130/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.006)	Loss 2.8198e-03 (1.3692e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [62][140/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.006)	Loss 2.9896e-02 (1.3937e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [62][150/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.006)	Loss 2.9053e-02 (1.3794e-02)	Acc@1  98.44 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [62][160/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.006)	Loss 5.3679e-02 (1.3994e-02)	Acc@1  98.44 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [62][170/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0153e-02 (1.3730e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [62][180/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.1560e-03 (1.3333e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [62][190/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.5004e-02 (1.3047e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [62][200/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.8233e-02 (1.3527e-02)	Acc@1  98.44 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [62][210/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.4167e-03 (1.3558e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [62][220/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.8062e-03 (1.3367e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [62][230/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.1838e-02 (1.3065e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [62][240/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.4105e-03 (1.3023e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [62][250/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.9693e-02 (1.2997e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [62][260/391]	Time  0.132 ( 0.137)	Data  0.002 ( 0.005)	Loss 1.3297e-02 (1.2890e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [62][270/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3895e-02 (1.3183e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [62][280/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.6822e-03 (1.3190e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [62][290/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.5444e-03 (1.3202e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [62][300/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.9835e-03 (1.3076e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [62][310/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.0993e-02 (1.3243e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [62][320/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.1836e-03 (1.3053e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [62][330/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.8098e-03 (1.2972e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [62][340/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.3566e-03 (1.3098e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [62][350/391]	Time  0.130 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.8500e-03 (1.2995e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [62][360/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.5216e-02 (1.2927e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [62][370/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3002e-02 (1.2782e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [62][380/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.1223e-03 (1.2672e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [62][390/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.5390e-02 (1.2708e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
## e[62] optimizer.zero_grad (sum) time: 0.2879652976989746
## e[62]       loss.backward (sum) time: 18.48467755317688
## e[62]      optimizer.step (sum) time: 2.898467779159546
## epoch[62] training(only) time: 53.67310905456543
# Switched to evaluate mode...
Test: [  0/100]	Time  0.202 ( 0.202)	Loss 1.1233e-01 (1.1233e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.044 ( 0.058)	Loss 4.1459e-01 (2.9199e-01)	Acc@1  92.00 ( 93.09)	Acc@5  99.00 ( 99.73)
Test: [ 20/100]	Time  0.043 ( 0.051)	Loss 4.7997e-01 (3.7519e-01)	Acc@1  88.00 ( 91.95)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.049 ( 0.049)	Loss 2.6939e-01 (3.9276e-01)	Acc@1  93.00 ( 92.10)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.042 ( 0.048)	Loss 4.7084e-01 (4.0000e-01)	Acc@1  89.00 ( 91.93)	Acc@5  99.00 ( 99.76)
Test: [ 50/100]	Time  0.044 ( 0.047)	Loss 1.1755e-01 (3.9586e-01)	Acc@1  97.00 ( 92.00)	Acc@5 100.00 ( 99.80)
Test: [ 60/100]	Time  0.045 ( 0.046)	Loss 3.8738e-01 (3.8222e-01)	Acc@1  96.00 ( 92.23)	Acc@5 100.00 ( 99.82)
Test: [ 70/100]	Time  0.041 ( 0.046)	Loss 5.4602e-01 (3.7862e-01)	Acc@1  89.00 ( 92.28)	Acc@5 100.00 ( 99.83)
Test: [ 80/100]	Time  0.043 ( 0.046)	Loss 1.7455e-01 (3.8113e-01)	Acc@1  95.00 ( 92.31)	Acc@5 100.00 ( 99.81)
Test: [ 90/100]	Time  0.047 ( 0.045)	Loss 2.0565e-01 (3.7440e-01)	Acc@1  95.00 ( 92.37)	Acc@5 100.00 ( 99.84)
 * Acc@1 92.410 Acc@5 99.820
### epoch[62] execution time: 58.28508138656616
EPOCH 63
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [63][  0/391]	Time  0.300 ( 0.300)	Data  0.183 ( 0.183)	Loss 9.5707e-03 (9.5707e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [63][ 10/391]	Time  0.136 ( 0.151)	Data  0.001 ( 0.020)	Loss 9.2313e-04 (9.8151e-03)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [63][ 20/391]	Time  0.137 ( 0.144)	Data  0.001 ( 0.013)	Loss 2.2718e-02 (1.2474e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [63][ 30/391]	Time  0.130 ( 0.141)	Data  0.001 ( 0.010)	Loss 2.5166e-02 (1.2658e-02)	Acc@1  98.44 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [63][ 40/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.009)	Loss 1.1846e-02 (1.2744e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [63][ 50/391]	Time  0.130 ( 0.139)	Data  0.001 ( 0.008)	Loss 3.4737e-03 (1.2057e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [63][ 60/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.007)	Loss 4.0712e-03 (1.1060e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [63][ 70/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.007)	Loss 4.3990e-03 (1.1032e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [63][ 80/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.007)	Loss 5.8758e-03 (1.1206e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [63][ 90/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 5.8244e-03 (1.1315e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [63][100/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.7449e-02 (1.1576e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [63][110/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.4117e-03 (1.1373e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [63][120/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.006)	Loss 3.1027e-03 (1.1039e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [63][130/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.006)	Loss 3.9568e-03 (1.0962e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [63][140/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.006)	Loss 1.8788e-02 (1.0872e-02)	Acc@1  98.44 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [63][150/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.006)	Loss 6.5209e-03 (1.0862e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [63][160/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.006)	Loss 2.0354e-02 (1.1095e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [63][170/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.4937e-02 (1.1101e-02)	Acc@1  98.44 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [63][180/391]	Time  0.142 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.7165e-03 (1.0805e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [63][190/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.4765e-02 (1.0784e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [63][200/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.4613e-03 (1.0563e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [63][210/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.5065e-02 (1.0626e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [63][220/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.2693e-03 (1.0632e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [63][230/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.3117e-03 (1.0656e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [63][240/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.5662e-03 (1.0594e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [63][250/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.0724e-02 (1.0629e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [63][260/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.5592e-02 (1.1023e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [63][270/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.1821e-03 (1.1019e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [63][280/391]	Time  0.139 ( 0.137)	Data  0.002 ( 0.005)	Loss 3.5021e-03 (1.1257e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [63][290/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.1859e-03 (1.1062e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [63][300/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1612e-02 (1.1109e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [63][310/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.9796e-03 (1.1303e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [63][320/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.4798e-03 (1.1274e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [63][330/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.4929e-02 (1.1308e-02)	Acc@1  98.44 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [63][340/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.8369e-03 (1.1336e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [63][350/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.0983e-02 (1.1341e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [63][360/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.1724e-03 (1.1333e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [63][370/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.4627e-03 (1.1373e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [63][380/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.4970e-02 (1.1517e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [63][390/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.2881e-02 (1.1460e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
## e[63] optimizer.zero_grad (sum) time: 0.28290700912475586
## e[63]       loss.backward (sum) time: 18.49810481071472
## e[63]      optimizer.step (sum) time: 2.8855137825012207
## epoch[63] training(only) time: 53.78815841674805
# Switched to evaluate mode...
Test: [  0/100]	Time  0.201 ( 0.201)	Loss 1.2742e-01 (1.2742e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.043 ( 0.058)	Loss 4.1845e-01 (2.8868e-01)	Acc@1  92.00 ( 93.64)	Acc@5  99.00 ( 99.73)
Test: [ 20/100]	Time  0.045 ( 0.051)	Loss 4.8467e-01 (3.7849e-01)	Acc@1  88.00 ( 92.10)	Acc@5  99.00 ( 99.76)
Test: [ 30/100]	Time  0.041 ( 0.049)	Loss 2.5719e-01 (3.9648e-01)	Acc@1  94.00 ( 92.23)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.042 ( 0.048)	Loss 4.3505e-01 (4.0345e-01)	Acc@1  90.00 ( 92.07)	Acc@5  99.00 ( 99.71)
Test: [ 50/100]	Time  0.041 ( 0.047)	Loss 1.4976e-01 (3.9819e-01)	Acc@1  96.00 ( 92.14)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.041 ( 0.046)	Loss 3.7452e-01 (3.8498e-01)	Acc@1  96.00 ( 92.33)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.045 ( 0.046)	Loss 5.3835e-01 (3.7863e-01)	Acc@1  89.00 ( 92.39)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.044 ( 0.046)	Loss 2.0022e-01 (3.8134e-01)	Acc@1  95.00 ( 92.46)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.044 ( 0.045)	Loss 2.5578e-01 (3.7357e-01)	Acc@1  95.00 ( 92.54)	Acc@5 100.00 ( 99.77)
 * Acc@1 92.580 Acc@5 99.760
### epoch[63] execution time: 58.4200975894928
EPOCH 64
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [64][  0/391]	Time  0.302 ( 0.302)	Data  0.167 ( 0.167)	Loss 1.8159e-02 (1.8159e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [64][ 10/391]	Time  0.133 ( 0.151)	Data  0.001 ( 0.019)	Loss 7.5488e-03 (8.8494e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [64][ 20/391]	Time  0.135 ( 0.144)	Data  0.001 ( 0.012)	Loss 5.6874e-03 (1.2594e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [64][ 30/391]	Time  0.138 ( 0.141)	Data  0.001 ( 0.010)	Loss 1.0185e-02 (1.2272e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [64][ 40/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.008)	Loss 1.0374e-02 (1.2137e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [64][ 50/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.008)	Loss 1.0548e-02 (1.2289e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [64][ 60/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.007)	Loss 3.1239e-02 (1.2431e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [64][ 70/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.007)	Loss 2.7347e-02 (1.2466e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [64][ 80/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.2198e-02 (1.1724e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [64][ 90/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.7622e-03 (1.1227e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [64][100/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.6463e-02 (1.0800e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [64][110/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.4345e-03 (1.0949e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [64][120/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 6.2235e-03 (1.0941e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [64][130/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.3941e-02 (1.0854e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [64][140/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.8466e-03 (1.0533e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [64][150/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 9.6626e-03 (1.0596e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [64][160/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.3262e-02 (1.0803e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [64][170/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 6.7246e-03 (1.0614e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [64][180/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.8071e-03 (1.0358e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [64][190/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.1980e-02 (1.0651e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [64][200/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.005)	Loss 7.8339e-03 (1.0738e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [64][210/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.3353e-03 (1.0738e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [64][220/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.4809e-03 (1.0976e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [64][230/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.8360e-02 (1.1056e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [64][240/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.2889e-03 (1.0815e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [64][250/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0672e-02 (1.0913e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [64][260/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.8681e-03 (1.0885e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [64][270/391]	Time  0.141 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.2590e-03 (1.0728e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [64][280/391]	Time  0.134 ( 0.137)	Data  0.002 ( 0.005)	Loss 3.7302e-03 (1.0586e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [64][290/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.8360e-02 (1.0622e-02)	Acc@1  98.44 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [64][300/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.1770e-03 (1.0517e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [64][310/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.8017e-02 (1.0606e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [64][320/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.5820e-03 (1.0620e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [64][330/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.2826e-03 (1.0559e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [64][340/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.7069e-02 (1.0549e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [64][350/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.0706e-03 (1.0742e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [64][360/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.1409e-02 (1.0869e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [64][370/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.3662e-03 (1.0937e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [64][380/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.6152e-03 (1.0926e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [64][390/391]	Time  0.125 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.3521e-03 (1.0895e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
## e[64] optimizer.zero_grad (sum) time: 0.28589844703674316
## e[64]       loss.backward (sum) time: 18.539727449417114
## e[64]      optimizer.step (sum) time: 2.8994300365448
## epoch[64] training(only) time: 53.7934684753418
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 1.2749e-01 (1.2749e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.043 ( 0.058)	Loss 4.3553e-01 (2.9211e-01)	Acc@1  92.00 ( 93.55)	Acc@5  99.00 ( 99.73)
Test: [ 20/100]	Time  0.044 ( 0.051)	Loss 4.9659e-01 (3.7850e-01)	Acc@1  89.00 ( 92.19)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.045 ( 0.049)	Loss 2.6079e-01 (3.9589e-01)	Acc@1  94.00 ( 92.35)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.043 ( 0.048)	Loss 4.5102e-01 (4.0140e-01)	Acc@1  90.00 ( 92.24)	Acc@5  99.00 ( 99.76)
Test: [ 50/100]	Time  0.044 ( 0.047)	Loss 1.3297e-01 (3.9723e-01)	Acc@1  97.00 ( 92.24)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.045 ( 0.046)	Loss 3.9098e-01 (3.8284e-01)	Acc@1  96.00 ( 92.46)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.042 ( 0.046)	Loss 5.3271e-01 (3.7706e-01)	Acc@1  89.00 ( 92.54)	Acc@5 100.00 ( 99.80)
Test: [ 80/100]	Time  0.044 ( 0.046)	Loss 1.7496e-01 (3.7954e-01)	Acc@1  96.00 ( 92.57)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.044 ( 0.045)	Loss 2.3047e-01 (3.7149e-01)	Acc@1  94.00 ( 92.63)	Acc@5 100.00 ( 99.80)
 * Acc@1 92.670 Acc@5 99.790
### epoch[64] execution time: 58.42324662208557
EPOCH 65
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [65][  0/391]	Time  0.301 ( 0.301)	Data  0.180 ( 0.180)	Loss 4.2908e-03 (4.2908e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [65][ 10/391]	Time  0.131 ( 0.150)	Data  0.001 ( 0.020)	Loss 1.4206e-02 (7.1777e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [65][ 20/391]	Time  0.134 ( 0.143)	Data  0.001 ( 0.013)	Loss 2.8372e-02 (1.0309e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [65][ 30/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.010)	Loss 6.3213e-03 (9.7894e-03)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [65][ 40/391]	Time  0.132 ( 0.139)	Data  0.001 ( 0.009)	Loss 3.5795e-03 (9.3773e-03)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [65][ 50/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.008)	Loss 2.5424e-02 (1.0818e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [65][ 60/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.007)	Loss 1.6566e-02 (9.9779e-03)	Acc@1  98.44 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [65][ 70/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.007)	Loss 2.5859e-02 (1.0094e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [65][ 80/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.007)	Loss 1.5121e-02 (9.9010e-03)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [65][ 90/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.4743e-02 (1.0518e-02)	Acc@1  98.44 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [65][100/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.2284e-02 (1.0758e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [65][110/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.006)	Loss 5.9808e-03 (1.0850e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [65][120/391]	Time  0.129 ( 0.137)	Data  0.001 ( 0.006)	Loss 8.4337e-03 (1.0967e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [65][130/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.006)	Loss 1.8590e-02 (1.0615e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [65][140/391]	Time  0.130 ( 0.137)	Data  0.001 ( 0.006)	Loss 5.3657e-03 (1.0205e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [65][150/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.006)	Loss 9.8207e-03 (1.0103e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [65][160/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.006)	Loss 7.9876e-03 (1.0296e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [65][170/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.8858e-03 (1.0299e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [65][180/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.9826e-03 (1.0129e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [65][190/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.8027e-03 (1.0039e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [65][200/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.2405e-03 (1.0270e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [65][210/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0261e-02 (1.0603e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [65][220/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3475e-02 (1.0696e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [65][230/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.9138e-03 (1.1010e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [65][240/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.2008e-03 (1.0826e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [65][250/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.4104e-03 (1.0791e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [65][260/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.3939e-03 (1.0704e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [65][270/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.4554e-02 (1.0808e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [65][280/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.5126e-03 (1.0702e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [65][290/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.6287e-03 (1.0877e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [65][300/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.4585e-03 (1.0803e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [65][310/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.5372e-03 (1.0813e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [65][320/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.5016e-02 (1.0788e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [65][330/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.9109e-03 (1.0677e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [65][340/391]	Time  0.135 ( 0.137)	Data  0.002 ( 0.005)	Loss 8.3528e-03 (1.0714e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [65][350/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.5708e-02 (1.0657e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [65][360/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.4644e-03 (1.0889e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [65][370/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.2699e-02 (1.0943e-02)	Acc@1  98.44 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [65][380/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.4921e-03 (1.0909e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [65][390/391]	Time  0.129 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.4700e-02 (1.0864e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
## e[65] optimizer.zero_grad (sum) time: 0.2879068851470947
## e[65]       loss.backward (sum) time: 18.467475652694702
## e[65]      optimizer.step (sum) time: 2.8909096717834473
## epoch[65] training(only) time: 53.6426796913147
# Switched to evaluate mode...
Test: [  0/100]	Time  0.204 ( 0.204)	Loss 1.3778e-01 (1.3778e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.058)	Loss 4.4412e-01 (2.9453e-01)	Acc@1  92.00 ( 93.00)	Acc@5  99.00 ( 99.82)
Test: [ 20/100]	Time  0.045 ( 0.051)	Loss 4.9793e-01 (3.8025e-01)	Acc@1  89.00 ( 91.90)	Acc@5  99.00 ( 99.81)
Test: [ 30/100]	Time  0.043 ( 0.049)	Loss 2.6365e-01 (3.9922e-01)	Acc@1  94.00 ( 92.16)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.041 ( 0.048)	Loss 4.3850e-01 (4.0356e-01)	Acc@1  90.00 ( 92.05)	Acc@5  99.00 ( 99.76)
Test: [ 50/100]	Time  0.042 ( 0.047)	Loss 1.6017e-01 (3.9930e-01)	Acc@1  96.00 ( 92.08)	Acc@5 100.00 ( 99.80)
Test: [ 60/100]	Time  0.043 ( 0.046)	Loss 3.7735e-01 (3.8424e-01)	Acc@1  96.00 ( 92.31)	Acc@5 100.00 ( 99.82)
Test: [ 70/100]	Time  0.044 ( 0.046)	Loss 5.0195e-01 (3.7699e-01)	Acc@1  89.00 ( 92.41)	Acc@5 100.00 ( 99.82)
Test: [ 80/100]	Time  0.049 ( 0.046)	Loss 1.7451e-01 (3.7920e-01)	Acc@1  96.00 ( 92.46)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.042 ( 0.045)	Loss 2.4519e-01 (3.7156e-01)	Acc@1  94.00 ( 92.51)	Acc@5 100.00 ( 99.81)
 * Acc@1 92.530 Acc@5 99.800
### epoch[65] execution time: 58.32188630104065
EPOCH 66
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [66][  0/391]	Time  0.291 ( 0.291)	Data  0.176 ( 0.176)	Loss 2.0644e-02 (2.0644e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [66][ 10/391]	Time  0.132 ( 0.150)	Data  0.001 ( 0.020)	Loss 9.6110e-03 (1.2129e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [66][ 20/391]	Time  0.133 ( 0.143)	Data  0.001 ( 0.012)	Loss 7.8575e-03 (1.1249e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [66][ 30/391]	Time  0.131 ( 0.141)	Data  0.001 ( 0.010)	Loss 3.8727e-03 (9.8384e-03)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [66][ 40/391]	Time  0.131 ( 0.140)	Data  0.001 ( 0.009)	Loss 2.0656e-02 (1.0324e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [66][ 50/391]	Time  0.132 ( 0.139)	Data  0.001 ( 0.008)	Loss 3.2588e-03 (1.1267e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [66][ 60/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.007)	Loss 3.1994e-03 (1.0842e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [66][ 70/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.007)	Loss 1.7106e-02 (1.0960e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [66][ 80/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.007)	Loss 1.6231e-02 (1.0570e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [66][ 90/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.0522e-02 (1.0346e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [66][100/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 8.7472e-04 (9.9958e-03)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [66][110/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 5.1770e-03 (1.0074e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [66][120/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.8488e-02 (1.0271e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [66][130/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 5.3744e-03 (9.9916e-03)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [66][140/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 6.4371e-03 (1.0082e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [66][150/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.0801e-03 (9.8534e-03)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [66][160/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.4533e-02 (9.8880e-03)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [66][170/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.9537e-03 (9.8383e-03)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [66][180/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.0071e-03 (9.6746e-03)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [66][190/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.0064e-03 (9.6592e-03)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [66][200/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3663e-02 (9.5152e-03)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [66][210/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.1881e-03 (9.5642e-03)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [66][220/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.6314e-02 (1.0005e-02)	Acc@1  98.44 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [66][230/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.8177e-03 (1.0248e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [66][240/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.0473e-03 (1.0352e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [66][250/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.5954e-03 (1.0188e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [66][260/391]	Time  0.130 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0709e-02 (1.0213e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [66][270/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.9748e-02 (1.0304e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [66][280/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.0309e-03 (1.0297e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [66][290/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.3825e-03 (1.0154e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [66][300/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.2437e-03 (1.0080e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [66][310/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.9243e-02 (1.0128e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [66][320/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.0537e-03 (1.0155e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [66][330/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.6705e-03 (1.0063e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [66][340/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.4339e-03 (1.0108e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [66][350/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.4982e-02 (1.0177e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [66][360/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.4674e-02 (1.0242e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [66][370/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.6346e-02 (1.0250e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [66][380/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.0446e-02 (1.0223e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [66][390/391]	Time  0.124 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3723e-02 (1.0106e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
## e[66] optimizer.zero_grad (sum) time: 0.287994384765625
## e[66]       loss.backward (sum) time: 18.548194885253906
## e[66]      optimizer.step (sum) time: 2.921978712081909
## epoch[66] training(only) time: 53.66463613510132
# Switched to evaluate mode...
Test: [  0/100]	Time  0.199 ( 0.199)	Loss 1.3896e-01 (1.3896e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.043 ( 0.058)	Loss 3.9869e-01 (2.8989e-01)	Acc@1  93.00 ( 93.36)	Acc@5  99.00 ( 99.73)
Test: [ 20/100]	Time  0.044 ( 0.051)	Loss 4.8795e-01 (3.7967e-01)	Acc@1  90.00 ( 92.10)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.043 ( 0.049)	Loss 2.7998e-01 (3.9811e-01)	Acc@1  93.00 ( 92.19)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.046 ( 0.047)	Loss 4.6478e-01 (4.0379e-01)	Acc@1  90.00 ( 92.00)	Acc@5  99.00 ( 99.76)
Test: [ 50/100]	Time  0.044 ( 0.047)	Loss 1.4570e-01 (3.9753e-01)	Acc@1  96.00 ( 92.12)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.043 ( 0.046)	Loss 4.0682e-01 (3.8513e-01)	Acc@1  94.00 ( 92.30)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.042 ( 0.046)	Loss 5.5531e-01 (3.7994e-01)	Acc@1  88.00 ( 92.34)	Acc@5 100.00 ( 99.80)
Test: [ 80/100]	Time  0.044 ( 0.045)	Loss 1.9197e-01 (3.8271e-01)	Acc@1  95.00 ( 92.41)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.044 ( 0.045)	Loss 2.5278e-01 (3.7445e-01)	Acc@1  94.00 ( 92.47)	Acc@5 100.00 ( 99.78)
 * Acc@1 92.460 Acc@5 99.760
### epoch[66] execution time: 58.28741955757141
EPOCH 67
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [67][  0/391]	Time  0.288 ( 0.288)	Data  0.175 ( 0.175)	Loss 7.6271e-03 (7.6271e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [67][ 10/391]	Time  0.135 ( 0.149)	Data  0.001 ( 0.020)	Loss 1.2236e-02 (1.0271e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [67][ 20/391]	Time  0.134 ( 0.143)	Data  0.001 ( 0.012)	Loss 6.7038e-03 (1.0668e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [67][ 30/391]	Time  0.131 ( 0.140)	Data  0.001 ( 0.010)	Loss 4.7463e-03 (1.0392e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [67][ 40/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.009)	Loss 7.9831e-03 (1.0036e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [67][ 50/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.008)	Loss 1.0584e-02 (1.0380e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [67][ 60/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.007)	Loss 3.2231e-03 (1.0160e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [67][ 70/391]	Time  0.131 ( 0.138)	Data  0.001 ( 0.007)	Loss 4.8398e-02 (1.0584e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [67][ 80/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.007)	Loss 1.4079e-02 (1.0826e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [67][ 90/391]	Time  0.131 ( 0.138)	Data  0.002 ( 0.006)	Loss 1.8083e-02 (1.1209e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [67][100/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.8890e-03 (1.0846e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [67][110/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.9124e-03 (1.1247e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [67][120/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.3332e-03 (1.1879e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 ( 99.99)
Epoch: [67][130/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.006)	Loss 1.6256e-03 (1.1833e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 ( 99.99)
Epoch: [67][140/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.006)	Loss 5.1684e-03 (1.1468e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 ( 99.99)
Epoch: [67][150/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.006)	Loss 2.2576e-02 (1.1402e-02)	Acc@1  98.44 ( 99.65)	Acc@5 100.00 ( 99.99)
Epoch: [67][160/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.006)	Loss 1.9376e-02 (1.1225e-02)	Acc@1  98.44 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [67][170/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.4895e-03 (1.1100e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [67][180/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.8276e-03 (1.0939e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [67][190/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.0963e-03 (1.1153e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [67][200/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.5725e-02 (1.0989e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [67][210/391]	Time  0.130 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.6687e-02 (1.0907e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [67][220/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.5390e-03 (1.0808e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [67][230/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.8170e-03 (1.0673e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [67][240/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.6917e-03 (1.0580e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [67][250/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.5496e-02 (1.0478e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [67][260/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.9411e-03 (1.0569e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [67][270/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.6413e-03 (1.0495e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [67][280/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.2353e-03 (1.0550e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [67][290/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.0325e-03 (1.0477e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [67][300/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.0528e-03 (1.0468e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [67][310/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.3125e-03 (1.0361e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [67][320/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.5223e-03 (1.0353e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [67][330/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.2119e-02 (1.0384e-02)	Acc@1  98.44 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [67][340/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.6503e-03 (1.0383e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [67][350/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.5246e-03 (1.0303e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [67][360/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.4693e-03 (1.0318e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [67][370/391]	Time  0.136 ( 0.137)	Data  0.002 ( 0.005)	Loss 6.9362e-03 (1.0321e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [67][380/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.3061e-03 (1.0353e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [67][390/391]	Time  0.128 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.0132e-03 (1.0215e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
## e[67] optimizer.zero_grad (sum) time: 0.284649133682251
## e[67]       loss.backward (sum) time: 18.483006477355957
## e[67]      optimizer.step (sum) time: 2.892510175704956
## epoch[67] training(only) time: 53.62409281730652
# Switched to evaluate mode...
Test: [  0/100]	Time  0.201 ( 0.201)	Loss 1.5950e-01 (1.5950e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.042 ( 0.058)	Loss 4.1491e-01 (2.9399e-01)	Acc@1  93.00 ( 93.45)	Acc@5  99.00 ( 99.73)
Test: [ 20/100]	Time  0.044 ( 0.051)	Loss 4.8095e-01 (3.7959e-01)	Acc@1  91.00 ( 92.38)	Acc@5  99.00 ( 99.76)
Test: [ 30/100]	Time  0.045 ( 0.049)	Loss 2.7348e-01 (3.9881e-01)	Acc@1  95.00 ( 92.42)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.043 ( 0.048)	Loss 4.6431e-01 (4.0373e-01)	Acc@1  90.00 ( 92.15)	Acc@5  99.00 ( 99.68)
Test: [ 50/100]	Time  0.042 ( 0.047)	Loss 1.7876e-01 (3.9636e-01)	Acc@1  96.00 ( 92.25)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.044 ( 0.047)	Loss 3.9321e-01 (3.8443e-01)	Acc@1  95.00 ( 92.48)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.045 ( 0.046)	Loss 5.7806e-01 (3.8035e-01)	Acc@1  88.00 ( 92.52)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.045 ( 0.046)	Loss 2.0635e-01 (3.8232e-01)	Acc@1  94.00 ( 92.54)	Acc@5 100.00 ( 99.73)
Test: [ 90/100]	Time  0.044 ( 0.046)	Loss 2.6480e-01 (3.7398e-01)	Acc@1  94.00 ( 92.59)	Acc@5 100.00 ( 99.76)
 * Acc@1 92.610 Acc@5 99.750
### epoch[67] execution time: 58.28627347946167
EPOCH 68
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [68][  0/391]	Time  0.299 ( 0.299)	Data  0.177 ( 0.177)	Loss 1.3638e-02 (1.3638e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [68][ 10/391]	Time  0.135 ( 0.150)	Data  0.001 ( 0.020)	Loss 8.2708e-03 (8.4917e-03)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [68][ 20/391]	Time  0.137 ( 0.143)	Data  0.001 ( 0.012)	Loss 3.6757e-03 (1.0038e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [68][ 30/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.010)	Loss 2.5981e-03 (1.0316e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [68][ 40/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.008)	Loss 1.9159e-03 (1.0171e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [68][ 50/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.008)	Loss 4.2967e-03 (1.0578e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [68][ 60/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.007)	Loss 2.9615e-03 (9.9165e-03)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [68][ 70/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.007)	Loss 1.9530e-02 (9.5896e-03)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [68][ 80/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 9.0592e-03 (9.3590e-03)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [68][ 90/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.1681e-02 (9.7961e-03)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [68][100/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.1815e-03 (9.8641e-03)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [68][110/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.5025e-02 (9.7000e-03)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [68][120/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.5512e-02 (9.5491e-03)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [68][130/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.8280e-02 (9.5654e-03)	Acc@1  98.44 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [68][140/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.006)	Loss 2.4103e-03 (9.5322e-03)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [68][150/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.7415e-02 (9.5779e-03)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [68][160/391]	Time  0.130 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.5866e-03 (9.4247e-03)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [68][170/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.3963e-03 (9.3915e-03)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [68][180/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.5571e-02 (9.4228e-03)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [68][190/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.6306e-02 (9.4468e-03)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [68][200/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.4409e-03 (9.6252e-03)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [68][210/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0821e-02 (9.4997e-03)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [68][220/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.6638e-03 (9.3946e-03)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [68][230/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.1701e-03 (9.4667e-03)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [68][240/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.6574e-03 (9.4022e-03)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [68][250/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.5769e-03 (9.3634e-03)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [68][260/391]	Time  0.135 ( 0.137)	Data  0.002 ( 0.005)	Loss 7.6509e-03 (9.2350e-03)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [68][270/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.7396e-03 (9.1253e-03)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [68][280/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.6370e-03 (9.2038e-03)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [68][290/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.2008e-03 (9.2076e-03)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [68][300/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.5504e-02 (9.4942e-03)	Acc@1  97.66 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [68][310/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.8261e-03 (9.5664e-03)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [68][320/391]	Time  0.136 ( 0.137)	Data  0.002 ( 0.005)	Loss 7.5040e-04 (9.5498e-03)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [68][330/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.1321e-02 (9.5137e-03)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [68][340/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.4928e-03 (9.7446e-03)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [68][350/391]	Time  0.129 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1085e-02 (9.8173e-03)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [68][360/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.5844e-03 (9.6975e-03)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [68][370/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.5741e-03 (9.7177e-03)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [68][380/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.3367e-03 (9.6047e-03)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [68][390/391]	Time  0.128 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.0943e-03 (9.6401e-03)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
## e[68] optimizer.zero_grad (sum) time: 0.28694868087768555
## e[68]       loss.backward (sum) time: 18.545523405075073
## e[68]      optimizer.step (sum) time: 2.9106709957122803
## epoch[68] training(only) time: 53.662060260772705
# Switched to evaluate mode...
Test: [  0/100]	Time  0.207 ( 0.207)	Loss 1.4543e-01 (1.4543e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.043 ( 0.059)	Loss 4.1556e-01 (3.0085e-01)	Acc@1  93.00 ( 93.55)	Acc@5  99.00 ( 99.73)
Test: [ 20/100]	Time  0.046 ( 0.052)	Loss 5.1854e-01 (3.8420e-01)	Acc@1  87.00 ( 92.14)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.041 ( 0.049)	Loss 2.7856e-01 (4.0454e-01)	Acc@1  94.00 ( 92.23)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.046 ( 0.048)	Loss 4.8214e-01 (4.1113e-01)	Acc@1  90.00 ( 91.98)	Acc@5  99.00 ( 99.76)
Test: [ 50/100]	Time  0.042 ( 0.047)	Loss 1.7528e-01 (4.0367e-01)	Acc@1  96.00 ( 92.12)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.044 ( 0.046)	Loss 3.7203e-01 (3.8999e-01)	Acc@1  95.00 ( 92.30)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.044 ( 0.046)	Loss 6.1568e-01 (3.8598e-01)	Acc@1  88.00 ( 92.39)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.042 ( 0.046)	Loss 1.7559e-01 (3.8729e-01)	Acc@1  96.00 ( 92.48)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.043 ( 0.045)	Loss 2.1292e-01 (3.7943e-01)	Acc@1  94.00 ( 92.49)	Acc@5 100.00 ( 99.78)
 * Acc@1 92.520 Acc@5 99.770
### epoch[68] execution time: 58.29537606239319
EPOCH 69
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [69][  0/391]	Time  0.301 ( 0.301)	Data  0.180 ( 0.180)	Loss 4.3756e-03 (4.3756e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [69][ 10/391]	Time  0.135 ( 0.150)	Data  0.001 ( 0.020)	Loss 1.1025e-02 (7.2321e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [69][ 20/391]	Time  0.136 ( 0.144)	Data  0.001 ( 0.013)	Loss 2.0410e-02 (8.5887e-03)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [69][ 30/391]	Time  0.136 ( 0.141)	Data  0.001 ( 0.010)	Loss 2.5088e-03 (7.2103e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [69][ 40/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.009)	Loss 5.9632e-03 (7.2899e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [69][ 50/391]	Time  0.132 ( 0.139)	Data  0.002 ( 0.008)	Loss 1.8420e-02 (7.7262e-03)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [69][ 60/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.007)	Loss 4.1452e-03 (7.3732e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [69][ 70/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.007)	Loss 4.9128e-03 (7.6989e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [69][ 80/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.007)	Loss 1.0899e-02 (7.4445e-03)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [69][ 90/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.6017e-02 (7.6121e-03)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [69][100/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.8897e-03 (8.1755e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [69][110/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.006)	Loss 7.2977e-03 (8.4787e-03)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [69][120/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.006)	Loss 7.1159e-04 (8.4690e-03)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [69][130/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.006)	Loss 4.5489e-02 (8.5418e-03)	Acc@1  98.44 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [69][140/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.006)	Loss 5.8513e-03 (8.3125e-03)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [69][150/391]	Time  0.130 ( 0.137)	Data  0.001 ( 0.006)	Loss 7.6226e-03 (8.3847e-03)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [69][160/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.006)	Loss 1.1022e-02 (8.4002e-03)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [69][170/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1900e-02 (8.3570e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [69][180/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1766e-02 (8.4667e-03)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [69][190/391]	Time  0.126 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.8134e-02 (8.7237e-03)	Acc@1  96.88 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [69][200/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.7375e-02 (8.7540e-03)	Acc@1  98.44 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [69][210/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.8185e-03 (8.9740e-03)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [69][220/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.1353e-03 (8.8561e-03)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [69][230/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0292e-02 (8.8810e-03)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [69][240/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.8390e-03 (8.8022e-03)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [69][250/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.2810e-02 (8.8964e-03)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [69][260/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.7119e-03 (8.9464e-03)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [69][270/391]	Time  0.139 ( 0.137)	Data  0.002 ( 0.005)	Loss 3.1378e-03 (8.9584e-03)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [69][280/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.1512e-03 (8.8234e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [69][290/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.4571e-03 (8.8971e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [69][300/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.0258e-03 (8.8159e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [69][310/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.1899e-03 (8.8104e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [69][320/391]	Time  0.141 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.5199e-03 (8.7675e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [69][330/391]	Time  0.130 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.5680e-03 (8.7651e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [69][340/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.8181e-02 (8.7965e-03)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [69][350/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.5785e-03 (8.7708e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [69][360/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.9745e-03 (8.6818e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [69][370/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.6717e-02 (8.8526e-03)	Acc@1  98.44 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [69][380/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.4306e-03 (8.7783e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [69][390/391]	Time  0.127 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.4342e-02 (8.8406e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
## e[69] optimizer.zero_grad (sum) time: 0.29023170471191406
## e[69]       loss.backward (sum) time: 18.52998924255371
## e[69]      optimizer.step (sum) time: 2.942861795425415
## epoch[69] training(only) time: 53.763224363327026
# Switched to evaluate mode...
Test: [  0/100]	Time  0.203 ( 0.203)	Loss 1.5480e-01 (1.5480e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.059)	Loss 4.1649e-01 (3.0735e-01)	Acc@1  94.00 ( 93.36)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.044 ( 0.052)	Loss 4.9627e-01 (3.8631e-01)	Acc@1  90.00 ( 92.24)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.043 ( 0.049)	Loss 2.5627e-01 (4.0209e-01)	Acc@1  95.00 ( 92.29)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.042 ( 0.048)	Loss 4.7929e-01 (4.1076e-01)	Acc@1  90.00 ( 92.07)	Acc@5  99.00 ( 99.76)
Test: [ 50/100]	Time  0.042 ( 0.047)	Loss 1.7144e-01 (4.0281e-01)	Acc@1  96.00 ( 92.18)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.043 ( 0.046)	Loss 3.8683e-01 (3.8984e-01)	Acc@1  95.00 ( 92.38)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.042 ( 0.046)	Loss 5.7102e-01 (3.8497e-01)	Acc@1  88.00 ( 92.41)	Acc@5 100.00 ( 99.82)
Test: [ 80/100]	Time  0.045 ( 0.045)	Loss 1.7923e-01 (3.8528e-01)	Acc@1  96.00 ( 92.48)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.041 ( 0.045)	Loss 2.1980e-01 (3.7712e-01)	Acc@1  95.00 ( 92.55)	Acc@5 100.00 ( 99.80)
 * Acc@1 92.580 Acc@5 99.790
### epoch[69] execution time: 58.375680685043335
EPOCH 70
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [70][  0/391]	Time  0.286 ( 0.286)	Data  0.169 ( 0.169)	Loss 3.7548e-03 (3.7548e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [70][ 10/391]	Time  0.134 ( 0.149)	Data  0.001 ( 0.019)	Loss 2.5618e-02 (1.1040e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [70][ 20/391]	Time  0.131 ( 0.142)	Data  0.001 ( 0.012)	Loss 1.2327e-02 (1.2466e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [70][ 30/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.010)	Loss 5.6066e-03 (1.0519e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [70][ 40/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.008)	Loss 4.0753e-03 (1.0411e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [70][ 50/391]	Time  0.128 ( 0.138)	Data  0.001 ( 0.008)	Loss 5.5554e-03 (1.0002e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [70][ 60/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.007)	Loss 4.8310e-03 (1.0344e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [70][ 70/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.007)	Loss 2.9889e-03 (1.0068e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [70][ 80/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.6431e-02 (1.0914e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [70][ 90/391]	Time  0.129 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.3219e-02 (1.0846e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [70][100/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.006)	Loss 3.4466e-03 (1.0391e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [70][110/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.006)	Loss 4.3581e-03 (1.0245e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [70][120/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.006)	Loss 2.8899e-03 (9.9996e-03)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [70][130/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.006)	Loss 1.2348e-02 (9.8926e-03)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [70][140/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.006)	Loss 1.4621e-02 (9.8230e-03)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [70][150/391]	Time  0.134 ( 0.137)	Data  0.002 ( 0.006)	Loss 3.4890e-03 (9.4190e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [70][160/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.1286e-02 (9.5577e-03)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [70][170/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.5515e-03 (9.3954e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [70][180/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.6400e-03 (9.2393e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [70][190/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.2259e-03 (9.4274e-03)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [70][200/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.3239e-03 (9.1914e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [70][210/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.6509e-03 (9.3348e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [70][220/391]	Time  0.139 ( 0.137)	Data  0.002 ( 0.005)	Loss 4.9819e-03 (9.3441e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [70][230/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.5171e-03 (9.3089e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [70][240/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.3076e-03 (9.1673e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [70][250/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.5204e-03 (9.0354e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [70][260/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.8555e-03 (8.9577e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [70][270/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1240e-02 (8.8922e-03)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [70][280/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.7213e-02 (8.8318e-03)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [70][290/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.3398e-02 (8.9353e-03)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [70][300/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.6152e-03 (8.8196e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [70][310/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.2642e-02 (8.8512e-03)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [70][320/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.7313e-03 (8.9510e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [70][330/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.3142e-03 (8.9930e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [70][340/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.7507e-02 (9.1944e-03)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [70][350/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.0093e-03 (9.2215e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [70][360/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.1985e-03 (9.0989e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [70][370/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.4033e-02 (9.0537e-03)	Acc@1  98.44 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [70][380/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.1565e-03 (9.0002e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [70][390/391]	Time  0.128 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.4437e-03 (9.0004e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
## e[70] optimizer.zero_grad (sum) time: 0.28818702697753906
## e[70]       loss.backward (sum) time: 18.501492977142334
## e[70]      optimizer.step (sum) time: 2.898390293121338
## epoch[70] training(only) time: 53.626994132995605
# Switched to evaluate mode...
Test: [  0/100]	Time  0.207 ( 0.207)	Loss 1.2940e-01 (1.2940e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.043 ( 0.059)	Loss 4.2516e-01 (2.9609e-01)	Acc@1  91.00 ( 93.09)	Acc@5  99.00 ( 99.73)
Test: [ 20/100]	Time  0.044 ( 0.052)	Loss 5.0396e-01 (3.8018e-01)	Acc@1  91.00 ( 92.19)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.041 ( 0.049)	Loss 2.7336e-01 (3.9849e-01)	Acc@1  94.00 ( 92.29)	Acc@5 100.00 ( 99.77)
Test: [ 40/100]	Time  0.042 ( 0.048)	Loss 4.8081e-01 (4.0948e-01)	Acc@1  90.00 ( 92.12)	Acc@5  99.00 ( 99.78)
Test: [ 50/100]	Time  0.044 ( 0.047)	Loss 1.7576e-01 (4.0275e-01)	Acc@1  96.00 ( 92.18)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.041 ( 0.046)	Loss 3.9873e-01 (3.9001e-01)	Acc@1  96.00 ( 92.38)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.042 ( 0.046)	Loss 6.4384e-01 (3.8637e-01)	Acc@1  88.00 ( 92.42)	Acc@5 100.00 ( 99.80)
Test: [ 80/100]	Time  0.045 ( 0.046)	Loss 1.9192e-01 (3.8702e-01)	Acc@1  96.00 ( 92.54)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.042 ( 0.045)	Loss 2.3364e-01 (3.7958e-01)	Acc@1  95.00 ( 92.58)	Acc@5 100.00 ( 99.79)
 * Acc@1 92.620 Acc@5 99.770
### epoch[70] execution time: 58.27937936782837
EPOCH 71
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [71][  0/391]	Time  0.301 ( 0.301)	Data  0.179 ( 0.179)	Loss 3.8242e-02 (3.8242e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [71][ 10/391]	Time  0.132 ( 0.149)	Data  0.001 ( 0.020)	Loss 1.1474e-03 (1.0293e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [71][ 20/391]	Time  0.135 ( 0.143)	Data  0.001 ( 0.012)	Loss 7.5082e-03 (9.2113e-03)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [71][ 30/391]	Time  0.134 ( 0.141)	Data  0.001 ( 0.010)	Loss 1.9934e-03 (8.4972e-03)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [71][ 40/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.008)	Loss 1.8609e-03 (8.5930e-03)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [71][ 50/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.008)	Loss 2.5077e-03 (8.1818e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [71][ 60/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.007)	Loss 1.1476e-02 (8.0029e-03)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [71][ 70/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.007)	Loss 8.3932e-03 (7.8824e-03)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [71][ 80/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.6855e-03 (7.9734e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [71][ 90/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.8356e-03 (7.9493e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [71][100/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.2337e-03 (7.5067e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [71][110/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.2360e-03 (7.5684e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [71][120/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.4013e-03 (7.5014e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [71][130/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.7511e-03 (7.6431e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [71][140/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.7175e-03 (7.5108e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [71][150/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.0397e-03 (7.4449e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [71][160/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.0124e-02 (7.6245e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [71][170/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.005)	Loss 7.1542e-03 (7.5651e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [71][180/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.0690e-02 (7.4548e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [71][190/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.005)	Loss 6.9803e-03 (7.6307e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [71][200/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.4511e-02 (7.6095e-03)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [71][210/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.6073e-03 (7.5631e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [71][220/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.7335e-02 (7.6782e-03)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [71][230/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.7704e-03 (7.7001e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [71][240/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.8955e-03 (7.6040e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [71][250/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.0637e-02 (7.6256e-03)	Acc@1  98.44 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [71][260/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.5998e-03 (7.4897e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [71][270/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1907e-03 (7.5631e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [71][280/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.0288e-03 (7.7315e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [71][290/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1364e-02 (7.8773e-03)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [71][300/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.9428e-03 (7.7889e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [71][310/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.1438e-03 (7.7877e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [71][320/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.5675e-04 (7.8704e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [71][330/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.5809e-03 (7.8683e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [71][340/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1034e-03 (7.8543e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [71][350/391]	Time  0.131 ( 0.137)	Data  0.002 ( 0.005)	Loss 1.3378e-02 (7.8617e-03)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [71][360/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.2824e-03 (7.8837e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [71][370/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.5676e-04 (7.8353e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [71][380/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.1564e-03 (7.8237e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [71][390/391]	Time  0.128 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.3319e-02 (7.9159e-03)	Acc@1  98.75 ( 99.78)	Acc@5 100.00 (100.00)
## e[71] optimizer.zero_grad (sum) time: 0.28853845596313477
## e[71]       loss.backward (sum) time: 18.542672157287598
## e[71]      optimizer.step (sum) time: 2.9284844398498535
## epoch[71] training(only) time: 53.74197745323181
# Switched to evaluate mode...
Test: [  0/100]	Time  0.202 ( 0.202)	Loss 1.6431e-01 (1.6431e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.045 ( 0.059)	Loss 4.1182e-01 (3.0550e-01)	Acc@1  92.00 ( 93.36)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.046 ( 0.052)	Loss 4.9283e-01 (3.8855e-01)	Acc@1  89.00 ( 92.19)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.044 ( 0.049)	Loss 2.5726e-01 (4.0614e-01)	Acc@1  95.00 ( 92.35)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.041 ( 0.048)	Loss 4.7660e-01 (4.1445e-01)	Acc@1  90.00 ( 92.15)	Acc@5  99.00 ( 99.71)
Test: [ 50/100]	Time  0.041 ( 0.047)	Loss 1.8923e-01 (4.0569e-01)	Acc@1  96.00 ( 92.20)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.044 ( 0.046)	Loss 3.8711e-01 (3.9158e-01)	Acc@1  95.00 ( 92.36)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.043 ( 0.046)	Loss 6.1477e-01 (3.8616e-01)	Acc@1  89.00 ( 92.48)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.043 ( 0.046)	Loss 1.9954e-01 (3.8797e-01)	Acc@1  95.00 ( 92.48)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.042 ( 0.045)	Loss 2.7084e-01 (3.8045e-01)	Acc@1  94.00 ( 92.54)	Acc@5 100.00 ( 99.76)
 * Acc@1 92.590 Acc@5 99.730
### epoch[71] execution time: 58.38279461860657
EPOCH 72
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [72][  0/391]	Time  0.316 ( 0.316)	Data  0.189 ( 0.189)	Loss 5.1101e-03 (5.1101e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [72][ 10/391]	Time  0.135 ( 0.152)	Data  0.001 ( 0.021)	Loss 3.2300e-03 (7.1459e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [72][ 20/391]	Time  0.132 ( 0.144)	Data  0.001 ( 0.013)	Loss 6.9579e-03 (6.0864e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [72][ 30/391]	Time  0.139 ( 0.141)	Data  0.002 ( 0.010)	Loss 1.5528e-02 (7.5769e-03)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [72][ 40/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.009)	Loss 1.9228e-02 (8.5531e-03)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [72][ 50/391]	Time  0.132 ( 0.139)	Data  0.001 ( 0.008)	Loss 3.1171e-03 (7.7972e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [72][ 60/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.007)	Loss 5.8763e-03 (7.4876e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [72][ 70/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.007)	Loss 3.9973e-03 (7.5359e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [72][ 80/391]	Time  0.130 ( 0.139)	Data  0.001 ( 0.007)	Loss 1.8858e-03 (7.7400e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [72][ 90/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 8.3148e-03 (8.3997e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [72][100/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.0726e-03 (8.3296e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [72][110/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.2529e-03 (8.3697e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [72][120/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.006)	Loss 9.7781e-04 (7.9851e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [72][130/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.006)	Loss 7.5168e-03 (8.3866e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [72][140/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.4155e-03 (8.4273e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [72][150/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.006)	Loss 2.5619e-03 (8.4995e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [72][160/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.006)	Loss 1.3326e-03 (8.4737e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [72][170/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.006)	Loss 1.1751e-02 (8.1973e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [72][180/391]	Time  0.136 ( 0.137)	Data  0.002 ( 0.005)	Loss 3.1854e-02 (8.3527e-03)	Acc@1  98.44 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [72][190/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0064e-02 (8.4218e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [72][200/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.0763e-02 (8.4596e-03)	Acc@1  98.44 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [72][210/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.2921e-02 (8.6269e-03)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [72][220/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.7556e-03 (8.4778e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [72][230/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.7669e-03 (8.5974e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [72][240/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.7330e-02 (8.5008e-03)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [72][250/391]	Time  0.141 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.4731e-02 (8.6646e-03)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [72][260/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.1542e-03 (8.6674e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [72][270/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.5621e-03 (8.6947e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [72][280/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.9507e-03 (8.5714e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [72][290/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.1167e-03 (8.6439e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [72][300/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1248e-02 (8.5655e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [72][310/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.5441e-03 (8.6214e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [72][320/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.5409e-03 (8.6197e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [72][330/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.6811e-03 (8.5649e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [72][340/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.8057e-03 (8.5146e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [72][350/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1030e-02 (8.5052e-03)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [72][360/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.7858e-03 (8.4529e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [72][370/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.9184e-03 (8.6411e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [72][380/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0629e-02 (8.7783e-03)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [72][390/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.3201e-03 (8.7435e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
## e[72] optimizer.zero_grad (sum) time: 0.28835439682006836
## e[72]       loss.backward (sum) time: 18.485440254211426
## e[72]      optimizer.step (sum) time: 2.8931872844696045
## epoch[72] training(only) time: 53.699710845947266
# Switched to evaluate mode...
Test: [  0/100]	Time  0.198 ( 0.198)	Loss 1.5480e-01 (1.5480e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.043 ( 0.059)	Loss 4.1024e-01 (3.0524e-01)	Acc@1  92.00 ( 93.36)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.045 ( 0.051)	Loss 5.0139e-01 (3.8977e-01)	Acc@1  89.00 ( 92.38)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.044 ( 0.049)	Loss 2.6467e-01 (4.0332e-01)	Acc@1  95.00 ( 92.42)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.043 ( 0.048)	Loss 5.0161e-01 (4.1443e-01)	Acc@1  90.00 ( 92.07)	Acc@5  99.00 ( 99.76)
Test: [ 50/100]	Time  0.042 ( 0.047)	Loss 1.3988e-01 (4.0608e-01)	Acc@1  97.00 ( 92.14)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.044 ( 0.046)	Loss 4.0504e-01 (3.9122e-01)	Acc@1  96.00 ( 92.36)	Acc@5 100.00 ( 99.82)
Test: [ 70/100]	Time  0.043 ( 0.046)	Loss 5.6882e-01 (3.8640e-01)	Acc@1  88.00 ( 92.42)	Acc@5 100.00 ( 99.82)
Test: [ 80/100]	Time  0.044 ( 0.046)	Loss 1.7296e-01 (3.8846e-01)	Acc@1  95.00 ( 92.47)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.042 ( 0.046)	Loss 2.4495e-01 (3.8056e-01)	Acc@1  94.00 ( 92.52)	Acc@5 100.00 ( 99.81)
 * Acc@1 92.520 Acc@5 99.800
### epoch[72] execution time: 58.33957099914551
EPOCH 73
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [73][  0/391]	Time  0.291 ( 0.291)	Data  0.176 ( 0.176)	Loss 5.5868e-03 (5.5868e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [73][ 10/391]	Time  0.135 ( 0.150)	Data  0.001 ( 0.020)	Loss 1.5480e-02 (8.7185e-03)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [73][ 20/391]	Time  0.132 ( 0.143)	Data  0.001 ( 0.012)	Loss 3.4705e-03 (8.1623e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [73][ 30/391]	Time  0.136 ( 0.141)	Data  0.001 ( 0.010)	Loss 2.1997e-03 (7.0831e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [73][ 40/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.009)	Loss 4.2185e-03 (6.2570e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [73][ 50/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.008)	Loss 2.8622e-03 (6.4792e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [73][ 60/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.007)	Loss 1.9473e-03 (6.7437e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [73][ 70/391]	Time  0.136 ( 0.138)	Data  0.002 ( 0.007)	Loss 4.5514e-03 (7.0798e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [73][ 80/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.007)	Loss 4.1629e-03 (7.1152e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [73][ 90/391]	Time  0.131 ( 0.138)	Data  0.001 ( 0.006)	Loss 5.9538e-03 (7.1610e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [73][100/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 6.9458e-03 (6.8546e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [73][110/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 8.0193e-04 (6.7319e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [73][120/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.7457e-02 (7.0303e-03)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [73][130/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.6646e-03 (7.2544e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [73][140/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.1029e-02 (7.8177e-03)	Acc@1  98.44 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [73][150/391]	Time  0.131 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.0114e-02 (7.9932e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [73][160/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.005)	Loss 5.2018e-02 (8.2922e-03)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [73][170/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.005)	Loss 6.3152e-03 (8.1875e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [73][180/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.3021e-03 (8.2784e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [73][190/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.2425e-03 (8.0676e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [73][200/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 4.9112e-03 (8.0352e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [73][210/391]	Time  0.135 ( 0.138)	Data  0.002 ( 0.005)	Loss 2.0924e-02 (8.1917e-03)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [73][220/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.2579e-03 (8.3695e-03)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [73][230/391]	Time  0.130 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.8644e-02 (8.4845e-03)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [73][240/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.7509e-03 (8.3183e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [73][250/391]	Time  0.129 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.2299e-03 (8.2168e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [73][260/391]	Time  0.129 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.3592e-03 (8.2252e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [73][270/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.9209e-03 (8.1037e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [73][280/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.3962e-03 (8.1879e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [73][290/391]	Time  0.135 ( 0.137)	Data  0.002 ( 0.005)	Loss 1.5051e-03 (8.2047e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [73][300/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.6534e-02 (8.2851e-03)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [73][310/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.0178e-03 (8.2119e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [73][320/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.6500e-03 (8.1096e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [73][330/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.9955e-03 (8.0608e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [73][340/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.4750e-03 (7.9919e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [73][350/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.8738e-03 (7.9974e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [73][360/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.5784e-03 (7.8752e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [73][370/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.2461e-03 (7.8643e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [73][380/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.6910e-04 (7.8609e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [73][390/391]	Time  0.125 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.2581e-03 (7.8199e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
## e[73] optimizer.zero_grad (sum) time: 0.28696513175964355
## e[73]       loss.backward (sum) time: 18.533772230148315
## e[73]      optimizer.step (sum) time: 2.8823416233062744
## epoch[73] training(only) time: 53.75723195075989
# Switched to evaluate mode...
Test: [  0/100]	Time  0.204 ( 0.204)	Loss 1.4827e-01 (1.4827e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.042 ( 0.058)	Loss 4.0920e-01 (3.0878e-01)	Acc@1  91.00 ( 92.82)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.045 ( 0.051)	Loss 4.9928e-01 (3.9386e-01)	Acc@1  89.00 ( 91.90)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.044 ( 0.049)	Loss 2.5474e-01 (4.0787e-01)	Acc@1  94.00 ( 92.29)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.041 ( 0.047)	Loss 5.1470e-01 (4.1853e-01)	Acc@1  90.00 ( 92.02)	Acc@5  99.00 ( 99.76)
Test: [ 50/100]	Time  0.044 ( 0.047)	Loss 1.3767e-01 (4.1032e-01)	Acc@1  97.00 ( 92.10)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.044 ( 0.046)	Loss 3.7705e-01 (3.9364e-01)	Acc@1  96.00 ( 92.34)	Acc@5 100.00 ( 99.82)
Test: [ 70/100]	Time  0.042 ( 0.046)	Loss 5.8546e-01 (3.8709e-01)	Acc@1  88.00 ( 92.39)	Acc@5 100.00 ( 99.82)
Test: [ 80/100]	Time  0.043 ( 0.046)	Loss 1.8760e-01 (3.8836e-01)	Acc@1  95.00 ( 92.48)	Acc@5 100.00 ( 99.81)
Test: [ 90/100]	Time  0.043 ( 0.045)	Loss 2.5885e-01 (3.8036e-01)	Acc@1  94.00 ( 92.54)	Acc@5 100.00 ( 99.82)
 * Acc@1 92.580 Acc@5 99.800
### epoch[73] execution time: 58.383551597595215
EPOCH 74
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [74][  0/391]	Time  0.300 ( 0.300)	Data  0.180 ( 0.180)	Loss 1.7719e-02 (1.7719e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [74][ 10/391]	Time  0.134 ( 0.151)	Data  0.001 ( 0.020)	Loss 3.7634e-03 (1.0525e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [74][ 20/391]	Time  0.134 ( 0.144)	Data  0.002 ( 0.013)	Loss 2.9100e-02 (1.1630e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [74][ 30/391]	Time  0.133 ( 0.141)	Data  0.001 ( 0.010)	Loss 2.3663e-03 (9.9903e-03)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [74][ 40/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.009)	Loss 4.9063e-03 (9.1302e-03)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [74][ 50/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.008)	Loss 1.5666e-03 (9.0453e-03)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [74][ 60/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.007)	Loss 7.5881e-03 (8.7630e-03)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [74][ 70/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.007)	Loss 4.6569e-03 (8.2061e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [74][ 80/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.007)	Loss 1.6077e-03 (8.7742e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [74][ 90/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.8943e-03 (9.0395e-03)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [74][100/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 6.4441e-03 (8.7694e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [74][110/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.2258e-02 (9.1205e-03)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [74][120/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.0037e-03 (8.7590e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [74][130/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 8.9917e-04 (8.8062e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [74][140/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.2281e-02 (9.0625e-03)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [74][150/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.4545e-02 (8.9431e-03)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [74][160/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 8.8794e-03 (8.9335e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [74][170/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.0172e-02 (8.9301e-03)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [74][180/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.6573e-02 (8.7836e-03)	Acc@1  98.44 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [74][190/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.4993e-03 (8.7544e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [74][200/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.1958e-02 (8.6889e-03)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [74][210/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.3664e-03 (8.6687e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [74][220/391]	Time  0.130 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3666e-02 (8.7153e-03)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [74][230/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0506e-02 (8.8087e-03)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [74][240/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.7715e-03 (8.7724e-03)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [74][250/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1351e-03 (8.6472e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [74][260/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.8034e-02 (8.7057e-03)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [74][270/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.0951e-03 (8.7634e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [74][280/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.3414e-03 (8.7233e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [74][290/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3112e-03 (8.6839e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [74][300/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.8081e-03 (8.5356e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [74][310/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1494e-03 (8.4508e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [74][320/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.3605e-03 (8.3320e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [74][330/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.9610e-03 (8.1952e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [74][340/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.0690e-03 (8.2550e-03)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [74][350/391]	Time  0.136 ( 0.137)	Data  0.002 ( 0.005)	Loss 4.1921e-03 (8.1629e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [74][360/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.1645e-02 (8.1817e-03)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [74][370/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1986e-02 (8.1946e-03)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [74][380/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.7103e-03 (8.1984e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [74][390/391]	Time  0.129 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.6569e-03 (8.0920e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
## e[74] optimizer.zero_grad (sum) time: 0.287168025970459
## e[74]       loss.backward (sum) time: 18.497833967208862
## e[74]      optimizer.step (sum) time: 2.861462116241455
## epoch[74] training(only) time: 53.759575843811035
# Switched to evaluate mode...
Test: [  0/100]	Time  0.193 ( 0.193)	Loss 1.7514e-01 (1.7514e-01)	Acc@1  97.00 ( 97.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.044 ( 0.058)	Loss 4.3692e-01 (3.0874e-01)	Acc@1  92.00 ( 93.18)	Acc@5  99.00 ( 99.73)
Test: [ 20/100]	Time  0.043 ( 0.051)	Loss 5.2767e-01 (3.9345e-01)	Acc@1  89.00 ( 92.19)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.044 ( 0.049)	Loss 2.7036e-01 (4.0770e-01)	Acc@1  92.00 ( 92.29)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.042 ( 0.047)	Loss 4.7796e-01 (4.1363e-01)	Acc@1  90.00 ( 92.12)	Acc@5  99.00 ( 99.78)
Test: [ 50/100]	Time  0.042 ( 0.047)	Loss 1.4600e-01 (4.0705e-01)	Acc@1  96.00 ( 92.10)	Acc@5 100.00 ( 99.80)
Test: [ 60/100]	Time  0.044 ( 0.046)	Loss 3.8448e-01 (3.9254e-01)	Acc@1  95.00 ( 92.30)	Acc@5 100.00 ( 99.84)
Test: [ 70/100]	Time  0.043 ( 0.046)	Loss 6.1031e-01 (3.8735e-01)	Acc@1  88.00 ( 92.39)	Acc@5 100.00 ( 99.83)
Test: [ 80/100]	Time  0.041 ( 0.046)	Loss 1.6666e-01 (3.9106e-01)	Acc@1  95.00 ( 92.42)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.042 ( 0.045)	Loss 2.5832e-01 (3.8293e-01)	Acc@1  94.00 ( 92.49)	Acc@5 100.00 ( 99.82)
 * Acc@1 92.520 Acc@5 99.810
### epoch[74] execution time: 58.39491105079651
EPOCH 75
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [75][  0/391]	Time  0.295 ( 0.295)	Data  0.176 ( 0.176)	Loss 1.0711e-02 (1.0711e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [75][ 10/391]	Time  0.134 ( 0.150)	Data  0.001 ( 0.020)	Loss 1.5527e-03 (7.2797e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [75][ 20/391]	Time  0.134 ( 0.144)	Data  0.001 ( 0.012)	Loss 1.9857e-03 (7.0005e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [75][ 30/391]	Time  0.135 ( 0.141)	Data  0.001 ( 0.010)	Loss 3.2360e-03 (7.9630e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [75][ 40/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.009)	Loss 8.8509e-03 (7.7204e-03)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [75][ 50/391]	Time  0.139 ( 0.140)	Data  0.002 ( 0.008)	Loss 1.2126e-02 (7.1725e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [75][ 60/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.007)	Loss 2.6390e-03 (7.2564e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [75][ 70/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.007)	Loss 4.0221e-03 (6.9630e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [75][ 80/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.007)	Loss 3.7967e-03 (6.8419e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [75][ 90/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.3630e-03 (6.6700e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [75][100/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.9968e-03 (6.5969e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [75][110/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.006)	Loss 9.7108e-03 (6.5044e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [75][120/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.1711e-03 (6.4825e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [75][130/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.2173e-03 (6.8067e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [75][140/391]	Time  0.136 ( 0.138)	Data  0.002 ( 0.006)	Loss 1.4413e-02 (6.9309e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [75][150/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 8.6159e-04 (7.0634e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [75][160/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 7.0333e-03 (7.4484e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [75][170/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.0040e-02 (7.2434e-03)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [75][180/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.2694e-03 (7.1109e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [75][190/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.3612e-02 (7.0978e-03)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [75][200/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.2441e-02 (7.1298e-03)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [75][210/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.5133e-03 (7.2572e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [75][220/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.1782e-03 (7.2703e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [75][230/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3522e-02 (7.4667e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [75][240/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.0497e-03 (7.6459e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [75][250/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.5455e-03 (7.7318e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [75][260/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.2401e-02 (7.7288e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [75][270/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.3250e-03 (7.6865e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [75][280/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.4565e-03 (7.6765e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [75][290/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.4360e-02 (7.7599e-03)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [75][300/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3872e-02 (7.7678e-03)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [75][310/391]	Time  0.131 ( 0.137)	Data  0.002 ( 0.005)	Loss 2.1745e-02 (7.6943e-03)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [75][320/391]	Time  0.137 ( 0.137)	Data  0.002 ( 0.005)	Loss 1.3850e-03 (7.5896e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [75][330/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.2455e-03 (7.6482e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [75][340/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.6671e-02 (7.7006e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [75][350/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.5686e-02 (7.8020e-03)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [75][360/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.8229e-03 (7.7822e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [75][370/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.5358e-03 (7.7893e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [75][380/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.2153e-02 (7.7902e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [75][390/391]	Time  0.128 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.2395e-02 (7.7214e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
## e[75] optimizer.zero_grad (sum) time: 0.28624892234802246
## e[75]       loss.backward (sum) time: 18.449785470962524
## e[75]      optimizer.step (sum) time: 2.8995065689086914
## epoch[75] training(only) time: 53.72222828865051
# Switched to evaluate mode...
Test: [  0/100]	Time  0.200 ( 0.200)	Loss 1.4325e-01 (1.4325e-01)	Acc@1  97.00 ( 97.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.044 ( 0.058)	Loss 4.3648e-01 (2.9689e-01)	Acc@1  91.00 ( 93.27)	Acc@5  99.00 ( 99.73)
Test: [ 20/100]	Time  0.044 ( 0.051)	Loss 5.3466e-01 (3.8523e-01)	Acc@1  89.00 ( 92.24)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.044 ( 0.049)	Loss 2.4655e-01 (4.0446e-01)	Acc@1  94.00 ( 92.32)	Acc@5 100.00 ( 99.77)
Test: [ 40/100]	Time  0.043 ( 0.047)	Loss 4.8754e-01 (4.1347e-01)	Acc@1  90.00 ( 92.10)	Acc@5  99.00 ( 99.78)
Test: [ 50/100]	Time  0.042 ( 0.046)	Loss 1.5916e-01 (4.0855e-01)	Acc@1  96.00 ( 92.18)	Acc@5 100.00 ( 99.80)
Test: [ 60/100]	Time  0.046 ( 0.046)	Loss 4.0106e-01 (3.9461e-01)	Acc@1  96.00 ( 92.43)	Acc@5 100.00 ( 99.84)
Test: [ 70/100]	Time  0.043 ( 0.046)	Loss 6.3415e-01 (3.8922e-01)	Acc@1  88.00 ( 92.55)	Acc@5 100.00 ( 99.83)
Test: [ 80/100]	Time  0.047 ( 0.045)	Loss 1.9059e-01 (3.9130e-01)	Acc@1  95.00 ( 92.59)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.042 ( 0.045)	Loss 2.5092e-01 (3.8313e-01)	Acc@1  94.00 ( 92.65)	Acc@5 100.00 ( 99.82)
 * Acc@1 92.650 Acc@5 99.810
### epoch[75] execution time: 58.35531449317932
EPOCH 76
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [76][  0/391]	Time  0.315 ( 0.315)	Data  0.181 ( 0.181)	Loss 1.2266e-02 (1.2266e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [76][ 10/391]	Time  0.135 ( 0.151)	Data  0.001 ( 0.020)	Loss 2.7077e-03 (5.9023e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [76][ 20/391]	Time  0.137 ( 0.144)	Data  0.001 ( 0.013)	Loss 3.8688e-03 (7.3472e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [76][ 30/391]	Time  0.132 ( 0.141)	Data  0.001 ( 0.010)	Loss 2.4081e-03 (6.7671e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [76][ 40/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.009)	Loss 1.3545e-02 (7.6140e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [76][ 50/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.008)	Loss 8.2548e-03 (7.1478e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [76][ 60/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.007)	Loss 2.9998e-03 (7.0167e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [76][ 70/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.007)	Loss 1.4801e-03 (6.5376e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [76][ 80/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.007)	Loss 1.2112e-03 (6.4895e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [76][ 90/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.1371e-03 (7.2671e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [76][100/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.0000e-03 (7.0967e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [76][110/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.5029e-03 (7.1008e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [76][120/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.5550e-03 (7.1900e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [76][130/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.8655e-03 (7.1293e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [76][140/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.9265e-02 (7.4717e-03)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [76][150/391]	Time  0.131 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.1949e-02 (8.1333e-03)	Acc@1  98.44 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [76][160/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 8.1347e-03 (8.0377e-03)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [76][170/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 6.5124e-03 (8.1779e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [76][180/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.7161e-02 (8.1888e-03)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [76][190/391]	Time  0.134 ( 0.137)	Data  0.002 ( 0.005)	Loss 2.7876e-03 (8.3255e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [76][200/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.6361e-03 (8.3197e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [76][210/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.9896e-02 (8.2880e-03)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [76][220/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.0268e-02 (8.3219e-03)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [76][230/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.1335e-03 (8.2657e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [76][240/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.1868e-03 (8.2696e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [76][250/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.4908e-03 (8.3584e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [76][260/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0723e-03 (8.2204e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [76][270/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.1283e-02 (8.2674e-03)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [76][280/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.6969e-03 (8.1498e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [76][290/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.9221e-03 (8.1251e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [76][300/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.3851e-03 (7.9938e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [76][310/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.3155e-04 (8.0355e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [76][320/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.5088e-03 (8.0174e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [76][330/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.4938e-03 (7.9768e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [76][340/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.1705e-03 (7.9930e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [76][350/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.0064e-03 (7.8942e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [76][360/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1644e-02 (7.8943e-03)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [76][370/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.2359e-03 (8.0549e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [76][380/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.9520e-02 (8.0776e-03)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [76][390/391]	Time  0.130 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3490e-02 (8.1659e-03)	Acc@1  98.75 ( 99.77)	Acc@5 100.00 (100.00)
## e[76] optimizer.zero_grad (sum) time: 0.28493523597717285
## e[76]       loss.backward (sum) time: 18.50227952003479
## e[76]      optimizer.step (sum) time: 2.88569712638855
## epoch[76] training(only) time: 53.81046438217163
# Switched to evaluate mode...
Test: [  0/100]	Time  0.202 ( 0.202)	Loss 1.6163e-01 (1.6163e-01)	Acc@1  97.00 ( 97.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.044 ( 0.058)	Loss 4.2989e-01 (3.0031e-01)	Acc@1  91.00 ( 93.45)	Acc@5  99.00 ( 99.73)
Test: [ 20/100]	Time  0.044 ( 0.052)	Loss 5.5217e-01 (3.9069e-01)	Acc@1  88.00 ( 92.29)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.044 ( 0.049)	Loss 2.5552e-01 (4.0964e-01)	Acc@1  94.00 ( 92.29)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.044 ( 0.048)	Loss 4.5698e-01 (4.1669e-01)	Acc@1  90.00 ( 92.02)	Acc@5  99.00 ( 99.68)
Test: [ 50/100]	Time  0.042 ( 0.047)	Loss 1.6861e-01 (4.1004e-01)	Acc@1  96.00 ( 92.14)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.041 ( 0.046)	Loss 3.9739e-01 (3.9583e-01)	Acc@1  96.00 ( 92.41)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.042 ( 0.046)	Loss 6.4034e-01 (3.9142e-01)	Acc@1  89.00 ( 92.54)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.043 ( 0.046)	Loss 1.7245e-01 (3.9285e-01)	Acc@1  95.00 ( 92.60)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.044 ( 0.045)	Loss 2.2099e-01 (3.8490e-01)	Acc@1  95.00 ( 92.63)	Acc@5 100.00 ( 99.77)
 * Acc@1 92.640 Acc@5 99.760
### epoch[76] execution time: 58.43906116485596
EPOCH 77
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [77][  0/391]	Time  0.288 ( 0.288)	Data  0.173 ( 0.173)	Loss 4.1259e-03 (4.1259e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [77][ 10/391]	Time  0.132 ( 0.150)	Data  0.001 ( 0.020)	Loss 1.2660e-02 (1.0461e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [77][ 20/391]	Time  0.135 ( 0.143)	Data  0.001 ( 0.012)	Loss 8.4639e-04 (7.9656e-03)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [77][ 30/391]	Time  0.134 ( 0.141)	Data  0.001 ( 0.010)	Loss 8.0979e-03 (6.8966e-03)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [77][ 40/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.008)	Loss 9.1544e-03 (7.7326e-03)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [77][ 50/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.008)	Loss 3.0528e-03 (7.1545e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [77][ 60/391]	Time  0.132 ( 0.139)	Data  0.001 ( 0.007)	Loss 3.0383e-03 (7.1249e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [77][ 70/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.007)	Loss 1.2008e-02 (7.3751e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [77][ 80/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.007)	Loss 5.7169e-03 (7.0697e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [77][ 90/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 7.1596e-03 (6.9142e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [77][100/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.6651e-03 (6.9370e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [77][110/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 8.1819e-03 (6.8989e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [77][120/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.7410e-03 (6.9128e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [77][130/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 6.6409e-03 (7.0921e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [77][140/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.7329e-03 (7.1011e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [77][150/391]	Time  0.132 ( 0.137)	Data  0.002 ( 0.006)	Loss 5.0415e-03 (7.4649e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [77][160/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.006)	Loss 9.5665e-03 (7.8587e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [77][170/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.3833e-03 (7.8443e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [77][180/391]	Time  0.127 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.6879e-02 (7.9490e-03)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [77][190/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.5070e-03 (7.7684e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [77][200/391]	Time  0.129 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.0657e-02 (7.7535e-03)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [77][210/391]	Time  0.129 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.3193e-02 (7.6481e-03)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [77][220/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.2600e-02 (7.6288e-03)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [77][230/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3770e-02 (7.5365e-03)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [77][240/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.3526e-03 (7.6604e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [77][250/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.7738e-03 (7.6560e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [77][260/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.7373e-03 (7.6122e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [77][270/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.5266e-03 (7.6535e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [77][280/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.9697e-03 (7.7508e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [77][290/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.6765e-03 (7.6871e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [77][300/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.6567e-02 (7.6867e-03)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [77][310/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.9671e-03 (7.5756e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [77][320/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0618e-02 (7.6045e-03)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [77][330/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.5301e-03 (7.7147e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [77][340/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.9552e-03 (7.6695e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [77][350/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.1308e-03 (7.6936e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [77][360/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.1926e-03 (7.5926e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [77][370/391]	Time  0.130 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.2832e-03 (7.6882e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [77][380/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.2217e-02 (7.7150e-03)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [77][390/391]	Time  0.128 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.0512e-03 (7.7052e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
## e[77] optimizer.zero_grad (sum) time: 0.28711915016174316
## e[77]       loss.backward (sum) time: 18.46608567237854
## e[77]      optimizer.step (sum) time: 2.8774428367614746
## epoch[77] training(only) time: 53.64809584617615
# Switched to evaluate mode...
Test: [  0/100]	Time  0.197 ( 0.197)	Loss 1.6568e-01 (1.6568e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.044 ( 0.058)	Loss 4.0242e-01 (3.0052e-01)	Acc@1  93.00 ( 93.36)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.044 ( 0.051)	Loss 5.1535e-01 (3.9005e-01)	Acc@1  90.00 ( 92.43)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.045 ( 0.049)	Loss 2.6533e-01 (4.0588e-01)	Acc@1  94.00 ( 92.58)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.046 ( 0.048)	Loss 4.6281e-01 (4.1347e-01)	Acc@1  90.00 ( 92.32)	Acc@5  99.00 ( 99.73)
Test: [ 50/100]	Time  0.044 ( 0.047)	Loss 1.5046e-01 (4.0567e-01)	Acc@1  96.00 ( 92.35)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.042 ( 0.046)	Loss 3.8813e-01 (3.9178e-01)	Acc@1  96.00 ( 92.57)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.043 ( 0.046)	Loss 6.4433e-01 (3.8758e-01)	Acc@1  88.00 ( 92.66)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.045 ( 0.046)	Loss 1.6096e-01 (3.8924e-01)	Acc@1  95.00 ( 92.72)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.042 ( 0.045)	Loss 2.5453e-01 (3.8155e-01)	Acc@1  94.00 ( 92.76)	Acc@5 100.00 ( 99.80)
 * Acc@1 92.790 Acc@5 99.780
### epoch[77] execution time: 58.3299663066864
EPOCH 78
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [78][  0/391]	Time  0.314 ( 0.314)	Data  0.176 ( 0.176)	Loss 2.8147e-02 (2.8147e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [78][ 10/391]	Time  0.135 ( 0.152)	Data  0.001 ( 0.020)	Loss 1.3238e-03 (5.5125e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [78][ 20/391]	Time  0.132 ( 0.144)	Data  0.001 ( 0.012)	Loss 2.9149e-03 (7.4333e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [78][ 30/391]	Time  0.136 ( 0.142)	Data  0.001 ( 0.010)	Loss 1.1593e-02 (8.0988e-03)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [78][ 40/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.008)	Loss 4.3131e-03 (7.2736e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [78][ 50/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.008)	Loss 1.2911e-02 (7.5003e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [78][ 60/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.007)	Loss 2.3120e-03 (7.4497e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [78][ 70/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.007)	Loss 1.2548e-03 (7.4251e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [78][ 80/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.007)	Loss 1.7032e-02 (8.0861e-03)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [78][ 90/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.4088e-02 (8.3380e-03)	Acc@1  98.44 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [78][100/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.7999e-03 (8.4093e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [78][110/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 5.0660e-03 (7.9969e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [78][120/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.8657e-03 (8.2907e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [78][130/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 9.8005e-03 (8.4478e-03)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [78][140/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.006)	Loss 5.1040e-03 (8.5244e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [78][150/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.6641e-03 (8.4535e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [78][160/391]	Time  0.130 ( 0.138)	Data  0.002 ( 0.006)	Loss 7.2284e-03 (8.4527e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [78][170/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.1128e-03 (8.1973e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [78][180/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.3837e-03 (8.2429e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [78][190/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.7706e-02 (8.2092e-03)	Acc@1  98.44 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [78][200/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.1316e-02 (8.2704e-03)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [78][210/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.6264e-03 (8.3277e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [78][220/391]	Time  0.130 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0968e-02 (8.5365e-03)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [78][230/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.5129e-03 (8.3491e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [78][240/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.8664e-03 (8.4137e-03)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [78][250/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.6330e-03 (8.4499e-03)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [78][260/391]	Time  0.141 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.7741e-03 (8.5561e-03)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [78][270/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.2332e-03 (8.3955e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [78][280/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.4112e-03 (8.4253e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [78][290/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.8934e-03 (8.4547e-03)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [78][300/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.5919e-03 (8.3782e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [78][310/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1371e-02 (8.2828e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [78][320/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.7841e-03 (8.3785e-03)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [78][330/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.9321e-03 (8.5796e-03)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [78][340/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.5175e-03 (8.5434e-03)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [78][350/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.1265e-03 (8.4419e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [78][360/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.5137e-03 (8.3314e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [78][370/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.2717e-03 (8.3171e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [78][380/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.8226e-03 (8.2994e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [78][390/391]	Time  0.129 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.6916e-03 (8.2824e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
## e[78] optimizer.zero_grad (sum) time: 0.2857697010040283
## e[78]       loss.backward (sum) time: 18.503741025924683
## e[78]      optimizer.step (sum) time: 2.870154619216919
## epoch[78] training(only) time: 53.80354595184326
# Switched to evaluate mode...
Test: [  0/100]	Time  0.190 ( 0.190)	Loss 1.4145e-01 (1.4145e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.057)	Loss 4.2751e-01 (3.0836e-01)	Acc@1  91.00 ( 93.27)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.045 ( 0.051)	Loss 5.1506e-01 (3.9558e-01)	Acc@1  89.00 ( 92.24)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.043 ( 0.049)	Loss 2.6171e-01 (4.1736e-01)	Acc@1  94.00 ( 92.45)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.044 ( 0.047)	Loss 5.0263e-01 (4.2400e-01)	Acc@1  90.00 ( 92.10)	Acc@5  99.00 ( 99.66)
Test: [ 50/100]	Time  0.043 ( 0.047)	Loss 2.1591e-01 (4.1677e-01)	Acc@1  96.00 ( 92.18)	Acc@5 100.00 ( 99.69)
Test: [ 60/100]	Time  0.044 ( 0.046)	Loss 3.8307e-01 (4.0315e-01)	Acc@1  96.00 ( 92.34)	Acc@5 100.00 ( 99.72)
Test: [ 70/100]	Time  0.044 ( 0.046)	Loss 7.2728e-01 (4.0007e-01)	Acc@1  88.00 ( 92.48)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.043 ( 0.045)	Loss 1.9517e-01 (4.0004e-01)	Acc@1  95.00 ( 92.54)	Acc@5 100.00 ( 99.72)
Test: [ 90/100]	Time  0.043 ( 0.045)	Loss 2.1437e-01 (3.9195e-01)	Acc@1  95.00 ( 92.57)	Acc@5 100.00 ( 99.74)
 * Acc@1 92.640 Acc@5 99.730
### epoch[78] execution time: 58.419891119003296
EPOCH 79
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [79][  0/391]	Time  0.287 ( 0.287)	Data  0.169 ( 0.169)	Loss 1.9633e-03 (1.9633e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [79][ 10/391]	Time  0.132 ( 0.149)	Data  0.001 ( 0.019)	Loss 1.2270e-03 (3.6659e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [79][ 20/391]	Time  0.134 ( 0.142)	Data  0.001 ( 0.012)	Loss 3.1676e-03 (4.3555e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [79][ 30/391]	Time  0.132 ( 0.140)	Data  0.001 ( 0.010)	Loss 8.6603e-03 (5.7407e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [79][ 40/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.008)	Loss 7.9301e-03 (6.4319e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [79][ 50/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.008)	Loss 1.4798e-03 (7.1282e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [79][ 60/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.007)	Loss 8.0524e-03 (6.9070e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [79][ 70/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.007)	Loss 4.7636e-03 (7.4375e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [79][ 80/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.006)	Loss 8.2925e-03 (7.2017e-03)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [79][ 90/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.006)	Loss 3.2302e-03 (7.0834e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [79][100/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.006)	Loss 4.4400e-03 (7.0432e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [79][110/391]	Time  0.138 ( 0.137)	Data  0.002 ( 0.006)	Loss 2.4967e-03 (7.0402e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [79][120/391]	Time  0.134 ( 0.137)	Data  0.002 ( 0.006)	Loss 3.2512e-02 (7.1902e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [79][130/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.006)	Loss 2.7063e-03 (7.1016e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [79][140/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.006)	Loss 1.4296e-02 (7.3454e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [79][150/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.7791e-03 (7.0904e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [79][160/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.9339e-03 (6.9199e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [79][170/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.9329e-03 (6.8727e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [79][180/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.4552e-02 (6.9479e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [79][190/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.9487e-03 (6.8682e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [79][200/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.8120e-03 (6.7951e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [79][210/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.0703e-03 (6.9506e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [79][220/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.5347e-03 (7.1472e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [79][230/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.8335e-03 (7.1807e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [79][240/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.0905e-02 (7.3532e-03)	Acc@1  98.44 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [79][250/391]	Time  0.131 ( 0.137)	Data  0.002 ( 0.005)	Loss 3.3158e-03 (7.5411e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [79][260/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.2346e-03 (7.4602e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [79][270/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.3865e-02 (7.5481e-03)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [79][280/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.9122e-03 (7.4823e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [79][290/391]	Time  0.135 ( 0.137)	Data  0.002 ( 0.005)	Loss 3.8872e-03 (7.3617e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [79][300/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.0534e-03 (7.3402e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [79][310/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.5121e-04 (7.4406e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [79][320/391]	Time  0.137 ( 0.137)	Data  0.002 ( 0.005)	Loss 2.8475e-03 (7.5120e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [79][330/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.8023e-03 (7.4407e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [79][340/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.9628e-03 (7.4340e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [79][350/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.1562e-03 (7.3438e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [79][360/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.5433e-02 (7.3777e-03)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [79][370/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.7068e-03 (7.2918e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [79][380/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.9368e-02 (7.3532e-03)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [79][390/391]	Time  0.123 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.8692e-03 (7.3045e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
## e[79] optimizer.zero_grad (sum) time: 0.2887759208679199
## e[79]       loss.backward (sum) time: 18.493908166885376
## e[79]      optimizer.step (sum) time: 2.888453245162964
## epoch[79] training(only) time: 53.629520654678345
# Switched to evaluate mode...
Test: [  0/100]	Time  0.197 ( 0.197)	Loss 1.6565e-01 (1.6565e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.044 ( 0.058)	Loss 4.2603e-01 (3.0916e-01)	Acc@1  93.00 ( 93.64)	Acc@5  99.00 ( 99.55)
Test: [ 20/100]	Time  0.044 ( 0.051)	Loss 5.5134e-01 (3.9714e-01)	Acc@1  88.00 ( 92.57)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.045 ( 0.049)	Loss 2.5409e-01 (4.1649e-01)	Acc@1  94.00 ( 92.61)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.042 ( 0.047)	Loss 4.5721e-01 (4.2200e-01)	Acc@1  90.00 ( 92.29)	Acc@5  99.00 ( 99.68)
Test: [ 50/100]	Time  0.041 ( 0.047)	Loss 1.7072e-01 (4.1365e-01)	Acc@1  96.00 ( 92.29)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.042 ( 0.046)	Loss 3.8319e-01 (3.9868e-01)	Acc@1  95.00 ( 92.48)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.041 ( 0.046)	Loss 6.4093e-01 (3.9445e-01)	Acc@1  90.00 ( 92.59)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.045 ( 0.046)	Loss 1.9144e-01 (3.9550e-01)	Acc@1  94.00 ( 92.60)	Acc@5 100.00 ( 99.72)
Test: [ 90/100]	Time  0.045 ( 0.045)	Loss 2.4066e-01 (3.8719e-01)	Acc@1  95.00 ( 92.68)	Acc@5 100.00 ( 99.74)
 * Acc@1 92.730 Acc@5 99.720
### epoch[79] execution time: 58.27223062515259
EPOCH 80
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [80][  0/391]	Time  0.302 ( 0.302)	Data  0.184 ( 0.184)	Loss 2.4735e-02 (2.4735e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [80][ 10/391]	Time  0.134 ( 0.151)	Data  0.001 ( 0.021)	Loss 2.2569e-03 (1.2613e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [80][ 20/391]	Time  0.136 ( 0.144)	Data  0.001 ( 0.013)	Loss 2.6026e-02 (1.0255e-02)	Acc@1  98.44 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [80][ 30/391]	Time  0.132 ( 0.141)	Data  0.002 ( 0.010)	Loss 3.2023e-02 (1.0338e-02)	Acc@1  98.44 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [80][ 40/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.009)	Loss 5.4450e-03 (9.6676e-03)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [80][ 50/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.008)	Loss 3.1265e-02 (9.8512e-03)	Acc@1  98.44 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [80][ 60/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.007)	Loss 2.6779e-03 (9.3473e-03)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [80][ 70/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.007)	Loss 1.0467e-02 (1.0093e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [80][ 80/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.007)	Loss 9.9476e-03 (9.7025e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [80][ 90/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.0473e-02 (9.1086e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [80][100/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.2191e-02 (9.0783e-03)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [80][110/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.3766e-03 (9.1631e-03)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [80][120/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.006)	Loss 5.2683e-03 (9.1813e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [80][130/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.006)	Loss 7.1237e-03 (8.8298e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [80][140/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.006)	Loss 1.1554e-03 (8.7692e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [80][150/391]	Time  0.132 ( 0.137)	Data  0.002 ( 0.006)	Loss 2.1805e-03 (8.7178e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [80][160/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.006)	Loss 8.6149e-03 (8.7012e-03)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [80][170/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.3568e-03 (8.5491e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [80][180/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.3864e-03 (8.6641e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [80][190/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.5621e-04 (8.8955e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [80][200/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.2818e-03 (8.6753e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [80][210/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.4746e-03 (8.8029e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [80][220/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0466e-02 (8.7558e-03)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [80][230/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0531e-03 (8.5738e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [80][240/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.2020e-02 (8.5867e-03)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [80][250/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.2771e-03 (8.7004e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [80][260/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.6723e-03 (8.7086e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [80][270/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.2313e-03 (8.5990e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [80][280/391]	Time  0.130 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.5012e-03 (8.7020e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [80][290/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.7320e-03 (8.6589e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [80][300/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3961e-03 (8.5258e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [80][310/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.7414e-03 (8.4641e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [80][320/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.9745e-03 (8.3384e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [80][330/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3335e-03 (8.3033e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [80][340/391]	Time  0.138 ( 0.137)	Data  0.002 ( 0.005)	Loss 9.3777e-03 (8.2695e-03)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [80][350/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.9249e-03 (8.2356e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [80][360/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.1391e-04 (8.2336e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [80][370/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.5967e-03 (8.2621e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [80][380/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.2635e-03 (8.2186e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [80][390/391]	Time  0.128 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.7115e-03 (8.2489e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
## e[80] optimizer.zero_grad (sum) time: 0.28888750076293945
## e[80]       loss.backward (sum) time: 18.518840789794922
## e[80]      optimizer.step (sum) time: 2.9805784225463867
## epoch[80] training(only) time: 53.6278281211853
# Switched to evaluate mode...
Test: [  0/100]	Time  0.199 ( 0.199)	Loss 1.6330e-01 (1.6330e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.042 ( 0.058)	Loss 4.3053e-01 (3.1088e-01)	Acc@1  92.00 ( 93.55)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.043 ( 0.052)	Loss 5.3256e-01 (3.9622e-01)	Acc@1  90.00 ( 92.62)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.046 ( 0.049)	Loss 2.6785e-01 (4.1350e-01)	Acc@1  94.00 ( 92.71)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.045 ( 0.048)	Loss 4.8264e-01 (4.1994e-01)	Acc@1  90.00 ( 92.39)	Acc@5  99.00 ( 99.73)
Test: [ 50/100]	Time  0.044 ( 0.047)	Loss 1.4338e-01 (4.1253e-01)	Acc@1  97.00 ( 92.35)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.041 ( 0.047)	Loss 3.9820e-01 (3.9716e-01)	Acc@1  96.00 ( 92.56)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.045 ( 0.046)	Loss 6.3724e-01 (3.9308e-01)	Acc@1  88.00 ( 92.65)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.046 ( 0.046)	Loss 1.8582e-01 (3.9509e-01)	Acc@1  94.00 ( 92.69)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.041 ( 0.045)	Loss 2.4283e-01 (3.8758e-01)	Acc@1  95.00 ( 92.73)	Acc@5 100.00 ( 99.78)
 * Acc@1 92.720 Acc@5 99.760
### epoch[80] execution time: 58.26200222969055
EPOCH 81
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [81][  0/391]	Time  0.315 ( 0.315)	Data  0.193 ( 0.193)	Loss 1.2551e-02 (1.2551e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [81][ 10/391]	Time  0.138 ( 0.152)	Data  0.001 ( 0.021)	Loss 3.1146e-03 (9.4203e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [81][ 20/391]	Time  0.132 ( 0.145)	Data  0.001 ( 0.013)	Loss 5.8107e-03 (9.9208e-03)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [81][ 30/391]	Time  0.137 ( 0.142)	Data  0.001 ( 0.010)	Loss 4.0266e-03 (8.8375e-03)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [81][ 40/391]	Time  0.135 ( 0.141)	Data  0.001 ( 0.009)	Loss 1.2416e-03 (8.0756e-03)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [81][ 50/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.008)	Loss 7.4154e-03 (7.8444e-03)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [81][ 60/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.007)	Loss 4.8438e-03 (7.9014e-03)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [81][ 70/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.007)	Loss 1.8338e-03 (7.7931e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [81][ 80/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.007)	Loss 2.8434e-03 (7.6867e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [81][ 90/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.3936e-03 (7.1910e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [81][100/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 6.7643e-03 (7.0535e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [81][110/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.7897e-03 (6.9113e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [81][120/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.006)	Loss 6.8551e-03 (6.9687e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [81][130/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.4699e-02 (7.6068e-03)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [81][140/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.6767e-03 (7.6057e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [81][150/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.7204e-03 (7.6786e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [81][160/391]	Time  0.131 ( 0.138)	Data  0.001 ( 0.006)	Loss 8.0955e-03 (7.7964e-03)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [81][170/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 6.4775e-03 (7.8045e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [81][180/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.0502e-02 (7.7165e-03)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [81][190/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.3000e-03 (7.6779e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [81][200/391]	Time  0.142 ( 0.138)	Data  0.002 ( 0.005)	Loss 7.9653e-03 (7.5732e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [81][210/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.9934e-03 (7.7392e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [81][220/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.6213e-03 (7.7889e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [81][230/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.7906e-03 (7.7730e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [81][240/391]	Time  0.141 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0313e-02 (7.7594e-03)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [81][250/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1470e-02 (7.7227e-03)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [81][260/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.7738e-04 (7.7365e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [81][270/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.7797e-03 (7.8301e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [81][280/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1901e-02 (7.7453e-03)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [81][290/391]	Time  0.130 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.4382e-03 (7.9132e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [81][300/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.9545e-03 (7.8031e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [81][310/391]	Time  0.128 ( 0.137)	Data  0.002 ( 0.005)	Loss 3.4223e-03 (7.7230e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [81][320/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.9692e-03 (7.7027e-03)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [81][330/391]	Time  0.141 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.2303e-03 (7.6651e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [81][340/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.6836e-03 (7.7492e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [81][350/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.3043e-02 (7.8281e-03)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [81][360/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.8794e-03 (7.8274e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [81][370/391]	Time  0.129 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.2247e-03 (7.8798e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [81][380/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.5728e-03 (7.8942e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [81][390/391]	Time  0.126 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3004e-03 (7.9928e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
## e[81] optimizer.zero_grad (sum) time: 0.2886652946472168
## e[81]       loss.backward (sum) time: 18.51114535331726
## e[81]      optimizer.step (sum) time: 2.9029364585876465
## epoch[81] training(only) time: 53.76775670051575
# Switched to evaluate mode...
Test: [  0/100]	Time  0.208 ( 0.208)	Loss 1.5154e-01 (1.5154e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.041 ( 0.058)	Loss 4.3980e-01 (3.0885e-01)	Acc@1  92.00 ( 93.18)	Acc@5  99.00 ( 99.73)
Test: [ 20/100]	Time  0.043 ( 0.051)	Loss 5.1886e-01 (3.9228e-01)	Acc@1  89.00 ( 92.24)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.041 ( 0.049)	Loss 2.8587e-01 (4.1427e-01)	Acc@1  94.00 ( 92.35)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.042 ( 0.048)	Loss 4.9706e-01 (4.2076e-01)	Acc@1  91.00 ( 92.22)	Acc@5  99.00 ( 99.71)
Test: [ 50/100]	Time  0.042 ( 0.047)	Loss 1.7290e-01 (4.1358e-01)	Acc@1  96.00 ( 92.25)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.041 ( 0.046)	Loss 3.8674e-01 (3.9898e-01)	Acc@1  96.00 ( 92.49)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.045 ( 0.046)	Loss 6.9724e-01 (3.9537e-01)	Acc@1  89.00 ( 92.61)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.045 ( 0.045)	Loss 1.9339e-01 (3.9760e-01)	Acc@1  95.00 ( 92.64)	Acc@5 100.00 ( 99.73)
Test: [ 90/100]	Time  0.041 ( 0.045)	Loss 2.2987e-01 (3.9091e-01)	Acc@1  94.00 ( 92.60)	Acc@5 100.00 ( 99.75)
 * Acc@1 92.650 Acc@5 99.730
### epoch[81] execution time: 58.37281084060669
EPOCH 82
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [82][  0/391]	Time  0.301 ( 0.301)	Data  0.177 ( 0.177)	Loss 1.0283e-02 (1.0283e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [82][ 10/391]	Time  0.135 ( 0.151)	Data  0.001 ( 0.020)	Loss 6.7335e-03 (7.9005e-03)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [82][ 20/391]	Time  0.136 ( 0.144)	Data  0.001 ( 0.012)	Loss 1.8750e-02 (7.0035e-03)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [82][ 30/391]	Time  0.134 ( 0.141)	Data  0.001 ( 0.010)	Loss 1.4869e-02 (6.7735e-03)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [82][ 40/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.008)	Loss 2.7216e-03 (6.4110e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [82][ 50/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.008)	Loss 2.5650e-03 (5.8627e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [82][ 60/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.007)	Loss 4.0142e-03 (6.0315e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [82][ 70/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.007)	Loss 1.9421e-03 (6.1111e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [82][ 80/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.007)	Loss 5.1009e-03 (6.0607e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [82][ 90/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.4737e-03 (6.2177e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [82][100/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 8.5606e-04 (6.1473e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [82][110/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.006)	Loss 6.6220e-03 (6.6088e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [82][120/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.006)	Loss 1.5527e-03 (6.5001e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [82][130/391]	Time  0.134 ( 0.137)	Data  0.002 ( 0.006)	Loss 1.2632e-02 (6.6964e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [82][140/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.006)	Loss 4.2838e-03 (6.5234e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [82][150/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.006)	Loss 5.6252e-03 (6.7378e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [82][160/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.2979e-03 (6.6872e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [82][170/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.8865e-03 (6.8426e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [82][180/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.3974e-02 (6.7676e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [82][190/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.5329e-04 (6.6654e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [82][200/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.6816e-04 (6.5482e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [82][210/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.4443e-03 (6.5487e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [82][220/391]	Time  0.137 ( 0.137)	Data  0.002 ( 0.005)	Loss 9.8146e-04 (6.4883e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [82][230/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.3039e-03 (6.5872e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [82][240/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.6708e-03 (6.7043e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [82][250/391]	Time  0.134 ( 0.137)	Data  0.002 ( 0.005)	Loss 2.3893e-02 (6.7619e-03)	Acc@1  98.44 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [82][260/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.7698e-03 (6.6909e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [82][270/391]	Time  0.128 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.5069e-03 (6.6183e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [82][280/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0439e-02 (6.6560e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [82][290/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3442e-03 (6.6603e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [82][300/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.2865e-03 (6.8250e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [82][310/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3344e-03 (6.7412e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [82][320/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.8563e-03 (6.7109e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [82][330/391]	Time  0.130 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.7390e-03 (6.6458e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [82][340/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.3327e-03 (6.7769e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [82][350/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.4960e-02 (6.8524e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [82][360/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.4288e-03 (6.7862e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [82][370/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.7263e-03 (6.7890e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [82][380/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.6164e-02 (6.9562e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [82][390/391]	Time  0.130 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.1085e-02 (6.9094e-03)	Acc@1  98.75 ( 99.82)	Acc@5 100.00 (100.00)
## e[82] optimizer.zero_grad (sum) time: 0.2901327610015869
## e[82]       loss.backward (sum) time: 18.52309799194336
## e[82]      optimizer.step (sum) time: 2.930385112762451
## epoch[82] training(only) time: 53.58956813812256
# Switched to evaluate mode...
Test: [  0/100]	Time  0.202 ( 0.202)	Loss 1.6352e-01 (1.6352e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.043 ( 0.058)	Loss 4.2490e-01 (3.1197e-01)	Acc@1  92.00 ( 93.45)	Acc@5  99.00 ( 99.45)
Test: [ 20/100]	Time  0.044 ( 0.051)	Loss 5.2675e-01 (4.0248e-01)	Acc@1  88.00 ( 92.29)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.043 ( 0.049)	Loss 2.7113e-01 (4.1938e-01)	Acc@1  94.00 ( 92.39)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.050 ( 0.048)	Loss 5.1358e-01 (4.2876e-01)	Acc@1  90.00 ( 92.12)	Acc@5  99.00 ( 99.71)
Test: [ 50/100]	Time  0.043 ( 0.047)	Loss 1.7452e-01 (4.1983e-01)	Acc@1  95.00 ( 92.14)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.045 ( 0.046)	Loss 4.0398e-01 (4.0500e-01)	Acc@1  96.00 ( 92.39)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.043 ( 0.046)	Loss 6.5236e-01 (3.9778e-01)	Acc@1  89.00 ( 92.49)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.044 ( 0.045)	Loss 1.9521e-01 (3.9931e-01)	Acc@1  97.00 ( 92.59)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.040 ( 0.045)	Loss 2.4050e-01 (3.9164e-01)	Acc@1  94.00 ( 92.59)	Acc@5 100.00 ( 99.77)
 * Acc@1 92.610 Acc@5 99.750
### epoch[82] execution time: 58.199806690216064
EPOCH 83
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [83][  0/391]	Time  0.300 ( 0.300)	Data  0.180 ( 0.180)	Loss 6.3626e-03 (6.3626e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [83][ 10/391]	Time  0.128 ( 0.150)	Data  0.001 ( 0.020)	Loss 1.7237e-03 (4.9361e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [83][ 20/391]	Time  0.134 ( 0.143)	Data  0.001 ( 0.013)	Loss 2.6158e-02 (6.9600e-03)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [83][ 30/391]	Time  0.135 ( 0.141)	Data  0.001 ( 0.010)	Loss 3.5170e-03 (6.7457e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [83][ 40/391]	Time  0.130 ( 0.140)	Data  0.001 ( 0.009)	Loss 2.2061e-03 (6.1551e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [83][ 50/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.008)	Loss 2.5748e-03 (6.4488e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [83][ 60/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.007)	Loss 3.1842e-03 (6.4086e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [83][ 70/391]	Time  0.131 ( 0.139)	Data  0.001 ( 0.007)	Loss 1.1181e-02 (6.9856e-03)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [83][ 80/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.007)	Loss 1.1218e-02 (7.2132e-03)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [83][ 90/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.5559e-03 (6.8378e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [83][100/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.1712e-03 (6.6826e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [83][110/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.6527e-03 (6.7899e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [83][120/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 6.4830e-03 (6.5359e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [83][130/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 8.8821e-03 (6.8819e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [83][140/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 7.7242e-03 (6.7748e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [83][150/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.9567e-03 (6.7662e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [83][160/391]	Time  0.129 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.7943e-03 (6.6815e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [83][170/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.4738e-03 (6.8150e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [83][180/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.6670e-03 (6.8406e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [83][190/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.9786e-03 (6.9477e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [83][200/391]	Time  0.133 ( 0.137)	Data  0.002 ( 0.005)	Loss 2.3359e-02 (7.1601e-03)	Acc@1  98.44 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [83][210/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.0873e-03 (7.1711e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [83][220/391]	Time  0.129 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.9953e-03 (7.2112e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [83][230/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.3899e-03 (7.3618e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [83][240/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.4260e-02 (7.3462e-03)	Acc@1  98.44 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [83][250/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.0187e-03 (7.2824e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [83][260/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3750e-03 (7.3218e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [83][270/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.6542e-02 (7.4012e-03)	Acc@1  97.66 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [83][280/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.7615e-02 (7.4452e-03)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [83][290/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0031e-02 (7.3900e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [83][300/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.2514e-02 (7.3259e-03)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [83][310/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.9602e-03 (7.2406e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [83][320/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.4141e-03 (7.1792e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [83][330/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3607e-03 (7.1455e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [83][340/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.5903e-03 (7.0879e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [83][350/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.7992e-03 (7.1663e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [83][360/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0883e-03 (7.1508e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [83][370/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.9840e-03 (7.0919e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [83][380/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.5596e-03 (7.0807e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [83][390/391]	Time  0.127 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.4275e-02 (7.1791e-03)	Acc@1  97.50 ( 99.79)	Acc@5 100.00 (100.00)
## e[83] optimizer.zero_grad (sum) time: 0.2858257293701172
## e[83]       loss.backward (sum) time: 18.543573141098022
## e[83]      optimizer.step (sum) time: 2.8723671436309814
## epoch[83] training(only) time: 53.744123458862305
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 1.6556e-01 (1.6556e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.042 ( 0.057)	Loss 4.2066e-01 (3.0673e-01)	Acc@1  91.00 ( 93.27)	Acc@5  99.00 ( 99.55)
Test: [ 20/100]	Time  0.046 ( 0.051)	Loss 5.4004e-01 (4.0073e-01)	Acc@1  88.00 ( 92.14)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.042 ( 0.048)	Loss 2.7576e-01 (4.1912e-01)	Acc@1  94.00 ( 92.32)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.045 ( 0.047)	Loss 5.1643e-01 (4.2720e-01)	Acc@1  89.00 ( 91.98)	Acc@5  99.00 ( 99.68)
Test: [ 50/100]	Time  0.044 ( 0.047)	Loss 1.7459e-01 (4.1877e-01)	Acc@1  96.00 ( 92.04)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.042 ( 0.046)	Loss 3.9682e-01 (4.0330e-01)	Acc@1  95.00 ( 92.30)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.043 ( 0.046)	Loss 6.9764e-01 (3.9826e-01)	Acc@1  88.00 ( 92.42)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.046 ( 0.046)	Loss 1.9039e-01 (4.0094e-01)	Acc@1  96.00 ( 92.51)	Acc@5 100.00 ( 99.72)
Test: [ 90/100]	Time  0.043 ( 0.045)	Loss 2.4651e-01 (3.9350e-01)	Acc@1  94.00 ( 92.51)	Acc@5 100.00 ( 99.75)
 * Acc@1 92.560 Acc@5 99.740
### epoch[83] execution time: 58.372706174850464
EPOCH 84
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [84][  0/391]	Time  0.297 ( 0.297)	Data  0.176 ( 0.176)	Loss 3.1449e-03 (3.1449e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [84][ 10/391]	Time  0.134 ( 0.150)	Data  0.001 ( 0.020)	Loss 1.2649e-02 (8.2061e-03)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [84][ 20/391]	Time  0.137 ( 0.143)	Data  0.001 ( 0.013)	Loss 1.2772e-02 (6.6326e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [84][ 30/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.010)	Loss 6.5400e-04 (7.7154e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [84][ 40/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.009)	Loss 3.9615e-02 (8.3000e-03)	Acc@1  98.44 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [84][ 50/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.008)	Loss 1.3065e-02 (8.8329e-03)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [84][ 60/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.007)	Loss 9.0024e-03 (9.2323e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [84][ 70/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.007)	Loss 3.0436e-03 (8.8205e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [84][ 80/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.1710e-03 (8.4005e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [84][ 90/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 5.5146e-04 (8.0638e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [84][100/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.6133e-03 (7.7639e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [84][110/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.0212e-03 (7.9416e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [84][120/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.7489e-03 (7.9603e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [84][130/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.9992e-03 (8.0063e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [84][140/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.006)	Loss 5.2538e-03 (8.0657e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [84][150/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.006)	Loss 7.4508e-03 (7.9379e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [84][160/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0476e-02 (7.9345e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [84][170/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.2941e-03 (7.7654e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [84][180/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.1854e-03 (7.6111e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [84][190/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.7189e-03 (7.4699e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [84][200/391]	Time  0.130 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.8072e-02 (7.5117e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [84][210/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.5449e-02 (7.6126e-03)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [84][220/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.1298e-04 (7.5529e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [84][230/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.5581e-03 (7.6108e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [84][240/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.4661e-03 (7.6338e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [84][250/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.2211e-02 (7.6867e-03)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [84][260/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.2993e-03 (7.6322e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [84][270/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.1097e-04 (7.5727e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [84][280/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.0472e-03 (7.5951e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [84][290/391]	Time  0.127 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0912e-03 (7.6277e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [84][300/391]	Time  0.135 ( 0.137)	Data  0.002 ( 0.005)	Loss 3.0703e-03 (7.5116e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [84][310/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.2772e-02 (7.4358e-03)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [84][320/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.2988e-02 (7.4911e-03)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [84][330/391]	Time  0.134 ( 0.137)	Data  0.002 ( 0.005)	Loss 1.9857e-03 (7.5753e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [84][340/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.8512e-03 (7.6042e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [84][350/391]	Time  0.141 ( 0.137)	Data  0.002 ( 0.005)	Loss 1.9285e-03 (7.5081e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [84][360/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.3029e-03 (7.4946e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [84][370/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3692e-02 (7.5670e-03)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [84][380/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.1261e-03 (7.5795e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [84][390/391]	Time  0.126 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3483e-03 (7.5304e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
## e[84] optimizer.zero_grad (sum) time: 0.28641247749328613
## e[84]       loss.backward (sum) time: 18.551777124404907
## e[84]      optimizer.step (sum) time: 2.9077510833740234
## epoch[84] training(only) time: 53.66160845756531
# Switched to evaluate mode...
Test: [  0/100]	Time  0.198 ( 0.198)	Loss 1.8453e-01 (1.8453e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.045 ( 0.058)	Loss 4.6594e-01 (3.0699e-01)	Acc@1  93.00 ( 93.36)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.044 ( 0.051)	Loss 5.3141e-01 (3.9872e-01)	Acc@1  90.00 ( 92.52)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.045 ( 0.049)	Loss 2.5978e-01 (4.1798e-01)	Acc@1  94.00 ( 92.52)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.052 ( 0.048)	Loss 4.7088e-01 (4.2272e-01)	Acc@1  91.00 ( 92.34)	Acc@5  99.00 ( 99.68)
Test: [ 50/100]	Time  0.043 ( 0.047)	Loss 1.9132e-01 (4.1570e-01)	Acc@1  96.00 ( 92.27)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.041 ( 0.046)	Loss 4.1671e-01 (3.9987e-01)	Acc@1  96.00 ( 92.54)	Acc@5 100.00 ( 99.72)
Test: [ 70/100]	Time  0.043 ( 0.046)	Loss 6.1555e-01 (3.9509e-01)	Acc@1  88.00 ( 92.61)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.042 ( 0.045)	Loss 2.1488e-01 (3.9842e-01)	Acc@1  94.00 ( 92.68)	Acc@5 100.00 ( 99.69)
Test: [ 90/100]	Time  0.043 ( 0.045)	Loss 2.7491e-01 (3.9004e-01)	Acc@1  95.00 ( 92.70)	Acc@5 100.00 ( 99.73)
 * Acc@1 92.700 Acc@5 99.710
### epoch[84] execution time: 58.29003071784973
EPOCH 85
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [85][  0/391]	Time  0.300 ( 0.300)	Data  0.178 ( 0.178)	Loss 3.1533e-03 (3.1533e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [85][ 10/391]	Time  0.137 ( 0.151)	Data  0.001 ( 0.020)	Loss 1.8627e-03 (5.5913e-03)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [85][ 20/391]	Time  0.136 ( 0.144)	Data  0.001 ( 0.012)	Loss 3.6343e-04 (4.5429e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [85][ 30/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.010)	Loss 3.5139e-03 (5.2315e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [85][ 40/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.008)	Loss 3.5995e-03 (5.3646e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [85][ 50/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.008)	Loss 2.0937e-03 (5.2618e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [85][ 60/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.007)	Loss 2.4583e-03 (5.9903e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [85][ 70/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.007)	Loss 1.2309e-03 (5.9647e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [85][ 80/391]	Time  0.129 ( 0.139)	Data  0.001 ( 0.006)	Loss 5.4053e-03 (6.2279e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [85][ 90/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.5962e-03 (6.2681e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [85][100/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.4072e-02 (6.2796e-03)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [85][110/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.3505e-02 (6.3728e-03)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [85][120/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.4541e-02 (6.6766e-03)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [85][130/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.7446e-03 (6.8890e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [85][140/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.5869e-02 (6.9804e-03)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [85][150/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.5146e-03 (7.1262e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [85][160/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.3524e-03 (7.1125e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [85][170/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.2531e-03 (6.9890e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [85][180/391]	Time  0.132 ( 0.137)	Data  0.002 ( 0.005)	Loss 8.2814e-03 (7.1154e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [85][190/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.4607e-02 (7.1704e-03)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [85][200/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.4893e-03 (6.9726e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [85][210/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.0789e-03 (6.8264e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [85][220/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0055e-03 (6.8504e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [85][230/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3435e-02 (6.9893e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [85][240/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.2530e-04 (6.9182e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [85][250/391]	Time  0.133 ( 0.137)	Data  0.002 ( 0.005)	Loss 3.3807e-03 (6.9324e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [85][260/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.2992e-03 (6.8979e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [85][270/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.9974e-04 (6.9784e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [85][280/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.6967e-03 (6.8953e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [85][290/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.2247e-03 (7.0663e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [85][300/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.8218e-03 (7.1131e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [85][310/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.2895e-03 (7.0891e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [85][320/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.6683e-03 (7.0192e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [85][330/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.1968e-04 (6.9386e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [85][340/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.9143e-03 (6.8870e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [85][350/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.9233e-03 (6.8517e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [85][360/391]	Time  0.135 ( 0.137)	Data  0.002 ( 0.005)	Loss 4.5949e-03 (6.7699e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [85][370/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.9876e-03 (6.7069e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [85][380/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.0867e-03 (6.6827e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [85][390/391]	Time  0.124 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.0421e-03 (6.7722e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
## e[85] optimizer.zero_grad (sum) time: 0.28514933586120605
## e[85]       loss.backward (sum) time: 18.560497045516968
## e[85]      optimizer.step (sum) time: 2.9559855461120605
## epoch[85] training(only) time: 53.71549701690674
# Switched to evaluate mode...
Test: [  0/100]	Time  0.207 ( 0.207)	Loss 1.5464e-01 (1.5464e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.043 ( 0.060)	Loss 4.1939e-01 (3.1159e-01)	Acc@1  91.00 ( 93.00)	Acc@5  99.00 ( 99.45)
Test: [ 20/100]	Time  0.044 ( 0.052)	Loss 5.8633e-01 (4.0673e-01)	Acc@1  88.00 ( 92.14)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.043 ( 0.049)	Loss 2.6134e-01 (4.2253e-01)	Acc@1  94.00 ( 92.32)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.046 ( 0.048)	Loss 4.9708e-01 (4.3135e-01)	Acc@1  90.00 ( 91.98)	Acc@5  99.00 ( 99.71)
Test: [ 50/100]	Time  0.046 ( 0.047)	Loss 1.3377e-01 (4.2510e-01)	Acc@1  97.00 ( 92.02)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.043 ( 0.047)	Loss 4.0445e-01 (4.0756e-01)	Acc@1  96.00 ( 92.28)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.042 ( 0.046)	Loss 6.1918e-01 (4.0173e-01)	Acc@1  89.00 ( 92.45)	Acc@5 100.00 ( 99.80)
Test: [ 80/100]	Time  0.043 ( 0.046)	Loss 1.8183e-01 (4.0374e-01)	Acc@1  95.00 ( 92.52)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.044 ( 0.046)	Loss 2.2808e-01 (3.9626e-01)	Acc@1  94.00 ( 92.58)	Acc@5 100.00 ( 99.81)
 * Acc@1 92.610 Acc@5 99.800
### epoch[85] execution time: 58.37427020072937
EPOCH 86
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [86][  0/391]	Time  0.300 ( 0.300)	Data  0.180 ( 0.180)	Loss 1.2226e-03 (1.2226e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [86][ 10/391]	Time  0.136 ( 0.151)	Data  0.001 ( 0.020)	Loss 6.6001e-03 (5.4681e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [86][ 20/391]	Time  0.140 ( 0.144)	Data  0.001 ( 0.013)	Loss 3.8055e-03 (5.9065e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [86][ 30/391]	Time  0.134 ( 0.141)	Data  0.001 ( 0.010)	Loss 7.0634e-03 (6.1559e-03)	Acc@1  99.22 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [86][ 40/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.009)	Loss 1.9600e-02 (5.7241e-03)	Acc@1  99.22 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [86][ 50/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.008)	Loss 1.3337e-02 (6.5519e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [86][ 60/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.007)	Loss 1.0953e-03 (6.5533e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [86][ 70/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.007)	Loss 7.8826e-03 (6.6078e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [86][ 80/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.007)	Loss 5.6859e-04 (6.8288e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [86][ 90/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.8618e-03 (6.9208e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [86][100/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.0406e-03 (6.8780e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [86][110/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.9285e-03 (6.8071e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [86][120/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.8095e-03 (7.1007e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [86][130/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.5694e-02 (7.4000e-03)	Acc@1  98.44 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [86][140/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.006)	Loss 3.7352e-03 (7.4033e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [86][150/391]	Time  0.131 ( 0.138)	Data  0.001 ( 0.006)	Loss 7.3217e-03 (7.4088e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [86][160/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.8481e-02 (7.4261e-03)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [86][170/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.2335e-03 (7.5104e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [86][180/391]	Time  0.131 ( 0.138)	Data  0.002 ( 0.005)	Loss 2.7428e-03 (7.3130e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [86][190/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.005)	Loss 5.9822e-03 (7.1174e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [86][200/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.2158e-03 (6.9822e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [86][210/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3445e-03 (7.0941e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [86][220/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.5454e-04 (7.0043e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [86][230/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.2650e-03 (6.8891e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [86][240/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.2190e-02 (6.9284e-03)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [86][250/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.2496e-03 (6.9089e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [86][260/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3752e-02 (6.8996e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [86][270/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.3244e-03 (6.9126e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [86][280/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.3750e-03 (6.8254e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [86][290/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.1076e-03 (6.8806e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [86][300/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.0134e-03 (7.0917e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [86][310/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.6816e-03 (7.1116e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [86][320/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 8.0191e-03 (7.0890e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [86][330/391]	Time  0.135 ( 0.137)	Data  0.002 ( 0.005)	Loss 2.8052e-02 (7.1606e-03)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [86][340/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.9676e-03 (7.1333e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [86][350/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.2633e-03 (7.0313e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [86][360/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.7284e-03 (6.9681e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [86][370/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.6726e-02 (7.1086e-03)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [86][380/391]	Time  0.142 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.1920e-03 (7.1586e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [86][390/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.6956e-03 (7.1208e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
## e[86] optimizer.zero_grad (sum) time: 0.28475499153137207
## e[86]       loss.backward (sum) time: 18.546202421188354
## e[86]      optimizer.step (sum) time: 2.8625869750976562
## epoch[86] training(only) time: 53.80590009689331
# Switched to evaluate mode...
Test: [  0/100]	Time  0.201 ( 0.201)	Loss 1.7474e-01 (1.7474e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.059)	Loss 4.3517e-01 (3.1999e-01)	Acc@1  92.00 ( 93.55)	Acc@5  98.00 ( 99.55)
Test: [ 20/100]	Time  0.052 ( 0.053)	Loss 5.3120e-01 (4.0378e-01)	Acc@1  88.00 ( 92.43)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.044 ( 0.050)	Loss 2.5228e-01 (4.2132e-01)	Acc@1  94.00 ( 92.42)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.043 ( 0.049)	Loss 4.8853e-01 (4.2809e-01)	Acc@1  89.00 ( 92.07)	Acc@5  99.00 ( 99.66)
Test: [ 50/100]	Time  0.044 ( 0.048)	Loss 1.7939e-01 (4.2201e-01)	Acc@1  96.00 ( 92.10)	Acc@5 100.00 ( 99.69)
Test: [ 60/100]	Time  0.044 ( 0.047)	Loss 4.0305e-01 (4.0624e-01)	Acc@1  96.00 ( 92.31)	Acc@5 100.00 ( 99.72)
Test: [ 70/100]	Time  0.045 ( 0.047)	Loss 7.1979e-01 (4.0133e-01)	Acc@1  88.00 ( 92.44)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.044 ( 0.046)	Loss 2.1687e-01 (4.0448e-01)	Acc@1  94.00 ( 92.51)	Acc@5 100.00 ( 99.72)
Test: [ 90/100]	Time  0.042 ( 0.046)	Loss 2.6393e-01 (3.9643e-01)	Acc@1  95.00 ( 92.57)	Acc@5 100.00 ( 99.74)
 * Acc@1 92.650 Acc@5 99.730
### epoch[86] execution time: 58.48616075515747
EPOCH 87
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [87][  0/391]	Time  0.303 ( 0.303)	Data  0.176 ( 0.176)	Loss 2.4290e-02 (2.4290e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [87][ 10/391]	Time  0.132 ( 0.152)	Data  0.001 ( 0.020)	Loss 5.3152e-03 (6.9803e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [87][ 20/391]	Time  0.136 ( 0.145)	Data  0.001 ( 0.012)	Loss 4.0987e-03 (6.1466e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [87][ 30/391]	Time  0.135 ( 0.142)	Data  0.001 ( 0.010)	Loss 3.2384e-02 (6.8249e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [87][ 40/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.008)	Loss 2.8072e-02 (7.9152e-03)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [87][ 50/391]	Time  0.132 ( 0.140)	Data  0.001 ( 0.008)	Loss 1.7464e-03 (7.9304e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [87][ 60/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.007)	Loss 4.8125e-03 (7.4823e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [87][ 70/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.007)	Loss 8.5716e-03 (7.4658e-03)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [87][ 80/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.006)	Loss 9.4959e-04 (7.4723e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [87][ 90/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 5.5320e-03 (7.3921e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [87][100/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.2492e-02 (7.2130e-03)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [87][110/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.006)	Loss 7.9874e-04 (7.1648e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [87][120/391]	Time  0.133 ( 0.138)	Data  0.002 ( 0.006)	Loss 5.6272e-03 (7.0324e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [87][130/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 4.5724e-03 (6.9416e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [87][140/391]	Time  0.131 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.7685e-03 (6.8791e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [87][150/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.3442e-03 (7.0432e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [87][160/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.2746e-03 (7.0533e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [87][170/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 3.3588e-03 (6.9353e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [87][180/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.005)	Loss 7.5659e-03 (7.0768e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [87][190/391]	Time  0.136 ( 0.138)	Data  0.002 ( 0.005)	Loss 2.5694e-03 (7.1912e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [87][200/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.5389e-03 (7.0573e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [87][210/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.3097e-03 (6.8857e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [87][220/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 6.7664e-03 (6.9392e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [87][230/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.2974e-03 (6.9070e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [87][240/391]	Time  0.135 ( 0.137)	Data  0.002 ( 0.005)	Loss 8.0851e-03 (6.8012e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [87][250/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.2117e-03 (6.8062e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [87][260/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.6027e-02 (6.7597e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [87][270/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.1193e-03 (6.6592e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [87][280/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.4114e-03 (6.6392e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [87][290/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.8468e-03 (6.5352e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [87][300/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.2959e-03 (6.5596e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [87][310/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.6989e-04 (6.5343e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [87][320/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.0502e-02 (6.5882e-03)	Acc@1  98.44 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [87][330/391]	Time  0.129 ( 0.137)	Data  0.001 ( 0.005)	Loss 9.3193e-03 (6.6571e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [87][340/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.3873e-03 (6.6665e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [87][350/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.7158e-02 (6.6521e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [87][360/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.2861e-03 (6.6135e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [87][370/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.4592e-02 (6.6765e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [87][380/391]	Time  0.136 ( 0.137)	Data  0.002 ( 0.005)	Loss 5.3854e-03 (6.6981e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [87][390/391]	Time  0.126 ( 0.137)	Data  0.001 ( 0.005)	Loss 5.0284e-02 (6.7605e-03)	Acc@1  98.75 ( 99.81)	Acc@5 100.00 (100.00)
## e[87] optimizer.zero_grad (sum) time: 0.2892622947692871
## e[87]       loss.backward (sum) time: 18.570920944213867
## e[87]      optimizer.step (sum) time: 2.8957245349884033
## epoch[87] training(only) time: 53.78194785118103
# Switched to evaluate mode...
Test: [  0/100]	Time  0.192 ( 0.192)	Loss 1.9021e-01 (1.9021e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.043 ( 0.057)	Loss 4.1957e-01 (3.1369e-01)	Acc@1  92.00 ( 93.09)	Acc@5  99.00 ( 99.73)
Test: [ 20/100]	Time  0.046 ( 0.051)	Loss 5.4490e-01 (4.0685e-01)	Acc@1  89.00 ( 92.24)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.044 ( 0.048)	Loss 2.3257e-01 (4.2116e-01)	Acc@1  94.00 ( 92.42)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.047 ( 0.047)	Loss 4.6323e-01 (4.2716e-01)	Acc@1  89.00 ( 92.05)	Acc@5  99.00 ( 99.71)
Test: [ 50/100]	Time  0.041 ( 0.047)	Loss 1.8190e-01 (4.2125e-01)	Acc@1  96.00 ( 92.02)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.043 ( 0.046)	Loss 3.8191e-01 (4.0394e-01)	Acc@1  96.00 ( 92.30)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.044 ( 0.045)	Loss 6.4594e-01 (3.9775e-01)	Acc@1  88.00 ( 92.41)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.044 ( 0.045)	Loss 2.2792e-01 (4.0143e-01)	Acc@1  95.00 ( 92.48)	Acc@5 100.00 ( 99.72)
Test: [ 90/100]	Time  0.043 ( 0.045)	Loss 2.9449e-01 (3.9312e-01)	Acc@1  94.00 ( 92.56)	Acc@5 100.00 ( 99.75)
 * Acc@1 92.590 Acc@5 99.740
### epoch[87] execution time: 58.38598084449768
EPOCH 88
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [88][  0/391]	Time  0.295 ( 0.295)	Data  0.179 ( 0.179)	Loss 4.5890e-04 (4.5890e-04)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [88][ 10/391]	Time  0.135 ( 0.150)	Data  0.001 ( 0.020)	Loss 9.7893e-03 (3.6870e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [88][ 20/391]	Time  0.132 ( 0.144)	Data  0.001 ( 0.013)	Loss 3.4928e-03 (4.5354e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [88][ 30/391]	Time  0.133 ( 0.141)	Data  0.001 ( 0.010)	Loss 3.7585e-03 (5.2596e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [88][ 40/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.009)	Loss 1.6451e-03 (5.4014e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [88][ 50/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.008)	Loss 4.0981e-03 (5.3895e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [88][ 60/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.007)	Loss 2.2386e-02 (5.5974e-03)	Acc@1  98.44 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [88][ 70/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.007)	Loss 1.5056e-03 (5.5973e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [88][ 80/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.007)	Loss 4.4184e-03 (6.0902e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [88][ 90/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 8.6994e-03 (6.2130e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [88][100/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.3125e-03 (6.0737e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [88][110/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.006)	Loss 7.7173e-04 (6.0704e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [88][120/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.006)	Loss 8.3850e-03 (6.0982e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [88][130/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.9100e-03 (6.2118e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [88][140/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.1648e-03 (6.5830e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [88][150/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.7695e-03 (6.4744e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [88][160/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.2707e-02 (6.3782e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [88][170/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.7807e-03 (6.2171e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [88][180/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.005)	Loss 7.1339e-03 (6.1711e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [88][190/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 9.0903e-04 (5.9648e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [88][200/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.3049e-03 (6.0387e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [88][210/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.005)	Loss 6.6540e-03 (6.0196e-03)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [88][220/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.4416e-03 (5.9119e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [88][230/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.005)	Loss 4.1514e-03 (5.9593e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [88][240/391]	Time  0.141 ( 0.138)	Data  0.002 ( 0.005)	Loss 2.2869e-03 (6.2011e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [88][250/391]	Time  0.138 ( 0.138)	Data  0.002 ( 0.005)	Loss 7.5909e-04 (6.3218e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [88][260/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.005)	Loss 9.3558e-03 (6.3182e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [88][270/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.4319e-03 (6.2827e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [88][280/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.5574e-02 (6.4775e-03)	Acc@1  98.44 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [88][290/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.005)	Loss 4.5491e-03 (6.4114e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [88][300/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.8695e-03 (6.2897e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [88][310/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.3364e-03 (6.2158e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [88][320/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.005)	Loss 5.3012e-04 (6.2386e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [88][330/391]	Time  0.134 ( 0.138)	Data  0.002 ( 0.005)	Loss 1.6693e-02 (6.2586e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [88][340/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.3039e-03 (6.2751e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [88][350/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.8687e-03 (6.3212e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [88][360/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.9468e-03 (6.3053e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [88][370/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.005)	Loss 1.9643e-02 (6.3431e-03)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [88][380/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.005)	Loss 2.7180e-02 (6.3786e-03)	Acc@1  98.44 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [88][390/391]	Time  0.129 ( 0.138)	Data  0.001 ( 0.005)	Loss 7.2722e-03 (6.4018e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
## e[88] optimizer.zero_grad (sum) time: 0.28840208053588867
## e[88]       loss.backward (sum) time: 18.586519479751587
## e[88]      optimizer.step (sum) time: 2.8730576038360596
## epoch[88] training(only) time: 53.94533324241638
# Switched to evaluate mode...
Test: [  0/100]	Time  0.204 ( 0.204)	Loss 1.7807e-01 (1.7807e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.043 ( 0.059)	Loss 4.1479e-01 (3.0928e-01)	Acc@1  93.00 ( 93.64)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.042 ( 0.052)	Loss 5.3070e-01 (3.9809e-01)	Acc@1  90.00 ( 92.57)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.042 ( 0.049)	Loss 2.5508e-01 (4.1516e-01)	Acc@1  95.00 ( 92.68)	Acc@5 100.00 ( 99.77)
Test: [ 40/100]	Time  0.044 ( 0.048)	Loss 4.6760e-01 (4.2135e-01)	Acc@1  90.00 ( 92.37)	Acc@5  99.00 ( 99.78)
Test: [ 50/100]	Time  0.043 ( 0.047)	Loss 1.7513e-01 (4.1510e-01)	Acc@1  96.00 ( 92.33)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.044 ( 0.047)	Loss 4.1263e-01 (4.0036e-01)	Acc@1  96.00 ( 92.46)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.043 ( 0.046)	Loss 6.7913e-01 (3.9607e-01)	Acc@1  88.00 ( 92.56)	Acc@5 100.00 ( 99.80)
Test: [ 80/100]	Time  0.043 ( 0.046)	Loss 2.1063e-01 (3.9940e-01)	Acc@1  95.00 ( 92.64)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.042 ( 0.046)	Loss 3.1381e-01 (3.9163e-01)	Acc@1  93.00 ( 92.69)	Acc@5 100.00 ( 99.80)
 * Acc@1 92.690 Acc@5 99.790
### epoch[88] execution time: 58.60519552230835
EPOCH 89
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [89][  0/391]	Time  0.283 ( 0.283)	Data  0.168 ( 0.168)	Loss 3.0605e-03 (3.0605e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [89][ 10/391]	Time  0.140 ( 0.149)	Data  0.001 ( 0.019)	Loss 1.3867e-02 (7.6659e-03)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [89][ 20/391]	Time  0.131 ( 0.143)	Data  0.001 ( 0.012)	Loss 1.1259e-03 (7.9779e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [89][ 30/391]	Time  0.132 ( 0.141)	Data  0.001 ( 0.009)	Loss 1.3248e-03 (7.0699e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [89][ 40/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.008)	Loss 3.9912e-03 (6.5044e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [89][ 50/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.008)	Loss 1.4195e-03 (6.5317e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [89][ 60/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.007)	Loss 2.2434e-03 (6.4975e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [89][ 70/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.007)	Loss 1.7613e-03 (6.3646e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [89][ 80/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.8358e-03 (6.0957e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [89][ 90/391]	Time  0.136 ( 0.138)	Data  0.002 ( 0.006)	Loss 9.1577e-03 (6.3275e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [89][100/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.006)	Loss 9.8573e-03 (6.2829e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [89][110/391]	Time  0.129 ( 0.138)	Data  0.001 ( 0.006)	Loss 9.6320e-03 (6.4013e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [89][120/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.006)	Loss 2.7776e-03 (6.4638e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [89][130/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.006)	Loss 1.2657e-02 (6.3999e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [89][140/391]	Time  0.136 ( 0.138)	Data  0.002 ( 0.006)	Loss 8.0028e-03 (6.4224e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [89][150/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.005)	Loss 8.1696e-04 (6.5188e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [89][160/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.8918e-02 (7.1883e-03)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [89][170/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.0887e-02 (7.0487e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [89][180/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.7732e-02 (7.2813e-03)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [89][190/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.9286e-03 (7.1672e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [89][200/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.2697e-02 (7.1963e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [89][210/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.6902e-03 (7.0120e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [89][220/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.5627e-03 (6.8522e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [89][230/391]	Time  0.141 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.2436e-02 (7.1113e-03)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [89][240/391]	Time  0.138 ( 0.137)	Data  0.002 ( 0.005)	Loss 3.0799e-03 (7.1295e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [89][250/391]	Time  0.134 ( 0.137)	Data  0.002 ( 0.005)	Loss 4.6161e-03 (7.2483e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [89][260/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3646e-03 (7.0993e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [89][270/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.0425e-03 (7.0500e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [89][280/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.005)	Loss 4.0091e-03 (7.1357e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [89][290/391]	Time  0.134 ( 0.137)	Data  0.002 ( 0.005)	Loss 4.6860e-03 (7.1827e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [89][300/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.0655e-03 (7.0136e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [89][310/391]	Time  0.141 ( 0.137)	Data  0.001 ( 0.005)	Loss 7.7127e-03 (6.9040e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [89][320/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.8014e-03 (6.8006e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [89][330/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.5330e-03 (6.8205e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [89][340/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.7647e-03 (6.8753e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [89][350/391]	Time  0.135 ( 0.137)	Data  0.002 ( 0.005)	Loss 4.1180e-03 (6.8266e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [89][360/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.005)	Loss 1.3632e-02 (6.8306e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [89][370/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.6083e-02 (6.8227e-03)	Acc@1  98.44 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [89][380/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.005)	Loss 2.7649e-03 (6.8339e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [89][390/391]	Time  0.126 ( 0.137)	Data  0.001 ( 0.005)	Loss 3.8705e-04 (6.8026e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
## e[89] optimizer.zero_grad (sum) time: 0.2831871509552002
## e[89]       loss.backward (sum) time: 18.55287766456604
## e[89]      optimizer.step (sum) time: 2.9196414947509766
## epoch[89] training(only) time: 53.703726291656494
# Switched to evaluate mode...
Test: [  0/100]	Time  0.209 ( 0.209)	Loss 1.8644e-01 (1.8644e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.042 ( 0.059)	Loss 4.2090e-01 (3.1330e-01)	Acc@1  93.00 ( 93.45)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.042 ( 0.051)	Loss 5.3295e-01 (3.9732e-01)	Acc@1  88.00 ( 92.43)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.042 ( 0.049)	Loss 2.8022e-01 (4.1883e-01)	Acc@1  94.00 ( 92.58)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.043 ( 0.048)	Loss 4.7550e-01 (4.2331e-01)	Acc@1  90.00 ( 92.29)	Acc@5  99.00 ( 99.73)
Test: [ 50/100]	Time  0.044 ( 0.047)	Loss 2.1170e-01 (4.1707e-01)	Acc@1  96.00 ( 92.29)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.042 ( 0.046)	Loss 3.8246e-01 (4.0105e-01)	Acc@1  96.00 ( 92.51)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.043 ( 0.046)	Loss 7.1742e-01 (3.9626e-01)	Acc@1  88.00 ( 92.61)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.042 ( 0.045)	Loss 2.3330e-01 (3.9954e-01)	Acc@1  93.00 ( 92.59)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.042 ( 0.045)	Loss 2.9723e-01 (3.9209e-01)	Acc@1  94.00 ( 92.66)	Acc@5 100.00 ( 99.77)
 * Acc@1 92.710 Acc@5 99.750
### epoch[89] execution time: 58.32080936431885
### Training complete:
#### total training(only) time: 4841.746778488159
##### Total run time: 5262.6544098854065
