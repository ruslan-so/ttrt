# Model: vgg11_bn
# Dataset: cifardecem
# Batch size: 128
# Freezeout: False
# Low precision: False
# Epochs: 90
# Initial learning rate: 0.1
# module_name: models_cifardecem.vgg
<function vgg11_bn at 0x7f0fc91300d0>
# model requested: 'vgg11_bn'
# printing out the model
VGG(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU(inplace=True)
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): ReLU(inplace=True)
    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (13): ReLU(inplace=True)
    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (17): ReLU(inplace=True)
    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): ReLU(inplace=True)
    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): ReLU(inplace=True)
    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (27): ReLU(inplace=True)
    (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (classifier): Sequential(
    (0): Linear(in_features=512, out_features=4096, bias=True)
    (1): ReLU(inplace=True)
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=4096, out_features=4096, bias=True)
    (4): ReLU(inplace=True)
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=4096, out_features=10, bias=True)
  )
)
# model is full precision
# Model: vgg11_bn
# Dataset: cifardecem
# Freezeout: False
# Low precision: False
# Initial learning rate: 0.1
# Criterion: CrossEntropyLoss()
# Optimizer: SGD
#	lr 0.1
#	momentum 0.9
#	dampening 0
#	weight_decay 0.0001
#	nesterov False
CIFAR10 loading and normalization
==> Preparing data..
# CIFAR10 / full precision
Files already downloaded and verified
Files already downloaded and verified
#--> Training data: <--#
#-->>
EPOCH 0
# current learning rate: 0.1
# Switched to train mode...
Epoch: [0][  0/391]	Time  3.400 ( 3.400)	Data  0.105 ( 0.105)	Loss 2.3183e+00 (2.3183e+00)	Acc@1  10.16 ( 10.16)	Acc@5  51.56 ( 51.56)
Epoch: [0][ 10/391]	Time  0.025 ( 0.332)	Data  0.001 ( 0.011)	Loss 2.3056e+00 (2.4862e+00)	Acc@1  14.06 ( 13.78)	Acc@5  65.62 ( 57.03)
Epoch: [0][ 20/391]	Time  0.024 ( 0.186)	Data  0.001 ( 0.007)	Loss 2.1987e+00 (2.3401e+00)	Acc@1  22.66 ( 16.48)	Acc@5  74.22 ( 63.95)
Epoch: [0][ 30/391]	Time  0.031 ( 0.134)	Data  0.001 ( 0.005)	Loss 1.9239e+00 (2.2548e+00)	Acc@1  24.22 ( 18.20)	Acc@5  81.25 ( 68.09)
Epoch: [0][ 40/391]	Time  0.035 ( 0.108)	Data  0.001 ( 0.005)	Loss 2.0181e+00 (2.2097e+00)	Acc@1  28.12 ( 19.26)	Acc@5  78.12 ( 70.71)
Epoch: [0][ 50/391]	Time  0.023 ( 0.092)	Data  0.002 ( 0.004)	Loss 1.9314e+00 (2.1686e+00)	Acc@1  21.88 ( 19.94)	Acc@5  80.47 ( 72.89)
Epoch: [0][ 60/391]	Time  0.022 ( 0.081)	Data  0.001 ( 0.004)	Loss 1.9196e+00 (2.1305e+00)	Acc@1  24.22 ( 20.39)	Acc@5  85.16 ( 74.63)
Epoch: [0][ 70/391]	Time  0.023 ( 0.073)	Data  0.002 ( 0.004)	Loss 2.0035e+00 (2.1094e+00)	Acc@1  16.41 ( 20.92)	Acc@5  85.16 ( 75.57)
Epoch: [0][ 80/391]	Time  0.023 ( 0.067)	Data  0.002 ( 0.004)	Loss 1.8171e+00 (2.0972e+00)	Acc@1  29.69 ( 21.09)	Acc@5  81.25 ( 75.99)
Epoch: [0][ 90/391]	Time  0.023 ( 0.063)	Data  0.001 ( 0.003)	Loss 1.9750e+00 (2.0799e+00)	Acc@1  21.88 ( 21.45)	Acc@5  85.16 ( 76.88)
Epoch: [0][100/391]	Time  0.030 ( 0.059)	Data  0.002 ( 0.003)	Loss 1.9920e+00 (2.0643e+00)	Acc@1  22.66 ( 21.68)	Acc@5  81.25 ( 77.47)
Epoch: [0][110/391]	Time  0.025 ( 0.056)	Data  0.001 ( 0.003)	Loss 1.8997e+00 (2.0507e+00)	Acc@1  24.22 ( 21.76)	Acc@5  82.81 ( 78.09)
Epoch: [0][120/391]	Time  0.023 ( 0.054)	Data  0.001 ( 0.003)	Loss 1.9076e+00 (2.0378e+00)	Acc@1  25.78 ( 22.15)	Acc@5  82.03 ( 78.53)
Epoch: [0][130/391]	Time  0.023 ( 0.052)	Data  0.001 ( 0.003)	Loss 1.8698e+00 (2.0225e+00)	Acc@1  29.69 ( 22.75)	Acc@5  84.38 ( 78.85)
Epoch: [0][140/391]	Time  0.024 ( 0.050)	Data  0.001 ( 0.003)	Loss 1.8460e+00 (2.0095e+00)	Acc@1  27.34 ( 23.09)	Acc@5  81.25 ( 79.29)
Epoch: [0][150/391]	Time  0.024 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.9774e+00 (1.9989e+00)	Acc@1  19.53 ( 23.47)	Acc@5  81.25 ( 79.79)
Epoch: [0][160/391]	Time  0.027 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9040e+00 (1.9880e+00)	Acc@1  21.09 ( 23.80)	Acc@5  86.72 ( 80.10)
Epoch: [0][170/391]	Time  0.022 ( 0.045)	Data  0.002 ( 0.003)	Loss 1.7359e+00 (1.9770e+00)	Acc@1  31.25 ( 24.10)	Acc@5  92.19 ( 80.46)
Epoch: [0][180/391]	Time  0.023 ( 0.044)	Data  0.001 ( 0.003)	Loss 1.6283e+00 (1.9670e+00)	Acc@1  34.38 ( 24.41)	Acc@5  91.41 ( 80.76)
Epoch: [0][190/391]	Time  0.023 ( 0.044)	Data  0.001 ( 0.003)	Loss 1.8672e+00 (1.9606e+00)	Acc@1  32.81 ( 24.64)	Acc@5  86.72 ( 80.98)
Epoch: [0][200/391]	Time  0.023 ( 0.043)	Data  0.002 ( 0.003)	Loss 1.8871e+00 (1.9522e+00)	Acc@1  25.00 ( 24.94)	Acc@5  82.03 ( 81.24)
Epoch: [0][210/391]	Time  0.023 ( 0.042)	Data  0.002 ( 0.003)	Loss 1.8082e+00 (1.9473e+00)	Acc@1  28.12 ( 25.10)	Acc@5  88.28 ( 81.43)
Epoch: [0][220/391]	Time  0.028 ( 0.041)	Data  0.001 ( 0.003)	Loss 2.0318e+00 (1.9424e+00)	Acc@1  29.69 ( 25.31)	Acc@5  82.81 ( 81.60)
Epoch: [0][230/391]	Time  0.023 ( 0.040)	Data  0.001 ( 0.003)	Loss 1.6770e+00 (1.9352e+00)	Acc@1  30.47 ( 25.49)	Acc@5  86.72 ( 81.82)
Epoch: [0][240/391]	Time  0.023 ( 0.040)	Data  0.001 ( 0.003)	Loss 1.7289e+00 (1.9266e+00)	Acc@1  32.81 ( 25.79)	Acc@5  88.28 ( 82.10)
Epoch: [0][250/391]	Time  0.023 ( 0.039)	Data  0.002 ( 0.003)	Loss 1.6879e+00 (1.9178e+00)	Acc@1  34.38 ( 26.06)	Acc@5  84.38 ( 82.36)
Epoch: [0][260/391]	Time  0.027 ( 0.039)	Data  0.003 ( 0.003)	Loss 1.7912e+00 (1.9112e+00)	Acc@1  35.16 ( 26.26)	Acc@5  84.38 ( 82.55)
Epoch: [0][270/391]	Time  0.032 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.7023e+00 (1.9052e+00)	Acc@1  32.81 ( 26.50)	Acc@5  90.62 ( 82.77)
Epoch: [0][280/391]	Time  0.026 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.6235e+00 (1.8981e+00)	Acc@1  30.47 ( 26.78)	Acc@5  91.41 ( 82.97)
Epoch: [0][290/391]	Time  0.023 ( 0.037)	Data  0.001 ( 0.003)	Loss 1.7451e+00 (1.8901e+00)	Acc@1  32.03 ( 27.04)	Acc@5  92.97 ( 83.23)
Epoch: [0][300/391]	Time  0.031 ( 0.037)	Data  0.002 ( 0.003)	Loss 1.7359e+00 (1.8845e+00)	Acc@1  24.22 ( 27.27)	Acc@5  90.62 ( 83.42)
Epoch: [0][310/391]	Time  0.024 ( 0.037)	Data  0.002 ( 0.003)	Loss 1.8839e+00 (1.8793e+00)	Acc@1  24.22 ( 27.44)	Acc@5  82.81 ( 83.60)
Epoch: [0][320/391]	Time  0.033 ( 0.036)	Data  0.003 ( 0.003)	Loss 1.4573e+00 (1.8732e+00)	Acc@1  37.50 ( 27.64)	Acc@5  95.31 ( 83.76)
Epoch: [0][330/391]	Time  0.030 ( 0.036)	Data  0.000 ( 0.003)	Loss 1.6016e+00 (1.8668e+00)	Acc@1  34.38 ( 27.82)	Acc@5  91.41 ( 83.94)
Epoch: [0][340/391]	Time  0.023 ( 0.036)	Data  0.002 ( 0.003)	Loss 1.6400e+00 (1.8609e+00)	Acc@1  37.50 ( 28.07)	Acc@5  88.28 ( 84.10)
Epoch: [0][350/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.7457e+00 (1.8551e+00)	Acc@1  29.69 ( 28.33)	Acc@5  88.28 ( 84.23)
Epoch: [0][360/391]	Time  0.024 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.7684e+00 (1.8508e+00)	Acc@1  32.81 ( 28.52)	Acc@5  89.84 ( 84.37)
Epoch: [0][370/391]	Time  0.023 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.7786e+00 (1.8458e+00)	Acc@1  29.69 ( 28.67)	Acc@5  87.50 ( 84.52)
Epoch: [0][380/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.6434e+00 (1.8411e+00)	Acc@1  33.59 ( 28.78)	Acc@5  92.19 ( 84.66)
Epoch: [0][390/391]	Time  0.310 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.5972e+00 (1.8374e+00)	Acc@1  37.50 ( 28.92)	Acc@5  88.75 ( 84.78)
## e[0] optimizer.zero_grad (sum) time: 0.11415672302246094
## e[0]       loss.backward (sum) time: 2.5965828895568848
## e[0]      optimizer.step (sum) time: 0.8911375999450684
## epoch[0] training(only) time: 13.86773681640625
# Switched to evaluate mode...
Test: [  0/100]	Time  0.259 ( 0.259)	Loss 1.5376e+00 (1.5376e+00)	Acc@1  49.00 ( 49.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.023 ( 0.039)	Loss 1.4512e+00 (1.6979e+00)	Acc@1  44.00 ( 39.45)	Acc@5  87.00 ( 89.73)
Test: [ 20/100]	Time  0.014 ( 0.027)	Loss 2.1142e+00 (1.7397e+00)	Acc@1  39.00 ( 38.81)	Acc@5  87.00 ( 89.71)
Test: [ 30/100]	Time  0.011 ( 0.022)	Loss 1.7142e+00 (1.7702e+00)	Acc@1  40.00 ( 38.13)	Acc@5  91.00 ( 89.03)
Test: [ 40/100]	Time  0.018 ( 0.020)	Loss 2.0358e+00 (1.7677e+00)	Acc@1  32.00 ( 38.15)	Acc@5  81.00 ( 88.88)
Test: [ 50/100]	Time  0.011 ( 0.019)	Loss 1.6336e+00 (1.7616e+00)	Acc@1  35.00 ( 38.20)	Acc@5  92.00 ( 89.02)
Test: [ 60/100]	Time  0.011 ( 0.018)	Loss 1.6865e+00 (1.7633e+00)	Acc@1  38.00 ( 38.07)	Acc@5  92.00 ( 88.89)
Test: [ 70/100]	Time  0.017 ( 0.018)	Loss 1.8509e+00 (1.7647e+00)	Acc@1  34.00 ( 38.10)	Acc@5  82.00 ( 88.76)
Test: [ 80/100]	Time  0.011 ( 0.017)	Loss 1.7597e+00 (1.7623e+00)	Acc@1  43.00 ( 38.31)	Acc@5  88.00 ( 88.96)
Test: [ 90/100]	Time  0.011 ( 0.017)	Loss 1.8558e+00 (1.7622e+00)	Acc@1  30.00 ( 38.22)	Acc@5  92.00 ( 89.01)
 * Acc@1 38.300 Acc@5 88.950
### epoch[0] execution time: 15.669650793075562
EPOCH 1
# current learning rate: 0.1
# Switched to train mode...
Epoch: [1][  0/391]	Time  0.202 ( 0.202)	Data  0.173 ( 0.173)	Loss 1.5047e+00 (1.5047e+00)	Acc@1  43.75 ( 43.75)	Acc@5  92.97 ( 92.97)
Epoch: [1][ 10/391]	Time  0.031 ( 0.041)	Data  0.012 ( 0.018)	Loss 1.7473e+00 (1.6283e+00)	Acc@1  30.47 ( 36.29)	Acc@5  90.62 ( 90.27)
Epoch: [1][ 20/391]	Time  0.033 ( 0.034)	Data  0.002 ( 0.011)	Loss 1.6722e+00 (1.6296e+00)	Acc@1  35.94 ( 36.90)	Acc@5  94.53 ( 90.48)
Epoch: [1][ 30/391]	Time  0.028 ( 0.031)	Data  0.001 ( 0.008)	Loss 1.6251e+00 (1.6025e+00)	Acc@1  35.94 ( 38.21)	Acc@5  90.62 ( 91.03)
Epoch: [1][ 40/391]	Time  0.029 ( 0.030)	Data  0.001 ( 0.006)	Loss 1.6998e+00 (1.5963e+00)	Acc@1  35.16 ( 38.13)	Acc@5  89.84 ( 91.18)
Epoch: [1][ 50/391]	Time  0.020 ( 0.029)	Data  0.001 ( 0.005)	Loss 1.5453e+00 (1.5847e+00)	Acc@1  44.53 ( 38.89)	Acc@5  92.19 ( 91.68)
Epoch: [1][ 60/391]	Time  0.026 ( 0.029)	Data  0.001 ( 0.005)	Loss 1.5421e+00 (1.5831e+00)	Acc@1  37.50 ( 38.82)	Acc@5  92.97 ( 91.46)
Epoch: [1][ 70/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.5083e+00 (1.5802e+00)	Acc@1  42.19 ( 39.34)	Acc@5  92.19 ( 91.46)
Epoch: [1][ 80/391]	Time  0.025 ( 0.028)	Data  0.005 ( 0.004)	Loss 1.5318e+00 (1.5791e+00)	Acc@1  42.97 ( 39.55)	Acc@5  92.19 ( 91.41)
Epoch: [1][ 90/391]	Time  0.033 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.6572e+00 (1.5789e+00)	Acc@1  36.72 ( 39.50)	Acc@5  92.19 ( 91.41)
Epoch: [1][100/391]	Time  0.029 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.5534e+00 (1.5781e+00)	Acc@1  42.97 ( 39.49)	Acc@5  90.62 ( 91.45)
Epoch: [1][110/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.4745e+00 (1.5756e+00)	Acc@1  46.09 ( 39.60)	Acc@5  92.97 ( 91.50)
Epoch: [1][120/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.5852e+00 (1.5696e+00)	Acc@1  41.41 ( 40.00)	Acc@5  91.41 ( 91.55)
Epoch: [1][130/391]	Time  0.032 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.4029e+00 (1.5623e+00)	Acc@1  42.19 ( 40.30)	Acc@5  92.97 ( 91.63)
Epoch: [1][140/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.4755e+00 (1.5632e+00)	Acc@1  39.06 ( 40.33)	Acc@5  92.97 ( 91.59)
Epoch: [1][150/391]	Time  0.033 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.4674e+00 (1.5613e+00)	Acc@1  48.44 ( 40.42)	Acc@5  95.31 ( 91.59)
Epoch: [1][160/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.3983e+00 (1.5544e+00)	Acc@1  46.09 ( 40.81)	Acc@5  95.31 ( 91.63)
Epoch: [1][170/391]	Time  0.028 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.6094e+00 (1.5503e+00)	Acc@1  46.09 ( 41.12)	Acc@5  89.06 ( 91.71)
Epoch: [1][180/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.6687e+00 (1.5448e+00)	Acc@1  35.94 ( 41.34)	Acc@5  88.28 ( 91.80)
Epoch: [1][190/391]	Time  0.035 ( 0.027)	Data  0.000 ( 0.003)	Loss 1.5136e+00 (1.5389e+00)	Acc@1  39.84 ( 41.59)	Acc@5  92.97 ( 91.88)
Epoch: [1][200/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 1.3983e+00 (1.5365e+00)	Acc@1  48.44 ( 41.80)	Acc@5  94.53 ( 91.90)
Epoch: [1][210/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 1.5513e+00 (1.5312e+00)	Acc@1  45.31 ( 42.20)	Acc@5  86.72 ( 91.91)
Epoch: [1][220/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.4454e+00 (1.5237e+00)	Acc@1  52.34 ( 42.65)	Acc@5  93.75 ( 91.98)
Epoch: [1][230/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.2243e+00 (1.5198e+00)	Acc@1  57.03 ( 42.92)	Acc@5  95.31 ( 91.96)
Epoch: [1][240/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.4499e+00 (1.5139e+00)	Acc@1  43.75 ( 43.26)	Acc@5  92.19 ( 92.01)
Epoch: [1][250/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.4005e+00 (1.5093e+00)	Acc@1  47.66 ( 43.50)	Acc@5  91.41 ( 92.07)
Epoch: [1][260/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.2485e+00 (1.5020e+00)	Acc@1  56.25 ( 43.87)	Acc@5  94.53 ( 92.14)
Epoch: [1][270/391]	Time  0.030 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.4918e+00 (1.4972e+00)	Acc@1  51.56 ( 44.17)	Acc@5  90.62 ( 92.17)
Epoch: [1][280/391]	Time  0.033 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.3667e+00 (1.4925e+00)	Acc@1  50.78 ( 44.42)	Acc@5  95.31 ( 92.22)
Epoch: [1][290/391]	Time  0.026 ( 0.027)	Data  0.002 ( 0.003)	Loss 1.3016e+00 (1.4877e+00)	Acc@1  50.00 ( 44.63)	Acc@5  93.75 ( 92.26)
Epoch: [1][300/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3143e+00 (1.4850e+00)	Acc@1  53.12 ( 44.81)	Acc@5  92.19 ( 92.27)
Epoch: [1][310/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3558e+00 (1.4819e+00)	Acc@1  51.56 ( 44.98)	Acc@5  92.19 ( 92.32)
Epoch: [1][320/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3188e+00 (1.4763e+00)	Acc@1  56.25 ( 45.27)	Acc@5  92.19 ( 92.37)
Epoch: [1][330/391]	Time  0.035 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4041e+00 (1.4711e+00)	Acc@1  52.34 ( 45.50)	Acc@5  91.41 ( 92.41)
Epoch: [1][340/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2451e+00 (1.4670e+00)	Acc@1  58.59 ( 45.74)	Acc@5  93.75 ( 92.44)
Epoch: [1][350/391]	Time  0.025 ( 0.026)	Data  0.004 ( 0.003)	Loss 1.2685e+00 (1.4625e+00)	Acc@1  55.47 ( 46.00)	Acc@5  95.31 ( 92.48)
Epoch: [1][360/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4894e+00 (1.4591e+00)	Acc@1  48.44 ( 46.22)	Acc@5  92.19 ( 92.54)
Epoch: [1][370/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4530e+00 (1.4559e+00)	Acc@1  52.34 ( 46.41)	Acc@5  92.19 ( 92.55)
Epoch: [1][380/391]	Time  0.026 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.3555e+00 (1.4525e+00)	Acc@1  57.81 ( 46.64)	Acc@5  96.88 ( 92.58)
Epoch: [1][390/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2713e+00 (1.4505e+00)	Acc@1  50.00 ( 46.75)	Acc@5  91.25 ( 92.62)
## e[1] optimizer.zero_grad (sum) time: 0.10871267318725586
## e[1]       loss.backward (sum) time: 2.192997932434082
## e[1]      optimizer.step (sum) time: 0.8906886577606201
## epoch[1] training(only) time: 10.481623649597168
# Switched to evaluate mode...
Test: [  0/100]	Time  0.170 ( 0.170)	Loss 1.3204e+00 (1.3204e+00)	Acc@1  55.00 ( 55.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.011 ( 0.028)	Loss 1.2453e+00 (1.3140e+00)	Acc@1  56.00 ( 53.00)	Acc@5  94.00 ( 94.45)
Test: [ 20/100]	Time  0.016 ( 0.022)	Loss 1.4102e+00 (1.3272e+00)	Acc@1  48.00 ( 52.48)	Acc@5  92.00 ( 94.33)
Test: [ 30/100]	Time  0.029 ( 0.019)	Loss 1.3193e+00 (1.3448e+00)	Acc@1  56.00 ( 52.58)	Acc@5  94.00 ( 94.23)
Test: [ 40/100]	Time  0.024 ( 0.018)	Loss 1.2174e+00 (1.3348e+00)	Acc@1  55.00 ( 52.71)	Acc@5  97.00 ( 94.05)
Test: [ 50/100]	Time  0.011 ( 0.017)	Loss 1.2296e+00 (1.3177e+00)	Acc@1  51.00 ( 52.98)	Acc@5  95.00 ( 94.37)
Test: [ 60/100]	Time  0.015 ( 0.017)	Loss 1.5043e+00 (1.3252e+00)	Acc@1  45.00 ( 52.74)	Acc@5  96.00 ( 94.33)
Test: [ 70/100]	Time  0.012 ( 0.016)	Loss 1.1936e+00 (1.3371e+00)	Acc@1  49.00 ( 52.18)	Acc@5  98.00 ( 94.37)
Test: [ 80/100]	Time  0.011 ( 0.016)	Loss 1.2808e+00 (1.3363e+00)	Acc@1  56.00 ( 52.46)	Acc@5  92.00 ( 94.33)
Test: [ 90/100]	Time  0.011 ( 0.016)	Loss 1.2679e+00 (1.3413e+00)	Acc@1  57.00 ( 52.18)	Acc@5  96.00 ( 94.22)
 * Acc@1 52.230 Acc@5 94.240
### epoch[1] execution time: 12.175259113311768
EPOCH 2
# current learning rate: 0.1
# Switched to train mode...
Epoch: [2][  0/391]	Time  0.194 ( 0.194)	Data  0.170 ( 0.170)	Loss 1.4044e+00 (1.4044e+00)	Acc@1  46.09 ( 46.09)	Acc@5  95.31 ( 95.31)
Epoch: [2][ 10/391]	Time  0.024 ( 0.041)	Data  0.001 ( 0.017)	Loss 1.2107e+00 (1.2610e+00)	Acc@1  60.94 ( 55.97)	Acc@5  96.09 ( 93.96)
Epoch: [2][ 20/391]	Time  0.023 ( 0.034)	Data  0.001 ( 0.010)	Loss 1.1509e+00 (1.2681e+00)	Acc@1  60.94 ( 55.32)	Acc@5  93.75 ( 94.42)
Epoch: [2][ 30/391]	Time  0.029 ( 0.031)	Data  0.003 ( 0.007)	Loss 1.4613e+00 (1.2628e+00)	Acc@1  46.88 ( 55.07)	Acc@5  91.41 ( 94.78)
Epoch: [2][ 40/391]	Time  0.026 ( 0.030)	Data  0.001 ( 0.006)	Loss 1.3108e+00 (1.2586e+00)	Acc@1  52.34 ( 55.62)	Acc@5  96.88 ( 94.59)
Epoch: [2][ 50/391]	Time  0.027 ( 0.029)	Data  0.004 ( 0.005)	Loss 1.2892e+00 (1.2593e+00)	Acc@1  52.34 ( 55.62)	Acc@5  96.09 ( 94.59)
Epoch: [2][ 60/391]	Time  0.028 ( 0.028)	Data  0.004 ( 0.005)	Loss 1.2889e+00 (1.2569e+00)	Acc@1  53.12 ( 55.56)	Acc@5  95.31 ( 94.72)
Epoch: [2][ 70/391]	Time  0.032 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.2447e+00 (1.2533e+00)	Acc@1  59.38 ( 55.77)	Acc@5  95.31 ( 94.85)
Epoch: [2][ 80/391]	Time  0.023 ( 0.028)	Data  0.002 ( 0.004)	Loss 1.1800e+00 (1.2478e+00)	Acc@1  63.28 ( 56.10)	Acc@5  93.75 ( 94.86)
Epoch: [2][ 90/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.004)	Loss 1.2044e+00 (1.2479e+00)	Acc@1  63.28 ( 56.25)	Acc@5  93.75 ( 94.75)
Epoch: [2][100/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.1189e+00 (1.2392e+00)	Acc@1  56.25 ( 56.50)	Acc@5  97.66 ( 94.72)
Epoch: [2][110/391]	Time  0.024 ( 0.027)	Data  0.000 ( 0.004)	Loss 1.2139e+00 (1.2335e+00)	Acc@1  53.12 ( 56.71)	Acc@5  95.31 ( 94.84)
Epoch: [2][120/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.3644e+00 (1.2383e+00)	Acc@1  50.00 ( 56.42)	Acc@5  95.31 ( 94.84)
Epoch: [2][130/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.0964e+00 (1.2310e+00)	Acc@1  63.28 ( 56.73)	Acc@5  96.09 ( 94.91)
Epoch: [2][140/391]	Time  0.026 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.2755e+00 (1.2251e+00)	Acc@1  50.78 ( 56.99)	Acc@5  94.53 ( 94.99)
Epoch: [2][150/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.2553e+00 (1.2265e+00)	Acc@1  57.81 ( 57.02)	Acc@5  92.97 ( 94.93)
Epoch: [2][160/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.0575e+00 (1.2225e+00)	Acc@1  62.50 ( 57.18)	Acc@5  97.66 ( 94.98)
Epoch: [2][170/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.3403e+00 (1.2168e+00)	Acc@1  51.56 ( 57.37)	Acc@5  91.41 ( 95.04)
Epoch: [2][180/391]	Time  0.022 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.1972e+00 (1.2144e+00)	Acc@1  61.72 ( 57.51)	Acc@5  93.75 ( 95.07)
Epoch: [2][190/391]	Time  0.027 ( 0.026)	Data  0.005 ( 0.003)	Loss 1.1871e+00 (1.2117e+00)	Acc@1  60.16 ( 57.66)	Acc@5  94.53 ( 95.04)
Epoch: [2][200/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.1521e+00 (1.2101e+00)	Acc@1  60.94 ( 57.69)	Acc@5  94.53 ( 95.03)
Epoch: [2][210/391]	Time  0.023 ( 0.027)	Data  0.000 ( 0.003)	Loss 1.1561e+00 (1.2043e+00)	Acc@1  63.28 ( 58.02)	Acc@5  93.75 ( 95.06)
Epoch: [2][220/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 9.3828e-01 (1.1992e+00)	Acc@1  67.97 ( 58.25)	Acc@5  98.44 ( 95.08)
Epoch: [2][230/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2439e+00 (1.1969e+00)	Acc@1  59.38 ( 58.37)	Acc@5  92.19 ( 95.06)
Epoch: [2][240/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2953e+00 (1.1966e+00)	Acc@1  58.59 ( 58.36)	Acc@5  93.75 ( 95.07)
Epoch: [2][250/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4779e+00 (1.1942e+00)	Acc@1  54.69 ( 58.42)	Acc@5  97.66 ( 95.11)
Epoch: [2][260/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4042e+00 (1.1906e+00)	Acc@1  49.22 ( 58.57)	Acc@5  95.31 ( 95.14)
Epoch: [2][270/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1229e+00 (1.1850e+00)	Acc@1  59.38 ( 58.75)	Acc@5  96.88 ( 95.16)
Epoch: [2][280/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1395e+00 (1.1819e+00)	Acc@1  57.81 ( 58.86)	Acc@5  96.09 ( 95.18)
Epoch: [2][290/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0796e+00 (1.1789e+00)	Acc@1  64.06 ( 58.94)	Acc@5  94.53 ( 95.21)
Epoch: [2][300/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1246e+00 (1.1789e+00)	Acc@1  64.06 ( 58.92)	Acc@5  93.75 ( 95.19)
Epoch: [2][310/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.6435e-01 (1.1772e+00)	Acc@1  66.41 ( 59.02)	Acc@5  97.66 ( 95.19)
Epoch: [2][320/391]	Time  0.037 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.0496e+00 (1.1733e+00)	Acc@1  65.62 ( 59.14)	Acc@5  98.44 ( 95.24)
Epoch: [2][330/391]	Time  0.037 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.0703e+00 (1.1704e+00)	Acc@1  64.84 ( 59.29)	Acc@5  95.31 ( 95.27)
Epoch: [2][340/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1421e+00 (1.1716e+00)	Acc@1  58.59 ( 59.23)	Acc@5  95.31 ( 95.29)
Epoch: [2][350/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1525e+00 (1.1683e+00)	Acc@1  63.28 ( 59.39)	Acc@5  92.97 ( 95.30)
Epoch: [2][360/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1279e+00 (1.1682e+00)	Acc@1  58.59 ( 59.40)	Acc@5  96.09 ( 95.32)
Epoch: [2][370/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1542e+00 (1.1674e+00)	Acc@1  55.47 ( 59.43)	Acc@5  95.31 ( 95.32)
Epoch: [2][380/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.9798e-01 (1.1640e+00)	Acc@1  70.31 ( 59.56)	Acc@5  95.31 ( 95.35)
Epoch: [2][390/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1506e+00 (1.1623e+00)	Acc@1  51.25 ( 59.60)	Acc@5  98.75 ( 95.38)
## e[2] optimizer.zero_grad (sum) time: 0.10717988014221191
## e[2]       loss.backward (sum) time: 2.244030237197876
## e[2]      optimizer.step (sum) time: 0.8896315097808838
## epoch[2] training(only) time: 10.36559247970581
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 1.1994e+00 (1.1994e+00)	Acc@1  61.00 ( 61.00)	Acc@5  96.00 ( 96.00)
Test: [ 10/100]	Time  0.012 ( 0.029)	Loss 1.0911e+00 (1.1997e+00)	Acc@1  64.00 ( 57.09)	Acc@5  93.00 ( 95.36)
Test: [ 20/100]	Time  0.017 ( 0.022)	Loss 1.2086e+00 (1.1948e+00)	Acc@1  54.00 ( 57.52)	Acc@5  97.00 ( 95.52)
Test: [ 30/100]	Time  0.015 ( 0.019)	Loss 1.0539e+00 (1.2023e+00)	Acc@1  66.00 ( 57.68)	Acc@5  95.00 ( 95.32)
Test: [ 40/100]	Time  0.011 ( 0.018)	Loss 1.1783e+00 (1.1902e+00)	Acc@1  56.00 ( 57.12)	Acc@5  96.00 ( 95.66)
Test: [ 50/100]	Time  0.011 ( 0.017)	Loss 1.1209e+00 (1.1883e+00)	Acc@1  56.00 ( 57.35)	Acc@5  97.00 ( 95.61)
Test: [ 60/100]	Time  0.011 ( 0.017)	Loss 1.3986e+00 (1.2016e+00)	Acc@1  46.00 ( 56.77)	Acc@5  97.00 ( 95.46)
Test: [ 70/100]	Time  0.011 ( 0.016)	Loss 1.1854e+00 (1.2058e+00)	Acc@1  52.00 ( 56.55)	Acc@5  95.00 ( 95.46)
Test: [ 80/100]	Time  0.031 ( 0.016)	Loss 1.0650e+00 (1.2032e+00)	Acc@1  59.00 ( 56.53)	Acc@5  97.00 ( 95.48)
Test: [ 90/100]	Time  0.013 ( 0.016)	Loss 1.3379e+00 (1.2098e+00)	Acc@1  45.00 ( 56.16)	Acc@5  98.00 ( 95.49)
 * Acc@1 56.370 Acc@5 95.510
### epoch[2] execution time: 12.04799771308899
EPOCH 3
# current learning rate: 0.1
# Switched to train mode...
Epoch: [3][  0/391]	Time  0.196 ( 0.196)	Data  0.175 ( 0.175)	Loss 9.2915e-01 (9.2915e-01)	Acc@1  64.06 ( 64.06)	Acc@5  99.22 ( 99.22)
Epoch: [3][ 10/391]	Time  0.023 ( 0.041)	Data  0.001 ( 0.018)	Loss 1.0029e+00 (1.0103e+00)	Acc@1  67.19 ( 64.91)	Acc@5  96.09 ( 96.16)
Epoch: [3][ 20/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.010)	Loss 9.4577e-01 (1.0251e+00)	Acc@1  67.19 ( 64.88)	Acc@5  97.66 ( 96.13)
Epoch: [3][ 30/391]	Time  0.026 ( 0.031)	Data  0.001 ( 0.007)	Loss 9.2353e-01 (1.0253e+00)	Acc@1  67.97 ( 64.62)	Acc@5  95.31 ( 96.02)
Epoch: [3][ 40/391]	Time  0.023 ( 0.030)	Data  0.001 ( 0.006)	Loss 1.1112e+00 (1.0165e+00)	Acc@1  64.84 ( 64.86)	Acc@5  93.75 ( 96.25)
Epoch: [3][ 50/391]	Time  0.023 ( 0.029)	Data  0.001 ( 0.005)	Loss 1.0737e+00 (1.0094e+00)	Acc@1  64.06 ( 65.03)	Acc@5  96.09 ( 96.22)
Epoch: [3][ 60/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.0721e+00 (1.0174e+00)	Acc@1  69.53 ( 64.93)	Acc@5  96.09 ( 96.04)
Epoch: [3][ 70/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.005)	Loss 9.7196e-01 (1.0191e+00)	Acc@1  67.97 ( 64.70)	Acc@5  97.66 ( 96.08)
Epoch: [3][ 80/391]	Time  0.024 ( 0.028)	Data  0.000 ( 0.004)	Loss 8.4651e-01 (1.0107e+00)	Acc@1  71.88 ( 65.08)	Acc@5  96.88 ( 96.08)
Epoch: [3][ 90/391]	Time  0.031 ( 0.027)	Data  0.002 ( 0.004)	Loss 1.0701e+00 (1.0133e+00)	Acc@1  67.19 ( 65.31)	Acc@5  94.53 ( 96.08)
Epoch: [3][100/391]	Time  0.027 ( 0.027)	Data  0.001 ( 0.004)	Loss 8.1190e-01 (1.0148e+00)	Acc@1  71.09 ( 65.18)	Acc@5  95.31 ( 95.97)
Epoch: [3][110/391]	Time  0.031 ( 0.027)	Data  0.001 ( 0.004)	Loss 7.5318e-01 (1.0098e+00)	Acc@1  77.34 ( 65.43)	Acc@5  98.44 ( 96.14)
Epoch: [3][120/391]	Time  0.030 ( 0.027)	Data  0.004 ( 0.004)	Loss 1.0428e+00 (1.0113e+00)	Acc@1  64.06 ( 65.26)	Acc@5  96.09 ( 96.18)
Epoch: [3][130/391]	Time  0.023 ( 0.027)	Data  0.000 ( 0.004)	Loss 1.1978e+00 (1.0124e+00)	Acc@1  63.28 ( 65.23)	Acc@5  93.75 ( 96.20)
Epoch: [3][140/391]	Time  0.022 ( 0.027)	Data  0.005 ( 0.004)	Loss 1.0633e+00 (1.0129e+00)	Acc@1  66.41 ( 65.28)	Acc@5  97.66 ( 96.19)
Epoch: [3][150/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 8.2039e-01 (1.0086e+00)	Acc@1  68.75 ( 65.42)	Acc@5  98.44 ( 96.23)
Epoch: [3][160/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 9.2326e-01 (1.0048e+00)	Acc@1  67.19 ( 65.52)	Acc@5  96.09 ( 96.24)
Epoch: [3][170/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.9243e-01 (1.0003e+00)	Acc@1  75.00 ( 65.75)	Acc@5  96.88 ( 96.28)
Epoch: [3][180/391]	Time  0.026 ( 0.027)	Data  0.001 ( 0.003)	Loss 9.6914e-01 (9.9874e-01)	Acc@1  71.09 ( 65.80)	Acc@5  95.31 ( 96.28)
Epoch: [3][190/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.0612e+00 (1.0015e+00)	Acc@1  60.16 ( 65.71)	Acc@5  93.75 ( 96.26)
Epoch: [3][200/391]	Time  0.026 ( 0.027)	Data  0.001 ( 0.003)	Loss 8.8395e-01 (1.0021e+00)	Acc@1  71.09 ( 65.67)	Acc@5  94.53 ( 96.23)
Epoch: [3][210/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1001e+00 (1.0029e+00)	Acc@1  64.06 ( 65.72)	Acc@5  96.09 ( 96.24)
Epoch: [3][220/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2017e+00 (1.0022e+00)	Acc@1  57.81 ( 65.74)	Acc@5  94.53 ( 96.22)
Epoch: [3][230/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.0327e-01 (1.0028e+00)	Acc@1  66.41 ( 65.68)	Acc@5  98.44 ( 96.24)
Epoch: [3][240/391]	Time  0.022 ( 0.026)	Data  0.002 ( 0.003)	Loss 8.3274e-01 (1.0021e+00)	Acc@1  69.53 ( 65.71)	Acc@5  98.44 ( 96.22)
Epoch: [3][250/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0245e+00 (1.0020e+00)	Acc@1  62.50 ( 65.69)	Acc@5  95.31 ( 96.21)
Epoch: [3][260/391]	Time  0.024 ( 0.026)	Data  0.000 ( 0.003)	Loss 8.3823e-01 (9.9892e-01)	Acc@1  69.53 ( 65.75)	Acc@5  97.66 ( 96.25)
Epoch: [3][270/391]	Time  0.029 ( 0.026)	Data  0.002 ( 0.003)	Loss 9.9758e-01 (9.9717e-01)	Acc@1  66.41 ( 65.83)	Acc@5  96.88 ( 96.29)
Epoch: [3][280/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.0437e-01 (9.9549e-01)	Acc@1  74.22 ( 65.86)	Acc@5  97.66 ( 96.32)
Epoch: [3][290/391]	Time  0.031 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.0037e+00 (9.9442e-01)	Acc@1  67.19 ( 65.89)	Acc@5  98.44 ( 96.34)
Epoch: [3][300/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.3029e-01 (9.9247e-01)	Acc@1  74.22 ( 66.04)	Acc@5  98.44 ( 96.36)
Epoch: [3][310/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.7205e-01 (9.9109e-01)	Acc@1  67.19 ( 66.08)	Acc@5  94.53 ( 96.34)
Epoch: [3][320/391]	Time  0.023 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.0720e+00 (9.9039e-01)	Acc@1  67.19 ( 66.09)	Acc@5  99.22 ( 96.35)
Epoch: [3][330/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.1665e-01 (9.8970e-01)	Acc@1  74.22 ( 66.16)	Acc@5  98.44 ( 96.37)
Epoch: [3][340/391]	Time  0.026 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.0302e+00 (9.8882e-01)	Acc@1  67.19 ( 66.20)	Acc@5  97.66 ( 96.38)
Epoch: [3][350/391]	Time  0.025 ( 0.026)	Data  0.002 ( 0.003)	Loss 8.9470e-01 (9.8860e-01)	Acc@1  71.09 ( 66.27)	Acc@5  98.44 ( 96.38)
Epoch: [3][360/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.8385e-01 (9.8629e-01)	Acc@1  71.88 ( 66.37)	Acc@5  96.88 ( 96.39)
Epoch: [3][370/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.0309e-01 (9.8228e-01)	Acc@1  66.41 ( 66.51)	Acc@5  96.09 ( 96.41)
Epoch: [3][380/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.7548e-01 (9.7908e-01)	Acc@1  74.22 ( 66.60)	Acc@5  98.44 ( 96.44)
Epoch: [3][390/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0700e+00 (9.7819e-01)	Acc@1  63.75 ( 66.68)	Acc@5 100.00 ( 96.43)
## e[3] optimizer.zero_grad (sum) time: 0.10839200019836426
## e[3]       loss.backward (sum) time: 2.224740743637085
## e[3]      optimizer.step (sum) time: 0.8991348743438721
## epoch[3] training(only) time: 10.33010220527649
# Switched to evaluate mode...
Test: [  0/100]	Time  0.171 ( 0.171)	Loss 8.8970e-01 (8.8970e-01)	Acc@1  66.00 ( 66.00)	Acc@5  97.00 ( 97.00)
Test: [ 10/100]	Time  0.011 ( 0.028)	Loss 8.3437e-01 (9.1057e-01)	Acc@1  71.00 ( 70.18)	Acc@5  97.00 ( 96.91)
Test: [ 20/100]	Time  0.020 ( 0.022)	Loss 1.0373e+00 (9.6658e-01)	Acc@1  66.00 ( 68.76)	Acc@5  97.00 ( 96.19)
Test: [ 30/100]	Time  0.013 ( 0.020)	Loss 1.0143e+00 (9.8002e-01)	Acc@1  68.00 ( 68.03)	Acc@5  95.00 ( 96.06)
Test: [ 40/100]	Time  0.016 ( 0.018)	Loss 1.0600e+00 (9.8942e-01)	Acc@1  68.00 ( 67.41)	Acc@5  96.00 ( 96.05)
Test: [ 50/100]	Time  0.011 ( 0.017)	Loss 8.9791e-01 (9.7520e-01)	Acc@1  72.00 ( 67.69)	Acc@5  97.00 ( 96.20)
Test: [ 60/100]	Time  0.015 ( 0.017)	Loss 1.0576e+00 (9.8894e-01)	Acc@1  60.00 ( 67.15)	Acc@5  97.00 ( 96.18)
Test: [ 70/100]	Time  0.012 ( 0.016)	Loss 9.5966e-01 (9.8700e-01)	Acc@1  65.00 ( 67.07)	Acc@5  95.00 ( 96.30)
Test: [ 80/100]	Time  0.011 ( 0.016)	Loss 8.6106e-01 (9.8390e-01)	Acc@1  69.00 ( 67.21)	Acc@5  96.00 ( 96.33)
Test: [ 90/100]	Time  0.020 ( 0.016)	Loss 1.0004e+00 (9.8243e-01)	Acc@1  69.00 ( 67.44)	Acc@5  98.00 ( 96.31)
 * Acc@1 67.500 Acc@5 96.340
### epoch[3] execution time: 12.021565914154053
EPOCH 4
# current learning rate: 0.1
# Switched to train mode...
Epoch: [4][  0/391]	Time  0.183 ( 0.183)	Data  0.163 ( 0.163)	Loss 9.3536e-01 (9.3536e-01)	Acc@1  67.97 ( 67.97)	Acc@5  99.22 ( 99.22)
Epoch: [4][ 10/391]	Time  0.027 ( 0.040)	Data  0.002 ( 0.016)	Loss 8.8647e-01 (9.2400e-01)	Acc@1  70.31 ( 67.97)	Acc@5  95.31 ( 97.37)
Epoch: [4][ 20/391]	Time  0.023 ( 0.033)	Data  0.001 ( 0.010)	Loss 9.1729e-01 (9.0445e-01)	Acc@1  71.09 ( 69.53)	Acc@5  96.09 ( 97.54)
Epoch: [4][ 30/391]	Time  0.023 ( 0.031)	Data  0.001 ( 0.007)	Loss 8.6643e-01 (8.8197e-01)	Acc@1  71.88 ( 70.16)	Acc@5  97.66 ( 97.35)
Epoch: [4][ 40/391]	Time  0.025 ( 0.030)	Data  0.001 ( 0.006)	Loss 9.3115e-01 (8.9233e-01)	Acc@1  67.19 ( 69.95)	Acc@5  97.66 ( 97.10)
Epoch: [4][ 50/391]	Time  0.023 ( 0.029)	Data  0.001 ( 0.005)	Loss 8.7723e-01 (8.9835e-01)	Acc@1  64.84 ( 69.56)	Acc@5  98.44 ( 96.98)
Epoch: [4][ 60/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.005)	Loss 9.7408e-01 (9.0205e-01)	Acc@1  63.28 ( 69.44)	Acc@5  97.66 ( 97.03)
Epoch: [4][ 70/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.004)	Loss 9.4244e-01 (9.0044e-01)	Acc@1  73.44 ( 69.59)	Acc@5  96.88 ( 97.03)
Epoch: [4][ 80/391]	Time  0.029 ( 0.028)	Data  0.000 ( 0.004)	Loss 9.7144e-01 (9.0978e-01)	Acc@1  71.88 ( 69.16)	Acc@5  96.09 ( 97.03)
Epoch: [4][ 90/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 9.8743e-01 (9.2080e-01)	Acc@1  67.97 ( 68.83)	Acc@5  92.97 ( 96.88)
Epoch: [4][100/391]	Time  0.031 ( 0.027)	Data  0.003 ( 0.004)	Loss 1.0934e+00 (9.1745e-01)	Acc@1  67.97 ( 68.94)	Acc@5  94.53 ( 96.91)
Epoch: [4][110/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.004)	Loss 7.9435e-01 (9.1961e-01)	Acc@1  78.91 ( 68.88)	Acc@5  97.66 ( 96.90)
Epoch: [4][120/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 8.7195e-01 (9.2012e-01)	Acc@1  69.53 ( 68.83)	Acc@5  95.31 ( 96.87)
Epoch: [4][130/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 9.1885e-01 (9.1744e-01)	Acc@1  62.50 ( 68.92)	Acc@5  97.66 ( 96.89)
Epoch: [4][140/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 8.7775e-01 (9.1341e-01)	Acc@1  69.53 ( 69.05)	Acc@5  98.44 ( 96.94)
Epoch: [4][150/391]	Time  0.031 ( 0.027)	Data  0.002 ( 0.003)	Loss 9.8008e-01 (9.1406e-01)	Acc@1  65.62 ( 69.01)	Acc@5  92.97 ( 97.00)
Epoch: [4][160/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 8.3214e-01 (9.1271e-01)	Acc@1  73.44 ( 69.02)	Acc@5  95.31 ( 97.00)
Epoch: [4][170/391]	Time  0.032 ( 0.027)	Data  0.006 ( 0.003)	Loss 9.1417e-01 (9.1343e-01)	Acc@1  69.53 ( 69.03)	Acc@5  96.88 ( 96.97)
Epoch: [4][180/391]	Time  0.032 ( 0.027)	Data  0.001 ( 0.003)	Loss 8.4585e-01 (9.1319e-01)	Acc@1  75.78 ( 69.13)	Acc@5  96.09 ( 96.91)
Epoch: [4][190/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 8.0860e-01 (9.1048e-01)	Acc@1  73.44 ( 69.20)	Acc@5 100.00 ( 96.94)
Epoch: [4][200/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.7829e-01 (9.0363e-01)	Acc@1  73.44 ( 69.45)	Acc@5  96.88 ( 96.96)
Epoch: [4][210/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.0222e-01 (9.0240e-01)	Acc@1  72.66 ( 69.49)	Acc@5  99.22 ( 96.99)
Epoch: [4][220/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0382e+00 (9.0087e-01)	Acc@1  59.38 ( 69.57)	Acc@5  99.22 ( 96.98)
Epoch: [4][230/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.5901e-01 (8.9895e-01)	Acc@1  70.31 ( 69.64)	Acc@5  96.09 ( 96.97)
Epoch: [4][240/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.7084e-01 (8.9362e-01)	Acc@1  75.78 ( 69.81)	Acc@5  96.88 ( 97.00)
Epoch: [4][250/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0227e+00 (8.9123e-01)	Acc@1  69.53 ( 69.92)	Acc@5  94.53 ( 96.97)
Epoch: [4][260/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.4896e-01 (8.8861e-01)	Acc@1  69.53 ( 70.03)	Acc@5  99.22 ( 96.97)
Epoch: [4][270/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.5776e-01 (8.8682e-01)	Acc@1  78.91 ( 70.11)	Acc@5  97.66 ( 96.97)
Epoch: [4][280/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.3732e-01 (8.8533e-01)	Acc@1  67.97 ( 70.15)	Acc@5  98.44 ( 96.99)
Epoch: [4][290/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.9065e-01 (8.8492e-01)	Acc@1  76.56 ( 70.15)	Acc@5  98.44 ( 97.02)
Epoch: [4][300/391]	Time  0.026 ( 0.026)	Data  0.005 ( 0.003)	Loss 7.1390e-01 (8.8430e-01)	Acc@1  78.12 ( 70.17)	Acc@5  97.66 ( 97.04)
Epoch: [4][310/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.7526e-01 (8.8162e-01)	Acc@1  76.56 ( 70.29)	Acc@5  97.66 ( 97.05)
Epoch: [4][320/391]	Time  0.024 ( 0.026)	Data  0.003 ( 0.003)	Loss 6.2513e-01 (8.7833e-01)	Acc@1  78.91 ( 70.39)	Acc@5  98.44 ( 97.06)
Epoch: [4][330/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.5052e-01 (8.7681e-01)	Acc@1  76.56 ( 70.47)	Acc@5  95.31 ( 97.07)
Epoch: [4][340/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.2823e-01 (8.7818e-01)	Acc@1  71.09 ( 70.42)	Acc@5  93.75 ( 97.04)
Epoch: [4][350/391]	Time  0.025 ( 0.026)	Data  0.006 ( 0.003)	Loss 7.7058e-01 (8.7785e-01)	Acc@1  75.00 ( 70.46)	Acc@5  99.22 ( 97.05)
Epoch: [4][360/391]	Time  0.028 ( 0.026)	Data  0.005 ( 0.003)	Loss 9.5458e-01 (8.7705e-01)	Acc@1  71.09 ( 70.49)	Acc@5  96.09 ( 97.05)
Epoch: [4][370/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.7128e-01 (8.7647e-01)	Acc@1  70.31 ( 70.52)	Acc@5  97.66 ( 97.06)
Epoch: [4][380/391]	Time  0.022 ( 0.026)	Data  0.000 ( 0.003)	Loss 9.4745e-01 (8.7657e-01)	Acc@1  67.19 ( 70.50)	Acc@5  98.44 ( 97.06)
Epoch: [4][390/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.0298e-01 (8.7409e-01)	Acc@1  71.25 ( 70.58)	Acc@5  96.25 ( 97.06)
## e[4] optimizer.zero_grad (sum) time: 0.10877299308776855
## e[4]       loss.backward (sum) time: 2.2251813411712646
## e[4]      optimizer.step (sum) time: 0.8787434101104736
## epoch[4] training(only) time: 10.322559595108032
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 7.2381e-01 (7.2381e-01)	Acc@1  76.00 ( 76.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.013 ( 0.028)	Loss 8.3632e-01 (8.1132e-01)	Acc@1  77.00 ( 73.45)	Acc@5  98.00 ( 97.82)
Test: [ 20/100]	Time  0.021 ( 0.022)	Loss 8.5370e-01 (8.1899e-01)	Acc@1  67.00 ( 72.48)	Acc@5  99.00 ( 97.57)
Test: [ 30/100]	Time  0.011 ( 0.019)	Loss 8.2494e-01 (8.2744e-01)	Acc@1  75.00 ( 72.74)	Acc@5  97.00 ( 97.81)
Test: [ 40/100]	Time  0.011 ( 0.018)	Loss 7.7427e-01 (8.2422e-01)	Acc@1  71.00 ( 72.76)	Acc@5  98.00 ( 97.83)
Test: [ 50/100]	Time  0.011 ( 0.017)	Loss 6.9919e-01 (8.1644e-01)	Acc@1  74.00 ( 72.86)	Acc@5  99.00 ( 97.76)
Test: [ 60/100]	Time  0.011 ( 0.017)	Loss 8.8419e-01 (8.1845e-01)	Acc@1  66.00 ( 72.85)	Acc@5  96.00 ( 97.70)
Test: [ 70/100]	Time  0.018 ( 0.016)	Loss 6.4496e-01 (8.1970e-01)	Acc@1  80.00 ( 73.07)	Acc@5  97.00 ( 97.70)
Test: [ 80/100]	Time  0.014 ( 0.016)	Loss 5.7552e-01 (8.1183e-01)	Acc@1  77.00 ( 73.25)	Acc@5  97.00 ( 97.73)
Test: [ 90/100]	Time  0.015 ( 0.016)	Loss 8.1172e-01 (8.1453e-01)	Acc@1  78.00 ( 73.22)	Acc@5  98.00 ( 97.75)
 * Acc@1 73.290 Acc@5 97.780
### epoch[4] execution time: 12.038414239883423
EPOCH 5
# current learning rate: 0.1
# Switched to train mode...
Epoch: [5][  0/391]	Time  0.191 ( 0.191)	Data  0.171 ( 0.171)	Loss 7.3282e-01 (7.3282e-01)	Acc@1  78.12 ( 78.12)	Acc@5  98.44 ( 98.44)
Epoch: [5][ 10/391]	Time  0.024 ( 0.041)	Data  0.001 ( 0.017)	Loss 1.0848e+00 (8.1661e-01)	Acc@1  71.09 ( 72.73)	Acc@5  93.75 ( 96.95)
Epoch: [5][ 20/391]	Time  0.022 ( 0.034)	Data  0.001 ( 0.010)	Loss 7.6795e-01 (8.2043e-01)	Acc@1  75.78 ( 72.66)	Acc@5  98.44 ( 97.06)
Epoch: [5][ 30/391]	Time  0.025 ( 0.031)	Data  0.003 ( 0.008)	Loss 7.9414e-01 (7.9375e-01)	Acc@1  75.78 ( 73.64)	Acc@5  95.31 ( 97.05)
Epoch: [5][ 40/391]	Time  0.023 ( 0.030)	Data  0.001 ( 0.006)	Loss 8.0201e-01 (7.9513e-01)	Acc@1  69.53 ( 73.36)	Acc@5  99.22 ( 97.35)
Epoch: [5][ 50/391]	Time  0.024 ( 0.029)	Data  0.001 ( 0.006)	Loss 7.8966e-01 (8.0246e-01)	Acc@1  73.44 ( 73.16)	Acc@5  97.66 ( 97.32)
Epoch: [5][ 60/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.005)	Loss 7.2941e-01 (8.0235e-01)	Acc@1  75.78 ( 73.46)	Acc@5  97.66 ( 97.36)
Epoch: [5][ 70/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.005)	Loss 7.3647e-01 (8.0388e-01)	Acc@1  75.00 ( 73.40)	Acc@5  99.22 ( 97.47)
Epoch: [5][ 80/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 7.7083e-01 (8.0195e-01)	Acc@1  77.34 ( 73.55)	Acc@5  97.66 ( 97.47)
Epoch: [5][ 90/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 8.5612e-01 (8.0125e-01)	Acc@1  71.09 ( 73.48)	Acc@5  96.88 ( 97.50)
Epoch: [5][100/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 6.9890e-01 (8.0574e-01)	Acc@1  78.12 ( 73.24)	Acc@5  96.09 ( 97.46)
Epoch: [5][110/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 8.7155e-01 (8.0530e-01)	Acc@1  71.09 ( 73.21)	Acc@5  98.44 ( 97.47)
Epoch: [5][120/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.0609e+00 (8.0777e-01)	Acc@1  64.84 ( 73.10)	Acc@5  95.31 ( 97.51)
Epoch: [5][130/391]	Time  0.026 ( 0.027)	Data  0.001 ( 0.004)	Loss 7.4301e-01 (8.0501e-01)	Acc@1  72.66 ( 73.20)	Acc@5  99.22 ( 97.54)
Epoch: [5][140/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 9.1408e-01 (8.0100e-01)	Acc@1  73.44 ( 73.34)	Acc@5  96.09 ( 97.52)
Epoch: [5][150/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.8063e-01 (7.9641e-01)	Acc@1  76.56 ( 73.43)	Acc@5  96.88 ( 97.57)
Epoch: [5][160/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.2394e-01 (7.9185e-01)	Acc@1  77.34 ( 73.61)	Acc@5  99.22 ( 97.59)
Epoch: [5][170/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.7372e-01 (7.8968e-01)	Acc@1  72.66 ( 73.67)	Acc@5  96.09 ( 97.58)
Epoch: [5][180/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.5667e-01 (7.8878e-01)	Acc@1  75.00 ( 73.64)	Acc@5  98.44 ( 97.61)
Epoch: [5][190/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.4364e-01 (7.8850e-01)	Acc@1  74.22 ( 73.62)	Acc@5  96.88 ( 97.58)
Epoch: [5][200/391]	Time  0.024 ( 0.026)	Data  0.002 ( 0.003)	Loss 8.2662e-01 (7.8824e-01)	Acc@1  72.66 ( 73.65)	Acc@5  97.66 ( 97.57)
Epoch: [5][210/391]	Time  0.032 ( 0.026)	Data  0.005 ( 0.003)	Loss 6.7931e-01 (7.8559e-01)	Acc@1  79.69 ( 73.77)	Acc@5  96.88 ( 97.59)
Epoch: [5][220/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.5081e-01 (7.8445e-01)	Acc@1  74.22 ( 73.78)	Acc@5  95.31 ( 97.59)
Epoch: [5][230/391]	Time  0.040 ( 0.026)	Data  0.002 ( 0.003)	Loss 6.9509e-01 (7.8474e-01)	Acc@1  77.34 ( 73.77)	Acc@5  98.44 ( 97.59)
Epoch: [5][240/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.4639e-01 (7.8656e-01)	Acc@1  73.44 ( 73.66)	Acc@5  98.44 ( 97.59)
Epoch: [5][250/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.4684e-01 (7.8738e-01)	Acc@1  75.00 ( 73.64)	Acc@5  97.66 ( 97.60)
Epoch: [5][260/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.9076e-01 (7.8526e-01)	Acc@1  75.78 ( 73.66)	Acc@5  99.22 ( 97.59)
Epoch: [5][270/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.4212e-01 (7.8443e-01)	Acc@1  77.34 ( 73.73)	Acc@5  96.88 ( 97.58)
Epoch: [5][280/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.6855e-01 (7.8391e-01)	Acc@1  81.25 ( 73.73)	Acc@5  99.22 ( 97.60)
Epoch: [5][290/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.0069e-01 (7.8315e-01)	Acc@1  75.78 ( 73.78)	Acc@5  98.44 ( 97.62)
Epoch: [5][300/391]	Time  0.029 ( 0.026)	Data  0.002 ( 0.003)	Loss 8.4091e-01 (7.8291e-01)	Acc@1  71.09 ( 73.79)	Acc@5  98.44 ( 97.65)
Epoch: [5][310/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.1683e-01 (7.8115e-01)	Acc@1  78.12 ( 73.88)	Acc@5  99.22 ( 97.65)
Epoch: [5][320/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.4550e-01 (7.8063e-01)	Acc@1  71.88 ( 73.90)	Acc@5  96.09 ( 97.64)
Epoch: [5][330/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.0757e-01 (7.8002e-01)	Acc@1  72.66 ( 73.92)	Acc@5  96.88 ( 97.65)
Epoch: [5][340/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.2962e-01 (7.7897e-01)	Acc@1  78.12 ( 73.95)	Acc@5  99.22 ( 97.66)
Epoch: [5][350/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.4121e-01 (7.7705e-01)	Acc@1  82.03 ( 74.02)	Acc@5  99.22 ( 97.68)
Epoch: [5][360/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.1653e-01 (7.7810e-01)	Acc@1  70.31 ( 74.00)	Acc@5  96.88 ( 97.67)
Epoch: [5][370/391]	Time  0.023 ( 0.026)	Data  0.000 ( 0.003)	Loss 9.2337e-01 (7.7912e-01)	Acc@1  71.88 ( 73.97)	Acc@5  95.31 ( 97.67)
Epoch: [5][380/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.6004e-01 (7.7697e-01)	Acc@1  73.44 ( 74.07)	Acc@5 100.00 ( 97.68)
Epoch: [5][390/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.8825e-01 (7.7579e-01)	Acc@1  75.00 ( 74.11)	Acc@5  97.50 ( 97.69)
## e[5] optimizer.zero_grad (sum) time: 0.10812211036682129
## e[5]       loss.backward (sum) time: 2.2643234729766846
## e[5]      optimizer.step (sum) time: 0.8570606708526611
## epoch[5] training(only) time: 10.351605892181396
# Switched to evaluate mode...
Test: [  0/100]	Time  0.164 ( 0.164)	Loss 8.2985e-01 (8.2985e-01)	Acc@1  69.00 ( 69.00)	Acc@5  96.00 ( 96.00)
Test: [ 10/100]	Time  0.011 ( 0.028)	Loss 8.3352e-01 (8.3278e-01)	Acc@1  75.00 ( 72.09)	Acc@5  96.00 ( 97.55)
Test: [ 20/100]	Time  0.010 ( 0.021)	Loss 7.0783e-01 (8.2538e-01)	Acc@1  76.00 ( 72.62)	Acc@5 100.00 ( 97.86)
Test: [ 30/100]	Time  0.013 ( 0.019)	Loss 8.6112e-01 (8.2366e-01)	Acc@1  72.00 ( 73.00)	Acc@5  96.00 ( 97.68)
Test: [ 40/100]	Time  0.011 ( 0.018)	Loss 8.7690e-01 (8.2473e-01)	Acc@1  73.00 ( 72.88)	Acc@5  96.00 ( 97.56)
Test: [ 50/100]	Time  0.011 ( 0.017)	Loss 7.5548e-01 (8.2082e-01)	Acc@1  75.00 ( 72.94)	Acc@5  97.00 ( 97.53)
Test: [ 60/100]	Time  0.011 ( 0.017)	Loss 8.4169e-01 (8.2763e-01)	Acc@1  71.00 ( 72.92)	Acc@5  97.00 ( 97.54)
Test: [ 70/100]	Time  0.023 ( 0.017)	Loss 7.1706e-01 (8.2594e-01)	Acc@1  77.00 ( 72.89)	Acc@5  97.00 ( 97.62)
Test: [ 80/100]	Time  0.023 ( 0.016)	Loss 5.5495e-01 (8.2493e-01)	Acc@1  80.00 ( 72.93)	Acc@5  99.00 ( 97.63)
Test: [ 90/100]	Time  0.011 ( 0.016)	Loss 8.5612e-01 (8.3003e-01)	Acc@1  68.00 ( 72.47)	Acc@5  98.00 ( 97.68)
 * Acc@1 72.360 Acc@5 97.650
### epoch[5] execution time: 12.050102710723877
EPOCH 6
# current learning rate: 0.1
# Switched to train mode...
Epoch: [6][  0/391]	Time  0.198 ( 0.198)	Data  0.178 ( 0.178)	Loss 7.9746e-01 (7.9746e-01)	Acc@1  73.44 ( 73.44)	Acc@5  99.22 ( 99.22)
Epoch: [6][ 10/391]	Time  0.024 ( 0.042)	Data  0.001 ( 0.018)	Loss 5.8537e-01 (7.1606e-01)	Acc@1  81.25 ( 76.42)	Acc@5  97.66 ( 97.37)
Epoch: [6][ 20/391]	Time  0.023 ( 0.034)	Data  0.002 ( 0.010)	Loss 7.4566e-01 (7.0763e-01)	Acc@1  76.56 ( 76.41)	Acc@5  97.66 ( 97.73)
Epoch: [6][ 30/391]	Time  0.023 ( 0.031)	Data  0.004 ( 0.008)	Loss 9.3799e-01 (7.1694e-01)	Acc@1  67.97 ( 76.16)	Acc@5  96.09 ( 97.53)
Epoch: [6][ 40/391]	Time  0.025 ( 0.030)	Data  0.001 ( 0.006)	Loss 7.1683e-01 (7.2904e-01)	Acc@1  73.44 ( 75.55)	Acc@5  98.44 ( 97.62)
Epoch: [6][ 50/391]	Time  0.023 ( 0.029)	Data  0.001 ( 0.005)	Loss 6.8816e-01 (7.1521e-01)	Acc@1  78.91 ( 76.10)	Acc@5  96.09 ( 97.70)
Epoch: [6][ 60/391]	Time  0.038 ( 0.028)	Data  0.004 ( 0.005)	Loss 7.0591e-01 (7.1562e-01)	Acc@1  76.56 ( 76.18)	Acc@5 100.00 ( 97.75)
Epoch: [6][ 70/391]	Time  0.031 ( 0.028)	Data  0.001 ( 0.005)	Loss 7.5954e-01 (7.1287e-01)	Acc@1  78.12 ( 76.38)	Acc@5  97.66 ( 97.84)
Epoch: [6][ 80/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 7.8205e-01 (7.1597e-01)	Acc@1  74.22 ( 76.31)	Acc@5  98.44 ( 97.81)
Epoch: [6][ 90/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 6.9621e-01 (7.1474e-01)	Acc@1  74.22 ( 76.39)	Acc@5  99.22 ( 97.89)
Epoch: [6][100/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 7.8559e-01 (7.1439e-01)	Acc@1  75.78 ( 76.54)	Acc@5  96.88 ( 97.85)
Epoch: [6][110/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 5.8035e-01 (7.0824e-01)	Acc@1  78.12 ( 76.72)	Acc@5  97.66 ( 97.87)
Epoch: [6][120/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.004)	Loss 6.2363e-01 (7.0296e-01)	Acc@1  80.47 ( 76.81)	Acc@5  97.66 ( 97.93)
Epoch: [6][130/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 6.8894e-01 (7.0383e-01)	Acc@1  77.34 ( 76.94)	Acc@5  97.66 ( 97.88)
Epoch: [6][140/391]	Time  0.027 ( 0.027)	Data  0.001 ( 0.004)	Loss 7.7565e-01 (7.0593e-01)	Acc@1  73.44 ( 76.85)	Acc@5  98.44 ( 97.92)
Epoch: [6][150/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.6215e-01 (7.0455e-01)	Acc@1  80.47 ( 76.84)	Acc@5  96.09 ( 97.91)
Epoch: [6][160/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 8.8019e-01 (7.0653e-01)	Acc@1  71.09 ( 76.77)	Acc@5  96.09 ( 97.92)
Epoch: [6][170/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.4802e-01 (7.0906e-01)	Acc@1  85.16 ( 76.73)	Acc@5  97.66 ( 97.93)
Epoch: [6][180/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.5009e-01 (7.0755e-01)	Acc@1  74.22 ( 76.77)	Acc@5  99.22 ( 97.97)
Epoch: [6][190/391]	Time  0.022 ( 0.027)	Data  0.002 ( 0.003)	Loss 7.4246e-01 (7.1053e-01)	Acc@1  76.56 ( 76.65)	Acc@5  97.66 ( 97.96)
Epoch: [6][200/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.0450e-01 (7.1001e-01)	Acc@1  78.12 ( 76.65)	Acc@5  99.22 ( 97.99)
Epoch: [6][210/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.6244e-01 (7.0936e-01)	Acc@1  76.56 ( 76.70)	Acc@5  99.22 ( 98.00)
Epoch: [6][220/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.003)	Loss 8.3604e-01 (7.0993e-01)	Acc@1  73.44 ( 76.71)	Acc@5  97.66 ( 98.01)
Epoch: [6][230/391]	Time  0.040 ( 0.027)	Data  0.000 ( 0.003)	Loss 7.5872e-01 (7.0766e-01)	Acc@1  72.66 ( 76.76)	Acc@5  96.88 ( 98.01)
Epoch: [6][240/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 8.0550e-01 (7.0867e-01)	Acc@1  79.69 ( 76.73)	Acc@5  98.44 ( 98.02)
Epoch: [6][250/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.9515e-01 (7.1035e-01)	Acc@1  72.66 ( 76.69)	Acc@5  96.88 ( 98.02)
Epoch: [6][260/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.5505e-01 (7.1100e-01)	Acc@1  82.81 ( 76.64)	Acc@5  96.88 ( 98.02)
Epoch: [6][270/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.1716e-01 (7.1319e-01)	Acc@1  74.22 ( 76.53)	Acc@5  99.22 ( 98.04)
Epoch: [6][280/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.0442e-01 (7.1117e-01)	Acc@1  74.22 ( 76.57)	Acc@5  98.44 ( 98.06)
Epoch: [6][290/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.1455e-01 (7.1001e-01)	Acc@1  74.22 ( 76.58)	Acc@5  98.44 ( 98.06)
Epoch: [6][300/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.3063e-01 (7.1007e-01)	Acc@1  84.38 ( 76.58)	Acc@5  98.44 ( 98.04)
Epoch: [6][310/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.4434e-01 (7.0952e-01)	Acc@1  82.03 ( 76.61)	Acc@5  98.44 ( 98.05)
Epoch: [6][320/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.7448e-01 (7.0811e-01)	Acc@1  77.34 ( 76.61)	Acc@5 100.00 ( 98.07)
Epoch: [6][330/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.5533e-01 (7.0933e-01)	Acc@1  75.00 ( 76.54)	Acc@5  98.44 ( 98.07)
Epoch: [6][340/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.8786e-01 (7.0895e-01)	Acc@1  76.56 ( 76.56)	Acc@5  97.66 ( 98.07)
Epoch: [6][350/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.5886e-01 (7.0704e-01)	Acc@1  73.44 ( 76.58)	Acc@5  96.88 ( 98.08)
Epoch: [6][360/391]	Time  0.025 ( 0.026)	Data  0.005 ( 0.003)	Loss 6.8457e-01 (7.0645e-01)	Acc@1  77.34 ( 76.60)	Acc@5  97.66 ( 98.09)
Epoch: [6][370/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.0156e-01 (7.0635e-01)	Acc@1  79.69 ( 76.62)	Acc@5 100.00 ( 98.09)
Epoch: [6][380/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.7596e-01 (7.0608e-01)	Acc@1  79.69 ( 76.61)	Acc@5  99.22 ( 98.09)
Epoch: [6][390/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.4490e-01 (7.0678e-01)	Acc@1  66.25 ( 76.59)	Acc@5  97.50 ( 98.06)
## e[6] optimizer.zero_grad (sum) time: 0.10853147506713867
## e[6]       loss.backward (sum) time: 2.1901330947875977
## e[6]      optimizer.step (sum) time: 0.8680191040039062
## epoch[6] training(only) time: 10.376252889633179
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 5.8079e-01 (5.8079e-01)	Acc@1  79.00 ( 79.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.010 ( 0.028)	Loss 6.3917e-01 (6.8469e-01)	Acc@1  79.00 ( 76.55)	Acc@5  99.00 ( 98.91)
Test: [ 20/100]	Time  0.014 ( 0.022)	Loss 6.9945e-01 (6.9251e-01)	Acc@1  74.00 ( 76.14)	Acc@5 100.00 ( 98.43)
Test: [ 30/100]	Time  0.016 ( 0.020)	Loss 6.1647e-01 (6.9016e-01)	Acc@1  79.00 ( 76.35)	Acc@5  98.00 ( 98.26)
Test: [ 40/100]	Time  0.024 ( 0.018)	Loss 6.8218e-01 (6.9490e-01)	Acc@1  77.00 ( 76.39)	Acc@5  97.00 ( 98.17)
Test: [ 50/100]	Time  0.010 ( 0.017)	Loss 5.6653e-01 (6.9066e-01)	Acc@1  81.00 ( 76.86)	Acc@5  98.00 ( 98.22)
Test: [ 60/100]	Time  0.016 ( 0.017)	Loss 7.4071e-01 (6.9397e-01)	Acc@1  77.00 ( 76.75)	Acc@5  97.00 ( 98.26)
Test: [ 70/100]	Time  0.011 ( 0.017)	Loss 7.3508e-01 (6.9552e-01)	Acc@1  73.00 ( 76.58)	Acc@5  99.00 ( 98.34)
Test: [ 80/100]	Time  0.014 ( 0.016)	Loss 5.9132e-01 (6.9377e-01)	Acc@1  78.00 ( 76.52)	Acc@5  99.00 ( 98.26)
Test: [ 90/100]	Time  0.014 ( 0.016)	Loss 7.3012e-01 (6.9375e-01)	Acc@1  72.00 ( 76.57)	Acc@5  99.00 ( 98.25)
 * Acc@1 76.550 Acc@5 98.230
### epoch[6] execution time: 12.134253025054932
EPOCH 7
# current learning rate: 0.1
# Switched to train mode...
Epoch: [7][  0/391]	Time  0.186 ( 0.186)	Data  0.164 ( 0.164)	Loss 5.7707e-01 (5.7707e-01)	Acc@1  82.03 ( 82.03)	Acc@5  98.44 ( 98.44)
Epoch: [7][ 10/391]	Time  0.026 ( 0.041)	Data  0.001 ( 0.016)	Loss 8.7203e-01 (6.5887e-01)	Acc@1  76.56 ( 78.27)	Acc@5  97.66 ( 98.08)
Epoch: [7][ 20/391]	Time  0.023 ( 0.033)	Data  0.001 ( 0.010)	Loss 6.0405e-01 (6.4563e-01)	Acc@1  80.47 ( 78.98)	Acc@5  99.22 ( 98.18)
Epoch: [7][ 30/391]	Time  0.031 ( 0.031)	Data  0.001 ( 0.008)	Loss 4.0929e-01 (6.1942e-01)	Acc@1  86.72 ( 79.89)	Acc@5 100.00 ( 98.24)
Epoch: [7][ 40/391]	Time  0.024 ( 0.030)	Data  0.001 ( 0.006)	Loss 6.6485e-01 (6.3101e-01)	Acc@1  76.56 ( 79.23)	Acc@5  98.44 ( 98.25)
Epoch: [7][ 50/391]	Time  0.023 ( 0.029)	Data  0.001 ( 0.006)	Loss 8.6593e-01 (6.4870e-01)	Acc@1  70.31 ( 78.80)	Acc@5  98.44 ( 98.21)
Epoch: [7][ 60/391]	Time  0.028 ( 0.028)	Data  0.001 ( 0.005)	Loss 6.3209e-01 (6.5130e-01)	Acc@1  79.69 ( 78.82)	Acc@5  99.22 ( 98.14)
Epoch: [7][ 70/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.005)	Loss 6.7185e-01 (6.6128e-01)	Acc@1  75.78 ( 78.25)	Acc@5  98.44 ( 98.12)
Epoch: [7][ 80/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 5.6329e-01 (6.5796e-01)	Acc@1  82.81 ( 78.51)	Acc@5  98.44 ( 98.17)
Epoch: [7][ 90/391]	Time  0.027 ( 0.027)	Data  0.001 ( 0.004)	Loss 7.9145e-01 (6.6095e-01)	Acc@1  74.22 ( 78.44)	Acc@5  98.44 ( 98.18)
Epoch: [7][100/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 5.8151e-01 (6.6234e-01)	Acc@1  83.59 ( 78.41)	Acc@5  97.66 ( 98.14)
Epoch: [7][110/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 6.7272e-01 (6.6580e-01)	Acc@1  76.56 ( 78.22)	Acc@5  97.66 ( 98.13)
Epoch: [7][120/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 8.3388e-01 (6.6779e-01)	Acc@1  72.66 ( 78.06)	Acc@5  95.31 ( 98.10)
Epoch: [7][130/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.004)	Loss 6.2509e-01 (6.6505e-01)	Acc@1  78.91 ( 78.17)	Acc@5 100.00 ( 98.09)
Epoch: [7][140/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.5621e-01 (6.6259e-01)	Acc@1  78.91 ( 78.20)	Acc@5  96.88 ( 98.09)
Epoch: [7][150/391]	Time  0.031 ( 0.027)	Data  0.002 ( 0.003)	Loss 6.6665e-01 (6.6177e-01)	Acc@1  77.34 ( 78.26)	Acc@5  98.44 ( 98.12)
Epoch: [7][160/391]	Time  0.031 ( 0.026)	Data  0.000 ( 0.003)	Loss 6.1681e-01 (6.6109e-01)	Acc@1  78.91 ( 78.23)	Acc@5  98.44 ( 98.13)
Epoch: [7][170/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 6.1727e-01 (6.6038e-01)	Acc@1  75.00 ( 78.25)	Acc@5  98.44 ( 98.13)
Epoch: [7][180/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.5934e-01 (6.6163e-01)	Acc@1  80.47 ( 78.21)	Acc@5  98.44 ( 98.14)
Epoch: [7][190/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 5.9907e-01 (6.6146e-01)	Acc@1  82.81 ( 78.19)	Acc@5  96.88 ( 98.12)
Epoch: [7][200/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.6613e-01 (6.6111e-01)	Acc@1  78.91 ( 78.20)	Acc@5  96.88 ( 98.12)
Epoch: [7][210/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.0611e-01 (6.6079e-01)	Acc@1  82.03 ( 78.21)	Acc@5  99.22 ( 98.13)
Epoch: [7][220/391]	Time  0.025 ( 0.026)	Data  0.003 ( 0.003)	Loss 6.2651e-01 (6.6275e-01)	Acc@1  78.91 ( 78.13)	Acc@5  97.66 ( 98.13)
Epoch: [7][230/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.8675e-01 (6.6042e-01)	Acc@1  76.56 ( 78.18)	Acc@5  97.66 ( 98.16)
Epoch: [7][240/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.4509e-01 (6.5867e-01)	Acc@1  86.72 ( 78.26)	Acc@5  99.22 ( 98.18)
Epoch: [7][250/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.6421e-01 (6.5922e-01)	Acc@1  77.34 ( 78.24)	Acc@5  99.22 ( 98.18)
Epoch: [7][260/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.7739e-01 (6.5936e-01)	Acc@1  77.34 ( 78.24)	Acc@5 100.00 ( 98.18)
Epoch: [7][270/391]	Time  0.026 ( 0.026)	Data  0.003 ( 0.003)	Loss 7.2733e-01 (6.6046e-01)	Acc@1  78.91 ( 78.19)	Acc@5  99.22 ( 98.20)
Epoch: [7][280/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.9301e-01 (6.6208e-01)	Acc@1  79.69 ( 78.13)	Acc@5  98.44 ( 98.20)
Epoch: [7][290/391]	Time  0.027 ( 0.026)	Data  0.002 ( 0.003)	Loss 7.0371e-01 (6.6093e-01)	Acc@1  77.34 ( 78.15)	Acc@5  97.66 ( 98.20)
Epoch: [7][300/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.9379e-01 (6.6001e-01)	Acc@1  77.34 ( 78.20)	Acc@5  99.22 ( 98.22)
Epoch: [7][310/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.5072e-01 (6.5820e-01)	Acc@1  71.88 ( 78.25)	Acc@5  98.44 ( 98.23)
Epoch: [7][320/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.1428e-01 (6.5634e-01)	Acc@1  78.91 ( 78.32)	Acc@5  98.44 ( 98.25)
Epoch: [7][330/391]	Time  0.037 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.0193e-01 (6.5529e-01)	Acc@1  88.28 ( 78.34)	Acc@5  98.44 ( 98.26)
Epoch: [7][340/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.6864e-01 (6.5376e-01)	Acc@1  85.16 ( 78.38)	Acc@5  98.44 ( 98.27)
Epoch: [7][350/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.8009e-01 (6.5338e-01)	Acc@1  79.69 ( 78.38)	Acc@5  98.44 ( 98.28)
Epoch: [7][360/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.6294e-01 (6.5321e-01)	Acc@1  79.69 ( 78.39)	Acc@5  99.22 ( 98.28)
Epoch: [7][370/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.2373e-01 (6.5195e-01)	Acc@1  74.22 ( 78.43)	Acc@5  97.66 ( 98.29)
Epoch: [7][380/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.9340e-01 (6.5092e-01)	Acc@1  77.34 ( 78.41)	Acc@5  99.22 ( 98.30)
Epoch: [7][390/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.8308e-01 (6.5027e-01)	Acc@1  83.75 ( 78.45)	Acc@5 100.00 ( 98.31)
## e[7] optimizer.zero_grad (sum) time: 0.10909605026245117
## e[7]       loss.backward (sum) time: 2.2701191902160645
## e[7]      optimizer.step (sum) time: 0.8838226795196533
## epoch[7] training(only) time: 10.307758331298828
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 6.9309e-01 (6.9309e-01)	Acc@1  76.00 ( 76.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.012 ( 0.027)	Loss 6.4387e-01 (6.4883e-01)	Acc@1  77.00 ( 77.27)	Acc@5 100.00 ( 99.36)
Test: [ 20/100]	Time  0.011 ( 0.022)	Loss 6.5285e-01 (6.5738e-01)	Acc@1  76.00 ( 77.14)	Acc@5 100.00 ( 99.00)
Test: [ 30/100]	Time  0.011 ( 0.019)	Loss 6.8525e-01 (6.6106e-01)	Acc@1  79.00 ( 77.03)	Acc@5  99.00 ( 98.87)
Test: [ 40/100]	Time  0.011 ( 0.018)	Loss 6.5201e-01 (6.5741e-01)	Acc@1  74.00 ( 76.90)	Acc@5  98.00 ( 98.59)
Test: [ 50/100]	Time  0.013 ( 0.018)	Loss 6.6565e-01 (6.4861e-01)	Acc@1  75.00 ( 77.27)	Acc@5  97.00 ( 98.55)
Test: [ 60/100]	Time  0.019 ( 0.018)	Loss 6.3255e-01 (6.4648e-01)	Acc@1  80.00 ( 77.26)	Acc@5  99.00 ( 98.56)
Test: [ 70/100]	Time  0.011 ( 0.017)	Loss 6.1902e-01 (6.4839e-01)	Acc@1  80.00 ( 77.17)	Acc@5  99.00 ( 98.63)
Test: [ 80/100]	Time  0.011 ( 0.017)	Loss 4.6074e-01 (6.4635e-01)	Acc@1  82.00 ( 77.15)	Acc@5  99.00 ( 98.62)
Test: [ 90/100]	Time  0.012 ( 0.017)	Loss 5.2559e-01 (6.4564e-01)	Acc@1  81.00 ( 77.29)	Acc@5 100.00 ( 98.64)
 * Acc@1 77.410 Acc@5 98.680
### epoch[7] execution time: 12.081590414047241
EPOCH 8
# current learning rate: 0.1
# Switched to train mode...
Epoch: [8][  0/391]	Time  0.190 ( 0.190)	Data  0.169 ( 0.169)	Loss 6.4533e-01 (6.4533e-01)	Acc@1  75.78 ( 75.78)	Acc@5  97.66 ( 97.66)
Epoch: [8][ 10/391]	Time  0.023 ( 0.041)	Data  0.001 ( 0.017)	Loss 5.6045e-01 (5.9469e-01)	Acc@1  79.69 ( 79.33)	Acc@5  99.22 ( 98.72)
Epoch: [8][ 20/391]	Time  0.023 ( 0.033)	Data  0.001 ( 0.010)	Loss 5.9106e-01 (5.7958e-01)	Acc@1  80.47 ( 80.73)	Acc@5  99.22 ( 98.59)
Epoch: [8][ 30/391]	Time  0.023 ( 0.031)	Data  0.001 ( 0.007)	Loss 4.0125e-01 (5.7307e-01)	Acc@1  89.06 ( 80.82)	Acc@5 100.00 ( 98.54)
Epoch: [8][ 40/391]	Time  0.023 ( 0.029)	Data  0.002 ( 0.006)	Loss 5.7277e-01 (5.8613e-01)	Acc@1  77.34 ( 80.30)	Acc@5  98.44 ( 98.46)
Epoch: [8][ 50/391]	Time  0.026 ( 0.029)	Data  0.001 ( 0.005)	Loss 6.7043e-01 (5.9077e-01)	Acc@1  75.78 ( 80.16)	Acc@5  97.66 ( 98.48)
Epoch: [8][ 60/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.005)	Loss 7.0081e-01 (5.9441e-01)	Acc@1  78.91 ( 80.15)	Acc@5  95.31 ( 98.40)
Epoch: [8][ 70/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.004)	Loss 6.5986e-01 (6.0319e-01)	Acc@1  76.56 ( 80.01)	Acc@5  98.44 ( 98.47)
Epoch: [8][ 80/391]	Time  0.032 ( 0.028)	Data  0.001 ( 0.004)	Loss 7.0044e-01 (6.1311e-01)	Acc@1  78.12 ( 79.77)	Acc@5  97.66 ( 98.39)
Epoch: [8][ 90/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 5.8854e-01 (6.0940e-01)	Acc@1  77.34 ( 79.90)	Acc@5  99.22 ( 98.33)
Epoch: [8][100/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 5.4687e-01 (6.0707e-01)	Acc@1  82.81 ( 80.00)	Acc@5  99.22 ( 98.40)
Epoch: [8][110/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.004)	Loss 5.7305e-01 (6.0467e-01)	Acc@1  78.91 ( 80.05)	Acc@5 100.00 ( 98.42)
Epoch: [8][120/391]	Time  0.027 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.8352e-01 (6.0651e-01)	Acc@1  74.22 ( 80.00)	Acc@5  97.66 ( 98.39)
Epoch: [8][130/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.9451e-01 (6.1163e-01)	Acc@1  76.56 ( 79.89)	Acc@5  97.66 ( 98.38)
Epoch: [8][140/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.3763e-01 (6.1295e-01)	Acc@1  73.44 ( 79.85)	Acc@5  98.44 ( 98.38)
Epoch: [8][150/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.8952e-01 (6.0998e-01)	Acc@1  80.47 ( 79.99)	Acc@5  98.44 ( 98.39)
Epoch: [8][160/391]	Time  0.030 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.5618e-01 (6.0828e-01)	Acc@1  80.47 ( 80.05)	Acc@5  99.22 ( 98.42)
Epoch: [8][170/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 5.7919e-01 (6.0775e-01)	Acc@1  80.47 ( 80.08)	Acc@5 100.00 ( 98.41)
Epoch: [8][180/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.3830e-01 (6.0821e-01)	Acc@1  77.34 ( 79.99)	Acc@5  99.22 ( 98.42)
Epoch: [8][190/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 9.1341e-01 (6.1242e-01)	Acc@1  67.97 ( 79.86)	Acc@5  98.44 ( 98.40)
Epoch: [8][200/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.8271e-01 (6.1190e-01)	Acc@1  79.69 ( 79.85)	Acc@5  99.22 ( 98.43)
Epoch: [8][210/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 5.7124e-01 (6.1129e-01)	Acc@1  80.47 ( 79.86)	Acc@5  99.22 ( 98.45)
Epoch: [8][220/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.3855e-01 (6.1147e-01)	Acc@1  72.66 ( 79.79)	Acc@5  96.88 ( 98.46)
Epoch: [8][230/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.8172e-01 (6.1133e-01)	Acc@1  78.12 ( 79.84)	Acc@5  99.22 ( 98.46)
Epoch: [8][240/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.1739e-01 (6.1019e-01)	Acc@1  78.91 ( 79.86)	Acc@5 100.00 ( 98.47)
Epoch: [8][250/391]	Time  0.035 ( 0.026)	Data  0.010 ( 0.003)	Loss 5.5543e-01 (6.0844e-01)	Acc@1  82.81 ( 79.92)	Acc@5 100.00 ( 98.49)
Epoch: [8][260/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.3263e-01 (6.0913e-01)	Acc@1  78.91 ( 79.91)	Acc@5  97.66 ( 98.49)
Epoch: [8][270/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.6353e-01 (6.0939e-01)	Acc@1  73.44 ( 79.87)	Acc@5  96.88 ( 98.47)
Epoch: [8][280/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.0493e-01 (6.0831e-01)	Acc@1  82.03 ( 79.88)	Acc@5  98.44 ( 98.48)
Epoch: [8][290/391]	Time  0.034 ( 0.026)	Data  0.002 ( 0.003)	Loss 5.3278e-01 (6.0713e-01)	Acc@1  81.25 ( 79.91)	Acc@5 100.00 ( 98.49)
Epoch: [8][300/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.1696e-01 (6.0785e-01)	Acc@1  77.34 ( 79.90)	Acc@5  99.22 ( 98.49)
Epoch: [8][310/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.4071e-01 (6.0853e-01)	Acc@1  78.91 ( 79.89)	Acc@5  98.44 ( 98.51)
Epoch: [8][320/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 6.5465e-01 (6.0768e-01)	Acc@1  76.56 ( 79.93)	Acc@5  98.44 ( 98.49)
Epoch: [8][330/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.3978e-01 (6.0726e-01)	Acc@1  78.12 ( 79.94)	Acc@5  99.22 ( 98.50)
Epoch: [8][340/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.6390e-01 (6.0654e-01)	Acc@1  84.38 ( 79.93)	Acc@5 100.00 ( 98.52)
Epoch: [8][350/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.0260e-01 (6.0801e-01)	Acc@1  76.56 ( 79.88)	Acc@5  96.88 ( 98.50)
Epoch: [8][360/391]	Time  0.027 ( 0.026)	Data  0.007 ( 0.003)	Loss 6.5785e-01 (6.0718e-01)	Acc@1  78.12 ( 79.91)	Acc@5  97.66 ( 98.50)
Epoch: [8][370/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 5.7670e-01 (6.0671e-01)	Acc@1  76.56 ( 79.91)	Acc@5 100.00 ( 98.51)
Epoch: [8][380/391]	Time  0.025 ( 0.026)	Data  0.000 ( 0.003)	Loss 6.3182e-01 (6.0812e-01)	Acc@1  78.12 ( 79.85)	Acc@5  99.22 ( 98.50)
Epoch: [8][390/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.9488e-01 (6.0733e-01)	Acc@1  80.00 ( 79.89)	Acc@5  98.75 ( 98.50)
## e[8] optimizer.zero_grad (sum) time: 0.10921025276184082
## e[8]       loss.backward (sum) time: 2.2519724369049072
## e[8]      optimizer.step (sum) time: 0.8753390312194824
## epoch[8] training(only) time: 10.407080888748169
# Switched to evaluate mode...
Test: [  0/100]	Time  0.157 ( 0.157)	Loss 6.2522e-01 (6.2522e-01)	Acc@1  82.00 ( 82.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.013 ( 0.029)	Loss 7.0837e-01 (6.0288e-01)	Acc@1  77.00 ( 79.64)	Acc@5  96.00 ( 99.00)
Test: [ 20/100]	Time  0.014 ( 0.022)	Loss 6.4517e-01 (6.2365e-01)	Acc@1  77.00 ( 79.43)	Acc@5 100.00 ( 98.62)
Test: [ 30/100]	Time  0.016 ( 0.020)	Loss 6.6149e-01 (6.2304e-01)	Acc@1  82.00 ( 79.39)	Acc@5  98.00 ( 98.55)
Test: [ 40/100]	Time  0.011 ( 0.018)	Loss 6.4746e-01 (6.2243e-01)	Acc@1  80.00 ( 79.37)	Acc@5  98.00 ( 98.59)
Test: [ 50/100]	Time  0.026 ( 0.017)	Loss 5.4705e-01 (6.1414e-01)	Acc@1  83.00 ( 79.80)	Acc@5  97.00 ( 98.49)
Test: [ 60/100]	Time  0.028 ( 0.017)	Loss 5.8541e-01 (6.1899e-01)	Acc@1  81.00 ( 79.57)	Acc@5  99.00 ( 98.49)
Test: [ 70/100]	Time  0.016 ( 0.016)	Loss 5.8040e-01 (6.2014e-01)	Acc@1  80.00 ( 79.35)	Acc@5  99.00 ( 98.62)
Test: [ 80/100]	Time  0.012 ( 0.016)	Loss 4.2332e-01 (6.1818e-01)	Acc@1  86.00 ( 79.38)	Acc@5 100.00 ( 98.67)
Test: [ 90/100]	Time  0.015 ( 0.016)	Loss 4.3582e-01 (6.1728e-01)	Acc@1  85.00 ( 79.43)	Acc@5 100.00 ( 98.70)
 * Acc@1 79.390 Acc@5 98.720
### epoch[8] execution time: 12.104905605316162
EPOCH 9
# current learning rate: 0.1
# Switched to train mode...
Epoch: [9][  0/391]	Time  0.186 ( 0.186)	Data  0.164 ( 0.164)	Loss 5.6620e-01 (5.6620e-01)	Acc@1  84.38 ( 84.38)	Acc@5  99.22 ( 99.22)
Epoch: [9][ 10/391]	Time  0.024 ( 0.040)	Data  0.001 ( 0.017)	Loss 6.1823e-01 (5.8049e-01)	Acc@1  77.34 ( 81.32)	Acc@5  99.22 ( 98.15)
Epoch: [9][ 20/391]	Time  0.023 ( 0.033)	Data  0.001 ( 0.010)	Loss 5.8284e-01 (5.8269e-01)	Acc@1  81.25 ( 81.70)	Acc@5 100.00 ( 98.44)
Epoch: [9][ 30/391]	Time  0.029 ( 0.031)	Data  0.009 ( 0.008)	Loss 4.7144e-01 (5.7604e-01)	Acc@1  88.28 ( 81.70)	Acc@5  99.22 ( 98.64)
Epoch: [9][ 40/391]	Time  0.022 ( 0.029)	Data  0.001 ( 0.006)	Loss 6.8883e-01 (5.7644e-01)	Acc@1  80.47 ( 81.48)	Acc@5  99.22 ( 98.57)
Epoch: [9][ 50/391]	Time  0.023 ( 0.029)	Data  0.001 ( 0.005)	Loss 5.7959e-01 (5.7518e-01)	Acc@1  83.59 ( 81.53)	Acc@5 100.00 ( 98.67)
Epoch: [9][ 60/391]	Time  0.033 ( 0.028)	Data  0.002 ( 0.005)	Loss 6.6822e-01 (5.7688e-01)	Acc@1  76.56 ( 81.25)	Acc@5  97.66 ( 98.71)
Epoch: [9][ 70/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 5.1031e-01 (5.7496e-01)	Acc@1  84.38 ( 81.27)	Acc@5  98.44 ( 98.73)
Epoch: [9][ 80/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 5.5681e-01 (5.7481e-01)	Acc@1  84.38 ( 81.32)	Acc@5  97.66 ( 98.73)
Epoch: [9][ 90/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 4.8872e-01 (5.7405e-01)	Acc@1  83.59 ( 81.30)	Acc@5 100.00 ( 98.76)
Epoch: [9][100/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 4.5448e-01 (5.7290e-01)	Acc@1  85.16 ( 81.37)	Acc@5  99.22 ( 98.78)
Epoch: [9][110/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 6.7122e-01 (5.7246e-01)	Acc@1  80.47 ( 81.42)	Acc@5  97.66 ( 98.75)
Epoch: [9][120/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 4.8256e-01 (5.6963e-01)	Acc@1  85.94 ( 81.42)	Acc@5 100.00 ( 98.81)
Epoch: [9][130/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.4811e-01 (5.7037e-01)	Acc@1  79.69 ( 81.37)	Acc@5  98.44 ( 98.78)
Epoch: [9][140/391]	Time  0.024 ( 0.027)	Data  0.005 ( 0.003)	Loss 6.4953e-01 (5.7372e-01)	Acc@1  77.34 ( 81.29)	Acc@5  96.88 ( 98.71)
Epoch: [9][150/391]	Time  0.022 ( 0.027)	Data  0.002 ( 0.003)	Loss 5.1414e-01 (5.7495e-01)	Acc@1  82.81 ( 81.27)	Acc@5  96.88 ( 98.73)
Epoch: [9][160/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.5892e-01 (5.7633e-01)	Acc@1  84.38 ( 81.22)	Acc@5  98.44 ( 98.75)
Epoch: [9][170/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.6763e-01 (5.7769e-01)	Acc@1  80.47 ( 81.12)	Acc@5  99.22 ( 98.72)
Epoch: [9][180/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.4990e-01 (5.7896e-01)	Acc@1  80.47 ( 81.03)	Acc@5  99.22 ( 98.71)
Epoch: [9][190/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.2011e-01 (5.7952e-01)	Acc@1  85.94 ( 81.03)	Acc@5 100.00 ( 98.68)
Epoch: [9][200/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.0882e-01 (5.7892e-01)	Acc@1  80.47 ( 81.06)	Acc@5  97.66 ( 98.68)
Epoch: [9][210/391]	Time  0.023 ( 0.027)	Data  0.004 ( 0.003)	Loss 6.1652e-01 (5.7873e-01)	Acc@1  80.47 ( 81.11)	Acc@5  99.22 ( 98.70)
Epoch: [9][220/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.5856e-01 (5.7812e-01)	Acc@1  77.34 ( 81.07)	Acc@5  99.22 ( 98.70)
Epoch: [9][230/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.8664e-01 (5.8080e-01)	Acc@1  82.81 ( 81.03)	Acc@5  97.66 ( 98.68)
Epoch: [9][240/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.8593e-01 (5.8199e-01)	Acc@1  70.31 ( 81.00)	Acc@5  98.44 ( 98.66)
Epoch: [9][250/391]	Time  0.022 ( 0.026)	Data  0.000 ( 0.003)	Loss 5.8929e-01 (5.8133e-01)	Acc@1  78.91 ( 80.98)	Acc@5  98.44 ( 98.65)
Epoch: [9][260/391]	Time  0.032 ( 0.026)	Data  0.006 ( 0.003)	Loss 5.0474e-01 (5.7996e-01)	Acc@1  82.03 ( 81.01)	Acc@5  99.22 ( 98.66)
Epoch: [9][270/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.8340e-01 (5.7974e-01)	Acc@1  78.12 ( 80.99)	Acc@5  95.31 ( 98.63)
Epoch: [9][280/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.7562e-01 (5.7896e-01)	Acc@1  80.47 ( 81.01)	Acc@5  99.22 ( 98.64)
Epoch: [9][290/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.9671e-01 (5.7869e-01)	Acc@1  82.03 ( 81.01)	Acc@5  99.22 ( 98.65)
Epoch: [9][300/391]	Time  0.025 ( 0.026)	Data  0.002 ( 0.003)	Loss 5.9401e-01 (5.7801e-01)	Acc@1  78.91 ( 81.04)	Acc@5 100.00 ( 98.66)
Epoch: [9][310/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.1293e-01 (5.7829e-01)	Acc@1  75.78 ( 81.03)	Acc@5  98.44 ( 98.65)
Epoch: [9][320/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.7115e-01 (5.7977e-01)	Acc@1  76.56 ( 80.97)	Acc@5  97.66 ( 98.63)
Epoch: [9][330/391]	Time  0.033 ( 0.026)	Data  0.000 ( 0.003)	Loss 6.6333e-01 (5.8056e-01)	Acc@1  81.25 ( 80.93)	Acc@5  98.44 ( 98.63)
Epoch: [9][340/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.6808e-01 (5.8116e-01)	Acc@1  74.22 ( 80.92)	Acc@5  99.22 ( 98.63)
Epoch: [9][350/391]	Time  0.026 ( 0.026)	Data  0.006 ( 0.003)	Loss 5.7288e-01 (5.8047e-01)	Acc@1  79.69 ( 80.94)	Acc@5  98.44 ( 98.64)
Epoch: [9][360/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.2317e-01 (5.8013e-01)	Acc@1  76.56 ( 80.94)	Acc@5 100.00 ( 98.66)
Epoch: [9][370/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.8144e-01 (5.7937e-01)	Acc@1  80.47 ( 80.96)	Acc@5  99.22 ( 98.67)
Epoch: [9][380/391]	Time  0.036 ( 0.026)	Data  0.004 ( 0.003)	Loss 4.5453e-01 (5.7872e-01)	Acc@1  82.81 ( 80.98)	Acc@5 100.00 ( 98.67)
Epoch: [9][390/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.7313e-01 (5.7907e-01)	Acc@1  80.00 ( 80.96)	Acc@5  98.75 ( 98.67)
## e[9] optimizer.zero_grad (sum) time: 0.11006784439086914
## e[9]       loss.backward (sum) time: 2.1848576068878174
## e[9]      optimizer.step (sum) time: 0.8749783039093018
## epoch[9] training(only) time: 10.441004276275635
# Switched to evaluate mode...
Test: [  0/100]	Time  0.165 ( 0.165)	Loss 6.7220e-01 (6.7220e-01)	Acc@1  80.00 ( 80.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.012 ( 0.027)	Loss 6.7418e-01 (7.0527e-01)	Acc@1  83.00 ( 77.27)	Acc@5  99.00 ( 98.64)
Test: [ 20/100]	Time  0.011 ( 0.021)	Loss 6.8636e-01 (7.0192e-01)	Acc@1  79.00 ( 76.62)	Acc@5  98.00 ( 98.24)
Test: [ 30/100]	Time  0.012 ( 0.018)	Loss 7.1147e-01 (7.2411e-01)	Acc@1  77.00 ( 76.29)	Acc@5  99.00 ( 98.13)
Test: [ 40/100]	Time  0.013 ( 0.017)	Loss 7.2986e-01 (7.1598e-01)	Acc@1  78.00 ( 76.76)	Acc@5  98.00 ( 98.07)
Test: [ 50/100]	Time  0.018 ( 0.017)	Loss 7.2112e-01 (7.1613e-01)	Acc@1  76.00 ( 76.73)	Acc@5  98.00 ( 98.06)
Test: [ 60/100]	Time  0.011 ( 0.016)	Loss 6.4637e-01 (7.2345e-01)	Acc@1  77.00 ( 76.39)	Acc@5  98.00 ( 98.07)
Test: [ 70/100]	Time  0.014 ( 0.016)	Loss 7.6439e-01 (7.1906e-01)	Acc@1  72.00 ( 76.51)	Acc@5  97.00 ( 98.10)
Test: [ 80/100]	Time  0.012 ( 0.016)	Loss 6.8681e-01 (7.2028e-01)	Acc@1  78.00 ( 76.40)	Acc@5  99.00 ( 98.12)
Test: [ 90/100]	Time  0.023 ( 0.016)	Loss 6.6521e-01 (7.2388e-01)	Acc@1  78.00 ( 76.24)	Acc@5 100.00 ( 98.14)
 * Acc@1 76.410 Acc@5 98.130
### epoch[9] execution time: 12.097524166107178
EPOCH 10
# current learning rate: 0.1
# Switched to train mode...
Epoch: [10][  0/391]	Time  0.197 ( 0.197)	Data  0.177 ( 0.177)	Loss 6.7080e-01 (6.7080e-01)	Acc@1  82.03 ( 82.03)	Acc@5  96.09 ( 96.09)
Epoch: [10][ 10/391]	Time  0.024 ( 0.041)	Data  0.001 ( 0.018)	Loss 4.1652e-01 (5.7356e-01)	Acc@1  86.72 ( 81.11)	Acc@5  99.22 ( 98.22)
Epoch: [10][ 20/391]	Time  0.023 ( 0.033)	Data  0.001 ( 0.010)	Loss 5.2062e-01 (5.3017e-01)	Acc@1  85.94 ( 82.74)	Acc@5  96.88 ( 98.66)
Epoch: [10][ 30/391]	Time  0.025 ( 0.031)	Data  0.001 ( 0.008)	Loss 4.0793e-01 (5.4403e-01)	Acc@1  85.94 ( 82.03)	Acc@5 100.00 ( 98.79)
Epoch: [10][ 40/391]	Time  0.023 ( 0.030)	Data  0.001 ( 0.006)	Loss 6.8491e-01 (5.4160e-01)	Acc@1  77.34 ( 82.09)	Acc@5  99.22 ( 98.76)
Epoch: [10][ 50/391]	Time  0.023 ( 0.029)	Data  0.001 ( 0.005)	Loss 5.4360e-01 (5.4360e-01)	Acc@1  82.81 ( 81.99)	Acc@5  99.22 ( 98.74)
Epoch: [10][ 60/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.005)	Loss 5.4327e-01 (5.4275e-01)	Acc@1  83.59 ( 82.13)	Acc@5  98.44 ( 98.73)
Epoch: [10][ 70/391]	Time  0.023 ( 0.028)	Data  0.000 ( 0.005)	Loss 5.0184e-01 (5.3737e-01)	Acc@1  84.38 ( 82.37)	Acc@5  97.66 ( 98.73)
Epoch: [10][ 80/391]	Time  0.022 ( 0.028)	Data  0.001 ( 0.004)	Loss 5.6783e-01 (5.3897e-01)	Acc@1  81.25 ( 82.25)	Acc@5 100.00 ( 98.76)
Epoch: [10][ 90/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 4.5628e-01 (5.3760e-01)	Acc@1  82.81 ( 82.27)	Acc@5 100.00 ( 98.70)
Epoch: [10][100/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 8.3482e-01 (5.4039e-01)	Acc@1  76.56 ( 82.20)	Acc@5  96.09 ( 98.69)
Epoch: [10][110/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.004)	Loss 4.4954e-01 (5.3991e-01)	Acc@1  85.94 ( 82.24)	Acc@5  97.66 ( 98.66)
Epoch: [10][120/391]	Time  0.037 ( 0.027)	Data  0.001 ( 0.004)	Loss 5.5018e-01 (5.3752e-01)	Acc@1  78.91 ( 82.18)	Acc@5 100.00 ( 98.68)
Epoch: [10][130/391]	Time  0.023 ( 0.027)	Data  0.000 ( 0.003)	Loss 6.4347e-01 (5.4328e-01)	Acc@1  79.69 ( 82.05)	Acc@5  99.22 ( 98.68)
Epoch: [10][140/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.2850e-01 (5.4524e-01)	Acc@1  79.69 ( 82.05)	Acc@5  98.44 ( 98.69)
Epoch: [10][150/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 5.3282e-01 (5.4310e-01)	Acc@1  81.25 ( 82.08)	Acc@5  98.44 ( 98.71)
Epoch: [10][160/391]	Time  0.028 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.0164e-01 (5.4743e-01)	Acc@1  83.59 ( 82.03)	Acc@5  99.22 ( 98.69)
Epoch: [10][170/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.3777e-01 (5.4701e-01)	Acc@1  85.16 ( 82.03)	Acc@5  99.22 ( 98.66)
Epoch: [10][180/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.9383e-01 (5.4952e-01)	Acc@1  80.47 ( 81.99)	Acc@5  97.66 ( 98.65)
Epoch: [10][190/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.6079e-01 (5.4959e-01)	Acc@1  80.47 ( 81.97)	Acc@5  97.66 ( 98.63)
Epoch: [10][200/391]	Time  0.024 ( 0.026)	Data  0.002 ( 0.003)	Loss 8.0889e-01 (5.5067e-01)	Acc@1  70.31 ( 81.87)	Acc@5  96.88 ( 98.64)
Epoch: [10][210/391]	Time  0.027 ( 0.027)	Data  0.002 ( 0.003)	Loss 5.6082e-01 (5.4991e-01)	Acc@1  81.25 ( 81.89)	Acc@5  99.22 ( 98.67)
Epoch: [10][220/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.1831e-01 (5.4900e-01)	Acc@1  87.50 ( 81.91)	Acc@5  99.22 ( 98.66)
Epoch: [10][230/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.3623e-01 (5.4749e-01)	Acc@1  85.16 ( 81.99)	Acc@5 100.00 ( 98.67)
Epoch: [10][240/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.9271e-01 (5.4681e-01)	Acc@1  83.59 ( 81.99)	Acc@5  99.22 ( 98.68)
Epoch: [10][250/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.0067e-01 (5.4537e-01)	Acc@1  81.25 ( 82.06)	Acc@5  99.22 ( 98.69)
Epoch: [10][260/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.5695e-01 (5.4433e-01)	Acc@1  81.25 ( 82.07)	Acc@5 100.00 ( 98.69)
Epoch: [10][270/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.2218e-01 (5.4329e-01)	Acc@1  79.69 ( 82.12)	Acc@5  96.88 ( 98.70)
Epoch: [10][280/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.6353e-01 (5.4169e-01)	Acc@1  79.69 ( 82.17)	Acc@5 100.00 ( 98.72)
Epoch: [10][290/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.4509e-01 (5.4182e-01)	Acc@1  80.47 ( 82.16)	Acc@5  99.22 ( 98.73)
Epoch: [10][300/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.6499e-01 (5.4092e-01)	Acc@1  82.81 ( 82.19)	Acc@5 100.00 ( 98.74)
Epoch: [10][310/391]	Time  0.035 ( 0.026)	Data  0.002 ( 0.003)	Loss 5.3641e-01 (5.4191e-01)	Acc@1  81.25 ( 82.19)	Acc@5  99.22 ( 98.72)
Epoch: [10][320/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.4569e-01 (5.4258e-01)	Acc@1  82.81 ( 82.17)	Acc@5  98.44 ( 98.72)
Epoch: [10][330/391]	Time  0.038 ( 0.026)	Data  0.000 ( 0.003)	Loss 5.3074e-01 (5.4340e-01)	Acc@1  82.81 ( 82.14)	Acc@5  98.44 ( 98.74)
Epoch: [10][340/391]	Time  0.024 ( 0.026)	Data  0.004 ( 0.003)	Loss 5.9102e-01 (5.4253e-01)	Acc@1  77.34 ( 82.15)	Acc@5  98.44 ( 98.75)
Epoch: [10][350/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 5.6497e-01 (5.4254e-01)	Acc@1  79.69 ( 82.16)	Acc@5  99.22 ( 98.75)
Epoch: [10][360/391]	Time  0.022 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.5703e-01 (5.4133e-01)	Acc@1  87.50 ( 82.18)	Acc@5 100.00 ( 98.77)
Epoch: [10][370/391]	Time  0.030 ( 0.026)	Data  0.003 ( 0.003)	Loss 5.8859e-01 (5.4040e-01)	Acc@1  82.03 ( 82.24)	Acc@5  98.44 ( 98.77)
Epoch: [10][380/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.6018e-01 (5.4080e-01)	Acc@1  85.94 ( 82.21)	Acc@5  97.66 ( 98.78)
Epoch: [10][390/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.4846e-01 (5.3953e-01)	Acc@1  87.50 ( 82.24)	Acc@5 100.00 ( 98.78)
## e[10] optimizer.zero_grad (sum) time: 0.10901331901550293
## e[10]       loss.backward (sum) time: 2.267214775085449
## e[10]      optimizer.step (sum) time: 0.880821704864502
## epoch[10] training(only) time: 10.389490842819214
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 4.9944e-01 (4.9944e-01)	Acc@1  84.00 ( 84.00)	Acc@5  97.00 ( 97.00)
Test: [ 10/100]	Time  0.010 ( 0.028)	Loss 6.6967e-01 (5.7825e-01)	Acc@1  79.00 ( 80.45)	Acc@5  99.00 ( 99.00)
Test: [ 20/100]	Time  0.012 ( 0.021)	Loss 5.5539e-01 (5.6654e-01)	Acc@1  81.00 ( 80.29)	Acc@5  99.00 ( 99.10)
Test: [ 30/100]	Time  0.016 ( 0.019)	Loss 6.2697e-01 (5.9271e-01)	Acc@1  75.00 ( 79.94)	Acc@5 100.00 ( 99.10)
Test: [ 40/100]	Time  0.011 ( 0.018)	Loss 5.1691e-01 (5.9081e-01)	Acc@1  84.00 ( 80.12)	Acc@5  99.00 ( 99.00)
Test: [ 50/100]	Time  0.028 ( 0.017)	Loss 6.3200e-01 (5.9534e-01)	Acc@1  79.00 ( 79.92)	Acc@5  98.00 ( 98.92)
Test: [ 60/100]	Time  0.016 ( 0.017)	Loss 4.9373e-01 (5.9870e-01)	Acc@1  81.00 ( 79.95)	Acc@5  99.00 ( 98.98)
Test: [ 70/100]	Time  0.022 ( 0.017)	Loss 5.3006e-01 (5.9347e-01)	Acc@1  84.00 ( 80.18)	Acc@5 100.00 ( 99.01)
Test: [ 80/100]	Time  0.021 ( 0.016)	Loss 5.1725e-01 (5.9235e-01)	Acc@1  85.00 ( 80.23)	Acc@5 100.00 ( 99.00)
Test: [ 90/100]	Time  0.015 ( 0.016)	Loss 5.6089e-01 (5.9026e-01)	Acc@1  83.00 ( 80.33)	Acc@5 100.00 ( 99.08)
 * Acc@1 80.270 Acc@5 99.120
### epoch[10] execution time: 12.120690822601318
EPOCH 11
# current learning rate: 0.1
# Switched to train mode...
Epoch: [11][  0/391]	Time  0.194 ( 0.194)	Data  0.174 ( 0.174)	Loss 3.8682e-01 (3.8682e-01)	Acc@1  86.72 ( 86.72)	Acc@5  99.22 ( 99.22)
Epoch: [11][ 10/391]	Time  0.028 ( 0.041)	Data  0.001 ( 0.018)	Loss 6.8247e-01 (4.8797e-01)	Acc@1  78.91 ( 83.95)	Acc@5  98.44 ( 98.86)
Epoch: [11][ 20/391]	Time  0.024 ( 0.034)	Data  0.001 ( 0.011)	Loss 5.9620e-01 (5.0369e-01)	Acc@1  82.03 ( 83.82)	Acc@5  99.22 ( 99.00)
Epoch: [11][ 30/391]	Time  0.024 ( 0.031)	Data  0.001 ( 0.008)	Loss 4.1440e-01 (5.1414e-01)	Acc@1  87.50 ( 83.34)	Acc@5  99.22 ( 98.94)
Epoch: [11][ 40/391]	Time  0.023 ( 0.030)	Data  0.001 ( 0.007)	Loss 4.3463e-01 (5.1271e-01)	Acc@1  87.50 ( 83.19)	Acc@5  98.44 ( 98.93)
Epoch: [11][ 50/391]	Time  0.024 ( 0.029)	Data  0.001 ( 0.006)	Loss 5.3477e-01 (5.1349e-01)	Acc@1  79.69 ( 83.09)	Acc@5  97.66 ( 98.88)
Epoch: [11][ 60/391]	Time  0.023 ( 0.029)	Data  0.001 ( 0.005)	Loss 4.6434e-01 (5.1385e-01)	Acc@1  82.81 ( 82.95)	Acc@5 100.00 ( 98.87)
Epoch: [11][ 70/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.005)	Loss 5.4617e-01 (5.1727e-01)	Acc@1  80.47 ( 82.78)	Acc@5  99.22 ( 98.84)
Epoch: [11][ 80/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 4.3763e-01 (5.0884e-01)	Acc@1  85.94 ( 83.03)	Acc@5  97.66 ( 98.84)
Epoch: [11][ 90/391]	Time  0.027 ( 0.028)	Data  0.001 ( 0.004)	Loss 5.5179e-01 (5.0575e-01)	Acc@1  78.91 ( 83.18)	Acc@5  99.22 ( 98.86)
Epoch: [11][100/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.004)	Loss 5.9853e-01 (5.0804e-01)	Acc@1  76.56 ( 82.97)	Acc@5  98.44 ( 98.88)
Epoch: [11][110/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.004)	Loss 4.8816e-01 (5.1114e-01)	Acc@1  82.03 ( 82.79)	Acc@5 100.00 ( 98.92)
Epoch: [11][120/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.004)	Loss 5.4982e-01 (5.1066e-01)	Acc@1  82.03 ( 82.85)	Acc@5  99.22 ( 98.93)
Epoch: [11][130/391]	Time  0.028 ( 0.027)	Data  0.001 ( 0.004)	Loss 6.2648e-01 (5.1301e-01)	Acc@1  82.03 ( 82.81)	Acc@5  96.09 ( 98.88)
Epoch: [11][140/391]	Time  0.021 ( 0.027)	Data  0.002 ( 0.003)	Loss 4.8771e-01 (5.1719e-01)	Acc@1  82.03 ( 82.66)	Acc@5 100.00 ( 98.87)
Epoch: [11][150/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.2206e-01 (5.1798e-01)	Acc@1  82.03 ( 82.64)	Acc@5 100.00 ( 98.89)
Epoch: [11][160/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.5867e-01 (5.1819e-01)	Acc@1  75.00 ( 82.56)	Acc@5 100.00 ( 98.87)
Epoch: [11][170/391]	Time  0.022 ( 0.027)	Data  0.002 ( 0.003)	Loss 6.3846e-01 (5.1335e-01)	Acc@1  78.91 ( 82.78)	Acc@5  99.22 ( 98.90)
Epoch: [11][180/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.1325e-01 (5.1548e-01)	Acc@1  85.94 ( 82.80)	Acc@5  98.44 ( 98.86)
Epoch: [11][190/391]	Time  0.030 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.4616e-01 (5.1595e-01)	Acc@1  90.62 ( 82.81)	Acc@5  98.44 ( 98.86)
Epoch: [11][200/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 5.2692e-01 (5.1747e-01)	Acc@1  83.59 ( 82.84)	Acc@5  98.44 ( 98.87)
Epoch: [11][210/391]	Time  0.031 ( 0.027)	Data  0.002 ( 0.003)	Loss 3.7702e-01 (5.1846e-01)	Acc@1  91.41 ( 82.86)	Acc@5 100.00 ( 98.86)
Epoch: [11][220/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.7994e-01 (5.1578e-01)	Acc@1  88.28 ( 83.01)	Acc@5  97.66 ( 98.87)
Epoch: [11][230/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.2104e-01 (5.1317e-01)	Acc@1  87.50 ( 83.10)	Acc@5  98.44 ( 98.88)
Epoch: [11][240/391]	Time  0.039 ( 0.027)	Data  0.002 ( 0.003)	Loss 4.9184e-01 (5.1462e-01)	Acc@1  81.25 ( 83.07)	Acc@5 100.00 ( 98.90)
Epoch: [11][250/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.9330e-01 (5.1585e-01)	Acc@1  85.16 ( 82.98)	Acc@5  98.44 ( 98.88)
Epoch: [11][260/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.4906e-01 (5.1605e-01)	Acc@1  87.50 ( 82.94)	Acc@5 100.00 ( 98.87)
Epoch: [11][270/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.9983e-01 (5.1458e-01)	Acc@1  86.72 ( 83.01)	Acc@5 100.00 ( 98.90)
Epoch: [11][280/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 6.6491e-01 (5.1547e-01)	Acc@1  78.91 ( 82.99)	Acc@5  98.44 ( 98.90)
Epoch: [11][290/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.9060e-01 (5.1483e-01)	Acc@1  77.34 ( 83.00)	Acc@5  99.22 ( 98.91)
Epoch: [11][300/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.6588e-01 (5.1517e-01)	Acc@1  80.47 ( 82.99)	Acc@5  98.44 ( 98.92)
Epoch: [11][310/391]	Time  0.024 ( 0.026)	Data  0.002 ( 0.003)	Loss 5.0358e-01 (5.1506e-01)	Acc@1  82.03 ( 83.01)	Acc@5 100.00 ( 98.90)
Epoch: [11][320/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.6478e-01 (5.1613e-01)	Acc@1  78.12 ( 82.97)	Acc@5  98.44 ( 98.90)
Epoch: [11][330/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.9132e-01 (5.1560e-01)	Acc@1  80.47 ( 82.96)	Acc@5  97.66 ( 98.89)
Epoch: [11][340/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.4435e-01 (5.1663e-01)	Acc@1  84.38 ( 82.95)	Acc@5  97.66 ( 98.88)
Epoch: [11][350/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.7071e-01 (5.1625e-01)	Acc@1  86.72 ( 82.97)	Acc@5  99.22 ( 98.88)
Epoch: [11][360/391]	Time  0.022 ( 0.026)	Data  0.002 ( 0.003)	Loss 7.0762e-01 (5.1744e-01)	Acc@1  77.34 ( 82.96)	Acc@5  98.44 ( 98.88)
Epoch: [11][370/391]	Time  0.034 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.9702e-01 (5.1764e-01)	Acc@1  85.94 ( 82.96)	Acc@5  97.66 ( 98.87)
Epoch: [11][380/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.7999e-01 (5.1695e-01)	Acc@1  85.94 ( 82.97)	Acc@5 100.00 ( 98.88)
Epoch: [11][390/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.0386e-01 (5.1623e-01)	Acc@1  85.00 ( 82.98)	Acc@5  98.75 ( 98.88)
## e[11] optimizer.zero_grad (sum) time: 0.10898208618164062
## e[11]       loss.backward (sum) time: 2.216089963912964
## e[11]      optimizer.step (sum) time: 0.8674042224884033
## epoch[11] training(only) time: 10.40737795829773
# Switched to evaluate mode...
Test: [  0/100]	Time  0.153 ( 0.153)	Loss 5.0997e-01 (5.0997e-01)	Acc@1  84.00 ( 84.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.012 ( 0.027)	Loss 4.8649e-01 (5.5476e-01)	Acc@1  85.00 ( 82.09)	Acc@5  99.00 ( 99.18)
Test: [ 20/100]	Time  0.020 ( 0.021)	Loss 6.6291e-01 (5.7510e-01)	Acc@1  82.00 ( 81.52)	Acc@5  98.00 ( 98.71)
Test: [ 30/100]	Time  0.025 ( 0.019)	Loss 5.6285e-01 (5.7638e-01)	Acc@1  80.00 ( 81.48)	Acc@5  98.00 ( 98.68)
Test: [ 40/100]	Time  0.014 ( 0.018)	Loss 5.5030e-01 (5.7932e-01)	Acc@1  84.00 ( 81.46)	Acc@5  98.00 ( 98.63)
Test: [ 50/100]	Time  0.012 ( 0.017)	Loss 4.8007e-01 (5.7942e-01)	Acc@1  86.00 ( 81.88)	Acc@5  98.00 ( 98.55)
Test: [ 60/100]	Time  0.012 ( 0.017)	Loss 5.6832e-01 (5.8852e-01)	Acc@1  84.00 ( 81.48)	Acc@5 100.00 ( 98.59)
Test: [ 70/100]	Time  0.020 ( 0.016)	Loss 6.1185e-01 (5.8587e-01)	Acc@1  81.00 ( 81.58)	Acc@5  98.00 ( 98.59)
Test: [ 80/100]	Time  0.023 ( 0.016)	Loss 3.8406e-01 (5.8961e-01)	Acc@1  85.00 ( 81.43)	Acc@5 100.00 ( 98.60)
Test: [ 90/100]	Time  0.010 ( 0.016)	Loss 4.2753e-01 (5.8945e-01)	Acc@1  89.00 ( 81.44)	Acc@5 100.00 ( 98.60)
 * Acc@1 81.410 Acc@5 98.590
### epoch[11] execution time: 12.14488410949707
EPOCH 12
# current learning rate: 0.1
# Switched to train mode...
Epoch: [12][  0/391]	Time  0.192 ( 0.192)	Data  0.170 ( 0.170)	Loss 4.8919e-01 (4.8919e-01)	Acc@1  80.47 ( 80.47)	Acc@5  99.22 ( 99.22)
Epoch: [12][ 10/391]	Time  0.027 ( 0.041)	Data  0.001 ( 0.017)	Loss 6.1733e-01 (5.1117e-01)	Acc@1  78.91 ( 82.17)	Acc@5 100.00 ( 98.93)
Epoch: [12][ 20/391]	Time  0.027 ( 0.033)	Data  0.001 ( 0.010)	Loss 4.0958e-01 (4.9302e-01)	Acc@1  86.72 ( 83.15)	Acc@5  99.22 ( 99.00)
Epoch: [12][ 30/391]	Time  0.023 ( 0.031)	Data  0.001 ( 0.008)	Loss 6.0100e-01 (4.9150e-01)	Acc@1  79.69 ( 83.44)	Acc@5  98.44 ( 99.02)
Epoch: [12][ 40/391]	Time  0.023 ( 0.030)	Data  0.004 ( 0.007)	Loss 3.7819e-01 (4.7929e-01)	Acc@1  85.94 ( 83.82)	Acc@5  97.66 ( 99.01)
Epoch: [12][ 50/391]	Time  0.022 ( 0.029)	Data  0.001 ( 0.006)	Loss 4.8394e-01 (4.8278e-01)	Acc@1  83.59 ( 83.73)	Acc@5 100.00 ( 99.05)
Epoch: [12][ 60/391]	Time  0.027 ( 0.028)	Data  0.001 ( 0.005)	Loss 4.9075e-01 (4.8042e-01)	Acc@1  83.59 ( 83.97)	Acc@5  99.22 ( 99.10)
Epoch: [12][ 70/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.005)	Loss 5.0006e-01 (4.7625e-01)	Acc@1  81.25 ( 84.09)	Acc@5  99.22 ( 99.06)
Epoch: [12][ 80/391]	Time  0.025 ( 0.028)	Data  0.002 ( 0.004)	Loss 4.9518e-01 (4.7820e-01)	Acc@1  79.69 ( 84.07)	Acc@5  98.44 ( 99.04)
Epoch: [12][ 90/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.004)	Loss 5.4847e-01 (4.7475e-01)	Acc@1  79.69 ( 84.12)	Acc@5  98.44 ( 99.04)
Epoch: [12][100/391]	Time  0.027 ( 0.027)	Data  0.001 ( 0.004)	Loss 4.6565e-01 (4.7682e-01)	Acc@1  86.72 ( 84.04)	Acc@5  99.22 ( 99.06)
Epoch: [12][110/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 6.2715e-01 (4.8014e-01)	Acc@1  78.91 ( 83.88)	Acc@5  98.44 ( 99.03)
Epoch: [12][120/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.004)	Loss 5.3253e-01 (4.8244e-01)	Acc@1  82.03 ( 83.79)	Acc@5  99.22 ( 99.06)
Epoch: [12][130/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 5.1943e-01 (4.8157e-01)	Acc@1  83.59 ( 83.89)	Acc@5  97.66 ( 99.05)
Epoch: [12][140/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.5984e-01 (4.8517e-01)	Acc@1  80.47 ( 83.83)	Acc@5  98.44 ( 99.05)
Epoch: [12][150/391]	Time  0.030 ( 0.027)	Data  0.010 ( 0.003)	Loss 5.1520e-01 (4.8534e-01)	Acc@1  81.25 ( 83.86)	Acc@5 100.00 ( 99.06)
Epoch: [12][160/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.8837e-01 (4.8504e-01)	Acc@1  87.50 ( 83.88)	Acc@5  97.66 ( 99.04)
Epoch: [12][170/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.6358e-01 (4.8702e-01)	Acc@1  88.28 ( 83.85)	Acc@5 100.00 ( 99.05)
Epoch: [12][180/391]	Time  0.028 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.3215e-01 (4.8763e-01)	Acc@1  83.59 ( 83.81)	Acc@5  99.22 ( 99.06)
Epoch: [12][190/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.7630e-01 (4.8793e-01)	Acc@1  84.38 ( 83.87)	Acc@5  98.44 ( 99.05)
Epoch: [12][200/391]	Time  0.027 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.6227e-01 (4.8846e-01)	Acc@1  82.81 ( 83.86)	Acc@5  98.44 ( 99.06)
Epoch: [12][210/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.2131e-01 (4.8785e-01)	Acc@1  85.16 ( 83.84)	Acc@5  99.22 ( 99.07)
Epoch: [12][220/391]	Time  0.023 ( 0.026)	Data  0.003 ( 0.003)	Loss 4.7331e-01 (4.8643e-01)	Acc@1  85.16 ( 83.89)	Acc@5  99.22 ( 99.08)
Epoch: [12][230/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.6696e-01 (4.8564e-01)	Acc@1  79.69 ( 83.87)	Acc@5  99.22 ( 99.08)
Epoch: [12][240/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 5.1097e-01 (4.8648e-01)	Acc@1  83.59 ( 83.85)	Acc@5  98.44 ( 99.08)
Epoch: [12][250/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.3427e-01 (4.8524e-01)	Acc@1  88.28 ( 83.91)	Acc@5  99.22 ( 99.07)
Epoch: [12][260/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.0370e-01 (4.8421e-01)	Acc@1  86.72 ( 83.93)	Acc@5 100.00 ( 99.07)
Epoch: [12][270/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3603e-01 (4.8302e-01)	Acc@1  89.06 ( 83.99)	Acc@5  99.22 ( 99.07)
Epoch: [12][280/391]	Time  0.031 ( 0.026)	Data  0.003 ( 0.003)	Loss 6.5610e-01 (4.8265e-01)	Acc@1  82.81 ( 84.01)	Acc@5  99.22 ( 99.07)
Epoch: [12][290/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.4727e-01 (4.8484e-01)	Acc@1  85.16 ( 83.99)	Acc@5  98.44 ( 99.05)
Epoch: [12][300/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.9754e-01 (4.8571e-01)	Acc@1  84.38 ( 83.95)	Acc@5  99.22 ( 99.05)
Epoch: [12][310/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.5954e-01 (4.8679e-01)	Acc@1  82.81 ( 83.93)	Acc@5  99.22 ( 99.06)
Epoch: [12][320/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.1377e-01 (4.8675e-01)	Acc@1  82.03 ( 83.94)	Acc@5  98.44 ( 99.06)
Epoch: [12][330/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.2225e-01 (4.8658e-01)	Acc@1  81.25 ( 83.93)	Acc@5  97.66 ( 99.04)
Epoch: [12][340/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.0260e-01 (4.8721e-01)	Acc@1  82.81 ( 83.94)	Acc@5 100.00 ( 99.03)
Epoch: [12][350/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.6332e-01 (4.8754e-01)	Acc@1  81.25 ( 83.95)	Acc@5  99.22 ( 99.03)
Epoch: [12][360/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.6398e-01 (4.8810e-01)	Acc@1  83.59 ( 83.96)	Acc@5  99.22 ( 99.03)
Epoch: [12][370/391]	Time  0.023 ( 0.026)	Data  0.000 ( 0.003)	Loss 5.9282e-01 (4.8944e-01)	Acc@1  81.25 ( 83.93)	Acc@5  99.22 ( 99.01)
Epoch: [12][380/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.9399e-01 (4.8919e-01)	Acc@1  79.69 ( 83.96)	Acc@5 100.00 ( 99.01)
Epoch: [12][390/391]	Time  0.025 ( 0.026)	Data  0.000 ( 0.003)	Loss 5.3942e-01 (4.9034e-01)	Acc@1  82.50 ( 83.93)	Acc@5 100.00 ( 99.00)
## e[12] optimizer.zero_grad (sum) time: 0.10952019691467285
## e[12]       loss.backward (sum) time: 2.244946002960205
## e[12]      optimizer.step (sum) time: 0.8745982646942139
## epoch[12] training(only) time: 10.37245774269104
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 4.4325e-01 (4.4325e-01)	Acc@1  86.00 ( 86.00)	Acc@5  97.00 ( 97.00)
Test: [ 10/100]	Time  0.016 ( 0.030)	Loss 5.6160e-01 (4.8998e-01)	Acc@1  84.00 ( 83.91)	Acc@5  97.00 ( 98.73)
Test: [ 20/100]	Time  0.020 ( 0.022)	Loss 5.7568e-01 (5.1155e-01)	Acc@1  78.00 ( 82.67)	Acc@5  99.00 ( 98.71)
Test: [ 30/100]	Time  0.020 ( 0.020)	Loss 5.7793e-01 (5.1996e-01)	Acc@1  79.00 ( 82.58)	Acc@5  99.00 ( 98.77)
Test: [ 40/100]	Time  0.023 ( 0.018)	Loss 4.9896e-01 (5.2096e-01)	Acc@1  85.00 ( 82.54)	Acc@5 100.00 ( 98.85)
Test: [ 50/100]	Time  0.011 ( 0.017)	Loss 3.7261e-01 (5.1401e-01)	Acc@1  86.00 ( 82.71)	Acc@5 100.00 ( 98.88)
Test: [ 60/100]	Time  0.012 ( 0.017)	Loss 4.5657e-01 (5.1931e-01)	Acc@1  83.00 ( 82.59)	Acc@5 100.00 ( 98.97)
Test: [ 70/100]	Time  0.012 ( 0.017)	Loss 4.0072e-01 (5.1406e-01)	Acc@1  86.00 ( 82.77)	Acc@5 100.00 ( 99.00)
Test: [ 80/100]	Time  0.022 ( 0.017)	Loss 4.5796e-01 (5.1186e-01)	Acc@1  82.00 ( 82.84)	Acc@5 100.00 ( 99.02)
Test: [ 90/100]	Time  0.015 ( 0.017)	Loss 4.4817e-01 (5.1081e-01)	Acc@1  85.00 ( 82.79)	Acc@5  99.00 ( 99.09)
 * Acc@1 82.940 Acc@5 99.140
### epoch[12] execution time: 12.121012926101685
EPOCH 13
# current learning rate: 0.1
# Switched to train mode...
Epoch: [13][  0/391]	Time  0.184 ( 0.184)	Data  0.164 ( 0.164)	Loss 3.6883e-01 (3.6883e-01)	Acc@1  89.84 ( 89.84)	Acc@5  99.22 ( 99.22)
Epoch: [13][ 10/391]	Time  0.023 ( 0.040)	Data  0.001 ( 0.017)	Loss 5.7506e-01 (4.6827e-01)	Acc@1  82.81 ( 84.80)	Acc@5  98.44 ( 99.15)
Epoch: [13][ 20/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.010)	Loss 5.0037e-01 (4.8699e-01)	Acc@1  82.03 ( 83.89)	Acc@5  97.66 ( 98.96)
Epoch: [13][ 30/391]	Time  0.024 ( 0.031)	Data  0.001 ( 0.008)	Loss 4.7478e-01 (4.6419e-01)	Acc@1  85.16 ( 84.50)	Acc@5  98.44 ( 99.09)
Epoch: [13][ 40/391]	Time  0.024 ( 0.030)	Data  0.001 ( 0.006)	Loss 4.5704e-01 (4.5722e-01)	Acc@1  88.28 ( 84.98)	Acc@5  98.44 ( 99.10)
Epoch: [13][ 50/391]	Time  0.023 ( 0.029)	Data  0.001 ( 0.006)	Loss 4.3305e-01 (4.5728e-01)	Acc@1  86.72 ( 84.87)	Acc@5  99.22 ( 99.03)
Epoch: [13][ 60/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.005)	Loss 4.2681e-01 (4.6667e-01)	Acc@1  87.50 ( 84.57)	Acc@5 100.00 ( 99.05)
Epoch: [13][ 70/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.005)	Loss 5.0576e-01 (4.6753e-01)	Acc@1  83.59 ( 84.63)	Acc@5 100.00 ( 99.04)
Epoch: [13][ 80/391]	Time  0.030 ( 0.028)	Data  0.002 ( 0.004)	Loss 3.1075e-01 (4.6675e-01)	Acc@1  90.62 ( 84.70)	Acc@5  99.22 ( 99.07)
Epoch: [13][ 90/391]	Time  0.030 ( 0.027)	Data  0.000 ( 0.004)	Loss 3.7194e-01 (4.6174e-01)	Acc@1  85.94 ( 84.79)	Acc@5  99.22 ( 99.05)
Epoch: [13][100/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.3807e-01 (4.6016e-01)	Acc@1  89.06 ( 84.83)	Acc@5 100.00 ( 99.09)
Epoch: [13][110/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 4.1252e-01 (4.5543e-01)	Acc@1  86.72 ( 84.95)	Acc@5  98.44 ( 99.11)
Epoch: [13][120/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 5.3445e-01 (4.5563e-01)	Acc@1  84.38 ( 84.99)	Acc@5  99.22 ( 99.12)
Epoch: [13][130/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 4.5675e-01 (4.5674e-01)	Acc@1  84.38 ( 84.92)	Acc@5  99.22 ( 99.13)
Epoch: [13][140/391]	Time  0.033 ( 0.027)	Data  0.002 ( 0.003)	Loss 5.1929e-01 (4.5852e-01)	Acc@1  82.81 ( 84.84)	Acc@5  98.44 ( 99.12)
Epoch: [13][150/391]	Time  0.024 ( 0.027)	Data  0.002 ( 0.003)	Loss 4.4255e-01 (4.6012e-01)	Acc@1  82.81 ( 84.76)	Acc@5  99.22 ( 99.13)
Epoch: [13][160/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.6211e-01 (4.6135e-01)	Acc@1  85.16 ( 84.74)	Acc@5  97.66 ( 99.11)
Epoch: [13][170/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.3184e-01 (4.6161e-01)	Acc@1  81.25 ( 84.74)	Acc@5  98.44 ( 99.11)
Epoch: [13][180/391]	Time  0.034 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.0914e-01 (4.6006e-01)	Acc@1  88.28 ( 84.81)	Acc@5 100.00 ( 99.12)
Epoch: [13][190/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.6535e-01 (4.6229e-01)	Acc@1  88.28 ( 84.73)	Acc@5 100.00 ( 99.11)
Epoch: [13][200/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.4810e-01 (4.6238e-01)	Acc@1  85.16 ( 84.76)	Acc@5  98.44 ( 99.09)
Epoch: [13][210/391]	Time  0.041 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.3364e-01 (4.6155e-01)	Acc@1  84.38 ( 84.76)	Acc@5  98.44 ( 99.11)
Epoch: [13][220/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.7837e-01 (4.6261e-01)	Acc@1  83.59 ( 84.72)	Acc@5  99.22 ( 99.09)
Epoch: [13][230/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.7338e-01 (4.6491e-01)	Acc@1  80.47 ( 84.65)	Acc@5  97.66 ( 99.05)
Epoch: [13][240/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.3717e-01 (4.6413e-01)	Acc@1  85.16 ( 84.64)	Acc@5  97.66 ( 99.05)
Epoch: [13][250/391]	Time  0.032 ( 0.026)	Data  0.008 ( 0.003)	Loss 3.7890e-01 (4.6256e-01)	Acc@1  84.38 ( 84.69)	Acc@5 100.00 ( 99.08)
Epoch: [13][260/391]	Time  0.022 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.4645e-01 (4.6181e-01)	Acc@1  91.41 ( 84.71)	Acc@5 100.00 ( 99.08)
Epoch: [13][270/391]	Time  0.033 ( 0.026)	Data  0.000 ( 0.003)	Loss 4.3034e-01 (4.6038e-01)	Acc@1  86.72 ( 84.77)	Acc@5  99.22 ( 99.08)
Epoch: [13][280/391]	Time  0.036 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.7435e-01 (4.6051e-01)	Acc@1  80.47 ( 84.76)	Acc@5  98.44 ( 99.08)
Epoch: [13][290/391]	Time  0.034 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.1896e-01 (4.6078e-01)	Acc@1  82.81 ( 84.76)	Acc@5  99.22 ( 99.08)
Epoch: [13][300/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.0524e-01 (4.6115e-01)	Acc@1  85.16 ( 84.77)	Acc@5  98.44 ( 99.08)
Epoch: [13][310/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 5.5112e-01 (4.5988e-01)	Acc@1  84.38 ( 84.83)	Acc@5  99.22 ( 99.10)
Epoch: [13][320/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.7261e-01 (4.6153e-01)	Acc@1  78.91 ( 84.82)	Acc@5 100.00 ( 99.10)
Epoch: [13][330/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.9762e-01 (4.6176e-01)	Acc@1  85.16 ( 84.80)	Acc@5  99.22 ( 99.10)
Epoch: [13][340/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.5926e-01 (4.6145e-01)	Acc@1  85.16 ( 84.81)	Acc@5  97.66 ( 99.10)
Epoch: [13][350/391]	Time  0.031 ( 0.026)	Data  0.002 ( 0.003)	Loss 4.7383e-01 (4.6105e-01)	Acc@1  83.59 ( 84.81)	Acc@5  98.44 ( 99.09)
Epoch: [13][360/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.1787e-01 (4.6104e-01)	Acc@1  83.59 ( 84.81)	Acc@5  99.22 ( 99.10)
Epoch: [13][370/391]	Time  0.024 ( 0.026)	Data  0.005 ( 0.003)	Loss 4.6473e-01 (4.6188e-01)	Acc@1  85.16 ( 84.79)	Acc@5  96.88 ( 99.10)
Epoch: [13][380/391]	Time  0.023 ( 0.026)	Data  0.000 ( 0.003)	Loss 4.4842e-01 (4.6160e-01)	Acc@1  85.94 ( 84.83)	Acc@5 100.00 ( 99.09)
Epoch: [13][390/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3181e-01 (4.6173e-01)	Acc@1  90.00 ( 84.82)	Acc@5 100.00 ( 99.09)
## e[13] optimizer.zero_grad (sum) time: 0.1088411808013916
## e[13]       loss.backward (sum) time: 2.226212739944458
## e[13]      optimizer.step (sum) time: 0.8613924980163574
## epoch[13] training(only) time: 10.439088821411133
# Switched to evaluate mode...
Test: [  0/100]	Time  0.164 ( 0.164)	Loss 5.3019e-01 (5.3019e-01)	Acc@1  83.00 ( 83.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.013 ( 0.029)	Loss 3.6130e-01 (5.0388e-01)	Acc@1  87.00 ( 83.64)	Acc@5 100.00 ( 99.45)
Test: [ 20/100]	Time  0.015 ( 0.022)	Loss 5.0714e-01 (4.9783e-01)	Acc@1  82.00 ( 83.29)	Acc@5 100.00 ( 99.24)
Test: [ 30/100]	Time  0.027 ( 0.020)	Loss 4.7279e-01 (5.0105e-01)	Acc@1  86.00 ( 83.32)	Acc@5  99.00 ( 99.16)
Test: [ 40/100]	Time  0.011 ( 0.019)	Loss 6.4266e-01 (5.0092e-01)	Acc@1  80.00 ( 83.51)	Acc@5  98.00 ( 99.00)
Test: [ 50/100]	Time  0.011 ( 0.018)	Loss 4.3329e-01 (4.9719e-01)	Acc@1  88.00 ( 83.69)	Acc@5  98.00 ( 98.98)
Test: [ 60/100]	Time  0.011 ( 0.017)	Loss 5.1528e-01 (4.9868e-01)	Acc@1  85.00 ( 83.82)	Acc@5 100.00 ( 98.97)
Test: [ 70/100]	Time  0.016 ( 0.017)	Loss 5.7847e-01 (4.9538e-01)	Acc@1  82.00 ( 83.94)	Acc@5 100.00 ( 99.06)
Test: [ 80/100]	Time  0.011 ( 0.017)	Loss 4.0466e-01 (4.9555e-01)	Acc@1  87.00 ( 84.02)	Acc@5 100.00 ( 99.02)
Test: [ 90/100]	Time  0.014 ( 0.016)	Loss 3.5818e-01 (4.9431e-01)	Acc@1  91.00 ( 84.03)	Acc@5 100.00 ( 99.04)
 * Acc@1 84.100 Acc@5 99.100
### epoch[13] execution time: 12.16093897819519
EPOCH 14
# current learning rate: 0.1
# Switched to train mode...
Epoch: [14][  0/391]	Time  0.184 ( 0.184)	Data  0.159 ( 0.159)	Loss 4.4989e-01 (4.4989e-01)	Acc@1  85.16 ( 85.16)	Acc@5  99.22 ( 99.22)
Epoch: [14][ 10/391]	Time  0.027 ( 0.041)	Data  0.001 ( 0.016)	Loss 5.5758e-01 (4.2612e-01)	Acc@1  82.81 ( 86.15)	Acc@5  97.66 ( 99.29)
Epoch: [14][ 20/391]	Time  0.023 ( 0.033)	Data  0.001 ( 0.010)	Loss 3.3745e-01 (4.3669e-01)	Acc@1  89.84 ( 85.64)	Acc@5  99.22 ( 99.29)
Epoch: [14][ 30/391]	Time  0.023 ( 0.031)	Data  0.001 ( 0.007)	Loss 5.1794e-01 (4.4201e-01)	Acc@1  83.59 ( 85.56)	Acc@5 100.00 ( 99.29)
Epoch: [14][ 40/391]	Time  0.023 ( 0.030)	Data  0.001 ( 0.006)	Loss 3.8987e-01 (4.4027e-01)	Acc@1  87.50 ( 85.46)	Acc@5  99.22 ( 99.37)
Epoch: [14][ 50/391]	Time  0.023 ( 0.029)	Data  0.001 ( 0.005)	Loss 3.2078e-01 (4.3584e-01)	Acc@1  88.28 ( 85.49)	Acc@5 100.00 ( 99.37)
Epoch: [14][ 60/391]	Time  0.026 ( 0.028)	Data  0.004 ( 0.005)	Loss 4.2921e-01 (4.3671e-01)	Acc@1  86.72 ( 85.50)	Acc@5  98.44 ( 99.35)
Epoch: [14][ 70/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 4.7945e-01 (4.3791e-01)	Acc@1  82.81 ( 85.40)	Acc@5  99.22 ( 99.30)
Epoch: [14][ 80/391]	Time  0.023 ( 0.028)	Data  0.002 ( 0.004)	Loss 4.8407e-01 (4.3917e-01)	Acc@1  87.50 ( 85.39)	Acc@5  98.44 ( 99.30)
Epoch: [14][ 90/391]	Time  0.023 ( 0.027)	Data  0.000 ( 0.004)	Loss 3.1858e-01 (4.3760e-01)	Acc@1  88.28 ( 85.46)	Acc@5  99.22 ( 99.26)
Epoch: [14][100/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 6.6068e-01 (4.3941e-01)	Acc@1  77.34 ( 85.35)	Acc@5  99.22 ( 99.25)
Epoch: [14][110/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 4.0865e-01 (4.4026e-01)	Acc@1  88.28 ( 85.35)	Acc@5 100.00 ( 99.25)
Epoch: [14][120/391]	Time  0.032 ( 0.027)	Data  0.001 ( 0.004)	Loss 4.4249e-01 (4.4392e-01)	Acc@1  85.94 ( 85.25)	Acc@5 100.00 ( 99.22)
Epoch: [14][130/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 5.1225e-01 (4.4370e-01)	Acc@1  83.59 ( 85.25)	Acc@5  98.44 ( 99.21)
Epoch: [14][140/391]	Time  0.024 ( 0.027)	Data  0.002 ( 0.003)	Loss 4.0636e-01 (4.4337e-01)	Acc@1  86.72 ( 85.24)	Acc@5  99.22 ( 99.21)
Epoch: [14][150/391]	Time  0.029 ( 0.027)	Data  0.000 ( 0.003)	Loss 4.3736e-01 (4.4608e-01)	Acc@1  82.03 ( 85.19)	Acc@5 100.00 ( 99.20)
Epoch: [14][160/391]	Time  0.037 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.9516e-01 (4.4452e-01)	Acc@1  80.47 ( 85.25)	Acc@5  98.44 ( 99.20)
Epoch: [14][170/391]	Time  0.037 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.0976e-01 (4.4485e-01)	Acc@1  83.59 ( 85.20)	Acc@5  99.22 ( 99.21)
Epoch: [14][180/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.2919e-01 (4.4136e-01)	Acc@1  86.72 ( 85.35)	Acc@5 100.00 ( 99.21)
Epoch: [14][190/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.8561e-01 (4.4110e-01)	Acc@1  85.94 ( 85.43)	Acc@5  99.22 ( 99.22)
Epoch: [14][200/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.8650e-01 (4.4108e-01)	Acc@1  85.16 ( 85.44)	Acc@5  98.44 ( 99.21)
Epoch: [14][210/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.2321e-01 (4.4171e-01)	Acc@1  79.69 ( 85.39)	Acc@5  99.22 ( 99.23)
Epoch: [14][220/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.2042e-01 (4.4148e-01)	Acc@1  85.16 ( 85.39)	Acc@5  99.22 ( 99.24)
Epoch: [14][230/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 6.1609e-01 (4.4375e-01)	Acc@1  82.03 ( 85.31)	Acc@5  99.22 ( 99.23)
Epoch: [14][240/391]	Time  0.035 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.9137e-01 (4.4340e-01)	Acc@1  85.94 ( 85.33)	Acc@5  98.44 ( 99.23)
Epoch: [14][250/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.5656e-01 (4.4303e-01)	Acc@1  89.84 ( 85.30)	Acc@5 100.00 ( 99.23)
Epoch: [14][260/391]	Time  0.032 ( 0.026)	Data  0.000 ( 0.003)	Loss 3.4582e-01 (4.4283e-01)	Acc@1  88.28 ( 85.31)	Acc@5 100.00 ( 99.24)
Epoch: [14][270/391]	Time  0.031 ( 0.026)	Data  0.007 ( 0.003)	Loss 3.7035e-01 (4.4364e-01)	Acc@1  87.50 ( 85.27)	Acc@5  99.22 ( 99.21)
Epoch: [14][280/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.7545e-01 (4.4378e-01)	Acc@1  85.94 ( 85.27)	Acc@5  99.22 ( 99.22)
Epoch: [14][290/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.0716e-01 (4.4258e-01)	Acc@1  89.84 ( 85.29)	Acc@5 100.00 ( 99.23)
Epoch: [14][300/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.5811e-01 (4.4369e-01)	Acc@1  89.84 ( 85.27)	Acc@5  99.22 ( 99.23)
Epoch: [14][310/391]	Time  0.025 ( 0.026)	Data  0.003 ( 0.003)	Loss 3.8681e-01 (4.4416e-01)	Acc@1  91.41 ( 85.26)	Acc@5 100.00 ( 99.24)
Epoch: [14][320/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.1449e-01 (4.4452e-01)	Acc@1  87.50 ( 85.26)	Acc@5  99.22 ( 99.24)
Epoch: [14][330/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.2402e-01 (4.4296e-01)	Acc@1  75.78 ( 85.25)	Acc@5 100.00 ( 99.25)
Epoch: [14][340/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.7241e-01 (4.4304e-01)	Acc@1  82.81 ( 85.21)	Acc@5  99.22 ( 99.26)
Epoch: [14][350/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.0260e-01 (4.4298e-01)	Acc@1  85.94 ( 85.24)	Acc@5  98.44 ( 99.25)
Epoch: [14][360/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.5720e-01 (4.4317e-01)	Acc@1  88.28 ( 85.26)	Acc@5  99.22 ( 99.26)
Epoch: [14][370/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.0872e-01 (4.4457e-01)	Acc@1  87.50 ( 85.20)	Acc@5 100.00 ( 99.25)
Epoch: [14][380/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.3522e-01 (4.4520e-01)	Acc@1  82.81 ( 85.17)	Acc@5 100.00 ( 99.25)
Epoch: [14][390/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.4014e-01 (4.4548e-01)	Acc@1  78.75 ( 85.15)	Acc@5  98.75 ( 99.24)
## e[14] optimizer.zero_grad (sum) time: 0.10949182510375977
## e[14]       loss.backward (sum) time: 2.281644105911255
## e[14]      optimizer.step (sum) time: 0.8798713684082031
## epoch[14] training(only) time: 10.40274429321289
# Switched to evaluate mode...
Test: [  0/100]	Time  0.164 ( 0.164)	Loss 7.6703e-01 (7.6703e-01)	Acc@1  76.00 ( 76.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.011 ( 0.027)	Loss 7.5675e-01 (6.8105e-01)	Acc@1  75.00 ( 78.82)	Acc@5  99.00 ( 98.82)
Test: [ 20/100]	Time  0.012 ( 0.021)	Loss 8.0615e-01 (6.9350e-01)	Acc@1  77.00 ( 78.57)	Acc@5  99.00 ( 98.86)
Test: [ 30/100]	Time  0.010 ( 0.019)	Loss 7.2088e-01 (7.1147e-01)	Acc@1  72.00 ( 78.16)	Acc@5 100.00 ( 98.94)
Test: [ 40/100]	Time  0.012 ( 0.018)	Loss 8.1227e-01 (7.1527e-01)	Acc@1  74.00 ( 77.93)	Acc@5  98.00 ( 98.78)
Test: [ 50/100]	Time  0.011 ( 0.017)	Loss 5.8698e-01 (7.1224e-01)	Acc@1  77.00 ( 77.90)	Acc@5  98.00 ( 98.80)
Test: [ 60/100]	Time  0.010 ( 0.017)	Loss 5.1541e-01 (7.1394e-01)	Acc@1  84.00 ( 77.90)	Acc@5 100.00 ( 98.80)
Test: [ 70/100]	Time  0.014 ( 0.017)	Loss 7.8236e-01 (7.1325e-01)	Acc@1  76.00 ( 77.86)	Acc@5  98.00 ( 98.77)
Test: [ 80/100]	Time  0.012 ( 0.016)	Loss 6.4310e-01 (7.1470e-01)	Acc@1  81.00 ( 77.95)	Acc@5  99.00 ( 98.80)
Test: [ 90/100]	Time  0.011 ( 0.016)	Loss 6.7776e-01 (7.1713e-01)	Acc@1  79.00 ( 77.93)	Acc@5 100.00 ( 98.79)
 * Acc@1 78.100 Acc@5 98.790
### epoch[14] execution time: 12.091357707977295
EPOCH 15
# current learning rate: 0.1
# Switched to train mode...
Epoch: [15][  0/391]	Time  0.195 ( 0.195)	Data  0.174 ( 0.174)	Loss 5.1122e-01 (5.1122e-01)	Acc@1  82.81 ( 82.81)	Acc@5  99.22 ( 99.22)
Epoch: [15][ 10/391]	Time  0.030 ( 0.042)	Data  0.001 ( 0.018)	Loss 4.1374e-01 (4.1785e-01)	Acc@1  87.50 ( 86.58)	Acc@5  99.22 ( 99.36)
Epoch: [15][ 20/391]	Time  0.023 ( 0.034)	Data  0.001 ( 0.011)	Loss 4.3055e-01 (4.2679e-01)	Acc@1  85.94 ( 85.83)	Acc@5  99.22 ( 99.11)
Epoch: [15][ 30/391]	Time  0.032 ( 0.031)	Data  0.001 ( 0.008)	Loss 4.8825e-01 (4.1237e-01)	Acc@1  83.59 ( 86.59)	Acc@5  99.22 ( 99.07)
Epoch: [15][ 40/391]	Time  0.030 ( 0.030)	Data  0.000 ( 0.006)	Loss 4.3479e-01 (4.1155e-01)	Acc@1  85.94 ( 86.83)	Acc@5 100.00 ( 99.10)
Epoch: [15][ 50/391]	Time  0.023 ( 0.029)	Data  0.001 ( 0.006)	Loss 3.2037e-01 (4.0348e-01)	Acc@1  90.62 ( 86.99)	Acc@5  99.22 ( 99.19)
Epoch: [15][ 60/391]	Time  0.022 ( 0.028)	Data  0.001 ( 0.005)	Loss 4.2303e-01 (4.0781e-01)	Acc@1  83.59 ( 86.82)	Acc@5 100.00 ( 99.22)
Epoch: [15][ 70/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.005)	Loss 4.4633e-01 (4.1600e-01)	Acc@1  85.16 ( 86.59)	Acc@5  98.44 ( 99.23)
Epoch: [15][ 80/391]	Time  0.030 ( 0.028)	Data  0.001 ( 0.004)	Loss 4.7545e-01 (4.1787e-01)	Acc@1  86.72 ( 86.55)	Acc@5 100.00 ( 99.24)
Epoch: [15][ 90/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.004)	Loss 4.1818e-01 (4.1367e-01)	Acc@1  85.16 ( 86.63)	Acc@5  98.44 ( 99.24)
Epoch: [15][100/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.7425e-01 (4.0794e-01)	Acc@1  88.28 ( 86.73)	Acc@5  98.44 ( 99.28)
Epoch: [15][110/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.004)	Loss 5.5588e-01 (4.1347e-01)	Acc@1  79.69 ( 86.61)	Acc@5 100.00 ( 99.26)
Epoch: [15][120/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 5.1354e-01 (4.1737e-01)	Acc@1  82.81 ( 86.43)	Acc@5  99.22 ( 99.25)
Epoch: [15][130/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.9332e-01 (4.1975e-01)	Acc@1  84.38 ( 86.36)	Acc@5 100.00 ( 99.24)
Epoch: [15][140/391]	Time  0.026 ( 0.027)	Data  0.002 ( 0.003)	Loss 3.8233e-01 (4.1808e-01)	Acc@1  86.72 ( 86.38)	Acc@5  98.44 ( 99.23)
Epoch: [15][150/391]	Time  0.033 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.1767e-01 (4.1827e-01)	Acc@1  85.94 ( 86.30)	Acc@5  98.44 ( 99.25)
Epoch: [15][160/391]	Time  0.027 ( 0.027)	Data  0.005 ( 0.003)	Loss 3.6686e-01 (4.2000e-01)	Acc@1  85.94 ( 86.21)	Acc@5  99.22 ( 99.25)
Epoch: [15][170/391]	Time  0.033 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.2869e-01 (4.2264e-01)	Acc@1  82.81 ( 86.17)	Acc@5  99.22 ( 99.22)
Epoch: [15][180/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.3664e-01 (4.2384e-01)	Acc@1  85.94 ( 86.13)	Acc@5  99.22 ( 99.23)
Epoch: [15][190/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.5234e-01 (4.2643e-01)	Acc@1  84.38 ( 86.06)	Acc@5 100.00 ( 99.21)
Epoch: [15][200/391]	Time  0.027 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.7180e-01 (4.2799e-01)	Acc@1  87.50 ( 86.05)	Acc@5  99.22 ( 99.20)
Epoch: [15][210/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.8824e-01 (4.2817e-01)	Acc@1  91.41 ( 86.04)	Acc@5 100.00 ( 99.21)
Epoch: [15][220/391]	Time  0.026 ( 0.027)	Data  0.002 ( 0.003)	Loss 4.8868e-01 (4.2792e-01)	Acc@1  84.38 ( 86.05)	Acc@5 100.00 ( 99.22)
Epoch: [15][230/391]	Time  0.031 ( 0.027)	Data  0.000 ( 0.003)	Loss 5.7896e-01 (4.2760e-01)	Acc@1  82.03 ( 86.09)	Acc@5  96.88 ( 99.20)
Epoch: [15][240/391]	Time  0.032 ( 0.026)	Data  0.013 ( 0.003)	Loss 5.4674e-01 (4.2733e-01)	Acc@1  84.38 ( 86.14)	Acc@5  98.44 ( 99.19)
Epoch: [15][250/391]	Time  0.028 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.1392e-01 (4.2808e-01)	Acc@1  84.38 ( 86.10)	Acc@5  99.22 ( 99.19)
Epoch: [15][260/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.2312e-01 (4.2825e-01)	Acc@1  81.25 ( 86.11)	Acc@5  99.22 ( 99.18)
Epoch: [15][270/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.8671e-01 (4.2753e-01)	Acc@1  82.81 ( 86.11)	Acc@5  99.22 ( 99.19)
Epoch: [15][280/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.5790e-01 (4.2958e-01)	Acc@1  85.94 ( 86.03)	Acc@5 100.00 ( 99.18)
Epoch: [15][290/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.0019e-01 (4.3030e-01)	Acc@1  89.06 ( 85.98)	Acc@5  99.22 ( 99.18)
Epoch: [15][300/391]	Time  0.023 ( 0.026)	Data  0.000 ( 0.003)	Loss 4.3589e-01 (4.2980e-01)	Acc@1  85.16 ( 85.98)	Acc@5  99.22 ( 99.19)
Epoch: [15][310/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 4.9250e-01 (4.3146e-01)	Acc@1  89.06 ( 85.97)	Acc@5  98.44 ( 99.18)
Epoch: [15][320/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.1497e-01 (4.3360e-01)	Acc@1  82.81 ( 85.88)	Acc@5  98.44 ( 99.18)
Epoch: [15][330/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 4.7594e-01 (4.3377e-01)	Acc@1  83.59 ( 85.87)	Acc@5 100.00 ( 99.19)
Epoch: [15][340/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.7216e-01 (4.3226e-01)	Acc@1  86.72 ( 85.89)	Acc@5  98.44 ( 99.20)
Epoch: [15][350/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.1653e-01 (4.3136e-01)	Acc@1  85.94 ( 85.88)	Acc@5  99.22 ( 99.20)
Epoch: [15][360/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.8605e-01 (4.3190e-01)	Acc@1  80.47 ( 85.88)	Acc@5  97.66 ( 99.20)
Epoch: [15][370/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.9790e-01 (4.3207e-01)	Acc@1  83.59 ( 85.83)	Acc@5  99.22 ( 99.20)
Epoch: [15][380/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.2427e-01 (4.3273e-01)	Acc@1  81.25 ( 85.83)	Acc@5  98.44 ( 99.20)
Epoch: [15][390/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.0563e-01 (4.3212e-01)	Acc@1  81.25 ( 85.84)	Acc@5 100.00 ( 99.20)
## e[15] optimizer.zero_grad (sum) time: 0.10823631286621094
## e[15]       loss.backward (sum) time: 2.2023518085479736
## e[15]      optimizer.step (sum) time: 0.8735449314117432
## epoch[15] training(only) time: 10.393449068069458
# Switched to evaluate mode...
Test: [  0/100]	Time  0.165 ( 0.165)	Loss 5.0504e-01 (5.0504e-01)	Acc@1  87.00 ( 87.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.013 ( 0.029)	Loss 5.6415e-01 (5.5828e-01)	Acc@1  78.00 ( 81.45)	Acc@5 100.00 ( 98.82)
Test: [ 20/100]	Time  0.011 ( 0.021)	Loss 6.0329e-01 (5.7266e-01)	Acc@1  79.00 ( 81.95)	Acc@5 100.00 ( 99.00)
Test: [ 30/100]	Time  0.014 ( 0.020)	Loss 6.6045e-01 (5.7481e-01)	Acc@1  78.00 ( 81.94)	Acc@5  98.00 ( 99.03)
Test: [ 40/100]	Time  0.018 ( 0.018)	Loss 4.4418e-01 (5.8961e-01)	Acc@1  85.00 ( 81.78)	Acc@5  99.00 ( 99.05)
Test: [ 50/100]	Time  0.011 ( 0.017)	Loss 4.7186e-01 (5.8579e-01)	Acc@1  85.00 ( 82.00)	Acc@5  98.00 ( 98.96)
Test: [ 60/100]	Time  0.013 ( 0.017)	Loss 8.3276e-01 (5.9106e-01)	Acc@1  79.00 ( 82.00)	Acc@5 100.00 ( 98.97)
Test: [ 70/100]	Time  0.027 ( 0.017)	Loss 4.9925e-01 (5.8564e-01)	Acc@1  88.00 ( 82.08)	Acc@5 100.00 ( 98.99)
Test: [ 80/100]	Time  0.020 ( 0.016)	Loss 4.5217e-01 (5.8552e-01)	Acc@1  87.00 ( 82.12)	Acc@5  99.00 ( 98.99)
Test: [ 90/100]	Time  0.013 ( 0.016)	Loss 4.9004e-01 (5.8216e-01)	Acc@1  86.00 ( 82.19)	Acc@5 100.00 ( 99.05)
 * Acc@1 82.100 Acc@5 99.090
### epoch[15] execution time: 12.092679738998413
EPOCH 16
# current learning rate: 0.1
# Switched to train mode...
Epoch: [16][  0/391]	Time  0.198 ( 0.198)	Data  0.178 ( 0.178)	Loss 3.5649e-01 (3.5649e-01)	Acc@1  87.50 ( 87.50)	Acc@5  98.44 ( 98.44)
Epoch: [16][ 10/391]	Time  0.036 ( 0.041)	Data  0.004 ( 0.018)	Loss 3.1849e-01 (3.9440e-01)	Acc@1  89.06 ( 87.14)	Acc@5  99.22 ( 99.08)
Epoch: [16][ 20/391]	Time  0.024 ( 0.034)	Data  0.001 ( 0.011)	Loss 5.6074e-01 (4.3650e-01)	Acc@1  80.47 ( 85.45)	Acc@5  99.22 ( 99.18)
Epoch: [16][ 30/391]	Time  0.023 ( 0.031)	Data  0.001 ( 0.008)	Loss 2.6944e-01 (4.1457e-01)	Acc@1  89.84 ( 86.24)	Acc@5 100.00 ( 99.27)
Epoch: [16][ 40/391]	Time  0.023 ( 0.030)	Data  0.002 ( 0.007)	Loss 5.1178e-01 (4.2015e-01)	Acc@1  82.81 ( 86.05)	Acc@5  97.66 ( 99.29)
Epoch: [16][ 50/391]	Time  0.023 ( 0.029)	Data  0.001 ( 0.006)	Loss 3.2955e-01 (4.0748e-01)	Acc@1  92.19 ( 86.49)	Acc@5 100.00 ( 99.37)
Epoch: [16][ 60/391]	Time  0.023 ( 0.028)	Data  0.002 ( 0.005)	Loss 3.0779e-01 (4.0101e-01)	Acc@1  89.84 ( 86.74)	Acc@5 100.00 ( 99.33)
Epoch: [16][ 70/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.005)	Loss 3.2895e-01 (4.0097e-01)	Acc@1  87.50 ( 86.83)	Acc@5  99.22 ( 99.34)
Epoch: [16][ 80/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 5.6069e-01 (3.9829e-01)	Acc@1  79.69 ( 86.79)	Acc@5  98.44 ( 99.37)
Epoch: [16][ 90/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 5.5411e-01 (4.0503e-01)	Acc@1  82.81 ( 86.52)	Acc@5  98.44 ( 99.36)
Epoch: [16][100/391]	Time  0.023 ( 0.027)	Data  0.000 ( 0.004)	Loss 5.4564e-01 (4.0188e-01)	Acc@1  82.03 ( 86.66)	Acc@5  98.44 ( 99.36)
Epoch: [16][110/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.9639e-01 (4.0186e-01)	Acc@1  85.94 ( 86.63)	Acc@5  99.22 ( 99.32)
Epoch: [16][120/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.004)	Loss 5.0283e-01 (4.0419e-01)	Acc@1  82.03 ( 86.65)	Acc@5  96.88 ( 99.29)
Epoch: [16][130/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 4.5063e-01 (4.0423e-01)	Acc@1  86.72 ( 86.68)	Acc@5  99.22 ( 99.28)
Epoch: [16][140/391]	Time  0.027 ( 0.027)	Data  0.003 ( 0.003)	Loss 2.2086e-01 (4.0166e-01)	Acc@1  92.97 ( 86.72)	Acc@5 100.00 ( 99.28)
Epoch: [16][150/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.7202e-01 (4.0470e-01)	Acc@1  85.16 ( 86.64)	Acc@5  99.22 ( 99.26)
Epoch: [16][160/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.3468e-01 (4.0648e-01)	Acc@1  87.50 ( 86.50)	Acc@5  99.22 ( 99.26)
Epoch: [16][170/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.5941e-01 (4.0670e-01)	Acc@1  82.81 ( 86.46)	Acc@5  99.22 ( 99.25)
Epoch: [16][180/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.0104e-01 (4.0629e-01)	Acc@1  88.28 ( 86.45)	Acc@5 100.00 ( 99.26)
Epoch: [16][190/391]	Time  0.030 ( 0.027)	Data  0.005 ( 0.003)	Loss 3.4103e-01 (4.0515e-01)	Acc@1  90.62 ( 86.48)	Acc@5 100.00 ( 99.28)
Epoch: [16][200/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.1134e-01 (4.0528e-01)	Acc@1  85.16 ( 86.48)	Acc@5  97.66 ( 99.27)
Epoch: [16][210/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.4112e-01 (4.0616e-01)	Acc@1  78.91 ( 86.51)	Acc@5  97.66 ( 99.24)
Epoch: [16][220/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.0146e-01 (4.0901e-01)	Acc@1  83.59 ( 86.41)	Acc@5  98.44 ( 99.24)
Epoch: [16][230/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.0529e-01 (4.1098e-01)	Acc@1  86.72 ( 86.35)	Acc@5 100.00 ( 99.24)
Epoch: [16][240/391]	Time  0.027 ( 0.026)	Data  0.000 ( 0.003)	Loss 5.6990e-01 (4.1320e-01)	Acc@1  82.81 ( 86.30)	Acc@5  98.44 ( 99.22)
Epoch: [16][250/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.8485e-01 (4.1426e-01)	Acc@1  88.28 ( 86.30)	Acc@5 100.00 ( 99.23)
Epoch: [16][260/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.2254e-01 (4.1479e-01)	Acc@1  85.16 ( 86.28)	Acc@5 100.00 ( 99.22)
Epoch: [16][270/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.9084e-01 (4.1372e-01)	Acc@1  86.72 ( 86.31)	Acc@5  98.44 ( 99.23)
Epoch: [16][280/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.1735e-01 (4.1363e-01)	Acc@1  90.62 ( 86.30)	Acc@5  99.22 ( 99.23)
Epoch: [16][290/391]	Time  0.024 ( 0.026)	Data  0.002 ( 0.003)	Loss 4.0264e-01 (4.1424e-01)	Acc@1  88.28 ( 86.27)	Acc@5 100.00 ( 99.22)
Epoch: [16][300/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.0692e-01 (4.1253e-01)	Acc@1  89.06 ( 86.32)	Acc@5 100.00 ( 99.24)
Epoch: [16][310/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.9335e-01 (4.1176e-01)	Acc@1  82.03 ( 86.35)	Acc@5  98.44 ( 99.25)
Epoch: [16][320/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.7507e-01 (4.1334e-01)	Acc@1  84.38 ( 86.33)	Acc@5  99.22 ( 99.25)
Epoch: [16][330/391]	Time  0.023 ( 0.026)	Data  0.000 ( 0.003)	Loss 3.5997e-01 (4.1408e-01)	Acc@1  85.94 ( 86.29)	Acc@5 100.00 ( 99.25)
Epoch: [16][340/391]	Time  0.025 ( 0.026)	Data  0.003 ( 0.003)	Loss 4.9031e-01 (4.1441e-01)	Acc@1  89.06 ( 86.31)	Acc@5  99.22 ( 99.25)
Epoch: [16][350/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.1076e-01 (4.1480e-01)	Acc@1  87.50 ( 86.32)	Acc@5 100.00 ( 99.27)
Epoch: [16][360/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.1495e-01 (4.1458e-01)	Acc@1  85.94 ( 86.31)	Acc@5 100.00 ( 99.28)
Epoch: [16][370/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.7103e-01 (4.1451e-01)	Acc@1  85.16 ( 86.31)	Acc@5  98.44 ( 99.27)
Epoch: [16][380/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.7976e-01 (4.1369e-01)	Acc@1  83.59 ( 86.33)	Acc@5  99.22 ( 99.28)
Epoch: [16][390/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.8809e-01 (4.1384e-01)	Acc@1  77.50 ( 86.33)	Acc@5  98.75 ( 99.28)
## e[16] optimizer.zero_grad (sum) time: 0.10858845710754395
## e[16]       loss.backward (sum) time: 2.253760576248169
## e[16]      optimizer.step (sum) time: 0.8797144889831543
## epoch[16] training(only) time: 10.383659839630127
# Switched to evaluate mode...
Test: [  0/100]	Time  0.173 ( 0.173)	Loss 5.2720e-01 (5.2720e-01)	Acc@1  86.00 ( 86.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.011 ( 0.029)	Loss 5.4138e-01 (5.5501e-01)	Acc@1  83.00 ( 82.45)	Acc@5  99.00 ( 99.09)
Test: [ 20/100]	Time  0.016 ( 0.022)	Loss 5.7632e-01 (5.8452e-01)	Acc@1  80.00 ( 81.48)	Acc@5 100.00 ( 98.62)
Test: [ 30/100]	Time  0.014 ( 0.020)	Loss 6.7055e-01 (6.0223e-01)	Acc@1  76.00 ( 80.87)	Acc@5  99.00 ( 98.65)
Test: [ 40/100]	Time  0.011 ( 0.018)	Loss 5.2056e-01 (5.8446e-01)	Acc@1  85.00 ( 81.51)	Acc@5  99.00 ( 98.71)
Test: [ 50/100]	Time  0.016 ( 0.018)	Loss 4.4079e-01 (5.8769e-01)	Acc@1  86.00 ( 81.31)	Acc@5  99.00 ( 98.69)
Test: [ 60/100]	Time  0.020 ( 0.017)	Loss 5.4887e-01 (5.8225e-01)	Acc@1  84.00 ( 81.46)	Acc@5 100.00 ( 98.77)
Test: [ 70/100]	Time  0.011 ( 0.017)	Loss 4.9255e-01 (5.8043e-01)	Acc@1  81.00 ( 81.41)	Acc@5  99.00 ( 98.80)
Test: [ 80/100]	Time  0.018 ( 0.017)	Loss 6.0061e-01 (5.8183e-01)	Acc@1  81.00 ( 81.35)	Acc@5  98.00 ( 98.85)
Test: [ 90/100]	Time  0.019 ( 0.016)	Loss 4.5792e-01 (5.7892e-01)	Acc@1  86.00 ( 81.40)	Acc@5 100.00 ( 98.92)
 * Acc@1 81.330 Acc@5 98.950
### epoch[16] execution time: 12.099773406982422
EPOCH 17
# current learning rate: 0.1
# Switched to train mode...
Epoch: [17][  0/391]	Time  0.185 ( 0.185)	Data  0.163 ( 0.163)	Loss 2.5978e-01 (2.5978e-01)	Acc@1  92.97 ( 92.97)	Acc@5  99.22 ( 99.22)
Epoch: [17][ 10/391]	Time  0.032 ( 0.041)	Data  0.001 ( 0.017)	Loss 4.0740e-01 (3.8271e-01)	Acc@1  84.38 ( 86.86)	Acc@5 100.00 ( 99.57)
Epoch: [17][ 20/391]	Time  0.023 ( 0.034)	Data  0.001 ( 0.010)	Loss 4.1682e-01 (3.8859e-01)	Acc@1  86.72 ( 87.13)	Acc@5 100.00 ( 99.40)
Epoch: [17][ 30/391]	Time  0.023 ( 0.031)	Data  0.000 ( 0.007)	Loss 4.6546e-01 (3.9324e-01)	Acc@1  86.72 ( 87.05)	Acc@5  98.44 ( 99.32)
Epoch: [17][ 40/391]	Time  0.028 ( 0.030)	Data  0.000 ( 0.006)	Loss 4.6556e-01 (3.9351e-01)	Acc@1  83.59 ( 87.10)	Acc@5  99.22 ( 99.31)
Epoch: [17][ 50/391]	Time  0.023 ( 0.029)	Data  0.001 ( 0.005)	Loss 4.5585e-01 (3.9783e-01)	Acc@1  86.72 ( 86.89)	Acc@5 100.00 ( 99.42)
Epoch: [17][ 60/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.005)	Loss 3.9146e-01 (3.9030e-01)	Acc@1  87.50 ( 87.15)	Acc@5 100.00 ( 99.42)
Epoch: [17][ 70/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 4.2956e-01 (3.9618e-01)	Acc@1  84.38 ( 86.99)	Acc@5  99.22 ( 99.32)
Epoch: [17][ 80/391]	Time  0.030 ( 0.028)	Data  0.001 ( 0.004)	Loss 5.2283e-01 (3.9085e-01)	Acc@1  86.72 ( 87.24)	Acc@5  98.44 ( 99.32)
Epoch: [17][ 90/391]	Time  0.030 ( 0.028)	Data  0.001 ( 0.004)	Loss 3.7746e-01 (3.8848e-01)	Acc@1  84.38 ( 87.24)	Acc@5  99.22 ( 99.31)
Epoch: [17][100/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.4310e-01 (3.8323e-01)	Acc@1  93.75 ( 87.42)	Acc@5  99.22 ( 99.33)
Epoch: [17][110/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.3530e-01 (3.8877e-01)	Acc@1  89.84 ( 87.24)	Acc@5  99.22 ( 99.35)
Epoch: [17][120/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.6846e-01 (3.9069e-01)	Acc@1  81.25 ( 87.14)	Acc@5  99.22 ( 99.34)
Epoch: [17][130/391]	Time  0.040 ( 0.027)	Data  0.002 ( 0.003)	Loss 4.9770e-01 (3.9003e-01)	Acc@1  84.38 ( 87.14)	Acc@5  98.44 ( 99.34)
Epoch: [17][140/391]	Time  0.024 ( 0.027)	Data  0.002 ( 0.003)	Loss 3.5716e-01 (3.8866e-01)	Acc@1  90.62 ( 87.19)	Acc@5 100.00 ( 99.35)
Epoch: [17][150/391]	Time  0.029 ( 0.027)	Data  0.000 ( 0.003)	Loss 4.9188e-01 (3.8810e-01)	Acc@1  91.41 ( 87.23)	Acc@5  98.44 ( 99.36)
Epoch: [17][160/391]	Time  0.033 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.4795e-01 (3.9165e-01)	Acc@1  85.94 ( 87.10)	Acc@5  99.22 ( 99.34)
Epoch: [17][170/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.4903e-01 (3.9311e-01)	Acc@1  85.94 ( 87.02)	Acc@5 100.00 ( 99.36)
Epoch: [17][180/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.7589e-01 (3.9163e-01)	Acc@1  92.19 ( 87.08)	Acc@5 100.00 ( 99.37)
Epoch: [17][190/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.4417e-01 (3.9251e-01)	Acc@1  86.72 ( 87.05)	Acc@5 100.00 ( 99.36)
Epoch: [17][200/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.4226e-01 (3.9128e-01)	Acc@1  85.16 ( 87.11)	Acc@5  99.22 ( 99.35)
Epoch: [17][210/391]	Time  0.030 ( 0.027)	Data  0.003 ( 0.003)	Loss 4.1286e-01 (3.9265e-01)	Acc@1  87.50 ( 87.10)	Acc@5 100.00 ( 99.36)
Epoch: [17][220/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.5869e-01 (3.9408e-01)	Acc@1  88.28 ( 87.01)	Acc@5  98.44 ( 99.35)
Epoch: [17][230/391]	Time  0.030 ( 0.027)	Data  0.002 ( 0.003)	Loss 5.0420e-01 (3.9676e-01)	Acc@1  82.81 ( 86.92)	Acc@5  99.22 ( 99.32)
Epoch: [17][240/391]	Time  0.023 ( 0.026)	Data  0.000 ( 0.003)	Loss 3.4916e-01 (3.9660e-01)	Acc@1  85.94 ( 86.92)	Acc@5 100.00 ( 99.33)
Epoch: [17][250/391]	Time  0.024 ( 0.026)	Data  0.002 ( 0.003)	Loss 4.5655e-01 (3.9522e-01)	Acc@1  85.16 ( 86.95)	Acc@5 100.00 ( 99.34)
Epoch: [17][260/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.5241e-01 (3.9455e-01)	Acc@1  85.94 ( 86.98)	Acc@5 100.00 ( 99.34)
Epoch: [17][270/391]	Time  0.026 ( 0.026)	Data  0.004 ( 0.003)	Loss 4.2427e-01 (3.9576e-01)	Acc@1  85.94 ( 86.92)	Acc@5 100.00 ( 99.34)
Epoch: [17][280/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.4682e-01 (3.9618e-01)	Acc@1  89.84 ( 86.93)	Acc@5 100.00 ( 99.33)
Epoch: [17][290/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.8583e-01 (3.9692e-01)	Acc@1  82.81 ( 86.94)	Acc@5 100.00 ( 99.32)
Epoch: [17][300/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.7215e-01 (3.9622e-01)	Acc@1  85.94 ( 86.93)	Acc@5 100.00 ( 99.33)
Epoch: [17][310/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.8123e-01 (3.9790e-01)	Acc@1  90.62 ( 86.88)	Acc@5 100.00 ( 99.33)
Epoch: [17][320/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.4610e-01 (4.0002e-01)	Acc@1  88.28 ( 86.81)	Acc@5 100.00 ( 99.33)
Epoch: [17][330/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5016e-01 (4.0149e-01)	Acc@1  91.41 ( 86.80)	Acc@5 100.00 ( 99.32)
Epoch: [17][340/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.6615e-01 (4.0219e-01)	Acc@1  88.28 ( 86.79)	Acc@5 100.00 ( 99.32)
Epoch: [17][350/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.2569e-01 (4.0201e-01)	Acc@1  87.50 ( 86.79)	Acc@5 100.00 ( 99.32)
Epoch: [17][360/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.4685e-01 (4.0208e-01)	Acc@1  78.12 ( 86.81)	Acc@5  98.44 ( 99.31)
Epoch: [17][370/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.8831e-01 (4.0188e-01)	Acc@1  89.06 ( 86.82)	Acc@5  99.22 ( 99.31)
Epoch: [17][380/391]	Time  0.022 ( 0.026)	Data  0.002 ( 0.003)	Loss 4.2946e-01 (4.0264e-01)	Acc@1  85.94 ( 86.80)	Acc@5 100.00 ( 99.31)
Epoch: [17][390/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.7099e-01 (4.0250e-01)	Acc@1  92.50 ( 86.80)	Acc@5  98.75 ( 99.32)
## e[17] optimizer.zero_grad (sum) time: 0.10781741142272949
## e[17]       loss.backward (sum) time: 2.237725019454956
## e[17]      optimizer.step (sum) time: 0.8671562671661377
## epoch[17] training(only) time: 10.38575291633606
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 5.6024e-01 (5.6024e-01)	Acc@1  85.00 ( 85.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.012 ( 0.027)	Loss 5.5291e-01 (5.3751e-01)	Acc@1  88.00 ( 83.27)	Acc@5  98.00 ( 99.18)
Test: [ 20/100]	Time  0.011 ( 0.022)	Loss 5.3897e-01 (5.3814e-01)	Acc@1  80.00 ( 82.05)	Acc@5  99.00 ( 99.19)
Test: [ 30/100]	Time  0.010 ( 0.019)	Loss 5.3673e-01 (5.4360e-01)	Acc@1  81.00 ( 82.10)	Acc@5  98.00 ( 99.26)
Test: [ 40/100]	Time  0.023 ( 0.018)	Loss 6.4308e-01 (5.5043e-01)	Acc@1  82.00 ( 82.39)	Acc@5  97.00 ( 99.12)
Test: [ 50/100]	Time  0.010 ( 0.017)	Loss 4.8612e-01 (5.4826e-01)	Acc@1  84.00 ( 82.53)	Acc@5  99.00 ( 99.02)
Test: [ 60/100]	Time  0.011 ( 0.017)	Loss 4.1276e-01 (5.4508e-01)	Acc@1  87.00 ( 82.69)	Acc@5 100.00 ( 99.07)
Test: [ 70/100]	Time  0.013 ( 0.017)	Loss 4.9862e-01 (5.3963e-01)	Acc@1  80.00 ( 82.70)	Acc@5  99.00 ( 99.06)
Test: [ 80/100]	Time  0.016 ( 0.017)	Loss 5.2788e-01 (5.3354e-01)	Acc@1  85.00 ( 83.02)	Acc@5  99.00 ( 99.07)
Test: [ 90/100]	Time  0.011 ( 0.017)	Loss 2.7565e-01 (5.2994e-01)	Acc@1  94.00 ( 83.10)	Acc@5 100.00 ( 99.13)
 * Acc@1 83.110 Acc@5 99.140
### epoch[17] execution time: 12.173402070999146
EPOCH 18
# current learning rate: 0.1
# Switched to train mode...
Epoch: [18][  0/391]	Time  0.202 ( 0.202)	Data  0.175 ( 0.175)	Loss 4.1222e-01 (4.1222e-01)	Acc@1  86.72 ( 86.72)	Acc@5 100.00 (100.00)
Epoch: [18][ 10/391]	Time  0.026 ( 0.042)	Data  0.001 ( 0.018)	Loss 5.7180e-01 (4.1221e-01)	Acc@1  82.03 ( 87.36)	Acc@5 100.00 ( 99.29)
Epoch: [18][ 20/391]	Time  0.023 ( 0.034)	Data  0.001 ( 0.011)	Loss 3.1445e-01 (3.6888e-01)	Acc@1  88.28 ( 88.17)	Acc@5  99.22 ( 99.48)
Epoch: [18][ 30/391]	Time  0.023 ( 0.031)	Data  0.001 ( 0.008)	Loss 5.7019e-01 (3.5862e-01)	Acc@1  80.47 ( 88.16)	Acc@5  98.44 ( 99.45)
Epoch: [18][ 40/391]	Time  0.023 ( 0.030)	Data  0.001 ( 0.007)	Loss 3.3749e-01 (3.6857e-01)	Acc@1  89.84 ( 87.80)	Acc@5 100.00 ( 99.45)
Epoch: [18][ 50/391]	Time  0.023 ( 0.029)	Data  0.001 ( 0.006)	Loss 3.8940e-01 (3.7336e-01)	Acc@1  85.94 ( 87.70)	Acc@5  98.44 ( 99.40)
Epoch: [18][ 60/391]	Time  0.022 ( 0.028)	Data  0.001 ( 0.005)	Loss 3.9979e-01 (3.7586e-01)	Acc@1  86.72 ( 87.68)	Acc@5 100.00 ( 99.42)
Epoch: [18][ 70/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.005)	Loss 3.9778e-01 (3.7729e-01)	Acc@1  86.72 ( 87.63)	Acc@5  99.22 ( 99.39)
Epoch: [18][ 80/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 3.2376e-01 (3.7828e-01)	Acc@1  86.72 ( 87.57)	Acc@5 100.00 ( 99.36)
Epoch: [18][ 90/391]	Time  0.027 ( 0.028)	Data  0.001 ( 0.004)	Loss 3.7472e-01 (3.7985e-01)	Acc@1  89.84 ( 87.52)	Acc@5 100.00 ( 99.39)
Epoch: [18][100/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.0643e-01 (3.7692e-01)	Acc@1  92.19 ( 87.63)	Acc@5 100.00 ( 99.40)
Epoch: [18][110/391]	Time  0.032 ( 0.027)	Data  0.001 ( 0.004)	Loss 4.9897e-01 (3.7835e-01)	Acc@1  85.16 ( 87.62)	Acc@5 100.00 ( 99.36)
Epoch: [18][120/391]	Time  0.025 ( 0.027)	Data  0.005 ( 0.004)	Loss 4.6436e-01 (3.8119e-01)	Acc@1  85.94 ( 87.62)	Acc@5 100.00 ( 99.35)
Epoch: [18][130/391]	Time  0.030 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.4022e-01 (3.7910e-01)	Acc@1  89.06 ( 87.65)	Acc@5 100.00 ( 99.37)
Epoch: [18][140/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.4987e-01 (3.7895e-01)	Acc@1  89.06 ( 87.71)	Acc@5 100.00 ( 99.38)
Epoch: [18][150/391]	Time  0.025 ( 0.027)	Data  0.004 ( 0.003)	Loss 4.3799e-01 (3.8035e-01)	Acc@1  83.59 ( 87.66)	Acc@5 100.00 ( 99.37)
Epoch: [18][160/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.6255e-01 (3.8047e-01)	Acc@1  92.97 ( 87.66)	Acc@5  99.22 ( 99.35)
Epoch: [18][170/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.1821e-01 (3.7864e-01)	Acc@1  91.41 ( 87.72)	Acc@5  99.22 ( 99.36)
Epoch: [18][180/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.1279e-01 (3.7760e-01)	Acc@1  89.84 ( 87.79)	Acc@5  99.22 ( 99.37)
Epoch: [18][190/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.3659e-01 (3.7773e-01)	Acc@1  85.94 ( 87.79)	Acc@5  99.22 ( 99.36)
Epoch: [18][200/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.2191e-01 (3.7896e-01)	Acc@1  83.59 ( 87.71)	Acc@5  98.44 ( 99.36)
Epoch: [18][210/391]	Time  0.032 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.9601e-01 (3.8055e-01)	Acc@1  85.16 ( 87.57)	Acc@5  99.22 ( 99.37)
Epoch: [18][220/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.9468e-01 (3.8038e-01)	Acc@1  86.72 ( 87.59)	Acc@5 100.00 ( 99.36)
Epoch: [18][230/391]	Time  0.032 ( 0.027)	Data  0.000 ( 0.003)	Loss 3.4822e-01 (3.8073e-01)	Acc@1  85.94 ( 87.56)	Acc@5 100.00 ( 99.37)
Epoch: [18][240/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 4.3775e-01 (3.8049e-01)	Acc@1  86.72 ( 87.57)	Acc@5  98.44 ( 99.37)
Epoch: [18][250/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.1033e-01 (3.8061e-01)	Acc@1  80.47 ( 87.61)	Acc@5  99.22 ( 99.37)
Epoch: [18][260/391]	Time  0.022 ( 0.026)	Data  0.002 ( 0.003)	Loss 4.0099e-01 (3.8216e-01)	Acc@1  85.94 ( 87.55)	Acc@5  99.22 ( 99.37)
Epoch: [18][270/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.1894e-01 (3.8289e-01)	Acc@1  90.62 ( 87.54)	Acc@5  99.22 ( 99.37)
Epoch: [18][280/391]	Time  0.030 ( 0.026)	Data  0.003 ( 0.003)	Loss 3.2214e-01 (3.8255e-01)	Acc@1  89.84 ( 87.54)	Acc@5  99.22 ( 99.37)
Epoch: [18][290/391]	Time  0.024 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.5860e-01 (3.8256e-01)	Acc@1  87.50 ( 87.52)	Acc@5  99.22 ( 99.38)
Epoch: [18][300/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.5106e-01 (3.8253e-01)	Acc@1  86.72 ( 87.51)	Acc@5  99.22 ( 99.38)
Epoch: [18][310/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.3405e-01 (3.8344e-01)	Acc@1  76.56 ( 87.45)	Acc@5  99.22 ( 99.38)
Epoch: [18][320/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.7110e-01 (3.8265e-01)	Acc@1  87.50 ( 87.48)	Acc@5  99.22 ( 99.39)
Epoch: [18][330/391]	Time  0.026 ( 0.026)	Data  0.003 ( 0.003)	Loss 5.6265e-01 (3.8371e-01)	Acc@1  82.03 ( 87.45)	Acc@5  97.66 ( 99.38)
Epoch: [18][340/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.4930e-01 (3.8305e-01)	Acc@1  89.06 ( 87.45)	Acc@5  99.22 ( 99.38)
Epoch: [18][350/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.1810e-01 (3.8376e-01)	Acc@1  86.72 ( 87.42)	Acc@5  99.22 ( 99.39)
Epoch: [18][360/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.7247e-01 (3.8341e-01)	Acc@1  86.72 ( 87.39)	Acc@5 100.00 ( 99.39)
Epoch: [18][370/391]	Time  0.027 ( 0.026)	Data  0.003 ( 0.003)	Loss 2.2109e-01 (3.8268e-01)	Acc@1  91.41 ( 87.39)	Acc@5 100.00 ( 99.40)
Epoch: [18][380/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.2648e-01 (3.8382e-01)	Acc@1  85.94 ( 87.34)	Acc@5  99.22 ( 99.39)
Epoch: [18][390/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.8152e-01 (3.8445e-01)	Acc@1  83.75 ( 87.33)	Acc@5 100.00 ( 99.38)
## e[18] optimizer.zero_grad (sum) time: 0.10947132110595703
## e[18]       loss.backward (sum) time: 2.247849941253662
## e[18]      optimizer.step (sum) time: 0.8607792854309082
## epoch[18] training(only) time: 10.39339566230774
# Switched to evaluate mode...
Test: [  0/100]	Time  0.162 ( 0.162)	Loss 5.0757e-01 (5.0757e-01)	Acc@1  82.00 ( 82.00)	Acc@5  97.00 ( 97.00)
Test: [ 10/100]	Time  0.016 ( 0.028)	Loss 5.9069e-01 (5.5936e-01)	Acc@1  80.00 ( 81.00)	Acc@5  99.00 ( 99.18)
Test: [ 20/100]	Time  0.012 ( 0.021)	Loss 7.7045e-01 (5.6474e-01)	Acc@1  77.00 ( 81.24)	Acc@5 100.00 ( 99.00)
Test: [ 30/100]	Time  0.015 ( 0.019)	Loss 7.2970e-01 (6.0894e-01)	Acc@1  75.00 ( 80.58)	Acc@5  98.00 ( 98.81)
Test: [ 40/100]	Time  0.011 ( 0.018)	Loss 6.2264e-01 (6.1993e-01)	Acc@1  80.00 ( 80.54)	Acc@5  98.00 ( 98.71)
Test: [ 50/100]	Time  0.012 ( 0.017)	Loss 5.4738e-01 (6.1829e-01)	Acc@1  85.00 ( 80.31)	Acc@5  99.00 ( 98.73)
Test: [ 60/100]	Time  0.011 ( 0.017)	Loss 7.3101e-01 (6.1950e-01)	Acc@1  77.00 ( 80.39)	Acc@5 100.00 ( 98.75)
Test: [ 70/100]	Time  0.020 ( 0.017)	Loss 6.5431e-01 (6.1845e-01)	Acc@1  81.00 ( 80.56)	Acc@5  99.00 ( 98.72)
Test: [ 80/100]	Time  0.011 ( 0.016)	Loss 4.6388e-01 (6.1753e-01)	Acc@1  86.00 ( 80.72)	Acc@5  99.00 ( 98.68)
Test: [ 90/100]	Time  0.011 ( 0.016)	Loss 5.4774e-01 (6.1969e-01)	Acc@1  84.00 ( 80.74)	Acc@5 100.00 ( 98.66)
 * Acc@1 80.780 Acc@5 98.670
### epoch[18] execution time: 12.101735353469849
EPOCH 19
# current learning rate: 0.1
# Switched to train mode...
Epoch: [19][  0/391]	Time  0.197 ( 0.197)	Data  0.176 ( 0.176)	Loss 4.7278e-01 (4.7278e-01)	Acc@1  84.38 ( 84.38)	Acc@5  98.44 ( 98.44)
Epoch: [19][ 10/391]	Time  0.032 ( 0.042)	Data  0.001 ( 0.018)	Loss 3.2331e-01 (3.6388e-01)	Acc@1  90.62 ( 88.71)	Acc@5 100.00 ( 99.50)
Epoch: [19][ 20/391]	Time  0.024 ( 0.035)	Data  0.002 ( 0.011)	Loss 3.6958e-01 (3.7453e-01)	Acc@1  87.50 ( 87.80)	Acc@5 100.00 ( 99.59)
Epoch: [19][ 30/391]	Time  0.023 ( 0.031)	Data  0.001 ( 0.008)	Loss 3.8675e-01 (3.7077e-01)	Acc@1  86.72 ( 87.53)	Acc@5  99.22 ( 99.47)
Epoch: [19][ 40/391]	Time  0.029 ( 0.030)	Data  0.006 ( 0.007)	Loss 4.4700e-01 (3.7021e-01)	Acc@1  82.81 ( 87.16)	Acc@5 100.00 ( 99.54)
Epoch: [19][ 50/391]	Time  0.023 ( 0.029)	Data  0.001 ( 0.006)	Loss 2.2607e-01 (3.6974e-01)	Acc@1  92.19 ( 87.45)	Acc@5 100.00 ( 99.54)
Epoch: [19][ 60/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.005)	Loss 4.1055e-01 (3.7399e-01)	Acc@1  85.16 ( 87.35)	Acc@5  99.22 ( 99.53)
Epoch: [19][ 70/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.005)	Loss 5.0719e-01 (3.7988e-01)	Acc@1  82.81 ( 87.14)	Acc@5  98.44 ( 99.48)
Epoch: [19][ 80/391]	Time  0.026 ( 0.028)	Data  0.001 ( 0.004)	Loss 2.6876e-01 (3.7361e-01)	Acc@1  92.97 ( 87.43)	Acc@5  98.44 ( 99.50)
Epoch: [19][ 90/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.004)	Loss 4.2652e-01 (3.6931e-01)	Acc@1  86.72 ( 87.62)	Acc@5 100.00 ( 99.50)
Epoch: [19][100/391]	Time  0.033 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.1915e-01 (3.6819e-01)	Acc@1  92.19 ( 87.65)	Acc@5 100.00 ( 99.53)
Epoch: [19][110/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 4.3191e-01 (3.7102e-01)	Acc@1  83.59 ( 87.52)	Acc@5  99.22 ( 99.50)
Epoch: [19][120/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.004)	Loss 4.4283e-01 (3.7280e-01)	Acc@1  82.03 ( 87.48)	Acc@5  99.22 ( 99.50)
Epoch: [19][130/391]	Time  0.026 ( 0.027)	Data  0.001 ( 0.004)	Loss 4.3466e-01 (3.7519e-01)	Acc@1  85.16 ( 87.43)	Acc@5  99.22 ( 99.49)
Epoch: [19][140/391]	Time  0.026 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.8911e-01 (3.7718e-01)	Acc@1  92.19 ( 87.37)	Acc@5  99.22 ( 99.48)
Epoch: [19][150/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.5819e-01 (3.7699e-01)	Acc@1  91.41 ( 87.40)	Acc@5  99.22 ( 99.44)
Epoch: [19][160/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.0311e-01 (3.7473e-01)	Acc@1  85.16 ( 87.50)	Acc@5  99.22 ( 99.44)
Epoch: [19][170/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.7140e-01 (3.7470e-01)	Acc@1  90.62 ( 87.50)	Acc@5  99.22 ( 99.44)
Epoch: [19][180/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.5503e-01 (3.7412e-01)	Acc@1  85.94 ( 87.51)	Acc@5  99.22 ( 99.43)
Epoch: [19][190/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.7608e-01 (3.7471e-01)	Acc@1  83.59 ( 87.45)	Acc@5 100.00 ( 99.43)
Epoch: [19][200/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.4000e-01 (3.7414e-01)	Acc@1  86.72 ( 87.47)	Acc@5 100.00 ( 99.44)
Epoch: [19][210/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.0335e-01 (3.7471e-01)	Acc@1  89.06 ( 87.46)	Acc@5  98.44 ( 99.45)
Epoch: [19][220/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.4232e-01 (3.7445e-01)	Acc@1  88.28 ( 87.49)	Acc@5  99.22 ( 99.44)
Epoch: [19][230/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.3287e-01 (3.7368e-01)	Acc@1  91.41 ( 87.53)	Acc@5  99.22 ( 99.44)
Epoch: [19][240/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.0533e-01 (3.7468e-01)	Acc@1  88.28 ( 87.48)	Acc@5 100.00 ( 99.44)
Epoch: [19][250/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.8245e-01 (3.7638e-01)	Acc@1  87.50 ( 87.47)	Acc@5 100.00 ( 99.43)
Epoch: [19][260/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.2018e-01 (3.7825e-01)	Acc@1  88.28 ( 87.42)	Acc@5  99.22 ( 99.43)
Epoch: [19][270/391]	Time  0.023 ( 0.026)	Data  0.003 ( 0.003)	Loss 2.9243e-01 (3.8000e-01)	Acc@1  91.41 ( 87.39)	Acc@5 100.00 ( 99.41)
Epoch: [19][280/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3363e-01 (3.8134e-01)	Acc@1  88.28 ( 87.37)	Acc@5 100.00 ( 99.40)
Epoch: [19][290/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.9367e-01 (3.8218e-01)	Acc@1  87.50 ( 87.37)	Acc@5  99.22 ( 99.40)
Epoch: [19][300/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.2814e-01 (3.8004e-01)	Acc@1  88.28 ( 87.44)	Acc@5 100.00 ( 99.41)
Epoch: [19][310/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.4092e-01 (3.8005e-01)	Acc@1  91.41 ( 87.44)	Acc@5  99.22 ( 99.40)
Epoch: [19][320/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.9974e-01 (3.7918e-01)	Acc@1  86.72 ( 87.47)	Acc@5  98.44 ( 99.40)
Epoch: [19][330/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.0282e-01 (3.7899e-01)	Acc@1  91.41 ( 87.49)	Acc@5  98.44 ( 99.39)
Epoch: [19][340/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.3401e-01 (3.8040e-01)	Acc@1  85.16 ( 87.47)	Acc@5 100.00 ( 99.38)
Epoch: [19][350/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.8203e-01 (3.8017e-01)	Acc@1  85.94 ( 87.49)	Acc@5 100.00 ( 99.39)
Epoch: [19][360/391]	Time  0.022 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.5252e-01 (3.7892e-01)	Acc@1  90.62 ( 87.56)	Acc@5 100.00 ( 99.40)
Epoch: [19][370/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.5459e-01 (3.7891e-01)	Acc@1  84.38 ( 87.58)	Acc@5  99.22 ( 99.40)
Epoch: [19][380/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.7636e-01 (3.7987e-01)	Acc@1  87.50 ( 87.53)	Acc@5  99.22 ( 99.39)
Epoch: [19][390/391]	Time  0.024 ( 0.026)	Data  0.000 ( 0.003)	Loss 4.4735e-01 (3.7943e-01)	Acc@1  86.25 ( 87.56)	Acc@5 100.00 ( 99.39)
## e[19] optimizer.zero_grad (sum) time: 0.11028170585632324
## e[19]       loss.backward (sum) time: 2.2524726390838623
## e[19]      optimizer.step (sum) time: 0.8723609447479248
## epoch[19] training(only) time: 10.37808609008789
# Switched to evaluate mode...
Test: [  0/100]	Time  0.165 ( 0.165)	Loss 4.4035e-01 (4.4035e-01)	Acc@1  84.00 ( 84.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.011 ( 0.028)	Loss 3.7144e-01 (4.4615e-01)	Acc@1  88.00 ( 84.64)	Acc@5 100.00 ( 99.27)
Test: [ 20/100]	Time  0.013 ( 0.022)	Loss 4.3869e-01 (4.5867e-01)	Acc@1  83.00 ( 84.29)	Acc@5 100.00 ( 99.10)
Test: [ 30/100]	Time  0.011 ( 0.019)	Loss 5.1154e-01 (4.7260e-01)	Acc@1  81.00 ( 83.84)	Acc@5  99.00 ( 99.10)
Test: [ 40/100]	Time  0.011 ( 0.018)	Loss 4.5627e-01 (4.8064e-01)	Acc@1  88.00 ( 83.63)	Acc@5  97.00 ( 99.07)
Test: [ 50/100]	Time  0.011 ( 0.018)	Loss 3.7223e-01 (4.8549e-01)	Acc@1  84.00 ( 83.63)	Acc@5  99.00 ( 99.08)
Test: [ 60/100]	Time  0.026 ( 0.017)	Loss 4.7178e-01 (4.8671e-01)	Acc@1  86.00 ( 83.59)	Acc@5 100.00 ( 99.10)
Test: [ 70/100]	Time  0.015 ( 0.017)	Loss 4.9141e-01 (4.8011e-01)	Acc@1  87.00 ( 83.92)	Acc@5  99.00 ( 99.10)
Test: [ 80/100]	Time  0.011 ( 0.016)	Loss 5.0341e-01 (4.8352e-01)	Acc@1  84.00 ( 83.81)	Acc@5  99.00 ( 99.14)
Test: [ 90/100]	Time  0.015 ( 0.016)	Loss 3.6832e-01 (4.8172e-01)	Acc@1  90.00 ( 83.92)	Acc@5 100.00 ( 99.12)
 * Acc@1 83.990 Acc@5 99.150
### epoch[19] execution time: 12.135244369506836
EPOCH 20
# current learning rate: 0.1
# Switched to train mode...
Epoch: [20][  0/391]	Time  0.186 ( 0.186)	Data  0.165 ( 0.165)	Loss 3.1704e-01 (3.1704e-01)	Acc@1  88.28 ( 88.28)	Acc@5  99.22 ( 99.22)
Epoch: [20][ 10/391]	Time  0.027 ( 0.040)	Data  0.003 ( 0.017)	Loss 1.5889e-01 (3.3241e-01)	Acc@1  94.53 ( 88.64)	Acc@5 100.00 ( 99.79)
Epoch: [20][ 20/391]	Time  0.023 ( 0.033)	Data  0.001 ( 0.010)	Loss 4.3116e-01 (3.4681e-01)	Acc@1  87.50 ( 89.06)	Acc@5 100.00 ( 99.59)
Epoch: [20][ 30/391]	Time  0.023 ( 0.031)	Data  0.002 ( 0.007)	Loss 3.6212e-01 (3.5577e-01)	Acc@1  86.72 ( 88.56)	Acc@5 100.00 ( 99.47)
Epoch: [20][ 40/391]	Time  0.023 ( 0.029)	Data  0.000 ( 0.006)	Loss 3.7276e-01 (3.6555e-01)	Acc@1  86.72 ( 88.11)	Acc@5  99.22 ( 99.39)
Epoch: [20][ 50/391]	Time  0.025 ( 0.029)	Data  0.001 ( 0.005)	Loss 3.1319e-01 (3.6566e-01)	Acc@1  89.84 ( 87.91)	Acc@5 100.00 ( 99.36)
Epoch: [20][ 60/391]	Time  0.028 ( 0.028)	Data  0.001 ( 0.005)	Loss 3.5229e-01 (3.5552e-01)	Acc@1  90.62 ( 88.51)	Acc@5  99.22 ( 99.37)
Epoch: [20][ 70/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 4.4552e-01 (3.6237e-01)	Acc@1  83.59 ( 88.24)	Acc@5 100.00 ( 99.35)
Epoch: [20][ 80/391]	Time  0.023 ( 0.028)	Data  0.003 ( 0.004)	Loss 2.9997e-01 (3.6235e-01)	Acc@1  90.62 ( 88.17)	Acc@5 100.00 ( 99.33)
Epoch: [20][ 90/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.004)	Loss 3.4472e-01 (3.6034e-01)	Acc@1  89.84 ( 88.18)	Acc@5 100.00 ( 99.33)
Epoch: [20][100/391]	Time  0.024 ( 0.027)	Data  0.002 ( 0.004)	Loss 3.7532e-01 (3.5899e-01)	Acc@1  85.94 ( 88.15)	Acc@5  99.22 ( 99.36)
Epoch: [20][110/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.8805e-01 (3.6169e-01)	Acc@1  90.62 ( 88.04)	Acc@5 100.00 ( 99.38)
Epoch: [20][120/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.9954e-01 (3.6229e-01)	Acc@1  89.84 ( 87.98)	Acc@5 100.00 ( 99.39)
Epoch: [20][130/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.3705e-01 (3.6267e-01)	Acc@1  84.38 ( 87.98)	Acc@5 100.00 ( 99.41)
Epoch: [20][140/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.9104e-01 (3.6382e-01)	Acc@1  87.50 ( 87.95)	Acc@5  98.44 ( 99.41)
Epoch: [20][150/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 3.4260e-01 (3.6369e-01)	Acc@1  89.06 ( 87.94)	Acc@5  98.44 ( 99.40)
Epoch: [20][160/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.7152e-01 (3.6246e-01)	Acc@1  90.62 ( 88.04)	Acc@5 100.00 ( 99.41)
Epoch: [20][170/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.3186e-01 (3.6197e-01)	Acc@1  88.28 ( 88.06)	Acc@5  99.22 ( 99.41)
Epoch: [20][180/391]	Time  0.032 ( 0.027)	Data  0.007 ( 0.003)	Loss 5.7655e-01 (3.6252e-01)	Acc@1  82.81 ( 88.06)	Acc@5  96.09 ( 99.40)
Epoch: [20][190/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.4233e-01 (3.6183e-01)	Acc@1  85.94 ( 88.04)	Acc@5 100.00 ( 99.42)
Epoch: [20][200/391]	Time  0.027 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.3544e-01 (3.6070e-01)	Acc@1  86.72 ( 88.05)	Acc@5 100.00 ( 99.43)
Epoch: [20][210/391]	Time  0.032 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.1854e-01 (3.6080e-01)	Acc@1  86.72 ( 88.03)	Acc@5 100.00 ( 99.44)
Epoch: [20][220/391]	Time  0.028 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.5105e-01 (3.5989e-01)	Acc@1  87.50 ( 88.08)	Acc@5 100.00 ( 99.44)
Epoch: [20][230/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.2099e-01 (3.5896e-01)	Acc@1  85.94 ( 88.13)	Acc@5  99.22 ( 99.44)
Epoch: [20][240/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.2898e-01 (3.5957e-01)	Acc@1  91.41 ( 88.09)	Acc@5  99.22 ( 99.43)
Epoch: [20][250/391]	Time  0.025 ( 0.026)	Data  0.000 ( 0.003)	Loss 3.2499e-01 (3.5980e-01)	Acc@1  89.84 ( 88.10)	Acc@5 100.00 ( 99.44)
Epoch: [20][260/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.7784e-01 (3.5832e-01)	Acc@1  89.84 ( 88.15)	Acc@5  99.22 ( 99.43)
Epoch: [20][270/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.7798e-01 (3.5990e-01)	Acc@1  89.06 ( 88.11)	Acc@5 100.00 ( 99.44)
Epoch: [20][280/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.1515e-01 (3.6002e-01)	Acc@1  86.72 ( 88.10)	Acc@5  99.22 ( 99.46)
Epoch: [20][290/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.2521e-01 (3.6186e-01)	Acc@1  88.28 ( 88.06)	Acc@5  99.22 ( 99.45)
Epoch: [20][300/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.7817e-01 (3.6334e-01)	Acc@1  86.72 ( 88.01)	Acc@5 100.00 ( 99.44)
Epoch: [20][310/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.9454e-01 (3.6342e-01)	Acc@1  90.62 ( 88.01)	Acc@5  98.44 ( 99.43)
Epoch: [20][320/391]	Time  0.026 ( 0.026)	Data  0.003 ( 0.003)	Loss 3.7825e-01 (3.6385e-01)	Acc@1  86.72 ( 88.00)	Acc@5  99.22 ( 99.43)
Epoch: [20][330/391]	Time  0.023 ( 0.026)	Data  0.003 ( 0.003)	Loss 2.3324e-01 (3.6322e-01)	Acc@1  94.53 ( 88.03)	Acc@5 100.00 ( 99.43)
Epoch: [20][340/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.1106e-01 (3.6321e-01)	Acc@1  84.38 ( 88.03)	Acc@5  98.44 ( 99.43)
Epoch: [20][350/391]	Time  0.029 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.3002e-01 (3.6238e-01)	Acc@1  92.97 ( 88.03)	Acc@5  99.22 ( 99.44)
Epoch: [20][360/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.9460e-01 (3.6229e-01)	Acc@1  89.84 ( 88.03)	Acc@5  99.22 ( 99.44)
Epoch: [20][370/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.3626e-01 (3.6283e-01)	Acc@1  87.50 ( 88.00)	Acc@5  99.22 ( 99.43)
Epoch: [20][380/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.0471e-01 (3.6184e-01)	Acc@1  90.62 ( 88.02)	Acc@5 100.00 ( 99.44)
Epoch: [20][390/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.2966e-01 (3.6139e-01)	Acc@1  87.50 ( 88.01)	Acc@5  98.75 ( 99.44)
## e[20] optimizer.zero_grad (sum) time: 0.10815310478210449
## e[20]       loss.backward (sum) time: 2.2231979370117188
## e[20]      optimizer.step (sum) time: 0.8482909202575684
## epoch[20] training(only) time: 10.423107862472534
# Switched to evaluate mode...
Test: [  0/100]	Time  0.172 ( 0.172)	Loss 5.9722e-01 (5.9722e-01)	Acc@1  81.00 ( 81.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.021 ( 0.030)	Loss 6.1754e-01 (5.9013e-01)	Acc@1  77.00 ( 81.55)	Acc@5  99.00 ( 98.73)
Test: [ 20/100]	Time  0.015 ( 0.022)	Loss 6.6432e-01 (6.2537e-01)	Acc@1  82.00 ( 81.19)	Acc@5 100.00 ( 98.62)
Test: [ 30/100]	Time  0.011 ( 0.019)	Loss 7.7948e-01 (6.4397e-01)	Acc@1  76.00 ( 80.84)	Acc@5  98.00 ( 98.84)
Test: [ 40/100]	Time  0.011 ( 0.018)	Loss 7.4261e-01 (6.4479e-01)	Acc@1  78.00 ( 81.10)	Acc@5  98.00 ( 98.83)
Test: [ 50/100]	Time  0.013 ( 0.017)	Loss 4.9264e-01 (6.3865e-01)	Acc@1  86.00 ( 81.27)	Acc@5  97.00 ( 98.73)
Test: [ 60/100]	Time  0.020 ( 0.017)	Loss 6.1974e-01 (6.3224e-01)	Acc@1  81.00 ( 81.36)	Acc@5  97.00 ( 98.70)
Test: [ 70/100]	Time  0.012 ( 0.017)	Loss 6.5153e-01 (6.2186e-01)	Acc@1  83.00 ( 81.49)	Acc@5 100.00 ( 98.80)
Test: [ 80/100]	Time  0.012 ( 0.016)	Loss 6.7496e-01 (6.2463e-01)	Acc@1  83.00 ( 81.41)	Acc@5  99.00 ( 98.85)
Test: [ 90/100]	Time  0.022 ( 0.016)	Loss 6.7938e-01 (6.2329e-01)	Acc@1  80.00 ( 81.47)	Acc@5  98.00 ( 98.82)
 * Acc@1 81.640 Acc@5 98.870
### epoch[20] execution time: 12.133162021636963
EPOCH 21
# current learning rate: 0.1
# Switched to train mode...
Epoch: [21][  0/391]	Time  0.186 ( 0.186)	Data  0.164 ( 0.164)	Loss 2.5714e-01 (2.5714e-01)	Acc@1  94.53 ( 94.53)	Acc@5  99.22 ( 99.22)
Epoch: [21][ 10/391]	Time  0.035 ( 0.041)	Data  0.001 ( 0.016)	Loss 3.3396e-01 (3.5881e-01)	Acc@1  89.06 ( 88.14)	Acc@5  99.22 ( 99.43)
Epoch: [21][ 20/391]	Time  0.024 ( 0.034)	Data  0.001 ( 0.010)	Loss 3.3636e-01 (3.5688e-01)	Acc@1  88.28 ( 88.06)	Acc@5 100.00 ( 99.55)
Epoch: [21][ 30/391]	Time  0.024 ( 0.031)	Data  0.001 ( 0.007)	Loss 2.7233e-01 (3.5508e-01)	Acc@1  92.97 ( 88.26)	Acc@5  99.22 ( 99.60)
Epoch: [21][ 40/391]	Time  0.035 ( 0.030)	Data  0.001 ( 0.006)	Loss 3.9562e-01 (3.6239e-01)	Acc@1  90.62 ( 88.13)	Acc@5 100.00 ( 99.64)
Epoch: [21][ 50/391]	Time  0.030 ( 0.029)	Data  0.005 ( 0.005)	Loss 2.7608e-01 (3.5404e-01)	Acc@1  92.19 ( 88.48)	Acc@5  99.22 ( 99.56)
Epoch: [21][ 60/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.005)	Loss 3.0257e-01 (3.4798e-01)	Acc@1  89.06 ( 88.81)	Acc@5 100.00 ( 99.56)
Epoch: [21][ 70/391]	Time  0.028 ( 0.028)	Data  0.005 ( 0.004)	Loss 3.6386e-01 (3.4919e-01)	Acc@1  85.16 ( 88.67)	Acc@5 100.00 ( 99.57)
Epoch: [21][ 80/391]	Time  0.028 ( 0.028)	Data  0.003 ( 0.004)	Loss 4.3334e-01 (3.5188e-01)	Acc@1  85.94 ( 88.59)	Acc@5 100.00 ( 99.57)
Epoch: [21][ 90/391]	Time  0.028 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.5589e-01 (3.4707e-01)	Acc@1  92.97 ( 88.78)	Acc@5 100.00 ( 99.59)
Epoch: [21][100/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.9694e-01 (3.4629e-01)	Acc@1  92.19 ( 88.71)	Acc@5  99.22 ( 99.58)
Epoch: [21][110/391]	Time  0.032 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.9434e-01 (3.4368e-01)	Acc@1  89.06 ( 88.80)	Acc@5 100.00 ( 99.58)
Epoch: [21][120/391]	Time  0.030 ( 0.027)	Data  0.002 ( 0.003)	Loss 3.7548e-01 (3.4791e-01)	Acc@1  89.06 ( 88.69)	Acc@5 100.00 ( 99.55)
Epoch: [21][130/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.3761e-01 (3.4862e-01)	Acc@1  87.50 ( 88.60)	Acc@5 100.00 ( 99.54)
Epoch: [21][140/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.1181e-01 (3.4783e-01)	Acc@1  86.72 ( 88.64)	Acc@5 100.00 ( 99.54)
Epoch: [21][150/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.2723e-01 (3.4816e-01)	Acc@1  88.28 ( 88.58)	Acc@5 100.00 ( 99.52)
Epoch: [21][160/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.7387e-01 (3.4531e-01)	Acc@1  89.84 ( 88.65)	Acc@5  97.66 ( 99.52)
Epoch: [21][170/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.2086e-01 (3.4586e-01)	Acc@1  82.03 ( 88.59)	Acc@5  99.22 ( 99.52)
Epoch: [21][180/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.9537e-01 (3.4842e-01)	Acc@1  89.06 ( 88.50)	Acc@5 100.00 ( 99.52)
Epoch: [21][190/391]	Time  0.024 ( 0.026)	Data  0.005 ( 0.003)	Loss 4.2260e-01 (3.4770e-01)	Acc@1  83.59 ( 88.49)	Acc@5 100.00 ( 99.53)
Epoch: [21][200/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.8266e-01 (3.4986e-01)	Acc@1  88.28 ( 88.40)	Acc@5  99.22 ( 99.51)
Epoch: [21][210/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.5976e-01 (3.4997e-01)	Acc@1  89.06 ( 88.40)	Acc@5 100.00 ( 99.50)
Epoch: [21][220/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.2583e-01 (3.4903e-01)	Acc@1  91.41 ( 88.44)	Acc@5  99.22 ( 99.50)
Epoch: [21][230/391]	Time  0.025 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.1896e-01 (3.5113e-01)	Acc@1  89.06 ( 88.41)	Acc@5 100.00 ( 99.49)
Epoch: [21][240/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.9860e-01 (3.5257e-01)	Acc@1  89.06 ( 88.32)	Acc@5  99.22 ( 99.48)
Epoch: [21][250/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.7905e-01 (3.5400e-01)	Acc@1  79.69 ( 88.26)	Acc@5 100.00 ( 99.49)
Epoch: [21][260/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.8360e-01 (3.5348e-01)	Acc@1  89.06 ( 88.24)	Acc@5 100.00 ( 99.50)
Epoch: [21][270/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3716e-01 (3.5390e-01)	Acc@1  87.50 ( 88.23)	Acc@5  99.22 ( 99.50)
Epoch: [21][280/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.6817e-01 (3.5396e-01)	Acc@1  92.19 ( 88.27)	Acc@5 100.00 ( 99.50)
Epoch: [21][290/391]	Time  0.025 ( 0.026)	Data  0.005 ( 0.003)	Loss 4.0850e-01 (3.5543e-01)	Acc@1  87.50 ( 88.22)	Acc@5 100.00 ( 99.50)
Epoch: [21][300/391]	Time  0.030 ( 0.026)	Data  0.000 ( 0.003)	Loss 3.5483e-01 (3.5508e-01)	Acc@1  88.28 ( 88.24)	Acc@5  98.44 ( 99.49)
Epoch: [21][310/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.2513e-01 (3.5524e-01)	Acc@1  90.62 ( 88.25)	Acc@5  99.22 ( 99.49)
Epoch: [21][320/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.5296e-01 (3.5798e-01)	Acc@1  87.50 ( 88.19)	Acc@5  99.22 ( 99.48)
Epoch: [21][330/391]	Time  0.031 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.7486e-01 (3.5717e-01)	Acc@1  92.19 ( 88.23)	Acc@5  99.22 ( 99.47)
Epoch: [21][340/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.8525e-01 (3.5554e-01)	Acc@1  90.62 ( 88.26)	Acc@5  99.22 ( 99.48)
Epoch: [21][350/391]	Time  0.038 ( 0.026)	Data  0.005 ( 0.003)	Loss 2.4968e-01 (3.5578e-01)	Acc@1  91.41 ( 88.27)	Acc@5  99.22 ( 99.46)
Epoch: [21][360/391]	Time  0.023 ( 0.026)	Data  0.003 ( 0.003)	Loss 4.0801e-01 (3.5706e-01)	Acc@1  86.72 ( 88.26)	Acc@5  98.44 ( 99.46)
Epoch: [21][370/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.4772e-01 (3.5854e-01)	Acc@1  82.03 ( 88.18)	Acc@5  99.22 ( 99.47)
Epoch: [21][380/391]	Time  0.033 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.1963e-01 (3.5791e-01)	Acc@1  86.72 ( 88.19)	Acc@5 100.00 ( 99.47)
Epoch: [21][390/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3382e-01 (3.5787e-01)	Acc@1  90.00 ( 88.19)	Acc@5 100.00 ( 99.47)
## e[21] optimizer.zero_grad (sum) time: 0.10884904861450195
## e[21]       loss.backward (sum) time: 2.249544382095337
## e[21]      optimizer.step (sum) time: 0.8796632289886475
## epoch[21] training(only) time: 10.336057424545288
# Switched to evaluate mode...
Test: [  0/100]	Time  0.152 ( 0.152)	Loss 5.0565e-01 (5.0565e-01)	Acc@1  87.00 ( 87.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.011 ( 0.027)	Loss 5.7255e-01 (4.6525e-01)	Acc@1  82.00 ( 85.27)	Acc@5  97.00 ( 99.09)
Test: [ 20/100]	Time  0.016 ( 0.021)	Loss 4.8012e-01 (4.8594e-01)	Acc@1  82.00 ( 84.67)	Acc@5  99.00 ( 98.86)
Test: [ 30/100]	Time  0.012 ( 0.019)	Loss 5.1058e-01 (5.0141e-01)	Acc@1  82.00 ( 84.29)	Acc@5  99.00 ( 98.87)
Test: [ 40/100]	Time  0.012 ( 0.018)	Loss 5.0843e-01 (5.0319e-01)	Acc@1  80.00 ( 84.27)	Acc@5  98.00 ( 98.90)
Test: [ 50/100]	Time  0.012 ( 0.017)	Loss 4.2107e-01 (5.0510e-01)	Acc@1  88.00 ( 84.49)	Acc@5  98.00 ( 98.86)
Test: [ 60/100]	Time  0.011 ( 0.017)	Loss 4.9110e-01 (5.0373e-01)	Acc@1  86.00 ( 84.34)	Acc@5  98.00 ( 98.89)
Test: [ 70/100]	Time  0.020 ( 0.016)	Loss 4.3410e-01 (4.9381e-01)	Acc@1  87.00 ( 84.62)	Acc@5 100.00 ( 98.92)
Test: [ 80/100]	Time  0.017 ( 0.016)	Loss 5.1367e-01 (4.9431e-01)	Acc@1  86.00 ( 84.54)	Acc@5  98.00 ( 98.94)
Test: [ 90/100]	Time  0.015 ( 0.016)	Loss 4.3304e-01 (4.9202e-01)	Acc@1  88.00 ( 84.58)	Acc@5 100.00 ( 98.95)
 * Acc@1 84.630 Acc@5 98.990
### epoch[21] execution time: 12.052865505218506
EPOCH 22
# current learning rate: 0.1
# Switched to train mode...
Epoch: [22][  0/391]	Time  0.196 ( 0.196)	Data  0.175 ( 0.175)	Loss 2.9937e-01 (2.9937e-01)	Acc@1  89.06 ( 89.06)	Acc@5  99.22 ( 99.22)
Epoch: [22][ 10/391]	Time  0.024 ( 0.041)	Data  0.001 ( 0.018)	Loss 3.1622e-01 (3.4296e-01)	Acc@1  89.06 ( 88.64)	Acc@5  99.22 ( 99.22)
Epoch: [22][ 20/391]	Time  0.025 ( 0.034)	Data  0.001 ( 0.010)	Loss 2.4879e-01 (3.4627e-01)	Acc@1  95.31 ( 88.76)	Acc@5  99.22 ( 99.37)
Epoch: [22][ 30/391]	Time  0.023 ( 0.031)	Data  0.001 ( 0.008)	Loss 3.1837e-01 (3.3974e-01)	Acc@1  89.06 ( 88.73)	Acc@5 100.00 ( 99.34)
Epoch: [22][ 40/391]	Time  0.025 ( 0.030)	Data  0.001 ( 0.006)	Loss 4.0694e-01 (3.4760e-01)	Acc@1  89.06 ( 88.70)	Acc@5  99.22 ( 99.33)
Epoch: [22][ 50/391]	Time  0.024 ( 0.029)	Data  0.001 ( 0.006)	Loss 2.6235e-01 (3.3614e-01)	Acc@1  89.06 ( 89.06)	Acc@5 100.00 ( 99.40)
Epoch: [22][ 60/391]	Time  0.023 ( 0.029)	Data  0.001 ( 0.005)	Loss 4.0284e-01 (3.3428e-01)	Acc@1  88.28 ( 89.15)	Acc@5  99.22 ( 99.46)
Epoch: [22][ 70/391]	Time  0.022 ( 0.028)	Data  0.001 ( 0.005)	Loss 3.9323e-01 (3.3384e-01)	Acc@1  88.28 ( 89.18)	Acc@5  98.44 ( 99.42)
Epoch: [22][ 80/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.004)	Loss 3.4489e-01 (3.3567e-01)	Acc@1  89.84 ( 89.13)	Acc@5 100.00 ( 99.43)
Epoch: [22][ 90/391]	Time  0.027 ( 0.028)	Data  0.002 ( 0.004)	Loss 2.9017e-01 (3.3522e-01)	Acc@1  89.06 ( 89.02)	Acc@5  99.22 ( 99.46)
Epoch: [22][100/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.8019e-01 (3.3825e-01)	Acc@1  89.84 ( 88.95)	Acc@5  98.44 ( 99.45)
Epoch: [22][110/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.7047e-01 (3.3927e-01)	Acc@1  92.19 ( 88.93)	Acc@5 100.00 ( 99.47)
Epoch: [22][120/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.4485e-01 (3.3895e-01)	Acc@1  93.75 ( 88.96)	Acc@5  99.22 ( 99.47)
Epoch: [22][130/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.6636e-01 (3.3939e-01)	Acc@1  87.50 ( 88.89)	Acc@5  99.22 ( 99.49)
Epoch: [22][140/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.7273e-01 (3.3766e-01)	Acc@1  88.28 ( 88.89)	Acc@5 100.00 ( 99.51)
Epoch: [22][150/391]	Time  0.027 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.8896e-01 (3.3749e-01)	Acc@1  87.50 ( 88.90)	Acc@5  99.22 ( 99.50)
Epoch: [22][160/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.8802e-01 (3.3769e-01)	Acc@1  93.75 ( 88.87)	Acc@5 100.00 ( 99.49)
Epoch: [22][170/391]	Time  0.044 ( 0.027)	Data  0.016 ( 0.003)	Loss 4.3569e-01 (3.3968e-01)	Acc@1  88.28 ( 88.84)	Acc@5  99.22 ( 99.47)
Epoch: [22][180/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.3228e-01 (3.4137e-01)	Acc@1  87.50 ( 88.81)	Acc@5  99.22 ( 99.45)
Epoch: [22][190/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.8256e-01 (3.4086e-01)	Acc@1  89.84 ( 88.82)	Acc@5 100.00 ( 99.46)
Epoch: [22][200/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.4603e-01 (3.4279e-01)	Acc@1  89.84 ( 88.82)	Acc@5  99.22 ( 99.45)
Epoch: [22][210/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.5478e-01 (3.4290e-01)	Acc@1  87.50 ( 88.80)	Acc@5 100.00 ( 99.46)
Epoch: [22][220/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5184e-01 (3.4206e-01)	Acc@1  91.41 ( 88.83)	Acc@5  99.22 ( 99.47)
Epoch: [22][230/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.1230e-01 (3.4247e-01)	Acc@1  92.19 ( 88.86)	Acc@5 100.00 ( 99.46)
Epoch: [22][240/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.2724e-01 (3.4357e-01)	Acc@1  92.19 ( 88.82)	Acc@5  98.44 ( 99.46)
Epoch: [22][250/391]	Time  0.023 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.5801e-01 (3.4420e-01)	Acc@1  90.62 ( 88.79)	Acc@5 100.00 ( 99.47)
Epoch: [22][260/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.9368e-01 (3.4470e-01)	Acc@1  90.62 ( 88.80)	Acc@5  99.22 ( 99.45)
Epoch: [22][270/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.0437e-01 (3.4393e-01)	Acc@1  87.50 ( 88.87)	Acc@5  99.22 ( 99.45)
Epoch: [22][280/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.1681e-01 (3.4467e-01)	Acc@1  82.03 ( 88.84)	Acc@5 100.00 ( 99.46)
Epoch: [22][290/391]	Time  0.033 ( 0.026)	Data  0.005 ( 0.003)	Loss 3.4465e-01 (3.4266e-01)	Acc@1  88.28 ( 88.91)	Acc@5  98.44 ( 99.46)
Epoch: [22][300/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.2803e-01 (3.4175e-01)	Acc@1  89.06 ( 88.93)	Acc@5  98.44 ( 99.47)
Epoch: [22][310/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.1788e-01 (3.4115e-01)	Acc@1  86.72 ( 88.94)	Acc@5  98.44 ( 99.47)
Epoch: [22][320/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.1234e-01 (3.4144e-01)	Acc@1  91.41 ( 88.94)	Acc@5 100.00 ( 99.47)
Epoch: [22][330/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.7989e-01 (3.4194e-01)	Acc@1  92.97 ( 88.94)	Acc@5 100.00 ( 99.46)
Epoch: [22][340/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.6277e-01 (3.4236e-01)	Acc@1  91.41 ( 88.91)	Acc@5 100.00 ( 99.46)
Epoch: [22][350/391]	Time  0.038 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.8314e-01 (3.4324e-01)	Acc@1  88.28 ( 88.88)	Acc@5  99.22 ( 99.46)
Epoch: [22][360/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.7465e-01 (3.4344e-01)	Acc@1  82.81 ( 88.87)	Acc@5  98.44 ( 99.45)
Epoch: [22][370/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.8376e-01 (3.4401e-01)	Acc@1  89.06 ( 88.83)	Acc@5  99.22 ( 99.45)
Epoch: [22][380/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.5750e-01 (3.4464e-01)	Acc@1  84.38 ( 88.80)	Acc@5 100.00 ( 99.45)
Epoch: [22][390/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.3320e-01 (3.4545e-01)	Acc@1  86.25 ( 88.77)	Acc@5  98.75 ( 99.43)
## e[22] optimizer.zero_grad (sum) time: 0.11007452011108398
## e[22]       loss.backward (sum) time: 2.228565216064453
## e[22]      optimizer.step (sum) time: 0.8610751628875732
## epoch[22] training(only) time: 10.413043022155762
# Switched to evaluate mode...
Test: [  0/100]	Time  0.165 ( 0.165)	Loss 5.5146e-01 (5.5146e-01)	Acc@1  85.00 ( 85.00)	Acc@5  97.00 ( 97.00)
Test: [ 10/100]	Time  0.011 ( 0.027)	Loss 6.0405e-01 (4.8330e-01)	Acc@1  82.00 ( 84.18)	Acc@5  98.00 ( 98.82)
Test: [ 20/100]	Time  0.011 ( 0.021)	Loss 6.8094e-01 (5.0100e-01)	Acc@1  75.00 ( 83.52)	Acc@5  99.00 ( 98.62)
Test: [ 30/100]	Time  0.011 ( 0.019)	Loss 5.7496e-01 (5.0899e-01)	Acc@1  83.00 ( 83.74)	Acc@5  97.00 ( 98.68)
Test: [ 40/100]	Time  0.012 ( 0.018)	Loss 5.3490e-01 (5.1620e-01)	Acc@1  83.00 ( 83.73)	Acc@5  96.00 ( 98.63)
Test: [ 50/100]	Time  0.014 ( 0.017)	Loss 3.7332e-01 (5.1943e-01)	Acc@1  88.00 ( 83.80)	Acc@5 100.00 ( 98.65)
Test: [ 60/100]	Time  0.010 ( 0.016)	Loss 5.3064e-01 (5.1757e-01)	Acc@1  85.00 ( 83.72)	Acc@5  99.00 ( 98.79)
Test: [ 70/100]	Time  0.020 ( 0.016)	Loss 6.8300e-01 (5.1814e-01)	Acc@1  77.00 ( 83.72)	Acc@5  99.00 ( 98.80)
Test: [ 80/100]	Time  0.017 ( 0.016)	Loss 3.6470e-01 (5.1434e-01)	Acc@1  88.00 ( 83.80)	Acc@5 100.00 ( 98.84)
Test: [ 90/100]	Time  0.010 ( 0.016)	Loss 4.6971e-01 (5.1092e-01)	Acc@1  85.00 ( 83.86)	Acc@5 100.00 ( 98.88)
 * Acc@1 83.870 Acc@5 98.910
### epoch[22] execution time: 12.11734938621521
EPOCH 23
# current learning rate: 0.1
# Switched to train mode...
Epoch: [23][  0/391]	Time  0.191 ( 0.191)	Data  0.171 ( 0.171)	Loss 2.4037e-01 (2.4037e-01)	Acc@1  90.62 ( 90.62)	Acc@5 100.00 (100.00)
Epoch: [23][ 10/391]	Time  0.024 ( 0.041)	Data  0.004 ( 0.018)	Loss 2.7394e-01 (3.1258e-01)	Acc@1  89.84 ( 90.06)	Acc@5 100.00 ( 99.57)
Epoch: [23][ 20/391]	Time  0.021 ( 0.034)	Data  0.002 ( 0.011)	Loss 5.3053e-01 (3.3892e-01)	Acc@1  83.59 ( 88.69)	Acc@5  97.66 ( 99.52)
Epoch: [23][ 30/391]	Time  0.023 ( 0.031)	Data  0.001 ( 0.008)	Loss 3.1338e-01 (3.4079e-01)	Acc@1  89.84 ( 88.33)	Acc@5 100.00 ( 99.62)
Epoch: [23][ 40/391]	Time  0.022 ( 0.030)	Data  0.002 ( 0.007)	Loss 3.1702e-01 (3.3833e-01)	Acc@1  87.50 ( 88.43)	Acc@5  98.44 ( 99.56)
Epoch: [23][ 50/391]	Time  0.023 ( 0.029)	Data  0.001 ( 0.006)	Loss 3.1485e-01 (3.2947e-01)	Acc@1  91.41 ( 88.79)	Acc@5 100.00 ( 99.62)
Epoch: [23][ 60/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.7485e-01 (3.2477e-01)	Acc@1  90.62 ( 89.04)	Acc@5  99.22 ( 99.60)
Epoch: [23][ 70/391]	Time  0.028 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.2605e-01 (3.1460e-01)	Acc@1  94.53 ( 89.37)	Acc@5  99.22 ( 99.61)
Epoch: [23][ 80/391]	Time  0.023 ( 0.028)	Data  0.002 ( 0.004)	Loss 1.8714e-01 (3.1181e-01)	Acc@1  94.53 ( 89.56)	Acc@5 100.00 ( 99.62)
Epoch: [23][ 90/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 2.7354e-01 (3.1541e-01)	Acc@1  90.62 ( 89.39)	Acc@5 100.00 ( 99.61)
Epoch: [23][100/391]	Time  0.033 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.1610e-01 (3.1572e-01)	Acc@1  89.84 ( 89.34)	Acc@5 100.00 ( 99.63)
Epoch: [23][110/391]	Time  0.030 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.3242e-01 (3.2171e-01)	Acc@1  85.94 ( 89.15)	Acc@5 100.00 ( 99.58)
Epoch: [23][120/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.3832e-01 (3.2382e-01)	Acc@1  86.72 ( 89.09)	Acc@5  99.22 ( 99.57)
Epoch: [23][130/391]	Time  0.031 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.6210e-01 (3.2479e-01)	Acc@1  90.62 ( 89.01)	Acc@5 100.00 ( 99.57)
Epoch: [23][140/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.3843e-01 (3.2876e-01)	Acc@1  84.38 ( 88.94)	Acc@5  98.44 ( 99.55)
Epoch: [23][150/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.4007e-01 (3.2931e-01)	Acc@1  88.28 ( 88.91)	Acc@5 100.00 ( 99.55)
Epoch: [23][160/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.8879e-01 (3.3184e-01)	Acc@1  86.72 ( 88.87)	Acc@5  99.22 ( 99.54)
Epoch: [23][170/391]	Time  0.023 ( 0.027)	Data  0.000 ( 0.003)	Loss 2.9492e-01 (3.3223e-01)	Acc@1  88.28 ( 88.83)	Acc@5 100.00 ( 99.54)
Epoch: [23][180/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.9551e-01 (3.3235e-01)	Acc@1  89.06 ( 88.80)	Acc@5 100.00 ( 99.54)
Epoch: [23][190/391]	Time  0.023 ( 0.027)	Data  0.000 ( 0.003)	Loss 3.4152e-01 (3.3319e-01)	Acc@1  88.28 ( 88.81)	Acc@5  98.44 ( 99.51)
Epoch: [23][200/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.3425e-01 (3.3389e-01)	Acc@1  82.81 ( 88.79)	Acc@5  98.44 ( 99.51)
Epoch: [23][210/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 3.5971e-01 (3.3446e-01)	Acc@1  85.94 ( 88.76)	Acc@5  99.22 ( 99.49)
Epoch: [23][220/391]	Time  0.027 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.1286e-01 (3.3406e-01)	Acc@1  92.19 ( 88.80)	Acc@5  98.44 ( 99.48)
Epoch: [23][230/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.5345e-01 (3.3606e-01)	Acc@1  86.72 ( 88.79)	Acc@5  98.44 ( 99.46)
Epoch: [23][240/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.3290e-01 (3.3475e-01)	Acc@1  90.62 ( 88.83)	Acc@5 100.00 ( 99.47)
Epoch: [23][250/391]	Time  0.032 ( 0.027)	Data  0.000 ( 0.003)	Loss 3.1064e-01 (3.3594e-01)	Acc@1  90.62 ( 88.81)	Acc@5  99.22 ( 99.46)
Epoch: [23][260/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4755e-01 (3.3489e-01)	Acc@1  90.62 ( 88.85)	Acc@5 100.00 ( 99.48)
Epoch: [23][270/391]	Time  0.023 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.6827e-01 (3.3490e-01)	Acc@1  92.97 ( 88.88)	Acc@5  99.22 ( 99.48)
Epoch: [23][280/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.5865e-01 (3.3679e-01)	Acc@1  89.06 ( 88.82)	Acc@5  96.88 ( 99.46)
Epoch: [23][290/391]	Time  0.028 ( 0.026)	Data  0.004 ( 0.003)	Loss 3.7984e-01 (3.3861e-01)	Acc@1  85.16 ( 88.75)	Acc@5 100.00 ( 99.47)
Epoch: [23][300/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.4629e-01 (3.3902e-01)	Acc@1  86.72 ( 88.75)	Acc@5  98.44 ( 99.47)
Epoch: [23][310/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.5216e-01 (3.3995e-01)	Acc@1  89.06 ( 88.73)	Acc@5  98.44 ( 99.45)
Epoch: [23][320/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.2158e-01 (3.4019e-01)	Acc@1  87.50 ( 88.71)	Acc@5  99.22 ( 99.45)
Epoch: [23][330/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.2819e-01 (3.4032e-01)	Acc@1  88.28 ( 88.72)	Acc@5  99.22 ( 99.45)
Epoch: [23][340/391]	Time  0.026 ( 0.026)	Data  0.003 ( 0.003)	Loss 3.8374e-01 (3.3988e-01)	Acc@1  87.50 ( 88.74)	Acc@5 100.00 ( 99.45)
Epoch: [23][350/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4955e-01 (3.3947e-01)	Acc@1  90.62 ( 88.75)	Acc@5  98.44 ( 99.45)
Epoch: [23][360/391]	Time  0.036 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3285e-01 (3.3948e-01)	Acc@1  89.06 ( 88.76)	Acc@5 100.00 ( 99.45)
Epoch: [23][370/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.2356e-01 (3.4016e-01)	Acc@1  83.59 ( 88.73)	Acc@5  99.22 ( 99.45)
Epoch: [23][380/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.7183e-01 (3.3879e-01)	Acc@1  90.62 ( 88.78)	Acc@5  99.22 ( 99.45)
Epoch: [23][390/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.0069e-01 (3.3960e-01)	Acc@1  88.75 ( 88.77)	Acc@5  98.75 ( 99.45)
## e[23] optimizer.zero_grad (sum) time: 0.10914373397827148
## e[23]       loss.backward (sum) time: 2.2342751026153564
## e[23]      optimizer.step (sum) time: 0.8771910667419434
## epoch[23] training(only) time: 10.423568725585938
# Switched to evaluate mode...
Test: [  0/100]	Time  0.164 ( 0.164)	Loss 3.7684e-01 (3.7684e-01)	Acc@1  87.00 ( 87.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.015 ( 0.028)	Loss 4.9819e-01 (4.7726e-01)	Acc@1  87.00 ( 84.00)	Acc@5  98.00 ( 98.91)
Test: [ 20/100]	Time  0.015 ( 0.021)	Loss 5.9430e-01 (5.0134e-01)	Acc@1  80.00 ( 83.43)	Acc@5 100.00 ( 98.95)
Test: [ 30/100]	Time  0.011 ( 0.019)	Loss 5.1098e-01 (5.1250e-01)	Acc@1  81.00 ( 83.42)	Acc@5  98.00 ( 99.03)
Test: [ 40/100]	Time  0.013 ( 0.018)	Loss 5.8955e-01 (5.1671e-01)	Acc@1  81.00 ( 83.41)	Acc@5  98.00 ( 99.05)
Test: [ 50/100]	Time  0.011 ( 0.017)	Loss 5.2379e-01 (5.1806e-01)	Acc@1  90.00 ( 83.53)	Acc@5 100.00 ( 99.00)
Test: [ 60/100]	Time  0.011 ( 0.017)	Loss 6.8090e-01 (5.1945e-01)	Acc@1  79.00 ( 83.48)	Acc@5  99.00 ( 99.07)
Test: [ 70/100]	Time  0.016 ( 0.017)	Loss 7.1060e-01 (5.1964e-01)	Acc@1  83.00 ( 83.66)	Acc@5  99.00 ( 99.06)
Test: [ 80/100]	Time  0.011 ( 0.016)	Loss 3.2879e-01 (5.2009e-01)	Acc@1  88.00 ( 83.58)	Acc@5 100.00 ( 99.07)
Test: [ 90/100]	Time  0.019 ( 0.016)	Loss 4.7711e-01 (5.1568e-01)	Acc@1  88.00 ( 83.77)	Acc@5 100.00 ( 99.04)
 * Acc@1 83.920 Acc@5 99.110
### epoch[23] execution time: 12.142173767089844
EPOCH 24
# current learning rate: 0.1
# Switched to train mode...
Epoch: [24][  0/391]	Time  0.185 ( 0.185)	Data  0.164 ( 0.164)	Loss 3.1515e-01 (3.1515e-01)	Acc@1  89.06 ( 89.06)	Acc@5 100.00 (100.00)
Epoch: [24][ 10/391]	Time  0.022 ( 0.040)	Data  0.001 ( 0.017)	Loss 3.6062e-01 (3.0622e-01)	Acc@1  88.28 ( 90.27)	Acc@5  99.22 ( 99.29)
Epoch: [24][ 20/391]	Time  0.023 ( 0.033)	Data  0.001 ( 0.010)	Loss 5.4909e-01 (3.1873e-01)	Acc@1  85.94 ( 89.96)	Acc@5 100.00 ( 99.44)
Epoch: [24][ 30/391]	Time  0.025 ( 0.031)	Data  0.002 ( 0.007)	Loss 4.2224e-01 (3.2742e-01)	Acc@1  89.06 ( 89.49)	Acc@5  99.22 ( 99.50)
Epoch: [24][ 40/391]	Time  0.023 ( 0.029)	Data  0.001 ( 0.006)	Loss 2.4768e-01 (3.1341e-01)	Acc@1  91.41 ( 89.77)	Acc@5  99.22 ( 99.56)
Epoch: [24][ 50/391]	Time  0.027 ( 0.029)	Data  0.007 ( 0.005)	Loss 2.0326e-01 (3.1435e-01)	Acc@1  94.53 ( 89.60)	Acc@5 100.00 ( 99.62)
Epoch: [24][ 60/391]	Time  0.028 ( 0.028)	Data  0.001 ( 0.005)	Loss 3.3938e-01 (3.2184e-01)	Acc@1  87.50 ( 89.42)	Acc@5 100.00 ( 99.54)
Epoch: [24][ 70/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.004)	Loss 4.3201e-01 (3.2602e-01)	Acc@1  86.72 ( 89.33)	Acc@5  98.44 ( 99.54)
Epoch: [24][ 80/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 4.0296e-01 (3.1985e-01)	Acc@1  85.16 ( 89.52)	Acc@5  98.44 ( 99.55)
Epoch: [24][ 90/391]	Time  0.023 ( 0.027)	Data  0.000 ( 0.004)	Loss 2.9717e-01 (3.2283e-01)	Acc@1  89.84 ( 89.40)	Acc@5  99.22 ( 99.55)
Epoch: [24][100/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.9401e-01 (3.2071e-01)	Acc@1  89.84 ( 89.47)	Acc@5 100.00 ( 99.57)
Epoch: [24][110/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.8489e-01 (3.2107e-01)	Acc@1  89.84 ( 89.39)	Acc@5 100.00 ( 99.58)
Epoch: [24][120/391]	Time  0.028 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.8165e-01 (3.1794e-01)	Acc@1  91.41 ( 89.48)	Acc@5  99.22 ( 99.56)
Epoch: [24][130/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.1603e-01 (3.1549e-01)	Acc@1  89.06 ( 89.58)	Acc@5 100.00 ( 99.57)
Epoch: [24][140/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 2.7478e-01 (3.1856e-01)	Acc@1  91.41 ( 89.57)	Acc@5 100.00 ( 99.58)
Epoch: [24][150/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.2211e-01 (3.2070e-01)	Acc@1  88.28 ( 89.45)	Acc@5  99.22 ( 99.59)
Epoch: [24][160/391]	Time  0.026 ( 0.027)	Data  0.005 ( 0.003)	Loss 3.8292e-01 (3.1986e-01)	Acc@1  86.72 ( 89.48)	Acc@5 100.00 ( 99.59)
Epoch: [24][170/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.6694e-01 (3.1878e-01)	Acc@1  88.28 ( 89.50)	Acc@5  99.22 ( 99.59)
Epoch: [24][180/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.8341e-01 (3.1885e-01)	Acc@1  87.50 ( 89.50)	Acc@5  98.44 ( 99.59)
Epoch: [24][190/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.5954e-01 (3.1831e-01)	Acc@1  88.28 ( 89.55)	Acc@5  99.22 ( 99.59)
Epoch: [24][200/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.1480e-01 (3.1900e-01)	Acc@1  94.53 ( 89.51)	Acc@5 100.00 ( 99.59)
Epoch: [24][210/391]	Time  0.026 ( 0.027)	Data  0.003 ( 0.003)	Loss 3.7917e-01 (3.1849e-01)	Acc@1  88.28 ( 89.54)	Acc@5 100.00 ( 99.60)
Epoch: [24][220/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.7921e-01 (3.1906e-01)	Acc@1  85.94 ( 89.49)	Acc@5 100.00 ( 99.60)
Epoch: [24][230/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.9553e-01 (3.2170e-01)	Acc@1  88.28 ( 89.40)	Acc@5 100.00 ( 99.57)
Epoch: [24][240/391]	Time  0.024 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.4699e-01 (3.2213e-01)	Acc@1  92.97 ( 89.38)	Acc@5 100.00 ( 99.58)
Epoch: [24][250/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3253e-01 (3.2280e-01)	Acc@1  89.84 ( 89.38)	Acc@5 100.00 ( 99.58)
Epoch: [24][260/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.2544e-01 (3.2414e-01)	Acc@1  88.28 ( 89.34)	Acc@5  98.44 ( 99.57)
Epoch: [24][270/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3959e-01 (3.2305e-01)	Acc@1  91.41 ( 89.34)	Acc@5 100.00 ( 99.57)
Epoch: [24][280/391]	Time  0.029 ( 0.026)	Data  0.008 ( 0.003)	Loss 2.4953e-01 (3.2321e-01)	Acc@1  91.41 ( 89.34)	Acc@5  98.44 ( 99.57)
Epoch: [24][290/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.0369e-01 (3.2161e-01)	Acc@1  89.06 ( 89.38)	Acc@5  99.22 ( 99.57)
Epoch: [24][300/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.1254e-01 (3.2227e-01)	Acc@1  89.06 ( 89.39)	Acc@5 100.00 ( 99.56)
Epoch: [24][310/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.0597e-01 (3.2238e-01)	Acc@1  92.19 ( 89.39)	Acc@5 100.00 ( 99.57)
Epoch: [24][320/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 4.5381e-01 (3.2304e-01)	Acc@1  88.28 ( 89.36)	Acc@5  99.22 ( 99.58)
Epoch: [24][330/391]	Time  0.024 ( 0.026)	Data  0.000 ( 0.003)	Loss 4.4567e-01 (3.2349e-01)	Acc@1  86.72 ( 89.34)	Acc@5  99.22 ( 99.58)
Epoch: [24][340/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.6528e-01 (3.2404e-01)	Acc@1  91.41 ( 89.32)	Acc@5 100.00 ( 99.57)
Epoch: [24][350/391]	Time  0.039 ( 0.026)	Data  0.020 ( 0.003)	Loss 4.0041e-01 (3.2544e-01)	Acc@1  87.50 ( 89.27)	Acc@5  98.44 ( 99.56)
Epoch: [24][360/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.7457e-01 (3.2610e-01)	Acc@1  89.84 ( 89.26)	Acc@5 100.00 ( 99.55)
Epoch: [24][370/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.4858e-01 (3.2689e-01)	Acc@1  85.94 ( 89.22)	Acc@5 100.00 ( 99.56)
Epoch: [24][380/391]	Time  0.033 ( 0.026)	Data  0.011 ( 0.003)	Loss 3.6152e-01 (3.2850e-01)	Acc@1  87.50 ( 89.18)	Acc@5  99.22 ( 99.54)
Epoch: [24][390/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.3832e-01 (3.2868e-01)	Acc@1  83.75 ( 89.16)	Acc@5 100.00 ( 99.55)
## e[24] optimizer.zero_grad (sum) time: 0.10929584503173828
## e[24]       loss.backward (sum) time: 2.227558135986328
## e[24]      optimizer.step (sum) time: 0.8786580562591553
## epoch[24] training(only) time: 10.41341519355774
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 7.4935e-01 (7.4935e-01)	Acc@1  80.00 ( 80.00)	Acc@5  96.00 ( 96.00)
Test: [ 10/100]	Time  0.011 ( 0.027)	Loss 7.0613e-01 (5.7121e-01)	Acc@1  80.00 ( 83.27)	Acc@5  97.00 ( 98.27)
Test: [ 20/100]	Time  0.010 ( 0.021)	Loss 5.4844e-01 (5.7132e-01)	Acc@1  82.00 ( 83.14)	Acc@5 100.00 ( 98.14)
Test: [ 30/100]	Time  0.011 ( 0.019)	Loss 5.9237e-01 (5.6968e-01)	Acc@1  81.00 ( 83.10)	Acc@5  97.00 ( 98.29)
Test: [ 40/100]	Time  0.016 ( 0.018)	Loss 5.1790e-01 (5.8185e-01)	Acc@1  87.00 ( 82.85)	Acc@5  98.00 ( 98.27)
Test: [ 50/100]	Time  0.015 ( 0.017)	Loss 3.7209e-01 (5.7682e-01)	Acc@1  87.00 ( 82.88)	Acc@5  99.00 ( 98.31)
Test: [ 60/100]	Time  0.020 ( 0.017)	Loss 6.6616e-01 (5.7865e-01)	Acc@1  78.00 ( 82.70)	Acc@5  99.00 ( 98.46)
Test: [ 70/100]	Time  0.019 ( 0.017)	Loss 7.5089e-01 (5.8249e-01)	Acc@1  84.00 ( 82.83)	Acc@5  98.00 ( 98.44)
Test: [ 80/100]	Time  0.011 ( 0.016)	Loss 4.3665e-01 (5.7093e-01)	Acc@1  82.00 ( 82.96)	Acc@5  98.00 ( 98.47)
Test: [ 90/100]	Time  0.014 ( 0.016)	Loss 4.2002e-01 (5.7064e-01)	Acc@1  86.00 ( 82.93)	Acc@5 100.00 ( 98.49)
 * Acc@1 82.990 Acc@5 98.540
### epoch[24] execution time: 12.142712593078613
EPOCH 25
# current learning rate: 0.1
# Switched to train mode...
Epoch: [25][  0/391]	Time  0.190 ( 0.190)	Data  0.169 ( 0.169)	Loss 3.9854e-01 (3.9854e-01)	Acc@1  89.84 ( 89.84)	Acc@5  98.44 ( 98.44)
Epoch: [25][ 10/391]	Time  0.023 ( 0.040)	Data  0.001 ( 0.017)	Loss 2.7387e-01 (2.9157e-01)	Acc@1  91.41 ( 91.34)	Acc@5  99.22 ( 99.57)
Epoch: [25][ 20/391]	Time  0.027 ( 0.033)	Data  0.002 ( 0.010)	Loss 3.9026e-01 (3.0477e-01)	Acc@1  85.94 ( 90.51)	Acc@5 100.00 ( 99.67)
Epoch: [25][ 30/391]	Time  0.022 ( 0.031)	Data  0.001 ( 0.008)	Loss 3.5548e-01 (3.0336e-01)	Acc@1  90.62 ( 90.27)	Acc@5 100.00 ( 99.67)
Epoch: [25][ 40/391]	Time  0.023 ( 0.030)	Data  0.001 ( 0.006)	Loss 2.1810e-01 (3.0429e-01)	Acc@1  91.41 ( 89.94)	Acc@5 100.00 ( 99.64)
Epoch: [25][ 50/391]	Time  0.023 ( 0.029)	Data  0.001 ( 0.006)	Loss 3.7074e-01 (2.9840e-01)	Acc@1  90.62 ( 90.29)	Acc@5  99.22 ( 99.60)
Epoch: [25][ 60/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.5296e-01 (2.9556e-01)	Acc@1  91.41 ( 90.36)	Acc@5  99.22 ( 99.59)
Epoch: [25][ 70/391]	Time  0.025 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.5219e-01 (2.9833e-01)	Acc@1  92.19 ( 90.34)	Acc@5 100.00 ( 99.60)
Epoch: [25][ 80/391]	Time  0.025 ( 0.028)	Data  0.001 ( 0.004)	Loss 4.3301e-01 (3.0489e-01)	Acc@1  85.16 ( 90.08)	Acc@5  99.22 ( 99.61)
Epoch: [25][ 90/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.8018e-01 (3.0826e-01)	Acc@1  87.50 ( 89.97)	Acc@5  99.22 ( 99.61)
Epoch: [25][100/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.8755e-01 (3.0853e-01)	Acc@1  89.84 ( 89.96)	Acc@5 100.00 ( 99.62)
Epoch: [25][110/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.4054e-01 (3.0580e-01)	Acc@1  92.19 ( 90.08)	Acc@5 100.00 ( 99.62)
Epoch: [25][120/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.004)	Loss 4.0290e-01 (3.0405e-01)	Acc@1  89.84 ( 90.22)	Acc@5  99.22 ( 99.62)
Epoch: [25][130/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 3.9061e-01 (3.0680e-01)	Acc@1  88.28 ( 90.14)	Acc@5 100.00 ( 99.64)
Epoch: [25][140/391]	Time  0.031 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.4423e-01 (3.0780e-01)	Acc@1  89.84 ( 90.04)	Acc@5  99.22 ( 99.61)
Epoch: [25][150/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.0743e-01 (3.1099e-01)	Acc@1  83.59 ( 89.90)	Acc@5  99.22 ( 99.60)
Epoch: [25][160/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.7044e-01 (3.0918e-01)	Acc@1  90.62 ( 89.97)	Acc@5 100.00 ( 99.60)
Epoch: [25][170/391]	Time  0.024 ( 0.026)	Data  0.000 ( 0.003)	Loss 3.1437e-01 (3.1015e-01)	Acc@1  89.06 ( 89.96)	Acc@5  99.22 ( 99.60)
Epoch: [25][180/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.6854e-01 (3.1177e-01)	Acc@1  92.97 ( 89.93)	Acc@5 100.00 ( 99.61)
Epoch: [25][190/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.4394e-01 (3.1377e-01)	Acc@1  82.81 ( 89.86)	Acc@5  97.66 ( 99.60)
Epoch: [25][200/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 4.0548e-01 (3.1410e-01)	Acc@1  85.94 ( 89.84)	Acc@5  99.22 ( 99.61)
Epoch: [25][210/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.6133e-01 (3.1403e-01)	Acc@1  92.19 ( 89.83)	Acc@5  98.44 ( 99.60)
Epoch: [25][220/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3101e-01 (3.1454e-01)	Acc@1  89.06 ( 89.83)	Acc@5  98.44 ( 99.59)
Epoch: [25][230/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.5202e-01 (3.1378e-01)	Acc@1  85.16 ( 89.81)	Acc@5  98.44 ( 99.58)
Epoch: [25][240/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5207e-01 (3.1376e-01)	Acc@1  89.84 ( 89.80)	Acc@5 100.00 ( 99.59)
Epoch: [25][250/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8473e-01 (3.1279e-01)	Acc@1  93.75 ( 89.81)	Acc@5 100.00 ( 99.60)
Epoch: [25][260/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.7926e-01 (3.1267e-01)	Acc@1  89.06 ( 89.83)	Acc@5 100.00 ( 99.60)
Epoch: [25][270/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2847e-01 (3.1274e-01)	Acc@1  92.97 ( 89.82)	Acc@5 100.00 ( 99.59)
Epoch: [25][280/391]	Time  0.033 ( 0.026)	Data  0.004 ( 0.003)	Loss 2.2599e-01 (3.1183e-01)	Acc@1  91.41 ( 89.84)	Acc@5  99.22 ( 99.59)
Epoch: [25][290/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.4017e-01 (3.1141e-01)	Acc@1  89.84 ( 89.84)	Acc@5  99.22 ( 99.59)
Epoch: [25][300/391]	Time  0.030 ( 0.026)	Data  0.005 ( 0.003)	Loss 4.6913e-01 (3.1250e-01)	Acc@1  84.38 ( 89.81)	Acc@5  99.22 ( 99.59)
Epoch: [25][310/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.8598e-01 (3.1280e-01)	Acc@1  85.16 ( 89.79)	Acc@5  98.44 ( 99.59)
Epoch: [25][320/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.4499e-01 (3.1423e-01)	Acc@1  92.19 ( 89.78)	Acc@5  99.22 ( 99.58)
Epoch: [25][330/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.8429e-01 (3.1376e-01)	Acc@1  90.62 ( 89.76)	Acc@5  99.22 ( 99.58)
Epoch: [25][340/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.9948e-01 (3.1412e-01)	Acc@1  86.72 ( 89.72)	Acc@5  99.22 ( 99.59)
Epoch: [25][350/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.7234e-01 (3.1445e-01)	Acc@1  88.28 ( 89.73)	Acc@5  98.44 ( 99.58)
Epoch: [25][360/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.6727e-01 (3.1614e-01)	Acc@1  89.06 ( 89.67)	Acc@5 100.00 ( 99.58)
Epoch: [25][370/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.6413e-01 (3.1650e-01)	Acc@1  91.41 ( 89.69)	Acc@5 100.00 ( 99.57)
Epoch: [25][380/391]	Time  0.031 ( 0.026)	Data  0.000 ( 0.003)	Loss 3.8045e-01 (3.1700e-01)	Acc@1  88.28 ( 89.69)	Acc@5  99.22 ( 99.57)
Epoch: [25][390/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.0914e-01 (3.1682e-01)	Acc@1  90.00 ( 89.70)	Acc@5 100.00 ( 99.57)
## e[25] optimizer.zero_grad (sum) time: 0.10893559455871582
## e[25]       loss.backward (sum) time: 2.212038993835449
## e[25]      optimizer.step (sum) time: 0.8703069686889648
## epoch[25] training(only) time: 10.376808881759644
# Switched to evaluate mode...
Test: [  0/100]	Time  0.205 ( 0.205)	Loss 3.4534e-01 (3.4534e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.011 ( 0.032)	Loss 5.8855e-01 (5.0373e-01)	Acc@1  84.00 ( 83.82)	Acc@5  99.00 ( 99.18)
Test: [ 20/100]	Time  0.011 ( 0.023)	Loss 6.1879e-01 (4.8943e-01)	Acc@1  78.00 ( 84.67)	Acc@5 100.00 ( 99.05)
Test: [ 30/100]	Time  0.012 ( 0.020)	Loss 4.5477e-01 (4.9501e-01)	Acc@1  83.00 ( 84.71)	Acc@5 100.00 ( 99.06)
Test: [ 40/100]	Time  0.014 ( 0.019)	Loss 4.3389e-01 (4.9493e-01)	Acc@1  85.00 ( 84.46)	Acc@5  99.00 ( 99.10)
Test: [ 50/100]	Time  0.011 ( 0.018)	Loss 3.6173e-01 (4.9243e-01)	Acc@1  87.00 ( 84.41)	Acc@5 100.00 ( 99.10)
Test: [ 60/100]	Time  0.013 ( 0.018)	Loss 6.5875e-01 (4.9497e-01)	Acc@1  79.00 ( 84.39)	Acc@5 100.00 ( 99.16)
Test: [ 70/100]	Time  0.011 ( 0.017)	Loss 4.1631e-01 (4.9389e-01)	Acc@1  86.00 ( 84.39)	Acc@5  99.00 ( 99.17)
Test: [ 80/100]	Time  0.017 ( 0.017)	Loss 4.6804e-01 (4.8719e-01)	Acc@1  87.00 ( 84.48)	Acc@5  98.00 ( 99.19)
Test: [ 90/100]	Time  0.016 ( 0.017)	Loss 5.1891e-01 (4.8579e-01)	Acc@1  86.00 ( 84.60)	Acc@5 100.00 ( 99.16)
 * Acc@1 84.730 Acc@5 99.210
### epoch[25] execution time: 12.142797946929932
EPOCH 26
# current learning rate: 0.1
# Switched to train mode...
Epoch: [26][  0/391]	Time  0.189 ( 0.189)	Data  0.164 ( 0.164)	Loss 2.4866e-01 (2.4866e-01)	Acc@1  92.19 ( 92.19)	Acc@5  99.22 ( 99.22)
Epoch: [26][ 10/391]	Time  0.024 ( 0.040)	Data  0.001 ( 0.017)	Loss 3.6659e-01 (3.0238e-01)	Acc@1  87.50 ( 90.84)	Acc@5  99.22 ( 99.57)
Epoch: [26][ 20/391]	Time  0.032 ( 0.034)	Data  0.002 ( 0.010)	Loss 2.7783e-01 (3.0635e-01)	Acc@1  89.84 ( 90.10)	Acc@5 100.00 ( 99.55)
Epoch: [26][ 30/391]	Time  0.024 ( 0.031)	Data  0.001 ( 0.007)	Loss 2.5992e-01 (3.0120e-01)	Acc@1  90.62 ( 90.52)	Acc@5  99.22 ( 99.52)
Epoch: [26][ 40/391]	Time  0.031 ( 0.030)	Data  0.001 ( 0.006)	Loss 3.7158e-01 (3.0022e-01)	Acc@1  85.94 ( 90.53)	Acc@5 100.00 ( 99.58)
Epoch: [26][ 50/391]	Time  0.029 ( 0.029)	Data  0.005 ( 0.005)	Loss 3.0210e-01 (2.8964e-01)	Acc@1  86.72 ( 90.85)	Acc@5 100.00 ( 99.59)
Epoch: [26][ 60/391]	Time  0.025 ( 0.028)	Data  0.004 ( 0.005)	Loss 3.6231e-01 (3.0020e-01)	Acc@1  89.06 ( 90.71)	Acc@5 100.00 ( 99.56)
Epoch: [26][ 70/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.004)	Loss 2.3631e-01 (2.9980e-01)	Acc@1  92.19 ( 90.67)	Acc@5  99.22 ( 99.57)
Epoch: [26][ 80/391]	Time  0.028 ( 0.028)	Data  0.001 ( 0.004)	Loss 2.4437e-01 (2.9542e-01)	Acc@1  92.97 ( 90.78)	Acc@5 100.00 ( 99.60)
Epoch: [26][ 90/391]	Time  0.028 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.9460e-01 (2.9850e-01)	Acc@1  92.19 ( 90.64)	Acc@5 100.00 ( 99.61)
Epoch: [26][100/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.7205e-01 (2.9470e-01)	Acc@1  87.50 ( 90.75)	Acc@5 100.00 ( 99.61)
Epoch: [26][110/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.8334e-01 (2.9150e-01)	Acc@1  89.84 ( 90.79)	Acc@5  99.22 ( 99.62)
Epoch: [26][120/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.004)	Loss 5.1778e-01 (2.9533e-01)	Acc@1  80.47 ( 90.68)	Acc@5 100.00 ( 99.60)
Epoch: [26][130/391]	Time  0.034 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.1738e-01 (2.9607e-01)	Acc@1  87.50 ( 90.58)	Acc@5 100.00 ( 99.59)
Epoch: [26][140/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.8198e-01 (2.9843e-01)	Acc@1  89.84 ( 90.56)	Acc@5  98.44 ( 99.59)
Epoch: [26][150/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.0981e-01 (3.0005e-01)	Acc@1  94.53 ( 90.49)	Acc@5 100.00 ( 99.59)
Epoch: [26][160/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 2.3752e-01 (2.9954e-01)	Acc@1  90.62 ( 90.48)	Acc@5 100.00 ( 99.58)
Epoch: [26][170/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.3001e-01 (2.9904e-01)	Acc@1  89.06 ( 90.52)	Acc@5  99.22 ( 99.58)
Epoch: [26][180/391]	Time  0.023 ( 0.027)	Data  0.000 ( 0.003)	Loss 4.4463e-01 (3.0093e-01)	Acc@1  86.72 ( 90.47)	Acc@5  99.22 ( 99.58)
Epoch: [26][190/391]	Time  0.030 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.3281e-01 (3.0034e-01)	Acc@1  92.97 ( 90.51)	Acc@5 100.00 ( 99.59)
Epoch: [26][200/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.7459e-01 (3.0072e-01)	Acc@1  85.94 ( 90.48)	Acc@5  99.22 ( 99.59)
Epoch: [26][210/391]	Time  0.026 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.2531e-01 (2.9985e-01)	Acc@1  92.19 ( 90.48)	Acc@5 100.00 ( 99.59)
Epoch: [26][220/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.3299e-01 (2.9998e-01)	Acc@1  89.84 ( 90.48)	Acc@5  99.22 ( 99.58)
Epoch: [26][230/391]	Time  0.030 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.1058e-01 (2.9912e-01)	Acc@1  88.28 ( 90.49)	Acc@5 100.00 ( 99.59)
Epoch: [26][240/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.2060e-01 (2.9876e-01)	Acc@1  85.94 ( 90.47)	Acc@5  99.22 ( 99.59)
Epoch: [26][250/391]	Time  0.028 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.7654e-01 (2.9764e-01)	Acc@1  90.62 ( 90.53)	Acc@5 100.00 ( 99.60)
Epoch: [26][260/391]	Time  0.031 ( 0.026)	Data  0.005 ( 0.003)	Loss 2.5977e-01 (2.9708e-01)	Acc@1  89.84 ( 90.52)	Acc@5 100.00 ( 99.61)
Epoch: [26][270/391]	Time  0.023 ( 0.026)	Data  0.003 ( 0.003)	Loss 3.3314e-01 (3.0031e-01)	Acc@1  88.28 ( 90.43)	Acc@5  99.22 ( 99.60)
Epoch: [26][280/391]	Time  0.028 ( 0.026)	Data  0.003 ( 0.003)	Loss 5.2926e-01 (3.0389e-01)	Acc@1  82.03 ( 90.32)	Acc@5  98.44 ( 99.58)
Epoch: [26][290/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.0153e-01 (3.0527e-01)	Acc@1  91.41 ( 90.29)	Acc@5  98.44 ( 99.58)
Epoch: [26][300/391]	Time  0.022 ( 0.026)	Data  0.000 ( 0.003)	Loss 3.3148e-01 (3.0561e-01)	Acc@1  90.62 ( 90.26)	Acc@5  99.22 ( 99.58)
Epoch: [26][310/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.1251e-01 (3.0653e-01)	Acc@1  88.28 ( 90.21)	Acc@5  99.22 ( 99.59)
Epoch: [26][320/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.6936e-01 (3.0813e-01)	Acc@1  90.62 ( 90.14)	Acc@5 100.00 ( 99.58)
Epoch: [26][330/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.4352e-01 (3.0992e-01)	Acc@1  87.50 ( 90.06)	Acc@5  99.22 ( 99.58)
Epoch: [26][340/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1919e-01 (3.1150e-01)	Acc@1  92.19 ( 89.99)	Acc@5 100.00 ( 99.59)
Epoch: [26][350/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.8852e-01 (3.1138e-01)	Acc@1  86.72 ( 89.98)	Acc@5  99.22 ( 99.59)
Epoch: [26][360/391]	Time  0.036 ( 0.026)	Data  0.004 ( 0.003)	Loss 2.8846e-01 (3.1255e-01)	Acc@1  89.84 ( 89.93)	Acc@5 100.00 ( 99.58)
Epoch: [26][370/391]	Time  0.028 ( 0.026)	Data  0.000 ( 0.003)	Loss 3.4052e-01 (3.1170e-01)	Acc@1  91.41 ( 89.97)	Acc@5  97.66 ( 99.58)
Epoch: [26][380/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.6981e-01 (3.1152e-01)	Acc@1  88.28 ( 89.99)	Acc@5 100.00 ( 99.59)
Epoch: [26][390/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.0775e-01 (3.1159e-01)	Acc@1  93.75 ( 89.98)	Acc@5  98.75 ( 99.59)
## e[26] optimizer.zero_grad (sum) time: 0.1104893684387207
## e[26]       loss.backward (sum) time: 2.233391761779785
## e[26]      optimizer.step (sum) time: 0.8794879913330078
## epoch[26] training(only) time: 10.399628162384033
# Switched to evaluate mode...
Test: [  0/100]	Time  0.162 ( 0.162)	Loss 6.7361e-01 (6.7361e-01)	Acc@1  79.00 ( 79.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.011 ( 0.026)	Loss 6.1253e-01 (4.5379e-01)	Acc@1  87.00 ( 85.45)	Acc@5  98.00 ( 99.45)
Test: [ 20/100]	Time  0.011 ( 0.021)	Loss 6.2199e-01 (4.7152e-01)	Acc@1  78.00 ( 84.86)	Acc@5 100.00 ( 99.10)
Test: [ 30/100]	Time  0.016 ( 0.019)	Loss 4.5257e-01 (4.7605e-01)	Acc@1  83.00 ( 84.58)	Acc@5  98.00 ( 99.13)
Test: [ 40/100]	Time  0.013 ( 0.018)	Loss 5.1235e-01 (4.7564e-01)	Acc@1  83.00 ( 84.95)	Acc@5  98.00 ( 99.07)
Test: [ 50/100]	Time  0.024 ( 0.017)	Loss 3.0117e-01 (4.6611e-01)	Acc@1  88.00 ( 85.45)	Acc@5  99.00 ( 99.06)
Test: [ 60/100]	Time  0.011 ( 0.016)	Loss 4.6788e-01 (4.7240e-01)	Acc@1  82.00 ( 85.20)	Acc@5 100.00 ( 99.07)
Test: [ 70/100]	Time  0.014 ( 0.016)	Loss 6.0110e-01 (4.7515e-01)	Acc@1  86.00 ( 85.38)	Acc@5  98.00 ( 99.06)
Test: [ 80/100]	Time  0.029 ( 0.016)	Loss 3.9214e-01 (4.7413e-01)	Acc@1  87.00 ( 85.49)	Acc@5 100.00 ( 99.10)
Test: [ 90/100]	Time  0.012 ( 0.016)	Loss 3.4072e-01 (4.7234e-01)	Acc@1  86.00 ( 85.46)	Acc@5 100.00 ( 99.11)
 * Acc@1 85.650 Acc@5 99.130
### epoch[26] execution time: 12.120509147644043
EPOCH 27
# current learning rate: 0.1
# Switched to train mode...
Epoch: [27][  0/391]	Time  0.201 ( 0.201)	Data  0.172 ( 0.172)	Loss 2.2821e-01 (2.2821e-01)	Acc@1  90.62 ( 90.62)	Acc@5 100.00 (100.00)
Epoch: [27][ 10/391]	Time  0.023 ( 0.042)	Data  0.001 ( 0.017)	Loss 3.7359e-01 (3.1246e-01)	Acc@1  86.72 ( 89.84)	Acc@5  99.22 ( 99.36)
Epoch: [27][ 20/391]	Time  0.023 ( 0.034)	Data  0.002 ( 0.010)	Loss 2.6680e-01 (3.0066e-01)	Acc@1  91.41 ( 90.33)	Acc@5 100.00 ( 99.48)
Epoch: [27][ 30/391]	Time  0.024 ( 0.031)	Data  0.001 ( 0.008)	Loss 3.1353e-01 (3.0109e-01)	Acc@1  87.50 ( 90.02)	Acc@5 100.00 ( 99.52)
Epoch: [27][ 40/391]	Time  0.023 ( 0.030)	Data  0.001 ( 0.006)	Loss 3.7275e-01 (2.9976e-01)	Acc@1  87.50 ( 90.22)	Acc@5  99.22 ( 99.52)
Epoch: [27][ 50/391]	Time  0.025 ( 0.029)	Data  0.001 ( 0.005)	Loss 2.2567e-01 (2.9873e-01)	Acc@1  92.19 ( 90.36)	Acc@5 100.00 ( 99.49)
Epoch: [27][ 60/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.005)	Loss 3.0290e-01 (2.9466e-01)	Acc@1  90.62 ( 90.43)	Acc@5 100.00 ( 99.50)
Epoch: [27][ 70/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 3.3725e-01 (2.9260e-01)	Acc@1  86.72 ( 90.40)	Acc@5 100.00 ( 99.56)
Epoch: [27][ 80/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 2.7914e-01 (2.9298e-01)	Acc@1  86.72 ( 90.40)	Acc@5 100.00 ( 99.59)
Epoch: [27][ 90/391]	Time  0.027 ( 0.028)	Data  0.001 ( 0.004)	Loss 2.1861e-01 (2.9047e-01)	Acc@1  92.19 ( 90.48)	Acc@5 100.00 ( 99.59)
Epoch: [27][100/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.2975e-01 (2.8735e-01)	Acc@1  91.41 ( 90.62)	Acc@5 100.00 ( 99.58)
Epoch: [27][110/391]	Time  0.028 ( 0.027)	Data  0.000 ( 0.004)	Loss 2.3774e-01 (2.8826e-01)	Acc@1  92.19 ( 90.61)	Acc@5  99.22 ( 99.59)
Epoch: [27][120/391]	Time  0.025 ( 0.027)	Data  0.000 ( 0.003)	Loss 5.3544e-01 (2.9316e-01)	Acc@1  83.59 ( 90.53)	Acc@5  98.44 ( 99.57)
Epoch: [27][130/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.6088e-01 (2.9381e-01)	Acc@1  85.16 ( 90.40)	Acc@5  99.22 ( 99.55)
Epoch: [27][140/391]	Time  0.035 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.8981e-01 (2.9420e-01)	Acc@1  92.97 ( 90.33)	Acc@5 100.00 ( 99.56)
Epoch: [27][150/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.8336e-01 (2.9437e-01)	Acc@1  90.62 ( 90.26)	Acc@5 100.00 ( 99.56)
Epoch: [27][160/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.3286e-01 (2.9642e-01)	Acc@1  86.72 ( 90.27)	Acc@5  98.44 ( 99.55)
Epoch: [27][170/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.1041e-01 (2.9939e-01)	Acc@1  87.50 ( 90.11)	Acc@5 100.00 ( 99.57)
Epoch: [27][180/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.7596e-01 (2.9849e-01)	Acc@1  89.84 ( 90.14)	Acc@5 100.00 ( 99.57)
Epoch: [27][190/391]	Time  0.026 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.6834e-01 (2.9788e-01)	Acc@1  89.06 ( 90.18)	Acc@5 100.00 ( 99.58)
Epoch: [27][200/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.2379e-01 (2.9736e-01)	Acc@1  89.06 ( 90.22)	Acc@5  99.22 ( 99.58)
Epoch: [27][210/391]	Time  0.023 ( 0.027)	Data  0.003 ( 0.003)	Loss 3.4358e-01 (2.9909e-01)	Acc@1  89.06 ( 90.13)	Acc@5 100.00 ( 99.60)
Epoch: [27][220/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.9298e-01 (2.9986e-01)	Acc@1  83.59 ( 90.07)	Acc@5 100.00 ( 99.59)
Epoch: [27][230/391]	Time  0.032 ( 0.027)	Data  0.000 ( 0.003)	Loss 2.9047e-01 (3.0191e-01)	Acc@1  88.28 ( 90.01)	Acc@5 100.00 ( 99.59)
Epoch: [27][240/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.8239e-01 (3.0284e-01)	Acc@1  90.62 ( 89.99)	Acc@5 100.00 ( 99.60)
Epoch: [27][250/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.4093e-01 (3.0421e-01)	Acc@1  87.50 ( 89.96)	Acc@5 100.00 ( 99.62)
Epoch: [27][260/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.5975e-01 (3.0444e-01)	Acc@1  86.72 ( 89.92)	Acc@5  99.22 ( 99.62)
Epoch: [27][270/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9854e-01 (3.0418e-01)	Acc@1  94.53 ( 89.96)	Acc@5 100.00 ( 99.61)
Epoch: [27][280/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3918e-01 (3.0447e-01)	Acc@1  92.19 ( 89.95)	Acc@5 100.00 ( 99.61)
Epoch: [27][290/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.8952e-01 (3.0553e-01)	Acc@1  89.84 ( 89.93)	Acc@5  99.22 ( 99.61)
Epoch: [27][300/391]	Time  0.036 ( 0.026)	Data  0.000 ( 0.003)	Loss 3.8383e-01 (3.0575e-01)	Acc@1  85.16 ( 89.90)	Acc@5  98.44 ( 99.61)
Epoch: [27][310/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3341e-01 (3.0595e-01)	Acc@1  87.50 ( 89.90)	Acc@5  98.44 ( 99.60)
Epoch: [27][320/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.0392e-01 (3.0430e-01)	Acc@1  88.28 ( 89.94)	Acc@5  99.22 ( 99.61)
Epoch: [27][330/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3787e-01 (3.0380e-01)	Acc@1  89.84 ( 89.95)	Acc@5 100.00 ( 99.61)
Epoch: [27][340/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.1120e-01 (3.0461e-01)	Acc@1  90.62 ( 89.92)	Acc@5  99.22 ( 99.61)
Epoch: [27][350/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.9108e-01 (3.0460e-01)	Acc@1  89.84 ( 89.90)	Acc@5 100.00 ( 99.61)
Epoch: [27][360/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.0152e-01 (3.0518e-01)	Acc@1  92.97 ( 89.88)	Acc@5  96.88 ( 99.60)
Epoch: [27][370/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.7685e-01 (3.0479e-01)	Acc@1  88.28 ( 89.91)	Acc@5 100.00 ( 99.60)
Epoch: [27][380/391]	Time  0.024 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.7944e-01 (3.0487e-01)	Acc@1  92.97 ( 89.90)	Acc@5 100.00 ( 99.60)
Epoch: [27][390/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5174e-01 (3.0513e-01)	Acc@1  90.00 ( 89.92)	Acc@5 100.00 ( 99.60)
## e[27] optimizer.zero_grad (sum) time: 0.11152887344360352
## e[27]       loss.backward (sum) time: 2.2396368980407715
## e[27]      optimizer.step (sum) time: 0.876596212387085
## epoch[27] training(only) time: 10.43474268913269
# Switched to evaluate mode...
Test: [  0/100]	Time  0.175 ( 0.175)	Loss 4.7825e-01 (4.7825e-01)	Acc@1  90.00 ( 90.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.011 ( 0.029)	Loss 5.7465e-01 (4.9289e-01)	Acc@1  86.00 ( 84.91)	Acc@5 100.00 ( 99.45)
Test: [ 20/100]	Time  0.015 ( 0.021)	Loss 4.9448e-01 (5.0599e-01)	Acc@1  84.00 ( 84.48)	Acc@5 100.00 ( 99.14)
Test: [ 30/100]	Time  0.012 ( 0.019)	Loss 5.0337e-01 (5.2167e-01)	Acc@1  82.00 ( 83.74)	Acc@5  99.00 ( 98.97)
Test: [ 40/100]	Time  0.012 ( 0.018)	Loss 6.7071e-01 (5.1869e-01)	Acc@1  87.00 ( 83.80)	Acc@5  96.00 ( 98.93)
Test: [ 50/100]	Time  0.015 ( 0.017)	Loss 3.6909e-01 (5.1746e-01)	Acc@1  87.00 ( 83.80)	Acc@5  98.00 ( 98.90)
Test: [ 60/100]	Time  0.011 ( 0.017)	Loss 4.4909e-01 (5.1869e-01)	Acc@1  87.00 ( 83.69)	Acc@5 100.00 ( 98.95)
Test: [ 70/100]	Time  0.011 ( 0.017)	Loss 8.0470e-01 (5.2050e-01)	Acc@1  82.00 ( 83.68)	Acc@5  97.00 ( 98.92)
Test: [ 80/100]	Time  0.012 ( 0.016)	Loss 5.0975e-01 (5.1496e-01)	Acc@1  80.00 ( 83.58)	Acc@5 100.00 ( 98.96)
Test: [ 90/100]	Time  0.010 ( 0.016)	Loss 4.3048e-01 (5.1462e-01)	Acc@1  88.00 ( 83.59)	Acc@5 100.00 ( 99.00)
 * Acc@1 83.810 Acc@5 99.010
### epoch[27] execution time: 12.155162334442139
EPOCH 28
# current learning rate: 0.1
# Switched to train mode...
Epoch: [28][  0/391]	Time  0.195 ( 0.195)	Data  0.175 ( 0.175)	Loss 2.7906e-01 (2.7906e-01)	Acc@1  91.41 ( 91.41)	Acc@5  99.22 ( 99.22)
Epoch: [28][ 10/391]	Time  0.025 ( 0.041)	Data  0.002 ( 0.018)	Loss 2.8141e-01 (3.1657e-01)	Acc@1  90.62 ( 89.35)	Acc@5 100.00 ( 99.79)
Epoch: [28][ 20/391]	Time  0.023 ( 0.033)	Data  0.001 ( 0.010)	Loss 2.9185e-01 (3.2799e-01)	Acc@1  89.84 ( 89.25)	Acc@5 100.00 ( 99.48)
Epoch: [28][ 30/391]	Time  0.023 ( 0.031)	Data  0.001 ( 0.008)	Loss 1.8575e-01 (3.1411e-01)	Acc@1  95.31 ( 89.79)	Acc@5 100.00 ( 99.52)
Epoch: [28][ 40/391]	Time  0.025 ( 0.030)	Data  0.002 ( 0.007)	Loss 2.0396e-01 (3.1073e-01)	Acc@1  92.19 ( 89.92)	Acc@5 100.00 ( 99.45)
Epoch: [28][ 50/391]	Time  0.023 ( 0.029)	Data  0.001 ( 0.006)	Loss 2.0632e-01 (3.0921e-01)	Acc@1  92.97 ( 89.94)	Acc@5 100.00 ( 99.49)
Epoch: [28][ 60/391]	Time  0.028 ( 0.028)	Data  0.001 ( 0.005)	Loss 3.3036e-01 (3.0526e-01)	Acc@1  88.28 ( 89.96)	Acc@5  99.22 ( 99.54)
Epoch: [28][ 70/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.005)	Loss 3.2760e-01 (3.0272e-01)	Acc@1  90.62 ( 90.11)	Acc@5  99.22 ( 99.54)
Epoch: [28][ 80/391]	Time  0.031 ( 0.028)	Data  0.005 ( 0.004)	Loss 1.9264e-01 (2.9870e-01)	Acc@1  92.19 ( 90.21)	Acc@5 100.00 ( 99.56)
Epoch: [28][ 90/391]	Time  0.027 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.5765e-01 (2.9934e-01)	Acc@1  90.62 ( 90.18)	Acc@5 100.00 ( 99.54)
Epoch: [28][100/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.4134e-01 (2.9684e-01)	Acc@1  92.97 ( 90.22)	Acc@5  99.22 ( 99.54)
Epoch: [28][110/391]	Time  0.027 ( 0.027)	Data  0.003 ( 0.004)	Loss 2.0638e-01 (2.9611e-01)	Acc@1  92.97 ( 90.25)	Acc@5 100.00 ( 99.56)
Epoch: [28][120/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.7969e-01 (2.9804e-01)	Acc@1  88.28 ( 90.15)	Acc@5 100.00 ( 99.58)
Epoch: [28][130/391]	Time  0.033 ( 0.027)	Data  0.002 ( 0.004)	Loss 4.1231e-01 (2.9959e-01)	Acc@1  85.94 ( 90.10)	Acc@5 100.00 ( 99.58)
Epoch: [28][140/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.7797e-01 (3.0005e-01)	Acc@1  88.28 ( 90.06)	Acc@5 100.00 ( 99.59)
Epoch: [28][150/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.0581e-01 (3.0142e-01)	Acc@1  90.62 ( 89.99)	Acc@5  99.22 ( 99.59)
Epoch: [28][160/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 3.2817e-01 (3.0100e-01)	Acc@1  89.06 ( 89.97)	Acc@5 100.00 ( 99.59)
Epoch: [28][170/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 4.0145e-01 (2.9906e-01)	Acc@1  89.06 ( 90.02)	Acc@5  98.44 ( 99.59)
Epoch: [28][180/391]	Time  0.032 ( 0.026)	Data  0.011 ( 0.003)	Loss 4.6555e-01 (2.9953e-01)	Acc@1  84.38 ( 90.03)	Acc@5 100.00 ( 99.59)
Epoch: [28][190/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.0846e-01 (2.9873e-01)	Acc@1  89.06 ( 90.04)	Acc@5 100.00 ( 99.59)
Epoch: [28][200/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.9951e-01 (2.9673e-01)	Acc@1  89.06 ( 90.12)	Acc@5  99.22 ( 99.60)
Epoch: [28][210/391]	Time  0.024 ( 0.026)	Data  0.004 ( 0.003)	Loss 4.3089e-01 (2.9555e-01)	Acc@1  86.72 ( 90.18)	Acc@5 100.00 ( 99.61)
Epoch: [28][220/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4632e-01 (2.9538e-01)	Acc@1  89.84 ( 90.17)	Acc@5 100.00 ( 99.62)
Epoch: [28][230/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.9551e-01 (2.9801e-01)	Acc@1  91.41 ( 90.10)	Acc@5 100.00 ( 99.61)
Epoch: [28][240/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5385e-01 (2.9819e-01)	Acc@1  89.84 ( 90.10)	Acc@5 100.00 ( 99.61)
Epoch: [28][250/391]	Time  0.040 ( 0.026)	Data  0.003 ( 0.003)	Loss 3.3421e-01 (2.9945e-01)	Acc@1  89.06 ( 90.07)	Acc@5 100.00 ( 99.60)
Epoch: [28][260/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.0507e-01 (3.0059e-01)	Acc@1  90.62 ( 90.01)	Acc@5  99.22 ( 99.58)
Epoch: [28][270/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.7014e-01 (3.0056e-01)	Acc@1  89.84 ( 90.03)	Acc@5  99.22 ( 99.58)
Epoch: [28][280/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.2820e-01 (3.0083e-01)	Acc@1  89.06 ( 90.04)	Acc@5  99.22 ( 99.57)
Epoch: [28][290/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.8628e-01 (3.0131e-01)	Acc@1  86.72 ( 90.04)	Acc@5 100.00 ( 99.58)
Epoch: [28][300/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2938e-01 (3.0135e-01)	Acc@1  91.41 ( 90.01)	Acc@5 100.00 ( 99.58)
Epoch: [28][310/391]	Time  0.030 ( 0.026)	Data  0.005 ( 0.003)	Loss 3.6303e-01 (3.0460e-01)	Acc@1  86.72 ( 89.91)	Acc@5 100.00 ( 99.58)
Epoch: [28][320/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2744e-01 (3.0541e-01)	Acc@1  92.97 ( 89.91)	Acc@5 100.00 ( 99.58)
Epoch: [28][330/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.5020e-01 (3.0584e-01)	Acc@1  89.84 ( 89.89)	Acc@5 100.00 ( 99.58)
Epoch: [28][340/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.9414e-01 (3.0611e-01)	Acc@1  90.62 ( 89.89)	Acc@5 100.00 ( 99.58)
Epoch: [28][350/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.6136e-01 (3.0627e-01)	Acc@1  87.50 ( 89.88)	Acc@5  99.22 ( 99.57)
Epoch: [28][360/391]	Time  0.031 ( 0.026)	Data  0.011 ( 0.003)	Loss 3.4344e-01 (3.0649e-01)	Acc@1  86.72 ( 89.83)	Acc@5 100.00 ( 99.58)
Epoch: [28][370/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.1661e-01 (3.0659e-01)	Acc@1  91.41 ( 89.84)	Acc@5  99.22 ( 99.59)
Epoch: [28][380/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.0482e-01 (3.0666e-01)	Acc@1  89.84 ( 89.82)	Acc@5 100.00 ( 99.59)
Epoch: [28][390/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3679e-01 (3.0587e-01)	Acc@1  90.00 ( 89.85)	Acc@5 100.00 ( 99.58)
## e[28] optimizer.zero_grad (sum) time: 0.11193680763244629
## e[28]       loss.backward (sum) time: 2.253782033920288
## e[28]      optimizer.step (sum) time: 0.8821430206298828
## epoch[28] training(only) time: 10.378675699234009
# Switched to evaluate mode...
Test: [  0/100]	Time  0.201 ( 0.201)	Loss 5.9530e-01 (5.9530e-01)	Acc@1  84.00 ( 84.00)	Acc@5  97.00 ( 97.00)
Test: [ 10/100]	Time  0.019 ( 0.031)	Loss 7.2064e-01 (5.1748e-01)	Acc@1  79.00 ( 83.64)	Acc@5  99.00 ( 99.27)
Test: [ 20/100]	Time  0.021 ( 0.023)	Loss 4.9854e-01 (4.9133e-01)	Acc@1  84.00 ( 84.57)	Acc@5 100.00 ( 99.19)
Test: [ 30/100]	Time  0.010 ( 0.020)	Loss 5.0888e-01 (5.0312e-01)	Acc@1  84.00 ( 84.68)	Acc@5  99.00 ( 99.10)
Test: [ 40/100]	Time  0.014 ( 0.019)	Loss 5.2767e-01 (4.9640e-01)	Acc@1  84.00 ( 84.66)	Acc@5  99.00 ( 99.07)
Test: [ 50/100]	Time  0.011 ( 0.018)	Loss 2.6570e-01 (4.8467e-01)	Acc@1  92.00 ( 85.29)	Acc@5 100.00 ( 99.04)
Test: [ 60/100]	Time  0.012 ( 0.017)	Loss 4.0386e-01 (4.8417e-01)	Acc@1  88.00 ( 85.31)	Acc@5 100.00 ( 99.02)
Test: [ 70/100]	Time  0.011 ( 0.017)	Loss 5.9836e-01 (4.8019e-01)	Acc@1  83.00 ( 85.52)	Acc@5  98.00 ( 99.06)
Test: [ 80/100]	Time  0.022 ( 0.017)	Loss 3.8566e-01 (4.7640e-01)	Acc@1  86.00 ( 85.49)	Acc@5 100.00 ( 99.06)
Test: [ 90/100]	Time  0.014 ( 0.016)	Loss 4.5386e-01 (4.7444e-01)	Acc@1  86.00 ( 85.48)	Acc@5  99.00 ( 99.05)
 * Acc@1 85.620 Acc@5 99.110
### epoch[28] execution time: 12.118001461029053
EPOCH 29
# current learning rate: 0.1
# Switched to train mode...
Epoch: [29][  0/391]	Time  0.202 ( 0.202)	Data  0.176 ( 0.176)	Loss 2.2403e-01 (2.2403e-01)	Acc@1  92.97 ( 92.97)	Acc@5  99.22 ( 99.22)
Epoch: [29][ 10/391]	Time  0.036 ( 0.043)	Data  0.001 ( 0.018)	Loss 3.1963e-01 (2.6537e-01)	Acc@1  88.28 ( 91.34)	Acc@5  99.22 ( 99.36)
Epoch: [29][ 20/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.011)	Loss 3.1178e-01 (2.7064e-01)	Acc@1  90.62 ( 91.11)	Acc@5  99.22 ( 99.44)
Epoch: [29][ 30/391]	Time  0.023 ( 0.032)	Data  0.001 ( 0.008)	Loss 3.0944e-01 (2.8911e-01)	Acc@1  93.75 ( 90.83)	Acc@5 100.00 ( 99.52)
Epoch: [29][ 40/391]	Time  0.030 ( 0.031)	Data  0.001 ( 0.007)	Loss 1.9586e-01 (2.8501e-01)	Acc@1  93.75 ( 90.82)	Acc@5 100.00 ( 99.56)
Epoch: [29][ 50/391]	Time  0.023 ( 0.030)	Data  0.001 ( 0.006)	Loss 2.0098e-01 (2.8740e-01)	Acc@1  93.75 ( 90.76)	Acc@5 100.00 ( 99.59)
Epoch: [29][ 60/391]	Time  0.023 ( 0.029)	Data  0.000 ( 0.005)	Loss 2.8091e-01 (2.8366e-01)	Acc@1  89.84 ( 90.84)	Acc@5 100.00 ( 99.60)
Epoch: [29][ 70/391]	Time  0.031 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.4205e-01 (2.8823e-01)	Acc@1  94.53 ( 90.76)	Acc@5 100.00 ( 99.60)
Epoch: [29][ 80/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.004)	Loss 4.2515e-01 (2.8747e-01)	Acc@1  85.94 ( 90.77)	Acc@5  99.22 ( 99.60)
Epoch: [29][ 90/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 2.5357e-01 (2.8740e-01)	Acc@1  92.97 ( 90.70)	Acc@5 100.00 ( 99.62)
Epoch: [29][100/391]	Time  0.029 ( 0.028)	Data  0.002 ( 0.004)	Loss 2.1059e-01 (2.8725e-01)	Acc@1  92.97 ( 90.68)	Acc@5 100.00 ( 99.64)
Epoch: [29][110/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.0707e-01 (2.8717e-01)	Acc@1  89.84 ( 90.72)	Acc@5 100.00 ( 99.61)
Epoch: [29][120/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.3352e-01 (2.8843e-01)	Acc@1  89.84 ( 90.72)	Acc@5  99.22 ( 99.59)
Epoch: [29][130/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.1862e-01 (2.8798e-01)	Acc@1  91.41 ( 90.66)	Acc@5 100.00 ( 99.56)
Epoch: [29][140/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.1052e-01 (2.8616e-01)	Acc@1  89.06 ( 90.71)	Acc@5 100.00 ( 99.57)
Epoch: [29][150/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.2076e-01 (2.8678e-01)	Acc@1  88.28 ( 90.70)	Acc@5  98.44 ( 99.58)
Epoch: [29][160/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.3139e-01 (2.8821e-01)	Acc@1  89.84 ( 90.72)	Acc@5  99.22 ( 99.57)
Epoch: [29][170/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.7522e-01 (2.9019e-01)	Acc@1  92.19 ( 90.67)	Acc@5  99.22 ( 99.56)
Epoch: [29][180/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.8832e-01 (2.8959e-01)	Acc@1  92.19 ( 90.69)	Acc@5  99.22 ( 99.56)
Epoch: [29][190/391]	Time  0.032 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.1526e-01 (2.9120e-01)	Acc@1  90.62 ( 90.62)	Acc@5  98.44 ( 99.55)
Epoch: [29][200/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.7229e-01 (2.9212e-01)	Acc@1  92.19 ( 90.59)	Acc@5 100.00 ( 99.55)
Epoch: [29][210/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.7799e-01 (2.9221e-01)	Acc@1  89.06 ( 90.61)	Acc@5 100.00 ( 99.56)
Epoch: [29][220/391]	Time  0.026 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.7180e-01 (2.9203e-01)	Acc@1  93.75 ( 90.60)	Acc@5 100.00 ( 99.56)
Epoch: [29][230/391]	Time  0.025 ( 0.027)	Data  0.005 ( 0.003)	Loss 4.7632e-01 (2.9301e-01)	Acc@1  82.81 ( 90.54)	Acc@5  99.22 ( 99.56)
Epoch: [29][240/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.8334e-01 (2.9674e-01)	Acc@1  89.84 ( 90.39)	Acc@5  99.22 ( 99.55)
Epoch: [29][250/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.3738e-01 (2.9777e-01)	Acc@1  92.97 ( 90.34)	Acc@5  99.22 ( 99.55)
Epoch: [29][260/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.8085e-01 (2.9742e-01)	Acc@1  94.53 ( 90.35)	Acc@5  99.22 ( 99.56)
Epoch: [29][270/391]	Time  0.025 ( 0.027)	Data  0.002 ( 0.003)	Loss 2.9411e-01 (2.9704e-01)	Acc@1  89.84 ( 90.37)	Acc@5  99.22 ( 99.55)
Epoch: [29][280/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.2653e-01 (2.9898e-01)	Acc@1  96.09 ( 90.33)	Acc@5 100.00 ( 99.55)
Epoch: [29][290/391]	Time  0.029 ( 0.027)	Data  0.003 ( 0.003)	Loss 3.6407e-01 (3.0031e-01)	Acc@1  88.28 ( 90.28)	Acc@5  98.44 ( 99.53)
Epoch: [29][300/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.4166e-01 (3.0050e-01)	Acc@1  89.06 ( 90.26)	Acc@5 100.00 ( 99.53)
Epoch: [29][310/391]	Time  0.025 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.6540e-01 (2.9902e-01)	Acc@1  90.62 ( 90.30)	Acc@5 100.00 ( 99.54)
Epoch: [29][320/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.7528e-01 (2.9799e-01)	Acc@1  90.62 ( 90.34)	Acc@5 100.00 ( 99.55)
Epoch: [29][330/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4131e-01 (2.9659e-01)	Acc@1  91.41 ( 90.39)	Acc@5 100.00 ( 99.56)
Epoch: [29][340/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5626e-01 (2.9813e-01)	Acc@1  90.62 ( 90.35)	Acc@5  99.22 ( 99.54)
Epoch: [29][350/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.9002e-01 (2.9889e-01)	Acc@1  89.06 ( 90.33)	Acc@5 100.00 ( 99.54)
Epoch: [29][360/391]	Time  0.024 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.4826e-01 (2.9897e-01)	Acc@1  92.19 ( 90.33)	Acc@5 100.00 ( 99.53)
Epoch: [29][370/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2959e-01 (2.9943e-01)	Acc@1  90.62 ( 90.33)	Acc@5  99.22 ( 99.53)
Epoch: [29][380/391]	Time  0.036 ( 0.026)	Data  0.003 ( 0.003)	Loss 2.1347e-01 (2.9909e-01)	Acc@1  92.19 ( 90.33)	Acc@5 100.00 ( 99.54)
Epoch: [29][390/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4231e-01 (3.0030e-01)	Acc@1  92.50 ( 90.32)	Acc@5  98.75 ( 99.53)
## e[29] optimizer.zero_grad (sum) time: 0.10915374755859375
## e[29]       loss.backward (sum) time: 2.2335448265075684
## e[29]      optimizer.step (sum) time: 0.8772220611572266
## epoch[29] training(only) time: 10.432780981063843
# Switched to evaluate mode...
Test: [  0/100]	Time  0.165 ( 0.165)	Loss 5.0223e-01 (5.0223e-01)	Acc@1  83.00 ( 83.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.022 ( 0.028)	Loss 4.7295e-01 (4.3862e-01)	Acc@1  87.00 ( 85.36)	Acc@5  98.00 ( 99.09)
Test: [ 20/100]	Time  0.011 ( 0.021)	Loss 4.2034e-01 (4.5451e-01)	Acc@1  84.00 ( 84.67)	Acc@5  99.00 ( 99.00)
Test: [ 30/100]	Time  0.010 ( 0.019)	Loss 6.0006e-01 (4.6976e-01)	Acc@1  75.00 ( 84.52)	Acc@5  98.00 ( 99.10)
Test: [ 40/100]	Time  0.011 ( 0.018)	Loss 4.9596e-01 (4.7191e-01)	Acc@1  87.00 ( 84.51)	Acc@5  99.00 ( 99.07)
Test: [ 50/100]	Time  0.013 ( 0.017)	Loss 2.8673e-01 (4.7319e-01)	Acc@1  88.00 ( 84.61)	Acc@5 100.00 ( 99.02)
Test: [ 60/100]	Time  0.011 ( 0.017)	Loss 4.7300e-01 (4.7878e-01)	Acc@1  86.00 ( 84.61)	Acc@5 100.00 ( 99.03)
Test: [ 70/100]	Time  0.013 ( 0.017)	Loss 6.1337e-01 (4.7548e-01)	Acc@1  84.00 ( 84.56)	Acc@5 100.00 ( 99.04)
Test: [ 80/100]	Time  0.010 ( 0.016)	Loss 4.2629e-01 (4.7117e-01)	Acc@1  88.00 ( 84.68)	Acc@5 100.00 ( 99.09)
Test: [ 90/100]	Time  0.019 ( 0.016)	Loss 3.3984e-01 (4.6820e-01)	Acc@1  88.00 ( 84.75)	Acc@5  99.00 ( 99.07)
 * Acc@1 84.840 Acc@5 99.100
### epoch[29] execution time: 12.132062911987305
EPOCH 30
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [30][  0/391]	Time  0.183 ( 0.183)	Data  0.164 ( 0.164)	Loss 2.0408e-01 (2.0408e-01)	Acc@1  92.97 ( 92.97)	Acc@5 100.00 (100.00)
Epoch: [30][ 10/391]	Time  0.025 ( 0.040)	Data  0.001 ( 0.017)	Loss 2.7880e-01 (2.7172e-01)	Acc@1  90.62 ( 90.98)	Acc@5 100.00 ( 99.50)
Epoch: [30][ 20/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.010)	Loss 1.6952e-01 (2.5232e-01)	Acc@1  93.75 ( 91.67)	Acc@5 100.00 ( 99.63)
Epoch: [30][ 30/391]	Time  0.024 ( 0.031)	Data  0.001 ( 0.007)	Loss 1.9567e-01 (2.4627e-01)	Acc@1  92.97 ( 91.83)	Acc@5 100.00 ( 99.72)
Epoch: [30][ 40/391]	Time  0.023 ( 0.029)	Data  0.002 ( 0.006)	Loss 2.5319e-01 (2.3689e-01)	Acc@1  88.28 ( 92.09)	Acc@5 100.00 ( 99.75)
Epoch: [30][ 50/391]	Time  0.024 ( 0.029)	Data  0.001 ( 0.005)	Loss 2.9656e-01 (2.3044e-01)	Acc@1  89.84 ( 92.42)	Acc@5  99.22 ( 99.77)
Epoch: [30][ 60/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.7080e-01 (2.2664e-01)	Acc@1  91.41 ( 92.46)	Acc@5  99.22 ( 99.78)
Epoch: [30][ 70/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.0930e-01 (2.1594e-01)	Acc@1  96.88 ( 92.74)	Acc@5 100.00 ( 99.78)
Epoch: [30][ 80/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 2.1602e-01 (2.1397e-01)	Acc@1  92.97 ( 92.75)	Acc@5  99.22 ( 99.76)
Epoch: [30][ 90/391]	Time  0.032 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.6960e-01 (2.1064e-01)	Acc@1  95.31 ( 92.83)	Acc@5 100.00 ( 99.78)
Epoch: [30][100/391]	Time  0.027 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.8830e-01 (2.0919e-01)	Acc@1  92.97 ( 92.94)	Acc@5 100.00 ( 99.78)
Epoch: [30][110/391]	Time  0.029 ( 0.027)	Data  0.005 ( 0.004)	Loss 1.0056e-01 (2.0456e-01)	Acc@1  96.88 ( 93.01)	Acc@5 100.00 ( 99.79)
Epoch: [30][120/391]	Time  0.032 ( 0.027)	Data  0.005 ( 0.004)	Loss 1.3565e-01 (2.0338e-01)	Acc@1  95.31 ( 93.01)	Acc@5 100.00 ( 99.80)
Epoch: [30][130/391]	Time  0.029 ( 0.027)	Data  0.004 ( 0.003)	Loss 1.0283e-01 (1.9884e-01)	Acc@1  96.88 ( 93.18)	Acc@5  99.22 ( 99.81)
Epoch: [30][140/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.9386e-01 (1.9675e-01)	Acc@1  92.19 ( 93.27)	Acc@5 100.00 ( 99.82)
Epoch: [30][150/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.8302e-01 (1.9552e-01)	Acc@1  92.19 ( 93.32)	Acc@5 100.00 ( 99.81)
Epoch: [30][160/391]	Time  0.037 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.4789e-01 (1.9342e-01)	Acc@1  93.75 ( 93.41)	Acc@5 100.00 ( 99.81)
Epoch: [30][170/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.2080e-01 (1.9214e-01)	Acc@1  93.75 ( 93.45)	Acc@5 100.00 ( 99.80)
Epoch: [30][180/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.3989e-01 (1.9130e-01)	Acc@1  95.31 ( 93.48)	Acc@5 100.00 ( 99.81)
Epoch: [30][190/391]	Time  0.032 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.9845e-01 (1.9161e-01)	Acc@1  93.75 ( 93.47)	Acc@5  99.22 ( 99.80)
Epoch: [30][200/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 1.2056e-01 (1.8913e-01)	Acc@1  97.66 ( 93.57)	Acc@5 100.00 ( 99.81)
Epoch: [30][210/391]	Time  0.025 ( 0.027)	Data  0.004 ( 0.003)	Loss 2.2660e-01 (1.8906e-01)	Acc@1  92.97 ( 93.58)	Acc@5 100.00 ( 99.80)
Epoch: [30][220/391]	Time  0.028 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.5281e-01 (1.8796e-01)	Acc@1  94.53 ( 93.60)	Acc@5 100.00 ( 99.80)
Epoch: [30][230/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.4945e-01 (1.8663e-01)	Acc@1  95.31 ( 93.67)	Acc@5 100.00 ( 99.81)
Epoch: [30][240/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.3348e-01 (1.8428e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 ( 99.81)
Epoch: [30][250/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6753e-01 (1.8334e-01)	Acc@1  93.75 ( 93.78)	Acc@5 100.00 ( 99.82)
Epoch: [30][260/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5708e-01 (1.8290e-01)	Acc@1  96.88 ( 93.80)	Acc@5 100.00 ( 99.81)
Epoch: [30][270/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9164e-01 (1.8162e-01)	Acc@1  95.31 ( 93.87)	Acc@5 100.00 ( 99.82)
Epoch: [30][280/391]	Time  0.023 ( 0.026)	Data  0.003 ( 0.003)	Loss 1.2957e-01 (1.8057e-01)	Acc@1  96.88 ( 93.90)	Acc@5 100.00 ( 99.82)
Epoch: [30][290/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5940e-01 (1.7980e-01)	Acc@1  93.75 ( 93.94)	Acc@5 100.00 ( 99.83)
Epoch: [30][300/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6215e-01 (1.7954e-01)	Acc@1  93.75 ( 93.95)	Acc@5 100.00 ( 99.83)
Epoch: [30][310/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4394e-01 (1.7774e-01)	Acc@1  94.53 ( 94.01)	Acc@5 100.00 ( 99.83)
Epoch: [30][320/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.8186e-02 (1.7778e-01)	Acc@1  96.88 ( 94.01)	Acc@5 100.00 ( 99.83)
Epoch: [30][330/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6383e-01 (1.7708e-01)	Acc@1  95.31 ( 94.05)	Acc@5 100.00 ( 99.83)
Epoch: [30][340/391]	Time  0.029 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.5111e-01 (1.7657e-01)	Acc@1  95.31 ( 94.06)	Acc@5 100.00 ( 99.84)
Epoch: [30][350/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8965e-01 (1.7550e-01)	Acc@1  93.75 ( 94.11)	Acc@5 100.00 ( 99.84)
Epoch: [30][360/391]	Time  0.024 ( 0.026)	Data  0.003 ( 0.003)	Loss 1.3333e-01 (1.7552e-01)	Acc@1  93.75 ( 94.10)	Acc@5 100.00 ( 99.84)
Epoch: [30][370/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9025e-01 (1.7559e-01)	Acc@1  91.41 ( 94.10)	Acc@5  99.22 ( 99.83)
Epoch: [30][380/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4948e-01 (1.7538e-01)	Acc@1  94.53 ( 94.14)	Acc@5 100.00 ( 99.84)
Epoch: [30][390/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.2758e-01 (1.7586e-01)	Acc@1  90.00 ( 94.11)	Acc@5 100.00 ( 99.84)
## e[30] optimizer.zero_grad (sum) time: 0.10926151275634766
## e[30]       loss.backward (sum) time: 2.227207899093628
## e[30]      optimizer.step (sum) time: 0.8691380023956299
## epoch[30] training(only) time: 10.409472465515137
# Switched to evaluate mode...
Test: [  0/100]	Time  0.164 ( 0.164)	Loss 2.5521e-01 (2.5521e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.027 ( 0.028)	Loss 3.7770e-01 (3.0718e-01)	Acc@1  88.00 ( 89.36)	Acc@5  99.00 ( 99.55)
Test: [ 20/100]	Time  0.020 ( 0.022)	Loss 3.6349e-01 (3.2348e-01)	Acc@1  86.00 ( 89.00)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.011 ( 0.019)	Loss 3.7072e-01 (3.4289e-01)	Acc@1  85.00 ( 89.19)	Acc@5  99.00 ( 99.58)
Test: [ 40/100]	Time  0.016 ( 0.018)	Loss 4.4528e-01 (3.4947e-01)	Acc@1  89.00 ( 89.24)	Acc@5  99.00 ( 99.59)
Test: [ 50/100]	Time  0.011 ( 0.017)	Loss 1.7707e-01 (3.4430e-01)	Acc@1  96.00 ( 89.51)	Acc@5 100.00 ( 99.57)
Test: [ 60/100]	Time  0.010 ( 0.017)	Loss 3.1775e-01 (3.4087e-01)	Acc@1  89.00 ( 89.61)	Acc@5 100.00 ( 99.59)
Test: [ 70/100]	Time  0.020 ( 0.017)	Loss 4.3942e-01 (3.3685e-01)	Acc@1  88.00 ( 89.68)	Acc@5 100.00 ( 99.59)
Test: [ 80/100]	Time  0.016 ( 0.016)	Loss 3.0371e-01 (3.3198e-01)	Acc@1  92.00 ( 89.77)	Acc@5  99.00 ( 99.59)
Test: [ 90/100]	Time  0.011 ( 0.016)	Loss 2.3478e-01 (3.2842e-01)	Acc@1  95.00 ( 89.87)	Acc@5 100.00 ( 99.62)
 * Acc@1 89.930 Acc@5 99.630
### epoch[30] execution time: 12.103303670883179
EPOCH 31
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [31][  0/391]	Time  0.195 ( 0.195)	Data  0.174 ( 0.174)	Loss 2.0019e-01 (2.0019e-01)	Acc@1  93.75 ( 93.75)	Acc@5  99.22 ( 99.22)
Epoch: [31][ 10/391]	Time  0.024 ( 0.041)	Data  0.001 ( 0.018)	Loss 1.5982e-01 (1.4673e-01)	Acc@1  94.53 ( 95.10)	Acc@5 100.00 ( 99.79)
Epoch: [31][ 20/391]	Time  0.027 ( 0.034)	Data  0.001 ( 0.010)	Loss 1.1020e-01 (1.4652e-01)	Acc@1  96.88 ( 95.16)	Acc@5 100.00 ( 99.81)
Epoch: [31][ 30/391]	Time  0.030 ( 0.031)	Data  0.000 ( 0.008)	Loss 1.2463e-01 (1.3645e-01)	Acc@1  96.09 ( 95.56)	Acc@5 100.00 ( 99.87)
Epoch: [31][ 40/391]	Time  0.023 ( 0.030)	Data  0.001 ( 0.006)	Loss 2.2574e-01 (1.4340e-01)	Acc@1  93.75 ( 95.52)	Acc@5 100.00 ( 99.89)
Epoch: [31][ 50/391]	Time  0.037 ( 0.029)	Data  0.001 ( 0.005)	Loss 9.1441e-02 (1.4302e-01)	Acc@1  96.09 ( 95.54)	Acc@5 100.00 ( 99.89)
Epoch: [31][ 60/391]	Time  0.031 ( 0.029)	Data  0.001 ( 0.005)	Loss 1.8241e-01 (1.4583e-01)	Acc@1  94.53 ( 95.36)	Acc@5 100.00 ( 99.88)
Epoch: [31][ 70/391]	Time  0.024 ( 0.028)	Data  0.004 ( 0.005)	Loss 1.6041e-01 (1.4729e-01)	Acc@1  93.75 ( 95.24)	Acc@5 100.00 ( 99.89)
Epoch: [31][ 80/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.1432e-01 (1.4621e-01)	Acc@1  96.88 ( 95.24)	Acc@5  99.22 ( 99.87)
Epoch: [31][ 90/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.9216e-01 (1.4790e-01)	Acc@1  93.75 ( 95.19)	Acc@5  99.22 ( 99.86)
Epoch: [31][100/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.5302e-01 (1.4852e-01)	Acc@1  93.75 ( 95.20)	Acc@5 100.00 ( 99.87)
Epoch: [31][110/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.7721e-01 (1.4723e-01)	Acc@1  94.53 ( 95.23)	Acc@5 100.00 ( 99.87)
Epoch: [31][120/391]	Time  0.028 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.5919e-01 (1.4558e-01)	Acc@1  95.31 ( 95.27)	Acc@5  99.22 ( 99.86)
Epoch: [31][130/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.1907e-01 (1.4542e-01)	Acc@1  95.31 ( 95.23)	Acc@5 100.00 ( 99.87)
Epoch: [31][140/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.5811e-01 (1.4505e-01)	Acc@1  95.31 ( 95.25)	Acc@5 100.00 ( 99.88)
Epoch: [31][150/391]	Time  0.036 ( 0.027)	Data  0.011 ( 0.003)	Loss 1.1390e-01 (1.4431e-01)	Acc@1  96.09 ( 95.28)	Acc@5 100.00 ( 99.88)
Epoch: [31][160/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.3994e-01 (1.4300e-01)	Acc@1  95.31 ( 95.33)	Acc@5 100.00 ( 99.89)
Epoch: [31][170/391]	Time  0.030 ( 0.027)	Data  0.002 ( 0.003)	Loss 1.8774e-01 (1.4500e-01)	Acc@1  95.31 ( 95.28)	Acc@5  99.22 ( 99.88)
Epoch: [31][180/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 9.3188e-02 (1.4555e-01)	Acc@1  96.88 ( 95.27)	Acc@5 100.00 ( 99.88)
Epoch: [31][190/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 9.3425e-02 (1.4570e-01)	Acc@1  96.09 ( 95.23)	Acc@5 100.00 ( 99.89)
Epoch: [31][200/391]	Time  0.024 ( 0.027)	Data  0.002 ( 0.003)	Loss 1.7278e-01 (1.4650e-01)	Acc@1  96.09 ( 95.25)	Acc@5 100.00 ( 99.89)
Epoch: [31][210/391]	Time  0.024 ( 0.027)	Data  0.002 ( 0.003)	Loss 1.3087e-01 (1.4587e-01)	Acc@1  95.31 ( 95.25)	Acc@5 100.00 ( 99.89)
Epoch: [31][220/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.7433e-01 (1.4511e-01)	Acc@1  93.75 ( 95.26)	Acc@5 100.00 ( 99.90)
Epoch: [31][230/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.3470e-01 (1.4478e-01)	Acc@1  96.09 ( 95.28)	Acc@5 100.00 ( 99.90)
Epoch: [31][240/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.7418e-01 (1.4506e-01)	Acc@1  91.41 ( 95.24)	Acc@5 100.00 ( 99.90)
Epoch: [31][250/391]	Time  0.026 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.0537e-01 (1.4498e-01)	Acc@1  94.53 ( 95.27)	Acc@5 100.00 ( 99.90)
Epoch: [31][260/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6429e-01 (1.4514e-01)	Acc@1  94.53 ( 95.24)	Acc@5 100.00 ( 99.90)
Epoch: [31][270/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.9190e-02 (1.4471e-01)	Acc@1  97.66 ( 95.25)	Acc@5 100.00 ( 99.90)
Epoch: [31][280/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3070e-01 (1.4459e-01)	Acc@1  95.31 ( 95.26)	Acc@5 100.00 ( 99.90)
Epoch: [31][290/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.2511e-02 (1.4470e-01)	Acc@1  98.44 ( 95.26)	Acc@5 100.00 ( 99.90)
Epoch: [31][300/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0834e-01 (1.4445e-01)	Acc@1  96.09 ( 95.27)	Acc@5 100.00 ( 99.90)
Epoch: [31][310/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5445e-01 (1.4380e-01)	Acc@1  94.53 ( 95.29)	Acc@5 100.00 ( 99.90)
Epoch: [31][320/391]	Time  0.036 ( 0.026)	Data  0.011 ( 0.003)	Loss 1.0998e-01 (1.4296e-01)	Acc@1  96.09 ( 95.32)	Acc@5 100.00 ( 99.90)
Epoch: [31][330/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3487e-01 (1.4227e-01)	Acc@1  94.53 ( 95.35)	Acc@5 100.00 ( 99.90)
Epoch: [31][340/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.9168e-02 (1.4206e-01)	Acc@1  97.66 ( 95.35)	Acc@5 100.00 ( 99.90)
Epoch: [31][350/391]	Time  0.024 ( 0.026)	Data  0.000 ( 0.003)	Loss 8.0567e-02 (1.4207e-01)	Acc@1  98.44 ( 95.34)	Acc@5 100.00 ( 99.90)
Epoch: [31][360/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.1636e-02 (1.4250e-01)	Acc@1  96.09 ( 95.32)	Acc@5 100.00 ( 99.90)
Epoch: [31][370/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0732e-01 (1.4272e-01)	Acc@1  97.66 ( 95.31)	Acc@5  99.22 ( 99.89)
Epoch: [31][380/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.4832e-02 (1.4232e-01)	Acc@1  96.88 ( 95.32)	Acc@5 100.00 ( 99.90)
Epoch: [31][390/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2174e-01 (1.4244e-01)	Acc@1  92.50 ( 95.32)	Acc@5  98.75 ( 99.89)
## e[31] optimizer.zero_grad (sum) time: 0.10995841026306152
## e[31]       loss.backward (sum) time: 2.2479732036590576
## e[31]      optimizer.step (sum) time: 0.8555727005004883
## epoch[31] training(only) time: 10.457888126373291
# Switched to evaluate mode...
Test: [  0/100]	Time  0.156 ( 0.156)	Loss 2.4788e-01 (2.4788e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.015 ( 0.027)	Loss 2.8703e-01 (2.9564e-01)	Acc@1  91.00 ( 90.27)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.015 ( 0.020)	Loss 3.4354e-01 (3.1643e-01)	Acc@1  89.00 ( 89.95)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.012 ( 0.018)	Loss 3.3086e-01 (3.4172e-01)	Acc@1  88.00 ( 89.65)	Acc@5  99.00 ( 99.55)
Test: [ 40/100]	Time  0.012 ( 0.017)	Loss 4.3985e-01 (3.4813e-01)	Acc@1  88.00 ( 89.73)	Acc@5  99.00 ( 99.56)
Test: [ 50/100]	Time  0.011 ( 0.017)	Loss 1.9776e-01 (3.4528e-01)	Acc@1  94.00 ( 89.80)	Acc@5  99.00 ( 99.53)
Test: [ 60/100]	Time  0.011 ( 0.016)	Loss 3.3403e-01 (3.4324e-01)	Acc@1  90.00 ( 89.79)	Acc@5 100.00 ( 99.56)
Test: [ 70/100]	Time  0.012 ( 0.016)	Loss 4.7726e-01 (3.3829e-01)	Acc@1  87.00 ( 89.87)	Acc@5  98.00 ( 99.54)
Test: [ 80/100]	Time  0.012 ( 0.016)	Loss 3.0665e-01 (3.3223e-01)	Acc@1  90.00 ( 89.93)	Acc@5 100.00 ( 99.57)
Test: [ 90/100]	Time  0.010 ( 0.016)	Loss 2.1739e-01 (3.2835e-01)	Acc@1  94.00 ( 89.96)	Acc@5 100.00 ( 99.58)
 * Acc@1 90.070 Acc@5 99.600
### epoch[31] execution time: 12.186111450195312
EPOCH 32
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [32][  0/391]	Time  0.190 ( 0.190)	Data  0.167 ( 0.167)	Loss 9.9962e-02 (9.9962e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [32][ 10/391]	Time  0.024 ( 0.041)	Data  0.001 ( 0.017)	Loss 1.9213e-01 (1.1126e-01)	Acc@1  93.75 ( 96.16)	Acc@5 100.00 (100.00)
Epoch: [32][ 20/391]	Time  0.022 ( 0.033)	Data  0.001 ( 0.010)	Loss 2.4980e-01 (1.1713e-01)	Acc@1  95.31 ( 96.06)	Acc@5  99.22 ( 99.93)
Epoch: [32][ 30/391]	Time  0.024 ( 0.031)	Data  0.001 ( 0.007)	Loss 1.1259e-01 (1.2742e-01)	Acc@1  98.44 ( 96.04)	Acc@5 100.00 ( 99.95)
Epoch: [32][ 40/391]	Time  0.024 ( 0.029)	Data  0.001 ( 0.006)	Loss 1.8104e-01 (1.2675e-01)	Acc@1  93.75 ( 96.02)	Acc@5 100.00 ( 99.96)
Epoch: [32][ 50/391]	Time  0.023 ( 0.029)	Data  0.001 ( 0.005)	Loss 1.2301e-01 (1.2813e-01)	Acc@1  94.53 ( 95.93)	Acc@5 100.00 ( 99.97)
Epoch: [32][ 60/391]	Time  0.022 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.2195e-01 (1.3109e-01)	Acc@1  95.31 ( 95.77)	Acc@5 100.00 ( 99.94)
Epoch: [32][ 70/391]	Time  0.023 ( 0.028)	Data  0.002 ( 0.004)	Loss 1.2818e-01 (1.3228e-01)	Acc@1  95.31 ( 95.64)	Acc@5 100.00 ( 99.94)
Epoch: [32][ 80/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.4507e-01 (1.2963e-01)	Acc@1  95.31 ( 95.71)	Acc@5 100.00 ( 99.94)
Epoch: [32][ 90/391]	Time  0.028 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.2738e-01 (1.3015e-01)	Acc@1  93.75 ( 95.71)	Acc@5 100.00 ( 99.93)
Epoch: [32][100/391]	Time  0.028 ( 0.027)	Data  0.004 ( 0.004)	Loss 6.6154e-02 (1.3003e-01)	Acc@1  97.66 ( 95.69)	Acc@5 100.00 ( 99.93)
Epoch: [32][110/391]	Time  0.027 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.6088e-01 (1.2948e-01)	Acc@1  92.19 ( 95.72)	Acc@5 100.00 ( 99.92)
Epoch: [32][120/391]	Time  0.023 ( 0.027)	Data  0.003 ( 0.003)	Loss 8.1941e-02 (1.2855e-01)	Acc@1  97.66 ( 95.74)	Acc@5 100.00 ( 99.92)
Epoch: [32][130/391]	Time  0.025 ( 0.027)	Data  0.002 ( 0.003)	Loss 1.9295e-01 (1.2819e-01)	Acc@1  92.19 ( 95.74)	Acc@5 100.00 ( 99.92)
Epoch: [32][140/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.2264e-01 (1.2707e-01)	Acc@1  97.66 ( 95.79)	Acc@5 100.00 ( 99.92)
Epoch: [32][150/391]	Time  0.027 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.0168e-01 (1.2695e-01)	Acc@1  96.09 ( 95.78)	Acc@5 100.00 ( 99.91)
Epoch: [32][160/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.4417e-02 (1.2550e-01)	Acc@1  97.66 ( 95.80)	Acc@5 100.00 ( 99.91)
Epoch: [32][170/391]	Time  0.031 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.0806e-01 (1.2450e-01)	Acc@1  94.53 ( 95.82)	Acc@5 100.00 ( 99.91)
Epoch: [32][180/391]	Time  0.025 ( 0.027)	Data  0.005 ( 0.003)	Loss 2.4225e-01 (1.2588e-01)	Acc@1  92.19 ( 95.77)	Acc@5 100.00 ( 99.91)
Epoch: [32][190/391]	Time  0.028 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.7047e-02 (1.2530e-01)	Acc@1  98.44 ( 95.78)	Acc@5 100.00 ( 99.92)
Epoch: [32][200/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.3147e-01 (1.2575e-01)	Acc@1  94.53 ( 95.77)	Acc@5  99.22 ( 99.91)
Epoch: [32][210/391]	Time  0.032 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.0267e-01 (1.2578e-01)	Acc@1  96.09 ( 95.75)	Acc@5 100.00 ( 99.91)
Epoch: [32][220/391]	Time  0.026 ( 0.027)	Data  0.002 ( 0.003)	Loss 7.0818e-02 (1.2513e-01)	Acc@1  99.22 ( 95.80)	Acc@5 100.00 ( 99.90)
Epoch: [32][230/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.2028e-01 (1.2401e-01)	Acc@1  95.31 ( 95.82)	Acc@5 100.00 ( 99.91)
Epoch: [32][240/391]	Time  0.028 ( 0.027)	Data  0.004 ( 0.003)	Loss 1.2714e-01 (1.2390e-01)	Acc@1  96.09 ( 95.83)	Acc@5 100.00 ( 99.91)
Epoch: [32][250/391]	Time  0.024 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.8668e-01 (1.2466e-01)	Acc@1  93.75 ( 95.79)	Acc@5 100.00 ( 99.91)
Epoch: [32][260/391]	Time  0.026 ( 0.026)	Data  0.004 ( 0.003)	Loss 1.9407e-01 (1.2405e-01)	Acc@1  92.19 ( 95.82)	Acc@5 100.00 ( 99.91)
Epoch: [32][270/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3124e-01 (1.2405e-01)	Acc@1  95.31 ( 95.82)	Acc@5 100.00 ( 99.92)
Epoch: [32][280/391]	Time  0.025 ( 0.026)	Data  0.000 ( 0.003)	Loss 6.7391e-02 (1.2323e-01)	Acc@1  96.88 ( 95.84)	Acc@5 100.00 ( 99.92)
Epoch: [32][290/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.8708e-02 (1.2273e-01)	Acc@1  96.09 ( 95.85)	Acc@5 100.00 ( 99.92)
Epoch: [32][300/391]	Time  0.030 ( 0.026)	Data  0.012 ( 0.003)	Loss 1.2040e-01 (1.2333e-01)	Acc@1  95.31 ( 95.82)	Acc@5 100.00 ( 99.92)
Epoch: [32][310/391]	Time  0.033 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.8246e-01 (1.2436e-01)	Acc@1  94.53 ( 95.79)	Acc@5 100.00 ( 99.92)
Epoch: [32][320/391]	Time  0.024 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.2468e-01 (1.2445e-01)	Acc@1  92.97 ( 95.79)	Acc@5 100.00 ( 99.92)
Epoch: [32][330/391]	Time  0.024 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.3259e-01 (1.2363e-01)	Acc@1  95.31 ( 95.82)	Acc@5 100.00 ( 99.92)
Epoch: [32][340/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.3406e-02 (1.2345e-01)	Acc@1  97.66 ( 95.82)	Acc@5 100.00 ( 99.92)
Epoch: [32][350/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7204e-01 (1.2360e-01)	Acc@1  95.31 ( 95.82)	Acc@5 100.00 ( 99.92)
Epoch: [32][360/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3366e-01 (1.2365e-01)	Acc@1  95.31 ( 95.81)	Acc@5 100.00 ( 99.92)
Epoch: [32][370/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8536e-01 (1.2352e-01)	Acc@1  92.97 ( 95.83)	Acc@5 100.00 ( 99.93)
Epoch: [32][380/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3557e-01 (1.2355e-01)	Acc@1  96.88 ( 95.83)	Acc@5 100.00 ( 99.93)
Epoch: [32][390/391]	Time  0.024 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.1891e-01 (1.2328e-01)	Acc@1  96.25 ( 95.83)	Acc@5 100.00 ( 99.92)
## e[32] optimizer.zero_grad (sum) time: 0.1076650619506836
## e[32]       loss.backward (sum) time: 2.2417469024658203
## e[32]      optimizer.step (sum) time: 0.8531012535095215
## epoch[32] training(only) time: 10.382838487625122
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 2.3708e-01 (2.3708e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.012 ( 0.027)	Loss 3.1104e-01 (3.1190e-01)	Acc@1  91.00 ( 90.09)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.023 ( 0.021)	Loss 3.8650e-01 (3.1981e-01)	Acc@1  87.00 ( 89.90)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.011 ( 0.019)	Loss 3.2495e-01 (3.4080e-01)	Acc@1  85.00 ( 89.58)	Acc@5  99.00 ( 99.65)
Test: [ 40/100]	Time  0.010 ( 0.017)	Loss 4.8754e-01 (3.5081e-01)	Acc@1  89.00 ( 89.66)	Acc@5  99.00 ( 99.63)
Test: [ 50/100]	Time  0.016 ( 0.017)	Loss 1.8379e-01 (3.4729e-01)	Acc@1  94.00 ( 89.84)	Acc@5  99.00 ( 99.59)
Test: [ 60/100]	Time  0.011 ( 0.016)	Loss 2.9570e-01 (3.4489e-01)	Acc@1  92.00 ( 89.93)	Acc@5 100.00 ( 99.61)
Test: [ 70/100]	Time  0.010 ( 0.016)	Loss 5.1315e-01 (3.4161e-01)	Acc@1  89.00 ( 89.97)	Acc@5  98.00 ( 99.58)
Test: [ 80/100]	Time  0.010 ( 0.016)	Loss 2.4058e-01 (3.3512e-01)	Acc@1  92.00 ( 90.07)	Acc@5 100.00 ( 99.60)
Test: [ 90/100]	Time  0.012 ( 0.016)	Loss 2.2733e-01 (3.3085e-01)	Acc@1  94.00 ( 90.18)	Acc@5 100.00 ( 99.63)
 * Acc@1 90.270 Acc@5 99.650
### epoch[32] execution time: 12.093006372451782
EPOCH 33
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [33][  0/391]	Time  0.186 ( 0.186)	Data  0.165 ( 0.165)	Loss 8.5450e-02 (8.5450e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [33][ 10/391]	Time  0.023 ( 0.040)	Data  0.001 ( 0.017)	Loss 5.9132e-02 (1.0627e-01)	Acc@1  97.66 ( 96.66)	Acc@5 100.00 (100.00)
Epoch: [33][ 20/391]	Time  0.025 ( 0.033)	Data  0.001 ( 0.010)	Loss 1.1716e-01 (1.0339e-01)	Acc@1  96.88 ( 96.47)	Acc@5 100.00 (100.00)
Epoch: [33][ 30/391]	Time  0.024 ( 0.031)	Data  0.001 ( 0.007)	Loss 1.2364e-01 (1.0896e-01)	Acc@1  96.88 ( 96.37)	Acc@5  99.22 ( 99.95)
Epoch: [33][ 40/391]	Time  0.023 ( 0.030)	Data  0.001 ( 0.006)	Loss 8.8474e-02 (1.0381e-01)	Acc@1  97.66 ( 96.63)	Acc@5 100.00 ( 99.94)
Epoch: [33][ 50/391]	Time  0.032 ( 0.029)	Data  0.004 ( 0.005)	Loss 2.3672e-01 (1.0480e-01)	Acc@1  92.97 ( 96.63)	Acc@5 100.00 ( 99.94)
Epoch: [33][ 60/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.3664e-01 (1.0662e-01)	Acc@1  95.31 ( 96.52)	Acc@5 100.00 ( 99.94)
Epoch: [33][ 70/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.8140e-01 (1.0980e-01)	Acc@1  96.09 ( 96.46)	Acc@5  99.22 ( 99.93)
Epoch: [33][ 80/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.5354e-01 (1.0980e-01)	Acc@1  93.75 ( 96.42)	Acc@5 100.00 ( 99.90)
Epoch: [33][ 90/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.1746e-01 (1.0830e-01)	Acc@1  96.09 ( 96.50)	Acc@5 100.00 ( 99.91)
Epoch: [33][100/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.0672e-01 (1.0807e-01)	Acc@1  96.88 ( 96.44)	Acc@5 100.00 ( 99.91)
Epoch: [33][110/391]	Time  0.023 ( 0.027)	Data  0.003 ( 0.004)	Loss 6.8641e-02 (1.0843e-01)	Acc@1  97.66 ( 96.44)	Acc@5 100.00 ( 99.92)
Epoch: [33][120/391]	Time  0.026 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.7332e-02 (1.0898e-01)	Acc@1  98.44 ( 96.46)	Acc@5 100.00 ( 99.92)
Epoch: [33][130/391]	Time  0.023 ( 0.027)	Data  0.004 ( 0.003)	Loss 7.1814e-02 (1.0929e-01)	Acc@1  97.66 ( 96.44)	Acc@5 100.00 ( 99.93)
Epoch: [33][140/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.5136e-01 (1.1069e-01)	Acc@1  94.53 ( 96.40)	Acc@5 100.00 ( 99.92)
Epoch: [33][150/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 2.2264e-01 (1.1085e-01)	Acc@1  92.97 ( 96.38)	Acc@5 100.00 ( 99.93)
Epoch: [33][160/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.4901e-02 (1.1242e-01)	Acc@1  97.66 ( 96.33)	Acc@5 100.00 ( 99.92)
Epoch: [33][170/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.2802e-02 (1.1125e-01)	Acc@1  96.88 ( 96.34)	Acc@5 100.00 ( 99.93)
Epoch: [33][180/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 8.3756e-02 (1.1072e-01)	Acc@1  97.66 ( 96.34)	Acc@5 100.00 ( 99.93)
Epoch: [33][190/391]	Time  0.024 ( 0.027)	Data  0.002 ( 0.003)	Loss 1.0947e-01 (1.1000e-01)	Acc@1  96.88 ( 96.37)	Acc@5 100.00 ( 99.93)
Epoch: [33][200/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.4712e-01 (1.1056e-01)	Acc@1  96.09 ( 96.35)	Acc@5 100.00 ( 99.93)
Epoch: [33][210/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.3158e-02 (1.0999e-01)	Acc@1  98.44 ( 96.38)	Acc@5 100.00 ( 99.93)
Epoch: [33][220/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.7080e-02 (1.0973e-01)	Acc@1  99.22 ( 96.39)	Acc@5 100.00 ( 99.93)
Epoch: [33][230/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.9313e-02 (1.1022e-01)	Acc@1  97.66 ( 96.35)	Acc@5 100.00 ( 99.93)
Epoch: [33][240/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9367e-01 (1.1079e-01)	Acc@1  92.19 ( 96.34)	Acc@5 100.00 ( 99.93)
Epoch: [33][250/391]	Time  0.026 ( 0.026)	Data  0.003 ( 0.003)	Loss 1.0463e-01 (1.1086e-01)	Acc@1  96.88 ( 96.33)	Acc@5 100.00 ( 99.93)
Epoch: [33][260/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0599e-01 (1.1007e-01)	Acc@1  96.88 ( 96.36)	Acc@5  99.22 ( 99.93)
Epoch: [33][270/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.2706e-01 (1.0986e-01)	Acc@1  96.09 ( 96.35)	Acc@5 100.00 ( 99.93)
Epoch: [33][280/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.4537e-02 (1.1037e-01)	Acc@1  96.88 ( 96.36)	Acc@5 100.00 ( 99.93)
Epoch: [33][290/391]	Time  0.037 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.7815e-02 (1.1042e-01)	Acc@1  97.66 ( 96.36)	Acc@5 100.00 ( 99.92)
Epoch: [33][300/391]	Time  0.035 ( 0.026)	Data  0.006 ( 0.003)	Loss 8.2250e-02 (1.1063e-01)	Acc@1  97.66 ( 96.37)	Acc@5 100.00 ( 99.92)
Epoch: [33][310/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7479e-01 (1.1040e-01)	Acc@1  92.19 ( 96.36)	Acc@5 100.00 ( 99.92)
Epoch: [33][320/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0761e-01 (1.1031e-01)	Acc@1  96.88 ( 96.36)	Acc@5 100.00 ( 99.92)
Epoch: [33][330/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4013e-01 (1.1040e-01)	Acc@1  95.31 ( 96.35)	Acc@5 100.00 ( 99.92)
Epoch: [33][340/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.1998e-02 (1.1054e-01)	Acc@1  96.88 ( 96.34)	Acc@5 100.00 ( 99.92)
Epoch: [33][350/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.5053e-02 (1.0948e-01)	Acc@1  97.66 ( 96.37)	Acc@5 100.00 ( 99.93)
Epoch: [33][360/391]	Time  0.034 ( 0.026)	Data  0.000 ( 0.003)	Loss 3.9036e-02 (1.0944e-01)	Acc@1  98.44 ( 96.38)	Acc@5 100.00 ( 99.93)
Epoch: [33][370/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3757e-01 (1.0951e-01)	Acc@1  96.09 ( 96.37)	Acc@5  99.22 ( 99.93)
Epoch: [33][380/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3706e-01 (1.0945e-01)	Acc@1  95.31 ( 96.37)	Acc@5 100.00 ( 99.93)
Epoch: [33][390/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.3721e-02 (1.0970e-01)	Acc@1  98.75 ( 96.36)	Acc@5 100.00 ( 99.93)
## e[33] optimizer.zero_grad (sum) time: 0.10828089714050293
## e[33]       loss.backward (sum) time: 2.2357020378112793
## e[33]      optimizer.step (sum) time: 0.8858151435852051
## epoch[33] training(only) time: 10.3822660446167
# Switched to evaluate mode...
Test: [  0/100]	Time  0.162 ( 0.162)	Loss 2.3996e-01 (2.3996e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.016 ( 0.028)	Loss 3.3077e-01 (3.0844e-01)	Acc@1  89.00 ( 90.45)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.012 ( 0.021)	Loss 3.6498e-01 (3.2487e-01)	Acc@1  90.00 ( 89.76)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.011 ( 0.019)	Loss 3.1659e-01 (3.4679e-01)	Acc@1  87.00 ( 89.48)	Acc@5  99.00 ( 99.58)
Test: [ 40/100]	Time  0.011 ( 0.017)	Loss 4.5457e-01 (3.5154e-01)	Acc@1  88.00 ( 89.73)	Acc@5  99.00 ( 99.51)
Test: [ 50/100]	Time  0.010 ( 0.017)	Loss 1.6917e-01 (3.4801e-01)	Acc@1  95.00 ( 89.84)	Acc@5  99.00 ( 99.51)
Test: [ 60/100]	Time  0.013 ( 0.016)	Loss 3.1210e-01 (3.4433e-01)	Acc@1  91.00 ( 89.97)	Acc@5 100.00 ( 99.54)
Test: [ 70/100]	Time  0.018 ( 0.016)	Loss 5.1517e-01 (3.4191e-01)	Acc@1  90.00 ( 90.08)	Acc@5  98.00 ( 99.54)
Test: [ 80/100]	Time  0.015 ( 0.016)	Loss 2.9696e-01 (3.3663e-01)	Acc@1  92.00 ( 90.12)	Acc@5 100.00 ( 99.59)
Test: [ 90/100]	Time  0.011 ( 0.016)	Loss 2.9364e-01 (3.3281e-01)	Acc@1  92.00 ( 90.11)	Acc@5  99.00 ( 99.60)
 * Acc@1 90.140 Acc@5 99.620
### epoch[33] execution time: 12.055630207061768
EPOCH 34
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [34][  0/391]	Time  0.197 ( 0.197)	Data  0.173 ( 0.173)	Loss 1.0130e-01 (1.0130e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [34][ 10/391]	Time  0.032 ( 0.042)	Data  0.002 ( 0.018)	Loss 1.2144e-01 (1.1238e-01)	Acc@1  96.88 ( 95.74)	Acc@5 100.00 (100.00)
Epoch: [34][ 20/391]	Time  0.023 ( 0.034)	Data  0.001 ( 0.010)	Loss 1.1189e-01 (1.0945e-01)	Acc@1  96.09 ( 96.06)	Acc@5 100.00 ( 99.96)
Epoch: [34][ 30/391]	Time  0.023 ( 0.031)	Data  0.004 ( 0.008)	Loss 7.4092e-02 (1.0673e-01)	Acc@1  97.66 ( 96.14)	Acc@5 100.00 ( 99.95)
Epoch: [34][ 40/391]	Time  0.021 ( 0.030)	Data  0.001 ( 0.006)	Loss 1.0536e-01 (1.0479e-01)	Acc@1  96.09 ( 96.30)	Acc@5 100.00 ( 99.90)
Epoch: [34][ 50/391]	Time  0.033 ( 0.029)	Data  0.001 ( 0.006)	Loss 1.0858e-01 (1.0278e-01)	Acc@1  96.88 ( 96.46)	Acc@5 100.00 ( 99.92)
Epoch: [34][ 60/391]	Time  0.029 ( 0.029)	Data  0.001 ( 0.005)	Loss 9.7570e-02 (1.0576e-01)	Acc@1  96.09 ( 96.35)	Acc@5 100.00 ( 99.94)
Epoch: [34][ 70/391]	Time  0.028 ( 0.028)	Data  0.000 ( 0.005)	Loss 1.2207e-01 (1.0806e-01)	Acc@1  94.53 ( 96.36)	Acc@5 100.00 ( 99.93)
Epoch: [34][ 80/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.004)	Loss 7.9590e-02 (1.0765e-01)	Acc@1  96.88 ( 96.32)	Acc@5 100.00 ( 99.94)
Epoch: [34][ 90/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 7.6728e-02 (1.0622e-01)	Acc@1  98.44 ( 96.39)	Acc@5 100.00 ( 99.93)
Epoch: [34][100/391]	Time  0.023 ( 0.027)	Data  0.000 ( 0.004)	Loss 9.5389e-02 (1.0556e-01)	Acc@1  97.66 ( 96.39)	Acc@5 100.00 ( 99.92)
Epoch: [34][110/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.5195e-01 (1.0672e-01)	Acc@1  94.53 ( 96.31)	Acc@5 100.00 ( 99.90)
Epoch: [34][120/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.3275e-01 (1.0634e-01)	Acc@1  95.31 ( 96.36)	Acc@5 100.00 ( 99.91)
Epoch: [34][130/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 9.2005e-02 (1.0671e-01)	Acc@1  96.09 ( 96.33)	Acc@5  99.22 ( 99.90)
Epoch: [34][140/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.3015e-01 (1.0649e-01)	Acc@1  96.88 ( 96.37)	Acc@5 100.00 ( 99.91)
Epoch: [34][150/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.2276e-01 (1.0668e-01)	Acc@1  94.53 ( 96.38)	Acc@5 100.00 ( 99.91)
Epoch: [34][160/391]	Time  0.033 ( 0.027)	Data  0.004 ( 0.003)	Loss 7.7767e-02 (1.0601e-01)	Acc@1  98.44 ( 96.42)	Acc@5 100.00 ( 99.91)
Epoch: [34][170/391]	Time  0.027 ( 0.027)	Data  0.000 ( 0.003)	Loss 7.8568e-02 (1.0620e-01)	Acc@1  97.66 ( 96.40)	Acc@5 100.00 ( 99.90)
Epoch: [34][180/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.9232e-02 (1.0636e-01)	Acc@1  97.66 ( 96.38)	Acc@5 100.00 ( 99.89)
Epoch: [34][190/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.3288e-01 (1.0621e-01)	Acc@1  96.09 ( 96.40)	Acc@5 100.00 ( 99.89)
Epoch: [34][200/391]	Time  0.032 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.4713e-02 (1.0562e-01)	Acc@1  96.88 ( 96.43)	Acc@5 100.00 ( 99.90)
Epoch: [34][210/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.0879e-01 (1.0537e-01)	Acc@1  95.31 ( 96.40)	Acc@5 100.00 ( 99.90)
Epoch: [34][220/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.003)	Loss 9.7375e-02 (1.0594e-01)	Acc@1  97.66 ( 96.41)	Acc@5 100.00 ( 99.90)
Epoch: [34][230/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7216e-01 (1.0698e-01)	Acc@1  95.31 ( 96.38)	Acc@5 100.00 ( 99.91)
Epoch: [34][240/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.9241e-02 (1.0638e-01)	Acc@1  96.09 ( 96.40)	Acc@5 100.00 ( 99.91)
Epoch: [34][250/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1490e-01 (1.0637e-01)	Acc@1  95.31 ( 96.40)	Acc@5 100.00 ( 99.91)
Epoch: [34][260/391]	Time  0.024 ( 0.026)	Data  0.002 ( 0.003)	Loss 7.7780e-02 (1.0708e-01)	Acc@1  96.88 ( 96.38)	Acc@5 100.00 ( 99.91)
Epoch: [34][270/391]	Time  0.023 ( 0.026)	Data  0.000 ( 0.003)	Loss 3.8839e-02 (1.0691e-01)	Acc@1 100.00 ( 96.37)	Acc@5 100.00 ( 99.92)
Epoch: [34][280/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1050e-01 (1.0602e-01)	Acc@1  96.09 ( 96.39)	Acc@5 100.00 ( 99.92)
Epoch: [34][290/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.9943e-02 (1.0708e-01)	Acc@1  99.22 ( 96.36)	Acc@5 100.00 ( 99.92)
Epoch: [34][300/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5641e-01 (1.0891e-01)	Acc@1  93.75 ( 96.31)	Acc@5  99.22 ( 99.92)
Epoch: [34][310/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.7767e-02 (1.0903e-01)	Acc@1  96.88 ( 96.31)	Acc@5 100.00 ( 99.91)
Epoch: [34][320/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 7.2423e-02 (1.0814e-01)	Acc@1  97.66 ( 96.34)	Acc@5 100.00 ( 99.92)
Epoch: [34][330/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.5973e-02 (1.0853e-01)	Acc@1  96.09 ( 96.32)	Acc@5 100.00 ( 99.92)
Epoch: [34][340/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.2807e-02 (1.0782e-01)	Acc@1  97.66 ( 96.35)	Acc@5 100.00 ( 99.92)
Epoch: [34][350/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2680e-01 (1.0827e-01)	Acc@1  94.53 ( 96.32)	Acc@5 100.00 ( 99.91)
Epoch: [34][360/391]	Time  0.028 ( 0.026)	Data  0.003 ( 0.003)	Loss 3.6586e-02 (1.0783e-01)	Acc@1 100.00 ( 96.34)	Acc@5 100.00 ( 99.92)
Epoch: [34][370/391]	Time  0.030 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.8663e-01 (1.0779e-01)	Acc@1  94.53 ( 96.34)	Acc@5 100.00 ( 99.92)
Epoch: [34][380/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.3454e-02 (1.0723e-01)	Acc@1  97.66 ( 96.36)	Acc@5 100.00 ( 99.92)
Epoch: [34][390/391]	Time  0.027 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.2900e-01 (1.0776e-01)	Acc@1  91.25 ( 96.35)	Acc@5 100.00 ( 99.92)
## e[34] optimizer.zero_grad (sum) time: 0.11018133163452148
## e[34]       loss.backward (sum) time: 2.2679154872894287
## e[34]      optimizer.step (sum) time: 0.8969376087188721
## epoch[34] training(only) time: 10.370315790176392
# Switched to evaluate mode...
Test: [  0/100]	Time  0.162 ( 0.162)	Loss 2.6001e-01 (2.6001e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.011 ( 0.028)	Loss 4.1308e-01 (3.2437e-01)	Acc@1  89.00 ( 90.09)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.011 ( 0.022)	Loss 3.8805e-01 (3.3554e-01)	Acc@1  87.00 ( 89.67)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.011 ( 0.020)	Loss 3.0449e-01 (3.5472e-01)	Acc@1  87.00 ( 89.61)	Acc@5  99.00 ( 99.65)
Test: [ 40/100]	Time  0.011 ( 0.018)	Loss 4.4443e-01 (3.5899e-01)	Acc@1  88.00 ( 89.66)	Acc@5  99.00 ( 99.54)
Test: [ 50/100]	Time  0.015 ( 0.018)	Loss 2.1514e-01 (3.5571e-01)	Acc@1  93.00 ( 89.86)	Acc@5  99.00 ( 99.51)
Test: [ 60/100]	Time  0.011 ( 0.017)	Loss 3.7389e-01 (3.4966e-01)	Acc@1  89.00 ( 90.00)	Acc@5 100.00 ( 99.54)
Test: [ 70/100]	Time  0.027 ( 0.017)	Loss 5.3991e-01 (3.4571e-01)	Acc@1  89.00 ( 90.11)	Acc@5  99.00 ( 99.52)
Test: [ 80/100]	Time  0.012 ( 0.017)	Loss 2.7615e-01 (3.4037e-01)	Acc@1  92.00 ( 90.12)	Acc@5 100.00 ( 99.57)
Test: [ 90/100]	Time  0.011 ( 0.016)	Loss 2.4793e-01 (3.3524e-01)	Acc@1  93.00 ( 90.20)	Acc@5 100.00 ( 99.59)
 * Acc@1 90.250 Acc@5 99.610
### epoch[34] execution time: 12.120813369750977
EPOCH 35
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [35][  0/391]	Time  0.191 ( 0.191)	Data  0.172 ( 0.172)	Loss 5.0129e-02 (5.0129e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [35][ 10/391]	Time  0.024 ( 0.041)	Data  0.001 ( 0.017)	Loss 6.2251e-02 (9.3976e-02)	Acc@1  97.66 ( 96.80)	Acc@5 100.00 ( 99.93)
Epoch: [35][ 20/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.010)	Loss 1.1626e-01 (9.8185e-02)	Acc@1  95.31 ( 96.58)	Acc@5 100.00 ( 99.93)
Epoch: [35][ 30/391]	Time  0.027 ( 0.031)	Data  0.001 ( 0.008)	Loss 7.1093e-02 (9.6483e-02)	Acc@1  98.44 ( 96.67)	Acc@5 100.00 ( 99.95)
Epoch: [35][ 40/391]	Time  0.023 ( 0.030)	Data  0.000 ( 0.006)	Loss 5.3651e-02 (9.3092e-02)	Acc@1  98.44 ( 96.84)	Acc@5 100.00 ( 99.94)
Epoch: [35][ 50/391]	Time  0.031 ( 0.029)	Data  0.001 ( 0.005)	Loss 1.2040e-01 (9.5960e-02)	Acc@1  96.88 ( 96.80)	Acc@5  99.22 ( 99.94)
Epoch: [35][ 60/391]	Time  0.030 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.1940e-01 (9.5867e-02)	Acc@1  93.75 ( 96.84)	Acc@5 100.00 ( 99.95)
Epoch: [35][ 70/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.005)	Loss 9.1073e-02 (9.4257e-02)	Acc@1  96.88 ( 96.95)	Acc@5 100.00 ( 99.93)
Epoch: [35][ 80/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.6837e-01 (9.4705e-02)	Acc@1  96.09 ( 96.92)	Acc@5 100.00 ( 99.93)
Epoch: [35][ 90/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.004)	Loss 6.4701e-02 (9.5872e-02)	Acc@1  96.88 ( 96.84)	Acc@5 100.00 ( 99.94)
Epoch: [35][100/391]	Time  0.032 ( 0.027)	Data  0.001 ( 0.004)	Loss 6.0330e-02 (9.7291e-02)	Acc@1  97.66 ( 96.74)	Acc@5 100.00 ( 99.95)
Epoch: [35][110/391]	Time  0.028 ( 0.027)	Data  0.000 ( 0.004)	Loss 1.3308e-01 (9.6040e-02)	Acc@1  96.09 ( 96.81)	Acc@5 100.00 ( 99.95)
Epoch: [35][120/391]	Time  0.024 ( 0.027)	Data  0.000 ( 0.004)	Loss 1.5272e-01 (9.6513e-02)	Acc@1  95.31 ( 96.84)	Acc@5  99.22 ( 99.93)
Epoch: [35][130/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 6.4860e-02 (9.5578e-02)	Acc@1  97.66 ( 96.88)	Acc@5 100.00 ( 99.93)
Epoch: [35][140/391]	Time  0.026 ( 0.027)	Data  0.005 ( 0.004)	Loss 1.7654e-01 (9.5954e-02)	Acc@1  93.75 ( 96.86)	Acc@5 100.00 ( 99.93)
Epoch: [35][150/391]	Time  0.024 ( 0.027)	Data  0.002 ( 0.003)	Loss 7.8128e-02 (9.5429e-02)	Acc@1  97.66 ( 96.86)	Acc@5 100.00 ( 99.94)
Epoch: [35][160/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 1.8838e-01 (9.5063e-02)	Acc@1  93.75 ( 96.87)	Acc@5 100.00 ( 99.94)
Epoch: [35][170/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.1611e-02 (9.6832e-02)	Acc@1  98.44 ( 96.82)	Acc@5 100.00 ( 99.94)
Epoch: [35][180/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.9274e-02 (9.5103e-02)	Acc@1  98.44 ( 96.89)	Acc@5 100.00 ( 99.94)
Epoch: [35][190/391]	Time  0.022 ( 0.027)	Data  0.002 ( 0.003)	Loss 4.8959e-02 (9.4819e-02)	Acc@1  97.66 ( 96.89)	Acc@5 100.00 ( 99.94)
Epoch: [35][200/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 9.9651e-02 (9.5552e-02)	Acc@1  97.66 ( 96.87)	Acc@5 100.00 ( 99.94)
Epoch: [35][210/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.5858e-01 (9.6388e-02)	Acc@1  93.75 ( 96.85)	Acc@5 100.00 ( 99.94)
Epoch: [35][220/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.1371e-02 (9.6497e-02)	Acc@1  96.88 ( 96.84)	Acc@5 100.00 ( 99.94)
Epoch: [35][230/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6941e-01 (9.7753e-02)	Acc@1  92.97 ( 96.81)	Acc@5 100.00 ( 99.94)
Epoch: [35][240/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.2502e-02 (9.7469e-02)	Acc@1  98.44 ( 96.81)	Acc@5 100.00 ( 99.94)
Epoch: [35][250/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.7527e-02 (9.7437e-02)	Acc@1  97.66 ( 96.81)	Acc@5 100.00 ( 99.94)
Epoch: [35][260/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.5683e-02 (9.7173e-02)	Acc@1  96.88 ( 96.82)	Acc@5 100.00 ( 99.94)
Epoch: [35][270/391]	Time  0.025 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.8409e-01 (9.8296e-02)	Acc@1  93.75 ( 96.79)	Acc@5 100.00 ( 99.95)
Epoch: [35][280/391]	Time  0.034 ( 0.026)	Data  0.002 ( 0.003)	Loss 6.4512e-02 (9.8403e-02)	Acc@1  97.66 ( 96.78)	Acc@5 100.00 ( 99.94)
Epoch: [35][290/391]	Time  0.024 ( 0.026)	Data  0.004 ( 0.003)	Loss 4.2547e-02 (9.8420e-02)	Acc@1  99.22 ( 96.79)	Acc@5 100.00 ( 99.94)
Epoch: [35][300/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0034e-01 (9.8679e-02)	Acc@1  97.66 ( 96.79)	Acc@5 100.00 ( 99.94)
Epoch: [35][310/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.1103e-01 (9.8023e-02)	Acc@1  95.31 ( 96.81)	Acc@5 100.00 ( 99.94)
Epoch: [35][320/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7299e-01 (9.7705e-02)	Acc@1  93.75 ( 96.83)	Acc@5 100.00 ( 99.95)
Epoch: [35][330/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5640e-01 (9.7747e-02)	Acc@1  94.53 ( 96.83)	Acc@5 100.00 ( 99.95)
Epoch: [35][340/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.2382e-02 (9.8032e-02)	Acc@1  96.88 ( 96.82)	Acc@5 100.00 ( 99.95)
Epoch: [35][350/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5174e-01 (9.7872e-02)	Acc@1  96.09 ( 96.83)	Acc@5 100.00 ( 99.95)
Epoch: [35][360/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1223e-01 (9.7888e-02)	Acc@1  96.09 ( 96.82)	Acc@5  99.22 ( 99.94)
Epoch: [35][370/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.2143e-01 (9.7980e-02)	Acc@1  96.09 ( 96.81)	Acc@5 100.00 ( 99.94)
Epoch: [35][380/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5097e-01 (9.7905e-02)	Acc@1  95.31 ( 96.82)	Acc@5 100.00 ( 99.94)
Epoch: [35][390/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.9248e-02 (9.8114e-02)	Acc@1  97.50 ( 96.82)	Acc@5 100.00 ( 99.94)
## e[35] optimizer.zero_grad (sum) time: 0.10880017280578613
## e[35]       loss.backward (sum) time: 2.243410587310791
## e[35]      optimizer.step (sum) time: 0.872363805770874
## epoch[35] training(only) time: 10.420903205871582
# Switched to evaluate mode...
Test: [  0/100]	Time  0.167 ( 0.167)	Loss 2.4802e-01 (2.4802e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.011 ( 0.028)	Loss 3.7256e-01 (3.1221e-01)	Acc@1  88.00 ( 89.82)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.012 ( 0.021)	Loss 3.7818e-01 (3.3706e-01)	Acc@1  87.00 ( 89.48)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.022 ( 0.019)	Loss 3.0661e-01 (3.5184e-01)	Acc@1  89.00 ( 89.58)	Acc@5  99.00 ( 99.58)
Test: [ 40/100]	Time  0.011 ( 0.018)	Loss 4.6300e-01 (3.6193e-01)	Acc@1  90.00 ( 89.78)	Acc@5  99.00 ( 99.61)
Test: [ 50/100]	Time  0.012 ( 0.017)	Loss 2.0493e-01 (3.6095e-01)	Acc@1  94.00 ( 89.78)	Acc@5  99.00 ( 99.57)
Test: [ 60/100]	Time  0.011 ( 0.017)	Loss 2.9116e-01 (3.5453e-01)	Acc@1  90.00 ( 90.02)	Acc@5 100.00 ( 99.59)
Test: [ 70/100]	Time  0.019 ( 0.016)	Loss 5.4455e-01 (3.5154e-01)	Acc@1  87.00 ( 90.00)	Acc@5 100.00 ( 99.56)
Test: [ 80/100]	Time  0.010 ( 0.016)	Loss 2.3558e-01 (3.4714e-01)	Acc@1  94.00 ( 90.14)	Acc@5 100.00 ( 99.62)
Test: [ 90/100]	Time  0.010 ( 0.016)	Loss 2.5022e-01 (3.4251e-01)	Acc@1  94.00 ( 90.20)	Acc@5 100.00 ( 99.64)
 * Acc@1 90.220 Acc@5 99.650
### epoch[35] execution time: 12.106215715408325
EPOCH 36
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [36][  0/391]	Time  0.191 ( 0.191)	Data  0.171 ( 0.171)	Loss 5.4755e-02 (5.4755e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [36][ 10/391]	Time  0.024 ( 0.041)	Data  0.001 ( 0.017)	Loss 5.2488e-02 (9.8367e-02)	Acc@1  98.44 ( 96.66)	Acc@5 100.00 ( 99.86)
Epoch: [36][ 20/391]	Time  0.023 ( 0.033)	Data  0.001 ( 0.010)	Loss 1.0991e-01 (9.5127e-02)	Acc@1  95.31 ( 96.65)	Acc@5 100.00 ( 99.89)
Epoch: [36][ 30/391]	Time  0.021 ( 0.031)	Data  0.004 ( 0.008)	Loss 5.8216e-02 (9.3858e-02)	Acc@1  96.88 ( 96.72)	Acc@5 100.00 ( 99.87)
Epoch: [36][ 40/391]	Time  0.023 ( 0.030)	Data  0.001 ( 0.006)	Loss 3.7172e-02 (9.7343e-02)	Acc@1 100.00 ( 96.61)	Acc@5 100.00 ( 99.87)
Epoch: [36][ 50/391]	Time  0.026 ( 0.029)	Data  0.001 ( 0.005)	Loss 5.8847e-02 (9.3246e-02)	Acc@1  97.66 ( 96.77)	Acc@5 100.00 ( 99.88)
Epoch: [36][ 60/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.005)	Loss 5.7502e-02 (9.4095e-02)	Acc@1  99.22 ( 96.84)	Acc@5 100.00 ( 99.85)
Epoch: [36][ 70/391]	Time  0.023 ( 0.028)	Data  0.002 ( 0.005)	Loss 2.0573e-02 (9.2355e-02)	Acc@1 100.00 ( 96.90)	Acc@5 100.00 ( 99.86)
Epoch: [36][ 80/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 8.4236e-02 (9.1011e-02)	Acc@1  97.66 ( 96.92)	Acc@5 100.00 ( 99.87)
Epoch: [36][ 90/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.3435e-01 (9.0851e-02)	Acc@1  94.53 ( 96.93)	Acc@5 100.00 ( 99.89)
Epoch: [36][100/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.004)	Loss 6.6404e-02 (8.8989e-02)	Acc@1  98.44 ( 96.96)	Acc@5 100.00 ( 99.90)
Epoch: [36][110/391]	Time  0.024 ( 0.027)	Data  0.003 ( 0.004)	Loss 1.0245e-01 (9.0862e-02)	Acc@1  96.88 ( 96.89)	Acc@5 100.00 ( 99.90)
Epoch: [36][120/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.004)	Loss 1.4614e-01 (9.1466e-02)	Acc@1  94.53 ( 96.91)	Acc@5 100.00 ( 99.91)
Epoch: [36][130/391]	Time  0.033 ( 0.027)	Data  0.000 ( 0.003)	Loss 1.3641e-01 (9.0401e-02)	Acc@1  96.09 ( 96.92)	Acc@5  99.22 ( 99.91)
Epoch: [36][140/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.0246e-01 (9.0086e-02)	Acc@1  96.88 ( 96.93)	Acc@5 100.00 ( 99.92)
Epoch: [36][150/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.3266e-02 (8.9614e-02)	Acc@1  98.44 ( 96.94)	Acc@5 100.00 ( 99.92)
Epoch: [36][160/391]	Time  0.026 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.8379e-02 (8.9209e-02)	Acc@1  96.88 ( 96.98)	Acc@5 100.00 ( 99.93)
Epoch: [36][170/391]	Time  0.026 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.0259e-02 (8.9129e-02)	Acc@1  98.44 ( 96.98)	Acc@5 100.00 ( 99.93)
Epoch: [36][180/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.6768e-02 (9.0138e-02)	Acc@1  98.44 ( 96.95)	Acc@5 100.00 ( 99.93)
Epoch: [36][190/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.6363e-02 (9.0193e-02)	Acc@1  99.22 ( 96.96)	Acc@5 100.00 ( 99.93)
Epoch: [36][200/391]	Time  0.031 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.0372e-01 (9.0801e-02)	Acc@1  97.66 ( 96.97)	Acc@5 100.00 ( 99.93)
Epoch: [36][210/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.7624e-01 (9.0289e-02)	Acc@1  95.31 ( 96.99)	Acc@5 100.00 ( 99.93)
Epoch: [36][220/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.2459e-01 (9.0956e-02)	Acc@1  94.53 ( 96.97)	Acc@5 100.00 ( 99.94)
Epoch: [36][230/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 8.3799e-02 (9.2053e-02)	Acc@1  96.09 ( 96.94)	Acc@5 100.00 ( 99.93)
Epoch: [36][240/391]	Time  0.027 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.2001e-01 (9.2359e-02)	Acc@1  93.75 ( 96.93)	Acc@5 100.00 ( 99.94)
Epoch: [36][250/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.2139e-02 (9.2706e-02)	Acc@1  98.44 ( 96.93)	Acc@5 100.00 ( 99.93)
Epoch: [36][260/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.0446e-02 (9.2647e-02)	Acc@1  97.66 ( 96.92)	Acc@5 100.00 ( 99.94)
Epoch: [36][270/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.0141e-02 (9.3196e-02)	Acc@1  97.66 ( 96.92)	Acc@5 100.00 ( 99.94)
Epoch: [36][280/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1732e-01 (9.3426e-02)	Acc@1  96.09 ( 96.91)	Acc@5  99.22 ( 99.94)
Epoch: [36][290/391]	Time  0.027 ( 0.026)	Data  0.002 ( 0.003)	Loss 6.0431e-02 (9.3270e-02)	Acc@1  98.44 ( 96.90)	Acc@5 100.00 ( 99.94)
Epoch: [36][300/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.5176e-02 (9.3202e-02)	Acc@1  99.22 ( 96.89)	Acc@5 100.00 ( 99.94)
Epoch: [36][310/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0386e-01 (9.3379e-02)	Acc@1  96.09 ( 96.88)	Acc@5 100.00 ( 99.94)
Epoch: [36][320/391]	Time  0.032 ( 0.026)	Data  0.000 ( 0.003)	Loss 4.5699e-02 (9.3141e-02)	Acc@1  98.44 ( 96.88)	Acc@5 100.00 ( 99.94)
Epoch: [36][330/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.1239e-02 (9.3446e-02)	Acc@1  98.44 ( 96.88)	Acc@5 100.00 ( 99.94)
Epoch: [36][340/391]	Time  0.023 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.5380e-02 (9.3357e-02)	Acc@1  99.22 ( 96.89)	Acc@5 100.00 ( 99.94)
Epoch: [36][350/391]	Time  0.023 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.4750e-01 (9.4251e-02)	Acc@1  96.09 ( 96.87)	Acc@5 100.00 ( 99.94)
Epoch: [36][360/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.7357e-02 (9.5000e-02)	Acc@1  96.88 ( 96.85)	Acc@5 100.00 ( 99.94)
Epoch: [36][370/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.6284e-02 (9.4570e-02)	Acc@1  96.88 ( 96.86)	Acc@5 100.00 ( 99.94)
Epoch: [36][380/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.8433e-02 (9.4430e-02)	Acc@1  95.31 ( 96.86)	Acc@5 100.00 ( 99.94)
Epoch: [36][390/391]	Time  0.023 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.3496e-01 (9.4811e-02)	Acc@1  92.50 ( 96.85)	Acc@5 100.00 ( 99.94)
## e[36] optimizer.zero_grad (sum) time: 0.10801959037780762
## e[36]       loss.backward (sum) time: 2.237243890762329
## e[36]      optimizer.step (sum) time: 0.8689210414886475
## epoch[36] training(only) time: 10.414698839187622
# Switched to evaluate mode...
Test: [  0/100]	Time  0.165 ( 0.165)	Loss 2.5310e-01 (2.5310e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.028)	Loss 4.4143e-01 (3.0738e-01)	Acc@1  87.00 ( 91.18)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.014 ( 0.021)	Loss 4.5178e-01 (3.3002e-01)	Acc@1  86.00 ( 90.19)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.011 ( 0.019)	Loss 2.6488e-01 (3.4704e-01)	Acc@1  90.00 ( 90.13)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.014 ( 0.018)	Loss 4.6529e-01 (3.5529e-01)	Acc@1  89.00 ( 90.05)	Acc@5  99.00 ( 99.63)
Test: [ 50/100]	Time  0.011 ( 0.017)	Loss 2.1523e-01 (3.5295e-01)	Acc@1  93.00 ( 90.08)	Acc@5  99.00 ( 99.59)
Test: [ 60/100]	Time  0.011 ( 0.017)	Loss 2.9478e-01 (3.4757e-01)	Acc@1  91.00 ( 90.16)	Acc@5 100.00 ( 99.61)
Test: [ 70/100]	Time  0.012 ( 0.016)	Loss 5.9130e-01 (3.4592e-01)	Acc@1  90.00 ( 90.27)	Acc@5  98.00 ( 99.56)
Test: [ 80/100]	Time  0.011 ( 0.016)	Loss 2.5084e-01 (3.4094e-01)	Acc@1  94.00 ( 90.28)	Acc@5 100.00 ( 99.60)
Test: [ 90/100]	Time  0.012 ( 0.016)	Loss 2.4654e-01 (3.3593e-01)	Acc@1  92.00 ( 90.26)	Acc@5 100.00 ( 99.63)
 * Acc@1 90.300 Acc@5 99.640
### epoch[36] execution time: 12.11938738822937
EPOCH 37
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [37][  0/391]	Time  0.186 ( 0.186)	Data  0.163 ( 0.163)	Loss 2.3960e-02 (2.3960e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [37][ 10/391]	Time  0.025 ( 0.040)	Data  0.001 ( 0.016)	Loss 8.8241e-02 (7.9520e-02)	Acc@1  97.66 ( 97.02)	Acc@5 100.00 (100.00)
Epoch: [37][ 20/391]	Time  0.027 ( 0.033)	Data  0.004 ( 0.010)	Loss 4.7796e-02 (7.8018e-02)	Acc@1  99.22 ( 97.40)	Acc@5 100.00 (100.00)
Epoch: [37][ 30/391]	Time  0.032 ( 0.031)	Data  0.001 ( 0.007)	Loss 8.1883e-02 (8.1303e-02)	Acc@1  98.44 ( 97.20)	Acc@5 100.00 (100.00)
Epoch: [37][ 40/391]	Time  0.022 ( 0.029)	Data  0.001 ( 0.006)	Loss 7.5086e-02 (8.3764e-02)	Acc@1  96.09 ( 97.10)	Acc@5 100.00 ( 99.98)
Epoch: [37][ 50/391]	Time  0.029 ( 0.029)	Data  0.011 ( 0.005)	Loss 8.1613e-02 (8.3892e-02)	Acc@1  96.88 ( 97.09)	Acc@5 100.00 ( 99.98)
Epoch: [37][ 60/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.2254e-01 (8.3137e-02)	Acc@1  96.88 ( 97.16)	Acc@5 100.00 ( 99.99)
Epoch: [37][ 70/391]	Time  0.028 ( 0.028)	Data  0.001 ( 0.005)	Loss 7.6281e-02 (8.4041e-02)	Acc@1  96.88 ( 97.10)	Acc@5 100.00 ( 99.98)
Epoch: [37][ 80/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.0356e-01 (8.4959e-02)	Acc@1  96.09 ( 97.05)	Acc@5 100.00 ( 99.98)
Epoch: [37][ 90/391]	Time  0.032 ( 0.028)	Data  0.003 ( 0.004)	Loss 1.3578e-01 (8.4558e-02)	Acc@1  96.88 ( 97.15)	Acc@5  99.22 ( 99.97)
Epoch: [37][100/391]	Time  0.030 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.3302e-02 (8.1936e-02)	Acc@1 100.00 ( 97.28)	Acc@5 100.00 ( 99.98)
Epoch: [37][110/391]	Time  0.024 ( 0.027)	Data  0.000 ( 0.004)	Loss 7.7012e-02 (8.3104e-02)	Acc@1  97.66 ( 97.25)	Acc@5 100.00 ( 99.98)
Epoch: [37][120/391]	Time  0.022 ( 0.027)	Data  0.002 ( 0.004)	Loss 7.0394e-02 (8.2528e-02)	Acc@1  97.66 ( 97.24)	Acc@5 100.00 ( 99.98)
Epoch: [37][130/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 9.9666e-02 (8.2149e-02)	Acc@1  96.88 ( 97.28)	Acc@5  99.22 ( 99.98)
Epoch: [37][140/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.8958e-02 (8.0847e-02)	Acc@1  97.66 ( 97.30)	Acc@5 100.00 ( 99.98)
Epoch: [37][150/391]	Time  0.027 ( 0.027)	Data  0.003 ( 0.003)	Loss 1.2578e-01 (8.1302e-02)	Acc@1  94.53 ( 97.26)	Acc@5 100.00 ( 99.98)
Epoch: [37][160/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 9.3190e-02 (8.1607e-02)	Acc@1  95.31 ( 97.22)	Acc@5 100.00 ( 99.98)
Epoch: [37][170/391]	Time  0.030 ( 0.027)	Data  0.005 ( 0.003)	Loss 6.2805e-02 (8.1700e-02)	Acc@1  96.88 ( 97.25)	Acc@5 100.00 ( 99.98)
Epoch: [37][180/391]	Time  0.023 ( 0.027)	Data  0.000 ( 0.003)	Loss 1.3140e-01 (8.3726e-02)	Acc@1  96.09 ( 97.17)	Acc@5  99.22 ( 99.97)
Epoch: [37][190/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.6055e-02 (8.3936e-02)	Acc@1  96.88 ( 97.17)	Acc@5 100.00 ( 99.97)
Epoch: [37][200/391]	Time  0.023 ( 0.027)	Data  0.000 ( 0.003)	Loss 5.7741e-02 (8.4356e-02)	Acc@1  96.88 ( 97.14)	Acc@5 100.00 ( 99.97)
Epoch: [37][210/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.6230e-02 (8.5778e-02)	Acc@1  96.88 ( 97.12)	Acc@5 100.00 ( 99.96)
Epoch: [37][220/391]	Time  0.024 ( 0.027)	Data  0.002 ( 0.003)	Loss 9.3915e-02 (8.5544e-02)	Acc@1  97.66 ( 97.12)	Acc@5 100.00 ( 99.95)
Epoch: [37][230/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.4417e-02 (8.5546e-02)	Acc@1  98.44 ( 97.13)	Acc@5 100.00 ( 99.96)
Epoch: [37][240/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.5421e-02 (8.5194e-02)	Acc@1  99.22 ( 97.14)	Acc@5 100.00 ( 99.96)
Epoch: [37][250/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0747e-01 (8.6354e-02)	Acc@1  96.09 ( 97.09)	Acc@5 100.00 ( 99.96)
Epoch: [37][260/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1987e-01 (8.6848e-02)	Acc@1  95.31 ( 97.07)	Acc@5 100.00 ( 99.96)
Epoch: [37][270/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0129e-01 (8.7173e-02)	Acc@1  96.09 ( 97.05)	Acc@5 100.00 ( 99.96)
Epoch: [37][280/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.6535e-02 (8.6734e-02)	Acc@1  99.22 ( 97.07)	Acc@5 100.00 ( 99.96)
Epoch: [37][290/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2545e-01 (8.7116e-02)	Acc@1  94.53 ( 97.07)	Acc@5 100.00 ( 99.96)
Epoch: [37][300/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.3192e-02 (8.7624e-02)	Acc@1  97.66 ( 97.05)	Acc@5 100.00 ( 99.96)
Epoch: [37][310/391]	Time  0.028 ( 0.026)	Data  0.005 ( 0.003)	Loss 1.2705e-01 (8.7319e-02)	Acc@1  96.88 ( 97.07)	Acc@5  99.22 ( 99.96)
Epoch: [37][320/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.4255e-02 (8.7545e-02)	Acc@1  96.09 ( 97.06)	Acc@5 100.00 ( 99.96)
Epoch: [37][330/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1302e-01 (8.7259e-02)	Acc@1  95.31 ( 97.08)	Acc@5 100.00 ( 99.96)
Epoch: [37][340/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0709e-02 (8.7370e-02)	Acc@1  99.22 ( 97.08)	Acc@5 100.00 ( 99.96)
Epoch: [37][350/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.5339e-02 (8.7363e-02)	Acc@1  95.31 ( 97.08)	Acc@5 100.00 ( 99.96)
Epoch: [37][360/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.1215e-02 (8.8066e-02)	Acc@1  96.88 ( 97.06)	Acc@5 100.00 ( 99.96)
Epoch: [37][370/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.0967e-02 (8.8183e-02)	Acc@1  96.09 ( 97.06)	Acc@5 100.00 ( 99.96)
Epoch: [37][380/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3243e-01 (8.8388e-02)	Acc@1  96.09 ( 97.05)	Acc@5  99.22 ( 99.95)
Epoch: [37][390/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.1930e-02 (8.8323e-02)	Acc@1  97.50 ( 97.05)	Acc@5 100.00 ( 99.96)
## e[37] optimizer.zero_grad (sum) time: 0.10965871810913086
## e[37]       loss.backward (sum) time: 2.216221809387207
## e[37]      optimizer.step (sum) time: 0.8713631629943848
## epoch[37] training(only) time: 10.35883116722107
# Switched to evaluate mode...
Test: [  0/100]	Time  0.155 ( 0.155)	Loss 2.5668e-01 (2.5668e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.012 ( 0.027)	Loss 4.5833e-01 (3.3268e-01)	Acc@1  91.00 ( 90.18)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.017 ( 0.021)	Loss 4.4067e-01 (3.4857e-01)	Acc@1  86.00 ( 89.76)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.013 ( 0.019)	Loss 2.2392e-01 (3.5819e-01)	Acc@1  90.00 ( 89.71)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.017 ( 0.018)	Loss 4.3830e-01 (3.6492e-01)	Acc@1  90.00 ( 89.46)	Acc@5  99.00 ( 99.66)
Test: [ 50/100]	Time  0.015 ( 0.017)	Loss 2.2271e-01 (3.6013e-01)	Acc@1  93.00 ( 89.69)	Acc@5 100.00 ( 99.67)
Test: [ 60/100]	Time  0.015 ( 0.016)	Loss 3.5855e-01 (3.5564e-01)	Acc@1  89.00 ( 89.87)	Acc@5 100.00 ( 99.67)
Test: [ 70/100]	Time  0.011 ( 0.016)	Loss 5.4296e-01 (3.5090e-01)	Acc@1  90.00 ( 90.06)	Acc@5 100.00 ( 99.66)
Test: [ 80/100]	Time  0.011 ( 0.016)	Loss 3.2662e-01 (3.4536e-01)	Acc@1  91.00 ( 90.20)	Acc@5 100.00 ( 99.70)
Test: [ 90/100]	Time  0.011 ( 0.016)	Loss 2.6799e-01 (3.4002e-01)	Acc@1  92.00 ( 90.30)	Acc@5 100.00 ( 99.71)
 * Acc@1 90.400 Acc@5 99.720
### epoch[37] execution time: 12.015723705291748
EPOCH 38
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [38][  0/391]	Time  0.192 ( 0.192)	Data  0.173 ( 0.173)	Loss 5.3050e-02 (5.3050e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [38][ 10/391]	Time  0.030 ( 0.042)	Data  0.001 ( 0.018)	Loss 1.2325e-01 (7.2591e-02)	Acc@1  94.53 ( 97.37)	Acc@5 100.00 (100.00)
Epoch: [38][ 20/391]	Time  0.024 ( 0.034)	Data  0.001 ( 0.010)	Loss 1.1127e-01 (7.8022e-02)	Acc@1  96.88 ( 97.40)	Acc@5 100.00 (100.00)
Epoch: [38][ 30/391]	Time  0.023 ( 0.031)	Data  0.001 ( 0.008)	Loss 3.5820e-02 (7.4473e-02)	Acc@1  99.22 ( 97.43)	Acc@5 100.00 (100.00)
Epoch: [38][ 40/391]	Time  0.029 ( 0.030)	Data  0.004 ( 0.006)	Loss 1.1129e-01 (7.7814e-02)	Acc@1  96.88 ( 97.29)	Acc@5 100.00 (100.00)
Epoch: [38][ 50/391]	Time  0.027 ( 0.029)	Data  0.001 ( 0.005)	Loss 9.3950e-02 (7.7654e-02)	Acc@1  96.09 ( 97.37)	Acc@5 100.00 (100.00)
Epoch: [38][ 60/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.005)	Loss 9.1709e-02 (7.7259e-02)	Acc@1  95.31 ( 97.32)	Acc@5 100.00 ( 99.97)
Epoch: [38][ 70/391]	Time  0.029 ( 0.028)	Data  0.001 ( 0.005)	Loss 5.9506e-02 (7.9393e-02)	Acc@1  98.44 ( 97.34)	Acc@5 100.00 ( 99.94)
Epoch: [38][ 80/391]	Time  0.026 ( 0.028)	Data  0.002 ( 0.004)	Loss 8.9317e-02 (7.9993e-02)	Acc@1  96.09 ( 97.28)	Acc@5 100.00 ( 99.95)
Epoch: [38][ 90/391]	Time  0.035 ( 0.027)	Data  0.008 ( 0.004)	Loss 9.7794e-02 (7.8869e-02)	Acc@1  96.88 ( 97.32)	Acc@5 100.00 ( 99.96)
Epoch: [38][100/391]	Time  0.027 ( 0.027)	Data  0.002 ( 0.004)	Loss 8.4869e-02 (7.7764e-02)	Acc@1  95.31 ( 97.32)	Acc@5 100.00 ( 99.96)
Epoch: [38][110/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.004)	Loss 2.8068e-02 (7.8780e-02)	Acc@1 100.00 ( 97.33)	Acc@5 100.00 ( 99.96)
Epoch: [38][120/391]	Time  0.033 ( 0.027)	Data  0.002 ( 0.004)	Loss 6.2967e-02 (7.9465e-02)	Acc@1  98.44 ( 97.27)	Acc@5 100.00 ( 99.96)
Epoch: [38][130/391]	Time  0.029 ( 0.027)	Data  0.003 ( 0.004)	Loss 1.1491e-01 (7.9605e-02)	Acc@1  96.09 ( 97.30)	Acc@5 100.00 ( 99.96)
Epoch: [38][140/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 9.0301e-02 (8.0414e-02)	Acc@1  96.88 ( 97.29)	Acc@5 100.00 ( 99.96)
Epoch: [38][150/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 8.2137e-02 (8.0425e-02)	Acc@1  96.88 ( 97.27)	Acc@5 100.00 ( 99.96)
Epoch: [38][160/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.0329e-01 (8.2368e-02)	Acc@1  95.31 ( 97.21)	Acc@5 100.00 ( 99.96)
Epoch: [38][170/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 6.1472e-02 (8.1778e-02)	Acc@1  97.66 ( 97.24)	Acc@5 100.00 ( 99.95)
Epoch: [38][180/391]	Time  0.039 ( 0.027)	Data  0.009 ( 0.003)	Loss 4.1004e-02 (8.2444e-02)	Acc@1  98.44 ( 97.24)	Acc@5 100.00 ( 99.95)
Epoch: [38][190/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.1323e-02 (8.1857e-02)	Acc@1  99.22 ( 97.24)	Acc@5 100.00 ( 99.96)
Epoch: [38][200/391]	Time  0.030 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.4880e-02 (8.1940e-02)	Acc@1  99.22 ( 97.24)	Acc@5 100.00 ( 99.96)
Epoch: [38][210/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 9.4334e-02 (8.1663e-02)	Acc@1  97.66 ( 97.25)	Acc@5  99.22 ( 99.96)
Epoch: [38][220/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.5689e-02 (8.1254e-02)	Acc@1  97.66 ( 97.25)	Acc@5 100.00 ( 99.95)
Epoch: [38][230/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.7828e-02 (8.1635e-02)	Acc@1  99.22 ( 97.22)	Acc@5 100.00 ( 99.96)
Epoch: [38][240/391]	Time  0.027 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.9245e-02 (8.1338e-02)	Acc@1  96.88 ( 97.23)	Acc@5 100.00 ( 99.95)
Epoch: [38][250/391]	Time  0.029 ( 0.027)	Data  0.004 ( 0.003)	Loss 1.4046e-01 (8.1665e-02)	Acc@1  97.66 ( 97.22)	Acc@5 100.00 ( 99.95)
Epoch: [38][260/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.1248e-02 (8.1092e-02)	Acc@1  96.88 ( 97.23)	Acc@5 100.00 ( 99.96)
Epoch: [38][270/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.9833e-02 (8.1220e-02)	Acc@1  99.22 ( 97.24)	Acc@5 100.00 ( 99.96)
Epoch: [38][280/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.9449e-02 (8.0698e-02)	Acc@1  97.66 ( 97.26)	Acc@5 100.00 ( 99.96)
Epoch: [38][290/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.1406e-02 (8.1089e-02)	Acc@1  96.88 ( 97.25)	Acc@5 100.00 ( 99.96)
Epoch: [38][300/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.9761e-02 (8.0988e-02)	Acc@1  98.44 ( 97.25)	Acc@5 100.00 ( 99.96)
Epoch: [38][310/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.0171e-02 (8.1233e-02)	Acc@1  97.66 ( 97.25)	Acc@5 100.00 ( 99.95)
Epoch: [38][320/391]	Time  0.023 ( 0.026)	Data  0.004 ( 0.003)	Loss 5.5246e-02 (8.1173e-02)	Acc@1  96.88 ( 97.25)	Acc@5 100.00 ( 99.96)
Epoch: [38][330/391]	Time  0.036 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.6302e-02 (8.1521e-02)	Acc@1  96.88 ( 97.24)	Acc@5 100.00 ( 99.96)
Epoch: [38][340/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.4556e-02 (8.1331e-02)	Acc@1  98.44 ( 97.25)	Acc@5 100.00 ( 99.96)
Epoch: [38][350/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7147e-01 (8.1900e-02)	Acc@1  92.97 ( 97.24)	Acc@5  99.22 ( 99.95)
Epoch: [38][360/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.1092e-02 (8.1670e-02)	Acc@1  98.44 ( 97.25)	Acc@5 100.00 ( 99.95)
Epoch: [38][370/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.8587e-02 (8.1517e-02)	Acc@1  97.66 ( 97.25)	Acc@5 100.00 ( 99.96)
Epoch: [38][380/391]	Time  0.044 ( 0.026)	Data  0.013 ( 0.003)	Loss 3.2360e-02 (8.1687e-02)	Acc@1  98.44 ( 97.23)	Acc@5 100.00 ( 99.95)
Epoch: [38][390/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.6701e-02 (8.1684e-02)	Acc@1  98.75 ( 97.23)	Acc@5 100.00 ( 99.96)
## e[38] optimizer.zero_grad (sum) time: 0.1086881160736084
## e[38]       loss.backward (sum) time: 2.2428030967712402
## e[38]      optimizer.step (sum) time: 0.8624355792999268
## epoch[38] training(only) time: 10.396905183792114
# Switched to evaluate mode...
Test: [  0/100]	Time  0.162 ( 0.162)	Loss 1.9911e-01 (1.9911e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.011 ( 0.028)	Loss 3.5574e-01 (3.1362e-01)	Acc@1  91.00 ( 90.82)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.011 ( 0.021)	Loss 4.7574e-01 (3.3340e-01)	Acc@1  85.00 ( 90.19)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.011 ( 0.019)	Loss 2.8445e-01 (3.5335e-01)	Acc@1  88.00 ( 89.90)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.015 ( 0.018)	Loss 3.6450e-01 (3.6013e-01)	Acc@1  91.00 ( 89.95)	Acc@5  99.00 ( 99.61)
Test: [ 50/100]	Time  0.011 ( 0.017)	Loss 1.7951e-01 (3.5330e-01)	Acc@1  95.00 ( 90.08)	Acc@5 100.00 ( 99.59)
Test: [ 60/100]	Time  0.011 ( 0.016)	Loss 3.5274e-01 (3.4913e-01)	Acc@1  88.00 ( 90.11)	Acc@5 100.00 ( 99.61)
Test: [ 70/100]	Time  0.011 ( 0.016)	Loss 6.1508e-01 (3.4669e-01)	Acc@1  90.00 ( 90.30)	Acc@5  98.00 ( 99.54)
Test: [ 80/100]	Time  0.023 ( 0.016)	Loss 2.8204e-01 (3.4255e-01)	Acc@1  90.00 ( 90.31)	Acc@5 100.00 ( 99.59)
Test: [ 90/100]	Time  0.011 ( 0.016)	Loss 2.6948e-01 (3.3988e-01)	Acc@1  92.00 ( 90.34)	Acc@5 100.00 ( 99.60)
 * Acc@1 90.390 Acc@5 99.620
### epoch[38] execution time: 12.054990291595459
EPOCH 39
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [39][  0/391]	Time  0.196 ( 0.196)	Data  0.172 ( 0.172)	Loss 9.9985e-02 (9.9985e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [39][ 10/391]	Time  0.025 ( 0.041)	Data  0.001 ( 0.017)	Loss 4.4587e-02 (9.3079e-02)	Acc@1  98.44 ( 96.88)	Acc@5 100.00 ( 99.93)
Epoch: [39][ 20/391]	Time  0.026 ( 0.034)	Data  0.001 ( 0.010)	Loss 8.5897e-02 (9.4448e-02)	Acc@1  96.88 ( 96.65)	Acc@5 100.00 ( 99.96)
Epoch: [39][ 30/391]	Time  0.023 ( 0.031)	Data  0.000 ( 0.007)	Loss 7.7818e-02 (8.4983e-02)	Acc@1  99.22 ( 97.13)	Acc@5 100.00 ( 99.97)
Epoch: [39][ 40/391]	Time  0.024 ( 0.030)	Data  0.001 ( 0.006)	Loss 6.4483e-02 (8.4769e-02)	Acc@1  96.88 ( 97.18)	Acc@5 100.00 ( 99.96)
Epoch: [39][ 50/391]	Time  0.023 ( 0.029)	Data  0.001 ( 0.005)	Loss 4.1872e-02 (8.3400e-02)	Acc@1  98.44 ( 97.17)	Acc@5 100.00 ( 99.95)
Epoch: [39][ 60/391]	Time  0.023 ( 0.028)	Data  0.002 ( 0.005)	Loss 1.1050e-01 (8.4379e-02)	Acc@1  96.88 ( 97.22)	Acc@5 100.00 ( 99.94)
Epoch: [39][ 70/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.004)	Loss 6.1353e-02 (8.1220e-02)	Acc@1  96.88 ( 97.30)	Acc@5 100.00 ( 99.93)
Epoch: [39][ 80/391]	Time  0.029 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.8217e-02 (8.2002e-02)	Acc@1 100.00 ( 97.23)	Acc@5 100.00 ( 99.94)
Epoch: [39][ 90/391]	Time  0.025 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.9085e-01 (8.0984e-02)	Acc@1  95.31 ( 97.30)	Acc@5  99.22 ( 99.94)
Epoch: [39][100/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.2657e-01 (8.1853e-02)	Acc@1  94.53 ( 97.27)	Acc@5 100.00 ( 99.93)
Epoch: [39][110/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 8.8601e-02 (8.0851e-02)	Acc@1  96.88 ( 97.27)	Acc@5 100.00 ( 99.94)
Epoch: [39][120/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.5731e-02 (8.0254e-02)	Acc@1  98.44 ( 97.33)	Acc@5 100.00 ( 99.93)
Epoch: [39][130/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 9.5496e-02 (7.8591e-02)	Acc@1  94.53 ( 97.36)	Acc@5 100.00 ( 99.93)
Epoch: [39][140/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.8676e-02 (7.7884e-02)	Acc@1  98.44 ( 97.41)	Acc@5 100.00 ( 99.94)
Epoch: [39][150/391]	Time  0.025 ( 0.027)	Data  0.000 ( 0.003)	Loss 1.7971e-01 (7.8755e-02)	Acc@1  94.53 ( 97.33)	Acc@5 100.00 ( 99.94)
Epoch: [39][160/391]	Time  0.032 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.9976e-02 (7.8227e-02)	Acc@1 100.00 ( 97.37)	Acc@5 100.00 ( 99.95)
Epoch: [39][170/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.7247e-02 (7.8609e-02)	Acc@1  96.88 ( 97.33)	Acc@5 100.00 ( 99.95)
Epoch: [39][180/391]	Time  0.022 ( 0.027)	Data  0.000 ( 0.003)	Loss 7.6970e-02 (7.8813e-02)	Acc@1  96.09 ( 97.32)	Acc@5 100.00 ( 99.95)
Epoch: [39][190/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.2548e-02 (7.9565e-02)	Acc@1  98.44 ( 97.30)	Acc@5 100.00 ( 99.96)
Epoch: [39][200/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.6855e-02 (7.9741e-02)	Acc@1  97.66 ( 97.28)	Acc@5 100.00 ( 99.96)
Epoch: [39][210/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 8.7877e-02 (8.0010e-02)	Acc@1  97.66 ( 97.25)	Acc@5 100.00 ( 99.96)
Epoch: [39][220/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.2368e-02 (8.0502e-02)	Acc@1  97.66 ( 97.24)	Acc@5 100.00 ( 99.96)
Epoch: [39][230/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.1923e-02 (8.0939e-02)	Acc@1  97.66 ( 97.22)	Acc@5 100.00 ( 99.96)
Epoch: [39][240/391]	Time  0.029 ( 0.026)	Data  0.004 ( 0.003)	Loss 6.8849e-02 (8.0824e-02)	Acc@1  96.88 ( 97.24)	Acc@5 100.00 ( 99.96)
Epoch: [39][250/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.0225e-02 (8.1415e-02)	Acc@1 100.00 ( 97.24)	Acc@5 100.00 ( 99.96)
Epoch: [39][260/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.1920e-02 (8.1591e-02)	Acc@1  98.44 ( 97.25)	Acc@5 100.00 ( 99.96)
Epoch: [39][270/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5616e-01 (8.0791e-02)	Acc@1  95.31 ( 97.29)	Acc@5  99.22 ( 99.96)
Epoch: [39][280/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.9568e-02 (8.0522e-02)	Acc@1  96.88 ( 97.29)	Acc@5 100.00 ( 99.96)
Epoch: [39][290/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.5705e-02 (8.0743e-02)	Acc@1  97.66 ( 97.28)	Acc@5 100.00 ( 99.96)
Epoch: [39][300/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.1413e-02 (8.0626e-02)	Acc@1  98.44 ( 97.30)	Acc@5 100.00 ( 99.95)
Epoch: [39][310/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.2843e-02 (8.0541e-02)	Acc@1  96.88 ( 97.30)	Acc@5 100.00 ( 99.95)
Epoch: [39][320/391]	Time  0.023 ( 0.026)	Data  0.000 ( 0.003)	Loss 5.7063e-02 (7.9839e-02)	Acc@1  98.44 ( 97.33)	Acc@5 100.00 ( 99.95)
Epoch: [39][330/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4737e-01 (8.0138e-02)	Acc@1  95.31 ( 97.32)	Acc@5 100.00 ( 99.96)
Epoch: [39][340/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.9420e-02 (8.0157e-02)	Acc@1  97.66 ( 97.33)	Acc@5 100.00 ( 99.95)
Epoch: [39][350/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.8036e-02 (8.0017e-02)	Acc@1  98.44 ( 97.33)	Acc@5 100.00 ( 99.96)
Epoch: [39][360/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 6.4762e-02 (7.9705e-02)	Acc@1  96.88 ( 97.31)	Acc@5 100.00 ( 99.96)
Epoch: [39][370/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.2499e-02 (7.9980e-02)	Acc@1  99.22 ( 97.31)	Acc@5 100.00 ( 99.96)
Epoch: [39][380/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.9097e-02 (8.0196e-02)	Acc@1  96.88 ( 97.30)	Acc@5 100.00 ( 99.96)
Epoch: [39][390/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2767e-01 (8.0699e-02)	Acc@1  97.50 ( 97.29)	Acc@5 100.00 ( 99.96)
## e[39] optimizer.zero_grad (sum) time: 0.10839581489562988
## e[39]       loss.backward (sum) time: 2.232450485229492
## e[39]      optimizer.step (sum) time: 0.8878927230834961
## epoch[39] training(only) time: 10.395939350128174
# Switched to evaluate mode...
Test: [  0/100]	Time  0.158 ( 0.158)	Loss 2.4617e-01 (2.4617e-01)	Acc@1  94.00 ( 94.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.011 ( 0.027)	Loss 4.4780e-01 (3.2196e-01)	Acc@1  89.00 ( 91.09)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.011 ( 0.021)	Loss 4.6011e-01 (3.4831e-01)	Acc@1  86.00 ( 89.81)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.019 ( 0.019)	Loss 3.1004e-01 (3.6718e-01)	Acc@1  88.00 ( 89.55)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.012 ( 0.018)	Loss 4.2222e-01 (3.7428e-01)	Acc@1  87.00 ( 89.56)	Acc@5  99.00 ( 99.66)
Test: [ 50/100]	Time  0.011 ( 0.017)	Loss 1.6448e-01 (3.6608e-01)	Acc@1  96.00 ( 89.76)	Acc@5  99.00 ( 99.59)
Test: [ 60/100]	Time  0.012 ( 0.017)	Loss 3.0138e-01 (3.5872e-01)	Acc@1  93.00 ( 90.03)	Acc@5 100.00 ( 99.61)
Test: [ 70/100]	Time  0.016 ( 0.017)	Loss 5.5702e-01 (3.5409e-01)	Acc@1  89.00 ( 90.27)	Acc@5  99.00 ( 99.58)
Test: [ 80/100]	Time  0.011 ( 0.016)	Loss 2.9954e-01 (3.5002e-01)	Acc@1  90.00 ( 90.32)	Acc@5 100.00 ( 99.63)
Test: [ 90/100]	Time  0.015 ( 0.016)	Loss 3.2203e-01 (3.4791e-01)	Acc@1  90.00 ( 90.27)	Acc@5 100.00 ( 99.66)
 * Acc@1 90.400 Acc@5 99.670
### epoch[39] execution time: 12.127054691314697
EPOCH 40
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [40][  0/391]	Time  0.192 ( 0.192)	Data  0.173 ( 0.173)	Loss 3.8482e-02 (3.8482e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [40][ 10/391]	Time  0.022 ( 0.041)	Data  0.000 ( 0.018)	Loss 1.0313e-01 (6.9322e-02)	Acc@1  96.09 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [40][ 20/391]	Time  0.031 ( 0.035)	Data  0.000 ( 0.011)	Loss 5.6500e-02 (7.0788e-02)	Acc@1  98.44 ( 97.54)	Acc@5 100.00 ( 99.96)
Epoch: [40][ 30/391]	Time  0.032 ( 0.032)	Data  0.001 ( 0.008)	Loss 8.0890e-02 (7.4989e-02)	Acc@1  97.66 ( 97.43)	Acc@5 100.00 ( 99.97)
Epoch: [40][ 40/391]	Time  0.024 ( 0.030)	Data  0.001 ( 0.006)	Loss 8.0594e-02 (6.9901e-02)	Acc@1  96.09 ( 97.58)	Acc@5 100.00 ( 99.98)
Epoch: [40][ 50/391]	Time  0.031 ( 0.029)	Data  0.002 ( 0.006)	Loss 5.5785e-02 (6.9931e-02)	Acc@1  97.66 ( 97.59)	Acc@5 100.00 ( 99.97)
Epoch: [40][ 60/391]	Time  0.025 ( 0.029)	Data  0.002 ( 0.005)	Loss 8.3765e-02 (6.9977e-02)	Acc@1  97.66 ( 97.71)	Acc@5 100.00 ( 99.97)
Epoch: [40][ 70/391]	Time  0.023 ( 0.028)	Data  0.002 ( 0.005)	Loss 1.2567e-01 (7.3070e-02)	Acc@1  95.31 ( 97.61)	Acc@5 100.00 ( 99.96)
Epoch: [40][ 80/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 3.0856e-02 (7.2597e-02)	Acc@1  98.44 ( 97.66)	Acc@5 100.00 ( 99.96)
Epoch: [40][ 90/391]	Time  0.032 ( 0.028)	Data  0.001 ( 0.004)	Loss 8.0053e-02 (7.2972e-02)	Acc@1  96.88 ( 97.65)	Acc@5 100.00 ( 99.97)
Epoch: [40][100/391]	Time  0.029 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.2009e-01 (7.4297e-02)	Acc@1  96.09 ( 97.59)	Acc@5 100.00 ( 99.97)
Epoch: [40][110/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.004)	Loss 7.8703e-02 (7.4241e-02)	Acc@1  95.31 ( 97.53)	Acc@5 100.00 ( 99.97)
Epoch: [40][120/391]	Time  0.036 ( 0.027)	Data  0.005 ( 0.004)	Loss 1.0494e-01 (7.3818e-02)	Acc@1  97.66 ( 97.54)	Acc@5 100.00 ( 99.97)
Epoch: [40][130/391]	Time  0.030 ( 0.027)	Data  0.001 ( 0.004)	Loss 7.6964e-02 (7.3056e-02)	Acc@1  98.44 ( 97.55)	Acc@5 100.00 ( 99.97)
Epoch: [40][140/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.1527e-02 (7.3572e-02)	Acc@1  97.66 ( 97.55)	Acc@5 100.00 ( 99.97)
Epoch: [40][150/391]	Time  0.023 ( 0.027)	Data  0.003 ( 0.003)	Loss 5.7093e-02 (7.2915e-02)	Acc@1  98.44 ( 97.58)	Acc@5 100.00 ( 99.97)
Epoch: [40][160/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.7486e-02 (7.2802e-02)	Acc@1  98.44 ( 97.60)	Acc@5 100.00 ( 99.97)
Epoch: [40][170/391]	Time  0.032 ( 0.027)	Data  0.003 ( 0.003)	Loss 4.0976e-02 (7.3660e-02)	Acc@1  98.44 ( 97.55)	Acc@5 100.00 ( 99.96)
Epoch: [40][180/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.7695e-01 (7.4465e-02)	Acc@1  92.19 ( 97.53)	Acc@5 100.00 ( 99.96)
Epoch: [40][190/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 1.1301e-01 (7.5937e-02)	Acc@1  94.53 ( 97.48)	Acc@5 100.00 ( 99.96)
Epoch: [40][200/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.4503e-02 (7.5713e-02)	Acc@1  97.66 ( 97.49)	Acc@5 100.00 ( 99.96)
Epoch: [40][210/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.1186e-01 (7.5846e-02)	Acc@1  96.09 ( 97.49)	Acc@5 100.00 ( 99.96)
Epoch: [40][220/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.4636e-02 (7.5079e-02)	Acc@1  96.88 ( 97.51)	Acc@5 100.00 ( 99.96)
Epoch: [40][230/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.7467e-02 (7.4558e-02)	Acc@1  99.22 ( 97.53)	Acc@5 100.00 ( 99.96)
Epoch: [40][240/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.3879e-01 (7.4179e-02)	Acc@1  95.31 ( 97.52)	Acc@5 100.00 ( 99.96)
Epoch: [40][250/391]	Time  0.024 ( 0.027)	Data  0.000 ( 0.003)	Loss 6.5179e-02 (7.4201e-02)	Acc@1  96.88 ( 97.52)	Acc@5 100.00 ( 99.97)
Epoch: [40][260/391]	Time  0.037 ( 0.027)	Data  0.005 ( 0.003)	Loss 8.6193e-02 (7.5219e-02)	Acc@1  96.88 ( 97.50)	Acc@5 100.00 ( 99.97)
Epoch: [40][270/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.4144e-02 (7.4900e-02)	Acc@1  99.22 ( 97.51)	Acc@5 100.00 ( 99.97)
Epoch: [40][280/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.3426e-02 (7.5187e-02)	Acc@1  97.66 ( 97.48)	Acc@5 100.00 ( 99.97)
Epoch: [40][290/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.6973e-02 (7.5663e-02)	Acc@1  96.09 ( 97.47)	Acc@5 100.00 ( 99.97)
Epoch: [40][300/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.8756e-02 (7.5450e-02)	Acc@1  97.66 ( 97.46)	Acc@5 100.00 ( 99.97)
Epoch: [40][310/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1136e-01 (7.5298e-02)	Acc@1  96.88 ( 97.49)	Acc@5 100.00 ( 99.97)
Epoch: [40][320/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.4165e-02 (7.5141e-02)	Acc@1  99.22 ( 97.50)	Acc@5 100.00 ( 99.97)
Epoch: [40][330/391]	Time  0.030 ( 0.026)	Data  0.006 ( 0.003)	Loss 1.0893e-01 (7.5372e-02)	Acc@1  96.09 ( 97.49)	Acc@5 100.00 ( 99.97)
Epoch: [40][340/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5272e-02 (7.5684e-02)	Acc@1  99.22 ( 97.49)	Acc@5 100.00 ( 99.97)
Epoch: [40][350/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 4.2321e-02 (7.5287e-02)	Acc@1  97.66 ( 97.50)	Acc@5 100.00 ( 99.97)
Epoch: [40][360/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.1719e-02 (7.5478e-02)	Acc@1  99.22 ( 97.49)	Acc@5 100.00 ( 99.97)
Epoch: [40][370/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 4.1067e-02 (7.5356e-02)	Acc@1  98.44 ( 97.49)	Acc@5 100.00 ( 99.97)
Epoch: [40][380/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 5.7471e-02 (7.5375e-02)	Acc@1  97.66 ( 97.50)	Acc@5 100.00 ( 99.97)
Epoch: [40][390/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.0421e-02 (7.5928e-02)	Acc@1  96.25 ( 97.49)	Acc@5 100.00 ( 99.97)
## e[40] optimizer.zero_grad (sum) time: 0.10685086250305176
## e[40]       loss.backward (sum) time: 2.2822046279907227
## e[40]      optimizer.step (sum) time: 0.8713099956512451
## epoch[40] training(only) time: 10.429879903793335
# Switched to evaluate mode...
Test: [  0/100]	Time  0.200 ( 0.200)	Loss 2.4286e-01 (2.4286e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.011 ( 0.032)	Loss 3.5473e-01 (3.3167e-01)	Acc@1  90.00 ( 90.73)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.011 ( 0.023)	Loss 3.6277e-01 (3.5074e-01)	Acc@1  89.00 ( 90.05)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.022 ( 0.020)	Loss 3.1866e-01 (3.7167e-01)	Acc@1  87.00 ( 89.77)	Acc@5 100.00 ( 99.58)
Test: [ 40/100]	Time  0.014 ( 0.019)	Loss 4.6280e-01 (3.7757e-01)	Acc@1  90.00 ( 89.76)	Acc@5  99.00 ( 99.61)
Test: [ 50/100]	Time  0.014 ( 0.018)	Loss 1.9478e-01 (3.7150e-01)	Acc@1  95.00 ( 89.92)	Acc@5  99.00 ( 99.59)
Test: [ 60/100]	Time  0.012 ( 0.017)	Loss 3.5070e-01 (3.6560e-01)	Acc@1  90.00 ( 90.13)	Acc@5 100.00 ( 99.61)
Test: [ 70/100]	Time  0.021 ( 0.017)	Loss 5.3941e-01 (3.6323e-01)	Acc@1  89.00 ( 90.21)	Acc@5  99.00 ( 99.58)
Test: [ 80/100]	Time  0.010 ( 0.017)	Loss 3.2948e-01 (3.6048e-01)	Acc@1  92.00 ( 90.25)	Acc@5  99.00 ( 99.60)
Test: [ 90/100]	Time  0.022 ( 0.016)	Loss 3.0140e-01 (3.5784e-01)	Acc@1  94.00 ( 90.29)	Acc@5 100.00 ( 99.64)
 * Acc@1 90.450 Acc@5 99.660
### epoch[40] execution time: 12.188296556472778
EPOCH 41
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [41][  0/391]	Time  0.189 ( 0.189)	Data  0.166 ( 0.166)	Loss 4.8475e-02 (4.8475e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [41][ 10/391]	Time  0.023 ( 0.040)	Data  0.001 ( 0.017)	Loss 6.7411e-02 (6.3815e-02)	Acc@1  98.44 ( 98.01)	Acc@5 100.00 (100.00)
Epoch: [41][ 20/391]	Time  0.023 ( 0.033)	Data  0.001 ( 0.010)	Loss 3.6509e-02 (6.1893e-02)	Acc@1  99.22 ( 98.03)	Acc@5 100.00 ( 99.96)
Epoch: [41][ 30/391]	Time  0.028 ( 0.031)	Data  0.001 ( 0.007)	Loss 6.3477e-02 (6.7905e-02)	Acc@1  98.44 ( 97.73)	Acc@5 100.00 ( 99.95)
Epoch: [41][ 40/391]	Time  0.024 ( 0.029)	Data  0.001 ( 0.006)	Loss 5.9377e-02 (6.8056e-02)	Acc@1  98.44 ( 97.75)	Acc@5 100.00 ( 99.96)
Epoch: [41][ 50/391]	Time  0.026 ( 0.029)	Data  0.005 ( 0.005)	Loss 8.5550e-02 (6.9042e-02)	Acc@1  97.66 ( 97.72)	Acc@5 100.00 ( 99.95)
Epoch: [41][ 60/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.005)	Loss 8.1158e-02 (6.8652e-02)	Acc@1  97.66 ( 97.72)	Acc@5 100.00 ( 99.95)
Epoch: [41][ 70/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.0599e-01 (6.7877e-02)	Acc@1  95.31 ( 97.67)	Acc@5 100.00 ( 99.96)
Epoch: [41][ 80/391]	Time  0.023 ( 0.028)	Data  0.002 ( 0.004)	Loss 6.7983e-02 (6.7058e-02)	Acc@1  99.22 ( 97.67)	Acc@5  99.22 ( 99.94)
Epoch: [41][ 90/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.0447e-01 (6.6076e-02)	Acc@1  96.88 ( 97.69)	Acc@5 100.00 ( 99.95)
Epoch: [41][100/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.1831e-01 (6.6673e-02)	Acc@1  94.53 ( 97.69)	Acc@5 100.00 ( 99.95)
Epoch: [41][110/391]	Time  0.027 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.6924e-02 (6.7259e-02)	Acc@1  98.44 ( 97.66)	Acc@5 100.00 ( 99.96)
Epoch: [41][120/391]	Time  0.026 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.3654e-01 (6.6828e-02)	Acc@1  96.88 ( 97.69)	Acc@5 100.00 ( 99.96)
Epoch: [41][130/391]	Time  0.027 ( 0.027)	Data  0.000 ( 0.003)	Loss 1.2750e-01 (6.6307e-02)	Acc@1  96.88 ( 97.70)	Acc@5 100.00 ( 99.96)
Epoch: [41][140/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.9014e-02 (6.6544e-02)	Acc@1  96.88 ( 97.71)	Acc@5 100.00 ( 99.97)
Epoch: [41][150/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.9199e-02 (6.6119e-02)	Acc@1  98.44 ( 97.74)	Acc@5 100.00 ( 99.97)
Epoch: [41][160/391]	Time  0.024 ( 0.027)	Data  0.002 ( 0.003)	Loss 1.0764e-01 (6.7604e-02)	Acc@1  96.09 ( 97.71)	Acc@5 100.00 ( 99.96)
Epoch: [41][170/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.5116e-02 (6.7137e-02)	Acc@1  98.44 ( 97.73)	Acc@5 100.00 ( 99.96)
Epoch: [41][180/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.0834e-01 (6.8242e-02)	Acc@1  96.09 ( 97.69)	Acc@5 100.00 ( 99.96)
Epoch: [41][190/391]	Time  0.024 ( 0.027)	Data  0.000 ( 0.003)	Loss 5.3068e-02 (6.9810e-02)	Acc@1  97.66 ( 97.64)	Acc@5 100.00 ( 99.96)
Epoch: [41][200/391]	Time  0.028 ( 0.026)	Data  0.004 ( 0.003)	Loss 7.5409e-02 (7.0255e-02)	Acc@1  96.88 ( 97.65)	Acc@5 100.00 ( 99.96)
Epoch: [41][210/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6647e-01 (7.0620e-02)	Acc@1  93.75 ( 97.61)	Acc@5 100.00 ( 99.96)
Epoch: [41][220/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.7283e-02 (7.0470e-02)	Acc@1  99.22 ( 97.61)	Acc@5 100.00 ( 99.96)
Epoch: [41][230/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.7544e-02 (7.0953e-02)	Acc@1  98.44 ( 97.59)	Acc@5 100.00 ( 99.97)
Epoch: [41][240/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2502e-01 (7.1956e-02)	Acc@1  94.53 ( 97.55)	Acc@5 100.00 ( 99.96)
Epoch: [41][250/391]	Time  0.028 ( 0.026)	Data  0.002 ( 0.003)	Loss 5.1764e-02 (7.2298e-02)	Acc@1  98.44 ( 97.55)	Acc@5 100.00 ( 99.96)
Epoch: [41][260/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3282e-02 (7.1674e-02)	Acc@1  99.22 ( 97.58)	Acc@5 100.00 ( 99.96)
Epoch: [41][270/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.9119e-02 (7.1914e-02)	Acc@1  98.44 ( 97.57)	Acc@5 100.00 ( 99.96)
Epoch: [41][280/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.7884e-02 (7.1385e-02)	Acc@1  98.44 ( 97.58)	Acc@5 100.00 ( 99.96)
Epoch: [41][290/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.4267e-02 (7.1976e-02)	Acc@1  96.88 ( 97.55)	Acc@5 100.00 ( 99.96)
Epoch: [41][300/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.1512e-02 (7.1898e-02)	Acc@1  97.66 ( 97.56)	Acc@5 100.00 ( 99.96)
Epoch: [41][310/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.7436e-02 (7.2123e-02)	Acc@1  96.88 ( 97.54)	Acc@5 100.00 ( 99.96)
Epoch: [41][320/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7241e-01 (7.1959e-02)	Acc@1  96.09 ( 97.54)	Acc@5 100.00 ( 99.97)
Epoch: [41][330/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.9015e-02 (7.1796e-02)	Acc@1  98.44 ( 97.55)	Acc@5 100.00 ( 99.97)
Epoch: [41][340/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.9155e-02 (7.1630e-02)	Acc@1  96.88 ( 97.54)	Acc@5 100.00 ( 99.97)
Epoch: [41][350/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.4646e-02 (7.2036e-02)	Acc@1  97.66 ( 97.53)	Acc@5 100.00 ( 99.97)
Epoch: [41][360/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.0370e-02 (7.1463e-02)	Acc@1  98.44 ( 97.55)	Acc@5 100.00 ( 99.97)
Epoch: [41][370/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.7099e-02 (7.1369e-02)	Acc@1  99.22 ( 97.55)	Acc@5 100.00 ( 99.97)
Epoch: [41][380/391]	Time  0.026 ( 0.026)	Data  0.005 ( 0.003)	Loss 1.0546e-01 (7.1634e-02)	Acc@1  97.66 ( 97.56)	Acc@5 100.00 ( 99.97)
Epoch: [41][390/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.8721e-02 (7.1774e-02)	Acc@1  98.75 ( 97.55)	Acc@5 100.00 ( 99.97)
## e[41] optimizer.zero_grad (sum) time: 0.10905671119689941
## e[41]       loss.backward (sum) time: 2.281625270843506
## e[41]      optimizer.step (sum) time: 0.8790674209594727
## epoch[41] training(only) time: 10.373710632324219
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 2.6240e-01 (2.6240e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.011 ( 0.028)	Loss 4.7923e-01 (3.3951e-01)	Acc@1  88.00 ( 90.36)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.016 ( 0.021)	Loss 4.8072e-01 (3.5727e-01)	Acc@1  86.00 ( 90.14)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.011 ( 0.019)	Loss 2.9646e-01 (3.6544e-01)	Acc@1  89.00 ( 90.06)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.011 ( 0.018)	Loss 4.8070e-01 (3.8302e-01)	Acc@1  89.00 ( 89.76)	Acc@5  98.00 ( 99.63)
Test: [ 50/100]	Time  0.029 ( 0.017)	Loss 1.8730e-01 (3.7438e-01)	Acc@1  95.00 ( 89.96)	Acc@5 100.00 ( 99.55)
Test: [ 60/100]	Time  0.018 ( 0.017)	Loss 3.3700e-01 (3.6908e-01)	Acc@1  89.00 ( 90.15)	Acc@5 100.00 ( 99.57)
Test: [ 70/100]	Time  0.022 ( 0.016)	Loss 6.7436e-01 (3.6841e-01)	Acc@1  89.00 ( 90.31)	Acc@5  99.00 ( 99.58)
Test: [ 80/100]	Time  0.027 ( 0.016)	Loss 2.8051e-01 (3.6558e-01)	Acc@1  93.00 ( 90.38)	Acc@5  99.00 ( 99.62)
Test: [ 90/100]	Time  0.012 ( 0.016)	Loss 3.2014e-01 (3.6179e-01)	Acc@1  91.00 ( 90.35)	Acc@5 100.00 ( 99.65)
 * Acc@1 90.460 Acc@5 99.670
### epoch[41] execution time: 12.080666065216064
EPOCH 42
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [42][  0/391]	Time  0.194 ( 0.194)	Data  0.173 ( 0.173)	Loss 1.0868e-01 (1.0868e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [42][ 10/391]	Time  0.024 ( 0.042)	Data  0.001 ( 0.017)	Loss 1.0685e-01 (7.2381e-02)	Acc@1  96.88 ( 97.51)	Acc@5 100.00 ( 99.93)
Epoch: [42][ 20/391]	Time  0.025 ( 0.034)	Data  0.001 ( 0.010)	Loss 7.2160e-02 (6.6962e-02)	Acc@1  97.66 ( 97.73)	Acc@5 100.00 ( 99.96)
Epoch: [42][ 30/391]	Time  0.031 ( 0.032)	Data  0.001 ( 0.007)	Loss 1.3698e-01 (6.8662e-02)	Acc@1  96.09 ( 97.81)	Acc@5  99.22 ( 99.95)
Epoch: [42][ 40/391]	Time  0.023 ( 0.030)	Data  0.002 ( 0.006)	Loss 6.6758e-02 (6.6214e-02)	Acc@1  96.88 ( 97.81)	Acc@5 100.00 ( 99.96)
Epoch: [42][ 50/391]	Time  0.023 ( 0.029)	Data  0.001 ( 0.005)	Loss 5.3722e-02 (6.8475e-02)	Acc@1  97.66 ( 97.69)	Acc@5 100.00 ( 99.95)
Epoch: [42][ 60/391]	Time  0.036 ( 0.029)	Data  0.001 ( 0.005)	Loss 2.4792e-02 (6.9151e-02)	Acc@1  99.22 ( 97.62)	Acc@5 100.00 ( 99.96)
Epoch: [42][ 70/391]	Time  0.023 ( 0.028)	Data  0.002 ( 0.004)	Loss 4.8898e-02 (6.9632e-02)	Acc@1  98.44 ( 97.62)	Acc@5 100.00 ( 99.97)
Epoch: [42][ 80/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 5.5321e-02 (6.8656e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 ( 99.97)
Epoch: [42][ 90/391]	Time  0.030 ( 0.028)	Data  0.002 ( 0.004)	Loss 7.8526e-02 (6.8511e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 ( 99.97)
Epoch: [42][100/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 7.7949e-02 (6.8278e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 ( 99.98)
Epoch: [42][110/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.8247e-02 (6.8502e-02)	Acc@1  99.22 ( 97.64)	Acc@5 100.00 ( 99.98)
Epoch: [42][120/391]	Time  0.033 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.7818e-02 (6.7550e-02)	Acc@1  96.88 ( 97.68)	Acc@5 100.00 ( 99.97)
Epoch: [42][130/391]	Time  0.026 ( 0.027)	Data  0.001 ( 0.003)	Loss 8.5450e-02 (6.8425e-02)	Acc@1  98.44 ( 97.66)	Acc@5 100.00 ( 99.98)
Epoch: [42][140/391]	Time  0.027 ( 0.027)	Data  0.002 ( 0.003)	Loss 1.2680e-02 (6.8942e-02)	Acc@1 100.00 ( 97.63)	Acc@5 100.00 ( 99.97)
Epoch: [42][150/391]	Time  0.028 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.9464e-02 (6.8438e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 ( 99.97)
Epoch: [42][160/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.9232e-02 (6.8193e-02)	Acc@1  98.44 ( 97.67)	Acc@5 100.00 ( 99.98)
Epoch: [42][170/391]	Time  0.038 ( 0.027)	Data  0.003 ( 0.003)	Loss 3.9910e-02 (6.8217e-02)	Acc@1  98.44 ( 97.66)	Acc@5 100.00 ( 99.98)
Epoch: [42][180/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.2959e-02 (6.8053e-02)	Acc@1  99.22 ( 97.67)	Acc@5 100.00 ( 99.98)
Epoch: [42][190/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.2582e-02 (6.8678e-02)	Acc@1  96.09 ( 97.64)	Acc@5 100.00 ( 99.98)
Epoch: [42][200/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.7699e-02 (6.8713e-02)	Acc@1  99.22 ( 97.63)	Acc@5 100.00 ( 99.98)
Epoch: [42][210/391]	Time  0.032 ( 0.027)	Data  0.002 ( 0.003)	Loss 6.6819e-02 (6.9248e-02)	Acc@1  97.66 ( 97.59)	Acc@5 100.00 ( 99.98)
Epoch: [42][220/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.4470e-02 (6.9335e-02)	Acc@1  97.66 ( 97.61)	Acc@5 100.00 ( 99.98)
Epoch: [42][230/391]	Time  0.028 ( 0.027)	Data  0.009 ( 0.003)	Loss 6.0152e-02 (6.9729e-02)	Acc@1  97.66 ( 97.60)	Acc@5 100.00 ( 99.98)
Epoch: [42][240/391]	Time  0.024 ( 0.027)	Data  0.004 ( 0.003)	Loss 5.2848e-02 (6.9469e-02)	Acc@1  98.44 ( 97.61)	Acc@5 100.00 ( 99.98)
Epoch: [42][250/391]	Time  0.033 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.2560e-01 (6.9611e-02)	Acc@1  95.31 ( 97.60)	Acc@5 100.00 ( 99.98)
Epoch: [42][260/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 4.4046e-02 (6.9113e-02)	Acc@1  99.22 ( 97.62)	Acc@5 100.00 ( 99.98)
Epoch: [42][270/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.1521e-01 (6.9764e-02)	Acc@1  96.09 ( 97.61)	Acc@5 100.00 ( 99.98)
Epoch: [42][280/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.4633e-02 (6.9450e-02)	Acc@1  98.44 ( 97.61)	Acc@5 100.00 ( 99.98)
Epoch: [42][290/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.4932e-02 (7.0134e-02)	Acc@1  98.44 ( 97.59)	Acc@5 100.00 ( 99.98)
Epoch: [42][300/391]	Time  0.024 ( 0.026)	Data  0.002 ( 0.003)	Loss 8.8207e-02 (7.0318e-02)	Acc@1  96.09 ( 97.58)	Acc@5 100.00 ( 99.98)
Epoch: [42][310/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.0491e-02 (7.0373e-02)	Acc@1  96.88 ( 97.58)	Acc@5 100.00 ( 99.98)
Epoch: [42][320/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.7748e-02 (7.0401e-02)	Acc@1  96.88 ( 97.56)	Acc@5 100.00 ( 99.98)
Epoch: [42][330/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2499e-01 (7.0805e-02)	Acc@1  97.66 ( 97.55)	Acc@5 100.00 ( 99.98)
Epoch: [42][340/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.9348e-02 (7.0869e-02)	Acc@1  99.22 ( 97.55)	Acc@5 100.00 ( 99.98)
Epoch: [42][350/391]	Time  0.027 ( 0.026)	Data  0.003 ( 0.003)	Loss 2.1481e-01 (7.1278e-02)	Acc@1  95.31 ( 97.55)	Acc@5 100.00 ( 99.98)
Epoch: [42][360/391]	Time  0.023 ( 0.026)	Data  0.003 ( 0.003)	Loss 7.8268e-02 (7.0573e-02)	Acc@1  96.88 ( 97.58)	Acc@5 100.00 ( 99.98)
Epoch: [42][370/391]	Time  0.031 ( 0.026)	Data  0.004 ( 0.003)	Loss 5.6448e-02 (7.0976e-02)	Acc@1  96.88 ( 97.56)	Acc@5 100.00 ( 99.98)
Epoch: [42][380/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.5253e-02 (7.0719e-02)	Acc@1  98.44 ( 97.57)	Acc@5 100.00 ( 99.98)
Epoch: [42][390/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5895e-02 (6.9985e-02)	Acc@1 100.00 ( 97.60)	Acc@5 100.00 ( 99.98)
## e[42] optimizer.zero_grad (sum) time: 0.10850358009338379
## e[42]       loss.backward (sum) time: 2.291020154953003
## e[42]      optimizer.step (sum) time: 0.874107837677002
## epoch[42] training(only) time: 10.40563154220581
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 2.2382e-01 (2.2382e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.011 ( 0.028)	Loss 4.6486e-01 (3.4326e-01)	Acc@1  88.00 ( 89.82)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.015 ( 0.021)	Loss 4.6589e-01 (3.5485e-01)	Acc@1  85.00 ( 89.52)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.014 ( 0.019)	Loss 3.3270e-01 (3.6434e-01)	Acc@1  90.00 ( 89.65)	Acc@5  99.00 ( 99.65)
Test: [ 40/100]	Time  0.011 ( 0.018)	Loss 5.0604e-01 (3.7857e-01)	Acc@1  89.00 ( 89.56)	Acc@5  99.00 ( 99.61)
Test: [ 50/100]	Time  0.020 ( 0.018)	Loss 1.6980e-01 (3.6789e-01)	Acc@1  96.00 ( 89.86)	Acc@5 100.00 ( 99.59)
Test: [ 60/100]	Time  0.027 ( 0.017)	Loss 3.2840e-01 (3.6461e-01)	Acc@1  91.00 ( 89.97)	Acc@5 100.00 ( 99.61)
Test: [ 70/100]	Time  0.012 ( 0.017)	Loss 6.1587e-01 (3.6349e-01)	Acc@1  89.00 ( 90.10)	Acc@5  99.00 ( 99.59)
Test: [ 80/100]	Time  0.011 ( 0.016)	Loss 2.9837e-01 (3.5990e-01)	Acc@1  92.00 ( 90.17)	Acc@5 100.00 ( 99.64)
Test: [ 90/100]	Time  0.016 ( 0.016)	Loss 2.9117e-01 (3.5607e-01)	Acc@1  93.00 ( 90.22)	Acc@5 100.00 ( 99.67)
 * Acc@1 90.320 Acc@5 99.670
### epoch[42] execution time: 12.154061079025269
EPOCH 43
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [43][  0/391]	Time  0.197 ( 0.197)	Data  0.175 ( 0.175)	Loss 4.3809e-02 (4.3809e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [43][ 10/391]	Time  0.024 ( 0.041)	Data  0.001 ( 0.018)	Loss 9.2950e-02 (6.6777e-02)	Acc@1  96.09 ( 97.80)	Acc@5 100.00 ( 99.93)
Epoch: [43][ 20/391]	Time  0.030 ( 0.034)	Data  0.002 ( 0.010)	Loss 1.6008e-01 (6.7472e-02)	Acc@1  96.09 ( 97.84)	Acc@5 100.00 ( 99.96)
Epoch: [43][ 30/391]	Time  0.024 ( 0.031)	Data  0.001 ( 0.008)	Loss 1.1182e-01 (6.8837e-02)	Acc@1  96.09 ( 97.66)	Acc@5  99.22 ( 99.95)
Epoch: [43][ 40/391]	Time  0.023 ( 0.030)	Data  0.001 ( 0.006)	Loss 4.5216e-02 (6.9957e-02)	Acc@1  98.44 ( 97.62)	Acc@5 100.00 ( 99.96)
Epoch: [43][ 50/391]	Time  0.026 ( 0.029)	Data  0.001 ( 0.005)	Loss 8.6112e-02 (6.7681e-02)	Acc@1  97.66 ( 97.76)	Acc@5 100.00 ( 99.97)
Epoch: [43][ 60/391]	Time  0.025 ( 0.028)	Data  0.001 ( 0.005)	Loss 7.7663e-02 (6.7925e-02)	Acc@1  97.66 ( 97.75)	Acc@5 100.00 ( 99.97)
Epoch: [43][ 70/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.0485e-02 (6.6054e-02)	Acc@1 100.00 ( 97.79)	Acc@5 100.00 ( 99.98)
Epoch: [43][ 80/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.004)	Loss 4.6182e-02 (6.4924e-02)	Acc@1  98.44 ( 97.83)	Acc@5 100.00 ( 99.98)
Epoch: [43][ 90/391]	Time  0.028 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.2005e-01 (6.3907e-02)	Acc@1  96.09 ( 97.90)	Acc@5 100.00 ( 99.97)
Epoch: [43][100/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.0368e-02 (6.3419e-02)	Acc@1  99.22 ( 97.92)	Acc@5 100.00 ( 99.98)
Epoch: [43][110/391]	Time  0.029 ( 0.027)	Data  0.009 ( 0.004)	Loss 6.9183e-02 (6.6197e-02)	Acc@1  99.22 ( 97.85)	Acc@5 100.00 ( 99.97)
Epoch: [43][120/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 4.8032e-02 (6.6340e-02)	Acc@1  98.44 ( 97.84)	Acc@5 100.00 ( 99.97)
Epoch: [43][130/391]	Time  0.024 ( 0.027)	Data  0.002 ( 0.004)	Loss 1.1615e-02 (6.5840e-02)	Acc@1 100.00 ( 97.85)	Acc@5 100.00 ( 99.98)
Epoch: [43][140/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.9863e-02 (6.5144e-02)	Acc@1 100.00 ( 97.88)	Acc@5 100.00 ( 99.98)
Epoch: [43][150/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.0658e-02 (6.4574e-02)	Acc@1 100.00 ( 97.89)	Acc@5 100.00 ( 99.98)
Epoch: [43][160/391]	Time  0.030 ( 0.027)	Data  0.002 ( 0.003)	Loss 4.0871e-02 (6.4431e-02)	Acc@1  98.44 ( 97.91)	Acc@5 100.00 ( 99.98)
Epoch: [43][170/391]	Time  0.026 ( 0.027)	Data  0.005 ( 0.003)	Loss 3.7359e-02 (6.3891e-02)	Acc@1  97.66 ( 97.93)	Acc@5 100.00 ( 99.98)
Epoch: [43][180/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.9129e-02 (6.4204e-02)	Acc@1  98.44 ( 97.92)	Acc@5 100.00 ( 99.98)
Epoch: [43][190/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.2312e-01 (6.4510e-02)	Acc@1  94.53 ( 97.87)	Acc@5 100.00 ( 99.98)
Epoch: [43][200/391]	Time  0.025 ( 0.027)	Data  0.002 ( 0.003)	Loss 1.8856e-02 (6.4082e-02)	Acc@1 100.00 ( 97.90)	Acc@5 100.00 ( 99.98)
Epoch: [43][210/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.5899e-02 (6.5125e-02)	Acc@1  96.88 ( 97.86)	Acc@5 100.00 ( 99.97)
Epoch: [43][220/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.6355e-02 (6.5106e-02)	Acc@1  99.22 ( 97.86)	Acc@5 100.00 ( 99.98)
Epoch: [43][230/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.6215e-02 (6.5125e-02)	Acc@1  99.22 ( 97.86)	Acc@5 100.00 ( 99.98)
Epoch: [43][240/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.9080e-02 (6.4726e-02)	Acc@1  96.09 ( 97.87)	Acc@5 100.00 ( 99.98)
Epoch: [43][250/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.4504e-02 (6.4664e-02)	Acc@1  97.66 ( 97.88)	Acc@5 100.00 ( 99.97)
Epoch: [43][260/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0747e-01 (6.5540e-02)	Acc@1  97.66 ( 97.85)	Acc@5 100.00 ( 99.97)
Epoch: [43][270/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3879e-02 (6.4626e-02)	Acc@1  99.22 ( 97.88)	Acc@5 100.00 ( 99.97)
Epoch: [43][280/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2682e-01 (6.5310e-02)	Acc@1  95.31 ( 97.85)	Acc@5 100.00 ( 99.97)
Epoch: [43][290/391]	Time  0.023 ( 0.026)	Data  0.000 ( 0.003)	Loss 6.3475e-02 (6.5676e-02)	Acc@1  96.88 ( 97.83)	Acc@5 100.00 ( 99.97)
Epoch: [43][300/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.9729e-02 (6.5473e-02)	Acc@1 100.00 ( 97.84)	Acc@5 100.00 ( 99.97)
Epoch: [43][310/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.1421e-02 (6.5418e-02)	Acc@1  99.22 ( 97.84)	Acc@5 100.00 ( 99.97)
Epoch: [43][320/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.0950e-01 (6.5562e-02)	Acc@1  96.88 ( 97.83)	Acc@5 100.00 ( 99.97)
Epoch: [43][330/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.6794e-02 (6.5845e-02)	Acc@1  96.88 ( 97.83)	Acc@5 100.00 ( 99.97)
Epoch: [43][340/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.1497e-02 (6.5589e-02)	Acc@1  99.22 ( 97.84)	Acc@5 100.00 ( 99.97)
Epoch: [43][350/391]	Time  0.023 ( 0.026)	Data  0.003 ( 0.003)	Loss 4.6337e-02 (6.6405e-02)	Acc@1  98.44 ( 97.81)	Acc@5 100.00 ( 99.97)
Epoch: [43][360/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.9770e-02 (6.6406e-02)	Acc@1  97.66 ( 97.82)	Acc@5 100.00 ( 99.97)
Epoch: [43][370/391]	Time  0.028 ( 0.026)	Data  0.004 ( 0.003)	Loss 4.4615e-02 (6.5866e-02)	Acc@1  98.44 ( 97.84)	Acc@5 100.00 ( 99.97)
Epoch: [43][380/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.4708e-02 (6.5923e-02)	Acc@1 100.00 ( 97.83)	Acc@5 100.00 ( 99.97)
Epoch: [43][390/391]	Time  0.022 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.1279e-01 (6.6034e-02)	Acc@1  93.75 ( 97.81)	Acc@5 100.00 ( 99.97)
## e[43] optimizer.zero_grad (sum) time: 0.10886716842651367
## e[43]       loss.backward (sum) time: 2.253441095352173
## e[43]      optimizer.step (sum) time: 0.8619461059570312
## epoch[43] training(only) time: 10.394115447998047
# Switched to evaluate mode...
Test: [  0/100]	Time  0.164 ( 0.164)	Loss 1.9106e-01 (1.9106e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.011 ( 0.027)	Loss 4.1839e-01 (3.1868e-01)	Acc@1  91.00 ( 91.18)	Acc@5  99.00 ( 99.73)
Test: [ 20/100]	Time  0.014 ( 0.021)	Loss 4.0828e-01 (3.5168e-01)	Acc@1  86.00 ( 90.19)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.013 ( 0.019)	Loss 3.0995e-01 (3.6648e-01)	Acc@1  91.00 ( 90.13)	Acc@5  99.00 ( 99.65)
Test: [ 40/100]	Time  0.013 ( 0.018)	Loss 5.0904e-01 (3.7853e-01)	Acc@1  90.00 ( 89.95)	Acc@5  99.00 ( 99.59)
Test: [ 50/100]	Time  0.020 ( 0.017)	Loss 1.5289e-01 (3.6975e-01)	Acc@1  96.00 ( 90.08)	Acc@5 100.00 ( 99.61)
Test: [ 60/100]	Time  0.020 ( 0.016)	Loss 3.4709e-01 (3.6493e-01)	Acc@1  88.00 ( 90.20)	Acc@5 100.00 ( 99.62)
Test: [ 70/100]	Time  0.011 ( 0.016)	Loss 6.3849e-01 (3.6506e-01)	Acc@1  88.00 ( 90.21)	Acc@5  98.00 ( 99.59)
Test: [ 80/100]	Time  0.011 ( 0.016)	Loss 2.7184e-01 (3.6096e-01)	Acc@1  91.00 ( 90.27)	Acc@5 100.00 ( 99.64)
Test: [ 90/100]	Time  0.018 ( 0.016)	Loss 3.3365e-01 (3.5823e-01)	Acc@1  92.00 ( 90.32)	Acc@5 100.00 ( 99.67)
 * Acc@1 90.420 Acc@5 99.690
### epoch[43] execution time: 12.096279859542847
EPOCH 44
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [44][  0/391]	Time  0.194 ( 0.194)	Data  0.174 ( 0.174)	Loss 8.9893e-02 (8.9893e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [44][ 10/391]	Time  0.023 ( 0.041)	Data  0.002 ( 0.018)	Loss 4.0298e-02 (7.6819e-02)	Acc@1  98.44 ( 97.02)	Acc@5 100.00 (100.00)
Epoch: [44][ 20/391]	Time  0.024 ( 0.034)	Data  0.001 ( 0.010)	Loss 7.3539e-02 (7.4024e-02)	Acc@1  97.66 ( 97.51)	Acc@5 100.00 (100.00)
Epoch: [44][ 30/391]	Time  0.023 ( 0.031)	Data  0.001 ( 0.008)	Loss 6.0668e-02 (6.8772e-02)	Acc@1  98.44 ( 97.81)	Acc@5 100.00 ( 99.97)
Epoch: [44][ 40/391]	Time  0.031 ( 0.030)	Data  0.004 ( 0.006)	Loss 7.8588e-02 (6.9726e-02)	Acc@1  95.31 ( 97.62)	Acc@5 100.00 ( 99.98)
Epoch: [44][ 50/391]	Time  0.037 ( 0.029)	Data  0.010 ( 0.006)	Loss 1.8300e-02 (6.7149e-02)	Acc@1  99.22 ( 97.75)	Acc@5 100.00 ( 99.98)
Epoch: [44][ 60/391]	Time  0.026 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.8413e-02 (6.2488e-02)	Acc@1 100.00 ( 97.90)	Acc@5 100.00 ( 99.99)
Epoch: [44][ 70/391]	Time  0.031 ( 0.028)	Data  0.002 ( 0.005)	Loss 3.4643e-02 (6.1017e-02)	Acc@1  98.44 ( 97.93)	Acc@5 100.00 ( 99.99)
Epoch: [44][ 80/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 5.3411e-02 (5.8464e-02)	Acc@1  97.66 ( 97.99)	Acc@5 100.00 ( 99.99)
Epoch: [44][ 90/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.5009e-01 (5.8971e-02)	Acc@1  95.31 ( 97.96)	Acc@5  99.22 ( 99.98)
Epoch: [44][100/391]	Time  0.028 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.1307e-01 (6.0422e-02)	Acc@1  96.88 ( 97.94)	Acc@5 100.00 ( 99.98)
Epoch: [44][110/391]	Time  0.023 ( 0.027)	Data  0.000 ( 0.004)	Loss 3.8254e-02 (6.1119e-02)	Acc@1  98.44 ( 97.95)	Acc@5 100.00 ( 99.98)
Epoch: [44][120/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.004)	Loss 4.0205e-02 (6.1468e-02)	Acc@1  97.66 ( 97.96)	Acc@5 100.00 ( 99.98)
Epoch: [44][130/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.1606e-01 (6.2055e-02)	Acc@1  95.31 ( 97.94)	Acc@5 100.00 ( 99.98)
Epoch: [44][140/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.5635e-03 (6.2563e-02)	Acc@1 100.00 ( 97.93)	Acc@5 100.00 ( 99.98)
Epoch: [44][150/391]	Time  0.033 ( 0.027)	Data  0.003 ( 0.003)	Loss 7.3605e-02 (6.1773e-02)	Acc@1  96.88 ( 97.95)	Acc@5 100.00 ( 99.98)
Epoch: [44][160/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.2677e-02 (6.1990e-02)	Acc@1  98.44 ( 97.97)	Acc@5 100.00 ( 99.98)
Epoch: [44][170/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.7963e-02 (6.1991e-02)	Acc@1  98.44 ( 97.97)	Acc@5 100.00 ( 99.98)
Epoch: [44][180/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 8.5911e-02 (6.2673e-02)	Acc@1  96.88 ( 97.92)	Acc@5 100.00 ( 99.98)
Epoch: [44][190/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.8989e-02 (6.1936e-02)	Acc@1  99.22 ( 97.95)	Acc@5 100.00 ( 99.98)
Epoch: [44][200/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 8.8064e-02 (6.2548e-02)	Acc@1  97.66 ( 97.94)	Acc@5 100.00 ( 99.98)
Epoch: [44][210/391]	Time  0.030 ( 0.027)	Data  0.005 ( 0.003)	Loss 4.1165e-02 (6.2776e-02)	Acc@1  99.22 ( 97.93)	Acc@5 100.00 ( 99.97)
Epoch: [44][220/391]	Time  0.037 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.7197e-02 (6.2504e-02)	Acc@1  97.66 ( 97.92)	Acc@5 100.00 ( 99.98)
Epoch: [44][230/391]	Time  0.029 ( 0.027)	Data  0.009 ( 0.003)	Loss 2.5330e-02 (6.1745e-02)	Acc@1  99.22 ( 97.95)	Acc@5 100.00 ( 99.98)
Epoch: [44][240/391]	Time  0.031 ( 0.027)	Data  0.009 ( 0.003)	Loss 7.5231e-02 (6.1632e-02)	Acc@1  97.66 ( 97.96)	Acc@5 100.00 ( 99.98)
Epoch: [44][250/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4396e-01 (6.1819e-02)	Acc@1  96.88 ( 97.96)	Acc@5 100.00 ( 99.98)
Epoch: [44][260/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2524e-01 (6.2377e-02)	Acc@1  96.09 ( 97.93)	Acc@5 100.00 ( 99.98)
Epoch: [44][270/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2540e-01 (6.2734e-02)	Acc@1  96.88 ( 97.94)	Acc@5 100.00 ( 99.98)
Epoch: [44][280/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.6009e-02 (6.3365e-02)	Acc@1  98.44 ( 97.92)	Acc@5 100.00 ( 99.98)
Epoch: [44][290/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.9835e-02 (6.3218e-02)	Acc@1  97.66 ( 97.93)	Acc@5 100.00 ( 99.98)
Epoch: [44][300/391]	Time  0.030 ( 0.026)	Data  0.005 ( 0.003)	Loss 8.4811e-02 (6.2995e-02)	Acc@1  95.31 ( 97.92)	Acc@5 100.00 ( 99.98)
Epoch: [44][310/391]	Time  0.027 ( 0.026)	Data  0.006 ( 0.003)	Loss 9.1026e-02 (6.3545e-02)	Acc@1  96.88 ( 97.90)	Acc@5 100.00 ( 99.98)
Epoch: [44][320/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.6881e-02 (6.3741e-02)	Acc@1  98.44 ( 97.89)	Acc@5 100.00 ( 99.98)
Epoch: [44][330/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.3738e-02 (6.3962e-02)	Acc@1  96.88 ( 97.88)	Acc@5 100.00 ( 99.98)
Epoch: [44][340/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.2552e-02 (6.3681e-02)	Acc@1  99.22 ( 97.89)	Acc@5 100.00 ( 99.98)
Epoch: [44][350/391]	Time  0.028 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.9864e-02 (6.3636e-02)	Acc@1  98.44 ( 97.88)	Acc@5 100.00 ( 99.98)
Epoch: [44][360/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.5972e-02 (6.3743e-02)	Acc@1  99.22 ( 97.88)	Acc@5 100.00 ( 99.98)
Epoch: [44][370/391]	Time  0.029 ( 0.026)	Data  0.004 ( 0.003)	Loss 3.4039e-02 (6.3474e-02)	Acc@1  98.44 ( 97.89)	Acc@5 100.00 ( 99.98)
Epoch: [44][380/391]	Time  0.023 ( 0.026)	Data  0.000 ( 0.003)	Loss 4.9898e-02 (6.3395e-02)	Acc@1  97.66 ( 97.88)	Acc@5 100.00 ( 99.98)
Epoch: [44][390/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.9953e-02 (6.3097e-02)	Acc@1  98.75 ( 97.89)	Acc@5 100.00 ( 99.98)
## e[44] optimizer.zero_grad (sum) time: 0.10891318321228027
## e[44]       loss.backward (sum) time: 2.3032658100128174
## e[44]      optimizer.step (sum) time: 0.8927524089813232
## epoch[44] training(only) time: 10.404883623123169
# Switched to evaluate mode...
Test: [  0/100]	Time  0.167 ( 0.167)	Loss 2.2770e-01 (2.2770e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.011 ( 0.028)	Loss 4.2103e-01 (3.4589e-01)	Acc@1  89.00 ( 90.55)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.014 ( 0.022)	Loss 4.6148e-01 (3.6266e-01)	Acc@1  86.00 ( 89.71)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.012 ( 0.019)	Loss 3.1066e-01 (3.7369e-01)	Acc@1  90.00 ( 89.94)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.011 ( 0.018)	Loss 5.2605e-01 (3.9144e-01)	Acc@1  90.00 ( 89.85)	Acc@5  99.00 ( 99.63)
Test: [ 50/100]	Time  0.017 ( 0.018)	Loss 1.6779e-01 (3.8291e-01)	Acc@1  94.00 ( 89.90)	Acc@5 100.00 ( 99.61)
Test: [ 60/100]	Time  0.012 ( 0.017)	Loss 3.5977e-01 (3.7796e-01)	Acc@1  88.00 ( 90.00)	Acc@5 100.00 ( 99.62)
Test: [ 70/100]	Time  0.015 ( 0.017)	Loss 6.3922e-01 (3.7457e-01)	Acc@1  89.00 ( 90.17)	Acc@5  99.00 ( 99.59)
Test: [ 80/100]	Time  0.011 ( 0.016)	Loss 4.3102e-01 (3.7221e-01)	Acc@1  88.00 ( 90.20)	Acc@5 100.00 ( 99.63)
Test: [ 90/100]	Time  0.014 ( 0.016)	Loss 3.0553e-01 (3.7017e-01)	Acc@1  90.00 ( 90.24)	Acc@5 100.00 ( 99.65)
 * Acc@1 90.320 Acc@5 99.660
### epoch[44] execution time: 12.128368616104126
EPOCH 45
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [45][  0/391]	Time  0.186 ( 0.186)	Data  0.161 ( 0.161)	Loss 6.5988e-02 (6.5988e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [45][ 10/391]	Time  0.026 ( 0.040)	Data  0.001 ( 0.017)	Loss 2.1673e-02 (7.2437e-02)	Acc@1  99.22 ( 97.73)	Acc@5 100.00 ( 99.93)
Epoch: [45][ 20/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.010)	Loss 6.1777e-02 (6.3980e-02)	Acc@1  97.66 ( 97.92)	Acc@5 100.00 ( 99.96)
Epoch: [45][ 30/391]	Time  0.023 ( 0.031)	Data  0.001 ( 0.007)	Loss 7.1846e-02 (6.1790e-02)	Acc@1  96.88 ( 97.96)	Acc@5 100.00 ( 99.97)
Epoch: [45][ 40/391]	Time  0.023 ( 0.030)	Data  0.001 ( 0.006)	Loss 3.0381e-02 (6.0251e-02)	Acc@1  99.22 ( 97.90)	Acc@5 100.00 ( 99.98)
Epoch: [45][ 50/391]	Time  0.023 ( 0.029)	Data  0.001 ( 0.005)	Loss 6.6208e-02 (6.1152e-02)	Acc@1  96.88 ( 97.84)	Acc@5 100.00 ( 99.98)
Epoch: [45][ 60/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.7626e-02 (5.7723e-02)	Acc@1  99.22 ( 97.95)	Acc@5 100.00 ( 99.99)
Epoch: [45][ 70/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.004)	Loss 3.5507e-02 (5.8160e-02)	Acc@1  98.44 ( 97.91)	Acc@5 100.00 ( 99.98)
Epoch: [45][ 80/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 5.6012e-02 (5.8741e-02)	Acc@1  99.22 ( 97.96)	Acc@5 100.00 ( 99.97)
Epoch: [45][ 90/391]	Time  0.028 ( 0.027)	Data  0.001 ( 0.004)	Loss 4.9432e-02 (5.8330e-02)	Acc@1  99.22 ( 97.99)	Acc@5  99.22 ( 99.97)
Epoch: [45][100/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 5.7893e-02 (5.8778e-02)	Acc@1  98.44 ( 97.97)	Acc@5 100.00 ( 99.96)
Epoch: [45][110/391]	Time  0.027 ( 0.027)	Data  0.001 ( 0.004)	Loss 6.7776e-02 (5.8612e-02)	Acc@1  96.88 ( 97.97)	Acc@5 100.00 ( 99.96)
Epoch: [45][120/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.6142e-02 (5.8604e-02)	Acc@1  97.66 ( 97.97)	Acc@5 100.00 ( 99.97)
Epoch: [45][130/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.4604e-02 (5.7670e-02)	Acc@1  98.44 ( 98.01)	Acc@5 100.00 ( 99.97)
Epoch: [45][140/391]	Time  0.026 ( 0.027)	Data  0.002 ( 0.003)	Loss 8.9427e-02 (5.7528e-02)	Acc@1  96.09 ( 98.01)	Acc@5 100.00 ( 99.97)
Epoch: [45][150/391]	Time  0.038 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.1453e-02 (5.7966e-02)	Acc@1  99.22 ( 98.00)	Acc@5 100.00 ( 99.97)
Epoch: [45][160/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.3699e-02 (5.7771e-02)	Acc@1  99.22 ( 98.03)	Acc@5 100.00 ( 99.98)
Epoch: [45][170/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.0699e-02 (5.8074e-02)	Acc@1 100.00 ( 98.03)	Acc@5 100.00 ( 99.98)
Epoch: [45][180/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.2624e-01 (5.7403e-02)	Acc@1  96.88 ( 98.05)	Acc@5 100.00 ( 99.98)
Epoch: [45][190/391]	Time  0.023 ( 0.027)	Data  0.000 ( 0.003)	Loss 4.0927e-02 (5.6973e-02)	Acc@1  98.44 ( 98.06)	Acc@5 100.00 ( 99.98)
Epoch: [45][200/391]	Time  0.032 ( 0.027)	Data  0.004 ( 0.003)	Loss 8.0876e-02 (5.6744e-02)	Acc@1  95.31 ( 98.05)	Acc@5 100.00 ( 99.98)
Epoch: [45][210/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.0681e-02 (5.6901e-02)	Acc@1  99.22 ( 98.04)	Acc@5 100.00 ( 99.98)
Epoch: [45][220/391]	Time  0.032 ( 0.027)	Data  0.003 ( 0.003)	Loss 7.2846e-02 (5.7007e-02)	Acc@1  95.31 ( 98.02)	Acc@5 100.00 ( 99.98)
Epoch: [45][230/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.5015e-02 (5.8051e-02)	Acc@1  98.44 ( 97.98)	Acc@5 100.00 ( 99.98)
Epoch: [45][240/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.8253e-02 (5.7397e-02)	Acc@1  99.22 ( 98.00)	Acc@5 100.00 ( 99.98)
Epoch: [45][250/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6525e-02 (5.7988e-02)	Acc@1 100.00 ( 97.99)	Acc@5 100.00 ( 99.98)
Epoch: [45][260/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2654e-02 (5.8361e-02)	Acc@1  99.22 ( 97.97)	Acc@5 100.00 ( 99.98)
Epoch: [45][270/391]	Time  0.023 ( 0.026)	Data  0.000 ( 0.003)	Loss 6.2440e-02 (5.8506e-02)	Acc@1  98.44 ( 97.98)	Acc@5 100.00 ( 99.98)
Epoch: [45][280/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.1950e-02 (5.8916e-02)	Acc@1  98.44 ( 97.97)	Acc@5 100.00 ( 99.98)
Epoch: [45][290/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.2577e-01 (5.8730e-02)	Acc@1  96.09 ( 97.98)	Acc@5 100.00 ( 99.98)
Epoch: [45][300/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.8904e-02 (5.8952e-02)	Acc@1  99.22 ( 97.98)	Acc@5 100.00 ( 99.98)
Epoch: [45][310/391]	Time  0.030 ( 0.026)	Data  0.005 ( 0.003)	Loss 5.8011e-02 (5.8882e-02)	Acc@1  97.66 ( 97.98)	Acc@5 100.00 ( 99.98)
Epoch: [45][320/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3964e-01 (5.9205e-02)	Acc@1  96.88 ( 97.98)	Acc@5  99.22 ( 99.98)
Epoch: [45][330/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.4722e-02 (5.9128e-02)	Acc@1  98.44 ( 97.98)	Acc@5 100.00 ( 99.98)
Epoch: [45][340/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.7648e-02 (5.9210e-02)	Acc@1  99.22 ( 98.00)	Acc@5 100.00 ( 99.98)
Epoch: [45][350/391]	Time  0.031 ( 0.026)	Data  0.003 ( 0.003)	Loss 5.1537e-02 (5.9739e-02)	Acc@1  97.66 ( 97.98)	Acc@5 100.00 ( 99.98)
Epoch: [45][360/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.8622e-02 (5.9682e-02)	Acc@1  99.22 ( 97.99)	Acc@5 100.00 ( 99.98)
Epoch: [45][370/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.1206e-02 (6.0447e-02)	Acc@1  98.44 ( 97.96)	Acc@5 100.00 ( 99.98)
Epoch: [45][380/391]	Time  0.035 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.8436e-02 (6.0582e-02)	Acc@1  98.44 ( 97.96)	Acc@5 100.00 ( 99.98)
Epoch: [45][390/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.6761e-01 (6.0928e-02)	Acc@1  95.00 ( 97.95)	Acc@5 100.00 ( 99.98)
## e[45] optimizer.zero_grad (sum) time: 0.11017298698425293
## e[45]       loss.backward (sum) time: 2.2610924243927
## e[45]      optimizer.step (sum) time: 0.8904500007629395
## epoch[45] training(only) time: 10.430841445922852
# Switched to evaluate mode...
Test: [  0/100]	Time  0.157 ( 0.157)	Loss 2.3292e-01 (2.3292e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.011 ( 0.028)	Loss 3.7437e-01 (3.3011e-01)	Acc@1  91.00 ( 90.82)	Acc@5  99.00 ( 99.73)
Test: [ 20/100]	Time  0.015 ( 0.022)	Loss 4.2393e-01 (3.5280e-01)	Acc@1  87.00 ( 90.00)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.014 ( 0.020)	Loss 3.0485e-01 (3.6850e-01)	Acc@1  92.00 ( 89.94)	Acc@5  99.00 ( 99.65)
Test: [ 40/100]	Time  0.014 ( 0.018)	Loss 5.2457e-01 (3.8513e-01)	Acc@1  88.00 ( 89.80)	Acc@5 100.00 ( 99.63)
Test: [ 50/100]	Time  0.023 ( 0.018)	Loss 1.6608e-01 (3.7799e-01)	Acc@1  96.00 ( 89.94)	Acc@5 100.00 ( 99.63)
Test: [ 60/100]	Time  0.013 ( 0.017)	Loss 4.0093e-01 (3.7547e-01)	Acc@1  91.00 ( 90.07)	Acc@5 100.00 ( 99.64)
Test: [ 70/100]	Time  0.017 ( 0.017)	Loss 6.9856e-01 (3.7386e-01)	Acc@1  88.00 ( 90.07)	Acc@5  99.00 ( 99.63)
Test: [ 80/100]	Time  0.012 ( 0.016)	Loss 3.4099e-01 (3.7223e-01)	Acc@1  90.00 ( 90.05)	Acc@5  99.00 ( 99.67)
Test: [ 90/100]	Time  0.016 ( 0.016)	Loss 2.7904e-01 (3.6778e-01)	Acc@1  92.00 ( 90.14)	Acc@5 100.00 ( 99.69)
 * Acc@1 90.190 Acc@5 99.700
### epoch[45] execution time: 12.196872234344482
EPOCH 46
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [46][  0/391]	Time  0.196 ( 0.196)	Data  0.175 ( 0.175)	Loss 2.8248e-02 (2.8248e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [46][ 10/391]	Time  0.028 ( 0.041)	Data  0.001 ( 0.018)	Loss 2.6243e-02 (4.5921e-02)	Acc@1  98.44 ( 98.51)	Acc@5 100.00 (100.00)
Epoch: [46][ 20/391]	Time  0.027 ( 0.034)	Data  0.002 ( 0.010)	Loss 4.7741e-02 (5.0898e-02)	Acc@1  98.44 ( 98.18)	Acc@5 100.00 (100.00)
Epoch: [46][ 30/391]	Time  0.027 ( 0.031)	Data  0.001 ( 0.008)	Loss 2.1066e-02 (5.1263e-02)	Acc@1  99.22 ( 98.24)	Acc@5 100.00 (100.00)
Epoch: [46][ 40/391]	Time  0.025 ( 0.030)	Data  0.001 ( 0.006)	Loss 5.6740e-02 (5.6710e-02)	Acc@1  99.22 ( 98.06)	Acc@5 100.00 (100.00)
Epoch: [46][ 50/391]	Time  0.023 ( 0.029)	Data  0.001 ( 0.005)	Loss 7.6857e-02 (5.6680e-02)	Acc@1  96.88 ( 98.13)	Acc@5 100.00 (100.00)
Epoch: [46][ 60/391]	Time  0.024 ( 0.029)	Data  0.001 ( 0.005)	Loss 3.0664e-02 (5.9235e-02)	Acc@1  99.22 ( 98.09)	Acc@5 100.00 (100.00)
Epoch: [46][ 70/391]	Time  0.025 ( 0.028)	Data  0.001 ( 0.004)	Loss 4.2504e-02 (5.8142e-02)	Acc@1  98.44 ( 98.12)	Acc@5 100.00 (100.00)
Epoch: [46][ 80/391]	Time  0.023 ( 0.028)	Data  0.002 ( 0.004)	Loss 1.1019e-01 (5.8291e-02)	Acc@1  97.66 ( 98.15)	Acc@5 100.00 (100.00)
Epoch: [46][ 90/391]	Time  0.023 ( 0.028)	Data  0.002 ( 0.004)	Loss 1.5356e-02 (5.7795e-02)	Acc@1 100.00 ( 98.13)	Acc@5 100.00 (100.00)
Epoch: [46][100/391]	Time  0.025 ( 0.028)	Data  0.002 ( 0.004)	Loss 1.5029e-02 (5.7750e-02)	Acc@1 100.00 ( 98.13)	Acc@5 100.00 ( 99.99)
Epoch: [46][110/391]	Time  0.027 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.3120e-01 (5.8924e-02)	Acc@1  94.53 ( 98.04)	Acc@5  99.22 ( 99.99)
Epoch: [46][120/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.8993e-02 (5.9356e-02)	Acc@1  98.44 ( 98.04)	Acc@5 100.00 ( 99.99)
Epoch: [46][130/391]	Time  0.025 ( 0.027)	Data  0.005 ( 0.003)	Loss 5.2045e-02 (5.9247e-02)	Acc@1  98.44 ( 98.04)	Acc@5 100.00 ( 99.99)
Epoch: [46][140/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.9496e-02 (5.8985e-02)	Acc@1  98.44 ( 98.05)	Acc@5 100.00 ( 99.98)
Epoch: [46][150/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.9473e-02 (5.8319e-02)	Acc@1  99.22 ( 98.06)	Acc@5 100.00 ( 99.98)
Epoch: [46][160/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.0542e-02 (5.8063e-02)	Acc@1  98.44 ( 98.08)	Acc@5 100.00 ( 99.98)
Epoch: [46][170/391]	Time  0.028 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.7799e-02 (5.8986e-02)	Acc@1  98.44 ( 98.04)	Acc@5 100.00 ( 99.98)
Epoch: [46][180/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 5.4762e-02 (5.8508e-02)	Acc@1  97.66 ( 98.05)	Acc@5 100.00 ( 99.98)
Epoch: [46][190/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.6052e-02 (5.7930e-02)	Acc@1  97.66 ( 98.05)	Acc@5 100.00 ( 99.98)
Epoch: [46][200/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.3523e-02 (5.8769e-02)	Acc@1  97.66 ( 98.02)	Acc@5 100.00 ( 99.98)
Epoch: [46][210/391]	Time  0.026 ( 0.027)	Data  0.002 ( 0.003)	Loss 9.8270e-02 (5.9226e-02)	Acc@1  96.88 ( 97.99)	Acc@5 100.00 ( 99.98)
Epoch: [46][220/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.6324e-02 (5.9383e-02)	Acc@1  99.22 ( 97.99)	Acc@5 100.00 ( 99.98)
Epoch: [46][230/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.7782e-02 (5.9005e-02)	Acc@1  97.66 ( 98.00)	Acc@5 100.00 ( 99.98)
Epoch: [46][240/391]	Time  0.031 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.2232e-01 (5.8721e-02)	Acc@1  96.09 ( 98.02)	Acc@5 100.00 ( 99.98)
Epoch: [46][250/391]	Time  0.024 ( 0.027)	Data  0.000 ( 0.003)	Loss 6.5663e-02 (5.9645e-02)	Acc@1  97.66 ( 98.00)	Acc@5 100.00 ( 99.98)
Epoch: [46][260/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 1.2668e-02 (5.9683e-02)	Acc@1 100.00 ( 98.00)	Acc@5 100.00 ( 99.98)
Epoch: [46][270/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.0116e-01 (5.9344e-02)	Acc@1  97.66 ( 98.02)	Acc@5 100.00 ( 99.98)
Epoch: [46][280/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.0894e-01 (5.9298e-02)	Acc@1  96.88 ( 98.03)	Acc@5 100.00 ( 99.98)
Epoch: [46][290/391]	Time  0.029 ( 0.027)	Data  0.003 ( 0.003)	Loss 6.5776e-02 (5.8695e-02)	Acc@1  98.44 ( 98.05)	Acc@5 100.00 ( 99.98)
Epoch: [46][300/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.8917e-02 (5.8022e-02)	Acc@1  96.88 ( 98.07)	Acc@5 100.00 ( 99.98)
Epoch: [46][310/391]	Time  0.029 ( 0.026)	Data  0.000 ( 0.003)	Loss 4.8647e-02 (5.7930e-02)	Acc@1  99.22 ( 98.08)	Acc@5 100.00 ( 99.98)
Epoch: [46][320/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.9938e-02 (5.7527e-02)	Acc@1  99.22 ( 98.09)	Acc@5 100.00 ( 99.98)
Epoch: [46][330/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.0308e-02 (5.7606e-02)	Acc@1  97.66 ( 98.09)	Acc@5 100.00 ( 99.98)
Epoch: [46][340/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.8723e-02 (5.7389e-02)	Acc@1  99.22 ( 98.09)	Acc@5 100.00 ( 99.98)
Epoch: [46][350/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.7255e-02 (5.7285e-02)	Acc@1  98.44 ( 98.09)	Acc@5 100.00 ( 99.98)
Epoch: [46][360/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.9962e-02 (5.7565e-02)	Acc@1  98.44 ( 98.08)	Acc@5 100.00 ( 99.98)
Epoch: [46][370/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.3430e-02 (5.7927e-02)	Acc@1  96.88 ( 98.06)	Acc@5 100.00 ( 99.98)
Epoch: [46][380/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0523e-02 (5.7725e-02)	Acc@1  99.22 ( 98.07)	Acc@5 100.00 ( 99.98)
Epoch: [46][390/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7060e-02 (5.7623e-02)	Acc@1 100.00 ( 98.07)	Acc@5 100.00 ( 99.98)
## e[46] optimizer.zero_grad (sum) time: 0.10913801193237305
## e[46]       loss.backward (sum) time: 2.2173871994018555
## e[46]      optimizer.step (sum) time: 0.8681561946868896
## epoch[46] training(only) time: 10.413815975189209
# Switched to evaluate mode...
Test: [  0/100]	Time  0.164 ( 0.164)	Loss 3.0288e-01 (3.0288e-01)	Acc@1  90.00 ( 90.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.011 ( 0.027)	Loss 4.7532e-01 (3.4986e-01)	Acc@1  90.00 ( 91.27)	Acc@5 100.00 ( 99.55)
Test: [ 20/100]	Time  0.015 ( 0.021)	Loss 4.8132e-01 (3.6060e-01)	Acc@1  84.00 ( 90.67)	Acc@5 100.00 ( 99.52)
Test: [ 30/100]	Time  0.012 ( 0.019)	Loss 3.7855e-01 (3.8649e-01)	Acc@1  88.00 ( 90.29)	Acc@5  99.00 ( 99.55)
Test: [ 40/100]	Time  0.012 ( 0.018)	Loss 5.0722e-01 (4.0459e-01)	Acc@1  88.00 ( 89.88)	Acc@5  98.00 ( 99.51)
Test: [ 50/100]	Time  0.017 ( 0.017)	Loss 2.1378e-01 (3.9461e-01)	Acc@1  95.00 ( 90.16)	Acc@5 100.00 ( 99.51)
Test: [ 60/100]	Time  0.012 ( 0.016)	Loss 4.2846e-01 (3.9274e-01)	Acc@1  90.00 ( 90.16)	Acc@5 100.00 ( 99.56)
Test: [ 70/100]	Time  0.011 ( 0.016)	Loss 7.2890e-01 (3.9006e-01)	Acc@1  86.00 ( 90.24)	Acc@5  99.00 ( 99.56)
Test: [ 80/100]	Time  0.021 ( 0.016)	Loss 3.7066e-01 (3.8779e-01)	Acc@1  92.00 ( 90.22)	Acc@5 100.00 ( 99.60)
Test: [ 90/100]	Time  0.010 ( 0.016)	Loss 2.8621e-01 (3.8266e-01)	Acc@1  92.00 ( 90.26)	Acc@5 100.00 ( 99.64)
 * Acc@1 90.340 Acc@5 99.650
### epoch[46] execution time: 12.11272668838501
EPOCH 47
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [47][  0/391]	Time  0.197 ( 0.197)	Data  0.177 ( 0.177)	Loss 3.1613e-02 (3.1613e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [47][ 10/391]	Time  0.030 ( 0.042)	Data  0.002 ( 0.018)	Loss 4.3765e-02 (7.1320e-02)	Acc@1  98.44 ( 97.59)	Acc@5 100.00 (100.00)
Epoch: [47][ 20/391]	Time  0.023 ( 0.034)	Data  0.002 ( 0.010)	Loss 5.6803e-02 (7.7390e-02)	Acc@1  97.66 ( 97.25)	Acc@5 100.00 (100.00)
Epoch: [47][ 30/391]	Time  0.027 ( 0.031)	Data  0.001 ( 0.008)	Loss 1.2001e-01 (7.6284e-02)	Acc@1  96.88 ( 97.23)	Acc@5 100.00 (100.00)
Epoch: [47][ 40/391]	Time  0.024 ( 0.030)	Data  0.001 ( 0.006)	Loss 1.9551e-02 (6.8145e-02)	Acc@1  99.22 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [47][ 50/391]	Time  0.023 ( 0.029)	Data  0.001 ( 0.005)	Loss 7.6266e-02 (6.4345e-02)	Acc@1  96.88 ( 97.82)	Acc@5 100.00 (100.00)
Epoch: [47][ 60/391]	Time  0.025 ( 0.028)	Data  0.001 ( 0.005)	Loss 4.8156e-02 (6.5472e-02)	Acc@1  97.66 ( 97.77)	Acc@5 100.00 ( 99.99)
Epoch: [47][ 70/391]	Time  0.023 ( 0.028)	Data  0.002 ( 0.005)	Loss 5.5516e-02 (6.5238e-02)	Acc@1  97.66 ( 97.74)	Acc@5 100.00 ( 99.98)
Epoch: [47][ 80/391]	Time  0.023 ( 0.028)	Data  0.002 ( 0.004)	Loss 2.8901e-02 (6.3948e-02)	Acc@1  99.22 ( 97.82)	Acc@5 100.00 ( 99.97)
Epoch: [47][ 90/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 4.2705e-02 (6.3196e-02)	Acc@1  98.44 ( 97.87)	Acc@5 100.00 ( 99.97)
Epoch: [47][100/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.004)	Loss 8.4787e-02 (6.2897e-02)	Acc@1  97.66 ( 97.89)	Acc@5 100.00 ( 99.98)
Epoch: [47][110/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.7116e-02 (6.1163e-02)	Acc@1  98.44 ( 97.94)	Acc@5 100.00 ( 99.98)
Epoch: [47][120/391]	Time  0.030 ( 0.027)	Data  0.009 ( 0.004)	Loss 1.4238e-01 (6.0568e-02)	Acc@1  94.53 ( 97.96)	Acc@5 100.00 ( 99.98)
Epoch: [47][130/391]	Time  0.027 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.9191e-02 (5.9399e-02)	Acc@1  99.22 ( 98.03)	Acc@5 100.00 ( 99.98)
Epoch: [47][140/391]	Time  0.023 ( 0.027)	Data  0.000 ( 0.003)	Loss 2.7245e-02 (5.8623e-02)	Acc@1  99.22 ( 98.05)	Acc@5 100.00 ( 99.97)
Epoch: [47][150/391]	Time  0.032 ( 0.027)	Data  0.000 ( 0.003)	Loss 3.4770e-02 (5.7459e-02)	Acc@1  98.44 ( 98.09)	Acc@5 100.00 ( 99.97)
Epoch: [47][160/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 3.4209e-02 (5.6808e-02)	Acc@1  98.44 ( 98.10)	Acc@5 100.00 ( 99.98)
Epoch: [47][170/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 8.1234e-02 (5.6822e-02)	Acc@1  96.88 ( 98.09)	Acc@5 100.00 ( 99.97)
Epoch: [47][180/391]	Time  0.026 ( 0.027)	Data  0.001 ( 0.003)	Loss 9.0145e-02 (5.7211e-02)	Acc@1  97.66 ( 98.10)	Acc@5 100.00 ( 99.97)
Epoch: [47][190/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 2.6865e-02 (5.8056e-02)	Acc@1  99.22 ( 98.06)	Acc@5 100.00 ( 99.97)
Epoch: [47][200/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.7754e-02 (5.7509e-02)	Acc@1  97.66 ( 98.08)	Acc@5 100.00 ( 99.97)
Epoch: [47][210/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.003)	Loss 8.6935e-02 (5.6826e-02)	Acc@1  96.88 ( 98.09)	Acc@5 100.00 ( 99.97)
Epoch: [47][220/391]	Time  0.028 ( 0.027)	Data  0.002 ( 0.003)	Loss 1.6287e-02 (5.5938e-02)	Acc@1  99.22 ( 98.12)	Acc@5 100.00 ( 99.98)
Epoch: [47][230/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.5357e-02 (5.5415e-02)	Acc@1  97.66 ( 98.13)	Acc@5 100.00 ( 99.98)
Epoch: [47][240/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.9292e-02 (5.5225e-02)	Acc@1 100.00 ( 98.15)	Acc@5 100.00 ( 99.98)
Epoch: [47][250/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.7356e-02 (5.4737e-02)	Acc@1  97.66 ( 98.17)	Acc@5 100.00 ( 99.98)
Epoch: [47][260/391]	Time  0.023 ( 0.026)	Data  0.004 ( 0.003)	Loss 2.6024e-02 (5.4230e-02)	Acc@1  99.22 ( 98.19)	Acc@5 100.00 ( 99.98)
Epoch: [47][270/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.5540e-02 (5.4172e-02)	Acc@1  96.09 ( 98.19)	Acc@5 100.00 ( 99.98)
Epoch: [47][280/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.8510e-02 (5.4267e-02)	Acc@1  98.44 ( 98.20)	Acc@5 100.00 ( 99.98)
Epoch: [47][290/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.5268e-02 (5.4964e-02)	Acc@1  98.44 ( 98.17)	Acc@5 100.00 ( 99.98)
Epoch: [47][300/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.6560e-02 (5.5086e-02)	Acc@1  96.88 ( 98.16)	Acc@5 100.00 ( 99.98)
Epoch: [47][310/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.6348e-02 (5.5229e-02)	Acc@1  97.66 ( 98.16)	Acc@5 100.00 ( 99.98)
Epoch: [47][320/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.5213e-02 (5.5507e-02)	Acc@1  98.44 ( 98.16)	Acc@5 100.00 ( 99.98)
Epoch: [47][330/391]	Time  0.030 ( 0.026)	Data  0.002 ( 0.003)	Loss 6.2991e-02 (5.5949e-02)	Acc@1  98.44 ( 98.14)	Acc@5 100.00 ( 99.98)
Epoch: [47][340/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.5347e-02 (5.5844e-02)	Acc@1  96.09 ( 98.15)	Acc@5 100.00 ( 99.98)
Epoch: [47][350/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3169e-02 (5.5858e-02)	Acc@1  99.22 ( 98.15)	Acc@5 100.00 ( 99.98)
Epoch: [47][360/391]	Time  0.029 ( 0.026)	Data  0.004 ( 0.003)	Loss 1.3453e-02 (5.5549e-02)	Acc@1  99.22 ( 98.16)	Acc@5 100.00 ( 99.98)
Epoch: [47][370/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.0492e-02 (5.5273e-02)	Acc@1  99.22 ( 98.18)	Acc@5 100.00 ( 99.99)
Epoch: [47][380/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.4299e-02 (5.5178e-02)	Acc@1  98.44 ( 98.18)	Acc@5 100.00 ( 99.99)
Epoch: [47][390/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.6543e-02 (5.5016e-02)	Acc@1  96.25 ( 98.18)	Acc@5 100.00 ( 99.99)
## e[47] optimizer.zero_grad (sum) time: 0.10969901084899902
## e[47]       loss.backward (sum) time: 2.258497714996338
## e[47]      optimizer.step (sum) time: 0.8787276744842529
## epoch[47] training(only) time: 10.377617835998535
# Switched to evaluate mode...
Test: [  0/100]	Time  0.150 ( 0.150)	Loss 2.6166e-01 (2.6166e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.012 ( 0.027)	Loss 4.3376e-01 (3.3493e-01)	Acc@1  91.00 ( 91.27)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.011 ( 0.021)	Loss 5.0917e-01 (3.7409e-01)	Acc@1  82.00 ( 89.95)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.013 ( 0.019)	Loss 3.1724e-01 (3.8910e-01)	Acc@1  90.00 ( 89.84)	Acc@5  99.00 ( 99.65)
Test: [ 40/100]	Time  0.013 ( 0.018)	Loss 5.2421e-01 (3.9794e-01)	Acc@1  89.00 ( 89.85)	Acc@5 100.00 ( 99.66)
Test: [ 50/100]	Time  0.015 ( 0.017)	Loss 1.4562e-01 (3.9136e-01)	Acc@1  97.00 ( 90.14)	Acc@5  99.00 ( 99.65)
Test: [ 60/100]	Time  0.011 ( 0.017)	Loss 4.3747e-01 (3.9000e-01)	Acc@1  89.00 ( 90.18)	Acc@5 100.00 ( 99.64)
Test: [ 70/100]	Time  0.019 ( 0.017)	Loss 7.4027e-01 (3.8782e-01)	Acc@1  87.00 ( 90.32)	Acc@5 100.00 ( 99.65)
Test: [ 80/100]	Time  0.012 ( 0.016)	Loss 3.5206e-01 (3.8551e-01)	Acc@1  91.00 ( 90.33)	Acc@5 100.00 ( 99.67)
Test: [ 90/100]	Time  0.012 ( 0.016)	Loss 3.3996e-01 (3.8062e-01)	Acc@1  92.00 ( 90.42)	Acc@5 100.00 ( 99.68)
 * Acc@1 90.530 Acc@5 99.690
### epoch[47] execution time: 12.092087268829346
EPOCH 48
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [48][  0/391]	Time  0.198 ( 0.198)	Data  0.176 ( 0.176)	Loss 1.0158e-01 (1.0158e-01)	Acc@1  97.66 ( 97.66)	Acc@5  99.22 ( 99.22)
Epoch: [48][ 10/391]	Time  0.027 ( 0.041)	Data  0.003 ( 0.018)	Loss 5.7911e-02 (4.8187e-02)	Acc@1  98.44 ( 98.72)	Acc@5 100.00 ( 99.93)
Epoch: [48][ 20/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.010)	Loss 5.2244e-02 (4.4332e-02)	Acc@1  99.22 ( 98.77)	Acc@5 100.00 ( 99.96)
Epoch: [48][ 30/391]	Time  0.034 ( 0.032)	Data  0.005 ( 0.008)	Loss 2.3971e-02 (4.1410e-02)	Acc@1  99.22 ( 98.71)	Acc@5 100.00 ( 99.97)
Epoch: [48][ 40/391]	Time  0.031 ( 0.030)	Data  0.004 ( 0.006)	Loss 4.5778e-02 (3.9272e-02)	Acc@1  99.22 ( 98.76)	Acc@5 100.00 ( 99.98)
Epoch: [48][ 50/391]	Time  0.023 ( 0.029)	Data  0.001 ( 0.006)	Loss 8.4639e-02 (4.2701e-02)	Acc@1  96.88 ( 98.62)	Acc@5 100.00 ( 99.98)
Epoch: [48][ 60/391]	Time  0.023 ( 0.029)	Data  0.001 ( 0.005)	Loss 1.0496e-01 (4.2093e-02)	Acc@1  95.31 ( 98.63)	Acc@5 100.00 ( 99.99)
Epoch: [48][ 70/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 6.1468e-02 (4.3602e-02)	Acc@1  97.66 ( 98.53)	Acc@5 100.00 ( 99.99)
Epoch: [48][ 80/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 2.7084e-02 (4.3960e-02)	Acc@1  99.22 ( 98.50)	Acc@5 100.00 ( 99.99)
Epoch: [48][ 90/391]	Time  0.028 ( 0.028)	Data  0.001 ( 0.004)	Loss 7.3831e-02 (4.4770e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 ( 99.98)
Epoch: [48][100/391]	Time  0.027 ( 0.028)	Data  0.001 ( 0.004)	Loss 7.1836e-02 (4.4947e-02)	Acc@1  97.66 ( 98.41)	Acc@5 100.00 ( 99.98)
Epoch: [48][110/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 6.6406e-02 (4.6358e-02)	Acc@1  97.66 ( 98.36)	Acc@5 100.00 ( 99.99)
Epoch: [48][120/391]	Time  0.029 ( 0.027)	Data  0.005 ( 0.004)	Loss 5.2809e-02 (4.6932e-02)	Acc@1  98.44 ( 98.37)	Acc@5 100.00 ( 99.99)
Epoch: [48][130/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.9309e-02 (4.7469e-02)	Acc@1  98.44 ( 98.35)	Acc@5 100.00 ( 99.99)
Epoch: [48][140/391]	Time  0.027 ( 0.027)	Data  0.000 ( 0.003)	Loss 2.0936e-02 (4.7364e-02)	Acc@1 100.00 ( 98.37)	Acc@5 100.00 ( 99.99)
Epoch: [48][150/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.9977e-02 (4.7641e-02)	Acc@1  97.66 ( 98.35)	Acc@5 100.00 ( 99.99)
Epoch: [48][160/391]	Time  0.033 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.3662e-02 (4.8081e-02)	Acc@1  99.22 ( 98.34)	Acc@5 100.00 ( 99.99)
Epoch: [48][170/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 9.2680e-02 (4.9140e-02)	Acc@1  96.88 ( 98.32)	Acc@5 100.00 ( 99.99)
Epoch: [48][180/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.0547e-02 (4.9554e-02)	Acc@1  98.44 ( 98.31)	Acc@5 100.00 ( 99.99)
Epoch: [48][190/391]	Time  0.026 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.4664e-02 (5.0357e-02)	Acc@1  99.22 ( 98.28)	Acc@5 100.00 ( 99.99)
Epoch: [48][200/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.5249e-03 (4.9924e-02)	Acc@1 100.00 ( 98.31)	Acc@5 100.00 ( 99.99)
Epoch: [48][210/391]	Time  0.032 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.2762e-02 (5.0323e-02)	Acc@1  97.66 ( 98.29)	Acc@5 100.00 ( 99.99)
Epoch: [48][220/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.2544e-02 (4.9542e-02)	Acc@1  99.22 ( 98.32)	Acc@5 100.00 ( 99.99)
Epoch: [48][230/391]	Time  0.033 ( 0.027)	Data  0.004 ( 0.003)	Loss 3.6339e-02 (4.9935e-02)	Acc@1  98.44 ( 98.31)	Acc@5 100.00 ( 99.99)
Epoch: [48][240/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.4172e-02 (5.0229e-02)	Acc@1  99.22 ( 98.31)	Acc@5 100.00 ( 99.99)
Epoch: [48][250/391]	Time  0.023 ( 0.027)	Data  0.000 ( 0.003)	Loss 1.1426e-01 (5.0614e-02)	Acc@1  96.88 ( 98.31)	Acc@5 100.00 ( 99.99)
Epoch: [48][260/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.5717e-02 (5.0735e-02)	Acc@1  98.44 ( 98.31)	Acc@5 100.00 ( 99.99)
Epoch: [48][270/391]	Time  0.024 ( 0.027)	Data  0.003 ( 0.003)	Loss 7.1775e-02 (5.1063e-02)	Acc@1  96.88 ( 98.30)	Acc@5 100.00 ( 99.99)
Epoch: [48][280/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.9867e-02 (5.0904e-02)	Acc@1  98.44 ( 98.30)	Acc@5 100.00 ( 99.99)
Epoch: [48][290/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5901e-02 (5.0343e-02)	Acc@1  99.22 ( 98.32)	Acc@5 100.00 ( 99.99)
Epoch: [48][300/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.3359e-02 (5.0062e-02)	Acc@1  96.88 ( 98.33)	Acc@5 100.00 ( 99.99)
Epoch: [48][310/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.5295e-02 (5.0569e-02)	Acc@1  98.44 ( 98.31)	Acc@5 100.00 ( 99.99)
Epoch: [48][320/391]	Time  0.027 ( 0.026)	Data  0.003 ( 0.003)	Loss 5.4217e-02 (5.1395e-02)	Acc@1  98.44 ( 98.27)	Acc@5 100.00 (100.00)
Epoch: [48][330/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.8860e-02 (5.1394e-02)	Acc@1  98.44 ( 98.27)	Acc@5 100.00 (100.00)
Epoch: [48][340/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.4955e-02 (5.2032e-02)	Acc@1  98.44 ( 98.25)	Acc@5 100.00 (100.00)
Epoch: [48][350/391]	Time  0.027 ( 0.026)	Data  0.000 ( 0.003)	Loss 7.2069e-02 (5.2233e-02)	Acc@1  98.44 ( 98.24)	Acc@5 100.00 (100.00)
Epoch: [48][360/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3361e-01 (5.2541e-02)	Acc@1  96.09 ( 98.24)	Acc@5  99.22 ( 99.99)
Epoch: [48][370/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.9058e-02 (5.3218e-02)	Acc@1  97.66 ( 98.22)	Acc@5 100.00 ( 99.99)
Epoch: [48][380/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.2804e-02 (5.3718e-02)	Acc@1  96.88 ( 98.22)	Acc@5 100.00 ( 99.99)
Epoch: [48][390/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.1938e-02 (5.4230e-02)	Acc@1  96.25 ( 98.20)	Acc@5 100.00 ( 99.99)
## e[48] optimizer.zero_grad (sum) time: 0.10933613777160645
## e[48]       loss.backward (sum) time: 2.2260398864746094
## e[48]      optimizer.step (sum) time: 0.8813252449035645
## epoch[48] training(only) time: 10.413591384887695
# Switched to evaluate mode...
Test: [  0/100]	Time  0.166 ( 0.166)	Loss 2.0621e-01 (2.0621e-01)	Acc@1  94.00 ( 94.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.013 ( 0.028)	Loss 4.1796e-01 (3.2669e-01)	Acc@1  90.00 ( 91.36)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.015 ( 0.022)	Loss 3.9956e-01 (3.4212e-01)	Acc@1  88.00 ( 90.90)	Acc@5 100.00 ( 99.52)
Test: [ 30/100]	Time  0.020 ( 0.020)	Loss 2.5862e-01 (3.6328e-01)	Acc@1  92.00 ( 90.68)	Acc@5 100.00 ( 99.55)
Test: [ 40/100]	Time  0.010 ( 0.018)	Loss 5.4971e-01 (3.8262e-01)	Acc@1  89.00 ( 90.46)	Acc@5  99.00 ( 99.54)
Test: [ 50/100]	Time  0.012 ( 0.017)	Loss 2.0843e-01 (3.7677e-01)	Acc@1  93.00 ( 90.53)	Acc@5  99.00 ( 99.47)
Test: [ 60/100]	Time  0.011 ( 0.017)	Loss 4.1286e-01 (3.7411e-01)	Acc@1  88.00 ( 90.56)	Acc@5 100.00 ( 99.51)
Test: [ 70/100]	Time  0.016 ( 0.017)	Loss 6.1517e-01 (3.7381e-01)	Acc@1  90.00 ( 90.69)	Acc@5  98.00 ( 99.51)
Test: [ 80/100]	Time  0.022 ( 0.016)	Loss 3.5942e-01 (3.7040e-01)	Acc@1  91.00 ( 90.64)	Acc@5 100.00 ( 99.56)
Test: [ 90/100]	Time  0.012 ( 0.016)	Loss 3.0583e-01 (3.6738e-01)	Acc@1  92.00 ( 90.59)	Acc@5 100.00 ( 99.59)
 * Acc@1 90.640 Acc@5 99.620
### epoch[48] execution time: 12.147112131118774
EPOCH 49
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [49][  0/391]	Time  0.247 ( 0.247)	Data  0.220 ( 0.220)	Loss 2.8824e-02 (2.8824e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [49][ 10/391]	Time  0.027 ( 0.047)	Data  0.001 ( 0.022)	Loss 1.5064e-02 (3.4017e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [49][ 20/391]	Time  0.028 ( 0.037)	Data  0.005 ( 0.012)	Loss 6.4949e-02 (4.3303e-02)	Acc@1  96.09 ( 98.55)	Acc@5 100.00 ( 99.96)
Epoch: [49][ 30/391]	Time  0.033 ( 0.033)	Data  0.003 ( 0.009)	Loss 5.1932e-02 (4.5235e-02)	Acc@1  98.44 ( 98.46)	Acc@5 100.00 ( 99.97)
Epoch: [49][ 40/391]	Time  0.022 ( 0.031)	Data  0.002 ( 0.008)	Loss 1.3616e-02 (4.3858e-02)	Acc@1 100.00 ( 98.53)	Acc@5 100.00 ( 99.96)
Epoch: [49][ 50/391]	Time  0.023 ( 0.030)	Data  0.001 ( 0.006)	Loss 3.1657e-02 (4.4005e-02)	Acc@1  98.44 ( 98.59)	Acc@5 100.00 ( 99.97)
Epoch: [49][ 60/391]	Time  0.037 ( 0.030)	Data  0.002 ( 0.006)	Loss 2.9641e-02 (4.5786e-02)	Acc@1  98.44 ( 98.49)	Acc@5 100.00 ( 99.97)
Epoch: [49][ 70/391]	Time  0.029 ( 0.029)	Data  0.001 ( 0.005)	Loss 2.7919e-02 (4.5475e-02)	Acc@1  99.22 ( 98.48)	Acc@5 100.00 ( 99.98)
Epoch: [49][ 80/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.005)	Loss 3.7682e-02 (4.6130e-02)	Acc@1  97.66 ( 98.47)	Acc@5 100.00 ( 99.98)
Epoch: [49][ 90/391]	Time  0.024 ( 0.028)	Data  0.000 ( 0.005)	Loss 3.0553e-02 (4.5056e-02)	Acc@1  97.66 ( 98.50)	Acc@5 100.00 ( 99.98)
Epoch: [49][100/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.004)	Loss 8.6725e-02 (4.6432e-02)	Acc@1  98.44 ( 98.45)	Acc@5 100.00 ( 99.98)
Epoch: [49][110/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.9534e-02 (4.7552e-02)	Acc@1  99.22 ( 98.42)	Acc@5 100.00 ( 99.98)
Epoch: [49][120/391]	Time  0.025 ( 0.028)	Data  0.001 ( 0.004)	Loss 2.1566e-02 (4.6985e-02)	Acc@1  99.22 ( 98.42)	Acc@5 100.00 ( 99.98)
Epoch: [49][130/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.004)	Loss 9.0322e-02 (4.8711e-02)	Acc@1  98.44 ( 98.40)	Acc@5 100.00 ( 99.98)
Epoch: [49][140/391]	Time  0.026 ( 0.027)	Data  0.005 ( 0.004)	Loss 2.6281e-02 (4.9275e-02)	Acc@1  99.22 ( 98.37)	Acc@5 100.00 ( 99.98)
Epoch: [49][150/391]	Time  0.027 ( 0.027)	Data  0.006 ( 0.004)	Loss 3.0591e-02 (4.8591e-02)	Acc@1  99.22 ( 98.40)	Acc@5 100.00 ( 99.98)
Epoch: [49][160/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 9.0741e-02 (4.8901e-02)	Acc@1  96.88 ( 98.37)	Acc@5 100.00 ( 99.99)
Epoch: [49][170/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.004)	Loss 4.0173e-02 (4.8398e-02)	Acc@1  98.44 ( 98.38)	Acc@5 100.00 ( 99.98)
Epoch: [49][180/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.3659e-02 (4.8618e-02)	Acc@1  99.22 ( 98.36)	Acc@5 100.00 ( 99.98)
Epoch: [49][190/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.3583e-02 (4.9338e-02)	Acc@1  99.22 ( 98.33)	Acc@5 100.00 ( 99.98)
Epoch: [49][200/391]	Time  0.034 ( 0.027)	Data  0.002 ( 0.003)	Loss 4.8526e-02 (4.8893e-02)	Acc@1  96.88 ( 98.35)	Acc@5 100.00 ( 99.98)
Epoch: [49][210/391]	Time  0.027 ( 0.027)	Data  0.005 ( 0.003)	Loss 3.6506e-02 (4.8732e-02)	Acc@1  99.22 ( 98.36)	Acc@5 100.00 ( 99.99)
Epoch: [49][220/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 3.2897e-02 (4.9230e-02)	Acc@1  99.22 ( 98.35)	Acc@5 100.00 ( 99.98)
Epoch: [49][230/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.7679e-02 (4.9462e-02)	Acc@1  98.44 ( 98.35)	Acc@5 100.00 ( 99.98)
Epoch: [49][240/391]	Time  0.024 ( 0.027)	Data  0.005 ( 0.003)	Loss 2.2243e-02 (4.9198e-02)	Acc@1  99.22 ( 98.35)	Acc@5 100.00 ( 99.98)
Epoch: [49][250/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.4464e-02 (4.9909e-02)	Acc@1  97.66 ( 98.33)	Acc@5 100.00 ( 99.98)
Epoch: [49][260/391]	Time  0.032 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.0999e-02 (5.0533e-02)	Acc@1  99.22 ( 98.32)	Acc@5 100.00 ( 99.99)
Epoch: [49][270/391]	Time  0.030 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.9140e-02 (5.0490e-02)	Acc@1  98.44 ( 98.33)	Acc@5 100.00 ( 99.99)
Epoch: [49][280/391]	Time  0.027 ( 0.027)	Data  0.000 ( 0.003)	Loss 4.2413e-02 (5.0483e-02)	Acc@1  97.66 ( 98.33)	Acc@5 100.00 ( 99.99)
Epoch: [49][290/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 7.5053e-02 (5.0796e-02)	Acc@1  98.44 ( 98.32)	Acc@5 100.00 ( 99.99)
Epoch: [49][300/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 3.2489e-02 (5.0438e-02)	Acc@1  98.44 ( 98.33)	Acc@5 100.00 ( 99.99)
Epoch: [49][310/391]	Time  0.031 ( 0.027)	Data  0.011 ( 0.003)	Loss 5.9440e-02 (5.0444e-02)	Acc@1  98.44 ( 98.33)	Acc@5 100.00 ( 99.99)
Epoch: [49][320/391]	Time  0.030 ( 0.027)	Data  0.004 ( 0.003)	Loss 5.9281e-02 (5.0264e-02)	Acc@1  96.88 ( 98.34)	Acc@5 100.00 ( 99.99)
Epoch: [49][330/391]	Time  0.027 ( 0.027)	Data  0.002 ( 0.003)	Loss 3.6939e-02 (4.9963e-02)	Acc@1  99.22 ( 98.34)	Acc@5 100.00 ( 99.99)
Epoch: [49][340/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.2765e-02 (4.9921e-02)	Acc@1  98.44 ( 98.33)	Acc@5 100.00 ( 99.99)
Epoch: [49][350/391]	Time  0.031 ( 0.026)	Data  0.003 ( 0.003)	Loss 9.2450e-03 (5.0280e-02)	Acc@1 100.00 ( 98.33)	Acc@5 100.00 ( 99.99)
Epoch: [49][360/391]	Time  0.023 ( 0.026)	Data  0.003 ( 0.003)	Loss 1.6065e-02 (4.9875e-02)	Acc@1  99.22 ( 98.34)	Acc@5 100.00 ( 99.99)
Epoch: [49][370/391]	Time  0.026 ( 0.026)	Data  0.000 ( 0.003)	Loss 8.6196e-03 (4.9718e-02)	Acc@1 100.00 ( 98.34)	Acc@5 100.00 ( 99.99)
Epoch: [49][380/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.2469e-02 (4.9543e-02)	Acc@1  98.44 ( 98.35)	Acc@5 100.00 ( 99.99)
Epoch: [49][390/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.7634e-02 (4.9424e-02)	Acc@1  98.75 ( 98.35)	Acc@5 100.00 ( 99.99)
## e[49] optimizer.zero_grad (sum) time: 0.10822629928588867
## e[49]       loss.backward (sum) time: 2.2553350925445557
## e[49]      optimizer.step (sum) time: 0.879852294921875
## epoch[49] training(only) time: 10.462939739227295
# Switched to evaluate mode...
Test: [  0/100]	Time  0.164 ( 0.164)	Loss 2.3448e-01 (2.3448e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.011 ( 0.027)	Loss 3.9464e-01 (3.5114e-01)	Acc@1  89.00 ( 91.00)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.011 ( 0.021)	Loss 4.6875e-01 (3.6982e-01)	Acc@1  84.00 ( 90.43)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.012 ( 0.019)	Loss 3.3755e-01 (3.9128e-01)	Acc@1  89.00 ( 90.13)	Acc@5  99.00 ( 99.65)
Test: [ 40/100]	Time  0.015 ( 0.018)	Loss 4.5690e-01 (4.0772e-01)	Acc@1  89.00 ( 89.98)	Acc@5  99.00 ( 99.61)
Test: [ 50/100]	Time  0.011 ( 0.017)	Loss 1.9461e-01 (3.9890e-01)	Acc@1  94.00 ( 90.10)	Acc@5  99.00 ( 99.55)
Test: [ 60/100]	Time  0.012 ( 0.017)	Loss 4.7535e-01 (3.9476e-01)	Acc@1  88.00 ( 90.20)	Acc@5 100.00 ( 99.56)
Test: [ 70/100]	Time  0.017 ( 0.016)	Loss 7.3578e-01 (3.9512e-01)	Acc@1  90.00 ( 90.25)	Acc@5  99.00 ( 99.55)
Test: [ 80/100]	Time  0.014 ( 0.016)	Loss 3.0651e-01 (3.8962e-01)	Acc@1  91.00 ( 90.35)	Acc@5 100.00 ( 99.59)
Test: [ 90/100]	Time  0.012 ( 0.016)	Loss 2.7053e-01 (3.8815e-01)	Acc@1  94.00 ( 90.40)	Acc@5 100.00 ( 99.63)
 * Acc@1 90.480 Acc@5 99.630
### epoch[49] execution time: 12.158320426940918
EPOCH 50
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [50][  0/391]	Time  0.197 ( 0.197)	Data  0.174 ( 0.174)	Loss 4.5434e-02 (4.5434e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [50][ 10/391]	Time  0.023 ( 0.041)	Data  0.001 ( 0.018)	Loss 6.1956e-02 (3.9204e-02)	Acc@1  96.88 ( 98.08)	Acc@5 100.00 (100.00)
Epoch: [50][ 20/391]	Time  0.027 ( 0.034)	Data  0.001 ( 0.010)	Loss 5.1516e-02 (4.3238e-02)	Acc@1  99.22 ( 98.25)	Acc@5 100.00 ( 99.96)
Epoch: [50][ 30/391]	Time  0.031 ( 0.031)	Data  0.001 ( 0.008)	Loss 4.3373e-02 (4.4931e-02)	Acc@1  99.22 ( 98.34)	Acc@5 100.00 ( 99.97)
Epoch: [50][ 40/391]	Time  0.027 ( 0.030)	Data  0.005 ( 0.006)	Loss 3.6103e-02 (4.4940e-02)	Acc@1  99.22 ( 98.42)	Acc@5 100.00 ( 99.98)
Epoch: [50][ 50/391]	Time  0.024 ( 0.029)	Data  0.001 ( 0.006)	Loss 2.1961e-02 (4.4864e-02)	Acc@1  99.22 ( 98.41)	Acc@5 100.00 ( 99.97)
Epoch: [50][ 60/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.005)	Loss 7.7356e-02 (4.4903e-02)	Acc@1  96.09 ( 98.36)	Acc@5 100.00 ( 99.97)
Epoch: [50][ 70/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.8406e-02 (4.3280e-02)	Acc@1 100.00 ( 98.43)	Acc@5 100.00 ( 99.98)
Epoch: [50][ 80/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.004)	Loss 6.1730e-02 (4.4706e-02)	Acc@1  98.44 ( 98.42)	Acc@5 100.00 ( 99.98)
Epoch: [50][ 90/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.004)	Loss 2.1769e-02 (4.5042e-02)	Acc@1  99.22 ( 98.43)	Acc@5 100.00 ( 99.98)
Epoch: [50][100/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 9.9741e-03 (4.5065e-02)	Acc@1  99.22 ( 98.42)	Acc@5 100.00 ( 99.98)
Epoch: [50][110/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.7336e-02 (4.6015e-02)	Acc@1  98.44 ( 98.37)	Acc@5 100.00 ( 99.99)
Epoch: [50][120/391]	Time  0.040 ( 0.027)	Data  0.001 ( 0.004)	Loss 7.5184e-03 (4.5897e-02)	Acc@1 100.00 ( 98.39)	Acc@5 100.00 ( 99.98)
Epoch: [50][130/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.6914e-02 (4.5957e-02)	Acc@1 100.00 ( 98.40)	Acc@5 100.00 ( 99.98)
Epoch: [50][140/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.7621e-02 (4.6258e-02)	Acc@1  97.66 ( 98.38)	Acc@5 100.00 ( 99.98)
Epoch: [50][150/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.3242e-02 (4.6661e-02)	Acc@1  96.88 ( 98.37)	Acc@5 100.00 ( 99.98)
Epoch: [50][160/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.2693e-02 (4.6549e-02)	Acc@1  97.66 ( 98.36)	Acc@5 100.00 ( 99.99)
Epoch: [50][170/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.3860e-02 (4.7352e-02)	Acc@1  99.22 ( 98.33)	Acc@5 100.00 ( 99.99)
Epoch: [50][180/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.6024e-02 (4.7289e-02)	Acc@1 100.00 ( 98.35)	Acc@5 100.00 ( 99.99)
Epoch: [50][190/391]	Time  0.026 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.3398e-02 (4.6872e-02)	Acc@1  99.22 ( 98.37)	Acc@5 100.00 ( 99.99)
Epoch: [50][200/391]	Time  0.033 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.6895e-02 (4.6805e-02)	Acc@1  97.66 ( 98.37)	Acc@5 100.00 ( 99.99)
Epoch: [50][210/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7471e-01 (4.7481e-02)	Acc@1  94.53 ( 98.36)	Acc@5 100.00 ( 99.99)
Epoch: [50][220/391]	Time  0.024 ( 0.026)	Data  0.003 ( 0.003)	Loss 8.2811e-02 (4.8279e-02)	Acc@1  96.88 ( 98.31)	Acc@5 100.00 ( 99.99)
Epoch: [50][230/391]	Time  0.023 ( 0.026)	Data  0.000 ( 0.003)	Loss 8.5088e-02 (4.8882e-02)	Acc@1  96.88 ( 98.28)	Acc@5  99.22 ( 99.98)
Epoch: [50][240/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.0006e-02 (4.8935e-02)	Acc@1  96.88 ( 98.27)	Acc@5 100.00 ( 99.98)
Epoch: [50][250/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.6762e-02 (4.8813e-02)	Acc@1  96.88 ( 98.29)	Acc@5 100.00 ( 99.98)
Epoch: [50][260/391]	Time  0.031 ( 0.026)	Data  0.000 ( 0.003)	Loss 6.9302e-03 (4.8503e-02)	Acc@1 100.00 ( 98.31)	Acc@5 100.00 ( 99.98)
Epoch: [50][270/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.6091e-02 (4.8868e-02)	Acc@1  98.44 ( 98.30)	Acc@5 100.00 ( 99.98)
Epoch: [50][280/391]	Time  0.034 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.6341e-02 (4.8806e-02)	Acc@1  98.44 ( 98.30)	Acc@5 100.00 ( 99.98)
Epoch: [50][290/391]	Time  0.028 ( 0.026)	Data  0.005 ( 0.003)	Loss 3.0731e-02 (4.8756e-02)	Acc@1  99.22 ( 98.30)	Acc@5 100.00 ( 99.98)
Epoch: [50][300/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.7237e-02 (4.8870e-02)	Acc@1  99.22 ( 98.29)	Acc@5 100.00 ( 99.98)
Epoch: [50][310/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.5816e-02 (4.8634e-02)	Acc@1  97.66 ( 98.30)	Acc@5 100.00 ( 99.98)
Epoch: [50][320/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.3100e-02 (4.8625e-02)	Acc@1  98.44 ( 98.30)	Acc@5 100.00 ( 99.99)
Epoch: [50][330/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1783e-02 (4.8726e-02)	Acc@1  99.22 ( 98.28)	Acc@5 100.00 ( 99.99)
Epoch: [50][340/391]	Time  0.031 ( 0.026)	Data  0.002 ( 0.003)	Loss 9.0476e-02 (4.9215e-02)	Acc@1  94.53 ( 98.27)	Acc@5 100.00 ( 99.99)
Epoch: [50][350/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.1371e-02 (4.9137e-02)	Acc@1  98.44 ( 98.27)	Acc@5 100.00 ( 99.99)
Epoch: [50][360/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4636e-02 (4.8738e-02)	Acc@1  99.22 ( 98.28)	Acc@5 100.00 ( 99.99)
Epoch: [50][370/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6980e-02 (4.8621e-02)	Acc@1  99.22 ( 98.28)	Acc@5 100.00 ( 99.99)
Epoch: [50][380/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4785e-02 (4.8622e-02)	Acc@1 100.00 ( 98.27)	Acc@5 100.00 ( 99.99)
Epoch: [50][390/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0399e-01 (4.8541e-02)	Acc@1  96.25 ( 98.27)	Acc@5 100.00 ( 99.99)
## e[50] optimizer.zero_grad (sum) time: 0.10656452178955078
## e[50]       loss.backward (sum) time: 2.2696821689605713
## e[50]      optimizer.step (sum) time: 0.8606970310211182
## epoch[50] training(only) time: 10.406416416168213
# Switched to evaluate mode...
Test: [  0/100]	Time  0.180 ( 0.180)	Loss 2.4422e-01 (2.4422e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.011 ( 0.030)	Loss 3.9546e-01 (3.5335e-01)	Acc@1  89.00 ( 90.64)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.012 ( 0.022)	Loss 4.7415e-01 (3.7562e-01)	Acc@1  85.00 ( 90.43)	Acc@5 100.00 ( 99.48)
Test: [ 30/100]	Time  0.019 ( 0.020)	Loss 3.4334e-01 (3.8678e-01)	Acc@1  90.00 ( 90.32)	Acc@5  99.00 ( 99.52)
Test: [ 40/100]	Time  0.011 ( 0.019)	Loss 4.8608e-01 (4.0296e-01)	Acc@1  90.00 ( 90.07)	Acc@5 100.00 ( 99.56)
Test: [ 50/100]	Time  0.012 ( 0.018)	Loss 1.6356e-01 (3.9537e-01)	Acc@1  96.00 ( 90.25)	Acc@5  99.00 ( 99.53)
Test: [ 60/100]	Time  0.024 ( 0.017)	Loss 5.2904e-01 (3.9797e-01)	Acc@1  84.00 ( 90.21)	Acc@5 100.00 ( 99.56)
Test: [ 70/100]	Time  0.011 ( 0.017)	Loss 7.5646e-01 (3.9505e-01)	Acc@1  88.00 ( 90.34)	Acc@5  99.00 ( 99.54)
Test: [ 80/100]	Time  0.018 ( 0.017)	Loss 2.6862e-01 (3.9149e-01)	Acc@1  94.00 ( 90.46)	Acc@5 100.00 ( 99.59)
Test: [ 90/100]	Time  0.021 ( 0.017)	Loss 2.4702e-01 (3.8865e-01)	Acc@1  93.00 ( 90.47)	Acc@5 100.00 ( 99.63)
 * Acc@1 90.490 Acc@5 99.650
### epoch[50] execution time: 12.148789167404175
EPOCH 51
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [51][  0/391]	Time  0.183 ( 0.183)	Data  0.162 ( 0.162)	Loss 9.6991e-02 (9.6991e-02)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [51][ 10/391]	Time  0.032 ( 0.041)	Data  0.001 ( 0.017)	Loss 2.0843e-02 (3.7734e-02)	Acc@1  99.22 ( 98.79)	Acc@5 100.00 (100.00)
Epoch: [51][ 20/391]	Time  0.024 ( 0.034)	Data  0.001 ( 0.010)	Loss 1.2166e-01 (4.6098e-02)	Acc@1  96.09 ( 98.59)	Acc@5 100.00 (100.00)
Epoch: [51][ 30/391]	Time  0.024 ( 0.031)	Data  0.001 ( 0.007)	Loss 6.1228e-02 (4.4823e-02)	Acc@1  98.44 ( 98.66)	Acc@5 100.00 (100.00)
Epoch: [51][ 40/391]	Time  0.024 ( 0.030)	Data  0.001 ( 0.006)	Loss 6.0702e-02 (4.4510e-02)	Acc@1  97.66 ( 98.63)	Acc@5 100.00 (100.00)
Epoch: [51][ 50/391]	Time  0.024 ( 0.029)	Data  0.001 ( 0.005)	Loss 4.7317e-02 (4.3660e-02)	Acc@1  97.66 ( 98.61)	Acc@5 100.00 (100.00)
Epoch: [51][ 60/391]	Time  0.022 ( 0.028)	Data  0.001 ( 0.005)	Loss 4.1980e-02 (4.3815e-02)	Acc@1  97.66 ( 98.58)	Acc@5 100.00 (100.00)
Epoch: [51][ 70/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 3.2815e-02 (4.2896e-02)	Acc@1  99.22 ( 98.59)	Acc@5 100.00 (100.00)
Epoch: [51][ 80/391]	Time  0.033 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.1479e-01 (4.4238e-02)	Acc@1  95.31 ( 98.50)	Acc@5 100.00 (100.00)
Epoch: [51][ 90/391]	Time  0.026 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.6569e-02 (4.3328e-02)	Acc@1 100.00 ( 98.53)	Acc@5 100.00 (100.00)
Epoch: [51][100/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.3348e-02 (4.4637e-02)	Acc@1  99.22 ( 98.48)	Acc@5 100.00 (100.00)
Epoch: [51][110/391]	Time  0.026 ( 0.027)	Data  0.001 ( 0.004)	Loss 8.8672e-02 (4.4591e-02)	Acc@1  95.31 ( 98.47)	Acc@5 100.00 (100.00)
Epoch: [51][120/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.003)	Loss 9.0722e-02 (4.5374e-02)	Acc@1  96.88 ( 98.42)	Acc@5 100.00 (100.00)
Epoch: [51][130/391]	Time  0.026 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.3850e-02 (4.5147e-02)	Acc@1  97.66 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [51][140/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.4435e-02 (4.5723e-02)	Acc@1  98.44 ( 98.39)	Acc@5 100.00 (100.00)
Epoch: [51][150/391]	Time  0.041 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.4572e-02 (4.5227e-02)	Acc@1  99.22 ( 98.42)	Acc@5 100.00 ( 99.99)
Epoch: [51][160/391]	Time  0.025 ( 0.027)	Data  0.005 ( 0.003)	Loss 7.8321e-02 (4.5740e-02)	Acc@1  98.44 ( 98.42)	Acc@5  99.22 ( 99.99)
Epoch: [51][170/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.1519e-02 (4.5134e-02)	Acc@1  98.44 ( 98.45)	Acc@5 100.00 ( 99.99)
Epoch: [51][180/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.8167e-02 (4.5410e-02)	Acc@1  98.44 ( 98.45)	Acc@5 100.00 ( 99.99)
Epoch: [51][190/391]	Time  0.033 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.4320e-02 (4.6252e-02)	Acc@1  96.88 ( 98.40)	Acc@5 100.00 ( 99.99)
Epoch: [51][200/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.1958e-02 (4.7194e-02)	Acc@1  98.44 ( 98.35)	Acc@5 100.00 ( 99.99)
Epoch: [51][210/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.003)	Loss 8.7602e-02 (4.7869e-02)	Acc@1  97.66 ( 98.34)	Acc@5 100.00 ( 99.99)
Epoch: [51][220/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.8550e-02 (4.8292e-02)	Acc@1  99.22 ( 98.34)	Acc@5 100.00 ( 99.99)
Epoch: [51][230/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 7.0936e-02 (4.8100e-02)	Acc@1  96.09 ( 98.33)	Acc@5 100.00 ( 99.99)
Epoch: [51][240/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.0528e-01 (4.8670e-02)	Acc@1  98.44 ( 98.33)	Acc@5 100.00 ( 99.99)
Epoch: [51][250/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.0959e-02 (4.8938e-02)	Acc@1  98.44 ( 98.31)	Acc@5 100.00 ( 99.99)
Epoch: [51][260/391]	Time  0.026 ( 0.027)	Data  0.002 ( 0.003)	Loss 6.5651e-02 (4.9492e-02)	Acc@1  97.66 ( 98.29)	Acc@5 100.00 ( 99.99)
Epoch: [51][270/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.9551e-02 (4.9633e-02)	Acc@1  99.22 ( 98.30)	Acc@5 100.00 ( 99.99)
Epoch: [51][280/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.3658e-02 (4.9967e-02)	Acc@1  97.66 ( 98.29)	Acc@5 100.00 ( 99.99)
Epoch: [51][290/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.4162e-02 (5.0514e-02)	Acc@1  98.44 ( 98.28)	Acc@5 100.00 ( 99.99)
Epoch: [51][300/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.9513e-02 (5.0315e-02)	Acc@1  98.44 ( 98.29)	Acc@5 100.00 ( 99.99)
Epoch: [51][310/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.4721e-02 (5.0129e-02)	Acc@1  98.44 ( 98.30)	Acc@5 100.00 ( 99.99)
Epoch: [51][320/391]	Time  0.025 ( 0.026)	Data  0.000 ( 0.003)	Loss 6.6414e-02 (4.9929e-02)	Acc@1  99.22 ( 98.31)	Acc@5 100.00 ( 99.99)
Epoch: [51][330/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.3197e-02 (4.9891e-02)	Acc@1  97.66 ( 98.31)	Acc@5 100.00 ( 99.99)
Epoch: [51][340/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4330e-02 (4.9660e-02)	Acc@1 100.00 ( 98.33)	Acc@5 100.00 ( 99.99)
Epoch: [51][350/391]	Time  0.030 ( 0.026)	Data  0.005 ( 0.003)	Loss 6.4002e-02 (4.9806e-02)	Acc@1  96.88 ( 98.32)	Acc@5 100.00 ( 99.99)
Epoch: [51][360/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.0392e-02 (4.9811e-02)	Acc@1 100.00 ( 98.32)	Acc@5 100.00 ( 99.99)
Epoch: [51][370/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.9630e-02 (4.9916e-02)	Acc@1  98.44 ( 98.31)	Acc@5 100.00 ( 99.99)
Epoch: [51][380/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.1769e-02 (4.9676e-02)	Acc@1  96.88 ( 98.31)	Acc@5 100.00 ( 99.99)
Epoch: [51][390/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.3683e-02 (4.9297e-02)	Acc@1  96.25 ( 98.32)	Acc@5 100.00 ( 99.99)
## e[51] optimizer.zero_grad (sum) time: 0.10992288589477539
## e[51]       loss.backward (sum) time: 2.2248764038085938
## e[51]      optimizer.step (sum) time: 0.8580832481384277
## epoch[51] training(only) time: 10.448724508285522
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 2.9287e-01 (2.9287e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.027 ( 0.029)	Loss 4.9029e-01 (3.5390e-01)	Acc@1  89.00 ( 89.91)	Acc@5  99.00 ( 99.73)
Test: [ 20/100]	Time  0.012 ( 0.022)	Loss 4.5407e-01 (3.7172e-01)	Acc@1  83.00 ( 89.71)	Acc@5 100.00 ( 99.48)
Test: [ 30/100]	Time  0.012 ( 0.019)	Loss 2.8479e-01 (3.8551e-01)	Acc@1  93.00 ( 89.90)	Acc@5  99.00 ( 99.52)
Test: [ 40/100]	Time  0.012 ( 0.018)	Loss 4.6103e-01 (3.9571e-01)	Acc@1  88.00 ( 89.61)	Acc@5 100.00 ( 99.59)
Test: [ 50/100]	Time  0.015 ( 0.017)	Loss 2.0872e-01 (3.9396e-01)	Acc@1  96.00 ( 89.76)	Acc@5  99.00 ( 99.57)
Test: [ 60/100]	Time  0.028 ( 0.017)	Loss 4.4089e-01 (3.8991e-01)	Acc@1  88.00 ( 89.87)	Acc@5 100.00 ( 99.61)
Test: [ 70/100]	Time  0.011 ( 0.016)	Loss 6.5633e-01 (3.8693e-01)	Acc@1  91.00 ( 90.10)	Acc@5  99.00 ( 99.61)
Test: [ 80/100]	Time  0.016 ( 0.016)	Loss 3.7483e-01 (3.8208e-01)	Acc@1  89.00 ( 90.12)	Acc@5 100.00 ( 99.62)
Test: [ 90/100]	Time  0.011 ( 0.016)	Loss 3.0380e-01 (3.8084e-01)	Acc@1  94.00 ( 90.13)	Acc@5 100.00 ( 99.65)
 * Acc@1 90.200 Acc@5 99.660
### epoch[51] execution time: 12.153149127960205
EPOCH 52
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [52][  0/391]	Time  0.201 ( 0.201)	Data  0.179 ( 0.179)	Loss 5.3823e-02 (5.3823e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [52][ 10/391]	Time  0.026 ( 0.042)	Data  0.001 ( 0.018)	Loss 3.0914e-02 (3.9161e-02)	Acc@1  99.22 ( 98.65)	Acc@5 100.00 (100.00)
Epoch: [52][ 20/391]	Time  0.024 ( 0.034)	Data  0.001 ( 0.010)	Loss 3.0735e-02 (4.2325e-02)	Acc@1  98.44 ( 98.40)	Acc@5 100.00 (100.00)
Epoch: [52][ 30/391]	Time  0.029 ( 0.032)	Data  0.009 ( 0.008)	Loss 9.8337e-03 (3.6441e-02)	Acc@1 100.00 ( 98.74)	Acc@5 100.00 (100.00)
Epoch: [52][ 40/391]	Time  0.024 ( 0.030)	Data  0.001 ( 0.007)	Loss 8.7176e-02 (3.9572e-02)	Acc@1  97.66 ( 98.63)	Acc@5 100.00 (100.00)
Epoch: [52][ 50/391]	Time  0.023 ( 0.029)	Data  0.001 ( 0.006)	Loss 1.8158e-02 (4.1721e-02)	Acc@1  99.22 ( 98.58)	Acc@5 100.00 (100.00)
Epoch: [52][ 60/391]	Time  0.024 ( 0.029)	Data  0.001 ( 0.005)	Loss 3.0625e-02 (4.0646e-02)	Acc@1  98.44 ( 98.64)	Acc@5 100.00 (100.00)
Epoch: [52][ 70/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.1811e-01 (4.2485e-02)	Acc@1  96.09 ( 98.53)	Acc@5  99.22 ( 99.99)
Epoch: [52][ 80/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 2.2780e-02 (4.0473e-02)	Acc@1  99.22 ( 98.59)	Acc@5 100.00 ( 99.99)
Epoch: [52][ 90/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 4.5334e-02 (4.1329e-02)	Acc@1  99.22 ( 98.57)	Acc@5 100.00 ( 99.99)
Epoch: [52][100/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 4.8189e-02 (4.1593e-02)	Acc@1  98.44 ( 98.53)	Acc@5 100.00 ( 99.99)
Epoch: [52][110/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.8565e-02 (4.1946e-02)	Acc@1  99.22 ( 98.54)	Acc@5 100.00 ( 99.99)
Epoch: [52][120/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.5543e-02 (4.2029e-02)	Acc@1  96.88 ( 98.55)	Acc@5 100.00 ( 99.99)
Epoch: [52][130/391]	Time  0.023 ( 0.027)	Data  0.000 ( 0.004)	Loss 3.1890e-02 (4.1768e-02)	Acc@1  97.66 ( 98.56)	Acc@5 100.00 ( 99.99)
Epoch: [52][140/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.5546e-02 (4.1853e-02)	Acc@1  99.22 ( 98.56)	Acc@5 100.00 ( 99.99)
Epoch: [52][150/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.7439e-02 (4.2220e-02)	Acc@1  96.09 ( 98.54)	Acc@5 100.00 ( 99.99)
Epoch: [52][160/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.6994e-02 (4.3018e-02)	Acc@1  96.88 ( 98.49)	Acc@5 100.00 ( 99.99)
Epoch: [52][170/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.5718e-02 (4.2798e-02)	Acc@1  98.44 ( 98.52)	Acc@5 100.00 ( 99.99)
Epoch: [52][180/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.0824e-02 (4.3452e-02)	Acc@1  97.66 ( 98.52)	Acc@5 100.00 ( 99.99)
Epoch: [52][190/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.3468e-02 (4.3147e-02)	Acc@1  99.22 ( 98.52)	Acc@5 100.00 ( 99.99)
Epoch: [52][200/391]	Time  0.030 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.2366e-02 (4.2779e-02)	Acc@1  96.88 ( 98.52)	Acc@5 100.00 ( 99.99)
Epoch: [52][210/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.5627e-02 (4.2813e-02)	Acc@1 100.00 ( 98.54)	Acc@5 100.00 ( 99.99)
Epoch: [52][220/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.5929e-02 (4.2801e-02)	Acc@1  98.44 ( 98.53)	Acc@5 100.00 ( 99.99)
Epoch: [52][230/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.6539e-02 (4.2482e-02)	Acc@1  98.44 ( 98.54)	Acc@5 100.00 ( 99.99)
Epoch: [52][240/391]	Time  0.030 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.2000e-02 (4.2771e-02)	Acc@1  98.44 ( 98.53)	Acc@5 100.00 ( 99.99)
Epoch: [52][250/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.2500e-03 (4.2905e-02)	Acc@1 100.00 ( 98.53)	Acc@5 100.00 ( 99.99)
Epoch: [52][260/391]	Time  0.024 ( 0.026)	Data  0.004 ( 0.003)	Loss 3.9171e-02 (4.2775e-02)	Acc@1  97.66 ( 98.53)	Acc@5 100.00 ( 99.99)
Epoch: [52][270/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.9778e-02 (4.2591e-02)	Acc@1  97.66 ( 98.53)	Acc@5 100.00 ( 99.99)
Epoch: [52][280/391]	Time  0.032 ( 0.026)	Data  0.003 ( 0.003)	Loss 5.3147e-02 (4.2707e-02)	Acc@1  96.88 ( 98.51)	Acc@5 100.00 ( 99.99)
Epoch: [52][290/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 4.6780e-02 (4.2488e-02)	Acc@1  98.44 ( 98.52)	Acc@5 100.00 ( 99.99)
Epoch: [52][300/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.6351e-02 (4.2342e-02)	Acc@1  98.44 ( 98.54)	Acc@5 100.00 ( 99.99)
Epoch: [52][310/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.7242e-02 (4.2631e-02)	Acc@1  99.22 ( 98.53)	Acc@5 100.00 ( 99.99)
Epoch: [52][320/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.5909e-02 (4.2910e-02)	Acc@1  98.44 ( 98.53)	Acc@5 100.00 ( 99.99)
Epoch: [52][330/391]	Time  0.035 ( 0.026)	Data  0.000 ( 0.003)	Loss 8.3434e-02 (4.2907e-02)	Acc@1  98.44 ( 98.53)	Acc@5 100.00 ( 99.99)
Epoch: [52][340/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.4298e-02 (4.2875e-02)	Acc@1  98.44 ( 98.54)	Acc@5 100.00 ( 99.99)
Epoch: [52][350/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.3846e-03 (4.2815e-02)	Acc@1 100.00 ( 98.54)	Acc@5 100.00 ( 99.99)
Epoch: [52][360/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.3748e-02 (4.3521e-02)	Acc@1  96.88 ( 98.51)	Acc@5 100.00 ( 99.99)
Epoch: [52][370/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.9128e-03 (4.3403e-02)	Acc@1 100.00 ( 98.52)	Acc@5 100.00 ( 99.99)
Epoch: [52][380/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.1374e-02 (4.3408e-02)	Acc@1  98.44 ( 98.54)	Acc@5 100.00 ( 99.99)
Epoch: [52][390/391]	Time  0.025 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.5325e-02 (4.3672e-02)	Acc@1  98.75 ( 98.53)	Acc@5 100.00 ( 99.99)
## e[52] optimizer.zero_grad (sum) time: 0.10934901237487793
## e[52]       loss.backward (sum) time: 2.2335524559020996
## e[52]      optimizer.step (sum) time: 0.8798198699951172
## epoch[52] training(only) time: 10.423455953598022
# Switched to evaluate mode...
Test: [  0/100]	Time  0.167 ( 0.167)	Loss 2.2633e-01 (2.2633e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.019 ( 0.028)	Loss 3.9735e-01 (3.5280e-01)	Acc@1  92.00 ( 91.73)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.011 ( 0.021)	Loss 4.0822e-01 (3.7623e-01)	Acc@1  89.00 ( 91.05)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.016 ( 0.019)	Loss 3.4403e-01 (3.9544e-01)	Acc@1  92.00 ( 90.55)	Acc@5 100.00 ( 99.55)
Test: [ 40/100]	Time  0.016 ( 0.018)	Loss 5.6383e-01 (4.1646e-01)	Acc@1  89.00 ( 90.32)	Acc@5  99.00 ( 99.56)
Test: [ 50/100]	Time  0.011 ( 0.017)	Loss 2.1304e-01 (4.0833e-01)	Acc@1  95.00 ( 90.31)	Acc@5  99.00 ( 99.55)
Test: [ 60/100]	Time  0.010 ( 0.017)	Loss 4.4020e-01 (4.0347e-01)	Acc@1  90.00 ( 90.44)	Acc@5 100.00 ( 99.56)
Test: [ 70/100]	Time  0.021 ( 0.017)	Loss 8.0048e-01 (4.0239e-01)	Acc@1  89.00 ( 90.63)	Acc@5  98.00 ( 99.55)
Test: [ 80/100]	Time  0.023 ( 0.016)	Loss 3.6755e-01 (3.9996e-01)	Acc@1  92.00 ( 90.64)	Acc@5 100.00 ( 99.58)
Test: [ 90/100]	Time  0.015 ( 0.016)	Loss 3.0074e-01 (3.9879e-01)	Acc@1  93.00 ( 90.52)	Acc@5  99.00 ( 99.60)
 * Acc@1 90.540 Acc@5 99.610
### epoch[52] execution time: 12.151668548583984
EPOCH 53
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [53][  0/391]	Time  0.192 ( 0.192)	Data  0.171 ( 0.171)	Loss 3.5502e-02 (3.5502e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [53][ 10/391]	Time  0.025 ( 0.041)	Data  0.003 ( 0.017)	Loss 1.2678e-02 (3.9397e-02)	Acc@1 100.00 ( 98.93)	Acc@5 100.00 ( 99.93)
Epoch: [53][ 20/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.010)	Loss 2.6351e-02 (3.6084e-02)	Acc@1  98.44 ( 98.81)	Acc@5 100.00 ( 99.96)
Epoch: [53][ 30/391]	Time  0.024 ( 0.031)	Data  0.001 ( 0.008)	Loss 4.6874e-03 (4.0954e-02)	Acc@1 100.00 ( 98.59)	Acc@5 100.00 ( 99.97)
Epoch: [53][ 40/391]	Time  0.027 ( 0.030)	Data  0.001 ( 0.006)	Loss 7.6308e-03 (4.1095e-02)	Acc@1 100.00 ( 98.61)	Acc@5 100.00 ( 99.98)
Epoch: [53][ 50/391]	Time  0.024 ( 0.029)	Data  0.001 ( 0.005)	Loss 5.4959e-02 (4.0215e-02)	Acc@1  97.66 ( 98.68)	Acc@5 100.00 ( 99.98)
Epoch: [53][ 60/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.005)	Loss 7.4449e-02 (4.1016e-02)	Acc@1  97.66 ( 98.67)	Acc@5 100.00 ( 99.99)
Epoch: [53][ 70/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 3.1908e-02 (3.9864e-02)	Acc@1  99.22 ( 98.69)	Acc@5 100.00 ( 99.99)
Epoch: [53][ 80/391]	Time  0.024 ( 0.028)	Data  0.000 ( 0.004)	Loss 2.2684e-02 (3.9871e-02)	Acc@1  99.22 ( 98.71)	Acc@5 100.00 ( 99.99)
Epoch: [53][ 90/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.004)	Loss 5.8162e-02 (4.0377e-02)	Acc@1  98.44 ( 98.71)	Acc@5 100.00 ( 99.99)
Epoch: [53][100/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.004)	Loss 4.1696e-02 (4.1255e-02)	Acc@1  99.22 ( 98.69)	Acc@5 100.00 ( 99.99)
Epoch: [53][110/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 6.9264e-02 (4.1981e-02)	Acc@1  98.44 ( 98.65)	Acc@5 100.00 ( 99.99)
Epoch: [53][120/391]	Time  0.039 ( 0.027)	Data  0.004 ( 0.004)	Loss 2.2378e-02 (4.1978e-02)	Acc@1  99.22 ( 98.64)	Acc@5 100.00 ( 99.99)
Epoch: [53][130/391]	Time  0.023 ( 0.027)	Data  0.000 ( 0.004)	Loss 5.5159e-02 (4.1586e-02)	Acc@1  98.44 ( 98.62)	Acc@5 100.00 ( 99.99)
Epoch: [53][140/391]	Time  0.025 ( 0.027)	Data  0.004 ( 0.003)	Loss 6.1481e-03 (4.0931e-02)	Acc@1 100.00 ( 98.61)	Acc@5 100.00 ( 99.99)
Epoch: [53][150/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.2236e-02 (4.0695e-02)	Acc@1  98.44 ( 98.61)	Acc@5 100.00 ( 99.99)
Epoch: [53][160/391]	Time  0.029 ( 0.027)	Data  0.000 ( 0.003)	Loss 5.7284e-02 (4.0414e-02)	Acc@1  96.88 ( 98.64)	Acc@5 100.00 (100.00)
Epoch: [53][170/391]	Time  0.038 ( 0.027)	Data  0.001 ( 0.003)	Loss 8.1797e-02 (4.0654e-02)	Acc@1  96.88 ( 98.63)	Acc@5 100.00 (100.00)
Epoch: [53][180/391]	Time  0.027 ( 0.027)	Data  0.000 ( 0.003)	Loss 9.0855e-02 (4.1196e-02)	Acc@1  97.66 ( 98.63)	Acc@5 100.00 (100.00)
Epoch: [53][190/391]	Time  0.023 ( 0.027)	Data  0.000 ( 0.003)	Loss 5.8120e-02 (4.1797e-02)	Acc@1  98.44 ( 98.62)	Acc@5 100.00 ( 99.99)
Epoch: [53][200/391]	Time  0.028 ( 0.027)	Data  0.008 ( 0.003)	Loss 2.0804e-02 (4.1425e-02)	Acc@1  99.22 ( 98.62)	Acc@5 100.00 ( 99.99)
Epoch: [53][210/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.4719e-02 (4.1769e-02)	Acc@1  99.22 ( 98.61)	Acc@5 100.00 ( 99.99)
Epoch: [53][220/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.2117e-02 (4.1896e-02)	Acc@1  98.44 ( 98.60)	Acc@5 100.00 ( 99.99)
Epoch: [53][230/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.6829e-02 (4.2222e-02)	Acc@1  98.44 ( 98.58)	Acc@5 100.00 ( 99.99)
Epoch: [53][240/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.8548e-02 (4.1969e-02)	Acc@1  98.44 ( 98.59)	Acc@5 100.00 ( 99.99)
Epoch: [53][250/391]	Time  0.030 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.0344e-02 (4.2438e-02)	Acc@1  97.66 ( 98.57)	Acc@5 100.00 ( 99.99)
Epoch: [53][260/391]	Time  0.024 ( 0.026)	Data  0.004 ( 0.003)	Loss 3.2744e-02 (4.2130e-02)	Acc@1  98.44 ( 98.58)	Acc@5 100.00 ( 99.99)
Epoch: [53][270/391]	Time  0.024 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.2675e-02 (4.1854e-02)	Acc@1  99.22 ( 98.59)	Acc@5 100.00 ( 99.99)
Epoch: [53][280/391]	Time  0.028 ( 0.026)	Data  0.005 ( 0.003)	Loss 2.1856e-02 (4.1854e-02)	Acc@1  98.44 ( 98.59)	Acc@5 100.00 ( 99.99)
Epoch: [53][290/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.2994e-02 (4.2032e-02)	Acc@1  98.44 ( 98.58)	Acc@5 100.00 ( 99.99)
Epoch: [53][300/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 5.6190e-03 (4.2724e-02)	Acc@1 100.00 ( 98.55)	Acc@5 100.00 ( 99.99)
Epoch: [53][310/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1088e-02 (4.2663e-02)	Acc@1  99.22 ( 98.56)	Acc@5 100.00 ( 99.99)
Epoch: [53][320/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2851e-02 (4.2678e-02)	Acc@1 100.00 ( 98.56)	Acc@5 100.00 ( 99.99)
Epoch: [53][330/391]	Time  0.032 ( 0.026)	Data  0.003 ( 0.003)	Loss 3.3586e-02 (4.2778e-02)	Acc@1  98.44 ( 98.56)	Acc@5 100.00 ( 99.99)
Epoch: [53][340/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.7775e-02 (4.2854e-02)	Acc@1  98.44 ( 98.56)	Acc@5 100.00 ( 99.99)
Epoch: [53][350/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.0265e-02 (4.2531e-02)	Acc@1  99.22 ( 98.58)	Acc@5 100.00 ( 99.99)
Epoch: [53][360/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.1775e-02 (4.2372e-02)	Acc@1  98.44 ( 98.58)	Acc@5 100.00 ( 99.99)
Epoch: [53][370/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7160e-02 (4.2222e-02)	Acc@1  99.22 ( 98.59)	Acc@5 100.00 ( 99.99)
Epoch: [53][380/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.1010e-02 (4.2530e-02)	Acc@1  98.44 ( 98.58)	Acc@5 100.00 ( 99.99)
Epoch: [53][390/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3022e-02 (4.2243e-02)	Acc@1  98.75 ( 98.59)	Acc@5 100.00 ( 99.99)
## e[53] optimizer.zero_grad (sum) time: 0.109649658203125
## e[53]       loss.backward (sum) time: 2.2634189128875732
## e[53]      optimizer.step (sum) time: 0.8858263492584229
## epoch[53] training(only) time: 10.37286114692688
# Switched to evaluate mode...
Test: [  0/100]	Time  0.164 ( 0.164)	Loss 2.8361e-01 (2.8361e-01)	Acc@1  91.00 ( 91.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.016 ( 0.027)	Loss 3.8962e-01 (3.8013e-01)	Acc@1  89.00 ( 90.36)	Acc@5 100.00 ( 99.55)
Test: [ 20/100]	Time  0.012 ( 0.021)	Loss 4.2451e-01 (3.8836e-01)	Acc@1  88.00 ( 90.29)	Acc@5 100.00 ( 99.52)
Test: [ 30/100]	Time  0.011 ( 0.019)	Loss 3.0959e-01 (4.0497e-01)	Acc@1  92.00 ( 90.29)	Acc@5 100.00 ( 99.52)
Test: [ 40/100]	Time  0.024 ( 0.018)	Loss 5.1560e-01 (4.2331e-01)	Acc@1  88.00 ( 90.22)	Acc@5 100.00 ( 99.54)
Test: [ 50/100]	Time  0.010 ( 0.017)	Loss 1.8243e-01 (4.1393e-01)	Acc@1  94.00 ( 90.43)	Acc@5  99.00 ( 99.51)
Test: [ 60/100]	Time  0.013 ( 0.017)	Loss 4.2202e-01 (4.0396e-01)	Acc@1  89.00 ( 90.62)	Acc@5 100.00 ( 99.54)
Test: [ 70/100]	Time  0.013 ( 0.016)	Loss 7.6619e-01 (4.0210e-01)	Acc@1  89.00 ( 90.76)	Acc@5 100.00 ( 99.56)
Test: [ 80/100]	Time  0.016 ( 0.016)	Loss 3.4612e-01 (4.0102e-01)	Acc@1  89.00 ( 90.67)	Acc@5 100.00 ( 99.59)
Test: [ 90/100]	Time  0.012 ( 0.016)	Loss 2.4615e-01 (3.9806e-01)	Acc@1  93.00 ( 90.71)	Acc@5 100.00 ( 99.62)
 * Acc@1 90.790 Acc@5 99.620
### epoch[53] execution time: 12.091820240020752
EPOCH 54
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [54][  0/391]	Time  0.186 ( 0.186)	Data  0.165 ( 0.165)	Loss 2.9216e-02 (2.9216e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [54][ 10/391]	Time  0.023 ( 0.041)	Data  0.001 ( 0.017)	Loss 4.3078e-02 (4.3071e-02)	Acc@1  98.44 ( 98.22)	Acc@5 100.00 (100.00)
Epoch: [54][ 20/391]	Time  0.024 ( 0.034)	Data  0.001 ( 0.010)	Loss 3.9060e-02 (4.0308e-02)	Acc@1  97.66 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [54][ 30/391]	Time  0.022 ( 0.031)	Data  0.001 ( 0.007)	Loss 2.8726e-02 (3.7272e-02)	Acc@1  99.22 ( 98.49)	Acc@5 100.00 (100.00)
Epoch: [54][ 40/391]	Time  0.023 ( 0.030)	Data  0.001 ( 0.006)	Loss 3.5112e-02 (3.6721e-02)	Acc@1  98.44 ( 98.65)	Acc@5 100.00 ( 99.98)
Epoch: [54][ 50/391]	Time  0.032 ( 0.029)	Data  0.000 ( 0.005)	Loss 1.9130e-02 (4.0701e-02)	Acc@1  99.22 ( 98.50)	Acc@5 100.00 ( 99.97)
Epoch: [54][ 60/391]	Time  0.026 ( 0.028)	Data  0.001 ( 0.005)	Loss 6.3158e-02 (4.1939e-02)	Acc@1  96.88 ( 98.46)	Acc@5 100.00 ( 99.96)
Epoch: [54][ 70/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 5.0843e-02 (4.0842e-02)	Acc@1  99.22 ( 98.53)	Acc@5 100.00 ( 99.97)
Epoch: [54][ 80/391]	Time  0.022 ( 0.028)	Data  0.000 ( 0.004)	Loss 5.1130e-02 (4.0532e-02)	Acc@1  98.44 ( 98.55)	Acc@5 100.00 ( 99.97)
Epoch: [54][ 90/391]	Time  0.028 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.5238e-02 (4.0906e-02)	Acc@1 100.00 ( 98.57)	Acc@5 100.00 ( 99.97)
Epoch: [54][100/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.004)	Loss 4.6468e-02 (3.9472e-02)	Acc@1  97.66 ( 98.63)	Acc@5 100.00 ( 99.98)
Epoch: [54][110/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 5.2864e-02 (4.0376e-02)	Acc@1  99.22 ( 98.63)	Acc@5 100.00 ( 99.98)
Epoch: [54][120/391]	Time  0.032 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.8011e-02 (4.0112e-02)	Acc@1  98.44 ( 98.61)	Acc@5 100.00 ( 99.98)
Epoch: [54][130/391]	Time  0.023 ( 0.027)	Data  0.000 ( 0.003)	Loss 8.2992e-02 (4.0072e-02)	Acc@1  97.66 ( 98.60)	Acc@5 100.00 ( 99.98)
Epoch: [54][140/391]	Time  0.030 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.2564e-02 (4.0290e-02)	Acc@1  97.66 ( 98.57)	Acc@5 100.00 ( 99.98)
Epoch: [54][150/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.8624e-03 (4.1464e-02)	Acc@1 100.00 ( 98.54)	Acc@5 100.00 ( 99.98)
Epoch: [54][160/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.2278e-02 (4.0912e-02)	Acc@1  98.44 ( 98.57)	Acc@5 100.00 ( 99.99)
Epoch: [54][170/391]	Time  0.030 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.8805e-02 (4.0607e-02)	Acc@1  98.44 ( 98.57)	Acc@5 100.00 ( 99.99)
Epoch: [54][180/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.3726e-02 (4.0345e-02)	Acc@1  99.22 ( 98.58)	Acc@5 100.00 ( 99.99)
Epoch: [54][190/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.1159e-02 (4.0597e-02)	Acc@1  98.44 ( 98.58)	Acc@5 100.00 ( 99.99)
Epoch: [54][200/391]	Time  0.031 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.0593e-02 (4.0173e-02)	Acc@1  99.22 ( 98.58)	Acc@5 100.00 ( 99.99)
Epoch: [54][210/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.2121e-02 (3.9549e-02)	Acc@1  97.66 ( 98.60)	Acc@5 100.00 ( 99.99)
Epoch: [54][220/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.5934e-02 (3.9782e-02)	Acc@1  98.44 ( 98.60)	Acc@5 100.00 ( 99.99)
Epoch: [54][230/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.8549e-02 (4.0133e-02)	Acc@1  97.66 ( 98.59)	Acc@5 100.00 ( 99.99)
Epoch: [54][240/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.6331e-02 (3.9908e-02)	Acc@1  99.22 ( 98.60)	Acc@5 100.00 ( 99.99)
Epoch: [54][250/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.8976e-02 (3.9626e-02)	Acc@1  98.44 ( 98.61)	Acc@5 100.00 ( 99.99)
Epoch: [54][260/391]	Time  0.022 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.6388e-02 (3.9763e-02)	Acc@1 100.00 ( 98.62)	Acc@5 100.00 ( 99.99)
Epoch: [54][270/391]	Time  0.032 ( 0.026)	Data  0.004 ( 0.003)	Loss 2.4422e-02 (4.0037e-02)	Acc@1  99.22 ( 98.62)	Acc@5 100.00 ( 99.99)
Epoch: [54][280/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.5153e-02 (4.0149e-02)	Acc@1  98.44 ( 98.60)	Acc@5 100.00 ( 99.99)
Epoch: [54][290/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.5215e-02 (4.0121e-02)	Acc@1  97.66 ( 98.61)	Acc@5 100.00 ( 99.99)
Epoch: [54][300/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4250e-02 (4.0281e-02)	Acc@1  98.44 ( 98.59)	Acc@5 100.00 ( 99.99)
Epoch: [54][310/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1107e-01 (4.0876e-02)	Acc@1  98.44 ( 98.58)	Acc@5  99.22 ( 99.99)
Epoch: [54][320/391]	Time  0.023 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.8547e-02 (4.0463e-02)	Acc@1  99.22 ( 98.60)	Acc@5 100.00 ( 99.99)
Epoch: [54][330/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.7591e-02 (4.0659e-02)	Acc@1  97.66 ( 98.59)	Acc@5 100.00 ( 99.99)
Epoch: [54][340/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.9212e-02 (4.0807e-02)	Acc@1  96.88 ( 98.59)	Acc@5 100.00 ( 99.99)
Epoch: [54][350/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0791e-01 (4.0817e-02)	Acc@1  96.88 ( 98.58)	Acc@5 100.00 ( 99.99)
Epoch: [54][360/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.7074e-02 (4.1015e-02)	Acc@1  96.88 ( 98.57)	Acc@5 100.00 ( 99.99)
Epoch: [54][370/391]	Time  0.025 ( 0.026)	Data  0.005 ( 0.003)	Loss 4.8103e-02 (4.1277e-02)	Acc@1  99.22 ( 98.56)	Acc@5 100.00 ( 99.99)
Epoch: [54][380/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.5825e-02 (4.1247e-02)	Acc@1  99.22 ( 98.55)	Acc@5 100.00 ( 99.99)
Epoch: [54][390/391]	Time  0.026 ( 0.026)	Data  0.000 ( 0.003)	Loss 4.4999e-02 (4.1082e-02)	Acc@1  98.75 ( 98.56)	Acc@5 100.00 ( 99.99)
## e[54] optimizer.zero_grad (sum) time: 0.10880517959594727
## e[54]       loss.backward (sum) time: 2.2361371517181396
## e[54]      optimizer.step (sum) time: 0.8690271377563477
## epoch[54] training(only) time: 10.39005994796753
# Switched to evaluate mode...
Test: [  0/100]	Time  0.166 ( 0.166)	Loss 2.3974e-01 (2.3974e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.017 ( 0.029)	Loss 4.1118e-01 (3.6069e-01)	Acc@1  91.00 ( 91.36)	Acc@5  99.00 ( 99.55)
Test: [ 20/100]	Time  0.012 ( 0.021)	Loss 3.7634e-01 (3.8384e-01)	Acc@1  90.00 ( 90.71)	Acc@5 100.00 ( 99.52)
Test: [ 30/100]	Time  0.014 ( 0.019)	Loss 3.0895e-01 (3.9675e-01)	Acc@1  91.00 ( 90.58)	Acc@5  99.00 ( 99.48)
Test: [ 40/100]	Time  0.021 ( 0.018)	Loss 5.0457e-01 (4.1295e-01)	Acc@1  90.00 ( 90.34)	Acc@5 100.00 ( 99.51)
Test: [ 50/100]	Time  0.012 ( 0.017)	Loss 2.2537e-01 (4.0885e-01)	Acc@1  94.00 ( 90.47)	Acc@5 100.00 ( 99.51)
Test: [ 60/100]	Time  0.010 ( 0.017)	Loss 4.8521e-01 (4.0454e-01)	Acc@1  86.00 ( 90.49)	Acc@5 100.00 ( 99.52)
Test: [ 70/100]	Time  0.016 ( 0.016)	Loss 7.7036e-01 (4.0308e-01)	Acc@1  89.00 ( 90.55)	Acc@5  99.00 ( 99.52)
Test: [ 80/100]	Time  0.011 ( 0.016)	Loss 3.6733e-01 (3.9827e-01)	Acc@1  93.00 ( 90.65)	Acc@5  99.00 ( 99.54)
Test: [ 90/100]	Time  0.011 ( 0.016)	Loss 2.4981e-01 (3.9449e-01)	Acc@1  95.00 ( 90.64)	Acc@5  99.00 ( 99.57)
 * Acc@1 90.740 Acc@5 99.600
### epoch[54] execution time: 12.094327211380005
EPOCH 55
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [55][  0/391]	Time  0.192 ( 0.192)	Data  0.172 ( 0.172)	Loss 1.9120e-02 (1.9120e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [55][ 10/391]	Time  0.024 ( 0.041)	Data  0.002 ( 0.017)	Loss 1.1328e-02 (2.9328e-02)	Acc@1 100.00 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [55][ 20/391]	Time  0.024 ( 0.034)	Data  0.001 ( 0.010)	Loss 4.6223e-02 (2.8741e-02)	Acc@1  98.44 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [55][ 30/391]	Time  0.025 ( 0.031)	Data  0.002 ( 0.008)	Loss 3.0492e-02 (3.2051e-02)	Acc@1  99.22 ( 98.82)	Acc@5 100.00 (100.00)
Epoch: [55][ 40/391]	Time  0.021 ( 0.030)	Data  0.002 ( 0.006)	Loss 6.9911e-02 (3.4287e-02)	Acc@1  97.66 ( 98.76)	Acc@5 100.00 (100.00)
Epoch: [55][ 50/391]	Time  0.023 ( 0.029)	Data  0.000 ( 0.006)	Loss 6.8671e-02 (3.6988e-02)	Acc@1  98.44 ( 98.77)	Acc@5 100.00 ( 99.98)
Epoch: [55][ 60/391]	Time  0.031 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.4464e-01 (3.9061e-02)	Acc@1  94.53 ( 98.60)	Acc@5 100.00 ( 99.99)
Epoch: [55][ 70/391]	Time  0.025 ( 0.028)	Data  0.005 ( 0.005)	Loss 7.2375e-02 (4.1593e-02)	Acc@1  98.44 ( 98.55)	Acc@5 100.00 ( 99.99)
Epoch: [55][ 80/391]	Time  0.032 ( 0.028)	Data  0.002 ( 0.004)	Loss 6.7238e-02 (4.3120e-02)	Acc@1  97.66 ( 98.51)	Acc@5 100.00 ( 99.99)
Epoch: [55][ 90/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 7.9345e-03 (4.3717e-02)	Acc@1 100.00 ( 98.49)	Acc@5 100.00 ( 99.98)
Epoch: [55][100/391]	Time  0.023 ( 0.027)	Data  0.003 ( 0.004)	Loss 4.1722e-02 (4.2583e-02)	Acc@1  98.44 ( 98.52)	Acc@5 100.00 ( 99.98)
Epoch: [55][110/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 5.2654e-02 (4.2272e-02)	Acc@1  97.66 ( 98.54)	Acc@5 100.00 ( 99.99)
Epoch: [55][120/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 4.9480e-02 (4.2051e-02)	Acc@1  97.66 ( 98.55)	Acc@5 100.00 ( 99.99)
Epoch: [55][130/391]	Time  0.026 ( 0.027)	Data  0.005 ( 0.004)	Loss 7.4008e-03 (4.1520e-02)	Acc@1 100.00 ( 98.59)	Acc@5 100.00 ( 99.99)
Epoch: [55][140/391]	Time  0.031 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.8040e-02 (4.0621e-02)	Acc@1  97.66 ( 98.59)	Acc@5 100.00 ( 99.99)
Epoch: [55][150/391]	Time  0.023 ( 0.027)	Data  0.006 ( 0.003)	Loss 6.2262e-02 (4.0603e-02)	Acc@1  98.44 ( 98.61)	Acc@5 100.00 ( 99.99)
Epoch: [55][160/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.3049e-02 (4.1346e-02)	Acc@1  98.44 ( 98.58)	Acc@5 100.00 ( 99.99)
Epoch: [55][170/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.6559e-02 (4.0863e-02)	Acc@1  99.22 ( 98.61)	Acc@5 100.00 ( 99.99)
Epoch: [55][180/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.2409e-02 (4.1350e-02)	Acc@1  99.22 ( 98.60)	Acc@5 100.00 ( 99.99)
Epoch: [55][190/391]	Time  0.033 ( 0.027)	Data  0.002 ( 0.003)	Loss 3.0092e-02 (4.0800e-02)	Acc@1  99.22 ( 98.63)	Acc@5 100.00 ( 99.99)
Epoch: [55][200/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.4652e-02 (4.0908e-02)	Acc@1  96.88 ( 98.62)	Acc@5 100.00 ( 99.99)
Epoch: [55][210/391]	Time  0.031 ( 0.027)	Data  0.002 ( 0.003)	Loss 6.7691e-02 (4.1191e-02)	Acc@1  98.44 ( 98.61)	Acc@5 100.00 ( 99.99)
Epoch: [55][220/391]	Time  0.029 ( 0.027)	Data  0.005 ( 0.003)	Loss 4.0061e-02 (4.1409e-02)	Acc@1  98.44 ( 98.62)	Acc@5 100.00 ( 99.99)
Epoch: [55][230/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.5579e-02 (4.1642e-02)	Acc@1  97.66 ( 98.60)	Acc@5 100.00 ( 99.98)
Epoch: [55][240/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.5652e-02 (4.1616e-02)	Acc@1  98.44 ( 98.61)	Acc@5 100.00 ( 99.98)
Epoch: [55][250/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6491e-02 (4.1373e-02)	Acc@1 100.00 ( 98.61)	Acc@5 100.00 ( 99.98)
Epoch: [55][260/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.7237e-02 (4.2013e-02)	Acc@1  98.44 ( 98.59)	Acc@5 100.00 ( 99.99)
Epoch: [55][270/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.7223e-02 (4.1544e-02)	Acc@1  99.22 ( 98.61)	Acc@5 100.00 ( 99.99)
Epoch: [55][280/391]	Time  0.035 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0599e-02 (4.1454e-02)	Acc@1 100.00 ( 98.62)	Acc@5 100.00 ( 99.99)
Epoch: [55][290/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.4073e-02 (4.1435e-02)	Acc@1  99.22 ( 98.62)	Acc@5 100.00 ( 99.99)
Epoch: [55][300/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.7032e-02 (4.1467e-02)	Acc@1  97.66 ( 98.63)	Acc@5 100.00 ( 99.99)
Epoch: [55][310/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.5730e-02 (4.1486e-02)	Acc@1  97.66 ( 98.63)	Acc@5 100.00 ( 99.99)
Epoch: [55][320/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.2457e-02 (4.1430e-02)	Acc@1  97.66 ( 98.63)	Acc@5 100.00 ( 99.99)
Epoch: [55][330/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2628e-02 (4.1470e-02)	Acc@1 100.00 ( 98.63)	Acc@5 100.00 ( 99.99)
Epoch: [55][340/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.2105e-02 (4.1753e-02)	Acc@1  99.22 ( 98.62)	Acc@5 100.00 ( 99.99)
Epoch: [55][350/391]	Time  0.022 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.3816e-02 (4.1772e-02)	Acc@1  99.22 ( 98.62)	Acc@5 100.00 ( 99.99)
Epoch: [55][360/391]	Time  0.031 ( 0.026)	Data  0.002 ( 0.003)	Loss 6.6892e-02 (4.1634e-02)	Acc@1  98.44 ( 98.63)	Acc@5 100.00 ( 99.99)
Epoch: [55][370/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3374e-02 (4.1358e-02)	Acc@1  98.44 ( 98.63)	Acc@5 100.00 ( 99.99)
Epoch: [55][380/391]	Time  0.026 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.5522e-02 (4.1478e-02)	Acc@1 100.00 ( 98.63)	Acc@5 100.00 ( 99.99)
Epoch: [55][390/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.4639e-02 (4.1299e-02)	Acc@1  98.75 ( 98.63)	Acc@5 100.00 ( 99.99)
## e[55] optimizer.zero_grad (sum) time: 0.10810708999633789
## e[55]       loss.backward (sum) time: 2.2718312740325928
## e[55]      optimizer.step (sum) time: 0.8801429271697998
## epoch[55] training(only) time: 10.419525623321533
# Switched to evaluate mode...
Test: [  0/100]	Time  0.157 ( 0.157)	Loss 2.8133e-01 (2.8133e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.015 ( 0.028)	Loss 4.4094e-01 (3.5195e-01)	Acc@1  88.00 ( 91.27)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.010 ( 0.021)	Loss 3.7144e-01 (3.6327e-01)	Acc@1  91.00 ( 91.05)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.010 ( 0.019)	Loss 3.9754e-01 (3.8678e-01)	Acc@1  90.00 ( 90.81)	Acc@5  99.00 ( 99.58)
Test: [ 40/100]	Time  0.011 ( 0.018)	Loss 5.1124e-01 (4.0530e-01)	Acc@1  88.00 ( 90.54)	Acc@5  99.00 ( 99.61)
Test: [ 50/100]	Time  0.016 ( 0.018)	Loss 1.8236e-01 (3.9926e-01)	Acc@1  93.00 ( 90.63)	Acc@5  99.00 ( 99.55)
Test: [ 60/100]	Time  0.022 ( 0.017)	Loss 4.4237e-01 (3.9751e-01)	Acc@1  89.00 ( 90.82)	Acc@5 100.00 ( 99.56)
Test: [ 70/100]	Time  0.023 ( 0.017)	Loss 7.6812e-01 (3.9985e-01)	Acc@1  90.00 ( 90.83)	Acc@5 100.00 ( 99.56)
Test: [ 80/100]	Time  0.011 ( 0.017)	Loss 3.3198e-01 (3.9693e-01)	Acc@1  92.00 ( 90.85)	Acc@5  99.00 ( 99.59)
Test: [ 90/100]	Time  0.011 ( 0.016)	Loss 2.4982e-01 (3.9106e-01)	Acc@1  93.00 ( 90.81)	Acc@5 100.00 ( 99.63)
 * Acc@1 90.890 Acc@5 99.640
### epoch[55] execution time: 12.151100635528564
EPOCH 56
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [56][  0/391]	Time  0.194 ( 0.194)	Data  0.173 ( 0.173)	Loss 1.9752e-02 (1.9752e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [56][ 10/391]	Time  0.023 ( 0.041)	Data  0.001 ( 0.018)	Loss 3.1873e-02 (2.8007e-02)	Acc@1  98.44 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [56][ 20/391]	Time  0.027 ( 0.033)	Data  0.001 ( 0.010)	Loss 9.2823e-02 (3.6511e-02)	Acc@1  96.09 ( 98.66)	Acc@5 100.00 (100.00)
Epoch: [56][ 30/391]	Time  0.025 ( 0.031)	Data  0.001 ( 0.008)	Loss 3.6717e-02 (3.7230e-02)	Acc@1  98.44 ( 98.71)	Acc@5 100.00 (100.00)
Epoch: [56][ 40/391]	Time  0.024 ( 0.029)	Data  0.001 ( 0.006)	Loss 3.5770e-02 (3.8530e-02)	Acc@1  98.44 ( 98.69)	Acc@5 100.00 (100.00)
Epoch: [56][ 50/391]	Time  0.024 ( 0.029)	Data  0.001 ( 0.006)	Loss 6.9913e-02 (4.0573e-02)	Acc@1  97.66 ( 98.67)	Acc@5 100.00 ( 99.98)
Epoch: [56][ 60/391]	Time  0.031 ( 0.028)	Data  0.001 ( 0.005)	Loss 3.4736e-02 (4.0513e-02)	Acc@1  97.66 ( 98.68)	Acc@5 100.00 ( 99.99)
Epoch: [56][ 70/391]	Time  0.027 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.1079e-02 (4.0445e-02)	Acc@1  99.22 ( 98.62)	Acc@5 100.00 ( 99.99)
Epoch: [56][ 80/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.005)	Loss 4.1811e-02 (3.9956e-02)	Acc@1  97.66 ( 98.65)	Acc@5 100.00 ( 99.99)
Epoch: [56][ 90/391]	Time  0.030 ( 0.028)	Data  0.000 ( 0.004)	Loss 3.4957e-02 (4.1043e-02)	Acc@1  98.44 ( 98.59)	Acc@5 100.00 ( 99.99)
Epoch: [56][100/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.4528e-02 (4.1943e-02)	Acc@1  99.22 ( 98.54)	Acc@5 100.00 ( 99.98)
Epoch: [56][110/391]	Time  0.037 ( 0.027)	Data  0.002 ( 0.004)	Loss 1.6449e-02 (4.1300e-02)	Acc@1  99.22 ( 98.57)	Acc@5 100.00 ( 99.99)
Epoch: [56][120/391]	Time  0.028 ( 0.027)	Data  0.000 ( 0.004)	Loss 2.8454e-02 (4.0722e-02)	Acc@1  99.22 ( 98.57)	Acc@5 100.00 ( 99.99)
Epoch: [56][130/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.004)	Loss 4.4584e-02 (4.0181e-02)	Acc@1  98.44 ( 98.60)	Acc@5 100.00 ( 99.99)
Epoch: [56][140/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.3608e-02 (4.0695e-02)	Acc@1  98.44 ( 98.58)	Acc@5 100.00 ( 99.99)
Epoch: [56][150/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.8293e-02 (4.0346e-02)	Acc@1 100.00 ( 98.61)	Acc@5 100.00 ( 99.99)
Epoch: [56][160/391]	Time  0.030 ( 0.027)	Data  0.000 ( 0.003)	Loss 1.8602e-02 (4.0142e-02)	Acc@1  99.22 ( 98.62)	Acc@5 100.00 ( 99.99)
Epoch: [56][170/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.2453e-03 (3.9801e-02)	Acc@1 100.00 ( 98.63)	Acc@5 100.00 ( 99.99)
Epoch: [56][180/391]	Time  0.026 ( 0.027)	Data  0.004 ( 0.003)	Loss 4.2251e-02 (4.0685e-02)	Acc@1  97.66 ( 98.61)	Acc@5 100.00 ( 99.99)
Epoch: [56][190/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.1455e-03 (4.0143e-02)	Acc@1 100.00 ( 98.63)	Acc@5 100.00 ( 99.99)
Epoch: [56][200/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.1139e-02 (4.0070e-02)	Acc@1  99.22 ( 98.62)	Acc@5 100.00 ( 99.99)
Epoch: [56][210/391]	Time  0.037 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.5565e-02 (4.0291e-02)	Acc@1  99.22 ( 98.62)	Acc@5 100.00 ( 99.99)
Epoch: [56][220/391]	Time  0.034 ( 0.027)	Data  0.002 ( 0.003)	Loss 8.5515e-02 (4.0005e-02)	Acc@1  97.66 ( 98.62)	Acc@5 100.00 ( 99.99)
Epoch: [56][230/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.7712e-02 (3.9914e-02)	Acc@1  98.44 ( 98.62)	Acc@5 100.00 ( 99.99)
Epoch: [56][240/391]	Time  0.028 ( 0.027)	Data  0.002 ( 0.003)	Loss 9.9577e-02 (4.0512e-02)	Acc@1  96.09 ( 98.59)	Acc@5 100.00 ( 99.99)
Epoch: [56][250/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.7431e-02 (4.0568e-02)	Acc@1 100.00 ( 98.59)	Acc@5 100.00 ( 99.99)
Epoch: [56][260/391]	Time  0.030 ( 0.027)	Data  0.000 ( 0.003)	Loss 3.5769e-02 (4.0761e-02)	Acc@1  99.22 ( 98.58)	Acc@5 100.00 ( 99.99)
Epoch: [56][270/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.0544e-02 (4.1263e-02)	Acc@1  98.44 ( 98.55)	Acc@5 100.00 ( 99.99)
Epoch: [56][280/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 2.0684e-02 (4.1238e-02)	Acc@1  99.22 ( 98.56)	Acc@5 100.00 ( 99.99)
Epoch: [56][290/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.3627e-02 (4.0757e-02)	Acc@1  98.44 ( 98.57)	Acc@5 100.00 ( 99.99)
Epoch: [56][300/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.4131e-02 (4.0777e-02)	Acc@1  99.22 ( 98.57)	Acc@5 100.00 ( 99.99)
Epoch: [56][310/391]	Time  0.027 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.1535e-02 (4.0977e-02)	Acc@1  98.44 ( 98.57)	Acc@5 100.00 ( 99.99)
Epoch: [56][320/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.3873e-02 (4.0928e-02)	Acc@1  99.22 ( 98.57)	Acc@5 100.00 ( 99.99)
Epoch: [56][330/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.2241e-02 (4.0993e-02)	Acc@1  97.66 ( 98.57)	Acc@5 100.00 ( 99.99)
Epoch: [56][340/391]	Time  0.033 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.7100e-02 (4.0649e-02)	Acc@1  99.22 ( 98.58)	Acc@5 100.00 ( 99.99)
Epoch: [56][350/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.1051e-02 (4.0603e-02)	Acc@1 100.00 ( 98.59)	Acc@5 100.00 ( 99.99)
Epoch: [56][360/391]	Time  0.023 ( 0.027)	Data  0.003 ( 0.003)	Loss 1.4175e-02 (4.0440e-02)	Acc@1 100.00 ( 98.60)	Acc@5 100.00 ( 99.99)
Epoch: [56][370/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.003)	Loss 9.4801e-02 (4.0681e-02)	Acc@1  98.44 ( 98.59)	Acc@5 100.00 ( 99.99)
Epoch: [56][380/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2101e-02 (4.0674e-02)	Acc@1 100.00 ( 98.60)	Acc@5 100.00 ( 99.99)
Epoch: [56][390/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.5577e-02 (4.0358e-02)	Acc@1  97.50 ( 98.61)	Acc@5 100.00 ( 99.99)
## e[56] optimizer.zero_grad (sum) time: 0.10978555679321289
## e[56]       loss.backward (sum) time: 2.149937629699707
## e[56]      optimizer.step (sum) time: 0.8708477020263672
## epoch[56] training(only) time: 10.478633403778076
# Switched to evaluate mode...
Test: [  0/100]	Time  0.164 ( 0.164)	Loss 2.5361e-01 (2.5361e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.011 ( 0.028)	Loss 4.0551e-01 (3.5962e-01)	Acc@1  92.00 ( 90.64)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.011 ( 0.022)	Loss 3.5400e-01 (3.8448e-01)	Acc@1  92.00 ( 90.10)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.011 ( 0.019)	Loss 3.7960e-01 (4.1101e-01)	Acc@1  91.00 ( 89.81)	Acc@5  99.00 ( 99.58)
Test: [ 40/100]	Time  0.016 ( 0.018)	Loss 6.2244e-01 (4.2797e-01)	Acc@1  87.00 ( 89.80)	Acc@5  99.00 ( 99.56)
Test: [ 50/100]	Time  0.012 ( 0.017)	Loss 2.3599e-01 (4.2499e-01)	Acc@1  95.00 ( 89.96)	Acc@5  99.00 ( 99.53)
Test: [ 60/100]	Time  0.012 ( 0.016)	Loss 3.6466e-01 (4.2312e-01)	Acc@1  92.00 ( 90.18)	Acc@5 100.00 ( 99.54)
Test: [ 70/100]	Time  0.014 ( 0.016)	Loss 9.0266e-01 (4.2610e-01)	Acc@1  88.00 ( 90.25)	Acc@5  99.00 ( 99.54)
Test: [ 80/100]	Time  0.011 ( 0.016)	Loss 3.7391e-01 (4.2090e-01)	Acc@1  92.00 ( 90.38)	Acc@5  99.00 ( 99.57)
Test: [ 90/100]	Time  0.013 ( 0.016)	Loss 3.3262e-01 (4.1715e-01)	Acc@1  92.00 ( 90.35)	Acc@5 100.00 ( 99.60)
 * Acc@1 90.400 Acc@5 99.630
### epoch[56] execution time: 12.157516479492188
EPOCH 57
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [57][  0/391]	Time  0.196 ( 0.196)	Data  0.173 ( 0.173)	Loss 2.3847e-02 (2.3847e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [57][ 10/391]	Time  0.032 ( 0.041)	Data  0.001 ( 0.018)	Loss 6.3314e-02 (3.8009e-02)	Acc@1  99.22 ( 98.65)	Acc@5 100.00 (100.00)
Epoch: [57][ 20/391]	Time  0.023 ( 0.034)	Data  0.001 ( 0.010)	Loss 1.5585e-02 (3.0910e-02)	Acc@1 100.00 ( 98.88)	Acc@5 100.00 (100.00)
Epoch: [57][ 30/391]	Time  0.031 ( 0.031)	Data  0.001 ( 0.008)	Loss 3.4882e-02 (3.8803e-02)	Acc@1  99.22 ( 98.79)	Acc@5 100.00 (100.00)
Epoch: [57][ 40/391]	Time  0.023 ( 0.030)	Data  0.001 ( 0.006)	Loss 4.4973e-02 (3.5672e-02)	Acc@1  97.66 ( 98.82)	Acc@5 100.00 (100.00)
Epoch: [57][ 50/391]	Time  0.027 ( 0.029)	Data  0.001 ( 0.005)	Loss 4.5265e-02 (3.5121e-02)	Acc@1  98.44 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [57][ 60/391]	Time  0.028 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.7910e-02 (3.3343e-02)	Acc@1  99.22 ( 98.86)	Acc@5 100.00 (100.00)
Epoch: [57][ 70/391]	Time  0.023 ( 0.028)	Data  0.002 ( 0.004)	Loss 4.1819e-02 (3.2987e-02)	Acc@1  98.44 ( 98.88)	Acc@5 100.00 (100.00)
Epoch: [57][ 80/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 2.1459e-02 (3.2572e-02)	Acc@1  99.22 ( 98.90)	Acc@5 100.00 (100.00)
Epoch: [57][ 90/391]	Time  0.026 ( 0.027)	Data  0.005 ( 0.004)	Loss 1.0963e-02 (3.2735e-02)	Acc@1  99.22 ( 98.88)	Acc@5 100.00 (100.00)
Epoch: [57][100/391]	Time  0.026 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.1273e-02 (3.3443e-02)	Acc@1 100.00 ( 98.88)	Acc@5 100.00 (100.00)
Epoch: [57][110/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 4.2406e-02 (3.3869e-02)	Acc@1  99.22 ( 98.87)	Acc@5 100.00 (100.00)
Epoch: [57][120/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.2222e-02 (3.3728e-02)	Acc@1  98.44 ( 98.86)	Acc@5 100.00 (100.00)
Epoch: [57][130/391]	Time  0.028 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.6659e-02 (3.3942e-02)	Acc@1  99.22 ( 98.87)	Acc@5 100.00 (100.00)
Epoch: [57][140/391]	Time  0.028 ( 0.027)	Data  0.003 ( 0.003)	Loss 3.4262e-02 (3.4651e-02)	Acc@1  99.22 ( 98.86)	Acc@5 100.00 (100.00)
Epoch: [57][150/391]	Time  0.030 ( 0.027)	Data  0.000 ( 0.003)	Loss 4.3415e-02 (3.4635e-02)	Acc@1  98.44 ( 98.85)	Acc@5 100.00 (100.00)
Epoch: [57][160/391]	Time  0.033 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.5718e-03 (3.4864e-02)	Acc@1 100.00 ( 98.85)	Acc@5 100.00 (100.00)
Epoch: [57][170/391]	Time  0.034 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.9850e-02 (3.4442e-02)	Acc@1 100.00 ( 98.86)	Acc@5 100.00 (100.00)
Epoch: [57][180/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.3253e-02 (3.4956e-02)	Acc@1  98.44 ( 98.84)	Acc@5 100.00 (100.00)
Epoch: [57][190/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.3017e-02 (3.5553e-02)	Acc@1  97.66 ( 98.82)	Acc@5 100.00 (100.00)
Epoch: [57][200/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 4.3904e-03 (3.4962e-02)	Acc@1 100.00 ( 98.83)	Acc@5 100.00 (100.00)
Epoch: [57][210/391]	Time  0.027 ( 0.027)	Data  0.001 ( 0.003)	Loss 8.1209e-02 (3.5235e-02)	Acc@1  96.88 ( 98.82)	Acc@5 100.00 (100.00)
Epoch: [57][220/391]	Time  0.035 ( 0.027)	Data  0.001 ( 0.003)	Loss 9.8983e-02 (3.5505e-02)	Acc@1  96.09 ( 98.80)	Acc@5 100.00 (100.00)
Epoch: [57][230/391]	Time  0.024 ( 0.027)	Data  0.004 ( 0.003)	Loss 7.3029e-02 (3.5924e-02)	Acc@1  96.88 ( 98.78)	Acc@5 100.00 (100.00)
Epoch: [57][240/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 8.0494e-02 (3.6578e-02)	Acc@1  98.44 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [57][250/391]	Time  0.023 ( 0.026)	Data  0.000 ( 0.003)	Loss 6.7847e-02 (3.6989e-02)	Acc@1  97.66 ( 98.75)	Acc@5 100.00 (100.00)
Epoch: [57][260/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.9611e-02 (3.7193e-02)	Acc@1  98.44 ( 98.74)	Acc@5  99.22 (100.00)
Epoch: [57][270/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.2176e-02 (3.7846e-02)	Acc@1  97.66 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [57][280/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.6746e-02 (3.7697e-02)	Acc@1  97.66 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [57][290/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2668e-02 (3.7844e-02)	Acc@1  99.22 ( 98.71)	Acc@5 100.00 (100.00)
Epoch: [57][300/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0330e-02 (3.7605e-02)	Acc@1 100.00 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [57][310/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.9658e-02 (3.8281e-02)	Acc@1  98.44 ( 98.69)	Acc@5 100.00 (100.00)
Epoch: [57][320/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.9212e-02 (3.8506e-02)	Acc@1  98.44 ( 98.68)	Acc@5 100.00 (100.00)
Epoch: [57][330/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.9869e-02 (3.8837e-02)	Acc@1  99.22 ( 98.68)	Acc@5 100.00 (100.00)
Epoch: [57][340/391]	Time  0.025 ( 0.026)	Data  0.005 ( 0.003)	Loss 4.1573e-02 (3.9029e-02)	Acc@1  97.66 ( 98.67)	Acc@5 100.00 (100.00)
Epoch: [57][350/391]	Time  0.022 ( 0.026)	Data  0.002 ( 0.003)	Loss 6.7654e-03 (3.8708e-02)	Acc@1 100.00 ( 98.69)	Acc@5 100.00 (100.00)
Epoch: [57][360/391]	Time  0.032 ( 0.026)	Data  0.013 ( 0.003)	Loss 3.8769e-02 (3.8425e-02)	Acc@1  98.44 ( 98.70)	Acc@5 100.00 (100.00)
Epoch: [57][370/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.5820e-02 (3.8555e-02)	Acc@1  97.66 ( 98.68)	Acc@5 100.00 (100.00)
Epoch: [57][380/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2238e-02 (3.8710e-02)	Acc@1 100.00 ( 98.67)	Acc@5 100.00 (100.00)
Epoch: [57][390/391]	Time  0.024 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.8737e-02 (3.8617e-02)	Acc@1  98.75 ( 98.67)	Acc@5 100.00 (100.00)
## e[57] optimizer.zero_grad (sum) time: 0.11084723472595215
## e[57]       loss.backward (sum) time: 2.2546751499176025
## e[57]      optimizer.step (sum) time: 0.8930649757385254
## epoch[57] training(only) time: 10.409918785095215
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 2.0379e-01 (2.0379e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.021 ( 0.029)	Loss 4.1253e-01 (3.5905e-01)	Acc@1  90.00 ( 91.45)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.011 ( 0.022)	Loss 4.3034e-01 (3.7241e-01)	Acc@1  89.00 ( 90.43)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.012 ( 0.020)	Loss 4.1144e-01 (4.0154e-01)	Acc@1  90.00 ( 90.13)	Acc@5  98.00 ( 99.52)
Test: [ 40/100]	Time  0.015 ( 0.019)	Loss 5.5071e-01 (4.1500e-01)	Acc@1  88.00 ( 89.83)	Acc@5  99.00 ( 99.51)
Test: [ 50/100]	Time  0.012 ( 0.018)	Loss 1.7052e-01 (4.1052e-01)	Acc@1  95.00 ( 89.96)	Acc@5 100.00 ( 99.53)
Test: [ 60/100]	Time  0.013 ( 0.018)	Loss 4.7487e-01 (4.0799e-01)	Acc@1  91.00 ( 90.16)	Acc@5 100.00 ( 99.56)
Test: [ 70/100]	Time  0.014 ( 0.017)	Loss 7.5722e-01 (4.0583e-01)	Acc@1  89.00 ( 90.34)	Acc@5  99.00 ( 99.54)
Test: [ 80/100]	Time  0.012 ( 0.017)	Loss 3.9390e-01 (4.0425e-01)	Acc@1  90.00 ( 90.25)	Acc@5  99.00 ( 99.56)
Test: [ 90/100]	Time  0.023 ( 0.017)	Loss 2.9976e-01 (4.0005e-01)	Acc@1  93.00 ( 90.23)	Acc@5 100.00 ( 99.59)
 * Acc@1 90.280 Acc@5 99.600
### epoch[57] execution time: 12.154683589935303
EPOCH 58
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [58][  0/391]	Time  0.192 ( 0.192)	Data  0.164 ( 0.164)	Loss 6.1703e-02 (6.1703e-02)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [58][ 10/391]	Time  0.028 ( 0.041)	Data  0.002 ( 0.017)	Loss 2.0092e-02 (4.3546e-02)	Acc@1  99.22 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [58][ 20/391]	Time  0.025 ( 0.034)	Data  0.001 ( 0.010)	Loss 9.0557e-02 (3.4730e-02)	Acc@1  96.88 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [58][ 30/391]	Time  0.023 ( 0.031)	Data  0.000 ( 0.007)	Loss 4.5217e-02 (3.6917e-02)	Acc@1  98.44 ( 98.66)	Acc@5 100.00 (100.00)
Epoch: [58][ 40/391]	Time  0.021 ( 0.030)	Data  0.004 ( 0.006)	Loss 7.7011e-03 (3.7958e-02)	Acc@1 100.00 ( 98.70)	Acc@5 100.00 ( 99.98)
Epoch: [58][ 50/391]	Time  0.023 ( 0.029)	Data  0.001 ( 0.005)	Loss 3.1294e-02 (3.6996e-02)	Acc@1  97.66 ( 98.68)	Acc@5 100.00 ( 99.98)
Epoch: [58][ 60/391]	Time  0.026 ( 0.028)	Data  0.001 ( 0.005)	Loss 4.7124e-02 (3.7434e-02)	Acc@1  99.22 ( 98.73)	Acc@5 100.00 ( 99.99)
Epoch: [58][ 70/391]	Time  0.025 ( 0.028)	Data  0.001 ( 0.004)	Loss 3.7931e-02 (3.8072e-02)	Acc@1  98.44 ( 98.68)	Acc@5 100.00 ( 99.99)
Epoch: [58][ 80/391]	Time  0.026 ( 0.028)	Data  0.001 ( 0.004)	Loss 3.0019e-02 (3.7052e-02)	Acc@1  99.22 ( 98.72)	Acc@5 100.00 ( 99.99)
Epoch: [58][ 90/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.1194e-02 (3.5273e-02)	Acc@1 100.00 ( 98.80)	Acc@5 100.00 ( 99.99)
Epoch: [58][100/391]	Time  0.027 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.2061e-02 (3.4630e-02)	Acc@1 100.00 ( 98.84)	Acc@5 100.00 ( 99.99)
Epoch: [58][110/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 7.2230e-02 (3.5944e-02)	Acc@1  97.66 ( 98.79)	Acc@5 100.00 ( 99.99)
Epoch: [58][120/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.0844e-01 (3.5854e-02)	Acc@1  97.66 ( 98.81)	Acc@5 100.00 ( 99.99)
Epoch: [58][130/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 6.0520e-02 (3.5513e-02)	Acc@1  97.66 ( 98.82)	Acc@5 100.00 ( 99.99)
Epoch: [58][140/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.0761e-03 (3.5456e-02)	Acc@1 100.00 ( 98.83)	Acc@5 100.00 ( 99.99)
Epoch: [58][150/391]	Time  0.028 ( 0.027)	Data  0.005 ( 0.003)	Loss 7.7044e-03 (3.5231e-02)	Acc@1 100.00 ( 98.83)	Acc@5 100.00 ( 99.99)
Epoch: [58][160/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.2050e-02 (3.6256e-02)	Acc@1  99.22 ( 98.79)	Acc@5 100.00 (100.00)
Epoch: [58][170/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.1024e-02 (3.5552e-02)	Acc@1  98.44 ( 98.82)	Acc@5 100.00 (100.00)
Epoch: [58][180/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 3.1502e-02 (3.5242e-02)	Acc@1  99.22 ( 98.83)	Acc@5 100.00 (100.00)
Epoch: [58][190/391]	Time  0.026 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.9534e-02 (3.5122e-02)	Acc@1  99.22 ( 98.83)	Acc@5 100.00 (100.00)
Epoch: [58][200/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6411e-02 (3.5266e-02)	Acc@1  99.22 ( 98.81)	Acc@5 100.00 (100.00)
Epoch: [58][210/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3344e-02 (3.5619e-02)	Acc@1  99.22 ( 98.83)	Acc@5 100.00 (100.00)
Epoch: [58][220/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.7690e-02 (3.6547e-02)	Acc@1  97.66 ( 98.79)	Acc@5 100.00 (100.00)
Epoch: [58][230/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.8344e-02 (3.6173e-02)	Acc@1  99.22 ( 98.81)	Acc@5 100.00 (100.00)
Epoch: [58][240/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.7499e-02 (3.6748e-02)	Acc@1  98.44 ( 98.79)	Acc@5 100.00 (100.00)
Epoch: [58][250/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.3770e-03 (3.6466e-02)	Acc@1 100.00 ( 98.80)	Acc@5 100.00 (100.00)
Epoch: [58][260/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6969e-02 (3.6187e-02)	Acc@1  99.22 ( 98.80)	Acc@5 100.00 (100.00)
Epoch: [58][270/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.4615e-02 (3.5917e-02)	Acc@1  97.66 ( 98.81)	Acc@5 100.00 (100.00)
Epoch: [58][280/391]	Time  0.024 ( 0.026)	Data  0.002 ( 0.003)	Loss 6.0751e-02 (3.6338e-02)	Acc@1  96.88 ( 98.79)	Acc@5 100.00 (100.00)
Epoch: [58][290/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0037e-02 (3.6534e-02)	Acc@1  99.22 ( 98.78)	Acc@5 100.00 (100.00)
Epoch: [58][300/391]	Time  0.024 ( 0.026)	Data  0.002 ( 0.003)	Loss 5.5162e-02 (3.6740e-02)	Acc@1  97.66 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [58][310/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.2458e-02 (3.7167e-02)	Acc@1  98.44 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [58][320/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1563e-02 (3.7154e-02)	Acc@1 100.00 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [58][330/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 6.2268e-02 (3.7349e-02)	Acc@1  98.44 ( 98.75)	Acc@5 100.00 (100.00)
Epoch: [58][340/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.1934e-02 (3.7744e-02)	Acc@1  98.44 ( 98.74)	Acc@5 100.00 (100.00)
Epoch: [58][350/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.5334e-02 (3.7936e-02)	Acc@1  97.66 ( 98.74)	Acc@5 100.00 (100.00)
Epoch: [58][360/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.5423e-02 (3.7925e-02)	Acc@1  99.22 ( 98.74)	Acc@5 100.00 (100.00)
Epoch: [58][370/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 5.6578e-02 (3.8072e-02)	Acc@1  98.44 ( 98.75)	Acc@5 100.00 ( 99.99)
Epoch: [58][380/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0190e-01 (3.8304e-02)	Acc@1  98.44 ( 98.73)	Acc@5 100.00 ( 99.99)
Epoch: [58][390/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8978e-02 (3.8479e-02)	Acc@1  98.75 ( 98.73)	Acc@5 100.00 ( 99.99)
## e[58] optimizer.zero_grad (sum) time: 0.10959553718566895
## e[58]       loss.backward (sum) time: 2.2566628456115723
## e[58]      optimizer.step (sum) time: 0.8667700290679932
## epoch[58] training(only) time: 10.401853084564209
# Switched to evaluate mode...
Test: [  0/100]	Time  0.165 ( 0.165)	Loss 1.9156e-01 (1.9156e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.010 ( 0.027)	Loss 4.3499e-01 (3.5193e-01)	Acc@1  90.00 ( 91.45)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.020 ( 0.023)	Loss 4.1072e-01 (3.7483e-01)	Acc@1  88.00 ( 90.67)	Acc@5 100.00 ( 99.48)
Test: [ 30/100]	Time  0.012 ( 0.020)	Loss 3.6538e-01 (3.9828e-01)	Acc@1  91.00 ( 90.16)	Acc@5  99.00 ( 99.48)
Test: [ 40/100]	Time  0.024 ( 0.018)	Loss 5.1783e-01 (4.1928e-01)	Acc@1  88.00 ( 89.71)	Acc@5  99.00 ( 99.51)
Test: [ 50/100]	Time  0.019 ( 0.018)	Loss 1.9075e-01 (4.1760e-01)	Acc@1  96.00 ( 89.75)	Acc@5  99.00 ( 99.49)
Test: [ 60/100]	Time  0.013 ( 0.017)	Loss 4.8281e-01 (4.1458e-01)	Acc@1  89.00 ( 89.92)	Acc@5 100.00 ( 99.52)
Test: [ 70/100]	Time  0.013 ( 0.017)	Loss 7.1176e-01 (4.1307e-01)	Acc@1  90.00 ( 90.07)	Acc@5  99.00 ( 99.51)
Test: [ 80/100]	Time  0.020 ( 0.017)	Loss 4.2413e-01 (4.1102e-01)	Acc@1  90.00 ( 90.19)	Acc@5 100.00 ( 99.54)
Test: [ 90/100]	Time  0.016 ( 0.016)	Loss 2.6080e-01 (4.0639e-01)	Acc@1  96.00 ( 90.21)	Acc@5 100.00 ( 99.58)
 * Acc@1 90.320 Acc@5 99.580
### epoch[58] execution time: 12.150435447692871
EPOCH 59
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [59][  0/391]	Time  0.190 ( 0.190)	Data  0.171 ( 0.171)	Loss 4.3290e-02 (4.3290e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [59][ 10/391]	Time  0.023 ( 0.041)	Data  0.002 ( 0.018)	Loss 1.7420e-02 (4.2485e-02)	Acc@1  99.22 ( 98.79)	Acc@5 100.00 (100.00)
Epoch: [59][ 20/391]	Time  0.022 ( 0.034)	Data  0.001 ( 0.010)	Loss 1.4281e-02 (3.3285e-02)	Acc@1 100.00 ( 99.03)	Acc@5 100.00 ( 99.96)
Epoch: [59][ 30/391]	Time  0.025 ( 0.031)	Data  0.001 ( 0.008)	Loss 3.2285e-02 (3.1291e-02)	Acc@1  99.22 ( 99.09)	Acc@5 100.00 ( 99.97)
Epoch: [59][ 40/391]	Time  0.033 ( 0.030)	Data  0.001 ( 0.006)	Loss 5.2118e-02 (3.0932e-02)	Acc@1  98.44 ( 99.05)	Acc@5 100.00 ( 99.98)
Epoch: [59][ 50/391]	Time  0.025 ( 0.029)	Data  0.001 ( 0.005)	Loss 1.7060e-02 (3.2193e-02)	Acc@1  99.22 ( 98.90)	Acc@5 100.00 ( 99.97)
Epoch: [59][ 60/391]	Time  0.024 ( 0.029)	Data  0.001 ( 0.005)	Loss 7.7173e-03 (3.2264e-02)	Acc@1 100.00 ( 98.89)	Acc@5 100.00 ( 99.97)
Epoch: [59][ 70/391]	Time  0.030 ( 0.028)	Data  0.000 ( 0.005)	Loss 5.5529e-02 (3.2352e-02)	Acc@1  97.66 ( 98.87)	Acc@5 100.00 ( 99.98)
Epoch: [59][ 80/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.7947e-02 (3.2615e-02)	Acc@1  99.22 ( 98.84)	Acc@5 100.00 ( 99.98)
Epoch: [59][ 90/391]	Time  0.022 ( 0.028)	Data  0.001 ( 0.004)	Loss 7.0507e-03 (3.3199e-02)	Acc@1 100.00 ( 98.82)	Acc@5 100.00 ( 99.98)
Epoch: [59][100/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 5.2044e-02 (3.2767e-02)	Acc@1  98.44 ( 98.81)	Acc@5 100.00 ( 99.98)
Epoch: [59][110/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 5.8691e-02 (3.3524e-02)	Acc@1  97.66 ( 98.80)	Acc@5 100.00 ( 99.99)
Epoch: [59][120/391]	Time  0.031 ( 0.027)	Data  0.003 ( 0.004)	Loss 1.5453e-02 (3.3215e-02)	Acc@1 100.00 ( 98.82)	Acc@5 100.00 ( 99.99)
Epoch: [59][130/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.0037e-02 (3.2665e-02)	Acc@1  99.22 ( 98.85)	Acc@5 100.00 ( 99.99)
Epoch: [59][140/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.8328e-02 (3.2989e-02)	Acc@1  99.22 ( 98.84)	Acc@5 100.00 ( 99.99)
Epoch: [59][150/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.4465e-02 (3.2843e-02)	Acc@1  98.44 ( 98.86)	Acc@5 100.00 ( 99.99)
Epoch: [59][160/391]	Time  0.028 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.1799e-02 (3.2803e-02)	Acc@1  98.44 ( 98.84)	Acc@5 100.00 ( 99.99)
Epoch: [59][170/391]	Time  0.029 ( 0.027)	Data  0.000 ( 0.003)	Loss 7.0038e-02 (3.3096e-02)	Acc@1  98.44 ( 98.83)	Acc@5 100.00 ( 99.99)
Epoch: [59][180/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.8343e-02 (3.3431e-02)	Acc@1  99.22 ( 98.81)	Acc@5 100.00 ( 99.99)
Epoch: [59][190/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.1814e-02 (3.3463e-02)	Acc@1  96.88 ( 98.81)	Acc@5 100.00 ( 99.99)
Epoch: [59][200/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.7870e-02 (3.3633e-02)	Acc@1  98.44 ( 98.80)	Acc@5 100.00 ( 99.99)
Epoch: [59][210/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.2928e-02 (3.3929e-02)	Acc@1  99.22 ( 98.81)	Acc@5 100.00 ( 99.99)
Epoch: [59][220/391]	Time  0.024 ( 0.027)	Data  0.003 ( 0.003)	Loss 5.7914e-02 (3.4665e-02)	Acc@1  97.66 ( 98.78)	Acc@5 100.00 ( 99.99)
Epoch: [59][230/391]	Time  0.022 ( 0.027)	Data  0.002 ( 0.003)	Loss 8.5078e-03 (3.4797e-02)	Acc@1 100.00 ( 98.77)	Acc@5 100.00 ( 99.99)
Epoch: [59][240/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.6332e-02 (3.4734e-02)	Acc@1  99.22 ( 98.77)	Acc@5 100.00 ( 99.99)
Epoch: [59][250/391]	Time  0.024 ( 0.027)	Data  0.000 ( 0.003)	Loss 4.0095e-02 (3.4391e-02)	Acc@1  98.44 ( 98.79)	Acc@5 100.00 ( 99.99)
Epoch: [59][260/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.1879e-01 (3.4609e-02)	Acc@1  96.88 ( 98.78)	Acc@5 100.00 ( 99.99)
Epoch: [59][270/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.1464e-01 (3.4997e-02)	Acc@1  97.66 ( 98.78)	Acc@5 100.00 ( 99.99)
Epoch: [59][280/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.3735e-03 (3.4805e-02)	Acc@1 100.00 ( 98.79)	Acc@5 100.00 ( 99.99)
Epoch: [59][290/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.7925e-02 (3.4446e-02)	Acc@1  98.44 ( 98.81)	Acc@5 100.00 ( 99.99)
Epoch: [59][300/391]	Time  0.029 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.5251e-02 (3.4479e-02)	Acc@1  98.44 ( 98.80)	Acc@5 100.00 ( 99.99)
Epoch: [59][310/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.1682e-02 (3.5279e-02)	Acc@1  99.22 ( 98.78)	Acc@5 100.00 ( 99.99)
Epoch: [59][320/391]	Time  0.035 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6723e-02 (3.5404e-02)	Acc@1  99.22 ( 98.76)	Acc@5 100.00 ( 99.99)
Epoch: [59][330/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.4957e-02 (3.5390e-02)	Acc@1  99.22 ( 98.76)	Acc@5 100.00 ( 99.99)
Epoch: [59][340/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.7355e-02 (3.5318e-02)	Acc@1  98.44 ( 98.76)	Acc@5 100.00 ( 99.99)
Epoch: [59][350/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.9169e-03 (3.5323e-02)	Acc@1 100.00 ( 98.75)	Acc@5 100.00 ( 99.99)
Epoch: [59][360/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.9952e-02 (3.5271e-02)	Acc@1  97.66 ( 98.74)	Acc@5 100.00 ( 99.99)
Epoch: [59][370/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.5886e-02 (3.5411e-02)	Acc@1  99.22 ( 98.74)	Acc@5 100.00 ( 99.99)
Epoch: [59][380/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5340e-02 (3.5526e-02)	Acc@1  99.22 ( 98.74)	Acc@5 100.00 ( 99.99)
Epoch: [59][390/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.2603e-02 (3.5483e-02)	Acc@1  98.75 ( 98.75)	Acc@5 100.00 ( 99.99)
## e[59] optimizer.zero_grad (sum) time: 0.10885286331176758
## e[59]       loss.backward (sum) time: 2.250458002090454
## e[59]      optimizer.step (sum) time: 0.8737409114837646
## epoch[59] training(only) time: 10.436579704284668
# Switched to evaluate mode...
Test: [  0/100]	Time  0.170 ( 0.170)	Loss 2.5165e-01 (2.5165e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.019 ( 0.029)	Loss 4.8511e-01 (4.0319e-01)	Acc@1  89.00 ( 90.36)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.019 ( 0.022)	Loss 4.2624e-01 (4.1380e-01)	Acc@1  90.00 ( 90.05)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.011 ( 0.019)	Loss 3.6712e-01 (4.3420e-01)	Acc@1  90.00 ( 89.71)	Acc@5 100.00 ( 99.58)
Test: [ 40/100]	Time  0.011 ( 0.018)	Loss 5.0462e-01 (4.4373e-01)	Acc@1  88.00 ( 89.54)	Acc@5  99.00 ( 99.59)
Test: [ 50/100]	Time  0.022 ( 0.017)	Loss 2.1554e-01 (4.3637e-01)	Acc@1  93.00 ( 89.78)	Acc@5  99.00 ( 99.53)
Test: [ 60/100]	Time  0.017 ( 0.017)	Loss 3.5199e-01 (4.2882e-01)	Acc@1  90.00 ( 90.10)	Acc@5 100.00 ( 99.54)
Test: [ 70/100]	Time  0.013 ( 0.016)	Loss 7.9336e-01 (4.3033e-01)	Acc@1  87.00 ( 90.11)	Acc@5  98.00 ( 99.49)
Test: [ 80/100]	Time  0.011 ( 0.016)	Loss 3.8676e-01 (4.2744e-01)	Acc@1  92.00 ( 90.30)	Acc@5 100.00 ( 99.53)
Test: [ 90/100]	Time  0.011 ( 0.016)	Loss 3.2229e-01 (4.2294e-01)	Acc@1  92.00 ( 90.37)	Acc@5 100.00 ( 99.56)
 * Acc@1 90.480 Acc@5 99.580
### epoch[59] execution time: 12.124974012374878
EPOCH 60
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [60][  0/391]	Time  0.184 ( 0.184)	Data  0.164 ( 0.164)	Loss 5.2886e-02 (5.2886e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [60][ 10/391]	Time  0.030 ( 0.041)	Data  0.004 ( 0.017)	Loss 4.9965e-02 (3.4160e-02)	Acc@1  98.44 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [60][ 20/391]	Time  0.031 ( 0.034)	Data  0.000 ( 0.010)	Loss 5.9543e-02 (3.5416e-02)	Acc@1  98.44 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [60][ 30/391]	Time  0.030 ( 0.032)	Data  0.001 ( 0.007)	Loss 1.2072e-02 (3.2887e-02)	Acc@1 100.00 ( 98.82)	Acc@5 100.00 (100.00)
Epoch: [60][ 40/391]	Time  0.035 ( 0.030)	Data  0.001 ( 0.006)	Loss 6.1308e-02 (3.4292e-02)	Acc@1  97.66 ( 98.78)	Acc@5 100.00 (100.00)
Epoch: [60][ 50/391]	Time  0.024 ( 0.029)	Data  0.001 ( 0.005)	Loss 2.5700e-02 (3.2044e-02)	Acc@1  98.44 ( 98.90)	Acc@5 100.00 (100.00)
Epoch: [60][ 60/391]	Time  0.022 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.2982e-02 (2.9799e-02)	Acc@1 100.00 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [60][ 70/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 2.1882e-02 (3.0236e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [60][ 80/391]	Time  0.031 ( 0.028)	Data  0.001 ( 0.004)	Loss 2.1010e-02 (2.9833e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 ( 99.99)
Epoch: [60][ 90/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 2.6947e-02 (3.0315e-02)	Acc@1  98.44 ( 99.00)	Acc@5 100.00 ( 99.99)
Epoch: [60][100/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 4.1012e-03 (3.1396e-02)	Acc@1 100.00 ( 98.92)	Acc@5 100.00 ( 99.99)
Epoch: [60][110/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.1131e-02 (3.0525e-02)	Acc@1  99.22 ( 98.94)	Acc@5 100.00 ( 99.99)
Epoch: [60][120/391]	Time  0.024 ( 0.027)	Data  0.000 ( 0.004)	Loss 5.1426e-03 (3.0130e-02)	Acc@1 100.00 ( 98.94)	Acc@5 100.00 ( 99.99)
Epoch: [60][130/391]	Time  0.027 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.3270e-02 (3.0587e-02)	Acc@1 100.00 ( 98.94)	Acc@5 100.00 ( 99.99)
Epoch: [60][140/391]	Time  0.031 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.2336e-02 (3.0896e-02)	Acc@1 100.00 ( 98.93)	Acc@5 100.00 ( 99.99)
Epoch: [60][150/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.9345e-02 (3.0681e-02)	Acc@1  98.44 ( 98.93)	Acc@5 100.00 ( 99.99)
Epoch: [60][160/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 4.0042e-02 (3.0763e-02)	Acc@1  99.22 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [60][170/391]	Time  0.028 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.9233e-02 (3.0429e-02)	Acc@1  99.22 ( 98.94)	Acc@5 100.00 (100.00)
Epoch: [60][180/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.6907e-02 (3.0412e-02)	Acc@1  97.66 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [60][190/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.9708e-03 (3.0067e-02)	Acc@1 100.00 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [60][200/391]	Time  0.026 ( 0.027)	Data  0.009 ( 0.003)	Loss 3.1339e-02 (2.9958e-02)	Acc@1  98.44 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [60][210/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.3689e-02 (2.9565e-02)	Acc@1  97.66 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [60][220/391]	Time  0.032 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.7528e-02 (3.0049e-02)	Acc@1  97.66 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [60][230/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 2.8950e-02 (2.9841e-02)	Acc@1  99.22 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [60][240/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.0610e-02 (3.0102e-02)	Acc@1  97.66 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [60][250/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.9254e-03 (2.9936e-02)	Acc@1 100.00 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [60][260/391]	Time  0.032 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.6783e-03 (2.9528e-02)	Acc@1 100.00 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [60][270/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.0382e-02 (2.9559e-02)	Acc@1  99.22 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [60][280/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 2.7192e-02 (2.9672e-02)	Acc@1  99.22 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [60][290/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.2052e-02 (2.9424e-02)	Acc@1  99.22 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [60][300/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.6405e-02 (2.9168e-02)	Acc@1 100.00 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [60][310/391]	Time  0.024 ( 0.026)	Data  0.002 ( 0.003)	Loss 8.0135e-02 (2.9366e-02)	Acc@1  96.09 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [60][320/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6906e-02 (2.9190e-02)	Acc@1  99.22 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [60][330/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3039e-02 (2.9201e-02)	Acc@1  99.22 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [60][340/391]	Time  0.039 ( 0.026)	Data  0.000 ( 0.003)	Loss 5.1094e-02 (2.9513e-02)	Acc@1  98.44 ( 98.97)	Acc@5  99.22 (100.00)
Epoch: [60][350/391]	Time  0.025 ( 0.026)	Data  0.005 ( 0.003)	Loss 1.0784e-02 (2.9668e-02)	Acc@1 100.00 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [60][360/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 6.2326e-02 (2.9624e-02)	Acc@1  96.88 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [60][370/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3047e-02 (2.9633e-02)	Acc@1  98.44 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [60][380/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.8340e-03 (2.9690e-02)	Acc@1 100.00 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [60][390/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4672e-02 (2.9600e-02)	Acc@1  98.75 ( 98.97)	Acc@5 100.00 (100.00)
## e[60] optimizer.zero_grad (sum) time: 0.11264181137084961
## e[60]       loss.backward (sum) time: 2.2544333934783936
## e[60]      optimizer.step (sum) time: 0.86568284034729
## epoch[60] training(only) time: 10.446813344955444
# Switched to evaluate mode...
Test: [  0/100]	Time  0.162 ( 0.162)	Loss 1.9558e-01 (1.9558e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.011 ( 0.027)	Loss 4.1663e-01 (3.6434e-01)	Acc@1  91.00 ( 91.55)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.012 ( 0.021)	Loss 3.6622e-01 (3.8346e-01)	Acc@1  92.00 ( 90.95)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.011 ( 0.019)	Loss 3.7108e-01 (4.0617e-01)	Acc@1  87.00 ( 90.35)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.011 ( 0.018)	Loss 5.7112e-01 (4.2209e-01)	Acc@1  89.00 ( 90.07)	Acc@5  99.00 ( 99.63)
Test: [ 50/100]	Time  0.016 ( 0.017)	Loss 1.7700e-01 (4.1418e-01)	Acc@1  96.00 ( 90.25)	Acc@5 100.00 ( 99.59)
Test: [ 60/100]	Time  0.022 ( 0.017)	Loss 4.2192e-01 (4.0985e-01)	Acc@1  89.00 ( 90.49)	Acc@5 100.00 ( 99.59)
Test: [ 70/100]	Time  0.012 ( 0.017)	Loss 7.5403e-01 (4.0965e-01)	Acc@1  89.00 ( 90.48)	Acc@5 100.00 ( 99.58)
Test: [ 80/100]	Time  0.016 ( 0.016)	Loss 4.0084e-01 (4.0877e-01)	Acc@1  93.00 ( 90.49)	Acc@5 100.00 ( 99.63)
Test: [ 90/100]	Time  0.014 ( 0.016)	Loss 2.7320e-01 (4.0432e-01)	Acc@1  93.00 ( 90.54)	Acc@5 100.00 ( 99.66)
 * Acc@1 90.650 Acc@5 99.670
### epoch[60] execution time: 12.16382122039795
EPOCH 61
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [61][  0/391]	Time  0.189 ( 0.189)	Data  0.169 ( 0.169)	Loss 2.5690e-02 (2.5690e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [61][ 10/391]	Time  0.024 ( 0.041)	Data  0.001 ( 0.017)	Loss 5.2423e-02 (2.5719e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [61][ 20/391]	Time  0.027 ( 0.034)	Data  0.001 ( 0.011)	Loss 2.3360e-02 (2.3262e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [61][ 30/391]	Time  0.029 ( 0.031)	Data  0.001 ( 0.008)	Loss 3.8660e-03 (2.1460e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [61][ 40/391]	Time  0.023 ( 0.030)	Data  0.001 ( 0.006)	Loss 3.4893e-02 (2.2047e-02)	Acc@1  97.66 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [61][ 50/391]	Time  0.024 ( 0.029)	Data  0.001 ( 0.006)	Loss 1.4501e-02 (2.3655e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [61][ 60/391]	Time  0.027 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.4228e-02 (2.3722e-02)	Acc@1  99.22 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [61][ 70/391]	Time  0.023 ( 0.028)	Data  0.003 ( 0.005)	Loss 5.8900e-03 (2.5174e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 ( 99.99)
Epoch: [61][ 80/391]	Time  0.027 ( 0.028)	Data  0.006 ( 0.004)	Loss 5.0719e-02 (2.5739e-02)	Acc@1  98.44 ( 99.17)	Acc@5 100.00 ( 99.99)
Epoch: [61][ 90/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 6.7101e-03 (2.5589e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 ( 99.99)
Epoch: [61][100/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.8176e-02 (2.6167e-02)	Acc@1  97.66 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [61][110/391]	Time  0.025 ( 0.027)	Data  0.003 ( 0.004)	Loss 1.6031e-02 (2.6042e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [61][120/391]	Time  0.028 ( 0.027)	Data  0.004 ( 0.004)	Loss 1.0135e-02 (2.6563e-02)	Acc@1 100.00 ( 99.12)	Acc@5 100.00 ( 99.99)
Epoch: [61][130/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.4590e-02 (2.5828e-02)	Acc@1  98.44 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [61][140/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.4732e-03 (2.5580e-02)	Acc@1 100.00 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [61][150/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.3662e-02 (2.6514e-02)	Acc@1  98.44 ( 99.10)	Acc@5 100.00 ( 99.99)
Epoch: [61][160/391]	Time  0.024 ( 0.027)	Data  0.003 ( 0.003)	Loss 5.0485e-03 (2.6426e-02)	Acc@1 100.00 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [61][170/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.0895e-02 (2.6045e-02)	Acc@1  99.22 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [61][180/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.4005e-03 (2.5991e-02)	Acc@1 100.00 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [61][190/391]	Time  0.030 ( 0.027)	Data  0.000 ( 0.003)	Loss 5.6176e-02 (2.6266e-02)	Acc@1  96.09 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [61][200/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.3594e-02 (2.6030e-02)	Acc@1  99.22 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [61][210/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.1122e-03 (2.6150e-02)	Acc@1 100.00 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [61][220/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 9.4604e-03 (2.6169e-02)	Acc@1 100.00 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [61][230/391]	Time  0.027 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.2987e-02 (2.6224e-02)	Acc@1  99.22 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [61][240/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.8426e-02 (2.6263e-02)	Acc@1  99.22 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [61][250/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.1372e-02 (2.6191e-02)	Acc@1 100.00 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [61][260/391]	Time  0.030 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.5244e-03 (2.6068e-02)	Acc@1 100.00 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [61][270/391]	Time  0.032 ( 0.027)	Data  0.001 ( 0.003)	Loss 8.9033e-03 (2.5746e-02)	Acc@1 100.00 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [61][280/391]	Time  0.038 ( 0.027)	Data  0.008 ( 0.003)	Loss 6.3723e-03 (2.5639e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 ( 99.99)
Epoch: [61][290/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.4005e-03 (2.5691e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 ( 99.99)
Epoch: [61][300/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.5458e-02 (2.5799e-02)	Acc@1  97.66 ( 99.16)	Acc@5 100.00 ( 99.99)
Epoch: [61][310/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4120e-02 (2.5652e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 ( 99.99)
Epoch: [61][320/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.4270e-03 (2.5541e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 ( 99.99)
Epoch: [61][330/391]	Time  0.025 ( 0.026)	Data  0.002 ( 0.003)	Loss 5.3150e-03 (2.5468e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 ( 99.99)
Epoch: [61][340/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0944e-01 (2.5808e-02)	Acc@1  98.44 ( 99.17)	Acc@5  99.22 ( 99.99)
Epoch: [61][350/391]	Time  0.040 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.1690e-02 (2.5838e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 ( 99.99)
Epoch: [61][360/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.2568e-03 (2.6200e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 ( 99.99)
Epoch: [61][370/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.3557e-02 (2.6373e-02)	Acc@1  97.66 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [61][380/391]	Time  0.026 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.9677e-02 (2.6509e-02)	Acc@1  99.22 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [61][390/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.4718e-03 (2.6418e-02)	Acc@1 100.00 ( 99.15)	Acc@5 100.00 ( 99.99)
## e[61] optimizer.zero_grad (sum) time: 0.10911321640014648
## e[61]       loss.backward (sum) time: 2.252873659133911
## e[61]      optimizer.step (sum) time: 0.8679754734039307
## epoch[61] training(only) time: 10.450884580612183
# Switched to evaluate mode...
Test: [  0/100]	Time  0.156 ( 0.156)	Loss 2.2613e-01 (2.2613e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.015 ( 0.027)	Loss 3.9324e-01 (3.6309e-01)	Acc@1  91.00 ( 91.45)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.012 ( 0.021)	Loss 3.9276e-01 (3.8311e-01)	Acc@1  92.00 ( 91.10)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.015 ( 0.019)	Loss 3.6183e-01 (4.0567e-01)	Acc@1  88.00 ( 90.39)	Acc@5 100.00 ( 99.61)
Test: [ 40/100]	Time  0.016 ( 0.018)	Loss 5.4310e-01 (4.2122e-01)	Acc@1  88.00 ( 90.10)	Acc@5  99.00 ( 99.61)
Test: [ 50/100]	Time  0.011 ( 0.017)	Loss 1.7133e-01 (4.1483e-01)	Acc@1  96.00 ( 90.29)	Acc@5 100.00 ( 99.59)
Test: [ 60/100]	Time  0.010 ( 0.017)	Loss 4.1601e-01 (4.1027e-01)	Acc@1  90.00 ( 90.52)	Acc@5 100.00 ( 99.59)
Test: [ 70/100]	Time  0.017 ( 0.016)	Loss 7.5617e-01 (4.1060e-01)	Acc@1  89.00 ( 90.56)	Acc@5 100.00 ( 99.56)
Test: [ 80/100]	Time  0.022 ( 0.016)	Loss 3.8274e-01 (4.0968e-01)	Acc@1  93.00 ( 90.63)	Acc@5 100.00 ( 99.60)
Test: [ 90/100]	Time  0.011 ( 0.016)	Loss 2.5813e-01 (4.0509e-01)	Acc@1  94.00 ( 90.67)	Acc@5 100.00 ( 99.64)
 * Acc@1 90.780 Acc@5 99.650
### epoch[61] execution time: 12.148207664489746
EPOCH 62
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [62][  0/391]	Time  0.187 ( 0.187)	Data  0.165 ( 0.165)	Loss 5.2652e-02 (5.2652e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [62][ 10/391]	Time  0.024 ( 0.040)	Data  0.001 ( 0.017)	Loss 4.1237e-02 (2.4404e-02)	Acc@1  98.44 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [62][ 20/391]	Time  0.023 ( 0.033)	Data  0.001 ( 0.010)	Loss 4.9953e-03 (2.4659e-02)	Acc@1 100.00 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [62][ 30/391]	Time  0.023 ( 0.031)	Data  0.001 ( 0.008)	Loss 3.1514e-02 (2.5387e-02)	Acc@1  98.44 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [62][ 40/391]	Time  0.023 ( 0.030)	Data  0.001 ( 0.007)	Loss 1.7663e-02 (2.5376e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [62][ 50/391]	Time  0.030 ( 0.029)	Data  0.002 ( 0.006)	Loss 1.7439e-02 (2.5912e-02)	Acc@1 100.00 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [62][ 60/391]	Time  0.033 ( 0.028)	Data  0.001 ( 0.005)	Loss 6.3222e-02 (2.7191e-02)	Acc@1  98.44 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [62][ 70/391]	Time  0.030 ( 0.028)	Data  0.005 ( 0.005)	Loss 2.7741e-02 (2.6732e-02)	Acc@1  98.44 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [62][ 80/391]	Time  0.025 ( 0.028)	Data  0.003 ( 0.004)	Loss 4.2911e-03 (2.6213e-02)	Acc@1 100.00 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [62][ 90/391]	Time  0.022 ( 0.027)	Data  0.002 ( 0.004)	Loss 1.8832e-02 (2.4812e-02)	Acc@1  98.44 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [62][100/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.6631e-02 (2.5572e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [62][110/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 6.7093e-03 (2.5693e-02)	Acc@1 100.00 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [62][120/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.3435e-03 (2.5584e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [62][130/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.1884e-02 (2.5221e-02)	Acc@1  98.44 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [62][140/391]	Time  0.030 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.6492e-02 (2.5798e-02)	Acc@1  98.44 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [62][150/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.1003e-02 (2.6915e-02)	Acc@1  99.22 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [62][160/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.3025e-02 (2.7445e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [62][170/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 9.8438e-02 (2.7857e-02)	Acc@1  96.09 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [62][180/391]	Time  0.022 ( 0.027)	Data  0.002 ( 0.003)	Loss 5.0691e-03 (2.7439e-02)	Acc@1 100.00 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [62][190/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.2560e-02 (2.7621e-02)	Acc@1  99.22 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [62][200/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.5613e-02 (2.7609e-02)	Acc@1  98.44 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [62][210/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.7683e-02 (2.7341e-02)	Acc@1  99.22 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [62][220/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.7471e-03 (2.6894e-02)	Acc@1 100.00 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [62][230/391]	Time  0.022 ( 0.026)	Data  0.002 ( 0.003)	Loss 6.1363e-03 (2.6701e-02)	Acc@1 100.00 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [62][240/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.7938e-03 (2.6186e-02)	Acc@1 100.00 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [62][250/391]	Time  0.029 ( 0.026)	Data  0.006 ( 0.003)	Loss 1.8918e-03 (2.5945e-02)	Acc@1 100.00 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [62][260/391]	Time  0.028 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.3354e-02 (2.5809e-02)	Acc@1 100.00 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [62][270/391]	Time  0.022 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.7594e-02 (2.5844e-02)	Acc@1  98.44 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [62][280/391]	Time  0.025 ( 0.026)	Data  0.006 ( 0.003)	Loss 3.6576e-02 (2.5792e-02)	Acc@1  98.44 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [62][290/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6555e-02 (2.6165e-02)	Acc@1  99.22 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [62][300/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.5508e-03 (2.6037e-02)	Acc@1 100.00 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [62][310/391]	Time  0.036 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.9001e-02 (2.5925e-02)	Acc@1  99.22 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [62][320/391]	Time  0.033 ( 0.026)	Data  0.012 ( 0.003)	Loss 2.5155e-02 (2.5756e-02)	Acc@1  99.22 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [62][330/391]	Time  0.022 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.0836e-02 (2.6091e-02)	Acc@1  98.44 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [62][340/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.8437e-03 (2.6393e-02)	Acc@1 100.00 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [62][350/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.4132e-03 (2.6064e-02)	Acc@1 100.00 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [62][360/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.6399e-03 (2.5905e-02)	Acc@1  99.22 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [62][370/391]	Time  0.032 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.2892e-02 (2.6008e-02)	Acc@1  99.22 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [62][380/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.5948e-02 (2.6000e-02)	Acc@1  99.22 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [62][390/391]	Time  0.023 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.0468e-02 (2.6136e-02)	Acc@1 100.00 ( 99.13)	Acc@5 100.00 (100.00)
## e[62] optimizer.zero_grad (sum) time: 0.10854530334472656
## e[62]       loss.backward (sum) time: 2.249887704849243
## e[62]      optimizer.step (sum) time: 0.8661794662475586
## epoch[62] training(only) time: 10.419621229171753
# Switched to evaluate mode...
Test: [  0/100]	Time  0.156 ( 0.156)	Loss 1.9965e-01 (1.9965e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.011 ( 0.027)	Loss 4.0379e-01 (3.4866e-01)	Acc@1  91.00 ( 91.82)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.012 ( 0.021)	Loss 3.6584e-01 (3.6973e-01)	Acc@1  91.00 ( 91.24)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.016 ( 0.019)	Loss 3.4175e-01 (3.9213e-01)	Acc@1  89.00 ( 90.45)	Acc@5 100.00 ( 99.61)
Test: [ 40/100]	Time  0.011 ( 0.018)	Loss 5.5094e-01 (4.1058e-01)	Acc@1  88.00 ( 90.17)	Acc@5  99.00 ( 99.61)
Test: [ 50/100]	Time  0.012 ( 0.017)	Loss 1.8112e-01 (4.0550e-01)	Acc@1  95.00 ( 90.39)	Acc@5 100.00 ( 99.57)
Test: [ 60/100]	Time  0.011 ( 0.017)	Loss 4.0044e-01 (4.0290e-01)	Acc@1  91.00 ( 90.57)	Acc@5 100.00 ( 99.57)
Test: [ 70/100]	Time  0.016 ( 0.016)	Loss 7.6255e-01 (4.0351e-01)	Acc@1  91.00 ( 90.65)	Acc@5 100.00 ( 99.55)
Test: [ 80/100]	Time  0.011 ( 0.016)	Loss 3.8653e-01 (4.0247e-01)	Acc@1  93.00 ( 90.69)	Acc@5 100.00 ( 99.60)
Test: [ 90/100]	Time  0.018 ( 0.016)	Loss 2.6461e-01 (3.9850e-01)	Acc@1  94.00 ( 90.74)	Acc@5 100.00 ( 99.64)
 * Acc@1 90.820 Acc@5 99.650
### epoch[62] execution time: 12.111368656158447
EPOCH 63
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [63][  0/391]	Time  0.192 ( 0.192)	Data  0.173 ( 0.173)	Loss 1.1309e-02 (1.1309e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [63][ 10/391]	Time  0.027 ( 0.040)	Data  0.002 ( 0.018)	Loss 3.4772e-02 (2.1911e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [63][ 20/391]	Time  0.023 ( 0.034)	Data  0.001 ( 0.010)	Loss 2.8542e-03 (2.2151e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [63][ 30/391]	Time  0.022 ( 0.031)	Data  0.002 ( 0.008)	Loss 2.5157e-02 (2.2076e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [63][ 40/391]	Time  0.024 ( 0.030)	Data  0.001 ( 0.007)	Loss 4.1862e-02 (2.4107e-02)	Acc@1  99.22 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [63][ 50/391]	Time  0.027 ( 0.029)	Data  0.001 ( 0.006)	Loss 6.8764e-02 (2.5321e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [63][ 60/391]	Time  0.026 ( 0.029)	Data  0.002 ( 0.005)	Loss 3.3086e-02 (2.5064e-02)	Acc@1  98.44 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [63][ 70/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.005)	Loss 3.4236e-03 (2.4587e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [63][ 80/391]	Time  0.023 ( 0.028)	Data  0.000 ( 0.004)	Loss 5.9751e-02 (2.7458e-02)	Acc@1  98.44 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [63][ 90/391]	Time  0.030 ( 0.028)	Data  0.002 ( 0.004)	Loss 5.0347e-02 (2.6524e-02)	Acc@1  97.66 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [63][100/391]	Time  0.023 ( 0.027)	Data  0.000 ( 0.004)	Loss 1.6297e-02 (2.5609e-02)	Acc@1  99.22 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [63][110/391]	Time  0.028 ( 0.027)	Data  0.001 ( 0.004)	Loss 6.0502e-03 (2.4987e-02)	Acc@1 100.00 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [63][120/391]	Time  0.028 ( 0.027)	Data  0.001 ( 0.004)	Loss 4.5286e-02 (2.4612e-02)	Acc@1  98.44 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [63][130/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.5801e-02 (2.4655e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [63][140/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.2520e-02 (2.4754e-02)	Acc@1  98.44 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [63][150/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.0869e-02 (2.4542e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [63][160/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.9606e-03 (2.4302e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [63][170/391]	Time  0.024 ( 0.027)	Data  0.002 ( 0.003)	Loss 1.6028e-02 (2.4747e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [63][180/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.003)	Loss 9.0315e-03 (2.5125e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [63][190/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.9744e-02 (2.5310e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [63][200/391]	Time  0.030 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.6874e-02 (2.5715e-02)	Acc@1  96.88 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [63][210/391]	Time  0.024 ( 0.027)	Data  0.002 ( 0.003)	Loss 1.4390e-02 (2.5890e-02)	Acc@1  99.22 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [63][220/391]	Time  0.026 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.5470e-02 (2.5815e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [63][230/391]	Time  0.030 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.2527e-02 (2.6256e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [63][240/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.0259e-02 (2.5885e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [63][250/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 8.4616e-03 (2.6120e-02)	Acc@1 100.00 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [63][260/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.6466e-02 (2.5950e-02)	Acc@1  99.22 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [63][270/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 7.4853e-03 (2.5487e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [63][280/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.9827e-03 (2.5244e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [63][290/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.5503e-03 (2.5261e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [63][300/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4129e-02 (2.5161e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [63][310/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.1155e-02 (2.5218e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [63][320/391]	Time  0.043 ( 0.026)	Data  0.019 ( 0.003)	Loss 4.8480e-02 (2.5362e-02)	Acc@1  98.44 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [63][330/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.4539e-03 (2.5289e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [63][340/391]	Time  0.027 ( 0.026)	Data  0.003 ( 0.003)	Loss 4.6214e-02 (2.5174e-02)	Acc@1  98.44 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [63][350/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7890e-02 (2.4931e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [63][360/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3312e-02 (2.5092e-02)	Acc@1  98.44 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [63][370/391]	Time  0.026 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.1165e-02 (2.4950e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [63][380/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.7645e-03 (2.4852e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [63][390/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0288e-03 (2.4863e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
## e[63] optimizer.zero_grad (sum) time: 0.10984325408935547
## e[63]       loss.backward (sum) time: 2.2667999267578125
## e[63]      optimizer.step (sum) time: 0.8736526966094971
## epoch[63] training(only) time: 10.472429513931274
# Switched to evaluate mode...
Test: [  0/100]	Time  0.158 ( 0.158)	Loss 2.0239e-01 (2.0239e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.018 ( 0.027)	Loss 4.0483e-01 (3.6303e-01)	Acc@1  91.00 ( 91.18)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.019 ( 0.021)	Loss 3.8643e-01 (3.8011e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.010 ( 0.019)	Loss 3.3972e-01 (3.9759e-01)	Acc@1  88.00 ( 90.35)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.019 ( 0.018)	Loss 5.7820e-01 (4.1510e-01)	Acc@1  88.00 ( 90.22)	Acc@5  99.00 ( 99.63)
Test: [ 50/100]	Time  0.013 ( 0.018)	Loss 1.7671e-01 (4.0915e-01)	Acc@1  97.00 ( 90.47)	Acc@5 100.00 ( 99.63)
Test: [ 60/100]	Time  0.015 ( 0.017)	Loss 4.0739e-01 (4.0637e-01)	Acc@1  90.00 ( 90.67)	Acc@5 100.00 ( 99.64)
Test: [ 70/100]	Time  0.013 ( 0.017)	Loss 7.5418e-01 (4.0685e-01)	Acc@1  90.00 ( 90.68)	Acc@5 100.00 ( 99.62)
Test: [ 80/100]	Time  0.013 ( 0.016)	Loss 3.8503e-01 (4.0487e-01)	Acc@1  92.00 ( 90.70)	Acc@5 100.00 ( 99.67)
Test: [ 90/100]	Time  0.012 ( 0.016)	Loss 2.4676e-01 (4.0099e-01)	Acc@1  94.00 ( 90.74)	Acc@5 100.00 ( 99.69)
 * Acc@1 90.820 Acc@5 99.700
### epoch[63] execution time: 12.223446369171143
EPOCH 64
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [64][  0/391]	Time  0.197 ( 0.197)	Data  0.176 ( 0.176)	Loss 3.7505e-02 (3.7505e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [64][ 10/391]	Time  0.022 ( 0.041)	Data  0.001 ( 0.018)	Loss 5.2563e-02 (2.5788e-02)	Acc@1  96.88 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [64][ 20/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.010)	Loss 2.5982e-02 (2.6904e-02)	Acc@1  98.44 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [64][ 30/391]	Time  0.025 ( 0.031)	Data  0.001 ( 0.008)	Loss 1.0592e-02 (2.3639e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [64][ 40/391]	Time  0.023 ( 0.030)	Data  0.001 ( 0.006)	Loss 4.7003e-02 (2.3071e-02)	Acc@1  99.22 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [64][ 50/391]	Time  0.025 ( 0.029)	Data  0.005 ( 0.006)	Loss 3.4293e-02 (2.3033e-02)	Acc@1  99.22 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [64][ 60/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.005)	Loss 7.8206e-03 (2.4492e-02)	Acc@1 100.00 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [64][ 70/391]	Time  0.030 ( 0.028)	Data  0.000 ( 0.005)	Loss 2.5449e-02 (2.3750e-02)	Acc@1  98.44 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [64][ 80/391]	Time  0.027 ( 0.028)	Data  0.001 ( 0.004)	Loss 5.3725e-03 (2.3898e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [64][ 90/391]	Time  0.029 ( 0.028)	Data  0.001 ( 0.004)	Loss 4.8673e-03 (2.2557e-02)	Acc@1 100.00 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [64][100/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.7291e-02 (2.2889e-02)	Acc@1  99.22 ( 99.21)	Acc@5 100.00 ( 99.99)
Epoch: [64][110/391]	Time  0.032 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.5040e-02 (2.2302e-02)	Acc@1  99.22 ( 99.23)	Acc@5 100.00 ( 99.99)
Epoch: [64][120/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.6993e-02 (2.2353e-02)	Acc@1  98.44 ( 99.23)	Acc@5 100.00 ( 99.99)
Epoch: [64][130/391]	Time  0.033 ( 0.027)	Data  0.005 ( 0.004)	Loss 2.0496e-03 (2.1827e-02)	Acc@1 100.00 ( 99.23)	Acc@5 100.00 ( 99.99)
Epoch: [64][140/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.6269e-02 (2.1592e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 ( 99.99)
Epoch: [64][150/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 9.2208e-02 (2.2710e-02)	Acc@1  97.66 ( 99.21)	Acc@5  99.22 ( 99.99)
Epoch: [64][160/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 3.3863e-02 (2.2934e-02)	Acc@1  98.44 ( 99.19)	Acc@5 100.00 ( 99.99)
Epoch: [64][170/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 8.4316e-03 (2.2568e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 ( 99.99)
Epoch: [64][180/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 3.9949e-02 (2.2937e-02)	Acc@1  98.44 ( 99.18)	Acc@5 100.00 ( 99.99)
Epoch: [64][190/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.4956e-02 (2.3300e-02)	Acc@1  99.22 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [64][200/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.2032e-03 (2.3410e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [64][210/391]	Time  0.024 ( 0.027)	Data  0.002 ( 0.003)	Loss 3.5001e-03 (2.3029e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 ( 99.99)
Epoch: [64][220/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 2.2216e-03 (2.2570e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 ( 99.99)
Epoch: [64][230/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.5464e-02 (2.2561e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 ( 99.99)
Epoch: [64][240/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 1.5522e-02 (2.2370e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 ( 99.99)
Epoch: [64][250/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.7612e-03 (2.2395e-02)	Acc@1 100.00 ( 99.21)	Acc@5 100.00 ( 99.99)
Epoch: [64][260/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.6038e-02 (2.2214e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 ( 99.99)
Epoch: [64][270/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.9774e-02 (2.2343e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 ( 99.99)
Epoch: [64][280/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.5463e-03 (2.2629e-02)	Acc@1 100.00 ( 99.21)	Acc@5 100.00 ( 99.99)
Epoch: [64][290/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.5009e-03 (2.2698e-02)	Acc@1 100.00 ( 99.21)	Acc@5 100.00 ( 99.99)
Epoch: [64][300/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3279e-02 (2.3019e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 ( 99.99)
Epoch: [64][310/391]	Time  0.031 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.3057e-02 (2.3223e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 ( 99.99)
Epoch: [64][320/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3130e-02 (2.3280e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [64][330/391]	Time  0.028 ( 0.026)	Data  0.003 ( 0.003)	Loss 4.1027e-02 (2.3292e-02)	Acc@1  99.22 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [64][340/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.2919e-03 (2.3457e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [64][350/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.8305e-02 (2.3569e-02)	Acc@1  98.44 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [64][360/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.4235e-03 (2.3619e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 ( 99.99)
Epoch: [64][370/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.6479e-02 (2.3434e-02)	Acc@1  98.44 ( 99.20)	Acc@5 100.00 ( 99.99)
Epoch: [64][380/391]	Time  0.040 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.1438e-02 (2.3453e-02)	Acc@1  98.44 ( 99.20)	Acc@5 100.00 ( 99.99)
Epoch: [64][390/391]	Time  0.026 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.3342e-03 (2.3145e-02)	Acc@1 100.00 ( 99.21)	Acc@5 100.00 ( 99.99)
## e[64] optimizer.zero_grad (sum) time: 0.10957121849060059
## e[64]       loss.backward (sum) time: 2.1911094188690186
## e[64]      optimizer.step (sum) time: 0.8628358840942383
## epoch[64] training(only) time: 10.439183473587036
# Switched to evaluate mode...
Test: [  0/100]	Time  0.164 ( 0.164)	Loss 2.0562e-01 (2.0562e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.020 ( 0.028)	Loss 4.1228e-01 (3.5730e-01)	Acc@1  91.00 ( 91.82)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.016 ( 0.021)	Loss 3.9870e-01 (3.7657e-01)	Acc@1  92.00 ( 91.33)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.011 ( 0.019)	Loss 3.4753e-01 (3.9772e-01)	Acc@1  89.00 ( 90.68)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.011 ( 0.018)	Loss 6.0255e-01 (4.1882e-01)	Acc@1  89.00 ( 90.41)	Acc@5  99.00 ( 99.61)
Test: [ 50/100]	Time  0.011 ( 0.017)	Loss 1.8031e-01 (4.1229e-01)	Acc@1  97.00 ( 90.61)	Acc@5 100.00 ( 99.57)
Test: [ 60/100]	Time  0.012 ( 0.017)	Loss 4.2963e-01 (4.0974e-01)	Acc@1  90.00 ( 90.79)	Acc@5 100.00 ( 99.57)
Test: [ 70/100]	Time  0.011 ( 0.016)	Loss 7.9498e-01 (4.0995e-01)	Acc@1  90.00 ( 90.79)	Acc@5 100.00 ( 99.58)
Test: [ 80/100]	Time  0.026 ( 0.016)	Loss 3.9007e-01 (4.0831e-01)	Acc@1  93.00 ( 90.81)	Acc@5 100.00 ( 99.63)
Test: [ 90/100]	Time  0.015 ( 0.016)	Loss 2.4664e-01 (4.0450e-01)	Acc@1  95.00 ( 90.87)	Acc@5 100.00 ( 99.66)
 * Acc@1 90.950 Acc@5 99.680
### epoch[64] execution time: 12.142599821090698
EPOCH 65
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [65][  0/391]	Time  0.199 ( 0.199)	Data  0.174 ( 0.174)	Loss 3.2056e-03 (3.2056e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [65][ 10/391]	Time  0.024 ( 0.042)	Data  0.001 ( 0.017)	Loss 5.6631e-02 (1.9531e-02)	Acc@1  98.44 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [65][ 20/391]	Time  0.032 ( 0.034)	Data  0.004 ( 0.010)	Loss 3.1917e-02 (1.7722e-02)	Acc@1  98.44 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [65][ 30/391]	Time  0.023 ( 0.031)	Data  0.001 ( 0.007)	Loss 4.9120e-03 (1.7332e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [65][ 40/391]	Time  0.024 ( 0.030)	Data  0.001 ( 0.006)	Loss 4.8057e-02 (1.8577e-02)	Acc@1  97.66 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [65][ 50/391]	Time  0.023 ( 0.029)	Data  0.001 ( 0.005)	Loss 4.5190e-03 (1.7966e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [65][ 60/391]	Time  0.023 ( 0.029)	Data  0.001 ( 0.005)	Loss 5.3109e-03 (1.7870e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [65][ 70/391]	Time  0.037 ( 0.028)	Data  0.004 ( 0.005)	Loss 3.1514e-03 (1.7387e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [65][ 80/391]	Time  0.023 ( 0.028)	Data  0.002 ( 0.004)	Loss 1.8599e-02 (1.8362e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [65][ 90/391]	Time  0.029 ( 0.028)	Data  0.001 ( 0.004)	Loss 4.2955e-02 (1.8684e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [65][100/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.1359e-03 (2.0012e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [65][110/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.4951e-02 (1.9526e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [65][120/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.1416e-02 (1.9958e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [65][130/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 6.4163e-03 (2.0097e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [65][140/391]	Time  0.026 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.2757e-02 (2.0369e-02)	Acc@1  97.66 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [65][150/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.0846e-02 (2.1072e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [65][160/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.3671e-02 (2.1268e-02)	Acc@1  98.44 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [65][170/391]	Time  0.036 ( 0.027)	Data  0.003 ( 0.003)	Loss 8.9371e-03 (2.1317e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [65][180/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.2677e-03 (2.0777e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [65][190/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 3.0580e-03 (2.0634e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [65][200/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.8781e-02 (2.0553e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [65][210/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.4622e-02 (2.1025e-02)	Acc@1  98.44 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [65][220/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.7357e-02 (2.1109e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [65][230/391]	Time  0.030 ( 0.027)	Data  0.005 ( 0.003)	Loss 1.5820e-02 (2.0792e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [65][240/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.003)	Loss 8.4371e-03 (2.0618e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [65][250/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.6986e-02 (2.0612e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [65][260/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.5458e-03 (2.0795e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [65][270/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1787e-02 (2.0698e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [65][280/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.3413e-02 (2.1057e-02)	Acc@1  98.44 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [65][290/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.1456e-02 (2.0893e-02)	Acc@1  97.66 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [65][300/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.9041e-03 (2.0742e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [65][310/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.0836e-03 (2.0821e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [65][320/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.8022e-03 (2.1363e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [65][330/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3421e-02 (2.1406e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [65][340/391]	Time  0.035 ( 0.026)	Data  0.000 ( 0.003)	Loss 9.6019e-03 (2.1667e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [65][350/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.5926e-02 (2.1807e-02)	Acc@1  98.44 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [65][360/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2267e-02 (2.1926e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [65][370/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.9898e-03 (2.1939e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [65][380/391]	Time  0.035 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9130e-03 (2.1884e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [65][390/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.8526e-02 (2.2015e-02)	Acc@1  98.75 ( 99.26)	Acc@5 100.00 (100.00)
## e[65] optimizer.zero_grad (sum) time: 0.1080317497253418
## e[65]       loss.backward (sum) time: 2.2503862380981445
## e[65]      optimizer.step (sum) time: 0.8686611652374268
## epoch[65] training(only) time: 10.396812200546265
# Switched to evaluate mode...
Test: [  0/100]	Time  0.208 ( 0.208)	Loss 1.9666e-01 (1.9666e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.016 ( 0.031)	Loss 3.8939e-01 (3.6042e-01)	Acc@1  91.00 ( 91.73)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.013 ( 0.023)	Loss 3.8435e-01 (3.8064e-01)	Acc@1  91.00 ( 91.29)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.011 ( 0.020)	Loss 3.6227e-01 (4.0167e-01)	Acc@1  89.00 ( 90.77)	Acc@5  99.00 ( 99.55)
Test: [ 40/100]	Time  0.011 ( 0.019)	Loss 6.2071e-01 (4.2294e-01)	Acc@1  89.00 ( 90.54)	Acc@5  99.00 ( 99.56)
Test: [ 50/100]	Time  0.012 ( 0.018)	Loss 1.7325e-01 (4.1663e-01)	Acc@1  97.00 ( 90.65)	Acc@5 100.00 ( 99.57)
Test: [ 60/100]	Time  0.022 ( 0.018)	Loss 4.3717e-01 (4.1411e-01)	Acc@1  90.00 ( 90.80)	Acc@5 100.00 ( 99.57)
Test: [ 70/100]	Time  0.025 ( 0.018)	Loss 7.8406e-01 (4.1313e-01)	Acc@1  90.00 ( 90.82)	Acc@5 100.00 ( 99.55)
Test: [ 80/100]	Time  0.011 ( 0.017)	Loss 4.1851e-01 (4.1217e-01)	Acc@1  93.00 ( 90.84)	Acc@5 100.00 ( 99.58)
Test: [ 90/100]	Time  0.013 ( 0.017)	Loss 2.6813e-01 (4.0836e-01)	Acc@1  94.00 ( 90.85)	Acc@5 100.00 ( 99.62)
 * Acc@1 90.940 Acc@5 99.630
### epoch[65] execution time: 12.167316436767578
EPOCH 66
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [66][  0/391]	Time  0.196 ( 0.196)	Data  0.176 ( 0.176)	Loss 5.7684e-02 (5.7684e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [66][ 10/391]	Time  0.025 ( 0.041)	Data  0.006 ( 0.018)	Loss 1.1180e-02 (1.5866e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [66][ 20/391]	Time  0.023 ( 0.034)	Data  0.001 ( 0.010)	Loss 4.8837e-02 (1.5982e-02)	Acc@1  97.66 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [66][ 30/391]	Time  0.023 ( 0.031)	Data  0.001 ( 0.008)	Loss 8.7789e-03 (1.8838e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [66][ 40/391]	Time  0.024 ( 0.030)	Data  0.001 ( 0.007)	Loss 1.5807e-03 (1.9372e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [66][ 50/391]	Time  0.024 ( 0.029)	Data  0.001 ( 0.006)	Loss 1.2199e-02 (2.1296e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [66][ 60/391]	Time  0.029 ( 0.029)	Data  0.000 ( 0.005)	Loss 2.2482e-02 (2.2447e-02)	Acc@1  99.22 ( 99.24)	Acc@5 100.00 ( 99.99)
Epoch: [66][ 70/391]	Time  0.022 ( 0.028)	Data  0.001 ( 0.005)	Loss 7.7117e-03 (2.1662e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 ( 99.99)
Epoch: [66][ 80/391]	Time  0.030 ( 0.028)	Data  0.001 ( 0.004)	Loss 2.1927e-02 (2.1431e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 ( 99.99)
Epoch: [66][ 90/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.004)	Loss 5.4533e-02 (2.2228e-02)	Acc@1  98.44 ( 99.27)	Acc@5 100.00 ( 99.99)
Epoch: [66][100/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.2142e-02 (2.2795e-02)	Acc@1  99.22 ( 99.24)	Acc@5 100.00 ( 99.99)
Epoch: [66][110/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 4.8472e-03 (2.2524e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 ( 99.99)
Epoch: [66][120/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.7344e-02 (2.2002e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 ( 99.99)
Epoch: [66][130/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.9713e-02 (2.1996e-02)	Acc@1  98.44 ( 99.27)	Acc@5 100.00 ( 99.99)
Epoch: [66][140/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.9053e-02 (2.2200e-02)	Acc@1  98.44 ( 99.26)	Acc@5 100.00 ( 99.99)
Epoch: [66][150/391]	Time  0.031 ( 0.027)	Data  0.003 ( 0.003)	Loss 1.4007e-02 (2.2642e-02)	Acc@1  99.22 ( 99.24)	Acc@5 100.00 ( 99.99)
Epoch: [66][160/391]	Time  0.034 ( 0.027)	Data  0.002 ( 0.003)	Loss 9.1376e-03 (2.2715e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [66][170/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.2254e-02 (2.2293e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [66][180/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.0300e-02 (2.2243e-02)	Acc@1  98.44 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [66][190/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.2163e-03 (2.2139e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [66][200/391]	Time  0.028 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.4101e-03 (2.1928e-02)	Acc@1 100.00 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [66][210/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.0996e-02 (2.2201e-02)	Acc@1  98.44 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [66][220/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.9543e-02 (2.2460e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [66][230/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.6343e-03 (2.1947e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [66][240/391]	Time  0.028 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.4572e-02 (2.2067e-02)	Acc@1  99.22 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [66][250/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4356e-02 (2.1929e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [66][260/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.6535e-03 (2.1602e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [66][270/391]	Time  0.032 ( 0.026)	Data  0.003 ( 0.003)	Loss 7.3396e-03 (2.1543e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 ( 99.99)
Epoch: [66][280/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.1272e-03 (2.1796e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 ( 99.99)
Epoch: [66][290/391]	Time  0.022 ( 0.026)	Data  0.004 ( 0.003)	Loss 6.4495e-03 (2.1790e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 ( 99.99)
Epoch: [66][300/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.3481e-03 (2.1627e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 ( 99.99)
Epoch: [66][310/391]	Time  0.032 ( 0.026)	Data  0.003 ( 0.003)	Loss 4.1358e-02 (2.1519e-02)	Acc@1  96.88 ( 99.27)	Acc@5 100.00 ( 99.99)
Epoch: [66][320/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.0867e-03 (2.1327e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [66][330/391]	Time  0.031 ( 0.026)	Data  0.002 ( 0.003)	Loss 4.2337e-02 (2.1335e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [66][340/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4654e-02 (2.1184e-02)	Acc@1  98.44 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [66][350/391]	Time  0.024 ( 0.026)	Data  0.003 ( 0.003)	Loss 1.3027e-02 (2.1395e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [66][360/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.1111e-03 (2.1131e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [66][370/391]	Time  0.030 ( 0.026)	Data  0.006 ( 0.003)	Loss 4.7942e-02 (2.1572e-02)	Acc@1  98.44 ( 99.28)	Acc@5 100.00 ( 99.99)
Epoch: [66][380/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6351e-02 (2.1654e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 ( 99.99)
Epoch: [66][390/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.4446e-02 (2.1410e-02)	Acc@1  97.50 ( 99.28)	Acc@5 100.00 ( 99.99)
## e[66] optimizer.zero_grad (sum) time: 0.108154296875
## e[66]       loss.backward (sum) time: 2.2626497745513916
## e[66]      optimizer.step (sum) time: 0.8812618255615234
## epoch[66] training(only) time: 10.412221908569336
# Switched to evaluate mode...
Test: [  0/100]	Time  0.164 ( 0.164)	Loss 1.9241e-01 (1.9241e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.021 ( 0.028)	Loss 3.6286e-01 (3.6137e-01)	Acc@1  91.00 ( 91.45)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.016 ( 0.021)	Loss 3.8488e-01 (3.8278e-01)	Acc@1  87.00 ( 90.81)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.017 ( 0.019)	Loss 3.5799e-01 (4.0204e-01)	Acc@1  90.00 ( 90.48)	Acc@5  99.00 ( 99.55)
Test: [ 40/100]	Time  0.015 ( 0.018)	Loss 5.6889e-01 (4.1922e-01)	Acc@1  88.00 ( 90.29)	Acc@5  99.00 ( 99.56)
Test: [ 50/100]	Time  0.012 ( 0.017)	Loss 1.9145e-01 (4.1410e-01)	Acc@1  95.00 ( 90.39)	Acc@5 100.00 ( 99.59)
Test: [ 60/100]	Time  0.011 ( 0.016)	Loss 4.5819e-01 (4.1291e-01)	Acc@1  91.00 ( 90.59)	Acc@5 100.00 ( 99.59)
Test: [ 70/100]	Time  0.014 ( 0.016)	Loss 7.9956e-01 (4.1224e-01)	Acc@1  90.00 ( 90.63)	Acc@5 100.00 ( 99.56)
Test: [ 80/100]	Time  0.011 ( 0.016)	Loss 4.0360e-01 (4.1122e-01)	Acc@1  93.00 ( 90.69)	Acc@5 100.00 ( 99.60)
Test: [ 90/100]	Time  0.014 ( 0.016)	Loss 2.6632e-01 (4.0668e-01)	Acc@1  95.00 ( 90.76)	Acc@5 100.00 ( 99.64)
 * Acc@1 90.850 Acc@5 99.650
### epoch[66] execution time: 12.113974332809448
EPOCH 67
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [67][  0/391]	Time  0.186 ( 0.186)	Data  0.168 ( 0.168)	Loss 4.6003e-02 (4.6003e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [67][ 10/391]	Time  0.026 ( 0.042)	Data  0.004 ( 0.018)	Loss 2.2584e-02 (2.4564e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [67][ 20/391]	Time  0.027 ( 0.034)	Data  0.001 ( 0.010)	Loss 1.4735e-02 (2.4544e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [67][ 30/391]	Time  0.023 ( 0.031)	Data  0.001 ( 0.008)	Loss 1.7107e-02 (2.2105e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [67][ 40/391]	Time  0.023 ( 0.030)	Data  0.001 ( 0.006)	Loss 3.9239e-03 (2.1498e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [67][ 50/391]	Time  0.025 ( 0.029)	Data  0.001 ( 0.006)	Loss 3.5522e-02 (2.1417e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [67][ 60/391]	Time  0.023 ( 0.029)	Data  0.001 ( 0.005)	Loss 8.6947e-03 (2.1153e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [67][ 70/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.9714e-02 (2.0635e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [67][ 80/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 3.8153e-02 (2.0364e-02)	Acc@1  97.66 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [67][ 90/391]	Time  0.031 ( 0.028)	Data  0.003 ( 0.004)	Loss 1.7115e-02 (1.9553e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [67][100/391]	Time  0.026 ( 0.027)	Data  0.002 ( 0.004)	Loss 1.9082e-03 (1.9139e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [67][110/391]	Time  0.026 ( 0.027)	Data  0.001 ( 0.004)	Loss 7.2358e-03 (2.0277e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [67][120/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 8.5019e-03 (1.9675e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [67][130/391]	Time  0.025 ( 0.027)	Data  0.005 ( 0.003)	Loss 4.1798e-02 (1.9543e-02)	Acc@1  98.44 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [67][140/391]	Time  0.026 ( 0.027)	Data  0.003 ( 0.003)	Loss 5.7102e-03 (1.9207e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [67][150/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.1675e-03 (1.8960e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [67][160/391]	Time  0.032 ( 0.027)	Data  0.000 ( 0.003)	Loss 2.2393e-02 (1.9216e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [67][170/391]	Time  0.026 ( 0.027)	Data  0.004 ( 0.003)	Loss 6.2722e-02 (1.9791e-02)	Acc@1  98.44 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [67][180/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.2403e-02 (2.0594e-02)	Acc@1  98.44 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [67][190/391]	Time  0.028 ( 0.027)	Data  0.003 ( 0.003)	Loss 1.3977e-02 (2.0976e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [67][200/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.003)	Loss 8.3369e-03 (2.0452e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [67][210/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.4793e-02 (2.0319e-02)	Acc@1  97.66 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [67][220/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.3766e-03 (2.0229e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [67][230/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.7273e-03 (2.0751e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [67][240/391]	Time  0.026 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.8243e-03 (2.0566e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [67][250/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 7.6059e-03 (2.0668e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [67][260/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3393e-02 (2.0522e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [67][270/391]	Time  0.023 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.4773e-02 (2.0408e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [67][280/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.2459e-03 (2.0590e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [67][290/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4843e-02 (2.0645e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [67][300/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6269e-02 (2.0636e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [67][310/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5974e-02 (2.0493e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [67][320/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8069e-03 (2.0630e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [67][330/391]	Time  0.023 ( 0.026)	Data  0.000 ( 0.003)	Loss 5.3058e-03 (2.0840e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [67][340/391]	Time  0.030 ( 0.026)	Data  0.010 ( 0.003)	Loss 1.0069e-02 (2.0844e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [67][350/391]	Time  0.026 ( 0.026)	Data  0.005 ( 0.003)	Loss 4.2709e-02 (2.0844e-02)	Acc@1  98.44 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [67][360/391]	Time  0.023 ( 0.026)	Data  0.003 ( 0.003)	Loss 4.7521e-03 (2.1050e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [67][370/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7872e-02 (2.0885e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [67][380/391]	Time  0.033 ( 0.026)	Data  0.000 ( 0.003)	Loss 6.3750e-03 (2.0904e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [67][390/391]	Time  0.024 ( 0.026)	Data  0.000 ( 0.003)	Loss 3.5734e-02 (2.1050e-02)	Acc@1  98.75 ( 99.28)	Acc@5 100.00 (100.00)
## e[67] optimizer.zero_grad (sum) time: 0.10870099067687988
## e[67]       loss.backward (sum) time: 2.241569995880127
## e[67]      optimizer.step (sum) time: 0.876166820526123
## epoch[67] training(only) time: 10.46543550491333
# Switched to evaluate mode...
Test: [  0/100]	Time  0.162 ( 0.162)	Loss 1.9675e-01 (1.9675e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.015 ( 0.028)	Loss 3.8885e-01 (3.6948e-01)	Acc@1  91.00 ( 91.18)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.026 ( 0.022)	Loss 3.9248e-01 (3.9089e-01)	Acc@1  89.00 ( 90.86)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.028 ( 0.019)	Loss 3.4585e-01 (4.1175e-01)	Acc@1  90.00 ( 90.52)	Acc@5 100.00 ( 99.61)
Test: [ 40/100]	Time  0.013 ( 0.018)	Loss 5.8146e-01 (4.3092e-01)	Acc@1  88.00 ( 90.34)	Acc@5  99.00 ( 99.61)
Test: [ 50/100]	Time  0.017 ( 0.017)	Loss 1.7970e-01 (4.2337e-01)	Acc@1  96.00 ( 90.65)	Acc@5 100.00 ( 99.61)
Test: [ 60/100]	Time  0.011 ( 0.017)	Loss 4.4413e-01 (4.2073e-01)	Acc@1  90.00 ( 90.79)	Acc@5 100.00 ( 99.61)
Test: [ 70/100]	Time  0.011 ( 0.017)	Loss 8.0583e-01 (4.2068e-01)	Acc@1  90.00 ( 90.79)	Acc@5 100.00 ( 99.58)
Test: [ 80/100]	Time  0.025 ( 0.017)	Loss 4.2257e-01 (4.1916e-01)	Acc@1  92.00 ( 90.79)	Acc@5 100.00 ( 99.62)
Test: [ 90/100]	Time  0.011 ( 0.016)	Loss 2.7718e-01 (4.1569e-01)	Acc@1  95.00 ( 90.84)	Acc@5 100.00 ( 99.65)
 * Acc@1 90.890 Acc@5 99.660
### epoch[67] execution time: 12.215210914611816
EPOCH 68
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [68][  0/391]	Time  0.197 ( 0.197)	Data  0.173 ( 0.173)	Loss 6.4680e-03 (6.4680e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [68][ 10/391]	Time  0.025 ( 0.041)	Data  0.004 ( 0.018)	Loss 3.3468e-03 (1.9533e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [68][ 20/391]	Time  0.027 ( 0.034)	Data  0.002 ( 0.010)	Loss 1.9915e-02 (2.1979e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [68][ 30/391]	Time  0.021 ( 0.031)	Data  0.001 ( 0.008)	Loss 7.2552e-03 (2.3129e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [68][ 40/391]	Time  0.024 ( 0.030)	Data  0.002 ( 0.006)	Loss 4.5104e-03 (2.0506e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [68][ 50/391]	Time  0.027 ( 0.029)	Data  0.001 ( 0.005)	Loss 2.2911e-02 (2.0937e-02)	Acc@1  98.44 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [68][ 60/391]	Time  0.023 ( 0.029)	Data  0.001 ( 0.005)	Loss 8.8453e-03 (2.1081e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [68][ 70/391]	Time  0.023 ( 0.028)	Data  0.002 ( 0.004)	Loss 2.2575e-02 (2.0693e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [68][ 80/391]	Time  0.023 ( 0.028)	Data  0.003 ( 0.004)	Loss 8.7796e-03 (2.1478e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [68][ 90/391]	Time  0.031 ( 0.028)	Data  0.000 ( 0.004)	Loss 1.5326e-02 (2.1183e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [68][100/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.7205e-02 (2.0968e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [68][110/391]	Time  0.026 ( 0.027)	Data  0.002 ( 0.004)	Loss 1.2760e-02 (2.1332e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [68][120/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 5.1484e-03 (2.0659e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [68][130/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 3.6899e-03 (2.0987e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [68][140/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.2947e-02 (2.0976e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [68][150/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.6037e-02 (2.0909e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [68][160/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.9891e-02 (2.1398e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [68][170/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.4228e-02 (2.1113e-02)	Acc@1  97.66 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [68][180/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.5157e-03 (2.0816e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [68][190/391]	Time  0.030 ( 0.027)	Data  0.002 ( 0.003)	Loss 3.9193e-02 (2.0644e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [68][200/391]	Time  0.026 ( 0.027)	Data  0.004 ( 0.003)	Loss 1.1564e-02 (2.0780e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [68][210/391]	Time  0.028 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.0671e-02 (2.1185e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [68][220/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.4955e-02 (2.1458e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [68][230/391]	Time  0.034 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.4244e-02 (2.1442e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [68][240/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.6541e-03 (2.1362e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [68][250/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.5751e-03 (2.1165e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [68][260/391]	Time  0.031 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.9237e-02 (2.1382e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [68][270/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.8078e-02 (2.1471e-02)	Acc@1  98.44 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [68][280/391]	Time  0.033 ( 0.026)	Data  0.003 ( 0.003)	Loss 1.1979e-02 (2.1585e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [68][290/391]	Time  0.025 ( 0.026)	Data  0.005 ( 0.003)	Loss 5.9593e-03 (2.1659e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [68][300/391]	Time  0.029 ( 0.026)	Data  0.005 ( 0.003)	Loss 5.8338e-02 (2.1608e-02)	Acc@1  98.44 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [68][310/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.6904e-03 (2.1654e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 ( 99.99)
Epoch: [68][320/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.1246e-03 (2.1386e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [68][330/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1206e-02 (2.1329e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [68][340/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 4.4536e-03 (2.1228e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [68][350/391]	Time  0.044 ( 0.026)	Data  0.022 ( 0.003)	Loss 1.7095e-02 (2.0911e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [68][360/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.0936e-03 (2.0903e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [68][370/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.2587e-03 (2.0882e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [68][380/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.9144e-03 (2.0875e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [68][390/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.2009e-02 (2.1007e-02)	Acc@1  98.75 ( 99.32)	Acc@5 100.00 (100.00)
## e[68] optimizer.zero_grad (sum) time: 0.10897636413574219
## e[68]       loss.backward (sum) time: 2.255713939666748
## e[68]      optimizer.step (sum) time: 0.8846673965454102
## epoch[68] training(only) time: 10.419318914413452
# Switched to evaluate mode...
Test: [  0/100]	Time  0.164 ( 0.164)	Loss 2.2215e-01 (2.2215e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.028)	Loss 3.9741e-01 (3.6689e-01)	Acc@1  91.00 ( 91.64)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.020 ( 0.021)	Loss 3.8777e-01 (3.8651e-01)	Acc@1  89.00 ( 91.10)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.014 ( 0.019)	Loss 3.4515e-01 (4.0673e-01)	Acc@1  88.00 ( 90.48)	Acc@5 100.00 ( 99.55)
Test: [ 40/100]	Time  0.021 ( 0.018)	Loss 6.0757e-01 (4.2690e-01)	Acc@1  88.00 ( 90.24)	Acc@5  99.00 ( 99.56)
Test: [ 50/100]	Time  0.021 ( 0.018)	Loss 2.0502e-01 (4.2107e-01)	Acc@1  95.00 ( 90.35)	Acc@5 100.00 ( 99.57)
Test: [ 60/100]	Time  0.011 ( 0.017)	Loss 4.1782e-01 (4.1765e-01)	Acc@1  92.00 ( 90.56)	Acc@5 100.00 ( 99.57)
Test: [ 70/100]	Time  0.020 ( 0.017)	Loss 8.1732e-01 (4.1812e-01)	Acc@1  88.00 ( 90.58)	Acc@5 100.00 ( 99.55)
Test: [ 80/100]	Time  0.012 ( 0.016)	Loss 3.9937e-01 (4.1661e-01)	Acc@1  92.00 ( 90.56)	Acc@5 100.00 ( 99.59)
Test: [ 90/100]	Time  0.012 ( 0.016)	Loss 2.7296e-01 (4.1234e-01)	Acc@1  94.00 ( 90.64)	Acc@5 100.00 ( 99.63)
 * Acc@1 90.740 Acc@5 99.650
### epoch[68] execution time: 12.158941984176636
EPOCH 69
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [69][  0/391]	Time  0.194 ( 0.194)	Data  0.173 ( 0.173)	Loss 6.9333e-03 (6.9333e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [69][ 10/391]	Time  0.025 ( 0.041)	Data  0.001 ( 0.017)	Loss 1.2351e-02 (1.1316e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [69][ 20/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.010)	Loss 2.3919e-02 (1.6879e-02)	Acc@1  98.44 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [69][ 30/391]	Time  0.023 ( 0.031)	Data  0.001 ( 0.007)	Loss 3.6515e-02 (1.6409e-02)	Acc@1  98.44 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [69][ 40/391]	Time  0.022 ( 0.030)	Data  0.001 ( 0.006)	Loss 4.1821e-02 (1.7384e-02)	Acc@1  98.44 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [69][ 50/391]	Time  0.027 ( 0.029)	Data  0.002 ( 0.005)	Loss 3.5857e-03 (1.8555e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [69][ 60/391]	Time  0.023 ( 0.028)	Data  0.002 ( 0.005)	Loss 7.8207e-03 (1.7790e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [69][ 70/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.005)	Loss 5.9317e-03 (1.8039e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [69][ 80/391]	Time  0.025 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.6074e-02 (1.7898e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [69][ 90/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.2423e-02 (1.7085e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [69][100/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.1081e-02 (1.6901e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [69][110/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.4831e-03 (1.6842e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [69][120/391]	Time  0.033 ( 0.027)	Data  0.004 ( 0.004)	Loss 2.2945e-02 (1.6867e-02)	Acc@1  98.44 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [69][130/391]	Time  0.024 ( 0.027)	Data  0.003 ( 0.004)	Loss 8.1381e-03 (1.6947e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [69][140/391]	Time  0.026 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.0933e-02 (1.7175e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [69][150/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.9985e-03 (1.7525e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [69][160/391]	Time  0.023 ( 0.027)	Data  0.003 ( 0.003)	Loss 7.6113e-03 (1.7453e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [69][170/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.5520e-02 (1.7154e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [69][180/391]	Time  0.026 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.5391e-02 (1.6998e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [69][190/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.1273e-02 (1.7174e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [69][200/391]	Time  0.026 ( 0.027)	Data  0.000 ( 0.003)	Loss 1.7572e-02 (1.7352e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [69][210/391]	Time  0.023 ( 0.027)	Data  0.005 ( 0.003)	Loss 2.7538e-03 (1.7388e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [69][220/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 1.4420e-02 (1.7532e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [69][230/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.3105e-02 (1.8084e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [69][240/391]	Time  0.034 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.1273e-02 (1.8323e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 ( 99.99)
Epoch: [69][250/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.1787e-02 (1.8397e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 ( 99.99)
Epoch: [69][260/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.3437e-03 (1.8137e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 ( 99.99)
Epoch: [69][270/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.1611e-02 (1.8121e-02)	Acc@1  98.44 ( 99.39)	Acc@5 100.00 ( 99.99)
Epoch: [69][280/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.1360e-02 (1.8054e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 ( 99.99)
Epoch: [69][290/391]	Time  0.028 ( 0.026)	Data  0.009 ( 0.003)	Loss 4.1029e-03 (1.8043e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 ( 99.99)
Epoch: [69][300/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1606e-03 (1.7845e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 ( 99.99)
Epoch: [69][310/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.9536e-03 (1.8061e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 ( 99.99)
Epoch: [69][320/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.5797e-03 (1.7996e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [69][330/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.4078e-03 (1.8136e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [69][340/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 4.7386e-03 (1.8666e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [69][350/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.5617e-02 (1.8724e-02)	Acc@1  98.44 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [69][360/391]	Time  0.026 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.3903e-03 (1.8779e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [69][370/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8243e-02 (1.9056e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [69][380/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0411e-02 (1.9041e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [69][390/391]	Time  0.024 ( 0.026)	Data  0.000 ( 0.003)	Loss 8.9892e-03 (1.9014e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
## e[69] optimizer.zero_grad (sum) time: 0.10911941528320312
## e[69]       loss.backward (sum) time: 2.2305541038513184
## e[69]      optimizer.step (sum) time: 0.8823270797729492
## epoch[69] training(only) time: 10.420028924942017
# Switched to evaluate mode...
Test: [  0/100]	Time  0.165 ( 0.165)	Loss 2.1764e-01 (2.1764e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.015 ( 0.027)	Loss 4.1650e-01 (3.7330e-01)	Acc@1  91.00 ( 91.82)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.018 ( 0.021)	Loss 4.1828e-01 (3.9228e-01)	Acc@1  89.00 ( 91.10)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.021 ( 0.019)	Loss 3.4194e-01 (4.0892e-01)	Acc@1  91.00 ( 90.61)	Acc@5 100.00 ( 99.61)
Test: [ 40/100]	Time  0.016 ( 0.018)	Loss 6.3099e-01 (4.3025e-01)	Acc@1  88.00 ( 90.39)	Acc@5  99.00 ( 99.61)
Test: [ 50/100]	Time  0.011 ( 0.017)	Loss 2.0921e-01 (4.2344e-01)	Acc@1  95.00 ( 90.37)	Acc@5 100.00 ( 99.59)
Test: [ 60/100]	Time  0.011 ( 0.016)	Loss 4.2311e-01 (4.2025e-01)	Acc@1  91.00 ( 90.64)	Acc@5 100.00 ( 99.59)
Test: [ 70/100]	Time  0.017 ( 0.016)	Loss 8.2198e-01 (4.2140e-01)	Acc@1  90.00 ( 90.69)	Acc@5 100.00 ( 99.59)
Test: [ 80/100]	Time  0.011 ( 0.016)	Loss 3.8103e-01 (4.1966e-01)	Acc@1  92.00 ( 90.69)	Acc@5 100.00 ( 99.64)
Test: [ 90/100]	Time  0.011 ( 0.016)	Loss 2.6222e-01 (4.1624e-01)	Acc@1  94.00 ( 90.69)	Acc@5 100.00 ( 99.67)
 * Acc@1 90.800 Acc@5 99.680
### epoch[69] execution time: 12.10088324546814
EPOCH 70
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [70][  0/391]	Time  0.195 ( 0.195)	Data  0.172 ( 0.172)	Loss 9.4725e-03 (9.4725e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [70][ 10/391]	Time  0.025 ( 0.042)	Data  0.000 ( 0.018)	Loss 4.0101e-03 (2.7162e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 ( 99.86)
Epoch: [70][ 20/391]	Time  0.025 ( 0.034)	Data  0.001 ( 0.010)	Loss 5.9469e-03 (2.1693e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 ( 99.93)
Epoch: [70][ 30/391]	Time  0.029 ( 0.031)	Data  0.001 ( 0.008)	Loss 1.3710e-02 (2.2057e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 ( 99.95)
Epoch: [70][ 40/391]	Time  0.027 ( 0.030)	Data  0.001 ( 0.006)	Loss 1.2152e-02 (1.9978e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 ( 99.96)
Epoch: [70][ 50/391]	Time  0.023 ( 0.029)	Data  0.001 ( 0.005)	Loss 3.2058e-02 (1.9820e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 ( 99.97)
Epoch: [70][ 60/391]	Time  0.026 ( 0.028)	Data  0.004 ( 0.005)	Loss 2.3264e-02 (2.0057e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 ( 99.97)
Epoch: [70][ 70/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.004)	Loss 3.4164e-03 (1.9531e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 ( 99.98)
Epoch: [70][ 80/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 4.0055e-02 (2.0750e-02)	Acc@1  97.66 ( 99.33)	Acc@5 100.00 ( 99.98)
Epoch: [70][ 90/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.0910e-01 (2.1431e-02)	Acc@1  97.66 ( 99.32)	Acc@5 100.00 ( 99.98)
Epoch: [70][100/391]	Time  0.026 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.9718e-02 (2.0219e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 ( 99.98)
Epoch: [70][110/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.1562e-02 (2.0904e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 ( 99.99)
Epoch: [70][120/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.2597e-02 (2.0670e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 ( 99.99)
Epoch: [70][130/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.7063e-02 (2.1377e-02)	Acc@1  98.44 ( 99.31)	Acc@5 100.00 ( 99.98)
Epoch: [70][140/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.9517e-02 (2.1236e-02)	Acc@1  97.66 ( 99.31)	Acc@5 100.00 ( 99.98)
Epoch: [70][150/391]	Time  0.024 ( 0.027)	Data  0.000 ( 0.003)	Loss 3.5288e-03 (2.1148e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 ( 99.98)
Epoch: [70][160/391]	Time  0.030 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.2541e-03 (2.0900e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 ( 99.99)
Epoch: [70][170/391]	Time  0.028 ( 0.027)	Data  0.003 ( 0.003)	Loss 2.6755e-02 (2.1070e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 ( 99.99)
Epoch: [70][180/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.0575e-02 (2.0816e-02)	Acc@1  98.44 ( 99.34)	Acc@5 100.00 ( 99.99)
Epoch: [70][190/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 4.6231e-03 (2.0441e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 ( 99.99)
Epoch: [70][200/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 9.6679e-03 (2.0060e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 ( 99.99)
Epoch: [70][210/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.5283e-03 (2.0016e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 ( 99.99)
Epoch: [70][220/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.7965e-03 (1.9779e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 ( 99.99)
Epoch: [70][230/391]	Time  0.027 ( 0.027)	Data  0.002 ( 0.003)	Loss 1.8088e-02 (2.0038e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 ( 99.99)
Epoch: [70][240/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.8201e-03 (1.9633e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 ( 99.99)
Epoch: [70][250/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.6758e-02 (1.9859e-02)	Acc@1  98.44 ( 99.36)	Acc@5 100.00 ( 99.99)
Epoch: [70][260/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.5827e-02 (1.9961e-02)	Acc@1  98.44 ( 99.35)	Acc@5 100.00 ( 99.99)
Epoch: [70][270/391]	Time  0.039 ( 0.027)	Data  0.000 ( 0.003)	Loss 9.9920e-03 (2.0058e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 ( 99.99)
Epoch: [70][280/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.6027e-03 (1.9692e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 ( 99.99)
Epoch: [70][290/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.2042e-02 (1.9624e-02)	Acc@1  98.44 ( 99.36)	Acc@5 100.00 ( 99.99)
Epoch: [70][300/391]	Time  0.025 ( 0.026)	Data  0.000 ( 0.003)	Loss 5.5878e-03 (1.9506e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 ( 99.99)
Epoch: [70][310/391]	Time  0.026 ( 0.026)	Data  0.005 ( 0.003)	Loss 3.0009e-02 (1.9423e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 ( 99.99)
Epoch: [70][320/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8925e-02 (1.9501e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 ( 99.99)
Epoch: [70][330/391]	Time  0.031 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.5577e-02 (1.9807e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 ( 99.99)
Epoch: [70][340/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7571e-02 (1.9657e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 ( 99.99)
Epoch: [70][350/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3789e-02 (1.9654e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 ( 99.99)
Epoch: [70][360/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2484e-02 (1.9770e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 ( 99.99)
Epoch: [70][370/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.0071e-02 (1.9643e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 ( 99.99)
Epoch: [70][380/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3427e-02 (1.9686e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 ( 99.99)
Epoch: [70][390/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.5088e-02 (2.0006e-02)	Acc@1  97.50 ( 99.35)	Acc@5 100.00 ( 99.99)
## e[70] optimizer.zero_grad (sum) time: 0.10935378074645996
## e[70]       loss.backward (sum) time: 2.242891550064087
## e[70]      optimizer.step (sum) time: 0.8706231117248535
## epoch[70] training(only) time: 10.440991878509521
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 2.0800e-01 (2.0800e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.011 ( 0.027)	Loss 3.7204e-01 (3.7816e-01)	Acc@1  91.00 ( 91.18)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.011 ( 0.021)	Loss 4.3990e-01 (3.9830e-01)	Acc@1  89.00 ( 90.81)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.013 ( 0.019)	Loss 3.4953e-01 (4.1577e-01)	Acc@1  91.00 ( 90.32)	Acc@5 100.00 ( 99.61)
Test: [ 40/100]	Time  0.011 ( 0.018)	Loss 6.3619e-01 (4.3533e-01)	Acc@1  88.00 ( 90.27)	Acc@5  99.00 ( 99.59)
Test: [ 50/100]	Time  0.011 ( 0.017)	Loss 1.8085e-01 (4.2810e-01)	Acc@1  95.00 ( 90.39)	Acc@5 100.00 ( 99.57)
Test: [ 60/100]	Time  0.011 ( 0.017)	Loss 4.5324e-01 (4.2472e-01)	Acc@1  90.00 ( 90.66)	Acc@5 100.00 ( 99.57)
Test: [ 70/100]	Time  0.021 ( 0.017)	Loss 8.4230e-01 (4.2589e-01)	Acc@1  89.00 ( 90.65)	Acc@5 100.00 ( 99.58)
Test: [ 80/100]	Time  0.011 ( 0.016)	Loss 4.0676e-01 (4.2372e-01)	Acc@1  92.00 ( 90.69)	Acc@5 100.00 ( 99.62)
Test: [ 90/100]	Time  0.016 ( 0.016)	Loss 2.5095e-01 (4.2019e-01)	Acc@1  95.00 ( 90.67)	Acc@5 100.00 ( 99.65)
 * Acc@1 90.800 Acc@5 99.650
### epoch[70] execution time: 12.180791139602661
EPOCH 71
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [71][  0/391]	Time  0.189 ( 0.189)	Data  0.169 ( 0.169)	Loss 3.4183e-02 (3.4183e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [71][ 10/391]	Time  0.029 ( 0.041)	Data  0.001 ( 0.017)	Loss 1.0109e-02 (1.8825e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [71][ 20/391]	Time  0.024 ( 0.033)	Data  0.001 ( 0.010)	Loss 3.2636e-02 (1.8193e-02)	Acc@1  98.44 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [71][ 30/391]	Time  0.023 ( 0.031)	Data  0.001 ( 0.007)	Loss 3.9274e-02 (2.0640e-02)	Acc@1  98.44 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [71][ 40/391]	Time  0.024 ( 0.029)	Data  0.001 ( 0.006)	Loss 1.8903e-02 (1.9862e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [71][ 50/391]	Time  0.025 ( 0.029)	Data  0.001 ( 0.005)	Loss 6.1397e-03 (2.1589e-02)	Acc@1 100.00 ( 99.23)	Acc@5 100.00 ( 99.98)
Epoch: [71][ 60/391]	Time  0.026 ( 0.028)	Data  0.001 ( 0.005)	Loss 3.6795e-02 (2.1131e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 ( 99.99)
Epoch: [71][ 70/391]	Time  0.027 ( 0.028)	Data  0.005 ( 0.004)	Loss 7.2766e-03 (2.1387e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 ( 99.99)
Epoch: [71][ 80/391]	Time  0.030 ( 0.028)	Data  0.002 ( 0.004)	Loss 5.9217e-03 (2.0412e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 ( 99.99)
Epoch: [71][ 90/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.004)	Loss 5.5913e-03 (2.0010e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 ( 99.99)
Epoch: [71][100/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 9.6614e-03 (2.0156e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 ( 99.98)
Epoch: [71][110/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.3915e-02 (2.0743e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 ( 99.99)
Epoch: [71][120/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.7090e-03 (2.0537e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 ( 99.99)
Epoch: [71][130/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.1517e-02 (2.0618e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 ( 99.99)
Epoch: [71][140/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.7294e-02 (2.0927e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 ( 99.99)
Epoch: [71][150/391]	Time  0.028 ( 0.027)	Data  0.000 ( 0.003)	Loss 6.6968e-03 (2.0840e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 ( 99.99)
Epoch: [71][160/391]	Time  0.028 ( 0.027)	Data  0.004 ( 0.003)	Loss 2.6402e-02 (2.1110e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 ( 99.99)
Epoch: [71][170/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.2832e-02 (2.1657e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 ( 99.99)
Epoch: [71][180/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.2189e-02 (2.1183e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 ( 99.99)
Epoch: [71][190/391]	Time  0.036 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.1970e-02 (2.2071e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 ( 99.99)
Epoch: [71][200/391]	Time  0.037 ( 0.027)	Data  0.005 ( 0.003)	Loss 1.7651e-02 (2.1927e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 ( 99.99)
Epoch: [71][210/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2650e-02 (2.1692e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 ( 99.99)
Epoch: [71][220/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0845e-02 (2.1656e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 ( 99.99)
Epoch: [71][230/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 6.0610e-03 (2.1086e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 ( 99.99)
Epoch: [71][240/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.1529e-03 (2.1152e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 ( 99.99)
Epoch: [71][250/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.4766e-03 (2.1460e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 ( 99.99)
Epoch: [71][260/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.4010e-03 (2.1049e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 ( 99.99)
Epoch: [71][270/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0382e-02 (2.0869e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 ( 99.99)
Epoch: [71][280/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.1963e-02 (2.0846e-02)	Acc@1  97.66 ( 99.36)	Acc@5 100.00 ( 99.99)
Epoch: [71][290/391]	Time  0.022 ( 0.026)	Data  0.002 ( 0.003)	Loss 8.9920e-03 (2.0832e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 ( 99.99)
Epoch: [71][300/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1579e-03 (2.0558e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 ( 99.99)
Epoch: [71][310/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.2042e-02 (2.0469e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 ( 99.99)
Epoch: [71][320/391]	Time  0.032 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.1330e-02 (2.0345e-02)	Acc@1  98.44 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [71][330/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.8279e-02 (2.0565e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [71][340/391]	Time  0.031 ( 0.026)	Data  0.000 ( 0.003)	Loss 3.6239e-02 (2.0759e-02)	Acc@1  98.44 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [71][350/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.2565e-02 (2.0802e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [71][360/391]	Time  0.023 ( 0.026)	Data  0.000 ( 0.003)	Loss 8.2134e-03 (2.0522e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [71][370/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.1692e-03 (2.0536e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [71][380/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3721e-02 (2.0686e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [71][390/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.8147e-02 (2.0662e-02)	Acc@1  97.50 ( 99.35)	Acc@5 100.00 (100.00)
## e[71] optimizer.zero_grad (sum) time: 0.10915160179138184
## e[71]       loss.backward (sum) time: 2.2473156452178955
## e[71]      optimizer.step (sum) time: 0.8807528018951416
## epoch[71] training(only) time: 10.416701316833496
# Switched to evaluate mode...
Test: [  0/100]	Time  0.166 ( 0.166)	Loss 2.0464e-01 (2.0464e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.015 ( 0.028)	Loss 3.7792e-01 (3.7411e-01)	Acc@1  92.00 ( 91.09)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.019 ( 0.022)	Loss 4.3710e-01 (3.9720e-01)	Acc@1  89.00 ( 90.71)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.012 ( 0.019)	Loss 3.3481e-01 (4.1399e-01)	Acc@1  91.00 ( 90.32)	Acc@5 100.00 ( 99.58)
Test: [ 40/100]	Time  0.027 ( 0.018)	Loss 6.0054e-01 (4.3147e-01)	Acc@1  88.00 ( 90.22)	Acc@5  99.00 ( 99.59)
Test: [ 50/100]	Time  0.011 ( 0.018)	Loss 1.7562e-01 (4.2481e-01)	Acc@1  95.00 ( 90.45)	Acc@5 100.00 ( 99.59)
Test: [ 60/100]	Time  0.011 ( 0.017)	Loss 4.2111e-01 (4.2220e-01)	Acc@1  90.00 ( 90.64)	Acc@5 100.00 ( 99.59)
Test: [ 70/100]	Time  0.011 ( 0.017)	Loss 8.2430e-01 (4.2207e-01)	Acc@1  90.00 ( 90.73)	Acc@5 100.00 ( 99.56)
Test: [ 80/100]	Time  0.012 ( 0.016)	Loss 3.9442e-01 (4.1968e-01)	Acc@1  92.00 ( 90.75)	Acc@5 100.00 ( 99.60)
Test: [ 90/100]	Time  0.026 ( 0.016)	Loss 2.7432e-01 (4.1600e-01)	Acc@1  95.00 ( 90.80)	Acc@5 100.00 ( 99.64)
 * Acc@1 90.910 Acc@5 99.660
### epoch[71] execution time: 12.169759750366211
EPOCH 72
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [72][  0/391]	Time  0.188 ( 0.188)	Data  0.171 ( 0.171)	Loss 1.1225e-02 (1.1225e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [72][ 10/391]	Time  0.024 ( 0.042)	Data  0.001 ( 0.017)	Loss 7.3063e-03 (1.7615e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [72][ 20/391]	Time  0.023 ( 0.034)	Data  0.001 ( 0.010)	Loss 3.0833e-03 (2.2017e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [72][ 30/391]	Time  0.023 ( 0.031)	Data  0.001 ( 0.008)	Loss 3.6339e-02 (2.2256e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [72][ 40/391]	Time  0.025 ( 0.030)	Data  0.001 ( 0.007)	Loss 9.7810e-03 (2.2640e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [72][ 50/391]	Time  0.025 ( 0.029)	Data  0.001 ( 0.006)	Loss 1.2613e-02 (2.2154e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [72][ 60/391]	Time  0.028 ( 0.029)	Data  0.005 ( 0.005)	Loss 1.3148e-02 (2.0032e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [72][ 70/391]	Time  0.036 ( 0.028)	Data  0.002 ( 0.005)	Loss 1.7803e-02 (1.9654e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [72][ 80/391]	Time  0.027 ( 0.028)	Data  0.002 ( 0.004)	Loss 5.4132e-02 (1.9713e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [72][ 90/391]	Time  0.022 ( 0.028)	Data  0.002 ( 0.004)	Loss 1.3603e-02 (1.8850e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [72][100/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 4.5160e-02 (1.8799e-02)	Acc@1  97.66 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [72][110/391]	Time  0.032 ( 0.027)	Data  0.005 ( 0.004)	Loss 2.7513e-03 (1.9036e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 ( 99.99)
Epoch: [72][120/391]	Time  0.031 ( 0.027)	Data  0.003 ( 0.004)	Loss 8.4606e-03 (1.9319e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 ( 99.99)
Epoch: [72][130/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.3352e-02 (1.9561e-02)	Acc@1  98.44 ( 99.36)	Acc@5 100.00 ( 99.99)
Epoch: [72][140/391]	Time  0.031 ( 0.027)	Data  0.003 ( 0.004)	Loss 6.5233e-03 (1.9094e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 ( 99.99)
Epoch: [72][150/391]	Time  0.037 ( 0.027)	Data  0.000 ( 0.004)	Loss 2.5145e-02 (1.9163e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 ( 99.99)
Epoch: [72][160/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.0133e-02 (1.8691e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [72][170/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.0100e-03 (1.8870e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [72][180/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 1.3953e-02 (1.8263e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [72][190/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.7512e-02 (1.8373e-02)	Acc@1  98.44 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [72][200/391]	Time  0.022 ( 0.027)	Data  0.000 ( 0.003)	Loss 3.7643e-02 (1.8756e-02)	Acc@1  98.44 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [72][210/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.7795e-02 (1.8885e-02)	Acc@1  97.66 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [72][220/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.0746e-02 (1.8871e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [72][230/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.4913e-03 (1.9611e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [72][240/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.7597e-02 (1.9519e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [72][250/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.2665e-03 (1.9702e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [72][260/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.9817e-02 (2.0225e-02)	Acc@1  96.88 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [72][270/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.3353e-02 (2.0386e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [72][280/391]	Time  0.024 ( 0.027)	Data  0.003 ( 0.003)	Loss 4.6575e-02 (2.0298e-02)	Acc@1  97.66 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [72][290/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.6197e-03 (2.0360e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [72][300/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0332e-02 (2.0274e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [72][310/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7459e-02 (2.0388e-02)	Acc@1  98.44 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [72][320/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.0113e-03 (2.0397e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [72][330/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.2289e-03 (2.0133e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [72][340/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3398e-02 (2.0081e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [72][350/391]	Time  0.034 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.7468e-03 (1.9995e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [72][360/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.9920e-03 (1.9705e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [72][370/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.4393e-03 (1.9574e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [72][380/391]	Time  0.023 ( 0.026)	Data  0.004 ( 0.003)	Loss 5.0714e-03 (1.9666e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [72][390/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5000e-02 (1.9490e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
## e[72] optimizer.zero_grad (sum) time: 0.10988283157348633
## e[72]       loss.backward (sum) time: 2.2134523391723633
## e[72]      optimizer.step (sum) time: 0.8632586002349854
## epoch[72] training(only) time: 10.462169170379639
# Switched to evaluate mode...
Test: [  0/100]	Time  0.162 ( 0.162)	Loss 2.2125e-01 (2.2125e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.011 ( 0.029)	Loss 4.0586e-01 (3.7308e-01)	Acc@1  91.00 ( 91.64)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.014 ( 0.022)	Loss 4.3067e-01 (3.9477e-01)	Acc@1  91.00 ( 91.14)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.013 ( 0.020)	Loss 3.2219e-01 (4.0948e-01)	Acc@1  89.00 ( 90.52)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.011 ( 0.018)	Loss 5.9092e-01 (4.2867e-01)	Acc@1  88.00 ( 90.24)	Acc@5  99.00 ( 99.61)
Test: [ 50/100]	Time  0.015 ( 0.017)	Loss 2.0577e-01 (4.2213e-01)	Acc@1  94.00 ( 90.43)	Acc@5 100.00 ( 99.59)
Test: [ 60/100]	Time  0.014 ( 0.017)	Loss 4.3039e-01 (4.1921e-01)	Acc@1  91.00 ( 90.66)	Acc@5 100.00 ( 99.59)
Test: [ 70/100]	Time  0.016 ( 0.016)	Loss 8.4123e-01 (4.1984e-01)	Acc@1  91.00 ( 90.73)	Acc@5 100.00 ( 99.58)
Test: [ 80/100]	Time  0.010 ( 0.016)	Loss 3.8467e-01 (4.1820e-01)	Acc@1  92.00 ( 90.74)	Acc@5 100.00 ( 99.62)
Test: [ 90/100]	Time  0.011 ( 0.016)	Loss 2.7193e-01 (4.1489e-01)	Acc@1  94.00 ( 90.81)	Acc@5 100.00 ( 99.65)
 * Acc@1 90.920 Acc@5 99.660
### epoch[72] execution time: 12.145797729492188
EPOCH 73
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [73][  0/391]	Time  0.191 ( 0.191)	Data  0.173 ( 0.173)	Loss 9.5603e-03 (9.5603e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [73][ 10/391]	Time  0.028 ( 0.042)	Data  0.000 ( 0.018)	Loss 1.8604e-02 (1.5333e-02)	Acc@1  98.44 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [73][ 20/391]	Time  0.023 ( 0.034)	Data  0.001 ( 0.010)	Loss 1.0759e-02 (2.1095e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [73][ 30/391]	Time  0.023 ( 0.031)	Data  0.002 ( 0.008)	Loss 2.1269e-02 (1.9135e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [73][ 40/391]	Time  0.023 ( 0.030)	Data  0.001 ( 0.006)	Loss 1.2108e-03 (2.0801e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [73][ 50/391]	Time  0.024 ( 0.029)	Data  0.001 ( 0.005)	Loss 5.0736e-02 (1.9874e-02)	Acc@1  97.66 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [73][ 60/391]	Time  0.024 ( 0.029)	Data  0.001 ( 0.005)	Loss 1.9677e-02 (1.9034e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [73][ 70/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.4741e-02 (1.8914e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [73][ 80/391]	Time  0.024 ( 0.028)	Data  0.004 ( 0.004)	Loss 1.0812e-02 (1.8298e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [73][ 90/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 7.4276e-03 (1.9309e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 ( 99.99)
Epoch: [73][100/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.7211e-02 (1.9020e-02)	Acc@1  96.88 ( 99.38)	Acc@5 100.00 ( 99.99)
Epoch: [73][110/391]	Time  0.028 ( 0.027)	Data  0.001 ( 0.004)	Loss 6.6569e-03 (1.8683e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 ( 99.99)
Epoch: [73][120/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.3595e-03 (1.8934e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 ( 99.99)
Epoch: [73][130/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.3283e-02 (1.8776e-02)	Acc@1  98.44 ( 99.39)	Acc@5 100.00 ( 99.99)
Epoch: [73][140/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.8295e-03 (1.9143e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 ( 99.99)
Epoch: [73][150/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 9.8746e-03 (1.8834e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 ( 99.99)
Epoch: [73][160/391]	Time  0.026 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.9557e-03 (1.8829e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [73][170/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 5.0430e-03 (1.8476e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [73][180/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.3304e-02 (1.7954e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [73][190/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.7168e-02 (1.8099e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [73][200/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.1879e-03 (1.8475e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [73][210/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.4210e-02 (1.8292e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [73][220/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.3765e-03 (1.8501e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [73][230/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.1206e-03 (1.8994e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [73][240/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.5795e-03 (1.8667e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [73][250/391]	Time  0.031 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.6635e-03 (1.8555e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [73][260/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5320e-03 (1.8269e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [73][270/391]	Time  0.022 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.0676e-02 (1.8478e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [73][280/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.2619e-03 (1.8714e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [73][290/391]	Time  0.036 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.5529e-02 (1.8961e-02)	Acc@1  96.88 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [73][300/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.8603e-03 (1.8895e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [73][310/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.6817e-03 (1.8641e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [73][320/391]	Time  0.034 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0140e-02 (1.8729e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [73][330/391]	Time  0.034 ( 0.026)	Data  0.005 ( 0.003)	Loss 7.2981e-03 (1.8608e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [73][340/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0508e-02 (1.8645e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [73][350/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.9791e-02 (1.8557e-02)	Acc@1  96.88 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [73][360/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0760e-02 (1.8796e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [73][370/391]	Time  0.036 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.6162e-03 (1.8993e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [73][380/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0189e-03 (1.8785e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [73][390/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.0589e-02 (1.8913e-02)	Acc@1  98.75 ( 99.39)	Acc@5 100.00 (100.00)
## e[73] optimizer.zero_grad (sum) time: 0.1091160774230957
## e[73]       loss.backward (sum) time: 2.2239418029785156
## e[73]      optimizer.step (sum) time: 0.8691356182098389
## epoch[73] training(only) time: 10.478304624557495
# Switched to evaluate mode...
Test: [  0/100]	Time  0.165 ( 0.165)	Loss 2.0539e-01 (2.0539e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.020 ( 0.028)	Loss 3.7689e-01 (3.7816e-01)	Acc@1  92.00 ( 91.45)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.020 ( 0.021)	Loss 4.0267e-01 (3.9773e-01)	Acc@1  91.00 ( 90.86)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.012 ( 0.019)	Loss 3.5005e-01 (4.1513e-01)	Acc@1  90.00 ( 90.39)	Acc@5 100.00 ( 99.61)
Test: [ 40/100]	Time  0.012 ( 0.018)	Loss 6.2059e-01 (4.3602e-01)	Acc@1  88.00 ( 90.24)	Acc@5  99.00 ( 99.54)
Test: [ 50/100]	Time  0.016 ( 0.017)	Loss 1.9217e-01 (4.2953e-01)	Acc@1  95.00 ( 90.37)	Acc@5 100.00 ( 99.53)
Test: [ 60/100]	Time  0.012 ( 0.016)	Loss 4.4864e-01 (4.2724e-01)	Acc@1  90.00 ( 90.59)	Acc@5 100.00 ( 99.54)
Test: [ 70/100]	Time  0.011 ( 0.016)	Loss 8.4200e-01 (4.2643e-01)	Acc@1  90.00 ( 90.62)	Acc@5  99.00 ( 99.52)
Test: [ 80/100]	Time  0.015 ( 0.016)	Loss 4.1196e-01 (4.2561e-01)	Acc@1  93.00 ( 90.59)	Acc@5 100.00 ( 99.56)
Test: [ 90/100]	Time  0.026 ( 0.016)	Loss 2.6318e-01 (4.2162e-01)	Acc@1  94.00 ( 90.59)	Acc@5 100.00 ( 99.59)
 * Acc@1 90.720 Acc@5 99.600
### epoch[73] execution time: 12.170282125473022
EPOCH 74
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [74][  0/391]	Time  0.203 ( 0.203)	Data  0.182 ( 0.182)	Loss 3.3124e-02 (3.3124e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [74][ 10/391]	Time  0.028 ( 0.042)	Data  0.006 ( 0.019)	Loss 1.1899e-02 (1.5499e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [74][ 20/391]	Time  0.022 ( 0.034)	Data  0.001 ( 0.011)	Loss 8.5779e-03 (1.3654e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [74][ 30/391]	Time  0.025 ( 0.032)	Data  0.001 ( 0.008)	Loss 1.8916e-02 (1.4379e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [74][ 40/391]	Time  0.025 ( 0.030)	Data  0.001 ( 0.006)	Loss 2.0967e-02 (1.6859e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [74][ 50/391]	Time  0.024 ( 0.029)	Data  0.001 ( 0.005)	Loss 2.3118e-02 (1.7453e-02)	Acc@1  98.44 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [74][ 60/391]	Time  0.023 ( 0.029)	Data  0.001 ( 0.005)	Loss 1.6998e-02 (1.8337e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [74][ 70/391]	Time  0.026 ( 0.028)	Data  0.001 ( 0.005)	Loss 3.3491e-02 (1.8356e-02)	Acc@1  98.44 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [74][ 80/391]	Time  0.032 ( 0.028)	Data  0.001 ( 0.004)	Loss 2.6407e-02 (1.9153e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [74][ 90/391]	Time  0.025 ( 0.028)	Data  0.001 ( 0.004)	Loss 8.0580e-03 (1.8511e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [74][100/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.004)	Loss 2.9926e-02 (1.9561e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [74][110/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.5031e-02 (1.9729e-02)	Acc@1  98.44 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [74][120/391]	Time  0.033 ( 0.027)	Data  0.004 ( 0.004)	Loss 8.8360e-03 (1.9796e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [74][130/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.2693e-02 (2.0062e-02)	Acc@1  98.44 ( 99.34)	Acc@5 100.00 ( 99.99)
Epoch: [74][140/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.2633e-03 (2.0340e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 ( 99.99)
Epoch: [74][150/391]	Time  0.026 ( 0.027)	Data  0.003 ( 0.003)	Loss 2.6575e-02 (1.9793e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 ( 99.99)
Epoch: [74][160/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.2686e-02 (1.9221e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [74][170/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.7942e-03 (2.0063e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [74][180/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.0904e-03 (1.9903e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [74][190/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.2105e-02 (2.0140e-02)	Acc@1  97.66 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [74][200/391]	Time  0.032 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.4358e-03 (1.9863e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [74][210/391]	Time  0.026 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.0292e-03 (1.9633e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [74][220/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.9480e-02 (2.0229e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [74][230/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.9875e-03 (2.0367e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [74][240/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.6087e-03 (2.0180e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [74][250/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.6541e-02 (2.0049e-02)	Acc@1  97.66 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [74][260/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.8591e-02 (2.0199e-02)	Acc@1  98.44 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [74][270/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4777e-02 (1.9862e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [74][280/391]	Time  0.026 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.7453e-02 (1.9897e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [74][290/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1940e-02 (1.9645e-02)	Acc@1  98.44 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [74][300/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.9242e-03 (1.9478e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [74][310/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.9876e-02 (1.9675e-02)	Acc@1  97.66 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [74][320/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.6864e-03 (1.9734e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [74][330/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5754e-02 (1.9689e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [74][340/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 8.4703e-02 (2.0167e-02)	Acc@1  98.44 ( 99.34)	Acc@5  99.22 (100.00)
Epoch: [74][350/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.4070e-02 (2.0199e-02)	Acc@1  97.66 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [74][360/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.8400e-03 (2.0021e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 ( 99.99)
Epoch: [74][370/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.2577e-03 (1.9860e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 ( 99.99)
Epoch: [74][380/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.2343e-02 (1.9946e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 ( 99.99)
Epoch: [74][390/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.6879e-03 (1.9787e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 ( 99.99)
## e[74] optimizer.zero_grad (sum) time: 0.10961174964904785
## e[74]       loss.backward (sum) time: 2.2543060779571533
## e[74]      optimizer.step (sum) time: 0.8768761157989502
## epoch[74] training(only) time: 10.419621706008911
# Switched to evaluate mode...
Test: [  0/100]	Time  0.160 ( 0.160)	Loss 2.2514e-01 (2.2514e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.012 ( 0.027)	Loss 4.1885e-01 (3.8964e-01)	Acc@1  92.00 ( 91.45)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.016 ( 0.022)	Loss 4.4411e-01 (4.0759e-01)	Acc@1  90.00 ( 91.10)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.015 ( 0.019)	Loss 3.2475e-01 (4.1990e-01)	Acc@1  90.00 ( 90.61)	Acc@5 100.00 ( 99.58)
Test: [ 40/100]	Time  0.014 ( 0.018)	Loss 6.3826e-01 (4.4055e-01)	Acc@1  88.00 ( 90.41)	Acc@5  99.00 ( 99.56)
Test: [ 50/100]	Time  0.014 ( 0.017)	Loss 1.8103e-01 (4.3325e-01)	Acc@1  96.00 ( 90.55)	Acc@5  99.00 ( 99.51)
Test: [ 60/100]	Time  0.012 ( 0.016)	Loss 4.5600e-01 (4.2992e-01)	Acc@1  91.00 ( 90.75)	Acc@5 100.00 ( 99.52)
Test: [ 70/100]	Time  0.016 ( 0.016)	Loss 8.5917e-01 (4.3081e-01)	Acc@1  89.00 ( 90.80)	Acc@5  99.00 ( 99.49)
Test: [ 80/100]	Time  0.025 ( 0.016)	Loss 4.0898e-01 (4.2852e-01)	Acc@1  92.00 ( 90.79)	Acc@5 100.00 ( 99.56)
Test: [ 90/100]	Time  0.015 ( 0.016)	Loss 2.4985e-01 (4.2462e-01)	Acc@1  95.00 ( 90.78)	Acc@5 100.00 ( 99.59)
 * Acc@1 90.910 Acc@5 99.610
### epoch[74] execution time: 12.076223611831665
EPOCH 75
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [75][  0/391]	Time  0.188 ( 0.188)	Data  0.170 ( 0.170)	Loss 1.4132e-02 (1.4132e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [75][ 10/391]	Time  0.030 ( 0.041)	Data  0.001 ( 0.018)	Loss 1.4488e-02 (1.4892e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [75][ 20/391]	Time  0.025 ( 0.034)	Data  0.001 ( 0.010)	Loss 3.0350e-03 (1.9836e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [75][ 30/391]	Time  0.027 ( 0.031)	Data  0.001 ( 0.007)	Loss 3.1255e-03 (1.9717e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [75][ 40/391]	Time  0.023 ( 0.030)	Data  0.001 ( 0.006)	Loss 9.4536e-04 (1.6631e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [75][ 50/391]	Time  0.029 ( 0.029)	Data  0.001 ( 0.005)	Loss 1.8288e-02 (1.5492e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [75][ 60/391]	Time  0.023 ( 0.028)	Data  0.003 ( 0.005)	Loss 2.0652e-02 (1.4968e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [75][ 70/391]	Time  0.026 ( 0.028)	Data  0.005 ( 0.005)	Loss 3.6892e-03 (1.4824e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [75][ 80/391]	Time  0.028 ( 0.028)	Data  0.001 ( 0.004)	Loss 6.4749e-03 (1.4827e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [75][ 90/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.8962e-03 (1.5129e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 ( 99.99)
Epoch: [75][100/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 9.6721e-03 (1.5331e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 ( 99.99)
Epoch: [75][110/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.004)	Loss 3.1954e-02 (1.6901e-02)	Acc@1  98.44 ( 99.49)	Acc@5 100.00 ( 99.99)
Epoch: [75][120/391]	Time  0.035 ( 0.027)	Data  0.003 ( 0.004)	Loss 1.7741e-02 (1.7537e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [75][130/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.7630e-02 (1.8932e-02)	Acc@1  98.44 ( 99.42)	Acc@5 100.00 ( 99.99)
Epoch: [75][140/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.6084e-02 (1.9470e-02)	Acc@1  98.44 ( 99.40)	Acc@5 100.00 ( 99.99)
Epoch: [75][150/391]	Time  0.030 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.6412e-02 (1.9306e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 ( 99.99)
Epoch: [75][160/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.8539e-02 (1.9761e-02)	Acc@1  96.88 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [75][170/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 8.0255e-02 (1.9879e-02)	Acc@1  97.66 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [75][180/391]	Time  0.030 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.9780e-03 (1.9378e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [75][190/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 8.3023e-03 (1.8857e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [75][200/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2797e-02 (1.8542e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [75][210/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.0312e-03 (1.8574e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [75][220/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.4904e-03 (1.8257e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [75][230/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.7287e-02 (1.8299e-02)	Acc@1  98.44 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [75][240/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4226e-02 (1.8621e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [75][250/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.6774e-03 (1.8610e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [75][260/391]	Time  0.039 ( 0.026)	Data  0.004 ( 0.003)	Loss 3.1292e-02 (1.8416e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [75][270/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5450e-02 (1.8489e-02)	Acc@1  98.44 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [75][280/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.8393e-03 (1.8579e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [75][290/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.2453e-02 (1.8443e-02)	Acc@1  98.44 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [75][300/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.9733e-03 (1.8559e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [75][310/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.7645e-03 (1.8458e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [75][320/391]	Time  0.032 ( 0.026)	Data  0.000 ( 0.003)	Loss 6.3063e-03 (1.8184e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [75][330/391]	Time  0.024 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.7976e-02 (1.8024e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [75][340/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.2079e-03 (1.7983e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [75][350/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2722e-02 (1.7978e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [75][360/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.0993e-02 (1.7914e-02)	Acc@1  98.44 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [75][370/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.8361e-03 (1.7869e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [75][380/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.3407e-03 (1.7885e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [75][390/391]	Time  0.024 ( 0.026)	Data  0.000 ( 0.003)	Loss 8.9609e-03 (1.7844e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
## e[75] optimizer.zero_grad (sum) time: 0.10880589485168457
## e[75]       loss.backward (sum) time: 2.2454519271850586
## e[75]      optimizer.step (sum) time: 0.8640785217285156
## epoch[75] training(only) time: 10.402992248535156
# Switched to evaluate mode...
Test: [  0/100]	Time  0.162 ( 0.162)	Loss 2.2149e-01 (2.2149e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.011 ( 0.028)	Loss 4.0572e-01 (3.8444e-01)	Acc@1  92.00 ( 91.27)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.010 ( 0.021)	Loss 4.2489e-01 (4.0684e-01)	Acc@1  90.00 ( 91.05)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.011 ( 0.019)	Loss 3.2070e-01 (4.2195e-01)	Acc@1  89.00 ( 90.52)	Acc@5 100.00 ( 99.61)
Test: [ 40/100]	Time  0.022 ( 0.018)	Loss 6.1722e-01 (4.4251e-01)	Acc@1  88.00 ( 90.34)	Acc@5  99.00 ( 99.56)
Test: [ 50/100]	Time  0.011 ( 0.017)	Loss 1.9563e-01 (4.3535e-01)	Acc@1  95.00 ( 90.49)	Acc@5  99.00 ( 99.55)
Test: [ 60/100]	Time  0.023 ( 0.017)	Loss 4.4682e-01 (4.3308e-01)	Acc@1  89.00 ( 90.66)	Acc@5 100.00 ( 99.56)
Test: [ 70/100]	Time  0.011 ( 0.017)	Loss 8.6773e-01 (4.3356e-01)	Acc@1  89.00 ( 90.68)	Acc@5  99.00 ( 99.52)
Test: [ 80/100]	Time  0.011 ( 0.016)	Loss 4.0391e-01 (4.3169e-01)	Acc@1  92.00 ( 90.73)	Acc@5 100.00 ( 99.56)
Test: [ 90/100]	Time  0.011 ( 0.016)	Loss 2.4724e-01 (4.2701e-01)	Acc@1  95.00 ( 90.73)	Acc@5 100.00 ( 99.59)
 * Acc@1 90.840 Acc@5 99.600
### epoch[75] execution time: 12.137546062469482
EPOCH 76
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [76][  0/391]	Time  0.236 ( 0.236)	Data  0.214 ( 0.214)	Loss 2.1883e-02 (2.1883e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [76][ 10/391]	Time  0.024 ( 0.045)	Data  0.001 ( 0.021)	Loss 1.3257e-03 (2.1429e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [76][ 20/391]	Time  0.026 ( 0.036)	Data  0.001 ( 0.012)	Loss 1.1665e-02 (1.7312e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [76][ 30/391]	Time  0.029 ( 0.033)	Data  0.001 ( 0.009)	Loss 2.3615e-03 (1.6473e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [76][ 40/391]	Time  0.031 ( 0.031)	Data  0.001 ( 0.007)	Loss 2.0080e-02 (1.5268e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [76][ 50/391]	Time  0.022 ( 0.030)	Data  0.001 ( 0.006)	Loss 3.0414e-03 (1.5091e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [76][ 60/391]	Time  0.024 ( 0.030)	Data  0.001 ( 0.006)	Loss 1.7189e-02 (1.5401e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [76][ 70/391]	Time  0.024 ( 0.029)	Data  0.001 ( 0.005)	Loss 1.3815e-02 (1.5548e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [76][ 80/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.005)	Loss 4.0669e-03 (1.5953e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [76][ 90/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.004)	Loss 5.0346e-03 (1.5844e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [76][100/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.7960e-03 (1.5468e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [76][110/391]	Time  0.025 ( 0.028)	Data  0.001 ( 0.004)	Loss 2.8314e-02 (1.6078e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [76][120/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.004)	Loss 3.1825e-02 (1.7624e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 ( 99.99)
Epoch: [76][130/391]	Time  0.029 ( 0.027)	Data  0.009 ( 0.004)	Loss 8.8727e-03 (1.7321e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 ( 99.99)
Epoch: [76][140/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 4.0555e-03 (1.6746e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [76][150/391]	Time  0.028 ( 0.027)	Data  0.008 ( 0.004)	Loss 3.1747e-02 (1.6447e-02)	Acc@1  98.44 ( 99.46)	Acc@5 100.00 ( 99.99)
Epoch: [76][160/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.9667e-03 (1.6520e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [76][170/391]	Time  0.024 ( 0.027)	Data  0.002 ( 0.003)	Loss 3.1957e-02 (1.6404e-02)	Acc@1  98.44 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [76][180/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.1420e-03 (1.6095e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [76][190/391]	Time  0.024 ( 0.027)	Data  0.000 ( 0.003)	Loss 4.4875e-03 (1.6531e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [76][200/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.0071e-02 (1.6595e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 ( 99.99)
Epoch: [76][210/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.5818e-02 (1.6601e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 ( 99.99)
Epoch: [76][220/391]	Time  0.032 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.9378e-02 (1.7404e-02)	Acc@1  98.44 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [76][230/391]	Time  0.030 ( 0.027)	Data  0.010 ( 0.003)	Loss 9.6697e-03 (1.7406e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [76][240/391]	Time  0.022 ( 0.027)	Data  0.000 ( 0.003)	Loss 1.1889e-02 (1.7160e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 ( 99.99)
Epoch: [76][250/391]	Time  0.030 ( 0.027)	Data  0.005 ( 0.003)	Loss 4.3509e-02 (1.7266e-02)	Acc@1  98.44 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [76][260/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.6089e-03 (1.7071e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [76][270/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.5032e-03 (1.7078e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [76][280/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.1126e-03 (1.6906e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 ( 99.99)
Epoch: [76][290/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.4498e-02 (1.6925e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 ( 99.99)
Epoch: [76][300/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.7532e-03 (1.6874e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 ( 99.99)
Epoch: [76][310/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.2285e-02 (1.6874e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 ( 99.99)
Epoch: [76][320/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.1344e-03 (1.6780e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 ( 99.99)
Epoch: [76][330/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 3.9944e-03 (1.6952e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 ( 99.99)
Epoch: [76][340/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.1559e-03 (1.6748e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 ( 99.99)
Epoch: [76][350/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.9462e-02 (1.7299e-02)	Acc@1  97.66 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [76][360/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9382e-02 (1.7240e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [76][370/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.1950e-02 (1.7257e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [76][380/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1465e-02 (1.7117e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [76][390/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.2447e-03 (1.7134e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 ( 99.99)
## e[76] optimizer.zero_grad (sum) time: 0.10960531234741211
## e[76]       loss.backward (sum) time: 2.2588179111480713
## e[76]      optimizer.step (sum) time: 0.8806116580963135
## epoch[76] training(only) time: 10.446426153182983
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 2.0508e-01 (2.0508e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.023 ( 0.028)	Loss 4.0915e-01 (3.6747e-01)	Acc@1  90.00 ( 91.73)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.011 ( 0.021)	Loss 4.2226e-01 (3.9842e-01)	Acc@1  87.00 ( 90.95)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.012 ( 0.019)	Loss 3.5587e-01 (4.1641e-01)	Acc@1  90.00 ( 90.52)	Acc@5 100.00 ( 99.61)
Test: [ 40/100]	Time  0.011 ( 0.018)	Loss 6.2084e-01 (4.3706e-01)	Acc@1  88.00 ( 90.34)	Acc@5  99.00 ( 99.59)
Test: [ 50/100]	Time  0.011 ( 0.017)	Loss 2.1214e-01 (4.2923e-01)	Acc@1  96.00 ( 90.51)	Acc@5 100.00 ( 99.57)
Test: [ 60/100]	Time  0.018 ( 0.017)	Loss 4.5491e-01 (4.2673e-01)	Acc@1  90.00 ( 90.72)	Acc@5 100.00 ( 99.57)
Test: [ 70/100]	Time  0.011 ( 0.016)	Loss 8.3635e-01 (4.2602e-01)	Acc@1  89.00 ( 90.75)	Acc@5  99.00 ( 99.55)
Test: [ 80/100]	Time  0.011 ( 0.016)	Loss 3.7888e-01 (4.2434e-01)	Acc@1  93.00 ( 90.73)	Acc@5 100.00 ( 99.59)
Test: [ 90/100]	Time  0.012 ( 0.016)	Loss 2.7332e-01 (4.1986e-01)	Acc@1  95.00 ( 90.77)	Acc@5 100.00 ( 99.63)
 * Acc@1 90.850 Acc@5 99.630
### epoch[76] execution time: 12.178679466247559
EPOCH 77
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [77][  0/391]	Time  0.203 ( 0.203)	Data  0.183 ( 0.183)	Loss 4.4135e-02 (4.4135e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [77][ 10/391]	Time  0.027 ( 0.042)	Data  0.001 ( 0.018)	Loss 8.1561e-03 (1.4360e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [77][ 20/391]	Time  0.023 ( 0.034)	Data  0.001 ( 0.010)	Loss 7.5435e-02 (1.4940e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [77][ 30/391]	Time  0.032 ( 0.032)	Data  0.001 ( 0.008)	Loss 4.0213e-02 (1.7190e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [77][ 40/391]	Time  0.023 ( 0.030)	Data  0.001 ( 0.006)	Loss 5.3871e-02 (1.7071e-02)	Acc@1  97.66 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [77][ 50/391]	Time  0.029 ( 0.029)	Data  0.001 ( 0.006)	Loss 3.6761e-03 (1.7796e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [77][ 60/391]	Time  0.033 ( 0.029)	Data  0.001 ( 0.005)	Loss 2.3644e-02 (1.8465e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [77][ 70/391]	Time  0.027 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.0518e-02 (2.0175e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [77][ 80/391]	Time  0.022 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.4302e-03 (1.9099e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [77][ 90/391]	Time  0.027 ( 0.028)	Data  0.002 ( 0.004)	Loss 7.3503e-03 (1.8778e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [77][100/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 2.7913e-02 (1.8578e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [77][110/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 5.5365e-02 (1.8159e-02)	Acc@1  98.44 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [77][120/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.004)	Loss 4.7058e-03 (1.7650e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [77][130/391]	Time  0.032 ( 0.027)	Data  0.005 ( 0.004)	Loss 7.7676e-03 (1.7693e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [77][140/391]	Time  0.029 ( 0.027)	Data  0.003 ( 0.003)	Loss 3.0781e-02 (1.7629e-02)	Acc@1  97.66 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [77][150/391]	Time  0.027 ( 0.027)	Data  0.004 ( 0.003)	Loss 6.1274e-03 (1.7123e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [77][160/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.7498e-03 (1.6893e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [77][170/391]	Time  0.027 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.8697e-03 (1.6829e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [77][180/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.5931e-03 (1.6813e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [77][190/391]	Time  0.033 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.8431e-03 (1.6927e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [77][200/391]	Time  0.027 ( 0.027)	Data  0.000 ( 0.003)	Loss 1.8507e-02 (1.6746e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [77][210/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.5238e-03 (1.7008e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [77][220/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.7671e-02 (1.6975e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [77][230/391]	Time  0.030 ( 0.027)	Data  0.005 ( 0.003)	Loss 2.3864e-03 (1.6970e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [77][240/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.2941e-03 (1.7051e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [77][250/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.5078e-02 (1.7257e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [77][260/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.3211e-02 (1.7389e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [77][270/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.2132e-03 (1.7388e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [77][280/391]	Time  0.028 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.7958e-03 (1.7411e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [77][290/391]	Time  0.035 ( 0.027)	Data  0.003 ( 0.003)	Loss 2.0097e-02 (1.7607e-02)	Acc@1  98.44 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [77][300/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.2363e-02 (1.7808e-02)	Acc@1  98.44 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [77][310/391]	Time  0.025 ( 0.026)	Data  0.004 ( 0.003)	Loss 3.7143e-03 (1.7622e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [77][320/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.2190e-03 (1.7687e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [77][330/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.9692e-03 (1.8150e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [77][340/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.5354e-03 (1.8027e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [77][350/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.9481e-03 (1.7914e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [77][360/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0573e-02 (1.7824e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [77][370/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.4757e-03 (1.7616e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [77][380/391]	Time  0.034 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3183e-02 (1.7795e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [77][390/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1694e-02 (1.7592e-02)	Acc@1  98.75 ( 99.41)	Acc@5 100.00 (100.00)
## e[77] optimizer.zero_grad (sum) time: 0.10859894752502441
## e[77]       loss.backward (sum) time: 2.191890001296997
## e[77]      optimizer.step (sum) time: 0.8813776969909668
## epoch[77] training(only) time: 10.4288969039917
# Switched to evaluate mode...
Test: [  0/100]	Time  0.153 ( 0.153)	Loss 2.0135e-01 (2.0135e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.011 ( 0.027)	Loss 3.9500e-01 (3.7055e-01)	Acc@1  90.00 ( 91.55)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.011 ( 0.021)	Loss 4.4163e-01 (4.0183e-01)	Acc@1  90.00 ( 90.95)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.011 ( 0.019)	Loss 3.3404e-01 (4.1916e-01)	Acc@1  89.00 ( 90.58)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.021 ( 0.018)	Loss 6.1772e-01 (4.3922e-01)	Acc@1  88.00 ( 90.34)	Acc@5  99.00 ( 99.61)
Test: [ 50/100]	Time  0.011 ( 0.017)	Loss 1.8003e-01 (4.3255e-01)	Acc@1  94.00 ( 90.45)	Acc@5  99.00 ( 99.59)
Test: [ 60/100]	Time  0.011 ( 0.017)	Loss 4.6409e-01 (4.3170e-01)	Acc@1  90.00 ( 90.66)	Acc@5 100.00 ( 99.59)
Test: [ 70/100]	Time  0.011 ( 0.016)	Loss 8.2953e-01 (4.3102e-01)	Acc@1  91.00 ( 90.70)	Acc@5 100.00 ( 99.58)
Test: [ 80/100]	Time  0.020 ( 0.016)	Loss 4.0125e-01 (4.2914e-01)	Acc@1  93.00 ( 90.73)	Acc@5 100.00 ( 99.60)
Test: [ 90/100]	Time  0.013 ( 0.016)	Loss 2.6813e-01 (4.2468e-01)	Acc@1  95.00 ( 90.77)	Acc@5 100.00 ( 99.64)
 * Acc@1 90.890 Acc@5 99.640
### epoch[77] execution time: 12.122990846633911
EPOCH 78
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [78][  0/391]	Time  0.204 ( 0.204)	Data  0.181 ( 0.181)	Loss 7.3383e-02 (7.3383e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [78][ 10/391]	Time  0.022 ( 0.042)	Data  0.001 ( 0.019)	Loss 1.0888e-02 (2.6249e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [78][ 20/391]	Time  0.025 ( 0.034)	Data  0.001 ( 0.011)	Loss 3.3962e-03 (2.4850e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [78][ 30/391]	Time  0.034 ( 0.031)	Data  0.000 ( 0.008)	Loss 1.5539e-02 (2.0811e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [78][ 40/391]	Time  0.024 ( 0.030)	Data  0.001 ( 0.007)	Loss 7.4791e-03 (1.8767e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [78][ 50/391]	Time  0.024 ( 0.029)	Data  0.000 ( 0.006)	Loss 2.0525e-03 (1.8414e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [78][ 60/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.9043e-03 (1.7446e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [78][ 70/391]	Time  0.026 ( 0.028)	Data  0.002 ( 0.005)	Loss 6.3802e-02 (1.7505e-02)	Acc@1  98.44 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [78][ 80/391]	Time  0.023 ( 0.028)	Data  0.002 ( 0.004)	Loss 2.9001e-02 (1.8462e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 ( 99.99)
Epoch: [78][ 90/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.5654e-03 (1.8771e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 ( 99.99)
Epoch: [78][100/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 6.9120e-03 (1.9474e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 ( 99.99)
Epoch: [78][110/391]	Time  0.037 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.6803e-02 (1.8940e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 ( 99.99)
Epoch: [78][120/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 7.5984e-03 (1.9954e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 ( 99.99)
Epoch: [78][130/391]	Time  0.027 ( 0.027)	Data  0.005 ( 0.004)	Loss 7.1092e-03 (1.9624e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 ( 99.99)
Epoch: [78][140/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.8747e-03 (1.9225e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 ( 99.99)
Epoch: [78][150/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.9214e-03 (1.8756e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 ( 99.99)
Epoch: [78][160/391]	Time  0.028 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.4352e-02 (1.8670e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [78][170/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.4113e-02 (1.8471e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [78][180/391]	Time  0.027 ( 0.027)	Data  0.001 ( 0.003)	Loss 8.8824e-03 (1.8782e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 ( 99.99)
Epoch: [78][190/391]	Time  0.031 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.1844e-02 (1.8920e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 ( 99.99)
Epoch: [78][200/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.1083e-02 (1.9034e-02)	Acc@1  98.44 ( 99.42)	Acc@5 100.00 ( 99.99)
Epoch: [78][210/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.5741e-02 (1.9008e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 ( 99.99)
Epoch: [78][220/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.1008e-02 (1.9047e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 ( 99.99)
Epoch: [78][230/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.0474e-03 (1.9232e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 ( 99.99)
Epoch: [78][240/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.3754e-03 (1.8866e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 ( 99.99)
Epoch: [78][250/391]	Time  0.024 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.7654e-02 (1.8902e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 ( 99.99)
Epoch: [78][260/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.7278e-02 (1.9103e-02)	Acc@1  97.66 ( 99.40)	Acc@5 100.00 ( 99.99)
Epoch: [78][270/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.8017e-03 (1.9071e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 ( 99.99)
Epoch: [78][280/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4718e-02 (1.9090e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 ( 99.99)
Epoch: [78][290/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1768e-02 (1.8843e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 ( 99.99)
Epoch: [78][300/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3790e-02 (1.8592e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 ( 99.99)
Epoch: [78][310/391]	Time  0.030 ( 0.026)	Data  0.005 ( 0.003)	Loss 1.8694e-02 (1.8559e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 ( 99.99)
Epoch: [78][320/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.0923e-02 (1.8935e-02)	Acc@1  96.88 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [78][330/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.6511e-02 (1.8808e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [78][340/391]	Time  0.028 ( 0.026)	Data  0.000 ( 0.003)	Loss 7.5597e-03 (1.8556e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [78][350/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.0154e-03 (1.8444e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [78][360/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2329e-02 (1.8739e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [78][370/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 4.6268e-02 (1.8962e-02)	Acc@1  99.22 ( 99.41)	Acc@5  99.22 ( 99.99)
Epoch: [78][380/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3600e-03 (1.9030e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 ( 99.99)
Epoch: [78][390/391]	Time  0.026 ( 0.026)	Data  0.000 ( 0.003)	Loss 8.0259e-02 (1.9000e-02)	Acc@1  98.75 ( 99.41)	Acc@5 100.00 ( 99.99)
## e[78] optimizer.zero_grad (sum) time: 0.11063170433044434
## e[78]       loss.backward (sum) time: 2.2316088676452637
## e[78]      optimizer.step (sum) time: 0.8853528499603271
## epoch[78] training(only) time: 10.426623344421387
# Switched to evaluate mode...
Test: [  0/100]	Time  0.153 ( 0.153)	Loss 2.1742e-01 (2.1742e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.012 ( 0.027)	Loss 3.9564e-01 (3.7353e-01)	Acc@1  92.00 ( 91.73)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.011 ( 0.021)	Loss 4.0907e-01 (4.0147e-01)	Acc@1  91.00 ( 91.10)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.016 ( 0.019)	Loss 3.2590e-01 (4.2198e-01)	Acc@1  90.00 ( 90.65)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.014 ( 0.017)	Loss 6.2005e-01 (4.4272e-01)	Acc@1  88.00 ( 90.41)	Acc@5  99.00 ( 99.59)
Test: [ 50/100]	Time  0.012 ( 0.017)	Loss 2.0371e-01 (4.3533e-01)	Acc@1  95.00 ( 90.55)	Acc@5 100.00 ( 99.57)
Test: [ 60/100]	Time  0.010 ( 0.016)	Loss 4.1963e-01 (4.3308e-01)	Acc@1  90.00 ( 90.72)	Acc@5 100.00 ( 99.57)
Test: [ 70/100]	Time  0.010 ( 0.016)	Loss 8.3928e-01 (4.3362e-01)	Acc@1  90.00 ( 90.73)	Acc@5  99.00 ( 99.55)
Test: [ 80/100]	Time  0.010 ( 0.016)	Loss 3.9706e-01 (4.3243e-01)	Acc@1  93.00 ( 90.75)	Acc@5 100.00 ( 99.58)
Test: [ 90/100]	Time  0.013 ( 0.016)	Loss 2.7837e-01 (4.2805e-01)	Acc@1  94.00 ( 90.79)	Acc@5 100.00 ( 99.62)
 * Acc@1 90.870 Acc@5 99.630
### epoch[78] execution time: 12.158740282058716
EPOCH 79
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [79][  0/391]	Time  0.190 ( 0.190)	Data  0.170 ( 0.170)	Loss 1.2463e-02 (1.2463e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [79][ 10/391]	Time  0.028 ( 0.041)	Data  0.001 ( 0.018)	Loss 4.3461e-03 (1.4092e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [79][ 20/391]	Time  0.023 ( 0.033)	Data  0.000 ( 0.011)	Loss 1.1129e-02 (1.7140e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [79][ 30/391]	Time  0.023 ( 0.031)	Data  0.001 ( 0.008)	Loss 3.5681e-02 (1.6752e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [79][ 40/391]	Time  0.022 ( 0.029)	Data  0.002 ( 0.006)	Loss 5.2989e-03 (1.7950e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [79][ 50/391]	Time  0.027 ( 0.029)	Data  0.001 ( 0.006)	Loss 5.1504e-03 (1.7750e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [79][ 60/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.005)	Loss 8.8950e-03 (1.6639e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [79][ 70/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.6621e-02 (1.7075e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [79][ 80/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.3823e-02 (1.6973e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [79][ 90/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.5102e-02 (1.6545e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [79][100/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 4.6233e-03 (1.6324e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [79][110/391]	Time  0.028 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.3353e-03 (1.6761e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [79][120/391]	Time  0.030 ( 0.027)	Data  0.006 ( 0.004)	Loss 7.6118e-02 (1.7501e-02)	Acc@1  98.44 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [79][130/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.9940e-02 (1.8237e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 ( 99.99)
Epoch: [79][140/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.0964e-02 (1.8071e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 ( 99.99)
Epoch: [79][150/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.9339e-02 (1.8450e-02)	Acc@1  98.44 ( 99.42)	Acc@5 100.00 ( 99.99)
Epoch: [79][160/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.9395e-03 (1.8528e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [79][170/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.0274e-02 (1.8710e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [79][180/391]	Time  0.027 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.7159e-03 (1.8441e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [79][190/391]	Time  0.030 ( 0.027)	Data  0.006 ( 0.003)	Loss 2.7487e-02 (1.8639e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [79][200/391]	Time  0.026 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.4690e-03 (1.8359e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [79][210/391]	Time  0.025 ( 0.027)	Data  0.004 ( 0.003)	Loss 2.5098e-03 (1.8197e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [79][220/391]	Time  0.031 ( 0.027)	Data  0.003 ( 0.003)	Loss 6.1013e-03 (1.8047e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [79][230/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.3037e-02 (1.8073e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [79][240/391]	Time  0.032 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.6276e-03 (1.8052e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [79][250/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.1015e-02 (1.8493e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [79][260/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2598e-02 (1.8291e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [79][270/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.8979e-03 (1.8100e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [79][280/391]	Time  0.029 ( 0.026)	Data  0.004 ( 0.003)	Loss 1.4440e-02 (1.7872e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [79][290/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9799e-02 (1.7834e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [79][300/391]	Time  0.022 ( 0.026)	Data  0.002 ( 0.003)	Loss 6.8067e-03 (1.7670e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [79][310/391]	Time  0.026 ( 0.026)	Data  0.002 ( 0.003)	Loss 6.4800e-03 (1.7667e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [79][320/391]	Time  0.036 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3834e-02 (1.7849e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [79][330/391]	Time  0.024 ( 0.026)	Data  0.005 ( 0.003)	Loss 1.0175e-02 (1.7563e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [79][340/391]	Time  0.027 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.4504e-02 (1.7619e-02)	Acc@1  98.44 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [79][350/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.5038e-02 (1.7433e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [79][360/391]	Time  0.037 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.8501e-03 (1.7418e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [79][370/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.8419e-03 (1.7528e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [79][380/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.3163e-03 (1.7515e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [79][390/391]	Time  0.024 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.4079e-03 (1.7627e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
## e[79] optimizer.zero_grad (sum) time: 0.10758829116821289
## e[79]       loss.backward (sum) time: 2.238832712173462
## e[79]      optimizer.step (sum) time: 0.8744456768035889
## epoch[79] training(only) time: 10.449294090270996
# Switched to evaluate mode...
Test: [  0/100]	Time  0.166 ( 0.166)	Loss 2.1385e-01 (2.1385e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.013 ( 0.029)	Loss 4.0127e-01 (3.6990e-01)	Acc@1  92.00 ( 91.91)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.011 ( 0.021)	Loss 4.2713e-01 (3.9820e-01)	Acc@1  90.00 ( 91.14)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.012 ( 0.019)	Loss 3.2370e-01 (4.1598e-01)	Acc@1  90.00 ( 90.68)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.011 ( 0.018)	Loss 6.5990e-01 (4.3912e-01)	Acc@1  88.00 ( 90.41)	Acc@5  99.00 ( 99.61)
Test: [ 50/100]	Time  0.020 ( 0.017)	Loss 2.0703e-01 (4.3284e-01)	Acc@1  96.00 ( 90.57)	Acc@5 100.00 ( 99.59)
Test: [ 60/100]	Time  0.023 ( 0.017)	Loss 4.3760e-01 (4.3127e-01)	Acc@1  90.00 ( 90.74)	Acc@5 100.00 ( 99.59)
Test: [ 70/100]	Time  0.010 ( 0.016)	Loss 8.4926e-01 (4.3182e-01)	Acc@1  90.00 ( 90.77)	Acc@5  99.00 ( 99.55)
Test: [ 80/100]	Time  0.023 ( 0.016)	Loss 3.9623e-01 (4.3055e-01)	Acc@1  93.00 ( 90.80)	Acc@5 100.00 ( 99.58)
Test: [ 90/100]	Time  0.015 ( 0.016)	Loss 2.7607e-01 (4.2620e-01)	Acc@1  95.00 ( 90.87)	Acc@5 100.00 ( 99.62)
 * Acc@1 90.960 Acc@5 99.620
### epoch[79] execution time: 12.181402206420898
EPOCH 80
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [80][  0/391]	Time  0.188 ( 0.188)	Data  0.167 ( 0.167)	Loss 2.0649e-02 (2.0649e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [80][ 10/391]	Time  0.037 ( 0.041)	Data  0.002 ( 0.018)	Loss 1.6154e-03 (2.1574e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [80][ 20/391]	Time  0.030 ( 0.034)	Data  0.002 ( 0.010)	Loss 5.3235e-02 (2.1206e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [80][ 30/391]	Time  0.023 ( 0.031)	Data  0.001 ( 0.008)	Loss 9.0481e-03 (1.9786e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [80][ 40/391]	Time  0.028 ( 0.030)	Data  0.004 ( 0.006)	Loss 7.5421e-03 (1.8420e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [80][ 50/391]	Time  0.024 ( 0.029)	Data  0.001 ( 0.006)	Loss 5.0817e-03 (1.7661e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [80][ 60/391]	Time  0.023 ( 0.028)	Data  0.002 ( 0.005)	Loss 1.1707e-02 (1.9098e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [80][ 70/391]	Time  0.027 ( 0.028)	Data  0.003 ( 0.005)	Loss 1.5095e-02 (1.8208e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [80][ 80/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.9701e-02 (1.7765e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [80][ 90/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.004)	Loss 5.1276e-03 (1.7410e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [80][100/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.6652e-02 (1.7104e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [80][110/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.4244e-02 (1.7545e-02)	Acc@1  99.22 ( 99.36)	Acc@5  99.22 ( 99.99)
Epoch: [80][120/391]	Time  0.026 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.7249e-02 (1.7336e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 ( 99.99)
Epoch: [80][130/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.1716e-02 (1.6877e-02)	Acc@1  98.44 ( 99.39)	Acc@5 100.00 ( 99.99)
Epoch: [80][140/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.1623e-03 (1.6482e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 ( 99.99)
Epoch: [80][150/391]	Time  0.038 ( 0.027)	Data  0.003 ( 0.003)	Loss 8.3663e-04 (1.6763e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 ( 99.99)
Epoch: [80][160/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.2751e-02 (1.6708e-02)	Acc@1  97.66 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [80][170/391]	Time  0.036 ( 0.027)	Data  0.002 ( 0.003)	Loss 1.2745e-02 (1.6274e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [80][180/391]	Time  0.026 ( 0.027)	Data  0.002 ( 0.003)	Loss 1.4057e-02 (1.5905e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [80][190/391]	Time  0.026 ( 0.027)	Data  0.002 ( 0.003)	Loss 9.0901e-03 (1.5952e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [80][200/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 5.2896e-03 (1.5733e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [80][210/391]	Time  0.026 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.8699e-02 (1.5905e-02)	Acc@1  97.66 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [80][220/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 2.0944e-02 (1.5825e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [80][230/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.5588e-02 (1.6227e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [80][240/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.9731e-02 (1.6561e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [80][250/391]	Time  0.029 ( 0.027)	Data  0.008 ( 0.003)	Loss 5.5305e-03 (1.6535e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [80][260/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.4682e-02 (1.6770e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [80][270/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.5005e-03 (1.6791e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [80][280/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.7633e-03 (1.6917e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 ( 99.99)
Epoch: [80][290/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9837e-02 (1.6968e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 ( 99.99)
Epoch: [80][300/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.1027e-02 (1.7124e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 ( 99.99)
Epoch: [80][310/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2446e-02 (1.7016e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [80][320/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.9492e-02 (1.6829e-02)	Acc@1  97.66 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [80][330/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3049e-02 (1.6758e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [80][340/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3787e-02 (1.6831e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [80][350/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.1234e-02 (1.7068e-02)	Acc@1  98.44 ( 99.43)	Acc@5 100.00 ( 99.99)
Epoch: [80][360/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1869e-02 (1.6948e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [80][370/391]	Time  0.023 ( 0.026)	Data  0.000 ( 0.003)	Loss 5.8981e-03 (1.6874e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [80][380/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1081e-03 (1.6715e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [80][390/391]	Time  0.024 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.6878e-03 (1.6571e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 ( 99.99)
## e[80] optimizer.zero_grad (sum) time: 0.10862374305725098
## e[80]       loss.backward (sum) time: 2.2717199325561523
## e[80]      optimizer.step (sum) time: 0.8696568012237549
## epoch[80] training(only) time: 10.422795295715332
# Switched to evaluate mode...
Test: [  0/100]	Time  0.151 ( 0.151)	Loss 2.2554e-01 (2.2554e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.011 ( 0.027)	Loss 4.2124e-01 (3.7522e-01)	Acc@1  90.00 ( 91.73)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.016 ( 0.021)	Loss 4.2887e-01 (4.0467e-01)	Acc@1  90.00 ( 91.14)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.013 ( 0.019)	Loss 3.2203e-01 (4.2184e-01)	Acc@1  91.00 ( 90.77)	Acc@5 100.00 ( 99.61)
Test: [ 40/100]	Time  0.021 ( 0.018)	Loss 6.4800e-01 (4.4353e-01)	Acc@1  88.00 ( 90.44)	Acc@5  99.00 ( 99.59)
Test: [ 50/100]	Time  0.015 ( 0.017)	Loss 1.9048e-01 (4.3724e-01)	Acc@1  95.00 ( 90.51)	Acc@5 100.00 ( 99.57)
Test: [ 60/100]	Time  0.011 ( 0.017)	Loss 4.5671e-01 (4.3571e-01)	Acc@1  90.00 ( 90.77)	Acc@5 100.00 ( 99.57)
Test: [ 70/100]	Time  0.011 ( 0.017)	Loss 8.7038e-01 (4.3657e-01)	Acc@1  90.00 ( 90.77)	Acc@5 100.00 ( 99.56)
Test: [ 80/100]	Time  0.011 ( 0.016)	Loss 3.7660e-01 (4.3508e-01)	Acc@1  93.00 ( 90.79)	Acc@5 100.00 ( 99.59)
Test: [ 90/100]	Time  0.013 ( 0.016)	Loss 2.8974e-01 (4.3203e-01)	Acc@1  95.00 ( 90.82)	Acc@5 100.00 ( 99.63)
 * Acc@1 90.900 Acc@5 99.630
### epoch[80] execution time: 12.135915040969849
EPOCH 81
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [81][  0/391]	Time  0.200 ( 0.200)	Data  0.176 ( 0.176)	Loss 3.2474e-02 (3.2474e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [81][ 10/391]	Time  0.024 ( 0.040)	Data  0.001 ( 0.018)	Loss 2.8425e-02 (1.7392e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [81][ 20/391]	Time  0.023 ( 0.034)	Data  0.001 ( 0.010)	Loss 5.4004e-03 (1.5135e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [81][ 30/391]	Time  0.034 ( 0.031)	Data  0.001 ( 0.008)	Loss 7.4214e-03 (1.7423e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [81][ 40/391]	Time  0.024 ( 0.030)	Data  0.001 ( 0.006)	Loss 2.3102e-03 (1.6834e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [81][ 50/391]	Time  0.023 ( 0.029)	Data  0.001 ( 0.005)	Loss 1.2906e-02 (1.6748e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [81][ 60/391]	Time  0.022 ( 0.029)	Data  0.001 ( 0.005)	Loss 7.3671e-03 (1.5865e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [81][ 70/391]	Time  0.041 ( 0.028)	Data  0.002 ( 0.004)	Loss 5.2067e-03 (1.6502e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [81][ 80/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 4.1540e-03 (1.5911e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [81][ 90/391]	Time  0.025 ( 0.028)	Data  0.001 ( 0.004)	Loss 5.4522e-02 (1.6995e-02)	Acc@1  98.44 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [81][100/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 5.9102e-03 (1.8041e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [81][110/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.004)	Loss 8.3877e-03 (1.7404e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [81][120/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.0261e-03 (1.7796e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [81][130/391]	Time  0.025 ( 0.027)	Data  0.004 ( 0.003)	Loss 1.8324e-02 (1.7144e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [81][140/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.2252e-02 (1.6891e-02)	Acc@1  98.44 ( 99.48)	Acc@5  99.22 ( 99.99)
Epoch: [81][150/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.1806e-03 (1.7873e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [81][160/391]	Time  0.027 ( 0.027)	Data  0.000 ( 0.003)	Loss 1.0545e-02 (1.7676e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [81][170/391]	Time  0.033 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.2351e-02 (1.7297e-02)	Acc@1  98.44 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [81][180/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 3.1122e-03 (1.6881e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [81][190/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.5655e-03 (1.6766e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [81][200/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.3906e-02 (1.6824e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [81][210/391]	Time  0.024 ( 0.027)	Data  0.000 ( 0.003)	Loss 4.1553e-02 (1.7202e-02)	Acc@1  98.44 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [81][220/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.5554e-02 (1.7004e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [81][230/391]	Time  0.032 ( 0.027)	Data  0.002 ( 0.003)	Loss 1.3360e-02 (1.6805e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [81][240/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.5121e-02 (1.6971e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [81][250/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.1513e-02 (1.7151e-02)	Acc@1  98.44 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [81][260/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0065e-02 (1.7342e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [81][270/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8970e-02 (1.7287e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [81][280/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.5113e-03 (1.7183e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [81][290/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5337e-02 (1.7256e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [81][300/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3092e-02 (1.7252e-02)	Acc@1  98.44 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [81][310/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3436e-03 (1.7211e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [81][320/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.6767e-02 (1.7196e-02)	Acc@1  98.44 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [81][330/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 9.8696e-03 (1.7103e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [81][340/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.0148e-02 (1.7047e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [81][350/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5192e-02 (1.7077e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [81][360/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.0719e-03 (1.7238e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [81][370/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.9649e-03 (1.7032e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [81][380/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.6456e-02 (1.6967e-02)	Acc@1  97.66 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [81][390/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.6320e-03 (1.6981e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
## e[81] optimizer.zero_grad (sum) time: 0.10883665084838867
## e[81]       loss.backward (sum) time: 2.2338550090789795
## e[81]      optimizer.step (sum) time: 0.8810434341430664
## epoch[81] training(only) time: 10.387470245361328
# Switched to evaluate mode...
Test: [  0/100]	Time  0.166 ( 0.166)	Loss 2.1333e-01 (2.1333e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.011 ( 0.028)	Loss 3.8857e-01 (3.7345e-01)	Acc@1  91.00 ( 91.91)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.011 ( 0.021)	Loss 4.4231e-01 (4.0320e-01)	Acc@1  90.00 ( 91.05)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.011 ( 0.019)	Loss 3.4886e-01 (4.2203e-01)	Acc@1  90.00 ( 90.52)	Acc@5 100.00 ( 99.61)
Test: [ 40/100]	Time  0.023 ( 0.018)	Loss 6.0906e-01 (4.4338e-01)	Acc@1  88.00 ( 90.34)	Acc@5  99.00 ( 99.59)
Test: [ 50/100]	Time  0.021 ( 0.018)	Loss 1.8168e-01 (4.3616e-01)	Acc@1  96.00 ( 90.53)	Acc@5 100.00 ( 99.59)
Test: [ 60/100]	Time  0.015 ( 0.017)	Loss 4.4273e-01 (4.3328e-01)	Acc@1  90.00 ( 90.74)	Acc@5 100.00 ( 99.59)
Test: [ 70/100]	Time  0.012 ( 0.017)	Loss 8.3373e-01 (4.3410e-01)	Acc@1  89.00 ( 90.73)	Acc@5 100.00 ( 99.58)
Test: [ 80/100]	Time  0.013 ( 0.016)	Loss 3.9321e-01 (4.3233e-01)	Acc@1  93.00 ( 90.75)	Acc@5 100.00 ( 99.60)
Test: [ 90/100]	Time  0.015 ( 0.016)	Loss 2.7732e-01 (4.2970e-01)	Acc@1  95.00 ( 90.75)	Acc@5 100.00 ( 99.64)
 * Acc@1 90.880 Acc@5 99.640
### epoch[81] execution time: 12.09777045249939
EPOCH 82
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [82][  0/391]	Time  0.194 ( 0.194)	Data  0.171 ( 0.171)	Loss 4.3419e-02 (4.3419e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [82][ 10/391]	Time  0.024 ( 0.041)	Data  0.001 ( 0.017)	Loss 6.0774e-02 (2.3154e-02)	Acc@1  98.44 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [82][ 20/391]	Time  0.023 ( 0.034)	Data  0.001 ( 0.010)	Loss 4.7037e-03 (1.9551e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [82][ 30/391]	Time  0.031 ( 0.031)	Data  0.001 ( 0.007)	Loss 1.5618e-03 (1.8389e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [82][ 40/391]	Time  0.024 ( 0.030)	Data  0.001 ( 0.006)	Loss 4.6571e-03 (1.8652e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [82][ 50/391]	Time  0.025 ( 0.029)	Data  0.001 ( 0.006)	Loss 2.9021e-02 (1.8625e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [82][ 60/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.0350e-02 (1.8595e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [82][ 70/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.005)	Loss 5.8576e-03 (1.8762e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [82][ 80/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.7835e-02 (1.8812e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [82][ 90/391]	Time  0.024 ( 0.027)	Data  0.000 ( 0.004)	Loss 5.9178e-03 (1.9071e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [82][100/391]	Time  0.027 ( 0.027)	Data  0.001 ( 0.004)	Loss 6.6470e-03 (1.8351e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [82][110/391]	Time  0.033 ( 0.027)	Data  0.000 ( 0.004)	Loss 4.5567e-02 (1.8316e-02)	Acc@1  98.44 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [82][120/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.3906e-03 (1.7991e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [82][130/391]	Time  0.023 ( 0.027)	Data  0.003 ( 0.003)	Loss 2.1937e-02 (1.8319e-02)	Acc@1  98.44 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [82][140/391]	Time  0.030 ( 0.027)	Data  0.003 ( 0.003)	Loss 3.7907e-03 (1.8005e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [82][150/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.6103e-03 (1.8063e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [82][160/391]	Time  0.022 ( 0.027)	Data  0.000 ( 0.003)	Loss 2.9482e-02 (1.8452e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [82][170/391]	Time  0.026 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.6977e-03 (1.8226e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [82][180/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 9.3836e-04 (1.8206e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [82][190/391]	Time  0.025 ( 0.027)	Data  0.002 ( 0.003)	Loss 6.1243e-03 (1.8354e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [82][200/391]	Time  0.030 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.8017e-03 (1.8378e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [82][210/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.5097e-03 (1.8244e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [82][220/391]	Time  0.031 ( 0.027)	Data  0.002 ( 0.003)	Loss 3.8938e-02 (1.8117e-02)	Acc@1  97.66 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [82][230/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.5599e-02 (1.8101e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [82][240/391]	Time  0.023 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.0407e-02 (1.7820e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [82][250/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5670e-03 (1.7433e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [82][260/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.5151e-03 (1.7364e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [82][270/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.7858e-03 (1.7075e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [82][280/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.3531e-02 (1.7102e-02)	Acc@1  97.66 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [82][290/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5172e-02 (1.7284e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [82][300/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4735e-02 (1.7426e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [82][310/391]	Time  0.028 ( 0.026)	Data  0.002 ( 0.003)	Loss 7.2471e-03 (1.7183e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [82][320/391]	Time  0.026 ( 0.026)	Data  0.004 ( 0.003)	Loss 1.7024e-02 (1.7484e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [82][330/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3754e-03 (1.7397e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [82][340/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.6931e-02 (1.7593e-02)	Acc@1  98.44 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [82][350/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.6095e-02 (1.8019e-02)	Acc@1  98.44 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [82][360/391]	Time  0.023 ( 0.026)	Data  0.000 ( 0.003)	Loss 4.7816e-02 (1.8155e-02)	Acc@1  97.66 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [82][370/391]	Time  0.033 ( 0.026)	Data  0.004 ( 0.003)	Loss 3.1816e-03 (1.7901e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [82][380/391]	Time  0.038 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0255e-02 (1.7835e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [82][390/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.6236e-03 (1.7620e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
## e[82] optimizer.zero_grad (sum) time: 0.10823512077331543
## e[82]       loss.backward (sum) time: 2.265801191329956
## e[82]      optimizer.step (sum) time: 0.8767671585083008
## epoch[82] training(only) time: 10.406919717788696
# Switched to evaluate mode...
Test: [  0/100]	Time  0.164 ( 0.164)	Loss 2.1411e-01 (2.1411e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.018 ( 0.029)	Loss 4.0660e-01 (3.8425e-01)	Acc@1  90.00 ( 91.64)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.011 ( 0.022)	Loss 4.7410e-01 (4.1143e-01)	Acc@1  87.00 ( 90.95)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.011 ( 0.019)	Loss 3.3821e-01 (4.2757e-01)	Acc@1  90.00 ( 90.52)	Acc@5 100.00 ( 99.58)
Test: [ 40/100]	Time  0.017 ( 0.019)	Loss 6.2994e-01 (4.4910e-01)	Acc@1  88.00 ( 90.41)	Acc@5  99.00 ( 99.56)
Test: [ 50/100]	Time  0.011 ( 0.018)	Loss 1.6524e-01 (4.3862e-01)	Acc@1  95.00 ( 90.63)	Acc@5 100.00 ( 99.55)
Test: [ 60/100]	Time  0.013 ( 0.017)	Loss 4.4691e-01 (4.3615e-01)	Acc@1  90.00 ( 90.79)	Acc@5 100.00 ( 99.56)
Test: [ 70/100]	Time  0.010 ( 0.017)	Loss 8.4476e-01 (4.3792e-01)	Acc@1  89.00 ( 90.76)	Acc@5 100.00 ( 99.54)
Test: [ 80/100]	Time  0.013 ( 0.017)	Loss 4.2667e-01 (4.3563e-01)	Acc@1  92.00 ( 90.78)	Acc@5 100.00 ( 99.59)
Test: [ 90/100]	Time  0.011 ( 0.016)	Loss 2.7917e-01 (4.3221e-01)	Acc@1  95.00 ( 90.82)	Acc@5 100.00 ( 99.63)
 * Acc@1 90.950 Acc@5 99.640
### epoch[82] execution time: 12.153816938400269
EPOCH 83
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [83][  0/391]	Time  0.191 ( 0.191)	Data  0.170 ( 0.170)	Loss 2.3501e-03 (2.3501e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [83][ 10/391]	Time  0.024 ( 0.042)	Data  0.001 ( 0.018)	Loss 3.9544e-03 (2.1756e-02)	Acc@1 100.00 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [83][ 20/391]	Time  0.035 ( 0.034)	Data  0.002 ( 0.010)	Loss 2.0536e-03 (2.2596e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [83][ 30/391]	Time  0.024 ( 0.032)	Data  0.001 ( 0.008)	Loss 7.7956e-03 (2.2122e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [83][ 40/391]	Time  0.030 ( 0.030)	Data  0.011 ( 0.007)	Loss 5.3505e-02 (2.1363e-02)	Acc@1  99.22 ( 99.29)	Acc@5  99.22 ( 99.98)
Epoch: [83][ 50/391]	Time  0.023 ( 0.029)	Data  0.001 ( 0.006)	Loss 5.1064e-02 (2.1130e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 ( 99.98)
Epoch: [83][ 60/391]	Time  0.029 ( 0.029)	Data  0.003 ( 0.005)	Loss 7.2473e-03 (1.9749e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 ( 99.99)
Epoch: [83][ 70/391]	Time  0.025 ( 0.028)	Data  0.005 ( 0.005)	Loss 7.6424e-03 (1.8642e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 ( 99.99)
Epoch: [83][ 80/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.7333e-02 (1.8671e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 ( 99.99)
Epoch: [83][ 90/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.004)	Loss 7.2506e-03 (1.8420e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 ( 99.98)
Epoch: [83][100/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.1121e-02 (1.7746e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 ( 99.98)
Epoch: [83][110/391]	Time  0.028 ( 0.027)	Data  0.005 ( 0.004)	Loss 3.8906e-02 (1.7595e-02)	Acc@1  98.44 ( 99.42)	Acc@5 100.00 ( 99.99)
Epoch: [83][120/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.1440e-02 (1.8070e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 ( 99.99)
Epoch: [83][130/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.8657e-02 (1.8190e-02)	Acc@1  98.44 ( 99.40)	Acc@5 100.00 ( 99.99)
Epoch: [83][140/391]	Time  0.032 ( 0.027)	Data  0.002 ( 0.003)	Loss 1.4948e-02 (1.7502e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 ( 99.99)
Epoch: [83][150/391]	Time  0.030 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.3738e-03 (1.7316e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [83][160/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.3369e-03 (1.7721e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [83][170/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.8736e-02 (1.7589e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [83][180/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.8481e-04 (1.7273e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 ( 99.99)
Epoch: [83][190/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.1963e-02 (1.7271e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [83][200/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 9.0374e-03 (1.7446e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 ( 99.99)
Epoch: [83][210/391]	Time  0.031 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.3316e-02 (1.7533e-02)	Acc@1  98.44 ( 99.43)	Acc@5 100.00 ( 99.99)
Epoch: [83][220/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.6903e-02 (1.7426e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 ( 99.99)
Epoch: [83][230/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 8.0784e-03 (1.7484e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 ( 99.99)
Epoch: [83][240/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 1.8560e-02 (1.7166e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [83][250/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.1526e-02 (1.7363e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 ( 99.99)
Epoch: [83][260/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.9505e-03 (1.7290e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 ( 99.99)
Epoch: [83][270/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.9476e-02 (1.7318e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 ( 99.99)
Epoch: [83][280/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.1890e-02 (1.7309e-02)	Acc@1  98.44 ( 99.43)	Acc@5 100.00 ( 99.99)
Epoch: [83][290/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.5053e-03 (1.7464e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 ( 99.99)
Epoch: [83][300/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.2321e-03 (1.7130e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [83][310/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.0563e-03 (1.7278e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [83][320/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.0081e-03 (1.7136e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [83][330/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1829e-03 (1.6994e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 ( 99.99)
Epoch: [83][340/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8107e-03 (1.6995e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 ( 99.99)
Epoch: [83][350/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.1439e-02 (1.7232e-02)	Acc@1  98.44 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [83][360/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.9553e-03 (1.7054e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [83][370/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4210e-02 (1.7080e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [83][380/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7581e-03 (1.6993e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [83][390/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8736e-03 (1.7083e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 ( 99.99)
## e[83] optimizer.zero_grad (sum) time: 0.11002421379089355
## e[83]       loss.backward (sum) time: 2.27036714553833
## e[83]      optimizer.step (sum) time: 0.8816783428192139
## epoch[83] training(only) time: 10.41605281829834
# Switched to evaluate mode...
Test: [  0/100]	Time  0.162 ( 0.162)	Loss 1.9215e-01 (1.9215e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.012 ( 0.027)	Loss 4.0030e-01 (3.8170e-01)	Acc@1  91.00 ( 91.91)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.012 ( 0.021)	Loss 4.3388e-01 (4.1089e-01)	Acc@1  89.00 ( 91.14)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.013 ( 0.019)	Loss 3.4941e-01 (4.2463e-01)	Acc@1  90.00 ( 90.65)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.012 ( 0.018)	Loss 6.7017e-01 (4.4937e-01)	Acc@1  88.00 ( 90.44)	Acc@5  99.00 ( 99.63)
Test: [ 50/100]	Time  0.010 ( 0.017)	Loss 1.7006e-01 (4.3968e-01)	Acc@1  96.00 ( 90.69)	Acc@5 100.00 ( 99.61)
Test: [ 60/100]	Time  0.011 ( 0.017)	Loss 4.6666e-01 (4.3813e-01)	Acc@1  90.00 ( 90.87)	Acc@5 100.00 ( 99.61)
Test: [ 70/100]	Time  0.011 ( 0.016)	Loss 8.6193e-01 (4.3891e-01)	Acc@1  90.00 ( 90.90)	Acc@5 100.00 ( 99.59)
Test: [ 80/100]	Time  0.012 ( 0.016)	Loss 4.0455e-01 (4.3633e-01)	Acc@1  93.00 ( 90.88)	Acc@5 100.00 ( 99.63)
Test: [ 90/100]	Time  0.011 ( 0.016)	Loss 2.8076e-01 (4.3332e-01)	Acc@1  95.00 ( 90.87)	Acc@5 100.00 ( 99.66)
 * Acc@1 90.970 Acc@5 99.660
### epoch[83] execution time: 12.135719299316406
EPOCH 84
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [84][  0/391]	Time  0.199 ( 0.199)	Data  0.176 ( 0.176)	Loss 2.3182e-02 (2.3182e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [84][ 10/391]	Time  0.023 ( 0.042)	Data  0.001 ( 0.018)	Loss 6.6205e-02 (1.6773e-02)	Acc@1  97.66 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [84][ 20/391]	Time  0.025 ( 0.035)	Data  0.001 ( 0.010)	Loss 1.8619e-02 (1.4629e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [84][ 30/391]	Time  0.023 ( 0.032)	Data  0.001 ( 0.008)	Loss 4.1856e-03 (1.3894e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [84][ 40/391]	Time  0.023 ( 0.030)	Data  0.001 ( 0.006)	Loss 1.4111e-03 (1.7561e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [84][ 50/391]	Time  0.024 ( 0.029)	Data  0.001 ( 0.006)	Loss 4.2915e-03 (1.5807e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [84][ 60/391]	Time  0.025 ( 0.029)	Data  0.001 ( 0.005)	Loss 5.4187e-03 (1.5600e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [84][ 70/391]	Time  0.026 ( 0.028)	Data  0.001 ( 0.005)	Loss 9.6680e-03 (1.5625e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [84][ 80/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.004)	Loss 3.5627e-02 (1.5599e-02)	Acc@1  98.44 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [84][ 90/391]	Time  0.025 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.2974e-02 (1.5135e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [84][100/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.1152e-02 (1.4909e-02)	Acc@1  98.44 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [84][110/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 8.6451e-03 (1.5254e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [84][120/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.7897e-02 (1.4992e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [84][130/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.4938e-02 (1.5130e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [84][140/391]	Time  0.024 ( 0.027)	Data  0.002 ( 0.003)	Loss 2.6951e-03 (1.5389e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [84][150/391]	Time  0.027 ( 0.027)	Data  0.006 ( 0.003)	Loss 1.7694e-02 (1.5237e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [84][160/391]	Time  0.035 ( 0.027)	Data  0.004 ( 0.003)	Loss 5.9196e-03 (1.5612e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [84][170/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.6607e-03 (1.5367e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [84][180/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.5162e-02 (1.5932e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [84][190/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.6438e-03 (1.6542e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [84][200/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.2193e-02 (1.6472e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [84][210/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 9.3527e-03 (1.6354e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [84][220/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.8829e-03 (1.6504e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [84][230/391]	Time  0.027 ( 0.026)	Data  0.002 ( 0.003)	Loss 4.4324e-02 (1.6836e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [84][240/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.0428e-02 (1.6939e-02)	Acc@1  98.44 ( 99.43)	Acc@5 100.00 ( 99.99)
Epoch: [84][250/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3442e-02 (1.6830e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [84][260/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.5956e-02 (1.6873e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [84][270/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.8275e-03 (1.6674e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [84][280/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.7063e-03 (1.6682e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [84][290/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.7691e-02 (1.6474e-02)	Acc@1  98.44 ( 99.46)	Acc@5 100.00 ( 99.99)
Epoch: [84][300/391]	Time  0.033 ( 0.026)	Data  0.005 ( 0.003)	Loss 1.0237e-02 (1.6449e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 ( 99.99)
Epoch: [84][310/391]	Time  0.036 ( 0.026)	Data  0.005 ( 0.003)	Loss 1.1173e-02 (1.6422e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 ( 99.99)
Epoch: [84][320/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5569e-02 (1.6533e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [84][330/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.5100e-03 (1.6554e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [84][340/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0196e-02 (1.6340e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [84][350/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8690e-02 (1.6196e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [84][360/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.9947e-03 (1.6238e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [84][370/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.7684e-03 (1.6259e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [84][380/391]	Time  0.025 ( 0.026)	Data  0.000 ( 0.003)	Loss 3.3095e-02 (1.6316e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [84][390/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.0664e-03 (1.6265e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
## e[84] optimizer.zero_grad (sum) time: 0.10957050323486328
## e[84]       loss.backward (sum) time: 2.285733699798584
## e[84]      optimizer.step (sum) time: 0.8821988105773926
## epoch[84] training(only) time: 10.419930696487427
# Switched to evaluate mode...
Test: [  0/100]	Time  0.159 ( 0.159)	Loss 1.9186e-01 (1.9186e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.012 ( 0.026)	Loss 4.0225e-01 (3.7295e-01)	Acc@1  90.00 ( 91.73)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.011 ( 0.021)	Loss 4.5375e-01 (4.0677e-01)	Acc@1  89.00 ( 91.24)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.012 ( 0.018)	Loss 3.3908e-01 (4.2722e-01)	Acc@1  90.00 ( 90.65)	Acc@5 100.00 ( 99.58)
Test: [ 40/100]	Time  0.019 ( 0.018)	Loss 6.6394e-01 (4.5101e-01)	Acc@1  88.00 ( 90.37)	Acc@5  99.00 ( 99.56)
Test: [ 50/100]	Time  0.020 ( 0.017)	Loss 1.8666e-01 (4.4135e-01)	Acc@1  95.00 ( 90.59)	Acc@5 100.00 ( 99.57)
Test: [ 60/100]	Time  0.011 ( 0.017)	Loss 4.4998e-01 (4.3876e-01)	Acc@1  91.00 ( 90.75)	Acc@5 100.00 ( 99.57)
Test: [ 70/100]	Time  0.010 ( 0.016)	Loss 8.4449e-01 (4.3857e-01)	Acc@1  91.00 ( 90.80)	Acc@5 100.00 ( 99.55)
Test: [ 80/100]	Time  0.011 ( 0.016)	Loss 3.9404e-01 (4.3619e-01)	Acc@1  92.00 ( 90.78)	Acc@5 100.00 ( 99.58)
Test: [ 90/100]	Time  0.016 ( 0.016)	Loss 3.0525e-01 (4.3308e-01)	Acc@1  95.00 ( 90.80)	Acc@5 100.00 ( 99.62)
 * Acc@1 90.960 Acc@5 99.620
### epoch[84] execution time: 12.158491849899292
EPOCH 85
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [85][  0/391]	Time  0.192 ( 0.192)	Data  0.171 ( 0.171)	Loss 5.7952e-03 (5.7952e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [85][ 10/391]	Time  0.025 ( 0.042)	Data  0.001 ( 0.018)	Loss 6.3392e-04 (8.3272e-03)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [85][ 20/391]	Time  0.025 ( 0.034)	Data  0.001 ( 0.010)	Loss 1.0840e-02 (1.0008e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [85][ 30/391]	Time  0.023 ( 0.031)	Data  0.001 ( 0.007)	Loss 6.9890e-02 (1.4935e-02)	Acc@1  98.44 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [85][ 40/391]	Time  0.023 ( 0.030)	Data  0.001 ( 0.006)	Loss 3.6099e-03 (1.5348e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [85][ 50/391]	Time  0.024 ( 0.029)	Data  0.001 ( 0.005)	Loss 7.8985e-04 (1.4014e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [85][ 60/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.6101e-02 (1.3895e-02)	Acc@1  98.44 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [85][ 70/391]	Time  0.023 ( 0.028)	Data  0.004 ( 0.004)	Loss 2.1171e-02 (1.4932e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [85][ 80/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.2000e-03 (1.4295e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [85][ 90/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.5136e-02 (1.4296e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [85][100/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.2847e-02 (1.4403e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [85][110/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 7.2480e-03 (1.4212e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [85][120/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.7069e-02 (1.4743e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [85][130/391]	Time  0.028 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.0974e-02 (1.6239e-02)	Acc@1  97.66 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [85][140/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 8.1526e-03 (1.6418e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [85][150/391]	Time  0.031 ( 0.027)	Data  0.000 ( 0.003)	Loss 5.8281e-03 (1.5733e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [85][160/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.1371e-03 (1.5486e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [85][170/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 9.9015e-04 (1.5524e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [85][180/391]	Time  0.030 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.0385e-02 (1.5465e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [85][190/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 2.1032e-02 (1.5465e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [85][200/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.7200e-02 (1.5568e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [85][210/391]	Time  0.029 ( 0.027)	Data  0.000 ( 0.003)	Loss 3.1660e-03 (1.5675e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [85][220/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 9.4383e-03 (1.5358e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [85][230/391]	Time  0.032 ( 0.027)	Data  0.001 ( 0.003)	Loss 8.9029e-03 (1.5480e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [85][240/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.5039e-02 (1.5358e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [85][250/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3630e-03 (1.5614e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [85][260/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.2535e-03 (1.6040e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [85][270/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4410e-02 (1.5908e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [85][280/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3480e-03 (1.5878e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [85][290/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.7346e-03 (1.5785e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [85][300/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8932e-02 (1.6131e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [85][310/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.3327e-03 (1.6085e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [85][320/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4104e-02 (1.6233e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [85][330/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.9867e-02 (1.6166e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [85][340/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7214e-03 (1.6214e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [85][350/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.0498e-03 (1.6110e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [85][360/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.3922e-03 (1.5856e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [85][370/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.6834e-04 (1.5676e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [85][380/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8322e-02 (1.5859e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [85][390/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.4292e-02 (1.6014e-02)	Acc@1  98.75 ( 99.50)	Acc@5 100.00 (100.00)
## e[85] optimizer.zero_grad (sum) time: 0.1102454662322998
## e[85]       loss.backward (sum) time: 2.2373342514038086
## e[85]      optimizer.step (sum) time: 0.865372896194458
## epoch[85] training(only) time: 10.461136102676392
# Switched to evaluate mode...
Test: [  0/100]	Time  0.160 ( 0.160)	Loss 2.0276e-01 (2.0276e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.011 ( 0.027)	Loss 3.8198e-01 (3.7494e-01)	Acc@1  90.00 ( 91.73)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.015 ( 0.022)	Loss 4.5800e-01 (4.0565e-01)	Acc@1  89.00 ( 91.00)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.011 ( 0.019)	Loss 3.5098e-01 (4.2624e-01)	Acc@1  90.00 ( 90.55)	Acc@5 100.00 ( 99.61)
Test: [ 40/100]	Time  0.011 ( 0.018)	Loss 6.5674e-01 (4.4915e-01)	Acc@1  88.00 ( 90.29)	Acc@5  99.00 ( 99.59)
Test: [ 50/100]	Time  0.010 ( 0.018)	Loss 2.0505e-01 (4.4032e-01)	Acc@1  94.00 ( 90.51)	Acc@5  99.00 ( 99.55)
Test: [ 60/100]	Time  0.024 ( 0.017)	Loss 4.6485e-01 (4.3898e-01)	Acc@1  91.00 ( 90.70)	Acc@5 100.00 ( 99.56)
Test: [ 70/100]	Time  0.024 ( 0.017)	Loss 8.6740e-01 (4.3884e-01)	Acc@1  90.00 ( 90.76)	Acc@5  99.00 ( 99.54)
Test: [ 80/100]	Time  0.011 ( 0.017)	Loss 4.0474e-01 (4.3677e-01)	Acc@1  93.00 ( 90.79)	Acc@5 100.00 ( 99.57)
Test: [ 90/100]	Time  0.012 ( 0.017)	Loss 2.9588e-01 (4.3281e-01)	Acc@1  95.00 ( 90.85)	Acc@5 100.00 ( 99.60)
 * Acc@1 90.990 Acc@5 99.620
### epoch[85] execution time: 12.22695279121399
EPOCH 86
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [86][  0/391]	Time  0.195 ( 0.195)	Data  0.173 ( 0.173)	Loss 6.6346e-03 (6.6346e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [86][ 10/391]	Time  0.030 ( 0.042)	Data  0.002 ( 0.017)	Loss 1.2631e-03 (1.5414e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [86][ 20/391]	Time  0.023 ( 0.034)	Data  0.001 ( 0.010)	Loss 6.0191e-03 (1.4466e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [86][ 30/391]	Time  0.023 ( 0.031)	Data  0.002 ( 0.008)	Loss 4.3369e-03 (1.5099e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [86][ 40/391]	Time  0.023 ( 0.030)	Data  0.002 ( 0.006)	Loss 5.0037e-03 (1.5222e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [86][ 50/391]	Time  0.028 ( 0.029)	Data  0.001 ( 0.006)	Loss 5.8261e-03 (1.5673e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [86][ 60/391]	Time  0.029 ( 0.028)	Data  0.002 ( 0.005)	Loss 1.9814e-02 (1.5666e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 ( 99.99)
Epoch: [86][ 70/391]	Time  0.025 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.2384e-02 (1.6017e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 ( 99.99)
Epoch: [86][ 80/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 4.8986e-03 (1.6098e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 ( 99.99)
Epoch: [86][ 90/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.6179e-03 (1.6254e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 ( 99.99)
Epoch: [86][100/391]	Time  0.028 ( 0.027)	Data  0.004 ( 0.004)	Loss 5.0039e-02 (1.6246e-02)	Acc@1  98.44 ( 99.51)	Acc@5 100.00 ( 99.99)
Epoch: [86][110/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.004)	Loss 1.1417e-02 (1.5504e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 ( 99.99)
Epoch: [86][120/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.004)	Loss 4.4649e-03 (1.5215e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 ( 99.99)
Epoch: [86][130/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.4311e-02 (1.5210e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 ( 99.99)
Epoch: [86][140/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 1.8524e-03 (1.4909e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 ( 99.99)
Epoch: [86][150/391]	Time  0.027 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.2961e-03 (1.5271e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 ( 99.99)
Epoch: [86][160/391]	Time  0.025 ( 0.027)	Data  0.002 ( 0.003)	Loss 6.3163e-03 (1.5294e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [86][170/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.2279e-03 (1.5093e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [86][180/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.2502e-02 (1.5297e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [86][190/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.0078e-02 (1.5400e-02)	Acc@1  98.44 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [86][200/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.1934e-02 (1.6301e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [86][210/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.3450e-02 (1.6187e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [86][220/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 1.0196e-02 (1.6155e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [86][230/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.4169e-03 (1.5975e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [86][240/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.0403e-02 (1.5913e-02)	Acc@1  98.44 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [86][250/391]	Time  0.027 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.2236e-02 (1.6092e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [86][260/391]	Time  0.041 ( 0.027)	Data  0.004 ( 0.003)	Loss 1.4992e-02 (1.6280e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [86][270/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.8874e-03 (1.5997e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [86][280/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0347e-02 (1.6342e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [86][290/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2756e-02 (1.6133e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [86][300/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 4.5345e-02 (1.6214e-02)	Acc@1  98.44 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [86][310/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.2587e-03 (1.6088e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [86][320/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.9406e-02 (1.5998e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [86][330/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3201e-02 (1.5987e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [86][340/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.9276e-03 (1.5860e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [86][350/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.2317e-03 (1.5972e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [86][360/391]	Time  0.029 ( 0.026)	Data  0.005 ( 0.003)	Loss 2.6047e-02 (1.6064e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [86][370/391]	Time  0.029 ( 0.026)	Data  0.008 ( 0.003)	Loss 3.0591e-02 (1.6176e-02)	Acc@1  98.44 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [86][380/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.7913e-03 (1.6256e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [86][390/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.1280e-03 (1.6378e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
## e[86] optimizer.zero_grad (sum) time: 0.11001849174499512
## e[86]       loss.backward (sum) time: 2.245774269104004
## e[86]      optimizer.step (sum) time: 0.8786134719848633
## epoch[86] training(only) time: 10.44642186164856
# Switched to evaluate mode...
Test: [  0/100]	Time  0.158 ( 0.158)	Loss 2.0793e-01 (2.0793e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.013 ( 0.027)	Loss 4.2194e-01 (3.8392e-01)	Acc@1  91.00 ( 91.36)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.024 ( 0.021)	Loss 5.1372e-01 (4.1678e-01)	Acc@1  88.00 ( 90.81)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.011 ( 0.019)	Loss 3.6090e-01 (4.3274e-01)	Acc@1  90.00 ( 90.48)	Acc@5 100.00 ( 99.58)
Test: [ 40/100]	Time  0.014 ( 0.018)	Loss 6.5565e-01 (4.5515e-01)	Acc@1  88.00 ( 90.27)	Acc@5  99.00 ( 99.56)
Test: [ 50/100]	Time  0.010 ( 0.017)	Loss 1.9315e-01 (4.4641e-01)	Acc@1  95.00 ( 90.37)	Acc@5 100.00 ( 99.55)
Test: [ 60/100]	Time  0.011 ( 0.017)	Loss 4.6561e-01 (4.4545e-01)	Acc@1  91.00 ( 90.61)	Acc@5 100.00 ( 99.56)
Test: [ 70/100]	Time  0.012 ( 0.016)	Loss 8.4529e-01 (4.4527e-01)	Acc@1  90.00 ( 90.68)	Acc@5  99.00 ( 99.52)
Test: [ 80/100]	Time  0.022 ( 0.016)	Loss 4.0441e-01 (4.4212e-01)	Acc@1  93.00 ( 90.70)	Acc@5 100.00 ( 99.56)
Test: [ 90/100]	Time  0.011 ( 0.016)	Loss 2.9422e-01 (4.3938e-01)	Acc@1  95.00 ( 90.71)	Acc@5 100.00 ( 99.59)
 * Acc@1 90.840 Acc@5 99.610
### epoch[86] execution time: 12.132178783416748
EPOCH 87
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [87][  0/391]	Time  0.197 ( 0.197)	Data  0.177 ( 0.177)	Loss 4.0042e-02 (4.0042e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [87][ 10/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.018)	Loss 1.2362e-03 (1.2841e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [87][ 20/391]	Time  0.023 ( 0.034)	Data  0.001 ( 0.010)	Loss 6.2241e-02 (1.3012e-02)	Acc@1  98.44 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [87][ 30/391]	Time  0.024 ( 0.031)	Data  0.003 ( 0.008)	Loss 1.3017e-02 (1.2153e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [87][ 40/391]	Time  0.023 ( 0.030)	Data  0.001 ( 0.006)	Loss 3.6779e-02 (1.3687e-02)	Acc@1  97.66 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [87][ 50/391]	Time  0.022 ( 0.029)	Data  0.001 ( 0.006)	Loss 2.1995e-02 (1.4580e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [87][ 60/391]	Time  0.024 ( 0.029)	Data  0.001 ( 0.005)	Loss 1.4443e-02 (1.5959e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [87][ 70/391]	Time  0.028 ( 0.028)	Data  0.002 ( 0.005)	Loss 9.5824e-03 (1.6578e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [87][ 80/391]	Time  0.030 ( 0.028)	Data  0.001 ( 0.004)	Loss 3.5065e-03 (1.5841e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [87][ 90/391]	Time  0.027 ( 0.028)	Data  0.002 ( 0.004)	Loss 2.1869e-02 (1.5285e-02)	Acc@1  98.44 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [87][100/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.5398e-02 (1.5754e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [87][110/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.6967e-02 (1.5997e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [87][120/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.004)	Loss 6.4313e-02 (1.6591e-02)	Acc@1  97.66 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [87][130/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.7479e-02 (1.6736e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [87][140/391]	Time  0.031 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.6992e-03 (1.5982e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [87][150/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.8577e-02 (1.6490e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [87][160/391]	Time  0.027 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.5443e-02 (1.5902e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [87][170/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.4974e-02 (1.5939e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [87][180/391]	Time  0.033 ( 0.027)	Data  0.003 ( 0.003)	Loss 1.9400e-02 (1.5846e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [87][190/391]	Time  0.024 ( 0.027)	Data  0.002 ( 0.003)	Loss 6.4363e-03 (1.5974e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [87][200/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.6330e-03 (1.5932e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [87][210/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.2896e-02 (1.5719e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [87][220/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.9143e-02 (1.5999e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [87][230/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.3239e-02 (1.6136e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [87][240/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 5.3614e-03 (1.6020e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [87][250/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.9608e-02 (1.6119e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [87][260/391]	Time  0.032 ( 0.027)	Data  0.005 ( 0.003)	Loss 1.2701e-02 (1.6088e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [87][270/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.8255e-03 (1.6126e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [87][280/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.3205e-03 (1.6099e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [87][290/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6248e-03 (1.6081e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [87][300/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0567e-02 (1.6025e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [87][310/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.5348e-02 (1.6167e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [87][320/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2898e-03 (1.5971e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [87][330/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1799e-02 (1.6633e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [87][340/391]	Time  0.041 ( 0.026)	Data  0.002 ( 0.003)	Loss 4.3048e-03 (1.6820e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [87][350/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2820e-02 (1.6655e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [87][360/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3899e-02 (1.6573e-02)	Acc@1  98.44 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [87][370/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.6785e-03 (1.6481e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [87][380/391]	Time  0.028 ( 0.026)	Data  0.004 ( 0.003)	Loss 7.1415e-03 (1.6367e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 ( 99.99)
Epoch: [87][390/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.6772e-03 (1.6334e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 ( 99.99)
## e[87] optimizer.zero_grad (sum) time: 0.10871624946594238
## e[87]       loss.backward (sum) time: 2.2703187465667725
## e[87]      optimizer.step (sum) time: 0.8741044998168945
## epoch[87] training(only) time: 10.415078401565552
# Switched to evaluate mode...
Test: [  0/100]	Time  0.158 ( 0.158)	Loss 2.0428e-01 (2.0428e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.011 ( 0.028)	Loss 3.8569e-01 (3.7144e-01)	Acc@1  91.00 ( 91.82)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.013 ( 0.022)	Loss 4.6129e-01 (4.0987e-01)	Acc@1  88.00 ( 90.71)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.011 ( 0.019)	Loss 3.7732e-01 (4.2907e-01)	Acc@1  89.00 ( 90.23)	Acc@5 100.00 ( 99.58)
Test: [ 40/100]	Time  0.016 ( 0.018)	Loss 6.8010e-01 (4.5286e-01)	Acc@1  88.00 ( 90.12)	Acc@5  99.00 ( 99.56)
Test: [ 50/100]	Time  0.011 ( 0.017)	Loss 2.0810e-01 (4.4547e-01)	Acc@1  95.00 ( 90.37)	Acc@5  99.00 ( 99.55)
Test: [ 60/100]	Time  0.011 ( 0.017)	Loss 4.6048e-01 (4.4174e-01)	Acc@1  90.00 ( 90.54)	Acc@5 100.00 ( 99.56)
Test: [ 70/100]	Time  0.015 ( 0.016)	Loss 8.3399e-01 (4.4060e-01)	Acc@1  90.00 ( 90.69)	Acc@5 100.00 ( 99.55)
Test: [ 80/100]	Time  0.011 ( 0.016)	Loss 4.1738e-01 (4.3897e-01)	Acc@1  93.00 ( 90.69)	Acc@5 100.00 ( 99.58)
Test: [ 90/100]	Time  0.013 ( 0.016)	Loss 2.9742e-01 (4.3532e-01)	Acc@1  94.00 ( 90.71)	Acc@5 100.00 ( 99.62)
 * Acc@1 90.830 Acc@5 99.620
### epoch[87] execution time: 12.141015768051147
EPOCH 88
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [88][  0/391]	Time  0.186 ( 0.186)	Data  0.168 ( 0.168)	Loss 1.1917e-02 (1.1917e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [88][ 10/391]	Time  0.023 ( 0.041)	Data  0.001 ( 0.017)	Loss 1.6714e-02 (1.1235e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [88][ 20/391]	Time  0.027 ( 0.034)	Data  0.001 ( 0.010)	Loss 2.3621e-03 (1.4070e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [88][ 30/391]	Time  0.033 ( 0.031)	Data  0.007 ( 0.008)	Loss 9.0174e-03 (1.3867e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [88][ 40/391]	Time  0.024 ( 0.030)	Data  0.001 ( 0.006)	Loss 4.9033e-03 (1.4086e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [88][ 50/391]	Time  0.023 ( 0.029)	Data  0.001 ( 0.005)	Loss 4.4245e-03 (1.4316e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [88][ 60/391]	Time  0.025 ( 0.028)	Data  0.005 ( 0.005)	Loss 4.3677e-02 (1.7690e-02)	Acc@1  98.44 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [88][ 70/391]	Time  0.029 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.5790e-02 (1.6759e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [88][ 80/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.2490e-02 (1.5879e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [88][ 90/391]	Time  0.029 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.4331e-02 (1.5653e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [88][100/391]	Time  0.033 ( 0.027)	Data  0.004 ( 0.004)	Loss 9.8010e-03 (1.5383e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [88][110/391]	Time  0.027 ( 0.027)	Data  0.002 ( 0.004)	Loss 7.6436e-03 (1.5533e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [88][120/391]	Time  0.028 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.0823e-02 (1.6142e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [88][130/391]	Time  0.031 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.1856e-03 (1.6098e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [88][140/391]	Time  0.030 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.8588e-03 (1.6706e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [88][150/391]	Time  0.028 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.6304e-02 (1.6919e-02)	Acc@1  98.44 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [88][160/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.8479e-03 (1.6792e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [88][170/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.7423e-02 (1.6645e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [88][180/391]	Time  0.032 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.9159e-03 (1.6131e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [88][190/391]	Time  0.030 ( 0.027)	Data  0.001 ( 0.003)	Loss 9.8175e-03 (1.6123e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [88][200/391]	Time  0.023 ( 0.027)	Data  0.000 ( 0.003)	Loss 5.3514e-03 (1.5942e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [88][210/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.5456e-03 (1.5870e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [88][220/391]	Time  0.038 ( 0.027)	Data  0.005 ( 0.003)	Loss 6.5617e-04 (1.6021e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [88][230/391]	Time  0.029 ( 0.027)	Data  0.002 ( 0.003)	Loss 4.8946e-03 (1.5872e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [88][240/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.6491e-03 (1.5660e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [88][250/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 1.8486e-02 (1.5447e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [88][260/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.0524e-02 (1.5460e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [88][270/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.2242e-02 (1.5293e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [88][280/391]	Time  0.024 ( 0.027)	Data  0.002 ( 0.003)	Loss 2.3617e-02 (1.5062e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [88][290/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.3485e-02 (1.5266e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [88][300/391]	Time  0.036 ( 0.027)	Data  0.003 ( 0.003)	Loss 2.2217e-03 (1.5120e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [88][310/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.6326e-04 (1.5160e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [88][320/391]	Time  0.023 ( 0.026)	Data  0.003 ( 0.003)	Loss 3.1036e-02 (1.5524e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [88][330/391]	Time  0.028 ( 0.026)	Data  0.002 ( 0.003)	Loss 5.4305e-03 (1.5651e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [88][340/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.9992e-03 (1.5876e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [88][350/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1088e-02 (1.5766e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [88][360/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3144e-02 (1.5725e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [88][370/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 7.6238e-03 (1.5593e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [88][380/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3130e-02 (1.5503e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [88][390/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.1921e-02 (1.5638e-02)	Acc@1  98.75 ( 99.47)	Acc@5 100.00 (100.00)
## e[88] optimizer.zero_grad (sum) time: 0.10908722877502441
## e[88]       loss.backward (sum) time: 2.2759156227111816
## e[88]      optimizer.step (sum) time: 0.8717975616455078
## epoch[88] training(only) time: 10.446564197540283
# Switched to evaluate mode...
Test: [  0/100]	Time  0.204 ( 0.204)	Loss 2.1013e-01 (2.1013e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.011 ( 0.032)	Loss 4.0048e-01 (3.7697e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.012 ( 0.023)	Loss 4.6333e-01 (4.1094e-01)	Acc@1  89.00 ( 91.19)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.015 ( 0.021)	Loss 3.7041e-01 (4.2749e-01)	Acc@1  89.00 ( 90.74)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.016 ( 0.019)	Loss 6.8250e-01 (4.5428e-01)	Acc@1  88.00 ( 90.46)	Acc@5  99.00 ( 99.59)
Test: [ 50/100]	Time  0.012 ( 0.018)	Loss 2.3032e-01 (4.4672e-01)	Acc@1  93.00 ( 90.53)	Acc@5  99.00 ( 99.53)
Test: [ 60/100]	Time  0.021 ( 0.017)	Loss 4.5379e-01 (4.4320e-01)	Acc@1  91.00 ( 90.72)	Acc@5 100.00 ( 99.54)
Test: [ 70/100]	Time  0.016 ( 0.017)	Loss 8.8656e-01 (4.4490e-01)	Acc@1  89.00 ( 90.77)	Acc@5  99.00 ( 99.51)
Test: [ 80/100]	Time  0.011 ( 0.017)	Loss 4.1241e-01 (4.4366e-01)	Acc@1  92.00 ( 90.77)	Acc@5 100.00 ( 99.54)
Test: [ 90/100]	Time  0.011 ( 0.016)	Loss 2.6927e-01 (4.4059e-01)	Acc@1  95.00 ( 90.77)	Acc@5 100.00 ( 99.58)
 * Acc@1 90.920 Acc@5 99.590
### epoch[88] execution time: 12.170453786849976
EPOCH 89
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [89][  0/391]	Time  0.192 ( 0.192)	Data  0.172 ( 0.172)	Loss 1.4277e-01 (1.4277e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [89][ 10/391]	Time  0.026 ( 0.041)	Data  0.001 ( 0.017)	Loss 8.4452e-03 (2.4689e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [89][ 20/391]	Time  0.022 ( 0.033)	Data  0.002 ( 0.010)	Loss 1.6007e-02 (2.0953e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [89][ 30/391]	Time  0.021 ( 0.031)	Data  0.001 ( 0.008)	Loss 3.5266e-03 (1.9055e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [89][ 40/391]	Time  0.023 ( 0.030)	Data  0.001 ( 0.006)	Loss 5.9543e-03 (1.8749e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [89][ 50/391]	Time  0.030 ( 0.029)	Data  0.001 ( 0.005)	Loss 2.2144e-03 (1.8009e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [89][ 60/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.005)	Loss 7.1091e-03 (1.7990e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [89][ 70/391]	Time  0.027 ( 0.028)	Data  0.001 ( 0.004)	Loss 3.4033e-02 (1.8295e-02)	Acc@1  98.44 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [89][ 80/391]	Time  0.026 ( 0.028)	Data  0.001 ( 0.004)	Loss 7.6216e-04 (1.6994e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [89][ 90/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 8.3550e-03 (1.6871e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [89][100/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 7.5192e-03 (1.7001e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [89][110/391]	Time  0.032 ( 0.027)	Data  0.002 ( 0.004)	Loss 8.0359e-03 (1.6945e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [89][120/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.4638e-02 (1.7163e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [89][130/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.7209e-02 (1.7191e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [89][140/391]	Time  0.035 ( 0.027)	Data  0.005 ( 0.003)	Loss 4.0425e-03 (1.6873e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [89][150/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.2747e-03 (1.6367e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [89][160/391]	Time  0.026 ( 0.027)	Data  0.002 ( 0.003)	Loss 7.0020e-03 (1.6539e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [89][170/391]	Time  0.029 ( 0.027)	Data  0.004 ( 0.003)	Loss 1.2100e-02 (1.6641e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [89][180/391]	Time  0.046 ( 0.027)	Data  0.014 ( 0.003)	Loss 6.7262e-03 (1.6927e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [89][190/391]	Time  0.027 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.0611e-03 (1.6790e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [89][200/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.8681e-02 (1.6887e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [89][210/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.6448e-02 (1.7293e-02)	Acc@1  97.66 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [89][220/391]	Time  0.026 ( 0.026)	Data  0.002 ( 0.003)	Loss 4.1497e-02 (1.7069e-02)	Acc@1  98.44 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [89][230/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1362e-03 (1.7282e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [89][240/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.4094e-03 (1.6849e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [89][250/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5801e-03 (1.6740e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [89][260/391]	Time  0.031 ( 0.026)	Data  0.000 ( 0.003)	Loss 5.2112e-03 (1.6863e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [89][270/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.8566e-03 (1.6540e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [89][280/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.9762e-03 (1.6428e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [89][290/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6884e-03 (1.6165e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [89][300/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1529e-02 (1.6452e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [89][310/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1693e-03 (1.6401e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [89][320/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.6553e-04 (1.6195e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [89][330/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1336e-02 (1.6229e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [89][340/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.9163e-03 (1.6165e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [89][350/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 8.8598e-03 (1.5949e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [89][360/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0565e-02 (1.5857e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [89][370/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.8927e-03 (1.6039e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [89][380/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.3388e-02 (1.5978e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [89][390/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8356e-02 (1.5862e-02)	Acc@1  98.75 ( 99.49)	Acc@5 100.00 (100.00)
## e[89] optimizer.zero_grad (sum) time: 0.10900259017944336
## e[89]       loss.backward (sum) time: 2.2734742164611816
## e[89]      optimizer.step (sum) time: 0.8737196922302246
## epoch[89] training(only) time: 10.373968601226807
# Switched to evaluate mode...
Test: [  0/100]	Time  0.169 ( 0.169)	Loss 2.0171e-01 (2.0171e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.014 ( 0.029)	Loss 3.9978e-01 (3.6981e-01)	Acc@1  90.00 ( 91.82)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.014 ( 0.022)	Loss 4.7012e-01 (4.0680e-01)	Acc@1  87.00 ( 90.90)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.022 ( 0.020)	Loss 3.9563e-01 (4.2579e-01)	Acc@1  89.00 ( 90.48)	Acc@5 100.00 ( 99.61)
Test: [ 40/100]	Time  0.014 ( 0.018)	Loss 6.7373e-01 (4.5014e-01)	Acc@1  88.00 ( 90.29)	Acc@5  99.00 ( 99.59)
Test: [ 50/100]	Time  0.011 ( 0.018)	Loss 1.9081e-01 (4.4279e-01)	Acc@1  94.00 ( 90.49)	Acc@5 100.00 ( 99.57)
Test: [ 60/100]	Time  0.010 ( 0.017)	Loss 5.0177e-01 (4.4005e-01)	Acc@1  89.00 ( 90.70)	Acc@5 100.00 ( 99.57)
Test: [ 70/100]	Time  0.016 ( 0.017)	Loss 8.6378e-01 (4.4028e-01)	Acc@1  90.00 ( 90.73)	Acc@5 100.00 ( 99.58)
Test: [ 80/100]	Time  0.011 ( 0.016)	Loss 4.0461e-01 (4.3887e-01)	Acc@1  92.00 ( 90.74)	Acc@5 100.00 ( 99.60)
Test: [ 90/100]	Time  0.011 ( 0.016)	Loss 2.7457e-01 (4.3490e-01)	Acc@1  95.00 ( 90.79)	Acc@5 100.00 ( 99.64)
 * Acc@1 90.910 Acc@5 99.640
### epoch[89] execution time: 12.100026607513428
### Training complete:
#### total training(only) time: 940.424872636795
##### Total run time: 1098.9674088954926
