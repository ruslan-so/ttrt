# Model: inception
# Dataset: cifardecem
# Batch size: 128
# Freezeout: False
# Low precision: False
# Epochs: 90
# Initial learning rate: 0.1
# module_name: models_cifardecem.inception
<function inception at 0x7f02fd938f28>
# model requested: 'inception'
# printing out the model
InceptionV3(
  (Conv2d_1a_3x3): BasicConv2d(
    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (Conv2d_2a_3x3): BasicConv2d(
    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (Conv2d_2b_3x3): BasicConv2d(
    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (Conv2d_3b_1x1): BasicConv2d(
    (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (Conv2d_4a_3x3): BasicConv2d(
    (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)
    (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (Mixed_5b): InceptionA(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch5x5): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branch3x3): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branchpool): Sequential(
      (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
      (1): BasicConv2d(
        (conv): Conv2d(192, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
  )
  (Mixed_5c): InceptionA(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch5x5): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branch3x3): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branchpool): Sequential(
      (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
      (1): BasicConv2d(
        (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
  )
  (Mixed_5d): InceptionA(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch5x5): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branch3x3): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branchpool): Sequential(
      (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
      (1): BasicConv2d(
        (conv): Conv2d(288, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
  )
  (Mixed_6a): InceptionB(
    (branch3x3): BasicConv2d(
      (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3stack): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branchpool): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (Mixed_6b): InceptionC(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch7x7): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branch7x7stack): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): BasicConv2d(
        (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): BasicConv2d(
        (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branch_pool): Sequential(
      (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
      (1): BasicConv2d(
        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
  )
  (Mixed_6c): InceptionC(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch7x7): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branch7x7stack): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): BasicConv2d(
        (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): BasicConv2d(
        (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branch_pool): Sequential(
      (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
      (1): BasicConv2d(
        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
  )
  (Mixed_6d): InceptionC(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch7x7): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branch7x7stack): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): BasicConv2d(
        (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): BasicConv2d(
        (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branch_pool): Sequential(
      (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
      (1): BasicConv2d(
        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
  )
  (Mixed_6e): InceptionC(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch7x7): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branch7x7stack): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): BasicConv2d(
        (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): BasicConv2d(
        (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branch_pool): Sequential(
      (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
      (1): BasicConv2d(
        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
  )
  (Mixed_7a): InceptionD(
    (branch3x3): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)
        (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branch7x7): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): BasicConv2d(
        (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branchpool): AvgPool2d(kernel_size=3, stride=2, padding=0)
  )
  (Mixed_7b): InceptionE(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3_1): BasicConv2d(
      (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3_2a): BasicConv2d(
      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3_2b): BasicConv2d(
      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3stack_1): BasicConv2d(
      (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3stack_2): BasicConv2d(
      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3stack_3a): BasicConv2d(
      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3stack_3b): BasicConv2d(
      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch_pool): Sequential(
      (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
      (1): BasicConv2d(
        (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
  )
  (Mixed_7c): InceptionE(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3_1): BasicConv2d(
      (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3_2a): BasicConv2d(
      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3_2b): BasicConv2d(
      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3stack_1): BasicConv2d(
      (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3stack_2): BasicConv2d(
      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3stack_3a): BasicConv2d(
      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3stack_3b): BasicConv2d(
      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch_pool): Sequential(
      (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
      (1): BasicConv2d(
        (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout2d(p=0.5, inplace=False)
  (linear): Linear(in_features=2048, out_features=10, bias=True)
)
# model is full precision
# Model: inception
# Dataset: cifardecem
# Freezeout: False
# Low precision: False
# Initial learning rate: 0.1
# Criterion: CrossEntropyLoss()
# Optimizer: SGD
#	lr 0.1
#	momentum 0.9
#	dampening 0
#	weight_decay 0.0001
#	nesterov False
CIFAR10 loading and normalization
==> Preparing data..
# CIFAR10 / full precision
Files already downloaded and verified
Files already downloaded and verified
#--> Training data: <--#
#-->>
EPOCH 0
# current learning rate: 0.1
# Switched to train mode...
Epoch: [0][  0/391]	Time  4.777 ( 4.777)	Data  0.121 ( 0.121)	Loss 2.3945e+00 (2.3945e+00)	Acc@1  10.94 ( 10.94)	Acc@5  52.34 ( 52.34)
Epoch: [0][ 10/391]	Time  0.236 ( 0.648)	Data  0.001 ( 0.012)	Loss 3.0643e+00 (3.1426e+00)	Acc@1  15.62 ( 12.00)	Acc@5  60.16 ( 52.34)
Epoch: [0][ 20/391]	Time  0.235 ( 0.452)	Data  0.001 ( 0.007)	Loss 2.5571e+00 (3.1643e+00)	Acc@1  14.84 ( 13.54)	Acc@5  64.84 ( 56.66)
Epoch: [0][ 30/391]	Time  0.234 ( 0.382)	Data  0.001 ( 0.005)	Loss 2.3390e+00 (2.9875e+00)	Acc@1  20.31 ( 14.52)	Acc@5  72.66 ( 60.66)
Epoch: [0][ 40/391]	Time  0.235 ( 0.346)	Data  0.001 ( 0.004)	Loss 2.1809e+00 (2.8064e+00)	Acc@1  18.75 ( 16.35)	Acc@5  72.66 ( 64.06)
Epoch: [0][ 50/391]	Time  0.236 ( 0.325)	Data  0.001 ( 0.004)	Loss 2.2481e+00 (2.6935e+00)	Acc@1  20.31 ( 16.99)	Acc@5  77.34 ( 66.04)
Epoch: [0][ 60/391]	Time  0.236 ( 0.310)	Data  0.001 ( 0.003)	Loss 2.2329e+00 (2.6129e+00)	Acc@1  20.31 ( 17.28)	Acc@5  79.69 ( 67.64)
Epoch: [0][ 70/391]	Time  0.236 ( 0.300)	Data  0.001 ( 0.003)	Loss 2.2451e+00 (2.5533e+00)	Acc@1  17.19 ( 17.50)	Acc@5  76.56 ( 69.23)
Epoch: [0][ 80/391]	Time  0.234 ( 0.292)	Data  0.001 ( 0.003)	Loss 1.9891e+00 (2.5018e+00)	Acc@1  25.78 ( 17.90)	Acc@5  80.47 ( 70.27)
Epoch: [0][ 90/391]	Time  0.235 ( 0.286)	Data  0.001 ( 0.002)	Loss 2.0898e+00 (2.4641e+00)	Acc@1  18.75 ( 18.11)	Acc@5  78.91 ( 71.03)
Epoch: [0][100/391]	Time  0.236 ( 0.281)	Data  0.001 ( 0.002)	Loss 2.2653e+00 (2.4278e+00)	Acc@1  21.09 ( 18.39)	Acc@5  84.38 ( 71.96)
Epoch: [0][110/391]	Time  0.236 ( 0.277)	Data  0.001 ( 0.002)	Loss 2.0298e+00 (2.3963e+00)	Acc@1  21.88 ( 18.49)	Acc@5  79.69 ( 72.83)
Epoch: [0][120/391]	Time  0.238 ( 0.273)	Data  0.001 ( 0.002)	Loss 2.1306e+00 (2.3768e+00)	Acc@1  24.22 ( 18.83)	Acc@5  79.69 ( 73.37)
Epoch: [0][130/391]	Time  0.237 ( 0.271)	Data  0.001 ( 0.002)	Loss 2.1405e+00 (2.3554e+00)	Acc@1  15.62 ( 19.07)	Acc@5  81.25 ( 73.91)
Epoch: [0][140/391]	Time  0.237 ( 0.268)	Data  0.001 ( 0.002)	Loss 2.0433e+00 (2.3389e+00)	Acc@1  22.66 ( 19.36)	Acc@5  86.72 ( 74.30)
Epoch: [0][150/391]	Time  0.237 ( 0.266)	Data  0.001 ( 0.002)	Loss 2.0576e+00 (2.3211e+00)	Acc@1  19.53 ( 19.50)	Acc@5  84.38 ( 74.79)
Epoch: [0][160/391]	Time  0.237 ( 0.264)	Data  0.001 ( 0.002)	Loss 1.9982e+00 (2.3034e+00)	Acc@1  22.66 ( 19.77)	Acc@5  80.47 ( 75.20)
Epoch: [0][170/391]	Time  0.237 ( 0.263)	Data  0.001 ( 0.002)	Loss 2.1122e+00 (2.2931e+00)	Acc@1  22.66 ( 19.78)	Acc@5  81.25 ( 75.29)
Epoch: [0][180/391]	Time  0.237 ( 0.261)	Data  0.001 ( 0.002)	Loss 2.0440e+00 (2.2802e+00)	Acc@1  25.00 ( 19.86)	Acc@5  82.81 ( 75.58)
Epoch: [0][190/391]	Time  0.237 ( 0.260)	Data  0.001 ( 0.002)	Loss 2.0344e+00 (2.2686e+00)	Acc@1  17.19 ( 20.01)	Acc@5  79.69 ( 75.87)
Epoch: [0][200/391]	Time  0.237 ( 0.259)	Data  0.001 ( 0.002)	Loss 1.9634e+00 (2.2575e+00)	Acc@1  21.09 ( 20.15)	Acc@5  84.38 ( 76.13)
Epoch: [0][210/391]	Time  0.237 ( 0.258)	Data  0.001 ( 0.002)	Loss 1.9696e+00 (2.2491e+00)	Acc@1  35.16 ( 20.43)	Acc@5  85.94 ( 76.37)
Epoch: [0][220/391]	Time  0.236 ( 0.257)	Data  0.001 ( 0.002)	Loss 1.9269e+00 (2.2378e+00)	Acc@1  30.47 ( 20.66)	Acc@5  84.38 ( 76.65)
Epoch: [0][230/391]	Time  0.237 ( 0.256)	Data  0.001 ( 0.002)	Loss 1.9148e+00 (2.2285e+00)	Acc@1  27.34 ( 20.86)	Acc@5  85.16 ( 76.86)
Epoch: [0][240/391]	Time  0.238 ( 0.255)	Data  0.001 ( 0.002)	Loss 1.9363e+00 (2.2167e+00)	Acc@1  23.44 ( 21.07)	Acc@5  82.81 ( 77.08)
Epoch: [0][250/391]	Time  0.237 ( 0.255)	Data  0.001 ( 0.002)	Loss 2.0473e+00 (2.2079e+00)	Acc@1  28.12 ( 21.28)	Acc@5  81.25 ( 77.27)
Epoch: [0][260/391]	Time  0.238 ( 0.254)	Data  0.001 ( 0.002)	Loss 1.9507e+00 (2.1991e+00)	Acc@1  24.22 ( 21.46)	Acc@5  78.12 ( 77.42)
Epoch: [0][270/391]	Time  0.238 ( 0.253)	Data  0.001 ( 0.002)	Loss 2.0480e+00 (2.1917e+00)	Acc@1  23.44 ( 21.61)	Acc@5  79.69 ( 77.63)
Epoch: [0][280/391]	Time  0.238 ( 0.253)	Data  0.001 ( 0.002)	Loss 2.0453e+00 (2.1854e+00)	Acc@1  27.34 ( 21.76)	Acc@5  78.12 ( 77.76)
Epoch: [0][290/391]	Time  0.241 ( 0.252)	Data  0.001 ( 0.002)	Loss 1.9900e+00 (2.1795e+00)	Acc@1  22.66 ( 21.88)	Acc@5  87.50 ( 77.92)
Epoch: [0][300/391]	Time  0.239 ( 0.252)	Data  0.001 ( 0.001)	Loss 2.0009e+00 (2.1718e+00)	Acc@1  34.38 ( 22.11)	Acc@5  86.72 ( 78.10)
Epoch: [0][310/391]	Time  0.239 ( 0.252)	Data  0.001 ( 0.001)	Loss 1.8181e+00 (2.1659e+00)	Acc@1  31.25 ( 22.24)	Acc@5  85.94 ( 78.21)
Epoch: [0][320/391]	Time  0.238 ( 0.251)	Data  0.001 ( 0.001)	Loss 1.9435e+00 (2.1600e+00)	Acc@1  27.34 ( 22.34)	Acc@5  87.50 ( 78.30)
Epoch: [0][330/391]	Time  0.239 ( 0.251)	Data  0.001 ( 0.001)	Loss 2.0040e+00 (2.1537e+00)	Acc@1  28.91 ( 22.50)	Acc@5  85.94 ( 78.48)
Epoch: [0][340/391]	Time  0.239 ( 0.250)	Data  0.001 ( 0.001)	Loss 2.0000e+00 (2.1476e+00)	Acc@1  29.69 ( 22.68)	Acc@5  85.94 ( 78.64)
Epoch: [0][350/391]	Time  0.238 ( 0.250)	Data  0.001 ( 0.001)	Loss 1.9700e+00 (2.1413e+00)	Acc@1  27.34 ( 22.83)	Acc@5  85.16 ( 78.76)
Epoch: [0][360/391]	Time  0.240 ( 0.250)	Data  0.001 ( 0.001)	Loss 1.9450e+00 (2.1353e+00)	Acc@1  29.69 ( 23.04)	Acc@5  81.25 ( 78.89)
Epoch: [0][370/391]	Time  0.239 ( 0.249)	Data  0.001 ( 0.001)	Loss 1.9761e+00 (2.1294e+00)	Acc@1  28.12 ( 23.29)	Acc@5  81.25 ( 79.05)
Epoch: [0][380/391]	Time  0.239 ( 0.249)	Data  0.001 ( 0.001)	Loss 1.7506e+00 (2.1234e+00)	Acc@1  29.69 ( 23.46)	Acc@5  90.62 ( 79.20)
Epoch: [0][390/391]	Time  1.805 ( 0.253)	Data  0.001 ( 0.001)	Loss 1.9712e+00 (2.1174e+00)	Acc@1  38.75 ( 23.64)	Acc@5  80.00 ( 79.33)
## e[0] optimizer.zero_grad (sum) time: 0.48267388343811035
## e[0]       loss.backward (sum) time: 13.04363751411438
## e[0]      optimizer.step (sum) time: 46.442903995513916
## epoch[0] training(only) time: 98.95680356025696
# Switched to evaluate mode...
Test: [  0/100]	Time  0.782 ( 0.782)	Loss 1.7265e+00 (1.7265e+00)	Acc@1  38.00 ( 38.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.078 ( 0.147)	Loss 1.7354e+00 (1.8151e+00)	Acc@1  29.00 ( 33.45)	Acc@5  88.00 ( 86.91)
Test: [ 20/100]	Time  0.077 ( 0.114)	Loss 1.7703e+00 (1.8140e+00)	Acc@1  35.00 ( 33.48)	Acc@5  86.00 ( 86.90)
Test: [ 30/100]	Time  0.077 ( 0.102)	Loss 1.7069e+00 (1.8220e+00)	Acc@1  33.00 ( 32.68)	Acc@5  88.00 ( 86.52)
Test: [ 40/100]	Time  0.078 ( 0.096)	Loss 1.7789e+00 (1.8107e+00)	Acc@1  32.00 ( 32.49)	Acc@5  82.00 ( 86.32)
Test: [ 50/100]	Time  0.077 ( 0.093)	Loss 1.8559e+00 (1.8109e+00)	Acc@1  31.00 ( 32.49)	Acc@5  84.00 ( 86.35)
Test: [ 60/100]	Time  0.077 ( 0.090)	Loss 1.7505e+00 (1.8092e+00)	Acc@1  32.00 ( 32.79)	Acc@5  85.00 ( 86.34)
Test: [ 70/100]	Time  0.080 ( 0.088)	Loss 1.9322e+00 (1.8244e+00)	Acc@1  26.00 ( 32.11)	Acc@5  79.00 ( 86.25)
Test: [ 80/100]	Time  0.078 ( 0.087)	Loss 1.6704e+00 (1.8126e+00)	Acc@1  37.00 ( 31.96)	Acc@5  86.00 ( 86.52)
Test: [ 90/100]	Time  0.077 ( 0.086)	Loss 1.6728e+00 (1.8324e+00)	Acc@1  31.00 ( 31.99)	Acc@5  93.00 ( 86.41)
 * Acc@1 32.060 Acc@5 86.290
### epoch[0] execution time: 107.57194423675537
EPOCH 1
# current learning rate: 0.1
# Switched to train mode...
Epoch: [1][  0/391]	Time  0.398 ( 0.398)	Data  0.152 ( 0.152)	Loss 1.8229e+00 (1.8229e+00)	Acc@1  28.12 ( 28.12)	Acc@5  86.72 ( 86.72)
Epoch: [1][ 10/391]	Time  0.239 ( 0.253)	Data  0.001 ( 0.015)	Loss 1.6515e+00 (1.8221e+00)	Acc@1  28.91 ( 30.11)	Acc@5  92.19 ( 84.87)
Epoch: [1][ 20/391]	Time  0.237 ( 0.247)	Data  0.001 ( 0.008)	Loss 1.8928e+00 (1.8290e+00)	Acc@1  25.78 ( 30.77)	Acc@5  85.94 ( 84.90)
Epoch: [1][ 30/391]	Time  0.239 ( 0.244)	Data  0.001 ( 0.006)	Loss 1.7199e+00 (1.8287e+00)	Acc@1  39.84 ( 31.60)	Acc@5  86.72 ( 84.48)
Epoch: [1][ 40/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.005)	Loss 1.8543e+00 (1.8349e+00)	Acc@1  32.03 ( 31.44)	Acc@5  82.03 ( 84.22)
Epoch: [1][ 50/391]	Time  0.238 ( 0.242)	Data  0.001 ( 0.004)	Loss 1.7694e+00 (1.8426e+00)	Acc@1  35.94 ( 31.25)	Acc@5  89.06 ( 84.33)
Epoch: [1][ 60/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.004)	Loss 1.7561e+00 (1.8457e+00)	Acc@1  34.38 ( 31.12)	Acc@5  86.72 ( 84.49)
Epoch: [1][ 70/391]	Time  0.244 ( 0.241)	Data  0.001 ( 0.003)	Loss 1.8273e+00 (1.8480e+00)	Acc@1  32.81 ( 31.14)	Acc@5  85.94 ( 84.74)
Epoch: [1][ 80/391]	Time  0.238 ( 0.241)	Data  0.001 ( 0.003)	Loss 1.7795e+00 (1.8505e+00)	Acc@1  36.72 ( 31.10)	Acc@5  89.84 ( 84.72)
Epoch: [1][ 90/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.003)	Loss 1.8068e+00 (1.8523e+00)	Acc@1  38.28 ( 31.10)	Acc@5  83.59 ( 84.69)
Epoch: [1][100/391]	Time  0.238 ( 0.241)	Data  0.001 ( 0.003)	Loss 1.6899e+00 (1.8521e+00)	Acc@1  39.06 ( 31.24)	Acc@5  89.06 ( 84.77)
Epoch: [1][110/391]	Time  0.243 ( 0.241)	Data  0.001 ( 0.003)	Loss 1.6886e+00 (1.8431e+00)	Acc@1  34.38 ( 31.62)	Acc@5  87.50 ( 85.00)
Epoch: [1][120/391]	Time  0.238 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.7931e+00 (1.8437e+00)	Acc@1  35.94 ( 31.73)	Acc@5  82.03 ( 85.01)
Epoch: [1][130/391]	Time  0.239 ( 0.240)	Data  0.001 ( 0.002)	Loss 1.7004e+00 (1.8398e+00)	Acc@1  36.72 ( 31.81)	Acc@5  88.28 ( 85.12)
Epoch: [1][140/391]	Time  0.240 ( 0.240)	Data  0.001 ( 0.002)	Loss 1.6571e+00 (1.8369e+00)	Acc@1  40.62 ( 32.01)	Acc@5  89.84 ( 85.23)
Epoch: [1][150/391]	Time  0.239 ( 0.240)	Data  0.001 ( 0.002)	Loss 1.6931e+00 (1.8338e+00)	Acc@1  32.03 ( 32.17)	Acc@5  91.41 ( 85.30)
Epoch: [1][160/391]	Time  0.242 ( 0.240)	Data  0.001 ( 0.002)	Loss 1.8720e+00 (1.8343e+00)	Acc@1  30.47 ( 32.23)	Acc@5  83.59 ( 85.29)
Epoch: [1][170/391]	Time  0.239 ( 0.240)	Data  0.001 ( 0.002)	Loss 1.7364e+00 (1.8306e+00)	Acc@1  33.59 ( 32.38)	Acc@5  85.16 ( 85.34)
Epoch: [1][180/391]	Time  0.239 ( 0.240)	Data  0.001 ( 0.002)	Loss 1.6765e+00 (1.8275e+00)	Acc@1  32.03 ( 32.39)	Acc@5  89.06 ( 85.49)
Epoch: [1][190/391]	Time  0.239 ( 0.240)	Data  0.001 ( 0.002)	Loss 1.7678e+00 (1.8239e+00)	Acc@1  37.50 ( 32.58)	Acc@5  87.50 ( 85.54)
Epoch: [1][200/391]	Time  0.238 ( 0.240)	Data  0.001 ( 0.002)	Loss 1.8415e+00 (1.8204e+00)	Acc@1  35.16 ( 32.80)	Acc@5  76.56 ( 85.62)
Epoch: [1][210/391]	Time  0.244 ( 0.240)	Data  0.001 ( 0.002)	Loss 1.6881e+00 (1.8142e+00)	Acc@1  40.62 ( 33.00)	Acc@5  88.28 ( 85.72)
Epoch: [1][220/391]	Time  0.239 ( 0.240)	Data  0.001 ( 0.002)	Loss 1.8649e+00 (1.8116e+00)	Acc@1  28.12 ( 33.08)	Acc@5  87.50 ( 85.78)
Epoch: [1][230/391]	Time  0.239 ( 0.240)	Data  0.001 ( 0.002)	Loss 1.5621e+00 (1.8049e+00)	Acc@1  42.19 ( 33.32)	Acc@5  91.41 ( 85.92)
Epoch: [1][240/391]	Time  0.239 ( 0.240)	Data  0.001 ( 0.002)	Loss 1.8828e+00 (1.8007e+00)	Acc@1  25.78 ( 33.40)	Acc@5  85.16 ( 86.03)
Epoch: [1][250/391]	Time  0.238 ( 0.240)	Data  0.001 ( 0.002)	Loss 1.7078e+00 (1.7959e+00)	Acc@1  35.94 ( 33.58)	Acc@5  88.28 ( 86.14)
Epoch: [1][260/391]	Time  0.246 ( 0.240)	Data  0.001 ( 0.002)	Loss 1.7451e+00 (1.7916e+00)	Acc@1  37.50 ( 33.76)	Acc@5  88.28 ( 86.22)
Epoch: [1][270/391]	Time  0.241 ( 0.240)	Data  0.001 ( 0.002)	Loss 1.6522e+00 (1.7870e+00)	Acc@1  42.19 ( 33.94)	Acc@5  87.50 ( 86.34)
Epoch: [1][280/391]	Time  0.239 ( 0.240)	Data  0.001 ( 0.002)	Loss 1.5819e+00 (1.7833e+00)	Acc@1  41.41 ( 34.05)	Acc@5  90.62 ( 86.42)
Epoch: [1][290/391]	Time  0.240 ( 0.240)	Data  0.001 ( 0.002)	Loss 1.5229e+00 (1.7784e+00)	Acc@1  46.88 ( 34.27)	Acc@5  89.06 ( 86.49)
Epoch: [1][300/391]	Time  0.239 ( 0.240)	Data  0.001 ( 0.002)	Loss 1.5544e+00 (1.7736e+00)	Acc@1  39.84 ( 34.43)	Acc@5  93.75 ( 86.63)
Epoch: [1][310/391]	Time  0.247 ( 0.240)	Data  0.001 ( 0.002)	Loss 1.6379e+00 (1.7702e+00)	Acc@1  35.16 ( 34.55)	Acc@5  89.84 ( 86.71)
Epoch: [1][320/391]	Time  0.239 ( 0.240)	Data  0.001 ( 0.002)	Loss 1.6029e+00 (1.7666e+00)	Acc@1  36.72 ( 34.66)	Acc@5  90.62 ( 86.82)
Epoch: [1][330/391]	Time  0.240 ( 0.240)	Data  0.001 ( 0.002)	Loss 1.7574e+00 (1.7636e+00)	Acc@1  36.72 ( 34.81)	Acc@5  86.72 ( 86.89)
Epoch: [1][340/391]	Time  0.241 ( 0.240)	Data  0.001 ( 0.002)	Loss 1.6066e+00 (1.7602e+00)	Acc@1  35.94 ( 34.93)	Acc@5  89.06 ( 86.93)
Epoch: [1][350/391]	Time  0.240 ( 0.240)	Data  0.001 ( 0.002)	Loss 1.4455e+00 (1.7557e+00)	Acc@1  49.22 ( 35.11)	Acc@5  90.62 ( 87.01)
Epoch: [1][360/391]	Time  0.243 ( 0.240)	Data  0.001 ( 0.002)	Loss 1.6138e+00 (1.7526e+00)	Acc@1  41.41 ( 35.25)	Acc@5  90.62 ( 87.02)
Epoch: [1][370/391]	Time  0.240 ( 0.240)	Data  0.001 ( 0.002)	Loss 1.9636e+00 (1.7506e+00)	Acc@1  30.47 ( 35.39)	Acc@5  84.38 ( 87.07)
Epoch: [1][380/391]	Time  0.240 ( 0.240)	Data  0.001 ( 0.002)	Loss 1.7374e+00 (1.7472e+00)	Acc@1  35.16 ( 35.50)	Acc@5  91.41 ( 87.16)
Epoch: [1][390/391]	Time  0.176 ( 0.240)	Data  0.001 ( 0.002)	Loss 1.6114e+00 (1.7449e+00)	Acc@1  38.75 ( 35.57)	Acc@5  88.75 ( 87.20)
## e[1] optimizer.zero_grad (sum) time: 0.47692251205444336
## e[1]       loss.backward (sum) time: 10.912862062454224
## e[1]      optimizer.step (sum) time: 47.150853395462036
## epoch[1] training(only) time: 93.89025688171387
# Switched to evaluate mode...
Test: [  0/100]	Time  0.201 ( 0.201)	Loss 1.4615e+00 (1.4615e+00)	Acc@1  44.00 ( 44.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.078 ( 0.089)	Loss 1.3721e+00 (1.5242e+00)	Acc@1  50.00 ( 43.18)	Acc@5  91.00 ( 90.64)
Test: [ 20/100]	Time  0.077 ( 0.084)	Loss 1.5154e+00 (1.5147e+00)	Acc@1  46.00 ( 44.67)	Acc@5  91.00 ( 91.24)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 1.3375e+00 (1.5391e+00)	Acc@1  49.00 ( 43.35)	Acc@5  96.00 ( 91.32)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.8309e+00 (1.5369e+00)	Acc@1  35.00 ( 43.80)	Acc@5  83.00 ( 91.05)
Test: [ 50/100]	Time  0.078 ( 0.080)	Loss 1.5314e+00 (1.5292e+00)	Acc@1  37.00 ( 44.18)	Acc@5  92.00 ( 91.08)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.6496e+00 (1.5431e+00)	Acc@1  36.00 ( 43.61)	Acc@5  92.00 ( 91.05)
Test: [ 70/100]	Time  0.077 ( 0.080)	Loss 1.5540e+00 (1.5583e+00)	Acc@1  37.00 ( 42.83)	Acc@5  90.00 ( 91.10)
Test: [ 80/100]	Time  0.077 ( 0.079)	Loss 1.4246e+00 (1.5441e+00)	Acc@1  49.00 ( 43.15)	Acc@5  91.00 ( 91.26)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.4265e+00 (1.5497e+00)	Acc@1  41.00 ( 42.77)	Acc@5  92.00 ( 91.13)
 * Acc@1 42.890 Acc@5 91.100
### epoch[1] execution time: 101.88603138923645
EPOCH 2
# current learning rate: 0.1
# Switched to train mode...
Epoch: [2][  0/391]	Time  0.390 ( 0.390)	Data  0.149 ( 0.149)	Loss 1.6243e+00 (1.6243e+00)	Acc@1  37.50 ( 37.50)	Acc@5  86.72 ( 86.72)
Epoch: [2][ 10/391]	Time  0.239 ( 0.254)	Data  0.001 ( 0.015)	Loss 1.6075e+00 (1.6514e+00)	Acc@1  38.28 ( 38.57)	Acc@5  90.62 ( 89.56)
Epoch: [2][ 20/391]	Time  0.239 ( 0.247)	Data  0.001 ( 0.008)	Loss 1.5525e+00 (1.6399e+00)	Acc@1  41.41 ( 39.25)	Acc@5  91.41 ( 89.92)
Epoch: [2][ 30/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.006)	Loss 1.6509e+00 (1.6273e+00)	Acc@1  35.94 ( 40.02)	Acc@5  89.06 ( 89.84)
Epoch: [2][ 40/391]	Time  0.239 ( 0.244)	Data  0.001 ( 0.005)	Loss 1.6491e+00 (1.6263e+00)	Acc@1  36.72 ( 39.75)	Acc@5  89.06 ( 89.60)
Epoch: [2][ 50/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.004)	Loss 1.6381e+00 (1.6069e+00)	Acc@1  38.28 ( 40.59)	Acc@5  85.94 ( 89.72)
Epoch: [2][ 60/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.004)	Loss 1.5543e+00 (1.5929e+00)	Acc@1  42.97 ( 41.14)	Acc@5  90.62 ( 89.97)
Epoch: [2][ 70/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.7476e+00 (1.5908e+00)	Acc@1  36.72 ( 41.46)	Acc@5  88.28 ( 89.90)
Epoch: [2][ 80/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.5363e+00 (1.5941e+00)	Acc@1  46.88 ( 41.65)	Acc@5  89.06 ( 89.88)
Epoch: [2][ 90/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.5806e+00 (1.5922e+00)	Acc@1  45.31 ( 41.78)	Acc@5  89.84 ( 89.92)
Epoch: [2][100/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.003)	Loss 1.4097e+00 (1.5901e+00)	Acc@1  48.44 ( 41.76)	Acc@5  95.31 ( 89.96)
Epoch: [2][110/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.5374e+00 (1.5893e+00)	Acc@1  41.41 ( 41.86)	Acc@5  89.06 ( 89.95)
Epoch: [2][120/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.6054e+00 (1.5837e+00)	Acc@1  42.97 ( 42.16)	Acc@5  92.19 ( 89.97)
Epoch: [2][130/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.5552e+00 (1.5802e+00)	Acc@1  43.75 ( 42.27)	Acc@5  87.50 ( 90.02)
Epoch: [2][140/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.6443e+00 (1.5788e+00)	Acc@1  35.94 ( 42.17)	Acc@5  89.06 ( 90.09)
Epoch: [2][150/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.5659e+00 (1.5724e+00)	Acc@1  40.62 ( 42.31)	Acc@5  90.62 ( 90.22)
Epoch: [2][160/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.5485e+00 (1.5709e+00)	Acc@1  45.31 ( 42.36)	Acc@5  90.62 ( 90.27)
Epoch: [2][170/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.5873e+00 (1.5684e+00)	Acc@1  43.75 ( 42.43)	Acc@5  89.84 ( 90.25)
Epoch: [2][180/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.5940e+00 (1.5645e+00)	Acc@1  41.41 ( 42.58)	Acc@5  95.31 ( 90.38)
Epoch: [2][190/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.5895e+00 (1.5636e+00)	Acc@1  40.62 ( 42.60)	Acc@5  89.84 ( 90.37)
Epoch: [2][200/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.4501e+00 (1.5613e+00)	Acc@1  46.09 ( 42.66)	Acc@5  92.19 ( 90.41)
Epoch: [2][210/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.5253e+00 (1.5558e+00)	Acc@1  46.88 ( 42.84)	Acc@5  92.19 ( 90.49)
Epoch: [2][220/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.4933e+00 (1.5526e+00)	Acc@1  45.31 ( 42.96)	Acc@5  91.41 ( 90.55)
Epoch: [2][230/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.4566e+00 (1.5511e+00)	Acc@1  43.75 ( 42.99)	Acc@5  96.09 ( 90.62)
Epoch: [2][240/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.5760e+00 (1.5502e+00)	Acc@1  41.41 ( 42.98)	Acc@5  89.84 ( 90.64)
Epoch: [2][250/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.3908e+00 (1.5467e+00)	Acc@1  43.75 ( 43.12)	Acc@5  97.66 ( 90.71)
Epoch: [2][260/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.4300e+00 (1.5443e+00)	Acc@1  44.53 ( 43.22)	Acc@5  90.62 ( 90.76)
Epoch: [2][270/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.4655e+00 (1.5438e+00)	Acc@1  50.00 ( 43.26)	Acc@5  91.41 ( 90.75)
Epoch: [2][280/391]	Time  0.248 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.4295e+00 (1.5430e+00)	Acc@1  47.66 ( 43.24)	Acc@5  90.62 ( 90.79)
Epoch: [2][290/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.6656e+00 (1.5416e+00)	Acc@1  34.38 ( 43.27)	Acc@5  89.84 ( 90.83)
Epoch: [2][300/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.5700e+00 (1.5407e+00)	Acc@1  37.50 ( 43.32)	Acc@5  92.97 ( 90.87)
Epoch: [2][310/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2746e+00 (1.5369e+00)	Acc@1  60.16 ( 43.42)	Acc@5  92.97 ( 90.91)
Epoch: [2][320/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.4669e+00 (1.5355e+00)	Acc@1  45.31 ( 43.51)	Acc@5  95.31 ( 90.94)
Epoch: [2][330/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.5210e+00 (1.5325e+00)	Acc@1  43.75 ( 43.62)	Acc@5  89.06 ( 90.94)
Epoch: [2][340/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.4894e+00 (1.5293e+00)	Acc@1  46.88 ( 43.83)	Acc@5  86.72 ( 90.97)
Epoch: [2][350/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.6172e+00 (1.5280e+00)	Acc@1  41.41 ( 43.87)	Acc@5  90.62 ( 90.99)
Epoch: [2][360/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.4445e+00 (1.5254e+00)	Acc@1  45.31 ( 43.94)	Acc@5  92.19 ( 91.03)
Epoch: [2][370/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.4819e+00 (1.5223e+00)	Acc@1  48.44 ( 44.07)	Acc@5  92.19 ( 91.05)
Epoch: [2][380/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.4972e+00 (1.5212e+00)	Acc@1  45.31 ( 44.14)	Acc@5  92.97 ( 91.06)
Epoch: [2][390/391]	Time  0.177 ( 0.240)	Data  0.001 ( 0.002)	Loss 1.3304e+00 (1.5205e+00)	Acc@1  51.25 ( 44.16)	Acc@5  92.50 ( 91.07)
## e[2] optimizer.zero_grad (sum) time: 0.47643327713012695
## e[2]       loss.backward (sum) time: 10.83318042755127
## e[2]      optimizer.step (sum) time: 47.430325984954834
## epoch[2] training(only) time: 94.12078905105591
# Switched to evaluate mode...
Test: [  0/100]	Time  0.209 ( 0.209)	Loss 1.4545e+00 (1.4545e+00)	Acc@1  52.00 ( 52.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.4077e+00 (1.5291e+00)	Acc@1  51.00 ( 44.18)	Acc@5  94.00 ( 92.09)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 1.5640e+00 (1.5411e+00)	Acc@1  47.00 ( 44.19)	Acc@5  92.00 ( 91.57)
Test: [ 30/100]	Time  0.079 ( 0.082)	Loss 1.3172e+00 (1.5521e+00)	Acc@1  53.00 ( 43.65)	Acc@5  91.00 ( 91.29)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.8355e+00 (1.5634e+00)	Acc@1  36.00 ( 43.46)	Acc@5  90.00 ( 91.15)
Test: [ 50/100]	Time  0.078 ( 0.080)	Loss 1.7645e+00 (1.5574e+00)	Acc@1  32.00 ( 43.43)	Acc@5  88.00 ( 91.31)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.6009e+00 (1.5614e+00)	Acc@1  37.00 ( 43.34)	Acc@5  92.00 ( 91.11)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 1.7005e+00 (1.5667e+00)	Acc@1  43.00 ( 43.10)	Acc@5  93.00 ( 91.10)
Test: [ 80/100]	Time  0.078 ( 0.079)	Loss 1.3688e+00 (1.5612e+00)	Acc@1  49.00 ( 43.40)	Acc@5  91.00 ( 91.14)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.4845e+00 (1.5611e+00)	Acc@1  41.00 ( 43.34)	Acc@5  95.00 ( 91.15)
 * Acc@1 43.540 Acc@5 91.140
### epoch[2] execution time: 102.12622427940369
EPOCH 3
# current learning rate: 0.1
# Switched to train mode...
Epoch: [3][  0/391]	Time  0.401 ( 0.401)	Data  0.153 ( 0.153)	Loss 1.5868e+00 (1.5868e+00)	Acc@1  41.41 ( 41.41)	Acc@5  85.94 ( 85.94)
Epoch: [3][ 10/391]	Time  0.239 ( 0.255)	Data  0.001 ( 0.015)	Loss 1.4207e+00 (1.4632e+00)	Acc@1  52.34 ( 47.59)	Acc@5  92.97 ( 90.91)
Epoch: [3][ 20/391]	Time  0.240 ( 0.248)	Data  0.001 ( 0.008)	Loss 1.3619e+00 (1.4424e+00)	Acc@1  53.12 ( 47.84)	Acc@5  94.53 ( 92.00)
Epoch: [3][ 30/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.006)	Loss 1.5205e+00 (1.4410e+00)	Acc@1  45.31 ( 47.78)	Acc@5  92.97 ( 91.76)
Epoch: [3][ 40/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.005)	Loss 1.2263e+00 (1.4203e+00)	Acc@1  52.34 ( 48.51)	Acc@5  94.53 ( 92.32)
Epoch: [3][ 50/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.4905e+00 (1.4272e+00)	Acc@1  47.66 ( 48.42)	Acc@5  92.19 ( 92.45)
Epoch: [3][ 60/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.004)	Loss 1.3206e+00 (1.4127e+00)	Acc@1  53.12 ( 49.03)	Acc@5  94.53 ( 92.58)
Epoch: [3][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.5020e+00 (1.4098e+00)	Acc@1  46.09 ( 49.01)	Acc@5  89.84 ( 92.67)
Epoch: [3][ 80/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.4279e+00 (1.4147e+00)	Acc@1  42.19 ( 48.65)	Acc@5  93.75 ( 92.62)
Epoch: [3][ 90/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.4287e+00 (1.4210e+00)	Acc@1  51.56 ( 48.48)	Acc@5  92.97 ( 92.52)
Epoch: [3][100/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.5944e+00 (1.4241e+00)	Acc@1  43.75 ( 48.15)	Acc@5  91.41 ( 92.54)
Epoch: [3][110/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.3691e+00 (1.4203e+00)	Acc@1  50.00 ( 48.21)	Acc@5  96.88 ( 92.69)
Epoch: [3][120/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5448e+00 (1.4152e+00)	Acc@1  44.53 ( 48.36)	Acc@5  88.28 ( 92.68)
Epoch: [3][130/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.4989e+00 (1.4150e+00)	Acc@1  42.97 ( 48.43)	Acc@5  91.41 ( 92.64)
Epoch: [3][140/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.4781e+00 (1.4156e+00)	Acc@1  45.31 ( 48.40)	Acc@5  91.41 ( 92.57)
Epoch: [3][150/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.3790e+00 (1.4128e+00)	Acc@1  54.69 ( 48.58)	Acc@5  91.41 ( 92.53)
Epoch: [3][160/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1792e+00 (1.4043e+00)	Acc@1  53.91 ( 48.90)	Acc@5  94.53 ( 92.58)
Epoch: [3][170/391]	Time  0.244 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.4115e+00 (1.4026e+00)	Acc@1  51.56 ( 48.91)	Acc@5  93.75 ( 92.64)
Epoch: [3][180/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2345e+00 (1.4024e+00)	Acc@1  53.12 ( 48.84)	Acc@5  94.53 ( 92.67)
Epoch: [3][190/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.4229e+00 (1.3993e+00)	Acc@1  46.09 ( 48.97)	Acc@5  89.06 ( 92.67)
Epoch: [3][200/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2064e+00 (1.3954e+00)	Acc@1  57.03 ( 49.16)	Acc@5  93.75 ( 92.71)
Epoch: [3][210/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.3280e+00 (1.3904e+00)	Acc@1  49.22 ( 49.37)	Acc@5  96.88 ( 92.82)
Epoch: [3][220/391]	Time  0.243 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.3962e+00 (1.3914e+00)	Acc@1  46.09 ( 49.35)	Acc@5  91.41 ( 92.81)
Epoch: [3][230/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.4256e+00 (1.3896e+00)	Acc@1  50.78 ( 49.42)	Acc@5  89.84 ( 92.80)
Epoch: [3][240/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2726e+00 (1.3897e+00)	Acc@1  49.22 ( 49.46)	Acc@5  97.66 ( 92.82)
Epoch: [3][250/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.3922e+00 (1.3867e+00)	Acc@1  50.78 ( 49.59)	Acc@5  94.53 ( 92.82)
Epoch: [3][260/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.3568e+00 (1.3842e+00)	Acc@1  53.12 ( 49.76)	Acc@5  94.53 ( 92.86)
Epoch: [3][270/391]	Time  0.248 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2789e+00 (1.3831e+00)	Acc@1  53.12 ( 49.83)	Acc@5  92.19 ( 92.85)
Epoch: [3][280/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2078e+00 (1.3808e+00)	Acc@1  58.59 ( 50.00)	Acc@5  96.09 ( 92.89)
Epoch: [3][290/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.3139e+00 (1.3786e+00)	Acc@1  52.34 ( 50.09)	Acc@5  95.31 ( 92.89)
Epoch: [3][300/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.3813e+00 (1.3766e+00)	Acc@1  53.12 ( 50.18)	Acc@5  91.41 ( 92.91)
Epoch: [3][310/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.8812e-01 (1.3725e+00)	Acc@1  64.06 ( 50.36)	Acc@5  98.44 ( 92.96)
Epoch: [3][320/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1997e+00 (1.3690e+00)	Acc@1  59.38 ( 50.49)	Acc@5  94.53 ( 93.01)
Epoch: [3][330/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2795e+00 (1.3659e+00)	Acc@1  50.00 ( 50.60)	Acc@5  95.31 ( 93.07)
Epoch: [3][340/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0465e+00 (1.3627e+00)	Acc@1  60.16 ( 50.75)	Acc@5  95.31 ( 93.09)
Epoch: [3][350/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.3817e+00 (1.3598e+00)	Acc@1  50.00 ( 50.81)	Acc@5  91.41 ( 93.12)
Epoch: [3][360/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.3518e+00 (1.3573e+00)	Acc@1  50.78 ( 50.93)	Acc@5  91.41 ( 93.13)
Epoch: [3][370/391]	Time  0.249 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2885e+00 (1.3540e+00)	Acc@1  50.00 ( 51.05)	Acc@5  92.97 ( 93.16)
Epoch: [3][380/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1243e+00 (1.3506e+00)	Acc@1  63.28 ( 51.18)	Acc@5  95.31 ( 93.17)
Epoch: [3][390/391]	Time  0.176 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2214e+00 (1.3490e+00)	Acc@1  56.25 ( 51.23)	Acc@5  93.75 ( 93.19)
## e[3] optimizer.zero_grad (sum) time: 0.47692036628723145
## e[3]       loss.backward (sum) time: 10.861578702926636
## e[3]      optimizer.step (sum) time: 47.454617977142334
## epoch[3] training(only) time: 94.18545913696289
# Switched to evaluate mode...
Test: [  0/100]	Time  0.201 ( 0.201)	Loss 1.3179e+00 (1.3179e+00)	Acc@1  55.00 ( 55.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.077 ( 0.089)	Loss 1.1214e+00 (1.3259e+00)	Acc@1  60.00 ( 53.91)	Acc@5  96.00 ( 94.36)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 1.2314e+00 (1.2934e+00)	Acc@1  58.00 ( 54.24)	Acc@5  95.00 ( 94.10)
Test: [ 30/100]	Time  0.077 ( 0.082)	Loss 1.2482e+00 (1.3133e+00)	Acc@1  55.00 ( 53.90)	Acc@5  93.00 ( 93.77)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.3691e+00 (1.3087e+00)	Acc@1  52.00 ( 54.02)	Acc@5  92.00 ( 93.63)
Test: [ 50/100]	Time  0.078 ( 0.080)	Loss 1.3258e+00 (1.2950e+00)	Acc@1  52.00 ( 54.49)	Acc@5  94.00 ( 93.71)
Test: [ 60/100]	Time  0.077 ( 0.080)	Loss 1.3912e+00 (1.3015e+00)	Acc@1  56.00 ( 54.31)	Acc@5  93.00 ( 93.54)
Test: [ 70/100]	Time  0.079 ( 0.080)	Loss 1.2234e+00 (1.3118e+00)	Acc@1  54.00 ( 54.06)	Acc@5  95.00 ( 93.46)
Test: [ 80/100]	Time  0.078 ( 0.079)	Loss 1.2451e+00 (1.3039e+00)	Acc@1  51.00 ( 54.02)	Acc@5  94.00 ( 93.58)
Test: [ 90/100]	Time  0.079 ( 0.079)	Loss 1.1259e+00 (1.3063e+00)	Acc@1  58.00 ( 54.20)	Acc@5  96.00 ( 93.47)
 * Acc@1 54.070 Acc@5 93.370
### epoch[3] execution time: 102.21354746818542
EPOCH 4
# current learning rate: 0.1
# Switched to train mode...
Epoch: [4][  0/391]	Time  0.403 ( 0.403)	Data  0.153 ( 0.153)	Loss 1.3747e+00 (1.3747e+00)	Acc@1  52.34 ( 52.34)	Acc@5  92.19 ( 92.19)
Epoch: [4][ 10/391]	Time  0.239 ( 0.255)	Data  0.001 ( 0.015)	Loss 1.3130e+00 (1.2809e+00)	Acc@1  53.91 ( 54.40)	Acc@5  95.31 ( 94.11)
Epoch: [4][ 20/391]	Time  0.240 ( 0.248)	Data  0.001 ( 0.008)	Loss 1.1830e+00 (1.2830e+00)	Acc@1  54.69 ( 53.72)	Acc@5  96.09 ( 94.23)
Epoch: [4][ 30/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.006)	Loss 1.3413e+00 (1.2720e+00)	Acc@1  53.91 ( 53.86)	Acc@5  92.97 ( 94.51)
Epoch: [4][ 40/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.005)	Loss 1.2814e+00 (1.2749e+00)	Acc@1  48.44 ( 53.56)	Acc@5  92.19 ( 94.23)
Epoch: [4][ 50/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.004)	Loss 1.2633e+00 (1.2734e+00)	Acc@1  54.69 ( 53.52)	Acc@5  93.75 ( 94.27)
Epoch: [4][ 60/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.004)	Loss 1.2547e+00 (1.2661e+00)	Acc@1  51.56 ( 53.91)	Acc@5  92.19 ( 94.33)
Epoch: [4][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.0754e+00 (1.2661e+00)	Acc@1  62.50 ( 53.94)	Acc@5  94.53 ( 94.34)
Epoch: [4][ 80/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.0449e+00 (1.2584e+00)	Acc@1  62.50 ( 54.15)	Acc@5  96.09 ( 94.44)
Epoch: [4][ 90/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.4184e+00 (1.2646e+00)	Acc@1  48.44 ( 54.11)	Acc@5  92.97 ( 94.32)
Epoch: [4][100/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.0867e+00 (1.2610e+00)	Acc@1  63.28 ( 54.16)	Acc@5  95.31 ( 94.37)
Epoch: [4][110/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.4876e+00 (1.2549e+00)	Acc@1  53.12 ( 54.53)	Acc@5  89.84 ( 94.36)
Epoch: [4][120/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2055e+00 (1.2457e+00)	Acc@1  56.25 ( 54.98)	Acc@5  94.53 ( 94.48)
Epoch: [4][130/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3119e+00 (1.2456e+00)	Acc@1  53.12 ( 55.02)	Acc@5  94.53 ( 94.51)
Epoch: [4][140/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1401e+00 (1.2446e+00)	Acc@1  54.69 ( 54.99)	Acc@5  98.44 ( 94.52)
Epoch: [4][150/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0589e+00 (1.2414e+00)	Acc@1  62.50 ( 55.08)	Acc@5  96.88 ( 94.56)
Epoch: [4][160/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1284e+00 (1.2364e+00)	Acc@1  61.72 ( 55.30)	Acc@5  96.09 ( 94.60)
Epoch: [4][170/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1392e+00 (1.2348e+00)	Acc@1  57.03 ( 55.47)	Acc@5  97.66 ( 94.57)
Epoch: [4][180/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.3794e+00 (1.2309e+00)	Acc@1  49.22 ( 55.61)	Acc@5  90.62 ( 94.61)
Epoch: [4][190/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2357e+00 (1.2259e+00)	Acc@1  56.25 ( 55.81)	Acc@5  94.53 ( 94.64)
Epoch: [4][200/391]	Time  0.243 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0729e+00 (1.2254e+00)	Acc@1  64.06 ( 55.88)	Acc@5  96.09 ( 94.70)
Epoch: [4][210/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2388e+00 (1.2222e+00)	Acc@1  60.16 ( 56.05)	Acc@5  94.53 ( 94.74)
Epoch: [4][220/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2064e+00 (1.2195e+00)	Acc@1  54.69 ( 56.13)	Acc@5  92.97 ( 94.76)
Epoch: [4][230/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1009e+00 (1.2151e+00)	Acc@1  66.41 ( 56.36)	Acc@5  91.41 ( 94.77)
Epoch: [4][240/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1201e+00 (1.2122e+00)	Acc@1  61.72 ( 56.52)	Acc@5  94.53 ( 94.79)
Epoch: [4][250/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2035e+00 (1.2102e+00)	Acc@1  57.81 ( 56.61)	Acc@5  94.53 ( 94.77)
Epoch: [4][260/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1627e+00 (1.2071e+00)	Acc@1  61.72 ( 56.77)	Acc@5  93.75 ( 94.79)
Epoch: [4][270/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2592e+00 (1.2057e+00)	Acc@1  53.91 ( 56.83)	Acc@5  89.84 ( 94.76)
Epoch: [4][280/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2799e+00 (1.2038e+00)	Acc@1  50.78 ( 56.87)	Acc@5  93.75 ( 94.78)
Epoch: [4][290/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0639e+00 (1.2021e+00)	Acc@1  66.41 ( 56.95)	Acc@5  95.31 ( 94.76)
Epoch: [4][300/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2298e+00 (1.2001e+00)	Acc@1  58.59 ( 57.05)	Acc@5  96.09 ( 94.80)
Epoch: [4][310/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.3092e+00 (1.1970e+00)	Acc@1  53.12 ( 57.13)	Acc@5  93.75 ( 94.83)
Epoch: [4][320/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0920e+00 (1.1936e+00)	Acc@1  57.03 ( 57.25)	Acc@5  98.44 ( 94.86)
Epoch: [4][330/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0800e+00 (1.1920e+00)	Acc@1  57.03 ( 57.31)	Acc@5  96.88 ( 94.89)
Epoch: [4][340/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.7629e-01 (1.1892e+00)	Acc@1  69.53 ( 57.44)	Acc@5  96.88 ( 94.92)
Epoch: [4][350/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2204e+00 (1.1881e+00)	Acc@1  53.12 ( 57.48)	Acc@5  97.66 ( 94.95)
Epoch: [4][360/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1495e+00 (1.1844e+00)	Acc@1  60.16 ( 57.62)	Acc@5  96.09 ( 94.98)
Epoch: [4][370/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.3245e+00 (1.1834e+00)	Acc@1  51.56 ( 57.62)	Acc@5  96.88 ( 95.01)
Epoch: [4][380/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1653e+00 (1.1827e+00)	Acc@1  57.03 ( 57.63)	Acc@5  96.09 ( 95.02)
Epoch: [4][390/391]	Time  0.176 ( 0.241)	Data  0.001 ( 0.001)	Loss 1.0352e+00 (1.1821e+00)	Acc@1  61.25 ( 57.69)	Acc@5  98.75 ( 95.03)
## e[4] optimizer.zero_grad (sum) time: 0.4744558334350586
## e[4]       loss.backward (sum) time: 10.83869218826294
## e[4]      optimizer.step (sum) time: 47.5149621963501
## epoch[4] training(only) time: 94.23581767082214
# Switched to evaluate mode...
Test: [  0/100]	Time  0.205 ( 0.205)	Loss 9.2395e-01 (9.2395e-01)	Acc@1  65.00 ( 65.00)	Acc@5  97.00 ( 97.00)
Test: [ 10/100]	Time  0.078 ( 0.089)	Loss 9.8350e-01 (1.0126e+00)	Acc@1  64.00 ( 63.09)	Acc@5  95.00 ( 97.27)
Test: [ 20/100]	Time  0.079 ( 0.084)	Loss 1.0272e+00 (1.0315e+00)	Acc@1  58.00 ( 61.76)	Acc@5  96.00 ( 96.81)
Test: [ 30/100]	Time  0.079 ( 0.082)	Loss 8.8983e-01 (1.0368e+00)	Acc@1  69.00 ( 62.26)	Acc@5  96.00 ( 96.65)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 9.9082e-01 (1.0380e+00)	Acc@1  69.00 ( 62.71)	Acc@5  94.00 ( 96.44)
Test: [ 50/100]	Time  0.077 ( 0.080)	Loss 1.0227e+00 (1.0263e+00)	Acc@1  63.00 ( 63.14)	Acc@5  95.00 ( 96.51)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.1107e+00 (1.0306e+00)	Acc@1  58.00 ( 63.15)	Acc@5  95.00 ( 96.49)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 9.5022e-01 (1.0357e+00)	Acc@1  68.00 ( 63.08)	Acc@5  97.00 ( 96.48)
Test: [ 80/100]	Time  0.078 ( 0.079)	Loss 9.2317e-01 (1.0321e+00)	Acc@1  69.00 ( 63.23)	Acc@5  98.00 ( 96.46)
Test: [ 90/100]	Time  0.079 ( 0.079)	Loss 8.6793e-01 (1.0355e+00)	Acc@1  69.00 ( 63.10)	Acc@5  99.00 ( 96.42)
 * Acc@1 63.210 Acc@5 96.330
### epoch[4] execution time: 102.23916006088257
EPOCH 5
# current learning rate: 0.1
# Switched to train mode...
Epoch: [5][  0/391]	Time  0.379 ( 0.379)	Data  0.138 ( 0.138)	Loss 1.1479e+00 (1.1479e+00)	Acc@1  52.34 ( 52.34)	Acc@5  97.66 ( 97.66)
Epoch: [5][ 10/391]	Time  0.240 ( 0.253)	Data  0.001 ( 0.014)	Loss 9.0475e-01 (1.0680e+00)	Acc@1  68.75 ( 62.50)	Acc@5  99.22 ( 96.38)
Epoch: [5][ 20/391]	Time  0.240 ( 0.247)	Data  0.001 ( 0.008)	Loss 9.9438e-01 (1.0793e+00)	Acc@1  58.59 ( 61.61)	Acc@5  98.44 ( 96.39)
Epoch: [5][ 30/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.006)	Loss 1.1288e+00 (1.1061e+00)	Acc@1  65.62 ( 61.27)	Acc@5  93.75 ( 95.89)
Epoch: [5][ 40/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.005)	Loss 1.0126e+00 (1.1050e+00)	Acc@1  61.72 ( 61.17)	Acc@5  96.09 ( 95.69)
Epoch: [5][ 50/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.004)	Loss 1.0062e+00 (1.0988e+00)	Acc@1  63.28 ( 61.35)	Acc@5  98.44 ( 95.76)
Epoch: [5][ 60/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.0535e+00 (1.0909e+00)	Acc@1  64.06 ( 61.85)	Acc@5  96.88 ( 95.91)
Epoch: [5][ 70/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 9.4838e-01 (1.0930e+00)	Acc@1  67.19 ( 61.67)	Acc@5  96.88 ( 95.86)
Epoch: [5][ 80/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.1097e+00 (1.0934e+00)	Acc@1  59.38 ( 61.58)	Acc@5  95.31 ( 95.84)
Epoch: [5][ 90/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.0812e+00 (1.0906e+00)	Acc@1  58.59 ( 61.63)	Acc@5  95.31 ( 95.83)
Epoch: [5][100/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.5478e-01 (1.0842e+00)	Acc@1  67.97 ( 62.00)	Acc@5  95.31 ( 95.82)
Epoch: [5][110/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1061e+00 (1.0866e+00)	Acc@1  60.94 ( 61.83)	Acc@5  95.31 ( 95.76)
Epoch: [5][120/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.8115e-01 (1.0832e+00)	Acc@1  66.41 ( 61.98)	Acc@5  97.66 ( 95.84)
Epoch: [5][130/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0596e+00 (1.0777e+00)	Acc@1  56.25 ( 62.09)	Acc@5  96.88 ( 95.94)
Epoch: [5][140/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0473e+00 (1.0780e+00)	Acc@1  67.97 ( 62.01)	Acc@5  95.31 ( 95.98)
Epoch: [5][150/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0533e+00 (1.0787e+00)	Acc@1  64.06 ( 62.01)	Acc@5  96.09 ( 95.93)
Epoch: [5][160/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.7832e-01 (1.0787e+00)	Acc@1  71.09 ( 62.06)	Acc@5  98.44 ( 95.95)
Epoch: [5][170/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.5354e-01 (1.0770e+00)	Acc@1  66.41 ( 62.21)	Acc@5  96.09 ( 95.94)
Epoch: [5][180/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0732e+00 (1.0763e+00)	Acc@1  63.28 ( 62.32)	Acc@5  93.75 ( 95.89)
Epoch: [5][190/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.4931e-01 (1.0727e+00)	Acc@1  64.84 ( 62.46)	Acc@5  95.31 ( 95.89)
Epoch: [5][200/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0299e+00 (1.0697e+00)	Acc@1  58.59 ( 62.52)	Acc@5  96.88 ( 95.93)
Epoch: [5][210/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.1136e-01 (1.0679e+00)	Acc@1  69.53 ( 62.59)	Acc@5  95.31 ( 95.90)
Epoch: [5][220/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0363e+00 (1.0671e+00)	Acc@1  60.94 ( 62.57)	Acc@5  96.88 ( 95.91)
Epoch: [5][230/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0251e+00 (1.0668e+00)	Acc@1  68.75 ( 62.57)	Acc@5  96.09 ( 95.90)
Epoch: [5][240/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.8495e-01 (1.0646e+00)	Acc@1  60.94 ( 62.62)	Acc@5  97.66 ( 95.94)
Epoch: [5][250/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0499e+00 (1.0635e+00)	Acc@1  64.84 ( 62.69)	Acc@5  96.09 ( 95.94)
Epoch: [5][260/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1483e+00 (1.0615e+00)	Acc@1  60.94 ( 62.75)	Acc@5  94.53 ( 95.97)
Epoch: [5][270/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.4839e-01 (1.0588e+00)	Acc@1  64.84 ( 62.82)	Acc@5  97.66 ( 95.99)
Epoch: [5][280/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1885e+00 (1.0574e+00)	Acc@1  57.81 ( 62.82)	Acc@5  94.53 ( 96.03)
Epoch: [5][290/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0438e+00 (1.0552e+00)	Acc@1  64.84 ( 62.86)	Acc@5  93.75 ( 96.07)
Epoch: [5][300/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.9177e-01 (1.0537e+00)	Acc@1  64.84 ( 62.89)	Acc@5  96.09 ( 96.07)
Epoch: [5][310/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.9948e-01 (1.0527e+00)	Acc@1  67.97 ( 62.92)	Acc@5  95.31 ( 96.06)
Epoch: [5][320/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.4052e-01 (1.0522e+00)	Acc@1  67.19 ( 62.96)	Acc@5  98.44 ( 96.06)
Epoch: [5][330/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.4540e-01 (1.0504e+00)	Acc@1  68.75 ( 63.04)	Acc@5  96.88 ( 96.07)
Epoch: [5][340/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.3245e-01 (1.0483e+00)	Acc@1  64.84 ( 63.08)	Acc@5  97.66 ( 96.09)
Epoch: [5][350/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1453e+00 (1.0450e+00)	Acc@1  59.38 ( 63.17)	Acc@5  93.75 ( 96.13)
Epoch: [5][360/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.001)	Loss 1.0233e+00 (1.0426e+00)	Acc@1  62.50 ( 63.27)	Acc@5  95.31 ( 96.15)
Epoch: [5][370/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.001)	Loss 1.0675e+00 (1.0420e+00)	Acc@1  66.41 ( 63.28)	Acc@5  95.31 ( 96.16)
Epoch: [5][380/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.001)	Loss 1.0800e+00 (1.0412e+00)	Acc@1  64.06 ( 63.30)	Acc@5  96.09 ( 96.18)
Epoch: [5][390/391]	Time  0.176 ( 0.240)	Data  0.001 ( 0.001)	Loss 9.4805e-01 (1.0391e+00)	Acc@1  66.25 ( 63.35)	Acc@5  97.50 ( 96.19)
## e[5] optimizer.zero_grad (sum) time: 0.476590633392334
## e[5]       loss.backward (sum) time: 10.801481246948242
## e[5]      optimizer.step (sum) time: 47.52996850013733
## epoch[5] training(only) time: 94.14604759216309
# Switched to evaluate mode...
Test: [  0/100]	Time  0.213 ( 0.213)	Loss 1.2511e+00 (1.2511e+00)	Acc@1  55.00 ( 55.00)	Acc@5  96.00 ( 96.00)
Test: [ 10/100]	Time  0.078 ( 0.091)	Loss 1.1925e+00 (1.1338e+00)	Acc@1  67.00 ( 63.36)	Acc@5  95.00 ( 96.27)
Test: [ 20/100]	Time  0.078 ( 0.085)	Loss 9.0351e-01 (1.1347e+00)	Acc@1  66.00 ( 62.24)	Acc@5  97.00 ( 96.43)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 1.0875e+00 (1.1382e+00)	Acc@1  59.00 ( 61.90)	Acc@5  97.00 ( 96.26)
Test: [ 40/100]	Time  0.079 ( 0.081)	Loss 1.1969e+00 (1.1448e+00)	Acc@1  62.00 ( 61.83)	Acc@5  94.00 ( 95.98)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 1.1240e+00 (1.1548e+00)	Acc@1  61.00 ( 61.88)	Acc@5  94.00 ( 95.92)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.1545e+00 (1.1621e+00)	Acc@1  59.00 ( 61.36)	Acc@5  95.00 ( 96.02)
Test: [ 70/100]	Time  0.079 ( 0.080)	Loss 1.1589e+00 (1.1630e+00)	Acc@1  62.00 ( 61.27)	Acc@5  98.00 ( 96.08)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 1.0397e+00 (1.1516e+00)	Acc@1  63.00 ( 61.51)	Acc@5  96.00 ( 96.12)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 9.8221e-01 (1.1570e+00)	Acc@1  66.00 ( 61.26)	Acc@5  98.00 ( 96.09)
 * Acc@1 61.380 Acc@5 96.140
### epoch[5] execution time: 102.16768407821655
EPOCH 6
# current learning rate: 0.1
# Switched to train mode...
Epoch: [6][  0/391]	Time  0.403 ( 0.403)	Data  0.158 ( 0.158)	Loss 9.0138e-01 (9.0138e-01)	Acc@1  65.62 ( 65.62)	Acc@5  98.44 ( 98.44)
Epoch: [6][ 10/391]	Time  0.241 ( 0.255)	Data  0.001 ( 0.015)	Loss 1.0139e+00 (9.5677e-01)	Acc@1  64.84 ( 68.11)	Acc@5  98.44 ( 96.59)
Epoch: [6][ 20/391]	Time  0.244 ( 0.248)	Data  0.001 ( 0.009)	Loss 9.8500e-01 (9.7479e-01)	Acc@1  66.41 ( 66.18)	Acc@5  96.88 ( 96.54)
Epoch: [6][ 30/391]	Time  0.239 ( 0.245)	Data  0.001 ( 0.006)	Loss 9.6322e-01 (9.5785e-01)	Acc@1  65.62 ( 66.76)	Acc@5  95.31 ( 96.82)
Epoch: [6][ 40/391]	Time  0.252 ( 0.244)	Data  0.001 ( 0.005)	Loss 9.9893e-01 (9.5914e-01)	Acc@1  62.50 ( 66.75)	Acc@5  95.31 ( 96.86)
Epoch: [6][ 50/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.0186e+00 (9.5480e-01)	Acc@1  61.72 ( 66.70)	Acc@5  98.44 ( 97.01)
Epoch: [6][ 60/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.004)	Loss 1.0120e+00 (9.5794e-01)	Acc@1  64.84 ( 66.44)	Acc@5  98.44 ( 96.94)
Epoch: [6][ 70/391]	Time  0.239 ( 0.243)	Data  0.001 ( 0.003)	Loss 9.2747e-01 (9.6206e-01)	Acc@1  64.84 ( 66.27)	Acc@5  95.31 ( 96.93)
Epoch: [6][ 80/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 8.4415e-01 (9.5437e-01)	Acc@1  66.41 ( 66.38)	Acc@5  98.44 ( 96.93)
Epoch: [6][ 90/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.003)	Loss 9.2679e-01 (9.5580e-01)	Acc@1  67.97 ( 66.55)	Acc@5  97.66 ( 96.94)
Epoch: [6][100/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.0542e+00 (9.5757e-01)	Acc@1  60.94 ( 66.38)	Acc@5  96.88 ( 96.93)
Epoch: [6][110/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 8.9408e-01 (9.5515e-01)	Acc@1  69.53 ( 66.46)	Acc@5  98.44 ( 96.97)
Epoch: [6][120/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.6781e-01 (9.5514e-01)	Acc@1  67.19 ( 66.43)	Acc@5  98.44 ( 96.98)
Epoch: [6][130/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.3397e-01 (9.5793e-01)	Acc@1  69.53 ( 66.41)	Acc@5  96.09 ( 96.95)
Epoch: [6][140/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.5215e-01 (9.5928e-01)	Acc@1  71.09 ( 66.30)	Acc@5  97.66 ( 96.96)
Epoch: [6][150/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.3495e-01 (9.5860e-01)	Acc@1  71.09 ( 66.28)	Acc@5  95.31 ( 96.95)
Epoch: [6][160/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.2249e-01 (9.5701e-01)	Acc@1  67.97 ( 66.37)	Acc@5  96.88 ( 96.96)
Epoch: [6][170/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.2411e-01 (9.5298e-01)	Acc@1  69.53 ( 66.57)	Acc@5  96.09 ( 96.96)
Epoch: [6][180/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0492e+00 (9.5196e-01)	Acc@1  60.94 ( 66.64)	Acc@5  96.09 ( 96.95)
Epoch: [6][190/391]	Time  0.244 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0650e+00 (9.5185e-01)	Acc@1  59.38 ( 66.58)	Acc@5  96.09 ( 96.96)
Epoch: [6][200/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.9393e-01 (9.4864e-01)	Acc@1  71.88 ( 66.65)	Acc@5  96.88 ( 96.96)
Epoch: [6][210/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.9021e-01 (9.4733e-01)	Acc@1  69.53 ( 66.66)	Acc@5  96.09 ( 96.97)
Epoch: [6][220/391]	Time  0.245 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.4408e-01 (9.4539e-01)	Acc@1  69.53 ( 66.72)	Acc@5  97.66 ( 97.00)
Epoch: [6][230/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.5146e-01 (9.4556e-01)	Acc@1  67.97 ( 66.70)	Acc@5  98.44 ( 96.96)
Epoch: [6][240/391]	Time  0.244 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0881e+00 (9.4533e-01)	Acc@1  59.38 ( 66.69)	Acc@5  96.88 ( 96.97)
Epoch: [6][250/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.6231e-01 (9.4574e-01)	Acc@1  73.44 ( 66.64)	Acc@5  96.88 ( 96.96)
Epoch: [6][260/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.5020e-01 (9.4507e-01)	Acc@1  64.06 ( 66.65)	Acc@5  96.09 ( 96.98)
Epoch: [6][270/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.9151e-01 (9.4452e-01)	Acc@1  67.19 ( 66.62)	Acc@5  97.66 ( 97.00)
Epoch: [6][280/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.9417e-01 (9.4212e-01)	Acc@1  71.09 ( 66.72)	Acc@5  94.53 ( 97.00)
Epoch: [6][290/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0550e+00 (9.4178e-01)	Acc@1  64.84 ( 66.73)	Acc@5  96.09 ( 97.01)
Epoch: [6][300/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.0481e-01 (9.4157e-01)	Acc@1  72.66 ( 66.76)	Acc@5  96.88 ( 97.03)
Epoch: [6][310/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1023e+00 (9.4095e-01)	Acc@1  58.59 ( 66.76)	Acc@5  96.88 ( 97.04)
Epoch: [6][320/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.8597e-01 (9.3933e-01)	Acc@1  70.31 ( 66.80)	Acc@5  95.31 ( 97.05)
Epoch: [6][330/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.6050e-01 (9.3963e-01)	Acc@1  71.88 ( 66.81)	Acc@5  97.66 ( 97.02)
Epoch: [6][340/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.6506e-01 (9.3851e-01)	Acc@1  71.09 ( 66.84)	Acc@5  98.44 ( 97.03)
Epoch: [6][350/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.7885e-01 (9.3846e-01)	Acc@1  68.75 ( 66.87)	Acc@5  96.09 ( 97.04)
Epoch: [6][360/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.2890e-01 (9.3732e-01)	Acc@1  66.41 ( 66.91)	Acc@5  95.31 ( 97.04)
Epoch: [6][370/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.5146e-01 (9.3568e-01)	Acc@1  67.19 ( 66.99)	Acc@5  96.88 ( 97.06)
Epoch: [6][380/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.0733e-01 (9.3519e-01)	Acc@1  73.44 ( 67.02)	Acc@5  95.31 ( 97.05)
Epoch: [6][390/391]	Time  0.177 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.0943e-01 (9.3497e-01)	Acc@1  72.50 ( 67.03)	Acc@5  98.75 ( 97.05)
## e[6] optimizer.zero_grad (sum) time: 0.4751591682434082
## e[6]       loss.backward (sum) time: 10.775418758392334
## e[6]      optimizer.step (sum) time: 47.570191621780396
## epoch[6] training(only) time: 94.2061095237732
# Switched to evaluate mode...
Test: [  0/100]	Time  0.207 ( 0.207)	Loss 8.6897e-01 (8.6897e-01)	Acc@1  66.00 ( 66.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 8.2023e-01 (9.1390e-01)	Acc@1  64.00 ( 66.82)	Acc@5  97.00 ( 98.00)
Test: [ 20/100]	Time  0.077 ( 0.084)	Loss 9.0983e-01 (9.1257e-01)	Acc@1  66.00 ( 67.29)	Acc@5 100.00 ( 97.95)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 7.2891e-01 (9.0271e-01)	Acc@1  72.00 ( 67.74)	Acc@5  99.00 ( 97.81)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 9.8038e-01 (9.1815e-01)	Acc@1  69.00 ( 67.39)	Acc@5  96.00 ( 97.49)
Test: [ 50/100]	Time  0.077 ( 0.080)	Loss 9.7175e-01 (9.0952e-01)	Acc@1  71.00 ( 67.88)	Acc@5  97.00 ( 97.59)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 8.7973e-01 (9.1836e-01)	Acc@1  69.00 ( 67.79)	Acc@5  98.00 ( 97.52)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 8.3977e-01 (9.1553e-01)	Acc@1  74.00 ( 68.03)	Acc@5  96.00 ( 97.58)
Test: [ 80/100]	Time  0.079 ( 0.079)	Loss 8.3126e-01 (9.1284e-01)	Acc@1  70.00 ( 68.36)	Acc@5  96.00 ( 97.58)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 6.6355e-01 (9.1650e-01)	Acc@1  72.00 ( 68.03)	Acc@5 100.00 ( 97.57)
 * Acc@1 68.050 Acc@5 97.550
### epoch[6] execution time: 102.23938727378845
EPOCH 7
# current learning rate: 0.1
# Switched to train mode...
Epoch: [7][  0/391]	Time  0.386 ( 0.386)	Data  0.145 ( 0.145)	Loss 8.2931e-01 (8.2931e-01)	Acc@1  74.22 ( 74.22)	Acc@5  96.88 ( 96.88)
Epoch: [7][ 10/391]	Time  0.240 ( 0.254)	Data  0.001 ( 0.014)	Loss 9.3533e-01 (8.7532e-01)	Acc@1  66.41 ( 69.03)	Acc@5  96.09 ( 97.37)
Epoch: [7][ 20/391]	Time  0.240 ( 0.247)	Data  0.001 ( 0.008)	Loss 9.1808e-01 (8.7553e-01)	Acc@1  66.41 ( 69.75)	Acc@5  96.88 ( 97.25)
Epoch: [7][ 30/391]	Time  0.243 ( 0.245)	Data  0.001 ( 0.006)	Loss 8.4344e-01 (8.7812e-01)	Acc@1  73.44 ( 69.73)	Acc@5 100.00 ( 97.43)
Epoch: [7][ 40/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.005)	Loss 8.5190e-01 (8.7819e-01)	Acc@1  68.75 ( 69.51)	Acc@5  98.44 ( 97.43)
Epoch: [7][ 50/391]	Time  0.239 ( 0.243)	Data  0.001 ( 0.004)	Loss 8.6559e-01 (8.7845e-01)	Acc@1  71.09 ( 69.44)	Acc@5  96.09 ( 97.29)
Epoch: [7][ 60/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 8.1396e-01 (8.7714e-01)	Acc@1  70.31 ( 69.34)	Acc@5  99.22 ( 97.26)
Epoch: [7][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 7.7385e-01 (8.6993e-01)	Acc@1  71.09 ( 69.44)	Acc@5  96.09 ( 97.36)
Epoch: [7][ 80/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 9.8789e-01 (8.7061e-01)	Acc@1  67.19 ( 69.38)	Acc@5  96.09 ( 97.33)
Epoch: [7][ 90/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.0578e+00 (8.7588e-01)	Acc@1  64.06 ( 69.27)	Acc@5  93.75 ( 97.28)
Epoch: [7][100/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 8.4342e-01 (8.7307e-01)	Acc@1  68.75 ( 69.39)	Acc@5  96.88 ( 97.35)
Epoch: [7][110/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.2250e-01 (8.6990e-01)	Acc@1  67.97 ( 69.43)	Acc@5  96.88 ( 97.38)
Epoch: [7][120/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.9037e-01 (8.7130e-01)	Acc@1  67.19 ( 69.34)	Acc@5  96.09 ( 97.35)
Epoch: [7][130/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.7090e-01 (8.7193e-01)	Acc@1  71.88 ( 69.33)	Acc@5  96.88 ( 97.35)
Epoch: [7][140/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.9308e-01 (8.6935e-01)	Acc@1  70.31 ( 69.44)	Acc@5  96.88 ( 97.39)
Epoch: [7][150/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.9189e-01 (8.6739e-01)	Acc@1  67.97 ( 69.47)	Acc@5  94.53 ( 97.33)
Epoch: [7][160/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.2054e-01 (8.6395e-01)	Acc@1  75.78 ( 69.62)	Acc@5  97.66 ( 97.33)
Epoch: [7][170/391]	Time  0.244 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.0051e-01 (8.6445e-01)	Acc@1  72.66 ( 69.61)	Acc@5  97.66 ( 97.35)
Epoch: [7][180/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.8621e-01 (8.6237e-01)	Acc@1  75.00 ( 69.71)	Acc@5  99.22 ( 97.38)
Epoch: [7][190/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.5128e-01 (8.6000e-01)	Acc@1  74.22 ( 69.80)	Acc@5  99.22 ( 97.39)
Epoch: [7][200/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.4385e-01 (8.5984e-01)	Acc@1  71.88 ( 69.74)	Acc@5  96.88 ( 97.42)
Epoch: [7][210/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.9754e-01 (8.5774e-01)	Acc@1  71.88 ( 69.83)	Acc@5  96.09 ( 97.41)
Epoch: [7][220/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.6213e-01 (8.5853e-01)	Acc@1  71.09 ( 69.84)	Acc@5  93.75 ( 97.38)
Epoch: [7][230/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.5733e-01 (8.5706e-01)	Acc@1  71.88 ( 69.90)	Acc@5  98.44 ( 97.41)
Epoch: [7][240/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.4565e-01 (8.5938e-01)	Acc@1  74.22 ( 69.85)	Acc@5  98.44 ( 97.37)
Epoch: [7][250/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.2473e-01 (8.5785e-01)	Acc@1  71.88 ( 69.90)	Acc@5  96.88 ( 97.39)
Epoch: [7][260/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.9006e-01 (8.5887e-01)	Acc@1  67.97 ( 69.89)	Acc@5  97.66 ( 97.38)
Epoch: [7][270/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.8114e-01 (8.5746e-01)	Acc@1  77.34 ( 69.94)	Acc@5  98.44 ( 97.39)
Epoch: [7][280/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.8483e-01 (8.5523e-01)	Acc@1  75.00 ( 70.03)	Acc@5  98.44 ( 97.40)
Epoch: [7][290/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0007e+00 (8.5311e-01)	Acc@1  62.50 ( 70.08)	Acc@5  96.88 ( 97.42)
Epoch: [7][300/391]	Time  0.247 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.9078e-01 (8.5422e-01)	Acc@1  70.31 ( 70.05)	Acc@5  95.31 ( 97.41)
Epoch: [7][310/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.5271e-01 (8.5391e-01)	Acc@1  74.22 ( 70.04)	Acc@5  96.88 ( 97.42)
Epoch: [7][320/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.2137e-01 (8.5298e-01)	Acc@1  73.44 ( 70.04)	Acc@5  96.09 ( 97.42)
Epoch: [7][330/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.4217e-01 (8.5068e-01)	Acc@1  71.09 ( 70.12)	Acc@5  99.22 ( 97.43)
Epoch: [7][340/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.8809e-01 (8.5118e-01)	Acc@1  72.66 ( 70.11)	Acc@5  99.22 ( 97.45)
Epoch: [7][350/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.001)	Loss 7.9314e-01 (8.5031e-01)	Acc@1  79.69 ( 70.16)	Acc@5  96.88 ( 97.45)
Epoch: [7][360/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.001)	Loss 9.5179e-01 (8.5024e-01)	Acc@1  64.06 ( 70.15)	Acc@5  97.66 ( 97.43)
Epoch: [7][370/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.001)	Loss 9.2297e-01 (8.5036e-01)	Acc@1  68.75 ( 70.14)	Acc@5  97.66 ( 97.44)
Epoch: [7][380/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.001)	Loss 8.1471e-01 (8.4903e-01)	Acc@1  72.66 ( 70.20)	Acc@5  97.66 ( 97.45)
Epoch: [7][390/391]	Time  0.175 ( 0.241)	Data  0.001 ( 0.001)	Loss 8.3054e-01 (8.4821e-01)	Acc@1  75.00 ( 70.26)	Acc@5 100.00 ( 97.45)
## e[7] optimizer.zero_grad (sum) time: 0.4731326103210449
## e[7]       loss.backward (sum) time: 10.788039445877075
## e[7]      optimizer.step (sum) time: 47.569000244140625
## epoch[7] training(only) time: 94.21599388122559
# Switched to evaluate mode...
Test: [  0/100]	Time  0.206 ( 0.206)	Loss 8.9008e-01 (8.9008e-01)	Acc@1  69.00 ( 69.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 8.0513e-01 (8.5566e-01)	Acc@1  71.00 ( 70.36)	Acc@5  98.00 ( 98.00)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 8.8365e-01 (9.0089e-01)	Acc@1  68.00 ( 69.05)	Acc@5  99.00 ( 98.14)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 9.8831e-01 (9.0987e-01)	Acc@1  74.00 ( 69.23)	Acc@5  96.00 ( 98.10)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 7.8951e-01 (9.1460e-01)	Acc@1  74.00 ( 69.41)	Acc@5  98.00 ( 97.90)
Test: [ 50/100]	Time  0.077 ( 0.080)	Loss 1.0485e+00 (9.1668e-01)	Acc@1  69.00 ( 69.45)	Acc@5  99.00 ( 97.88)
Test: [ 60/100]	Time  0.077 ( 0.080)	Loss 9.6756e-01 (9.2872e-01)	Acc@1  65.00 ( 69.25)	Acc@5  99.00 ( 97.85)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 9.3000e-01 (9.3399e-01)	Acc@1  70.00 ( 69.17)	Acc@5  98.00 ( 97.94)
Test: [ 80/100]	Time  0.078 ( 0.079)	Loss 6.8605e-01 (9.2874e-01)	Acc@1  75.00 ( 69.40)	Acc@5  97.00 ( 97.94)
Test: [ 90/100]	Time  0.079 ( 0.079)	Loss 6.9898e-01 (9.3429e-01)	Acc@1  75.00 ( 69.05)	Acc@5  99.00 ( 97.87)
 * Acc@1 69.020 Acc@5 97.880
### epoch[7] execution time: 102.22093367576599
EPOCH 8
# current learning rate: 0.1
# Switched to train mode...
Epoch: [8][  0/391]	Time  0.392 ( 0.392)	Data  0.148 ( 0.148)	Loss 7.9243e-01 (7.9243e-01)	Acc@1  68.75 ( 68.75)	Acc@5  98.44 ( 98.44)
Epoch: [8][ 10/391]	Time  0.240 ( 0.254)	Data  0.001 ( 0.014)	Loss 6.6338e-01 (7.2302e-01)	Acc@1  78.91 ( 74.22)	Acc@5  99.22 ( 98.22)
Epoch: [8][ 20/391]	Time  0.242 ( 0.248)	Data  0.001 ( 0.008)	Loss 6.6008e-01 (7.5907e-01)	Acc@1  74.22 ( 72.99)	Acc@5  98.44 ( 98.14)
Epoch: [8][ 30/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.006)	Loss 7.8043e-01 (7.5916e-01)	Acc@1  71.88 ( 72.98)	Acc@5  96.88 ( 98.14)
Epoch: [8][ 40/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.005)	Loss 7.6034e-01 (7.6974e-01)	Acc@1  75.00 ( 72.52)	Acc@5  98.44 ( 98.30)
Epoch: [8][ 50/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.004)	Loss 7.7499e-01 (7.7205e-01)	Acc@1  73.44 ( 72.70)	Acc@5  99.22 ( 98.25)
Epoch: [8][ 60/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.004)	Loss 9.6900e-01 (7.7445e-01)	Acc@1  62.50 ( 72.52)	Acc@5  96.88 ( 98.19)
Epoch: [8][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 7.1287e-01 (7.8239e-01)	Acc@1  75.00 ( 72.35)	Acc@5 100.00 ( 98.17)
Epoch: [8][ 80/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 8.7316e-01 (7.7996e-01)	Acc@1  71.88 ( 72.51)	Acc@5  99.22 ( 98.22)
Epoch: [8][ 90/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 7.1255e-01 (7.7355e-01)	Acc@1  75.00 ( 72.65)	Acc@5  98.44 ( 98.17)
Epoch: [8][100/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.003)	Loss 8.6504e-01 (7.7647e-01)	Acc@1  68.75 ( 72.61)	Acc@5  98.44 ( 98.11)
Epoch: [8][110/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.5160e-01 (7.8214e-01)	Acc@1  76.56 ( 72.50)	Acc@5 100.00 ( 98.07)
Epoch: [8][120/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.7978e-01 (7.8038e-01)	Acc@1  76.56 ( 72.58)	Acc@5  99.22 ( 98.08)
Epoch: [8][130/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.5859e-01 (7.8019e-01)	Acc@1  68.75 ( 72.72)	Acc@5  98.44 ( 98.10)
Epoch: [8][140/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.7154e-01 (7.7798e-01)	Acc@1  71.09 ( 72.74)	Acc@5  99.22 ( 98.11)
Epoch: [8][150/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.5561e-01 (7.7854e-01)	Acc@1  75.00 ( 72.68)	Acc@5 100.00 ( 98.08)
Epoch: [8][160/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.7858e-01 (7.7693e-01)	Acc@1  74.22 ( 72.79)	Acc@5  99.22 ( 98.04)
Epoch: [8][170/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.5443e-01 (7.7638e-01)	Acc@1  77.34 ( 72.80)	Acc@5  99.22 ( 98.06)
Epoch: [8][180/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.7057e-01 (7.7600e-01)	Acc@1  68.75 ( 72.78)	Acc@5  99.22 ( 98.07)
Epoch: [8][190/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.1345e-01 (7.7696e-01)	Acc@1  66.41 ( 72.69)	Acc@5  96.88 ( 98.06)
Epoch: [8][200/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.1289e-01 (7.7617e-01)	Acc@1  70.31 ( 72.74)	Acc@5  97.66 ( 98.05)
Epoch: [8][210/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.3025e-01 (7.7371e-01)	Acc@1  71.88 ( 72.79)	Acc@5  99.22 ( 98.07)
Epoch: [8][220/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.8694e-01 (7.7288e-01)	Acc@1  81.25 ( 72.89)	Acc@5  99.22 ( 98.07)
Epoch: [8][230/391]	Time  0.246 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.9306e-01 (7.7285e-01)	Acc@1  78.12 ( 72.90)	Acc@5  99.22 ( 98.06)
Epoch: [8][240/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.5958e-01 (7.7155e-01)	Acc@1  68.75 ( 72.94)	Acc@5  98.44 ( 98.07)
Epoch: [8][250/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.3838e-01 (7.7144e-01)	Acc@1  77.34 ( 72.99)	Acc@5  99.22 ( 98.05)
Epoch: [8][260/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.2268e-01 (7.7399e-01)	Acc@1  64.06 ( 72.94)	Acc@5  95.31 ( 98.01)
Epoch: [8][270/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.9016e-01 (7.7400e-01)	Acc@1  70.31 ( 73.00)	Acc@5  98.44 ( 98.03)
Epoch: [8][280/391]	Time  0.244 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.2003e-01 (7.7202e-01)	Acc@1  80.47 ( 73.10)	Acc@5  98.44 ( 98.03)
Epoch: [8][290/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.2326e-01 (7.7114e-01)	Acc@1  70.31 ( 73.15)	Acc@5  96.88 ( 98.01)
Epoch: [8][300/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.3065e-01 (7.7160e-01)	Acc@1  72.66 ( 73.16)	Acc@5  97.66 ( 98.03)
Epoch: [8][310/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.6113e-01 (7.7028e-01)	Acc@1  73.44 ( 73.17)	Acc@5  98.44 ( 98.05)
Epoch: [8][320/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.8755e-01 (7.6918e-01)	Acc@1  71.88 ( 73.23)	Acc@5  99.22 ( 98.06)
Epoch: [8][330/391]	Time  0.243 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.8349e-01 (7.6879e-01)	Acc@1  74.22 ( 73.24)	Acc@5  99.22 ( 98.08)
Epoch: [8][340/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.2388e-01 (7.6708e-01)	Acc@1  72.66 ( 73.28)	Acc@5 100.00 ( 98.09)
Epoch: [8][350/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.2925e-01 (7.6658e-01)	Acc@1  82.03 ( 73.32)	Acc@5  98.44 ( 98.09)
Epoch: [8][360/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.5197e-01 (7.6676e-01)	Acc@1  75.78 ( 73.31)	Acc@5  95.31 ( 98.09)
Epoch: [8][370/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.3466e-01 (7.6641e-01)	Acc@1  69.53 ( 73.33)	Acc@5  98.44 ( 98.08)
Epoch: [8][380/391]	Time  0.251 ( 0.241)	Data  0.001 ( 0.001)	Loss 6.5753e-01 (7.6639e-01)	Acc@1  77.34 ( 73.34)	Acc@5  97.66 ( 98.08)
Epoch: [8][390/391]	Time  0.177 ( 0.241)	Data  0.001 ( 0.001)	Loss 7.0711e-01 (7.6615e-01)	Acc@1  81.25 ( 73.33)	Acc@5  96.25 ( 98.08)
## e[8] optimizer.zero_grad (sum) time: 0.473496675491333
## e[8]       loss.backward (sum) time: 10.789037704467773
## e[8]      optimizer.step (sum) time: 47.57349181175232
## epoch[8] training(only) time: 94.2497067451477
# Switched to evaluate mode...
Test: [  0/100]	Time  0.210 ( 0.210)	Loss 7.9428e-01 (7.9428e-01)	Acc@1  75.00 ( 75.00)	Acc@5  97.00 ( 97.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 6.8323e-01 (8.2853e-01)	Acc@1  74.00 ( 69.55)	Acc@5  98.00 ( 98.36)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 7.5894e-01 (8.1409e-01)	Acc@1  70.00 ( 70.33)	Acc@5 100.00 ( 98.38)
Test: [ 30/100]	Time  0.077 ( 0.082)	Loss 7.3996e-01 (8.0895e-01)	Acc@1  76.00 ( 71.03)	Acc@5  99.00 ( 98.23)
Test: [ 40/100]	Time  0.079 ( 0.081)	Loss 8.2486e-01 (8.1589e-01)	Acc@1  75.00 ( 70.80)	Acc@5  97.00 ( 97.93)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 7.7032e-01 (8.0445e-01)	Acc@1  73.00 ( 71.31)	Acc@5  97.00 ( 97.96)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 8.7957e-01 (8.1643e-01)	Acc@1  65.00 ( 71.08)	Acc@5  99.00 ( 98.08)
Test: [ 70/100]	Time  0.077 ( 0.080)	Loss 8.6256e-01 (8.1857e-01)	Acc@1  72.00 ( 71.11)	Acc@5  99.00 ( 98.07)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 8.4525e-01 (8.1172e-01)	Acc@1  70.00 ( 71.32)	Acc@5  98.00 ( 98.12)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 5.3656e-01 (8.1660e-01)	Acc@1  81.00 ( 71.11)	Acc@5 100.00 ( 98.09)
 * Acc@1 71.210 Acc@5 98.140
### epoch[8] execution time: 102.26011109352112
EPOCH 9
# current learning rate: 0.1
# Switched to train mode...
Epoch: [9][  0/391]	Time  0.398 ( 0.398)	Data  0.152 ( 0.152)	Loss 6.7549e-01 (6.7549e-01)	Acc@1  75.78 ( 75.78)	Acc@5  98.44 ( 98.44)
Epoch: [9][ 10/391]	Time  0.239 ( 0.255)	Data  0.001 ( 0.015)	Loss 7.3721e-01 (6.9518e-01)	Acc@1  73.44 ( 75.36)	Acc@5  98.44 ( 98.79)
Epoch: [9][ 20/391]	Time  0.239 ( 0.248)	Data  0.001 ( 0.008)	Loss 6.1609e-01 (6.9661e-01)	Acc@1  79.69 ( 75.56)	Acc@5  98.44 ( 98.55)
Epoch: [9][ 30/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.006)	Loss 6.8471e-01 (6.9822e-01)	Acc@1  79.69 ( 75.43)	Acc@5  97.66 ( 98.66)
Epoch: [9][ 40/391]	Time  0.239 ( 0.244)	Data  0.001 ( 0.005)	Loss 6.8031e-01 (7.1937e-01)	Acc@1  76.56 ( 74.79)	Acc@5  96.88 ( 98.55)
Epoch: [9][ 50/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.004)	Loss 7.9193e-01 (7.1914e-01)	Acc@1  71.09 ( 74.77)	Acc@5  97.66 ( 98.50)
Epoch: [9][ 60/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.004)	Loss 6.6716e-01 (7.2301e-01)	Acc@1  74.22 ( 74.40)	Acc@5 100.00 ( 98.57)
Epoch: [9][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 6.4720e-01 (7.2110e-01)	Acc@1  74.22 ( 74.35)	Acc@5  97.66 ( 98.55)
Epoch: [9][ 80/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 6.7747e-01 (7.1856e-01)	Acc@1  78.91 ( 74.63)	Acc@5  99.22 ( 98.56)
Epoch: [9][ 90/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 6.5390e-01 (7.1475e-01)	Acc@1  81.25 ( 74.87)	Acc@5  98.44 ( 98.51)
Epoch: [9][100/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.003)	Loss 7.1456e-01 (7.1898e-01)	Acc@1  74.22 ( 74.59)	Acc@5  96.09 ( 98.45)
Epoch: [9][110/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 6.5887e-01 (7.1652e-01)	Acc@1  76.56 ( 74.83)	Acc@5  98.44 ( 98.44)
Epoch: [9][120/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.9406e-01 (7.1987e-01)	Acc@1  66.41 ( 74.77)	Acc@5 100.00 ( 98.43)
Epoch: [9][130/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.5863e-01 (7.1651e-01)	Acc@1  78.91 ( 74.95)	Acc@5  96.88 ( 98.38)
Epoch: [9][140/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.1658e-01 (7.1754e-01)	Acc@1  70.31 ( 75.02)	Acc@5  96.88 ( 98.35)
Epoch: [9][150/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.2284e-01 (7.1975e-01)	Acc@1  77.34 ( 74.96)	Acc@5 100.00 ( 98.30)
Epoch: [9][160/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.0087e-01 (7.2031e-01)	Acc@1  69.53 ( 74.95)	Acc@5  98.44 ( 98.28)
Epoch: [9][170/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.4085e-01 (7.2433e-01)	Acc@1  66.41 ( 74.81)	Acc@5  99.22 ( 98.31)
Epoch: [9][180/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.4270e-01 (7.2634e-01)	Acc@1  72.66 ( 74.77)	Acc@5  98.44 ( 98.28)
Epoch: [9][190/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.0730e-01 (7.2784e-01)	Acc@1  70.31 ( 74.69)	Acc@5  98.44 ( 98.31)
Epoch: [9][200/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.7999e-01 (7.3004e-01)	Acc@1  74.22 ( 74.67)	Acc@5  99.22 ( 98.30)
Epoch: [9][210/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.7358e-01 (7.2970e-01)	Acc@1  78.12 ( 74.66)	Acc@5  99.22 ( 98.31)
Epoch: [9][220/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.5514e-01 (7.3086e-01)	Acc@1  75.78 ( 74.66)	Acc@5  97.66 ( 98.29)
Epoch: [9][230/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.0619e-01 (7.3355e-01)	Acc@1  77.34 ( 74.55)	Acc@5  98.44 ( 98.29)
Epoch: [9][240/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.2621e-01 (7.3327e-01)	Acc@1  71.88 ( 74.53)	Acc@5  96.09 ( 98.27)
Epoch: [9][250/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.3113e-01 (7.3349e-01)	Acc@1  75.78 ( 74.48)	Acc@5  99.22 ( 98.26)
Epoch: [9][260/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.6819e-01 (7.3277e-01)	Acc@1  75.78 ( 74.51)	Acc@5 100.00 ( 98.29)
Epoch: [9][270/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.2964e-01 (7.3363e-01)	Acc@1  74.22 ( 74.49)	Acc@5  97.66 ( 98.28)
Epoch: [9][280/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.5457e-01 (7.3280e-01)	Acc@1  78.12 ( 74.48)	Acc@5  97.66 ( 98.28)
Epoch: [9][290/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.9782e-01 (7.3247e-01)	Acc@1  67.97 ( 74.48)	Acc@5  99.22 ( 98.27)
Epoch: [9][300/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.0378e-01 (7.3052e-01)	Acc@1  75.00 ( 74.54)	Acc@5  96.88 ( 98.26)
Epoch: [9][310/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.7151e-01 (7.3006e-01)	Acc@1  75.78 ( 74.53)	Acc@5  96.88 ( 98.27)
Epoch: [9][320/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.0463e-01 (7.3050e-01)	Acc@1  75.00 ( 74.50)	Acc@5  98.44 ( 98.27)
Epoch: [9][330/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.7257e-01 (7.3096e-01)	Acc@1  84.38 ( 74.51)	Acc@5  97.66 ( 98.26)
Epoch: [9][340/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.5471e-01 (7.2908e-01)	Acc@1  71.09 ( 74.57)	Acc@5  98.44 ( 98.28)
Epoch: [9][350/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.5210e-01 (7.2866e-01)	Acc@1  78.12 ( 74.59)	Acc@5  99.22 ( 98.27)
Epoch: [9][360/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.6697e-01 (7.2925e-01)	Acc@1  71.88 ( 74.57)	Acc@5  98.44 ( 98.28)
Epoch: [9][370/391]	Time  0.245 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.1097e-01 (7.2946e-01)	Acc@1  73.44 ( 74.54)	Acc@5  97.66 ( 98.29)
Epoch: [9][380/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.8637e-01 (7.2724e-01)	Acc@1  76.56 ( 74.60)	Acc@5  98.44 ( 98.31)
Epoch: [9][390/391]	Time  0.176 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.5247e-01 (7.2577e-01)	Acc@1  72.50 ( 74.64)	Acc@5 100.00 ( 98.32)
## e[9] optimizer.zero_grad (sum) time: 0.4748351573944092
## e[9]       loss.backward (sum) time: 10.766855239868164
## e[9]      optimizer.step (sum) time: 47.60051918029785
## epoch[9] training(only) time: 94.21944189071655
# Switched to evaluate mode...
Test: [  0/100]	Time  0.212 ( 0.212)	Loss 6.1669e-01 (6.1669e-01)	Acc@1  75.00 ( 75.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 6.6700e-01 (7.0873e-01)	Acc@1  77.00 ( 74.27)	Acc@5 100.00 ( 99.27)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 8.3382e-01 (6.9280e-01)	Acc@1  72.00 ( 75.48)	Acc@5  99.00 ( 99.05)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 7.3302e-01 (7.1893e-01)	Acc@1  73.00 ( 75.23)	Acc@5 100.00 ( 98.81)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 6.9477e-01 (7.2886e-01)	Acc@1  78.00 ( 75.34)	Acc@5 100.00 ( 98.63)
Test: [ 50/100]	Time  0.078 ( 0.080)	Loss 5.9332e-01 (7.1832e-01)	Acc@1  78.00 ( 75.67)	Acc@5  99.00 ( 98.67)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 6.1729e-01 (7.2696e-01)	Acc@1  73.00 ( 75.51)	Acc@5 100.00 ( 98.64)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 7.1272e-01 (7.2543e-01)	Acc@1  75.00 ( 75.39)	Acc@5  99.00 ( 98.59)
Test: [ 80/100]	Time  0.078 ( 0.079)	Loss 6.3757e-01 (7.2238e-01)	Acc@1  76.00 ( 75.51)	Acc@5 100.00 ( 98.64)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 5.5546e-01 (7.2581e-01)	Acc@1  79.00 ( 75.34)	Acc@5 100.00 ( 98.62)
 * Acc@1 75.310 Acc@5 98.580
### epoch[9] execution time: 102.2244701385498
EPOCH 10
# current learning rate: 0.1
# Switched to train mode...
Epoch: [10][  0/391]	Time  0.391 ( 0.391)	Data  0.150 ( 0.150)	Loss 9.4264e-01 (9.4264e-01)	Acc@1  70.31 ( 70.31)	Acc@5  98.44 ( 98.44)
Epoch: [10][ 10/391]	Time  0.240 ( 0.255)	Data  0.001 ( 0.015)	Loss 7.8674e-01 (7.4282e-01)	Acc@1  70.31 ( 74.72)	Acc@5  98.44 ( 98.44)
Epoch: [10][ 20/391]	Time  0.240 ( 0.248)	Data  0.001 ( 0.008)	Loss 6.7827e-01 (7.1666e-01)	Acc@1  78.91 ( 75.19)	Acc@5  99.22 ( 98.25)
Epoch: [10][ 30/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.006)	Loss 5.9018e-01 (6.9530e-01)	Acc@1  78.91 ( 75.53)	Acc@5  98.44 ( 98.41)
Epoch: [10][ 40/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.005)	Loss 5.7676e-01 (6.7940e-01)	Acc@1  79.69 ( 76.03)	Acc@5  99.22 ( 98.49)
Epoch: [10][ 50/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 5.4458e-01 (6.6935e-01)	Acc@1  82.03 ( 76.42)	Acc@5 100.00 ( 98.56)
Epoch: [10][ 60/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.004)	Loss 7.1682e-01 (6.6906e-01)	Acc@1  75.00 ( 76.52)	Acc@5  97.66 ( 98.53)
Epoch: [10][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 6.7329e-01 (6.6497e-01)	Acc@1  73.44 ( 76.55)	Acc@5 100.00 ( 98.64)
Epoch: [10][ 80/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 6.3118e-01 (6.6112e-01)	Acc@1  78.91 ( 76.73)	Acc@5  98.44 ( 98.70)
Epoch: [10][ 90/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 7.5047e-01 (6.6212e-01)	Acc@1  72.66 ( 76.75)	Acc@5  99.22 ( 98.67)
Epoch: [10][100/391]	Time  0.249 ( 0.242)	Data  0.001 ( 0.003)	Loss 6.5472e-01 (6.6609e-01)	Acc@1  79.69 ( 76.69)	Acc@5 100.00 ( 98.67)
Epoch: [10][110/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.003)	Loss 7.3133e-01 (6.6492e-01)	Acc@1  75.78 ( 76.71)	Acc@5  97.66 ( 98.63)
Epoch: [10][120/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.3651e-01 (6.6122e-01)	Acc@1  78.91 ( 76.94)	Acc@5  98.44 ( 98.62)
Epoch: [10][130/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.2874e-01 (6.6324e-01)	Acc@1  76.56 ( 76.91)	Acc@5  99.22 ( 98.60)
Epoch: [10][140/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.0414e-01 (6.5859e-01)	Acc@1  79.69 ( 77.14)	Acc@5  99.22 ( 98.60)
Epoch: [10][150/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.9423e-01 (6.6034e-01)	Acc@1  74.22 ( 77.11)	Acc@5  96.09 ( 98.58)
Epoch: [10][160/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.2347e-01 (6.6626e-01)	Acc@1  77.34 ( 76.93)	Acc@5  97.66 ( 98.51)
Epoch: [10][170/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.8679e-01 (6.6587e-01)	Acc@1  80.47 ( 76.92)	Acc@5  98.44 ( 98.50)
Epoch: [10][180/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.9149e-01 (6.6429e-01)	Acc@1  76.56 ( 76.95)	Acc@5  99.22 ( 98.50)
Epoch: [10][190/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.1495e-01 (6.5997e-01)	Acc@1  81.25 ( 77.13)	Acc@5 100.00 ( 98.50)
Epoch: [10][200/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.7205e-01 (6.6085e-01)	Acc@1  78.91 ( 77.04)	Acc@5  98.44 ( 98.52)
Epoch: [10][210/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.3059e-01 (6.6018e-01)	Acc@1  82.03 ( 77.05)	Acc@5  98.44 ( 98.53)
Epoch: [10][220/391]	Time  0.245 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.2200e-01 (6.5891e-01)	Acc@1  83.59 ( 77.14)	Acc@5 100.00 ( 98.55)
Epoch: [10][230/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.8940e-01 (6.5747e-01)	Acc@1  78.12 ( 77.21)	Acc@5  99.22 ( 98.55)
Epoch: [10][240/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.6805e-01 (6.5596e-01)	Acc@1  78.12 ( 77.25)	Acc@5  98.44 ( 98.55)
Epoch: [10][250/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.8373e-01 (6.5523e-01)	Acc@1  76.56 ( 77.26)	Acc@5  99.22 ( 98.54)
Epoch: [10][260/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.1795e-01 (6.5616e-01)	Acc@1  75.78 ( 77.23)	Acc@5 100.00 ( 98.55)
Epoch: [10][270/391]	Time  0.244 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.2595e-01 (6.5745e-01)	Acc@1  82.03 ( 77.18)	Acc@5 100.00 ( 98.53)
Epoch: [10][280/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.3178e-01 (6.5690e-01)	Acc@1  81.25 ( 77.19)	Acc@5 100.00 ( 98.53)
Epoch: [10][290/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.8883e-01 (6.5653e-01)	Acc@1  84.38 ( 77.22)	Acc@5  98.44 ( 98.51)
Epoch: [10][300/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.0283e-01 (6.5639e-01)	Acc@1  73.44 ( 77.20)	Acc@5  96.88 ( 98.51)
Epoch: [10][310/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.1052e-01 (6.5690e-01)	Acc@1  75.00 ( 77.22)	Acc@5  98.44 ( 98.52)
Epoch: [10][320/391]	Time  0.243 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.8225e-01 (6.5770e-01)	Acc@1  80.47 ( 77.19)	Acc@5 100.00 ( 98.52)
Epoch: [10][330/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.1941e-01 (6.5597e-01)	Acc@1  81.25 ( 77.29)	Acc@5  99.22 ( 98.53)
Epoch: [10][340/391]	Time  0.247 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.3655e-01 (6.5502e-01)	Acc@1  81.25 ( 77.31)	Acc@5  99.22 ( 98.56)
Epoch: [10][350/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.3361e-01 (6.5315e-01)	Acc@1  81.25 ( 77.37)	Acc@5  98.44 ( 98.57)
Epoch: [10][360/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.8111e-01 (6.5179e-01)	Acc@1  77.34 ( 77.41)	Acc@5  98.44 ( 98.58)
Epoch: [10][370/391]	Time  0.246 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.1176e-01 (6.5237e-01)	Acc@1  80.47 ( 77.39)	Acc@5 100.00 ( 98.57)
Epoch: [10][380/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.6550e-01 (6.5193e-01)	Acc@1  75.00 ( 77.40)	Acc@5  96.88 ( 98.57)
Epoch: [10][390/391]	Time  0.177 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.4351e-01 (6.5008e-01)	Acc@1  88.75 ( 77.49)	Acc@5  98.75 ( 98.59)
## e[10] optimizer.zero_grad (sum) time: 0.4769904613494873
## e[10]       loss.backward (sum) time: 10.764575481414795
## e[10]      optimizer.step (sum) time: 47.68367314338684
## epoch[10] training(only) time: 94.35301661491394
# Switched to evaluate mode...
Test: [  0/100]	Time  0.210 ( 0.210)	Loss 7.4705e-01 (7.4705e-01)	Acc@1  79.00 ( 79.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.077 ( 0.090)	Loss 7.8566e-01 (7.7584e-01)	Acc@1  80.00 ( 74.55)	Acc@5  98.00 ( 98.82)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 7.7092e-01 (7.8231e-01)	Acc@1  73.00 ( 73.90)	Acc@5 100.00 ( 98.67)
Test: [ 30/100]	Time  0.077 ( 0.082)	Loss 7.4919e-01 (7.8258e-01)	Acc@1  75.00 ( 73.94)	Acc@5 100.00 ( 98.68)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 8.3875e-01 (7.8074e-01)	Acc@1  78.00 ( 73.95)	Acc@5  97.00 ( 98.49)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 6.5794e-01 (7.7185e-01)	Acc@1  75.00 ( 74.27)	Acc@5 100.00 ( 98.59)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 6.4341e-01 (7.7366e-01)	Acc@1  79.00 ( 74.31)	Acc@5  99.00 ( 98.52)
Test: [ 70/100]	Time  0.077 ( 0.080)	Loss 9.2050e-01 (7.7723e-01)	Acc@1  73.00 ( 74.21)	Acc@5 100.00 ( 98.61)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 6.5366e-01 (7.7262e-01)	Acc@1  79.00 ( 74.37)	Acc@5  99.00 ( 98.64)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 6.3830e-01 (7.7839e-01)	Acc@1  74.00 ( 74.14)	Acc@5 100.00 ( 98.68)
 * Acc@1 74.170 Acc@5 98.650
### epoch[10] execution time: 102.37346124649048
EPOCH 11
# current learning rate: 0.1
# Switched to train mode...
Epoch: [11][  0/391]	Time  0.407 ( 0.407)	Data  0.153 ( 0.153)	Loss 6.2615e-01 (6.2615e-01)	Acc@1  80.47 ( 80.47)	Acc@5  98.44 ( 98.44)
Epoch: [11][ 10/391]	Time  0.240 ( 0.256)	Data  0.001 ( 0.015)	Loss 5.3839e-01 (6.0795e-01)	Acc@1  82.03 ( 78.69)	Acc@5 100.00 ( 99.08)
Epoch: [11][ 20/391]	Time  0.240 ( 0.249)	Data  0.001 ( 0.008)	Loss 6.9365e-01 (6.2666e-01)	Acc@1  77.34 ( 77.94)	Acc@5  99.22 ( 98.92)
Epoch: [11][ 30/391]	Time  0.240 ( 0.246)	Data  0.001 ( 0.006)	Loss 4.9920e-01 (6.3346e-01)	Acc@1  80.47 ( 78.10)	Acc@5 100.00 ( 98.79)
Epoch: [11][ 40/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.005)	Loss 6.0367e-01 (6.1308e-01)	Acc@1  78.91 ( 78.72)	Acc@5  98.44 ( 98.86)
Epoch: [11][ 50/391]	Time  0.244 ( 0.244)	Data  0.001 ( 0.004)	Loss 6.1617e-01 (6.1331e-01)	Acc@1  76.56 ( 78.57)	Acc@5  98.44 ( 98.90)
Epoch: [11][ 60/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.004)	Loss 5.8742e-01 (6.0713e-01)	Acc@1  78.91 ( 78.79)	Acc@5  99.22 ( 98.96)
Epoch: [11][ 70/391]	Time  0.239 ( 0.243)	Data  0.001 ( 0.003)	Loss 5.7110e-01 (6.0897e-01)	Acc@1  78.91 ( 78.64)	Acc@5  99.22 ( 98.98)
Epoch: [11][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 4.5102e-01 (6.0407e-01)	Acc@1  82.03 ( 78.79)	Acc@5 100.00 ( 98.97)
Epoch: [11][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 7.3935e-01 (6.0403e-01)	Acc@1  72.66 ( 78.81)	Acc@5  98.44 ( 98.94)
Epoch: [11][100/391]	Time  0.253 ( 0.243)	Data  0.001 ( 0.003)	Loss 6.2661e-01 (6.0763e-01)	Acc@1  78.12 ( 78.73)	Acc@5  99.22 ( 98.95)
Epoch: [11][110/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 4.6925e-01 (6.0933e-01)	Acc@1  84.38 ( 78.86)	Acc@5  99.22 ( 98.91)
Epoch: [11][120/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.2366e-01 (6.0705e-01)	Acc@1  77.34 ( 78.93)	Acc@5  97.66 ( 98.86)
Epoch: [11][130/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.3104e-01 (6.0291e-01)	Acc@1  78.91 ( 79.04)	Acc@5 100.00 ( 98.89)
Epoch: [11][140/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.5000e-01 (6.0400e-01)	Acc@1  79.69 ( 79.01)	Acc@5  97.66 ( 98.88)
Epoch: [11][150/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.1784e-01 (6.0808e-01)	Acc@1  75.00 ( 78.95)	Acc@5  96.88 ( 98.83)
Epoch: [11][160/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.6560e-01 (6.0753e-01)	Acc@1  85.16 ( 79.06)	Acc@5  99.22 ( 98.81)
Epoch: [11][170/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.6811e-01 (6.0598e-01)	Acc@1  78.91 ( 79.12)	Acc@5  98.44 ( 98.82)
Epoch: [11][180/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.8304e-01 (6.0705e-01)	Acc@1  75.00 ( 79.10)	Acc@5  99.22 ( 98.84)
Epoch: [11][190/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.4467e-01 (6.0960e-01)	Acc@1  81.25 ( 79.00)	Acc@5  98.44 ( 98.83)
Epoch: [11][200/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.1644e-01 (6.0957e-01)	Acc@1  75.78 ( 78.95)	Acc@5  99.22 ( 98.83)
Epoch: [11][210/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.9701e-01 (6.0928e-01)	Acc@1  78.91 ( 78.92)	Acc@5  99.22 ( 98.83)
Epoch: [11][220/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.0862e-01 (6.0954e-01)	Acc@1  77.34 ( 78.91)	Acc@5  96.88 ( 98.81)
Epoch: [11][230/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.4743e-01 (6.0869e-01)	Acc@1  82.03 ( 78.92)	Acc@5 100.00 ( 98.84)
Epoch: [11][240/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.0935e-01 (6.0646e-01)	Acc@1  83.59 ( 78.99)	Acc@5  98.44 ( 98.84)
Epoch: [11][250/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.4389e-01 (6.0364e-01)	Acc@1  80.47 ( 79.12)	Acc@5 100.00 ( 98.86)
Epoch: [11][260/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.1615e-01 (6.0318e-01)	Acc@1  80.47 ( 79.11)	Acc@5  98.44 ( 98.84)
Epoch: [11][270/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.3476e-01 (6.0403e-01)	Acc@1  75.00 ( 79.09)	Acc@5  97.66 ( 98.84)
Epoch: [11][280/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.8058e-01 (6.0417e-01)	Acc@1  78.12 ( 79.07)	Acc@5  98.44 ( 98.85)
Epoch: [11][290/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.8927e-01 (6.0290e-01)	Acc@1  83.59 ( 79.13)	Acc@5 100.00 ( 98.85)
Epoch: [11][300/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.4689e-01 (6.0278e-01)	Acc@1  84.38 ( 79.12)	Acc@5  97.66 ( 98.84)
Epoch: [11][310/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.5872e-01 (6.0330e-01)	Acc@1  75.00 ( 79.11)	Acc@5  97.66 ( 98.83)
Epoch: [11][320/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.4928e-01 (6.0170e-01)	Acc@1  75.78 ( 79.14)	Acc@5  99.22 ( 98.84)
Epoch: [11][330/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.4035e-01 (6.0139e-01)	Acc@1  77.34 ( 79.14)	Acc@5  99.22 ( 98.83)
Epoch: [11][340/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.3477e-01 (6.0141e-01)	Acc@1  80.47 ( 79.10)	Acc@5  99.22 ( 98.83)
Epoch: [11][350/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.6882e-01 (6.0178e-01)	Acc@1  76.56 ( 79.10)	Acc@5  99.22 ( 98.83)
Epoch: [11][360/391]	Time  0.246 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.2067e-01 (6.0255e-01)	Acc@1  77.34 ( 79.09)	Acc@5  99.22 ( 98.82)
Epoch: [11][370/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.4014e-01 (6.0265e-01)	Acc@1  80.47 ( 79.06)	Acc@5  99.22 ( 98.82)
Epoch: [11][380/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.0088e-01 (6.0153e-01)	Acc@1  78.91 ( 79.09)	Acc@5  99.22 ( 98.83)
Epoch: [11][390/391]	Time  0.177 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.2160e-01 (6.0012e-01)	Acc@1  85.00 ( 79.14)	Acc@5 100.00 ( 98.83)
## e[11] optimizer.zero_grad (sum) time: 0.4762699604034424
## e[11]       loss.backward (sum) time: 10.850945711135864
## e[11]      optimizer.step (sum) time: 47.64725422859192
## epoch[11] training(only) time: 94.38599920272827
# Switched to evaluate mode...
Test: [  0/100]	Time  0.215 ( 0.215)	Loss 6.7995e-01 (6.7995e-01)	Acc@1  80.00 ( 80.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 6.5382e-01 (6.5136e-01)	Acc@1  81.00 ( 76.82)	Acc@5  99.00 ( 99.00)
Test: [ 20/100]	Time  0.079 ( 0.084)	Loss 6.8491e-01 (6.5497e-01)	Acc@1  80.00 ( 77.38)	Acc@5 100.00 ( 98.81)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 6.3179e-01 (6.7243e-01)	Acc@1  83.00 ( 77.48)	Acc@5  98.00 ( 98.81)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 7.8633e-01 (6.7902e-01)	Acc@1  73.00 ( 77.34)	Acc@5  98.00 ( 98.80)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 6.6017e-01 (6.7329e-01)	Acc@1  77.00 ( 77.45)	Acc@5  99.00 ( 98.82)
Test: [ 60/100]	Time  0.077 ( 0.080)	Loss 7.8226e-01 (6.7801e-01)	Acc@1  74.00 ( 77.08)	Acc@5  99.00 ( 98.85)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 7.9219e-01 (6.8269e-01)	Acc@1  75.00 ( 77.00)	Acc@5  98.00 ( 98.79)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 6.1671e-01 (6.8152e-01)	Acc@1  78.00 ( 77.11)	Acc@5  99.00 ( 98.77)
Test: [ 90/100]	Time  0.077 ( 0.079)	Loss 6.4804e-01 (6.8189e-01)	Acc@1  76.00 ( 77.08)	Acc@5 100.00 ( 98.77)
 * Acc@1 77.220 Acc@5 98.690
### epoch[11] execution time: 102.40039658546448
EPOCH 12
# current learning rate: 0.1
# Switched to train mode...
Epoch: [12][  0/391]	Time  0.400 ( 0.400)	Data  0.155 ( 0.155)	Loss 5.1949e-01 (5.1949e-01)	Acc@1  80.47 ( 80.47)	Acc@5  99.22 ( 99.22)
Epoch: [12][ 10/391]	Time  0.240 ( 0.255)	Data  0.001 ( 0.015)	Loss 4.7279e-01 (5.6100e-01)	Acc@1  81.25 ( 80.61)	Acc@5  99.22 ( 99.29)
Epoch: [12][ 20/391]	Time  0.241 ( 0.248)	Data  0.001 ( 0.008)	Loss 4.3219e-01 (5.4018e-01)	Acc@1  80.47 ( 81.21)	Acc@5 100.00 ( 99.18)
Epoch: [12][ 30/391]	Time  0.240 ( 0.246)	Data  0.001 ( 0.006)	Loss 7.2666e-01 (5.5542e-01)	Acc@1  78.91 ( 80.97)	Acc@5  96.88 ( 98.89)
Epoch: [12][ 40/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.005)	Loss 5.6713e-01 (5.5368e-01)	Acc@1  82.03 ( 81.00)	Acc@5  99.22 ( 98.91)
Epoch: [12][ 50/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 6.2635e-01 (5.5072e-01)	Acc@1  78.12 ( 81.19)	Acc@5  98.44 ( 98.96)
Epoch: [12][ 60/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 7.5816e-01 (5.5537e-01)	Acc@1  75.78 ( 81.06)	Acc@5  98.44 ( 98.91)
Epoch: [12][ 70/391]	Time  0.246 ( 0.243)	Data  0.001 ( 0.003)	Loss 5.3868e-01 (5.5604e-01)	Acc@1  83.59 ( 81.00)	Acc@5  99.22 ( 98.93)
Epoch: [12][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 5.3104e-01 (5.5372e-01)	Acc@1  80.47 ( 81.02)	Acc@5  99.22 ( 98.96)
Epoch: [12][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 5.8496e-01 (5.5313e-01)	Acc@1  75.78 ( 80.89)	Acc@5  98.44 ( 98.98)
Epoch: [12][100/391]	Time  0.239 ( 0.243)	Data  0.001 ( 0.003)	Loss 6.0808e-01 (5.5564e-01)	Acc@1  81.25 ( 80.82)	Acc@5  98.44 ( 98.93)
Epoch: [12][110/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 5.3731e-01 (5.5762e-01)	Acc@1  82.03 ( 80.79)	Acc@5  99.22 ( 98.92)
Epoch: [12][120/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.4965e-01 (5.5669e-01)	Acc@1  81.25 ( 80.79)	Acc@5 100.00 ( 98.93)
Epoch: [12][130/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.8198e-01 (5.6127e-01)	Acc@1  77.34 ( 80.66)	Acc@5  99.22 ( 98.95)
Epoch: [12][140/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.8494e-01 (5.6134e-01)	Acc@1  80.47 ( 80.61)	Acc@5  99.22 ( 98.96)
Epoch: [12][150/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.5620e-01 (5.5965e-01)	Acc@1  79.69 ( 80.73)	Acc@5  97.66 ( 98.97)
Epoch: [12][160/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.7955e-01 (5.5477e-01)	Acc@1  85.94 ( 80.88)	Acc@5  99.22 ( 98.97)
Epoch: [12][170/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.6836e-01 (5.5406e-01)	Acc@1  82.81 ( 80.85)	Acc@5 100.00 ( 98.99)
Epoch: [12][180/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.5532e-01 (5.5431e-01)	Acc@1  83.59 ( 80.81)	Acc@5 100.00 ( 98.98)
Epoch: [12][190/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.5979e-01 (5.5259e-01)	Acc@1  86.72 ( 80.89)	Acc@5  98.44 ( 99.01)
Epoch: [12][200/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.7904e-01 (5.5121e-01)	Acc@1  81.25 ( 80.98)	Acc@5  98.44 ( 99.01)
Epoch: [12][210/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.2130e-01 (5.5036e-01)	Acc@1  83.59 ( 81.01)	Acc@5 100.00 ( 99.03)
Epoch: [12][220/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.1857e-01 (5.5018e-01)	Acc@1  82.81 ( 81.02)	Acc@5  98.44 ( 99.02)
Epoch: [12][230/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.0688e-01 (5.5168e-01)	Acc@1  78.12 ( 81.02)	Acc@5  99.22 ( 99.03)
Epoch: [12][240/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.6308e-01 (5.5176e-01)	Acc@1  84.38 ( 81.05)	Acc@5  99.22 ( 99.02)
Epoch: [12][250/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.7941e-01 (5.5366e-01)	Acc@1  75.78 ( 80.94)	Acc@5  97.66 ( 98.99)
Epoch: [12][260/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.5921e-01 (5.5404e-01)	Acc@1  81.25 ( 80.97)	Acc@5 100.00 ( 98.99)
Epoch: [12][270/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.8109e-01 (5.5444e-01)	Acc@1  82.03 ( 80.94)	Acc@5  99.22 ( 98.99)
Epoch: [12][280/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.2221e-01 (5.5388e-01)	Acc@1  82.81 ( 80.99)	Acc@5  99.22 ( 98.99)
Epoch: [12][290/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.1135e-01 (5.5348e-01)	Acc@1  81.25 ( 81.00)	Acc@5  99.22 ( 98.99)
Epoch: [12][300/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.2052e-01 (5.5405e-01)	Acc@1  75.78 ( 81.00)	Acc@5  99.22 ( 98.98)
Epoch: [12][310/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.0571e-01 (5.5404e-01)	Acc@1  89.06 ( 80.99)	Acc@5 100.00 ( 98.99)
Epoch: [12][320/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.0595e-01 (5.5355e-01)	Acc@1  81.25 ( 80.99)	Acc@5 100.00 ( 98.98)
Epoch: [12][330/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.1610e-01 (5.5225e-01)	Acc@1  77.34 ( 81.03)	Acc@5  99.22 ( 98.99)
Epoch: [12][340/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.6667e-01 (5.5346e-01)	Acc@1  84.38 ( 81.02)	Acc@5  99.22 ( 98.98)
Epoch: [12][350/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.4108e-01 (5.5271e-01)	Acc@1  78.91 ( 81.04)	Acc@5  99.22 ( 98.98)
Epoch: [12][360/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.3861e-01 (5.5184e-01)	Acc@1  83.59 ( 81.05)	Acc@5 100.00 ( 98.99)
Epoch: [12][370/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.6753e-01 (5.5074e-01)	Acc@1  81.25 ( 81.08)	Acc@5  97.66 ( 98.99)
Epoch: [12][380/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.5547e-01 (5.4927e-01)	Acc@1  82.81 ( 81.14)	Acc@5 100.00 ( 98.99)
Epoch: [12][390/391]	Time  0.177 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.4325e-01 (5.4882e-01)	Acc@1  77.50 ( 81.15)	Acc@5 100.00 ( 98.99)
## e[12] optimizer.zero_grad (sum) time: 0.4769623279571533
## e[12]       loss.backward (sum) time: 10.857983827590942
## e[12]      optimizer.step (sum) time: 47.618350982666016
## epoch[12] training(only) time: 94.46644449234009
# Switched to evaluate mode...
Test: [  0/100]	Time  0.211 ( 0.211)	Loss 6.3805e-01 (6.3805e-01)	Acc@1  78.00 ( 78.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 4.9574e-01 (6.3483e-01)	Acc@1  88.00 ( 78.82)	Acc@5  99.00 ( 99.00)
Test: [ 20/100]	Time  0.079 ( 0.084)	Loss 5.3941e-01 (6.1888e-01)	Acc@1  81.00 ( 79.57)	Acc@5 100.00 ( 98.76)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 5.7281e-01 (6.1310e-01)	Acc@1  80.00 ( 79.94)	Acc@5  99.00 ( 98.87)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 5.7554e-01 (6.0726e-01)	Acc@1  79.00 ( 79.95)	Acc@5 100.00 ( 98.83)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 4.1798e-01 (6.0145e-01)	Acc@1  87.00 ( 80.20)	Acc@5  98.00 ( 98.86)
Test: [ 60/100]	Time  0.079 ( 0.080)	Loss 7.4195e-01 (6.0462e-01)	Acc@1  80.00 ( 80.02)	Acc@5  99.00 ( 98.97)
Test: [ 70/100]	Time  0.077 ( 0.080)	Loss 5.2861e-01 (6.0331e-01)	Acc@1  82.00 ( 79.97)	Acc@5  99.00 ( 99.00)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 5.3393e-01 (5.9887e-01)	Acc@1  77.00 ( 80.05)	Acc@5  99.00 ( 98.95)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 5.3097e-01 (5.9848e-01)	Acc@1  86.00 ( 80.14)	Acc@5  98.00 ( 98.90)
 * Acc@1 80.090 Acc@5 98.920
### epoch[12] execution time: 102.51640939712524
EPOCH 13
# current learning rate: 0.1
# Switched to train mode...
Epoch: [13][  0/391]	Time  0.400 ( 0.400)	Data  0.159 ( 0.159)	Loss 3.8115e-01 (3.8115e-01)	Acc@1  86.72 ( 86.72)	Acc@5 100.00 (100.00)
Epoch: [13][ 10/391]	Time  0.240 ( 0.255)	Data  0.001 ( 0.015)	Loss 4.2796e-01 (4.7977e-01)	Acc@1  83.59 ( 83.66)	Acc@5  99.22 ( 99.29)
Epoch: [13][ 20/391]	Time  0.240 ( 0.248)	Data  0.001 ( 0.009)	Loss 4.1443e-01 (4.8583e-01)	Acc@1  85.94 ( 83.26)	Acc@5  99.22 ( 99.22)
Epoch: [13][ 30/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.006)	Loss 5.6146e-01 (4.9855e-01)	Acc@1  81.25 ( 82.59)	Acc@5  98.44 ( 99.24)
Epoch: [13][ 40/391]	Time  0.246 ( 0.245)	Data  0.001 ( 0.005)	Loss 4.3479e-01 (4.9787e-01)	Acc@1  85.94 ( 82.79)	Acc@5  99.22 ( 99.31)
Epoch: [13][ 50/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 6.1236e-01 (5.1079e-01)	Acc@1  81.25 ( 82.63)	Acc@5  99.22 ( 99.25)
Epoch: [13][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 6.1915e-01 (5.1601e-01)	Acc@1  81.25 ( 82.57)	Acc@5  98.44 ( 99.28)
Epoch: [13][ 70/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 4.9201e-01 (5.1333e-01)	Acc@1  81.25 ( 82.53)	Acc@5 100.00 ( 99.28)
Epoch: [13][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 4.3846e-01 (5.0838e-01)	Acc@1  82.81 ( 82.74)	Acc@5 100.00 ( 99.31)
Epoch: [13][ 90/391]	Time  0.246 ( 0.243)	Data  0.001 ( 0.003)	Loss 4.5295e-01 (5.0797e-01)	Acc@1  85.94 ( 82.62)	Acc@5  98.44 ( 99.32)
Epoch: [13][100/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 5.1312e-01 (5.1014e-01)	Acc@1  82.81 ( 82.51)	Acc@5  99.22 ( 99.33)
Epoch: [13][110/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 4.5515e-01 (5.0635e-01)	Acc@1  82.03 ( 82.59)	Acc@5  99.22 ( 99.32)
Epoch: [13][120/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.5815e-01 (5.0710e-01)	Acc@1  82.81 ( 82.62)	Acc@5  99.22 ( 99.27)
Epoch: [13][130/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.7448e-01 (5.0530e-01)	Acc@1  80.47 ( 82.61)	Acc@5 100.00 ( 99.30)
Epoch: [13][140/391]	Time  0.246 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.5211e-01 (5.0750e-01)	Acc@1  80.47 ( 82.57)	Acc@5  96.09 ( 99.24)
Epoch: [13][150/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.9151e-01 (5.0913e-01)	Acc@1  84.38 ( 82.50)	Acc@5  99.22 ( 99.19)
Epoch: [13][160/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.3797e-01 (5.1289e-01)	Acc@1  78.12 ( 82.42)	Acc@5 100.00 ( 99.19)
Epoch: [13][170/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.8665e-01 (5.1144e-01)	Acc@1  77.34 ( 82.40)	Acc@5  99.22 ( 99.20)
Epoch: [13][180/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.2140e-01 (5.1069e-01)	Acc@1  86.72 ( 82.42)	Acc@5 100.00 ( 99.23)
Epoch: [13][190/391]	Time  0.248 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.4178e-01 (5.1044e-01)	Acc@1  82.03 ( 82.37)	Acc@5  96.88 ( 99.21)
Epoch: [13][200/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.7622e-01 (5.1192e-01)	Acc@1  84.38 ( 82.29)	Acc@5  98.44 ( 99.21)
Epoch: [13][210/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.1602e-01 (5.1174e-01)	Acc@1  86.72 ( 82.29)	Acc@5 100.00 ( 99.19)
Epoch: [13][220/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.0339e-01 (5.1337e-01)	Acc@1  86.72 ( 82.24)	Acc@5  99.22 ( 99.17)
Epoch: [13][230/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.4719e-01 (5.1345e-01)	Acc@1  76.56 ( 82.21)	Acc@5  99.22 ( 99.18)
Epoch: [13][240/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.0568e-01 (5.1326e-01)	Acc@1  83.59 ( 82.22)	Acc@5 100.00 ( 99.18)
Epoch: [13][250/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.9672e-01 (5.1506e-01)	Acc@1  77.34 ( 82.17)	Acc@5  99.22 ( 99.16)
Epoch: [13][260/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.0929e-01 (5.1656e-01)	Acc@1  80.47 ( 82.16)	Acc@5 100.00 ( 99.16)
Epoch: [13][270/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.2299e-01 (5.1667e-01)	Acc@1  79.69 ( 82.11)	Acc@5 100.00 ( 99.15)
Epoch: [13][280/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.3139e-01 (5.1553e-01)	Acc@1  87.50 ( 82.16)	Acc@5  99.22 ( 99.14)
Epoch: [13][290/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.3971e-01 (5.1443e-01)	Acc@1  82.81 ( 82.18)	Acc@5  98.44 ( 99.14)
Epoch: [13][300/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.5676e-01 (5.1446e-01)	Acc@1  80.47 ( 82.18)	Acc@5  98.44 ( 99.14)
Epoch: [13][310/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.6865e-01 (5.1394e-01)	Acc@1  85.94 ( 82.24)	Acc@5  97.66 ( 99.13)
Epoch: [13][320/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.2586e-01 (5.1395e-01)	Acc@1  81.25 ( 82.22)	Acc@5  99.22 ( 99.14)
Epoch: [13][330/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.5658e-01 (5.1318e-01)	Acc@1  85.16 ( 82.26)	Acc@5  99.22 ( 99.14)
Epoch: [13][340/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.1758e-01 (5.1194e-01)	Acc@1  81.25 ( 82.31)	Acc@5  99.22 ( 99.14)
Epoch: [13][350/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.6611e-01 (5.1292e-01)	Acc@1  76.56 ( 82.27)	Acc@5  98.44 ( 99.13)
Epoch: [13][360/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.5111e-01 (5.1237e-01)	Acc@1  82.03 ( 82.30)	Acc@5 100.00 ( 99.13)
Epoch: [13][370/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.4565e-01 (5.1263e-01)	Acc@1  75.00 ( 82.30)	Acc@5  98.44 ( 99.13)
Epoch: [13][380/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.0405e-01 (5.1299e-01)	Acc@1  85.94 ( 82.28)	Acc@5 100.00 ( 99.14)
Epoch: [13][390/391]	Time  0.177 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.0944e-01 (5.1220e-01)	Acc@1  81.25 ( 82.31)	Acc@5  98.75 ( 99.13)
## e[13] optimizer.zero_grad (sum) time: 0.477158784866333
## e[13]       loss.backward (sum) time: 10.815807819366455
## e[13]      optimizer.step (sum) time: 47.65683650970459
## epoch[13] training(only) time: 94.41835856437683
# Switched to evaluate mode...
Test: [  0/100]	Time  0.218 ( 0.218)	Loss 5.9647e-01 (5.9647e-01)	Acc@1  81.00 ( 81.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.078 ( 0.091)	Loss 5.1467e-01 (5.5929e-01)	Acc@1  80.00 ( 80.73)	Acc@5  99.00 ( 99.27)
Test: [ 20/100]	Time  0.078 ( 0.085)	Loss 6.9025e-01 (5.6512e-01)	Acc@1  76.00 ( 80.81)	Acc@5 100.00 ( 99.14)
Test: [ 30/100]	Time  0.077 ( 0.082)	Loss 6.1087e-01 (5.8170e-01)	Acc@1  80.00 ( 80.84)	Acc@5 100.00 ( 99.06)
Test: [ 40/100]	Time  0.079 ( 0.081)	Loss 5.1401e-01 (5.8065e-01)	Acc@1  83.00 ( 80.80)	Acc@5 100.00 ( 99.20)
Test: [ 50/100]	Time  0.077 ( 0.081)	Loss 4.3328e-01 (5.7292e-01)	Acc@1  82.00 ( 81.12)	Acc@5  98.00 ( 99.10)
Test: [ 60/100]	Time  0.079 ( 0.080)	Loss 6.0563e-01 (5.7652e-01)	Acc@1  78.00 ( 81.00)	Acc@5 100.00 ( 99.15)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 6.3881e-01 (5.7888e-01)	Acc@1  79.00 ( 80.92)	Acc@5 100.00 ( 99.13)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 4.6320e-01 (5.7823e-01)	Acc@1  81.00 ( 80.88)	Acc@5  99.00 ( 99.11)
Test: [ 90/100]	Time  0.077 ( 0.079)	Loss 5.1580e-01 (5.8065e-01)	Acc@1  79.00 ( 80.64)	Acc@5 100.00 ( 99.08)
 * Acc@1 80.780 Acc@5 99.080
### epoch[13] execution time: 102.46302771568298
EPOCH 14
# current learning rate: 0.1
# Switched to train mode...
Epoch: [14][  0/391]	Time  0.386 ( 0.386)	Data  0.142 ( 0.142)	Loss 4.2038e-01 (4.2038e-01)	Acc@1  82.81 ( 82.81)	Acc@5  99.22 ( 99.22)
Epoch: [14][ 10/391]	Time  0.239 ( 0.253)	Data  0.001 ( 0.014)	Loss 4.3588e-01 (4.6905e-01)	Acc@1  85.16 ( 84.23)	Acc@5 100.00 ( 99.08)
Epoch: [14][ 20/391]	Time  0.240 ( 0.247)	Data  0.001 ( 0.008)	Loss 4.7367e-01 (4.7271e-01)	Acc@1  82.81 ( 83.63)	Acc@5  99.22 ( 99.26)
Epoch: [14][ 30/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.006)	Loss 3.6266e-01 (4.8352e-01)	Acc@1  86.72 ( 83.62)	Acc@5 100.00 ( 99.19)
Epoch: [14][ 40/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.005)	Loss 4.0391e-01 (4.7331e-01)	Acc@1  85.94 ( 83.67)	Acc@5  98.44 ( 99.29)
Epoch: [14][ 50/391]	Time  0.239 ( 0.244)	Data  0.001 ( 0.004)	Loss 3.8158e-01 (4.7358e-01)	Acc@1  87.50 ( 83.81)	Acc@5 100.00 ( 99.33)
Epoch: [14][ 60/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 4.8173e-01 (4.7255e-01)	Acc@1  82.81 ( 83.76)	Acc@5 100.00 ( 99.33)
Epoch: [14][ 70/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 4.2063e-01 (4.7150e-01)	Acc@1  85.16 ( 83.71)	Acc@5 100.00 ( 99.32)
Epoch: [14][ 80/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 5.6704e-01 (4.7243e-01)	Acc@1  76.56 ( 83.81)	Acc@5  99.22 ( 99.28)
Epoch: [14][ 90/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 4.3498e-01 (4.7038e-01)	Acc@1  82.81 ( 83.86)	Acc@5 100.00 ( 99.26)
Epoch: [14][100/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 4.3695e-01 (4.7071e-01)	Acc@1  86.72 ( 83.89)	Acc@5  99.22 ( 99.21)
Epoch: [14][110/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.2765e-01 (4.7525e-01)	Acc@1  81.25 ( 83.65)	Acc@5  98.44 ( 99.21)
Epoch: [14][120/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.1768e-01 (4.7401e-01)	Acc@1  85.94 ( 83.62)	Acc@5 100.00 ( 99.24)
Epoch: [14][130/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.5642e-01 (4.7447e-01)	Acc@1  80.47 ( 83.60)	Acc@5 100.00 ( 99.25)
Epoch: [14][140/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.4625e-01 (4.7312e-01)	Acc@1  85.94 ( 83.68)	Acc@5  98.44 ( 99.26)
Epoch: [14][150/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.5483e-01 (4.7434e-01)	Acc@1  85.16 ( 83.63)	Acc@5 100.00 ( 99.28)
Epoch: [14][160/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.9668e-01 (4.7763e-01)	Acc@1  87.50 ( 83.51)	Acc@5  98.44 ( 99.26)
Epoch: [14][170/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.4217e-01 (4.7722e-01)	Acc@1  82.81 ( 83.48)	Acc@5 100.00 ( 99.26)
Epoch: [14][180/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.3130e-01 (4.7931e-01)	Acc@1  82.81 ( 83.42)	Acc@5  96.88 ( 99.22)
Epoch: [14][190/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.5020e-01 (4.7816e-01)	Acc@1  84.38 ( 83.45)	Acc@5 100.00 ( 99.23)
Epoch: [14][200/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.2617e-01 (4.8058e-01)	Acc@1  84.38 ( 83.38)	Acc@5  99.22 ( 99.23)
Epoch: [14][210/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.6787e-01 (4.8152e-01)	Acc@1  86.72 ( 83.35)	Acc@5  96.88 ( 99.22)
Epoch: [14][220/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.6216e-01 (4.8238e-01)	Acc@1  81.25 ( 83.33)	Acc@5  99.22 ( 99.21)
Epoch: [14][230/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.8762e-01 (4.8157e-01)	Acc@1  85.16 ( 83.30)	Acc@5  99.22 ( 99.22)
Epoch: [14][240/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.4455e-01 (4.8171e-01)	Acc@1  80.47 ( 83.32)	Acc@5  99.22 ( 99.21)
Epoch: [14][250/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.1550e-01 (4.8110e-01)	Acc@1  85.16 ( 83.33)	Acc@5  99.22 ( 99.21)
Epoch: [14][260/391]	Time  0.244 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.4458e-01 (4.8032e-01)	Acc@1  82.81 ( 83.34)	Acc@5  99.22 ( 99.22)
Epoch: [14][270/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.6104e-01 (4.8092e-01)	Acc@1  85.16 ( 83.29)	Acc@5  99.22 ( 99.22)
Epoch: [14][280/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.5734e-01 (4.7967e-01)	Acc@1  84.38 ( 83.34)	Acc@5 100.00 ( 99.22)
Epoch: [14][290/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.2887e-01 (4.7804e-01)	Acc@1  85.16 ( 83.41)	Acc@5  99.22 ( 99.23)
Epoch: [14][300/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.3313e-01 (4.7786e-01)	Acc@1  82.03 ( 83.43)	Acc@5 100.00 ( 99.24)
Epoch: [14][310/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.8126e-01 (4.7770e-01)	Acc@1  87.50 ( 83.44)	Acc@5  99.22 ( 99.24)
Epoch: [14][320/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.0744e-01 (4.7843e-01)	Acc@1  81.25 ( 83.42)	Acc@5  98.44 ( 99.24)
Epoch: [14][330/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.6236e-01 (4.7781e-01)	Acc@1  79.69 ( 83.44)	Acc@5 100.00 ( 99.24)
Epoch: [14][340/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.0808e-01 (4.7736e-01)	Acc@1  84.38 ( 83.45)	Acc@5 100.00 ( 99.24)
Epoch: [14][350/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.5467e-01 (4.7871e-01)	Acc@1  82.03 ( 83.42)	Acc@5  96.88 ( 99.25)
Epoch: [14][360/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.6697e-01 (4.7866e-01)	Acc@1  81.25 ( 83.43)	Acc@5  99.22 ( 99.26)
Epoch: [14][370/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.4841e-01 (4.7818e-01)	Acc@1  78.91 ( 83.45)	Acc@5  98.44 ( 99.26)
Epoch: [14][380/391]	Time  0.245 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.6217e-01 (4.7807e-01)	Acc@1  83.59 ( 83.49)	Acc@5  98.44 ( 99.24)
Epoch: [14][390/391]	Time  0.176 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.8443e-01 (4.7698e-01)	Acc@1  82.50 ( 83.54)	Acc@5  98.75 ( 99.24)
## e[14] optimizer.zero_grad (sum) time: 0.47696638107299805
## e[14]       loss.backward (sum) time: 10.856989860534668
## e[14]      optimizer.step (sum) time: 47.60832500457764
## epoch[14] training(only) time: 94.35123467445374
# Switched to evaluate mode...
Test: [  0/100]	Time  0.207 ( 0.207)	Loss 6.6884e-01 (6.6884e-01)	Acc@1  78.00 ( 78.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.077 ( 0.089)	Loss 5.9746e-01 (5.9621e-01)	Acc@1  82.00 ( 80.18)	Acc@5  98.00 ( 99.18)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 5.8269e-01 (6.1549e-01)	Acc@1  81.00 ( 79.38)	Acc@5  99.00 ( 98.76)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 7.0700e-01 (6.0829e-01)	Acc@1  75.00 ( 79.68)	Acc@5  99.00 ( 98.77)
Test: [ 40/100]	Time  0.077 ( 0.081)	Loss 7.2066e-01 (6.0713e-01)	Acc@1  80.00 ( 79.90)	Acc@5  99.00 ( 98.83)
Test: [ 50/100]	Time  0.079 ( 0.080)	Loss 5.4569e-01 (5.9388e-01)	Acc@1  82.00 ( 80.24)	Acc@5  98.00 ( 98.84)
Test: [ 60/100]	Time  0.077 ( 0.080)	Loss 5.4611e-01 (6.0000e-01)	Acc@1  84.00 ( 79.95)	Acc@5  98.00 ( 98.97)
Test: [ 70/100]	Time  0.079 ( 0.080)	Loss 6.2030e-01 (6.0155e-01)	Acc@1  82.00 ( 79.96)	Acc@5  98.00 ( 99.00)
Test: [ 80/100]	Time  0.078 ( 0.079)	Loss 6.0226e-01 (6.0066e-01)	Acc@1  81.00 ( 79.99)	Acc@5  98.00 ( 99.01)
Test: [ 90/100]	Time  0.076 ( 0.079)	Loss 4.9849e-01 (6.0712e-01)	Acc@1  82.00 ( 79.80)	Acc@5 100.00 ( 98.99)
 * Acc@1 79.910 Acc@5 98.990
### epoch[14] execution time: 102.3467288017273
EPOCH 15
# current learning rate: 0.1
# Switched to train mode...
Epoch: [15][  0/391]	Time  0.389 ( 0.389)	Data  0.136 ( 0.136)	Loss 4.3353e-01 (4.3353e-01)	Acc@1  85.94 ( 85.94)	Acc@5 100.00 (100.00)
Epoch: [15][ 10/391]	Time  0.239 ( 0.254)	Data  0.001 ( 0.013)	Loss 3.8294e-01 (4.4043e-01)	Acc@1  91.41 ( 85.87)	Acc@5 100.00 ( 99.50)
Epoch: [15][ 20/391]	Time  0.246 ( 0.248)	Data  0.001 ( 0.008)	Loss 3.3537e-01 (4.3849e-01)	Acc@1  88.28 ( 85.83)	Acc@5 100.00 ( 99.55)
Epoch: [15][ 30/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.006)	Loss 4.0594e-01 (4.4077e-01)	Acc@1  88.28 ( 85.69)	Acc@5  99.22 ( 99.42)
Epoch: [15][ 40/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 4.3659e-01 (4.3432e-01)	Acc@1  83.59 ( 86.01)	Acc@5 100.00 ( 99.43)
Epoch: [15][ 50/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 5.6838e-01 (4.4678e-01)	Acc@1  80.47 ( 85.60)	Acc@5  99.22 ( 99.43)
Epoch: [15][ 60/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.5228e-01 (4.4298e-01)	Acc@1  89.84 ( 85.53)	Acc@5  99.22 ( 99.49)
Epoch: [15][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.7226e-01 (4.3992e-01)	Acc@1  83.59 ( 85.60)	Acc@5 100.00 ( 99.48)
Epoch: [15][ 80/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 4.7637e-01 (4.3577e-01)	Acc@1  83.59 ( 85.58)	Acc@5  97.66 ( 99.45)
Epoch: [15][ 90/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 4.3740e-01 (4.3825e-01)	Acc@1  85.94 ( 85.40)	Acc@5  98.44 ( 99.42)
Epoch: [15][100/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 3.9963e-01 (4.3827e-01)	Acc@1  86.72 ( 85.39)	Acc@5  99.22 ( 99.40)
Epoch: [15][110/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.6434e-01 (4.4048e-01)	Acc@1  90.62 ( 85.31)	Acc@5  98.44 ( 99.37)
Epoch: [15][120/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8193e-01 (4.3940e-01)	Acc@1  89.84 ( 85.34)	Acc@5 100.00 ( 99.36)
Epoch: [15][130/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.7307e-01 (4.3966e-01)	Acc@1  83.59 ( 85.26)	Acc@5 100.00 ( 99.33)
Epoch: [15][140/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.7559e-01 (4.3875e-01)	Acc@1  88.28 ( 85.23)	Acc@5 100.00 ( 99.35)
Epoch: [15][150/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.6220e-01 (4.4079e-01)	Acc@1  78.91 ( 85.11)	Acc@5  99.22 ( 99.33)
Epoch: [15][160/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.3921e-01 (4.4358e-01)	Acc@1  82.81 ( 85.01)	Acc@5 100.00 ( 99.32)
Epoch: [15][170/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.9957e-01 (4.4337e-01)	Acc@1  80.47 ( 84.97)	Acc@5  98.44 ( 99.31)
Epoch: [15][180/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.9510e-01 (4.4298e-01)	Acc@1  86.72 ( 84.95)	Acc@5  97.66 ( 99.31)
Epoch: [15][190/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.5184e-01 (4.4205e-01)	Acc@1  86.72 ( 84.94)	Acc@5  99.22 ( 99.34)
Epoch: [15][200/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.3772e-01 (4.4375e-01)	Acc@1  85.16 ( 84.88)	Acc@5  97.66 ( 99.33)
Epoch: [15][210/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.8765e-01 (4.4407e-01)	Acc@1  85.94 ( 84.85)	Acc@5  99.22 ( 99.33)
Epoch: [15][220/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.2387e-01 (4.4589e-01)	Acc@1  85.16 ( 84.79)	Acc@5 100.00 ( 99.34)
Epoch: [15][230/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.2635e-01 (4.4674e-01)	Acc@1  88.28 ( 84.79)	Acc@5  99.22 ( 99.32)
Epoch: [15][240/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.3177e-01 (4.4892e-01)	Acc@1  78.12 ( 84.68)	Acc@5  96.88 ( 99.30)
Epoch: [15][250/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.5384e-01 (4.4782e-01)	Acc@1  85.16 ( 84.73)	Acc@5 100.00 ( 99.30)
Epoch: [15][260/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.8162e-01 (4.4697e-01)	Acc@1  82.03 ( 84.72)	Acc@5 100.00 ( 99.31)
Epoch: [15][270/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.8631e-01 (4.4901e-01)	Acc@1  82.81 ( 84.67)	Acc@5 100.00 ( 99.31)
Epoch: [15][280/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.6880e-01 (4.4803e-01)	Acc@1  85.94 ( 84.72)	Acc@5 100.00 ( 99.32)
Epoch: [15][290/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.6554e-01 (4.4761e-01)	Acc@1  84.38 ( 84.76)	Acc@5 100.00 ( 99.32)
Epoch: [15][300/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.0419e-01 (4.4848e-01)	Acc@1  80.47 ( 84.72)	Acc@5  98.44 ( 99.31)
Epoch: [15][310/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.0090e-01 (4.4736e-01)	Acc@1  90.62 ( 84.77)	Acc@5  98.44 ( 99.31)
Epoch: [15][320/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.2160e-01 (4.4840e-01)	Acc@1  78.91 ( 84.73)	Acc@5  99.22 ( 99.30)
Epoch: [15][330/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.2253e-01 (4.4821e-01)	Acc@1  85.94 ( 84.73)	Acc@5  99.22 ( 99.29)
Epoch: [15][340/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.3432e-01 (4.4686e-01)	Acc@1  89.84 ( 84.76)	Acc@5  99.22 ( 99.29)
Epoch: [15][350/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.6766e-01 (4.4622e-01)	Acc@1  86.72 ( 84.75)	Acc@5 100.00 ( 99.30)
Epoch: [15][360/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.5160e-01 (4.4653e-01)	Acc@1  82.03 ( 84.76)	Acc@5  98.44 ( 99.29)
Epoch: [15][370/391]	Time  0.248 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.5109e-01 (4.4630e-01)	Acc@1  84.38 ( 84.74)	Acc@5  99.22 ( 99.29)
Epoch: [15][380/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.5179e-01 (4.4706e-01)	Acc@1  85.94 ( 84.69)	Acc@5  99.22 ( 99.29)
Epoch: [15][390/391]	Time  0.177 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.9164e-01 (4.4705e-01)	Acc@1  86.25 ( 84.71)	Acc@5  98.75 ( 99.30)
## e[15] optimizer.zero_grad (sum) time: 0.47345471382141113
## e[15]       loss.backward (sum) time: 10.842497825622559
## e[15]      optimizer.step (sum) time: 47.58781170845032
## epoch[15] training(only) time: 94.33955597877502
# Switched to evaluate mode...
Test: [  0/100]	Time  0.209 ( 0.209)	Loss 5.1953e-01 (5.1953e-01)	Acc@1  80.00 ( 80.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.077 ( 0.090)	Loss 5.2229e-01 (5.4472e-01)	Acc@1  82.00 ( 81.27)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 6.1586e-01 (5.4878e-01)	Acc@1  78.00 ( 82.00)	Acc@5  99.00 ( 99.24)
Test: [ 30/100]	Time  0.077 ( 0.082)	Loss 6.1672e-01 (5.6171e-01)	Acc@1  80.00 ( 81.90)	Acc@5  99.00 ( 99.10)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 6.8134e-01 (5.7081e-01)	Acc@1  82.00 ( 81.63)	Acc@5  99.00 ( 99.22)
Test: [ 50/100]	Time  0.079 ( 0.081)	Loss 4.9728e-01 (5.5954e-01)	Acc@1  85.00 ( 81.94)	Acc@5  98.00 ( 99.20)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 6.3366e-01 (5.6040e-01)	Acc@1  79.00 ( 81.84)	Acc@5  99.00 ( 99.23)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 5.2180e-01 (5.6086e-01)	Acc@1  83.00 ( 81.85)	Acc@5  99.00 ( 99.18)
Test: [ 80/100]	Time  0.079 ( 0.080)	Loss 4.3676e-01 (5.5852e-01)	Acc@1  85.00 ( 81.99)	Acc@5  99.00 ( 99.21)
Test: [ 90/100]	Time  0.077 ( 0.079)	Loss 5.3291e-01 (5.5829e-01)	Acc@1  83.00 ( 81.93)	Acc@5 100.00 ( 99.20)
 * Acc@1 81.970 Acc@5 99.170
### epoch[15] execution time: 102.3586061000824
EPOCH 16
# current learning rate: 0.1
# Switched to train mode...
Epoch: [16][  0/391]	Time  0.393 ( 0.393)	Data  0.141 ( 0.141)	Loss 4.4265e-01 (4.4265e-01)	Acc@1  81.25 ( 81.25)	Acc@5 100.00 (100.00)
Epoch: [16][ 10/391]	Time  0.246 ( 0.255)	Data  0.001 ( 0.014)	Loss 4.4396e-01 (4.3130e-01)	Acc@1  82.03 ( 84.52)	Acc@5 100.00 ( 99.64)
Epoch: [16][ 20/391]	Time  0.241 ( 0.248)	Data  0.001 ( 0.008)	Loss 3.7329e-01 (4.0767e-01)	Acc@1  85.16 ( 85.45)	Acc@5  99.22 ( 99.63)
Epoch: [16][ 30/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.006)	Loss 5.1255e-01 (4.0981e-01)	Acc@1  83.59 ( 85.46)	Acc@5  98.44 ( 99.55)
Epoch: [16][ 40/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.005)	Loss 4.1454e-01 (4.1386e-01)	Acc@1  86.72 ( 85.50)	Acc@5  99.22 ( 99.54)
Epoch: [16][ 50/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.004)	Loss 4.2119e-01 (4.1399e-01)	Acc@1  85.16 ( 85.37)	Acc@5  98.44 ( 99.56)
Epoch: [16][ 60/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.3126e-01 (4.1539e-01)	Acc@1  88.28 ( 85.35)	Acc@5 100.00 ( 99.49)
Epoch: [16][ 70/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.9529e-01 (4.1610e-01)	Acc@1  85.94 ( 85.44)	Acc@5 100.00 ( 99.50)
Epoch: [16][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 4.6925e-01 (4.1548e-01)	Acc@1  78.12 ( 85.40)	Acc@5 100.00 ( 99.48)
Epoch: [16][ 90/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 4.7894e-01 (4.1788e-01)	Acc@1  85.94 ( 85.38)	Acc@5  99.22 ( 99.49)
Epoch: [16][100/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 4.4057e-01 (4.2208e-01)	Acc@1  85.16 ( 85.30)	Acc@5  99.22 ( 99.47)
Epoch: [16][110/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.1697e-01 (4.1828e-01)	Acc@1  91.41 ( 85.46)	Acc@5  98.44 ( 99.47)
Epoch: [16][120/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.2078e-01 (4.1572e-01)	Acc@1  85.94 ( 85.58)	Acc@5 100.00 ( 99.47)
Epoch: [16][130/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.0439e-01 (4.1655e-01)	Acc@1  83.59 ( 85.45)	Acc@5  99.22 ( 99.46)
Epoch: [16][140/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4901e-01 (4.1587e-01)	Acc@1  91.41 ( 85.47)	Acc@5 100.00 ( 99.47)
Epoch: [16][150/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.8847e-01 (4.1653e-01)	Acc@1  87.50 ( 85.50)	Acc@5  99.22 ( 99.46)
Epoch: [16][160/391]	Time  0.246 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.1098e-01 (4.1426e-01)	Acc@1  89.06 ( 85.55)	Acc@5 100.00 ( 99.46)
Epoch: [16][170/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.9104e-01 (4.1324e-01)	Acc@1  84.38 ( 85.59)	Acc@5 100.00 ( 99.47)
Epoch: [16][180/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.8712e-01 (4.1396e-01)	Acc@1  83.59 ( 85.54)	Acc@5  99.22 ( 99.46)
Epoch: [16][190/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.8283e-01 (4.1604e-01)	Acc@1  89.84 ( 85.50)	Acc@5  99.22 ( 99.47)
Epoch: [16][200/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.6255e-01 (4.1967e-01)	Acc@1  80.47 ( 85.38)	Acc@5 100.00 ( 99.47)
Epoch: [16][210/391]	Time  0.245 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.3456e-01 (4.2074e-01)	Acc@1  86.72 ( 85.42)	Acc@5 100.00 ( 99.45)
Epoch: [16][220/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.8608e-01 (4.2079e-01)	Acc@1  82.81 ( 85.41)	Acc@5  99.22 ( 99.42)
Epoch: [16][230/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.5368e-01 (4.2117e-01)	Acc@1  85.94 ( 85.43)	Acc@5  99.22 ( 99.43)
Epoch: [16][240/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.7213e-01 (4.2062e-01)	Acc@1  86.72 ( 85.42)	Acc@5 100.00 ( 99.45)
Epoch: [16][250/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.6419e-01 (4.1933e-01)	Acc@1  85.94 ( 85.51)	Acc@5  99.22 ( 99.44)
Epoch: [16][260/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.9293e-01 (4.1809e-01)	Acc@1  88.28 ( 85.53)	Acc@5  99.22 ( 99.45)
Epoch: [16][270/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.2799e-01 (4.1742e-01)	Acc@1  86.72 ( 85.58)	Acc@5  99.22 ( 99.44)
Epoch: [16][280/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.8145e-01 (4.1972e-01)	Acc@1  82.03 ( 85.49)	Acc@5 100.00 ( 99.42)
Epoch: [16][290/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.8989e-01 (4.2066e-01)	Acc@1  80.47 ( 85.46)	Acc@5  99.22 ( 99.42)
Epoch: [16][300/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.2870e-01 (4.2112e-01)	Acc@1  85.16 ( 85.41)	Acc@5  99.22 ( 99.42)
Epoch: [16][310/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.0092e-01 (4.1976e-01)	Acc@1  89.06 ( 85.49)	Acc@5  99.22 ( 99.43)
Epoch: [16][320/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.2819e-01 (4.1995e-01)	Acc@1  81.25 ( 85.47)	Acc@5 100.00 ( 99.44)
Epoch: [16][330/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.6566e-01 (4.2216e-01)	Acc@1  80.47 ( 85.39)	Acc@5 100.00 ( 99.43)
Epoch: [16][340/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.4388e-01 (4.2309e-01)	Acc@1  81.25 ( 85.35)	Acc@5 100.00 ( 99.42)
Epoch: [16][350/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.9819e-01 (4.2275e-01)	Acc@1  84.38 ( 85.36)	Acc@5  98.44 ( 99.41)
Epoch: [16][360/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.001)	Loss 3.7750e-01 (4.2070e-01)	Acc@1  85.16 ( 85.42)	Acc@5 100.00 ( 99.42)
Epoch: [16][370/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.001)	Loss 3.0607e-01 (4.2139e-01)	Acc@1  87.50 ( 85.39)	Acc@5 100.00 ( 99.41)
Epoch: [16][380/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.001)	Loss 4.5438e-01 (4.2170e-01)	Acc@1  90.62 ( 85.40)	Acc@5  98.44 ( 99.41)
Epoch: [16][390/391]	Time  0.178 ( 0.241)	Data  0.001 ( 0.001)	Loss 5.6821e-01 (4.2171e-01)	Acc@1  77.50 ( 85.41)	Acc@5 100.00 ( 99.40)
## e[16] optimizer.zero_grad (sum) time: 0.478069543838501
## e[16]       loss.backward (sum) time: 10.800037860870361
## e[16]      optimizer.step (sum) time: 47.59565186500549
## epoch[16] training(only) time: 94.34988594055176
# Switched to evaluate mode...
Test: [  0/100]	Time  0.207 ( 0.207)	Loss 4.1053e-01 (4.1053e-01)	Acc@1  86.00 ( 86.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.077 ( 0.090)	Loss 4.0983e-01 (4.7655e-01)	Acc@1  85.00 ( 84.27)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 4.8131e-01 (4.9596e-01)	Acc@1  84.00 ( 83.71)	Acc@5 100.00 ( 99.38)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 4.6049e-01 (5.0949e-01)	Acc@1  79.00 ( 83.35)	Acc@5 100.00 ( 99.29)
Test: [ 40/100]	Time  0.077 ( 0.081)	Loss 5.1868e-01 (5.1391e-01)	Acc@1  84.00 ( 83.24)	Acc@5 100.00 ( 99.27)
Test: [ 50/100]	Time  0.077 ( 0.080)	Loss 5.4148e-01 (5.0694e-01)	Acc@1  85.00 ( 83.51)	Acc@5  97.00 ( 99.27)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 4.1847e-01 (5.0356e-01)	Acc@1  87.00 ( 83.48)	Acc@5 100.00 ( 99.36)
Test: [ 70/100]	Time  0.077 ( 0.080)	Loss 5.6079e-01 (4.9834e-01)	Acc@1  82.00 ( 83.49)	Acc@5  99.00 ( 99.38)
Test: [ 80/100]	Time  0.078 ( 0.079)	Loss 5.4432e-01 (4.9604e-01)	Acc@1  80.00 ( 83.49)	Acc@5  99.00 ( 99.38)
Test: [ 90/100]	Time  0.077 ( 0.079)	Loss 4.5137e-01 (4.9914e-01)	Acc@1  85.00 ( 83.25)	Acc@5 100.00 ( 99.36)
 * Acc@1 83.240 Acc@5 99.350
### epoch[16] execution time: 102.33827543258667
EPOCH 17
# current learning rate: 0.1
# Switched to train mode...
Epoch: [17][  0/391]	Time  0.390 ( 0.390)	Data  0.149 ( 0.149)	Loss 3.2717e-01 (3.2717e-01)	Acc@1  87.50 ( 87.50)	Acc@5 100.00 (100.00)
Epoch: [17][ 10/391]	Time  0.240 ( 0.255)	Data  0.001 ( 0.015)	Loss 3.6996e-01 (3.9129e-01)	Acc@1  89.06 ( 86.43)	Acc@5  99.22 ( 99.64)
Epoch: [17][ 20/391]	Time  0.240 ( 0.248)	Data  0.001 ( 0.008)	Loss 3.6446e-01 (3.9310e-01)	Acc@1  82.81 ( 86.01)	Acc@5 100.00 ( 99.55)
Epoch: [17][ 30/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.006)	Loss 4.2512e-01 (4.0086e-01)	Acc@1  84.38 ( 86.24)	Acc@5  97.66 ( 99.47)
Epoch: [17][ 40/391]	Time  0.239 ( 0.244)	Data  0.001 ( 0.005)	Loss 4.6560e-01 (4.0588e-01)	Acc@1  83.59 ( 86.05)	Acc@5 100.00 ( 99.52)
Epoch: [17][ 50/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 3.5348e-01 (4.0560e-01)	Acc@1  89.84 ( 86.17)	Acc@5  98.44 ( 99.49)
Epoch: [17][ 60/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.004)	Loss 3.4622e-01 (4.0777e-01)	Acc@1  88.28 ( 86.00)	Acc@5  98.44 ( 99.42)
Epoch: [17][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.7647e-01 (4.0810e-01)	Acc@1  88.28 ( 86.04)	Acc@5 100.00 ( 99.45)
Epoch: [17][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 4.0773e-01 (4.0795e-01)	Acc@1  85.94 ( 86.00)	Acc@5 100.00 ( 99.46)
Epoch: [17][ 90/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 4.2718e-01 (4.0599e-01)	Acc@1  83.59 ( 86.09)	Acc@5 100.00 ( 99.47)
Epoch: [17][100/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 3.7569e-01 (4.0237e-01)	Acc@1  88.28 ( 86.29)	Acc@5 100.00 ( 99.45)
Epoch: [17][110/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 3.8645e-01 (3.9737e-01)	Acc@1  86.72 ( 86.42)	Acc@5 100.00 ( 99.47)
Epoch: [17][120/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8301e-01 (3.9385e-01)	Acc@1  92.19 ( 86.54)	Acc@5  99.22 ( 99.46)
Epoch: [17][130/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.5555e-01 (3.9322e-01)	Acc@1  87.50 ( 86.51)	Acc@5 100.00 ( 99.48)
Epoch: [17][140/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.9354e-01 (3.9043e-01)	Acc@1  85.94 ( 86.61)	Acc@5 100.00 ( 99.50)
Epoch: [17][150/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8405e-01 (3.9089e-01)	Acc@1  89.84 ( 86.59)	Acc@5 100.00 ( 99.50)
Epoch: [17][160/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.6338e-01 (3.9059e-01)	Acc@1  86.72 ( 86.51)	Acc@5 100.00 ( 99.51)
Epoch: [17][170/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.2871e-01 (3.8940e-01)	Acc@1  89.06 ( 86.60)	Acc@5  99.22 ( 99.51)
Epoch: [17][180/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.9017e-01 (3.8808e-01)	Acc@1  85.94 ( 86.65)	Acc@5  99.22 ( 99.50)
Epoch: [17][190/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.2175e-01 (3.8907e-01)	Acc@1  87.50 ( 86.65)	Acc@5 100.00 ( 99.50)
Epoch: [17][200/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.7417e-01 (3.8834e-01)	Acc@1  89.84 ( 86.70)	Acc@5 100.00 ( 99.51)
Epoch: [17][210/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.7909e-01 (3.8928e-01)	Acc@1  86.72 ( 86.70)	Acc@5 100.00 ( 99.51)
Epoch: [17][220/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.5287e-01 (3.8864e-01)	Acc@1  86.72 ( 86.69)	Acc@5 100.00 ( 99.51)
Epoch: [17][230/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.2064e-01 (3.8971e-01)	Acc@1  88.28 ( 86.64)	Acc@5 100.00 ( 99.51)
Epoch: [17][240/391]	Time  0.246 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.6213e-01 (3.8992e-01)	Acc@1  87.50 ( 86.64)	Acc@5 100.00 ( 99.52)
Epoch: [17][250/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.1112e-01 (3.8963e-01)	Acc@1  85.16 ( 86.69)	Acc@5  99.22 ( 99.51)
Epoch: [17][260/391]	Time  0.247 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.9900e-01 (3.9145e-01)	Acc@1  84.38 ( 86.62)	Acc@5  99.22 ( 99.51)
Epoch: [17][270/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.7267e-01 (3.9066e-01)	Acc@1  81.25 ( 86.65)	Acc@5 100.00 ( 99.51)
Epoch: [17][280/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.9587e-01 (3.9107e-01)	Acc@1  86.72 ( 86.61)	Acc@5  99.22 ( 99.52)
Epoch: [17][290/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.3243e-01 (3.9232e-01)	Acc@1  87.50 ( 86.60)	Acc@5 100.00 ( 99.52)
Epoch: [17][300/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.9950e-01 (3.9238e-01)	Acc@1  82.03 ( 86.59)	Acc@5  99.22 ( 99.51)
Epoch: [17][310/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.8862e-01 (3.9161e-01)	Acc@1  83.59 ( 86.62)	Acc@5 100.00 ( 99.52)
Epoch: [17][320/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.7624e-01 (3.9117e-01)	Acc@1  84.38 ( 86.63)	Acc@5  99.22 ( 99.52)
Epoch: [17][330/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.2207e-01 (3.9121e-01)	Acc@1  87.50 ( 86.64)	Acc@5  96.88 ( 99.51)
Epoch: [17][340/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.3632e-01 (3.9145e-01)	Acc@1  84.38 ( 86.60)	Acc@5 100.00 ( 99.51)
Epoch: [17][350/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.1898e-01 (3.9129e-01)	Acc@1  83.59 ( 86.60)	Acc@5 100.00 ( 99.51)
Epoch: [17][360/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.5927e-01 (3.9083e-01)	Acc@1  90.62 ( 86.63)	Acc@5 100.00 ( 99.51)
Epoch: [17][370/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.1993e-01 (3.9015e-01)	Acc@1  85.94 ( 86.61)	Acc@5  99.22 ( 99.52)
Epoch: [17][380/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.6809e-01 (3.9151e-01)	Acc@1  83.59 ( 86.58)	Acc@5  99.22 ( 99.51)
Epoch: [17][390/391]	Time  0.177 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.8568e-01 (3.9103e-01)	Acc@1  81.25 ( 86.57)	Acc@5  98.75 ( 99.51)
## e[17] optimizer.zero_grad (sum) time: 0.47568416595458984
## e[17]       loss.backward (sum) time: 10.810762405395508
## e[17]      optimizer.step (sum) time: 47.600093603134155
## epoch[17] training(only) time: 94.32463431358337
# Switched to evaluate mode...
Test: [  0/100]	Time  0.210 ( 0.210)	Loss 5.6517e-01 (5.6517e-01)	Acc@1  81.00 ( 81.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.077 ( 0.090)	Loss 5.1156e-01 (4.9647e-01)	Acc@1  84.00 ( 83.82)	Acc@5 100.00 ( 99.45)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 5.0327e-01 (4.8900e-01)	Acc@1  81.00 ( 84.14)	Acc@5 100.00 ( 99.29)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 4.5249e-01 (4.9597e-01)	Acc@1  86.00 ( 84.06)	Acc@5 100.00 ( 99.13)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 5.6832e-01 (5.0072e-01)	Acc@1  79.00 ( 83.76)	Acc@5 100.00 ( 99.07)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 3.2611e-01 (4.8880e-01)	Acc@1  90.00 ( 84.16)	Acc@5  99.00 ( 99.08)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 4.8943e-01 (4.8855e-01)	Acc@1  83.00 ( 84.16)	Acc@5 100.00 ( 99.15)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 4.5583e-01 (4.9166e-01)	Acc@1  81.00 ( 84.01)	Acc@5 100.00 ( 99.25)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 4.2132e-01 (4.9097e-01)	Acc@1  87.00 ( 84.04)	Acc@5  98.00 ( 99.26)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 3.4873e-01 (4.9347e-01)	Acc@1  86.00 ( 83.91)	Acc@5 100.00 ( 99.22)
 * Acc@1 83.810 Acc@5 99.220
### epoch[17] execution time: 102.34285688400269
EPOCH 18
# current learning rate: 0.1
# Switched to train mode...
Epoch: [18][  0/391]	Time  0.387 ( 0.387)	Data  0.146 ( 0.146)	Loss 3.7501e-01 (3.7501e-01)	Acc@1  88.28 ( 88.28)	Acc@5  99.22 ( 99.22)
Epoch: [18][ 10/391]	Time  0.240 ( 0.254)	Data  0.001 ( 0.014)	Loss 5.1838e-01 (4.1033e-01)	Acc@1  81.25 ( 85.51)	Acc@5  99.22 ( 99.29)
Epoch: [18][ 20/391]	Time  0.240 ( 0.248)	Data  0.001 ( 0.008)	Loss 3.6559e-01 (4.0012e-01)	Acc@1  90.62 ( 86.12)	Acc@5 100.00 ( 99.40)
Epoch: [18][ 30/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.006)	Loss 3.7472e-01 (4.0115e-01)	Acc@1  89.84 ( 86.39)	Acc@5 100.00 ( 99.37)
Epoch: [18][ 40/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.005)	Loss 3.4203e-01 (3.8242e-01)	Acc@1  89.06 ( 86.97)	Acc@5  98.44 ( 99.37)
Epoch: [18][ 50/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.004)	Loss 4.7826e-01 (3.7475e-01)	Acc@1  83.59 ( 87.24)	Acc@5  99.22 ( 99.46)
Epoch: [18][ 60/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.004)	Loss 3.9687e-01 (3.6976e-01)	Acc@1  87.50 ( 87.54)	Acc@5 100.00 ( 99.49)
Epoch: [18][ 70/391]	Time  0.239 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.5743e-01 (3.6990e-01)	Acc@1  85.94 ( 87.42)	Acc@5 100.00 ( 99.53)
Epoch: [18][ 80/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 4.0122e-01 (3.6766e-01)	Acc@1  85.94 ( 87.46)	Acc@5 100.00 ( 99.56)
Epoch: [18][ 90/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 2.9861e-01 (3.6473e-01)	Acc@1  89.06 ( 87.57)	Acc@5  99.22 ( 99.57)
Epoch: [18][100/391]	Time  0.248 ( 0.242)	Data  0.001 ( 0.003)	Loss 3.9208e-01 (3.6422e-01)	Acc@1  89.84 ( 87.60)	Acc@5  99.22 ( 99.55)
Epoch: [18][110/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.6631e-01 (3.6868e-01)	Acc@1  84.38 ( 87.49)	Acc@5  98.44 ( 99.52)
Epoch: [18][120/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.1056e-01 (3.6795e-01)	Acc@1  87.50 ( 87.55)	Acc@5  99.22 ( 99.53)
Epoch: [18][130/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8624e-01 (3.6842e-01)	Acc@1  87.50 ( 87.49)	Acc@5 100.00 ( 99.53)
Epoch: [18][140/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.0078e-01 (3.6637e-01)	Acc@1  85.94 ( 87.57)	Acc@5 100.00 ( 99.55)
Epoch: [18][150/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.5466e-01 (3.6526e-01)	Acc@1  89.84 ( 87.57)	Acc@5 100.00 ( 99.56)
Epoch: [18][160/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.7990e-01 (3.6624e-01)	Acc@1  85.94 ( 87.56)	Acc@5 100.00 ( 99.55)
Epoch: [18][170/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.5427e-01 (3.6639e-01)	Acc@1  85.94 ( 87.49)	Acc@5  99.22 ( 99.54)
Epoch: [18][180/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.1166e-01 (3.6763e-01)	Acc@1  88.28 ( 87.40)	Acc@5 100.00 ( 99.53)
Epoch: [18][190/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.1136e-01 (3.6754e-01)	Acc@1  85.94 ( 87.39)	Acc@5 100.00 ( 99.53)
Epoch: [18][200/391]	Time  0.244 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.7887e-01 (3.6770e-01)	Acc@1  85.16 ( 87.33)	Acc@5 100.00 ( 99.52)
Epoch: [18][210/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.3148e-01 (3.6892e-01)	Acc@1  77.34 ( 87.24)	Acc@5 100.00 ( 99.54)
Epoch: [18][220/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.6601e-01 (3.7126e-01)	Acc@1  85.16 ( 87.17)	Acc@5 100.00 ( 99.54)
Epoch: [18][230/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.7346e-01 (3.7118e-01)	Acc@1  87.50 ( 87.21)	Acc@5  99.22 ( 99.54)
Epoch: [18][240/391]	Time  0.251 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.1993e-01 (3.7211e-01)	Acc@1  89.06 ( 87.21)	Acc@5  99.22 ( 99.54)
Epoch: [18][250/391]	Time  0.244 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.0465e-01 (3.7094e-01)	Acc@1  89.84 ( 87.26)	Acc@5  99.22 ( 99.54)
Epoch: [18][260/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.8717e-01 (3.7064e-01)	Acc@1  83.59 ( 87.29)	Acc@5  99.22 ( 99.53)
Epoch: [18][270/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.5451e-01 (3.6984e-01)	Acc@1  87.50 ( 87.32)	Acc@5 100.00 ( 99.54)
Epoch: [18][280/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.8535e-01 (3.6927e-01)	Acc@1  88.28 ( 87.32)	Acc@5 100.00 ( 99.54)
Epoch: [18][290/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.8974e-01 (3.6822e-01)	Acc@1  86.72 ( 87.35)	Acc@5  99.22 ( 99.54)
Epoch: [18][300/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.6162e-01 (3.6826e-01)	Acc@1  93.75 ( 87.34)	Acc@5 100.00 ( 99.54)
Epoch: [18][310/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.1096e-01 (3.6906e-01)	Acc@1  86.72 ( 87.33)	Acc@5  99.22 ( 99.54)
Epoch: [18][320/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.7994e-01 (3.7025e-01)	Acc@1  82.03 ( 87.30)	Acc@5  98.44 ( 99.53)
Epoch: [18][330/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.8776e-01 (3.7083e-01)	Acc@1  85.16 ( 87.29)	Acc@5 100.00 ( 99.54)
Epoch: [18][340/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.0704e-01 (3.7108e-01)	Acc@1  85.94 ( 87.27)	Acc@5 100.00 ( 99.54)
Epoch: [18][350/391]	Time  0.243 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.2857e-01 (3.7097e-01)	Acc@1  88.28 ( 87.26)	Acc@5 100.00 ( 99.55)
Epoch: [18][360/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.2226e-01 (3.7150e-01)	Acc@1  87.50 ( 87.25)	Acc@5  98.44 ( 99.55)
Epoch: [18][370/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.5856e-01 (3.7048e-01)	Acc@1  85.94 ( 87.27)	Acc@5  99.22 ( 99.55)
Epoch: [18][380/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.8206e-01 (3.7124e-01)	Acc@1  83.59 ( 87.24)	Acc@5  99.22 ( 99.55)
Epoch: [18][390/391]	Time  0.176 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.0002e-01 (3.7167e-01)	Acc@1  85.00 ( 87.23)	Acc@5  98.75 ( 99.55)
## e[18] optimizer.zero_grad (sum) time: 0.4753296375274658
## e[18]       loss.backward (sum) time: 10.795364618301392
## e[18]      optimizer.step (sum) time: 47.59719514846802
## epoch[18] training(only) time: 94.33220291137695
# Switched to evaluate mode...
Test: [  0/100]	Time  0.210 ( 0.210)	Loss 4.4325e-01 (4.4325e-01)	Acc@1  84.00 ( 84.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.078 ( 0.089)	Loss 4.3590e-01 (4.8705e-01)	Acc@1  85.00 ( 82.55)	Acc@5 100.00 ( 99.45)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 5.5726e-01 (5.1185e-01)	Acc@1  82.00 ( 82.33)	Acc@5  99.00 ( 99.05)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 4.0824e-01 (5.2160e-01)	Acc@1  84.00 ( 82.81)	Acc@5 100.00 ( 99.06)
Test: [ 40/100]	Time  0.077 ( 0.081)	Loss 5.6157e-01 (5.1636e-01)	Acc@1  82.00 ( 82.95)	Acc@5 100.00 ( 99.17)
Test: [ 50/100]	Time  0.079 ( 0.080)	Loss 4.9828e-01 (5.0494e-01)	Acc@1  82.00 ( 83.39)	Acc@5  98.00 ( 99.16)
Test: [ 60/100]	Time  0.077 ( 0.080)	Loss 5.3148e-01 (5.0717e-01)	Acc@1  85.00 ( 83.13)	Acc@5 100.00 ( 99.26)
Test: [ 70/100]	Time  0.077 ( 0.080)	Loss 4.6374e-01 (5.0615e-01)	Acc@1  84.00 ( 83.25)	Acc@5 100.00 ( 99.31)
Test: [ 80/100]	Time  0.079 ( 0.079)	Loss 5.3210e-01 (5.0146e-01)	Acc@1  80.00 ( 83.42)	Acc@5  99.00 ( 99.33)
Test: [ 90/100]	Time  0.079 ( 0.079)	Loss 3.9790e-01 (5.0024e-01)	Acc@1  84.00 ( 83.43)	Acc@5 100.00 ( 99.32)
 * Acc@1 83.510 Acc@5 99.350
### epoch[18] execution time: 102.33379030227661
EPOCH 19
# current learning rate: 0.1
# Switched to train mode...
Epoch: [19][  0/391]	Time  0.392 ( 0.392)	Data  0.144 ( 0.144)	Loss 3.7536e-01 (3.7536e-01)	Acc@1  87.50 ( 87.50)	Acc@5 100.00 (100.00)
Epoch: [19][ 10/391]	Time  0.240 ( 0.254)	Data  0.001 ( 0.014)	Loss 4.6726e-01 (4.1809e-01)	Acc@1  81.25 ( 86.65)	Acc@5 100.00 ( 99.15)
Epoch: [19][ 20/391]	Time  0.239 ( 0.248)	Data  0.001 ( 0.008)	Loss 3.4749e-01 (3.8601e-01)	Acc@1  86.72 ( 87.31)	Acc@5 100.00 ( 99.44)
Epoch: [19][ 30/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.006)	Loss 1.9550e-01 (3.5018e-01)	Acc@1  92.97 ( 88.33)	Acc@5 100.00 ( 99.50)
Epoch: [19][ 40/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.005)	Loss 2.8258e-01 (3.4562e-01)	Acc@1  89.84 ( 88.45)	Acc@5 100.00 ( 99.49)
Epoch: [19][ 50/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.004)	Loss 3.5106e-01 (3.4754e-01)	Acc@1  87.50 ( 88.27)	Acc@5  97.66 ( 99.42)
Epoch: [19][ 60/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.7949e-01 (3.4847e-01)	Acc@1  86.72 ( 88.31)	Acc@5  99.22 ( 99.46)
Epoch: [19][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.5576e-01 (3.4549e-01)	Acc@1  89.84 ( 88.36)	Acc@5 100.00 ( 99.50)
Epoch: [19][ 80/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.003)	Loss 3.0118e-01 (3.4803e-01)	Acc@1  89.84 ( 88.28)	Acc@5  99.22 ( 99.46)
Epoch: [19][ 90/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 3.1762e-01 (3.4652e-01)	Acc@1  89.84 ( 88.33)	Acc@5  99.22 ( 99.47)
Epoch: [19][100/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 4.2344e-01 (3.4842e-01)	Acc@1  85.94 ( 88.29)	Acc@5  99.22 ( 99.47)
Epoch: [19][110/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.6724e-01 (3.4579e-01)	Acc@1  88.28 ( 88.35)	Acc@5  98.44 ( 99.48)
Epoch: [19][120/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.2268e-01 (3.4706e-01)	Acc@1  87.50 ( 88.25)	Acc@5 100.00 ( 99.50)
Epoch: [19][130/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.7210e-01 (3.4703e-01)	Acc@1  91.41 ( 88.23)	Acc@5 100.00 ( 99.50)
Epoch: [19][140/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6047e-01 (3.4844e-01)	Acc@1  90.62 ( 88.19)	Acc@5 100.00 ( 99.48)
Epoch: [19][150/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.6666e-01 (3.5085e-01)	Acc@1  85.94 ( 88.12)	Acc@5 100.00 ( 99.49)
Epoch: [19][160/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.9674e-01 (3.4914e-01)	Acc@1  86.72 ( 88.14)	Acc@5 100.00 ( 99.51)
Epoch: [19][170/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.6543e-01 (3.4958e-01)	Acc@1  84.38 ( 88.11)	Acc@5  98.44 ( 99.50)
Epoch: [19][180/391]	Time  0.249 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.4291e-01 (3.4978e-01)	Acc@1  85.94 ( 88.10)	Acc@5 100.00 ( 99.51)
Epoch: [19][190/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.9394e-01 (3.4842e-01)	Acc@1  86.72 ( 88.09)	Acc@5  99.22 ( 99.50)
Epoch: [19][200/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.5715e-01 (3.5255e-01)	Acc@1  80.47 ( 87.94)	Acc@5  99.22 ( 99.49)
Epoch: [19][210/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.0602e-01 (3.5389e-01)	Acc@1  88.28 ( 87.89)	Acc@5  99.22 ( 99.49)
Epoch: [19][220/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.5783e-01 (3.5388e-01)	Acc@1  91.41 ( 87.88)	Acc@5  99.22 ( 99.49)
Epoch: [19][230/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.1926e-01 (3.5436e-01)	Acc@1  90.62 ( 87.91)	Acc@5 100.00 ( 99.49)
Epoch: [19][240/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.0838e-01 (3.5244e-01)	Acc@1  92.19 ( 88.00)	Acc@5  99.22 ( 99.50)
Epoch: [19][250/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.8216e-01 (3.5278e-01)	Acc@1  89.06 ( 87.99)	Acc@5 100.00 ( 99.51)
Epoch: [19][260/391]	Time  0.247 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.5359e-01 (3.5385e-01)	Acc@1  82.81 ( 87.92)	Acc@5 100.00 ( 99.51)
Epoch: [19][270/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.5837e-01 (3.5424e-01)	Acc@1  85.16 ( 87.94)	Acc@5 100.00 ( 99.52)
Epoch: [19][280/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.4665e-01 (3.5469e-01)	Acc@1  92.19 ( 87.95)	Acc@5 100.00 ( 99.53)
Epoch: [19][290/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.7199e-01 (3.5612e-01)	Acc@1  82.81 ( 87.90)	Acc@5 100.00 ( 99.54)
Epoch: [19][300/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.4325e-01 (3.5565e-01)	Acc@1  85.16 ( 87.90)	Acc@5 100.00 ( 99.54)
Epoch: [19][310/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.1618e-01 (3.5571e-01)	Acc@1  89.84 ( 87.89)	Acc@5  99.22 ( 99.53)
Epoch: [19][320/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.0107e-01 (3.5595e-01)	Acc@1  87.50 ( 87.87)	Acc@5 100.00 ( 99.53)
Epoch: [19][330/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.5038e-01 (3.5514e-01)	Acc@1  86.72 ( 87.89)	Acc@5 100.00 ( 99.54)
Epoch: [19][340/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.4006e-01 (3.5748e-01)	Acc@1  85.16 ( 87.79)	Acc@5 100.00 ( 99.54)
Epoch: [19][350/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.8905e-01 (3.5782e-01)	Acc@1  91.41 ( 87.79)	Acc@5 100.00 ( 99.55)
Epoch: [19][360/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.8203e-01 (3.5843e-01)	Acc@1  86.72 ( 87.75)	Acc@5  99.22 ( 99.55)
Epoch: [19][370/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.2647e-01 (3.5982e-01)	Acc@1  91.41 ( 87.71)	Acc@5 100.00 ( 99.55)
Epoch: [19][380/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.4496e-01 (3.5950e-01)	Acc@1  88.28 ( 87.74)	Acc@5  99.22 ( 99.54)
Epoch: [19][390/391]	Time  0.176 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.0755e-01 (3.5974e-01)	Acc@1  87.50 ( 87.73)	Acc@5 100.00 ( 99.55)
## e[19] optimizer.zero_grad (sum) time: 0.4772179126739502
## e[19]       loss.backward (sum) time: 10.763328790664673
## e[19]      optimizer.step (sum) time: 47.604889154434204
## epoch[19] training(only) time: 94.29833889007568
# Switched to evaluate mode...
Test: [  0/100]	Time  0.207 ( 0.207)	Loss 6.3538e-01 (6.3538e-01)	Acc@1  81.00 ( 81.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 4.8798e-01 (5.0136e-01)	Acc@1  82.00 ( 83.55)	Acc@5 100.00 ( 99.27)
Test: [ 20/100]	Time  0.079 ( 0.084)	Loss 6.0949e-01 (4.9091e-01)	Acc@1  78.00 ( 83.71)	Acc@5  99.00 ( 99.19)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 3.9624e-01 (4.7520e-01)	Acc@1  85.00 ( 84.06)	Acc@5 100.00 ( 99.32)
Test: [ 40/100]	Time  0.079 ( 0.081)	Loss 4.7013e-01 (4.7467e-01)	Acc@1  87.00 ( 83.88)	Acc@5  98.00 ( 99.34)
Test: [ 50/100]	Time  0.077 ( 0.080)	Loss 3.0327e-01 (4.6523e-01)	Acc@1  94.00 ( 84.33)	Acc@5  99.00 ( 99.33)
Test: [ 60/100]	Time  0.079 ( 0.080)	Loss 4.3795e-01 (4.6445e-01)	Acc@1  88.00 ( 84.41)	Acc@5 100.00 ( 99.38)
Test: [ 70/100]	Time  0.077 ( 0.080)	Loss 4.5502e-01 (4.6455e-01)	Acc@1  82.00 ( 84.37)	Acc@5 100.00 ( 99.38)
Test: [ 80/100]	Time  0.078 ( 0.079)	Loss 3.1110e-01 (4.6172e-01)	Acc@1  90.00 ( 84.40)	Acc@5 100.00 ( 99.44)
Test: [ 90/100]	Time  0.077 ( 0.079)	Loss 4.2642e-01 (4.6425e-01)	Acc@1  85.00 ( 84.31)	Acc@5 100.00 ( 99.44)
 * Acc@1 84.280 Acc@5 99.470
### epoch[19] execution time: 102.29949808120728
EPOCH 20
# current learning rate: 0.1
# Switched to train mode...
Epoch: [20][  0/391]	Time  0.397 ( 0.397)	Data  0.141 ( 0.141)	Loss 2.6839e-01 (2.6839e-01)	Acc@1  89.84 ( 89.84)	Acc@5 100.00 (100.00)
Epoch: [20][ 10/391]	Time  0.240 ( 0.255)	Data  0.001 ( 0.014)	Loss 3.2670e-01 (3.1173e-01)	Acc@1  89.84 ( 88.64)	Acc@5  99.22 ( 99.64)
Epoch: [20][ 20/391]	Time  0.241 ( 0.248)	Data  0.001 ( 0.008)	Loss 3.8280e-01 (3.2036e-01)	Acc@1  85.16 ( 88.50)	Acc@5 100.00 ( 99.67)
Epoch: [20][ 30/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.006)	Loss 2.2495e-01 (3.2500e-01)	Acc@1  91.41 ( 88.23)	Acc@5 100.00 ( 99.62)
Epoch: [20][ 40/391]	Time  0.239 ( 0.244)	Data  0.001 ( 0.005)	Loss 3.1339e-01 (3.2848e-01)	Acc@1  89.06 ( 88.34)	Acc@5  99.22 ( 99.64)
Epoch: [20][ 50/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 3.3277e-01 (3.3931e-01)	Acc@1  90.62 ( 88.17)	Acc@5  99.22 ( 99.62)
Epoch: [20][ 60/391]	Time  0.239 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.4156e-01 (3.3925e-01)	Acc@1  91.41 ( 88.19)	Acc@5 100.00 ( 99.62)
Epoch: [20][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.3135e-01 (3.4155e-01)	Acc@1  91.41 ( 88.09)	Acc@5 100.00 ( 99.57)
Epoch: [20][ 80/391]	Time  0.239 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.4628e-01 (3.3774e-01)	Acc@1  91.41 ( 88.25)	Acc@5  99.22 ( 99.59)
Epoch: [20][ 90/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 3.5460e-01 (3.3940e-01)	Acc@1  88.28 ( 88.16)	Acc@5  99.22 ( 99.60)
Epoch: [20][100/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 3.8690e-01 (3.3761e-01)	Acc@1  82.81 ( 88.17)	Acc@5 100.00 ( 99.60)
Epoch: [20][110/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.1494e-01 (3.3498e-01)	Acc@1  86.72 ( 88.30)	Acc@5 100.00 ( 99.61)
Epoch: [20][120/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.0027e-01 (3.3308e-01)	Acc@1  89.06 ( 88.29)	Acc@5  99.22 ( 99.61)
Epoch: [20][130/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.1101e-01 (3.3243e-01)	Acc@1  85.16 ( 88.34)	Acc@5 100.00 ( 99.62)
Epoch: [20][140/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8297e-01 (3.3023e-01)	Acc@1  89.84 ( 88.43)	Acc@5 100.00 ( 99.63)
Epoch: [20][150/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.3826e-01 (3.3033e-01)	Acc@1  84.38 ( 88.46)	Acc@5  99.22 ( 99.62)
Epoch: [20][160/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0428e-01 (3.2738e-01)	Acc@1  92.19 ( 88.62)	Acc@5  99.22 ( 99.62)
Epoch: [20][170/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.5820e-01 (3.2728e-01)	Acc@1  92.19 ( 88.63)	Acc@5 100.00 ( 99.63)
Epoch: [20][180/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.8593e-01 (3.2891e-01)	Acc@1  87.50 ( 88.56)	Acc@5 100.00 ( 99.62)
Epoch: [20][190/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.7705e-01 (3.2866e-01)	Acc@1  87.50 ( 88.61)	Acc@5 100.00 ( 99.62)
Epoch: [20][200/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.1602e-01 (3.2661e-01)	Acc@1  93.75 ( 88.66)	Acc@5 100.00 ( 99.63)
Epoch: [20][210/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.4322e-01 (3.2564e-01)	Acc@1  90.62 ( 88.68)	Acc@5  99.22 ( 99.64)
Epoch: [20][220/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.4790e-01 (3.2655e-01)	Acc@1  88.28 ( 88.66)	Acc@5  99.22 ( 99.64)
Epoch: [20][230/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.1033e-01 (3.2596e-01)	Acc@1  89.06 ( 88.66)	Acc@5 100.00 ( 99.64)
Epoch: [20][240/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.9785e-01 (3.2631e-01)	Acc@1  91.41 ( 88.67)	Acc@5 100.00 ( 99.62)
Epoch: [20][250/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.7987e-01 (3.2807e-01)	Acc@1  85.16 ( 88.63)	Acc@5 100.00 ( 99.63)
Epoch: [20][260/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.0379e-01 (3.2898e-01)	Acc@1  90.62 ( 88.61)	Acc@5  99.22 ( 99.61)
Epoch: [20][270/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.8605e-01 (3.3026e-01)	Acc@1  84.38 ( 88.53)	Acc@5 100.00 ( 99.61)
Epoch: [20][280/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.2307e-01 (3.3054e-01)	Acc@1  86.72 ( 88.49)	Acc@5 100.00 ( 99.61)
Epoch: [20][290/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.5178e-01 (3.3147e-01)	Acc@1  90.62 ( 88.43)	Acc@5 100.00 ( 99.61)
Epoch: [20][300/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.0804e-01 (3.3171e-01)	Acc@1  85.94 ( 88.43)	Acc@5 100.00 ( 99.62)
Epoch: [20][310/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.5714e-01 (3.3122e-01)	Acc@1  85.94 ( 88.45)	Acc@5  99.22 ( 99.62)
Epoch: [20][320/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.0532e-01 (3.3195e-01)	Acc@1  88.28 ( 88.45)	Acc@5 100.00 ( 99.61)
Epoch: [20][330/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.001)	Loss 3.3611e-01 (3.3194e-01)	Acc@1  90.62 ( 88.46)	Acc@5  99.22 ( 99.61)
Epoch: [20][340/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.001)	Loss 3.7086e-01 (3.3373e-01)	Acc@1  85.94 ( 88.40)	Acc@5  98.44 ( 99.59)
Epoch: [20][350/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.001)	Loss 3.3532e-01 (3.3488e-01)	Acc@1  88.28 ( 88.38)	Acc@5 100.00 ( 99.59)
Epoch: [20][360/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.001)	Loss 3.4926e-01 (3.3545e-01)	Acc@1  88.28 ( 88.39)	Acc@5 100.00 ( 99.60)
Epoch: [20][370/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.001)	Loss 2.3468e-01 (3.3518e-01)	Acc@1  92.97 ( 88.42)	Acc@5 100.00 ( 99.60)
Epoch: [20][380/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.001)	Loss 4.0084e-01 (3.3549e-01)	Acc@1  85.16 ( 88.41)	Acc@5 100.00 ( 99.60)
Epoch: [20][390/391]	Time  0.175 ( 0.241)	Data  0.001 ( 0.001)	Loss 4.5257e-01 (3.3566e-01)	Acc@1  85.00 ( 88.40)	Acc@5  98.75 ( 99.60)
## e[20] optimizer.zero_grad (sum) time: 0.4773750305175781
## e[20]       loss.backward (sum) time: 10.795548677444458
## e[20]      optimizer.step (sum) time: 47.572166204452515
## epoch[20] training(only) time: 94.26559972763062
# Switched to evaluate mode...
Test: [  0/100]	Time  0.206 ( 0.206)	Loss 5.3060e-01 (5.3060e-01)	Acc@1  84.00 ( 84.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.077 ( 0.089)	Loss 4.1394e-01 (4.6884e-01)	Acc@1  85.00 ( 85.00)	Acc@5 100.00 ( 99.55)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 4.8423e-01 (4.6023e-01)	Acc@1  85.00 ( 85.67)	Acc@5 100.00 ( 99.33)
Test: [ 30/100]	Time  0.077 ( 0.082)	Loss 4.8508e-01 (4.7319e-01)	Acc@1  84.00 ( 85.32)	Acc@5 100.00 ( 99.42)
Test: [ 40/100]	Time  0.077 ( 0.081)	Loss 4.0330e-01 (4.6585e-01)	Acc@1  86.00 ( 85.02)	Acc@5  99.00 ( 99.49)
Test: [ 50/100]	Time  0.078 ( 0.080)	Loss 3.6038e-01 (4.5743e-01)	Acc@1  88.00 ( 85.22)	Acc@5 100.00 ( 99.51)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 3.6664e-01 (4.4994e-01)	Acc@1  89.00 ( 85.26)	Acc@5 100.00 ( 99.56)
Test: [ 70/100]	Time  0.079 ( 0.080)	Loss 5.4302e-01 (4.4149e-01)	Acc@1  82.00 ( 85.49)	Acc@5 100.00 ( 99.59)
Test: [ 80/100]	Time  0.078 ( 0.079)	Loss 2.9358e-01 (4.4136e-01)	Acc@1  92.00 ( 85.57)	Acc@5 100.00 ( 99.62)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 4.5533e-01 (4.4291e-01)	Acc@1  83.00 ( 85.57)	Acc@5 100.00 ( 99.59)
 * Acc@1 85.520 Acc@5 99.590
### epoch[20] execution time: 102.27034163475037
EPOCH 21
# current learning rate: 0.1
# Switched to train mode...
Epoch: [21][  0/391]	Time  0.397 ( 0.397)	Data  0.156 ( 0.156)	Loss 2.4969e-01 (2.4969e-01)	Acc@1  92.97 ( 92.97)	Acc@5 100.00 (100.00)
Epoch: [21][ 10/391]	Time  0.240 ( 0.255)	Data  0.001 ( 0.015)	Loss 3.2100e-01 (2.7914e-01)	Acc@1  86.72 ( 90.06)	Acc@5 100.00 ( 99.64)
Epoch: [21][ 20/391]	Time  0.244 ( 0.248)	Data  0.001 ( 0.009)	Loss 3.4747e-01 (2.7744e-01)	Acc@1  87.50 ( 90.59)	Acc@5 100.00 ( 99.70)
Epoch: [21][ 30/391]	Time  0.240 ( 0.246)	Data  0.001 ( 0.006)	Loss 2.6172e-01 (2.8949e-01)	Acc@1  91.41 ( 90.10)	Acc@5  99.22 ( 99.65)
Epoch: [21][ 40/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.005)	Loss 2.1883e-01 (2.8393e-01)	Acc@1  93.75 ( 90.26)	Acc@5 100.00 ( 99.71)
Epoch: [21][ 50/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 2.1636e-01 (2.8608e-01)	Acc@1  91.41 ( 90.00)	Acc@5 100.00 ( 99.75)
Epoch: [21][ 60/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.004)	Loss 2.9975e-01 (2.8810e-01)	Acc@1  91.41 ( 89.87)	Acc@5 100.00 ( 99.80)
Epoch: [21][ 70/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.8584e-01 (2.9649e-01)	Acc@1  86.72 ( 89.73)	Acc@5 100.00 ( 99.78)
Epoch: [21][ 80/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 3.8321e-01 (2.9574e-01)	Acc@1  86.72 ( 89.64)	Acc@5 100.00 ( 99.80)
Epoch: [21][ 90/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 3.6043e-01 (2.9916e-01)	Acc@1  88.28 ( 89.55)	Acc@5 100.00 ( 99.81)
Epoch: [21][100/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.003)	Loss 3.7257e-01 (3.0219e-01)	Acc@1  89.06 ( 89.48)	Acc@5 100.00 ( 99.79)
Epoch: [21][110/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.003)	Loss 3.8959e-01 (3.0371e-01)	Acc@1  87.50 ( 89.39)	Acc@5  99.22 ( 99.78)
Epoch: [21][120/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.1657e-01 (3.0560e-01)	Acc@1  86.72 ( 89.31)	Acc@5 100.00 ( 99.78)
Epoch: [21][130/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8004e-01 (3.0582e-01)	Acc@1  89.84 ( 89.35)	Acc@5  99.22 ( 99.76)
Epoch: [21][140/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6177e-01 (3.0860e-01)	Acc@1  92.19 ( 89.30)	Acc@5 100.00 ( 99.73)
Epoch: [21][150/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.1554e-01 (3.0924e-01)	Acc@1  89.06 ( 89.28)	Acc@5  99.22 ( 99.73)
Epoch: [21][160/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0220e-01 (3.0877e-01)	Acc@1  92.19 ( 89.26)	Acc@5 100.00 ( 99.73)
Epoch: [21][170/391]	Time  0.246 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.6639e-01 (3.1125e-01)	Acc@1  92.19 ( 89.23)	Acc@5 100.00 ( 99.73)
Epoch: [21][180/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.2428e-01 (3.1097e-01)	Acc@1  90.62 ( 89.27)	Acc@5  99.22 ( 99.72)
Epoch: [21][190/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.4381e-01 (3.1218e-01)	Acc@1  92.97 ( 89.23)	Acc@5 100.00 ( 99.71)
Epoch: [21][200/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.9594e-01 (3.1390e-01)	Acc@1  86.72 ( 89.19)	Acc@5 100.00 ( 99.70)
Epoch: [21][210/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.0797e-01 (3.1528e-01)	Acc@1  81.25 ( 89.16)	Acc@5 100.00 ( 99.70)
Epoch: [21][220/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.5679e-01 (3.1630e-01)	Acc@1  89.84 ( 89.12)	Acc@5 100.00 ( 99.70)
Epoch: [21][230/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.2868e-01 (3.1557e-01)	Acc@1  91.41 ( 89.18)	Acc@5  99.22 ( 99.70)
Epoch: [21][240/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.5921e-01 (3.1742e-01)	Acc@1  84.38 ( 89.12)	Acc@5 100.00 ( 99.70)
Epoch: [21][250/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.0102e-01 (3.1769e-01)	Acc@1  88.28 ( 89.13)	Acc@5  98.44 ( 99.70)
Epoch: [21][260/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.3980e-01 (3.1749e-01)	Acc@1  92.97 ( 89.13)	Acc@5 100.00 ( 99.70)
Epoch: [21][270/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.4917e-01 (3.1716e-01)	Acc@1  91.41 ( 89.15)	Acc@5 100.00 ( 99.70)
Epoch: [21][280/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.2642e-01 (3.1690e-01)	Acc@1  85.94 ( 89.15)	Acc@5 100.00 ( 99.69)
Epoch: [21][290/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.0697e-01 (3.1721e-01)	Acc@1  89.06 ( 89.12)	Acc@5 100.00 ( 99.70)
Epoch: [21][300/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.2694e-01 (3.1643e-01)	Acc@1  87.50 ( 89.14)	Acc@5  99.22 ( 99.70)
Epoch: [21][310/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.1288e-01 (3.1711e-01)	Acc@1  87.50 ( 89.11)	Acc@5 100.00 ( 99.70)
Epoch: [21][320/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.7201e-01 (3.1831e-01)	Acc@1  85.16 ( 89.06)	Acc@5 100.00 ( 99.70)
Epoch: [21][330/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.4191e-01 (3.1900e-01)	Acc@1  90.62 ( 89.06)	Acc@5  99.22 ( 99.69)
Epoch: [21][340/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.1077e-01 (3.1851e-01)	Acc@1  89.06 ( 89.08)	Acc@5  99.22 ( 99.69)
Epoch: [21][350/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.2968e-01 (3.1888e-01)	Acc@1  82.81 ( 89.08)	Acc@5  98.44 ( 99.70)
Epoch: [21][360/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.4172e-01 (3.1905e-01)	Acc@1  87.50 ( 89.06)	Acc@5  98.44 ( 99.69)
Epoch: [21][370/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.6770e-01 (3.2047e-01)	Acc@1  87.50 ( 89.02)	Acc@5 100.00 ( 99.69)
Epoch: [21][380/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.0215e-01 (3.2119e-01)	Acc@1  89.06 ( 89.02)	Acc@5 100.00 ( 99.70)
Epoch: [21][390/391]	Time  0.176 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.2065e-01 (3.2125e-01)	Acc@1  82.50 ( 89.00)	Acc@5 100.00 ( 99.70)
## e[21] optimizer.zero_grad (sum) time: 0.4776017665863037
## e[21]       loss.backward (sum) time: 10.780338764190674
## e[21]      optimizer.step (sum) time: 47.59117531776428
## epoch[21] training(only) time: 94.2693738937378
# Switched to evaluate mode...
Test: [  0/100]	Time  0.207 ( 0.207)	Loss 3.7411e-01 (3.7411e-01)	Acc@1  86.00 ( 86.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 3.8986e-01 (3.8838e-01)	Acc@1  88.00 ( 86.73)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.077 ( 0.084)	Loss 4.2623e-01 (3.9104e-01)	Acc@1  86.00 ( 86.76)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 3.7515e-01 (3.9659e-01)	Acc@1  86.00 ( 86.65)	Acc@5 100.00 ( 99.58)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 4.0054e-01 (3.9344e-01)	Acc@1  89.00 ( 86.68)	Acc@5 100.00 ( 99.61)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 3.2527e-01 (3.9109e-01)	Acc@1  89.00 ( 86.76)	Acc@5 100.00 ( 99.65)
Test: [ 60/100]	Time  0.079 ( 0.080)	Loss 3.0022e-01 (3.8702e-01)	Acc@1  92.00 ( 86.79)	Acc@5 100.00 ( 99.62)
Test: [ 70/100]	Time  0.079 ( 0.080)	Loss 3.7571e-01 (3.8743e-01)	Acc@1  84.00 ( 86.73)	Acc@5  99.00 ( 99.62)
Test: [ 80/100]	Time  0.077 ( 0.080)	Loss 4.0681e-01 (3.9052e-01)	Acc@1  81.00 ( 86.58)	Acc@5  99.00 ( 99.59)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 2.8937e-01 (3.9234e-01)	Acc@1  89.00 ( 86.59)	Acc@5 100.00 ( 99.56)
 * Acc@1 86.640 Acc@5 99.580
### epoch[21] execution time: 102.29040288925171
EPOCH 22
# current learning rate: 0.1
# Switched to train mode...
Epoch: [22][  0/391]	Time  0.397 ( 0.397)	Data  0.156 ( 0.156)	Loss 3.4635e-01 (3.4635e-01)	Acc@1  83.59 ( 83.59)	Acc@5 100.00 (100.00)
Epoch: [22][ 10/391]	Time  0.240 ( 0.254)	Data  0.001 ( 0.015)	Loss 4.1579e-01 (3.0289e-01)	Acc@1  85.94 ( 88.99)	Acc@5  99.22 ( 99.72)
Epoch: [22][ 20/391]	Time  0.240 ( 0.248)	Data  0.001 ( 0.008)	Loss 3.6343e-01 (2.8592e-01)	Acc@1  87.50 ( 89.62)	Acc@5 100.00 ( 99.78)
Epoch: [22][ 30/391]	Time  0.239 ( 0.246)	Data  0.001 ( 0.006)	Loss 1.8710e-01 (2.8108e-01)	Acc@1  92.19 ( 89.92)	Acc@5 100.00 ( 99.80)
Epoch: [22][ 40/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.005)	Loss 3.0056e-01 (2.9226e-01)	Acc@1  89.06 ( 89.42)	Acc@5  97.66 ( 99.75)
Epoch: [22][ 50/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.6259e-01 (2.8589e-01)	Acc@1  94.53 ( 89.72)	Acc@5 100.00 ( 99.75)
Epoch: [22][ 60/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.004)	Loss 3.5675e-01 (2.9423e-01)	Acc@1  89.06 ( 89.49)	Acc@5  99.22 ( 99.68)
Epoch: [22][ 70/391]	Time  0.239 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.9374e-01 (2.9809e-01)	Acc@1  90.62 ( 89.36)	Acc@5 100.00 ( 99.69)
Epoch: [22][ 80/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 2.9115e-01 (2.9847e-01)	Acc@1  89.84 ( 89.40)	Acc@5 100.00 ( 99.70)
Epoch: [22][ 90/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 3.6233e-01 (2.9963e-01)	Acc@1  86.72 ( 89.38)	Acc@5 100.00 ( 99.71)
Epoch: [22][100/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 2.4195e-01 (2.9678e-01)	Acc@1  92.19 ( 89.52)	Acc@5  99.22 ( 99.72)
Epoch: [22][110/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.8244e-01 (2.9634e-01)	Acc@1  94.53 ( 89.48)	Acc@5  99.22 ( 99.71)
Epoch: [22][120/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.8999e-01 (2.9586e-01)	Acc@1  88.28 ( 89.53)	Acc@5 100.00 ( 99.72)
Epoch: [22][130/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.1398e-01 (2.9897e-01)	Acc@1  89.84 ( 89.49)	Acc@5  99.22 ( 99.70)
Epoch: [22][140/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4964e-01 (2.9902e-01)	Acc@1  92.19 ( 89.53)	Acc@5 100.00 ( 99.72)
Epoch: [22][150/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.7456e-01 (2.9932e-01)	Acc@1  89.84 ( 89.54)	Acc@5 100.00 ( 99.73)
Epoch: [22][160/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.5494e-01 (2.9884e-01)	Acc@1  90.62 ( 89.57)	Acc@5 100.00 ( 99.72)
Epoch: [22][170/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.7280e-01 (2.9858e-01)	Acc@1  89.84 ( 89.57)	Acc@5 100.00 ( 99.71)
Epoch: [22][180/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.3206e-01 (2.9961e-01)	Acc@1  89.84 ( 89.54)	Acc@5  99.22 ( 99.69)
Epoch: [22][190/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.3946e-01 (3.0123e-01)	Acc@1  85.94 ( 89.48)	Acc@5 100.00 ( 99.68)
Epoch: [22][200/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.2355e-01 (3.0157e-01)	Acc@1  90.62 ( 89.47)	Acc@5  99.22 ( 99.67)
Epoch: [22][210/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.5771e-01 (3.0005e-01)	Acc@1  89.06 ( 89.54)	Acc@5 100.00 ( 99.67)
Epoch: [22][220/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.4947e-01 (2.9991e-01)	Acc@1  91.41 ( 89.50)	Acc@5 100.00 ( 99.67)
Epoch: [22][230/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.0913e-01 (3.0142e-01)	Acc@1  87.50 ( 89.48)	Acc@5  98.44 ( 99.68)
Epoch: [22][240/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.2983e-01 (3.0284e-01)	Acc@1  91.41 ( 89.45)	Acc@5 100.00 ( 99.67)
Epoch: [22][250/391]	Time  0.243 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.0482e-01 (3.0192e-01)	Acc@1  91.41 ( 89.48)	Acc@5 100.00 ( 99.67)
Epoch: [22][260/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.3819e-01 (3.0376e-01)	Acc@1  85.16 ( 89.43)	Acc@5  99.22 ( 99.67)
Epoch: [22][270/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.2092e-01 (3.0632e-01)	Acc@1  87.50 ( 89.35)	Acc@5  99.22 ( 99.68)
Epoch: [22][280/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.3637e-01 (3.0657e-01)	Acc@1  86.72 ( 89.37)	Acc@5 100.00 ( 99.69)
Epoch: [22][290/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.6240e-01 (3.0673e-01)	Acc@1  89.06 ( 89.36)	Acc@5 100.00 ( 99.68)
Epoch: [22][300/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.7120e-01 (3.0624e-01)	Acc@1  89.06 ( 89.36)	Acc@5 100.00 ( 99.69)
Epoch: [22][310/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.5828e-01 (3.0550e-01)	Acc@1  90.62 ( 89.35)	Acc@5  99.22 ( 99.69)
Epoch: [22][320/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.7952e-01 (3.0605e-01)	Acc@1  88.28 ( 89.34)	Acc@5 100.00 ( 99.69)
Epoch: [22][330/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.2546e-01 (3.0543e-01)	Acc@1  93.75 ( 89.37)	Acc@5  99.22 ( 99.69)
Epoch: [22][340/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.8634e-01 (3.0585e-01)	Acc@1  85.16 ( 89.35)	Acc@5  99.22 ( 99.69)
Epoch: [22][350/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.0822e-01 (3.0596e-01)	Acc@1  90.62 ( 89.37)	Acc@5 100.00 ( 99.69)
Epoch: [22][360/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.4292e-01 (3.0564e-01)	Acc@1  88.28 ( 89.39)	Acc@5 100.00 ( 99.70)
Epoch: [22][370/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.3599e-01 (3.0571e-01)	Acc@1  90.62 ( 89.40)	Acc@5  98.44 ( 99.68)
Epoch: [22][380/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.5622e-01 (3.0613e-01)	Acc@1  91.41 ( 89.39)	Acc@5 100.00 ( 99.68)
Epoch: [22][390/391]	Time  0.177 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.2266e-01 (3.0718e-01)	Acc@1  82.50 ( 89.37)	Acc@5 100.00 ( 99.68)
## e[22] optimizer.zero_grad (sum) time: 0.47403788566589355
## e[22]       loss.backward (sum) time: 10.802196025848389
## e[22]      optimizer.step (sum) time: 47.59254455566406
## epoch[22] training(only) time: 94.24981808662415
# Switched to evaluate mode...
Test: [  0/100]	Time  0.212 ( 0.212)	Loss 3.7240e-01 (3.7240e-01)	Acc@1  89.00 ( 89.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 3.3625e-01 (3.9189e-01)	Acc@1  89.00 ( 87.36)	Acc@5 100.00 ( 99.36)
Test: [ 20/100]	Time  0.077 ( 0.084)	Loss 3.9495e-01 (4.0646e-01)	Acc@1  86.00 ( 87.14)	Acc@5  99.00 ( 99.19)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 4.4661e-01 (4.1660e-01)	Acc@1  85.00 ( 86.84)	Acc@5 100.00 ( 99.26)
Test: [ 40/100]	Time  0.077 ( 0.081)	Loss 5.2704e-01 (4.2334e-01)	Acc@1  84.00 ( 86.68)	Acc@5  99.00 ( 99.22)
Test: [ 50/100]	Time  0.079 ( 0.080)	Loss 3.1839e-01 (4.2914e-01)	Acc@1  88.00 ( 86.49)	Acc@5  99.00 ( 99.20)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 3.5961e-01 (4.2465e-01)	Acc@1  88.00 ( 86.52)	Acc@5 100.00 ( 99.31)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 5.2459e-01 (4.2638e-01)	Acc@1  86.00 ( 86.55)	Acc@5  99.00 ( 99.31)
Test: [ 80/100]	Time  0.078 ( 0.079)	Loss 3.2135e-01 (4.2266e-01)	Acc@1  86.00 ( 86.62)	Acc@5 100.00 ( 99.37)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 2.1859e-01 (4.2378e-01)	Acc@1  93.00 ( 86.54)	Acc@5 100.00 ( 99.37)
 * Acc@1 86.620 Acc@5 99.380
### epoch[22] execution time: 102.24939465522766
EPOCH 23
# current learning rate: 0.1
# Switched to train mode...
Epoch: [23][  0/391]	Time  0.387 ( 0.387)	Data  0.143 ( 0.143)	Loss 2.7353e-01 (2.7353e-01)	Acc@1  92.19 ( 92.19)	Acc@5  99.22 ( 99.22)
Epoch: [23][ 10/391]	Time  0.240 ( 0.253)	Data  0.001 ( 0.014)	Loss 2.7229e-01 (2.7145e-01)	Acc@1  92.19 ( 91.41)	Acc@5  99.22 ( 99.64)
Epoch: [23][ 20/391]	Time  0.241 ( 0.247)	Data  0.001 ( 0.008)	Loss 3.0250e-01 (2.7528e-01)	Acc@1  86.72 ( 90.96)	Acc@5 100.00 ( 99.67)
Epoch: [23][ 30/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.006)	Loss 2.8594e-01 (2.7114e-01)	Acc@1  89.84 ( 91.10)	Acc@5 100.00 ( 99.72)
Epoch: [23][ 40/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.005)	Loss 2.6691e-01 (2.8255e-01)	Acc@1  92.97 ( 90.89)	Acc@5  99.22 ( 99.66)
Epoch: [23][ 50/391]	Time  0.239 ( 0.243)	Data  0.001 ( 0.004)	Loss 3.0594e-01 (2.8655e-01)	Acc@1  88.28 ( 90.59)	Acc@5  99.22 ( 99.66)
Epoch: [23][ 60/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.004)	Loss 3.6813e-01 (2.8587e-01)	Acc@1  88.28 ( 90.45)	Acc@5  99.22 ( 99.68)
Epoch: [23][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.9694e-01 (2.8404e-01)	Acc@1  94.53 ( 90.54)	Acc@5 100.00 ( 99.68)
Epoch: [23][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.4935e-01 (2.8405e-01)	Acc@1  91.41 ( 90.48)	Acc@5 100.00 ( 99.70)
Epoch: [23][ 90/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 2.2611e-01 (2.7968e-01)	Acc@1  89.84 ( 90.51)	Acc@5 100.00 ( 99.73)
Epoch: [23][100/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.003)	Loss 2.3010e-01 (2.7990e-01)	Acc@1  91.41 ( 90.40)	Acc@5 100.00 ( 99.75)
Epoch: [23][110/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.7426e-01 (2.8175e-01)	Acc@1  89.84 ( 90.31)	Acc@5  98.44 ( 99.74)
Epoch: [23][120/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.2326e-01 (2.8760e-01)	Acc@1  86.72 ( 90.13)	Acc@5 100.00 ( 99.74)
Epoch: [23][130/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.1063e-01 (2.9069e-01)	Acc@1  89.06 ( 89.98)	Acc@5 100.00 ( 99.73)
Epoch: [23][140/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8856e-01 (2.9489e-01)	Acc@1  88.28 ( 89.81)	Acc@5 100.00 ( 99.72)
Epoch: [23][150/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4767e-01 (2.9131e-01)	Acc@1  92.97 ( 89.89)	Acc@5 100.00 ( 99.72)
Epoch: [23][160/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.0107e-01 (2.9041e-01)	Acc@1  88.28 ( 89.92)	Acc@5 100.00 ( 99.73)
Epoch: [23][170/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0193e-01 (2.9141e-01)	Acc@1  93.75 ( 89.88)	Acc@5 100.00 ( 99.74)
Epoch: [23][180/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.5802e-01 (2.9125e-01)	Acc@1  89.84 ( 89.85)	Acc@5  99.22 ( 99.74)
Epoch: [23][190/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.3754e-01 (2.9196e-01)	Acc@1  90.62 ( 89.89)	Acc@5  99.22 ( 99.74)
Epoch: [23][200/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.5773e-01 (2.9167e-01)	Acc@1  88.28 ( 89.87)	Acc@5 100.00 ( 99.73)
Epoch: [23][210/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.7456e-01 (2.9051e-01)	Acc@1  86.72 ( 89.94)	Acc@5 100.00 ( 99.73)
Epoch: [23][220/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.4868e-01 (2.9034e-01)	Acc@1  91.41 ( 89.94)	Acc@5 100.00 ( 99.73)
Epoch: [23][230/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.1497e-01 (2.9175e-01)	Acc@1  89.06 ( 89.89)	Acc@5 100.00 ( 99.73)
Epoch: [23][240/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.5956e-01 (2.9343e-01)	Acc@1  92.19 ( 89.87)	Acc@5 100.00 ( 99.73)
Epoch: [23][250/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.4779e-01 (2.9367e-01)	Acc@1  88.28 ( 89.88)	Acc@5  99.22 ( 99.71)
Epoch: [23][260/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.5502e-01 (2.9264e-01)	Acc@1  89.84 ( 89.91)	Acc@5  99.22 ( 99.71)
Epoch: [23][270/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.5172e-01 (2.9213e-01)	Acc@1  90.62 ( 89.88)	Acc@5 100.00 ( 99.72)
Epoch: [23][280/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.0424e-01 (2.9249e-01)	Acc@1  89.06 ( 89.85)	Acc@5 100.00 ( 99.72)
Epoch: [23][290/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.3590e-01 (2.9324e-01)	Acc@1  84.38 ( 89.85)	Acc@5  99.22 ( 99.72)
Epoch: [23][300/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.2421e-01 (2.9272e-01)	Acc@1  93.75 ( 89.87)	Acc@5 100.00 ( 99.73)
Epoch: [23][310/391]	Time  0.247 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.8328e-01 (2.9256e-01)	Acc@1  89.06 ( 89.86)	Acc@5 100.00 ( 99.73)
Epoch: [23][320/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.9698e-01 (2.9256e-01)	Acc@1  87.50 ( 89.83)	Acc@5 100.00 ( 99.73)
Epoch: [23][330/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.0022e-01 (2.9240e-01)	Acc@1  89.84 ( 89.83)	Acc@5 100.00 ( 99.74)
Epoch: [23][340/391]	Time  0.249 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.1270e-01 (2.9361e-01)	Acc@1  88.28 ( 89.81)	Acc@5 100.00 ( 99.74)
Epoch: [23][350/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.4706e-01 (2.9423e-01)	Acc@1  87.50 ( 89.81)	Acc@5  98.44 ( 99.73)
Epoch: [23][360/391]	Time  0.252 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.5770e-01 (2.9536e-01)	Acc@1  85.16 ( 89.77)	Acc@5 100.00 ( 99.72)
Epoch: [23][370/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.1131e-01 (2.9595e-01)	Acc@1  89.06 ( 89.78)	Acc@5 100.00 ( 99.72)
Epoch: [23][380/391]	Time  0.243 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.5506e-01 (2.9608e-01)	Acc@1  92.19 ( 89.77)	Acc@5  99.22 ( 99.72)
Epoch: [23][390/391]	Time  0.176 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.7899e-01 (2.9631e-01)	Acc@1  93.75 ( 89.78)	Acc@5 100.00 ( 99.72)
## e[23] optimizer.zero_grad (sum) time: 0.47722434997558594
## e[23]       loss.backward (sum) time: 10.813151597976685
## e[23]      optimizer.step (sum) time: 47.60666561126709
## epoch[23] training(only) time: 94.39947724342346
# Switched to evaluate mode...
Test: [  0/100]	Time  0.205 ( 0.205)	Loss 4.6569e-01 (4.6569e-01)	Acc@1  84.00 ( 84.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.077 ( 0.090)	Loss 5.8420e-01 (5.0089e-01)	Acc@1  81.00 ( 83.55)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 3.1775e-01 (4.7943e-01)	Acc@1  89.00 ( 84.10)	Acc@5  98.00 ( 99.43)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 4.7522e-01 (4.8940e-01)	Acc@1  83.00 ( 84.19)	Acc@5 100.00 ( 99.42)
Test: [ 40/100]	Time  0.079 ( 0.081)	Loss 5.3495e-01 (4.8767e-01)	Acc@1  84.00 ( 84.37)	Acc@5  99.00 ( 99.41)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 2.8897e-01 (4.7074e-01)	Acc@1  90.00 ( 84.76)	Acc@5 100.00 ( 99.41)
Test: [ 60/100]	Time  0.079 ( 0.080)	Loss 4.6621e-01 (4.6950e-01)	Acc@1  84.00 ( 84.80)	Acc@5 100.00 ( 99.41)
Test: [ 70/100]	Time  0.077 ( 0.080)	Loss 5.1959e-01 (4.6464e-01)	Acc@1  81.00 ( 84.92)	Acc@5 100.00 ( 99.45)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 4.3266e-01 (4.6347e-01)	Acc@1  83.00 ( 84.98)	Acc@5 100.00 ( 99.48)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 4.0021e-01 (4.6142e-01)	Acc@1  84.00 ( 84.95)	Acc@5 100.00 ( 99.48)
 * Acc@1 85.080 Acc@5 99.490
### epoch[23] execution time: 102.4171736240387
EPOCH 24
# current learning rate: 0.1
# Switched to train mode...
Epoch: [24][  0/391]	Time  0.399 ( 0.399)	Data  0.150 ( 0.150)	Loss 2.5376e-01 (2.5376e-01)	Acc@1  91.41 ( 91.41)	Acc@5 100.00 (100.00)
Epoch: [24][ 10/391]	Time  0.241 ( 0.255)	Data  0.001 ( 0.015)	Loss 2.0863e-01 (2.3208e-01)	Acc@1  92.97 ( 91.76)	Acc@5 100.00 ( 99.93)
Epoch: [24][ 20/391]	Time  0.240 ( 0.248)	Data  0.001 ( 0.008)	Loss 2.6305e-01 (2.5775e-01)	Acc@1  90.62 ( 90.92)	Acc@5 100.00 ( 99.89)
Epoch: [24][ 30/391]	Time  0.240 ( 0.246)	Data  0.001 ( 0.006)	Loss 2.1733e-01 (2.5350e-01)	Acc@1  94.53 ( 91.05)	Acc@5  98.44 ( 99.80)
Epoch: [24][ 40/391]	Time  0.239 ( 0.245)	Data  0.001 ( 0.005)	Loss 2.8258e-01 (2.5932e-01)	Acc@1  87.50 ( 90.78)	Acc@5  99.22 ( 99.75)
Epoch: [24][ 50/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 3.0273e-01 (2.5528e-01)	Acc@1  89.84 ( 91.07)	Acc@5 100.00 ( 99.80)
Epoch: [24][ 60/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 2.4026e-01 (2.4977e-01)	Acc@1  92.97 ( 91.25)	Acc@5  99.22 ( 99.80)
Epoch: [24][ 70/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.3472e-01 (2.4982e-01)	Acc@1  91.41 ( 91.22)	Acc@5 100.00 ( 99.81)
Epoch: [24][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.7885e-01 (2.4947e-01)	Acc@1  90.62 ( 91.30)	Acc@5 100.00 ( 99.82)
Epoch: [24][ 90/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.6607e-01 (2.5123e-01)	Acc@1  84.38 ( 91.28)	Acc@5 100.00 ( 99.83)
Epoch: [24][100/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 2.6586e-01 (2.5691e-01)	Acc@1  91.41 ( 91.14)	Acc@5  98.44 ( 99.79)
Epoch: [24][110/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 4.6046e-01 (2.6082e-01)	Acc@1  87.50 ( 91.03)	Acc@5  99.22 ( 99.79)
Epoch: [24][120/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5851e-01 (2.6464e-01)	Acc@1  96.09 ( 90.88)	Acc@5 100.00 ( 99.78)
Epoch: [24][130/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.4862e-01 (2.6610e-01)	Acc@1  86.72 ( 90.79)	Acc@5 100.00 ( 99.78)
Epoch: [24][140/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8232e-01 (2.6490e-01)	Acc@1  92.97 ( 90.78)	Acc@5 100.00 ( 99.79)
Epoch: [24][150/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.5359e-01 (2.6566e-01)	Acc@1  91.41 ( 90.84)	Acc@5  99.22 ( 99.77)
Epoch: [24][160/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6182e-01 (2.6485e-01)	Acc@1  89.84 ( 90.85)	Acc@5 100.00 ( 99.76)
Epoch: [24][170/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.3149e-01 (2.6591e-01)	Acc@1  86.72 ( 90.82)	Acc@5 100.00 ( 99.74)
Epoch: [24][180/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3045e-01 (2.6515e-01)	Acc@1  92.19 ( 90.83)	Acc@5 100.00 ( 99.76)
Epoch: [24][190/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.7086e-01 (2.6625e-01)	Acc@1  88.28 ( 90.78)	Acc@5 100.00 ( 99.77)
Epoch: [24][200/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.2811e-01 (2.6673e-01)	Acc@1  88.28 ( 90.78)	Acc@5  98.44 ( 99.76)
Epoch: [24][210/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.9952e-01 (2.6741e-01)	Acc@1  89.06 ( 90.73)	Acc@5 100.00 ( 99.76)
Epoch: [24][220/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.7371e-01 (2.6689e-01)	Acc@1  92.19 ( 90.75)	Acc@5 100.00 ( 99.76)
Epoch: [24][230/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.0126e-01 (2.6684e-01)	Acc@1  90.62 ( 90.75)	Acc@5 100.00 ( 99.76)
Epoch: [24][240/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.9028e-01 (2.6765e-01)	Acc@1  89.84 ( 90.72)	Acc@5  99.22 ( 99.75)
Epoch: [24][250/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8075e-01 (2.6874e-01)	Acc@1  96.09 ( 90.72)	Acc@5 100.00 ( 99.76)
Epoch: [24][260/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3408e-01 (2.7043e-01)	Acc@1  93.75 ( 90.69)	Acc@5  99.22 ( 99.75)
Epoch: [24][270/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.5300e-01 (2.7107e-01)	Acc@1  89.06 ( 90.69)	Acc@5 100.00 ( 99.75)
Epoch: [24][280/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.0642e-01 (2.7127e-01)	Acc@1  87.50 ( 90.66)	Acc@5 100.00 ( 99.75)
Epoch: [24][290/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8046e-01 (2.7236e-01)	Acc@1  90.62 ( 90.65)	Acc@5  99.22 ( 99.74)
Epoch: [24][300/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.1808e-01 (2.7273e-01)	Acc@1  89.84 ( 90.65)	Acc@5  99.22 ( 99.74)
Epoch: [24][310/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1334e-01 (2.7326e-01)	Acc@1  95.31 ( 90.64)	Acc@5 100.00 ( 99.75)
Epoch: [24][320/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6955e-01 (2.7515e-01)	Acc@1  90.62 ( 90.57)	Acc@5  99.22 ( 99.74)
Epoch: [24][330/391]	Time  0.249 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.5128e-01 (2.7597e-01)	Acc@1  87.50 ( 90.54)	Acc@5  99.22 ( 99.74)
Epoch: [24][340/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.5820e-01 (2.7617e-01)	Acc@1  84.38 ( 90.53)	Acc@5 100.00 ( 99.75)
Epoch: [24][350/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.5978e-01 (2.7707e-01)	Acc@1  89.84 ( 90.51)	Acc@5 100.00 ( 99.75)
Epoch: [24][360/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4674e-01 (2.7659e-01)	Acc@1  92.97 ( 90.52)	Acc@5  99.22 ( 99.74)
Epoch: [24][370/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.7271e-01 (2.7715e-01)	Acc@1  89.84 ( 90.49)	Acc@5 100.00 ( 99.74)
Epoch: [24][380/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3234e-01 (2.7741e-01)	Acc@1  92.97 ( 90.48)	Acc@5 100.00 ( 99.74)
Epoch: [24][390/391]	Time  0.182 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.8961e-01 (2.7809e-01)	Acc@1  83.75 ( 90.45)	Acc@5 100.00 ( 99.74)
## e[24] optimizer.zero_grad (sum) time: 0.47553324699401855
## e[24]       loss.backward (sum) time: 10.86342453956604
## e[24]      optimizer.step (sum) time: 47.59687042236328
## epoch[24] training(only) time: 94.46328258514404
# Switched to evaluate mode...
Test: [  0/100]	Time  0.210 ( 0.210)	Loss 3.3308e-01 (3.3308e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.077 ( 0.090)	Loss 3.2734e-01 (3.7411e-01)	Acc@1  87.00 ( 88.00)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 4.7116e-01 (3.9935e-01)	Acc@1  82.00 ( 87.24)	Acc@5 100.00 ( 99.48)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 4.9650e-01 (4.1484e-01)	Acc@1  81.00 ( 86.35)	Acc@5 100.00 ( 99.45)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 3.8770e-01 (4.1898e-01)	Acc@1  89.00 ( 86.34)	Acc@5  99.00 ( 99.49)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 4.4231e-01 (4.1942e-01)	Acc@1  86.00 ( 86.14)	Acc@5 100.00 ( 99.47)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 5.6566e-01 (4.2091e-01)	Acc@1  82.00 ( 86.07)	Acc@5 100.00 ( 99.52)
Test: [ 70/100]	Time  0.077 ( 0.080)	Loss 3.5525e-01 (4.2178e-01)	Acc@1  89.00 ( 86.13)	Acc@5 100.00 ( 99.51)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 3.0039e-01 (4.1867e-01)	Acc@1  92.00 ( 86.25)	Acc@5 100.00 ( 99.51)
Test: [ 90/100]	Time  0.077 ( 0.079)	Loss 2.8809e-01 (4.2102e-01)	Acc@1  90.00 ( 86.19)	Acc@5  99.00 ( 99.47)
 * Acc@1 86.200 Acc@5 99.510
### epoch[24] execution time: 102.5047698020935
EPOCH 25
# current learning rate: 0.1
# Switched to train mode...
Epoch: [25][  0/391]	Time  0.410 ( 0.410)	Data  0.161 ( 0.161)	Loss 2.5966e-01 (2.5966e-01)	Acc@1  89.06 ( 89.06)	Acc@5 100.00 (100.00)
Epoch: [25][ 10/391]	Time  0.240 ( 0.257)	Data  0.001 ( 0.016)	Loss 2.3802e-01 (2.4524e-01)	Acc@1  91.41 ( 91.55)	Acc@5 100.00 ( 99.79)
Epoch: [25][ 20/391]	Time  0.240 ( 0.249)	Data  0.001 ( 0.009)	Loss 3.3914e-01 (2.5239e-01)	Acc@1  87.50 ( 91.22)	Acc@5 100.00 ( 99.74)
Epoch: [25][ 30/391]	Time  0.240 ( 0.246)	Data  0.001 ( 0.006)	Loss 1.9926e-01 (2.4858e-01)	Acc@1  92.97 ( 91.15)	Acc@5 100.00 ( 99.80)
Epoch: [25][ 40/391]	Time  0.239 ( 0.245)	Data  0.001 ( 0.005)	Loss 2.8523e-01 (2.5373e-01)	Acc@1  91.41 ( 91.03)	Acc@5 100.00 ( 99.81)
Epoch: [25][ 50/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 3.1339e-01 (2.5547e-01)	Acc@1  89.06 ( 91.04)	Acc@5 100.00 ( 99.82)
Epoch: [25][ 60/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 3.2558e-01 (2.5636e-01)	Acc@1  89.06 ( 91.09)	Acc@5 100.00 ( 99.80)
Epoch: [25][ 70/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.3974e-01 (2.5623e-01)	Acc@1  92.19 ( 91.07)	Acc@5  99.22 ( 99.76)
Epoch: [25][ 80/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.0598e-01 (2.5676e-01)	Acc@1  86.72 ( 91.11)	Acc@5 100.00 ( 99.77)
Epoch: [25][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.2437e-01 (2.5686e-01)	Acc@1  93.75 ( 91.15)	Acc@5 100.00 ( 99.77)
Epoch: [25][100/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 2.5987e-01 (2.5520e-01)	Acc@1  88.28 ( 91.17)	Acc@5  99.22 ( 99.77)
Epoch: [25][110/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.2853e-01 (2.5342e-01)	Acc@1  89.06 ( 91.24)	Acc@5  99.22 ( 99.77)
Epoch: [25][120/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8235e-01 (2.5518e-01)	Acc@1  89.84 ( 91.15)	Acc@5  99.22 ( 99.76)
Epoch: [25][130/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0317e-01 (2.5393e-01)	Acc@1  92.19 ( 91.19)	Acc@5  99.22 ( 99.76)
Epoch: [25][140/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.9850e-01 (2.5479e-01)	Acc@1  90.62 ( 91.13)	Acc@5 100.00 ( 99.77)
Epoch: [25][150/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3820e-01 (2.5699e-01)	Acc@1  90.62 ( 91.03)	Acc@5 100.00 ( 99.77)
Epoch: [25][160/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6978e-01 (2.5690e-01)	Acc@1  93.75 ( 91.01)	Acc@5  99.22 ( 99.76)
Epoch: [25][170/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1108e-01 (2.5591e-01)	Acc@1  92.97 ( 91.08)	Acc@5 100.00 ( 99.75)
Epoch: [25][180/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6958e-01 (2.5719e-01)	Acc@1  91.41 ( 91.02)	Acc@5 100.00 ( 99.74)
Epoch: [25][190/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.3421e-01 (2.5985e-01)	Acc@1  86.72 ( 90.93)	Acc@5 100.00 ( 99.75)
Epoch: [25][200/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6183e-01 (2.6001e-01)	Acc@1  96.09 ( 90.95)	Acc@5 100.00 ( 99.76)
Epoch: [25][210/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.9284e-01 (2.6007e-01)	Acc@1  91.41 ( 90.92)	Acc@5  99.22 ( 99.76)
Epoch: [25][220/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.9174e-01 (2.6036e-01)	Acc@1  89.06 ( 90.93)	Acc@5 100.00 ( 99.75)
Epoch: [25][230/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.9024e-01 (2.6069e-01)	Acc@1  89.84 ( 90.93)	Acc@5 100.00 ( 99.74)
Epoch: [25][240/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8696e-01 (2.6092e-01)	Acc@1  87.50 ( 90.91)	Acc@5 100.00 ( 99.74)
Epoch: [25][250/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.5479e-01 (2.6131e-01)	Acc@1  93.75 ( 90.91)	Acc@5  99.22 ( 99.74)
Epoch: [25][260/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4863e-01 (2.6100e-01)	Acc@1  91.41 ( 90.93)	Acc@5 100.00 ( 99.75)
Epoch: [25][270/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1136e-01 (2.6121e-01)	Acc@1  92.19 ( 90.92)	Acc@5 100.00 ( 99.75)
Epoch: [25][280/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.7093e-01 (2.6112e-01)	Acc@1  90.62 ( 90.95)	Acc@5 100.00 ( 99.76)
Epoch: [25][290/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.2410e-01 (2.6254e-01)	Acc@1  89.06 ( 90.92)	Acc@5 100.00 ( 99.75)
Epoch: [25][300/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2445e-01 (2.6338e-01)	Acc@1  91.41 ( 90.89)	Acc@5 100.00 ( 99.76)
Epoch: [25][310/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9306e-01 (2.6344e-01)	Acc@1  92.97 ( 90.89)	Acc@5 100.00 ( 99.76)
Epoch: [25][320/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.1644e-01 (2.6372e-01)	Acc@1  83.59 ( 90.85)	Acc@5 100.00 ( 99.77)
Epoch: [25][330/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.9212e-01 (2.6338e-01)	Acc@1  89.84 ( 90.86)	Acc@5 100.00 ( 99.77)
Epoch: [25][340/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3095e-01 (2.6334e-01)	Acc@1  92.19 ( 90.88)	Acc@5 100.00 ( 99.76)
Epoch: [25][350/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.4613e-01 (2.6423e-01)	Acc@1  90.62 ( 90.85)	Acc@5 100.00 ( 99.77)
Epoch: [25][360/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.9467e-01 (2.6534e-01)	Acc@1  89.06 ( 90.82)	Acc@5 100.00 ( 99.77)
Epoch: [25][370/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.0091e-01 (2.6625e-01)	Acc@1  91.41 ( 90.78)	Acc@5 100.00 ( 99.77)
Epoch: [25][380/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.8072e-01 (2.6635e-01)	Acc@1  91.41 ( 90.75)	Acc@5 100.00 ( 99.77)
Epoch: [25][390/391]	Time  0.177 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.3010e-01 (2.6757e-01)	Acc@1  85.00 ( 90.70)	Acc@5  98.75 ( 99.77)
## e[25] optimizer.zero_grad (sum) time: 0.47788333892822266
## e[25]       loss.backward (sum) time: 10.845974683761597
## e[25]      optimizer.step (sum) time: 47.615756034851074
## epoch[25] training(only) time: 94.4474527835846
# Switched to evaluate mode...
Test: [  0/100]	Time  0.203 ( 0.203)	Loss 3.9974e-01 (3.9974e-01)	Acc@1  84.00 ( 84.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.078 ( 0.089)	Loss 4.2357e-01 (4.5059e-01)	Acc@1  87.00 ( 85.36)	Acc@5 100.00 ( 99.55)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 5.2508e-01 (4.6343e-01)	Acc@1  81.00 ( 85.33)	Acc@5 100.00 ( 99.48)
Test: [ 30/100]	Time  0.077 ( 0.082)	Loss 5.0966e-01 (4.6337e-01)	Acc@1  80.00 ( 85.39)	Acc@5 100.00 ( 99.48)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 4.8771e-01 (4.6402e-01)	Acc@1  88.00 ( 85.51)	Acc@5  98.00 ( 99.46)
Test: [ 50/100]	Time  0.079 ( 0.080)	Loss 4.6002e-01 (4.6257e-01)	Acc@1  85.00 ( 85.49)	Acc@5 100.00 ( 99.49)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 4.4805e-01 (4.5960e-01)	Acc@1  85.00 ( 85.39)	Acc@5 100.00 ( 99.54)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 4.3287e-01 (4.6637e-01)	Acc@1  86.00 ( 85.07)	Acc@5  99.00 ( 99.56)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 4.1399e-01 (4.6424e-01)	Acc@1  89.00 ( 85.10)	Acc@5  99.00 ( 99.57)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 3.6025e-01 (4.6760e-01)	Acc@1  89.00 ( 85.10)	Acc@5 100.00 ( 99.54)
 * Acc@1 85.260 Acc@5 99.550
### epoch[25] execution time: 102.4540753364563
EPOCH 26
# current learning rate: 0.1
# Switched to train mode...
Epoch: [26][  0/391]	Time  0.383 ( 0.383)	Data  0.139 ( 0.139)	Loss 2.9251e-01 (2.9251e-01)	Acc@1  88.28 ( 88.28)	Acc@5 100.00 (100.00)
Epoch: [26][ 10/391]	Time  0.241 ( 0.253)	Data  0.001 ( 0.014)	Loss 2.4299e-01 (2.2634e-01)	Acc@1  93.75 ( 92.68)	Acc@5  99.22 ( 99.79)
Epoch: [26][ 20/391]	Time  0.240 ( 0.247)	Data  0.001 ( 0.008)	Loss 3.3484e-01 (2.3603e-01)	Acc@1  86.72 ( 92.11)	Acc@5 100.00 ( 99.81)
Epoch: [26][ 30/391]	Time  0.243 ( 0.245)	Data  0.001 ( 0.006)	Loss 1.7178e-01 (2.2558e-01)	Acc@1  95.31 ( 92.31)	Acc@5 100.00 ( 99.87)
Epoch: [26][ 40/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.005)	Loss 2.1190e-01 (2.2334e-01)	Acc@1  92.97 ( 92.23)	Acc@5 100.00 ( 99.87)
Epoch: [26][ 50/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 2.3987e-01 (2.3180e-01)	Acc@1  91.41 ( 92.03)	Acc@5  99.22 ( 99.82)
Epoch: [26][ 60/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.7372e-01 (2.3312e-01)	Acc@1  94.53 ( 92.14)	Acc@5 100.00 ( 99.81)
Epoch: [26][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.9540e-01 (2.3764e-01)	Acc@1  92.97 ( 91.98)	Acc@5 100.00 ( 99.79)
Epoch: [26][ 80/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.2527e-01 (2.4002e-01)	Acc@1  92.19 ( 92.02)	Acc@5 100.00 ( 99.80)
Epoch: [26][ 90/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.9609e-01 (2.4056e-01)	Acc@1  93.75 ( 91.89)	Acc@5 100.00 ( 99.80)
Epoch: [26][100/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 2.3067e-01 (2.4009e-01)	Acc@1  92.19 ( 91.81)	Acc@5 100.00 ( 99.81)
Epoch: [26][110/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6060e-01 (2.4086e-01)	Acc@1  94.53 ( 91.74)	Acc@5 100.00 ( 99.83)
Epoch: [26][120/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8115e-01 (2.4195e-01)	Acc@1  92.19 ( 91.67)	Acc@5 100.00 ( 99.83)
Epoch: [26][130/391]	Time  0.246 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1314e-01 (2.4157e-01)	Acc@1  91.41 ( 91.63)	Acc@5 100.00 ( 99.83)
Epoch: [26][140/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3023e-01 (2.4625e-01)	Acc@1  91.41 ( 91.40)	Acc@5 100.00 ( 99.82)
Epoch: [26][150/391]	Time  0.252 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.1338e-01 (2.4833e-01)	Acc@1  93.75 ( 91.29)	Acc@5 100.00 ( 99.83)
Epoch: [26][160/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.6315e-01 (2.4875e-01)	Acc@1  87.50 ( 91.30)	Acc@5  99.22 ( 99.83)
Epoch: [26][170/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.3793e-01 (2.5046e-01)	Acc@1  89.84 ( 91.21)	Acc@5 100.00 ( 99.83)
Epoch: [26][180/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3751e-01 (2.4928e-01)	Acc@1  89.06 ( 91.26)	Acc@5 100.00 ( 99.83)
Epoch: [26][190/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.2918e-01 (2.5113e-01)	Acc@1  85.16 ( 91.21)	Acc@5  99.22 ( 99.82)
Epoch: [26][200/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.3083e-01 (2.5287e-01)	Acc@1  85.16 ( 91.11)	Acc@5 100.00 ( 99.83)
Epoch: [26][210/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.9032e-01 (2.5311e-01)	Acc@1  91.41 ( 91.11)	Acc@5  99.22 ( 99.82)
Epoch: [26][220/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6020e-01 (2.5325e-01)	Acc@1  90.62 ( 91.13)	Acc@5 100.00 ( 99.81)
Epoch: [26][230/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3988e-01 (2.5222e-01)	Acc@1  92.97 ( 91.18)	Acc@5 100.00 ( 99.81)
Epoch: [26][240/391]	Time  0.247 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6028e-01 (2.5203e-01)	Acc@1  90.62 ( 91.21)	Acc@5 100.00 ( 99.82)
Epoch: [26][250/391]	Time  0.248 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0805e-01 (2.5340e-01)	Acc@1  90.62 ( 91.18)	Acc@5 100.00 ( 99.81)
Epoch: [26][260/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.5673e-01 (2.5411e-01)	Acc@1  89.06 ( 91.15)	Acc@5 100.00 ( 99.82)
Epoch: [26][270/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1527e-01 (2.5402e-01)	Acc@1  92.97 ( 91.16)	Acc@5 100.00 ( 99.82)
Epoch: [26][280/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6303e-01 (2.5325e-01)	Acc@1  91.41 ( 91.20)	Acc@5 100.00 ( 99.82)
Epoch: [26][290/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6419e-01 (2.5267e-01)	Acc@1  94.53 ( 91.21)	Acc@5 100.00 ( 99.81)
Epoch: [26][300/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8211e-01 (2.5234e-01)	Acc@1  90.62 ( 91.22)	Acc@5 100.00 ( 99.82)
Epoch: [26][310/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.4879e-01 (2.5305e-01)	Acc@1  89.84 ( 91.20)	Acc@5 100.00 ( 99.82)
Epoch: [26][320/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.3260e-01 (2.5412e-01)	Acc@1  87.50 ( 91.19)	Acc@5 100.00 ( 99.81)
Epoch: [26][330/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.1049e-01 (2.5398e-01)	Acc@1  92.19 ( 91.17)	Acc@5 100.00 ( 99.81)
Epoch: [26][340/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.5328e-01 (2.5440e-01)	Acc@1  90.62 ( 91.15)	Acc@5  99.22 ( 99.81)
Epoch: [26][350/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.4410e-01 (2.5428e-01)	Acc@1  91.41 ( 91.14)	Acc@5 100.00 ( 99.81)
Epoch: [26][360/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.9761e-01 (2.5456e-01)	Acc@1  90.62 ( 91.13)	Acc@5  99.22 ( 99.80)
Epoch: [26][370/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.8772e-01 (2.5479e-01)	Acc@1  90.62 ( 91.11)	Acc@5 100.00 ( 99.80)
Epoch: [26][380/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.6068e-01 (2.5662e-01)	Acc@1  90.62 ( 91.04)	Acc@5 100.00 ( 99.80)
Epoch: [26][390/391]	Time  0.176 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.1845e-01 (2.5736e-01)	Acc@1  85.00 ( 91.00)	Acc@5 100.00 ( 99.80)
## e[26] optimizer.zero_grad (sum) time: 0.47816014289855957
## e[26]       loss.backward (sum) time: 10.85996389389038
## e[26]      optimizer.step (sum) time: 47.57164645195007
## epoch[26] training(only) time: 94.40952897071838
# Switched to evaluate mode...
Test: [  0/100]	Time  0.203 ( 0.203)	Loss 4.3721e-01 (4.3721e-01)	Acc@1  86.00 ( 86.00)	Acc@5  97.00 ( 97.00)
Test: [ 10/100]	Time  0.078 ( 0.089)	Loss 4.8648e-01 (4.3860e-01)	Acc@1  87.00 ( 86.45)	Acc@5  99.00 ( 99.18)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 4.1729e-01 (4.2362e-01)	Acc@1  89.00 ( 86.95)	Acc@5 100.00 ( 99.29)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 4.0190e-01 (4.3724e-01)	Acc@1  84.00 ( 86.26)	Acc@5 100.00 ( 99.35)
Test: [ 40/100]	Time  0.079 ( 0.081)	Loss 5.3605e-01 (4.3826e-01)	Acc@1  84.00 ( 86.54)	Acc@5 100.00 ( 99.41)
Test: [ 50/100]	Time  0.077 ( 0.080)	Loss 5.2157e-01 (4.2963e-01)	Acc@1  89.00 ( 86.84)	Acc@5  99.00 ( 99.41)
Test: [ 60/100]	Time  0.079 ( 0.080)	Loss 4.9532e-01 (4.2881e-01)	Acc@1  83.00 ( 86.66)	Acc@5  99.00 ( 99.46)
Test: [ 70/100]	Time  0.077 ( 0.080)	Loss 4.9798e-01 (4.2542e-01)	Acc@1  83.00 ( 86.68)	Acc@5 100.00 ( 99.46)
Test: [ 80/100]	Time  0.078 ( 0.079)	Loss 5.2648e-01 (4.3005e-01)	Acc@1  84.00 ( 86.59)	Acc@5  99.00 ( 99.47)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 3.3397e-01 (4.3367e-01)	Acc@1  89.00 ( 86.51)	Acc@5 100.00 ( 99.46)
 * Acc@1 86.500 Acc@5 99.460
### epoch[26] execution time: 102.40795993804932
EPOCH 27
# current learning rate: 0.1
# Switched to train mode...
Epoch: [27][  0/391]	Time  0.378 ( 0.378)	Data  0.138 ( 0.138)	Loss 2.2112e-01 (2.2112e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [27][ 10/391]	Time  0.240 ( 0.253)	Data  0.001 ( 0.014)	Loss 2.1642e-01 (2.4726e-01)	Acc@1  94.53 ( 91.83)	Acc@5  99.22 ( 99.72)
Epoch: [27][ 20/391]	Time  0.241 ( 0.247)	Data  0.001 ( 0.008)	Loss 3.3142e-01 (2.4297e-01)	Acc@1  89.84 ( 91.67)	Acc@5 100.00 ( 99.85)
Epoch: [27][ 30/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.006)	Loss 3.1115e-01 (2.4506e-01)	Acc@1  89.06 ( 91.58)	Acc@5  99.22 ( 99.87)
Epoch: [27][ 40/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 2.0370e-01 (2.3908e-01)	Acc@1  94.53 ( 91.79)	Acc@5 100.00 ( 99.89)
Epoch: [27][ 50/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.004)	Loss 2.6685e-01 (2.3769e-01)	Acc@1  89.84 ( 91.79)	Acc@5 100.00 ( 99.88)
Epoch: [27][ 60/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.0600e-01 (2.3277e-01)	Acc@1  92.19 ( 91.98)	Acc@5 100.00 ( 99.85)
Epoch: [27][ 70/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 2.0886e-01 (2.3080e-01)	Acc@1  93.75 ( 92.03)	Acc@5 100.00 ( 99.82)
Epoch: [27][ 80/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.6540e-01 (2.2971e-01)	Acc@1  93.75 ( 92.10)	Acc@5 100.00 ( 99.84)
Epoch: [27][ 90/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 2.2327e-01 (2.3468e-01)	Acc@1  92.19 ( 91.88)	Acc@5 100.00 ( 99.83)
Epoch: [27][100/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.6628e-01 (2.3439e-01)	Acc@1  96.09 ( 91.96)	Acc@5 100.00 ( 99.81)
Epoch: [27][110/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8802e-01 (2.3504e-01)	Acc@1  92.19 ( 91.96)	Acc@5 100.00 ( 99.81)
Epoch: [27][120/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3598e-01 (2.3753e-01)	Acc@1  92.19 ( 91.83)	Acc@5 100.00 ( 99.82)
Epoch: [27][130/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4542e-01 (2.3751e-01)	Acc@1  92.19 ( 91.90)	Acc@5 100.00 ( 99.82)
Epoch: [27][140/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.5390e-01 (2.3670e-01)	Acc@1  89.06 ( 91.86)	Acc@5 100.00 ( 99.81)
Epoch: [27][150/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6963e-01 (2.3546e-01)	Acc@1  86.72 ( 91.87)	Acc@5 100.00 ( 99.79)
Epoch: [27][160/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.9477e-01 (2.3462e-01)	Acc@1  92.97 ( 91.88)	Acc@5 100.00 ( 99.79)
Epoch: [27][170/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.5120e-01 (2.3478e-01)	Acc@1  92.97 ( 91.88)	Acc@5 100.00 ( 99.80)
Epoch: [27][180/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.5837e-01 (2.3659e-01)	Acc@1  90.62 ( 91.81)	Acc@5  99.22 ( 99.80)
Epoch: [27][190/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.1703e-01 (2.3647e-01)	Acc@1  90.62 ( 91.79)	Acc@5 100.00 ( 99.80)
Epoch: [27][200/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.9001e-01 (2.3889e-01)	Acc@1  89.84 ( 91.70)	Acc@5 100.00 ( 99.81)
Epoch: [27][210/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.9334e-01 (2.4107e-01)	Acc@1  88.28 ( 91.65)	Acc@5 100.00 ( 99.80)
Epoch: [27][220/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.9633e-01 (2.4457e-01)	Acc@1  87.50 ( 91.55)	Acc@5  99.22 ( 99.81)
Epoch: [27][230/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.2132e-01 (2.4477e-01)	Acc@1  91.41 ( 91.53)	Acc@5 100.00 ( 99.81)
Epoch: [27][240/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.8949e-01 (2.4599e-01)	Acc@1  90.62 ( 91.47)	Acc@5 100.00 ( 99.81)
Epoch: [27][250/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.9748e-01 (2.4546e-01)	Acc@1  92.97 ( 91.51)	Acc@5 100.00 ( 99.81)
Epoch: [27][260/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.1079e-01 (2.4517e-01)	Acc@1  87.50 ( 91.52)	Acc@5 100.00 ( 99.81)
Epoch: [27][270/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.1156e-01 (2.4535e-01)	Acc@1  89.06 ( 91.51)	Acc@5  99.22 ( 99.81)
Epoch: [27][280/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.8010e-01 (2.4598e-01)	Acc@1  92.97 ( 91.47)	Acc@5 100.00 ( 99.81)
Epoch: [27][290/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.0340e-01 (2.4630e-01)	Acc@1  92.97 ( 91.44)	Acc@5 100.00 ( 99.82)
Epoch: [27][300/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.2105e-01 (2.4672e-01)	Acc@1  90.62 ( 91.43)	Acc@5 100.00 ( 99.82)
Epoch: [27][310/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.8067e-01 (2.4726e-01)	Acc@1  86.72 ( 91.41)	Acc@5 100.00 ( 99.81)
Epoch: [27][320/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.5890e-01 (2.4764e-01)	Acc@1  89.06 ( 91.39)	Acc@5 100.00 ( 99.81)
Epoch: [27][330/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.3141e-01 (2.4798e-01)	Acc@1  91.41 ( 91.38)	Acc@5 100.00 ( 99.82)
Epoch: [27][340/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.2380e-01 (2.4894e-01)	Acc@1  89.84 ( 91.35)	Acc@5 100.00 ( 99.82)
Epoch: [27][350/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.7205e-01 (2.4977e-01)	Acc@1  90.62 ( 91.32)	Acc@5  99.22 ( 99.82)
Epoch: [27][360/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.0569e-01 (2.5051e-01)	Acc@1  88.28 ( 91.31)	Acc@5  99.22 ( 99.81)
Epoch: [27][370/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.7449e-01 (2.5120e-01)	Acc@1  91.41 ( 91.31)	Acc@5  99.22 ( 99.81)
Epoch: [27][380/391]	Time  0.246 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.4382e-01 (2.5163e-01)	Acc@1  91.41 ( 91.29)	Acc@5 100.00 ( 99.80)
Epoch: [27][390/391]	Time  0.176 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.7114e-01 (2.5156e-01)	Acc@1  87.50 ( 91.29)	Acc@5 100.00 ( 99.80)
## e[27] optimizer.zero_grad (sum) time: 0.47739362716674805
## e[27]       loss.backward (sum) time: 10.798792362213135
## e[27]      optimizer.step (sum) time: 47.59747004508972
## epoch[27] training(only) time: 94.29492139816284
# Switched to evaluate mode...
Test: [  0/100]	Time  0.214 ( 0.214)	Loss 3.2299e-01 (3.2299e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 3.0368e-01 (3.6316e-01)	Acc@1  90.00 ( 87.91)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.079 ( 0.084)	Loss 4.2036e-01 (3.8291e-01)	Acc@1  87.00 ( 87.43)	Acc@5  99.00 ( 99.76)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 2.7221e-01 (3.8139e-01)	Acc@1  91.00 ( 87.65)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 3.8758e-01 (3.8423e-01)	Acc@1  88.00 ( 87.68)	Acc@5 100.00 ( 99.61)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 3.0080e-01 (3.8295e-01)	Acc@1  88.00 ( 87.84)	Acc@5 100.00 ( 99.55)
Test: [ 60/100]	Time  0.079 ( 0.080)	Loss 3.4189e-01 (3.8007e-01)	Acc@1  92.00 ( 88.00)	Acc@5 100.00 ( 99.61)
Test: [ 70/100]	Time  0.077 ( 0.080)	Loss 2.5910e-01 (3.8017e-01)	Acc@1  91.00 ( 88.06)	Acc@5 100.00 ( 99.63)
Test: [ 80/100]	Time  0.079 ( 0.080)	Loss 2.7123e-01 (3.8275e-01)	Acc@1  92.00 ( 88.04)	Acc@5  99.00 ( 99.62)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.9245e-01 (3.8033e-01)	Acc@1  94.00 ( 88.10)	Acc@5 100.00 ( 99.62)
 * Acc@1 88.130 Acc@5 99.650
### epoch[27] execution time: 102.31776404380798
EPOCH 28
# current learning rate: 0.1
# Switched to train mode...
Epoch: [28][  0/391]	Time  0.392 ( 0.392)	Data  0.149 ( 0.149)	Loss 1.8318e-01 (1.8318e-01)	Acc@1  92.97 ( 92.97)	Acc@5 100.00 (100.00)
Epoch: [28][ 10/391]	Time  0.240 ( 0.254)	Data  0.001 ( 0.015)	Loss 1.8849e-01 (2.1725e-01)	Acc@1  94.53 ( 92.90)	Acc@5 100.00 ( 99.86)
Epoch: [28][ 20/391]	Time  0.244 ( 0.248)	Data  0.001 ( 0.008)	Loss 1.7747e-01 (2.1853e-01)	Acc@1  91.41 ( 92.93)	Acc@5 100.00 ( 99.85)
Epoch: [28][ 30/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.006)	Loss 2.0807e-01 (2.2043e-01)	Acc@1  93.75 ( 92.79)	Acc@5 100.00 ( 99.80)
Epoch: [28][ 40/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.005)	Loss 1.8339e-01 (2.1711e-01)	Acc@1  94.53 ( 92.91)	Acc@5 100.00 ( 99.81)
Epoch: [28][ 50/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.004)	Loss 3.0738e-01 (2.1908e-01)	Acc@1  90.62 ( 92.77)	Acc@5 100.00 ( 99.82)
Epoch: [28][ 60/391]	Time  0.239 ( 0.243)	Data  0.001 ( 0.004)	Loss 2.4435e-01 (2.1876e-01)	Acc@1  89.06 ( 92.78)	Acc@5 100.00 ( 99.82)
Epoch: [28][ 70/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.9377e-01 (2.2097e-01)	Acc@1  92.19 ( 92.58)	Acc@5 100.00 ( 99.82)
Epoch: [28][ 80/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 2.1676e-01 (2.2197e-01)	Acc@1  92.19 ( 92.53)	Acc@5 100.00 ( 99.84)
Epoch: [28][ 90/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.003)	Loss 2.7754e-01 (2.2483e-01)	Acc@1  89.84 ( 92.41)	Acc@5  98.44 ( 99.82)
Epoch: [28][100/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.8389e-01 (2.2308e-01)	Acc@1  93.75 ( 92.47)	Acc@5 100.00 ( 99.82)
Epoch: [28][110/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.8323e-01 (2.2151e-01)	Acc@1  94.53 ( 92.50)	Acc@5 100.00 ( 99.81)
Epoch: [28][120/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0575e-01 (2.2105e-01)	Acc@1  93.75 ( 92.52)	Acc@5 100.00 ( 99.82)
Epoch: [28][130/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5513e-01 (2.1977e-01)	Acc@1  94.53 ( 92.58)	Acc@5 100.00 ( 99.83)
Epoch: [28][140/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.9410e-01 (2.1766e-01)	Acc@1  91.41 ( 92.64)	Acc@5 100.00 ( 99.83)
Epoch: [28][150/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.2722e-01 (2.1880e-01)	Acc@1  89.06 ( 92.60)	Acc@5  99.22 ( 99.83)
Epoch: [28][160/391]	Time  0.243 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.8006e-01 (2.2014e-01)	Acc@1  92.97 ( 92.54)	Acc@5 100.00 ( 99.83)
Epoch: [28][170/391]	Time  0.244 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.8218e-01 (2.2245e-01)	Acc@1  89.84 ( 92.47)	Acc@5 100.00 ( 99.83)
Epoch: [28][180/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.3755e-01 (2.2398e-01)	Acc@1  91.41 ( 92.41)	Acc@5 100.00 ( 99.83)
Epoch: [28][190/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.7205e-01 (2.2500e-01)	Acc@1  90.62 ( 92.37)	Acc@5 100.00 ( 99.82)
Epoch: [28][200/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.4814e-01 (2.2720e-01)	Acc@1  89.06 ( 92.27)	Acc@5 100.00 ( 99.83)
Epoch: [28][210/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.2833e-01 (2.2901e-01)	Acc@1  89.06 ( 92.19)	Acc@5 100.00 ( 99.81)
Epoch: [28][220/391]	Time  0.244 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.5419e-01 (2.3024e-01)	Acc@1  93.75 ( 92.13)	Acc@5 100.00 ( 99.81)
Epoch: [28][230/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.6617e-01 (2.3221e-01)	Acc@1  96.09 ( 92.07)	Acc@5 100.00 ( 99.81)
Epoch: [28][240/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.5935e-01 (2.3377e-01)	Acc@1  88.28 ( 92.01)	Acc@5 100.00 ( 99.82)
Epoch: [28][250/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.2071e-01 (2.3331e-01)	Acc@1  87.50 ( 92.02)	Acc@5 100.00 ( 99.83)
Epoch: [28][260/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.7111e-01 (2.3295e-01)	Acc@1  91.41 ( 92.01)	Acc@5  99.22 ( 99.82)
Epoch: [28][270/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.6068e-01 (2.3317e-01)	Acc@1  93.75 ( 92.01)	Acc@5 100.00 ( 99.83)
Epoch: [28][280/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.0338e-01 (2.3387e-01)	Acc@1  92.19 ( 91.99)	Acc@5 100.00 ( 99.82)
Epoch: [28][290/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.6414e-01 (2.3387e-01)	Acc@1  89.84 ( 91.98)	Acc@5 100.00 ( 99.82)
Epoch: [28][300/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.4131e-01 (2.3405e-01)	Acc@1  92.97 ( 91.95)	Acc@5  99.22 ( 99.83)
Epoch: [28][310/391]	Time  0.247 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.5246e-01 (2.3438e-01)	Acc@1  91.41 ( 91.95)	Acc@5 100.00 ( 99.82)
Epoch: [28][320/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.0118e-01 (2.3470e-01)	Acc@1  92.19 ( 91.92)	Acc@5 100.00 ( 99.82)
Epoch: [28][330/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.6871e-01 (2.3514e-01)	Acc@1  89.84 ( 91.92)	Acc@5  99.22 ( 99.83)
Epoch: [28][340/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.2155e-01 (2.3527e-01)	Acc@1  94.53 ( 91.92)	Acc@5 100.00 ( 99.82)
Epoch: [28][350/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.8487e-01 (2.3527e-01)	Acc@1  92.19 ( 91.94)	Acc@5 100.00 ( 99.82)
Epoch: [28][360/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.1274e-01 (2.3544e-01)	Acc@1  94.53 ( 91.93)	Acc@5 100.00 ( 99.82)
Epoch: [28][370/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.4985e-01 (2.3535e-01)	Acc@1  94.53 ( 91.95)	Acc@5 100.00 ( 99.82)
Epoch: [28][380/391]	Time  0.244 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.7688e-01 (2.3619e-01)	Acc@1  93.75 ( 91.92)	Acc@5 100.00 ( 99.82)
Epoch: [28][390/391]	Time  0.177 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.2760e-01 (2.3731e-01)	Acc@1  92.50 ( 91.88)	Acc@5 100.00 ( 99.82)
## e[28] optimizer.zero_grad (sum) time: 0.4753146171569824
## e[28]       loss.backward (sum) time: 10.896326541900635
## e[28]      optimizer.step (sum) time: 47.49678182601929
## epoch[28] training(only) time: 94.30702114105225
# Switched to evaluate mode...
Test: [  0/100]	Time  0.210 ( 0.210)	Loss 4.0587e-01 (4.0587e-01)	Acc@1  88.00 ( 88.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 2.8503e-01 (4.2937e-01)	Acc@1  90.00 ( 86.36)	Acc@5 100.00 ( 99.09)
Test: [ 20/100]	Time  0.077 ( 0.084)	Loss 3.5179e-01 (4.4530e-01)	Acc@1  91.00 ( 86.14)	Acc@5  99.00 ( 99.14)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 3.6281e-01 (4.5315e-01)	Acc@1  85.00 ( 86.19)	Acc@5 100.00 ( 99.03)
Test: [ 40/100]	Time  0.077 ( 0.081)	Loss 5.3789e-01 (4.4337e-01)	Acc@1  87.00 ( 86.54)	Acc@5  98.00 ( 99.07)
Test: [ 50/100]	Time  0.077 ( 0.080)	Loss 4.4378e-01 (4.3744e-01)	Acc@1  87.00 ( 86.80)	Acc@5 100.00 ( 99.14)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 3.9602e-01 (4.3460e-01)	Acc@1  88.00 ( 86.75)	Acc@5  99.00 ( 99.18)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 6.2173e-01 (4.3572e-01)	Acc@1  83.00 ( 86.77)	Acc@5 100.00 ( 99.18)
Test: [ 80/100]	Time  0.077 ( 0.079)	Loss 3.1929e-01 (4.3381e-01)	Acc@1  92.00 ( 86.81)	Acc@5 100.00 ( 99.22)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 2.3761e-01 (4.3401e-01)	Acc@1  94.00 ( 86.84)	Acc@5 100.00 ( 99.22)
 * Acc@1 86.890 Acc@5 99.260
### epoch[28] execution time: 102.29505395889282
EPOCH 29
# current learning rate: 0.1
# Switched to train mode...
Epoch: [29][  0/391]	Time  0.396 ( 0.396)	Data  0.150 ( 0.150)	Loss 3.4835e-01 (3.4835e-01)	Acc@1  87.50 ( 87.50)	Acc@5 100.00 (100.00)
Epoch: [29][ 10/391]	Time  0.241 ( 0.254)	Data  0.001 ( 0.015)	Loss 2.5137e-01 (2.2781e-01)	Acc@1  89.06 ( 92.05)	Acc@5 100.00 ( 99.79)
Epoch: [29][ 20/391]	Time  0.241 ( 0.248)	Data  0.001 ( 0.008)	Loss 2.0352e-01 (2.1784e-01)	Acc@1  92.19 ( 92.60)	Acc@5 100.00 ( 99.85)
Epoch: [29][ 30/391]	Time  0.240 ( 0.246)	Data  0.001 ( 0.006)	Loss 2.5090e-01 (2.2282e-01)	Acc@1  89.06 ( 92.31)	Acc@5 100.00 ( 99.77)
Epoch: [29][ 40/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.005)	Loss 2.3434e-01 (2.2099e-01)	Acc@1  92.19 ( 92.66)	Acc@5 100.00 ( 99.81)
Epoch: [29][ 50/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 2.2004e-01 (2.1868e-01)	Acc@1  92.97 ( 92.65)	Acc@5 100.00 ( 99.85)
Epoch: [29][ 60/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.004)	Loss 3.4502e-01 (2.2026e-01)	Acc@1  90.62 ( 92.53)	Acc@5  99.22 ( 99.85)
Epoch: [29][ 70/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.3853e-01 (2.2353e-01)	Acc@1  92.19 ( 92.42)	Acc@5 100.00 ( 99.85)
Epoch: [29][ 80/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.4940e-01 (2.2265e-01)	Acc@1  90.62 ( 92.45)	Acc@5 100.00 ( 99.86)
Epoch: [29][ 90/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 2.5509e-01 (2.2436e-01)	Acc@1  93.75 ( 92.47)	Acc@5 100.00 ( 99.85)
Epoch: [29][100/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.7284e-01 (2.2325e-01)	Acc@1  92.97 ( 92.41)	Acc@5 100.00 ( 99.84)
Epoch: [29][110/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0040e-01 (2.2263e-01)	Acc@1  90.62 ( 92.40)	Acc@5 100.00 ( 99.85)
Epoch: [29][120/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1377e-01 (2.2188e-01)	Acc@1  93.75 ( 92.45)	Acc@5 100.00 ( 99.85)
Epoch: [29][130/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.0875e-01 (2.2437e-01)	Acc@1  89.06 ( 92.36)	Acc@5 100.00 ( 99.84)
Epoch: [29][140/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.4968e-01 (2.2343e-01)	Acc@1  94.53 ( 92.41)	Acc@5 100.00 ( 99.86)
Epoch: [29][150/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2050e-01 (2.2464e-01)	Acc@1  93.75 ( 92.37)	Acc@5 100.00 ( 99.86)
Epoch: [29][160/391]	Time  0.247 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.7183e-01 (2.2568e-01)	Acc@1  89.84 ( 92.30)	Acc@5 100.00 ( 99.85)
Epoch: [29][170/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.3341e-01 (2.2671e-01)	Acc@1  90.62 ( 92.22)	Acc@5 100.00 ( 99.85)
Epoch: [29][180/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9840e-01 (2.2690e-01)	Acc@1  92.19 ( 92.20)	Acc@5 100.00 ( 99.86)
Epoch: [29][190/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.5294e-01 (2.2892e-01)	Acc@1  90.62 ( 92.13)	Acc@5 100.00 ( 99.87)
Epoch: [29][200/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2769e-01 (2.2865e-01)	Acc@1  92.97 ( 92.11)	Acc@5 100.00 ( 99.87)
Epoch: [29][210/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.8439e-01 (2.2959e-01)	Acc@1  91.41 ( 92.04)	Acc@5 100.00 ( 99.87)
Epoch: [29][220/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.8759e-01 (2.2983e-01)	Acc@1  89.84 ( 92.01)	Acc@5 100.00 ( 99.87)
Epoch: [29][230/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.7494e-01 (2.2970e-01)	Acc@1  92.19 ( 91.99)	Acc@5 100.00 ( 99.87)
Epoch: [29][240/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.2444e-01 (2.3034e-01)	Acc@1  91.41 ( 91.96)	Acc@5 100.00 ( 99.86)
Epoch: [29][250/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.9189e-01 (2.3065e-01)	Acc@1  92.97 ( 91.97)	Acc@5 100.00 ( 99.87)
Epoch: [29][260/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.6201e-01 (2.3118e-01)	Acc@1  89.06 ( 91.94)	Acc@5 100.00 ( 99.87)
Epoch: [29][270/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.4181e-01 (2.3166e-01)	Acc@1  95.31 ( 91.94)	Acc@5  99.22 ( 99.86)
Epoch: [29][280/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1643e-01 (2.3016e-01)	Acc@1  96.09 ( 92.01)	Acc@5  99.22 ( 99.86)
Epoch: [29][290/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.9545e-01 (2.3041e-01)	Acc@1  92.97 ( 91.98)	Acc@5  99.22 ( 99.86)
Epoch: [29][300/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.6411e-01 (2.3035e-01)	Acc@1  89.06 ( 91.99)	Acc@5  99.22 ( 99.86)
Epoch: [29][310/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.6507e-01 (2.3087e-01)	Acc@1  88.28 ( 91.96)	Acc@5  99.22 ( 99.84)
Epoch: [29][320/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.7102e-01 (2.3064e-01)	Acc@1  92.97 ( 91.97)	Acc@5 100.00 ( 99.84)
Epoch: [29][330/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.6431e-01 (2.3152e-01)	Acc@1  89.84 ( 91.93)	Acc@5 100.00 ( 99.83)
Epoch: [29][340/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.3582e-01 (2.3183e-01)	Acc@1  91.41 ( 91.92)	Acc@5 100.00 ( 99.83)
Epoch: [29][350/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.3280e-01 (2.3153e-01)	Acc@1  92.97 ( 91.93)	Acc@5  99.22 ( 99.83)
Epoch: [29][360/391]	Time  0.246 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.2366e-01 (2.3063e-01)	Acc@1  92.97 ( 91.96)	Acc@5 100.00 ( 99.82)
Epoch: [29][370/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.8818e-01 (2.3077e-01)	Acc@1  92.97 ( 91.97)	Acc@5 100.00 ( 99.83)
Epoch: [29][380/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.9570e-01 (2.3032e-01)	Acc@1  85.94 ( 91.97)	Acc@5 100.00 ( 99.83)
Epoch: [29][390/391]	Time  0.177 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.4326e-01 (2.3044e-01)	Acc@1  87.50 ( 91.96)	Acc@5  98.75 ( 99.82)
## e[29] optimizer.zero_grad (sum) time: 0.4771416187286377
## e[29]       loss.backward (sum) time: 10.841206789016724
## e[29]      optimizer.step (sum) time: 47.57026386260986
## epoch[29] training(only) time: 94.3302149772644
# Switched to evaluate mode...
Test: [  0/100]	Time  0.205 ( 0.205)	Loss 3.0773e-01 (3.0773e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.078 ( 0.089)	Loss 2.4277e-01 (3.5784e-01)	Acc@1  92.00 ( 88.55)	Acc@5 100.00 ( 99.55)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 5.1061e-01 (3.8754e-01)	Acc@1  84.00 ( 88.19)	Acc@5  99.00 ( 99.38)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 4.5290e-01 (3.9854e-01)	Acc@1  83.00 ( 87.71)	Acc@5 100.00 ( 99.48)
Test: [ 40/100]	Time  0.077 ( 0.081)	Loss 3.4163e-01 (4.0011e-01)	Acc@1  88.00 ( 87.66)	Acc@5 100.00 ( 99.46)
Test: [ 50/100]	Time  0.079 ( 0.080)	Loss 3.2618e-01 (3.9495e-01)	Acc@1  90.00 ( 87.71)	Acc@5  99.00 ( 99.51)
Test: [ 60/100]	Time  0.077 ( 0.080)	Loss 3.9654e-01 (3.9452e-01)	Acc@1  87.00 ( 87.77)	Acc@5 100.00 ( 99.56)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 2.8168e-01 (3.9470e-01)	Acc@1  91.00 ( 87.80)	Acc@5 100.00 ( 99.58)
Test: [ 80/100]	Time  0.077 ( 0.080)	Loss 2.3283e-01 (3.8770e-01)	Acc@1  94.00 ( 88.02)	Acc@5 100.00 ( 99.59)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 2.5490e-01 (3.8687e-01)	Acc@1  92.00 ( 87.97)	Acc@5 100.00 ( 99.59)
 * Acc@1 87.890 Acc@5 99.610
### epoch[29] execution time: 102.3614342212677
EPOCH 30
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [30][  0/391]	Time  0.401 ( 0.401)	Data  0.153 ( 0.153)	Loss 2.0854e-01 (2.0854e-01)	Acc@1  91.41 ( 91.41)	Acc@5 100.00 (100.00)
Epoch: [30][ 10/391]	Time  0.240 ( 0.255)	Data  0.001 ( 0.015)	Loss 9.2942e-02 (1.7429e-01)	Acc@1  96.88 ( 94.32)	Acc@5 100.00 (100.00)
Epoch: [30][ 20/391]	Time  0.240 ( 0.248)	Data  0.001 ( 0.008)	Loss 1.6938e-01 (1.7491e-01)	Acc@1  95.31 ( 94.01)	Acc@5 100.00 ( 99.93)
Epoch: [30][ 30/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.006)	Loss 1.3107e-01 (1.6397e-01)	Acc@1  95.31 ( 94.51)	Acc@5 100.00 ( 99.95)
Epoch: [30][ 40/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.005)	Loss 1.6538e-01 (1.6050e-01)	Acc@1  94.53 ( 94.68)	Acc@5 100.00 ( 99.92)
Epoch: [30][ 50/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.3645e-01 (1.6068e-01)	Acc@1  95.31 ( 94.62)	Acc@5 100.00 ( 99.91)
Epoch: [30][ 60/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.004)	Loss 9.9227e-02 (1.5589e-01)	Acc@1  95.31 ( 94.85)	Acc@5 100.00 ( 99.90)
Epoch: [30][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 6.5450e-02 (1.5460e-01)	Acc@1  97.66 ( 94.86)	Acc@5 100.00 ( 99.89)
Epoch: [30][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.2022e-01 (1.5227e-01)	Acc@1  97.66 ( 94.92)	Acc@5 100.00 ( 99.90)
Epoch: [30][ 90/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.8731e-01 (1.4951e-01)	Acc@1  91.41 ( 94.95)	Acc@5 100.00 ( 99.90)
Epoch: [30][100/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.1546e-01 (1.4746e-01)	Acc@1  96.09 ( 95.00)	Acc@5 100.00 ( 99.91)
Epoch: [30][110/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.9390e-01 (1.4513e-01)	Acc@1  92.19 ( 95.07)	Acc@5 100.00 ( 99.91)
Epoch: [30][120/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.6317e-02 (1.4294e-01)	Acc@1  97.66 ( 95.15)	Acc@5  99.22 ( 99.90)
Epoch: [30][130/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0289e-01 (1.4076e-01)	Acc@1  96.88 ( 95.21)	Acc@5 100.00 ( 99.91)
Epoch: [30][140/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1219e-01 (1.4071e-01)	Acc@1  95.31 ( 95.23)	Acc@5 100.00 ( 99.91)
Epoch: [30][150/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.3599e-02 (1.3931e-01)	Acc@1  96.88 ( 95.27)	Acc@5 100.00 ( 99.91)
Epoch: [30][160/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.5593e-02 (1.3763e-01)	Acc@1  97.66 ( 95.34)	Acc@5 100.00 ( 99.91)
Epoch: [30][170/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6007e-01 (1.3697e-01)	Acc@1  96.09 ( 95.37)	Acc@5 100.00 ( 99.92)
Epoch: [30][180/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.3660e-02 (1.3713e-01)	Acc@1  95.31 ( 95.35)	Acc@5 100.00 ( 99.92)
Epoch: [30][190/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3728e-01 (1.3698e-01)	Acc@1  96.88 ( 95.34)	Acc@5 100.00 ( 99.92)
Epoch: [30][200/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1301e-01 (1.3641e-01)	Acc@1  96.88 ( 95.33)	Acc@5 100.00 ( 99.92)
Epoch: [30][210/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.0163e-02 (1.3464e-01)	Acc@1  97.66 ( 95.41)	Acc@5 100.00 ( 99.92)
Epoch: [30][220/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.4485e-02 (1.3349e-01)	Acc@1  96.88 ( 95.45)	Acc@5 100.00 ( 99.92)
Epoch: [30][230/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.4482e-01 (1.3255e-01)	Acc@1  96.09 ( 95.48)	Acc@5 100.00 ( 99.92)
Epoch: [30][240/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1675e-01 (1.3130e-01)	Acc@1  95.31 ( 95.53)	Acc@5 100.00 ( 99.93)
Epoch: [30][250/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2758e-01 (1.3037e-01)	Acc@1  96.09 ( 95.57)	Acc@5 100.00 ( 99.93)
Epoch: [30][260/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.3016e-01 (1.2993e-01)	Acc@1  96.88 ( 95.60)	Acc@5 100.00 ( 99.93)
Epoch: [30][270/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0583e-01 (1.2859e-01)	Acc@1  96.09 ( 95.66)	Acc@5 100.00 ( 99.93)
Epoch: [30][280/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.8860e-02 (1.2837e-01)	Acc@1  99.22 ( 95.67)	Acc@5 100.00 ( 99.93)
Epoch: [30][290/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.5620e-02 (1.2732e-01)	Acc@1  97.66 ( 95.70)	Acc@5 100.00 ( 99.93)
Epoch: [30][300/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.5726e-02 (1.2643e-01)	Acc@1  96.88 ( 95.73)	Acc@5 100.00 ( 99.94)
Epoch: [30][310/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.2591e-02 (1.2510e-01)	Acc@1  98.44 ( 95.78)	Acc@5 100.00 ( 99.93)
Epoch: [30][320/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1708e-01 (1.2463e-01)	Acc@1  95.31 ( 95.79)	Acc@5 100.00 ( 99.94)
Epoch: [30][330/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0880e-01 (1.2328e-01)	Acc@1  95.31 ( 95.83)	Acc@5 100.00 ( 99.94)
Epoch: [30][340/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.4461e-02 (1.2259e-01)	Acc@1  97.66 ( 95.85)	Acc@5 100.00 ( 99.94)
Epoch: [30][350/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.2741e-02 (1.2228e-01)	Acc@1  97.66 ( 95.86)	Acc@5 100.00 ( 99.94)
Epoch: [30][360/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1578e-01 (1.2170e-01)	Acc@1  95.31 ( 95.88)	Acc@5 100.00 ( 99.94)
Epoch: [30][370/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.9010e-01 (1.2117e-01)	Acc@1  92.19 ( 95.90)	Acc@5 100.00 ( 99.94)
Epoch: [30][380/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.5115e-01 (1.2068e-01)	Acc@1  95.31 ( 95.91)	Acc@5  99.22 ( 99.94)
Epoch: [30][390/391]	Time  0.176 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.4143e-01 (1.2040e-01)	Acc@1  95.00 ( 95.93)	Acc@5 100.00 ( 99.94)
## e[30] optimizer.zero_grad (sum) time: 0.47437047958374023
## e[30]       loss.backward (sum) time: 10.821176290512085
## e[30]      optimizer.step (sum) time: 47.54441213607788
## epoch[30] training(only) time: 94.37186861038208
# Switched to evaluate mode...
Test: [  0/100]	Time  0.210 ( 0.210)	Loss 2.5700e-01 (2.5700e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.077 ( 0.090)	Loss 2.2690e-01 (2.5700e-01)	Acc@1  93.00 ( 91.82)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.079 ( 0.084)	Loss 3.4239e-01 (2.6810e-01)	Acc@1  90.00 ( 91.57)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.077 ( 0.082)	Loss 2.7651e-01 (2.7376e-01)	Acc@1  91.00 ( 91.68)	Acc@5 100.00 ( 99.87)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 3.0342e-01 (2.7264e-01)	Acc@1  91.00 ( 91.71)	Acc@5  99.00 ( 99.85)
Test: [ 50/100]	Time  0.077 ( 0.080)	Loss 2.4658e-01 (2.7613e-01)	Acc@1  93.00 ( 91.67)	Acc@5 100.00 ( 99.88)
Test: [ 60/100]	Time  0.079 ( 0.080)	Loss 2.9287e-01 (2.7166e-01)	Acc@1  92.00 ( 91.77)	Acc@5 100.00 ( 99.87)
Test: [ 70/100]	Time  0.079 ( 0.080)	Loss 3.1526e-01 (2.6991e-01)	Acc@1  91.00 ( 91.80)	Acc@5 100.00 ( 99.85)
Test: [ 80/100]	Time  0.078 ( 0.079)	Loss 2.4679e-01 (2.6768e-01)	Acc@1  93.00 ( 91.90)	Acc@5 100.00 ( 99.84)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.6438e-01 (2.6584e-01)	Acc@1  93.00 ( 91.93)	Acc@5 100.00 ( 99.81)
 * Acc@1 91.900 Acc@5 99.820
### epoch[30] execution time: 102.37286329269409
EPOCH 31
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [31][  0/391]	Time  0.400 ( 0.400)	Data  0.154 ( 0.154)	Loss 8.0480e-02 (8.0480e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [31][ 10/391]	Time  0.241 ( 0.255)	Data  0.001 ( 0.015)	Loss 4.2223e-02 (7.9105e-02)	Acc@1 100.00 ( 97.37)	Acc@5 100.00 ( 99.86)
Epoch: [31][ 20/391]	Time  0.239 ( 0.248)	Data  0.001 ( 0.008)	Loss 8.6357e-02 (7.9351e-02)	Acc@1  96.09 ( 97.17)	Acc@5 100.00 ( 99.93)
Epoch: [31][ 30/391]	Time  0.240 ( 0.246)	Data  0.001 ( 0.006)	Loss 1.1068e-01 (8.5523e-02)	Acc@1  94.53 ( 96.98)	Acc@5 100.00 ( 99.95)
Epoch: [31][ 40/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.005)	Loss 9.9983e-02 (8.4754e-02)	Acc@1  96.09 ( 97.01)	Acc@5 100.00 ( 99.96)
Epoch: [31][ 50/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 7.7620e-02 (8.1106e-02)	Acc@1  96.88 ( 97.18)	Acc@5 100.00 ( 99.97)
Epoch: [31][ 60/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.004)	Loss 4.8925e-02 (8.1043e-02)	Acc@1  98.44 ( 97.18)	Acc@5 100.00 ( 99.97)
Epoch: [31][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 8.5743e-02 (8.3675e-02)	Acc@1  96.09 ( 97.13)	Acc@5 100.00 ( 99.97)
Epoch: [31][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.4136e-01 (8.5660e-02)	Acc@1  94.53 ( 97.06)	Acc@5 100.00 ( 99.96)
Epoch: [31][ 90/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 7.3123e-02 (8.6857e-02)	Acc@1  96.88 ( 97.01)	Acc@5 100.00 ( 99.97)
Epoch: [31][100/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 8.7666e-02 (8.7200e-02)	Acc@1  97.66 ( 97.03)	Acc@5 100.00 ( 99.97)
Epoch: [31][110/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 9.9829e-02 (8.7023e-02)	Acc@1  96.09 ( 97.04)	Acc@5 100.00 ( 99.97)
Epoch: [31][120/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.8407e-02 (8.6458e-02)	Acc@1  98.44 ( 97.08)	Acc@5 100.00 ( 99.97)
Epoch: [31][130/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.0557e-02 (8.5899e-02)	Acc@1  97.66 ( 97.11)	Acc@5 100.00 ( 99.98)
Epoch: [31][140/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0273e-01 (8.6513e-02)	Acc@1  96.88 ( 97.09)	Acc@5 100.00 ( 99.97)
Epoch: [31][150/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.5179e-02 (8.7489e-02)	Acc@1  97.66 ( 97.01)	Acc@5 100.00 ( 99.97)
Epoch: [31][160/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.3943e-02 (8.7502e-02)	Acc@1  96.88 ( 97.03)	Acc@5 100.00 ( 99.98)
Epoch: [31][170/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.3349e-02 (8.7465e-02)	Acc@1  99.22 ( 97.02)	Acc@5 100.00 ( 99.98)
Epoch: [31][180/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.7872e-02 (8.7051e-02)	Acc@1  96.09 ( 97.03)	Acc@5 100.00 ( 99.98)
Epoch: [31][190/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.6306e-02 (8.6731e-02)	Acc@1  99.22 ( 97.05)	Acc@5 100.00 ( 99.98)
Epoch: [31][200/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.4912e-02 (8.6844e-02)	Acc@1  99.22 ( 97.05)	Acc@5 100.00 ( 99.97)
Epoch: [31][210/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.0966e-02 (8.8347e-02)	Acc@1  96.88 ( 96.99)	Acc@5 100.00 ( 99.97)
Epoch: [31][220/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.8000e-02 (8.8057e-02)	Acc@1  96.88 ( 97.00)	Acc@5 100.00 ( 99.98)
Epoch: [31][230/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.7741e-02 (8.8625e-02)	Acc@1  99.22 ( 96.99)	Acc@5 100.00 ( 99.97)
Epoch: [31][240/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.6801e-02 (8.8011e-02)	Acc@1  97.66 ( 97.01)	Acc@5 100.00 ( 99.97)
Epoch: [31][250/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.3747e-02 (8.8055e-02)	Acc@1  98.44 ( 97.02)	Acc@5 100.00 ( 99.97)
Epoch: [31][260/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.7313e-02 (8.8173e-02)	Acc@1  98.44 ( 97.03)	Acc@5 100.00 ( 99.97)
Epoch: [31][270/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0634e-01 (8.8304e-02)	Acc@1  96.88 ( 97.03)	Acc@5 100.00 ( 99.97)
Epoch: [31][280/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.2496e-02 (8.8145e-02)	Acc@1  98.44 ( 97.06)	Acc@5 100.00 ( 99.97)
Epoch: [31][290/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.7194e-02 (8.7672e-02)	Acc@1  99.22 ( 97.08)	Acc@5 100.00 ( 99.97)
Epoch: [31][300/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.3632e-01 (8.7285e-02)	Acc@1  94.53 ( 97.09)	Acc@5 100.00 ( 99.97)
Epoch: [31][310/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2015e-01 (8.6871e-02)	Acc@1  94.53 ( 97.10)	Acc@5 100.00 ( 99.97)
Epoch: [31][320/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.1650e-02 (8.6527e-02)	Acc@1  99.22 ( 97.13)	Acc@5 100.00 ( 99.97)
Epoch: [31][330/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.3470e-02 (8.5788e-02)	Acc@1  98.44 ( 97.15)	Acc@5 100.00 ( 99.97)
Epoch: [31][340/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.7006e-02 (8.5987e-02)	Acc@1  96.88 ( 97.14)	Acc@5 100.00 ( 99.97)
Epoch: [31][350/391]	Time  0.244 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.4446e-01 (8.5720e-02)	Acc@1  92.97 ( 97.14)	Acc@5 100.00 ( 99.98)
Epoch: [31][360/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1878e-01 (8.5997e-02)	Acc@1  96.88 ( 97.13)	Acc@5  99.22 ( 99.97)
Epoch: [31][370/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.3357e-02 (8.5972e-02)	Acc@1  97.66 ( 97.13)	Acc@5 100.00 ( 99.97)
Epoch: [31][380/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.7190e-02 (8.5746e-02)	Acc@1  99.22 ( 97.15)	Acc@5 100.00 ( 99.97)
Epoch: [31][390/391]	Time  0.177 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2585e-01 (8.5478e-02)	Acc@1  93.75 ( 97.15)	Acc@5 100.00 ( 99.97)
## e[31] optimizer.zero_grad (sum) time: 0.4750971794128418
## e[31]       loss.backward (sum) time: 10.861786127090454
## e[31]      optimizer.step (sum) time: 47.506258964538574
## epoch[31] training(only) time: 94.2732765674591
# Switched to evaluate mode...
Test: [  0/100]	Time  0.217 ( 0.217)	Loss 2.5279e-01 (2.5279e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.079 ( 0.090)	Loss 2.2808e-01 (2.6065e-01)	Acc@1  93.00 ( 92.55)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.077 ( 0.084)	Loss 3.0485e-01 (2.6552e-01)	Acc@1  90.00 ( 92.33)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 2.7002e-01 (2.7332e-01)	Acc@1  88.00 ( 92.06)	Acc@5 100.00 ( 99.84)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 2.7703e-01 (2.6819e-01)	Acc@1  93.00 ( 92.20)	Acc@5 100.00 ( 99.85)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 2.4663e-01 (2.6888e-01)	Acc@1  93.00 ( 92.25)	Acc@5 100.00 ( 99.88)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 3.3167e-01 (2.6381e-01)	Acc@1  92.00 ( 92.30)	Acc@5 100.00 ( 99.87)
Test: [ 70/100]	Time  0.077 ( 0.080)	Loss 3.3873e-01 (2.6206e-01)	Acc@1  90.00 ( 92.31)	Acc@5 100.00 ( 99.86)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 2.4114e-01 (2.5926e-01)	Acc@1  94.00 ( 92.33)	Acc@5 100.00 ( 99.84)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.8371e-01 (2.5824e-01)	Acc@1  94.00 ( 92.32)	Acc@5 100.00 ( 99.82)
 * Acc@1 92.260 Acc@5 99.830
### epoch[31] execution time: 102.29171180725098
EPOCH 32
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [32][  0/391]	Time  0.390 ( 0.390)	Data  0.149 ( 0.149)	Loss 3.7917e-02 (3.7917e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [32][ 10/391]	Time  0.240 ( 0.254)	Data  0.001 ( 0.015)	Loss 5.2443e-02 (5.9161e-02)	Acc@1  98.44 ( 98.01)	Acc@5 100.00 (100.00)
Epoch: [32][ 20/391]	Time  0.241 ( 0.248)	Data  0.001 ( 0.008)	Loss 6.4831e-02 (7.2419e-02)	Acc@1  97.66 ( 97.69)	Acc@5 100.00 (100.00)
Epoch: [32][ 30/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.006)	Loss 3.3581e-02 (6.9726e-02)	Acc@1  98.44 ( 97.71)	Acc@5 100.00 (100.00)
Epoch: [32][ 40/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.005)	Loss 3.8064e-02 (7.1759e-02)	Acc@1  99.22 ( 97.62)	Acc@5 100.00 (100.00)
Epoch: [32][ 50/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.004)	Loss 6.5870e-02 (6.9535e-02)	Acc@1  98.44 ( 97.72)	Acc@5 100.00 ( 99.97)
Epoch: [32][ 60/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.004)	Loss 6.9816e-02 (7.1958e-02)	Acc@1  96.88 ( 97.69)	Acc@5 100.00 ( 99.97)
Epoch: [32][ 70/391]	Time  0.238 ( 0.243)	Data  0.001 ( 0.003)	Loss 5.2369e-02 (7.1209e-02)	Acc@1  99.22 ( 97.79)	Acc@5 100.00 ( 99.98)
Epoch: [32][ 80/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.2793e-01 (7.0364e-02)	Acc@1  93.75 ( 97.74)	Acc@5 100.00 ( 99.98)
Epoch: [32][ 90/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 4.1669e-02 (7.0650e-02)	Acc@1  99.22 ( 97.72)	Acc@5 100.00 ( 99.97)
Epoch: [32][100/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.0298e-01 (7.3032e-02)	Acc@1  96.88 ( 97.66)	Acc@5 100.00 ( 99.98)
Epoch: [32][110/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 6.2236e-02 (7.1321e-02)	Acc@1  98.44 ( 97.72)	Acc@5 100.00 ( 99.98)
Epoch: [32][120/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.4951e-02 (7.1363e-02)	Acc@1  97.66 ( 97.71)	Acc@5 100.00 ( 99.97)
Epoch: [32][130/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.1023e-02 (7.1209e-02)	Acc@1  96.09 ( 97.73)	Acc@5 100.00 ( 99.98)
Epoch: [32][140/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.3030e-02 (7.0343e-02)	Acc@1  98.44 ( 97.73)	Acc@5 100.00 ( 99.98)
Epoch: [32][150/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.7077e-02 (7.0442e-02)	Acc@1  98.44 ( 97.72)	Acc@5 100.00 ( 99.98)
Epoch: [32][160/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0097e-01 (7.1571e-02)	Acc@1  96.09 ( 97.68)	Acc@5 100.00 ( 99.98)
Epoch: [32][170/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.3580e-02 (7.1612e-02)	Acc@1  96.09 ( 97.67)	Acc@5 100.00 ( 99.98)
Epoch: [32][180/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.5414e-02 (7.1186e-02)	Acc@1  99.22 ( 97.67)	Acc@5 100.00 ( 99.98)
Epoch: [32][190/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.7480e-02 (7.1290e-02)	Acc@1  99.22 ( 97.68)	Acc@5 100.00 ( 99.98)
Epoch: [32][200/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.7194e-02 (7.1989e-02)	Acc@1  98.44 ( 97.65)	Acc@5 100.00 ( 99.98)
Epoch: [32][210/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.6575e-02 (7.1367e-02)	Acc@1  98.44 ( 97.67)	Acc@5  99.22 ( 99.98)
Epoch: [32][220/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.5914e-02 (7.1087e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 ( 99.98)
Epoch: [32][230/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.6860e-01 (7.1303e-02)	Acc@1  95.31 ( 97.65)	Acc@5  99.22 ( 99.98)
Epoch: [32][240/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.4752e-02 (7.1167e-02)	Acc@1  99.22 ( 97.65)	Acc@5 100.00 ( 99.97)
Epoch: [32][250/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.3913e-02 (7.0948e-02)	Acc@1  99.22 ( 97.67)	Acc@5 100.00 ( 99.97)
Epoch: [32][260/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.6343e-02 (7.1208e-02)	Acc@1  98.44 ( 97.66)	Acc@5 100.00 ( 99.97)
Epoch: [32][270/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.7539e-02 (7.1203e-02)	Acc@1  98.44 ( 97.66)	Acc@5 100.00 ( 99.97)
Epoch: [32][280/391]	Time  0.248 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.1009e-02 (7.1203e-02)	Acc@1  97.66 ( 97.67)	Acc@5 100.00 ( 99.97)
Epoch: [32][290/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.2790e-02 (7.0901e-02)	Acc@1  98.44 ( 97.68)	Acc@5 100.00 ( 99.98)
Epoch: [32][300/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.1806e-02 (7.0476e-02)	Acc@1  96.88 ( 97.70)	Acc@5 100.00 ( 99.98)
Epoch: [32][310/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.2505e-02 (7.0310e-02)	Acc@1  98.44 ( 97.69)	Acc@5 100.00 ( 99.98)
Epoch: [32][320/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.6202e-02 (7.0058e-02)	Acc@1  97.66 ( 97.71)	Acc@5 100.00 ( 99.98)
Epoch: [32][330/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.4128e-02 (6.9930e-02)	Acc@1  99.22 ( 97.70)	Acc@5 100.00 ( 99.98)
Epoch: [32][340/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.4439e-02 (6.9497e-02)	Acc@1  98.44 ( 97.72)	Acc@5 100.00 ( 99.98)
Epoch: [32][350/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2263e-01 (7.0237e-02)	Acc@1  96.88 ( 97.69)	Acc@5 100.00 ( 99.98)
Epoch: [32][360/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1387e-01 (7.0136e-02)	Acc@1  96.09 ( 97.69)	Acc@5 100.00 ( 99.98)
Epoch: [32][370/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.6401e-02 (6.9739e-02)	Acc@1  99.22 ( 97.69)	Acc@5 100.00 ( 99.98)
Epoch: [32][380/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.2188e-02 (6.9924e-02)	Acc@1 100.00 ( 97.69)	Acc@5 100.00 ( 99.98)
Epoch: [32][390/391]	Time  0.177 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.5952e-02 (6.9751e-02)	Acc@1  97.50 ( 97.68)	Acc@5 100.00 ( 99.98)
## e[32] optimizer.zero_grad (sum) time: 0.4778258800506592
## e[32]       loss.backward (sum) time: 10.855140447616577
## e[32]      optimizer.step (sum) time: 47.50747013092041
## epoch[32] training(only) time: 94.24199223518372
# Switched to evaluate mode...
Test: [  0/100]	Time  0.208 ( 0.208)	Loss 2.2881e-01 (2.2881e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.078 ( 0.089)	Loss 2.2869e-01 (2.6053e-01)	Acc@1  95.00 ( 92.73)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 3.2077e-01 (2.7396e-01)	Acc@1  92.00 ( 92.33)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 2.5134e-01 (2.8177e-01)	Acc@1  89.00 ( 92.23)	Acc@5 100.00 ( 99.77)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 2.6777e-01 (2.7802e-01)	Acc@1  93.00 ( 92.39)	Acc@5 100.00 ( 99.80)
Test: [ 50/100]	Time  0.078 ( 0.080)	Loss 2.5703e-01 (2.7714e-01)	Acc@1  93.00 ( 92.29)	Acc@5 100.00 ( 99.82)
Test: [ 60/100]	Time  0.077 ( 0.080)	Loss 3.5259e-01 (2.7174e-01)	Acc@1  91.00 ( 92.28)	Acc@5 100.00 ( 99.82)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 3.1362e-01 (2.6834e-01)	Acc@1  91.00 ( 92.31)	Acc@5 100.00 ( 99.82)
Test: [ 80/100]	Time  0.078 ( 0.079)	Loss 2.3058e-01 (2.6372e-01)	Acc@1  93.00 ( 92.35)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.5855e-01 (2.6392e-01)	Acc@1  92.00 ( 92.32)	Acc@5 100.00 ( 99.80)
 * Acc@1 92.260 Acc@5 99.810
### epoch[32] execution time: 102.2383542060852
EPOCH 33
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [33][  0/391]	Time  0.388 ( 0.388)	Data  0.145 ( 0.145)	Loss 4.1781e-02 (4.1781e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [33][ 10/391]	Time  0.240 ( 0.254)	Data  0.001 ( 0.014)	Loss 2.1135e-02 (4.7207e-02)	Acc@1 100.00 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [33][ 20/391]	Time  0.240 ( 0.248)	Data  0.001 ( 0.008)	Loss 6.1247e-02 (4.9268e-02)	Acc@1  99.22 ( 98.70)	Acc@5 100.00 (100.00)
Epoch: [33][ 30/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.006)	Loss 3.4179e-02 (5.0833e-02)	Acc@1 100.00 ( 98.54)	Acc@5 100.00 (100.00)
Epoch: [33][ 40/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.005)	Loss 3.2649e-02 (5.2579e-02)	Acc@1 100.00 ( 98.48)	Acc@5 100.00 (100.00)
Epoch: [33][ 50/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 7.2556e-02 (5.4525e-02)	Acc@1  96.88 ( 98.35)	Acc@5 100.00 (100.00)
Epoch: [33][ 60/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.004)	Loss 3.6572e-02 (5.3504e-02)	Acc@1  98.44 ( 98.34)	Acc@5 100.00 (100.00)
Epoch: [33][ 70/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 7.3692e-02 (5.4398e-02)	Acc@1  96.88 ( 98.31)	Acc@5 100.00 (100.00)
Epoch: [33][ 80/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.0772e-01 (5.6128e-02)	Acc@1  96.09 ( 98.22)	Acc@5 100.00 (100.00)
Epoch: [33][ 90/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 4.8146e-02 (5.8456e-02)	Acc@1  99.22 ( 98.10)	Acc@5 100.00 (100.00)
Epoch: [33][100/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 9.6950e-02 (5.7682e-02)	Acc@1  95.31 ( 98.15)	Acc@5 100.00 ( 99.99)
Epoch: [33][110/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.0535e-02 (5.7696e-02)	Acc@1  97.66 ( 98.16)	Acc@5 100.00 ( 99.99)
Epoch: [33][120/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0813e-01 (5.8394e-02)	Acc@1  96.88 ( 98.12)	Acc@5 100.00 ( 99.99)
Epoch: [33][130/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.1603e-02 (5.9271e-02)	Acc@1  98.44 ( 98.10)	Acc@5 100.00 ( 99.99)
Epoch: [33][140/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.3060e-02 (5.9083e-02)	Acc@1  99.22 ( 98.12)	Acc@5 100.00 ( 99.98)
Epoch: [33][150/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.0115e-02 (5.9180e-02)	Acc@1  98.44 ( 98.12)	Acc@5 100.00 ( 99.98)
Epoch: [33][160/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.5127e-02 (5.9068e-02)	Acc@1  97.66 ( 98.10)	Acc@5  99.22 ( 99.98)
Epoch: [33][170/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.3817e-02 (5.8940e-02)	Acc@1  96.09 ( 98.10)	Acc@5 100.00 ( 99.98)
Epoch: [33][180/391]	Time  0.243 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0583e-01 (5.8845e-02)	Acc@1  96.88 ( 98.11)	Acc@5 100.00 ( 99.98)
Epoch: [33][190/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.9301e-02 (5.9289e-02)	Acc@1  98.44 ( 98.09)	Acc@5 100.00 ( 99.98)
Epoch: [33][200/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.4139e-02 (5.9746e-02)	Acc@1  96.09 ( 98.06)	Acc@5 100.00 ( 99.98)
Epoch: [33][210/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.0684e-02 (6.0199e-02)	Acc@1  98.44 ( 98.03)	Acc@5 100.00 ( 99.98)
Epoch: [33][220/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.3216e-01 (6.1419e-02)	Acc@1  96.88 ( 98.02)	Acc@5  99.22 ( 99.98)
Epoch: [33][230/391]	Time  0.243 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.0764e-02 (6.1707e-02)	Acc@1  98.44 ( 98.01)	Acc@5 100.00 ( 99.98)
Epoch: [33][240/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.8949e-02 (6.1149e-02)	Acc@1  98.44 ( 98.01)	Acc@5 100.00 ( 99.98)
Epoch: [33][250/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.5773e-02 (6.0604e-02)	Acc@1  99.22 ( 98.03)	Acc@5 100.00 ( 99.98)
Epoch: [33][260/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.7600e-02 (6.0542e-02)	Acc@1  99.22 ( 98.02)	Acc@5 100.00 ( 99.98)
Epoch: [33][270/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.2774e-02 (6.0703e-02)	Acc@1  99.22 ( 98.01)	Acc@5 100.00 ( 99.98)
Epoch: [33][280/391]	Time  0.245 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.8378e-02 (6.1306e-02)	Acc@1  96.09 ( 97.98)	Acc@5 100.00 ( 99.98)
Epoch: [33][290/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.1368e-02 (6.1687e-02)	Acc@1  99.22 ( 97.96)	Acc@5 100.00 ( 99.98)
Epoch: [33][300/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.1945e-02 (6.1720e-02)	Acc@1  99.22 ( 97.97)	Acc@5 100.00 ( 99.98)
Epoch: [33][310/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.5436e-02 (6.1610e-02)	Acc@1  97.66 ( 97.97)	Acc@5 100.00 ( 99.98)
Epoch: [33][320/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0156e-01 (6.1654e-02)	Acc@1  95.31 ( 97.97)	Acc@5 100.00 ( 99.98)
Epoch: [33][330/391]	Time  0.248 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.8315e-02 (6.1903e-02)	Acc@1  96.09 ( 97.94)	Acc@5 100.00 ( 99.98)
Epoch: [33][340/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.9124e-02 (6.1731e-02)	Acc@1  98.44 ( 97.95)	Acc@5 100.00 ( 99.98)
Epoch: [33][350/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.3826e-02 (6.1581e-02)	Acc@1  97.66 ( 97.96)	Acc@5 100.00 ( 99.98)
Epoch: [33][360/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.3362e-02 (6.1489e-02)	Acc@1  95.31 ( 97.97)	Acc@5 100.00 ( 99.98)
Epoch: [33][370/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.7120e-02 (6.1563e-02)	Acc@1  97.66 ( 97.96)	Acc@5 100.00 ( 99.98)
Epoch: [33][380/391]	Time  0.244 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.0068e-02 (6.1434e-02)	Acc@1  96.09 ( 97.96)	Acc@5 100.00 ( 99.98)
Epoch: [33][390/391]	Time  0.178 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.0839e-02 (6.1562e-02)	Acc@1  97.50 ( 97.96)	Acc@5 100.00 ( 99.98)
## e[33] optimizer.zero_grad (sum) time: 0.47771668434143066
## e[33]       loss.backward (sum) time: 10.80182671546936
## e[33]      optimizer.step (sum) time: 47.57549238204956
## epoch[33] training(only) time: 94.26889610290527
# Switched to evaluate mode...
Test: [  0/100]	Time  0.203 ( 0.203)	Loss 2.4640e-01 (2.4640e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.078 ( 0.089)	Loss 2.0647e-01 (2.7544e-01)	Acc@1  95.00 ( 92.55)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.077 ( 0.084)	Loss 3.7601e-01 (2.7709e-01)	Acc@1  89.00 ( 92.29)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.077 ( 0.082)	Loss 2.5140e-01 (2.8533e-01)	Acc@1  90.00 ( 92.26)	Acc@5 100.00 ( 99.81)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 2.5769e-01 (2.8099e-01)	Acc@1  90.00 ( 92.27)	Acc@5 100.00 ( 99.83)
Test: [ 50/100]	Time  0.077 ( 0.080)	Loss 2.6665e-01 (2.8077e-01)	Acc@1  93.00 ( 92.27)	Acc@5 100.00 ( 99.84)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 3.3618e-01 (2.7533e-01)	Acc@1  93.00 ( 92.31)	Acc@5 100.00 ( 99.84)
Test: [ 70/100]	Time  0.077 ( 0.080)	Loss 3.3789e-01 (2.7265e-01)	Acc@1  91.00 ( 92.39)	Acc@5 100.00 ( 99.83)
Test: [ 80/100]	Time  0.077 ( 0.079)	Loss 2.0034e-01 (2.6749e-01)	Acc@1  92.00 ( 92.52)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.3675e-01 (2.6625e-01)	Acc@1  93.00 ( 92.40)	Acc@5 100.00 ( 99.82)
 * Acc@1 92.330 Acc@5 99.830
### epoch[33] execution time: 102.2610969543457
EPOCH 34
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [34][  0/391]	Time  0.387 ( 0.387)	Data  0.144 ( 0.144)	Loss 3.8784e-02 (3.8784e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [34][ 10/391]	Time  0.241 ( 0.254)	Data  0.001 ( 0.014)	Loss 5.2590e-02 (4.9649e-02)	Acc@1  98.44 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [34][ 20/391]	Time  0.241 ( 0.248)	Data  0.001 ( 0.008)	Loss 4.9455e-02 (5.4035e-02)	Acc@1  97.66 ( 98.14)	Acc@5 100.00 (100.00)
Epoch: [34][ 30/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.006)	Loss 4.4969e-02 (5.4433e-02)	Acc@1  98.44 ( 98.14)	Acc@5 100.00 (100.00)
Epoch: [34][ 40/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.005)	Loss 3.9390e-02 (5.4514e-02)	Acc@1  98.44 ( 98.25)	Acc@5 100.00 ( 99.98)
Epoch: [34][ 50/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 5.8046e-02 (5.5149e-02)	Acc@1  96.88 ( 98.21)	Acc@5 100.00 ( 99.98)
Epoch: [34][ 60/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.004)	Loss 4.1648e-02 (5.6909e-02)	Acc@1  98.44 ( 98.12)	Acc@5 100.00 ( 99.97)
Epoch: [34][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.3447e-01 (5.7776e-02)	Acc@1  95.31 ( 98.07)	Acc@5 100.00 ( 99.98)
Epoch: [34][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 6.3673e-02 (5.6447e-02)	Acc@1  98.44 ( 98.15)	Acc@5 100.00 ( 99.98)
Epoch: [34][ 90/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 5.3476e-02 (5.4869e-02)	Acc@1  97.66 ( 98.21)	Acc@5 100.00 ( 99.98)
Epoch: [34][100/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 3.0177e-02 (5.4174e-02)	Acc@1  99.22 ( 98.26)	Acc@5 100.00 ( 99.98)
Epoch: [34][110/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6228e-02 (5.3462e-02)	Acc@1  99.22 ( 98.28)	Acc@5 100.00 ( 99.99)
Epoch: [34][120/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.1820e-02 (5.2585e-02)	Acc@1 100.00 ( 98.33)	Acc@5 100.00 ( 99.99)
Epoch: [34][130/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1971e-02 (5.1891e-02)	Acc@1 100.00 ( 98.38)	Acc@5 100.00 ( 99.99)
Epoch: [34][140/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.2595e-02 (5.2251e-02)	Acc@1  97.66 ( 98.36)	Acc@5 100.00 ( 99.99)
Epoch: [34][150/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.9312e-03 (5.1511e-02)	Acc@1 100.00 ( 98.38)	Acc@5 100.00 ( 99.99)
Epoch: [34][160/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5665e-02 (5.1089e-02)	Acc@1 100.00 ( 98.39)	Acc@5 100.00 ( 99.99)
Epoch: [34][170/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.6633e-02 (5.1178e-02)	Acc@1  97.66 ( 98.38)	Acc@5 100.00 ( 99.99)
Epoch: [34][180/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6350e-02 (5.1623e-02)	Acc@1  99.22 ( 98.37)	Acc@5 100.00 ( 99.99)
Epoch: [34][190/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.2489e-02 (5.2012e-02)	Acc@1  98.44 ( 98.34)	Acc@5 100.00 ( 99.99)
Epoch: [34][200/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.3240e-02 (5.1908e-02)	Acc@1  99.22 ( 98.35)	Acc@5 100.00 ( 99.99)
Epoch: [34][210/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.1354e-02 (5.1964e-02)	Acc@1 100.00 ( 98.34)	Acc@5 100.00 ( 99.99)
Epoch: [34][220/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.6224e-02 (5.1785e-02)	Acc@1  99.22 ( 98.35)	Acc@5 100.00 ( 99.99)
Epoch: [34][230/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.9007e-02 (5.1723e-02)	Acc@1 100.00 ( 98.34)	Acc@5 100.00 ( 99.99)
Epoch: [34][240/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.6654e-02 (5.2127e-02)	Acc@1  96.09 ( 98.30)	Acc@5 100.00 ( 99.99)
Epoch: [34][250/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.9999e-02 (5.2094e-02)	Acc@1  96.88 ( 98.31)	Acc@5 100.00 ( 99.99)
Epoch: [34][260/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.2840e-02 (5.2242e-02)	Acc@1 100.00 ( 98.31)	Acc@5 100.00 ( 99.99)
Epoch: [34][270/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.3511e-02 (5.2390e-02)	Acc@1  98.44 ( 98.31)	Acc@5 100.00 ( 99.99)
Epoch: [34][280/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.0379e-02 (5.2308e-02)	Acc@1  97.66 ( 98.31)	Acc@5 100.00 ( 99.99)
Epoch: [34][290/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.0745e-02 (5.2154e-02)	Acc@1  98.44 ( 98.32)	Acc@5 100.00 ( 99.99)
Epoch: [34][300/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.8324e-02 (5.1889e-02)	Acc@1  99.22 ( 98.33)	Acc@5 100.00 ( 99.99)
Epoch: [34][310/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.1367e-02 (5.1508e-02)	Acc@1 100.00 ( 98.35)	Acc@5 100.00 ( 99.99)
Epoch: [34][320/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.3003e-02 (5.1429e-02)	Acc@1  99.22 ( 98.36)	Acc@5 100.00 ( 99.99)
Epoch: [34][330/391]	Time  0.243 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.7665e-02 (5.1763e-02)	Acc@1  98.44 ( 98.36)	Acc@5 100.00 ( 99.99)
Epoch: [34][340/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.0880e-02 (5.1776e-02)	Acc@1 100.00 ( 98.36)	Acc@5 100.00 ( 99.99)
Epoch: [34][350/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.5947e-02 (5.2079e-02)	Acc@1  99.22 ( 98.34)	Acc@5 100.00 ( 99.99)
Epoch: [34][360/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.1243e-02 (5.2035e-02)	Acc@1 100.00 ( 98.34)	Acc@5 100.00 ( 99.99)
Epoch: [34][370/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.5975e-02 (5.1806e-02)	Acc@1 100.00 ( 98.34)	Acc@5 100.00 ( 99.99)
Epoch: [34][380/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.7098e-02 (5.2116e-02)	Acc@1  96.88 ( 98.33)	Acc@5 100.00 ( 99.99)
Epoch: [34][390/391]	Time  0.177 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.6168e-02 (5.2300e-02)	Acc@1  97.50 ( 98.33)	Acc@5 100.00 ( 99.99)
## e[34] optimizer.zero_grad (sum) time: 0.47666025161743164
## e[34]       loss.backward (sum) time: 10.787821531295776
## e[34]      optimizer.step (sum) time: 47.56223940849304
## epoch[34] training(only) time: 94.30866312980652
# Switched to evaluate mode...
Test: [  0/100]	Time  0.199 ( 0.199)	Loss 2.2998e-01 (2.2998e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.078 ( 0.089)	Loss 2.5781e-01 (2.8101e-01)	Acc@1  94.00 ( 92.45)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 3.4037e-01 (2.8798e-01)	Acc@1  89.00 ( 92.29)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 2.7881e-01 (2.9487e-01)	Acc@1  90.00 ( 92.13)	Acc@5 100.00 ( 99.77)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 2.4016e-01 (2.8654e-01)	Acc@1  93.00 ( 92.41)	Acc@5 100.00 ( 99.80)
Test: [ 50/100]	Time  0.077 ( 0.080)	Loss 2.3160e-01 (2.8406e-01)	Acc@1  94.00 ( 92.39)	Acc@5 100.00 ( 99.84)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 4.0560e-01 (2.8017e-01)	Acc@1  91.00 ( 92.36)	Acc@5 100.00 ( 99.84)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 3.0140e-01 (2.7703e-01)	Acc@1  93.00 ( 92.45)	Acc@5 100.00 ( 99.83)
Test: [ 80/100]	Time  0.078 ( 0.079)	Loss 2.4636e-01 (2.7192e-01)	Acc@1  92.00 ( 92.52)	Acc@5 100.00 ( 99.81)
Test: [ 90/100]	Time  0.077 ( 0.079)	Loss 1.3480e-01 (2.7220e-01)	Acc@1  95.00 ( 92.40)	Acc@5 100.00 ( 99.82)
 * Acc@1 92.360 Acc@5 99.830
### epoch[34] execution time: 102.31294012069702
EPOCH 35
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [35][  0/391]	Time  0.385 ( 0.385)	Data  0.144 ( 0.144)	Loss 5.4593e-02 (5.4593e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [35][ 10/391]	Time  0.241 ( 0.254)	Data  0.001 ( 0.014)	Loss 2.5711e-02 (4.8580e-02)	Acc@1  99.22 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [35][ 20/391]	Time  0.241 ( 0.248)	Data  0.001 ( 0.008)	Loss 2.2613e-02 (4.7398e-02)	Acc@1 100.00 ( 98.47)	Acc@5 100.00 (100.00)
Epoch: [35][ 30/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.006)	Loss 5.5991e-02 (4.7097e-02)	Acc@1  96.88 ( 98.51)	Acc@5 100.00 ( 99.97)
Epoch: [35][ 40/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.005)	Loss 5.1148e-02 (4.7102e-02)	Acc@1  98.44 ( 98.55)	Acc@5 100.00 ( 99.98)
Epoch: [35][ 50/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.004)	Loss 1.4176e-02 (4.6571e-02)	Acc@1 100.00 ( 98.54)	Acc@5 100.00 ( 99.98)
Epoch: [35][ 60/391]	Time  0.239 ( 0.243)	Data  0.001 ( 0.003)	Loss 6.2209e-02 (4.6114e-02)	Acc@1  97.66 ( 98.54)	Acc@5 100.00 ( 99.99)
Epoch: [35][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 5.8445e-02 (4.7482e-02)	Acc@1  97.66 ( 98.49)	Acc@5 100.00 ( 99.99)
Epoch: [35][ 80/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 4.4872e-02 (4.8569e-02)	Acc@1  99.22 ( 98.44)	Acc@5 100.00 ( 99.99)
Epoch: [35][ 90/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 6.6423e-02 (4.7951e-02)	Acc@1  98.44 ( 98.45)	Acc@5 100.00 ( 99.99)
Epoch: [35][100/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 7.8365e-02 (4.7548e-02)	Acc@1  96.88 ( 98.48)	Acc@5 100.00 ( 99.99)
Epoch: [35][110/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.9972e-02 (4.6415e-02)	Acc@1  99.22 ( 98.54)	Acc@5 100.00 ( 99.99)
Epoch: [35][120/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.2675e-02 (4.6605e-02)	Acc@1  97.66 ( 98.55)	Acc@5 100.00 ( 99.99)
Epoch: [35][130/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.3877e-02 (4.6611e-02)	Acc@1  96.88 ( 98.53)	Acc@5 100.00 ( 99.99)
Epoch: [35][140/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4009e-02 (4.6035e-02)	Acc@1 100.00 ( 98.54)	Acc@5 100.00 ( 99.99)
Epoch: [35][150/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.2681e-02 (4.6680e-02)	Acc@1  97.66 ( 98.49)	Acc@5 100.00 ( 99.99)
Epoch: [35][160/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.9392e-02 (4.5889e-02)	Acc@1  99.22 ( 98.54)	Acc@5 100.00 (100.00)
Epoch: [35][170/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.1490e-02 (4.5926e-02)	Acc@1  98.44 ( 98.56)	Acc@5 100.00 (100.00)
Epoch: [35][180/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.0455e-02 (4.5890e-02)	Acc@1  98.44 ( 98.54)	Acc@5 100.00 (100.00)
Epoch: [35][190/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.1235e-02 (4.5787e-02)	Acc@1  98.44 ( 98.54)	Acc@5 100.00 (100.00)
Epoch: [35][200/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.8207e-02 (4.6084e-02)	Acc@1  97.66 ( 98.53)	Acc@5 100.00 (100.00)
Epoch: [35][210/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.5247e-02 (4.6020e-02)	Acc@1  99.22 ( 98.52)	Acc@5 100.00 (100.00)
Epoch: [35][220/391]	Time  0.247 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.9041e-02 (4.6206e-02)	Acc@1  98.44 ( 98.50)	Acc@5 100.00 (100.00)
Epoch: [35][230/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.2003e-02 (4.6752e-02)	Acc@1  96.88 ( 98.48)	Acc@5 100.00 (100.00)
Epoch: [35][240/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.9684e-02 (4.6962e-02)	Acc@1  96.09 ( 98.46)	Acc@5 100.00 (100.00)
Epoch: [35][250/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.3854e-02 (4.7325e-02)	Acc@1  99.22 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [35][260/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.5362e-02 (4.7229e-02)	Acc@1  99.22 ( 98.45)	Acc@5 100.00 (100.00)
Epoch: [35][270/391]	Time  0.252 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.6146e-02 (4.7378e-02)	Acc@1  99.22 ( 98.45)	Acc@5 100.00 ( 99.99)
Epoch: [35][280/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.5565e-02 (4.7579e-02)	Acc@1  98.44 ( 98.45)	Acc@5 100.00 ( 99.99)
Epoch: [35][290/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.9241e-02 (4.7618e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 ( 99.99)
Epoch: [35][300/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.1188e-02 (4.8008e-02)	Acc@1  97.66 ( 98.43)	Acc@5 100.00 ( 99.99)
Epoch: [35][310/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.2570e-02 (4.8041e-02)	Acc@1  99.22 ( 98.42)	Acc@5 100.00 ( 99.99)
Epoch: [35][320/391]	Time  0.243 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.8889e-02 (4.8181e-02)	Acc@1 100.00 ( 98.42)	Acc@5 100.00 ( 99.99)
Epoch: [35][330/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.8851e-02 (4.8301e-02)	Acc@1  99.22 ( 98.41)	Acc@5 100.00 ( 99.99)
Epoch: [35][340/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.2935e-02 (4.8287e-02)	Acc@1  98.44 ( 98.40)	Acc@5 100.00 ( 99.99)
Epoch: [35][350/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.1582e-02 (4.8063e-02)	Acc@1  99.22 ( 98.41)	Acc@5 100.00 ( 99.99)
Epoch: [35][360/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0467e-02 (4.7879e-02)	Acc@1 100.00 ( 98.42)	Acc@5 100.00 ( 99.99)
Epoch: [35][370/391]	Time  0.249 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.4401e-02 (4.7657e-02)	Acc@1 100.00 ( 98.43)	Acc@5 100.00 ( 99.99)
Epoch: [35][380/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.7897e-02 (4.7580e-02)	Acc@1  96.88 ( 98.43)	Acc@5 100.00 ( 99.99)
Epoch: [35][390/391]	Time  0.176 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.9677e-02 (4.7667e-02)	Acc@1  95.00 ( 98.42)	Acc@5 100.00 ( 99.99)
## e[35] optimizer.zero_grad (sum) time: 0.4755866527557373
## e[35]       loss.backward (sum) time: 10.847114086151123
## e[35]      optimizer.step (sum) time: 47.57002520561218
## epoch[35] training(only) time: 94.3913927078247
# Switched to evaluate mode...
Test: [  0/100]	Time  0.210 ( 0.210)	Loss 2.5831e-01 (2.5831e-01)	Acc@1  93.00 ( 93.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.079 ( 0.090)	Loss 2.3100e-01 (2.8514e-01)	Acc@1  94.00 ( 92.36)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 3.1633e-01 (2.9027e-01)	Acc@1  92.00 ( 92.29)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 1.9563e-01 (2.9657e-01)	Acc@1  91.00 ( 92.26)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.077 ( 0.081)	Loss 2.3256e-01 (2.8992e-01)	Acc@1  92.00 ( 92.49)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 1.8966e-01 (2.8781e-01)	Acc@1  94.00 ( 92.47)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 4.1976e-01 (2.8214e-01)	Acc@1  91.00 ( 92.44)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 3.5857e-01 (2.7808e-01)	Acc@1  91.00 ( 92.49)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 2.1798e-01 (2.7333e-01)	Acc@1  93.00 ( 92.57)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.6255e-01 (2.7354e-01)	Acc@1  92.00 ( 92.47)	Acc@5 100.00 ( 99.75)
 * Acc@1 92.450 Acc@5 99.760
### epoch[35] execution time: 102.43079233169556
EPOCH 36
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [36][  0/391]	Time  0.402 ( 0.402)	Data  0.145 ( 0.145)	Loss 1.6835e-02 (1.6835e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [36][ 10/391]	Time  0.241 ( 0.255)	Data  0.001 ( 0.014)	Loss 3.5376e-02 (4.0488e-02)	Acc@1  97.66 ( 98.86)	Acc@5 100.00 (100.00)
Epoch: [36][ 20/391]	Time  0.240 ( 0.248)	Data  0.001 ( 0.008)	Loss 3.4269e-02 (3.8315e-02)	Acc@1  99.22 ( 98.88)	Acc@5 100.00 (100.00)
Epoch: [36][ 30/391]	Time  0.240 ( 0.246)	Data  0.001 ( 0.006)	Loss 2.0148e-02 (3.6486e-02)	Acc@1  99.22 ( 98.92)	Acc@5 100.00 (100.00)
Epoch: [36][ 40/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.005)	Loss 2.7085e-02 (3.6416e-02)	Acc@1 100.00 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [36][ 50/391]	Time  0.244 ( 0.244)	Data  0.001 ( 0.004)	Loss 2.2155e-02 (3.8353e-02)	Acc@1 100.00 ( 98.85)	Acc@5 100.00 (100.00)
Epoch: [36][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 5.0322e-02 (3.8843e-02)	Acc@1  98.44 ( 98.85)	Acc@5 100.00 (100.00)
Epoch: [36][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.1085e-02 (3.8526e-02)	Acc@1  99.22 ( 98.87)	Acc@5 100.00 (100.00)
Epoch: [36][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 5.9891e-02 (3.9257e-02)	Acc@1  97.66 ( 98.83)	Acc@5 100.00 (100.00)
Epoch: [36][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 4.2458e-02 (3.9705e-02)	Acc@1  99.22 ( 98.82)	Acc@5 100.00 (100.00)
Epoch: [36][100/391]	Time  0.246 ( 0.243)	Data  0.001 ( 0.002)	Loss 6.1832e-02 (4.0609e-02)	Acc@1  99.22 ( 98.79)	Acc@5 100.00 (100.00)
Epoch: [36][110/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.6036e-02 (4.1541e-02)	Acc@1 100.00 ( 98.73)	Acc@5 100.00 (100.00)
Epoch: [36][120/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5714e-02 (4.1327e-02)	Acc@1 100.00 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [36][130/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.3026e-02 (4.1120e-02)	Acc@1  98.44 ( 98.73)	Acc@5 100.00 (100.00)
Epoch: [36][140/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.0195e-02 (4.1415e-02)	Acc@1  99.22 ( 98.71)	Acc@5 100.00 (100.00)
Epoch: [36][150/391]	Time  0.247 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.7457e-02 (4.1340e-02)	Acc@1  99.22 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [36][160/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.2717e-02 (4.1053e-02)	Acc@1  97.66 ( 98.73)	Acc@5 100.00 (100.00)
Epoch: [36][170/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.9581e-02 (4.1270e-02)	Acc@1  97.66 ( 98.73)	Acc@5 100.00 (100.00)
Epoch: [36][180/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.5628e-02 (4.1586e-02)	Acc@1  95.31 ( 98.71)	Acc@5 100.00 (100.00)
Epoch: [36][190/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.0663e-02 (4.2562e-02)	Acc@1  98.44 ( 98.68)	Acc@5 100.00 (100.00)
Epoch: [36][200/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.3562e-02 (4.3374e-02)	Acc@1  97.66 ( 98.64)	Acc@5 100.00 (100.00)
Epoch: [36][210/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.9862e-02 (4.2774e-02)	Acc@1  99.22 ( 98.66)	Acc@5 100.00 (100.00)
Epoch: [36][220/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1872e-02 (4.2728e-02)	Acc@1 100.00 ( 98.64)	Acc@5 100.00 (100.00)
Epoch: [36][230/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.0871e-02 (4.3143e-02)	Acc@1  98.44 ( 98.62)	Acc@5 100.00 (100.00)
Epoch: [36][240/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8010e-02 (4.3144e-02)	Acc@1  99.22 ( 98.62)	Acc@5 100.00 (100.00)
Epoch: [36][250/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.0470e-02 (4.2869e-02)	Acc@1 100.00 ( 98.63)	Acc@5 100.00 (100.00)
Epoch: [36][260/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.3945e-02 (4.2803e-02)	Acc@1  97.66 ( 98.63)	Acc@5 100.00 (100.00)
Epoch: [36][270/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.2246e-02 (4.2789e-02)	Acc@1  98.44 ( 98.64)	Acc@5 100.00 (100.00)
Epoch: [36][280/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7652e-02 (4.2510e-02)	Acc@1  99.22 ( 98.65)	Acc@5 100.00 (100.00)
Epoch: [36][290/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7476e-02 (4.2516e-02)	Acc@1  99.22 ( 98.65)	Acc@5 100.00 (100.00)
Epoch: [36][300/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.9018e-02 (4.2488e-02)	Acc@1  98.44 ( 98.65)	Acc@5 100.00 (100.00)
Epoch: [36][310/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.3083e-02 (4.2903e-02)	Acc@1  99.22 ( 98.63)	Acc@5 100.00 (100.00)
Epoch: [36][320/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2433e-02 (4.3181e-02)	Acc@1  99.22 ( 98.62)	Acc@5 100.00 (100.00)
Epoch: [36][330/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.0312e-02 (4.3247e-02)	Acc@1  99.22 ( 98.62)	Acc@5 100.00 (100.00)
Epoch: [36][340/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.0336e-02 (4.3489e-02)	Acc@1  98.44 ( 98.61)	Acc@5 100.00 (100.00)
Epoch: [36][350/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.7426e-02 (4.3956e-02)	Acc@1  99.22 ( 98.59)	Acc@5 100.00 (100.00)
Epoch: [36][360/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.1426e-02 (4.4385e-02)	Acc@1  98.44 ( 98.57)	Acc@5 100.00 (100.00)
Epoch: [36][370/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.4009e-02 (4.4514e-02)	Acc@1  98.44 ( 98.56)	Acc@5 100.00 (100.00)
Epoch: [36][380/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.001)	Loss 2.2424e-02 (4.4742e-02)	Acc@1 100.00 ( 98.55)	Acc@5 100.00 (100.00)
Epoch: [36][390/391]	Time  0.176 ( 0.241)	Data  0.001 ( 0.001)	Loss 1.0301e-02 (4.4848e-02)	Acc@1 100.00 ( 98.55)	Acc@5 100.00 (100.00)
## e[36] optimizer.zero_grad (sum) time: 0.4769015312194824
## e[36]       loss.backward (sum) time: 10.877339363098145
## e[36]      optimizer.step (sum) time: 47.5986704826355
## epoch[36] training(only) time: 94.52062058448792
# Switched to evaluate mode...
Test: [  0/100]	Time  0.208 ( 0.208)	Loss 2.4062e-01 (2.4062e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.079 ( 0.090)	Loss 2.0485e-01 (2.8121e-01)	Acc@1  94.00 ( 92.82)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.079 ( 0.084)	Loss 3.0787e-01 (2.8495e-01)	Acc@1  91.00 ( 92.57)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 2.7533e-01 (2.9909e-01)	Acc@1  90.00 ( 92.39)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 2.7954e-01 (2.9941e-01)	Acc@1  92.00 ( 92.51)	Acc@5 100.00 ( 99.76)
Test: [ 50/100]	Time  0.077 ( 0.081)	Loss 2.1295e-01 (2.9683e-01)	Acc@1  93.00 ( 92.41)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.079 ( 0.080)	Loss 4.3743e-01 (2.8808e-01)	Acc@1  93.00 ( 92.39)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 4.0656e-01 (2.8707e-01)	Acc@1  92.00 ( 92.41)	Acc@5  99.00 ( 99.76)
Test: [ 80/100]	Time  0.079 ( 0.080)	Loss 1.8942e-01 (2.8313e-01)	Acc@1  95.00 ( 92.51)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.4676e-01 (2.8285e-01)	Acc@1  91.00 ( 92.42)	Acc@5 100.00 ( 99.78)
 * Acc@1 92.340 Acc@5 99.790
### epoch[36] execution time: 102.5737566947937
EPOCH 37
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [37][  0/391]	Time  0.387 ( 0.387)	Data  0.147 ( 0.147)	Loss 1.3535e-02 (1.3535e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [37][ 10/391]	Time  0.240 ( 0.254)	Data  0.001 ( 0.014)	Loss 5.2503e-02 (3.2577e-02)	Acc@1  97.66 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [37][ 20/391]	Time  0.241 ( 0.248)	Data  0.001 ( 0.008)	Loss 4.7351e-02 (3.3168e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [37][ 30/391]	Time  0.240 ( 0.246)	Data  0.001 ( 0.006)	Loss 7.3140e-02 (3.4239e-02)	Acc@1  96.88 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [37][ 40/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.005)	Loss 4.9555e-02 (3.7659e-02)	Acc@1  98.44 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [37][ 50/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 2.6507e-02 (3.6498e-02)	Acc@1  99.22 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [37][ 60/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.004)	Loss 5.1091e-02 (3.7127e-02)	Acc@1  97.66 ( 98.90)	Acc@5 100.00 (100.00)
Epoch: [37][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.4017e-02 (3.6942e-02)	Acc@1  97.66 ( 98.88)	Acc@5 100.00 (100.00)
Epoch: [37][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.8919e-02 (3.6430e-02)	Acc@1  99.22 ( 98.88)	Acc@5 100.00 (100.00)
Epoch: [37][ 90/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 4.1091e-02 (3.5940e-02)	Acc@1  99.22 ( 98.89)	Acc@5 100.00 (100.00)
Epoch: [37][100/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 3.1325e-02 (3.5081e-02)	Acc@1  99.22 ( 98.90)	Acc@5 100.00 (100.00)
Epoch: [37][110/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5565e-02 (3.5075e-02)	Acc@1 100.00 ( 98.92)	Acc@5 100.00 (100.00)
Epoch: [37][120/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.2949e-02 (3.5770e-02)	Acc@1  98.44 ( 98.86)	Acc@5 100.00 (100.00)
Epoch: [37][130/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6541e-02 (3.7072e-02)	Acc@1  98.44 ( 98.80)	Acc@5 100.00 (100.00)
Epoch: [37][140/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.1320e-02 (3.6896e-02)	Acc@1  98.44 ( 98.78)	Acc@5 100.00 (100.00)
Epoch: [37][150/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.5156e-02 (3.6951e-02)	Acc@1  99.22 ( 98.79)	Acc@5 100.00 (100.00)
Epoch: [37][160/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.9989e-02 (3.7139e-02)	Acc@1  99.22 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [37][170/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2489e-02 (3.7191e-02)	Acc@1 100.00 ( 98.78)	Acc@5 100.00 (100.00)
Epoch: [37][180/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1574e-01 (3.7420e-02)	Acc@1  96.09 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [37][190/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.9246e-02 (3.7605e-02)	Acc@1  97.66 ( 98.76)	Acc@5 100.00 (100.00)
Epoch: [37][200/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.9522e-02 (3.7876e-02)	Acc@1  98.44 ( 98.75)	Acc@5 100.00 (100.00)
Epoch: [37][210/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2974e-02 (3.8352e-02)	Acc@1 100.00 ( 98.74)	Acc@5 100.00 (100.00)
Epoch: [37][220/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.8225e-02 (3.8106e-02)	Acc@1  96.88 ( 98.75)	Acc@5 100.00 (100.00)
Epoch: [37][230/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3300e-02 (3.7855e-02)	Acc@1 100.00 ( 98.76)	Acc@5 100.00 (100.00)
Epoch: [37][240/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.4426e-02 (3.8013e-02)	Acc@1  99.22 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [37][250/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.8863e-02 (3.8283e-02)	Acc@1  96.88 ( 98.75)	Acc@5 100.00 (100.00)
Epoch: [37][260/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.4304e-02 (3.8576e-02)	Acc@1  97.66 ( 98.73)	Acc@5 100.00 (100.00)
Epoch: [37][270/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.2697e-02 (3.8947e-02)	Acc@1  98.44 ( 98.71)	Acc@5 100.00 (100.00)
Epoch: [37][280/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2338e-01 (3.8989e-02)	Acc@1  93.75 ( 98.71)	Acc@5 100.00 (100.00)
Epoch: [37][290/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6968e-02 (3.9308e-02)	Acc@1 100.00 ( 98.69)	Acc@5 100.00 (100.00)
Epoch: [37][300/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1305e-02 (3.9416e-02)	Acc@1 100.00 ( 98.69)	Acc@5 100.00 (100.00)
Epoch: [37][310/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.7816e-02 (3.9247e-02)	Acc@1  99.22 ( 98.69)	Acc@5 100.00 (100.00)
Epoch: [37][320/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6825e-02 (3.9087e-02)	Acc@1  99.22 ( 98.69)	Acc@5 100.00 (100.00)
Epoch: [37][330/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.5622e-02 (3.9336e-02)	Acc@1  99.22 ( 98.69)	Acc@5 100.00 (100.00)
Epoch: [37][340/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.6524e-02 (3.9461e-02)	Acc@1  97.66 ( 98.68)	Acc@5 100.00 (100.00)
Epoch: [37][350/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1742e-02 (3.9258e-02)	Acc@1 100.00 ( 98.68)	Acc@5 100.00 (100.00)
Epoch: [37][360/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.2242e-02 (3.9413e-02)	Acc@1  97.66 ( 98.68)	Acc@5 100.00 (100.00)
Epoch: [37][370/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.3417e-02 (3.9734e-02)	Acc@1  98.44 ( 98.67)	Acc@5 100.00 (100.00)
Epoch: [37][380/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.6820e-02 (3.9858e-02)	Acc@1  98.44 ( 98.67)	Acc@5 100.00 (100.00)
Epoch: [37][390/391]	Time  0.176 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.9196e-02 (3.9948e-02)	Acc@1 100.00 ( 98.68)	Acc@5 100.00 (100.00)
## e[37] optimizer.zero_grad (sum) time: 0.47810912132263184
## e[37]       loss.backward (sum) time: 10.91481351852417
## e[37]      optimizer.step (sum) time: 47.55476522445679
## epoch[37] training(only) time: 94.45034742355347
# Switched to evaluate mode...
Test: [  0/100]	Time  0.209 ( 0.209)	Loss 2.4398e-01 (2.4398e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 2.0289e-01 (2.8671e-01)	Acc@1  95.00 ( 92.82)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 3.2004e-01 (2.8864e-01)	Acc@1  91.00 ( 92.38)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.077 ( 0.082)	Loss 2.4994e-01 (2.9541e-01)	Acc@1  90.00 ( 92.35)	Acc@5 100.00 ( 99.81)
Test: [ 40/100]	Time  0.077 ( 0.081)	Loss 2.4758e-01 (2.9262e-01)	Acc@1  93.00 ( 92.54)	Acc@5 100.00 ( 99.83)
Test: [ 50/100]	Time  0.078 ( 0.080)	Loss 1.8334e-01 (2.9171e-01)	Acc@1  94.00 ( 92.51)	Acc@5 100.00 ( 99.80)
Test: [ 60/100]	Time  0.077 ( 0.080)	Loss 3.9750e-01 (2.8438e-01)	Acc@1  94.00 ( 92.49)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.079 ( 0.080)	Loss 3.6180e-01 (2.8203e-01)	Acc@1  92.00 ( 92.51)	Acc@5  99.00 ( 99.79)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 2.1066e-01 (2.7713e-01)	Acc@1  94.00 ( 92.63)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.079 ( 0.079)	Loss 1.3844e-01 (2.8001e-01)	Acc@1  94.00 ( 92.53)	Acc@5 100.00 ( 99.80)
 * Acc@1 92.440 Acc@5 99.810
### epoch[37] execution time: 102.46825194358826
EPOCH 38
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [38][  0/391]	Time  0.391 ( 0.391)	Data  0.152 ( 0.152)	Loss 1.4495e-02 (1.4495e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [38][ 10/391]	Time  0.240 ( 0.254)	Data  0.001 ( 0.015)	Loss 3.3695e-02 (2.3805e-02)	Acc@1  98.44 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [38][ 20/391]	Time  0.241 ( 0.248)	Data  0.001 ( 0.008)	Loss 1.0807e-02 (2.9803e-02)	Acc@1 100.00 ( 98.85)	Acc@5 100.00 (100.00)
Epoch: [38][ 30/391]	Time  0.242 ( 0.245)	Data  0.001 ( 0.006)	Loss 3.7429e-02 (3.3186e-02)	Acc@1  99.22 ( 98.79)	Acc@5 100.00 (100.00)
Epoch: [38][ 40/391]	Time  0.244 ( 0.244)	Data  0.001 ( 0.005)	Loss 1.8119e-02 (3.4174e-02)	Acc@1 100.00 ( 98.78)	Acc@5 100.00 (100.00)
Epoch: [38][ 50/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 3.8766e-02 (3.3966e-02)	Acc@1  98.44 ( 98.85)	Acc@5 100.00 (100.00)
Epoch: [38][ 60/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.004)	Loss 1.3662e-02 (3.3036e-02)	Acc@1 100.00 ( 98.90)	Acc@5 100.00 (100.00)
Epoch: [38][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 4.3493e-02 (3.3489e-02)	Acc@1  97.66 ( 98.87)	Acc@5 100.00 (100.00)
Epoch: [38][ 80/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 5.8658e-02 (3.3389e-02)	Acc@1  98.44 ( 98.88)	Acc@5 100.00 (100.00)
Epoch: [38][ 90/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.003)	Loss 2.1344e-02 (3.5063e-02)	Acc@1  99.22 ( 98.84)	Acc@5 100.00 (100.00)
Epoch: [38][100/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.003)	Loss 2.4590e-02 (3.5231e-02)	Acc@1 100.00 ( 98.80)	Acc@5 100.00 (100.00)
Epoch: [38][110/391]	Time  0.250 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6332e-02 (3.5085e-02)	Acc@1  98.44 ( 98.80)	Acc@5 100.00 ( 99.99)
Epoch: [38][120/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6869e-02 (3.4829e-02)	Acc@1  99.22 ( 98.82)	Acc@5 100.00 ( 99.99)
Epoch: [38][130/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.7130e-02 (3.6049e-02)	Acc@1  97.66 ( 98.77)	Acc@5 100.00 ( 99.99)
Epoch: [38][140/391]	Time  0.249 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.5233e-02 (3.6228e-02)	Acc@1  98.44 ( 98.76)	Acc@5 100.00 ( 99.99)
Epoch: [38][150/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3410e-02 (3.6148e-02)	Acc@1  99.22 ( 98.75)	Acc@5 100.00 ( 99.99)
Epoch: [38][160/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.0353e-02 (3.5826e-02)	Acc@1  98.44 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [38][170/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9533e-02 (3.5417e-02)	Acc@1  99.22 ( 98.80)	Acc@5 100.00 (100.00)
Epoch: [38][180/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.4735e-02 (3.5463e-02)	Acc@1  97.66 ( 98.80)	Acc@5 100.00 (100.00)
Epoch: [38][190/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.6018e-02 (3.5427e-02)	Acc@1  98.44 ( 98.80)	Acc@5 100.00 (100.00)
Epoch: [38][200/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.7287e-02 (3.5111e-02)	Acc@1  98.44 ( 98.82)	Acc@5 100.00 (100.00)
Epoch: [38][210/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.5914e-02 (3.4826e-02)	Acc@1  99.22 ( 98.84)	Acc@5 100.00 (100.00)
Epoch: [38][220/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.1627e-02 (3.4906e-02)	Acc@1  98.44 ( 98.84)	Acc@5 100.00 (100.00)
Epoch: [38][230/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2122e-02 (3.4747e-02)	Acc@1  99.22 ( 98.84)	Acc@5 100.00 (100.00)
Epoch: [38][240/391]	Time  0.249 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.3285e-03 (3.4197e-02)	Acc@1 100.00 ( 98.86)	Acc@5 100.00 (100.00)
Epoch: [38][250/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.7315e-02 (3.4054e-02)	Acc@1 100.00 ( 98.87)	Acc@5 100.00 (100.00)
Epoch: [38][260/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.0962e-02 (3.4199e-02)	Acc@1  98.44 ( 98.87)	Acc@5 100.00 ( 99.99)
Epoch: [38][270/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.1672e-02 (3.4458e-02)	Acc@1  97.66 ( 98.86)	Acc@5 100.00 ( 99.99)
Epoch: [38][280/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.1542e-02 (3.4433e-02)	Acc@1  98.44 ( 98.85)	Acc@5 100.00 ( 99.99)
Epoch: [38][290/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.8494e-02 (3.4518e-02)	Acc@1  99.22 ( 98.85)	Acc@5 100.00 ( 99.99)
Epoch: [38][300/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.9674e-02 (3.4762e-02)	Acc@1  97.66 ( 98.84)	Acc@5 100.00 ( 99.99)
Epoch: [38][310/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.3871e-02 (3.5006e-02)	Acc@1  98.44 ( 98.83)	Acc@5 100.00 ( 99.99)
Epoch: [38][320/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.0900e-03 (3.4643e-02)	Acc@1 100.00 ( 98.85)	Acc@5 100.00 (100.00)
Epoch: [38][330/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.8270e-02 (3.4721e-02)	Acc@1  98.44 ( 98.84)	Acc@5 100.00 (100.00)
Epoch: [38][340/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.9030e-02 (3.4580e-02)	Acc@1  99.22 ( 98.85)	Acc@5 100.00 (100.00)
Epoch: [38][350/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.7788e-02 (3.4593e-02)	Acc@1  97.66 ( 98.84)	Acc@5 100.00 (100.00)
Epoch: [38][360/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.9651e-02 (3.5059e-02)	Acc@1  99.22 ( 98.84)	Acc@5 100.00 (100.00)
Epoch: [38][370/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.9452e-02 (3.5064e-02)	Acc@1  99.22 ( 98.84)	Acc@5 100.00 (100.00)
Epoch: [38][380/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.1647e-02 (3.5271e-02)	Acc@1  99.22 ( 98.83)	Acc@5 100.00 (100.00)
Epoch: [38][390/391]	Time  0.176 ( 0.241)	Data  0.001 ( 0.001)	Loss 3.6061e-02 (3.5245e-02)	Acc@1  98.75 ( 98.83)	Acc@5 100.00 (100.00)
## e[38] optimizer.zero_grad (sum) time: 0.4776170253753662
## e[38]       loss.backward (sum) time: 10.87763237953186
## e[38]      optimizer.step (sum) time: 47.57252764701843
## epoch[38] training(only) time: 94.3682177066803
# Switched to evaluate mode...
Test: [  0/100]	Time  0.208 ( 0.208)	Loss 2.1243e-01 (2.1243e-01)	Acc@1  94.00 ( 94.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.078 ( 0.089)	Loss 2.4777e-01 (2.8845e-01)	Acc@1  95.00 ( 93.09)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 3.8873e-01 (2.9695e-01)	Acc@1  90.00 ( 92.81)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 2.9176e-01 (3.0415e-01)	Acc@1  90.00 ( 92.61)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.077 ( 0.081)	Loss 2.2747e-01 (2.9966e-01)	Acc@1  92.00 ( 92.59)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.078 ( 0.080)	Loss 2.0467e-01 (2.9740e-01)	Acc@1  94.00 ( 92.55)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.079 ( 0.080)	Loss 4.3538e-01 (2.9330e-01)	Acc@1  92.00 ( 92.49)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.079 ( 0.080)	Loss 3.5860e-01 (2.8958e-01)	Acc@1  93.00 ( 92.55)	Acc@5  99.00 ( 99.75)
Test: [ 80/100]	Time  0.078 ( 0.079)	Loss 1.8676e-01 (2.8264e-01)	Acc@1  95.00 ( 92.77)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.7028e-01 (2.8495e-01)	Acc@1  92.00 ( 92.65)	Acc@5 100.00 ( 99.76)
 * Acc@1 92.580 Acc@5 99.760
### epoch[38] execution time: 102.37921690940857
EPOCH 39
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [39][  0/391]	Time  0.399 ( 0.399)	Data  0.156 ( 0.156)	Loss 1.6734e-02 (1.6734e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [39][ 10/391]	Time  0.240 ( 0.255)	Data  0.001 ( 0.015)	Loss 2.4836e-02 (3.6956e-02)	Acc@1  99.22 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [39][ 20/391]	Time  0.240 ( 0.248)	Data  0.001 ( 0.009)	Loss 4.0773e-02 (3.0860e-02)	Acc@1  99.22 ( 98.70)	Acc@5 100.00 (100.00)
Epoch: [39][ 30/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.006)	Loss 1.3930e-02 (2.9049e-02)	Acc@1 100.00 ( 98.87)	Acc@5 100.00 (100.00)
Epoch: [39][ 40/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.005)	Loss 1.4727e-02 (2.9747e-02)	Acc@1 100.00 ( 98.84)	Acc@5 100.00 (100.00)
Epoch: [39][ 50/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.5622e-02 (3.1077e-02)	Acc@1  99.22 ( 98.87)	Acc@5 100.00 (100.00)
Epoch: [39][ 60/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 5.0149e-02 (3.2482e-02)	Acc@1  97.66 ( 98.81)	Acc@5 100.00 (100.00)
Epoch: [39][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.6591e-02 (3.2198e-02)	Acc@1 100.00 ( 98.84)	Acc@5 100.00 (100.00)
Epoch: [39][ 80/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.3135e-02 (3.3029e-02)	Acc@1  98.44 ( 98.81)	Acc@5 100.00 (100.00)
Epoch: [39][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.5843e-02 (3.2027e-02)	Acc@1  99.22 ( 98.88)	Acc@5 100.00 (100.00)
Epoch: [39][100/391]	Time  0.239 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.0818e-02 (3.1286e-02)	Acc@1 100.00 ( 98.90)	Acc@5 100.00 (100.00)
Epoch: [39][110/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.003)	Loss 4.4074e-02 (3.0865e-02)	Acc@1  98.44 ( 98.92)	Acc@5 100.00 (100.00)
Epoch: [39][120/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1824e-02 (3.1156e-02)	Acc@1 100.00 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [39][130/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.9007e-02 (3.0726e-02)	Acc@1 100.00 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [39][140/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2674e-02 (3.1146e-02)	Acc@1  99.22 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [39][150/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9082e-02 (3.1507e-02)	Acc@1  99.22 ( 98.92)	Acc@5 100.00 (100.00)
Epoch: [39][160/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.6190e-02 (3.1451e-02)	Acc@1  98.44 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [39][170/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.6576e-02 (3.1776e-02)	Acc@1  97.66 ( 98.91)	Acc@5 100.00 (100.00)
Epoch: [39][180/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.2580e-02 (3.1626e-02)	Acc@1  98.44 ( 98.92)	Acc@5 100.00 (100.00)
Epoch: [39][190/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4782e-02 (3.2029e-02)	Acc@1  98.44 ( 98.90)	Acc@5 100.00 (100.00)
Epoch: [39][200/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8550e-02 (3.1815e-02)	Acc@1  99.22 ( 98.91)	Acc@5 100.00 (100.00)
Epoch: [39][210/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.3647e-02 (3.1540e-02)	Acc@1  97.66 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [39][220/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.6957e-03 (3.1474e-02)	Acc@1 100.00 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [39][230/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.8991e-02 (3.1236e-02)	Acc@1  96.09 ( 98.94)	Acc@5 100.00 (100.00)
Epoch: [39][240/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0932e-02 (3.1046e-02)	Acc@1  99.22 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [39][250/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.9468e-02 (3.1127e-02)	Acc@1  99.22 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [39][260/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.6783e-02 (3.0992e-02)	Acc@1  99.22 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [39][270/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.4863e-02 (3.0871e-02)	Acc@1  99.22 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [39][280/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.5719e-02 (3.1049e-02)	Acc@1 100.00 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [39][290/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.9366e-02 (3.1139e-02)	Acc@1  99.22 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [39][300/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.7108e-02 (3.1133e-02)	Acc@1  97.66 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [39][310/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.7191e-02 (3.1075e-02)	Acc@1  98.44 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [39][320/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.9275e-02 (3.0923e-02)	Acc@1  99.22 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [39][330/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.8044e-02 (3.0906e-02)	Acc@1 100.00 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [39][340/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.6299e-02 (3.0713e-02)	Acc@1 100.00 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [39][350/391]	Time  0.247 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.0772e-02 (3.0628e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [39][360/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.3542e-02 (3.0717e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [39][370/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.1837e-02 (3.1041e-02)	Acc@1  98.44 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [39][380/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.6177e-02 (3.1301e-02)	Acc@1  99.22 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [39][390/391]	Time  0.176 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.0870e-02 (3.1433e-02)	Acc@1  96.25 ( 98.98)	Acc@5 100.00 (100.00)
## e[39] optimizer.zero_grad (sum) time: 0.4800417423248291
## e[39]       loss.backward (sum) time: 10.876777410507202
## e[39]      optimizer.step (sum) time: 47.54130244255066
## epoch[39] training(only) time: 94.3809585571289
# Switched to evaluate mode...
Test: [  0/100]	Time  0.206 ( 0.206)	Loss 2.3030e-01 (2.3030e-01)	Acc@1  93.00 ( 93.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 2.0177e-01 (3.1671e-01)	Acc@1  95.00 ( 92.64)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 3.5679e-01 (3.1807e-01)	Acc@1  90.00 ( 92.29)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 3.0560e-01 (3.2768e-01)	Acc@1  90.00 ( 92.13)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.079 ( 0.081)	Loss 2.3367e-01 (3.1833e-01)	Acc@1  92.00 ( 92.27)	Acc@5 100.00 ( 99.78)
Test: [ 50/100]	Time  0.079 ( 0.081)	Loss 2.0692e-01 (3.1268e-01)	Acc@1  95.00 ( 92.39)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.077 ( 0.080)	Loss 4.0967e-01 (3.0406e-01)	Acc@1  93.00 ( 92.48)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.077 ( 0.080)	Loss 4.1035e-01 (3.0027e-01)	Acc@1  91.00 ( 92.55)	Acc@5  99.00 ( 99.76)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 1.8715e-01 (2.9487e-01)	Acc@1  93.00 ( 92.62)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.079 ( 0.079)	Loss 1.5554e-01 (2.9520e-01)	Acc@1  95.00 ( 92.59)	Acc@5 100.00 ( 99.77)
 * Acc@1 92.520 Acc@5 99.770
### epoch[39] execution time: 102.39809989929199
EPOCH 40
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [40][  0/391]	Time  0.392 ( 0.392)	Data  0.150 ( 0.150)	Loss 2.7174e-02 (2.7174e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [40][ 10/391]	Time  0.240 ( 0.254)	Data  0.001 ( 0.015)	Loss 6.1037e-02 (4.4259e-02)	Acc@1  96.88 ( 98.37)	Acc@5 100.00 ( 99.93)
Epoch: [40][ 20/391]	Time  0.240 ( 0.248)	Data  0.001 ( 0.008)	Loss 1.0959e-02 (3.2714e-02)	Acc@1 100.00 ( 98.77)	Acc@5 100.00 ( 99.96)
Epoch: [40][ 30/391]	Time  0.239 ( 0.245)	Data  0.001 ( 0.006)	Loss 1.9322e-02 (3.2224e-02)	Acc@1  99.22 ( 98.74)	Acc@5 100.00 ( 99.97)
Epoch: [40][ 40/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.005)	Loss 4.6610e-03 (3.2445e-02)	Acc@1 100.00 ( 98.76)	Acc@5 100.00 ( 99.98)
Epoch: [40][ 50/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.004)	Loss 2.5199e-02 (3.2773e-02)	Acc@1  99.22 ( 98.87)	Acc@5 100.00 ( 99.98)
Epoch: [40][ 60/391]	Time  0.239 ( 0.243)	Data  0.001 ( 0.004)	Loss 6.1288e-03 (3.1516e-02)	Acc@1 100.00 ( 98.91)	Acc@5 100.00 ( 99.99)
Epoch: [40][ 70/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.8144e-02 (3.1603e-02)	Acc@1  99.22 ( 98.89)	Acc@5 100.00 ( 99.99)
Epoch: [40][ 80/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.0708e-02 (3.1842e-02)	Acc@1 100.00 ( 98.91)	Acc@5 100.00 ( 99.99)
Epoch: [40][ 90/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.3948e-02 (3.0778e-02)	Acc@1 100.00 ( 98.94)	Acc@5 100.00 ( 99.99)
Epoch: [40][100/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.5307e-02 (3.0707e-02)	Acc@1 100.00 ( 98.99)	Acc@5 100.00 ( 99.99)
Epoch: [40][110/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5827e-02 (3.0115e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 ( 99.99)
Epoch: [40][120/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8835e-02 (3.0451e-02)	Acc@1  99.22 ( 98.98)	Acc@5 100.00 ( 99.99)
Epoch: [40][130/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1383e-02 (2.9963e-02)	Acc@1 100.00 ( 99.00)	Acc@5 100.00 ( 99.99)
Epoch: [40][140/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3894e-02 (2.9611e-02)	Acc@1  99.22 ( 99.02)	Acc@5 100.00 ( 99.99)
Epoch: [40][150/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.5006e-02 (3.0288e-02)	Acc@1  99.22 ( 99.02)	Acc@5 100.00 ( 99.99)
Epoch: [40][160/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.2088e-02 (2.9998e-02)	Acc@1  98.44 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [40][170/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.5105e-02 (3.0989e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [40][180/391]	Time  0.247 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.5317e-02 (3.1287e-02)	Acc@1  99.22 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [40][190/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.2526e-02 (3.1132e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [40][200/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.1609e-02 (3.0826e-02)	Acc@1  99.22 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [40][210/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2342e-02 (3.0683e-02)	Acc@1 100.00 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [40][220/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.2898e-02 (3.0418e-02)	Acc@1  97.66 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [40][230/391]	Time  0.245 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.6622e-02 (3.0680e-02)	Acc@1  96.88 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [40][240/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.0033e-02 (3.0701e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 ( 99.99)
Epoch: [40][250/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.9946e-02 (3.0929e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [40][260/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.8761e-02 (3.1284e-02)	Acc@1  99.22 ( 98.99)	Acc@5 100.00 ( 99.99)
Epoch: [40][270/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.7931e-02 (3.1067e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 ( 99.99)
Epoch: [40][280/391]	Time  0.243 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.8025e-02 (3.1102e-02)	Acc@1  97.66 ( 99.00)	Acc@5 100.00 ( 99.99)
Epoch: [40][290/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.4512e-02 (3.0892e-02)	Acc@1  98.44 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [40][300/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.3534e-02 (3.1276e-02)	Acc@1  99.22 ( 98.98)	Acc@5 100.00 ( 99.99)
Epoch: [40][310/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.3374e-02 (3.1700e-02)	Acc@1  98.44 ( 98.97)	Acc@5 100.00 ( 99.99)
Epoch: [40][320/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.8177e-03 (3.1790e-02)	Acc@1 100.00 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [40][330/391]	Time  0.247 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.3220e-02 (3.1682e-02)	Acc@1  97.66 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [40][340/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.6278e-02 (3.1586e-02)	Acc@1  99.22 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [40][350/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.7352e-02 (3.1320e-02)	Acc@1  98.44 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [40][360/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.6266e-02 (3.1248e-02)	Acc@1 100.00 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [40][370/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.2337e-02 (3.1409e-02)	Acc@1 100.00 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [40][380/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.9289e-02 (3.1396e-02)	Acc@1  99.22 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [40][390/391]	Time  0.176 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.8977e-02 (3.1143e-02)	Acc@1  96.25 ( 98.97)	Acc@5 100.00 (100.00)
## e[40] optimizer.zero_grad (sum) time: 0.4777214527130127
## e[40]       loss.backward (sum) time: 10.88559627532959
## e[40]      optimizer.step (sum) time: 47.49848747253418
## epoch[40] training(only) time: 94.27963948249817
# Switched to evaluate mode...
Test: [  0/100]	Time  0.208 ( 0.208)	Loss 2.5846e-01 (2.5846e-01)	Acc@1  94.00 ( 94.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.079 ( 0.090)	Loss 2.0887e-01 (3.1665e-01)	Acc@1  95.00 ( 92.45)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 3.6233e-01 (3.1507e-01)	Acc@1  90.00 ( 92.19)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.077 ( 0.082)	Loss 2.3350e-01 (3.2251e-01)	Acc@1  93.00 ( 92.10)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.079 ( 0.081)	Loss 2.9155e-01 (3.1777e-01)	Acc@1  93.00 ( 92.29)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.077 ( 0.080)	Loss 2.2302e-01 (3.0938e-01)	Acc@1  95.00 ( 92.37)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.080 ( 0.080)	Loss 4.0730e-01 (3.0058e-01)	Acc@1  92.00 ( 92.51)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 4.0825e-01 (2.9824e-01)	Acc@1  92.00 ( 92.62)	Acc@5  99.00 ( 99.70)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 1.9991e-01 (2.9321e-01)	Acc@1  94.00 ( 92.78)	Acc@5 100.00 ( 99.69)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.4998e-01 (2.9359e-01)	Acc@1  94.00 ( 92.68)	Acc@5 100.00 ( 99.73)
 * Acc@1 92.640 Acc@5 99.730
### epoch[40] execution time: 102.29532599449158
EPOCH 41
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [41][  0/391]	Time  0.390 ( 0.390)	Data  0.148 ( 0.148)	Loss 2.9122e-02 (2.9122e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [41][ 10/391]	Time  0.240 ( 0.255)	Data  0.001 ( 0.014)	Loss 1.5023e-02 (3.0155e-02)	Acc@1 100.00 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [41][ 20/391]	Time  0.240 ( 0.248)	Data  0.001 ( 0.008)	Loss 9.7065e-02 (3.0914e-02)	Acc@1  96.88 ( 98.92)	Acc@5 100.00 (100.00)
Epoch: [41][ 30/391]	Time  0.239 ( 0.246)	Data  0.001 ( 0.006)	Loss 1.9266e-02 (2.8031e-02)	Acc@1  99.22 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [41][ 40/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.005)	Loss 2.6266e-02 (2.8746e-02)	Acc@1  99.22 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [41][ 50/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 3.1389e-02 (2.9602e-02)	Acc@1  99.22 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [41][ 60/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 3.0249e-02 (2.8331e-02)	Acc@1  99.22 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [41][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 8.6166e-03 (2.7413e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [41][ 80/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 4.8936e-02 (2.7459e-02)	Acc@1  97.66 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [41][ 90/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.003)	Loss 6.0076e-03 (2.7775e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [41][100/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.003)	Loss 9.7011e-03 (2.7756e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [41][110/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2663e-02 (2.6888e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [41][120/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.6518e-02 (2.6695e-02)	Acc@1  97.66 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [41][130/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.0750e-03 (2.6728e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [41][140/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.6784e-02 (2.6580e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [41][150/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.3386e-02 (2.6953e-02)	Acc@1  98.44 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [41][160/391]	Time  0.252 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5914e-02 (2.7002e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [41][170/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9540e-02 (2.6434e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [41][180/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9745e-02 (2.6694e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [41][190/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.1049e-02 (2.7223e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [41][200/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8917e-02 (2.6884e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [41][210/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.4236e-02 (2.6518e-02)	Acc@1  97.66 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [41][220/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.9364e-03 (2.6279e-02)	Acc@1 100.00 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [41][230/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.5925e-02 (2.6513e-02)	Acc@1  96.88 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [41][240/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.3217e-02 (2.6454e-02)	Acc@1  98.44 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [41][250/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.2437e-02 (2.6503e-02)	Acc@1  97.66 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [41][260/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.1280e-02 (2.6564e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [41][270/391]	Time  0.251 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.3182e-02 (2.6552e-02)	Acc@1  99.22 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [41][280/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.3525e-02 (2.6398e-02)	Acc@1  98.44 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [41][290/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.9476e-02 (2.6269e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [41][300/391]	Time  0.243 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.3475e-02 (2.6192e-02)	Acc@1  99.22 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [41][310/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.0329e-02 (2.6064e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [41][320/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1635e-02 (2.5994e-02)	Acc@1  99.22 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [41][330/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.1505e-03 (2.5942e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [41][340/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.0786e-02 (2.5968e-02)	Acc@1  99.22 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [41][350/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2283e-02 (2.5812e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [41][360/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.1989e-03 (2.6164e-02)	Acc@1 100.00 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [41][370/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.7636e-03 (2.6041e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [41][380/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.0085e-03 (2.5854e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [41][390/391]	Time  0.177 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.5301e-02 (2.5956e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 (100.00)
## e[41] optimizer.zero_grad (sum) time: 0.48107290267944336
## e[41]       loss.backward (sum) time: 10.820065021514893
## e[41]      optimizer.step (sum) time: 47.56209063529968
## epoch[41] training(only) time: 94.28912496566772
# Switched to evaluate mode...
Test: [  0/100]	Time  0.210 ( 0.210)	Loss 2.0805e-01 (2.0805e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 2.1691e-01 (3.1298e-01)	Acc@1  94.00 ( 92.27)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 3.6551e-01 (3.0919e-01)	Acc@1  91.00 ( 92.43)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 2.9911e-01 (3.2617e-01)	Acc@1  90.00 ( 92.16)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.079 ( 0.081)	Loss 2.3909e-01 (3.1777e-01)	Acc@1  92.00 ( 92.41)	Acc@5 100.00 ( 99.76)
Test: [ 50/100]	Time  0.078 ( 0.080)	Loss 2.0356e-01 (3.1419e-01)	Acc@1  94.00 ( 92.53)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 4.1537e-01 (3.0454e-01)	Acc@1  92.00 ( 92.52)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.077 ( 0.080)	Loss 3.9711e-01 (3.0177e-01)	Acc@1  91.00 ( 92.59)	Acc@5  99.00 ( 99.75)
Test: [ 80/100]	Time  0.078 ( 0.079)	Loss 1.9098e-01 (2.9598e-01)	Acc@1  94.00 ( 92.78)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.6005e-01 (2.9886e-01)	Acc@1  93.00 ( 92.70)	Acc@5 100.00 ( 99.77)
 * Acc@1 92.670 Acc@5 99.780
### epoch[41] execution time: 102.30310893058777
EPOCH 42
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [42][  0/391]	Time  0.383 ( 0.383)	Data  0.136 ( 0.136)	Loss 3.1366e-02 (3.1366e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [42][ 10/391]	Time  0.241 ( 0.253)	Data  0.001 ( 0.013)	Loss 5.6182e-03 (1.8469e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [42][ 20/391]	Time  0.240 ( 0.247)	Data  0.001 ( 0.008)	Loss 2.3874e-02 (1.8021e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [42][ 30/391]	Time  0.242 ( 0.245)	Data  0.001 ( 0.006)	Loss 2.3651e-02 (1.9796e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [42][ 40/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 3.7915e-02 (2.2137e-02)	Acc@1  98.44 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [42][ 50/391]	Time  0.243 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.6132e-02 (2.2682e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [42][ 60/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 4.1497e-02 (2.4275e-02)	Acc@1  98.44 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [42][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.4336e-02 (2.3172e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [42][ 80/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 5.1650e-02 (2.3840e-02)	Acc@1  97.66 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [42][ 90/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 3.2849e-02 (2.3930e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [42][100/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.003)	Loss 3.8592e-03 (2.3954e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [42][110/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.0420e-02 (2.3935e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [42][120/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.6066e-03 (2.3640e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [42][130/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3716e-02 (2.4679e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [42][140/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.0024e-02 (2.4664e-02)	Acc@1  98.44 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [42][150/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1112e-02 (2.4828e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [42][160/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.3660e-03 (2.4497e-02)	Acc@1 100.00 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [42][170/391]	Time  0.246 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1992e-02 (2.5066e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [42][180/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.0404e-02 (2.5140e-02)	Acc@1  98.44 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [42][190/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.2554e-02 (2.5397e-02)	Acc@1  97.66 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [42][200/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2274e-02 (2.5312e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [42][210/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.6307e-02 (2.5162e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [42][220/391]	Time  0.245 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.0694e-02 (2.5293e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [42][230/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1308e-02 (2.4975e-02)	Acc@1 100.00 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [42][240/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.8435e-02 (2.4928e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [42][250/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.5254e-03 (2.4881e-02)	Acc@1 100.00 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [42][260/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.0453e-03 (2.5048e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [42][270/391]	Time  0.246 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.1801e-02 (2.5030e-02)	Acc@1  98.44 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [42][280/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2837e-02 (2.4943e-02)	Acc@1 100.00 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [42][290/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.8386e-02 (2.5346e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [42][300/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.8154e-02 (2.5332e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [42][310/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.6744e-03 (2.5479e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [42][320/391]	Time  0.245 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.1537e-02 (2.5398e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [42][330/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.3387e-02 (2.5189e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [42][340/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.9026e-02 (2.5336e-02)	Acc@1  96.88 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [42][350/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.6996e-02 (2.5229e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [42][360/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.3751e-02 (2.5238e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [42][370/391]	Time  0.245 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.3972e-02 (2.5041e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [42][380/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.9517e-02 (2.5148e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [42][390/391]	Time  0.178 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2529e-02 (2.4935e-02)	Acc@1 100.00 ( 99.21)	Acc@5 100.00 (100.00)
## e[42] optimizer.zero_grad (sum) time: 0.47702860832214355
## e[42]       loss.backward (sum) time: 10.83894681930542
## e[42]      optimizer.step (sum) time: 47.52658557891846
## epoch[42] training(only) time: 94.28107190132141
# Switched to evaluate mode...
Test: [  0/100]	Time  0.206 ( 0.206)	Loss 2.1913e-01 (2.1913e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.8609e-01 (2.9882e-01)	Acc@1  94.00 ( 92.27)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.077 ( 0.084)	Loss 4.0153e-01 (3.0589e-01)	Acc@1  91.00 ( 92.29)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 2.5139e-01 (3.1722e-01)	Acc@1  92.00 ( 92.26)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 2.3952e-01 (3.1196e-01)	Acc@1  91.00 ( 92.39)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.079 ( 0.081)	Loss 2.5098e-01 (3.0888e-01)	Acc@1  95.00 ( 92.61)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 4.1428e-01 (3.0119e-01)	Acc@1  93.00 ( 92.66)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 4.0710e-01 (2.9789e-01)	Acc@1  92.00 ( 92.76)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 2.1344e-01 (2.9362e-01)	Acc@1  95.00 ( 92.91)	Acc@5 100.00 ( 99.73)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.5420e-01 (2.9702e-01)	Acc@1  95.00 ( 92.80)	Acc@5 100.00 ( 99.75)
 * Acc@1 92.710 Acc@5 99.750
### epoch[42] execution time: 102.32444024085999
EPOCH 43
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [43][  0/391]	Time  0.402 ( 0.402)	Data  0.153 ( 0.153)	Loss 7.6540e-03 (7.6540e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [43][ 10/391]	Time  0.240 ( 0.255)	Data  0.001 ( 0.015)	Loss 2.3231e-02 (1.9493e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [43][ 20/391]	Time  0.240 ( 0.248)	Data  0.001 ( 0.008)	Loss 6.2589e-03 (2.2163e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [43][ 30/391]	Time  0.240 ( 0.246)	Data  0.001 ( 0.006)	Loss 1.8602e-02 (2.0046e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [43][ 40/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.005)	Loss 2.6267e-02 (2.0576e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [43][ 50/391]	Time  0.243 ( 0.244)	Data  0.001 ( 0.004)	Loss 9.0274e-03 (1.9379e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [43][ 60/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.004)	Loss 2.1429e-02 (2.0516e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [43][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.2400e-02 (2.1597e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [43][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.5932e-02 (2.1421e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [43][ 90/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 4.1902e-02 (2.2045e-02)	Acc@1  98.44 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [43][100/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.0354e-02 (2.2911e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [43][110/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 6.9463e-03 (2.3624e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [43][120/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3775e-02 (2.3099e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [43][130/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6056e-02 (2.3030e-02)	Acc@1  98.44 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [43][140/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.7163e-02 (2.3132e-02)	Acc@1  97.66 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [43][150/391]	Time  0.243 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.0669e-03 (2.3186e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [43][160/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.3381e-02 (2.2998e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [43][170/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.0607e-02 (2.3088e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [43][180/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.0771e-02 (2.3038e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [43][190/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.2561e-02 (2.2703e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [43][200/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.8985e-02 (2.2748e-02)	Acc@1  98.44 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [43][210/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0038e-02 (2.2566e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [43][220/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.0724e-02 (2.2557e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [43][230/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.3285e-02 (2.2964e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [43][240/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1804e-02 (2.3052e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [43][250/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2196e-02 (2.3215e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [43][260/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.9236e-02 (2.3019e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [43][270/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2799e-02 (2.3140e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [43][280/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.3026e-02 (2.2963e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [43][290/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.2784e-03 (2.2891e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [43][300/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.4510e-02 (2.3082e-02)	Acc@1  97.66 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [43][310/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1030e-02 (2.2914e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [43][320/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1880e-02 (2.2794e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [43][330/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.6770e-02 (2.3020e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [43][340/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.2027e-03 (2.2787e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [43][350/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.9394e-02 (2.3009e-02)	Acc@1  98.44 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [43][360/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.7045e-03 (2.2862e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [43][370/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2657e-02 (2.2880e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [43][380/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.6215e-02 (2.2800e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [43][390/391]	Time  0.176 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.8072e-02 (2.2794e-02)	Acc@1  98.75 ( 99.31)	Acc@5 100.00 (100.00)
## e[43] optimizer.zero_grad (sum) time: 0.4771556854248047
## e[43]       loss.backward (sum) time: 10.809144258499146
## e[43]      optimizer.step (sum) time: 47.551286697387695
## epoch[43] training(only) time: 94.2334897518158
# Switched to evaluate mode...
Test: [  0/100]	Time  0.207 ( 0.207)	Loss 2.6324e-01 (2.6324e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.078 ( 0.089)	Loss 2.0787e-01 (3.2974e-01)	Acc@1  95.00 ( 92.55)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 3.5807e-01 (3.3525e-01)	Acc@1  90.00 ( 92.29)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.079 ( 0.082)	Loss 3.1003e-01 (3.4799e-01)	Acc@1  90.00 ( 92.03)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 3.0510e-01 (3.3992e-01)	Acc@1  90.00 ( 92.15)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.078 ( 0.080)	Loss 2.5129e-01 (3.3486e-01)	Acc@1  94.00 ( 92.33)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 4.3516e-01 (3.2324e-01)	Acc@1  93.00 ( 92.48)	Acc@5 100.00 ( 99.72)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 4.3038e-01 (3.2307e-01)	Acc@1  92.00 ( 92.41)	Acc@5 100.00 ( 99.69)
Test: [ 80/100]	Time  0.078 ( 0.079)	Loss 2.1055e-01 (3.1569e-01)	Acc@1  93.00 ( 92.56)	Acc@5 100.00 ( 99.69)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.4120e-01 (3.1653e-01)	Acc@1  93.00 ( 92.48)	Acc@5 100.00 ( 99.73)
 * Acc@1 92.480 Acc@5 99.730
### epoch[43] execution time: 102.23244333267212
EPOCH 44
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [44][  0/391]	Time  0.396 ( 0.396)	Data  0.156 ( 0.156)	Loss 1.3144e-02 (1.3144e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [44][ 10/391]	Time  0.242 ( 0.255)	Data  0.001 ( 0.015)	Loss 4.1358e-02 (3.1042e-02)	Acc@1  97.66 ( 98.86)	Acc@5 100.00 (100.00)
Epoch: [44][ 20/391]	Time  0.240 ( 0.248)	Data  0.001 ( 0.009)	Loss 7.8965e-03 (2.7952e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [44][ 30/391]	Time  0.240 ( 0.246)	Data  0.001 ( 0.006)	Loss 7.7388e-03 (2.5579e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [44][ 40/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.005)	Loss 8.9605e-03 (2.3731e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [44][ 50/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 3.6302e-02 (2.4017e-02)	Acc@1  98.44 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [44][ 60/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.004)	Loss 2.7439e-02 (2.3286e-02)	Acc@1  97.66 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [44][ 70/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 7.5004e-03 (2.2806e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [44][ 80/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 7.7657e-03 (2.3057e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [44][ 90/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 4.6446e-02 (2.2642e-02)	Acc@1  97.66 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [44][100/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.9905e-02 (2.2470e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [44][110/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 4.2647e-02 (2.2854e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [44][120/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.4720e-02 (2.2730e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [44][130/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.8188e-02 (2.2810e-02)	Acc@1  98.44 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [44][140/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.2677e-02 (2.2527e-02)	Acc@1  98.44 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [44][150/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.6139e-03 (2.1846e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [44][160/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.3253e-02 (2.1791e-02)	Acc@1  98.44 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [44][170/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7123e-02 (2.2101e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [44][180/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.9591e-02 (2.2167e-02)	Acc@1  98.44 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [44][190/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.1384e-02 (2.1904e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [44][200/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.3450e-02 (2.1838e-02)	Acc@1  98.44 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [44][210/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.1614e-02 (2.1871e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [44][220/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.7271e-02 (2.1661e-02)	Acc@1  98.44 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [44][230/391]	Time  0.246 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0684e-02 (2.2375e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [44][240/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.2681e-02 (2.2524e-02)	Acc@1  98.44 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [44][250/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.3649e-02 (2.2607e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [44][260/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.1870e-02 (2.2638e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [44][270/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.4336e-02 (2.2622e-02)	Acc@1  98.44 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [44][280/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.4755e-02 (2.2530e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [44][290/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.1822e-02 (2.2504e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [44][300/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.7121e-02 (2.2571e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [44][310/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.5730e-03 (2.2387e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [44][320/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.0573e-02 (2.2403e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [44][330/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.3901e-02 (2.2270e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [44][340/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2205e-02 (2.2357e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [44][350/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.4882e-02 (2.2376e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [44][360/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.6540e-02 (2.2667e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [44][370/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.5417e-03 (2.2505e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [44][380/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.1728e-02 (2.2643e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [44][390/391]	Time  0.176 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2325e-02 (2.2498e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
## e[44] optimizer.zero_grad (sum) time: 0.47550106048583984
## e[44]       loss.backward (sum) time: 10.877127647399902
## e[44]      optimizer.step (sum) time: 47.51119875907898
## epoch[44] training(only) time: 94.33308625221252
# Switched to evaluate mode...
Test: [  0/100]	Time  0.211 ( 0.211)	Loss 2.9209e-01 (2.9209e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.7938e-01 (3.0593e-01)	Acc@1  94.00 ( 92.73)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 3.4791e-01 (3.0839e-01)	Acc@1  91.00 ( 93.00)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 2.6414e-01 (3.2498e-01)	Acc@1  92.00 ( 92.81)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 2.1570e-01 (3.1997e-01)	Acc@1  95.00 ( 92.90)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.077 ( 0.080)	Loss 2.3536e-01 (3.1792e-01)	Acc@1  94.00 ( 92.94)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.079 ( 0.080)	Loss 4.6923e-01 (3.0741e-01)	Acc@1  93.00 ( 92.93)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.077 ( 0.080)	Loss 4.7724e-01 (3.0631e-01)	Acc@1  92.00 ( 92.93)	Acc@5  99.00 ( 99.70)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 2.0804e-01 (3.0094e-01)	Acc@1  92.00 ( 93.02)	Acc@5 100.00 ( 99.69)
Test: [ 90/100]	Time  0.077 ( 0.079)	Loss 1.4707e-01 (3.0259e-01)	Acc@1  96.00 ( 92.97)	Acc@5 100.00 ( 99.73)
 * Acc@1 92.840 Acc@5 99.740
### epoch[44] execution time: 102.34052181243896
EPOCH 45
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [45][  0/391]	Time  0.392 ( 0.392)	Data  0.152 ( 0.152)	Loss 1.0555e-02 (1.0555e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [45][ 10/391]	Time  0.240 ( 0.254)	Data  0.001 ( 0.015)	Loss 1.5084e-02 (1.6489e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [45][ 20/391]	Time  0.241 ( 0.248)	Data  0.001 ( 0.008)	Loss 3.9747e-02 (2.0201e-02)	Acc@1  98.44 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [45][ 30/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.006)	Loss 4.2522e-03 (1.7181e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [45][ 40/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.005)	Loss 1.1459e-02 (1.8369e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [45][ 50/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.004)	Loss 1.3580e-02 (1.8671e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [45][ 60/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.004)	Loss 1.0744e-02 (1.8856e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [45][ 70/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 4.5147e-03 (1.8402e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [45][ 80/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.1309e-02 (1.7917e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [45][ 90/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.003)	Loss 2.1872e-02 (1.7928e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [45][100/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.003)	Loss 7.3576e-03 (1.8132e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [45][110/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0047e-02 (1.8082e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [45][120/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.4288e-02 (1.8593e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [45][130/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.7989e-02 (1.9450e-02)	Acc@1  97.66 ( 99.36)	Acc@5  99.22 ( 99.99)
Epoch: [45][140/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.3831e-03 (1.9728e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 ( 99.99)
Epoch: [45][150/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.7559e-02 (1.9545e-02)	Acc@1  98.44 ( 99.38)	Acc@5 100.00 ( 99.99)
Epoch: [45][160/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.7431e-02 (1.9672e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [45][170/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0597e-02 (1.9309e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [45][180/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0097e-02 (1.9175e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [45][190/391]	Time  0.246 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0854e-02 (1.9769e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [45][200/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.0797e-02 (1.9851e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [45][210/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.0265e-02 (1.9979e-02)	Acc@1  98.44 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [45][220/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.7076e-02 (1.9959e-02)	Acc@1  97.66 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [45][230/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.8370e-02 (1.9902e-02)	Acc@1  98.44 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [45][240/391]	Time  0.252 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1988e-02 (1.9865e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [45][250/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2564e-02 (1.9824e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [45][260/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.5839e-03 (1.9760e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [45][270/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.8280e-02 (1.9770e-02)	Acc@1  98.44 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [45][280/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.7161e-02 (1.9575e-02)	Acc@1  98.44 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [45][290/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.5246e-03 (1.9269e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [45][300/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2484e-02 (1.9316e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [45][310/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.9031e-03 (1.9538e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [45][320/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.4607e-02 (1.9666e-02)	Acc@1  98.44 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [45][330/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.5105e-02 (1.9669e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [45][340/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.3912e-03 (1.9656e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [45][350/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.1998e-02 (1.9890e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [45][360/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.4078e-02 (2.0163e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [45][370/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2841e-02 (2.0179e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [45][380/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.4270e-03 (2.0445e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [45][390/391]	Time  0.176 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.5917e-02 (2.0198e-02)	Acc@1  98.75 ( 99.36)	Acc@5 100.00 (100.00)
## e[45] optimizer.zero_grad (sum) time: 0.47496891021728516
## e[45]       loss.backward (sum) time: 10.794538259506226
## e[45]      optimizer.step (sum) time: 47.541422605514526
## epoch[45] training(only) time: 94.23013520240784
# Switched to evaluate mode...
Test: [  0/100]	Time  0.204 ( 0.204)	Loss 2.7360e-01 (2.7360e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.078 ( 0.089)	Loss 2.0664e-01 (3.2223e-01)	Acc@1  95.00 ( 92.45)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 3.2398e-01 (3.2555e-01)	Acc@1  92.00 ( 92.43)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 2.4588e-01 (3.3808e-01)	Acc@1  91.00 ( 92.29)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 2.2395e-01 (3.3411e-01)	Acc@1  94.00 ( 92.44)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.078 ( 0.080)	Loss 2.7253e-01 (3.2932e-01)	Acc@1  93.00 ( 92.49)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 4.9521e-01 (3.1956e-01)	Acc@1  91.00 ( 92.54)	Acc@5 100.00 ( 99.72)
Test: [ 70/100]	Time  0.077 ( 0.080)	Loss 4.2814e-01 (3.1678e-01)	Acc@1  92.00 ( 92.62)	Acc@5 100.00 ( 99.72)
Test: [ 80/100]	Time  0.079 ( 0.079)	Loss 2.2346e-01 (3.1139e-01)	Acc@1  95.00 ( 92.83)	Acc@5 100.00 ( 99.72)
Test: [ 90/100]	Time  0.077 ( 0.079)	Loss 1.4254e-01 (3.1358e-01)	Acc@1  94.00 ( 92.75)	Acc@5 100.00 ( 99.75)
 * Acc@1 92.720 Acc@5 99.760
### epoch[45] execution time: 102.23249506950378
EPOCH 46
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [46][  0/391]	Time  0.387 ( 0.387)	Data  0.147 ( 0.147)	Loss 2.8647e-02 (2.8647e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [46][ 10/391]	Time  0.240 ( 0.253)	Data  0.001 ( 0.014)	Loss 9.0122e-03 (1.5553e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [46][ 20/391]	Time  0.241 ( 0.247)	Data  0.001 ( 0.008)	Loss 1.7642e-02 (1.7756e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [46][ 30/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.006)	Loss 1.6150e-02 (1.6972e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [46][ 40/391]	Time  0.239 ( 0.244)	Data  0.001 ( 0.005)	Loss 2.7454e-02 (1.7719e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [46][ 50/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.8436e-02 (1.7829e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [46][ 60/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.004)	Loss 2.8000e-02 (1.7359e-02)	Acc@1  98.44 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [46][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.5770e-02 (1.8001e-02)	Acc@1  98.44 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [46][ 80/391]	Time  0.239 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.0296e-02 (1.7742e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [46][ 90/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 2.8768e-02 (1.7609e-02)	Acc@1  98.44 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [46][100/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 2.3478e-02 (1.7803e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [46][110/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 9.9424e-03 (1.7307e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [46][120/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.6102e-03 (1.6867e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [46][130/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3817e-02 (1.7855e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [46][140/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2623e-02 (1.8110e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [46][150/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2884e-02 (1.8298e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [46][160/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0578e-02 (1.8109e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [46][170/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2607e-02 (1.8398e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [46][180/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.2351e-03 (1.8361e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [46][190/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.8941e-02 (1.8305e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [46][200/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.5722e-02 (1.8837e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [46][210/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.3391e-02 (1.9047e-02)	Acc@1  97.66 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [46][220/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.5442e-02 (1.8792e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [46][230/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.8040e-03 (1.8624e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [46][240/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2812e-02 (1.8825e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [46][250/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.5345e-02 (1.8778e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [46][260/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1887e-02 (1.8756e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [46][270/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.4334e-03 (1.8577e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [46][280/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.3342e-02 (1.8453e-02)	Acc@1  98.44 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [46][290/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.0848e-02 (1.8619e-02)	Acc@1  98.44 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [46][300/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.9474e-03 (1.8487e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [46][310/391]	Time  0.240 ( 0.241)	Data  0.002 ( 0.002)	Loss 9.1208e-03 (1.8551e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [46][320/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.2838e-02 (1.8688e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [46][330/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.7443e-02 (1.8616e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [46][340/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.3829e-02 (1.8675e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [46][350/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.7004e-03 (1.8696e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [46][360/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.2792e-02 (1.8617e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [46][370/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.6368e-03 (1.8595e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [46][380/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.7898e-02 (1.8480e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [46][390/391]	Time  0.176 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.5948e-02 (1.8464e-02)	Acc@1  98.75 ( 99.47)	Acc@5 100.00 (100.00)
## e[46] optimizer.zero_grad (sum) time: 0.4786195755004883
## e[46]       loss.backward (sum) time: 10.849061489105225
## e[46]      optimizer.step (sum) time: 47.47175598144531
## epoch[46] training(only) time: 94.20904850959778
# Switched to evaluate mode...
Test: [  0/100]	Time  0.208 ( 0.208)	Loss 2.8040e-01 (2.8040e-01)	Acc@1  94.00 ( 94.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.077 ( 0.090)	Loss 1.9035e-01 (3.1714e-01)	Acc@1  95.00 ( 93.18)	Acc@5 100.00 ( 99.55)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 3.5849e-01 (3.1799e-01)	Acc@1  93.00 ( 93.10)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.077 ( 0.082)	Loss 1.7117e-01 (3.3175e-01)	Acc@1  95.00 ( 93.03)	Acc@5 100.00 ( 99.61)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 2.0487e-01 (3.2914e-01)	Acc@1  94.00 ( 92.85)	Acc@5 100.00 ( 99.66)
Test: [ 50/100]	Time  0.077 ( 0.080)	Loss 2.6626e-01 (3.2778e-01)	Acc@1  93.00 ( 92.82)	Acc@5 100.00 ( 99.67)
Test: [ 60/100]	Time  0.079 ( 0.080)	Loss 4.4582e-01 (3.1725e-01)	Acc@1  94.00 ( 92.87)	Acc@5 100.00 ( 99.69)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 4.7331e-01 (3.1409e-01)	Acc@1  91.00 ( 92.89)	Acc@5 100.00 ( 99.70)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 2.0737e-01 (3.0925e-01)	Acc@1  95.00 ( 93.00)	Acc@5 100.00 ( 99.70)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.1716e-01 (3.1230e-01)	Acc@1  95.00 ( 92.91)	Acc@5 100.00 ( 99.73)
 * Acc@1 92.820 Acc@5 99.740
### epoch[46] execution time: 102.23451399803162
EPOCH 47
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [47][  0/391]	Time  0.388 ( 0.388)	Data  0.132 ( 0.132)	Loss 2.4538e-02 (2.4538e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [47][ 10/391]	Time  0.240 ( 0.253)	Data  0.001 ( 0.013)	Loss 5.0727e-03 (1.7724e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [47][ 20/391]	Time  0.240 ( 0.247)	Data  0.001 ( 0.007)	Loss 1.5838e-02 (1.3446e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [47][ 30/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.005)	Loss 1.8596e-02 (1.3059e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [47][ 40/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 2.1425e-02 (1.4956e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [47][ 50/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.004)	Loss 3.1087e-02 (1.5430e-02)	Acc@1  96.88 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [47][ 60/391]	Time  0.239 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.7478e-02 (1.5014e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [47][ 70/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.8146e-02 (1.5312e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [47][ 80/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.1527e-02 (1.5808e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [47][ 90/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 2.0345e-03 (1.5536e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [47][100/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.0584e-03 (1.5508e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [47][110/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.6120e-03 (1.5344e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [47][120/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.7005e-03 (1.6031e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [47][130/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.7426e-03 (1.6170e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [47][140/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.1599e-03 (1.5970e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [47][150/391]	Time  0.240 ( 0.241)	Data  0.002 ( 0.002)	Loss 1.1386e-02 (1.6006e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [47][160/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.8126e-02 (1.5708e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [47][170/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.5899e-02 (1.5767e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [47][180/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.8463e-02 (1.5668e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [47][190/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.5313e-02 (1.5813e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [47][200/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.8927e-02 (1.6070e-02)	Acc@1  97.66 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [47][210/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.0681e-03 (1.6280e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [47][220/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.2946e-03 (1.6213e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [47][230/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.6587e-02 (1.6411e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [47][240/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.2082e-03 (1.6454e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [47][250/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.1311e-02 (1.6631e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [47][260/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.7217e-03 (1.6607e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [47][270/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.7158e-03 (1.6617e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [47][280/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.0364e-03 (1.6597e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [47][290/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.9442e-02 (1.6693e-02)	Acc@1  98.44 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [47][300/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.7656e-02 (1.6687e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [47][310/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.6112e-02 (1.6598e-02)	Acc@1  98.44 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [47][320/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1694e-02 (1.6581e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [47][330/391]	Time  0.247 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.6450e-03 (1.6487e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [47][340/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.8831e-02 (1.6631e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [47][350/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.8827e-02 (1.6673e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [47][360/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.7497e-03 (1.6777e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [47][370/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.6882e-02 (1.6925e-02)	Acc@1  98.44 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [47][380/391]	Time  0.248 ( 0.241)	Data  0.001 ( 0.001)	Loss 2.2842e-02 (1.6794e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [47][390/391]	Time  0.177 ( 0.241)	Data  0.001 ( 0.001)	Loss 1.3098e-02 (1.6868e-02)	Acc@1  98.75 ( 99.49)	Acc@5 100.00 (100.00)
## e[47] optimizer.zero_grad (sum) time: 0.47568464279174805
## e[47]       loss.backward (sum) time: 10.815059423446655
## e[47]      optimizer.step (sum) time: 47.5374710559845
## epoch[47] training(only) time: 94.2754237651825
# Switched to evaluate mode...
Test: [  0/100]	Time  0.203 ( 0.203)	Loss 2.4293e-01 (2.4293e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.077 ( 0.089)	Loss 1.7081e-01 (3.0476e-01)	Acc@1  95.00 ( 93.09)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 3.6183e-01 (3.1032e-01)	Acc@1  92.00 ( 92.71)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.077 ( 0.082)	Loss 2.3804e-01 (3.2954e-01)	Acc@1  94.00 ( 92.52)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 2.3291e-01 (3.3066e-01)	Acc@1  92.00 ( 92.56)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.077 ( 0.080)	Loss 2.5919e-01 (3.2682e-01)	Acc@1  94.00 ( 92.67)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 5.3837e-01 (3.1832e-01)	Acc@1  90.00 ( 92.67)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 4.4686e-01 (3.1718e-01)	Acc@1  91.00 ( 92.75)	Acc@5  99.00 ( 99.72)
Test: [ 80/100]	Time  0.078 ( 0.079)	Loss 1.8122e-01 (3.1057e-01)	Acc@1  95.00 ( 92.89)	Acc@5 100.00 ( 99.70)
Test: [ 90/100]	Time  0.077 ( 0.079)	Loss 1.1790e-01 (3.1256e-01)	Acc@1  96.00 ( 92.85)	Acc@5 100.00 ( 99.74)
 * Acc@1 92.810 Acc@5 99.750
### epoch[47] execution time: 102.2948808670044
EPOCH 48
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [48][  0/391]	Time  0.398 ( 0.398)	Data  0.158 ( 0.158)	Loss 2.6334e-02 (2.6334e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [48][ 10/391]	Time  0.246 ( 0.255)	Data  0.001 ( 0.015)	Loss 1.8565e-02 (1.9438e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [48][ 20/391]	Time  0.240 ( 0.248)	Data  0.001 ( 0.009)	Loss 1.3136e-02 (1.6330e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [48][ 30/391]	Time  0.239 ( 0.245)	Data  0.001 ( 0.006)	Loss 1.6335e-02 (1.7106e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [48][ 40/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.005)	Loss 2.8579e-03 (1.6591e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [48][ 50/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.004)	Loss 2.1454e-02 (1.6341e-02)	Acc@1  98.44 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [48][ 60/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.004)	Loss 4.0296e-03 (1.5761e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [48][ 70/391]	Time  0.239 ( 0.243)	Data  0.001 ( 0.003)	Loss 7.7506e-03 (1.6033e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [48][ 80/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.9188e-03 (1.5897e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [48][ 90/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 9.9244e-03 (1.5883e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [48][100/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 2.0813e-02 (1.6065e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [48][110/391]	Time  0.246 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.3978e-02 (1.6181e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [48][120/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3053e-02 (1.5745e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [48][130/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2566e-02 (1.5729e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [48][140/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.4966e-03 (1.5931e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [48][150/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2619e-02 (1.5886e-02)	Acc@1  98.44 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [48][160/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.3085e-02 (1.6457e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [48][170/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.4861e-03 (1.6289e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [48][180/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.7205e-02 (1.6197e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [48][190/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.8226e-03 (1.6233e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [48][200/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.8509e-03 (1.6082e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [48][210/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.8116e-03 (1.6258e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [48][220/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.8566e-02 (1.6198e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [48][230/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.0682e-03 (1.6128e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [48][240/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1149e-02 (1.6214e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [48][250/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.5952e-02 (1.6400e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [48][260/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.6876e-02 (1.6638e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [48][270/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.4108e-02 (1.6529e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [48][280/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.7167e-02 (1.6320e-02)	Acc@1  98.44 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [48][290/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.4517e-03 (1.6257e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [48][300/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.7590e-02 (1.6107e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [48][310/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.6917e-02 (1.6224e-02)	Acc@1  98.44 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [48][320/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.4066e-03 (1.6266e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [48][330/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.4151e-03 (1.6375e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [48][340/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.7114e-02 (1.6455e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [48][350/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.1643e-02 (1.6607e-02)	Acc@1  98.44 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [48][360/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.7647e-02 (1.6613e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [48][370/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2827e-02 (1.6645e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [48][380/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.8195e-02 (1.6636e-02)	Acc@1  98.44 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [48][390/391]	Time  0.177 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.7203e-02 (1.6634e-02)	Acc@1  98.75 ( 99.49)	Acc@5 100.00 (100.00)
## e[48] optimizer.zero_grad (sum) time: 0.47567319869995117
## e[48]       loss.backward (sum) time: 10.823885679244995
## e[48]      optimizer.step (sum) time: 47.544973850250244
## epoch[48] training(only) time: 94.31831049919128
# Switched to evaluate mode...
Test: [  0/100]	Time  0.206 ( 0.206)	Loss 2.8790e-01 (2.8790e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.4543e-01 (3.1321e-01)	Acc@1  96.00 ( 93.00)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 4.6685e-01 (3.3442e-01)	Acc@1  89.00 ( 92.52)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 3.0851e-01 (3.4754e-01)	Acc@1  90.00 ( 92.29)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.077 ( 0.081)	Loss 2.2960e-01 (3.4767e-01)	Acc@1  93.00 ( 92.44)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.077 ( 0.080)	Loss 2.0421e-01 (3.4676e-01)	Acc@1  95.00 ( 92.49)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 4.9981e-01 (3.3433e-01)	Acc@1  93.00 ( 92.57)	Acc@5 100.00 ( 99.72)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 4.5725e-01 (3.3014e-01)	Acc@1  90.00 ( 92.58)	Acc@5 100.00 ( 99.72)
Test: [ 80/100]	Time  0.078 ( 0.079)	Loss 1.9739e-01 (3.2599e-01)	Acc@1  95.00 ( 92.63)	Acc@5 100.00 ( 99.70)
Test: [ 90/100]	Time  0.077 ( 0.079)	Loss 1.0762e-01 (3.2605e-01)	Acc@1  97.00 ( 92.63)	Acc@5 100.00 ( 99.74)
 * Acc@1 92.580 Acc@5 99.750
### epoch[48] execution time: 102.31904602050781
EPOCH 49
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [49][  0/391]	Time  0.387 ( 0.387)	Data  0.144 ( 0.144)	Loss 2.2814e-03 (2.2814e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [49][ 10/391]	Time  0.240 ( 0.254)	Data  0.001 ( 0.014)	Loss 2.3429e-03 (1.3815e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [49][ 20/391]	Time  0.239 ( 0.248)	Data  0.001 ( 0.008)	Loss 3.8009e-02 (1.9190e-02)	Acc@1  98.44 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [49][ 30/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.006)	Loss 3.5918e-03 (2.0395e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [49][ 40/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.005)	Loss 1.5969e-02 (2.1253e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [49][ 50/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 4.2934e-03 (1.9623e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [49][ 60/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.8174e-02 (1.8123e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [49][ 70/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.5476e-02 (1.8769e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [49][ 80/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.0421e-02 (1.8773e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [49][ 90/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 3.4972e-02 (1.8009e-02)	Acc@1  98.44 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [49][100/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 8.0460e-03 (1.7478e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [49][110/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5219e-02 (1.7402e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [49][120/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.4091e-02 (1.7139e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [49][130/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9091e-02 (1.6629e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [49][140/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.5286e-03 (1.6573e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [49][150/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3222e-02 (1.6779e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [49][160/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.4991e-02 (1.7099e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [49][170/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2686e-02 (1.7247e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [49][180/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.4697e-02 (1.7194e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [49][190/391]	Time  0.243 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.8199e-02 (1.7324e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [49][200/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.3735e-03 (1.7181e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [49][210/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2465e-02 (1.7289e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [49][220/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.3655e-02 (1.7061e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [49][230/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.7993e-02 (1.7062e-02)	Acc@1  97.66 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [49][240/391]	Time  0.245 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.7155e-02 (1.6773e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [49][250/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.3836e-02 (1.6927e-02)	Acc@1  96.88 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [49][260/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.6268e-02 (1.6752e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [49][270/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2707e-02 (1.6607e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [49][280/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.3615e-02 (1.6480e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [49][290/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.4683e-02 (1.6670e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [49][300/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.7387e-03 (1.6694e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [49][310/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.6599e-03 (1.6501e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [49][320/391]	Time  0.247 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.1910e-02 (1.6562e-02)	Acc@1  96.88 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [49][330/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.7928e-02 (1.6445e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [49][340/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2062e-03 (1.6608e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [49][350/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.0161e-03 (1.6752e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [49][360/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1817e-02 (1.6670e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [49][370/391]	Time  0.248 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1418e-02 (1.6632e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [49][380/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.3571e-02 (1.6578e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [49][390/391]	Time  0.177 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.7862e-02 (1.6547e-02)	Acc@1  98.75 ( 99.48)	Acc@5 100.00 (100.00)
## e[49] optimizer.zero_grad (sum) time: 0.48052430152893066
## e[49]       loss.backward (sum) time: 10.88092303276062
## e[49]      optimizer.step (sum) time: 47.55819535255432
## epoch[49] training(only) time: 94.392507314682
# Switched to evaluate mode...
Test: [  0/100]	Time  0.210 ( 0.210)	Loss 2.3849e-01 (2.3849e-01)	Acc@1  94.00 ( 94.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.6026e-01 (3.2487e-01)	Acc@1  96.00 ( 92.45)	Acc@5 100.00 ( 99.55)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 3.8113e-01 (3.2953e-01)	Acc@1  91.00 ( 92.81)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.077 ( 0.082)	Loss 2.7390e-01 (3.4630e-01)	Acc@1  92.00 ( 92.23)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 2.2877e-01 (3.4565e-01)	Acc@1  92.00 ( 92.29)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 1.8157e-01 (3.4159e-01)	Acc@1  95.00 ( 92.41)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.079 ( 0.080)	Loss 4.7819e-01 (3.2829e-01)	Acc@1  92.00 ( 92.59)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.079 ( 0.080)	Loss 4.7269e-01 (3.2624e-01)	Acc@1  91.00 ( 92.58)	Acc@5 100.00 ( 99.72)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 2.1973e-01 (3.2046e-01)	Acc@1  94.00 ( 92.72)	Acc@5 100.00 ( 99.70)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.4318e-01 (3.2426e-01)	Acc@1  94.00 ( 92.66)	Acc@5 100.00 ( 99.74)
 * Acc@1 92.600 Acc@5 99.750
### epoch[49] execution time: 102.40832233428955
EPOCH 50
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [50][  0/391]	Time  0.401 ( 0.401)	Data  0.152 ( 0.152)	Loss 4.1563e-03 (4.1563e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [50][ 10/391]	Time  0.240 ( 0.256)	Data  0.001 ( 0.015)	Loss 5.3356e-03 (9.3178e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [50][ 20/391]	Time  0.243 ( 0.249)	Data  0.001 ( 0.008)	Loss 9.4510e-03 (1.1855e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [50][ 30/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.006)	Loss 1.8419e-02 (1.3362e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [50][ 40/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.005)	Loss 1.0831e-02 (1.5823e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [50][ 50/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.0722e-02 (1.5396e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [50][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 2.1670e-02 (1.4576e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [50][ 70/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.8109e-02 (1.4006e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [50][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.1570e-02 (1.3997e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [50][ 90/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 5.6506e-03 (1.4361e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [50][100/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.7434e-02 (1.4179e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [50][110/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 2.8578e-02 (1.4574e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [50][120/391]	Time  0.247 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.5646e-03 (1.4376e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [50][130/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.4043e-03 (1.3961e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [50][140/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.5599e-03 (1.3702e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [50][150/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.7188e-03 (1.3690e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [50][160/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.9105e-03 (1.3949e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [50][170/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.2723e-03 (1.4266e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [50][180/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.4413e-03 (1.3924e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [50][190/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3588e-02 (1.4039e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [50][200/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.7690e-03 (1.4173e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [50][210/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0590e-02 (1.4347e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [50][220/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.3712e-02 (1.4347e-02)	Acc@1  98.44 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [50][230/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.8494e-02 (1.4479e-02)	Acc@1  98.44 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [50][240/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.6493e-02 (1.4637e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [50][250/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.2885e-03 (1.4900e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [50][260/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.6126e-02 (1.4971e-02)	Acc@1  98.44 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [50][270/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.6965e-03 (1.4967e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [50][280/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.1048e-02 (1.4882e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [50][290/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2124e-02 (1.4891e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [50][300/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.9569e-02 (1.4998e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [50][310/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.4123e-03 (1.4857e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [50][320/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.3222e-02 (1.4841e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [50][330/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.0885e-02 (1.4811e-02)	Acc@1  98.44 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [50][340/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.5247e-02 (1.4897e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [50][350/391]	Time  0.243 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.1217e-03 (1.5090e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [50][360/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.0067e-02 (1.5319e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [50][370/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.5802e-02 (1.5304e-02)	Acc@1  98.44 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [50][380/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.5293e-03 (1.5182e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [50][390/391]	Time  0.176 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.7180e-02 (1.5368e-02)	Acc@1  98.75 ( 99.55)	Acc@5 100.00 (100.00)
## e[50] optimizer.zero_grad (sum) time: 0.4804694652557373
## e[50]       loss.backward (sum) time: 10.868770360946655
## e[50]      optimizer.step (sum) time: 47.588191986083984
## epoch[50] training(only) time: 94.39872193336487
# Switched to evaluate mode...
Test: [  0/100]	Time  0.209 ( 0.209)	Loss 2.5845e-01 (2.5845e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.7320e-01 (3.5052e-01)	Acc@1  96.00 ( 92.73)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 4.1518e-01 (3.4386e-01)	Acc@1  90.00 ( 92.57)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.077 ( 0.082)	Loss 2.9398e-01 (3.6611e-01)	Acc@1  92.00 ( 92.29)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 2.6662e-01 (3.5855e-01)	Acc@1  94.00 ( 92.37)	Acc@5 100.00 ( 99.66)
Test: [ 50/100]	Time  0.078 ( 0.080)	Loss 1.9106e-01 (3.5637e-01)	Acc@1  95.00 ( 92.39)	Acc@5 100.00 ( 99.67)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 5.4940e-01 (3.4733e-01)	Acc@1  93.00 ( 92.43)	Acc@5 100.00 ( 99.69)
Test: [ 70/100]	Time  0.077 ( 0.080)	Loss 5.5603e-01 (3.4438e-01)	Acc@1  91.00 ( 92.41)	Acc@5 100.00 ( 99.69)
Test: [ 80/100]	Time  0.078 ( 0.079)	Loss 2.3973e-01 (3.3925e-01)	Acc@1  94.00 ( 92.49)	Acc@5 100.00 ( 99.67)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.4887e-01 (3.4238e-01)	Acc@1  94.00 ( 92.47)	Acc@5 100.00 ( 99.69)
 * Acc@1 92.380 Acc@5 99.710
### epoch[50] execution time: 102.40051174163818
EPOCH 51
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [51][  0/391]	Time  0.402 ( 0.402)	Data  0.157 ( 0.157)	Loss 4.2296e-03 (4.2296e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [51][ 10/391]	Time  0.241 ( 0.255)	Data  0.001 ( 0.015)	Loss 3.6024e-02 (1.2932e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [51][ 20/391]	Time  0.240 ( 0.248)	Data  0.001 ( 0.008)	Loss 5.7025e-02 (1.8437e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [51][ 30/391]	Time  0.248 ( 0.246)	Data  0.001 ( 0.006)	Loss 1.7289e-02 (1.7256e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [51][ 40/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.005)	Loss 1.0890e-02 (1.5912e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [51][ 50/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 3.3821e-03 (1.5521e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [51][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 7.6113e-03 (1.5233e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [51][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.3136e-02 (1.4057e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [51][ 80/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.4758e-02 (1.3588e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [51][ 90/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 6.5778e-03 (1.4054e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [51][100/391]	Time  0.239 ( 0.243)	Data  0.001 ( 0.003)	Loss 7.2692e-03 (1.4046e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [51][110/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.4363e-03 (1.4092e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [51][120/391]	Time  0.239 ( 0.243)	Data  0.001 ( 0.002)	Loss 5.5544e-03 (1.4723e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [51][130/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.0143e-03 (1.4515e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [51][140/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2476e-02 (1.4991e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [51][150/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.0400e-02 (1.5582e-02)	Acc@1  98.44 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [51][160/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.1409e-03 (1.5468e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [51][170/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.8828e-03 (1.5329e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [51][180/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.1087e-03 (1.5234e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [51][190/391]	Time  0.250 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.4452e-02 (1.5182e-02)	Acc@1  98.44 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [51][200/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.2682e-03 (1.5217e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [51][210/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8075e-02 (1.5235e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [51][220/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.8839e-03 (1.5320e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [51][230/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.0342e-02 (1.5412e-02)	Acc@1  97.66 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [51][240/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0310e-02 (1.5615e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [51][250/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8202e-02 (1.5593e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [51][260/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.6377e-02 (1.5686e-02)	Acc@1  98.44 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [51][270/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5527e-02 (1.5758e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [51][280/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.9929e-03 (1.5722e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [51][290/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.5345e-03 (1.5681e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [51][300/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7686e-02 (1.5604e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [51][310/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.6776e-03 (1.5393e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [51][320/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.9879e-02 (1.5360e-02)	Acc@1  98.44 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [51][330/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.3653e-02 (1.5258e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [51][340/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0427e-02 (1.5180e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [51][350/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.3793e-02 (1.5097e-02)	Acc@1  98.44 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [51][360/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1708e-02 (1.5187e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [51][370/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.4192e-02 (1.5263e-02)	Acc@1  98.44 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [51][380/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0990e-02 (1.5224e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [51][390/391]	Time  0.178 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.1517e-02 (1.5253e-02)	Acc@1  98.75 ( 99.52)	Acc@5 100.00 (100.00)
## e[51] optimizer.zero_grad (sum) time: 0.4763815402984619
## e[51]       loss.backward (sum) time: 10.918630123138428
## e[51]      optimizer.step (sum) time: 47.54756450653076
## epoch[51] training(only) time: 94.45225048065186
# Switched to evaluate mode...
Test: [  0/100]	Time  0.214 ( 0.214)	Loss 2.7476e-01 (2.7476e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.077 ( 0.090)	Loss 2.0170e-01 (3.4917e-01)	Acc@1  96.00 ( 92.36)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.079 ( 0.085)	Loss 4.1287e-01 (3.5164e-01)	Acc@1  92.00 ( 92.38)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.077 ( 0.082)	Loss 1.9217e-01 (3.5353e-01)	Acc@1  94.00 ( 92.13)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 2.5388e-01 (3.4953e-01)	Acc@1  94.00 ( 92.15)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 2.1449e-01 (3.4686e-01)	Acc@1  94.00 ( 92.25)	Acc@5 100.00 ( 99.67)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 5.3208e-01 (3.3455e-01)	Acc@1  92.00 ( 92.31)	Acc@5 100.00 ( 99.67)
Test: [ 70/100]	Time  0.077 ( 0.080)	Loss 5.2560e-01 (3.3630e-01)	Acc@1  91.00 ( 92.31)	Acc@5  99.00 ( 99.63)
Test: [ 80/100]	Time  0.079 ( 0.080)	Loss 1.9305e-01 (3.3168e-01)	Acc@1  95.00 ( 92.52)	Acc@5 100.00 ( 99.63)
Test: [ 90/100]	Time  0.078 ( 0.080)	Loss 1.2702e-01 (3.3249e-01)	Acc@1  95.00 ( 92.45)	Acc@5 100.00 ( 99.67)
 * Acc@1 92.410 Acc@5 99.690
### epoch[51] execution time: 102.50613307952881
EPOCH 52
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [52][  0/391]	Time  0.396 ( 0.396)	Data  0.129 ( 0.129)	Loss 4.4048e-03 (4.4048e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [52][ 10/391]	Time  0.240 ( 0.255)	Data  0.001 ( 0.013)	Loss 3.2916e-02 (1.0550e-02)	Acc@1  98.44 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [52][ 20/391]	Time  0.242 ( 0.249)	Data  0.001 ( 0.007)	Loss 1.2966e-02 (9.8796e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [52][ 30/391]	Time  0.240 ( 0.246)	Data  0.001 ( 0.005)	Loss 9.9134e-03 (1.2310e-02)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [52][ 40/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.004)	Loss 6.9631e-03 (1.2613e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [52][ 50/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 3.8924e-03 (1.2073e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [52][ 60/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.003)	Loss 3.5473e-03 (1.2201e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [52][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.0494e-02 (1.2357e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [52][ 80/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 5.2806e-02 (1.3700e-02)	Acc@1  98.44 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [52][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 7.3607e-03 (1.3128e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [52][100/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.3613e-03 (1.3213e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [52][110/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6862e-02 (1.3114e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [52][120/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.7285e-03 (1.3007e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [52][130/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0112e-02 (1.2925e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [52][140/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5810e-03 (1.2612e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [52][150/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.8845e-03 (1.2688e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [52][160/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8553e-02 (1.3119e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [52][170/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0927e-02 (1.3312e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [52][180/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8257e-02 (1.3312e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [52][190/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9472e-03 (1.3451e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [52][200/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1413e-02 (1.3162e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [52][210/391]	Time  0.246 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5212e-02 (1.3191e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [52][220/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.9645e-03 (1.3070e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [52][230/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.0290e-03 (1.3004e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [52][240/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.7854e-02 (1.3063e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [52][250/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.5472e-03 (1.3170e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [52][260/391]	Time  0.243 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.0422e-02 (1.3200e-02)	Acc@1  98.44 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [52][270/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.6619e-03 (1.3115e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [52][280/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.5312e-02 (1.3369e-02)	Acc@1  98.44 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [52][290/391]	Time  0.243 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.8621e-03 (1.3398e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [52][300/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.4821e-02 (1.3467e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [52][310/391]	Time  0.246 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.3095e-03 (1.3269e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [52][320/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.9541e-02 (1.3449e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [52][330/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.4712e-03 (1.3467e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [52][340/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.2455e-02 (1.3554e-02)	Acc@1  97.66 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [52][350/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.7842e-02 (1.3537e-02)	Acc@1  97.66 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [52][360/391]	Time  0.244 ( 0.241)	Data  0.001 ( 0.001)	Loss 2.0368e-03 (1.3591e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [52][370/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.001)	Loss 5.4857e-03 (1.3583e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [52][380/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.001)	Loss 1.5809e-02 (1.3621e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [52][390/391]	Time  0.176 ( 0.241)	Data  0.001 ( 0.001)	Loss 3.2389e-02 (1.3550e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
## e[52] optimizer.zero_grad (sum) time: 0.4757876396179199
## e[52]       loss.backward (sum) time: 10.87522029876709
## e[52]      optimizer.step (sum) time: 47.562297105789185
## epoch[52] training(only) time: 94.40070843696594
# Switched to evaluate mode...
Test: [  0/100]	Time  0.209 ( 0.209)	Loss 2.7556e-01 (2.7556e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.6959e-01 (3.5147e-01)	Acc@1  95.00 ( 92.18)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 4.3719e-01 (3.5472e-01)	Acc@1  88.00 ( 92.33)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 2.6716e-01 (3.7557e-01)	Acc@1  94.00 ( 92.13)	Acc@5  99.00 ( 99.65)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 2.6222e-01 (3.6689e-01)	Acc@1  93.00 ( 92.10)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.077 ( 0.080)	Loss 2.0764e-01 (3.6351e-01)	Acc@1  94.00 ( 92.20)	Acc@5 100.00 ( 99.69)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 4.9636e-01 (3.4882e-01)	Acc@1  91.00 ( 92.34)	Acc@5 100.00 ( 99.69)
Test: [ 70/100]	Time  0.077 ( 0.080)	Loss 4.6329e-01 (3.4542e-01)	Acc@1  91.00 ( 92.38)	Acc@5  99.00 ( 99.68)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 2.6514e-01 (3.4089e-01)	Acc@1  94.00 ( 92.52)	Acc@5 100.00 ( 99.65)
Test: [ 90/100]	Time  0.077 ( 0.079)	Loss 1.4944e-01 (3.4009e-01)	Acc@1  95.00 ( 92.51)	Acc@5 100.00 ( 99.67)
 * Acc@1 92.450 Acc@5 99.690
### epoch[52] execution time: 102.4107449054718
EPOCH 53
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [53][  0/391]	Time  0.380 ( 0.380)	Data  0.139 ( 0.139)	Loss 1.3504e-02 (1.3504e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [53][ 10/391]	Time  0.241 ( 0.253)	Data  0.001 ( 0.014)	Loss 4.0520e-03 (7.9517e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [53][ 20/391]	Time  0.241 ( 0.247)	Data  0.001 ( 0.008)	Loss 8.9927e-03 (1.2063e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [53][ 30/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.006)	Loss 3.6446e-02 (1.4129e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 ( 99.97)
Epoch: [53][ 40/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.005)	Loss 1.1296e-02 (1.3305e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 ( 99.98)
Epoch: [53][ 50/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 4.2083e-02 (1.5083e-02)	Acc@1  98.44 ( 99.59)	Acc@5 100.00 ( 99.98)
Epoch: [53][ 60/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 5.5676e-03 (1.4787e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 ( 99.99)
Epoch: [53][ 70/391]	Time  0.239 ( 0.243)	Data  0.001 ( 0.003)	Loss 4.3044e-02 (1.4627e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 ( 99.99)
Epoch: [53][ 80/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 5.9420e-02 (1.5059e-02)	Acc@1  97.66 ( 99.57)	Acc@5 100.00 ( 99.99)
Epoch: [53][ 90/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.6597e-02 (1.5152e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 ( 99.99)
Epoch: [53][100/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.003)	Loss 3.9395e-03 (1.4399e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 ( 99.99)
Epoch: [53][110/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.3372e-03 (1.3875e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 ( 99.99)
Epoch: [53][120/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3666e-02 (1.3575e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 ( 99.99)
Epoch: [53][130/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7118e-02 (1.3277e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 ( 99.99)
Epoch: [53][140/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.3845e-03 (1.2908e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 ( 99.99)
Epoch: [53][150/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2858e-03 (1.2708e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 ( 99.99)
Epoch: [53][160/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3129e-02 (1.3162e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [53][170/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1575e-02 (1.3237e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [53][180/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.8420e-03 (1.3125e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [53][190/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.3012e-03 (1.2967e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [53][200/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7686e-02 (1.2909e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [53][210/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.2928e-02 (1.3100e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [53][220/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.6444e-02 (1.3321e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [53][230/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.1076e-03 (1.3924e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [53][240/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2698e-02 (1.3918e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [53][250/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.5991e-03 (1.3851e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [53][260/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.4022e-03 (1.3994e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [53][270/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.5341e-02 (1.3893e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [53][280/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.6789e-02 (1.3957e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [53][290/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.8645e-03 (1.3858e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [53][300/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0040e-02 (1.3837e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [53][310/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.7139e-03 (1.3849e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [53][320/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.4607e-03 (1.3893e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [53][330/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.5606e-03 (1.3882e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [53][340/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.4772e-02 (1.3795e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [53][350/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.9136e-03 (1.3905e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [53][360/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.8615e-03 (1.3821e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [53][370/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.5378e-03 (1.3718e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [53][380/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.5410e-03 (1.3657e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [53][390/391]	Time  0.177 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.6283e-02 (1.3657e-02)	Acc@1  98.75 ( 99.62)	Acc@5 100.00 (100.00)
## e[53] optimizer.zero_grad (sum) time: 0.47786450386047363
## e[53]       loss.backward (sum) time: 10.852506399154663
## e[53]      optimizer.step (sum) time: 47.556307792663574
## epoch[53] training(only) time: 94.3925552368164
# Switched to evaluate mode...
Test: [  0/100]	Time  0.219 ( 0.219)	Loss 2.3374e-01 (2.3374e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.6499e-01 (3.5112e-01)	Acc@1  96.00 ( 92.55)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.077 ( 0.084)	Loss 4.2752e-01 (3.4643e-01)	Acc@1  90.00 ( 92.33)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 3.3025e-01 (3.6445e-01)	Acc@1  92.00 ( 92.26)	Acc@5 100.00 ( 99.77)
Test: [ 40/100]	Time  0.077 ( 0.081)	Loss 2.9760e-01 (3.6152e-01)	Acc@1  93.00 ( 92.17)	Acc@5 100.00 ( 99.76)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 1.8095e-01 (3.5531e-01)	Acc@1  95.00 ( 92.31)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 4.9689e-01 (3.4495e-01)	Acc@1  92.00 ( 92.39)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.079 ( 0.080)	Loss 5.1398e-01 (3.4400e-01)	Acc@1  91.00 ( 92.41)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.077 ( 0.080)	Loss 2.5671e-01 (3.3815e-01)	Acc@1  95.00 ( 92.62)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 2.0206e-01 (3.3907e-01)	Acc@1  93.00 ( 92.60)	Acc@5 100.00 ( 99.77)
 * Acc@1 92.550 Acc@5 99.780
### epoch[53] execution time: 102.41403841972351
EPOCH 54
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [54][  0/391]	Time  0.392 ( 0.392)	Data  0.149 ( 0.149)	Loss 9.7659e-03 (9.7659e-03)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [54][ 10/391]	Time  0.239 ( 0.254)	Data  0.001 ( 0.015)	Loss 5.7945e-03 (1.2291e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [54][ 20/391]	Time  0.239 ( 0.248)	Data  0.001 ( 0.008)	Loss 9.1666e-03 (1.2473e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [54][ 30/391]	Time  0.240 ( 0.246)	Data  0.001 ( 0.006)	Loss 7.8872e-03 (1.3607e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [54][ 40/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.005)	Loss 8.9422e-03 (1.3123e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [54][ 50/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 2.9998e-02 (1.4162e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [54][ 60/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.004)	Loss 9.5941e-03 (1.4564e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [54][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 4.7674e-02 (1.4399e-02)	Acc@1  98.44 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [54][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 4.3837e-03 (1.4645e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [54][ 90/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 3.5445e-02 (1.4959e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [54][100/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 5.6429e-03 (1.4360e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [54][110/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.6006e-02 (1.4893e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [54][120/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.2247e-02 (1.4733e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [54][130/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.0877e-03 (1.4397e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [54][140/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.0537e-02 (1.3893e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [54][150/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3591e-02 (1.3943e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [54][160/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.9273e-03 (1.3840e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [54][170/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5827e-02 (1.3601e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [54][180/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8809e-02 (1.3562e-02)	Acc@1  98.44 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [54][190/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.7309e-03 (1.3380e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [54][200/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.3422e-02 (1.3417e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [54][210/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.3187e-03 (1.3640e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [54][220/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1254e-02 (1.3564e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [54][230/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1609e-02 (1.3424e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [54][240/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.5081e-03 (1.3403e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [54][250/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.3022e-03 (1.3458e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [54][260/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.1425e-02 (1.3657e-02)	Acc@1  98.44 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [54][270/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.5275e-03 (1.3516e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [54][280/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.2134e-03 (1.3450e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [54][290/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.7521e-03 (1.3499e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [54][300/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.2831e-03 (1.3431e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [54][310/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.9862e-02 (1.3489e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [54][320/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1751e-02 (1.3632e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [54][330/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1168e-02 (1.3653e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [54][340/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.3784e-02 (1.3712e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [54][350/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.2147e-03 (1.3797e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [54][360/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.7777e-03 (1.3707e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [54][370/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.9878e-03 (1.3883e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [54][380/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.7049e-03 (1.3869e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [54][390/391]	Time  0.177 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.6587e-03 (1.4062e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
## e[54] optimizer.zero_grad (sum) time: 0.4762694835662842
## e[54]       loss.backward (sum) time: 10.877902507781982
## e[54]      optimizer.step (sum) time: 47.50174045562744
## epoch[54] training(only) time: 94.28859424591064
# Switched to evaluate mode...
Test: [  0/100]	Time  0.205 ( 0.205)	Loss 3.2604e-01 (3.2604e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.077 ( 0.089)	Loss 1.9664e-01 (3.6610e-01)	Acc@1  95.00 ( 92.73)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 4.4888e-01 (3.6075e-01)	Acc@1  91.00 ( 92.67)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.077 ( 0.082)	Loss 2.8242e-01 (3.6518e-01)	Acc@1  93.00 ( 92.35)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 2.3099e-01 (3.6449e-01)	Acc@1  94.00 ( 92.41)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.077 ( 0.080)	Loss 2.6552e-01 (3.5887e-01)	Acc@1  95.00 ( 92.41)	Acc@5 100.00 ( 99.69)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 5.5960e-01 (3.5072e-01)	Acc@1  91.00 ( 92.44)	Acc@5 100.00 ( 99.72)
Test: [ 70/100]	Time  0.077 ( 0.080)	Loss 5.5592e-01 (3.5172e-01)	Acc@1  90.00 ( 92.42)	Acc@5  99.00 ( 99.69)
Test: [ 80/100]	Time  0.078 ( 0.079)	Loss 2.0224e-01 (3.4386e-01)	Acc@1  95.00 ( 92.59)	Acc@5 100.00 ( 99.69)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.7114e-01 (3.4447e-01)	Acc@1  94.00 ( 92.49)	Acc@5 100.00 ( 99.73)
 * Acc@1 92.440 Acc@5 99.740
### epoch[54] execution time: 102.29221653938293
EPOCH 55
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [55][  0/391]	Time  0.401 ( 0.401)	Data  0.148 ( 0.148)	Loss 4.4668e-03 (4.4668e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [55][ 10/391]	Time  0.240 ( 0.255)	Data  0.001 ( 0.015)	Loss 1.1819e-02 (1.6859e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [55][ 20/391]	Time  0.240 ( 0.248)	Data  0.001 ( 0.008)	Loss 1.2862e-02 (1.2784e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [55][ 30/391]	Time  0.240 ( 0.246)	Data  0.001 ( 0.006)	Loss 9.9832e-03 (1.2543e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [55][ 40/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.005)	Loss 6.5716e-03 (1.2539e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [55][ 50/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.8453e-03 (1.1993e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [55][ 60/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.004)	Loss 6.7277e-03 (1.2226e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [55][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.0541e-02 (1.2122e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [55][ 80/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.8106e-02 (1.1942e-02)	Acc@1  98.44 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [55][ 90/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.4296e-02 (1.1928e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [55][100/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 7.5623e-03 (1.1957e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [55][110/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 5.1688e-03 (1.1935e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [55][120/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.3789e-03 (1.1940e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [55][130/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.2229e-03 (1.1735e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [55][140/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.3307e-03 (1.1407e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [55][150/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8052e-02 (1.1643e-02)	Acc@1  98.44 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [55][160/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9502e-03 (1.2036e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [55][170/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9501e-03 (1.1947e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [55][180/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.8664e-02 (1.2501e-02)	Acc@1  97.66 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [55][190/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.1026e-02 (1.2515e-02)	Acc@1  98.44 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [55][200/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.4912e-03 (1.2489e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [55][210/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.6127e-03 (1.2378e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [55][220/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.0499e-02 (1.2319e-02)	Acc@1  98.44 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [55][230/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.9739e-03 (1.2211e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [55][240/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.9851e-03 (1.2213e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [55][250/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0253e-02 (1.2140e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [55][260/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.4549e-03 (1.2287e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [55][270/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.4370e-02 (1.2359e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [55][280/391]	Time  0.247 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.7465e-02 (1.2257e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [55][290/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.6714e-02 (1.2133e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [55][300/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.0873e-02 (1.2168e-02)	Acc@1  98.44 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [55][310/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2870e-02 (1.2120e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [55][320/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.7710e-03 (1.2000e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [55][330/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.4783e-03 (1.1978e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [55][340/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.5491e-03 (1.2101e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [55][350/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.2722e-02 (1.2168e-02)	Acc@1  98.44 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [55][360/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.0001e-03 (1.2173e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [55][370/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.3944e-03 (1.2227e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [55][380/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.8344e-03 (1.2082e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [55][390/391]	Time  0.176 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0408e-02 (1.2096e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
## e[55] optimizer.zero_grad (sum) time: 0.4745471477508545
## e[55]       loss.backward (sum) time: 10.810526609420776
## e[55]      optimizer.step (sum) time: 47.56195855140686
## epoch[55] training(only) time: 94.32732892036438
# Switched to evaluate mode...
Test: [  0/100]	Time  0.206 ( 0.206)	Loss 2.3273e-01 (2.3273e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.5849e-01 (3.3895e-01)	Acc@1  95.00 ( 92.18)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.077 ( 0.084)	Loss 4.0071e-01 (3.4254e-01)	Acc@1  91.00 ( 92.38)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.077 ( 0.082)	Loss 2.9640e-01 (3.5078e-01)	Acc@1  93.00 ( 92.35)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 3.3039e-01 (3.5088e-01)	Acc@1  91.00 ( 92.34)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.077 ( 0.080)	Loss 1.8983e-01 (3.4598e-01)	Acc@1  96.00 ( 92.57)	Acc@5 100.00 ( 99.69)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 5.5262e-01 (3.3733e-01)	Acc@1  92.00 ( 92.59)	Acc@5 100.00 ( 99.69)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 4.7680e-01 (3.3811e-01)	Acc@1  91.00 ( 92.62)	Acc@5  99.00 ( 99.66)
Test: [ 80/100]	Time  0.078 ( 0.079)	Loss 2.1667e-01 (3.3164e-01)	Acc@1  94.00 ( 92.73)	Acc@5 100.00 ( 99.67)
Test: [ 90/100]	Time  0.077 ( 0.079)	Loss 1.9202e-01 (3.3265e-01)	Acc@1  94.00 ( 92.68)	Acc@5 100.00 ( 99.70)
 * Acc@1 92.630 Acc@5 99.720
### epoch[55] execution time: 102.34170746803284
EPOCH 56
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [56][  0/391]	Time  0.399 ( 0.399)	Data  0.154 ( 0.154)	Loss 8.4755e-03 (8.4755e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [56][ 10/391]	Time  0.248 ( 0.255)	Data  0.001 ( 0.015)	Loss 2.9451e-03 (9.4349e-03)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [56][ 20/391]	Time  0.240 ( 0.248)	Data  0.001 ( 0.008)	Loss 8.6648e-03 (1.1317e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [56][ 30/391]	Time  0.240 ( 0.246)	Data  0.001 ( 0.006)	Loss 3.8535e-02 (1.1451e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [56][ 40/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.005)	Loss 2.7835e-03 (1.2795e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [56][ 50/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 4.1303e-03 (1.4084e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [56][ 60/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.004)	Loss 1.0950e-02 (1.5055e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [56][ 70/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 6.0094e-03 (1.4844e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [56][ 80/391]	Time  0.239 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.3876e-03 (1.4241e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [56][ 90/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 3.4231e-03 (1.3287e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [56][100/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.003)	Loss 9.7783e-03 (1.4039e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [56][110/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.9052e-02 (1.4181e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [56][120/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.5076e-02 (1.3957e-02)	Acc@1  98.44 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [56][130/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9602e-02 (1.3998e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [56][140/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.5927e-03 (1.3918e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [56][150/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.3631e-03 (1.3605e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [56][160/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.1625e-03 (1.3580e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [56][170/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.6188e-03 (1.3339e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [56][180/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.9720e-03 (1.3231e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [56][190/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.3589e-03 (1.2983e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [56][200/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.5628e-03 (1.2702e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [56][210/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.7803e-03 (1.2519e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [56][220/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.2500e-03 (1.2426e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [56][230/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.8192e-03 (1.2320e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [56][240/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.1793e-03 (1.2224e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [56][250/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.0913e-02 (1.2226e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [56][260/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.4000e-02 (1.2242e-02)	Acc@1  97.66 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [56][270/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.2206e-03 (1.2412e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [56][280/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.4601e-03 (1.2430e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [56][290/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.6508e-03 (1.2217e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [56][300/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1168e-02 (1.2376e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [56][310/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0842e-02 (1.2192e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [56][320/391]	Time  0.244 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.0048e-02 (1.2192e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [56][330/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0919e-02 (1.2291e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [56][340/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.2862e-03 (1.2106e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [56][350/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.6079e-02 (1.2122e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [56][360/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.0796e-03 (1.2075e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [56][370/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.6796e-03 (1.1954e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [56][380/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.9325e-03 (1.2027e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [56][390/391]	Time  0.177 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.7975e-02 (1.2023e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
## e[56] optimizer.zero_grad (sum) time: 0.4817056655883789
## e[56]       loss.backward (sum) time: 10.777854919433594
## e[56]      optimizer.step (sum) time: 47.58398509025574
## epoch[56] training(only) time: 94.27283930778503
# Switched to evaluate mode...
Test: [  0/100]	Time  0.212 ( 0.212)	Loss 2.7468e-01 (2.7468e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 2.1232e-01 (3.2851e-01)	Acc@1  95.00 ( 92.64)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 4.7771e-01 (3.4762e-01)	Acc@1  90.00 ( 92.38)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.079 ( 0.082)	Loss 3.2137e-01 (3.6140e-01)	Acc@1  91.00 ( 92.26)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.077 ( 0.081)	Loss 3.1786e-01 (3.6831e-01)	Acc@1  92.00 ( 92.20)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.078 ( 0.080)	Loss 2.5008e-01 (3.6439e-01)	Acc@1  95.00 ( 92.33)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.077 ( 0.080)	Loss 6.1717e-01 (3.5384e-01)	Acc@1  91.00 ( 92.48)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.079 ( 0.080)	Loss 4.3585e-01 (3.5073e-01)	Acc@1  91.00 ( 92.52)	Acc@5  99.00 ( 99.72)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 1.9508e-01 (3.4125e-01)	Acc@1  95.00 ( 92.69)	Acc@5 100.00 ( 99.70)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.7690e-01 (3.4315e-01)	Acc@1  95.00 ( 92.67)	Acc@5 100.00 ( 99.74)
 * Acc@1 92.640 Acc@5 99.750
### epoch[56] execution time: 102.28146815299988
EPOCH 57
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [57][  0/391]	Time  0.385 ( 0.385)	Data  0.131 ( 0.131)	Loss 7.5643e-03 (7.5643e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [57][ 10/391]	Time  0.240 ( 0.254)	Data  0.001 ( 0.013)	Loss 2.2566e-02 (1.3099e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [57][ 20/391]	Time  0.240 ( 0.248)	Data  0.001 ( 0.007)	Loss 3.0460e-03 (1.1371e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [57][ 30/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.005)	Loss 7.2612e-03 (1.2064e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [57][ 40/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.2539e-02 (1.1463e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [57][ 50/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.004)	Loss 3.8377e-03 (1.0945e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [57][ 60/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.4088e-02 (1.0489e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [57][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 4.7351e-03 (1.0567e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [57][ 80/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 3.3651e-03 (1.0608e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [57][ 90/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 6.9368e-03 (1.0431e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [57][100/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.2067e-03 (1.0002e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [57][110/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4914e-03 (9.7765e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [57][120/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.4182e-02 (9.5154e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [57][130/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.7669e-03 (9.6021e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [57][140/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.4155e-03 (9.5173e-03)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [57][150/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8773e-03 (9.1664e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [57][160/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.8929e-02 (9.2109e-03)	Acc@1  98.44 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [57][170/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.4969e-03 (9.4299e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [57][180/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.1406e-03 (9.3186e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [57][190/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.3470e-02 (9.3913e-03)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [57][200/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0154e-02 (9.3840e-03)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [57][210/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.3129e-03 (9.5121e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [57][220/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.8611e-03 (9.3295e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [57][230/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.3827e-03 (9.1938e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [57][240/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.5047e-03 (9.1834e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [57][250/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.6054e-03 (9.2442e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [57][260/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.5211e-02 (9.2716e-03)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [57][270/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.6270e-02 (9.2957e-03)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [57][280/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.4232e-03 (9.2665e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [57][290/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1380e-02 (9.4376e-03)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [57][300/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.6444e-02 (9.6992e-03)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [57][310/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.9329e-03 (9.8727e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [57][320/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.5420e-03 (9.8991e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [57][330/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.5190e-03 (9.8765e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [57][340/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.6590e-03 (1.0164e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [57][350/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.6861e-03 (1.0154e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [57][360/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.2118e-03 (1.0228e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [57][370/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.5494e-02 (1.0239e-02)	Acc@1  96.09 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [57][380/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.4964e-03 (1.0191e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [57][390/391]	Time  0.177 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.8989e-02 (1.0410e-02)	Acc@1  97.50 ( 99.72)	Acc@5 100.00 (100.00)
## e[57] optimizer.zero_grad (sum) time: 0.4742085933685303
## e[57]       loss.backward (sum) time: 10.752284049987793
## e[57]      optimizer.step (sum) time: 47.58624291419983
## epoch[57] training(only) time: 94.26681685447693
# Switched to evaluate mode...
Test: [  0/100]	Time  0.203 ( 0.203)	Loss 2.4464e-01 (2.4464e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.078 ( 0.089)	Loss 2.0131e-01 (3.3144e-01)	Acc@1  95.00 ( 93.09)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 3.4353e-01 (3.3674e-01)	Acc@1  91.00 ( 92.76)	Acc@5  99.00 ( 99.67)
Test: [ 30/100]	Time  0.077 ( 0.082)	Loss 3.7283e-01 (3.5102e-01)	Acc@1  90.00 ( 92.55)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 2.7461e-01 (3.5530e-01)	Acc@1  91.00 ( 92.51)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.078 ( 0.080)	Loss 2.0478e-01 (3.4972e-01)	Acc@1  94.00 ( 92.67)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 5.8913e-01 (3.4216e-01)	Acc@1  92.00 ( 92.66)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 5.4700e-01 (3.4251e-01)	Acc@1  90.00 ( 92.68)	Acc@5  99.00 ( 99.72)
Test: [ 80/100]	Time  0.078 ( 0.079)	Loss 2.1334e-01 (3.3710e-01)	Acc@1  95.00 ( 92.78)	Acc@5 100.00 ( 99.70)
Test: [ 90/100]	Time  0.077 ( 0.079)	Loss 1.8936e-01 (3.3718e-01)	Acc@1  95.00 ( 92.79)	Acc@5 100.00 ( 99.73)
 * Acc@1 92.760 Acc@5 99.740
### epoch[57] execution time: 102.28781270980835
EPOCH 58
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [58][  0/391]	Time  0.386 ( 0.386)	Data  0.146 ( 0.146)	Loss 6.7861e-03 (6.7861e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [58][ 10/391]	Time  0.241 ( 0.254)	Data  0.001 ( 0.014)	Loss 7.5103e-04 (9.9403e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [58][ 20/391]	Time  0.240 ( 0.247)	Data  0.001 ( 0.008)	Loss 9.3124e-03 (8.6187e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [58][ 30/391]	Time  0.242 ( 0.245)	Data  0.001 ( 0.006)	Loss 3.6399e-02 (9.6469e-03)	Acc@1  98.44 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [58][ 40/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.005)	Loss 3.7638e-03 (9.2941e-03)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [58][ 50/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.004)	Loss 2.8755e-03 (9.3067e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [58][ 60/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.004)	Loss 1.9752e-02 (9.4122e-03)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [58][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 4.7639e-03 (9.7454e-03)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [58][ 80/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.003)	Loss 6.2989e-03 (9.4362e-03)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [58][ 90/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.003)	Loss 4.3956e-03 (9.4576e-03)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [58][100/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 3.3357e-03 (9.5623e-03)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [58][110/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2107e-02 (9.5204e-03)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [58][120/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.8309e-04 (9.5930e-03)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [58][130/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.2713e-03 (9.8418e-03)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [58][140/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.8120e-03 (1.0274e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [58][150/391]	Time  0.243 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.7078e-03 (1.0673e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [58][160/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.9546e-02 (1.0732e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [58][170/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.4747e-03 (1.1120e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [58][180/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2704e-02 (1.0941e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [58][190/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.0402e-03 (1.0857e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [58][200/391]	Time  0.247 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.7566e-02 (1.0866e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [58][210/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.1228e-03 (1.0913e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [58][220/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2152e-02 (1.0846e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [58][230/391]	Time  0.241 ( 0.241)	Data  0.002 ( 0.002)	Loss 1.1436e-02 (1.1198e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [58][240/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.5969e-02 (1.1132e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [58][250/391]	Time  0.244 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.5507e-03 (1.0963e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [58][260/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.1272e-02 (1.1091e-02)	Acc@1  98.44 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [58][270/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.8258e-03 (1.0952e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [58][280/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.0928e-03 (1.1058e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [58][290/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.6463e-03 (1.1133e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [58][300/391]	Time  0.245 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.5481e-02 (1.1234e-02)	Acc@1  98.44 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [58][310/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.9682e-02 (1.1253e-02)	Acc@1  98.44 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [58][320/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.3561e-03 (1.1135e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [58][330/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.4692e-03 (1.1247e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [58][340/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.8631e-03 (1.1209e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [58][350/391]	Time  0.245 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.8664e-03 (1.1189e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [58][360/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.5882e-03 (1.1258e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [58][370/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2632e-02 (1.1201e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [58][380/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.3763e-03 (1.1162e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [58][390/391]	Time  0.182 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.2438e-02 (1.1103e-02)	Acc@1  98.75 ( 99.67)	Acc@5 100.00 (100.00)
## e[58] optimizer.zero_grad (sum) time: 0.4773368835449219
## e[58]       loss.backward (sum) time: 10.794387817382812
## e[58]      optimizer.step (sum) time: 47.58445692062378
## epoch[58] training(only) time: 94.25970888137817
# Switched to evaluate mode...
Test: [  0/100]	Time  0.209 ( 0.209)	Loss 2.5219e-01 (2.5219e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 2.0568e-01 (3.2937e-01)	Acc@1  96.00 ( 93.27)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 4.0784e-01 (3.3229e-01)	Acc@1  91.00 ( 93.19)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.077 ( 0.082)	Loss 3.2265e-01 (3.4290e-01)	Acc@1  93.00 ( 92.94)	Acc@5 100.00 ( 99.81)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 2.6925e-01 (3.5007e-01)	Acc@1  91.00 ( 92.71)	Acc@5 100.00 ( 99.83)
Test: [ 50/100]	Time  0.077 ( 0.080)	Loss 2.6482e-01 (3.5019e-01)	Acc@1  93.00 ( 92.67)	Acc@5 100.00 ( 99.84)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 6.1505e-01 (3.3829e-01)	Acc@1  91.00 ( 92.74)	Acc@5 100.00 ( 99.85)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 5.6172e-01 (3.4075e-01)	Acc@1  91.00 ( 92.65)	Acc@5  99.00 ( 99.80)
Test: [ 80/100]	Time  0.078 ( 0.079)	Loss 2.2689e-01 (3.3212e-01)	Acc@1  95.00 ( 92.90)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 2.4651e-01 (3.3327e-01)	Acc@1  95.00 ( 92.88)	Acc@5 100.00 ( 99.80)
 * Acc@1 92.810 Acc@5 99.810
### epoch[58] execution time: 102.26446866989136
EPOCH 59
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [59][  0/391]	Time  0.394 ( 0.394)	Data  0.154 ( 0.154)	Loss 4.7367e-03 (4.7367e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [59][ 10/391]	Time  0.240 ( 0.254)	Data  0.001 ( 0.015)	Loss 6.3588e-03 (7.2246e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [59][ 20/391]	Time  0.240 ( 0.248)	Data  0.001 ( 0.008)	Loss 8.5190e-03 (6.8523e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [59][ 30/391]	Time  0.240 ( 0.246)	Data  0.001 ( 0.006)	Loss 1.1813e-02 (7.6358e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [59][ 40/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.005)	Loss 7.1205e-03 (9.0004e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [59][ 50/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 5.4305e-03 (8.7393e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [59][ 60/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.004)	Loss 2.0543e-02 (9.2128e-03)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [59][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 6.2275e-03 (9.7023e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [59][ 80/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.8612e-02 (9.4153e-03)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [59][ 90/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 7.6190e-03 (9.3258e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [59][100/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.2693e-03 (9.5595e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [59][110/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.4742e-02 (9.7060e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [59][120/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.6319e-03 (9.6597e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [59][130/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.0570e-03 (1.0065e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [59][140/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.8204e-03 (1.0370e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [59][150/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.5715e-03 (1.0210e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [59][160/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.3944e-03 (1.0579e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [59][170/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.5212e-03 (1.0504e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [59][180/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.4219e-03 (1.0622e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [59][190/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.1835e-03 (1.0413e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [59][200/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.6257e-02 (1.0370e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [59][210/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0660e-02 (1.0291e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [59][220/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.1401e-03 (1.0238e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [59][230/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.5460e-02 (1.0215e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [59][240/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.8110e-03 (1.0043e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [59][250/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.6364e-02 (1.0397e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [59][260/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.6353e-03 (1.0419e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [59][270/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.9412e-02 (1.0551e-02)	Acc@1  98.44 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [59][280/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.6251e-02 (1.1067e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [59][290/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.6765e-02 (1.1073e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [59][300/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1300e-02 (1.1111e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [59][310/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.9990e-03 (1.1312e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [59][320/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.7605e-03 (1.1462e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [59][330/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.3473e-03 (1.1282e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [59][340/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.2819e-03 (1.1145e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [59][350/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.3534e-03 (1.1151e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [59][360/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.9918e-03 (1.1435e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [59][370/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.4775e-02 (1.1749e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [59][380/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.7689e-02 (1.1777e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [59][390/391]	Time  0.178 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.6356e-03 (1.1964e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
## e[59] optimizer.zero_grad (sum) time: 0.4767136573791504
## e[59]       loss.backward (sum) time: 10.872236728668213
## e[59]      optimizer.step (sum) time: 47.569133281707764
## epoch[59] training(only) time: 94.34017825126648
# Switched to evaluate mode...
Test: [  0/100]	Time  0.207 ( 0.207)	Loss 3.2506e-01 (3.2506e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.6138e-01 (3.4753e-01)	Acc@1  96.00 ( 92.55)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 5.2871e-01 (3.6661e-01)	Acc@1  91.00 ( 92.52)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 4.2189e-01 (3.6886e-01)	Acc@1  89.00 ( 92.42)	Acc@5 100.00 ( 99.77)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 2.8735e-01 (3.6276e-01)	Acc@1  91.00 ( 92.46)	Acc@5 100.00 ( 99.78)
Test: [ 50/100]	Time  0.077 ( 0.080)	Loss 2.3495e-01 (3.6036e-01)	Acc@1  96.00 ( 92.55)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 6.2684e-01 (3.5191e-01)	Acc@1  92.00 ( 92.57)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 6.0791e-01 (3.5587e-01)	Acc@1  90.00 ( 92.48)	Acc@5  99.00 ( 99.75)
Test: [ 80/100]	Time  0.077 ( 0.079)	Loss 2.7837e-01 (3.4801e-01)	Acc@1  93.00 ( 92.59)	Acc@5 100.00 ( 99.72)
Test: [ 90/100]	Time  0.077 ( 0.079)	Loss 1.5528e-01 (3.4797e-01)	Acc@1  94.00 ( 92.62)	Acc@5 100.00 ( 99.75)
 * Acc@1 92.630 Acc@5 99.760
### epoch[59] execution time: 102.3360595703125
EPOCH 60
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [60][  0/391]	Time  0.395 ( 0.395)	Data  0.151 ( 0.151)	Loss 3.6696e-02 (3.6696e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [60][ 10/391]	Time  0.241 ( 0.255)	Data  0.001 ( 0.015)	Loss 1.2951e-03 (1.7752e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [60][ 20/391]	Time  0.241 ( 0.248)	Data  0.001 ( 0.008)	Loss 6.7975e-03 (1.3586e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [60][ 30/391]	Time  0.240 ( 0.246)	Data  0.001 ( 0.006)	Loss 1.5242e-03 (1.2623e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [60][ 40/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.005)	Loss 1.6529e-02 (1.1148e-02)	Acc@1  98.44 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [60][ 50/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 2.4232e-03 (1.0299e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [60][ 60/391]	Time  0.239 ( 0.243)	Data  0.001 ( 0.004)	Loss 3.2183e-03 (9.9240e-03)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [60][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.8628e-03 (9.3817e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [60][ 80/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 7.4425e-03 (9.2365e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [60][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.0163e-03 (9.6474e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [60][100/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.8543e-03 (9.6410e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [60][110/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7112e-03 (9.6521e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [60][120/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.5569e-03 (9.3992e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [60][130/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.3289e-03 (9.7211e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [60][140/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.3003e-03 (9.5073e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [60][150/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1516e-02 (9.6139e-03)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [60][160/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.7232e-03 (9.3630e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [60][170/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.9100e-03 (9.2934e-03)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [60][180/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.2364e-03 (9.7390e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [60][190/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7821e-03 (1.0010e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [60][200/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8510e-02 (1.0112e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [60][210/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.3770e-03 (1.0159e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [60][220/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8367e-03 (1.0205e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [60][230/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.9085e-03 (1.0018e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [60][240/391]	Time  0.246 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9317e-02 (1.0231e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [60][250/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.1146e-03 (1.0038e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [60][260/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.8203e-03 (1.0057e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [60][270/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2519e-02 (1.0022e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [60][280/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.0219e-03 (9.9297e-03)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [60][290/391]	Time  0.246 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.0771e-03 (1.0031e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [60][300/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.5648e-04 (9.8721e-03)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [60][310/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.7495e-03 (9.6994e-03)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [60][320/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0822e-02 (9.5642e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [60][330/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0502e-03 (9.4352e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [60][340/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.8775e-03 (9.3294e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [60][350/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5239e-02 (9.2556e-03)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [60][360/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0565e-02 (9.3140e-03)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [60][370/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5979e-02 (9.2952e-03)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [60][380/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.7312e-03 (9.3605e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [60][390/391]	Time  0.179 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.6532e-03 (9.3451e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
## e[60] optimizer.zero_grad (sum) time: 0.4793128967285156
## e[60]       loss.backward (sum) time: 10.926584482192993
## e[60]      optimizer.step (sum) time: 47.55586242675781
## epoch[60] training(only) time: 94.4774215221405
# Switched to evaluate mode...
Test: [  0/100]	Time  0.214 ( 0.214)	Loss 3.0335e-01 (3.0335e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.8180e-01 (3.4974e-01)	Acc@1  96.00 ( 93.00)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.079 ( 0.085)	Loss 4.8307e-01 (3.6027e-01)	Acc@1  92.00 ( 92.76)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 4.0837e-01 (3.6584e-01)	Acc@1  91.00 ( 92.65)	Acc@5 100.00 ( 99.81)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 2.4671e-01 (3.6191e-01)	Acc@1  92.00 ( 92.56)	Acc@5 100.00 ( 99.80)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 2.5292e-01 (3.5869e-01)	Acc@1  94.00 ( 92.63)	Acc@5 100.00 ( 99.80)
Test: [ 60/100]	Time  0.079 ( 0.080)	Loss 5.9022e-01 (3.4812e-01)	Acc@1  93.00 ( 92.70)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.079 ( 0.080)	Loss 5.6562e-01 (3.4939e-01)	Acc@1  90.00 ( 92.63)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 2.7282e-01 (3.4263e-01)	Acc@1  93.00 ( 92.75)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.6909e-01 (3.4271e-01)	Acc@1  93.00 ( 92.71)	Acc@5 100.00 ( 99.78)
 * Acc@1 92.690 Acc@5 99.790
### epoch[60] execution time: 102.5020272731781
EPOCH 61
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [61][  0/391]	Time  0.388 ( 0.388)	Data  0.137 ( 0.137)	Loss 4.2171e-03 (4.2171e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [61][ 10/391]	Time  0.240 ( 0.255)	Data  0.001 ( 0.013)	Loss 3.1462e-02 (8.7356e-03)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [61][ 20/391]	Time  0.245 ( 0.248)	Data  0.001 ( 0.008)	Loss 9.1222e-04 (6.7763e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [61][ 30/391]	Time  0.240 ( 0.246)	Data  0.001 ( 0.006)	Loss 4.5269e-03 (8.3755e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [61][ 40/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.004)	Loss 1.5675e-02 (7.8693e-03)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [61][ 50/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 2.6883e-02 (7.9531e-03)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [61][ 60/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.3051e-02 (8.1594e-03)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [61][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.3980e-02 (8.3513e-03)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [61][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.9518e-03 (8.2333e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [61][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.0029e-03 (7.9115e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [61][100/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.7792e-03 (7.5929e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [61][110/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.7753e-03 (8.1390e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [61][120/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.6711e-03 (8.2304e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [61][130/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6803e-03 (8.0689e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [61][140/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.8245e-03 (8.2215e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [61][150/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7556e-02 (8.2297e-03)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [61][160/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.7307e-03 (7.9984e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [61][170/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2976e-02 (8.3986e-03)	Acc@1  98.44 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [61][180/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.3929e-03 (8.2727e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [61][190/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.4960e-02 (8.4528e-03)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [61][200/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.5096e-03 (8.2725e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [61][210/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.7261e-03 (8.5424e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [61][220/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6202e-02 (8.6816e-03)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [61][230/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.6045e-03 (8.7238e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [61][240/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.4054e-03 (8.5670e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [61][250/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.0424e-03 (8.5965e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [61][260/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.9340e-03 (8.4807e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [61][270/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.2749e-03 (8.5241e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [61][280/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5684e-02 (8.4512e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [61][290/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1977e-02 (8.4215e-03)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [61][300/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8259e-03 (8.3210e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [61][310/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0177e-02 (8.3170e-03)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [61][320/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0079e-03 (8.4266e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [61][330/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.5774e-03 (8.5161e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [61][340/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2292e-02 (8.6357e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [61][350/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.9410e-02 (8.6296e-03)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [61][360/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.5739e-03 (8.5784e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [61][370/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.9817e-03 (8.5052e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [61][380/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.3786e-03 (8.4249e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [61][390/391]	Time  0.178 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.9371e-03 (8.3581e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
## e[61] optimizer.zero_grad (sum) time: 0.4769594669342041
## e[61]       loss.backward (sum) time: 10.879547119140625
## e[61]      optimizer.step (sum) time: 47.55958080291748
## epoch[61] training(only) time: 94.44981169700623
# Switched to evaluate mode...
Test: [  0/100]	Time  0.212 ( 0.212)	Loss 2.8099e-01 (2.8099e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.9139e-01 (3.3999e-01)	Acc@1  96.00 ( 93.09)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.079 ( 0.084)	Loss 5.0867e-01 (3.5795e-01)	Acc@1  92.00 ( 92.71)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 3.9131e-01 (3.6734e-01)	Acc@1  92.00 ( 92.52)	Acc@5 100.00 ( 99.77)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 2.3570e-01 (3.6179e-01)	Acc@1  93.00 ( 92.54)	Acc@5 100.00 ( 99.78)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 2.4370e-01 (3.5755e-01)	Acc@1  95.00 ( 92.59)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.077 ( 0.080)	Loss 6.3102e-01 (3.4910e-01)	Acc@1  92.00 ( 92.62)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 5.4163e-01 (3.4919e-01)	Acc@1  90.00 ( 92.54)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.077 ( 0.080)	Loss 2.8264e-01 (3.4101e-01)	Acc@1  93.00 ( 92.72)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.8531e-01 (3.4253e-01)	Acc@1  95.00 ( 92.70)	Acc@5 100.00 ( 99.77)
 * Acc@1 92.640 Acc@5 99.780
### epoch[61] execution time: 102.45718669891357
EPOCH 62
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [62][  0/391]	Time  0.398 ( 0.398)	Data  0.158 ( 0.158)	Loss 2.9619e-03 (2.9619e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [62][ 10/391]	Time  0.241 ( 0.254)	Data  0.001 ( 0.015)	Loss 5.7013e-03 (1.1422e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [62][ 20/391]	Time  0.239 ( 0.248)	Data  0.001 ( 0.009)	Loss 7.0334e-03 (1.1509e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [62][ 30/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.006)	Loss 2.6922e-02 (1.1312e-02)	Acc@1  98.44 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [62][ 40/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.005)	Loss 2.9351e-03 (1.0809e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [62][ 50/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 2.5923e-03 (9.8837e-03)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [62][ 60/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.004)	Loss 1.0067e-03 (9.8052e-03)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [62][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 4.2452e-03 (9.7625e-03)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [62][ 80/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.7868e-03 (9.5030e-03)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [62][ 90/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 4.0865e-03 (9.2678e-03)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [62][100/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 6.3965e-03 (9.1767e-03)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [62][110/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.7335e-02 (9.1142e-03)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [62][120/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6317e-02 (9.3367e-03)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [62][130/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.9573e-03 (8.9577e-03)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [62][140/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.8396e-03 (8.6533e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [62][150/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.3951e-03 (8.8006e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [62][160/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1821e-02 (8.6550e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [62][170/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.9982e-03 (8.4548e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [62][180/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.0163e-02 (8.4618e-03)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [62][190/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.2637e-03 (8.5651e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [62][200/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0410e-03 (8.6126e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [62][210/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.0143e-03 (8.5179e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [62][220/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3166e-03 (8.7046e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [62][230/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6259e-02 (8.5998e-03)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [62][240/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.6966e-03 (8.4729e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [62][250/391]	Time  0.247 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.2083e-03 (8.3652e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [62][260/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.9470e-03 (8.2637e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [62][270/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.2575e-03 (8.1723e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [62][280/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.9884e-03 (8.2618e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [62][290/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.6597e-03 (8.1696e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [62][300/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.4107e-03 (8.0413e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [62][310/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.6116e-03 (8.0142e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [62][320/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.4449e-03 (7.9410e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [62][330/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.3414e-03 (7.8350e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [62][340/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2304e-02 (7.7346e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [62][350/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.3506e-03 (7.7106e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [62][360/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.8627e-02 (7.7658e-03)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [62][370/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.9220e-03 (7.8436e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [62][380/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.0048e-03 (7.7626e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [62][390/391]	Time  0.176 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.9475e-03 (7.7437e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
## e[62] optimizer.zero_grad (sum) time: 0.47759056091308594
## e[62]       loss.backward (sum) time: 10.899694442749023
## e[62]      optimizer.step (sum) time: 47.52488589286804
## epoch[62] training(only) time: 94.40156579017639
# Switched to evaluate mode...
Test: [  0/100]	Time  0.212 ( 0.212)	Loss 2.5705e-01 (2.5705e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.077 ( 0.090)	Loss 1.8467e-01 (3.4114e-01)	Acc@1  96.00 ( 93.00)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.079 ( 0.084)	Loss 4.6828e-01 (3.5525e-01)	Acc@1  92.00 ( 92.62)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 3.6221e-01 (3.5896e-01)	Acc@1  91.00 ( 92.58)	Acc@5 100.00 ( 99.81)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 2.1793e-01 (3.5496e-01)	Acc@1  92.00 ( 92.63)	Acc@5 100.00 ( 99.80)
Test: [ 50/100]	Time  0.077 ( 0.081)	Loss 2.3682e-01 (3.5301e-01)	Acc@1  95.00 ( 92.69)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 5.9933e-01 (3.4172e-01)	Acc@1  91.00 ( 92.74)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 5.3772e-01 (3.4394e-01)	Acc@1  90.00 ( 92.66)	Acc@5  99.00 ( 99.75)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 2.2108e-01 (3.3598e-01)	Acc@1  93.00 ( 92.79)	Acc@5 100.00 ( 99.73)
Test: [ 90/100]	Time  0.079 ( 0.079)	Loss 1.6419e-01 (3.3719e-01)	Acc@1  94.00 ( 92.77)	Acc@5 100.00 ( 99.76)
 * Acc@1 92.750 Acc@5 99.770
### epoch[62] execution time: 102.41294956207275
EPOCH 63
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [63][  0/391]	Time  0.387 ( 0.387)	Data  0.145 ( 0.145)	Loss 2.1989e-02 (2.1989e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [63][ 10/391]	Time  0.246 ( 0.255)	Data  0.001 ( 0.014)	Loss 1.8577e-03 (1.2056e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [63][ 20/391]	Time  0.241 ( 0.248)	Data  0.001 ( 0.008)	Loss 1.3581e-03 (9.4225e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [63][ 30/391]	Time  0.240 ( 0.246)	Data  0.001 ( 0.006)	Loss 2.4133e-02 (9.0828e-03)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [63][ 40/391]	Time  0.242 ( 0.245)	Data  0.001 ( 0.005)	Loss 4.0352e-03 (8.3304e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [63][ 50/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 2.1332e-02 (8.3912e-03)	Acc@1  98.44 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [63][ 60/391]	Time  0.246 ( 0.243)	Data  0.001 ( 0.004)	Loss 2.7768e-03 (8.4309e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [63][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.4698e-03 (8.0769e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [63][ 80/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.5634e-02 (7.7959e-03)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [63][ 90/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 7.4492e-03 (7.6127e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [63][100/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 5.4585e-03 (7.4209e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [63][110/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.5665e-03 (7.4566e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [63][120/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.6536e-03 (7.5129e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [63][130/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8889e-03 (7.2323e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [63][140/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8223e-03 (7.1727e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [63][150/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.5207e-02 (7.3158e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [63][160/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.0434e-03 (7.6248e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [63][170/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6930e-03 (7.5525e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [63][180/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.4038e-02 (7.7545e-03)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [63][190/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.5534e-03 (7.5515e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [63][200/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2009e-02 (7.5020e-03)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [63][210/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.0269e-03 (7.4860e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [63][220/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.7267e-02 (7.4136e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [63][230/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.6952e-02 (7.5198e-03)	Acc@1  98.44 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [63][240/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.9932e-03 (7.4094e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [63][250/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.5323e-03 (7.4360e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [63][260/391]	Time  0.253 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.5412e-02 (7.6465e-03)	Acc@1  98.44 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [63][270/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.4193e-02 (7.6719e-03)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [63][280/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.1751e-03 (7.7334e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [63][290/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2365e-03 (7.7460e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [63][300/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.2724e-03 (7.6465e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [63][310/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.7687e-02 (7.6034e-03)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [63][320/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.1024e-03 (7.6759e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [63][330/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.7719e-03 (7.6008e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [63][340/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.3421e-03 (7.5491e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [63][350/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.3121e-03 (7.7339e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [63][360/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.3456e-03 (7.7132e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [63][370/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.6605e-03 (7.6722e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [63][380/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1093e-03 (7.6903e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [63][390/391]	Time  0.177 ( 0.241)	Data  0.001 ( 0.001)	Loss 1.3043e-02 (7.7161e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
## e[63] optimizer.zero_grad (sum) time: 0.4770646095275879
## e[63]       loss.backward (sum) time: 10.824620962142944
## e[63]      optimizer.step (sum) time: 47.58699178695679
## epoch[63] training(only) time: 94.37243366241455
# Switched to evaluate mode...
Test: [  0/100]	Time  0.204 ( 0.204)	Loss 2.5601e-01 (2.5601e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 2.1030e-01 (3.4339e-01)	Acc@1  96.00 ( 93.18)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.077 ( 0.084)	Loss 4.5809e-01 (3.5586e-01)	Acc@1  92.00 ( 93.05)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.077 ( 0.082)	Loss 3.5050e-01 (3.6386e-01)	Acc@1  91.00 ( 92.74)	Acc@5 100.00 ( 99.81)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 2.4423e-01 (3.5683e-01)	Acc@1  92.00 ( 92.63)	Acc@5 100.00 ( 99.80)
Test: [ 50/100]	Time  0.078 ( 0.080)	Loss 2.1655e-01 (3.5207e-01)	Acc@1  96.00 ( 92.78)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.079 ( 0.080)	Loss 5.9598e-01 (3.4073e-01)	Acc@1  92.00 ( 92.85)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.077 ( 0.080)	Loss 5.4858e-01 (3.4135e-01)	Acc@1  90.00 ( 92.76)	Acc@5  99.00 ( 99.76)
Test: [ 80/100]	Time  0.078 ( 0.079)	Loss 2.6492e-01 (3.3482e-01)	Acc@1  93.00 ( 92.90)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.6921e-01 (3.3589e-01)	Acc@1  94.00 ( 92.87)	Acc@5 100.00 ( 99.77)
 * Acc@1 92.820 Acc@5 99.780
### epoch[63] execution time: 102.37588286399841
EPOCH 64
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [64][  0/391]	Time  0.392 ( 0.392)	Data  0.141 ( 0.141)	Loss 2.0593e-03 (2.0593e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [64][ 10/391]	Time  0.240 ( 0.254)	Data  0.001 ( 0.014)	Loss 3.6541e-03 (6.5525e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [64][ 20/391]	Time  0.239 ( 0.248)	Data  0.001 ( 0.008)	Loss 2.4086e-03 (5.1383e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [64][ 30/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.006)	Loss 1.0415e-03 (4.5622e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [64][ 40/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.005)	Loss 2.2198e-03 (4.7523e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [64][ 50/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.004)	Loss 2.9370e-03 (5.1035e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [64][ 60/391]	Time  0.239 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.4513e-03 (6.2741e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [64][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.6923e-03 (6.0488e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [64][ 80/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 3.1314e-03 (5.8318e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [64][ 90/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.3135e-03 (5.9305e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [64][100/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6087e-02 (5.8708e-03)	Acc@1  98.44 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [64][110/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6025e-03 (5.8866e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [64][120/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.8341e-03 (6.1324e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [64][130/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.4908e-03 (6.2211e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [64][140/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1215e-03 (6.3803e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [64][150/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2572e-02 (6.3365e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [64][160/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8461e-02 (6.3084e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [64][170/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.4959e-03 (6.1770e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [64][180/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.1617e-03 (6.2809e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [64][190/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2301e-03 (6.5814e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [64][200/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.9783e-03 (6.5011e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [64][210/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0367e-03 (6.5167e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [64][220/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.6792e-03 (6.5601e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [64][230/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.3419e-02 (6.6582e-03)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [64][240/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.5090e-03 (6.7375e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [64][250/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.0086e-02 (6.7485e-03)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [64][260/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.0395e-03 (6.6757e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [64][270/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.7242e-03 (6.7097e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [64][280/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.9876e-03 (6.6897e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [64][290/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.1874e-03 (6.6830e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [64][300/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.0793e-02 (6.7853e-03)	Acc@1  97.66 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [64][310/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.7232e-03 (6.8275e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [64][320/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0048e-03 (6.8374e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [64][330/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.8910e-03 (6.8689e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [64][340/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.3558e-03 (6.8230e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [64][350/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.2947e-03 (6.7946e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [64][360/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.5284e-03 (6.7759e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [64][370/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.9479e-02 (6.7962e-03)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [64][380/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.001)	Loss 1.8342e-03 (6.8374e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [64][390/391]	Time  0.177 ( 0.241)	Data  0.001 ( 0.001)	Loss 1.9621e-03 (6.8382e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
## e[64] optimizer.zero_grad (sum) time: 0.4777028560638428
## e[64]       loss.backward (sum) time: 10.846949577331543
## e[64]      optimizer.step (sum) time: 47.566898822784424
## epoch[64] training(only) time: 94.35151529312134
# Switched to evaluate mode...
Test: [  0/100]	Time  0.210 ( 0.210)	Loss 2.4396e-01 (2.4396e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.077 ( 0.090)	Loss 1.9113e-01 (3.3154e-01)	Acc@1  96.00 ( 93.18)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.077 ( 0.084)	Loss 4.7157e-01 (3.4841e-01)	Acc@1  91.00 ( 92.86)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.077 ( 0.082)	Loss 3.2276e-01 (3.5376e-01)	Acc@1  92.00 ( 92.68)	Acc@5 100.00 ( 99.81)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 2.1322e-01 (3.5137e-01)	Acc@1  94.00 ( 92.63)	Acc@5 100.00 ( 99.80)
Test: [ 50/100]	Time  0.077 ( 0.080)	Loss 2.3550e-01 (3.4856e-01)	Acc@1  95.00 ( 92.71)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 5.6570e-01 (3.3652e-01)	Acc@1  92.00 ( 92.79)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.079 ( 0.080)	Loss 5.2494e-01 (3.3812e-01)	Acc@1  90.00 ( 92.72)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.078 ( 0.079)	Loss 2.2710e-01 (3.3107e-01)	Acc@1  93.00 ( 92.90)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.7217e-01 (3.3355e-01)	Acc@1  94.00 ( 92.81)	Acc@5 100.00 ( 99.77)
 * Acc@1 92.770 Acc@5 99.780
### epoch[64] execution time: 102.34464120864868
EPOCH 65
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [65][  0/391]	Time  0.396 ( 0.396)	Data  0.153 ( 0.153)	Loss 9.3086e-03 (9.3086e-03)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [65][ 10/391]	Time  0.241 ( 0.255)	Data  0.001 ( 0.015)	Loss 5.3578e-03 (5.5991e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [65][ 20/391]	Time  0.239 ( 0.248)	Data  0.001 ( 0.008)	Loss 1.8707e-03 (6.6326e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [65][ 30/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.006)	Loss 1.1202e-02 (6.4738e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [65][ 40/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.005)	Loss 9.5173e-04 (5.9862e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [65][ 50/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 4.5809e-03 (6.2477e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [65][ 60/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.004)	Loss 3.5159e-03 (6.2842e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [65][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 5.6300e-03 (6.2127e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [65][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 4.4817e-03 (6.4502e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [65][ 90/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 6.5217e-03 (6.2726e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [65][100/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.5791e-02 (6.2243e-03)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [65][110/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 6.4211e-03 (6.0964e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [65][120/391]	Time  0.247 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.7157e-03 (6.0366e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [65][130/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8687e-02 (6.5273e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [65][140/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1021e-03 (6.4014e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [65][150/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.6516e-03 (6.2489e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [65][160/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2038e-02 (6.2698e-03)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [65][170/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3952e-02 (6.3361e-03)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [65][180/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8053e-03 (6.2089e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [65][190/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.4762e-02 (6.2537e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [65][200/391]	Time  0.243 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.6511e-03 (6.2832e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [65][210/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.7451e-03 (6.2641e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [65][220/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.8353e-03 (6.1144e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [65][230/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.3406e-03 (6.0819e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [65][240/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.3700e-03 (6.1599e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [65][250/391]	Time  0.246 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.0022e-03 (6.1063e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [65][260/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.0877e-03 (6.2100e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [65][270/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.2849e-03 (6.1094e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [65][280/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.0024e-03 (6.1250e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [65][290/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.9346e-03 (6.1169e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [65][300/391]	Time  0.245 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.7461e-02 (6.1049e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [65][310/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.5975e-03 (6.0556e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [65][320/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.4973e-03 (6.1715e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [65][330/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.5708e-03 (6.1264e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [65][340/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0188e-02 (6.3277e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [65][350/391]	Time  0.244 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.7450e-03 (6.4074e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [65][360/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.8478e-03 (6.3456e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [65][370/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.6160e-03 (6.3677e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [65][380/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.7024e-03 (6.3854e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [65][390/391]	Time  0.177 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.6172e-03 (6.3629e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
## e[65] optimizer.zero_grad (sum) time: 0.4758298397064209
## e[65]       loss.backward (sum) time: 10.83306360244751
## e[65]      optimizer.step (sum) time: 47.56574869155884
## epoch[65] training(only) time: 94.33239817619324
# Switched to evaluate mode...
Test: [  0/100]	Time  0.203 ( 0.203)	Loss 2.7349e-01 (2.7349e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.078 ( 0.089)	Loss 1.8502e-01 (3.4411e-01)	Acc@1  96.00 ( 92.91)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 4.3458e-01 (3.5734e-01)	Acc@1  92.00 ( 92.81)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.077 ( 0.082)	Loss 3.3441e-01 (3.6542e-01)	Acc@1  92.00 ( 92.61)	Acc@5 100.00 ( 99.81)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 2.3725e-01 (3.5726e-01)	Acc@1  95.00 ( 92.73)	Acc@5 100.00 ( 99.80)
Test: [ 50/100]	Time  0.077 ( 0.080)	Loss 1.9357e-01 (3.5318e-01)	Acc@1  96.00 ( 92.76)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.079 ( 0.080)	Loss 5.9390e-01 (3.4117e-01)	Acc@1  93.00 ( 92.85)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.077 ( 0.080)	Loss 5.4957e-01 (3.4103e-01)	Acc@1  89.00 ( 92.75)	Acc@5  99.00 ( 99.76)
Test: [ 80/100]	Time  0.078 ( 0.079)	Loss 2.7377e-01 (3.3593e-01)	Acc@1  93.00 ( 92.80)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.077 ( 0.079)	Loss 1.6450e-01 (3.3659e-01)	Acc@1  94.00 ( 92.75)	Acc@5 100.00 ( 99.77)
 * Acc@1 92.700 Acc@5 99.780
### epoch[65] execution time: 102.33849287033081
EPOCH 66
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [66][  0/391]	Time  0.381 ( 0.381)	Data  0.139 ( 0.139)	Loss 7.8497e-03 (7.8497e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [66][ 10/391]	Time  0.240 ( 0.253)	Data  0.001 ( 0.014)	Loss 2.0029e-03 (4.5832e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [66][ 20/391]	Time  0.240 ( 0.247)	Data  0.001 ( 0.008)	Loss 2.9500e-03 (5.5312e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [66][ 30/391]	Time  0.249 ( 0.245)	Data  0.001 ( 0.006)	Loss 4.5289e-03 (5.4185e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [66][ 40/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.005)	Loss 4.0692e-03 (5.9667e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [66][ 50/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.004)	Loss 4.0464e-03 (5.7853e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [66][ 60/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.4831e-03 (6.7291e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [66][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.0244e-03 (6.5144e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [66][ 80/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.9250e-03 (6.0775e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [66][ 90/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 3.3617e-03 (6.0780e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [66][100/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 6.5864e-03 (6.0646e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [66][110/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5456e-02 (6.6430e-03)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [66][120/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9896e-03 (6.7048e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [66][130/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8713e-03 (6.5833e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [66][140/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2957e-03 (6.7904e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [66][150/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2471e-02 (6.6028e-03)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [66][160/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.6829e-03 (6.6371e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [66][170/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.3735e-02 (6.9032e-03)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [66][180/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.0134e-03 (6.8399e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [66][190/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.5513e-03 (6.8627e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [66][200/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.8091e-03 (6.7654e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [66][210/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.7445e-03 (6.7364e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [66][220/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.4586e-03 (6.6337e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [66][230/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.7804e-03 (6.5460e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [66][240/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.5426e-03 (6.5468e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [66][250/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.7992e-03 (6.8253e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [66][260/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.6590e-03 (6.7007e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [66][270/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.3958e-03 (6.6520e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [66][280/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.8105e-03 (6.5835e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [66][290/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1967e-03 (6.7013e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [66][300/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.3635e-03 (6.6343e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [66][310/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.3799e-02 (6.6927e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [66][320/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.2693e-03 (6.5991e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [66][330/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.7891e-02 (6.5658e-03)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [66][340/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.2590e-03 (6.5100e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [66][350/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.3739e-03 (6.5024e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [66][360/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.6737e-03 (6.4888e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [66][370/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.7815e-03 (6.4584e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [66][380/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.4943e-03 (6.5353e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [66][390/391]	Time  0.177 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.4835e-02 (6.4916e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
## e[66] optimizer.zero_grad (sum) time: 0.48032379150390625
## e[66]       loss.backward (sum) time: 10.834910154342651
## e[66]      optimizer.step (sum) time: 47.52954816818237
## epoch[66] training(only) time: 94.24765372276306
# Switched to evaluate mode...
Test: [  0/100]	Time  0.204 ( 0.204)	Loss 2.5426e-01 (2.5426e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.078 ( 0.089)	Loss 1.8618e-01 (3.3889e-01)	Acc@1  96.00 ( 92.91)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 4.3541e-01 (3.5182e-01)	Acc@1  92.00 ( 92.71)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 3.1918e-01 (3.6048e-01)	Acc@1  92.00 ( 92.61)	Acc@5 100.00 ( 99.77)
Test: [ 40/100]	Time  0.077 ( 0.081)	Loss 2.2156e-01 (3.5723e-01)	Acc@1  93.00 ( 92.54)	Acc@5 100.00 ( 99.76)
Test: [ 50/100]	Time  0.079 ( 0.080)	Loss 2.2151e-01 (3.5447e-01)	Acc@1  95.00 ( 92.57)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.077 ( 0.080)	Loss 5.5846e-01 (3.4169e-01)	Acc@1  93.00 ( 92.69)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.077 ( 0.080)	Loss 5.2943e-01 (3.4143e-01)	Acc@1  91.00 ( 92.73)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.078 ( 0.079)	Loss 2.5214e-01 (3.3567e-01)	Acc@1  93.00 ( 92.84)	Acc@5 100.00 ( 99.73)
Test: [ 90/100]	Time  0.077 ( 0.079)	Loss 1.4311e-01 (3.3532e-01)	Acc@1  96.00 ( 92.82)	Acc@5 100.00 ( 99.76)
 * Acc@1 92.730 Acc@5 99.770
### epoch[66] execution time: 102.24730706214905
EPOCH 67
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [67][  0/391]	Time  0.378 ( 0.378)	Data  0.133 ( 0.133)	Loss 1.6928e-02 (1.6928e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [67][ 10/391]	Time  0.241 ( 0.253)	Data  0.001 ( 0.013)	Loss 8.1724e-04 (4.5821e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [67][ 20/391]	Time  0.240 ( 0.247)	Data  0.001 ( 0.007)	Loss 1.6369e-02 (5.1590e-03)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [67][ 30/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.005)	Loss 3.1705e-03 (6.1146e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [67][ 40/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 5.5503e-03 (5.3606e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [67][ 50/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.004)	Loss 2.1803e-02 (5.5829e-03)	Acc@1  99.22 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [67][ 60/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.4580e-03 (5.5715e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [67][ 70/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 8.0920e-04 (5.6773e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [67][ 80/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.1688e-02 (5.7121e-03)	Acc@1  99.22 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [67][ 90/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.003)	Loss 5.1762e-03 (5.4432e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [67][100/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.4833e-03 (5.5009e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [67][110/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.6554e-03 (5.9680e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [67][120/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.8444e-02 (6.2105e-03)	Acc@1  98.44 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [67][130/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.9993e-03 (6.2017e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [67][140/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.7422e-03 (6.0095e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [67][150/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.6945e-03 (5.8898e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [67][160/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.3819e-02 (6.0583e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [67][170/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.6636e-03 (5.9870e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [67][180/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0725e-02 (6.0874e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [67][190/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.8014e-03 (6.0976e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [67][200/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.8760e-03 (6.1459e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [67][210/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.4846e-03 (6.1358e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [67][220/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.5394e-03 (6.1599e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [67][230/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.9432e-03 (6.0318e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [67][240/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.4330e-03 (6.0236e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [67][250/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.4576e-02 (5.9788e-03)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [67][260/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.7292e-03 (5.9194e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [67][270/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.6328e-03 (5.8354e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [67][280/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.3194e-03 (5.8493e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [67][290/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.3715e-03 (5.7927e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [67][300/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1358e-02 (5.8071e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [67][310/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.8946e-03 (5.8433e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [67][320/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.5785e-03 (5.9737e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [67][330/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.5394e-03 (5.9427e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [67][340/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.9438e-03 (6.1162e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [67][350/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.7645e-02 (6.1244e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [67][360/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.7309e-04 (6.1452e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [67][370/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.001)	Loss 5.9534e-03 (6.1748e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [67][380/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.001)	Loss 6.5887e-03 (6.2844e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [67][390/391]	Time  0.177 ( 0.241)	Data  0.001 ( 0.001)	Loss 8.4636e-04 (6.2629e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
## e[67] optimizer.zero_grad (sum) time: 0.47786736488342285
## e[67]       loss.backward (sum) time: 10.794512271881104
## e[67]      optimizer.step (sum) time: 47.56623554229736
## epoch[67] training(only) time: 94.24601149559021
# Switched to evaluate mode...
Test: [  0/100]	Time  0.206 ( 0.206)	Loss 2.3520e-01 (2.3520e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.078 ( 0.089)	Loss 1.9865e-01 (3.3259e-01)	Acc@1  96.00 ( 93.27)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 4.6220e-01 (3.4868e-01)	Acc@1  92.00 ( 93.05)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.079 ( 0.082)	Loss 3.4809e-01 (3.5644e-01)	Acc@1  92.00 ( 92.71)	Acc@5 100.00 ( 99.81)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 2.0864e-01 (3.5536e-01)	Acc@1  94.00 ( 92.71)	Acc@5 100.00 ( 99.83)
Test: [ 50/100]	Time  0.078 ( 0.080)	Loss 2.3598e-01 (3.5269e-01)	Acc@1  95.00 ( 92.80)	Acc@5 100.00 ( 99.80)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 5.9209e-01 (3.4149e-01)	Acc@1  93.00 ( 92.82)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.077 ( 0.080)	Loss 5.1582e-01 (3.4289e-01)	Acc@1  91.00 ( 92.85)	Acc@5  99.00 ( 99.76)
Test: [ 80/100]	Time  0.078 ( 0.079)	Loss 2.3995e-01 (3.3571e-01)	Acc@1  93.00 ( 92.99)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.077 ( 0.079)	Loss 1.4488e-01 (3.3645e-01)	Acc@1  95.00 ( 92.95)	Acc@5 100.00 ( 99.77)
 * Acc@1 92.890 Acc@5 99.780
### epoch[67] execution time: 102.24503326416016
EPOCH 68
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [68][  0/391]	Time  0.392 ( 0.392)	Data  0.151 ( 0.151)	Loss 3.7157e-03 (3.7157e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [68][ 10/391]	Time  0.240 ( 0.254)	Data  0.001 ( 0.015)	Loss 1.1768e-02 (5.3099e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [68][ 20/391]	Time  0.244 ( 0.248)	Data  0.001 ( 0.008)	Loss 3.9805e-03 (6.8404e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [68][ 30/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.006)	Loss 1.0021e-02 (6.4381e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [68][ 40/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.005)	Loss 1.0180e-02 (5.9868e-03)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [68][ 50/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.004)	Loss 9.9935e-03 (5.9264e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [68][ 60/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.004)	Loss 9.9614e-03 (5.8727e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [68][ 70/391]	Time  0.246 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.2031e-03 (5.8626e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [68][ 80/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 2.0192e-03 (5.7952e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [68][ 90/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 5.0283e-03 (5.6812e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [68][100/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 3.4021e-03 (6.0879e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [68][110/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.2953e-02 (5.9659e-03)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [68][120/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0058e-03 (5.8422e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [68][130/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.6908e-03 (5.6840e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [68][140/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0039e-03 (5.6221e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [68][150/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.1151e-03 (5.5444e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [68][160/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.6942e-03 (5.5507e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [68][170/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.0596e-03 (5.6211e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [68][180/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2446e-03 (5.7223e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [68][190/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.4793e-03 (5.6369e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [68][200/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.8702e-03 (5.5574e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [68][210/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.2154e-03 (5.7181e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [68][220/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.4775e-03 (5.7017e-03)	Acc@1  99.22 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [68][230/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.5973e-03 (5.6563e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [68][240/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.9109e-03 (5.6010e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [68][250/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.4344e-02 (5.6513e-03)	Acc@1  99.22 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [68][260/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.3422e-03 (5.6567e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [68][270/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.0195e-04 (5.5746e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [68][280/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.1670e-03 (5.6498e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [68][290/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.0314e-03 (5.8316e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [68][300/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.0829e-03 (5.8655e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [68][310/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.6103e-03 (5.9610e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [68][320/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.2570e-03 (5.9909e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [68][330/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.8508e-03 (5.9699e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [68][340/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.9491e-02 (6.0791e-03)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [68][350/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.3506e-03 (6.1012e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [68][360/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.0079e-03 (6.1077e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [68][370/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.5844e-03 (6.0474e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [68][380/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.0902e-03 (6.1217e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [68][390/391]	Time  0.177 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.7583e-02 (6.1335e-03)	Acc@1  98.75 ( 99.89)	Acc@5 100.00 (100.00)
## e[68] optimizer.zero_grad (sum) time: 0.47560763359069824
## e[68]       loss.backward (sum) time: 10.873356580734253
## e[68]      optimizer.step (sum) time: 47.508060693740845
## epoch[68] training(only) time: 94.23460459709167
# Switched to evaluate mode...
Test: [  0/100]	Time  0.213 ( 0.213)	Loss 2.5307e-01 (2.5307e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.8681e-01 (3.3248e-01)	Acc@1  96.00 ( 93.27)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.077 ( 0.084)	Loss 4.0466e-01 (3.4462e-01)	Acc@1  92.00 ( 93.10)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 3.1925e-01 (3.5763e-01)	Acc@1  93.00 ( 92.77)	Acc@5 100.00 ( 99.77)
Test: [ 40/100]	Time  0.077 ( 0.081)	Loss 2.3244e-01 (3.5530e-01)	Acc@1  93.00 ( 92.71)	Acc@5 100.00 ( 99.78)
Test: [ 50/100]	Time  0.078 ( 0.080)	Loss 2.0720e-01 (3.5172e-01)	Acc@1  95.00 ( 92.76)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 6.0434e-01 (3.4030e-01)	Acc@1  91.00 ( 92.84)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 5.3103e-01 (3.4095e-01)	Acc@1  89.00 ( 92.75)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.078 ( 0.079)	Loss 2.9045e-01 (3.3522e-01)	Acc@1  93.00 ( 92.85)	Acc@5 100.00 ( 99.73)
Test: [ 90/100]	Time  0.077 ( 0.079)	Loss 1.7077e-01 (3.3532e-01)	Acc@1  95.00 ( 92.80)	Acc@5 100.00 ( 99.76)
 * Acc@1 92.740 Acc@5 99.770
### epoch[68] execution time: 102.23370552062988
EPOCH 69
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [69][  0/391]	Time  0.392 ( 0.392)	Data  0.151 ( 0.151)	Loss 1.2838e-03 (1.2838e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [69][ 10/391]	Time  0.241 ( 0.254)	Data  0.001 ( 0.015)	Loss 1.0289e-03 (5.5233e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [69][ 20/391]	Time  0.240 ( 0.247)	Data  0.001 ( 0.008)	Loss 2.4703e-03 (6.1094e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [69][ 30/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.006)	Loss 3.9002e-02 (7.7901e-03)	Acc@1  98.44 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [69][ 40/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.005)	Loss 5.6458e-04 (7.1343e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [69][ 50/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.004)	Loss 1.3627e-03 (7.4684e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [69][ 60/391]	Time  0.239 ( 0.243)	Data  0.001 ( 0.004)	Loss 2.2798e-03 (8.2623e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [69][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 4.3554e-03 (7.8375e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [69][ 80/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 4.2314e-03 (7.3904e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [69][ 90/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.8933e-02 (7.3710e-03)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [69][100/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.1908e-03 (7.2350e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [69][110/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8398e-03 (6.8887e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [69][120/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.7749e-03 (6.8432e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [69][130/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.1928e-03 (6.5606e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [69][140/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.8483e-04 (6.6431e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [69][150/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.0492e-03 (6.5109e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [69][160/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.3334e-02 (6.4148e-03)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [69][170/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.3200e-03 (6.2485e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [69][180/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.6459e-03 (6.2470e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [69][190/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.1102e-03 (6.1416e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [69][200/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.6067e-04 (5.9660e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [69][210/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.1924e-03 (5.9952e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [69][220/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.4197e-03 (5.9200e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [69][230/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.4509e-03 (5.8669e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [69][240/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.7428e-03 (6.0851e-03)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [69][250/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.4686e-03 (6.1574e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [69][260/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.3875e-03 (6.1351e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [69][270/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.5932e-03 (6.0106e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [69][280/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.9297e-03 (5.9318e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [69][290/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.7879e-02 (5.9198e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [69][300/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.7949e-03 (5.9901e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [69][310/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.7942e-03 (5.9024e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [69][320/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.0449e-03 (5.8594e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [69][330/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.5483e-03 (5.8473e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [69][340/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.7741e-03 (5.8954e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [69][350/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.6957e-03 (5.8570e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [69][360/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.1812e-03 (5.8281e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [69][370/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.4028e-03 (5.8245e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [69][380/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.0952e-03 (5.8806e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [69][390/391]	Time  0.175 ( 0.241)	Data  0.001 ( 0.001)	Loss 1.8279e-02 (5.8821e-03)	Acc@1  98.75 ( 99.86)	Acc@5 100.00 (100.00)
## e[69] optimizer.zero_grad (sum) time: 0.47527623176574707
## e[69]       loss.backward (sum) time: 10.799564838409424
## e[69]      optimizer.step (sum) time: 47.56653571128845
## epoch[69] training(only) time: 94.25886821746826
# Switched to evaluate mode...
Test: [  0/100]	Time  0.214 ( 0.214)	Loss 2.3869e-01 (2.3869e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.9365e-01 (3.3655e-01)	Acc@1  96.00 ( 92.91)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 4.2014e-01 (3.4885e-01)	Acc@1  92.00 ( 92.76)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 3.1349e-01 (3.5590e-01)	Acc@1  93.00 ( 92.65)	Acc@5 100.00 ( 99.81)
Test: [ 40/100]	Time  0.077 ( 0.081)	Loss 2.2383e-01 (3.5239e-01)	Acc@1  94.00 ( 92.78)	Acc@5 100.00 ( 99.80)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 2.1095e-01 (3.4969e-01)	Acc@1  95.00 ( 92.82)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 5.8598e-01 (3.3783e-01)	Acc@1  92.00 ( 92.85)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.077 ( 0.080)	Loss 5.3845e-01 (3.3868e-01)	Acc@1  91.00 ( 92.80)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.077 ( 0.080)	Loss 2.5283e-01 (3.3262e-01)	Acc@1  93.00 ( 92.93)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.3745e-01 (3.3277e-01)	Acc@1  96.00 ( 92.90)	Acc@5 100.00 ( 99.77)
 * Acc@1 92.830 Acc@5 99.780
### epoch[69] execution time: 102.2718300819397
EPOCH 70
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [70][  0/391]	Time  0.401 ( 0.401)	Data  0.147 ( 0.147)	Loss 1.7182e-03 (1.7182e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [70][ 10/391]	Time  0.240 ( 0.254)	Data  0.001 ( 0.014)	Loss 1.9689e-02 (8.6018e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [70][ 20/391]	Time  0.240 ( 0.248)	Data  0.001 ( 0.008)	Loss 1.3273e-02 (7.4239e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [70][ 30/391]	Time  0.239 ( 0.245)	Data  0.001 ( 0.006)	Loss 1.2554e-03 (6.6945e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [70][ 40/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.005)	Loss 1.0721e-02 (6.5262e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [70][ 50/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.004)	Loss 4.5480e-03 (6.1016e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [70][ 60/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.004)	Loss 4.4576e-03 (5.9071e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [70][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.7177e-03 (5.7994e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [70][ 80/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.3487e-03 (5.5909e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [70][ 90/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.1431e-02 (5.5419e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [70][100/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 3.5660e-03 (5.3468e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [70][110/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.7892e-03 (5.4450e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [70][120/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2993e-02 (5.5954e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [70][130/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.4224e-03 (5.4811e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [70][140/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8105e-03 (5.4814e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [70][150/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5521e-03 (5.5246e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [70][160/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.9915e-03 (5.4580e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [70][170/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2184e-02 (5.6477e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [70][180/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.0542e-03 (5.7762e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [70][190/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.2312e-03 (5.7238e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [70][200/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.1059e-03 (5.6534e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [70][210/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.9481e-03 (5.6406e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [70][220/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.1376e-03 (5.5548e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [70][230/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.8256e-03 (5.6580e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [70][240/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.9315e-03 (5.6245e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [70][250/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0326e-02 (5.5951e-03)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [70][260/391]	Time  0.244 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.9145e-03 (5.5422e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [70][270/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2372e-02 (5.5608e-03)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [70][280/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.9600e-03 (5.6504e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [70][290/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.1585e-03 (5.6020e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [70][300/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.8395e-03 (5.5862e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [70][310/391]	Time  0.246 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.9198e-03 (5.6065e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [70][320/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.4079e-03 (5.5245e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [70][330/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.9108e-04 (5.4685e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [70][340/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1694e-03 (5.3994e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [70][350/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.1138e-03 (5.4248e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [70][360/391]	Time  0.244 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.4926e-03 (5.5561e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [70][370/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.3409e-03 (5.5320e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [70][380/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.3774e-03 (5.4927e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [70][390/391]	Time  0.176 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.0467e-03 (5.4859e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
## e[70] optimizer.zero_grad (sum) time: 0.4758493900299072
## e[70]       loss.backward (sum) time: 10.825977802276611
## e[70]      optimizer.step (sum) time: 47.51481318473816
## epoch[70] training(only) time: 94.26911807060242
# Switched to evaluate mode...
Test: [  0/100]	Time  0.207 ( 0.207)	Loss 2.5562e-01 (2.5562e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 2.1470e-01 (3.3703e-01)	Acc@1  96.00 ( 93.27)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 4.3172e-01 (3.5379e-01)	Acc@1  91.00 ( 92.90)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 3.3611e-01 (3.6148e-01)	Acc@1  92.00 ( 92.71)	Acc@5 100.00 ( 99.81)
Test: [ 40/100]	Time  0.079 ( 0.081)	Loss 2.3303e-01 (3.5602e-01)	Acc@1  92.00 ( 92.73)	Acc@5 100.00 ( 99.83)
Test: [ 50/100]	Time  0.078 ( 0.080)	Loss 2.2142e-01 (3.5237e-01)	Acc@1  95.00 ( 92.75)	Acc@5 100.00 ( 99.80)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 5.8944e-01 (3.4112e-01)	Acc@1  94.00 ( 92.77)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 5.3012e-01 (3.4144e-01)	Acc@1  89.00 ( 92.73)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.077 ( 0.079)	Loss 2.8562e-01 (3.3554e-01)	Acc@1  93.00 ( 92.84)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.3988e-01 (3.3515e-01)	Acc@1  94.00 ( 92.82)	Acc@5 100.00 ( 99.78)
 * Acc@1 92.800 Acc@5 99.790
### epoch[70] execution time: 102.2674171924591
EPOCH 71
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [71][  0/391]	Time  0.387 ( 0.387)	Data  0.147 ( 0.147)	Loss 6.5958e-03 (6.5958e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [71][ 10/391]	Time  0.241 ( 0.254)	Data  0.001 ( 0.014)	Loss 6.7573e-03 (4.7767e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [71][ 20/391]	Time  0.240 ( 0.247)	Data  0.001 ( 0.008)	Loss 1.4577e-03 (4.8210e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [71][ 30/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.006)	Loss 5.9079e-03 (4.6744e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [71][ 40/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.005)	Loss 3.1508e-03 (4.9845e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [71][ 50/391]	Time  0.239 ( 0.243)	Data  0.001 ( 0.004)	Loss 2.8748e-03 (5.0708e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [71][ 60/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.004)	Loss 6.2303e-03 (5.1565e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [71][ 70/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.5567e-03 (5.1636e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [71][ 80/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.6101e-02 (5.3053e-03)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [71][ 90/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 3.5775e-03 (5.1699e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [71][100/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 2.1825e-03 (5.0718e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [71][110/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.2072e-03 (5.0443e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [71][120/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.6073e-03 (4.8742e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [71][130/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.1428e-03 (5.1280e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [71][140/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.5653e-03 (5.1902e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [71][150/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.3530e-03 (5.1940e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [71][160/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.8052e-03 (5.1930e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [71][170/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.0933e-03 (5.3183e-03)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [71][180/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.1699e-03 (5.2858e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [71][190/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.0248e-03 (5.2837e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [71][200/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.0213e-03 (5.2517e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [71][210/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.5647e-03 (5.4582e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [71][220/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.0132e-02 (5.5942e-03)	Acc@1  97.66 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [71][230/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.7581e-03 (5.5829e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [71][240/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.2095e-02 (5.5843e-03)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [71][250/391]	Time  0.243 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.6921e-03 (5.5540e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [71][260/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.1165e-04 (5.4750e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [71][270/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.1516e-03 (5.3969e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [71][280/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.0129e-03 (5.4573e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [71][290/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.1561e-03 (5.3986e-03)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [71][300/391]	Time  0.249 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.9618e-04 (5.4462e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [71][310/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.9066e-03 (5.3909e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [71][320/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.4725e-02 (5.3691e-03)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [71][330/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.1128e-03 (5.2996e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [71][340/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.2342e-03 (5.3998e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [71][350/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.9269e-03 (5.3944e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [71][360/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.4637e-03 (5.4359e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [71][370/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.8898e-02 (5.4940e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [71][380/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.001)	Loss 4.8801e-03 (5.4671e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [71][390/391]	Time  0.178 ( 0.241)	Data  0.001 ( 0.001)	Loss 1.0629e-03 (5.4272e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
## e[71] optimizer.zero_grad (sum) time: 0.4741232395172119
## e[71]       loss.backward (sum) time: 10.799762487411499
## e[71]      optimizer.step (sum) time: 47.54962396621704
## epoch[71] training(only) time: 94.19824123382568
# Switched to evaluate mode...
Test: [  0/100]	Time  0.211 ( 0.211)	Loss 2.3820e-01 (2.3820e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.077 ( 0.090)	Loss 1.7552e-01 (3.2822e-01)	Acc@1  96.00 ( 93.18)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 4.1587e-01 (3.4435e-01)	Acc@1  92.00 ( 92.90)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.077 ( 0.082)	Loss 3.2136e-01 (3.5326e-01)	Acc@1  92.00 ( 92.71)	Acc@5 100.00 ( 99.81)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 2.2564e-01 (3.4985e-01)	Acc@1  90.00 ( 92.71)	Acc@5 100.00 ( 99.80)
Test: [ 50/100]	Time  0.077 ( 0.081)	Loss 2.3529e-01 (3.4792e-01)	Acc@1  94.00 ( 92.76)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 6.0113e-01 (3.3747e-01)	Acc@1  92.00 ( 92.80)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 5.1210e-01 (3.3878e-01)	Acc@1  90.00 ( 92.72)	Acc@5  99.00 ( 99.75)
Test: [ 80/100]	Time  0.077 ( 0.080)	Loss 2.5232e-01 (3.3242e-01)	Acc@1  93.00 ( 92.84)	Acc@5 100.00 ( 99.73)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.3046e-01 (3.3213e-01)	Acc@1  95.00 ( 92.82)	Acc@5 100.00 ( 99.76)
 * Acc@1 92.760 Acc@5 99.770
### epoch[71] execution time: 102.21889114379883
EPOCH 72
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [72][  0/391]	Time  0.404 ( 0.404)	Data  0.156 ( 0.156)	Loss 1.7212e-03 (1.7212e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [72][ 10/391]	Time  0.240 ( 0.256)	Data  0.001 ( 0.015)	Loss 1.5942e-03 (4.7183e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [72][ 20/391]	Time  0.241 ( 0.249)	Data  0.001 ( 0.009)	Loss 1.7294e-02 (5.6187e-03)	Acc@1  98.44 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [72][ 30/391]	Time  0.240 ( 0.246)	Data  0.001 ( 0.006)	Loss 4.1907e-03 (6.2942e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [72][ 40/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.005)	Loss 3.3457e-03 (6.5330e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [72][ 50/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.8341e-03 (6.1718e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [72][ 60/391]	Time  0.239 ( 0.243)	Data  0.001 ( 0.004)	Loss 1.5544e-03 (5.9834e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [72][ 70/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.8279e-03 (5.7571e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [72][ 80/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.4114e-03 (5.6208e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [72][ 90/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.9039e-03 (5.6646e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [72][100/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 4.2465e-03 (5.6838e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [72][110/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 3.0783e-03 (5.4609e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [72][120/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0953e-03 (6.1728e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [72][130/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.9287e-03 (6.1700e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [72][140/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.0141e-03 (6.1003e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [72][150/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9137e-03 (6.0553e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [72][160/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.0589e-03 (6.0282e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [72][170/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5611e-03 (6.2724e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [72][180/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.6189e-03 (6.3214e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [72][190/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8255e-03 (6.2289e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [72][200/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.1001e-02 (6.3247e-03)	Acc@1  98.44 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [72][210/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2199e-03 (6.2802e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [72][220/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.9469e-03 (6.2119e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [72][230/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.8086e-03 (6.1498e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [72][240/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0726e-02 (6.1300e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [72][250/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6287e-03 (6.0794e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [72][260/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.3398e-02 (6.2369e-03)	Acc@1  98.44 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [72][270/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.1613e-03 (6.2339e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [72][280/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.8506e-03 (6.1769e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [72][290/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.4309e-03 (6.3249e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [72][300/391]	Time  0.246 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.2937e-03 (6.2813e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [72][310/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.6323e-03 (6.2124e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [72][320/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.9447e-03 (6.1600e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [72][330/391]	Time  0.247 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.2920e-03 (6.1969e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [72][340/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.7625e-03 (6.1474e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [72][350/391]	Time  0.245 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.0452e-03 (6.2208e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [72][360/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.8018e-03 (6.1473e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [72][370/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.8516e-03 (6.1089e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [72][380/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.8174e-03 (6.0532e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [72][390/391]	Time  0.177 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.3013e-02 (6.0190e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
## e[72] optimizer.zero_grad (sum) time: 0.4790196418762207
## e[72]       loss.backward (sum) time: 10.791521549224854
## e[72]      optimizer.step (sum) time: 47.61019420623779
## epoch[72] training(only) time: 94.4205105304718
# Switched to evaluate mode...
Test: [  0/100]	Time  0.207 ( 0.207)	Loss 2.4591e-01 (2.4591e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.9394e-01 (3.3622e-01)	Acc@1  96.00 ( 93.45)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 4.0791e-01 (3.4495e-01)	Acc@1  91.00 ( 93.19)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 3.1973e-01 (3.5024e-01)	Acc@1  92.00 ( 93.03)	Acc@5 100.00 ( 99.81)
Test: [ 40/100]	Time  0.077 ( 0.081)	Loss 2.2153e-01 (3.4757e-01)	Acc@1  92.00 ( 93.05)	Acc@5 100.00 ( 99.83)
Test: [ 50/100]	Time  0.079 ( 0.081)	Loss 2.3125e-01 (3.4627e-01)	Acc@1  95.00 ( 93.06)	Acc@5 100.00 ( 99.80)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 5.8283e-01 (3.3540e-01)	Acc@1  92.00 ( 93.03)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.077 ( 0.080)	Loss 5.6268e-01 (3.3843e-01)	Acc@1  90.00 ( 92.92)	Acc@5  99.00 ( 99.76)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 2.4348e-01 (3.3230e-01)	Acc@1  95.00 ( 93.07)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.078 ( 0.080)	Loss 1.3189e-01 (3.3243e-01)	Acc@1  95.00 ( 93.04)	Acc@5 100.00 ( 99.77)
 * Acc@1 92.970 Acc@5 99.780
### epoch[72] execution time: 102.44808340072632
EPOCH 73
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [73][  0/391]	Time  0.393 ( 0.393)	Data  0.152 ( 0.152)	Loss 1.5243e-03 (1.5243e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [73][ 10/391]	Time  0.240 ( 0.254)	Data  0.001 ( 0.015)	Loss 1.4640e-03 (4.6790e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [73][ 20/391]	Time  0.241 ( 0.248)	Data  0.001 ( 0.008)	Loss 1.0187e-03 (4.8879e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [73][ 30/391]	Time  0.243 ( 0.246)	Data  0.001 ( 0.006)	Loss 1.2837e-02 (5.5919e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [73][ 40/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.005)	Loss 3.3271e-03 (5.3777e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [73][ 50/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 6.6788e-03 (5.5913e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [73][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.2530e-03 (6.1498e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [73][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.1069e-03 (5.8991e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [73][ 80/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.003)	Loss 6.0043e-03 (6.0548e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [73][ 90/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.3710e-03 (5.7151e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [73][100/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.7193e-03 (5.5284e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [73][110/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 2.2550e-03 (5.6393e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [73][120/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.7387e-03 (5.5876e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [73][130/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.6086e-03 (5.8951e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [73][140/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.7531e-03 (5.9034e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [73][150/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8591e-03 (5.9138e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [73][160/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.9184e-03 (5.9934e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [73][170/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.7677e-03 (6.0139e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [73][180/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.7751e-03 (5.9965e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [73][190/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.9235e-02 (6.1836e-03)	Acc@1  98.44 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [73][200/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.8842e-03 (6.1396e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [73][210/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.3021e-03 (6.0480e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [73][220/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3953e-03 (5.9302e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [73][230/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0049e-02 (5.9421e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [73][240/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1941e-02 (5.9385e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [73][250/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3576e-02 (5.9669e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [73][260/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.9997e-03 (6.0048e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [73][270/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.3551e-03 (5.9623e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [73][280/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2022e-03 (5.9207e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [73][290/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.9143e-03 (5.9156e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [73][300/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0109e-03 (5.9216e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [73][310/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.3871e-03 (5.8926e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [73][320/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1977e-02 (5.8860e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [73][330/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.3261e-03 (5.8882e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [73][340/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5559e-03 (5.9145e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [73][350/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3604e-02 (5.9109e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [73][360/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.5543e-03 (5.8496e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [73][370/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8305e-02 (5.8350e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [73][380/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0706e-03 (5.8428e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [73][390/391]	Time  0.177 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.3977e-03 (5.7728e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
## e[73] optimizer.zero_grad (sum) time: 0.47697877883911133
## e[73]       loss.backward (sum) time: 10.891335248947144
## e[73]      optimizer.step (sum) time: 47.567110776901245
## epoch[73] training(only) time: 94.46870946884155
# Switched to evaluate mode...
Test: [  0/100]	Time  0.207 ( 0.207)	Loss 2.3897e-01 (2.3897e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.079 ( 0.089)	Loss 1.9386e-01 (3.3533e-01)	Acc@1  96.00 ( 93.18)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 4.1766e-01 (3.5017e-01)	Acc@1  92.00 ( 92.86)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 3.2072e-01 (3.5717e-01)	Acc@1  92.00 ( 92.68)	Acc@5 100.00 ( 99.81)
Test: [ 40/100]	Time  0.077 ( 0.081)	Loss 2.3741e-01 (3.5464e-01)	Acc@1  90.00 ( 92.63)	Acc@5 100.00 ( 99.80)
Test: [ 50/100]	Time  0.078 ( 0.080)	Loss 2.2698e-01 (3.5192e-01)	Acc@1  95.00 ( 92.73)	Acc@5 100.00 ( 99.80)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 5.8026e-01 (3.4051e-01)	Acc@1  92.00 ( 92.80)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 5.1797e-01 (3.4132e-01)	Acc@1  91.00 ( 92.79)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.078 ( 0.079)	Loss 2.5327e-01 (3.3535e-01)	Acc@1  94.00 ( 92.90)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.077 ( 0.079)	Loss 1.1593e-01 (3.3451e-01)	Acc@1  95.00 ( 92.87)	Acc@5 100.00 ( 99.78)
 * Acc@1 92.790 Acc@5 99.790
### epoch[73] execution time: 102.46992659568787
EPOCH 74
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [74][  0/391]	Time  0.398 ( 0.398)	Data  0.156 ( 0.156)	Loss 5.9045e-03 (5.9045e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [74][ 10/391]	Time  0.240 ( 0.256)	Data  0.001 ( 0.015)	Loss 1.0141e-02 (4.5757e-03)	Acc@1  99.22 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [74][ 20/391]	Time  0.240 ( 0.249)	Data  0.001 ( 0.009)	Loss 8.4013e-04 (3.3812e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [74][ 30/391]	Time  0.249 ( 0.246)	Data  0.001 ( 0.006)	Loss 2.4811e-03 (4.1634e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [74][ 40/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.005)	Loss 2.8873e-03 (5.3492e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [74][ 50/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 6.5252e-03 (5.1430e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [74][ 60/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 3.2435e-03 (5.1633e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [74][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.2142e-02 (5.4561e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [74][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 5.6734e-03 (5.3385e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [74][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.9260e-03 (5.2232e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [74][100/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 5.9071e-03 (5.5106e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [74][110/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 2.5305e-03 (5.4786e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [74][120/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.3937e-03 (5.6773e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [74][130/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5246e-03 (5.6343e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [74][140/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.9213e-03 (5.5639e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [74][150/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.5624e-03 (5.5314e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [74][160/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8835e-03 (5.5186e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [74][170/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.1801e-03 (5.4145e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [74][180/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3008e-03 (5.4969e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [74][190/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.4700e-03 (5.4715e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [74][200/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.9262e-03 (5.5446e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [74][210/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0997e-03 (5.5781e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [74][220/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0628e-03 (5.5612e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [74][230/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8281e-03 (5.5198e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [74][240/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.2254e-03 (5.4452e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [74][250/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6238e-02 (5.6041e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [74][260/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2069e-03 (5.5885e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [74][270/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.9482e-03 (5.5384e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [74][280/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.6829e-02 (5.6958e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [74][290/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.7728e-03 (5.6404e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [74][300/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.3748e-02 (5.8079e-03)	Acc@1  98.44 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [74][310/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.8524e-03 (5.8016e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [74][320/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.5138e-03 (5.7679e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [74][330/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.5085e-03 (5.7303e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [74][340/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.2738e-03 (5.7287e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [74][350/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.4540e-03 (5.7328e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [74][360/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.2336e-03 (5.8060e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [74][370/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.9291e-03 (5.8164e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [74][380/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.5175e-03 (5.7967e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [74][390/391]	Time  0.177 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.3415e-03 (5.7923e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
## e[74] optimizer.zero_grad (sum) time: 0.48111915588378906
## e[74]       loss.backward (sum) time: 10.853569984436035
## e[74]      optimizer.step (sum) time: 47.58340501785278
## epoch[74] training(only) time: 94.38738679885864
# Switched to evaluate mode...
Test: [  0/100]	Time  0.214 ( 0.214)	Loss 2.3326e-01 (2.3326e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.077 ( 0.090)	Loss 1.7213e-01 (3.3060e-01)	Acc@1  96.00 ( 93.27)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 4.0618e-01 (3.4447e-01)	Acc@1  92.00 ( 93.05)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 3.0887e-01 (3.5235e-01)	Acc@1  92.00 ( 92.81)	Acc@5 100.00 ( 99.81)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 2.2912e-01 (3.4923e-01)	Acc@1  92.00 ( 92.76)	Acc@5 100.00 ( 99.80)
Test: [ 50/100]	Time  0.078 ( 0.080)	Loss 2.2561e-01 (3.4820e-01)	Acc@1  95.00 ( 92.71)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.077 ( 0.080)	Loss 5.7904e-01 (3.3714e-01)	Acc@1  91.00 ( 92.74)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 5.3953e-01 (3.3843e-01)	Acc@1  90.00 ( 92.66)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.078 ( 0.079)	Loss 2.5010e-01 (3.3249e-01)	Acc@1  94.00 ( 92.84)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.2976e-01 (3.3205e-01)	Acc@1  95.00 ( 92.80)	Acc@5 100.00 ( 99.77)
 * Acc@1 92.740 Acc@5 99.780
### epoch[74] execution time: 102.40155577659607
EPOCH 75
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [75][  0/391]	Time  0.401 ( 0.401)	Data  0.161 ( 0.161)	Loss 8.7484e-03 (8.7484e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [75][ 10/391]	Time  0.240 ( 0.255)	Data  0.001 ( 0.016)	Loss 2.7972e-03 (6.6446e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [75][ 20/391]	Time  0.242 ( 0.248)	Data  0.001 ( 0.009)	Loss 3.7990e-03 (7.4434e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [75][ 30/391]	Time  0.240 ( 0.246)	Data  0.001 ( 0.006)	Loss 2.7060e-03 (6.6696e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [75][ 40/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.005)	Loss 1.4529e-03 (5.8500e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [75][ 50/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 3.4302e-03 (5.9755e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [75][ 60/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.004)	Loss 2.6298e-03 (5.9845e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [75][ 70/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.0860e-03 (6.0539e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [75][ 80/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 9.6670e-03 (5.8159e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [75][ 90/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 7.9986e-03 (5.7288e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [75][100/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.0935e-03 (5.5396e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [75][110/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 6.2648e-03 (5.4119e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [75][120/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.5146e-02 (5.6728e-03)	Acc@1  98.44 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [75][130/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5953e-03 (5.5509e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [75][140/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3447e-03 (5.4774e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [75][150/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6258e-02 (5.6701e-03)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [75][160/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.5379e-03 (5.5974e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [75][170/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.8350e-04 (5.3915e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [75][180/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.7718e-04 (5.3079e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [75][190/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.3325e-03 (5.3101e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [75][200/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.3856e-02 (5.5320e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [75][210/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.3661e-02 (5.5410e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [75][220/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.2943e-03 (5.5583e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [75][230/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0514e-03 (5.4239e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [75][240/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.3725e-03 (5.4269e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [75][250/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.9528e-03 (5.4283e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [75][260/391]	Time  0.246 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.9244e-03 (5.3759e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [75][270/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.2084e-03 (5.3398e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [75][280/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.0616e-03 (5.4924e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [75][290/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.0559e-03 (5.4347e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [75][300/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.0956e-03 (5.4302e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [75][310/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2076e-03 (5.3949e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [75][320/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.8309e-03 (5.3471e-03)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [75][330/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.9292e-03 (5.2869e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [75][340/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.2667e-03 (5.2677e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [75][350/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.7509e-03 (5.2470e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [75][360/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.8729e-03 (5.2128e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [75][370/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1106e-02 (5.2092e-03)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [75][380/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.9219e-03 (5.1596e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [75][390/391]	Time  0.177 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.4561e-03 (5.3392e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
## e[75] optimizer.zero_grad (sum) time: 0.4756944179534912
## e[75]       loss.backward (sum) time: 10.809794187545776
## e[75]      optimizer.step (sum) time: 47.577810525894165
## epoch[75] training(only) time: 94.27151703834534
# Switched to evaluate mode...
Test: [  0/100]	Time  0.209 ( 0.209)	Loss 2.5786e-01 (2.5786e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 2.0732e-01 (3.4467e-01)	Acc@1  96.00 ( 93.09)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 4.1986e-01 (3.5249e-01)	Acc@1  91.00 ( 93.05)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 3.1859e-01 (3.6356e-01)	Acc@1  92.00 ( 92.77)	Acc@5 100.00 ( 99.77)
Test: [ 40/100]	Time  0.077 ( 0.081)	Loss 2.3429e-01 (3.5738e-01)	Acc@1  94.00 ( 92.80)	Acc@5 100.00 ( 99.78)
Test: [ 50/100]	Time  0.078 ( 0.080)	Loss 2.2063e-01 (3.5456e-01)	Acc@1  95.00 ( 92.84)	Acc@5 100.00 ( 99.80)
Test: [ 60/100]	Time  0.077 ( 0.080)	Loss 6.1879e-01 (3.4323e-01)	Acc@1  91.00 ( 92.87)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.077 ( 0.080)	Loss 5.5402e-01 (3.4333e-01)	Acc@1  90.00 ( 92.83)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.077 ( 0.079)	Loss 3.0444e-01 (3.3792e-01)	Acc@1  93.00 ( 92.93)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.4806e-01 (3.3694e-01)	Acc@1  94.00 ( 92.87)	Acc@5 100.00 ( 99.78)
 * Acc@1 92.770 Acc@5 99.790
### epoch[75] execution time: 102.26383686065674
EPOCH 76
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [76][  0/391]	Time  0.398 ( 0.398)	Data  0.153 ( 0.153)	Loss 3.5869e-03 (3.5869e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [76][ 10/391]	Time  0.240 ( 0.255)	Data  0.001 ( 0.015)	Loss 7.6503e-03 (5.6442e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [76][ 20/391]	Time  0.240 ( 0.248)	Data  0.001 ( 0.008)	Loss 1.0811e-03 (4.8243e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [76][ 30/391]	Time  0.240 ( 0.246)	Data  0.001 ( 0.006)	Loss 1.0120e-02 (4.7802e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [76][ 40/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.005)	Loss 3.0159e-03 (5.4192e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [76][ 50/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.7774e-02 (5.8926e-03)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [76][ 60/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.004)	Loss 1.8573e-02 (5.9631e-03)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [76][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.2123e-02 (5.9183e-03)	Acc@1  98.44 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [76][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 4.9196e-03 (5.9389e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [76][ 90/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 6.0593e-03 (5.7418e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [76][100/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 5.8850e-03 (5.5011e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [76][110/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 7.6573e-03 (5.5842e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [76][120/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.0029e-03 (5.6024e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [76][130/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3055e-02 (5.7293e-03)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [76][140/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.8508e-03 (5.7028e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [76][150/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.4703e-03 (5.7972e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [76][160/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.9779e-03 (5.6093e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [76][170/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.4317e-03 (5.7456e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [76][180/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.4436e-03 (5.7980e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [76][190/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3237e-03 (5.9257e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [76][200/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.1812e-04 (6.1929e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [76][210/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.3569e-03 (6.1054e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [76][220/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.9829e-03 (6.0049e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [76][230/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.4270e-03 (6.0887e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [76][240/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.7844e-03 (6.0818e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [76][250/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.9574e-02 (6.1516e-03)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [76][260/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.3453e-03 (6.0883e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [76][270/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.8541e-03 (6.0207e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [76][280/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.3182e-03 (6.0117e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [76][290/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0421e-03 (5.9803e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [76][300/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.3691e-03 (5.9574e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [76][310/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.9124e-03 (5.9332e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [76][320/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.7221e-04 (5.9036e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [76][330/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.7833e-03 (5.8616e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [76][340/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.7317e-03 (5.8260e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [76][350/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.4706e-02 (5.8665e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [76][360/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.9236e-02 (5.8411e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [76][370/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.3750e-03 (5.7747e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [76][380/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.6575e-03 (5.6976e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [76][390/391]	Time  0.178 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.5157e-03 (5.7504e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
## e[76] optimizer.zero_grad (sum) time: 0.4785139560699463
## e[76]       loss.backward (sum) time: 10.849282503128052
## e[76]      optimizer.step (sum) time: 47.55404829978943
## epoch[76] training(only) time: 94.3466866016388
# Switched to evaluate mode...
Test: [  0/100]	Time  0.212 ( 0.212)	Loss 2.7124e-01 (2.7124e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 2.0662e-01 (3.4835e-01)	Acc@1  96.00 ( 93.00)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 4.3495e-01 (3.5754e-01)	Acc@1  91.00 ( 93.10)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 3.2909e-01 (3.6468e-01)	Acc@1  92.00 ( 92.77)	Acc@5 100.00 ( 99.77)
Test: [ 40/100]	Time  0.079 ( 0.081)	Loss 2.1482e-01 (3.6000e-01)	Acc@1  95.00 ( 92.88)	Acc@5 100.00 ( 99.78)
Test: [ 50/100]	Time  0.077 ( 0.081)	Loss 2.4816e-01 (3.5526e-01)	Acc@1  95.00 ( 92.84)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 6.0437e-01 (3.4411e-01)	Acc@1  92.00 ( 92.84)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.077 ( 0.080)	Loss 5.3424e-01 (3.4480e-01)	Acc@1  91.00 ( 92.77)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 2.6142e-01 (3.3856e-01)	Acc@1  95.00 ( 92.89)	Acc@5 100.00 ( 99.73)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.3777e-01 (3.3735e-01)	Acc@1  95.00 ( 92.86)	Acc@5 100.00 ( 99.76)
 * Acc@1 92.800 Acc@5 99.770
### epoch[76] execution time: 102.36001753807068
EPOCH 77
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [77][  0/391]	Time  0.407 ( 0.407)	Data  0.162 ( 0.162)	Loss 1.2458e-02 (1.2458e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [77][ 10/391]	Time  0.242 ( 0.256)	Data  0.001 ( 0.016)	Loss 8.5134e-03 (9.6788e-03)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [77][ 20/391]	Time  0.240 ( 0.249)	Data  0.001 ( 0.009)	Loss 3.8595e-03 (7.1355e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [77][ 30/391]	Time  0.251 ( 0.246)	Data  0.001 ( 0.006)	Loss 3.4319e-03 (6.2806e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [77][ 40/391]	Time  0.239 ( 0.245)	Data  0.001 ( 0.005)	Loss 2.1667e-03 (6.3516e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [77][ 50/391]	Time  0.243 ( 0.244)	Data  0.001 ( 0.004)	Loss 6.7794e-03 (6.5198e-03)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [77][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 4.0941e-03 (6.0923e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [77][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 6.9416e-03 (5.7801e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [77][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.5012e-03 (5.5494e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [77][ 90/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.8737e-03 (5.2736e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [77][100/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.0581e-03 (5.2102e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [77][110/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.6431e-03 (5.1971e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [77][120/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0242e-03 (5.0597e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [77][130/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0579e-02 (5.2104e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [77][140/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.5350e-03 (5.1992e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [77][150/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1260e-03 (5.1027e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [77][160/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.5640e-03 (5.0464e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [77][170/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6663e-03 (5.1547e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [77][180/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.0791e-03 (5.0718e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [77][190/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.8304e-03 (5.1707e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [77][200/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.1654e-03 (5.1343e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [77][210/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0781e-02 (5.0156e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [77][220/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3024e-03 (4.9530e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [77][230/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.7408e-03 (4.9192e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [77][240/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.3938e-03 (4.9732e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [77][250/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.4628e-03 (5.0084e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [77][260/391]	Time  0.246 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.6072e-03 (5.0169e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [77][270/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1073e-03 (5.2119e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [77][280/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.7248e-03 (5.1720e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [77][290/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.6529e-03 (5.1524e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [77][300/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.2202e-03 (5.1267e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [77][310/391]	Time  0.244 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.9312e-03 (5.0763e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [77][320/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.8708e-03 (5.0808e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [77][330/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.6228e-04 (5.0361e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [77][340/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.1661e-03 (5.0499e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [77][350/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.0125e-03 (4.9779e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [77][360/391]	Time  0.247 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.9403e-03 (5.0163e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [77][370/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.5095e-03 (5.0361e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [77][380/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.3972e-04 (5.0535e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [77][390/391]	Time  0.177 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2692e-02 (5.0652e-03)	Acc@1  98.75 ( 99.88)	Acc@5 100.00 (100.00)
## e[77] optimizer.zero_grad (sum) time: 0.47627949714660645
## e[77]       loss.backward (sum) time: 10.865254163742065
## e[77]      optimizer.step (sum) time: 47.498480558395386
## epoch[77] training(only) time: 94.27763533592224
# Switched to evaluate mode...
Test: [  0/100]	Time  0.212 ( 0.212)	Loss 2.3805e-01 (2.3805e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.8875e-01 (3.3215e-01)	Acc@1  96.00 ( 93.18)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.077 ( 0.084)	Loss 3.9902e-01 (3.4657e-01)	Acc@1  92.00 ( 93.05)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 3.0867e-01 (3.5695e-01)	Acc@1  92.00 ( 92.81)	Acc@5 100.00 ( 99.77)
Test: [ 40/100]	Time  0.077 ( 0.081)	Loss 2.2765e-01 (3.5427e-01)	Acc@1  93.00 ( 92.83)	Acc@5 100.00 ( 99.80)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 2.2902e-01 (3.5114e-01)	Acc@1  95.00 ( 92.82)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 5.9681e-01 (3.4012e-01)	Acc@1  92.00 ( 92.82)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.077 ( 0.080)	Loss 5.2244e-01 (3.4006e-01)	Acc@1  91.00 ( 92.89)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.079 ( 0.080)	Loss 2.7249e-01 (3.3426e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.4515e-01 (3.3394e-01)	Acc@1  95.00 ( 92.97)	Acc@5 100.00 ( 99.78)
 * Acc@1 92.900 Acc@5 99.790
### epoch[77] execution time: 102.29012966156006
EPOCH 78
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [78][  0/391]	Time  0.386 ( 0.386)	Data  0.145 ( 0.145)	Loss 1.2580e-02 (1.2580e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [78][ 10/391]	Time  0.240 ( 0.254)	Data  0.001 ( 0.014)	Loss 7.5698e-03 (4.2978e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [78][ 20/391]	Time  0.239 ( 0.247)	Data  0.001 ( 0.008)	Loss 2.0667e-03 (3.6723e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [78][ 30/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.006)	Loss 4.0624e-03 (4.4689e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [78][ 40/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.005)	Loss 2.3528e-02 (5.8151e-03)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [78][ 50/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 2.7785e-03 (6.1141e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [78][ 60/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.004)	Loss 6.1064e-03 (5.8260e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [78][ 70/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.7249e-03 (5.5780e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [78][ 80/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 7.6355e-03 (5.4433e-03)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [78][ 90/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 2.0124e-03 (5.5642e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [78][100/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.7704e-03 (5.2641e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [78][110/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.8398e-03 (5.0788e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][120/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.7230e-03 (5.1329e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][130/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.5937e-03 (5.1043e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][140/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1790e-03 (5.1664e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][150/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.0518e-03 (5.1368e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][160/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.2165e-03 (5.0779e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][170/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.3960e-02 (5.3501e-03)	Acc@1  98.44 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][180/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.5421e-02 (5.5937e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [78][190/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.8206e-03 (5.5462e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][200/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.6207e-03 (5.5035e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][210/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.4537e-03 (5.8309e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [78][220/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1469e-03 (5.8362e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [78][230/391]	Time  0.246 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.3888e-03 (5.9393e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [78][240/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.5099e-03 (5.8777e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [78][250/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.6419e-03 (5.8750e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][260/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.5221e-03 (5.9155e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [78][270/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.5518e-02 (5.9385e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [78][280/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.5417e-02 (5.9869e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [78][290/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.2192e-03 (5.9234e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [78][300/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.3044e-04 (5.8991e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [78][310/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.2554e-03 (5.8930e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [78][320/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.4847e-03 (5.8382e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][330/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.5863e-02 (5.8479e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][340/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.7886e-03 (5.8498e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][350/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.3026e-02 (5.8523e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [78][360/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.5815e-02 (5.7883e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][370/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0017e-02 (5.7906e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][380/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.8759e-04 (5.8196e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][390/391]	Time  0.185 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.7730e-03 (5.8135e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
## e[78] optimizer.zero_grad (sum) time: 0.4750185012817383
## e[78]       loss.backward (sum) time: 10.857080936431885
## e[78]      optimizer.step (sum) time: 47.514647245407104
## epoch[78] training(only) time: 94.3092029094696
# Switched to evaluate mode...
Test: [  0/100]	Time  0.206 ( 0.206)	Loss 2.4519e-01 (2.4519e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.077 ( 0.089)	Loss 1.8602e-01 (3.2590e-01)	Acc@1  96.00 ( 93.64)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 4.2656e-01 (3.4280e-01)	Acc@1  90.00 ( 93.10)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.077 ( 0.082)	Loss 3.1359e-01 (3.5354e-01)	Acc@1  92.00 ( 93.00)	Acc@5 100.00 ( 99.77)
Test: [ 40/100]	Time  0.077 ( 0.081)	Loss 2.4537e-01 (3.5100e-01)	Acc@1  93.00 ( 92.93)	Acc@5 100.00 ( 99.78)
Test: [ 50/100]	Time  0.078 ( 0.080)	Loss 2.3119e-01 (3.4946e-01)	Acc@1  94.00 ( 92.94)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 6.1555e-01 (3.3914e-01)	Acc@1  92.00 ( 92.97)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.079 ( 0.080)	Loss 5.5704e-01 (3.4103e-01)	Acc@1  90.00 ( 92.90)	Acc@5  99.00 ( 99.75)
Test: [ 80/100]	Time  0.077 ( 0.079)	Loss 2.8702e-01 (3.3577e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 ( 99.72)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.4535e-01 (3.3562e-01)	Acc@1  94.00 ( 92.92)	Acc@5 100.00 ( 99.75)
 * Acc@1 92.840 Acc@5 99.760
### epoch[78] execution time: 102.30918836593628
EPOCH 79
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [79][  0/391]	Time  0.383 ( 0.383)	Data  0.138 ( 0.138)	Loss 4.6329e-03 (4.6329e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [79][ 10/391]	Time  0.240 ( 0.253)	Data  0.001 ( 0.014)	Loss 1.1998e-02 (6.7357e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [79][ 20/391]	Time  0.241 ( 0.247)	Data  0.001 ( 0.008)	Loss 2.9554e-02 (7.1343e-03)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [79][ 30/391]	Time  0.239 ( 0.245)	Data  0.001 ( 0.006)	Loss 1.8109e-03 (7.2886e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [79][ 40/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.005)	Loss 2.6624e-03 (6.6426e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [79][ 50/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.004)	Loss 8.7337e-03 (6.6140e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [79][ 60/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 4.4334e-03 (6.4777e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [79][ 70/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 6.5773e-03 (6.5289e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [79][ 80/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 3.3286e-03 (6.4567e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [79][ 90/391]	Time  0.248 ( 0.242)	Data  0.001 ( 0.003)	Loss 5.1735e-03 (6.4039e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [79][100/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.4302e-02 (6.4483e-03)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [79][110/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6712e-03 (6.3327e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [79][120/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.4710e-03 (6.0768e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [79][130/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.9280e-03 (5.9517e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [79][140/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.2660e-02 (5.9505e-03)	Acc@1  98.44 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [79][150/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.6927e-03 (5.8397e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [79][160/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.5466e-03 (5.7576e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [79][170/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.7709e-03 (5.7776e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [79][180/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2931e-02 (5.7605e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [79][190/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.6862e-03 (5.6050e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [79][200/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.8084e-03 (5.6261e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [79][210/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.2200e-03 (5.6337e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [79][220/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.0183e-03 (5.5522e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [79][230/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.7622e-04 (5.4545e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [79][240/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.0016e-03 (5.4542e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [79][250/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.6268e-04 (5.5843e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [79][260/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.3792e-02 (5.6052e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [79][270/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.4805e-03 (5.5808e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [79][280/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.3015e-03 (5.5932e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [79][290/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.9983e-03 (5.4863e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [79][300/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.3398e-03 (5.4835e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [79][310/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.6441e-03 (5.4737e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [79][320/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.3148e-02 (5.5872e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [79][330/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.3870e-03 (5.5429e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [79][340/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.8166e-03 (5.5312e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [79][350/391]	Time  0.246 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.3091e-03 (5.4724e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [79][360/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.8767e-03 (5.4205e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [79][370/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.4353e-03 (5.3884e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [79][380/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.5018e-03 (5.5539e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [79][390/391]	Time  0.176 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.7843e-03 (5.6325e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
## e[79] optimizer.zero_grad (sum) time: 0.4755878448486328
## e[79]       loss.backward (sum) time: 10.841379642486572
## e[79]      optimizer.step (sum) time: 47.53636455535889
## epoch[79] training(only) time: 94.25104880332947
# Switched to evaluate mode...
Test: [  0/100]	Time  0.204 ( 0.204)	Loss 2.3716e-01 (2.3716e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.077 ( 0.089)	Loss 1.6597e-01 (3.3089e-01)	Acc@1  96.00 ( 93.55)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.079 ( 0.084)	Loss 4.2559e-01 (3.4539e-01)	Acc@1  91.00 ( 92.95)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.077 ( 0.082)	Loss 2.9318e-01 (3.5183e-01)	Acc@1  92.00 ( 92.77)	Acc@5 100.00 ( 99.81)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 2.2870e-01 (3.5039e-01)	Acc@1  92.00 ( 92.73)	Acc@5 100.00 ( 99.80)
Test: [ 50/100]	Time  0.077 ( 0.080)	Loss 2.1018e-01 (3.5019e-01)	Acc@1  95.00 ( 92.73)	Acc@5 100.00 ( 99.80)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 5.9111e-01 (3.4004e-01)	Acc@1  91.00 ( 92.77)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 5.3429e-01 (3.4188e-01)	Acc@1  90.00 ( 92.75)	Acc@5  99.00 ( 99.76)
Test: [ 80/100]	Time  0.077 ( 0.079)	Loss 2.4199e-01 (3.3619e-01)	Acc@1  94.00 ( 92.93)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.077 ( 0.079)	Loss 1.4268e-01 (3.3658e-01)	Acc@1  95.00 ( 92.87)	Acc@5 100.00 ( 99.77)
 * Acc@1 92.800 Acc@5 99.780
### epoch[79] execution time: 102.24887013435364
EPOCH 80
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [80][  0/391]	Time  0.388 ( 0.388)	Data  0.147 ( 0.147)	Loss 3.4203e-03 (3.4203e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [80][ 10/391]	Time  0.240 ( 0.254)	Data  0.001 ( 0.014)	Loss 2.4006e-03 (5.9374e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [80][ 20/391]	Time  0.239 ( 0.247)	Data  0.001 ( 0.008)	Loss 2.6263e-02 (7.8125e-03)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [80][ 30/391]	Time  0.243 ( 0.245)	Data  0.001 ( 0.006)	Loss 3.7004e-02 (7.8334e-03)	Acc@1  98.44 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [80][ 40/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.005)	Loss 6.8353e-03 (6.9392e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [80][ 50/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.3516e-03 (6.3533e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [80][ 60/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.004)	Loss 5.1888e-03 (5.9194e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [80][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.8088e-03 (5.7186e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [80][ 80/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.003)	Loss 6.8684e-03 (5.5646e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [80][ 90/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 2.5177e-03 (5.2147e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [80][100/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.3290e-02 (5.2109e-03)	Acc@1  99.22 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [80][110/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.2485e-03 (5.4146e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [80][120/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.5341e-03 (5.6917e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [80][130/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.1369e-03 (5.6365e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [80][140/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.2205e-03 (5.6156e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [80][150/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.6428e-04 (5.6865e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [80][160/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.9554e-03 (5.6711e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [80][170/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.3027e-02 (5.6545e-03)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [80][180/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.5845e-03 (5.7099e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [80][190/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.9796e-03 (6.0168e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [80][200/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.6187e-03 (5.9093e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [80][210/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.0705e-02 (5.9659e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [80][220/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.9599e-03 (5.8321e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [80][230/391]	Time  0.244 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.9860e-03 (5.7106e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [80][240/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.7226e-03 (5.6222e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [80][250/391]	Time  0.248 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.6875e-03 (5.6038e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [80][260/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.3617e-03 (5.5864e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [80][270/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.5012e-03 (5.5419e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [80][280/391]	Time  0.247 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.7847e-03 (5.5359e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [80][290/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.6872e-03 (5.5674e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [80][300/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.2721e-03 (5.5323e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [80][310/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.7609e-03 (5.5453e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [80][320/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.3703e-03 (5.5053e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [80][330/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.4419e-03 (5.5548e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [80][340/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.4394e-04 (5.5350e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [80][350/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.5752e-04 (5.4392e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [80][360/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.3395e-02 (5.4699e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [80][370/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.6611e-03 (5.4389e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [80][380/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.4323e-03 (5.4997e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [80][390/391]	Time  0.176 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.5882e-03 (5.5220e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
## e[80] optimizer.zero_grad (sum) time: 0.4795660972595215
## e[80]       loss.backward (sum) time: 10.852586030960083
## e[80]      optimizer.step (sum) time: 47.501646757125854
## epoch[80] training(only) time: 94.26808524131775
# Switched to evaluate mode...
Test: [  0/100]	Time  0.211 ( 0.211)	Loss 2.3779e-01 (2.3779e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.077 ( 0.090)	Loss 1.7581e-01 (3.3793e-01)	Acc@1  96.00 ( 93.00)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.079 ( 0.084)	Loss 3.7583e-01 (3.4583e-01)	Acc@1  92.00 ( 93.05)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.077 ( 0.082)	Loss 2.9844e-01 (3.5569e-01)	Acc@1  93.00 ( 92.84)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 2.2878e-01 (3.5217e-01)	Acc@1  92.00 ( 92.83)	Acc@5 100.00 ( 99.76)
Test: [ 50/100]	Time  0.077 ( 0.080)	Loss 2.2105e-01 (3.4992e-01)	Acc@1  94.00 ( 92.75)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.079 ( 0.080)	Loss 5.8353e-01 (3.3850e-01)	Acc@1  92.00 ( 92.82)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.077 ( 0.080)	Loss 5.5075e-01 (3.4017e-01)	Acc@1  90.00 ( 92.73)	Acc@5  99.00 ( 99.73)
Test: [ 80/100]	Time  0.078 ( 0.079)	Loss 2.6823e-01 (3.3491e-01)	Acc@1  93.00 ( 92.83)	Acc@5 100.00 ( 99.72)
Test: [ 90/100]	Time  0.077 ( 0.079)	Loss 1.3160e-01 (3.3415e-01)	Acc@1  95.00 ( 92.80)	Acc@5 100.00 ( 99.75)
 * Acc@1 92.730 Acc@5 99.760
### epoch[80] execution time: 102.25431203842163
EPOCH 81
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [81][  0/391]	Time  0.391 ( 0.391)	Data  0.151 ( 0.151)	Loss 2.3307e-03 (2.3307e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [81][ 10/391]	Time  0.240 ( 0.254)	Data  0.001 ( 0.015)	Loss 5.3507e-03 (5.5495e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [81][ 20/391]	Time  0.241 ( 0.247)	Data  0.001 ( 0.008)	Loss 4.6865e-03 (7.5640e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [81][ 30/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.006)	Loss 3.2131e-03 (6.9355e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [81][ 40/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.005)	Loss 8.8071e-04 (6.2515e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [81][ 50/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.004)	Loss 6.5451e-03 (5.6982e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [81][ 60/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.004)	Loss 2.3251e-03 (5.3873e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [81][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 8.5394e-03 (5.2945e-03)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [81][ 80/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 2.4491e-02 (5.5477e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [81][ 90/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 2.2441e-03 (5.2674e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [81][100/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.4396e-03 (5.6354e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [81][110/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.4293e-03 (5.3790e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [81][120/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.3944e-03 (5.4654e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [81][130/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9953e-02 (5.5819e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [81][140/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.8014e-03 (5.3643e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [81][150/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.1463e-03 (5.2752e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [81][160/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.7913e-03 (5.2423e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [81][170/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1338e-02 (5.2958e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [81][180/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.7006e-03 (5.2489e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [81][190/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.6754e-03 (5.3057e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [81][200/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.4578e-03 (5.4426e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [81][210/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.2919e-02 (5.5404e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [81][220/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.8986e-03 (5.4201e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [81][230/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.0861e-03 (5.5002e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [81][240/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.5172e-03 (5.4632e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [81][250/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.3833e-03 (5.4043e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [81][260/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.5181e-03 (5.4238e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [81][270/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.9759e-03 (5.5400e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [81][280/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.9914e-03 (5.5513e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [81][290/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.4066e-03 (5.4511e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [81][300/391]	Time  0.248 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.3889e-03 (5.4299e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [81][310/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.1847e-03 (5.3940e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [81][320/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.4714e-03 (5.3214e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [81][330/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.3502e-03 (5.3331e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [81][340/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.9425e-04 (5.3592e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [81][350/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.6742e-03 (5.3607e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [81][360/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.4020e-03 (5.3160e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [81][370/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.3824e-03 (5.2484e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [81][380/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.1422e-03 (5.1788e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [81][390/391]	Time  0.177 ( 0.241)	Data  0.001 ( 0.001)	Loss 2.0066e-03 (5.1820e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
## e[81] optimizer.zero_grad (sum) time: 0.4772045612335205
## e[81]       loss.backward (sum) time: 10.84761905670166
## e[81]      optimizer.step (sum) time: 47.50874423980713
## epoch[81] training(only) time: 94.24305820465088
# Switched to evaluate mode...
Test: [  0/100]	Time  0.210 ( 0.210)	Loss 2.3118e-01 (2.3118e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.8455e-01 (3.3740e-01)	Acc@1  96.00 ( 93.45)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.077 ( 0.084)	Loss 4.3844e-01 (3.5070e-01)	Acc@1  91.00 ( 93.10)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 3.0170e-01 (3.5837e-01)	Acc@1  92.00 ( 92.84)	Acc@5 100.00 ( 99.77)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 2.2168e-01 (3.5802e-01)	Acc@1  94.00 ( 92.80)	Acc@5 100.00 ( 99.78)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 2.1832e-01 (3.5478e-01)	Acc@1  95.00 ( 92.86)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 5.8223e-01 (3.4398e-01)	Acc@1  92.00 ( 92.97)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 5.0780e-01 (3.4482e-01)	Acc@1  90.00 ( 92.90)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.079 ( 0.080)	Loss 2.4112e-01 (3.3860e-01)	Acc@1  94.00 ( 93.06)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.077 ( 0.079)	Loss 1.3816e-01 (3.3903e-01)	Acc@1  96.00 ( 92.99)	Acc@5 100.00 ( 99.78)
 * Acc@1 92.900 Acc@5 99.790
### epoch[81] execution time: 102.26190638542175
EPOCH 82
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [82][  0/391]	Time  0.394 ( 0.394)	Data  0.150 ( 0.150)	Loss 2.4948e-02 (2.4948e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [82][ 10/391]	Time  0.240 ( 0.255)	Data  0.001 ( 0.015)	Loss 1.4594e-02 (7.0880e-03)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [82][ 20/391]	Time  0.246 ( 0.248)	Data  0.001 ( 0.008)	Loss 2.2918e-03 (7.2215e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [82][ 30/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.006)	Loss 1.0218e-02 (6.9837e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [82][ 40/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.005)	Loss 1.6345e-02 (6.9751e-03)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [82][ 50/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 4.4908e-03 (6.5724e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [82][ 60/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.004)	Loss 1.3832e-03 (6.1508e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [82][ 70/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.003)	Loss 5.7772e-03 (6.1518e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [82][ 80/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 4.0462e-03 (6.0630e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [82][ 90/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.4899e-03 (5.7035e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [82][100/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.5005e-02 (5.7243e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [82][110/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 6.5932e-03 (5.5460e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [82][120/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.5299e-04 (5.4635e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [82][130/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9468e-03 (5.4588e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [82][140/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2539e-02 (5.3536e-03)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [82][150/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.7002e-03 (5.4256e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [82][160/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.9305e-03 (5.4451e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [82][170/391]	Time  0.243 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.4103e-02 (5.3906e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [82][180/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.6246e-03 (5.4934e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [82][190/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.4506e-04 (5.3572e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [82][200/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.7346e-03 (5.2167e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [82][210/391]	Time  0.244 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.9231e-03 (5.2954e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [82][220/391]	Time  0.243 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.0287e-03 (5.3302e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [82][230/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.2730e-03 (5.4398e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [82][240/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.9637e-03 (5.4448e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [82][250/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.4889e-03 (5.4758e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [82][260/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.7692e-03 (5.3845e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [82][270/391]	Time  0.246 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.6746e-03 (5.3954e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [82][280/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.2027e-04 (5.3285e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [82][290/391]	Time  0.248 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.2699e-03 (5.3586e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [82][300/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.2894e-03 (5.3271e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [82][310/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.8941e-03 (5.3139e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [82][320/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.5660e-04 (5.3380e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [82][330/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.3365e-03 (5.2991e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [82][340/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.8759e-03 (5.2420e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [82][350/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.8541e-03 (5.1582e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [82][360/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.1999e-03 (5.1913e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [82][370/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.9883e-03 (5.1735e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [82][380/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.1262e-03 (5.1810e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [82][390/391]	Time  0.176 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0101e-03 (5.1670e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
## e[82] optimizer.zero_grad (sum) time: 0.4771115779876709
## e[82]       loss.backward (sum) time: 10.780345678329468
## e[82]      optimizer.step (sum) time: 47.549906969070435
## epoch[82] training(only) time: 94.26687216758728
# Switched to evaluate mode...
Test: [  0/100]	Time  0.214 ( 0.214)	Loss 2.2543e-01 (2.2543e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.9189e-01 (3.3529e-01)	Acc@1  96.00 ( 93.55)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.077 ( 0.084)	Loss 4.0873e-01 (3.4593e-01)	Acc@1  91.00 ( 93.29)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 2.9807e-01 (3.5591e-01)	Acc@1  92.00 ( 93.06)	Acc@5 100.00 ( 99.77)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 2.2184e-01 (3.5212e-01)	Acc@1  93.00 ( 93.05)	Acc@5 100.00 ( 99.78)
Test: [ 50/100]	Time  0.077 ( 0.081)	Loss 2.0724e-01 (3.4924e-01)	Acc@1  95.00 ( 93.08)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 5.7683e-01 (3.3806e-01)	Acc@1  93.00 ( 93.08)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.077 ( 0.080)	Loss 5.5561e-01 (3.3838e-01)	Acc@1  91.00 ( 93.00)	Acc@5  99.00 ( 99.76)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 2.7052e-01 (3.3322e-01)	Acc@1  93.00 ( 93.14)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.4288e-01 (3.3386e-01)	Acc@1  96.00 ( 93.11)	Acc@5 100.00 ( 99.77)
 * Acc@1 93.020 Acc@5 99.780
### epoch[82] execution time: 102.28267168998718
EPOCH 83
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [83][  0/391]	Time  0.390 ( 0.390)	Data  0.147 ( 0.147)	Loss 1.1003e-03 (1.1003e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [83][ 10/391]	Time  0.241 ( 0.254)	Data  0.001 ( 0.014)	Loss 4.9757e-04 (4.0730e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [83][ 20/391]	Time  0.241 ( 0.248)	Data  0.001 ( 0.008)	Loss 1.0985e-03 (3.3744e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [83][ 30/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.006)	Loss 6.3782e-03 (3.9228e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [83][ 40/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.005)	Loss 2.7412e-03 (3.8861e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [83][ 50/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.004)	Loss 9.0519e-03 (4.2620e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [83][ 60/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.004)	Loss 9.4812e-03 (4.2203e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [83][ 70/391]	Time  0.239 ( 0.243)	Data  0.001 ( 0.003)	Loss 9.6720e-03 (4.4336e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [83][ 80/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 4.2585e-03 (4.6861e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [83][ 90/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 6.6349e-03 (4.8066e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [83][100/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.1047e-03 (4.6696e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [83][110/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.8617e-04 (4.7119e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [83][120/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5541e-03 (4.8072e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [83][130/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2975e-03 (4.5734e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [83][140/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.1201e-03 (4.5458e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [83][150/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.3421e-03 (4.8661e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [83][160/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.3479e-03 (4.7608e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [83][170/391]	Time  0.240 ( 0.241)	Data  0.002 ( 0.002)	Loss 3.9636e-03 (4.9520e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [83][180/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0134e-02 (5.0104e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [83][190/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.9663e-03 (4.9703e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [83][200/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.0075e-04 (4.8736e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [83][210/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.9444e-03 (4.9278e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [83][220/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.4008e-03 (4.8735e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [83][230/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.4423e-03 (4.8775e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [83][240/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.5962e-03 (4.8515e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [83][250/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.3078e-03 (4.8317e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [83][260/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.2938e-03 (4.8299e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [83][270/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.7343e-04 (4.7827e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [83][280/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.3200e-03 (4.7333e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [83][290/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.1098e-03 (4.6780e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [83][300/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.5738e-03 (4.6974e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [83][310/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.7492e-03 (4.6617e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [83][320/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.5559e-03 (4.6439e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [83][330/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2818e-03 (4.7257e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [83][340/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0637e-03 (4.8011e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [83][350/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2099e-03 (4.8380e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [83][360/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0294e-03 (4.9239e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [83][370/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.4623e-03 (4.9678e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [83][380/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.7356e-03 (4.9643e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [83][390/391]	Time  0.177 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.9798e-02 (5.0092e-03)	Acc@1  98.75 ( 99.90)	Acc@5 100.00 (100.00)
## e[83] optimizer.zero_grad (sum) time: 0.47815370559692383
## e[83]       loss.backward (sum) time: 10.78661036491394
## e[83]      optimizer.step (sum) time: 47.58501482009888
## epoch[83] training(only) time: 94.32881951332092
# Switched to evaluate mode...
Test: [  0/100]	Time  0.211 ( 0.211)	Loss 2.4910e-01 (2.4910e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.077 ( 0.090)	Loss 1.9081e-01 (3.4866e-01)	Acc@1  96.00 ( 93.09)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 3.9217e-01 (3.5170e-01)	Acc@1  91.00 ( 93.05)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 2.8443e-01 (3.6031e-01)	Acc@1  92.00 ( 92.77)	Acc@5 100.00 ( 99.77)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 2.3181e-01 (3.5716e-01)	Acc@1  94.00 ( 92.83)	Acc@5 100.00 ( 99.78)
Test: [ 50/100]	Time  0.079 ( 0.081)	Loss 2.0955e-01 (3.5337e-01)	Acc@1  94.00 ( 92.80)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 5.7349e-01 (3.4135e-01)	Acc@1  91.00 ( 92.87)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 5.4908e-01 (3.4159e-01)	Acc@1  90.00 ( 92.86)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.079 ( 0.080)	Loss 2.6184e-01 (3.3609e-01)	Acc@1  93.00 ( 92.96)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.5030e-01 (3.3668e-01)	Acc@1  95.00 ( 92.91)	Acc@5 100.00 ( 99.77)
 * Acc@1 92.830 Acc@5 99.780
### epoch[83] execution time: 102.33654308319092
EPOCH 84
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [84][  0/391]	Time  0.390 ( 0.390)	Data  0.149 ( 0.149)	Loss 3.6203e-03 (3.6203e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [84][ 10/391]	Time  0.240 ( 0.254)	Data  0.002 ( 0.015)	Loss 4.3451e-03 (5.2759e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [84][ 20/391]	Time  0.240 ( 0.248)	Data  0.001 ( 0.008)	Loss 5.9372e-03 (5.7416e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [84][ 30/391]	Time  0.240 ( 0.246)	Data  0.001 ( 0.006)	Loss 5.5345e-03 (5.0281e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [84][ 40/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.005)	Loss 3.5866e-02 (5.9021e-03)	Acc@1  98.44 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [84][ 50/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.0604e-02 (5.6551e-03)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [84][ 60/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.004)	Loss 2.6904e-03 (5.2816e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [84][ 70/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.0644e-02 (5.3128e-03)	Acc@1  99.22 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [84][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.0477e-03 (5.0458e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [84][ 90/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.1058e-02 (5.4900e-03)	Acc@1  99.22 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [84][100/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.6310e-03 (5.5128e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [84][110/391]	Time  0.239 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.6916e-03 (5.4869e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [84][120/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.5738e-03 (5.3950e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [84][130/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2226e-02 (5.6333e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [84][140/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.1567e-04 (5.4496e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [84][150/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6006e-02 (5.6971e-03)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [84][160/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.9192e-03 (5.7225e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [84][170/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.4327e-02 (5.8336e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [84][180/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3942e-03 (5.8466e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [84][190/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.6047e-03 (5.7892e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [84][200/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.4943e-04 (5.7343e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [84][210/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3188e-03 (5.6498e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [84][220/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.7423e-03 (5.5437e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [84][230/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.7340e-03 (5.4911e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [84][240/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3424e-03 (5.3851e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [84][250/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6438e-03 (5.5158e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [84][260/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.8073e-03 (5.5514e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [84][270/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1348e-03 (5.5179e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [84][280/391]	Time  0.248 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2143e-03 (5.5095e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [84][290/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0898e-02 (5.4815e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [84][300/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1874e-02 (5.5343e-03)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [84][310/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.8529e-03 (5.4730e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [84][320/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.5098e-04 (5.4106e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [84][330/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2734e-03 (5.3808e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [84][340/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.9940e-03 (5.3579e-03)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [84][350/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.3304e-03 (5.3645e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [84][360/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.1674e-02 (5.4208e-03)	Acc@1  98.44 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [84][370/391]	Time  0.248 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.6456e-03 (5.3905e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [84][380/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.0125e-03 (5.4501e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [84][390/391]	Time  0.177 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1820e-02 (5.4291e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
## e[84] optimizer.zero_grad (sum) time: 0.4771239757537842
## e[84]       loss.backward (sum) time: 10.829919338226318
## e[84]      optimizer.step (sum) time: 47.63493728637695
## epoch[84] training(only) time: 94.42084789276123
# Switched to evaluate mode...
Test: [  0/100]	Time  0.216 ( 0.216)	Loss 2.4043e-01 (2.4043e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.078 ( 0.091)	Loss 1.8131e-01 (3.4650e-01)	Acc@1  96.00 ( 93.55)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.078 ( 0.085)	Loss 4.2318e-01 (3.5606e-01)	Acc@1  91.00 ( 93.10)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.079 ( 0.083)	Loss 2.9634e-01 (3.5827e-01)	Acc@1  92.00 ( 92.81)	Acc@5 100.00 ( 99.77)
Test: [ 40/100]	Time  0.077 ( 0.081)	Loss 2.1847e-01 (3.5805e-01)	Acc@1  94.00 ( 92.88)	Acc@5 100.00 ( 99.78)
Test: [ 50/100]	Time  0.079 ( 0.081)	Loss 2.2533e-01 (3.5743e-01)	Acc@1  95.00 ( 92.94)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 5.7960e-01 (3.4544e-01)	Acc@1  93.00 ( 93.02)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 5.2111e-01 (3.4696e-01)	Acc@1  91.00 ( 92.96)	Acc@5  99.00 ( 99.75)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 2.2445e-01 (3.4067e-01)	Acc@1  95.00 ( 93.06)	Acc@5 100.00 ( 99.73)
Test: [ 90/100]	Time  0.078 ( 0.080)	Loss 1.2381e-01 (3.4078e-01)	Acc@1  96.00 ( 92.99)	Acc@5 100.00 ( 99.76)
 * Acc@1 92.930 Acc@5 99.770
### epoch[84] execution time: 102.44916534423828
EPOCH 85
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [85][  0/391]	Time  0.390 ( 0.390)	Data  0.148 ( 0.148)	Loss 1.4538e-03 (1.4538e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [85][ 10/391]	Time  0.241 ( 0.255)	Data  0.001 ( 0.015)	Loss 4.0307e-03 (4.8879e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [85][ 20/391]	Time  0.240 ( 0.248)	Data  0.001 ( 0.008)	Loss 1.3261e-02 (5.3467e-03)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [85][ 30/391]	Time  0.240 ( 0.246)	Data  0.001 ( 0.006)	Loss 3.6556e-03 (5.7325e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [85][ 40/391]	Time  0.246 ( 0.245)	Data  0.001 ( 0.005)	Loss 3.4552e-03 (5.3798e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [85][ 50/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 3.0679e-03 (5.5025e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [85][ 60/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.7290e-03 (5.0950e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [85][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.7632e-03 (5.0560e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [85][ 80/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.3480e-03 (5.0557e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [85][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 6.2294e-03 (5.1323e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [85][100/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.2598e-03 (5.0460e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [85][110/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 9.3778e-03 (5.0769e-03)	Acc@1  99.22 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [85][120/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.9883e-04 (5.0368e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [85][130/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9695e-03 (4.9422e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [85][140/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6427e-03 (4.9357e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [85][150/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1850e-02 (5.0032e-03)	Acc@1  99.22 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [85][160/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.6918e-03 (5.0914e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [85][170/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.2024e-03 (5.1582e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [85][180/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8100e-02 (5.1167e-03)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [85][190/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6218e-03 (5.0796e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [85][200/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.0853e-03 (4.9935e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [85][210/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.8370e-03 (5.1349e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [85][220/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8541e-02 (5.2922e-03)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [85][230/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3376e-03 (5.5122e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [85][240/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.9730e-04 (5.4358e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [85][250/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9117e-03 (5.4504e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [85][260/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7362e-02 (5.4409e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [85][270/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.4476e-03 (5.6028e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [85][280/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4905e-03 (5.5849e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [85][290/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.2222e-03 (5.5154e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [85][300/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3632e-03 (5.5212e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [85][310/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6125e-03 (5.3976e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [85][320/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6045e-03 (5.4437e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [85][330/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0971e-03 (5.4655e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [85][340/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.0668e-03 (5.4134e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [85][350/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2359e-03 (5.4597e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [85][360/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8193e-03 (5.5057e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [85][370/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2439e-03 (5.4957e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [85][380/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.9822e-03 (5.4432e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [85][390/391]	Time  0.177 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.8111e-04 (5.3781e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
## e[85] optimizer.zero_grad (sum) time: 0.4767310619354248
## e[85]       loss.backward (sum) time: 10.845151901245117
## e[85]      optimizer.step (sum) time: 47.61832594871521
## epoch[85] training(only) time: 94.42916321754456
# Switched to evaluate mode...
Test: [  0/100]	Time  0.212 ( 0.212)	Loss 2.2466e-01 (2.2466e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.077 ( 0.090)	Loss 1.9448e-01 (3.3463e-01)	Acc@1  96.00 ( 93.36)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 4.1159e-01 (3.4398e-01)	Acc@1  91.00 ( 93.10)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 2.9734e-01 (3.5174e-01)	Acc@1  92.00 ( 92.77)	Acc@5 100.00 ( 99.81)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 2.2374e-01 (3.5001e-01)	Acc@1  93.00 ( 92.88)	Acc@5 100.00 ( 99.83)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 2.2275e-01 (3.4812e-01)	Acc@1  95.00 ( 92.92)	Acc@5 100.00 ( 99.82)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 5.8913e-01 (3.3719e-01)	Acc@1  93.00 ( 93.02)	Acc@5 100.00 ( 99.82)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 5.1684e-01 (3.3944e-01)	Acc@1  91.00 ( 92.93)	Acc@5  99.00 ( 99.77)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 2.4989e-01 (3.3275e-01)	Acc@1  94.00 ( 93.11)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.077 ( 0.079)	Loss 1.5619e-01 (3.3471e-01)	Acc@1  96.00 ( 93.04)	Acc@5 100.00 ( 99.78)
 * Acc@1 92.980 Acc@5 99.790
### epoch[85] execution time: 102.44248247146606
EPOCH 86
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [86][  0/391]	Time  0.392 ( 0.392)	Data  0.134 ( 0.134)	Loss 3.6377e-03 (3.6377e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [86][ 10/391]	Time  0.241 ( 0.255)	Data  0.001 ( 0.013)	Loss 1.6788e-02 (5.5202e-03)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [86][ 20/391]	Time  0.241 ( 0.248)	Data  0.001 ( 0.007)	Loss 7.7827e-03 (4.7693e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [86][ 30/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.005)	Loss 2.3278e-03 (4.4985e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [86][ 40/391]	Time  0.242 ( 0.245)	Data  0.001 ( 0.004)	Loss 3.8017e-03 (4.6217e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [86][ 50/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 3.9375e-03 (4.7053e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [86][ 60/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.003)	Loss 3.1314e-03 (4.6499e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [86][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.3767e-03 (4.3401e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [86][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.7576e-03 (4.2371e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [86][ 90/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.3092e-03 (4.4498e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [86][100/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 3.9535e-03 (4.5322e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [86][110/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0221e-03 (4.7074e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [86][120/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.9353e-03 (4.6515e-03)	Acc@1  99.22 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [86][130/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.5623e-04 (4.4966e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [86][140/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.3080e-03 (4.7799e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [86][150/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.1041e-03 (4.7283e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [86][160/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2903e-03 (4.6881e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [86][170/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.0998e-03 (4.6597e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [86][180/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.7834e-03 (4.6474e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [86][190/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5696e-03 (4.8221e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [86][200/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0767e-03 (4.8363e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [86][210/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.6036e-03 (4.8568e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [86][220/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.9202e-03 (4.7821e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [86][230/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.8455e-03 (4.8019e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [86][240/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.7964e-03 (4.9173e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [86][250/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.9852e-03 (4.9818e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [86][260/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1088e-03 (5.0709e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [86][270/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.4479e-04 (5.0134e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [86][280/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.0886e-03 (5.1084e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [86][290/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7265e-03 (5.0678e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [86][300/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.5557e-03 (5.0737e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [86][310/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.3490e-03 (5.0273e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [86][320/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.5804e-02 (5.0269e-03)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [86][330/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.9801e-03 (4.9429e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [86][340/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.7740e-03 (4.8937e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [86][350/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.9684e-03 (4.9179e-03)	Acc@1  99.22 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [86][360/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.2437e-03 (4.8678e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [86][370/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.6266e-03 (4.8518e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [86][380/391]	Time  0.244 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.5510e-02 (4.9055e-03)	Acc@1  99.22 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [86][390/391]	Time  0.178 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.2158e-04 (4.8809e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
## e[86] optimizer.zero_grad (sum) time: 0.47757697105407715
## e[86]       loss.backward (sum) time: 10.846865892410278
## e[86]      optimizer.step (sum) time: 47.595043420791626
## epoch[86] training(only) time: 94.40880012512207
# Switched to evaluate mode...
Test: [  0/100]	Time  0.217 ( 0.217)	Loss 2.2137e-01 (2.2137e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.8351e-01 (3.3601e-01)	Acc@1  96.00 ( 93.36)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.078 ( 0.085)	Loss 4.1167e-01 (3.4920e-01)	Acc@1  91.00 ( 93.05)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.080 ( 0.082)	Loss 2.8196e-01 (3.5605e-01)	Acc@1  92.00 ( 92.71)	Acc@5 100.00 ( 99.81)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 2.2631e-01 (3.5349e-01)	Acc@1  93.00 ( 92.83)	Acc@5 100.00 ( 99.80)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 2.2721e-01 (3.5044e-01)	Acc@1  94.00 ( 92.92)	Acc@5 100.00 ( 99.80)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 5.7890e-01 (3.3910e-01)	Acc@1  92.00 ( 92.97)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 5.1686e-01 (3.3991e-01)	Acc@1  90.00 ( 92.96)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 2.4699e-01 (3.3390e-01)	Acc@1  94.00 ( 93.09)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.078 ( 0.080)	Loss 1.2738e-01 (3.3378e-01)	Acc@1  95.00 ( 93.03)	Acc@5 100.00 ( 99.78)
 * Acc@1 92.950 Acc@5 99.790
### epoch[86] execution time: 102.4534821510315
EPOCH 87
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [87][  0/391]	Time  0.391 ( 0.391)	Data  0.145 ( 0.145)	Loss 3.3714e-03 (3.3714e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [87][ 10/391]	Time  0.240 ( 0.254)	Data  0.001 ( 0.014)	Loss 2.6848e-03 (2.8159e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [87][ 20/391]	Time  0.241 ( 0.248)	Data  0.001 ( 0.008)	Loss 1.0656e-03 (3.5696e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [87][ 30/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.006)	Loss 5.8582e-03 (3.7140e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [87][ 40/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.005)	Loss 2.2976e-02 (4.0842e-03)	Acc@1  99.22 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [87][ 50/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.7365e-03 (3.8114e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [87][ 60/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.004)	Loss 1.5319e-02 (4.0484e-03)	Acc@1  99.22 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [87][ 70/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 8.4554e-04 (4.0145e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [87][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.4158e-03 (4.0597e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [87][ 90/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 1.5164e-02 (4.0267e-03)	Acc@1  99.22 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [87][100/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 7.8767e-03 (4.7291e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [87][110/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.2966e-03 (4.8715e-03)	Acc@1  99.22 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [87][120/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1100e-03 (4.7396e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [87][130/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0822e-03 (4.6401e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [87][140/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9608e-03 (4.5230e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [87][150/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2562e-03 (4.3652e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [87][160/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.6210e-03 (4.3734e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [87][170/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3550e-03 (4.3514e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [87][180/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.0736e-03 (4.2854e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [87][190/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7869e-03 (4.3710e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [87][200/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.3752e-04 (4.4802e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [87][210/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.5934e-03 (4.5039e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [87][220/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.4592e-03 (4.4653e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [87][230/391]	Time  0.246 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.8630e-02 (4.6108e-03)	Acc@1  99.22 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [87][240/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2901e-03 (4.5247e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [87][250/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.9603e-03 (4.5881e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [87][260/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.1528e-03 (4.5934e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [87][270/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.5558e-03 (4.6947e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [87][280/391]	Time  0.244 ( 0.241)	Data  0.001 ( 0.002)	Loss 8.8811e-04 (4.6621e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [87][290/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 9.4893e-04 (4.5966e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [87][300/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.5737e-03 (4.5913e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [87][310/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1786e-02 (4.6315e-03)	Acc@1  99.22 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [87][320/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.6019e-03 (4.6607e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [87][330/391]	Time  0.248 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.6004e-03 (4.6415e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [87][340/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.1668e-03 (4.6366e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [87][350/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 7.1499e-03 (4.6769e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [87][360/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.8271e-03 (4.6265e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [87][370/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.9404e-03 (4.6435e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [87][380/391]	Time  0.245 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.6970e-03 (4.7466e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [87][390/391]	Time  0.175 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.0068e-03 (4.7816e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
## e[87] optimizer.zero_grad (sum) time: 0.47715258598327637
## e[87]       loss.backward (sum) time: 10.812376976013184
## e[87]      optimizer.step (sum) time: 47.57950258255005
## epoch[87] training(only) time: 94.34830355644226
# Switched to evaluate mode...
Test: [  0/100]	Time  0.205 ( 0.205)	Loss 2.2075e-01 (2.2075e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.078 ( 0.089)	Loss 1.7203e-01 (3.3165e-01)	Acc@1  96.00 ( 93.36)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 4.1268e-01 (3.4305e-01)	Acc@1  90.00 ( 93.05)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 2.9467e-01 (3.5263e-01)	Acc@1  92.00 ( 92.81)	Acc@5 100.00 ( 99.77)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 2.2740e-01 (3.5072e-01)	Acc@1  93.00 ( 92.73)	Acc@5 100.00 ( 99.78)
Test: [ 50/100]	Time  0.077 ( 0.080)	Loss 2.0967e-01 (3.4848e-01)	Acc@1  94.00 ( 92.82)	Acc@5 100.00 ( 99.80)
Test: [ 60/100]	Time  0.079 ( 0.080)	Loss 6.0166e-01 (3.3823e-01)	Acc@1  91.00 ( 92.84)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.077 ( 0.080)	Loss 5.2216e-01 (3.4020e-01)	Acc@1  90.00 ( 92.77)	Acc@5  99.00 ( 99.77)
Test: [ 80/100]	Time  0.079 ( 0.079)	Loss 2.6611e-01 (3.3379e-01)	Acc@1  93.00 ( 92.93)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.077 ( 0.079)	Loss 1.6271e-01 (3.3533e-01)	Acc@1  95.00 ( 92.85)	Acc@5 100.00 ( 99.78)
 * Acc@1 92.790 Acc@5 99.790
### epoch[87] execution time: 102.35529923439026
EPOCH 88
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [88][  0/391]	Time  0.386 ( 0.386)	Data  0.144 ( 0.144)	Loss 1.3362e-03 (1.3362e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [88][ 10/391]	Time  0.239 ( 0.254)	Data  0.001 ( 0.014)	Loss 1.2196e-02 (4.3864e-03)	Acc@1  99.22 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [88][ 20/391]	Time  0.240 ( 0.248)	Data  0.001 ( 0.008)	Loss 1.9195e-03 (4.5194e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [88][ 30/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.006)	Loss 1.9390e-02 (5.2217e-03)	Acc@1  98.44 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [88][ 40/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.005)	Loss 2.7718e-03 (4.9146e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [88][ 50/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 3.7915e-03 (4.6716e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [88][ 60/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.3418e-03 (4.6907e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [88][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.2007e-03 (4.4839e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [88][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.0602e-02 (4.4403e-03)	Acc@1  99.22 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [88][ 90/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.003)	Loss 3.4318e-03 (4.7160e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [88][100/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 2.9918e-03 (4.7829e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [88][110/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7313e-03 (4.7489e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [88][120/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.4591e-04 (4.8001e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [88][130/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0788e-03 (5.0263e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [88][140/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4095e-03 (5.0949e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [88][150/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8749e-03 (5.2297e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [88][160/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.6018e-03 (5.2790e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [88][170/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.2282e-03 (5.2265e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [88][180/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.3246e-03 (5.2463e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [88][190/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.3075e-03 (5.2504e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [88][200/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.8710e-03 (5.1624e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [88][210/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.4939e-03 (5.1180e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [88][220/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.6476e-02 (5.1008e-03)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [88][230/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.6771e-03 (5.0105e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [88][240/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.5196e-02 (5.0567e-03)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [88][250/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2722e-03 (4.9732e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [88][260/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.2863e-02 (5.1440e-03)	Acc@1  98.44 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [88][270/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.1280e-03 (5.1174e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [88][280/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.4347e-03 (5.0372e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [88][290/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.8935e-02 (5.0223e-03)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [88][300/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2134e-02 (5.0028e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [88][310/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.4729e-03 (4.9504e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [88][320/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.3409e-03 (5.0798e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [88][330/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.9805e-04 (5.0823e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [88][340/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2302e-02 (5.2063e-03)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [88][350/391]	Time  0.242 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.3718e-03 (5.1537e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [88][360/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.7664e-03 (5.0656e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [88][370/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.7162e-02 (5.2209e-03)	Acc@1  98.44 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [88][380/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.2362e-03 (5.2800e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [88][390/391]	Time  0.176 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.7685e-03 (5.2195e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
## e[88] optimizer.zero_grad (sum) time: 0.4757833480834961
## e[88]       loss.backward (sum) time: 10.853395938873291
## e[88]      optimizer.step (sum) time: 47.54873704910278
## epoch[88] training(only) time: 94.28195691108704
# Switched to evaluate mode...
Test: [  0/100]	Time  0.204 ( 0.204)	Loss 2.0907e-01 (2.0907e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.078 ( 0.089)	Loss 1.6274e-01 (3.3311e-01)	Acc@1  96.00 ( 93.45)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.077 ( 0.084)	Loss 3.9570e-01 (3.4026e-01)	Acc@1  91.00 ( 93.19)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 2.8697e-01 (3.4578e-01)	Acc@1  93.00 ( 92.94)	Acc@5 100.00 ( 99.81)
Test: [ 40/100]	Time  0.077 ( 0.081)	Loss 2.1691e-01 (3.4868e-01)	Acc@1  94.00 ( 92.93)	Acc@5 100.00 ( 99.80)
Test: [ 50/100]	Time  0.079 ( 0.080)	Loss 2.4131e-01 (3.4630e-01)	Acc@1  95.00 ( 93.00)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 5.8973e-01 (3.3588e-01)	Acc@1  90.00 ( 93.02)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.077 ( 0.080)	Loss 5.0807e-01 (3.3831e-01)	Acc@1  91.00 ( 92.94)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.078 ( 0.079)	Loss 2.1792e-01 (3.3161e-01)	Acc@1  95.00 ( 93.11)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.5331e-01 (3.3239e-01)	Acc@1  95.00 ( 93.03)	Acc@5 100.00 ( 99.78)
 * Acc@1 92.980 Acc@5 99.790
### epoch[88] execution time: 102.27587223052979
EPOCH 89
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [89][  0/391]	Time  0.397 ( 0.397)	Data  0.152 ( 0.152)	Loss 7.3536e-03 (7.3536e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [89][ 10/391]	Time  0.240 ( 0.255)	Data  0.001 ( 0.015)	Loss 2.7796e-03 (4.7426e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [89][ 20/391]	Time  0.241 ( 0.248)	Data  0.001 ( 0.008)	Loss 2.2931e-03 (3.9803e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [89][ 30/391]	Time  0.240 ( 0.246)	Data  0.001 ( 0.006)	Loss 8.3871e-04 (4.1156e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [89][ 40/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.005)	Loss 2.0932e-03 (3.9990e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [89][ 50/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 9.1967e-04 (3.8130e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [89][ 60/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.004)	Loss 2.2321e-03 (3.9530e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [89][ 70/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.0451e-03 (4.0215e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [89][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.4959e-03 (3.9549e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [89][ 90/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 2.4326e-03 (4.0968e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [89][100/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.003)	Loss 7.9120e-03 (4.2547e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [89][110/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.003)	Loss 4.6419e-03 (4.1191e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [89][120/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0459e-02 (4.4287e-03)	Acc@1  99.22 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [89][130/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1407e-03 (4.3524e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [89][140/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.5386e-03 (4.3234e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [89][150/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0396e-02 (4.4132e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [89][160/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3527e-03 (4.3152e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [89][170/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4147e-02 (4.4517e-03)	Acc@1  99.22 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [89][180/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.5712e-03 (4.5125e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [89][190/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.2176e-03 (4.4376e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [89][200/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.0605e-02 (4.4272e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [89][210/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.3934e-03 (4.4742e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [89][220/391]	Time  0.239 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.4125e-02 (4.5818e-03)	Acc@1  99.22 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [89][230/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.5505e-03 (4.5342e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [89][240/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.5711e-03 (4.4849e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [89][250/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.3322e-03 (4.5355e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [89][260/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.2737e-03 (4.6458e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [89][270/391]	Time  0.244 ( 0.241)	Data  0.001 ( 0.002)	Loss 6.0947e-03 (4.6148e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [89][280/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.9230e-03 (4.6017e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [89][290/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.1820e-03 (4.6188e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [89][300/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.5546e-03 (4.6120e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [89][310/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.1571e-03 (4.5687e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [89][320/391]	Time  0.245 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.5828e-03 (4.5087e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [89][330/391]	Time  0.240 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.5039e-02 (4.5370e-03)	Acc@1  99.22 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [89][340/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.5490e-03 (4.4877e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [89][350/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 2.4568e-03 (4.5884e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [89][360/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 5.5638e-03 (4.6395e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [89][370/391]	Time  0.245 ( 0.241)	Data  0.001 ( 0.002)	Loss 3.8922e-03 (4.6132e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [89][380/391]	Time  0.241 ( 0.241)	Data  0.001 ( 0.002)	Loss 4.4043e-03 (4.7023e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [89][390/391]	Time  0.177 ( 0.241)	Data  0.001 ( 0.002)	Loss 1.6582e-02 (4.7176e-03)	Acc@1  98.75 ( 99.91)	Acc@5 100.00 (100.00)
## e[89] optimizer.zero_grad (sum) time: 0.47899866104125977
## e[89]       loss.backward (sum) time: 10.839624643325806
## e[89]      optimizer.step (sum) time: 47.54854941368103
## epoch[89] training(only) time: 94.3463397026062
# Switched to evaluate mode...
Test: [  0/100]	Time  0.211 ( 0.211)	Loss 2.3561e-01 (2.3561e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.7756e-01 (3.3674e-01)	Acc@1  96.00 ( 93.45)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.077 ( 0.084)	Loss 4.1032e-01 (3.5233e-01)	Acc@1  91.00 ( 93.10)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.079 ( 0.082)	Loss 2.8447e-01 (3.5824e-01)	Acc@1  93.00 ( 92.74)	Acc@5 100.00 ( 99.77)
Test: [ 40/100]	Time  0.077 ( 0.081)	Loss 2.3016e-01 (3.5541e-01)	Acc@1  92.00 ( 92.80)	Acc@5 100.00 ( 99.78)
Test: [ 50/100]	Time  0.079 ( 0.081)	Loss 2.0194e-01 (3.5295e-01)	Acc@1  94.00 ( 92.86)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 6.0266e-01 (3.4149e-01)	Acc@1  91.00 ( 92.93)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 5.1368e-01 (3.4212e-01)	Acc@1  90.00 ( 92.86)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.077 ( 0.080)	Loss 2.5413e-01 (3.3644e-01)	Acc@1  94.00 ( 93.00)	Acc@5 100.00 ( 99.72)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.4201e-01 (3.3736e-01)	Acc@1  95.00 ( 92.91)	Acc@5 100.00 ( 99.75)
 * Acc@1 92.840 Acc@5 99.760
### epoch[89] execution time: 102.36466884613037
### Training complete:
#### total training(only) time: 8493.215149641037
##### Total run time: 9218.959868431091
