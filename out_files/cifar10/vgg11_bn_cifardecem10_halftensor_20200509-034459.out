# Model: vgg11_bn
# Dataset: cifardecem
# Batch size: 128
# Freezeout: False
# Low precision: True
# Epochs: 90
# Initial learning rate: 0.1
# module_name: models_cifardecem.vgg
<function vgg11_bn at 0x7fecb39040d0>
# model requested: 'vgg11_bn'
# printing out the model
VGG(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU(inplace=True)
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): ReLU(inplace=True)
    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (13): ReLU(inplace=True)
    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (17): ReLU(inplace=True)
    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): ReLU(inplace=True)
    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): ReLU(inplace=True)
    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (27): ReLU(inplace=True)
    (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (classifier): Sequential(
    (0): Linear(in_features=512, out_features=4096, bias=True)
    (1): ReLU(inplace=True)
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=4096, out_features=4096, bias=True)
    (4): ReLU(inplace=True)
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=4096, out_features=10, bias=True)
  )
)
# model is low precision
# Model: vgg11_bn
# Dataset: cifardecem
# Freezeout: False
# Low precision: True
# Initial learning rate: 0.1
# Criterion: CrossEntropyLoss()
# Optimizer: SGD
#	lr 0.1
#	momentum 0.9
#	dampening 0
#	weight_decay 0.0001
#	nesterov False
CIFAR10 loading and normalization
==> Preparing data..
# CIFAR10 / low precision
Files already downloaded and verified
Files already downloaded and verified
#--> Training data: <--#
#-->>
EPOCH 0
# current learning rate: 0.1
# Switched to train mode...
Epoch: [0][  0/391]	Time  3.407 ( 3.407)	Data  0.099 ( 0.099)	Loss 2.3242e+00 (2.3242e+00)	Acc@1  10.94 ( 10.94)	Acc@5  53.91 ( 53.91)
Epoch: [0][ 10/391]	Time  0.030 ( 0.332)	Data  0.001 ( 0.010)	Loss 2.2051e+00 (2.4783e+00)	Acc@1  17.97 ( 13.71)	Acc@5  74.22 ( 60.01)
Epoch: [0][ 20/391]	Time  0.031 ( 0.186)	Data  0.001 ( 0.006)	Loss 2.1836e+00 (2.3299e+00)	Acc@1  13.28 ( 15.40)	Acc@5  75.00 ( 66.59)
Epoch: [0][ 30/391]	Time  0.019 ( 0.133)	Data  0.001 ( 0.005)	Loss 2.0859e+00 (2.2367e+00)	Acc@1  24.22 ( 17.36)	Acc@5  75.78 ( 70.72)
Epoch: [0][ 40/391]	Time  0.032 ( 0.107)	Data  0.000 ( 0.004)	Loss 1.9541e+00 (2.1951e+00)	Acc@1  23.44 ( 18.04)	Acc@5  83.59 ( 72.85)
Epoch: [0][ 50/391]	Time  0.021 ( 0.091)	Data  0.001 ( 0.003)	Loss 1.9775e+00 (2.1555e+00)	Acc@1  25.78 ( 18.95)	Acc@5  85.94 ( 74.56)
Epoch: [0][ 60/391]	Time  0.019 ( 0.080)	Data  0.001 ( 0.003)	Loss 1.9336e+00 (2.1248e+00)	Acc@1  20.31 ( 19.65)	Acc@5  80.47 ( 75.73)
Epoch: [0][ 70/391]	Time  0.018 ( 0.072)	Data  0.001 ( 0.003)	Loss 2.0547e+00 (2.0994e+00)	Acc@1  21.09 ( 20.41)	Acc@5  79.69 ( 76.55)
Epoch: [0][ 80/391]	Time  0.019 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.0703e+00 (2.0846e+00)	Acc@1  15.62 ( 20.99)	Acc@5  82.03 ( 77.33)
Epoch: [0][ 90/391]	Time  0.018 ( 0.061)	Data  0.001 ( 0.003)	Loss 1.9541e+00 (2.0657e+00)	Acc@1  26.56 ( 21.52)	Acc@5  82.81 ( 77.86)
Epoch: [0][100/391]	Time  0.039 ( 0.058)	Data  0.002 ( 0.003)	Loss 1.7139e+00 (2.0474e+00)	Acc@1  26.56 ( 21.81)	Acc@5  89.06 ( 78.40)
Epoch: [0][110/391]	Time  0.023 ( 0.055)	Data  0.001 ( 0.003)	Loss 1.7832e+00 (2.0334e+00)	Acc@1  32.81 ( 22.28)	Acc@5  83.59 ( 78.98)
Epoch: [0][120/391]	Time  0.017 ( 0.052)	Data  0.001 ( 0.002)	Loss 1.8564e+00 (2.0202e+00)	Acc@1  26.56 ( 22.69)	Acc@5  76.56 ( 79.38)
Epoch: [0][130/391]	Time  0.024 ( 0.050)	Data  0.003 ( 0.002)	Loss 1.9668e+00 (2.0134e+00)	Acc@1  21.09 ( 22.79)	Acc@5  82.81 ( 79.71)
Epoch: [0][140/391]	Time  0.018 ( 0.048)	Data  0.001 ( 0.002)	Loss 1.9268e+00 (2.0056e+00)	Acc@1  25.78 ( 23.06)	Acc@5  81.25 ( 80.03)
Epoch: [0][150/391]	Time  0.036 ( 0.047)	Data  0.001 ( 0.002)	Loss 1.7725e+00 (2.0001e+00)	Acc@1  31.25 ( 23.18)	Acc@5  86.72 ( 80.19)
Epoch: [0][160/391]	Time  0.038 ( 0.045)	Data  0.001 ( 0.002)	Loss 1.8926e+00 (1.9923e+00)	Acc@1  30.47 ( 23.60)	Acc@5  87.50 ( 80.52)
Epoch: [0][170/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.002)	Loss 1.9443e+00 (1.9846e+00)	Acc@1  22.66 ( 23.76)	Acc@5  80.47 ( 80.81)
Epoch: [0][180/391]	Time  0.028 ( 0.043)	Data  0.001 ( 0.002)	Loss 1.9229e+00 (1.9754e+00)	Acc@1  23.44 ( 23.95)	Acc@5  80.47 ( 81.10)
Epoch: [0][190/391]	Time  0.022 ( 0.042)	Data  0.005 ( 0.002)	Loss 1.6836e+00 (1.9672e+00)	Acc@1  30.47 ( 24.09)	Acc@5  88.28 ( 81.29)
Epoch: [0][200/391]	Time  0.019 ( 0.041)	Data  0.002 ( 0.002)	Loss 1.8311e+00 (1.9571e+00)	Acc@1  27.34 ( 24.46)	Acc@5  86.72 ( 81.61)
Epoch: [0][210/391]	Time  0.024 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.7314e+00 (1.9510e+00)	Acc@1  28.91 ( 24.65)	Acc@5  89.06 ( 81.82)
Epoch: [0][220/391]	Time  0.018 ( 0.039)	Data  0.001 ( 0.002)	Loss 1.6221e+00 (1.9425e+00)	Acc@1  34.38 ( 24.87)	Acc@5  88.28 ( 82.09)
Epoch: [0][230/391]	Time  0.019 ( 0.039)	Data  0.002 ( 0.002)	Loss 1.8115e+00 (1.9353e+00)	Acc@1  32.03 ( 25.17)	Acc@5  86.72 ( 82.28)
Epoch: [0][240/391]	Time  0.020 ( 0.038)	Data  0.002 ( 0.002)	Loss 1.7451e+00 (1.9260e+00)	Acc@1  26.56 ( 25.40)	Acc@5  84.38 ( 82.50)
Epoch: [0][250/391]	Time  0.018 ( 0.038)	Data  0.001 ( 0.002)	Loss 1.6934e+00 (1.9173e+00)	Acc@1  34.38 ( 25.70)	Acc@5  89.84 ( 82.79)
Epoch: [0][260/391]	Time  0.031 ( 0.037)	Data  0.001 ( 0.002)	Loss 1.6143e+00 (1.9097e+00)	Acc@1  36.72 ( 25.93)	Acc@5  89.84 ( 82.95)
Epoch: [0][270/391]	Time  0.018 ( 0.037)	Data  0.001 ( 0.002)	Loss 1.6201e+00 (1.9022e+00)	Acc@1  30.47 ( 26.15)	Acc@5  92.97 ( 83.14)
Epoch: [0][280/391]	Time  0.019 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.6992e+00 (1.8950e+00)	Acc@1  27.34 ( 26.47)	Acc@5  90.62 ( 83.34)
Epoch: [0][290/391]	Time  0.019 ( 0.036)	Data  0.002 ( 0.002)	Loss 1.6895e+00 (1.8895e+00)	Acc@1  28.12 ( 26.66)	Acc@5  89.06 ( 83.49)
Epoch: [0][300/391]	Time  0.023 ( 0.035)	Data  0.002 ( 0.002)	Loss 1.6006e+00 (1.8807e+00)	Acc@1  34.38 ( 27.00)	Acc@5  86.72 ( 83.70)
Epoch: [0][310/391]	Time  0.024 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.6582e+00 (1.8734e+00)	Acc@1  35.94 ( 27.32)	Acc@5  86.72 ( 83.92)
Epoch: [0][320/391]	Time  0.019 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.6914e+00 (1.8661e+00)	Acc@1  40.62 ( 27.65)	Acc@5  90.62 ( 84.11)
Epoch: [0][330/391]	Time  0.017 ( 0.034)	Data  0.000 ( 0.002)	Loss 1.6475e+00 (1.8590e+00)	Acc@1  31.25 ( 27.90)	Acc@5  89.06 ( 84.26)
Epoch: [0][340/391]	Time  0.019 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5391e+00 (1.8526e+00)	Acc@1  39.84 ( 28.13)	Acc@5  87.50 ( 84.42)
Epoch: [0][350/391]	Time  0.023 ( 0.034)	Data  0.002 ( 0.002)	Loss 1.7295e+00 (1.8473e+00)	Acc@1  28.12 ( 28.38)	Acc@5  89.06 ( 84.58)
Epoch: [0][360/391]	Time  0.046 ( 0.034)	Data  0.005 ( 0.002)	Loss 1.6143e+00 (1.8413e+00)	Acc@1  38.28 ( 28.61)	Acc@5  92.97 ( 84.76)
Epoch: [0][370/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.002)	Loss 1.6934e+00 (1.8366e+00)	Acc@1  33.59 ( 28.81)	Acc@5  91.41 ( 84.89)
Epoch: [0][380/391]	Time  0.018 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.6631e+00 (1.8305e+00)	Acc@1  41.41 ( 29.08)	Acc@5  85.94 ( 85.05)
Epoch: [0][390/391]	Time  0.326 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.7100e+00 (1.8235e+00)	Acc@1  36.25 ( 29.36)	Acc@5  88.75 ( 85.19)
## e[0] optimizer.zero_grad (sum) time: 0.10840630531311035
## e[0]       loss.backward (sum) time: 2.578521490097046
## e[0]      optimizer.step (sum) time: 0.9175474643707275
## epoch[0] training(only) time: 13.210776090621948
# Switched to evaluate mode...
Test: [  0/100]	Time  0.294 ( 0.294)	Loss 1.5840e+00 (1.5840e+00)	Acc@1  37.00 ( 37.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.012 ( 0.039)	Loss 1.5312e+00 (1.5179e+00)	Acc@1  41.00 ( 41.09)	Acc@5  88.00 ( 90.55)
Test: [ 20/100]	Time  0.011 ( 0.027)	Loss 1.4043e+00 (1.5137e+00)	Acc@1  50.00 ( 41.71)	Acc@5  94.00 ( 91.19)
Test: [ 30/100]	Time  0.011 ( 0.022)	Loss 1.3770e+00 (1.5246e+00)	Acc@1  41.00 ( 40.81)	Acc@5  93.00 ( 91.32)
Test: [ 40/100]	Time  0.016 ( 0.020)	Loss 1.5869e+00 (1.5267e+00)	Acc@1  40.00 ( 40.66)	Acc@5  91.00 ( 91.39)
Test: [ 50/100]	Time  0.007 ( 0.018)	Loss 1.5996e+00 (1.5206e+00)	Acc@1  40.00 ( 40.92)	Acc@5  92.00 ( 91.59)
Test: [ 60/100]	Time  0.016 ( 0.018)	Loss 1.5352e+00 (1.5233e+00)	Acc@1  39.00 ( 40.98)	Acc@5  94.00 ( 91.69)
Test: [ 70/100]	Time  0.008 ( 0.017)	Loss 1.5283e+00 (1.5277e+00)	Acc@1  42.00 ( 41.03)	Acc@5  94.00 ( 91.66)
Test: [ 80/100]	Time  0.016 ( 0.016)	Loss 1.3975e+00 (1.5189e+00)	Acc@1  47.00 ( 41.44)	Acc@5  91.00 ( 91.67)
Test: [ 90/100]	Time  0.011 ( 0.016)	Loss 1.5645e+00 (1.5223e+00)	Acc@1  42.00 ( 41.57)	Acc@5  98.00 ( 91.69)
 * Acc@1 41.640 Acc@5 91.700
### epoch[0] execution time: 14.926721334457397
EPOCH 1
# current learning rate: 0.1
# Switched to train mode...
Epoch: [1][  0/391]	Time  0.204 ( 0.204)	Data  0.174 ( 0.174)	Loss 1.4551e+00 (1.4551e+00)	Acc@1  43.75 ( 43.75)	Acc@5  92.97 ( 92.97)
Epoch: [1][ 10/391]	Time  0.022 ( 0.039)	Data  0.005 ( 0.018)	Loss 1.7002e+00 (1.6644e+00)	Acc@1  34.38 ( 37.14)	Acc@5  86.72 ( 88.71)
Epoch: [1][ 20/391]	Time  0.019 ( 0.032)	Data  0.001 ( 0.010)	Loss 1.5283e+00 (1.6519e+00)	Acc@1  37.50 ( 37.17)	Acc@5  91.41 ( 89.62)
Epoch: [1][ 30/391]	Time  0.029 ( 0.030)	Data  0.003 ( 0.008)	Loss 1.5420e+00 (1.6253e+00)	Acc@1  39.06 ( 37.65)	Acc@5  90.62 ( 90.12)
Epoch: [1][ 40/391]	Time  0.016 ( 0.029)	Data  0.001 ( 0.006)	Loss 1.6289e+00 (1.6189e+00)	Acc@1  35.94 ( 37.82)	Acc@5  91.41 ( 90.02)
Epoch: [1][ 50/391]	Time  0.017 ( 0.027)	Data  0.000 ( 0.005)	Loss 1.5762e+00 (1.6212e+00)	Acc@1  40.62 ( 37.96)	Acc@5  91.41 ( 90.01)
Epoch: [1][ 60/391]	Time  0.017 ( 0.027)	Data  0.000 ( 0.005)	Loss 1.5547e+00 (1.6129e+00)	Acc@1  46.09 ( 38.36)	Acc@5  89.06 ( 90.20)
Epoch: [1][ 70/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.5352e+00 (1.6008e+00)	Acc@1  42.19 ( 39.32)	Acc@5  90.62 ( 90.29)
Epoch: [1][ 80/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.4561e+00 (1.5966e+00)	Acc@1  45.31 ( 39.48)	Acc@5  94.53 ( 90.36)
Epoch: [1][ 90/391]	Time  0.028 ( 0.026)	Data  0.000 ( 0.004)	Loss 1.4160e+00 (1.5896e+00)	Acc@1  47.66 ( 39.91)	Acc@5  94.53 ( 90.47)
Epoch: [1][100/391]	Time  0.019 ( 0.026)	Data  0.002 ( 0.004)	Loss 1.5820e+00 (1.5836e+00)	Acc@1  39.06 ( 40.27)	Acc@5  92.97 ( 90.67)
Epoch: [1][110/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5195e+00 (1.5782e+00)	Acc@1  47.66 ( 40.50)	Acc@5  93.75 ( 90.82)
Epoch: [1][120/391]	Time  0.037 ( 0.025)	Data  0.003 ( 0.003)	Loss 1.4492e+00 (1.5732e+00)	Acc@1  49.22 ( 40.66)	Acc@5  90.62 ( 90.99)
Epoch: [1][130/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.5020e+00 (1.5658e+00)	Acc@1  39.84 ( 40.92)	Acc@5  94.53 ( 91.08)
Epoch: [1][140/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.6436e+00 (1.5623e+00)	Acc@1  42.19 ( 41.23)	Acc@5  90.62 ( 91.19)
Epoch: [1][150/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.6064e+00 (1.5596e+00)	Acc@1  44.53 ( 41.34)	Acc@5  91.41 ( 91.25)
Epoch: [1][160/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.5771e+00 (1.5555e+00)	Acc@1  50.00 ( 41.58)	Acc@5  92.97 ( 91.26)
Epoch: [1][170/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 1.4355e+00 (1.5558e+00)	Acc@1  45.31 ( 41.74)	Acc@5  89.84 ( 91.19)
Epoch: [1][180/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.6182e+00 (1.5536e+00)	Acc@1  42.97 ( 41.92)	Acc@5  91.41 ( 91.26)
Epoch: [1][190/391]	Time  0.028 ( 0.025)	Data  0.003 ( 0.003)	Loss 1.5938e+00 (1.5498e+00)	Acc@1  42.97 ( 42.13)	Acc@5  91.41 ( 91.35)
Epoch: [1][200/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 1.4570e+00 (1.5434e+00)	Acc@1  46.09 ( 42.41)	Acc@5  95.31 ( 91.44)
Epoch: [1][210/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.4795e+00 (1.5407e+00)	Acc@1  50.78 ( 42.64)	Acc@5  92.19 ( 91.42)
Epoch: [1][220/391]	Time  0.019 ( 0.024)	Data  0.002 ( 0.003)	Loss 1.3369e+00 (1.5377e+00)	Acc@1  51.56 ( 42.86)	Acc@5  92.97 ( 91.46)
Epoch: [1][230/391]	Time  0.032 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.3867e+00 (1.5341e+00)	Acc@1  50.00 ( 43.06)	Acc@5  95.31 ( 91.49)
Epoch: [1][240/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.003)	Loss 1.5215e+00 (1.5316e+00)	Acc@1  42.97 ( 43.28)	Acc@5  85.94 ( 91.48)
Epoch: [1][250/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.4023e+00 (1.5316e+00)	Acc@1  47.66 ( 43.30)	Acc@5  92.19 ( 91.49)
Epoch: [1][260/391]	Time  0.035 ( 0.024)	Data  0.001 ( 0.003)	Loss 1.4600e+00 (1.5255e+00)	Acc@1  49.22 ( 43.61)	Acc@5  92.19 ( 91.57)
Epoch: [1][270/391]	Time  0.017 ( 0.024)	Data  0.000 ( 0.002)	Loss 1.3008e+00 (1.5197e+00)	Acc@1  51.56 ( 43.91)	Acc@5  93.75 ( 91.65)
Epoch: [1][280/391]	Time  0.032 ( 0.024)	Data  0.000 ( 0.002)	Loss 1.3701e+00 (1.5180e+00)	Acc@1  47.66 ( 44.01)	Acc@5  86.72 ( 91.66)
Epoch: [1][290/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.6387e+00 (1.5166e+00)	Acc@1  44.53 ( 44.13)	Acc@5  92.19 ( 91.69)
Epoch: [1][300/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.1416e+00 (1.5126e+00)	Acc@1  57.03 ( 44.34)	Acc@5  96.88 ( 91.79)
Epoch: [1][310/391]	Time  0.017 ( 0.024)	Data  0.000 ( 0.002)	Loss 1.4336e+00 (1.5073e+00)	Acc@1  47.66 ( 44.61)	Acc@5  92.19 ( 91.82)
Epoch: [1][320/391]	Time  0.027 ( 0.024)	Data  0.003 ( 0.002)	Loss 1.1719e+00 (1.5022e+00)	Acc@1  58.59 ( 44.86)	Acc@5  96.09 ( 91.85)
Epoch: [1][330/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.2637e+00 (1.4978e+00)	Acc@1  55.47 ( 45.08)	Acc@5  92.97 ( 91.90)
Epoch: [1][340/391]	Time  0.021 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.3203e+00 (1.4934e+00)	Acc@1  51.56 ( 45.33)	Acc@5  94.53 ( 91.96)
Epoch: [1][350/391]	Time  0.025 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.3418e+00 (1.4878e+00)	Acc@1  49.22 ( 45.56)	Acc@5  92.19 ( 92.03)
Epoch: [1][360/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.4668e+00 (1.4851e+00)	Acc@1  49.22 ( 45.72)	Acc@5  89.06 ( 92.07)
Epoch: [1][370/391]	Time  0.036 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.4648e+00 (1.4822e+00)	Acc@1  52.34 ( 45.95)	Acc@5  95.31 ( 92.10)
Epoch: [1][380/391]	Time  0.023 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.5898e+00 (1.4789e+00)	Acc@1  48.44 ( 46.13)	Acc@5  89.06 ( 92.13)
Epoch: [1][390/391]	Time  0.016 ( 0.024)	Data  0.000 ( 0.002)	Loss 1.2969e+00 (1.4753e+00)	Acc@1  58.75 ( 46.35)	Acc@5  96.25 ( 92.16)
## e[1] optimizer.zero_grad (sum) time: 0.10603952407836914
## e[1]       loss.backward (sum) time: 2.132709264755249
## e[1]      optimizer.step (sum) time: 0.9038059711456299
## epoch[1] training(only) time: 9.603421926498413
# Switched to evaluate mode...
Test: [  0/100]	Time  0.203 ( 0.203)	Loss 1.1699e+00 (1.1699e+00)	Acc@1  60.00 ( 60.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.009 ( 0.030)	Loss 1.0156e+00 (1.2123e+00)	Acc@1  68.00 ( 56.91)	Acc@5  96.00 ( 95.45)
Test: [ 20/100]	Time  0.011 ( 0.022)	Loss 1.2705e+00 (1.2325e+00)	Acc@1  52.00 ( 56.24)	Acc@5  94.00 ( 95.24)
Test: [ 30/100]	Time  0.020 ( 0.019)	Loss 1.1514e+00 (1.2493e+00)	Acc@1  60.00 ( 55.58)	Acc@5  97.00 ( 95.06)
Test: [ 40/100]	Time  0.008 ( 0.018)	Loss 1.3223e+00 (1.2540e+00)	Acc@1  56.00 ( 55.44)	Acc@5  94.00 ( 94.93)
Test: [ 50/100]	Time  0.008 ( 0.017)	Loss 1.3232e+00 (1.2478e+00)	Acc@1  55.00 ( 55.43)	Acc@5  94.00 ( 94.88)
Test: [ 60/100]	Time  0.008 ( 0.016)	Loss 1.3066e+00 (1.2563e+00)	Acc@1  51.00 ( 54.89)	Acc@5  95.00 ( 94.67)
Test: [ 70/100]	Time  0.014 ( 0.016)	Loss 1.1758e+00 (1.2607e+00)	Acc@1  58.00 ( 54.83)	Acc@5  94.00 ( 94.66)
Test: [ 80/100]	Time  0.016 ( 0.015)	Loss 1.2158e+00 (1.2551e+00)	Acc@1  59.00 ( 55.14)	Acc@5  93.00 ( 94.70)
Test: [ 90/100]	Time  0.030 ( 0.015)	Loss 1.2959e+00 (1.2590e+00)	Acc@1  50.00 ( 54.93)	Acc@5  97.00 ( 94.63)
 * Acc@1 55.200 Acc@5 94.620
### epoch[1] execution time: 11.194957971572876
EPOCH 2
# current learning rate: 0.1
# Switched to train mode...
Epoch: [2][  0/391]	Time  0.191 ( 0.191)	Data  0.172 ( 0.172)	Loss 1.2051e+00 (1.2051e+00)	Acc@1  57.03 ( 57.03)	Acc@5  96.09 ( 96.09)
Epoch: [2][ 10/391]	Time  0.022 ( 0.040)	Data  0.001 ( 0.017)	Loss 1.3916e+00 (1.3699e+00)	Acc@1  55.47 ( 52.49)	Acc@5  92.97 ( 93.47)
Epoch: [2][ 20/391]	Time  0.018 ( 0.032)	Data  0.001 ( 0.010)	Loss 1.1729e+00 (1.3286e+00)	Acc@1  60.16 ( 53.72)	Acc@5  92.97 ( 93.68)
Epoch: [2][ 30/391]	Time  0.029 ( 0.030)	Data  0.001 ( 0.007)	Loss 1.1895e+00 (1.2992e+00)	Acc@1  57.81 ( 54.46)	Acc@5  96.88 ( 93.67)
Epoch: [2][ 40/391]	Time  0.016 ( 0.028)	Data  0.001 ( 0.006)	Loss 1.2207e+00 (1.2939e+00)	Acc@1  60.16 ( 54.65)	Acc@5  96.88 ( 94.09)
Epoch: [2][ 50/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.005)	Loss 1.2900e+00 (1.2876e+00)	Acc@1  58.59 ( 54.99)	Acc@5  96.09 ( 94.19)
Epoch: [2][ 60/391]	Time  0.038 ( 0.027)	Data  0.004 ( 0.005)	Loss 1.2139e+00 (1.2788e+00)	Acc@1  55.47 ( 55.16)	Acc@5  96.09 ( 94.30)
Epoch: [2][ 70/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.1875e+00 (1.2700e+00)	Acc@1  58.59 ( 55.47)	Acc@5  96.09 ( 94.18)
Epoch: [2][ 80/391]	Time  0.017 ( 0.026)	Data  0.000 ( 0.004)	Loss 1.2969e+00 (1.2746e+00)	Acc@1  55.47 ( 55.37)	Acc@5  94.53 ( 94.10)
Epoch: [2][ 90/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.1631e+00 (1.2702e+00)	Acc@1  62.50 ( 55.53)	Acc@5  95.31 ( 94.14)
Epoch: [2][100/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1787e+00 (1.2578e+00)	Acc@1  59.38 ( 56.04)	Acc@5  92.19 ( 94.27)
Epoch: [2][110/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.5439e+00 (1.2553e+00)	Acc@1  43.75 ( 56.10)	Acc@5  89.84 ( 94.31)
Epoch: [2][120/391]	Time  0.026 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.0732e+00 (1.2483e+00)	Acc@1  60.94 ( 56.24)	Acc@5  97.66 ( 94.47)
Epoch: [2][130/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.0654e+00 (1.2422e+00)	Acc@1  60.94 ( 56.46)	Acc@5  96.09 ( 94.58)
Epoch: [2][140/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.2285e+00 (1.2413e+00)	Acc@1  63.28 ( 56.57)	Acc@5  92.19 ( 94.56)
Epoch: [2][150/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 9.6289e-01 (1.2342e+00)	Acc@1  63.28 ( 56.71)	Acc@5  97.66 ( 94.62)
Epoch: [2][160/391]	Time  0.033 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.2070e+00 (1.2342e+00)	Acc@1  55.47 ( 56.64)	Acc@5  96.09 ( 94.70)
Epoch: [2][170/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.3145e+00 (1.2309e+00)	Acc@1  56.25 ( 56.84)	Acc@5  92.19 ( 94.68)
Epoch: [2][180/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.1738e+00 (1.2291e+00)	Acc@1  61.72 ( 56.96)	Acc@5  95.31 ( 94.68)
Epoch: [2][190/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.3223e+00 (1.2273e+00)	Acc@1  51.56 ( 56.95)	Acc@5  89.84 ( 94.72)
Epoch: [2][200/391]	Time  0.024 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.2402e+00 (1.2261e+00)	Acc@1  55.47 ( 57.08)	Acc@5  89.06 ( 94.70)
Epoch: [2][210/391]	Time  0.030 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.1699e+00 (1.2229e+00)	Acc@1  60.94 ( 57.21)	Acc@5  96.88 ( 94.72)
Epoch: [2][220/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.2520e+00 (1.2181e+00)	Acc@1  52.34 ( 57.41)	Acc@5  92.97 ( 94.74)
Epoch: [2][230/391]	Time  0.041 ( 0.025)	Data  0.005 ( 0.002)	Loss 1.2256e+00 (1.2130e+00)	Acc@1  64.84 ( 57.62)	Acc@5  93.75 ( 94.78)
Epoch: [2][240/391]	Time  0.032 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.1875e+00 (1.2111e+00)	Acc@1  60.16 ( 57.64)	Acc@5  92.19 ( 94.81)
Epoch: [2][250/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.2734e+00 (1.2071e+00)	Acc@1  55.47 ( 57.74)	Acc@5  93.75 ( 94.86)
Epoch: [2][260/391]	Time  0.033 ( 0.025)	Data  0.003 ( 0.002)	Loss 9.1602e-01 (1.2026e+00)	Acc@1  72.66 ( 57.93)	Acc@5  96.09 ( 94.88)
Epoch: [2][270/391]	Time  0.038 ( 0.025)	Data  0.002 ( 0.002)	Loss 1.1953e+00 (1.1992e+00)	Acc@1  59.38 ( 58.11)	Acc@5  95.31 ( 94.91)
Epoch: [2][280/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.002)	Loss 1.1973e+00 (1.1957e+00)	Acc@1  59.38 ( 58.28)	Acc@5  93.75 ( 94.93)
Epoch: [2][290/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 8.3447e-01 (1.1917e+00)	Acc@1  71.09 ( 58.42)	Acc@5  97.66 ( 94.97)
Epoch: [2][300/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.0400e+00 (1.1882e+00)	Acc@1  60.16 ( 58.53)	Acc@5  94.53 ( 95.01)
Epoch: [2][310/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.0908e+00 (1.1859e+00)	Acc@1  59.38 ( 58.61)	Acc@5  96.88 ( 95.01)
Epoch: [2][320/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.1680e+00 (1.1842e+00)	Acc@1  58.59 ( 58.68)	Acc@5  92.97 ( 95.01)
Epoch: [2][330/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.0820e+00 (1.1806e+00)	Acc@1  61.72 ( 58.78)	Acc@5  99.22 ( 95.06)
Epoch: [2][340/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 9.7168e-01 (1.1773e+00)	Acc@1  62.50 ( 58.86)	Acc@5  96.88 ( 95.10)
Epoch: [2][350/391]	Time  0.028 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.1035e+00 (1.1764e+00)	Acc@1  62.50 ( 58.93)	Acc@5  96.09 ( 95.11)
Epoch: [2][360/391]	Time  0.039 ( 0.025)	Data  0.001 ( 0.002)	Loss 9.5752e-01 (1.1743e+00)	Acc@1  65.62 ( 58.97)	Acc@5  98.44 ( 95.12)
Epoch: [2][370/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.0137e+00 (1.1723e+00)	Acc@1  68.75 ( 59.07)	Acc@5  96.88 ( 95.13)
Epoch: [2][380/391]	Time  0.021 ( 0.024)	Data  0.004 ( 0.002)	Loss 1.2939e+00 (1.1703e+00)	Acc@1  55.47 ( 59.16)	Acc@5  94.53 ( 95.16)
Epoch: [2][390/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.002)	Loss 8.8525e-01 (1.1678e+00)	Acc@1  65.00 ( 59.26)	Acc@5  98.75 ( 95.18)
## e[2] optimizer.zero_grad (sum) time: 0.10476899147033691
## e[2]       loss.backward (sum) time: 2.116554021835327
## e[2]      optimizer.step (sum) time: 0.8793070316314697
## epoch[2] training(only) time: 9.684356927871704
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 1.1182e+00 (1.1182e+00)	Acc@1  64.00 ( 64.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.008 ( 0.026)	Loss 9.8047e-01 (1.1502e+00)	Acc@1  63.00 ( 60.73)	Acc@5  96.00 ( 94.55)
Test: [ 20/100]	Time  0.011 ( 0.020)	Loss 9.8633e-01 (1.1384e+00)	Acc@1  67.00 ( 60.76)	Acc@5  97.00 ( 94.38)
Test: [ 30/100]	Time  0.014 ( 0.018)	Loss 1.0986e+00 (1.1671e+00)	Acc@1  60.00 ( 59.90)	Acc@5  96.00 ( 94.16)
Test: [ 40/100]	Time  0.016 ( 0.017)	Loss 1.1475e+00 (1.1665e+00)	Acc@1  59.00 ( 59.76)	Acc@5  92.00 ( 93.98)
Test: [ 50/100]	Time  0.008 ( 0.016)	Loss 1.2021e+00 (1.1538e+00)	Acc@1  62.00 ( 60.18)	Acc@5  93.00 ( 94.10)
Test: [ 60/100]	Time  0.009 ( 0.016)	Loss 1.0742e+00 (1.1588e+00)	Acc@1  63.00 ( 59.95)	Acc@5  97.00 ( 94.13)
Test: [ 70/100]	Time  0.008 ( 0.015)	Loss 1.1299e+00 (1.1606e+00)	Acc@1  63.00 ( 59.85)	Acc@5  93.00 ( 94.04)
Test: [ 80/100]	Time  0.012 ( 0.015)	Loss 1.3008e+00 (1.1591e+00)	Acc@1  48.00 ( 59.73)	Acc@5  93.00 ( 94.06)
Test: [ 90/100]	Time  0.020 ( 0.015)	Loss 1.1562e+00 (1.1626e+00)	Acc@1  56.00 ( 59.63)	Acc@5  98.00 ( 94.10)
 * Acc@1 59.340 Acc@5 93.990
### epoch[2] execution time: 11.299753904342651
EPOCH 3
# current learning rate: 0.1
# Switched to train mode...
Epoch: [3][  0/391]	Time  0.193 ( 0.193)	Data  0.172 ( 0.172)	Loss 8.8574e-01 (8.8574e-01)	Acc@1  69.53 ( 69.53)	Acc@5  99.22 ( 99.22)
Epoch: [3][ 10/391]	Time  0.027 ( 0.040)	Data  0.001 ( 0.017)	Loss 1.1846e+00 (1.0684e+00)	Acc@1  66.41 ( 62.86)	Acc@5  96.88 ( 96.24)
Epoch: [3][ 20/391]	Time  0.018 ( 0.032)	Data  0.001 ( 0.010)	Loss 1.0098e+00 (1.0515e+00)	Acc@1  60.94 ( 63.13)	Acc@5  98.44 ( 96.28)
Epoch: [3][ 30/391]	Time  0.034 ( 0.030)	Data  0.001 ( 0.008)	Loss 8.8672e-01 (1.0624e+00)	Acc@1  68.75 ( 63.31)	Acc@5  96.09 ( 96.19)
Epoch: [3][ 40/391]	Time  0.020 ( 0.028)	Data  0.000 ( 0.006)	Loss 1.0137e+00 (1.0764e+00)	Acc@1  62.50 ( 62.73)	Acc@5  97.66 ( 96.04)
Epoch: [3][ 50/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.005)	Loss 1.0654e+00 (1.0803e+00)	Acc@1  62.50 ( 62.39)	Acc@5  96.88 ( 96.16)
Epoch: [3][ 60/391]	Time  0.030 ( 0.027)	Data  0.001 ( 0.005)	Loss 1.0479e+00 (1.0718e+00)	Acc@1  63.28 ( 62.72)	Acc@5  96.09 ( 96.12)
Epoch: [3][ 70/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 9.1406e-01 (1.0593e+00)	Acc@1  64.06 ( 62.95)	Acc@5  95.31 ( 96.12)
Epoch: [3][ 80/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.2295e+00 (1.0600e+00)	Acc@1  58.59 ( 62.98)	Acc@5  93.75 ( 96.14)
Epoch: [3][ 90/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 9.5459e-01 (1.0610e+00)	Acc@1  66.41 ( 63.00)	Acc@5  96.88 ( 96.11)
Epoch: [3][100/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.004)	Loss 9.9121e-01 (1.0562e+00)	Acc@1  62.50 ( 63.18)	Acc@5  97.66 ( 96.19)
Epoch: [3][110/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 1.1846e+00 (1.0574e+00)	Acc@1  60.94 ( 63.21)	Acc@5  96.09 ( 96.17)
Epoch: [3][120/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.003)	Loss 7.8027e-01 (1.0491e+00)	Acc@1  75.78 ( 63.49)	Acc@5  97.66 ( 96.13)
Epoch: [3][130/391]	Time  0.040 ( 0.025)	Data  0.004 ( 0.003)	Loss 9.3359e-01 (1.0450e+00)	Acc@1  68.75 ( 63.66)	Acc@5  96.88 ( 96.09)
Epoch: [3][140/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.4961e-01 (1.0421e+00)	Acc@1  69.53 ( 63.72)	Acc@5  98.44 ( 96.14)
Epoch: [3][150/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.2744e+00 (1.0482e+00)	Acc@1  53.91 ( 63.50)	Acc@5  91.41 ( 96.07)
Epoch: [3][160/391]	Time  0.028 ( 0.025)	Data  0.002 ( 0.003)	Loss 1.1055e+00 (1.0448e+00)	Acc@1  60.94 ( 63.57)	Acc@5  96.88 ( 96.17)
Epoch: [3][170/391]	Time  0.018 ( 0.025)	Data  0.002 ( 0.003)	Loss 1.2109e+00 (1.0445e+00)	Acc@1  54.69 ( 63.60)	Acc@5  95.31 ( 96.13)
Epoch: [3][180/391]	Time  0.020 ( 0.025)	Data  0.003 ( 0.003)	Loss 9.3555e-01 (1.0402e+00)	Acc@1  69.53 ( 63.83)	Acc@5  95.31 ( 96.12)
Epoch: [3][190/391]	Time  0.038 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.0703e+00 (1.0367e+00)	Acc@1  65.62 ( 64.03)	Acc@5  94.53 ( 96.12)
Epoch: [3][200/391]	Time  0.040 ( 0.025)	Data  0.006 ( 0.003)	Loss 1.0205e+00 (1.0354e+00)	Acc@1  63.28 ( 64.07)	Acc@5  97.66 ( 96.16)
Epoch: [3][210/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 9.6973e-01 (1.0337e+00)	Acc@1  64.84 ( 64.06)	Acc@5  94.53 ( 96.18)
Epoch: [3][220/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.2324e+00 (1.0328e+00)	Acc@1  58.59 ( 64.09)	Acc@5  97.66 ( 96.18)
Epoch: [3][230/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 9.4580e-01 (1.0300e+00)	Acc@1  71.09 ( 64.18)	Acc@5 100.00 ( 96.21)
Epoch: [3][240/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.003)	Loss 9.9365e-01 (1.0262e+00)	Acc@1  71.09 ( 64.37)	Acc@5  94.53 ( 96.19)
Epoch: [3][250/391]	Time  0.025 ( 0.025)	Data  0.000 ( 0.003)	Loss 1.0664e+00 (1.0243e+00)	Acc@1  61.72 ( 64.43)	Acc@5  97.66 ( 96.20)
Epoch: [3][260/391]	Time  0.018 ( 0.024)	Data  0.002 ( 0.002)	Loss 8.5205e-01 (1.0246e+00)	Acc@1  73.44 ( 64.42)	Acc@5  93.75 ( 96.19)
Epoch: [3][270/391]	Time  0.037 ( 0.025)	Data  0.003 ( 0.002)	Loss 8.7695e-01 (1.0235e+00)	Acc@1  71.09 ( 64.43)	Acc@5  97.66 ( 96.20)
Epoch: [3][280/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 9.9219e-01 (1.0226e+00)	Acc@1  65.62 ( 64.45)	Acc@5  96.88 ( 96.23)
Epoch: [3][290/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.0459e+00 (1.0227e+00)	Acc@1  62.50 ( 64.44)	Acc@5  96.88 ( 96.21)
Epoch: [3][300/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 9.5605e-01 (1.0214e+00)	Acc@1  64.06 ( 64.48)	Acc@5  97.66 ( 96.24)
Epoch: [3][310/391]	Time  0.028 ( 0.025)	Data  0.001 ( 0.002)	Loss 9.1504e-01 (1.0191e+00)	Acc@1  65.62 ( 64.55)	Acc@5  95.31 ( 96.23)
Epoch: [3][320/391]	Time  0.028 ( 0.025)	Data  0.005 ( 0.002)	Loss 9.3799e-01 (1.0180e+00)	Acc@1  64.84 ( 64.59)	Acc@5  96.88 ( 96.29)
Epoch: [3][330/391]	Time  0.028 ( 0.025)	Data  0.001 ( 0.002)	Loss 8.4521e-01 (1.0161e+00)	Acc@1  69.53 ( 64.67)	Acc@5  97.66 ( 96.33)
Epoch: [3][340/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 8.8477e-01 (1.0139e+00)	Acc@1  66.41 ( 64.76)	Acc@5  96.88 ( 96.34)
Epoch: [3][350/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 7.1338e-01 (1.0119e+00)	Acc@1  75.00 ( 64.82)	Acc@5  98.44 ( 96.35)
Epoch: [3][360/391]	Time  0.030 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.0479e+00 (1.0097e+00)	Acc@1  64.84 ( 64.92)	Acc@5  96.09 ( 96.36)
Epoch: [3][370/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 8.4717e-01 (1.0080e+00)	Acc@1  67.97 ( 64.97)	Acc@5  98.44 ( 96.36)
Epoch: [3][380/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 9.7949e-01 (1.0064e+00)	Acc@1  70.31 ( 65.01)	Acc@5  94.53 ( 96.36)
Epoch: [3][390/391]	Time  0.014 ( 0.024)	Data  0.001 ( 0.002)	Loss 9.5605e-01 (1.0046e+00)	Acc@1  65.00 ( 65.06)	Acc@5  96.25 ( 96.39)
## e[3] optimizer.zero_grad (sum) time: 0.10495948791503906
## e[3]       loss.backward (sum) time: 2.1640567779541016
## e[3]      optimizer.step (sum) time: 0.8906073570251465
## epoch[3] training(only) time: 9.654894828796387
# Switched to evaluate mode...
Test: [  0/100]	Time  0.162 ( 0.162)	Loss 7.9785e-01 (7.9785e-01)	Acc@1  72.00 ( 72.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.008 ( 0.027)	Loss 8.7305e-01 (8.7251e-01)	Acc@1  66.00 ( 69.73)	Acc@5 100.00 ( 98.00)
Test: [ 20/100]	Time  0.016 ( 0.020)	Loss 9.8535e-01 (8.6819e-01)	Acc@1  64.00 ( 69.38)	Acc@5  98.00 ( 97.67)
Test: [ 30/100]	Time  0.008 ( 0.018)	Loss 8.7939e-01 (8.8576e-01)	Acc@1  69.00 ( 68.74)	Acc@5  99.00 ( 97.32)
Test: [ 40/100]	Time  0.017 ( 0.017)	Loss 9.8389e-01 (8.7975e-01)	Acc@1  64.00 ( 69.02)	Acc@5  97.00 ( 97.37)
Test: [ 50/100]	Time  0.008 ( 0.016)	Loss 8.6133e-01 (8.7191e-01)	Acc@1  66.00 ( 69.49)	Acc@5  96.00 ( 97.29)
Test: [ 60/100]	Time  0.019 ( 0.016)	Loss 9.9023e-01 (8.7903e-01)	Acc@1  62.00 ( 69.31)	Acc@5  98.00 ( 97.28)
Test: [ 70/100]	Time  0.020 ( 0.015)	Loss 8.7305e-01 (8.8063e-01)	Acc@1  70.00 ( 69.37)	Acc@5  96.00 ( 97.31)
Test: [ 80/100]	Time  0.008 ( 0.015)	Loss 9.0088e-01 (8.8068e-01)	Acc@1  65.00 ( 69.33)	Acc@5  96.00 ( 97.31)
Test: [ 90/100]	Time  0.008 ( 0.015)	Loss 9.3018e-01 (8.8067e-01)	Acc@1  68.00 ( 69.37)	Acc@5  98.00 ( 97.36)
 * Acc@1 69.270 Acc@5 97.310
### epoch[3] execution time: 11.264145135879517
EPOCH 4
# current learning rate: 0.1
# Switched to train mode...
Epoch: [4][  0/391]	Time  0.252 ( 0.252)	Data  0.229 ( 0.229)	Loss 7.4170e-01 (7.4170e-01)	Acc@1  77.34 ( 77.34)	Acc@5  96.09 ( 96.09)
Epoch: [4][ 10/391]	Time  0.019 ( 0.044)	Data  0.001 ( 0.023)	Loss 1.0039e+00 (8.7487e-01)	Acc@1  64.06 ( 70.03)	Acc@5  96.88 ( 96.52)
Epoch: [4][ 20/391]	Time  0.022 ( 0.035)	Data  0.001 ( 0.013)	Loss 1.1582e+00 (9.2267e-01)	Acc@1  60.16 ( 68.45)	Acc@5  96.88 ( 96.39)
Epoch: [4][ 30/391]	Time  0.018 ( 0.031)	Data  0.001 ( 0.009)	Loss 8.7061e-01 (9.2800e-01)	Acc@1  65.62 ( 67.69)	Acc@5 100.00 ( 96.67)
Epoch: [4][ 40/391]	Time  0.026 ( 0.030)	Data  0.001 ( 0.007)	Loss 8.4766e-01 (9.3762e-01)	Acc@1  71.09 ( 67.53)	Acc@5  96.88 ( 96.65)
Epoch: [4][ 50/391]	Time  0.037 ( 0.028)	Data  0.002 ( 0.006)	Loss 8.5010e-01 (9.3081e-01)	Acc@1  71.09 ( 67.98)	Acc@5  98.44 ( 96.80)
Epoch: [4][ 60/391]	Time  0.019 ( 0.028)	Data  0.001 ( 0.005)	Loss 8.1006e-01 (9.2123e-01)	Acc@1  76.56 ( 68.24)	Acc@5  96.09 ( 96.96)
Epoch: [4][ 70/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.005)	Loss 8.8379e-01 (9.1044e-01)	Acc@1  74.22 ( 68.72)	Acc@5  96.88 ( 97.07)
Epoch: [4][ 80/391]	Time  0.017 ( 0.027)	Data  0.000 ( 0.004)	Loss 8.1836e-01 (9.0957e-01)	Acc@1  73.44 ( 68.69)	Acc@5  96.88 ( 97.10)
Epoch: [4][ 90/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 8.5449e-01 (9.0908e-01)	Acc@1  69.53 ( 68.81)	Acc@5  97.66 ( 97.08)
Epoch: [4][100/391]	Time  0.018 ( 0.026)	Data  0.002 ( 0.004)	Loss 7.8613e-01 (9.0836e-01)	Acc@1  73.44 ( 68.68)	Acc@5  98.44 ( 97.11)
Epoch: [4][110/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 7.3877e-01 (9.0498e-01)	Acc@1  75.00 ( 68.83)	Acc@5  99.22 ( 97.11)
Epoch: [4][120/391]	Time  0.019 ( 0.026)	Data  0.002 ( 0.004)	Loss 9.0283e-01 (9.0236e-01)	Acc@1  64.84 ( 68.90)	Acc@5  97.66 ( 97.18)
Epoch: [4][130/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.8086e-01 (9.0548e-01)	Acc@1  69.53 ( 68.86)	Acc@5  96.09 ( 97.05)
Epoch: [4][140/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.9150e-01 (9.0122e-01)	Acc@1  70.31 ( 68.96)	Acc@5  97.66 ( 97.07)
Epoch: [4][150/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 9.6924e-01 (9.0142e-01)	Acc@1  68.75 ( 69.02)	Acc@5  96.88 ( 97.06)
Epoch: [4][160/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.003)	Loss 9.8242e-01 (9.0440e-01)	Acc@1  64.84 ( 68.91)	Acc@5  97.66 ( 97.08)
Epoch: [4][170/391]	Time  0.029 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.1924e+00 (9.0763e-01)	Acc@1  65.62 ( 68.93)	Acc@5  96.09 ( 97.06)
Epoch: [4][180/391]	Time  0.026 ( 0.025)	Data  0.002 ( 0.003)	Loss 9.9023e-01 (9.0656e-01)	Acc@1  67.97 ( 68.94)	Acc@5  94.53 ( 97.07)
Epoch: [4][190/391]	Time  0.017 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.2617e-01 (9.0583e-01)	Acc@1  71.09 ( 69.06)	Acc@5  97.66 ( 97.10)
Epoch: [4][200/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 9.6875e-01 (9.0478e-01)	Acc@1  70.31 ( 69.13)	Acc@5  94.53 ( 97.11)
Epoch: [4][210/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 9.6826e-01 (9.0431e-01)	Acc@1  67.19 ( 69.14)	Acc@5  98.44 ( 97.10)
Epoch: [4][220/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.0420e+00 (9.0310e-01)	Acc@1  67.19 ( 69.18)	Acc@5  94.53 ( 97.10)
Epoch: [4][230/391]	Time  0.026 ( 0.025)	Data  0.002 ( 0.003)	Loss 7.8613e-01 (8.9867e-01)	Acc@1  72.66 ( 69.31)	Acc@5  96.88 ( 97.12)
Epoch: [4][240/391]	Time  0.031 ( 0.025)	Data  0.001 ( 0.003)	Loss 7.3975e-01 (8.9691e-01)	Acc@1  71.88 ( 69.31)	Acc@5 100.00 ( 97.14)
Epoch: [4][250/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 7.9004e-01 (8.9590e-01)	Acc@1  75.78 ( 69.31)	Acc@5  97.66 ( 97.14)
Epoch: [4][260/391]	Time  0.036 ( 0.025)	Data  0.002 ( 0.003)	Loss 8.4229e-01 (8.9351e-01)	Acc@1  75.78 ( 69.36)	Acc@5  98.44 ( 97.17)
Epoch: [4][270/391]	Time  0.037 ( 0.025)	Data  0.000 ( 0.003)	Loss 9.4824e-01 (8.9346e-01)	Acc@1  67.97 ( 69.38)	Acc@5  97.66 ( 97.18)
Epoch: [4][280/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.5791e-01 (8.9163e-01)	Acc@1  72.66 ( 69.45)	Acc@5  94.53 ( 97.21)
Epoch: [4][290/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 7.5049e-01 (8.8854e-01)	Acc@1  74.22 ( 69.50)	Acc@5  97.66 ( 97.23)
Epoch: [4][300/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.0156e+00 (8.8564e-01)	Acc@1  66.41 ( 69.62)	Acc@5  96.09 ( 97.24)
Epoch: [4][310/391]	Time  0.033 ( 0.025)	Data  0.004 ( 0.002)	Loss 1.0068e+00 (8.8581e-01)	Acc@1  68.75 ( 69.62)	Acc@5  97.66 ( 97.24)
Epoch: [4][320/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 6.7090e-01 (8.8295e-01)	Acc@1  83.59 ( 69.76)	Acc@5  97.66 ( 97.26)
Epoch: [4][330/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 8.9648e-01 (8.8151e-01)	Acc@1  71.88 ( 69.83)	Acc@5  96.09 ( 97.26)
Epoch: [4][340/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.002)	Loss 7.0947e-01 (8.7948e-01)	Acc@1  77.34 ( 69.92)	Acc@5  99.22 ( 97.28)
Epoch: [4][350/391]	Time  0.017 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.0498e+00 (8.8144e-01)	Acc@1  62.50 ( 69.83)	Acc@5  94.53 ( 97.26)
Epoch: [4][360/391]	Time  0.038 ( 0.025)	Data  0.000 ( 0.002)	Loss 7.6221e-01 (8.8056e-01)	Acc@1  76.56 ( 69.86)	Acc@5  98.44 ( 97.27)
Epoch: [4][370/391]	Time  0.033 ( 0.025)	Data  0.001 ( 0.002)	Loss 9.2285e-01 (8.8040e-01)	Acc@1  70.31 ( 69.91)	Acc@5  96.88 ( 97.26)
Epoch: [4][380/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 7.7100e-01 (8.7849e-01)	Acc@1  71.88 ( 69.99)	Acc@5  96.88 ( 97.29)
Epoch: [4][390/391]	Time  0.016 ( 0.025)	Data  0.000 ( 0.002)	Loss 8.1836e-01 (8.7793e-01)	Acc@1  72.50 ( 70.04)	Acc@5  97.50 ( 97.28)
## e[4] optimizer.zero_grad (sum) time: 0.10547471046447754
## e[4]       loss.backward (sum) time: 2.14249324798584
## e[4]      optimizer.step (sum) time: 0.8871204853057861
## epoch[4] training(only) time: 9.722541809082031
# Switched to evaluate mode...
Test: [  0/100]	Time  0.150 ( 0.150)	Loss 7.8906e-01 (7.8906e-01)	Acc@1  73.00 ( 73.00)	Acc@5  96.00 ( 96.00)
Test: [ 10/100]	Time  0.008 ( 0.026)	Loss 7.3486e-01 (7.9150e-01)	Acc@1  75.00 ( 72.55)	Acc@5  99.00 ( 97.91)
Test: [ 20/100]	Time  0.015 ( 0.020)	Loss 8.8135e-01 (8.0573e-01)	Acc@1  66.00 ( 72.57)	Acc@5  99.00 ( 97.67)
Test: [ 30/100]	Time  0.009 ( 0.018)	Loss 9.3750e-01 (8.1877e-01)	Acc@1  67.00 ( 72.39)	Acc@5  97.00 ( 97.58)
Test: [ 40/100]	Time  0.009 ( 0.017)	Loss 8.7939e-01 (8.1853e-01)	Acc@1  69.00 ( 72.22)	Acc@5  96.00 ( 97.51)
Test: [ 50/100]	Time  0.008 ( 0.016)	Loss 7.3926e-01 (8.1332e-01)	Acc@1  72.00 ( 72.22)	Acc@5  98.00 ( 97.67)
Test: [ 60/100]	Time  0.017 ( 0.016)	Loss 7.8564e-01 (8.2192e-01)	Acc@1  70.00 ( 72.10)	Acc@5  99.00 ( 97.74)
Test: [ 70/100]	Time  0.008 ( 0.015)	Loss 7.3340e-01 (8.2443e-01)	Acc@1  79.00 ( 72.17)	Acc@5  96.00 ( 97.66)
Test: [ 80/100]	Time  0.020 ( 0.015)	Loss 7.8613e-01 (8.2261e-01)	Acc@1  69.00 ( 72.30)	Acc@5  97.00 ( 97.63)
Test: [ 90/100]	Time  0.016 ( 0.015)	Loss 7.4268e-01 (8.2487e-01)	Acc@1  75.00 ( 72.29)	Acc@5  99.00 ( 97.60)
 * Acc@1 72.380 Acc@5 97.570
### epoch[4] execution time: 11.289245128631592
EPOCH 5
# current learning rate: 0.1
# Switched to train mode...
Epoch: [5][  0/391]	Time  0.187 ( 0.187)	Data  0.155 ( 0.155)	Loss 6.8652e-01 (6.8652e-01)	Acc@1  78.12 ( 78.12)	Acc@5  98.44 ( 98.44)
Epoch: [5][ 10/391]	Time  0.019 ( 0.038)	Data  0.001 ( 0.016)	Loss 8.5840e-01 (8.2715e-01)	Acc@1  71.88 ( 72.80)	Acc@5  96.88 ( 97.51)
Epoch: [5][ 20/391]	Time  0.018 ( 0.032)	Data  0.001 ( 0.009)	Loss 9.5361e-01 (8.4466e-01)	Acc@1  65.62 ( 71.54)	Acc@5  98.44 ( 97.54)
Epoch: [5][ 30/391]	Time  0.033 ( 0.029)	Data  0.001 ( 0.007)	Loss 8.5791e-01 (8.4002e-01)	Acc@1  67.19 ( 71.77)	Acc@5  98.44 ( 97.66)
Epoch: [5][ 40/391]	Time  0.021 ( 0.028)	Data  0.002 ( 0.006)	Loss 7.8418e-01 (8.3758e-01)	Acc@1  74.22 ( 72.16)	Acc@5  98.44 ( 97.58)
Epoch: [5][ 50/391]	Time  0.017 ( 0.027)	Data  0.000 ( 0.005)	Loss 7.1289e-01 (8.2733e-01)	Acc@1  78.91 ( 72.29)	Acc@5  96.09 ( 97.55)
Epoch: [5][ 60/391]	Time  0.026 ( 0.026)	Data  0.002 ( 0.004)	Loss 7.1533e-01 (8.1838e-01)	Acc@1  74.22 ( 72.54)	Acc@5  98.44 ( 97.55)
Epoch: [5][ 70/391]	Time  0.032 ( 0.026)	Data  0.005 ( 0.004)	Loss 7.4658e-01 (8.1167e-01)	Acc@1  75.78 ( 72.68)	Acc@5  98.44 ( 97.63)
Epoch: [5][ 80/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 6.6064e-01 (8.0169e-01)	Acc@1  78.91 ( 73.04)	Acc@5  97.66 ( 97.71)
Epoch: [5][ 90/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.004)	Loss 6.6943e-01 (7.9359e-01)	Acc@1  77.34 ( 73.22)	Acc@5  97.66 ( 97.74)
Epoch: [5][100/391]	Time  0.039 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.4424e-01 (7.9709e-01)	Acc@1  71.88 ( 73.11)	Acc@5  98.44 ( 97.71)
Epoch: [5][110/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 9.9365e-01 (8.0111e-01)	Acc@1  68.75 ( 72.97)	Acc@5  94.53 ( 97.66)
Epoch: [5][120/391]	Time  0.035 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.4375e-01 (7.9845e-01)	Acc@1  71.88 ( 73.04)	Acc@5  97.66 ( 97.67)
Epoch: [5][130/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 7.1289e-01 (7.9509e-01)	Acc@1  72.66 ( 73.15)	Acc@5 100.00 ( 97.70)
Epoch: [5][140/391]	Time  0.040 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.8896e-01 (7.9183e-01)	Acc@1  71.88 ( 73.22)	Acc@5 100.00 ( 97.73)
Epoch: [5][150/391]	Time  0.018 ( 0.025)	Data  0.002 ( 0.003)	Loss 7.9688e-01 (7.9227e-01)	Acc@1  66.41 ( 73.14)	Acc@5  98.44 ( 97.71)
Epoch: [5][160/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.2764e-01 (7.9320e-01)	Acc@1  71.88 ( 73.07)	Acc@5  96.88 ( 97.67)
Epoch: [5][170/391]	Time  0.037 ( 0.025)	Data  0.008 ( 0.003)	Loss 9.3408e-01 (7.9415e-01)	Acc@1  71.09 ( 73.02)	Acc@5  95.31 ( 97.66)
Epoch: [5][180/391]	Time  0.018 ( 0.025)	Data  0.002 ( 0.003)	Loss 8.0713e-01 (7.9345e-01)	Acc@1  76.56 ( 73.08)	Acc@5  98.44 ( 97.66)
Epoch: [5][190/391]	Time  0.043 ( 0.025)	Data  0.001 ( 0.003)	Loss 7.9883e-01 (7.9502e-01)	Acc@1  75.00 ( 73.07)	Acc@5  96.88 ( 97.64)
Epoch: [5][200/391]	Time  0.040 ( 0.025)	Data  0.002 ( 0.003)	Loss 9.2383e-01 (7.9350e-01)	Acc@1  68.75 ( 73.06)	Acc@5  95.31 ( 97.64)
Epoch: [5][210/391]	Time  0.035 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.3301e-01 (7.8927e-01)	Acc@1  70.31 ( 73.19)	Acc@5  98.44 ( 97.65)
Epoch: [5][220/391]	Time  0.027 ( 0.025)	Data  0.003 ( 0.003)	Loss 7.6221e-01 (7.8752e-01)	Acc@1  79.69 ( 73.29)	Acc@5  96.88 ( 97.65)
Epoch: [5][230/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.003)	Loss 7.3535e-01 (7.8658e-01)	Acc@1  75.78 ( 73.34)	Acc@5  98.44 ( 97.64)
Epoch: [5][240/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 9.9072e-01 (7.8686e-01)	Acc@1  64.84 ( 73.37)	Acc@5  96.09 ( 97.62)
Epoch: [5][250/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.002)	Loss 7.4170e-01 (7.8517e-01)	Acc@1  74.22 ( 73.43)	Acc@5  99.22 ( 97.64)
Epoch: [5][260/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 7.4805e-01 (7.8459e-01)	Acc@1  71.88 ( 73.47)	Acc@5  97.66 ( 97.64)
Epoch: [5][270/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 7.7783e-01 (7.8508e-01)	Acc@1  73.44 ( 73.49)	Acc@5  98.44 ( 97.64)
Epoch: [5][280/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 8.8086e-01 (7.8528e-01)	Acc@1  67.97 ( 73.48)	Acc@5  98.44 ( 97.64)
Epoch: [5][290/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 7.1045e-01 (7.8622e-01)	Acc@1  74.22 ( 73.45)	Acc@5  97.66 ( 97.61)
Epoch: [5][300/391]	Time  0.037 ( 0.025)	Data  0.000 ( 0.002)	Loss 8.1006e-01 (7.8579e-01)	Acc@1  73.44 ( 73.45)	Acc@5  97.66 ( 97.61)
Epoch: [5][310/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.002)	Loss 6.1475e-01 (7.8499e-01)	Acc@1  78.91 ( 73.47)	Acc@5  99.22 ( 97.61)
Epoch: [5][320/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 7.4609e-01 (7.8422e-01)	Acc@1  75.00 ( 73.50)	Acc@5  98.44 ( 97.63)
Epoch: [5][330/391]	Time  0.019 ( 0.024)	Data  0.000 ( 0.002)	Loss 6.3330e-01 (7.8252e-01)	Acc@1  80.47 ( 73.56)	Acc@5  98.44 ( 97.63)
Epoch: [5][340/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 8.2568e-01 (7.8217e-01)	Acc@1  71.09 ( 73.54)	Acc@5  95.31 ( 97.63)
Epoch: [5][350/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 7.8613e-01 (7.8175e-01)	Acc@1  76.56 ( 73.57)	Acc@5  97.66 ( 97.64)
Epoch: [5][360/391]	Time  0.032 ( 0.025)	Data  0.001 ( 0.002)	Loss 8.2227e-01 (7.7971e-01)	Acc@1  72.66 ( 73.65)	Acc@5  99.22 ( 97.66)
Epoch: [5][370/391]	Time  0.021 ( 0.024)	Data  0.001 ( 0.002)	Loss 8.2275e-01 (7.7984e-01)	Acc@1  72.66 ( 73.66)	Acc@5  96.88 ( 97.64)
Epoch: [5][380/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 6.5967e-01 (7.7880e-01)	Acc@1  76.56 ( 73.68)	Acc@5 100.00 ( 97.66)
Epoch: [5][390/391]	Time  0.015 ( 0.024)	Data  0.000 ( 0.002)	Loss 9.8291e-01 (7.7819e-01)	Acc@1  67.50 ( 73.71)	Acc@5  96.25 ( 97.67)
## e[5] optimizer.zero_grad (sum) time: 0.10526490211486816
## e[5]       loss.backward (sum) time: 2.1597139835357666
## e[5]      optimizer.step (sum) time: 0.891202449798584
## epoch[5] training(only) time: 9.646456241607666
# Switched to evaluate mode...
Test: [  0/100]	Time  0.164 ( 0.164)	Loss 1.0293e+00 (1.0293e+00)	Acc@1  67.00 ( 67.00)	Acc@5  96.00 ( 96.00)
Test: [ 10/100]	Time  0.016 ( 0.027)	Loss 9.0283e-01 (8.3523e-01)	Acc@1  75.00 ( 75.00)	Acc@5  94.00 ( 97.09)
Test: [ 20/100]	Time  0.008 ( 0.019)	Loss 8.9453e-01 (8.2385e-01)	Acc@1  67.00 ( 74.33)	Acc@5  97.00 ( 96.90)
Test: [ 30/100]	Time  0.017 ( 0.018)	Loss 8.1006e-01 (8.2102e-01)	Acc@1  69.00 ( 73.77)	Acc@5  98.00 ( 97.13)
Test: [ 40/100]	Time  0.013 ( 0.017)	Loss 8.5059e-01 (8.1555e-01)	Acc@1  74.00 ( 73.78)	Acc@5  97.00 ( 97.27)
Test: [ 50/100]	Time  0.008 ( 0.016)	Loss 7.4365e-01 (7.9887e-01)	Acc@1  75.00 ( 74.25)	Acc@5  98.00 ( 97.45)
Test: [ 60/100]	Time  0.016 ( 0.016)	Loss 8.4473e-01 (8.1197e-01)	Acc@1  68.00 ( 73.67)	Acc@5  98.00 ( 97.46)
Test: [ 70/100]	Time  0.017 ( 0.015)	Loss 8.1934e-01 (8.1493e-01)	Acc@1  76.00 ( 73.69)	Acc@5  95.00 ( 97.51)
Test: [ 80/100]	Time  0.008 ( 0.015)	Loss 6.7285e-01 (8.0727e-01)	Acc@1  78.00 ( 73.94)	Acc@5 100.00 ( 97.57)
Test: [ 90/100]	Time  0.008 ( 0.015)	Loss 6.9141e-01 (8.0360e-01)	Acc@1  78.00 ( 74.01)	Acc@5 100.00 ( 97.63)
 * Acc@1 74.060 Acc@5 97.610
### epoch[5] execution time: 11.230664491653442
EPOCH 6
# current learning rate: 0.1
# Switched to train mode...
Epoch: [6][  0/391]	Time  0.206 ( 0.206)	Data  0.182 ( 0.182)	Loss 6.2451e-01 (6.2451e-01)	Acc@1  79.69 ( 79.69)	Acc@5  98.44 ( 98.44)
Epoch: [6][ 10/391]	Time  0.027 ( 0.040)	Data  0.001 ( 0.018)	Loss 8.2861e-01 (7.3335e-01)	Acc@1  74.22 ( 76.42)	Acc@5  96.09 ( 97.23)
Epoch: [6][ 20/391]	Time  0.018 ( 0.032)	Data  0.001 ( 0.010)	Loss 6.5869e-01 (7.1545e-01)	Acc@1  79.69 ( 76.38)	Acc@5  99.22 ( 97.58)
Epoch: [6][ 30/391]	Time  0.018 ( 0.030)	Data  0.001 ( 0.008)	Loss 6.8555e-01 (7.1121e-01)	Acc@1  75.00 ( 76.49)	Acc@5  99.22 ( 97.66)
Epoch: [6][ 40/391]	Time  0.018 ( 0.028)	Data  0.001 ( 0.006)	Loss 6.5771e-01 (7.0370e-01)	Acc@1  81.25 ( 76.75)	Acc@5  98.44 ( 97.68)
Epoch: [6][ 50/391]	Time  0.028 ( 0.027)	Data  0.001 ( 0.005)	Loss 7.7637e-01 (6.9755e-01)	Acc@1  72.66 ( 76.70)	Acc@5  97.66 ( 97.84)
Epoch: [6][ 60/391]	Time  0.040 ( 0.027)	Data  0.001 ( 0.005)	Loss 7.5244e-01 (7.0637e-01)	Acc@1  74.22 ( 76.51)	Acc@5  97.66 ( 97.72)
Epoch: [6][ 70/391]	Time  0.038 ( 0.026)	Data  0.001 ( 0.004)	Loss 6.6309e-01 (7.0429e-01)	Acc@1  74.22 ( 76.50)	Acc@5  98.44 ( 97.81)
Epoch: [6][ 80/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 7.1289e-01 (7.0743e-01)	Acc@1  79.69 ( 76.36)	Acc@5  98.44 ( 97.84)
Epoch: [6][ 90/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 6.7578e-01 (7.1074e-01)	Acc@1  75.78 ( 76.18)	Acc@5  99.22 ( 97.90)
Epoch: [6][100/391]	Time  0.038 ( 0.026)	Data  0.001 ( 0.004)	Loss 8.2422e-01 (7.1331e-01)	Acc@1  75.78 ( 76.20)	Acc@5  97.66 ( 97.88)
Epoch: [6][110/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 8.6523e-01 (7.1875e-01)	Acc@1  65.62 ( 75.99)	Acc@5 100.00 ( 97.91)
Epoch: [6][120/391]	Time  0.029 ( 0.025)	Data  0.000 ( 0.003)	Loss 1.0088e+00 (7.2212e-01)	Acc@1  67.97 ( 75.86)	Acc@5  94.53 ( 97.91)
Epoch: [6][130/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 8.1982e-01 (7.1995e-01)	Acc@1  74.22 ( 75.86)	Acc@5  97.66 ( 97.89)
Epoch: [6][140/391]	Time  0.017 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.3916e-01 (7.1783e-01)	Acc@1  78.12 ( 75.91)	Acc@5  97.66 ( 97.88)
Epoch: [6][150/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.003)	Loss 7.4561e-01 (7.1851e-01)	Acc@1  70.31 ( 75.82)	Acc@5 100.00 ( 97.89)
Epoch: [6][160/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.3721e-01 (7.1637e-01)	Acc@1  76.56 ( 75.88)	Acc@5 100.00 ( 97.93)
Epoch: [6][170/391]	Time  0.038 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.4590e-01 (7.1595e-01)	Acc@1  84.38 ( 75.95)	Acc@5  98.44 ( 97.96)
Epoch: [6][180/391]	Time  0.025 ( 0.025)	Data  0.000 ( 0.003)	Loss 6.8896e-01 (7.1521e-01)	Acc@1  74.22 ( 76.00)	Acc@5  99.22 ( 97.97)
Epoch: [6][190/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.9678e-01 (7.1588e-01)	Acc@1  78.91 ( 75.99)	Acc@5  98.44 ( 97.98)
Epoch: [6][200/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 7.9443e-01 (7.1803e-01)	Acc@1  73.44 ( 75.92)	Acc@5  98.44 ( 97.95)
Epoch: [6][210/391]	Time  0.045 ( 0.025)	Data  0.005 ( 0.003)	Loss 7.5977e-01 (7.1950e-01)	Acc@1  73.44 ( 75.93)	Acc@5  98.44 ( 97.94)
Epoch: [6][220/391]	Time  0.031 ( 0.025)	Data  0.001 ( 0.003)	Loss 7.8076e-01 (7.1899e-01)	Acc@1  76.56 ( 75.97)	Acc@5  96.88 ( 97.95)
Epoch: [6][230/391]	Time  0.033 ( 0.025)	Data  0.004 ( 0.003)	Loss 6.9434e-01 (7.2059e-01)	Acc@1  75.78 ( 76.00)	Acc@5  96.88 ( 97.95)
Epoch: [6][240/391]	Time  0.024 ( 0.025)	Data  0.002 ( 0.003)	Loss 7.5146e-01 (7.2066e-01)	Acc@1  72.66 ( 75.99)	Acc@5  97.66 ( 97.95)
Epoch: [6][250/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 7.9150e-01 (7.2035e-01)	Acc@1  70.31 ( 76.01)	Acc@5  96.88 ( 97.94)
Epoch: [6][260/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 7.7246e-01 (7.2160e-01)	Acc@1  73.44 ( 75.96)	Acc@5  98.44 ( 97.91)
Epoch: [6][270/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.9043e-01 (7.2098e-01)	Acc@1  77.34 ( 75.93)	Acc@5  96.88 ( 97.92)
Epoch: [6][280/391]	Time  0.037 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.4697e-01 (7.1852e-01)	Acc@1  80.47 ( 76.02)	Acc@5  99.22 ( 97.95)
Epoch: [6][290/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 7.2559e-01 (7.1946e-01)	Acc@1  81.25 ( 76.07)	Acc@5  98.44 ( 97.96)
Epoch: [6][300/391]	Time  0.028 ( 0.025)	Data  0.001 ( 0.002)	Loss 6.6553e-01 (7.1968e-01)	Acc@1  80.47 ( 76.07)	Acc@5  96.88 ( 97.97)
Epoch: [6][310/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.002)	Loss 6.7529e-01 (7.2032e-01)	Acc@1  77.34 ( 76.05)	Acc@5  98.44 ( 97.98)
Epoch: [6][320/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 7.3828e-01 (7.2026e-01)	Acc@1  75.00 ( 76.08)	Acc@5  96.88 ( 97.97)
Epoch: [6][330/391]	Time  0.035 ( 0.025)	Data  0.005 ( 0.002)	Loss 8.7842e-01 (7.2149e-01)	Acc@1  70.31 ( 76.04)	Acc@5  96.88 ( 97.96)
Epoch: [6][340/391]	Time  0.037 ( 0.025)	Data  0.001 ( 0.002)	Loss 8.2812e-01 (7.2155e-01)	Acc@1  72.66 ( 76.02)	Acc@5  99.22 ( 97.96)
Epoch: [6][350/391]	Time  0.036 ( 0.025)	Data  0.001 ( 0.002)	Loss 6.9238e-01 (7.2010e-01)	Acc@1  78.12 ( 76.09)	Acc@5  96.88 ( 97.97)
Epoch: [6][360/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 6.9385e-01 (7.1904e-01)	Acc@1  78.12 ( 76.14)	Acc@5  98.44 ( 97.98)
Epoch: [6][370/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.6924e-01 (7.1766e-01)	Acc@1  87.50 ( 76.21)	Acc@5  98.44 ( 97.98)
Epoch: [6][380/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 5.8398e-01 (7.1657e-01)	Acc@1  78.12 ( 76.24)	Acc@5 100.00 ( 97.99)
Epoch: [6][390/391]	Time  0.014 ( 0.024)	Data  0.001 ( 0.002)	Loss 8.0420e-01 (7.1672e-01)	Acc@1  73.75 ( 76.23)	Acc@5  98.75 ( 97.99)
## e[6] optimizer.zero_grad (sum) time: 0.10578513145446777
## e[6]       loss.backward (sum) time: 2.1551430225372314
## e[6]      optimizer.step (sum) time: 0.8934276103973389
## epoch[6] training(only) time: 9.715675830841064
# Switched to evaluate mode...
Test: [  0/100]	Time  0.156 ( 0.156)	Loss 7.0312e-01 (7.0312e-01)	Acc@1  77.00 ( 77.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.008 ( 0.027)	Loss 7.0215e-01 (6.7913e-01)	Acc@1  80.00 ( 77.00)	Acc@5  97.00 ( 97.82)
Test: [ 20/100]	Time  0.008 ( 0.020)	Loss 7.9883e-01 (6.8012e-01)	Acc@1  72.00 ( 77.24)	Acc@5  99.00 ( 97.76)
Test: [ 30/100]	Time  0.012 ( 0.018)	Loss 7.0361e-01 (6.9680e-01)	Acc@1  77.00 ( 76.90)	Acc@5  98.00 ( 97.84)
Test: [ 40/100]	Time  0.008 ( 0.016)	Loss 6.8164e-01 (6.9759e-01)	Acc@1  80.00 ( 76.80)	Acc@5  97.00 ( 97.93)
Test: [ 50/100]	Time  0.016 ( 0.016)	Loss 5.7324e-01 (6.8788e-01)	Acc@1  82.00 ( 77.51)	Acc@5  99.00 ( 98.02)
Test: [ 60/100]	Time  0.024 ( 0.016)	Loss 7.3633e-01 (6.9523e-01)	Acc@1  77.00 ( 77.21)	Acc@5  99.00 ( 98.08)
Test: [ 70/100]	Time  0.012 ( 0.015)	Loss 7.1240e-01 (6.9025e-01)	Acc@1  72.00 ( 77.08)	Acc@5  98.00 ( 98.11)
Test: [ 80/100]	Time  0.008 ( 0.015)	Loss 6.0986e-01 (6.8877e-01)	Acc@1  77.00 ( 76.84)	Acc@5  99.00 ( 98.11)
Test: [ 90/100]	Time  0.018 ( 0.015)	Loss 7.0703e-01 (6.8919e-01)	Acc@1  76.00 ( 76.70)	Acc@5 100.00 ( 98.15)
 * Acc@1 76.740 Acc@5 98.150
### epoch[6] execution time: 11.281132459640503
EPOCH 7
# current learning rate: 0.1
# Switched to train mode...
Epoch: [7][  0/391]	Time  0.197 ( 0.197)	Data  0.176 ( 0.176)	Loss 7.4463e-01 (7.4463e-01)	Acc@1  75.00 ( 75.00)	Acc@5  98.44 ( 98.44)
Epoch: [7][ 10/391]	Time  0.018 ( 0.039)	Data  0.001 ( 0.018)	Loss 6.0254e-01 (7.1080e-01)	Acc@1  78.12 ( 75.28)	Acc@5  99.22 ( 98.51)
Epoch: [7][ 20/391]	Time  0.018 ( 0.031)	Data  0.001 ( 0.010)	Loss 7.6367e-01 (7.0710e-01)	Acc@1  75.00 ( 76.00)	Acc@5  98.44 ( 98.33)
Epoch: [7][ 30/391]	Time  0.017 ( 0.029)	Data  0.000 ( 0.008)	Loss 5.3271e-01 (6.8501e-01)	Acc@1  81.25 ( 76.56)	Acc@5  99.22 ( 98.31)
Epoch: [7][ 40/391]	Time  0.040 ( 0.028)	Data  0.003 ( 0.007)	Loss 7.8516e-01 (6.9354e-01)	Acc@1  74.22 ( 76.43)	Acc@5  97.66 ( 98.25)
Epoch: [7][ 50/391]	Time  0.017 ( 0.026)	Data  0.000 ( 0.006)	Loss 6.4893e-01 (7.0567e-01)	Acc@1  78.12 ( 76.29)	Acc@5  97.66 ( 98.07)
Epoch: [7][ 60/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.005)	Loss 7.5439e-01 (6.9719e-01)	Acc@1  73.44 ( 76.50)	Acc@5  99.22 ( 98.25)
Epoch: [7][ 70/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.005)	Loss 6.5234e-01 (6.9349e-01)	Acc@1  80.47 ( 76.56)	Acc@5  96.88 ( 98.20)
Epoch: [7][ 80/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 6.9482e-01 (6.9471e-01)	Acc@1  75.78 ( 76.57)	Acc@5 100.00 ( 98.18)
Epoch: [7][ 90/391]	Time  0.024 ( 0.025)	Data  0.001 ( 0.004)	Loss 5.2637e-01 (6.8692e-01)	Acc@1  82.03 ( 76.95)	Acc@5 100.00 ( 98.25)
Epoch: [7][100/391]	Time  0.043 ( 0.025)	Data  0.005 ( 0.004)	Loss 4.4189e-01 (6.7689e-01)	Acc@1  85.94 ( 77.35)	Acc@5  98.44 ( 98.28)
Epoch: [7][110/391]	Time  0.030 ( 0.025)	Data  0.001 ( 0.004)	Loss 7.0898e-01 (6.7647e-01)	Acc@1  82.81 ( 77.40)	Acc@5  98.44 ( 98.28)
Epoch: [7][120/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.004)	Loss 7.3779e-01 (6.7345e-01)	Acc@1  75.78 ( 77.43)	Acc@5  96.09 ( 98.28)
Epoch: [7][130/391]	Time  0.016 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.1719e-01 (6.7165e-01)	Acc@1  80.47 ( 77.54)	Acc@5  97.66 ( 98.27)
Epoch: [7][140/391]	Time  0.027 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.4600e-01 (6.7135e-01)	Acc@1  75.78 ( 77.53)	Acc@5  99.22 ( 98.28)
Epoch: [7][150/391]	Time  0.025 ( 0.025)	Data  0.000 ( 0.003)	Loss 5.5420e-01 (6.6903e-01)	Acc@1  79.69 ( 77.62)	Acc@5  98.44 ( 98.25)
Epoch: [7][160/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 7.4072e-01 (6.6643e-01)	Acc@1  76.56 ( 77.78)	Acc@5  96.09 ( 98.21)
Epoch: [7][170/391]	Time  0.036 ( 0.025)	Data  0.003 ( 0.003)	Loss 6.4941e-01 (6.6436e-01)	Acc@1  78.12 ( 77.94)	Acc@5  97.66 ( 98.21)
Epoch: [7][180/391]	Time  0.021 ( 0.025)	Data  0.004 ( 0.003)	Loss 7.2363e-01 (6.6426e-01)	Acc@1  78.12 ( 77.92)	Acc@5  97.66 ( 98.23)
Epoch: [7][190/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 7.4805e-01 (6.6216e-01)	Acc@1  77.34 ( 77.99)	Acc@5  98.44 ( 98.27)
Epoch: [7][200/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.7334e-01 (6.6219e-01)	Acc@1  75.00 ( 78.02)	Acc@5  96.09 ( 98.27)
Epoch: [7][210/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.6699e-01 (6.6274e-01)	Acc@1  79.69 ( 78.04)	Acc@5  99.22 ( 98.26)
Epoch: [7][220/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.4443e-01 (6.6298e-01)	Acc@1  81.25 ( 78.03)	Acc@5  99.22 ( 98.29)
Epoch: [7][230/391]	Time  0.042 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.0498e-01 (6.6277e-01)	Acc@1  82.81 ( 78.03)	Acc@5  99.22 ( 98.27)
Epoch: [7][240/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.0156e-01 (6.6283e-01)	Acc@1  81.25 ( 78.07)	Acc@5  97.66 ( 98.27)
Epoch: [7][250/391]	Time  0.034 ( 0.025)	Data  0.003 ( 0.003)	Loss 8.0566e-01 (6.6315e-01)	Acc@1  75.00 ( 78.07)	Acc@5  96.88 ( 98.28)
Epoch: [7][260/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 7.1289e-01 (6.6200e-01)	Acc@1  81.25 ( 78.17)	Acc@5  98.44 ( 98.29)
Epoch: [7][270/391]	Time  0.018 ( 0.025)	Data  0.002 ( 0.003)	Loss 7.3926e-01 (6.6303e-01)	Acc@1  73.44 ( 78.12)	Acc@5  96.88 ( 98.29)
Epoch: [7][280/391]	Time  0.027 ( 0.025)	Data  0.003 ( 0.003)	Loss 6.5039e-01 (6.6180e-01)	Acc@1  77.34 ( 78.13)	Acc@5  98.44 ( 98.31)
Epoch: [7][290/391]	Time  0.039 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.0938e-01 (6.6236e-01)	Acc@1  82.03 ( 78.14)	Acc@5  96.88 ( 98.31)
Epoch: [7][300/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.6104e-01 (6.6295e-01)	Acc@1  83.59 ( 78.13)	Acc@5  98.44 ( 98.31)
Epoch: [7][310/391]	Time  0.042 ( 0.025)	Data  0.004 ( 0.003)	Loss 7.0361e-01 (6.6137e-01)	Acc@1  76.56 ( 78.19)	Acc@5  98.44 ( 98.32)
Epoch: [7][320/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.4648e-01 (6.6232e-01)	Acc@1  78.12 ( 78.12)	Acc@5  97.66 ( 98.32)
Epoch: [7][330/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.003)	Loss 7.1582e-01 (6.6239e-01)	Acc@1  78.12 ( 78.16)	Acc@5  97.66 ( 98.31)
Epoch: [7][340/391]	Time  0.024 ( 0.024)	Data  0.001 ( 0.003)	Loss 7.9736e-01 (6.6258e-01)	Acc@1  74.22 ( 78.13)	Acc@5  96.09 ( 98.30)
Epoch: [7][350/391]	Time  0.019 ( 0.024)	Data  0.002 ( 0.003)	Loss 5.1025e-01 (6.6090e-01)	Acc@1  82.03 ( 78.17)	Acc@5 100.00 ( 98.30)
Epoch: [7][360/391]	Time  0.027 ( 0.024)	Data  0.002 ( 0.002)	Loss 5.0830e-01 (6.5928e-01)	Acc@1  85.16 ( 78.20)	Acc@5  99.22 ( 98.31)
Epoch: [7][370/391]	Time  0.019 ( 0.024)	Data  0.001 ( 0.002)	Loss 7.9834e-01 (6.6011e-01)	Acc@1  71.88 ( 78.17)	Acc@5  99.22 ( 98.32)
Epoch: [7][380/391]	Time  0.037 ( 0.024)	Data  0.001 ( 0.002)	Loss 5.7959e-01 (6.5851e-01)	Acc@1  80.47 ( 78.23)	Acc@5  98.44 ( 98.33)
Epoch: [7][390/391]	Time  0.016 ( 0.024)	Data  0.001 ( 0.002)	Loss 5.7520e-01 (6.5883e-01)	Acc@1  72.50 ( 78.20)	Acc@5 100.00 ( 98.33)
## e[7] optimizer.zero_grad (sum) time: 0.10537290573120117
## e[7]       loss.backward (sum) time: 2.1657135486602783
## e[7]      optimizer.step (sum) time: 0.8935370445251465
## epoch[7] training(only) time: 9.654604196548462
# Switched to evaluate mode...
Test: [  0/100]	Time  0.164 ( 0.164)	Loss 6.4746e-01 (6.4746e-01)	Acc@1  79.00 ( 79.00)	Acc@5  96.00 ( 96.00)
Test: [ 10/100]	Time  0.011 ( 0.027)	Loss 6.0059e-01 (6.8106e-01)	Acc@1  80.00 ( 77.09)	Acc@5  98.00 ( 98.64)
Test: [ 20/100]	Time  0.008 ( 0.019)	Loss 6.7090e-01 (6.9694e-01)	Acc@1  71.00 ( 76.33)	Acc@5  99.00 ( 98.52)
Test: [ 30/100]	Time  0.016 ( 0.018)	Loss 7.9443e-01 (7.1053e-01)	Acc@1  72.00 ( 76.29)	Acc@5 100.00 ( 98.48)
Test: [ 40/100]	Time  0.016 ( 0.017)	Loss 8.2178e-01 (7.1358e-01)	Acc@1  78.00 ( 76.32)	Acc@5  96.00 ( 98.39)
Test: [ 50/100]	Time  0.027 ( 0.016)	Loss 7.6953e-01 (7.0606e-01)	Acc@1  77.00 ( 76.67)	Acc@5  97.00 ( 98.37)
Test: [ 60/100]	Time  0.013 ( 0.016)	Loss 6.0010e-01 (7.1225e-01)	Acc@1  78.00 ( 76.56)	Acc@5  99.00 ( 98.33)
Test: [ 70/100]	Time  0.008 ( 0.015)	Loss 7.7783e-01 (7.1497e-01)	Acc@1  70.00 ( 76.28)	Acc@5  98.00 ( 98.24)
Test: [ 80/100]	Time  0.013 ( 0.015)	Loss 7.7441e-01 (7.1431e-01)	Acc@1  75.00 ( 76.40)	Acc@5 100.00 ( 98.36)
Test: [ 90/100]	Time  0.013 ( 0.015)	Loss 7.0020e-01 (7.0856e-01)	Acc@1  75.00 ( 76.56)	Acc@5  99.00 ( 98.46)
 * Acc@1 76.530 Acc@5 98.490
### epoch[7] execution time: 11.236479043960571
EPOCH 8
# current learning rate: 0.1
# Switched to train mode...
Epoch: [8][  0/391]	Time  0.192 ( 0.192)	Data  0.167 ( 0.167)	Loss 7.3438e-01 (7.3438e-01)	Acc@1  75.00 ( 75.00)	Acc@5  97.66 ( 97.66)
Epoch: [8][ 10/391]	Time  0.025 ( 0.040)	Data  0.001 ( 0.017)	Loss 6.5771e-01 (6.6673e-01)	Acc@1  76.56 ( 77.70)	Acc@5  99.22 ( 98.08)
Epoch: [8][ 20/391]	Time  0.024 ( 0.032)	Data  0.001 ( 0.010)	Loss 6.9141e-01 (6.4667e-01)	Acc@1  78.12 ( 78.72)	Acc@5  98.44 ( 98.33)
Epoch: [8][ 30/391]	Time  0.042 ( 0.029)	Data  0.008 ( 0.007)	Loss 5.0049e-01 (6.2331e-01)	Acc@1  83.59 ( 79.41)	Acc@5 100.00 ( 98.49)
Epoch: [8][ 40/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.006)	Loss 5.4980e-01 (6.1976e-01)	Acc@1  84.38 ( 79.57)	Acc@5  97.66 ( 98.40)
Epoch: [8][ 50/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.005)	Loss 7.5537e-01 (6.1766e-01)	Acc@1  75.00 ( 79.66)	Acc@5  98.44 ( 98.39)
Epoch: [8][ 60/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.004)	Loss 5.8008e-01 (6.0785e-01)	Acc@1  80.47 ( 79.82)	Acc@5  96.88 ( 98.42)
Epoch: [8][ 70/391]	Time  0.017 ( 0.026)	Data  0.000 ( 0.004)	Loss 5.4199e-01 (6.0931e-01)	Acc@1  85.94 ( 79.80)	Acc@5  99.22 ( 98.48)
Epoch: [8][ 80/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 5.2979e-01 (6.0785e-01)	Acc@1  82.03 ( 79.83)	Acc@5  98.44 ( 98.51)
Epoch: [8][ 90/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 7.7148e-01 (6.0804e-01)	Acc@1  74.22 ( 79.73)	Acc@5  96.88 ( 98.54)
Epoch: [8][100/391]	Time  0.023 ( 0.026)	Data  0.000 ( 0.004)	Loss 7.1533e-01 (6.1539e-01)	Acc@1  75.00 ( 79.46)	Acc@5  97.66 ( 98.53)
Epoch: [8][110/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.8271e-01 (6.2292e-01)	Acc@1  76.56 ( 79.34)	Acc@5  96.09 ( 98.47)
Epoch: [8][120/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 7.2461e-01 (6.2733e-01)	Acc@1  78.91 ( 79.24)	Acc@5  98.44 ( 98.47)
Epoch: [8][130/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.4541e-01 (6.2531e-01)	Acc@1  79.69 ( 79.17)	Acc@5  98.44 ( 98.48)
Epoch: [8][140/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.3428e-01 (6.2428e-01)	Acc@1  80.47 ( 79.16)	Acc@5  98.44 ( 98.50)
Epoch: [8][150/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 7.3193e-01 (6.2176e-01)	Acc@1  76.56 ( 79.30)	Acc@5  98.44 ( 98.49)
Epoch: [8][160/391]	Time  0.042 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.7520e-01 (6.2202e-01)	Acc@1  80.47 ( 79.35)	Acc@5  98.44 ( 98.46)
Epoch: [8][170/391]	Time  0.034 ( 0.025)	Data  0.003 ( 0.003)	Loss 6.4697e-01 (6.2120e-01)	Acc@1  79.69 ( 79.41)	Acc@5  98.44 ( 98.47)
Epoch: [8][180/391]	Time  0.025 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.6885e-01 (6.1972e-01)	Acc@1  78.91 ( 79.44)	Acc@5 100.00 ( 98.49)
Epoch: [8][190/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 5.1758e-01 (6.1773e-01)	Acc@1  84.38 ( 79.59)	Acc@5  98.44 ( 98.49)
Epoch: [8][200/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.0928e-01 (6.1659e-01)	Acc@1  83.59 ( 79.66)	Acc@5  99.22 ( 98.48)
Epoch: [8][210/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.7471e-01 (6.1626e-01)	Acc@1  82.81 ( 79.73)	Acc@5  98.44 ( 98.46)
Epoch: [8][220/391]	Time  0.039 ( 0.025)	Data  0.001 ( 0.002)	Loss 6.4893e-01 (6.1810e-01)	Acc@1  82.03 ( 79.67)	Acc@5  97.66 ( 98.46)
Epoch: [8][230/391]	Time  0.040 ( 0.025)	Data  0.001 ( 0.002)	Loss 5.0244e-01 (6.1764e-01)	Acc@1  81.25 ( 79.70)	Acc@5 100.00 ( 98.46)
Epoch: [8][240/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.002)	Loss 6.8799e-01 (6.1378e-01)	Acc@1  78.91 ( 79.80)	Acc@5  96.88 ( 98.48)
Epoch: [8][250/391]	Time  0.034 ( 0.025)	Data  0.005 ( 0.002)	Loss 5.8057e-01 (6.1347e-01)	Acc@1  77.34 ( 79.79)	Acc@5  98.44 ( 98.48)
Epoch: [8][260/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 7.3877e-01 (6.1373e-01)	Acc@1  72.66 ( 79.76)	Acc@5  96.88 ( 98.47)
Epoch: [8][270/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.002)	Loss 6.7920e-01 (6.1506e-01)	Acc@1  78.12 ( 79.70)	Acc@5  97.66 ( 98.46)
Epoch: [8][280/391]	Time  0.055 ( 0.025)	Data  0.015 ( 0.002)	Loss 6.8359e-01 (6.1688e-01)	Acc@1  77.34 ( 79.62)	Acc@5  97.66 ( 98.47)
Epoch: [8][290/391]	Time  0.035 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.3994e-01 (6.1474e-01)	Acc@1  86.72 ( 79.68)	Acc@5  99.22 ( 98.48)
Epoch: [8][300/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 5.6738e-01 (6.1315e-01)	Acc@1  82.03 ( 79.73)	Acc@5  98.44 ( 98.47)
Epoch: [8][310/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 6.1572e-01 (6.1259e-01)	Acc@1  78.12 ( 79.74)	Acc@5  98.44 ( 98.48)
Epoch: [8][320/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 7.5732e-01 (6.1411e-01)	Acc@1  71.09 ( 79.68)	Acc@5  96.88 ( 98.47)
Epoch: [8][330/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.002)	Loss 6.0352e-01 (6.1343e-01)	Acc@1  77.34 ( 79.69)	Acc@5  99.22 ( 98.49)
Epoch: [8][340/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 5.7324e-01 (6.1147e-01)	Acc@1  79.69 ( 79.77)	Acc@5  97.66 ( 98.49)
Epoch: [8][350/391]	Time  0.032 ( 0.025)	Data  0.000 ( 0.002)	Loss 6.5332e-01 (6.1187e-01)	Acc@1  75.78 ( 79.75)	Acc@5  98.44 ( 98.50)
Epoch: [8][360/391]	Time  0.027 ( 0.025)	Data  0.000 ( 0.002)	Loss 6.4160e-01 (6.1220e-01)	Acc@1  79.69 ( 79.76)	Acc@5  98.44 ( 98.49)
Epoch: [8][370/391]	Time  0.040 ( 0.025)	Data  0.001 ( 0.002)	Loss 7.1826e-01 (6.1157e-01)	Acc@1  78.12 ( 79.79)	Acc@5  99.22 ( 98.49)
Epoch: [8][380/391]	Time  0.032 ( 0.024)	Data  0.006 ( 0.002)	Loss 6.1914e-01 (6.1171e-01)	Acc@1  80.47 ( 79.81)	Acc@5  97.66 ( 98.50)
Epoch: [8][390/391]	Time  0.015 ( 0.024)	Data  0.001 ( 0.002)	Loss 5.5518e-01 (6.1115e-01)	Acc@1  78.75 ( 79.83)	Acc@5  98.75 ( 98.50)
## e[8] optimizer.zero_grad (sum) time: 0.10569357872009277
## e[8]       loss.backward (sum) time: 2.152050018310547
## e[8]      optimizer.step (sum) time: 0.8824882507324219
## epoch[8] training(only) time: 9.671491861343384
# Switched to evaluate mode...
Test: [  0/100]	Time  0.171 ( 0.171)	Loss 5.7617e-01 (5.7617e-01)	Acc@1  80.00 ( 80.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.009 ( 0.027)	Loss 5.7666e-01 (5.5651e-01)	Acc@1  82.00 ( 82.18)	Acc@5  98.00 ( 98.91)
Test: [ 20/100]	Time  0.017 ( 0.020)	Loss 7.2852e-01 (5.8901e-01)	Acc@1  71.00 ( 80.48)	Acc@5  97.00 ( 98.43)
Test: [ 30/100]	Time  0.018 ( 0.018)	Loss 6.2646e-01 (6.0017e-01)	Acc@1  77.00 ( 80.13)	Acc@5  97.00 ( 98.29)
Test: [ 40/100]	Time  0.008 ( 0.016)	Loss 5.4932e-01 (6.0129e-01)	Acc@1  80.00 ( 79.93)	Acc@5  99.00 ( 98.22)
Test: [ 50/100]	Time  0.019 ( 0.016)	Loss 5.6152e-01 (5.9127e-01)	Acc@1  83.00 ( 80.41)	Acc@5  99.00 ( 98.24)
Test: [ 60/100]	Time  0.017 ( 0.016)	Loss 5.5322e-01 (5.9616e-01)	Acc@1  80.00 ( 80.23)	Acc@5  99.00 ( 98.28)
Test: [ 70/100]	Time  0.017 ( 0.015)	Loss 6.2354e-01 (5.9682e-01)	Acc@1  79.00 ( 80.11)	Acc@5 100.00 ( 98.32)
Test: [ 80/100]	Time  0.012 ( 0.015)	Loss 4.8804e-01 (5.9788e-01)	Acc@1  77.00 ( 80.12)	Acc@5 100.00 ( 98.33)
Test: [ 90/100]	Time  0.017 ( 0.015)	Loss 6.4014e-01 (5.9965e-01)	Acc@1  75.00 ( 79.95)	Acc@5 100.00 ( 98.33)
 * Acc@1 79.970 Acc@5 98.420
### epoch[8] execution time: 11.231462955474854
EPOCH 9
# current learning rate: 0.1
# Switched to train mode...
Epoch: [9][  0/391]	Time  0.258 ( 0.258)	Data  0.233 ( 0.233)	Loss 5.1123e-01 (5.1123e-01)	Acc@1  82.03 ( 82.03)	Acc@5  98.44 ( 98.44)
Epoch: [9][ 10/391]	Time  0.019 ( 0.043)	Data  0.002 ( 0.023)	Loss 2.9956e-01 (4.8300e-01)	Acc@1  89.06 ( 83.52)	Acc@5 100.00 ( 99.22)
Epoch: [9][ 20/391]	Time  0.049 ( 0.034)	Data  0.009 ( 0.013)	Loss 6.1670e-01 (5.2908e-01)	Acc@1  79.69 ( 82.51)	Acc@5  98.44 ( 98.77)
Epoch: [9][ 30/391]	Time  0.018 ( 0.030)	Data  0.001 ( 0.009)	Loss 5.3076e-01 (5.5544e-01)	Acc@1  78.91 ( 81.48)	Acc@5  98.44 ( 98.59)
Epoch: [9][ 40/391]	Time  0.018 ( 0.029)	Data  0.001 ( 0.008)	Loss 5.2295e-01 (5.6891e-01)	Acc@1  82.03 ( 80.89)	Acc@5 100.00 ( 98.65)
Epoch: [9][ 50/391]	Time  0.019 ( 0.028)	Data  0.001 ( 0.007)	Loss 7.1094e-01 (5.7552e-01)	Acc@1  78.12 ( 80.64)	Acc@5  96.88 ( 98.61)
Epoch: [9][ 60/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.006)	Loss 5.3174e-01 (5.7474e-01)	Acc@1  80.47 ( 80.67)	Acc@5 100.00 ( 98.62)
Epoch: [9][ 70/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.005)	Loss 6.6357e-01 (5.6729e-01)	Acc@1  78.91 ( 81.11)	Acc@5  98.44 ( 98.68)
Epoch: [9][ 80/391]	Time  0.028 ( 0.027)	Data  0.001 ( 0.005)	Loss 5.0928e-01 (5.6835e-01)	Acc@1  85.16 ( 81.02)	Acc@5  97.66 ( 98.60)
Epoch: [9][ 90/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 4.8535e-01 (5.6459e-01)	Acc@1  82.81 ( 81.10)	Acc@5  99.22 ( 98.66)
Epoch: [9][100/391]	Time  0.033 ( 0.026)	Data  0.005 ( 0.004)	Loss 6.2305e-01 (5.6594e-01)	Acc@1  78.91 ( 81.09)	Acc@5  97.66 ( 98.59)
Epoch: [9][110/391]	Time  0.018 ( 0.026)	Data  0.002 ( 0.004)	Loss 5.1416e-01 (5.6571e-01)	Acc@1  83.59 ( 81.16)	Acc@5  99.22 ( 98.61)
Epoch: [9][120/391]	Time  0.038 ( 0.026)	Data  0.001 ( 0.004)	Loss 5.2783e-01 (5.6618e-01)	Acc@1  82.81 ( 81.17)	Acc@5  99.22 ( 98.64)
Epoch: [9][130/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.004)	Loss 5.0098e-01 (5.6355e-01)	Acc@1  82.81 ( 81.20)	Acc@5  98.44 ( 98.66)
Epoch: [9][140/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.6729e-01 (5.6425e-01)	Acc@1  83.59 ( 81.21)	Acc@5  99.22 ( 98.63)
Epoch: [9][150/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.9336e-01 (5.6482e-01)	Acc@1  78.12 ( 81.14)	Acc@5  96.88 ( 98.62)
Epoch: [9][160/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.8301e-01 (5.6502e-01)	Acc@1  80.47 ( 81.09)	Acc@5  99.22 ( 98.67)
Epoch: [9][170/391]	Time  0.024 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.1719e-01 (5.6701e-01)	Acc@1  80.47 ( 81.06)	Acc@5  98.44 ( 98.63)
Epoch: [9][180/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.3027e-01 (5.6648e-01)	Acc@1  83.59 ( 81.12)	Acc@5  98.44 ( 98.64)
Epoch: [9][190/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.1416e-01 (5.6787e-01)	Acc@1  82.81 ( 81.03)	Acc@5  98.44 ( 98.65)
Epoch: [9][200/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.3320e-01 (5.6750e-01)	Acc@1  80.47 ( 81.08)	Acc@5 100.00 ( 98.65)
Epoch: [9][210/391]	Time  0.031 ( 0.025)	Data  0.000 ( 0.003)	Loss 5.2734e-01 (5.6614e-01)	Acc@1  82.81 ( 81.19)	Acc@5  98.44 ( 98.64)
Epoch: [9][220/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.0537e-01 (5.6634e-01)	Acc@1  86.72 ( 81.21)	Acc@5  98.44 ( 98.67)
Epoch: [9][230/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.2090e-01 (5.6792e-01)	Acc@1  82.81 ( 81.14)	Acc@5 100.00 ( 98.66)
Epoch: [9][240/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.5420e-01 (5.6769e-01)	Acc@1  80.47 ( 81.13)	Acc@5  99.22 ( 98.67)
Epoch: [9][250/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.9824e-01 (5.6888e-01)	Acc@1  84.38 ( 81.13)	Acc@5  96.88 ( 98.67)
Epoch: [9][260/391]	Time  0.034 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.4800e-01 (5.6810e-01)	Acc@1  87.50 ( 81.18)	Acc@5  97.66 ( 98.68)
Epoch: [9][270/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 4.9414e-01 (5.6734e-01)	Acc@1  83.59 ( 81.21)	Acc@5 100.00 ( 98.70)
Epoch: [9][280/391]	Time  0.017 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.4834e-01 (5.6791e-01)	Acc@1  80.47 ( 81.17)	Acc@5  98.44 ( 98.70)
Epoch: [9][290/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 5.7959e-01 (5.6854e-01)	Acc@1  83.59 ( 81.18)	Acc@5  99.22 ( 98.69)
Epoch: [9][300/391]	Time  0.032 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.9438e-01 (5.6818e-01)	Acc@1  85.94 ( 81.18)	Acc@5  98.44 ( 98.69)
Epoch: [9][310/391]	Time  0.027 ( 0.025)	Data  0.002 ( 0.003)	Loss 4.9854e-01 (5.6638e-01)	Acc@1  86.72 ( 81.26)	Acc@5  98.44 ( 98.68)
Epoch: [9][320/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 4.8706e-01 (5.6761e-01)	Acc@1  84.38 ( 81.24)	Acc@5  97.66 ( 98.67)
Epoch: [9][330/391]	Time  0.029 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.7520e-01 (5.6713e-01)	Acc@1  79.69 ( 81.25)	Acc@5  98.44 ( 98.66)
Epoch: [9][340/391]	Time  0.039 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.9097e-01 (5.6639e-01)	Acc@1  83.59 ( 81.26)	Acc@5  99.22 ( 98.67)
Epoch: [9][350/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.0596e-01 (5.6543e-01)	Acc@1  79.69 ( 81.29)	Acc@5 100.00 ( 98.67)
Epoch: [9][360/391]	Time  0.031 ( 0.025)	Data  0.003 ( 0.003)	Loss 5.0732e-01 (5.6568e-01)	Acc@1  80.47 ( 81.27)	Acc@5  98.44 ( 98.66)
Epoch: [9][370/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 7.2607e-01 (5.6583e-01)	Acc@1  79.69 ( 81.28)	Acc@5  99.22 ( 98.67)
Epoch: [9][380/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.1172e-01 (5.6458e-01)	Acc@1  82.03 ( 81.29)	Acc@5  97.66 ( 98.68)
Epoch: [9][390/391]	Time  0.016 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.9336e-01 (5.6464e-01)	Acc@1  81.25 ( 81.30)	Acc@5 100.00 ( 98.69)
## e[9] optimizer.zero_grad (sum) time: 0.10624456405639648
## e[9]       loss.backward (sum) time: 2.1377716064453125
## e[9]      optimizer.step (sum) time: 0.8972494602203369
## epoch[9] training(only) time: 9.731533288955688
# Switched to evaluate mode...
Test: [  0/100]	Time  0.214 ( 0.214)	Loss 5.0439e-01 (5.0439e-01)	Acc@1  82.00 ( 82.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.012 ( 0.030)	Loss 4.8779e-01 (5.5791e-01)	Acc@1  85.00 ( 81.00)	Acc@5 100.00 ( 99.00)
Test: [ 20/100]	Time  0.024 ( 0.023)	Loss 6.2207e-01 (5.8225e-01)	Acc@1  76.00 ( 79.33)	Acc@5  99.00 ( 99.00)
Test: [ 30/100]	Time  0.008 ( 0.020)	Loss 6.1182e-01 (5.9832e-01)	Acc@1  79.00 ( 79.03)	Acc@5  98.00 ( 98.97)
Test: [ 40/100]	Time  0.014 ( 0.018)	Loss 5.2295e-01 (5.9492e-01)	Acc@1  83.00 ( 79.34)	Acc@5  98.00 ( 98.83)
Test: [ 50/100]	Time  0.011 ( 0.017)	Loss 5.5957e-01 (5.8725e-01)	Acc@1  83.00 ( 79.71)	Acc@5  97.00 ( 98.76)
Test: [ 60/100]	Time  0.012 ( 0.017)	Loss 5.5273e-01 (5.8685e-01)	Acc@1  81.00 ( 79.70)	Acc@5  99.00 ( 98.82)
Test: [ 70/100]	Time  0.023 ( 0.016)	Loss 6.1279e-01 (5.8639e-01)	Acc@1  83.00 ( 79.92)	Acc@5  98.00 ( 98.82)
Test: [ 80/100]	Time  0.016 ( 0.016)	Loss 5.2783e-01 (5.8288e-01)	Acc@1  83.00 ( 80.12)	Acc@5  98.00 ( 98.83)
Test: [ 90/100]	Time  0.013 ( 0.016)	Loss 5.1465e-01 (5.8266e-01)	Acc@1  84.00 ( 80.22)	Acc@5  99.00 ( 98.86)
 * Acc@1 80.250 Acc@5 98.870
### epoch[9] execution time: 11.370524168014526
EPOCH 10
# current learning rate: 0.1
# Switched to train mode...
Epoch: [10][  0/391]	Time  0.187 ( 0.187)	Data  0.166 ( 0.166)	Loss 2.8345e-01 (2.8345e-01)	Acc@1  89.06 ( 89.06)	Acc@5 100.00 (100.00)
Epoch: [10][ 10/391]	Time  0.020 ( 0.037)	Data  0.002 ( 0.017)	Loss 5.7373e-01 (5.3908e-01)	Acc@1  85.16 ( 81.68)	Acc@5 100.00 ( 99.08)
Epoch: [10][ 20/391]	Time  0.020 ( 0.032)	Data  0.001 ( 0.010)	Loss 5.5127e-01 (5.5015e-01)	Acc@1  79.69 ( 81.70)	Acc@5  98.44 ( 98.81)
Epoch: [10][ 30/391]	Time  0.017 ( 0.029)	Data  0.001 ( 0.007)	Loss 3.9111e-01 (5.2033e-01)	Acc@1  88.28 ( 82.81)	Acc@5 100.00 ( 98.99)
Epoch: [10][ 40/391]	Time  0.026 ( 0.028)	Data  0.001 ( 0.006)	Loss 7.1338e-01 (5.2091e-01)	Acc@1  78.91 ( 82.81)	Acc@5 100.00 ( 99.01)
Epoch: [10][ 50/391]	Time  0.018 ( 0.026)	Data  0.002 ( 0.005)	Loss 5.8740e-01 (5.2451e-01)	Acc@1  82.81 ( 82.74)	Acc@5  97.66 ( 98.88)
Epoch: [10][ 60/391]	Time  0.034 ( 0.026)	Data  0.001 ( 0.004)	Loss 3.5205e-01 (5.2080e-01)	Acc@1  89.84 ( 82.81)	Acc@5  99.22 ( 98.85)
Epoch: [10][ 70/391]	Time  0.024 ( 0.026)	Data  0.002 ( 0.004)	Loss 5.7178e-01 (5.2311e-01)	Acc@1  84.38 ( 82.82)	Acc@5  99.22 ( 98.87)
Epoch: [10][ 80/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.004)	Loss 5.7227e-01 (5.1909e-01)	Acc@1  80.47 ( 82.98)	Acc@5  96.88 ( 98.84)
Epoch: [10][ 90/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.004)	Loss 5.2539e-01 (5.2438e-01)	Acc@1  81.25 ( 82.73)	Acc@5  99.22 ( 98.87)
Epoch: [10][100/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.004)	Loss 5.1074e-01 (5.2746e-01)	Acc@1  84.38 ( 82.67)	Acc@5  98.44 ( 98.84)
Epoch: [10][110/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.8066e-01 (5.3613e-01)	Acc@1  76.56 ( 82.33)	Acc@5  99.22 ( 98.81)
Epoch: [10][120/391]	Time  0.029 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.9717e-01 (5.3860e-01)	Acc@1  78.12 ( 82.22)	Acc@5  99.22 ( 98.81)
Epoch: [10][130/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.0938e-01 (5.3925e-01)	Acc@1  78.91 ( 82.21)	Acc@5  96.88 ( 98.77)
Epoch: [10][140/391]	Time  0.018 ( 0.025)	Data  0.002 ( 0.003)	Loss 4.9243e-01 (5.4196e-01)	Acc@1  85.94 ( 82.15)	Acc@5 100.00 ( 98.76)
Epoch: [10][150/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.6558e-01 (5.4096e-01)	Acc@1  84.38 ( 82.21)	Acc@5 100.00 ( 98.75)
Epoch: [10][160/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.3252e-01 (5.4212e-01)	Acc@1  90.62 ( 82.29)	Acc@5  99.22 ( 98.77)
Epoch: [10][170/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.1699e-01 (5.4094e-01)	Acc@1  85.16 ( 82.29)	Acc@5  98.44 ( 98.78)
Epoch: [10][180/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.9180e-01 (5.3822e-01)	Acc@1  78.12 ( 82.39)	Acc@5  99.22 ( 98.76)
Epoch: [10][190/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.8169e-01 (5.3597e-01)	Acc@1  83.59 ( 82.49)	Acc@5 100.00 ( 98.78)
Epoch: [10][200/391]	Time  0.040 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.6494e-01 (5.3713e-01)	Acc@1  81.25 ( 82.43)	Acc@5  99.22 ( 98.77)
Epoch: [10][210/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 4.8779e-01 (5.3624e-01)	Acc@1  82.81 ( 82.43)	Acc@5  99.22 ( 98.77)
Epoch: [10][220/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.5410e-01 (5.3666e-01)	Acc@1  82.81 ( 82.43)	Acc@5  99.22 ( 98.78)
Epoch: [10][230/391]	Time  0.025 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.5532e-01 (5.3616e-01)	Acc@1  85.16 ( 82.43)	Acc@5  99.22 ( 98.78)
Epoch: [10][240/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.003)	Loss 7.2656e-01 (5.3741e-01)	Acc@1  76.56 ( 82.37)	Acc@5  97.66 ( 98.77)
Epoch: [10][250/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.5361e-01 (5.3844e-01)	Acc@1  85.16 ( 82.36)	Acc@5  98.44 ( 98.77)
Epoch: [10][260/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 5.7031e-01 (5.3973e-01)	Acc@1  82.81 ( 82.35)	Acc@5  98.44 ( 98.78)
Epoch: [10][270/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 5.0586e-01 (5.4020e-01)	Acc@1  84.38 ( 82.33)	Acc@5  98.44 ( 98.78)
Epoch: [10][280/391]	Time  0.021 ( 0.024)	Data  0.001 ( 0.002)	Loss 4.3433e-01 (5.3993e-01)	Acc@1  86.72 ( 82.36)	Acc@5  98.44 ( 98.79)
Epoch: [10][290/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 5.2344e-01 (5.3866e-01)	Acc@1  82.03 ( 82.39)	Acc@5  99.22 ( 98.80)
Epoch: [10][300/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 6.0303e-01 (5.3931e-01)	Acc@1  81.25 ( 82.37)	Acc@5  96.88 ( 98.79)
Epoch: [10][310/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 4.2554e-01 (5.3845e-01)	Acc@1  84.38 ( 82.41)	Acc@5 100.00 ( 98.79)
Epoch: [10][320/391]	Time  0.031 ( 0.024)	Data  0.001 ( 0.002)	Loss 4.6411e-01 (5.3694e-01)	Acc@1  79.69 ( 82.44)	Acc@5 100.00 ( 98.80)
Epoch: [10][330/391]	Time  0.040 ( 0.024)	Data  0.001 ( 0.002)	Loss 5.7275e-01 (5.3736e-01)	Acc@1  77.34 ( 82.41)	Acc@5  99.22 ( 98.81)
Epoch: [10][340/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 6.4209e-01 (5.3751e-01)	Acc@1  82.81 ( 82.41)	Acc@5  98.44 ( 98.82)
Epoch: [10][350/391]	Time  0.019 ( 0.024)	Data  0.002 ( 0.002)	Loss 5.8594e-01 (5.3745e-01)	Acc@1  78.91 ( 82.42)	Acc@5  98.44 ( 98.83)
Epoch: [10][360/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 6.2842e-01 (5.3783e-01)	Acc@1  79.69 ( 82.44)	Acc@5  97.66 ( 98.81)
Epoch: [10][370/391]	Time  0.026 ( 0.024)	Data  0.001 ( 0.002)	Loss 4.1724e-01 (5.3681e-01)	Acc@1  85.94 ( 82.44)	Acc@5  99.22 ( 98.82)
Epoch: [10][380/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 6.2061e-01 (5.3666e-01)	Acc@1  83.59 ( 82.48)	Acc@5  99.22 ( 98.83)
Epoch: [10][390/391]	Time  0.015 ( 0.024)	Data  0.000 ( 0.002)	Loss 6.5576e-01 (5.3663e-01)	Acc@1  80.00 ( 82.49)	Acc@5  97.50 ( 98.83)
## e[10] optimizer.zero_grad (sum) time: 0.10453557968139648
## e[10]       loss.backward (sum) time: 2.1506481170654297
## e[10]      optimizer.step (sum) time: 0.8941998481750488
## epoch[10] training(only) time: 9.605514764785767
# Switched to evaluate mode...
Test: [  0/100]	Time  0.171 ( 0.171)	Loss 5.3809e-01 (5.3809e-01)	Acc@1  79.00 ( 79.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.008 ( 0.027)	Loss 6.6602e-01 (6.1896e-01)	Acc@1  77.00 ( 80.18)	Acc@5  98.00 ( 98.36)
Test: [ 20/100]	Time  0.013 ( 0.021)	Loss 6.0010e-01 (6.0948e-01)	Acc@1  79.00 ( 80.14)	Acc@5  99.00 ( 98.38)
Test: [ 30/100]	Time  0.013 ( 0.018)	Loss 7.8857e-01 (6.1917e-01)	Acc@1  75.00 ( 79.87)	Acc@5  97.00 ( 98.45)
Test: [ 40/100]	Time  0.019 ( 0.017)	Loss 6.6895e-01 (6.2418e-01)	Acc@1  76.00 ( 79.61)	Acc@5  98.00 ( 98.44)
Test: [ 50/100]	Time  0.017 ( 0.016)	Loss 6.6309e-01 (6.2083e-01)	Acc@1  83.00 ( 79.71)	Acc@5  97.00 ( 98.33)
Test: [ 60/100]	Time  0.011 ( 0.016)	Loss 7.2021e-01 (6.2703e-01)	Acc@1  80.00 ( 79.70)	Acc@5  98.00 ( 98.31)
Test: [ 70/100]	Time  0.012 ( 0.015)	Loss 6.4209e-01 (6.3281e-01)	Acc@1  80.00 ( 79.42)	Acc@5  98.00 ( 98.28)
Test: [ 80/100]	Time  0.017 ( 0.015)	Loss 6.0889e-01 (6.3116e-01)	Acc@1  82.00 ( 79.48)	Acc@5  96.00 ( 98.26)
Test: [ 90/100]	Time  0.015 ( 0.015)	Loss 4.6826e-01 (6.3074e-01)	Acc@1  85.00 ( 79.53)	Acc@5 100.00 ( 98.25)
 * Acc@1 79.520 Acc@5 98.260
### epoch[10] execution time: 11.21995496749878
EPOCH 11
# current learning rate: 0.1
# Switched to train mode...
Epoch: [11][  0/391]	Time  0.193 ( 0.193)	Data  0.172 ( 0.172)	Loss 6.7236e-01 (6.7236e-01)	Acc@1  80.47 ( 80.47)	Acc@5  97.66 ( 97.66)
Epoch: [11][ 10/391]	Time  0.019 ( 0.039)	Data  0.001 ( 0.017)	Loss 3.4473e-01 (5.2588e-01)	Acc@1  88.28 ( 82.88)	Acc@5 100.00 ( 98.79)
Epoch: [11][ 20/391]	Time  0.023 ( 0.032)	Data  0.001 ( 0.010)	Loss 4.7534e-01 (5.2377e-01)	Acc@1  84.38 ( 83.07)	Acc@5 100.00 ( 99.00)
Epoch: [11][ 30/391]	Time  0.019 ( 0.030)	Data  0.001 ( 0.007)	Loss 5.8789e-01 (5.3263e-01)	Acc@1  76.56 ( 82.51)	Acc@5  98.44 ( 99.02)
Epoch: [11][ 40/391]	Time  0.018 ( 0.028)	Data  0.001 ( 0.006)	Loss 5.0684e-01 (5.2407e-01)	Acc@1  83.59 ( 82.79)	Acc@5  99.22 ( 98.99)
Epoch: [11][ 50/391]	Time  0.018 ( 0.028)	Data  0.001 ( 0.005)	Loss 6.3721e-01 (5.2653e-01)	Acc@1  80.47 ( 82.71)	Acc@5  98.44 ( 98.94)
Epoch: [11][ 60/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.004)	Loss 5.9033e-01 (5.2034e-01)	Acc@1  85.16 ( 83.08)	Acc@5  96.88 ( 98.89)
Epoch: [11][ 70/391]	Time  0.017 ( 0.026)	Data  0.000 ( 0.004)	Loss 5.5664e-01 (5.1320e-01)	Acc@1  82.81 ( 83.23)	Acc@5  96.88 ( 98.87)
Epoch: [11][ 80/391]	Time  0.019 ( 0.026)	Data  0.002 ( 0.004)	Loss 6.8164e-01 (5.0931e-01)	Acc@1  78.91 ( 83.48)	Acc@5  98.44 ( 98.84)
Epoch: [11][ 90/391]	Time  0.017 ( 0.026)	Data  0.000 ( 0.004)	Loss 4.6729e-01 (5.0498e-01)	Acc@1  84.38 ( 83.53)	Acc@5 100.00 ( 98.92)
Epoch: [11][100/391]	Time  0.041 ( 0.026)	Data  0.002 ( 0.003)	Loss 6.2744e-01 (5.1030e-01)	Acc@1  82.81 ( 83.40)	Acc@5  99.22 ( 98.94)
Epoch: [11][110/391]	Time  0.036 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.3730e-01 (5.1336e-01)	Acc@1  76.56 ( 83.31)	Acc@5  99.22 ( 98.95)
Epoch: [11][120/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.9536e-01 (5.1419e-01)	Acc@1  83.59 ( 83.23)	Acc@5  99.22 ( 98.95)
Epoch: [11][130/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.5962e-01 (5.1196e-01)	Acc@1  82.81 ( 83.29)	Acc@5 100.00 ( 98.94)
Epoch: [11][140/391]	Time  0.029 ( 0.025)	Data  0.000 ( 0.003)	Loss 4.6533e-01 (5.1224e-01)	Acc@1  86.72 ( 83.34)	Acc@5 100.00 ( 98.93)
Epoch: [11][150/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.2002e-01 (5.1091e-01)	Acc@1  82.81 ( 83.39)	Acc@5  99.22 ( 98.93)
Epoch: [11][160/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.4590e-01 (5.1471e-01)	Acc@1  82.03 ( 83.30)	Acc@5  99.22 ( 98.92)
Epoch: [11][170/391]	Time  0.024 ( 0.025)	Data  0.002 ( 0.003)	Loss 6.2500e-01 (5.1579e-01)	Acc@1  81.25 ( 83.29)	Acc@5  99.22 ( 98.93)
Epoch: [11][180/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.9561e-01 (5.1544e-01)	Acc@1  78.12 ( 83.26)	Acc@5  99.22 ( 98.90)
Epoch: [11][190/391]	Time  0.026 ( 0.025)	Data  0.002 ( 0.003)	Loss 5.9131e-01 (5.1546e-01)	Acc@1  79.69 ( 83.23)	Acc@5  98.44 ( 98.91)
Epoch: [11][200/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.0308e-01 (5.1343e-01)	Acc@1  86.72 ( 83.24)	Acc@5 100.00 ( 98.92)
Epoch: [11][210/391]	Time  0.024 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.9746e-01 (5.1254e-01)	Acc@1  83.59 ( 83.21)	Acc@5  99.22 ( 98.92)
Epoch: [11][220/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.7666e-01 (5.1356e-01)	Acc@1  82.81 ( 83.19)	Acc@5  99.22 ( 98.92)
Epoch: [11][230/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 3.9868e-01 (5.1410e-01)	Acc@1  88.28 ( 83.20)	Acc@5 100.00 ( 98.93)
Epoch: [11][240/391]	Time  0.038 ( 0.025)	Data  0.005 ( 0.003)	Loss 3.9868e-01 (5.1445e-01)	Acc@1  89.06 ( 83.23)	Acc@5  99.22 ( 98.94)
Epoch: [11][250/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.3809e-01 (5.1471e-01)	Acc@1  83.59 ( 83.20)	Acc@5  99.22 ( 98.94)
Epoch: [11][260/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.4785e-01 (5.1545e-01)	Acc@1  85.16 ( 83.18)	Acc@5  98.44 ( 98.95)
Epoch: [11][270/391]	Time  0.022 ( 0.025)	Data  0.002 ( 0.003)	Loss 4.7144e-01 (5.1268e-01)	Acc@1  83.59 ( 83.28)	Acc@5  99.22 ( 98.96)
Epoch: [11][280/391]	Time  0.039 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.1172e-01 (5.1154e-01)	Acc@1  83.59 ( 83.32)	Acc@5  98.44 ( 98.96)
Epoch: [11][290/391]	Time  0.024 ( 0.025)	Data  0.001 ( 0.002)	Loss 5.7861e-01 (5.1365e-01)	Acc@1  79.69 ( 83.23)	Acc@5  96.88 ( 98.92)
Epoch: [11][300/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 5.2295e-01 (5.1510e-01)	Acc@1  85.16 ( 83.17)	Acc@5 100.00 ( 98.92)
Epoch: [11][310/391]	Time  0.045 ( 0.025)	Data  0.003 ( 0.002)	Loss 4.4287e-01 (5.1394e-01)	Acc@1  85.16 ( 83.17)	Acc@5 100.00 ( 98.94)
Epoch: [11][320/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.002)	Loss 3.8501e-01 (5.1287e-01)	Acc@1  89.06 ( 83.23)	Acc@5 100.00 ( 98.94)
Epoch: [11][330/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 5.6982e-01 (5.1238e-01)	Acc@1  82.03 ( 83.24)	Acc@5 100.00 ( 98.94)
Epoch: [11][340/391]	Time  0.025 ( 0.025)	Data  0.001 ( 0.002)	Loss 5.2539e-01 (5.1267e-01)	Acc@1  80.47 ( 83.21)	Acc@5  98.44 ( 98.94)
Epoch: [11][350/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.5190e-01 (5.1266e-01)	Acc@1  82.81 ( 83.21)	Acc@5  98.44 ( 98.92)
Epoch: [11][360/391]	Time  0.036 ( 0.025)	Data  0.002 ( 0.002)	Loss 5.0439e-01 (5.1300e-01)	Acc@1  85.16 ( 83.17)	Acc@5  98.44 ( 98.93)
Epoch: [11][370/391]	Time  0.044 ( 0.025)	Data  0.004 ( 0.002)	Loss 4.9365e-01 (5.1215e-01)	Acc@1  84.38 ( 83.19)	Acc@5  98.44 ( 98.93)
Epoch: [11][380/391]	Time  0.030 ( 0.025)	Data  0.001 ( 0.002)	Loss 5.9326e-01 (5.1191e-01)	Acc@1  80.47 ( 83.20)	Acc@5  99.22 ( 98.94)
Epoch: [11][390/391]	Time  0.016 ( 0.024)	Data  0.001 ( 0.002)	Loss 5.6299e-01 (5.1318e-01)	Acc@1  78.75 ( 83.15)	Acc@5  98.75 ( 98.94)
## e[11] optimizer.zero_grad (sum) time: 0.10581278800964355
## e[11]       loss.backward (sum) time: 2.135985851287842
## e[11]      optimizer.step (sum) time: 0.8831191062927246
## epoch[11] training(only) time: 9.703522443771362
# Switched to evaluate mode...
Test: [  0/100]	Time  0.215 ( 0.215)	Loss 6.0596e-01 (6.0596e-01)	Acc@1  82.00 ( 82.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.016 ( 0.032)	Loss 6.8799e-01 (5.6090e-01)	Acc@1  78.00 ( 79.73)	Acc@5  97.00 ( 98.82)
Test: [ 20/100]	Time  0.008 ( 0.022)	Loss 7.1143e-01 (5.6269e-01)	Acc@1  77.00 ( 80.14)	Acc@5  96.00 ( 98.67)
Test: [ 30/100]	Time  0.009 ( 0.020)	Loss 6.1035e-01 (5.6646e-01)	Acc@1  74.00 ( 80.42)	Acc@5  98.00 ( 98.74)
Test: [ 40/100]	Time  0.016 ( 0.018)	Loss 5.5664e-01 (5.6356e-01)	Acc@1  80.00 ( 80.54)	Acc@5  98.00 ( 98.76)
Test: [ 50/100]	Time  0.011 ( 0.017)	Loss 4.3481e-01 (5.5933e-01)	Acc@1  87.00 ( 80.55)	Acc@5  98.00 ( 98.76)
Test: [ 60/100]	Time  0.013 ( 0.016)	Loss 5.7568e-01 (5.6124e-01)	Acc@1  75.00 ( 80.52)	Acc@5  99.00 ( 98.72)
Test: [ 70/100]	Time  0.008 ( 0.016)	Loss 6.9873e-01 (5.6591e-01)	Acc@1  75.00 ( 80.39)	Acc@5  99.00 ( 98.72)
Test: [ 80/100]	Time  0.008 ( 0.015)	Loss 5.0146e-01 (5.6347e-01)	Acc@1  83.00 ( 80.49)	Acc@5 100.00 ( 98.78)
Test: [ 90/100]	Time  0.008 ( 0.015)	Loss 4.9731e-01 (5.6643e-01)	Acc@1  84.00 ( 80.44)	Acc@5 100.00 ( 98.78)
 * Acc@1 80.470 Acc@5 98.820
### epoch[11] execution time: 11.326932668685913
EPOCH 12
# current learning rate: 0.1
# Switched to train mode...
Epoch: [12][  0/391]	Time  0.189 ( 0.189)	Data  0.168 ( 0.168)	Loss 4.2212e-01 (4.2212e-01)	Acc@1  85.94 ( 85.94)	Acc@5  99.22 ( 99.22)
Epoch: [12][ 10/391]	Time  0.018 ( 0.038)	Data  0.001 ( 0.017)	Loss 5.0049e-01 (4.7217e-01)	Acc@1  88.28 ( 85.58)	Acc@5  97.66 ( 99.22)
Epoch: [12][ 20/391]	Time  0.042 ( 0.031)	Data  0.009 ( 0.010)	Loss 2.9150e-01 (4.5081e-01)	Acc@1  91.41 ( 85.34)	Acc@5  99.22 ( 99.18)
Epoch: [12][ 30/391]	Time  0.019 ( 0.029)	Data  0.002 ( 0.007)	Loss 3.9453e-01 (4.6816e-01)	Acc@1  86.72 ( 84.95)	Acc@5 100.00 ( 99.12)
Epoch: [12][ 40/391]	Time  0.018 ( 0.028)	Data  0.001 ( 0.006)	Loss 4.2334e-01 (4.7732e-01)	Acc@1  85.94 ( 84.47)	Acc@5 100.00 ( 99.07)
Epoch: [12][ 50/391]	Time  0.025 ( 0.027)	Data  0.003 ( 0.005)	Loss 4.8682e-01 (4.9233e-01)	Acc@1  80.47 ( 83.92)	Acc@5  99.22 ( 98.99)
Epoch: [12][ 60/391]	Time  0.032 ( 0.026)	Data  0.004 ( 0.005)	Loss 4.2163e-01 (4.9560e-01)	Acc@1  84.38 ( 83.68)	Acc@5  98.44 ( 99.01)
Epoch: [12][ 70/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 4.8413e-01 (4.9736e-01)	Acc@1  82.03 ( 83.46)	Acc@5  98.44 ( 99.02)
Epoch: [12][ 80/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.004)	Loss 4.9536e-01 (4.9682e-01)	Acc@1  82.03 ( 83.43)	Acc@5 100.00 ( 99.05)
Epoch: [12][ 90/391]	Time  0.034 ( 0.026)	Data  0.001 ( 0.004)	Loss 3.9355e-01 (4.9876e-01)	Acc@1  85.94 ( 83.43)	Acc@5  99.22 ( 99.03)
Epoch: [12][100/391]	Time  0.032 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.4922e-01 (4.9299e-01)	Acc@1  82.81 ( 83.56)	Acc@5 100.00 ( 99.06)
Epoch: [12][110/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.3174e-01 (4.9222e-01)	Acc@1  80.47 ( 83.57)	Acc@5  99.22 ( 99.04)
Epoch: [12][120/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.5327e-01 (4.9052e-01)	Acc@1  89.84 ( 83.70)	Acc@5  99.22 ( 99.03)
Epoch: [12][130/391]	Time  0.044 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.3076e-01 (4.8860e-01)	Acc@1  78.91 ( 83.73)	Acc@5  98.44 ( 99.04)
Epoch: [12][140/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.9053e-01 (4.8264e-01)	Acc@1  91.41 ( 83.89)	Acc@5  98.44 ( 99.06)
Epoch: [12][150/391]	Time  0.026 ( 0.025)	Data  0.002 ( 0.003)	Loss 5.1758e-01 (4.8652e-01)	Acc@1  84.38 ( 83.77)	Acc@5 100.00 ( 99.07)
Epoch: [12][160/391]	Time  0.025 ( 0.025)	Data  0.004 ( 0.003)	Loss 4.8169e-01 (4.9068e-01)	Acc@1  81.25 ( 83.68)	Acc@5 100.00 ( 99.04)
Epoch: [12][170/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 4.5020e-01 (4.9058e-01)	Acc@1  83.59 ( 83.66)	Acc@5 100.00 ( 99.07)
Epoch: [12][180/391]	Time  0.030 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.8525e-01 (4.8968e-01)	Acc@1  86.72 ( 83.71)	Acc@5 100.00 ( 99.09)
Epoch: [12][190/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.5864e-01 (4.8958e-01)	Acc@1  86.72 ( 83.70)	Acc@5  99.22 ( 99.10)
Epoch: [12][200/391]	Time  0.039 ( 0.025)	Data  0.005 ( 0.003)	Loss 5.5322e-01 (4.8860e-01)	Acc@1  81.25 ( 83.76)	Acc@5  99.22 ( 99.12)
Epoch: [12][210/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.8403e-01 (4.8655e-01)	Acc@1  84.38 ( 83.87)	Acc@5 100.00 ( 99.10)
Epoch: [12][220/391]	Time  0.037 ( 0.025)	Data  0.003 ( 0.003)	Loss 6.0889e-01 (4.8734e-01)	Acc@1  82.03 ( 83.83)	Acc@5  99.22 ( 99.10)
Epoch: [12][230/391]	Time  0.019 ( 0.025)	Data  0.003 ( 0.003)	Loss 3.4448e-01 (4.8655e-01)	Acc@1  89.84 ( 83.87)	Acc@5 100.00 ( 99.09)
Epoch: [12][240/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.2871e-01 (4.8597e-01)	Acc@1  85.16 ( 83.90)	Acc@5  99.22 ( 99.11)
Epoch: [12][250/391]	Time  0.025 ( 0.025)	Data  0.000 ( 0.003)	Loss 4.5459e-01 (4.8709e-01)	Acc@1  83.59 ( 83.87)	Acc@5  99.22 ( 99.10)
Epoch: [12][260/391]	Time  0.018 ( 0.025)	Data  0.002 ( 0.003)	Loss 5.5469e-01 (4.8595e-01)	Acc@1  81.25 ( 83.92)	Acc@5  99.22 ( 99.11)
Epoch: [12][270/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.0107e-01 (4.8573e-01)	Acc@1  81.25 ( 83.94)	Acc@5  96.88 ( 99.09)
Epoch: [12][280/391]	Time  0.020 ( 0.025)	Data  0.002 ( 0.002)	Loss 6.1572e-01 (4.8556e-01)	Acc@1  82.81 ( 83.96)	Acc@5  98.44 ( 99.07)
Epoch: [12][290/391]	Time  0.025 ( 0.025)	Data  0.000 ( 0.002)	Loss 5.7422e-01 (4.8512e-01)	Acc@1  78.91 ( 83.99)	Acc@5 100.00 ( 99.06)
Epoch: [12][300/391]	Time  0.028 ( 0.025)	Data  0.001 ( 0.002)	Loss 6.5283e-01 (4.8675e-01)	Acc@1  80.47 ( 83.95)	Acc@5  97.66 ( 99.05)
Epoch: [12][310/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.8257e-01 (4.8661e-01)	Acc@1  87.50 ( 83.95)	Acc@5  99.22 ( 99.05)
Epoch: [12][320/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 5.3955e-01 (4.8725e-01)	Acc@1  80.47 ( 83.91)	Acc@5  97.66 ( 99.04)
Epoch: [12][330/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 6.0791e-01 (4.8830e-01)	Acc@1  80.47 ( 83.90)	Acc@5  97.66 ( 99.01)
Epoch: [12][340/391]	Time  0.033 ( 0.025)	Data  0.002 ( 0.002)	Loss 6.2842e-01 (4.8794e-01)	Acc@1  75.00 ( 83.87)	Acc@5  98.44 ( 99.01)
Epoch: [12][350/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.8965e-01 (4.8803e-01)	Acc@1  89.06 ( 83.85)	Acc@5  99.22 ( 99.00)
Epoch: [12][360/391]	Time  0.021 ( 0.024)	Data  0.001 ( 0.002)	Loss 6.4307e-01 (4.8715e-01)	Acc@1  79.69 ( 83.89)	Acc@5  98.44 ( 99.00)
Epoch: [12][370/391]	Time  0.023 ( 0.024)	Data  0.001 ( 0.002)	Loss 5.9473e-01 (4.8774e-01)	Acc@1  82.81 ( 83.89)	Acc@5  99.22 ( 99.01)
Epoch: [12][380/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 4.0601e-01 (4.8742e-01)	Acc@1  87.50 ( 83.91)	Acc@5  99.22 ( 99.00)
Epoch: [12][390/391]	Time  0.016 ( 0.024)	Data  0.001 ( 0.002)	Loss 4.8682e-01 (4.8705e-01)	Acc@1  85.00 ( 83.92)	Acc@5 100.00 ( 99.01)
## e[12] optimizer.zero_grad (sum) time: 0.10610342025756836
## e[12]       loss.backward (sum) time: 2.166921854019165
## e[12]      optimizer.step (sum) time: 0.8934860229492188
## epoch[12] training(only) time: 9.661963701248169
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 6.3281e-01 (6.3281e-01)	Acc@1  82.00 ( 82.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.009 ( 0.027)	Loss 6.2305e-01 (6.1048e-01)	Acc@1  79.00 ( 80.64)	Acc@5  96.00 ( 98.45)
Test: [ 20/100]	Time  0.011 ( 0.020)	Loss 7.5928e-01 (6.2835e-01)	Acc@1  73.00 ( 79.43)	Acc@5  99.00 ( 98.33)
Test: [ 30/100]	Time  0.018 ( 0.018)	Loss 6.8945e-01 (6.4701e-01)	Acc@1  77.00 ( 79.16)	Acc@5  97.00 ( 98.35)
Test: [ 40/100]	Time  0.008 ( 0.016)	Loss 5.7129e-01 (6.3837e-01)	Acc@1  80.00 ( 79.37)	Acc@5  99.00 ( 98.39)
Test: [ 50/100]	Time  0.013 ( 0.016)	Loss 5.4346e-01 (6.2879e-01)	Acc@1  79.00 ( 79.80)	Acc@5  99.00 ( 98.39)
Test: [ 60/100]	Time  0.008 ( 0.015)	Loss 4.9976e-01 (6.3066e-01)	Acc@1  86.00 ( 79.93)	Acc@5  99.00 ( 98.36)
Test: [ 70/100]	Time  0.008 ( 0.015)	Loss 5.2246e-01 (6.2237e-01)	Acc@1  87.00 ( 80.20)	Acc@5  99.00 ( 98.39)
Test: [ 80/100]	Time  0.016 ( 0.015)	Loss 5.6006e-01 (6.1457e-01)	Acc@1  83.00 ( 80.37)	Acc@5  99.00 ( 98.38)
Test: [ 90/100]	Time  0.008 ( 0.015)	Loss 5.1758e-01 (6.1300e-01)	Acc@1  81.00 ( 80.36)	Acc@5  97.00 ( 98.38)
 * Acc@1 80.440 Acc@5 98.420
### epoch[12] execution time: 11.254046201705933
EPOCH 13
# current learning rate: 0.1
# Switched to train mode...
Epoch: [13][  0/391]	Time  0.196 ( 0.196)	Data  0.169 ( 0.169)	Loss 5.6787e-01 (5.6787e-01)	Acc@1  82.81 ( 82.81)	Acc@5  98.44 ( 98.44)
Epoch: [13][ 10/391]	Time  0.019 ( 0.039)	Data  0.001 ( 0.017)	Loss 5.7568e-01 (4.5827e-01)	Acc@1  81.25 ( 84.38)	Acc@5  99.22 ( 99.22)
Epoch: [13][ 20/391]	Time  0.022 ( 0.032)	Data  0.001 ( 0.010)	Loss 3.7964e-01 (4.7017e-01)	Acc@1  89.84 ( 84.49)	Acc@5 100.00 ( 99.33)
Epoch: [13][ 30/391]	Time  0.038 ( 0.030)	Data  0.005 ( 0.007)	Loss 2.6318e-01 (4.4653e-01)	Acc@1  90.62 ( 85.13)	Acc@5 100.00 ( 99.37)
Epoch: [13][ 40/391]	Time  0.018 ( 0.028)	Data  0.001 ( 0.006)	Loss 3.3521e-01 (4.3305e-01)	Acc@1  89.06 ( 85.63)	Acc@5  98.44 ( 99.39)
Epoch: [13][ 50/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.005)	Loss 3.5181e-01 (4.3975e-01)	Acc@1  86.72 ( 85.45)	Acc@5 100.00 ( 99.36)
Epoch: [13][ 60/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 5.3564e-01 (4.5121e-01)	Acc@1  80.47 ( 85.17)	Acc@5  98.44 ( 99.26)
Epoch: [13][ 70/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 5.2734e-01 (4.5642e-01)	Acc@1  79.69 ( 84.91)	Acc@5  99.22 ( 99.28)
Epoch: [13][ 80/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 5.0928e-01 (4.6298e-01)	Acc@1  80.47 ( 84.69)	Acc@5  99.22 ( 99.25)
Epoch: [13][ 90/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.004)	Loss 5.5225e-01 (4.6366e-01)	Acc@1  86.72 ( 84.82)	Acc@5  99.22 ( 99.27)
Epoch: [13][100/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 4.8486e-01 (4.5890e-01)	Acc@1  82.81 ( 84.98)	Acc@5  99.22 ( 99.28)
Epoch: [13][110/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.3184e-01 (4.6246e-01)	Acc@1  78.12 ( 84.82)	Acc@5 100.00 ( 99.28)
Epoch: [13][120/391]	Time  0.036 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.9683e-01 (4.6230e-01)	Acc@1  84.38 ( 84.79)	Acc@5  99.22 ( 99.29)
Epoch: [13][130/391]	Time  0.031 ( 0.025)	Data  0.000 ( 0.003)	Loss 5.9326e-01 (4.6470e-01)	Acc@1  84.38 ( 84.66)	Acc@5  99.22 ( 99.28)
Epoch: [13][140/391]	Time  0.039 ( 0.025)	Data  0.002 ( 0.003)	Loss 4.8291e-01 (4.6131e-01)	Acc@1  82.81 ( 84.75)	Acc@5 100.00 ( 99.26)
Epoch: [13][150/391]	Time  0.033 ( 0.025)	Data  0.000 ( 0.003)	Loss 5.4688e-01 (4.6105e-01)	Acc@1  82.03 ( 84.74)	Acc@5  98.44 ( 99.27)
Epoch: [13][160/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.2930e-01 (4.6462e-01)	Acc@1  85.16 ( 84.65)	Acc@5  99.22 ( 99.24)
Epoch: [13][170/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.5728e-01 (4.6496e-01)	Acc@1  83.59 ( 84.59)	Acc@5  99.22 ( 99.26)
Epoch: [13][180/391]	Time  0.035 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.2358e-01 (4.6493e-01)	Acc@1  85.16 ( 84.56)	Acc@5  99.22 ( 99.23)
Epoch: [13][190/391]	Time  0.025 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.2334e-01 (4.6374e-01)	Acc@1  88.28 ( 84.63)	Acc@5  99.22 ( 99.21)
Epoch: [13][200/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.5898e-01 (4.6408e-01)	Acc@1  85.16 ( 84.65)	Acc@5  98.44 ( 99.19)
Epoch: [13][210/391]	Time  0.017 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.4639e-01 (4.6220e-01)	Acc@1  84.38 ( 84.76)	Acc@5  99.22 ( 99.19)
Epoch: [13][220/391]	Time  0.029 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.7632e-01 (4.6507e-01)	Acc@1  84.38 ( 84.69)	Acc@5  98.44 ( 99.17)
Epoch: [13][230/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.2930e-01 (4.6563e-01)	Acc@1  84.38 ( 84.69)	Acc@5  96.88 ( 99.17)
Epoch: [13][240/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 4.1455e-01 (4.6402e-01)	Acc@1  85.94 ( 84.74)	Acc@5  99.22 ( 99.18)
Epoch: [13][250/391]	Time  0.035 ( 0.025)	Data  0.004 ( 0.003)	Loss 3.7134e-01 (4.6249e-01)	Acc@1  86.72 ( 84.73)	Acc@5  99.22 ( 99.17)
Epoch: [13][260/391]	Time  0.029 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.3936e-01 (4.6199e-01)	Acc@1  87.50 ( 84.73)	Acc@5 100.00 ( 99.18)
Epoch: [13][270/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.002)	Loss 4.9048e-01 (4.6265e-01)	Acc@1  83.59 ( 84.67)	Acc@5 100.00 ( 99.18)
Epoch: [13][280/391]	Time  0.032 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.7803e-01 (4.6356e-01)	Acc@1  85.16 ( 84.64)	Acc@5  99.22 ( 99.17)
Epoch: [13][290/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.7607e-01 (4.6440e-01)	Acc@1  82.81 ( 84.60)	Acc@5  98.44 ( 99.17)
Epoch: [13][300/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.8257e-01 (4.6484e-01)	Acc@1  89.06 ( 84.63)	Acc@5 100.00 ( 99.18)
Epoch: [13][310/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.002)	Loss 4.0356e-01 (4.6532e-01)	Acc@1  85.16 ( 84.62)	Acc@5 100.00 ( 99.19)
Epoch: [13][320/391]	Time  0.027 ( 0.025)	Data  0.002 ( 0.002)	Loss 6.5967e-01 (4.6623e-01)	Acc@1  78.91 ( 84.61)	Acc@5  98.44 ( 99.18)
Epoch: [13][330/391]	Time  0.030 ( 0.025)	Data  0.004 ( 0.002)	Loss 5.3760e-01 (4.6684e-01)	Acc@1  79.69 ( 84.58)	Acc@5  99.22 ( 99.16)
Epoch: [13][340/391]	Time  0.032 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.8901e-01 (4.6724e-01)	Acc@1  84.38 ( 84.57)	Acc@5  98.44 ( 99.16)
Epoch: [13][350/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.002)	Loss 4.3213e-01 (4.6771e-01)	Acc@1  85.94 ( 84.55)	Acc@5  99.22 ( 99.15)
Epoch: [13][360/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 4.5752e-01 (4.6725e-01)	Acc@1  89.06 ( 84.59)	Acc@5  98.44 ( 99.15)
Epoch: [13][370/391]	Time  0.027 ( 0.024)	Data  0.001 ( 0.002)	Loss 4.3750e-01 (4.6790e-01)	Acc@1  86.72 ( 84.60)	Acc@5  98.44 ( 99.15)
Epoch: [13][380/391]	Time  0.018 ( 0.024)	Data  0.002 ( 0.002)	Loss 4.6045e-01 (4.6847e-01)	Acc@1  89.84 ( 84.61)	Acc@5  97.66 ( 99.15)
Epoch: [13][390/391]	Time  0.016 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.5815e-01 (4.6736e-01)	Acc@1  91.25 ( 84.66)	Acc@5 100.00 ( 99.15)
## e[13] optimizer.zero_grad (sum) time: 0.10460424423217773
## e[13]       loss.backward (sum) time: 2.1781301498413086
## e[13]      optimizer.step (sum) time: 0.8819398880004883
## epoch[13] training(only) time: 9.650045156478882
# Switched to evaluate mode...
Test: [  0/100]	Time  0.165 ( 0.165)	Loss 5.1855e-01 (5.1855e-01)	Acc@1  82.00 ( 82.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.016 ( 0.028)	Loss 5.4004e-01 (5.3662e-01)	Acc@1  84.00 ( 82.91)	Acc@5  99.00 ( 99.09)
Test: [ 20/100]	Time  0.010 ( 0.020)	Loss 6.3330e-01 (5.3995e-01)	Acc@1  79.00 ( 82.95)	Acc@5 100.00 ( 98.86)
Test: [ 30/100]	Time  0.013 ( 0.018)	Loss 4.9683e-01 (5.5849e-01)	Acc@1  82.00 ( 82.35)	Acc@5 100.00 ( 98.97)
Test: [ 40/100]	Time  0.008 ( 0.017)	Loss 5.8398e-01 (5.6039e-01)	Acc@1  83.00 ( 82.12)	Acc@5  97.00 ( 98.90)
Test: [ 50/100]	Time  0.007 ( 0.016)	Loss 4.6143e-01 (5.5429e-01)	Acc@1  86.00 ( 82.29)	Acc@5  98.00 ( 98.92)
Test: [ 60/100]	Time  0.016 ( 0.016)	Loss 4.9927e-01 (5.5695e-01)	Acc@1  85.00 ( 82.30)	Acc@5 100.00 ( 98.95)
Test: [ 70/100]	Time  0.008 ( 0.016)	Loss 5.4736e-01 (5.4836e-01)	Acc@1  85.00 ( 82.54)	Acc@5  98.00 ( 98.96)
Test: [ 80/100]	Time  0.012 ( 0.015)	Loss 4.4775e-01 (5.4349e-01)	Acc@1  85.00 ( 82.49)	Acc@5  99.00 ( 99.01)
Test: [ 90/100]	Time  0.014 ( 0.015)	Loss 3.8086e-01 (5.3967e-01)	Acc@1  83.00 ( 82.49)	Acc@5 100.00 ( 99.07)
 * Acc@1 82.420 Acc@5 99.100
### epoch[13] execution time: 11.242716312408447
EPOCH 14
# current learning rate: 0.1
# Switched to train mode...
Epoch: [14][  0/391]	Time  0.197 ( 0.197)	Data  0.171 ( 0.171)	Loss 3.6597e-01 (3.6597e-01)	Acc@1  88.28 ( 88.28)	Acc@5 100.00 (100.00)
Epoch: [14][ 10/391]	Time  0.019 ( 0.039)	Data  0.001 ( 0.017)	Loss 3.4644e-01 (3.8512e-01)	Acc@1  89.84 ( 87.29)	Acc@5  99.22 ( 99.43)
Epoch: [14][ 20/391]	Time  0.020 ( 0.032)	Data  0.001 ( 0.010)	Loss 5.2783e-01 (4.2427e-01)	Acc@1  82.81 ( 85.68)	Acc@5 100.00 ( 99.44)
Epoch: [14][ 30/391]	Time  0.019 ( 0.029)	Data  0.002 ( 0.007)	Loss 4.4141e-01 (4.3392e-01)	Acc@1  84.38 ( 85.23)	Acc@5 100.00 ( 99.34)
Epoch: [14][ 40/391]	Time  0.017 ( 0.027)	Data  0.000 ( 0.006)	Loss 4.7485e-01 (4.4415e-01)	Acc@1  83.59 ( 84.89)	Acc@5 100.00 ( 99.31)
Epoch: [14][ 50/391]	Time  0.032 ( 0.027)	Data  0.001 ( 0.005)	Loss 2.8589e-01 (4.3672e-01)	Acc@1  89.84 ( 84.96)	Acc@5 100.00 ( 99.33)
Epoch: [14][ 60/391]	Time  0.017 ( 0.026)	Data  0.000 ( 0.005)	Loss 4.2334e-01 (4.3324e-01)	Acc@1  84.38 ( 85.08)	Acc@5  99.22 ( 99.26)
Epoch: [14][ 70/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 5.8008e-01 (4.4108e-01)	Acc@1  82.03 ( 84.89)	Acc@5 100.00 ( 99.24)
Epoch: [14][ 80/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.004)	Loss 4.2432e-01 (4.4203e-01)	Acc@1  82.81 ( 84.85)	Acc@5 100.00 ( 99.23)
Epoch: [14][ 90/391]	Time  0.047 ( 0.026)	Data  0.010 ( 0.004)	Loss 3.9673e-01 (4.3909e-01)	Acc@1  88.28 ( 85.09)	Acc@5  99.22 ( 99.18)
Epoch: [14][100/391]	Time  0.034 ( 0.025)	Data  0.001 ( 0.004)	Loss 5.2930e-01 (4.3946e-01)	Acc@1  83.59 ( 85.08)	Acc@5  98.44 ( 99.16)
Epoch: [14][110/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.1123e-01 (4.3958e-01)	Acc@1  83.59 ( 85.15)	Acc@5  96.09 ( 99.17)
Epoch: [14][120/391]	Time  0.027 ( 0.025)	Data  0.002 ( 0.003)	Loss 3.9136e-01 (4.3636e-01)	Acc@1  87.50 ( 85.34)	Acc@5 100.00 ( 99.21)
Epoch: [14][130/391]	Time  0.027 ( 0.025)	Data  0.002 ( 0.003)	Loss 3.9941e-01 (4.3637e-01)	Acc@1  85.94 ( 85.39)	Acc@5 100.00 ( 99.17)
Epoch: [14][140/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.0210e-01 (4.3779e-01)	Acc@1  85.94 ( 85.41)	Acc@5  99.22 ( 99.17)
Epoch: [14][150/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.5620e-01 (4.3747e-01)	Acc@1  88.28 ( 85.42)	Acc@5  99.22 ( 99.19)
Epoch: [14][160/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.7231e-01 (4.3821e-01)	Acc@1  85.16 ( 85.34)	Acc@5  99.22 ( 99.18)
Epoch: [14][170/391]	Time  0.032 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.5850e-01 (4.3931e-01)	Acc@1  83.59 ( 85.31)	Acc@5  98.44 ( 99.18)
Epoch: [14][180/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.5898e-01 (4.4146e-01)	Acc@1  87.50 ( 85.23)	Acc@5  98.44 ( 99.17)
Epoch: [14][190/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.3643e-01 (4.4266e-01)	Acc@1  90.62 ( 85.22)	Acc@5  99.22 ( 99.15)
Epoch: [14][200/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.3516e-01 (4.4454e-01)	Acc@1  82.81 ( 85.13)	Acc@5  97.66 ( 99.14)
Epoch: [14][210/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 4.9756e-01 (4.4487e-01)	Acc@1  84.38 ( 85.11)	Acc@5  97.66 ( 99.15)
Epoch: [14][220/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.5645e-01 (4.4308e-01)	Acc@1  90.62 ( 85.17)	Acc@5  99.22 ( 99.14)
Epoch: [14][230/391]	Time  0.040 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.5791e-01 (4.4104e-01)	Acc@1  83.59 ( 85.22)	Acc@5 100.00 ( 99.15)
Epoch: [14][240/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.7290e-01 (4.4017e-01)	Acc@1  85.16 ( 85.28)	Acc@5  98.44 ( 99.15)
Epoch: [14][250/391]	Time  0.017 ( 0.024)	Data  0.000 ( 0.002)	Loss 5.8057e-01 (4.4290e-01)	Acc@1  81.25 ( 85.22)	Acc@5  96.09 ( 99.14)
Epoch: [14][260/391]	Time  0.031 ( 0.025)	Data  0.000 ( 0.002)	Loss 4.8486e-01 (4.4217e-01)	Acc@1  85.94 ( 85.26)	Acc@5  97.66 ( 99.13)
Epoch: [14][270/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.4766e-01 (4.4241e-01)	Acc@1  92.19 ( 85.26)	Acc@5  98.44 ( 99.10)
Epoch: [14][280/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 4.2432e-01 (4.4135e-01)	Acc@1  83.59 ( 85.27)	Acc@5 100.00 ( 99.11)
Epoch: [14][290/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 4.4287e-01 (4.4238e-01)	Acc@1  86.72 ( 85.24)	Acc@5  99.22 ( 99.11)
Epoch: [14][300/391]	Time  0.024 ( 0.024)	Data  0.001 ( 0.002)	Loss 5.5811e-01 (4.4214e-01)	Acc@1  82.03 ( 85.28)	Acc@5  99.22 ( 99.12)
Epoch: [14][310/391]	Time  0.029 ( 0.024)	Data  0.001 ( 0.002)	Loss 5.1611e-01 (4.4203e-01)	Acc@1  85.16 ( 85.30)	Acc@5  99.22 ( 99.12)
Epoch: [14][320/391]	Time  0.021 ( 0.024)	Data  0.001 ( 0.002)	Loss 4.9707e-01 (4.4147e-01)	Acc@1  85.94 ( 85.34)	Acc@5 100.00 ( 99.12)
Epoch: [14][330/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 4.4458e-01 (4.4057e-01)	Acc@1  83.59 ( 85.35)	Acc@5  99.22 ( 99.13)
Epoch: [14][340/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 4.4263e-01 (4.4136e-01)	Acc@1  86.72 ( 85.35)	Acc@5 100.00 ( 99.13)
Epoch: [14][350/391]	Time  0.032 ( 0.024)	Data  0.003 ( 0.002)	Loss 3.5205e-01 (4.4195e-01)	Acc@1  87.50 ( 85.35)	Acc@5  98.44 ( 99.11)
Epoch: [14][360/391]	Time  0.031 ( 0.024)	Data  0.001 ( 0.002)	Loss 5.1172e-01 (4.4317e-01)	Acc@1  81.25 ( 85.32)	Acc@5 100.00 ( 99.10)
Epoch: [14][370/391]	Time  0.019 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.6865e-01 (4.4358e-01)	Acc@1  89.06 ( 85.28)	Acc@5  97.66 ( 99.10)
Epoch: [14][380/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.8062e-01 (4.4307e-01)	Acc@1  84.38 ( 85.27)	Acc@5 100.00 ( 99.10)
Epoch: [14][390/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.0249e-01 (4.4280e-01)	Acc@1  88.75 ( 85.28)	Acc@5 100.00 ( 99.11)
## e[14] optimizer.zero_grad (sum) time: 0.10348176956176758
## e[14]       loss.backward (sum) time: 2.1656460762023926
## e[14]      optimizer.step (sum) time: 0.8875353336334229
## epoch[14] training(only) time: 9.632288932800293
# Switched to evaluate mode...
Test: [  0/100]	Time  0.210 ( 0.210)	Loss 4.8535e-01 (4.8535e-01)	Acc@1  86.00 ( 86.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.017 ( 0.032)	Loss 7.4512e-01 (5.9320e-01)	Acc@1  80.00 ( 82.00)	Acc@5  98.00 ( 98.45)
Test: [ 20/100]	Time  0.009 ( 0.022)	Loss 8.6914e-01 (5.9697e-01)	Acc@1  71.00 ( 81.57)	Acc@5  98.00 ( 98.10)
Test: [ 30/100]	Time  0.008 ( 0.019)	Loss 6.9336e-01 (6.2312e-01)	Acc@1  78.00 ( 80.65)	Acc@5  98.00 ( 98.06)
Test: [ 40/100]	Time  0.013 ( 0.018)	Loss 6.5625e-01 (6.2733e-01)	Acc@1  77.00 ( 80.41)	Acc@5  98.00 ( 97.98)
Test: [ 50/100]	Time  0.016 ( 0.017)	Loss 5.3564e-01 (6.1660e-01)	Acc@1  88.00 ( 80.47)	Acc@5  97.00 ( 98.02)
Test: [ 60/100]	Time  0.008 ( 0.016)	Loss 5.8545e-01 (6.1327e-01)	Acc@1  80.00 ( 80.26)	Acc@5 100.00 ( 98.16)
Test: [ 70/100]	Time  0.009 ( 0.016)	Loss 5.5859e-01 (6.1845e-01)	Acc@1  81.00 ( 80.13)	Acc@5  99.00 ( 98.14)
Test: [ 80/100]	Time  0.015 ( 0.015)	Loss 7.5391e-01 (6.1494e-01)	Acc@1  74.00 ( 80.06)	Acc@5  96.00 ( 98.15)
Test: [ 90/100]	Time  0.008 ( 0.015)	Loss 5.2441e-01 (6.1659e-01)	Acc@1  83.00 ( 80.05)	Acc@5 100.00 ( 98.12)
 * Acc@1 80.180 Acc@5 98.050
### epoch[14] execution time: 11.228533506393433
EPOCH 15
# current learning rate: 0.1
# Switched to train mode...
Epoch: [15][  0/391]	Time  0.196 ( 0.196)	Data  0.178 ( 0.178)	Loss 4.1626e-01 (4.1626e-01)	Acc@1  87.50 ( 87.50)	Acc@5  99.22 ( 99.22)
Epoch: [15][ 10/391]	Time  0.019 ( 0.039)	Data  0.001 ( 0.018)	Loss 4.5679e-01 (4.0141e-01)	Acc@1  82.81 ( 86.08)	Acc@5  98.44 ( 99.43)
Epoch: [15][ 20/391]	Time  0.018 ( 0.030)	Data  0.002 ( 0.010)	Loss 3.2446e-01 (4.0705e-01)	Acc@1  88.28 ( 86.50)	Acc@5  99.22 ( 99.44)
Epoch: [15][ 30/391]	Time  0.031 ( 0.029)	Data  0.003 ( 0.008)	Loss 5.4102e-01 (4.3296e-01)	Acc@1  81.25 ( 85.61)	Acc@5  99.22 ( 99.37)
Epoch: [15][ 40/391]	Time  0.037 ( 0.028)	Data  0.005 ( 0.006)	Loss 5.1221e-01 (4.3153e-01)	Acc@1  84.38 ( 85.79)	Acc@5 100.00 ( 99.33)
Epoch: [15][ 50/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.005)	Loss 3.7549e-01 (4.2821e-01)	Acc@1  89.06 ( 86.08)	Acc@5 100.00 ( 99.28)
Epoch: [15][ 60/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.005)	Loss 5.1025e-01 (4.3113e-01)	Acc@1  85.16 ( 86.04)	Acc@5  96.88 ( 99.22)
Epoch: [15][ 70/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.004)	Loss 5.5908e-01 (4.3553e-01)	Acc@1  82.03 ( 85.88)	Acc@5  97.66 ( 99.21)
Epoch: [15][ 80/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.004)	Loss 5.4590e-01 (4.3829e-01)	Acc@1  82.81 ( 85.75)	Acc@5  99.22 ( 99.20)
Epoch: [15][ 90/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.004)	Loss 3.6523e-01 (4.3733e-01)	Acc@1  89.06 ( 85.81)	Acc@5  98.44 ( 99.18)
Epoch: [15][100/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.004)	Loss 3.8159e-01 (4.3728e-01)	Acc@1  88.28 ( 85.84)	Acc@5 100.00 ( 99.17)
Epoch: [15][110/391]	Time  0.035 ( 0.025)	Data  0.002 ( 0.003)	Loss 4.4385e-01 (4.3350e-01)	Acc@1  87.50 ( 85.94)	Acc@5  97.66 ( 99.16)
Epoch: [15][120/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.8501e-01 (4.3343e-01)	Acc@1  86.72 ( 85.98)	Acc@5 100.00 ( 99.17)
Epoch: [15][130/391]	Time  0.025 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.4727e-01 (4.3392e-01)	Acc@1  85.94 ( 85.90)	Acc@5  99.22 ( 99.15)
Epoch: [15][140/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.1763e-01 (4.2930e-01)	Acc@1  91.41 ( 86.08)	Acc@5 100.00 ( 99.17)
Epoch: [15][150/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.1821e-01 (4.3398e-01)	Acc@1  85.94 ( 85.92)	Acc@5  99.22 ( 99.15)
Epoch: [15][160/391]	Time  0.028 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.3325e-01 (4.3499e-01)	Acc@1  89.06 ( 85.88)	Acc@5 100.00 ( 99.16)
Epoch: [15][170/391]	Time  0.028 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.1465e-01 (4.3700e-01)	Acc@1  81.25 ( 85.81)	Acc@5  99.22 ( 99.16)
Epoch: [15][180/391]	Time  0.026 ( 0.025)	Data  0.002 ( 0.003)	Loss 3.7817e-01 (4.3659e-01)	Acc@1  85.94 ( 85.79)	Acc@5 100.00 ( 99.18)
Epoch: [15][190/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.7241e-01 (4.3643e-01)	Acc@1  85.94 ( 85.77)	Acc@5  98.44 ( 99.17)
Epoch: [15][200/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.6260e-01 (4.3783e-01)	Acc@1  78.12 ( 85.70)	Acc@5  99.22 ( 99.17)
Epoch: [15][210/391]	Time  0.027 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.6646e-01 (4.3706e-01)	Acc@1  89.06 ( 85.78)	Acc@5  99.22 ( 99.17)
Epoch: [15][220/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 4.3359e-01 (4.3889e-01)	Acc@1  88.28 ( 85.72)	Acc@5  99.22 ( 99.16)
Epoch: [15][230/391]	Time  0.026 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.0176e-01 (4.4018e-01)	Acc@1  90.62 ( 85.68)	Acc@5  99.22 ( 99.15)
Epoch: [15][240/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 5.5615e-01 (4.3976e-01)	Acc@1  79.69 ( 85.67)	Acc@5  98.44 ( 99.15)
Epoch: [15][250/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.5596e-01 (4.3912e-01)	Acc@1  88.28 ( 85.66)	Acc@5 100.00 ( 99.16)
Epoch: [15][260/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.0088e-01 (4.3795e-01)	Acc@1  85.94 ( 85.67)	Acc@5 100.00 ( 99.17)
Epoch: [15][270/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.1016e-01 (4.3769e-01)	Acc@1  83.59 ( 85.68)	Acc@5 100.00 ( 99.18)
Epoch: [15][280/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.0493e-01 (4.3765e-01)	Acc@1  89.84 ( 85.67)	Acc@5 100.00 ( 99.19)
Epoch: [15][290/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.8257e-01 (4.3566e-01)	Acc@1  89.84 ( 85.73)	Acc@5 100.00 ( 99.19)
Epoch: [15][300/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 6.7236e-01 (4.3686e-01)	Acc@1  81.25 ( 85.71)	Acc@5  96.88 ( 99.17)
Epoch: [15][310/391]	Time  0.024 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.6191e-01 (4.3649e-01)	Acc@1  82.03 ( 85.73)	Acc@5  99.22 ( 99.17)
Epoch: [15][320/391]	Time  0.039 ( 0.024)	Data  0.004 ( 0.002)	Loss 4.0063e-01 (4.3432e-01)	Acc@1  85.16 ( 85.77)	Acc@5 100.00 ( 99.18)
Epoch: [15][330/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 4.1333e-01 (4.3323e-01)	Acc@1  82.03 ( 85.79)	Acc@5 100.00 ( 99.19)
Epoch: [15][340/391]	Time  0.030 ( 0.024)	Data  0.001 ( 0.002)	Loss 4.5532e-01 (4.3318e-01)	Acc@1  86.72 ( 85.81)	Acc@5 100.00 ( 99.20)
Epoch: [15][350/391]	Time  0.025 ( 0.024)	Data  0.002 ( 0.002)	Loss 5.8789e-01 (4.3322e-01)	Acc@1  82.03 ( 85.81)	Acc@5  98.44 ( 99.19)
Epoch: [15][360/391]	Time  0.024 ( 0.024)	Data  0.001 ( 0.002)	Loss 4.2773e-01 (4.3438e-01)	Acc@1  86.72 ( 85.77)	Acc@5  97.66 ( 99.17)
Epoch: [15][370/391]	Time  0.024 ( 0.024)	Data  0.001 ( 0.002)	Loss 4.3115e-01 (4.3423e-01)	Acc@1  86.72 ( 85.78)	Acc@5  99.22 ( 99.18)
Epoch: [15][380/391]	Time  0.018 ( 0.024)	Data  0.002 ( 0.002)	Loss 3.3398e-01 (4.3379e-01)	Acc@1  90.62 ( 85.81)	Acc@5 100.00 ( 99.18)
Epoch: [15][390/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.8330e-01 (4.3367e-01)	Acc@1  87.50 ( 85.83)	Acc@5 100.00 ( 99.18)
## e[15] optimizer.zero_grad (sum) time: 0.10466194152832031
## e[15]       loss.backward (sum) time: 2.145935297012329
## e[15]      optimizer.step (sum) time: 0.8943700790405273
## epoch[15] training(only) time: 9.644562244415283
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 4.9683e-01 (4.9683e-01)	Acc@1  82.00 ( 82.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.014 ( 0.027)	Loss 6.3867e-01 (5.1278e-01)	Acc@1  79.00 ( 83.36)	Acc@5  99.00 ( 99.00)
Test: [ 20/100]	Time  0.016 ( 0.020)	Loss 5.6641e-01 (5.1795e-01)	Acc@1  82.00 ( 83.48)	Acc@5  98.00 ( 98.57)
Test: [ 30/100]	Time  0.015 ( 0.018)	Loss 4.9146e-01 (5.3093e-01)	Acc@1  82.00 ( 83.16)	Acc@5 100.00 ( 98.58)
Test: [ 40/100]	Time  0.017 ( 0.016)	Loss 6.3379e-01 (5.3928e-01)	Acc@1  81.00 ( 83.20)	Acc@5  98.00 ( 98.59)
Test: [ 50/100]	Time  0.008 ( 0.016)	Loss 4.8511e-01 (5.3581e-01)	Acc@1  85.00 ( 83.12)	Acc@5  98.00 ( 98.59)
Test: [ 60/100]	Time  0.008 ( 0.015)	Loss 6.7627e-01 (5.4228e-01)	Acc@1  77.00 ( 83.10)	Acc@5  99.00 ( 98.66)
Test: [ 70/100]	Time  0.008 ( 0.015)	Loss 4.6997e-01 (5.3756e-01)	Acc@1  84.00 ( 83.10)	Acc@5  99.00 ( 98.68)
Test: [ 80/100]	Time  0.020 ( 0.015)	Loss 4.1724e-01 (5.3023e-01)	Acc@1  86.00 ( 83.22)	Acc@5 100.00 ( 98.80)
Test: [ 90/100]	Time  0.012 ( 0.015)	Loss 3.5498e-01 (5.3260e-01)	Acc@1  88.00 ( 83.19)	Acc@5 100.00 ( 98.87)
 * Acc@1 83.290 Acc@5 98.890
### epoch[15] execution time: 11.20359468460083
EPOCH 16
# current learning rate: 0.1
# Switched to train mode...
Epoch: [16][  0/391]	Time  0.192 ( 0.192)	Data  0.173 ( 0.173)	Loss 5.7031e-01 (5.7031e-01)	Acc@1  81.25 ( 81.25)	Acc@5  97.66 ( 97.66)
Epoch: [16][ 10/391]	Time  0.018 ( 0.039)	Data  0.001 ( 0.018)	Loss 3.4155e-01 (4.4937e-01)	Acc@1  89.84 ( 85.30)	Acc@5  98.44 ( 98.79)
Epoch: [16][ 20/391]	Time  0.018 ( 0.031)	Data  0.001 ( 0.010)	Loss 3.7842e-01 (4.1024e-01)	Acc@1  85.94 ( 86.31)	Acc@5  99.22 ( 99.18)
Epoch: [16][ 30/391]	Time  0.018 ( 0.029)	Data  0.001 ( 0.007)	Loss 2.9004e-01 (3.9567e-01)	Acc@1  90.62 ( 86.59)	Acc@5  97.66 ( 99.22)
Epoch: [16][ 40/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.006)	Loss 4.0601e-01 (3.9235e-01)	Acc@1  88.28 ( 86.53)	Acc@5 100.00 ( 99.33)
Epoch: [16][ 50/391]	Time  0.035 ( 0.027)	Data  0.002 ( 0.005)	Loss 4.7266e-01 (3.9866e-01)	Acc@1  87.50 ( 86.53)	Acc@5  98.44 ( 99.31)
Epoch: [16][ 60/391]	Time  0.034 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.4790e-01 (4.0999e-01)	Acc@1  89.06 ( 86.40)	Acc@5 100.00 ( 99.30)
Epoch: [16][ 70/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 3.1958e-01 (4.0943e-01)	Acc@1  89.06 ( 86.38)	Acc@5 100.00 ( 99.27)
Epoch: [16][ 80/391]	Time  0.017 ( 0.026)	Data  0.000 ( 0.004)	Loss 3.8647e-01 (4.0728e-01)	Acc@1  88.28 ( 86.56)	Acc@5 100.00 ( 99.26)
Epoch: [16][ 90/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.004)	Loss 4.7534e-01 (4.0335e-01)	Acc@1  85.16 ( 86.72)	Acc@5  98.44 ( 99.26)
Epoch: [16][100/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.004)	Loss 5.4736e-01 (4.0603e-01)	Acc@1  84.38 ( 86.76)	Acc@5  99.22 ( 99.23)
Epoch: [16][110/391]	Time  0.039 ( 0.025)	Data  0.005 ( 0.003)	Loss 3.6182e-01 (4.0299e-01)	Acc@1  86.72 ( 86.81)	Acc@5 100.00 ( 99.25)
Epoch: [16][120/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.7661e-01 (3.9958e-01)	Acc@1  90.62 ( 86.93)	Acc@5  99.22 ( 99.26)
Epoch: [16][130/391]	Time  0.020 ( 0.025)	Data  0.003 ( 0.003)	Loss 5.5762e-01 (3.9953e-01)	Acc@1  81.25 ( 86.92)	Acc@5 100.00 ( 99.29)
Epoch: [16][140/391]	Time  0.040 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.9658e-01 (4.0677e-01)	Acc@1  85.94 ( 86.70)	Acc@5  99.22 ( 99.24)
Epoch: [16][150/391]	Time  0.032 ( 0.025)	Data  0.000 ( 0.003)	Loss 5.2295e-01 (4.0841e-01)	Acc@1  87.50 ( 86.73)	Acc@5  99.22 ( 99.21)
Epoch: [16][160/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.3564e-01 (4.0903e-01)	Acc@1  85.16 ( 86.74)	Acc@5  99.22 ( 99.23)
Epoch: [16][170/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.3223e-01 (4.0965e-01)	Acc@1  82.81 ( 86.69)	Acc@5  97.66 ( 99.22)
Epoch: [16][180/391]	Time  0.038 ( 0.025)	Data  0.005 ( 0.003)	Loss 4.4214e-01 (4.1073e-01)	Acc@1  85.94 ( 86.67)	Acc@5  97.66 ( 99.21)
Epoch: [16][190/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.5156e-01 (4.1144e-01)	Acc@1  89.06 ( 86.65)	Acc@5  98.44 ( 99.22)
Epoch: [16][200/391]	Time  0.036 ( 0.025)	Data  0.010 ( 0.003)	Loss 4.5264e-01 (4.1541e-01)	Acc@1  83.59 ( 86.45)	Acc@5  97.66 ( 99.20)
Epoch: [16][210/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.2129e-01 (4.1474e-01)	Acc@1  89.84 ( 86.49)	Acc@5  99.22 ( 99.20)
Epoch: [16][220/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.0625e-01 (4.1442e-01)	Acc@1  85.94 ( 86.46)	Acc@5  99.22 ( 99.21)
Epoch: [16][230/391]	Time  0.018 ( 0.025)	Data  0.002 ( 0.003)	Loss 3.8574e-01 (4.1370e-01)	Acc@1  89.84 ( 86.48)	Acc@5  99.22 ( 99.24)
Epoch: [16][240/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 3.4619e-01 (4.1274e-01)	Acc@1  89.84 ( 86.51)	Acc@5  97.66 ( 99.23)
Epoch: [16][250/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 4.0259e-01 (4.1281e-01)	Acc@1  85.94 ( 86.52)	Acc@5  99.22 ( 99.23)
Epoch: [16][260/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.2344e-01 (4.1218e-01)	Acc@1  82.03 ( 86.53)	Acc@5  99.22 ( 99.24)
Epoch: [16][270/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.4302e-01 (4.1178e-01)	Acc@1  85.94 ( 86.51)	Acc@5 100.00 ( 99.24)
Epoch: [16][280/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.2651e-01 (4.1234e-01)	Acc@1  85.94 ( 86.52)	Acc@5  97.66 ( 99.23)
Epoch: [16][290/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.6084e-01 (4.1252e-01)	Acc@1  89.84 ( 86.54)	Acc@5  99.22 ( 99.23)
Epoch: [16][300/391]	Time  0.027 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.6582e-01 (4.1203e-01)	Acc@1  85.94 ( 86.54)	Acc@5 100.00 ( 99.24)
Epoch: [16][310/391]	Time  0.019 ( 0.024)	Data  0.002 ( 0.002)	Loss 4.2700e-01 (4.1152e-01)	Acc@1  89.06 ( 86.55)	Acc@5 100.00 ( 99.25)
Epoch: [16][320/391]	Time  0.032 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.2505e-01 (4.1161e-01)	Acc@1  86.72 ( 86.54)	Acc@5  99.22 ( 99.25)
Epoch: [16][330/391]	Time  0.043 ( 0.024)	Data  0.009 ( 0.002)	Loss 4.0869e-01 (4.1219e-01)	Acc@1  85.16 ( 86.53)	Acc@5 100.00 ( 99.24)
Epoch: [16][340/391]	Time  0.029 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.8940e-01 (4.1203e-01)	Acc@1  87.50 ( 86.53)	Acc@5  99.22 ( 99.24)
Epoch: [16][350/391]	Time  0.028 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.9795e-01 (4.1150e-01)	Acc@1  86.72 ( 86.55)	Acc@5 100.00 ( 99.24)
Epoch: [16][360/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 5.1172e-01 (4.1213e-01)	Acc@1  85.16 ( 86.52)	Acc@5  99.22 ( 99.24)
Epoch: [16][370/391]	Time  0.048 ( 0.024)	Data  0.002 ( 0.002)	Loss 4.5361e-01 (4.1161e-01)	Acc@1  83.59 ( 86.54)	Acc@5 100.00 ( 99.25)
Epoch: [16][380/391]	Time  0.026 ( 0.024)	Data  0.005 ( 0.002)	Loss 3.7427e-01 (4.1110e-01)	Acc@1  87.50 ( 86.55)	Acc@5 100.00 ( 99.26)
Epoch: [16][390/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.9478e-01 (4.1100e-01)	Acc@1  83.75 ( 86.55)	Acc@5 100.00 ( 99.27)
## e[16] optimizer.zero_grad (sum) time: 0.10503554344177246
## e[16]       loss.backward (sum) time: 2.1813175678253174
## e[16]      optimizer.step (sum) time: 0.8923821449279785
## epoch[16] training(only) time: 9.64509916305542
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 4.3457e-01 (4.3457e-01)	Acc@1  86.00 ( 86.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.017 ( 0.027)	Loss 5.4395e-01 (4.4993e-01)	Acc@1  83.00 ( 85.09)	Acc@5  98.00 ( 99.00)
Test: [ 20/100]	Time  0.013 ( 0.020)	Loss 6.6113e-01 (4.7191e-01)	Acc@1  73.00 ( 84.10)	Acc@5 100.00 ( 98.86)
Test: [ 30/100]	Time  0.014 ( 0.018)	Loss 5.5078e-01 (4.9359e-01)	Acc@1  83.00 ( 83.87)	Acc@5  98.00 ( 98.81)
Test: [ 40/100]	Time  0.008 ( 0.016)	Loss 5.9766e-01 (4.9640e-01)	Acc@1  83.00 ( 83.95)	Acc@5  99.00 ( 98.93)
Test: [ 50/100]	Time  0.007 ( 0.016)	Loss 4.1235e-01 (4.8902e-01)	Acc@1  88.00 ( 84.20)	Acc@5  99.00 ( 98.94)
Test: [ 60/100]	Time  0.016 ( 0.016)	Loss 3.9624e-01 (4.8912e-01)	Acc@1  87.00 ( 84.18)	Acc@5  99.00 ( 98.98)
Test: [ 70/100]	Time  0.008 ( 0.015)	Loss 6.4014e-01 (4.8621e-01)	Acc@1  82.00 ( 84.17)	Acc@5  98.00 ( 98.99)
Test: [ 80/100]	Time  0.010 ( 0.015)	Loss 3.8623e-01 (4.8519e-01)	Acc@1  83.00 ( 84.07)	Acc@5 100.00 ( 99.04)
Test: [ 90/100]	Time  0.008 ( 0.015)	Loss 3.9429e-01 (4.8213e-01)	Acc@1  88.00 ( 84.15)	Acc@5 100.00 ( 99.05)
 * Acc@1 84.270 Acc@5 99.130
### epoch[16] execution time: 11.21022653579712
EPOCH 17
# current learning rate: 0.1
# Switched to train mode...
Epoch: [17][  0/391]	Time  0.192 ( 0.192)	Data  0.168 ( 0.168)	Loss 3.3203e-01 (3.3203e-01)	Acc@1  89.84 ( 89.84)	Acc@5  99.22 ( 99.22)
Epoch: [17][ 10/391]	Time  0.024 ( 0.039)	Data  0.002 ( 0.017)	Loss 3.0249e-01 (3.6808e-01)	Acc@1  90.62 ( 88.14)	Acc@5  99.22 ( 99.22)
Epoch: [17][ 20/391]	Time  0.018 ( 0.032)	Data  0.001 ( 0.010)	Loss 4.4995e-01 (3.7783e-01)	Acc@1  87.50 ( 88.21)	Acc@5 100.00 ( 99.18)
Epoch: [17][ 30/391]	Time  0.035 ( 0.029)	Data  0.003 ( 0.007)	Loss 3.5303e-01 (3.8458e-01)	Acc@1  85.16 ( 87.60)	Acc@5 100.00 ( 99.17)
Epoch: [17][ 40/391]	Time  0.028 ( 0.028)	Data  0.001 ( 0.006)	Loss 4.0063e-01 (3.8440e-01)	Acc@1  85.94 ( 87.23)	Acc@5 100.00 ( 99.26)
Epoch: [17][ 50/391]	Time  0.033 ( 0.027)	Data  0.002 ( 0.005)	Loss 5.2148e-01 (4.0346e-01)	Acc@1  79.69 ( 86.50)	Acc@5 100.00 ( 99.31)
Epoch: [17][ 60/391]	Time  0.035 ( 0.027)	Data  0.001 ( 0.005)	Loss 3.6841e-01 (4.0810e-01)	Acc@1  88.28 ( 86.44)	Acc@5  99.22 ( 99.30)
Epoch: [17][ 70/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 3.0933e-01 (4.0759e-01)	Acc@1  87.50 ( 86.36)	Acc@5 100.00 ( 99.32)
Epoch: [17][ 80/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 5.1074e-01 (4.0592e-01)	Acc@1  82.81 ( 86.45)	Acc@5  98.44 ( 99.32)
Epoch: [17][ 90/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 3.5864e-01 (4.0357e-01)	Acc@1  89.84 ( 86.50)	Acc@5  99.22 ( 99.30)
Epoch: [17][100/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.1982e-01 (4.0698e-01)	Acc@1  89.84 ( 86.45)	Acc@5  99.22 ( 99.22)
Epoch: [17][110/391]	Time  0.041 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.3457e-01 (4.0454e-01)	Acc@1  85.94 ( 86.55)	Acc@5  98.44 ( 99.24)
Epoch: [17][120/391]	Time  0.029 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.7793e-01 (4.0169e-01)	Acc@1  89.06 ( 86.66)	Acc@5  99.22 ( 99.26)
Epoch: [17][130/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.5483e-01 (3.9855e-01)	Acc@1  85.16 ( 86.77)	Acc@5 100.00 ( 99.28)
Epoch: [17][140/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.0186e-01 (3.9659e-01)	Acc@1  89.06 ( 86.90)	Acc@5  99.22 ( 99.30)
Epoch: [17][150/391]	Time  0.020 ( 0.025)	Data  0.002 ( 0.003)	Loss 3.7402e-01 (3.9550e-01)	Acc@1  86.72 ( 86.92)	Acc@5 100.00 ( 99.31)
Epoch: [17][160/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.2637e-01 (3.9558e-01)	Acc@1  82.03 ( 86.92)	Acc@5  98.44 ( 99.31)
Epoch: [17][170/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.3428e-01 (3.9623e-01)	Acc@1  82.03 ( 86.86)	Acc@5  96.09 ( 99.30)
Epoch: [17][180/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.3398e-01 (3.9295e-01)	Acc@1  91.41 ( 86.98)	Acc@5 100.00 ( 99.32)
Epoch: [17][190/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.6011e-01 (3.9299e-01)	Acc@1  88.28 ( 87.02)	Acc@5  98.44 ( 99.30)
Epoch: [17][200/391]	Time  0.042 ( 0.025)	Data  0.002 ( 0.003)	Loss 3.6377e-01 (3.9372e-01)	Acc@1  85.94 ( 86.97)	Acc@5 100.00 ( 99.30)
Epoch: [17][210/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.6206e-01 (3.9555e-01)	Acc@1  88.28 ( 86.95)	Acc@5 100.00 ( 99.31)
Epoch: [17][220/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.5010e-01 (3.9601e-01)	Acc@1  89.06 ( 86.97)	Acc@5 100.00 ( 99.33)
Epoch: [17][230/391]	Time  0.025 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.4312e-01 (3.9638e-01)	Acc@1  86.72 ( 86.95)	Acc@5  99.22 ( 99.34)
Epoch: [17][240/391]	Time  0.034 ( 0.025)	Data  0.000 ( 0.002)	Loss 3.5132e-01 (3.9752e-01)	Acc@1  87.50 ( 86.93)	Acc@5 100.00 ( 99.34)
Epoch: [17][250/391]	Time  0.033 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.5361e-01 (3.9949e-01)	Acc@1  85.94 ( 86.90)	Acc@5  99.22 ( 99.32)
Epoch: [17][260/391]	Time  0.028 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.9917e-01 (4.0122e-01)	Acc@1  89.06 ( 86.89)	Acc@5 100.00 ( 99.31)
Epoch: [17][270/391]	Time  0.029 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.1323e-01 (4.0136e-01)	Acc@1  91.41 ( 86.90)	Acc@5  98.44 ( 99.30)
Epoch: [17][280/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 4.9756e-01 (4.0262e-01)	Acc@1  85.94 ( 86.87)	Acc@5  97.66 ( 99.28)
Epoch: [17][290/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.002)	Loss 5.1074e-01 (4.0219e-01)	Acc@1  83.59 ( 86.90)	Acc@5  99.22 ( 99.28)
Epoch: [17][300/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.3359e-01 (4.0224e-01)	Acc@1  87.50 ( 86.91)	Acc@5  98.44 ( 99.28)
Epoch: [17][310/391]	Time  0.029 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.1372e-01 (4.0103e-01)	Acc@1  92.19 ( 86.94)	Acc@5  98.44 ( 99.27)
Epoch: [17][320/391]	Time  0.032 ( 0.025)	Data  0.000 ( 0.002)	Loss 3.5620e-01 (4.0058e-01)	Acc@1  88.28 ( 86.92)	Acc@5 100.00 ( 99.28)
Epoch: [17][330/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.4043e-01 (4.0156e-01)	Acc@1  85.16 ( 86.87)	Acc@5  97.66 ( 99.26)
Epoch: [17][340/391]	Time  0.025 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.2700e-01 (4.0247e-01)	Acc@1  85.16 ( 86.83)	Acc@5 100.00 ( 99.25)
Epoch: [17][350/391]	Time  0.020 ( 0.024)	Data  0.003 ( 0.002)	Loss 3.3252e-01 (4.0148e-01)	Acc@1  88.28 ( 86.85)	Acc@5  99.22 ( 99.26)
Epoch: [17][360/391]	Time  0.019 ( 0.024)	Data  0.002 ( 0.002)	Loss 3.1079e-01 (4.0134e-01)	Acc@1  89.84 ( 86.89)	Acc@5 100.00 ( 99.26)
Epoch: [17][370/391]	Time  0.027 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.7207e-01 (4.0229e-01)	Acc@1  90.62 ( 86.87)	Acc@5  99.22 ( 99.25)
Epoch: [17][380/391]	Time  0.019 ( 0.024)	Data  0.002 ( 0.002)	Loss 4.9658e-01 (4.0280e-01)	Acc@1  82.03 ( 86.85)	Acc@5  98.44 ( 99.25)
Epoch: [17][390/391]	Time  0.016 ( 0.024)	Data  0.001 ( 0.002)	Loss 4.4458e-01 (4.0177e-01)	Acc@1  85.00 ( 86.87)	Acc@5  96.25 ( 99.25)
## e[17] optimizer.zero_grad (sum) time: 0.1058034896850586
## e[17]       loss.backward (sum) time: 2.1397595405578613
## e[17]      optimizer.step (sum) time: 0.891920804977417
## epoch[17] training(only) time: 9.65851902961731
# Switched to evaluate mode...
Test: [  0/100]	Time  0.166 ( 0.166)	Loss 3.3325e-01 (3.3325e-01)	Acc@1  90.00 ( 90.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.017 ( 0.027)	Loss 6.2939e-01 (4.8220e-01)	Acc@1  84.00 ( 84.00)	Acc@5  98.00 ( 99.36)
Test: [ 20/100]	Time  0.013 ( 0.021)	Loss 7.1191e-01 (4.9726e-01)	Acc@1  80.00 ( 83.67)	Acc@5  98.00 ( 98.81)
Test: [ 30/100]	Time  0.011 ( 0.018)	Loss 4.9634e-01 (5.2322e-01)	Acc@1  82.00 ( 83.26)	Acc@5 100.00 ( 98.90)
Test: [ 40/100]	Time  0.008 ( 0.017)	Loss 5.8984e-01 (5.2529e-01)	Acc@1  79.00 ( 83.12)	Acc@5 100.00 ( 98.95)
Test: [ 50/100]	Time  0.019 ( 0.016)	Loss 4.1162e-01 (5.1278e-01)	Acc@1  86.00 ( 83.45)	Acc@5  98.00 ( 98.96)
Test: [ 60/100]	Time  0.012 ( 0.016)	Loss 4.5801e-01 (5.0411e-01)	Acc@1  87.00 ( 83.66)	Acc@5  99.00 ( 99.05)
Test: [ 70/100]	Time  0.020 ( 0.015)	Loss 5.7227e-01 (5.0241e-01)	Acc@1  87.00 ( 83.77)	Acc@5  98.00 ( 99.06)
Test: [ 80/100]	Time  0.017 ( 0.015)	Loss 3.9502e-01 (4.9734e-01)	Acc@1  83.00 ( 83.77)	Acc@5  99.00 ( 99.05)
Test: [ 90/100]	Time  0.009 ( 0.015)	Loss 3.0371e-01 (4.9519e-01)	Acc@1  87.00 ( 83.80)	Acc@5 100.00 ( 99.09)
 * Acc@1 83.790 Acc@5 99.070
### epoch[17] execution time: 11.214871883392334
EPOCH 18
# current learning rate: 0.1
# Switched to train mode...
Epoch: [18][  0/391]	Time  0.186 ( 0.186)	Data  0.163 ( 0.163)	Loss 2.3047e-01 (2.3047e-01)	Acc@1  92.19 ( 92.19)	Acc@5 100.00 (100.00)
Epoch: [18][ 10/391]	Time  0.018 ( 0.039)	Data  0.001 ( 0.017)	Loss 3.7427e-01 (3.7100e-01)	Acc@1  87.50 ( 87.22)	Acc@5  98.44 ( 99.72)
Epoch: [18][ 20/391]	Time  0.032 ( 0.032)	Data  0.001 ( 0.009)	Loss 3.8770e-01 (3.6342e-01)	Acc@1  85.94 ( 87.98)	Acc@5  99.22 ( 99.59)
Epoch: [18][ 30/391]	Time  0.024 ( 0.029)	Data  0.003 ( 0.007)	Loss 2.6611e-01 (3.6352e-01)	Acc@1  93.75 ( 87.95)	Acc@5 100.00 ( 99.47)
Epoch: [18][ 40/391]	Time  0.019 ( 0.028)	Data  0.002 ( 0.006)	Loss 4.7827e-01 (3.7616e-01)	Acc@1  84.38 ( 87.50)	Acc@5  98.44 ( 99.41)
Epoch: [18][ 50/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.005)	Loss 3.5669e-01 (3.8574e-01)	Acc@1  89.84 ( 87.41)	Acc@5  97.66 ( 99.33)
Epoch: [18][ 60/391]	Time  0.031 ( 0.026)	Data  0.002 ( 0.004)	Loss 4.2798e-01 (3.8273e-01)	Acc@1  86.72 ( 87.54)	Acc@5  99.22 ( 99.31)
Epoch: [18][ 70/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.004)	Loss 3.6328e-01 (3.8189e-01)	Acc@1  87.50 ( 87.53)	Acc@5  99.22 ( 99.27)
Epoch: [18][ 80/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 2.5732e-01 (3.7615e-01)	Acc@1  89.84 ( 87.69)	Acc@5 100.00 ( 99.32)
Epoch: [18][ 90/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.004)	Loss 2.2339e-01 (3.7723e-01)	Acc@1  93.75 ( 87.71)	Acc@5  99.22 ( 99.32)
Epoch: [18][100/391]	Time  0.018 ( 0.025)	Data  0.002 ( 0.003)	Loss 3.2227e-01 (3.8029e-01)	Acc@1  86.72 ( 87.66)	Acc@5 100.00 ( 99.31)
Epoch: [18][110/391]	Time  0.027 ( 0.025)	Data  0.002 ( 0.003)	Loss 3.5693e-01 (3.7989e-01)	Acc@1  84.38 ( 87.70)	Acc@5 100.00 ( 99.33)
Epoch: [18][120/391]	Time  0.036 ( 0.025)	Data  0.003 ( 0.003)	Loss 3.9575e-01 (3.8315e-01)	Acc@1  86.72 ( 87.54)	Acc@5  99.22 ( 99.30)
Epoch: [18][130/391]	Time  0.048 ( 0.025)	Data  0.005 ( 0.003)	Loss 3.3008e-01 (3.8756e-01)	Acc@1  90.62 ( 87.40)	Acc@5 100.00 ( 99.26)
Epoch: [18][140/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 2.7881e-01 (3.8631e-01)	Acc@1  90.62 ( 87.44)	Acc@5 100.00 ( 99.27)
Epoch: [18][150/391]	Time  0.040 ( 0.025)	Data  0.005 ( 0.003)	Loss 3.7158e-01 (3.8547e-01)	Acc@1  87.50 ( 87.53)	Acc@5 100.00 ( 99.31)
Epoch: [18][160/391]	Time  0.051 ( 0.025)	Data  0.000 ( 0.003)	Loss 3.8550e-01 (3.8613e-01)	Acc@1  85.16 ( 87.44)	Acc@5  99.22 ( 99.30)
Epoch: [18][170/391]	Time  0.029 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.9150e-01 (3.8806e-01)	Acc@1  89.84 ( 87.36)	Acc@5  99.22 ( 99.31)
Epoch: [18][180/391]	Time  0.036 ( 0.025)	Data  0.003 ( 0.003)	Loss 3.7378e-01 (3.8718e-01)	Acc@1  86.72 ( 87.37)	Acc@5  99.22 ( 99.31)
Epoch: [18][190/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 3.3838e-01 (3.8887e-01)	Acc@1  87.50 ( 87.32)	Acc@5 100.00 ( 99.31)
Epoch: [18][200/391]	Time  0.024 ( 0.025)	Data  0.000 ( 0.003)	Loss 2.5977e-01 (3.8972e-01)	Acc@1  92.19 ( 87.33)	Acc@5 100.00 ( 99.31)
Epoch: [18][210/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.5488e-01 (3.8967e-01)	Acc@1  91.41 ( 87.33)	Acc@5 100.00 ( 99.30)
Epoch: [18][220/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.8013e-01 (3.8983e-01)	Acc@1  88.28 ( 87.35)	Acc@5  98.44 ( 99.29)
Epoch: [18][230/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.1226e-01 (3.8807e-01)	Acc@1  86.72 ( 87.36)	Acc@5 100.00 ( 99.30)
Epoch: [18][240/391]	Time  0.030 ( 0.025)	Data  0.005 ( 0.003)	Loss 4.6973e-01 (3.8802e-01)	Acc@1  84.38 ( 87.35)	Acc@5 100.00 ( 99.31)
Epoch: [18][250/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 4.1577e-01 (3.8883e-01)	Acc@1  85.16 ( 87.34)	Acc@5  99.22 ( 99.32)
Epoch: [18][260/391]	Time  0.029 ( 0.025)	Data  0.001 ( 0.002)	Loss 5.2979e-01 (3.8908e-01)	Acc@1  79.69 ( 87.33)	Acc@5  97.66 ( 99.31)
Epoch: [18][270/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.7817e-01 (3.8956e-01)	Acc@1  86.72 ( 87.32)	Acc@5  99.22 ( 99.30)
Epoch: [18][280/391]	Time  0.020 ( 0.025)	Data  0.002 ( 0.002)	Loss 2.6489e-01 (3.8892e-01)	Acc@1  89.84 ( 87.34)	Acc@5 100.00 ( 99.30)
Epoch: [18][290/391]	Time  0.019 ( 0.024)	Data  0.002 ( 0.002)	Loss 3.8110e-01 (3.8847e-01)	Acc@1  89.84 ( 87.36)	Acc@5 100.00 ( 99.31)
Epoch: [18][300/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.2937e-01 (3.8806e-01)	Acc@1  94.53 ( 87.38)	Acc@5 100.00 ( 99.31)
Epoch: [18][310/391]	Time  0.027 ( 0.024)	Data  0.004 ( 0.002)	Loss 3.5498e-01 (3.8580e-01)	Acc@1  85.16 ( 87.44)	Acc@5  99.22 ( 99.32)
Epoch: [18][320/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.0518e-01 (3.8380e-01)	Acc@1  90.62 ( 87.48)	Acc@5  99.22 ( 99.33)
Epoch: [18][330/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 4.5703e-01 (3.8444e-01)	Acc@1  85.16 ( 87.48)	Acc@5 100.00 ( 99.33)
Epoch: [18][340/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 6.5918e-01 (3.8605e-01)	Acc@1  82.81 ( 87.44)	Acc@5  97.66 ( 99.33)
Epoch: [18][350/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 4.0552e-01 (3.8571e-01)	Acc@1  85.94 ( 87.45)	Acc@5  99.22 ( 99.32)
Epoch: [18][360/391]	Time  0.019 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.9785e-01 (3.8433e-01)	Acc@1  88.28 ( 87.48)	Acc@5 100.00 ( 99.33)
Epoch: [18][370/391]	Time  0.026 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.9648e-01 (3.8474e-01)	Acc@1  87.50 ( 87.46)	Acc@5  99.22 ( 99.32)
Epoch: [18][380/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 4.0112e-01 (3.8590e-01)	Acc@1  84.38 ( 87.41)	Acc@5  99.22 ( 99.32)
Epoch: [18][390/391]	Time  0.016 ( 0.024)	Data  0.000 ( 0.002)	Loss 4.5996e-01 (3.8574e-01)	Acc@1  82.50 ( 87.40)	Acc@5 100.00 ( 99.32)
## e[18] optimizer.zero_grad (sum) time: 0.10476994514465332
## e[18]       loss.backward (sum) time: 2.196474075317383
## e[18]      optimizer.step (sum) time: 0.8939168453216553
## epoch[18] training(only) time: 9.61075758934021
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 4.4409e-01 (4.4409e-01)	Acc@1  88.00 ( 88.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.008 ( 0.027)	Loss 7.2314e-01 (5.4703e-01)	Acc@1  81.00 ( 83.00)	Acc@5  99.00 ( 98.91)
Test: [ 20/100]	Time  0.012 ( 0.020)	Loss 7.1582e-01 (5.3625e-01)	Acc@1  77.00 ( 82.86)	Acc@5  99.00 ( 98.90)
Test: [ 30/100]	Time  0.012 ( 0.018)	Loss 3.9819e-01 (5.3058e-01)	Acc@1  89.00 ( 83.39)	Acc@5  99.00 ( 99.00)
Test: [ 40/100]	Time  0.013 ( 0.017)	Loss 5.0000e-01 (5.2702e-01)	Acc@1  85.00 ( 83.34)	Acc@5  97.00 ( 99.00)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 3.9893e-01 (5.1837e-01)	Acc@1  87.00 ( 83.53)	Acc@5  99.00 ( 99.04)
Test: [ 60/100]	Time  0.008 ( 0.015)	Loss 4.6436e-01 (5.1123e-01)	Acc@1  84.00 ( 83.67)	Acc@5  99.00 ( 99.08)
Test: [ 70/100]	Time  0.008 ( 0.015)	Loss 5.2295e-01 (5.0837e-01)	Acc@1  79.00 ( 83.61)	Acc@5  97.00 ( 99.04)
Test: [ 80/100]	Time  0.008 ( 0.015)	Loss 3.8062e-01 (5.0334e-01)	Acc@1  86.00 ( 83.62)	Acc@5  99.00 ( 99.07)
Test: [ 90/100]	Time  0.009 ( 0.015)	Loss 3.0566e-01 (5.0635e-01)	Acc@1  85.00 ( 83.45)	Acc@5 100.00 ( 99.10)
 * Acc@1 83.650 Acc@5 99.150
### epoch[18] execution time: 11.1800057888031
EPOCH 19
# current learning rate: 0.1
# Switched to train mode...
Epoch: [19][  0/391]	Time  0.197 ( 0.197)	Data  0.176 ( 0.176)	Loss 2.8052e-01 (2.8052e-01)	Acc@1  91.41 ( 91.41)	Acc@5  99.22 ( 99.22)
Epoch: [19][ 10/391]	Time  0.018 ( 0.039)	Data  0.001 ( 0.018)	Loss 3.8843e-01 (3.9966e-01)	Acc@1  85.94 ( 86.72)	Acc@5  98.44 ( 99.43)
Epoch: [19][ 20/391]	Time  0.033 ( 0.032)	Data  0.000 ( 0.010)	Loss 2.1521e-01 (3.7862e-01)	Acc@1  93.75 ( 87.31)	Acc@5 100.00 ( 99.48)
Epoch: [19][ 30/391]	Time  0.043 ( 0.030)	Data  0.003 ( 0.007)	Loss 3.5913e-01 (3.6545e-01)	Acc@1  85.94 ( 87.80)	Acc@5 100.00 ( 99.47)
Epoch: [19][ 40/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.006)	Loss 3.4619e-01 (3.5994e-01)	Acc@1  89.06 ( 88.15)	Acc@5  97.66 ( 99.41)
Epoch: [19][ 50/391]	Time  0.033 ( 0.027)	Data  0.001 ( 0.005)	Loss 4.7290e-01 (3.6438e-01)	Acc@1  87.50 ( 87.99)	Acc@5  98.44 ( 99.37)
Epoch: [19][ 60/391]	Time  0.030 ( 0.027)	Data  0.001 ( 0.005)	Loss 3.9819e-01 (3.6437e-01)	Acc@1  89.84 ( 88.09)	Acc@5 100.00 ( 99.37)
Epoch: [19][ 70/391]	Time  0.018 ( 0.026)	Data  0.000 ( 0.004)	Loss 2.9102e-01 (3.6296e-01)	Acc@1  88.28 ( 88.06)	Acc@5 100.00 ( 99.41)
Epoch: [19][ 80/391]	Time  0.047 ( 0.026)	Data  0.008 ( 0.004)	Loss 2.9541e-01 (3.6449e-01)	Acc@1  90.62 ( 87.97)	Acc@5  99.22 ( 99.38)
Epoch: [19][ 90/391]	Time  0.030 ( 0.026)	Data  0.002 ( 0.004)	Loss 4.2456e-01 (3.6192e-01)	Acc@1  87.50 ( 88.07)	Acc@5  98.44 ( 99.38)
Epoch: [19][100/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 3.9282e-01 (3.6474e-01)	Acc@1  88.28 ( 88.00)	Acc@5 100.00 ( 99.37)
Epoch: [19][110/391]	Time  0.018 ( 0.025)	Data  0.000 ( 0.003)	Loss 3.9868e-01 (3.6406e-01)	Acc@1  91.41 ( 88.13)	Acc@5  99.22 ( 99.33)
Epoch: [19][120/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 3.3984e-01 (3.6608e-01)	Acc@1  87.50 ( 88.04)	Acc@5  99.22 ( 99.32)
Epoch: [19][130/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.9980e-01 (3.6519e-01)	Acc@1  87.50 ( 88.07)	Acc@5  99.22 ( 99.31)
Epoch: [19][140/391]	Time  0.024 ( 0.025)	Data  0.002 ( 0.003)	Loss 3.9282e-01 (3.6572e-01)	Acc@1  90.62 ( 88.10)	Acc@5 100.00 ( 99.32)
Epoch: [19][150/391]	Time  0.037 ( 0.025)	Data  0.002 ( 0.003)	Loss 4.0625e-01 (3.6562e-01)	Acc@1  89.06 ( 88.09)	Acc@5  98.44 ( 99.34)
Epoch: [19][160/391]	Time  0.041 ( 0.025)	Data  0.002 ( 0.003)	Loss 3.4595e-01 (3.6432e-01)	Acc@1  88.28 ( 88.15)	Acc@5  98.44 ( 99.34)
Epoch: [19][170/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.0103e-01 (3.6496e-01)	Acc@1  88.28 ( 88.17)	Acc@5 100.00 ( 99.35)
Epoch: [19][180/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.4155e-01 (3.6626e-01)	Acc@1  89.06 ( 88.14)	Acc@5 100.00 ( 99.36)
Epoch: [19][190/391]	Time  0.032 ( 0.025)	Data  0.002 ( 0.003)	Loss 4.9780e-01 (3.6687e-01)	Acc@1  86.72 ( 88.12)	Acc@5  98.44 ( 99.35)
Epoch: [19][200/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.8574e-01 (3.6540e-01)	Acc@1  88.28 ( 88.17)	Acc@5 100.00 ( 99.36)
Epoch: [19][210/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.9805e-01 (3.6468e-01)	Acc@1  83.59 ( 88.21)	Acc@5  98.44 ( 99.36)
Epoch: [19][220/391]	Time  0.037 ( 0.025)	Data  0.000 ( 0.003)	Loss 2.8589e-01 (3.6682e-01)	Acc@1  91.41 ( 88.14)	Acc@5 100.00 ( 99.35)
Epoch: [19][230/391]	Time  0.024 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.6631e-01 (3.6785e-01)	Acc@1  85.16 ( 88.09)	Acc@5 100.00 ( 99.36)
Epoch: [19][240/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.1973e-01 (3.6604e-01)	Acc@1  90.62 ( 88.15)	Acc@5 100.00 ( 99.36)
Epoch: [19][250/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.8110e-01 (3.6627e-01)	Acc@1  86.72 ( 88.12)	Acc@5  99.22 ( 99.36)
Epoch: [19][260/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.5679e-01 (3.6604e-01)	Acc@1  86.72 ( 88.12)	Acc@5  99.22 ( 99.37)
Epoch: [19][270/391]	Time  0.025 ( 0.025)	Data  0.003 ( 0.002)	Loss 3.0957e-01 (3.6542e-01)	Acc@1  89.06 ( 88.13)	Acc@5 100.00 ( 99.37)
Epoch: [19][280/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.7280e-01 (3.6535e-01)	Acc@1  87.50 ( 88.14)	Acc@5 100.00 ( 99.37)
Epoch: [19][290/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.6597e-01 (3.6545e-01)	Acc@1  90.62 ( 88.13)	Acc@5  99.22 ( 99.38)
Epoch: [19][300/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.4570e-01 (3.6641e-01)	Acc@1  88.28 ( 88.07)	Acc@5  99.22 ( 99.38)
Epoch: [19][310/391]	Time  0.017 ( 0.024)	Data  0.000 ( 0.002)	Loss 5.8643e-01 (3.6700e-01)	Acc@1  84.38 ( 88.08)	Acc@5  98.44 ( 99.37)
Epoch: [19][320/391]	Time  0.026 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.6499e-01 (3.6856e-01)	Acc@1  87.50 ( 88.02)	Acc@5  99.22 ( 99.37)
Epoch: [19][330/391]	Time  0.039 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.4692e-01 (3.6789e-01)	Acc@1  88.28 ( 88.05)	Acc@5 100.00 ( 99.37)
Epoch: [19][340/391]	Time  0.015 ( 0.024)	Data  0.001 ( 0.002)	Loss 5.0830e-01 (3.6900e-01)	Acc@1  85.16 ( 88.02)	Acc@5  99.22 ( 99.37)
Epoch: [19][350/391]	Time  0.025 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.8647e-01 (3.6894e-01)	Acc@1  87.50 ( 88.02)	Acc@5  99.22 ( 99.37)
Epoch: [19][360/391]	Time  0.037 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.2129e-01 (3.6881e-01)	Acc@1  86.72 ( 88.03)	Acc@5 100.00 ( 99.37)
Epoch: [19][370/391]	Time  0.036 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.2617e-01 (3.6889e-01)	Acc@1  88.28 ( 88.02)	Acc@5 100.00 ( 99.37)
Epoch: [19][380/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.3057e-01 (3.6955e-01)	Acc@1  88.28 ( 88.00)	Acc@5 100.00 ( 99.37)
Epoch: [19][390/391]	Time  0.014 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.9980e-01 (3.7046e-01)	Acc@1  88.75 ( 87.96)	Acc@5 100.00 ( 99.37)
## e[19] optimizer.zero_grad (sum) time: 0.10627412796020508
## e[19]       loss.backward (sum) time: 2.1581034660339355
## e[19]      optimizer.step (sum) time: 0.8901355266571045
## epoch[19] training(only) time: 9.665547370910645
# Switched to evaluate mode...
Test: [  0/100]	Time  0.151 ( 0.151)	Loss 4.9951e-01 (4.9951e-01)	Acc@1  86.00 ( 86.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.021 ( 0.026)	Loss 5.9912e-01 (4.8844e-01)	Acc@1  85.00 ( 84.82)	Acc@5  99.00 ( 99.00)
Test: [ 20/100]	Time  0.011 ( 0.021)	Loss 6.6064e-01 (5.3059e-01)	Acc@1  78.00 ( 83.81)	Acc@5  99.00 ( 98.76)
Test: [ 30/100]	Time  0.020 ( 0.018)	Loss 5.0146e-01 (5.1449e-01)	Acc@1  82.00 ( 83.74)	Acc@5  97.00 ( 98.87)
Test: [ 40/100]	Time  0.011 ( 0.017)	Loss 4.5923e-01 (5.1418e-01)	Acc@1  86.00 ( 83.66)	Acc@5  99.00 ( 98.85)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 3.5693e-01 (5.0697e-01)	Acc@1  90.00 ( 83.98)	Acc@5  99.00 ( 98.86)
Test: [ 60/100]	Time  0.017 ( 0.016)	Loss 5.0781e-01 (5.0602e-01)	Acc@1  86.00 ( 84.13)	Acc@5 100.00 ( 98.82)
Test: [ 70/100]	Time  0.013 ( 0.015)	Loss 5.0781e-01 (5.0506e-01)	Acc@1  85.00 ( 84.08)	Acc@5 100.00 ( 98.94)
Test: [ 80/100]	Time  0.013 ( 0.015)	Loss 5.0586e-01 (5.0224e-01)	Acc@1  84.00 ( 84.05)	Acc@5  99.00 ( 98.96)
Test: [ 90/100]	Time  0.018 ( 0.015)	Loss 3.7915e-01 (4.9778e-01)	Acc@1  87.00 ( 84.10)	Acc@5 100.00 ( 98.98)
 * Acc@1 84.130 Acc@5 98.990
### epoch[19] execution time: 11.23084282875061
EPOCH 20
# current learning rate: 0.1
# Switched to train mode...
Epoch: [20][  0/391]	Time  0.194 ( 0.194)	Data  0.171 ( 0.171)	Loss 3.1885e-01 (3.1885e-01)	Acc@1  86.72 ( 86.72)	Acc@5 100.00 (100.00)
Epoch: [20][ 10/391]	Time  0.019 ( 0.038)	Data  0.001 ( 0.017)	Loss 3.3301e-01 (3.1944e-01)	Acc@1  89.06 ( 88.92)	Acc@5  99.22 ( 99.57)
Epoch: [20][ 20/391]	Time  0.019 ( 0.032)	Data  0.001 ( 0.010)	Loss 3.5181e-01 (3.3943e-01)	Acc@1  86.72 ( 88.54)	Acc@5  99.22 ( 99.48)
Epoch: [20][ 30/391]	Time  0.020 ( 0.029)	Data  0.001 ( 0.007)	Loss 5.0732e-01 (3.3369e-01)	Acc@1  85.94 ( 88.79)	Acc@5  97.66 ( 99.52)
Epoch: [20][ 40/391]	Time  0.019 ( 0.028)	Data  0.001 ( 0.006)	Loss 2.5537e-01 (3.3250e-01)	Acc@1  91.41 ( 88.83)	Acc@5 100.00 ( 99.54)
Epoch: [20][ 50/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.005)	Loss 2.6611e-01 (3.2956e-01)	Acc@1  91.41 ( 88.94)	Acc@5 100.00 ( 99.51)
Epoch: [20][ 60/391]	Time  0.018 ( 0.026)	Data  0.002 ( 0.005)	Loss 2.8296e-01 (3.2795e-01)	Acc@1  87.50 ( 88.99)	Acc@5 100.00 ( 99.55)
Epoch: [20][ 70/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 3.4912e-01 (3.3060e-01)	Acc@1  89.84 ( 88.97)	Acc@5 100.00 ( 99.53)
Epoch: [20][ 80/391]	Time  0.023 ( 0.026)	Data  0.005 ( 0.004)	Loss 3.6450e-01 (3.3484e-01)	Acc@1  88.28 ( 88.90)	Acc@5 100.00 ( 99.52)
Epoch: [20][ 90/391]	Time  0.018 ( 0.026)	Data  0.002 ( 0.004)	Loss 4.4849e-01 (3.4013e-01)	Acc@1  86.72 ( 88.79)	Acc@5 100.00 ( 99.52)
Epoch: [20][100/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 3.6865e-01 (3.4253e-01)	Acc@1  88.28 ( 88.61)	Acc@5 100.00 ( 99.50)
Epoch: [20][110/391]	Time  0.035 ( 0.025)	Data  0.003 ( 0.003)	Loss 5.5957e-01 (3.4228e-01)	Acc@1  80.47 ( 88.65)	Acc@5  99.22 ( 99.51)
Epoch: [20][120/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.4863e-01 (3.4296e-01)	Acc@1  88.28 ( 88.63)	Acc@5  99.22 ( 99.49)
Epoch: [20][130/391]	Time  0.034 ( 0.025)	Data  0.004 ( 0.003)	Loss 3.4888e-01 (3.4078e-01)	Acc@1  87.50 ( 88.64)	Acc@5 100.00 ( 99.51)
Epoch: [20][140/391]	Time  0.029 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.7451e-01 (3.4257e-01)	Acc@1  87.50 ( 88.55)	Acc@5 100.00 ( 99.51)
Epoch: [20][150/391]	Time  0.025 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.8159e-01 (3.4635e-01)	Acc@1  88.28 ( 88.47)	Acc@5  98.44 ( 99.51)
Epoch: [20][160/391]	Time  0.026 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.0771e-01 (3.4807e-01)	Acc@1  84.38 ( 88.36)	Acc@5  99.22 ( 99.51)
Epoch: [20][170/391]	Time  0.033 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.5815e-01 (3.4606e-01)	Acc@1  87.50 ( 88.43)	Acc@5  99.22 ( 99.52)
Epoch: [20][180/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.0981e-01 (3.4717e-01)	Acc@1  87.50 ( 88.40)	Acc@5 100.00 ( 99.53)
Epoch: [20][190/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 3.2739e-01 (3.4892e-01)	Acc@1  89.84 ( 88.35)	Acc@5 100.00 ( 99.52)
Epoch: [20][200/391]	Time  0.025 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.3311e-01 (3.5174e-01)	Acc@1  85.94 ( 88.29)	Acc@5  99.22 ( 99.50)
Epoch: [20][210/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.6841e-01 (3.5075e-01)	Acc@1  89.84 ( 88.37)	Acc@5  99.22 ( 99.51)
Epoch: [20][220/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 5.5371e-01 (3.5225e-01)	Acc@1  82.03 ( 88.33)	Acc@5  97.66 ( 99.51)
Epoch: [20][230/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.4302e-01 (3.5363e-01)	Acc@1  89.84 ( 88.27)	Acc@5 100.00 ( 99.50)
Epoch: [20][240/391]	Time  0.025 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.6294e-01 (3.5236e-01)	Acc@1  92.19 ( 88.31)	Acc@5  99.22 ( 99.49)
Epoch: [20][250/391]	Time  0.035 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.6865e-01 (3.5238e-01)	Acc@1  91.41 ( 88.32)	Acc@5  98.44 ( 99.49)
Epoch: [20][260/391]	Time  0.025 ( 0.024)	Data  0.005 ( 0.002)	Loss 4.6045e-01 (3.5373e-01)	Acc@1  85.94 ( 88.31)	Acc@5  98.44 ( 99.48)
Epoch: [20][270/391]	Time  0.023 ( 0.025)	Data  0.000 ( 0.002)	Loss 2.8662e-01 (3.5408e-01)	Acc@1  89.06 ( 88.26)	Acc@5 100.00 ( 99.49)
Epoch: [20][280/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.7915e-01 (3.5594e-01)	Acc@1  89.84 ( 88.18)	Acc@5 100.00 ( 99.49)
Epoch: [20][290/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.5107e-01 (3.5660e-01)	Acc@1  87.50 ( 88.14)	Acc@5  99.22 ( 99.48)
Epoch: [20][300/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 5.3223e-01 (3.5725e-01)	Acc@1  84.38 ( 88.14)	Acc@5  97.66 ( 99.47)
Epoch: [20][310/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.3643e-01 (3.5634e-01)	Acc@1  89.06 ( 88.17)	Acc@5  99.22 ( 99.48)
Epoch: [20][320/391]	Time  0.043 ( 0.024)	Data  0.002 ( 0.002)	Loss 3.1348e-01 (3.5493e-01)	Acc@1  90.62 ( 88.23)	Acc@5 100.00 ( 99.48)
Epoch: [20][330/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 4.9390e-01 (3.5588e-01)	Acc@1  86.72 ( 88.23)	Acc@5  98.44 ( 99.47)
Epoch: [20][340/391]	Time  0.034 ( 0.024)	Data  0.005 ( 0.002)	Loss 2.5439e-01 (3.5713e-01)	Acc@1  92.97 ( 88.21)	Acc@5 100.00 ( 99.47)
Epoch: [20][350/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.5571e-01 (3.5869e-01)	Acc@1  85.94 ( 88.15)	Acc@5  99.22 ( 99.47)
Epoch: [20][360/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 5.1855e-01 (3.5864e-01)	Acc@1  87.50 ( 88.19)	Acc@5 100.00 ( 99.46)
Epoch: [20][370/391]	Time  0.019 ( 0.024)	Data  0.002 ( 0.002)	Loss 4.1187e-01 (3.5967e-01)	Acc@1  85.94 ( 88.15)	Acc@5 100.00 ( 99.46)
Epoch: [20][380/391]	Time  0.017 ( 0.024)	Data  0.000 ( 0.002)	Loss 3.9917e-01 (3.5996e-01)	Acc@1  87.50 ( 88.16)	Acc@5  98.44 ( 99.45)
Epoch: [20][390/391]	Time  0.013 ( 0.024)	Data  0.001 ( 0.002)	Loss 4.6021e-01 (3.6036e-01)	Acc@1  87.50 ( 88.14)	Acc@5  98.75 ( 99.45)
## e[20] optimizer.zero_grad (sum) time: 0.10455703735351562
## e[20]       loss.backward (sum) time: 2.17875599861145
## e[20]      optimizer.step (sum) time: 0.8864006996154785
## epoch[20] training(only) time: 9.63354229927063
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 4.1602e-01 (4.1602e-01)	Acc@1  87.00 ( 87.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.017 ( 0.027)	Loss 5.6104e-01 (4.3932e-01)	Acc@1  83.00 ( 85.36)	Acc@5  99.00 ( 99.27)
Test: [ 20/100]	Time  0.016 ( 0.020)	Loss 7.1924e-01 (4.6060e-01)	Acc@1  77.00 ( 84.71)	Acc@5  99.00 ( 99.10)
Test: [ 30/100]	Time  0.013 ( 0.019)	Loss 5.2783e-01 (4.6372e-01)	Acc@1  82.00 ( 84.74)	Acc@5  99.00 ( 99.06)
Test: [ 40/100]	Time  0.012 ( 0.017)	Loss 4.7290e-01 (4.6456e-01)	Acc@1  86.00 ( 84.66)	Acc@5  99.00 ( 99.05)
Test: [ 50/100]	Time  0.011 ( 0.016)	Loss 2.8809e-01 (4.5084e-01)	Acc@1  90.00 ( 85.20)	Acc@5  99.00 ( 99.00)
Test: [ 60/100]	Time  0.012 ( 0.016)	Loss 4.4312e-01 (4.5237e-01)	Acc@1  83.00 ( 85.20)	Acc@5 100.00 ( 98.98)
Test: [ 70/100]	Time  0.014 ( 0.015)	Loss 6.8115e-01 (4.5458e-01)	Acc@1  84.00 ( 85.35)	Acc@5  98.00 ( 98.99)
Test: [ 80/100]	Time  0.017 ( 0.015)	Loss 3.3643e-01 (4.5504e-01)	Acc@1  85.00 ( 85.28)	Acc@5 100.00 ( 99.05)
Test: [ 90/100]	Time  0.016 ( 0.015)	Loss 2.6782e-01 (4.5152e-01)	Acc@1  91.00 ( 85.34)	Acc@5 100.00 ( 99.12)
 * Acc@1 85.450 Acc@5 99.110
### epoch[20] execution time: 11.220157623291016
EPOCH 21
# current learning rate: 0.1
# Switched to train mode...
Epoch: [21][  0/391]	Time  0.248 ( 0.248)	Data  0.227 ( 0.227)	Loss 2.8394e-01 (2.8394e-01)	Acc@1  90.62 ( 90.62)	Acc@5  98.44 ( 98.44)
Epoch: [21][ 10/391]	Time  0.030 ( 0.045)	Data  0.001 ( 0.022)	Loss 4.5996e-01 (3.1935e-01)	Acc@1  81.25 ( 89.28)	Acc@5  99.22 ( 99.43)
Epoch: [21][ 20/391]	Time  0.018 ( 0.034)	Data  0.001 ( 0.012)	Loss 2.8857e-01 (3.1631e-01)	Acc@1  90.62 ( 89.77)	Acc@5  99.22 ( 99.63)
Epoch: [21][ 30/391]	Time  0.017 ( 0.030)	Data  0.000 ( 0.009)	Loss 3.2520e-01 (3.1395e-01)	Acc@1  92.19 ( 90.07)	Acc@5  99.22 ( 99.55)
Epoch: [21][ 40/391]	Time  0.018 ( 0.029)	Data  0.001 ( 0.007)	Loss 4.6265e-01 (3.2927e-01)	Acc@1  85.16 ( 89.60)	Acc@5  99.22 ( 99.52)
Epoch: [21][ 50/391]	Time  0.019 ( 0.028)	Data  0.002 ( 0.006)	Loss 4.1113e-01 (3.3676e-01)	Acc@1  87.50 ( 89.28)	Acc@5 100.00 ( 99.46)
Epoch: [21][ 60/391]	Time  0.040 ( 0.028)	Data  0.001 ( 0.005)	Loss 5.1074e-01 (3.4092e-01)	Acc@1  84.38 ( 89.05)	Acc@5  98.44 ( 99.44)
Epoch: [21][ 70/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.005)	Loss 4.7168e-01 (3.4478e-01)	Acc@1  85.16 ( 89.10)	Acc@5  98.44 ( 99.39)
Epoch: [21][ 80/391]	Time  0.036 ( 0.027)	Data  0.006 ( 0.005)	Loss 2.2131e-01 (3.4129e-01)	Acc@1  92.19 ( 89.18)	Acc@5 100.00 ( 99.40)
Epoch: [21][ 90/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.004)	Loss 2.7612e-01 (3.4049e-01)	Acc@1  89.06 ( 89.09)	Acc@5 100.00 ( 99.39)
Epoch: [21][100/391]	Time  0.017 ( 0.026)	Data  0.000 ( 0.004)	Loss 3.2153e-01 (3.4127e-01)	Acc@1  89.06 ( 89.01)	Acc@5 100.00 ( 99.40)
Epoch: [21][110/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.9727e-01 (3.3640e-01)	Acc@1  93.75 ( 89.12)	Acc@5 100.00 ( 99.43)
Epoch: [21][120/391]	Time  0.027 ( 0.026)	Data  0.004 ( 0.004)	Loss 5.0830e-01 (3.3836e-01)	Acc@1  82.03 ( 89.06)	Acc@5  98.44 ( 99.43)
Epoch: [21][130/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.004)	Loss 2.9712e-01 (3.3835e-01)	Acc@1  87.50 ( 89.01)	Acc@5 100.00 ( 99.42)
Epoch: [21][140/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.004)	Loss 3.9429e-01 (3.3915e-01)	Acc@1  89.06 ( 88.96)	Acc@5  98.44 ( 99.41)
Epoch: [21][150/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.6436e-01 (3.4000e-01)	Acc@1  89.06 ( 88.94)	Acc@5  99.22 ( 99.42)
Epoch: [21][160/391]	Time  0.026 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.6108e-01 (3.4146e-01)	Acc@1  87.50 ( 88.84)	Acc@5  99.22 ( 99.42)
Epoch: [21][170/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.6099e-01 (3.4164e-01)	Acc@1  90.62 ( 88.81)	Acc@5 100.00 ( 99.43)
Epoch: [21][180/391]	Time  0.042 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.2473e-01 (3.4119e-01)	Acc@1  93.75 ( 88.82)	Acc@5 100.00 ( 99.44)
Epoch: [21][190/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.2534e-01 (3.4297e-01)	Acc@1  92.19 ( 88.73)	Acc@5 100.00 ( 99.43)
Epoch: [21][200/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.8804e-01 (3.4366e-01)	Acc@1  85.16 ( 88.72)	Acc@5  98.44 ( 99.43)
Epoch: [21][210/391]	Time  0.027 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.2812e-01 (3.4455e-01)	Acc@1  87.50 ( 88.69)	Acc@5 100.00 ( 99.42)
Epoch: [21][220/391]	Time  0.034 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.7134e-01 (3.4322e-01)	Acc@1  89.06 ( 88.74)	Acc@5  98.44 ( 99.42)
Epoch: [21][230/391]	Time  0.017 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.4326e-01 (3.4131e-01)	Acc@1  85.16 ( 88.74)	Acc@5 100.00 ( 99.44)
Epoch: [21][240/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.2334e-01 (3.4251e-01)	Acc@1  82.81 ( 88.73)	Acc@5  99.22 ( 99.43)
Epoch: [21][250/391]	Time  0.028 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.7681e-01 (3.4555e-01)	Acc@1  84.38 ( 88.64)	Acc@5  97.66 ( 99.42)
Epoch: [21][260/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.8638e-01 (3.4437e-01)	Acc@1  89.84 ( 88.69)	Acc@5 100.00 ( 99.43)
Epoch: [21][270/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.4717e-01 (3.4504e-01)	Acc@1  85.16 ( 88.64)	Acc@5 100.00 ( 99.42)
Epoch: [21][280/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.8511e-01 (3.4772e-01)	Acc@1  86.72 ( 88.56)	Acc@5  99.22 ( 99.40)
Epoch: [21][290/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.1348e-01 (3.4768e-01)	Acc@1  89.84 ( 88.57)	Acc@5 100.00 ( 99.40)
Epoch: [21][300/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 3.5767e-01 (3.4898e-01)	Acc@1  87.50 ( 88.50)	Acc@5 100.00 ( 99.42)
Epoch: [21][310/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.9600e-01 (3.4862e-01)	Acc@1  87.50 ( 88.52)	Acc@5 100.00 ( 99.43)
Epoch: [21][320/391]	Time  0.027 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.4863e-01 (3.5027e-01)	Acc@1  89.06 ( 88.49)	Acc@5  99.22 ( 99.42)
Epoch: [21][330/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.4082e-01 (3.4983e-01)	Acc@1  87.50 ( 88.51)	Acc@5  99.22 ( 99.43)
Epoch: [21][340/391]	Time  0.035 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.5986e-01 (3.5019e-01)	Acc@1  89.84 ( 88.48)	Acc@5 100.00 ( 99.44)
Epoch: [21][350/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.1855e-01 (3.5042e-01)	Acc@1  85.16 ( 88.48)	Acc@5 100.00 ( 99.44)
Epoch: [21][360/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 2.9077e-01 (3.5090e-01)	Acc@1  89.84 ( 88.47)	Acc@5 100.00 ( 99.44)
Epoch: [21][370/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.0112e-01 (3.5138e-01)	Acc@1  87.50 ( 88.46)	Acc@5 100.00 ( 99.44)
Epoch: [21][380/391]	Time  0.024 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.2627e-01 (3.5241e-01)	Acc@1  86.72 ( 88.43)	Acc@5 100.00 ( 99.44)
Epoch: [21][390/391]	Time  0.013 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.3779e-01 (3.5301e-01)	Acc@1  90.00 ( 88.41)	Acc@5 100.00 ( 99.44)
## e[21] optimizer.zero_grad (sum) time: 0.10552668571472168
## e[21]       loss.backward (sum) time: 2.139571189880371
## e[21]      optimizer.step (sum) time: 0.8712372779846191
## epoch[21] training(only) time: 9.800378799438477
# Switched to evaluate mode...
Test: [  0/100]	Time  0.215 ( 0.215)	Loss 4.0674e-01 (4.0674e-01)	Acc@1  87.00 ( 87.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.023 ( 0.032)	Loss 5.6836e-01 (4.6318e-01)	Acc@1  83.00 ( 84.27)	Acc@5 100.00 ( 99.45)
Test: [ 20/100]	Time  0.016 ( 0.023)	Loss 5.9082e-01 (4.6932e-01)	Acc@1  81.00 ( 84.52)	Acc@5 100.00 ( 99.33)
Test: [ 30/100]	Time  0.009 ( 0.020)	Loss 4.7046e-01 (4.6223e-01)	Acc@1  84.00 ( 84.94)	Acc@5  98.00 ( 99.29)
Test: [ 40/100]	Time  0.017 ( 0.018)	Loss 4.3872e-01 (4.6274e-01)	Acc@1  85.00 ( 84.78)	Acc@5  99.00 ( 99.22)
Test: [ 50/100]	Time  0.008 ( 0.017)	Loss 3.8110e-01 (4.5299e-01)	Acc@1  88.00 ( 85.04)	Acc@5  97.00 ( 99.24)
Test: [ 60/100]	Time  0.020 ( 0.017)	Loss 5.3564e-01 (4.5568e-01)	Acc@1  85.00 ( 85.05)	Acc@5  99.00 ( 99.25)
Test: [ 70/100]	Time  0.013 ( 0.016)	Loss 5.7568e-01 (4.5806e-01)	Acc@1  85.00 ( 85.04)	Acc@5  98.00 ( 99.27)
Test: [ 80/100]	Time  0.008 ( 0.016)	Loss 2.9248e-01 (4.5486e-01)	Acc@1  91.00 ( 85.19)	Acc@5 100.00 ( 99.26)
Test: [ 90/100]	Time  0.010 ( 0.016)	Loss 2.8223e-01 (4.5191e-01)	Acc@1  91.00 ( 85.23)	Acc@5 100.00 ( 99.27)
 * Acc@1 85.180 Acc@5 99.320
### epoch[21] execution time: 11.447692632675171
EPOCH 22
# current learning rate: 0.1
# Switched to train mode...
Epoch: [22][  0/391]	Time  0.200 ( 0.200)	Data  0.168 ( 0.168)	Loss 5.6592e-01 (5.6592e-01)	Acc@1  78.91 ( 78.91)	Acc@5  99.22 ( 99.22)
Epoch: [22][ 10/391]	Time  0.028 ( 0.039)	Data  0.001 ( 0.017)	Loss 2.2034e-01 (3.2019e-01)	Acc@1  91.41 ( 89.20)	Acc@5 100.00 ( 99.64)
Epoch: [22][ 20/391]	Time  0.018 ( 0.032)	Data  0.001 ( 0.010)	Loss 2.5049e-01 (3.0739e-01)	Acc@1  92.19 ( 89.69)	Acc@5 100.00 ( 99.67)
Epoch: [22][ 30/391]	Time  0.032 ( 0.030)	Data  0.001 ( 0.007)	Loss 3.9746e-01 (3.1568e-01)	Acc@1  86.72 ( 89.49)	Acc@5 100.00 ( 99.62)
Epoch: [22][ 40/391]	Time  0.020 ( 0.028)	Data  0.001 ( 0.006)	Loss 2.3364e-01 (3.1251e-01)	Acc@1  91.41 ( 89.65)	Acc@5  99.22 ( 99.58)
Epoch: [22][ 50/391]	Time  0.018 ( 0.027)	Data  0.002 ( 0.005)	Loss 4.2383e-01 (3.2131e-01)	Acc@1  85.16 ( 89.41)	Acc@5 100.00 ( 99.57)
Epoch: [22][ 60/391]	Time  0.027 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.7075e-01 (3.2263e-01)	Acc@1  93.75 ( 89.49)	Acc@5  99.22 ( 99.54)
Epoch: [22][ 70/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 2.8687e-01 (3.2581e-01)	Acc@1  88.28 ( 89.29)	Acc@5 100.00 ( 99.53)
Epoch: [22][ 80/391]	Time  0.022 ( 0.026)	Data  0.005 ( 0.004)	Loss 2.7563e-01 (3.2761e-01)	Acc@1  90.62 ( 89.28)	Acc@5  99.22 ( 99.52)
Epoch: [22][ 90/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.004)	Loss 4.3896e-01 (3.3030e-01)	Acc@1  85.16 ( 89.14)	Acc@5  98.44 ( 99.52)
Epoch: [22][100/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.9663e-01 (3.2880e-01)	Acc@1  91.41 ( 89.17)	Acc@5  99.22 ( 99.50)
Epoch: [22][110/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.3926e-01 (3.2622e-01)	Acc@1  92.97 ( 89.22)	Acc@5  99.22 ( 99.48)
Epoch: [22][120/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.9624e-01 (3.2710e-01)	Acc@1  88.28 ( 89.24)	Acc@5  99.22 ( 99.48)
Epoch: [22][130/391]	Time  0.042 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.0503e-01 (3.2959e-01)	Acc@1  85.16 ( 89.17)	Acc@5 100.00 ( 99.48)
Epoch: [22][140/391]	Time  0.026 ( 0.025)	Data  0.006 ( 0.003)	Loss 1.8640e-01 (3.2685e-01)	Acc@1  94.53 ( 89.23)	Acc@5  99.22 ( 99.50)
Epoch: [22][150/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.3604e-01 (3.2933e-01)	Acc@1  85.94 ( 89.17)	Acc@5  97.66 ( 99.49)
Epoch: [22][160/391]	Time  0.028 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.5630e-01 (3.3238e-01)	Acc@1  87.50 ( 89.09)	Acc@5  98.44 ( 99.47)
Epoch: [22][170/391]	Time  0.026 ( 0.025)	Data  0.002 ( 0.003)	Loss 2.7759e-01 (3.3380e-01)	Acc@1  89.84 ( 89.02)	Acc@5 100.00 ( 99.47)
Epoch: [22][180/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 2.5977e-01 (3.3336e-01)	Acc@1  92.97 ( 89.05)	Acc@5 100.00 ( 99.47)
Epoch: [22][190/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.0471e-01 (3.3268e-01)	Acc@1  94.53 ( 89.09)	Acc@5  99.22 ( 99.48)
Epoch: [22][200/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 2.8223e-01 (3.3501e-01)	Acc@1  89.84 ( 89.06)	Acc@5 100.00 ( 99.47)
Epoch: [22][210/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.9199e-01 (3.3437e-01)	Acc@1  93.75 ( 89.07)	Acc@5  99.22 ( 99.47)
Epoch: [22][220/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.9297e-01 (3.3433e-01)	Acc@1  92.19 ( 89.08)	Acc@5  99.22 ( 99.48)
Epoch: [22][230/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.7026e-01 (3.3369e-01)	Acc@1  91.41 ( 89.06)	Acc@5 100.00 ( 99.49)
Epoch: [22][240/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.9258e-01 (3.3358e-01)	Acc@1  85.94 ( 89.09)	Acc@5 100.00 ( 99.49)
Epoch: [22][250/391]	Time  0.032 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.0005e-01 (3.3370e-01)	Acc@1  91.41 ( 89.06)	Acc@5  99.22 ( 99.50)
Epoch: [22][260/391]	Time  0.017 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.0298e-01 (3.3473e-01)	Acc@1  89.06 ( 89.05)	Acc@5 100.00 ( 99.48)
Epoch: [22][270/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.6880e-01 (3.3430e-01)	Acc@1  89.84 ( 89.05)	Acc@5 100.00 ( 99.50)
Epoch: [22][280/391]	Time  0.021 ( 0.025)	Data  0.004 ( 0.002)	Loss 4.5386e-01 (3.3621e-01)	Acc@1  86.72 ( 89.00)	Acc@5  99.22 ( 99.50)
Epoch: [22][290/391]	Time  0.040 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.1235e-01 (3.3706e-01)	Acc@1  84.38 ( 88.92)	Acc@5 100.00 ( 99.50)
Epoch: [22][300/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 5.8936e-01 (3.3801e-01)	Acc@1  80.47 ( 88.86)	Acc@5  99.22 ( 99.50)
Epoch: [22][310/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.7974e-01 (3.3805e-01)	Acc@1  82.03 ( 88.86)	Acc@5  99.22 ( 99.50)
Epoch: [22][320/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.8452e-01 (3.3914e-01)	Acc@1  85.94 ( 88.81)	Acc@5 100.00 ( 99.50)
Epoch: [22][330/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.1616e-01 (3.3881e-01)	Acc@1  87.50 ( 88.80)	Acc@5 100.00 ( 99.50)
Epoch: [22][340/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.8369e-01 (3.3952e-01)	Acc@1  90.62 ( 88.79)	Acc@5  99.22 ( 99.49)
Epoch: [22][350/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 5.1465e-01 (3.4061e-01)	Acc@1  83.59 ( 88.75)	Acc@5  98.44 ( 99.49)
Epoch: [22][360/391]	Time  0.024 ( 0.025)	Data  0.004 ( 0.002)	Loss 3.9453e-01 (3.4087e-01)	Acc@1  87.50 ( 88.73)	Acc@5  99.22 ( 99.49)
Epoch: [22][370/391]	Time  0.034 ( 0.025)	Data  0.005 ( 0.002)	Loss 3.4180e-01 (3.4156e-01)	Acc@1  89.06 ( 88.72)	Acc@5  99.22 ( 99.48)
Epoch: [22][380/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.8833e-01 (3.4195e-01)	Acc@1  90.62 ( 88.70)	Acc@5  99.22 ( 99.48)
Epoch: [22][390/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.9673e-01 (3.4283e-01)	Acc@1  86.25 ( 88.68)	Acc@5  98.75 ( 99.47)
## e[22] optimizer.zero_grad (sum) time: 0.10461807250976562
## e[22]       loss.backward (sum) time: 2.1846134662628174
## e[22]      optimizer.step (sum) time: 0.8956620693206787
## epoch[22] training(only) time: 9.665481805801392
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 4.5947e-01 (4.5947e-01)	Acc@1  83.00 ( 83.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.008 ( 0.026)	Loss 5.2881e-01 (4.7550e-01)	Acc@1  83.00 ( 83.91)	Acc@5  99.00 ( 99.09)
Test: [ 20/100]	Time  0.012 ( 0.020)	Loss 4.3921e-01 (4.6989e-01)	Acc@1  85.00 ( 84.67)	Acc@5 100.00 ( 98.95)
Test: [ 30/100]	Time  0.028 ( 0.018)	Loss 4.2480e-01 (4.7890e-01)	Acc@1  85.00 ( 84.58)	Acc@5 100.00 ( 98.90)
Test: [ 40/100]	Time  0.008 ( 0.017)	Loss 5.8008e-01 (4.7971e-01)	Acc@1  82.00 ( 84.51)	Acc@5  98.00 ( 98.93)
Test: [ 50/100]	Time  0.015 ( 0.016)	Loss 3.0786e-01 (4.7266e-01)	Acc@1  89.00 ( 84.75)	Acc@5  99.00 ( 98.88)
Test: [ 60/100]	Time  0.019 ( 0.016)	Loss 4.2236e-01 (4.7921e-01)	Acc@1  86.00 ( 84.59)	Acc@5 100.00 ( 98.90)
Test: [ 70/100]	Time  0.008 ( 0.015)	Loss 6.5088e-01 (4.8276e-01)	Acc@1  83.00 ( 84.69)	Acc@5  97.00 ( 98.92)
Test: [ 80/100]	Time  0.010 ( 0.015)	Loss 4.6533e-01 (4.8048e-01)	Acc@1  83.00 ( 84.57)	Acc@5  99.00 ( 98.90)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 3.6597e-01 (4.7740e-01)	Acc@1  88.00 ( 84.68)	Acc@5 100.00 ( 98.96)
 * Acc@1 84.790 Acc@5 99.000
### epoch[22] execution time: 11.219886779785156
EPOCH 23
# current learning rate: 0.1
# Switched to train mode...
Epoch: [23][  0/391]	Time  0.186 ( 0.186)	Data  0.164 ( 0.164)	Loss 2.0471e-01 (2.0471e-01)	Acc@1  92.97 ( 92.97)	Acc@5 100.00 (100.00)
Epoch: [23][ 10/391]	Time  0.018 ( 0.038)	Data  0.001 ( 0.016)	Loss 2.9810e-01 (3.1837e-01)	Acc@1  89.06 ( 88.92)	Acc@5 100.00 ( 99.64)
Epoch: [23][ 20/391]	Time  0.033 ( 0.032)	Data  0.004 ( 0.009)	Loss 2.8223e-01 (3.3447e-01)	Acc@1  92.19 ( 88.95)	Acc@5 100.00 ( 99.70)
Epoch: [23][ 30/391]	Time  0.024 ( 0.029)	Data  0.002 ( 0.007)	Loss 2.8149e-01 (3.1997e-01)	Acc@1  90.62 ( 89.64)	Acc@5  99.22 ( 99.70)
Epoch: [23][ 40/391]	Time  0.018 ( 0.028)	Data  0.001 ( 0.006)	Loss 3.7744e-01 (3.2204e-01)	Acc@1  88.28 ( 89.50)	Acc@5 100.00 ( 99.66)
Epoch: [23][ 50/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.005)	Loss 4.6069e-01 (3.2519e-01)	Acc@1  85.16 ( 89.57)	Acc@5  99.22 ( 99.60)
Epoch: [23][ 60/391]	Time  0.029 ( 0.026)	Data  0.002 ( 0.004)	Loss 2.7173e-01 (3.2192e-01)	Acc@1  92.97 ( 89.73)	Acc@5  99.22 ( 99.59)
Epoch: [23][ 70/391]	Time  0.022 ( 0.026)	Data  0.005 ( 0.004)	Loss 3.1055e-01 (3.2277e-01)	Acc@1  89.84 ( 89.66)	Acc@5  98.44 ( 99.58)
Epoch: [23][ 80/391]	Time  0.039 ( 0.026)	Data  0.003 ( 0.004)	Loss 2.2852e-01 (3.1565e-01)	Acc@1  92.19 ( 89.86)	Acc@5  99.22 ( 99.59)
Epoch: [23][ 90/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 5.3613e-01 (3.2134e-01)	Acc@1  82.81 ( 89.56)	Acc@5  98.44 ( 99.57)
Epoch: [23][100/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.1055e-01 (3.2174e-01)	Acc@1  86.72 ( 89.49)	Acc@5 100.00 ( 99.57)
Epoch: [23][110/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.6660e-01 (3.2289e-01)	Acc@1  92.19 ( 89.51)	Acc@5 100.00 ( 99.56)
Epoch: [23][120/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.1860e-01 (3.2307e-01)	Acc@1  87.50 ( 89.47)	Acc@5  99.22 ( 99.57)
Epoch: [23][130/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.0151e-01 (3.2421e-01)	Acc@1  89.06 ( 89.47)	Acc@5  99.22 ( 99.56)
Epoch: [23][140/391]	Time  0.032 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.3970e-01 (3.2593e-01)	Acc@1  87.50 ( 89.36)	Acc@5 100.00 ( 99.55)
Epoch: [23][150/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.9556e-01 (3.2433e-01)	Acc@1  92.97 ( 89.44)	Acc@5 100.00 ( 99.55)
Epoch: [23][160/391]	Time  0.033 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.9397e-01 (3.2394e-01)	Acc@1  93.75 ( 89.45)	Acc@5 100.00 ( 99.54)
Epoch: [23][170/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.5664e-01 (3.2557e-01)	Acc@1  85.16 ( 89.45)	Acc@5  97.66 ( 99.52)
Epoch: [23][180/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.0283e-01 (3.2642e-01)	Acc@1  86.72 ( 89.41)	Acc@5 100.00 ( 99.52)
Epoch: [23][190/391]	Time  0.032 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.9995e-01 (3.2580e-01)	Acc@1  94.53 ( 89.47)	Acc@5 100.00 ( 99.52)
Epoch: [23][200/391]	Time  0.027 ( 0.025)	Data  0.002 ( 0.003)	Loss 3.0396e-01 (3.2795e-01)	Acc@1  90.62 ( 89.44)	Acc@5  98.44 ( 99.50)
Epoch: [23][210/391]	Time  0.031 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.7036e-01 (3.2872e-01)	Acc@1  86.72 ( 89.38)	Acc@5 100.00 ( 99.50)
Epoch: [23][220/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.6245e-01 (3.2961e-01)	Acc@1  92.19 ( 89.37)	Acc@5  99.22 ( 99.48)
Epoch: [23][230/391]	Time  0.041 ( 0.025)	Data  0.000 ( 0.002)	Loss 4.0552e-01 (3.3191e-01)	Acc@1  85.94 ( 89.25)	Acc@5 100.00 ( 99.47)
Epoch: [23][240/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.1592e-01 (3.3278e-01)	Acc@1  86.72 ( 89.20)	Acc@5  99.22 ( 99.48)
Epoch: [23][250/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.8062e-01 (3.3294e-01)	Acc@1  85.94 ( 89.17)	Acc@5 100.00 ( 99.49)
Epoch: [23][260/391]	Time  0.035 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.3154e-01 (3.3047e-01)	Acc@1  85.16 ( 89.19)	Acc@5 100.00 ( 99.50)
Epoch: [23][270/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.2212e-01 (3.3135e-01)	Acc@1  82.03 ( 89.15)	Acc@5 100.00 ( 99.50)
Epoch: [23][280/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.6719e-01 (3.3138e-01)	Acc@1  86.72 ( 89.13)	Acc@5 100.00 ( 99.52)
Epoch: [23][290/391]	Time  0.034 ( 0.025)	Data  0.000 ( 0.002)	Loss 2.9541e-01 (3.3147e-01)	Acc@1  91.41 ( 89.14)	Acc@5  99.22 ( 99.52)
Epoch: [23][300/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.2725e-01 (3.3205e-01)	Acc@1  88.28 ( 89.16)	Acc@5  99.22 ( 99.51)
Epoch: [23][310/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.3154e-01 (3.3315e-01)	Acc@1  89.06 ( 89.12)	Acc@5 100.00 ( 99.51)
Epoch: [23][320/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.7212e-01 (3.3129e-01)	Acc@1  92.19 ( 89.16)	Acc@5 100.00 ( 99.52)
Epoch: [23][330/391]	Time  0.032 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.2227e-01 (3.3135e-01)	Acc@1  86.72 ( 89.14)	Acc@5 100.00 ( 99.52)
Epoch: [23][340/391]	Time  0.036 ( 0.025)	Data  0.004 ( 0.002)	Loss 2.9150e-01 (3.3166e-01)	Acc@1  91.41 ( 89.13)	Acc@5  99.22 ( 99.53)
Epoch: [23][350/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.5850e-01 (3.3320e-01)	Acc@1  88.28 ( 89.07)	Acc@5 100.00 ( 99.53)
Epoch: [23][360/391]	Time  0.036 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.2473e-01 (3.3238e-01)	Acc@1  93.75 ( 89.07)	Acc@5  99.22 ( 99.53)
Epoch: [23][370/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.1667e-01 (3.3238e-01)	Acc@1  95.31 ( 89.10)	Acc@5 100.00 ( 99.54)
Epoch: [23][380/391]	Time  0.019 ( 0.024)	Data  0.002 ( 0.002)	Loss 3.1030e-01 (3.3350e-01)	Acc@1  92.19 ( 89.09)	Acc@5  98.44 ( 99.53)
Epoch: [23][390/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.9614e-01 (3.3484e-01)	Acc@1  90.00 ( 89.04)	Acc@5 100.00 ( 99.54)
## e[23] optimizer.zero_grad (sum) time: 0.10691094398498535
## e[23]       loss.backward (sum) time: 2.1466403007507324
## e[23]      optimizer.step (sum) time: 0.892711877822876
## epoch[23] training(only) time: 9.655627489089966
# Switched to evaluate mode...
Test: [  0/100]	Time  0.225 ( 0.225)	Loss 4.2383e-01 (4.2383e-01)	Acc@1  89.00 ( 89.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.018 ( 0.033)	Loss 5.4980e-01 (4.5807e-01)	Acc@1  82.00 ( 84.55)	Acc@5  99.00 ( 99.45)
Test: [ 20/100]	Time  0.012 ( 0.024)	Loss 5.7031e-01 (4.8183e-01)	Acc@1  82.00 ( 84.05)	Acc@5  99.00 ( 99.19)
Test: [ 30/100]	Time  0.008 ( 0.021)	Loss 4.9878e-01 (4.9235e-01)	Acc@1  86.00 ( 84.00)	Acc@5  99.00 ( 99.23)
Test: [ 40/100]	Time  0.011 ( 0.019)	Loss 6.4111e-01 (4.9318e-01)	Acc@1  80.00 ( 84.34)	Acc@5  98.00 ( 99.15)
Test: [ 50/100]	Time  0.019 ( 0.018)	Loss 4.0332e-01 (4.8425e-01)	Acc@1  84.00 ( 84.57)	Acc@5  98.00 ( 99.16)
Test: [ 60/100]	Time  0.008 ( 0.017)	Loss 4.9048e-01 (4.8742e-01)	Acc@1  84.00 ( 84.52)	Acc@5 100.00 ( 99.18)
Test: [ 70/100]	Time  0.019 ( 0.017)	Loss 5.7910e-01 (4.8532e-01)	Acc@1  83.00 ( 84.48)	Acc@5  99.00 ( 99.24)
Test: [ 80/100]	Time  0.008 ( 0.016)	Loss 3.4399e-01 (4.7937e-01)	Acc@1  86.00 ( 84.57)	Acc@5  99.00 ( 99.28)
Test: [ 90/100]	Time  0.016 ( 0.016)	Loss 2.5562e-01 (4.7835e-01)	Acc@1  88.00 ( 84.55)	Acc@5 100.00 ( 99.32)
 * Acc@1 84.540 Acc@5 99.310
### epoch[23] execution time: 11.348076820373535
EPOCH 24
# current learning rate: 0.1
# Switched to train mode...
Epoch: [24][  0/391]	Time  0.189 ( 0.189)	Data  0.168 ( 0.168)	Loss 2.5757e-01 (2.5757e-01)	Acc@1  92.19 ( 92.19)	Acc@5  99.22 ( 99.22)
Epoch: [24][ 10/391]	Time  0.019 ( 0.038)	Data  0.002 ( 0.017)	Loss 2.1704e-01 (2.8006e-01)	Acc@1  92.97 ( 90.48)	Acc@5 100.00 ( 99.72)
Epoch: [24][ 20/391]	Time  0.021 ( 0.032)	Data  0.001 ( 0.010)	Loss 3.9600e-01 (3.1695e-01)	Acc@1  90.62 ( 89.32)	Acc@5  98.44 ( 99.67)
Epoch: [24][ 30/391]	Time  0.018 ( 0.029)	Data  0.001 ( 0.007)	Loss 3.6255e-01 (3.3000e-01)	Acc@1  87.50 ( 88.73)	Acc@5 100.00 ( 99.67)
Epoch: [24][ 40/391]	Time  0.018 ( 0.028)	Data  0.001 ( 0.006)	Loss 2.6562e-01 (3.3578e-01)	Acc@1  89.84 ( 88.74)	Acc@5 100.00 ( 99.64)
Epoch: [24][ 50/391]	Time  0.018 ( 0.027)	Data  0.002 ( 0.005)	Loss 2.1008e-01 (3.2257e-01)	Acc@1  93.75 ( 89.14)	Acc@5 100.00 ( 99.65)
Epoch: [24][ 60/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.005)	Loss 2.4902e-01 (3.2462e-01)	Acc@1  89.84 ( 89.09)	Acc@5 100.00 ( 99.59)
Epoch: [24][ 70/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 2.3950e-01 (3.2176e-01)	Acc@1  91.41 ( 89.21)	Acc@5 100.00 ( 99.57)
Epoch: [24][ 80/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.3940e-01 (3.1818e-01)	Acc@1  94.53 ( 89.28)	Acc@5 100.00 ( 99.58)
Epoch: [24][ 90/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.004)	Loss 2.6709e-01 (3.2082e-01)	Acc@1  90.62 ( 89.26)	Acc@5  99.22 ( 99.58)
Epoch: [24][100/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.4790e-01 (3.1766e-01)	Acc@1  87.50 ( 89.34)	Acc@5  99.22 ( 99.60)
Epoch: [24][110/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.1675e-01 (3.1794e-01)	Acc@1  86.72 ( 89.34)	Acc@5  99.22 ( 99.60)
Epoch: [24][120/391]	Time  0.024 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.9185e-01 (3.2442e-01)	Acc@1  85.16 ( 89.16)	Acc@5  99.22 ( 99.56)
Epoch: [24][130/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.1260e-01 (3.2763e-01)	Acc@1  85.94 ( 89.03)	Acc@5  99.22 ( 99.56)
Epoch: [24][140/391]	Time  0.050 ( 0.025)	Data  0.010 ( 0.003)	Loss 3.1274e-01 (3.2673e-01)	Acc@1  89.84 ( 89.12)	Acc@5  99.22 ( 99.58)
Epoch: [24][150/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.9272e-01 (3.2534e-01)	Acc@1  89.06 ( 89.23)	Acc@5 100.00 ( 99.60)
Epoch: [24][160/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.7744e-01 (3.2455e-01)	Acc@1  89.84 ( 89.29)	Acc@5 100.00 ( 99.59)
Epoch: [24][170/391]	Time  0.026 ( 0.025)	Data  0.002 ( 0.003)	Loss 2.7466e-01 (3.2602e-01)	Acc@1  91.41 ( 89.20)	Acc@5  99.22 ( 99.59)
Epoch: [24][180/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.8149e-01 (3.2614e-01)	Acc@1  89.06 ( 89.20)	Acc@5 100.00 ( 99.60)
Epoch: [24][190/391]	Time  0.031 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.1982e-01 (3.2664e-01)	Acc@1  88.28 ( 89.25)	Acc@5 100.00 ( 99.61)
Epoch: [24][200/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 2.1887e-01 (3.2618e-01)	Acc@1  92.97 ( 89.25)	Acc@5 100.00 ( 99.61)
Epoch: [24][210/391]	Time  0.040 ( 0.025)	Data  0.002 ( 0.003)	Loss 3.0371e-01 (3.2565e-01)	Acc@1  89.84 ( 89.26)	Acc@5 100.00 ( 99.62)
Epoch: [24][220/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.1958e-01 (3.2488e-01)	Acc@1  90.62 ( 89.22)	Acc@5 100.00 ( 99.63)
Epoch: [24][230/391]	Time  0.017 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.0518e-01 (3.2459e-01)	Acc@1  89.84 ( 89.24)	Acc@5 100.00 ( 99.62)
Epoch: [24][240/391]	Time  0.038 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.0063e-01 (3.2480e-01)	Acc@1  85.16 ( 89.25)	Acc@5  98.44 ( 99.61)
Epoch: [24][250/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.3706e-01 (3.2331e-01)	Acc@1  95.31 ( 89.31)	Acc@5 100.00 ( 99.61)
Epoch: [24][260/391]	Time  0.037 ( 0.025)	Data  0.004 ( 0.002)	Loss 3.2178e-01 (3.2222e-01)	Acc@1  92.19 ( 89.35)	Acc@5  99.22 ( 99.60)
Epoch: [24][270/391]	Time  0.030 ( 0.025)	Data  0.000 ( 0.002)	Loss 3.6377e-01 (3.2384e-01)	Acc@1  87.50 ( 89.29)	Acc@5  97.66 ( 99.58)
Epoch: [24][280/391]	Time  0.034 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.7271e-01 (3.2434e-01)	Acc@1  89.84 ( 89.27)	Acc@5 100.00 ( 99.57)
Epoch: [24][290/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.1030e-01 (3.2412e-01)	Acc@1  88.28 ( 89.27)	Acc@5 100.00 ( 99.57)
Epoch: [24][300/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.3496e-01 (3.2623e-01)	Acc@1  88.28 ( 89.20)	Acc@5 100.00 ( 99.57)
Epoch: [24][310/391]	Time  0.039 ( 0.025)	Data  0.004 ( 0.002)	Loss 2.4609e-01 (3.2665e-01)	Acc@1  90.62 ( 89.18)	Acc@5 100.00 ( 99.57)
Epoch: [24][320/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.002)	Loss 2.8076e-01 (3.2805e-01)	Acc@1  89.84 ( 89.12)	Acc@5  99.22 ( 99.56)
Epoch: [24][330/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.3975e-01 (3.2870e-01)	Acc@1  92.97 ( 89.08)	Acc@5 100.00 ( 99.57)
Epoch: [24][340/391]	Time  0.018 ( 0.024)	Data  0.002 ( 0.002)	Loss 3.0957e-01 (3.2833e-01)	Acc@1  89.84 ( 89.11)	Acc@5 100.00 ( 99.57)
Epoch: [24][350/391]	Time  0.017 ( 0.024)	Data  0.000 ( 0.002)	Loss 3.3276e-01 (3.2797e-01)	Acc@1  87.50 ( 89.11)	Acc@5  99.22 ( 99.56)
Epoch: [24][360/391]	Time  0.023 ( 0.024)	Data  0.000 ( 0.002)	Loss 1.9788e-01 (3.2991e-01)	Acc@1  96.88 ( 89.08)	Acc@5 100.00 ( 99.55)
Epoch: [24][370/391]	Time  0.048 ( 0.024)	Data  0.008 ( 0.002)	Loss 2.6929e-01 (3.2989e-01)	Acc@1  89.06 ( 89.04)	Acc@5 100.00 ( 99.56)
Epoch: [24][380/391]	Time  0.019 ( 0.024)	Data  0.002 ( 0.002)	Loss 3.6133e-01 (3.2956e-01)	Acc@1  87.50 ( 89.06)	Acc@5 100.00 ( 99.54)
Epoch: [24][390/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.002)	Loss 4.0137e-01 (3.3027e-01)	Acc@1  85.00 ( 89.04)	Acc@5  98.75 ( 99.55)
## e[24] optimizer.zero_grad (sum) time: 0.1058049201965332
## e[24]       loss.backward (sum) time: 2.1652870178222656
## e[24]      optimizer.step (sum) time: 0.8912961483001709
## epoch[24] training(only) time: 9.655715942382812
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 4.7437e-01 (4.7437e-01)	Acc@1  87.00 ( 87.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.021 ( 0.027)	Loss 5.6055e-01 (4.7246e-01)	Acc@1  82.00 ( 84.91)	Acc@5  99.00 ( 99.18)
Test: [ 20/100]	Time  0.008 ( 0.020)	Loss 5.7080e-01 (4.7990e-01)	Acc@1  79.00 ( 84.67)	Acc@5 100.00 ( 99.00)
Test: [ 30/100]	Time  0.017 ( 0.018)	Loss 5.4443e-01 (4.9669e-01)	Acc@1  83.00 ( 84.39)	Acc@5  99.00 ( 98.94)
Test: [ 40/100]	Time  0.016 ( 0.017)	Loss 4.2334e-01 (4.9006e-01)	Acc@1  85.00 ( 84.41)	Acc@5  98.00 ( 99.00)
Test: [ 50/100]	Time  0.009 ( 0.016)	Loss 2.5854e-01 (4.7836e-01)	Acc@1  91.00 ( 84.76)	Acc@5 100.00 ( 99.08)
Test: [ 60/100]	Time  0.008 ( 0.015)	Loss 4.1675e-01 (4.7706e-01)	Acc@1  86.00 ( 84.90)	Acc@5  99.00 ( 99.07)
Test: [ 70/100]	Time  0.007 ( 0.015)	Loss 5.2148e-01 (4.7408e-01)	Acc@1  81.00 ( 85.10)	Acc@5 100.00 ( 99.13)
Test: [ 80/100]	Time  0.015 ( 0.015)	Loss 4.7632e-01 (4.7155e-01)	Acc@1  85.00 ( 85.12)	Acc@5  99.00 ( 99.17)
Test: [ 90/100]	Time  0.013 ( 0.015)	Loss 2.8247e-01 (4.7228e-01)	Acc@1  90.00 ( 84.99)	Acc@5 100.00 ( 99.16)
 * Acc@1 85.040 Acc@5 99.200
### epoch[24] execution time: 11.218444108963013
EPOCH 25
# current learning rate: 0.1
# Switched to train mode...
Epoch: [25][  0/391]	Time  0.188 ( 0.188)	Data  0.160 ( 0.160)	Loss 2.9004e-01 (2.9004e-01)	Acc@1  91.41 ( 91.41)	Acc@5 100.00 (100.00)
Epoch: [25][ 10/391]	Time  0.026 ( 0.039)	Data  0.001 ( 0.016)	Loss 2.4451e-01 (2.8802e-01)	Acc@1  92.97 ( 90.06)	Acc@5 100.00 ( 99.79)
Epoch: [25][ 20/391]	Time  0.023 ( 0.031)	Data  0.001 ( 0.009)	Loss 3.3618e-01 (2.9252e-01)	Acc@1  88.28 ( 90.03)	Acc@5  98.44 ( 99.74)
Epoch: [25][ 30/391]	Time  0.032 ( 0.029)	Data  0.001 ( 0.007)	Loss 2.5928e-01 (3.0467e-01)	Acc@1  89.84 ( 89.79)	Acc@5 100.00 ( 99.65)
Epoch: [25][ 40/391]	Time  0.019 ( 0.028)	Data  0.001 ( 0.006)	Loss 2.8296e-01 (2.8968e-01)	Acc@1  92.19 ( 90.38)	Acc@5  99.22 ( 99.60)
Epoch: [25][ 50/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.005)	Loss 2.1484e-01 (2.9273e-01)	Acc@1  92.19 ( 90.41)	Acc@5 100.00 ( 99.56)
Epoch: [25][ 60/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 3.5962e-01 (2.9633e-01)	Acc@1  89.84 ( 90.19)	Acc@5  99.22 ( 99.55)
Epoch: [25][ 70/391]	Time  0.035 ( 0.026)	Data  0.001 ( 0.004)	Loss 2.1289e-01 (2.9385e-01)	Acc@1  91.41 ( 90.29)	Acc@5 100.00 ( 99.54)
Epoch: [25][ 80/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.004)	Loss 3.6157e-01 (3.0054e-01)	Acc@1  90.62 ( 90.02)	Acc@5  98.44 ( 99.54)
Epoch: [25][ 90/391]	Time  0.024 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.1128e-01 (3.0458e-01)	Acc@1  92.19 ( 89.88)	Acc@5 100.00 ( 99.54)
Epoch: [25][100/391]	Time  0.035 ( 0.025)	Data  0.002 ( 0.003)	Loss 2.9272e-01 (3.0454e-01)	Acc@1  92.19 ( 89.94)	Acc@5  99.22 ( 99.54)
Epoch: [25][110/391]	Time  0.024 ( 0.025)	Data  0.002 ( 0.003)	Loss 2.2522e-01 (3.0653e-01)	Acc@1  91.41 ( 89.81)	Acc@5  99.22 ( 99.56)
Epoch: [25][120/391]	Time  0.037 ( 0.025)	Data  0.003 ( 0.003)	Loss 3.4741e-01 (3.0921e-01)	Acc@1  89.06 ( 89.69)	Acc@5 100.00 ( 99.57)
Epoch: [25][130/391]	Time  0.027 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.9907e-01 (3.1101e-01)	Acc@1  89.06 ( 89.61)	Acc@5 100.00 ( 99.55)
Epoch: [25][140/391]	Time  0.030 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.3154e-01 (3.1316e-01)	Acc@1  86.72 ( 89.56)	Acc@5 100.00 ( 99.52)
Epoch: [25][150/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.5288e-01 (3.1538e-01)	Acc@1  90.62 ( 89.56)	Acc@5  99.22 ( 99.52)
Epoch: [25][160/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.0947e-01 (3.1564e-01)	Acc@1  92.19 ( 89.58)	Acc@5 100.00 ( 99.52)
Epoch: [25][170/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.1299e-01 (3.1681e-01)	Acc@1  89.84 ( 89.48)	Acc@5 100.00 ( 99.52)
Epoch: [25][180/391]	Time  0.034 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.4424e-01 (3.1711e-01)	Acc@1  87.50 ( 89.46)	Acc@5  99.22 ( 99.52)
Epoch: [25][190/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.0703e-01 (3.1525e-01)	Acc@1  94.53 ( 89.52)	Acc@5 100.00 ( 99.52)
Epoch: [25][200/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.7466e-01 (3.1619e-01)	Acc@1  93.75 ( 89.49)	Acc@5  99.22 ( 99.53)
Epoch: [25][210/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.0615e-01 (3.1718e-01)	Acc@1  87.50 ( 89.42)	Acc@5 100.00 ( 99.51)
Epoch: [25][220/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.0884e-01 (3.1734e-01)	Acc@1  89.84 ( 89.44)	Acc@5  99.22 ( 99.52)
Epoch: [25][230/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.002)	Loss 4.4409e-01 (3.1898e-01)	Acc@1  81.25 ( 89.37)	Acc@5  98.44 ( 99.51)
Epoch: [25][240/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.5903e-01 (3.1942e-01)	Acc@1  91.41 ( 89.36)	Acc@5 100.00 ( 99.51)
Epoch: [25][250/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.2407e-01 (3.1921e-01)	Acc@1  87.50 ( 89.36)	Acc@5  96.88 ( 99.50)
Epoch: [25][260/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.3096e-01 (3.1866e-01)	Acc@1  91.41 ( 89.37)	Acc@5 100.00 ( 99.50)
Epoch: [25][270/391]	Time  0.017 ( 0.024)	Data  0.000 ( 0.002)	Loss 3.1372e-01 (3.1980e-01)	Acc@1  91.41 ( 89.37)	Acc@5 100.00 ( 99.50)
Epoch: [25][280/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.5684e-01 (3.1946e-01)	Acc@1  90.62 ( 89.38)	Acc@5 100.00 ( 99.51)
Epoch: [25][290/391]	Time  0.027 ( 0.024)	Data  0.002 ( 0.002)	Loss 3.8989e-01 (3.1890e-01)	Acc@1  89.84 ( 89.38)	Acc@5 100.00 ( 99.51)
Epoch: [25][300/391]	Time  0.030 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.5967e-01 (3.1886e-01)	Acc@1  95.31 ( 89.39)	Acc@5 100.00 ( 99.51)
Epoch: [25][310/391]	Time  0.037 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.5742e-01 (3.1881e-01)	Acc@1  87.50 ( 89.39)	Acc@5 100.00 ( 99.51)
Epoch: [25][320/391]	Time  0.035 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.4863e-01 (3.1843e-01)	Acc@1  86.72 ( 89.40)	Acc@5  99.22 ( 99.52)
Epoch: [25][330/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.6318e-01 (3.1941e-01)	Acc@1  92.19 ( 89.39)	Acc@5 100.00 ( 99.52)
Epoch: [25][340/391]	Time  0.019 ( 0.024)	Data  0.002 ( 0.002)	Loss 3.3765e-01 (3.2000e-01)	Acc@1  89.06 ( 89.35)	Acc@5 100.00 ( 99.51)
Epoch: [25][350/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.002)	Loss 3.5571e-01 (3.1971e-01)	Acc@1  86.72 ( 89.33)	Acc@5 100.00 ( 99.52)
Epoch: [25][360/391]	Time  0.023 ( 0.024)	Data  0.001 ( 0.002)	Loss 5.5371e-01 (3.1951e-01)	Acc@1  80.47 ( 89.33)	Acc@5  98.44 ( 99.51)
Epoch: [25][370/391]	Time  0.019 ( 0.024)	Data  0.002 ( 0.002)	Loss 2.2864e-01 (3.1928e-01)	Acc@1  89.84 ( 89.32)	Acc@5 100.00 ( 99.52)
Epoch: [25][380/391]	Time  0.024 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.1570e-01 (3.1902e-01)	Acc@1  92.97 ( 89.32)	Acc@5  99.22 ( 99.51)
Epoch: [25][390/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.7603e-01 (3.1818e-01)	Acc@1  93.75 ( 89.34)	Acc@5 100.00 ( 99.52)
## e[25] optimizer.zero_grad (sum) time: 0.10477805137634277
## e[25]       loss.backward (sum) time: 2.1405766010284424
## e[25]      optimizer.step (sum) time: 0.8845992088317871
## epoch[25] training(only) time: 9.6587975025177
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 4.2480e-01 (4.2480e-01)	Acc@1  85.00 ( 85.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.008 ( 0.026)	Loss 6.1865e-01 (4.7243e-01)	Acc@1  84.00 ( 85.18)	Acc@5  99.00 ( 98.82)
Test: [ 20/100]	Time  0.008 ( 0.020)	Loss 5.7910e-01 (4.9974e-01)	Acc@1  81.00 ( 83.52)	Acc@5 100.00 ( 98.81)
Test: [ 30/100]	Time  0.010 ( 0.018)	Loss 4.1626e-01 (5.0780e-01)	Acc@1  86.00 ( 83.84)	Acc@5  98.00 ( 98.84)
Test: [ 40/100]	Time  0.008 ( 0.017)	Loss 5.0732e-01 (5.0829e-01)	Acc@1  87.00 ( 84.10)	Acc@5  99.00 ( 98.95)
Test: [ 50/100]	Time  0.016 ( 0.016)	Loss 4.5410e-01 (4.9464e-01)	Acc@1  86.00 ( 84.49)	Acc@5 100.00 ( 99.04)
Test: [ 60/100]	Time  0.015 ( 0.016)	Loss 4.5654e-01 (4.9384e-01)	Acc@1  85.00 ( 84.52)	Acc@5  98.00 ( 99.02)
Test: [ 70/100]	Time  0.010 ( 0.015)	Loss 6.6211e-01 (4.9024e-01)	Acc@1  82.00 ( 84.48)	Acc@5 100.00 ( 99.11)
Test: [ 80/100]	Time  0.012 ( 0.015)	Loss 3.5767e-01 (4.8872e-01)	Acc@1  89.00 ( 84.60)	Acc@5  99.00 ( 99.12)
Test: [ 90/100]	Time  0.008 ( 0.015)	Loss 4.6167e-01 (4.8608e-01)	Acc@1  85.00 ( 84.66)	Acc@5  99.00 ( 99.13)
 * Acc@1 84.680 Acc@5 99.180
### epoch[25] execution time: 11.202239036560059
EPOCH 26
# current learning rate: 0.1
# Switched to train mode...
Epoch: [26][  0/391]	Time  0.183 ( 0.183)	Data  0.162 ( 0.162)	Loss 3.4546e-01 (3.4546e-01)	Acc@1  87.50 ( 87.50)	Acc@5  99.22 ( 99.22)
Epoch: [26][ 10/391]	Time  0.027 ( 0.038)	Data  0.001 ( 0.016)	Loss 2.8027e-01 (2.7902e-01)	Acc@1  92.19 ( 90.77)	Acc@5 100.00 ( 99.86)
Epoch: [26][ 20/391]	Time  0.037 ( 0.032)	Data  0.005 ( 0.010)	Loss 2.5610e-01 (2.8637e-01)	Acc@1  92.19 ( 90.48)	Acc@5 100.00 ( 99.89)
Epoch: [26][ 30/391]	Time  0.024 ( 0.030)	Data  0.001 ( 0.007)	Loss 2.6099e-01 (2.8330e-01)	Acc@1  92.19 ( 90.55)	Acc@5 100.00 ( 99.82)
Epoch: [26][ 40/391]	Time  0.034 ( 0.028)	Data  0.001 ( 0.006)	Loss 5.4785e-01 (2.9523e-01)	Acc@1  83.59 ( 90.30)	Acc@5  99.22 ( 99.79)
Epoch: [26][ 50/391]	Time  0.017 ( 0.027)	Data  0.000 ( 0.005)	Loss 2.8784e-01 (3.0512e-01)	Acc@1  93.75 ( 89.98)	Acc@5 100.00 ( 99.69)
Epoch: [26][ 60/391]	Time  0.027 ( 0.027)	Data  0.000 ( 0.005)	Loss 3.5962e-01 (3.1166e-01)	Acc@1  90.62 ( 89.86)	Acc@5 100.00 ( 99.68)
Epoch: [26][ 70/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.004)	Loss 2.7319e-01 (3.1046e-01)	Acc@1  89.06 ( 89.85)	Acc@5 100.00 ( 99.64)
Epoch: [26][ 80/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 2.8101e-01 (3.0840e-01)	Acc@1  91.41 ( 89.96)	Acc@5 100.00 ( 99.62)
Epoch: [26][ 90/391]	Time  0.017 ( 0.026)	Data  0.000 ( 0.004)	Loss 2.7026e-01 (3.0662e-01)	Acc@1  90.62 ( 90.05)	Acc@5 100.00 ( 99.63)
Epoch: [26][100/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 2.9224e-01 (3.0832e-01)	Acc@1  88.28 ( 89.99)	Acc@5  99.22 ( 99.64)
Epoch: [26][110/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 2.7295e-01 (3.0500e-01)	Acc@1  92.19 ( 90.10)	Acc@5 100.00 ( 99.66)
Epoch: [26][120/391]	Time  0.016 ( 0.025)	Data  0.000 ( 0.003)	Loss 3.4497e-01 (3.0333e-01)	Acc@1  90.62 ( 90.15)	Acc@5  99.22 ( 99.65)
Epoch: [26][130/391]	Time  0.024 ( 0.025)	Data  0.005 ( 0.003)	Loss 2.0776e-01 (3.0131e-01)	Acc@1  92.97 ( 90.18)	Acc@5 100.00 ( 99.65)
Epoch: [26][140/391]	Time  0.019 ( 0.025)	Data  0.003 ( 0.003)	Loss 4.5947e-01 (3.0168e-01)	Acc@1  87.50 ( 90.21)	Acc@5  99.22 ( 99.67)
Epoch: [26][150/391]	Time  0.025 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.2983e-01 (3.0585e-01)	Acc@1  86.72 ( 90.07)	Acc@5  99.22 ( 99.64)
Epoch: [26][160/391]	Time  0.033 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.2363e-01 (3.0803e-01)	Acc@1  92.19 ( 90.02)	Acc@5 100.00 ( 99.63)
Epoch: [26][170/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.3962e-01 (3.0696e-01)	Acc@1  92.97 ( 90.07)	Acc@5  98.44 ( 99.61)
Epoch: [26][180/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.8564e-01 (3.0861e-01)	Acc@1  88.28 ( 89.99)	Acc@5 100.00 ( 99.61)
Epoch: [26][190/391]	Time  0.025 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.1836e-01 (3.0967e-01)	Acc@1  89.06 ( 89.96)	Acc@5  98.44 ( 99.60)
Epoch: [26][200/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.1592e-01 (3.1099e-01)	Acc@1  89.84 ( 89.90)	Acc@5 100.00 ( 99.61)
Epoch: [26][210/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.4214e-01 (3.1352e-01)	Acc@1  86.72 ( 89.83)	Acc@5  99.22 ( 99.60)
Epoch: [26][220/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.1445e-01 (3.1200e-01)	Acc@1  89.84 ( 89.86)	Acc@5  98.44 ( 99.61)
Epoch: [26][230/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.7646e-01 (3.1306e-01)	Acc@1  86.72 ( 89.81)	Acc@5  99.22 ( 99.60)
Epoch: [26][240/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.0322e-01 (3.1291e-01)	Acc@1  89.06 ( 89.82)	Acc@5 100.00 ( 99.60)
Epoch: [26][250/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.6025e-01 (3.1352e-01)	Acc@1  87.50 ( 89.79)	Acc@5  99.22 ( 99.60)
Epoch: [26][260/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.002)	Loss 2.1631e-01 (3.1362e-01)	Acc@1  94.53 ( 89.75)	Acc@5 100.00 ( 99.60)
Epoch: [26][270/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.3164e-01 (3.1415e-01)	Acc@1  83.59 ( 89.72)	Acc@5 100.00 ( 99.60)
Epoch: [26][280/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.6523e-01 (3.1577e-01)	Acc@1  91.41 ( 89.71)	Acc@5  99.22 ( 99.59)
Epoch: [26][290/391]	Time  0.027 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.3669e-01 (3.1584e-01)	Acc@1  92.97 ( 89.71)	Acc@5 100.00 ( 99.60)
Epoch: [26][300/391]	Time  0.033 ( 0.025)	Data  0.000 ( 0.002)	Loss 1.9177e-01 (3.1378e-01)	Acc@1  91.41 ( 89.74)	Acc@5 100.00 ( 99.62)
Epoch: [26][310/391]	Time  0.025 ( 0.025)	Data  0.002 ( 0.002)	Loss 1.9250e-01 (3.1381e-01)	Acc@1  94.53 ( 89.75)	Acc@5 100.00 ( 99.61)
Epoch: [26][320/391]	Time  0.038 ( 0.025)	Data  0.004 ( 0.002)	Loss 3.7451e-01 (3.1344e-01)	Acc@1  85.16 ( 89.76)	Acc@5 100.00 ( 99.62)
Epoch: [26][330/391]	Time  0.024 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.6465e-01 (3.1357e-01)	Acc@1  90.62 ( 89.73)	Acc@5 100.00 ( 99.62)
Epoch: [26][340/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.1470e-01 (3.1392e-01)	Acc@1  89.06 ( 89.71)	Acc@5  99.22 ( 99.62)
Epoch: [26][350/391]	Time  0.053 ( 0.024)	Data  0.009 ( 0.002)	Loss 4.0137e-01 (3.1392e-01)	Acc@1  88.28 ( 89.72)	Acc@5 100.00 ( 99.62)
Epoch: [26][360/391]	Time  0.038 ( 0.024)	Data  0.012 ( 0.002)	Loss 2.6025e-01 (3.1454e-01)	Acc@1  92.19 ( 89.68)	Acc@5 100.00 ( 99.62)
Epoch: [26][370/391]	Time  0.039 ( 0.024)	Data  0.003 ( 0.002)	Loss 4.4556e-01 (3.1540e-01)	Acc@1  85.16 ( 89.64)	Acc@5 100.00 ( 99.62)
Epoch: [26][380/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.7539e-01 (3.1564e-01)	Acc@1  91.41 ( 89.64)	Acc@5 100.00 ( 99.62)
Epoch: [26][390/391]	Time  0.014 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.9771e-01 (3.1555e-01)	Acc@1  85.00 ( 89.63)	Acc@5  98.75 ( 99.61)
## e[26] optimizer.zero_grad (sum) time: 0.10611605644226074
## e[26]       loss.backward (sum) time: 2.111149787902832
## e[26]      optimizer.step (sum) time: 0.8974969387054443
## epoch[26] training(only) time: 9.648589611053467
# Switched to evaluate mode...
Test: [  0/100]	Time  0.162 ( 0.162)	Loss 4.5679e-01 (4.5679e-01)	Acc@1  87.00 ( 87.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.013 ( 0.026)	Loss 4.5386e-01 (4.6466e-01)	Acc@1  85.00 ( 85.55)	Acc@5  99.00 ( 99.45)
Test: [ 20/100]	Time  0.014 ( 0.020)	Loss 5.1270e-01 (4.7077e-01)	Acc@1  81.00 ( 84.86)	Acc@5  99.00 ( 99.33)
Test: [ 30/100]	Time  0.012 ( 0.018)	Loss 5.4199e-01 (4.7472e-01)	Acc@1  83.00 ( 84.65)	Acc@5  99.00 ( 99.32)
Test: [ 40/100]	Time  0.008 ( 0.017)	Loss 5.3467e-01 (4.8410e-01)	Acc@1  82.00 ( 84.73)	Acc@5  98.00 ( 99.20)
Test: [ 50/100]	Time  0.008 ( 0.016)	Loss 3.8647e-01 (4.7053e-01)	Acc@1  87.00 ( 85.00)	Acc@5  98.00 ( 99.24)
Test: [ 60/100]	Time  0.016 ( 0.016)	Loss 3.3911e-01 (4.6718e-01)	Acc@1  91.00 ( 85.13)	Acc@5 100.00 ( 99.26)
Test: [ 70/100]	Time  0.008 ( 0.015)	Loss 8.2617e-01 (4.6977e-01)	Acc@1  79.00 ( 85.04)	Acc@5 100.00 ( 99.24)
Test: [ 80/100]	Time  0.008 ( 0.015)	Loss 3.7158e-01 (4.6925e-01)	Acc@1  86.00 ( 84.91)	Acc@5 100.00 ( 99.23)
Test: [ 90/100]	Time  0.012 ( 0.015)	Loss 3.2007e-01 (4.6924e-01)	Acc@1  89.00 ( 84.93)	Acc@5 100.00 ( 99.26)
 * Acc@1 85.050 Acc@5 99.290
### epoch[26] execution time: 11.252433776855469
EPOCH 27
# current learning rate: 0.1
# Switched to train mode...
Epoch: [27][  0/391]	Time  0.198 ( 0.198)	Data  0.177 ( 0.177)	Loss 3.2593e-01 (3.2593e-01)	Acc@1  89.84 ( 89.84)	Acc@5  99.22 ( 99.22)
Epoch: [27][ 10/391]	Time  0.019 ( 0.038)	Data  0.001 ( 0.018)	Loss 1.1938e-01 (2.7430e-01)	Acc@1  96.88 ( 91.48)	Acc@5 100.00 ( 99.57)
Epoch: [27][ 20/391]	Time  0.021 ( 0.032)	Data  0.001 ( 0.010)	Loss 1.7590e-01 (2.7080e-01)	Acc@1  92.97 ( 91.41)	Acc@5 100.00 ( 99.52)
Epoch: [27][ 30/391]	Time  0.036 ( 0.029)	Data  0.001 ( 0.007)	Loss 2.6440e-01 (2.7580e-01)	Acc@1  92.19 ( 91.23)	Acc@5  99.22 ( 99.47)
Epoch: [27][ 40/391]	Time  0.028 ( 0.028)	Data  0.000 ( 0.006)	Loss 4.9390e-01 (2.8201e-01)	Acc@1  86.72 ( 90.91)	Acc@5 100.00 ( 99.54)
Epoch: [27][ 50/391]	Time  0.030 ( 0.027)	Data  0.000 ( 0.005)	Loss 2.8809e-01 (2.8342e-01)	Acc@1  89.06 ( 90.87)	Acc@5  98.44 ( 99.59)
Epoch: [27][ 60/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.005)	Loss 2.4048e-01 (2.9201e-01)	Acc@1  89.84 ( 90.47)	Acc@5 100.00 ( 99.56)
Epoch: [27][ 70/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.004)	Loss 4.1260e-01 (2.9475e-01)	Acc@1  86.72 ( 90.36)	Acc@5 100.00 ( 99.58)
Epoch: [27][ 80/391]	Time  0.037 ( 0.026)	Data  0.001 ( 0.004)	Loss 2.3352e-01 (2.9633e-01)	Acc@1  92.19 ( 90.22)	Acc@5 100.00 ( 99.60)
Epoch: [27][ 90/391]	Time  0.037 ( 0.026)	Data  0.002 ( 0.004)	Loss 4.0454e-01 (2.9618e-01)	Acc@1  85.94 ( 90.23)	Acc@5 100.00 ( 99.62)
Epoch: [27][100/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.004)	Loss 3.1494e-01 (2.9424e-01)	Acc@1  86.72 ( 90.25)	Acc@5 100.00 ( 99.61)
Epoch: [27][110/391]	Time  0.041 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.0000e-01 (2.9528e-01)	Acc@1  81.25 ( 90.17)	Acc@5  99.22 ( 99.63)
Epoch: [27][120/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.6387e-01 (2.9482e-01)	Acc@1  86.72 ( 90.19)	Acc@5 100.00 ( 99.63)
Epoch: [27][130/391]	Time  0.025 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.0762e-01 (2.9443e-01)	Acc@1  89.84 ( 90.20)	Acc@5 100.00 ( 99.63)
Epoch: [27][140/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.8250e-01 (2.9400e-01)	Acc@1  95.31 ( 90.24)	Acc@5 100.00 ( 99.63)
Epoch: [27][150/391]	Time  0.017 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.6890e-01 (2.9251e-01)	Acc@1  88.28 ( 90.29)	Acc@5 100.00 ( 99.64)
Epoch: [27][160/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.8394e-01 (2.9209e-01)	Acc@1  88.28 ( 90.30)	Acc@5  99.22 ( 99.64)
Epoch: [27][170/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.8452e-01 (2.9345e-01)	Acc@1  89.84 ( 90.21)	Acc@5  98.44 ( 99.64)
Epoch: [27][180/391]	Time  0.017 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.6660e-01 (2.9274e-01)	Acc@1  89.06 ( 90.21)	Acc@5 100.00 ( 99.66)
Epoch: [27][190/391]	Time  0.029 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.2847e-01 (2.9510e-01)	Acc@1  86.72 ( 90.18)	Acc@5  99.22 ( 99.66)
Epoch: [27][200/391]	Time  0.042 ( 0.025)	Data  0.000 ( 0.003)	Loss 2.9321e-01 (2.9590e-01)	Acc@1  89.06 ( 90.12)	Acc@5 100.00 ( 99.65)
Epoch: [27][210/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.5571e-01 (2.9807e-01)	Acc@1  88.28 ( 90.05)	Acc@5 100.00 ( 99.65)
Epoch: [27][220/391]	Time  0.023 ( 0.025)	Data  0.002 ( 0.003)	Loss 2.2852e-01 (3.0073e-01)	Acc@1  94.53 ( 89.93)	Acc@5 100.00 ( 99.64)
Epoch: [27][230/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.8135e-01 (3.0101e-01)	Acc@1  87.50 ( 89.91)	Acc@5  98.44 ( 99.64)
Epoch: [27][240/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.8054e-01 (3.0213e-01)	Acc@1  92.19 ( 89.89)	Acc@5 100.00 ( 99.65)
Epoch: [27][250/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.0552e-01 (3.0171e-01)	Acc@1  89.84 ( 89.86)	Acc@5 100.00 ( 99.65)
Epoch: [27][260/391]	Time  0.029 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.1982e-01 (3.0081e-01)	Acc@1  88.28 ( 89.84)	Acc@5  99.22 ( 99.64)
Epoch: [27][270/391]	Time  0.030 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.7197e-01 (3.0282e-01)	Acc@1  91.41 ( 89.79)	Acc@5 100.00 ( 99.63)
Epoch: [27][280/391]	Time  0.036 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.2480e-01 (3.0346e-01)	Acc@1  87.50 ( 89.77)	Acc@5  98.44 ( 99.63)
Epoch: [27][290/391]	Time  0.039 ( 0.025)	Data  0.004 ( 0.002)	Loss 2.9395e-01 (3.0225e-01)	Acc@1  88.28 ( 89.81)	Acc@5 100.00 ( 99.63)
Epoch: [27][300/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.002)	Loss 3.9526e-01 (3.0257e-01)	Acc@1  86.72 ( 89.80)	Acc@5  98.44 ( 99.62)
Epoch: [27][310/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.7573e-01 (3.0406e-01)	Acc@1  87.50 ( 89.78)	Acc@5  99.22 ( 99.61)
Epoch: [27][320/391]	Time  0.025 ( 0.025)	Data  0.000 ( 0.002)	Loss 4.9219e-01 (3.0547e-01)	Acc@1  84.38 ( 89.76)	Acc@5  99.22 ( 99.61)
Epoch: [27][330/391]	Time  0.025 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.9639e-01 (3.0454e-01)	Acc@1  92.97 ( 89.82)	Acc@5 100.00 ( 99.61)
Epoch: [27][340/391]	Time  0.020 ( 0.025)	Data  0.003 ( 0.002)	Loss 3.4814e-01 (3.0497e-01)	Acc@1  89.06 ( 89.82)	Acc@5  99.22 ( 99.60)
Epoch: [27][350/391]	Time  0.025 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.9868e-01 (3.0673e-01)	Acc@1  86.72 ( 89.76)	Acc@5 100.00 ( 99.59)
Epoch: [27][360/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.7876e-01 (3.0761e-01)	Acc@1  84.38 ( 89.74)	Acc@5  99.22 ( 99.59)
Epoch: [27][370/391]	Time  0.025 ( 0.025)	Data  0.002 ( 0.002)	Loss 3.5889e-01 (3.0855e-01)	Acc@1  89.84 ( 89.72)	Acc@5  99.22 ( 99.59)
Epoch: [27][380/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.3667e-01 (3.0921e-01)	Acc@1  85.16 ( 89.68)	Acc@5 100.00 ( 99.58)
Epoch: [27][390/391]	Time  0.014 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.5718e-01 (3.0958e-01)	Acc@1  87.50 ( 89.69)	Acc@5 100.00 ( 99.58)
## e[27] optimizer.zero_grad (sum) time: 0.10659146308898926
## e[27]       loss.backward (sum) time: 2.17081356048584
## e[27]      optimizer.step (sum) time: 0.8901002407073975
## epoch[27] training(only) time: 9.684781551361084
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 3.6255e-01 (3.6255e-01)	Acc@1  84.00 ( 84.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.013 ( 0.027)	Loss 5.5811e-01 (4.9365e-01)	Acc@1  83.00 ( 84.09)	Acc@5  98.00 ( 99.18)
Test: [ 20/100]	Time  0.013 ( 0.020)	Loss 5.7764e-01 (4.8288e-01)	Acc@1  81.00 ( 84.19)	Acc@5 100.00 ( 99.38)
Test: [ 30/100]	Time  0.025 ( 0.018)	Loss 4.0234e-01 (4.7738e-01)	Acc@1  88.00 ( 84.97)	Acc@5  99.00 ( 99.29)
Test: [ 40/100]	Time  0.007 ( 0.017)	Loss 5.7617e-01 (4.9051e-01)	Acc@1  82.00 ( 84.80)	Acc@5  99.00 ( 99.27)
Test: [ 50/100]	Time  0.020 ( 0.016)	Loss 4.0723e-01 (4.8070e-01)	Acc@1  88.00 ( 85.25)	Acc@5 100.00 ( 99.31)
Test: [ 60/100]	Time  0.008 ( 0.016)	Loss 4.5776e-01 (4.8327e-01)	Acc@1  81.00 ( 85.03)	Acc@5 100.00 ( 99.31)
Test: [ 70/100]	Time  0.009 ( 0.015)	Loss 5.5908e-01 (4.8940e-01)	Acc@1  85.00 ( 84.86)	Acc@5  99.00 ( 99.30)
Test: [ 80/100]	Time  0.017 ( 0.015)	Loss 4.1919e-01 (4.8394e-01)	Acc@1  86.00 ( 84.89)	Acc@5  98.00 ( 99.32)
Test: [ 90/100]	Time  0.009 ( 0.015)	Loss 2.4854e-01 (4.8306e-01)	Acc@1  92.00 ( 84.84)	Acc@5  99.00 ( 99.29)
 * Acc@1 84.900 Acc@5 99.330
### epoch[27] execution time: 11.297183752059937
EPOCH 28
# current learning rate: 0.1
# Switched to train mode...
Epoch: [28][  0/391]	Time  0.194 ( 0.194)	Data  0.173 ( 0.173)	Loss 3.0176e-01 (3.0176e-01)	Acc@1  91.41 ( 91.41)	Acc@5 100.00 (100.00)
Epoch: [28][ 10/391]	Time  0.028 ( 0.039)	Data  0.002 ( 0.018)	Loss 3.1274e-01 (2.6588e-01)	Acc@1  89.84 ( 91.34)	Acc@5 100.00 ( 99.86)
Epoch: [28][ 20/391]	Time  0.018 ( 0.032)	Data  0.001 ( 0.010)	Loss 2.4988e-01 (2.6227e-01)	Acc@1  92.19 ( 91.41)	Acc@5 100.00 ( 99.93)
Epoch: [28][ 30/391]	Time  0.018 ( 0.029)	Data  0.001 ( 0.007)	Loss 2.6758e-01 (2.6343e-01)	Acc@1  90.62 ( 91.36)	Acc@5 100.00 ( 99.85)
Epoch: [28][ 40/391]	Time  0.029 ( 0.028)	Data  0.003 ( 0.006)	Loss 2.1252e-01 (2.6470e-01)	Acc@1  92.97 ( 91.12)	Acc@5  99.22 ( 99.81)
Epoch: [28][ 50/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.005)	Loss 3.2886e-01 (2.7450e-01)	Acc@1  87.50 ( 90.76)	Acc@5 100.00 ( 99.79)
Epoch: [28][ 60/391]	Time  0.035 ( 0.027)	Data  0.004 ( 0.005)	Loss 3.6157e-01 (2.8179e-01)	Acc@1  89.06 ( 90.48)	Acc@5 100.00 ( 99.73)
Epoch: [28][ 70/391]	Time  0.018 ( 0.026)	Data  0.002 ( 0.004)	Loss 3.1201e-01 (2.8246e-01)	Acc@1  91.41 ( 90.57)	Acc@5  99.22 ( 99.70)
Epoch: [28][ 80/391]	Time  0.032 ( 0.026)	Data  0.002 ( 0.004)	Loss 2.6074e-01 (2.8507e-01)	Acc@1  92.19 ( 90.46)	Acc@5 100.00 ( 99.67)
Epoch: [28][ 90/391]	Time  0.035 ( 0.026)	Data  0.001 ( 0.004)	Loss 2.5708e-01 (2.8422e-01)	Acc@1  92.97 ( 90.47)	Acc@5  99.22 ( 99.69)
Epoch: [28][100/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3694e-01 (2.8300e-01)	Acc@1  91.41 ( 90.49)	Acc@5 100.00 ( 99.70)
Epoch: [28][110/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.8491e-01 (2.8374e-01)	Acc@1  91.41 ( 90.53)	Acc@5  99.22 ( 99.67)
Epoch: [28][120/391]	Time  0.025 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.3862e-01 (2.8500e-01)	Acc@1  88.28 ( 90.50)	Acc@5 100.00 ( 99.68)
Epoch: [28][130/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.1758e-01 (2.8721e-01)	Acc@1  82.03 ( 90.39)	Acc@5  99.22 ( 99.69)
Epoch: [28][140/391]	Time  0.034 ( 0.025)	Data  0.004 ( 0.003)	Loss 3.4106e-01 (2.9343e-01)	Acc@1  89.84 ( 90.26)	Acc@5 100.00 ( 99.68)
Epoch: [28][150/391]	Time  0.041 ( 0.025)	Data  0.003 ( 0.003)	Loss 2.3083e-01 (2.9125e-01)	Acc@1  92.19 ( 90.28)	Acc@5 100.00 ( 99.69)
Epoch: [28][160/391]	Time  0.027 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.9663e-01 (2.8973e-01)	Acc@1  89.06 ( 90.33)	Acc@5 100.00 ( 99.70)
Epoch: [28][170/391]	Time  0.031 ( 0.025)	Data  0.000 ( 0.003)	Loss 2.0947e-01 (2.8865e-01)	Acc@1  93.75 ( 90.39)	Acc@5 100.00 ( 99.69)
Epoch: [28][180/391]	Time  0.026 ( 0.025)	Data  0.005 ( 0.003)	Loss 3.4766e-01 (2.8706e-01)	Acc@1  85.94 ( 90.46)	Acc@5 100.00 ( 99.70)
Epoch: [28][190/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.1675e-01 (2.8829e-01)	Acc@1  85.16 ( 90.38)	Acc@5  98.44 ( 99.69)
Epoch: [28][200/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.6108e-01 (2.8987e-01)	Acc@1  89.06 ( 90.38)	Acc@5 100.00 ( 99.69)
Epoch: [28][210/391]	Time  0.031 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.9404e-01 (2.9095e-01)	Acc@1  86.72 ( 90.34)	Acc@5  98.44 ( 99.67)
Epoch: [28][220/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.0283e-01 (2.9231e-01)	Acc@1  88.28 ( 90.30)	Acc@5 100.00 ( 99.66)
Epoch: [28][230/391]	Time  0.031 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.5132e-01 (2.9378e-01)	Acc@1  86.72 ( 90.24)	Acc@5 100.00 ( 99.66)
Epoch: [28][240/391]	Time  0.028 ( 0.025)	Data  0.000 ( 0.003)	Loss 2.5391e-01 (2.9322e-01)	Acc@1  92.97 ( 90.26)	Acc@5 100.00 ( 99.65)
Epoch: [28][250/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.4133e-01 (2.9207e-01)	Acc@1  92.19 ( 90.29)	Acc@5  99.22 ( 99.65)
Epoch: [28][260/391]	Time  0.039 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.7834e-01 (2.9296e-01)	Acc@1  93.75 ( 90.27)	Acc@5 100.00 ( 99.65)
Epoch: [28][270/391]	Time  0.021 ( 0.025)	Data  0.000 ( 0.002)	Loss 4.4824e-01 (2.9368e-01)	Acc@1  86.72 ( 90.27)	Acc@5  97.66 ( 99.65)
Epoch: [28][280/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.002)	Loss 3.8330e-01 (2.9295e-01)	Acc@1  89.06 ( 90.32)	Acc@5  97.66 ( 99.64)
Epoch: [28][290/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.6768e-01 (2.9160e-01)	Acc@1  87.50 ( 90.35)	Acc@5 100.00 ( 99.65)
Epoch: [28][300/391]	Time  0.036 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.7817e-01 (2.9227e-01)	Acc@1  89.84 ( 90.34)	Acc@5  99.22 ( 99.65)
Epoch: [28][310/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 5.0049e-01 (2.9455e-01)	Acc@1  81.25 ( 90.25)	Acc@5  98.44 ( 99.65)
Epoch: [28][320/391]	Time  0.034 ( 0.024)	Data  0.003 ( 0.002)	Loss 2.8516e-01 (2.9506e-01)	Acc@1  92.19 ( 90.26)	Acc@5  99.22 ( 99.65)
Epoch: [28][330/391]	Time  0.020 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.7026e-01 (2.9485e-01)	Acc@1  90.62 ( 90.24)	Acc@5 100.00 ( 99.66)
Epoch: [28][340/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.4766e-01 (2.9491e-01)	Acc@1  86.72 ( 90.24)	Acc@5 100.00 ( 99.65)
Epoch: [28][350/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 5.0830e-01 (2.9510e-01)	Acc@1  86.72 ( 90.24)	Acc@5 100.00 ( 99.65)
Epoch: [28][360/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.7476e-01 (2.9609e-01)	Acc@1  87.50 ( 90.21)	Acc@5  99.22 ( 99.65)
Epoch: [28][370/391]	Time  0.038 ( 0.024)	Data  0.004 ( 0.002)	Loss 3.0664e-01 (2.9648e-01)	Acc@1  89.06 ( 90.19)	Acc@5 100.00 ( 99.65)
Epoch: [28][380/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.3789e-01 (2.9684e-01)	Acc@1  88.28 ( 90.19)	Acc@5  99.22 ( 99.66)
Epoch: [28][390/391]	Time  0.016 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.1860e-01 (2.9706e-01)	Acc@1  88.75 ( 90.20)	Acc@5 100.00 ( 99.65)
## e[28] optimizer.zero_grad (sum) time: 0.10528850555419922
## e[28]       loss.backward (sum) time: 2.1904406547546387
## e[28]      optimizer.step (sum) time: 0.8854107856750488
## epoch[28] training(only) time: 9.625895977020264
# Switched to evaluate mode...
Test: [  0/100]	Time  0.205 ( 0.205)	Loss 4.3970e-01 (4.3970e-01)	Acc@1  86.00 ( 86.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.015 ( 0.031)	Loss 4.7705e-01 (4.4775e-01)	Acc@1  85.00 ( 85.45)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.016 ( 0.022)	Loss 5.1416e-01 (4.6336e-01)	Acc@1  83.00 ( 84.81)	Acc@5  99.00 ( 99.43)
Test: [ 30/100]	Time  0.018 ( 0.019)	Loss 4.9194e-01 (4.7304e-01)	Acc@1  84.00 ( 84.84)	Acc@5 100.00 ( 99.42)
Test: [ 40/100]	Time  0.012 ( 0.018)	Loss 5.2686e-01 (4.7615e-01)	Acc@1  82.00 ( 85.00)	Acc@5  98.00 ( 99.34)
Test: [ 50/100]	Time  0.019 ( 0.017)	Loss 5.4395e-01 (4.6669e-01)	Acc@1  82.00 ( 85.08)	Acc@5  99.00 ( 99.31)
Test: [ 60/100]	Time  0.021 ( 0.016)	Loss 4.0771e-01 (4.6463e-01)	Acc@1  84.00 ( 85.00)	Acc@5 100.00 ( 99.38)
Test: [ 70/100]	Time  0.019 ( 0.016)	Loss 7.2363e-01 (4.7047e-01)	Acc@1  80.00 ( 84.96)	Acc@5  99.00 ( 99.38)
Test: [ 80/100]	Time  0.031 ( 0.015)	Loss 3.2959e-01 (4.6251e-01)	Acc@1  90.00 ( 85.19)	Acc@5  99.00 ( 99.40)
Test: [ 90/100]	Time  0.008 ( 0.015)	Loss 2.6587e-01 (4.5945e-01)	Acc@1  92.00 ( 85.16)	Acc@5 100.00 ( 99.37)
 * Acc@1 85.190 Acc@5 99.420
### epoch[28] execution time: 11.227238893508911
EPOCH 29
# current learning rate: 0.1
# Switched to train mode...
Epoch: [29][  0/391]	Time  0.228 ( 0.228)	Data  0.205 ( 0.205)	Loss 2.5464e-01 (2.5464e-01)	Acc@1  89.84 ( 89.84)	Acc@5 100.00 (100.00)
Epoch: [29][ 10/391]	Time  0.018 ( 0.041)	Data  0.001 ( 0.020)	Loss 1.7126e-01 (2.3849e-01)	Acc@1  94.53 ( 92.47)	Acc@5 100.00 (100.00)
Epoch: [29][ 20/391]	Time  0.018 ( 0.033)	Data  0.001 ( 0.011)	Loss 1.8835e-01 (2.5268e-01)	Acc@1  93.75 ( 92.08)	Acc@5 100.00 ( 99.78)
Epoch: [29][ 30/391]	Time  0.036 ( 0.030)	Data  0.002 ( 0.008)	Loss 2.0081e-01 (2.4828e-01)	Acc@1  92.19 ( 91.73)	Acc@5 100.00 ( 99.82)
Epoch: [29][ 40/391]	Time  0.018 ( 0.029)	Data  0.001 ( 0.007)	Loss 3.1567e-01 (2.5580e-01)	Acc@1  89.84 ( 91.43)	Acc@5  99.22 ( 99.81)
Epoch: [29][ 50/391]	Time  0.018 ( 0.028)	Data  0.001 ( 0.006)	Loss 2.4414e-01 (2.6202e-01)	Acc@1  89.84 ( 91.08)	Acc@5 100.00 ( 99.80)
Epoch: [29][ 60/391]	Time  0.032 ( 0.027)	Data  0.000 ( 0.005)	Loss 4.1113e-01 (2.6806e-01)	Acc@1  84.38 ( 90.86)	Acc@5 100.00 ( 99.78)
Epoch: [29][ 70/391]	Time  0.033 ( 0.027)	Data  0.000 ( 0.005)	Loss 2.9980e-01 (2.7410e-01)	Acc@1  83.59 ( 90.51)	Acc@5 100.00 ( 99.77)
Epoch: [29][ 80/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 2.8076e-01 (2.7673e-01)	Acc@1  90.62 ( 90.53)	Acc@5 100.00 ( 99.76)
Epoch: [29][ 90/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.004)	Loss 2.5342e-01 (2.7900e-01)	Acc@1  88.28 ( 90.35)	Acc@5 100.00 ( 99.75)
Epoch: [29][100/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 2.6611e-01 (2.8078e-01)	Acc@1  87.50 ( 90.24)	Acc@5 100.00 ( 99.75)
Epoch: [29][110/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.004)	Loss 3.0615e-01 (2.8649e-01)	Acc@1  89.06 ( 90.17)	Acc@5 100.00 ( 99.72)
Epoch: [29][120/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5342e-01 (2.8275e-01)	Acc@1  90.62 ( 90.30)	Acc@5 100.00 ( 99.74)
Epoch: [29][130/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.6724e-01 (2.8005e-01)	Acc@1  95.31 ( 90.36)	Acc@5 100.00 ( 99.73)
Epoch: [29][140/391]	Time  0.031 ( 0.025)	Data  0.003 ( 0.003)	Loss 2.9639e-01 (2.8054e-01)	Acc@1  92.19 ( 90.36)	Acc@5 100.00 ( 99.72)
Epoch: [29][150/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.4414e-01 (2.8276e-01)	Acc@1  90.62 ( 90.40)	Acc@5 100.00 ( 99.72)
Epoch: [29][160/391]	Time  0.027 ( 0.025)	Data  0.002 ( 0.003)	Loss 3.9941e-01 (2.8697e-01)	Acc@1  85.16 ( 90.31)	Acc@5 100.00 ( 99.71)
Epoch: [29][170/391]	Time  0.031 ( 0.025)	Data  0.000 ( 0.003)	Loss 3.1641e-01 (2.8708e-01)	Acc@1  88.28 ( 90.30)	Acc@5 100.00 ( 99.72)
Epoch: [29][180/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.0820e-01 (2.8812e-01)	Acc@1  87.50 ( 90.25)	Acc@5 100.00 ( 99.72)
Epoch: [29][190/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.0093e-01 (2.8728e-01)	Acc@1  92.19 ( 90.27)	Acc@5 100.00 ( 99.71)
Epoch: [29][200/391]	Time  0.033 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.8188e-01 (2.8714e-01)	Acc@1  92.19 ( 90.28)	Acc@5 100.00 ( 99.72)
Epoch: [29][210/391]	Time  0.035 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.0200e-01 (2.8726e-01)	Acc@1  89.06 ( 90.34)	Acc@5 100.00 ( 99.71)
Epoch: [29][220/391]	Time  0.022 ( 0.025)	Data  0.002 ( 0.003)	Loss 2.9175e-01 (2.8824e-01)	Acc@1  88.28 ( 90.32)	Acc@5 100.00 ( 99.70)
Epoch: [29][230/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.4297e-01 (2.8759e-01)	Acc@1  84.38 ( 90.34)	Acc@5  98.44 ( 99.69)
Epoch: [29][240/391]	Time  0.037 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.8438e-01 (2.8848e-01)	Acc@1  82.81 ( 90.29)	Acc@5  99.22 ( 99.69)
Epoch: [29][250/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.7905e-01 (2.8903e-01)	Acc@1  89.84 ( 90.30)	Acc@5 100.00 ( 99.69)
Epoch: [29][260/391]	Time  0.038 ( 0.025)	Data  0.000 ( 0.003)	Loss 3.3496e-01 (2.9062e-01)	Acc@1  87.50 ( 90.24)	Acc@5 100.00 ( 99.68)
Epoch: [29][270/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.6060e-01 (2.9159e-01)	Acc@1  90.62 ( 90.19)	Acc@5 100.00 ( 99.68)
Epoch: [29][280/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.7964e-01 (2.9233e-01)	Acc@1  85.94 ( 90.13)	Acc@5  98.44 ( 99.67)
Epoch: [29][290/391]	Time  0.023 ( 0.025)	Data  0.005 ( 0.002)	Loss 4.3433e-01 (2.9269e-01)	Acc@1  85.16 ( 90.10)	Acc@5  99.22 ( 99.67)
Epoch: [29][300/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.8921e-01 (2.9303e-01)	Acc@1  94.53 ( 90.11)	Acc@5 100.00 ( 99.66)
Epoch: [29][310/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.2837e-01 (2.9286e-01)	Acc@1  89.84 ( 90.13)	Acc@5 100.00 ( 99.66)
Epoch: [29][320/391]	Time  0.026 ( 0.025)	Data  0.002 ( 0.002)	Loss 3.0200e-01 (2.9386e-01)	Acc@1  89.84 ( 90.10)	Acc@5 100.00 ( 99.66)
Epoch: [29][330/391]	Time  0.031 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.9639e-01 (2.9327e-01)	Acc@1  87.50 ( 90.12)	Acc@5 100.00 ( 99.66)
Epoch: [29][340/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.002)	Loss 2.4646e-01 (2.9400e-01)	Acc@1  88.28 ( 90.11)	Acc@5 100.00 ( 99.66)
Epoch: [29][350/391]	Time  0.043 ( 0.025)	Data  0.004 ( 0.002)	Loss 3.6157e-01 (2.9441e-01)	Acc@1  85.16 ( 90.09)	Acc@5 100.00 ( 99.66)
Epoch: [29][360/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.2300e-01 (2.9412e-01)	Acc@1  85.94 ( 90.09)	Acc@5  99.22 ( 99.66)
Epoch: [29][370/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.7075e-01 (2.9450e-01)	Acc@1  89.84 ( 90.08)	Acc@5  99.22 ( 99.67)
Epoch: [29][380/391]	Time  0.038 ( 0.025)	Data  0.000 ( 0.002)	Loss 2.3145e-01 (2.9512e-01)	Acc@1  93.75 ( 90.06)	Acc@5 100.00 ( 99.67)
Epoch: [29][390/391]	Time  0.016 ( 0.024)	Data  0.000 ( 0.002)	Loss 1.8091e-01 (2.9514e-01)	Acc@1  93.75 ( 90.07)	Acc@5 100.00 ( 99.67)
## e[29] optimizer.zero_grad (sum) time: 0.1048440933227539
## e[29]       loss.backward (sum) time: 2.1616153717041016
## e[29]      optimizer.step (sum) time: 0.8929743766784668
## epoch[29] training(only) time: 9.672135591506958
# Switched to evaluate mode...
Test: [  0/100]	Time  0.159 ( 0.159)	Loss 3.3276e-01 (3.3276e-01)	Acc@1  90.00 ( 90.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.010 ( 0.026)	Loss 4.5605e-01 (4.1129e-01)	Acc@1  83.00 ( 87.18)	Acc@5 100.00 ( 99.36)
Test: [ 20/100]	Time  0.014 ( 0.020)	Loss 6.3525e-01 (4.5016e-01)	Acc@1  78.00 ( 85.81)	Acc@5 100.00 ( 99.10)
Test: [ 30/100]	Time  0.013 ( 0.018)	Loss 4.0820e-01 (4.6101e-01)	Acc@1  91.00 ( 85.94)	Acc@5  99.00 ( 98.97)
Test: [ 40/100]	Time  0.008 ( 0.016)	Loss 4.6021e-01 (4.6389e-01)	Acc@1  91.00 ( 86.12)	Acc@5  96.00 ( 98.93)
Test: [ 50/100]	Time  0.022 ( 0.016)	Loss 3.6768e-01 (4.5601e-01)	Acc@1  89.00 ( 86.29)	Acc@5  99.00 ( 98.98)
Test: [ 60/100]	Time  0.011 ( 0.015)	Loss 4.3750e-01 (4.5237e-01)	Acc@1  84.00 ( 86.25)	Acc@5 100.00 ( 99.03)
Test: [ 70/100]	Time  0.010 ( 0.015)	Loss 5.3027e-01 (4.5177e-01)	Acc@1  87.00 ( 86.30)	Acc@5  99.00 ( 99.08)
Test: [ 80/100]	Time  0.026 ( 0.015)	Loss 3.3228e-01 (4.5056e-01)	Acc@1  89.00 ( 86.33)	Acc@5  99.00 ( 99.14)
Test: [ 90/100]	Time  0.008 ( 0.014)	Loss 3.9038e-01 (4.4547e-01)	Acc@1  89.00 ( 86.40)	Acc@5 100.00 ( 99.18)
 * Acc@1 86.370 Acc@5 99.230
### epoch[29] execution time: 11.225694179534912
EPOCH 30
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [30][  0/391]	Time  0.189 ( 0.189)	Data  0.170 ( 0.170)	Loss 1.3696e-01 (1.3696e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [30][ 10/391]	Time  0.020 ( 0.039)	Data  0.001 ( 0.017)	Loss 1.9336e-01 (2.4437e-01)	Acc@1  92.97 ( 92.68)	Acc@5 100.00 ( 99.57)
Epoch: [30][ 20/391]	Time  0.018 ( 0.032)	Data  0.001 ( 0.010)	Loss 2.8809e-01 (2.3847e-01)	Acc@1  89.84 ( 92.22)	Acc@5 100.00 ( 99.70)
Epoch: [30][ 30/391]	Time  0.033 ( 0.029)	Data  0.001 ( 0.007)	Loss 2.4036e-01 (2.4591e-01)	Acc@1  92.97 ( 91.99)	Acc@5 100.00 ( 99.70)
Epoch: [30][ 40/391]	Time  0.018 ( 0.028)	Data  0.001 ( 0.006)	Loss 3.2129e-01 (2.3881e-01)	Acc@1  89.84 ( 92.32)	Acc@5 100.00 ( 99.70)
Epoch: [30][ 50/391]	Time  0.039 ( 0.027)	Data  0.001 ( 0.005)	Loss 1.5479e-01 (2.3228e-01)	Acc@1  94.53 ( 92.45)	Acc@5 100.00 ( 99.72)
Epoch: [30][ 60/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.005)	Loss 1.6663e-01 (2.2870e-01)	Acc@1  92.97 ( 92.57)	Acc@5 100.00 ( 99.72)
Epoch: [30][ 70/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.004)	Loss 2.8296e-01 (2.2327e-01)	Acc@1  94.53 ( 92.90)	Acc@5  99.22 ( 99.72)
Epoch: [30][ 80/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.6162e-01 (2.1825e-01)	Acc@1  94.53 ( 93.00)	Acc@5 100.00 ( 99.75)
Epoch: [30][ 90/391]	Time  0.025 ( 0.026)	Data  0.000 ( 0.004)	Loss 3.0420e-01 (2.1430e-01)	Acc@1  90.62 ( 93.16)	Acc@5  99.22 ( 99.76)
Epoch: [30][100/391]	Time  0.032 ( 0.026)	Data  0.000 ( 0.004)	Loss 1.6907e-01 (2.1299e-01)	Acc@1  93.75 ( 93.20)	Acc@5 100.00 ( 99.77)
Epoch: [30][110/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.004)	Loss 2.5220e-01 (2.1115e-01)	Acc@1  94.53 ( 93.24)	Acc@5 100.00 ( 99.77)
Epoch: [30][120/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.7173e-01 (2.0985e-01)	Acc@1  89.84 ( 93.25)	Acc@5 100.00 ( 99.78)
Epoch: [30][130/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.7932e-01 (2.0754e-01)	Acc@1  93.75 ( 93.31)	Acc@5 100.00 ( 99.79)
Epoch: [30][140/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.2433e-01 (2.0548e-01)	Acc@1  96.09 ( 93.33)	Acc@5 100.00 ( 99.79)
Epoch: [30][150/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.6050e-01 (2.0307e-01)	Acc@1  92.19 ( 93.43)	Acc@5 100.00 ( 99.80)
Epoch: [30][160/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.6260e-01 (2.0043e-01)	Acc@1  93.75 ( 93.42)	Acc@5 100.00 ( 99.81)
Epoch: [30][170/391]	Time  0.027 ( 0.025)	Data  0.002 ( 0.003)	Loss 1.3074e-01 (1.9863e-01)	Acc@1  96.09 ( 93.51)	Acc@5 100.00 ( 99.82)
Epoch: [30][180/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.3708e-01 (1.9701e-01)	Acc@1  96.09 ( 93.58)	Acc@5 100.00 ( 99.81)
Epoch: [30][190/391]	Time  0.036 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.3647e-01 (1.9644e-01)	Acc@1  94.53 ( 93.59)	Acc@5 100.00 ( 99.81)
Epoch: [30][200/391]	Time  0.040 ( 0.025)	Data  0.008 ( 0.003)	Loss 1.9067e-01 (1.9535e-01)	Acc@1  92.19 ( 93.63)	Acc@5 100.00 ( 99.82)
Epoch: [30][210/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.8896e-01 (1.9517e-01)	Acc@1  93.75 ( 93.61)	Acc@5 100.00 ( 99.82)
Epoch: [30][220/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.5857e-01 (1.9435e-01)	Acc@1  93.75 ( 93.64)	Acc@5  99.22 ( 99.82)
Epoch: [30][230/391]	Time  0.035 ( 0.025)	Data  0.000 ( 0.003)	Loss 8.8745e-02 (1.9247e-01)	Acc@1  96.88 ( 93.72)	Acc@5 100.00 ( 99.82)
Epoch: [30][240/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.6370e-01 (1.9204e-01)	Acc@1  93.75 ( 93.74)	Acc@5 100.00 ( 99.81)
Epoch: [30][250/391]	Time  0.026 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.2213e-01 (1.9114e-01)	Acc@1  95.31 ( 93.74)	Acc@5 100.00 ( 99.81)
Epoch: [30][260/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.5479e-01 (1.8935e-01)	Acc@1  94.53 ( 93.81)	Acc@5 100.00 ( 99.81)
Epoch: [30][270/391]	Time  0.032 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.0242e-01 (1.8826e-01)	Acc@1  97.66 ( 93.86)	Acc@5 100.00 ( 99.82)
Epoch: [30][280/391]	Time  0.026 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.3877e-01 (1.8734e-01)	Acc@1  94.53 ( 93.89)	Acc@5 100.00 ( 99.82)
Epoch: [30][290/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.5576e-01 (1.8715e-01)	Acc@1  95.31 ( 93.91)	Acc@5 100.00 ( 99.82)
Epoch: [30][300/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.6907e-01 (1.8681e-01)	Acc@1  94.53 ( 93.91)	Acc@5 100.00 ( 99.82)
Epoch: [30][310/391]	Time  0.030 ( 0.025)	Data  0.003 ( 0.002)	Loss 1.7102e-01 (1.8529e-01)	Acc@1  94.53 ( 93.96)	Acc@5 100.00 ( 99.82)
Epoch: [30][320/391]	Time  0.035 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.2854e-01 (1.8517e-01)	Acc@1  94.53 ( 93.97)	Acc@5 100.00 ( 99.82)
Epoch: [30][330/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.8445e-01 (1.8361e-01)	Acc@1  93.75 ( 94.02)	Acc@5 100.00 ( 99.83)
Epoch: [30][340/391]	Time  0.037 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.2207e-01 (1.8200e-01)	Acc@1  95.31 ( 94.07)	Acc@5 100.00 ( 99.83)
Epoch: [30][350/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.5356e-01 (1.8103e-01)	Acc@1  96.09 ( 94.10)	Acc@5 100.00 ( 99.83)
Epoch: [30][360/391]	Time  0.017 ( 0.024)	Data  0.000 ( 0.002)	Loss 1.5063e-01 (1.8005e-01)	Acc@1  94.53 ( 94.14)	Acc@5  99.22 ( 99.82)
Epoch: [30][370/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.7554e-01 (1.7890e-01)	Acc@1  93.75 ( 94.17)	Acc@5 100.00 ( 99.83)
Epoch: [30][380/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 7.5684e-02 (1.7790e-01)	Acc@1  97.66 ( 94.20)	Acc@5 100.00 ( 99.83)
Epoch: [30][390/391]	Time  0.014 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.8750e-01 (1.7724e-01)	Acc@1  92.50 ( 94.22)	Acc@5 100.00 ( 99.83)
## e[30] optimizer.zero_grad (sum) time: 0.10619068145751953
## e[30]       loss.backward (sum) time: 2.1308629512786865
## e[30]      optimizer.step (sum) time: 0.8936834335327148
## epoch[30] training(only) time: 9.704878091812134
# Switched to evaluate mode...
Test: [  0/100]	Time  0.218 ( 0.218)	Loss 2.4329e-01 (2.4329e-01)	Acc@1  93.00 ( 93.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.008 ( 0.031)	Loss 3.3813e-01 (3.0086e-01)	Acc@1  91.00 ( 90.82)	Acc@5  99.00 ( 99.45)
Test: [ 20/100]	Time  0.014 ( 0.023)	Loss 4.2236e-01 (3.4142e-01)	Acc@1  83.00 ( 89.43)	Acc@5 100.00 ( 99.48)
Test: [ 30/100]	Time  0.012 ( 0.020)	Loss 3.5010e-01 (3.4914e-01)	Acc@1  90.00 ( 89.68)	Acc@5  99.00 ( 99.45)
Test: [ 40/100]	Time  0.008 ( 0.018)	Loss 4.9658e-01 (3.6151e-01)	Acc@1  88.00 ( 89.59)	Acc@5  99.00 ( 99.39)
Test: [ 50/100]	Time  0.008 ( 0.017)	Loss 2.0972e-01 (3.4907e-01)	Acc@1  91.00 ( 89.80)	Acc@5  99.00 ( 99.39)
Test: [ 60/100]	Time  0.015 ( 0.017)	Loss 2.6562e-01 (3.4831e-01)	Acc@1  89.00 ( 89.79)	Acc@5 100.00 ( 99.41)
Test: [ 70/100]	Time  0.012 ( 0.016)	Loss 5.3857e-01 (3.4624e-01)	Acc@1  88.00 ( 89.77)	Acc@5  99.00 ( 99.42)
Test: [ 80/100]	Time  0.009 ( 0.016)	Loss 3.1470e-01 (3.4448e-01)	Acc@1  88.00 ( 89.65)	Acc@5 100.00 ( 99.48)
Test: [ 90/100]	Time  0.014 ( 0.015)	Loss 2.1118e-01 (3.4015e-01)	Acc@1  94.00 ( 89.67)	Acc@5 100.00 ( 99.53)
 * Acc@1 89.740 Acc@5 99.540
### epoch[30] execution time: 11.317960739135742
EPOCH 31
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [31][  0/391]	Time  0.194 ( 0.194)	Data  0.169 ( 0.169)	Loss 1.2433e-01 (1.2433e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [31][ 10/391]	Time  0.019 ( 0.039)	Data  0.001 ( 0.017)	Loss 1.3794e-01 (1.3397e-01)	Acc@1  96.09 ( 95.38)	Acc@5 100.00 ( 99.79)
Epoch: [31][ 20/391]	Time  0.029 ( 0.032)	Data  0.003 ( 0.010)	Loss 1.8457e-01 (1.3859e-01)	Acc@1  94.53 ( 95.46)	Acc@5  98.44 ( 99.81)
Epoch: [31][ 30/391]	Time  0.020 ( 0.030)	Data  0.001 ( 0.007)	Loss 1.6699e-01 (1.4699e-01)	Acc@1  93.75 ( 95.19)	Acc@5 100.00 ( 99.82)
Epoch: [31][ 40/391]	Time  0.030 ( 0.028)	Data  0.002 ( 0.006)	Loss 8.6121e-02 (1.4472e-01)	Acc@1  98.44 ( 95.39)	Acc@5 100.00 ( 99.83)
Epoch: [31][ 50/391]	Time  0.026 ( 0.027)	Data  0.002 ( 0.005)	Loss 1.2183e-01 (1.4854e-01)	Acc@1  95.31 ( 95.17)	Acc@5 100.00 ( 99.86)
Epoch: [31][ 60/391]	Time  0.028 ( 0.027)	Data  0.001 ( 0.005)	Loss 1.3428e-01 (1.4769e-01)	Acc@1  95.31 ( 95.20)	Acc@5 100.00 ( 99.88)
Epoch: [31][ 70/391]	Time  0.035 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.4417e-01 (1.4731e-01)	Acc@1  94.53 ( 95.16)	Acc@5 100.00 ( 99.88)
Epoch: [31][ 80/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.3770e-01 (1.4779e-01)	Acc@1  95.31 ( 95.19)	Acc@5 100.00 ( 99.87)
Epoch: [31][ 90/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.2384e-01 (1.4443e-01)	Acc@1  96.88 ( 95.33)	Acc@5 100.00 ( 99.88)
Epoch: [31][100/391]	Time  0.030 ( 0.026)	Data  0.000 ( 0.004)	Loss 1.2207e-01 (1.4176e-01)	Acc@1  94.53 ( 95.35)	Acc@5 100.00 ( 99.88)
Epoch: [31][110/391]	Time  0.020 ( 0.025)	Data  0.003 ( 0.003)	Loss 1.5112e-01 (1.4370e-01)	Acc@1  96.09 ( 95.31)	Acc@5 100.00 ( 99.87)
Epoch: [31][120/391]	Time  0.037 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.9774e-02 (1.4242e-01)	Acc@1  98.44 ( 95.33)	Acc@5 100.00 ( 99.88)
Epoch: [31][130/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.2744e-01 (1.4300e-01)	Acc@1  96.09 ( 95.28)	Acc@5 100.00 ( 99.87)
Epoch: [31][140/391]	Time  0.023 ( 0.025)	Data  0.004 ( 0.003)	Loss 1.7773e-01 (1.4252e-01)	Acc@1  92.97 ( 95.27)	Acc@5  99.22 ( 99.88)
Epoch: [31][150/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.2598e-01 (1.4158e-01)	Acc@1  96.09 ( 95.26)	Acc@5 100.00 ( 99.89)
Epoch: [31][160/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.1627e-01 (1.4120e-01)	Acc@1  96.09 ( 95.29)	Acc@5 100.00 ( 99.89)
Epoch: [31][170/391]	Time  0.027 ( 0.025)	Data  0.002 ( 0.003)	Loss 1.8530e-01 (1.4058e-01)	Acc@1  92.19 ( 95.32)	Acc@5 100.00 ( 99.89)
Epoch: [31][180/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.7395e-01 (1.4062e-01)	Acc@1  96.09 ( 95.30)	Acc@5 100.00 ( 99.89)
Epoch: [31][190/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.3940e-01 (1.4024e-01)	Acc@1  93.75 ( 95.31)	Acc@5 100.00 ( 99.89)
Epoch: [31][200/391]	Time  0.024 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.4473e-02 (1.4066e-01)	Acc@1  96.09 ( 95.30)	Acc@5 100.00 ( 99.89)
Epoch: [31][210/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.1035e-01 (1.3987e-01)	Acc@1  97.66 ( 95.33)	Acc@5  99.22 ( 99.89)
Epoch: [31][220/391]	Time  0.026 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.0669e-01 (1.4067e-01)	Acc@1  96.88 ( 95.31)	Acc@5 100.00 ( 99.89)
Epoch: [31][230/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.0077e-01 (1.4010e-01)	Acc@1  97.66 ( 95.33)	Acc@5 100.00 ( 99.89)
Epoch: [31][240/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.0089e-01 (1.3967e-01)	Acc@1  96.88 ( 95.35)	Acc@5 100.00 ( 99.89)
Epoch: [31][250/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.002)	Loss 6.8726e-02 (1.3898e-01)	Acc@1  96.09 ( 95.39)	Acc@5 100.00 ( 99.88)
Epoch: [31][260/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.3818e-01 (1.3856e-01)	Acc@1  95.31 ( 95.41)	Acc@5 100.00 ( 99.89)
Epoch: [31][270/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 9.7595e-02 (1.3900e-01)	Acc@1  97.66 ( 95.40)	Acc@5  99.22 ( 99.89)
Epoch: [31][280/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.5076e-01 (1.3877e-01)	Acc@1  95.31 ( 95.42)	Acc@5 100.00 ( 99.88)
Epoch: [31][290/391]	Time  0.020 ( 0.025)	Data  0.003 ( 0.002)	Loss 9.6741e-02 (1.3875e-01)	Acc@1  97.66 ( 95.43)	Acc@5 100.00 ( 99.89)
Epoch: [31][300/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.5747e-01 (1.3896e-01)	Acc@1  93.75 ( 95.43)	Acc@5  99.22 ( 99.88)
Epoch: [31][310/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.3657e-01 (1.3932e-01)	Acc@1  91.41 ( 95.42)	Acc@5 100.00 ( 99.88)
Epoch: [31][320/391]	Time  0.033 ( 0.024)	Data  0.000 ( 0.002)	Loss 1.3916e-01 (1.3977e-01)	Acc@1  96.09 ( 95.41)	Acc@5 100.00 ( 99.89)
Epoch: [31][330/391]	Time  0.032 ( 0.024)	Data  0.002 ( 0.002)	Loss 2.3193e-01 (1.4005e-01)	Acc@1  91.41 ( 95.39)	Acc@5 100.00 ( 99.89)
Epoch: [31][340/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 9.7046e-02 (1.3973e-01)	Acc@1  94.53 ( 95.38)	Acc@5 100.00 ( 99.89)
Epoch: [31][350/391]	Time  0.037 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.3062e-01 (1.3969e-01)	Acc@1  96.09 ( 95.38)	Acc@5  99.22 ( 99.88)
Epoch: [31][360/391]	Time  0.036 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.7078e-01 (1.4032e-01)	Acc@1  92.97 ( 95.34)	Acc@5 100.00 ( 99.88)
Epoch: [31][370/391]	Time  0.041 ( 0.024)	Data  0.005 ( 0.002)	Loss 1.3831e-01 (1.4035e-01)	Acc@1  95.31 ( 95.35)	Acc@5 100.00 ( 99.88)
Epoch: [31][380/391]	Time  0.040 ( 0.024)	Data  0.008 ( 0.002)	Loss 1.3599e-01 (1.3977e-01)	Acc@1  96.88 ( 95.37)	Acc@5 100.00 ( 99.89)
Epoch: [31][390/391]	Time  0.015 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.0520e-01 (1.3922e-01)	Acc@1  95.00 ( 95.38)	Acc@5  98.75 ( 99.89)
## e[31] optimizer.zero_grad (sum) time: 0.10616111755371094
## e[31]       loss.backward (sum) time: 2.15194034576416
## e[31]      optimizer.step (sum) time: 0.8926620483398438
## epoch[31] training(only) time: 9.643195629119873
# Switched to evaluate mode...
Test: [  0/100]	Time  0.164 ( 0.164)	Loss 2.5073e-01 (2.5073e-01)	Acc@1  93.00 ( 93.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.009 ( 0.027)	Loss 3.5767e-01 (2.9643e-01)	Acc@1  88.00 ( 90.27)	Acc@5  99.00 ( 99.45)
Test: [ 20/100]	Time  0.013 ( 0.020)	Loss 3.8379e-01 (3.3073e-01)	Acc@1  83.00 ( 89.19)	Acc@5 100.00 ( 99.52)
Test: [ 30/100]	Time  0.008 ( 0.017)	Loss 3.6401e-01 (3.4567e-01)	Acc@1  89.00 ( 89.55)	Acc@5  99.00 ( 99.42)
Test: [ 40/100]	Time  0.012 ( 0.017)	Loss 5.0879e-01 (3.6052e-01)	Acc@1  86.00 ( 89.51)	Acc@5  98.00 ( 99.37)
Test: [ 50/100]	Time  0.031 ( 0.016)	Loss 2.2888e-01 (3.4858e-01)	Acc@1  91.00 ( 89.76)	Acc@5  99.00 ( 99.39)
Test: [ 60/100]	Time  0.008 ( 0.015)	Loss 2.1008e-01 (3.4351e-01)	Acc@1  93.00 ( 89.90)	Acc@5 100.00 ( 99.44)
Test: [ 70/100]	Time  0.016 ( 0.015)	Loss 5.2930e-01 (3.4092e-01)	Acc@1  88.00 ( 90.00)	Acc@5  99.00 ( 99.46)
Test: [ 80/100]	Time  0.008 ( 0.015)	Loss 2.8076e-01 (3.3958e-01)	Acc@1  91.00 ( 89.95)	Acc@5 100.00 ( 99.51)
Test: [ 90/100]	Time  0.008 ( 0.015)	Loss 2.5928e-01 (3.3481e-01)	Acc@1  91.00 ( 89.87)	Acc@5 100.00 ( 99.54)
 * Acc@1 89.910 Acc@5 99.570
### epoch[31] execution time: 11.214889526367188
EPOCH 32
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [32][  0/391]	Time  0.192 ( 0.192)	Data  0.172 ( 0.172)	Loss 1.6370e-01 (1.6370e-01)	Acc@1  96.88 ( 96.88)	Acc@5  99.22 ( 99.22)
Epoch: [32][ 10/391]	Time  0.019 ( 0.037)	Data  0.002 ( 0.017)	Loss 8.6548e-02 (1.3137e-01)	Acc@1  96.88 ( 95.88)	Acc@5 100.00 ( 99.86)
Epoch: [32][ 20/391]	Time  0.035 ( 0.031)	Data  0.001 ( 0.010)	Loss 8.7585e-02 (1.2245e-01)	Acc@1  96.09 ( 95.94)	Acc@5 100.00 ( 99.89)
Epoch: [32][ 30/391]	Time  0.018 ( 0.028)	Data  0.001 ( 0.007)	Loss 1.8726e-01 (1.1533e-01)	Acc@1  95.31 ( 96.22)	Acc@5 100.00 ( 99.92)
Epoch: [32][ 40/391]	Time  0.018 ( 0.028)	Data  0.001 ( 0.006)	Loss 1.0608e-01 (1.1340e-01)	Acc@1  96.09 ( 96.15)	Acc@5 100.00 ( 99.94)
Epoch: [32][ 50/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.005)	Loss 8.1482e-02 (1.1425e-01)	Acc@1  97.66 ( 96.09)	Acc@5 100.00 ( 99.94)
Epoch: [32][ 60/391]	Time  0.031 ( 0.026)	Data  0.004 ( 0.005)	Loss 1.6089e-01 (1.1691e-01)	Acc@1  93.75 ( 95.99)	Acc@5 100.00 ( 99.90)
Epoch: [32][ 70/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.2231e-01 (1.2145e-01)	Acc@1  96.09 ( 95.94)	Acc@5 100.00 ( 99.90)
Epoch: [32][ 80/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.3538e-01 (1.2131e-01)	Acc@1  93.75 ( 95.88)	Acc@5 100.00 ( 99.90)
Epoch: [32][ 90/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.004)	Loss 6.5308e-02 (1.2087e-01)	Acc@1  96.88 ( 95.90)	Acc@5 100.00 ( 99.90)
Epoch: [32][100/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.004)	Loss 9.4299e-02 (1.2181e-01)	Acc@1  96.09 ( 95.82)	Acc@5 100.00 ( 99.90)
Epoch: [32][110/391]	Time  0.018 ( 0.025)	Data  0.000 ( 0.003)	Loss 3.2074e-02 (1.2475e-01)	Acc@1  99.22 ( 95.71)	Acc@5 100.00 ( 99.89)
Epoch: [32][120/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.5833e-01 (1.2423e-01)	Acc@1  96.09 ( 95.74)	Acc@5 100.00 ( 99.89)
Epoch: [32][130/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.6731e-02 (1.2249e-01)	Acc@1  98.44 ( 95.82)	Acc@5 100.00 ( 99.89)
Epoch: [32][140/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.4109e-01 (1.2420e-01)	Acc@1  90.62 ( 95.76)	Acc@5 100.00 ( 99.89)
Epoch: [32][150/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.1084e-01 (1.2482e-01)	Acc@1  96.09 ( 95.77)	Acc@5 100.00 ( 99.89)
Epoch: [32][160/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.3025e-01 (1.2652e-01)	Acc@1  95.31 ( 95.71)	Acc@5 100.00 ( 99.89)
Epoch: [32][170/391]	Time  0.041 ( 0.025)	Data  0.006 ( 0.003)	Loss 1.7639e-01 (1.2632e-01)	Acc@1  94.53 ( 95.71)	Acc@5  99.22 ( 99.89)
Epoch: [32][180/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 1.1481e-01 (1.2789e-01)	Acc@1  96.88 ( 95.67)	Acc@5 100.00 ( 99.88)
Epoch: [32][190/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 1.5393e-01 (1.2788e-01)	Acc@1  94.53 ( 95.65)	Acc@5 100.00 ( 99.88)
Epoch: [32][200/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.2146e-01 (1.2884e-01)	Acc@1  93.75 ( 95.64)	Acc@5 100.00 ( 99.88)
Epoch: [32][210/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.003)	Loss 7.5562e-02 (1.2932e-01)	Acc@1  96.88 ( 95.64)	Acc@5 100.00 ( 99.88)
Epoch: [32][220/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.3611e-01 (1.3029e-01)	Acc@1  92.97 ( 95.61)	Acc@5 100.00 ( 99.88)
Epoch: [32][230/391]	Time  0.027 ( 0.025)	Data  0.005 ( 0.003)	Loss 1.5906e-01 (1.3004e-01)	Acc@1  94.53 ( 95.62)	Acc@5 100.00 ( 99.89)
Epoch: [32][240/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.003)	Loss 1.2122e-01 (1.3027e-01)	Acc@1  96.09 ( 95.60)	Acc@5 100.00 ( 99.89)
Epoch: [32][250/391]	Time  0.045 ( 0.025)	Data  0.008 ( 0.003)	Loss 7.6233e-02 (1.2889e-01)	Acc@1  97.66 ( 95.64)	Acc@5 100.00 ( 99.89)
Epoch: [32][260/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.3538e-01 (1.2879e-01)	Acc@1  95.31 ( 95.65)	Acc@5  99.22 ( 99.89)
Epoch: [32][270/391]	Time  0.019 ( 0.024)	Data  0.002 ( 0.003)	Loss 5.8502e-02 (1.2865e-01)	Acc@1  97.66 ( 95.65)	Acc@5 100.00 ( 99.89)
Epoch: [32][280/391]	Time  0.027 ( 0.024)	Data  0.002 ( 0.003)	Loss 1.1383e-01 (1.2914e-01)	Acc@1  96.09 ( 95.63)	Acc@5  99.22 ( 99.88)
Epoch: [32][290/391]	Time  0.020 ( 0.024)	Data  0.001 ( 0.002)	Loss 8.5144e-02 (1.2948e-01)	Acc@1  96.88 ( 95.63)	Acc@5 100.00 ( 99.88)
Epoch: [32][300/391]	Time  0.020 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.4282e-01 (1.2919e-01)	Acc@1  95.31 ( 95.64)	Acc@5 100.00 ( 99.89)
Epoch: [32][310/391]	Time  0.024 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.2744e-01 (1.2956e-01)	Acc@1  95.31 ( 95.65)	Acc@5 100.00 ( 99.89)
Epoch: [32][320/391]	Time  0.030 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.0828e-01 (1.2964e-01)	Acc@1  98.44 ( 95.66)	Acc@5 100.00 ( 99.89)
Epoch: [32][330/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.3110e-01 (1.2989e-01)	Acc@1  95.31 ( 95.65)	Acc@5 100.00 ( 99.88)
Epoch: [32][340/391]	Time  0.028 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.2646e-01 (1.2964e-01)	Acc@1  95.31 ( 95.65)	Acc@5 100.00 ( 99.89)
Epoch: [32][350/391]	Time  0.020 ( 0.024)	Data  0.001 ( 0.002)	Loss 8.9050e-02 (1.2969e-01)	Acc@1  96.88 ( 95.65)	Acc@5 100.00 ( 99.89)
Epoch: [32][360/391]	Time  0.025 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.3757e-01 (1.2992e-01)	Acc@1  92.97 ( 95.63)	Acc@5 100.00 ( 99.89)
Epoch: [32][370/391]	Time  0.035 ( 0.024)	Data  0.000 ( 0.002)	Loss 2.8491e-01 (1.3006e-01)	Acc@1  93.75 ( 95.64)	Acc@5 100.00 ( 99.89)
Epoch: [32][380/391]	Time  0.035 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.5247e-01 (1.3013e-01)	Acc@1  97.66 ( 95.65)	Acc@5  99.22 ( 99.89)
Epoch: [32][390/391]	Time  0.016 ( 0.024)	Data  0.001 ( 0.002)	Loss 8.1116e-02 (1.2986e-01)	Acc@1  97.50 ( 95.66)	Acc@5 100.00 ( 99.89)
## e[32] optimizer.zero_grad (sum) time: 0.10556459426879883
## e[32]       loss.backward (sum) time: 2.1741721630096436
## e[32]      optimizer.step (sum) time: 0.8886208534240723
## epoch[32] training(only) time: 9.655814409255981
# Switched to evaluate mode...
Test: [  0/100]	Time  0.165 ( 0.165)	Loss 2.7661e-01 (2.7661e-01)	Acc@1  93.00 ( 93.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.019 ( 0.028)	Loss 3.8428e-01 (3.0798e-01)	Acc@1  87.00 ( 89.82)	Acc@5  99.00 ( 99.45)
Test: [ 20/100]	Time  0.020 ( 0.021)	Loss 3.8037e-01 (3.3411e-01)	Acc@1  86.00 ( 89.62)	Acc@5 100.00 ( 99.52)
Test: [ 30/100]	Time  0.024 ( 0.018)	Loss 3.5913e-01 (3.4947e-01)	Acc@1  89.00 ( 89.84)	Acc@5  98.00 ( 99.42)
Test: [ 40/100]	Time  0.014 ( 0.017)	Loss 4.7925e-01 (3.5909e-01)	Acc@1  88.00 ( 89.66)	Acc@5  99.00 ( 99.37)
Test: [ 50/100]	Time  0.008 ( 0.017)	Loss 1.3049e-01 (3.4401e-01)	Acc@1  94.00 ( 89.86)	Acc@5 100.00 ( 99.41)
Test: [ 60/100]	Time  0.013 ( 0.016)	Loss 2.3804e-01 (3.4095e-01)	Acc@1  92.00 ( 89.92)	Acc@5 100.00 ( 99.44)
Test: [ 70/100]	Time  0.021 ( 0.016)	Loss 5.4248e-01 (3.3948e-01)	Acc@1  85.00 ( 89.94)	Acc@5  99.00 ( 99.45)
Test: [ 80/100]	Time  0.013 ( 0.015)	Loss 2.8369e-01 (3.3895e-01)	Acc@1  89.00 ( 89.90)	Acc@5 100.00 ( 99.48)
Test: [ 90/100]	Time  0.013 ( 0.015)	Loss 2.4146e-01 (3.3440e-01)	Acc@1  91.00 ( 89.91)	Acc@5 100.00 ( 99.52)
 * Acc@1 89.910 Acc@5 99.540
### epoch[32] execution time: 11.255885124206543
EPOCH 33
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [33][  0/391]	Time  0.251 ( 0.251)	Data  0.231 ( 0.231)	Loss 1.0150e-01 (1.0150e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [33][ 10/391]	Time  0.019 ( 0.044)	Data  0.001 ( 0.023)	Loss 1.0492e-01 (1.1515e-01)	Acc@1  95.31 ( 96.09)	Acc@5 100.00 ( 99.86)
Epoch: [33][ 20/391]	Time  0.018 ( 0.034)	Data  0.001 ( 0.013)	Loss 1.9202e-01 (1.2659e-01)	Acc@1  93.75 ( 95.87)	Acc@5 100.00 ( 99.93)
Epoch: [33][ 30/391]	Time  0.033 ( 0.031)	Data  0.001 ( 0.009)	Loss 9.8328e-02 (1.2076e-01)	Acc@1  96.88 ( 96.04)	Acc@5 100.00 ( 99.92)
Epoch: [33][ 40/391]	Time  0.018 ( 0.029)	Data  0.001 ( 0.007)	Loss 1.5710e-01 (1.2494e-01)	Acc@1  96.09 ( 95.90)	Acc@5 100.00 ( 99.90)
Epoch: [33][ 50/391]	Time  0.018 ( 0.028)	Data  0.001 ( 0.006)	Loss 6.5002e-02 (1.1881e-01)	Acc@1  98.44 ( 96.06)	Acc@5 100.00 ( 99.92)
Epoch: [33][ 60/391]	Time  0.019 ( 0.027)	Data  0.002 ( 0.005)	Loss 1.5112e-01 (1.2052e-01)	Acc@1  96.09 ( 96.04)	Acc@5  99.22 ( 99.91)
Epoch: [33][ 70/391]	Time  0.026 ( 0.027)	Data  0.001 ( 0.005)	Loss 9.6558e-02 (1.1948e-01)	Acc@1  96.88 ( 96.01)	Acc@5 100.00 ( 99.92)
Epoch: [33][ 80/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 9.8022e-02 (1.1809e-01)	Acc@1  96.09 ( 96.01)	Acc@5 100.00 ( 99.92)
Epoch: [33][ 90/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.3745e-01 (1.2166e-01)	Acc@1  94.53 ( 95.87)	Acc@5 100.00 ( 99.93)
Epoch: [33][100/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.0620e-01 (1.2101e-01)	Acc@1  96.88 ( 95.95)	Acc@5 100.00 ( 99.93)
Epoch: [33][110/391]	Time  0.017 ( 0.026)	Data  0.000 ( 0.004)	Loss 2.4341e-01 (1.2241e-01)	Acc@1  94.53 ( 95.94)	Acc@5 100.00 ( 99.92)
Epoch: [33][120/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.004)	Loss 6.8726e-02 (1.1962e-01)	Acc@1  99.22 ( 96.07)	Acc@5 100.00 ( 99.92)
Epoch: [33][130/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.7515e-02 (1.1901e-01)	Acc@1  97.66 ( 96.08)	Acc@5 100.00 ( 99.92)
Epoch: [33][140/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 1.0077e-01 (1.1845e-01)	Acc@1  97.66 ( 96.11)	Acc@5 100.00 ( 99.91)
Epoch: [33][150/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 7.1167e-02 (1.1850e-01)	Acc@1  96.88 ( 96.09)	Acc@5 100.00 ( 99.91)
Epoch: [33][160/391]	Time  0.037 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.3232e-01 (1.1659e-01)	Acc@1  93.75 ( 96.16)	Acc@5 100.00 ( 99.92)
Epoch: [33][170/391]	Time  0.032 ( 0.025)	Data  0.002 ( 0.003)	Loss 1.3464e-01 (1.1666e-01)	Acc@1  96.88 ( 96.16)	Acc@5 100.00 ( 99.92)
Epoch: [33][180/391]	Time  0.035 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.4062e-01 (1.1681e-01)	Acc@1  96.09 ( 96.15)	Acc@5 100.00 ( 99.93)
Epoch: [33][190/391]	Time  0.026 ( 0.025)	Data  0.000 ( 0.003)	Loss 6.4758e-02 (1.1663e-01)	Acc@1  98.44 ( 96.16)	Acc@5 100.00 ( 99.93)
Epoch: [33][200/391]	Time  0.039 ( 0.025)	Data  0.005 ( 0.003)	Loss 1.0382e-01 (1.1719e-01)	Acc@1  96.88 ( 96.10)	Acc@5 100.00 ( 99.93)
Epoch: [33][210/391]	Time  0.045 ( 0.025)	Data  0.005 ( 0.003)	Loss 1.2817e-01 (1.1672e-01)	Acc@1  95.31 ( 96.11)	Acc@5 100.00 ( 99.93)
Epoch: [33][220/391]	Time  0.034 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.0105e-01 (1.1677e-01)	Acc@1  95.31 ( 96.12)	Acc@5 100.00 ( 99.93)
Epoch: [33][230/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 9.1675e-02 (1.1726e-01)	Acc@1  96.09 ( 96.11)	Acc@5 100.00 ( 99.92)
Epoch: [33][240/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.4790e-02 (1.1539e-01)	Acc@1  99.22 ( 96.17)	Acc@5 100.00 ( 99.92)
Epoch: [33][250/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 8.6487e-02 (1.1537e-01)	Acc@1  96.88 ( 96.16)	Acc@5 100.00 ( 99.92)
Epoch: [33][260/391]	Time  0.024 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.1603e-01 (1.1503e-01)	Acc@1  97.66 ( 96.17)	Acc@5  99.22 ( 99.92)
Epoch: [33][270/391]	Time  0.025 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.4839e-02 (1.1492e-01)	Acc@1  97.66 ( 96.18)	Acc@5 100.00 ( 99.92)
Epoch: [33][280/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.003)	Loss 9.0759e-02 (1.1490e-01)	Acc@1  96.88 ( 96.19)	Acc@5 100.00 ( 99.92)
Epoch: [33][290/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 7.5195e-02 (1.1470e-01)	Acc@1  98.44 ( 96.19)	Acc@5 100.00 ( 99.93)
Epoch: [33][300/391]	Time  0.025 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.3013e-01 (1.1502e-01)	Acc@1  95.31 ( 96.18)	Acc@5 100.00 ( 99.93)
Epoch: [33][310/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.3708e-01 (1.1435e-01)	Acc@1  96.09 ( 96.21)	Acc@5 100.00 ( 99.93)
Epoch: [33][320/391]	Time  0.038 ( 0.025)	Data  0.003 ( 0.003)	Loss 6.8298e-02 (1.1472e-01)	Acc@1  96.88 ( 96.20)	Acc@5 100.00 ( 99.93)
Epoch: [33][330/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 4.3610e-02 (1.1483e-01)	Acc@1  98.44 ( 96.19)	Acc@5 100.00 ( 99.93)
Epoch: [33][340/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.5898e-02 (1.1489e-01)	Acc@1  99.22 ( 96.18)	Acc@5 100.00 ( 99.93)
Epoch: [33][350/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.0498e-01 (1.1528e-01)	Acc@1  95.31 ( 96.16)	Acc@5 100.00 ( 99.93)
Epoch: [33][360/391]	Time  0.026 ( 0.025)	Data  0.000 ( 0.003)	Loss 2.0227e-01 (1.1549e-01)	Acc@1  92.97 ( 96.15)	Acc@5 100.00 ( 99.93)
Epoch: [33][370/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.3525e-01 (1.1550e-01)	Acc@1  95.31 ( 96.14)	Acc@5 100.00 ( 99.93)
Epoch: [33][380/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.3025e-01 (1.1555e-01)	Acc@1  95.31 ( 96.16)	Acc@5 100.00 ( 99.93)
Epoch: [33][390/391]	Time  0.013 ( 0.025)	Data  0.000 ( 0.002)	Loss 6.6711e-02 (1.1574e-01)	Acc@1  97.50 ( 96.15)	Acc@5 100.00 ( 99.93)
## e[33] optimizer.zero_grad (sum) time: 0.10483288764953613
## e[33]       loss.backward (sum) time: 2.1709654331207275
## e[33]      optimizer.step (sum) time: 0.8911468982696533
## epoch[33] training(only) time: 9.739264011383057
# Switched to evaluate mode...
Test: [  0/100]	Time  0.168 ( 0.168)	Loss 2.4060e-01 (2.4060e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.010 ( 0.026)	Loss 3.8232e-01 (3.0204e-01)	Acc@1  88.00 ( 90.18)	Acc@5  99.00 ( 99.55)
Test: [ 20/100]	Time  0.014 ( 0.021)	Loss 4.4702e-01 (3.2868e-01)	Acc@1  82.00 ( 89.71)	Acc@5 100.00 ( 99.52)
Test: [ 30/100]	Time  0.017 ( 0.018)	Loss 3.3862e-01 (3.4509e-01)	Acc@1  90.00 ( 89.74)	Acc@5 100.00 ( 99.45)
Test: [ 40/100]	Time  0.008 ( 0.017)	Loss 4.8340e-01 (3.5652e-01)	Acc@1  88.00 ( 89.71)	Acc@5  98.00 ( 99.37)
Test: [ 50/100]	Time  0.007 ( 0.016)	Loss 1.9421e-01 (3.4460e-01)	Acc@1  91.00 ( 89.76)	Acc@5 100.00 ( 99.41)
Test: [ 60/100]	Time  0.013 ( 0.016)	Loss 2.6489e-01 (3.4417e-01)	Acc@1  93.00 ( 89.80)	Acc@5 100.00 ( 99.44)
Test: [ 70/100]	Time  0.012 ( 0.015)	Loss 5.2783e-01 (3.4347e-01)	Acc@1  87.00 ( 89.89)	Acc@5  99.00 ( 99.46)
Test: [ 80/100]	Time  0.013 ( 0.015)	Loss 2.6025e-01 (3.3982e-01)	Acc@1  89.00 ( 89.85)	Acc@5 100.00 ( 99.53)
Test: [ 90/100]	Time  0.013 ( 0.015)	Loss 2.0496e-01 (3.3530e-01)	Acc@1  93.00 ( 89.91)	Acc@5 100.00 ( 99.57)
 * Acc@1 89.960 Acc@5 99.600
### epoch[33] execution time: 11.310778617858887
EPOCH 34
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [34][  0/391]	Time  0.194 ( 0.194)	Data  0.173 ( 0.173)	Loss 7.1411e-02 (7.1411e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [34][ 10/391]	Time  0.022 ( 0.040)	Data  0.001 ( 0.017)	Loss 3.5431e-02 (8.3973e-02)	Acc@1  99.22 ( 97.09)	Acc@5 100.00 ( 99.93)
Epoch: [34][ 20/391]	Time  0.038 ( 0.032)	Data  0.000 ( 0.010)	Loss 5.3589e-02 (9.2986e-02)	Acc@1  98.44 ( 96.54)	Acc@5 100.00 ( 99.96)
Epoch: [34][ 30/391]	Time  0.019 ( 0.029)	Data  0.002 ( 0.007)	Loss 1.1823e-01 (9.8258e-02)	Acc@1  94.53 ( 96.40)	Acc@5 100.00 ( 99.95)
Epoch: [34][ 40/391]	Time  0.038 ( 0.028)	Data  0.005 ( 0.006)	Loss 1.5869e-01 (9.7385e-02)	Acc@1  94.53 ( 96.49)	Acc@5 100.00 ( 99.96)
Epoch: [34][ 50/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.005)	Loss 2.1008e-01 (1.0032e-01)	Acc@1  95.31 ( 96.54)	Acc@5 100.00 ( 99.95)
Epoch: [34][ 60/391]	Time  0.038 ( 0.027)	Data  0.001 ( 0.004)	Loss 7.1289e-02 (1.0303e-01)	Acc@1  98.44 ( 96.41)	Acc@5 100.00 ( 99.95)
Epoch: [34][ 70/391]	Time  0.035 ( 0.026)	Data  0.002 ( 0.004)	Loss 1.0553e-01 (1.0274e-01)	Acc@1  96.88 ( 96.49)	Acc@5  99.22 ( 99.94)
Epoch: [34][ 80/391]	Time  0.031 ( 0.026)	Data  0.002 ( 0.004)	Loss 4.3579e-02 (1.0509e-01)	Acc@1  98.44 ( 96.43)	Acc@5 100.00 ( 99.94)
Epoch: [34][ 90/391]	Time  0.043 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.4810e-02 (1.0512e-01)	Acc@1  98.44 ( 96.50)	Acc@5 100.00 ( 99.94)
Epoch: [34][100/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2732e-01 (1.0676e-01)	Acc@1  95.31 ( 96.43)	Acc@5 100.00 ( 99.95)
Epoch: [34][110/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.8579e-01 (1.0757e-01)	Acc@1  95.31 ( 96.41)	Acc@5 100.00 ( 99.94)
Epoch: [34][120/391]	Time  0.028 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.9958e-01 (1.0764e-01)	Acc@1  94.53 ( 96.42)	Acc@5 100.00 ( 99.94)
Epoch: [34][130/391]	Time  0.028 ( 0.025)	Data  0.000 ( 0.003)	Loss 1.1499e-01 (1.0707e-01)	Acc@1  96.88 ( 96.45)	Acc@5  99.22 ( 99.93)
Epoch: [34][140/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 9.3445e-02 (1.0827e-01)	Acc@1  96.09 ( 96.40)	Acc@5 100.00 ( 99.93)
Epoch: [34][150/391]	Time  0.025 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.0339e-01 (1.0893e-01)	Acc@1  96.09 ( 96.37)	Acc@5 100.00 ( 99.93)
Epoch: [34][160/391]	Time  0.039 ( 0.025)	Data  0.003 ( 0.003)	Loss 7.1655e-02 (1.1018e-01)	Acc@1  98.44 ( 96.28)	Acc@5 100.00 ( 99.92)
Epoch: [34][170/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.2275e-02 (1.0957e-01)	Acc@1  96.88 ( 96.31)	Acc@5 100.00 ( 99.92)
Epoch: [34][180/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.4868e-01 (1.1008e-01)	Acc@1  96.09 ( 96.32)	Acc@5 100.00 ( 99.92)
Epoch: [34][190/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.0079e-02 (1.0984e-01)	Acc@1  99.22 ( 96.36)	Acc@5 100.00 ( 99.92)
Epoch: [34][200/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.4929e-01 (1.0899e-01)	Acc@1  96.09 ( 96.38)	Acc@5 100.00 ( 99.93)
Epoch: [34][210/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.3904e-01 (1.0963e-01)	Acc@1  94.53 ( 96.38)	Acc@5 100.00 ( 99.93)
Epoch: [34][220/391]	Time  0.039 ( 0.025)	Data  0.000 ( 0.002)	Loss 1.0272e-01 (1.0921e-01)	Acc@1  96.88 ( 96.41)	Acc@5 100.00 ( 99.93)
Epoch: [34][230/391]	Time  0.018 ( 0.025)	Data  0.002 ( 0.002)	Loss 1.3403e-01 (1.0950e-01)	Acc@1  94.53 ( 96.39)	Acc@5 100.00 ( 99.93)
Epoch: [34][240/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 5.6458e-02 (1.0850e-01)	Acc@1  97.66 ( 96.41)	Acc@5 100.00 ( 99.93)
Epoch: [34][250/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.3538e-01 (1.0810e-01)	Acc@1  96.88 ( 96.45)	Acc@5 100.00 ( 99.93)
Epoch: [34][260/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.0300e-01 (1.0827e-01)	Acc@1  95.31 ( 96.44)	Acc@5  99.22 ( 99.93)
Epoch: [34][270/391]	Time  0.036 ( 0.025)	Data  0.002 ( 0.002)	Loss 1.5210e-01 (1.0783e-01)	Acc@1  94.53 ( 96.45)	Acc@5 100.00 ( 99.93)
Epoch: [34][280/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 5.5573e-02 (1.0706e-01)	Acc@1  97.66 ( 96.47)	Acc@5 100.00 ( 99.93)
Epoch: [34][290/391]	Time  0.035 ( 0.025)	Data  0.004 ( 0.002)	Loss 7.8491e-02 (1.0695e-01)	Acc@1  96.88 ( 96.47)	Acc@5 100.00 ( 99.94)
Epoch: [34][300/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 7.8674e-02 (1.0828e-01)	Acc@1  97.66 ( 96.43)	Acc@5  99.22 ( 99.93)
Epoch: [34][310/391]	Time  0.024 ( 0.025)	Data  0.001 ( 0.002)	Loss 6.8237e-02 (1.0755e-01)	Acc@1  96.88 ( 96.45)	Acc@5 100.00 ( 99.93)
Epoch: [34][320/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.0083e-01 (1.0739e-01)	Acc@1  94.53 ( 96.45)	Acc@5 100.00 ( 99.93)
Epoch: [34][330/391]	Time  0.022 ( 0.025)	Data  0.000 ( 0.002)	Loss 1.3806e-01 (1.0759e-01)	Acc@1  95.31 ( 96.45)	Acc@5 100.00 ( 99.93)
Epoch: [34][340/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.4648e-01 (1.0717e-01)	Acc@1  93.75 ( 96.47)	Acc@5 100.00 ( 99.93)
Epoch: [34][350/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.0065e-01 (1.0745e-01)	Acc@1  96.09 ( 96.45)	Acc@5 100.00 ( 99.93)
Epoch: [34][360/391]	Time  0.019 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.1017e-01 (1.0725e-01)	Acc@1  96.09 ( 96.45)	Acc@5 100.00 ( 99.93)
Epoch: [34][370/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 5.3314e-02 (1.0718e-01)	Acc@1  97.66 ( 96.46)	Acc@5 100.00 ( 99.93)
Epoch: [34][380/391]	Time  0.022 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.1823e-01 (1.0719e-01)	Acc@1  96.88 ( 96.47)	Acc@5 100.00 ( 99.93)
Epoch: [34][390/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.8748e-02 (1.0723e-01)	Acc@1 100.00 ( 96.47)	Acc@5 100.00 ( 99.93)
## e[34] optimizer.zero_grad (sum) time: 0.10482478141784668
## e[34]       loss.backward (sum) time: 2.141066074371338
## e[34]      optimizer.step (sum) time: 0.8753230571746826
## epoch[34] training(only) time: 9.665565490722656
# Switched to evaluate mode...
Test: [  0/100]	Time  0.170 ( 0.170)	Loss 2.2351e-01 (2.2351e-01)	Acc@1  93.00 ( 93.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.011 ( 0.028)	Loss 3.4937e-01 (2.9870e-01)	Acc@1  91.00 ( 91.36)	Acc@5  99.00 ( 99.55)
Test: [ 20/100]	Time  0.019 ( 0.022)	Loss 3.9600e-01 (3.2530e-01)	Acc@1  86.00 ( 90.52)	Acc@5 100.00 ( 99.52)
Test: [ 30/100]	Time  0.008 ( 0.018)	Loss 3.4668e-01 (3.4442e-01)	Acc@1  90.00 ( 90.26)	Acc@5 100.00 ( 99.45)
Test: [ 40/100]	Time  0.008 ( 0.017)	Loss 4.7510e-01 (3.5707e-01)	Acc@1  89.00 ( 90.24)	Acc@5  98.00 ( 99.34)
Test: [ 50/100]	Time  0.017 ( 0.017)	Loss 1.8860e-01 (3.4317e-01)	Acc@1  94.00 ( 90.47)	Acc@5 100.00 ( 99.37)
Test: [ 60/100]	Time  0.014 ( 0.016)	Loss 2.6538e-01 (3.4363e-01)	Acc@1  93.00 ( 90.46)	Acc@5 100.00 ( 99.43)
Test: [ 70/100]	Time  0.009 ( 0.015)	Loss 5.3516e-01 (3.4168e-01)	Acc@1  88.00 ( 90.41)	Acc@5  99.00 ( 99.46)
Test: [ 80/100]	Time  0.009 ( 0.015)	Loss 2.7246e-01 (3.3957e-01)	Acc@1  90.00 ( 90.32)	Acc@5 100.00 ( 99.49)
Test: [ 90/100]	Time  0.007 ( 0.015)	Loss 2.0349e-01 (3.3543e-01)	Acc@1  93.00 ( 90.33)	Acc@5 100.00 ( 99.53)
 * Acc@1 90.360 Acc@5 99.560
### epoch[34] execution time: 11.274130821228027
EPOCH 35
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [35][  0/391]	Time  0.198 ( 0.198)	Data  0.175 ( 0.175)	Loss 3.8971e-02 (3.8971e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [35][ 10/391]	Time  0.016 ( 0.040)	Data  0.001 ( 0.017)	Loss 1.3928e-01 (1.1027e-01)	Acc@1  94.53 ( 95.88)	Acc@5 100.00 ( 99.86)
Epoch: [35][ 20/391]	Time  0.019 ( 0.032)	Data  0.001 ( 0.010)	Loss 1.3660e-01 (1.0632e-01)	Acc@1  95.31 ( 96.24)	Acc@5  99.22 ( 99.85)
Epoch: [35][ 30/391]	Time  0.018 ( 0.030)	Data  0.001 ( 0.007)	Loss 8.5022e-02 (1.1146e-01)	Acc@1  95.31 ( 95.99)	Acc@5 100.00 ( 99.87)
Epoch: [35][ 40/391]	Time  0.029 ( 0.028)	Data  0.001 ( 0.006)	Loss 5.4871e-02 (1.0326e-01)	Acc@1  98.44 ( 96.34)	Acc@5 100.00 ( 99.89)
Epoch: [35][ 50/391]	Time  0.017 ( 0.027)	Data  0.000 ( 0.005)	Loss 6.1920e-02 (1.0182e-01)	Acc@1  97.66 ( 96.35)	Acc@5 100.00 ( 99.88)
Epoch: [35][ 60/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.004)	Loss 7.4036e-02 (9.9559e-02)	Acc@1  96.09 ( 96.47)	Acc@5 100.00 ( 99.90)
Epoch: [35][ 70/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 8.1482e-02 (1.0201e-01)	Acc@1  96.88 ( 96.42)	Acc@5 100.00 ( 99.91)
Epoch: [35][ 80/391]	Time  0.029 ( 0.026)	Data  0.005 ( 0.004)	Loss 9.7351e-02 (1.0299e-01)	Acc@1  96.88 ( 96.41)	Acc@5 100.00 ( 99.90)
Epoch: [35][ 90/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.1310e-01 (1.0246e-01)	Acc@1  97.66 ( 96.51)	Acc@5 100.00 ( 99.91)
Epoch: [35][100/391]	Time  0.027 ( 0.026)	Data  0.000 ( 0.003)	Loss 6.1676e-02 (1.0380e-01)	Acc@1  97.66 ( 96.45)	Acc@5 100.00 ( 99.92)
Epoch: [35][110/391]	Time  0.037 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5881e-01 (1.0425e-01)	Acc@1  93.75 ( 96.45)	Acc@5 100.00 ( 99.92)
Epoch: [35][120/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 7.9285e-02 (1.0369e-01)	Acc@1  97.66 ( 96.47)	Acc@5 100.00 ( 99.92)
Epoch: [35][130/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.3672e-01 (1.0441e-01)	Acc@1  94.53 ( 96.47)	Acc@5 100.00 ( 99.93)
Epoch: [35][140/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.7908e-01 (1.0603e-01)	Acc@1  92.97 ( 96.43)	Acc@5 100.00 ( 99.93)
Epoch: [35][150/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.3049e-01 (1.0608e-01)	Acc@1  94.53 ( 96.43)	Acc@5 100.00 ( 99.93)
Epoch: [35][160/391]	Time  0.017 ( 0.025)	Data  0.001 ( 0.003)	Loss 9.7107e-02 (1.0572e-01)	Acc@1  96.09 ( 96.46)	Acc@5 100.00 ( 99.93)
Epoch: [35][170/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.2114e-02 (1.0512e-01)	Acc@1  98.44 ( 96.50)	Acc@5 100.00 ( 99.93)
Epoch: [35][180/391]	Time  0.016 ( 0.025)	Data  0.001 ( 0.003)	Loss 9.5398e-02 (1.0446e-01)	Acc@1  95.31 ( 96.52)	Acc@5 100.00 ( 99.94)
Epoch: [35][190/391]	Time  0.030 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.4412e-02 (1.0510e-01)	Acc@1  96.88 ( 96.50)	Acc@5 100.00 ( 99.93)
Epoch: [35][200/391]	Time  0.027 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.9906e-02 (1.0470e-01)	Acc@1  97.66 ( 96.52)	Acc@5 100.00 ( 99.93)
Epoch: [35][210/391]	Time  0.047 ( 0.025)	Data  0.015 ( 0.003)	Loss 3.9703e-02 (1.0541e-01)	Acc@1  99.22 ( 96.51)	Acc@5 100.00 ( 99.93)
Epoch: [35][220/391]	Time  0.025 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.0913e-01 (1.0540e-01)	Acc@1  95.31 ( 96.52)	Acc@5 100.00 ( 99.93)
Epoch: [35][230/391]	Time  0.035 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.5532e-02 (1.0459e-01)	Acc@1  98.44 ( 96.55)	Acc@5 100.00 ( 99.93)
Epoch: [35][240/391]	Time  0.037 ( 0.025)	Data  0.004 ( 0.002)	Loss 5.5725e-02 (1.0437e-01)	Acc@1  97.66 ( 96.57)	Acc@5 100.00 ( 99.93)
Epoch: [35][250/391]	Time  0.035 ( 0.025)	Data  0.003 ( 0.002)	Loss 9.1248e-02 (1.0456e-01)	Acc@1  97.66 ( 96.57)	Acc@5 100.00 ( 99.92)
Epoch: [35][260/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 8.3069e-02 (1.0508e-01)	Acc@1  96.88 ( 96.56)	Acc@5 100.00 ( 99.92)
Epoch: [35][270/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.3416e-01 (1.0561e-01)	Acc@1  95.31 ( 96.52)	Acc@5 100.00 ( 99.92)
Epoch: [35][280/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.8359e-01 (1.0595e-01)	Acc@1  93.75 ( 96.52)	Acc@5 100.00 ( 99.92)
Epoch: [35][290/391]	Time  0.025 ( 0.025)	Data  0.001 ( 0.002)	Loss 6.0852e-02 (1.0508e-01)	Acc@1  99.22 ( 96.53)	Acc@5 100.00 ( 99.92)
Epoch: [35][300/391]	Time  0.027 ( 0.025)	Data  0.001 ( 0.002)	Loss 6.2561e-02 (1.0561e-01)	Acc@1  96.88 ( 96.52)	Acc@5 100.00 ( 99.92)
Epoch: [35][310/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 8.3069e-02 (1.0563e-01)	Acc@1  96.88 ( 96.51)	Acc@5 100.00 ( 99.92)
Epoch: [35][320/391]	Time  0.037 ( 0.025)	Data  0.000 ( 0.002)	Loss 1.2988e-01 (1.0534e-01)	Acc@1  96.09 ( 96.50)	Acc@5  99.22 ( 99.92)
Epoch: [35][330/391]	Time  0.030 ( 0.025)	Data  0.001 ( 0.002)	Loss 9.7046e-02 (1.0564e-01)	Acc@1  97.66 ( 96.50)	Acc@5 100.00 ( 99.92)
Epoch: [35][340/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.4673e-01 (1.0590e-01)	Acc@1  97.66 ( 96.50)	Acc@5  99.22 ( 99.92)
Epoch: [35][350/391]	Time  0.021 ( 0.025)	Data  0.004 ( 0.002)	Loss 1.7737e-01 (1.0596e-01)	Acc@1  93.75 ( 96.49)	Acc@5 100.00 ( 99.92)
Epoch: [35][360/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.002)	Loss 9.5520e-02 (1.0556e-01)	Acc@1  96.88 ( 96.50)	Acc@5 100.00 ( 99.92)
Epoch: [35][370/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.002)	Loss 1.2866e-01 (1.0551e-01)	Acc@1  96.09 ( 96.51)	Acc@5 100.00 ( 99.92)
Epoch: [35][380/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.0436e-02 (1.0521e-01)	Acc@1  98.44 ( 96.52)	Acc@5 100.00 ( 99.92)
Epoch: [35][390/391]	Time  0.015 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.1749e-01 (1.0518e-01)	Acc@1  98.75 ( 96.53)	Acc@5 100.00 ( 99.93)
## e[35] optimizer.zero_grad (sum) time: 0.1030128002166748
## e[35]       loss.backward (sum) time: 2.1226859092712402
## e[35]      optimizer.step (sum) time: 0.8718667030334473
## epoch[35] training(only) time: 9.704609394073486
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 2.3767e-01 (2.3767e-01)	Acc@1  93.00 ( 93.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.009 ( 0.027)	Loss 4.3262e-01 (3.0861e-01)	Acc@1  87.00 ( 90.64)	Acc@5  99.00 ( 99.55)
Test: [ 20/100]	Time  0.008 ( 0.019)	Loss 3.8843e-01 (3.2765e-01)	Acc@1  86.00 ( 90.33)	Acc@5 100.00 ( 99.43)
Test: [ 30/100]	Time  0.016 ( 0.018)	Loss 3.7915e-01 (3.4598e-01)	Acc@1  88.00 ( 90.03)	Acc@5  99.00 ( 99.35)
Test: [ 40/100]	Time  0.010 ( 0.017)	Loss 5.0586e-01 (3.5809e-01)	Acc@1  89.00 ( 89.95)	Acc@5  97.00 ( 99.29)
Test: [ 50/100]	Time  0.015 ( 0.016)	Loss 1.5259e-01 (3.4435e-01)	Acc@1  94.00 ( 90.27)	Acc@5 100.00 ( 99.35)
Test: [ 60/100]	Time  0.010 ( 0.016)	Loss 2.6392e-01 (3.4725e-01)	Acc@1  94.00 ( 90.23)	Acc@5 100.00 ( 99.41)
Test: [ 70/100]	Time  0.008 ( 0.015)	Loss 5.2490e-01 (3.4548e-01)	Acc@1  87.00 ( 90.31)	Acc@5  99.00 ( 99.39)
Test: [ 80/100]	Time  0.017 ( 0.015)	Loss 2.7295e-01 (3.4419e-01)	Acc@1  89.00 ( 90.22)	Acc@5 100.00 ( 99.44)
Test: [ 90/100]	Time  0.008 ( 0.015)	Loss 2.4585e-01 (3.3986e-01)	Acc@1  91.00 ( 90.25)	Acc@5 100.00 ( 99.49)
 * Acc@1 90.210 Acc@5 99.520
### epoch[35] execution time: 11.296057224273682
EPOCH 36
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [36][  0/391]	Time  0.206 ( 0.206)	Data  0.185 ( 0.185)	Loss 1.9360e-01 (1.9360e-01)	Acc@1  92.97 ( 92.97)	Acc@5 100.00 (100.00)
Epoch: [36][ 10/391]	Time  0.018 ( 0.040)	Data  0.001 ( 0.018)	Loss 4.8798e-02 (7.7234e-02)	Acc@1  99.22 ( 97.23)	Acc@5 100.00 ( 99.93)
Epoch: [36][ 20/391]	Time  0.019 ( 0.032)	Data  0.002 ( 0.011)	Loss 6.4209e-02 (8.2005e-02)	Acc@1  98.44 ( 97.32)	Acc@5 100.00 ( 99.93)
Epoch: [36][ 30/391]	Time  0.023 ( 0.029)	Data  0.005 ( 0.008)	Loss 1.1047e-01 (8.6309e-02)	Acc@1  95.31 ( 97.18)	Acc@5 100.00 ( 99.92)
Epoch: [36][ 40/391]	Time  0.022 ( 0.028)	Data  0.003 ( 0.006)	Loss 4.3365e-02 (8.3903e-02)	Acc@1 100.00 ( 97.18)	Acc@5 100.00 ( 99.94)
Epoch: [36][ 50/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.005)	Loss 6.8665e-02 (8.1003e-02)	Acc@1  96.88 ( 97.21)	Acc@5 100.00 ( 99.95)
Epoch: [36][ 60/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.005)	Loss 6.2439e-02 (8.4026e-02)	Acc@1  97.66 ( 97.11)	Acc@5 100.00 ( 99.92)
Epoch: [36][ 70/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.004)	Loss 6.2805e-02 (8.6604e-02)	Acc@1  99.22 ( 97.06)	Acc@5 100.00 ( 99.92)
Epoch: [36][ 80/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.004)	Loss 6.6772e-02 (8.6683e-02)	Acc@1  96.88 ( 97.04)	Acc@5 100.00 ( 99.92)
Epoch: [36][ 90/391]	Time  0.019 ( 0.026)	Data  0.002 ( 0.004)	Loss 7.3242e-02 (8.8332e-02)	Acc@1  97.66 ( 97.00)	Acc@5 100.00 ( 99.92)
Epoch: [36][100/391]	Time  0.040 ( 0.025)	Data  0.002 ( 0.004)	Loss 1.0333e-01 (8.8941e-02)	Acc@1  96.09 ( 96.95)	Acc@5 100.00 ( 99.91)
Epoch: [36][110/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.3313e-02 (9.0015e-02)	Acc@1  97.66 ( 96.95)	Acc@5 100.00 ( 99.92)
Epoch: [36][120/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.0541e-01 (9.0587e-02)	Acc@1  96.88 ( 96.93)	Acc@5 100.00 ( 99.93)
Epoch: [36][130/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.3772e-02 (9.0591e-02)	Acc@1  99.22 ( 96.91)	Acc@5 100.00 ( 99.93)
Epoch: [36][140/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 7.4402e-02 (9.1011e-02)	Acc@1  97.66 ( 96.90)	Acc@5 100.00 ( 99.92)
Epoch: [36][150/391]	Time  0.028 ( 0.025)	Data  0.002 ( 0.003)	Loss 1.1975e-01 (9.1886e-02)	Acc@1  96.88 ( 96.89)	Acc@5 100.00 ( 99.92)
Epoch: [36][160/391]	Time  0.026 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.8655e-02 (9.2477e-02)	Acc@1  98.44 ( 96.85)	Acc@5 100.00 ( 99.93)
Epoch: [36][170/391]	Time  0.035 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.8330e-02 (9.1890e-02)	Acc@1  99.22 ( 96.90)	Acc@5 100.00 ( 99.92)
Epoch: [36][180/391]	Time  0.033 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.1420e-01 (9.2103e-02)	Acc@1  96.88 ( 96.89)	Acc@5 100.00 ( 99.93)
Epoch: [36][190/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.7708e-02 (9.1745e-02)	Acc@1  96.09 ( 96.90)	Acc@5 100.00 ( 99.93)
Epoch: [36][200/391]	Time  0.018 ( 0.025)	Data  0.002 ( 0.003)	Loss 1.0333e-01 (9.0579e-02)	Acc@1  96.88 ( 96.93)	Acc@5 100.00 ( 99.93)
Epoch: [36][210/391]	Time  0.029 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.9984e-02 (9.0741e-02)	Acc@1  99.22 ( 96.93)	Acc@5 100.00 ( 99.93)
Epoch: [36][220/391]	Time  0.033 ( 0.025)	Data  0.001 ( 0.003)	Loss 9.1675e-02 (9.1782e-02)	Acc@1  98.44 ( 96.90)	Acc@5 100.00 ( 99.93)
Epoch: [36][230/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.4172e-01 (9.2405e-02)	Acc@1  95.31 ( 96.88)	Acc@5 100.00 ( 99.94)
Epoch: [36][240/391]	Time  0.030 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.3965e-01 (9.2315e-02)	Acc@1  94.53 ( 96.88)	Acc@5 100.00 ( 99.94)
Epoch: [36][250/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 4.6539e-02 (9.2431e-02)	Acc@1  98.44 ( 96.88)	Acc@5 100.00 ( 99.94)
Epoch: [36][260/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 9.5581e-02 (9.3234e-02)	Acc@1  96.09 ( 96.86)	Acc@5 100.00 ( 99.94)
Epoch: [36][270/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.002)	Loss 8.9783e-02 (9.3937e-02)	Acc@1  96.88 ( 96.85)	Acc@5 100.00 ( 99.94)
Epoch: [36][280/391]	Time  0.018 ( 0.025)	Data  0.000 ( 0.003)	Loss 9.1980e-02 (9.4686e-02)	Acc@1  96.88 ( 96.83)	Acc@5 100.00 ( 99.94)
Epoch: [36][290/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 1.1743e-01 (9.4939e-02)	Acc@1  96.88 ( 96.82)	Acc@5 100.00 ( 99.94)
Epoch: [36][300/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.3928e-01 (9.5225e-02)	Acc@1  96.88 ( 96.83)	Acc@5  99.22 ( 99.94)
Epoch: [36][310/391]	Time  0.037 ( 0.025)	Data  0.001 ( 0.002)	Loss 9.7595e-02 (9.5759e-02)	Acc@1  96.09 ( 96.80)	Acc@5 100.00 ( 99.94)
Epoch: [36][320/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.0278e-01 (9.6086e-02)	Acc@1  96.88 ( 96.80)	Acc@5 100.00 ( 99.94)
Epoch: [36][330/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.0516e-01 (9.6219e-02)	Acc@1  96.09 ( 96.80)	Acc@5 100.00 ( 99.94)
Epoch: [36][340/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.1670e-01 (9.6369e-02)	Acc@1  96.09 ( 96.79)	Acc@5 100.00 ( 99.94)
Epoch: [36][350/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.1670e-01 (9.6527e-02)	Acc@1  96.88 ( 96.79)	Acc@5 100.00 ( 99.94)
Epoch: [36][360/391]	Time  0.024 ( 0.025)	Data  0.001 ( 0.002)	Loss 9.5215e-02 (9.6287e-02)	Acc@1  96.88 ( 96.80)	Acc@5 100.00 ( 99.94)
Epoch: [36][370/391]	Time  0.027 ( 0.025)	Data  0.003 ( 0.002)	Loss 1.1615e-01 (9.6367e-02)	Acc@1  97.66 ( 96.81)	Acc@5 100.00 ( 99.94)
Epoch: [36][380/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.002)	Loss 9.9731e-02 (9.6051e-02)	Acc@1  96.88 ( 96.82)	Acc@5 100.00 ( 99.94)
Epoch: [36][390/391]	Time  0.016 ( 0.024)	Data  0.001 ( 0.002)	Loss 9.9365e-02 (9.5903e-02)	Acc@1  96.25 ( 96.82)	Acc@5 100.00 ( 99.94)
## e[36] optimizer.zero_grad (sum) time: 0.1051476001739502
## e[36]       loss.backward (sum) time: 2.1551032066345215
## e[36]      optimizer.step (sum) time: 0.8888983726501465
## epoch[36] training(only) time: 9.67954134941101
# Switched to evaluate mode...
Test: [  0/100]	Time  0.162 ( 0.162)	Loss 2.7002e-01 (2.7002e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.010 ( 0.026)	Loss 4.3164e-01 (3.0976e-01)	Acc@1  87.00 ( 90.55)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.008 ( 0.020)	Loss 4.2529e-01 (3.4244e-01)	Acc@1  84.00 ( 89.76)	Acc@5 100.00 ( 99.52)
Test: [ 30/100]	Time  0.008 ( 0.018)	Loss 3.3667e-01 (3.5356e-01)	Acc@1  89.00 ( 89.71)	Acc@5  99.00 ( 99.45)
Test: [ 40/100]	Time  0.016 ( 0.017)	Loss 4.9878e-01 (3.6687e-01)	Acc@1  89.00 ( 89.76)	Acc@5  98.00 ( 99.37)
Test: [ 50/100]	Time  0.016 ( 0.016)	Loss 1.9153e-01 (3.5150e-01)	Acc@1  93.00 ( 90.16)	Acc@5 100.00 ( 99.41)
Test: [ 60/100]	Time  0.016 ( 0.016)	Loss 2.6294e-01 (3.4984e-01)	Acc@1  91.00 ( 90.21)	Acc@5 100.00 ( 99.44)
Test: [ 70/100]	Time  0.012 ( 0.015)	Loss 5.8643e-01 (3.4931e-01)	Acc@1  88.00 ( 90.27)	Acc@5  99.00 ( 99.45)
Test: [ 80/100]	Time  0.023 ( 0.015)	Loss 2.7832e-01 (3.4733e-01)	Acc@1  88.00 ( 90.20)	Acc@5 100.00 ( 99.49)
Test: [ 90/100]	Time  0.020 ( 0.015)	Loss 1.9861e-01 (3.4302e-01)	Acc@1  91.00 ( 90.25)	Acc@5 100.00 ( 99.54)
 * Acc@1 90.280 Acc@5 99.570
### epoch[36] execution time: 11.24318528175354
EPOCH 37
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [37][  0/391]	Time  0.191 ( 0.191)	Data  0.167 ( 0.167)	Loss 1.0303e-01 (1.0303e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [37][ 10/391]	Time  0.040 ( 0.039)	Data  0.006 ( 0.017)	Loss 1.9629e-01 (1.0034e-01)	Acc@1  95.31 ( 97.02)	Acc@5 100.00 ( 99.93)
Epoch: [37][ 20/391]	Time  0.018 ( 0.031)	Data  0.001 ( 0.010)	Loss 1.0211e-01 (9.0517e-02)	Acc@1  95.31 ( 96.91)	Acc@5 100.00 ( 99.96)
Epoch: [37][ 30/391]	Time  0.034 ( 0.029)	Data  0.001 ( 0.007)	Loss 1.7059e-02 (8.7551e-02)	Acc@1 100.00 ( 97.00)	Acc@5 100.00 ( 99.95)
Epoch: [37][ 40/391]	Time  0.035 ( 0.028)	Data  0.002 ( 0.006)	Loss 5.9387e-02 (8.6214e-02)	Acc@1  97.66 ( 97.08)	Acc@5 100.00 ( 99.92)
Epoch: [37][ 50/391]	Time  0.038 ( 0.027)	Data  0.001 ( 0.005)	Loss 6.6528e-02 (8.5188e-02)	Acc@1  97.66 ( 97.10)	Acc@5 100.00 ( 99.94)
Epoch: [37][ 60/391]	Time  0.017 ( 0.026)	Data  0.000 ( 0.005)	Loss 1.5576e-01 (8.8559e-02)	Acc@1  94.53 ( 96.99)	Acc@5 100.00 ( 99.95)
Epoch: [37][ 70/391]	Time  0.035 ( 0.026)	Data  0.001 ( 0.004)	Loss 8.7433e-03 (8.5891e-02)	Acc@1 100.00 ( 97.08)	Acc@5 100.00 ( 99.96)
Epoch: [37][ 80/391]	Time  0.019 ( 0.026)	Data  0.002 ( 0.004)	Loss 1.0706e-01 (8.9194e-02)	Acc@1  95.31 ( 97.00)	Acc@5 100.00 ( 99.94)
Epoch: [37][ 90/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.004)	Loss 8.3557e-02 (8.7987e-02)	Acc@1  97.66 ( 97.03)	Acc@5 100.00 ( 99.94)
Epoch: [37][100/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.8623e-02 (8.9116e-02)	Acc@1  96.09 ( 97.00)	Acc@5 100.00 ( 99.94)
Epoch: [37][110/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.3586e-01 (8.8776e-02)	Acc@1  96.09 ( 97.04)	Acc@5 100.00 ( 99.94)
Epoch: [37][120/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 9.9731e-02 (9.0321e-02)	Acc@1  94.53 ( 96.98)	Acc@5 100.00 ( 99.94)
Epoch: [37][130/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.8196e-02 (9.0641e-02)	Acc@1  96.09 ( 96.99)	Acc@5 100.00 ( 99.94)
Epoch: [37][140/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.1372e-02 (8.9344e-02)	Acc@1  98.44 ( 97.01)	Acc@5 100.00 ( 99.94)
Epoch: [37][150/391]	Time  0.035 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.4099e-01 (9.0731e-02)	Acc@1  96.09 ( 96.96)	Acc@5 100.00 ( 99.95)
Epoch: [37][160/391]	Time  0.032 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.3855e-01 (9.1200e-02)	Acc@1  96.88 ( 96.91)	Acc@5 100.00 ( 99.95)
Epoch: [37][170/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.2335e-01 (9.1172e-02)	Acc@1  96.88 ( 96.93)	Acc@5  99.22 ( 99.95)
Epoch: [37][180/391]	Time  0.031 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.4819e-01 (9.1039e-02)	Acc@1  94.53 ( 96.92)	Acc@5 100.00 ( 99.95)
Epoch: [37][190/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.6753e-02 (9.1122e-02)	Acc@1  98.44 ( 96.92)	Acc@5 100.00 ( 99.94)
Epoch: [37][200/391]	Time  0.036 ( 0.025)	Data  0.002 ( 0.003)	Loss 1.6248e-01 (9.1274e-02)	Acc@1  95.31 ( 96.93)	Acc@5 100.00 ( 99.94)
Epoch: [37][210/391]	Time  0.017 ( 0.025)	Data  0.001 ( 0.002)	Loss 5.2979e-02 (9.1510e-02)	Acc@1  98.44 ( 96.92)	Acc@5 100.00 ( 99.94)
Epoch: [37][220/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 5.5756e-02 (9.1384e-02)	Acc@1  96.88 ( 96.92)	Acc@5 100.00 ( 99.94)
Epoch: [37][230/391]	Time  0.029 ( 0.025)	Data  0.001 ( 0.002)	Loss 7.1899e-02 (9.0945e-02)	Acc@1  97.66 ( 96.93)	Acc@5 100.00 ( 99.95)
Epoch: [37][240/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.2439e-01 (9.0369e-02)	Acc@1  95.31 ( 96.95)	Acc@5 100.00 ( 99.95)
Epoch: [37][250/391]	Time  0.018 ( 0.025)	Data  0.002 ( 0.002)	Loss 9.9670e-02 (9.0633e-02)	Acc@1  96.88 ( 96.95)	Acc@5 100.00 ( 99.95)
Epoch: [37][260/391]	Time  0.026 ( 0.025)	Data  0.002 ( 0.002)	Loss 9.9426e-02 (9.0884e-02)	Acc@1  96.09 ( 96.93)	Acc@5 100.00 ( 99.94)
Epoch: [37][270/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 8.9172e-02 (9.1298e-02)	Acc@1  95.31 ( 96.91)	Acc@5 100.00 ( 99.95)
Epoch: [37][280/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.2732e-01 (9.1547e-02)	Acc@1  95.31 ( 96.88)	Acc@5 100.00 ( 99.94)
Epoch: [37][290/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.3660e-01 (9.0963e-02)	Acc@1  96.88 ( 96.91)	Acc@5 100.00 ( 99.95)
Epoch: [37][300/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.1261e-01 (9.0656e-02)	Acc@1  95.31 ( 96.91)	Acc@5 100.00 ( 99.95)
Epoch: [37][310/391]	Time  0.028 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.2323e-01 (9.0661e-02)	Acc@1  94.53 ( 96.91)	Acc@5 100.00 ( 99.95)
Epoch: [37][320/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.6980e-01 (9.1706e-02)	Acc@1  95.31 ( 96.87)	Acc@5  99.22 ( 99.95)
Epoch: [37][330/391]	Time  0.040 ( 0.024)	Data  0.002 ( 0.002)	Loss 1.1053e-01 (9.1711e-02)	Acc@1  95.31 ( 96.87)	Acc@5  99.22 ( 99.94)
Epoch: [37][340/391]	Time  0.032 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.1066e-01 (9.1548e-02)	Acc@1  94.53 ( 96.87)	Acc@5 100.00 ( 99.95)
Epoch: [37][350/391]	Time  0.035 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.2964e-01 (9.1369e-02)	Acc@1  97.66 ( 96.90)	Acc@5 100.00 ( 99.95)
Epoch: [37][360/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 9.7595e-02 (9.1678e-02)	Acc@1  96.09 ( 96.87)	Acc@5 100.00 ( 99.95)
Epoch: [37][370/391]	Time  0.019 ( 0.024)	Data  0.000 ( 0.002)	Loss 4.6997e-02 (9.1862e-02)	Acc@1  98.44 ( 96.86)	Acc@5 100.00 ( 99.95)
Epoch: [37][380/391]	Time  0.029 ( 0.024)	Data  0.001 ( 0.002)	Loss 9.2957e-02 (9.1829e-02)	Acc@1  97.66 ( 96.86)	Acc@5 100.00 ( 99.95)
Epoch: [37][390/391]	Time  0.014 ( 0.024)	Data  0.000 ( 0.002)	Loss 7.3914e-02 (9.1538e-02)	Acc@1  98.75 ( 96.87)	Acc@5 100.00 ( 99.95)
## e[37] optimizer.zero_grad (sum) time: 0.10658836364746094
## e[37]       loss.backward (sum) time: 2.1895627975463867
## e[37]      optimizer.step (sum) time: 0.8992118835449219
## epoch[37] training(only) time: 9.632195234298706
# Switched to evaluate mode...
Test: [  0/100]	Time  0.168 ( 0.168)	Loss 2.4744e-01 (2.4744e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.015 ( 0.028)	Loss 3.8989e-01 (3.0815e-01)	Acc@1  88.00 ( 90.36)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.008 ( 0.020)	Loss 4.1455e-01 (3.3813e-01)	Acc@1  85.00 ( 90.14)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.012 ( 0.018)	Loss 3.4961e-01 (3.5515e-01)	Acc@1  88.00 ( 90.16)	Acc@5 100.00 ( 99.58)
Test: [ 40/100]	Time  0.008 ( 0.017)	Loss 4.9585e-01 (3.7045e-01)	Acc@1  90.00 ( 89.93)	Acc@5  98.00 ( 99.46)
Test: [ 50/100]	Time  0.011 ( 0.016)	Loss 1.5527e-01 (3.5639e-01)	Acc@1  93.00 ( 90.18)	Acc@5 100.00 ( 99.47)
Test: [ 60/100]	Time  0.008 ( 0.016)	Loss 2.4585e-01 (3.5705e-01)	Acc@1  94.00 ( 90.08)	Acc@5 100.00 ( 99.51)
Test: [ 70/100]	Time  0.015 ( 0.015)	Loss 5.8496e-01 (3.5579e-01)	Acc@1  87.00 ( 90.17)	Acc@5  99.00 ( 99.49)
Test: [ 80/100]	Time  0.008 ( 0.015)	Loss 2.7905e-01 (3.5140e-01)	Acc@1  92.00 ( 90.23)	Acc@5 100.00 ( 99.52)
Test: [ 90/100]	Time  0.008 ( 0.015)	Loss 2.3157e-01 (3.4789e-01)	Acc@1  91.00 ( 90.24)	Acc@5 100.00 ( 99.56)
 * Acc@1 90.250 Acc@5 99.580
### epoch[37] execution time: 11.217084407806396
EPOCH 38
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [38][  0/391]	Time  0.194 ( 0.194)	Data  0.172 ( 0.172)	Loss 1.5295e-01 (1.5295e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [38][ 10/391]	Time  0.028 ( 0.040)	Data  0.000 ( 0.017)	Loss 9.9426e-02 (8.8900e-02)	Acc@1  96.88 ( 97.02)	Acc@5 100.00 ( 99.93)
Epoch: [38][ 20/391]	Time  0.018 ( 0.032)	Data  0.001 ( 0.010)	Loss 1.4758e-01 (9.1081e-02)	Acc@1  96.09 ( 96.80)	Acc@5 100.00 ( 99.96)
Epoch: [38][ 30/391]	Time  0.027 ( 0.030)	Data  0.002 ( 0.007)	Loss 1.1475e-01 (8.6414e-02)	Acc@1  96.09 ( 97.00)	Acc@5 100.00 ( 99.97)
Epoch: [38][ 40/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.006)	Loss 3.7140e-02 (8.3157e-02)	Acc@1  98.44 ( 97.12)	Acc@5 100.00 ( 99.98)
Epoch: [38][ 50/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.005)	Loss 2.1347e-02 (7.9451e-02)	Acc@1  99.22 ( 97.29)	Acc@5 100.00 ( 99.98)
Epoch: [38][ 60/391]	Time  0.026 ( 0.027)	Data  0.002 ( 0.005)	Loss 8.3252e-02 (8.1416e-02)	Acc@1  96.88 ( 97.21)	Acc@5 100.00 ( 99.97)
Epoch: [38][ 70/391]	Time  0.037 ( 0.026)	Data  0.000 ( 0.004)	Loss 1.7615e-01 (8.2535e-02)	Acc@1  92.97 ( 97.16)	Acc@5 100.00 ( 99.98)
Epoch: [38][ 80/391]	Time  0.017 ( 0.026)	Data  0.001 ( 0.004)	Loss 7.0190e-02 (8.3777e-02)	Acc@1  97.66 ( 97.13)	Acc@5 100.00 ( 99.97)
Epoch: [38][ 90/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.1444e-01 (8.3551e-02)	Acc@1  96.09 ( 97.14)	Acc@5 100.00 ( 99.97)
Epoch: [38][100/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6211e-01 (8.4379e-02)	Acc@1  96.09 ( 97.10)	Acc@5 100.00 ( 99.97)
Epoch: [38][110/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 9.2773e-02 (8.4919e-02)	Acc@1  96.88 ( 97.09)	Acc@5 100.00 ( 99.97)
Epoch: [38][120/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.5388e-02 (8.6123e-02)	Acc@1  96.88 ( 97.04)	Acc@5 100.00 ( 99.97)
Epoch: [38][130/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.3984e-02 (8.6536e-02)	Acc@1  96.09 ( 96.99)	Acc@5 100.00 ( 99.96)
Epoch: [38][140/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.2755e-02 (8.5783e-02)	Acc@1  98.44 ( 97.00)	Acc@5 100.00 ( 99.97)
Epoch: [38][150/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 8.8074e-02 (8.6150e-02)	Acc@1  96.09 ( 96.99)	Acc@5 100.00 ( 99.96)
Epoch: [38][160/391]	Time  0.017 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.3232e-01 (8.6631e-02)	Acc@1  95.31 ( 96.99)	Acc@5 100.00 ( 99.97)
Epoch: [38][170/391]	Time  0.039 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.4321e-02 (8.5661e-02)	Acc@1  97.66 ( 97.02)	Acc@5 100.00 ( 99.97)
Epoch: [38][180/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.0131e-02 (8.6192e-02)	Acc@1  99.22 ( 97.01)	Acc@5 100.00 ( 99.97)
Epoch: [38][190/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.5796e-01 (8.6830e-02)	Acc@1  96.88 ( 97.01)	Acc@5  99.22 ( 99.96)
Epoch: [38][200/391]	Time  0.030 ( 0.025)	Data  0.002 ( 0.003)	Loss 1.2561e-01 (8.8182e-02)	Acc@1  96.88 ( 97.01)	Acc@5 100.00 ( 99.96)
Epoch: [38][210/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.2549e-01 (8.7663e-02)	Acc@1  97.66 ( 97.06)	Acc@5  99.22 ( 99.96)
Epoch: [38][220/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.9967e-02 (8.7339e-02)	Acc@1  97.66 ( 97.08)	Acc@5 100.00 ( 99.96)
Epoch: [38][230/391]	Time  0.037 ( 0.025)	Data  0.002 ( 0.003)	Loss 6.6040e-02 (8.8070e-02)	Acc@1  98.44 ( 97.07)	Acc@5 100.00 ( 99.96)
Epoch: [38][240/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 8.1238e-02 (8.7591e-02)	Acc@1  96.88 ( 97.10)	Acc@5 100.00 ( 99.96)
Epoch: [38][250/391]	Time  0.024 ( 0.025)	Data  0.001 ( 0.002)	Loss 7.2083e-02 (8.7517e-02)	Acc@1  96.88 ( 97.09)	Acc@5 100.00 ( 99.96)
Epoch: [38][260/391]	Time  0.032 ( 0.025)	Data  0.001 ( 0.002)	Loss 8.3313e-02 (8.7006e-02)	Acc@1  97.66 ( 97.11)	Acc@5 100.00 ( 99.96)
Epoch: [38][270/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 5.1666e-02 (8.6474e-02)	Acc@1  98.44 ( 97.13)	Acc@5 100.00 ( 99.96)
Epoch: [38][280/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.1328e-01 (8.6537e-02)	Acc@1  96.88 ( 97.14)	Acc@5 100.00 ( 99.96)
Epoch: [38][290/391]	Time  0.037 ( 0.025)	Data  0.002 ( 0.002)	Loss 1.0901e-01 (8.7158e-02)	Acc@1  97.66 ( 97.12)	Acc@5 100.00 ( 99.96)
Epoch: [38][300/391]	Time  0.035 ( 0.025)	Data  0.001 ( 0.002)	Loss 5.2002e-02 (8.7299e-02)	Acc@1  99.22 ( 97.12)	Acc@5 100.00 ( 99.96)
Epoch: [38][310/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 8.9417e-02 (8.7158e-02)	Acc@1  96.09 ( 97.12)	Acc@5  99.22 ( 99.95)
Epoch: [38][320/391]	Time  0.033 ( 0.025)	Data  0.000 ( 0.002)	Loss 7.5134e-02 (8.7676e-02)	Acc@1  97.66 ( 97.10)	Acc@5 100.00 ( 99.95)
Epoch: [38][330/391]	Time  0.018 ( 0.025)	Data  0.000 ( 0.002)	Loss 2.1606e-02 (8.8074e-02)	Acc@1 100.00 ( 97.10)	Acc@5 100.00 ( 99.95)
Epoch: [38][340/391]	Time  0.018 ( 0.025)	Data  0.002 ( 0.002)	Loss 1.2585e-01 (8.8419e-02)	Acc@1  96.09 ( 97.09)	Acc@5 100.00 ( 99.95)
Epoch: [38][350/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.002)	Loss 6.2012e-02 (8.7601e-02)	Acc@1  96.09 ( 97.11)	Acc@5 100.00 ( 99.95)
Epoch: [38][360/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.4209e-01 (8.7477e-02)	Acc@1  96.09 ( 97.11)	Acc@5 100.00 ( 99.95)
Epoch: [38][370/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.8229e-02 (8.7363e-02)	Acc@1 100.00 ( 97.12)	Acc@5 100.00 ( 99.95)
Epoch: [38][380/391]	Time  0.017 ( 0.024)	Data  0.000 ( 0.002)	Loss 5.5756e-02 (8.7405e-02)	Acc@1  97.66 ( 97.12)	Acc@5 100.00 ( 99.94)
Epoch: [38][390/391]	Time  0.013 ( 0.024)	Data  0.000 ( 0.002)	Loss 2.1255e-02 (8.7403e-02)	Acc@1 100.00 ( 97.11)	Acc@5 100.00 ( 99.95)
## e[38] optimizer.zero_grad (sum) time: 0.10468554496765137
## e[38]       loss.backward (sum) time: 2.1647439002990723
## e[38]      optimizer.step (sum) time: 0.8859059810638428
## epoch[38] training(only) time: 9.691074132919312
# Switched to evaluate mode...
Test: [  0/100]	Time  0.159 ( 0.159)	Loss 2.4109e-01 (2.4109e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.020 ( 0.028)	Loss 3.7231e-01 (3.1580e-01)	Acc@1  89.00 ( 91.00)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.010 ( 0.021)	Loss 4.4873e-01 (3.4763e-01)	Acc@1  86.00 ( 90.14)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.008 ( 0.018)	Loss 3.4888e-01 (3.6133e-01)	Acc@1  90.00 ( 90.13)	Acc@5 100.00 ( 99.52)
Test: [ 40/100]	Time  0.008 ( 0.017)	Loss 5.2979e-01 (3.7600e-01)	Acc@1  88.00 ( 89.93)	Acc@5  97.00 ( 99.44)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 1.8201e-01 (3.6112e-01)	Acc@1  93.00 ( 90.10)	Acc@5 100.00 ( 99.45)
Test: [ 60/100]	Time  0.008 ( 0.016)	Loss 2.2363e-01 (3.5598e-01)	Acc@1  93.00 ( 90.15)	Acc@5 100.00 ( 99.48)
Test: [ 70/100]	Time  0.016 ( 0.015)	Loss 6.3086e-01 (3.5358e-01)	Acc@1  89.00 ( 90.27)	Acc@5  99.00 ( 99.51)
Test: [ 80/100]	Time  0.008 ( 0.015)	Loss 2.6440e-01 (3.4961e-01)	Acc@1  88.00 ( 90.20)	Acc@5 100.00 ( 99.53)
Test: [ 90/100]	Time  0.032 ( 0.015)	Loss 2.2266e-01 (3.4657e-01)	Acc@1  90.00 ( 90.16)	Acc@5 100.00 ( 99.58)
 * Acc@1 90.220 Acc@5 99.610
### epoch[38] execution time: 11.254912614822388
EPOCH 39
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [39][  0/391]	Time  0.242 ( 0.242)	Data  0.219 ( 0.219)	Loss 1.6760e-01 (1.6760e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
Epoch: [39][ 10/391]	Time  0.019 ( 0.044)	Data  0.001 ( 0.021)	Loss 8.1726e-02 (9.7595e-02)	Acc@1  95.31 ( 96.31)	Acc@5 100.00 ( 99.93)
Epoch: [39][ 20/391]	Time  0.042 ( 0.035)	Data  0.005 ( 0.012)	Loss 7.4219e-02 (7.7564e-02)	Acc@1  97.66 ( 97.28)	Acc@5 100.00 ( 99.96)
Epoch: [39][ 30/391]	Time  0.030 ( 0.031)	Data  0.001 ( 0.009)	Loss 1.0803e-01 (8.0919e-02)	Acc@1  95.31 ( 97.18)	Acc@5 100.00 ( 99.95)
Epoch: [39][ 40/391]	Time  0.032 ( 0.029)	Data  0.001 ( 0.007)	Loss 5.3436e-02 (7.9670e-02)	Acc@1  98.44 ( 97.33)	Acc@5 100.00 ( 99.96)
Epoch: [39][ 50/391]	Time  0.018 ( 0.028)	Data  0.001 ( 0.006)	Loss 5.2734e-02 (7.8614e-02)	Acc@1  98.44 ( 97.27)	Acc@5 100.00 ( 99.97)
Epoch: [39][ 60/391]	Time  0.032 ( 0.027)	Data  0.005 ( 0.005)	Loss 5.2643e-02 (7.8229e-02)	Acc@1  98.44 ( 97.37)	Acc@5 100.00 ( 99.96)
Epoch: [39][ 70/391]	Time  0.019 ( 0.027)	Data  0.002 ( 0.005)	Loss 5.9418e-02 (7.9731e-02)	Acc@1  97.66 ( 97.34)	Acc@5 100.00 ( 99.97)
Epoch: [39][ 80/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.005)	Loss 1.0785e-01 (8.1623e-02)	Acc@1  96.09 ( 97.25)	Acc@5 100.00 ( 99.96)
Epoch: [39][ 90/391]	Time  0.026 ( 0.026)	Data  0.002 ( 0.004)	Loss 2.6245e-02 (8.2005e-02)	Acc@1  98.44 ( 97.22)	Acc@5 100.00 ( 99.97)
Epoch: [39][100/391]	Time  0.032 ( 0.026)	Data  0.002 ( 0.004)	Loss 6.5308e-02 (8.0165e-02)	Acc@1  96.88 ( 97.28)	Acc@5 100.00 ( 99.97)
Epoch: [39][110/391]	Time  0.036 ( 0.026)	Data  0.001 ( 0.004)	Loss 3.8025e-02 (7.8262e-02)	Acc@1  99.22 ( 97.38)	Acc@5 100.00 ( 99.97)
Epoch: [39][120/391]	Time  0.043 ( 0.025)	Data  0.010 ( 0.004)	Loss 6.8542e-02 (7.8954e-02)	Acc@1  97.66 ( 97.39)	Acc@5 100.00 ( 99.97)
Epoch: [39][130/391]	Time  0.037 ( 0.025)	Data  0.003 ( 0.004)	Loss 6.8542e-02 (7.8750e-02)	Acc@1  97.66 ( 97.42)	Acc@5 100.00 ( 99.96)
Epoch: [39][140/391]	Time  0.028 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.5248e-02 (7.8802e-02)	Acc@1  98.44 ( 97.41)	Acc@5 100.00 ( 99.96)
Epoch: [39][150/391]	Time  0.017 ( 0.025)	Data  0.001 ( 0.003)	Loss 7.5623e-02 (7.8734e-02)	Acc@1  97.66 ( 97.43)	Acc@5 100.00 ( 99.96)
Epoch: [39][160/391]	Time  0.030 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.3281e-01 (7.8312e-02)	Acc@1  96.88 ( 97.47)	Acc@5 100.00 ( 99.96)
Epoch: [39][170/391]	Time  0.030 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.5571e-02 (7.9215e-02)	Acc@1  96.09 ( 97.45)	Acc@5 100.00 ( 99.96)
Epoch: [39][180/391]	Time  0.028 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.9966e-02 (7.9806e-02)	Acc@1  97.66 ( 97.44)	Acc@5 100.00 ( 99.95)
Epoch: [39][190/391]	Time  0.023 ( 0.025)	Data  0.000 ( 0.003)	Loss 1.1853e-01 (8.0523e-02)	Acc@1  95.31 ( 97.41)	Acc@5 100.00 ( 99.95)
Epoch: [39][200/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.2030e-01 (8.1071e-02)	Acc@1  97.66 ( 97.38)	Acc@5 100.00 ( 99.95)
Epoch: [39][210/391]	Time  0.024 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.0558e-02 (8.0518e-02)	Acc@1  99.22 ( 97.39)	Acc@5 100.00 ( 99.96)
Epoch: [39][220/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.7383e-02 (7.9915e-02)	Acc@1  96.09 ( 97.38)	Acc@5 100.00 ( 99.96)
Epoch: [39][230/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.4392e-02 (8.0358e-02)	Acc@1  98.44 ( 97.39)	Acc@5 100.00 ( 99.95)
Epoch: [39][240/391]	Time  0.030 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.0223e-01 (8.0002e-02)	Acc@1  97.66 ( 97.42)	Acc@5 100.00 ( 99.95)
Epoch: [39][250/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.1066e-02 (7.9718e-02)	Acc@1  97.66 ( 97.41)	Acc@5 100.00 ( 99.95)
Epoch: [39][260/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.0873e-02 (7.9145e-02)	Acc@1  96.88 ( 97.41)	Acc@5 100.00 ( 99.96)
Epoch: [39][270/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 7.8552e-02 (7.9302e-02)	Acc@1  96.88 ( 97.40)	Acc@5 100.00 ( 99.96)
Epoch: [39][280/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 3.1708e-02 (7.9595e-02)	Acc@1  98.44 ( 97.39)	Acc@5 100.00 ( 99.96)
Epoch: [39][290/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 1.0645e-01 (8.0309e-02)	Acc@1  96.88 ( 97.37)	Acc@5 100.00 ( 99.96)
Epoch: [39][300/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.0773e-01 (8.0104e-02)	Acc@1  96.09 ( 97.37)	Acc@5 100.00 ( 99.96)
Epoch: [39][310/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.4586e-02 (8.0258e-02)	Acc@1  97.66 ( 97.35)	Acc@5 100.00 ( 99.96)
Epoch: [39][320/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 8.0750e-02 (8.0267e-02)	Acc@1  96.88 ( 97.35)	Acc@5 100.00 ( 99.96)
Epoch: [39][330/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.002)	Loss 5.0354e-02 (7.9434e-02)	Acc@1  98.44 ( 97.37)	Acc@5  99.22 ( 99.96)
Epoch: [39][340/391]	Time  0.037 ( 0.025)	Data  0.001 ( 0.002)	Loss 5.4840e-02 (7.9229e-02)	Acc@1  98.44 ( 97.39)	Acc@5 100.00 ( 99.96)
Epoch: [39][350/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 7.8247e-02 (7.9306e-02)	Acc@1  97.66 ( 97.39)	Acc@5 100.00 ( 99.96)
Epoch: [39][360/391]	Time  0.029 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.1523e-01 (7.9123e-02)	Acc@1  94.53 ( 97.39)	Acc@5 100.00 ( 99.96)
Epoch: [39][370/391]	Time  0.036 ( 0.025)	Data  0.001 ( 0.002)	Loss 7.0129e-02 (7.8823e-02)	Acc@1  98.44 ( 97.41)	Acc@5 100.00 ( 99.96)
Epoch: [39][380/391]	Time  0.031 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.2695e-01 (7.9285e-02)	Acc@1  98.44 ( 97.41)	Acc@5 100.00 ( 99.96)
Epoch: [39][390/391]	Time  0.015 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.3379e-01 (7.9467e-02)	Acc@1  96.25 ( 97.39)	Acc@5 100.00 ( 99.96)
## e[39] optimizer.zero_grad (sum) time: 0.10521626472473145
## e[39]       loss.backward (sum) time: 2.1585216522216797
## e[39]      optimizer.step (sum) time: 0.8887951374053955
## epoch[39] training(only) time: 9.698918581008911
# Switched to evaluate mode...
Test: [  0/100]	Time  0.173 ( 0.173)	Loss 2.4695e-01 (2.4695e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.010 ( 0.028)	Loss 3.7891e-01 (3.0818e-01)	Acc@1  91.00 ( 90.91)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.010 ( 0.021)	Loss 4.1650e-01 (3.3956e-01)	Acc@1  86.00 ( 90.19)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.022 ( 0.019)	Loss 3.7500e-01 (3.5735e-01)	Acc@1  88.00 ( 90.10)	Acc@5  99.00 ( 99.61)
Test: [ 40/100]	Time  0.017 ( 0.017)	Loss 5.5811e-01 (3.7672e-01)	Acc@1  90.00 ( 89.93)	Acc@5  97.00 ( 99.51)
Test: [ 50/100]	Time  0.008 ( 0.016)	Loss 1.4026e-01 (3.6151e-01)	Acc@1  94.00 ( 90.24)	Acc@5 100.00 ( 99.53)
Test: [ 60/100]	Time  0.008 ( 0.016)	Loss 2.5171e-01 (3.6235e-01)	Acc@1  93.00 ( 90.11)	Acc@5 100.00 ( 99.56)
Test: [ 70/100]	Time  0.015 ( 0.016)	Loss 5.9473e-01 (3.6042e-01)	Acc@1  87.00 ( 90.11)	Acc@5  99.00 ( 99.55)
Test: [ 80/100]	Time  0.012 ( 0.015)	Loss 2.1411e-01 (3.5658e-01)	Acc@1  90.00 ( 90.17)	Acc@5 100.00 ( 99.59)
Test: [ 90/100]	Time  0.008 ( 0.015)	Loss 2.0984e-01 (3.5256e-01)	Acc@1  91.00 ( 90.13)	Acc@5 100.00 ( 99.63)
 * Acc@1 90.110 Acc@5 99.660
### epoch[39] execution time: 11.317714214324951
EPOCH 40
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [40][  0/391]	Time  0.192 ( 0.192)	Data  0.173 ( 0.173)	Loss 1.3046e-02 (1.3046e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [40][ 10/391]	Time  0.019 ( 0.039)	Data  0.001 ( 0.018)	Loss 1.6980e-01 (6.9669e-02)	Acc@1  96.09 ( 97.94)	Acc@5  99.22 ( 99.93)
Epoch: [40][ 20/391]	Time  0.020 ( 0.032)	Data  0.001 ( 0.010)	Loss 1.5955e-01 (7.6598e-02)	Acc@1  94.53 ( 97.54)	Acc@5 100.00 ( 99.96)
Epoch: [40][ 30/391]	Time  0.018 ( 0.029)	Data  0.001 ( 0.007)	Loss 3.5095e-02 (7.9096e-02)	Acc@1  98.44 ( 97.28)	Acc@5 100.00 ( 99.97)
Epoch: [40][ 40/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.006)	Loss 1.0425e-01 (8.0490e-02)	Acc@1  96.09 ( 97.28)	Acc@5 100.00 ( 99.98)
Epoch: [40][ 50/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.005)	Loss 4.8340e-02 (8.2235e-02)	Acc@1  98.44 ( 97.21)	Acc@5 100.00 ( 99.95)
Epoch: [40][ 60/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.005)	Loss 5.8197e-02 (8.2724e-02)	Acc@1  97.66 ( 97.13)	Acc@5 100.00 ( 99.96)
Epoch: [40][ 70/391]	Time  0.019 ( 0.026)	Data  0.002 ( 0.004)	Loss 1.0413e-01 (8.2753e-02)	Acc@1  96.09 ( 97.05)	Acc@5 100.00 ( 99.97)
Epoch: [40][ 80/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.004)	Loss 2.9709e-02 (8.1999e-02)	Acc@1  99.22 ( 97.08)	Acc@5 100.00 ( 99.97)
Epoch: [40][ 90/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.004)	Loss 2.0874e-02 (7.9786e-02)	Acc@1  99.22 ( 97.19)	Acc@5 100.00 ( 99.97)
Epoch: [40][100/391]	Time  0.017 ( 0.026)	Data  0.000 ( 0.004)	Loss 2.6505e-02 (7.9232e-02)	Acc@1 100.00 ( 97.24)	Acc@5 100.00 ( 99.96)
Epoch: [40][110/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 1.2659e-01 (7.8618e-02)	Acc@1  96.88 ( 97.27)	Acc@5 100.00 ( 99.96)
Epoch: [40][120/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 9.3506e-02 (7.9215e-02)	Acc@1  97.66 ( 97.25)	Acc@5 100.00 ( 99.96)
Epoch: [40][130/391]	Time  0.032 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.8523e-02 (8.0666e-02)	Acc@1  97.66 ( 97.23)	Acc@5 100.00 ( 99.96)
Epoch: [40][140/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.1182e-01 (8.1480e-02)	Acc@1  97.66 ( 97.19)	Acc@5 100.00 ( 99.96)
Epoch: [40][150/391]	Time  0.042 ( 0.025)	Data  0.004 ( 0.003)	Loss 1.1462e-01 (8.1766e-02)	Acc@1  96.09 ( 97.18)	Acc@5 100.00 ( 99.96)
Epoch: [40][160/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.4551e-01 (8.1702e-02)	Acc@1  95.31 ( 97.16)	Acc@5 100.00 ( 99.96)
Epoch: [40][170/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 4.8492e-02 (8.1951e-02)	Acc@1  98.44 ( 97.15)	Acc@5 100.00 ( 99.96)
Epoch: [40][180/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 6.3904e-02 (8.1988e-02)	Acc@1  96.88 ( 97.19)	Acc@5 100.00 ( 99.97)
Epoch: [40][190/391]	Time  0.031 ( 0.025)	Data  0.002 ( 0.003)	Loss 4.2847e-02 (8.1109e-02)	Acc@1  98.44 ( 97.22)	Acc@5 100.00 ( 99.97)
Epoch: [40][200/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 7.2327e-02 (8.0505e-02)	Acc@1  96.09 ( 97.25)	Acc@5 100.00 ( 99.97)
Epoch: [40][210/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.7322e-02 (7.9825e-02)	Acc@1  96.88 ( 97.28)	Acc@5 100.00 ( 99.97)
Epoch: [40][220/391]	Time  0.037 ( 0.025)	Data  0.005 ( 0.003)	Loss 8.7524e-02 (8.0278e-02)	Acc@1  96.88 ( 97.28)	Acc@5 100.00 ( 99.96)
Epoch: [40][230/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.0344e-02 (8.0199e-02)	Acc@1  99.22 ( 97.30)	Acc@5 100.00 ( 99.97)
Epoch: [40][240/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.0258e-02 (8.0709e-02)	Acc@1  98.44 ( 97.29)	Acc@5 100.00 ( 99.96)
Epoch: [40][250/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.3894e-02 (8.0786e-02)	Acc@1  98.44 ( 97.29)	Acc@5 100.00 ( 99.96)
Epoch: [40][260/391]	Time  0.038 ( 0.025)	Data  0.005 ( 0.002)	Loss 7.8857e-02 (8.0859e-02)	Acc@1  98.44 ( 97.29)	Acc@5 100.00 ( 99.96)
Epoch: [40][270/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.0876e-01 (8.1782e-02)	Acc@1  96.88 ( 97.25)	Acc@5 100.00 ( 99.96)
Epoch: [40][280/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 5.0507e-02 (8.1444e-02)	Acc@1  98.44 ( 97.26)	Acc@5 100.00 ( 99.96)
Epoch: [40][290/391]	Time  0.035 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.1046e-02 (8.0885e-02)	Acc@1  98.44 ( 97.28)	Acc@5 100.00 ( 99.96)
Epoch: [40][300/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.3488e-02 (8.0892e-02)	Acc@1  97.66 ( 97.29)	Acc@5 100.00 ( 99.95)
Epoch: [40][310/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.3199e-02 (8.0793e-02)	Acc@1 100.00 ( 97.30)	Acc@5 100.00 ( 99.95)
Epoch: [40][320/391]	Time  0.029 ( 0.025)	Data  0.002 ( 0.002)	Loss 5.6549e-02 (8.0675e-02)	Acc@1  97.66 ( 97.30)	Acc@5 100.00 ( 99.96)
Epoch: [40][330/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 7.1533e-02 (8.0811e-02)	Acc@1  97.66 ( 97.29)	Acc@5 100.00 ( 99.96)
Epoch: [40][340/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 6.7322e-02 (8.0928e-02)	Acc@1  96.88 ( 97.28)	Acc@5 100.00 ( 99.95)
Epoch: [40][350/391]	Time  0.030 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.5406e-02 (8.1158e-02)	Acc@1  99.22 ( 97.29)	Acc@5 100.00 ( 99.96)
Epoch: [40][360/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.0205e-01 (8.1621e-02)	Acc@1  96.88 ( 97.28)	Acc@5 100.00 ( 99.96)
Epoch: [40][370/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 6.0028e-02 (8.1594e-02)	Acc@1  97.66 ( 97.29)	Acc@5 100.00 ( 99.95)
Epoch: [40][380/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 5.6396e-02 (8.1059e-02)	Acc@1  98.44 ( 97.32)	Acc@5 100.00 ( 99.95)
Epoch: [40][390/391]	Time  0.016 ( 0.024)	Data  0.001 ( 0.002)	Loss 9.2102e-02 (8.0829e-02)	Acc@1  96.25 ( 97.33)	Acc@5 100.00 ( 99.95)
## e[40] optimizer.zero_grad (sum) time: 0.10503602027893066
## e[40]       loss.backward (sum) time: 2.179517984390259
## e[40]      optimizer.step (sum) time: 0.8826682567596436
## epoch[40] training(only) time: 9.645291805267334
# Switched to evaluate mode...
Test: [  0/100]	Time  0.164 ( 0.164)	Loss 2.3328e-01 (2.3328e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.008 ( 0.027)	Loss 3.9893e-01 (3.1961e-01)	Acc@1  92.00 ( 90.91)	Acc@5  99.00 ( 99.55)
Test: [ 20/100]	Time  0.015 ( 0.021)	Loss 4.2578e-01 (3.4720e-01)	Acc@1  86.00 ( 90.19)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.015 ( 0.018)	Loss 2.9224e-01 (3.5923e-01)	Acc@1  91.00 ( 90.32)	Acc@5  99.00 ( 99.52)
Test: [ 40/100]	Time  0.016 ( 0.017)	Loss 5.4785e-01 (3.7828e-01)	Acc@1  87.00 ( 90.02)	Acc@5  98.00 ( 99.49)
Test: [ 50/100]	Time  0.017 ( 0.016)	Loss 1.7090e-01 (3.6308e-01)	Acc@1  92.00 ( 90.24)	Acc@5 100.00 ( 99.51)
Test: [ 60/100]	Time  0.017 ( 0.016)	Loss 2.7661e-01 (3.6135e-01)	Acc@1  89.00 ( 90.16)	Acc@5 100.00 ( 99.54)
Test: [ 70/100]	Time  0.016 ( 0.015)	Loss 6.0645e-01 (3.5963e-01)	Acc@1  87.00 ( 90.27)	Acc@5 100.00 ( 99.55)
Test: [ 80/100]	Time  0.008 ( 0.015)	Loss 1.8701e-01 (3.5508e-01)	Acc@1  94.00 ( 90.32)	Acc@5 100.00 ( 99.56)
Test: [ 90/100]	Time  0.016 ( 0.015)	Loss 1.6284e-01 (3.5089e-01)	Acc@1  94.00 ( 90.31)	Acc@5 100.00 ( 99.59)
 * Acc@1 90.330 Acc@5 99.630
### epoch[40] execution time: 11.228057384490967
EPOCH 41
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [41][  0/391]	Time  0.230 ( 0.230)	Data  0.209 ( 0.209)	Loss 5.5939e-02 (5.5939e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [41][ 10/391]	Time  0.018 ( 0.041)	Data  0.001 ( 0.020)	Loss 4.8462e-02 (6.2077e-02)	Acc@1  98.44 ( 97.80)	Acc@5 100.00 (100.00)
Epoch: [41][ 20/391]	Time  0.025 ( 0.033)	Data  0.001 ( 0.012)	Loss 1.2964e-01 (7.0476e-02)	Acc@1  94.53 ( 97.54)	Acc@5 100.00 (100.00)
Epoch: [41][ 30/391]	Time  0.018 ( 0.030)	Data  0.001 ( 0.008)	Loss 5.7037e-02 (7.2918e-02)	Acc@1  98.44 ( 97.53)	Acc@5 100.00 (100.00)
Epoch: [41][ 40/391]	Time  0.018 ( 0.028)	Data  0.001 ( 0.007)	Loss 1.0236e-01 (6.7722e-02)	Acc@1  96.09 ( 97.77)	Acc@5 100.00 (100.00)
Epoch: [41][ 50/391]	Time  0.017 ( 0.027)	Data  0.000 ( 0.006)	Loss 3.9429e-02 (6.7142e-02)	Acc@1 100.00 ( 97.86)	Acc@5 100.00 ( 99.98)
Epoch: [41][ 60/391]	Time  0.033 ( 0.027)	Data  0.001 ( 0.005)	Loss 3.2349e-02 (6.7433e-02)	Acc@1  99.22 ( 97.90)	Acc@5 100.00 ( 99.97)
Epoch: [41][ 70/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.005)	Loss 8.4351e-02 (6.7218e-02)	Acc@1  97.66 ( 97.89)	Acc@5 100.00 ( 99.98)
Epoch: [41][ 80/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.005)	Loss 8.7708e-02 (6.8685e-02)	Acc@1  96.09 ( 97.80)	Acc@5 100.00 ( 99.97)
Epoch: [41][ 90/391]	Time  0.038 ( 0.026)	Data  0.002 ( 0.004)	Loss 1.6223e-01 (7.0020e-02)	Acc@1  96.09 ( 97.80)	Acc@5  99.22 ( 99.96)
Epoch: [41][100/391]	Time  0.018 ( 0.025)	Data  0.002 ( 0.004)	Loss 1.2195e-01 (7.2067e-02)	Acc@1  96.88 ( 97.74)	Acc@5 100.00 ( 99.96)
Epoch: [41][110/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.004)	Loss 4.5227e-02 (7.1799e-02)	Acc@1  98.44 ( 97.74)	Acc@5 100.00 ( 99.96)
Epoch: [41][120/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.004)	Loss 6.0272e-02 (7.1092e-02)	Acc@1  98.44 ( 97.76)	Acc@5 100.00 ( 99.97)
Epoch: [41][130/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.9007e-02 (7.0215e-02)	Acc@1  99.22 ( 97.79)	Acc@5 100.00 ( 99.97)
Epoch: [41][140/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.003)	Loss 9.5947e-02 (7.0312e-02)	Acc@1  97.66 ( 97.79)	Acc@5 100.00 ( 99.97)
Epoch: [41][150/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.1261e-01 (7.1241e-02)	Acc@1  96.09 ( 97.75)	Acc@5 100.00 ( 99.97)
Epoch: [41][160/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.2634e-01 (7.2543e-02)	Acc@1  96.09 ( 97.70)	Acc@5 100.00 ( 99.97)
Epoch: [41][170/391]	Time  0.028 ( 0.025)	Data  0.000 ( 0.003)	Loss 5.8411e-02 (7.2186e-02)	Acc@1  97.66 ( 97.72)	Acc@5 100.00 ( 99.97)
Epoch: [41][180/391]	Time  0.033 ( 0.025)	Data  0.001 ( 0.003)	Loss 7.9590e-02 (7.3670e-02)	Acc@1  98.44 ( 97.67)	Acc@5 100.00 ( 99.97)
Epoch: [41][190/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.6692e-02 (7.2967e-02)	Acc@1  97.66 ( 97.70)	Acc@5 100.00 ( 99.97)
Epoch: [41][200/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.3508e-02 (7.2907e-02)	Acc@1  98.44 ( 97.68)	Acc@5 100.00 ( 99.97)
Epoch: [41][210/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.8605e-02 (7.3303e-02)	Acc@1  98.44 ( 97.63)	Acc@5 100.00 ( 99.97)
Epoch: [41][220/391]	Time  0.018 ( 0.025)	Data  0.000 ( 0.003)	Loss 4.5258e-02 (7.3497e-02)	Acc@1  98.44 ( 97.62)	Acc@5 100.00 ( 99.97)
Epoch: [41][230/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.1145e-01 (7.4089e-02)	Acc@1  95.31 ( 97.61)	Acc@5 100.00 ( 99.97)
Epoch: [41][240/391]	Time  0.032 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.1340e-02 (7.3978e-02)	Acc@1  99.22 ( 97.61)	Acc@5 100.00 ( 99.97)
Epoch: [41][250/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.0754e-01 (7.4082e-02)	Acc@1  97.66 ( 97.61)	Acc@5 100.00 ( 99.97)
Epoch: [41][260/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.3203e-02 (7.4130e-02)	Acc@1  98.44 ( 97.62)	Acc@5 100.00 ( 99.96)
Epoch: [41][270/391]	Time  0.044 ( 0.025)	Data  0.006 ( 0.003)	Loss 2.0166e-01 (7.4916e-02)	Acc@1  92.19 ( 97.57)	Acc@5 100.00 ( 99.96)
Epoch: [41][280/391]	Time  0.031 ( 0.025)	Data  0.003 ( 0.002)	Loss 1.0876e-01 (7.6023e-02)	Acc@1  94.53 ( 97.52)	Acc@5 100.00 ( 99.96)
Epoch: [41][290/391]	Time  0.024 ( 0.025)	Data  0.003 ( 0.002)	Loss 5.7312e-02 (7.5978e-02)	Acc@1  99.22 ( 97.52)	Acc@5 100.00 ( 99.96)
Epoch: [41][300/391]	Time  0.017 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.2675e-02 (7.5573e-02)	Acc@1 100.00 ( 97.53)	Acc@5 100.00 ( 99.96)
Epoch: [41][310/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 8.7280e-02 (7.5358e-02)	Acc@1  96.88 ( 97.52)	Acc@5 100.00 ( 99.96)
Epoch: [41][320/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 8.2092e-02 (7.5086e-02)	Acc@1  96.88 ( 97.52)	Acc@5 100.00 ( 99.96)
Epoch: [41][330/391]	Time  0.040 ( 0.024)	Data  0.003 ( 0.002)	Loss 4.5929e-02 (7.4661e-02)	Acc@1  98.44 ( 97.53)	Acc@5 100.00 ( 99.96)
Epoch: [41][340/391]	Time  0.027 ( 0.024)	Data  0.002 ( 0.002)	Loss 1.0938e-01 (7.5110e-02)	Acc@1  95.31 ( 97.52)	Acc@5 100.00 ( 99.96)
Epoch: [41][350/391]	Time  0.025 ( 0.024)	Data  0.001 ( 0.002)	Loss 9.4238e-02 (7.5199e-02)	Acc@1  96.09 ( 97.52)	Acc@5 100.00 ( 99.96)
Epoch: [41][360/391]	Time  0.024 ( 0.024)	Data  0.001 ( 0.002)	Loss 6.4331e-02 (7.5235e-02)	Acc@1  96.88 ( 97.52)	Acc@5 100.00 ( 99.96)
Epoch: [41][370/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.2708e-01 (7.5221e-02)	Acc@1  96.09 ( 97.52)	Acc@5 100.00 ( 99.96)
Epoch: [41][380/391]	Time  0.033 ( 0.024)	Data  0.001 ( 0.002)	Loss 7.6355e-02 (7.5978e-02)	Acc@1  97.66 ( 97.50)	Acc@5 100.00 ( 99.96)
Epoch: [41][390/391]	Time  0.014 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.0223e-01 (7.6458e-02)	Acc@1  95.00 ( 97.49)	Acc@5 100.00 ( 99.96)
## e[41] optimizer.zero_grad (sum) time: 0.10512471199035645
## e[41]       loss.backward (sum) time: 2.1697163581848145
## e[41]      optimizer.step (sum) time: 0.892815113067627
## epoch[41] training(only) time: 9.626156330108643
# Switched to evaluate mode...
Test: [  0/100]	Time  0.159 ( 0.159)	Loss 2.2388e-01 (2.2388e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.012 ( 0.026)	Loss 4.1260e-01 (3.3320e-01)	Acc@1  87.00 ( 90.18)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.009 ( 0.020)	Loss 4.1504e-01 (3.5880e-01)	Acc@1  87.00 ( 89.90)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.009 ( 0.018)	Loss 3.4985e-01 (3.7570e-01)	Acc@1  90.00 ( 89.77)	Acc@5 100.00 ( 99.55)
Test: [ 40/100]	Time  0.014 ( 0.017)	Loss 4.4458e-01 (3.8948e-01)	Acc@1  88.00 ( 89.51)	Acc@5  97.00 ( 99.41)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 1.5637e-01 (3.7107e-01)	Acc@1  91.00 ( 89.82)	Acc@5 100.00 ( 99.45)
Test: [ 60/100]	Time  0.012 ( 0.015)	Loss 2.6489e-01 (3.6770e-01)	Acc@1  92.00 ( 89.85)	Acc@5 100.00 ( 99.48)
Test: [ 70/100]	Time  0.013 ( 0.015)	Loss 5.1514e-01 (3.6546e-01)	Acc@1  88.00 ( 89.96)	Acc@5 100.00 ( 99.49)
Test: [ 80/100]	Time  0.008 ( 0.015)	Loss 2.4341e-01 (3.6045e-01)	Acc@1  93.00 ( 90.05)	Acc@5 100.00 ( 99.49)
Test: [ 90/100]	Time  0.011 ( 0.015)	Loss 2.0398e-01 (3.5639e-01)	Acc@1  95.00 ( 90.04)	Acc@5 100.00 ( 99.54)
 * Acc@1 90.050 Acc@5 99.580
### epoch[41] execution time: 11.204330444335938
EPOCH 42
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [42][  0/391]	Time  0.197 ( 0.197)	Data  0.172 ( 0.172)	Loss 6.6467e-02 (6.6467e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [42][ 10/391]	Time  0.021 ( 0.040)	Data  0.001 ( 0.017)	Loss 3.0548e-02 (8.2558e-02)	Acc@1  98.44 ( 97.09)	Acc@5 100.00 ( 99.93)
Epoch: [42][ 20/391]	Time  0.018 ( 0.031)	Data  0.001 ( 0.010)	Loss 5.9967e-02 (7.2612e-02)	Acc@1  97.66 ( 97.47)	Acc@5 100.00 ( 99.96)
Epoch: [42][ 30/391]	Time  0.024 ( 0.029)	Data  0.001 ( 0.007)	Loss 5.2979e-02 (6.7338e-02)	Acc@1  99.22 ( 97.66)	Acc@5 100.00 ( 99.97)
Epoch: [42][ 40/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.006)	Loss 3.8971e-02 (6.8255e-02)	Acc@1  97.66 ( 97.56)	Acc@5 100.00 ( 99.98)
Epoch: [42][ 50/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.005)	Loss 4.1718e-02 (6.4497e-02)	Acc@1  98.44 ( 97.75)	Acc@5 100.00 ( 99.98)
Epoch: [42][ 60/391]	Time  0.028 ( 0.027)	Data  0.001 ( 0.005)	Loss 2.3499e-02 (6.4705e-02)	Acc@1  99.22 ( 97.75)	Acc@5 100.00 ( 99.97)
Epoch: [42][ 70/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 5.4779e-02 (6.5243e-02)	Acc@1  98.44 ( 97.76)	Acc@5 100.00 ( 99.98)
Epoch: [42][ 80/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.004)	Loss 9.1614e-02 (6.3609e-02)	Acc@1  97.66 ( 97.82)	Acc@5 100.00 ( 99.98)
Epoch: [42][ 90/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.004)	Loss 9.2590e-02 (6.6345e-02)	Acc@1  96.88 ( 97.77)	Acc@5 100.00 ( 99.97)
Epoch: [42][100/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.6621e-02 (6.6488e-02)	Acc@1  98.44 ( 97.78)	Acc@5 100.00 ( 99.98)
Epoch: [42][110/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 6.1584e-02 (6.8169e-02)	Acc@1  98.44 ( 97.72)	Acc@5 100.00 ( 99.97)
Epoch: [42][120/391]	Time  0.031 ( 0.025)	Data  0.000 ( 0.003)	Loss 1.9714e-01 (6.9146e-02)	Acc@1  95.31 ( 97.68)	Acc@5  99.22 ( 99.95)
Epoch: [42][130/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.3406e-02 (7.0322e-02)	Acc@1  97.66 ( 97.67)	Acc@5 100.00 ( 99.96)
Epoch: [42][140/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 1.1127e-01 (7.1698e-02)	Acc@1  96.09 ( 97.61)	Acc@5 100.00 ( 99.96)
Epoch: [42][150/391]	Time  0.039 ( 0.025)	Data  0.004 ( 0.003)	Loss 4.7729e-02 (7.2463e-02)	Acc@1  98.44 ( 97.59)	Acc@5 100.00 ( 99.96)
Epoch: [42][160/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 2.6520e-02 (7.2641e-02)	Acc@1 100.00 ( 97.57)	Acc@5 100.00 ( 99.96)
Epoch: [42][170/391]	Time  0.021 ( 0.025)	Data  0.002 ( 0.003)	Loss 7.3181e-02 (7.2493e-02)	Acc@1  96.88 ( 97.59)	Acc@5  99.22 ( 99.96)
Epoch: [42][180/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.8798e-02 (7.2016e-02)	Acc@1  97.66 ( 97.61)	Acc@5 100.00 ( 99.95)
Epoch: [42][190/391]	Time  0.018 ( 0.025)	Data  0.002 ( 0.003)	Loss 7.5500e-02 (7.2369e-02)	Acc@1  97.66 ( 97.61)	Acc@5 100.00 ( 99.96)
Epoch: [42][200/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.5698e-01 (7.2732e-02)	Acc@1  96.88 ( 97.61)	Acc@5  99.22 ( 99.95)
Epoch: [42][210/391]	Time  0.043 ( 0.025)	Data  0.006 ( 0.003)	Loss 6.5186e-02 (7.3292e-02)	Acc@1  96.88 ( 97.59)	Acc@5 100.00 ( 99.94)
Epoch: [42][220/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.9642e-02 (7.3139e-02)	Acc@1  99.22 ( 97.60)	Acc@5 100.00 ( 99.95)
Epoch: [42][230/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 5.7526e-02 (7.2869e-02)	Acc@1  96.88 ( 97.60)	Acc@5 100.00 ( 99.95)
Epoch: [42][240/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 9.8999e-02 (7.3577e-02)	Acc@1  96.88 ( 97.58)	Acc@5 100.00 ( 99.94)
Epoch: [42][250/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 5.1880e-02 (7.2980e-02)	Acc@1  97.66 ( 97.58)	Acc@5 100.00 ( 99.94)
Epoch: [42][260/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 7.9590e-02 (7.3897e-02)	Acc@1  97.66 ( 97.57)	Acc@5 100.00 ( 99.95)
Epoch: [42][270/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 6.0486e-02 (7.3471e-02)	Acc@1  96.88 ( 97.57)	Acc@5 100.00 ( 99.95)
Epoch: [42][280/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.0913e-01 (7.3135e-02)	Acc@1  96.88 ( 97.59)	Acc@5 100.00 ( 99.95)
Epoch: [42][290/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 5.7465e-02 (7.3206e-02)	Acc@1  98.44 ( 97.58)	Acc@5 100.00 ( 99.95)
Epoch: [42][300/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 8.9111e-02 (7.3399e-02)	Acc@1  96.09 ( 97.56)	Acc@5 100.00 ( 99.95)
Epoch: [42][310/391]	Time  0.033 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.0199e-01 (7.4021e-02)	Acc@1  96.88 ( 97.54)	Acc@5 100.00 ( 99.95)
Epoch: [42][320/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.0676e-02 (7.3708e-02)	Acc@1 100.00 ( 97.56)	Acc@5 100.00 ( 99.95)
Epoch: [42][330/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.6957e-02 (7.3526e-02)	Acc@1  98.44 ( 97.56)	Acc@5 100.00 ( 99.95)
Epoch: [42][340/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 7.7332e-02 (7.3910e-02)	Acc@1  97.66 ( 97.55)	Acc@5 100.00 ( 99.95)
Epoch: [42][350/391]	Time  0.018 ( 0.024)	Data  0.002 ( 0.002)	Loss 6.4575e-02 (7.3596e-02)	Acc@1  96.88 ( 97.56)	Acc@5 100.00 ( 99.95)
Epoch: [42][360/391]	Time  0.024 ( 0.024)	Data  0.002 ( 0.002)	Loss 2.6062e-02 (7.3628e-02)	Acc@1  99.22 ( 97.56)	Acc@5 100.00 ( 99.95)
Epoch: [42][370/391]	Time  0.022 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.0187e-01 (7.3534e-02)	Acc@1  94.53 ( 97.55)	Acc@5 100.00 ( 99.96)
Epoch: [42][380/391]	Time  0.022 ( 0.024)	Data  0.000 ( 0.002)	Loss 5.9814e-02 (7.3595e-02)	Acc@1  99.22 ( 97.55)	Acc@5 100.00 ( 99.96)
Epoch: [42][390/391]	Time  0.016 ( 0.024)	Data  0.000 ( 0.002)	Loss 4.6875e-02 (7.3483e-02)	Acc@1  97.50 ( 97.56)	Acc@5 100.00 ( 99.96)
## e[42] optimizer.zero_grad (sum) time: 0.10595345497131348
## e[42]       loss.backward (sum) time: 2.1977462768554688
## e[42]      optimizer.step (sum) time: 0.8961362838745117
## epoch[42] training(only) time: 9.60969352722168
# Switched to evaluate mode...
Test: [  0/100]	Time  0.165 ( 0.165)	Loss 2.5171e-01 (2.5171e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.013 ( 0.027)	Loss 3.9575e-01 (3.2225e-01)	Acc@1  89.00 ( 90.73)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.015 ( 0.020)	Loss 3.6328e-01 (3.4945e-01)	Acc@1  88.00 ( 90.10)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.016 ( 0.018)	Loss 4.0723e-01 (3.6513e-01)	Acc@1  87.00 ( 90.03)	Acc@5  99.00 ( 99.55)
Test: [ 40/100]	Time  0.008 ( 0.017)	Loss 5.1611e-01 (3.8465e-01)	Acc@1  87.00 ( 89.83)	Acc@5  98.00 ( 99.49)
Test: [ 50/100]	Time  0.014 ( 0.016)	Loss 2.1289e-01 (3.7099e-01)	Acc@1  93.00 ( 90.00)	Acc@5 100.00 ( 99.51)
Test: [ 60/100]	Time  0.008 ( 0.015)	Loss 2.6758e-01 (3.7074e-01)	Acc@1  89.00 ( 89.90)	Acc@5 100.00 ( 99.54)
Test: [ 70/100]	Time  0.008 ( 0.015)	Loss 5.7715e-01 (3.6789e-01)	Acc@1  87.00 ( 89.97)	Acc@5 100.00 ( 99.55)
Test: [ 80/100]	Time  0.008 ( 0.015)	Loss 2.3242e-01 (3.6193e-01)	Acc@1  91.00 ( 89.98)	Acc@5 100.00 ( 99.56)
Test: [ 90/100]	Time  0.013 ( 0.015)	Loss 2.0691e-01 (3.5708e-01)	Acc@1  93.00 ( 90.00)	Acc@5 100.00 ( 99.59)
 * Acc@1 90.030 Acc@5 99.630
### epoch[42] execution time: 11.208881855010986
EPOCH 43
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [43][  0/391]	Time  0.190 ( 0.190)	Data  0.166 ( 0.166)	Loss 8.8501e-02 (8.8501e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [43][ 10/391]	Time  0.029 ( 0.038)	Data  0.002 ( 0.017)	Loss 9.2468e-02 (7.0447e-02)	Acc@1  97.66 ( 98.15)	Acc@5 100.00 (100.00)
Epoch: [43][ 20/391]	Time  0.043 ( 0.031)	Data  0.010 ( 0.010)	Loss 1.0516e-01 (6.8431e-02)	Acc@1  96.88 ( 98.07)	Acc@5 100.00 ( 99.96)
Epoch: [43][ 30/391]	Time  0.018 ( 0.029)	Data  0.001 ( 0.007)	Loss 1.2794e-02 (6.7917e-02)	Acc@1 100.00 ( 98.03)	Acc@5 100.00 ( 99.95)
Epoch: [43][ 40/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.006)	Loss 3.9642e-02 (7.4522e-02)	Acc@1  98.44 ( 97.83)	Acc@5 100.00 ( 99.94)
Epoch: [43][ 50/391]	Time  0.023 ( 0.027)	Data  0.004 ( 0.005)	Loss 5.2368e-02 (7.8286e-02)	Acc@1  98.44 ( 97.66)	Acc@5 100.00 ( 99.94)
Epoch: [43][ 60/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.005)	Loss 8.4656e-02 (7.9415e-02)	Acc@1  96.88 ( 97.54)	Acc@5 100.00 ( 99.95)
Epoch: [43][ 70/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.004)	Loss 9.0027e-02 (7.8427e-02)	Acc@1  98.44 ( 97.57)	Acc@5 100.00 ( 99.96)
Epoch: [43][ 80/391]	Time  0.043 ( 0.026)	Data  0.005 ( 0.004)	Loss 5.2155e-02 (7.8550e-02)	Acc@1  98.44 ( 97.60)	Acc@5 100.00 ( 99.95)
Epoch: [43][ 90/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.0236e-01 (7.7764e-02)	Acc@1  97.66 ( 97.62)	Acc@5 100.00 ( 99.96)
Epoch: [43][100/391]	Time  0.039 ( 0.025)	Data  0.002 ( 0.003)	Loss 3.3661e-02 (7.6430e-02)	Acc@1  99.22 ( 97.64)	Acc@5 100.00 ( 99.95)
Epoch: [43][110/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.0629e-02 (7.5549e-02)	Acc@1  98.44 ( 97.64)	Acc@5 100.00 ( 99.96)
Epoch: [43][120/391]	Time  0.029 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.7822e-01 (7.6199e-02)	Acc@1  93.75 ( 97.57)	Acc@5 100.00 ( 99.96)
Epoch: [43][130/391]	Time  0.045 ( 0.025)	Data  0.005 ( 0.003)	Loss 5.2063e-02 (7.5753e-02)	Acc@1  98.44 ( 97.58)	Acc@5 100.00 ( 99.96)
Epoch: [43][140/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.4351e-02 (7.6225e-02)	Acc@1  96.09 ( 97.56)	Acc@5 100.00 ( 99.96)
Epoch: [43][150/391]	Time  0.029 ( 0.025)	Data  0.002 ( 0.003)	Loss 4.0497e-02 (7.4718e-02)	Acc@1  98.44 ( 97.57)	Acc@5 100.00 ( 99.96)
Epoch: [43][160/391]	Time  0.029 ( 0.025)	Data  0.000 ( 0.003)	Loss 2.4185e-02 (7.4860e-02)	Acc@1  99.22 ( 97.58)	Acc@5 100.00 ( 99.95)
Epoch: [43][170/391]	Time  0.029 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.2225e-02 (7.5082e-02)	Acc@1  96.88 ( 97.53)	Acc@5 100.00 ( 99.95)
Epoch: [43][180/391]	Time  0.041 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.8867e-02 (7.4041e-02)	Acc@1  96.88 ( 97.56)	Acc@5 100.00 ( 99.96)
Epoch: [43][190/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.3142e-02 (7.4314e-02)	Acc@1  99.22 ( 97.55)	Acc@5 100.00 ( 99.96)
Epoch: [43][200/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 8.4991e-03 (7.3739e-02)	Acc@1 100.00 ( 97.58)	Acc@5 100.00 ( 99.95)
Epoch: [43][210/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.7638e-02 (7.3777e-02)	Acc@1  98.44 ( 97.58)	Acc@5 100.00 ( 99.95)
Epoch: [43][220/391]	Time  0.030 ( 0.025)	Data  0.005 ( 0.003)	Loss 1.3867e-01 (7.3895e-02)	Acc@1  93.75 ( 97.55)	Acc@5 100.00 ( 99.95)
Epoch: [43][230/391]	Time  0.039 ( 0.025)	Data  0.002 ( 0.002)	Loss 1.5906e-01 (7.3759e-02)	Acc@1  95.31 ( 97.53)	Acc@5 100.00 ( 99.95)
Epoch: [43][240/391]	Time  0.027 ( 0.025)	Data  0.001 ( 0.002)	Loss 9.0271e-02 (7.3712e-02)	Acc@1  96.88 ( 97.53)	Acc@5 100.00 ( 99.95)
Epoch: [43][250/391]	Time  0.019 ( 0.024)	Data  0.002 ( 0.002)	Loss 5.0354e-02 (7.3991e-02)	Acc@1  98.44 ( 97.53)	Acc@5 100.00 ( 99.96)
Epoch: [43][260/391]	Time  0.023 ( 0.025)	Data  0.002 ( 0.002)	Loss 2.1622e-02 (7.3431e-02)	Acc@1  99.22 ( 97.55)	Acc@5 100.00 ( 99.96)
Epoch: [43][270/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 8.2825e-02 (7.3541e-02)	Acc@1  96.09 ( 97.54)	Acc@5 100.00 ( 99.96)
Epoch: [43][280/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 9.1309e-02 (7.3689e-02)	Acc@1  97.66 ( 97.53)	Acc@5 100.00 ( 99.96)
Epoch: [43][290/391]	Time  0.021 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.2354e-01 (7.3732e-02)	Acc@1  96.09 ( 97.53)	Acc@5 100.00 ( 99.96)
Epoch: [43][300/391]	Time  0.035 ( 0.024)	Data  0.001 ( 0.002)	Loss 4.2328e-02 (7.3386e-02)	Acc@1  99.22 ( 97.55)	Acc@5 100.00 ( 99.96)
Epoch: [43][310/391]	Time  0.019 ( 0.024)	Data  0.001 ( 0.002)	Loss 7.1716e-02 (7.3776e-02)	Acc@1  97.66 ( 97.54)	Acc@5 100.00 ( 99.96)
Epoch: [43][320/391]	Time  0.021 ( 0.024)	Data  0.004 ( 0.002)	Loss 6.8237e-02 (7.3472e-02)	Acc@1  96.88 ( 97.55)	Acc@5 100.00 ( 99.96)
Epoch: [43][330/391]	Time  0.022 ( 0.024)	Data  0.000 ( 0.002)	Loss 3.0930e-02 (7.3160e-02)	Acc@1  99.22 ( 97.55)	Acc@5 100.00 ( 99.96)
Epoch: [43][340/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 6.9214e-02 (7.2836e-02)	Acc@1  96.88 ( 97.57)	Acc@5 100.00 ( 99.96)
Epoch: [43][350/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.4270e-01 (7.2803e-02)	Acc@1  95.31 ( 97.55)	Acc@5 100.00 ( 99.96)
Epoch: [43][360/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 4.2664e-02 (7.2553e-02)	Acc@1  98.44 ( 97.56)	Acc@5 100.00 ( 99.97)
Epoch: [43][370/391]	Time  0.021 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.3928e-01 (7.2845e-02)	Acc@1  95.31 ( 97.56)	Acc@5 100.00 ( 99.97)
Epoch: [43][380/391]	Time  0.021 ( 0.024)	Data  0.002 ( 0.002)	Loss 1.9458e-01 (7.2824e-02)	Acc@1  94.53 ( 97.56)	Acc@5 100.00 ( 99.97)
Epoch: [43][390/391]	Time  0.015 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.6685e-01 (7.2908e-02)	Acc@1  95.00 ( 97.57)	Acc@5 100.00 ( 99.96)
## e[43] optimizer.zero_grad (sum) time: 0.10518789291381836
## e[43]       loss.backward (sum) time: 2.160407066345215
## e[43]      optimizer.step (sum) time: 0.8843288421630859
## epoch[43] training(only) time: 9.65499472618103
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 2.4036e-01 (2.4036e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.020 ( 0.027)	Loss 3.6523e-01 (3.2414e-01)	Acc@1  88.00 ( 90.55)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.020 ( 0.021)	Loss 4.1235e-01 (3.5864e-01)	Acc@1  89.00 ( 90.24)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.018 ( 0.018)	Loss 3.9893e-01 (3.7712e-01)	Acc@1  89.00 ( 90.06)	Acc@5  98.00 ( 99.52)
Test: [ 40/100]	Time  0.007 ( 0.017)	Loss 4.8438e-01 (3.9673e-01)	Acc@1  88.00 ( 89.66)	Acc@5  97.00 ( 99.41)
Test: [ 50/100]	Time  0.012 ( 0.016)	Loss 2.0349e-01 (3.8127e-01)	Acc@1  92.00 ( 89.96)	Acc@5 100.00 ( 99.45)
Test: [ 60/100]	Time  0.009 ( 0.016)	Loss 2.3486e-01 (3.7774e-01)	Acc@1  94.00 ( 90.02)	Acc@5 100.00 ( 99.49)
Test: [ 70/100]	Time  0.016 ( 0.015)	Loss 5.3516e-01 (3.7218e-01)	Acc@1  90.00 ( 90.15)	Acc@5  99.00 ( 99.52)
Test: [ 80/100]	Time  0.019 ( 0.015)	Loss 2.6392e-01 (3.6906e-01)	Acc@1  89.00 ( 90.11)	Acc@5 100.00 ( 99.53)
Test: [ 90/100]	Time  0.008 ( 0.015)	Loss 2.6685e-01 (3.6479e-01)	Acc@1  92.00 ( 90.10)	Acc@5 100.00 ( 99.57)
 * Acc@1 90.090 Acc@5 99.600
### epoch[43] execution time: 11.230185508728027
EPOCH 44
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [44][  0/391]	Time  0.195 ( 0.195)	Data  0.175 ( 0.175)	Loss 1.2366e-01 (1.2366e-01)	Acc@1  97.66 ( 97.66)	Acc@5  99.22 ( 99.22)
Epoch: [44][ 10/391]	Time  0.019 ( 0.039)	Data  0.001 ( 0.017)	Loss 1.4624e-01 (1.0817e-01)	Acc@1  96.09 ( 96.73)	Acc@5  99.22 ( 99.86)
Epoch: [44][ 20/391]	Time  0.021 ( 0.032)	Data  0.001 ( 0.010)	Loss 6.6956e-02 (9.0687e-02)	Acc@1  96.88 ( 96.95)	Acc@5 100.00 ( 99.93)
Epoch: [44][ 30/391]	Time  0.018 ( 0.029)	Data  0.001 ( 0.007)	Loss 3.0838e-02 (8.1328e-02)	Acc@1  99.22 ( 97.28)	Acc@5 100.00 ( 99.95)
Epoch: [44][ 40/391]	Time  0.026 ( 0.028)	Data  0.002 ( 0.006)	Loss 5.4932e-02 (7.8227e-02)	Acc@1  97.66 ( 97.43)	Acc@5 100.00 ( 99.94)
Epoch: [44][ 50/391]	Time  0.016 ( 0.027)	Data  0.001 ( 0.005)	Loss 6.3721e-02 (7.9216e-02)	Acc@1  96.09 ( 97.27)	Acc@5 100.00 ( 99.95)
Epoch: [44][ 60/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 3.9429e-02 (7.6040e-02)	Acc@1  98.44 ( 97.44)	Acc@5 100.00 ( 99.96)
Epoch: [44][ 70/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.004)	Loss 4.9042e-02 (7.3019e-02)	Acc@1  98.44 ( 97.57)	Acc@5 100.00 ( 99.97)
Epoch: [44][ 80/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.004)	Loss 8.2642e-02 (7.2878e-02)	Acc@1  97.66 ( 97.58)	Acc@5 100.00 ( 99.97)
Epoch: [44][ 90/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.004)	Loss 6.3110e-02 (7.1300e-02)	Acc@1  97.66 ( 97.68)	Acc@5 100.00 ( 99.97)
Epoch: [44][100/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.2643e-02 (6.9923e-02)	Acc@1  98.44 ( 97.71)	Acc@5 100.00 ( 99.98)
Epoch: [44][110/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.7725e-02 (6.7815e-02)	Acc@1  99.22 ( 97.81)	Acc@5 100.00 ( 99.97)
Epoch: [44][120/391]	Time  0.030 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.0773e-01 (6.7828e-02)	Acc@1  96.88 ( 97.82)	Acc@5 100.00 ( 99.97)
Epoch: [44][130/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 3.3447e-02 (6.7787e-02)	Acc@1  99.22 ( 97.79)	Acc@5 100.00 ( 99.98)
Epoch: [44][140/391]	Time  0.040 ( 0.025)	Data  0.004 ( 0.003)	Loss 5.8746e-02 (6.6980e-02)	Acc@1  99.22 ( 97.82)	Acc@5 100.00 ( 99.98)
Epoch: [44][150/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.9642e-02 (6.7150e-02)	Acc@1  98.44 ( 97.81)	Acc@5 100.00 ( 99.97)
Epoch: [44][160/391]	Time  0.036 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.4534e-02 (6.6564e-02)	Acc@1  96.88 ( 97.82)	Acc@5 100.00 ( 99.98)
Epoch: [44][170/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 5.5054e-02 (6.5174e-02)	Acc@1  98.44 ( 97.88)	Acc@5 100.00 ( 99.97)
Epoch: [44][180/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 7.1350e-02 (6.5118e-02)	Acc@1  98.44 ( 97.89)	Acc@5 100.00 ( 99.97)
Epoch: [44][190/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 7.7209e-02 (6.4537e-02)	Acc@1  95.31 ( 97.90)	Acc@5 100.00 ( 99.98)
Epoch: [44][200/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.6987e-02 (6.4552e-02)	Acc@1  98.44 ( 97.92)	Acc@5 100.00 ( 99.98)
Epoch: [44][210/391]	Time  0.030 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.8523e-02 (6.4741e-02)	Acc@1  98.44 ( 97.91)	Acc@5 100.00 ( 99.97)
Epoch: [44][220/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.5187e-02 (6.4870e-02)	Acc@1  98.44 ( 97.90)	Acc@5 100.00 ( 99.97)
Epoch: [44][230/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.5675e-02 (6.4866e-02)	Acc@1  98.44 ( 97.91)	Acc@5 100.00 ( 99.97)
Epoch: [44][240/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.5084e-02 (6.4154e-02)	Acc@1  98.44 ( 97.94)	Acc@5 100.00 ( 99.97)
Epoch: [44][250/391]	Time  0.018 ( 0.025)	Data  0.002 ( 0.003)	Loss 2.6840e-02 (6.3521e-02)	Acc@1  98.44 ( 97.95)	Acc@5 100.00 ( 99.97)
Epoch: [44][260/391]	Time  0.028 ( 0.025)	Data  0.001 ( 0.003)	Loss 9.7961e-02 (6.3706e-02)	Acc@1  97.66 ( 97.95)	Acc@5 100.00 ( 99.97)
Epoch: [44][270/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.9744e-02 (6.3619e-02)	Acc@1  97.66 ( 97.95)	Acc@5 100.00 ( 99.97)
Epoch: [44][280/391]	Time  0.041 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.2250e-01 (6.4209e-02)	Acc@1  96.88 ( 97.94)	Acc@5 100.00 ( 99.97)
Epoch: [44][290/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.4050e-01 (6.4709e-02)	Acc@1  97.66 ( 97.94)	Acc@5  99.22 ( 99.97)
Epoch: [44][300/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.0314e-02 (6.4158e-02)	Acc@1  98.44 ( 97.96)	Acc@5 100.00 ( 99.97)
Epoch: [44][310/391]	Time  0.018 ( 0.025)	Data  0.002 ( 0.002)	Loss 5.0964e-02 (6.4175e-02)	Acc@1  97.66 ( 97.94)	Acc@5 100.00 ( 99.97)
Epoch: [44][320/391]	Time  0.031 ( 0.025)	Data  0.000 ( 0.002)	Loss 2.2430e-02 (6.4620e-02)	Acc@1  99.22 ( 97.94)	Acc@5 100.00 ( 99.97)
Epoch: [44][330/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 5.0934e-02 (6.4998e-02)	Acc@1  98.44 ( 97.93)	Acc@5 100.00 ( 99.97)
Epoch: [44][340/391]	Time  0.024 ( 0.025)	Data  0.002 ( 0.002)	Loss 8.3618e-02 (6.5096e-02)	Acc@1  98.44 ( 97.94)	Acc@5 100.00 ( 99.97)
Epoch: [44][350/391]	Time  0.038 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.0721e-02 (6.4796e-02)	Acc@1 100.00 ( 97.95)	Acc@5 100.00 ( 99.97)
Epoch: [44][360/391]	Time  0.027 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.0925e-01 (6.4818e-02)	Acc@1  96.09 ( 97.94)	Acc@5 100.00 ( 99.97)
Epoch: [44][370/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.5198e-01 (6.4961e-02)	Acc@1  96.09 ( 97.94)	Acc@5 100.00 ( 99.97)
Epoch: [44][380/391]	Time  0.026 ( 0.025)	Data  0.002 ( 0.002)	Loss 8.8928e-02 (6.5128e-02)	Acc@1  96.09 ( 97.94)	Acc@5 100.00 ( 99.97)
Epoch: [44][390/391]	Time  0.016 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.9337e-02 (6.5123e-02)	Acc@1  97.50 ( 97.94)	Acc@5 100.00 ( 99.97)
## e[44] optimizer.zero_grad (sum) time: 0.10525155067443848
## e[44]       loss.backward (sum) time: 2.1448771953582764
## e[44]      optimizer.step (sum) time: 0.8766210079193115
## epoch[44] training(only) time: 9.694823741912842
# Switched to evaluate mode...
Test: [  0/100]	Time  0.168 ( 0.168)	Loss 2.7539e-01 (2.7539e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.020 ( 0.028)	Loss 3.7500e-01 (3.4417e-01)	Acc@1  89.00 ( 90.18)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.030 ( 0.021)	Loss 4.3335e-01 (3.6745e-01)	Acc@1  85.00 ( 89.81)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.008 ( 0.018)	Loss 4.4678e-01 (3.8583e-01)	Acc@1  89.00 ( 89.84)	Acc@5  99.00 ( 99.55)
Test: [ 40/100]	Time  0.015 ( 0.017)	Loss 4.9292e-01 (4.0389e-01)	Acc@1  90.00 ( 89.59)	Acc@5  97.00 ( 99.46)
Test: [ 50/100]	Time  0.008 ( 0.016)	Loss 2.1899e-01 (3.8574e-01)	Acc@1  91.00 ( 89.90)	Acc@5 100.00 ( 99.49)
Test: [ 60/100]	Time  0.010 ( 0.016)	Loss 2.7905e-01 (3.8314e-01)	Acc@1  91.00 ( 89.92)	Acc@5 100.00 ( 99.52)
Test: [ 70/100]	Time  0.017 ( 0.015)	Loss 5.9863e-01 (3.8182e-01)	Acc@1  90.00 ( 90.04)	Acc@5  99.00 ( 99.54)
Test: [ 80/100]	Time  0.012 ( 0.015)	Loss 2.2998e-01 (3.7628e-01)	Acc@1  91.00 ( 90.00)	Acc@5 100.00 ( 99.53)
Test: [ 90/100]	Time  0.012 ( 0.015)	Loss 2.0618e-01 (3.7240e-01)	Acc@1  93.00 ( 90.03)	Acc@5 100.00 ( 99.57)
 * Acc@1 90.100 Acc@5 99.600
### epoch[44] execution time: 11.291432619094849
EPOCH 45
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [45][  0/391]	Time  0.196 ( 0.196)	Data  0.167 ( 0.167)	Loss 4.6997e-02 (4.6997e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [45][ 10/391]	Time  0.019 ( 0.039)	Data  0.001 ( 0.017)	Loss 3.9825e-02 (3.2446e-02)	Acc@1  98.44 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [45][ 20/391]	Time  0.044 ( 0.032)	Data  0.006 ( 0.010)	Loss 6.9946e-02 (5.4353e-02)	Acc@1  96.88 ( 98.40)	Acc@5 100.00 ( 99.96)
Epoch: [45][ 30/391]	Time  0.027 ( 0.030)	Data  0.001 ( 0.007)	Loss 1.0132e-01 (5.5949e-02)	Acc@1  96.09 ( 98.34)	Acc@5 100.00 ( 99.97)
Epoch: [45][ 40/391]	Time  0.017 ( 0.028)	Data  0.001 ( 0.006)	Loss 7.7637e-02 (5.8616e-02)	Acc@1  97.66 ( 98.23)	Acc@5 100.00 ( 99.96)
Epoch: [45][ 50/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.005)	Loss 1.3464e-01 (5.9682e-02)	Acc@1  96.09 ( 98.09)	Acc@5  99.22 ( 99.95)
Epoch: [45][ 60/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.004)	Loss 4.1321e-02 (5.7931e-02)	Acc@1  99.22 ( 98.18)	Acc@5 100.00 ( 99.96)
Epoch: [45][ 70/391]	Time  0.028 ( 0.027)	Data  0.001 ( 0.004)	Loss 7.9895e-02 (5.7997e-02)	Acc@1  97.66 ( 98.13)	Acc@5 100.00 ( 99.97)
Epoch: [45][ 80/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 3.9673e-02 (5.7678e-02)	Acc@1  99.22 ( 98.13)	Acc@5 100.00 ( 99.97)
Epoch: [45][ 90/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 2.7328e-02 (5.5602e-02)	Acc@1  99.22 ( 98.18)	Acc@5 100.00 ( 99.97)
Epoch: [45][100/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2672e-02 (5.5454e-02)	Acc@1 100.00 ( 98.18)	Acc@5 100.00 ( 99.98)
Epoch: [45][110/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 2.4780e-02 (5.5706e-02)	Acc@1  99.22 ( 98.18)	Acc@5 100.00 ( 99.97)
Epoch: [45][120/391]	Time  0.036 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.4185e-02 (5.5915e-02)	Acc@1 100.00 ( 98.17)	Acc@5 100.00 ( 99.97)
Epoch: [45][130/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 1.5823e-02 (5.5617e-02)	Acc@1 100.00 ( 98.18)	Acc@5 100.00 ( 99.97)
Epoch: [45][140/391]	Time  0.031 ( 0.025)	Data  0.000 ( 0.003)	Loss 5.6183e-02 (5.7220e-02)	Acc@1  97.66 ( 98.11)	Acc@5 100.00 ( 99.97)
Epoch: [45][150/391]	Time  0.017 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.4678e-02 (5.7013e-02)	Acc@1  98.44 ( 98.10)	Acc@5 100.00 ( 99.97)
Epoch: [45][160/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.5881e-01 (5.8045e-02)	Acc@1  95.31 ( 98.06)	Acc@5 100.00 ( 99.98)
Epoch: [45][170/391]	Time  0.025 ( 0.025)	Data  0.002 ( 0.003)	Loss 1.3147e-01 (5.8510e-02)	Acc@1  96.88 ( 98.07)	Acc@5 100.00 ( 99.97)
Epoch: [45][180/391]	Time  0.036 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.5736e-02 (5.8963e-02)	Acc@1  99.22 ( 98.07)	Acc@5 100.00 ( 99.97)
Epoch: [45][190/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.8726e-02 (5.8767e-02)	Acc@1  96.88 ( 98.07)	Acc@5 100.00 ( 99.97)
Epoch: [45][200/391]	Time  0.035 ( 0.025)	Data  0.000 ( 0.003)	Loss 8.3374e-02 (5.9884e-02)	Acc@1  97.66 ( 98.04)	Acc@5  99.22 ( 99.96)
Epoch: [45][210/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.7140e-02 (6.0277e-02)	Acc@1  99.22 ( 98.02)	Acc@5 100.00 ( 99.96)
Epoch: [45][220/391]	Time  0.037 ( 0.025)	Data  0.005 ( 0.003)	Loss 1.4969e-02 (6.0286e-02)	Acc@1  99.22 ( 98.01)	Acc@5 100.00 ( 99.96)
Epoch: [45][230/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 9.6436e-02 (6.0820e-02)	Acc@1  96.88 ( 97.99)	Acc@5 100.00 ( 99.96)
Epoch: [45][240/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 7.6050e-02 (6.0757e-02)	Acc@1  96.88 ( 97.99)	Acc@5 100.00 ( 99.96)
Epoch: [45][250/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.7058e-02 (6.1413e-02)	Acc@1  98.44 ( 97.96)	Acc@5 100.00 ( 99.96)
Epoch: [45][260/391]	Time  0.031 ( 0.025)	Data  0.002 ( 0.002)	Loss 1.2335e-01 (6.1515e-02)	Acc@1  96.88 ( 97.95)	Acc@5 100.00 ( 99.96)
Epoch: [45][270/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 6.2195e-02 (6.1688e-02)	Acc@1  96.09 ( 97.95)	Acc@5 100.00 ( 99.96)
Epoch: [45][280/391]	Time  0.040 ( 0.025)	Data  0.003 ( 0.002)	Loss 7.4524e-02 (6.1314e-02)	Acc@1  98.44 ( 97.96)	Acc@5 100.00 ( 99.96)
Epoch: [45][290/391]	Time  0.040 ( 0.025)	Data  0.001 ( 0.002)	Loss 6.1951e-02 (6.1596e-02)	Acc@1  97.66 ( 97.96)	Acc@5 100.00 ( 99.96)
Epoch: [45][300/391]	Time  0.027 ( 0.025)	Data  0.001 ( 0.002)	Loss 7.1960e-02 (6.1905e-02)	Acc@1  97.66 ( 97.95)	Acc@5 100.00 ( 99.96)
Epoch: [45][310/391]	Time  0.025 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.9683e-02 (6.1257e-02)	Acc@1  97.66 ( 97.97)	Acc@5 100.00 ( 99.96)
Epoch: [45][320/391]	Time  0.020 ( 0.025)	Data  0.002 ( 0.002)	Loss 2.9480e-02 (6.1512e-02)	Acc@1  99.22 ( 97.96)	Acc@5 100.00 ( 99.96)
Epoch: [45][330/391]	Time  0.016 ( 0.025)	Data  0.001 ( 0.002)	Loss 6.8604e-02 (6.1859e-02)	Acc@1  97.66 ( 97.95)	Acc@5 100.00 ( 99.96)
Epoch: [45][340/391]	Time  0.024 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.0510e-01 (6.2001e-02)	Acc@1  97.66 ( 97.95)	Acc@5 100.00 ( 99.96)
Epoch: [45][350/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.002)	Loss 5.2856e-02 (6.1602e-02)	Acc@1  98.44 ( 97.96)	Acc@5 100.00 ( 99.96)
Epoch: [45][360/391]	Time  0.016 ( 0.025)	Data  0.001 ( 0.002)	Loss 9.3384e-02 (6.2194e-02)	Acc@1  97.66 ( 97.94)	Acc@5 100.00 ( 99.96)
Epoch: [45][370/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.002)	Loss 5.5176e-02 (6.2758e-02)	Acc@1  98.44 ( 97.91)	Acc@5 100.00 ( 99.96)
Epoch: [45][380/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.0289e-02 (6.2616e-02)	Acc@1  98.44 ( 97.92)	Acc@5 100.00 ( 99.96)
Epoch: [45][390/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.0803e-01 (6.2553e-02)	Acc@1  97.50 ( 97.92)	Acc@5 100.00 ( 99.96)
## e[45] optimizer.zero_grad (sum) time: 0.10561418533325195
## e[45]       loss.backward (sum) time: 2.1357593536376953
## e[45]      optimizer.step (sum) time: 0.8866667747497559
## epoch[45] training(only) time: 9.726993799209595
# Switched to evaluate mode...
Test: [  0/100]	Time  0.162 ( 0.162)	Loss 2.4927e-01 (2.4927e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.011 ( 0.026)	Loss 3.8208e-01 (3.4398e-01)	Acc@1  91.00 ( 90.91)	Acc@5  99.00 ( 99.55)
Test: [ 20/100]	Time  0.008 ( 0.019)	Loss 4.7705e-01 (3.6605e-01)	Acc@1  87.00 ( 90.57)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.008 ( 0.017)	Loss 4.6533e-01 (3.8683e-01)	Acc@1  88.00 ( 90.45)	Acc@5 100.00 ( 99.55)
Test: [ 40/100]	Time  0.017 ( 0.016)	Loss 5.6543e-01 (4.0759e-01)	Acc@1  87.00 ( 90.02)	Acc@5  99.00 ( 99.49)
Test: [ 50/100]	Time  0.028 ( 0.016)	Loss 1.8347e-01 (3.8835e-01)	Acc@1  92.00 ( 90.27)	Acc@5 100.00 ( 99.47)
Test: [ 60/100]	Time  0.007 ( 0.015)	Loss 2.5098e-01 (3.8539e-01)	Acc@1  92.00 ( 90.23)	Acc@5 100.00 ( 99.51)
Test: [ 70/100]	Time  0.018 ( 0.015)	Loss 6.8115e-01 (3.8095e-01)	Acc@1  89.00 ( 90.39)	Acc@5  99.00 ( 99.52)
Test: [ 80/100]	Time  0.008 ( 0.015)	Loss 2.1558e-01 (3.7592e-01)	Acc@1  92.00 ( 90.44)	Acc@5 100.00 ( 99.51)
Test: [ 90/100]	Time  0.026 ( 0.015)	Loss 2.0386e-01 (3.7185e-01)	Acc@1  92.00 ( 90.41)	Acc@5 100.00 ( 99.55)
 * Acc@1 90.360 Acc@5 99.590
### epoch[45] execution time: 11.292909383773804
EPOCH 46
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [46][  0/391]	Time  0.192 ( 0.192)	Data  0.170 ( 0.170)	Loss 7.7148e-02 (7.7148e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [46][ 10/391]	Time  0.023 ( 0.039)	Data  0.001 ( 0.017)	Loss 1.0144e-01 (6.5376e-02)	Acc@1  96.09 ( 97.73)	Acc@5 100.00 (100.00)
Epoch: [46][ 20/391]	Time  0.020 ( 0.032)	Data  0.001 ( 0.010)	Loss 4.5319e-02 (5.9076e-02)	Acc@1  97.66 ( 97.84)	Acc@5 100.00 (100.00)
Epoch: [46][ 30/391]	Time  0.030 ( 0.030)	Data  0.001 ( 0.007)	Loss 4.4128e-02 (5.7045e-02)	Acc@1  99.22 ( 98.01)	Acc@5 100.00 ( 99.97)
Epoch: [46][ 40/391]	Time  0.018 ( 0.028)	Data  0.001 ( 0.006)	Loss 1.0797e-01 (6.1477e-02)	Acc@1  95.31 ( 97.92)	Acc@5 100.00 ( 99.96)
Epoch: [46][ 50/391]	Time  0.036 ( 0.027)	Data  0.001 ( 0.005)	Loss 1.0651e-01 (6.3100e-02)	Acc@1  96.09 ( 97.92)	Acc@5 100.00 ( 99.97)
Epoch: [46][ 60/391]	Time  0.022 ( 0.026)	Data  0.005 ( 0.005)	Loss 3.2379e-02 (6.1768e-02)	Acc@1  99.22 ( 97.94)	Acc@5 100.00 ( 99.97)
Epoch: [46][ 70/391]	Time  0.035 ( 0.026)	Data  0.005 ( 0.004)	Loss 3.3875e-02 (6.0802e-02)	Acc@1  99.22 ( 97.96)	Acc@5 100.00 ( 99.98)
Epoch: [46][ 80/391]	Time  0.039 ( 0.026)	Data  0.001 ( 0.004)	Loss 8.4106e-02 (6.1156e-02)	Acc@1  97.66 ( 97.93)	Acc@5 100.00 ( 99.97)
Epoch: [46][ 90/391]	Time  0.037 ( 0.026)	Data  0.004 ( 0.004)	Loss 2.9434e-02 (6.1040e-02)	Acc@1  99.22 ( 97.93)	Acc@5 100.00 ( 99.97)
Epoch: [46][100/391]	Time  0.038 ( 0.026)	Data  0.005 ( 0.004)	Loss 4.9164e-02 (5.9565e-02)	Acc@1  98.44 ( 98.00)	Acc@5 100.00 ( 99.98)
Epoch: [46][110/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.3081e-02 (5.8597e-02)	Acc@1  99.22 ( 98.04)	Acc@5 100.00 ( 99.97)
Epoch: [46][120/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.3660e-02 (5.9370e-02)	Acc@1  98.44 ( 98.05)	Acc@5 100.00 ( 99.97)
Epoch: [46][130/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 7.6416e-02 (6.0186e-02)	Acc@1  97.66 ( 98.01)	Acc@5 100.00 ( 99.97)
Epoch: [46][140/391]	Time  0.023 ( 0.025)	Data  0.000 ( 0.003)	Loss 1.2256e-01 (6.1096e-02)	Acc@1  96.09 ( 98.01)	Acc@5  99.22 ( 99.96)
Epoch: [46][150/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.7903e-02 (6.2102e-02)	Acc@1  98.44 ( 97.95)	Acc@5 100.00 ( 99.96)
Epoch: [46][160/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.6731e-02 (6.2047e-02)	Acc@1  96.88 ( 97.95)	Acc@5 100.00 ( 99.97)
Epoch: [46][170/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.7556e-02 (6.1575e-02)	Acc@1  96.88 ( 97.96)	Acc@5 100.00 ( 99.97)
Epoch: [46][180/391]	Time  0.046 ( 0.025)	Data  0.006 ( 0.003)	Loss 7.0496e-02 (6.1568e-02)	Acc@1  96.88 ( 97.97)	Acc@5 100.00 ( 99.97)
Epoch: [46][190/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.6101e-02 (6.1826e-02)	Acc@1  96.88 ( 97.95)	Acc@5 100.00 ( 99.97)
Epoch: [46][200/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 9.0866e-03 (6.1078e-02)	Acc@1 100.00 ( 97.97)	Acc@5 100.00 ( 99.97)
Epoch: [46][210/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.7903e-02 (6.0911e-02)	Acc@1  98.44 ( 97.98)	Acc@5 100.00 ( 99.97)
Epoch: [46][220/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.1482e-02 (6.1159e-02)	Acc@1  96.09 ( 97.97)	Acc@5 100.00 ( 99.97)
Epoch: [46][230/391]	Time  0.036 ( 0.025)	Data  0.003 ( 0.003)	Loss 1.4648e-01 (6.1246e-02)	Acc@1  95.31 ( 97.96)	Acc@5  99.22 ( 99.97)
Epoch: [46][240/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.2827e-02 (6.1718e-02)	Acc@1  99.22 ( 97.95)	Acc@5 100.00 ( 99.96)
Epoch: [46][250/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.1066e-02 (6.1813e-02)	Acc@1  97.66 ( 97.96)	Acc@5 100.00 ( 99.96)
Epoch: [46][260/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 7.3914e-02 (6.2398e-02)	Acc@1  96.88 ( 97.94)	Acc@5 100.00 ( 99.96)
Epoch: [46][270/391]	Time  0.034 ( 0.025)	Data  0.006 ( 0.003)	Loss 1.0339e-01 (6.2414e-02)	Acc@1  96.88 ( 97.94)	Acc@5 100.00 ( 99.96)
Epoch: [46][280/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.2684e-02 (6.2687e-02)	Acc@1  99.22 ( 97.94)	Acc@5 100.00 ( 99.96)
Epoch: [46][290/391]	Time  0.030 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.0815e-01 (6.3380e-02)	Acc@1  96.88 ( 97.92)	Acc@5 100.00 ( 99.97)
Epoch: [46][300/391]	Time  0.035 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.8726e-02 (6.3040e-02)	Acc@1  97.66 ( 97.94)	Acc@5 100.00 ( 99.97)
Epoch: [46][310/391]	Time  0.025 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.1847e-01 (6.2866e-02)	Acc@1  96.88 ( 97.94)	Acc@5 100.00 ( 99.97)
Epoch: [46][320/391]	Time  0.037 ( 0.024)	Data  0.001 ( 0.002)	Loss 6.7932e-02 (6.2338e-02)	Acc@1  97.66 ( 97.96)	Acc@5 100.00 ( 99.97)
Epoch: [46][330/391]	Time  0.042 ( 0.024)	Data  0.001 ( 0.002)	Loss 7.1350e-02 (6.2667e-02)	Acc@1  98.44 ( 97.95)	Acc@5 100.00 ( 99.97)
Epoch: [46][340/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.1372e-02 (6.2314e-02)	Acc@1  99.22 ( 97.96)	Acc@5 100.00 ( 99.97)
Epoch: [46][350/391]	Time  0.019 ( 0.024)	Data  0.002 ( 0.002)	Loss 8.3557e-02 (6.2615e-02)	Acc@1  95.31 ( 97.95)	Acc@5 100.00 ( 99.97)
Epoch: [46][360/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 7.6904e-02 (6.3061e-02)	Acc@1  96.09 ( 97.92)	Acc@5 100.00 ( 99.97)
Epoch: [46][370/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 6.3965e-02 (6.2931e-02)	Acc@1  97.66 ( 97.92)	Acc@5 100.00 ( 99.97)
Epoch: [46][380/391]	Time  0.037 ( 0.024)	Data  0.000 ( 0.002)	Loss 6.2439e-02 (6.3595e-02)	Acc@1  98.44 ( 97.92)	Acc@5 100.00 ( 99.97)
Epoch: [46][390/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 4.9408e-02 (6.3132e-02)	Acc@1  98.75 ( 97.93)	Acc@5 100.00 ( 99.97)
## e[46] optimizer.zero_grad (sum) time: 0.10416793823242188
## e[46]       loss.backward (sum) time: 2.1709470748901367
## e[46]      optimizer.step (sum) time: 0.8918876647949219
## epoch[46] training(only) time: 9.621585845947266
# Switched to evaluate mode...
Test: [  0/100]	Time  0.169 ( 0.169)	Loss 2.7710e-01 (2.7710e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.017 ( 0.028)	Loss 4.2163e-01 (3.3012e-01)	Acc@1  89.00 ( 91.36)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.019 ( 0.021)	Loss 4.1113e-01 (3.5648e-01)	Acc@1  87.00 ( 90.43)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.017 ( 0.018)	Loss 4.1382e-01 (3.6649e-01)	Acc@1  88.00 ( 90.23)	Acc@5  98.00 ( 99.45)
Test: [ 40/100]	Time  0.011 ( 0.017)	Loss 5.4785e-01 (3.8944e-01)	Acc@1  88.00 ( 89.95)	Acc@5  99.00 ( 99.46)
Test: [ 50/100]	Time  0.011 ( 0.016)	Loss 1.8335e-01 (3.7622e-01)	Acc@1  94.00 ( 90.25)	Acc@5 100.00 ( 99.49)
Test: [ 60/100]	Time  0.008 ( 0.015)	Loss 2.8931e-01 (3.7579e-01)	Acc@1  91.00 ( 90.20)	Acc@5 100.00 ( 99.51)
Test: [ 70/100]	Time  0.013 ( 0.015)	Loss 6.1768e-01 (3.7291e-01)	Acc@1  88.00 ( 90.34)	Acc@5  99.00 ( 99.52)
Test: [ 80/100]	Time  0.018 ( 0.015)	Loss 2.0020e-01 (3.6874e-01)	Acc@1  92.00 ( 90.31)	Acc@5 100.00 ( 99.57)
Test: [ 90/100]	Time  0.028 ( 0.015)	Loss 1.7395e-01 (3.6459e-01)	Acc@1  95.00 ( 90.26)	Acc@5 100.00 ( 99.62)
 * Acc@1 90.300 Acc@5 99.650
### epoch[46] execution time: 11.182151079177856
EPOCH 47
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [47][  0/391]	Time  0.188 ( 0.188)	Data  0.168 ( 0.168)	Loss 4.0070e-02 (4.0070e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [47][ 10/391]	Time  0.018 ( 0.039)	Data  0.001 ( 0.017)	Loss 4.9988e-02 (4.9713e-02)	Acc@1  99.22 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [47][ 20/391]	Time  0.038 ( 0.032)	Data  0.001 ( 0.010)	Loss 6.7444e-02 (4.8324e-02)	Acc@1  96.88 ( 98.33)	Acc@5 100.00 (100.00)
Epoch: [47][ 30/391]	Time  0.018 ( 0.029)	Data  0.001 ( 0.007)	Loss 1.3159e-01 (5.6860e-02)	Acc@1  96.88 ( 98.08)	Acc@5 100.00 (100.00)
Epoch: [47][ 40/391]	Time  0.052 ( 0.028)	Data  0.014 ( 0.006)	Loss 6.3843e-02 (5.7201e-02)	Acc@1  97.66 ( 98.04)	Acc@5 100.00 ( 99.98)
Epoch: [47][ 50/391]	Time  0.019 ( 0.027)	Data  0.001 ( 0.005)	Loss 7.6660e-02 (5.7688e-02)	Acc@1  97.66 ( 98.04)	Acc@5 100.00 ( 99.98)
Epoch: [47][ 60/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.005)	Loss 5.4321e-02 (5.7091e-02)	Acc@1  99.22 ( 98.05)	Acc@5 100.00 ( 99.97)
Epoch: [47][ 70/391]	Time  0.045 ( 0.026)	Data  0.005 ( 0.004)	Loss 6.6895e-02 (5.8621e-02)	Acc@1  98.44 ( 98.07)	Acc@5 100.00 ( 99.97)
Epoch: [47][ 80/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 3.9612e-02 (5.8439e-02)	Acc@1  99.22 ( 98.15)	Acc@5 100.00 ( 99.96)
Epoch: [47][ 90/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.004)	Loss 3.6499e-02 (5.6338e-02)	Acc@1  99.22 ( 98.22)	Acc@5 100.00 ( 99.96)
Epoch: [47][100/391]	Time  0.025 ( 0.026)	Data  0.005 ( 0.004)	Loss 5.1849e-02 (5.7454e-02)	Acc@1  98.44 ( 98.17)	Acc@5 100.00 ( 99.96)
Epoch: [47][110/391]	Time  0.030 ( 0.025)	Data  0.004 ( 0.003)	Loss 3.5370e-02 (5.7452e-02)	Acc@1  99.22 ( 98.13)	Acc@5 100.00 ( 99.96)
Epoch: [47][120/391]	Time  0.039 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.8298e-02 (5.7597e-02)	Acc@1  98.44 ( 98.14)	Acc@5 100.00 ( 99.97)
Epoch: [47][130/391]	Time  0.018 ( 0.025)	Data  0.000 ( 0.003)	Loss 5.2490e-02 (5.6495e-02)	Acc@1  97.66 ( 98.17)	Acc@5 100.00 ( 99.97)
Epoch: [47][140/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.7365e-02 (5.5158e-02)	Acc@1 100.00 ( 98.18)	Acc@5 100.00 ( 99.97)
Epoch: [47][150/391]	Time  0.026 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.6670e-02 (5.4694e-02)	Acc@1  96.88 ( 98.18)	Acc@5 100.00 ( 99.97)
Epoch: [47][160/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.0566e-02 (5.5988e-02)	Acc@1  97.66 ( 98.14)	Acc@5 100.00 ( 99.97)
Epoch: [47][170/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 3.6316e-02 (5.6882e-02)	Acc@1  99.22 ( 98.14)	Acc@5 100.00 ( 99.97)
Epoch: [47][180/391]	Time  0.044 ( 0.025)	Data  0.008 ( 0.003)	Loss 5.6580e-02 (5.7858e-02)	Acc@1  98.44 ( 98.11)	Acc@5 100.00 ( 99.96)
Epoch: [47][190/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.6987e-02 (5.7303e-02)	Acc@1  97.66 ( 98.12)	Acc@5 100.00 ( 99.96)
Epoch: [47][200/391]	Time  0.035 ( 0.025)	Data  0.005 ( 0.003)	Loss 2.6764e-02 (5.6841e-02)	Acc@1 100.00 ( 98.15)	Acc@5 100.00 ( 99.96)
Epoch: [47][210/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.0505e-02 (5.7129e-02)	Acc@1  96.88 ( 98.15)	Acc@5 100.00 ( 99.96)
Epoch: [47][220/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.2227e-02 (5.7304e-02)	Acc@1  98.44 ( 98.14)	Acc@5 100.00 ( 99.96)
Epoch: [47][230/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 4.9744e-02 (5.6539e-02)	Acc@1  97.66 ( 98.15)	Acc@5 100.00 ( 99.97)
Epoch: [47][240/391]	Time  0.017 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.0522e-01 (5.6702e-02)	Acc@1  97.66 ( 98.16)	Acc@5 100.00 ( 99.96)
Epoch: [47][250/391]	Time  0.037 ( 0.025)	Data  0.005 ( 0.003)	Loss 7.0312e-02 (5.6357e-02)	Acc@1  97.66 ( 98.16)	Acc@5 100.00 ( 99.97)
Epoch: [47][260/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.002)	Loss 5.1025e-02 (5.6543e-02)	Acc@1  97.66 ( 98.16)	Acc@5 100.00 ( 99.97)
Epoch: [47][270/391]	Time  0.026 ( 0.025)	Data  0.002 ( 0.002)	Loss 9.0088e-02 (5.6562e-02)	Acc@1  96.88 ( 98.16)	Acc@5 100.00 ( 99.97)
Epoch: [47][280/391]	Time  0.025 ( 0.025)	Data  0.005 ( 0.002)	Loss 2.0523e-02 (5.6274e-02)	Acc@1 100.00 ( 98.17)	Acc@5 100.00 ( 99.97)
Epoch: [47][290/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 6.8359e-02 (5.6489e-02)	Acc@1  97.66 ( 98.17)	Acc@5 100.00 ( 99.97)
Epoch: [47][300/391]	Time  0.026 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.4800e-02 (5.6139e-02)	Acc@1  98.44 ( 98.18)	Acc@5 100.00 ( 99.97)
Epoch: [47][310/391]	Time  0.025 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.8361e-02 (5.5910e-02)	Acc@1  98.44 ( 98.18)	Acc@5 100.00 ( 99.97)
Epoch: [47][320/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 6.9885e-02 (5.6261e-02)	Acc@1  97.66 ( 98.17)	Acc@5 100.00 ( 99.97)
Epoch: [47][330/391]	Time  0.030 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.2227e-02 (5.6023e-02)	Acc@1  99.22 ( 98.18)	Acc@5 100.00 ( 99.97)
Epoch: [47][340/391]	Time  0.020 ( 0.024)	Data  0.001 ( 0.002)	Loss 7.2632e-02 (5.6772e-02)	Acc@1  96.88 ( 98.16)	Acc@5 100.00 ( 99.97)
Epoch: [47][350/391]	Time  0.032 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.7415e-02 (5.6870e-02)	Acc@1  99.22 ( 98.16)	Acc@5 100.00 ( 99.97)
Epoch: [47][360/391]	Time  0.025 ( 0.024)	Data  0.005 ( 0.002)	Loss 5.8685e-02 (5.7283e-02)	Acc@1  98.44 ( 98.14)	Acc@5 100.00 ( 99.97)
Epoch: [47][370/391]	Time  0.020 ( 0.024)	Data  0.003 ( 0.002)	Loss 4.1504e-02 (5.7676e-02)	Acc@1  98.44 ( 98.13)	Acc@5 100.00 ( 99.97)
Epoch: [47][380/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 9.9792e-02 (5.7807e-02)	Acc@1  95.31 ( 98.13)	Acc@5 100.00 ( 99.97)
Epoch: [47][390/391]	Time  0.013 ( 0.024)	Data  0.000 ( 0.002)	Loss 3.8818e-02 (5.8006e-02)	Acc@1  97.50 ( 98.12)	Acc@5 100.00 ( 99.97)
## e[47] optimizer.zero_grad (sum) time: 0.10530257225036621
## e[47]       loss.backward (sum) time: 2.1680796146392822
## e[47]      optimizer.step (sum) time: 0.8885705471038818
## epoch[47] training(only) time: 9.67454981803894
# Switched to evaluate mode...
Test: [  0/100]	Time  0.154 ( 0.154)	Loss 2.8003e-01 (2.8003e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.013 ( 0.027)	Loss 3.3032e-01 (3.3798e-01)	Acc@1  90.00 ( 91.18)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.008 ( 0.020)	Loss 4.4385e-01 (3.6183e-01)	Acc@1  88.00 ( 90.86)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.009 ( 0.018)	Loss 4.2285e-01 (3.7090e-01)	Acc@1  89.00 ( 90.65)	Acc@5  99.00 ( 99.55)
Test: [ 40/100]	Time  0.015 ( 0.017)	Loss 4.8779e-01 (3.9185e-01)	Acc@1  88.00 ( 90.29)	Acc@5 100.00 ( 99.54)
Test: [ 50/100]	Time  0.022 ( 0.016)	Loss 1.8689e-01 (3.7851e-01)	Acc@1  94.00 ( 90.49)	Acc@5 100.00 ( 99.53)
Test: [ 60/100]	Time  0.007 ( 0.015)	Loss 2.8784e-01 (3.7569e-01)	Acc@1  90.00 ( 90.44)	Acc@5 100.00 ( 99.56)
Test: [ 70/100]	Time  0.017 ( 0.015)	Loss 6.0010e-01 (3.7413e-01)	Acc@1  87.00 ( 90.58)	Acc@5 100.00 ( 99.59)
Test: [ 80/100]	Time  0.008 ( 0.015)	Loss 2.1582e-01 (3.7285e-01)	Acc@1  94.00 ( 90.54)	Acc@5 100.00 ( 99.59)
Test: [ 90/100]	Time  0.018 ( 0.015)	Loss 2.2339e-01 (3.6905e-01)	Acc@1  91.00 ( 90.48)	Acc@5 100.00 ( 99.64)
 * Acc@1 90.550 Acc@5 99.670
### epoch[47] execution time: 11.237512350082397
EPOCH 48
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [48][  0/391]	Time  0.238 ( 0.238)	Data  0.217 ( 0.217)	Loss 2.1332e-02 (2.1332e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [48][ 10/391]	Time  0.023 ( 0.043)	Data  0.001 ( 0.021)	Loss 4.8004e-02 (4.9676e-02)	Acc@1  97.66 ( 98.37)	Acc@5 100.00 ( 99.93)
Epoch: [48][ 20/391]	Time  0.024 ( 0.034)	Data  0.002 ( 0.012)	Loss 4.1840e-02 (4.8936e-02)	Acc@1  97.66 ( 98.29)	Acc@5 100.00 ( 99.96)
Epoch: [48][ 30/391]	Time  0.021 ( 0.030)	Data  0.000 ( 0.009)	Loss 7.5256e-02 (4.8596e-02)	Acc@1  97.66 ( 98.34)	Acc@5 100.00 ( 99.97)
Epoch: [48][ 40/391]	Time  0.018 ( 0.029)	Data  0.001 ( 0.007)	Loss 6.6101e-02 (4.9231e-02)	Acc@1  98.44 ( 98.34)	Acc@5 100.00 ( 99.98)
Epoch: [48][ 50/391]	Time  0.023 ( 0.028)	Data  0.003 ( 0.006)	Loss 6.8604e-02 (5.0303e-02)	Acc@1  96.88 ( 98.25)	Acc@5 100.00 ( 99.98)
Epoch: [48][ 60/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.005)	Loss 2.8198e-02 (5.0786e-02)	Acc@1  99.22 ( 98.26)	Acc@5 100.00 ( 99.97)
Epoch: [48][ 70/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.005)	Loss 3.2196e-02 (5.0318e-02)	Acc@1 100.00 ( 98.33)	Acc@5 100.00 ( 99.97)
Epoch: [48][ 80/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 6.8359e-02 (5.1392e-02)	Acc@1  96.88 ( 98.26)	Acc@5 100.00 ( 99.97)
Epoch: [48][ 90/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.004)	Loss 3.0853e-02 (5.1304e-02)	Acc@1  99.22 ( 98.27)	Acc@5 100.00 ( 99.97)
Epoch: [48][100/391]	Time  0.031 ( 0.026)	Data  0.002 ( 0.004)	Loss 8.7585e-02 (5.1946e-02)	Acc@1  96.09 ( 98.24)	Acc@5 100.00 ( 99.98)
Epoch: [48][110/391]	Time  0.032 ( 0.026)	Data  0.000 ( 0.004)	Loss 2.8458e-02 (5.1276e-02)	Acc@1  98.44 ( 98.24)	Acc@5 100.00 ( 99.97)
Epoch: [48][120/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.004)	Loss 2.6138e-02 (5.1158e-02)	Acc@1  99.22 ( 98.24)	Acc@5 100.00 ( 99.97)
Epoch: [48][130/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.1494e-02 (5.1740e-02)	Acc@1  97.66 ( 98.22)	Acc@5 100.00 ( 99.98)
Epoch: [48][140/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.5786e-02 (5.1254e-02)	Acc@1  96.88 ( 98.20)	Acc@5 100.00 ( 99.98)
Epoch: [48][150/391]	Time  0.029 ( 0.025)	Data  0.002 ( 0.003)	Loss 6.5735e-02 (5.1886e-02)	Acc@1  98.44 ( 98.19)	Acc@5 100.00 ( 99.98)
Epoch: [48][160/391]	Time  0.037 ( 0.025)	Data  0.003 ( 0.003)	Loss 1.0767e-01 (5.1872e-02)	Acc@1  97.66 ( 98.19)	Acc@5 100.00 ( 99.98)
Epoch: [48][170/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.1951e-02 (5.1300e-02)	Acc@1  98.44 ( 98.23)	Acc@5 100.00 ( 99.98)
Epoch: [48][180/391]	Time  0.033 ( 0.025)	Data  0.000 ( 0.003)	Loss 5.3162e-02 (5.2218e-02)	Acc@1  98.44 ( 98.19)	Acc@5 100.00 ( 99.98)
Epoch: [48][190/391]	Time  0.031 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.5698e-01 (5.2583e-02)	Acc@1  95.31 ( 98.17)	Acc@5 100.00 ( 99.98)
Epoch: [48][200/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 5.3345e-02 (5.2809e-02)	Acc@1  98.44 ( 98.17)	Acc@5 100.00 ( 99.98)
Epoch: [48][210/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.2830e-01 (5.3667e-02)	Acc@1  96.88 ( 98.12)	Acc@5  99.22 ( 99.97)
Epoch: [48][220/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.0803e-01 (5.4359e-02)	Acc@1  96.88 ( 98.09)	Acc@5 100.00 ( 99.97)
Epoch: [48][230/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 9.3140e-02 (5.4394e-02)	Acc@1  96.88 ( 98.09)	Acc@5 100.00 ( 99.97)
Epoch: [48][240/391]	Time  0.026 ( 0.025)	Data  0.002 ( 0.003)	Loss 3.8055e-02 (5.3813e-02)	Acc@1  98.44 ( 98.12)	Acc@5 100.00 ( 99.97)
Epoch: [48][250/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.1408e-02 (5.4355e-02)	Acc@1  99.22 ( 98.11)	Acc@5 100.00 ( 99.97)
Epoch: [48][260/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.2450e-02 (5.4238e-02)	Acc@1  98.44 ( 98.12)	Acc@5 100.00 ( 99.97)
Epoch: [48][270/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 6.9214e-02 (5.4228e-02)	Acc@1  97.66 ( 98.12)	Acc@5 100.00 ( 99.97)
Epoch: [48][280/391]	Time  0.024 ( 0.025)	Data  0.001 ( 0.002)	Loss 7.9712e-02 (5.4643e-02)	Acc@1  96.88 ( 98.11)	Acc@5 100.00 ( 99.97)
Epoch: [48][290/391]	Time  0.033 ( 0.025)	Data  0.002 ( 0.002)	Loss 5.9692e-02 (5.5059e-02)	Acc@1  97.66 ( 98.11)	Acc@5 100.00 ( 99.97)
Epoch: [48][300/391]	Time  0.027 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.8798e-02 (5.5011e-02)	Acc@1  98.44 ( 98.13)	Acc@5 100.00 ( 99.96)
Epoch: [48][310/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.002)	Loss 6.7749e-02 (5.4755e-02)	Acc@1  96.88 ( 98.14)	Acc@5 100.00 ( 99.96)
Epoch: [48][320/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 6.0913e-02 (5.4730e-02)	Acc@1  97.66 ( 98.14)	Acc@5 100.00 ( 99.96)
Epoch: [48][330/391]	Time  0.026 ( 0.025)	Data  0.001 ( 0.002)	Loss 6.1401e-02 (5.5112e-02)	Acc@1  98.44 ( 98.13)	Acc@5 100.00 ( 99.96)
Epoch: [48][340/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.0792e-02 (5.4985e-02)	Acc@1  98.44 ( 98.13)	Acc@5 100.00 ( 99.96)
Epoch: [48][350/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.4948e-02 (5.4634e-02)	Acc@1  99.22 ( 98.13)	Acc@5 100.00 ( 99.96)
Epoch: [48][360/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.8051e-02 (5.4865e-02)	Acc@1 100.00 ( 98.13)	Acc@5 100.00 ( 99.96)
Epoch: [48][370/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.2720e-02 (5.4769e-02)	Acc@1 100.00 ( 98.15)	Acc@5 100.00 ( 99.96)
Epoch: [48][380/391]	Time  0.036 ( 0.025)	Data  0.001 ( 0.002)	Loss 8.4595e-02 (5.5359e-02)	Acc@1  97.66 ( 98.13)	Acc@5 100.00 ( 99.96)
Epoch: [48][390/391]	Time  0.017 ( 0.025)	Data  0.001 ( 0.002)	Loss 5.0537e-02 (5.5785e-02)	Acc@1  97.50 ( 98.11)	Acc@5 100.00 ( 99.96)
## e[48] optimizer.zero_grad (sum) time: 0.10487580299377441
## e[48]       loss.backward (sum) time: 2.134416341781616
## e[48]      optimizer.step (sum) time: 0.8875253200531006
## epoch[48] training(only) time: 9.732167959213257
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 2.8174e-01 (2.8174e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.020 ( 0.027)	Loss 4.0747e-01 (3.5976e-01)	Acc@1  88.00 ( 90.18)	Acc@5  99.00 ( 99.73)
Test: [ 20/100]	Time  0.019 ( 0.021)	Loss 4.2041e-01 (3.7437e-01)	Acc@1  86.00 ( 90.05)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.008 ( 0.018)	Loss 4.4263e-01 (3.8285e-01)	Acc@1  86.00 ( 90.03)	Acc@5  99.00 ( 99.58)
Test: [ 40/100]	Time  0.008 ( 0.017)	Loss 5.2881e-01 (4.0221e-01)	Acc@1  88.00 ( 89.63)	Acc@5  98.00 ( 99.49)
Test: [ 50/100]	Time  0.012 ( 0.016)	Loss 1.8750e-01 (3.8549e-01)	Acc@1  92.00 ( 90.06)	Acc@5 100.00 ( 99.51)
Test: [ 60/100]	Time  0.012 ( 0.016)	Loss 2.4695e-01 (3.8370e-01)	Acc@1  92.00 ( 90.03)	Acc@5 100.00 ( 99.54)
Test: [ 70/100]	Time  0.009 ( 0.015)	Loss 6.4355e-01 (3.8149e-01)	Acc@1  88.00 ( 90.14)	Acc@5 100.00 ( 99.56)
Test: [ 80/100]	Time  0.032 ( 0.015)	Loss 2.0386e-01 (3.7833e-01)	Acc@1  93.00 ( 90.21)	Acc@5 100.00 ( 99.56)
Test: [ 90/100]	Time  0.014 ( 0.015)	Loss 2.0276e-01 (3.7443e-01)	Acc@1  92.00 ( 90.22)	Acc@5 100.00 ( 99.60)
 * Acc@1 90.290 Acc@5 99.610
### epoch[48] execution time: 11.326963424682617
EPOCH 49
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [49][  0/391]	Time  0.193 ( 0.193)	Data  0.173 ( 0.173)	Loss 3.6316e-02 (3.6316e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [49][ 10/391]	Time  0.028 ( 0.041)	Data  0.001 ( 0.017)	Loss 1.6235e-02 (4.5633e-02)	Acc@1 100.00 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [49][ 20/391]	Time  0.018 ( 0.032)	Data  0.001 ( 0.010)	Loss 8.1543e-02 (5.3670e-02)	Acc@1  96.88 ( 98.18)	Acc@5 100.00 (100.00)
Epoch: [49][ 30/391]	Time  0.031 ( 0.030)	Data  0.001 ( 0.007)	Loss 1.5381e-02 (4.9343e-02)	Acc@1 100.00 ( 98.39)	Acc@5 100.00 (100.00)
Epoch: [49][ 40/391]	Time  0.018 ( 0.028)	Data  0.000 ( 0.006)	Loss 9.8877e-02 (5.4244e-02)	Acc@1  97.66 ( 98.21)	Acc@5 100.00 (100.00)
Epoch: [49][ 50/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.005)	Loss 1.4191e-02 (5.0703e-02)	Acc@1 100.00 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [49][ 60/391]	Time  0.031 ( 0.027)	Data  0.002 ( 0.004)	Loss 4.9835e-02 (5.0329e-02)	Acc@1  97.66 ( 98.25)	Acc@5 100.00 (100.00)
Epoch: [49][ 70/391]	Time  0.027 ( 0.026)	Data  0.002 ( 0.004)	Loss 3.6438e-02 (4.9197e-02)	Acc@1  99.22 ( 98.29)	Acc@5 100.00 (100.00)
Epoch: [49][ 80/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 2.6947e-02 (5.0479e-02)	Acc@1  98.44 ( 98.24)	Acc@5 100.00 (100.00)
Epoch: [49][ 90/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.004)	Loss 3.3234e-02 (4.9761e-02)	Acc@1  98.44 ( 98.29)	Acc@5 100.00 ( 99.99)
Epoch: [49][100/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2604e-02 (4.8860e-02)	Acc@1 100.00 ( 98.33)	Acc@5 100.00 ( 99.99)
Epoch: [49][110/391]	Time  0.025 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.0461e-01 (4.9885e-02)	Acc@1  96.88 ( 98.29)	Acc@5 100.00 ( 99.99)
Epoch: [49][120/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.2491e-02 (5.0771e-02)	Acc@1  99.22 ( 98.29)	Acc@5 100.00 ( 99.98)
Epoch: [49][130/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.5144e-02 (5.1750e-02)	Acc@1  97.66 ( 98.29)	Acc@5 100.00 ( 99.98)
Epoch: [49][140/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.2867e-02 (5.2349e-02)	Acc@1  99.22 ( 98.25)	Acc@5 100.00 ( 99.98)
Epoch: [49][150/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 3.0807e-02 (5.2136e-02)	Acc@1  98.44 ( 98.28)	Acc@5 100.00 ( 99.98)
Epoch: [49][160/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 9.0210e-02 (5.2602e-02)	Acc@1  96.88 ( 98.25)	Acc@5 100.00 ( 99.99)
Epoch: [49][170/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.6112e-02 (5.2984e-02)	Acc@1  96.88 ( 98.22)	Acc@5 100.00 ( 99.99)
Epoch: [49][180/391]	Time  0.025 ( 0.025)	Data  0.000 ( 0.003)	Loss 1.2138e-02 (5.3719e-02)	Acc@1 100.00 ( 98.20)	Acc@5 100.00 ( 99.98)
Epoch: [49][190/391]	Time  0.016 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.0297e-01 (5.4296e-02)	Acc@1  96.88 ( 98.15)	Acc@5 100.00 ( 99.98)
Epoch: [49][200/391]	Time  0.022 ( 0.025)	Data  0.002 ( 0.003)	Loss 3.3813e-02 (5.4635e-02)	Acc@1  99.22 ( 98.15)	Acc@5 100.00 ( 99.98)
Epoch: [49][210/391]	Time  0.029 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.3159e-01 (5.4820e-02)	Acc@1  97.66 ( 98.15)	Acc@5 100.00 ( 99.99)
Epoch: [49][220/391]	Time  0.035 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.0436e-02 (5.4882e-02)	Acc@1  98.44 ( 98.14)	Acc@5 100.00 ( 99.98)
Epoch: [49][230/391]	Time  0.038 ( 0.025)	Data  0.007 ( 0.002)	Loss 4.5532e-02 (5.4706e-02)	Acc@1  98.44 ( 98.15)	Acc@5 100.00 ( 99.98)
Epoch: [49][240/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.002)	Loss 2.7435e-02 (5.4359e-02)	Acc@1  98.44 ( 98.16)	Acc@5 100.00 ( 99.98)
Epoch: [49][250/391]	Time  0.026 ( 0.025)	Data  0.001 ( 0.002)	Loss 6.3721e-02 (5.4821e-02)	Acc@1  98.44 ( 98.13)	Acc@5 100.00 ( 99.98)
Epoch: [49][260/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.7140e-02 (5.4504e-02)	Acc@1  98.44 ( 98.14)	Acc@5 100.00 ( 99.98)
Epoch: [49][270/391]	Time  0.035 ( 0.025)	Data  0.000 ( 0.002)	Loss 7.4158e-02 (5.4628e-02)	Acc@1  97.66 ( 98.14)	Acc@5 100.00 ( 99.98)
Epoch: [49][280/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.1511e-01 (5.4338e-02)	Acc@1  97.66 ( 98.17)	Acc@5 100.00 ( 99.98)
Epoch: [49][290/391]	Time  0.036 ( 0.025)	Data  0.002 ( 0.002)	Loss 4.8737e-02 (5.3817e-02)	Acc@1  97.66 ( 98.18)	Acc@5 100.00 ( 99.98)
Epoch: [49][300/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.5105e-02 (5.4097e-02)	Acc@1  98.44 ( 98.18)	Acc@5 100.00 ( 99.98)
Epoch: [49][310/391]	Time  0.032 ( 0.025)	Data  0.000 ( 0.002)	Loss 7.6538e-02 (5.4129e-02)	Acc@1  97.66 ( 98.18)	Acc@5 100.00 ( 99.98)
Epoch: [49][320/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.002)	Loss 2.9739e-02 (5.4562e-02)	Acc@1  99.22 ( 98.16)	Acc@5 100.00 ( 99.98)
Epoch: [49][330/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.7283e-02 (5.4520e-02)	Acc@1  98.44 ( 98.15)	Acc@5 100.00 ( 99.98)
Epoch: [49][340/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 6.7261e-02 (5.4353e-02)	Acc@1  97.66 ( 98.15)	Acc@5 100.00 ( 99.98)
Epoch: [49][350/391]	Time  0.017 ( 0.024)	Data  0.000 ( 0.002)	Loss 8.0017e-02 (5.4417e-02)	Acc@1  96.88 ( 98.16)	Acc@5 100.00 ( 99.98)
Epoch: [49][360/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.4811e-02 (5.4221e-02)	Acc@1 100.00 ( 98.17)	Acc@5 100.00 ( 99.98)
Epoch: [49][370/391]	Time  0.031 ( 0.025)	Data  0.001 ( 0.002)	Loss 7.5562e-02 (5.4116e-02)	Acc@1  97.66 ( 98.16)	Acc@5 100.00 ( 99.98)
Epoch: [49][380/391]	Time  0.027 ( 0.025)	Data  0.001 ( 0.002)	Loss 7.5256e-02 (5.4191e-02)	Acc@1  96.88 ( 98.15)	Acc@5 100.00 ( 99.98)
Epoch: [49][390/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.002)	Loss 4.0863e-02 (5.3799e-02)	Acc@1  98.75 ( 98.17)	Acc@5 100.00 ( 99.98)
## e[49] optimizer.zero_grad (sum) time: 0.1051785945892334
## e[49]       loss.backward (sum) time: 2.1320979595184326
## e[49]      optimizer.step (sum) time: 0.87831711769104
## epoch[49] training(only) time: 9.677356719970703
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 2.6196e-01 (2.6196e-01)	Acc@1  93.00 ( 93.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.011 ( 0.027)	Loss 4.3799e-01 (3.4629e-01)	Acc@1  90.00 ( 90.73)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.013 ( 0.020)	Loss 4.5068e-01 (3.6609e-01)	Acc@1  84.00 ( 90.48)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.012 ( 0.018)	Loss 4.9854e-01 (3.8358e-01)	Acc@1  85.00 ( 90.26)	Acc@5 100.00 ( 99.58)
Test: [ 40/100]	Time  0.013 ( 0.017)	Loss 4.5068e-01 (3.9840e-01)	Acc@1  90.00 ( 90.10)	Acc@5  99.00 ( 99.56)
Test: [ 50/100]	Time  0.008 ( 0.016)	Loss 1.5979e-01 (3.8267e-01)	Acc@1  94.00 ( 90.31)	Acc@5 100.00 ( 99.55)
Test: [ 60/100]	Time  0.009 ( 0.016)	Loss 2.4255e-01 (3.8189e-01)	Acc@1  93.00 ( 90.20)	Acc@5 100.00 ( 99.57)
Test: [ 70/100]	Time  0.018 ( 0.015)	Loss 6.1475e-01 (3.8294e-01)	Acc@1  87.00 ( 90.23)	Acc@5  99.00 ( 99.58)
Test: [ 80/100]	Time  0.019 ( 0.015)	Loss 3.1982e-01 (3.8010e-01)	Acc@1  91.00 ( 90.22)	Acc@5 100.00 ( 99.59)
Test: [ 90/100]	Time  0.012 ( 0.015)	Loss 1.8518e-01 (3.7556e-01)	Acc@1  94.00 ( 90.26)	Acc@5 100.00 ( 99.64)
 * Acc@1 90.300 Acc@5 99.660
### epoch[49] execution time: 11.287899732589722
EPOCH 50
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [50][  0/391]	Time  0.189 ( 0.189)	Data  0.168 ( 0.168)	Loss 1.7715e-02 (1.7715e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [50][ 10/391]	Time  0.019 ( 0.036)	Data  0.001 ( 0.017)	Loss 3.7720e-02 (3.5002e-02)	Acc@1  98.44 ( 98.79)	Acc@5 100.00 (100.00)
Epoch: [50][ 20/391]	Time  0.018 ( 0.031)	Data  0.001 ( 0.010)	Loss 3.9551e-02 (3.5908e-02)	Acc@1  98.44 ( 98.70)	Acc@5 100.00 (100.00)
Epoch: [50][ 30/391]	Time  0.032 ( 0.029)	Data  0.001 ( 0.007)	Loss 6.7871e-02 (4.0222e-02)	Acc@1  98.44 ( 98.69)	Acc@5 100.00 (100.00)
Epoch: [50][ 40/391]	Time  0.017 ( 0.027)	Data  0.000 ( 0.006)	Loss 9.9365e-02 (4.4480e-02)	Acc@1  95.31 ( 98.46)	Acc@5 100.00 (100.00)
Epoch: [50][ 50/391]	Time  0.026 ( 0.027)	Data  0.002 ( 0.005)	Loss 2.4506e-02 (4.6093e-02)	Acc@1  99.22 ( 98.33)	Acc@5 100.00 (100.00)
Epoch: [50][ 60/391]	Time  0.023 ( 0.026)	Data  0.003 ( 0.005)	Loss 6.2744e-02 (4.7225e-02)	Acc@1  96.88 ( 98.26)	Acc@5 100.00 ( 99.99)
Epoch: [50][ 70/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 9.7717e-02 (4.8892e-02)	Acc@1  96.88 ( 98.25)	Acc@5 100.00 ( 99.99)
Epoch: [50][ 80/391]	Time  0.043 ( 0.026)	Data  0.002 ( 0.004)	Loss 6.6101e-02 (4.9773e-02)	Acc@1  98.44 ( 98.28)	Acc@5 100.00 ( 99.99)
Epoch: [50][ 90/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 2.4597e-02 (4.7984e-02)	Acc@1  99.22 ( 98.36)	Acc@5 100.00 ( 99.99)
Epoch: [50][100/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.2919e-02 (4.8368e-02)	Acc@1  99.22 ( 98.32)	Acc@5 100.00 ( 99.99)
Epoch: [50][110/391]	Time  0.028 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.0405e-02 (4.8900e-02)	Acc@1  99.22 ( 98.31)	Acc@5 100.00 ( 99.99)
Epoch: [50][120/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.2643e-02 (4.8991e-02)	Acc@1  97.66 ( 98.33)	Acc@5 100.00 ( 99.99)
Epoch: [50][130/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.2053e-02 (4.9064e-02)	Acc@1  97.66 ( 98.32)	Acc@5 100.00 ( 99.99)
Epoch: [50][140/391]	Time  0.034 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.5796e-02 (4.9783e-02)	Acc@1  98.44 ( 98.30)	Acc@5 100.00 ( 99.99)
Epoch: [50][150/391]	Time  0.024 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.1179e-02 (4.8696e-02)	Acc@1  99.22 ( 98.34)	Acc@5 100.00 ( 99.99)
Epoch: [50][160/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.3721e-02 (4.8617e-02)	Acc@1  98.44 ( 98.34)	Acc@5  99.22 ( 99.99)
Epoch: [50][170/391]	Time  0.018 ( 0.025)	Data  0.002 ( 0.003)	Loss 4.4373e-02 (4.8893e-02)	Acc@1  99.22 ( 98.34)	Acc@5 100.00 ( 99.99)
Epoch: [50][180/391]	Time  0.034 ( 0.025)	Data  0.002 ( 0.003)	Loss 1.5625e-02 (4.8498e-02)	Acc@1 100.00 ( 98.36)	Acc@5 100.00 ( 99.99)
Epoch: [50][190/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.3880e-02 (4.8199e-02)	Acc@1  99.22 ( 98.37)	Acc@5 100.00 ( 99.99)
Epoch: [50][200/391]	Time  0.038 ( 0.025)	Data  0.005 ( 0.003)	Loss 4.0771e-02 (4.7888e-02)	Acc@1  97.66 ( 98.38)	Acc@5 100.00 ( 99.99)
Epoch: [50][210/391]	Time  0.017 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.8615e-02 (4.8048e-02)	Acc@1  98.44 ( 98.37)	Acc@5 100.00 ( 99.99)
Epoch: [50][220/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.8778e-02 (4.7995e-02)	Acc@1  99.22 ( 98.37)	Acc@5 100.00 ( 99.99)
Epoch: [50][230/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 7.7332e-02 (4.7764e-02)	Acc@1  96.09 ( 98.37)	Acc@5 100.00 ( 99.99)
Epoch: [50][240/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.002)	Loss 1.6162e-01 (4.8484e-02)	Acc@1  97.66 ( 98.35)	Acc@5 100.00 ( 99.99)
Epoch: [50][250/391]	Time  0.030 ( 0.025)	Data  0.001 ( 0.002)	Loss 8.8135e-02 (4.8915e-02)	Acc@1  96.88 ( 98.34)	Acc@5 100.00 ( 99.99)
Epoch: [50][260/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 4.8065e-02 (4.8879e-02)	Acc@1  99.22 ( 98.34)	Acc@5 100.00 ( 99.99)
Epoch: [50][270/391]	Time  0.041 ( 0.025)	Data  0.001 ( 0.002)	Loss 7.1533e-02 (4.8493e-02)	Acc@1  98.44 ( 98.36)	Acc@5 100.00 ( 99.99)
Epoch: [50][280/391]	Time  0.018 ( 0.024)	Data  0.002 ( 0.002)	Loss 1.2598e-01 (4.9517e-02)	Acc@1  96.09 ( 98.33)	Acc@5  99.22 ( 99.98)
Epoch: [50][290/391]	Time  0.038 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.5662e-01 (5.0324e-02)	Acc@1  96.09 ( 98.30)	Acc@5 100.00 ( 99.98)
Epoch: [50][300/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.9775e-02 (5.0037e-02)	Acc@1 100.00 ( 98.32)	Acc@5 100.00 ( 99.98)
Epoch: [50][310/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.4851e-02 (5.0065e-02)	Acc@1  96.88 ( 98.31)	Acc@5 100.00 ( 99.98)
Epoch: [50][320/391]	Time  0.017 ( 0.024)	Data  0.000 ( 0.002)	Loss 6.0577e-02 (5.0222e-02)	Acc@1  98.44 ( 98.32)	Acc@5 100.00 ( 99.98)
Epoch: [50][330/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 4.9347e-02 (5.0220e-02)	Acc@1  97.66 ( 98.31)	Acc@5 100.00 ( 99.98)
Epoch: [50][340/391]	Time  0.029 ( 0.024)	Data  0.001 ( 0.002)	Loss 5.4291e-02 (5.0408e-02)	Acc@1  96.88 ( 98.31)	Acc@5 100.00 ( 99.98)
Epoch: [50][350/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 7.8125e-02 (5.0864e-02)	Acc@1  97.66 ( 98.30)	Acc@5 100.00 ( 99.98)
Epoch: [50][360/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.1261e-02 (5.1344e-02)	Acc@1 100.00 ( 98.28)	Acc@5 100.00 ( 99.98)
Epoch: [50][370/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.1281e-02 (5.1524e-02)	Acc@1  99.22 ( 98.27)	Acc@5 100.00 ( 99.99)
Epoch: [50][380/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 9.3689e-02 (5.2109e-02)	Acc@1  97.66 ( 98.25)	Acc@5 100.00 ( 99.98)
Epoch: [50][390/391]	Time  0.016 ( 0.024)	Data  0.001 ( 0.002)	Loss 6.9031e-02 (5.1967e-02)	Acc@1  97.50 ( 98.25)	Acc@5 100.00 ( 99.98)
## e[50] optimizer.zero_grad (sum) time: 0.10446381568908691
## e[50]       loss.backward (sum) time: 2.181600332260132
## e[50]      optimizer.step (sum) time: 0.8824868202209473
## epoch[50] training(only) time: 9.621817111968994
# Switched to evaluate mode...
Test: [  0/100]	Time  0.158 ( 0.158)	Loss 2.6416e-01 (2.6416e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.008 ( 0.026)	Loss 5.1611e-01 (3.5824e-01)	Acc@1  90.00 ( 90.27)	Acc@5  99.00 ( 99.55)
Test: [ 20/100]	Time  0.013 ( 0.020)	Loss 4.7876e-01 (3.8406e-01)	Acc@1  84.00 ( 89.95)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.008 ( 0.017)	Loss 4.0479e-01 (4.0253e-01)	Acc@1  88.00 ( 89.84)	Acc@5 100.00 ( 99.55)
Test: [ 40/100]	Time  0.008 ( 0.016)	Loss 5.1611e-01 (4.1932e-01)	Acc@1  89.00 ( 89.44)	Acc@5  98.00 ( 99.51)
Test: [ 50/100]	Time  0.018 ( 0.016)	Loss 1.3062e-01 (4.0480e-01)	Acc@1  94.00 ( 89.65)	Acc@5 100.00 ( 99.53)
Test: [ 60/100]	Time  0.008 ( 0.016)	Loss 2.2974e-01 (3.9990e-01)	Acc@1  95.00 ( 89.77)	Acc@5 100.00 ( 99.56)
Test: [ 70/100]	Time  0.016 ( 0.015)	Loss 6.5918e-01 (3.9949e-01)	Acc@1  89.00 ( 89.97)	Acc@5  98.00 ( 99.55)
Test: [ 80/100]	Time  0.011 ( 0.015)	Loss 2.7515e-01 (3.9595e-01)	Acc@1  91.00 ( 89.99)	Acc@5 100.00 ( 99.57)
Test: [ 90/100]	Time  0.012 ( 0.015)	Loss 2.0728e-01 (3.9051e-01)	Acc@1  93.00 ( 90.00)	Acc@5 100.00 ( 99.62)
 * Acc@1 90.050 Acc@5 99.630
### epoch[50] execution time: 11.178644895553589
EPOCH 51
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [51][  0/391]	Time  0.189 ( 0.189)	Data  0.164 ( 0.164)	Loss 8.7036e-02 (8.7036e-02)	Acc@1  98.44 ( 98.44)	Acc@5  99.22 ( 99.22)
Epoch: [51][ 10/391]	Time  0.021 ( 0.039)	Data  0.001 ( 0.017)	Loss 5.9509e-02 (5.2528e-02)	Acc@1  96.88 ( 98.08)	Acc@5 100.00 ( 99.93)
Epoch: [51][ 20/391]	Time  0.039 ( 0.032)	Data  0.004 ( 0.010)	Loss 5.7343e-02 (5.0605e-02)	Acc@1  98.44 ( 98.29)	Acc@5 100.00 ( 99.96)
Epoch: [51][ 30/391]	Time  0.028 ( 0.029)	Data  0.000 ( 0.007)	Loss 7.4707e-02 (5.3899e-02)	Acc@1  97.66 ( 98.26)	Acc@5 100.00 ( 99.92)
Epoch: [51][ 40/391]	Time  0.034 ( 0.028)	Data  0.000 ( 0.006)	Loss 2.2079e-02 (5.5076e-02)	Acc@1  99.22 ( 98.15)	Acc@5 100.00 ( 99.94)
Epoch: [51][ 50/391]	Time  0.016 ( 0.027)	Data  0.001 ( 0.005)	Loss 3.8666e-02 (5.3621e-02)	Acc@1  99.22 ( 98.16)	Acc@5 100.00 ( 99.95)
Epoch: [51][ 60/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.005)	Loss 3.5126e-02 (5.1445e-02)	Acc@1  98.44 ( 98.21)	Acc@5 100.00 ( 99.96)
Epoch: [51][ 70/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 4.9774e-02 (5.3861e-02)	Acc@1  98.44 ( 98.16)	Acc@5 100.00 ( 99.97)
Epoch: [51][ 80/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.4160e-02 (5.0781e-02)	Acc@1 100.00 ( 98.30)	Acc@5 100.00 ( 99.97)
Epoch: [51][ 90/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.0834e-01 (5.0810e-02)	Acc@1  96.88 ( 98.33)	Acc@5 100.00 ( 99.97)
Epoch: [51][100/391]	Time  0.039 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.0669e-02 (5.2194e-02)	Acc@1  97.66 ( 98.28)	Acc@5 100.00 ( 99.97)
Epoch: [51][110/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 2.2339e-02 (4.9992e-02)	Acc@1  98.44 ( 98.37)	Acc@5 100.00 ( 99.97)
Epoch: [51][120/391]	Time  0.037 ( 0.025)	Data  0.002 ( 0.003)	Loss 2.6215e-02 (5.0018e-02)	Acc@1  99.22 ( 98.36)	Acc@5 100.00 ( 99.97)
Epoch: [51][130/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 1.2769e-01 (4.9609e-02)	Acc@1  97.66 ( 98.39)	Acc@5 100.00 ( 99.98)
Epoch: [51][140/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.9852e-02 (5.0039e-02)	Acc@1 100.00 ( 98.38)	Acc@5 100.00 ( 99.98)
Epoch: [51][150/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 9.4421e-02 (5.0791e-02)	Acc@1  97.66 ( 98.36)	Acc@5 100.00 ( 99.97)
Epoch: [51][160/391]	Time  0.018 ( 0.025)	Data  0.002 ( 0.003)	Loss 6.2988e-02 (5.1310e-02)	Acc@1  96.88 ( 98.34)	Acc@5 100.00 ( 99.98)
Epoch: [51][170/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.0566e-02 (5.1292e-02)	Acc@1  96.88 ( 98.33)	Acc@5 100.00 ( 99.98)
Epoch: [51][180/391]	Time  0.024 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.0535e-01 (5.1104e-02)	Acc@1  94.53 ( 98.32)	Acc@5 100.00 ( 99.98)
Epoch: [51][190/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.3365e-02 (5.0829e-02)	Acc@1  99.22 ( 98.33)	Acc@5 100.00 ( 99.98)
Epoch: [51][200/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 7.9041e-02 (5.0841e-02)	Acc@1  98.44 ( 98.34)	Acc@5 100.00 ( 99.98)
Epoch: [51][210/391]	Time  0.018 ( 0.024)	Data  0.002 ( 0.003)	Loss 3.4882e-02 (5.0751e-02)	Acc@1  99.22 ( 98.33)	Acc@5 100.00 ( 99.98)
Epoch: [51][220/391]	Time  0.027 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.0899e-02 (5.0473e-02)	Acc@1  99.22 ( 98.36)	Acc@5 100.00 ( 99.98)
Epoch: [51][230/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.1179e-02 (5.0310e-02)	Acc@1  99.22 ( 98.37)	Acc@5 100.00 ( 99.98)
Epoch: [51][240/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.3422e-02 (5.0059e-02)	Acc@1 100.00 ( 98.39)	Acc@5 100.00 ( 99.98)
Epoch: [51][250/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.9255e-02 (4.9641e-02)	Acc@1  98.44 ( 98.41)	Acc@5 100.00 ( 99.98)
Epoch: [51][260/391]	Time  0.030 ( 0.025)	Data  0.000 ( 0.002)	Loss 2.4323e-02 (4.9277e-02)	Acc@1  99.22 ( 98.42)	Acc@5 100.00 ( 99.98)
Epoch: [51][270/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.7506e-02 (4.9599e-02)	Acc@1  99.22 ( 98.42)	Acc@5 100.00 ( 99.97)
Epoch: [51][280/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.0823e-02 (4.9273e-02)	Acc@1  97.66 ( 98.42)	Acc@5 100.00 ( 99.97)
Epoch: [51][290/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.8635e-02 (4.8992e-02)	Acc@1  98.44 ( 98.42)	Acc@5 100.00 ( 99.98)
Epoch: [51][300/391]	Time  0.023 ( 0.025)	Data  0.004 ( 0.002)	Loss 5.4810e-02 (4.9301e-02)	Acc@1  97.66 ( 98.40)	Acc@5 100.00 ( 99.98)
Epoch: [51][310/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.7588e-02 (4.9777e-02)	Acc@1  99.22 ( 98.39)	Acc@5 100.00 ( 99.98)
Epoch: [51][320/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 7.6141e-03 (4.9483e-02)	Acc@1 100.00 ( 98.39)	Acc@5 100.00 ( 99.98)
Epoch: [51][330/391]	Time  0.029 ( 0.024)	Data  0.002 ( 0.002)	Loss 5.4565e-02 (4.9377e-02)	Acc@1  97.66 ( 98.39)	Acc@5 100.00 ( 99.98)
Epoch: [51][340/391]	Time  0.018 ( 0.024)	Data  0.000 ( 0.002)	Loss 7.5684e-02 (4.9605e-02)	Acc@1  96.88 ( 98.37)	Acc@5 100.00 ( 99.98)
Epoch: [51][350/391]	Time  0.034 ( 0.024)	Data  0.002 ( 0.002)	Loss 3.3722e-02 (4.9617e-02)	Acc@1  99.22 ( 98.37)	Acc@5 100.00 ( 99.98)
Epoch: [51][360/391]	Time  0.024 ( 0.024)	Data  0.001 ( 0.002)	Loss 8.2520e-02 (4.9816e-02)	Acc@1  97.66 ( 98.37)	Acc@5 100.00 ( 99.98)
Epoch: [51][370/391]	Time  0.024 ( 0.024)	Data  0.004 ( 0.002)	Loss 6.4819e-02 (5.0294e-02)	Acc@1  98.44 ( 98.36)	Acc@5 100.00 ( 99.98)
Epoch: [51][380/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 6.4453e-02 (5.0111e-02)	Acc@1  98.44 ( 98.37)	Acc@5 100.00 ( 99.98)
Epoch: [51][390/391]	Time  0.014 ( 0.024)	Data  0.000 ( 0.002)	Loss 1.5327e-02 (5.0130e-02)	Acc@1 100.00 ( 98.37)	Acc@5 100.00 ( 99.98)
## e[51] optimizer.zero_grad (sum) time: 0.10552740097045898
## e[51]       loss.backward (sum) time: 2.1618459224700928
## e[51]      optimizer.step (sum) time: 0.8941612243652344
## epoch[51] training(only) time: 9.628320932388306
# Switched to evaluate mode...
Test: [  0/100]	Time  0.149 ( 0.149)	Loss 2.8027e-01 (2.8027e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.008 ( 0.025)	Loss 4.7217e-01 (3.6427e-01)	Acc@1  90.00 ( 90.64)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.016 ( 0.020)	Loss 5.2930e-01 (3.8662e-01)	Acc@1  86.00 ( 90.43)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.028 ( 0.018)	Loss 4.3140e-01 (3.9506e-01)	Acc@1  87.00 ( 90.32)	Acc@5  99.00 ( 99.58)
Test: [ 40/100]	Time  0.014 ( 0.016)	Loss 5.1758e-01 (4.1367e-01)	Acc@1  88.00 ( 90.02)	Acc@5  98.00 ( 99.51)
Test: [ 50/100]	Time  0.021 ( 0.016)	Loss 1.7859e-01 (3.9705e-01)	Acc@1  94.00 ( 90.43)	Acc@5 100.00 ( 99.53)
Test: [ 60/100]	Time  0.008 ( 0.015)	Loss 2.7710e-01 (3.9212e-01)	Acc@1  92.00 ( 90.38)	Acc@5 100.00 ( 99.56)
Test: [ 70/100]	Time  0.008 ( 0.015)	Loss 6.8408e-01 (3.9097e-01)	Acc@1  86.00 ( 90.37)	Acc@5  98.00 ( 99.54)
Test: [ 80/100]	Time  0.016 ( 0.015)	Loss 1.8848e-01 (3.8693e-01)	Acc@1  93.00 ( 90.44)	Acc@5 100.00 ( 99.54)
Test: [ 90/100]	Time  0.017 ( 0.015)	Loss 2.7710e-01 (3.8268e-01)	Acc@1  91.00 ( 90.45)	Acc@5 100.00 ( 99.58)
 * Acc@1 90.480 Acc@5 99.620
### epoch[51] execution time: 11.18592619895935
EPOCH 52
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [52][  0/391]	Time  0.190 ( 0.190)	Data  0.169 ( 0.169)	Loss 4.6295e-02 (4.6295e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [52][ 10/391]	Time  0.019 ( 0.039)	Data  0.001 ( 0.018)	Loss 2.5833e-02 (5.4030e-02)	Acc@1  99.22 ( 98.15)	Acc@5 100.00 (100.00)
Epoch: [52][ 20/391]	Time  0.015 ( 0.032)	Data  0.001 ( 0.010)	Loss 4.0131e-02 (4.4310e-02)	Acc@1  98.44 ( 98.51)	Acc@5 100.00 (100.00)
Epoch: [52][ 30/391]	Time  0.026 ( 0.030)	Data  0.001 ( 0.007)	Loss 2.4185e-02 (4.1250e-02)	Acc@1  99.22 ( 98.74)	Acc@5 100.00 (100.00)
Epoch: [52][ 40/391]	Time  0.032 ( 0.029)	Data  0.001 ( 0.006)	Loss 9.3689e-02 (4.3801e-02)	Acc@1  96.88 ( 98.65)	Acc@5 100.00 ( 99.98)
Epoch: [52][ 50/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.005)	Loss 1.2250e-01 (4.4249e-02)	Acc@1  93.75 ( 98.54)	Acc@5 100.00 ( 99.98)
Epoch: [52][ 60/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.005)	Loss 3.4729e-02 (4.5161e-02)	Acc@1  98.44 ( 98.49)	Acc@5 100.00 ( 99.99)
Epoch: [52][ 70/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 8.0811e-02 (4.8379e-02)	Acc@1  96.88 ( 98.34)	Acc@5 100.00 ( 99.99)
Epoch: [52][ 80/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 3.1891e-02 (4.7214e-02)	Acc@1  99.22 ( 98.39)	Acc@5 100.00 ( 99.99)
Epoch: [52][ 90/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.004)	Loss 5.8014e-02 (4.5271e-02)	Acc@1  98.44 ( 98.45)	Acc@5 100.00 ( 99.99)
Epoch: [52][100/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.2898e-02 (4.5217e-02)	Acc@1  98.44 ( 98.45)	Acc@5 100.00 ( 99.99)
Epoch: [52][110/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.3732e-02 (4.5428e-02)	Acc@1  98.44 ( 98.43)	Acc@5 100.00 ( 99.99)
Epoch: [52][120/391]	Time  0.040 ( 0.025)	Data  0.003 ( 0.003)	Loss 1.2566e-02 (4.4575e-02)	Acc@1 100.00 ( 98.50)	Acc@5 100.00 ( 99.99)
Epoch: [52][130/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 2.1912e-02 (4.4371e-02)	Acc@1  99.22 ( 98.50)	Acc@5 100.00 ( 99.99)
Epoch: [52][140/391]	Time  0.035 ( 0.025)	Data  0.000 ( 0.003)	Loss 1.7609e-02 (4.3665e-02)	Acc@1 100.00 ( 98.54)	Acc@5 100.00 ( 99.98)
Epoch: [52][150/391]	Time  0.035 ( 0.025)	Data  0.003 ( 0.003)	Loss 1.1475e-01 (4.5292e-02)	Acc@1  95.31 ( 98.49)	Acc@5 100.00 ( 99.98)
Epoch: [52][160/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.1514e-02 (4.4944e-02)	Acc@1  99.22 ( 98.52)	Acc@5 100.00 ( 99.98)
Epoch: [52][170/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.8492e-02 (4.4840e-02)	Acc@1  98.44 ( 98.52)	Acc@5 100.00 ( 99.98)
Epoch: [52][180/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.2023e-02 (4.4307e-02)	Acc@1  99.22 ( 98.53)	Acc@5 100.00 ( 99.98)
Epoch: [52][190/391]	Time  0.029 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.8183e-02 (4.4695e-02)	Acc@1  99.22 ( 98.52)	Acc@5 100.00 ( 99.98)
Epoch: [52][200/391]	Time  0.023 ( 0.025)	Data  0.000 ( 0.003)	Loss 4.7668e-02 (4.5958e-02)	Acc@1  98.44 ( 98.48)	Acc@5 100.00 ( 99.98)
Epoch: [52][210/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.5654e-02 (4.5554e-02)	Acc@1  99.22 ( 98.50)	Acc@5 100.00 ( 99.99)
Epoch: [52][220/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.9153e-02 (4.5516e-02)	Acc@1  98.44 ( 98.50)	Acc@5 100.00 ( 99.99)
Epoch: [52][230/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.6678e-02 (4.5860e-02)	Acc@1 100.00 ( 98.48)	Acc@5 100.00 ( 99.98)
Epoch: [52][240/391]	Time  0.015 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.6589e-02 (4.6208e-02)	Acc@1  96.88 ( 98.47)	Acc@5 100.00 ( 99.98)
Epoch: [52][250/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.5327e-02 (4.6795e-02)	Acc@1  96.88 ( 98.46)	Acc@5 100.00 ( 99.98)
Epoch: [52][260/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.8122e-02 (4.6743e-02)	Acc@1  99.22 ( 98.47)	Acc@5 100.00 ( 99.99)
Epoch: [52][270/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.0781e-02 (4.6826e-02)	Acc@1  98.44 ( 98.47)	Acc@5 100.00 ( 99.99)
Epoch: [52][280/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.002)	Loss 7.2083e-02 (4.6700e-02)	Acc@1  96.88 ( 98.47)	Acc@5 100.00 ( 99.98)
Epoch: [52][290/391]	Time  0.029 ( 0.025)	Data  0.003 ( 0.002)	Loss 5.4901e-02 (4.6950e-02)	Acc@1  97.66 ( 98.46)	Acc@5 100.00 ( 99.98)
Epoch: [52][300/391]	Time  0.019 ( 0.024)	Data  0.001 ( 0.002)	Loss 8.4000e-03 (4.6491e-02)	Acc@1 100.00 ( 98.48)	Acc@5 100.00 ( 99.98)
Epoch: [52][310/391]	Time  0.035 ( 0.025)	Data  0.001 ( 0.002)	Loss 6.2256e-02 (4.6573e-02)	Acc@1  98.44 ( 98.48)	Acc@5 100.00 ( 99.98)
Epoch: [52][320/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.5685e-02 (4.6850e-02)	Acc@1  97.66 ( 98.47)	Acc@5 100.00 ( 99.99)
Epoch: [52][330/391]	Time  0.036 ( 0.024)	Data  0.002 ( 0.002)	Loss 1.2299e-02 (4.6829e-02)	Acc@1 100.00 ( 98.47)	Acc@5 100.00 ( 99.98)
Epoch: [52][340/391]	Time  0.015 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.4180e-02 (4.6613e-02)	Acc@1  99.22 ( 98.49)	Acc@5 100.00 ( 99.98)
Epoch: [52][350/391]	Time  0.033 ( 0.025)	Data  0.001 ( 0.002)	Loss 5.9814e-02 (4.6476e-02)	Acc@1  97.66 ( 98.49)	Acc@5 100.00 ( 99.98)
Epoch: [52][360/391]	Time  0.021 ( 0.024)	Data  0.001 ( 0.002)	Loss 8.2520e-02 (4.6678e-02)	Acc@1  96.88 ( 98.47)	Acc@5 100.00 ( 99.98)
Epoch: [52][370/391]	Time  0.027 ( 0.024)	Data  0.002 ( 0.002)	Loss 2.4170e-02 (4.6251e-02)	Acc@1  99.22 ( 98.49)	Acc@5 100.00 ( 99.98)
Epoch: [52][380/391]	Time  0.027 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.4088e-02 (4.6388e-02)	Acc@1  99.22 ( 98.48)	Acc@5 100.00 ( 99.98)
Epoch: [52][390/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.002)	Loss 9.2239e-03 (4.6689e-02)	Acc@1 100.00 ( 98.46)	Acc@5 100.00 ( 99.98)
## e[52] optimizer.zero_grad (sum) time: 0.1051645278930664
## e[52]       loss.backward (sum) time: 2.1556642055511475
## e[52]      optimizer.step (sum) time: 0.8804221153259277
## epoch[52] training(only) time: 9.635310173034668
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 3.1299e-01 (3.1299e-01)	Acc@1  89.00 ( 89.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.017 ( 0.027)	Loss 4.0649e-01 (3.7026e-01)	Acc@1  90.00 ( 90.55)	Acc@5  99.00 ( 99.55)
Test: [ 20/100]	Time  0.019 ( 0.021)	Loss 4.9194e-01 (3.9419e-01)	Acc@1  86.00 ( 90.24)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.011 ( 0.018)	Loss 3.8867e-01 (4.0310e-01)	Acc@1  90.00 ( 90.10)	Acc@5  98.00 ( 99.58)
Test: [ 40/100]	Time  0.023 ( 0.017)	Loss 4.6021e-01 (4.2538e-01)	Acc@1  90.00 ( 89.78)	Acc@5  99.00 ( 99.54)
Test: [ 50/100]	Time  0.023 ( 0.016)	Loss 1.3257e-01 (4.0431e-01)	Acc@1  96.00 ( 90.29)	Acc@5 100.00 ( 99.55)
Test: [ 60/100]	Time  0.022 ( 0.016)	Loss 2.6245e-01 (4.0241e-01)	Acc@1  94.00 ( 90.23)	Acc@5 100.00 ( 99.57)
Test: [ 70/100]	Time  0.010 ( 0.015)	Loss 6.2549e-01 (4.0238e-01)	Acc@1  88.00 ( 90.21)	Acc@5  99.00 ( 99.58)
Test: [ 80/100]	Time  0.016 ( 0.015)	Loss 2.9858e-01 (3.9789e-01)	Acc@1  90.00 ( 90.30)	Acc@5 100.00 ( 99.59)
Test: [ 90/100]	Time  0.011 ( 0.015)	Loss 3.1519e-01 (3.9437e-01)	Acc@1  91.00 ( 90.23)	Acc@5 100.00 ( 99.64)
 * Acc@1 90.250 Acc@5 99.660
### epoch[52] execution time: 11.205173254013062
EPOCH 53
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [53][  0/391]	Time  0.253 ( 0.253)	Data  0.228 ( 0.228)	Loss 1.1267e-01 (1.1267e-01)	Acc@1  96.88 ( 96.88)	Acc@5  99.22 ( 99.22)
Epoch: [53][ 10/391]	Time  0.028 ( 0.045)	Data  0.001 ( 0.022)	Loss 4.1443e-02 (5.0455e-02)	Acc@1  99.22 ( 98.44)	Acc@5 100.00 ( 99.86)
Epoch: [53][ 20/391]	Time  0.019 ( 0.034)	Data  0.002 ( 0.012)	Loss 5.9418e-02 (5.0397e-02)	Acc@1  97.66 ( 98.36)	Acc@5 100.00 ( 99.93)
Epoch: [53][ 30/391]	Time  0.033 ( 0.031)	Data  0.001 ( 0.009)	Loss 4.2236e-02 (4.6363e-02)	Acc@1  97.66 ( 98.34)	Acc@5 100.00 ( 99.95)
Epoch: [53][ 40/391]	Time  0.018 ( 0.029)	Data  0.001 ( 0.007)	Loss 1.2854e-01 (5.1679e-02)	Acc@1  96.09 ( 98.19)	Acc@5 100.00 ( 99.96)
Epoch: [53][ 50/391]	Time  0.028 ( 0.028)	Data  0.001 ( 0.006)	Loss 2.0508e-02 (5.1666e-02)	Acc@1  99.22 ( 98.25)	Acc@5 100.00 ( 99.97)
Epoch: [53][ 60/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.006)	Loss 4.5807e-02 (5.1191e-02)	Acc@1  99.22 ( 98.27)	Acc@5 100.00 ( 99.97)
Epoch: [53][ 70/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.005)	Loss 6.5491e-02 (5.1174e-02)	Acc@1  98.44 ( 98.29)	Acc@5  99.22 ( 99.97)
Epoch: [53][ 80/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.005)	Loss 1.2036e-01 (4.9707e-02)	Acc@1  95.31 ( 98.35)	Acc@5 100.00 ( 99.97)
Epoch: [53][ 90/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 4.7211e-02 (5.0167e-02)	Acc@1  98.44 ( 98.34)	Acc@5 100.00 ( 99.97)
Epoch: [53][100/391]	Time  0.041 ( 0.026)	Data  0.001 ( 0.004)	Loss 5.5389e-02 (4.9343e-02)	Acc@1  97.66 ( 98.37)	Acc@5 100.00 ( 99.97)
Epoch: [53][110/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.004)	Loss 5.7434e-02 (4.8741e-02)	Acc@1  98.44 ( 98.40)	Acc@5 100.00 ( 99.97)
Epoch: [53][120/391]	Time  0.017 ( 0.026)	Data  0.000 ( 0.004)	Loss 5.2246e-02 (4.7859e-02)	Acc@1  98.44 ( 98.41)	Acc@5 100.00 ( 99.97)
Epoch: [53][130/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.8157e-02 (4.7846e-02)	Acc@1  97.66 ( 98.39)	Acc@5 100.00 ( 99.98)
Epoch: [53][140/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.8219e-02 (4.7737e-02)	Acc@1  99.22 ( 98.38)	Acc@5 100.00 ( 99.98)
Epoch: [53][150/391]	Time  0.038 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.3660e-02 (4.6927e-02)	Acc@1  98.44 ( 98.40)	Acc@5 100.00 ( 99.98)
Epoch: [53][160/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.9221e-02 (4.5866e-02)	Acc@1  99.22 ( 98.44)	Acc@5 100.00 ( 99.98)
Epoch: [53][170/391]	Time  0.027 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.5685e-02 (4.5716e-02)	Acc@1  98.44 ( 98.41)	Acc@5 100.00 ( 99.98)
Epoch: [53][180/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.8481e-02 (4.5887e-02)	Acc@1  97.66 ( 98.41)	Acc@5 100.00 ( 99.98)
Epoch: [53][190/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 5.1514e-02 (4.5728e-02)	Acc@1  98.44 ( 98.42)	Acc@5 100.00 ( 99.98)
Epoch: [53][200/391]	Time  0.033 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.8961e-02 (4.5665e-02)	Acc@1  97.66 ( 98.41)	Acc@5 100.00 ( 99.98)
Epoch: [53][210/391]	Time  0.033 ( 0.025)	Data  0.003 ( 0.003)	Loss 7.8857e-02 (4.6479e-02)	Acc@1  96.09 ( 98.38)	Acc@5 100.00 ( 99.99)
Epoch: [53][220/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 7.6416e-02 (4.6698e-02)	Acc@1  97.66 ( 98.38)	Acc@5 100.00 ( 99.98)
Epoch: [53][230/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 3.5431e-02 (4.6294e-02)	Acc@1  98.44 ( 98.38)	Acc@5 100.00 ( 99.98)
Epoch: [53][240/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 1.9241e-02 (4.6293e-02)	Acc@1  99.22 ( 98.38)	Acc@5 100.00 ( 99.98)
Epoch: [53][250/391]	Time  0.024 ( 0.025)	Data  0.002 ( 0.003)	Loss 5.6488e-02 (4.6285e-02)	Acc@1  98.44 ( 98.38)	Acc@5 100.00 ( 99.98)
Epoch: [53][260/391]	Time  0.042 ( 0.025)	Data  0.011 ( 0.003)	Loss 2.6337e-02 (4.6520e-02)	Acc@1  99.22 ( 98.38)	Acc@5 100.00 ( 99.99)
Epoch: [53][270/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.2214e-02 (4.6686e-02)	Acc@1  96.88 ( 98.37)	Acc@5 100.00 ( 99.99)
Epoch: [53][280/391]	Time  0.024 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.4392e-02 (4.6955e-02)	Acc@1  97.66 ( 98.37)	Acc@5 100.00 ( 99.99)
Epoch: [53][290/391]	Time  0.035 ( 0.025)	Data  0.004 ( 0.003)	Loss 5.3223e-02 (4.6681e-02)	Acc@1  98.44 ( 98.37)	Acc@5 100.00 ( 99.99)
Epoch: [53][300/391]	Time  0.030 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.3722e-02 (4.7049e-02)	Acc@1  97.66 ( 98.37)	Acc@5 100.00 ( 99.98)
Epoch: [53][310/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.1412e-02 (4.7075e-02)	Acc@1  97.66 ( 98.37)	Acc@5 100.00 ( 99.98)
Epoch: [53][320/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.0234e-03 (4.6731e-02)	Acc@1 100.00 ( 98.39)	Acc@5 100.00 ( 99.98)
Epoch: [53][330/391]	Time  0.028 ( 0.025)	Data  0.003 ( 0.003)	Loss 5.8350e-02 (4.6622e-02)	Acc@1  98.44 ( 98.39)	Acc@5 100.00 ( 99.98)
Epoch: [53][340/391]	Time  0.017 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.9022e-02 (4.6279e-02)	Acc@1  99.22 ( 98.40)	Acc@5 100.00 ( 99.98)
Epoch: [53][350/391]	Time  0.024 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.2694e-02 (4.6648e-02)	Acc@1  99.22 ( 98.40)	Acc@5 100.00 ( 99.98)
Epoch: [53][360/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.3758e-02 (4.6597e-02)	Acc@1  99.22 ( 98.40)	Acc@5 100.00 ( 99.98)
Epoch: [53][370/391]	Time  0.025 ( 0.025)	Data  0.000 ( 0.003)	Loss 2.3575e-02 (4.6815e-02)	Acc@1  99.22 ( 98.40)	Acc@5 100.00 ( 99.98)
Epoch: [53][380/391]	Time  0.027 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.6885e-02 (4.6792e-02)	Acc@1  98.44 ( 98.40)	Acc@5 100.00 ( 99.98)
Epoch: [53][390/391]	Time  0.017 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.2854e-01 (4.6983e-02)	Acc@1  96.25 ( 98.40)	Acc@5 100.00 ( 99.98)
## e[53] optimizer.zero_grad (sum) time: 0.10506272315979004
## e[53]       loss.backward (sum) time: 2.144134521484375
## e[53]      optimizer.step (sum) time: 0.88053297996521
## epoch[53] training(only) time: 9.7160325050354
# Switched to evaluate mode...
Test: [  0/100]	Time  0.162 ( 0.162)	Loss 3.4058e-01 (3.4058e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.017 ( 0.027)	Loss 4.5312e-01 (3.8495e-01)	Acc@1  89.00 ( 89.55)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.012 ( 0.020)	Loss 5.0830e-01 (4.0912e-01)	Acc@1  86.00 ( 89.38)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.016 ( 0.018)	Loss 3.7329e-01 (4.0996e-01)	Acc@1  90.00 ( 89.71)	Acc@5 100.00 ( 99.58)
Test: [ 40/100]	Time  0.011 ( 0.017)	Loss 5.8496e-01 (4.2923e-01)	Acc@1  87.00 ( 89.56)	Acc@5 100.00 ( 99.61)
Test: [ 50/100]	Time  0.009 ( 0.016)	Loss 1.5967e-01 (4.1107e-01)	Acc@1  94.00 ( 89.98)	Acc@5 100.00 ( 99.61)
Test: [ 60/100]	Time  0.024 ( 0.015)	Loss 3.3496e-01 (4.1376e-01)	Acc@1  90.00 ( 89.75)	Acc@5 100.00 ( 99.61)
Test: [ 70/100]	Time  0.009 ( 0.015)	Loss 7.1191e-01 (4.1335e-01)	Acc@1  87.00 ( 89.80)	Acc@5  99.00 ( 99.61)
Test: [ 80/100]	Time  0.013 ( 0.015)	Loss 2.6611e-01 (4.0602e-01)	Acc@1  91.00 ( 89.98)	Acc@5 100.00 ( 99.59)
Test: [ 90/100]	Time  0.013 ( 0.015)	Loss 2.6270e-01 (4.0428e-01)	Acc@1  94.00 ( 89.96)	Acc@5 100.00 ( 99.64)
 * Acc@1 90.000 Acc@5 99.670
### epoch[53] execution time: 11.28893780708313
EPOCH 54
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [54][  0/391]	Time  0.197 ( 0.197)	Data  0.175 ( 0.175)	Loss 8.7036e-02 (8.7036e-02)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [54][ 10/391]	Time  0.019 ( 0.039)	Data  0.001 ( 0.017)	Loss 4.7119e-02 (5.6904e-02)	Acc@1  99.22 ( 98.22)	Acc@5 100.00 ( 99.93)
Epoch: [54][ 20/391]	Time  0.019 ( 0.032)	Data  0.001 ( 0.010)	Loss 1.6418e-02 (4.8544e-02)	Acc@1  99.22 ( 98.40)	Acc@5 100.00 ( 99.96)
Epoch: [54][ 30/391]	Time  0.024 ( 0.029)	Data  0.002 ( 0.007)	Loss 7.6050e-02 (4.3643e-02)	Acc@1  97.66 ( 98.54)	Acc@5 100.00 ( 99.97)
Epoch: [54][ 40/391]	Time  0.018 ( 0.028)	Data  0.001 ( 0.006)	Loss 1.1511e-01 (4.3513e-02)	Acc@1  96.88 ( 98.51)	Acc@5 100.00 ( 99.98)
Epoch: [54][ 50/391]	Time  0.017 ( 0.027)	Data  0.000 ( 0.005)	Loss 1.6205e-02 (4.0547e-02)	Acc@1 100.00 ( 98.65)	Acc@5 100.00 ( 99.98)
Epoch: [54][ 60/391]	Time  0.030 ( 0.027)	Data  0.001 ( 0.005)	Loss 1.0872e-02 (4.0918e-02)	Acc@1 100.00 ( 98.60)	Acc@5 100.00 ( 99.99)
Epoch: [54][ 70/391]	Time  0.025 ( 0.026)	Data  0.003 ( 0.004)	Loss 7.2510e-02 (4.0341e-02)	Acc@1  97.66 ( 98.62)	Acc@5  99.22 ( 99.98)
Epoch: [54][ 80/391]	Time  0.016 ( 0.026)	Data  0.001 ( 0.004)	Loss 3.7262e-02 (4.1637e-02)	Acc@1  98.44 ( 98.58)	Acc@5 100.00 ( 99.98)
Epoch: [54][ 90/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 3.1219e-02 (4.2060e-02)	Acc@1  99.22 ( 98.60)	Acc@5 100.00 ( 99.98)
Epoch: [54][100/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.9734e-02 (4.2100e-02)	Acc@1  98.44 ( 98.58)	Acc@5 100.00 ( 99.98)
Epoch: [54][110/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 9.5367e-03 (4.1623e-02)	Acc@1 100.00 ( 98.62)	Acc@5 100.00 ( 99.99)
Epoch: [54][120/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.7791e-02 (4.1857e-02)	Acc@1  98.44 ( 98.61)	Acc@5 100.00 ( 99.99)
Epoch: [54][130/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.4922e-02 (4.2198e-02)	Acc@1  98.44 ( 98.63)	Acc@5 100.00 ( 99.99)
Epoch: [54][140/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.3171e-02 (4.1866e-02)	Acc@1  98.44 ( 98.63)	Acc@5 100.00 ( 99.99)
Epoch: [54][150/391]	Time  0.022 ( 0.025)	Data  0.004 ( 0.003)	Loss 4.3915e-02 (4.2835e-02)	Acc@1  99.22 ( 98.60)	Acc@5 100.00 ( 99.99)
Epoch: [54][160/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.8176e-02 (4.3434e-02)	Acc@1  96.09 ( 98.56)	Acc@5 100.00 ( 99.99)
Epoch: [54][170/391]	Time  0.042 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.3367e-01 (4.5750e-02)	Acc@1  95.31 ( 98.49)	Acc@5 100.00 ( 99.99)
Epoch: [54][180/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.3142e-02 (4.5490e-02)	Acc@1  99.22 ( 98.49)	Acc@5 100.00 ( 99.99)
Epoch: [54][190/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.5013e-02 (4.5749e-02)	Acc@1  98.44 ( 98.45)	Acc@5 100.00 ( 99.99)
Epoch: [54][200/391]	Time  0.016 ( 0.025)	Data  0.000 ( 0.003)	Loss 2.8336e-02 (4.5190e-02)	Acc@1  99.22 ( 98.47)	Acc@5 100.00 ( 99.99)
Epoch: [54][210/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.0674e-02 (4.5631e-02)	Acc@1 100.00 ( 98.45)	Acc@5 100.00 ( 99.99)
Epoch: [54][220/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.4037e-02 (4.5339e-02)	Acc@1  98.44 ( 98.46)	Acc@5 100.00 ( 99.99)
Epoch: [54][230/391]	Time  0.024 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.3844e-02 (4.4995e-02)	Acc@1  99.22 ( 98.49)	Acc@5 100.00 ( 99.99)
Epoch: [54][240/391]	Time  0.018 ( 0.025)	Data  0.002 ( 0.003)	Loss 5.2643e-02 (4.5753e-02)	Acc@1  98.44 ( 98.46)	Acc@5 100.00 ( 99.99)
Epoch: [54][250/391]	Time  0.034 ( 0.025)	Data  0.005 ( 0.003)	Loss 4.0192e-02 (4.5422e-02)	Acc@1  99.22 ( 98.47)	Acc@5 100.00 ( 99.99)
Epoch: [54][260/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.1497e-02 (4.5225e-02)	Acc@1 100.00 ( 98.47)	Acc@5 100.00 ( 99.99)
Epoch: [54][270/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 7.3425e-02 (4.5469e-02)	Acc@1  96.88 ( 98.47)	Acc@5 100.00 ( 99.99)
Epoch: [54][280/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 5.1270e-02 (4.5580e-02)	Acc@1  97.66 ( 98.46)	Acc@5 100.00 ( 99.99)
Epoch: [54][290/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.5116e-02 (4.5670e-02)	Acc@1  99.22 ( 98.46)	Acc@5 100.00 ( 99.98)
Epoch: [54][300/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.0975e-02 (4.5444e-02)	Acc@1  99.22 ( 98.46)	Acc@5 100.00 ( 99.98)
Epoch: [54][310/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.4067e-02 (4.5215e-02)	Acc@1  99.22 ( 98.47)	Acc@5 100.00 ( 99.98)
Epoch: [54][320/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.002)	Loss 9.4971e-02 (4.5603e-02)	Acc@1  96.88 ( 98.45)	Acc@5 100.00 ( 99.98)
Epoch: [54][330/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 4.2633e-02 (4.5292e-02)	Acc@1  98.44 ( 98.46)	Acc@5 100.00 ( 99.98)
Epoch: [54][340/391]	Time  0.021 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.1487e-01 (4.5378e-02)	Acc@1  95.31 ( 98.45)	Acc@5 100.00 ( 99.98)
Epoch: [54][350/391]	Time  0.039 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.2593e-02 (4.5056e-02)	Acc@1  99.22 ( 98.47)	Acc@5 100.00 ( 99.98)
Epoch: [54][360/391]	Time  0.019 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.5330e-02 (4.4829e-02)	Acc@1  99.22 ( 98.47)	Acc@5 100.00 ( 99.98)
Epoch: [54][370/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 8.9905e-02 (4.5360e-02)	Acc@1  95.31 ( 98.46)	Acc@5 100.00 ( 99.98)
Epoch: [54][380/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.2104e-02 (4.5429e-02)	Acc@1  98.44 ( 98.45)	Acc@5 100.00 ( 99.98)
Epoch: [54][390/391]	Time  0.016 ( 0.024)	Data  0.000 ( 0.002)	Loss 4.0802e-02 (4.5762e-02)	Acc@1  98.75 ( 98.43)	Acc@5 100.00 ( 99.98)
## e[54] optimizer.zero_grad (sum) time: 0.10656118392944336
## e[54]       loss.backward (sum) time: 2.1622095108032227
## e[54]      optimizer.step (sum) time: 0.8883366584777832
## epoch[54] training(only) time: 9.660235166549683
# Switched to evaluate mode...
Test: [  0/100]	Time  0.166 ( 0.166)	Loss 3.2080e-01 (3.2080e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.008 ( 0.026)	Loss 5.1025e-01 (3.8898e-01)	Acc@1  89.00 ( 90.00)	Acc@5 100.00 ( 99.55)
Test: [ 20/100]	Time  0.007 ( 0.020)	Loss 4.5581e-01 (4.0447e-01)	Acc@1  87.00 ( 90.00)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.023 ( 0.018)	Loss 4.1089e-01 (4.0245e-01)	Acc@1  90.00 ( 90.16)	Acc@5  99.00 ( 99.48)
Test: [ 40/100]	Time  0.008 ( 0.017)	Loss 4.6289e-01 (4.2191e-01)	Acc@1  90.00 ( 89.98)	Acc@5  99.00 ( 99.46)
Test: [ 50/100]	Time  0.022 ( 0.016)	Loss 1.3074e-01 (4.0327e-01)	Acc@1  95.00 ( 90.35)	Acc@5 100.00 ( 99.47)
Test: [ 60/100]	Time  0.007 ( 0.015)	Loss 3.0176e-01 (4.0385e-01)	Acc@1  91.00 ( 90.21)	Acc@5 100.00 ( 99.51)
Test: [ 70/100]	Time  0.014 ( 0.015)	Loss 6.4844e-01 (3.9788e-01)	Acc@1  87.00 ( 90.39)	Acc@5 100.00 ( 99.55)
Test: [ 80/100]	Time  0.016 ( 0.015)	Loss 1.8726e-01 (3.9294e-01)	Acc@1  96.00 ( 90.46)	Acc@5 100.00 ( 99.57)
Test: [ 90/100]	Time  0.027 ( 0.015)	Loss 2.5537e-01 (3.8823e-01)	Acc@1  93.00 ( 90.45)	Acc@5 100.00 ( 99.62)
 * Acc@1 90.370 Acc@5 99.650
### epoch[54] execution time: 11.225628137588501
EPOCH 55
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [55][  0/391]	Time  0.200 ( 0.200)	Data  0.179 ( 0.179)	Loss 4.2603e-02 (4.2603e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [55][ 10/391]	Time  0.026 ( 0.041)	Data  0.004 ( 0.018)	Loss 3.3966e-02 (2.6646e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [55][ 20/391]	Time  0.020 ( 0.033)	Data  0.001 ( 0.010)	Loss 3.5065e-02 (3.5775e-02)	Acc@1  98.44 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [55][ 30/391]	Time  0.028 ( 0.030)	Data  0.001 ( 0.008)	Loss 1.0239e-02 (3.4883e-02)	Acc@1 100.00 ( 98.74)	Acc@5 100.00 (100.00)
Epoch: [55][ 40/391]	Time  0.018 ( 0.028)	Data  0.001 ( 0.006)	Loss 8.9722e-02 (3.4829e-02)	Acc@1  96.88 ( 98.70)	Acc@5 100.00 (100.00)
Epoch: [55][ 50/391]	Time  0.034 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.6550e-02 (3.4316e-02)	Acc@1  99.22 ( 98.76)	Acc@5 100.00 (100.00)
Epoch: [55][ 60/391]	Time  0.036 ( 0.027)	Data  0.001 ( 0.005)	Loss 3.5675e-02 (3.5723e-02)	Acc@1  98.44 ( 98.71)	Acc@5 100.00 (100.00)
Epoch: [55][ 70/391]	Time  0.035 ( 0.026)	Data  0.000 ( 0.004)	Loss 6.8542e-02 (3.7761e-02)	Acc@1  98.44 ( 98.62)	Acc@5 100.00 (100.00)
Epoch: [55][ 80/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 3.8757e-02 (3.9355e-02)	Acc@1  98.44 ( 98.60)	Acc@5 100.00 (100.00)
Epoch: [55][ 90/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.004)	Loss 4.8187e-02 (3.9614e-02)	Acc@1  98.44 ( 98.55)	Acc@5 100.00 (100.00)
Epoch: [55][100/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.004)	Loss 7.9773e-02 (4.0773e-02)	Acc@1  96.09 ( 98.53)	Acc@5 100.00 (100.00)
Epoch: [55][110/391]	Time  0.036 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.0627e-02 (4.1579e-02)	Acc@1  97.66 ( 98.53)	Acc@5 100.00 (100.00)
Epoch: [55][120/391]	Time  0.023 ( 0.025)	Data  0.005 ( 0.003)	Loss 4.2175e-02 (4.1193e-02)	Acc@1  98.44 ( 98.53)	Acc@5 100.00 (100.00)
Epoch: [55][130/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.3000e-02 (4.1426e-02)	Acc@1 100.00 ( 98.54)	Acc@5 100.00 (100.00)
Epoch: [55][140/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 2.3071e-02 (4.0947e-02)	Acc@1  99.22 ( 98.55)	Acc@5 100.00 (100.00)
Epoch: [55][150/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.8030e-02 (4.1502e-02)	Acc@1  98.44 ( 98.55)	Acc@5 100.00 (100.00)
Epoch: [55][160/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.1194e-02 (4.1387e-02)	Acc@1 100.00 ( 98.56)	Acc@5 100.00 (100.00)
Epoch: [55][170/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.6321e-02 (4.3422e-02)	Acc@1  99.22 ( 98.52)	Acc@5 100.00 (100.00)
Epoch: [55][180/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.2145e-02 (4.3190e-02)	Acc@1  98.44 ( 98.52)	Acc@5 100.00 (100.00)
Epoch: [55][190/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.8188e-02 (4.2980e-02)	Acc@1  99.22 ( 98.52)	Acc@5 100.00 (100.00)
Epoch: [55][200/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.1982e-02 (4.2811e-02)	Acc@1  99.22 ( 98.52)	Acc@5 100.00 (100.00)
Epoch: [55][210/391]	Time  0.047 ( 0.025)	Data  0.012 ( 0.003)	Loss 5.0598e-02 (4.3011e-02)	Acc@1  97.66 ( 98.50)	Acc@5 100.00 (100.00)
Epoch: [55][220/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.3243e-02 (4.2994e-02)	Acc@1  98.44 ( 98.51)	Acc@5 100.00 (100.00)
Epoch: [55][230/391]	Time  0.033 ( 0.025)	Data  0.002 ( 0.003)	Loss 3.2104e-02 (4.3016e-02)	Acc@1  98.44 ( 98.52)	Acc@5 100.00 (100.00)
Epoch: [55][240/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.0742e-02 (4.3610e-02)	Acc@1 100.00 ( 98.50)	Acc@5 100.00 ( 99.99)
Epoch: [55][250/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.3107e-02 (4.3235e-02)	Acc@1 100.00 ( 98.51)	Acc@5 100.00 ( 99.99)
Epoch: [55][260/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.9333e-02 (4.3148e-02)	Acc@1  99.22 ( 98.52)	Acc@5 100.00 ( 99.99)
Epoch: [55][270/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 5.2185e-02 (4.3258e-02)	Acc@1  98.44 ( 98.52)	Acc@5 100.00 ( 99.99)
Epoch: [55][280/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.002)	Loss 3.7781e-02 (4.3004e-02)	Acc@1  98.44 ( 98.54)	Acc@5 100.00 ( 99.99)
Epoch: [55][290/391]	Time  0.033 ( 0.025)	Data  0.002 ( 0.002)	Loss 8.9111e-03 (4.3166e-02)	Acc@1 100.00 ( 98.53)	Acc@5 100.00 ( 99.99)
Epoch: [55][300/391]	Time  0.031 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.8483e-02 (4.2847e-02)	Acc@1  97.66 ( 98.54)	Acc@5 100.00 ( 99.99)
Epoch: [55][310/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 6.1981e-02 (4.3227e-02)	Acc@1  96.88 ( 98.53)	Acc@5 100.00 ( 99.99)
Epoch: [55][320/391]	Time  0.017 ( 0.024)	Data  0.000 ( 0.002)	Loss 2.4033e-02 (4.3324e-02)	Acc@1  98.44 ( 98.53)	Acc@5 100.00 (100.00)
Epoch: [55][330/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.002)	Loss 5.1331e-02 (4.2946e-02)	Acc@1  97.66 ( 98.54)	Acc@5 100.00 (100.00)
Epoch: [55][340/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.6586e-02 (4.3387e-02)	Acc@1  99.22 ( 98.52)	Acc@5 100.00 (100.00)
Epoch: [55][350/391]	Time  0.019 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.3264e-02 (4.3203e-02)	Acc@1  99.22 ( 98.52)	Acc@5 100.00 (100.00)
Epoch: [55][360/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.0701e-02 (4.3442e-02)	Acc@1  98.44 ( 98.51)	Acc@5 100.00 (100.00)
Epoch: [55][370/391]	Time  0.022 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.8839e-02 (4.3658e-02)	Acc@1  99.22 ( 98.51)	Acc@5 100.00 ( 99.99)
Epoch: [55][380/391]	Time  0.032 ( 0.024)	Data  0.002 ( 0.002)	Loss 4.7974e-02 (4.3476e-02)	Acc@1  98.44 ( 98.51)	Acc@5 100.00 ( 99.99)
Epoch: [55][390/391]	Time  0.014 ( 0.024)	Data  0.001 ( 0.002)	Loss 7.5874e-03 (4.3759e-02)	Acc@1 100.00 ( 98.50)	Acc@5 100.00 ( 99.99)
## e[55] optimizer.zero_grad (sum) time: 0.10402488708496094
## e[55]       loss.backward (sum) time: 2.1741299629211426
## e[55]      optimizer.step (sum) time: 0.890540599822998
## epoch[55] training(only) time: 9.634581089019775
# Switched to evaluate mode...
Test: [  0/100]	Time  0.165 ( 0.165)	Loss 2.6147e-01 (2.6147e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.010 ( 0.028)	Loss 4.1699e-01 (3.8002e-01)	Acc@1  93.00 ( 91.45)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.008 ( 0.020)	Loss 5.1904e-01 (4.0044e-01)	Acc@1  87.00 ( 90.71)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.020 ( 0.019)	Loss 4.1431e-01 (4.0738e-01)	Acc@1  89.00 ( 90.55)	Acc@5  99.00 ( 99.58)
Test: [ 40/100]	Time  0.014 ( 0.017)	Loss 4.8096e-01 (4.2441e-01)	Acc@1  86.00 ( 90.15)	Acc@5  99.00 ( 99.56)
Test: [ 50/100]	Time  0.009 ( 0.016)	Loss 1.3208e-01 (4.0698e-01)	Acc@1  96.00 ( 90.51)	Acc@5 100.00 ( 99.55)
Test: [ 60/100]	Time  0.019 ( 0.016)	Loss 2.9053e-01 (4.0801e-01)	Acc@1  93.00 ( 90.38)	Acc@5 100.00 ( 99.57)
Test: [ 70/100]	Time  0.009 ( 0.015)	Loss 6.4404e-01 (4.0373e-01)	Acc@1  89.00 ( 90.54)	Acc@5 100.00 ( 99.61)
Test: [ 80/100]	Time  0.008 ( 0.015)	Loss 2.8442e-01 (4.0052e-01)	Acc@1  93.00 ( 90.57)	Acc@5 100.00 ( 99.59)
Test: [ 90/100]	Time  0.008 ( 0.015)	Loss 3.0737e-01 (3.9579e-01)	Acc@1  91.00 ( 90.43)	Acc@5 100.00 ( 99.64)
 * Acc@1 90.430 Acc@5 99.670
### epoch[55] execution time: 11.216017484664917
EPOCH 56
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [56][  0/391]	Time  0.186 ( 0.186)	Data  0.165 ( 0.165)	Loss 4.9591e-02 (4.9591e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [56][ 10/391]	Time  0.027 ( 0.039)	Data  0.001 ( 0.017)	Loss 8.5144e-03 (5.0871e-02)	Acc@1 100.00 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [56][ 20/391]	Time  0.018 ( 0.031)	Data  0.001 ( 0.010)	Loss 5.4871e-02 (6.0009e-02)	Acc@1  97.66 ( 97.88)	Acc@5 100.00 ( 99.96)
Epoch: [56][ 30/391]	Time  0.024 ( 0.029)	Data  0.001 ( 0.007)	Loss 6.3843e-02 (5.7374e-02)	Acc@1  97.66 ( 98.03)	Acc@5 100.00 ( 99.97)
Epoch: [56][ 40/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.006)	Loss 1.0303e-01 (5.9241e-02)	Acc@1  96.88 ( 98.04)	Acc@5 100.00 ( 99.98)
Epoch: [56][ 50/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.005)	Loss 2.7237e-02 (5.5028e-02)	Acc@1  99.22 ( 98.19)	Acc@5 100.00 ( 99.98)
Epoch: [56][ 60/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 2.7023e-02 (5.2909e-02)	Acc@1  99.22 ( 98.25)	Acc@5 100.00 ( 99.99)
Epoch: [56][ 70/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 5.5084e-02 (5.1222e-02)	Acc@1  98.44 ( 98.27)	Acc@5 100.00 ( 99.99)
Epoch: [56][ 80/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.5007e-02 (4.7844e-02)	Acc@1  99.22 ( 98.38)	Acc@5 100.00 ( 99.99)
Epoch: [56][ 90/391]	Time  0.029 ( 0.025)	Data  0.001 ( 0.004)	Loss 2.6794e-02 (4.7358e-02)	Acc@1  99.22 ( 98.38)	Acc@5 100.00 ( 99.99)
Epoch: [56][100/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.9042e-02 (4.6202e-02)	Acc@1  97.66 ( 98.42)	Acc@5 100.00 ( 99.99)
Epoch: [56][110/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.7463e-02 (4.5994e-02)	Acc@1  99.22 ( 98.47)	Acc@5 100.00 ( 99.99)
Epoch: [56][120/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.5152e-02 (4.5126e-02)	Acc@1  99.22 ( 98.47)	Acc@5 100.00 ( 99.99)
Epoch: [56][130/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.0472e-02 (4.5050e-02)	Acc@1  99.22 ( 98.48)	Acc@5 100.00 ( 99.99)
Epoch: [56][140/391]	Time  0.024 ( 0.025)	Data  0.003 ( 0.003)	Loss 5.7495e-02 (4.4728e-02)	Acc@1  98.44 ( 98.50)	Acc@5 100.00 ( 99.99)
Epoch: [56][150/391]	Time  0.046 ( 0.025)	Data  0.008 ( 0.003)	Loss 1.2405e-02 (4.4625e-02)	Acc@1  99.22 ( 98.48)	Acc@5 100.00 ( 99.99)
Epoch: [56][160/391]	Time  0.023 ( 0.025)	Data  0.000 ( 0.003)	Loss 5.1117e-02 (4.4659e-02)	Acc@1  99.22 ( 98.47)	Acc@5 100.00 (100.00)
Epoch: [56][170/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.6566e-02 (4.4787e-02)	Acc@1  99.22 ( 98.50)	Acc@5 100.00 ( 99.99)
Epoch: [56][180/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.1082e-02 (4.4646e-02)	Acc@1  99.22 ( 98.51)	Acc@5 100.00 ( 99.99)
Epoch: [56][190/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.9948e-02 (4.4606e-02)	Acc@1  99.22 ( 98.52)	Acc@5 100.00 ( 99.98)
Epoch: [56][200/391]	Time  0.024 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.0518e-02 (4.4261e-02)	Acc@1  99.22 ( 98.53)	Acc@5 100.00 ( 99.98)
Epoch: [56][210/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.0985e-02 (4.3451e-02)	Acc@1  98.44 ( 98.54)	Acc@5 100.00 ( 99.99)
Epoch: [56][220/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.9214e-02 (4.3239e-02)	Acc@1  98.44 ( 98.56)	Acc@5 100.00 ( 99.99)
Epoch: [56][230/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.2400e-02 (4.3353e-02)	Acc@1  98.44 ( 98.55)	Acc@5 100.00 ( 99.99)
Epoch: [56][240/391]	Time  0.037 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.3173e-02 (4.2898e-02)	Acc@1  98.44 ( 98.57)	Acc@5 100.00 ( 99.99)
Epoch: [56][250/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 3.3386e-02 (4.2606e-02)	Acc@1  99.22 ( 98.58)	Acc@5 100.00 ( 99.98)
Epoch: [56][260/391]	Time  0.026 ( 0.025)	Data  0.000 ( 0.002)	Loss 6.3110e-02 (4.2691e-02)	Acc@1  99.22 ( 98.58)	Acc@5 100.00 ( 99.99)
Epoch: [56][270/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.0029e-02 (4.2297e-02)	Acc@1  99.22 ( 98.60)	Acc@5 100.00 ( 99.99)
Epoch: [56][280/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.1107e-02 (4.2522e-02)	Acc@1  99.22 ( 98.58)	Acc@5 100.00 ( 99.99)
Epoch: [56][290/391]	Time  0.027 ( 0.025)	Data  0.001 ( 0.002)	Loss 7.2510e-02 (4.2950e-02)	Acc@1  97.66 ( 98.57)	Acc@5 100.00 ( 99.98)
Epoch: [56][300/391]	Time  0.041 ( 0.024)	Data  0.009 ( 0.002)	Loss 5.2032e-02 (4.2712e-02)	Acc@1  97.66 ( 98.58)	Acc@5 100.00 ( 99.98)
Epoch: [56][310/391]	Time  0.036 ( 0.024)	Data  0.002 ( 0.002)	Loss 6.5552e-02 (4.2679e-02)	Acc@1  96.88 ( 98.58)	Acc@5 100.00 ( 99.98)
Epoch: [56][320/391]	Time  0.018 ( 0.024)	Data  0.002 ( 0.002)	Loss 3.7537e-02 (4.2490e-02)	Acc@1  98.44 ( 98.59)	Acc@5 100.00 ( 99.98)
Epoch: [56][330/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.5492e-02 (4.2475e-02)	Acc@1  98.44 ( 98.59)	Acc@5 100.00 ( 99.98)
Epoch: [56][340/391]	Time  0.019 ( 0.024)	Data  0.001 ( 0.002)	Loss 5.6091e-02 (4.2575e-02)	Acc@1  96.88 ( 98.58)	Acc@5 100.00 ( 99.98)
Epoch: [56][350/391]	Time  0.028 ( 0.024)	Data  0.001 ( 0.002)	Loss 4.2938e-02 (4.2546e-02)	Acc@1  98.44 ( 98.57)	Acc@5 100.00 ( 99.98)
Epoch: [56][360/391]	Time  0.034 ( 0.024)	Data  0.003 ( 0.002)	Loss 4.3854e-02 (4.2360e-02)	Acc@1  98.44 ( 98.57)	Acc@5 100.00 ( 99.98)
Epoch: [56][370/391]	Time  0.044 ( 0.024)	Data  0.004 ( 0.002)	Loss 7.1472e-02 (4.2155e-02)	Acc@1  96.88 ( 98.58)	Acc@5  99.22 ( 99.98)
Epoch: [56][380/391]	Time  0.031 ( 0.024)	Data  0.000 ( 0.002)	Loss 6.4209e-02 (4.2123e-02)	Acc@1  97.66 ( 98.58)	Acc@5 100.00 ( 99.98)
Epoch: [56][390/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.1896e-02 (4.1925e-02)	Acc@1 100.00 ( 98.59)	Acc@5 100.00 ( 99.98)
## e[56] optimizer.zero_grad (sum) time: 0.1050875186920166
## e[56]       loss.backward (sum) time: 2.1447529792785645
## e[56]      optimizer.step (sum) time: 0.8953258991241455
## epoch[56] training(only) time: 9.688067436218262
# Switched to evaluate mode...
Test: [  0/100]	Time  0.156 ( 0.156)	Loss 2.6074e-01 (2.6074e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.008 ( 0.026)	Loss 4.5728e-01 (3.8677e-01)	Acc@1  90.00 ( 90.36)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.021 ( 0.020)	Loss 5.6396e-01 (4.1597e-01)	Acc@1  86.00 ( 89.81)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.008 ( 0.018)	Loss 4.5752e-01 (4.1747e-01)	Acc@1  87.00 ( 89.81)	Acc@5  99.00 ( 99.58)
Test: [ 40/100]	Time  0.009 ( 0.017)	Loss 5.4150e-01 (4.3688e-01)	Acc@1  88.00 ( 89.51)	Acc@5 100.00 ( 99.61)
Test: [ 50/100]	Time  0.017 ( 0.016)	Loss 1.5161e-01 (4.1392e-01)	Acc@1  95.00 ( 89.98)	Acc@5 100.00 ( 99.61)
Test: [ 60/100]	Time  0.008 ( 0.016)	Loss 2.8174e-01 (4.0986e-01)	Acc@1  93.00 ( 89.93)	Acc@5 100.00 ( 99.62)
Test: [ 70/100]	Time  0.011 ( 0.015)	Loss 6.9775e-01 (4.0845e-01)	Acc@1  87.00 ( 90.07)	Acc@5 100.00 ( 99.62)
Test: [ 80/100]	Time  0.015 ( 0.015)	Loss 1.9092e-01 (4.0619e-01)	Acc@1  93.00 ( 90.11)	Acc@5 100.00 ( 99.63)
Test: [ 90/100]	Time  0.008 ( 0.015)	Loss 2.7661e-01 (4.0238e-01)	Acc@1  92.00 ( 90.11)	Acc@5 100.00 ( 99.66)
 * Acc@1 90.110 Acc@5 99.680
### epoch[56] execution time: 11.247910261154175
EPOCH 57
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [57][  0/391]	Time  0.196 ( 0.196)	Data  0.175 ( 0.175)	Loss 2.8046e-02 (2.8046e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [57][ 10/391]	Time  0.019 ( 0.037)	Data  0.001 ( 0.017)	Loss 1.6785e-02 (3.4898e-02)	Acc@1 100.00 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [57][ 20/391]	Time  0.018 ( 0.032)	Data  0.001 ( 0.010)	Loss 3.0945e-02 (3.3403e-02)	Acc@1  98.44 ( 98.85)	Acc@5 100.00 (100.00)
Epoch: [57][ 30/391]	Time  0.018 ( 0.029)	Data  0.001 ( 0.008)	Loss 2.7924e-02 (3.6679e-02)	Acc@1  98.44 ( 98.79)	Acc@5 100.00 (100.00)
Epoch: [57][ 40/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.006)	Loss 3.8757e-02 (3.5305e-02)	Acc@1  98.44 ( 98.86)	Acc@5 100.00 (100.00)
Epoch: [57][ 50/391]	Time  0.024 ( 0.027)	Data  0.002 ( 0.005)	Loss 2.1881e-02 (3.7375e-02)	Acc@1  98.44 ( 98.77)	Acc@5 100.00 ( 99.98)
Epoch: [57][ 60/391]	Time  0.047 ( 0.026)	Data  0.009 ( 0.005)	Loss 1.9440e-02 (3.7290e-02)	Acc@1  99.22 ( 98.78)	Acc@5 100.00 ( 99.99)
Epoch: [57][ 70/391]	Time  0.034 ( 0.026)	Data  0.001 ( 0.004)	Loss 6.6772e-02 (3.6864e-02)	Acc@1  97.66 ( 98.78)	Acc@5 100.00 ( 99.99)
Epoch: [57][ 80/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.8463e-02 (3.7049e-02)	Acc@1  99.22 ( 98.79)	Acc@5 100.00 ( 99.99)
Epoch: [57][ 90/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.004)	Loss 6.6101e-02 (3.8351e-02)	Acc@1  97.66 ( 98.75)	Acc@5 100.00 ( 99.99)
Epoch: [57][100/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.004)	Loss 5.3680e-02 (3.8868e-02)	Acc@1  99.22 ( 98.72)	Acc@5 100.00 ( 99.99)
Epoch: [57][110/391]	Time  0.026 ( 0.025)	Data  0.004 ( 0.003)	Loss 2.6352e-02 (3.8420e-02)	Acc@1  99.22 ( 98.72)	Acc@5 100.00 ( 99.99)
Epoch: [57][120/391]	Time  0.037 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.6194e-02 (3.8418e-02)	Acc@1  97.66 ( 98.72)	Acc@5 100.00 ( 99.99)
Epoch: [57][130/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 4.6600e-02 (3.8365e-02)	Acc@1  99.22 ( 98.69)	Acc@5 100.00 ( 99.99)
Epoch: [57][140/391]	Time  0.026 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.9989e-02 (3.9375e-02)	Acc@1  99.22 ( 98.67)	Acc@5 100.00 ( 99.99)
Epoch: [57][150/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 9.7046e-02 (3.9478e-02)	Acc@1  96.88 ( 98.67)	Acc@5 100.00 ( 99.99)
Epoch: [57][160/391]	Time  0.045 ( 0.025)	Data  0.016 ( 0.003)	Loss 7.6233e-02 (4.0106e-02)	Acc@1  97.66 ( 98.65)	Acc@5 100.00 ( 99.99)
Epoch: [57][170/391]	Time  0.033 ( 0.025)	Data  0.000 ( 0.003)	Loss 3.5370e-02 (4.0269e-02)	Acc@1  98.44 ( 98.65)	Acc@5 100.00 ( 99.99)
Epoch: [57][180/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.2001e-02 (4.0153e-02)	Acc@1  99.22 ( 98.67)	Acc@5 100.00 ( 99.99)
Epoch: [57][190/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.8116e-02 (3.9746e-02)	Acc@1  99.22 ( 98.69)	Acc@5 100.00 ( 99.99)
Epoch: [57][200/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.0629e-02 (4.0047e-02)	Acc@1  97.66 ( 98.67)	Acc@5 100.00 ( 99.99)
Epoch: [57][210/391]	Time  0.039 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.5640e-02 (4.0085e-02)	Acc@1 100.00 ( 98.66)	Acc@5 100.00 ( 99.99)
Epoch: [57][220/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.5244e-02 (4.0058e-02)	Acc@1 100.00 ( 98.68)	Acc@5 100.00 ( 99.99)
Epoch: [57][230/391]	Time  0.019 ( 0.025)	Data  0.000 ( 0.003)	Loss 1.0933e-02 (3.9314e-02)	Acc@1 100.00 ( 98.71)	Acc@5 100.00 ( 99.99)
Epoch: [57][240/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.3468e-02 (3.9165e-02)	Acc@1  99.22 ( 98.71)	Acc@5 100.00 ( 99.99)
Epoch: [57][250/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.8685e-02 (3.9154e-02)	Acc@1  96.88 ( 98.69)	Acc@5 100.00 ( 99.99)
Epoch: [57][260/391]	Time  0.017 ( 0.024)	Data  0.000 ( 0.003)	Loss 1.7044e-02 (3.9027e-02)	Acc@1  99.22 ( 98.69)	Acc@5 100.00 ( 99.99)
Epoch: [57][270/391]	Time  0.042 ( 0.024)	Data  0.000 ( 0.003)	Loss 4.7516e-02 (3.9568e-02)	Acc@1  98.44 ( 98.67)	Acc@5 100.00 ( 99.99)
Epoch: [57][280/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.003)	Loss 3.6194e-02 (3.9504e-02)	Acc@1  98.44 ( 98.68)	Acc@5 100.00 ( 99.99)
Epoch: [57][290/391]	Time  0.028 ( 0.024)	Data  0.001 ( 0.003)	Loss 5.7892e-02 (3.9335e-02)	Acc@1  96.88 ( 98.69)	Acc@5 100.00 ( 99.99)
Epoch: [57][300/391]	Time  0.034 ( 0.024)	Data  0.001 ( 0.003)	Loss 6.0577e-02 (3.9727e-02)	Acc@1  98.44 ( 98.67)	Acc@5 100.00 ( 99.99)
Epoch: [57][310/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.003)	Loss 4.0405e-02 (3.9888e-02)	Acc@1  99.22 ( 98.66)	Acc@5 100.00 ( 99.99)
Epoch: [57][320/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.003)	Loss 2.8656e-02 (4.0207e-02)	Acc@1  99.22 ( 98.66)	Acc@5 100.00 ( 99.99)
Epoch: [57][330/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.8422e-02 (4.0376e-02)	Acc@1  98.44 ( 98.65)	Acc@5 100.00 ( 99.99)
Epoch: [57][340/391]	Time  0.037 ( 0.024)	Data  0.002 ( 0.002)	Loss 3.5583e-02 (4.0027e-02)	Acc@1  99.22 ( 98.66)	Acc@5 100.00 ( 99.99)
Epoch: [57][350/391]	Time  0.025 ( 0.024)	Data  0.001 ( 0.002)	Loss 6.9763e-02 (4.0031e-02)	Acc@1  96.88 ( 98.66)	Acc@5 100.00 ( 99.99)
Epoch: [57][360/391]	Time  0.043 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.6093e-02 (4.0308e-02)	Acc@1  98.44 ( 98.66)	Acc@5 100.00 ( 99.99)
Epoch: [57][370/391]	Time  0.032 ( 0.024)	Data  0.003 ( 0.002)	Loss 1.9394e-02 (4.0491e-02)	Acc@1  99.22 ( 98.64)	Acc@5 100.00 ( 99.99)
Epoch: [57][380/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.9739e-02 (4.0224e-02)	Acc@1  99.22 ( 98.64)	Acc@5 100.00 ( 99.99)
Epoch: [57][390/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.002)	Loss 5.8441e-02 (4.0192e-02)	Acc@1  98.75 ( 98.65)	Acc@5 100.00 ( 99.99)
## e[57] optimizer.zero_grad (sum) time: 0.10486650466918945
## e[57]       loss.backward (sum) time: 2.1715075969696045
## e[57]      optimizer.step (sum) time: 0.8948252201080322
## epoch[57] training(only) time: 9.602828025817871
# Switched to evaluate mode...
Test: [  0/100]	Time  0.214 ( 0.214)	Loss 3.2300e-01 (3.2300e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.015 ( 0.032)	Loss 4.9854e-01 (3.8780e-01)	Acc@1  89.00 ( 90.09)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.008 ( 0.022)	Loss 5.1611e-01 (4.1302e-01)	Acc@1  87.00 ( 89.71)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.007 ( 0.019)	Loss 4.9023e-01 (4.1955e-01)	Acc@1  89.00 ( 89.84)	Acc@5  99.00 ( 99.52)
Test: [ 40/100]	Time  0.015 ( 0.018)	Loss 5.1416e-01 (4.4034e-01)	Acc@1  88.00 ( 89.56)	Acc@5  99.00 ( 99.51)
Test: [ 50/100]	Time  0.017 ( 0.017)	Loss 1.5283e-01 (4.1889e-01)	Acc@1  94.00 ( 89.94)	Acc@5 100.00 ( 99.53)
Test: [ 60/100]	Time  0.008 ( 0.016)	Loss 2.9028e-01 (4.1727e-01)	Acc@1  90.00 ( 89.92)	Acc@5 100.00 ( 99.56)
Test: [ 70/100]	Time  0.020 ( 0.016)	Loss 7.0947e-01 (4.1425e-01)	Acc@1  86.00 ( 89.99)	Acc@5  98.00 ( 99.55)
Test: [ 80/100]	Time  0.020 ( 0.016)	Loss 2.6953e-01 (4.1326e-01)	Acc@1  92.00 ( 89.99)	Acc@5 100.00 ( 99.58)
Test: [ 90/100]	Time  0.016 ( 0.015)	Loss 2.5293e-01 (4.1168e-01)	Acc@1  93.00 ( 89.95)	Acc@5 100.00 ( 99.63)
 * Acc@1 89.920 Acc@5 99.650
### epoch[57] execution time: 11.247347593307495
EPOCH 58
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [58][  0/391]	Time  0.184 ( 0.184)	Data  0.163 ( 0.163)	Loss 4.8767e-02 (4.8767e-02)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [58][ 10/391]	Time  0.034 ( 0.039)	Data  0.002 ( 0.017)	Loss 1.2970e-02 (3.7202e-02)	Acc@1 100.00 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [58][ 20/391]	Time  0.018 ( 0.031)	Data  0.001 ( 0.010)	Loss 7.9590e-02 (4.3943e-02)	Acc@1  96.09 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [58][ 30/391]	Time  0.017 ( 0.029)	Data  0.000 ( 0.007)	Loss 2.6764e-02 (4.2101e-02)	Acc@1  98.44 ( 98.61)	Acc@5 100.00 (100.00)
Epoch: [58][ 40/391]	Time  0.018 ( 0.028)	Data  0.001 ( 0.006)	Loss 9.9945e-03 (3.8193e-02)	Acc@1 100.00 ( 98.70)	Acc@5 100.00 (100.00)
Epoch: [58][ 50/391]	Time  0.041 ( 0.027)	Data  0.002 ( 0.005)	Loss 3.9124e-02 (3.8845e-02)	Acc@1  99.22 ( 98.70)	Acc@5 100.00 (100.00)
Epoch: [58][ 60/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 5.7678e-02 (3.8664e-02)	Acc@1  99.22 ( 98.71)	Acc@5 100.00 ( 99.99)
Epoch: [58][ 70/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 5.3833e-02 (3.8466e-02)	Acc@1  97.66 ( 98.70)	Acc@5 100.00 ( 99.99)
Epoch: [58][ 80/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 2.3285e-02 (3.8317e-02)	Acc@1  99.22 ( 98.70)	Acc@5 100.00 ( 99.99)
Epoch: [58][ 90/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.004)	Loss 2.8687e-02 (3.7250e-02)	Acc@1  99.22 ( 98.74)	Acc@5 100.00 ( 99.99)
Epoch: [58][100/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.2741e-02 (3.7546e-02)	Acc@1 100.00 ( 98.72)	Acc@5 100.00 ( 99.99)
Epoch: [58][110/391]	Time  0.029 ( 0.025)	Data  0.004 ( 0.003)	Loss 1.8570e-02 (3.7548e-02)	Acc@1  99.22 ( 98.75)	Acc@5 100.00 ( 99.99)
Epoch: [58][120/391]	Time  0.034 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.0355e-02 (3.7631e-02)	Acc@1  99.22 ( 98.74)	Acc@5 100.00 ( 99.99)
Epoch: [58][130/391]	Time  0.025 ( 0.025)	Data  0.002 ( 0.003)	Loss 1.3016e-02 (3.7229e-02)	Acc@1 100.00 ( 98.75)	Acc@5 100.00 ( 99.99)
Epoch: [58][140/391]	Time  0.030 ( 0.025)	Data  0.003 ( 0.003)	Loss 2.8992e-02 (3.7559e-02)	Acc@1  99.22 ( 98.73)	Acc@5 100.00 ( 99.99)
Epoch: [58][150/391]	Time  0.038 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.6987e-02 (3.8519e-02)	Acc@1  98.44 ( 98.69)	Acc@5 100.00 ( 99.99)
Epoch: [58][160/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.1952e-02 (3.9075e-02)	Acc@1  99.22 ( 98.68)	Acc@5 100.00 ( 99.99)
Epoch: [58][170/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.2603e-02 (3.8934e-02)	Acc@1  98.44 ( 98.68)	Acc@5 100.00 ( 99.99)
Epoch: [58][180/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.3092e-02 (3.8456e-02)	Acc@1 100.00 ( 98.71)	Acc@5 100.00 ( 99.99)
Epoch: [58][190/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.7322e-02 (3.9245e-02)	Acc@1  96.88 ( 98.67)	Acc@5 100.00 ( 99.99)
Epoch: [58][200/391]	Time  0.033 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.6304e-02 (3.9741e-02)	Acc@1  97.66 ( 98.67)	Acc@5 100.00 ( 99.99)
Epoch: [58][210/391]	Time  0.031 ( 0.025)	Data  0.000 ( 0.003)	Loss 4.5715e-02 (3.9415e-02)	Acc@1  98.44 ( 98.69)	Acc@5 100.00 ( 99.99)
Epoch: [58][220/391]	Time  0.032 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.1433e-02 (3.9363e-02)	Acc@1  97.66 ( 98.70)	Acc@5 100.00 ( 99.99)
Epoch: [58][230/391]	Time  0.028 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.1464e-02 (3.9441e-02)	Acc@1  98.44 ( 98.68)	Acc@5 100.00 ( 99.99)
Epoch: [58][240/391]	Time  0.027 ( 0.025)	Data  0.003 ( 0.002)	Loss 9.0759e-02 (4.0035e-02)	Acc@1  96.09 ( 98.65)	Acc@5 100.00 ( 99.99)
Epoch: [58][250/391]	Time  0.026 ( 0.025)	Data  0.002 ( 0.002)	Loss 5.4565e-02 (3.9664e-02)	Acc@1  97.66 ( 98.67)	Acc@5 100.00 ( 99.99)
Epoch: [58][260/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.8483e-02 (3.9896e-02)	Acc@1  99.22 ( 98.66)	Acc@5 100.00 ( 99.99)
Epoch: [58][270/391]	Time  0.032 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.0089e-01 (4.0092e-02)	Acc@1  96.09 ( 98.65)	Acc@5  99.22 ( 99.99)
Epoch: [58][280/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.002)	Loss 2.3346e-02 (3.9845e-02)	Acc@1  99.22 ( 98.65)	Acc@5 100.00 ( 99.99)
Epoch: [58][290/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.3550e-02 (3.9568e-02)	Acc@1 100.00 ( 98.67)	Acc@5 100.00 ( 99.99)
Epoch: [58][300/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.1246e-02 (3.9350e-02)	Acc@1  99.22 ( 98.68)	Acc@5 100.00 ( 99.99)
Epoch: [58][310/391]	Time  0.031 ( 0.025)	Data  0.001 ( 0.002)	Loss 5.5145e-02 (3.8862e-02)	Acc@1  97.66 ( 98.69)	Acc@5 100.00 ( 99.99)
Epoch: [58][320/391]	Time  0.025 ( 0.025)	Data  0.005 ( 0.002)	Loss 2.0996e-02 (3.8848e-02)	Acc@1 100.00 ( 98.68)	Acc@5 100.00 ( 99.99)
Epoch: [58][330/391]	Time  0.027 ( 0.025)	Data  0.003 ( 0.002)	Loss 5.4718e-02 (3.8846e-02)	Acc@1  97.66 ( 98.68)	Acc@5 100.00 ( 99.99)
Epoch: [58][340/391]	Time  0.024 ( 0.025)	Data  0.005 ( 0.002)	Loss 1.2337e-02 (3.8606e-02)	Acc@1  99.22 ( 98.69)	Acc@5 100.00 ( 99.99)
Epoch: [58][350/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.4290e-02 (3.8174e-02)	Acc@1 100.00 ( 98.71)	Acc@5 100.00 ( 99.99)
Epoch: [58][360/391]	Time  0.034 ( 0.025)	Data  0.005 ( 0.002)	Loss 3.9673e-02 (3.8484e-02)	Acc@1  98.44 ( 98.71)	Acc@5 100.00 ( 99.99)
Epoch: [58][370/391]	Time  0.032 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.2593e-02 (3.8234e-02)	Acc@1  98.44 ( 98.71)	Acc@5 100.00 ( 99.99)
Epoch: [58][380/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.5040e-02 (3.7951e-02)	Acc@1  98.44 ( 98.72)	Acc@5 100.00 ( 99.99)
Epoch: [58][390/391]	Time  0.015 ( 0.024)	Data  0.000 ( 0.002)	Loss 5.6000e-02 (3.7920e-02)	Acc@1  98.75 ( 98.73)	Acc@5 100.00 ( 99.99)
## e[58] optimizer.zero_grad (sum) time: 0.10429620742797852
## e[58]       loss.backward (sum) time: 2.126146078109741
## e[58]      optimizer.step (sum) time: 0.8817377090454102
## epoch[58] training(only) time: 9.668423175811768
# Switched to evaluate mode...
Test: [  0/100]	Time  0.164 ( 0.164)	Loss 2.5684e-01 (2.5684e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.018 ( 0.028)	Loss 4.6777e-01 (3.7747e-01)	Acc@1  90.00 ( 90.45)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.011 ( 0.020)	Loss 5.5322e-01 (4.0324e-01)	Acc@1  87.00 ( 90.14)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.008 ( 0.018)	Loss 4.6606e-01 (4.1659e-01)	Acc@1  89.00 ( 90.29)	Acc@5  99.00 ( 99.55)
Test: [ 40/100]	Time  0.016 ( 0.017)	Loss 5.4688e-01 (4.3077e-01)	Acc@1  88.00 ( 90.05)	Acc@5 100.00 ( 99.54)
Test: [ 50/100]	Time  0.018 ( 0.016)	Loss 1.5112e-01 (4.1358e-01)	Acc@1  95.00 ( 90.31)	Acc@5 100.00 ( 99.49)
Test: [ 60/100]	Time  0.013 ( 0.016)	Loss 2.7783e-01 (4.1391e-01)	Acc@1  93.00 ( 90.16)	Acc@5 100.00 ( 99.52)
Test: [ 70/100]	Time  0.012 ( 0.015)	Loss 7.0410e-01 (4.1128e-01)	Acc@1  86.00 ( 90.23)	Acc@5 100.00 ( 99.54)
Test: [ 80/100]	Time  0.008 ( 0.015)	Loss 2.1313e-01 (4.1004e-01)	Acc@1  93.00 ( 90.22)	Acc@5 100.00 ( 99.53)
Test: [ 90/100]	Time  0.021 ( 0.015)	Loss 2.8198e-01 (4.0812e-01)	Acc@1  91.00 ( 90.18)	Acc@5 100.00 ( 99.58)
 * Acc@1 90.130 Acc@5 99.620
### epoch[58] execution time: 11.279233932495117
EPOCH 59
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [59][  0/391]	Time  0.187 ( 0.187)	Data  0.166 ( 0.166)	Loss 2.0889e-02 (2.0889e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [59][ 10/391]	Time  0.019 ( 0.038)	Data  0.001 ( 0.017)	Loss 9.0210e-02 (4.0371e-02)	Acc@1  96.88 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [59][ 20/391]	Time  0.018 ( 0.031)	Data  0.001 ( 0.010)	Loss 2.7679e-02 (3.9929e-02)	Acc@1  97.66 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [59][ 30/391]	Time  0.025 ( 0.029)	Data  0.000 ( 0.007)	Loss 2.7176e-02 (4.3208e-02)	Acc@1  99.22 ( 98.29)	Acc@5 100.00 (100.00)
Epoch: [59][ 40/391]	Time  0.036 ( 0.028)	Data  0.003 ( 0.006)	Loss 2.2369e-02 (4.1490e-02)	Acc@1  99.22 ( 98.36)	Acc@5 100.00 ( 99.98)
Epoch: [59][ 50/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.005)	Loss 2.8152e-02 (3.9524e-02)	Acc@1  99.22 ( 98.51)	Acc@5 100.00 ( 99.98)
Epoch: [59][ 60/391]	Time  0.030 ( 0.026)	Data  0.002 ( 0.005)	Loss 7.2975e-03 (3.9005e-02)	Acc@1 100.00 ( 98.57)	Acc@5 100.00 ( 99.99)
Epoch: [59][ 70/391]	Time  0.042 ( 0.026)	Data  0.003 ( 0.004)	Loss 7.6233e-02 (3.7796e-02)	Acc@1  97.66 ( 98.59)	Acc@5  99.22 ( 99.98)
Epoch: [59][ 80/391]	Time  0.017 ( 0.026)	Data  0.000 ( 0.004)	Loss 1.2337e-02 (3.8401e-02)	Acc@1 100.00 ( 98.61)	Acc@5 100.00 ( 99.98)
Epoch: [59][ 90/391]	Time  0.035 ( 0.026)	Data  0.001 ( 0.004)	Loss 3.3539e-02 (3.6625e-02)	Acc@1  99.22 ( 98.69)	Acc@5 100.00 ( 99.98)
Epoch: [59][100/391]	Time  0.032 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.8208e-02 (3.5921e-02)	Acc@1  98.44 ( 98.71)	Acc@5 100.00 ( 99.98)
Epoch: [59][110/391]	Time  0.036 ( 0.025)	Data  0.004 ( 0.003)	Loss 6.0463e-03 (3.5894e-02)	Acc@1 100.00 ( 98.74)	Acc@5 100.00 ( 99.98)
Epoch: [59][120/391]	Time  0.031 ( 0.025)	Data  0.002 ( 0.003)	Loss 2.1393e-02 (3.6345e-02)	Acc@1 100.00 ( 98.75)	Acc@5 100.00 ( 99.97)
Epoch: [59][130/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.2034e-02 (3.7323e-02)	Acc@1 100.00 ( 98.72)	Acc@5 100.00 ( 99.98)
Epoch: [59][140/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 2.2095e-02 (3.6461e-02)	Acc@1  99.22 ( 98.74)	Acc@5 100.00 ( 99.98)
Epoch: [59][150/391]	Time  0.024 ( 0.025)	Data  0.002 ( 0.003)	Loss 3.6194e-02 (3.6463e-02)	Acc@1  98.44 ( 98.73)	Acc@5 100.00 ( 99.98)
Epoch: [59][160/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.7585e-02 (3.6552e-02)	Acc@1  95.31 ( 98.72)	Acc@5 100.00 ( 99.98)
Epoch: [59][170/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.3270e-02 (3.6288e-02)	Acc@1  98.44 ( 98.73)	Acc@5 100.00 ( 99.98)
Epoch: [59][180/391]	Time  0.031 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.1290e-02 (3.6124e-02)	Acc@1  98.44 ( 98.72)	Acc@5 100.00 ( 99.98)
Epoch: [59][190/391]	Time  0.024 ( 0.025)	Data  0.003 ( 0.003)	Loss 3.0319e-02 (3.6261e-02)	Acc@1  98.44 ( 98.71)	Acc@5 100.00 ( 99.98)
Epoch: [59][200/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.003)	Loss 3.1189e-02 (3.6387e-02)	Acc@1  98.44 ( 98.71)	Acc@5 100.00 ( 99.98)
Epoch: [59][210/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.0883e-01 (3.6206e-02)	Acc@1  98.44 ( 98.73)	Acc@5 100.00 ( 99.98)
Epoch: [59][220/391]	Time  0.036 ( 0.025)	Data  0.002 ( 0.003)	Loss 4.6570e-02 (3.6450e-02)	Acc@1  98.44 ( 98.73)	Acc@5 100.00 ( 99.98)
Epoch: [59][230/391]	Time  0.030 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.8929e-02 (3.6462e-02)	Acc@1  98.44 ( 98.73)	Acc@5 100.00 ( 99.98)
Epoch: [59][240/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.003)	Loss 1.4458e-02 (3.6616e-02)	Acc@1  99.22 ( 98.73)	Acc@5 100.00 ( 99.98)
Epoch: [59][250/391]	Time  0.027 ( 0.024)	Data  0.003 ( 0.003)	Loss 6.1401e-02 (3.6261e-02)	Acc@1  97.66 ( 98.75)	Acc@5 100.00 ( 99.98)
Epoch: [59][260/391]	Time  0.019 ( 0.024)	Data  0.002 ( 0.002)	Loss 8.6594e-03 (3.6198e-02)	Acc@1 100.00 ( 98.75)	Acc@5 100.00 ( 99.99)
Epoch: [59][270/391]	Time  0.023 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.1279e-01 (3.6758e-02)	Acc@1  96.09 ( 98.74)	Acc@5  99.22 ( 99.98)
Epoch: [59][280/391]	Time  0.045 ( 0.024)	Data  0.010 ( 0.002)	Loss 7.8979e-02 (3.6823e-02)	Acc@1  96.88 ( 98.73)	Acc@5 100.00 ( 99.98)
Epoch: [59][290/391]	Time  0.019 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.7939e-02 (3.6682e-02)	Acc@1  99.22 ( 98.73)	Acc@5 100.00 ( 99.98)
Epoch: [59][300/391]	Time  0.038 ( 0.024)	Data  0.001 ( 0.002)	Loss 6.9397e-02 (3.6932e-02)	Acc@1  98.44 ( 98.72)	Acc@5 100.00 ( 99.98)
Epoch: [59][310/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 4.2152e-03 (3.7088e-02)	Acc@1 100.00 ( 98.73)	Acc@5 100.00 ( 99.98)
Epoch: [59][320/391]	Time  0.025 ( 0.024)	Data  0.000 ( 0.002)	Loss 2.4872e-02 (3.6868e-02)	Acc@1  99.22 ( 98.73)	Acc@5 100.00 ( 99.99)
Epoch: [59][330/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.1005e-01 (3.7268e-02)	Acc@1  97.66 ( 98.71)	Acc@5 100.00 ( 99.99)
Epoch: [59][340/391]	Time  0.019 ( 0.024)	Data  0.002 ( 0.002)	Loss 3.8422e-02 (3.7106e-02)	Acc@1  98.44 ( 98.71)	Acc@5 100.00 ( 99.99)
Epoch: [59][350/391]	Time  0.017 ( 0.024)	Data  0.000 ( 0.002)	Loss 5.0964e-02 (3.6986e-02)	Acc@1  99.22 ( 98.72)	Acc@5 100.00 ( 99.99)
Epoch: [59][360/391]	Time  0.019 ( 0.024)	Data  0.002 ( 0.002)	Loss 5.4550e-03 (3.7224e-02)	Acc@1 100.00 ( 98.72)	Acc@5 100.00 ( 99.99)
Epoch: [59][370/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.5004e-02 (3.7156e-02)	Acc@1  98.44 ( 98.72)	Acc@5 100.00 ( 99.99)
Epoch: [59][380/391]	Time  0.041 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.3997e-02 (3.7435e-02)	Acc@1  99.22 ( 98.71)	Acc@5 100.00 ( 99.99)
Epoch: [59][390/391]	Time  0.015 ( 0.024)	Data  0.000 ( 0.002)	Loss 8.4778e-02 (3.7569e-02)	Acc@1  96.25 ( 98.71)	Acc@5 100.00 ( 99.99)
## e[59] optimizer.zero_grad (sum) time: 0.10547828674316406
## e[59]       loss.backward (sum) time: 2.164386510848999
## e[59]      optimizer.step (sum) time: 0.895843505859375
## epoch[59] training(only) time: 9.63736891746521
# Switched to evaluate mode...
Test: [  0/100]	Time  0.155 ( 0.155)	Loss 3.2642e-01 (3.2642e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.014 ( 0.026)	Loss 5.5371e-01 (4.0836e-01)	Acc@1  89.00 ( 90.45)	Acc@5  99.00 ( 99.45)
Test: [ 20/100]	Time  0.014 ( 0.020)	Loss 5.4443e-01 (4.2869e-01)	Acc@1  88.00 ( 89.95)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.008 ( 0.017)	Loss 5.0928e-01 (4.3298e-01)	Acc@1  86.00 ( 90.19)	Acc@5  99.00 ( 99.55)
Test: [ 40/100]	Time  0.018 ( 0.016)	Loss 5.8447e-01 (4.5366e-01)	Acc@1  87.00 ( 89.80)	Acc@5 100.00 ( 99.56)
Test: [ 50/100]	Time  0.008 ( 0.016)	Loss 1.6687e-01 (4.3331e-01)	Acc@1  95.00 ( 90.08)	Acc@5 100.00 ( 99.57)
Test: [ 60/100]	Time  0.016 ( 0.015)	Loss 3.1323e-01 (4.3192e-01)	Acc@1  90.00 ( 89.92)	Acc@5 100.00 ( 99.57)
Test: [ 70/100]	Time  0.011 ( 0.015)	Loss 7.2998e-01 (4.2740e-01)	Acc@1  86.00 ( 89.97)	Acc@5 100.00 ( 99.58)
Test: [ 80/100]	Time  0.013 ( 0.015)	Loss 1.6162e-01 (4.2217e-01)	Acc@1  96.00 ( 90.09)	Acc@5 100.00 ( 99.58)
Test: [ 90/100]	Time  0.008 ( 0.014)	Loss 2.7930e-01 (4.1868e-01)	Acc@1  92.00 ( 90.13)	Acc@5 100.00 ( 99.63)
 * Acc@1 90.110 Acc@5 99.650
### epoch[59] execution time: 11.21609902381897
EPOCH 60
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [60][  0/391]	Time  0.194 ( 0.194)	Data  0.173 ( 0.173)	Loss 3.5919e-02 (3.5919e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [60][ 10/391]	Time  0.019 ( 0.038)	Data  0.001 ( 0.017)	Loss 1.9836e-02 (3.4585e-02)	Acc@1  99.22 ( 98.51)	Acc@5 100.00 (100.00)
Epoch: [60][ 20/391]	Time  0.031 ( 0.032)	Data  0.001 ( 0.010)	Loss 3.0975e-02 (3.6282e-02)	Acc@1  98.44 ( 98.55)	Acc@5 100.00 ( 99.96)
Epoch: [60][ 30/391]	Time  0.018 ( 0.029)	Data  0.001 ( 0.007)	Loss 5.5885e-03 (3.5688e-02)	Acc@1 100.00 ( 98.69)	Acc@5 100.00 ( 99.97)
Epoch: [60][ 40/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.006)	Loss 1.5472e-02 (3.5701e-02)	Acc@1 100.00 ( 98.74)	Acc@5 100.00 ( 99.96)
Epoch: [60][ 50/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.005)	Loss 7.4219e-02 (3.7946e-02)	Acc@1  97.66 ( 98.61)	Acc@5 100.00 ( 99.97)
Epoch: [60][ 60/391]	Time  0.017 ( 0.026)	Data  0.000 ( 0.005)	Loss 2.6031e-02 (3.7667e-02)	Acc@1  99.22 ( 98.67)	Acc@5 100.00 ( 99.96)
Epoch: [60][ 70/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 2.1484e-02 (3.6928e-02)	Acc@1  98.44 ( 98.68)	Acc@5 100.00 ( 99.96)
Epoch: [60][ 80/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.004)	Loss 2.7283e-02 (3.6705e-02)	Acc@1  98.44 ( 98.72)	Acc@5 100.00 ( 99.96)
Epoch: [60][ 90/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.004)	Loss 5.4565e-02 (3.6178e-02)	Acc@1  97.66 ( 98.70)	Acc@5 100.00 ( 99.97)
Epoch: [60][100/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.004)	Loss 3.0197e-02 (3.5528e-02)	Acc@1  99.22 ( 98.73)	Acc@5 100.00 ( 99.97)
Epoch: [60][110/391]	Time  0.020 ( 0.025)	Data  0.003 ( 0.003)	Loss 3.4363e-02 (3.4737e-02)	Acc@1  97.66 ( 98.78)	Acc@5 100.00 ( 99.97)
Epoch: [60][120/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.2812e-02 (3.3386e-02)	Acc@1  98.44 ( 98.82)	Acc@5 100.00 ( 99.97)
Epoch: [60][130/391]	Time  0.025 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.8860e-02 (3.4023e-02)	Acc@1 100.00 ( 98.83)	Acc@5 100.00 ( 99.98)
Epoch: [60][140/391]	Time  0.029 ( 0.025)	Data  0.000 ( 0.003)	Loss 3.2959e-02 (3.3086e-02)	Acc@1  98.44 ( 98.85)	Acc@5 100.00 ( 99.98)
Epoch: [60][150/391]	Time  0.026 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.0996e-02 (3.3190e-02)	Acc@1  99.22 ( 98.85)	Acc@5 100.00 ( 99.98)
Epoch: [60][160/391]	Time  0.017 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.1017e-02 (3.3402e-02)	Acc@1 100.00 ( 98.83)	Acc@5 100.00 ( 99.98)
Epoch: [60][170/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.8030e-02 (3.3882e-02)	Acc@1  98.44 ( 98.82)	Acc@5 100.00 ( 99.98)
Epoch: [60][180/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.3213e-02 (3.3991e-02)	Acc@1  98.44 ( 98.81)	Acc@5 100.00 ( 99.98)
Epoch: [60][190/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.7649e-02 (3.4103e-02)	Acc@1  99.22 ( 98.81)	Acc@5 100.00 ( 99.98)
Epoch: [60][200/391]	Time  0.015 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.3176e-02 (3.3798e-02)	Acc@1 100.00 ( 98.84)	Acc@5 100.00 ( 99.98)
Epoch: [60][210/391]	Time  0.024 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.2124e-02 (3.3549e-02)	Acc@1  98.44 ( 98.84)	Acc@5 100.00 ( 99.99)
Epoch: [60][220/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.3123e-02 (3.3567e-02)	Acc@1 100.00 ( 98.84)	Acc@5 100.00 ( 99.99)
Epoch: [60][230/391]	Time  0.039 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.0273e-02 (3.3550e-02)	Acc@1  99.22 ( 98.86)	Acc@5 100.00 ( 99.99)
Epoch: [60][240/391]	Time  0.037 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.4826e-02 (3.3701e-02)	Acc@1  99.22 ( 98.86)	Acc@5 100.00 ( 99.99)
Epoch: [60][250/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.7679e-02 (3.3434e-02)	Acc@1  98.44 ( 98.87)	Acc@5 100.00 ( 99.99)
Epoch: [60][260/391]	Time  0.031 ( 0.025)	Data  0.000 ( 0.003)	Loss 1.6098e-02 (3.3459e-02)	Acc@1 100.00 ( 98.87)	Acc@5 100.00 ( 99.99)
Epoch: [60][270/391]	Time  0.029 ( 0.025)	Data  0.001 ( 0.002)	Loss 5.8105e-02 (3.3629e-02)	Acc@1  97.66 ( 98.87)	Acc@5 100.00 ( 99.99)
Epoch: [60][280/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 5.7037e-02 (3.3818e-02)	Acc@1  98.44 ( 98.86)	Acc@5 100.00 ( 99.99)
Epoch: [60][290/391]	Time  0.041 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.7231e-02 (3.3684e-02)	Acc@1  98.44 ( 98.86)	Acc@5 100.00 ( 99.99)
Epoch: [60][300/391]	Time  0.035 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.4525e-02 (3.3752e-02)	Acc@1  97.66 ( 98.85)	Acc@5 100.00 ( 99.99)
Epoch: [60][310/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.3813e-02 (3.3754e-02)	Acc@1  98.44 ( 98.85)	Acc@5 100.00 ( 99.99)
Epoch: [60][320/391]	Time  0.027 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.9363e-02 (3.3497e-02)	Acc@1  99.22 ( 98.87)	Acc@5 100.00 ( 99.99)
Epoch: [60][330/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.4297e-02 (3.3094e-02)	Acc@1 100.00 ( 98.88)	Acc@5 100.00 ( 99.99)
Epoch: [60][340/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 8.1863e-03 (3.3436e-02)	Acc@1 100.00 ( 98.87)	Acc@5 100.00 ( 99.99)
Epoch: [60][350/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.1088e-02 (3.3340e-02)	Acc@1  99.22 ( 98.87)	Acc@5 100.00 ( 99.99)
Epoch: [60][360/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.0838e-02 (3.3316e-02)	Acc@1  98.44 ( 98.86)	Acc@5 100.00 ( 99.99)
Epoch: [60][370/391]	Time  0.035 ( 0.024)	Data  0.000 ( 0.002)	Loss 7.5806e-02 (3.3396e-02)	Acc@1  99.22 ( 98.87)	Acc@5  99.22 ( 99.99)
Epoch: [60][380/391]	Time  0.033 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.9892e-02 (3.3578e-02)	Acc@1  99.22 ( 98.87)	Acc@5 100.00 ( 99.99)
Epoch: [60][390/391]	Time  0.017 ( 0.024)	Data  0.000 ( 0.002)	Loss 9.6512e-03 (3.3386e-02)	Acc@1 100.00 ( 98.87)	Acc@5 100.00 ( 99.99)
## e[60] optimizer.zero_grad (sum) time: 0.10556674003601074
## e[60]       loss.backward (sum) time: 2.1407785415649414
## e[60]      optimizer.step (sum) time: 0.8992173671722412
## epoch[60] training(only) time: 9.65428638458252
# Switched to evaluate mode...
Test: [  0/100]	Time  0.160 ( 0.160)	Loss 3.1787e-01 (3.1787e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.017 ( 0.026)	Loss 5.5908e-01 (4.0019e-01)	Acc@1  90.00 ( 90.27)	Acc@5  99.00 ( 99.45)
Test: [ 20/100]	Time  0.015 ( 0.020)	Loss 5.4785e-01 (4.1992e-01)	Acc@1  87.00 ( 89.81)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.009 ( 0.018)	Loss 4.8120e-01 (4.2406e-01)	Acc@1  87.00 ( 90.06)	Acc@5  99.00 ( 99.55)
Test: [ 40/100]	Time  0.017 ( 0.017)	Loss 5.7666e-01 (4.4641e-01)	Acc@1  89.00 ( 89.68)	Acc@5 100.00 ( 99.56)
Test: [ 50/100]	Time  0.008 ( 0.016)	Loss 1.6394e-01 (4.2764e-01)	Acc@1  95.00 ( 90.00)	Acc@5 100.00 ( 99.57)
Test: [ 60/100]	Time  0.012 ( 0.015)	Loss 3.1396e-01 (4.2722e-01)	Acc@1  90.00 ( 89.89)	Acc@5 100.00 ( 99.59)
Test: [ 70/100]	Time  0.008 ( 0.015)	Loss 7.1631e-01 (4.2272e-01)	Acc@1  87.00 ( 90.03)	Acc@5  99.00 ( 99.59)
Test: [ 80/100]	Time  0.015 ( 0.015)	Loss 1.6821e-01 (4.1693e-01)	Acc@1  96.00 ( 90.12)	Acc@5 100.00 ( 99.60)
Test: [ 90/100]	Time  0.013 ( 0.015)	Loss 2.7783e-01 (4.1298e-01)	Acc@1  91.00 ( 90.15)	Acc@5 100.00 ( 99.65)
 * Acc@1 90.090 Acc@5 99.670
### epoch[60] execution time: 11.235318660736084
EPOCH 61
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [61][  0/391]	Time  0.260 ( 0.260)	Data  0.230 ( 0.230)	Loss 2.3010e-02 (2.3010e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [61][ 10/391]	Time  0.018 ( 0.043)	Data  0.000 ( 0.023)	Loss 1.3695e-02 (2.6055e-02)	Acc@1 100.00 ( 98.86)	Acc@5 100.00 (100.00)
Epoch: [61][ 20/391]	Time  0.019 ( 0.035)	Data  0.002 ( 0.013)	Loss 3.7231e-02 (2.8754e-02)	Acc@1  98.44 ( 98.85)	Acc@5 100.00 (100.00)
Epoch: [61][ 30/391]	Time  0.029 ( 0.032)	Data  0.001 ( 0.010)	Loss 6.6589e-02 (3.3990e-02)	Acc@1  97.66 ( 98.87)	Acc@5 100.00 ( 99.97)
Epoch: [61][ 40/391]	Time  0.018 ( 0.030)	Data  0.001 ( 0.008)	Loss 3.4851e-02 (3.0824e-02)	Acc@1  98.44 ( 98.93)	Acc@5 100.00 ( 99.98)
Epoch: [61][ 50/391]	Time  0.020 ( 0.028)	Data  0.003 ( 0.007)	Loss 1.2077e-02 (3.0331e-02)	Acc@1  99.22 ( 98.96)	Acc@5 100.00 ( 99.98)
Epoch: [61][ 60/391]	Time  0.019 ( 0.028)	Data  0.002 ( 0.006)	Loss 6.7383e-02 (3.1620e-02)	Acc@1  97.66 ( 98.94)	Acc@5 100.00 ( 99.99)
Epoch: [61][ 70/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.005)	Loss 7.9575e-03 (3.4150e-02)	Acc@1 100.00 ( 98.83)	Acc@5 100.00 ( 99.99)
Epoch: [61][ 80/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.005)	Loss 4.4769e-02 (3.3059e-02)	Acc@1  98.44 ( 98.88)	Acc@5 100.00 ( 99.99)
Epoch: [61][ 90/391]	Time  0.040 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.0490e-02 (3.4339e-02)	Acc@1 100.00 ( 98.85)	Acc@5 100.00 ( 99.99)
Epoch: [61][100/391]	Time  0.028 ( 0.026)	Data  0.000 ( 0.004)	Loss 2.5574e-02 (3.4930e-02)	Acc@1  98.44 ( 98.84)	Acc@5 100.00 ( 99.99)
Epoch: [61][110/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 6.6650e-02 (3.5835e-02)	Acc@1  97.66 ( 98.81)	Acc@5 100.00 ( 99.99)
Epoch: [61][120/391]	Time  0.017 ( 0.026)	Data  0.000 ( 0.004)	Loss 3.6133e-02 (3.5422e-02)	Acc@1  97.66 ( 98.80)	Acc@5 100.00 ( 99.99)
Epoch: [61][130/391]	Time  0.042 ( 0.026)	Data  0.007 ( 0.004)	Loss 3.8513e-02 (3.5087e-02)	Acc@1  99.22 ( 98.80)	Acc@5 100.00 ( 99.99)
Epoch: [61][140/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.004)	Loss 6.1455e-03 (3.4208e-02)	Acc@1 100.00 ( 98.83)	Acc@5 100.00 ( 99.99)
Epoch: [61][150/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.2185e-02 (3.4072e-02)	Acc@1  98.44 ( 98.85)	Acc@5 100.00 ( 99.99)
Epoch: [61][160/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 8.9264e-03 (3.3808e-02)	Acc@1 100.00 ( 98.85)	Acc@5 100.00 ( 99.99)
Epoch: [61][170/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.1082e-02 (3.3638e-02)	Acc@1  98.44 ( 98.86)	Acc@5 100.00 ( 99.99)
Epoch: [61][180/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 4.4250e-03 (3.3480e-02)	Acc@1 100.00 ( 98.86)	Acc@5 100.00 ( 99.99)
Epoch: [61][190/391]	Time  0.017 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.7302e-02 (3.3621e-02)	Acc@1  97.66 ( 98.86)	Acc@5 100.00 ( 99.99)
Epoch: [61][200/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.0201e-02 (3.3988e-02)	Acc@1 100.00 ( 98.85)	Acc@5 100.00 ( 99.99)
Epoch: [61][210/391]	Time  0.016 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.8030e-02 (3.4323e-02)	Acc@1  99.22 ( 98.85)	Acc@5 100.00 ( 99.99)
Epoch: [61][220/391]	Time  0.038 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.1525e-02 (3.3988e-02)	Acc@1  99.22 ( 98.87)	Acc@5 100.00 ( 99.99)
Epoch: [61][230/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.9763e-02 (3.4255e-02)	Acc@1  98.44 ( 98.87)	Acc@5 100.00 ( 99.99)
Epoch: [61][240/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.2013e-02 (3.4098e-02)	Acc@1  99.22 ( 98.88)	Acc@5 100.00 ( 99.99)
Epoch: [61][250/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.8309e-02 (3.4310e-02)	Acc@1  99.22 ( 98.87)	Acc@5 100.00 ( 99.99)
Epoch: [61][260/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.3477e-02 (3.4892e-02)	Acc@1  98.44 ( 98.85)	Acc@5 100.00 ( 99.99)
Epoch: [61][270/391]	Time  0.031 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.1378e-02 (3.4462e-02)	Acc@1  99.22 ( 98.87)	Acc@5 100.00 ( 99.99)
Epoch: [61][280/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.5510e-02 (3.4297e-02)	Acc@1  97.66 ( 98.88)	Acc@5 100.00 ( 99.99)
Epoch: [61][290/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.7593e-02 (3.4306e-02)	Acc@1 100.00 ( 98.89)	Acc@5 100.00 ( 99.99)
Epoch: [61][300/391]	Time  0.026 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.9763e-02 (3.4170e-02)	Acc@1  96.88 ( 98.89)	Acc@5 100.00 ( 99.99)
Epoch: [61][310/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.3884e-02 (3.4029e-02)	Acc@1  98.44 ( 98.90)	Acc@5 100.00 ( 99.99)
Epoch: [61][320/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.1210e-02 (3.3579e-02)	Acc@1  99.22 ( 98.91)	Acc@5 100.00 ( 99.99)
Epoch: [61][330/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.2690e-02 (3.3271e-02)	Acc@1  99.22 ( 98.92)	Acc@5 100.00 ( 99.99)
Epoch: [61][340/391]	Time  0.034 ( 0.025)	Data  0.005 ( 0.002)	Loss 4.2511e-02 (3.3007e-02)	Acc@1  98.44 ( 98.93)	Acc@5 100.00 ( 99.99)
Epoch: [61][350/391]	Time  0.031 ( 0.025)	Data  0.001 ( 0.002)	Loss 7.5134e-02 (3.3223e-02)	Acc@1  98.44 ( 98.92)	Acc@5 100.00 ( 99.99)
Epoch: [61][360/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.1189e-02 (3.3099e-02)	Acc@1  98.44 ( 98.92)	Acc@5 100.00 ( 99.99)
Epoch: [61][370/391]	Time  0.034 ( 0.025)	Data  0.000 ( 0.002)	Loss 3.1555e-02 (3.3104e-02)	Acc@1  98.44 ( 98.93)	Acc@5 100.00 ( 99.99)
Epoch: [61][380/391]	Time  0.038 ( 0.025)	Data  0.003 ( 0.002)	Loss 1.4412e-02 (3.3011e-02)	Acc@1  99.22 ( 98.93)	Acc@5 100.00 ( 99.99)
Epoch: [61][390/391]	Time  0.013 ( 0.025)	Data  0.000 ( 0.002)	Loss 2.6215e-02 (3.2830e-02)	Acc@1  98.75 ( 98.93)	Acc@5 100.00 ( 99.99)
## e[61] optimizer.zero_grad (sum) time: 0.10607767105102539
## e[61]       loss.backward (sum) time: 2.144179105758667
## e[61]      optimizer.step (sum) time: 0.8881027698516846
## epoch[61] training(only) time: 9.777947425842285
# Switched to evaluate mode...
Test: [  0/100]	Time  0.166 ( 0.166)	Loss 3.0298e-01 (3.0298e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.015 ( 0.027)	Loss 5.5518e-01 (3.9550e-01)	Acc@1  90.00 ( 90.36)	Acc@5  99.00 ( 99.55)
Test: [ 20/100]	Time  0.019 ( 0.021)	Loss 5.3027e-01 (4.1658e-01)	Acc@1  87.00 ( 89.76)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.008 ( 0.018)	Loss 4.9756e-01 (4.2093e-01)	Acc@1  86.00 ( 89.97)	Acc@5  99.00 ( 99.58)
Test: [ 40/100]	Time  0.019 ( 0.017)	Loss 5.7422e-01 (4.4189e-01)	Acc@1  88.00 ( 89.56)	Acc@5 100.00 ( 99.59)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 1.5088e-01 (4.2265e-01)	Acc@1  95.00 ( 89.92)	Acc@5 100.00 ( 99.61)
Test: [ 60/100]	Time  0.008 ( 0.016)	Loss 2.9541e-01 (4.2208e-01)	Acc@1  91.00 ( 89.89)	Acc@5 100.00 ( 99.62)
Test: [ 70/100]	Time  0.008 ( 0.016)	Loss 6.9482e-01 (4.1689e-01)	Acc@1  87.00 ( 90.03)	Acc@5  99.00 ( 99.62)
Test: [ 80/100]	Time  0.008 ( 0.015)	Loss 1.6956e-01 (4.1047e-01)	Acc@1  95.00 ( 90.16)	Acc@5 100.00 ( 99.63)
Test: [ 90/100]	Time  0.012 ( 0.015)	Loss 2.8101e-01 (4.0616e-01)	Acc@1  91.00 ( 90.26)	Acc@5 100.00 ( 99.67)
 * Acc@1 90.260 Acc@5 99.700
### epoch[61] execution time: 11.397130489349365
EPOCH 62
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [62][  0/391]	Time  0.192 ( 0.192)	Data  0.162 ( 0.162)	Loss 1.4648e-02 (1.4648e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [62][ 10/391]	Time  0.019 ( 0.039)	Data  0.001 ( 0.016)	Loss 5.5389e-02 (2.3394e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [62][ 20/391]	Time  0.019 ( 0.031)	Data  0.002 ( 0.009)	Loss 9.5596e-03 (2.8274e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [62][ 30/391]	Time  0.018 ( 0.029)	Data  0.001 ( 0.007)	Loss 3.6560e-02 (2.7708e-02)	Acc@1  98.44 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [62][ 40/391]	Time  0.028 ( 0.028)	Data  0.001 ( 0.005)	Loss 6.4209e-02 (2.9196e-02)	Acc@1  97.66 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [62][ 50/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.005)	Loss 9.0714e-03 (2.8754e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [62][ 60/391]	Time  0.034 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.7863e-02 (3.1288e-02)	Acc@1  97.66 ( 99.05)	Acc@5 100.00 ( 99.99)
Epoch: [62][ 70/391]	Time  0.031 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.0487e-02 (3.1885e-02)	Acc@1  98.44 ( 99.02)	Acc@5 100.00 ( 99.99)
Epoch: [62][ 80/391]	Time  0.035 ( 0.026)	Data  0.000 ( 0.004)	Loss 3.7292e-02 (3.1155e-02)	Acc@1  99.22 ( 99.02)	Acc@5 100.00 ( 99.99)
Epoch: [62][ 90/391]	Time  0.026 ( 0.026)	Data  0.004 ( 0.004)	Loss 1.8997e-02 (3.0598e-02)	Acc@1 100.00 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [62][100/391]	Time  0.019 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.3132e-02 (2.9504e-02)	Acc@1  99.22 ( 99.09)	Acc@5 100.00 ( 99.99)
Epoch: [62][110/391]	Time  0.039 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7258e-02 (3.0118e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [62][120/391]	Time  0.030 ( 0.025)	Data  0.000 ( 0.003)	Loss 7.6828e-03 (3.1253e-02)	Acc@1 100.00 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [62][130/391]	Time  0.035 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.6749e-02 (3.2660e-02)	Acc@1  99.22 ( 98.97)	Acc@5 100.00 ( 99.99)
Epoch: [62][140/391]	Time  0.035 ( 0.025)	Data  0.002 ( 0.003)	Loss 2.6382e-02 (3.2413e-02)	Acc@1  99.22 ( 98.97)	Acc@5 100.00 ( 99.99)
Epoch: [62][150/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 2.1484e-02 (3.2375e-02)	Acc@1  99.22 ( 98.97)	Acc@5 100.00 ( 99.99)
Epoch: [62][160/391]	Time  0.017 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.7506e-02 (3.2592e-02)	Acc@1  98.44 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [62][170/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.8336e-02 (3.2670e-02)	Acc@1  99.22 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [62][180/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.4534e-02 (3.2679e-02)	Acc@1  97.66 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [62][190/391]	Time  0.034 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.7008e-02 (3.3074e-02)	Acc@1  98.44 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [62][200/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.6718e-02 (3.2781e-02)	Acc@1  98.44 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [62][210/391]	Time  0.036 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.9133e-03 (3.2791e-02)	Acc@1 100.00 ( 98.94)	Acc@5 100.00 (100.00)
Epoch: [62][220/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.7689e-02 (3.2871e-02)	Acc@1  99.22 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [62][230/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.9458e-02 (3.2778e-02)	Acc@1  96.88 ( 98.91)	Acc@5 100.00 (100.00)
Epoch: [62][240/391]	Time  0.029 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.3771e-02 (3.2498e-02)	Acc@1 100.00 ( 98.92)	Acc@5 100.00 (100.00)
Epoch: [62][250/391]	Time  0.029 ( 0.025)	Data  0.000 ( 0.002)	Loss 6.7688e-02 (3.2754e-02)	Acc@1  99.22 ( 98.91)	Acc@5 100.00 (100.00)
Epoch: [62][260/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.8600e-02 (3.2338e-02)	Acc@1  99.22 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [62][270/391]	Time  0.029 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.9846e-02 (3.2448e-02)	Acc@1  98.44 ( 98.92)	Acc@5 100.00 (100.00)
Epoch: [62][280/391]	Time  0.027 ( 0.025)	Data  0.002 ( 0.002)	Loss 3.3936e-02 (3.2759e-02)	Acc@1  98.44 ( 98.92)	Acc@5 100.00 (100.00)
Epoch: [62][290/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.002)	Loss 2.4887e-02 (3.2798e-02)	Acc@1  99.22 ( 98.91)	Acc@5 100.00 (100.00)
Epoch: [62][300/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.002)	Loss 1.9943e-02 (3.2978e-02)	Acc@1  99.22 ( 98.89)	Acc@5 100.00 (100.00)
Epoch: [62][310/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.8351e-02 (3.2909e-02)	Acc@1  99.22 ( 98.89)	Acc@5 100.00 (100.00)
Epoch: [62][320/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.8849e-02 (3.2921e-02)	Acc@1  98.44 ( 98.90)	Acc@5 100.00 (100.00)
Epoch: [62][330/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 5.2429e-02 (3.2754e-02)	Acc@1  98.44 ( 98.91)	Acc@5 100.00 (100.00)
Epoch: [62][340/391]	Time  0.020 ( 0.024)	Data  0.001 ( 0.002)	Loss 6.0730e-03 (3.2433e-02)	Acc@1 100.00 ( 98.92)	Acc@5 100.00 (100.00)
Epoch: [62][350/391]	Time  0.026 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.4393e-02 (3.2283e-02)	Acc@1  98.44 ( 98.92)	Acc@5 100.00 (100.00)
Epoch: [62][360/391]	Time  0.053 ( 0.024)	Data  0.020 ( 0.002)	Loss 1.1009e-02 (3.2335e-02)	Acc@1  99.22 ( 98.92)	Acc@5 100.00 (100.00)
Epoch: [62][370/391]	Time  0.028 ( 0.024)	Data  0.001 ( 0.002)	Loss 6.9641e-02 (3.2170e-02)	Acc@1  96.88 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [62][380/391]	Time  0.043 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.5236e-02 (3.2212e-02)	Acc@1 100.00 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [62][390/391]	Time  0.016 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.7756e-02 (3.2658e-02)	Acc@1  98.75 ( 98.92)	Acc@5 100.00 (100.00)
## e[62] optimizer.zero_grad (sum) time: 0.10436081886291504
## e[62]       loss.backward (sum) time: 2.1788151264190674
## e[62]      optimizer.step (sum) time: 0.884401798248291
## epoch[62] training(only) time: 9.640404462814331
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 3.1689e-01 (3.1689e-01)	Acc@1  93.00 ( 93.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.010 ( 0.027)	Loss 5.4297e-01 (3.8869e-01)	Acc@1  90.00 ( 90.64)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.008 ( 0.020)	Loss 5.2930e-01 (4.1536e-01)	Acc@1  87.00 ( 90.00)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.013 ( 0.018)	Loss 4.8657e-01 (4.1952e-01)	Acc@1  88.00 ( 90.23)	Acc@5  99.00 ( 99.65)
Test: [ 40/100]	Time  0.012 ( 0.017)	Loss 5.8496e-01 (4.4207e-01)	Acc@1  88.00 ( 89.73)	Acc@5  99.00 ( 99.61)
Test: [ 50/100]	Time  0.016 ( 0.016)	Loss 1.5039e-01 (4.2186e-01)	Acc@1  95.00 ( 90.02)	Acc@5 100.00 ( 99.59)
Test: [ 60/100]	Time  0.019 ( 0.016)	Loss 2.9102e-01 (4.2190e-01)	Acc@1  91.00 ( 89.98)	Acc@5 100.00 ( 99.61)
Test: [ 70/100]	Time  0.014 ( 0.015)	Loss 6.9336e-01 (4.1622e-01)	Acc@1  87.00 ( 90.14)	Acc@5  99.00 ( 99.61)
Test: [ 80/100]	Time  0.013 ( 0.015)	Loss 1.7102e-01 (4.1029e-01)	Acc@1  95.00 ( 90.25)	Acc@5 100.00 ( 99.62)
Test: [ 90/100]	Time  0.007 ( 0.015)	Loss 2.8442e-01 (4.0636e-01)	Acc@1  91.00 ( 90.29)	Acc@5 100.00 ( 99.66)
 * Acc@1 90.230 Acc@5 99.680
### epoch[62] execution time: 11.21618366241455
EPOCH 63
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [63][  0/391]	Time  0.255 ( 0.255)	Data  0.224 ( 0.224)	Loss 2.1606e-02 (2.1606e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [63][ 10/391]	Time  0.028 ( 0.045)	Data  0.000 ( 0.022)	Loss 6.7017e-02 (3.8017e-02)	Acc@1  98.44 ( 98.79)	Acc@5 100.00 (100.00)
Epoch: [63][ 20/391]	Time  0.018 ( 0.034)	Data  0.002 ( 0.012)	Loss 7.2479e-03 (3.2235e-02)	Acc@1 100.00 ( 98.92)	Acc@5 100.00 (100.00)
Epoch: [63][ 30/391]	Time  0.017 ( 0.031)	Data  0.000 ( 0.009)	Loss 1.7792e-02 (3.0962e-02)	Acc@1  98.44 ( 98.84)	Acc@5 100.00 (100.00)
Epoch: [63][ 40/391]	Time  0.027 ( 0.030)	Data  0.000 ( 0.007)	Loss 4.2969e-02 (2.8464e-02)	Acc@1  99.22 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [63][ 50/391]	Time  0.017 ( 0.028)	Data  0.000 ( 0.006)	Loss 4.7638e-02 (2.9735e-02)	Acc@1  99.22 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [63][ 60/391]	Time  0.017 ( 0.028)	Data  0.000 ( 0.005)	Loss 3.1830e-02 (2.9431e-02)	Acc@1  98.44 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [63][ 70/391]	Time  0.019 ( 0.027)	Data  0.002 ( 0.005)	Loss 1.9180e-02 (2.8114e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [63][ 80/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.0752e-02 (2.8377e-02)	Acc@1  99.22 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [63][ 90/391]	Time  0.017 ( 0.026)	Data  0.000 ( 0.004)	Loss 4.9133e-02 (2.7575e-02)	Acc@1  98.44 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [63][100/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.004)	Loss 2.4612e-02 (2.9047e-02)	Acc@1  99.22 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [63][110/391]	Time  0.042 ( 0.026)	Data  0.004 ( 0.004)	Loss 2.4261e-02 (2.9800e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [63][120/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.004)	Loss 3.9795e-02 (2.8896e-02)	Acc@1  97.66 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [63][130/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8585e-02 (2.8147e-02)	Acc@1  98.44 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [63][140/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1009e-02 (2.8819e-02)	Acc@1 100.00 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [63][150/391]	Time  0.024 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.0231e-02 (2.8815e-02)	Acc@1 100.00 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [63][160/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.2786e-02 (2.9616e-02)	Acc@1  97.66 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [63][170/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.2114e-02 (2.9987e-02)	Acc@1  98.44 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [63][180/391]	Time  0.036 ( 0.025)	Data  0.002 ( 0.003)	Loss 2.2995e-02 (3.0138e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [63][190/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 5.6030e-02 (3.0035e-02)	Acc@1  98.44 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [63][200/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.9093e-02 (2.9961e-02)	Acc@1  98.44 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [63][210/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 9.9487e-03 (3.0269e-02)	Acc@1 100.00 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [63][220/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.9194e-02 (3.0394e-02)	Acc@1  97.66 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [63][230/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.6962e-02 (3.0754e-02)	Acc@1  98.44 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [63][240/391]	Time  0.037 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.4647e-02 (3.0717e-02)	Acc@1  98.44 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [63][250/391]	Time  0.024 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.0883e-01 (3.1349e-02)	Acc@1  96.88 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [63][260/391]	Time  0.029 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.3752e-02 (3.1567e-02)	Acc@1  99.22 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [63][270/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.2257e-02 (3.1599e-02)	Acc@1  99.22 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [63][280/391]	Time  0.038 ( 0.025)	Data  0.005 ( 0.003)	Loss 4.4739e-02 (3.1251e-02)	Acc@1  97.66 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [63][290/391]	Time  0.020 ( 0.025)	Data  0.002 ( 0.003)	Loss 3.3142e-02 (3.1168e-02)	Acc@1  97.66 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [63][300/391]	Time  0.033 ( 0.025)	Data  0.004 ( 0.003)	Loss 8.8577e-03 (3.0979e-02)	Acc@1 100.00 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [63][310/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.7288e-02 (3.1083e-02)	Acc@1  99.22 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [63][320/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.4763e-03 (3.0965e-02)	Acc@1 100.00 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [63][330/391]	Time  0.029 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.3732e-02 (3.1138e-02)	Acc@1  98.44 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [63][340/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 7.3059e-02 (3.1063e-02)	Acc@1  97.66 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [63][350/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.8269e-02 (3.1165e-02)	Acc@1  98.44 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [63][360/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.002)	Loss 7.1259e-03 (3.1116e-02)	Acc@1 100.00 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [63][370/391]	Time  0.024 ( 0.025)	Data  0.004 ( 0.002)	Loss 1.1322e-02 (3.1181e-02)	Acc@1 100.00 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [63][380/391]	Time  0.024 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.2736e-02 (3.1174e-02)	Acc@1  98.44 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [63][390/391]	Time  0.016 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.9948e-02 (3.1204e-02)	Acc@1  98.75 ( 98.97)	Acc@5 100.00 (100.00)
## e[63] optimizer.zero_grad (sum) time: 0.10392022132873535
## e[63]       loss.backward (sum) time: 2.187875509262085
## e[63]      optimizer.step (sum) time: 0.8998463153839111
## epoch[63] training(only) time: 9.714903831481934
# Switched to evaluate mode...
Test: [  0/100]	Time  0.162 ( 0.162)	Loss 3.2959e-01 (3.2959e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.008 ( 0.025)	Loss 5.7031e-01 (3.9039e-01)	Acc@1  90.00 ( 90.55)	Acc@5  99.00 ( 99.55)
Test: [ 20/100]	Time  0.019 ( 0.020)	Loss 5.3174e-01 (4.1500e-01)	Acc@1  87.00 ( 89.90)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.010 ( 0.018)	Loss 4.5410e-01 (4.1852e-01)	Acc@1  89.00 ( 90.19)	Acc@5  99.00 ( 99.58)
Test: [ 40/100]	Time  0.008 ( 0.017)	Loss 5.5664e-01 (4.4015e-01)	Acc@1  88.00 ( 89.78)	Acc@5 100.00 ( 99.59)
Test: [ 50/100]	Time  0.011 ( 0.016)	Loss 1.4978e-01 (4.2029e-01)	Acc@1  95.00 ( 90.16)	Acc@5 100.00 ( 99.61)
Test: [ 60/100]	Time  0.012 ( 0.015)	Loss 3.0615e-01 (4.2057e-01)	Acc@1  91.00 ( 90.10)	Acc@5 100.00 ( 99.62)
Test: [ 70/100]	Time  0.011 ( 0.015)	Loss 6.9678e-01 (4.1568e-01)	Acc@1  88.00 ( 90.23)	Acc@5  99.00 ( 99.62)
Test: [ 80/100]	Time  0.008 ( 0.015)	Loss 1.7407e-01 (4.1014e-01)	Acc@1  95.00 ( 90.30)	Acc@5 100.00 ( 99.63)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 2.8003e-01 (4.0614e-01)	Acc@1  92.00 ( 90.35)	Acc@5 100.00 ( 99.67)
 * Acc@1 90.290 Acc@5 99.700
### epoch[63] execution time: 11.26400375366211
EPOCH 64
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [64][  0/391]	Time  0.185 ( 0.185)	Data  0.164 ( 0.164)	Loss 7.0984e-02 (7.0984e-02)	Acc@1  97.66 ( 97.66)	Acc@5  99.22 ( 99.22)
Epoch: [64][ 10/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.017)	Loss 4.9095e-03 (3.7106e-02)	Acc@1 100.00 ( 98.93)	Acc@5 100.00 ( 99.93)
Epoch: [64][ 20/391]	Time  0.019 ( 0.031)	Data  0.002 ( 0.010)	Loss 1.0094e-02 (3.7319e-02)	Acc@1 100.00 ( 98.88)	Acc@5 100.00 ( 99.96)
Epoch: [64][ 30/391]	Time  0.035 ( 0.029)	Data  0.003 ( 0.007)	Loss 5.9845e-02 (3.6979e-02)	Acc@1  99.22 ( 98.92)	Acc@5 100.00 ( 99.95)
Epoch: [64][ 40/391]	Time  0.023 ( 0.028)	Data  0.000 ( 0.006)	Loss 3.5675e-02 (3.6614e-02)	Acc@1  98.44 ( 98.91)	Acc@5 100.00 ( 99.96)
Epoch: [64][ 50/391]	Time  0.018 ( 0.026)	Data  0.002 ( 0.005)	Loss 2.5253e-02 (3.7334e-02)	Acc@1  99.22 ( 98.90)	Acc@5 100.00 ( 99.97)
Epoch: [64][ 60/391]	Time  0.017 ( 0.026)	Data  0.000 ( 0.005)	Loss 1.4374e-02 (3.5442e-02)	Acc@1 100.00 ( 98.90)	Acc@5 100.00 ( 99.97)
Epoch: [64][ 70/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.004)	Loss 2.7527e-02 (3.3463e-02)	Acc@1  99.22 ( 98.95)	Acc@5 100.00 ( 99.98)
Epoch: [64][ 80/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.004)	Loss 6.5735e-02 (3.5066e-02)	Acc@1  98.44 ( 98.94)	Acc@5 100.00 ( 99.98)
Epoch: [64][ 90/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 3.0365e-02 (3.5921e-02)	Acc@1  98.44 ( 98.88)	Acc@5 100.00 ( 99.98)
Epoch: [64][100/391]	Time  0.027 ( 0.025)	Data  0.003 ( 0.004)	Loss 1.5869e-02 (3.4741e-02)	Acc@1 100.00 ( 98.93)	Acc@5 100.00 ( 99.98)
Epoch: [64][110/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.1799e-02 (3.4022e-02)	Acc@1  99.22 ( 98.95)	Acc@5 100.00 ( 99.98)
Epoch: [64][120/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 9.3155e-03 (3.2911e-02)	Acc@1 100.00 ( 99.00)	Acc@5 100.00 ( 99.98)
Epoch: [64][130/391]	Time  0.035 ( 0.025)	Data  0.004 ( 0.003)	Loss 2.7481e-02 (3.3186e-02)	Acc@1  99.22 ( 98.96)	Acc@5 100.00 ( 99.98)
Epoch: [64][140/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.6757e-03 (3.2186e-02)	Acc@1 100.00 ( 98.97)	Acc@5 100.00 ( 99.98)
Epoch: [64][150/391]	Time  0.029 ( 0.025)	Data  0.000 ( 0.003)	Loss 2.6321e-02 (3.2208e-02)	Acc@1  99.22 ( 98.98)	Acc@5 100.00 ( 99.98)
Epoch: [64][160/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.6488e-02 (3.1749e-02)	Acc@1  97.66 ( 98.99)	Acc@5 100.00 ( 99.99)
Epoch: [64][170/391]	Time  0.024 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.9581e-02 (3.1746e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [64][180/391]	Time  0.027 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.1860e-02 (3.2343e-02)	Acc@1  99.22 ( 98.99)	Acc@5 100.00 ( 99.99)
Epoch: [64][190/391]	Time  0.025 ( 0.025)	Data  0.002 ( 0.003)	Loss 6.3133e-03 (3.1741e-02)	Acc@1 100.00 ( 99.02)	Acc@5 100.00 ( 99.99)
Epoch: [64][200/391]	Time  0.030 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.5983e-03 (3.1292e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 ( 99.99)
Epoch: [64][210/391]	Time  0.031 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.4185e-02 (3.0828e-02)	Acc@1  98.44 ( 99.04)	Acc@5 100.00 ( 99.99)
Epoch: [64][220/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 2.9404e-02 (3.0630e-02)	Acc@1  98.44 ( 99.05)	Acc@5 100.00 ( 99.99)
Epoch: [64][230/391]	Time  0.024 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.6335e-02 (3.0900e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 ( 99.99)
Epoch: [64][240/391]	Time  0.027 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.2278e-02 (3.0619e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 ( 99.99)
Epoch: [64][250/391]	Time  0.026 ( 0.025)	Data  0.002 ( 0.002)	Loss 1.1169e-02 (3.0735e-02)	Acc@1 100.00 ( 99.04)	Acc@5 100.00 ( 99.99)
Epoch: [64][260/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.9805e-02 (3.0656e-02)	Acc@1  97.66 ( 99.03)	Acc@5 100.00 ( 99.99)
Epoch: [64][270/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.1311e-02 (3.0965e-02)	Acc@1  98.44 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [64][280/391]	Time  0.024 ( 0.025)	Data  0.003 ( 0.002)	Loss 2.0782e-02 (3.0773e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [64][290/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 6.2073e-02 (3.0560e-02)	Acc@1  97.66 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [64][300/391]	Time  0.028 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.1240e-02 (3.0199e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 ( 99.99)
Epoch: [64][310/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.5858e-02 (3.0040e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 ( 99.99)
Epoch: [64][320/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.3102e-02 (2.9865e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 ( 99.99)
Epoch: [64][330/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.1281e-02 (2.9896e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 ( 99.99)
Epoch: [64][340/391]	Time  0.033 ( 0.024)	Data  0.003 ( 0.002)	Loss 7.3120e-02 (2.9808e-02)	Acc@1  97.66 ( 99.05)	Acc@5 100.00 ( 99.99)
Epoch: [64][350/391]	Time  0.024 ( 0.024)	Data  0.002 ( 0.002)	Loss 2.1576e-02 (3.0104e-02)	Acc@1  98.44 ( 99.03)	Acc@5 100.00 ( 99.99)
Epoch: [64][360/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.4704e-02 (2.9828e-02)	Acc@1  98.44 ( 99.05)	Acc@5 100.00 ( 99.99)
Epoch: [64][370/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.5406e-02 (2.9966e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 ( 99.99)
Epoch: [64][380/391]	Time  0.038 ( 0.024)	Data  0.005 ( 0.002)	Loss 1.7517e-02 (3.0167e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 ( 99.99)
Epoch: [64][390/391]	Time  0.015 ( 0.024)	Data  0.001 ( 0.002)	Loss 6.6528e-02 (3.0179e-02)	Acc@1  98.75 ( 99.03)	Acc@5 100.00 ( 99.99)
## e[64] optimizer.zero_grad (sum) time: 0.10521697998046875
## e[64]       loss.backward (sum) time: 2.140366792678833
## e[64]      optimizer.step (sum) time: 0.8834209442138672
## epoch[64] training(only) time: 9.635279893875122
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 3.1934e-01 (3.1934e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.023 ( 0.027)	Loss 5.3613e-01 (3.8823e-01)	Acc@1  90.00 ( 90.55)	Acc@5  99.00 ( 99.73)
Test: [ 20/100]	Time  0.007 ( 0.020)	Loss 5.2197e-01 (4.1280e-01)	Acc@1  87.00 ( 90.05)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.011 ( 0.018)	Loss 4.8071e-01 (4.1691e-01)	Acc@1  88.00 ( 90.35)	Acc@5  99.00 ( 99.65)
Test: [ 40/100]	Time  0.007 ( 0.017)	Loss 5.6689e-01 (4.3955e-01)	Acc@1  88.00 ( 89.85)	Acc@5 100.00 ( 99.61)
Test: [ 50/100]	Time  0.022 ( 0.016)	Loss 1.4600e-01 (4.1964e-01)	Acc@1  94.00 ( 90.10)	Acc@5 100.00 ( 99.63)
Test: [ 60/100]	Time  0.008 ( 0.015)	Loss 2.8857e-01 (4.1952e-01)	Acc@1  92.00 ( 90.07)	Acc@5 100.00 ( 99.64)
Test: [ 70/100]	Time  0.027 ( 0.015)	Loss 6.9336e-01 (4.1456e-01)	Acc@1  87.00 ( 90.21)	Acc@5  99.00 ( 99.63)
Test: [ 80/100]	Time  0.008 ( 0.015)	Loss 1.7664e-01 (4.0882e-01)	Acc@1  95.00 ( 90.32)	Acc@5 100.00 ( 99.64)
Test: [ 90/100]	Time  0.008 ( 0.015)	Loss 2.9370e-01 (4.0456e-01)	Acc@1  90.00 ( 90.35)	Acc@5 100.00 ( 99.68)
 * Acc@1 90.320 Acc@5 99.710
### epoch[64] execution time: 11.200616598129272
EPOCH 65
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [65][  0/391]	Time  0.196 ( 0.196)	Data  0.176 ( 0.176)	Loss 5.3741e-02 (5.3741e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [65][ 10/391]	Time  0.019 ( 0.039)	Data  0.001 ( 0.018)	Loss 2.3026e-02 (3.0442e-02)	Acc@1  99.22 ( 98.86)	Acc@5 100.00 (100.00)
Epoch: [65][ 20/391]	Time  0.027 ( 0.032)	Data  0.001 ( 0.010)	Loss 2.6535e-02 (3.1126e-02)	Acc@1  99.22 ( 98.85)	Acc@5 100.00 (100.00)
Epoch: [65][ 30/391]	Time  0.019 ( 0.029)	Data  0.002 ( 0.007)	Loss 2.8656e-02 (3.2430e-02)	Acc@1  98.44 ( 98.84)	Acc@5 100.00 (100.00)
Epoch: [65][ 40/391]	Time  0.030 ( 0.028)	Data  0.001 ( 0.006)	Loss 9.3536e-03 (3.0218e-02)	Acc@1 100.00 ( 98.91)	Acc@5 100.00 (100.00)
Epoch: [65][ 50/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.005)	Loss 2.5955e-02 (2.9062e-02)	Acc@1  98.44 ( 98.94)	Acc@5 100.00 (100.00)
Epoch: [65][ 60/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.005)	Loss 2.8488e-02 (2.8526e-02)	Acc@1  98.44 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [65][ 70/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 2.2629e-02 (2.7226e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [65][ 80/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.3474e-02 (2.7567e-02)	Acc@1 100.00 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [65][ 90/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.004)	Loss 2.0294e-02 (2.7967e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [65][100/391]	Time  0.027 ( 0.025)	Data  0.002 ( 0.003)	Loss 1.8814e-02 (2.7765e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [65][110/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.8239e-02 (2.8146e-02)	Acc@1  98.44 ( 99.05)	Acc@5 100.00 ( 99.99)
Epoch: [65][120/391]	Time  0.038 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.3115e-02 (2.8443e-02)	Acc@1 100.00 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [65][130/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.8143e-02 (2.8430e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 ( 99.99)
Epoch: [65][140/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.8075e-02 (2.9206e-02)	Acc@1  97.66 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [65][150/391]	Time  0.032 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.3020e-02 (2.9342e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [65][160/391]	Time  0.032 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.0874e-02 (2.9521e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [65][170/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.6040e-02 (2.9379e-02)	Acc@1  98.44 ( 99.03)	Acc@5 100.00 ( 99.99)
Epoch: [65][180/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.4931e-02 (2.9746e-02)	Acc@1 100.00 ( 99.02)	Acc@5 100.00 ( 99.99)
Epoch: [65][190/391]	Time  0.036 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.2934e-02 (3.0000e-02)	Acc@1  99.22 ( 99.02)	Acc@5 100.00 ( 99.99)
Epoch: [65][200/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.3956e-02 (2.9689e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 ( 99.99)
Epoch: [65][210/391]	Time  0.027 ( 0.025)	Data  0.003 ( 0.003)	Loss 5.6030e-02 (2.9849e-02)	Acc@1  97.66 ( 99.02)	Acc@5 100.00 ( 99.99)
Epoch: [65][220/391]	Time  0.033 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.7059e-02 (2.9767e-02)	Acc@1  99.22 ( 99.02)	Acc@5 100.00 ( 99.99)
Epoch: [65][230/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.002)	Loss 5.7922e-02 (3.0254e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 ( 99.99)
Epoch: [65][240/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.002)	Loss 5.7892e-02 (3.0556e-02)	Acc@1  97.66 ( 98.99)	Acc@5 100.00 ( 99.99)
Epoch: [65][250/391]	Time  0.020 ( 0.025)	Data  0.002 ( 0.002)	Loss 2.1301e-02 (3.0330e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 ( 99.99)
Epoch: [65][260/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.0222e-02 (3.0208e-02)	Acc@1  98.44 ( 99.00)	Acc@5 100.00 ( 99.99)
Epoch: [65][270/391]	Time  0.035 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.8849e-02 (3.0348e-02)	Acc@1  98.44 ( 98.99)	Acc@5 100.00 ( 99.99)
Epoch: [65][280/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.002)	Loss 8.0200e-02 (3.0886e-02)	Acc@1  97.66 ( 98.97)	Acc@5 100.00 ( 99.99)
Epoch: [65][290/391]	Time  0.030 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.8300e-02 (3.0958e-02)	Acc@1  99.22 ( 98.98)	Acc@5 100.00 ( 99.99)
Epoch: [65][300/391]	Time  0.026 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.3901e-02 (3.0760e-02)	Acc@1 100.00 ( 99.00)	Acc@5 100.00 ( 99.99)
Epoch: [65][310/391]	Time  0.035 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.3573e-02 (3.1071e-02)	Acc@1  99.22 ( 98.98)	Acc@5 100.00 ( 99.99)
Epoch: [65][320/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.1748e-02 (3.1022e-02)	Acc@1  98.44 ( 98.98)	Acc@5 100.00 ( 99.99)
Epoch: [65][330/391]	Time  0.019 ( 0.024)	Data  0.002 ( 0.002)	Loss 1.2047e-02 (3.0973e-02)	Acc@1  99.22 ( 98.98)	Acc@5 100.00 ( 99.99)
Epoch: [65][340/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.9699e-02 (3.0883e-02)	Acc@1 100.00 ( 98.99)	Acc@5 100.00 ( 99.99)
Epoch: [65][350/391]	Time  0.019 ( 0.024)	Data  0.002 ( 0.002)	Loss 6.1127e-02 (3.0709e-02)	Acc@1  96.88 ( 99.00)	Acc@5 100.00 ( 99.99)
Epoch: [65][360/391]	Time  0.023 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.5991e-02 (3.0783e-02)	Acc@1 100.00 ( 99.00)	Acc@5 100.00 ( 99.99)
Epoch: [65][370/391]	Time  0.036 ( 0.024)	Data  0.005 ( 0.002)	Loss 4.8981e-02 (3.0714e-02)	Acc@1  97.66 ( 99.00)	Acc@5 100.00 ( 99.99)
Epoch: [65][380/391]	Time  0.022 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.7609e-02 (3.0551e-02)	Acc@1 100.00 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [65][390/391]	Time  0.014 ( 0.024)	Data  0.001 ( 0.002)	Loss 5.7068e-02 (3.0629e-02)	Acc@1  97.50 ( 99.00)	Acc@5 100.00 ( 99.99)
## e[65] optimizer.zero_grad (sum) time: 0.10488319396972656
## e[65]       loss.backward (sum) time: 2.153996467590332
## e[65]      optimizer.step (sum) time: 0.8872311115264893
## epoch[65] training(only) time: 9.640947103500366
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 3.1226e-01 (3.1226e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.016 ( 0.027)	Loss 5.1953e-01 (3.7718e-01)	Acc@1  90.00 ( 90.45)	Acc@5  99.00 ( 99.45)
Test: [ 20/100]	Time  0.008 ( 0.020)	Loss 5.3271e-01 (4.0877e-01)	Acc@1  87.00 ( 89.81)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.018 ( 0.017)	Loss 4.7754e-01 (4.1633e-01)	Acc@1  89.00 ( 90.13)	Acc@5  99.00 ( 99.55)
Test: [ 40/100]	Time  0.008 ( 0.017)	Loss 5.8057e-01 (4.3878e-01)	Acc@1  88.00 ( 89.73)	Acc@5 100.00 ( 99.56)
Test: [ 50/100]	Time  0.013 ( 0.016)	Loss 1.6370e-01 (4.1966e-01)	Acc@1  94.00 ( 90.08)	Acc@5 100.00 ( 99.57)
Test: [ 60/100]	Time  0.014 ( 0.016)	Loss 2.9248e-01 (4.2035e-01)	Acc@1  93.00 ( 90.03)	Acc@5 100.00 ( 99.57)
Test: [ 70/100]	Time  0.032 ( 0.015)	Loss 7.0801e-01 (4.1498e-01)	Acc@1  87.00 ( 90.21)	Acc@5  99.00 ( 99.56)
Test: [ 80/100]	Time  0.008 ( 0.015)	Loss 1.7969e-01 (4.0948e-01)	Acc@1  95.00 ( 90.28)	Acc@5 100.00 ( 99.58)
Test: [ 90/100]	Time  0.007 ( 0.015)	Loss 2.8174e-01 (4.0556e-01)	Acc@1  91.00 ( 90.34)	Acc@5 100.00 ( 99.63)
 * Acc@1 90.280 Acc@5 99.660
### epoch[65] execution time: 11.206686973571777
EPOCH 66
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [66][  0/391]	Time  0.184 ( 0.184)	Data  0.163 ( 0.163)	Loss 3.7903e-02 (3.7903e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [66][ 10/391]	Time  0.019 ( 0.038)	Data  0.001 ( 0.016)	Loss 6.1768e-02 (4.3866e-02)	Acc@1  97.66 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [66][ 20/391]	Time  0.018 ( 0.032)	Data  0.001 ( 0.009)	Loss 1.2260e-02 (3.6607e-02)	Acc@1 100.00 ( 98.74)	Acc@5 100.00 (100.00)
Epoch: [66][ 30/391]	Time  0.018 ( 0.029)	Data  0.001 ( 0.007)	Loss 8.5220e-03 (3.4552e-02)	Acc@1 100.00 ( 98.87)	Acc@5 100.00 (100.00)
Epoch: [66][ 40/391]	Time  0.018 ( 0.028)	Data  0.001 ( 0.006)	Loss 4.3518e-02 (3.4144e-02)	Acc@1  98.44 ( 98.88)	Acc@5 100.00 (100.00)
Epoch: [66][ 50/391]	Time  0.019 ( 0.027)	Data  0.002 ( 0.005)	Loss 1.9699e-02 (3.3749e-02)	Acc@1  99.22 ( 98.94)	Acc@5 100.00 (100.00)
Epoch: [66][ 60/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.5373e-02 (3.2702e-02)	Acc@1 100.00 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [66][ 70/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 5.1193e-03 (3.1414e-02)	Acc@1 100.00 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [66][ 80/391]	Time  0.027 ( 0.026)	Data  0.002 ( 0.004)	Loss 1.3138e-02 (3.1584e-02)	Acc@1 100.00 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [66][ 90/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 2.0233e-02 (3.1602e-02)	Acc@1 100.00 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [66][100/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.3972e-02 (3.1530e-02)	Acc@1  99.22 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [66][110/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.6541e-02 (3.1901e-02)	Acc@1  99.22 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [66][120/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.3641e-02 (3.2139e-02)	Acc@1 100.00 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [66][130/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 3.6224e-02 (3.2690e-02)	Acc@1  97.66 ( 98.88)	Acc@5 100.00 (100.00)
Epoch: [66][140/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.4586e-03 (3.1951e-02)	Acc@1 100.00 ( 98.92)	Acc@5 100.00 (100.00)
Epoch: [66][150/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 7.2098e-03 (3.1542e-02)	Acc@1 100.00 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [66][160/391]	Time  0.016 ( 0.025)	Data  0.000 ( 0.003)	Loss 4.7226e-03 (3.1362e-02)	Acc@1 100.00 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [66][170/391]	Time  0.027 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.2128e-01 (3.1579e-02)	Acc@1  97.66 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [66][180/391]	Time  0.028 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.6739e-02 (3.1085e-02)	Acc@1  98.44 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [66][190/391]	Time  0.031 ( 0.025)	Data  0.003 ( 0.003)	Loss 1.2207e-02 (3.0743e-02)	Acc@1 100.00 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [66][200/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.5009e-02 (3.1208e-02)	Acc@1  98.44 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [66][210/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 4.9667e-03 (3.1049e-02)	Acc@1 100.00 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [66][220/391]	Time  0.026 ( 0.025)	Data  0.002 ( 0.002)	Loss 1.7502e-02 (3.0725e-02)	Acc@1  99.22 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [66][230/391]	Time  0.031 ( 0.025)	Data  0.001 ( 0.002)	Loss 6.4735e-03 (3.0563e-02)	Acc@1 100.00 ( 99.00)	Acc@5 100.00 ( 99.99)
Epoch: [66][240/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.0863e-02 (3.1120e-02)	Acc@1  99.22 ( 98.99)	Acc@5 100.00 ( 99.99)
Epoch: [66][250/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 9.6985e-02 (3.1385e-02)	Acc@1  97.66 ( 98.98)	Acc@5 100.00 ( 99.99)
Epoch: [66][260/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.0325e-02 (3.1444e-02)	Acc@1  98.44 ( 98.98)	Acc@5 100.00 ( 99.99)
Epoch: [66][270/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.8723e-02 (3.1575e-02)	Acc@1 100.00 ( 98.99)	Acc@5 100.00 ( 99.99)
Epoch: [66][280/391]	Time  0.017 ( 0.024)	Data  0.000 ( 0.002)	Loss 1.4076e-02 (3.1414e-02)	Acc@1 100.00 ( 99.00)	Acc@5 100.00 ( 99.99)
Epoch: [66][290/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.1948e-03 (3.1079e-02)	Acc@1 100.00 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [66][300/391]	Time  0.027 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.9409e-02 (3.0825e-02)	Acc@1  99.22 ( 99.02)	Acc@5 100.00 ( 99.99)
Epoch: [66][310/391]	Time  0.026 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.7384e-02 (3.0774e-02)	Acc@1  97.66 ( 99.02)	Acc@5 100.00 ( 99.99)
Epoch: [66][320/391]	Time  0.022 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.7506e-02 (3.1016e-02)	Acc@1  98.44 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [66][330/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.0696e-02 (3.0719e-02)	Acc@1 100.00 ( 99.02)	Acc@5 100.00 ( 99.99)
Epoch: [66][340/391]	Time  0.019 ( 0.024)	Data  0.002 ( 0.002)	Loss 8.5266e-02 (3.0552e-02)	Acc@1  96.88 ( 99.02)	Acc@5  99.22 ( 99.99)
Epoch: [66][350/391]	Time  0.019 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.1314e-02 (3.0449e-02)	Acc@1 100.00 ( 99.02)	Acc@5 100.00 ( 99.99)
Epoch: [66][360/391]	Time  0.025 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.2385e-02 (3.0416e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [66][370/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.6367e-02 (3.0353e-02)	Acc@1  98.44 ( 99.02)	Acc@5 100.00 ( 99.99)
Epoch: [66][380/391]	Time  0.041 ( 0.024)	Data  0.001 ( 0.002)	Loss 6.4621e-03 (2.9999e-02)	Acc@1 100.00 ( 99.03)	Acc@5 100.00 ( 99.99)
Epoch: [66][390/391]	Time  0.014 ( 0.024)	Data  0.001 ( 0.002)	Loss 7.6103e-03 (3.0116e-02)	Acc@1 100.00 ( 99.02)	Acc@5 100.00 ( 99.99)
## e[66] optimizer.zero_grad (sum) time: 0.10550808906555176
## e[66]       loss.backward (sum) time: 2.1552276611328125
## e[66]      optimizer.step (sum) time: 0.892681360244751
## epoch[66] training(only) time: 9.615027666091919
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 3.2324e-01 (3.2324e-01)	Acc@1  91.00 ( 91.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.016 ( 0.027)	Loss 5.3271e-01 (3.8364e-01)	Acc@1  90.00 ( 90.64)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.008 ( 0.020)	Loss 5.2979e-01 (4.1266e-01)	Acc@1  87.00 ( 90.00)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.008 ( 0.018)	Loss 4.6875e-01 (4.1761e-01)	Acc@1  90.00 ( 90.32)	Acc@5  99.00 ( 99.61)
Test: [ 40/100]	Time  0.008 ( 0.017)	Loss 5.6787e-01 (4.4058e-01)	Acc@1  88.00 ( 89.85)	Acc@5 100.00 ( 99.61)
Test: [ 50/100]	Time  0.011 ( 0.016)	Loss 1.5930e-01 (4.2142e-01)	Acc@1  95.00 ( 90.22)	Acc@5 100.00 ( 99.61)
Test: [ 60/100]	Time  0.014 ( 0.016)	Loss 2.8125e-01 (4.2079e-01)	Acc@1  92.00 ( 90.16)	Acc@5 100.00 ( 99.61)
Test: [ 70/100]	Time  0.011 ( 0.015)	Loss 7.1191e-01 (4.1601e-01)	Acc@1  86.00 ( 90.30)	Acc@5  99.00 ( 99.59)
Test: [ 80/100]	Time  0.015 ( 0.015)	Loss 1.7676e-01 (4.1088e-01)	Acc@1  96.00 ( 90.38)	Acc@5 100.00 ( 99.60)
Test: [ 90/100]	Time  0.014 ( 0.015)	Loss 2.9395e-01 (4.0674e-01)	Acc@1  90.00 ( 90.35)	Acc@5 100.00 ( 99.65)
 * Acc@1 90.350 Acc@5 99.680
### epoch[66] execution time: 11.174859285354614
EPOCH 67
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [67][  0/391]	Time  0.255 ( 0.255)	Data  0.234 ( 0.234)	Loss 7.9346e-02 (7.9346e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [67][ 10/391]	Time  0.020 ( 0.044)	Data  0.001 ( 0.023)	Loss 7.2937e-02 (3.7202e-02)	Acc@1  96.88 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [67][ 20/391]	Time  0.018 ( 0.034)	Data  0.001 ( 0.014)	Loss 1.2909e-02 (4.3707e-02)	Acc@1  99.22 ( 98.66)	Acc@5 100.00 (100.00)
Epoch: [67][ 30/391]	Time  0.018 ( 0.031)	Data  0.001 ( 0.010)	Loss 1.5129e-02 (3.7258e-02)	Acc@1 100.00 ( 98.89)	Acc@5 100.00 (100.00)
Epoch: [67][ 40/391]	Time  0.017 ( 0.029)	Data  0.000 ( 0.008)	Loss 7.7782e-03 (3.4018e-02)	Acc@1 100.00 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [67][ 50/391]	Time  0.036 ( 0.028)	Data  0.001 ( 0.007)	Loss 8.1299e-02 (3.4527e-02)	Acc@1  96.88 ( 98.88)	Acc@5 100.00 (100.00)
Epoch: [67][ 60/391]	Time  0.032 ( 0.027)	Data  0.000 ( 0.006)	Loss 3.2501e-02 (3.2742e-02)	Acc@1  99.22 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [67][ 70/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.005)	Loss 1.4053e-02 (3.3989e-02)	Acc@1  99.22 ( 98.94)	Acc@5 100.00 (100.00)
Epoch: [67][ 80/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.005)	Loss 1.3290e-02 (3.2211e-02)	Acc@1 100.00 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [67][ 90/391]	Time  0.035 ( 0.026)	Data  0.004 ( 0.004)	Loss 1.1986e-02 (3.1840e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [67][100/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 3.2745e-02 (3.3492e-02)	Acc@1  97.66 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [67][110/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.9440e-02 (3.3691e-02)	Acc@1  99.22 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [67][120/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.004)	Loss 2.3193e-02 (3.3934e-02)	Acc@1  99.22 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [67][130/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.004)	Loss 5.5389e-02 (3.3442e-02)	Acc@1  97.66 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [67][140/391]	Time  0.019 ( 0.026)	Data  0.002 ( 0.004)	Loss 8.1635e-03 (3.3115e-02)	Acc@1 100.00 ( 98.94)	Acc@5 100.00 (100.00)
Epoch: [67][150/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.7932e-02 (3.2619e-02)	Acc@1  98.44 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [67][160/391]	Time  0.044 ( 0.025)	Data  0.009 ( 0.003)	Loss 1.9608e-02 (3.2273e-02)	Acc@1  99.22 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [67][170/391]	Time  0.032 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.5299e-02 (3.1878e-02)	Acc@1  98.44 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [67][180/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.8447e-03 (3.2015e-02)	Acc@1 100.00 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [67][190/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.0701e-02 (3.1923e-02)	Acc@1  99.22 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [67][200/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 2.1881e-02 (3.1615e-02)	Acc@1  98.44 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [67][210/391]	Time  0.026 ( 0.025)	Data  0.000 ( 0.003)	Loss 1.3672e-02 (3.2192e-02)	Acc@1 100.00 ( 98.97)	Acc@5 100.00 ( 99.99)
Epoch: [67][220/391]	Time  0.017 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.7313e-02 (3.2127e-02)	Acc@1  99.22 ( 98.96)	Acc@5 100.00 ( 99.99)
Epoch: [67][230/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.4354e-03 (3.1847e-02)	Acc@1 100.00 ( 98.96)	Acc@5 100.00 ( 99.99)
Epoch: [67][240/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.6989e-03 (3.1999e-02)	Acc@1 100.00 ( 98.95)	Acc@5 100.00 ( 99.99)
Epoch: [67][250/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.2745e-02 (3.1724e-02)	Acc@1  97.66 ( 98.96)	Acc@5 100.00 ( 99.99)
Epoch: [67][260/391]	Time  0.034 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.5533e-02 (3.1598e-02)	Acc@1  99.22 ( 98.96)	Acc@5 100.00 ( 99.99)
Epoch: [67][270/391]	Time  0.027 ( 0.025)	Data  0.003 ( 0.003)	Loss 2.8763e-02 (3.1795e-02)	Acc@1  99.22 ( 98.97)	Acc@5 100.00 ( 99.99)
Epoch: [67][280/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 7.9956e-02 (3.1939e-02)	Acc@1  98.44 ( 98.97)	Acc@5 100.00 ( 99.99)
Epoch: [67][290/391]	Time  0.026 ( 0.025)	Data  0.001 ( 0.003)	Loss 7.1526e-03 (3.2216e-02)	Acc@1 100.00 ( 98.94)	Acc@5 100.00 ( 99.99)
Epoch: [67][300/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 1.5388e-02 (3.2295e-02)	Acc@1  99.22 ( 98.95)	Acc@5 100.00 ( 99.99)
Epoch: [67][310/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.2522e-02 (3.1991e-02)	Acc@1  99.22 ( 98.95)	Acc@5 100.00 ( 99.99)
Epoch: [67][320/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 7.5455e-03 (3.1725e-02)	Acc@1 100.00 ( 98.97)	Acc@5 100.00 ( 99.99)
Epoch: [67][330/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.8757e-02 (3.1732e-02)	Acc@1  98.44 ( 98.96)	Acc@5 100.00 ( 99.99)
Epoch: [67][340/391]	Time  0.045 ( 0.025)	Data  0.012 ( 0.003)	Loss 2.7451e-02 (3.1786e-02)	Acc@1  99.22 ( 98.96)	Acc@5 100.00 ( 99.99)
Epoch: [67][350/391]	Time  0.018 ( 0.025)	Data  0.002 ( 0.002)	Loss 3.7567e-02 (3.1897e-02)	Acc@1  98.44 ( 98.96)	Acc@5 100.00 ( 99.99)
Epoch: [67][360/391]	Time  0.017 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.0994e-02 (3.1699e-02)	Acc@1 100.00 ( 98.97)	Acc@5 100.00 ( 99.99)
Epoch: [67][370/391]	Time  0.032 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.1536e-02 (3.1522e-02)	Acc@1  99.22 ( 98.98)	Acc@5 100.00 ( 99.99)
Epoch: [67][380/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.6016e-02 (3.1776e-02)	Acc@1  99.22 ( 98.96)	Acc@5 100.00 ( 99.99)
Epoch: [67][390/391]	Time  0.017 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.0300e-02 (3.1515e-02)	Acc@1 100.00 ( 98.97)	Acc@5 100.00 ( 99.99)
## e[67] optimizer.zero_grad (sum) time: 0.1056523323059082
## e[67]       loss.backward (sum) time: 2.1258394718170166
## e[67]      optimizer.step (sum) time: 0.8904335498809814
## epoch[67] training(only) time: 9.737212657928467
# Switched to evaluate mode...
Test: [  0/100]	Time  0.169 ( 0.169)	Loss 3.1128e-01 (3.1128e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.016 ( 0.028)	Loss 5.0098e-01 (3.7823e-01)	Acc@1  90.00 ( 90.64)	Acc@5  99.00 ( 99.36)
Test: [ 20/100]	Time  0.015 ( 0.021)	Loss 5.3027e-01 (4.0812e-01)	Acc@1  87.00 ( 89.76)	Acc@5 100.00 ( 99.52)
Test: [ 30/100]	Time  0.008 ( 0.018)	Loss 4.7900e-01 (4.1236e-01)	Acc@1  89.00 ( 90.16)	Acc@5  99.00 ( 99.52)
Test: [ 40/100]	Time  0.011 ( 0.017)	Loss 5.8398e-01 (4.3555e-01)	Acc@1  88.00 ( 89.78)	Acc@5  99.00 ( 99.51)
Test: [ 50/100]	Time  0.011 ( 0.016)	Loss 1.5869e-01 (4.1677e-01)	Acc@1  95.00 ( 90.14)	Acc@5 100.00 ( 99.53)
Test: [ 60/100]	Time  0.015 ( 0.016)	Loss 2.9126e-01 (4.1642e-01)	Acc@1  92.00 ( 90.11)	Acc@5 100.00 ( 99.56)
Test: [ 70/100]	Time  0.008 ( 0.015)	Loss 7.1240e-01 (4.1149e-01)	Acc@1  87.00 ( 90.25)	Acc@5  99.00 ( 99.55)
Test: [ 80/100]	Time  0.016 ( 0.015)	Loss 1.7480e-01 (4.0636e-01)	Acc@1  95.00 ( 90.32)	Acc@5 100.00 ( 99.57)
Test: [ 90/100]	Time  0.016 ( 0.015)	Loss 2.6416e-01 (4.0197e-01)	Acc@1  91.00 ( 90.34)	Acc@5 100.00 ( 99.62)
 * Acc@1 90.300 Acc@5 99.650
### epoch[67] execution time: 11.297845363616943
EPOCH 68
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [68][  0/391]	Time  0.192 ( 0.192)	Data  0.171 ( 0.171)	Loss 1.5182e-02 (1.5182e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [68][ 10/391]	Time  0.019 ( 0.039)	Data  0.001 ( 0.017)	Loss 9.2285e-02 (3.7535e-02)	Acc@1  97.66 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [68][ 20/391]	Time  0.018 ( 0.032)	Data  0.001 ( 0.010)	Loss 4.9622e-02 (3.8462e-02)	Acc@1  96.88 ( 98.81)	Acc@5 100.00 ( 99.96)
Epoch: [68][ 30/391]	Time  0.018 ( 0.029)	Data  0.001 ( 0.008)	Loss 2.5467e-02 (3.4771e-02)	Acc@1 100.00 ( 98.94)	Acc@5 100.00 ( 99.97)
Epoch: [68][ 40/391]	Time  0.018 ( 0.028)	Data  0.001 ( 0.006)	Loss 1.0864e-01 (3.5800e-02)	Acc@1  96.88 ( 98.91)	Acc@5 100.00 ( 99.98)
Epoch: [68][ 50/391]	Time  0.031 ( 0.027)	Data  0.001 ( 0.005)	Loss 4.1016e-02 (3.3865e-02)	Acc@1  98.44 ( 98.94)	Acc@5 100.00 ( 99.98)
Epoch: [68][ 60/391]	Time  0.017 ( 0.026)	Data  0.000 ( 0.005)	Loss 7.6714e-03 (3.2040e-02)	Acc@1 100.00 ( 99.00)	Acc@5 100.00 ( 99.99)
Epoch: [68][ 70/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 4.1840e-02 (3.2640e-02)	Acc@1  97.66 ( 98.93)	Acc@5 100.00 ( 99.99)
Epoch: [68][ 80/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.8600e-02 (3.3290e-02)	Acc@1  99.22 ( 98.86)	Acc@5 100.00 ( 99.99)
Epoch: [68][ 90/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.004)	Loss 1.0388e-01 (3.3499e-02)	Acc@1  96.88 ( 98.90)	Acc@5  99.22 ( 99.98)
Epoch: [68][100/391]	Time  0.029 ( 0.025)	Data  0.002 ( 0.003)	Loss 1.4786e-02 (3.3475e-02)	Acc@1 100.00 ( 98.91)	Acc@5 100.00 ( 99.98)
Epoch: [68][110/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.0863e-02 (3.3430e-02)	Acc@1  98.44 ( 98.89)	Acc@5 100.00 ( 99.99)
Epoch: [68][120/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.3894e-02 (3.3468e-02)	Acc@1  97.66 ( 98.86)	Acc@5 100.00 ( 99.99)
Epoch: [68][130/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.6736e-03 (3.2344e-02)	Acc@1 100.00 ( 98.92)	Acc@5 100.00 ( 99.99)
Epoch: [68][140/391]	Time  0.027 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.8910e-02 (3.2271e-02)	Acc@1  98.44 ( 98.92)	Acc@5 100.00 ( 99.99)
Epoch: [68][150/391]	Time  0.037 ( 0.025)	Data  0.005 ( 0.003)	Loss 1.5793e-02 (3.2136e-02)	Acc@1  99.22 ( 98.93)	Acc@5 100.00 ( 99.99)
Epoch: [68][160/391]	Time  0.032 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.5925e-02 (3.1306e-02)	Acc@1  99.22 ( 98.96)	Acc@5 100.00 ( 99.99)
Epoch: [68][170/391]	Time  0.028 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.0930e-02 (3.1710e-02)	Acc@1  99.22 ( 98.92)	Acc@5 100.00 ( 99.99)
Epoch: [68][180/391]	Time  0.036 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.4862e-02 (3.2055e-02)	Acc@1 100.00 ( 98.91)	Acc@5 100.00 ( 99.99)
Epoch: [68][190/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 1.1261e-02 (3.1655e-02)	Acc@1 100.00 ( 98.95)	Acc@5 100.00 ( 99.99)
Epoch: [68][200/391]	Time  0.032 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.6713e-02 (3.1255e-02)	Acc@1  98.44 ( 98.94)	Acc@5 100.00 ( 99.99)
Epoch: [68][210/391]	Time  0.026 ( 0.025)	Data  0.003 ( 0.003)	Loss 3.0060e-02 (3.1176e-02)	Acc@1  98.44 ( 98.93)	Acc@5 100.00 ( 99.99)
Epoch: [68][220/391]	Time  0.025 ( 0.025)	Data  0.003 ( 0.003)	Loss 9.2926e-03 (3.0692e-02)	Acc@1 100.00 ( 98.95)	Acc@5 100.00 ( 99.99)
Epoch: [68][230/391]	Time  0.024 ( 0.025)	Data  0.002 ( 0.003)	Loss 2.8076e-02 (3.0700e-02)	Acc@1  98.44 ( 98.95)	Acc@5 100.00 ( 99.99)
Epoch: [68][240/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.1002e-02 (3.0376e-02)	Acc@1  99.22 ( 98.96)	Acc@5 100.00 ( 99.99)
Epoch: [68][250/391]	Time  0.017 ( 0.024)	Data  0.000 ( 0.002)	Loss 1.6205e-02 (2.9895e-02)	Acc@1  99.22 ( 98.98)	Acc@5 100.00 ( 99.99)
Epoch: [68][260/391]	Time  0.027 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.3634e-02 (2.9814e-02)	Acc@1  99.22 ( 98.99)	Acc@5 100.00 ( 99.99)
Epoch: [68][270/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.6321e-03 (2.9960e-02)	Acc@1 100.00 ( 98.98)	Acc@5 100.00 ( 99.99)
Epoch: [68][280/391]	Time  0.019 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.9241e-02 (2.9663e-02)	Acc@1 100.00 ( 99.00)	Acc@5 100.00 ( 99.99)
Epoch: [68][290/391]	Time  0.019 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.0905e-02 (2.9691e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [68][300/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.6526e-03 (2.9721e-02)	Acc@1 100.00 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [68][310/391]	Time  0.021 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.3712e-02 (2.9744e-02)	Acc@1 100.00 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [68][320/391]	Time  0.029 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.2039e-02 (2.9754e-02)	Acc@1 100.00 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [68][330/391]	Time  0.019 ( 0.024)	Data  0.002 ( 0.002)	Loss 2.6718e-02 (2.9954e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 ( 99.99)
Epoch: [68][340/391]	Time  0.027 ( 0.024)	Data  0.001 ( 0.002)	Loss 7.5455e-03 (2.9680e-02)	Acc@1 100.00 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [68][350/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.9459e-02 (2.9776e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [68][360/391]	Time  0.031 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.0721e-02 (2.9717e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [68][370/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 4.4220e-02 (2.9565e-02)	Acc@1  99.22 ( 99.02)	Acc@5 100.00 ( 99.99)
Epoch: [68][380/391]	Time  0.033 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.4307e-02 (2.9632e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [68][390/391]	Time  0.015 ( 0.024)	Data  0.000 ( 0.002)	Loss 2.9984e-03 (2.9759e-02)	Acc@1 100.00 ( 99.01)	Acc@5 100.00 ( 99.99)
## e[68] optimizer.zero_grad (sum) time: 0.10579109191894531
## e[68]       loss.backward (sum) time: 2.1691532135009766
## e[68]      optimizer.step (sum) time: 0.8979747295379639
## epoch[68] training(only) time: 9.61625051498413
# Switched to evaluate mode...
Test: [  0/100]	Time  0.158 ( 0.158)	Loss 3.1104e-01 (3.1104e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.019 ( 0.027)	Loss 5.2393e-01 (3.8994e-01)	Acc@1  90.00 ( 90.45)	Acc@5  99.00 ( 99.55)
Test: [ 20/100]	Time  0.016 ( 0.020)	Loss 5.3223e-01 (4.1603e-01)	Acc@1  87.00 ( 90.05)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.017 ( 0.018)	Loss 4.8486e-01 (4.2017e-01)	Acc@1  90.00 ( 90.29)	Acc@5  99.00 ( 99.61)
Test: [ 40/100]	Time  0.013 ( 0.017)	Loss 5.7324e-01 (4.4126e-01)	Acc@1  89.00 ( 89.85)	Acc@5 100.00 ( 99.61)
Test: [ 50/100]	Time  0.014 ( 0.016)	Loss 1.5771e-01 (4.2140e-01)	Acc@1  95.00 ( 90.14)	Acc@5 100.00 ( 99.63)
Test: [ 60/100]	Time  0.016 ( 0.016)	Loss 2.8979e-01 (4.2204e-01)	Acc@1  93.00 ( 90.08)	Acc@5 100.00 ( 99.64)
Test: [ 70/100]	Time  0.011 ( 0.015)	Loss 6.9922e-01 (4.1713e-01)	Acc@1  89.00 ( 90.27)	Acc@5  99.00 ( 99.63)
Test: [ 80/100]	Time  0.014 ( 0.015)	Loss 1.6931e-01 (4.0996e-01)	Acc@1  96.00 ( 90.40)	Acc@5 100.00 ( 99.64)
Test: [ 90/100]	Time  0.014 ( 0.015)	Loss 2.6294e-01 (4.0549e-01)	Acc@1  92.00 ( 90.44)	Acc@5 100.00 ( 99.68)
 * Acc@1 90.430 Acc@5 99.710
### epoch[68] execution time: 11.174164295196533
EPOCH 69
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [69][  0/391]	Time  0.191 ( 0.191)	Data  0.170 ( 0.170)	Loss 3.2257e-02 (3.2257e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [69][ 10/391]	Time  0.025 ( 0.039)	Data  0.001 ( 0.017)	Loss 2.8076e-02 (3.7981e-02)	Acc@1  99.22 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [69][ 20/391]	Time  0.020 ( 0.032)	Data  0.001 ( 0.010)	Loss 3.9093e-02 (3.2168e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 ( 99.96)
Epoch: [69][ 30/391]	Time  0.020 ( 0.029)	Data  0.001 ( 0.007)	Loss 3.2867e-02 (3.2598e-02)	Acc@1  98.44 ( 99.07)	Acc@5 100.00 ( 99.97)
Epoch: [69][ 40/391]	Time  0.018 ( 0.028)	Data  0.001 ( 0.006)	Loss 3.0701e-02 (3.1172e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 ( 99.98)
Epoch: [69][ 50/391]	Time  0.018 ( 0.027)	Data  0.002 ( 0.005)	Loss 3.7048e-02 (3.3858e-02)	Acc@1  99.22 ( 99.02)	Acc@5 100.00 ( 99.98)
Epoch: [69][ 60/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.005)	Loss 1.2581e-02 (3.2626e-02)	Acc@1 100.00 ( 99.03)	Acc@5 100.00 ( 99.99)
Epoch: [69][ 70/391]	Time  0.017 ( 0.026)	Data  0.001 ( 0.004)	Loss 7.4829e-02 (3.1625e-02)	Acc@1  97.66 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [69][ 80/391]	Time  0.027 ( 0.026)	Data  0.002 ( 0.004)	Loss 3.5126e-02 (3.1309e-02)	Acc@1  99.22 ( 99.09)	Acc@5 100.00 ( 99.99)
Epoch: [69][ 90/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 3.4866e-03 (3.1411e-02)	Acc@1 100.00 ( 99.09)	Acc@5 100.00 ( 99.99)
Epoch: [69][100/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1826e-02 (3.0991e-02)	Acc@1 100.00 ( 99.08)	Acc@5 100.00 ( 99.99)
Epoch: [69][110/391]	Time  0.026 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.6047e-02 (3.1775e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 ( 99.99)
Epoch: [69][120/391]	Time  0.039 ( 0.025)	Data  0.001 ( 0.003)	Loss 7.0915e-03 (3.1080e-02)	Acc@1 100.00 ( 99.04)	Acc@5 100.00 ( 99.99)
Epoch: [69][130/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 2.3926e-02 (3.0685e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [69][140/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.3056e-02 (3.0649e-02)	Acc@1  99.22 ( 99.08)	Acc@5 100.00 ( 99.99)
Epoch: [69][150/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.2552e-02 (3.0678e-02)	Acc@1  99.22 ( 99.08)	Acc@5 100.00 ( 99.99)
Epoch: [69][160/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.7532e-02 (3.0260e-02)	Acc@1  99.22 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [69][170/391]	Time  0.029 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.8877e-03 (2.9598e-02)	Acc@1 100.00 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [69][180/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.9581e-02 (2.9409e-02)	Acc@1  97.66 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [69][190/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 1.8433e-02 (2.9156e-02)	Acc@1  99.22 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [69][200/391]	Time  0.030 ( 0.025)	Data  0.000 ( 0.003)	Loss 1.1145e-01 (2.9353e-02)	Acc@1  96.09 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [69][210/391]	Time  0.018 ( 0.025)	Data  0.002 ( 0.003)	Loss 7.6843e-02 (2.9458e-02)	Acc@1  98.44 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [69][220/391]	Time  0.026 ( 0.025)	Data  0.001 ( 0.003)	Loss 9.1858e-03 (2.8987e-02)	Acc@1 100.00 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [69][230/391]	Time  0.028 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.8065e-03 (2.9590e-02)	Acc@1 100.00 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [69][240/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 8.4106e-02 (2.9698e-02)	Acc@1  98.44 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [69][250/391]	Time  0.039 ( 0.025)	Data  0.001 ( 0.002)	Loss 5.0354e-02 (3.0422e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [69][260/391]	Time  0.029 ( 0.025)	Data  0.000 ( 0.002)	Loss 2.7451e-02 (3.0387e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [69][270/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 8.4778e-02 (3.0598e-02)	Acc@1  97.66 ( 99.04)	Acc@5  99.22 ( 99.99)
Epoch: [69][280/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.6052e-02 (3.0251e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 ( 99.99)
Epoch: [69][290/391]	Time  0.028 ( 0.025)	Data  0.001 ( 0.002)	Loss 8.5602e-03 (3.0243e-02)	Acc@1 100.00 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [69][300/391]	Time  0.028 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.5065e-02 (3.0209e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [69][310/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.2572e-02 (2.9912e-02)	Acc@1  98.44 ( 99.07)	Acc@5 100.00 ( 99.99)
Epoch: [69][320/391]	Time  0.038 ( 0.025)	Data  0.004 ( 0.002)	Loss 6.0486e-02 (2.9834e-02)	Acc@1  98.44 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [69][330/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.2947e-02 (2.9920e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 ( 99.99)
Epoch: [69][340/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.4557e-02 (2.9653e-02)	Acc@1  99.22 ( 99.08)	Acc@5 100.00 ( 99.99)
Epoch: [69][350/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.7838e-02 (2.9669e-02)	Acc@1 100.00 ( 99.08)	Acc@5 100.00 ( 99.99)
Epoch: [69][360/391]	Time  0.033 ( 0.025)	Data  0.003 ( 0.002)	Loss 4.4785e-03 (2.9145e-02)	Acc@1 100.00 ( 99.10)	Acc@5 100.00 ( 99.99)
Epoch: [69][370/391]	Time  0.034 ( 0.025)	Data  0.002 ( 0.002)	Loss 4.1016e-02 (2.9098e-02)	Acc@1  99.22 ( 99.09)	Acc@5 100.00 ( 99.99)
Epoch: [69][380/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.6596e-02 (2.8930e-02)	Acc@1  99.22 ( 99.10)	Acc@5 100.00 ( 99.99)
Epoch: [69][390/391]	Time  0.015 ( 0.024)	Data  0.000 ( 0.002)	Loss 3.3569e-02 (2.8862e-02)	Acc@1  97.50 ( 99.10)	Acc@5 100.00 ( 99.99)
## e[69] optimizer.zero_grad (sum) time: 0.1050405502319336
## e[69]       loss.backward (sum) time: 2.1653006076812744
## e[69]      optimizer.step (sum) time: 0.8739340305328369
## epoch[69] training(only) time: 9.712303400039673
# Switched to evaluate mode...
Test: [  0/100]	Time  0.156 ( 0.156)	Loss 2.9858e-01 (2.9858e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.014 ( 0.026)	Loss 5.1416e-01 (3.8401e-01)	Acc@1  90.00 ( 90.64)	Acc@5  99.00 ( 99.45)
Test: [ 20/100]	Time  0.017 ( 0.020)	Loss 5.4248e-01 (4.1276e-01)	Acc@1  87.00 ( 90.05)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.010 ( 0.018)	Loss 4.6606e-01 (4.1687e-01)	Acc@1  90.00 ( 90.32)	Acc@5  99.00 ( 99.58)
Test: [ 40/100]	Time  0.014 ( 0.017)	Loss 5.6201e-01 (4.3804e-01)	Acc@1  89.00 ( 89.90)	Acc@5 100.00 ( 99.59)
Test: [ 50/100]	Time  0.019 ( 0.016)	Loss 1.5515e-01 (4.1835e-01)	Acc@1  95.00 ( 90.18)	Acc@5 100.00 ( 99.57)
Test: [ 60/100]	Time  0.007 ( 0.016)	Loss 2.8833e-01 (4.1809e-01)	Acc@1  93.00 ( 90.07)	Acc@5 100.00 ( 99.59)
Test: [ 70/100]	Time  0.013 ( 0.015)	Loss 6.9434e-01 (4.1255e-01)	Acc@1  88.00 ( 90.24)	Acc@5  99.00 ( 99.58)
Test: [ 80/100]	Time  0.013 ( 0.015)	Loss 1.7554e-01 (4.0593e-01)	Acc@1  96.00 ( 90.36)	Acc@5 100.00 ( 99.59)
Test: [ 90/100]	Time  0.012 ( 0.015)	Loss 2.7539e-01 (4.0163e-01)	Acc@1  91.00 ( 90.38)	Acc@5 100.00 ( 99.64)
 * Acc@1 90.380 Acc@5 99.660
### epoch[69] execution time: 11.329070568084717
EPOCH 70
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [70][  0/391]	Time  0.198 ( 0.198)	Data  0.176 ( 0.176)	Loss 6.6101e-02 (6.6101e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [70][ 10/391]	Time  0.025 ( 0.040)	Data  0.001 ( 0.017)	Loss 6.3667e-03 (2.5589e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [70][ 20/391]	Time  0.018 ( 0.032)	Data  0.001 ( 0.010)	Loss 1.0544e-02 (2.7380e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [70][ 30/391]	Time  0.018 ( 0.029)	Data  0.001 ( 0.007)	Loss 5.0018e-02 (2.5204e-02)	Acc@1  97.66 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [70][ 40/391]	Time  0.018 ( 0.028)	Data  0.001 ( 0.006)	Loss 1.6586e-02 (2.6275e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [70][ 50/391]	Time  0.031 ( 0.027)	Data  0.000 ( 0.005)	Loss 1.7853e-02 (2.6631e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [70][ 60/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.005)	Loss 9.5749e-03 (2.4835e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [70][ 70/391]	Time  0.037 ( 0.026)	Data  0.002 ( 0.004)	Loss 4.6539e-02 (2.5466e-02)	Acc@1  98.44 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [70][ 80/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.4114e-02 (2.4756e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [70][ 90/391]	Time  0.038 ( 0.026)	Data  0.005 ( 0.004)	Loss 4.7394e-02 (2.5842e-02)	Acc@1  98.44 ( 99.23)	Acc@5  99.22 ( 99.99)
Epoch: [70][100/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.8265e-02 (2.6540e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 ( 99.99)
Epoch: [70][110/391]	Time  0.037 ( 0.025)	Data  0.005 ( 0.003)	Loss 4.8523e-02 (2.7023e-02)	Acc@1  98.44 ( 99.16)	Acc@5 100.00 ( 99.99)
Epoch: [70][120/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.5084e-03 (2.7167e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 ( 99.99)
Epoch: [70][130/391]	Time  0.038 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.7861e-02 (2.7021e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 ( 99.99)
Epoch: [70][140/391]	Time  0.038 ( 0.025)	Data  0.005 ( 0.003)	Loss 7.2021e-03 (2.6855e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 ( 99.99)
Epoch: [70][150/391]	Time  0.027 ( 0.025)	Data  0.004 ( 0.003)	Loss 2.3010e-02 (2.7038e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 ( 99.99)
Epoch: [70][160/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.0986e-02 (2.6814e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [70][170/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.8157e-02 (2.7048e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [70][180/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.3100e-02 (2.7260e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [70][190/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.9612e-02 (2.7572e-02)	Acc@1  98.44 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [70][200/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.6173e-02 (2.7271e-02)	Acc@1  97.66 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [70][210/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.5472e-02 (2.7282e-02)	Acc@1 100.00 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [70][220/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.7105e-02 (2.6922e-02)	Acc@1  99.22 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [70][230/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.6749e-02 (2.7068e-02)	Acc@1  99.22 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [70][240/391]	Time  0.041 ( 0.025)	Data  0.004 ( 0.003)	Loss 1.0605e-02 (2.7559e-02)	Acc@1 100.00 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [70][250/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 1.8890e-02 (2.7244e-02)	Acc@1  99.22 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [70][260/391]	Time  0.017 ( 0.024)	Data  0.000 ( 0.003)	Loss 1.8387e-02 (2.7217e-02)	Acc@1  99.22 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [70][270/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 4.5105e-02 (2.7330e-02)	Acc@1  97.66 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [70][280/391]	Time  0.035 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.4637e-02 (2.7027e-02)	Acc@1  98.44 ( 99.11)	Acc@5 100.00 ( 99.99)
Epoch: [70][290/391]	Time  0.039 ( 0.024)	Data  0.000 ( 0.002)	Loss 7.8186e-02 (2.7285e-02)	Acc@1  97.66 ( 99.10)	Acc@5 100.00 ( 99.99)
Epoch: [70][300/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 8.9035e-03 (2.7380e-02)	Acc@1 100.00 ( 99.10)	Acc@5 100.00 ( 99.99)
Epoch: [70][310/391]	Time  0.033 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.6647e-02 (2.7850e-02)	Acc@1  99.22 ( 99.09)	Acc@5 100.00 ( 99.99)
Epoch: [70][320/391]	Time  0.040 ( 0.024)	Data  0.001 ( 0.002)	Loss 4.0314e-02 (2.7875e-02)	Acc@1  99.22 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [70][330/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.0645e-02 (2.8621e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [70][340/391]	Time  0.022 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.6901e-02 (2.9080e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [70][350/391]	Time  0.035 ( 0.024)	Data  0.001 ( 0.002)	Loss 9.9411e-03 (2.9026e-02)	Acc@1 100.00 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [70][360/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 6.5498e-03 (2.9021e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 ( 99.99)
Epoch: [70][370/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.8483e-02 (2.9254e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 ( 99.99)
Epoch: [70][380/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.0246e-02 (2.9353e-02)	Acc@1 100.00 ( 99.04)	Acc@5 100.00 ( 99.99)
Epoch: [70][390/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.7197e-02 (2.9281e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 ( 99.99)
## e[70] optimizer.zero_grad (sum) time: 0.10464668273925781
## e[70]       loss.backward (sum) time: 2.1801631450653076
## e[70]      optimizer.step (sum) time: 0.887152910232544
## epoch[70] training(only) time: 9.626354217529297
# Switched to evaluate mode...
Test: [  0/100]	Time  0.173 ( 0.173)	Loss 3.0542e-01 (3.0542e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.016 ( 0.028)	Loss 5.0439e-01 (3.8239e-01)	Acc@1  90.00 ( 90.45)	Acc@5  99.00 ( 99.36)
Test: [ 20/100]	Time  0.008 ( 0.020)	Loss 5.5029e-01 (4.1155e-01)	Acc@1  87.00 ( 89.86)	Acc@5 100.00 ( 99.52)
Test: [ 30/100]	Time  0.016 ( 0.018)	Loss 4.8438e-01 (4.1547e-01)	Acc@1  88.00 ( 90.13)	Acc@5  99.00 ( 99.52)
Test: [ 40/100]	Time  0.014 ( 0.017)	Loss 5.6934e-01 (4.3693e-01)	Acc@1  89.00 ( 89.83)	Acc@5  99.00 ( 99.51)
Test: [ 50/100]	Time  0.015 ( 0.016)	Loss 1.5955e-01 (4.1740e-01)	Acc@1  95.00 ( 90.18)	Acc@5 100.00 ( 99.53)
Test: [ 60/100]	Time  0.013 ( 0.016)	Loss 2.9565e-01 (4.1689e-01)	Acc@1  92.00 ( 90.13)	Acc@5 100.00 ( 99.56)
Test: [ 70/100]	Time  0.010 ( 0.015)	Loss 7.0166e-01 (4.1137e-01)	Acc@1  87.00 ( 90.28)	Acc@5  99.00 ( 99.55)
Test: [ 80/100]	Time  0.019 ( 0.015)	Loss 1.8298e-01 (4.0546e-01)	Acc@1  96.00 ( 90.41)	Acc@5 100.00 ( 99.57)
Test: [ 90/100]	Time  0.015 ( 0.015)	Loss 2.7173e-01 (4.0117e-01)	Acc@1  90.00 ( 90.43)	Acc@5 100.00 ( 99.62)
 * Acc@1 90.400 Acc@5 99.650
### epoch[70] execution time: 11.178390264511108
EPOCH 71
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [71][  0/391]	Time  0.183 ( 0.183)	Data  0.161 ( 0.161)	Loss 3.5919e-02 (3.5919e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [71][ 10/391]	Time  0.024 ( 0.039)	Data  0.001 ( 0.016)	Loss 1.0994e-02 (2.6686e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [71][ 20/391]	Time  0.029 ( 0.032)	Data  0.001 ( 0.009)	Loss 4.2175e-02 (2.9760e-02)	Acc@1  97.66 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [71][ 30/391]	Time  0.027 ( 0.029)	Data  0.003 ( 0.007)	Loss 5.0781e-02 (3.0070e-02)	Acc@1  97.66 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [71][ 40/391]	Time  0.027 ( 0.028)	Data  0.001 ( 0.006)	Loss 1.6998e-02 (2.8789e-02)	Acc@1  99.22 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [71][ 50/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.005)	Loss 1.8173e-02 (2.8314e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [71][ 60/391]	Time  0.026 ( 0.026)	Data  0.002 ( 0.004)	Loss 9.4757e-03 (2.7419e-02)	Acc@1 100.00 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [71][ 70/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 3.6835e-02 (2.7710e-02)	Acc@1  97.66 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [71][ 80/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 5.3223e-02 (2.8628e-02)	Acc@1  97.66 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [71][ 90/391]	Time  0.038 ( 0.025)	Data  0.000 ( 0.004)	Loss 1.4900e-02 (2.8401e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [71][100/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.004)	Loss 1.3344e-02 (2.7456e-02)	Acc@1  99.22 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [71][110/391]	Time  0.026 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.3630e-02 (2.7696e-02)	Acc@1  98.44 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [71][120/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.8656e-02 (2.7190e-02)	Acc@1  97.66 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [71][130/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.1952e-02 (2.6969e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [71][140/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.5247e-02 (2.7341e-02)	Acc@1  97.66 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [71][150/391]	Time  0.028 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.0590e-02 (2.7072e-02)	Acc@1 100.00 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [71][160/391]	Time  0.029 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.2145e-02 (2.7119e-02)	Acc@1  98.44 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [71][170/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 9.6741e-03 (2.8028e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [71][180/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.1342e-02 (2.8678e-02)	Acc@1  98.44 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [71][190/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.4983e-02 (2.8587e-02)	Acc@1  98.44 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [71][200/391]	Time  0.027 ( 0.025)	Data  0.003 ( 0.003)	Loss 6.6833e-03 (2.8626e-02)	Acc@1 100.00 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [71][210/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.5603e-02 (2.9257e-02)	Acc@1  98.44 ( 98.97)	Acc@5  99.22 ( 99.99)
Epoch: [71][220/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.0501e-03 (2.8749e-02)	Acc@1 100.00 ( 99.00)	Acc@5 100.00 ( 99.99)
Epoch: [71][230/391]	Time  0.019 ( 0.024)	Data  0.002 ( 0.003)	Loss 3.5797e-02 (2.8853e-02)	Acc@1  97.66 ( 98.99)	Acc@5 100.00 ( 99.99)
Epoch: [71][240/391]	Time  0.034 ( 0.025)	Data  0.001 ( 0.003)	Loss 7.3814e-03 (2.8445e-02)	Acc@1 100.00 ( 99.00)	Acc@5 100.00 ( 99.99)
Epoch: [71][250/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.0050e-02 (2.8192e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [71][260/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.3346e-02 (2.8034e-02)	Acc@1  99.22 ( 99.02)	Acc@5 100.00 ( 99.99)
Epoch: [71][270/391]	Time  0.035 ( 0.025)	Data  0.004 ( 0.003)	Loss 4.2755e-02 (2.8101e-02)	Acc@1  98.44 ( 99.03)	Acc@5 100.00 ( 99.99)
Epoch: [71][280/391]	Time  0.024 ( 0.025)	Data  0.002 ( 0.003)	Loss 1.3733e-02 (2.8459e-02)	Acc@1 100.00 ( 99.03)	Acc@5 100.00 ( 99.99)
Epoch: [71][290/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.3529e-02 (2.8070e-02)	Acc@1  97.66 ( 99.04)	Acc@5 100.00 ( 99.99)
Epoch: [71][300/391]	Time  0.031 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.7863e-02 (2.8241e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 ( 99.99)
Epoch: [71][310/391]	Time  0.033 ( 0.025)	Data  0.000 ( 0.002)	Loss 2.6138e-02 (2.7902e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 ( 99.99)
Epoch: [71][320/391]	Time  0.040 ( 0.025)	Data  0.006 ( 0.002)	Loss 3.4485e-02 (2.8072e-02)	Acc@1  99.22 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [71][330/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.0370e-02 (2.8511e-02)	Acc@1 100.00 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [71][340/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.8137e-02 (2.8447e-02)	Acc@1  98.44 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [71][350/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.3539e-02 (2.8661e-02)	Acc@1  98.44 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [71][360/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.6169e-02 (2.8628e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 ( 99.99)
Epoch: [71][370/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.6403e-02 (2.8483e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [71][380/391]	Time  0.038 ( 0.024)	Data  0.001 ( 0.002)	Loss 6.2744e-02 (2.8897e-02)	Acc@1  98.44 ( 99.04)	Acc@5 100.00 ( 99.99)
Epoch: [71][390/391]	Time  0.015 ( 0.024)	Data  0.000 ( 0.002)	Loss 3.2768e-03 (2.8821e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 ( 99.99)
## e[71] optimizer.zero_grad (sum) time: 0.10558104515075684
## e[71]       loss.backward (sum) time: 2.190154790878296
## e[71]      optimizer.step (sum) time: 0.8924283981323242
## epoch[71] training(only) time: 9.654975652694702
# Switched to evaluate mode...
Test: [  0/100]	Time  0.214 ( 0.214)	Loss 3.1250e-01 (3.1250e-01)	Acc@1  91.00 ( 91.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.008 ( 0.030)	Loss 5.2002e-01 (3.8090e-01)	Acc@1  90.00 ( 90.45)	Acc@5  99.00 ( 99.45)
Test: [ 20/100]	Time  0.020 ( 0.023)	Loss 5.3516e-01 (4.0776e-01)	Acc@1  87.00 ( 89.90)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.011 ( 0.020)	Loss 4.6948e-01 (4.1400e-01)	Acc@1  89.00 ( 90.13)	Acc@5  99.00 ( 99.55)
Test: [ 40/100]	Time  0.017 ( 0.018)	Loss 5.5225e-01 (4.3476e-01)	Acc@1  89.00 ( 89.80)	Acc@5 100.00 ( 99.56)
Test: [ 50/100]	Time  0.013 ( 0.017)	Loss 1.5564e-01 (4.1689e-01)	Acc@1  95.00 ( 90.16)	Acc@5 100.00 ( 99.55)
Test: [ 60/100]	Time  0.017 ( 0.017)	Loss 2.9224e-01 (4.1733e-01)	Acc@1  93.00 ( 90.11)	Acc@5 100.00 ( 99.57)
Test: [ 70/100]	Time  0.015 ( 0.016)	Loss 7.0312e-01 (4.1182e-01)	Acc@1  88.00 ( 90.28)	Acc@5  99.00 ( 99.56)
Test: [ 80/100]	Time  0.011 ( 0.016)	Loss 1.8127e-01 (4.0599e-01)	Acc@1  94.00 ( 90.37)	Acc@5 100.00 ( 99.58)
Test: [ 90/100]	Time  0.008 ( 0.015)	Loss 2.6929e-01 (4.0163e-01)	Acc@1  91.00 ( 90.40)	Acc@5 100.00 ( 99.63)
 * Acc@1 90.350 Acc@5 99.660
### epoch[71] execution time: 11.298974514007568
EPOCH 72
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [72][  0/391]	Time  0.191 ( 0.191)	Data  0.170 ( 0.170)	Loss 2.4811e-02 (2.4811e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [72][ 10/391]	Time  0.018 ( 0.038)	Data  0.001 ( 0.017)	Loss 3.1586e-02 (2.6259e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [72][ 20/391]	Time  0.018 ( 0.032)	Data  0.001 ( 0.010)	Loss 1.0948e-02 (2.6415e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [72][ 30/391]	Time  0.017 ( 0.029)	Data  0.000 ( 0.007)	Loss 7.8735e-03 (2.4002e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [72][ 40/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.006)	Loss 3.9581e-02 (2.6418e-02)	Acc@1  99.22 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [72][ 50/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.005)	Loss 3.7262e-02 (2.6883e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [72][ 60/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.005)	Loss 1.3321e-02 (2.9202e-02)	Acc@1 100.00 ( 99.08)	Acc@5 100.00 ( 99.99)
Epoch: [72][ 70/391]	Time  0.025 ( 0.026)	Data  0.002 ( 0.004)	Loss 2.1851e-02 (2.7192e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 ( 99.99)
Epoch: [72][ 80/391]	Time  0.032 ( 0.026)	Data  0.004 ( 0.004)	Loss 4.1199e-02 (2.7667e-02)	Acc@1  99.22 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [72][ 90/391]	Time  0.022 ( 0.025)	Data  0.005 ( 0.004)	Loss 1.0437e-02 (2.7913e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [72][100/391]	Time  0.043 ( 0.025)	Data  0.005 ( 0.004)	Loss 7.1526e-03 (2.7986e-02)	Acc@1 100.00 ( 99.10)	Acc@5 100.00 ( 99.99)
Epoch: [72][110/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.004)	Loss 1.8845e-02 (2.7933e-02)	Acc@1  99.22 ( 99.09)	Acc@5 100.00 ( 99.99)
Epoch: [72][120/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 1.3931e-02 (2.6705e-02)	Acc@1  99.22 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [72][130/391]	Time  0.028 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.9913e-02 (2.6716e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [72][140/391]	Time  0.027 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.4689e-02 (2.6881e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [72][150/391]	Time  0.017 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.1555e-02 (2.6523e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 ( 99.99)
Epoch: [72][160/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 1.5869e-02 (2.7166e-02)	Acc@1  99.22 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [72][170/391]	Time  0.026 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.2156e-02 (2.7162e-02)	Acc@1  99.22 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [72][180/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.7720e-02 (2.7179e-02)	Acc@1  99.22 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [72][190/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 1.5434e-02 (2.6975e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [72][200/391]	Time  0.031 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.8513e-02 (2.7190e-02)	Acc@1  99.22 ( 99.13)	Acc@5  99.22 ( 99.99)
Epoch: [72][210/391]	Time  0.018 ( 0.025)	Data  0.002 ( 0.003)	Loss 5.4199e-02 (2.7296e-02)	Acc@1  97.66 ( 99.12)	Acc@5 100.00 ( 99.99)
Epoch: [72][220/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.4518e-03 (2.6911e-02)	Acc@1 100.00 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [72][230/391]	Time  0.024 ( 0.025)	Data  0.001 ( 0.003)	Loss 7.7209e-03 (2.7014e-02)	Acc@1 100.00 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [72][240/391]	Time  0.036 ( 0.025)	Data  0.004 ( 0.003)	Loss 1.1436e-02 (2.6933e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [72][250/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.4107e-02 (2.6982e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [72][260/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.7105e-02 (2.6628e-02)	Acc@1 100.00 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [72][270/391]	Time  0.042 ( 0.025)	Data  0.011 ( 0.003)	Loss 2.4109e-02 (2.6353e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 ( 99.99)
Epoch: [72][280/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 9.0027e-03 (2.6377e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 ( 99.99)
Epoch: [72][290/391]	Time  0.024 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.0368e-02 (2.6039e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 ( 99.99)
Epoch: [72][300/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.5343e-02 (2.5926e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 ( 99.99)
Epoch: [72][310/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.002)	Loss 7.2212e-03 (2.5680e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 ( 99.99)
Epoch: [72][320/391]	Time  0.026 ( 0.025)	Data  0.001 ( 0.002)	Loss 9.7275e-03 (2.5837e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [72][330/391]	Time  0.037 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.1342e-02 (2.5894e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [72][340/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.1840e-02 (2.5930e-02)	Acc@1  98.44 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [72][350/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.4830e-02 (2.5771e-02)	Acc@1  97.66 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [72][360/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 7.1564e-03 (2.5839e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [72][370/391]	Time  0.023 ( 0.024)	Data  0.001 ( 0.002)	Loss 4.8065e-02 (2.6264e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [72][380/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 4.1260e-02 (2.6335e-02)	Acc@1  98.44 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [72][390/391]	Time  0.016 ( 0.024)	Data  0.001 ( 0.002)	Loss 7.4158e-02 (2.6525e-02)	Acc@1  97.50 ( 99.16)	Acc@5 100.00 (100.00)
## e[72] optimizer.zero_grad (sum) time: 0.10570406913757324
## e[72]       loss.backward (sum) time: 2.1563143730163574
## e[72]      optimizer.step (sum) time: 0.8813717365264893
## epoch[72] training(only) time: 9.658724069595337
# Switched to evaluate mode...
Test: [  0/100]	Time  0.165 ( 0.165)	Loss 3.2056e-01 (3.2056e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.015 ( 0.027)	Loss 5.1318e-01 (3.7322e-01)	Acc@1  90.00 ( 91.09)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.019 ( 0.020)	Loss 5.0879e-01 (4.0511e-01)	Acc@1  88.00 ( 90.29)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.008 ( 0.018)	Loss 4.6997e-01 (4.1316e-01)	Acc@1  90.00 ( 90.45)	Acc@5  99.00 ( 99.65)
Test: [ 40/100]	Time  0.016 ( 0.017)	Loss 5.3125e-01 (4.3388e-01)	Acc@1  88.00 ( 89.93)	Acc@5 100.00 ( 99.63)
Test: [ 50/100]	Time  0.016 ( 0.016)	Loss 1.5247e-01 (4.1450e-01)	Acc@1  94.00 ( 90.27)	Acc@5 100.00 ( 99.63)
Test: [ 60/100]	Time  0.014 ( 0.016)	Loss 2.7686e-01 (4.1557e-01)	Acc@1  92.00 ( 90.20)	Acc@5 100.00 ( 99.62)
Test: [ 70/100]	Time  0.012 ( 0.015)	Loss 6.6602e-01 (4.0981e-01)	Acc@1  88.00 ( 90.34)	Acc@5  99.00 ( 99.62)
Test: [ 80/100]	Time  0.017 ( 0.015)	Loss 1.9214e-01 (4.0410e-01)	Acc@1  95.00 ( 90.43)	Acc@5 100.00 ( 99.63)
Test: [ 90/100]	Time  0.013 ( 0.015)	Loss 3.1006e-01 (3.9985e-01)	Acc@1  90.00 ( 90.41)	Acc@5 100.00 ( 99.67)
 * Acc@1 90.370 Acc@5 99.700
### epoch[72] execution time: 11.237894296646118
EPOCH 73
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [73][  0/391]	Time  0.199 ( 0.199)	Data  0.176 ( 0.176)	Loss 2.2278e-02 (2.2278e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [73][ 10/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.018)	Loss 1.9211e-02 (2.9146e-02)	Acc@1  99.22 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [73][ 20/391]	Time  0.036 ( 0.033)	Data  0.003 ( 0.010)	Loss 1.6342e-02 (2.7425e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [73][ 30/391]	Time  0.025 ( 0.030)	Data  0.001 ( 0.007)	Loss 4.6463e-03 (2.5971e-02)	Acc@1 100.00 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [73][ 40/391]	Time  0.018 ( 0.028)	Data  0.001 ( 0.006)	Loss 1.8799e-02 (2.3694e-02)	Acc@1  99.22 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [73][ 50/391]	Time  0.028 ( 0.028)	Data  0.000 ( 0.005)	Loss 5.7861e-02 (2.6215e-02)	Acc@1  96.09 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [73][ 60/391]	Time  0.017 ( 0.027)	Data  0.000 ( 0.005)	Loss 1.3237e-02 (2.7916e-02)	Acc@1 100.00 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [73][ 70/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.6108e-02 (2.8610e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [73][ 80/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.004)	Loss 2.3087e-02 (2.7063e-02)	Acc@1  99.22 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [73][ 90/391]	Time  0.017 ( 0.026)	Data  0.000 ( 0.004)	Loss 3.2257e-02 (2.6632e-02)	Acc@1  99.22 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [73][100/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 9.6436e-03 (2.6046e-02)	Acc@1 100.00 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [73][110/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1133e-02 (2.6404e-02)	Acc@1  98.44 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [73][120/391]	Time  0.017 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.2705e-02 (2.7635e-02)	Acc@1  99.22 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [73][130/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 3.0899e-02 (2.8102e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [73][140/391]	Time  0.035 ( 0.025)	Data  0.000 ( 0.003)	Loss 5.0415e-02 (2.8348e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [73][150/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 7.1472e-02 (2.8668e-02)	Acc@1  97.66 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [73][160/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.0258e-02 (2.8520e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [73][170/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.9180e-02 (2.8689e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [73][180/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.9114e-02 (2.9265e-02)	Acc@1  99.22 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [73][190/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.3802e-02 (2.9216e-02)	Acc@1  97.66 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [73][200/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.6123e-02 (2.9782e-02)	Acc@1  99.22 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [73][210/391]	Time  0.030 ( 0.025)	Data  0.000 ( 0.003)	Loss 6.4148e-02 (3.0152e-02)	Acc@1  98.44 ( 98.98)	Acc@5  99.22 (100.00)
Epoch: [73][220/391]	Time  0.021 ( 0.025)	Data  0.004 ( 0.003)	Loss 1.7838e-02 (2.9728e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [73][230/391]	Time  0.042 ( 0.025)	Data  0.004 ( 0.003)	Loss 2.3499e-02 (2.9411e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [73][240/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.2215e-02 (2.9218e-02)	Acc@1 100.00 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [73][250/391]	Time  0.033 ( 0.025)	Data  0.004 ( 0.003)	Loss 7.4844e-03 (2.8838e-02)	Acc@1 100.00 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [73][260/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.3102e-02 (2.8758e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [73][270/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.2253e-02 (2.9104e-02)	Acc@1 100.00 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [73][280/391]	Time  0.035 ( 0.025)	Data  0.003 ( 0.002)	Loss 1.3809e-02 (2.9011e-02)	Acc@1  99.22 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [73][290/391]	Time  0.043 ( 0.025)	Data  0.000 ( 0.002)	Loss 2.0065e-02 (2.9287e-02)	Acc@1  99.22 ( 99.02)	Acc@5 100.00 ( 99.99)
Epoch: [73][300/391]	Time  0.031 ( 0.025)	Data  0.001 ( 0.002)	Loss 8.6365e-03 (2.9075e-02)	Acc@1 100.00 ( 99.03)	Acc@5 100.00 ( 99.99)
Epoch: [73][310/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.002)	Loss 1.0674e-02 (2.9041e-02)	Acc@1 100.00 ( 99.03)	Acc@5 100.00 ( 99.99)
Epoch: [73][320/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.8397e-02 (2.9351e-02)	Acc@1  98.44 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [73][330/391]	Time  0.029 ( 0.025)	Data  0.000 ( 0.002)	Loss 2.4933e-02 (2.9105e-02)	Acc@1  99.22 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [73][340/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.3708e-03 (2.8994e-02)	Acc@1 100.00 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [73][350/391]	Time  0.033 ( 0.024)	Data  0.003 ( 0.002)	Loss 2.4185e-02 (2.9223e-02)	Acc@1  99.22 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [73][360/391]	Time  0.037 ( 0.025)	Data  0.000 ( 0.002)	Loss 1.3252e-02 (2.9149e-02)	Acc@1 100.00 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [73][370/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.3313e-02 (2.9104e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [73][380/391]	Time  0.017 ( 0.024)	Data  0.000 ( 0.002)	Loss 6.7505e-02 (2.8947e-02)	Acc@1  98.44 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [73][390/391]	Time  0.016 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.3178e-02 (2.8671e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 (100.00)
## e[73] optimizer.zero_grad (sum) time: 0.1056511402130127
## e[73]       loss.backward (sum) time: 2.1432578563690186
## e[73]      optimizer.step (sum) time: 0.8829872608184814
## epoch[73] training(only) time: 9.690135955810547
# Switched to evaluate mode...
Test: [  0/100]	Time  0.164 ( 0.164)	Loss 3.1543e-01 (3.1543e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.008 ( 0.025)	Loss 5.0781e-01 (3.6981e-01)	Acc@1  90.00 ( 91.27)	Acc@5  99.00 ( 99.73)
Test: [ 20/100]	Time  0.008 ( 0.020)	Loss 5.0977e-01 (4.0299e-01)	Acc@1  87.00 ( 90.33)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.019 ( 0.018)	Loss 4.6851e-01 (4.1114e-01)	Acc@1  90.00 ( 90.45)	Acc@5  99.00 ( 99.65)
Test: [ 40/100]	Time  0.022 ( 0.017)	Loss 5.6689e-01 (4.3307e-01)	Acc@1  88.00 ( 90.00)	Acc@5 100.00 ( 99.61)
Test: [ 50/100]	Time  0.020 ( 0.016)	Loss 1.5540e-01 (4.1439e-01)	Acc@1  94.00 ( 90.31)	Acc@5 100.00 ( 99.57)
Test: [ 60/100]	Time  0.015 ( 0.016)	Loss 2.6904e-01 (4.1453e-01)	Acc@1  92.00 ( 90.23)	Acc@5 100.00 ( 99.57)
Test: [ 70/100]	Time  0.021 ( 0.016)	Loss 6.9629e-01 (4.0988e-01)	Acc@1  88.00 ( 90.39)	Acc@5  99.00 ( 99.55)
Test: [ 80/100]	Time  0.016 ( 0.015)	Loss 1.9885e-01 (4.0466e-01)	Acc@1  95.00 ( 90.48)	Acc@5 100.00 ( 99.57)
Test: [ 90/100]	Time  0.013 ( 0.015)	Loss 3.0103e-01 (4.0060e-01)	Acc@1  91.00 ( 90.49)	Acc@5 100.00 ( 99.62)
 * Acc@1 90.420 Acc@5 99.640
### epoch[73] execution time: 11.282213687896729
EPOCH 74
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [74][  0/391]	Time  0.196 ( 0.196)	Data  0.176 ( 0.176)	Loss 1.2048e-01 (1.2048e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [74][ 10/391]	Time  0.018 ( 0.040)	Data  0.002 ( 0.018)	Loss 1.6449e-02 (3.3901e-02)	Acc@1  99.22 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [74][ 20/391]	Time  0.025 ( 0.032)	Data  0.001 ( 0.010)	Loss 1.8753e-02 (3.4552e-02)	Acc@1 100.00 ( 98.92)	Acc@5 100.00 (100.00)
Epoch: [74][ 30/391]	Time  0.018 ( 0.029)	Data  0.001 ( 0.007)	Loss 1.4801e-02 (3.0662e-02)	Acc@1 100.00 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [74][ 40/391]	Time  0.019 ( 0.028)	Data  0.002 ( 0.006)	Loss 7.2365e-03 (2.8824e-02)	Acc@1 100.00 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [74][ 50/391]	Time  0.046 ( 0.027)	Data  0.006 ( 0.005)	Loss 1.1978e-02 (2.9595e-02)	Acc@1 100.00 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [74][ 60/391]	Time  0.019 ( 0.027)	Data  0.001 ( 0.005)	Loss 5.6343e-03 (2.8550e-02)	Acc@1 100.00 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [74][ 70/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 4.2236e-02 (2.9188e-02)	Acc@1  97.66 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [74][ 80/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 3.9948e-02 (2.8677e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [74][ 90/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.004)	Loss 3.3630e-02 (2.8107e-02)	Acc@1  98.44 ( 99.16)	Acc@5 100.00 ( 99.99)
Epoch: [74][100/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.004)	Loss 4.3884e-02 (2.9630e-02)	Acc@1  98.44 ( 99.09)	Acc@5 100.00 ( 99.99)
Epoch: [74][110/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.9856e-02 (2.9758e-02)	Acc@1  98.44 ( 99.09)	Acc@5 100.00 ( 99.99)
Epoch: [74][120/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.0930e-02 (3.0212e-02)	Acc@1  99.22 ( 99.07)	Acc@5 100.00 ( 99.99)
Epoch: [74][130/391]	Time  0.036 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.1459e-02 (3.0318e-02)	Acc@1 100.00 ( 99.08)	Acc@5 100.00 ( 99.99)
Epoch: [74][140/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.2114e-02 (2.9642e-02)	Acc@1  99.22 ( 99.10)	Acc@5 100.00 ( 99.99)
Epoch: [74][150/391]	Time  0.037 ( 0.025)	Data  0.005 ( 0.003)	Loss 6.7322e-02 (2.9706e-02)	Acc@1  97.66 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [74][160/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.2979e-02 (2.9350e-02)	Acc@1  97.66 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [74][170/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.1179e-02 (2.9076e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [74][180/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.6517e-03 (2.9721e-02)	Acc@1 100.00 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [74][190/391]	Time  0.036 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.8870e-02 (2.9115e-02)	Acc@1  98.44 ( 99.08)	Acc@5 100.00 ( 99.99)
Epoch: [74][200/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.0323e-02 (2.8956e-02)	Acc@1 100.00 ( 99.09)	Acc@5 100.00 ( 99.99)
Epoch: [74][210/391]	Time  0.025 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.5754e-02 (2.8774e-02)	Acc@1  96.88 ( 99.09)	Acc@5 100.00 ( 99.99)
Epoch: [74][220/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 7.5607e-03 (2.8913e-02)	Acc@1 100.00 ( 99.09)	Acc@5 100.00 ( 99.99)
Epoch: [74][230/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.1340e-03 (2.8967e-02)	Acc@1 100.00 ( 99.08)	Acc@5 100.00 ( 99.99)
Epoch: [74][240/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.7502e-02 (2.8996e-02)	Acc@1  99.22 ( 99.08)	Acc@5 100.00 ( 99.99)
Epoch: [74][250/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.002)	Loss 7.6714e-03 (2.9182e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 ( 99.99)
Epoch: [74][260/391]	Time  0.038 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.1342e-02 (2.9261e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [74][270/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 6.1401e-02 (2.9066e-02)	Acc@1  98.44 ( 99.07)	Acc@5 100.00 ( 99.99)
Epoch: [74][280/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.1067e-02 (2.9052e-02)	Acc@1  98.44 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [74][290/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.4090e-03 (2.9097e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 ( 99.99)
Epoch: [74][300/391]	Time  0.042 ( 0.024)	Data  0.006 ( 0.002)	Loss 6.9580e-03 (2.8929e-02)	Acc@1 100.00 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [74][310/391]	Time  0.017 ( 0.024)	Data  0.000 ( 0.002)	Loss 7.2365e-03 (2.8736e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 ( 99.99)
Epoch: [74][320/391]	Time  0.041 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.2650e-02 (2.8548e-02)	Acc@1 100.00 ( 99.08)	Acc@5 100.00 ( 99.99)
Epoch: [74][330/391]	Time  0.019 ( 0.024)	Data  0.002 ( 0.002)	Loss 2.7237e-02 (2.8589e-02)	Acc@1  98.44 ( 99.07)	Acc@5 100.00 ( 99.99)
Epoch: [74][340/391]	Time  0.032 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.6133e-02 (2.8487e-02)	Acc@1  98.44 ( 99.08)	Acc@5 100.00 ( 99.99)
Epoch: [74][350/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.7328e-02 (2.8945e-02)	Acc@1  98.44 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [74][360/391]	Time  0.025 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.3356e-02 (2.8978e-02)	Acc@1  98.44 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [74][370/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.4641e-02 (2.8711e-02)	Acc@1  99.22 ( 99.07)	Acc@5 100.00 ( 99.99)
Epoch: [74][380/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.8494e-02 (2.8759e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [74][390/391]	Time  0.015 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.4734e-01 (2.8780e-02)	Acc@1  96.25 ( 99.07)	Acc@5 100.00 ( 99.99)
## e[74] optimizer.zero_grad (sum) time: 0.10553288459777832
## e[74]       loss.backward (sum) time: 2.172257900238037
## e[74]      optimizer.step (sum) time: 0.8893489837646484
## epoch[74] training(only) time: 9.634568691253662
# Switched to evaluate mode...
Test: [  0/100]	Time  0.153 ( 0.153)	Loss 3.1689e-01 (3.1689e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.008 ( 0.025)	Loss 5.1562e-01 (3.7743e-01)	Acc@1  90.00 ( 90.64)	Acc@5  99.00 ( 99.55)
Test: [ 20/100]	Time  0.028 ( 0.020)	Loss 5.2100e-01 (4.0601e-01)	Acc@1  87.00 ( 89.90)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.017 ( 0.018)	Loss 4.6704e-01 (4.1102e-01)	Acc@1  89.00 ( 90.19)	Acc@5  99.00 ( 99.58)
Test: [ 40/100]	Time  0.016 ( 0.016)	Loss 5.4150e-01 (4.3230e-01)	Acc@1  89.00 ( 89.80)	Acc@5 100.00 ( 99.59)
Test: [ 50/100]	Time  0.013 ( 0.016)	Loss 1.5430e-01 (4.1370e-01)	Acc@1  96.00 ( 90.16)	Acc@5 100.00 ( 99.59)
Test: [ 60/100]	Time  0.013 ( 0.015)	Loss 2.8442e-01 (4.1295e-01)	Acc@1  92.00 ( 90.11)	Acc@5 100.00 ( 99.61)
Test: [ 70/100]	Time  0.019 ( 0.015)	Loss 6.9238e-01 (4.0846e-01)	Acc@1  88.00 ( 90.30)	Acc@5  99.00 ( 99.59)
Test: [ 80/100]	Time  0.015 ( 0.015)	Loss 1.8628e-01 (4.0350e-01)	Acc@1  95.00 ( 90.41)	Acc@5 100.00 ( 99.60)
Test: [ 90/100]	Time  0.012 ( 0.015)	Loss 2.7930e-01 (3.9946e-01)	Acc@1  92.00 ( 90.44)	Acc@5 100.00 ( 99.65)
 * Acc@1 90.380 Acc@5 99.680
### epoch[74] execution time: 11.179258584976196
EPOCH 75
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [75][  0/391]	Time  0.193 ( 0.193)	Data  0.172 ( 0.172)	Loss 7.1655e-02 (7.1655e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [75][ 10/391]	Time  0.027 ( 0.039)	Data  0.003 ( 0.018)	Loss 7.5500e-02 (3.0194e-02)	Acc@1  97.66 ( 99.15)	Acc@5 100.00 ( 99.93)
Epoch: [75][ 20/391]	Time  0.018 ( 0.032)	Data  0.001 ( 0.010)	Loss 9.4833e-03 (2.8379e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 ( 99.96)
Epoch: [75][ 30/391]	Time  0.018 ( 0.029)	Data  0.001 ( 0.007)	Loss 2.7374e-02 (2.6943e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 ( 99.97)
Epoch: [75][ 40/391]	Time  0.027 ( 0.028)	Data  0.001 ( 0.006)	Loss 8.2474e-03 (2.6316e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 ( 99.96)
Epoch: [75][ 50/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.005)	Loss 7.3891e-03 (2.5379e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 ( 99.97)
Epoch: [75][ 60/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.005)	Loss 2.4658e-02 (2.6263e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 ( 99.97)
Epoch: [75][ 70/391]	Time  0.018 ( 0.026)	Data  0.002 ( 0.004)	Loss 1.5617e-02 (2.6458e-02)	Acc@1  99.22 ( 99.15)	Acc@5 100.00 ( 99.98)
Epoch: [75][ 80/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 3.9307e-02 (2.7866e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 ( 99.98)
Epoch: [75][ 90/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.004)	Loss 5.1300e-02 (2.8361e-02)	Acc@1  98.44 ( 99.12)	Acc@5 100.00 ( 99.98)
Epoch: [75][100/391]	Time  0.030 ( 0.025)	Data  0.000 ( 0.004)	Loss 2.9282e-02 (2.9962e-02)	Acc@1  98.44 ( 99.08)	Acc@5 100.00 ( 99.98)
Epoch: [75][110/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.004)	Loss 3.3203e-02 (2.9712e-02)	Acc@1  98.44 ( 99.08)	Acc@5 100.00 ( 99.99)
Epoch: [75][120/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 7.1831e-03 (2.8949e-02)	Acc@1 100.00 ( 99.12)	Acc@5 100.00 ( 99.99)
Epoch: [75][130/391]	Time  0.038 ( 0.025)	Data  0.000 ( 0.003)	Loss 3.8971e-02 (2.8194e-02)	Acc@1  97.66 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [75][140/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.4170e-02 (2.8348e-02)	Acc@1  97.66 ( 99.09)	Acc@5 100.00 ( 99.99)
Epoch: [75][150/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.8989e-02 (2.8635e-02)	Acc@1  96.88 ( 99.08)	Acc@5 100.00 ( 99.99)
Epoch: [75][160/391]	Time  0.043 ( 0.025)	Data  0.009 ( 0.003)	Loss 1.4946e-02 (2.8613e-02)	Acc@1 100.00 ( 99.08)	Acc@5 100.00 ( 99.99)
Epoch: [75][170/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.1340e-01 (2.9463e-02)	Acc@1  96.09 ( 99.05)	Acc@5 100.00 ( 99.99)
Epoch: [75][180/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.0416e-02 (2.9598e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 ( 99.98)
Epoch: [75][190/391]	Time  0.017 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.8524e-02 (2.9868e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 ( 99.98)
Epoch: [75][200/391]	Time  0.025 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.8481e-02 (3.0078e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 ( 99.98)
Epoch: [75][210/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.003)	Loss 4.3068e-03 (2.9371e-02)	Acc@1 100.00 ( 99.08)	Acc@5 100.00 ( 99.99)
Epoch: [75][220/391]	Time  0.029 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.3793e-02 (2.9914e-02)	Acc@1  98.44 ( 99.06)	Acc@5 100.00 ( 99.98)
Epoch: [75][230/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.003)	Loss 4.5929e-03 (2.9763e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 ( 99.98)
Epoch: [75][240/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.5579e-02 (2.9325e-02)	Acc@1  99.22 ( 99.07)	Acc@5 100.00 ( 99.98)
Epoch: [75][250/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.003)	Loss 1.1917e-02 (2.9203e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 ( 99.98)
Epoch: [75][260/391]	Time  0.031 ( 0.024)	Data  0.000 ( 0.003)	Loss 6.5460e-03 (2.8997e-02)	Acc@1 100.00 ( 99.08)	Acc@5 100.00 ( 99.99)
Epoch: [75][270/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.003)	Loss 4.9713e-02 (2.8896e-02)	Acc@1  97.66 ( 99.07)	Acc@5 100.00 ( 99.99)
Epoch: [75][280/391]	Time  0.023 ( 0.024)	Data  0.001 ( 0.002)	Loss 6.4453e-02 (2.9048e-02)	Acc@1  96.88 ( 99.06)	Acc@5 100.00 ( 99.98)
Epoch: [75][290/391]	Time  0.031 ( 0.024)	Data  0.001 ( 0.002)	Loss 5.8517e-03 (2.8698e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 ( 99.98)
Epoch: [75][300/391]	Time  0.028 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.1436e-02 (2.8400e-02)	Acc@1 100.00 ( 99.09)	Acc@5 100.00 ( 99.98)
Epoch: [75][310/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.2873e-02 (2.8160e-02)	Acc@1  99.22 ( 99.10)	Acc@5 100.00 ( 99.98)
Epoch: [75][320/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.8005e-02 (2.8094e-02)	Acc@1  99.22 ( 99.10)	Acc@5 100.00 ( 99.99)
Epoch: [75][330/391]	Time  0.026 ( 0.024)	Data  0.002 ( 0.002)	Loss 2.1606e-02 (2.8134e-02)	Acc@1  98.44 ( 99.10)	Acc@5 100.00 ( 99.99)
Epoch: [75][340/391]	Time  0.019 ( 0.024)	Data  0.001 ( 0.002)	Loss 6.5552e-02 (2.8495e-02)	Acc@1  97.66 ( 99.08)	Acc@5 100.00 ( 99.99)
Epoch: [75][350/391]	Time  0.020 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.4496e-02 (2.8493e-02)	Acc@1 100.00 ( 99.09)	Acc@5 100.00 ( 99.98)
Epoch: [75][360/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.2604e-02 (2.8647e-02)	Acc@1  99.22 ( 99.08)	Acc@5 100.00 ( 99.98)
Epoch: [75][370/391]	Time  0.033 ( 0.024)	Data  0.003 ( 0.002)	Loss 2.2232e-02 (2.8713e-02)	Acc@1  98.44 ( 99.09)	Acc@5 100.00 ( 99.98)
Epoch: [75][380/391]	Time  0.017 ( 0.024)	Data  0.000 ( 0.002)	Loss 2.6367e-02 (2.8472e-02)	Acc@1  99.22 ( 99.10)	Acc@5 100.00 ( 99.98)
Epoch: [75][390/391]	Time  0.015 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.7960e-02 (2.8486e-02)	Acc@1 100.00 ( 99.10)	Acc@5 100.00 ( 99.98)
## e[75] optimizer.zero_grad (sum) time: 0.1061105728149414
## e[75]       loss.backward (sum) time: 2.1701364517211914
## e[75]      optimizer.step (sum) time: 0.9034759998321533
## epoch[75] training(only) time: 9.600041151046753
# Switched to evaluate mode...
Test: [  0/100]	Time  0.160 ( 0.160)	Loss 3.2788e-01 (3.2788e-01)	Acc@1  91.00 ( 91.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.017 ( 0.027)	Loss 5.1514e-01 (3.7626e-01)	Acc@1  90.00 ( 90.36)	Acc@5  99.00 ( 99.55)
Test: [ 20/100]	Time  0.009 ( 0.020)	Loss 5.3125e-01 (4.0725e-01)	Acc@1  87.00 ( 89.86)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.012 ( 0.018)	Loss 4.7583e-01 (4.1254e-01)	Acc@1  90.00 ( 90.13)	Acc@5  99.00 ( 99.58)
Test: [ 40/100]	Time  0.011 ( 0.017)	Loss 5.8008e-01 (4.3641e-01)	Acc@1  88.00 ( 89.83)	Acc@5 100.00 ( 99.59)
Test: [ 50/100]	Time  0.032 ( 0.016)	Loss 1.7358e-01 (4.1705e-01)	Acc@1  95.00 ( 90.16)	Acc@5 100.00 ( 99.57)
Test: [ 60/100]	Time  0.012 ( 0.015)	Loss 2.6953e-01 (4.1585e-01)	Acc@1  92.00 ( 90.10)	Acc@5 100.00 ( 99.59)
Test: [ 70/100]	Time  0.016 ( 0.015)	Loss 7.0850e-01 (4.1096e-01)	Acc@1  87.00 ( 90.25)	Acc@5  99.00 ( 99.58)
Test: [ 80/100]	Time  0.011 ( 0.015)	Loss 1.8286e-01 (4.0621e-01)	Acc@1  95.00 ( 90.35)	Acc@5 100.00 ( 99.59)
Test: [ 90/100]	Time  0.008 ( 0.015)	Loss 2.7197e-01 (4.0248e-01)	Acc@1  92.00 ( 90.35)	Acc@5 100.00 ( 99.64)
 * Acc@1 90.320 Acc@5 99.670
### epoch[75] execution time: 11.160334348678589
EPOCH 76
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [76][  0/391]	Time  0.199 ( 0.199)	Data  0.178 ( 0.178)	Loss 1.3159e-01 (1.3159e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [76][ 10/391]	Time  0.019 ( 0.040)	Data  0.001 ( 0.018)	Loss 1.3969e-02 (4.0543e-02)	Acc@1 100.00 ( 98.65)	Acc@5 100.00 ( 99.93)
Epoch: [76][ 20/391]	Time  0.018 ( 0.031)	Data  0.001 ( 0.010)	Loss 3.6438e-02 (3.6961e-02)	Acc@1  98.44 ( 98.77)	Acc@5 100.00 ( 99.96)
Epoch: [76][ 30/391]	Time  0.022 ( 0.029)	Data  0.002 ( 0.008)	Loss 3.3173e-02 (3.3056e-02)	Acc@1  99.22 ( 98.92)	Acc@5 100.00 ( 99.97)
Epoch: [76][ 40/391]	Time  0.018 ( 0.028)	Data  0.001 ( 0.006)	Loss 3.4607e-02 (3.3654e-02)	Acc@1  98.44 ( 98.88)	Acc@5 100.00 ( 99.98)
Epoch: [76][ 50/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.005)	Loss 2.0508e-02 (2.9865e-02)	Acc@1  99.22 ( 99.02)	Acc@5 100.00 ( 99.98)
Epoch: [76][ 60/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.005)	Loss 1.5594e-02 (3.0575e-02)	Acc@1 100.00 ( 98.99)	Acc@5 100.00 ( 99.99)
Epoch: [76][ 70/391]	Time  0.018 ( 0.026)	Data  0.002 ( 0.004)	Loss 1.6418e-02 (2.9236e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 ( 99.99)
Epoch: [76][ 80/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.3557e-02 (2.9139e-02)	Acc@1  99.22 ( 99.02)	Acc@5 100.00 ( 99.99)
Epoch: [76][ 90/391]	Time  0.038 ( 0.026)	Data  0.000 ( 0.004)	Loss 2.5055e-02 (2.9652e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 ( 99.99)
Epoch: [76][100/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.7654e-02 (2.9978e-02)	Acc@1  99.22 ( 98.99)	Acc@5 100.00 ( 99.99)
Epoch: [76][110/391]	Time  0.035 ( 0.026)	Data  0.000 ( 0.004)	Loss 1.2360e-02 (2.9477e-02)	Acc@1 100.00 ( 99.00)	Acc@5 100.00 ( 99.99)
Epoch: [76][120/391]	Time  0.031 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.5267e-02 (2.9661e-02)	Acc@1  96.09 ( 98.99)	Acc@5 100.00 ( 99.99)
Epoch: [76][130/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 5.9814e-03 (2.9292e-02)	Acc@1 100.00 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [76][140/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.2772e-02 (2.9322e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 ( 99.99)
Epoch: [76][150/391]	Time  0.032 ( 0.025)	Data  0.003 ( 0.003)	Loss 3.3569e-02 (2.8673e-02)	Acc@1  98.44 ( 99.02)	Acc@5 100.00 ( 99.99)
Epoch: [76][160/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.1149e-02 (2.7998e-02)	Acc@1  98.44 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [76][170/391]	Time  0.023 ( 0.025)	Data  0.003 ( 0.003)	Loss 3.7109e-02 (2.8180e-02)	Acc@1  98.44 ( 99.04)	Acc@5 100.00 ( 99.99)
Epoch: [76][180/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.8116e-02 (2.8192e-02)	Acc@1  98.44 ( 99.03)	Acc@5 100.00 ( 99.99)
Epoch: [76][190/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 4.9316e-02 (2.8598e-02)	Acc@1  98.44 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [76][200/391]	Time  0.038 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.1372e-02 (2.8734e-02)	Acc@1  98.44 ( 99.03)	Acc@5 100.00 ( 99.99)
Epoch: [76][210/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.8188e-02 (2.8633e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 ( 99.99)
Epoch: [76][220/391]	Time  0.033 ( 0.025)	Data  0.002 ( 0.003)	Loss 7.3128e-03 (2.8397e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 ( 99.99)
Epoch: [76][230/391]	Time  0.024 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.0701e-02 (2.8465e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 ( 99.99)
Epoch: [76][240/391]	Time  0.025 ( 0.025)	Data  0.002 ( 0.003)	Loss 6.6956e-02 (2.8501e-02)	Acc@1  97.66 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [76][250/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.2725e-02 (2.8635e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 ( 99.99)
Epoch: [76][260/391]	Time  0.042 ( 0.025)	Data  0.004 ( 0.003)	Loss 1.8250e-02 (2.8456e-02)	Acc@1 100.00 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [76][270/391]	Time  0.025 ( 0.025)	Data  0.001 ( 0.002)	Loss 7.9651e-03 (2.8098e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 ( 99.99)
Epoch: [76][280/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.6896e-02 (2.8003e-02)	Acc@1  98.44 ( 99.07)	Acc@5 100.00 ( 99.99)
Epoch: [76][290/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.6077e-02 (2.8095e-02)	Acc@1  99.22 ( 99.07)	Acc@5 100.00 ( 99.99)
Epoch: [76][300/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.4551e-02 (2.8168e-02)	Acc@1  98.44 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [76][310/391]	Time  0.018 ( 0.025)	Data  0.002 ( 0.002)	Loss 2.5589e-02 (2.8393e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [76][320/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.0277e-02 (2.8568e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [76][330/391]	Time  0.034 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.8564e-02 (2.8721e-02)	Acc@1  98.44 ( 99.05)	Acc@5 100.00 ( 99.99)
Epoch: [76][340/391]	Time  0.025 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.4241e-02 (2.8637e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 ( 99.99)
Epoch: [76][350/391]	Time  0.031 ( 0.025)	Data  0.003 ( 0.002)	Loss 4.3671e-02 (2.8798e-02)	Acc@1  98.44 ( 99.04)	Acc@5 100.00 ( 99.99)
Epoch: [76][360/391]	Time  0.027 ( 0.025)	Data  0.005 ( 0.002)	Loss 2.4109e-02 (2.8645e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 ( 99.99)
Epoch: [76][370/391]	Time  0.031 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.5715e-02 (2.8737e-02)	Acc@1  98.44 ( 99.04)	Acc@5 100.00 ( 99.99)
Epoch: [76][380/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.6190e-02 (2.8516e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 ( 99.99)
Epoch: [76][390/391]	Time  0.016 ( 0.024)	Data  0.001 ( 0.002)	Loss 8.1787e-02 (2.8705e-02)	Acc@1  97.50 ( 99.04)	Acc@5 100.00 ( 99.99)
## e[76] optimizer.zero_grad (sum) time: 0.10542964935302734
## e[76]       loss.backward (sum) time: 2.099928140640259
## e[76]      optimizer.step (sum) time: 0.8878002166748047
## epoch[76] training(only) time: 9.698925971984863
# Switched to evaluate mode...
Test: [  0/100]	Time  0.162 ( 0.162)	Loss 3.1348e-01 (3.1348e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.016 ( 0.027)	Loss 4.9780e-01 (3.8108e-01)	Acc@1  90.00 ( 90.55)	Acc@5  99.00 ( 99.45)
Test: [ 20/100]	Time  0.011 ( 0.020)	Loss 5.4199e-01 (4.0887e-01)	Acc@1  87.00 ( 90.10)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.008 ( 0.018)	Loss 4.7266e-01 (4.1498e-01)	Acc@1  88.00 ( 90.26)	Acc@5  99.00 ( 99.58)
Test: [ 40/100]	Time  0.020 ( 0.017)	Loss 5.6250e-01 (4.3627e-01)	Acc@1  89.00 ( 89.85)	Acc@5  99.00 ( 99.56)
Test: [ 50/100]	Time  0.008 ( 0.016)	Loss 1.5356e-01 (4.1653e-01)	Acc@1  95.00 ( 90.18)	Acc@5 100.00 ( 99.53)
Test: [ 60/100]	Time  0.008 ( 0.016)	Loss 2.7905e-01 (4.1660e-01)	Acc@1  93.00 ( 90.10)	Acc@5 100.00 ( 99.56)
Test: [ 70/100]	Time  0.020 ( 0.015)	Loss 6.9580e-01 (4.1144e-01)	Acc@1  89.00 ( 90.28)	Acc@5  99.00 ( 99.55)
Test: [ 80/100]	Time  0.014 ( 0.015)	Loss 1.8359e-01 (4.0524e-01)	Acc@1  96.00 ( 90.42)	Acc@5 100.00 ( 99.57)
Test: [ 90/100]	Time  0.018 ( 0.015)	Loss 2.5415e-01 (4.0119e-01)	Acc@1  93.00 ( 90.47)	Acc@5 100.00 ( 99.62)
 * Acc@1 90.420 Acc@5 99.650
### epoch[76] execution time: 11.268923997879028
EPOCH 77
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [77][  0/391]	Time  0.202 ( 0.202)	Data  0.177 ( 0.177)	Loss 5.7487e-03 (5.7487e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [77][ 10/391]	Time  0.029 ( 0.041)	Data  0.001 ( 0.018)	Loss 7.4219e-02 (3.1383e-02)	Acc@1  98.44 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [77][ 20/391]	Time  0.018 ( 0.031)	Data  0.002 ( 0.010)	Loss 2.1561e-02 (3.1996e-02)	Acc@1  98.44 ( 98.96)	Acc@5 100.00 ( 99.96)
Epoch: [77][ 30/391]	Time  0.035 ( 0.030)	Data  0.002 ( 0.008)	Loss 8.4045e-02 (3.3972e-02)	Acc@1  96.88 ( 98.79)	Acc@5 100.00 ( 99.97)
Epoch: [77][ 40/391]	Time  0.026 ( 0.028)	Data  0.001 ( 0.006)	Loss 1.2550e-02 (2.9884e-02)	Acc@1  99.22 ( 98.97)	Acc@5 100.00 ( 99.98)
Epoch: [77][ 50/391]	Time  0.031 ( 0.028)	Data  0.001 ( 0.005)	Loss 5.0888e-03 (3.0068e-02)	Acc@1 100.00 ( 98.99)	Acc@5 100.00 ( 99.98)
Epoch: [77][ 60/391]	Time  0.034 ( 0.027)	Data  0.002 ( 0.005)	Loss 2.4780e-02 (2.8156e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 ( 99.99)
Epoch: [77][ 70/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.1687e-03 (2.6956e-02)	Acc@1 100.00 ( 99.09)	Acc@5 100.00 ( 99.99)
Epoch: [77][ 80/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.0231e-02 (2.6458e-02)	Acc@1  99.22 ( 99.11)	Acc@5 100.00 ( 99.99)
Epoch: [77][ 90/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.7517e-02 (2.7452e-02)	Acc@1  99.22 ( 99.09)	Acc@5 100.00 ( 99.99)
Epoch: [77][100/391]	Time  0.036 ( 0.026)	Data  0.003 ( 0.003)	Loss 4.8340e-02 (2.7072e-02)	Acc@1  98.44 ( 99.10)	Acc@5 100.00 ( 99.99)
Epoch: [77][110/391]	Time  0.028 ( 0.025)	Data  0.002 ( 0.003)	Loss 1.3344e-02 (2.7440e-02)	Acc@1 100.00 ( 99.09)	Acc@5 100.00 ( 99.99)
Epoch: [77][120/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 1.4893e-02 (2.8170e-02)	Acc@1 100.00 ( 99.10)	Acc@5 100.00 ( 99.99)
Epoch: [77][130/391]	Time  0.024 ( 0.025)	Data  0.002 ( 0.003)	Loss 1.7609e-02 (2.8597e-02)	Acc@1  99.22 ( 99.08)	Acc@5 100.00 ( 99.99)
Epoch: [77][140/391]	Time  0.024 ( 0.025)	Data  0.002 ( 0.003)	Loss 1.0323e-02 (2.8466e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 ( 99.99)
Epoch: [77][150/391]	Time  0.017 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.8356e-02 (2.8158e-02)	Acc@1  99.22 ( 99.08)	Acc@5 100.00 ( 99.99)
Epoch: [77][160/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.4038e-02 (2.8077e-02)	Acc@1 100.00 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [77][170/391]	Time  0.043 ( 0.025)	Data  0.005 ( 0.003)	Loss 4.1107e-02 (2.7745e-02)	Acc@1  99.22 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [77][180/391]	Time  0.027 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.1375e-02 (2.7764e-02)	Acc@1 100.00 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [77][190/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.6213e-02 (2.7686e-02)	Acc@1  98.44 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [77][200/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.6589e-02 (2.8359e-02)	Acc@1  98.44 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [77][210/391]	Time  0.024 ( 0.025)	Data  0.002 ( 0.003)	Loss 1.8631e-02 (2.8121e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [77][220/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 2.0798e-02 (2.7842e-02)	Acc@1  99.22 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [77][230/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.8208e-02 (2.7571e-02)	Acc@1  98.44 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [77][240/391]	Time  0.025 ( 0.025)	Data  0.002 ( 0.003)	Loss 2.5787e-03 (2.7856e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [77][250/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.9907e-03 (2.7793e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [77][260/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.7523e-03 (2.8007e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [77][270/391]	Time  0.026 ( 0.025)	Data  0.003 ( 0.002)	Loss 1.9409e-02 (2.8162e-02)	Acc@1  99.22 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [77][280/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.0172e-02 (2.7810e-02)	Acc@1  99.22 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [77][290/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.002)	Loss 6.1432e-02 (2.7963e-02)	Acc@1  96.09 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [77][300/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 5.4131e-03 (2.7969e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [77][310/391]	Time  0.035 ( 0.024)	Data  0.004 ( 0.002)	Loss 1.2512e-02 (2.8259e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [77][320/391]	Time  0.018 ( 0.024)	Data  0.002 ( 0.002)	Loss 1.0674e-02 (2.8079e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [77][330/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.1143e-02 (2.8033e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [77][340/391]	Time  0.036 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.2614e-02 (2.8081e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [77][350/391]	Time  0.029 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.3651e-02 (2.7987e-02)	Acc@1  98.44 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [77][360/391]	Time  0.036 ( 0.024)	Data  0.003 ( 0.002)	Loss 5.3558e-02 (2.8126e-02)	Acc@1  98.44 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [77][370/391]	Time  0.018 ( 0.024)	Data  0.002 ( 0.002)	Loss 2.6367e-02 (2.8289e-02)	Acc@1  98.44 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [77][380/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.7822e-02 (2.8140e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [77][390/391]	Time  0.013 ( 0.024)	Data  0.001 ( 0.002)	Loss 5.9357e-02 (2.7967e-02)	Acc@1  96.25 ( 99.06)	Acc@5 100.00 (100.00)
## e[77] optimizer.zero_grad (sum) time: 0.10672569274902344
## e[77]       loss.backward (sum) time: 2.16790509223938
## e[77]      optimizer.step (sum) time: 0.8997170925140381
## epoch[77] training(only) time: 9.615236759185791
# Switched to evaluate mode...
Test: [  0/100]	Time  0.160 ( 0.160)	Loss 3.2617e-01 (3.2617e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.015 ( 0.026)	Loss 5.1270e-01 (3.7269e-01)	Acc@1  90.00 ( 90.91)	Acc@5  99.00 ( 99.55)
Test: [ 20/100]	Time  0.023 ( 0.020)	Loss 5.1221e-01 (4.0615e-01)	Acc@1  88.00 ( 90.19)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.008 ( 0.018)	Loss 4.8291e-01 (4.1453e-01)	Acc@1  89.00 ( 90.32)	Acc@5  99.00 ( 99.61)
Test: [ 40/100]	Time  0.011 ( 0.017)	Loss 5.5273e-01 (4.3672e-01)	Acc@1  87.00 ( 89.85)	Acc@5 100.00 ( 99.61)
Test: [ 50/100]	Time  0.017 ( 0.016)	Loss 1.6467e-01 (4.1798e-01)	Acc@1  94.00 ( 90.18)	Acc@5 100.00 ( 99.59)
Test: [ 60/100]	Time  0.013 ( 0.015)	Loss 2.7319e-01 (4.1839e-01)	Acc@1  93.00 ( 90.13)	Acc@5 100.00 ( 99.59)
Test: [ 70/100]	Time  0.016 ( 0.015)	Loss 6.9482e-01 (4.1275e-01)	Acc@1  87.00 ( 90.27)	Acc@5  99.00 ( 99.59)
Test: [ 80/100]	Time  0.014 ( 0.015)	Loss 1.8689e-01 (4.0717e-01)	Acc@1  95.00 ( 90.36)	Acc@5 100.00 ( 99.60)
Test: [ 90/100]	Time  0.011 ( 0.015)	Loss 3.0640e-01 (4.0333e-01)	Acc@1  90.00 ( 90.31)	Acc@5 100.00 ( 99.65)
 * Acc@1 90.300 Acc@5 99.680
### epoch[77] execution time: 11.171910762786865
EPOCH 78
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [78][  0/391]	Time  0.202 ( 0.202)	Data  0.178 ( 0.178)	Loss 5.0446e-02 (5.0446e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [78][ 10/391]	Time  0.028 ( 0.041)	Data  0.001 ( 0.018)	Loss 2.3865e-02 (4.8057e-02)	Acc@1  99.22 ( 98.51)	Acc@5 100.00 (100.00)
Epoch: [78][ 20/391]	Time  0.018 ( 0.032)	Data  0.001 ( 0.010)	Loss 4.7516e-02 (4.0272e-02)	Acc@1  97.66 ( 98.70)	Acc@5 100.00 (100.00)
Epoch: [78][ 30/391]	Time  0.018 ( 0.029)	Data  0.001 ( 0.007)	Loss 8.7204e-03 (3.4249e-02)	Acc@1 100.00 ( 98.87)	Acc@5 100.00 (100.00)
Epoch: [78][ 40/391]	Time  0.018 ( 0.028)	Data  0.001 ( 0.006)	Loss 5.5237e-03 (3.1188e-02)	Acc@1 100.00 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [78][ 50/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.005)	Loss 2.7222e-02 (2.8692e-02)	Acc@1  98.44 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [78][ 60/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.005)	Loss 2.9297e-02 (3.1469e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [78][ 70/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 2.5742e-02 (3.1151e-02)	Acc@1  98.44 ( 99.00)	Acc@5 100.00 ( 99.99)
Epoch: [78][ 80/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 6.3705e-03 (3.0995e-02)	Acc@1 100.00 ( 98.98)	Acc@5 100.00 ( 99.99)
Epoch: [78][ 90/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.6479e-02 (3.0133e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 ( 99.99)
Epoch: [78][100/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.004)	Loss 4.5593e-02 (3.0582e-02)	Acc@1  98.44 ( 98.99)	Acc@5 100.00 ( 99.99)
Epoch: [78][110/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 7.6721e-02 (3.1240e-02)	Acc@1  98.44 ( 98.99)	Acc@5 100.00 ( 99.99)
Epoch: [78][120/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.7602e-03 (3.0453e-02)	Acc@1 100.00 ( 98.99)	Acc@5 100.00 ( 99.99)
Epoch: [78][130/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.0615e-02 (2.9699e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 ( 99.99)
Epoch: [78][140/391]	Time  0.025 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.2511e-02 (3.0406e-02)	Acc@1  97.66 ( 99.02)	Acc@5 100.00 ( 99.99)
Epoch: [78][150/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.5685e-02 (3.0158e-02)	Acc@1  97.66 ( 99.04)	Acc@5 100.00 ( 99.99)
Epoch: [78][160/391]	Time  0.042 ( 0.025)	Data  0.005 ( 0.003)	Loss 3.6926e-02 (2.9674e-02)	Acc@1  97.66 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [78][170/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.5997e-03 (2.9896e-02)	Acc@1 100.00 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [78][180/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 1.5915e-02 (2.9913e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [78][190/391]	Time  0.034 ( 0.025)	Data  0.001 ( 0.003)	Loss 9.5520e-03 (3.0625e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 ( 99.99)
Epoch: [78][200/391]	Time  0.037 ( 0.025)	Data  0.002 ( 0.003)	Loss 1.7715e-02 (3.0985e-02)	Acc@1 100.00 ( 99.03)	Acc@5 100.00 ( 99.98)
Epoch: [78][210/391]	Time  0.019 ( 0.025)	Data  0.000 ( 0.003)	Loss 5.3635e-03 (3.0098e-02)	Acc@1 100.00 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [78][220/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.5686e-02 (2.9828e-02)	Acc@1  99.22 ( 99.07)	Acc@5 100.00 ( 99.99)
Epoch: [78][230/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 7.1106e-02 (3.0066e-02)	Acc@1  97.66 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [78][240/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.003)	Loss 6.1432e-02 (2.9997e-02)	Acc@1  97.66 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [78][250/391]	Time  0.017 ( 0.024)	Data  0.000 ( 0.003)	Loss 3.5461e-02 (2.9894e-02)	Acc@1  99.22 ( 99.07)	Acc@5 100.00 ( 99.99)
Epoch: [78][260/391]	Time  0.030 ( 0.025)	Data  0.000 ( 0.003)	Loss 5.1636e-02 (2.9812e-02)	Acc@1  99.22 ( 99.07)	Acc@5 100.00 ( 99.99)
Epoch: [78][270/391]	Time  0.027 ( 0.024)	Data  0.000 ( 0.003)	Loss 3.6224e-02 (2.9913e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [78][280/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.1525e-02 (2.9998e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 ( 99.99)
Epoch: [78][290/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.3665e-03 (2.9667e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 ( 99.99)
Epoch: [78][300/391]	Time  0.033 ( 0.024)	Data  0.000 ( 0.003)	Loss 1.0124e-02 (2.9397e-02)	Acc@1 100.00 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [78][310/391]	Time  0.017 ( 0.024)	Data  0.000 ( 0.002)	Loss 2.4994e-02 (2.9377e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [78][320/391]	Time  0.029 ( 0.024)	Data  0.000 ( 0.002)	Loss 1.6129e-02 (2.9371e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [78][330/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.5482e-02 (2.9215e-02)	Acc@1 100.00 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [78][340/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 9.0637e-03 (2.9114e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [78][350/391]	Time  0.031 ( 0.024)	Data  0.003 ( 0.002)	Loss 3.0167e-02 (2.9325e-02)	Acc@1  98.44 ( 99.05)	Acc@5 100.00 ( 99.99)
Epoch: [78][360/391]	Time  0.022 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.8055e-02 (2.8994e-02)	Acc@1  98.44 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [78][370/391]	Time  0.024 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.7517e-02 (2.8772e-02)	Acc@1  99.22 ( 99.07)	Acc@5 100.00 ( 99.99)
Epoch: [78][380/391]	Time  0.031 ( 0.024)	Data  0.001 ( 0.002)	Loss 4.1687e-02 (2.8526e-02)	Acc@1  98.44 ( 99.08)	Acc@5 100.00 ( 99.99)
Epoch: [78][390/391]	Time  0.015 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.0300e-02 (2.8504e-02)	Acc@1 100.00 ( 99.08)	Acc@5 100.00 ( 99.99)
## e[78] optimizer.zero_grad (sum) time: 0.10558962821960449
## e[78]       loss.backward (sum) time: 2.158167839050293
## e[78]      optimizer.step (sum) time: 0.8836393356323242
## epoch[78] training(only) time: 9.657856702804565
# Switched to evaluate mode...
Test: [  0/100]	Time  0.162 ( 0.162)	Loss 3.0688e-01 (3.0688e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.017 ( 0.027)	Loss 5.0488e-01 (3.7803e-01)	Acc@1  90.00 ( 90.64)	Acc@5  99.00 ( 99.36)
Test: [ 20/100]	Time  0.020 ( 0.021)	Loss 5.3320e-01 (4.0698e-01)	Acc@1  88.00 ( 90.00)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.010 ( 0.018)	Loss 4.8071e-01 (4.1447e-01)	Acc@1  89.00 ( 90.23)	Acc@5  99.00 ( 99.55)
Test: [ 40/100]	Time  0.011 ( 0.017)	Loss 5.6738e-01 (4.3640e-01)	Acc@1  89.00 ( 89.83)	Acc@5 100.00 ( 99.56)
Test: [ 50/100]	Time  0.011 ( 0.016)	Loss 1.5930e-01 (4.1736e-01)	Acc@1  95.00 ( 90.12)	Acc@5 100.00 ( 99.53)
Test: [ 60/100]	Time  0.016 ( 0.016)	Loss 2.8394e-01 (4.1725e-01)	Acc@1  93.00 ( 90.05)	Acc@5 100.00 ( 99.56)
Test: [ 70/100]	Time  0.009 ( 0.015)	Loss 6.9336e-01 (4.1209e-01)	Acc@1  88.00 ( 90.25)	Acc@5  99.00 ( 99.55)
Test: [ 80/100]	Time  0.010 ( 0.015)	Loss 1.8250e-01 (4.0584e-01)	Acc@1  96.00 ( 90.36)	Acc@5 100.00 ( 99.57)
Test: [ 90/100]	Time  0.009 ( 0.015)	Loss 2.6562e-01 (4.0128e-01)	Acc@1  92.00 ( 90.36)	Acc@5 100.00 ( 99.62)
 * Acc@1 90.380 Acc@5 99.640
### epoch[78] execution time: 11.228461265563965
EPOCH 79
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [79][  0/391]	Time  0.190 ( 0.190)	Data  0.172 ( 0.172)	Loss 1.2184e-02 (1.2184e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [79][ 10/391]	Time  0.026 ( 0.040)	Data  0.001 ( 0.017)	Loss 2.1225e-02 (2.3755e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [79][ 20/391]	Time  0.028 ( 0.032)	Data  0.001 ( 0.010)	Loss 2.7878e-02 (2.4055e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [79][ 30/391]	Time  0.018 ( 0.029)	Data  0.001 ( 0.007)	Loss 5.1270e-02 (2.4019e-02)	Acc@1  97.66 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [79][ 40/391]	Time  0.028 ( 0.028)	Data  0.001 ( 0.006)	Loss 1.0757e-02 (2.5575e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [79][ 50/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.005)	Loss 1.5671e-02 (2.7377e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [79][ 60/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.005)	Loss 4.8767e-02 (2.7164e-02)	Acc@1  98.44 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [79][ 70/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.004)	Loss 5.4512e-03 (2.6368e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [79][ 80/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.5152e-02 (2.5533e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [79][ 90/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 3.5217e-02 (2.5582e-02)	Acc@1  97.66 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [79][100/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5373e-02 (2.5868e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [79][110/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.8279e-02 (2.5145e-02)	Acc@1  97.66 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [79][120/391]	Time  0.033 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.5190e-02 (2.5214e-02)	Acc@1 100.00 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [79][130/391]	Time  0.024 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.0655e-02 (2.5387e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [79][140/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.9028e-02 (2.6081e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 ( 99.99)
Epoch: [79][150/391]	Time  0.018 ( 0.025)	Data  0.002 ( 0.003)	Loss 1.2825e-02 (2.5419e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 ( 99.99)
Epoch: [79][160/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.4236e-02 (2.5885e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [79][170/391]	Time  0.036 ( 0.025)	Data  0.006 ( 0.003)	Loss 7.6141e-03 (2.5310e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [79][180/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.2175e-02 (2.6522e-02)	Acc@1  98.44 ( 99.20)	Acc@5 100.00 ( 99.99)
Epoch: [79][190/391]	Time  0.049 ( 0.025)	Data  0.009 ( 0.003)	Loss 1.0719e-02 (2.6237e-02)	Acc@1 100.00 ( 99.21)	Acc@5 100.00 ( 99.99)
Epoch: [79][200/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.1016e-02 (2.6457e-02)	Acc@1  98.44 ( 99.20)	Acc@5 100.00 ( 99.99)
Epoch: [79][210/391]	Time  0.034 ( 0.025)	Data  0.000 ( 0.003)	Loss 1.3649e-02 (2.6344e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 ( 99.99)
Epoch: [79][220/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.5919e-02 (2.7119e-02)	Acc@1  98.44 ( 99.17)	Acc@5 100.00 ( 99.99)
Epoch: [79][230/391]	Time  0.025 ( 0.025)	Data  0.001 ( 0.002)	Loss 5.0781e-02 (2.7160e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 ( 99.99)
Epoch: [79][240/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.002)	Loss 7.6637e-03 (2.7169e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 ( 99.99)
Epoch: [79][250/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.5929e-02 (2.7057e-02)	Acc@1  97.66 ( 99.18)	Acc@5 100.00 ( 99.99)
Epoch: [79][260/391]	Time  0.025 ( 0.025)	Data  0.001 ( 0.002)	Loss 7.6790e-03 (2.6761e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 ( 99.99)
Epoch: [79][270/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.002)	Loss 5.9967e-02 (2.6626e-02)	Acc@1  98.44 ( 99.17)	Acc@5 100.00 ( 99.99)
Epoch: [79][280/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.7122e-03 (2.6810e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 ( 99.99)
Epoch: [79][290/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.002)	Loss 2.8412e-02 (2.6789e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 ( 99.99)
Epoch: [79][300/391]	Time  0.042 ( 0.025)	Data  0.003 ( 0.002)	Loss 2.8564e-02 (2.6630e-02)	Acc@1  98.44 ( 99.17)	Acc@5 100.00 ( 99.99)
Epoch: [79][310/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.4076e-02 (2.6318e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 ( 99.99)
Epoch: [79][320/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 9.6359e-03 (2.6296e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [79][330/391]	Time  0.029 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.2352e-02 (2.6233e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [79][340/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.6844e-02 (2.6314e-02)	Acc@1  98.44 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [79][350/391]	Time  0.029 ( 0.025)	Data  0.004 ( 0.002)	Loss 2.7023e-02 (2.6352e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [79][360/391]	Time  0.029 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.9587e-02 (2.6451e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [79][370/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.8250e-02 (2.6562e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [79][380/391]	Time  0.020 ( 0.025)	Data  0.002 ( 0.002)	Loss 1.0567e-02 (2.6380e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [79][390/391]	Time  0.015 ( 0.024)	Data  0.001 ( 0.002)	Loss 5.9967e-03 (2.6244e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 (100.00)
## e[79] optimizer.zero_grad (sum) time: 0.10381913185119629
## e[79]       loss.backward (sum) time: 2.116262912750244
## e[79]      optimizer.step (sum) time: 0.8747415542602539
## epoch[79] training(only) time: 9.700985670089722
# Switched to evaluate mode...
Test: [  0/100]	Time  0.162 ( 0.162)	Loss 3.1201e-01 (3.1201e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.019 ( 0.027)	Loss 4.8853e-01 (3.7460e-01)	Acc@1  90.00 ( 90.73)	Acc@5  99.00 ( 99.45)
Test: [ 20/100]	Time  0.016 ( 0.020)	Loss 5.4736e-01 (4.0797e-01)	Acc@1  87.00 ( 90.10)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.025 ( 0.018)	Loss 4.7534e-01 (4.1335e-01)	Acc@1  89.00 ( 90.29)	Acc@5  99.00 ( 99.58)
Test: [ 40/100]	Time  0.014 ( 0.017)	Loss 5.4883e-01 (4.3602e-01)	Acc@1  89.00 ( 89.98)	Acc@5  99.00 ( 99.56)
Test: [ 50/100]	Time  0.016 ( 0.016)	Loss 1.5637e-01 (4.1631e-01)	Acc@1  96.00 ( 90.37)	Acc@5 100.00 ( 99.57)
Test: [ 60/100]	Time  0.018 ( 0.016)	Loss 2.9980e-01 (4.1628e-01)	Acc@1  92.00 ( 90.26)	Acc@5 100.00 ( 99.59)
Test: [ 70/100]	Time  0.021 ( 0.015)	Loss 6.7627e-01 (4.1060e-01)	Acc@1  87.00 ( 90.39)	Acc@5  99.00 ( 99.58)
Test: [ 80/100]	Time  0.008 ( 0.015)	Loss 1.8250e-01 (4.0531e-01)	Acc@1  96.00 ( 90.49)	Acc@5 100.00 ( 99.59)
Test: [ 90/100]	Time  0.016 ( 0.015)	Loss 2.7881e-01 (4.0160e-01)	Acc@1  90.00 ( 90.48)	Acc@5 100.00 ( 99.64)
 * Acc@1 90.470 Acc@5 99.660
### epoch[79] execution time: 11.310562133789062
EPOCH 80
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [80][  0/391]	Time  0.192 ( 0.192)	Data  0.169 ( 0.169)	Loss 9.6664e-03 (9.6664e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [80][ 10/391]	Time  0.020 ( 0.039)	Data  0.002 ( 0.017)	Loss 1.4915e-02 (2.8256e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [80][ 20/391]	Time  0.038 ( 0.032)	Data  0.002 ( 0.010)	Loss 2.9648e-02 (2.7391e-02)	Acc@1  98.44 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [80][ 30/391]	Time  0.019 ( 0.030)	Data  0.001 ( 0.007)	Loss 8.6899e-03 (2.8226e-02)	Acc@1 100.00 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [80][ 40/391]	Time  0.018 ( 0.028)	Data  0.001 ( 0.006)	Loss 2.8809e-02 (2.7861e-02)	Acc@1  98.44 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [80][ 50/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.005)	Loss 1.1856e-02 (2.7295e-02)	Acc@1 100.00 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [80][ 60/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.005)	Loss 1.3115e-02 (2.7008e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [80][ 70/391]	Time  0.017 ( 0.027)	Data  0.000 ( 0.004)	Loss 2.4155e-02 (2.7681e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [80][ 80/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.004)	Loss 3.5034e-02 (2.7469e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [80][ 90/391]	Time  0.019 ( 0.026)	Data  0.002 ( 0.004)	Loss 3.2623e-02 (2.7503e-02)	Acc@1  98.44 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [80][100/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5345e-02 (2.7746e-02)	Acc@1  98.44 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [80][110/391]	Time  0.022 ( 0.025)	Data  0.002 ( 0.003)	Loss 1.9135e-02 (2.7302e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [80][120/391]	Time  0.023 ( 0.025)	Data  0.002 ( 0.003)	Loss 2.7374e-02 (2.7871e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [80][130/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.8422e-02 (2.7939e-02)	Acc@1  98.44 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [80][140/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.2657e-02 (2.7282e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [80][150/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.7262e-02 (2.7727e-02)	Acc@1  98.44 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [80][160/391]	Time  0.017 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.7587e-02 (2.7828e-02)	Acc@1  98.44 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [80][170/391]	Time  0.029 ( 0.025)	Data  0.001 ( 0.003)	Loss 7.3166e-03 (2.7934e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [80][180/391]	Time  0.029 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.1622e-02 (2.8817e-02)	Acc@1  99.22 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [80][190/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 2.5879e-02 (2.8802e-02)	Acc@1  98.44 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [80][200/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.9215e-02 (2.8703e-02)	Acc@1  99.22 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [80][210/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.8594e-02 (2.8622e-02)	Acc@1  97.66 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [80][220/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.5044e-02 (2.8508e-02)	Acc@1  97.66 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [80][230/391]	Time  0.033 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.7891e-03 (2.8384e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [80][240/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.0536e-02 (2.8337e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [80][250/391]	Time  0.020 ( 0.025)	Data  0.002 ( 0.003)	Loss 3.6201e-03 (2.8396e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [80][260/391]	Time  0.038 ( 0.025)	Data  0.005 ( 0.003)	Loss 5.5733e-03 (2.8035e-02)	Acc@1 100.00 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [80][270/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.003)	Loss 2.5681e-02 (2.7949e-02)	Acc@1  98.44 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [80][280/391]	Time  0.025 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.0446e-02 (2.7728e-02)	Acc@1  97.66 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [80][290/391]	Time  0.029 ( 0.025)	Data  0.001 ( 0.002)	Loss 6.3896e-03 (2.7555e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [80][300/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.8666e-02 (2.7364e-02)	Acc@1  99.22 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [80][310/391]	Time  0.027 ( 0.025)	Data  0.001 ( 0.002)	Loss 5.7945e-03 (2.7639e-02)	Acc@1 100.00 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [80][320/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.5152e-02 (2.7555e-02)	Acc@1  99.22 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [80][330/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 7.0915e-03 (2.7308e-02)	Acc@1 100.00 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [80][340/391]	Time  0.037 ( 0.024)	Data  0.001 ( 0.002)	Loss 4.1534e-02 (2.7082e-02)	Acc@1  99.22 ( 99.10)	Acc@5  99.22 (100.00)
Epoch: [80][350/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 5.5275e-03 (2.7230e-02)	Acc@1 100.00 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [80][360/391]	Time  0.018 ( 0.024)	Data  0.002 ( 0.002)	Loss 3.4088e-02 (2.7088e-02)	Acc@1  98.44 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [80][370/391]	Time  0.042 ( 0.024)	Data  0.006 ( 0.002)	Loss 1.9058e-02 (2.7234e-02)	Acc@1  99.22 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [80][380/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.0361e-02 (2.7144e-02)	Acc@1 100.00 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [80][390/391]	Time  0.013 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.2283e-02 (2.7257e-02)	Acc@1 100.00 ( 99.11)	Acc@5 100.00 (100.00)
## e[80] optimizer.zero_grad (sum) time: 0.10596442222595215
## e[80]       loss.backward (sum) time: 2.132673978805542
## e[80]      optimizer.step (sum) time: 0.8953099250793457
## epoch[80] training(only) time: 9.65781307220459
# Switched to evaluate mode...
Test: [  0/100]	Time  0.167 ( 0.167)	Loss 3.1128e-01 (3.1128e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.015 ( 0.027)	Loss 5.0342e-01 (3.7580e-01)	Acc@1  90.00 ( 90.64)	Acc@5  99.00 ( 99.45)
Test: [ 20/100]	Time  0.008 ( 0.020)	Loss 5.3223e-01 (4.0785e-01)	Acc@1  87.00 ( 89.95)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.017 ( 0.018)	Loss 4.8706e-01 (4.1378e-01)	Acc@1  89.00 ( 90.23)	Acc@5  99.00 ( 99.55)
Test: [ 40/100]	Time  0.020 ( 0.017)	Loss 5.5957e-01 (4.3721e-01)	Acc@1  88.00 ( 89.80)	Acc@5 100.00 ( 99.56)
Test: [ 50/100]	Time  0.008 ( 0.016)	Loss 1.6479e-01 (4.1728e-01)	Acc@1  95.00 ( 90.18)	Acc@5 100.00 ( 99.53)
Test: [ 60/100]	Time  0.032 ( 0.016)	Loss 2.8174e-01 (4.1690e-01)	Acc@1  93.00 ( 90.13)	Acc@5 100.00 ( 99.56)
Test: [ 70/100]	Time  0.009 ( 0.015)	Loss 6.9482e-01 (4.1191e-01)	Acc@1  87.00 ( 90.25)	Acc@5  99.00 ( 99.55)
Test: [ 80/100]	Time  0.014 ( 0.015)	Loss 1.7981e-01 (4.0593e-01)	Acc@1  96.00 ( 90.35)	Acc@5 100.00 ( 99.57)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 2.8516e-01 (4.0190e-01)	Acc@1  89.00 ( 90.30)	Acc@5 100.00 ( 99.62)
 * Acc@1 90.320 Acc@5 99.640
### epoch[80] execution time: 11.228461742401123
EPOCH 81
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [81][  0/391]	Time  0.255 ( 0.255)	Data  0.230 ( 0.230)	Loss 2.2079e-02 (2.2079e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [81][ 10/391]	Time  0.028 ( 0.044)	Data  0.001 ( 0.022)	Loss 7.0076e-03 (2.3378e-02)	Acc@1 100.00 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [81][ 20/391]	Time  0.018 ( 0.034)	Data  0.001 ( 0.013)	Loss 4.4250e-03 (2.2864e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [81][ 30/391]	Time  0.018 ( 0.030)	Data  0.001 ( 0.009)	Loss 9.4910e-03 (2.5177e-02)	Acc@1  99.22 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [81][ 40/391]	Time  0.034 ( 0.029)	Data  0.004 ( 0.007)	Loss 6.2485e-03 (2.6348e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [81][ 50/391]	Time  0.017 ( 0.028)	Data  0.000 ( 0.006)	Loss 2.4643e-02 (2.5065e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [81][ 60/391]	Time  0.020 ( 0.027)	Data  0.002 ( 0.005)	Loss 4.5990e-02 (2.5574e-02)	Acc@1  99.22 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [81][ 70/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.005)	Loss 3.6163e-03 (2.6036e-02)	Acc@1 100.00 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [81][ 80/391]	Time  0.037 ( 0.026)	Data  0.003 ( 0.005)	Loss 4.2847e-02 (2.6024e-02)	Acc@1  98.44 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [81][ 90/391]	Time  0.041 ( 0.026)	Data  0.003 ( 0.004)	Loss 5.6801e-03 (2.6468e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [81][100/391]	Time  0.035 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.7334e-02 (2.6076e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [81][110/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.4679e-02 (2.5434e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [81][120/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.4618e-02 (2.5254e-02)	Acc@1 100.00 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [81][130/391]	Time  0.045 ( 0.025)	Data  0.003 ( 0.003)	Loss 6.1218e-02 (2.6242e-02)	Acc@1  96.88 ( 99.18)	Acc@5 100.00 ( 99.99)
Epoch: [81][140/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.7985e-02 (2.6676e-02)	Acc@1  98.44 ( 99.16)	Acc@5 100.00 ( 99.99)
Epoch: [81][150/391]	Time  0.027 ( 0.025)	Data  0.002 ( 0.003)	Loss 8.1940e-03 (2.5828e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 ( 99.99)
Epoch: [81][160/391]	Time  0.018 ( 0.025)	Data  0.002 ( 0.003)	Loss 3.2349e-02 (2.5569e-02)	Acc@1  98.44 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [81][170/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.8875e-02 (2.5691e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [81][180/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 2.3632e-03 (2.5521e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [81][190/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.2216e-02 (2.6061e-02)	Acc@1  98.44 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [81][200/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.9022e-02 (2.5817e-02)	Acc@1  98.44 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [81][210/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.3345e-02 (2.6054e-02)	Acc@1  98.44 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [81][220/391]	Time  0.024 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.0971e-02 (2.5926e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [81][230/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.7802e-02 (2.6123e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [81][240/391]	Time  0.032 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.0795e-03 (2.6161e-02)	Acc@1 100.00 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [81][250/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.2999e-02 (2.6327e-02)	Acc@1  97.66 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [81][260/391]	Time  0.029 ( 0.025)	Data  0.000 ( 0.003)	Loss 3.1021e-02 (2.6142e-02)	Acc@1  98.44 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [81][270/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.4900e-02 (2.6138e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [81][280/391]	Time  0.017 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.1189e-02 (2.6345e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [81][290/391]	Time  0.035 ( 0.025)	Data  0.003 ( 0.003)	Loss 1.8661e-02 (2.6191e-02)	Acc@1  99.22 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [81][300/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.7426e-02 (2.6063e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [81][310/391]	Time  0.031 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.6886e-02 (2.6100e-02)	Acc@1  98.44 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [81][320/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.1787e-02 (2.6369e-02)	Acc@1  97.66 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [81][330/391]	Time  0.034 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.2222e-02 (2.6189e-02)	Acc@1 100.00 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [81][340/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.7410e-02 (2.6285e-02)	Acc@1  99.22 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [81][350/391]	Time  0.032 ( 0.025)	Data  0.000 ( 0.002)	Loss 4.3091e-02 (2.6120e-02)	Acc@1  98.44 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [81][360/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 5.1666e-02 (2.6008e-02)	Acc@1  98.44 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [81][370/391]	Time  0.024 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.0985e-02 (2.5699e-02)	Acc@1  99.22 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [81][380/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.7821e-02 (2.5636e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [81][390/391]	Time  0.014 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.6072e-02 (2.5749e-02)	Acc@1  98.75 ( 99.16)	Acc@5 100.00 (100.00)
## e[81] optimizer.zero_grad (sum) time: 0.10447049140930176
## e[81]       loss.backward (sum) time: 2.1797378063201904
## e[81]      optimizer.step (sum) time: 0.8824152946472168
## epoch[81] training(only) time: 9.69158935546875
# Switched to evaluate mode...
Test: [  0/100]	Time  0.158 ( 0.158)	Loss 3.0615e-01 (3.0615e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.011 ( 0.027)	Loss 5.1709e-01 (3.8338e-01)	Acc@1  90.00 ( 90.73)	Acc@5  99.00 ( 99.45)
Test: [ 20/100]	Time  0.008 ( 0.020)	Loss 5.4248e-01 (4.1290e-01)	Acc@1  87.00 ( 90.10)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.016 ( 0.018)	Loss 4.8730e-01 (4.1856e-01)	Acc@1  90.00 ( 90.35)	Acc@5  99.00 ( 99.58)
Test: [ 40/100]	Time  0.015 ( 0.017)	Loss 5.6982e-01 (4.4111e-01)	Acc@1  89.00 ( 89.98)	Acc@5 100.00 ( 99.59)
Test: [ 50/100]	Time  0.014 ( 0.016)	Loss 1.7114e-01 (4.2125e-01)	Acc@1  96.00 ( 90.33)	Acc@5 100.00 ( 99.55)
Test: [ 60/100]	Time  0.008 ( 0.015)	Loss 2.7124e-01 (4.1921e-01)	Acc@1  93.00 ( 90.31)	Acc@5 100.00 ( 99.57)
Test: [ 70/100]	Time  0.016 ( 0.015)	Loss 7.1240e-01 (4.1448e-01)	Acc@1  87.00 ( 90.44)	Acc@5  99.00 ( 99.56)
Test: [ 80/100]	Time  0.017 ( 0.015)	Loss 1.7249e-01 (4.0850e-01)	Acc@1  96.00 ( 90.52)	Acc@5 100.00 ( 99.58)
Test: [ 90/100]	Time  0.011 ( 0.015)	Loss 2.7148e-01 (4.0453e-01)	Acc@1  90.00 ( 90.46)	Acc@5 100.00 ( 99.63)
 * Acc@1 90.460 Acc@5 99.650
### epoch[81] execution time: 11.249184131622314
EPOCH 82
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [82][  0/391]	Time  0.182 ( 0.182)	Data  0.160 ( 0.160)	Loss 1.3031e-02 (1.3031e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [82][ 10/391]	Time  0.019 ( 0.038)	Data  0.001 ( 0.017)	Loss 3.3569e-02 (3.3000e-02)	Acc@1  99.22 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [82][ 20/391]	Time  0.018 ( 0.031)	Data  0.001 ( 0.010)	Loss 3.3447e-02 (2.9871e-02)	Acc@1  98.44 ( 98.81)	Acc@5 100.00 (100.00)
Epoch: [82][ 30/391]	Time  0.019 ( 0.029)	Data  0.001 ( 0.007)	Loss 1.5221e-02 (3.2978e-02)	Acc@1  99.22 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [82][ 40/391]	Time  0.025 ( 0.027)	Data  0.002 ( 0.006)	Loss 3.1143e-02 (3.2617e-02)	Acc@1  99.22 ( 98.82)	Acc@5 100.00 (100.00)
Epoch: [82][ 50/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.005)	Loss 3.3813e-02 (3.2459e-02)	Acc@1  99.22 ( 98.79)	Acc@5 100.00 (100.00)
Epoch: [82][ 60/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.005)	Loss 1.2154e-02 (3.2626e-02)	Acc@1 100.00 ( 98.83)	Acc@5 100.00 (100.00)
Epoch: [82][ 70/391]	Time  0.039 ( 0.026)	Data  0.001 ( 0.004)	Loss 4.7913e-02 (3.3379e-02)	Acc@1  98.44 ( 98.80)	Acc@5 100.00 (100.00)
Epoch: [82][ 80/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.4107e-02 (3.2991e-02)	Acc@1  99.22 ( 98.80)	Acc@5 100.00 (100.00)
Epoch: [82][ 90/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.004)	Loss 2.4094e-02 (3.3047e-02)	Acc@1  98.44 ( 98.81)	Acc@5 100.00 (100.00)
Epoch: [82][100/391]	Time  0.037 ( 0.025)	Data  0.005 ( 0.003)	Loss 1.2611e-02 (3.2245e-02)	Acc@1 100.00 ( 98.86)	Acc@5 100.00 (100.00)
Epoch: [82][110/391]	Time  0.026 ( 0.025)	Data  0.002 ( 0.003)	Loss 1.1940e-02 (3.1059e-02)	Acc@1 100.00 ( 98.92)	Acc@5 100.00 (100.00)
Epoch: [82][120/391]	Time  0.022 ( 0.025)	Data  0.002 ( 0.003)	Loss 1.0582e-02 (3.0238e-02)	Acc@1 100.00 ( 98.94)	Acc@5 100.00 (100.00)
Epoch: [82][130/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.0192e-02 (2.9865e-02)	Acc@1  99.22 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [82][140/391]	Time  0.029 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.4220e-02 (2.9705e-02)	Acc@1  98.44 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [82][150/391]	Time  0.029 ( 0.025)	Data  0.000 ( 0.003)	Loss 3.5248e-02 (3.0359e-02)	Acc@1  99.22 ( 98.94)	Acc@5 100.00 ( 99.99)
Epoch: [82][160/391]	Time  0.030 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.2604e-02 (2.9375e-02)	Acc@1  99.22 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [82][170/391]	Time  0.027 ( 0.025)	Data  0.000 ( 0.003)	Loss 2.0523e-03 (2.9562e-02)	Acc@1 100.00 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [82][180/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.3978e-03 (2.9291e-02)	Acc@1 100.00 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [82][190/391]	Time  0.032 ( 0.025)	Data  0.004 ( 0.003)	Loss 1.4824e-02 (2.9125e-02)	Acc@1 100.00 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [82][200/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.3504e-02 (2.8839e-02)	Acc@1 100.00 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [82][210/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 1.9287e-02 (2.8581e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [82][220/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 2.2964e-02 (2.8495e-02)	Acc@1 100.00 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [82][230/391]	Time  0.038 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.6458e-03 (2.8692e-02)	Acc@1 100.00 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [82][240/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.1957e-02 (2.8511e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [82][250/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.2688e-02 (2.8352e-02)	Acc@1 100.00 ( 99.03)	Acc@5 100.00 ( 99.99)
Epoch: [82][260/391]	Time  0.036 ( 0.025)	Data  0.003 ( 0.002)	Loss 3.0777e-02 (2.8318e-02)	Acc@1  98.44 ( 99.03)	Acc@5 100.00 ( 99.99)
Epoch: [82][270/391]	Time  0.040 ( 0.025)	Data  0.000 ( 0.003)	Loss 1.8906e-02 (2.8318e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 ( 99.99)
Epoch: [82][280/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.9577e-02 (2.8463e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 ( 99.99)
Epoch: [82][290/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 7.2937e-02 (2.8655e-02)	Acc@1  98.44 ( 99.03)	Acc@5 100.00 ( 99.99)
Epoch: [82][300/391]	Time  0.040 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.4948e-02 (2.8466e-02)	Acc@1  98.44 ( 99.03)	Acc@5 100.00 ( 99.99)
Epoch: [82][310/391]	Time  0.029 ( 0.025)	Data  0.001 ( 0.002)	Loss 8.8013e-02 (2.8450e-02)	Acc@1  97.66 ( 99.04)	Acc@5 100.00 ( 99.99)
Epoch: [82][320/391]	Time  0.037 ( 0.025)	Data  0.001 ( 0.002)	Loss 8.5678e-03 (2.8471e-02)	Acc@1 100.00 ( 99.04)	Acc@5 100.00 ( 99.99)
Epoch: [82][330/391]	Time  0.023 ( 0.024)	Data  0.001 ( 0.002)	Loss 9.8877e-03 (2.8499e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 ( 99.99)
Epoch: [82][340/391]	Time  0.037 ( 0.024)	Data  0.001 ( 0.002)	Loss 9.6817e-03 (2.8419e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 ( 99.99)
Epoch: [82][350/391]	Time  0.042 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.7201e-02 (2.8203e-02)	Acc@1  98.44 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [82][360/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 7.4158e-02 (2.8571e-02)	Acc@1  97.66 ( 99.05)	Acc@5 100.00 ( 99.99)
Epoch: [82][370/391]	Time  0.028 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.1925e-02 (2.8487e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [82][380/391]	Time  0.023 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.3087e-02 (2.8491e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [82][390/391]	Time  0.013 ( 0.024)	Data  0.000 ( 0.002)	Loss 5.7922e-02 (2.8625e-02)	Acc@1  97.50 ( 99.05)	Acc@5 100.00 ( 99.99)
## e[82] optimizer.zero_grad (sum) time: 0.10420680046081543
## e[82]       loss.backward (sum) time: 2.144076108932495
## e[82]      optimizer.step (sum) time: 0.883281946182251
## epoch[82] training(only) time: 9.646889925003052
# Switched to evaluate mode...
Test: [  0/100]	Time  0.164 ( 0.164)	Loss 3.0566e-01 (3.0566e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.009 ( 0.026)	Loss 4.9536e-01 (3.7629e-01)	Acc@1  90.00 ( 90.64)	Acc@5  99.00 ( 99.45)
Test: [ 20/100]	Time  0.023 ( 0.020)	Loss 5.4004e-01 (4.0640e-01)	Acc@1  88.00 ( 90.19)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.008 ( 0.018)	Loss 4.7290e-01 (4.1397e-01)	Acc@1  88.00 ( 90.23)	Acc@5 100.00 ( 99.61)
Test: [ 40/100]	Time  0.013 ( 0.017)	Loss 5.4443e-01 (4.3379e-01)	Acc@1  89.00 ( 89.83)	Acc@5  99.00 ( 99.59)
Test: [ 50/100]	Time  0.008 ( 0.016)	Loss 1.5430e-01 (4.1452e-01)	Acc@1  95.00 ( 90.16)	Acc@5 100.00 ( 99.55)
Test: [ 60/100]	Time  0.017 ( 0.016)	Loss 2.7319e-01 (4.1399e-01)	Acc@1  93.00 ( 90.15)	Acc@5 100.00 ( 99.57)
Test: [ 70/100]	Time  0.013 ( 0.015)	Loss 6.9775e-01 (4.0916e-01)	Acc@1  87.00 ( 90.30)	Acc@5  99.00 ( 99.58)
Test: [ 80/100]	Time  0.008 ( 0.015)	Loss 1.7908e-01 (4.0310e-01)	Acc@1  96.00 ( 90.43)	Acc@5 100.00 ( 99.59)
Test: [ 90/100]	Time  0.008 ( 0.015)	Loss 2.7246e-01 (3.9937e-01)	Acc@1  90.00 ( 90.44)	Acc@5 100.00 ( 99.64)
 * Acc@1 90.420 Acc@5 99.660
### epoch[82] execution time: 11.247639417648315
EPOCH 83
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [83][  0/391]	Time  0.198 ( 0.198)	Data  0.168 ( 0.168)	Loss 1.2566e-02 (1.2566e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [83][ 10/391]	Time  0.019 ( 0.037)	Data  0.001 ( 0.017)	Loss 8.1482e-03 (1.8986e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [83][ 20/391]	Time  0.018 ( 0.032)	Data  0.001 ( 0.010)	Loss 1.1475e-02 (2.2522e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [83][ 30/391]	Time  0.018 ( 0.029)	Data  0.001 ( 0.007)	Loss 3.6591e-02 (2.2912e-02)	Acc@1  98.44 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [83][ 40/391]	Time  0.027 ( 0.028)	Data  0.001 ( 0.006)	Loss 2.6016e-02 (2.3357e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [83][ 50/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.005)	Loss 2.0874e-02 (2.4286e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [83][ 60/391]	Time  0.026 ( 0.026)	Data  0.002 ( 0.004)	Loss 2.8183e-02 (2.4456e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [83][ 70/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.6602e-02 (2.4208e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [83][ 80/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.004)	Loss 2.2995e-02 (2.5723e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [83][ 90/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.004)	Loss 1.8417e-02 (2.6615e-02)	Acc@1  99.22 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [83][100/391]	Time  0.035 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.6082e-02 (2.6355e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [83][110/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.3702e-02 (2.7280e-02)	Acc@1 100.00 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [83][120/391]	Time  0.033 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.2505e-02 (2.6827e-02)	Acc@1 100.00 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [83][130/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.2898e-02 (2.7077e-02)	Acc@1  98.44 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [83][140/391]	Time  0.031 ( 0.025)	Data  0.000 ( 0.003)	Loss 4.8920e-02 (2.7067e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [83][150/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.2318e-02 (2.6942e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [83][160/391]	Time  0.021 ( 0.025)	Data  0.000 ( 0.003)	Loss 1.2520e-02 (2.7016e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [83][170/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.4694e-02 (2.7701e-02)	Acc@1 100.00 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [83][180/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.2741e-02 (2.7837e-02)	Acc@1 100.00 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [83][190/391]	Time  0.039 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.6305e-02 (2.7821e-02)	Acc@1  97.66 ( 99.04)	Acc@5  99.22 (100.00)
Epoch: [83][200/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.2283e-02 (2.7543e-02)	Acc@1 100.00 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [83][210/391]	Time  0.032 ( 0.025)	Data  0.000 ( 0.003)	Loss 9.6802e-02 (2.7679e-02)	Acc@1  96.88 ( 99.07)	Acc@5  99.22 ( 99.99)
Epoch: [83][220/391]	Time  0.027 ( 0.025)	Data  0.002 ( 0.002)	Loss 4.1229e-02 (2.7254e-02)	Acc@1  97.66 ( 99.09)	Acc@5 100.00 ( 99.99)
Epoch: [83][230/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 4.0649e-02 (2.7281e-02)	Acc@1  98.44 ( 99.09)	Acc@5 100.00 ( 99.99)
Epoch: [83][240/391]	Time  0.043 ( 0.024)	Data  0.011 ( 0.002)	Loss 1.4351e-02 (2.7263e-02)	Acc@1  99.22 ( 99.08)	Acc@5 100.00 ( 99.99)
Epoch: [83][250/391]	Time  0.028 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.9642e-02 (2.7240e-02)	Acc@1  98.44 ( 99.08)	Acc@5 100.00 ( 99.99)
Epoch: [83][260/391]	Time  0.019 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.0752e-02 (2.7253e-02)	Acc@1  98.44 ( 99.07)	Acc@5 100.00 ( 99.99)
Epoch: [83][270/391]	Time  0.024 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.1467e-02 (2.6844e-02)	Acc@1 100.00 ( 99.09)	Acc@5 100.00 ( 99.99)
Epoch: [83][280/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 6.9695e-03 (2.6650e-02)	Acc@1 100.00 ( 99.10)	Acc@5 100.00 ( 99.99)
Epoch: [83][290/391]	Time  0.036 ( 0.024)	Data  0.001 ( 0.002)	Loss 5.4901e-02 (2.6891e-02)	Acc@1  98.44 ( 99.09)	Acc@5 100.00 ( 99.99)
Epoch: [83][300/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.4763e-02 (2.6917e-02)	Acc@1 100.00 ( 99.09)	Acc@5 100.00 ( 99.99)
Epoch: [83][310/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 7.3738e-03 (2.6746e-02)	Acc@1 100.00 ( 99.10)	Acc@5 100.00 ( 99.99)
Epoch: [83][320/391]	Time  0.040 ( 0.024)	Data  0.004 ( 0.002)	Loss 2.4567e-02 (2.6584e-02)	Acc@1  98.44 ( 99.10)	Acc@5 100.00 ( 99.99)
Epoch: [83][330/391]	Time  0.023 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.1993e-02 (2.6949e-02)	Acc@1 100.00 ( 99.08)	Acc@5 100.00 ( 99.99)
Epoch: [83][340/391]	Time  0.039 ( 0.024)	Data  0.005 ( 0.002)	Loss 2.6825e-02 (2.7109e-02)	Acc@1  99.22 ( 99.08)	Acc@5 100.00 ( 99.99)
Epoch: [83][350/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.6194e-02 (2.7211e-02)	Acc@1  97.66 ( 99.07)	Acc@5 100.00 ( 99.99)
Epoch: [83][360/391]	Time  0.053 ( 0.024)	Data  0.019 ( 0.002)	Loss 1.1116e-02 (2.7453e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 ( 99.99)
Epoch: [83][370/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 4.9255e-02 (2.7418e-02)	Acc@1  98.44 ( 99.07)	Acc@5 100.00 ( 99.99)
Epoch: [83][380/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 5.1025e-02 (2.7176e-02)	Acc@1  98.44 ( 99.08)	Acc@5 100.00 ( 99.99)
Epoch: [83][390/391]	Time  0.015 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.0721e-02 (2.7269e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 ( 99.99)
## e[83] optimizer.zero_grad (sum) time: 0.10435891151428223
## e[83]       loss.backward (sum) time: 2.1996428966522217
## e[83]      optimizer.step (sum) time: 0.8901057243347168
## epoch[83] training(only) time: 9.581752300262451
# Switched to evaluate mode...
Test: [  0/100]	Time  0.160 ( 0.160)	Loss 3.0737e-01 (3.0737e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.011 ( 0.027)	Loss 4.8193e-01 (3.7783e-01)	Acc@1  90.00 ( 90.45)	Acc@5  99.00 ( 99.36)
Test: [ 20/100]	Time  0.008 ( 0.020)	Loss 5.4395e-01 (4.0641e-01)	Acc@1  87.00 ( 90.10)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.013 ( 0.018)	Loss 4.6948e-01 (4.1271e-01)	Acc@1  89.00 ( 90.32)	Acc@5 100.00 ( 99.58)
Test: [ 40/100]	Time  0.015 ( 0.017)	Loss 5.6104e-01 (4.3355e-01)	Acc@1  89.00 ( 89.98)	Acc@5  99.00 ( 99.56)
Test: [ 50/100]	Time  0.009 ( 0.016)	Loss 1.6248e-01 (4.1498e-01)	Acc@1  95.00 ( 90.27)	Acc@5 100.00 ( 99.53)
Test: [ 60/100]	Time  0.014 ( 0.016)	Loss 2.9565e-01 (4.1533e-01)	Acc@1  93.00 ( 90.23)	Acc@5 100.00 ( 99.56)
Test: [ 70/100]	Time  0.008 ( 0.015)	Loss 6.9678e-01 (4.1030e-01)	Acc@1  88.00 ( 90.41)	Acc@5  99.00 ( 99.55)
Test: [ 80/100]	Time  0.008 ( 0.015)	Loss 1.7896e-01 (4.0423e-01)	Acc@1  96.00 ( 90.49)	Acc@5 100.00 ( 99.57)
Test: [ 90/100]	Time  0.008 ( 0.015)	Loss 2.5049e-01 (4.0006e-01)	Acc@1  91.00 ( 90.49)	Acc@5 100.00 ( 99.62)
 * Acc@1 90.480 Acc@5 99.640
### epoch[83] execution time: 11.176384210586548
EPOCH 84
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [84][  0/391]	Time  0.201 ( 0.201)	Data  0.175 ( 0.175)	Loss 2.3926e-02 (2.3926e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [84][ 10/391]	Time  0.031 ( 0.041)	Data  0.000 ( 0.017)	Loss 3.8483e-02 (2.3044e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [84][ 20/391]	Time  0.020 ( 0.031)	Data  0.003 ( 0.010)	Loss 3.4576e-02 (2.7123e-02)	Acc@1  98.44 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [84][ 30/391]	Time  0.018 ( 0.029)	Data  0.001 ( 0.008)	Loss 4.0649e-02 (2.8625e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [84][ 40/391]	Time  0.018 ( 0.028)	Data  0.001 ( 0.006)	Loss 3.3630e-02 (2.6745e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [84][ 50/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.7075e-02 (2.5768e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [84][ 60/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.005)	Loss 9.4299e-02 (2.6048e-02)	Acc@1  98.44 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [84][ 70/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 2.1072e-02 (2.6417e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 ( 99.99)
Epoch: [84][ 80/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 2.9724e-02 (2.7143e-02)	Acc@1  98.44 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [84][ 90/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.6174e-02 (2.7057e-02)	Acc@1  99.22 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [84][100/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 4.6356e-02 (2.6576e-02)	Acc@1  98.44 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [84][110/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1826e-02 (2.6346e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 ( 99.99)
Epoch: [84][120/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 5.2856e-02 (2.6873e-02)	Acc@1  97.66 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [84][130/391]	Time  0.037 ( 0.025)	Data  0.002 ( 0.003)	Loss 4.0833e-02 (2.6628e-02)	Acc@1  98.44 ( 99.16)	Acc@5 100.00 ( 99.99)
Epoch: [84][140/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.9700e-03 (2.6685e-02)	Acc@1 100.00 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [84][150/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 4.4373e-02 (2.6813e-02)	Acc@1  98.44 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [84][160/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.9653e-02 (2.6777e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [84][170/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 2.5005e-03 (2.5958e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [84][180/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.0096e-02 (2.5595e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [84][190/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 2.6520e-02 (2.5644e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [84][200/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 9.9182e-03 (2.5859e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [84][210/391]	Time  0.021 ( 0.025)	Data  0.002 ( 0.003)	Loss 5.7678e-03 (2.5875e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 ( 99.99)
Epoch: [84][220/391]	Time  0.018 ( 0.025)	Data  0.002 ( 0.003)	Loss 1.1261e-02 (2.5759e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 ( 99.99)
Epoch: [84][230/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.8727e-02 (2.5928e-02)	Acc@1  98.44 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [84][240/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.7609e-02 (2.6266e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 ( 99.99)
Epoch: [84][250/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 6.4880e-02 (2.6259e-02)	Acc@1  97.66 ( 99.16)	Acc@5 100.00 ( 99.99)
Epoch: [84][260/391]	Time  0.032 ( 0.025)	Data  0.002 ( 0.002)	Loss 1.8021e-02 (2.6358e-02)	Acc@1  99.22 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [84][270/391]	Time  0.039 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.9774e-03 (2.6372e-02)	Acc@1 100.00 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [84][280/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.2480e-02 (2.6493e-02)	Acc@1  98.44 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [84][290/391]	Time  0.027 ( 0.025)	Data  0.006 ( 0.002)	Loss 2.1439e-02 (2.6681e-02)	Acc@1  99.22 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [84][300/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.002)	Loss 9.1782e-03 (2.7018e-02)	Acc@1  99.22 ( 99.12)	Acc@5 100.00 ( 99.99)
Epoch: [84][310/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.0849e-02 (2.7097e-02)	Acc@1 100.00 ( 99.11)	Acc@5 100.00 ( 99.99)
Epoch: [84][320/391]	Time  0.042 ( 0.025)	Data  0.006 ( 0.002)	Loss 2.7023e-02 (2.7341e-02)	Acc@1  99.22 ( 99.10)	Acc@5 100.00 ( 99.99)
Epoch: [84][330/391]	Time  0.034 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.1311e-02 (2.7546e-02)	Acc@1  98.44 ( 99.10)	Acc@5 100.00 ( 99.99)
Epoch: [84][340/391]	Time  0.018 ( 0.025)	Data  0.002 ( 0.002)	Loss 1.7914e-02 (2.7341e-02)	Acc@1  99.22 ( 99.10)	Acc@5 100.00 ( 99.99)
Epoch: [84][350/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 5.9662e-02 (2.7066e-02)	Acc@1  98.44 ( 99.12)	Acc@5 100.00 ( 99.99)
Epoch: [84][360/391]	Time  0.034 ( 0.025)	Data  0.004 ( 0.002)	Loss 1.7181e-02 (2.6910e-02)	Acc@1  99.22 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [84][370/391]	Time  0.034 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.2379e-02 (2.6748e-02)	Acc@1  98.44 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [84][380/391]	Time  0.034 ( 0.025)	Data  0.000 ( 0.002)	Loss 1.8738e-02 (2.6843e-02)	Acc@1  99.22 ( 99.12)	Acc@5 100.00 ( 99.99)
Epoch: [84][390/391]	Time  0.013 ( 0.024)	Data  0.000 ( 0.002)	Loss 4.8767e-02 (2.6794e-02)	Acc@1  97.50 ( 99.12)	Acc@5 100.00 ( 99.99)
## e[84] optimizer.zero_grad (sum) time: 0.10636305809020996
## e[84]       loss.backward (sum) time: 2.1210758686065674
## e[84]      optimizer.step (sum) time: 0.8890624046325684
## epoch[84] training(only) time: 9.69485354423523
# Switched to evaluate mode...
Test: [  0/100]	Time  0.160 ( 0.160)	Loss 3.1470e-01 (3.1470e-01)	Acc@1  91.00 ( 91.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.012 ( 0.026)	Loss 4.7046e-01 (3.7091e-01)	Acc@1  90.00 ( 90.82)	Acc@5  99.00 ( 99.55)
Test: [ 20/100]	Time  0.016 ( 0.020)	Loss 5.2393e-01 (4.0498e-01)	Acc@1  88.00 ( 90.19)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.008 ( 0.017)	Loss 4.8389e-01 (4.1026e-01)	Acc@1  88.00 ( 90.23)	Acc@5  99.00 ( 99.58)
Test: [ 40/100]	Time  0.008 ( 0.016)	Loss 5.6982e-01 (4.3368e-01)	Acc@1  89.00 ( 89.90)	Acc@5 100.00 ( 99.59)
Test: [ 50/100]	Time  0.017 ( 0.016)	Loss 1.6492e-01 (4.1498e-01)	Acc@1  95.00 ( 90.20)	Acc@5 100.00 ( 99.55)
Test: [ 60/100]	Time  0.008 ( 0.015)	Loss 2.7856e-01 (4.1422e-01)	Acc@1  92.00 ( 90.15)	Acc@5 100.00 ( 99.57)
Test: [ 70/100]	Time  0.034 ( 0.015)	Loss 7.3242e-01 (4.1003e-01)	Acc@1  87.00 ( 90.31)	Acc@5  99.00 ( 99.55)
Test: [ 80/100]	Time  0.008 ( 0.015)	Loss 1.8726e-01 (4.0501e-01)	Acc@1  96.00 ( 90.44)	Acc@5 100.00 ( 99.57)
Test: [ 90/100]	Time  0.011 ( 0.015)	Loss 2.6978e-01 (4.0081e-01)	Acc@1  90.00 ( 90.41)	Acc@5 100.00 ( 99.62)
 * Acc@1 90.390 Acc@5 99.640
### epoch[84] execution time: 11.269856214523315
EPOCH 85
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [85][  0/391]	Time  0.246 ( 0.246)	Data  0.223 ( 0.223)	Loss 7.9422e-03 (7.9422e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [85][ 10/391]	Time  0.038 ( 0.045)	Data  0.001 ( 0.022)	Loss 3.2806e-02 (1.8523e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [85][ 20/391]	Time  0.028 ( 0.035)	Data  0.001 ( 0.012)	Loss 1.0468e-02 (2.3216e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [85][ 30/391]	Time  0.028 ( 0.031)	Data  0.001 ( 0.009)	Loss 7.8278e-03 (2.3301e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 ( 99.97)
Epoch: [85][ 40/391]	Time  0.018 ( 0.029)	Data  0.001 ( 0.007)	Loss 7.1182e-03 (2.2227e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 ( 99.98)
Epoch: [85][ 50/391]	Time  0.020 ( 0.028)	Data  0.001 ( 0.006)	Loss 3.4698e-02 (2.3057e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 ( 99.98)
Epoch: [85][ 60/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.005)	Loss 9.4223e-03 (2.2756e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 ( 99.99)
Epoch: [85][ 70/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.005)	Loss 3.0594e-02 (2.4426e-02)	Acc@1  97.66 ( 99.23)	Acc@5 100.00 ( 99.99)
Epoch: [85][ 80/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.005)	Loss 4.4830e-02 (2.5026e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 ( 99.99)
Epoch: [85][ 90/391]	Time  0.031 ( 0.026)	Data  0.000 ( 0.004)	Loss 1.9684e-02 (2.5187e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 ( 99.98)
Epoch: [85][100/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 3.2623e-02 (2.4857e-02)	Acc@1  97.66 ( 99.18)	Acc@5 100.00 ( 99.98)
Epoch: [85][110/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.004)	Loss 3.6469e-02 (2.4878e-02)	Acc@1  97.66 ( 99.18)	Acc@5 100.00 ( 99.99)
Epoch: [85][120/391]	Time  0.028 ( 0.026)	Data  0.000 ( 0.004)	Loss 5.2826e-02 (2.5522e-02)	Acc@1  98.44 ( 99.14)	Acc@5 100.00 ( 99.98)
Epoch: [85][130/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.004)	Loss 3.1235e-02 (2.6411e-02)	Acc@1  98.44 ( 99.09)	Acc@5 100.00 ( 99.98)
Epoch: [85][140/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.6783e-02 (2.7587e-02)	Acc@1  98.44 ( 99.07)	Acc@5 100.00 ( 99.98)
Epoch: [85][150/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.3923e-03 (2.8011e-02)	Acc@1 100.00 ( 99.06)	Acc@5 100.00 ( 99.98)
Epoch: [85][160/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 3.3081e-02 (2.7484e-02)	Acc@1  97.66 ( 99.07)	Acc@5 100.00 ( 99.99)
Epoch: [85][170/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.9938e-02 (2.7595e-02)	Acc@1  99.22 ( 99.07)	Acc@5 100.00 ( 99.99)
Epoch: [85][180/391]	Time  0.018 ( 0.025)	Data  0.002 ( 0.003)	Loss 9.1629e-03 (2.7873e-02)	Acc@1 100.00 ( 99.08)	Acc@5 100.00 ( 99.98)
Epoch: [85][190/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.6062e-02 (2.7475e-02)	Acc@1  99.22 ( 99.08)	Acc@5 100.00 ( 99.98)
Epoch: [85][200/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 2.1957e-02 (2.7433e-02)	Acc@1  99.22 ( 99.09)	Acc@5 100.00 ( 99.98)
Epoch: [85][210/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.3041e-02 (2.7228e-02)	Acc@1  99.22 ( 99.10)	Acc@5 100.00 ( 99.99)
Epoch: [85][220/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.0523e-02 (2.7629e-02)	Acc@1  99.22 ( 99.07)	Acc@5 100.00 ( 99.99)
Epoch: [85][230/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 8.3923e-03 (2.7674e-02)	Acc@1 100.00 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [85][240/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.7883e-02 (2.7514e-02)	Acc@1  99.22 ( 99.08)	Acc@5 100.00 ( 99.99)
Epoch: [85][250/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.8534e-02 (2.7560e-02)	Acc@1  99.22 ( 99.08)	Acc@5 100.00 ( 99.99)
Epoch: [85][260/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.003)	Loss 7.2510e-02 (2.7267e-02)	Acc@1  98.44 ( 99.10)	Acc@5 100.00 ( 99.99)
Epoch: [85][270/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 9.1629e-03 (2.7183e-02)	Acc@1 100.00 ( 99.09)	Acc@5 100.00 ( 99.99)
Epoch: [85][280/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.6342e-02 (2.7170e-02)	Acc@1  99.22 ( 99.10)	Acc@5 100.00 ( 99.99)
Epoch: [85][290/391]	Time  0.021 ( 0.025)	Data  0.002 ( 0.003)	Loss 4.9469e-02 (2.7465e-02)	Acc@1  99.22 ( 99.10)	Acc@5  99.22 ( 99.99)
Epoch: [85][300/391]	Time  0.024 ( 0.025)	Data  0.003 ( 0.003)	Loss 4.6844e-03 (2.7237e-02)	Acc@1 100.00 ( 99.10)	Acc@5 100.00 ( 99.99)
Epoch: [85][310/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.2604e-02 (2.6961e-02)	Acc@1 100.00 ( 99.11)	Acc@5 100.00 ( 99.99)
Epoch: [85][320/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.0960e-02 (2.6932e-02)	Acc@1  99.22 ( 99.11)	Acc@5 100.00 ( 99.99)
Epoch: [85][330/391]	Time  0.022 ( 0.025)	Data  0.005 ( 0.002)	Loss 2.0569e-02 (2.6815e-02)	Acc@1  99.22 ( 99.11)	Acc@5 100.00 ( 99.99)
Epoch: [85][340/391]	Time  0.027 ( 0.025)	Data  0.003 ( 0.002)	Loss 5.7793e-03 (2.6645e-02)	Acc@1 100.00 ( 99.11)	Acc@5 100.00 ( 99.99)
Epoch: [85][350/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.002)	Loss 2.6382e-02 (2.6449e-02)	Acc@1  99.22 ( 99.12)	Acc@5 100.00 ( 99.99)
Epoch: [85][360/391]	Time  0.026 ( 0.025)	Data  0.002 ( 0.002)	Loss 4.3304e-02 (2.6465e-02)	Acc@1  98.44 ( 99.12)	Acc@5 100.00 ( 99.99)
Epoch: [85][370/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 8.3694e-03 (2.6816e-02)	Acc@1 100.00 ( 99.12)	Acc@5 100.00 ( 99.99)
Epoch: [85][380/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 5.8685e-02 (2.6804e-02)	Acc@1  98.44 ( 99.12)	Acc@5 100.00 ( 99.99)
Epoch: [85][390/391]	Time  0.013 ( 0.024)	Data  0.000 ( 0.002)	Loss 3.1769e-02 (2.6794e-02)	Acc@1  97.50 ( 99.11)	Acc@5 100.00 ( 99.99)
## e[85] optimizer.zero_grad (sum) time: 0.10547900199890137
## e[85]       loss.backward (sum) time: 2.1651248931884766
## e[85]      optimizer.step (sum) time: 0.8868539333343506
## epoch[85] training(only) time: 9.69440245628357
# Switched to evaluate mode...
Test: [  0/100]	Time  0.166 ( 0.166)	Loss 3.2153e-01 (3.2153e-01)	Acc@1  91.00 ( 91.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.008 ( 0.027)	Loss 5.0488e-01 (3.8279e-01)	Acc@1  90.00 ( 90.73)	Acc@5  99.00 ( 99.55)
Test: [ 20/100]	Time  0.008 ( 0.019)	Loss 5.2490e-01 (4.0860e-01)	Acc@1  87.00 ( 90.05)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.019 ( 0.018)	Loss 4.7363e-01 (4.1302e-01)	Acc@1  89.00 ( 90.29)	Acc@5  99.00 ( 99.61)
Test: [ 40/100]	Time  0.011 ( 0.017)	Loss 5.4248e-01 (4.3400e-01)	Acc@1  89.00 ( 89.93)	Acc@5 100.00 ( 99.61)
Test: [ 50/100]	Time  0.008 ( 0.016)	Loss 1.5906e-01 (4.1582e-01)	Acc@1  95.00 ( 90.24)	Acc@5 100.00 ( 99.59)
Test: [ 60/100]	Time  0.016 ( 0.016)	Loss 3.0518e-01 (4.1745e-01)	Acc@1  93.00 ( 90.16)	Acc@5 100.00 ( 99.61)
Test: [ 70/100]	Time  0.019 ( 0.015)	Loss 6.8896e-01 (4.1276e-01)	Acc@1  87.00 ( 90.32)	Acc@5  99.00 ( 99.59)
Test: [ 80/100]	Time  0.013 ( 0.015)	Loss 1.9861e-01 (4.0671e-01)	Acc@1  95.00 ( 90.41)	Acc@5 100.00 ( 99.60)
Test: [ 90/100]	Time  0.009 ( 0.015)	Loss 2.6831e-01 (4.0191e-01)	Acc@1  91.00 ( 90.43)	Acc@5 100.00 ( 99.65)
 * Acc@1 90.460 Acc@5 99.680
### epoch[85] execution time: 11.256870031356812
EPOCH 86
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [86][  0/391]	Time  0.230 ( 0.230)	Data  0.209 ( 0.209)	Loss 1.2154e-02 (1.2154e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [86][ 10/391]	Time  0.035 ( 0.043)	Data  0.001 ( 0.021)	Loss 3.0594e-02 (2.3075e-02)	Acc@1  99.22 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [86][ 20/391]	Time  0.018 ( 0.034)	Data  0.001 ( 0.012)	Loss 3.1204e-02 (2.0710e-02)	Acc@1  98.44 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [86][ 30/391]	Time  0.018 ( 0.030)	Data  0.001 ( 0.009)	Loss 2.9510e-02 (2.5198e-02)	Acc@1  99.22 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [86][ 40/391]	Time  0.037 ( 0.029)	Data  0.002 ( 0.007)	Loss 6.3515e-03 (2.5240e-02)	Acc@1 100.00 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [86][ 50/391]	Time  0.019 ( 0.028)	Data  0.002 ( 0.006)	Loss 9.6970e-03 (2.5902e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [86][ 60/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.005)	Loss 3.8269e-02 (2.5970e-02)	Acc@1  97.66 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [86][ 70/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.005)	Loss 5.7678e-03 (2.5803e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [86][ 80/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.1772e-02 (2.5232e-02)	Acc@1 100.00 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [86][ 90/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.6739e-02 (2.4157e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [86][100/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.004)	Loss 2.4094e-02 (2.4647e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [86][110/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.004)	Loss 9.4376e-03 (2.5196e-02)	Acc@1 100.00 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [86][120/391]	Time  0.017 ( 0.025)	Data  0.001 ( 0.004)	Loss 8.9951e-03 (2.5285e-02)	Acc@1 100.00 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [86][130/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.6861e-02 (2.5332e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [86][140/391]	Time  0.029 ( 0.025)	Data  0.000 ( 0.003)	Loss 1.9577e-02 (2.5050e-02)	Acc@1  99.22 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [86][150/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.1206e-01 (2.5686e-02)	Acc@1  96.88 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [86][160/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.4912e-02 (2.5875e-02)	Acc@1  99.22 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [86][170/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.4937e-03 (2.5657e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [86][180/391]	Time  0.018 ( 0.025)	Data  0.002 ( 0.003)	Loss 3.7170e-02 (2.5853e-02)	Acc@1  99.22 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [86][190/391]	Time  0.037 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.0167e-02 (2.6081e-02)	Acc@1  99.22 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [86][200/391]	Time  0.017 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.5422e-03 (2.5723e-02)	Acc@1 100.00 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [86][210/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 1.5373e-02 (2.5448e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 ( 99.99)
Epoch: [86][220/391]	Time  0.018 ( 0.025)	Data  0.002 ( 0.003)	Loss 2.3071e-02 (2.6048e-02)	Acc@1  99.22 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [86][230/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.3518e-02 (2.5877e-02)	Acc@1  96.88 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [86][240/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.4992e-02 (2.5828e-02)	Acc@1 100.00 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [86][250/391]	Time  0.032 ( 0.025)	Data  0.000 ( 0.003)	Loss 1.6418e-02 (2.5824e-02)	Acc@1  99.22 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [86][260/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.0948e-02 (2.5984e-02)	Acc@1  99.22 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [86][270/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 6.7673e-03 (2.5560e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 ( 99.99)
Epoch: [86][280/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 1.8814e-02 (2.5650e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 ( 99.99)
Epoch: [86][290/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.9493e-03 (2.6470e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 ( 99.99)
Epoch: [86][300/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.8341e-02 (2.6565e-02)	Acc@1 100.00 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [86][310/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.1749e-02 (2.6437e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 ( 99.99)
Epoch: [86][320/391]	Time  0.026 ( 0.025)	Data  0.004 ( 0.003)	Loss 4.9927e-02 (2.6859e-02)	Acc@1  97.66 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [86][330/391]	Time  0.041 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.4084e-02 (2.6645e-02)	Acc@1 100.00 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [86][340/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.003)	Loss 3.3966e-02 (2.6836e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [86][350/391]	Time  0.020 ( 0.024)	Data  0.002 ( 0.002)	Loss 3.6774e-02 (2.6759e-02)	Acc@1  98.44 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [86][360/391]	Time  0.027 ( 0.024)	Data  0.002 ( 0.002)	Loss 7.9803e-03 (2.6489e-02)	Acc@1 100.00 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [86][370/391]	Time  0.023 ( 0.024)	Data  0.001 ( 0.002)	Loss 4.2816e-02 (2.6325e-02)	Acc@1  98.44 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [86][380/391]	Time  0.039 ( 0.024)	Data  0.004 ( 0.002)	Loss 2.5696e-02 (2.6304e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 ( 99.99)
Epoch: [86][390/391]	Time  0.015 ( 0.024)	Data  0.000 ( 0.002)	Loss 9.9335e-03 (2.6387e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 ( 99.99)
## e[86] optimizer.zero_grad (sum) time: 0.10522079467773438
## e[86]       loss.backward (sum) time: 2.187105178833008
## e[86]      optimizer.step (sum) time: 0.8987421989440918
## epoch[86] training(only) time: 9.644269466400146
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 3.0933e-01 (3.0933e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.018 ( 0.027)	Loss 4.9707e-01 (3.7221e-01)	Acc@1  90.00 ( 90.82)	Acc@5  99.00 ( 99.45)
Test: [ 20/100]	Time  0.015 ( 0.020)	Loss 5.2393e-01 (4.0283e-01)	Acc@1  88.00 ( 90.19)	Acc@5 100.00 ( 99.52)
Test: [ 30/100]	Time  0.017 ( 0.018)	Loss 4.8193e-01 (4.0999e-01)	Acc@1  89.00 ( 90.42)	Acc@5  99.00 ( 99.52)
Test: [ 40/100]	Time  0.008 ( 0.016)	Loss 5.5908e-01 (4.3256e-01)	Acc@1  88.00 ( 89.93)	Acc@5  99.00 ( 99.51)
Test: [ 50/100]	Time  0.012 ( 0.016)	Loss 1.7346e-01 (4.1361e-01)	Acc@1  95.00 ( 90.29)	Acc@5 100.00 ( 99.51)
Test: [ 60/100]	Time  0.011 ( 0.015)	Loss 2.9517e-01 (4.1510e-01)	Acc@1  93.00 ( 90.26)	Acc@5 100.00 ( 99.54)
Test: [ 70/100]	Time  0.017 ( 0.015)	Loss 6.6699e-01 (4.1025e-01)	Acc@1  88.00 ( 90.41)	Acc@5  99.00 ( 99.54)
Test: [ 80/100]	Time  0.013 ( 0.015)	Loss 2.0105e-01 (4.0445e-01)	Acc@1  95.00 ( 90.49)	Acc@5 100.00 ( 99.56)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 2.7808e-01 (4.0017e-01)	Acc@1  91.00 ( 90.48)	Acc@5 100.00 ( 99.60)
 * Acc@1 90.480 Acc@5 99.630
### epoch[86] execution time: 11.197043895721436
EPOCH 87
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [87][  0/391]	Time  0.191 ( 0.191)	Data  0.170 ( 0.170)	Loss 3.1860e-02 (3.1860e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [87][ 10/391]	Time  0.028 ( 0.039)	Data  0.001 ( 0.017)	Loss 1.2527e-02 (3.9088e-02)	Acc@1  99.22 ( 98.79)	Acc@5 100.00 (100.00)
Epoch: [87][ 20/391]	Time  0.018 ( 0.032)	Data  0.001 ( 0.010)	Loss 5.5511e-02 (3.9073e-02)	Acc@1  98.44 ( 98.74)	Acc@5 100.00 ( 99.96)
Epoch: [87][ 30/391]	Time  0.018 ( 0.029)	Data  0.001 ( 0.007)	Loss 7.3357e-03 (3.6573e-02)	Acc@1 100.00 ( 98.84)	Acc@5 100.00 ( 99.97)
Epoch: [87][ 40/391]	Time  0.018 ( 0.028)	Data  0.001 ( 0.006)	Loss 2.5375e-02 (3.3363e-02)	Acc@1  99.22 ( 98.91)	Acc@5 100.00 ( 99.98)
Epoch: [87][ 50/391]	Time  0.027 ( 0.027)	Data  0.001 ( 0.005)	Loss 3.9429e-02 (3.1114e-02)	Acc@1  98.44 ( 98.99)	Acc@5 100.00 ( 99.98)
Epoch: [87][ 60/391]	Time  0.017 ( 0.026)	Data  0.001 ( 0.005)	Loss 2.7695e-02 (2.8707e-02)	Acc@1  98.44 ( 99.09)	Acc@5 100.00 ( 99.99)
Epoch: [87][ 70/391]	Time  0.019 ( 0.026)	Data  0.002 ( 0.004)	Loss 3.9978e-03 (2.7866e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [87][ 80/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.004)	Loss 2.0340e-02 (2.7526e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 ( 99.99)
Epoch: [87][ 90/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 9.6970e-03 (2.6680e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 ( 99.99)
Epoch: [87][100/391]	Time  0.036 ( 0.025)	Data  0.004 ( 0.003)	Loss 2.1454e-02 (2.6441e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 ( 99.99)
Epoch: [87][110/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 1.6693e-02 (2.6059e-02)	Acc@1  98.44 ( 99.18)	Acc@5 100.00 ( 99.99)
Epoch: [87][120/391]	Time  0.020 ( 0.025)	Data  0.002 ( 0.003)	Loss 3.3894e-03 (2.5646e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 ( 99.99)
Epoch: [87][130/391]	Time  0.020 ( 0.025)	Data  0.002 ( 0.003)	Loss 2.0187e-02 (2.5386e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 ( 99.99)
Epoch: [87][140/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 5.6488e-02 (2.6004e-02)	Acc@1  96.88 ( 99.17)	Acc@5 100.00 ( 99.99)
Epoch: [87][150/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 3.7292e-02 (2.5792e-02)	Acc@1  98.44 ( 99.18)	Acc@5 100.00 ( 99.99)
Epoch: [87][160/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.4392e-02 (2.6129e-02)	Acc@1  98.44 ( 99.18)	Acc@5  99.22 ( 99.99)
Epoch: [87][170/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.7078e-02 (2.6169e-02)	Acc@1  97.66 ( 99.16)	Acc@5 100.00 ( 99.99)
Epoch: [87][180/391]	Time  0.027 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.4643e-02 (2.6827e-02)	Acc@1  98.44 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [87][190/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.7699e-02 (2.6391e-02)	Acc@1  97.66 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [87][200/391]	Time  0.045 ( 0.025)	Data  0.006 ( 0.003)	Loss 1.1681e-02 (2.6861e-02)	Acc@1 100.00 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [87][210/391]	Time  0.023 ( 0.025)	Data  0.002 ( 0.003)	Loss 8.0811e-02 (2.7004e-02)	Acc@1  98.44 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [87][220/391]	Time  0.018 ( 0.025)	Data  0.002 ( 0.003)	Loss 6.6910e-03 (2.6836e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [87][230/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.6602e-02 (2.7042e-02)	Acc@1  99.22 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [87][240/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.003)	Loss 3.2593e-02 (2.6977e-02)	Acc@1  99.22 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [87][250/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.003)	Loss 3.6011e-02 (2.6721e-02)	Acc@1  98.44 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [87][260/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.0991e-02 (2.6697e-02)	Acc@1  98.44 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [87][270/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.7136e-02 (2.6748e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [87][280/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.2306e-02 (2.6487e-02)	Acc@1 100.00 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [87][290/391]	Time  0.021 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.5594e-02 (2.6376e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [87][300/391]	Time  0.036 ( 0.024)	Data  0.004 ( 0.002)	Loss 3.7766e-03 (2.6073e-02)	Acc@1 100.00 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [87][310/391]	Time  0.027 ( 0.024)	Data  0.003 ( 0.002)	Loss 1.7151e-02 (2.6255e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [87][320/391]	Time  0.024 ( 0.024)	Data  0.003 ( 0.002)	Loss 3.5736e-02 (2.6359e-02)	Acc@1  98.44 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [87][330/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.1713e-02 (2.6282e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [87][340/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 8.0109e-03 (2.6066e-02)	Acc@1 100.00 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [87][350/391]	Time  0.038 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.4893e-02 (2.6348e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [87][360/391]	Time  0.028 ( 0.024)	Data  0.000 ( 0.002)	Loss 4.8828e-02 (2.6664e-02)	Acc@1  98.44 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [87][370/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.8275e-02 (2.6510e-02)	Acc@1  98.44 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [87][380/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 5.4108e-02 (2.6792e-02)	Acc@1  98.44 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [87][390/391]	Time  0.016 ( 0.024)	Data  0.000 ( 0.002)	Loss 3.4058e-02 (2.6887e-02)	Acc@1  98.75 ( 99.12)	Acc@5 100.00 ( 99.99)
## e[87] optimizer.zero_grad (sum) time: 0.10523509979248047
## e[87]       loss.backward (sum) time: 2.172651529312134
## e[87]      optimizer.step (sum) time: 0.896557092666626
## epoch[87] training(only) time: 9.607670783996582
# Switched to evaluate mode...
Test: [  0/100]	Time  0.165 ( 0.165)	Loss 3.0713e-01 (3.0713e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.010 ( 0.028)	Loss 4.8657e-01 (3.7608e-01)	Acc@1  90.00 ( 90.82)	Acc@5 100.00 ( 99.55)
Test: [ 20/100]	Time  0.018 ( 0.021)	Loss 5.3467e-01 (4.0583e-01)	Acc@1  88.00 ( 90.19)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.015 ( 0.018)	Loss 4.7607e-01 (4.1269e-01)	Acc@1  89.00 ( 90.45)	Acc@5  99.00 ( 99.58)
Test: [ 40/100]	Time  0.008 ( 0.017)	Loss 5.6152e-01 (4.3522e-01)	Acc@1  89.00 ( 90.02)	Acc@5 100.00 ( 99.59)
Test: [ 50/100]	Time  0.008 ( 0.016)	Loss 1.6028e-01 (4.1582e-01)	Acc@1  96.00 ( 90.35)	Acc@5 100.00 ( 99.55)
Test: [ 60/100]	Time  0.023 ( 0.016)	Loss 2.9346e-01 (4.1592e-01)	Acc@1  93.00 ( 90.36)	Acc@5 100.00 ( 99.57)
Test: [ 70/100]	Time  0.020 ( 0.015)	Loss 6.8555e-01 (4.1145e-01)	Acc@1  88.00 ( 90.49)	Acc@5  99.00 ( 99.56)
Test: [ 80/100]	Time  0.008 ( 0.015)	Loss 1.9104e-01 (4.0625e-01)	Acc@1  95.00 ( 90.57)	Acc@5 100.00 ( 99.58)
Test: [ 90/100]	Time  0.018 ( 0.015)	Loss 2.7368e-01 (4.0185e-01)	Acc@1  92.00 ( 90.57)	Acc@5 100.00 ( 99.63)
 * Acc@1 90.560 Acc@5 99.660
### epoch[87] execution time: 11.201052904129028
EPOCH 88
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [88][  0/391]	Time  0.192 ( 0.192)	Data  0.167 ( 0.167)	Loss 6.7444e-02 (6.7444e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [88][ 10/391]	Time  0.018 ( 0.038)	Data  0.001 ( 0.017)	Loss 8.4763e-03 (2.2233e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [88][ 20/391]	Time  0.018 ( 0.031)	Data  0.001 ( 0.010)	Loss 2.4185e-02 (2.9045e-02)	Acc@1  98.44 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [88][ 30/391]	Time  0.039 ( 0.029)	Data  0.005 ( 0.007)	Loss 3.3760e-03 (2.5427e-02)	Acc@1 100.00 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [88][ 40/391]	Time  0.027 ( 0.028)	Data  0.000 ( 0.006)	Loss 1.2802e-02 (2.5956e-02)	Acc@1 100.00 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [88][ 50/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.005)	Loss 1.4137e-02 (2.7415e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [88][ 60/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.005)	Loss 1.7990e-02 (2.7565e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [88][ 70/391]	Time  0.033 ( 0.026)	Data  0.005 ( 0.004)	Loss 6.5308e-02 (2.7564e-02)	Acc@1  98.44 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [88][ 80/391]	Time  0.034 ( 0.026)	Data  0.002 ( 0.004)	Loss 3.5980e-02 (2.8215e-02)	Acc@1  98.44 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [88][ 90/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 4.3182e-02 (2.7111e-02)	Acc@1  98.44 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [88][100/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3809e-02 (2.6661e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [88][110/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.0746e-03 (2.6362e-02)	Acc@1 100.00 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [88][120/391]	Time  0.035 ( 0.025)	Data  0.005 ( 0.003)	Loss 1.1131e-02 (2.6089e-02)	Acc@1 100.00 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [88][130/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.1696e-02 (2.5725e-02)	Acc@1 100.00 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [88][140/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.6083e-02 (2.5983e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [88][150/391]	Time  0.035 ( 0.025)	Data  0.001 ( 0.003)	Loss 9.1476e-03 (2.5530e-02)	Acc@1 100.00 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [88][160/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.6098e-02 (2.5110e-02)	Acc@1  99.22 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [88][170/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.0096e-02 (2.5440e-02)	Acc@1  99.22 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [88][180/391]	Time  0.036 ( 0.025)	Data  0.000 ( 0.003)	Loss 1.0034e-01 (2.6132e-02)	Acc@1  95.31 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [88][190/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.7695e-02 (2.5941e-02)	Acc@1  98.44 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [88][200/391]	Time  0.027 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.1493e-03 (2.6423e-02)	Acc@1 100.00 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [88][210/391]	Time  0.038 ( 0.025)	Data  0.005 ( 0.003)	Loss 4.1656e-02 (2.6253e-02)	Acc@1  98.44 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [88][220/391]	Time  0.029 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.2856e-02 (2.6287e-02)	Acc@1  99.22 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [88][230/391]	Time  0.037 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.4246e-02 (2.6056e-02)	Acc@1  98.44 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [88][240/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.2410e-02 (2.6163e-02)	Acc@1  99.22 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [88][250/391]	Time  0.036 ( 0.025)	Data  0.002 ( 0.002)	Loss 1.6373e-02 (2.6064e-02)	Acc@1  99.22 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [88][260/391]	Time  0.026 ( 0.025)	Data  0.002 ( 0.002)	Loss 2.5711e-02 (2.5988e-02)	Acc@1  98.44 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [88][270/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.0798e-02 (2.5869e-02)	Acc@1  99.22 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [88][280/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.002)	Loss 9.4223e-03 (2.5740e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [88][290/391]	Time  0.036 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.1433e-02 (2.5535e-02)	Acc@1  98.44 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [88][300/391]	Time  0.035 ( 0.025)	Data  0.001 ( 0.002)	Loss 6.1150e-03 (2.5682e-02)	Acc@1 100.00 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [88][310/391]	Time  0.026 ( 0.025)	Data  0.002 ( 0.002)	Loss 2.5925e-02 (2.5447e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [88][320/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.0724e-01 (2.5605e-02)	Acc@1  97.66 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [88][330/391]	Time  0.028 ( 0.025)	Data  0.001 ( 0.002)	Loss 6.2134e-02 (2.5449e-02)	Acc@1  98.44 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [88][340/391]	Time  0.035 ( 0.024)	Data  0.002 ( 0.002)	Loss 7.5378e-03 (2.5451e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [88][350/391]	Time  0.018 ( 0.024)	Data  0.002 ( 0.002)	Loss 3.6255e-02 (2.5407e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [88][360/391]	Time  0.023 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.3580e-02 (2.5578e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [88][370/391]	Time  0.016 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.3550e-02 (2.5610e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [88][380/391]	Time  0.019 ( 0.024)	Data  0.002 ( 0.002)	Loss 9.7733e-03 (2.5622e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [88][390/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.1149e-02 (2.5500e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 (100.00)
## e[88] optimizer.zero_grad (sum) time: 0.10659408569335938
## e[88]       loss.backward (sum) time: 2.1631596088409424
## e[88]      optimizer.step (sum) time: 0.8960003852844238
## epoch[88] training(only) time: 9.638262033462524
# Switched to evaluate mode...
Test: [  0/100]	Time  0.159 ( 0.159)	Loss 3.1177e-01 (3.1177e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.008 ( 0.026)	Loss 4.9731e-01 (3.7808e-01)	Acc@1  90.00 ( 90.82)	Acc@5  99.00 ( 99.45)
Test: [ 20/100]	Time  0.013 ( 0.020)	Loss 5.2979e-01 (4.0886e-01)	Acc@1  88.00 ( 90.19)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.009 ( 0.017)	Loss 4.6606e-01 (4.1327e-01)	Acc@1  89.00 ( 90.42)	Acc@5  99.00 ( 99.55)
Test: [ 40/100]	Time  0.017 ( 0.017)	Loss 5.5225e-01 (4.3560e-01)	Acc@1  88.00 ( 90.05)	Acc@5 100.00 ( 99.56)
Test: [ 50/100]	Time  0.008 ( 0.016)	Loss 1.6321e-01 (4.1560e-01)	Acc@1  95.00 ( 90.35)	Acc@5 100.00 ( 99.53)
Test: [ 60/100]	Time  0.008 ( 0.015)	Loss 2.8540e-01 (4.1522e-01)	Acc@1  93.00 ( 90.33)	Acc@5 100.00 ( 99.54)
Test: [ 70/100]	Time  0.009 ( 0.015)	Loss 6.9629e-01 (4.0999e-01)	Acc@1  87.00 ( 90.46)	Acc@5  99.00 ( 99.54)
Test: [ 80/100]	Time  0.021 ( 0.015)	Loss 1.7590e-01 (4.0374e-01)	Acc@1  96.00 ( 90.59)	Acc@5 100.00 ( 99.56)
Test: [ 90/100]	Time  0.017 ( 0.015)	Loss 2.7881e-01 (3.9977e-01)	Acc@1  91.00 ( 90.55)	Acc@5 100.00 ( 99.60)
 * Acc@1 90.530 Acc@5 99.630
### epoch[88] execution time: 11.214136123657227
EPOCH 89
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [89][  0/391]	Time  0.250 ( 0.250)	Data  0.228 ( 0.228)	Loss 3.4668e-02 (3.4668e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [89][ 10/391]	Time  0.018 ( 0.044)	Data  0.001 ( 0.022)	Loss 2.4078e-02 (2.4132e-02)	Acc@1  98.44 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [89][ 20/391]	Time  0.021 ( 0.035)	Data  0.001 ( 0.012)	Loss 1.5854e-02 (2.5580e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [89][ 30/391]	Time  0.018 ( 0.031)	Data  0.001 ( 0.009)	Loss 8.1482e-03 (2.9345e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 ( 99.97)
Epoch: [89][ 40/391]	Time  0.035 ( 0.030)	Data  0.001 ( 0.007)	Loss 1.0590e-02 (2.9337e-02)	Acc@1 100.00 ( 99.03)	Acc@5 100.00 ( 99.98)
Epoch: [89][ 50/391]	Time  0.026 ( 0.029)	Data  0.005 ( 0.006)	Loss 1.6647e-02 (2.8945e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 ( 99.98)
Epoch: [89][ 60/391]	Time  0.027 ( 0.028)	Data  0.002 ( 0.005)	Loss 1.6998e-02 (2.7475e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 ( 99.99)
Epoch: [89][ 70/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.005)	Loss 1.0033e-02 (2.6359e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 ( 99.99)
Epoch: [89][ 80/391]	Time  0.017 ( 0.027)	Data  0.000 ( 0.005)	Loss 4.7417e-03 (2.6248e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 ( 99.99)
Epoch: [89][ 90/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.7075e-02 (2.5672e-02)	Acc@1  99.22 ( 99.10)	Acc@5 100.00 ( 99.99)
Epoch: [89][100/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.8295e-02 (2.5263e-02)	Acc@1 100.00 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [89][110/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.004)	Loss 2.0355e-02 (2.4486e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 ( 99.99)
Epoch: [89][120/391]	Time  0.046 ( 0.026)	Data  0.005 ( 0.004)	Loss 9.2712e-02 (2.5446e-02)	Acc@1  99.22 ( 99.15)	Acc@5  99.22 ( 99.99)
Epoch: [89][130/391]	Time  0.038 ( 0.026)	Data  0.001 ( 0.004)	Loss 7.5623e-02 (2.6340e-02)	Acc@1  98.44 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [89][140/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.8143e-02 (2.5537e-02)	Acc@1  99.22 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [89][150/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.6365e-03 (2.5927e-02)	Acc@1 100.00 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [89][160/391]	Time  0.041 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.4119e-02 (2.5764e-02)	Acc@1  99.22 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [89][170/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.7150e-03 (2.5503e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [89][180/391]	Time  0.038 ( 0.025)	Data  0.002 ( 0.003)	Loss 1.3245e-02 (2.5017e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 ( 99.99)
Epoch: [89][190/391]	Time  0.030 ( 0.025)	Data  0.000 ( 0.003)	Loss 1.8738e-02 (2.5343e-02)	Acc@1  99.22 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [89][200/391]	Time  0.030 ( 0.025)	Data  0.000 ( 0.003)	Loss 1.5808e-02 (2.5552e-02)	Acc@1  99.22 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [89][210/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.003)	Loss 3.4912e-02 (2.5871e-02)	Acc@1  99.22 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [89][220/391]	Time  0.033 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.7166e-02 (2.6390e-02)	Acc@1  99.22 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [89][230/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.0477e-02 (2.6476e-02)	Acc@1  99.22 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [89][240/391]	Time  0.033 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.7433e-03 (2.6353e-02)	Acc@1 100.00 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [89][250/391]	Time  0.035 ( 0.025)	Data  0.004 ( 0.003)	Loss 4.3854e-02 (2.6326e-02)	Acc@1  98.44 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [89][260/391]	Time  0.028 ( 0.025)	Data  0.001 ( 0.003)	Loss 9.8038e-03 (2.5878e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 ( 99.99)
Epoch: [89][270/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.7810e-02 (2.6151e-02)	Acc@1  98.44 ( 99.16)	Acc@5 100.00 ( 99.99)
Epoch: [89][280/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.6434e-02 (2.5832e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 ( 99.99)
Epoch: [89][290/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.0010e-02 (2.6012e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 ( 99.99)
Epoch: [89][300/391]	Time  0.029 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.4678e-02 (2.6248e-02)	Acc@1  98.44 ( 99.17)	Acc@5 100.00 ( 99.99)
Epoch: [89][310/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.5085e-02 (2.6133e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 ( 99.99)
Epoch: [89][320/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.002)	Loss 7.9727e-03 (2.6065e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 ( 99.99)
Epoch: [89][330/391]	Time  0.032 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.1921e-02 (2.6146e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 ( 99.99)
Epoch: [89][340/391]	Time  0.027 ( 0.025)	Data  0.005 ( 0.002)	Loss 2.2430e-02 (2.6520e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 ( 99.99)
Epoch: [89][350/391]	Time  0.031 ( 0.025)	Data  0.002 ( 0.002)	Loss 4.3945e-02 (2.6555e-02)	Acc@1  98.44 ( 99.16)	Acc@5 100.00 ( 99.99)
Epoch: [89][360/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.8961e-02 (2.6965e-02)	Acc@1  98.44 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [89][370/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.3447e-02 (2.7094e-02)	Acc@1  98.44 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [89][380/391]	Time  0.017 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.4933e-02 (2.7042e-02)	Acc@1  99.22 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [89][390/391]	Time  0.015 ( 0.024)	Data  0.000 ( 0.002)	Loss 7.4219e-02 (2.7138e-02)	Acc@1  96.25 ( 99.13)	Acc@5 100.00 ( 99.99)
## e[89] optimizer.zero_grad (sum) time: 0.10354781150817871
## e[89]       loss.backward (sum) time: 2.137946605682373
## e[89]      optimizer.step (sum) time: 0.8772501945495605
## epoch[89] training(only) time: 9.692821264266968
# Switched to evaluate mode...
Test: [  0/100]	Time  0.157 ( 0.157)	Loss 3.0493e-01 (3.0493e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.008 ( 0.026)	Loss 4.9072e-01 (3.7460e-01)	Acc@1  90.00 ( 91.00)	Acc@5  99.00 ( 99.55)
Test: [ 20/100]	Time  0.025 ( 0.020)	Loss 5.2686e-01 (4.0541e-01)	Acc@1  88.00 ( 90.24)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.014 ( 0.018)	Loss 4.7021e-01 (4.1045e-01)	Acc@1  89.00 ( 90.42)	Acc@5  99.00 ( 99.65)
Test: [ 40/100]	Time  0.017 ( 0.017)	Loss 5.3906e-01 (4.3093e-01)	Acc@1  89.00 ( 90.10)	Acc@5 100.00 ( 99.63)
Test: [ 50/100]	Time  0.013 ( 0.016)	Loss 1.5283e-01 (4.1221e-01)	Acc@1  96.00 ( 90.35)	Acc@5 100.00 ( 99.59)
Test: [ 60/100]	Time  0.010 ( 0.016)	Loss 2.8149e-01 (4.1215e-01)	Acc@1  93.00 ( 90.31)	Acc@5 100.00 ( 99.61)
Test: [ 70/100]	Time  0.008 ( 0.015)	Loss 6.9873e-01 (4.0736e-01)	Acc@1  88.00 ( 90.46)	Acc@5  99.00 ( 99.59)
Test: [ 80/100]	Time  0.008 ( 0.015)	Loss 2.0093e-01 (4.0180e-01)	Acc@1  94.00 ( 90.57)	Acc@5 100.00 ( 99.60)
Test: [ 90/100]	Time  0.022 ( 0.015)	Loss 2.6978e-01 (3.9754e-01)	Acc@1  91.00 ( 90.58)	Acc@5 100.00 ( 99.65)
 * Acc@1 90.560 Acc@5 99.680
### epoch[89] execution time: 11.2425696849823
### Training complete:
#### total training(only) time: 873.278867483139
##### Total run time: 1019.6786279678345
