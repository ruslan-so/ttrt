# Model: resnext50
# Dataset: cifardecem
# Batch size: 128
# Freezeout: False
# Low precision: True
# Epochs: 90
# Initial learning rate: 0.1
# module_name: models_cifardecem.resnext
<function resnext50 at 0x7f7badf16f28>
# model requested: 'resnext50'
# printing out the model
ResNext(
  (conv1): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (conv2): Sequential(
    (0): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
    (2): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
  )
  (conv3): Sequential(
    (0): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
    (2): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
    (3): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
  )
  (conv4): Sequential(
    (0): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
    (2): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
    (3): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
    (4): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
    (5): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
  )
  (conv5): Sequential(
    (0): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
    (2): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
  )
  (avg): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=2048, out_features=10, bias=True)
)
# model is low precision
# Model: resnext50
# Dataset: cifardecem
# Freezeout: False
# Low precision: True
# Initial learning rate: 0.1
# Criterion: CrossEntropyLoss()
# Optimizer: SGD
#	lr 0.1
#	momentum 0.9
#	dampening 0
#	weight_decay 0.0001
#	nesterov False
CIFAR10 loading and normalization
==> Preparing data..
# CIFAR10 / low precision
Files already downloaded and verified
Files already downloaded and verified
#--> Training data: <--#
#-->>
EPOCH 0
# current learning rate: 0.1
# Switched to train mode...
Epoch: [0][  0/391]	Time  4.389 ( 4.389)	Data  0.123 ( 0.123)	Loss 2.4395e+00 (2.4395e+00)	Acc@1  10.94 ( 10.94)	Acc@5  46.88 ( 46.88)
Epoch: [0][ 10/391]	Time  0.103 ( 0.492)	Data  0.001 ( 0.012)	Loss 1.7812e+01 (1.9203e+01)	Acc@1  15.62 ( 10.65)	Acc@5  60.94 ( 50.21)
Epoch: [0][ 20/391]	Time  0.100 ( 0.307)	Data  0.001 ( 0.008)	Loss 5.4102e+00 (1.4379e+01)	Acc@1   6.25 ( 10.49)	Acc@5  56.25 ( 51.04)
Epoch: [0][ 30/391]	Time  0.102 ( 0.241)	Data  0.001 ( 0.007)	Loss 3.8086e+00 (1.1011e+01)	Acc@1  10.16 ( 10.46)	Acc@5  56.25 ( 50.45)
Epoch: [0][ 40/391]	Time  0.100 ( 0.208)	Data  0.001 ( 0.006)	Loss 3.8320e+00 (9.8869e+00)	Acc@1  14.06 ( 10.75)	Acc@5  46.88 ( 50.40)
Epoch: [0][ 50/391]	Time  0.102 ( 0.187)	Data  0.001 ( 0.006)	Loss 2.4277e+00 (8.5005e+00)	Acc@1  11.72 ( 10.68)	Acc@5  55.47 ( 50.69)
Epoch: [0][ 60/391]	Time  0.101 ( 0.174)	Data  0.001 ( 0.005)	Loss 2.4062e+00 (7.5330e+00)	Acc@1  10.16 ( 10.87)	Acc@5  53.91 ( 51.19)
Epoch: [0][ 70/391]	Time  0.102 ( 0.164)	Data  0.001 ( 0.005)	Loss 3.0039e+00 (6.8945e+00)	Acc@1  14.06 ( 10.96)	Acc@5  57.81 ( 51.19)
Epoch: [0][ 80/391]	Time  0.101 ( 0.156)	Data  0.001 ( 0.005)	Loss 2.3457e+00 (6.3487e+00)	Acc@1   8.59 ( 11.06)	Acc@5  51.56 ( 51.58)
Epoch: [0][ 90/391]	Time  0.101 ( 0.151)	Data  0.001 ( 0.005)	Loss 2.3125e+00 (5.9177e+00)	Acc@1  12.50 ( 11.20)	Acc@5  60.94 ( 51.93)
Epoch: [0][100/391]	Time  0.101 ( 0.146)	Data  0.001 ( 0.005)	Loss 2.3164e+00 (5.5846e+00)	Acc@1  18.75 ( 11.70)	Acc@5  64.84 ( 53.16)
Epoch: [0][110/391]	Time  0.097 ( 0.142)	Data  0.001 ( 0.005)	Loss 2.1895e+00 (5.2905e+00)	Acc@1  17.97 ( 12.18)	Acc@5  60.94 ( 54.12)
Epoch: [0][120/391]	Time  0.105 ( 0.139)	Data  0.001 ( 0.005)	Loss 2.2109e+00 (5.0431e+00)	Acc@1  14.84 ( 12.60)	Acc@5  60.94 ( 55.20)
Epoch: [0][130/391]	Time  0.102 ( 0.136)	Data  0.001 ( 0.005)	Loss 2.2031e+00 (4.8325e+00)	Acc@1  15.62 ( 13.07)	Acc@5  71.88 ( 56.53)
Epoch: [0][140/391]	Time  0.106 ( 0.134)	Data  0.001 ( 0.005)	Loss 2.1230e+00 (4.6426e+00)	Acc@1  18.75 ( 13.45)	Acc@5  71.09 ( 57.72)
Epoch: [0][150/391]	Time  0.104 ( 0.132)	Data  0.001 ( 0.004)	Loss 2.1250e+00 (4.4811e+00)	Acc@1  28.12 ( 13.91)	Acc@5  71.09 ( 58.61)
Epoch: [0][160/391]	Time  0.104 ( 0.130)	Data  0.001 ( 0.004)	Loss 2.0371e+00 (4.3343e+00)	Acc@1  22.66 ( 14.34)	Acc@5  81.25 ( 59.66)
Epoch: [0][170/391]	Time  0.105 ( 0.129)	Data  0.001 ( 0.004)	Loss 2.0938e+00 (4.2169e+00)	Acc@1  21.88 ( 14.78)	Acc@5  74.22 ( 60.64)
Epoch: [0][180/391]	Time  0.102 ( 0.127)	Data  0.001 ( 0.004)	Loss 2.2578e+00 (4.1041e+00)	Acc@1  20.31 ( 15.07)	Acc@5  71.88 ( 61.47)
Epoch: [0][190/391]	Time  0.103 ( 0.126)	Data  0.001 ( 0.004)	Loss 2.2969e+00 (4.0092e+00)	Acc@1  22.66 ( 15.37)	Acc@5  83.59 ( 62.29)
Epoch: [0][200/391]	Time  0.107 ( 0.125)	Data  0.001 ( 0.004)	Loss 2.2500e+00 (3.9128e+00)	Acc@1  20.31 ( 15.77)	Acc@5  75.78 ( 63.08)
Epoch: [0][210/391]	Time  0.107 ( 0.124)	Data  0.001 ( 0.004)	Loss 2.0684e+00 (3.8257e+00)	Acc@1  26.56 ( 16.14)	Acc@5  78.91 ( 63.77)
Epoch: [0][220/391]	Time  0.106 ( 0.123)	Data  0.001 ( 0.004)	Loss 2.0996e+00 (3.7458e+00)	Acc@1  21.09 ( 16.41)	Acc@5  79.69 ( 64.40)
Epoch: [0][230/391]	Time  0.105 ( 0.123)	Data  0.001 ( 0.004)	Loss 1.9795e+00 (3.6779e+00)	Acc@1  27.34 ( 16.65)	Acc@5  83.59 ( 64.96)
Epoch: [0][240/391]	Time  0.102 ( 0.122)	Data  0.001 ( 0.004)	Loss 2.0859e+00 (3.6102e+00)	Acc@1  18.75 ( 16.85)	Acc@5  72.66 ( 65.51)
Epoch: [0][250/391]	Time  0.102 ( 0.121)	Data  0.001 ( 0.004)	Loss 3.1387e+00 (3.5510e+00)	Acc@1  28.12 ( 17.08)	Acc@5  82.81 ( 66.04)
Epoch: [0][260/391]	Time  0.105 ( 0.121)	Data  0.001 ( 0.004)	Loss 1.9541e+00 (3.4911e+00)	Acc@1  25.00 ( 17.38)	Acc@5  78.91 ( 66.55)
Epoch: [0][270/391]	Time  0.100 ( 0.120)	Data  0.001 ( 0.004)	Loss 2.0039e+00 (3.4361e+00)	Acc@1  21.09 ( 17.54)	Acc@5  78.12 ( 67.03)
Epoch: [0][280/391]	Time  0.102 ( 0.120)	Data  0.001 ( 0.004)	Loss 2.0039e+00 (3.3858e+00)	Acc@1  27.34 ( 17.73)	Acc@5  81.25 ( 67.45)
Epoch: [0][290/391]	Time  0.102 ( 0.119)	Data  0.001 ( 0.004)	Loss 2.0410e+00 (3.3408e+00)	Acc@1  25.00 ( 17.86)	Acc@5  78.12 ( 67.88)
Epoch: [0][300/391]	Time  0.103 ( 0.119)	Data  0.001 ( 0.004)	Loss 2.0566e+00 (3.2953e+00)	Acc@1  19.53 ( 18.03)	Acc@5  79.69 ( 68.30)
Epoch: [0][310/391]	Time  0.105 ( 0.118)	Data  0.001 ( 0.004)	Loss 2.0117e+00 (3.2529e+00)	Acc@1  21.09 ( 18.11)	Acc@5  78.91 ( 68.69)
Epoch: [0][320/391]	Time  0.102 ( 0.118)	Data  0.001 ( 0.004)	Loss 1.8516e+00 (3.2129e+00)	Acc@1  26.56 ( 18.32)	Acc@5  83.59 ( 69.04)
Epoch: [0][330/391]	Time  0.102 ( 0.117)	Data  0.001 ( 0.004)	Loss 1.9336e+00 (3.1750e+00)	Acc@1  27.34 ( 18.45)	Acc@5  78.12 ( 69.44)
Epoch: [0][340/391]	Time  0.107 ( 0.117)	Data  0.001 ( 0.004)	Loss 2.0078e+00 (3.1402e+00)	Acc@1  17.97 ( 18.65)	Acc@5  78.91 ( 69.74)
Epoch: [0][350/391]	Time  0.098 ( 0.117)	Data  0.001 ( 0.004)	Loss 1.9102e+00 (3.1058e+00)	Acc@1  25.00 ( 18.82)	Acc@5  78.91 ( 70.08)
Epoch: [0][360/391]	Time  0.102 ( 0.116)	Data  0.001 ( 0.004)	Loss 1.8887e+00 (3.0739e+00)	Acc@1  26.56 ( 19.03)	Acc@5  85.16 ( 70.40)
Epoch: [0][370/391]	Time  0.103 ( 0.116)	Data  0.001 ( 0.004)	Loss 1.8877e+00 (3.0444e+00)	Acc@1  23.44 ( 19.16)	Acc@5  84.38 ( 70.71)
Epoch: [0][380/391]	Time  0.104 ( 0.116)	Data  0.001 ( 0.004)	Loss 1.8662e+00 (3.0147e+00)	Acc@1  26.56 ( 19.31)	Acc@5  85.94 ( 71.03)
Epoch: [0][390/391]	Time  1.169 ( 0.118)	Data  0.001 ( 0.004)	Loss 1.8994e+00 (2.9881e+00)	Acc@1  22.50 ( 19.45)	Acc@5  80.00 ( 71.34)
## e[0] optimizer.zero_grad (sum) time: 0.3462533950805664
## e[0]       loss.backward (sum) time: 15.505412101745605
## e[0]      optimizer.step (sum) time: 2.756298303604126
## epoch[0] training(only) time: 46.36932849884033
# Switched to evaluate mode...
Test: [  0/100]	Time  0.468 ( 0.468)	Loss 1.9844e+00 (1.9844e+00)	Acc@1  19.00 ( 19.00)	Acc@5  76.00 ( 76.00)
Test: [ 10/100]	Time  0.036 ( 0.078)	Loss 2.2266e+00 (2.0597e+00)	Acc@1  25.00 ( 18.73)	Acc@5  79.00 ( 76.45)
Test: [ 20/100]	Time  0.036 ( 0.059)	Loss 2.0039e+00 (2.0356e+00)	Acc@1  16.00 ( 21.00)	Acc@5  88.00 ( 78.52)
Test: [ 30/100]	Time  0.036 ( 0.052)	Loss 1.8984e+00 (2.0245e+00)	Acc@1  23.00 ( 21.84)	Acc@5  81.00 ( 77.87)
Test: [ 40/100]	Time  0.038 ( 0.048)	Loss 1.9580e+00 (2.0235e+00)	Acc@1  29.00 ( 22.02)	Acc@5  79.00 ( 77.93)
Test: [ 50/100]	Time  0.038 ( 0.046)	Loss 2.0645e+00 (2.0176e+00)	Acc@1  20.00 ( 22.29)	Acc@5  77.00 ( 78.06)
Test: [ 60/100]	Time  0.038 ( 0.044)	Loss 1.9951e+00 (2.0151e+00)	Acc@1  22.00 ( 22.11)	Acc@5  81.00 ( 78.03)
Test: [ 70/100]	Time  0.038 ( 0.043)	Loss 2.0703e+00 (2.0204e+00)	Acc@1  18.00 ( 21.92)	Acc@5  78.00 ( 77.80)
Test: [ 80/100]	Time  0.039 ( 0.043)	Loss 2.1270e+00 (2.0185e+00)	Acc@1  20.00 ( 22.00)	Acc@5  73.00 ( 77.90)
Test: [ 90/100]	Time  0.037 ( 0.042)	Loss 1.9180e+00 (2.0188e+00)	Acc@1  18.00 ( 22.07)	Acc@5  77.00 ( 77.98)
 * Acc@1 22.250 Acc@5 77.960
### epoch[0] execution time: 50.60765790939331
EPOCH 1
# current learning rate: 0.1
# Switched to train mode...
Epoch: [1][  0/391]	Time  0.290 ( 0.290)	Data  0.176 ( 0.176)	Loss 1.9102e+00 (1.9102e+00)	Acc@1  27.34 ( 27.34)	Acc@5  82.81 ( 82.81)
Epoch: [1][ 10/391]	Time  0.105 ( 0.125)	Data  0.001 ( 0.019)	Loss 1.9570e+00 (1.8974e+00)	Acc@1  25.78 ( 28.69)	Acc@5  82.03 ( 83.38)
Epoch: [1][ 20/391]	Time  0.108 ( 0.116)	Data  0.001 ( 0.012)	Loss 1.8711e+00 (1.8960e+00)	Acc@1  26.56 ( 26.90)	Acc@5  89.06 ( 83.59)
Epoch: [1][ 30/391]	Time  0.105 ( 0.113)	Data  0.001 ( 0.009)	Loss 1.9414e+00 (1.9019e+00)	Acc@1  34.38 ( 27.07)	Acc@5  80.47 ( 83.24)
Epoch: [1][ 40/391]	Time  0.108 ( 0.112)	Data  0.001 ( 0.008)	Loss 1.9424e+00 (1.8924e+00)	Acc@1  20.31 ( 26.73)	Acc@5  84.38 ( 83.38)
Epoch: [1][ 50/391]	Time  0.106 ( 0.111)	Data  0.001 ( 0.007)	Loss 1.9131e+00 (1.8871e+00)	Acc@1  32.81 ( 27.04)	Acc@5  82.03 ( 83.46)
Epoch: [1][ 60/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.007)	Loss 1.8623e+00 (1.8878e+00)	Acc@1  23.44 ( 27.31)	Acc@5  85.94 ( 83.43)
Epoch: [1][ 70/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.8291e+00 (1.8835e+00)	Acc@1  32.03 ( 27.46)	Acc@5  85.16 ( 83.64)
Epoch: [1][ 80/391]	Time  0.103 ( 0.110)	Data  0.001 ( 0.006)	Loss 2.0547e+00 (1.8817e+00)	Acc@1  28.91 ( 27.59)	Acc@5  84.38 ( 83.83)
Epoch: [1][ 90/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.8838e+00 (1.8855e+00)	Acc@1  25.00 ( 27.56)	Acc@5  83.59 ( 83.72)
Epoch: [1][100/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.006)	Loss 2.0527e+00 (1.8848e+00)	Acc@1  25.00 ( 27.72)	Acc@5  74.22 ( 83.72)
Epoch: [1][110/391]	Time  0.114 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.7871e+00 (1.8785e+00)	Acc@1  39.84 ( 28.05)	Acc@5  92.19 ( 84.00)
Epoch: [1][120/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.9512e+00 (1.8770e+00)	Acc@1  25.78 ( 28.18)	Acc@5  81.25 ( 83.99)
Epoch: [1][130/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.7881e+00 (1.8769e+00)	Acc@1  27.34 ( 28.16)	Acc@5  85.16 ( 84.02)
Epoch: [1][140/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.8369e+00 (1.8752e+00)	Acc@1  32.03 ( 28.36)	Acc@5  83.59 ( 84.00)
Epoch: [1][150/391]	Time  0.110 ( 0.109)	Data  0.004 ( 0.005)	Loss 1.7891e+00 (1.8707e+00)	Acc@1  28.91 ( 28.47)	Acc@5  83.59 ( 84.18)
Epoch: [1][160/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.7939e+00 (1.8683e+00)	Acc@1  35.16 ( 28.53)	Acc@5  86.72 ( 84.29)
Epoch: [1][170/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.8486e+00 (1.8655e+00)	Acc@1  32.03 ( 28.64)	Acc@5  85.94 ( 84.37)
Epoch: [1][180/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.6514e+00 (1.8626e+00)	Acc@1  38.28 ( 28.71)	Acc@5  91.41 ( 84.49)
Epoch: [1][190/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.7441e+00 (1.8590e+00)	Acc@1  37.50 ( 28.90)	Acc@5  85.16 ( 84.55)
Epoch: [1][200/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.7412e+00 (1.8555e+00)	Acc@1  36.72 ( 29.10)	Acc@5  87.50 ( 84.68)
Epoch: [1][210/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.7139e+00 (1.8514e+00)	Acc@1  33.59 ( 29.23)	Acc@5  88.28 ( 84.83)
Epoch: [1][220/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.8193e+00 (1.8482e+00)	Acc@1  26.56 ( 29.35)	Acc@5  82.81 ( 84.95)
Epoch: [1][230/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.8457e+00 (1.8455e+00)	Acc@1  31.25 ( 29.53)	Acc@5  83.59 ( 85.00)
Epoch: [1][240/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.7617e+00 (1.8418e+00)	Acc@1  27.34 ( 29.72)	Acc@5  89.06 ( 85.13)
Epoch: [1][250/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.7686e+00 (1.8407e+00)	Acc@1  35.16 ( 29.82)	Acc@5  84.38 ( 85.15)
Epoch: [1][260/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.7148e+00 (1.8369e+00)	Acc@1  36.72 ( 30.01)	Acc@5  88.28 ( 85.26)
Epoch: [1][270/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.8408e+00 (1.8349e+00)	Acc@1  32.81 ( 30.12)	Acc@5  81.25 ( 85.29)
Epoch: [1][280/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.7314e+00 (1.8312e+00)	Acc@1  35.16 ( 30.23)	Acc@5  89.06 ( 85.32)
Epoch: [1][290/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.9023e+00 (1.8274e+00)	Acc@1  34.38 ( 30.48)	Acc@5  82.81 ( 85.41)
Epoch: [1][300/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.6357e+00 (1.8233e+00)	Acc@1  38.28 ( 30.65)	Acc@5  86.72 ( 85.55)
Epoch: [1][310/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.7393e+00 (1.8208e+00)	Acc@1  28.91 ( 30.78)	Acc@5  85.16 ( 85.56)
Epoch: [1][320/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.7334e+00 (1.8160e+00)	Acc@1  36.72 ( 30.96)	Acc@5  90.62 ( 85.66)
Epoch: [1][330/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.6357e+00 (1.8112e+00)	Acc@1  38.28 ( 31.17)	Acc@5  88.28 ( 85.77)
Epoch: [1][340/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.6602e+00 (1.8086e+00)	Acc@1  33.59 ( 31.27)	Acc@5  90.62 ( 85.84)
Epoch: [1][350/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.7041e+00 (1.8032e+00)	Acc@1  40.62 ( 31.47)	Acc@5  86.72 ( 85.98)
Epoch: [1][360/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.6514e+00 (1.8026e+00)	Acc@1  36.72 ( 31.64)	Acc@5  85.94 ( 86.03)
Epoch: [1][370/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5713e+00 (1.7996e+00)	Acc@1  41.41 ( 31.78)	Acc@5  92.19 ( 86.06)
Epoch: [1][380/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.6426e+00 (1.7961e+00)	Acc@1  35.16 ( 31.89)	Acc@5  89.84 ( 86.17)
Epoch: [1][390/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5459e+00 (1.7933e+00)	Acc@1  43.75 ( 32.04)	Acc@5  90.00 ( 86.18)
## e[1] optimizer.zero_grad (sum) time: 0.3435380458831787
## e[1]       loss.backward (sum) time: 13.669850826263428
## e[1]      optimizer.step (sum) time: 2.764699935913086
## epoch[1] training(only) time: 42.59610414505005
# Switched to evaluate mode...
Test: [  0/100]	Time  0.198 ( 0.198)	Loss 1.6270e+00 (1.6270e+00)	Acc@1  38.00 ( 38.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.038 ( 0.053)	Loss 1.7959e+00 (1.6900e+00)	Acc@1  38.00 ( 38.64)	Acc@5  92.00 ( 89.73)
Test: [ 20/100]	Time  0.036 ( 0.045)	Loss 1.5205e+00 (1.6871e+00)	Acc@1  45.00 ( 37.81)	Acc@5  88.00 ( 89.76)
Test: [ 30/100]	Time  0.037 ( 0.042)	Loss 1.4941e+00 (1.6962e+00)	Acc@1  36.00 ( 36.87)	Acc@5  92.00 ( 89.68)
Test: [ 40/100]	Time  0.037 ( 0.041)	Loss 1.7373e+00 (1.6815e+00)	Acc@1  31.00 ( 37.12)	Acc@5  91.00 ( 89.80)
Test: [ 50/100]	Time  0.036 ( 0.040)	Loss 1.6504e+00 (1.6719e+00)	Acc@1  35.00 ( 37.41)	Acc@5  89.00 ( 89.90)
Test: [ 60/100]	Time  0.036 ( 0.040)	Loss 1.7031e+00 (1.6738e+00)	Acc@1  39.00 ( 37.61)	Acc@5  88.00 ( 89.79)
Test: [ 70/100]	Time  0.036 ( 0.039)	Loss 1.7754e+00 (1.6832e+00)	Acc@1  36.00 ( 37.31)	Acc@5  87.00 ( 89.52)
Test: [ 80/100]	Time  0.036 ( 0.039)	Loss 1.5791e+00 (1.6816e+00)	Acc@1  38.00 ( 37.36)	Acc@5  88.00 ( 89.59)
Test: [ 90/100]	Time  0.036 ( 0.039)	Loss 1.5938e+00 (1.6845e+00)	Acc@1  40.00 ( 37.24)	Acc@5  94.00 ( 89.49)
 * Acc@1 37.200 Acc@5 89.580
### epoch[1] execution time: 46.552918910980225
EPOCH 2
# current learning rate: 0.1
# Switched to train mode...
Epoch: [2][  0/391]	Time  0.283 ( 0.283)	Data  0.188 ( 0.188)	Loss 1.7969e+00 (1.7969e+00)	Acc@1  33.59 ( 33.59)	Acc@5  89.84 ( 89.84)
Epoch: [2][ 10/391]	Time  0.105 ( 0.123)	Data  0.001 ( 0.020)	Loss 1.6328e+00 (1.6950e+00)	Acc@1  38.28 ( 34.87)	Acc@5  84.38 ( 88.71)
Epoch: [2][ 20/391]	Time  0.105 ( 0.116)	Data  0.001 ( 0.013)	Loss 1.6055e+00 (1.6864e+00)	Acc@1  41.41 ( 36.42)	Acc@5  85.94 ( 88.10)
Epoch: [2][ 30/391]	Time  0.106 ( 0.113)	Data  0.001 ( 0.010)	Loss 1.6768e+00 (1.6859e+00)	Acc@1  36.72 ( 36.57)	Acc@5  89.84 ( 88.26)
Epoch: [2][ 40/391]	Time  0.108 ( 0.112)	Data  0.001 ( 0.009)	Loss 1.6260e+00 (1.6798e+00)	Acc@1  36.72 ( 36.78)	Acc@5  90.62 ( 88.40)
Epoch: [2][ 50/391]	Time  0.107 ( 0.111)	Data  0.001 ( 0.008)	Loss 1.7441e+00 (1.6713e+00)	Acc@1  31.25 ( 36.92)	Acc@5  85.94 ( 88.57)
Epoch: [2][ 60/391]	Time  0.107 ( 0.111)	Data  0.001 ( 0.007)	Loss 1.6133e+00 (1.6640e+00)	Acc@1  37.50 ( 37.33)	Acc@5  91.41 ( 88.67)
Epoch: [2][ 70/391]	Time  0.103 ( 0.110)	Data  0.001 ( 0.007)	Loss 1.5254e+00 (1.6569e+00)	Acc@1  42.19 ( 37.60)	Acc@5  88.28 ( 88.70)
Epoch: [2][ 80/391]	Time  0.103 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.5811e+00 (1.6566e+00)	Acc@1  33.59 ( 37.59)	Acc@5  90.62 ( 88.75)
Epoch: [2][ 90/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.7207e+00 (1.6550e+00)	Acc@1  35.16 ( 37.70)	Acc@5  87.50 ( 88.66)
Epoch: [2][100/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.006)	Loss 1.5107e+00 (1.6561e+00)	Acc@1  43.75 ( 37.86)	Acc@5  92.19 ( 88.70)
Epoch: [2][110/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.006)	Loss 1.6611e+00 (1.6525e+00)	Acc@1  39.84 ( 38.08)	Acc@5  85.16 ( 88.80)
Epoch: [2][120/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.006)	Loss 1.6074e+00 (1.6511e+00)	Acc@1  45.31 ( 38.21)	Acc@5  89.06 ( 88.86)
Epoch: [2][130/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.006)	Loss 1.6338e+00 (1.6448e+00)	Acc@1  38.28 ( 38.40)	Acc@5  86.72 ( 88.96)
Epoch: [2][140/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.6162e+00 (1.6378e+00)	Acc@1  39.06 ( 38.63)	Acc@5  90.62 ( 89.15)
Epoch: [2][150/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5088e+00 (1.6354e+00)	Acc@1  44.53 ( 38.75)	Acc@5  92.97 ( 89.27)
Epoch: [2][160/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.6631e+00 (1.6348e+00)	Acc@1  42.19 ( 38.73)	Acc@5  89.84 ( 89.34)
Epoch: [2][170/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5703e+00 (1.6325e+00)	Acc@1  38.28 ( 38.89)	Acc@5  90.62 ( 89.37)
Epoch: [2][180/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4961e+00 (1.6300e+00)	Acc@1  46.09 ( 38.98)	Acc@5  94.53 ( 89.51)
Epoch: [2][190/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.7295e+00 (1.6298e+00)	Acc@1  33.59 ( 39.01)	Acc@5  90.62 ( 89.50)
Epoch: [2][200/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.7686e+00 (1.6268e+00)	Acc@1  35.94 ( 39.12)	Acc@5  81.25 ( 89.54)
Epoch: [2][210/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5391e+00 (1.6230e+00)	Acc@1  40.62 ( 39.27)	Acc@5  93.75 ( 89.64)
Epoch: [2][220/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5215e+00 (1.6237e+00)	Acc@1  46.88 ( 39.26)	Acc@5  89.06 ( 89.58)
Epoch: [2][230/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.8262e+00 (1.6229e+00)	Acc@1  28.12 ( 39.32)	Acc@5  86.72 ( 89.59)
Epoch: [2][240/391]	Time  0.108 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.4922e+00 (1.6212e+00)	Acc@1  45.31 ( 39.46)	Acc@5  89.06 ( 89.63)
Epoch: [2][250/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5527e+00 (1.6199e+00)	Acc@1  41.41 ( 39.56)	Acc@5  91.41 ( 89.64)
Epoch: [2][260/391]	Time  0.105 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.6240e+00 (1.6180e+00)	Acc@1  39.84 ( 39.62)	Acc@5  91.41 ( 89.70)
Epoch: [2][270/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.6445e+00 (1.6153e+00)	Acc@1  43.75 ( 39.79)	Acc@5  90.62 ( 89.77)
Epoch: [2][280/391]	Time  0.108 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.7227e+00 (1.6127e+00)	Acc@1  35.94 ( 39.86)	Acc@5  90.62 ( 89.82)
Epoch: [2][290/391]	Time  0.106 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.5918e+00 (1.6120e+00)	Acc@1  41.41 ( 39.89)	Acc@5  89.84 ( 89.80)
Epoch: [2][300/391]	Time  0.107 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.4932e+00 (1.6090e+00)	Acc@1  44.53 ( 39.96)	Acc@5  91.41 ( 89.82)
Epoch: [2][310/391]	Time  0.106 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.5830e+00 (1.6076e+00)	Acc@1  44.53 ( 40.03)	Acc@5  89.84 ( 89.86)
Epoch: [2][320/391]	Time  0.104 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.3750e+00 (1.6047e+00)	Acc@1  41.41 ( 40.12)	Acc@5  97.66 ( 89.92)
Epoch: [2][330/391]	Time  0.105 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.4443e+00 (1.6015e+00)	Acc@1  46.88 ( 40.25)	Acc@5  94.53 ( 89.96)
Epoch: [2][340/391]	Time  0.108 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.5635e+00 (1.5996e+00)	Acc@1  42.97 ( 40.36)	Acc@5  91.41 ( 89.98)
Epoch: [2][350/391]	Time  0.107 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.4902e+00 (1.5969e+00)	Acc@1  38.28 ( 40.44)	Acc@5  92.19 ( 90.01)
Epoch: [2][360/391]	Time  0.110 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.5166e+00 (1.5944e+00)	Acc@1  46.88 ( 40.52)	Acc@5  88.28 ( 90.03)
Epoch: [2][370/391]	Time  0.106 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.5176e+00 (1.5931e+00)	Acc@1  42.97 ( 40.59)	Acc@5  89.84 ( 90.05)
Epoch: [2][380/391]	Time  0.108 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.4658e+00 (1.5919e+00)	Acc@1  43.75 ( 40.64)	Acc@5  94.53 ( 90.07)
Epoch: [2][390/391]	Time  0.099 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.4248e+00 (1.5912e+00)	Acc@1  41.25 ( 40.65)	Acc@5  96.25 ( 90.09)
## e[2] optimizer.zero_grad (sum) time: 0.35216593742370605
## e[2]       loss.backward (sum) time: 13.709766626358032
## e[2]      optimizer.step (sum) time: 2.766561508178711
## epoch[2] training(only) time: 42.528247356414795
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 1.4600e+00 (1.4600e+00)	Acc@1  50.00 ( 50.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.036 ( 0.052)	Loss 1.3730e+00 (1.4718e+00)	Acc@1  58.00 ( 46.91)	Acc@5  95.00 ( 92.73)
Test: [ 20/100]	Time  0.036 ( 0.044)	Loss 1.3711e+00 (1.4842e+00)	Acc@1  46.00 ( 46.29)	Acc@5  93.00 ( 93.19)
Test: [ 30/100]	Time  0.036 ( 0.042)	Loss 1.4287e+00 (1.4893e+00)	Acc@1  49.00 ( 45.97)	Acc@5  96.00 ( 93.29)
Test: [ 40/100]	Time  0.035 ( 0.041)	Loss 1.6582e+00 (1.4833e+00)	Acc@1  41.00 ( 46.12)	Acc@5  94.00 ( 93.10)
Test: [ 50/100]	Time  0.037 ( 0.040)	Loss 1.4912e+00 (1.4706e+00)	Acc@1  41.00 ( 46.22)	Acc@5  95.00 ( 93.39)
Test: [ 60/100]	Time  0.039 ( 0.039)	Loss 1.5820e+00 (1.4782e+00)	Acc@1  41.00 ( 45.84)	Acc@5  91.00 ( 93.15)
Test: [ 70/100]	Time  0.037 ( 0.039)	Loss 1.5283e+00 (1.4838e+00)	Acc@1  40.00 ( 45.45)	Acc@5  89.00 ( 92.99)
Test: [ 80/100]	Time  0.037 ( 0.039)	Loss 1.3809e+00 (1.4860e+00)	Acc@1  50.00 ( 45.74)	Acc@5  92.00 ( 92.78)
Test: [ 90/100]	Time  0.036 ( 0.039)	Loss 1.4229e+00 (1.4830e+00)	Acc@1  42.00 ( 45.65)	Acc@5  98.00 ( 92.71)
 * Acc@1 45.770 Acc@5 92.700
### epoch[2] execution time: 46.47986912727356
EPOCH 3
# current learning rate: 0.1
# Switched to train mode...
Epoch: [3][  0/391]	Time  0.272 ( 0.272)	Data  0.178 ( 0.178)	Loss 1.4072e+00 (1.4072e+00)	Acc@1  51.56 ( 51.56)	Acc@5  92.97 ( 92.97)
Epoch: [3][ 10/391]	Time  0.110 ( 0.123)	Data  0.001 ( 0.020)	Loss 1.4639e+00 (1.5090e+00)	Acc@1  48.44 ( 44.89)	Acc@5  92.19 ( 91.34)
Epoch: [3][ 20/391]	Time  0.107 ( 0.116)	Data  0.001 ( 0.012)	Loss 1.3564e+00 (1.5122e+00)	Acc@1  50.00 ( 44.68)	Acc@5  91.41 ( 91.03)
Epoch: [3][ 30/391]	Time  0.104 ( 0.114)	Data  0.001 ( 0.010)	Loss 1.5283e+00 (1.5031e+00)	Acc@1  41.41 ( 44.91)	Acc@5  93.75 ( 91.23)
Epoch: [3][ 40/391]	Time  0.107 ( 0.112)	Data  0.001 ( 0.008)	Loss 1.4697e+00 (1.4914e+00)	Acc@1  48.44 ( 44.87)	Acc@5  89.06 ( 91.37)
Epoch: [3][ 50/391]	Time  0.105 ( 0.112)	Data  0.001 ( 0.008)	Loss 1.3926e+00 (1.4961e+00)	Acc@1  51.56 ( 45.14)	Acc@5  90.62 ( 91.16)
Epoch: [3][ 60/391]	Time  0.104 ( 0.111)	Data  0.001 ( 0.007)	Loss 1.3643e+00 (1.4852e+00)	Acc@1  50.78 ( 45.57)	Acc@5  91.41 ( 91.21)
Epoch: [3][ 70/391]	Time  0.105 ( 0.111)	Data  0.001 ( 0.007)	Loss 1.4922e+00 (1.4845e+00)	Acc@1  43.75 ( 45.31)	Acc@5  91.41 ( 91.33)
Epoch: [3][ 80/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.4844e+00 (1.4849e+00)	Acc@1  44.53 ( 45.26)	Acc@5  91.41 ( 91.36)
Epoch: [3][ 90/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.4893e+00 (1.4800e+00)	Acc@1  39.06 ( 45.55)	Acc@5  93.75 ( 91.41)
Epoch: [3][100/391]	Time  0.104 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.6484e+00 (1.4800e+00)	Acc@1  39.84 ( 45.56)	Acc@5  89.84 ( 91.52)
Epoch: [3][110/391]	Time  0.101 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.3750e+00 (1.4812e+00)	Acc@1  52.34 ( 45.44)	Acc@5  89.06 ( 91.46)
Epoch: [3][120/391]	Time  0.109 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.3818e+00 (1.4780e+00)	Acc@1  49.22 ( 45.53)	Acc@5  92.97 ( 91.55)
Epoch: [3][130/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.4639e+00 (1.4739e+00)	Acc@1  46.88 ( 45.78)	Acc@5  92.97 ( 91.60)
Epoch: [3][140/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.005)	Loss 1.5576e+00 (1.4729e+00)	Acc@1  44.53 ( 45.75)	Acc@5  87.50 ( 91.65)
Epoch: [3][150/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5029e+00 (1.4728e+00)	Acc@1  41.41 ( 45.75)	Acc@5  92.19 ( 91.70)
Epoch: [3][160/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3438e+00 (1.4720e+00)	Acc@1  51.56 ( 45.72)	Acc@5  96.09 ( 91.76)
Epoch: [3][170/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2861e+00 (1.4685e+00)	Acc@1  57.03 ( 45.89)	Acc@5  93.75 ( 91.83)
Epoch: [3][180/391]	Time  0.112 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5273e+00 (1.4682e+00)	Acc@1  46.88 ( 45.88)	Acc@5  90.62 ( 91.86)
Epoch: [3][190/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4541e+00 (1.4689e+00)	Acc@1  42.97 ( 45.80)	Acc@5  90.62 ( 91.86)
Epoch: [3][200/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5430e+00 (1.4659e+00)	Acc@1  39.06 ( 45.86)	Acc@5  93.75 ( 91.96)
Epoch: [3][210/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5547e+00 (1.4674e+00)	Acc@1  47.66 ( 45.83)	Acc@5  90.62 ( 91.99)
Epoch: [3][220/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3135e+00 (1.4657e+00)	Acc@1  51.56 ( 45.92)	Acc@5  95.31 ( 92.05)
Epoch: [3][230/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4219e+00 (1.4637e+00)	Acc@1  46.88 ( 46.02)	Acc@5  93.75 ( 92.05)
Epoch: [3][240/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4600e+00 (1.4620e+00)	Acc@1  43.75 ( 46.05)	Acc@5  93.75 ( 92.10)
Epoch: [3][250/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3037e+00 (1.4626e+00)	Acc@1  53.12 ( 46.12)	Acc@5  95.31 ( 92.07)
Epoch: [3][260/391]	Time  0.111 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3398e+00 (1.4610e+00)	Acc@1  43.75 ( 46.14)	Acc@5  97.66 ( 92.12)
Epoch: [3][270/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5273e+00 (1.4593e+00)	Acc@1  45.31 ( 46.26)	Acc@5  91.41 ( 92.14)
Epoch: [3][280/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3105e+00 (1.4574e+00)	Acc@1  53.12 ( 46.34)	Acc@5  92.97 ( 92.21)
Epoch: [3][290/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2432e+00 (1.4525e+00)	Acc@1  55.47 ( 46.45)	Acc@5  94.53 ( 92.28)
Epoch: [3][300/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3916e+00 (1.4496e+00)	Acc@1  50.00 ( 46.55)	Acc@5  92.19 ( 92.32)
Epoch: [3][310/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3145e+00 (1.4483e+00)	Acc@1  53.12 ( 46.62)	Acc@5  96.09 ( 92.33)
Epoch: [3][320/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2881e+00 (1.4456e+00)	Acc@1  59.38 ( 46.75)	Acc@5  92.19 ( 92.39)
Epoch: [3][330/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4883e+00 (1.4435e+00)	Acc@1  47.66 ( 46.86)	Acc@5  92.19 ( 92.41)
Epoch: [3][340/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3906e+00 (1.4412e+00)	Acc@1  46.09 ( 46.99)	Acc@5  92.97 ( 92.44)
Epoch: [3][350/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4697e+00 (1.4385e+00)	Acc@1  46.09 ( 47.08)	Acc@5  93.75 ( 92.47)
Epoch: [3][360/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2061e+00 (1.4362e+00)	Acc@1  56.25 ( 47.20)	Acc@5  96.09 ( 92.50)
Epoch: [3][370/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.6943e+00 (1.4340e+00)	Acc@1  39.84 ( 47.31)	Acc@5  85.94 ( 92.52)
Epoch: [3][380/391]	Time  0.108 ( 0.109)	Data  0.002 ( 0.005)	Loss 1.4688e+00 (1.4344e+00)	Acc@1  47.66 ( 47.31)	Acc@5  92.97 ( 92.53)
Epoch: [3][390/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2266e+00 (1.4330e+00)	Acc@1  51.25 ( 47.34)	Acc@5  98.75 ( 92.54)
## e[3] optimizer.zero_grad (sum) time: 0.3487095832824707
## e[3]       loss.backward (sum) time: 13.664884090423584
## e[3]      optimizer.step (sum) time: 2.7802138328552246
## epoch[3] training(only) time: 42.77636218070984
# Switched to evaluate mode...
Test: [  0/100]	Time  0.199 ( 0.199)	Loss 1.2910e+00 (1.2910e+00)	Acc@1  57.00 ( 57.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.037 ( 0.052)	Loss 1.2451e+00 (1.3488e+00)	Acc@1  54.00 ( 51.36)	Acc@5  93.00 ( 94.45)
Test: [ 20/100]	Time  0.039 ( 0.045)	Loss 1.1426e+00 (1.3353e+00)	Acc@1  59.00 ( 50.29)	Acc@5  97.00 ( 94.76)
Test: [ 30/100]	Time  0.037 ( 0.042)	Loss 1.1562e+00 (1.3377e+00)	Acc@1  56.00 ( 50.61)	Acc@5  97.00 ( 94.55)
Test: [ 40/100]	Time  0.036 ( 0.041)	Loss 1.4199e+00 (1.3360e+00)	Acc@1  49.00 ( 50.61)	Acc@5  94.00 ( 94.37)
Test: [ 50/100]	Time  0.038 ( 0.040)	Loss 1.2861e+00 (1.3217e+00)	Acc@1  50.00 ( 51.22)	Acc@5  98.00 ( 94.69)
Test: [ 60/100]	Time  0.036 ( 0.040)	Loss 1.3496e+00 (1.3282e+00)	Acc@1  48.00 ( 51.21)	Acc@5  98.00 ( 94.59)
Test: [ 70/100]	Time  0.036 ( 0.039)	Loss 1.2979e+00 (1.3363e+00)	Acc@1  45.00 ( 50.73)	Acc@5  94.00 ( 94.69)
Test: [ 80/100]	Time  0.036 ( 0.039)	Loss 1.2949e+00 (1.3281e+00)	Acc@1  57.00 ( 50.90)	Acc@5  96.00 ( 94.74)
Test: [ 90/100]	Time  0.036 ( 0.039)	Loss 1.1895e+00 (1.3333e+00)	Acc@1  48.00 ( 50.69)	Acc@5  98.00 ( 94.57)
 * Acc@1 50.870 Acc@5 94.530
### epoch[3] execution time: 46.743003606796265
EPOCH 4
# current learning rate: 0.1
# Switched to train mode...
Epoch: [4][  0/391]	Time  0.313 ( 0.313)	Data  0.217 ( 0.217)	Loss 1.3291e+00 (1.3291e+00)	Acc@1  53.12 ( 53.12)	Acc@5  94.53 ( 94.53)
Epoch: [4][ 10/391]	Time  0.107 ( 0.126)	Data  0.001 ( 0.023)	Loss 1.3428e+00 (1.3520e+00)	Acc@1  49.22 ( 50.14)	Acc@5  95.31 ( 94.18)
Epoch: [4][ 20/391]	Time  0.100 ( 0.118)	Data  0.002 ( 0.014)	Loss 1.2480e+00 (1.3393e+00)	Acc@1  50.00 ( 50.56)	Acc@5  96.09 ( 93.75)
Epoch: [4][ 30/391]	Time  0.106 ( 0.114)	Data  0.001 ( 0.011)	Loss 1.3193e+00 (1.3470e+00)	Acc@1  52.34 ( 50.60)	Acc@5  95.31 ( 93.90)
Epoch: [4][ 40/391]	Time  0.104 ( 0.113)	Data  0.001 ( 0.009)	Loss 1.3330e+00 (1.3410e+00)	Acc@1  51.56 ( 50.97)	Acc@5  96.88 ( 93.77)
Epoch: [4][ 50/391]	Time  0.106 ( 0.112)	Data  0.001 ( 0.008)	Loss 1.3594e+00 (1.3474e+00)	Acc@1  50.78 ( 51.33)	Acc@5  96.09 ( 93.69)
Epoch: [4][ 60/391]	Time  0.108 ( 0.111)	Data  0.001 ( 0.008)	Loss 1.4219e+00 (1.3472e+00)	Acc@1  44.53 ( 51.05)	Acc@5  92.97 ( 93.89)
Epoch: [4][ 70/391]	Time  0.108 ( 0.111)	Data  0.001 ( 0.007)	Loss 1.2725e+00 (1.3463e+00)	Acc@1  53.91 ( 51.00)	Acc@5  92.97 ( 93.72)
Epoch: [4][ 80/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.007)	Loss 1.3623e+00 (1.3395e+00)	Acc@1  47.66 ( 51.21)	Acc@5  92.97 ( 93.79)
Epoch: [4][ 90/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.1641e+00 (1.3452e+00)	Acc@1  54.69 ( 50.79)	Acc@5  94.53 ( 93.74)
Epoch: [4][100/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.2285e+00 (1.3417e+00)	Acc@1  55.47 ( 50.84)	Acc@5  96.09 ( 93.76)
Epoch: [4][110/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.2480e+00 (1.3397e+00)	Acc@1  53.12 ( 50.95)	Acc@5  92.97 ( 93.80)
Epoch: [4][120/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.006)	Loss 1.4199e+00 (1.3320e+00)	Acc@1  47.66 ( 51.28)	Acc@5  90.62 ( 93.89)
Epoch: [4][130/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.006)	Loss 1.2734e+00 (1.3302e+00)	Acc@1  53.12 ( 51.27)	Acc@5  97.66 ( 93.99)
Epoch: [4][140/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.006)	Loss 1.2461e+00 (1.3254e+00)	Acc@1  51.56 ( 51.48)	Acc@5  94.53 ( 94.09)
Epoch: [4][150/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.006)	Loss 1.4805e+00 (1.3249e+00)	Acc@1  44.53 ( 51.49)	Acc@5  92.19 ( 94.06)
Epoch: [4][160/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2559e+00 (1.3215e+00)	Acc@1  57.81 ( 51.61)	Acc@5  95.31 ( 94.08)
Epoch: [4][170/391]	Time  0.111 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2031e+00 (1.3190e+00)	Acc@1  57.03 ( 51.71)	Acc@5  94.53 ( 94.09)
Epoch: [4][180/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3262e+00 (1.3201e+00)	Acc@1  50.78 ( 51.63)	Acc@5  91.41 ( 94.07)
Epoch: [4][190/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3301e+00 (1.3182e+00)	Acc@1  54.69 ( 51.75)	Acc@5  92.97 ( 94.07)
Epoch: [4][200/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2070e+00 (1.3170e+00)	Acc@1  57.81 ( 51.89)	Acc@5  96.09 ( 94.08)
Epoch: [4][210/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4062e+00 (1.3166e+00)	Acc@1  49.22 ( 51.89)	Acc@5  95.31 ( 94.11)
Epoch: [4][220/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2676e+00 (1.3153e+00)	Acc@1  52.34 ( 51.93)	Acc@5  96.88 ( 94.14)
Epoch: [4][230/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2607e+00 (1.3124e+00)	Acc@1  53.12 ( 52.03)	Acc@5  92.19 ( 94.17)
Epoch: [4][240/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2803e+00 (1.3106e+00)	Acc@1  59.38 ( 52.14)	Acc@5  92.97 ( 94.19)
Epoch: [4][250/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1709e+00 (1.3084e+00)	Acc@1  60.16 ( 52.25)	Acc@5  94.53 ( 94.20)
Epoch: [4][260/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3350e+00 (1.3075e+00)	Acc@1  52.34 ( 52.32)	Acc@5  92.97 ( 94.20)
Epoch: [4][270/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1973e+00 (1.3041e+00)	Acc@1  60.94 ( 52.42)	Acc@5  92.97 ( 94.23)
Epoch: [4][280/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0840e+00 (1.3013e+00)	Acc@1  63.28 ( 52.55)	Acc@5  95.31 ( 94.25)
Epoch: [4][290/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3369e+00 (1.3001e+00)	Acc@1  56.25 ( 52.66)	Acc@5  90.62 ( 94.24)
Epoch: [4][300/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2197e+00 (1.2989e+00)	Acc@1  57.03 ( 52.73)	Acc@5  92.19 ( 94.27)
Epoch: [4][310/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0098e+00 (1.2963e+00)	Acc@1  63.28 ( 52.83)	Acc@5  96.88 ( 94.31)
Epoch: [4][320/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5156e+00 (1.2948e+00)	Acc@1  46.88 ( 52.90)	Acc@5  94.53 ( 94.32)
Epoch: [4][330/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2178e+00 (1.2929e+00)	Acc@1  54.69 ( 52.97)	Acc@5  96.09 ( 94.34)
Epoch: [4][340/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4160e+00 (1.2923e+00)	Acc@1  49.22 ( 53.02)	Acc@5  90.62 ( 94.33)
Epoch: [4][350/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2324e+00 (1.2905e+00)	Acc@1  59.38 ( 53.08)	Acc@5  93.75 ( 94.34)
Epoch: [4][360/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4307e+00 (1.2900e+00)	Acc@1  52.34 ( 53.06)	Acc@5  90.62 ( 94.35)
Epoch: [4][370/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1982e+00 (1.2869e+00)	Acc@1  54.69 ( 53.19)	Acc@5  96.09 ( 94.38)
Epoch: [4][380/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2432e+00 (1.2862e+00)	Acc@1  60.94 ( 53.23)	Acc@5  92.97 ( 94.38)
Epoch: [4][390/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3076e+00 (1.2842e+00)	Acc@1  55.00 ( 53.32)	Acc@5  92.50 ( 94.38)
## e[4] optimizer.zero_grad (sum) time: 0.356006383895874
## e[4]       loss.backward (sum) time: 13.694911003112793
## e[4]      optimizer.step (sum) time: 2.759575843811035
## epoch[4] training(only) time: 42.60096096992493
# Switched to evaluate mode...
Test: [  0/100]	Time  0.196 ( 0.196)	Loss 1.2725e+00 (1.2725e+00)	Acc@1  54.00 ( 54.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.039 ( 0.052)	Loss 1.2461e+00 (1.3298e+00)	Acc@1  57.00 ( 51.91)	Acc@5  91.00 ( 94.45)
Test: [ 20/100]	Time  0.036 ( 0.045)	Loss 1.1660e+00 (1.3306e+00)	Acc@1  60.00 ( 52.86)	Acc@5  97.00 ( 94.76)
Test: [ 30/100]	Time  0.038 ( 0.042)	Loss 1.0264e+00 (1.3345e+00)	Acc@1  66.00 ( 53.23)	Acc@5  98.00 ( 94.87)
Test: [ 40/100]	Time  0.038 ( 0.041)	Loss 1.1748e+00 (1.3291e+00)	Acc@1  55.00 ( 53.39)	Acc@5  97.00 ( 94.61)
Test: [ 50/100]	Time  0.037 ( 0.041)	Loss 1.3945e+00 (1.3169e+00)	Acc@1  50.00 ( 53.98)	Acc@5  95.00 ( 94.92)
Test: [ 60/100]	Time  0.039 ( 0.040)	Loss 1.1592e+00 (1.3167e+00)	Acc@1  54.00 ( 53.82)	Acc@5  99.00 ( 94.98)
Test: [ 70/100]	Time  0.036 ( 0.039)	Loss 1.2412e+00 (1.3170e+00)	Acc@1  55.00 ( 53.70)	Acc@5  94.00 ( 94.96)
Test: [ 80/100]	Time  0.037 ( 0.039)	Loss 1.1504e+00 (1.3175e+00)	Acc@1  57.00 ( 54.04)	Acc@5  94.00 ( 94.95)
Test: [ 90/100]	Time  0.041 ( 0.039)	Loss 1.2002e+00 (1.3202e+00)	Acc@1  48.00 ( 53.97)	Acc@5 100.00 ( 95.00)
 * Acc@1 54.090 Acc@5 94.960
### epoch[4] execution time: 46.580280780792236
EPOCH 5
# current learning rate: 0.1
# Switched to train mode...
Epoch: [5][  0/391]	Time  0.321 ( 0.321)	Data  0.221 ( 0.221)	Loss 1.2090e+00 (1.2090e+00)	Acc@1  57.81 ( 57.81)	Acc@5  93.75 ( 93.75)
Epoch: [5][ 10/391]	Time  0.105 ( 0.127)	Data  0.001 ( 0.023)	Loss 1.4912e+00 (1.1927e+00)	Acc@1  49.22 ( 56.32)	Acc@5  92.97 ( 95.60)
Epoch: [5][ 20/391]	Time  0.109 ( 0.119)	Data  0.001 ( 0.014)	Loss 1.2402e+00 (1.1683e+00)	Acc@1  47.66 ( 56.77)	Acc@5  96.09 ( 96.13)
Epoch: [5][ 30/391]	Time  0.107 ( 0.115)	Data  0.001 ( 0.011)	Loss 1.1650e+00 (1.1930e+00)	Acc@1  57.81 ( 56.10)	Acc@5  96.88 ( 95.41)
Epoch: [5][ 40/391]	Time  0.109 ( 0.114)	Data  0.001 ( 0.009)	Loss 1.1670e+00 (1.1919e+00)	Acc@1  58.59 ( 56.21)	Acc@5  94.53 ( 95.48)
Epoch: [5][ 50/391]	Time  0.108 ( 0.113)	Data  0.001 ( 0.008)	Loss 1.2432e+00 (1.1901e+00)	Acc@1  53.91 ( 56.45)	Acc@5  96.88 ( 95.54)
Epoch: [5][ 60/391]	Time  0.106 ( 0.112)	Data  0.001 ( 0.008)	Loss 1.1562e+00 (1.1873e+00)	Acc@1  57.81 ( 56.81)	Acc@5  96.09 ( 95.54)
Epoch: [5][ 70/391]	Time  0.108 ( 0.111)	Data  0.001 ( 0.007)	Loss 1.1592e+00 (1.1868e+00)	Acc@1  57.81 ( 57.00)	Acc@5  97.66 ( 95.43)
Epoch: [5][ 80/391]	Time  0.105 ( 0.111)	Data  0.001 ( 0.007)	Loss 1.0029e+00 (1.1850e+00)	Acc@1  60.16 ( 57.14)	Acc@5  99.22 ( 95.39)
Epoch: [5][ 90/391]	Time  0.109 ( 0.111)	Data  0.001 ( 0.006)	Loss 1.3525e+00 (1.1834e+00)	Acc@1  58.59 ( 57.31)	Acc@5  90.62 ( 95.32)
Epoch: [5][100/391]	Time  0.104 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.3545e+00 (1.1878e+00)	Acc@1  49.22 ( 57.23)	Acc@5  95.31 ( 95.28)
Epoch: [5][110/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.2354e+00 (1.1876e+00)	Acc@1  57.81 ( 57.24)	Acc@5  94.53 ( 95.35)
Epoch: [5][120/391]	Time  0.109 ( 0.110)	Data  0.001 ( 0.006)	Loss 8.3984e-01 (1.1812e+00)	Acc@1  75.78 ( 57.48)	Acc@5  99.22 ( 95.38)
Epoch: [5][130/391]	Time  0.111 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.2148e+00 (1.1809e+00)	Acc@1  58.59 ( 57.50)	Acc@5  93.75 ( 95.37)
Epoch: [5][140/391]	Time  0.104 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.2676e+00 (1.1845e+00)	Acc@1  59.38 ( 57.39)	Acc@5  94.53 ( 95.35)
Epoch: [5][150/391]	Time  0.102 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.2021e+00 (1.1844e+00)	Acc@1  59.38 ( 57.44)	Acc@5  94.53 ( 95.32)
Epoch: [5][160/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.005)	Loss 9.7363e-01 (1.1819e+00)	Acc@1  61.72 ( 57.52)	Acc@5  96.88 ( 95.33)
Epoch: [5][170/391]	Time  0.109 ( 0.110)	Data  0.001 ( 0.005)	Loss 1.2529e+00 (1.1814e+00)	Acc@1  57.03 ( 57.55)	Acc@5  96.09 ( 95.38)
Epoch: [5][180/391]	Time  0.104 ( 0.110)	Data  0.001 ( 0.005)	Loss 1.1084e+00 (1.1776e+00)	Acc@1  64.06 ( 57.59)	Acc@5  92.19 ( 95.38)
Epoch: [5][190/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.005)	Loss 1.1973e+00 (1.1767e+00)	Acc@1  56.25 ( 57.72)	Acc@5  96.09 ( 95.38)
Epoch: [5][200/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.005)	Loss 1.1338e+00 (1.1755e+00)	Acc@1  57.81 ( 57.85)	Acc@5  96.88 ( 95.40)
Epoch: [5][210/391]	Time  0.109 ( 0.110)	Data  0.001 ( 0.005)	Loss 1.2393e+00 (1.1738e+00)	Acc@1  55.47 ( 57.93)	Acc@5  94.53 ( 95.42)
Epoch: [5][220/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.005)	Loss 1.0088e+00 (1.1727e+00)	Acc@1  63.28 ( 57.99)	Acc@5  96.88 ( 95.47)
Epoch: [5][230/391]	Time  0.109 ( 0.110)	Data  0.001 ( 0.005)	Loss 1.0488e+00 (1.1718e+00)	Acc@1  64.84 ( 58.00)	Acc@5  95.31 ( 95.45)
Epoch: [5][240/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.005)	Loss 1.1250e+00 (1.1715e+00)	Acc@1  59.38 ( 58.01)	Acc@5  96.09 ( 95.46)
Epoch: [5][250/391]	Time  0.109 ( 0.110)	Data  0.001 ( 0.005)	Loss 1.0596e+00 (1.1706e+00)	Acc@1  57.81 ( 57.99)	Acc@5  98.44 ( 95.47)
Epoch: [5][260/391]	Time  0.109 ( 0.110)	Data  0.001 ( 0.005)	Loss 1.2266e+00 (1.1708e+00)	Acc@1  55.47 ( 58.01)	Acc@5  95.31 ( 95.46)
Epoch: [5][270/391]	Time  0.109 ( 0.110)	Data  0.001 ( 0.005)	Loss 1.2471e+00 (1.1679e+00)	Acc@1  50.00 ( 58.10)	Acc@5  97.66 ( 95.48)
Epoch: [5][280/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.005)	Loss 1.2441e+00 (1.1674e+00)	Acc@1  55.47 ( 58.07)	Acc@5  96.09 ( 95.52)
Epoch: [5][290/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1094e+00 (1.1659e+00)	Acc@1  59.38 ( 58.10)	Acc@5  94.53 ( 95.51)
Epoch: [5][300/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0752e+00 (1.1657e+00)	Acc@1  62.50 ( 58.12)	Acc@5  94.53 ( 95.50)
Epoch: [5][310/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2168e+00 (1.1652e+00)	Acc@1  59.38 ( 58.18)	Acc@5  90.62 ( 95.48)
Epoch: [5][320/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2070e+00 (1.1626e+00)	Acc@1  55.47 ( 58.28)	Acc@5  94.53 ( 95.48)
Epoch: [5][330/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2266e+00 (1.1607e+00)	Acc@1  60.16 ( 58.38)	Acc@5  95.31 ( 95.52)
Epoch: [5][340/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0010e+00 (1.1594e+00)	Acc@1  61.72 ( 58.43)	Acc@5  97.66 ( 95.54)
Epoch: [5][350/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0410e+00 (1.1574e+00)	Acc@1  64.06 ( 58.51)	Acc@5  92.97 ( 95.55)
Epoch: [5][360/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.8877e-01 (1.1549e+00)	Acc@1  64.06 ( 58.63)	Acc@5  96.88 ( 95.56)
Epoch: [5][370/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.9609e-01 (1.1532e+00)	Acc@1  64.06 ( 58.70)	Acc@5  97.66 ( 95.58)
Epoch: [5][380/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2275e+00 (1.1516e+00)	Acc@1  59.38 ( 58.76)	Acc@5  95.31 ( 95.60)
Epoch: [5][390/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1182e+00 (1.1493e+00)	Acc@1  63.75 ( 58.86)	Acc@5  97.50 ( 95.62)
## e[5] optimizer.zero_grad (sum) time: 0.34231114387512207
## e[5]       loss.backward (sum) time: 13.657657861709595
## e[5]      optimizer.step (sum) time: 2.8432540893554688
## epoch[5] training(only) time: 42.83985757827759
# Switched to evaluate mode...
Test: [  0/100]	Time  0.198 ( 0.198)	Loss 1.0654e+00 (1.0654e+00)	Acc@1  62.00 ( 62.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.039 ( 0.052)	Loss 9.6484e-01 (1.0793e+00)	Acc@1  69.00 ( 60.64)	Acc@5  94.00 ( 96.45)
Test: [ 20/100]	Time  0.036 ( 0.045)	Loss 9.6191e-01 (1.0882e+00)	Acc@1  68.00 ( 61.00)	Acc@5  97.00 ( 96.43)
Test: [ 30/100]	Time  0.038 ( 0.042)	Loss 8.9746e-01 (1.1080e+00)	Acc@1  70.00 ( 60.77)	Acc@5  96.00 ( 95.94)
Test: [ 40/100]	Time  0.036 ( 0.041)	Loss 9.3994e-01 (1.0943e+00)	Acc@1  63.00 ( 61.05)	Acc@5  96.00 ( 95.95)
Test: [ 50/100]	Time  0.036 ( 0.040)	Loss 1.0596e+00 (1.0797e+00)	Acc@1  62.00 ( 61.65)	Acc@5  99.00 ( 96.10)
Test: [ 60/100]	Time  0.036 ( 0.040)	Loss 1.0225e+00 (1.0864e+00)	Acc@1  59.00 ( 61.44)	Acc@5  98.00 ( 96.00)
Test: [ 70/100]	Time  0.036 ( 0.040)	Loss 1.0322e+00 (1.0939e+00)	Acc@1  65.00 ( 60.87)	Acc@5  95.00 ( 96.07)
Test: [ 80/100]	Time  0.036 ( 0.039)	Loss 9.8926e-01 (1.0917e+00)	Acc@1  60.00 ( 61.11)	Acc@5  95.00 ( 96.09)
Test: [ 90/100]	Time  0.038 ( 0.039)	Loss 1.0381e+00 (1.0957e+00)	Acc@1  59.00 ( 60.98)	Acc@5  98.00 ( 96.00)
 * Acc@1 60.940 Acc@5 95.900
### epoch[5] execution time: 46.859320640563965
EPOCH 6
# current learning rate: 0.1
# Switched to train mode...
Epoch: [6][  0/391]	Time  0.268 ( 0.268)	Data  0.168 ( 0.168)	Loss 1.0967e+00 (1.0967e+00)	Acc@1  61.72 ( 61.72)	Acc@5  95.31 ( 95.31)
Epoch: [6][ 10/391]	Time  0.104 ( 0.122)	Data  0.001 ( 0.019)	Loss 1.0732e+00 (1.0587e+00)	Acc@1  67.19 ( 61.93)	Acc@5  92.97 ( 96.02)
Epoch: [6][ 20/391]	Time  0.098 ( 0.115)	Data  0.001 ( 0.012)	Loss 1.0020e+00 (1.0436e+00)	Acc@1  64.84 ( 62.65)	Acc@5  94.53 ( 96.13)
Epoch: [6][ 30/391]	Time  0.108 ( 0.113)	Data  0.001 ( 0.009)	Loss 1.1055e+00 (1.0619e+00)	Acc@1  59.38 ( 62.50)	Acc@5  93.75 ( 95.84)
Epoch: [6][ 40/391]	Time  0.105 ( 0.112)	Data  0.001 ( 0.008)	Loss 1.0547e+00 (1.0671e+00)	Acc@1  58.59 ( 62.06)	Acc@5  99.22 ( 96.09)
Epoch: [6][ 50/391]	Time  0.104 ( 0.111)	Data  0.001 ( 0.007)	Loss 1.1084e+00 (1.0654e+00)	Acc@1  64.06 ( 62.48)	Acc@5  93.75 ( 96.16)
Epoch: [6][ 60/391]	Time  0.104 ( 0.110)	Data  0.001 ( 0.007)	Loss 9.8828e-01 (1.0621e+00)	Acc@1  65.62 ( 62.47)	Acc@5  96.88 ( 96.13)
Epoch: [6][ 70/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.006)	Loss 9.5605e-01 (1.0620e+00)	Acc@1  67.19 ( 62.52)	Acc@5  96.09 ( 96.19)
Epoch: [6][ 80/391]	Time  0.104 ( 0.110)	Data  0.001 ( 0.006)	Loss 9.6533e-01 (1.0608e+00)	Acc@1  64.84 ( 62.50)	Acc@5  97.66 ( 96.31)
Epoch: [6][ 90/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.0869e+00 (1.0623e+00)	Acc@1  59.38 ( 62.35)	Acc@5  98.44 ( 96.32)
Epoch: [6][100/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.0869e+00 (1.0593e+00)	Acc@1  57.03 ( 62.40)	Acc@5  98.44 ( 96.40)
Epoch: [6][110/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.006)	Loss 1.1660e+00 (1.0551e+00)	Acc@1  56.25 ( 62.52)	Acc@5  94.53 ( 96.41)
Epoch: [6][120/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1611e+00 (1.0512e+00)	Acc@1  62.50 ( 62.69)	Acc@5  92.97 ( 96.46)
Epoch: [6][130/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0371e+00 (1.0539e+00)	Acc@1  64.84 ( 62.55)	Acc@5  93.75 ( 96.39)
Epoch: [6][140/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.5312e-01 (1.0533e+00)	Acc@1  67.97 ( 62.58)	Acc@5  96.88 ( 96.40)
Epoch: [6][150/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.5996e-01 (1.0488e+00)	Acc@1  67.19 ( 62.87)	Acc@5  98.44 ( 96.47)
Epoch: [6][160/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0723e+00 (1.0471e+00)	Acc@1  66.41 ( 62.91)	Acc@5  96.09 ( 96.49)
Epoch: [6][170/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.7207e-01 (1.0441e+00)	Acc@1  71.09 ( 63.09)	Acc@5  96.09 ( 96.50)
Epoch: [6][180/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.9756e-01 (1.0432e+00)	Acc@1  64.06 ( 63.13)	Acc@5  96.88 ( 96.49)
Epoch: [6][190/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.6475e-01 (1.0418e+00)	Acc@1  71.09 ( 63.26)	Acc@5  98.44 ( 96.49)
Epoch: [6][200/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0117e+00 (1.0415e+00)	Acc@1  62.50 ( 63.23)	Acc@5 100.00 ( 96.53)
Epoch: [6][210/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.0723e-01 (1.0397e+00)	Acc@1  67.97 ( 63.24)	Acc@5  96.88 ( 96.51)
Epoch: [6][220/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.8145e-01 (1.0403e+00)	Acc@1  56.25 ( 63.13)	Acc@5  99.22 ( 96.53)
Epoch: [6][230/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.3262e-01 (1.0383e+00)	Acc@1  64.84 ( 63.15)	Acc@5  96.09 ( 96.56)
Epoch: [6][240/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.9316e-01 (1.0360e+00)	Acc@1  65.62 ( 63.25)	Acc@5  98.44 ( 96.58)
Epoch: [6][250/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1777e+00 (1.0366e+00)	Acc@1  54.69 ( 63.23)	Acc@5  98.44 ( 96.61)
Epoch: [6][260/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0039e+00 (1.0374e+00)	Acc@1  61.72 ( 63.13)	Acc@5  96.88 ( 96.61)
Epoch: [6][270/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.4521e-01 (1.0385e+00)	Acc@1  72.66 ( 63.16)	Acc@5  98.44 ( 96.57)
Epoch: [6][280/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.1113e-01 (1.0371e+00)	Acc@1  67.97 ( 63.25)	Acc@5  95.31 ( 96.55)
Epoch: [6][290/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0020e+00 (1.0362e+00)	Acc@1  62.50 ( 63.27)	Acc@5  97.66 ( 96.57)
Epoch: [6][300/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2109e+00 (1.0337e+00)	Acc@1  61.72 ( 63.40)	Acc@5  92.19 ( 96.57)
Epoch: [6][310/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.4141e-01 (1.0326e+00)	Acc@1  62.50 ( 63.42)	Acc@5  98.44 ( 96.58)
Epoch: [6][320/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0596e+00 (1.0323e+00)	Acc@1  60.16 ( 63.46)	Acc@5  96.88 ( 96.59)
Epoch: [6][330/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.0137e-01 (1.0312e+00)	Acc@1  64.84 ( 63.51)	Acc@5  98.44 ( 96.58)
Epoch: [6][340/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.6914e-01 (1.0296e+00)	Acc@1  71.09 ( 63.57)	Acc@5  99.22 ( 96.63)
Epoch: [6][350/391]	Time  0.100 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.9795e-01 (1.0289e+00)	Acc@1  67.19 ( 63.62)	Acc@5  97.66 ( 96.63)
Epoch: [6][360/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.6240e-01 (1.0293e+00)	Acc@1  65.62 ( 63.63)	Acc@5  96.09 ( 96.63)
Epoch: [6][370/391]	Time  0.100 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.6875e-01 (1.0277e+00)	Acc@1  64.84 ( 63.69)	Acc@5  96.88 ( 96.64)
Epoch: [6][380/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1357e+00 (1.0259e+00)	Acc@1  60.16 ( 63.76)	Acc@5  97.66 ( 96.65)
Epoch: [6][390/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.1992e-01 (1.0244e+00)	Acc@1  65.00 ( 63.76)	Acc@5 100.00 ( 96.66)
## e[6] optimizer.zero_grad (sum) time: 0.3490614891052246
## e[6]       loss.backward (sum) time: 13.649099588394165
## e[6]      optimizer.step (sum) time: 2.7500431537628174
## epoch[6] training(only) time: 42.576106548309326
# Switched to evaluate mode...
Test: [  0/100]	Time  0.231 ( 0.231)	Loss 1.0576e+00 (1.0576e+00)	Acc@1  65.00 ( 65.00)	Acc@5  97.00 ( 97.00)
Test: [ 10/100]	Time  0.036 ( 0.054)	Loss 9.6094e-01 (1.0050e+00)	Acc@1  66.00 ( 64.82)	Acc@5  96.00 ( 97.18)
Test: [ 20/100]	Time  0.037 ( 0.046)	Loss 1.1592e+00 (1.0101e+00)	Acc@1  59.00 ( 64.67)	Acc@5  99.00 ( 97.29)
Test: [ 30/100]	Time  0.037 ( 0.043)	Loss 8.5156e-01 (1.0143e+00)	Acc@1  70.00 ( 65.26)	Acc@5  97.00 ( 97.19)
Test: [ 40/100]	Time  0.037 ( 0.042)	Loss 8.1836e-01 (1.0060e+00)	Acc@1  75.00 ( 65.32)	Acc@5  98.00 ( 97.15)
Test: [ 50/100]	Time  0.036 ( 0.041)	Loss 8.5889e-01 (9.9107e-01)	Acc@1  68.00 ( 66.02)	Acc@5  97.00 ( 97.25)
Test: [ 60/100]	Time  0.037 ( 0.040)	Loss 1.0078e+00 (9.9833e-01)	Acc@1  59.00 ( 65.51)	Acc@5  98.00 ( 97.21)
Test: [ 70/100]	Time  0.038 ( 0.040)	Loss 1.0420e+00 (1.0080e+00)	Acc@1  63.00 ( 65.14)	Acc@5  98.00 ( 97.21)
Test: [ 80/100]	Time  0.039 ( 0.039)	Loss 7.8955e-01 (1.0055e+00)	Acc@1  73.00 ( 65.22)	Acc@5  99.00 ( 97.21)
Test: [ 90/100]	Time  0.038 ( 0.039)	Loss 8.1592e-01 (1.0083e+00)	Acc@1  67.00 ( 65.11)	Acc@5 100.00 ( 97.27)
 * Acc@1 64.990 Acc@5 97.200
### epoch[6] execution time: 46.569199562072754
EPOCH 7
# current learning rate: 0.1
# Switched to train mode...
Epoch: [7][  0/391]	Time  0.269 ( 0.269)	Data  0.174 ( 0.174)	Loss 1.0371e+00 (1.0371e+00)	Acc@1  65.62 ( 65.62)	Acc@5  94.53 ( 94.53)
Epoch: [7][ 10/391]	Time  0.107 ( 0.122)	Data  0.001 ( 0.020)	Loss 8.3838e-01 (9.9751e-01)	Acc@1  67.19 ( 63.71)	Acc@5  97.66 ( 97.23)
Epoch: [7][ 20/391]	Time  0.109 ( 0.115)	Data  0.001 ( 0.012)	Loss 7.2070e-01 (9.5459e-01)	Acc@1  75.00 ( 65.81)	Acc@5  98.44 ( 97.36)
Epoch: [7][ 30/391]	Time  0.108 ( 0.113)	Data  0.001 ( 0.010)	Loss 9.0771e-01 (9.5787e-01)	Acc@1  68.75 ( 65.68)	Acc@5  96.09 ( 97.23)
Epoch: [7][ 40/391]	Time  0.110 ( 0.112)	Data  0.001 ( 0.008)	Loss 1.1367e+00 (9.7167e-01)	Acc@1  59.38 ( 65.45)	Acc@5  97.66 ( 97.10)
Epoch: [7][ 50/391]	Time  0.110 ( 0.111)	Data  0.001 ( 0.007)	Loss 1.0869e+00 (9.6840e-01)	Acc@1  64.06 ( 65.66)	Acc@5  97.66 ( 97.18)
Epoch: [7][ 60/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.007)	Loss 1.0117e+00 (9.7326e-01)	Acc@1  64.06 ( 65.39)	Acc@5  96.09 ( 97.09)
Epoch: [7][ 70/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 9.9561e-01 (9.7024e-01)	Acc@1  64.84 ( 65.45)	Acc@5  97.66 ( 97.12)
Epoch: [7][ 80/391]	Time  0.100 ( 0.110)	Data  0.001 ( 0.006)	Loss 9.1162e-01 (9.6674e-01)	Acc@1  66.41 ( 65.44)	Acc@5  93.75 ( 97.09)
Epoch: [7][ 90/391]	Time  0.116 ( 0.110)	Data  0.001 ( 0.006)	Loss 9.2871e-01 (9.5762e-01)	Acc@1  65.62 ( 65.76)	Acc@5  96.88 ( 97.13)
Epoch: [7][100/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.006)	Loss 9.1992e-01 (9.5854e-01)	Acc@1  65.62 ( 65.62)	Acc@5  99.22 ( 97.10)
Epoch: [7][110/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.006)	Loss 1.0918e+00 (9.5802e-01)	Acc@1  55.47 ( 65.62)	Acc@5  96.09 ( 97.08)
Epoch: [7][120/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.5703e-01 (9.6213e-01)	Acc@1  62.50 ( 65.45)	Acc@5  98.44 ( 97.05)
Epoch: [7][130/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.6777e-01 (9.6177e-01)	Acc@1  63.28 ( 65.55)	Acc@5  97.66 ( 97.06)
Epoch: [7][140/391]	Time  0.101 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.5156e-01 (9.5889e-01)	Acc@1  74.22 ( 65.68)	Acc@5  96.09 ( 97.10)
Epoch: [7][150/391]	Time  0.111 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.9453e-01 (9.5872e-01)	Acc@1  70.31 ( 65.66)	Acc@5  96.09 ( 97.10)
Epoch: [7][160/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.0674e-01 (9.5722e-01)	Acc@1  64.84 ( 65.72)	Acc@5  96.88 ( 97.10)
Epoch: [7][170/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0117e+00 (9.5542e-01)	Acc@1  63.28 ( 65.78)	Acc@5  97.66 ( 97.12)
Epoch: [7][180/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.0088e-01 (9.5180e-01)	Acc@1  71.09 ( 65.97)	Acc@5  99.22 ( 97.15)
Epoch: [7][190/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.6436e-01 (9.5104e-01)	Acc@1  66.41 ( 65.97)	Acc@5  95.31 ( 97.15)
Epoch: [7][200/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.8828e-01 (9.4915e-01)	Acc@1  66.41 ( 66.09)	Acc@5  95.31 ( 97.14)
Epoch: [7][210/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.2070e-01 (9.4672e-01)	Acc@1  75.78 ( 66.13)	Acc@5  98.44 ( 97.17)
Epoch: [7][220/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0605e+00 (9.4402e-01)	Acc@1  63.28 ( 66.19)	Acc@5  96.09 ( 97.20)
Epoch: [7][230/391]	Time  0.101 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.9883e-01 (9.4233e-01)	Acc@1  71.88 ( 66.31)	Acc@5  96.88 ( 97.21)
Epoch: [7][240/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.4473e-01 (9.4095e-01)	Acc@1  73.44 ( 66.41)	Acc@5  97.66 ( 97.22)
Epoch: [7][250/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.2715e-01 (9.3997e-01)	Acc@1  71.09 ( 66.42)	Acc@5  99.22 ( 97.24)
Epoch: [7][260/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.0186e-01 (9.3950e-01)	Acc@1  66.41 ( 66.42)	Acc@5  97.66 ( 97.23)
Epoch: [7][270/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.7656e-01 (9.3755e-01)	Acc@1  64.06 ( 66.54)	Acc@5  96.09 ( 97.24)
Epoch: [7][280/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.8242e-01 (9.3798e-01)	Acc@1  60.94 ( 66.55)	Acc@5  96.88 ( 97.23)
Epoch: [7][290/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0684e+00 (9.3675e-01)	Acc@1  66.41 ( 66.63)	Acc@5  94.53 ( 97.23)
Epoch: [7][300/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.3691e-01 (9.3534e-01)	Acc@1  71.88 ( 66.67)	Acc@5  98.44 ( 97.25)
Epoch: [7][310/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.0576e-01 (9.3523e-01)	Acc@1  65.62 ( 66.65)	Acc@5  98.44 ( 97.23)
Epoch: [7][320/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.1396e-01 (9.3365e-01)	Acc@1  71.09 ( 66.74)	Acc@5  97.66 ( 97.22)
Epoch: [7][330/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.4131e-01 (9.3499e-01)	Acc@1  75.00 ( 66.74)	Acc@5  99.22 ( 97.21)
Epoch: [7][340/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.3633e-01 (9.3371e-01)	Acc@1  71.88 ( 66.78)	Acc@5  98.44 ( 97.23)
Epoch: [7][350/391]	Time  0.104 ( 0.109)	Data  0.002 ( 0.005)	Loss 9.2139e-01 (9.3361e-01)	Acc@1  68.75 ( 66.80)	Acc@5  97.66 ( 97.23)
Epoch: [7][360/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.4482e-01 (9.3399e-01)	Acc@1  64.06 ( 66.78)	Acc@5  96.88 ( 97.23)
Epoch: [7][370/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.0713e-01 (9.3193e-01)	Acc@1  70.31 ( 66.87)	Acc@5  98.44 ( 97.23)
Epoch: [7][380/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.1650e-01 (9.3008e-01)	Acc@1  66.41 ( 66.93)	Acc@5  97.66 ( 97.23)
Epoch: [7][390/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.9805e-01 (9.2837e-01)	Acc@1  66.25 ( 66.99)	Acc@5  97.50 ( 97.24)
## e[7] optimizer.zero_grad (sum) time: 0.34424614906311035
## e[7]       loss.backward (sum) time: 13.626059770584106
## e[7]      optimizer.step (sum) time: 2.728403329849243
## epoch[7] training(only) time: 42.66532850265503
# Switched to evaluate mode...
Test: [  0/100]	Time  0.196 ( 0.196)	Loss 7.0166e-01 (7.0166e-01)	Acc@1  76.00 ( 76.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.036 ( 0.051)	Loss 7.1533e-01 (8.3540e-01)	Acc@1  75.00 ( 69.64)	Acc@5  99.00 ( 98.00)
Test: [ 20/100]	Time  0.037 ( 0.044)	Loss 8.7695e-01 (8.6284e-01)	Acc@1  66.00 ( 68.86)	Acc@5  98.00 ( 97.95)
Test: [ 30/100]	Time  0.037 ( 0.042)	Loss 7.9980e-01 (8.7895e-01)	Acc@1  73.00 ( 68.94)	Acc@5  97.00 ( 97.81)
Test: [ 40/100]	Time  0.035 ( 0.041)	Loss 7.6660e-01 (8.7719e-01)	Acc@1  72.00 ( 69.37)	Acc@5  99.00 ( 97.63)
Test: [ 50/100]	Time  0.036 ( 0.040)	Loss 8.4131e-01 (8.6474e-01)	Acc@1  65.00 ( 69.86)	Acc@5  97.00 ( 97.61)
Test: [ 60/100]	Time  0.039 ( 0.039)	Loss 8.1836e-01 (8.7486e-01)	Acc@1  67.00 ( 69.18)	Acc@5 100.00 ( 97.64)
Test: [ 70/100]	Time  0.036 ( 0.039)	Loss 7.9785e-01 (8.7426e-01)	Acc@1  74.00 ( 69.37)	Acc@5  97.00 ( 97.79)
Test: [ 80/100]	Time  0.038 ( 0.039)	Loss 8.8916e-01 (8.7885e-01)	Acc@1  69.00 ( 69.31)	Acc@5  97.00 ( 97.74)
Test: [ 90/100]	Time  0.039 ( 0.039)	Loss 7.3535e-01 (8.7769e-01)	Acc@1  74.00 ( 69.22)	Acc@5  99.00 ( 97.78)
 * Acc@1 69.160 Acc@5 97.730
### epoch[7] execution time: 46.60002613067627
EPOCH 8
# current learning rate: 0.1
# Switched to train mode...
Epoch: [8][  0/391]	Time  0.274 ( 0.274)	Data  0.170 ( 0.170)	Loss 7.3242e-01 (7.3242e-01)	Acc@1  77.34 ( 77.34)	Acc@5  98.44 ( 98.44)
Epoch: [8][ 10/391]	Time  0.110 ( 0.124)	Data  0.001 ( 0.019)	Loss 7.8906e-01 (8.6772e-01)	Acc@1  68.75 ( 69.11)	Acc@5  97.66 ( 97.66)
Epoch: [8][ 20/391]	Time  0.106 ( 0.116)	Data  0.001 ( 0.012)	Loss 8.8867e-01 (8.5054e-01)	Acc@1  69.53 ( 70.35)	Acc@5  96.09 ( 97.43)
Epoch: [8][ 30/391]	Time  0.110 ( 0.114)	Data  0.001 ( 0.009)	Loss 8.0859e-01 (8.5411e-01)	Acc@1  75.00 ( 70.59)	Acc@5  95.31 ( 97.20)
Epoch: [8][ 40/391]	Time  0.106 ( 0.112)	Data  0.002 ( 0.008)	Loss 8.5400e-01 (8.5883e-01)	Acc@1  73.44 ( 70.27)	Acc@5  94.53 ( 97.22)
Epoch: [8][ 50/391]	Time  0.107 ( 0.111)	Data  0.001 ( 0.007)	Loss 8.0957e-01 (8.5757e-01)	Acc@1  71.88 ( 70.16)	Acc@5  99.22 ( 97.38)
Epoch: [8][ 60/391]	Time  0.108 ( 0.111)	Data  0.001 ( 0.007)	Loss 9.1113e-01 (8.6182e-01)	Acc@1  65.62 ( 69.93)	Acc@5  99.22 ( 97.43)
Epoch: [8][ 70/391]	Time  0.106 ( 0.111)	Data  0.001 ( 0.006)	Loss 7.2070e-01 (8.5783e-01)	Acc@1  77.34 ( 70.04)	Acc@5  96.88 ( 97.46)
Epoch: [8][ 80/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.006)	Loss 9.2529e-01 (8.5521e-01)	Acc@1  64.06 ( 70.12)	Acc@5  96.88 ( 97.45)
Epoch: [8][ 90/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.0020e+00 (8.6210e-01)	Acc@1  64.06 ( 69.86)	Acc@5  96.88 ( 97.43)
Epoch: [8][100/391]	Time  0.110 ( 0.110)	Data  0.001 ( 0.006)	Loss 8.1055e-01 (8.6361e-01)	Acc@1  75.78 ( 69.93)	Acc@5  96.09 ( 97.42)
Epoch: [8][110/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 8.8770e-01 (8.6484e-01)	Acc@1  67.19 ( 69.95)	Acc@5  96.09 ( 97.40)
Epoch: [8][120/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.006)	Loss 8.6670e-01 (8.6837e-01)	Acc@1  68.75 ( 69.71)	Acc@5  95.31 ( 97.39)
Epoch: [8][130/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.005)	Loss 7.0166e-01 (8.6650e-01)	Acc@1  75.78 ( 69.87)	Acc@5  99.22 ( 97.41)
Epoch: [8][140/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.3936e-01 (8.6719e-01)	Acc@1  73.44 ( 69.90)	Acc@5  96.09 ( 97.40)
Epoch: [8][150/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.7090e-01 (8.6807e-01)	Acc@1  73.44 ( 69.76)	Acc@5  98.44 ( 97.41)
Epoch: [8][160/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.6035e-01 (8.6502e-01)	Acc@1  66.41 ( 69.80)	Acc@5  96.09 ( 97.47)
Epoch: [8][170/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.9727e-01 (8.6515e-01)	Acc@1  74.22 ( 69.68)	Acc@5  99.22 ( 97.50)
Epoch: [8][180/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.2764e-01 (8.6452e-01)	Acc@1  73.44 ( 69.62)	Acc@5  98.44 ( 97.53)
Epoch: [8][190/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.2764e-01 (8.6362e-01)	Acc@1  66.41 ( 69.59)	Acc@5  98.44 ( 97.55)
Epoch: [8][200/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.2041e-01 (8.6605e-01)	Acc@1  67.97 ( 69.50)	Acc@5  96.88 ( 97.56)
Epoch: [8][210/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.8574e-01 (8.6801e-01)	Acc@1  71.09 ( 69.43)	Acc@5  97.66 ( 97.50)
Epoch: [8][220/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.9150e-01 (8.6629e-01)	Acc@1  67.97 ( 69.49)	Acc@5  99.22 ( 97.49)
Epoch: [8][230/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.4951e-01 (8.6661e-01)	Acc@1  71.09 ( 69.42)	Acc@5  97.66 ( 97.48)
Epoch: [8][240/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.3242e-01 (8.6446e-01)	Acc@1  71.09 ( 69.57)	Acc@5  99.22 ( 97.48)
Epoch: [8][250/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.7979e-01 (8.6608e-01)	Acc@1  69.53 ( 69.50)	Acc@5  97.66 ( 97.49)
Epoch: [8][260/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.7295e-01 (8.6721e-01)	Acc@1  69.53 ( 69.45)	Acc@5  98.44 ( 97.46)
Epoch: [8][270/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.8184e-01 (8.6640e-01)	Acc@1  66.41 ( 69.51)	Acc@5  97.66 ( 97.47)
Epoch: [8][280/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0479e+00 (8.6715e-01)	Acc@1  60.94 ( 69.46)	Acc@5  96.09 ( 97.46)
Epoch: [8][290/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0098e+00 (8.6736e-01)	Acc@1  65.62 ( 69.48)	Acc@5  96.09 ( 97.47)
Epoch: [8][300/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.8027e-01 (8.6669e-01)	Acc@1  72.66 ( 69.48)	Acc@5  99.22 ( 97.46)
Epoch: [8][310/391]	Time  0.104 ( 0.109)	Data  0.002 ( 0.005)	Loss 9.0625e-01 (8.6589e-01)	Acc@1  67.97 ( 69.51)	Acc@5  95.31 ( 97.46)
Epoch: [8][320/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.6523e-01 (8.6430e-01)	Acc@1  69.53 ( 69.58)	Acc@5  98.44 ( 97.47)
Epoch: [8][330/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.8076e-01 (8.6407e-01)	Acc@1  75.00 ( 69.62)	Acc@5  98.44 ( 97.47)
Epoch: [8][340/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.1592e-01 (8.6298e-01)	Acc@1  68.75 ( 69.66)	Acc@5  96.09 ( 97.48)
Epoch: [8][350/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.4082e-01 (8.6194e-01)	Acc@1  69.53 ( 69.69)	Acc@5  96.88 ( 97.48)
Epoch: [8][360/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.1104e-01 (8.6114e-01)	Acc@1  70.31 ( 69.72)	Acc@5  96.88 ( 97.49)
Epoch: [8][370/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.1826e-01 (8.6063e-01)	Acc@1  74.22 ( 69.80)	Acc@5  98.44 ( 97.49)
Epoch: [8][380/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.4814e-01 (8.6040e-01)	Acc@1  71.88 ( 69.80)	Acc@5  97.66 ( 97.52)
Epoch: [8][390/391]	Time  0.099 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.7236e-01 (8.5902e-01)	Acc@1  78.75 ( 69.83)	Acc@5  98.75 ( 97.54)
## e[8] optimizer.zero_grad (sum) time: 0.34647488594055176
## e[8]       loss.backward (sum) time: 13.623136043548584
## e[8]      optimizer.step (sum) time: 2.778439998626709
## epoch[8] training(only) time: 42.586268186569214
# Switched to evaluate mode...
Test: [  0/100]	Time  0.179 ( 0.179)	Loss 6.2695e-01 (6.2695e-01)	Acc@1  79.00 ( 79.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.038 ( 0.050)	Loss 7.5635e-01 (7.7401e-01)	Acc@1  69.00 ( 71.82)	Acc@5  97.00 ( 98.82)
Test: [ 20/100]	Time  0.039 ( 0.044)	Loss 7.8662e-01 (8.1187e-01)	Acc@1  72.00 ( 70.24)	Acc@5  98.00 ( 98.57)
Test: [ 30/100]	Time  0.040 ( 0.042)	Loss 8.0908e-01 (8.2283e-01)	Acc@1  68.00 ( 70.52)	Acc@5  99.00 ( 98.32)
Test: [ 40/100]	Time  0.037 ( 0.041)	Loss 7.1191e-01 (8.2260e-01)	Acc@1  74.00 ( 70.34)	Acc@5  97.00 ( 98.02)
Test: [ 50/100]	Time  0.037 ( 0.040)	Loss 6.6797e-01 (8.1230e-01)	Acc@1  79.00 ( 71.12)	Acc@5 100.00 ( 98.16)
Test: [ 60/100]	Time  0.036 ( 0.040)	Loss 7.1631e-01 (8.1970e-01)	Acc@1  74.00 ( 70.98)	Acc@5 100.00 ( 98.03)
Test: [ 70/100]	Time  0.039 ( 0.039)	Loss 8.6768e-01 (8.1831e-01)	Acc@1  69.00 ( 71.08)	Acc@5  96.00 ( 98.00)
Test: [ 80/100]	Time  0.039 ( 0.039)	Loss 7.3389e-01 (8.2204e-01)	Acc@1  74.00 ( 71.16)	Acc@5  97.00 ( 97.89)
Test: [ 90/100]	Time  0.036 ( 0.039)	Loss 6.8750e-01 (8.2300e-01)	Acc@1  76.00 ( 71.23)	Acc@5  99.00 ( 97.91)
 * Acc@1 71.290 Acc@5 97.950
### epoch[8] execution time: 46.55073356628418
EPOCH 9
# current learning rate: 0.1
# Switched to train mode...
Epoch: [9][  0/391]	Time  0.263 ( 0.263)	Data  0.166 ( 0.166)	Loss 8.5742e-01 (8.5742e-01)	Acc@1  74.22 ( 74.22)	Acc@5  95.31 ( 95.31)
Epoch: [9][ 10/391]	Time  0.109 ( 0.121)	Data  0.001 ( 0.018)	Loss 6.7139e-01 (7.9066e-01)	Acc@1  75.78 ( 71.88)	Acc@5  98.44 ( 97.80)
Epoch: [9][ 20/391]	Time  0.108 ( 0.114)	Data  0.001 ( 0.011)	Loss 7.3633e-01 (7.9918e-01)	Acc@1  72.66 ( 71.39)	Acc@5  98.44 ( 97.84)
Epoch: [9][ 30/391]	Time  0.105 ( 0.112)	Data  0.001 ( 0.009)	Loss 7.8613e-01 (7.9212e-01)	Acc@1  71.09 ( 71.82)	Acc@5  98.44 ( 97.86)
Epoch: [9][ 40/391]	Time  0.110 ( 0.111)	Data  0.001 ( 0.008)	Loss 7.5342e-01 (7.9158e-01)	Acc@1  74.22 ( 71.70)	Acc@5  97.66 ( 97.87)
Epoch: [9][ 50/391]	Time  0.105 ( 0.111)	Data  0.001 ( 0.007)	Loss 8.8281e-01 (7.9862e-01)	Acc@1  69.53 ( 71.94)	Acc@5  96.88 ( 97.75)
Epoch: [9][ 60/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.007)	Loss 7.9053e-01 (7.9788e-01)	Acc@1  67.97 ( 71.71)	Acc@5  99.22 ( 97.81)
Epoch: [9][ 70/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.0576e+00 (8.0269e-01)	Acc@1  60.94 ( 71.70)	Acc@5  98.44 ( 97.73)
Epoch: [9][ 80/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.006)	Loss 8.0469e-01 (8.0134e-01)	Acc@1  67.97 ( 71.75)	Acc@5  99.22 ( 97.84)
Epoch: [9][ 90/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.006)	Loss 7.9248e-01 (8.0334e-01)	Acc@1  71.88 ( 71.60)	Acc@5  96.88 ( 97.85)
Epoch: [9][100/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.006)	Loss 9.1992e-01 (8.0785e-01)	Acc@1  72.66 ( 71.54)	Acc@5  98.44 ( 97.83)
Epoch: [9][110/391]	Time  0.107 ( 0.109)	Data  0.002 ( 0.005)	Loss 7.7979e-01 (8.0737e-01)	Acc@1  75.00 ( 71.60)	Acc@5  97.66 ( 97.83)
Epoch: [9][120/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1270e+00 (8.0948e-01)	Acc@1  63.28 ( 71.64)	Acc@5  96.88 ( 97.80)
Epoch: [9][130/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.5508e-01 (8.1446e-01)	Acc@1  67.19 ( 71.45)	Acc@5  96.88 ( 97.80)
Epoch: [9][140/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.9453e-01 (8.1537e-01)	Acc@1  67.97 ( 71.33)	Acc@5  96.88 ( 97.82)
Epoch: [9][150/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.1211e-01 (8.1597e-01)	Acc@1  67.97 ( 71.33)	Acc@5  98.44 ( 97.84)
Epoch: [9][160/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.3643e-01 (8.1396e-01)	Acc@1  73.44 ( 71.41)	Acc@5  98.44 ( 97.86)
Epoch: [9][170/391]	Time  0.104 ( 0.109)	Data  0.002 ( 0.005)	Loss 8.5596e-01 (8.1304e-01)	Acc@1  67.19 ( 71.42)	Acc@5  98.44 ( 97.88)
Epoch: [9][180/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.1045e-01 (8.0990e-01)	Acc@1  71.09 ( 71.55)	Acc@5  98.44 ( 97.89)
Epoch: [9][190/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.1582e-01 (8.0901e-01)	Acc@1  75.78 ( 71.57)	Acc@5  94.53 ( 97.88)
Epoch: [9][200/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.3838e-01 (8.0707e-01)	Acc@1  75.78 ( 71.71)	Acc@5  97.66 ( 97.91)
Epoch: [9][210/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.1982e-01 (8.0701e-01)	Acc@1  73.44 ( 71.71)	Acc@5  95.31 ( 97.89)
Epoch: [9][220/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.1387e-01 (8.0698e-01)	Acc@1  71.88 ( 71.69)	Acc@5 100.00 ( 97.94)
Epoch: [9][230/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.6572e-01 (8.0530e-01)	Acc@1  67.97 ( 71.78)	Acc@5  96.88 ( 97.93)
Epoch: [9][240/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.1797e-01 (8.0561e-01)	Acc@1  72.66 ( 71.77)	Acc@5  96.09 ( 97.92)
Epoch: [9][250/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.5244e-01 (8.0545e-01)	Acc@1  77.34 ( 71.81)	Acc@5  97.66 ( 97.92)
Epoch: [9][260/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.8086e-01 (8.0495e-01)	Acc@1  67.19 ( 71.77)	Acc@5  96.09 ( 97.93)
Epoch: [9][270/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.8574e-01 (8.0550e-01)	Acc@1  69.53 ( 71.77)	Acc@5  98.44 ( 97.91)
Epoch: [9][280/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.9775e-01 (8.0442e-01)	Acc@1  78.12 ( 71.85)	Acc@5  97.66 ( 97.91)
Epoch: [9][290/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.2373e-01 (8.0250e-01)	Acc@1  69.53 ( 71.90)	Acc@5  99.22 ( 97.92)
Epoch: [9][300/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.9932e-01 (8.0298e-01)	Acc@1  68.75 ( 71.87)	Acc@5  96.88 ( 97.91)
Epoch: [9][310/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.6885e-01 (8.0134e-01)	Acc@1  81.25 ( 71.91)	Acc@5 100.00 ( 97.92)
Epoch: [9][320/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.5459e-01 (8.0183e-01)	Acc@1  67.97 ( 71.91)	Acc@5  96.88 ( 97.90)
Epoch: [9][330/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.6406e-01 (8.0103e-01)	Acc@1  76.56 ( 71.93)	Acc@5  98.44 ( 97.92)
Epoch: [9][340/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.2227e-01 (8.0113e-01)	Acc@1  69.53 ( 71.91)	Acc@5  97.66 ( 97.90)
Epoch: [9][350/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.1338e-01 (8.0172e-01)	Acc@1  75.00 ( 71.94)	Acc@5  97.66 ( 97.90)
Epoch: [9][360/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.6172e-01 (8.0190e-01)	Acc@1  72.66 ( 71.91)	Acc@5  96.88 ( 97.89)
Epoch: [9][370/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.7686e-01 (8.0084e-01)	Acc@1  71.88 ( 71.94)	Acc@5  99.22 ( 97.90)
Epoch: [9][380/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.9609e-01 (8.0001e-01)	Acc@1  67.19 ( 71.95)	Acc@5  96.88 ( 97.90)
Epoch: [9][390/391]	Time  0.100 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.7842e-01 (8.0078e-01)	Acc@1  67.50 ( 71.95)	Acc@5  96.25 ( 97.88)
## e[9] optimizer.zero_grad (sum) time: 0.34139299392700195
## e[9]       loss.backward (sum) time: 13.637564659118652
## e[9]      optimizer.step (sum) time: 2.7761428356170654
## epoch[9] training(only) time: 42.722737550735474
# Switched to evaluate mode...
Test: [  0/100]	Time  0.192 ( 0.192)	Loss 1.0029e+00 (1.0029e+00)	Acc@1  71.00 ( 71.00)	Acc@5  97.00 ( 97.00)
Test: [ 10/100]	Time  0.038 ( 0.051)	Loss 6.9287e-01 (8.5405e-01)	Acc@1  78.00 ( 72.09)	Acc@5  98.00 ( 98.27)
Test: [ 20/100]	Time  0.036 ( 0.045)	Loss 9.0527e-01 (9.3297e-01)	Acc@1  68.00 ( 70.71)	Acc@5  99.00 ( 98.14)
Test: [ 30/100]	Time  0.038 ( 0.042)	Loss 7.4121e-01 (9.1591e-01)	Acc@1  77.00 ( 70.61)	Acc@5  98.00 ( 98.10)
Test: [ 40/100]	Time  0.039 ( 0.041)	Loss 7.5439e-01 (9.1499e-01)	Acc@1  73.00 ( 70.66)	Acc@5 100.00 ( 97.83)
Test: [ 50/100]	Time  0.038 ( 0.040)	Loss 9.6924e-01 (8.9035e-01)	Acc@1  70.00 ( 71.41)	Acc@5  97.00 ( 97.75)
Test: [ 60/100]	Time  0.036 ( 0.040)	Loss 7.7734e-01 (8.8781e-01)	Acc@1  71.00 ( 71.13)	Acc@5 100.00 ( 97.74)
Test: [ 70/100]	Time  0.036 ( 0.039)	Loss 8.1250e-01 (8.9221e-01)	Acc@1  75.00 ( 71.01)	Acc@5  99.00 ( 97.85)
Test: [ 80/100]	Time  0.036 ( 0.039)	Loss 8.7939e-01 (9.1058e-01)	Acc@1  71.00 ( 70.81)	Acc@5  97.00 ( 97.89)
Test: [ 90/100]	Time  0.038 ( 0.039)	Loss 6.2549e-01 (9.0065e-01)	Acc@1  79.00 ( 71.00)	Acc@5 100.00 ( 97.91)
 * Acc@1 71.080 Acc@5 97.870
### epoch[9] execution time: 46.674925327301025
EPOCH 10
# current learning rate: 0.1
# Switched to train mode...
Epoch: [10][  0/391]	Time  0.271 ( 0.271)	Data  0.151 ( 0.151)	Loss 8.5547e-01 (8.5547e-01)	Acc@1  67.97 ( 67.97)	Acc@5  99.22 ( 99.22)
Epoch: [10][ 10/391]	Time  0.106 ( 0.123)	Data  0.001 ( 0.017)	Loss 6.9434e-01 (7.8591e-01)	Acc@1  74.22 ( 72.09)	Acc@5  99.22 ( 98.51)
Epoch: [10][ 20/391]	Time  0.110 ( 0.116)	Data  0.001 ( 0.011)	Loss 8.4521e-01 (7.7713e-01)	Acc@1  70.31 ( 72.58)	Acc@5  96.88 ( 98.33)
Epoch: [10][ 30/391]	Time  0.106 ( 0.113)	Data  0.001 ( 0.009)	Loss 8.1396e-01 (7.7774e-01)	Acc@1  71.09 ( 72.35)	Acc@5  97.66 ( 98.21)
Epoch: [10][ 40/391]	Time  0.106 ( 0.112)	Data  0.001 ( 0.008)	Loss 6.6211e-01 (7.7651e-01)	Acc@1  77.34 ( 72.75)	Acc@5 100.00 ( 98.13)
Epoch: [10][ 50/391]	Time  0.105 ( 0.111)	Data  0.001 ( 0.007)	Loss 7.6123e-01 (7.7068e-01)	Acc@1  73.44 ( 72.73)	Acc@5  98.44 ( 98.27)
Epoch: [10][ 60/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 6.6260e-01 (7.6479e-01)	Acc@1  76.56 ( 72.91)	Acc@5  99.22 ( 98.31)
Epoch: [10][ 70/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.006)	Loss 8.2080e-01 (7.5914e-01)	Acc@1  72.66 ( 73.00)	Acc@5  96.88 ( 98.28)
Epoch: [10][ 80/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.006)	Loss 7.2461e-01 (7.6012e-01)	Acc@1  70.31 ( 72.90)	Acc@5  99.22 ( 98.33)
Epoch: [10][ 90/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.006)	Loss 7.8467e-01 (7.6167e-01)	Acc@1  74.22 ( 72.97)	Acc@5  98.44 ( 98.33)
Epoch: [10][100/391]	Time  0.110 ( 0.110)	Data  0.001 ( 0.006)	Loss 9.0088e-01 (7.6339e-01)	Acc@1  69.53 ( 73.01)	Acc@5  98.44 ( 98.31)
Epoch: [10][110/391]	Time  0.109 ( 0.110)	Data  0.001 ( 0.005)	Loss 6.5967e-01 (7.6109e-01)	Acc@1  78.12 ( 73.08)	Acc@5 100.00 ( 98.32)
Epoch: [10][120/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.2725e-01 (7.6488e-01)	Acc@1  68.75 ( 72.83)	Acc@5  96.09 ( 98.31)
Epoch: [10][130/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.8467e-01 (7.6847e-01)	Acc@1  74.22 ( 72.82)	Acc@5  98.44 ( 98.26)
Epoch: [10][140/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.3877e-01 (7.6904e-01)	Acc@1  73.44 ( 72.81)	Acc@5  98.44 ( 98.22)
Epoch: [10][150/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.4570e-01 (7.6676e-01)	Acc@1  69.53 ( 72.89)	Acc@5  98.44 ( 98.25)
Epoch: [10][160/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.2373e-01 (7.6874e-01)	Acc@1  72.66 ( 72.85)	Acc@5  97.66 ( 98.19)
Epoch: [10][170/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.1729e-01 (7.6897e-01)	Acc@1  76.56 ( 72.81)	Acc@5  99.22 ( 98.20)
Epoch: [10][180/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.1924e-01 (7.7058e-01)	Acc@1  71.88 ( 72.73)	Acc@5  99.22 ( 98.20)
Epoch: [10][190/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.8887e-01 (7.6953e-01)	Acc@1  77.34 ( 72.78)	Acc@5  99.22 ( 98.18)
Epoch: [10][200/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.3330e-01 (7.6783e-01)	Acc@1  76.56 ( 72.81)	Acc@5  97.66 ( 98.18)
Epoch: [10][210/391]	Time  0.111 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.3682e-01 (7.6701e-01)	Acc@1  75.00 ( 72.85)	Acc@5 100.00 ( 98.19)
Epoch: [10][220/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.1729e-01 (7.6564e-01)	Acc@1  75.00 ( 72.91)	Acc@5  99.22 ( 98.17)
Epoch: [10][230/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.4531e-01 (7.6530e-01)	Acc@1  63.28 ( 72.93)	Acc@5  97.66 ( 98.17)
Epoch: [10][240/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.0176e-01 (7.6581e-01)	Acc@1  75.00 ( 72.92)	Acc@5  98.44 ( 98.15)
Epoch: [10][250/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.9971e-01 (7.6587e-01)	Acc@1  77.34 ( 72.95)	Acc@5  97.66 ( 98.14)
Epoch: [10][260/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.6406e-01 (7.6392e-01)	Acc@1  77.34 ( 73.03)	Acc@5  98.44 ( 98.13)
Epoch: [10][270/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.2021e-01 (7.6102e-01)	Acc@1  76.56 ( 73.13)	Acc@5  99.22 ( 98.15)
Epoch: [10][280/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.9258e-01 (7.6025e-01)	Acc@1  69.53 ( 73.17)	Acc@5  98.44 ( 98.14)
Epoch: [10][290/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.9414e-01 (7.6047e-01)	Acc@1  65.62 ( 73.17)	Acc@5  96.09 ( 98.13)
Epoch: [10][300/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.1641e-01 (7.5954e-01)	Acc@1  68.75 ( 73.18)	Acc@5  98.44 ( 98.15)
Epoch: [10][310/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.3643e-01 (7.5837e-01)	Acc@1  67.97 ( 73.26)	Acc@5  98.44 ( 98.16)
Epoch: [10][320/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.1982e-01 (7.5876e-01)	Acc@1  69.53 ( 73.24)	Acc@5  98.44 ( 98.16)
Epoch: [10][330/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.5625e-01 (7.5663e-01)	Acc@1  79.69 ( 73.34)	Acc@5  97.66 ( 98.17)
Epoch: [10][340/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.8750e-01 (7.5614e-01)	Acc@1  73.44 ( 73.34)	Acc@5  98.44 ( 98.16)
Epoch: [10][350/391]	Time  0.107 ( 0.109)	Data  0.002 ( 0.005)	Loss 6.7969e-01 (7.5545e-01)	Acc@1  80.47 ( 73.41)	Acc@5  97.66 ( 98.15)
Epoch: [10][360/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.1289e-01 (7.5529e-01)	Acc@1  74.22 ( 73.39)	Acc@5  98.44 ( 98.17)
Epoch: [10][370/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.1816e-01 (7.5490e-01)	Acc@1  77.34 ( 73.41)	Acc@5  99.22 ( 98.19)
Epoch: [10][380/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.0107e-01 (7.5313e-01)	Acc@1  78.91 ( 73.48)	Acc@5  99.22 ( 98.21)
Epoch: [10][390/391]	Time  0.100 ( 0.109)	Data  0.001 ( 0.004)	Loss 7.5977e-01 (7.5257e-01)	Acc@1  75.00 ( 73.49)	Acc@5  97.50 ( 98.21)
## e[10] optimizer.zero_grad (sum) time: 0.34657716751098633
## e[10]       loss.backward (sum) time: 13.666624307632446
## e[10]      optimizer.step (sum) time: 2.7623202800750732
## epoch[10] training(only) time: 42.62916350364685
# Switched to evaluate mode...
Test: [  0/100]	Time  0.179 ( 0.179)	Loss 7.9004e-01 (7.9004e-01)	Acc@1  77.00 ( 77.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.037 ( 0.051)	Loss 7.9492e-01 (8.0966e-01)	Acc@1  75.00 ( 74.18)	Acc@5  97.00 ( 98.82)
Test: [ 20/100]	Time  0.035 ( 0.044)	Loss 7.9834e-01 (8.6609e-01)	Acc@1  73.00 ( 72.71)	Acc@5  98.00 ( 98.52)
Test: [ 30/100]	Time  0.036 ( 0.042)	Loss 6.9043e-01 (8.3600e-01)	Acc@1  73.00 ( 73.42)	Acc@5  98.00 ( 98.39)
Test: [ 40/100]	Time  0.037 ( 0.041)	Loss 6.9287e-01 (8.2664e-01)	Acc@1  78.00 ( 73.44)	Acc@5  98.00 ( 98.24)
Test: [ 50/100]	Time  0.035 ( 0.040)	Loss 7.4512e-01 (8.1051e-01)	Acc@1  77.00 ( 73.73)	Acc@5  98.00 ( 98.31)
Test: [ 60/100]	Time  0.038 ( 0.040)	Loss 6.2012e-01 (7.9786e-01)	Acc@1  77.00 ( 73.95)	Acc@5 100.00 ( 98.33)
Test: [ 70/100]	Time  0.035 ( 0.039)	Loss 7.4170e-01 (8.0299e-01)	Acc@1  79.00 ( 73.87)	Acc@5  98.00 ( 98.37)
Test: [ 80/100]	Time  0.036 ( 0.039)	Loss 7.1533e-01 (8.0863e-01)	Acc@1  71.00 ( 73.67)	Acc@5  98.00 ( 98.38)
Test: [ 90/100]	Time  0.036 ( 0.039)	Loss 6.7334e-01 (8.1411e-01)	Acc@1  75.00 ( 73.34)	Acc@5 100.00 ( 98.33)
 * Acc@1 73.410 Acc@5 98.330
### epoch[10] execution time: 46.60930609703064
EPOCH 11
# current learning rate: 0.1
# Switched to train mode...
Epoch: [11][  0/391]	Time  0.270 ( 0.270)	Data  0.177 ( 0.177)	Loss 6.6748e-01 (6.6748e-01)	Acc@1  78.91 ( 78.91)	Acc@5  97.66 ( 97.66)
Epoch: [11][ 10/391]	Time  0.107 ( 0.123)	Data  0.001 ( 0.020)	Loss 6.6650e-01 (7.0752e-01)	Acc@1  80.47 ( 76.49)	Acc@5  98.44 ( 98.08)
Epoch: [11][ 20/391]	Time  0.107 ( 0.115)	Data  0.001 ( 0.012)	Loss 6.6846e-01 (6.9929e-01)	Acc@1  73.44 ( 75.63)	Acc@5  97.66 ( 98.25)
Epoch: [11][ 30/391]	Time  0.108 ( 0.113)	Data  0.001 ( 0.010)	Loss 7.7832e-01 (6.9796e-01)	Acc@1  71.88 ( 75.40)	Acc@5  99.22 ( 98.34)
Epoch: [11][ 40/391]	Time  0.106 ( 0.112)	Data  0.001 ( 0.008)	Loss 7.6709e-01 (7.0597e-01)	Acc@1  71.09 ( 75.36)	Acc@5  97.66 ( 98.25)
Epoch: [11][ 50/391]	Time  0.108 ( 0.111)	Data  0.001 ( 0.008)	Loss 7.2266e-01 (7.0798e-01)	Acc@1  75.00 ( 75.40)	Acc@5  99.22 ( 98.19)
Epoch: [11][ 60/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.007)	Loss 5.5176e-01 (6.9795e-01)	Acc@1  84.38 ( 75.87)	Acc@5 100.00 ( 98.28)
Epoch: [11][ 70/391]	Time  0.109 ( 0.110)	Data  0.001 ( 0.007)	Loss 7.4463e-01 (6.9398e-01)	Acc@1  71.88 ( 75.88)	Acc@5  96.88 ( 98.28)
Epoch: [11][ 80/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 6.7139e-01 (6.9383e-01)	Acc@1  75.78 ( 75.87)	Acc@5  99.22 ( 98.34)
Epoch: [11][ 90/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 9.5557e-01 (7.0037e-01)	Acc@1  67.97 ( 75.67)	Acc@5  94.53 ( 98.23)
Epoch: [11][100/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.006)	Loss 6.2305e-01 (6.9526e-01)	Acc@1  78.12 ( 75.84)	Acc@5  99.22 ( 98.28)
Epoch: [11][110/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.006)	Loss 7.9102e-01 (6.9922e-01)	Acc@1  74.22 ( 75.62)	Acc@5  97.66 ( 98.30)
Epoch: [11][120/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.006)	Loss 6.3818e-01 (7.0202e-01)	Acc@1  82.03 ( 75.59)	Acc@5  98.44 ( 98.33)
Epoch: [11][130/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.3369e-01 (7.0135e-01)	Acc@1  84.38 ( 75.57)	Acc@5  99.22 ( 98.35)
Epoch: [11][140/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.3828e-01 (7.0259e-01)	Acc@1  79.69 ( 75.65)	Acc@5  97.66 ( 98.33)
Epoch: [11][150/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.3486e-01 (7.0230e-01)	Acc@1  76.56 ( 75.70)	Acc@5  98.44 ( 98.35)
Epoch: [11][160/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.4717e-01 (7.0414e-01)	Acc@1  73.44 ( 75.63)	Acc@5  96.88 ( 98.35)
Epoch: [11][170/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.5762e-01 (7.0575e-01)	Acc@1  78.12 ( 75.59)	Acc@5  98.44 ( 98.31)
Epoch: [11][180/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.9775e-01 (7.0585e-01)	Acc@1  74.22 ( 75.55)	Acc@5  97.66 ( 98.33)
Epoch: [11][190/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.5771e-01 (7.0514e-01)	Acc@1  78.91 ( 75.60)	Acc@5  99.22 ( 98.33)
Epoch: [11][200/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.9287e-01 (7.0787e-01)	Acc@1  80.47 ( 75.46)	Acc@5 100.00 ( 98.32)
Epoch: [11][210/391]	Time  0.100 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.0273e-01 (7.0716e-01)	Acc@1  72.66 ( 75.44)	Acc@5  98.44 ( 98.32)
Epoch: [11][220/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.8789e-01 (7.0544e-01)	Acc@1  78.91 ( 75.47)	Acc@5  98.44 ( 98.33)
Epoch: [11][230/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.2900e-01 (7.0429e-01)	Acc@1  71.88 ( 75.48)	Acc@5  98.44 ( 98.37)
Epoch: [11][240/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.5010e-01 (7.0325e-01)	Acc@1  68.75 ( 75.50)	Acc@5  96.88 ( 98.38)
Epoch: [11][250/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.7061e-01 (7.0356e-01)	Acc@1  67.97 ( 75.46)	Acc@5  97.66 ( 98.37)
Epoch: [11][260/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.4316e-01 (7.0402e-01)	Acc@1  75.78 ( 75.46)	Acc@5  98.44 ( 98.36)
Epoch: [11][270/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.6982e-01 (7.0411e-01)	Acc@1  76.56 ( 75.46)	Acc@5  99.22 ( 98.37)
Epoch: [11][280/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.6152e-01 (7.0448e-01)	Acc@1  74.22 ( 75.43)	Acc@5  99.22 ( 98.36)
Epoch: [11][290/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.7207e-01 (7.0558e-01)	Acc@1  70.31 ( 75.36)	Acc@5  96.88 ( 98.35)
Epoch: [11][300/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.7627e-01 (7.0599e-01)	Acc@1  79.69 ( 75.38)	Acc@5  97.66 ( 98.34)
Epoch: [11][310/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.0859e-01 (7.0641e-01)	Acc@1  71.88 ( 75.34)	Acc@5  96.88 ( 98.36)
Epoch: [11][320/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.3477e-01 (7.0555e-01)	Acc@1  77.34 ( 75.37)	Acc@5  98.44 ( 98.38)
Epoch: [11][330/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.3721e-01 (7.0550e-01)	Acc@1  77.34 ( 75.35)	Acc@5  96.88 ( 98.38)
Epoch: [11][340/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.3730e-01 (7.0509e-01)	Acc@1  77.34 ( 75.38)	Acc@5 100.00 ( 98.39)
Epoch: [11][350/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.2256e-01 (7.0516e-01)	Acc@1  72.66 ( 75.36)	Acc@5  98.44 ( 98.40)
Epoch: [11][360/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.7974e-01 (7.0608e-01)	Acc@1  82.81 ( 75.30)	Acc@5  99.22 ( 98.40)
Epoch: [11][370/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.6309e-01 (7.0552e-01)	Acc@1  74.22 ( 75.31)	Acc@5 100.00 ( 98.41)
Epoch: [11][380/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.3076e-01 (7.0447e-01)	Acc@1  81.25 ( 75.35)	Acc@5  98.44 ( 98.40)
Epoch: [11][390/391]	Time  0.095 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.1396e-01 (7.0442e-01)	Acc@1  68.75 ( 75.37)	Acc@5  98.75 ( 98.40)
## e[11] optimizer.zero_grad (sum) time: 0.3530876636505127
## e[11]       loss.backward (sum) time: 13.64006233215332
## e[11]      optimizer.step (sum) time: 2.6918609142303467
## epoch[11] training(only) time: 42.61684703826904
# Switched to evaluate mode...
Test: [  0/100]	Time  0.176 ( 0.176)	Loss 7.4170e-01 (7.4170e-01)	Acc@1  75.00 ( 75.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.036 ( 0.050)	Loss 7.8662e-01 (7.0097e-01)	Acc@1  76.00 ( 75.73)	Acc@5  98.00 ( 98.82)
Test: [ 20/100]	Time  0.037 ( 0.044)	Loss 6.0791e-01 (7.2181e-01)	Acc@1  76.00 ( 75.29)	Acc@5  99.00 ( 98.67)
Test: [ 30/100]	Time  0.036 ( 0.041)	Loss 6.2695e-01 (7.1906e-01)	Acc@1  75.00 ( 75.68)	Acc@5  98.00 ( 98.61)
Test: [ 40/100]	Time  0.037 ( 0.040)	Loss 6.7725e-01 (7.1939e-01)	Acc@1  78.00 ( 75.73)	Acc@5  99.00 ( 98.56)
Test: [ 50/100]	Time  0.037 ( 0.040)	Loss 6.3428e-01 (7.1491e-01)	Acc@1  76.00 ( 75.94)	Acc@5  99.00 ( 98.61)
Test: [ 60/100]	Time  0.038 ( 0.040)	Loss 6.4795e-01 (7.1307e-01)	Acc@1  80.00 ( 76.08)	Acc@5  99.00 ( 98.57)
Test: [ 70/100]	Time  0.038 ( 0.039)	Loss 7.6807e-01 (7.1844e-01)	Acc@1  74.00 ( 76.06)	Acc@5  98.00 ( 98.59)
Test: [ 80/100]	Time  0.037 ( 0.039)	Loss 7.4023e-01 (7.1920e-01)	Acc@1  73.00 ( 75.85)	Acc@5  98.00 ( 98.63)
Test: [ 90/100]	Time  0.036 ( 0.039)	Loss 5.7568e-01 (7.2235e-01)	Acc@1  75.00 ( 75.69)	Acc@5 100.00 ( 98.59)
 * Acc@1 75.590 Acc@5 98.630
### epoch[11] execution time: 46.578571796417236
EPOCH 12
# current learning rate: 0.1
# Switched to train mode...
Epoch: [12][  0/391]	Time  0.276 ( 0.276)	Data  0.181 ( 0.181)	Loss 5.6689e-01 (5.6689e-01)	Acc@1  81.25 ( 81.25)	Acc@5  99.22 ( 99.22)
Epoch: [12][ 10/391]	Time  0.105 ( 0.123)	Data  0.001 ( 0.020)	Loss 5.6885e-01 (6.3086e-01)	Acc@1  77.34 ( 78.05)	Acc@5  98.44 ( 99.22)
Epoch: [12][ 20/391]	Time  0.109 ( 0.116)	Data  0.001 ( 0.013)	Loss 6.0156e-01 (6.4751e-01)	Acc@1  75.78 ( 77.49)	Acc@5  99.22 ( 98.81)
Epoch: [12][ 30/391]	Time  0.106 ( 0.113)	Data  0.002 ( 0.010)	Loss 7.8516e-01 (6.5033e-01)	Acc@1  75.00 ( 77.42)	Acc@5  97.66 ( 98.69)
Epoch: [12][ 40/391]	Time  0.107 ( 0.112)	Data  0.001 ( 0.008)	Loss 6.8262e-01 (6.5255e-01)	Acc@1  79.69 ( 77.25)	Acc@5  97.66 ( 98.59)
Epoch: [12][ 50/391]	Time  0.107 ( 0.111)	Data  0.001 ( 0.008)	Loss 6.2842e-01 (6.5566e-01)	Acc@1  81.25 ( 77.27)	Acc@5  97.66 ( 98.59)
Epoch: [12][ 60/391]	Time  0.105 ( 0.111)	Data  0.001 ( 0.007)	Loss 5.5322e-01 (6.6067e-01)	Acc@1  82.81 ( 77.23)	Acc@5  97.66 ( 98.54)
Epoch: [12][ 70/391]	Time  0.103 ( 0.110)	Data  0.001 ( 0.007)	Loss 5.9473e-01 (6.5917e-01)	Acc@1  75.78 ( 77.05)	Acc@5  98.44 ( 98.54)
Epoch: [12][ 80/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.006)	Loss 6.0254e-01 (6.6173e-01)	Acc@1  79.69 ( 77.04)	Acc@5  99.22 ( 98.51)
Epoch: [12][ 90/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 5.5566e-01 (6.6909e-01)	Acc@1  81.25 ( 76.83)	Acc@5 100.00 ( 98.48)
Epoch: [12][100/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 7.0166e-01 (6.6346e-01)	Acc@1  75.00 ( 77.00)	Acc@5 100.00 ( 98.55)
Epoch: [12][110/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.006)	Loss 6.7285e-01 (6.6644e-01)	Acc@1  74.22 ( 76.92)	Acc@5  97.66 ( 98.54)
Epoch: [12][120/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.006)	Loss 6.3623e-01 (6.6777e-01)	Acc@1  74.22 ( 76.72)	Acc@5 100.00 ( 98.58)
Epoch: [12][130/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.006)	Loss 5.4736e-01 (6.6939e-01)	Acc@1  78.12 ( 76.61)	Acc@5 100.00 ( 98.63)
Epoch: [12][140/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.6113e-01 (6.6910e-01)	Acc@1  75.78 ( 76.57)	Acc@5  97.66 ( 98.68)
Epoch: [12][150/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.9053e-01 (6.7161e-01)	Acc@1  67.97 ( 76.44)	Acc@5  99.22 ( 98.70)
Epoch: [12][160/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.7197e-01 (6.7611e-01)	Acc@1  72.66 ( 76.31)	Acc@5  98.44 ( 98.66)
Epoch: [12][170/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.0381e-01 (6.7556e-01)	Acc@1  67.19 ( 76.30)	Acc@5  95.31 ( 98.66)
Epoch: [12][180/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.0850e-01 (6.7435e-01)	Acc@1  77.34 ( 76.33)	Acc@5  99.22 ( 98.65)
Epoch: [12][190/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.9766e-01 (6.7308e-01)	Acc@1  77.34 ( 76.30)	Acc@5  99.22 ( 98.67)
Epoch: [12][200/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.3145e-01 (6.7353e-01)	Acc@1  69.53 ( 76.25)	Acc@5  99.22 ( 98.66)
Epoch: [12][210/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.9277e-01 (6.7156e-01)	Acc@1  84.38 ( 76.38)	Acc@5  99.22 ( 98.66)
Epoch: [12][220/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.2695e-01 (6.7176e-01)	Acc@1  75.78 ( 76.39)	Acc@5  98.44 ( 98.65)
Epoch: [12][230/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.8594e-01 (6.6875e-01)	Acc@1  82.03 ( 76.49)	Acc@5  97.66 ( 98.63)
Epoch: [12][240/391]	Time  0.111 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.6055e-01 (6.6836e-01)	Acc@1  81.25 ( 76.54)	Acc@5  98.44 ( 98.60)
Epoch: [12][250/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.4717e-01 (6.7067e-01)	Acc@1  71.88 ( 76.48)	Acc@5  97.66 ( 98.59)
Epoch: [12][260/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.1035e-01 (6.7325e-01)	Acc@1  81.25 ( 76.39)	Acc@5  98.44 ( 98.56)
Epoch: [12][270/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.4219e-01 (6.7176e-01)	Acc@1  76.56 ( 76.47)	Acc@5  98.44 ( 98.59)
Epoch: [12][280/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.3525e-01 (6.6992e-01)	Acc@1  78.91 ( 76.52)	Acc@5  99.22 ( 98.59)
Epoch: [12][290/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.2109e-01 (6.6994e-01)	Acc@1  80.47 ( 76.55)	Acc@5  99.22 ( 98.59)
Epoch: [12][300/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.9189e-01 (6.6935e-01)	Acc@1  75.78 ( 76.53)	Acc@5 100.00 ( 98.60)
Epoch: [12][310/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.1484e-01 (6.6899e-01)	Acc@1  75.78 ( 76.55)	Acc@5  97.66 ( 98.58)
Epoch: [12][320/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.3662e-01 (6.6762e-01)	Acc@1  82.81 ( 76.62)	Acc@5  98.44 ( 98.59)
Epoch: [12][330/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.7422e-01 (6.6614e-01)	Acc@1  78.91 ( 76.68)	Acc@5  98.44 ( 98.60)
Epoch: [12][340/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.9912e-01 (6.6596e-01)	Acc@1  76.56 ( 76.67)	Acc@5  99.22 ( 98.61)
Epoch: [12][350/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.7393e-01 (6.6679e-01)	Acc@1  69.53 ( 76.65)	Acc@5 100.00 ( 98.61)
Epoch: [12][360/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.2363e-01 (6.6579e-01)	Acc@1  71.88 ( 76.66)	Acc@5  97.66 ( 98.62)
Epoch: [12][370/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.9238e-01 (6.6636e-01)	Acc@1  73.44 ( 76.65)	Acc@5  98.44 ( 98.62)
Epoch: [12][380/391]	Time  0.108 ( 0.108)	Data  0.001 ( 0.005)	Loss 5.9717e-01 (6.6644e-01)	Acc@1  75.78 ( 76.66)	Acc@5 100.00 ( 98.63)
Epoch: [12][390/391]	Time  0.104 ( 0.108)	Data  0.001 ( 0.005)	Loss 7.0898e-01 (6.6674e-01)	Acc@1  78.75 ( 76.65)	Acc@5 100.00 ( 98.62)
## e[12] optimizer.zero_grad (sum) time: 0.35225462913513184
## e[12]       loss.backward (sum) time: 13.655727624893188
## e[12]      optimizer.step (sum) time: 2.747673273086548
## epoch[12] training(only) time: 42.54630947113037
# Switched to evaluate mode...
Test: [  0/100]	Time  0.205 ( 0.205)	Loss 5.7715e-01 (5.7715e-01)	Acc@1  77.00 ( 77.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.039 ( 0.054)	Loss 5.7178e-01 (6.6684e-01)	Acc@1  83.00 ( 76.82)	Acc@5  98.00 ( 98.91)
Test: [ 20/100]	Time  0.037 ( 0.046)	Loss 6.2256e-01 (6.6396e-01)	Acc@1  74.00 ( 76.48)	Acc@5  99.00 ( 98.90)
Test: [ 30/100]	Time  0.039 ( 0.043)	Loss 5.9863e-01 (6.5702e-01)	Acc@1  78.00 ( 77.00)	Acc@5 100.00 ( 99.00)
Test: [ 40/100]	Time  0.035 ( 0.042)	Loss 6.4551e-01 (6.5455e-01)	Acc@1  77.00 ( 77.02)	Acc@5  99.00 ( 98.93)
Test: [ 50/100]	Time  0.035 ( 0.041)	Loss 5.2539e-01 (6.4391e-01)	Acc@1  83.00 ( 77.59)	Acc@5  99.00 ( 98.86)
Test: [ 60/100]	Time  0.036 ( 0.040)	Loss 6.4648e-01 (6.4897e-01)	Acc@1  75.00 ( 77.54)	Acc@5 100.00 ( 98.84)
Test: [ 70/100]	Time  0.036 ( 0.040)	Loss 6.1182e-01 (6.5029e-01)	Acc@1  80.00 ( 77.44)	Acc@5  98.00 ( 98.83)
Test: [ 80/100]	Time  0.036 ( 0.039)	Loss 5.8105e-01 (6.5657e-01)	Acc@1  78.00 ( 77.31)	Acc@5  98.00 ( 98.83)
Test: [ 90/100]	Time  0.038 ( 0.039)	Loss 5.1611e-01 (6.5526e-01)	Acc@1  80.00 ( 77.36)	Acc@5 100.00 ( 98.77)
 * Acc@1 77.360 Acc@5 98.760
### epoch[12] execution time: 46.519376277923584
EPOCH 13
# current learning rate: 0.1
# Switched to train mode...
Epoch: [13][  0/391]	Time  0.277 ( 0.277)	Data  0.179 ( 0.179)	Loss 5.4541e-01 (5.4541e-01)	Acc@1  78.12 ( 78.12)	Acc@5 100.00 (100.00)
Epoch: [13][ 10/391]	Time  0.105 ( 0.123)	Data  0.001 ( 0.020)	Loss 6.3721e-01 (6.3414e-01)	Acc@1  78.12 ( 76.99)	Acc@5  98.44 ( 98.72)
Epoch: [13][ 20/391]	Time  0.110 ( 0.116)	Data  0.001 ( 0.012)	Loss 7.8320e-01 (6.4581e-01)	Acc@1  70.31 ( 76.49)	Acc@5  98.44 ( 98.51)
Epoch: [13][ 30/391]	Time  0.106 ( 0.113)	Data  0.001 ( 0.010)	Loss 5.7520e-01 (6.2875e-01)	Acc@1  78.12 ( 77.14)	Acc@5 100.00 ( 98.69)
Epoch: [13][ 40/391]	Time  0.108 ( 0.112)	Data  0.001 ( 0.008)	Loss 6.4844e-01 (6.2395e-01)	Acc@1  77.34 ( 77.32)	Acc@5  98.44 ( 98.76)
Epoch: [13][ 50/391]	Time  0.105 ( 0.111)	Data  0.001 ( 0.007)	Loss 4.9292e-01 (6.2594e-01)	Acc@1  86.72 ( 77.31)	Acc@5  99.22 ( 98.82)
Epoch: [13][ 60/391]	Time  0.106 ( 0.111)	Data  0.001 ( 0.007)	Loss 6.2744e-01 (6.2384e-01)	Acc@1  76.56 ( 77.47)	Acc@5 100.00 ( 98.86)
Epoch: [13][ 70/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.007)	Loss 6.0742e-01 (6.2170e-01)	Acc@1  78.91 ( 77.65)	Acc@5  98.44 ( 98.89)
Epoch: [13][ 80/391]	Time  0.103 ( 0.110)	Data  0.001 ( 0.006)	Loss 6.3867e-01 (6.2219e-01)	Acc@1  78.12 ( 77.74)	Acc@5  99.22 ( 98.91)
Epoch: [13][ 90/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 7.0117e-01 (6.1882e-01)	Acc@1  75.78 ( 77.94)	Acc@5  96.09 ( 98.88)
Epoch: [13][100/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.006)	Loss 5.7324e-01 (6.1962e-01)	Acc@1  81.25 ( 77.95)	Acc@5  98.44 ( 98.88)
Epoch: [13][110/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.006)	Loss 5.1904e-01 (6.2061e-01)	Acc@1  82.03 ( 78.00)	Acc@5  99.22 ( 98.86)
Epoch: [13][120/391]	Time  0.104 ( 0.110)	Data  0.001 ( 0.006)	Loss 7.7783e-01 (6.2298e-01)	Acc@1  75.78 ( 77.96)	Acc@5  97.66 ( 98.82)
Epoch: [13][130/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.1719e-01 (6.2182e-01)	Acc@1  78.91 ( 78.05)	Acc@5  98.44 ( 98.83)
Epoch: [13][140/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.0732e-01 (6.2166e-01)	Acc@1  81.25 ( 78.05)	Acc@5  98.44 ( 98.79)
Epoch: [13][150/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.9590e-01 (6.2446e-01)	Acc@1  71.09 ( 77.97)	Acc@5  99.22 ( 98.79)
Epoch: [13][160/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.2295e-01 (6.2209e-01)	Acc@1  82.81 ( 78.09)	Acc@5  98.44 ( 98.82)
Epoch: [13][170/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.1582e-01 (6.2295e-01)	Acc@1  76.56 ( 78.11)	Acc@5  99.22 ( 98.85)
Epoch: [13][180/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.3389e-01 (6.2492e-01)	Acc@1  71.88 ( 78.03)	Acc@5  95.31 ( 98.84)
Epoch: [13][190/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.9180e-01 (6.2587e-01)	Acc@1  78.12 ( 78.02)	Acc@5  98.44 ( 98.85)
Epoch: [13][200/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.0117e-01 (6.2853e-01)	Acc@1  72.66 ( 77.99)	Acc@5 100.00 ( 98.83)
Epoch: [13][210/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.5039e-01 (6.2846e-01)	Acc@1  77.34 ( 77.95)	Acc@5 100.00 ( 98.86)
Epoch: [13][220/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.9585e-01 (6.2590e-01)	Acc@1  82.03 ( 78.04)	Acc@5  99.22 ( 98.87)
Epoch: [13][230/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.4697e-01 (6.2630e-01)	Acc@1  81.25 ( 78.10)	Acc@5  98.44 ( 98.87)
Epoch: [13][240/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.2803e-01 (6.3000e-01)	Acc@1  75.00 ( 78.00)	Acc@5  96.88 ( 98.85)
Epoch: [13][250/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.5469e-01 (6.3055e-01)	Acc@1  82.81 ( 77.98)	Acc@5  99.22 ( 98.83)
Epoch: [13][260/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.5088e-01 (6.3008e-01)	Acc@1  77.34 ( 78.03)	Acc@5  99.22 ( 98.84)
Epoch: [13][270/391]	Time  0.111 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.2305e-01 (6.2801e-01)	Acc@1  82.81 ( 78.12)	Acc@5  98.44 ( 98.82)
Epoch: [13][280/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.0537e-01 (6.2752e-01)	Acc@1  83.59 ( 78.13)	Acc@5  98.44 ( 98.80)
Epoch: [13][290/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.6348e-01 (6.2675e-01)	Acc@1  78.12 ( 78.17)	Acc@5  98.44 ( 98.82)
Epoch: [13][300/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.0254e-01 (6.2693e-01)	Acc@1  82.03 ( 78.17)	Acc@5  97.66 ( 98.83)
Epoch: [13][310/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.6250e-01 (6.2529e-01)	Acc@1  78.91 ( 78.24)	Acc@5 100.00 ( 98.83)
Epoch: [13][320/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.3438e-01 (6.2489e-01)	Acc@1  71.09 ( 78.27)	Acc@5  96.88 ( 98.81)
Epoch: [13][330/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.9189e-01 (6.2660e-01)	Acc@1  74.22 ( 78.19)	Acc@5 100.00 ( 98.81)
Epoch: [13][340/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.5176e-01 (6.2750e-01)	Acc@1  81.25 ( 78.16)	Acc@5  99.22 ( 98.81)
Epoch: [13][350/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.8301e-01 (6.2743e-01)	Acc@1  78.12 ( 78.15)	Acc@5  99.22 ( 98.80)
Epoch: [13][360/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.5874e-01 (6.2683e-01)	Acc@1  86.72 ( 78.22)	Acc@5  99.22 ( 98.80)
Epoch: [13][370/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.6816e-01 (6.2640e-01)	Acc@1  74.22 ( 78.24)	Acc@5  97.66 ( 98.81)
Epoch: [13][380/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.9053e-01 (6.2641e-01)	Acc@1  71.88 ( 78.26)	Acc@5  98.44 ( 98.81)
Epoch: [13][390/391]	Time  0.100 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.6509e-01 (6.2582e-01)	Acc@1  83.75 ( 78.28)	Acc@5 100.00 ( 98.81)
## e[13] optimizer.zero_grad (sum) time: 0.3454437255859375
## e[13]       loss.backward (sum) time: 13.663797616958618
## e[13]      optimizer.step (sum) time: 2.7928316593170166
## epoch[13] training(only) time: 42.70612072944641
# Switched to evaluate mode...
Test: [  0/100]	Time  0.176 ( 0.176)	Loss 5.7324e-01 (5.7324e-01)	Acc@1  82.00 ( 82.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.038 ( 0.050)	Loss 6.0498e-01 (6.1685e-01)	Acc@1  82.00 ( 79.55)	Acc@5  97.00 ( 98.82)
Test: [ 20/100]	Time  0.038 ( 0.044)	Loss 7.7295e-01 (6.3632e-01)	Acc@1  72.00 ( 78.14)	Acc@5  99.00 ( 98.90)
Test: [ 30/100]	Time  0.036 ( 0.041)	Loss 6.2305e-01 (6.3954e-01)	Acc@1  77.00 ( 78.23)	Acc@5  98.00 ( 98.87)
Test: [ 40/100]	Time  0.035 ( 0.040)	Loss 6.3770e-01 (6.2736e-01)	Acc@1  76.00 ( 78.56)	Acc@5  98.00 ( 98.76)
Test: [ 50/100]	Time  0.036 ( 0.039)	Loss 5.2832e-01 (6.2538e-01)	Acc@1  81.00 ( 78.49)	Acc@5  99.00 ( 98.75)
Test: [ 60/100]	Time  0.036 ( 0.039)	Loss 5.5225e-01 (6.2727e-01)	Acc@1  77.00 ( 78.39)	Acc@5 100.00 ( 98.74)
Test: [ 70/100]	Time  0.036 ( 0.039)	Loss 6.2646e-01 (6.3155e-01)	Acc@1  84.00 ( 78.37)	Acc@5  99.00 ( 98.72)
Test: [ 80/100]	Time  0.038 ( 0.039)	Loss 5.3955e-01 (6.3311e-01)	Acc@1  76.00 ( 78.17)	Acc@5 100.00 ( 98.78)
Test: [ 90/100]	Time  0.038 ( 0.038)	Loss 5.8936e-01 (6.3448e-01)	Acc@1  81.00 ( 78.15)	Acc@5  99.00 ( 98.77)
 * Acc@1 78.180 Acc@5 98.830
### epoch[13] execution time: 46.63081431388855
EPOCH 14
# current learning rate: 0.1
# Switched to train mode...
Epoch: [14][  0/391]	Time  0.308 ( 0.308)	Data  0.209 ( 0.209)	Loss 5.8643e-01 (5.8643e-01)	Acc@1  79.69 ( 79.69)	Acc@5  98.44 ( 98.44)
Epoch: [14][ 10/391]	Time  0.107 ( 0.125)	Data  0.001 ( 0.023)	Loss 4.5532e-01 (5.3862e-01)	Acc@1  85.94 ( 81.53)	Acc@5  99.22 ( 98.79)
Epoch: [14][ 20/391]	Time  0.103 ( 0.117)	Data  0.001 ( 0.014)	Loss 4.3433e-01 (5.6147e-01)	Acc@1  85.94 ( 81.06)	Acc@5 100.00 ( 98.88)
Epoch: [14][ 30/391]	Time  0.107 ( 0.114)	Data  0.001 ( 0.011)	Loss 5.9521e-01 (5.6911e-01)	Acc@1  75.78 ( 81.05)	Acc@5 100.00 ( 98.84)
Epoch: [14][ 40/391]	Time  0.106 ( 0.113)	Data  0.001 ( 0.009)	Loss 4.8779e-01 (5.6395e-01)	Acc@1  82.81 ( 80.98)	Acc@5 100.00 ( 98.91)
Epoch: [14][ 50/391]	Time  0.106 ( 0.112)	Data  0.001 ( 0.008)	Loss 5.4004e-01 (5.7331e-01)	Acc@1  77.34 ( 80.58)	Acc@5  99.22 ( 98.88)
Epoch: [14][ 60/391]	Time  0.106 ( 0.111)	Data  0.001 ( 0.008)	Loss 5.9131e-01 (5.7833e-01)	Acc@1  81.25 ( 80.35)	Acc@5  98.44 ( 98.92)
Epoch: [14][ 70/391]	Time  0.100 ( 0.111)	Data  0.001 ( 0.007)	Loss 7.8125e-01 (5.8417e-01)	Acc@1  75.00 ( 80.17)	Acc@5  96.09 ( 98.92)
Epoch: [14][ 80/391]	Time  0.104 ( 0.111)	Data  0.001 ( 0.007)	Loss 5.2881e-01 (5.8341e-01)	Acc@1  81.25 ( 80.17)	Acc@5  99.22 ( 98.94)
Epoch: [14][ 90/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.006)	Loss 7.0850e-01 (5.8568e-01)	Acc@1  78.91 ( 80.06)	Acc@5  99.22 ( 98.94)
Epoch: [14][100/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 6.4600e-01 (5.8839e-01)	Acc@1  75.78 ( 79.87)	Acc@5  99.22 ( 98.95)
Epoch: [14][110/391]	Time  0.104 ( 0.110)	Data  0.001 ( 0.006)	Loss 4.8364e-01 (5.8349e-01)	Acc@1  80.47 ( 80.06)	Acc@5  99.22 ( 98.97)
Epoch: [14][120/391]	Time  0.099 ( 0.110)	Data  0.001 ( 0.006)	Loss 5.5322e-01 (5.8041e-01)	Acc@1  83.59 ( 80.17)	Acc@5  98.44 ( 98.99)
Epoch: [14][130/391]	Time  0.105 ( 0.110)	Data  0.002 ( 0.006)	Loss 6.1328e-01 (5.8161e-01)	Acc@1  77.34 ( 80.10)	Acc@5  98.44 ( 98.97)
Epoch: [14][140/391]	Time  0.109 ( 0.110)	Data  0.001 ( 0.006)	Loss 4.6484e-01 (5.8332e-01)	Acc@1  85.16 ( 80.06)	Acc@5 100.00 ( 98.99)
Epoch: [14][150/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 5.1855e-01 (5.8300e-01)	Acc@1  82.81 ( 80.10)	Acc@5 100.00 ( 98.99)
Epoch: [14][160/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.6104e-01 (5.8394e-01)	Acc@1  79.69 ( 80.06)	Acc@5 100.00 ( 98.99)
Epoch: [14][170/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.5039e-01 (5.8627e-01)	Acc@1  78.91 ( 79.99)	Acc@5  98.44 ( 98.99)
Epoch: [14][180/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.9316e-01 (5.8441e-01)	Acc@1  82.81 ( 80.06)	Acc@5 100.00 ( 99.02)
Epoch: [14][190/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.0889e-01 (5.8470e-01)	Acc@1  77.34 ( 79.99)	Acc@5  98.44 ( 99.04)
Epoch: [14][200/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.9873e-01 (5.8436e-01)	Acc@1  78.91 ( 79.97)	Acc@5  98.44 ( 99.05)
Epoch: [14][210/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.0601e-01 (5.8194e-01)	Acc@1  82.81 ( 80.00)	Acc@5  99.22 ( 99.03)
Epoch: [14][220/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.0059e-01 (5.8129e-01)	Acc@1  79.69 ( 80.01)	Acc@5  97.66 ( 99.00)
Epoch: [14][230/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.7632e-01 (5.8144e-01)	Acc@1  84.38 ( 79.97)	Acc@5  99.22 ( 99.01)
Epoch: [14][240/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.2051e-01 (5.8225e-01)	Acc@1  83.59 ( 79.94)	Acc@5  98.44 ( 99.00)
Epoch: [14][250/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.8218e-01 (5.8057e-01)	Acc@1  85.16 ( 80.03)	Acc@5  98.44 ( 98.98)
Epoch: [14][260/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.8398e-01 (5.7902e-01)	Acc@1  77.34 ( 80.02)	Acc@5 100.00 ( 98.98)
Epoch: [14][270/391]	Time  0.111 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.9951e-01 (5.7763e-01)	Acc@1  81.25 ( 80.06)	Acc@5  99.22 ( 98.99)
Epoch: [14][280/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.1670e-01 (5.7856e-01)	Acc@1  76.56 ( 79.99)	Acc@5  98.44 ( 98.98)
Epoch: [14][290/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.9180e-01 (5.7999e-01)	Acc@1  78.91 ( 79.92)	Acc@5  99.22 ( 98.99)
Epoch: [14][300/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.5996e-01 (5.8061e-01)	Acc@1  84.38 ( 79.87)	Acc@5 100.00 ( 98.98)
Epoch: [14][310/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.4834e-01 (5.8134e-01)	Acc@1  81.25 ( 79.87)	Acc@5  98.44 ( 98.96)
Epoch: [14][320/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.3760e-01 (5.8058e-01)	Acc@1  82.03 ( 79.90)	Acc@5 100.00 ( 98.97)
Epoch: [14][330/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.6299e-01 (5.7909e-01)	Acc@1  82.03 ( 79.98)	Acc@5  97.66 ( 98.97)
Epoch: [14][340/391]	Time  0.101 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.9561e-01 (5.7778e-01)	Acc@1  85.16 ( 80.01)	Acc@5  98.44 ( 98.97)
Epoch: [14][350/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.9580e-01 (5.7887e-01)	Acc@1  75.78 ( 79.94)	Acc@5  98.44 ( 98.98)
Epoch: [14][360/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.7324e-01 (5.7824e-01)	Acc@1  81.25 ( 79.96)	Acc@5  99.22 ( 98.99)
Epoch: [14][370/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.1514e-01 (5.7923e-01)	Acc@1  79.69 ( 79.92)	Acc@5  99.22 ( 99.00)
Epoch: [14][380/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.8447e-01 (5.7963e-01)	Acc@1  82.03 ( 79.89)	Acc@5  98.44 ( 99.00)
Epoch: [14][390/391]	Time  0.101 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.8755e-01 (5.8004e-01)	Acc@1  83.75 ( 79.88)	Acc@5  98.75 ( 99.00)
## e[14] optimizer.zero_grad (sum) time: 0.34743833541870117
## e[14]       loss.backward (sum) time: 13.700503826141357
## e[14]      optimizer.step (sum) time: 2.7897725105285645
## epoch[14] training(only) time: 42.74643850326538
# Switched to evaluate mode...
Test: [  0/100]	Time  0.246 ( 0.246)	Loss 5.2002e-01 (5.2002e-01)	Acc@1  82.00 ( 82.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.036 ( 0.056)	Loss 5.6982e-01 (5.9171e-01)	Acc@1  83.00 ( 79.36)	Acc@5  97.00 ( 99.18)
Test: [ 20/100]	Time  0.035 ( 0.047)	Loss 7.4707e-01 (6.1372e-01)	Acc@1  72.00 ( 78.76)	Acc@5 100.00 ( 99.05)
Test: [ 30/100]	Time  0.036 ( 0.044)	Loss 6.5234e-01 (6.1777e-01)	Acc@1  78.00 ( 79.16)	Acc@5  97.00 ( 98.84)
Test: [ 40/100]	Time  0.036 ( 0.042)	Loss 5.8594e-01 (6.1242e-01)	Acc@1  80.00 ( 79.15)	Acc@5  99.00 ( 98.85)
Test: [ 50/100]	Time  0.036 ( 0.041)	Loss 4.5703e-01 (6.0868e-01)	Acc@1  85.00 ( 79.43)	Acc@5  99.00 ( 98.80)
Test: [ 60/100]	Time  0.037 ( 0.040)	Loss 5.2979e-01 (6.0617e-01)	Acc@1  83.00 ( 79.44)	Acc@5 100.00 ( 98.80)
Test: [ 70/100]	Time  0.037 ( 0.040)	Loss 6.1621e-01 (6.0524e-01)	Acc@1  81.00 ( 79.46)	Acc@5 100.00 ( 98.86)
Test: [ 80/100]	Time  0.035 ( 0.039)	Loss 5.3271e-01 (6.0648e-01)	Acc@1  78.00 ( 79.37)	Acc@5 100.00 ( 98.91)
Test: [ 90/100]	Time  0.036 ( 0.039)	Loss 4.5215e-01 (6.0437e-01)	Acc@1  82.00 ( 79.47)	Acc@5 100.00 ( 98.92)
 * Acc@1 79.530 Acc@5 98.890
### epoch[14] execution time: 46.73895621299744
EPOCH 15
# current learning rate: 0.1
# Switched to train mode...
Epoch: [15][  0/391]	Time  0.272 ( 0.272)	Data  0.174 ( 0.174)	Loss 5.3027e-01 (5.3027e-01)	Acc@1  84.38 ( 84.38)	Acc@5 100.00 (100.00)
Epoch: [15][ 10/391]	Time  0.107 ( 0.123)	Data  0.001 ( 0.019)	Loss 4.8560e-01 (5.5482e-01)	Acc@1  84.38 ( 81.39)	Acc@5  99.22 ( 98.72)
Epoch: [15][ 20/391]	Time  0.106 ( 0.116)	Data  0.001 ( 0.012)	Loss 5.3027e-01 (5.6630e-01)	Acc@1  79.69 ( 80.88)	Acc@5 100.00 ( 98.40)
Epoch: [15][ 30/391]	Time  0.104 ( 0.113)	Data  0.001 ( 0.010)	Loss 6.1084e-01 (5.6104e-01)	Acc@1  77.34 ( 80.57)	Acc@5  98.44 ( 98.71)
Epoch: [15][ 40/391]	Time  0.106 ( 0.112)	Data  0.001 ( 0.008)	Loss 5.4443e-01 (5.5273e-01)	Acc@1  81.25 ( 80.68)	Acc@5 100.00 ( 98.80)
Epoch: [15][ 50/391]	Time  0.102 ( 0.111)	Data  0.001 ( 0.007)	Loss 4.7998e-01 (5.5582e-01)	Acc@1  82.03 ( 80.51)	Acc@5  99.22 ( 98.74)
Epoch: [15][ 60/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.007)	Loss 4.6655e-01 (5.4910e-01)	Acc@1  83.59 ( 80.79)	Acc@5  97.66 ( 98.80)
Epoch: [15][ 70/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 5.6641e-01 (5.4665e-01)	Acc@1  79.69 ( 80.88)	Acc@5 100.00 ( 98.84)
Epoch: [15][ 80/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 6.2207e-01 (5.4034e-01)	Acc@1  80.47 ( 81.05)	Acc@5  97.66 ( 98.87)
Epoch: [15][ 90/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.006)	Loss 2.7441e-01 (5.2926e-01)	Acc@1  90.62 ( 81.57)	Acc@5 100.00 ( 98.92)
Epoch: [15][100/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.006)	Loss 5.1758e-01 (5.3238e-01)	Acc@1  82.03 ( 81.40)	Acc@5  99.22 ( 98.92)
Epoch: [15][110/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.006)	Loss 6.5039e-01 (5.3515e-01)	Acc@1  77.34 ( 81.29)	Acc@5  98.44 ( 98.94)
Epoch: [15][120/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.006)	Loss 4.3726e-01 (5.3074e-01)	Acc@1  86.72 ( 81.48)	Acc@5  98.44 ( 98.98)
Epoch: [15][130/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.0640e-01 (5.2519e-01)	Acc@1  89.06 ( 81.70)	Acc@5 100.00 ( 99.01)
Epoch: [15][140/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.7485e-01 (5.2631e-01)	Acc@1  88.28 ( 81.69)	Acc@5 100.00 ( 99.02)
Epoch: [15][150/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.0781e-01 (5.2744e-01)	Acc@1  81.25 ( 81.66)	Acc@5 100.00 ( 99.03)
Epoch: [15][160/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.5811e-01 (5.2868e-01)	Acc@1  78.91 ( 81.57)	Acc@5  99.22 ( 99.01)
Epoch: [15][170/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.9072e-01 (5.3101e-01)	Acc@1  80.47 ( 81.51)	Acc@5 100.00 ( 99.01)
Epoch: [15][180/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.0991e-01 (5.2970e-01)	Acc@1  87.50 ( 81.56)	Acc@5  99.22 ( 99.02)
Epoch: [15][190/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.1709e-01 (5.2913e-01)	Acc@1  84.38 ( 81.65)	Acc@5  99.22 ( 99.02)
Epoch: [15][200/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.7725e-01 (5.3111e-01)	Acc@1  82.03 ( 81.58)	Acc@5  97.66 ( 99.03)
Epoch: [15][210/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.4746e-01 (5.3259e-01)	Acc@1  74.22 ( 81.50)	Acc@5 100.00 ( 99.05)
Epoch: [15][220/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.0586e-01 (5.3127e-01)	Acc@1  82.81 ( 81.57)	Acc@5 100.00 ( 99.06)
Epoch: [15][230/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.5957e-01 (5.3266e-01)	Acc@1  82.81 ( 81.53)	Acc@5  98.44 ( 99.06)
Epoch: [15][240/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.0742e-01 (5.3285e-01)	Acc@1  80.47 ( 81.51)	Acc@5 100.00 ( 99.06)
Epoch: [15][250/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.3662e-01 (5.3214e-01)	Acc@1  81.25 ( 81.57)	Acc@5  97.66 ( 99.05)
Epoch: [15][260/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.4346e-01 (5.3315e-01)	Acc@1  78.91 ( 81.52)	Acc@5 100.00 ( 99.06)
Epoch: [15][270/391]	Time  0.111 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.1855e-01 (5.3092e-01)	Acc@1  82.03 ( 81.59)	Acc@5 100.00 ( 99.08)
Epoch: [15][280/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.3604e-01 (5.3047e-01)	Acc@1  85.94 ( 81.62)	Acc@5  99.22 ( 99.07)
Epoch: [15][290/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.7192e-01 (5.3097e-01)	Acc@1  81.25 ( 81.64)	Acc@5 100.00 ( 99.07)
Epoch: [15][300/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.1919e-01 (5.3074e-01)	Acc@1  82.81 ( 81.64)	Acc@5  99.22 ( 99.07)
Epoch: [15][310/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.0000e-01 (5.2908e-01)	Acc@1  83.59 ( 81.70)	Acc@5  99.22 ( 99.09)
Epoch: [15][320/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.7510e-01 (5.2963e-01)	Acc@1  85.94 ( 81.68)	Acc@5  99.22 ( 99.10)
Epoch: [15][330/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.9634e-01 (5.2990e-01)	Acc@1  84.38 ( 81.67)	Acc@5  99.22 ( 99.09)
Epoch: [15][340/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.1367e-01 (5.3056e-01)	Acc@1  82.81 ( 81.64)	Acc@5  98.44 ( 99.08)
Epoch: [15][350/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.5088e-01 (5.3071e-01)	Acc@1  80.47 ( 81.61)	Acc@5  99.22 ( 99.09)
Epoch: [15][360/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.4590e-01 (5.3120e-01)	Acc@1  79.69 ( 81.59)	Acc@5 100.00 ( 99.10)
Epoch: [15][370/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.1172e-01 (5.3028e-01)	Acc@1  79.69 ( 81.60)	Acc@5  99.22 ( 99.11)
Epoch: [15][380/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.8008e-01 (5.3070e-01)	Acc@1  80.47 ( 81.59)	Acc@5  98.44 ( 99.11)
Epoch: [15][390/391]	Time  0.100 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.7705e-01 (5.3104e-01)	Acc@1  82.50 ( 81.58)	Acc@5 100.00 ( 99.11)
## e[15] optimizer.zero_grad (sum) time: 0.34553027153015137
## e[15]       loss.backward (sum) time: 13.650543451309204
## e[15]      optimizer.step (sum) time: 2.847316265106201
## epoch[15] training(only) time: 42.62774133682251
# Switched to evaluate mode...
Test: [  0/100]	Time  0.221 ( 0.221)	Loss 6.1230e-01 (6.1230e-01)	Acc@1  75.00 ( 75.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.036 ( 0.054)	Loss 5.3906e-01 (6.4147e-01)	Acc@1  84.00 ( 78.18)	Acc@5  98.00 ( 98.64)
Test: [ 20/100]	Time  0.038 ( 0.046)	Loss 7.5488e-01 (6.6903e-01)	Acc@1  73.00 ( 77.67)	Acc@5  99.00 ( 98.48)
Test: [ 30/100]	Time  0.036 ( 0.043)	Loss 5.0830e-01 (6.5785e-01)	Acc@1  82.00 ( 78.13)	Acc@5  99.00 ( 98.45)
Test: [ 40/100]	Time  0.037 ( 0.041)	Loss 6.1279e-01 (6.5178e-01)	Acc@1  78.00 ( 78.29)	Acc@5  97.00 ( 98.34)
Test: [ 50/100]	Time  0.037 ( 0.041)	Loss 5.1172e-01 (6.4600e-01)	Acc@1  87.00 ( 78.65)	Acc@5  99.00 ( 98.35)
Test: [ 60/100]	Time  0.040 ( 0.040)	Loss 7.0898e-01 (6.4572e-01)	Acc@1  77.00 ( 78.61)	Acc@5  99.00 ( 98.43)
Test: [ 70/100]	Time  0.038 ( 0.040)	Loss 6.7725e-01 (6.5069e-01)	Acc@1  78.00 ( 78.28)	Acc@5  98.00 ( 98.37)
Test: [ 80/100]	Time  0.038 ( 0.039)	Loss 5.4688e-01 (6.5067e-01)	Acc@1  81.00 ( 78.19)	Acc@5  98.00 ( 98.33)
Test: [ 90/100]	Time  0.035 ( 0.039)	Loss 4.9756e-01 (6.4944e-01)	Acc@1  80.00 ( 78.20)	Acc@5  99.00 ( 98.44)
 * Acc@1 78.290 Acc@5 98.470
### epoch[15] execution time: 46.612342834472656
EPOCH 16
# current learning rate: 0.1
# Switched to train mode...
Epoch: [16][  0/391]	Time  0.314 ( 0.314)	Data  0.218 ( 0.218)	Loss 4.0405e-01 (4.0405e-01)	Acc@1  86.72 ( 86.72)	Acc@5  99.22 ( 99.22)
Epoch: [16][ 10/391]	Time  0.107 ( 0.126)	Data  0.001 ( 0.023)	Loss 5.6885e-01 (5.1261e-01)	Acc@1  78.91 ( 81.32)	Acc@5  99.22 ( 99.29)
Epoch: [16][ 20/391]	Time  0.107 ( 0.118)	Data  0.001 ( 0.014)	Loss 5.1562e-01 (5.1539e-01)	Acc@1  81.25 ( 82.03)	Acc@5 100.00 ( 99.14)
Epoch: [16][ 30/391]	Time  0.108 ( 0.115)	Data  0.001 ( 0.011)	Loss 4.9902e-01 (5.1183e-01)	Acc@1  83.59 ( 82.18)	Acc@5  98.44 ( 99.09)
Epoch: [16][ 40/391]	Time  0.106 ( 0.113)	Data  0.001 ( 0.009)	Loss 5.2197e-01 (4.9691e-01)	Acc@1  81.25 ( 82.70)	Acc@5  99.22 ( 99.10)
Epoch: [16][ 50/391]	Time  0.102 ( 0.112)	Data  0.001 ( 0.008)	Loss 5.4932e-01 (4.9330e-01)	Acc@1  81.25 ( 82.64)	Acc@5 100.00 ( 99.19)
Epoch: [16][ 60/391]	Time  0.107 ( 0.112)	Data  0.001 ( 0.008)	Loss 5.9326e-01 (4.9788e-01)	Acc@1  79.69 ( 82.49)	Acc@5  99.22 ( 99.22)
Epoch: [16][ 70/391]	Time  0.109 ( 0.111)	Data  0.001 ( 0.007)	Loss 4.6826e-01 (5.0314e-01)	Acc@1  84.38 ( 82.34)	Acc@5  99.22 ( 99.17)
Epoch: [16][ 80/391]	Time  0.109 ( 0.111)	Data  0.001 ( 0.007)	Loss 4.6924e-01 (5.0672e-01)	Acc@1  85.94 ( 82.33)	Acc@5  99.22 ( 99.14)
Epoch: [16][ 90/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 6.3965e-01 (5.0538e-01)	Acc@1  79.69 ( 82.43)	Acc@5  97.66 ( 99.14)
Epoch: [16][100/391]	Time  0.109 ( 0.110)	Data  0.001 ( 0.006)	Loss 6.5234e-01 (5.0850e-01)	Acc@1  78.91 ( 82.34)	Acc@5 100.00 ( 99.16)
Epoch: [16][110/391]	Time  0.104 ( 0.110)	Data  0.001 ( 0.006)	Loss 6.9873e-01 (5.0958e-01)	Acc@1  73.44 ( 82.32)	Acc@5 100.00 ( 99.19)
Epoch: [16][120/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 5.3857e-01 (5.1148e-01)	Acc@1  82.81 ( 82.23)	Acc@5 100.00 ( 99.21)
Epoch: [16][130/391]	Time  0.105 ( 0.110)	Data  0.002 ( 0.006)	Loss 5.0342e-01 (5.1157e-01)	Acc@1  82.81 ( 82.17)	Acc@5  99.22 ( 99.22)
Epoch: [16][140/391]	Time  0.109 ( 0.110)	Data  0.002 ( 0.006)	Loss 5.0488e-01 (5.1207e-01)	Acc@1  81.25 ( 82.11)	Acc@5 100.00 ( 99.20)
Epoch: [16][150/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 3.8379e-01 (5.1269e-01)	Acc@1  88.28 ( 82.13)	Acc@5  98.44 ( 99.16)
Epoch: [16][160/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.3237e-01 (5.1203e-01)	Acc@1  85.16 ( 82.17)	Acc@5  99.22 ( 99.17)
Epoch: [16][170/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.1187e-01 (5.1239e-01)	Acc@1  87.50 ( 82.15)	Acc@5  99.22 ( 99.17)
Epoch: [16][180/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.6260e-01 (5.1307e-01)	Acc@1  80.47 ( 82.19)	Acc@5  99.22 ( 99.18)
Epoch: [16][190/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.9917e-01 (5.1187e-01)	Acc@1  85.94 ( 82.24)	Acc@5  99.22 ( 99.18)
Epoch: [16][200/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.2305e-01 (5.1201e-01)	Acc@1  81.25 ( 82.19)	Acc@5  98.44 ( 99.17)
Epoch: [16][210/391]	Time  0.107 ( 0.109)	Data  0.002 ( 0.005)	Loss 4.7681e-01 (5.1269e-01)	Acc@1  81.25 ( 82.16)	Acc@5  99.22 ( 99.15)
Epoch: [16][220/391]	Time  0.107 ( 0.109)	Data  0.002 ( 0.005)	Loss 6.4795e-01 (5.1333e-01)	Acc@1  78.12 ( 82.19)	Acc@5  97.66 ( 99.13)
Epoch: [16][230/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.6289e-01 (5.1137e-01)	Acc@1  82.81 ( 82.24)	Acc@5 100.00 ( 99.16)
Epoch: [16][240/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.4458e-01 (5.1142e-01)	Acc@1  84.38 ( 82.24)	Acc@5 100.00 ( 99.16)
Epoch: [16][250/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.2480e-01 (5.1225e-01)	Acc@1  84.38 ( 82.23)	Acc@5  99.22 ( 99.15)
Epoch: [16][260/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.4551e-01 (5.1215e-01)	Acc@1  74.22 ( 82.17)	Acc@5  99.22 ( 99.16)
Epoch: [16][270/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.0781e-01 (5.1182e-01)	Acc@1  82.03 ( 82.18)	Acc@5  99.22 ( 99.16)
Epoch: [16][280/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.9780e-01 (5.1220e-01)	Acc@1  84.38 ( 82.15)	Acc@5 100.00 ( 99.17)
Epoch: [16][290/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.9668e-01 (5.1350e-01)	Acc@1  82.03 ( 82.08)	Acc@5  99.22 ( 99.17)
Epoch: [16][300/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.1084e-01 (5.1522e-01)	Acc@1  78.91 ( 82.04)	Acc@5 100.00 ( 99.17)
Epoch: [16][310/391]	Time  0.109 ( 0.109)	Data  0.002 ( 0.005)	Loss 5.6104e-01 (5.1653e-01)	Acc@1  80.47 ( 82.02)	Acc@5  98.44 ( 99.17)
Epoch: [16][320/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.4214e-01 (5.1645e-01)	Acc@1  85.16 ( 82.00)	Acc@5  99.22 ( 99.17)
Epoch: [16][330/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.3564e-01 (5.1608e-01)	Acc@1  82.81 ( 81.98)	Acc@5 100.00 ( 99.17)
Epoch: [16][340/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.9731e-01 (5.1458e-01)	Acc@1  80.47 ( 82.00)	Acc@5 100.00 ( 99.18)
Epoch: [16][350/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.1953e-01 (5.1490e-01)	Acc@1  78.91 ( 81.96)	Acc@5 100.00 ( 99.19)
Epoch: [16][360/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.1621e-01 (5.1568e-01)	Acc@1  79.69 ( 81.94)	Acc@5 100.00 ( 99.19)
Epoch: [16][370/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.1465e-01 (5.1571e-01)	Acc@1  83.59 ( 81.97)	Acc@5  99.22 ( 99.20)
Epoch: [16][380/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.8184e-01 (5.1560e-01)	Acc@1  87.50 ( 81.95)	Acc@5  99.22 ( 99.20)
Epoch: [16][390/391]	Time  0.098 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.3379e-01 (5.1564e-01)	Acc@1  78.75 ( 81.97)	Acc@5  97.50 ( 99.21)
## e[16] optimizer.zero_grad (sum) time: 0.3513963222503662
## e[16]       loss.backward (sum) time: 13.699284553527832
## e[16]      optimizer.step (sum) time: 2.81973934173584
## epoch[16] training(only) time: 42.7299120426178
# Switched to evaluate mode...
Test: [  0/100]	Time  0.192 ( 0.192)	Loss 4.2236e-01 (4.2236e-01)	Acc@1  82.00 ( 82.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.044 ( 0.052)	Loss 5.2588e-01 (5.0519e-01)	Acc@1  85.00 ( 83.00)	Acc@5  98.00 ( 99.27)
Test: [ 20/100]	Time  0.037 ( 0.044)	Loss 6.2061e-01 (5.1235e-01)	Acc@1  78.00 ( 82.38)	Acc@5 100.00 ( 99.29)
Test: [ 30/100]	Time  0.036 ( 0.042)	Loss 5.1758e-01 (5.3290e-01)	Acc@1  79.00 ( 81.84)	Acc@5  98.00 ( 99.06)
Test: [ 40/100]	Time  0.036 ( 0.041)	Loss 4.2969e-01 (5.2804e-01)	Acc@1  85.00 ( 82.24)	Acc@5 100.00 ( 99.02)
Test: [ 50/100]	Time  0.036 ( 0.040)	Loss 3.6157e-01 (5.2542e-01)	Acc@1  85.00 ( 82.59)	Acc@5  98.00 ( 99.06)
Test: [ 60/100]	Time  0.039 ( 0.039)	Loss 4.8999e-01 (5.2381e-01)	Acc@1  82.00 ( 82.56)	Acc@5 100.00 ( 99.11)
Test: [ 70/100]	Time  0.038 ( 0.039)	Loss 6.4844e-01 (5.2615e-01)	Acc@1  79.00 ( 82.28)	Acc@5  99.00 ( 99.10)
Test: [ 80/100]	Time  0.038 ( 0.039)	Loss 5.4785e-01 (5.2964e-01)	Acc@1  83.00 ( 82.30)	Acc@5  99.00 ( 99.14)
Test: [ 90/100]	Time  0.037 ( 0.039)	Loss 4.4531e-01 (5.3058e-01)	Acc@1  84.00 ( 82.27)	Acc@5 100.00 ( 99.15)
 * Acc@1 82.250 Acc@5 99.160
### epoch[16] execution time: 46.702810525894165
EPOCH 17
# current learning rate: 0.1
# Switched to train mode...
Epoch: [17][  0/391]	Time  0.283 ( 0.283)	Data  0.179 ( 0.179)	Loss 2.7368e-01 (2.7368e-01)	Acc@1  89.06 ( 89.06)	Acc@5 100.00 (100.00)
Epoch: [17][ 10/391]	Time  0.110 ( 0.124)	Data  0.001 ( 0.020)	Loss 6.0205e-01 (4.3197e-01)	Acc@1  82.03 ( 85.37)	Acc@5  96.88 ( 99.29)
Epoch: [17][ 20/391]	Time  0.108 ( 0.116)	Data  0.001 ( 0.012)	Loss 5.2100e-01 (4.3956e-01)	Acc@1  82.81 ( 84.64)	Acc@5 100.00 ( 99.33)
Epoch: [17][ 30/391]	Time  0.102 ( 0.114)	Data  0.001 ( 0.010)	Loss 3.4546e-01 (4.3151e-01)	Acc@1  88.28 ( 84.88)	Acc@5  99.22 ( 99.40)
Epoch: [17][ 40/391]	Time  0.105 ( 0.112)	Data  0.001 ( 0.008)	Loss 4.5361e-01 (4.3912e-01)	Acc@1  86.72 ( 84.53)	Acc@5 100.00 ( 99.41)
Epoch: [17][ 50/391]	Time  0.105 ( 0.111)	Data  0.001 ( 0.008)	Loss 3.8989e-01 (4.5332e-01)	Acc@1  83.59 ( 84.10)	Acc@5 100.00 ( 99.39)
Epoch: [17][ 60/391]	Time  0.106 ( 0.111)	Data  0.001 ( 0.007)	Loss 6.6309e-01 (4.6114e-01)	Acc@1  78.91 ( 83.80)	Acc@5  98.44 ( 99.36)
Epoch: [17][ 70/391]	Time  0.101 ( 0.110)	Data  0.001 ( 0.007)	Loss 4.7925e-01 (4.6308e-01)	Acc@1  82.03 ( 83.78)	Acc@5  99.22 ( 99.36)
Epoch: [17][ 80/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.006)	Loss 5.7324e-01 (4.7446e-01)	Acc@1  78.91 ( 83.41)	Acc@5 100.00 ( 99.36)
Epoch: [17][ 90/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 5.5225e-01 (4.8273e-01)	Acc@1  82.81 ( 83.16)	Acc@5  97.66 ( 99.34)
Epoch: [17][100/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 5.6104e-01 (4.8863e-01)	Acc@1  78.12 ( 82.99)	Acc@5  98.44 ( 99.33)
Epoch: [17][110/391]	Time  0.104 ( 0.110)	Data  0.001 ( 0.006)	Loss 4.7852e-01 (4.9313e-01)	Acc@1  82.81 ( 82.64)	Acc@5 100.00 ( 99.35)
Epoch: [17][120/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.006)	Loss 4.3970e-01 (4.9279e-01)	Acc@1  81.25 ( 82.66)	Acc@5 100.00 ( 99.35)
Epoch: [17][130/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.9072e-01 (4.9128e-01)	Acc@1  86.72 ( 82.76)	Acc@5  99.22 ( 99.34)
Epoch: [17][140/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.1406e-01 (4.9303e-01)	Acc@1  86.72 ( 82.69)	Acc@5  99.22 ( 99.34)
Epoch: [17][150/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.7500e-01 (4.9288e-01)	Acc@1  86.72 ( 82.66)	Acc@5  99.22 ( 99.36)
Epoch: [17][160/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.0596e-01 (4.9383e-01)	Acc@1  75.78 ( 82.56)	Acc@5  98.44 ( 99.36)
Epoch: [17][170/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.8657e-01 (4.9228e-01)	Acc@1  83.59 ( 82.65)	Acc@5  98.44 ( 99.36)
Epoch: [17][180/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.6694e-01 (4.9220e-01)	Acc@1  86.72 ( 82.66)	Acc@5 100.00 ( 99.37)
Epoch: [17][190/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.7324e-01 (4.9272e-01)	Acc@1  78.91 ( 82.67)	Acc@5  98.44 ( 99.37)
Epoch: [17][200/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.4531e-01 (4.8853e-01)	Acc@1  82.81 ( 82.82)	Acc@5  99.22 ( 99.38)
Epoch: [17][210/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.2456e-01 (4.8621e-01)	Acc@1  87.50 ( 82.90)	Acc@5  99.22 ( 99.37)
Epoch: [17][220/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.0308e-01 (4.8318e-01)	Acc@1  86.72 ( 83.01)	Acc@5 100.00 ( 99.37)
Epoch: [17][230/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.4551e-01 (4.8516e-01)	Acc@1  76.56 ( 82.96)	Acc@5  99.22 ( 99.36)
Epoch: [17][240/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.3760e-01 (4.8526e-01)	Acc@1  81.25 ( 83.00)	Acc@5  99.22 ( 99.36)
Epoch: [17][250/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.6011e-01 (4.8386e-01)	Acc@1  89.84 ( 83.00)	Acc@5 100.00 ( 99.36)
Epoch: [17][260/391]	Time  0.101 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.1406e-01 (4.8505e-01)	Acc@1  89.06 ( 83.02)	Acc@5  99.22 ( 99.34)
Epoch: [17][270/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.5674e-01 (4.8561e-01)	Acc@1  78.12 ( 82.99)	Acc@5  99.22 ( 99.35)
Epoch: [17][280/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.4399e-01 (4.8449e-01)	Acc@1  86.72 ( 83.03)	Acc@5 100.00 ( 99.34)
Epoch: [17][290/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.9609e-01 (4.8498e-01)	Acc@1  84.38 ( 83.01)	Acc@5 100.00 ( 99.34)
Epoch: [17][300/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.8608e-01 (4.8600e-01)	Acc@1  78.12 ( 82.98)	Acc@5  99.22 ( 99.34)
Epoch: [17][310/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.4639e-01 (4.8495e-01)	Acc@1  82.03 ( 83.03)	Acc@5 100.00 ( 99.33)
Epoch: [17][320/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.0830e-01 (4.8478e-01)	Acc@1  84.38 ( 83.05)	Acc@5 100.00 ( 99.32)
Epoch: [17][330/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.6787e-01 (4.8604e-01)	Acc@1  83.59 ( 83.01)	Acc@5  98.44 ( 99.32)
Epoch: [17][340/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.1270e-01 (4.8499e-01)	Acc@1  83.59 ( 83.03)	Acc@5  99.22 ( 99.33)
Epoch: [17][350/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.5654e-01 (4.8562e-01)	Acc@1  83.59 ( 83.05)	Acc@5 100.00 ( 99.32)
Epoch: [17][360/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.8096e-01 (4.8619e-01)	Acc@1  82.03 ( 83.02)	Acc@5 100.00 ( 99.32)
Epoch: [17][370/391]	Time  0.104 ( 0.108)	Data  0.001 ( 0.005)	Loss 4.1504e-01 (4.8591e-01)	Acc@1  85.94 ( 83.02)	Acc@5  99.22 ( 99.33)
Epoch: [17][380/391]	Time  0.104 ( 0.108)	Data  0.001 ( 0.005)	Loss 5.5225e-01 (4.8638e-01)	Acc@1  85.16 ( 83.00)	Acc@5  99.22 ( 99.32)
Epoch: [17][390/391]	Time  0.100 ( 0.108)	Data  0.001 ( 0.005)	Loss 6.0840e-01 (4.8683e-01)	Acc@1  82.50 ( 83.00)	Acc@5 100.00 ( 99.32)
## e[17] optimizer.zero_grad (sum) time: 0.349987268447876
## e[17]       loss.backward (sum) time: 13.753960847854614
## e[17]      optimizer.step (sum) time: 2.8418827056884766
## epoch[17] training(only) time: 42.541908502578735
# Switched to evaluate mode...
Test: [  0/100]	Time  0.194 ( 0.194)	Loss 5.8447e-01 (5.8447e-01)	Acc@1  79.00 ( 79.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.036 ( 0.051)	Loss 6.8604e-01 (6.9181e-01)	Acc@1  75.00 ( 75.55)	Acc@5  99.00 ( 99.36)
Test: [ 20/100]	Time  0.036 ( 0.044)	Loss 8.5693e-01 (6.9906e-01)	Acc@1  73.00 ( 75.90)	Acc@5  97.00 ( 99.00)
Test: [ 30/100]	Time  0.036 ( 0.042)	Loss 7.0020e-01 (7.2110e-01)	Acc@1  74.00 ( 76.16)	Acc@5 100.00 ( 98.87)
Test: [ 40/100]	Time  0.036 ( 0.041)	Loss 6.5039e-01 (7.0933e-01)	Acc@1  76.00 ( 76.41)	Acc@5  99.00 ( 98.90)
Test: [ 50/100]	Time  0.037 ( 0.040)	Loss 5.4395e-01 (7.0290e-01)	Acc@1  78.00 ( 76.45)	Acc@5 100.00 ( 98.90)
Test: [ 60/100]	Time  0.038 ( 0.039)	Loss 7.2510e-01 (7.0392e-01)	Acc@1  73.00 ( 76.38)	Acc@5 100.00 ( 98.98)
Test: [ 70/100]	Time  0.038 ( 0.039)	Loss 5.6836e-01 (7.0505e-01)	Acc@1  78.00 ( 76.37)	Acc@5  99.00 ( 98.96)
Test: [ 80/100]	Time  0.036 ( 0.039)	Loss 8.2178e-01 (7.1307e-01)	Acc@1  76.00 ( 76.27)	Acc@5  98.00 ( 98.94)
Test: [ 90/100]	Time  0.038 ( 0.039)	Loss 5.2539e-01 (7.0895e-01)	Acc@1  82.00 ( 76.40)	Acc@5 100.00 ( 98.93)
 * Acc@1 76.540 Acc@5 98.940
### epoch[17] execution time: 46.48444223403931
EPOCH 18
# current learning rate: 0.1
# Switched to train mode...
Epoch: [18][  0/391]	Time  0.277 ( 0.277)	Data  0.176 ( 0.176)	Loss 5.1904e-01 (5.1904e-01)	Acc@1  82.81 ( 82.81)	Acc@5 100.00 (100.00)
Epoch: [18][ 10/391]	Time  0.109 ( 0.124)	Data  0.001 ( 0.019)	Loss 3.8330e-01 (4.4358e-01)	Acc@1  84.38 ( 85.01)	Acc@5  99.22 ( 99.57)
Epoch: [18][ 20/391]	Time  0.108 ( 0.116)	Data  0.001 ( 0.012)	Loss 3.4351e-01 (4.5848e-01)	Acc@1  89.06 ( 84.41)	Acc@5  99.22 ( 99.44)
Epoch: [18][ 30/391]	Time  0.107 ( 0.114)	Data  0.001 ( 0.010)	Loss 3.9990e-01 (4.5757e-01)	Acc@1  85.16 ( 84.30)	Acc@5 100.00 ( 99.45)
Epoch: [18][ 40/391]	Time  0.110 ( 0.112)	Data  0.001 ( 0.008)	Loss 6.1230e-01 (4.8402e-01)	Acc@1  77.34 ( 83.40)	Acc@5  99.22 ( 99.16)
Epoch: [18][ 50/391]	Time  0.106 ( 0.112)	Data  0.001 ( 0.007)	Loss 5.6641e-01 (4.9971e-01)	Acc@1  74.22 ( 82.52)	Acc@5 100.00 ( 99.17)
Epoch: [18][ 60/391]	Time  0.104 ( 0.111)	Data  0.001 ( 0.007)	Loss 6.3867e-01 (5.1905e-01)	Acc@1  78.91 ( 81.92)	Acc@5 100.00 ( 99.07)
Epoch: [18][ 70/391]	Time  0.106 ( 0.111)	Data  0.001 ( 0.006)	Loss 5.1465e-01 (5.1657e-01)	Acc@1  82.03 ( 81.95)	Acc@5  97.66 ( 99.04)
Epoch: [18][ 80/391]	Time  0.109 ( 0.111)	Data  0.001 ( 0.006)	Loss 4.2456e-01 (5.1632e-01)	Acc@1  85.16 ( 81.85)	Acc@5 100.00 ( 99.08)
Epoch: [18][ 90/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 4.5142e-01 (5.1710e-01)	Acc@1  85.16 ( 81.81)	Acc@5  99.22 ( 99.08)
Epoch: [18][100/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 5.4883e-01 (5.1603e-01)	Acc@1  82.81 ( 81.89)	Acc@5  99.22 ( 99.09)
Epoch: [18][110/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 4.8193e-01 (5.1096e-01)	Acc@1  83.59 ( 82.21)	Acc@5  99.22 ( 99.13)
Epoch: [18][120/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 4.7314e-01 (5.0725e-01)	Acc@1  80.47 ( 82.30)	Acc@5  99.22 ( 99.12)
Epoch: [18][130/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.005)	Loss 4.6338e-01 (5.0592e-01)	Acc@1  82.81 ( 82.32)	Acc@5  99.22 ( 99.15)
Epoch: [18][140/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.005)	Loss 4.4434e-01 (5.0692e-01)	Acc@1  85.94 ( 82.32)	Acc@5  99.22 ( 99.16)
Epoch: [18][150/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.5068e-01 (5.0436e-01)	Acc@1  85.16 ( 82.41)	Acc@5  98.44 ( 99.17)
Epoch: [18][160/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.5957e-01 (5.0140e-01)	Acc@1  80.47 ( 82.47)	Acc@5 100.00 ( 99.18)
Epoch: [18][170/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.0732e-01 (5.0143e-01)	Acc@1  83.59 ( 82.53)	Acc@5  99.22 ( 99.18)
Epoch: [18][180/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.0439e-01 (5.0028e-01)	Acc@1  82.81 ( 82.59)	Acc@5  99.22 ( 99.21)
Epoch: [18][190/391]	Time  0.112 ( 0.109)	Data  0.002 ( 0.005)	Loss 4.9243e-01 (4.9924e-01)	Acc@1  84.38 ( 82.62)	Acc@5  99.22 ( 99.21)
Epoch: [18][200/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.1919e-01 (4.9698e-01)	Acc@1  82.81 ( 82.68)	Acc@5 100.00 ( 99.24)
Epoch: [18][210/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.1406e-01 (4.9349e-01)	Acc@1  87.50 ( 82.84)	Acc@5  97.66 ( 99.25)
Epoch: [18][220/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.2178e-01 (4.9149e-01)	Acc@1  90.62 ( 82.92)	Acc@5 100.00 ( 99.25)
Epoch: [18][230/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.1870e-01 (4.9225e-01)	Acc@1  84.38 ( 82.88)	Acc@5  99.22 ( 99.25)
Epoch: [18][240/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.2114e-01 (4.9182e-01)	Acc@1  88.28 ( 82.87)	Acc@5  99.22 ( 99.25)
Epoch: [18][250/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.0840e-01 (4.9225e-01)	Acc@1  79.69 ( 82.86)	Acc@5 100.00 ( 99.25)
Epoch: [18][260/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.7988e-01 (4.9050e-01)	Acc@1  86.72 ( 82.92)	Acc@5 100.00 ( 99.24)
Epoch: [18][270/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.4702e-01 (4.9153e-01)	Acc@1  85.16 ( 82.93)	Acc@5 100.00 ( 99.26)
Epoch: [18][280/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.8975e-01 (4.8993e-01)	Acc@1  82.03 ( 82.95)	Acc@5 100.00 ( 99.27)
Epoch: [18][290/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.1572e-01 (4.8892e-01)	Acc@1  78.12 ( 82.97)	Acc@5  99.22 ( 99.26)
Epoch: [18][300/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.5469e-01 (4.8834e-01)	Acc@1  81.25 ( 82.98)	Acc@5  98.44 ( 99.27)
Epoch: [18][310/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.3481e-01 (4.8800e-01)	Acc@1  85.94 ( 83.01)	Acc@5 100.00 ( 99.26)
Epoch: [18][320/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.0293e-01 (4.8631e-01)	Acc@1  80.47 ( 83.05)	Acc@5  99.22 ( 99.27)
Epoch: [18][330/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.9526e-01 (4.8562e-01)	Acc@1  82.81 ( 83.09)	Acc@5 100.00 ( 99.28)
Epoch: [18][340/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.7974e-01 (4.8499e-01)	Acc@1  84.38 ( 83.12)	Acc@5 100.00 ( 99.29)
Epoch: [18][350/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.0000e-01 (4.8499e-01)	Acc@1  78.91 ( 83.11)	Acc@5 100.00 ( 99.30)
Epoch: [18][360/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.9194e-01 (4.8610e-01)	Acc@1  84.38 ( 83.07)	Acc@5  98.44 ( 99.29)
Epoch: [18][370/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.5996e-01 (4.8569e-01)	Acc@1  83.59 ( 83.07)	Acc@5  99.22 ( 99.29)
Epoch: [18][380/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.3467e-01 (4.8487e-01)	Acc@1  78.12 ( 83.09)	Acc@5 100.00 ( 99.29)
Epoch: [18][390/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.5957e-01 (4.8474e-01)	Acc@1  80.00 ( 83.10)	Acc@5  98.75 ( 99.28)
## e[18] optimizer.zero_grad (sum) time: 0.3480558395385742
## e[18]       loss.backward (sum) time: 13.726063966751099
## e[18]      optimizer.step (sum) time: 2.897921323776245
## epoch[18] training(only) time: 42.71600103378296
# Switched to evaluate mode...
Test: [  0/100]	Time  0.233 ( 0.233)	Loss 4.4409e-01 (4.4409e-01)	Acc@1  85.00 ( 85.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.044 ( 0.056)	Loss 5.9619e-01 (5.1589e-01)	Acc@1  81.00 ( 81.82)	Acc@5  99.00 ( 99.45)
Test: [ 20/100]	Time  0.036 ( 0.047)	Loss 5.5273e-01 (5.2407e-01)	Acc@1  80.00 ( 81.86)	Acc@5 100.00 ( 99.29)
Test: [ 30/100]	Time  0.037 ( 0.044)	Loss 4.9976e-01 (5.2913e-01)	Acc@1  83.00 ( 81.90)	Acc@5  99.00 ( 99.13)
Test: [ 40/100]	Time  0.036 ( 0.042)	Loss 5.4980e-01 (5.3186e-01)	Acc@1  82.00 ( 81.90)	Acc@5 100.00 ( 99.10)
Test: [ 50/100]	Time  0.036 ( 0.041)	Loss 4.4702e-01 (5.3011e-01)	Acc@1  84.00 ( 82.00)	Acc@5  98.00 ( 99.10)
Test: [ 60/100]	Time  0.036 ( 0.040)	Loss 5.4834e-01 (5.2878e-01)	Acc@1  83.00 ( 81.92)	Acc@5 100.00 ( 99.15)
Test: [ 70/100]	Time  0.036 ( 0.040)	Loss 6.1035e-01 (5.3201e-01)	Acc@1  77.00 ( 81.77)	Acc@5 100.00 ( 99.14)
Test: [ 80/100]	Time  0.036 ( 0.039)	Loss 5.0098e-01 (5.3083e-01)	Acc@1  81.00 ( 81.78)	Acc@5 100.00 ( 99.16)
Test: [ 90/100]	Time  0.038 ( 0.039)	Loss 4.4482e-01 (5.3227e-01)	Acc@1  85.00 ( 81.77)	Acc@5 100.00 ( 99.19)
 * Acc@1 81.820 Acc@5 99.190
### epoch[18] execution time: 46.75091218948364
EPOCH 19
# current learning rate: 0.1
# Switched to train mode...
Epoch: [19][  0/391]	Time  0.315 ( 0.315)	Data  0.222 ( 0.222)	Loss 4.2212e-01 (4.2212e-01)	Acc@1  84.38 ( 84.38)	Acc@5 100.00 (100.00)
Epoch: [19][ 10/391]	Time  0.107 ( 0.127)	Data  0.001 ( 0.024)	Loss 3.6328e-01 (4.4221e-01)	Acc@1  85.94 ( 85.37)	Acc@5 100.00 ( 99.64)
Epoch: [19][ 20/391]	Time  0.104 ( 0.118)	Data  0.001 ( 0.014)	Loss 4.4409e-01 (4.3562e-01)	Acc@1  85.16 ( 85.31)	Acc@5  99.22 ( 99.59)
Epoch: [19][ 30/391]	Time  0.105 ( 0.115)	Data  0.001 ( 0.011)	Loss 5.0391e-01 (4.3849e-01)	Acc@1  82.81 ( 85.33)	Acc@5 100.00 ( 99.47)
Epoch: [19][ 40/391]	Time  0.109 ( 0.113)	Data  0.001 ( 0.009)	Loss 5.6250e-01 (4.3983e-01)	Acc@1  80.47 ( 85.21)	Acc@5  98.44 ( 99.47)
Epoch: [19][ 50/391]	Time  0.106 ( 0.112)	Data  0.001 ( 0.008)	Loss 4.3896e-01 (4.3760e-01)	Acc@1  86.72 ( 85.20)	Acc@5 100.00 ( 99.53)
Epoch: [19][ 60/391]	Time  0.105 ( 0.112)	Data  0.001 ( 0.008)	Loss 3.6060e-01 (4.3720e-01)	Acc@1  86.72 ( 85.23)	Acc@5  99.22 ( 99.46)
Epoch: [19][ 70/391]	Time  0.106 ( 0.111)	Data  0.001 ( 0.007)	Loss 4.0234e-01 (4.3420e-01)	Acc@1  82.81 ( 85.21)	Acc@5  99.22 ( 99.50)
Epoch: [19][ 80/391]	Time  0.107 ( 0.111)	Data  0.001 ( 0.007)	Loss 3.6646e-01 (4.3166e-01)	Acc@1  87.50 ( 85.15)	Acc@5 100.00 ( 99.55)
Epoch: [19][ 90/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.007)	Loss 5.0146e-01 (4.3060e-01)	Acc@1  81.25 ( 85.13)	Acc@5  99.22 ( 99.54)
Epoch: [19][100/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.006)	Loss 4.2407e-01 (4.3142e-01)	Acc@1  86.72 ( 85.13)	Acc@5 100.00 ( 99.57)
Epoch: [19][110/391]	Time  0.110 ( 0.110)	Data  0.001 ( 0.006)	Loss 3.3789e-01 (4.3173e-01)	Acc@1  87.50 ( 85.14)	Acc@5 100.00 ( 99.54)
Epoch: [19][120/391]	Time  0.104 ( 0.110)	Data  0.001 ( 0.006)	Loss 4.9658e-01 (4.3335e-01)	Acc@1  84.38 ( 85.04)	Acc@5 100.00 ( 99.54)
Epoch: [19][130/391]	Time  0.104 ( 0.110)	Data  0.001 ( 0.006)	Loss 3.5986e-01 (4.3555e-01)	Acc@1  85.94 ( 84.94)	Acc@5 100.00 ( 99.53)
Epoch: [19][140/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.006)	Loss 5.1709e-01 (4.3831e-01)	Acc@1  76.56 ( 84.79)	Acc@5 100.00 ( 99.52)
Epoch: [19][150/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 5.1318e-01 (4.4291e-01)	Acc@1  83.59 ( 84.63)	Acc@5  99.22 ( 99.49)
Epoch: [19][160/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.006)	Loss 4.2114e-01 (4.4129e-01)	Acc@1  87.50 ( 84.73)	Acc@5  98.44 ( 99.48)
Epoch: [19][170/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.3262e-01 (4.3992e-01)	Acc@1  82.03 ( 84.76)	Acc@5  99.22 ( 99.48)
Epoch: [19][180/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.2344e-01 (4.4134e-01)	Acc@1  82.81 ( 84.70)	Acc@5  99.22 ( 99.49)
Epoch: [19][190/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.4189e-01 (4.4158e-01)	Acc@1  82.81 ( 84.68)	Acc@5 100.00 ( 99.50)
Epoch: [19][200/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.2432e-01 (4.4070e-01)	Acc@1  86.72 ( 84.76)	Acc@5 100.00 ( 99.50)
Epoch: [19][210/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.6396e-01 (4.4170e-01)	Acc@1  82.03 ( 84.70)	Acc@5  98.44 ( 99.49)
Epoch: [19][220/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.8770e-01 (4.4307e-01)	Acc@1  85.16 ( 84.63)	Acc@5 100.00 ( 99.48)
Epoch: [19][230/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.4214e-01 (4.4136e-01)	Acc@1  85.94 ( 84.66)	Acc@5  98.44 ( 99.46)
Epoch: [19][240/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.0244e-01 (4.4147e-01)	Acc@1  85.16 ( 84.67)	Acc@5  98.44 ( 99.46)
Epoch: [19][250/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.1357e-01 (4.4003e-01)	Acc@1  85.94 ( 84.71)	Acc@5  99.22 ( 99.45)
Epoch: [19][260/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.4531e-01 (4.3948e-01)	Acc@1  82.81 ( 84.70)	Acc@5 100.00 ( 99.46)
Epoch: [19][270/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.7368e-01 (4.3946e-01)	Acc@1  92.19 ( 84.73)	Acc@5 100.00 ( 99.45)
Epoch: [19][280/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.0732e-01 (4.4047e-01)	Acc@1  77.34 ( 84.69)	Acc@5 100.00 ( 99.43)
Epoch: [19][290/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.9771e-01 (4.4013e-01)	Acc@1  87.50 ( 84.73)	Acc@5 100.00 ( 99.43)
Epoch: [19][300/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.4546e-01 (4.3999e-01)	Acc@1  89.06 ( 84.76)	Acc@5 100.00 ( 99.43)
Epoch: [19][310/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.4150e-01 (4.4013e-01)	Acc@1  83.59 ( 84.75)	Acc@5  99.22 ( 99.44)
Epoch: [19][320/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.2871e-01 (4.3988e-01)	Acc@1  80.47 ( 84.77)	Acc@5  99.22 ( 99.45)
Epoch: [19][330/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.6646e-01 (4.4019e-01)	Acc@1  84.38 ( 84.79)	Acc@5 100.00 ( 99.43)
Epoch: [19][340/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.3872e-01 (4.3950e-01)	Acc@1  83.59 ( 84.82)	Acc@5  99.22 ( 99.44)
Epoch: [19][350/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.0049e-01 (4.3961e-01)	Acc@1  85.16 ( 84.81)	Acc@5  98.44 ( 99.44)
Epoch: [19][360/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.3188e-01 (4.3935e-01)	Acc@1  84.38 ( 84.81)	Acc@5  99.22 ( 99.43)
Epoch: [19][370/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.9023e-01 (4.3860e-01)	Acc@1  82.81 ( 84.82)	Acc@5  98.44 ( 99.43)
Epoch: [19][380/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.5815e-01 (4.3832e-01)	Acc@1  89.06 ( 84.80)	Acc@5 100.00 ( 99.44)
Epoch: [19][390/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.4692e-01 (4.3898e-01)	Acc@1  90.00 ( 84.77)	Acc@5 100.00 ( 99.44)
## e[19] optimizer.zero_grad (sum) time: 0.34905171394348145
## e[19]       loss.backward (sum) time: 13.67984127998352
## e[19]      optimizer.step (sum) time: 2.7787375450134277
## epoch[19] training(only) time: 42.70507788658142
# Switched to evaluate mode...
Test: [  0/100]	Time  0.198 ( 0.198)	Loss 5.1270e-01 (5.1270e-01)	Acc@1  84.00 ( 84.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.039 ( 0.053)	Loss 5.4199e-01 (5.3337e-01)	Acc@1  82.00 ( 82.18)	Acc@5 100.00 ( 99.36)
Test: [ 20/100]	Time  0.036 ( 0.045)	Loss 5.6934e-01 (5.3572e-01)	Acc@1  77.00 ( 82.00)	Acc@5  99.00 ( 99.24)
Test: [ 30/100]	Time  0.037 ( 0.043)	Loss 4.6631e-01 (5.3936e-01)	Acc@1  85.00 ( 82.39)	Acc@5 100.00 ( 99.19)
Test: [ 40/100]	Time  0.036 ( 0.041)	Loss 4.5776e-01 (5.3321e-01)	Acc@1  83.00 ( 82.41)	Acc@5  99.00 ( 99.17)
Test: [ 50/100]	Time  0.036 ( 0.040)	Loss 3.9990e-01 (5.2796e-01)	Acc@1  88.00 ( 82.69)	Acc@5  99.00 ( 99.16)
Test: [ 60/100]	Time  0.036 ( 0.040)	Loss 4.4775e-01 (5.1952e-01)	Acc@1  86.00 ( 82.82)	Acc@5 100.00 ( 99.20)
Test: [ 70/100]	Time  0.037 ( 0.039)	Loss 7.4414e-01 (5.1959e-01)	Acc@1  75.00 ( 82.72)	Acc@5  99.00 ( 99.23)
Test: [ 80/100]	Time  0.036 ( 0.039)	Loss 5.0586e-01 (5.2228e-01)	Acc@1  78.00 ( 82.62)	Acc@5 100.00 ( 99.26)
Test: [ 90/100]	Time  0.039 ( 0.039)	Loss 4.4800e-01 (5.2227e-01)	Acc@1  88.00 ( 82.67)	Acc@5 100.00 ( 99.26)
 * Acc@1 82.670 Acc@5 99.250
### epoch[19] execution time: 46.668410301208496
EPOCH 20
# current learning rate: 0.1
# Switched to train mode...
Epoch: [20][  0/391]	Time  0.270 ( 0.270)	Data  0.168 ( 0.168)	Loss 3.4888e-01 (3.4888e-01)	Acc@1  89.06 ( 89.06)	Acc@5  99.22 ( 99.22)
Epoch: [20][ 10/391]	Time  0.107 ( 0.122)	Data  0.001 ( 0.019)	Loss 3.8354e-01 (4.4023e-01)	Acc@1  86.72 ( 85.44)	Acc@5 100.00 ( 99.22)
Epoch: [20][ 20/391]	Time  0.106 ( 0.115)	Data  0.001 ( 0.012)	Loss 4.7900e-01 (4.3759e-01)	Acc@1  82.03 ( 85.49)	Acc@5 100.00 ( 99.33)
Epoch: [20][ 30/391]	Time  0.106 ( 0.113)	Data  0.001 ( 0.009)	Loss 4.4019e-01 (4.3419e-01)	Acc@1  82.81 ( 85.33)	Acc@5  99.22 ( 99.37)
Epoch: [20][ 40/391]	Time  0.107 ( 0.112)	Data  0.001 ( 0.008)	Loss 2.9712e-01 (4.1623e-01)	Acc@1  92.97 ( 85.96)	Acc@5  98.44 ( 99.43)
Epoch: [20][ 50/391]	Time  0.108 ( 0.111)	Data  0.001 ( 0.007)	Loss 4.1455e-01 (4.1064e-01)	Acc@1  86.72 ( 86.11)	Acc@5  99.22 ( 99.49)
Epoch: [20][ 60/391]	Time  0.106 ( 0.111)	Data  0.001 ( 0.007)	Loss 4.3091e-01 (4.1365e-01)	Acc@1  87.50 ( 86.05)	Acc@5  99.22 ( 99.50)
Epoch: [20][ 70/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 4.0405e-01 (4.1153e-01)	Acc@1  85.16 ( 86.07)	Acc@5 100.00 ( 99.56)
Epoch: [20][ 80/391]	Time  0.102 ( 0.110)	Data  0.001 ( 0.006)	Loss 3.3862e-01 (4.1130e-01)	Acc@1  88.28 ( 86.01)	Acc@5 100.00 ( 99.59)
Epoch: [20][ 90/391]	Time  0.101 ( 0.110)	Data  0.001 ( 0.006)	Loss 4.5166e-01 (4.1025e-01)	Acc@1  84.38 ( 86.07)	Acc@5 100.00 ( 99.61)
Epoch: [20][100/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.006)	Loss 3.5767e-01 (4.1158e-01)	Acc@1  88.28 ( 86.03)	Acc@5  98.44 ( 99.59)
Epoch: [20][110/391]	Time  0.101 ( 0.110)	Data  0.001 ( 0.006)	Loss 4.2383e-01 (4.1146e-01)	Acc@1  85.94 ( 85.91)	Acc@5  98.44 ( 99.58)
Epoch: [20][120/391]	Time  0.100 ( 0.109)	Data  0.001 ( 0.006)	Loss 2.7588e-01 (4.0729e-01)	Acc@1  89.06 ( 86.03)	Acc@5 100.00 ( 99.57)
Epoch: [20][130/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.6460e-01 (4.0609e-01)	Acc@1  84.38 ( 86.10)	Acc@5 100.00 ( 99.57)
Epoch: [20][140/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.6401e-01 (4.0781e-01)	Acc@1  86.72 ( 86.01)	Acc@5 100.00 ( 99.57)
Epoch: [20][150/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.7197e-01 (4.0851e-01)	Acc@1  91.41 ( 86.03)	Acc@5  99.22 ( 99.54)
Epoch: [20][160/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.5034e-01 (4.1013e-01)	Acc@1  87.50 ( 85.96)	Acc@5  99.22 ( 99.51)
Epoch: [20][170/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.4790e-01 (4.1015e-01)	Acc@1  89.06 ( 85.89)	Acc@5  99.22 ( 99.50)
Epoch: [20][180/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.8071e-01 (4.1084e-01)	Acc@1  84.38 ( 85.88)	Acc@5 100.00 ( 99.51)
Epoch: [20][190/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.1494e-01 (4.0937e-01)	Acc@1  89.06 ( 85.89)	Acc@5 100.00 ( 99.50)
Epoch: [20][200/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.4702e-01 (4.1012e-01)	Acc@1  83.59 ( 85.85)	Acc@5  99.22 ( 99.51)
Epoch: [20][210/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.1455e-01 (4.1031e-01)	Acc@1  86.72 ( 85.84)	Acc@5 100.00 ( 99.51)
Epoch: [20][220/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.5654e-01 (4.1054e-01)	Acc@1  83.59 ( 85.83)	Acc@5  99.22 ( 99.52)
Epoch: [20][230/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.0527e-01 (4.1113e-01)	Acc@1  83.59 ( 85.80)	Acc@5 100.00 ( 99.52)
Epoch: [20][240/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.4019e-01 (4.1166e-01)	Acc@1  81.25 ( 85.75)	Acc@5  99.22 ( 99.53)
Epoch: [20][250/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.3140e-01 (4.1132e-01)	Acc@1  85.16 ( 85.79)	Acc@5  99.22 ( 99.52)
Epoch: [20][260/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.1367e-01 (4.1131e-01)	Acc@1  79.69 ( 85.79)	Acc@5  99.22 ( 99.52)
Epoch: [20][270/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.2627e-01 (4.1156e-01)	Acc@1  88.28 ( 85.79)	Acc@5  98.44 ( 99.51)
Epoch: [20][280/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.9795e-01 (4.1165e-01)	Acc@1  83.59 ( 85.72)	Acc@5  99.22 ( 99.52)
Epoch: [20][290/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.5132e-01 (4.1257e-01)	Acc@1  88.28 ( 85.69)	Acc@5 100.00 ( 99.50)
Epoch: [20][300/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.0103e-01 (4.1149e-01)	Acc@1  91.41 ( 85.74)	Acc@5  99.22 ( 99.51)
Epoch: [20][310/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.4751e-01 (4.1185e-01)	Acc@1  84.38 ( 85.75)	Acc@5  99.22 ( 99.52)
Epoch: [20][320/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.2739e-01 (4.1227e-01)	Acc@1  89.84 ( 85.75)	Acc@5 100.00 ( 99.51)
Epoch: [20][330/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.4248e-01 (4.1417e-01)	Acc@1  81.25 ( 85.71)	Acc@5 100.00 ( 99.51)
Epoch: [20][340/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.7681e-01 (4.1452e-01)	Acc@1  82.03 ( 85.67)	Acc@5 100.00 ( 99.50)
Epoch: [20][350/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.9902e-01 (4.1391e-01)	Acc@1  82.03 ( 85.69)	Acc@5  99.22 ( 99.50)
Epoch: [20][360/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.2344e-01 (4.1456e-01)	Acc@1  82.81 ( 85.66)	Acc@5  99.22 ( 99.50)
Epoch: [20][370/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.1885e-01 (4.1438e-01)	Acc@1  89.06 ( 85.65)	Acc@5 100.00 ( 99.51)
Epoch: [20][380/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.4053e-01 (4.1532e-01)	Acc@1  81.25 ( 85.59)	Acc@5  99.22 ( 99.51)
Epoch: [20][390/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.8252e-01 (4.1451e-01)	Acc@1  81.25 ( 85.60)	Acc@5 100.00 ( 99.51)
## e[20] optimizer.zero_grad (sum) time: 0.35254597663879395
## e[20]       loss.backward (sum) time: 13.710793495178223
## e[20]      optimizer.step (sum) time: 2.7672622203826904
## epoch[20] training(only) time: 42.73482799530029
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 3.8086e-01 (3.8086e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.036 ( 0.052)	Loss 5.2490e-01 (4.3799e-01)	Acc@1  87.00 ( 85.64)	Acc@5  99.00 ( 99.36)
Test: [ 20/100]	Time  0.038 ( 0.045)	Loss 5.0488e-01 (4.4434e-01)	Acc@1  85.00 ( 85.43)	Acc@5 100.00 ( 99.14)
Test: [ 30/100]	Time  0.036 ( 0.042)	Loss 4.2480e-01 (4.5409e-01)	Acc@1  85.00 ( 85.19)	Acc@5  99.00 ( 99.29)
Test: [ 40/100]	Time  0.036 ( 0.041)	Loss 4.1895e-01 (4.5334e-01)	Acc@1  88.00 ( 85.07)	Acc@5  99.00 ( 99.27)
Test: [ 50/100]	Time  0.036 ( 0.040)	Loss 3.0493e-01 (4.4966e-01)	Acc@1  89.00 ( 85.18)	Acc@5 100.00 ( 99.29)
Test: [ 60/100]	Time  0.039 ( 0.040)	Loss 4.2163e-01 (4.4984e-01)	Acc@1  85.00 ( 85.10)	Acc@5 100.00 ( 99.36)
Test: [ 70/100]	Time  0.037 ( 0.040)	Loss 5.3711e-01 (4.4841e-01)	Acc@1  83.00 ( 85.00)	Acc@5 100.00 ( 99.39)
Test: [ 80/100]	Time  0.036 ( 0.039)	Loss 3.2568e-01 (4.4971e-01)	Acc@1  87.00 ( 84.72)	Acc@5 100.00 ( 99.46)
Test: [ 90/100]	Time  0.036 ( 0.039)	Loss 2.7808e-01 (4.4682e-01)	Acc@1  88.00 ( 84.75)	Acc@5 100.00 ( 99.45)
 * Acc@1 84.730 Acc@5 99.460
### epoch[20] execution time: 46.71807789802551
EPOCH 21
# current learning rate: 0.1
# Switched to train mode...
Epoch: [21][  0/391]	Time  0.278 ( 0.278)	Data  0.170 ( 0.170)	Loss 4.6973e-01 (4.6973e-01)	Acc@1  86.72 ( 86.72)	Acc@5  99.22 ( 99.22)
Epoch: [21][ 10/391]	Time  0.107 ( 0.124)	Data  0.001 ( 0.019)	Loss 3.7012e-01 (3.8241e-01)	Acc@1  85.94 ( 86.43)	Acc@5  99.22 ( 99.64)
Epoch: [21][ 20/391]	Time  0.103 ( 0.116)	Data  0.001 ( 0.012)	Loss 2.8833e-01 (3.8575e-01)	Acc@1  90.62 ( 86.53)	Acc@5 100.00 ( 99.63)
Epoch: [21][ 30/391]	Time  0.105 ( 0.114)	Data  0.001 ( 0.009)	Loss 3.8721e-01 (3.9054e-01)	Acc@1  82.81 ( 86.39)	Acc@5 100.00 ( 99.65)
Epoch: [21][ 40/391]	Time  0.106 ( 0.112)	Data  0.001 ( 0.008)	Loss 3.8379e-01 (3.8830e-01)	Acc@1  84.38 ( 86.43)	Acc@5  99.22 ( 99.66)
Epoch: [21][ 50/391]	Time  0.103 ( 0.111)	Data  0.001 ( 0.007)	Loss 2.3584e-01 (3.8798e-01)	Acc@1  90.62 ( 86.46)	Acc@5 100.00 ( 99.66)
Epoch: [21][ 60/391]	Time  0.107 ( 0.111)	Data  0.001 ( 0.007)	Loss 4.3530e-01 (3.8778e-01)	Acc@1  84.38 ( 86.46)	Acc@5 100.00 ( 99.68)
Epoch: [21][ 70/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.007)	Loss 4.1870e-01 (3.8666e-01)	Acc@1  89.06 ( 86.63)	Acc@5  99.22 ( 99.64)
Epoch: [21][ 80/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.006)	Loss 5.8398e-01 (3.8724e-01)	Acc@1  81.25 ( 86.77)	Acc@5  99.22 ( 99.60)
Epoch: [21][ 90/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 4.0039e-01 (3.8781e-01)	Acc@1  86.72 ( 86.82)	Acc@5  98.44 ( 99.60)
Epoch: [21][100/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 4.3945e-01 (3.8985e-01)	Acc@1  85.94 ( 86.69)	Acc@5  99.22 ( 99.57)
Epoch: [21][110/391]	Time  0.109 ( 0.110)	Data  0.001 ( 0.006)	Loss 3.9478e-01 (3.8811e-01)	Acc@1  84.38 ( 86.70)	Acc@5 100.00 ( 99.58)
Epoch: [21][120/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 5.1611e-01 (3.9377e-01)	Acc@1  84.38 ( 86.54)	Acc@5  98.44 ( 99.54)
Epoch: [21][130/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.5811e-01 (3.9490e-01)	Acc@1  85.16 ( 86.41)	Acc@5 100.00 ( 99.55)
Epoch: [21][140/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.0527e-01 (3.9351e-01)	Acc@1  86.72 ( 86.45)	Acc@5 100.00 ( 99.57)
Epoch: [21][150/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.3384e-01 (3.9321e-01)	Acc@1  82.81 ( 86.45)	Acc@5  99.22 ( 99.56)
Epoch: [21][160/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.4482e-01 (3.9347e-01)	Acc@1  83.59 ( 86.42)	Acc@5 100.00 ( 99.57)
Epoch: [21][170/391]	Time  0.106 ( 0.109)	Data  0.002 ( 0.005)	Loss 4.0991e-01 (3.9465e-01)	Acc@1  84.38 ( 86.34)	Acc@5 100.00 ( 99.58)
Epoch: [21][180/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.9648e-01 (3.9627e-01)	Acc@1  85.16 ( 86.30)	Acc@5 100.00 ( 99.57)
Epoch: [21][190/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.3105e-01 (3.9593e-01)	Acc@1  89.84 ( 86.33)	Acc@5  99.22 ( 99.57)
Epoch: [21][200/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.3911e-01 (3.9593e-01)	Acc@1  86.72 ( 86.27)	Acc@5 100.00 ( 99.56)
Epoch: [21][210/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.2847e-01 (3.9661e-01)	Acc@1  83.59 ( 86.20)	Acc@5 100.00 ( 99.56)
Epoch: [21][220/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.5420e-01 (3.9646e-01)	Acc@1  78.91 ( 86.22)	Acc@5  98.44 ( 99.55)
Epoch: [21][230/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.9468e-01 (3.9774e-01)	Acc@1  88.28 ( 86.22)	Acc@5 100.00 ( 99.54)
Epoch: [21][240/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.2080e-01 (3.9666e-01)	Acc@1  86.72 ( 86.24)	Acc@5 100.00 ( 99.55)
Epoch: [21][250/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.1821e-01 (3.9773e-01)	Acc@1  86.72 ( 86.22)	Acc@5 100.00 ( 99.55)
Epoch: [21][260/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.9541e-01 (3.9684e-01)	Acc@1  89.06 ( 86.22)	Acc@5 100.00 ( 99.56)
Epoch: [21][270/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.7515e-01 (3.9610e-01)	Acc@1  91.41 ( 86.26)	Acc@5 100.00 ( 99.56)
Epoch: [21][280/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.4556e-01 (3.9643e-01)	Acc@1  86.72 ( 86.26)	Acc@5  97.66 ( 99.56)
Epoch: [21][290/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.1504e-01 (3.9557e-01)	Acc@1  84.38 ( 86.33)	Acc@5 100.00 ( 99.56)
Epoch: [21][300/391]	Time  0.107 ( 0.109)	Data  0.002 ( 0.005)	Loss 3.5376e-01 (3.9442e-01)	Acc@1  92.19 ( 86.39)	Acc@5  99.22 ( 99.56)
Epoch: [21][310/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.6157e-01 (3.9476e-01)	Acc@1  87.50 ( 86.37)	Acc@5 100.00 ( 99.56)
Epoch: [21][320/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.4573e-01 (3.9549e-01)	Acc@1  92.19 ( 86.33)	Acc@5 100.00 ( 99.56)
Epoch: [21][330/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.2822e-01 (3.9515e-01)	Acc@1  82.81 ( 86.35)	Acc@5  98.44 ( 99.56)
Epoch: [21][340/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.3384e-01 (3.9729e-01)	Acc@1  83.59 ( 86.29)	Acc@5  99.22 ( 99.55)
Epoch: [21][350/391]	Time  0.107 ( 0.109)	Data  0.002 ( 0.005)	Loss 4.5801e-01 (3.9756e-01)	Acc@1  83.59 ( 86.26)	Acc@5 100.00 ( 99.54)
Epoch: [21][360/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.3809e-01 (3.9854e-01)	Acc@1  78.91 ( 86.21)	Acc@5 100.00 ( 99.54)
Epoch: [21][370/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.2456e-01 (3.9917e-01)	Acc@1  85.94 ( 86.19)	Acc@5  99.22 ( 99.53)
Epoch: [21][380/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.5669e-01 (4.0008e-01)	Acc@1  89.06 ( 86.16)	Acc@5  99.22 ( 99.53)
Epoch: [21][390/391]	Time  0.101 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.0015e-01 (3.9989e-01)	Acc@1  85.00 ( 86.15)	Acc@5 100.00 ( 99.53)
## e[21] optimizer.zero_grad (sum) time: 0.3405921459197998
## e[21]       loss.backward (sum) time: 13.658630132675171
## e[21]      optimizer.step (sum) time: 2.765362501144409
## epoch[21] training(only) time: 42.71645379066467
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 3.1323e-01 (3.1323e-01)	Acc@1  89.00 ( 89.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.038 ( 0.053)	Loss 3.6523e-01 (3.7991e-01)	Acc@1  90.00 ( 86.64)	Acc@5  99.00 ( 99.55)
Test: [ 20/100]	Time  0.037 ( 0.045)	Loss 6.2891e-01 (4.2650e-01)	Acc@1  80.00 ( 85.43)	Acc@5 100.00 ( 99.38)
Test: [ 30/100]	Time  0.036 ( 0.042)	Loss 4.7607e-01 (4.3362e-01)	Acc@1  84.00 ( 85.48)	Acc@5 100.00 ( 99.32)
Test: [ 40/100]	Time  0.037 ( 0.041)	Loss 4.3701e-01 (4.3372e-01)	Acc@1  85.00 ( 85.41)	Acc@5  98.00 ( 99.27)
Test: [ 50/100]	Time  0.037 ( 0.040)	Loss 3.0518e-01 (4.3107e-01)	Acc@1  91.00 ( 85.55)	Acc@5  99.00 ( 99.20)
Test: [ 60/100]	Time  0.037 ( 0.040)	Loss 4.5776e-01 (4.3308e-01)	Acc@1  89.00 ( 85.52)	Acc@5 100.00 ( 99.25)
Test: [ 70/100]	Time  0.039 ( 0.040)	Loss 4.8755e-01 (4.3470e-01)	Acc@1  82.00 ( 85.37)	Acc@5 100.00 ( 99.24)
Test: [ 80/100]	Time  0.039 ( 0.039)	Loss 4.7314e-01 (4.3489e-01)	Acc@1  85.00 ( 85.26)	Acc@5  99.00 ( 99.27)
Test: [ 90/100]	Time  0.037 ( 0.039)	Loss 2.8931e-01 (4.3250e-01)	Acc@1  87.00 ( 85.33)	Acc@5 100.00 ( 99.32)
 * Acc@1 85.430 Acc@5 99.350
### epoch[21] execution time: 46.689589977264404
EPOCH 22
# current learning rate: 0.1
# Switched to train mode...
Epoch: [22][  0/391]	Time  0.276 ( 0.276)	Data  0.178 ( 0.178)	Loss 4.2407e-01 (4.2407e-01)	Acc@1  82.03 ( 82.03)	Acc@5 100.00 (100.00)
Epoch: [22][ 10/391]	Time  0.105 ( 0.123)	Data  0.001 ( 0.020)	Loss 3.3130e-01 (3.7983e-01)	Acc@1  89.06 ( 85.87)	Acc@5 100.00 ( 99.50)
Epoch: [22][ 20/391]	Time  0.108 ( 0.115)	Data  0.001 ( 0.012)	Loss 3.8525e-01 (3.7630e-01)	Acc@1  88.28 ( 86.64)	Acc@5 100.00 ( 99.63)
Epoch: [22][ 30/391]	Time  0.102 ( 0.113)	Data  0.001 ( 0.010)	Loss 3.3643e-01 (3.7503e-01)	Acc@1  87.50 ( 86.84)	Acc@5 100.00 ( 99.62)
Epoch: [22][ 40/391]	Time  0.104 ( 0.111)	Data  0.001 ( 0.008)	Loss 3.5156e-01 (3.7172e-01)	Acc@1  88.28 ( 86.97)	Acc@5 100.00 ( 99.68)
Epoch: [22][ 50/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.008)	Loss 2.9004e-01 (3.7399e-01)	Acc@1  88.28 ( 86.92)	Acc@5 100.00 ( 99.69)
Epoch: [22][ 60/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.007)	Loss 3.7256e-01 (3.7833e-01)	Acc@1  85.16 ( 86.76)	Acc@5  99.22 ( 99.64)
Epoch: [22][ 70/391]	Time  0.104 ( 0.110)	Data  0.001 ( 0.007)	Loss 5.4053e-01 (3.8004e-01)	Acc@1  84.38 ( 86.63)	Acc@5  99.22 ( 99.66)
Epoch: [22][ 80/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.006)	Loss 3.3398e-01 (3.8244e-01)	Acc@1  89.06 ( 86.52)	Acc@5 100.00 ( 99.67)
Epoch: [22][ 90/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.006)	Loss 2.9834e-01 (3.8225e-01)	Acc@1  89.84 ( 86.58)	Acc@5 100.00 ( 99.65)
Epoch: [22][100/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.006)	Loss 3.3203e-01 (3.8178e-01)	Acc@1  90.62 ( 86.59)	Acc@5 100.00 ( 99.62)
Epoch: [22][110/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.006)	Loss 3.2080e-01 (3.8215e-01)	Acc@1  90.62 ( 86.62)	Acc@5  99.22 ( 99.61)
Epoch: [22][120/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.006)	Loss 2.4890e-01 (3.8131e-01)	Acc@1  87.50 ( 86.66)	Acc@5 100.00 ( 99.57)
Epoch: [22][130/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.1748e-01 (3.8186e-01)	Acc@1  87.50 ( 86.68)	Acc@5  99.22 ( 99.57)
Epoch: [22][140/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.3530e-01 (3.8393e-01)	Acc@1  82.03 ( 86.59)	Acc@5 100.00 ( 99.57)
Epoch: [22][150/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.3750e-01 (3.8527e-01)	Acc@1  83.59 ( 86.45)	Acc@5  99.22 ( 99.57)
Epoch: [22][160/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.8159e-01 (3.8368e-01)	Acc@1  84.38 ( 86.49)	Acc@5  99.22 ( 99.57)
Epoch: [22][170/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.6074e-01 (3.8359e-01)	Acc@1  90.62 ( 86.52)	Acc@5 100.00 ( 99.58)
Epoch: [22][180/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.4946e-01 (3.8386e-01)	Acc@1  85.94 ( 86.52)	Acc@5  99.22 ( 99.59)
Epoch: [22][190/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.1348e-01 (3.8363e-01)	Acc@1  87.50 ( 86.53)	Acc@5 100.00 ( 99.58)
Epoch: [22][200/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.4214e-01 (3.8281e-01)	Acc@1  84.38 ( 86.54)	Acc@5 100.00 ( 99.58)
Epoch: [22][210/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.9805e-01 (3.8385e-01)	Acc@1  80.47 ( 86.53)	Acc@5  99.22 ( 99.59)
Epoch: [22][220/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.2676e-01 (3.8506e-01)	Acc@1  83.59 ( 86.49)	Acc@5 100.00 ( 99.58)
Epoch: [22][230/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.5508e-01 (3.8709e-01)	Acc@1  84.38 ( 86.44)	Acc@5 100.00 ( 99.57)
Epoch: [22][240/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.2275e-01 (3.8600e-01)	Acc@1  90.62 ( 86.51)	Acc@5 100.00 ( 99.58)
Epoch: [22][250/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.3286e-01 (3.8664e-01)	Acc@1  86.72 ( 86.48)	Acc@5 100.00 ( 99.59)
Epoch: [22][260/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.3774e-01 (3.8786e-01)	Acc@1  86.72 ( 86.47)	Acc@5 100.00 ( 99.57)
Epoch: [22][270/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.9551e-01 (3.8755e-01)	Acc@1  87.50 ( 86.51)	Acc@5  99.22 ( 99.57)
Epoch: [22][280/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.0845e-01 (3.8743e-01)	Acc@1  85.94 ( 86.53)	Acc@5  99.22 ( 99.56)
Epoch: [22][290/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.2983e-01 (3.8796e-01)	Acc@1  87.50 ( 86.46)	Acc@5 100.00 ( 99.57)
Epoch: [22][300/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.2935e-01 (3.8703e-01)	Acc@1  87.50 ( 86.51)	Acc@5 100.00 ( 99.57)
Epoch: [22][310/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.7842e-01 (3.8661e-01)	Acc@1  87.50 ( 86.53)	Acc@5  99.22 ( 99.57)
Epoch: [22][320/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.9185e-01 (3.8608e-01)	Acc@1  89.06 ( 86.54)	Acc@5 100.00 ( 99.58)
Epoch: [22][330/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.1421e-01 (3.8605e-01)	Acc@1  89.06 ( 86.57)	Acc@5 100.00 ( 99.58)
Epoch: [22][340/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.7266e-01 (3.8552e-01)	Acc@1  83.59 ( 86.56)	Acc@5 100.00 ( 99.58)
Epoch: [22][350/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.8354e-01 (3.8514e-01)	Acc@1  88.28 ( 86.59)	Acc@5  99.22 ( 99.58)
Epoch: [22][360/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.3203e-01 (3.8509e-01)	Acc@1  89.06 ( 86.59)	Acc@5 100.00 ( 99.58)
Epoch: [22][370/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.4473e-01 (3.8474e-01)	Acc@1  82.03 ( 86.59)	Acc@5 100.00 ( 99.58)
Epoch: [22][380/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.3384e-01 (3.8501e-01)	Acc@1  85.16 ( 86.57)	Acc@5  99.22 ( 99.58)
Epoch: [22][390/391]	Time  0.101 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.1455e-01 (3.8528e-01)	Acc@1  78.75 ( 86.53)	Acc@5 100.00 ( 99.58)
## e[22] optimizer.zero_grad (sum) time: 0.3466026782989502
## e[22]       loss.backward (sum) time: 13.691499710083008
## e[22]      optimizer.step (sum) time: 2.7814693450927734
## epoch[22] training(only) time: 42.57394886016846
# Switched to evaluate mode...
Test: [  0/100]	Time  0.206 ( 0.206)	Loss 3.8232e-01 (3.8232e-01)	Acc@1  88.00 ( 88.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.037 ( 0.053)	Loss 3.9429e-01 (4.1497e-01)	Acc@1  88.00 ( 86.73)	Acc@5  99.00 ( 99.00)
Test: [ 20/100]	Time  0.036 ( 0.045)	Loss 6.1816e-01 (4.4582e-01)	Acc@1  80.00 ( 85.38)	Acc@5 100.00 ( 99.14)
Test: [ 30/100]	Time  0.038 ( 0.043)	Loss 4.4287e-01 (4.6656e-01)	Acc@1  87.00 ( 84.94)	Acc@5  99.00 ( 99.19)
Test: [ 40/100]	Time  0.036 ( 0.041)	Loss 4.5435e-01 (4.6406e-01)	Acc@1  85.00 ( 84.98)	Acc@5  99.00 ( 99.22)
Test: [ 50/100]	Time  0.038 ( 0.041)	Loss 3.0859e-01 (4.5942e-01)	Acc@1  93.00 ( 84.98)	Acc@5  99.00 ( 99.22)
Test: [ 60/100]	Time  0.039 ( 0.040)	Loss 4.1455e-01 (4.5336e-01)	Acc@1  86.00 ( 85.15)	Acc@5 100.00 ( 99.30)
Test: [ 70/100]	Time  0.037 ( 0.039)	Loss 3.8501e-01 (4.5508e-01)	Acc@1  88.00 ( 85.06)	Acc@5 100.00 ( 99.31)
Test: [ 80/100]	Time  0.036 ( 0.039)	Loss 3.7134e-01 (4.5340e-01)	Acc@1  85.00 ( 84.94)	Acc@5 100.00 ( 99.31)
Test: [ 90/100]	Time  0.036 ( 0.039)	Loss 3.8916e-01 (4.5467e-01)	Acc@1  87.00 ( 84.89)	Acc@5 100.00 ( 99.32)
 * Acc@1 85.000 Acc@5 99.370
### epoch[22] execution time: 46.54798173904419
EPOCH 23
# current learning rate: 0.1
# Switched to train mode...
Epoch: [23][  0/391]	Time  0.277 ( 0.277)	Data  0.171 ( 0.171)	Loss 3.4717e-01 (3.4717e-01)	Acc@1  86.72 ( 86.72)	Acc@5 100.00 (100.00)
Epoch: [23][ 10/391]	Time  0.107 ( 0.124)	Data  0.001 ( 0.019)	Loss 3.7769e-01 (3.8718e-01)	Acc@1  90.62 ( 86.15)	Acc@5  98.44 ( 99.72)
Epoch: [23][ 20/391]	Time  0.106 ( 0.116)	Data  0.001 ( 0.012)	Loss 3.5718e-01 (4.0395e-01)	Acc@1  87.50 ( 85.49)	Acc@5 100.00 ( 99.59)
Epoch: [23][ 30/391]	Time  0.106 ( 0.114)	Data  0.001 ( 0.010)	Loss 2.0947e-01 (3.8134e-01)	Acc@1  92.19 ( 86.39)	Acc@5 100.00 ( 99.57)
Epoch: [23][ 40/391]	Time  0.106 ( 0.113)	Data  0.001 ( 0.008)	Loss 3.4009e-01 (3.7174e-01)	Acc@1  85.94 ( 86.79)	Acc@5 100.00 ( 99.62)
Epoch: [23][ 50/391]	Time  0.107 ( 0.112)	Data  0.001 ( 0.007)	Loss 4.2310e-01 (3.6908e-01)	Acc@1  88.28 ( 87.19)	Acc@5  99.22 ( 99.59)
Epoch: [23][ 60/391]	Time  0.107 ( 0.111)	Data  0.001 ( 0.007)	Loss 3.7036e-01 (3.7571e-01)	Acc@1  86.72 ( 87.23)	Acc@5  98.44 ( 99.47)
Epoch: [23][ 70/391]	Time  0.109 ( 0.111)	Data  0.001 ( 0.007)	Loss 5.2881e-01 (3.7912e-01)	Acc@1  78.91 ( 87.13)	Acc@5  99.22 ( 99.48)
Epoch: [23][ 80/391]	Time  0.108 ( 0.111)	Data  0.002 ( 0.006)	Loss 2.9663e-01 (3.8149e-01)	Acc@1  89.84 ( 86.98)	Acc@5  99.22 ( 99.48)
Epoch: [23][ 90/391]	Time  0.111 ( 0.110)	Data  0.001 ( 0.006)	Loss 2.7319e-01 (3.8518e-01)	Acc@1  92.97 ( 86.81)	Acc@5 100.00 ( 99.51)
Epoch: [23][100/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 4.1724e-01 (3.9379e-01)	Acc@1  88.28 ( 86.53)	Acc@5 100.00 ( 99.52)
Epoch: [23][110/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 3.5596e-01 (3.9865e-01)	Acc@1  87.50 ( 86.35)	Acc@5  99.22 ( 99.48)
Epoch: [23][120/391]	Time  0.109 ( 0.110)	Data  0.001 ( 0.006)	Loss 4.0576e-01 (4.0276e-01)	Acc@1  85.94 ( 86.21)	Acc@5 100.00 ( 99.45)
Epoch: [23][130/391]	Time  0.104 ( 0.110)	Data  0.001 ( 0.005)	Loss 4.0405e-01 (4.0242e-01)	Acc@1  82.81 ( 86.15)	Acc@5 100.00 ( 99.47)
Epoch: [23][140/391]	Time  0.111 ( 0.110)	Data  0.001 ( 0.005)	Loss 4.0405e-01 (4.0156e-01)	Acc@1  86.72 ( 86.13)	Acc@5  99.22 ( 99.48)
Epoch: [23][150/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.005)	Loss 4.9707e-01 (4.0627e-01)	Acc@1  83.59 ( 86.04)	Acc@5  98.44 ( 99.48)
Epoch: [23][160/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.005)	Loss 3.9673e-01 (4.0472e-01)	Acc@1  88.28 ( 86.10)	Acc@5  97.66 ( 99.49)
Epoch: [23][170/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.005)	Loss 4.2065e-01 (4.0266e-01)	Acc@1  83.59 ( 86.10)	Acc@5  99.22 ( 99.50)
Epoch: [23][180/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.005)	Loss 3.6133e-01 (4.0127e-01)	Acc@1  85.94 ( 86.15)	Acc@5 100.00 ( 99.50)
Epoch: [23][190/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.5215e-01 (4.0028e-01)	Acc@1  83.59 ( 86.17)	Acc@5 100.00 ( 99.50)
Epoch: [23][200/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.5137e-01 (4.0058e-01)	Acc@1  78.12 ( 86.15)	Acc@5  96.88 ( 99.48)
Epoch: [23][210/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.2178e-01 (3.9958e-01)	Acc@1  89.84 ( 86.18)	Acc@5 100.00 ( 99.50)
Epoch: [23][220/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.3105e-01 (3.9944e-01)	Acc@1  88.28 ( 86.18)	Acc@5  99.22 ( 99.50)
Epoch: [23][230/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.7729e-01 (3.9894e-01)	Acc@1  84.38 ( 86.20)	Acc@5  99.22 ( 99.50)
Epoch: [23][240/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.2227e-01 (3.9685e-01)	Acc@1  88.28 ( 86.26)	Acc@5 100.00 ( 99.51)
Epoch: [23][250/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.1860e-01 (3.9605e-01)	Acc@1  89.84 ( 86.28)	Acc@5  99.22 ( 99.51)
Epoch: [23][260/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.2583e-01 (3.9395e-01)	Acc@1  90.62 ( 86.31)	Acc@5 100.00 ( 99.53)
Epoch: [23][270/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.5962e-01 (3.9306e-01)	Acc@1  86.72 ( 86.33)	Acc@5 100.00 ( 99.53)
Epoch: [23][280/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.5825e-01 (3.9497e-01)	Acc@1  85.94 ( 86.25)	Acc@5  99.22 ( 99.52)
Epoch: [23][290/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.0405e-01 (3.9516e-01)	Acc@1  85.94 ( 86.24)	Acc@5  99.22 ( 99.52)
Epoch: [23][300/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.7256e-01 (3.9446e-01)	Acc@1  86.72 ( 86.29)	Acc@5  98.44 ( 99.51)
Epoch: [23][310/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.1040e-01 (3.9441e-01)	Acc@1  82.81 ( 86.28)	Acc@5 100.00 ( 99.52)
Epoch: [23][320/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.2300e-01 (3.9308e-01)	Acc@1  87.50 ( 86.32)	Acc@5 100.00 ( 99.52)
Epoch: [23][330/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.6396e-01 (3.9320e-01)	Acc@1  80.47 ( 86.34)	Acc@5  99.22 ( 99.51)
Epoch: [23][340/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.8428e-01 (3.9473e-01)	Acc@1  86.72 ( 86.28)	Acc@5  98.44 ( 99.51)
Epoch: [23][350/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.2568e-01 (3.9484e-01)	Acc@1  86.72 ( 86.26)	Acc@5 100.00 ( 99.51)
Epoch: [23][360/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.4326e-01 (3.9382e-01)	Acc@1  88.28 ( 86.29)	Acc@5 100.00 ( 99.52)
Epoch: [23][370/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.6792e-01 (3.9359e-01)	Acc@1  85.94 ( 86.33)	Acc@5 100.00 ( 99.52)
Epoch: [23][380/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.5830e-01 (3.9297e-01)	Acc@1  90.62 ( 86.35)	Acc@5 100.00 ( 99.52)
Epoch: [23][390/391]	Time  0.100 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.0503e-01 (3.9225e-01)	Acc@1  87.50 ( 86.40)	Acc@5 100.00 ( 99.51)
## e[23] optimizer.zero_grad (sum) time: 0.35184335708618164
## e[23]       loss.backward (sum) time: 13.714313268661499
## e[23]      optimizer.step (sum) time: 2.7604899406433105
## epoch[23] training(only) time: 42.718271255493164
# Switched to evaluate mode...
Test: [  0/100]	Time  0.207 ( 0.207)	Loss 4.0723e-01 (4.0723e-01)	Acc@1  88.00 ( 88.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.036 ( 0.053)	Loss 3.3130e-01 (4.1158e-01)	Acc@1  89.00 ( 86.18)	Acc@5 100.00 ( 99.36)
Test: [ 20/100]	Time  0.037 ( 0.045)	Loss 5.9668e-01 (4.3667e-01)	Acc@1  81.00 ( 85.24)	Acc@5  99.00 ( 99.19)
Test: [ 30/100]	Time  0.037 ( 0.042)	Loss 3.9233e-01 (4.4557e-01)	Acc@1  83.00 ( 85.23)	Acc@5 100.00 ( 99.19)
Test: [ 40/100]	Time  0.036 ( 0.041)	Loss 3.3765e-01 (4.4937e-01)	Acc@1  92.00 ( 85.24)	Acc@5  99.00 ( 99.07)
Test: [ 50/100]	Time  0.038 ( 0.040)	Loss 3.1006e-01 (4.4661e-01)	Acc@1  89.00 ( 85.51)	Acc@5  99.00 ( 99.14)
Test: [ 60/100]	Time  0.036 ( 0.040)	Loss 4.1992e-01 (4.4529e-01)	Acc@1  84.00 ( 85.30)	Acc@5 100.00 ( 99.21)
Test: [ 70/100]	Time  0.036 ( 0.039)	Loss 5.9326e-01 (4.4684e-01)	Acc@1  77.00 ( 85.14)	Acc@5  99.00 ( 99.25)
Test: [ 80/100]	Time  0.037 ( 0.039)	Loss 3.8867e-01 (4.4495e-01)	Acc@1  85.00 ( 85.19)	Acc@5  99.00 ( 99.31)
Test: [ 90/100]	Time  0.039 ( 0.039)	Loss 3.2251e-01 (4.4156e-01)	Acc@1  89.00 ( 85.19)	Acc@5 100.00 ( 99.35)
 * Acc@1 85.250 Acc@5 99.380
### epoch[23] execution time: 46.68236589431763
EPOCH 24
# current learning rate: 0.1
# Switched to train mode...
Epoch: [24][  0/391]	Time  0.313 ( 0.313)	Data  0.217 ( 0.217)	Loss 3.0396e-01 (3.0396e-01)	Acc@1  87.50 ( 87.50)	Acc@5 100.00 (100.00)
Epoch: [24][ 10/391]	Time  0.108 ( 0.127)	Data  0.001 ( 0.023)	Loss 3.5571e-01 (3.5807e-01)	Acc@1  89.06 ( 87.14)	Acc@5  99.22 ( 99.79)
Epoch: [24][ 20/391]	Time  0.109 ( 0.117)	Data  0.001 ( 0.014)	Loss 4.1333e-01 (3.5627e-01)	Acc@1  84.38 ( 87.02)	Acc@5  99.22 ( 99.78)
Epoch: [24][ 30/391]	Time  0.104 ( 0.114)	Data  0.001 ( 0.011)	Loss 3.3667e-01 (3.4953e-01)	Acc@1  87.50 ( 87.40)	Acc@5 100.00 ( 99.70)
Epoch: [24][ 40/391]	Time  0.106 ( 0.113)	Data  0.001 ( 0.009)	Loss 3.9014e-01 (3.4940e-01)	Acc@1  85.16 ( 87.46)	Acc@5  97.66 ( 99.70)
Epoch: [24][ 50/391]	Time  0.105 ( 0.112)	Data  0.001 ( 0.008)	Loss 3.6938e-01 (3.4325e-01)	Acc@1  85.16 ( 87.62)	Acc@5 100.00 ( 99.71)
Epoch: [24][ 60/391]	Time  0.106 ( 0.111)	Data  0.001 ( 0.008)	Loss 3.2715e-01 (3.4817e-01)	Acc@1  89.06 ( 87.59)	Acc@5 100.00 ( 99.67)
Epoch: [24][ 70/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.007)	Loss 2.9565e-01 (3.4879e-01)	Acc@1  89.84 ( 87.63)	Acc@5 100.00 ( 99.66)
Epoch: [24][ 80/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.007)	Loss 3.3984e-01 (3.4880e-01)	Acc@1  89.06 ( 87.71)	Acc@5 100.00 ( 99.64)
Epoch: [24][ 90/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 3.6450e-01 (3.4939e-01)	Acc@1  87.50 ( 87.79)	Acc@5  99.22 ( 99.64)
Epoch: [24][100/391]	Time  0.099 ( 0.110)	Data  0.001 ( 0.006)	Loss 2.7661e-01 (3.5052e-01)	Acc@1  89.84 ( 87.68)	Acc@5  99.22 ( 99.63)
Epoch: [24][110/391]	Time  0.104 ( 0.110)	Data  0.001 ( 0.006)	Loss 3.4937e-01 (3.5430e-01)	Acc@1  86.72 ( 87.57)	Acc@5 100.00 ( 99.63)
Epoch: [24][120/391]	Time  0.099 ( 0.109)	Data  0.001 ( 0.006)	Loss 4.5361e-01 (3.5415e-01)	Acc@1  80.47 ( 87.55)	Acc@5 100.00 ( 99.62)
Epoch: [24][130/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.006)	Loss 3.6475e-01 (3.5379e-01)	Acc@1  88.28 ( 87.60)	Acc@5 100.00 ( 99.64)
Epoch: [24][140/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.006)	Loss 3.4204e-01 (3.5495e-01)	Acc@1  92.19 ( 87.59)	Acc@5 100.00 ( 99.65)
Epoch: [24][150/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.006)	Loss 2.8320e-01 (3.5547e-01)	Acc@1  88.28 ( 87.62)	Acc@5  99.22 ( 99.63)
Epoch: [24][160/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.0884e-01 (3.5448e-01)	Acc@1  89.84 ( 87.62)	Acc@5 100.00 ( 99.64)
Epoch: [24][170/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.9307e-01 (3.5484e-01)	Acc@1  87.50 ( 87.58)	Acc@5  99.22 ( 99.63)
Epoch: [24][180/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.3579e-01 (3.5665e-01)	Acc@1  83.59 ( 87.53)	Acc@5 100.00 ( 99.63)
Epoch: [24][190/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.7144e-01 (3.5888e-01)	Acc@1  86.72 ( 87.46)	Acc@5  98.44 ( 99.63)
Epoch: [24][200/391]	Time  0.111 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.4141e-01 (3.5936e-01)	Acc@1  82.81 ( 87.40)	Acc@5  99.22 ( 99.64)
Epoch: [24][210/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.0684e-01 (3.5971e-01)	Acc@1  82.03 ( 87.39)	Acc@5 100.00 ( 99.64)
Epoch: [24][220/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.1372e-01 (3.5970e-01)	Acc@1  87.50 ( 87.34)	Acc@5 100.00 ( 99.65)
Epoch: [24][230/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.8721e-01 (3.5968e-01)	Acc@1  84.38 ( 87.37)	Acc@5 100.00 ( 99.64)
Epoch: [24][240/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.0029e-01 (3.5823e-01)	Acc@1  89.06 ( 87.42)	Acc@5 100.00 ( 99.65)
Epoch: [24][250/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.4646e-01 (3.5728e-01)	Acc@1  89.06 ( 87.45)	Acc@5 100.00 ( 99.64)
Epoch: [24][260/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.8345e-01 (3.5950e-01)	Acc@1  86.72 ( 87.40)	Acc@5 100.00 ( 99.63)
Epoch: [24][270/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.1504e-01 (3.5904e-01)	Acc@1  85.16 ( 87.44)	Acc@5  98.44 ( 99.63)
Epoch: [24][280/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.5220e-01 (3.5848e-01)	Acc@1  90.62 ( 87.44)	Acc@5 100.00 ( 99.63)
Epoch: [24][290/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.2095e-01 (3.5802e-01)	Acc@1  92.97 ( 87.46)	Acc@5 100.00 ( 99.63)
Epoch: [24][300/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.3643e-01 (3.5895e-01)	Acc@1  89.84 ( 87.42)	Acc@5  98.44 ( 99.63)
Epoch: [24][310/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.1055e-01 (3.5842e-01)	Acc@1  90.62 ( 87.44)	Acc@5  98.44 ( 99.63)
Epoch: [24][320/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.5889e-01 (3.5906e-01)	Acc@1  88.28 ( 87.43)	Acc@5  99.22 ( 99.63)
Epoch: [24][330/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.4287e-01 (3.6028e-01)	Acc@1  88.28 ( 87.39)	Acc@5  98.44 ( 99.62)
Epoch: [24][340/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.7646e-01 (3.6007e-01)	Acc@1  85.94 ( 87.41)	Acc@5  99.22 ( 99.61)
Epoch: [24][350/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.2847e-01 (3.6084e-01)	Acc@1  86.72 ( 87.40)	Acc@5  99.22 ( 99.61)
Epoch: [24][360/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.0273e-01 (3.6092e-01)	Acc@1  88.28 ( 87.39)	Acc@5 100.00 ( 99.61)
Epoch: [24][370/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.2837e-01 (3.6084e-01)	Acc@1  89.06 ( 87.39)	Acc@5 100.00 ( 99.61)
Epoch: [24][380/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.5586e-01 (3.6045e-01)	Acc@1  91.41 ( 87.41)	Acc@5 100.00 ( 99.61)
Epoch: [24][390/391]	Time  0.099 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.2642e-01 (3.5982e-01)	Acc@1  88.75 ( 87.40)	Acc@5 100.00 ( 99.62)
## e[24] optimizer.zero_grad (sum) time: 0.35056042671203613
## e[24]       loss.backward (sum) time: 13.69841742515564
## e[24]      optimizer.step (sum) time: 2.7566537857055664
## epoch[24] training(only) time: 42.69012141227722
# Switched to evaluate mode...
Test: [  0/100]	Time  0.194 ( 0.194)	Loss 4.6606e-01 (4.6606e-01)	Acc@1  83.00 ( 83.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.036 ( 0.052)	Loss 4.4849e-01 (4.2705e-01)	Acc@1  86.00 ( 85.45)	Acc@5  99.00 ( 99.55)
Test: [ 20/100]	Time  0.037 ( 0.045)	Loss 5.1709e-01 (4.3319e-01)	Acc@1  81.00 ( 85.14)	Acc@5 100.00 ( 99.38)
Test: [ 30/100]	Time  0.035 ( 0.042)	Loss 3.2153e-01 (4.3659e-01)	Acc@1  88.00 ( 85.52)	Acc@5  99.00 ( 99.45)
Test: [ 40/100]	Time  0.036 ( 0.041)	Loss 4.0845e-01 (4.3702e-01)	Acc@1  88.00 ( 85.68)	Acc@5  99.00 ( 99.46)
Test: [ 50/100]	Time  0.038 ( 0.040)	Loss 3.0933e-01 (4.3376e-01)	Acc@1  89.00 ( 85.92)	Acc@5 100.00 ( 99.43)
Test: [ 60/100]	Time  0.041 ( 0.040)	Loss 3.0444e-01 (4.3313e-01)	Acc@1  89.00 ( 85.97)	Acc@5 100.00 ( 99.44)
Test: [ 70/100]	Time  0.038 ( 0.039)	Loss 5.7227e-01 (4.3426e-01)	Acc@1  80.00 ( 85.66)	Acc@5  99.00 ( 99.44)
Test: [ 80/100]	Time  0.036 ( 0.039)	Loss 4.4385e-01 (4.3662e-01)	Acc@1  82.00 ( 85.60)	Acc@5 100.00 ( 99.44)
Test: [ 90/100]	Time  0.036 ( 0.039)	Loss 2.7441e-01 (4.3398e-01)	Acc@1  89.00 ( 85.56)	Acc@5  99.00 ( 99.47)
 * Acc@1 85.450 Acc@5 99.430
### epoch[24] execution time: 46.62689757347107
EPOCH 25
# current learning rate: 0.1
# Switched to train mode...
Epoch: [25][  0/391]	Time  0.270 ( 0.270)	Data  0.176 ( 0.176)	Loss 2.7393e-01 (2.7393e-01)	Acc@1  89.06 ( 89.06)	Acc@5 100.00 (100.00)
Epoch: [25][ 10/391]	Time  0.107 ( 0.123)	Data  0.001 ( 0.020)	Loss 3.3521e-01 (3.2396e-01)	Acc@1  89.06 ( 88.99)	Acc@5 100.00 ( 99.79)
Epoch: [25][ 20/391]	Time  0.104 ( 0.116)	Data  0.001 ( 0.012)	Loss 4.2896e-01 (3.3794e-01)	Acc@1  86.72 ( 88.43)	Acc@5 100.00 ( 99.70)
Epoch: [25][ 30/391]	Time  0.108 ( 0.113)	Data  0.001 ( 0.010)	Loss 2.1094e-01 (3.4214e-01)	Acc@1  93.75 ( 88.21)	Acc@5  99.22 ( 99.70)
Epoch: [25][ 40/391]	Time  0.105 ( 0.112)	Data  0.001 ( 0.008)	Loss 3.6865e-01 (3.3574e-01)	Acc@1  85.94 ( 88.43)	Acc@5  99.22 ( 99.70)
Epoch: [25][ 50/391]	Time  0.105 ( 0.111)	Data  0.001 ( 0.007)	Loss 2.6929e-01 (3.3690e-01)	Acc@1  90.62 ( 88.47)	Acc@5 100.00 ( 99.65)
Epoch: [25][ 60/391]	Time  0.105 ( 0.111)	Data  0.001 ( 0.007)	Loss 2.0178e-01 (3.2847e-01)	Acc@1  92.19 ( 88.82)	Acc@5 100.00 ( 99.68)
Epoch: [25][ 70/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.007)	Loss 2.8442e-01 (3.3118e-01)	Acc@1  91.41 ( 88.91)	Acc@5 100.00 ( 99.66)
Epoch: [25][ 80/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.006)	Loss 3.6035e-01 (3.3513e-01)	Acc@1  85.94 ( 88.80)	Acc@5 100.00 ( 99.67)
Epoch: [25][ 90/391]	Time  0.114 ( 0.110)	Data  0.001 ( 0.006)	Loss 2.9443e-01 (3.3359e-01)	Acc@1  91.41 ( 88.91)	Acc@5 100.00 ( 99.68)
Epoch: [25][100/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 2.9590e-01 (3.3447e-01)	Acc@1  91.41 ( 88.91)	Acc@5 100.00 ( 99.68)
Epoch: [25][110/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.006)	Loss 4.0381e-01 (3.3714e-01)	Acc@1  83.59 ( 88.77)	Acc@5 100.00 ( 99.66)
Epoch: [25][120/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.006)	Loss 3.0420e-01 (3.3806e-01)	Acc@1  89.84 ( 88.67)	Acc@5 100.00 ( 99.67)
Epoch: [25][130/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.0933e-01 (3.3597e-01)	Acc@1  88.28 ( 88.66)	Acc@5 100.00 ( 99.67)
Epoch: [25][140/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.5938e-01 (3.3763e-01)	Acc@1  88.28 ( 88.62)	Acc@5 100.00 ( 99.67)
Epoch: [25][150/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.2861e-01 (3.3702e-01)	Acc@1  88.28 ( 88.65)	Acc@5  98.44 ( 99.66)
Epoch: [25][160/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.2056e-01 (3.3402e-01)	Acc@1  88.28 ( 88.72)	Acc@5 100.00 ( 99.67)
Epoch: [25][170/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.5391e-01 (3.3428e-01)	Acc@1  91.41 ( 88.69)	Acc@5 100.00 ( 99.64)
Epoch: [25][180/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.2139e-01 (3.3550e-01)	Acc@1  85.94 ( 88.60)	Acc@5  99.22 ( 99.62)
Epoch: [25][190/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.3862e-01 (3.3630e-01)	Acc@1  87.50 ( 88.53)	Acc@5 100.00 ( 99.63)
Epoch: [25][200/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.5156e-01 (3.3695e-01)	Acc@1  88.28 ( 88.55)	Acc@5  99.22 ( 99.63)
Epoch: [25][210/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.5522e-01 (3.3698e-01)	Acc@1  87.50 ( 88.54)	Acc@5  99.22 ( 99.63)
Epoch: [25][220/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.5474e-01 (3.3821e-01)	Acc@1  87.50 ( 88.45)	Acc@5  99.22 ( 99.63)
Epoch: [25][230/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.9258e-01 (3.4085e-01)	Acc@1  83.59 ( 88.34)	Acc@5 100.00 ( 99.63)
Epoch: [25][240/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.9663e-01 (3.3982e-01)	Acc@1  87.50 ( 88.37)	Acc@5 100.00 ( 99.64)
Epoch: [25][250/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.0332e-01 (3.4092e-01)	Acc@1  85.94 ( 88.30)	Acc@5  99.22 ( 99.64)
Epoch: [25][260/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.1323e-01 (3.4162e-01)	Acc@1  91.41 ( 88.26)	Acc@5  99.22 ( 99.63)
Epoch: [25][270/391]	Time  0.110 ( 0.109)	Data  0.002 ( 0.005)	Loss 2.9736e-01 (3.4136e-01)	Acc@1  90.62 ( 88.25)	Acc@5 100.00 ( 99.65)
Epoch: [25][280/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.2969e-01 (3.4071e-01)	Acc@1  85.16 ( 88.25)	Acc@5  99.22 ( 99.65)
Epoch: [25][290/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.3911e-01 (3.3992e-01)	Acc@1  89.84 ( 88.28)	Acc@5 100.00 ( 99.66)
Epoch: [25][300/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.0396e-01 (3.4061e-01)	Acc@1  91.41 ( 88.24)	Acc@5 100.00 ( 99.66)
Epoch: [25][310/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.0981e-01 (3.4185e-01)	Acc@1  88.28 ( 88.20)	Acc@5 100.00 ( 99.65)
Epoch: [25][320/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.9663e-01 (3.4248e-01)	Acc@1  89.06 ( 88.17)	Acc@5  99.22 ( 99.66)
Epoch: [25][330/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.8232e-01 (3.4304e-01)	Acc@1  82.03 ( 88.14)	Acc@5 100.00 ( 99.65)
Epoch: [25][340/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.0527e-01 (3.4386e-01)	Acc@1  86.72 ( 88.11)	Acc@5  99.22 ( 99.65)
Epoch: [25][350/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.2529e-01 (3.4395e-01)	Acc@1  85.16 ( 88.09)	Acc@5 100.00 ( 99.64)
Epoch: [25][360/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.3691e-01 (3.4428e-01)	Acc@1  89.84 ( 88.08)	Acc@5 100.00 ( 99.65)
Epoch: [25][370/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.7476e-01 (3.4405e-01)	Acc@1  89.06 ( 88.09)	Acc@5  98.44 ( 99.64)
Epoch: [25][380/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.9321e-01 (3.4228e-01)	Acc@1  90.62 ( 88.13)	Acc@5 100.00 ( 99.65)
Epoch: [25][390/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.6523e-01 (3.4250e-01)	Acc@1  90.00 ( 88.12)	Acc@5 100.00 ( 99.65)
## e[25] optimizer.zero_grad (sum) time: 0.3520481586456299
## e[25]       loss.backward (sum) time: 13.69447135925293
## e[25]      optimizer.step (sum) time: 2.751544952392578
## epoch[25] training(only) time: 42.67132234573364
# Switched to evaluate mode...
Test: [  0/100]	Time  0.193 ( 0.193)	Loss 4.5239e-01 (4.5239e-01)	Acc@1  82.00 ( 82.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.036 ( 0.052)	Loss 4.7046e-01 (4.9432e-01)	Acc@1  87.00 ( 84.09)	Acc@5  98.00 ( 99.09)
Test: [ 20/100]	Time  0.038 ( 0.045)	Loss 5.0586e-01 (4.9352e-01)	Acc@1  82.00 ( 84.29)	Acc@5  97.00 ( 98.95)
Test: [ 30/100]	Time  0.037 ( 0.042)	Loss 3.7280e-01 (4.8799e-01)	Acc@1  90.00 ( 84.48)	Acc@5  98.00 ( 99.06)
Test: [ 40/100]	Time  0.036 ( 0.041)	Loss 3.8818e-01 (4.8268e-01)	Acc@1  85.00 ( 84.54)	Acc@5  99.00 ( 98.95)
Test: [ 50/100]	Time  0.038 ( 0.040)	Loss 2.6685e-01 (4.7519e-01)	Acc@1  94.00 ( 84.92)	Acc@5  99.00 ( 99.04)
Test: [ 60/100]	Time  0.036 ( 0.040)	Loss 4.0454e-01 (4.8113e-01)	Acc@1  87.00 ( 84.54)	Acc@5 100.00 ( 99.11)
Test: [ 70/100]	Time  0.036 ( 0.039)	Loss 6.4844e-01 (4.8637e-01)	Acc@1  78.00 ( 84.34)	Acc@5 100.00 ( 99.11)
Test: [ 80/100]	Time  0.036 ( 0.039)	Loss 4.9243e-01 (4.8450e-01)	Acc@1  85.00 ( 84.40)	Acc@5 100.00 ( 99.11)
Test: [ 90/100]	Time  0.036 ( 0.039)	Loss 4.3579e-01 (4.8782e-01)	Acc@1  81.00 ( 84.21)	Acc@5 100.00 ( 99.11)
 * Acc@1 84.200 Acc@5 99.120
### epoch[25] execution time: 46.65191602706909
EPOCH 26
# current learning rate: 0.1
# Switched to train mode...
Epoch: [26][  0/391]	Time  0.311 ( 0.311)	Data  0.217 ( 0.217)	Loss 2.4292e-01 (2.4292e-01)	Acc@1  90.62 ( 90.62)	Acc@5 100.00 (100.00)
Epoch: [26][ 10/391]	Time  0.107 ( 0.126)	Data  0.001 ( 0.024)	Loss 4.5825e-01 (3.2729e-01)	Acc@1  84.38 ( 88.57)	Acc@5  99.22 ( 99.72)
Epoch: [26][ 20/391]	Time  0.102 ( 0.117)	Data  0.001 ( 0.014)	Loss 4.0283e-01 (3.2327e-01)	Acc@1  88.28 ( 88.73)	Acc@5  99.22 ( 99.70)
Epoch: [26][ 30/391]	Time  0.106 ( 0.114)	Data  0.001 ( 0.011)	Loss 1.7566e-01 (3.2515e-01)	Acc@1  94.53 ( 88.86)	Acc@5 100.00 ( 99.67)
Epoch: [26][ 40/391]	Time  0.103 ( 0.112)	Data  0.001 ( 0.009)	Loss 3.8770e-01 (3.1600e-01)	Acc@1  89.06 ( 89.16)	Acc@5 100.00 ( 99.71)
Epoch: [26][ 50/391]	Time  0.105 ( 0.111)	Data  0.001 ( 0.008)	Loss 2.7075e-01 (3.1756e-01)	Acc@1  88.28 ( 88.92)	Acc@5 100.00 ( 99.69)
Epoch: [26][ 60/391]	Time  0.109 ( 0.111)	Data  0.001 ( 0.008)	Loss 3.6304e-01 (3.2082e-01)	Acc@1  89.06 ( 88.72)	Acc@5  99.22 ( 99.68)
Epoch: [26][ 70/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.007)	Loss 3.6792e-01 (3.2579e-01)	Acc@1  86.72 ( 88.70)	Acc@5 100.00 ( 99.67)
Epoch: [26][ 80/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.007)	Loss 2.7197e-01 (3.2634e-01)	Acc@1  93.75 ( 88.67)	Acc@5 100.00 ( 99.65)
Epoch: [26][ 90/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.007)	Loss 3.5327e-01 (3.2638e-01)	Acc@1  88.28 ( 88.65)	Acc@5 100.00 ( 99.66)
Epoch: [26][100/391]	Time  0.104 ( 0.110)	Data  0.001 ( 0.006)	Loss 3.9111e-01 (3.2890e-01)	Acc@1  86.72 ( 88.56)	Acc@5  99.22 ( 99.65)
Epoch: [26][110/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.006)	Loss 2.3376e-01 (3.2880e-01)	Acc@1  93.75 ( 88.53)	Acc@5 100.00 ( 99.66)
Epoch: [26][120/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.006)	Loss 3.7427e-01 (3.3072e-01)	Acc@1  88.28 ( 88.46)	Acc@5  98.44 ( 99.64)
Epoch: [26][130/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.006)	Loss 2.5439e-01 (3.2870e-01)	Acc@1  92.19 ( 88.52)	Acc@5  99.22 ( 99.64)
Epoch: [26][140/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.006)	Loss 2.7368e-01 (3.2953e-01)	Acc@1  92.97 ( 88.50)	Acc@5 100.00 ( 99.65)
Epoch: [26][150/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.006)	Loss 2.9883e-01 (3.2761e-01)	Acc@1  88.28 ( 88.53)	Acc@5 100.00 ( 99.66)
Epoch: [26][160/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.006)	Loss 2.9468e-01 (3.2801e-01)	Acc@1  87.50 ( 88.52)	Acc@5 100.00 ( 99.67)
Epoch: [26][170/391]	Time  0.112 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.0898e-01 (3.2770e-01)	Acc@1  92.19 ( 88.58)	Acc@5 100.00 ( 99.67)
Epoch: [26][180/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.1372e-01 (3.2547e-01)	Acc@1  89.84 ( 88.64)	Acc@5 100.00 ( 99.67)
Epoch: [26][190/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.5146e-01 (3.2378e-01)	Acc@1  89.84 ( 88.67)	Acc@5 100.00 ( 99.68)
Epoch: [26][200/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.3267e-01 (3.2354e-01)	Acc@1  90.62 ( 88.69)	Acc@5 100.00 ( 99.68)
Epoch: [26][210/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.4058e-01 (3.2222e-01)	Acc@1  87.50 ( 88.73)	Acc@5  99.22 ( 99.69)
Epoch: [26][220/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.0503e-01 (3.2342e-01)	Acc@1  82.81 ( 88.66)	Acc@5 100.00 ( 99.69)
Epoch: [26][230/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.1333e-01 (3.2563e-01)	Acc@1  88.28 ( 88.61)	Acc@5  99.22 ( 99.69)
Epoch: [26][240/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.2471e-01 (3.2608e-01)	Acc@1  90.62 ( 88.61)	Acc@5 100.00 ( 99.68)
Epoch: [26][250/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.4141e-01 (3.2777e-01)	Acc@1  83.59 ( 88.58)	Acc@5  99.22 ( 99.67)
Epoch: [26][260/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.5952e-01 (3.2841e-01)	Acc@1  91.41 ( 88.58)	Acc@5 100.00 ( 99.66)
Epoch: [26][270/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.6963e-01 (3.2972e-01)	Acc@1  88.28 ( 88.54)	Acc@5  99.22 ( 99.66)
Epoch: [26][280/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.8906e-01 (3.3081e-01)	Acc@1  89.06 ( 88.51)	Acc@5 100.00 ( 99.66)
Epoch: [26][290/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.7500e-01 (3.3074e-01)	Acc@1  89.84 ( 88.51)	Acc@5  99.22 ( 99.65)
Epoch: [26][300/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.1382e-01 (3.3195e-01)	Acc@1  86.72 ( 88.47)	Acc@5  99.22 ( 99.64)
Epoch: [26][310/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.2278e-01 (3.3228e-01)	Acc@1  94.53 ( 88.47)	Acc@5 100.00 ( 99.64)
Epoch: [26][320/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.8154e-01 (3.3373e-01)	Acc@1  82.03 ( 88.42)	Acc@5  99.22 ( 99.64)
Epoch: [26][330/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.5728e-01 (3.3476e-01)	Acc@1  80.47 ( 88.38)	Acc@5 100.00 ( 99.64)
Epoch: [26][340/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.2334e-01 (3.3405e-01)	Acc@1  86.72 ( 88.41)	Acc@5 100.00 ( 99.64)
Epoch: [26][350/391]	Time  0.104 ( 0.109)	Data  0.002 ( 0.005)	Loss 2.9321e-01 (3.3435e-01)	Acc@1  90.62 ( 88.43)	Acc@5  98.44 ( 99.63)
Epoch: [26][360/391]	Time  0.105 ( 0.109)	Data  0.002 ( 0.005)	Loss 4.4727e-01 (3.3577e-01)	Acc@1  83.59 ( 88.36)	Acc@5  99.22 ( 99.63)
Epoch: [26][370/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.1372e-01 (3.3496e-01)	Acc@1  90.62 ( 88.38)	Acc@5 100.00 ( 99.63)
Epoch: [26][380/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.2886e-01 (3.3569e-01)	Acc@1  89.84 ( 88.35)	Acc@5 100.00 ( 99.64)
Epoch: [26][390/391]	Time  0.098 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.7109e-01 (3.3714e-01)	Acc@1  87.50 ( 88.30)	Acc@5 100.00 ( 99.64)
## e[26] optimizer.zero_grad (sum) time: 0.35190248489379883
## e[26]       loss.backward (sum) time: 13.68890380859375
## e[26]      optimizer.step (sum) time: 2.7801055908203125
## epoch[26] training(only) time: 42.567172050476074
# Switched to evaluate mode...
Test: [  0/100]	Time  0.196 ( 0.196)	Loss 4.1626e-01 (4.1626e-01)	Acc@1  84.00 ( 84.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.037 ( 0.052)	Loss 5.0098e-01 (5.1656e-01)	Acc@1  84.00 ( 82.64)	Acc@5  99.00 ( 99.45)
Test: [ 20/100]	Time  0.037 ( 0.045)	Loss 5.6641e-01 (5.4289e-01)	Acc@1  81.00 ( 82.48)	Acc@5 100.00 ( 99.00)
Test: [ 30/100]	Time  0.037 ( 0.043)	Loss 7.4707e-01 (5.4077e-01)	Acc@1  76.00 ( 82.71)	Acc@5  99.00 ( 99.10)
Test: [ 40/100]	Time  0.036 ( 0.041)	Loss 5.7178e-01 (5.4733e-01)	Acc@1  80.00 ( 82.41)	Acc@5  98.00 ( 99.12)
Test: [ 50/100]	Time  0.037 ( 0.040)	Loss 5.2637e-01 (5.4048e-01)	Acc@1  84.00 ( 82.65)	Acc@5  98.00 ( 99.12)
Test: [ 60/100]	Time  0.045 ( 0.040)	Loss 5.5566e-01 (5.4286e-01)	Acc@1  82.00 ( 82.51)	Acc@5 100.00 ( 99.13)
Test: [ 70/100]	Time  0.036 ( 0.040)	Loss 8.1543e-01 (5.4772e-01)	Acc@1  75.00 ( 82.41)	Acc@5 100.00 ( 99.17)
Test: [ 80/100]	Time  0.039 ( 0.039)	Loss 4.9146e-01 (5.4572e-01)	Acc@1  87.00 ( 82.43)	Acc@5  98.00 ( 99.21)
Test: [ 90/100]	Time  0.036 ( 0.039)	Loss 3.5107e-01 (5.4259e-01)	Acc@1  89.00 ( 82.53)	Acc@5 100.00 ( 99.21)
 * Acc@1 82.520 Acc@5 99.220
### epoch[26] execution time: 46.56309151649475
EPOCH 27
# current learning rate: 0.1
# Switched to train mode...
Epoch: [27][  0/391]	Time  0.260 ( 0.260)	Data  0.166 ( 0.166)	Loss 3.5815e-01 (3.5815e-01)	Acc@1  89.84 ( 89.84)	Acc@5  99.22 ( 99.22)
Epoch: [27][ 10/391]	Time  0.105 ( 0.121)	Data  0.001 ( 0.019)	Loss 3.1226e-01 (3.5340e-01)	Acc@1  89.84 ( 88.28)	Acc@5  99.22 ( 99.57)
Epoch: [27][ 20/391]	Time  0.104 ( 0.115)	Data  0.001 ( 0.012)	Loss 3.9600e-01 (3.2940e-01)	Acc@1  85.94 ( 89.17)	Acc@5  99.22 ( 99.74)
Epoch: [27][ 30/391]	Time  0.107 ( 0.113)	Data  0.001 ( 0.009)	Loss 3.9282e-01 (3.3191e-01)	Acc@1  87.50 ( 88.96)	Acc@5 100.00 ( 99.72)
Epoch: [27][ 40/391]	Time  0.108 ( 0.111)	Data  0.001 ( 0.008)	Loss 4.3066e-01 (3.3463e-01)	Acc@1  83.59 ( 88.91)	Acc@5  99.22 ( 99.71)
Epoch: [27][ 50/391]	Time  0.106 ( 0.111)	Data  0.001 ( 0.007)	Loss 3.1860e-01 (3.3146e-01)	Acc@1  88.28 ( 88.86)	Acc@5 100.00 ( 99.75)
Epoch: [27][ 60/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.007)	Loss 4.3213e-01 (3.2432e-01)	Acc@1  87.50 ( 89.10)	Acc@5 100.00 ( 99.73)
Epoch: [27][ 70/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.006)	Loss 2.3645e-01 (3.2175e-01)	Acc@1  90.62 ( 89.13)	Acc@5 100.00 ( 99.70)
Epoch: [27][ 80/391]	Time  0.103 ( 0.110)	Data  0.001 ( 0.006)	Loss 3.4668e-01 (3.2114e-01)	Acc@1  86.72 ( 88.99)	Acc@5 100.00 ( 99.72)
Epoch: [27][ 90/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.006)	Loss 4.1406e-01 (3.1961e-01)	Acc@1  85.94 ( 89.07)	Acc@5 100.00 ( 99.73)
Epoch: [27][100/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.006)	Loss 2.2925e-01 (3.2012e-01)	Acc@1  91.41 ( 89.05)	Acc@5 100.00 ( 99.72)
Epoch: [27][110/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.006)	Loss 3.7085e-01 (3.2073e-01)	Acc@1  89.84 ( 89.03)	Acc@5  99.22 ( 99.73)
Epoch: [27][120/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.0127e-01 (3.1958e-01)	Acc@1  89.84 ( 89.04)	Acc@5 100.00 ( 99.73)
Epoch: [27][130/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.1128e-01 (3.2446e-01)	Acc@1  85.94 ( 88.86)	Acc@5 100.00 ( 99.71)
Epoch: [27][140/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.2847e-01 (3.2816e-01)	Acc@1  86.72 ( 88.77)	Acc@5 100.00 ( 99.72)
Epoch: [27][150/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.4033e-01 (3.2975e-01)	Acc@1  88.28 ( 88.76)	Acc@5  99.22 ( 99.71)
Epoch: [27][160/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.2983e-01 (3.3066e-01)	Acc@1  90.62 ( 88.74)	Acc@5 100.00 ( 99.71)
Epoch: [27][170/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.1252e-01 (3.2883e-01)	Acc@1  90.62 ( 88.77)	Acc@5 100.00 ( 99.72)
Epoch: [27][180/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.9175e-01 (3.2992e-01)	Acc@1  86.72 ( 88.72)	Acc@5 100.00 ( 99.72)
Epoch: [27][190/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.8652e-01 (3.2804e-01)	Acc@1  92.97 ( 88.71)	Acc@5 100.00 ( 99.71)
Epoch: [27][200/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.4277e-01 (3.2699e-01)	Acc@1  86.72 ( 88.76)	Acc@5 100.00 ( 99.72)
Epoch: [27][210/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.7515e-01 (3.2732e-01)	Acc@1  88.28 ( 88.71)	Acc@5 100.00 ( 99.71)
Epoch: [27][220/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.0112e-01 (3.2708e-01)	Acc@1  86.72 ( 88.75)	Acc@5  99.22 ( 99.71)
Epoch: [27][230/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.2480e-01 (3.2966e-01)	Acc@1  85.16 ( 88.63)	Acc@5  98.44 ( 99.70)
Epoch: [27][240/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.7173e-01 (3.3055e-01)	Acc@1  92.19 ( 88.65)	Acc@5 100.00 ( 99.71)
Epoch: [27][250/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.6294e-01 (3.2991e-01)	Acc@1  89.84 ( 88.63)	Acc@5 100.00 ( 99.71)
Epoch: [27][260/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.3499e-01 (3.3077e-01)	Acc@1  92.97 ( 88.60)	Acc@5  98.44 ( 99.70)
Epoch: [27][270/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.9370e-01 (3.3089e-01)	Acc@1  90.62 ( 88.62)	Acc@5 100.00 ( 99.70)
Epoch: [27][280/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.3447e-01 (3.3126e-01)	Acc@1  87.50 ( 88.60)	Acc@5 100.00 ( 99.69)
Epoch: [27][290/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.4976e-01 (3.3059e-01)	Acc@1  91.41 ( 88.62)	Acc@5 100.00 ( 99.69)
Epoch: [27][300/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.4985e-01 (3.3118e-01)	Acc@1  88.28 ( 88.57)	Acc@5 100.00 ( 99.69)
Epoch: [27][310/391]	Time  0.098 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.5718e-01 (3.3075e-01)	Acc@1  87.50 ( 88.60)	Acc@5  98.44 ( 99.69)
Epoch: [27][320/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.5156e-01 (3.2953e-01)	Acc@1  90.62 ( 88.66)	Acc@5  98.44 ( 99.69)
Epoch: [27][330/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.6514e-01 (3.2731e-01)	Acc@1  91.41 ( 88.72)	Acc@5 100.00 ( 99.69)
Epoch: [27][340/391]	Time  0.106 ( 0.108)	Data  0.001 ( 0.005)	Loss 2.8394e-01 (3.2667e-01)	Acc@1  88.28 ( 88.72)	Acc@5 100.00 ( 99.69)
Epoch: [27][350/391]	Time  0.104 ( 0.108)	Data  0.001 ( 0.005)	Loss 4.4604e-01 (3.2835e-01)	Acc@1  85.94 ( 88.63)	Acc@5 100.00 ( 99.69)
Epoch: [27][360/391]	Time  0.107 ( 0.108)	Data  0.001 ( 0.005)	Loss 2.5952e-01 (3.2844e-01)	Acc@1  89.84 ( 88.62)	Acc@5 100.00 ( 99.69)
Epoch: [27][370/391]	Time  0.108 ( 0.108)	Data  0.001 ( 0.005)	Loss 3.1372e-01 (3.2907e-01)	Acc@1  88.28 ( 88.61)	Acc@5 100.00 ( 99.69)
Epoch: [27][380/391]	Time  0.107 ( 0.108)	Data  0.001 ( 0.005)	Loss 4.2236e-01 (3.2985e-01)	Acc@1  85.94 ( 88.60)	Acc@5  99.22 ( 99.68)
Epoch: [27][390/391]	Time  0.100 ( 0.108)	Data  0.001 ( 0.005)	Loss 3.7549e-01 (3.2922e-01)	Acc@1  87.50 ( 88.64)	Acc@5 100.00 ( 99.68)
## e[27] optimizer.zero_grad (sum) time: 0.3476569652557373
## e[27]       loss.backward (sum) time: 13.635772943496704
## e[27]      optimizer.step (sum) time: 2.760448932647705
## epoch[27] training(only) time: 42.54357361793518
# Switched to evaluate mode...
Test: [  0/100]	Time  0.194 ( 0.194)	Loss 3.5547e-01 (3.5547e-01)	Acc@1  87.00 ( 87.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.036 ( 0.051)	Loss 4.3262e-01 (4.0716e-01)	Acc@1  84.00 ( 85.55)	Acc@5 100.00 ( 99.55)
Test: [ 20/100]	Time  0.039 ( 0.045)	Loss 3.6279e-01 (4.1464e-01)	Acc@1  84.00 ( 85.33)	Acc@5 100.00 ( 99.52)
Test: [ 30/100]	Time  0.038 ( 0.042)	Loss 4.6729e-01 (4.2660e-01)	Acc@1  83.00 ( 85.10)	Acc@5  99.00 ( 99.42)
Test: [ 40/100]	Time  0.036 ( 0.041)	Loss 3.4668e-01 (4.2932e-01)	Acc@1  87.00 ( 85.20)	Acc@5  99.00 ( 99.41)
Test: [ 50/100]	Time  0.037 ( 0.040)	Loss 2.8296e-01 (4.2723e-01)	Acc@1  90.00 ( 85.45)	Acc@5 100.00 ( 99.45)
Test: [ 60/100]	Time  0.037 ( 0.040)	Loss 2.7856e-01 (4.2810e-01)	Acc@1  91.00 ( 85.43)	Acc@5 100.00 ( 99.44)
Test: [ 70/100]	Time  0.037 ( 0.039)	Loss 6.1865e-01 (4.3186e-01)	Acc@1  79.00 ( 85.34)	Acc@5 100.00 ( 99.45)
Test: [ 80/100]	Time  0.036 ( 0.039)	Loss 4.6851e-01 (4.3608e-01)	Acc@1  85.00 ( 85.27)	Acc@5  99.00 ( 99.44)
Test: [ 90/100]	Time  0.036 ( 0.039)	Loss 2.8247e-01 (4.3266e-01)	Acc@1  90.00 ( 85.34)	Acc@5 100.00 ( 99.46)
 * Acc@1 85.490 Acc@5 99.480
### epoch[27] execution time: 46.521857261657715
EPOCH 28
# current learning rate: 0.1
# Switched to train mode...
Epoch: [28][  0/391]	Time  0.275 ( 0.275)	Data  0.174 ( 0.174)	Loss 3.3008e-01 (3.3008e-01)	Acc@1  90.62 ( 90.62)	Acc@5  99.22 ( 99.22)
Epoch: [28][ 10/391]	Time  0.107 ( 0.123)	Data  0.001 ( 0.019)	Loss 2.4670e-01 (2.8820e-01)	Acc@1  92.19 ( 90.06)	Acc@5 100.00 ( 99.79)
Epoch: [28][ 20/391]	Time  0.106 ( 0.116)	Data  0.001 ( 0.012)	Loss 2.9346e-01 (2.8946e-01)	Acc@1  92.19 ( 89.84)	Acc@5  99.22 ( 99.85)
Epoch: [28][ 30/391]	Time  0.105 ( 0.113)	Data  0.001 ( 0.009)	Loss 2.4768e-01 (2.9352e-01)	Acc@1  91.41 ( 89.84)	Acc@5  99.22 ( 99.80)
Epoch: [28][ 40/391]	Time  0.108 ( 0.112)	Data  0.001 ( 0.008)	Loss 5.0391e-01 (3.0143e-01)	Acc@1  85.16 ( 89.69)	Acc@5  99.22 ( 99.79)
Epoch: [28][ 50/391]	Time  0.106 ( 0.112)	Data  0.001 ( 0.007)	Loss 3.8550e-01 (3.1015e-01)	Acc@1  87.50 ( 89.40)	Acc@5  99.22 ( 99.79)
Epoch: [28][ 60/391]	Time  0.109 ( 0.111)	Data  0.001 ( 0.007)	Loss 3.4424e-01 (3.1439e-01)	Acc@1  85.94 ( 89.37)	Acc@5 100.00 ( 99.76)
Epoch: [28][ 70/391]	Time  0.108 ( 0.111)	Data  0.001 ( 0.006)	Loss 2.7417e-01 (3.1518e-01)	Acc@1  89.84 ( 89.21)	Acc@5  99.22 ( 99.74)
Epoch: [28][ 80/391]	Time  0.102 ( 0.110)	Data  0.001 ( 0.006)	Loss 3.1104e-01 (3.1687e-01)	Acc@1  89.84 ( 89.13)	Acc@5  99.22 ( 99.73)
Epoch: [28][ 90/391]	Time  0.109 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.7981e-01 (3.1760e-01)	Acc@1  92.19 ( 88.98)	Acc@5 100.00 ( 99.73)
Epoch: [28][100/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.006)	Loss 2.3486e-01 (3.1838e-01)	Acc@1  92.19 ( 88.84)	Acc@5 100.00 ( 99.71)
Epoch: [28][110/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 3.7036e-01 (3.2038e-01)	Acc@1  85.94 ( 88.73)	Acc@5 100.00 ( 99.70)
Epoch: [28][120/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.006)	Loss 2.7734e-01 (3.2047e-01)	Acc@1  90.62 ( 88.75)	Acc@5  99.22 ( 99.70)
Epoch: [28][130/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.005)	Loss 3.4644e-01 (3.1910e-01)	Acc@1  86.72 ( 88.79)	Acc@5 100.00 ( 99.70)
Epoch: [28][140/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.1204e-01 (3.1785e-01)	Acc@1  92.97 ( 88.89)	Acc@5 100.00 ( 99.70)
Epoch: [28][150/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.5586e-01 (3.1559e-01)	Acc@1  92.97 ( 88.96)	Acc@5 100.00 ( 99.69)
Epoch: [28][160/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.7832e-01 (3.1509e-01)	Acc@1  90.62 ( 88.97)	Acc@5 100.00 ( 99.70)
Epoch: [28][170/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.3911e-01 (3.1477e-01)	Acc@1  85.94 ( 88.93)	Acc@5  99.22 ( 99.70)
Epoch: [28][180/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.0176e-01 (3.1486e-01)	Acc@1  90.62 ( 88.94)	Acc@5 100.00 ( 99.71)
Epoch: [28][190/391]	Time  0.111 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.1421e-01 (3.1801e-01)	Acc@1  89.06 ( 88.87)	Acc@5 100.00 ( 99.70)
Epoch: [28][200/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.1592e-01 (3.1952e-01)	Acc@1  89.06 ( 88.78)	Acc@5 100.00 ( 99.70)
Epoch: [28][210/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.9468e-01 (3.2020e-01)	Acc@1  88.28 ( 88.78)	Acc@5 100.00 ( 99.70)
Epoch: [28][220/391]	Time  0.107 ( 0.109)	Data  0.002 ( 0.005)	Loss 3.2739e-01 (3.1936e-01)	Acc@1  89.06 ( 88.77)	Acc@5 100.00 ( 99.71)
Epoch: [28][230/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.3975e-01 (3.1914e-01)	Acc@1  92.97 ( 88.81)	Acc@5 100.00 ( 99.72)
Epoch: [28][240/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.7505e-01 (3.1720e-01)	Acc@1  92.97 ( 88.90)	Acc@5 100.00 ( 99.73)
Epoch: [28][250/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.0835e-01 (3.1897e-01)	Acc@1  88.28 ( 88.86)	Acc@5  99.22 ( 99.72)
Epoch: [28][260/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.7256e-01 (3.1934e-01)	Acc@1  87.50 ( 88.84)	Acc@5  99.22 ( 99.72)
Epoch: [28][270/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.3091e-01 (3.2049e-01)	Acc@1  86.72 ( 88.80)	Acc@5  98.44 ( 99.71)
Epoch: [28][280/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.7930e-01 (3.2121e-01)	Acc@1  89.84 ( 88.78)	Acc@5 100.00 ( 99.71)
Epoch: [28][290/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.4937e-01 (3.2162e-01)	Acc@1  88.28 ( 88.78)	Acc@5 100.00 ( 99.72)
Epoch: [28][300/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.9136e-01 (3.2219e-01)	Acc@1  85.16 ( 88.77)	Acc@5  98.44 ( 99.71)
Epoch: [28][310/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.2876e-01 (3.2158e-01)	Acc@1  92.19 ( 88.78)	Acc@5 100.00 ( 99.72)
Epoch: [28][320/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.3938e-01 (3.2175e-01)	Acc@1  91.41 ( 88.77)	Acc@5 100.00 ( 99.71)
Epoch: [28][330/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.7246e-01 (3.2119e-01)	Acc@1  91.41 ( 88.80)	Acc@5 100.00 ( 99.71)
Epoch: [28][340/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.6914e-01 (3.2150e-01)	Acc@1  88.28 ( 88.80)	Acc@5  99.22 ( 99.71)
Epoch: [28][350/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.4109e-01 (3.2177e-01)	Acc@1  91.41 ( 88.78)	Acc@5 100.00 ( 99.71)
Epoch: [28][360/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.9038e-01 (3.2189e-01)	Acc@1  86.72 ( 88.78)	Acc@5  99.22 ( 99.71)
Epoch: [28][370/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.1904e-01 (3.2149e-01)	Acc@1  82.81 ( 88.78)	Acc@5 100.00 ( 99.71)
Epoch: [28][380/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.0674e-01 (3.2215e-01)	Acc@1  86.72 ( 88.76)	Acc@5  99.22 ( 99.70)
Epoch: [28][390/391]	Time  0.099 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.8315e-01 (3.2268e-01)	Acc@1  85.00 ( 88.75)	Acc@5 100.00 ( 99.70)
## e[28] optimizer.zero_grad (sum) time: 0.3509392738342285
## e[28]       loss.backward (sum) time: 13.72356915473938
## e[28]      optimizer.step (sum) time: 2.761328935623169
## epoch[28] training(only) time: 42.61976718902588
# Switched to evaluate mode...
Test: [  0/100]	Time  0.241 ( 0.241)	Loss 3.4375e-01 (3.4375e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.036 ( 0.055)	Loss 3.8501e-01 (3.7438e-01)	Acc@1  91.00 ( 88.27)	Acc@5  99.00 ( 99.55)
Test: [ 20/100]	Time  0.037 ( 0.046)	Loss 5.1611e-01 (3.9582e-01)	Acc@1  81.00 ( 87.43)	Acc@5 100.00 ( 99.38)
Test: [ 30/100]	Time  0.036 ( 0.044)	Loss 3.2031e-01 (4.0241e-01)	Acc@1  89.00 ( 87.19)	Acc@5 100.00 ( 99.35)
Test: [ 40/100]	Time  0.044 ( 0.042)	Loss 3.8110e-01 (3.9459e-01)	Acc@1  84.00 ( 87.32)	Acc@5  99.00 ( 99.34)
Test: [ 50/100]	Time  0.039 ( 0.041)	Loss 2.5098e-01 (3.9021e-01)	Acc@1  91.00 ( 87.39)	Acc@5 100.00 ( 99.37)
Test: [ 60/100]	Time  0.039 ( 0.041)	Loss 4.3774e-01 (3.8989e-01)	Acc@1  85.00 ( 87.25)	Acc@5  99.00 ( 99.36)
Test: [ 70/100]	Time  0.037 ( 0.040)	Loss 4.3042e-01 (3.9241e-01)	Acc@1  88.00 ( 87.17)	Acc@5 100.00 ( 99.41)
Test: [ 80/100]	Time  0.036 ( 0.040)	Loss 3.5938e-01 (3.9252e-01)	Acc@1  87.00 ( 87.09)	Acc@5 100.00 ( 99.42)
Test: [ 90/100]	Time  0.037 ( 0.039)	Loss 3.3032e-01 (3.9429e-01)	Acc@1  90.00 ( 86.90)	Acc@5 100.00 ( 99.43)
 * Acc@1 86.890 Acc@5 99.450
### epoch[28] execution time: 46.62487268447876
EPOCH 29
# current learning rate: 0.1
# Switched to train mode...
Epoch: [29][  0/391]	Time  0.272 ( 0.272)	Data  0.171 ( 0.171)	Loss 2.7905e-01 (2.7905e-01)	Acc@1  89.06 ( 89.06)	Acc@5 100.00 (100.00)
Epoch: [29][ 10/391]	Time  0.108 ( 0.123)	Data  0.001 ( 0.019)	Loss 2.9517e-01 (2.8952e-01)	Acc@1  88.28 ( 89.49)	Acc@5 100.00 ( 99.64)
Epoch: [29][ 20/391]	Time  0.106 ( 0.116)	Data  0.001 ( 0.012)	Loss 3.0713e-01 (2.8520e-01)	Acc@1  87.50 ( 89.25)	Acc@5 100.00 ( 99.74)
Epoch: [29][ 30/391]	Time  0.108 ( 0.114)	Data  0.001 ( 0.009)	Loss 3.2617e-01 (2.8275e-01)	Acc@1  87.50 ( 89.62)	Acc@5  99.22 ( 99.72)
Epoch: [29][ 40/391]	Time  0.105 ( 0.113)	Data  0.001 ( 0.008)	Loss 2.0325e-01 (2.7903e-01)	Acc@1  91.41 ( 89.71)	Acc@5 100.00 ( 99.73)
Epoch: [29][ 50/391]	Time  0.103 ( 0.112)	Data  0.001 ( 0.007)	Loss 2.5952e-01 (2.7682e-01)	Acc@1  88.28 ( 89.86)	Acc@5 100.00 ( 99.72)
Epoch: [29][ 60/391]	Time  0.106 ( 0.111)	Data  0.001 ( 0.007)	Loss 5.0830e-01 (2.7672e-01)	Acc@1  79.69 ( 89.83)	Acc@5 100.00 ( 99.72)
Epoch: [29][ 70/391]	Time  0.108 ( 0.111)	Data  0.001 ( 0.006)	Loss 3.5522e-01 (2.8427e-01)	Acc@1  86.72 ( 89.63)	Acc@5 100.00 ( 99.75)
Epoch: [29][ 80/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 3.0664e-01 (2.8781e-01)	Acc@1  89.06 ( 89.50)	Acc@5  99.22 ( 99.75)
Epoch: [29][ 90/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 3.8452e-01 (2.8828e-01)	Acc@1  85.94 ( 89.56)	Acc@5 100.00 ( 99.73)
Epoch: [29][100/391]	Time  0.104 ( 0.110)	Data  0.001 ( 0.006)	Loss 4.6362e-01 (2.9035e-01)	Acc@1  87.50 ( 89.57)	Acc@5 100.00 ( 99.73)
Epoch: [29][110/391]	Time  0.103 ( 0.110)	Data  0.001 ( 0.006)	Loss 2.3059e-01 (2.9034e-01)	Acc@1  92.19 ( 89.61)	Acc@5 100.00 ( 99.73)
Epoch: [29][120/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 4.2407e-01 (2.9211e-01)	Acc@1  84.38 ( 89.56)	Acc@5 100.00 ( 99.72)
Epoch: [29][130/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.005)	Loss 2.6147e-01 (2.9282e-01)	Acc@1  89.84 ( 89.50)	Acc@5 100.00 ( 99.73)
Epoch: [29][140/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.6538e-01 (2.9291e-01)	Acc@1  92.19 ( 89.52)	Acc@5 100.00 ( 99.73)
Epoch: [29][150/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.4302e-01 (2.9579e-01)	Acc@1  88.28 ( 89.48)	Acc@5  99.22 ( 99.73)
Epoch: [29][160/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.7588e-01 (2.9457e-01)	Acc@1  89.84 ( 89.50)	Acc@5 100.00 ( 99.74)
Epoch: [29][170/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.3047e-01 (2.9639e-01)	Acc@1  94.53 ( 89.48)	Acc@5 100.00 ( 99.74)
Epoch: [29][180/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.0737e-01 (2.9589e-01)	Acc@1  88.28 ( 89.47)	Acc@5 100.00 ( 99.75)
Epoch: [29][190/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.4829e-01 (2.9635e-01)	Acc@1  91.41 ( 89.41)	Acc@5 100.00 ( 99.76)
Epoch: [29][200/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.5220e-01 (2.9485e-01)	Acc@1  92.19 ( 89.47)	Acc@5 100.00 ( 99.76)
Epoch: [29][210/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.2422e-01 (2.9501e-01)	Acc@1  86.72 ( 89.48)	Acc@5 100.00 ( 99.77)
Epoch: [29][220/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.3179e-01 (2.9648e-01)	Acc@1  87.50 ( 89.46)	Acc@5  99.22 ( 99.77)
Epoch: [29][230/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.6343e-01 (2.9773e-01)	Acc@1  93.75 ( 89.45)	Acc@5 100.00 ( 99.76)
Epoch: [29][240/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.3450e-01 (2.9697e-01)	Acc@1  94.53 ( 89.53)	Acc@5  99.22 ( 99.76)
Epoch: [29][250/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.5229e-01 (2.9746e-01)	Acc@1  90.62 ( 89.50)	Acc@5  99.22 ( 99.76)
Epoch: [29][260/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.7490e-01 (2.9829e-01)	Acc@1  91.41 ( 89.48)	Acc@5 100.00 ( 99.76)
Epoch: [29][270/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.4146e-01 (2.9832e-01)	Acc@1  91.41 ( 89.51)	Acc@5 100.00 ( 99.76)
Epoch: [29][280/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.7134e-01 (2.9882e-01)	Acc@1  86.72 ( 89.51)	Acc@5  99.22 ( 99.76)
Epoch: [29][290/391]	Time  0.100 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.4521e-01 (2.9902e-01)	Acc@1  88.28 ( 89.50)	Acc@5 100.00 ( 99.76)
Epoch: [29][300/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.2644e-01 (2.9955e-01)	Acc@1  92.19 ( 89.49)	Acc@5 100.00 ( 99.75)
Epoch: [29][310/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.2568e-01 (3.0050e-01)	Acc@1  88.28 ( 89.43)	Acc@5 100.00 ( 99.75)
Epoch: [29][320/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.0933e-01 (3.0126e-01)	Acc@1  89.84 ( 89.38)	Acc@5 100.00 ( 99.75)
Epoch: [29][330/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.0737e-01 (3.0273e-01)	Acc@1  91.41 ( 89.34)	Acc@5 100.00 ( 99.75)
Epoch: [29][340/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.5620e-01 (3.0355e-01)	Acc@1  87.50 ( 89.34)	Acc@5  99.22 ( 99.74)
Epoch: [29][350/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.6475e-01 (3.0510e-01)	Acc@1  88.28 ( 89.27)	Acc@5 100.00 ( 99.74)
Epoch: [29][360/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.0176e-01 (3.0523e-01)	Acc@1  87.50 ( 89.25)	Acc@5 100.00 ( 99.74)
Epoch: [29][370/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.5327e-01 (3.0515e-01)	Acc@1  89.06 ( 89.25)	Acc@5  98.44 ( 99.74)
Epoch: [29][380/391]	Time  0.106 ( 0.109)	Data  0.002 ( 0.005)	Loss 3.7695e-01 (3.0441e-01)	Acc@1  85.94 ( 89.27)	Acc@5  99.22 ( 99.74)
Epoch: [29][390/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.2788e-01 (3.0489e-01)	Acc@1  83.75 ( 89.25)	Acc@5 100.00 ( 99.74)
## e[29] optimizer.zero_grad (sum) time: 0.347517728805542
## e[29]       loss.backward (sum) time: 13.721052646636963
## e[29]      optimizer.step (sum) time: 2.7893617153167725
## epoch[29] training(only) time: 42.72656750679016
# Switched to evaluate mode...
Test: [  0/100]	Time  0.244 ( 0.244)	Loss 3.7280e-01 (3.7280e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.037 ( 0.057)	Loss 5.4541e-01 (4.0656e-01)	Acc@1  82.00 ( 87.09)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.038 ( 0.048)	Loss 4.2725e-01 (4.1403e-01)	Acc@1  88.00 ( 86.95)	Acc@5 100.00 ( 99.52)
Test: [ 30/100]	Time  0.037 ( 0.044)	Loss 4.0234e-01 (4.2758e-01)	Acc@1  87.00 ( 86.90)	Acc@5 100.00 ( 99.52)
Test: [ 40/100]	Time  0.038 ( 0.043)	Loss 4.3774e-01 (4.3311e-01)	Acc@1  84.00 ( 86.37)	Acc@5  99.00 ( 99.44)
Test: [ 50/100]	Time  0.036 ( 0.042)	Loss 3.0518e-01 (4.3182e-01)	Acc@1  86.00 ( 86.18)	Acc@5  99.00 ( 99.45)
Test: [ 60/100]	Time  0.038 ( 0.041)	Loss 4.0063e-01 (4.3114e-01)	Acc@1  93.00 ( 86.05)	Acc@5 100.00 ( 99.51)
Test: [ 70/100]	Time  0.038 ( 0.040)	Loss 4.7290e-01 (4.3005e-01)	Acc@1  82.00 ( 85.94)	Acc@5 100.00 ( 99.52)
Test: [ 80/100]	Time  0.037 ( 0.040)	Loss 4.1089e-01 (4.3246e-01)	Acc@1  85.00 ( 85.93)	Acc@5 100.00 ( 99.53)
Test: [ 90/100]	Time  0.040 ( 0.040)	Loss 3.9551e-01 (4.3159e-01)	Acc@1  87.00 ( 85.98)	Acc@5 100.00 ( 99.56)
 * Acc@1 86.010 Acc@5 99.590
### epoch[29] execution time: 46.76262378692627
EPOCH 30
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [30][  0/391]	Time  0.315 ( 0.315)	Data  0.213 ( 0.213)	Loss 3.1128e-01 (3.1128e-01)	Acc@1  89.84 ( 89.84)	Acc@5 100.00 (100.00)
Epoch: [30][ 10/391]	Time  0.108 ( 0.127)	Data  0.001 ( 0.023)	Loss 3.3008e-01 (2.6855e-01)	Acc@1  88.28 ( 90.27)	Acc@5  99.22 ( 99.79)
Epoch: [30][ 20/391]	Time  0.106 ( 0.118)	Data  0.001 ( 0.014)	Loss 2.8345e-01 (2.6723e-01)	Acc@1  86.72 ( 90.29)	Acc@5 100.00 ( 99.78)
Epoch: [30][ 30/391]	Time  0.106 ( 0.115)	Data  0.001 ( 0.011)	Loss 2.0764e-01 (2.5825e-01)	Acc@1  90.62 ( 90.80)	Acc@5 100.00 ( 99.77)
Epoch: [30][ 40/391]	Time  0.107 ( 0.113)	Data  0.001 ( 0.009)	Loss 2.2083e-01 (2.4508e-01)	Acc@1  92.19 ( 91.23)	Acc@5 100.00 ( 99.79)
Epoch: [30][ 50/391]	Time  0.104 ( 0.112)	Data  0.001 ( 0.008)	Loss 2.8223e-01 (2.4078e-01)	Acc@1  87.50 ( 91.38)	Acc@5 100.00 ( 99.77)
Epoch: [30][ 60/391]	Time  0.104 ( 0.111)	Data  0.001 ( 0.008)	Loss 1.6394e-01 (2.3560e-01)	Acc@1  94.53 ( 91.68)	Acc@5  99.22 ( 99.80)
Epoch: [30][ 70/391]	Time  0.105 ( 0.111)	Data  0.001 ( 0.007)	Loss 2.6221e-01 (2.3387e-01)	Acc@1  89.84 ( 91.73)	Acc@5 100.00 ( 99.82)
Epoch: [30][ 80/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.007)	Loss 1.7944e-01 (2.3062e-01)	Acc@1  94.53 ( 91.91)	Acc@5 100.00 ( 99.85)
Epoch: [30][ 90/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.6736e-01 (2.2951e-01)	Acc@1  93.75 ( 91.96)	Acc@5 100.00 ( 99.86)
Epoch: [30][100/391]	Time  0.110 ( 0.110)	Data  0.001 ( 0.006)	Loss 2.1582e-01 (2.2574e-01)	Acc@1  91.41 ( 92.01)	Acc@5 100.00 ( 99.85)
Epoch: [30][110/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.9421e-01 (2.2258e-01)	Acc@1  93.75 ( 92.17)	Acc@5  99.22 ( 99.85)
Epoch: [30][120/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 2.4500e-01 (2.1932e-01)	Acc@1  91.41 ( 92.28)	Acc@5  99.22 ( 99.85)
Epoch: [30][130/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 2.1497e-01 (2.1822e-01)	Acc@1  91.41 ( 92.31)	Acc@5 100.00 ( 99.84)
Epoch: [30][140/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.006)	Loss 1.9177e-01 (2.1603e-01)	Acc@1  93.75 ( 92.44)	Acc@5 100.00 ( 99.84)
Epoch: [30][150/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.006)	Loss 2.2083e-01 (2.1461e-01)	Acc@1  91.41 ( 92.49)	Acc@5 100.00 ( 99.86)
Epoch: [30][160/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.9031e-01 (2.1333e-01)	Acc@1  92.97 ( 92.57)	Acc@5 100.00 ( 99.86)
Epoch: [30][170/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.0093e-01 (2.1201e-01)	Acc@1  93.75 ( 92.61)	Acc@5 100.00 ( 99.87)
Epoch: [30][180/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.0593e-01 (2.1003e-01)	Acc@1  92.97 ( 92.67)	Acc@5 100.00 ( 99.88)
Epoch: [30][190/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.5244e-01 (2.0877e-01)	Acc@1  91.41 ( 92.72)	Acc@5 100.00 ( 99.88)
Epoch: [30][200/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.9934e-01 (2.0762e-01)	Acc@1  92.19 ( 92.76)	Acc@5 100.00 ( 99.89)
Epoch: [30][210/391]	Time  0.101 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.6626e-01 (2.0633e-01)	Acc@1  94.53 ( 92.80)	Acc@5 100.00 ( 99.89)
Epoch: [30][220/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0614e-01 (2.0512e-01)	Acc@1  96.09 ( 92.88)	Acc@5 100.00 ( 99.89)
Epoch: [30][230/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.6821e-01 (2.0506e-01)	Acc@1  93.75 ( 92.90)	Acc@5 100.00 ( 99.90)
Epoch: [30][240/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.8201e-01 (2.0483e-01)	Acc@1  92.19 ( 92.90)	Acc@5 100.00 ( 99.89)
Epoch: [30][250/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.3596e-01 (2.0451e-01)	Acc@1  93.75 ( 92.92)	Acc@5 100.00 ( 99.88)
Epoch: [30][260/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.3926e-01 (2.0471e-01)	Acc@1  92.97 ( 92.87)	Acc@5 100.00 ( 99.88)
Epoch: [30][270/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.7319e-01 (2.0473e-01)	Acc@1  91.41 ( 92.84)	Acc@5 100.00 ( 99.88)
Epoch: [30][280/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.6833e-01 (2.0451e-01)	Acc@1  93.75 ( 92.84)	Acc@5 100.00 ( 99.89)
Epoch: [30][290/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.2290e-01 (2.0403e-01)	Acc@1  89.84 ( 92.86)	Acc@5  99.22 ( 99.89)
Epoch: [30][300/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.6431e-01 (2.0339e-01)	Acc@1  94.53 ( 92.89)	Acc@5 100.00 ( 99.88)
Epoch: [30][310/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3611e-01 (2.0253e-01)	Acc@1  95.31 ( 92.94)	Acc@5 100.00 ( 99.88)
Epoch: [30][320/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2659e-01 (2.0227e-01)	Acc@1  96.09 ( 92.96)	Acc@5 100.00 ( 99.88)
Epoch: [30][330/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5601e-01 (2.0159e-01)	Acc@1  96.09 ( 92.99)	Acc@5 100.00 ( 99.89)
Epoch: [30][340/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.9617e-01 (2.0168e-01)	Acc@1  92.19 ( 92.96)	Acc@5 100.00 ( 99.89)
Epoch: [30][350/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.8335e-01 (2.0145e-01)	Acc@1  91.41 ( 92.97)	Acc@5 100.00 ( 99.89)
Epoch: [30][360/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.3350e-01 (2.0133e-01)	Acc@1  88.28 ( 92.97)	Acc@5 100.00 ( 99.89)
Epoch: [30][370/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.9092e-01 (2.0117e-01)	Acc@1  94.53 ( 92.99)	Acc@5  99.22 ( 99.89)
Epoch: [30][380/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5344e-01 (2.0057e-01)	Acc@1  93.75 ( 93.02)	Acc@5 100.00 ( 99.89)
Epoch: [30][390/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4124e-01 (2.0004e-01)	Acc@1  93.75 ( 93.03)	Acc@5 100.00 ( 99.89)
## e[30] optimizer.zero_grad (sum) time: 0.3565239906311035
## e[30]       loss.backward (sum) time: 13.7210111618042
## e[30]      optimizer.step (sum) time: 2.8679604530334473
## epoch[30] training(only) time: 42.594907999038696
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 2.0569e-01 (2.0569e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.039 ( 0.052)	Loss 2.5342e-01 (2.7259e-01)	Acc@1  92.00 ( 90.82)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.037 ( 0.045)	Loss 3.6084e-01 (3.0019e-01)	Acc@1  88.00 ( 90.05)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.037 ( 0.043)	Loss 3.0957e-01 (3.1936e-01)	Acc@1  90.00 ( 90.03)	Acc@5  99.00 ( 99.58)
Test: [ 40/100]	Time  0.037 ( 0.042)	Loss 2.8027e-01 (3.2468e-01)	Acc@1  93.00 ( 89.95)	Acc@5  99.00 ( 99.61)
Test: [ 50/100]	Time  0.036 ( 0.041)	Loss 1.7883e-01 (3.2245e-01)	Acc@1  93.00 ( 90.00)	Acc@5 100.00 ( 99.57)
Test: [ 60/100]	Time  0.037 ( 0.040)	Loss 3.2935e-01 (3.1993e-01)	Acc@1  91.00 ( 89.87)	Acc@5 100.00 ( 99.57)
Test: [ 70/100]	Time  0.039 ( 0.040)	Loss 3.9136e-01 (3.1763e-01)	Acc@1  90.00 ( 89.85)	Acc@5 100.00 ( 99.58)
Test: [ 80/100]	Time  0.036 ( 0.039)	Loss 3.5156e-01 (3.1833e-01)	Acc@1  85.00 ( 89.79)	Acc@5 100.00 ( 99.60)
Test: [ 90/100]	Time  0.037 ( 0.039)	Loss 1.5015e-01 (3.1483e-01)	Acc@1  95.00 ( 89.79)	Acc@5 100.00 ( 99.63)
 * Acc@1 89.830 Acc@5 99.640
### epoch[30] execution time: 46.57518672943115
EPOCH 31
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [31][  0/391]	Time  0.263 ( 0.263)	Data  0.165 ( 0.165)	Loss 2.4426e-01 (2.4426e-01)	Acc@1  94.53 ( 94.53)	Acc@5  98.44 ( 98.44)
Epoch: [31][ 10/391]	Time  0.107 ( 0.123)	Data  0.001 ( 0.018)	Loss 1.3867e-01 (1.6261e-01)	Acc@1  93.75 ( 94.46)	Acc@5 100.00 ( 99.86)
Epoch: [31][ 20/391]	Time  0.108 ( 0.116)	Data  0.001 ( 0.012)	Loss 1.4783e-01 (1.6404e-01)	Acc@1  96.09 ( 94.31)	Acc@5 100.00 ( 99.89)
Epoch: [31][ 30/391]	Time  0.110 ( 0.114)	Data  0.001 ( 0.009)	Loss 2.3352e-01 (1.6742e-01)	Acc@1  92.19 ( 94.28)	Acc@5 100.00 ( 99.90)
Epoch: [31][ 40/391]	Time  0.106 ( 0.112)	Data  0.001 ( 0.008)	Loss 1.4075e-01 (1.6833e-01)	Acc@1  94.53 ( 94.25)	Acc@5 100.00 ( 99.89)
Epoch: [31][ 50/391]	Time  0.107 ( 0.111)	Data  0.001 ( 0.007)	Loss 2.1777e-01 (1.6790e-01)	Acc@1  93.75 ( 94.39)	Acc@5 100.00 ( 99.91)
Epoch: [31][ 60/391]	Time  0.110 ( 0.111)	Data  0.001 ( 0.007)	Loss 1.1908e-01 (1.6688e-01)	Acc@1  96.88 ( 94.33)	Acc@5 100.00 ( 99.92)
Epoch: [31][ 70/391]	Time  0.106 ( 0.111)	Data  0.001 ( 0.006)	Loss 7.2815e-02 (1.6879e-01)	Acc@1  99.22 ( 94.26)	Acc@5 100.00 ( 99.92)
Epoch: [31][ 80/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.1890e-01 (1.7001e-01)	Acc@1  96.88 ( 94.20)	Acc@5 100.00 ( 99.89)
Epoch: [31][ 90/391]	Time  0.109 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.6309e-01 (1.6795e-01)	Acc@1  94.53 ( 94.27)	Acc@5 100.00 ( 99.91)
Epoch: [31][100/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.2695e-01 (1.6753e-01)	Acc@1  96.88 ( 94.31)	Acc@5 100.00 ( 99.90)
Epoch: [31][110/391]	Time  0.104 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.3403e-01 (1.6706e-01)	Acc@1  94.53 ( 94.25)	Acc@5 100.00 ( 99.90)
Epoch: [31][120/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.005)	Loss 1.0211e-01 (1.6690e-01)	Acc@1  97.66 ( 94.26)	Acc@5 100.00 ( 99.90)
Epoch: [31][130/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.005)	Loss 1.4526e-01 (1.6864e-01)	Acc@1  93.75 ( 94.17)	Acc@5  99.22 ( 99.89)
Epoch: [31][140/391]	Time  0.102 ( 0.110)	Data  0.001 ( 0.005)	Loss 1.9604e-01 (1.7056e-01)	Acc@1  94.53 ( 94.15)	Acc@5 100.00 ( 99.88)
Epoch: [31][150/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.005)	Loss 2.1326e-01 (1.6983e-01)	Acc@1  96.09 ( 94.22)	Acc@5  99.22 ( 99.89)
Epoch: [31][160/391]	Time  0.104 ( 0.110)	Data  0.001 ( 0.005)	Loss 1.7468e-01 (1.6789e-01)	Acc@1  91.41 ( 94.28)	Acc@5 100.00 ( 99.89)
Epoch: [31][170/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.7651e-01 (1.6815e-01)	Acc@1  94.53 ( 94.29)	Acc@5 100.00 ( 99.89)
Epoch: [31][180/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.5327e-02 (1.6782e-01)	Acc@1  97.66 ( 94.28)	Acc@5 100.00 ( 99.90)
Epoch: [31][190/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2708e-01 (1.6641e-01)	Acc@1  96.09 ( 94.31)	Acc@5 100.00 ( 99.90)
Epoch: [31][200/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.0068e-01 (1.6607e-01)	Acc@1  91.41 ( 94.32)	Acc@5 100.00 ( 99.91)
Epoch: [31][210/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.3933e-02 (1.6514e-01)	Acc@1  96.88 ( 94.36)	Acc@5 100.00 ( 99.91)
Epoch: [31][220/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.3206e-01 (1.6466e-01)	Acc@1  92.19 ( 94.39)	Acc@5  99.22 ( 99.91)
Epoch: [31][230/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.6514e-01 (1.6576e-01)	Acc@1  93.75 ( 94.35)	Acc@5 100.00 ( 99.91)
Epoch: [31][240/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4905e-01 (1.6503e-01)	Acc@1  95.31 ( 94.39)	Acc@5 100.00 ( 99.91)
Epoch: [31][250/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1810e-01 (1.6549e-01)	Acc@1  96.88 ( 94.36)	Acc@5 100.00 ( 99.90)
Epoch: [31][260/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.9355e-02 (1.6642e-01)	Acc@1  96.09 ( 94.32)	Acc@5 100.00 ( 99.90)
Epoch: [31][270/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.0835e-01 (1.6660e-01)	Acc@1  89.84 ( 94.33)	Acc@5  99.22 ( 99.89)
Epoch: [31][280/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4978e-01 (1.6737e-01)	Acc@1  91.41 ( 94.30)	Acc@5 100.00 ( 99.89)
Epoch: [31][290/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.6321e-01 (1.6795e-01)	Acc@1  93.75 ( 94.27)	Acc@5 100.00 ( 99.89)
Epoch: [31][300/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.1265e-01 (1.6785e-01)	Acc@1  92.97 ( 94.26)	Acc@5  99.22 ( 99.89)
Epoch: [31][310/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3953e-01 (1.6896e-01)	Acc@1  96.09 ( 94.20)	Acc@5 100.00 ( 99.89)
Epoch: [31][320/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.9055e-01 (1.6866e-01)	Acc@1  92.19 ( 94.23)	Acc@5 100.00 ( 99.89)
Epoch: [31][330/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.9336e-01 (1.6799e-01)	Acc@1  91.41 ( 94.26)	Acc@5 100.00 ( 99.89)
Epoch: [31][340/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.0813e-01 (1.6851e-01)	Acc@1  92.97 ( 94.23)	Acc@5 100.00 ( 99.89)
Epoch: [31][350/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5747e-01 (1.6808e-01)	Acc@1  96.09 ( 94.25)	Acc@5 100.00 ( 99.89)
Epoch: [31][360/391]	Time  0.112 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.8811e-01 (1.6817e-01)	Acc@1  92.19 ( 94.22)	Acc@5 100.00 ( 99.89)
Epoch: [31][370/391]	Time  0.111 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.1960e-01 (1.6839e-01)	Acc@1  93.75 ( 94.22)	Acc@5  99.22 ( 99.89)
Epoch: [31][380/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3342e-01 (1.6881e-01)	Acc@1  93.75 ( 94.19)	Acc@5 100.00 ( 99.89)
Epoch: [31][390/391]	Time  0.098 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.3547e-02 (1.6900e-01)	Acc@1  98.75 ( 94.18)	Acc@5 100.00 ( 99.89)
## e[31] optimizer.zero_grad (sum) time: 0.3468966484069824
## e[31]       loss.backward (sum) time: 13.729957342147827
## e[31]      optimizer.step (sum) time: 2.8619558811187744
## epoch[31] training(only) time: 42.76243233680725
# Switched to evaluate mode...
Test: [  0/100]	Time  0.243 ( 0.243)	Loss 2.1204e-01 (2.1204e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.036 ( 0.056)	Loss 1.9751e-01 (2.6806e-01)	Acc@1  94.00 ( 91.55)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.036 ( 0.047)	Loss 3.5229e-01 (2.9539e-01)	Acc@1  86.00 ( 90.57)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.039 ( 0.044)	Loss 2.7417e-01 (3.1341e-01)	Acc@1  88.00 ( 90.45)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.039 ( 0.042)	Loss 3.0396e-01 (3.1791e-01)	Acc@1  92.00 ( 90.41)	Acc@5  99.00 ( 99.71)
Test: [ 50/100]	Time  0.039 ( 0.041)	Loss 1.6492e-01 (3.1326e-01)	Acc@1  94.00 ( 90.49)	Acc@5 100.00 ( 99.65)
Test: [ 60/100]	Time  0.038 ( 0.041)	Loss 3.1592e-01 (3.0855e-01)	Acc@1  95.00 ( 90.62)	Acc@5 100.00 ( 99.66)
Test: [ 70/100]	Time  0.039 ( 0.040)	Loss 3.9502e-01 (3.0698e-01)	Acc@1  88.00 ( 90.45)	Acc@5 100.00 ( 99.68)
Test: [ 80/100]	Time  0.035 ( 0.040)	Loss 3.0493e-01 (3.0939e-01)	Acc@1  87.00 ( 90.37)	Acc@5 100.00 ( 99.69)
Test: [ 90/100]	Time  0.036 ( 0.040)	Loss 1.7249e-01 (3.0533e-01)	Acc@1  94.00 ( 90.38)	Acc@5 100.00 ( 99.70)
 * Acc@1 90.370 Acc@5 99.720
### epoch[31] execution time: 46.78507399559021
EPOCH 32
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [32][  0/391]	Time  0.265 ( 0.265)	Data  0.164 ( 0.164)	Loss 1.9629e-01 (1.9629e-01)	Acc@1  92.97 ( 92.97)	Acc@5  99.22 ( 99.22)
Epoch: [32][ 10/391]	Time  0.108 ( 0.122)	Data  0.001 ( 0.018)	Loss 9.6130e-02 (1.3268e-01)	Acc@1  96.88 ( 94.96)	Acc@5 100.00 ( 99.93)
Epoch: [32][ 20/391]	Time  0.106 ( 0.116)	Data  0.001 ( 0.012)	Loss 2.8271e-01 (1.4767e-01)	Acc@1  89.84 ( 94.94)	Acc@5 100.00 ( 99.85)
Epoch: [32][ 30/391]	Time  0.109 ( 0.113)	Data  0.001 ( 0.009)	Loss 2.1289e-01 (1.5575e-01)	Acc@1  91.41 ( 94.71)	Acc@5 100.00 ( 99.87)
Epoch: [32][ 40/391]	Time  0.107 ( 0.112)	Data  0.001 ( 0.008)	Loss 1.9763e-01 (1.5702e-01)	Acc@1  92.97 ( 94.57)	Acc@5 100.00 ( 99.89)
Epoch: [32][ 50/391]	Time  0.104 ( 0.111)	Data  0.001 ( 0.007)	Loss 1.5698e-01 (1.5927e-01)	Acc@1  94.53 ( 94.52)	Acc@5 100.00 ( 99.88)
Epoch: [32][ 60/391]	Time  0.105 ( 0.111)	Data  0.001 ( 0.007)	Loss 1.2915e-01 (1.5650e-01)	Acc@1  96.09 ( 94.58)	Acc@5 100.00 ( 99.90)
Epoch: [32][ 70/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.006)	Loss 7.7271e-02 (1.5387e-01)	Acc@1  97.66 ( 94.70)	Acc@5 100.00 ( 99.91)
Epoch: [32][ 80/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 7.9285e-02 (1.5175e-01)	Acc@1  96.88 ( 94.73)	Acc@5 100.00 ( 99.92)
Epoch: [32][ 90/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.4099e-01 (1.5268e-01)	Acc@1  93.75 ( 94.64)	Acc@5 100.00 ( 99.92)
Epoch: [32][100/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.0565e-01 (1.5145e-01)	Acc@1  96.88 ( 94.68)	Acc@5 100.00 ( 99.93)
Epoch: [32][110/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.3318e-01 (1.5362e-01)	Acc@1  94.53 ( 94.61)	Acc@5  99.22 ( 99.90)
Epoch: [32][120/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1615e-01 (1.5341e-01)	Acc@1  95.31 ( 94.61)	Acc@5 100.00 ( 99.90)
Epoch: [32][130/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5369e-01 (1.5343e-01)	Acc@1  95.31 ( 94.65)	Acc@5 100.00 ( 99.90)
Epoch: [32][140/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.9727e-01 (1.5429e-01)	Acc@1  92.97 ( 94.60)	Acc@5 100.00 ( 99.90)
Epoch: [32][150/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.5928e-02 (1.5313e-01)	Acc@1  97.66 ( 94.63)	Acc@5 100.00 ( 99.91)
Epoch: [32][160/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5112e-01 (1.5201e-01)	Acc@1  94.53 ( 94.65)	Acc@5 100.00 ( 99.91)
Epoch: [32][170/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1578e-01 (1.5219e-01)	Acc@1  96.88 ( 94.67)	Acc@5 100.00 ( 99.92)
Epoch: [32][180/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.7261e-01 (1.5335e-01)	Acc@1  92.97 ( 94.62)	Acc@5 100.00 ( 99.92)
Epoch: [32][190/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.6003e-01 (1.5367e-01)	Acc@1  93.75 ( 94.58)	Acc@5  99.22 ( 99.92)
Epoch: [32][200/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.4829e-01 (1.5439e-01)	Acc@1  89.06 ( 94.57)	Acc@5 100.00 ( 99.92)
Epoch: [32][210/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.9873e-01 (1.5423e-01)	Acc@1  92.19 ( 94.57)	Acc@5 100.00 ( 99.92)
Epoch: [32][220/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.6309e-01 (1.5387e-01)	Acc@1  93.75 ( 94.61)	Acc@5 100.00 ( 99.93)
Epoch: [32][230/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.1228e-01 (1.5319e-01)	Acc@1  91.41 ( 94.64)	Acc@5 100.00 ( 99.92)
Epoch: [32][240/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.9507e-01 (1.5363e-01)	Acc@1  94.53 ( 94.64)	Acc@5 100.00 ( 99.92)
Epoch: [32][250/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.6626e-01 (1.5351e-01)	Acc@1  93.75 ( 94.65)	Acc@5 100.00 ( 99.92)
Epoch: [32][260/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2103e-01 (1.5301e-01)	Acc@1  96.09 ( 94.65)	Acc@5 100.00 ( 99.92)
Epoch: [32][270/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3757e-01 (1.5317e-01)	Acc@1  94.53 ( 94.64)	Acc@5 100.00 ( 99.92)
Epoch: [32][280/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2891e-01 (1.5311e-01)	Acc@1  96.88 ( 94.64)	Acc@5 100.00 ( 99.92)
Epoch: [32][290/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0052e-01 (1.5236e-01)	Acc@1  95.31 ( 94.68)	Acc@5 100.00 ( 99.92)
Epoch: [32][300/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3220e-01 (1.5194e-01)	Acc@1  94.53 ( 94.71)	Acc@5 100.00 ( 99.93)
Epoch: [32][310/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.0642e-01 (1.5199e-01)	Acc@1  93.75 ( 94.70)	Acc@5 100.00 ( 99.92)
Epoch: [32][320/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.6882e-01 (1.5177e-01)	Acc@1  92.97 ( 94.70)	Acc@5 100.00 ( 99.93)
Epoch: [32][330/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4429e-01 (1.5159e-01)	Acc@1  94.53 ( 94.70)	Acc@5 100.00 ( 99.93)
Epoch: [32][340/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5759e-01 (1.5151e-01)	Acc@1  94.53 ( 94.69)	Acc@5 100.00 ( 99.93)
Epoch: [32][350/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5820e-01 (1.5086e-01)	Acc@1  95.31 ( 94.73)	Acc@5 100.00 ( 99.93)
Epoch: [32][360/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2390e-01 (1.5056e-01)	Acc@1  95.31 ( 94.74)	Acc@5 100.00 ( 99.93)
Epoch: [32][370/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4246e-01 (1.5020e-01)	Acc@1  95.31 ( 94.75)	Acc@5 100.00 ( 99.93)
Epoch: [32][380/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.0911e-01 (1.5009e-01)	Acc@1  92.97 ( 94.76)	Acc@5 100.00 ( 99.93)
Epoch: [32][390/391]	Time  0.098 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.7725e-01 (1.5007e-01)	Acc@1  93.75 ( 94.74)	Acc@5 100.00 ( 99.93)
## e[32] optimizer.zero_grad (sum) time: 0.3476545810699463
## e[32]       loss.backward (sum) time: 13.64370608329773
## e[32]      optimizer.step (sum) time: 2.752324104309082
## epoch[32] training(only) time: 42.60257411003113
# Switched to evaluate mode...
Test: [  0/100]	Time  0.194 ( 0.194)	Loss 2.0740e-01 (2.0740e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.039 ( 0.052)	Loss 1.7847e-01 (2.6649e-01)	Acc@1  93.00 ( 91.55)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.038 ( 0.045)	Loss 3.8989e-01 (2.9813e-01)	Acc@1  86.00 ( 90.67)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.040 ( 0.043)	Loss 2.6245e-01 (3.1718e-01)	Acc@1  88.00 ( 90.71)	Acc@5 100.00 ( 99.55)
Test: [ 40/100]	Time  0.039 ( 0.041)	Loss 2.9346e-01 (3.2306e-01)	Acc@1  92.00 ( 90.61)	Acc@5  99.00 ( 99.59)
Test: [ 50/100]	Time  0.038 ( 0.040)	Loss 1.5417e-01 (3.1764e-01)	Acc@1  94.00 ( 90.80)	Acc@5 100.00 ( 99.59)
Test: [ 60/100]	Time  0.037 ( 0.040)	Loss 3.5229e-01 (3.1230e-01)	Acc@1  94.00 ( 90.89)	Acc@5  99.00 ( 99.61)
Test: [ 70/100]	Time  0.036 ( 0.040)	Loss 4.0259e-01 (3.1041e-01)	Acc@1  90.00 ( 90.82)	Acc@5 100.00 ( 99.63)
Test: [ 80/100]	Time  0.037 ( 0.039)	Loss 2.6782e-01 (3.1197e-01)	Acc@1  91.00 ( 90.67)	Acc@5 100.00 ( 99.65)
Test: [ 90/100]	Time  0.037 ( 0.039)	Loss 1.5674e-01 (3.0679e-01)	Acc@1  97.00 ( 90.82)	Acc@5 100.00 ( 99.66)
 * Acc@1 90.730 Acc@5 99.680
### epoch[32] execution time: 46.57161498069763
EPOCH 33
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [33][  0/391]	Time  0.312 ( 0.312)	Data  0.217 ( 0.217)	Loss 8.9600e-02 (8.9600e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [33][ 10/391]	Time  0.100 ( 0.126)	Data  0.001 ( 0.023)	Loss 1.1707e-01 (1.3224e-01)	Acc@1  94.53 ( 95.10)	Acc@5 100.00 ( 99.93)
Epoch: [33][ 20/391]	Time  0.107 ( 0.117)	Data  0.001 ( 0.014)	Loss 1.0858e-01 (1.4202e-01)	Acc@1  95.31 ( 94.79)	Acc@5 100.00 ( 99.85)
Epoch: [33][ 30/391]	Time  0.108 ( 0.115)	Data  0.002 ( 0.011)	Loss 1.6724e-01 (1.3808e-01)	Acc@1  95.31 ( 94.96)	Acc@5 100.00 ( 99.90)
Epoch: [33][ 40/391]	Time  0.107 ( 0.113)	Data  0.001 ( 0.009)	Loss 2.1240e-01 (1.4331e-01)	Acc@1  92.97 ( 94.70)	Acc@5 100.00 ( 99.90)
Epoch: [33][ 50/391]	Time  0.108 ( 0.113)	Data  0.001 ( 0.008)	Loss 1.6187e-01 (1.4164e-01)	Acc@1  93.75 ( 94.79)	Acc@5 100.00 ( 99.92)
Epoch: [33][ 60/391]	Time  0.108 ( 0.112)	Data  0.001 ( 0.008)	Loss 2.2900e-01 (1.4720e-01)	Acc@1  93.75 ( 94.68)	Acc@5 100.00 ( 99.94)
Epoch: [33][ 70/391]	Time  0.105 ( 0.111)	Data  0.001 ( 0.007)	Loss 5.7190e-02 (1.4456e-01)	Acc@1  99.22 ( 94.87)	Acc@5 100.00 ( 99.94)
Epoch: [33][ 80/391]	Time  0.110 ( 0.111)	Data  0.001 ( 0.007)	Loss 1.5381e-01 (1.4296e-01)	Acc@1  95.31 ( 94.98)	Acc@5 100.00 ( 99.94)
Epoch: [33][ 90/391]	Time  0.106 ( 0.111)	Data  0.001 ( 0.006)	Loss 2.1655e-01 (1.4204e-01)	Acc@1  90.62 ( 94.98)	Acc@5 100.00 ( 99.95)
Epoch: [33][100/391]	Time  0.108 ( 0.111)	Data  0.001 ( 0.006)	Loss 1.8823e-01 (1.4056e-01)	Acc@1  94.53 ( 95.04)	Acc@5 100.00 ( 99.95)
Epoch: [33][110/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.5405e-01 (1.4119e-01)	Acc@1  94.53 ( 95.05)	Acc@5 100.00 ( 99.96)
Epoch: [33][120/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.0455e-01 (1.4217e-01)	Acc@1  96.09 ( 95.07)	Acc@5 100.00 ( 99.96)
Epoch: [33][130/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.006)	Loss 2.3157e-01 (1.4230e-01)	Acc@1  90.62 ( 95.09)	Acc@5 100.00 ( 99.95)
Epoch: [33][140/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.6663e-01 (1.4223e-01)	Acc@1  95.31 ( 95.07)	Acc@5 100.00 ( 99.96)
Epoch: [33][150/391]	Time  0.104 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.4868e-01 (1.4433e-01)	Acc@1  94.53 ( 95.00)	Acc@5 100.00 ( 99.96)
Epoch: [33][160/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.005)	Loss 1.2231e-01 (1.4303e-01)	Acc@1  94.53 ( 95.05)	Acc@5 100.00 ( 99.96)
Epoch: [33][170/391]	Time  0.102 ( 0.110)	Data  0.001 ( 0.005)	Loss 1.5845e-01 (1.4410e-01)	Acc@1  93.75 ( 95.04)	Acc@5 100.00 ( 99.96)
Epoch: [33][180/391]	Time  0.104 ( 0.110)	Data  0.001 ( 0.005)	Loss 1.6138e-01 (1.4332e-01)	Acc@1  93.75 ( 95.05)	Acc@5 100.00 ( 99.97)
Epoch: [33][190/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.005)	Loss 1.1829e-01 (1.4204e-01)	Acc@1  97.66 ( 95.12)	Acc@5 100.00 ( 99.96)
Epoch: [33][200/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.005)	Loss 1.0309e-01 (1.4241e-01)	Acc@1  96.88 ( 95.11)	Acc@5 100.00 ( 99.97)
Epoch: [33][210/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.0630e-01 (1.4327e-01)	Acc@1  92.97 ( 95.05)	Acc@5 100.00 ( 99.97)
Epoch: [33][220/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.8677e-01 (1.4342e-01)	Acc@1  93.75 ( 95.05)	Acc@5 100.00 ( 99.97)
Epoch: [33][230/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.9714e-01 (1.4463e-01)	Acc@1  92.97 ( 95.00)	Acc@5 100.00 ( 99.96)
Epoch: [33][240/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.6304e-02 (1.4384e-01)	Acc@1  98.44 ( 95.01)	Acc@5 100.00 ( 99.96)
Epoch: [33][250/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3635e-01 (1.4370e-01)	Acc@1  94.53 ( 95.02)	Acc@5 100.00 ( 99.96)
Epoch: [33][260/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3708e-01 (1.4369e-01)	Acc@1  95.31 ( 95.03)	Acc@5 100.00 ( 99.96)
Epoch: [33][270/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.0027e-02 (1.4357e-01)	Acc@1  96.88 ( 95.02)	Acc@5 100.00 ( 99.96)
Epoch: [33][280/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.2458e-02 (1.4213e-01)	Acc@1  97.66 ( 95.07)	Acc@5 100.00 ( 99.96)
Epoch: [33][290/391]	Time  0.113 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.4596e-02 (1.4233e-01)	Acc@1  99.22 ( 95.05)	Acc@5 100.00 ( 99.96)
Epoch: [33][300/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.8938e-02 (1.4212e-01)	Acc@1  95.31 ( 95.06)	Acc@5 100.00 ( 99.96)
Epoch: [33][310/391]	Time  0.108 ( 0.109)	Data  0.002 ( 0.005)	Loss 1.1975e-01 (1.4233e-01)	Acc@1  95.31 ( 95.06)	Acc@5 100.00 ( 99.96)
Epoch: [33][320/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.1045e-02 (1.4196e-01)	Acc@1  98.44 ( 95.06)	Acc@5 100.00 ( 99.96)
Epoch: [33][330/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.6565e-01 (1.4149e-01)	Acc@1  92.97 ( 95.07)	Acc@5 100.00 ( 99.96)
Epoch: [33][340/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.4585e-02 (1.4073e-01)	Acc@1  95.31 ( 95.10)	Acc@5 100.00 ( 99.96)
Epoch: [33][350/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.9651e-02 (1.4086e-01)	Acc@1  97.66 ( 95.11)	Acc@5 100.00 ( 99.96)
Epoch: [33][360/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.9072e-02 (1.4015e-01)	Acc@1  98.44 ( 95.11)	Acc@5 100.00 ( 99.96)
Epoch: [33][370/391]	Time  0.113 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5808e-01 (1.4068e-01)	Acc@1  90.62 ( 95.09)	Acc@5 100.00 ( 99.96)
Epoch: [33][380/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.2876e-01 (1.4104e-01)	Acc@1  92.19 ( 95.08)	Acc@5 100.00 ( 99.95)
Epoch: [33][390/391]	Time  0.100 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2390e-01 (1.4111e-01)	Acc@1  96.25 ( 95.09)	Acc@5 100.00 ( 99.95)
## e[33] optimizer.zero_grad (sum) time: 0.3471074104309082
## e[33]       loss.backward (sum) time: 13.697603940963745
## e[33]      optimizer.step (sum) time: 2.778322219848633
## epoch[33] training(only) time: 42.75441527366638
# Switched to evaluate mode...
Test: [  0/100]	Time  0.204 ( 0.204)	Loss 1.9629e-01 (1.9629e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.036 ( 0.053)	Loss 1.9897e-01 (2.7738e-01)	Acc@1  94.00 ( 91.91)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.037 ( 0.046)	Loss 4.0186e-01 (3.0725e-01)	Acc@1  86.00 ( 90.95)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.036 ( 0.043)	Loss 2.7197e-01 (3.2779e-01)	Acc@1  89.00 ( 90.97)	Acc@5 100.00 ( 99.61)
Test: [ 40/100]	Time  0.036 ( 0.042)	Loss 2.9028e-01 (3.3153e-01)	Acc@1  90.00 ( 90.73)	Acc@5  99.00 ( 99.61)
Test: [ 50/100]	Time  0.036 ( 0.041)	Loss 1.3904e-01 (3.2721e-01)	Acc@1  95.00 ( 90.84)	Acc@5 100.00 ( 99.57)
Test: [ 60/100]	Time  0.036 ( 0.040)	Loss 3.4253e-01 (3.2257e-01)	Acc@1  95.00 ( 90.77)	Acc@5  99.00 ( 99.61)
Test: [ 70/100]	Time  0.039 ( 0.040)	Loss 4.0820e-01 (3.2052e-01)	Acc@1  89.00 ( 90.68)	Acc@5 100.00 ( 99.61)
Test: [ 80/100]	Time  0.040 ( 0.039)	Loss 2.3938e-01 (3.2262e-01)	Acc@1  90.00 ( 90.57)	Acc@5 100.00 ( 99.60)
Test: [ 90/100]	Time  0.037 ( 0.039)	Loss 1.8677e-01 (3.1829e-01)	Acc@1  95.00 ( 90.67)	Acc@5 100.00 ( 99.62)
 * Acc@1 90.590 Acc@5 99.640
### epoch[33] execution time: 46.75332951545715
EPOCH 34
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [34][  0/391]	Time  0.263 ( 0.263)	Data  0.169 ( 0.169)	Loss 1.1176e-01 (1.1176e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [34][ 10/391]	Time  0.103 ( 0.122)	Data  0.001 ( 0.019)	Loss 9.8999e-02 (1.2668e-01)	Acc@1  96.09 ( 95.45)	Acc@5 100.00 (100.00)
Epoch: [34][ 20/391]	Time  0.104 ( 0.115)	Data  0.001 ( 0.012)	Loss 7.7026e-02 (1.1348e-01)	Acc@1  98.44 ( 96.02)	Acc@5 100.00 ( 99.96)
Epoch: [34][ 30/391]	Time  0.105 ( 0.113)	Data  0.001 ( 0.009)	Loss 1.0260e-01 (1.2427e-01)	Acc@1  95.31 ( 95.72)	Acc@5 100.00 ( 99.92)
Epoch: [34][ 40/391]	Time  0.106 ( 0.112)	Data  0.001 ( 0.008)	Loss 1.1810e-01 (1.2115e-01)	Acc@1  95.31 ( 95.83)	Acc@5 100.00 ( 99.94)
Epoch: [34][ 50/391]	Time  0.105 ( 0.111)	Data  0.001 ( 0.007)	Loss 1.7700e-01 (1.2711e-01)	Acc@1  93.75 ( 95.56)	Acc@5 100.00 ( 99.91)
Epoch: [34][ 60/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.007)	Loss 1.6101e-01 (1.2599e-01)	Acc@1  95.31 ( 95.70)	Acc@5 100.00 ( 99.92)
Epoch: [34][ 70/391]	Time  0.110 ( 0.110)	Data  0.001 ( 0.006)	Loss 9.9426e-02 (1.2458e-01)	Acc@1  96.09 ( 95.70)	Acc@5 100.00 ( 99.93)
Epoch: [34][ 80/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.2335e-01 (1.2400e-01)	Acc@1  97.66 ( 95.74)	Acc@5 100.00 ( 99.94)
Epoch: [34][ 90/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.006)	Loss 1.6968e-01 (1.2436e-01)	Acc@1  92.97 ( 95.66)	Acc@5 100.00 ( 99.94)
Epoch: [34][100/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.006)	Loss 2.6294e-01 (1.2550e-01)	Acc@1  89.84 ( 95.56)	Acc@5 100.00 ( 99.95)
Epoch: [34][110/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.006)	Loss 1.5076e-01 (1.2691e-01)	Acc@1  93.75 ( 95.51)	Acc@5 100.00 ( 99.94)
Epoch: [34][120/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.3572e-01 (1.2794e-01)	Acc@1  93.75 ( 95.46)	Acc@5 100.00 ( 99.95)
Epoch: [34][130/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.7576e-02 (1.2781e-01)	Acc@1  98.44 ( 95.49)	Acc@5 100.00 ( 99.95)
Epoch: [34][140/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3965e-01 (1.2846e-01)	Acc@1  95.31 ( 95.49)	Acc@5 100.00 ( 99.94)
Epoch: [34][150/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2512e-01 (1.2890e-01)	Acc@1  96.09 ( 95.49)	Acc@5 100.00 ( 99.94)
Epoch: [34][160/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.7090e-01 (1.2897e-01)	Acc@1  93.75 ( 95.53)	Acc@5 100.00 ( 99.95)
Epoch: [34][170/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.1619e-01 (1.2867e-01)	Acc@1  92.19 ( 95.57)	Acc@5 100.00 ( 99.95)
Epoch: [34][180/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5454e-01 (1.2896e-01)	Acc@1  92.97 ( 95.53)	Acc@5 100.00 ( 99.95)
Epoch: [34][190/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.1544e-02 (1.2889e-01)	Acc@1  99.22 ( 95.51)	Acc@5 100.00 ( 99.94)
Epoch: [34][200/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0596e-01 (1.2820e-01)	Acc@1  96.88 ( 95.52)	Acc@5 100.00 ( 99.95)
Epoch: [34][210/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2292e-01 (1.2875e-01)	Acc@1  95.31 ( 95.50)	Acc@5 100.00 ( 99.95)
Epoch: [34][220/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.7615e-01 (1.2903e-01)	Acc@1  91.41 ( 95.46)	Acc@5 100.00 ( 99.95)
Epoch: [34][230/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.6382e-01 (1.2878e-01)	Acc@1  92.97 ( 95.45)	Acc@5 100.00 ( 99.95)
Epoch: [34][240/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.6907e-01 (1.2935e-01)	Acc@1  94.53 ( 95.41)	Acc@5 100.00 ( 99.94)
Epoch: [34][250/391]	Time  0.107 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.7432e-01 (1.2984e-01)	Acc@1  91.41 ( 95.40)	Acc@5 100.00 ( 99.95)
Epoch: [34][260/391]	Time  0.108 ( 0.108)	Data  0.001 ( 0.005)	Loss 8.7036e-02 (1.2960e-01)	Acc@1  97.66 ( 95.41)	Acc@5 100.00 ( 99.95)
Epoch: [34][270/391]	Time  0.108 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.1133e-01 (1.2934e-01)	Acc@1  95.31 ( 95.42)	Acc@5 100.00 ( 99.95)
Epoch: [34][280/391]	Time  0.106 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.1322e-01 (1.2969e-01)	Acc@1  96.88 ( 95.39)	Acc@5 100.00 ( 99.95)
Epoch: [34][290/391]	Time  0.105 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.2445e-01 (1.2979e-01)	Acc@1  94.53 ( 95.35)	Acc@5 100.00 ( 99.95)
Epoch: [34][300/391]	Time  0.109 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.5088e-01 (1.3051e-01)	Acc@1  95.31 ( 95.32)	Acc@5 100.00 ( 99.96)
Epoch: [34][310/391]	Time  0.106 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.6394e-01 (1.3015e-01)	Acc@1  93.75 ( 95.33)	Acc@5 100.00 ( 99.96)
Epoch: [34][320/391]	Time  0.106 ( 0.108)	Data  0.001 ( 0.005)	Loss 7.2632e-02 (1.2983e-01)	Acc@1  99.22 ( 95.35)	Acc@5 100.00 ( 99.96)
Epoch: [34][330/391]	Time  0.104 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.2781e-01 (1.2893e-01)	Acc@1  96.88 ( 95.37)	Acc@5 100.00 ( 99.96)
Epoch: [34][340/391]	Time  0.108 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.2476e-01 (1.2905e-01)	Acc@1  96.88 ( 95.37)	Acc@5 100.00 ( 99.96)
Epoch: [34][350/391]	Time  0.108 ( 0.108)	Data  0.001 ( 0.005)	Loss 8.2520e-02 (1.2933e-01)	Acc@1  97.66 ( 95.38)	Acc@5 100.00 ( 99.96)
Epoch: [34][360/391]	Time  0.109 ( 0.108)	Data  0.001 ( 0.005)	Loss 9.7595e-02 (1.2967e-01)	Acc@1  97.66 ( 95.38)	Acc@5 100.00 ( 99.96)
Epoch: [34][370/391]	Time  0.103 ( 0.108)	Data  0.001 ( 0.005)	Loss 8.7830e-02 (1.2977e-01)	Acc@1  96.88 ( 95.38)	Acc@5 100.00 ( 99.96)
Epoch: [34][380/391]	Time  0.105 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.3831e-01 (1.2950e-01)	Acc@1  93.75 ( 95.38)	Acc@5 100.00 ( 99.96)
Epoch: [34][390/391]	Time  0.098 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.0815e-01 (1.2951e-01)	Acc@1  97.50 ( 95.39)	Acc@5 100.00 ( 99.96)
## e[34] optimizer.zero_grad (sum) time: 0.3521428108215332
## e[34]       loss.backward (sum) time: 13.726134300231934
## e[34]      optimizer.step (sum) time: 2.7589662075042725
## epoch[34] training(only) time: 42.49914479255676
# Switched to evaluate mode...
Test: [  0/100]	Time  0.239 ( 0.239)	Loss 2.6025e-01 (2.6025e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.037 ( 0.056)	Loss 1.8921e-01 (2.7578e-01)	Acc@1  94.00 ( 91.36)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.036 ( 0.047)	Loss 4.2236e-01 (3.0791e-01)	Acc@1  86.00 ( 90.62)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.037 ( 0.044)	Loss 2.8613e-01 (3.2960e-01)	Acc@1  89.00 ( 90.65)	Acc@5 100.00 ( 99.61)
Test: [ 40/100]	Time  0.037 ( 0.042)	Loss 2.7051e-01 (3.3311e-01)	Acc@1  92.00 ( 90.46)	Acc@5  99.00 ( 99.61)
Test: [ 50/100]	Time  0.035 ( 0.041)	Loss 1.7590e-01 (3.2651e-01)	Acc@1  93.00 ( 90.63)	Acc@5 100.00 ( 99.59)
Test: [ 60/100]	Time  0.037 ( 0.041)	Loss 3.1445e-01 (3.1952e-01)	Acc@1  93.00 ( 90.67)	Acc@5  99.00 ( 99.62)
Test: [ 70/100]	Time  0.036 ( 0.040)	Loss 4.2383e-01 (3.1542e-01)	Acc@1  89.00 ( 90.61)	Acc@5 100.00 ( 99.65)
Test: [ 80/100]	Time  0.039 ( 0.040)	Loss 3.2715e-01 (3.1866e-01)	Acc@1  85.00 ( 90.42)	Acc@5 100.00 ( 99.65)
Test: [ 90/100]	Time  0.036 ( 0.039)	Loss 1.9519e-01 (3.1273e-01)	Acc@1  93.00 ( 90.54)	Acc@5 100.00 ( 99.67)
 * Acc@1 90.480 Acc@5 99.690
### epoch[34] execution time: 46.50963854789734
EPOCH 35
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [35][  0/391]	Time  0.275 ( 0.275)	Data  0.179 ( 0.179)	Loss 1.1005e-01 (1.1005e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [35][ 10/391]	Time  0.099 ( 0.123)	Data  0.001 ( 0.020)	Loss 1.3989e-01 (1.3182e-01)	Acc@1  93.75 ( 95.60)	Acc@5 100.00 ( 99.93)
Epoch: [35][ 20/391]	Time  0.105 ( 0.116)	Data  0.001 ( 0.012)	Loss 1.3977e-01 (1.3386e-01)	Acc@1  96.09 ( 95.76)	Acc@5 100.00 ( 99.89)
Epoch: [35][ 30/391]	Time  0.105 ( 0.113)	Data  0.001 ( 0.010)	Loss 6.7566e-02 (1.2745e-01)	Acc@1  97.66 ( 95.84)	Acc@5 100.00 ( 99.92)
Epoch: [35][ 40/391]	Time  0.106 ( 0.112)	Data  0.002 ( 0.008)	Loss 1.2097e-01 (1.2769e-01)	Acc@1  95.31 ( 95.66)	Acc@5 100.00 ( 99.92)
Epoch: [35][ 50/391]	Time  0.108 ( 0.111)	Data  0.001 ( 0.008)	Loss 1.1890e-01 (1.2280e-01)	Acc@1  95.31 ( 95.86)	Acc@5 100.00 ( 99.92)
Epoch: [35][ 60/391]	Time  0.110 ( 0.111)	Data  0.001 ( 0.007)	Loss 1.0742e-01 (1.2148e-01)	Acc@1  95.31 ( 95.84)	Acc@5 100.00 ( 99.92)
Epoch: [35][ 70/391]	Time  0.109 ( 0.110)	Data  0.001 ( 0.007)	Loss 9.3018e-02 (1.1868e-01)	Acc@1  94.53 ( 95.90)	Acc@5 100.00 ( 99.93)
Epoch: [35][ 80/391]	Time  0.112 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.6785e-01 (1.1928e-01)	Acc@1  92.97 ( 95.82)	Acc@5 100.00 ( 99.94)
Epoch: [35][ 90/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.7310e-01 (1.2047e-01)	Acc@1  95.31 ( 95.84)	Acc@5 100.00 ( 99.94)
Epoch: [35][100/391]	Time  0.112 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.0065e-01 (1.2148e-01)	Acc@1  96.88 ( 95.81)	Acc@5 100.00 ( 99.94)
Epoch: [35][110/391]	Time  0.113 ( 0.110)	Data  0.001 ( 0.006)	Loss 4.4159e-02 (1.2148e-01)	Acc@1  99.22 ( 95.77)	Acc@5 100.00 ( 99.94)
Epoch: [35][120/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.6235e-01 (1.2380e-01)	Acc@1  94.53 ( 95.71)	Acc@5  99.22 ( 99.93)
Epoch: [35][130/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.005)	Loss 9.7412e-02 (1.2445e-01)	Acc@1  97.66 ( 95.72)	Acc@5 100.00 ( 99.93)
Epoch: [35][140/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2549e-01 (1.2346e-01)	Acc@1  96.09 ( 95.76)	Acc@5 100.00 ( 99.93)
Epoch: [35][150/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0352e-01 (1.2421e-01)	Acc@1  97.66 ( 95.73)	Acc@5 100.00 ( 99.94)
Epoch: [35][160/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2329e-01 (1.2437e-01)	Acc@1  96.09 ( 95.72)	Acc@5 100.00 ( 99.94)
Epoch: [35][170/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.9417e-02 (1.2446e-01)	Acc@1  95.31 ( 95.69)	Acc@5 100.00 ( 99.95)
Epoch: [35][180/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.6223e-01 (1.2408e-01)	Acc@1  92.97 ( 95.66)	Acc@5 100.00 ( 99.95)
Epoch: [35][190/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.5867e-02 (1.2368e-01)	Acc@1  97.66 ( 95.68)	Acc@5 100.00 ( 99.95)
Epoch: [35][200/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1169e-01 (1.2360e-01)	Acc@1  96.09 ( 95.70)	Acc@5 100.00 ( 99.95)
Epoch: [35][210/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2854e-01 (1.2408e-01)	Acc@1  96.88 ( 95.68)	Acc@5 100.00 ( 99.95)
Epoch: [35][220/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1359e-01 (1.2403e-01)	Acc@1  94.53 ( 95.68)	Acc@5 100.00 ( 99.95)
Epoch: [35][230/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.9182e-02 (1.2463e-01)	Acc@1  94.53 ( 95.66)	Acc@5 100.00 ( 99.95)
Epoch: [35][240/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.2754e-02 (1.2448e-01)	Acc@1  97.66 ( 95.66)	Acc@5 100.00 ( 99.95)
Epoch: [35][250/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4600e-01 (1.2512e-01)	Acc@1  93.75 ( 95.62)	Acc@5 100.00 ( 99.95)
Epoch: [35][260/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.3506e-02 (1.2493e-01)	Acc@1  96.88 ( 95.65)	Acc@5 100.00 ( 99.96)
Epoch: [35][270/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1530e-01 (1.2469e-01)	Acc@1  94.53 ( 95.65)	Acc@5 100.00 ( 99.96)
Epoch: [35][280/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2189e-01 (1.2476e-01)	Acc@1  94.53 ( 95.64)	Acc@5 100.00 ( 99.96)
Epoch: [35][290/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3708e-01 (1.2441e-01)	Acc@1  96.09 ( 95.63)	Acc@5 100.00 ( 99.96)
Epoch: [35][300/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3245e-01 (1.2386e-01)	Acc@1  96.09 ( 95.67)	Acc@5 100.00 ( 99.96)
Epoch: [35][310/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.5012e-02 (1.2405e-01)	Acc@1  98.44 ( 95.67)	Acc@5 100.00 ( 99.95)
Epoch: [35][320/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0754e-01 (1.2393e-01)	Acc@1  97.66 ( 95.65)	Acc@5 100.00 ( 99.96)
Epoch: [35][330/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0901e-01 (1.2419e-01)	Acc@1  96.09 ( 95.66)	Acc@5 100.00 ( 99.96)
Epoch: [35][340/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0944e-01 (1.2413e-01)	Acc@1  97.66 ( 95.67)	Acc@5 100.00 ( 99.96)
Epoch: [35][350/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.9600e-02 (1.2407e-01)	Acc@1  96.09 ( 95.66)	Acc@5 100.00 ( 99.96)
Epoch: [35][360/391]	Time  0.100 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.6455e-01 (1.2452e-01)	Acc@1  94.53 ( 95.63)	Acc@5 100.00 ( 99.96)
Epoch: [35][370/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4429e-01 (1.2464e-01)	Acc@1  96.09 ( 95.62)	Acc@5 100.00 ( 99.96)
Epoch: [35][380/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3867e-01 (1.2504e-01)	Acc@1  94.53 ( 95.60)	Acc@5 100.00 ( 99.96)
Epoch: [35][390/391]	Time  0.098 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0248e-01 (1.2497e-01)	Acc@1  96.25 ( 95.62)	Acc@5 100.00 ( 99.96)
## e[35] optimizer.zero_grad (sum) time: 0.34699273109436035
## e[35]       loss.backward (sum) time: 13.714577436447144
## e[35]      optimizer.step (sum) time: 2.7671926021575928
## epoch[35] training(only) time: 42.68931245803833
# Switched to evaluate mode...
Test: [  0/100]	Time  0.224 ( 0.224)	Loss 2.2131e-01 (2.2131e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.038 ( 0.054)	Loss 1.6895e-01 (2.7454e-01)	Acc@1  95.00 ( 91.36)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.037 ( 0.046)	Loss 4.1846e-01 (3.1010e-01)	Acc@1  86.00 ( 90.43)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.037 ( 0.043)	Loss 2.8296e-01 (3.3361e-01)	Acc@1  90.00 ( 90.48)	Acc@5 100.00 ( 99.61)
Test: [ 40/100]	Time  0.039 ( 0.042)	Loss 2.6685e-01 (3.3568e-01)	Acc@1  92.00 ( 90.56)	Acc@5  99.00 ( 99.59)
Test: [ 50/100]	Time  0.036 ( 0.041)	Loss 1.7603e-01 (3.3035e-01)	Acc@1  95.00 ( 90.71)	Acc@5 100.00 ( 99.61)
Test: [ 60/100]	Time  0.036 ( 0.040)	Loss 3.9038e-01 (3.2427e-01)	Acc@1  92.00 ( 90.75)	Acc@5  99.00 ( 99.66)
Test: [ 70/100]	Time  0.037 ( 0.040)	Loss 4.2847e-01 (3.2174e-01)	Acc@1  90.00 ( 90.73)	Acc@5 100.00 ( 99.68)
Test: [ 80/100]	Time  0.036 ( 0.039)	Loss 3.1274e-01 (3.2287e-01)	Acc@1  87.00 ( 90.62)	Acc@5 100.00 ( 99.65)
Test: [ 90/100]	Time  0.038 ( 0.039)	Loss 1.7847e-01 (3.1578e-01)	Acc@1  93.00 ( 90.67)	Acc@5 100.00 ( 99.67)
 * Acc@1 90.570 Acc@5 99.690
### epoch[35] execution time: 46.685933351516724
EPOCH 36
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [36][  0/391]	Time  0.273 ( 0.273)	Data  0.179 ( 0.179)	Loss 1.3062e-01 (1.3062e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
Epoch: [36][ 10/391]	Time  0.105 ( 0.122)	Data  0.001 ( 0.020)	Loss 1.1884e-01 (1.1075e-01)	Acc@1  95.31 ( 96.09)	Acc@5  99.22 ( 99.93)
Epoch: [36][ 20/391]	Time  0.106 ( 0.115)	Data  0.001 ( 0.012)	Loss 1.9043e-01 (1.1466e-01)	Acc@1  92.19 ( 96.02)	Acc@5 100.00 ( 99.93)
Epoch: [36][ 30/391]	Time  0.105 ( 0.113)	Data  0.001 ( 0.010)	Loss 1.1914e-01 (1.1217e-01)	Acc@1  96.88 ( 95.97)	Acc@5  99.22 ( 99.92)
Epoch: [36][ 40/391]	Time  0.104 ( 0.112)	Data  0.001 ( 0.008)	Loss 1.7261e-01 (1.1440e-01)	Acc@1  95.31 ( 95.85)	Acc@5 100.00 ( 99.94)
Epoch: [36][ 50/391]	Time  0.112 ( 0.111)	Data  0.001 ( 0.008)	Loss 5.8594e-02 (1.1381e-01)	Acc@1  96.88 ( 95.76)	Acc@5 100.00 ( 99.95)
Epoch: [36][ 60/391]	Time  0.107 ( 0.111)	Data  0.001 ( 0.007)	Loss 1.7700e-01 (1.1552e-01)	Acc@1  92.19 ( 95.63)	Acc@5 100.00 ( 99.96)
Epoch: [36][ 70/391]	Time  0.107 ( 0.110)	Data  0.002 ( 0.007)	Loss 1.4783e-01 (1.1401e-01)	Acc@1  93.75 ( 95.77)	Acc@5 100.00 ( 99.94)
Epoch: [36][ 80/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 9.8816e-02 (1.1520e-01)	Acc@1  96.88 ( 95.79)	Acc@5 100.00 ( 99.94)
Epoch: [36][ 90/391]	Time  0.106 ( 0.110)	Data  0.002 ( 0.006)	Loss 1.4978e-01 (1.1641e-01)	Acc@1  94.53 ( 95.78)	Acc@5 100.00 ( 99.94)
Epoch: [36][100/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.006)	Loss 1.0126e-01 (1.1561e-01)	Acc@1  96.88 ( 95.85)	Acc@5 100.00 ( 99.95)
Epoch: [36][110/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.006)	Loss 1.5479e-01 (1.1802e-01)	Acc@1  94.53 ( 95.76)	Acc@5 100.00 ( 99.95)
Epoch: [36][120/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.006)	Loss 1.6028e-01 (1.1891e-01)	Acc@1  93.75 ( 95.73)	Acc@5 100.00 ( 99.95)
Epoch: [36][130/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.6492e-01 (1.1909e-01)	Acc@1  93.75 ( 95.75)	Acc@5 100.00 ( 99.94)
Epoch: [36][140/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2134e-01 (1.1948e-01)	Acc@1  96.09 ( 95.74)	Acc@5 100.00 ( 99.94)
Epoch: [36][150/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3940e-01 (1.1940e-01)	Acc@1  94.53 ( 95.76)	Acc@5 100.00 ( 99.94)
Epoch: [36][160/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1334e-01 (1.1966e-01)	Acc@1  95.31 ( 95.77)	Acc@5 100.00 ( 99.95)
Epoch: [36][170/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.4413e-02 (1.1944e-01)	Acc@1  97.66 ( 95.81)	Acc@5 100.00 ( 99.95)
Epoch: [36][180/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3123e-01 (1.1913e-01)	Acc@1  95.31 ( 95.82)	Acc@5 100.00 ( 99.94)
Epoch: [36][190/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2561e-01 (1.1957e-01)	Acc@1  96.09 ( 95.80)	Acc@5 100.00 ( 99.94)
Epoch: [36][200/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.7341e-02 (1.1857e-01)	Acc@1  97.66 ( 95.84)	Acc@5 100.00 ( 99.95)
Epoch: [36][210/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5454e-01 (1.1840e-01)	Acc@1  94.53 ( 95.88)	Acc@5 100.00 ( 99.94)
Epoch: [36][220/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0852e-01 (1.1844e-01)	Acc@1  96.88 ( 95.89)	Acc@5 100.00 ( 99.94)
Epoch: [36][230/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3477e-01 (1.1940e-01)	Acc@1  95.31 ( 95.85)	Acc@5 100.00 ( 99.95)
Epoch: [36][240/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0010e-01 (1.1917e-01)	Acc@1  97.66 ( 95.86)	Acc@5 100.00 ( 99.95)
Epoch: [36][250/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.0215e-01 (1.1981e-01)	Acc@1  95.31 ( 95.87)	Acc@5  99.22 ( 99.95)
Epoch: [36][260/391]	Time  0.105 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.4856e-01 (1.2046e-01)	Acc@1  94.53 ( 95.83)	Acc@5 100.00 ( 99.95)
Epoch: [36][270/391]	Time  0.107 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.8616e-01 (1.1985e-01)	Acc@1  94.53 ( 95.86)	Acc@5 100.00 ( 99.95)
Epoch: [36][280/391]	Time  0.110 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.6382e-01 (1.1972e-01)	Acc@1  91.41 ( 95.84)	Acc@5 100.00 ( 99.95)
Epoch: [36][290/391]	Time  0.106 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.5356e-01 (1.1992e-01)	Acc@1  93.75 ( 95.85)	Acc@5 100.00 ( 99.95)
Epoch: [36][300/391]	Time  0.108 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.6321e-01 (1.1941e-01)	Acc@1  95.31 ( 95.87)	Acc@5 100.00 ( 99.95)
Epoch: [36][310/391]	Time  0.106 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.2415e-01 (1.1939e-01)	Acc@1  95.31 ( 95.88)	Acc@5 100.00 ( 99.95)
Epoch: [36][320/391]	Time  0.105 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.5356e-01 (1.1930e-01)	Acc@1  93.75 ( 95.87)	Acc@5 100.00 ( 99.95)
Epoch: [36][330/391]	Time  0.105 ( 0.108)	Data  0.001 ( 0.005)	Loss 5.8228e-02 (1.1899e-01)	Acc@1  99.22 ( 95.88)	Acc@5 100.00 ( 99.96)
Epoch: [36][340/391]	Time  0.103 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.0181e-01 (1.1838e-01)	Acc@1  94.53 ( 95.90)	Acc@5 100.00 ( 99.96)
Epoch: [36][350/391]	Time  0.106 ( 0.108)	Data  0.001 ( 0.005)	Loss 2.1924e-01 (1.1866e-01)	Acc@1  89.84 ( 95.90)	Acc@5 100.00 ( 99.96)
Epoch: [36][360/391]	Time  0.104 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.3611e-01 (1.1842e-01)	Acc@1  93.75 ( 95.91)	Acc@5 100.00 ( 99.96)
Epoch: [36][370/391]	Time  0.107 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.2988e-01 (1.1815e-01)	Acc@1  96.88 ( 95.92)	Acc@5  99.22 ( 99.96)
Epoch: [36][380/391]	Time  0.105 ( 0.108)	Data  0.001 ( 0.005)	Loss 8.6121e-02 (1.1821e-01)	Acc@1  96.88 ( 95.92)	Acc@5  99.22 ( 99.95)
Epoch: [36][390/391]	Time  0.101 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.3037e-01 (1.1812e-01)	Acc@1  95.00 ( 95.92)	Acc@5 100.00 ( 99.95)
## e[36] optimizer.zero_grad (sum) time: 0.35933780670166016
## e[36]       loss.backward (sum) time: 13.717974424362183
## e[36]      optimizer.step (sum) time: 2.747347116470337
## epoch[36] training(only) time: 42.49395728111267
# Switched to evaluate mode...
Test: [  0/100]	Time  0.191 ( 0.191)	Loss 2.0544e-01 (2.0544e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.037 ( 0.051)	Loss 2.1716e-01 (2.7371e-01)	Acc@1  93.00 ( 91.73)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.036 ( 0.044)	Loss 4.3115e-01 (3.0944e-01)	Acc@1  87.00 ( 90.81)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.036 ( 0.042)	Loss 2.5195e-01 (3.3016e-01)	Acc@1  88.00 ( 90.68)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.036 ( 0.041)	Loss 2.5903e-01 (3.3582e-01)	Acc@1  92.00 ( 90.61)	Acc@5  99.00 ( 99.63)
Test: [ 50/100]	Time  0.036 ( 0.040)	Loss 1.7566e-01 (3.3200e-01)	Acc@1  94.00 ( 90.78)	Acc@5 100.00 ( 99.59)
Test: [ 60/100]	Time  0.037 ( 0.040)	Loss 3.8647e-01 (3.2475e-01)	Acc@1  94.00 ( 90.92)	Acc@5  99.00 ( 99.64)
Test: [ 70/100]	Time  0.037 ( 0.039)	Loss 4.7266e-01 (3.2304e-01)	Acc@1  89.00 ( 90.89)	Acc@5 100.00 ( 99.65)
Test: [ 80/100]	Time  0.037 ( 0.039)	Loss 3.1421e-01 (3.2502e-01)	Acc@1  88.00 ( 90.75)	Acc@5 100.00 ( 99.65)
Test: [ 90/100]	Time  0.044 ( 0.039)	Loss 1.9336e-01 (3.1805e-01)	Acc@1  96.00 ( 90.84)	Acc@5 100.00 ( 99.67)
 * Acc@1 90.790 Acc@5 99.690
### epoch[36] execution time: 46.45222187042236
EPOCH 37
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [37][  0/391]	Time  0.285 ( 0.285)	Data  0.183 ( 0.183)	Loss 8.4900e-02 (8.4900e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [37][ 10/391]	Time  0.111 ( 0.125)	Data  0.001 ( 0.020)	Loss 6.6772e-02 (1.1677e-01)	Acc@1  96.88 ( 95.60)	Acc@5 100.00 (100.00)
Epoch: [37][ 20/391]	Time  0.106 ( 0.117)	Data  0.001 ( 0.012)	Loss 5.6213e-02 (1.0342e-01)	Acc@1  98.44 ( 96.32)	Acc@5 100.00 (100.00)
Epoch: [37][ 30/391]	Time  0.105 ( 0.114)	Data  0.001 ( 0.010)	Loss 1.5564e-01 (1.0379e-01)	Acc@1  94.53 ( 96.24)	Acc@5 100.00 ( 99.97)
Epoch: [37][ 40/391]	Time  0.106 ( 0.112)	Data  0.001 ( 0.008)	Loss 4.1565e-02 (9.9806e-02)	Acc@1  99.22 ( 96.51)	Acc@5 100.00 ( 99.98)
Epoch: [37][ 50/391]	Time  0.104 ( 0.111)	Data  0.001 ( 0.008)	Loss 9.7290e-02 (1.0196e-01)	Acc@1  95.31 ( 96.40)	Acc@5 100.00 ( 99.98)
Epoch: [37][ 60/391]	Time  0.107 ( 0.111)	Data  0.001 ( 0.007)	Loss 1.1548e-01 (1.0345e-01)	Acc@1  96.09 ( 96.32)	Acc@5 100.00 ( 99.99)
Epoch: [37][ 70/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.007)	Loss 9.4543e-02 (1.0487e-01)	Acc@1  96.09 ( 96.25)	Acc@5 100.00 ( 99.99)
Epoch: [37][ 80/391]	Time  0.109 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.0352e-01 (1.0484e-01)	Acc@1  97.66 ( 96.23)	Acc@5 100.00 ( 99.99)
Epoch: [37][ 90/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.6016e-01 (1.0421e-01)	Acc@1  95.31 ( 96.27)	Acc@5  99.22 ( 99.98)
Epoch: [37][100/391]	Time  0.104 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.3110e-01 (1.0531e-01)	Acc@1  96.09 ( 96.23)	Acc@5 100.00 ( 99.98)
Epoch: [37][110/391]	Time  0.110 ( 0.110)	Data  0.001 ( 0.006)	Loss 5.5847e-02 (1.0579e-01)	Acc@1  99.22 ( 96.23)	Acc@5 100.00 ( 99.99)
Epoch: [37][120/391]	Time  0.109 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.7712e-01 (1.0791e-01)	Acc@1  93.75 ( 96.15)	Acc@5  99.22 ( 99.98)
Epoch: [37][130/391]	Time  0.110 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.3708e-01 (1.0972e-01)	Acc@1  96.88 ( 96.12)	Acc@5 100.00 ( 99.98)
Epoch: [37][140/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.005)	Loss 8.6304e-02 (1.0900e-01)	Acc@1  96.88 ( 96.17)	Acc@5 100.00 ( 99.98)
Epoch: [37][150/391]	Time  0.107 ( 0.109)	Data  0.002 ( 0.005)	Loss 1.4294e-01 (1.0850e-01)	Acc@1  95.31 ( 96.22)	Acc@5  99.22 ( 99.97)
Epoch: [37][160/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.7969e-01 (1.0876e-01)	Acc@1  91.41 ( 96.20)	Acc@5 100.00 ( 99.97)
Epoch: [37][170/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.0627e-02 (1.0854e-01)	Acc@1  96.88 ( 96.20)	Acc@5 100.00 ( 99.97)
Epoch: [37][180/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0211e-01 (1.0865e-01)	Acc@1  96.09 ( 96.20)	Acc@5 100.00 ( 99.97)
Epoch: [37][190/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5405e-01 (1.0873e-01)	Acc@1  91.41 ( 96.19)	Acc@5 100.00 ( 99.97)
Epoch: [37][200/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1853e-01 (1.0851e-01)	Acc@1  95.31 ( 96.19)	Acc@5  99.22 ( 99.97)
Epoch: [37][210/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.6304e-02 (1.0861e-01)	Acc@1  96.88 ( 96.18)	Acc@5 100.00 ( 99.96)
Epoch: [37][220/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1871e-01 (1.0876e-01)	Acc@1  95.31 ( 96.15)	Acc@5 100.00 ( 99.96)
Epoch: [37][230/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.7932e-01 (1.0962e-01)	Acc@1  92.97 ( 96.11)	Acc@5 100.00 ( 99.97)
Epoch: [37][240/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5149e-01 (1.1010e-01)	Acc@1  93.75 ( 96.07)	Acc@5 100.00 ( 99.97)
Epoch: [37][250/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.3496e-02 (1.0970e-01)	Acc@1  96.88 ( 96.08)	Acc@5 100.00 ( 99.97)
Epoch: [37][260/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.7402e-02 (1.0923e-01)	Acc@1  96.09 ( 96.11)	Acc@5 100.00 ( 99.97)
Epoch: [37][270/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5710e-01 (1.0914e-01)	Acc@1  95.31 ( 96.12)	Acc@5 100.00 ( 99.97)
Epoch: [37][280/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.7798e-01 (1.0940e-01)	Acc@1  92.19 ( 96.11)	Acc@5 100.00 ( 99.97)
Epoch: [37][290/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1627e-01 (1.0952e-01)	Acc@1  94.53 ( 96.10)	Acc@5 100.00 ( 99.97)
Epoch: [37][300/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5430e-01 (1.1023e-01)	Acc@1  96.88 ( 96.08)	Acc@5 100.00 ( 99.97)
Epoch: [37][310/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4343e-01 (1.1037e-01)	Acc@1  93.75 ( 96.07)	Acc@5 100.00 ( 99.97)
Epoch: [37][320/391]	Time  0.111 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.5510e-02 (1.1033e-01)	Acc@1  96.88 ( 96.09)	Acc@5 100.00 ( 99.97)
Epoch: [37][330/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0455e-01 (1.1048e-01)	Acc@1  96.88 ( 96.10)	Acc@5 100.00 ( 99.97)
Epoch: [37][340/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.6345e-02 (1.1080e-01)	Acc@1  99.22 ( 96.08)	Acc@5 100.00 ( 99.97)
Epoch: [37][350/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.2229e-01 (1.1174e-01)	Acc@1  92.19 ( 96.06)	Acc@5  99.22 ( 99.97)
Epoch: [37][360/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.3182e-02 (1.1162e-01)	Acc@1  99.22 ( 96.06)	Acc@5 100.00 ( 99.97)
Epoch: [37][370/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0699e-01 (1.1175e-01)	Acc@1  96.88 ( 96.05)	Acc@5 100.00 ( 99.97)
Epoch: [37][380/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.1960e-02 (1.1158e-01)	Acc@1  96.09 ( 96.05)	Acc@5 100.00 ( 99.97)
Epoch: [37][390/391]	Time  0.100 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.7585e-02 (1.1187e-01)	Acc@1  97.50 ( 96.04)	Acc@5 100.00 ( 99.97)
## e[37] optimizer.zero_grad (sum) time: 0.35208916664123535
## e[37]       loss.backward (sum) time: 13.706119537353516
## e[37]      optimizer.step (sum) time: 2.8166091442108154
## epoch[37] training(only) time: 42.74001741409302
# Switched to evaluate mode...
Test: [  0/100]	Time  0.197 ( 0.197)	Loss 2.0667e-01 (2.0667e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.036 ( 0.052)	Loss 1.5979e-01 (2.8409e-01)	Acc@1  96.00 ( 91.45)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.037 ( 0.045)	Loss 4.5557e-01 (3.2496e-01)	Acc@1  86.00 ( 90.38)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.036 ( 0.042)	Loss 3.4448e-01 (3.4072e-01)	Acc@1  89.00 ( 90.68)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.037 ( 0.041)	Loss 3.0005e-01 (3.4535e-01)	Acc@1  92.00 ( 90.73)	Acc@5  99.00 ( 99.61)
Test: [ 50/100]	Time  0.036 ( 0.040)	Loss 1.4319e-01 (3.3828e-01)	Acc@1  96.00 ( 90.84)	Acc@5 100.00 ( 99.63)
Test: [ 60/100]	Time  0.036 ( 0.040)	Loss 3.6694e-01 (3.3100e-01)	Acc@1  94.00 ( 90.92)	Acc@5  99.00 ( 99.66)
Test: [ 70/100]	Time  0.036 ( 0.039)	Loss 4.8950e-01 (3.2820e-01)	Acc@1  88.00 ( 90.79)	Acc@5 100.00 ( 99.68)
Test: [ 80/100]	Time  0.036 ( 0.039)	Loss 2.4536e-01 (3.2781e-01)	Acc@1  89.00 ( 90.67)	Acc@5 100.00 ( 99.68)
Test: [ 90/100]	Time  0.037 ( 0.039)	Loss 2.3157e-01 (3.2244e-01)	Acc@1  95.00 ( 90.77)	Acc@5 100.00 ( 99.69)
 * Acc@1 90.710 Acc@5 99.710
### epoch[37] execution time: 46.70344614982605
EPOCH 38
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [38][  0/391]	Time  0.267 ( 0.267)	Data  0.164 ( 0.164)	Loss 8.5754e-02 (8.5754e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [38][ 10/391]	Time  0.109 ( 0.122)	Data  0.001 ( 0.018)	Loss 8.1299e-02 (9.0096e-02)	Acc@1  96.09 ( 96.95)	Acc@5 100.00 (100.00)
Epoch: [38][ 20/391]	Time  0.107 ( 0.116)	Data  0.001 ( 0.012)	Loss 1.2451e-01 (1.0418e-01)	Acc@1  95.31 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [38][ 30/391]	Time  0.104 ( 0.113)	Data  0.001 ( 0.009)	Loss 1.5271e-01 (9.9943e-02)	Acc@1  94.53 ( 96.32)	Acc@5 100.00 (100.00)
Epoch: [38][ 40/391]	Time  0.104 ( 0.112)	Data  0.001 ( 0.008)	Loss 3.4515e-02 (9.6578e-02)	Acc@1  99.22 ( 96.42)	Acc@5 100.00 (100.00)
Epoch: [38][ 50/391]	Time  0.106 ( 0.111)	Data  0.001 ( 0.007)	Loss 1.5588e-01 (1.0073e-01)	Acc@1  92.97 ( 96.31)	Acc@5 100.00 (100.00)
Epoch: [38][ 60/391]	Time  0.105 ( 0.111)	Data  0.001 ( 0.007)	Loss 1.3428e-01 (1.0055e-01)	Acc@1  96.09 ( 96.29)	Acc@5 100.00 (100.00)
Epoch: [38][ 70/391]	Time  0.110 ( 0.111)	Data  0.001 ( 0.006)	Loss 6.5979e-02 (1.0018e-01)	Acc@1  98.44 ( 96.38)	Acc@5 100.00 ( 99.99)
Epoch: [38][ 80/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.3037e-01 (9.8461e-02)	Acc@1  95.31 ( 96.45)	Acc@5  99.22 ( 99.98)
Epoch: [38][ 90/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.3672e-01 (9.9341e-02)	Acc@1  94.53 ( 96.41)	Acc@5 100.00 ( 99.98)
Epoch: [38][100/391]	Time  0.111 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.6663e-01 (9.9416e-02)	Acc@1  94.53 ( 96.39)	Acc@5 100.00 ( 99.98)
Epoch: [38][110/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.2170e-01 (1.0084e-01)	Acc@1  96.09 ( 96.33)	Acc@5 100.00 ( 99.99)
Epoch: [38][120/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.4563e-01 (1.0169e-01)	Acc@1  96.09 ( 96.32)	Acc@5  99.22 ( 99.98)
Epoch: [38][130/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.005)	Loss 7.9346e-02 (1.0268e-01)	Acc@1  96.88 ( 96.31)	Acc@5 100.00 ( 99.97)
Epoch: [38][140/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.005)	Loss 6.5369e-02 (1.0297e-01)	Acc@1  99.22 ( 96.34)	Acc@5 100.00 ( 99.97)
Epoch: [38][150/391]	Time  0.103 ( 0.110)	Data  0.001 ( 0.005)	Loss 6.9153e-02 (1.0193e-01)	Acc@1  96.88 ( 96.38)	Acc@5 100.00 ( 99.96)
Epoch: [38][160/391]	Time  0.113 ( 0.110)	Data  0.001 ( 0.005)	Loss 8.3618e-02 (1.0097e-01)	Acc@1  96.88 ( 96.42)	Acc@5 100.00 ( 99.97)
Epoch: [38][170/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.005)	Loss 6.8787e-02 (1.0140e-01)	Acc@1  97.66 ( 96.40)	Acc@5 100.00 ( 99.96)
Epoch: [38][180/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5320e-01 (1.0191e-01)	Acc@1  93.75 ( 96.37)	Acc@5 100.00 ( 99.97)
Epoch: [38][190/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.3284e-02 (1.0254e-01)	Acc@1  99.22 ( 96.37)	Acc@5 100.00 ( 99.96)
Epoch: [38][200/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0614e-01 (1.0316e-01)	Acc@1  96.88 ( 96.34)	Acc@5 100.00 ( 99.97)
Epoch: [38][210/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.4604e-02 (1.0365e-01)	Acc@1  96.09 ( 96.33)	Acc@5 100.00 ( 99.97)
Epoch: [38][220/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.7585e-02 (1.0348e-01)	Acc@1  96.88 ( 96.32)	Acc@5 100.00 ( 99.97)
Epoch: [38][230/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0498e-01 (1.0372e-01)	Acc@1  96.88 ( 96.32)	Acc@5 100.00 ( 99.97)
Epoch: [38][240/391]	Time  0.101 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1371e-01 (1.0408e-01)	Acc@1  96.88 ( 96.30)	Acc@5 100.00 ( 99.97)
Epoch: [38][250/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2299e-01 (1.0405e-01)	Acc@1  92.19 ( 96.29)	Acc@5 100.00 ( 99.97)
Epoch: [38][260/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3708e-01 (1.0509e-01)	Acc@1  92.97 ( 96.26)	Acc@5 100.00 ( 99.97)
Epoch: [38][270/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.0139e-02 (1.0503e-01)	Acc@1  97.66 ( 96.27)	Acc@5 100.00 ( 99.97)
Epoch: [38][280/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4319e-01 (1.0511e-01)	Acc@1  96.09 ( 96.27)	Acc@5 100.00 ( 99.97)
Epoch: [38][290/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.2195e-02 (1.0436e-01)	Acc@1  97.66 ( 96.30)	Acc@5 100.00 ( 99.97)
Epoch: [38][300/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1835e-01 (1.0425e-01)	Acc@1  93.75 ( 96.30)	Acc@5 100.00 ( 99.97)
Epoch: [38][310/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.7029e-01 (1.0426e-01)	Acc@1  92.19 ( 96.28)	Acc@5 100.00 ( 99.97)
Epoch: [38][320/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.0728e-01 (1.0499e-01)	Acc@1  93.75 ( 96.26)	Acc@5 100.00 ( 99.98)
Epoch: [38][330/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.6736e-01 (1.0454e-01)	Acc@1  94.53 ( 96.28)	Acc@5 100.00 ( 99.98)
Epoch: [38][340/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2622e-01 (1.0411e-01)	Acc@1  95.31 ( 96.31)	Acc@5 100.00 ( 99.98)
Epoch: [38][350/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.6355e-02 (1.0436e-01)	Acc@1  97.66 ( 96.31)	Acc@5 100.00 ( 99.98)
Epoch: [38][360/391]	Time  0.101 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1371e-01 (1.0449e-01)	Acc@1  94.53 ( 96.29)	Acc@5 100.00 ( 99.98)
Epoch: [38][370/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1438e-01 (1.0459e-01)	Acc@1  95.31 ( 96.29)	Acc@5 100.00 ( 99.98)
Epoch: [38][380/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0089e-01 (1.0446e-01)	Acc@1  96.09 ( 96.29)	Acc@5 100.00 ( 99.98)
Epoch: [38][390/391]	Time  0.099 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.2400e-01 (1.0434e-01)	Acc@1  95.00 ( 96.30)	Acc@5 100.00 ( 99.98)
## e[38] optimizer.zero_grad (sum) time: 0.34668660163879395
## e[38]       loss.backward (sum) time: 13.721232652664185
## e[38]      optimizer.step (sum) time: 2.810361385345459
## epoch[38] training(only) time: 42.65635657310486
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 2.0959e-01 (2.0959e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.036 ( 0.051)	Loss 1.9714e-01 (2.7430e-01)	Acc@1  93.00 ( 91.45)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.037 ( 0.044)	Loss 4.1138e-01 (3.1425e-01)	Acc@1  87.00 ( 90.67)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.037 ( 0.042)	Loss 3.0762e-01 (3.3999e-01)	Acc@1  87.00 ( 90.68)	Acc@5 100.00 ( 99.55)
Test: [ 40/100]	Time  0.038 ( 0.041)	Loss 2.4097e-01 (3.4357e-01)	Acc@1  92.00 ( 90.68)	Acc@5  99.00 ( 99.56)
Test: [ 50/100]	Time  0.036 ( 0.040)	Loss 1.6821e-01 (3.3766e-01)	Acc@1  95.00 ( 90.86)	Acc@5 100.00 ( 99.57)
Test: [ 60/100]	Time  0.036 ( 0.040)	Loss 3.5596e-01 (3.3115e-01)	Acc@1  94.00 ( 91.07)	Acc@5  99.00 ( 99.61)
Test: [ 70/100]	Time  0.039 ( 0.039)	Loss 5.0537e-01 (3.2912e-01)	Acc@1  87.00 ( 90.96)	Acc@5  99.00 ( 99.62)
Test: [ 80/100]	Time  0.039 ( 0.039)	Loss 2.5854e-01 (3.2934e-01)	Acc@1  91.00 ( 90.94)	Acc@5 100.00 ( 99.62)
Test: [ 90/100]	Time  0.037 ( 0.039)	Loss 2.2083e-01 (3.2444e-01)	Acc@1  93.00 ( 90.98)	Acc@5 100.00 ( 99.63)
 * Acc@1 90.900 Acc@5 99.650
### epoch[38] execution time: 46.625184059143066
EPOCH 39
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [39][  0/391]	Time  0.274 ( 0.274)	Data  0.169 ( 0.169)	Loss 1.3940e-01 (1.3940e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [39][ 10/391]	Time  0.105 ( 0.123)	Data  0.001 ( 0.019)	Loss 7.3120e-02 (8.7327e-02)	Acc@1  96.88 ( 97.02)	Acc@5 100.00 ( 99.93)
Epoch: [39][ 20/391]	Time  0.102 ( 0.116)	Data  0.001 ( 0.012)	Loss 1.5161e-01 (9.3695e-02)	Acc@1  95.31 ( 96.91)	Acc@5 100.00 ( 99.96)
Epoch: [39][ 30/391]	Time  0.108 ( 0.113)	Data  0.001 ( 0.009)	Loss 1.0614e-01 (9.2759e-02)	Acc@1  96.09 ( 96.82)	Acc@5 100.00 ( 99.95)
Epoch: [39][ 40/391]	Time  0.108 ( 0.112)	Data  0.001 ( 0.008)	Loss 1.2866e-01 (8.9178e-02)	Acc@1  94.53 ( 96.95)	Acc@5 100.00 ( 99.96)
Epoch: [39][ 50/391]	Time  0.105 ( 0.111)	Data  0.001 ( 0.007)	Loss 1.0394e-01 (8.9302e-02)	Acc@1  95.31 ( 96.88)	Acc@5 100.00 ( 99.97)
Epoch: [39][ 60/391]	Time  0.101 ( 0.111)	Data  0.002 ( 0.007)	Loss 6.9946e-02 (9.0879e-02)	Acc@1  96.88 ( 96.77)	Acc@5 100.00 ( 99.97)
Epoch: [39][ 70/391]	Time  0.108 ( 0.110)	Data  0.002 ( 0.006)	Loss 3.6591e-02 (9.0431e-02)	Acc@1  99.22 ( 96.78)	Acc@5 100.00 ( 99.98)
Epoch: [39][ 80/391]	Time  0.104 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.6528e-01 (9.2474e-02)	Acc@1  95.31 ( 96.66)	Acc@5 100.00 ( 99.98)
Epoch: [39][ 90/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.1945e-01 (9.3498e-02)	Acc@1  93.75 ( 96.63)	Acc@5 100.00 ( 99.98)
Epoch: [39][100/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.006)	Loss 5.4260e-02 (9.1374e-02)	Acc@1  99.22 ( 96.74)	Acc@5 100.00 ( 99.98)
Epoch: [39][110/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.006)	Loss 1.2878e-01 (9.0643e-02)	Acc@1  95.31 ( 96.79)	Acc@5 100.00 ( 99.99)
Epoch: [39][120/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.9060e-02 (9.0666e-02)	Acc@1  96.09 ( 96.75)	Acc@5 100.00 ( 99.99)
Epoch: [39][130/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0333e-01 (9.1385e-02)	Acc@1  96.09 ( 96.71)	Acc@5 100.00 ( 99.99)
Epoch: [39][140/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.2307e-02 (9.2230e-02)	Acc@1  97.66 ( 96.71)	Acc@5 100.00 ( 99.99)
Epoch: [39][150/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.3619e-02 (9.1907e-02)	Acc@1  97.66 ( 96.74)	Acc@5 100.00 ( 99.98)
Epoch: [39][160/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.9670e-02 (9.2123e-02)	Acc@1  95.31 ( 96.71)	Acc@5 100.00 ( 99.99)
Epoch: [39][170/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1212e-01 (9.1783e-02)	Acc@1  95.31 ( 96.72)	Acc@5 100.00 ( 99.99)
Epoch: [39][180/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2585e-01 (9.2136e-02)	Acc@1  96.09 ( 96.72)	Acc@5 100.00 ( 99.99)
Epoch: [39][190/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.9214e-02 (9.2165e-02)	Acc@1  97.66 ( 96.71)	Acc@5 100.00 ( 99.99)
Epoch: [39][200/391]	Time  0.108 ( 0.109)	Data  0.002 ( 0.005)	Loss 1.4441e-01 (9.3845e-02)	Acc@1  93.75 ( 96.63)	Acc@5 100.00 ( 99.99)
Epoch: [39][210/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4709e-01 (9.4693e-02)	Acc@1  93.75 ( 96.59)	Acc@5 100.00 ( 99.99)
Epoch: [39][220/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.0637e-02 (9.4515e-02)	Acc@1  96.88 ( 96.60)	Acc@5 100.00 ( 99.99)
Epoch: [39][230/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.5725e-02 (9.6024e-02)	Acc@1  98.44 ( 96.54)	Acc@5 100.00 ( 99.99)
Epoch: [39][240/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2610e-01 (9.6650e-02)	Acc@1  93.75 ( 96.51)	Acc@5 100.00 ( 99.99)
Epoch: [39][250/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3269e-01 (9.7246e-02)	Acc@1  94.53 ( 96.47)	Acc@5 100.00 ( 99.99)
Epoch: [39][260/391]	Time  0.112 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.8430e-02 (9.7970e-02)	Acc@1  97.66 ( 96.44)	Acc@5 100.00 ( 99.99)
Epoch: [39][270/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.7839e-02 (9.7412e-02)	Acc@1  96.09 ( 96.48)	Acc@5 100.00 ( 99.99)
Epoch: [39][280/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.2571e-02 (9.8410e-02)	Acc@1  96.88 ( 96.44)	Acc@5 100.00 ( 99.99)
Epoch: [39][290/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0339e-01 (9.8239e-02)	Acc@1  95.31 ( 96.45)	Acc@5 100.00 ( 99.99)
Epoch: [39][300/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.8319e-02 (9.8375e-02)	Acc@1  97.66 ( 96.45)	Acc@5 100.00 ( 99.99)
Epoch: [39][310/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0840e-01 (9.7846e-02)	Acc@1  96.88 ( 96.46)	Acc@5 100.00 ( 99.99)
Epoch: [39][320/391]	Time  0.106 ( 0.109)	Data  0.002 ( 0.005)	Loss 7.5134e-02 (9.7497e-02)	Acc@1  96.88 ( 96.48)	Acc@5 100.00 ( 99.99)
Epoch: [39][330/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.6121e-02 (9.7691e-02)	Acc@1  96.09 ( 96.46)	Acc@5 100.00 ( 99.99)
Epoch: [39][340/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.9590e-02 (9.7215e-02)	Acc@1  97.66 ( 96.50)	Acc@5 100.00 ( 99.99)
Epoch: [39][350/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.6365e-02 (9.7685e-02)	Acc@1  96.09 ( 96.47)	Acc@5 100.00 ( 99.99)
Epoch: [39][360/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.0154e-01 (9.8455e-02)	Acc@1  93.75 ( 96.46)	Acc@5 100.00 ( 99.99)
Epoch: [39][370/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.6670e-02 (9.8579e-02)	Acc@1  96.88 ( 96.46)	Acc@5 100.00 ( 99.99)
Epoch: [39][380/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5979e-01 (9.8179e-02)	Acc@1  94.53 ( 96.47)	Acc@5 100.00 ( 99.99)
Epoch: [39][390/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.9113e-02 (9.8421e-02)	Acc@1  97.50 ( 96.47)	Acc@5 100.00 ( 99.99)
## e[39] optimizer.zero_grad (sum) time: 0.35394811630249023
## e[39]       loss.backward (sum) time: 13.703801393508911
## e[39]      optimizer.step (sum) time: 2.8195266723632812
## epoch[39] training(only) time: 42.66592454910278
# Switched to evaluate mode...
Test: [  0/100]	Time  0.192 ( 0.192)	Loss 2.7026e-01 (2.7026e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.039 ( 0.052)	Loss 1.8872e-01 (2.8627e-01)	Acc@1  93.00 ( 90.55)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.037 ( 0.045)	Loss 4.7729e-01 (3.2269e-01)	Acc@1  86.00 ( 90.14)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.036 ( 0.042)	Loss 3.0322e-01 (3.4350e-01)	Acc@1  91.00 ( 90.29)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.036 ( 0.041)	Loss 2.6587e-01 (3.4488e-01)	Acc@1  91.00 ( 90.41)	Acc@5  99.00 ( 99.63)
Test: [ 50/100]	Time  0.036 ( 0.040)	Loss 1.6125e-01 (3.4051e-01)	Acc@1  95.00 ( 90.67)	Acc@5 100.00 ( 99.65)
Test: [ 60/100]	Time  0.036 ( 0.040)	Loss 4.1064e-01 (3.3361e-01)	Acc@1  94.00 ( 90.82)	Acc@5  99.00 ( 99.67)
Test: [ 70/100]	Time  0.039 ( 0.040)	Loss 5.0342e-01 (3.3249e-01)	Acc@1  86.00 ( 90.76)	Acc@5 100.00 ( 99.68)
Test: [ 80/100]	Time  0.036 ( 0.039)	Loss 2.4915e-01 (3.3351e-01)	Acc@1  88.00 ( 90.68)	Acc@5 100.00 ( 99.67)
Test: [ 90/100]	Time  0.039 ( 0.039)	Loss 2.8955e-01 (3.3013e-01)	Acc@1  91.00 ( 90.60)	Acc@5 100.00 ( 99.66)
 * Acc@1 90.600 Acc@5 99.690
### epoch[39] execution time: 46.68815279006958
EPOCH 40
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [40][  0/391]	Time  0.270 ( 0.270)	Data  0.172 ( 0.172)	Loss 5.9753e-02 (5.9753e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [40][ 10/391]	Time  0.108 ( 0.123)	Data  0.001 ( 0.019)	Loss 2.5787e-02 (8.8470e-02)	Acc@1 100.00 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [40][ 20/391]	Time  0.105 ( 0.116)	Data  0.001 ( 0.012)	Loss 7.9407e-02 (9.0495e-02)	Acc@1  96.09 ( 96.73)	Acc@5 100.00 (100.00)
Epoch: [40][ 30/391]	Time  0.107 ( 0.114)	Data  0.001 ( 0.009)	Loss 9.7839e-02 (9.1200e-02)	Acc@1  96.88 ( 96.80)	Acc@5 100.00 ( 99.95)
Epoch: [40][ 40/391]	Time  0.108 ( 0.112)	Data  0.001 ( 0.008)	Loss 5.7465e-02 (9.0399e-02)	Acc@1  96.88 ( 96.74)	Acc@5 100.00 ( 99.96)
Epoch: [40][ 50/391]	Time  0.107 ( 0.111)	Data  0.001 ( 0.007)	Loss 6.3660e-02 (8.9673e-02)	Acc@1  97.66 ( 96.75)	Acc@5 100.00 ( 99.97)
Epoch: [40][ 60/391]	Time  0.108 ( 0.111)	Data  0.001 ( 0.007)	Loss 1.3562e-01 (9.0953e-02)	Acc@1  94.53 ( 96.71)	Acc@5 100.00 ( 99.97)
Epoch: [40][ 70/391]	Time  0.104 ( 0.111)	Data  0.001 ( 0.006)	Loss 1.1371e-01 (9.1203e-02)	Acc@1  95.31 ( 96.76)	Acc@5 100.00 ( 99.98)
Epoch: [40][ 80/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.006)	Loss 4.6692e-02 (9.1338e-02)	Acc@1  98.44 ( 96.75)	Acc@5 100.00 ( 99.98)
Epoch: [40][ 90/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.006)	Loss 3.9490e-02 (9.0721e-02)	Acc@1  98.44 ( 96.75)	Acc@5 100.00 ( 99.98)
Epoch: [40][100/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 7.2449e-02 (9.0609e-02)	Acc@1  96.88 ( 96.70)	Acc@5 100.00 ( 99.98)
Epoch: [40][110/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.6370e-01 (9.1211e-02)	Acc@1  96.09 ( 96.73)	Acc@5 100.00 ( 99.99)
Epoch: [40][120/391]	Time  0.110 ( 0.110)	Data  0.001 ( 0.005)	Loss 1.1023e-01 (9.3420e-02)	Acc@1  96.09 ( 96.62)	Acc@5 100.00 ( 99.98)
Epoch: [40][130/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.0444e-02 (9.3029e-02)	Acc@1  97.66 ( 96.65)	Acc@5 100.00 ( 99.98)
Epoch: [40][140/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0486e-01 (9.2100e-02)	Acc@1  96.09 ( 96.68)	Acc@5 100.00 ( 99.98)
Epoch: [40][150/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.3740e-02 (9.2236e-02)	Acc@1  96.88 ( 96.68)	Acc@5 100.00 ( 99.98)
Epoch: [40][160/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1841e-01 (9.1934e-02)	Acc@1  95.31 ( 96.69)	Acc@5 100.00 ( 99.99)
Epoch: [40][170/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.1666e-02 (9.2807e-02)	Acc@1  99.22 ( 96.64)	Acc@5 100.00 ( 99.98)
Epoch: [40][180/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0077e-01 (9.2685e-02)	Acc@1  96.88 ( 96.65)	Acc@5 100.00 ( 99.98)
Epoch: [40][190/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2402e-01 (9.2151e-02)	Acc@1  96.09 ( 96.67)	Acc@5 100.00 ( 99.98)
Epoch: [40][200/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.2164e-02 (9.2304e-02)	Acc@1  96.88 ( 96.65)	Acc@5 100.00 ( 99.98)
Epoch: [40][210/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.8328e-02 (9.3232e-02)	Acc@1  96.09 ( 96.58)	Acc@5 100.00 ( 99.98)
Epoch: [40][220/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1908e-01 (9.3014e-02)	Acc@1  95.31 ( 96.59)	Acc@5 100.00 ( 99.98)
Epoch: [40][230/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1041e-01 (9.3347e-02)	Acc@1  96.09 ( 96.57)	Acc@5 100.00 ( 99.98)
Epoch: [40][240/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.1802e-01 (9.2904e-02)	Acc@1  91.41 ( 96.60)	Acc@5 100.00 ( 99.98)
Epoch: [40][250/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2134e-01 (9.4693e-02)	Acc@1  96.88 ( 96.54)	Acc@5 100.00 ( 99.98)
Epoch: [40][260/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.4230e-02 (9.4313e-02)	Acc@1  98.44 ( 96.56)	Acc@5 100.00 ( 99.98)
Epoch: [40][270/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.3872e-02 (9.4750e-02)	Acc@1  96.09 ( 96.55)	Acc@5 100.00 ( 99.98)
Epoch: [40][280/391]	Time  0.100 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.7830e-02 (9.4437e-02)	Acc@1  98.44 ( 96.56)	Acc@5 100.00 ( 99.98)
Epoch: [40][290/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.8064e-02 (9.4580e-02)	Acc@1  96.88 ( 96.54)	Acc@5 100.00 ( 99.98)
Epoch: [40][300/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1700e-01 (9.5362e-02)	Acc@1  96.88 ( 96.53)	Acc@5 100.00 ( 99.98)
Epoch: [40][310/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.5298e-02 (9.4842e-02)	Acc@1  97.66 ( 96.57)	Acc@5 100.00 ( 99.98)
Epoch: [40][320/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.7717e-02 (9.4586e-02)	Acc@1  95.31 ( 96.57)	Acc@5 100.00 ( 99.98)
Epoch: [40][330/391]	Time  0.103 ( 0.108)	Data  0.001 ( 0.005)	Loss 2.0532e-01 (9.4994e-02)	Acc@1  91.41 ( 96.54)	Acc@5 100.00 ( 99.98)
Epoch: [40][340/391]	Time  0.105 ( 0.108)	Data  0.001 ( 0.005)	Loss 5.1361e-02 (9.4438e-02)	Acc@1  99.22 ( 96.56)	Acc@5 100.00 ( 99.98)
Epoch: [40][350/391]	Time  0.109 ( 0.108)	Data  0.001 ( 0.005)	Loss 9.8999e-02 (9.4130e-02)	Acc@1  96.09 ( 96.58)	Acc@5 100.00 ( 99.98)
Epoch: [40][360/391]	Time  0.104 ( 0.108)	Data  0.001 ( 0.005)	Loss 7.5928e-02 (9.4677e-02)	Acc@1  98.44 ( 96.56)	Acc@5 100.00 ( 99.98)
Epoch: [40][370/391]	Time  0.104 ( 0.108)	Data  0.001 ( 0.005)	Loss 7.2998e-02 (9.4631e-02)	Acc@1  98.44 ( 96.56)	Acc@5 100.00 ( 99.98)
Epoch: [40][380/391]	Time  0.107 ( 0.108)	Data  0.002 ( 0.005)	Loss 1.0950e-01 (9.4630e-02)	Acc@1  95.31 ( 96.56)	Acc@5 100.00 ( 99.98)
Epoch: [40][390/391]	Time  0.100 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.2964e-01 (9.4533e-02)	Acc@1  95.00 ( 96.56)	Acc@5 100.00 ( 99.98)
## e[40] optimizer.zero_grad (sum) time: 0.3551933765411377
## e[40]       loss.backward (sum) time: 13.72971796989441
## e[40]      optimizer.step (sum) time: 2.764275550842285
## epoch[40] training(only) time: 42.55568265914917
# Switched to evaluate mode...
Test: [  0/100]	Time  0.193 ( 0.193)	Loss 2.2424e-01 (2.2424e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.037 ( 0.053)	Loss 2.1411e-01 (3.1009e-01)	Acc@1  95.00 ( 90.45)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.038 ( 0.045)	Loss 4.7632e-01 (3.3364e-01)	Acc@1  89.00 ( 90.33)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.036 ( 0.042)	Loss 2.4219e-01 (3.5170e-01)	Acc@1  89.00 ( 90.23)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.037 ( 0.041)	Loss 3.1274e-01 (3.5339e-01)	Acc@1  92.00 ( 90.29)	Acc@5  99.00 ( 99.63)
Test: [ 50/100]	Time  0.035 ( 0.040)	Loss 2.3059e-01 (3.4978e-01)	Acc@1  94.00 ( 90.53)	Acc@5 100.00 ( 99.63)
Test: [ 60/100]	Time  0.037 ( 0.040)	Loss 3.9307e-01 (3.4312e-01)	Acc@1  96.00 ( 90.75)	Acc@5  99.00 ( 99.67)
Test: [ 70/100]	Time  0.039 ( 0.039)	Loss 4.5239e-01 (3.4089e-01)	Acc@1  88.00 ( 90.65)	Acc@5 100.00 ( 99.70)
Test: [ 80/100]	Time  0.036 ( 0.039)	Loss 2.3279e-01 (3.3972e-01)	Acc@1  93.00 ( 90.63)	Acc@5 100.00 ( 99.72)
Test: [ 90/100]	Time  0.036 ( 0.039)	Loss 2.5952e-01 (3.3414e-01)	Acc@1  92.00 ( 90.71)	Acc@5 100.00 ( 99.73)
 * Acc@1 90.700 Acc@5 99.740
### epoch[40] execution time: 46.53858804702759
EPOCH 41
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [41][  0/391]	Time  0.271 ( 0.271)	Data  0.168 ( 0.168)	Loss 4.6661e-02 (4.6661e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [41][ 10/391]	Time  0.109 ( 0.123)	Data  0.001 ( 0.019)	Loss 6.6467e-02 (7.5767e-02)	Acc@1  97.66 ( 97.73)	Acc@5 100.00 (100.00)
Epoch: [41][ 20/391]	Time  0.103 ( 0.115)	Data  0.001 ( 0.012)	Loss 1.3379e-01 (7.4569e-02)	Acc@1  95.31 ( 97.62)	Acc@5 100.00 (100.00)
Epoch: [41][ 30/391]	Time  0.105 ( 0.113)	Data  0.001 ( 0.009)	Loss 1.4099e-01 (7.7172e-02)	Acc@1  95.31 ( 97.53)	Acc@5 100.00 (100.00)
Epoch: [41][ 40/391]	Time  0.108 ( 0.112)	Data  0.001 ( 0.008)	Loss 7.1899e-02 (7.9811e-02)	Acc@1  97.66 ( 97.31)	Acc@5 100.00 (100.00)
Epoch: [41][ 50/391]	Time  0.105 ( 0.111)	Data  0.001 ( 0.007)	Loss 4.6356e-02 (7.8191e-02)	Acc@1  99.22 ( 97.44)	Acc@5 100.00 (100.00)
Epoch: [41][ 60/391]	Time  0.100 ( 0.110)	Data  0.001 ( 0.007)	Loss 8.4473e-02 (7.8998e-02)	Acc@1  96.88 ( 97.39)	Acc@5 100.00 (100.00)
Epoch: [41][ 70/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.5808e-01 (7.9414e-02)	Acc@1  94.53 ( 97.39)	Acc@5 100.00 ( 99.99)
Epoch: [41][ 80/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.1469e-01 (8.3089e-02)	Acc@1  95.31 ( 97.23)	Acc@5 100.00 ( 99.99)
Epoch: [41][ 90/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.006)	Loss 1.4429e-01 (8.1875e-02)	Acc@1  94.53 ( 97.27)	Acc@5 100.00 ( 99.99)
Epoch: [41][100/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.006)	Loss 1.2305e-01 (8.3118e-02)	Acc@1  95.31 ( 97.20)	Acc@5 100.00 ( 99.99)
Epoch: [41][110/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.006)	Loss 9.0881e-02 (8.3639e-02)	Acc@1  96.09 ( 97.19)	Acc@5 100.00 ( 99.99)
Epoch: [41][120/391]	Time  0.113 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.0425e-02 (8.3903e-02)	Acc@1  97.66 ( 97.17)	Acc@5 100.00 ( 99.99)
Epoch: [41][130/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.9177e-01 (8.6150e-02)	Acc@1  92.19 ( 97.11)	Acc@5 100.00 ( 99.99)
Epoch: [41][140/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0449e-01 (8.6017e-02)	Acc@1  95.31 ( 97.10)	Acc@5 100.00 ( 99.99)
Epoch: [41][150/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.7175e-01 (8.6902e-02)	Acc@1  94.53 ( 97.06)	Acc@5 100.00 ( 99.99)
Epoch: [41][160/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.4026e-02 (8.7089e-02)	Acc@1  97.66 ( 97.05)	Acc@5 100.00 (100.00)
Epoch: [41][170/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.5450e-02 (8.7743e-02)	Acc@1  97.66 ( 97.00)	Acc@5 100.00 (100.00)
Epoch: [41][180/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.2561e-02 (8.7908e-02)	Acc@1  97.66 ( 96.97)	Acc@5 100.00 (100.00)
Epoch: [41][190/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.3811e-02 (8.7340e-02)	Acc@1  95.31 ( 96.97)	Acc@5 100.00 (100.00)
Epoch: [41][200/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2708e-01 (8.7613e-02)	Acc@1  94.53 ( 96.96)	Acc@5 100.00 ( 99.99)
Epoch: [41][210/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.3793e-02 (8.7560e-02)	Acc@1  99.22 ( 96.95)	Acc@5 100.00 ( 99.99)
Epoch: [41][220/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3574e-01 (8.8365e-02)	Acc@1  93.75 ( 96.91)	Acc@5 100.00 ( 99.99)
Epoch: [41][230/391]	Time  0.105 ( 0.109)	Data  0.002 ( 0.005)	Loss 1.6467e-01 (8.8487e-02)	Acc@1  96.09 ( 96.91)	Acc@5  99.22 ( 99.98)
Epoch: [41][240/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.1412e-02 (8.8136e-02)	Acc@1  98.44 ( 96.93)	Acc@5 100.00 ( 99.98)
Epoch: [41][250/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1029e-01 (8.8392e-02)	Acc@1  94.53 ( 96.92)	Acc@5 100.00 ( 99.98)
Epoch: [41][260/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1938e-01 (8.8633e-02)	Acc@1  95.31 ( 96.91)	Acc@5 100.00 ( 99.99)
Epoch: [41][270/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.2937e-02 (8.8190e-02)	Acc@1  96.88 ( 96.94)	Acc@5 100.00 ( 99.98)
Epoch: [41][280/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1206e-01 (8.8537e-02)	Acc@1  96.88 ( 96.93)	Acc@5 100.00 ( 99.98)
Epoch: [41][290/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0724e-01 (8.8738e-02)	Acc@1  96.09 ( 96.91)	Acc@5 100.00 ( 99.98)
Epoch: [41][300/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1517e-01 (8.9508e-02)	Acc@1  96.09 ( 96.90)	Acc@5 100.00 ( 99.98)
Epoch: [41][310/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.8633e-02 (9.0208e-02)	Acc@1  96.09 ( 96.90)	Acc@5 100.00 ( 99.98)
Epoch: [41][320/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1902e-01 (9.0789e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 ( 99.98)
Epoch: [41][330/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.1919e-02 (9.0999e-02)	Acc@1  97.66 ( 96.86)	Acc@5 100.00 ( 99.98)
Epoch: [41][340/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.6260e-01 (9.1107e-02)	Acc@1  94.53 ( 96.87)	Acc@5 100.00 ( 99.98)
Epoch: [41][350/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3232e-01 (9.1099e-02)	Acc@1  95.31 ( 96.86)	Acc@5 100.00 ( 99.98)
Epoch: [41][360/391]	Time  0.109 ( 0.109)	Data  0.002 ( 0.005)	Loss 9.7473e-02 (9.1122e-02)	Acc@1  96.88 ( 96.87)	Acc@5 100.00 ( 99.98)
Epoch: [41][370/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4282e-01 (9.1211e-02)	Acc@1  94.53 ( 96.84)	Acc@5 100.00 ( 99.98)
Epoch: [41][380/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.5317e-02 (9.1156e-02)	Acc@1  97.66 ( 96.84)	Acc@5 100.00 ( 99.98)
Epoch: [41][390/391]	Time  0.100 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.8309e-02 (9.1232e-02)	Acc@1  98.75 ( 96.83)	Acc@5 100.00 ( 99.98)
## e[41] optimizer.zero_grad (sum) time: 0.34698915481567383
## e[41]       loss.backward (sum) time: 13.747099161148071
## e[41]      optimizer.step (sum) time: 2.7729804515838623
## epoch[41] training(only) time: 42.67482852935791
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 2.7197e-01 (2.7197e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.039 ( 0.052)	Loss 1.8506e-01 (2.9969e-01)	Acc@1  93.00 ( 90.91)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.035 ( 0.045)	Loss 4.7241e-01 (3.3327e-01)	Acc@1  88.00 ( 90.24)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.038 ( 0.043)	Loss 3.6353e-01 (3.5541e-01)	Acc@1  87.00 ( 90.26)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.036 ( 0.041)	Loss 2.8833e-01 (3.5726e-01)	Acc@1  91.00 ( 90.20)	Acc@5  99.00 ( 99.66)
Test: [ 50/100]	Time  0.039 ( 0.041)	Loss 2.1399e-01 (3.4834e-01)	Acc@1  95.00 ( 90.47)	Acc@5 100.00 ( 99.69)
Test: [ 60/100]	Time  0.036 ( 0.040)	Loss 4.1333e-01 (3.4176e-01)	Acc@1  93.00 ( 90.54)	Acc@5  99.00 ( 99.72)
Test: [ 70/100]	Time  0.036 ( 0.040)	Loss 4.5312e-01 (3.3834e-01)	Acc@1  88.00 ( 90.61)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.037 ( 0.039)	Loss 3.3032e-01 (3.4080e-01)	Acc@1  90.00 ( 90.59)	Acc@5 100.00 ( 99.73)
Test: [ 90/100]	Time  0.036 ( 0.039)	Loss 2.9565e-01 (3.3419e-01)	Acc@1  92.00 ( 90.64)	Acc@5 100.00 ( 99.73)
 * Acc@1 90.650 Acc@5 99.740
### epoch[41] execution time: 46.64968776702881
EPOCH 42
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [42][  0/391]	Time  0.280 ( 0.280)	Data  0.174 ( 0.174)	Loss 1.1475e-01 (1.1475e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [42][ 10/391]	Time  0.108 ( 0.124)	Data  0.001 ( 0.019)	Loss 8.9783e-02 (9.7312e-02)	Acc@1  96.88 ( 96.24)	Acc@5 100.00 (100.00)
Epoch: [42][ 20/391]	Time  0.107 ( 0.117)	Data  0.001 ( 0.012)	Loss 7.2754e-02 (9.0438e-02)	Acc@1  97.66 ( 96.76)	Acc@5 100.00 (100.00)
Epoch: [42][ 30/391]	Time  0.106 ( 0.114)	Data  0.001 ( 0.009)	Loss 8.0933e-02 (8.9826e-02)	Acc@1  96.88 ( 96.82)	Acc@5 100.00 (100.00)
Epoch: [42][ 40/391]	Time  0.105 ( 0.112)	Data  0.001 ( 0.008)	Loss 7.5989e-02 (8.8277e-02)	Acc@1  96.88 ( 96.84)	Acc@5 100.00 (100.00)
Epoch: [42][ 50/391]	Time  0.105 ( 0.112)	Data  0.001 ( 0.007)	Loss 9.4788e-02 (9.3183e-02)	Acc@1  96.88 ( 96.61)	Acc@5 100.00 (100.00)
Epoch: [42][ 60/391]	Time  0.105 ( 0.111)	Data  0.001 ( 0.007)	Loss 9.7778e-02 (9.2123e-02)	Acc@1  95.31 ( 96.67)	Acc@5 100.00 (100.00)
Epoch: [42][ 70/391]	Time  0.108 ( 0.111)	Data  0.001 ( 0.006)	Loss 3.8605e-02 (8.9479e-02)	Acc@1  98.44 ( 96.82)	Acc@5 100.00 (100.00)
Epoch: [42][ 80/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.006)	Loss 7.6538e-02 (8.8314e-02)	Acc@1  96.88 ( 96.84)	Acc@5 100.00 (100.00)
Epoch: [42][ 90/391]	Time  0.111 ( 0.110)	Data  0.001 ( 0.006)	Loss 3.4576e-02 (8.6588e-02)	Acc@1 100.00 ( 96.96)	Acc@5 100.00 (100.00)
Epoch: [42][100/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.0052e-01 (8.9785e-02)	Acc@1  96.88 ( 96.86)	Acc@5 100.00 (100.00)
Epoch: [42][110/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 4.8492e-02 (8.8353e-02)	Acc@1  98.44 ( 96.91)	Acc@5 100.00 (100.00)
Epoch: [42][120/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.6985e-02 (8.8100e-02)	Acc@1  98.44 ( 96.94)	Acc@5 100.00 (100.00)
Epoch: [42][130/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.8501e-02 (8.7631e-02)	Acc@1  96.88 ( 96.98)	Acc@5 100.00 (100.00)
Epoch: [42][140/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.2388e-02 (8.7523e-02)	Acc@1  97.66 ( 97.02)	Acc@5 100.00 (100.00)
Epoch: [42][150/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2866e-01 (8.8490e-02)	Acc@1  94.53 ( 96.94)	Acc@5 100.00 ( 99.99)
Epoch: [42][160/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2598e-01 (8.9271e-02)	Acc@1  96.09 ( 96.94)	Acc@5 100.00 (100.00)
Epoch: [42][170/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.1248e-02 (8.9467e-02)	Acc@1  96.09 ( 96.95)	Acc@5 100.00 ( 99.99)
Epoch: [42][180/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.6619e-02 (8.9819e-02)	Acc@1  96.09 ( 96.91)	Acc@5 100.00 ( 99.99)
Epoch: [42][190/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.8328e-02 (9.0729e-02)	Acc@1  96.09 ( 96.87)	Acc@5 100.00 ( 99.99)
Epoch: [42][200/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1261e-01 (9.0847e-02)	Acc@1  95.31 ( 96.86)	Acc@5 100.00 ( 99.99)
Epoch: [42][210/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0217e-01 (9.1592e-02)	Acc@1  94.53 ( 96.83)	Acc@5 100.00 ( 99.99)
Epoch: [42][220/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.5156e-02 (9.0911e-02)	Acc@1  98.44 ( 96.86)	Acc@5 100.00 ( 99.99)
Epoch: [42][230/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.7424e-02 (9.0136e-02)	Acc@1  99.22 ( 96.90)	Acc@5 100.00 ( 99.99)
Epoch: [42][240/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.4951e-02 (9.0673e-02)	Acc@1  97.66 ( 96.88)	Acc@5 100.00 ( 99.99)
Epoch: [42][250/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.0441e-02 (9.0655e-02)	Acc@1 100.00 ( 96.86)	Acc@5 100.00 ( 99.99)
Epoch: [42][260/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.6313e-02 (9.0653e-02)	Acc@1  97.66 ( 96.85)	Acc@5 100.00 ( 99.99)
Epoch: [42][270/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.4238e-02 (9.0659e-02)	Acc@1  96.09 ( 96.85)	Acc@5 100.00 ( 99.99)
Epoch: [42][280/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.3853e-02 (9.0402e-02)	Acc@1  95.31 ( 96.86)	Acc@5 100.00 ( 99.99)
Epoch: [42][290/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.7017e-02 (8.9839e-02)	Acc@1  98.44 ( 96.87)	Acc@5 100.00 ( 99.99)
Epoch: [42][300/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.3994e-02 (9.0251e-02)	Acc@1  96.88 ( 96.86)	Acc@5 100.00 ( 99.99)
Epoch: [42][310/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.7407e-01 (9.0276e-02)	Acc@1  93.75 ( 96.86)	Acc@5 100.00 ( 99.99)
Epoch: [42][320/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.7404e-02 (9.0138e-02)	Acc@1  97.66 ( 96.87)	Acc@5 100.00 ( 99.99)
Epoch: [42][330/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.9316e-02 (8.9675e-02)	Acc@1  99.22 ( 96.89)	Acc@5 100.00 ( 99.99)
Epoch: [42][340/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.0558e-02 (8.9644e-02)	Acc@1  98.44 ( 96.89)	Acc@5 100.00 ( 99.99)
Epoch: [42][350/391]	Time  0.107 ( 0.109)	Data  0.002 ( 0.005)	Loss 8.5266e-02 (8.9069e-02)	Acc@1  96.88 ( 96.91)	Acc@5 100.00 ( 99.99)
Epoch: [42][360/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3550e-01 (8.8742e-02)	Acc@1  96.88 ( 96.93)	Acc@5 100.00 ( 99.99)
Epoch: [42][370/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.4951e-02 (8.8648e-02)	Acc@1  96.88 ( 96.92)	Acc@5 100.00 ( 99.99)
Epoch: [42][380/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.3567e-02 (8.8705e-02)	Acc@1  97.66 ( 96.92)	Acc@5 100.00 ( 99.99)
Epoch: [42][390/391]	Time  0.101 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.8116e-02 (8.8197e-02)	Acc@1 100.00 ( 96.94)	Acc@5 100.00 ( 99.99)
## e[42] optimizer.zero_grad (sum) time: 0.3568994998931885
## e[42]       loss.backward (sum) time: 13.672916650772095
## e[42]      optimizer.step (sum) time: 2.7397172451019287
## epoch[42] training(only) time: 42.59280490875244
# Switched to evaluate mode...
Test: [  0/100]	Time  0.202 ( 0.202)	Loss 2.9175e-01 (2.9175e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.035 ( 0.053)	Loss 2.2107e-01 (3.0487e-01)	Acc@1  93.00 ( 91.18)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.038 ( 0.045)	Loss 5.0146e-01 (3.3736e-01)	Acc@1  87.00 ( 90.62)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.037 ( 0.043)	Loss 3.4058e-01 (3.6838e-01)	Acc@1  88.00 ( 90.26)	Acc@5 100.00 ( 99.61)
Test: [ 40/100]	Time  0.036 ( 0.041)	Loss 2.8491e-01 (3.6732e-01)	Acc@1  90.00 ( 90.41)	Acc@5  99.00 ( 99.61)
Test: [ 50/100]	Time  0.038 ( 0.040)	Loss 2.2864e-01 (3.6014e-01)	Acc@1  94.00 ( 90.51)	Acc@5 100.00 ( 99.65)
Test: [ 60/100]	Time  0.036 ( 0.040)	Loss 4.0674e-01 (3.4957e-01)	Acc@1  92.00 ( 90.59)	Acc@5  99.00 ( 99.67)
Test: [ 70/100]	Time  0.037 ( 0.040)	Loss 5.3223e-01 (3.4649e-01)	Acc@1  90.00 ( 90.68)	Acc@5  99.00 ( 99.68)
Test: [ 80/100]	Time  0.039 ( 0.039)	Loss 3.7158e-01 (3.4865e-01)	Acc@1  90.00 ( 90.62)	Acc@5 100.00 ( 99.67)
Test: [ 90/100]	Time  0.037 ( 0.039)	Loss 2.2620e-01 (3.3957e-01)	Acc@1  95.00 ( 90.77)	Acc@5 100.00 ( 99.66)
 * Acc@1 90.780 Acc@5 99.680
### epoch[42] execution time: 46.58384418487549
EPOCH 43
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [43][  0/391]	Time  0.267 ( 0.267)	Data  0.169 ( 0.169)	Loss 1.0516e-01 (1.0516e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [43][ 10/391]	Time  0.113 ( 0.123)	Data  0.001 ( 0.019)	Loss 1.3953e-01 (9.1320e-02)	Acc@1  95.31 ( 96.95)	Acc@5 100.00 (100.00)
Epoch: [43][ 20/391]	Time  0.102 ( 0.116)	Data  0.001 ( 0.012)	Loss 4.6936e-02 (8.4425e-02)	Acc@1  99.22 ( 97.02)	Acc@5 100.00 (100.00)
Epoch: [43][ 30/391]	Time  0.102 ( 0.113)	Data  0.001 ( 0.010)	Loss 4.9255e-02 (7.8284e-02)	Acc@1  97.66 ( 97.20)	Acc@5 100.00 (100.00)
Epoch: [43][ 40/391]	Time  0.105 ( 0.112)	Data  0.001 ( 0.008)	Loss 6.9275e-02 (8.0251e-02)	Acc@1  97.66 ( 97.08)	Acc@5 100.00 (100.00)
Epoch: [43][ 50/391]	Time  0.103 ( 0.111)	Data  0.001 ( 0.007)	Loss 7.7881e-02 (8.1217e-02)	Acc@1  96.88 ( 97.03)	Acc@5 100.00 (100.00)
Epoch: [43][ 60/391]	Time  0.101 ( 0.111)	Data  0.001 ( 0.007)	Loss 8.0505e-02 (7.8787e-02)	Acc@1  97.66 ( 97.12)	Acc@5 100.00 (100.00)
Epoch: [43][ 70/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.006)	Loss 6.8848e-02 (8.3187e-02)	Acc@1  97.66 ( 97.00)	Acc@5 100.00 (100.00)
Epoch: [43][ 80/391]	Time  0.104 ( 0.110)	Data  0.001 ( 0.006)	Loss 6.5979e-02 (8.2281e-02)	Acc@1  96.88 ( 97.09)	Acc@5 100.00 ( 99.99)
Epoch: [43][ 90/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.006)	Loss 6.4514e-02 (8.2795e-02)	Acc@1  96.88 ( 97.06)	Acc@5 100.00 ( 99.98)
Epoch: [43][100/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.006)	Loss 7.4036e-02 (8.2592e-02)	Acc@1  98.44 ( 97.09)	Acc@5 100.00 ( 99.98)
Epoch: [43][110/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.006)	Loss 6.4697e-02 (8.1093e-02)	Acc@1  98.44 ( 97.17)	Acc@5 100.00 ( 99.99)
Epoch: [43][120/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.006)	Loss 5.9937e-02 (8.1378e-02)	Acc@1  98.44 ( 97.19)	Acc@5 100.00 ( 99.98)
Epoch: [43][130/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.4830e-02 (8.0250e-02)	Acc@1  99.22 ( 97.21)	Acc@5 100.00 ( 99.98)
Epoch: [43][140/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.8105e-02 (8.0400e-02)	Acc@1  98.44 ( 97.21)	Acc@5 100.00 ( 99.98)
Epoch: [43][150/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.8491e-02 (7.9964e-02)	Acc@1  97.66 ( 97.24)	Acc@5 100.00 ( 99.98)
Epoch: [43][160/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.5491e-02 (8.0054e-02)	Acc@1  98.44 ( 97.22)	Acc@5 100.00 ( 99.98)
Epoch: [43][170/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5588e-01 (8.0958e-02)	Acc@1  96.09 ( 97.20)	Acc@5  99.22 ( 99.98)
Epoch: [43][180/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4160e-01 (8.0693e-02)	Acc@1  94.53 ( 97.19)	Acc@5 100.00 ( 99.97)
Epoch: [43][190/391]	Time  0.107 ( 0.109)	Data  0.002 ( 0.005)	Loss 4.8737e-02 (8.0268e-02)	Acc@1  98.44 ( 97.20)	Acc@5 100.00 ( 99.97)
Epoch: [43][200/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.1504e-02 (8.0214e-02)	Acc@1  97.66 ( 97.20)	Acc@5 100.00 ( 99.97)
Epoch: [43][210/391]	Time  0.108 ( 0.109)	Data  0.002 ( 0.005)	Loss 7.8735e-02 (8.0125e-02)	Acc@1  98.44 ( 97.23)	Acc@5 100.00 ( 99.97)
Epoch: [43][220/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.5684e-02 (7.9922e-02)	Acc@1  98.44 ( 97.24)	Acc@5 100.00 ( 99.98)
Epoch: [43][230/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.6965e-02 (8.0055e-02)	Acc@1  96.88 ( 97.21)	Acc@5 100.00 ( 99.98)
Epoch: [43][240/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.9214e-02 (7.9949e-02)	Acc@1  96.88 ( 97.21)	Acc@5 100.00 ( 99.98)
Epoch: [43][250/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.5439e-02 (8.0010e-02)	Acc@1  97.66 ( 97.19)	Acc@5 100.00 ( 99.98)
Epoch: [43][260/391]	Time  0.101 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4575e-01 (8.0148e-02)	Acc@1  96.09 ( 97.18)	Acc@5 100.00 ( 99.98)
Epoch: [43][270/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.9763e-02 (8.0353e-02)	Acc@1  96.88 ( 97.16)	Acc@5 100.00 ( 99.98)
Epoch: [43][280/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.9792e-02 (8.0338e-02)	Acc@1  96.09 ( 97.18)	Acc@5 100.00 ( 99.98)
Epoch: [43][290/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2817e-01 (8.0352e-02)	Acc@1  96.09 ( 97.18)	Acc@5 100.00 ( 99.98)
Epoch: [43][300/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.3234e-02 (8.0323e-02)	Acc@1  99.22 ( 97.19)	Acc@5 100.00 ( 99.98)
Epoch: [43][310/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.5825e-02 (8.1312e-02)	Acc@1  94.53 ( 97.14)	Acc@5 100.00 ( 99.98)
Epoch: [43][320/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2512e-01 (8.1243e-02)	Acc@1  96.88 ( 97.15)	Acc@5 100.00 ( 99.98)
Epoch: [43][330/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.9113e-02 (8.1150e-02)	Acc@1  97.66 ( 97.14)	Acc@5 100.00 ( 99.98)
Epoch: [43][340/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.9915e-02 (8.1102e-02)	Acc@1  96.09 ( 97.13)	Acc@5 100.00 ( 99.98)
Epoch: [43][350/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.0679e-02 (8.1256e-02)	Acc@1  97.66 ( 97.13)	Acc@5 100.00 ( 99.98)
Epoch: [43][360/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.6833e-02 (8.1629e-02)	Acc@1  97.66 ( 97.12)	Acc@5 100.00 ( 99.98)
Epoch: [43][370/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1664e-01 (8.1772e-02)	Acc@1  96.09 ( 97.12)	Acc@5 100.00 ( 99.98)
Epoch: [43][380/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.5308e-02 (8.1682e-02)	Acc@1  97.66 ( 97.12)	Acc@5 100.00 ( 99.98)
Epoch: [43][390/391]	Time  0.101 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.1482e-02 (8.1729e-02)	Acc@1  97.50 ( 97.10)	Acc@5 100.00 ( 99.98)
## e[43] optimizer.zero_grad (sum) time: 0.34993553161621094
## e[43]       loss.backward (sum) time: 13.754674673080444
## e[43]      optimizer.step (sum) time: 2.8153350353240967
## epoch[43] training(only) time: 42.6400101184845
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 2.6172e-01 (2.6172e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.036 ( 0.051)	Loss 2.0374e-01 (3.0029e-01)	Acc@1  93.00 ( 91.18)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.036 ( 0.044)	Loss 5.1514e-01 (3.5286e-01)	Acc@1  88.00 ( 90.86)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.037 ( 0.042)	Loss 3.5620e-01 (3.8020e-01)	Acc@1  90.00 ( 90.90)	Acc@5  99.00 ( 99.58)
Test: [ 40/100]	Time  0.039 ( 0.041)	Loss 2.6636e-01 (3.8070e-01)	Acc@1  90.00 ( 90.71)	Acc@5  99.00 ( 99.59)
Test: [ 50/100]	Time  0.037 ( 0.040)	Loss 2.4072e-01 (3.7344e-01)	Acc@1  93.00 ( 90.82)	Acc@5 100.00 ( 99.57)
Test: [ 60/100]	Time  0.037 ( 0.040)	Loss 4.8755e-01 (3.6758e-01)	Acc@1  91.00 ( 90.75)	Acc@5  99.00 ( 99.61)
Test: [ 70/100]	Time  0.039 ( 0.039)	Loss 4.8950e-01 (3.6267e-01)	Acc@1  88.00 ( 90.83)	Acc@5  99.00 ( 99.62)
Test: [ 80/100]	Time  0.036 ( 0.039)	Loss 2.7637e-01 (3.6041e-01)	Acc@1  92.00 ( 90.88)	Acc@5 100.00 ( 99.63)
Test: [ 90/100]	Time  0.036 ( 0.039)	Loss 1.9836e-01 (3.5181e-01)	Acc@1  94.00 ( 91.05)	Acc@5 100.00 ( 99.64)
 * Acc@1 90.970 Acc@5 99.660
### epoch[43] execution time: 46.58594298362732
EPOCH 44
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [44][  0/391]	Time  0.276 ( 0.276)	Data  0.177 ( 0.177)	Loss 1.0187e-01 (1.0187e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
Epoch: [44][ 10/391]	Time  0.107 ( 0.123)	Data  0.001 ( 0.019)	Loss 8.6243e-02 (6.6606e-02)	Acc@1  96.09 ( 97.59)	Acc@5 100.00 (100.00)
Epoch: [44][ 20/391]	Time  0.106 ( 0.116)	Data  0.001 ( 0.012)	Loss 6.4697e-02 (6.8707e-02)	Acc@1  98.44 ( 97.62)	Acc@5 100.00 (100.00)
Epoch: [44][ 30/391]	Time  0.103 ( 0.114)	Data  0.001 ( 0.010)	Loss 3.4088e-02 (6.2068e-02)	Acc@1  99.22 ( 97.93)	Acc@5 100.00 (100.00)
Epoch: [44][ 40/391]	Time  0.105 ( 0.113)	Data  0.001 ( 0.008)	Loss 2.9144e-02 (6.3504e-02)	Acc@1 100.00 ( 97.90)	Acc@5 100.00 (100.00)
Epoch: [44][ 50/391]	Time  0.107 ( 0.112)	Data  0.001 ( 0.007)	Loss 5.6152e-02 (7.0396e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [44][ 60/391]	Time  0.106 ( 0.111)	Data  0.001 ( 0.007)	Loss 6.3232e-02 (7.1477e-02)	Acc@1  96.09 ( 97.53)	Acc@5 100.00 ( 99.99)
Epoch: [44][ 70/391]	Time  0.104 ( 0.111)	Data  0.001 ( 0.007)	Loss 1.4404e-01 (7.1246e-02)	Acc@1  94.53 ( 97.52)	Acc@5 100.00 ( 99.99)
Epoch: [44][ 80/391]	Time  0.102 ( 0.110)	Data  0.001 ( 0.006)	Loss 5.2307e-02 (7.0011e-02)	Acc@1  98.44 ( 97.60)	Acc@5 100.00 ( 99.99)
Epoch: [44][ 90/391]	Time  0.104 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.0120e-01 (7.1361e-02)	Acc@1  96.88 ( 97.60)	Acc@5 100.00 ( 99.99)
Epoch: [44][100/391]	Time  0.109 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.1139e-01 (7.3397e-02)	Acc@1  96.09 ( 97.52)	Acc@5 100.00 ( 99.99)
Epoch: [44][110/391]	Time  0.110 ( 0.110)	Data  0.001 ( 0.006)	Loss 5.8258e-02 (7.1615e-02)	Acc@1  97.66 ( 97.59)	Acc@5 100.00 ( 99.99)
Epoch: [44][120/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 8.4229e-02 (7.2632e-02)	Acc@1  96.09 ( 97.57)	Acc@5 100.00 ( 99.99)
Epoch: [44][130/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.005)	Loss 1.0449e-01 (7.4187e-02)	Acc@1  96.09 ( 97.49)	Acc@5 100.00 ( 99.99)
Epoch: [44][140/391]	Time  0.104 ( 0.110)	Data  0.001 ( 0.005)	Loss 1.0602e-01 (7.4204e-02)	Acc@1  96.88 ( 97.48)	Acc@5 100.00 ( 99.99)
Epoch: [44][150/391]	Time  0.105 ( 0.109)	Data  0.002 ( 0.005)	Loss 1.1066e-01 (7.4435e-02)	Acc@1  96.88 ( 97.46)	Acc@5 100.00 ( 99.99)
Epoch: [44][160/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.8788e-02 (7.4304e-02)	Acc@1  98.44 ( 97.45)	Acc@5 100.00 ( 99.99)
Epoch: [44][170/391]	Time  0.111 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.4900e-02 (7.5093e-02)	Acc@1  97.66 ( 97.40)	Acc@5 100.00 ( 99.99)
Epoch: [44][180/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0022e-01 (7.5015e-02)	Acc@1  96.88 ( 97.41)	Acc@5 100.00 ( 99.99)
Epoch: [44][190/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.1403e-02 (7.4304e-02)	Acc@1  99.22 ( 97.42)	Acc@5 100.00 ( 99.99)
Epoch: [44][200/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.9133e-02 (7.4350e-02)	Acc@1  99.22 ( 97.42)	Acc@5 100.00 ( 99.99)
Epoch: [44][210/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.3640e-02 (7.4276e-02)	Acc@1  99.22 ( 97.43)	Acc@5 100.00 ( 99.99)
Epoch: [44][220/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.2110e-02 (7.4294e-02)	Acc@1 100.00 ( 97.41)	Acc@5 100.00 ( 99.99)
Epoch: [44][230/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.4342e-02 (7.4298e-02)	Acc@1  98.44 ( 97.41)	Acc@5 100.00 ( 99.99)
Epoch: [44][240/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2561e-01 (7.4712e-02)	Acc@1  96.88 ( 97.39)	Acc@5 100.00 ( 99.99)
Epoch: [44][250/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.3425e-02 (7.4444e-02)	Acc@1  96.88 ( 97.39)	Acc@5 100.00 ( 99.99)
Epoch: [44][260/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.8003e-02 (7.4776e-02)	Acc@1  96.09 ( 97.36)	Acc@5 100.00 ( 99.99)
Epoch: [44][270/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.1737e-02 (7.5039e-02)	Acc@1  98.44 ( 97.36)	Acc@5 100.00 ( 99.99)
Epoch: [44][280/391]	Time  0.111 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.7830e-02 (7.4833e-02)	Acc@1  96.88 ( 97.36)	Acc@5 100.00 ( 99.99)
Epoch: [44][290/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.1655e-02 (7.5013e-02)	Acc@1  97.66 ( 97.35)	Acc@5 100.00 ( 99.99)
Epoch: [44][300/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.5593e-02 (7.4969e-02)	Acc@1  98.44 ( 97.33)	Acc@5 100.00 ( 99.99)
Epoch: [44][310/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.0078e-02 (7.5279e-02)	Acc@1  98.44 ( 97.33)	Acc@5 100.00 ( 99.99)
Epoch: [44][320/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1212e-01 (7.5421e-02)	Acc@1  94.53 ( 97.32)	Acc@5 100.00 ( 99.99)
Epoch: [44][330/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.0791e-02 (7.5619e-02)	Acc@1  96.88 ( 97.30)	Acc@5 100.00 ( 99.99)
Epoch: [44][340/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.2042e-02 (7.5716e-02)	Acc@1  96.88 ( 97.31)	Acc@5 100.00 ( 99.99)
Epoch: [44][350/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.4321e-02 (7.5877e-02)	Acc@1  99.22 ( 97.30)	Acc@5 100.00 ( 99.99)
Epoch: [44][360/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.1096e-02 (7.6466e-02)	Acc@1  97.66 ( 97.29)	Acc@5 100.00 ( 99.99)
Epoch: [44][370/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.0106e-02 (7.6340e-02)	Acc@1  99.22 ( 97.30)	Acc@5 100.00 ( 99.99)
Epoch: [44][380/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.7831e-02 (7.6381e-02)	Acc@1  97.66 ( 97.29)	Acc@5 100.00 ( 99.99)
Epoch: [44][390/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.9429e-02 (7.6092e-02)	Acc@1  98.75 ( 97.30)	Acc@5 100.00 ( 99.99)
## e[44] optimizer.zero_grad (sum) time: 0.3534669876098633
## e[44]       loss.backward (sum) time: 13.777232885360718
## e[44]      optimizer.step (sum) time: 2.834926128387451
## epoch[44] training(only) time: 42.6948926448822
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 3.3423e-01 (3.3423e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.039 ( 0.052)	Loss 1.6016e-01 (3.3342e-01)	Acc@1  96.00 ( 91.00)	Acc@5 100.00 ( 99.55)
Test: [ 20/100]	Time  0.039 ( 0.045)	Loss 5.0488e-01 (3.5749e-01)	Acc@1  88.00 ( 90.71)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.037 ( 0.042)	Loss 3.5596e-01 (3.7928e-01)	Acc@1  88.00 ( 90.45)	Acc@5 100.00 ( 99.61)
Test: [ 40/100]	Time  0.039 ( 0.041)	Loss 2.9810e-01 (3.8268e-01)	Acc@1  92.00 ( 90.44)	Acc@5  99.00 ( 99.61)
Test: [ 50/100]	Time  0.038 ( 0.041)	Loss 1.7249e-01 (3.7562e-01)	Acc@1  93.00 ( 90.65)	Acc@5 100.00 ( 99.59)
Test: [ 60/100]	Time  0.036 ( 0.040)	Loss 5.1562e-01 (3.6818e-01)	Acc@1  93.00 ( 90.82)	Acc@5  99.00 ( 99.61)
Test: [ 70/100]	Time  0.036 ( 0.039)	Loss 4.9902e-01 (3.6445e-01)	Acc@1  90.00 ( 90.90)	Acc@5  99.00 ( 99.63)
Test: [ 80/100]	Time  0.036 ( 0.039)	Loss 2.2534e-01 (3.6293e-01)	Acc@1  92.00 ( 90.88)	Acc@5 100.00 ( 99.65)
Test: [ 90/100]	Time  0.039 ( 0.039)	Loss 2.4573e-01 (3.5659e-01)	Acc@1  94.00 ( 90.97)	Acc@5 100.00 ( 99.66)
 * Acc@1 90.970 Acc@5 99.680
### epoch[44] execution time: 46.67111825942993
EPOCH 45
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [45][  0/391]	Time  0.308 ( 0.308)	Data  0.212 ( 0.212)	Loss 1.0242e-01 (1.0242e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [45][ 10/391]	Time  0.106 ( 0.127)	Data  0.001 ( 0.023)	Loss 5.3772e-02 (6.6849e-02)	Acc@1  99.22 ( 97.87)	Acc@5 100.00 (100.00)
Epoch: [45][ 20/391]	Time  0.107 ( 0.118)	Data  0.001 ( 0.014)	Loss 1.0016e-01 (7.2980e-02)	Acc@1  96.88 ( 97.51)	Acc@5 100.00 (100.00)
Epoch: [45][ 30/391]	Time  0.105 ( 0.115)	Data  0.001 ( 0.011)	Loss 2.9495e-02 (6.8304e-02)	Acc@1  99.22 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [45][ 40/391]	Time  0.098 ( 0.113)	Data  0.001 ( 0.009)	Loss 6.6772e-02 (7.1544e-02)	Acc@1  97.66 ( 97.58)	Acc@5 100.00 ( 99.98)
Epoch: [45][ 50/391]	Time  0.106 ( 0.112)	Data  0.001 ( 0.008)	Loss 3.2593e-02 (6.9484e-02)	Acc@1  99.22 ( 97.55)	Acc@5 100.00 ( 99.98)
Epoch: [45][ 60/391]	Time  0.104 ( 0.111)	Data  0.001 ( 0.008)	Loss 5.2704e-02 (6.6750e-02)	Acc@1  98.44 ( 97.66)	Acc@5 100.00 ( 99.99)
Epoch: [45][ 70/391]	Time  0.107 ( 0.111)	Data  0.001 ( 0.007)	Loss 5.7159e-02 (6.7107e-02)	Acc@1  97.66 ( 97.59)	Acc@5 100.00 ( 99.99)
Epoch: [45][ 80/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.007)	Loss 3.9154e-02 (6.6436e-02)	Acc@1  98.44 ( 97.62)	Acc@5 100.00 ( 99.99)
Epoch: [45][ 90/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 5.6976e-02 (6.6035e-02)	Acc@1  97.66 ( 97.60)	Acc@5 100.00 ( 99.97)
Epoch: [45][100/391]	Time  0.104 ( 0.110)	Data  0.001 ( 0.006)	Loss 9.1492e-02 (6.4980e-02)	Acc@1  95.31 ( 97.62)	Acc@5 100.00 ( 99.98)
Epoch: [45][110/391]	Time  0.109 ( 0.110)	Data  0.001 ( 0.006)	Loss 6.6650e-02 (6.6187e-02)	Acc@1  98.44 ( 97.62)	Acc@5 100.00 ( 99.97)
Epoch: [45][120/391]	Time  0.109 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.7700e-02 (6.6180e-02)	Acc@1  99.22 ( 97.63)	Acc@5 100.00 ( 99.97)
Epoch: [45][130/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.2634e-01 (6.6747e-02)	Acc@1  97.66 ( 97.64)	Acc@5  99.22 ( 99.96)
Epoch: [45][140/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.0022e-01 (6.7621e-02)	Acc@1  96.88 ( 97.61)	Acc@5 100.00 ( 99.96)
Epoch: [45][150/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.006)	Loss 6.6406e-02 (6.8093e-02)	Acc@1  98.44 ( 97.59)	Acc@5 100.00 ( 99.96)
Epoch: [45][160/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.6987e-02 (6.8188e-02)	Acc@1 100.00 ( 97.61)	Acc@5 100.00 ( 99.97)
Epoch: [45][170/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.8665e-02 (6.8262e-02)	Acc@1  97.66 ( 97.61)	Acc@5 100.00 ( 99.97)
Epoch: [45][180/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.6407e-02 (6.9047e-02)	Acc@1 100.00 ( 97.59)	Acc@5 100.00 ( 99.97)
Epoch: [45][190/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.2712e-02 (6.9572e-02)	Acc@1  96.88 ( 97.55)	Acc@5 100.00 ( 99.97)
Epoch: [45][200/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0522e-01 (7.0888e-02)	Acc@1  96.88 ( 97.50)	Acc@5 100.00 ( 99.97)
Epoch: [45][210/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.8796e-02 (7.1967e-02)	Acc@1  96.09 ( 97.43)	Acc@5 100.00 ( 99.97)
Epoch: [45][220/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.0566e-02 (7.2349e-02)	Acc@1  96.88 ( 97.43)	Acc@5 100.00 ( 99.97)
Epoch: [45][230/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.1952e-02 (7.1889e-02)	Acc@1  99.22 ( 97.45)	Acc@5 100.00 ( 99.97)
Epoch: [45][240/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0492e-01 (7.1638e-02)	Acc@1  94.53 ( 97.45)	Acc@5 100.00 ( 99.97)
Epoch: [45][250/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.7811e-02 (7.1611e-02)	Acc@1  99.22 ( 97.46)	Acc@5 100.00 ( 99.98)
Epoch: [45][260/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.2969e-02 (7.0767e-02)	Acc@1  98.44 ( 97.50)	Acc@5 100.00 ( 99.98)
Epoch: [45][270/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.1462e-02 (7.1096e-02)	Acc@1  97.66 ( 97.47)	Acc@5 100.00 ( 99.98)
Epoch: [45][280/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.6985e-02 (7.1182e-02)	Acc@1  96.09 ( 97.46)	Acc@5 100.00 ( 99.98)
Epoch: [45][290/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.7588e-02 (7.1580e-02)	Acc@1 100.00 ( 97.47)	Acc@5 100.00 ( 99.98)
Epoch: [45][300/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.4788e-02 (7.1815e-02)	Acc@1  97.66 ( 97.46)	Acc@5 100.00 ( 99.98)
Epoch: [45][310/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.1431e-02 (7.2760e-02)	Acc@1  97.66 ( 97.43)	Acc@5 100.00 ( 99.98)
Epoch: [45][320/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2854e-01 (7.2985e-02)	Acc@1  92.97 ( 97.42)	Acc@5 100.00 ( 99.98)
Epoch: [45][330/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.1675e-02 (7.3186e-02)	Acc@1  97.66 ( 97.42)	Acc@5 100.00 ( 99.98)
Epoch: [45][340/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.2805e-02 (7.2906e-02)	Acc@1  98.44 ( 97.44)	Acc@5 100.00 ( 99.98)
Epoch: [45][350/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.5276e-02 (7.2779e-02)	Acc@1  96.88 ( 97.44)	Acc@5 100.00 ( 99.98)
Epoch: [45][360/391]	Time  0.101 ( 0.109)	Data  0.002 ( 0.005)	Loss 7.5378e-02 (7.2932e-02)	Acc@1  97.66 ( 97.42)	Acc@5 100.00 ( 99.98)
Epoch: [45][370/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.4473e-02 (7.3239e-02)	Acc@1  96.09 ( 97.41)	Acc@5 100.00 ( 99.98)
Epoch: [45][380/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5088e-01 (7.3414e-02)	Acc@1  96.09 ( 97.41)	Acc@5 100.00 ( 99.98)
Epoch: [45][390/391]	Time  0.099 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2317e-01 (7.3111e-02)	Acc@1  97.50 ( 97.43)	Acc@5 100.00 ( 99.98)
## e[45] optimizer.zero_grad (sum) time: 0.34871387481689453
## e[45]       loss.backward (sum) time: 13.74765133857727
## e[45]      optimizer.step (sum) time: 2.8387486934661865
## epoch[45] training(only) time: 42.61927676200867
# Switched to evaluate mode...
Test: [  0/100]	Time  0.239 ( 0.239)	Loss 3.2446e-01 (3.2446e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.038 ( 0.056)	Loss 1.9531e-01 (3.3082e-01)	Acc@1  95.00 ( 90.91)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.040 ( 0.047)	Loss 5.0049e-01 (3.6555e-01)	Acc@1  88.00 ( 90.43)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.036 ( 0.044)	Loss 3.7427e-01 (3.8615e-01)	Acc@1  87.00 ( 90.42)	Acc@5 100.00 ( 99.58)
Test: [ 40/100]	Time  0.039 ( 0.042)	Loss 2.9004e-01 (3.8215e-01)	Acc@1  91.00 ( 90.41)	Acc@5  99.00 ( 99.59)
Test: [ 50/100]	Time  0.036 ( 0.041)	Loss 1.7017e-01 (3.7434e-01)	Acc@1  95.00 ( 90.69)	Acc@5 100.00 ( 99.59)
Test: [ 60/100]	Time  0.036 ( 0.041)	Loss 5.1025e-01 (3.6894e-01)	Acc@1  93.00 ( 90.89)	Acc@5  99.00 ( 99.64)
Test: [ 70/100]	Time  0.037 ( 0.040)	Loss 5.6836e-01 (3.6785e-01)	Acc@1  88.00 ( 90.86)	Acc@5  99.00 ( 99.66)
Test: [ 80/100]	Time  0.037 ( 0.040)	Loss 2.5342e-01 (3.6633e-01)	Acc@1  90.00 ( 90.85)	Acc@5 100.00 ( 99.65)
Test: [ 90/100]	Time  0.038 ( 0.040)	Loss 2.2498e-01 (3.5764e-01)	Acc@1  94.00 ( 90.93)	Acc@5 100.00 ( 99.66)
 * Acc@1 90.860 Acc@5 99.680
### epoch[45] execution time: 46.64282441139221
EPOCH 46
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [46][  0/391]	Time  0.264 ( 0.264)	Data  0.167 ( 0.167)	Loss 6.2927e-02 (6.2927e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [46][ 10/391]	Time  0.106 ( 0.121)	Data  0.001 ( 0.018)	Loss 7.8613e-02 (7.5555e-02)	Acc@1  95.31 ( 97.44)	Acc@5 100.00 (100.00)
Epoch: [46][ 20/391]	Time  0.108 ( 0.115)	Data  0.001 ( 0.012)	Loss 3.8269e-02 (6.6775e-02)	Acc@1  99.22 ( 97.77)	Acc@5 100.00 (100.00)
Epoch: [46][ 30/391]	Time  0.108 ( 0.113)	Data  0.001 ( 0.009)	Loss 1.5063e-01 (7.0396e-02)	Acc@1  95.31 ( 97.68)	Acc@5 100.00 (100.00)
Epoch: [46][ 40/391]	Time  0.107 ( 0.112)	Data  0.001 ( 0.008)	Loss 1.0406e-01 (7.5356e-02)	Acc@1  95.31 ( 97.39)	Acc@5 100.00 (100.00)
Epoch: [46][ 50/391]	Time  0.107 ( 0.111)	Data  0.001 ( 0.007)	Loss 4.3610e-02 (7.6865e-02)	Acc@1  99.22 ( 97.32)	Acc@5 100.00 ( 99.98)
Epoch: [46][ 60/391]	Time  0.109 ( 0.111)	Data  0.001 ( 0.007)	Loss 4.4250e-02 (7.5757e-02)	Acc@1  97.66 ( 97.39)	Acc@5 100.00 ( 99.99)
Epoch: [46][ 70/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.006)	Loss 8.0444e-02 (7.2540e-02)	Acc@1  95.31 ( 97.57)	Acc@5 100.00 ( 99.99)
Epoch: [46][ 80/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.006)	Loss 4.0375e-02 (7.0365e-02)	Acc@1  99.22 ( 97.62)	Acc@5 100.00 ( 99.99)
Epoch: [46][ 90/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.1133e-01 (7.0459e-02)	Acc@1  96.88 ( 97.58)	Acc@5 100.00 ( 99.98)
Epoch: [46][100/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.0846e-01 (7.0729e-02)	Acc@1  95.31 ( 97.53)	Acc@5 100.00 ( 99.98)
Epoch: [46][110/391]	Time  0.104 ( 0.110)	Data  0.001 ( 0.006)	Loss 8.2458e-02 (7.0403e-02)	Acc@1  99.22 ( 97.56)	Acc@5 100.00 ( 99.99)
Epoch: [46][120/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.005)	Loss 1.0925e-01 (7.1793e-02)	Acc@1  96.09 ( 97.50)	Acc@5 100.00 ( 99.99)
Epoch: [46][130/391]	Time  0.110 ( 0.110)	Data  0.001 ( 0.005)	Loss 7.0618e-02 (7.1677e-02)	Acc@1  96.88 ( 97.50)	Acc@5 100.00 ( 99.99)
Epoch: [46][140/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.4656e-02 (7.0502e-02)	Acc@1  96.88 ( 97.53)	Acc@5 100.00 ( 99.99)
Epoch: [46][150/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.6743e-02 (7.0413e-02)	Acc@1  99.22 ( 97.54)	Acc@5 100.00 ( 99.99)
Epoch: [46][160/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.1777e-02 (7.0868e-02)	Acc@1  97.66 ( 97.52)	Acc@5 100.00 ( 99.99)
Epoch: [46][170/391]	Time  0.100 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.8035e-02 (7.1398e-02)	Acc@1  98.44 ( 97.50)	Acc@5 100.00 ( 99.99)
Epoch: [46][180/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.5735e-02 (7.0628e-02)	Acc@1  97.66 ( 97.52)	Acc@5 100.00 ( 99.99)
Epoch: [46][190/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.1309e-02 (7.0649e-02)	Acc@1  95.31 ( 97.52)	Acc@5 100.00 ( 99.99)
Epoch: [46][200/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2439e-01 (7.0366e-02)	Acc@1  94.53 ( 97.51)	Acc@5 100.00 ( 99.99)
Epoch: [46][210/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.6091e-02 (7.0583e-02)	Acc@1  97.66 ( 97.50)	Acc@5 100.00 ( 99.99)
Epoch: [46][220/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.4973e-02 (7.0489e-02)	Acc@1  99.22 ( 97.50)	Acc@5 100.00 ( 99.99)
Epoch: [46][230/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.9834e-02 (7.1129e-02)	Acc@1  96.88 ( 97.49)	Acc@5 100.00 ( 99.99)
Epoch: [46][240/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.2205e-02 (7.1200e-02)	Acc@1  98.44 ( 97.50)	Acc@5 100.00 ( 99.99)
Epoch: [46][250/391]	Time  0.099 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4734e-01 (7.1817e-02)	Acc@1  94.53 ( 97.47)	Acc@5  99.22 ( 99.99)
Epoch: [46][260/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1621e-01 (7.2219e-02)	Acc@1  97.66 ( 97.46)	Acc@5 100.00 ( 99.99)
Epoch: [46][270/391]	Time  0.100 ( 0.109)	Data  0.002 ( 0.005)	Loss 8.6060e-02 (7.2189e-02)	Acc@1  97.66 ( 97.45)	Acc@5 100.00 ( 99.99)
Epoch: [46][280/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.3130e-02 (7.2473e-02)	Acc@1  97.66 ( 97.44)	Acc@5 100.00 ( 99.99)
Epoch: [46][290/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.6478e-02 (7.2059e-02)	Acc@1  98.44 ( 97.45)	Acc@5 100.00 ( 99.99)
Epoch: [46][300/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.7290e-02 (7.2795e-02)	Acc@1  95.31 ( 97.40)	Acc@5 100.00 ( 99.99)
Epoch: [46][310/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.8472e-02 (7.2691e-02)	Acc@1  96.88 ( 97.40)	Acc@5 100.00 ( 99.99)
Epoch: [46][320/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.9418e-02 (7.2661e-02)	Acc@1  98.44 ( 97.41)	Acc@5 100.00 ( 99.99)
Epoch: [46][330/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.5735e-02 (7.2874e-02)	Acc@1  98.44 ( 97.41)	Acc@5  99.22 ( 99.99)
Epoch: [46][340/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.0039e-02 (7.2824e-02)	Acc@1  98.44 ( 97.42)	Acc@5 100.00 ( 99.99)
Epoch: [46][350/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.2338e-02 (7.3058e-02)	Acc@1  97.66 ( 97.41)	Acc@5 100.00 ( 99.99)
Epoch: [46][360/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.0009e-02 (7.3018e-02)	Acc@1  98.44 ( 97.41)	Acc@5 100.00 ( 99.99)
Epoch: [46][370/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.5623e-02 (7.3011e-02)	Acc@1  97.66 ( 97.42)	Acc@5 100.00 ( 99.99)
Epoch: [46][380/391]	Time  0.112 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.0507e-02 (7.2903e-02)	Acc@1  99.22 ( 97.43)	Acc@5 100.00 ( 99.99)
Epoch: [46][390/391]	Time  0.098 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4319e-01 (7.3039e-02)	Acc@1  93.75 ( 97.42)	Acc@5 100.00 ( 99.99)
## e[46] optimizer.zero_grad (sum) time: 0.35201311111450195
## e[46]       loss.backward (sum) time: 13.76006555557251
## e[46]      optimizer.step (sum) time: 2.810133934020996
## epoch[46] training(only) time: 42.722243785858154
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 3.2812e-01 (3.2812e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.038 ( 0.052)	Loss 2.3218e-01 (3.1864e-01)	Acc@1  95.00 ( 90.82)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.040 ( 0.045)	Loss 5.2295e-01 (3.5899e-01)	Acc@1  87.00 ( 90.00)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.036 ( 0.042)	Loss 3.8599e-01 (3.8597e-01)	Acc@1  86.00 ( 90.06)	Acc@5 100.00 ( 99.58)
Test: [ 40/100]	Time  0.037 ( 0.041)	Loss 2.9907e-01 (3.8387e-01)	Acc@1  92.00 ( 90.17)	Acc@5  99.00 ( 99.59)
Test: [ 50/100]	Time  0.035 ( 0.040)	Loss 2.4084e-01 (3.7677e-01)	Acc@1  93.00 ( 90.45)	Acc@5 100.00 ( 99.61)
Test: [ 60/100]	Time  0.037 ( 0.040)	Loss 4.3555e-01 (3.7105e-01)	Acc@1  94.00 ( 90.56)	Acc@5  99.00 ( 99.64)
Test: [ 70/100]	Time  0.038 ( 0.039)	Loss 5.4883e-01 (3.6603e-01)	Acc@1  90.00 ( 90.66)	Acc@5  99.00 ( 99.65)
Test: [ 80/100]	Time  0.035 ( 0.039)	Loss 2.5269e-01 (3.6527e-01)	Acc@1  93.00 ( 90.67)	Acc@5 100.00 ( 99.64)
Test: [ 90/100]	Time  0.037 ( 0.039)	Loss 2.5220e-01 (3.5548e-01)	Acc@1  94.00 ( 90.79)	Acc@5 100.00 ( 99.65)
 * Acc@1 90.740 Acc@5 99.660
### epoch[46] execution time: 46.689268827438354
EPOCH 47
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [47][  0/391]	Time  0.261 ( 0.261)	Data  0.165 ( 0.165)	Loss 8.6731e-02 (8.6731e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [47][ 10/391]	Time  0.107 ( 0.121)	Data  0.001 ( 0.018)	Loss 1.0297e-01 (7.0085e-02)	Acc@1  94.53 ( 97.37)	Acc@5 100.00 (100.00)
Epoch: [47][ 20/391]	Time  0.106 ( 0.115)	Data  0.001 ( 0.012)	Loss 7.5745e-02 (7.0467e-02)	Acc@1  96.88 ( 97.47)	Acc@5 100.00 (100.00)
Epoch: [47][ 30/391]	Time  0.106 ( 0.113)	Data  0.001 ( 0.009)	Loss 7.5989e-02 (7.0434e-02)	Acc@1  96.88 ( 97.40)	Acc@5 100.00 (100.00)
Epoch: [47][ 40/391]	Time  0.103 ( 0.112)	Data  0.001 ( 0.008)	Loss 5.8075e-02 (6.8732e-02)	Acc@1  98.44 ( 97.48)	Acc@5 100.00 (100.00)
Epoch: [47][ 50/391]	Time  0.106 ( 0.111)	Data  0.001 ( 0.007)	Loss 1.1737e-01 (7.1879e-02)	Acc@1  96.88 ( 97.44)	Acc@5  99.22 ( 99.97)
Epoch: [47][ 60/391]	Time  0.107 ( 0.111)	Data  0.001 ( 0.007)	Loss 3.2532e-02 (6.9583e-02)	Acc@1  98.44 ( 97.49)	Acc@5 100.00 ( 99.97)
Epoch: [47][ 70/391]	Time  0.104 ( 0.110)	Data  0.001 ( 0.006)	Loss 7.6538e-02 (6.8379e-02)	Acc@1  97.66 ( 97.47)	Acc@5 100.00 ( 99.98)
Epoch: [47][ 80/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 7.7271e-02 (6.8065e-02)	Acc@1  97.66 ( 97.48)	Acc@5 100.00 ( 99.98)
Epoch: [47][ 90/391]	Time  0.109 ( 0.110)	Data  0.001 ( 0.006)	Loss 5.8563e-02 (6.7982e-02)	Acc@1  98.44 ( 97.51)	Acc@5 100.00 ( 99.98)
Epoch: [47][100/391]	Time  0.103 ( 0.110)	Data  0.001 ( 0.006)	Loss 2.8259e-02 (6.8187e-02)	Acc@1  99.22 ( 97.53)	Acc@5 100.00 ( 99.98)
Epoch: [47][110/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 5.3894e-02 (6.8133e-02)	Acc@1  97.66 ( 97.54)	Acc@5 100.00 ( 99.99)
Epoch: [47][120/391]	Time  0.109 ( 0.110)	Data  0.001 ( 0.005)	Loss 3.4851e-02 (6.7361e-02)	Acc@1  97.66 ( 97.55)	Acc@5 100.00 ( 99.99)
Epoch: [47][130/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.2336e-02 (6.7314e-02)	Acc@1  97.66 ( 97.54)	Acc@5 100.00 ( 99.99)
Epoch: [47][140/391]	Time  0.106 ( 0.109)	Data  0.002 ( 0.005)	Loss 7.2083e-02 (6.7316e-02)	Acc@1  97.66 ( 97.52)	Acc@5 100.00 ( 99.99)
Epoch: [47][150/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.0507e-02 (6.7001e-02)	Acc@1  96.88 ( 97.53)	Acc@5 100.00 ( 99.99)
Epoch: [47][160/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.0232e-02 (6.6353e-02)	Acc@1  98.44 ( 97.55)	Acc@5 100.00 ( 99.99)
Epoch: [47][170/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.3914e-02 (6.6313e-02)	Acc@1  96.09 ( 97.55)	Acc@5 100.00 ( 99.99)
Epoch: [47][180/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.9892e-02 (6.5852e-02)	Acc@1  99.22 ( 97.59)	Acc@5 100.00 ( 99.99)
Epoch: [47][190/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.2092e-02 (6.5792e-02)	Acc@1  96.88 ( 97.59)	Acc@5 100.00 ( 99.99)
Epoch: [47][200/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.1848e-02 (6.5943e-02)	Acc@1  96.88 ( 97.58)	Acc@5 100.00 ( 99.99)
Epoch: [47][210/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.2593e-02 (6.6315e-02)	Acc@1  99.22 ( 97.57)	Acc@5 100.00 ( 99.99)
Epoch: [47][220/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.5032e-02 (6.6595e-02)	Acc@1  95.31 ( 97.55)	Acc@5 100.00 ( 99.99)
Epoch: [47][230/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.8054e-02 (6.7196e-02)	Acc@1  96.88 ( 97.53)	Acc@5 100.00 ( 99.99)
Epoch: [47][240/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.7139e-02 (6.7545e-02)	Acc@1  96.88 ( 97.53)	Acc@5 100.00 ( 99.98)
Epoch: [47][250/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.0314e-02 (6.7115e-02)	Acc@1  99.22 ( 97.56)	Acc@5 100.00 ( 99.98)
Epoch: [47][260/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.7603e-02 (6.7297e-02)	Acc@1 100.00 ( 97.54)	Acc@5 100.00 ( 99.99)
Epoch: [47][270/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.3914e-02 (6.7988e-02)	Acc@1  96.09 ( 97.51)	Acc@5 100.00 ( 99.99)
Epoch: [47][280/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.9946e-02 (6.8232e-02)	Acc@1  96.88 ( 97.49)	Acc@5 100.00 ( 99.99)
Epoch: [47][290/391]	Time  0.111 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1188e-01 (6.8402e-02)	Acc@1  96.09 ( 97.49)	Acc@5 100.00 ( 99.99)
Epoch: [47][300/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.4707e-02 (6.7873e-02)	Acc@1  97.66 ( 97.51)	Acc@5 100.00 ( 99.99)
Epoch: [47][310/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0596e-01 (6.8521e-02)	Acc@1  95.31 ( 97.49)	Acc@5 100.00 ( 99.99)
Epoch: [47][320/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.2267e-02 (6.8283e-02)	Acc@1  99.22 ( 97.50)	Acc@5 100.00 ( 99.99)
Epoch: [47][330/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.7603e-02 (6.8194e-02)	Acc@1  99.22 ( 97.50)	Acc@5 100.00 ( 99.99)
Epoch: [47][340/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.3894e-02 (6.8730e-02)	Acc@1  98.44 ( 97.48)	Acc@5 100.00 ( 99.99)
Epoch: [47][350/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.3131e-02 (6.8687e-02)	Acc@1  98.44 ( 97.48)	Acc@5 100.00 ( 99.99)
Epoch: [47][360/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.2866e-02 (6.8727e-02)	Acc@1  98.44 ( 97.49)	Acc@5 100.00 ( 99.99)
Epoch: [47][370/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.7351e-02 (6.8884e-02)	Acc@1  96.09 ( 97.49)	Acc@5 100.00 ( 99.99)
Epoch: [47][380/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.4688e-02 (6.8883e-02)	Acc@1  96.88 ( 97.49)	Acc@5 100.00 ( 99.99)
Epoch: [47][390/391]	Time  0.101 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.5815e-02 (6.8879e-02)	Acc@1  95.00 ( 97.49)	Acc@5 100.00 ( 99.99)
## e[47] optimizer.zero_grad (sum) time: 0.35376787185668945
## e[47]       loss.backward (sum) time: 13.723197221755981
## e[47]      optimizer.step (sum) time: 2.771627902984619
## epoch[47] training(only) time: 42.627180337905884
# Switched to evaluate mode...
Test: [  0/100]	Time  0.203 ( 0.203)	Loss 2.9126e-01 (2.9126e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.036 ( 0.053)	Loss 2.5903e-01 (3.2816e-01)	Acc@1  95.00 ( 90.91)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.036 ( 0.046)	Loss 5.4932e-01 (3.7422e-01)	Acc@1  86.00 ( 90.10)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.037 ( 0.043)	Loss 3.8721e-01 (3.9493e-01)	Acc@1  88.00 ( 90.35)	Acc@5 100.00 ( 99.55)
Test: [ 40/100]	Time  0.038 ( 0.042)	Loss 3.7720e-01 (3.9500e-01)	Acc@1  90.00 ( 90.32)	Acc@5  99.00 ( 99.56)
Test: [ 50/100]	Time  0.037 ( 0.041)	Loss 3.0322e-01 (3.8891e-01)	Acc@1  92.00 ( 90.57)	Acc@5 100.00 ( 99.59)
Test: [ 60/100]	Time  0.037 ( 0.040)	Loss 4.3066e-01 (3.8028e-01)	Acc@1  93.00 ( 90.66)	Acc@5  99.00 ( 99.59)
Test: [ 70/100]	Time  0.038 ( 0.040)	Loss 5.4004e-01 (3.7678e-01)	Acc@1  89.00 ( 90.77)	Acc@5 100.00 ( 99.62)
Test: [ 80/100]	Time  0.036 ( 0.039)	Loss 2.7856e-01 (3.7290e-01)	Acc@1  90.00 ( 90.70)	Acc@5 100.00 ( 99.63)
Test: [ 90/100]	Time  0.035 ( 0.039)	Loss 2.8833e-01 (3.6557e-01)	Acc@1  93.00 ( 90.67)	Acc@5 100.00 ( 99.65)
 * Acc@1 90.680 Acc@5 99.660
### epoch[47] execution time: 46.61220407485962
EPOCH 48
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [48][  0/391]	Time  0.278 ( 0.278)	Data  0.180 ( 0.180)	Loss 6.7932e-02 (6.7932e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [48][ 10/391]	Time  0.107 ( 0.122)	Data  0.001 ( 0.020)	Loss 4.0649e-02 (6.2433e-02)	Acc@1  99.22 ( 97.87)	Acc@5 100.00 (100.00)
Epoch: [48][ 20/391]	Time  0.108 ( 0.115)	Data  0.001 ( 0.012)	Loss 3.8025e-02 (5.3242e-02)	Acc@1 100.00 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [48][ 30/391]	Time  0.108 ( 0.113)	Data  0.001 ( 0.010)	Loss 1.1664e-01 (5.7189e-02)	Acc@1  95.31 ( 98.24)	Acc@5 100.00 (100.00)
Epoch: [48][ 40/391]	Time  0.105 ( 0.112)	Data  0.001 ( 0.008)	Loss 4.8981e-02 (6.0227e-02)	Acc@1  96.88 ( 98.08)	Acc@5 100.00 (100.00)
Epoch: [48][ 50/391]	Time  0.107 ( 0.111)	Data  0.001 ( 0.008)	Loss 3.5461e-02 (6.2071e-02)	Acc@1  98.44 ( 97.99)	Acc@5 100.00 (100.00)
Epoch: [48][ 60/391]	Time  0.103 ( 0.111)	Data  0.001 ( 0.007)	Loss 8.4045e-02 (6.1091e-02)	Acc@1  96.88 ( 97.95)	Acc@5 100.00 (100.00)
Epoch: [48][ 70/391]	Time  0.104 ( 0.110)	Data  0.001 ( 0.007)	Loss 5.1178e-02 (6.0741e-02)	Acc@1  97.66 ( 97.91)	Acc@5 100.00 (100.00)
Epoch: [48][ 80/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.006)	Loss 5.0934e-02 (5.8366e-02)	Acc@1  99.22 ( 98.02)	Acc@5 100.00 (100.00)
Epoch: [48][ 90/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 4.3060e-02 (5.8782e-02)	Acc@1  99.22 ( 98.03)	Acc@5 100.00 (100.00)
Epoch: [48][100/391]	Time  0.104 ( 0.110)	Data  0.001 ( 0.006)	Loss 5.6183e-02 (5.7940e-02)	Acc@1  97.66 ( 98.01)	Acc@5 100.00 (100.00)
Epoch: [48][110/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.006)	Loss 6.2195e-02 (5.7499e-02)	Acc@1  98.44 ( 98.05)	Acc@5 100.00 (100.00)
Epoch: [48][120/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.006)	Loss 5.4443e-02 (5.8084e-02)	Acc@1  99.22 ( 98.03)	Acc@5 100.00 (100.00)
Epoch: [48][130/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.006)	Loss 9.9365e-02 (5.8534e-02)	Acc@1  96.88 ( 98.04)	Acc@5 100.00 ( 99.99)
Epoch: [48][140/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.2267e-02 (6.0333e-02)	Acc@1  97.66 ( 97.98)	Acc@5 100.00 ( 99.98)
Epoch: [48][150/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.5898e-02 (6.0707e-02)	Acc@1  97.66 ( 97.95)	Acc@5 100.00 ( 99.98)
Epoch: [48][160/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.5308e-02 (6.0179e-02)	Acc@1  96.09 ( 97.97)	Acc@5 100.00 ( 99.99)
Epoch: [48][170/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.1586e-02 (6.0368e-02)	Acc@1  99.22 ( 97.94)	Acc@5 100.00 ( 99.99)
Epoch: [48][180/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.6978e-02 (6.0493e-02)	Acc@1  99.22 ( 97.94)	Acc@5 100.00 ( 99.99)
Epoch: [48][190/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.5176e-02 (6.0152e-02)	Acc@1  96.88 ( 97.94)	Acc@5 100.00 ( 99.99)
Epoch: [48][200/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.5375e-02 (5.9891e-02)	Acc@1  99.22 ( 97.95)	Acc@5 100.00 ( 99.99)
Epoch: [48][210/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.1005e-02 (6.0393e-02)	Acc@1  98.44 ( 97.92)	Acc@5 100.00 ( 99.99)
Epoch: [48][220/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.8572e-02 (6.1490e-02)	Acc@1  97.66 ( 97.89)	Acc@5 100.00 ( 99.99)
Epoch: [48][230/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.1779e-02 (6.1166e-02)	Acc@1  99.22 ( 97.90)	Acc@5 100.00 ( 99.99)
Epoch: [48][240/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.8757e-02 (6.1295e-02)	Acc@1  99.22 ( 97.89)	Acc@5 100.00 ( 99.99)
Epoch: [48][250/391]	Time  0.101 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.6284e-02 (6.1367e-02)	Acc@1  96.88 ( 97.88)	Acc@5 100.00 ( 99.99)
Epoch: [48][260/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.6610e-02 (6.2173e-02)	Acc@1  98.44 ( 97.85)	Acc@5 100.00 ( 99.99)
Epoch: [48][270/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.9164e-02 (6.2066e-02)	Acc@1  98.44 ( 97.84)	Acc@5 100.00 ( 99.99)
Epoch: [48][280/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.1127e-02 (6.2621e-02)	Acc@1  97.66 ( 97.82)	Acc@5 100.00 ( 99.99)
Epoch: [48][290/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.9988e-02 (6.3206e-02)	Acc@1  98.44 ( 97.81)	Acc@5 100.00 ( 99.99)
Epoch: [48][300/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.2847e-02 (6.3204e-02)	Acc@1  98.44 ( 97.79)	Acc@5 100.00 ( 99.99)
Epoch: [48][310/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0748e-01 (6.2941e-02)	Acc@1  96.88 ( 97.81)	Acc@5 100.00 ( 99.99)
Epoch: [48][320/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.3994e-02 (6.3772e-02)	Acc@1  96.88 ( 97.79)	Acc@5 100.00 ( 99.99)
Epoch: [48][330/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.5186e-02 (6.4023e-02)	Acc@1  96.88 ( 97.78)	Acc@5 100.00 ( 99.99)
Epoch: [48][340/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0968e-01 (6.4471e-02)	Acc@1  96.09 ( 97.76)	Acc@5 100.00 ( 99.99)
Epoch: [48][350/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.1177e-02 (6.4452e-02)	Acc@1  98.44 ( 97.76)	Acc@5 100.00 ( 99.99)
Epoch: [48][360/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.5989e-02 (6.4562e-02)	Acc@1  97.66 ( 97.76)	Acc@5 100.00 ( 99.99)
Epoch: [48][370/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.8818e-02 (6.4747e-02)	Acc@1  98.44 ( 97.76)	Acc@5 100.00 ( 99.99)
Epoch: [48][380/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.1829e-02 (6.4681e-02)	Acc@1  96.88 ( 97.75)	Acc@5 100.00 ( 99.99)
Epoch: [48][390/391]	Time  0.101 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.3862e-02 (6.4460e-02)	Acc@1  96.25 ( 97.74)	Acc@5 100.00 ( 99.99)
## e[48] optimizer.zero_grad (sum) time: 0.35692381858825684
## e[48]       loss.backward (sum) time: 13.708978176116943
## e[48]      optimizer.step (sum) time: 2.761992931365967
## epoch[48] training(only) time: 42.578874826431274
# Switched to evaluate mode...
Test: [  0/100]	Time  0.193 ( 0.193)	Loss 2.7979e-01 (2.7979e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.037 ( 0.051)	Loss 2.0056e-01 (3.2911e-01)	Acc@1  94.00 ( 90.73)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.036 ( 0.045)	Loss 4.6802e-01 (3.6976e-01)	Acc@1  88.00 ( 90.33)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.036 ( 0.042)	Loss 3.4570e-01 (3.9392e-01)	Acc@1  88.00 ( 90.29)	Acc@5 100.00 ( 99.61)
Test: [ 40/100]	Time  0.037 ( 0.041)	Loss 3.1982e-01 (3.9548e-01)	Acc@1  92.00 ( 90.24)	Acc@5  99.00 ( 99.61)
Test: [ 50/100]	Time  0.039 ( 0.040)	Loss 2.4121e-01 (3.9042e-01)	Acc@1  92.00 ( 90.51)	Acc@5 100.00 ( 99.59)
Test: [ 60/100]	Time  0.036 ( 0.040)	Loss 4.3359e-01 (3.8792e-01)	Acc@1  94.00 ( 90.62)	Acc@5 100.00 ( 99.59)
Test: [ 70/100]	Time  0.038 ( 0.039)	Loss 5.0830e-01 (3.8153e-01)	Acc@1  89.00 ( 90.73)	Acc@5 100.00 ( 99.63)
Test: [ 80/100]	Time  0.037 ( 0.039)	Loss 2.5317e-01 (3.7912e-01)	Acc@1  92.00 ( 90.78)	Acc@5 100.00 ( 99.65)
Test: [ 90/100]	Time  0.038 ( 0.039)	Loss 2.9443e-01 (3.7143e-01)	Acc@1  92.00 ( 90.88)	Acc@5 100.00 ( 99.66)
 * Acc@1 90.880 Acc@5 99.680
### epoch[48] execution time: 46.53613829612732
EPOCH 49
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [49][  0/391]	Time  0.284 ( 0.284)	Data  0.173 ( 0.173)	Loss 7.2266e-02 (7.2266e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [49][ 10/391]	Time  0.109 ( 0.124)	Data  0.001 ( 0.019)	Loss 3.9490e-02 (5.0980e-02)	Acc@1  97.66 ( 98.01)	Acc@5 100.00 (100.00)
Epoch: [49][ 20/391]	Time  0.106 ( 0.116)	Data  0.001 ( 0.012)	Loss 8.2947e-02 (6.0084e-02)	Acc@1  97.66 ( 97.92)	Acc@5 100.00 (100.00)
Epoch: [49][ 30/391]	Time  0.107 ( 0.114)	Data  0.001 ( 0.009)	Loss 3.3905e-02 (5.8460e-02)	Acc@1  98.44 ( 98.01)	Acc@5 100.00 (100.00)
Epoch: [49][ 40/391]	Time  0.106 ( 0.113)	Data  0.001 ( 0.008)	Loss 5.3040e-02 (5.8328e-02)	Acc@1  97.66 ( 98.02)	Acc@5 100.00 ( 99.98)
Epoch: [49][ 50/391]	Time  0.105 ( 0.112)	Data  0.001 ( 0.007)	Loss 6.2683e-02 (5.8000e-02)	Acc@1  96.09 ( 98.04)	Acc@5 100.00 ( 99.98)
Epoch: [49][ 60/391]	Time  0.103 ( 0.111)	Data  0.001 ( 0.007)	Loss 2.3758e-02 (5.8120e-02)	Acc@1 100.00 ( 98.09)	Acc@5 100.00 ( 99.99)
Epoch: [49][ 70/391]	Time  0.106 ( 0.111)	Data  0.001 ( 0.006)	Loss 3.7933e-02 (5.8646e-02)	Acc@1  99.22 ( 98.04)	Acc@5 100.00 ( 99.99)
Epoch: [49][ 80/391]	Time  0.108 ( 0.111)	Data  0.001 ( 0.006)	Loss 2.2934e-02 (5.8651e-02)	Acc@1  99.22 ( 98.00)	Acc@5 100.00 ( 99.99)
Epoch: [49][ 90/391]	Time  0.101 ( 0.110)	Data  0.001 ( 0.006)	Loss 5.4840e-02 (5.9884e-02)	Acc@1  97.66 ( 97.93)	Acc@5 100.00 ( 99.99)
Epoch: [49][100/391]	Time  0.102 ( 0.110)	Data  0.001 ( 0.006)	Loss 3.1769e-02 (5.9256e-02)	Acc@1  99.22 ( 97.96)	Acc@5 100.00 ( 99.99)
Epoch: [49][110/391]	Time  0.101 ( 0.110)	Data  0.001 ( 0.006)	Loss 3.1616e-02 (5.8510e-02)	Acc@1  99.22 ( 97.99)	Acc@5 100.00 ( 99.99)
Epoch: [49][120/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.005)	Loss 8.1970e-02 (6.0241e-02)	Acc@1  98.44 ( 97.97)	Acc@5 100.00 ( 99.99)
Epoch: [49][130/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.0812e-02 (5.9901e-02)	Acc@1  97.66 ( 97.97)	Acc@5 100.00 ( 99.99)
Epoch: [49][140/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.9906e-02 (5.9228e-02)	Acc@1  97.66 ( 97.97)	Acc@5 100.00 ( 99.99)
Epoch: [49][150/391]	Time  0.101 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.5563e-02 (5.9525e-02)	Acc@1  99.22 ( 97.97)	Acc@5 100.00 ( 99.99)
Epoch: [49][160/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.7781e-02 (5.9483e-02)	Acc@1 100.00 ( 97.98)	Acc@5 100.00 (100.00)
Epoch: [49][170/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.6062e-02 (5.9276e-02)	Acc@1  99.22 ( 98.03)	Acc@5 100.00 (100.00)
Epoch: [49][180/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.9753e-02 (5.9623e-02)	Acc@1  98.44 ( 98.01)	Acc@5 100.00 (100.00)
Epoch: [49][190/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.6355e-02 (5.9877e-02)	Acc@1  98.44 ( 98.01)	Acc@5 100.00 (100.00)
Epoch: [49][200/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.7068e-02 (5.9437e-02)	Acc@1  96.88 ( 98.03)	Acc@5 100.00 (100.00)
Epoch: [49][210/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.4849e-02 (5.9714e-02)	Acc@1  94.53 ( 97.99)	Acc@5 100.00 (100.00)
Epoch: [49][220/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.1514e-02 (5.9324e-02)	Acc@1  99.22 ( 98.03)	Acc@5 100.00 (100.00)
Epoch: [49][230/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.1472e-02 (5.9773e-02)	Acc@1  97.66 ( 98.01)	Acc@5 100.00 (100.00)
Epoch: [49][240/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.6313e-02 (5.9978e-02)	Acc@1  96.88 ( 98.00)	Acc@5 100.00 (100.00)
Epoch: [49][250/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.1594e-02 (5.9902e-02)	Acc@1  96.88 ( 97.99)	Acc@5 100.00 (100.00)
Epoch: [49][260/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.4108e-02 (6.0272e-02)	Acc@1  98.44 ( 97.96)	Acc@5 100.00 ( 99.99)
Epoch: [49][270/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.8889e-02 (5.9819e-02)	Acc@1  97.66 ( 97.97)	Acc@5 100.00 ( 99.99)
Epoch: [49][280/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.0716e-02 (5.9621e-02)	Acc@1  99.22 ( 97.98)	Acc@5 100.00 ( 99.99)
Epoch: [49][290/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.7749e-02 (5.9682e-02)	Acc@1  96.88 ( 97.98)	Acc@5 100.00 ( 99.99)
Epoch: [49][300/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.9561e-02 (5.9196e-02)	Acc@1  98.44 ( 98.00)	Acc@5 100.00 ( 99.99)
Epoch: [49][310/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.9408e-02 (5.9453e-02)	Acc@1  97.66 ( 97.98)	Acc@5 100.00 ( 99.99)
Epoch: [49][320/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.2715e-02 (5.9641e-02)	Acc@1 100.00 ( 97.98)	Acc@5 100.00 (100.00)
Epoch: [49][330/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.8879e-02 (5.9566e-02)	Acc@1  98.44 ( 97.98)	Acc@5 100.00 (100.00)
Epoch: [49][340/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.4220e-02 (5.9601e-02)	Acc@1  99.22 ( 97.98)	Acc@5 100.00 (100.00)
Epoch: [49][350/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0492e-01 (5.9263e-02)	Acc@1  95.31 ( 97.99)	Acc@5 100.00 (100.00)
Epoch: [49][360/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.0740e-02 (5.9502e-02)	Acc@1  96.09 ( 97.97)	Acc@5 100.00 (100.00)
Epoch: [49][370/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.6792e-02 (5.9982e-02)	Acc@1  95.31 ( 97.94)	Acc@5 100.00 (100.00)
Epoch: [49][380/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.4543e-02 (6.0181e-02)	Acc@1  95.31 ( 97.93)	Acc@5 100.00 (100.00)
Epoch: [49][390/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.9604e-01 (6.0892e-02)	Acc@1  93.75 ( 97.91)	Acc@5 100.00 (100.00)
## e[49] optimizer.zero_grad (sum) time: 0.35142040252685547
## e[49]       loss.backward (sum) time: 13.729837417602539
## e[49]      optimizer.step (sum) time: 2.7841172218322754
## epoch[49] training(only) time: 42.69317030906677
# Switched to evaluate mode...
Test: [  0/100]	Time  0.181 ( 0.181)	Loss 3.6035e-01 (3.6035e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.036 ( 0.051)	Loss 2.0093e-01 (3.5688e-01)	Acc@1  94.00 ( 90.55)	Acc@5 100.00 ( 99.55)
Test: [ 20/100]	Time  0.036 ( 0.045)	Loss 5.0537e-01 (3.9106e-01)	Acc@1  86.00 ( 90.05)	Acc@5 100.00 ( 99.48)
Test: [ 30/100]	Time  0.037 ( 0.042)	Loss 3.2861e-01 (4.1236e-01)	Acc@1  89.00 ( 90.23)	Acc@5 100.00 ( 99.48)
Test: [ 40/100]	Time  0.036 ( 0.041)	Loss 4.0088e-01 (4.1673e-01)	Acc@1  92.00 ( 90.10)	Acc@5  99.00 ( 99.51)
Test: [ 50/100]	Time  0.038 ( 0.040)	Loss 2.4976e-01 (4.1233e-01)	Acc@1  91.00 ( 90.33)	Acc@5 100.00 ( 99.57)
Test: [ 60/100]	Time  0.036 ( 0.040)	Loss 4.8022e-01 (4.0615e-01)	Acc@1  93.00 ( 90.43)	Acc@5  99.00 ( 99.56)
Test: [ 70/100]	Time  0.037 ( 0.039)	Loss 5.4004e-01 (3.9827e-01)	Acc@1  89.00 ( 90.54)	Acc@5  99.00 ( 99.59)
Test: [ 80/100]	Time  0.037 ( 0.039)	Loss 2.9224e-01 (3.9455e-01)	Acc@1  90.00 ( 90.57)	Acc@5 100.00 ( 99.62)
Test: [ 90/100]	Time  0.035 ( 0.039)	Loss 2.6660e-01 (3.8449e-01)	Acc@1  94.00 ( 90.64)	Acc@5 100.00 ( 99.63)
 * Acc@1 90.650 Acc@5 99.640
### epoch[49] execution time: 46.66812181472778
EPOCH 50
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [50][  0/391]	Time  0.271 ( 0.271)	Data  0.169 ( 0.169)	Loss 3.2166e-02 (3.2166e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [50][ 10/391]	Time  0.104 ( 0.122)	Data  0.001 ( 0.019)	Loss 7.4768e-02 (6.6772e-02)	Acc@1  98.44 ( 97.73)	Acc@5 100.00 (100.00)
Epoch: [50][ 20/391]	Time  0.104 ( 0.115)	Data  0.001 ( 0.012)	Loss 7.1655e-02 (6.7605e-02)	Acc@1  97.66 ( 97.81)	Acc@5 100.00 (100.00)
Epoch: [50][ 30/391]	Time  0.109 ( 0.113)	Data  0.001 ( 0.009)	Loss 8.7769e-02 (6.2587e-02)	Acc@1  96.09 ( 97.86)	Acc@5 100.00 (100.00)
Epoch: [50][ 40/391]	Time  0.106 ( 0.111)	Data  0.001 ( 0.008)	Loss 6.9519e-02 (6.4071e-02)	Acc@1  96.88 ( 97.77)	Acc@5 100.00 ( 99.98)
Epoch: [50][ 50/391]	Time  0.108 ( 0.111)	Data  0.001 ( 0.007)	Loss 5.6091e-02 (6.1699e-02)	Acc@1  96.88 ( 97.82)	Acc@5 100.00 ( 99.98)
Epoch: [50][ 60/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.007)	Loss 6.9153e-02 (6.2282e-02)	Acc@1  97.66 ( 97.81)	Acc@5 100.00 ( 99.99)
Epoch: [50][ 70/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 4.6448e-02 (6.1027e-02)	Acc@1  97.66 ( 97.81)	Acc@5 100.00 ( 99.99)
Epoch: [50][ 80/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.006)	Loss 8.1543e-02 (6.0291e-02)	Acc@1  96.88 ( 97.84)	Acc@5 100.00 ( 99.99)
Epoch: [50][ 90/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.006)	Loss 3.5828e-02 (5.8341e-02)	Acc@1 100.00 ( 97.95)	Acc@5 100.00 ( 99.99)
Epoch: [50][100/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.006)	Loss 5.7068e-02 (5.7409e-02)	Acc@1  96.88 ( 97.97)	Acc@5 100.00 ( 99.99)
Epoch: [50][110/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.006)	Loss 4.9255e-02 (5.7922e-02)	Acc@1  97.66 ( 97.97)	Acc@5 100.00 ( 99.99)
Epoch: [50][120/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.0598e-02 (5.6366e-02)	Acc@1  97.66 ( 98.02)	Acc@5 100.00 ( 99.99)
Epoch: [50][130/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.0405e-02 (5.6117e-02)	Acc@1  98.44 ( 98.02)	Acc@5 100.00 ( 99.99)
Epoch: [50][140/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.3436e-02 (5.6391e-02)	Acc@1  96.88 ( 98.02)	Acc@5 100.00 ( 99.99)
Epoch: [50][150/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.6548e-02 (5.6314e-02)	Acc@1  96.88 ( 98.00)	Acc@5 100.00 ( 99.99)
Epoch: [50][160/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.5889e-02 (5.6929e-02)	Acc@1 100.00 ( 97.97)	Acc@5 100.00 (100.00)
Epoch: [50][170/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.8176e-02 (5.8202e-02)	Acc@1  98.44 ( 97.94)	Acc@5 100.00 (100.00)
Epoch: [50][180/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.0271e-02 (5.8363e-02)	Acc@1  96.09 ( 97.92)	Acc@5 100.00 (100.00)
Epoch: [50][190/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.2266e-02 (5.8402e-02)	Acc@1  97.66 ( 97.90)	Acc@5 100.00 (100.00)
Epoch: [50][200/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.2867e-02 (5.8465e-02)	Acc@1  99.22 ( 97.90)	Acc@5 100.00 (100.00)
Epoch: [50][210/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.7170e-02 (5.8446e-02)	Acc@1  98.44 ( 97.90)	Acc@5 100.00 (100.00)
Epoch: [50][220/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.2643e-02 (5.8618e-02)	Acc@1  99.22 ( 97.88)	Acc@5 100.00 (100.00)
Epoch: [50][230/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.6932e-02 (5.8805e-02)	Acc@1  99.22 ( 97.87)	Acc@5 100.00 (100.00)
Epoch: [50][240/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.5979e-02 (5.9317e-02)	Acc@1  96.88 ( 97.84)	Acc@5 100.00 (100.00)
Epoch: [50][250/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.9692e-02 (5.9301e-02)	Acc@1  97.66 ( 97.86)	Acc@5 100.00 (100.00)
Epoch: [50][260/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.2266e-02 (5.9454e-02)	Acc@1  95.31 ( 97.82)	Acc@5 100.00 (100.00)
Epoch: [50][270/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.7373e-02 (5.9779e-02)	Acc@1  98.44 ( 97.81)	Acc@5 100.00 (100.00)
Epoch: [50][280/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.7089e-02 (5.9480e-02)	Acc@1  99.22 ( 97.83)	Acc@5 100.00 (100.00)
Epoch: [50][290/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.2683e-02 (5.9197e-02)	Acc@1  96.88 ( 97.83)	Acc@5 100.00 (100.00)
Epoch: [50][300/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.0558e-02 (5.8712e-02)	Acc@1  97.66 ( 97.85)	Acc@5 100.00 (100.00)
Epoch: [50][310/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.1736e-02 (5.9185e-02)	Acc@1  96.09 ( 97.83)	Acc@5 100.00 (100.00)
Epoch: [50][320/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.6721e-02 (5.9121e-02)	Acc@1  96.88 ( 97.83)	Acc@5 100.00 (100.00)
Epoch: [50][330/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.9028e-02 (5.8678e-02)	Acc@1  99.22 ( 97.85)	Acc@5 100.00 (100.00)
Epoch: [50][340/391]	Time  0.108 ( 0.108)	Data  0.001 ( 0.005)	Loss 5.3009e-02 (5.8903e-02)	Acc@1  98.44 ( 97.85)	Acc@5 100.00 (100.00)
Epoch: [50][350/391]	Time  0.108 ( 0.108)	Data  0.001 ( 0.005)	Loss 5.0171e-02 (5.8926e-02)	Acc@1  98.44 ( 97.85)	Acc@5 100.00 (100.00)
Epoch: [50][360/391]	Time  0.107 ( 0.108)	Data  0.001 ( 0.005)	Loss 5.4718e-02 (5.8850e-02)	Acc@1  97.66 ( 97.86)	Acc@5 100.00 (100.00)
Epoch: [50][370/391]	Time  0.110 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.1749e-01 (5.9031e-02)	Acc@1  96.88 ( 97.85)	Acc@5 100.00 (100.00)
Epoch: [50][380/391]	Time  0.109 ( 0.108)	Data  0.001 ( 0.005)	Loss 7.9895e-02 (5.9406e-02)	Acc@1  95.31 ( 97.83)	Acc@5 100.00 (100.00)
Epoch: [50][390/391]	Time  0.100 ( 0.108)	Data  0.001 ( 0.005)	Loss 9.3872e-02 (5.9903e-02)	Acc@1  97.50 ( 97.82)	Acc@5 100.00 (100.00)
## e[50] optimizer.zero_grad (sum) time: 0.3546624183654785
## e[50]       loss.backward (sum) time: 13.745120763778687
## e[50]      optimizer.step (sum) time: 2.8231301307678223
## epoch[50] training(only) time: 42.5855598449707
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 3.3057e-01 (3.3057e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.036 ( 0.051)	Loss 2.2046e-01 (3.6118e-01)	Acc@1  93.00 ( 90.73)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.037 ( 0.044)	Loss 5.2783e-01 (3.8990e-01)	Acc@1  87.00 ( 90.43)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.036 ( 0.042)	Loss 3.7207e-01 (4.1997e-01)	Acc@1  90.00 ( 90.42)	Acc@5  99.00 ( 99.48)
Test: [ 40/100]	Time  0.036 ( 0.041)	Loss 3.6548e-01 (4.1095e-01)	Acc@1  92.00 ( 90.44)	Acc@5  99.00 ( 99.51)
Test: [ 50/100]	Time  0.037 ( 0.040)	Loss 2.8052e-01 (4.0668e-01)	Acc@1  94.00 ( 90.51)	Acc@5 100.00 ( 99.59)
Test: [ 60/100]	Time  0.038 ( 0.040)	Loss 4.7876e-01 (4.0152e-01)	Acc@1  94.00 ( 90.51)	Acc@5  99.00 ( 99.59)
Test: [ 70/100]	Time  0.037 ( 0.039)	Loss 6.0010e-01 (3.9443e-01)	Acc@1  89.00 ( 90.54)	Acc@5 100.00 ( 99.63)
Test: [ 80/100]	Time  0.038 ( 0.039)	Loss 3.3301e-01 (3.9270e-01)	Acc@1  89.00 ( 90.53)	Acc@5 100.00 ( 99.64)
Test: [ 90/100]	Time  0.036 ( 0.039)	Loss 3.2104e-01 (3.8626e-01)	Acc@1  91.00 ( 90.58)	Acc@5 100.00 ( 99.63)
 * Acc@1 90.620 Acc@5 99.650
### epoch[50] execution time: 46.53494429588318
EPOCH 51
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [51][  0/391]	Time  0.269 ( 0.269)	Data  0.168 ( 0.168)	Loss 2.5986e-02 (2.5986e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [51][ 10/391]	Time  0.105 ( 0.123)	Data  0.001 ( 0.019)	Loss 5.5908e-02 (5.6960e-02)	Acc@1  97.66 ( 97.87)	Acc@5 100.00 (100.00)
Epoch: [51][ 20/391]	Time  0.106 ( 0.116)	Data  0.001 ( 0.012)	Loss 5.1453e-02 (5.2722e-02)	Acc@1  99.22 ( 98.03)	Acc@5 100.00 (100.00)
Epoch: [51][ 30/391]	Time  0.107 ( 0.113)	Data  0.001 ( 0.009)	Loss 4.3701e-02 (5.2883e-02)	Acc@1  99.22 ( 98.01)	Acc@5 100.00 (100.00)
Epoch: [51][ 40/391]	Time  0.108 ( 0.112)	Data  0.001 ( 0.008)	Loss 4.3182e-02 (5.1832e-02)	Acc@1  98.44 ( 98.06)	Acc@5 100.00 (100.00)
Epoch: [51][ 50/391]	Time  0.105 ( 0.111)	Data  0.001 ( 0.007)	Loss 3.1464e-02 (5.4762e-02)	Acc@1 100.00 ( 97.98)	Acc@5 100.00 (100.00)
Epoch: [51][ 60/391]	Time  0.106 ( 0.111)	Data  0.001 ( 0.007)	Loss 3.7170e-02 (5.4832e-02)	Acc@1  99.22 ( 98.01)	Acc@5 100.00 (100.00)
Epoch: [51][ 70/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 4.3762e-02 (5.4094e-02)	Acc@1  96.88 ( 98.00)	Acc@5 100.00 (100.00)
Epoch: [51][ 80/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.006)	Loss 5.3345e-02 (5.3770e-02)	Acc@1  97.66 ( 97.99)	Acc@5 100.00 (100.00)
Epoch: [51][ 90/391]	Time  0.104 ( 0.110)	Data  0.001 ( 0.006)	Loss 7.9529e-02 (5.3275e-02)	Acc@1  97.66 ( 98.03)	Acc@5 100.00 (100.00)
Epoch: [51][100/391]	Time  0.104 ( 0.110)	Data  0.001 ( 0.006)	Loss 4.2297e-02 (5.2276e-02)	Acc@1  99.22 ( 98.09)	Acc@5 100.00 (100.00)
Epoch: [51][110/391]	Time  0.099 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.2103e-01 (5.2362e-02)	Acc@1  95.31 ( 98.11)	Acc@5 100.00 (100.00)
Epoch: [51][120/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.2643e-02 (5.1532e-02)	Acc@1  97.66 ( 98.16)	Acc@5 100.00 (100.00)
Epoch: [51][130/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.3049e-02 (5.1446e-02)	Acc@1  96.88 ( 98.18)	Acc@5 100.00 (100.00)
Epoch: [51][140/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.4656e-02 (5.2372e-02)	Acc@1  95.31 ( 98.11)	Acc@5 100.00 (100.00)
Epoch: [51][150/391]	Time  0.103 ( 0.109)	Data  0.002 ( 0.005)	Loss 3.4454e-02 (5.3659e-02)	Acc@1  98.44 ( 98.06)	Acc@5 100.00 (100.00)
Epoch: [51][160/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.5581e-02 (5.4302e-02)	Acc@1  96.88 ( 98.04)	Acc@5 100.00 (100.00)
Epoch: [51][170/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.3833e-02 (5.4977e-02)	Acc@1  98.44 ( 98.02)	Acc@5 100.00 (100.00)
Epoch: [51][180/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0822e-01 (5.5281e-02)	Acc@1  94.53 ( 98.01)	Acc@5 100.00 (100.00)
Epoch: [51][190/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.4291e-02 (5.5016e-02)	Acc@1  98.44 ( 98.02)	Acc@5 100.00 (100.00)
Epoch: [51][200/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.0150e-02 (5.4451e-02)	Acc@1  99.22 ( 98.07)	Acc@5 100.00 (100.00)
Epoch: [51][210/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.4962e-02 (5.4690e-02)	Acc@1  99.22 ( 98.06)	Acc@5 100.00 (100.00)
Epoch: [51][220/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.3904e-02 (5.5003e-02)	Acc@1  99.22 ( 98.06)	Acc@5  99.22 (100.00)
Epoch: [51][230/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.6783e-02 (5.5182e-02)	Acc@1 100.00 ( 98.06)	Acc@5 100.00 (100.00)
Epoch: [51][240/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.0018e-02 (5.5496e-02)	Acc@1  96.88 ( 98.03)	Acc@5 100.00 (100.00)
Epoch: [51][250/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.3253e-02 (5.5258e-02)	Acc@1  98.44 ( 98.03)	Acc@5 100.00 (100.00)
Epoch: [51][260/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.0242e-02 (5.5060e-02)	Acc@1  99.22 ( 98.05)	Acc@5 100.00 (100.00)
Epoch: [51][270/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.4219e-02 (5.5347e-02)	Acc@1  96.88 ( 98.02)	Acc@5 100.00 (100.00)
Epoch: [51][280/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.4360e-02 (5.5284e-02)	Acc@1  97.66 ( 98.02)	Acc@5 100.00 (100.00)
Epoch: [51][290/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.2998e-02 (5.5786e-02)	Acc@1  97.66 ( 98.00)	Acc@5 100.00 (100.00)
Epoch: [51][300/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1090e-01 (5.5594e-02)	Acc@1  96.09 ( 98.01)	Acc@5 100.00 (100.00)
Epoch: [51][310/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.6417e-02 (5.5393e-02)	Acc@1  97.66 ( 98.03)	Acc@5 100.00 (100.00)
Epoch: [51][320/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.3741e-02 (5.5440e-02)	Acc@1  98.44 ( 98.02)	Acc@5 100.00 (100.00)
Epoch: [51][330/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.7863e-02 (5.5691e-02)	Acc@1  99.22 ( 98.01)	Acc@5 100.00 (100.00)
Epoch: [51][340/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2830e-01 (5.6115e-02)	Acc@1  94.53 ( 97.99)	Acc@5 100.00 (100.00)
Epoch: [51][350/391]	Time  0.106 ( 0.109)	Data  0.002 ( 0.005)	Loss 4.2175e-02 (5.6173e-02)	Acc@1  98.44 ( 97.99)	Acc@5 100.00 (100.00)
Epoch: [51][360/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.8676e-02 (5.6078e-02)	Acc@1  98.44 ( 97.99)	Acc@5 100.00 (100.00)
Epoch: [51][370/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.1838e-02 (5.6175e-02)	Acc@1  96.88 ( 97.98)	Acc@5 100.00 (100.00)
Epoch: [51][380/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.6050e-02 (5.6423e-02)	Acc@1  96.88 ( 97.97)	Acc@5 100.00 (100.00)
Epoch: [51][390/391]	Time  0.098 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2952e-01 (5.6872e-02)	Acc@1  95.00 ( 97.95)	Acc@5 100.00 (100.00)
## e[51] optimizer.zero_grad (sum) time: 0.3465704917907715
## e[51]       loss.backward (sum) time: 13.716422080993652
## e[51]      optimizer.step (sum) time: 2.7665457725524902
## epoch[51] training(only) time: 42.637929916381836
# Switched to evaluate mode...
Test: [  0/100]	Time  0.196 ( 0.196)	Loss 2.6050e-01 (2.6050e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.038 ( 0.052)	Loss 1.3660e-01 (3.5794e-01)	Acc@1  95.00 ( 90.55)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.039 ( 0.046)	Loss 6.0449e-01 (4.0102e-01)	Acc@1  88.00 ( 90.14)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.039 ( 0.043)	Loss 3.8818e-01 (4.2753e-01)	Acc@1  87.00 ( 90.16)	Acc@5 100.00 ( 99.55)
Test: [ 40/100]	Time  0.038 ( 0.042)	Loss 3.2568e-01 (4.2229e-01)	Acc@1  92.00 ( 90.27)	Acc@5  99.00 ( 99.56)
Test: [ 50/100]	Time  0.036 ( 0.041)	Loss 2.1423e-01 (4.1108e-01)	Acc@1  93.00 ( 90.43)	Acc@5 100.00 ( 99.55)
Test: [ 60/100]	Time  0.038 ( 0.040)	Loss 5.4102e-01 (4.0632e-01)	Acc@1  93.00 ( 90.59)	Acc@5  99.00 ( 99.56)
Test: [ 70/100]	Time  0.036 ( 0.040)	Loss 5.6104e-01 (3.9960e-01)	Acc@1  88.00 ( 90.61)	Acc@5 100.00 ( 99.58)
Test: [ 80/100]	Time  0.038 ( 0.039)	Loss 3.1006e-01 (3.9551e-01)	Acc@1  91.00 ( 90.74)	Acc@5 100.00 ( 99.58)
Test: [ 90/100]	Time  0.037 ( 0.039)	Loss 2.6074e-01 (3.8780e-01)	Acc@1  94.00 ( 90.87)	Acc@5 100.00 ( 99.60)
 * Acc@1 90.920 Acc@5 99.610
### epoch[51] execution time: 46.67175889015198
EPOCH 52
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [52][  0/391]	Time  0.261 ( 0.261)	Data  0.165 ( 0.165)	Loss 1.9562e-02 (1.9562e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [52][ 10/391]	Time  0.108 ( 0.123)	Data  0.001 ( 0.019)	Loss 3.5217e-02 (3.5110e-02)	Acc@1  99.22 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [52][ 20/391]	Time  0.104 ( 0.115)	Data  0.001 ( 0.012)	Loss 1.1084e-01 (4.7966e-02)	Acc@1  96.09 ( 98.59)	Acc@5 100.00 (100.00)
Epoch: [52][ 30/391]	Time  0.105 ( 0.113)	Data  0.001 ( 0.009)	Loss 4.1382e-02 (4.7845e-02)	Acc@1  97.66 ( 98.46)	Acc@5 100.00 (100.00)
Epoch: [52][ 40/391]	Time  0.104 ( 0.112)	Data  0.001 ( 0.008)	Loss 5.2979e-02 (5.2050e-02)	Acc@1  97.66 ( 98.25)	Acc@5 100.00 (100.00)
Epoch: [52][ 50/391]	Time  0.107 ( 0.111)	Data  0.001 ( 0.007)	Loss 7.9346e-02 (5.5364e-02)	Acc@1  97.66 ( 98.04)	Acc@5 100.00 (100.00)
Epoch: [52][ 60/391]	Time  0.106 ( 0.111)	Data  0.001 ( 0.007)	Loss 3.5828e-02 (5.5512e-02)	Acc@1  99.22 ( 98.07)	Acc@5 100.00 (100.00)
Epoch: [52][ 70/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 6.1249e-02 (5.6124e-02)	Acc@1  97.66 ( 98.11)	Acc@5 100.00 (100.00)
Epoch: [52][ 80/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 7.4768e-02 (5.4971e-02)	Acc@1  96.88 ( 98.12)	Acc@5 100.00 (100.00)
Epoch: [52][ 90/391]	Time  0.103 ( 0.110)	Data  0.001 ( 0.006)	Loss 4.9652e-02 (5.4789e-02)	Acc@1  99.22 ( 98.14)	Acc@5 100.00 (100.00)
Epoch: [52][100/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 2.2827e-02 (5.5539e-02)	Acc@1  99.22 ( 98.10)	Acc@5 100.00 (100.00)
Epoch: [52][110/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.006)	Loss 5.2979e-02 (5.5173e-02)	Acc@1  98.44 ( 98.09)	Acc@5 100.00 (100.00)
Epoch: [52][120/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.0441e-02 (5.4277e-02)	Acc@1  98.44 ( 98.10)	Acc@5 100.00 (100.00)
Epoch: [52][130/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.1544e-02 (5.3507e-02)	Acc@1  97.66 ( 98.09)	Acc@5 100.00 (100.00)
Epoch: [52][140/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.1453e-02 (5.3964e-02)	Acc@1  96.09 ( 98.06)	Acc@5 100.00 (100.00)
Epoch: [52][150/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.4678e-02 (5.3065e-02)	Acc@1  97.66 ( 98.11)	Acc@5 100.00 (100.00)
Epoch: [52][160/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.2593e-02 (5.3090e-02)	Acc@1  98.44 ( 98.12)	Acc@5 100.00 (100.00)
Epoch: [52][170/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.5817e-02 (5.2879e-02)	Acc@1  96.88 ( 98.10)	Acc@5 100.00 (100.00)
Epoch: [52][180/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.8239e-02 (5.3664e-02)	Acc@1  98.44 ( 98.10)	Acc@5 100.00 (100.00)
Epoch: [52][190/391]	Time  0.111 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.8318e-02 (5.4471e-02)	Acc@1  96.88 ( 98.06)	Acc@5 100.00 (100.00)
Epoch: [52][200/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.2736e-02 (5.4967e-02)	Acc@1 100.00 ( 98.06)	Acc@5 100.00 (100.00)
Epoch: [52][210/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.0933e-02 (5.4807e-02)	Acc@1  96.88 ( 98.07)	Acc@5 100.00 (100.00)
Epoch: [52][220/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.9174e-02 (5.4578e-02)	Acc@1  97.66 ( 98.06)	Acc@5 100.00 (100.00)
Epoch: [52][230/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.1127e-02 (5.5318e-02)	Acc@1  98.44 ( 98.04)	Acc@5 100.00 (100.00)
Epoch: [52][240/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.9495e-02 (5.5867e-02)	Acc@1  99.22 ( 98.01)	Acc@5 100.00 (100.00)
Epoch: [52][250/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.3303e-02 (5.5885e-02)	Acc@1  96.09 ( 97.99)	Acc@5 100.00 (100.00)
Epoch: [52][260/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.9333e-02 (5.6512e-02)	Acc@1 100.00 ( 97.99)	Acc@5 100.00 (100.00)
Epoch: [52][270/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.4962e-02 (5.6811e-02)	Acc@1  98.44 ( 97.98)	Acc@5 100.00 (100.00)
Epoch: [52][280/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.6416e-02 (5.6338e-02)	Acc@1  97.66 ( 98.00)	Acc@5 100.00 (100.00)
Epoch: [52][290/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.4704e-02 (5.6289e-02)	Acc@1 100.00 ( 98.01)	Acc@5 100.00 (100.00)
Epoch: [52][300/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.3331e-02 (5.6203e-02)	Acc@1  99.22 ( 98.02)	Acc@5 100.00 (100.00)
Epoch: [52][310/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4175e-02 (5.6104e-02)	Acc@1 100.00 ( 98.03)	Acc@5 100.00 (100.00)
Epoch: [52][320/391]	Time  0.100 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2097e-01 (5.6266e-02)	Acc@1  95.31 ( 98.03)	Acc@5 100.00 (100.00)
Epoch: [52][330/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.1025e-02 (5.6351e-02)	Acc@1  98.44 ( 98.03)	Acc@5 100.00 (100.00)
Epoch: [52][340/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.5345e-02 (5.7036e-02)	Acc@1  99.22 ( 98.02)	Acc@5 100.00 (100.00)
Epoch: [52][350/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.6824e-02 (5.7693e-02)	Acc@1  96.88 ( 97.99)	Acc@5 100.00 (100.00)
Epoch: [52][360/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.6957e-02 (5.7411e-02)	Acc@1  98.44 ( 98.01)	Acc@5 100.00 (100.00)
Epoch: [52][370/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5991e-02 (5.7264e-02)	Acc@1 100.00 ( 98.03)	Acc@5 100.00 (100.00)
Epoch: [52][380/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.8918e-02 (5.7300e-02)	Acc@1  97.66 ( 98.03)	Acc@5 100.00 (100.00)
Epoch: [52][390/391]	Time  0.101 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.4036e-02 (5.7457e-02)	Acc@1  97.50 ( 98.02)	Acc@5 100.00 (100.00)
## e[52] optimizer.zero_grad (sum) time: 0.34754157066345215
## e[52]       loss.backward (sum) time: 13.72121262550354
## e[52]      optimizer.step (sum) time: 2.7761943340301514
## epoch[52] training(only) time: 42.57510709762573
# Switched to evaluate mode...
Test: [  0/100]	Time  0.193 ( 0.193)	Loss 3.3545e-01 (3.3545e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.036 ( 0.051)	Loss 2.5000e-01 (3.6605e-01)	Acc@1  94.00 ( 91.00)	Acc@5 100.00 ( 99.55)
Test: [ 20/100]	Time  0.036 ( 0.044)	Loss 5.8691e-01 (3.9787e-01)	Acc@1  89.00 ( 90.76)	Acc@5 100.00 ( 99.52)
Test: [ 30/100]	Time  0.040 ( 0.042)	Loss 3.6328e-01 (4.1905e-01)	Acc@1  90.00 ( 91.00)	Acc@5 100.00 ( 99.48)
Test: [ 40/100]	Time  0.037 ( 0.041)	Loss 3.5669e-01 (4.1919e-01)	Acc@1  91.00 ( 90.93)	Acc@5  99.00 ( 99.51)
Test: [ 50/100]	Time  0.039 ( 0.040)	Loss 2.1423e-01 (4.0652e-01)	Acc@1  95.00 ( 90.90)	Acc@5 100.00 ( 99.57)
Test: [ 60/100]	Time  0.036 ( 0.040)	Loss 5.7227e-01 (3.9970e-01)	Acc@1  93.00 ( 90.93)	Acc@5  99.00 ( 99.59)
Test: [ 70/100]	Time  0.038 ( 0.039)	Loss 4.8682e-01 (3.9414e-01)	Acc@1  89.00 ( 90.89)	Acc@5 100.00 ( 99.62)
Test: [ 80/100]	Time  0.037 ( 0.039)	Loss 3.3301e-01 (3.9156e-01)	Acc@1  88.00 ( 90.88)	Acc@5 100.00 ( 99.63)
Test: [ 90/100]	Time  0.036 ( 0.039)	Loss 2.9126e-01 (3.8320e-01)	Acc@1  94.00 ( 90.95)	Acc@5 100.00 ( 99.63)
 * Acc@1 90.910 Acc@5 99.640
### epoch[52] execution time: 46.56951427459717
EPOCH 53
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [53][  0/391]	Time  0.268 ( 0.268)	Data  0.174 ( 0.174)	Loss 7.9834e-02 (7.9834e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [53][ 10/391]	Time  0.107 ( 0.122)	Data  0.001 ( 0.019)	Loss 8.9966e-02 (6.5893e-02)	Acc@1  96.88 ( 97.87)	Acc@5 100.00 (100.00)
Epoch: [53][ 20/391]	Time  0.106 ( 0.115)	Data  0.001 ( 0.012)	Loss 6.7993e-02 (5.6834e-02)	Acc@1  96.88 ( 97.95)	Acc@5 100.00 (100.00)
Epoch: [53][ 30/391]	Time  0.105 ( 0.113)	Data  0.001 ( 0.009)	Loss 3.1158e-02 (5.2521e-02)	Acc@1  99.22 ( 98.14)	Acc@5 100.00 (100.00)
Epoch: [53][ 40/391]	Time  0.106 ( 0.112)	Data  0.001 ( 0.008)	Loss 8.2153e-02 (4.9228e-02)	Acc@1  97.66 ( 98.27)	Acc@5 100.00 (100.00)
Epoch: [53][ 50/391]	Time  0.106 ( 0.111)	Data  0.001 ( 0.007)	Loss 5.3345e-02 (4.7084e-02)	Acc@1  97.66 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [53][ 60/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.007)	Loss 1.5656e-02 (4.6164e-02)	Acc@1  99.22 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [53][ 70/391]	Time  0.104 ( 0.110)	Data  0.001 ( 0.007)	Loss 5.3375e-02 (4.6327e-02)	Acc@1  98.44 ( 98.34)	Acc@5 100.00 (100.00)
Epoch: [53][ 80/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 9.0698e-02 (4.5670e-02)	Acc@1  95.31 ( 98.39)	Acc@5 100.00 (100.00)
Epoch: [53][ 90/391]	Time  0.098 ( 0.110)	Data  0.001 ( 0.006)	Loss 8.8745e-02 (4.5533e-02)	Acc@1  96.88 ( 98.45)	Acc@5 100.00 (100.00)
Epoch: [53][100/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.006)	Loss 4.8523e-02 (4.5155e-02)	Acc@1  98.44 ( 98.48)	Acc@5 100.00 (100.00)
Epoch: [53][110/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.006)	Loss 5.9296e-02 (4.4755e-02)	Acc@1  97.66 ( 98.49)	Acc@5 100.00 (100.00)
Epoch: [53][120/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.006)	Loss 4.4037e-02 (4.5693e-02)	Acc@1  99.22 ( 98.47)	Acc@5 100.00 (100.00)
Epoch: [53][130/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.6427e-02 (4.6489e-02)	Acc@1  97.66 ( 98.43)	Acc@5 100.00 ( 99.99)
Epoch: [53][140/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.3496e-02 (4.6789e-02)	Acc@1  96.09 ( 98.41)	Acc@5 100.00 ( 99.99)
Epoch: [53][150/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.1500e-02 (4.6798e-02)	Acc@1  99.22 ( 98.40)	Acc@5 100.00 ( 99.99)
Epoch: [53][160/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.6975e-02 (4.6938e-02)	Acc@1  97.66 ( 98.40)	Acc@5 100.00 (100.00)
Epoch: [53][170/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0017e-02 (4.6282e-02)	Acc@1 100.00 ( 98.41)	Acc@5 100.00 (100.00)
Epoch: [53][180/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.8796e-02 (4.6663e-02)	Acc@1  97.66 ( 98.39)	Acc@5 100.00 (100.00)
Epoch: [53][190/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.0638e-02 (4.6293e-02)	Acc@1  96.88 ( 98.40)	Acc@5 100.00 (100.00)
Epoch: [53][200/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.4230e-02 (4.6783e-02)	Acc@1  99.22 ( 98.39)	Acc@5 100.00 (100.00)
Epoch: [53][210/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.2257e-02 (4.6847e-02)	Acc@1  99.22 ( 98.38)	Acc@5 100.00 (100.00)
Epoch: [53][220/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.7466e-02 (4.7157e-02)	Acc@1  98.44 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [53][230/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.0283e-02 (4.7584e-02)	Acc@1  99.22 ( 98.34)	Acc@5 100.00 (100.00)
Epoch: [53][240/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.8555e-02 (4.7902e-02)	Acc@1 100.00 ( 98.32)	Acc@5 100.00 (100.00)
Epoch: [53][250/391]	Time  0.105 ( 0.108)	Data  0.001 ( 0.005)	Loss 8.1299e-02 (4.8061e-02)	Acc@1  97.66 ( 98.33)	Acc@5 100.00 (100.00)
Epoch: [53][260/391]	Time  0.108 ( 0.108)	Data  0.001 ( 0.005)	Loss 6.3538e-02 (4.7743e-02)	Acc@1  96.88 ( 98.34)	Acc@5 100.00 (100.00)
Epoch: [53][270/391]	Time  0.108 ( 0.108)	Data  0.001 ( 0.005)	Loss 2.2415e-02 (4.7740e-02)	Acc@1 100.00 ( 98.33)	Acc@5 100.00 (100.00)
Epoch: [53][280/391]	Time  0.107 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.4519e-02 (4.8194e-02)	Acc@1  99.22 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [53][290/391]	Time  0.110 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.1346e-01 (4.8352e-02)	Acc@1  96.88 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [53][300/391]	Time  0.109 ( 0.108)	Data  0.001 ( 0.005)	Loss 2.8305e-02 (4.8849e-02)	Acc@1  99.22 ( 98.29)	Acc@5 100.00 (100.00)
Epoch: [53][310/391]	Time  0.106 ( 0.108)	Data  0.001 ( 0.005)	Loss 6.5979e-02 (4.9209e-02)	Acc@1  97.66 ( 98.27)	Acc@5 100.00 (100.00)
Epoch: [53][320/391]	Time  0.101 ( 0.108)	Data  0.001 ( 0.005)	Loss 7.7271e-02 (4.9470e-02)	Acc@1  97.66 ( 98.25)	Acc@5 100.00 (100.00)
Epoch: [53][330/391]	Time  0.106 ( 0.108)	Data  0.001 ( 0.005)	Loss 2.5162e-02 (4.9292e-02)	Acc@1  99.22 ( 98.26)	Acc@5 100.00 (100.00)
Epoch: [53][340/391]	Time  0.105 ( 0.108)	Data  0.001 ( 0.005)	Loss 4.3121e-02 (4.9601e-02)	Acc@1  97.66 ( 98.25)	Acc@5 100.00 (100.00)
Epoch: [53][350/391]	Time  0.105 ( 0.108)	Data  0.001 ( 0.005)	Loss 4.3640e-02 (4.9850e-02)	Acc@1  98.44 ( 98.24)	Acc@5 100.00 (100.00)
Epoch: [53][360/391]	Time  0.105 ( 0.108)	Data  0.002 ( 0.005)	Loss 3.3325e-02 (4.9640e-02)	Acc@1  98.44 ( 98.25)	Acc@5 100.00 (100.00)
Epoch: [53][370/391]	Time  0.107 ( 0.108)	Data  0.001 ( 0.005)	Loss 2.8961e-02 (5.0175e-02)	Acc@1  99.22 ( 98.24)	Acc@5 100.00 (100.00)
Epoch: [53][380/391]	Time  0.107 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.6617e-02 (5.0256e-02)	Acc@1 100.00 ( 98.22)	Acc@5 100.00 (100.00)
Epoch: [53][390/391]	Time  0.104 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.7200e-01 (5.0674e-02)	Acc@1  93.75 ( 98.20)	Acc@5 100.00 ( 99.99)
## e[53] optimizer.zero_grad (sum) time: 0.34995341300964355
## e[53]       loss.backward (sum) time: 13.71259593963623
## e[53]      optimizer.step (sum) time: 2.7605092525482178
## epoch[53] training(only) time: 42.535133361816406
# Switched to evaluate mode...
Test: [  0/100]	Time  0.198 ( 0.198)	Loss 2.7734e-01 (2.7734e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.038 ( 0.054)	Loss 2.3132e-01 (3.4842e-01)	Acc@1  94.00 ( 91.09)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.039 ( 0.046)	Loss 5.4541e-01 (3.8238e-01)	Acc@1  89.00 ( 90.57)	Acc@5  99.00 ( 99.62)
Test: [ 30/100]	Time  0.040 ( 0.043)	Loss 4.2993e-01 (4.1718e-01)	Acc@1  87.00 ( 90.55)	Acc@5 100.00 ( 99.52)
Test: [ 40/100]	Time  0.036 ( 0.042)	Loss 2.7588e-01 (4.1683e-01)	Acc@1  94.00 ( 90.54)	Acc@5  99.00 ( 99.54)
Test: [ 50/100]	Time  0.042 ( 0.041)	Loss 3.1299e-01 (4.1009e-01)	Acc@1  93.00 ( 90.76)	Acc@5 100.00 ( 99.57)
Test: [ 60/100]	Time  0.037 ( 0.040)	Loss 5.3564e-01 (4.0487e-01)	Acc@1  94.00 ( 90.89)	Acc@5  99.00 ( 99.59)
Test: [ 70/100]	Time  0.037 ( 0.040)	Loss 6.0156e-01 (4.0138e-01)	Acc@1  90.00 ( 90.83)	Acc@5 100.00 ( 99.63)
Test: [ 80/100]	Time  0.036 ( 0.039)	Loss 2.7075e-01 (3.9977e-01)	Acc@1  91.00 ( 90.83)	Acc@5 100.00 ( 99.64)
Test: [ 90/100]	Time  0.038 ( 0.039)	Loss 3.5449e-01 (3.9457e-01)	Acc@1  93.00 ( 90.82)	Acc@5 100.00 ( 99.65)
 * Acc@1 90.860 Acc@5 99.670
### epoch[53] execution time: 46.5192437171936
EPOCH 54
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [54][  0/391]	Time  0.276 ( 0.276)	Data  0.163 ( 0.163)	Loss 4.9927e-02 (4.9927e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [54][ 10/391]	Time  0.107 ( 0.123)	Data  0.001 ( 0.018)	Loss 2.1454e-02 (5.3621e-02)	Acc@1 100.00 ( 98.15)	Acc@5 100.00 (100.00)
Epoch: [54][ 20/391]	Time  0.108 ( 0.115)	Data  0.001 ( 0.011)	Loss 5.7098e-02 (5.5336e-02)	Acc@1  97.66 ( 98.18)	Acc@5 100.00 (100.00)
Epoch: [54][ 30/391]	Time  0.104 ( 0.113)	Data  0.001 ( 0.009)	Loss 2.3987e-02 (5.3417e-02)	Acc@1  99.22 ( 98.14)	Acc@5 100.00 (100.00)
Epoch: [54][ 40/391]	Time  0.111 ( 0.112)	Data  0.001 ( 0.008)	Loss 8.3435e-02 (5.1650e-02)	Acc@1  96.09 ( 98.23)	Acc@5 100.00 (100.00)
Epoch: [54][ 50/391]	Time  0.105 ( 0.111)	Data  0.001 ( 0.007)	Loss 4.4159e-02 (5.1594e-02)	Acc@1  99.22 ( 98.18)	Acc@5 100.00 (100.00)
Epoch: [54][ 60/391]	Time  0.109 ( 0.110)	Data  0.001 ( 0.007)	Loss 1.7242e-02 (5.1201e-02)	Acc@1  99.22 ( 98.22)	Acc@5 100.00 (100.00)
Epoch: [54][ 70/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 4.9805e-02 (5.0947e-02)	Acc@1  96.88 ( 98.22)	Acc@5 100.00 (100.00)
Epoch: [54][ 80/391]	Time  0.111 ( 0.110)	Data  0.001 ( 0.006)	Loss 8.1604e-02 (5.1771e-02)	Acc@1  96.88 ( 98.16)	Acc@5 100.00 (100.00)
Epoch: [54][ 90/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.006)	Loss 2.6215e-02 (5.1447e-02)	Acc@1  99.22 ( 98.14)	Acc@5 100.00 (100.00)
Epoch: [54][100/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.006)	Loss 3.3936e-02 (5.1027e-02)	Acc@1  99.22 ( 98.14)	Acc@5 100.00 (100.00)
Epoch: [54][110/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.006)	Loss 5.3925e-02 (5.1541e-02)	Acc@1  99.22 ( 98.16)	Acc@5 100.00 (100.00)
Epoch: [54][120/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.1271e-02 (5.1701e-02)	Acc@1 100.00 ( 98.16)	Acc@5 100.00 (100.00)
Epoch: [54][130/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.2145e-02 (5.1819e-02)	Acc@1  98.44 ( 98.17)	Acc@5 100.00 (100.00)
Epoch: [54][140/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.0984e-02 (5.1901e-02)	Acc@1  98.44 ( 98.16)	Acc@5 100.00 (100.00)
Epoch: [54][150/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.8441e-02 (5.2296e-02)	Acc@1  98.44 ( 98.13)	Acc@5 100.00 (100.00)
Epoch: [54][160/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.6844e-02 (5.1891e-02)	Acc@1  98.44 ( 98.12)	Acc@5 100.00 (100.00)
Epoch: [54][170/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.0081e-02 (5.2237e-02)	Acc@1  99.22 ( 98.12)	Acc@5 100.00 (100.00)
Epoch: [54][180/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.4668e-02 (5.2263e-02)	Acc@1  99.22 ( 98.13)	Acc@5 100.00 (100.00)
Epoch: [54][190/391]	Time  0.106 ( 0.109)	Data  0.002 ( 0.005)	Loss 3.5828e-02 (5.1881e-02)	Acc@1  99.22 ( 98.13)	Acc@5 100.00 (100.00)
Epoch: [54][200/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.4800e-02 (5.1697e-02)	Acc@1  98.44 ( 98.14)	Acc@5 100.00 (100.00)
Epoch: [54][210/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.3721e-02 (5.1522e-02)	Acc@1  98.44 ( 98.14)	Acc@5 100.00 (100.00)
Epoch: [54][220/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.8951e-02 (5.1175e-02)	Acc@1  99.22 ( 98.16)	Acc@5 100.00 (100.00)
Epoch: [54][230/391]	Time  0.105 ( 0.108)	Data  0.001 ( 0.005)	Loss 2.9449e-02 (5.0430e-02)	Acc@1  99.22 ( 98.20)	Acc@5 100.00 (100.00)
Epoch: [54][240/391]	Time  0.105 ( 0.108)	Data  0.001 ( 0.005)	Loss 3.0151e-02 (5.0544e-02)	Acc@1  99.22 ( 98.19)	Acc@5 100.00 (100.00)
Epoch: [54][250/391]	Time  0.107 ( 0.108)	Data  0.001 ( 0.005)	Loss 3.7659e-02 (5.0393e-02)	Acc@1  98.44 ( 98.21)	Acc@5 100.00 (100.00)
Epoch: [54][260/391]	Time  0.105 ( 0.108)	Data  0.002 ( 0.005)	Loss 7.5256e-02 (5.1015e-02)	Acc@1  96.88 ( 98.21)	Acc@5 100.00 (100.00)
Epoch: [54][270/391]	Time  0.104 ( 0.108)	Data  0.001 ( 0.005)	Loss 5.2429e-02 (5.0878e-02)	Acc@1  97.66 ( 98.22)	Acc@5 100.00 (100.00)
Epoch: [54][280/391]	Time  0.107 ( 0.108)	Data  0.001 ( 0.005)	Loss 5.8746e-02 (5.0767e-02)	Acc@1  97.66 ( 98.22)	Acc@5 100.00 (100.00)
Epoch: [54][290/391]	Time  0.104 ( 0.108)	Data  0.001 ( 0.005)	Loss 6.1920e-02 (5.0996e-02)	Acc@1  97.66 ( 98.22)	Acc@5 100.00 (100.00)
Epoch: [54][300/391]	Time  0.107 ( 0.108)	Data  0.001 ( 0.005)	Loss 5.3162e-02 (5.1537e-02)	Acc@1  96.88 ( 98.20)	Acc@5 100.00 (100.00)
Epoch: [54][310/391]	Time  0.109 ( 0.108)	Data  0.001 ( 0.005)	Loss 5.1880e-02 (5.1384e-02)	Acc@1  97.66 ( 98.21)	Acc@5 100.00 (100.00)
Epoch: [54][320/391]	Time  0.102 ( 0.108)	Data  0.001 ( 0.005)	Loss 2.6749e-02 (5.1470e-02)	Acc@1  99.22 ( 98.20)	Acc@5 100.00 (100.00)
Epoch: [54][330/391]	Time  0.110 ( 0.108)	Data  0.001 ( 0.005)	Loss 3.8879e-02 (5.1592e-02)	Acc@1  98.44 ( 98.20)	Acc@5 100.00 (100.00)
Epoch: [54][340/391]	Time  0.107 ( 0.108)	Data  0.001 ( 0.005)	Loss 4.7150e-02 (5.1607e-02)	Acc@1  98.44 ( 98.19)	Acc@5 100.00 (100.00)
Epoch: [54][350/391]	Time  0.105 ( 0.108)	Data  0.001 ( 0.005)	Loss 8.0505e-02 (5.1475e-02)	Acc@1  97.66 ( 98.20)	Acc@5 100.00 (100.00)
Epoch: [54][360/391]	Time  0.107 ( 0.108)	Data  0.001 ( 0.005)	Loss 6.8359e-02 (5.1410e-02)	Acc@1  96.88 ( 98.20)	Acc@5 100.00 (100.00)
Epoch: [54][370/391]	Time  0.103 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.2610e-01 (5.1451e-02)	Acc@1  95.31 ( 98.20)	Acc@5 100.00 (100.00)
Epoch: [54][380/391]	Time  0.109 ( 0.108)	Data  0.001 ( 0.005)	Loss 4.3610e-02 (5.1122e-02)	Acc@1  99.22 ( 98.21)	Acc@5 100.00 (100.00)
Epoch: [54][390/391]	Time  0.101 ( 0.108)	Data  0.001 ( 0.005)	Loss 5.6183e-02 (5.1229e-02)	Acc@1  97.50 ( 98.21)	Acc@5 100.00 (100.00)
## e[54] optimizer.zero_grad (sum) time: 0.3497812747955322
## e[54]       loss.backward (sum) time: 13.713165283203125
## e[54]      optimizer.step (sum) time: 2.757077693939209
## epoch[54] training(only) time: 42.533387422561646
# Switched to evaluate mode...
Test: [  0/100]	Time  0.196 ( 0.196)	Loss 3.6084e-01 (3.6084e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.037 ( 0.052)	Loss 2.9150e-01 (3.9058e-01)	Acc@1  93.00 ( 90.91)	Acc@5 100.00 ( 99.45)
Test: [ 20/100]	Time  0.036 ( 0.045)	Loss 5.6299e-01 (4.0479e-01)	Acc@1  87.00 ( 90.67)	Acc@5 100.00 ( 99.43)
Test: [ 30/100]	Time  0.036 ( 0.042)	Loss 3.7524e-01 (4.2922e-01)	Acc@1  87.00 ( 90.71)	Acc@5 100.00 ( 99.42)
Test: [ 40/100]	Time  0.037 ( 0.041)	Loss 3.1177e-01 (4.2668e-01)	Acc@1  92.00 ( 90.68)	Acc@5  99.00 ( 99.46)
Test: [ 50/100]	Time  0.040 ( 0.040)	Loss 2.4231e-01 (4.2036e-01)	Acc@1  92.00 ( 90.86)	Acc@5 100.00 ( 99.45)
Test: [ 60/100]	Time  0.038 ( 0.040)	Loss 5.4395e-01 (4.1649e-01)	Acc@1  94.00 ( 90.87)	Acc@5  99.00 ( 99.49)
Test: [ 70/100]	Time  0.036 ( 0.039)	Loss 5.6104e-01 (4.1258e-01)	Acc@1  91.00 ( 90.89)	Acc@5 100.00 ( 99.55)
Test: [ 80/100]	Time  0.037 ( 0.039)	Loss 3.2397e-01 (4.0856e-01)	Acc@1  91.00 ( 90.91)	Acc@5 100.00 ( 99.57)
Test: [ 90/100]	Time  0.038 ( 0.039)	Loss 3.1689e-01 (4.0334e-01)	Acc@1  95.00 ( 90.85)	Acc@5  99.00 ( 99.58)
 * Acc@1 90.830 Acc@5 99.600
### epoch[54] execution time: 46.4855101108551
EPOCH 55
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [55][  0/391]	Time  0.277 ( 0.277)	Data  0.176 ( 0.176)	Loss 3.1311e-02 (3.1311e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [55][ 10/391]	Time  0.106 ( 0.123)	Data  0.001 ( 0.020)	Loss 9.8267e-02 (4.2688e-02)	Acc@1  97.66 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [55][ 20/391]	Time  0.107 ( 0.116)	Data  0.001 ( 0.012)	Loss 2.4673e-02 (3.8637e-02)	Acc@1 100.00 ( 98.85)	Acc@5 100.00 (100.00)
Epoch: [55][ 30/391]	Time  0.106 ( 0.114)	Data  0.001 ( 0.010)	Loss 8.0933e-02 (4.1828e-02)	Acc@1  96.09 ( 98.71)	Acc@5 100.00 (100.00)
Epoch: [55][ 40/391]	Time  0.107 ( 0.113)	Data  0.001 ( 0.008)	Loss 3.4760e-02 (3.9509e-02)	Acc@1  98.44 ( 98.76)	Acc@5 100.00 (100.00)
Epoch: [55][ 50/391]	Time  0.104 ( 0.112)	Data  0.001 ( 0.008)	Loss 5.7648e-02 (4.3375e-02)	Acc@1  98.44 ( 98.61)	Acc@5 100.00 (100.00)
Epoch: [55][ 60/391]	Time  0.107 ( 0.111)	Data  0.001 ( 0.007)	Loss 4.1199e-02 (4.3868e-02)	Acc@1  99.22 ( 98.60)	Acc@5 100.00 (100.00)
Epoch: [55][ 70/391]	Time  0.105 ( 0.111)	Data  0.001 ( 0.007)	Loss 2.8976e-02 (4.2391e-02)	Acc@1  99.22 ( 98.64)	Acc@5 100.00 (100.00)
Epoch: [55][ 80/391]	Time  0.104 ( 0.110)	Data  0.001 ( 0.006)	Loss 6.3538e-02 (4.1851e-02)	Acc@1  97.66 ( 98.62)	Acc@5 100.00 (100.00)
Epoch: [55][ 90/391]	Time  0.109 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.9089e-02 (4.1211e-02)	Acc@1  99.22 ( 98.63)	Acc@5 100.00 (100.00)
Epoch: [55][100/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 4.8615e-02 (4.1948e-02)	Acc@1  97.66 ( 98.62)	Acc@5 100.00 (100.00)
Epoch: [55][110/391]	Time  0.104 ( 0.110)	Data  0.001 ( 0.006)	Loss 4.2175e-02 (4.2726e-02)	Acc@1  98.44 ( 98.60)	Acc@5 100.00 (100.00)
Epoch: [55][120/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.006)	Loss 2.2659e-02 (4.1717e-02)	Acc@1  99.22 ( 98.65)	Acc@5 100.00 (100.00)
Epoch: [55][130/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.005)	Loss 2.2644e-02 (4.2077e-02)	Acc@1  99.22 ( 98.65)	Acc@5 100.00 (100.00)
Epoch: [55][140/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.005)	Loss 5.2399e-02 (4.2245e-02)	Acc@1  96.88 ( 98.63)	Acc@5 100.00 ( 99.99)
Epoch: [55][150/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.005)	Loss 1.7822e-02 (4.1727e-02)	Acc@1 100.00 ( 98.66)	Acc@5 100.00 ( 99.99)
Epoch: [55][160/391]	Time  0.106 ( 0.110)	Data  0.002 ( 0.005)	Loss 3.9581e-02 (4.2513e-02)	Acc@1  97.66 ( 98.62)	Acc@5 100.00 (100.00)
Epoch: [55][170/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.6804e-02 (4.3500e-02)	Acc@1  98.44 ( 98.55)	Acc@5 100.00 (100.00)
Epoch: [55][180/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.8350e-02 (4.3357e-02)	Acc@1  97.66 ( 98.55)	Acc@5 100.00 (100.00)
Epoch: [55][190/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.7750e-02 (4.3121e-02)	Acc@1  99.22 ( 98.55)	Acc@5 100.00 (100.00)
Epoch: [55][200/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.9684e-02 (4.2822e-02)	Acc@1 100.00 ( 98.56)	Acc@5 100.00 (100.00)
Epoch: [55][210/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.6539e-02 (4.3399e-02)	Acc@1  97.66 ( 98.52)	Acc@5 100.00 (100.00)
Epoch: [55][220/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.6276e-02 (4.3308e-02)	Acc@1  98.44 ( 98.50)	Acc@5 100.00 (100.00)
Epoch: [55][230/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.0924e-02 (4.3687e-02)	Acc@1  97.66 ( 98.48)	Acc@5 100.00 (100.00)
Epoch: [55][240/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.3331e-02 (4.3513e-02)	Acc@1  99.22 ( 98.48)	Acc@5 100.00 (100.00)
Epoch: [55][250/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.2144e-02 (4.4897e-02)	Acc@1  98.44 ( 98.42)	Acc@5 100.00 (100.00)
Epoch: [55][260/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.0393e-02 (4.5510e-02)	Acc@1  96.88 ( 98.38)	Acc@5 100.00 (100.00)
Epoch: [55][270/391]	Time  0.105 ( 0.109)	Data  0.002 ( 0.005)	Loss 7.5867e-02 (4.5791e-02)	Acc@1  96.09 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [55][280/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.4087e-02 (4.6241e-02)	Acc@1  98.44 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [55][290/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.2236e-02 (4.6206e-02)	Acc@1  98.44 ( 98.38)	Acc@5 100.00 (100.00)
Epoch: [55][300/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.5278e-02 (4.6204e-02)	Acc@1  99.22 ( 98.38)	Acc@5 100.00 (100.00)
Epoch: [55][310/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.1208e-02 (4.6269e-02)	Acc@1  98.44 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [55][320/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.1290e-02 (4.6431e-02)	Acc@1  97.66 ( 98.35)	Acc@5 100.00 (100.00)
Epoch: [55][330/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.9438e-02 (4.6430e-02)	Acc@1  98.44 ( 98.35)	Acc@5 100.00 (100.00)
Epoch: [55][340/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.7709e-02 (4.6497e-02)	Acc@1  97.66 ( 98.35)	Acc@5 100.00 (100.00)
Epoch: [55][350/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.1980e-02 (4.6914e-02)	Acc@1  97.66 ( 98.35)	Acc@5 100.00 (100.00)
Epoch: [55][360/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.4962e-02 (4.6914e-02)	Acc@1  98.44 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [55][370/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4114e-02 (4.6921e-02)	Acc@1  99.22 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [55][380/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.0599e-02 (4.6978e-02)	Acc@1  99.22 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [55][390/391]	Time  0.101 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.5589e-02 (4.7040e-02)	Acc@1  98.75 ( 98.35)	Acc@5 100.00 (100.00)
## e[55] optimizer.zero_grad (sum) time: 0.34926414489746094
## e[55]       loss.backward (sum) time: 13.759249925613403
## e[55]      optimizer.step (sum) time: 2.82738995552063
## epoch[55] training(only) time: 42.66607666015625
# Switched to evaluate mode...
Test: [  0/100]	Time  0.201 ( 0.201)	Loss 2.8320e-01 (2.8320e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.037 ( 0.053)	Loss 2.1106e-01 (4.0327e-01)	Acc@1  94.00 ( 91.09)	Acc@5 100.00 ( 99.27)
Test: [ 20/100]	Time  0.037 ( 0.045)	Loss 5.0781e-01 (4.2772e-01)	Acc@1  88.00 ( 90.67)	Acc@5 100.00 ( 99.33)
Test: [ 30/100]	Time  0.036 ( 0.043)	Loss 3.8428e-01 (4.4250e-01)	Acc@1  91.00 ( 90.87)	Acc@5  99.00 ( 99.32)
Test: [ 40/100]	Time  0.039 ( 0.041)	Loss 2.6636e-01 (4.3501e-01)	Acc@1  92.00 ( 90.85)	Acc@5  99.00 ( 99.37)
Test: [ 50/100]	Time  0.045 ( 0.041)	Loss 3.1421e-01 (4.2558e-01)	Acc@1  91.00 ( 90.92)	Acc@5 100.00 ( 99.45)
Test: [ 60/100]	Time  0.037 ( 0.040)	Loss 5.8008e-01 (4.1695e-01)	Acc@1  93.00 ( 91.02)	Acc@5  98.00 ( 99.49)
Test: [ 70/100]	Time  0.036 ( 0.040)	Loss 7.1729e-01 (4.1399e-01)	Acc@1  89.00 ( 91.01)	Acc@5  99.00 ( 99.51)
Test: [ 80/100]	Time  0.039 ( 0.039)	Loss 4.3921e-01 (4.1193e-01)	Acc@1  87.00 ( 90.94)	Acc@5 100.00 ( 99.52)
Test: [ 90/100]	Time  0.037 ( 0.039)	Loss 3.5474e-01 (4.1192e-01)	Acc@1  91.00 ( 90.79)	Acc@5 100.00 ( 99.54)
 * Acc@1 90.750 Acc@5 99.570
### epoch[55] execution time: 46.67574095726013
EPOCH 56
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [56][  0/391]	Time  0.319 ( 0.319)	Data  0.222 ( 0.222)	Loss 6.8787e-02 (6.8787e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [56][ 10/391]	Time  0.107 ( 0.127)	Data  0.001 ( 0.024)	Loss 5.6519e-02 (4.1694e-02)	Acc@1  98.44 ( 98.51)	Acc@5 100.00 (100.00)
Epoch: [56][ 20/391]	Time  0.106 ( 0.118)	Data  0.001 ( 0.014)	Loss 2.0218e-02 (4.2366e-02)	Acc@1 100.00 ( 98.70)	Acc@5 100.00 (100.00)
Epoch: [56][ 30/391]	Time  0.108 ( 0.115)	Data  0.001 ( 0.011)	Loss 1.3245e-02 (4.0011e-02)	Acc@1 100.00 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [56][ 40/391]	Time  0.106 ( 0.113)	Data  0.001 ( 0.009)	Loss 3.0823e-02 (4.3252e-02)	Acc@1  98.44 ( 98.70)	Acc@5 100.00 (100.00)
Epoch: [56][ 50/391]	Time  0.109 ( 0.112)	Data  0.001 ( 0.008)	Loss 5.2887e-02 (4.3020e-02)	Acc@1  99.22 ( 98.64)	Acc@5 100.00 (100.00)
Epoch: [56][ 60/391]	Time  0.107 ( 0.111)	Data  0.001 ( 0.008)	Loss 3.6926e-02 (4.3774e-02)	Acc@1  97.66 ( 98.54)	Acc@5 100.00 (100.00)
Epoch: [56][ 70/391]	Time  0.105 ( 0.111)	Data  0.001 ( 0.007)	Loss 3.6285e-02 (4.3147e-02)	Acc@1  99.22 ( 98.55)	Acc@5 100.00 (100.00)
Epoch: [56][ 80/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.007)	Loss 5.3375e-02 (4.4896e-02)	Acc@1  98.44 ( 98.48)	Acc@5 100.00 (100.00)
Epoch: [56][ 90/391]	Time  0.110 ( 0.110)	Data  0.001 ( 0.006)	Loss 3.7506e-02 (4.3546e-02)	Acc@1  98.44 ( 98.51)	Acc@5 100.00 (100.00)
Epoch: [56][100/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.006)	Loss 3.0045e-02 (4.3602e-02)	Acc@1 100.00 ( 98.51)	Acc@5 100.00 (100.00)
Epoch: [56][110/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 6.8665e-02 (4.3846e-02)	Acc@1  96.88 ( 98.49)	Acc@5 100.00 (100.00)
Epoch: [56][120/391]	Time  0.101 ( 0.110)	Data  0.001 ( 0.006)	Loss 5.4352e-02 (4.3687e-02)	Acc@1  97.66 ( 98.48)	Acc@5 100.00 (100.00)
Epoch: [56][130/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 4.1656e-02 (4.4501e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [56][140/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.006)	Loss 6.7017e-02 (4.4542e-02)	Acc@1  98.44 ( 98.46)	Acc@5 100.00 (100.00)
Epoch: [56][150/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.006)	Loss 1.8036e-02 (4.4456e-02)	Acc@1  99.22 ( 98.46)	Acc@5 100.00 ( 99.99)
Epoch: [56][160/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.4219e-02 (4.4588e-02)	Acc@1  96.88 ( 98.45)	Acc@5 100.00 (100.00)
Epoch: [56][170/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.9153e-02 (4.5400e-02)	Acc@1  97.66 ( 98.43)	Acc@5 100.00 (100.00)
Epoch: [56][180/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.6671e-02 (4.5511e-02)	Acc@1  99.22 ( 98.42)	Acc@5 100.00 (100.00)
Epoch: [56][190/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.2145e-02 (4.5916e-02)	Acc@1  97.66 ( 98.41)	Acc@5 100.00 (100.00)
Epoch: [56][200/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.9875e-02 (4.5916e-02)	Acc@1  99.22 ( 98.43)	Acc@5 100.00 (100.00)
Epoch: [56][210/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1169e-02 (4.5874e-02)	Acc@1 100.00 ( 98.45)	Acc@5 100.00 (100.00)
Epoch: [56][220/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.8654e-03 (4.5663e-02)	Acc@1 100.00 ( 98.45)	Acc@5 100.00 (100.00)
Epoch: [56][230/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.0618e-02 (4.6071e-02)	Acc@1  97.66 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [56][240/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.8867e-02 (4.6909e-02)	Acc@1  96.09 ( 98.40)	Acc@5 100.00 (100.00)
Epoch: [56][250/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.3722e-02 (4.7263e-02)	Acc@1  99.22 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [56][260/391]	Time  0.107 ( 0.109)	Data  0.002 ( 0.005)	Loss 6.9763e-02 (4.7581e-02)	Acc@1  97.66 ( 98.34)	Acc@5 100.00 (100.00)
Epoch: [56][270/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.8096e-02 (4.7130e-02)	Acc@1  98.44 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [56][280/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.4688e-02 (4.7402e-02)	Acc@1  97.66 ( 98.33)	Acc@5 100.00 (100.00)
Epoch: [56][290/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.6711e-02 (4.7297e-02)	Acc@1  97.66 ( 98.34)	Acc@5 100.00 (100.00)
Epoch: [56][300/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.3884e-02 (4.7499e-02)	Acc@1  97.66 ( 98.32)	Acc@5 100.00 (100.00)
Epoch: [56][310/391]	Time  0.112 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.9988e-02 (4.7308e-02)	Acc@1  98.44 ( 98.33)	Acc@5 100.00 (100.00)
Epoch: [56][320/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2231e-01 (4.7478e-02)	Acc@1  96.88 ( 98.34)	Acc@5 100.00 (100.00)
Epoch: [56][330/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.8849e-02 (4.7741e-02)	Acc@1  98.44 ( 98.34)	Acc@5 100.00 (100.00)
Epoch: [56][340/391]	Time  0.111 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.9988e-02 (4.7790e-02)	Acc@1  99.22 ( 98.34)	Acc@5 100.00 (100.00)
Epoch: [56][350/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.4948e-02 (4.7935e-02)	Acc@1  99.22 ( 98.33)	Acc@5 100.00 (100.00)
Epoch: [56][360/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.9946e-02 (4.8007e-02)	Acc@1  96.88 ( 98.32)	Acc@5 100.00 (100.00)
Epoch: [56][370/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.1838e-02 (4.7887e-02)	Acc@1  96.09 ( 98.32)	Acc@5 100.00 (100.00)
Epoch: [56][380/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.7627e-02 (4.8031e-02)	Acc@1  99.22 ( 98.32)	Acc@5 100.00 (100.00)
Epoch: [56][390/391]	Time  0.101 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.6661e-02 (4.8177e-02)	Acc@1  97.50 ( 98.31)	Acc@5 100.00 (100.00)
## e[56] optimizer.zero_grad (sum) time: 0.3540377616882324
## e[56]       loss.backward (sum) time: 13.773497343063354
## e[56]      optimizer.step (sum) time: 2.849482297897339
## epoch[56] training(only) time: 42.57659673690796
# Switched to evaluate mode...
Test: [  0/100]	Time  0.197 ( 0.197)	Loss 3.8037e-01 (3.8037e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.035 ( 0.052)	Loss 2.0178e-01 (3.6864e-01)	Acc@1  95.00 ( 91.36)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.039 ( 0.046)	Loss 4.8975e-01 (4.0705e-01)	Acc@1  86.00 ( 90.52)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.037 ( 0.043)	Loss 3.3350e-01 (4.2944e-01)	Acc@1  87.00 ( 90.68)	Acc@5 100.00 ( 99.52)
Test: [ 40/100]	Time  0.036 ( 0.042)	Loss 2.7124e-01 (4.2301e-01)	Acc@1  92.00 ( 90.66)	Acc@5  99.00 ( 99.54)
Test: [ 50/100]	Time  0.036 ( 0.041)	Loss 2.1606e-01 (4.1579e-01)	Acc@1  95.00 ( 90.90)	Acc@5 100.00 ( 99.57)
Test: [ 60/100]	Time  0.036 ( 0.040)	Loss 5.6152e-01 (4.1101e-01)	Acc@1  93.00 ( 90.92)	Acc@5  99.00 ( 99.59)
Test: [ 70/100]	Time  0.035 ( 0.040)	Loss 6.6992e-01 (4.0644e-01)	Acc@1  87.00 ( 90.92)	Acc@5  99.00 ( 99.61)
Test: [ 80/100]	Time  0.038 ( 0.040)	Loss 3.9038e-01 (4.0463e-01)	Acc@1  91.00 ( 90.95)	Acc@5 100.00 ( 99.62)
Test: [ 90/100]	Time  0.037 ( 0.039)	Loss 3.1885e-01 (3.9875e-01)	Acc@1  91.00 ( 90.95)	Acc@5 100.00 ( 99.63)
 * Acc@1 90.890 Acc@5 99.640
### epoch[56] execution time: 46.570974349975586
EPOCH 57
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [57][  0/391]	Time  0.272 ( 0.272)	Data  0.174 ( 0.174)	Loss 5.2124e-02 (5.2124e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [57][ 10/391]	Time  0.106 ( 0.123)	Data  0.001 ( 0.019)	Loss 2.4261e-02 (5.2701e-02)	Acc@1  99.22 ( 98.22)	Acc@5 100.00 (100.00)
Epoch: [57][ 20/391]	Time  0.103 ( 0.116)	Data  0.001 ( 0.012)	Loss 2.1057e-02 (4.2759e-02)	Acc@1 100.00 ( 98.62)	Acc@5 100.00 (100.00)
Epoch: [57][ 30/391]	Time  0.107 ( 0.114)	Data  0.001 ( 0.010)	Loss 4.3762e-02 (4.3130e-02)	Acc@1  98.44 ( 98.61)	Acc@5 100.00 (100.00)
Epoch: [57][ 40/391]	Time  0.107 ( 0.112)	Data  0.001 ( 0.008)	Loss 2.6733e-02 (4.1944e-02)	Acc@1  99.22 ( 98.63)	Acc@5 100.00 (100.00)
Epoch: [57][ 50/391]	Time  0.106 ( 0.112)	Data  0.001 ( 0.007)	Loss 4.2023e-02 (4.2758e-02)	Acc@1  98.44 ( 98.59)	Acc@5 100.00 (100.00)
Epoch: [57][ 60/391]	Time  0.106 ( 0.111)	Data  0.001 ( 0.007)	Loss 7.9117e-03 (4.1968e-02)	Acc@1 100.00 ( 98.58)	Acc@5 100.00 (100.00)
Epoch: [57][ 70/391]	Time  0.110 ( 0.111)	Data  0.001 ( 0.007)	Loss 7.5317e-02 (4.5132e-02)	Acc@1  96.88 ( 98.48)	Acc@5 100.00 (100.00)
Epoch: [57][ 80/391]	Time  0.107 ( 0.111)	Data  0.001 ( 0.006)	Loss 3.1647e-02 (4.3580e-02)	Acc@1  99.22 ( 98.51)	Acc@5 100.00 (100.00)
Epoch: [57][ 90/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.006)	Loss 7.8125e-02 (4.3581e-02)	Acc@1  97.66 ( 98.54)	Acc@5 100.00 (100.00)
Epoch: [57][100/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 3.8147e-02 (4.3877e-02)	Acc@1  98.44 ( 98.53)	Acc@5 100.00 (100.00)
Epoch: [57][110/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.006)	Loss 2.1164e-02 (4.3171e-02)	Acc@1  99.22 ( 98.57)	Acc@5 100.00 (100.00)
Epoch: [57][120/391]	Time  0.103 ( 0.110)	Data  0.001 ( 0.006)	Loss 3.4760e-02 (4.3058e-02)	Acc@1  99.22 ( 98.57)	Acc@5 100.00 (100.00)
Epoch: [57][130/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 3.5797e-02 (4.4179e-02)	Acc@1  98.44 ( 98.52)	Acc@5 100.00 (100.00)
Epoch: [57][140/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.005)	Loss 3.5156e-02 (4.4840e-02)	Acc@1  99.22 ( 98.48)	Acc@5 100.00 (100.00)
Epoch: [57][150/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.005)	Loss 2.4948e-02 (4.4601e-02)	Acc@1  98.44 ( 98.47)	Acc@5 100.00 (100.00)
Epoch: [57][160/391]	Time  0.101 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.4582e-02 (4.5169e-02)	Acc@1  99.22 ( 98.46)	Acc@5 100.00 (100.00)
Epoch: [57][170/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.9265e-02 (4.6120e-02)	Acc@1  96.09 ( 98.43)	Acc@5 100.00 (100.00)
Epoch: [57][180/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.2267e-02 (4.7064e-02)	Acc@1  97.66 ( 98.38)	Acc@5 100.00 (100.00)
Epoch: [57][190/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1969e-01 (4.7059e-02)	Acc@1  95.31 ( 98.38)	Acc@5 100.00 (100.00)
Epoch: [57][200/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.1707e-02 (4.7084e-02)	Acc@1  97.66 ( 98.39)	Acc@5 100.00 (100.00)
Epoch: [57][210/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.5532e-02 (4.7661e-02)	Acc@1  97.66 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [57][220/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.9612e-02 (4.7448e-02)	Acc@1  97.66 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [57][230/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.5146e-02 (4.6982e-02)	Acc@1  99.22 ( 98.39)	Acc@5 100.00 (100.00)
Epoch: [57][240/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.9103e-02 (4.6777e-02)	Acc@1  99.22 ( 98.41)	Acc@5 100.00 (100.00)
Epoch: [57][250/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.3823e-02 (4.7091e-02)	Acc@1  98.44 ( 98.39)	Acc@5 100.00 (100.00)
Epoch: [57][260/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.2542e-02 (4.6730e-02)	Acc@1  97.66 ( 98.40)	Acc@5 100.00 (100.00)
Epoch: [57][270/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.0444e-02 (4.7050e-02)	Acc@1  97.66 ( 98.39)	Acc@5 100.00 (100.00)
Epoch: [57][280/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.7089e-02 (4.7157e-02)	Acc@1  98.44 ( 98.38)	Acc@5 100.00 (100.00)
Epoch: [57][290/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.5635e-02 (4.7987e-02)	Acc@1 100.00 ( 98.35)	Acc@5 100.00 (100.00)
Epoch: [57][300/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.6853e-02 (4.8111e-02)	Acc@1  96.88 ( 98.35)	Acc@5 100.00 (100.00)
Epoch: [57][310/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.2003e-02 (4.7965e-02)	Acc@1  99.22 ( 98.35)	Acc@5 100.00 (100.00)
Epoch: [57][320/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.9957e-02 (4.8189e-02)	Acc@1  97.66 ( 98.33)	Acc@5 100.00 (100.00)
Epoch: [57][330/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.2164e-02 (4.8444e-02)	Acc@1  99.22 ( 98.32)	Acc@5 100.00 (100.00)
Epoch: [57][340/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.0435e-02 (4.8452e-02)	Acc@1  96.88 ( 98.32)	Acc@5 100.00 (100.00)
Epoch: [57][350/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.7341e-02 (4.8647e-02)	Acc@1  96.88 ( 98.31)	Acc@5 100.00 (100.00)
Epoch: [57][360/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.6449e-02 (4.8645e-02)	Acc@1  99.22 ( 98.32)	Acc@5 100.00 (100.00)
Epoch: [57][370/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.9806e-02 (4.8375e-02)	Acc@1 100.00 ( 98.33)	Acc@5 100.00 (100.00)
Epoch: [57][380/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.0558e-02 (4.8680e-02)	Acc@1  97.66 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [57][390/391]	Time  0.100 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.0333e-02 (4.8844e-02)	Acc@1  98.75 ( 98.29)	Acc@5 100.00 (100.00)
## e[57] optimizer.zero_grad (sum) time: 0.3513467311859131
## e[57]       loss.backward (sum) time: 13.744247198104858
## e[57]      optimizer.step (sum) time: 2.8511061668395996
## epoch[57] training(only) time: 42.7039999961853
# Switched to evaluate mode...
Test: [  0/100]	Time  0.197 ( 0.197)	Loss 3.3887e-01 (3.3887e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.039 ( 0.052)	Loss 2.5391e-01 (3.4199e-01)	Acc@1  93.00 ( 91.64)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.036 ( 0.045)	Loss 4.3970e-01 (3.8528e-01)	Acc@1  88.00 ( 90.81)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.045 ( 0.042)	Loss 3.4009e-01 (4.1832e-01)	Acc@1  88.00 ( 90.68)	Acc@5 100.00 ( 99.58)
Test: [ 40/100]	Time  0.036 ( 0.041)	Loss 3.0469e-01 (4.1789e-01)	Acc@1  92.00 ( 90.73)	Acc@5  99.00 ( 99.59)
Test: [ 50/100]	Time  0.038 ( 0.040)	Loss 2.3193e-01 (4.1363e-01)	Acc@1  93.00 ( 90.82)	Acc@5 100.00 ( 99.65)
Test: [ 60/100]	Time  0.038 ( 0.040)	Loss 4.6216e-01 (4.0738e-01)	Acc@1  96.00 ( 90.98)	Acc@5  99.00 ( 99.64)
Test: [ 70/100]	Time  0.040 ( 0.039)	Loss 6.8213e-01 (4.0470e-01)	Acc@1  86.00 ( 90.86)	Acc@5 100.00 ( 99.66)
Test: [ 80/100]	Time  0.039 ( 0.039)	Loss 2.4780e-01 (4.0714e-01)	Acc@1  92.00 ( 90.84)	Acc@5 100.00 ( 99.65)
Test: [ 90/100]	Time  0.036 ( 0.039)	Loss 3.3936e-01 (4.0100e-01)	Acc@1  93.00 ( 90.85)	Acc@5 100.00 ( 99.65)
 * Acc@1 90.880 Acc@5 99.670
### epoch[57] execution time: 46.68409967422485
EPOCH 58
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [58][  0/391]	Time  0.270 ( 0.270)	Data  0.177 ( 0.177)	Loss 8.7830e-02 (8.7830e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [58][ 10/391]	Time  0.108 ( 0.123)	Data  0.001 ( 0.020)	Loss 2.2797e-02 (3.9485e-02)	Acc@1 100.00 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [58][ 20/391]	Time  0.106 ( 0.115)	Data  0.001 ( 0.012)	Loss 8.5022e-02 (4.6530e-02)	Acc@1  95.31 ( 98.18)	Acc@5 100.00 (100.00)
Epoch: [58][ 30/391]	Time  0.110 ( 0.113)	Data  0.001 ( 0.010)	Loss 7.1655e-02 (4.5443e-02)	Acc@1  98.44 ( 98.29)	Acc@5 100.00 (100.00)
Epoch: [58][ 40/391]	Time  0.104 ( 0.112)	Data  0.001 ( 0.008)	Loss 5.7526e-02 (4.6446e-02)	Acc@1  97.66 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [58][ 50/391]	Time  0.108 ( 0.111)	Data  0.001 ( 0.008)	Loss 5.9418e-02 (4.8074e-02)	Acc@1  96.88 ( 98.19)	Acc@5 100.00 ( 99.98)
Epoch: [58][ 60/391]	Time  0.106 ( 0.111)	Data  0.001 ( 0.007)	Loss 2.3407e-02 (4.5759e-02)	Acc@1  99.22 ( 98.30)	Acc@5 100.00 ( 99.99)
Epoch: [58][ 70/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.007)	Loss 4.4830e-02 (4.5191e-02)	Acc@1  97.66 ( 98.34)	Acc@5 100.00 ( 99.99)
Epoch: [58][ 80/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.5076e-02 (4.3483e-02)	Acc@1 100.00 ( 98.40)	Acc@5 100.00 ( 99.99)
Epoch: [58][ 90/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 5.5389e-02 (4.4291e-02)	Acc@1  97.66 ( 98.35)	Acc@5 100.00 ( 99.99)
Epoch: [58][100/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 4.0375e-02 (4.3287e-02)	Acc@1  99.22 ( 98.41)	Acc@5 100.00 ( 99.99)
Epoch: [58][110/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.006)	Loss 2.4734e-02 (4.3547e-02)	Acc@1  98.44 ( 98.37)	Acc@5 100.00 ( 99.99)
Epoch: [58][120/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.006)	Loss 6.3965e-02 (4.3214e-02)	Acc@1  97.66 ( 98.37)	Acc@5 100.00 ( 99.99)
Epoch: [58][130/391]	Time  0.107 ( 0.109)	Data  0.002 ( 0.005)	Loss 6.1798e-02 (4.3169e-02)	Acc@1  97.66 ( 98.38)	Acc@5 100.00 ( 99.99)
Epoch: [58][140/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.5009e-02 (4.2996e-02)	Acc@1  99.22 ( 98.38)	Acc@5 100.00 ( 99.99)
Epoch: [58][150/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.9846e-02 (4.3583e-02)	Acc@1  99.22 ( 98.36)	Acc@5 100.00 ( 99.99)
Epoch: [58][160/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.8096e-02 (4.3172e-02)	Acc@1  97.66 ( 98.39)	Acc@5 100.00 (100.00)
Epoch: [58][170/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.4453e-02 (4.3829e-02)	Acc@1  96.09 ( 98.33)	Acc@5 100.00 (100.00)
Epoch: [58][180/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.9641e-02 (4.3943e-02)	Acc@1  96.88 ( 98.33)	Acc@5 100.00 (100.00)
Epoch: [58][190/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.4231e-02 (4.4214e-02)	Acc@1  98.44 ( 98.34)	Acc@5 100.00 (100.00)
Epoch: [58][200/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.8239e-02 (4.4082e-02)	Acc@1  99.22 ( 98.35)	Acc@5 100.00 (100.00)
Epoch: [58][210/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.2476e-02 (4.4031e-02)	Acc@1  99.22 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [58][220/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.4271e-02 (4.4034e-02)	Acc@1  98.44 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [58][230/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.9068e-02 (4.4032e-02)	Acc@1  99.22 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [58][240/391]	Time  0.111 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.3894e-02 (4.4559e-02)	Acc@1  97.66 ( 98.35)	Acc@5 100.00 (100.00)
Epoch: [58][250/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.3844e-02 (4.4950e-02)	Acc@1  98.44 ( 98.34)	Acc@5 100.00 (100.00)
Epoch: [58][260/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.2629e-02 (4.4570e-02)	Acc@1 100.00 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [58][270/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.5317e-02 (4.4929e-02)	Acc@1  98.44 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [58][280/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.1626e-02 (4.4784e-02)	Acc@1  99.22 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [58][290/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.6355e-02 (4.4586e-02)	Acc@1  97.66 ( 98.38)	Acc@5 100.00 (100.00)
Epoch: [58][300/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.2079e-02 (4.4429e-02)	Acc@1  99.22 ( 98.39)	Acc@5 100.00 (100.00)
Epoch: [58][310/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.3203e-02 (4.4354e-02)	Acc@1  98.44 ( 98.40)	Acc@5 100.00 (100.00)
Epoch: [58][320/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.2614e-02 (4.4384e-02)	Acc@1  99.22 ( 98.39)	Acc@5 100.00 (100.00)
Epoch: [58][330/391]	Time  0.101 ( 0.108)	Data  0.001 ( 0.005)	Loss 4.3304e-02 (4.4241e-02)	Acc@1  99.22 ( 98.39)	Acc@5 100.00 (100.00)
Epoch: [58][340/391]	Time  0.106 ( 0.108)	Data  0.001 ( 0.005)	Loss 6.1646e-02 (4.4701e-02)	Acc@1  98.44 ( 98.38)	Acc@5 100.00 (100.00)
Epoch: [58][350/391]	Time  0.108 ( 0.108)	Data  0.001 ( 0.005)	Loss 2.4002e-02 (4.4770e-02)	Acc@1  98.44 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [58][360/391]	Time  0.105 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.2558e-02 (4.5393e-02)	Acc@1 100.00 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [58][370/391]	Time  0.106 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.2433e-01 (4.6144e-02)	Acc@1  96.88 ( 98.34)	Acc@5 100.00 (100.00)
Epoch: [58][380/391]	Time  0.102 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.0364e-01 (4.6292e-02)	Acc@1  94.53 ( 98.33)	Acc@5 100.00 (100.00)
Epoch: [58][390/391]	Time  0.096 ( 0.108)	Data  0.001 ( 0.005)	Loss 9.1370e-02 (4.6234e-02)	Acc@1  96.25 ( 98.34)	Acc@5 100.00 (100.00)
## e[58] optimizer.zero_grad (sum) time: 0.359297513961792
## e[58]       loss.backward (sum) time: 13.689342498779297
## e[58]      optimizer.step (sum) time: 2.800825595855713
## epoch[58] training(only) time: 42.57167458534241
# Switched to evaluate mode...
Test: [  0/100]	Time  0.252 ( 0.252)	Loss 3.1714e-01 (3.1714e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.036 ( 0.057)	Loss 3.0933e-01 (4.0493e-01)	Acc@1  93.00 ( 90.36)	Acc@5 100.00 ( 99.55)
Test: [ 20/100]	Time  0.038 ( 0.047)	Loss 4.9414e-01 (4.2311e-01)	Acc@1  88.00 ( 90.52)	Acc@5 100.00 ( 99.52)
Test: [ 30/100]	Time  0.037 ( 0.044)	Loss 4.3213e-01 (4.4367e-01)	Acc@1  90.00 ( 90.48)	Acc@5  99.00 ( 99.39)
Test: [ 40/100]	Time  0.038 ( 0.042)	Loss 3.7793e-01 (4.3955e-01)	Acc@1  90.00 ( 90.44)	Acc@5  99.00 ( 99.44)
Test: [ 50/100]	Time  0.038 ( 0.042)	Loss 2.5781e-01 (4.3007e-01)	Acc@1  93.00 ( 90.49)	Acc@5 100.00 ( 99.45)
Test: [ 60/100]	Time  0.037 ( 0.041)	Loss 5.7666e-01 (4.2634e-01)	Acc@1  93.00 ( 90.56)	Acc@5  98.00 ( 99.49)
Test: [ 70/100]	Time  0.037 ( 0.040)	Loss 6.3086e-01 (4.2054e-01)	Acc@1  86.00 ( 90.63)	Acc@5  99.00 ( 99.52)
Test: [ 80/100]	Time  0.038 ( 0.040)	Loss 4.0356e-01 (4.2023e-01)	Acc@1  91.00 ( 90.69)	Acc@5 100.00 ( 99.53)
Test: [ 90/100]	Time  0.040 ( 0.040)	Loss 2.6807e-01 (4.1564e-01)	Acc@1  93.00 ( 90.68)	Acc@5 100.00 ( 99.54)
 * Acc@1 90.670 Acc@5 99.560
### epoch[58] execution time: 46.6075382232666
EPOCH 59
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [59][  0/391]	Time  0.270 ( 0.270)	Data  0.175 ( 0.175)	Loss 9.0027e-03 (9.0027e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [59][ 10/391]	Time  0.106 ( 0.122)	Data  0.001 ( 0.019)	Loss 7.1228e-02 (4.1939e-02)	Acc@1  99.22 ( 98.86)	Acc@5 100.00 (100.00)
Epoch: [59][ 20/391]	Time  0.109 ( 0.116)	Data  0.001 ( 0.012)	Loss 6.4819e-02 (4.2426e-02)	Acc@1  96.88 ( 98.59)	Acc@5 100.00 (100.00)
Epoch: [59][ 30/391]	Time  0.103 ( 0.113)	Data  0.001 ( 0.010)	Loss 2.5223e-02 (3.8804e-02)	Acc@1 100.00 ( 98.79)	Acc@5 100.00 (100.00)
Epoch: [59][ 40/391]	Time  0.108 ( 0.112)	Data  0.001 ( 0.008)	Loss 1.0852e-01 (4.3949e-02)	Acc@1  96.88 ( 98.61)	Acc@5 100.00 (100.00)
Epoch: [59][ 50/391]	Time  0.106 ( 0.111)	Data  0.001 ( 0.008)	Loss 4.4800e-02 (4.2753e-02)	Acc@1  97.66 ( 98.64)	Acc@5 100.00 (100.00)
Epoch: [59][ 60/391]	Time  0.106 ( 0.111)	Data  0.001 ( 0.007)	Loss 4.3152e-02 (4.2033e-02)	Acc@1  99.22 ( 98.66)	Acc@5 100.00 (100.00)
Epoch: [59][ 70/391]	Time  0.111 ( 0.111)	Data  0.001 ( 0.007)	Loss 2.8549e-02 (4.0175e-02)	Acc@1  98.44 ( 98.71)	Acc@5 100.00 (100.00)
Epoch: [59][ 80/391]	Time  0.103 ( 0.110)	Data  0.001 ( 0.006)	Loss 5.8441e-02 (3.9934e-02)	Acc@1  98.44 ( 98.67)	Acc@5 100.00 (100.00)
Epoch: [59][ 90/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.006)	Loss 9.2102e-02 (4.0628e-02)	Acc@1  96.88 ( 98.70)	Acc@5 100.00 (100.00)
Epoch: [59][100/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.006)	Loss 3.5034e-02 (4.0012e-02)	Acc@1  99.22 ( 98.73)	Acc@5 100.00 (100.00)
Epoch: [59][110/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.006)	Loss 4.8737e-02 (4.0433e-02)	Acc@1  97.66 ( 98.70)	Acc@5 100.00 (100.00)
Epoch: [59][120/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.006)	Loss 3.6987e-02 (4.1242e-02)	Acc@1  99.22 ( 98.66)	Acc@5 100.00 (100.00)
Epoch: [59][130/391]	Time  0.104 ( 0.110)	Data  0.001 ( 0.005)	Loss 1.5366e-02 (4.0737e-02)	Acc@1  99.22 ( 98.68)	Acc@5 100.00 ( 99.99)
Epoch: [59][140/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.005)	Loss 5.1849e-02 (4.1171e-02)	Acc@1  97.66 ( 98.66)	Acc@5 100.00 ( 99.99)
Epoch: [59][150/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.005)	Loss 7.0679e-02 (4.1636e-02)	Acc@1  96.88 ( 98.64)	Acc@5 100.00 ( 99.99)
Epoch: [59][160/391]	Time  0.099 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.3274e-02 (4.1942e-02)	Acc@1  98.44 ( 98.63)	Acc@5 100.00 (100.00)
Epoch: [59][170/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.0212e-02 (4.1866e-02)	Acc@1 100.00 ( 98.62)	Acc@5 100.00 (100.00)
Epoch: [59][180/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.5197e-02 (4.1198e-02)	Acc@1  99.22 ( 98.65)	Acc@5 100.00 (100.00)
Epoch: [59][190/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.9510e-02 (4.0887e-02)	Acc@1  99.22 ( 98.67)	Acc@5 100.00 (100.00)
Epoch: [59][200/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.2023e-02 (4.0894e-02)	Acc@1  97.66 ( 98.64)	Acc@5 100.00 (100.00)
Epoch: [59][210/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.8115e-02 (4.1307e-02)	Acc@1  96.88 ( 98.63)	Acc@5 100.00 (100.00)
Epoch: [59][220/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.4810e-02 (4.0823e-02)	Acc@1  98.44 ( 98.64)	Acc@5 100.00 (100.00)
Epoch: [59][230/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.4983e-02 (4.1207e-02)	Acc@1  97.66 ( 98.62)	Acc@5 100.00 (100.00)
Epoch: [59][240/391]	Time  0.111 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.4403e-02 (4.1032e-02)	Acc@1  98.44 ( 98.62)	Acc@5 100.00 (100.00)
Epoch: [59][250/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.1718e-02 (4.0991e-02)	Acc@1 100.00 ( 98.62)	Acc@5 100.00 (100.00)
Epoch: [59][260/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5511e-02 (4.1203e-02)	Acc@1  99.22 ( 98.61)	Acc@5 100.00 (100.00)
Epoch: [59][270/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.1988e-02 (4.1623e-02)	Acc@1 100.00 ( 98.59)	Acc@5 100.00 (100.00)
Epoch: [59][280/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.2867e-02 (4.1697e-02)	Acc@1  99.22 ( 98.59)	Acc@5 100.00 (100.00)
Epoch: [59][290/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.6782e-02 (4.2258e-02)	Acc@1  96.09 ( 98.57)	Acc@5 100.00 (100.00)
Epoch: [59][300/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.6204e-02 (4.2238e-02)	Acc@1  98.44 ( 98.57)	Acc@5 100.00 (100.00)
Epoch: [59][310/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5526e-02 (4.2330e-02)	Acc@1 100.00 ( 98.56)	Acc@5 100.00 (100.00)
Epoch: [59][320/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.9194e-02 (4.2466e-02)	Acc@1  98.44 ( 98.55)	Acc@5 100.00 (100.00)
Epoch: [59][330/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.6840e-02 (4.2700e-02)	Acc@1  98.44 ( 98.54)	Acc@5 100.00 (100.00)
Epoch: [59][340/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.9623e-02 (4.2615e-02)	Acc@1  99.22 ( 98.55)	Acc@5 100.00 (100.00)
Epoch: [59][350/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.7069e-02 (4.2566e-02)	Acc@1  99.22 ( 98.56)	Acc@5 100.00 (100.00)
Epoch: [59][360/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.6661e-02 (4.2160e-02)	Acc@1  98.44 ( 98.58)	Acc@5 100.00 (100.00)
Epoch: [59][370/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.9907e-02 (4.1939e-02)	Acc@1  98.44 ( 98.58)	Acc@5 100.00 (100.00)
Epoch: [59][380/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.2903e-02 (4.1868e-02)	Acc@1  99.22 ( 98.59)	Acc@5 100.00 (100.00)
Epoch: [59][390/391]	Time  0.098 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.7017e-02 (4.1958e-02)	Acc@1  97.50 ( 98.59)	Acc@5 100.00 (100.00)
## e[59] optimizer.zero_grad (sum) time: 0.35171985626220703
## e[59]       loss.backward (sum) time: 13.729801654815674
## e[59]      optimizer.step (sum) time: 2.7461066246032715
## epoch[59] training(only) time: 42.69064116477966
# Switched to evaluate mode...
Test: [  0/100]	Time  0.194 ( 0.194)	Loss 3.3521e-01 (3.3521e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.036 ( 0.051)	Loss 2.8882e-01 (3.9819e-01)	Acc@1  92.00 ( 90.82)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.039 ( 0.045)	Loss 4.6509e-01 (4.2424e-01)	Acc@1  87.00 ( 90.76)	Acc@5 100.00 ( 99.52)
Test: [ 30/100]	Time  0.037 ( 0.042)	Loss 4.3945e-01 (4.4713e-01)	Acc@1  89.00 ( 90.74)	Acc@5  99.00 ( 99.42)
Test: [ 40/100]	Time  0.036 ( 0.041)	Loss 2.9126e-01 (4.4474e-01)	Acc@1  93.00 ( 90.78)	Acc@5  99.00 ( 99.46)
Test: [ 50/100]	Time  0.037 ( 0.040)	Loss 3.1274e-01 (4.3538e-01)	Acc@1  92.00 ( 90.82)	Acc@5 100.00 ( 99.47)
Test: [ 60/100]	Time  0.039 ( 0.040)	Loss 5.3809e-01 (4.2772e-01)	Acc@1  93.00 ( 90.90)	Acc@5  99.00 ( 99.51)
Test: [ 70/100]	Time  0.038 ( 0.039)	Loss 7.0020e-01 (4.2465e-01)	Acc@1  87.00 ( 90.92)	Acc@5  99.00 ( 99.54)
Test: [ 80/100]	Time  0.037 ( 0.039)	Loss 2.8613e-01 (4.2299e-01)	Acc@1  93.00 ( 90.94)	Acc@5 100.00 ( 99.56)
Test: [ 90/100]	Time  0.038 ( 0.039)	Loss 3.4253e-01 (4.1880e-01)	Acc@1  94.00 ( 90.86)	Acc@5 100.00 ( 99.56)
 * Acc@1 90.960 Acc@5 99.580
### epoch[59] execution time: 46.660075187683105
EPOCH 60
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [60][  0/391]	Time  0.276 ( 0.276)	Data  0.177 ( 0.177)	Loss 8.4381e-03 (8.4381e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [60][ 10/391]	Time  0.104 ( 0.122)	Data  0.001 ( 0.019)	Loss 4.1901e-02 (3.4111e-02)	Acc@1  99.22 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [60][ 20/391]	Time  0.105 ( 0.116)	Data  0.001 ( 0.012)	Loss 5.4138e-02 (4.0034e-02)	Acc@1  98.44 ( 98.88)	Acc@5 100.00 (100.00)
Epoch: [60][ 30/391]	Time  0.107 ( 0.113)	Data  0.001 ( 0.010)	Loss 5.9998e-02 (3.6866e-02)	Acc@1  96.88 ( 98.92)	Acc@5 100.00 (100.00)
Epoch: [60][ 40/391]	Time  0.106 ( 0.112)	Data  0.001 ( 0.008)	Loss 2.0035e-02 (3.3821e-02)	Acc@1 100.00 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [60][ 50/391]	Time  0.107 ( 0.111)	Data  0.001 ( 0.007)	Loss 6.5247e-02 (3.2758e-02)	Acc@1  97.66 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [60][ 60/391]	Time  0.107 ( 0.111)	Data  0.001 ( 0.007)	Loss 2.6718e-02 (3.3813e-02)	Acc@1  99.22 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [60][ 70/391]	Time  0.109 ( 0.110)	Data  0.001 ( 0.007)	Loss 1.7349e-02 (3.4647e-02)	Acc@1 100.00 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [60][ 80/391]	Time  0.102 ( 0.110)	Data  0.001 ( 0.006)	Loss 2.7634e-02 (3.3289e-02)	Acc@1  98.44 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [60][ 90/391]	Time  0.111 ( 0.110)	Data  0.001 ( 0.006)	Loss 2.6230e-02 (3.2894e-02)	Acc@1  99.22 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [60][100/391]	Time  0.104 ( 0.110)	Data  0.001 ( 0.006)	Loss 2.0920e-02 (3.2923e-02)	Acc@1 100.00 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [60][110/391]	Time  0.110 ( 0.110)	Data  0.001 ( 0.006)	Loss 5.5450e-02 (3.3235e-02)	Acc@1  98.44 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [60][120/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.006)	Loss 1.5480e-02 (3.3256e-02)	Acc@1  99.22 ( 98.99)	Acc@5 100.00 ( 99.99)
Epoch: [60][130/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.3356e-02 (3.3760e-02)	Acc@1  99.22 ( 98.96)	Acc@5 100.00 ( 99.99)
Epoch: [60][140/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.6957e-02 (3.3533e-02)	Acc@1  97.66 ( 98.95)	Acc@5 100.00 ( 99.99)
Epoch: [60][150/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.0374e-02 (3.5104e-02)	Acc@1  97.66 ( 98.90)	Acc@5 100.00 ( 99.99)
Epoch: [60][160/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.9943e-02 (3.4692e-02)	Acc@1 100.00 ( 98.91)	Acc@5 100.00 ( 99.99)
Epoch: [60][170/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.2511e-02 (3.4632e-02)	Acc@1  97.66 ( 98.90)	Acc@5 100.00 ( 99.99)
Epoch: [60][180/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.7811e-02 (3.5064e-02)	Acc@1  98.44 ( 98.88)	Acc@5 100.00 ( 99.99)
Epoch: [60][190/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.8463e-02 (3.4957e-02)	Acc@1  99.22 ( 98.87)	Acc@5 100.00 ( 99.99)
Epoch: [60][200/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.5145e-02 (3.5546e-02)	Acc@1  96.88 ( 98.85)	Acc@5 100.00 ( 99.99)
Epoch: [60][210/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.0035e-02 (3.4979e-02)	Acc@1  99.22 ( 98.87)	Acc@5 100.00 ( 99.99)
Epoch: [60][220/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.4739e-02 (3.4577e-02)	Acc@1  98.44 ( 98.88)	Acc@5 100.00 ( 99.99)
Epoch: [60][230/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.7046e-03 (3.4422e-02)	Acc@1 100.00 ( 98.88)	Acc@5 100.00 ( 99.99)
Epoch: [60][240/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.3243e-02 (3.4309e-02)	Acc@1  99.22 ( 98.88)	Acc@5 100.00 ( 99.99)
Epoch: [60][250/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.8481e-02 (3.4532e-02)	Acc@1  99.22 ( 98.87)	Acc@5 100.00 ( 99.99)
Epoch: [60][260/391]	Time  0.106 ( 0.108)	Data  0.001 ( 0.005)	Loss 5.1239e-02 (3.4800e-02)	Acc@1  97.66 ( 98.85)	Acc@5 100.00 ( 99.99)
Epoch: [60][270/391]	Time  0.106 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.7853e-02 (3.4822e-02)	Acc@1  99.22 ( 98.86)	Acc@5 100.00 ( 99.99)
Epoch: [60][280/391]	Time  0.108 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.4290e-02 (3.4611e-02)	Acc@1 100.00 ( 98.86)	Acc@5 100.00 ( 99.99)
Epoch: [60][290/391]	Time  0.105 ( 0.108)	Data  0.001 ( 0.005)	Loss 3.0350e-02 (3.4528e-02)	Acc@1  99.22 ( 98.87)	Acc@5 100.00 ( 99.99)
Epoch: [60][300/391]	Time  0.107 ( 0.108)	Data  0.001 ( 0.005)	Loss 5.5542e-02 (3.4499e-02)	Acc@1  99.22 ( 98.87)	Acc@5 100.00 ( 99.99)
Epoch: [60][310/391]	Time  0.107 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.4938e-02 (3.4193e-02)	Acc@1  99.22 ( 98.88)	Acc@5 100.00 ( 99.99)
Epoch: [60][320/391]	Time  0.106 ( 0.108)	Data  0.001 ( 0.005)	Loss 4.4006e-02 (3.4378e-02)	Acc@1  98.44 ( 98.87)	Acc@5 100.00 ( 99.99)
Epoch: [60][330/391]	Time  0.105 ( 0.108)	Data  0.001 ( 0.005)	Loss 4.4647e-02 (3.4297e-02)	Acc@1  98.44 ( 98.87)	Acc@5 100.00 ( 99.99)
Epoch: [60][340/391]	Time  0.106 ( 0.108)	Data  0.001 ( 0.005)	Loss 2.5909e-02 (3.4373e-02)	Acc@1  98.44 ( 98.86)	Acc@5 100.00 ( 99.99)
Epoch: [60][350/391]	Time  0.105 ( 0.108)	Data  0.001 ( 0.005)	Loss 6.5613e-02 (3.4565e-02)	Acc@1  96.88 ( 98.84)	Acc@5 100.00 ( 99.99)
Epoch: [60][360/391]	Time  0.105 ( 0.108)	Data  0.001 ( 0.005)	Loss 4.0192e-02 (3.4720e-02)	Acc@1  99.22 ( 98.84)	Acc@5 100.00 ( 99.99)
Epoch: [60][370/391]	Time  0.100 ( 0.108)	Data  0.001 ( 0.005)	Loss 5.6610e-02 (3.4632e-02)	Acc@1  97.66 ( 98.85)	Acc@5 100.00 ( 99.99)
Epoch: [60][380/391]	Time  0.105 ( 0.108)	Data  0.001 ( 0.005)	Loss 2.0538e-02 (3.4417e-02)	Acc@1 100.00 ( 98.86)	Acc@5 100.00 ( 99.99)
Epoch: [60][390/391]	Time  0.102 ( 0.108)	Data  0.001 ( 0.005)	Loss 6.1798e-02 (3.4272e-02)	Acc@1  98.75 ( 98.87)	Acc@5 100.00 ( 99.99)
## e[60] optimizer.zero_grad (sum) time: 0.35382533073425293
## e[60]       loss.backward (sum) time: 13.728557825088501
## e[60]      optimizer.step (sum) time: 2.8525426387786865
## epoch[60] training(only) time: 42.45439267158508
# Switched to evaluate mode...
Test: [  0/100]	Time  0.197 ( 0.197)	Loss 3.5425e-01 (3.5425e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.037 ( 0.052)	Loss 2.8296e-01 (3.9638e-01)	Acc@1  92.00 ( 90.91)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.039 ( 0.045)	Loss 4.9805e-01 (4.1910e-01)	Acc@1  88.00 ( 90.90)	Acc@5 100.00 ( 99.52)
Test: [ 30/100]	Time  0.037 ( 0.042)	Loss 4.3359e-01 (4.4037e-01)	Acc@1  92.00 ( 91.06)	Acc@5  99.00 ( 99.42)
Test: [ 40/100]	Time  0.036 ( 0.041)	Loss 2.9053e-01 (4.3633e-01)	Acc@1  93.00 ( 90.98)	Acc@5  99.00 ( 99.46)
Test: [ 50/100]	Time  0.037 ( 0.040)	Loss 3.2446e-01 (4.2794e-01)	Acc@1  91.00 ( 90.96)	Acc@5 100.00 ( 99.51)
Test: [ 60/100]	Time  0.037 ( 0.040)	Loss 5.3809e-01 (4.2139e-01)	Acc@1  93.00 ( 91.05)	Acc@5  99.00 ( 99.52)
Test: [ 70/100]	Time  0.036 ( 0.039)	Loss 6.5332e-01 (4.1681e-01)	Acc@1  89.00 ( 91.11)	Acc@5  99.00 ( 99.55)
Test: [ 80/100]	Time  0.037 ( 0.039)	Loss 2.8467e-01 (4.1490e-01)	Acc@1  93.00 ( 91.15)	Acc@5 100.00 ( 99.57)
Test: [ 90/100]	Time  0.037 ( 0.039)	Loss 3.2935e-01 (4.0919e-01)	Acc@1  94.00 ( 91.11)	Acc@5 100.00 ( 99.57)
 * Acc@1 91.120 Acc@5 99.600
### epoch[60] execution time: 46.406355142593384
EPOCH 61
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [61][  0/391]	Time  0.313 ( 0.313)	Data  0.208 ( 0.208)	Loss 5.4535e-02 (5.4535e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [61][ 10/391]	Time  0.107 ( 0.127)	Data  0.001 ( 0.023)	Loss 3.0212e-02 (2.6525e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [61][ 20/391]	Time  0.106 ( 0.119)	Data  0.001 ( 0.014)	Loss 3.4454e-02 (3.1455e-02)	Acc@1  98.44 ( 98.92)	Acc@5 100.00 (100.00)
Epoch: [61][ 30/391]	Time  0.103 ( 0.116)	Data  0.001 ( 0.011)	Loss 2.1057e-02 (3.1116e-02)	Acc@1 100.00 ( 98.89)	Acc@5 100.00 (100.00)
Epoch: [61][ 40/391]	Time  0.105 ( 0.114)	Data  0.001 ( 0.009)	Loss 2.3376e-02 (3.0573e-02)	Acc@1  98.44 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [61][ 50/391]	Time  0.106 ( 0.113)	Data  0.001 ( 0.008)	Loss 3.6957e-02 (3.0241e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [61][ 60/391]	Time  0.108 ( 0.112)	Data  0.001 ( 0.008)	Loss 6.2439e-02 (3.0375e-02)	Acc@1  98.44 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [61][ 70/391]	Time  0.104 ( 0.111)	Data  0.001 ( 0.007)	Loss 2.9892e-02 (2.9706e-02)	Acc@1  98.44 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [61][ 80/391]	Time  0.106 ( 0.111)	Data  0.001 ( 0.007)	Loss 4.2877e-02 (3.3055e-02)	Acc@1  98.44 ( 98.92)	Acc@5 100.00 (100.00)
Epoch: [61][ 90/391]	Time  0.108 ( 0.111)	Data  0.001 ( 0.006)	Loss 8.4351e-02 (3.3366e-02)	Acc@1  96.88 ( 98.92)	Acc@5 100.00 (100.00)
Epoch: [61][100/391]	Time  0.107 ( 0.111)	Data  0.001 ( 0.006)	Loss 9.8038e-03 (3.2655e-02)	Acc@1 100.00 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [61][110/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 4.5746e-02 (3.1962e-02)	Acc@1  96.88 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [61][120/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.3969e-02 (3.1702e-02)	Acc@1  99.22 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [61][130/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 2.0309e-02 (3.1359e-02)	Acc@1 100.00 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [61][140/391]	Time  0.100 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.7670e-02 (3.1083e-02)	Acc@1  99.22 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [61][150/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.5854e-02 (3.1830e-02)	Acc@1 100.00 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [61][160/391]	Time  0.104 ( 0.110)	Data  0.001 ( 0.005)	Loss 2.4994e-02 (3.1860e-02)	Acc@1  98.44 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [61][170/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.005)	Loss 2.8381e-02 (3.2098e-02)	Acc@1  99.22 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [61][180/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.005)	Loss 1.5099e-02 (3.1889e-02)	Acc@1 100.00 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [61][190/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.005)	Loss 1.5312e-02 (3.2131e-02)	Acc@1 100.00 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [61][200/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.005)	Loss 2.1362e-02 (3.2354e-02)	Acc@1 100.00 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [61][210/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.005)	Loss 7.8888e-03 (3.2205e-02)	Acc@1 100.00 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [61][220/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.005)	Loss 2.7557e-02 (3.1898e-02)	Acc@1  98.44 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [61][230/391]	Time  0.103 ( 0.110)	Data  0.001 ( 0.005)	Loss 3.5248e-02 (3.2449e-02)	Acc@1  98.44 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [61][240/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.2227e-02 (3.2072e-02)	Acc@1  97.66 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [61][250/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.4643e-02 (3.1694e-02)	Acc@1  99.22 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [61][260/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.7964e-02 (3.1922e-02)	Acc@1  97.66 ( 98.94)	Acc@5 100.00 (100.00)
Epoch: [61][270/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.9957e-02 (3.1797e-02)	Acc@1  97.66 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [61][280/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.1138e-02 (3.1546e-02)	Acc@1  98.44 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [61][290/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.1088e-02 (3.1303e-02)	Acc@1 100.00 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [61][300/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.6417e-02 (3.1347e-02)	Acc@1  98.44 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [61][310/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0773e-02 (3.1147e-02)	Acc@1 100.00 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [61][320/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3527e-02 (3.0950e-02)	Acc@1  99.22 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [61][330/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.6113e-02 (3.0941e-02)	Acc@1  99.22 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [61][340/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.2629e-02 (3.0971e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [61][350/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1375e-02 (3.0743e-02)	Acc@1 100.00 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [61][360/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.8167e-02 (3.0850e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [61][370/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.9831e-02 (3.0755e-02)	Acc@1  98.44 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [61][380/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.4063e-02 (3.0829e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [61][390/391]	Time  0.101 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.8515e-03 (3.0723e-02)	Acc@1 100.00 ( 99.01)	Acc@5 100.00 (100.00)
## e[61] optimizer.zero_grad (sum) time: 0.3494701385498047
## e[61]       loss.backward (sum) time: 13.686519861221313
## e[61]      optimizer.step (sum) time: 2.788240671157837
## epoch[61] training(only) time: 42.758811235427856
# Switched to evaluate mode...
Test: [  0/100]	Time  0.231 ( 0.231)	Loss 3.3765e-01 (3.3765e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.037 ( 0.055)	Loss 2.6147e-01 (3.8947e-01)	Acc@1  93.00 ( 91.09)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.038 ( 0.047)	Loss 5.0977e-01 (4.2040e-01)	Acc@1  89.00 ( 91.05)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.036 ( 0.044)	Loss 4.2798e-01 (4.4354e-01)	Acc@1  92.00 ( 91.06)	Acc@5  99.00 ( 99.52)
Test: [ 40/100]	Time  0.041 ( 0.042)	Loss 3.1470e-01 (4.3824e-01)	Acc@1  91.00 ( 91.00)	Acc@5  99.00 ( 99.51)
Test: [ 50/100]	Time  0.036 ( 0.041)	Loss 3.1592e-01 (4.3080e-01)	Acc@1  91.00 ( 91.06)	Acc@5 100.00 ( 99.51)
Test: [ 60/100]	Time  0.040 ( 0.041)	Loss 5.7031e-01 (4.2483e-01)	Acc@1  93.00 ( 91.15)	Acc@5  99.00 ( 99.52)
Test: [ 70/100]	Time  0.036 ( 0.040)	Loss 6.8359e-01 (4.2140e-01)	Acc@1  89.00 ( 91.11)	Acc@5  99.00 ( 99.55)
Test: [ 80/100]	Time  0.036 ( 0.040)	Loss 2.5244e-01 (4.1955e-01)	Acc@1  94.00 ( 91.12)	Acc@5 100.00 ( 99.56)
Test: [ 90/100]	Time  0.036 ( 0.039)	Loss 3.0786e-01 (4.1459e-01)	Acc@1  94.00 ( 91.10)	Acc@5 100.00 ( 99.56)
 * Acc@1 91.150 Acc@5 99.590
### epoch[61] execution time: 46.78215003013611
EPOCH 62
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [62][  0/391]	Time  0.276 ( 0.276)	Data  0.172 ( 0.172)	Loss 3.2928e-02 (3.2928e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [62][ 10/391]	Time  0.108 ( 0.124)	Data  0.001 ( 0.019)	Loss 1.7609e-02 (3.1232e-02)	Acc@1  99.22 ( 98.65)	Acc@5 100.00 (100.00)
Epoch: [62][ 20/391]	Time  0.108 ( 0.116)	Data  0.001 ( 0.012)	Loss 1.6830e-02 (2.9561e-02)	Acc@1 100.00 ( 98.88)	Acc@5 100.00 (100.00)
Epoch: [62][ 30/391]	Time  0.105 ( 0.114)	Data  0.001 ( 0.009)	Loss 1.0742e-02 (2.8503e-02)	Acc@1 100.00 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [62][ 40/391]	Time  0.109 ( 0.112)	Data  0.001 ( 0.008)	Loss 5.4749e-02 (2.8997e-02)	Acc@1  97.66 ( 98.91)	Acc@5 100.00 (100.00)
Epoch: [62][ 50/391]	Time  0.106 ( 0.112)	Data  0.001 ( 0.007)	Loss 3.0151e-02 (2.7210e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [62][ 60/391]	Time  0.105 ( 0.111)	Data  0.001 ( 0.007)	Loss 4.3396e-02 (2.9128e-02)	Acc@1  99.22 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [62][ 70/391]	Time  0.106 ( 0.111)	Data  0.001 ( 0.007)	Loss 1.1909e-02 (2.8083e-02)	Acc@1 100.00 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [62][ 80/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 3.2440e-02 (2.8338e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [62][ 90/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 6.4201e-03 (2.7948e-02)	Acc@1 100.00 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [62][100/391]	Time  0.104 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.3351e-02 (2.7180e-02)	Acc@1 100.00 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [62][110/391]	Time  0.109 ( 0.110)	Data  0.001 ( 0.006)	Loss 4.0588e-02 (2.8721e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [62][120/391]	Time  0.109 ( 0.110)	Data  0.001 ( 0.006)	Loss 3.6377e-02 (2.8466e-02)	Acc@1  98.44 ( 99.08)	Acc@5 100.00 ( 99.99)
Epoch: [62][130/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3702e-02 (2.8533e-02)	Acc@1 100.00 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [62][140/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3412e-02 (2.7849e-02)	Acc@1  99.22 ( 99.09)	Acc@5 100.00 ( 99.99)
Epoch: [62][150/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.7176e-02 (2.7251e-02)	Acc@1  99.22 ( 99.12)	Acc@5 100.00 ( 99.99)
Epoch: [62][160/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.8748e-02 (2.7847e-02)	Acc@1  98.44 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [62][170/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.9784e-02 (2.8016e-02)	Acc@1  98.44 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [62][180/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.4749e-02 (2.8316e-02)	Acc@1  98.44 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [62][190/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4870e-02 (2.8675e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [62][200/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.9318e-02 (2.8571e-02)	Acc@1 100.00 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [62][210/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.6113e-02 (2.8527e-02)	Acc@1  99.22 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [62][220/391]	Time  0.107 ( 0.109)	Data  0.002 ( 0.005)	Loss 1.2589e-02 (2.8757e-02)	Acc@1 100.00 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [62][230/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.8137e-02 (2.8999e-02)	Acc@1 100.00 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [62][240/391]	Time  0.104 ( 0.109)	Data  0.002 ( 0.005)	Loss 3.4027e-02 (2.8960e-02)	Acc@1  98.44 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [62][250/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.2469e-02 (2.9060e-02)	Acc@1  96.09 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [62][260/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.7395e-02 (2.9254e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [62][270/391]	Time  0.101 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.8951e-02 (2.9183e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [62][280/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.8793e-02 (2.9445e-02)	Acc@1  98.44 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [62][290/391]	Time  0.111 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.8295e-02 (2.9315e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [62][300/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3474e-02 (2.9144e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [62][310/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.4078e-02 (2.9216e-02)	Acc@1  98.44 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [62][320/391]	Time  0.101 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.9333e-02 (2.9363e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [62][330/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.4271e-02 (2.9243e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [62][340/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.5532e-02 (2.9354e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [62][350/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.1281e-02 (2.9516e-02)	Acc@1  98.44 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [62][360/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.9587e-02 (2.9623e-02)	Acc@1  99.22 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [62][370/391]	Time  0.101 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.5297e-02 (2.9530e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [62][380/391]	Time  0.105 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.4046e-02 (2.9400e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [62][390/391]	Time  0.099 ( 0.108)	Data  0.001 ( 0.005)	Loss 3.9398e-02 (2.9647e-02)	Acc@1  98.75 ( 99.03)	Acc@5 100.00 (100.00)
## e[62] optimizer.zero_grad (sum) time: 0.3471677303314209
## e[62]       loss.backward (sum) time: 13.749772787094116
## e[62]      optimizer.step (sum) time: 2.791358470916748
## epoch[62] training(only) time: 42.56678915023804
# Switched to evaluate mode...
Test: [  0/100]	Time  0.231 ( 0.231)	Loss 3.3447e-01 (3.3447e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.037 ( 0.055)	Loss 2.6709e-01 (3.8379e-01)	Acc@1  94.00 ( 91.27)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.037 ( 0.046)	Loss 5.1709e-01 (4.1169e-01)	Acc@1  89.00 ( 91.05)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.037 ( 0.043)	Loss 4.0698e-01 (4.3246e-01)	Acc@1  91.00 ( 91.03)	Acc@5  99.00 ( 99.45)
Test: [ 40/100]	Time  0.036 ( 0.042)	Loss 3.3057e-01 (4.2960e-01)	Acc@1  92.00 ( 91.02)	Acc@5  99.00 ( 99.46)
Test: [ 50/100]	Time  0.036 ( 0.041)	Loss 3.0176e-01 (4.2107e-01)	Acc@1  92.00 ( 91.08)	Acc@5 100.00 ( 99.49)
Test: [ 60/100]	Time  0.036 ( 0.040)	Loss 5.5859e-01 (4.1387e-01)	Acc@1  93.00 ( 91.21)	Acc@5  99.00 ( 99.52)
Test: [ 70/100]	Time  0.036 ( 0.040)	Loss 6.5625e-01 (4.0971e-01)	Acc@1  89.00 ( 91.20)	Acc@5  99.00 ( 99.55)
Test: [ 80/100]	Time  0.036 ( 0.039)	Loss 2.8638e-01 (4.0815e-01)	Acc@1  92.00 ( 91.22)	Acc@5 100.00 ( 99.57)
Test: [ 90/100]	Time  0.038 ( 0.039)	Loss 3.0347e-01 (4.0292e-01)	Acc@1  94.00 ( 91.23)	Acc@5 100.00 ( 99.57)
 * Acc@1 91.260 Acc@5 99.590
### epoch[62] execution time: 46.560420751571655
EPOCH 63
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [63][  0/391]	Time  0.273 ( 0.273)	Data  0.177 ( 0.177)	Loss 4.7089e-02 (4.7089e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [63][ 10/391]	Time  0.104 ( 0.122)	Data  0.001 ( 0.020)	Loss 1.0941e-02 (3.3372e-02)	Acc@1  99.22 ( 98.86)	Acc@5 100.00 (100.00)
Epoch: [63][ 20/391]	Time  0.107 ( 0.115)	Data  0.001 ( 0.012)	Loss 1.5732e-02 (3.2822e-02)	Acc@1 100.00 ( 98.88)	Acc@5 100.00 (100.00)
Epoch: [63][ 30/391]	Time  0.110 ( 0.113)	Data  0.001 ( 0.010)	Loss 3.1982e-02 (3.3780e-02)	Acc@1  98.44 ( 98.89)	Acc@5 100.00 (100.00)
Epoch: [63][ 40/391]	Time  0.110 ( 0.112)	Data  0.001 ( 0.008)	Loss 3.4882e-02 (3.1917e-02)	Acc@1  99.22 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [63][ 50/391]	Time  0.106 ( 0.111)	Data  0.001 ( 0.007)	Loss 2.0691e-02 (2.9938e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [63][ 60/391]	Time  0.104 ( 0.111)	Data  0.001 ( 0.007)	Loss 5.4077e-02 (3.1721e-02)	Acc@1  97.66 ( 98.92)	Acc@5 100.00 (100.00)
Epoch: [63][ 70/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.007)	Loss 3.4454e-02 (3.1516e-02)	Acc@1  98.44 ( 98.92)	Acc@5 100.00 (100.00)
Epoch: [63][ 80/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 3.0121e-02 (3.0721e-02)	Acc@1  99.22 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [63][ 90/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 3.1555e-02 (3.0488e-02)	Acc@1 100.00 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [63][100/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.2085e-02 (3.0756e-02)	Acc@1 100.00 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [63][110/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.006)	Loss 3.6194e-02 (3.0856e-02)	Acc@1  98.44 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [63][120/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.006)	Loss 3.3875e-02 (3.1689e-02)	Acc@1  98.44 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [63][130/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.1443e-02 (3.1454e-02)	Acc@1  98.44 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [63][140/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.0507e-02 (3.2030e-02)	Acc@1  98.44 ( 98.94)	Acc@5 100.00 (100.00)
Epoch: [63][150/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.1595e-02 (3.1759e-02)	Acc@1  99.22 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [63][160/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4420e-02 (3.0987e-02)	Acc@1 100.00 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [63][170/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.4943e-02 (3.1414e-02)	Acc@1  98.44 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [63][180/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4351e-02 (3.0898e-02)	Acc@1 100.00 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [63][190/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.0721e-02 (3.1629e-02)	Acc@1  99.22 ( 98.94)	Acc@5 100.00 (100.00)
Epoch: [63][200/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1261e-02 (3.1009e-02)	Acc@1 100.00 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [63][210/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4824e-02 (3.0753e-02)	Acc@1 100.00 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [63][220/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.8523e-02 (3.0722e-02)	Acc@1  98.44 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [63][230/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.3304e-02 (3.0721e-02)	Acc@1  98.44 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [63][240/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.9459e-02 (3.0653e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [63][250/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.6194e-02 (3.0353e-02)	Acc@1  99.22 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [63][260/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.4643e-02 (3.0373e-02)	Acc@1  98.44 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [63][270/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.8122e-02 (3.0311e-02)	Acc@1 100.00 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [63][280/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1147e-02 (3.0282e-02)	Acc@1 100.00 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [63][290/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.6517e-03 (3.0061e-02)	Acc@1 100.00 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [63][300/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.6152e-02 (2.9933e-02)	Acc@1  97.66 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [63][310/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2306e-02 (3.0021e-02)	Acc@1 100.00 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [63][320/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.6285e-02 (2.9852e-02)	Acc@1  98.44 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [63][330/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.9335e-03 (2.9573e-02)	Acc@1 100.00 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [63][340/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.7847e-02 (2.9522e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [63][350/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.6896e-02 (2.9422e-02)	Acc@1  98.44 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [63][360/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4397e-02 (2.9431e-02)	Acc@1 100.00 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [63][370/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.3880e-02 (2.9444e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [63][380/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.3295e-02 (2.9586e-02)	Acc@1  97.66 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [63][390/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.9775e-02 (2.9701e-02)	Acc@1  98.75 ( 99.03)	Acc@5 100.00 (100.00)
## e[63] optimizer.zero_grad (sum) time: 0.34848928451538086
## e[63]       loss.backward (sum) time: 13.72249436378479
## e[63]      optimizer.step (sum) time: 2.772829055786133
## epoch[63] training(only) time: 42.636971950531006
# Switched to evaluate mode...
Test: [  0/100]	Time  0.196 ( 0.196)	Loss 3.2031e-01 (3.2031e-01)	Acc@1  87.00 ( 87.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.037 ( 0.052)	Loss 2.5781e-01 (3.8355e-01)	Acc@1  94.00 ( 91.18)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.036 ( 0.045)	Loss 5.2344e-01 (4.1104e-01)	Acc@1  89.00 ( 90.95)	Acc@5 100.00 ( 99.52)
Test: [ 30/100]	Time  0.038 ( 0.042)	Loss 3.9893e-01 (4.3215e-01)	Acc@1  92.00 ( 91.06)	Acc@5  99.00 ( 99.42)
Test: [ 40/100]	Time  0.039 ( 0.041)	Loss 3.0493e-01 (4.2885e-01)	Acc@1  91.00 ( 91.05)	Acc@5  99.00 ( 99.46)
Test: [ 50/100]	Time  0.036 ( 0.040)	Loss 2.8589e-01 (4.2051e-01)	Acc@1  91.00 ( 91.08)	Acc@5 100.00 ( 99.51)
Test: [ 60/100]	Time  0.038 ( 0.040)	Loss 5.3125e-01 (4.1230e-01)	Acc@1  93.00 ( 91.26)	Acc@5  99.00 ( 99.57)
Test: [ 70/100]	Time  0.037 ( 0.039)	Loss 6.3428e-01 (4.0755e-01)	Acc@1  89.00 ( 91.20)	Acc@5  99.00 ( 99.59)
Test: [ 80/100]	Time  0.037 ( 0.039)	Loss 2.6001e-01 (4.0584e-01)	Acc@1  92.00 ( 91.17)	Acc@5 100.00 ( 99.60)
Test: [ 90/100]	Time  0.039 ( 0.039)	Loss 3.0835e-01 (3.9980e-01)	Acc@1  94.00 ( 91.15)	Acc@5 100.00 ( 99.60)
 * Acc@1 91.180 Acc@5 99.620
### epoch[63] execution time: 46.6076283454895
EPOCH 64
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [64][  0/391]	Time  0.270 ( 0.270)	Data  0.165 ( 0.165)	Loss 1.2917e-02 (1.2917e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [64][ 10/391]	Time  0.109 ( 0.123)	Data  0.001 ( 0.019)	Loss 2.3895e-02 (2.1358e-02)	Acc@1  98.44 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [64][ 20/391]	Time  0.105 ( 0.116)	Data  0.001 ( 0.012)	Loss 7.0076e-03 (2.3487e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [64][ 30/391]	Time  0.104 ( 0.114)	Data  0.001 ( 0.009)	Loss 1.3763e-02 (2.5787e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [64][ 40/391]	Time  0.107 ( 0.112)	Data  0.001 ( 0.008)	Loss 3.6316e-02 (2.5927e-02)	Acc@1  99.22 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [64][ 50/391]	Time  0.109 ( 0.111)	Data  0.001 ( 0.007)	Loss 7.4036e-02 (2.6962e-02)	Acc@1  98.44 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [64][ 60/391]	Time  0.105 ( 0.111)	Data  0.001 ( 0.007)	Loss 1.5320e-02 (2.5563e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [64][ 70/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.006)	Loss 2.1637e-02 (2.6547e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [64][ 80/391]	Time  0.104 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.1131e-02 (2.5834e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [64][ 90/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 7.4158e-03 (2.6180e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [64][100/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.006)	Loss 1.4824e-02 (2.6043e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [64][110/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.006)	Loss 3.9642e-02 (2.6339e-02)	Acc@1  99.22 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [64][120/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.6434e-02 (2.6007e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [64][130/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.8290e-02 (2.5880e-02)	Acc@1  99.22 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [64][140/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2466e-02 (2.6041e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [64][150/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.7340e-03 (2.6725e-02)	Acc@1 100.00 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [64][160/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.7939e-02 (2.6833e-02)	Acc@1  98.44 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [64][170/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2161e-02 (2.6762e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [64][180/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.0261e-02 (2.7073e-02)	Acc@1  96.09 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [64][190/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.9333e-02 (2.6696e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [64][200/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.7566e-02 (2.6868e-02)	Acc@1  96.88 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [64][210/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.2664e-02 (2.6732e-02)	Acc@1  98.44 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [64][220/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0635e-02 (2.6806e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [64][230/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.6138e-02 (2.7040e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [64][240/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.9275e-02 (2.7626e-02)	Acc@1  98.44 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [64][250/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4503e-02 (2.7699e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [64][260/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.9760e-02 (2.8402e-02)	Acc@1  99.22 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [64][270/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.4169e-02 (2.8263e-02)	Acc@1  97.66 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [64][280/391]	Time  0.101 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.4941e-02 (2.8312e-02)	Acc@1  97.66 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [64][290/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.5531e-03 (2.8120e-02)	Acc@1 100.00 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [64][300/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.2461e-02 (2.8189e-02)	Acc@1 100.00 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [64][310/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.9907e-02 (2.8226e-02)	Acc@1  99.22 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [64][320/391]	Time  0.105 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.2917e-02 (2.8275e-02)	Acc@1 100.00 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [64][330/391]	Time  0.106 ( 0.108)	Data  0.001 ( 0.005)	Loss 3.1738e-02 (2.8559e-02)	Acc@1  98.44 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [64][340/391]	Time  0.106 ( 0.108)	Data  0.001 ( 0.005)	Loss 2.0798e-02 (2.8472e-02)	Acc@1 100.00 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [64][350/391]	Time  0.099 ( 0.108)	Data  0.001 ( 0.005)	Loss 2.5650e-02 (2.8406e-02)	Acc@1  99.22 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [64][360/391]	Time  0.104 ( 0.108)	Data  0.001 ( 0.005)	Loss 3.4119e-02 (2.8401e-02)	Acc@1  99.22 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [64][370/391]	Time  0.106 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.8250e-02 (2.8402e-02)	Acc@1  99.22 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [64][380/391]	Time  0.104 ( 0.108)	Data  0.001 ( 0.005)	Loss 2.1301e-02 (2.8148e-02)	Acc@1  99.22 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [64][390/391]	Time  0.102 ( 0.108)	Data  0.001 ( 0.005)	Loss 8.1360e-02 (2.8237e-02)	Acc@1  96.25 ( 99.12)	Acc@5 100.00 (100.00)
## e[64] optimizer.zero_grad (sum) time: 0.3541440963745117
## e[64]       loss.backward (sum) time: 13.716826915740967
## e[64]      optimizer.step (sum) time: 2.7954788208007812
## epoch[64] training(only) time: 42.518187284469604
# Switched to evaluate mode...
Test: [  0/100]	Time  0.197 ( 0.197)	Loss 3.3008e-01 (3.3008e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.037 ( 0.052)	Loss 2.6025e-01 (3.9215e-01)	Acc@1  94.00 ( 91.09)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.035 ( 0.045)	Loss 5.2148e-01 (4.1925e-01)	Acc@1  89.00 ( 91.05)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.036 ( 0.042)	Loss 4.3628e-01 (4.4132e-01)	Acc@1  92.00 ( 91.10)	Acc@5  99.00 ( 99.45)
Test: [ 40/100]	Time  0.038 ( 0.041)	Loss 3.1519e-01 (4.3626e-01)	Acc@1  91.00 ( 91.02)	Acc@5  99.00 ( 99.49)
Test: [ 50/100]	Time  0.040 ( 0.040)	Loss 3.2568e-01 (4.2798e-01)	Acc@1  91.00 ( 91.10)	Acc@5 100.00 ( 99.51)
Test: [ 60/100]	Time  0.039 ( 0.040)	Loss 5.4639e-01 (4.2080e-01)	Acc@1  93.00 ( 91.15)	Acc@5  99.00 ( 99.56)
Test: [ 70/100]	Time  0.038 ( 0.039)	Loss 6.4014e-01 (4.1420e-01)	Acc@1  89.00 ( 91.17)	Acc@5  99.00 ( 99.58)
Test: [ 80/100]	Time  0.036 ( 0.039)	Loss 2.8784e-01 (4.1224e-01)	Acc@1  92.00 ( 91.19)	Acc@5 100.00 ( 99.58)
Test: [ 90/100]	Time  0.035 ( 0.039)	Loss 3.0273e-01 (4.0598e-01)	Acc@1  94.00 ( 91.19)	Acc@5 100.00 ( 99.58)
 * Acc@1 91.160 Acc@5 99.610
### epoch[64] execution time: 46.480473279953
EPOCH 65
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [65][  0/391]	Time  0.267 ( 0.267)	Data  0.165 ( 0.165)	Loss 2.9022e-02 (2.9022e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [65][ 10/391]	Time  0.107 ( 0.122)	Data  0.001 ( 0.018)	Loss 2.8305e-02 (3.6579e-02)	Acc@1  99.22 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [65][ 20/391]	Time  0.105 ( 0.115)	Data  0.001 ( 0.012)	Loss 1.3756e-02 (3.0091e-02)	Acc@1 100.00 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [65][ 30/391]	Time  0.106 ( 0.113)	Data  0.001 ( 0.009)	Loss 8.3618e-03 (2.8193e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [65][ 40/391]	Time  0.104 ( 0.112)	Data  0.001 ( 0.008)	Loss 3.3081e-02 (2.9019e-02)	Acc@1  98.44 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [65][ 50/391]	Time  0.107 ( 0.111)	Data  0.001 ( 0.007)	Loss 1.1505e-02 (2.6314e-02)	Acc@1 100.00 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [65][ 60/391]	Time  0.111 ( 0.110)	Data  0.001 ( 0.007)	Loss 4.2603e-02 (2.7098e-02)	Acc@1  97.66 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [65][ 70/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.6586e-02 (2.6973e-02)	Acc@1 100.00 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [65][ 80/391]	Time  0.101 ( 0.110)	Data  0.001 ( 0.006)	Loss 3.3203e-02 (2.6478e-02)	Acc@1  98.44 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [65][ 90/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 3.0655e-02 (2.7883e-02)	Acc@1  99.22 ( 99.09)	Acc@5 100.00 ( 99.99)
Epoch: [65][100/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.006)	Loss 1.6083e-02 (2.7714e-02)	Acc@1  99.22 ( 99.09)	Acc@5 100.00 ( 99.99)
Epoch: [65][110/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.006)	Loss 2.5757e-02 (2.7008e-02)	Acc@1  99.22 ( 99.12)	Acc@5 100.00 ( 99.99)
Epoch: [65][120/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5038e-02 (2.7049e-02)	Acc@1 100.00 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [65][130/391]	Time  0.100 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.6031e-02 (2.6493e-02)	Acc@1  98.44 ( 99.16)	Acc@5 100.00 ( 99.99)
Epoch: [65][140/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.7807e-02 (2.5907e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 ( 99.99)
Epoch: [65][150/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.9363e-02 (2.5862e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 ( 99.99)
Epoch: [65][160/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4389e-02 (2.5979e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [65][170/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.0838e-02 (2.5651e-02)	Acc@1  98.44 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [65][180/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.7573e-02 (2.5622e-02)	Acc@1  99.22 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [65][190/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.5024e-02 (2.5739e-02)	Acc@1  98.44 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [65][200/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.2949e-02 (2.5853e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [65][210/391]	Time  0.098 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2978e-02 (2.5994e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [65][220/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2856e-02 (2.6063e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [65][230/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.0264e-02 (2.6069e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 ( 99.99)
Epoch: [65][240/391]	Time  0.107 ( 0.109)	Data  0.002 ( 0.005)	Loss 2.0844e-02 (2.5981e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 ( 99.99)
Epoch: [65][250/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.0075e-02 (2.6053e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 ( 99.99)
Epoch: [65][260/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5190e-02 (2.6777e-02)	Acc@1 100.00 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [65][270/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.9974e-02 (2.6727e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [65][280/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5404e-02 (2.6600e-02)	Acc@1 100.00 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [65][290/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.6876e-02 (2.6394e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 ( 99.99)
Epoch: [65][300/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0666e-02 (2.6374e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 ( 99.99)
Epoch: [65][310/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.7700e-02 (2.6786e-02)	Acc@1  99.22 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [65][320/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.1912e-02 (2.6875e-02)	Acc@1  99.22 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [65][330/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.9031e-02 (2.6973e-02)	Acc@1  98.44 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [65][340/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4488e-02 (2.7285e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [65][350/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.8799e-02 (2.7181e-02)	Acc@1  99.22 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [65][360/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.8051e-02 (2.7437e-02)	Acc@1  99.22 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [65][370/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.6652e-02 (2.7400e-02)	Acc@1  98.44 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [65][380/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3496e-02 (2.7354e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [65][390/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.7481e-02 (2.7434e-02)	Acc@1  98.75 ( 99.14)	Acc@5 100.00 ( 99.99)
## e[65] optimizer.zero_grad (sum) time: 0.35342955589294434
## e[65]       loss.backward (sum) time: 13.741183757781982
## e[65]      optimizer.step (sum) time: 2.845001459121704
## epoch[65] training(only) time: 42.56418466567993
# Switched to evaluate mode...
Test: [  0/100]	Time  0.192 ( 0.192)	Loss 3.4497e-01 (3.4497e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.036 ( 0.052)	Loss 2.6562e-01 (3.8795e-01)	Acc@1  93.00 ( 91.18)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.037 ( 0.045)	Loss 5.1270e-01 (4.1423e-01)	Acc@1  88.00 ( 91.05)	Acc@5 100.00 ( 99.52)
Test: [ 30/100]	Time  0.037 ( 0.042)	Loss 4.1040e-01 (4.3513e-01)	Acc@1  92.00 ( 91.16)	Acc@5  99.00 ( 99.45)
Test: [ 40/100]	Time  0.037 ( 0.041)	Loss 3.0103e-01 (4.3060e-01)	Acc@1  92.00 ( 91.10)	Acc@5  99.00 ( 99.46)
Test: [ 50/100]	Time  0.037 ( 0.040)	Loss 2.8467e-01 (4.2305e-01)	Acc@1  91.00 ( 91.08)	Acc@5 100.00 ( 99.47)
Test: [ 60/100]	Time  0.036 ( 0.040)	Loss 5.4980e-01 (4.1530e-01)	Acc@1  93.00 ( 91.31)	Acc@5  99.00 ( 99.52)
Test: [ 70/100]	Time  0.039 ( 0.039)	Loss 6.3867e-01 (4.1005e-01)	Acc@1  89.00 ( 91.30)	Acc@5  99.00 ( 99.55)
Test: [ 80/100]	Time  0.042 ( 0.039)	Loss 2.7222e-01 (4.0891e-01)	Acc@1  92.00 ( 91.31)	Acc@5 100.00 ( 99.57)
Test: [ 90/100]	Time  0.038 ( 0.039)	Loss 3.1494e-01 (4.0297e-01)	Acc@1  94.00 ( 91.31)	Acc@5 100.00 ( 99.57)
 * Acc@1 91.330 Acc@5 99.590
### epoch[65] execution time: 46.507967472076416
EPOCH 66
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [66][  0/391]	Time  0.313 ( 0.313)	Data  0.212 ( 0.212)	Loss 8.8806e-03 (8.8806e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [66][ 10/391]	Time  0.106 ( 0.126)	Data  0.001 ( 0.023)	Loss 3.5095e-02 (2.1688e-02)	Acc@1  98.44 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [66][ 20/391]	Time  0.105 ( 0.117)	Data  0.001 ( 0.014)	Loss 5.8899e-02 (2.7014e-02)	Acc@1  98.44 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [66][ 30/391]	Time  0.105 ( 0.114)	Data  0.001 ( 0.011)	Loss 2.2812e-02 (2.6591e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [66][ 40/391]	Time  0.105 ( 0.113)	Data  0.001 ( 0.009)	Loss 3.7292e-02 (2.4701e-02)	Acc@1  98.44 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [66][ 50/391]	Time  0.106 ( 0.111)	Data  0.001 ( 0.008)	Loss 5.0323e-02 (2.7515e-02)	Acc@1  97.66 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [66][ 60/391]	Time  0.105 ( 0.111)	Data  0.001 ( 0.007)	Loss 1.4755e-02 (2.6215e-02)	Acc@1  99.22 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [66][ 70/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.007)	Loss 1.0551e-02 (2.4885e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [66][ 80/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.007)	Loss 2.5497e-02 (2.4208e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [66][ 90/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 3.0579e-02 (2.4656e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [66][100/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 4.2206e-02 (2.4665e-02)	Acc@1  97.66 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [66][110/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.2238e-02 (2.4222e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [66][120/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.006)	Loss 1.6388e-02 (2.4642e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [66][130/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.006)	Loss 1.4969e-02 (2.4794e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [66][140/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.006)	Loss 2.1835e-02 (2.4683e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [66][150/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.006)	Loss 7.2571e-02 (2.4957e-02)	Acc@1  96.88 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [66][160/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.1805e-02 (2.5147e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [66][170/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.9703e-02 (2.5923e-02)	Acc@1  98.44 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [66][180/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.1149e-02 (2.5724e-02)	Acc@1 100.00 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [66][190/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.5309e-02 (2.5737e-02)	Acc@1  98.44 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [66][200/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.9978e-02 (2.6208e-02)	Acc@1  98.44 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [66][210/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4275e-02 (2.6227e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [66][220/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2039e-02 (2.6163e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [66][230/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5579e-02 (2.5936e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [66][240/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.2643e-02 (2.5908e-02)	Acc@1  97.66 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [66][250/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.3417e-02 (2.6053e-02)	Acc@1  98.44 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [66][260/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.8300e-02 (2.5816e-02)	Acc@1  98.44 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [66][270/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0750e-02 (2.5861e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [66][280/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.8727e-02 (2.5915e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [66][290/391]	Time  0.105 ( 0.109)	Data  0.002 ( 0.005)	Loss 2.9678e-02 (2.5878e-02)	Acc@1  98.44 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [66][300/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.2247e-02 (2.6078e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [66][310/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.7710e-02 (2.6238e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [66][320/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.0792e-02 (2.6201e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [66][330/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.6968e-02 (2.6208e-02)	Acc@1  98.44 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [66][340/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.8370e-02 (2.6497e-02)	Acc@1  98.44 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [66][350/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.5828e-02 (2.6377e-02)	Acc@1  98.44 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [66][360/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.6703e-02 (2.6456e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [66][370/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.2781e-02 (2.6398e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [66][380/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.1347e-02 (2.6421e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [66][390/391]	Time  0.099 ( 0.108)	Data  0.001 ( 0.005)	Loss 8.3466e-03 (2.6439e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
## e[66] optimizer.zero_grad (sum) time: 0.35413193702697754
## e[66]       loss.backward (sum) time: 13.69872498512268
## e[66]      optimizer.step (sum) time: 2.7227063179016113
## epoch[66] training(only) time: 42.594985008239746
# Switched to evaluate mode...
Test: [  0/100]	Time  0.193 ( 0.193)	Loss 3.4180e-01 (3.4180e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.036 ( 0.052)	Loss 2.4219e-01 (3.8296e-01)	Acc@1  94.00 ( 91.73)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.039 ( 0.045)	Loss 5.0732e-01 (4.0982e-01)	Acc@1  90.00 ( 91.57)	Acc@5 100.00 ( 99.52)
Test: [ 30/100]	Time  0.036 ( 0.043)	Loss 4.0430e-01 (4.3136e-01)	Acc@1  92.00 ( 91.52)	Acc@5  99.00 ( 99.45)
Test: [ 40/100]	Time  0.037 ( 0.041)	Loss 3.0322e-01 (4.2696e-01)	Acc@1  91.00 ( 91.34)	Acc@5  99.00 ( 99.49)
Test: [ 50/100]	Time  0.037 ( 0.041)	Loss 2.8589e-01 (4.1955e-01)	Acc@1  92.00 ( 91.25)	Acc@5 100.00 ( 99.49)
Test: [ 60/100]	Time  0.039 ( 0.040)	Loss 5.5713e-01 (4.1341e-01)	Acc@1  93.00 ( 91.43)	Acc@5  99.00 ( 99.54)
Test: [ 70/100]	Time  0.040 ( 0.039)	Loss 6.4160e-01 (4.0801e-01)	Acc@1  88.00 ( 91.44)	Acc@5  99.00 ( 99.58)
Test: [ 80/100]	Time  0.035 ( 0.039)	Loss 2.7246e-01 (4.0667e-01)	Acc@1  92.00 ( 91.51)	Acc@5 100.00 ( 99.59)
Test: [ 90/100]	Time  0.036 ( 0.039)	Loss 3.1665e-01 (4.0083e-01)	Acc@1  94.00 ( 91.45)	Acc@5 100.00 ( 99.59)
 * Acc@1 91.460 Acc@5 99.610
### epoch[66] execution time: 46.57048678398132
EPOCH 67
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [67][  0/391]	Time  0.275 ( 0.275)	Data  0.179 ( 0.179)	Loss 2.8519e-02 (2.8519e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [67][ 10/391]	Time  0.106 ( 0.122)	Data  0.001 ( 0.020)	Loss 2.2339e-02 (3.1414e-02)	Acc@1  99.22 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [67][ 20/391]	Time  0.105 ( 0.115)	Data  0.001 ( 0.012)	Loss 2.0554e-02 (2.9176e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 ( 99.96)
Epoch: [67][ 30/391]	Time  0.104 ( 0.112)	Data  0.001 ( 0.010)	Loss 6.0516e-02 (2.8452e-02)	Acc@1  97.66 ( 99.17)	Acc@5 100.00 ( 99.97)
Epoch: [67][ 40/391]	Time  0.112 ( 0.111)	Data  0.001 ( 0.008)	Loss 2.7695e-02 (2.6911e-02)	Acc@1  97.66 ( 99.14)	Acc@5 100.00 ( 99.98)
Epoch: [67][ 50/391]	Time  0.104 ( 0.111)	Data  0.001 ( 0.007)	Loss 1.7838e-02 (2.6620e-02)	Acc@1  99.22 ( 99.11)	Acc@5 100.00 ( 99.98)
Epoch: [67][ 60/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.007)	Loss 4.5166e-02 (2.6954e-02)	Acc@1  98.44 ( 99.12)	Acc@5 100.00 ( 99.99)
Epoch: [67][ 70/391]	Time  0.103 ( 0.110)	Data  0.001 ( 0.007)	Loss 5.6915e-02 (2.7077e-02)	Acc@1  98.44 ( 99.10)	Acc@5 100.00 ( 99.99)
Epoch: [67][ 80/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.006)	Loss 1.6510e-02 (2.6699e-02)	Acc@1  99.22 ( 99.10)	Acc@5 100.00 ( 99.99)
Epoch: [67][ 90/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.006)	Loss 2.4017e-02 (2.6660e-02)	Acc@1  99.22 ( 99.09)	Acc@5 100.00 ( 99.99)
Epoch: [67][100/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.006)	Loss 2.6138e-02 (2.6993e-02)	Acc@1  98.44 ( 99.07)	Acc@5 100.00 ( 99.99)
Epoch: [67][110/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.006)	Loss 1.4389e-02 (2.7900e-02)	Acc@1 100.00 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [67][120/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.006)	Loss 2.6581e-02 (2.8180e-02)	Acc@1 100.00 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [67][130/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.7105e-02 (2.7676e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 ( 99.99)
Epoch: [67][140/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.4231e-02 (2.7472e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 ( 99.99)
Epoch: [67][150/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.3335e-02 (2.7220e-02)	Acc@1  98.44 ( 99.05)	Acc@5 100.00 ( 99.99)
Epoch: [67][160/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.6143e-02 (2.7164e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [67][170/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.7133e-03 (2.7059e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [67][180/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.6779e-02 (2.7429e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [67][190/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.8398e-03 (2.7159e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [67][200/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.0853e-02 (2.7187e-02)	Acc@1  97.66 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [67][210/391]	Time  0.100 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2672e-02 (2.7218e-02)	Acc@1 100.00 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [67][220/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.6163e-02 (2.6931e-02)	Acc@1  98.44 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [67][230/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.6130e-03 (2.6613e-02)	Acc@1 100.00 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [67][240/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3031e-02 (2.6711e-02)	Acc@1 100.00 ( 99.09)	Acc@5 100.00 ( 99.99)
Epoch: [67][250/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.6871e-02 (2.6635e-02)	Acc@1  99.22 ( 99.09)	Acc@5 100.00 ( 99.99)
Epoch: [67][260/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.3618e-03 (2.6728e-02)	Acc@1 100.00 ( 99.09)	Acc@5 100.00 ( 99.99)
Epoch: [67][270/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.5651e-03 (2.6853e-02)	Acc@1 100.00 ( 99.09)	Acc@5 100.00 ( 99.99)
Epoch: [67][280/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.9805e-02 (2.6908e-02)	Acc@1  98.44 ( 99.09)	Acc@5 100.00 ( 99.99)
Epoch: [67][290/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.9053e-02 (2.7139e-02)	Acc@1  97.66 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [67][300/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3878e-02 (2.7300e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 ( 99.99)
Epoch: [67][310/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.0426e-02 (2.7242e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 ( 99.99)
Epoch: [67][320/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.8473e-02 (2.7546e-02)	Acc@1  98.44 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [67][330/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.5665e-02 (2.7196e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [67][340/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.0894e-02 (2.7221e-02)	Acc@1  98.44 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [67][350/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.0038e-03 (2.6997e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [67][360/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.5767e-02 (2.6885e-02)	Acc@1  98.44 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [67][370/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.3331e-02 (2.7068e-02)	Acc@1  99.22 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [67][380/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.6235e-02 (2.6840e-02)	Acc@1 100.00 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [67][390/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.3678e-03 (2.6703e-02)	Acc@1 100.00 ( 99.09)	Acc@5 100.00 (100.00)
## e[67] optimizer.zero_grad (sum) time: 0.3510465621948242
## e[67]       loss.backward (sum) time: 13.687199831008911
## e[67]      optimizer.step (sum) time: 2.7800259590148926
## epoch[67] training(only) time: 42.68230175971985
# Switched to evaluate mode...
Test: [  0/100]	Time  0.201 ( 0.201)	Loss 3.4814e-01 (3.4814e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.037 ( 0.052)	Loss 2.5610e-01 (3.9077e-01)	Acc@1  94.00 ( 91.36)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.035 ( 0.045)	Loss 5.2979e-01 (4.1570e-01)	Acc@1  89.00 ( 91.29)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.037 ( 0.042)	Loss 4.1895e-01 (4.3688e-01)	Acc@1  91.00 ( 91.23)	Acc@5  99.00 ( 99.48)
Test: [ 40/100]	Time  0.036 ( 0.041)	Loss 3.1201e-01 (4.3175e-01)	Acc@1  91.00 ( 91.17)	Acc@5  99.00 ( 99.49)
Test: [ 50/100]	Time  0.037 ( 0.040)	Loss 3.1982e-01 (4.2375e-01)	Acc@1  91.00 ( 91.10)	Acc@5 100.00 ( 99.53)
Test: [ 60/100]	Time  0.038 ( 0.040)	Loss 5.4980e-01 (4.1701e-01)	Acc@1  94.00 ( 91.26)	Acc@5  99.00 ( 99.56)
Test: [ 70/100]	Time  0.036 ( 0.039)	Loss 6.4355e-01 (4.1137e-01)	Acc@1  88.00 ( 91.24)	Acc@5  99.00 ( 99.58)
Test: [ 80/100]	Time  0.038 ( 0.039)	Loss 2.9199e-01 (4.0964e-01)	Acc@1  92.00 ( 91.30)	Acc@5 100.00 ( 99.58)
Test: [ 90/100]	Time  0.038 ( 0.039)	Loss 3.0347e-01 (4.0285e-01)	Acc@1  94.00 ( 91.29)	Acc@5 100.00 ( 99.58)
 * Acc@1 91.300 Acc@5 99.600
### epoch[67] execution time: 46.653141498565674
EPOCH 68
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [68][  0/391]	Time  0.265 ( 0.265)	Data  0.166 ( 0.166)	Loss 2.9739e-02 (2.9739e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [68][ 10/391]	Time  0.106 ( 0.122)	Data  0.001 ( 0.019)	Loss 4.1321e-02 (2.8720e-02)	Acc@1  97.66 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [68][ 20/391]	Time  0.106 ( 0.116)	Data  0.001 ( 0.012)	Loss 2.3590e-02 (2.6978e-02)	Acc@1  98.44 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [68][ 30/391]	Time  0.106 ( 0.113)	Data  0.001 ( 0.009)	Loss 2.5635e-02 (2.6198e-02)	Acc@1  98.44 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [68][ 40/391]	Time  0.107 ( 0.112)	Data  0.001 ( 0.008)	Loss 5.2032e-02 (2.7056e-02)	Acc@1  98.44 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [68][ 50/391]	Time  0.108 ( 0.111)	Data  0.001 ( 0.007)	Loss 3.8849e-02 (2.8198e-02)	Acc@1  98.44 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [68][ 60/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.007)	Loss 2.5421e-02 (2.8324e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [68][ 70/391]	Time  0.111 ( 0.110)	Data  0.001 ( 0.006)	Loss 3.7476e-02 (2.9406e-02)	Acc@1  98.44 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [68][ 80/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 8.9493e-03 (2.9813e-02)	Acc@1 100.00 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [68][ 90/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.006)	Loss 2.6459e-02 (2.9446e-02)	Acc@1  99.22 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [68][100/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.006)	Loss 9.3933e-02 (2.9792e-02)	Acc@1  96.88 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [68][110/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.006)	Loss 2.4353e-02 (2.9621e-02)	Acc@1  99.22 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [68][120/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.8198e-02 (3.0061e-02)	Acc@1  98.44 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [68][130/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.2812e-02 (2.9996e-02)	Acc@1  99.22 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [68][140/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.6152e-03 (2.9672e-02)	Acc@1 100.00 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [68][150/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.2124e-02 (2.9206e-02)	Acc@1  98.44 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [68][160/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.6599e-02 (2.9231e-02)	Acc@1  98.44 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [68][170/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.5278e-02 (2.9121e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [68][180/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.0838e-02 (2.8708e-02)	Acc@1  98.44 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [68][190/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.5956e-03 (2.8803e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [68][200/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.7227e-02 (2.8533e-02)	Acc@1 100.00 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [68][210/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.0293e-02 (2.8227e-02)	Acc@1  98.44 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [68][220/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.5940e-02 (2.8053e-02)	Acc@1  99.22 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [68][230/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1024e-02 (2.7574e-02)	Acc@1 100.00 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [68][240/391]	Time  0.105 ( 0.108)	Data  0.001 ( 0.005)	Loss 3.7079e-02 (2.7461e-02)	Acc@1  98.44 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [68][250/391]	Time  0.105 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.2482e-02 (2.7437e-02)	Acc@1  99.22 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [68][260/391]	Time  0.106 ( 0.108)	Data  0.001 ( 0.005)	Loss 2.6855e-02 (2.7566e-02)	Acc@1  99.22 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [68][270/391]	Time  0.107 ( 0.108)	Data  0.001 ( 0.005)	Loss 2.1118e-02 (2.7506e-02)	Acc@1 100.00 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [68][280/391]	Time  0.107 ( 0.108)	Data  0.001 ( 0.005)	Loss 4.3640e-02 (2.7514e-02)	Acc@1  99.22 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [68][290/391]	Time  0.108 ( 0.108)	Data  0.001 ( 0.005)	Loss 5.3329e-03 (2.7515e-02)	Acc@1 100.00 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [68][300/391]	Time  0.104 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.2657e-02 (2.7469e-02)	Acc@1 100.00 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [68][310/391]	Time  0.108 ( 0.108)	Data  0.001 ( 0.005)	Loss 5.7587e-02 (2.7515e-02)	Acc@1  99.22 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [68][320/391]	Time  0.106 ( 0.108)	Data  0.002 ( 0.005)	Loss 8.1406e-03 (2.7234e-02)	Acc@1 100.00 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [68][330/391]	Time  0.106 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.4641e-02 (2.7051e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [68][340/391]	Time  0.106 ( 0.108)	Data  0.001 ( 0.005)	Loss 4.6600e-02 (2.7234e-02)	Acc@1  97.66 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [68][350/391]	Time  0.106 ( 0.108)	Data  0.001 ( 0.005)	Loss 2.7924e-02 (2.7152e-02)	Acc@1  99.22 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [68][360/391]	Time  0.105 ( 0.108)	Data  0.001 ( 0.005)	Loss 3.9093e-02 (2.7177e-02)	Acc@1  99.22 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [68][370/391]	Time  0.109 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.3191e-02 (2.7029e-02)	Acc@1 100.00 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [68][380/391]	Time  0.109 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.3412e-02 (2.6869e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [68][390/391]	Time  0.100 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.4748e-02 (2.6794e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 (100.00)
## e[68] optimizer.zero_grad (sum) time: 0.3515768051147461
## e[68]       loss.backward (sum) time: 13.728204011917114
## e[68]      optimizer.step (sum) time: 2.7710182666778564
## epoch[68] training(only) time: 42.50678300857544
# Switched to evaluate mode...
Test: [  0/100]	Time  0.239 ( 0.239)	Loss 3.3765e-01 (3.3765e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.039 ( 0.056)	Loss 2.6538e-01 (3.8390e-01)	Acc@1  94.00 ( 91.27)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.036 ( 0.047)	Loss 5.3809e-01 (4.0902e-01)	Acc@1  90.00 ( 91.14)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.036 ( 0.044)	Loss 3.9111e-01 (4.2840e-01)	Acc@1  89.00 ( 91.10)	Acc@5 100.00 ( 99.52)
Test: [ 40/100]	Time  0.036 ( 0.042)	Loss 3.3228e-01 (4.2596e-01)	Acc@1  92.00 ( 91.00)	Acc@5  99.00 ( 99.54)
Test: [ 50/100]	Time  0.039 ( 0.041)	Loss 2.9858e-01 (4.1831e-01)	Acc@1  93.00 ( 91.04)	Acc@5 100.00 ( 99.59)
Test: [ 60/100]	Time  0.036 ( 0.041)	Loss 5.4150e-01 (4.1146e-01)	Acc@1  93.00 ( 91.20)	Acc@5  99.00 ( 99.62)
Test: [ 70/100]	Time  0.036 ( 0.040)	Loss 6.4355e-01 (4.0477e-01)	Acc@1  89.00 ( 91.24)	Acc@5  99.00 ( 99.63)
Test: [ 80/100]	Time  0.036 ( 0.040)	Loss 2.7881e-01 (4.0444e-01)	Acc@1  92.00 ( 91.23)	Acc@5 100.00 ( 99.64)
Test: [ 90/100]	Time  0.036 ( 0.039)	Loss 3.0933e-01 (3.9775e-01)	Acc@1  94.00 ( 91.25)	Acc@5 100.00 ( 99.64)
 * Acc@1 91.280 Acc@5 99.650
### epoch[68] execution time: 46.51454281806946
EPOCH 69
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [69][  0/391]	Time  0.278 ( 0.278)	Data  0.176 ( 0.176)	Loss 7.6180e-03 (7.6180e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [69][ 10/391]	Time  0.106 ( 0.123)	Data  0.001 ( 0.019)	Loss 2.4521e-02 (2.2412e-02)	Acc@1  98.44 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [69][ 20/391]	Time  0.109 ( 0.116)	Data  0.001 ( 0.012)	Loss 1.2474e-02 (2.3324e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [69][ 30/391]	Time  0.109 ( 0.113)	Data  0.001 ( 0.009)	Loss 4.0245e-03 (2.1458e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [69][ 40/391]	Time  0.106 ( 0.112)	Data  0.001 ( 0.008)	Loss 4.9927e-02 (2.1957e-02)	Acc@1  97.66 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [69][ 50/391]	Time  0.106 ( 0.111)	Data  0.001 ( 0.007)	Loss 2.1011e-02 (2.1649e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [69][ 60/391]	Time  0.104 ( 0.110)	Data  0.001 ( 0.007)	Loss 1.8250e-02 (2.0723e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [69][ 70/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 2.2171e-02 (2.0974e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [69][ 80/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.006)	Loss 4.8645e-02 (2.2034e-02)	Acc@1  97.66 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [69][ 90/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.006)	Loss 2.5192e-02 (2.2128e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [69][100/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.006)	Loss 3.2227e-02 (2.2748e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 ( 99.99)
Epoch: [69][110/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.006)	Loss 3.7750e-02 (2.3213e-02)	Acc@1  98.44 ( 99.23)	Acc@5 100.00 ( 99.99)
Epoch: [69][120/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.9433e-03 (2.2846e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 ( 99.99)
Epoch: [69][130/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.8228e-02 (2.3017e-02)	Acc@1  98.44 ( 99.28)	Acc@5 100.00 ( 99.99)
Epoch: [69][140/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3527e-02 (2.3151e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 ( 99.99)
Epoch: [69][150/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.4658e-02 (2.3574e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 ( 99.99)
Epoch: [69][160/391]	Time  0.111 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3306e-02 (2.4248e-02)	Acc@1  99.22 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [69][170/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.3539e-02 (2.4995e-02)	Acc@1  98.44 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [69][180/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.0950e-02 (2.4925e-02)	Acc@1  99.22 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [69][190/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.5192e-02 (2.5332e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 ( 99.99)
Epoch: [69][200/391]	Time  0.106 ( 0.108)	Data  0.001 ( 0.005)	Loss 4.0802e-02 (2.5266e-02)	Acc@1  97.66 ( 99.20)	Acc@5 100.00 ( 99.99)
Epoch: [69][210/391]	Time  0.104 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.1711e-02 (2.5577e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 ( 99.99)
Epoch: [69][220/391]	Time  0.107 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.6724e-02 (2.5655e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 ( 99.99)
Epoch: [69][230/391]	Time  0.105 ( 0.108)	Data  0.001 ( 0.005)	Loss 5.5267e-02 (2.5810e-02)	Acc@1  97.66 ( 99.16)	Acc@5 100.00 ( 99.99)
Epoch: [69][240/391]	Time  0.108 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.8372e-02 (2.5757e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 ( 99.99)
Epoch: [69][250/391]	Time  0.103 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.6846e-02 (2.6110e-02)	Acc@1  99.22 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [69][260/391]	Time  0.110 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.3855e-02 (2.6035e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 ( 99.99)
Epoch: [69][270/391]	Time  0.105 ( 0.108)	Data  0.002 ( 0.005)	Loss 1.3618e-02 (2.6223e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 ( 99.99)
Epoch: [69][280/391]	Time  0.106 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.3023e-02 (2.6446e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [69][290/391]	Time  0.108 ( 0.108)	Data  0.001 ( 0.005)	Loss 7.4959e-03 (2.6516e-02)	Acc@1 100.00 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [69][300/391]	Time  0.106 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.1826e-02 (2.6391e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [69][310/391]	Time  0.108 ( 0.108)	Data  0.001 ( 0.005)	Loss 3.5065e-02 (2.6409e-02)	Acc@1  98.44 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [69][320/391]	Time  0.105 ( 0.108)	Data  0.001 ( 0.005)	Loss 3.6133e-02 (2.6172e-02)	Acc@1  98.44 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [69][330/391]	Time  0.106 ( 0.108)	Data  0.001 ( 0.005)	Loss 2.0477e-02 (2.6093e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [69][340/391]	Time  0.103 ( 0.108)	Data  0.001 ( 0.005)	Loss 8.2932e-03 (2.5946e-02)	Acc@1 100.00 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [69][350/391]	Time  0.108 ( 0.108)	Data  0.001 ( 0.005)	Loss 8.2550e-03 (2.5869e-02)	Acc@1 100.00 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [69][360/391]	Time  0.105 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.9821e-02 (2.5724e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [69][370/391]	Time  0.105 ( 0.108)	Data  0.001 ( 0.005)	Loss 3.4393e-02 (2.5660e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [69][380/391]	Time  0.108 ( 0.108)	Data  0.001 ( 0.005)	Loss 2.4139e-02 (2.5544e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [69][390/391]	Time  0.102 ( 0.108)	Data  0.001 ( 0.005)	Loss 2.9663e-02 (2.5562e-02)	Acc@1  98.75 ( 99.17)	Acc@5 100.00 (100.00)
## e[69] optimizer.zero_grad (sum) time: 0.35256505012512207
## e[69]       loss.backward (sum) time: 13.732395887374878
## e[69]      optimizer.step (sum) time: 2.907752752304077
## epoch[69] training(only) time: 42.52712059020996
# Switched to evaluate mode...
Test: [  0/100]	Time  0.192 ( 0.192)	Loss 3.4839e-01 (3.4839e-01)	Acc@1  87.00 ( 87.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.037 ( 0.051)	Loss 2.5195e-01 (3.8804e-01)	Acc@1  94.00 ( 91.18)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.035 ( 0.044)	Loss 5.2686e-01 (4.1480e-01)	Acc@1  88.00 ( 91.00)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.039 ( 0.042)	Loss 4.0137e-01 (4.3727e-01)	Acc@1  92.00 ( 91.19)	Acc@5  99.00 ( 99.52)
Test: [ 40/100]	Time  0.039 ( 0.041)	Loss 3.2788e-01 (4.3266e-01)	Acc@1  91.00 ( 91.12)	Acc@5  99.00 ( 99.54)
Test: [ 50/100]	Time  0.037 ( 0.040)	Loss 3.1592e-01 (4.2632e-01)	Acc@1  91.00 ( 91.12)	Acc@5 100.00 ( 99.51)
Test: [ 60/100]	Time  0.039 ( 0.039)	Loss 5.8252e-01 (4.2021e-01)	Acc@1  93.00 ( 91.30)	Acc@5  99.00 ( 99.56)
Test: [ 70/100]	Time  0.037 ( 0.039)	Loss 6.5820e-01 (4.1535e-01)	Acc@1  88.00 ( 91.30)	Acc@5  99.00 ( 99.56)
Test: [ 80/100]	Time  0.036 ( 0.039)	Loss 2.8027e-01 (4.1396e-01)	Acc@1  92.00 ( 91.33)	Acc@5 100.00 ( 99.57)
Test: [ 90/100]	Time  0.036 ( 0.039)	Loss 2.9126e-01 (4.0732e-01)	Acc@1  94.00 ( 91.30)	Acc@5 100.00 ( 99.57)
 * Acc@1 91.300 Acc@5 99.600
### epoch[69] execution time: 46.47778511047363
EPOCH 70
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [70][  0/391]	Time  0.269 ( 0.269)	Data  0.173 ( 0.173)	Loss 2.0523e-02 (2.0523e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [70][ 10/391]	Time  0.106 ( 0.122)	Data  0.001 ( 0.019)	Loss 2.8351e-02 (2.6489e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [70][ 20/391]	Time  0.105 ( 0.115)	Data  0.001 ( 0.012)	Loss 1.1429e-02 (2.7846e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [70][ 30/391]	Time  0.109 ( 0.113)	Data  0.001 ( 0.010)	Loss 4.3457e-02 (2.8060e-02)	Acc@1  98.44 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [70][ 40/391]	Time  0.112 ( 0.112)	Data  0.001 ( 0.008)	Loss 3.1464e-02 (2.7101e-02)	Acc@1  99.22 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [70][ 50/391]	Time  0.102 ( 0.111)	Data  0.001 ( 0.007)	Loss 6.8359e-02 (2.6740e-02)	Acc@1  98.44 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [70][ 60/391]	Time  0.107 ( 0.111)	Data  0.002 ( 0.007)	Loss 5.2185e-02 (2.7406e-02)	Acc@1  99.22 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [70][ 70/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 3.9246e-02 (2.7972e-02)	Acc@1  98.44 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [70][ 80/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.006)	Loss 6.3782e-03 (2.7366e-02)	Acc@1 100.00 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [70][ 90/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.006)	Loss 3.0289e-02 (2.6688e-02)	Acc@1  99.22 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [70][100/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.006)	Loss 7.2746e-03 (2.7128e-02)	Acc@1 100.00 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [70][110/391]	Time  0.109 ( 0.110)	Data  0.001 ( 0.006)	Loss 4.4434e-02 (2.7160e-02)	Acc@1  97.66 ( 99.12)	Acc@5 100.00 ( 99.99)
Epoch: [70][120/391]	Time  0.110 ( 0.110)	Data  0.001 ( 0.005)	Loss 7.0343e-03 (2.7338e-02)	Acc@1 100.00 ( 99.12)	Acc@5 100.00 ( 99.99)
Epoch: [70][130/391]	Time  0.109 ( 0.110)	Data  0.001 ( 0.005)	Loss 1.6739e-02 (2.6823e-02)	Acc@1 100.00 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [70][140/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.8814e-02 (2.7113e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [70][150/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.1787e-03 (2.6774e-02)	Acc@1 100.00 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [70][160/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.3087e-02 (2.6419e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [70][170/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2222e-02 (2.6368e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [70][180/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.5267e-02 (2.5986e-02)	Acc@1  98.44 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [70][190/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1101e-02 (2.5485e-02)	Acc@1 100.00 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [70][200/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2405e-02 (2.5694e-02)	Acc@1 100.00 ( 99.21)	Acc@5 100.00 ( 99.99)
Epoch: [70][210/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3824e-02 (2.5594e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 ( 99.99)
Epoch: [70][220/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.1779e-02 (2.5740e-02)	Acc@1  98.44 ( 99.21)	Acc@5 100.00 ( 99.99)
Epoch: [70][230/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4145e-02 (2.5860e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 ( 99.99)
Epoch: [70][240/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.7710e-02 (2.5912e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 ( 99.99)
Epoch: [70][250/391]	Time  0.106 ( 0.109)	Data  0.002 ( 0.005)	Loss 1.3283e-02 (2.5789e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 ( 99.99)
Epoch: [70][260/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3855e-02 (2.5527e-02)	Acc@1 100.00 ( 99.21)	Acc@5 100.00 ( 99.99)
Epoch: [70][270/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4725e-02 (2.5293e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 ( 99.99)
Epoch: [70][280/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.3293e-02 (2.5618e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 ( 99.99)
Epoch: [70][290/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.7749e-03 (2.5343e-02)	Acc@1 100.00 ( 99.23)	Acc@5 100.00 ( 99.99)
Epoch: [70][300/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.8091e-02 (2.5581e-02)	Acc@1  99.22 ( 99.21)	Acc@5 100.00 ( 99.99)
Epoch: [70][310/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.6169e-02 (2.5600e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 ( 99.99)
Epoch: [70][320/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1620e-02 (2.5518e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [70][330/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.5131e-02 (2.5400e-02)	Acc@1  98.44 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [70][340/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.8501e-03 (2.5412e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [70][350/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.1229e-02 (2.5498e-02)	Acc@1  97.66 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [70][360/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.7242e-02 (2.5419e-02)	Acc@1  99.22 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [70][370/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.0561e-03 (2.5383e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [70][380/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0513e-02 (2.5304e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [70][390/391]	Time  0.099 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.9976e-02 (2.5489e-02)	Acc@1  97.50 ( 99.20)	Acc@5 100.00 (100.00)
## e[70] optimizer.zero_grad (sum) time: 0.3525111675262451
## e[70]       loss.backward (sum) time: 13.754678010940552
## e[70]      optimizer.step (sum) time: 2.8851280212402344
## epoch[70] training(only) time: 42.66400122642517
# Switched to evaluate mode...
Test: [  0/100]	Time  0.235 ( 0.235)	Loss 3.5913e-01 (3.5913e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.036 ( 0.055)	Loss 2.6025e-01 (3.9347e-01)	Acc@1  94.00 ( 91.00)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.037 ( 0.046)	Loss 5.1123e-01 (4.1632e-01)	Acc@1  89.00 ( 91.10)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.038 ( 0.044)	Loss 4.2310e-01 (4.3682e-01)	Acc@1  90.00 ( 91.13)	Acc@5  99.00 ( 99.45)
Test: [ 40/100]	Time  0.037 ( 0.042)	Loss 2.8711e-01 (4.3128e-01)	Acc@1  93.00 ( 91.12)	Acc@5  99.00 ( 99.49)
Test: [ 50/100]	Time  0.039 ( 0.041)	Loss 2.9517e-01 (4.2320e-01)	Acc@1  92.00 ( 91.06)	Acc@5 100.00 ( 99.55)
Test: [ 60/100]	Time  0.037 ( 0.041)	Loss 5.6396e-01 (4.1728e-01)	Acc@1  93.00 ( 91.25)	Acc@5  99.00 ( 99.59)
Test: [ 70/100]	Time  0.036 ( 0.040)	Loss 6.2256e-01 (4.1077e-01)	Acc@1  88.00 ( 91.34)	Acc@5  99.00 ( 99.62)
Test: [ 80/100]	Time  0.039 ( 0.040)	Loss 2.8955e-01 (4.0979e-01)	Acc@1  92.00 ( 91.40)	Acc@5 100.00 ( 99.60)
Test: [ 90/100]	Time  0.036 ( 0.039)	Loss 3.1543e-01 (4.0248e-01)	Acc@1  94.00 ( 91.42)	Acc@5 100.00 ( 99.59)
 * Acc@1 91.420 Acc@5 99.610
### epoch[70] execution time: 46.66717314720154
EPOCH 71
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [71][  0/391]	Time  0.292 ( 0.292)	Data  0.193 ( 0.193)	Loss 8.0109e-03 (8.0109e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [71][ 10/391]	Time  0.108 ( 0.124)	Data  0.001 ( 0.021)	Loss 2.6474e-02 (2.1127e-02)	Acc@1  98.44 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [71][ 20/391]	Time  0.107 ( 0.116)	Data  0.001 ( 0.013)	Loss 3.4424e-02 (2.1383e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [71][ 30/391]	Time  0.106 ( 0.113)	Data  0.001 ( 0.010)	Loss 1.3969e-02 (2.1761e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [71][ 40/391]	Time  0.107 ( 0.112)	Data  0.001 ( 0.009)	Loss 9.9792e-03 (2.2423e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [71][ 50/391]	Time  0.107 ( 0.111)	Data  0.001 ( 0.008)	Loss 1.2604e-02 (2.2435e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [71][ 60/391]	Time  0.105 ( 0.111)	Data  0.001 ( 0.007)	Loss 2.2110e-02 (2.3203e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [71][ 70/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.007)	Loss 2.2156e-02 (2.3481e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [71][ 80/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 3.2013e-02 (2.3556e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [71][ 90/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.006)	Loss 2.9297e-02 (2.3596e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [71][100/391]	Time  0.109 ( 0.110)	Data  0.001 ( 0.006)	Loss 4.9347e-02 (2.3789e-02)	Acc@1  98.44 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [71][110/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 5.9875e-02 (2.5511e-02)	Acc@1  97.66 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [71][120/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.006)	Loss 7.0435e-02 (2.6352e-02)	Acc@1  97.66 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [71][130/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.006)	Loss 1.4931e-02 (2.6290e-02)	Acc@1  99.22 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [71][140/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.006)	Loss 1.6357e-02 (2.5736e-02)	Acc@1  99.22 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [71][150/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.1412e-02 (2.6198e-02)	Acc@1  99.22 ( 99.21)	Acc@5 100.00 ( 99.99)
Epoch: [71][160/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.3671e-02 (2.5951e-02)	Acc@1  98.44 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [71][170/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.7914e-02 (2.6322e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [71][180/391]	Time  0.111 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.2410e-02 (2.6493e-02)	Acc@1  97.66 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [71][190/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.8564e-02 (2.6214e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [71][200/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4847e-02 (2.5771e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [71][210/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1139e-02 (2.5581e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [71][220/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1353e-02 (2.5616e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [71][230/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.8258e-02 (2.5715e-02)	Acc@1  97.66 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [71][240/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.6846e-02 (2.5711e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [71][250/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.6835e-02 (2.5847e-02)	Acc@1  98.44 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [71][260/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5854e-02 (2.5740e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [71][270/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.3422e-02 (2.5688e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [71][280/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.0680e-02 (2.5695e-02)	Acc@1  98.44 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [71][290/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5839e-02 (2.5679e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [71][300/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3664e-02 (2.5898e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [71][310/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.3804e-02 (2.5954e-02)	Acc@1  98.44 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [71][320/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.8177e-02 (2.6296e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [71][330/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.2827e-02 (2.6232e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [71][340/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.7975e-02 (2.5902e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [71][350/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.0027e-03 (2.5920e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [71][360/391]	Time  0.105 ( 0.109)	Data  0.002 ( 0.005)	Loss 4.0161e-02 (2.6180e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [71][370/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.3880e-02 (2.5960e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [71][380/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.1005e-02 (2.6041e-02)	Acc@1  97.66 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [71][390/391]	Time  0.099 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.6697e-03 (2.5976e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
## e[71] optimizer.zero_grad (sum) time: 0.34821629524230957
## e[71]       loss.backward (sum) time: 13.661661386489868
## e[71]      optimizer.step (sum) time: 2.7711212635040283
## epoch[71] training(only) time: 42.614800691604614
# Switched to evaluate mode...
Test: [  0/100]	Time  0.229 ( 0.229)	Loss 3.3984e-01 (3.3984e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.038 ( 0.055)	Loss 2.4426e-01 (3.8495e-01)	Acc@1  94.00 ( 91.36)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.036 ( 0.046)	Loss 5.3125e-01 (4.1254e-01)	Acc@1  90.00 ( 91.29)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.037 ( 0.043)	Loss 4.0845e-01 (4.3278e-01)	Acc@1  91.00 ( 91.26)	Acc@5  99.00 ( 99.48)
Test: [ 40/100]	Time  0.036 ( 0.041)	Loss 3.1201e-01 (4.2920e-01)	Acc@1  92.00 ( 91.20)	Acc@5  99.00 ( 99.51)
Test: [ 50/100]	Time  0.036 ( 0.041)	Loss 2.8345e-01 (4.2148e-01)	Acc@1  92.00 ( 91.24)	Acc@5 100.00 ( 99.55)
Test: [ 60/100]	Time  0.036 ( 0.040)	Loss 5.5322e-01 (4.1430e-01)	Acc@1  93.00 ( 91.43)	Acc@5  99.00 ( 99.57)
Test: [ 70/100]	Time  0.036 ( 0.040)	Loss 6.3232e-01 (4.0773e-01)	Acc@1  89.00 ( 91.41)	Acc@5  99.00 ( 99.61)
Test: [ 80/100]	Time  0.038 ( 0.039)	Loss 2.7539e-01 (4.0700e-01)	Acc@1  92.00 ( 91.46)	Acc@5 100.00 ( 99.60)
Test: [ 90/100]	Time  0.036 ( 0.039)	Loss 3.1030e-01 (4.0019e-01)	Acc@1  94.00 ( 91.43)	Acc@5 100.00 ( 99.60)
 * Acc@1 91.440 Acc@5 99.620
### epoch[71] execution time: 46.615761518478394
EPOCH 72
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [72][  0/391]	Time  0.261 ( 0.261)	Data  0.161 ( 0.161)	Loss 1.6647e-02 (1.6647e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [72][ 10/391]	Time  0.107 ( 0.121)	Data  0.001 ( 0.018)	Loss 1.4389e-02 (2.2795e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [72][ 20/391]	Time  0.106 ( 0.115)	Data  0.001 ( 0.012)	Loss 6.9542e-03 (2.2613e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [72][ 30/391]	Time  0.105 ( 0.113)	Data  0.001 ( 0.009)	Loss 3.9764e-02 (2.5076e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [72][ 40/391]	Time  0.109 ( 0.112)	Data  0.001 ( 0.008)	Loss 1.5945e-02 (2.6967e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [72][ 50/391]	Time  0.097 ( 0.111)	Data  0.001 ( 0.007)	Loss 4.5532e-02 (2.6111e-02)	Acc@1  98.44 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [72][ 60/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.007)	Loss 5.2567e-03 (2.6097e-02)	Acc@1 100.00 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [72][ 70/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.1676e-01 (2.7605e-02)	Acc@1  95.31 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [72][ 80/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 3.4760e-02 (2.7124e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [72][ 90/391]	Time  0.113 ( 0.110)	Data  0.001 ( 0.006)	Loss 6.7825e-03 (2.6538e-02)	Acc@1 100.00 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [72][100/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 8.4686e-03 (2.5599e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [72][110/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.006)	Loss 7.3891e-03 (2.5369e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [72][120/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.006)	Loss 1.6266e-02 (2.5915e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [72][130/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.9053e-02 (2.5607e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [72][140/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.3758e-02 (2.5188e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [72][150/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.9404e-02 (2.4792e-02)	Acc@1  99.22 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [72][160/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.3600e-02 (2.4282e-02)	Acc@1  98.44 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [72][170/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.5227e-02 (2.4595e-02)	Acc@1  97.66 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [72][180/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.9989e-02 (2.4581e-02)	Acc@1 100.00 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [72][190/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.0624e-02 (2.5072e-02)	Acc@1  99.22 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [72][200/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.9215e-02 (2.5463e-02)	Acc@1  97.66 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [72][210/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.6987e-02 (2.6048e-02)	Acc@1  98.44 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [72][220/391]	Time  0.111 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.3844e-02 (2.6257e-02)	Acc@1  98.44 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [72][230/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.6321e-02 (2.6170e-02)	Acc@1  99.22 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [72][240/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4732e-02 (2.5933e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [72][250/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.2415e-02 (2.5799e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [72][260/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.4246e-02 (2.5698e-02)	Acc@1  98.44 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [72][270/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.1128e-02 (2.5659e-02)	Acc@1  98.44 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [72][280/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.7139e-02 (2.5613e-02)	Acc@1  97.66 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [72][290/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.9806e-02 (2.5657e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [72][300/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.8860e-02 (2.5674e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [72][310/391]	Time  0.106 ( 0.108)	Data  0.001 ( 0.005)	Loss 2.1240e-02 (2.5805e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [72][320/391]	Time  0.105 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.1269e-02 (2.5620e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [72][330/391]	Time  0.105 ( 0.108)	Data  0.001 ( 0.005)	Loss 3.6255e-02 (2.5399e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [72][340/391]	Time  0.105 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.1421e-02 (2.5211e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [72][350/391]	Time  0.106 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.8707e-02 (2.5038e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [72][360/391]	Time  0.103 ( 0.108)	Data  0.001 ( 0.005)	Loss 5.5603e-02 (2.4985e-02)	Acc@1  98.44 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [72][370/391]	Time  0.106 ( 0.108)	Data  0.001 ( 0.005)	Loss 4.0161e-02 (2.4868e-02)	Acc@1  99.22 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [72][380/391]	Time  0.107 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.3786e-02 (2.4620e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [72][390/391]	Time  0.099 ( 0.108)	Data  0.001 ( 0.005)	Loss 9.6375e-02 (2.4589e-02)	Acc@1  95.00 ( 99.22)	Acc@5 100.00 (100.00)
## e[72] optimizer.zero_grad (sum) time: 0.3498547077178955
## e[72]       loss.backward (sum) time: 13.733333587646484
## e[72]      optimizer.step (sum) time: 2.7706191539764404
## epoch[72] training(only) time: 42.54293203353882
# Switched to evaluate mode...
Test: [  0/100]	Time  0.243 ( 0.243)	Loss 3.3472e-01 (3.3472e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.035 ( 0.056)	Loss 2.4915e-01 (3.8190e-01)	Acc@1  94.00 ( 91.00)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.036 ( 0.047)	Loss 5.3418e-01 (4.1233e-01)	Acc@1  90.00 ( 90.95)	Acc@5 100.00 ( 99.52)
Test: [ 30/100]	Time  0.035 ( 0.043)	Loss 4.1797e-01 (4.3183e-01)	Acc@1  91.00 ( 91.10)	Acc@5  99.00 ( 99.45)
Test: [ 40/100]	Time  0.035 ( 0.042)	Loss 3.0469e-01 (4.2703e-01)	Acc@1  91.00 ( 91.05)	Acc@5  99.00 ( 99.46)
Test: [ 50/100]	Time  0.037 ( 0.041)	Loss 2.9907e-01 (4.1859e-01)	Acc@1  92.00 ( 91.08)	Acc@5 100.00 ( 99.53)
Test: [ 60/100]	Time  0.037 ( 0.040)	Loss 5.4639e-01 (4.1194e-01)	Acc@1  93.00 ( 91.30)	Acc@5  99.00 ( 99.57)
Test: [ 70/100]	Time  0.036 ( 0.040)	Loss 6.2695e-01 (4.0479e-01)	Acc@1  88.00 ( 91.34)	Acc@5  99.00 ( 99.59)
Test: [ 80/100]	Time  0.036 ( 0.040)	Loss 2.9761e-01 (4.0376e-01)	Acc@1  91.00 ( 91.38)	Acc@5 100.00 ( 99.60)
Test: [ 90/100]	Time  0.039 ( 0.039)	Loss 3.0859e-01 (3.9678e-01)	Acc@1  94.00 ( 91.38)	Acc@5 100.00 ( 99.60)
 * Acc@1 91.380 Acc@5 99.620
### epoch[72] execution time: 46.590104818344116
EPOCH 73
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [73][  0/391]	Time  0.303 ( 0.303)	Data  0.205 ( 0.205)	Loss 5.6366e-02 (5.6366e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [73][ 10/391]	Time  0.101 ( 0.125)	Data  0.001 ( 0.022)	Loss 3.8666e-02 (2.7437e-02)	Acc@1  98.44 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [73][ 20/391]	Time  0.104 ( 0.116)	Data  0.001 ( 0.014)	Loss 7.4646e-02 (2.9792e-02)	Acc@1  97.66 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [73][ 30/391]	Time  0.106 ( 0.114)	Data  0.001 ( 0.010)	Loss 6.8970e-03 (3.0216e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [73][ 40/391]	Time  0.107 ( 0.112)	Data  0.001 ( 0.009)	Loss 1.4122e-02 (2.9984e-02)	Acc@1 100.00 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [73][ 50/391]	Time  0.106 ( 0.112)	Data  0.001 ( 0.008)	Loss 2.0721e-02 (3.1215e-02)	Acc@1 100.00 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [73][ 60/391]	Time  0.106 ( 0.111)	Data  0.001 ( 0.007)	Loss 2.3941e-02 (3.0168e-02)	Acc@1  99.22 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [73][ 70/391]	Time  0.110 ( 0.110)	Data  0.001 ( 0.007)	Loss 1.0399e-02 (2.8638e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [73][ 80/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.007)	Loss 3.2349e-02 (2.8050e-02)	Acc@1  98.44 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [73][ 90/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.1139e-02 (2.7091e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [73][100/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.006)	Loss 2.2934e-02 (2.6476e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [73][110/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.006)	Loss 2.0386e-02 (2.6157e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [73][120/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.006)	Loss 1.8753e-02 (2.5717e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [73][130/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.006)	Loss 1.6327e-02 (2.4768e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [73][140/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.006)	Loss 3.5706e-02 (2.4595e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [73][150/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.6861e-02 (2.4599e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [73][160/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.1240e-02 (2.4747e-02)	Acc@1  98.44 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [73][170/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.9592e-02 (2.4569e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [73][180/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.7807e-02 (2.4659e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [73][190/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.7197e-02 (2.4763e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [73][200/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.1705e-03 (2.4878e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [73][210/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.2812e-02 (2.5251e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [73][220/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.2446e-02 (2.5125e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [73][230/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.6835e-02 (2.5134e-02)	Acc@1  98.44 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [73][240/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.5253e-02 (2.4969e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [73][250/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.1525e-02 (2.4656e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [73][260/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1635e-02 (2.4640e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [73][270/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.7212e-02 (2.4678e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [73][280/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.3117e-02 (2.4589e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [73][290/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.2675e-02 (2.4617e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [73][300/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2589e-02 (2.4410e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [73][310/391]	Time  0.105 ( 0.109)	Data  0.002 ( 0.005)	Loss 1.8600e-02 (2.4166e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [73][320/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.9455e-02 (2.4151e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [73][330/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.3956e-02 (2.4289e-02)	Acc@1  98.44 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [73][340/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.4094e-02 (2.4344e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [73][350/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.8239e-02 (2.4363e-02)	Acc@1  98.44 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [73][360/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.2975e-03 (2.4200e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [73][370/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2260e-02 (2.4270e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [73][380/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.4567e-02 (2.4374e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [73][390/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1940e-02 (2.4250e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
## e[73] optimizer.zero_grad (sum) time: 0.3467538356781006
## e[73]       loss.backward (sum) time: 13.711368799209595
## e[73]      optimizer.step (sum) time: 2.7476954460144043
## epoch[73] training(only) time: 42.62159776687622
# Switched to evaluate mode...
Test: [  0/100]	Time  0.243 ( 0.243)	Loss 3.5913e-01 (3.5913e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.038 ( 0.055)	Loss 2.7905e-01 (3.8882e-01)	Acc@1  93.00 ( 91.09)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.036 ( 0.047)	Loss 4.9854e-01 (4.1268e-01)	Acc@1  89.00 ( 91.19)	Acc@5 100.00 ( 99.52)
Test: [ 30/100]	Time  0.035 ( 0.044)	Loss 3.9282e-01 (4.3754e-01)	Acc@1  92.00 ( 91.35)	Acc@5  99.00 ( 99.45)
Test: [ 40/100]	Time  0.037 ( 0.042)	Loss 2.9712e-01 (4.3042e-01)	Acc@1  92.00 ( 91.22)	Acc@5  99.00 ( 99.49)
Test: [ 50/100]	Time  0.035 ( 0.041)	Loss 2.9468e-01 (4.2374e-01)	Acc@1  92.00 ( 91.20)	Acc@5 100.00 ( 99.49)
Test: [ 60/100]	Time  0.037 ( 0.040)	Loss 5.6836e-01 (4.1666e-01)	Acc@1  93.00 ( 91.31)	Acc@5  99.00 ( 99.52)
Test: [ 70/100]	Time  0.036 ( 0.040)	Loss 6.9824e-01 (4.1209e-01)	Acc@1  88.00 ( 91.28)	Acc@5  99.00 ( 99.55)
Test: [ 80/100]	Time  0.035 ( 0.040)	Loss 2.8394e-01 (4.1191e-01)	Acc@1  92.00 ( 91.31)	Acc@5 100.00 ( 99.56)
Test: [ 90/100]	Time  0.038 ( 0.039)	Loss 3.0957e-01 (4.0501e-01)	Acc@1  94.00 ( 91.35)	Acc@5 100.00 ( 99.56)
 * Acc@1 91.370 Acc@5 99.590
### epoch[73] execution time: 46.624335289001465
EPOCH 74
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [74][  0/391]	Time  0.317 ( 0.317)	Data  0.220 ( 0.220)	Loss 2.1652e-02 (2.1652e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [74][ 10/391]	Time  0.106 ( 0.127)	Data  0.001 ( 0.024)	Loss 4.6875e-02 (2.6775e-02)	Acc@1  97.66 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [74][ 20/391]	Time  0.104 ( 0.118)	Data  0.001 ( 0.014)	Loss 2.2858e-02 (2.6443e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [74][ 30/391]	Time  0.099 ( 0.115)	Data  0.001 ( 0.011)	Loss 2.6627e-02 (2.4130e-02)	Acc@1  98.44 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [74][ 40/391]	Time  0.108 ( 0.113)	Data  0.001 ( 0.009)	Loss 1.4450e-02 (2.5103e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [74][ 50/391]	Time  0.106 ( 0.112)	Data  0.001 ( 0.008)	Loss 7.2899e-03 (2.4899e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [74][ 60/391]	Time  0.110 ( 0.112)	Data  0.001 ( 0.008)	Loss 8.6517e-03 (2.5527e-02)	Acc@1 100.00 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [74][ 70/391]	Time  0.108 ( 0.111)	Data  0.001 ( 0.007)	Loss 2.3636e-02 (2.5050e-02)	Acc@1  99.22 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [74][ 80/391]	Time  0.106 ( 0.111)	Data  0.001 ( 0.007)	Loss 3.5797e-02 (2.4495e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [74][ 90/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.007)	Loss 1.6846e-02 (2.5080e-02)	Acc@1 100.00 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [74][100/391]	Time  0.111 ( 0.110)	Data  0.001 ( 0.006)	Loss 2.6550e-03 (2.4919e-02)	Acc@1 100.00 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [74][110/391]	Time  0.110 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.2177e-02 (2.4857e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [74][120/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.006)	Loss 3.1799e-02 (2.5221e-02)	Acc@1  98.44 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [74][130/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.006)	Loss 5.2582e-02 (2.5412e-02)	Acc@1  96.88 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [74][140/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.006)	Loss 3.6469e-02 (2.5920e-02)	Acc@1  98.44 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [74][150/391]	Time  0.110 ( 0.110)	Data  0.001 ( 0.006)	Loss 3.2074e-02 (2.5630e-02)	Acc@1  98.44 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [74][160/391]	Time  0.104 ( 0.110)	Data  0.001 ( 0.005)	Loss 1.9028e-02 (2.5880e-02)	Acc@1  99.22 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [74][170/391]	Time  0.109 ( 0.110)	Data  0.001 ( 0.005)	Loss 1.7319e-02 (2.5475e-02)	Acc@1 100.00 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [74][180/391]	Time  0.110 ( 0.110)	Data  0.001 ( 0.005)	Loss 1.5572e-02 (2.5396e-02)	Acc@1  99.22 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [74][190/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3924e-02 (2.5607e-02)	Acc@1 100.00 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [74][200/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.3422e-02 (2.5312e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [74][210/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3687e-02 (2.4972e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [74][220/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1673e-02 (2.4760e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [74][230/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0521e-02 (2.4783e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [74][240/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.6968e-02 (2.4654e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [74][250/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.2501e-02 (2.4617e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [74][260/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1185e-02 (2.4514e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [74][270/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.5314e-02 (2.4268e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [74][280/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.0103e-03 (2.4305e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [74][290/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0941e-02 (2.4321e-02)	Acc@1 100.00 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [74][300/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.1744e-02 (2.4409e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [74][310/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.3026e-02 (2.4456e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [74][320/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.3694e-03 (2.4113e-02)	Acc@1 100.00 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [74][330/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.9945e-03 (2.4215e-02)	Acc@1 100.00 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [74][340/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.1576e-02 (2.4401e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [74][350/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1780e-02 (2.4407e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [74][360/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4664e-02 (2.4392e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [74][370/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.3844e-02 (2.4596e-02)	Acc@1  98.44 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [74][380/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.8976e-02 (2.4426e-02)	Acc@1 100.00 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [74][390/391]	Time  0.099 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1360e-02 (2.4354e-02)	Acc@1 100.00 ( 99.21)	Acc@5 100.00 (100.00)
## e[74] optimizer.zero_grad (sum) time: 0.34795618057250977
## e[74]       loss.backward (sum) time: 13.704884767532349
## e[74]      optimizer.step (sum) time: 2.7945196628570557
## epoch[74] training(only) time: 42.74041795730591
# Switched to evaluate mode...
Test: [  0/100]	Time  0.239 ( 0.239)	Loss 3.8208e-01 (3.8208e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.036 ( 0.056)	Loss 2.5244e-01 (3.9508e-01)	Acc@1  94.00 ( 91.00)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.039 ( 0.047)	Loss 5.2686e-01 (4.2207e-01)	Acc@1  89.00 ( 91.14)	Acc@5 100.00 ( 99.52)
Test: [ 30/100]	Time  0.037 ( 0.044)	Loss 4.2944e-01 (4.4227e-01)	Acc@1  91.00 ( 91.29)	Acc@5  99.00 ( 99.45)
Test: [ 40/100]	Time  0.036 ( 0.042)	Loss 2.9980e-01 (4.3646e-01)	Acc@1  91.00 ( 91.12)	Acc@5  99.00 ( 99.49)
Test: [ 50/100]	Time  0.036 ( 0.041)	Loss 3.1128e-01 (4.2784e-01)	Acc@1  92.00 ( 91.12)	Acc@5 100.00 ( 99.51)
Test: [ 60/100]	Time  0.036 ( 0.040)	Loss 5.8740e-01 (4.2113e-01)	Acc@1  93.00 ( 91.34)	Acc@5  98.00 ( 99.52)
Test: [ 70/100]	Time  0.036 ( 0.040)	Loss 6.3721e-01 (4.1496e-01)	Acc@1  88.00 ( 91.35)	Acc@5  99.00 ( 99.55)
Test: [ 80/100]	Time  0.037 ( 0.040)	Loss 2.9614e-01 (4.1285e-01)	Acc@1  91.00 ( 91.43)	Acc@5 100.00 ( 99.56)
Test: [ 90/100]	Time  0.038 ( 0.039)	Loss 3.2959e-01 (4.0579e-01)	Acc@1  94.00 ( 91.46)	Acc@5 100.00 ( 99.55)
 * Acc@1 91.430 Acc@5 99.580
### epoch[74] execution time: 46.754807472229004
EPOCH 75
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [75][  0/391]	Time  0.267 ( 0.267)	Data  0.162 ( 0.162)	Loss 3.9368e-02 (3.9368e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [75][ 10/391]	Time  0.106 ( 0.122)	Data  0.001 ( 0.018)	Loss 4.4098e-02 (3.0592e-02)	Acc@1  98.44 ( 98.65)	Acc@5 100.00 (100.00)
Epoch: [75][ 20/391]	Time  0.104 ( 0.115)	Data  0.001 ( 0.012)	Loss 3.3741e-03 (2.4223e-02)	Acc@1 100.00 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [75][ 30/391]	Time  0.106 ( 0.113)	Data  0.001 ( 0.009)	Loss 6.4087e-02 (2.6982e-02)	Acc@1  97.66 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [75][ 40/391]	Time  0.103 ( 0.112)	Data  0.001 ( 0.008)	Loss 5.2338e-02 (2.4767e-02)	Acc@1  98.44 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [75][ 50/391]	Time  0.104 ( 0.111)	Data  0.001 ( 0.007)	Loss 2.6199e-02 (2.4704e-02)	Acc@1  98.44 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [75][ 60/391]	Time  0.104 ( 0.111)	Data  0.001 ( 0.007)	Loss 4.4746e-03 (2.3726e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [75][ 70/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.1528e-02 (2.2753e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [75][ 80/391]	Time  0.112 ( 0.110)	Data  0.001 ( 0.006)	Loss 2.1683e-02 (2.2783e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [75][ 90/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.006)	Loss 7.0190e-03 (2.2169e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [75][100/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 3.7415e-02 (2.2559e-02)	Acc@1  98.44 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [75][110/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.006)	Loss 1.4084e-02 (2.2844e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [75][120/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.2934e-02 (2.2593e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [75][130/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.7639e-02 (2.2149e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [75][140/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.0203e-02 (2.1949e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [75][150/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.7863e-02 (2.1950e-02)	Acc@1  98.44 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [75][160/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.4027e-02 (2.1894e-02)	Acc@1  98.44 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [75][170/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.3682e-02 (2.2052e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [75][180/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2009e-02 (2.1842e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [75][190/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1848e-02 (2.2113e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [75][200/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.2598e-02 (2.1822e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [75][210/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4015e-02 (2.1821e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [75][220/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.9556e-02 (2.2511e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [75][230/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1932e-02 (2.2620e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [75][240/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.9800e-02 (2.2661e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [75][250/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.9642e-02 (2.2604e-02)	Acc@1  98.44 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [75][260/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.0218e-02 (2.2617e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [75][270/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.5665e-02 (2.2470e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [75][280/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.6693e-02 (2.2683e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [75][290/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.6523e-03 (2.2878e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [75][300/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.9419e-02 (2.2897e-02)	Acc@1  98.44 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [75][310/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.7166e-02 (2.2747e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [75][320/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.5833e-02 (2.2929e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [75][330/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2177e-02 (2.2965e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [75][340/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.0771e-02 (2.3127e-02)	Acc@1  98.44 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [75][350/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.2074e-02 (2.3541e-02)	Acc@1  98.44 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [75][360/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1757e-02 (2.3441e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [75][370/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.0035e-02 (2.3255e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [75][380/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2352e-02 (2.3430e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [75][390/391]	Time  0.101 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.2725e-02 (2.3452e-02)	Acc@1  98.75 ( 99.26)	Acc@5 100.00 (100.00)
## e[75] optimizer.zero_grad (sum) time: 0.3496377468109131
## e[75]       loss.backward (sum) time: 13.703821420669556
## e[75]      optimizer.step (sum) time: 2.7625300884246826
## epoch[75] training(only) time: 42.613330602645874
# Switched to evaluate mode...
Test: [  0/100]	Time  0.239 ( 0.239)	Loss 3.4326e-01 (3.4326e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.039 ( 0.056)	Loss 2.6147e-01 (3.8975e-01)	Acc@1  94.00 ( 91.27)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.037 ( 0.047)	Loss 5.2148e-01 (4.1434e-01)	Acc@1  89.00 ( 91.24)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.037 ( 0.044)	Loss 4.1382e-01 (4.3674e-01)	Acc@1  91.00 ( 91.35)	Acc@5  99.00 ( 99.48)
Test: [ 40/100]	Time  0.038 ( 0.042)	Loss 3.2520e-01 (4.3196e-01)	Acc@1  91.00 ( 91.24)	Acc@5  99.00 ( 99.51)
Test: [ 50/100]	Time  0.037 ( 0.041)	Loss 3.0298e-01 (4.2408e-01)	Acc@1  91.00 ( 91.24)	Acc@5 100.00 ( 99.53)
Test: [ 60/100]	Time  0.040 ( 0.041)	Loss 5.8154e-01 (4.1701e-01)	Acc@1  93.00 ( 91.38)	Acc@5  99.00 ( 99.56)
Test: [ 70/100]	Time  0.037 ( 0.040)	Loss 6.4648e-01 (4.1165e-01)	Acc@1  88.00 ( 91.30)	Acc@5  99.00 ( 99.59)
Test: [ 80/100]	Time  0.037 ( 0.040)	Loss 2.6904e-01 (4.1010e-01)	Acc@1  93.00 ( 91.35)	Acc@5 100.00 ( 99.59)
Test: [ 90/100]	Time  0.037 ( 0.040)	Loss 2.9468e-01 (4.0392e-01)	Acc@1  94.00 ( 91.30)	Acc@5 100.00 ( 99.59)
 * Acc@1 91.300 Acc@5 99.620
### epoch[75] execution time: 46.641618490219116
EPOCH 76
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [76][  0/391]	Time  0.265 ( 0.265)	Data  0.167 ( 0.167)	Loss 6.2073e-02 (6.2073e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [76][ 10/391]	Time  0.107 ( 0.123)	Data  0.001 ( 0.019)	Loss 2.4902e-02 (2.5011e-02)	Acc@1  98.44 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [76][ 20/391]	Time  0.106 ( 0.116)	Data  0.001 ( 0.012)	Loss 6.6101e-02 (2.6012e-02)	Acc@1  98.44 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [76][ 30/391]	Time  0.106 ( 0.114)	Data  0.001 ( 0.009)	Loss 2.9816e-02 (2.4020e-02)	Acc@1  98.44 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [76][ 40/391]	Time  0.106 ( 0.113)	Data  0.001 ( 0.008)	Loss 2.0325e-02 (2.3392e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [76][ 50/391]	Time  0.106 ( 0.112)	Data  0.001 ( 0.007)	Loss 7.3700e-03 (2.3226e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [76][ 60/391]	Time  0.104 ( 0.111)	Data  0.001 ( 0.007)	Loss 2.5955e-02 (2.3352e-02)	Acc@1  98.44 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [76][ 70/391]	Time  0.107 ( 0.111)	Data  0.001 ( 0.006)	Loss 4.9835e-02 (2.3609e-02)	Acc@1  97.66 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [76][ 80/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.006)	Loss 7.2937e-02 (2.4338e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [76][ 90/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 5.4596e-02 (2.4394e-02)	Acc@1  97.66 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [76][100/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 2.9602e-02 (2.4289e-02)	Acc@1  98.44 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [76][110/391]	Time  0.103 ( 0.110)	Data  0.001 ( 0.006)	Loss 2.6367e-02 (2.4026e-02)	Acc@1  98.44 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [76][120/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.7883e-02 (2.3933e-02)	Acc@1  99.22 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [76][130/391]	Time  0.107 ( 0.110)	Data  0.002 ( 0.005)	Loss 2.6306e-02 (2.3886e-02)	Acc@1  99.22 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [76][140/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.8425e-03 (2.4623e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [76][150/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.6207e-03 (2.4076e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [76][160/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.5302e-03 (2.3627e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [76][170/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.6754e-02 (2.3856e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [76][180/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.7527e-02 (2.4096e-02)	Acc@1  99.22 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [76][190/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.2385e-02 (2.4484e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [76][200/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.5735e-02 (2.5006e-02)	Acc@1  98.44 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [76][210/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2611e-02 (2.4850e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [76][220/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1002e-02 (2.4753e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [76][230/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5915e-02 (2.4642e-02)	Acc@1 100.00 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [76][240/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.0537e-02 (2.4628e-02)	Acc@1  98.44 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [76][250/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.8763e-02 (2.4530e-02)	Acc@1  99.22 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [76][260/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.5461e-02 (2.4896e-02)	Acc@1  98.44 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [76][270/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.3849e-02 (2.4784e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [76][280/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.7466e-02 (2.4657e-02)	Acc@1  98.44 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [76][290/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.5807e-02 (2.4644e-02)	Acc@1  97.66 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [76][300/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.0485e-03 (2.4457e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [76][310/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0010e-02 (2.4069e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [76][320/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.4536e-02 (2.4012e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [76][330/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.8503e-02 (2.4203e-02)	Acc@1  98.44 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [76][340/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2482e-02 (2.4166e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [76][350/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2657e-02 (2.4186e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [76][360/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.8043e-03 (2.4084e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [76][370/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0231e-02 (2.3877e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [76][380/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.6617e-02 (2.3799e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [76][390/391]	Time  0.101 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.0319e-02 (2.3694e-02)	Acc@1  98.75 ( 99.27)	Acc@5 100.00 (100.00)
## e[76] optimizer.zero_grad (sum) time: 0.3487284183502197
## e[76]       loss.backward (sum) time: 13.695420026779175
## e[76]      optimizer.step (sum) time: 2.7968358993530273
## epoch[76] training(only) time: 42.640084743499756
# Switched to evaluate mode...
Test: [  0/100]	Time  0.196 ( 0.196)	Loss 3.5400e-01 (3.5400e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.038 ( 0.052)	Loss 2.5610e-01 (3.9344e-01)	Acc@1  94.00 ( 91.36)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.036 ( 0.045)	Loss 5.4443e-01 (4.1903e-01)	Acc@1  90.00 ( 90.95)	Acc@5 100.00 ( 99.52)
Test: [ 30/100]	Time  0.037 ( 0.042)	Loss 4.1772e-01 (4.3641e-01)	Acc@1  91.00 ( 91.16)	Acc@5  99.00 ( 99.42)
Test: [ 40/100]	Time  0.039 ( 0.041)	Loss 3.2568e-01 (4.3276e-01)	Acc@1  91.00 ( 91.07)	Acc@5  99.00 ( 99.44)
Test: [ 50/100]	Time  0.039 ( 0.040)	Loss 3.0200e-01 (4.2444e-01)	Acc@1  91.00 ( 91.08)	Acc@5 100.00 ( 99.49)
Test: [ 60/100]	Time  0.036 ( 0.040)	Loss 5.6641e-01 (4.1755e-01)	Acc@1  93.00 ( 91.23)	Acc@5  99.00 ( 99.56)
Test: [ 70/100]	Time  0.037 ( 0.039)	Loss 6.3770e-01 (4.1062e-01)	Acc@1  88.00 ( 91.28)	Acc@5  99.00 ( 99.58)
Test: [ 80/100]	Time  0.036 ( 0.039)	Loss 2.8711e-01 (4.0976e-01)	Acc@1  92.00 ( 91.32)	Acc@5 100.00 ( 99.58)
Test: [ 90/100]	Time  0.037 ( 0.039)	Loss 3.0371e-01 (4.0267e-01)	Acc@1  94.00 ( 91.35)	Acc@5 100.00 ( 99.58)
 * Acc@1 91.380 Acc@5 99.610
### epoch[76] execution time: 46.61636447906494
EPOCH 77
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [77][  0/391]	Time  0.274 ( 0.274)	Data  0.173 ( 0.173)	Loss 1.4893e-02 (1.4893e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [77][ 10/391]	Time  0.108 ( 0.124)	Data  0.001 ( 0.019)	Loss 1.3275e-02 (2.0137e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [77][ 20/391]	Time  0.104 ( 0.117)	Data  0.001 ( 0.012)	Loss 1.7380e-02 (2.0950e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [77][ 30/391]	Time  0.107 ( 0.114)	Data  0.001 ( 0.010)	Loss 1.4778e-02 (1.9113e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [77][ 40/391]	Time  0.109 ( 0.113)	Data  0.001 ( 0.008)	Loss 1.2642e-02 (1.9618e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [77][ 50/391]	Time  0.106 ( 0.112)	Data  0.001 ( 0.007)	Loss 7.8011e-03 (2.0222e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [77][ 60/391]	Time  0.108 ( 0.111)	Data  0.001 ( 0.007)	Loss 2.3987e-02 (2.0392e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [77][ 70/391]	Time  0.109 ( 0.111)	Data  0.001 ( 0.007)	Loss 1.9547e-02 (2.0926e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [77][ 80/391]	Time  0.103 ( 0.110)	Data  0.001 ( 0.006)	Loss 5.7129e-02 (2.1817e-02)	Acc@1  98.44 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [77][ 90/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 9.8572e-03 (2.1825e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [77][100/391]	Time  0.107 ( 0.110)	Data  0.002 ( 0.006)	Loss 2.8473e-02 (2.1704e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [77][110/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.006)	Loss 5.3711e-03 (2.1124e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [77][120/391]	Time  0.103 ( 0.110)	Data  0.001 ( 0.006)	Loss 3.7720e-02 (2.0729e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [77][130/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2268e-02 (2.0822e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [77][140/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.1830e-02 (2.1106e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [77][150/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.4216e-02 (2.1128e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [77][160/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.0172e-02 (2.0729e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [77][170/391]	Time  0.107 ( 0.109)	Data  0.002 ( 0.005)	Loss 4.4746e-03 (2.0961e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [77][180/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.8595e-02 (2.1487e-02)	Acc@1  97.66 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [77][190/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.0004e-02 (2.2032e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [77][200/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.1290e-02 (2.2169e-02)	Acc@1  98.44 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [77][210/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.0883e-02 (2.2413e-02)	Acc@1  96.88 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [77][220/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.3539e-02 (2.2301e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [77][230/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.8964e-03 (2.2295e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [77][240/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.0096e-02 (2.2692e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [77][250/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2154e-02 (2.2486e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [77][260/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.6936e-02 (2.2689e-02)	Acc@1  97.66 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [77][270/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.2227e-02 (2.2476e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [77][280/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.1057e-02 (2.2420e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [77][290/391]	Time  0.101 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.3956e-02 (2.2780e-02)	Acc@1  98.44 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [77][300/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.7487e-02 (2.3057e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [77][310/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.6215e-02 (2.3021e-02)	Acc@1  98.44 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [77][320/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.6260e-02 (2.2914e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [77][330/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.1525e-02 (2.3092e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [77][340/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.4368e-02 (2.3052e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [77][350/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.2324e-02 (2.3110e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [77][360/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.2141e-02 (2.3074e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [77][370/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4030e-02 (2.3113e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [77][380/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1147e-02 (2.2931e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [77][390/391]	Time  0.100 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4664e-02 (2.2837e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
## e[77] optimizer.zero_grad (sum) time: 0.34874606132507324
## e[77]       loss.backward (sum) time: 13.716400861740112
## e[77]      optimizer.step (sum) time: 2.7238194942474365
## epoch[77] training(only) time: 42.613908529281616
# Switched to evaluate mode...
Test: [  0/100]	Time  0.230 ( 0.230)	Loss 3.2520e-01 (3.2520e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.037 ( 0.056)	Loss 2.6880e-01 (3.8507e-01)	Acc@1  94.00 ( 91.09)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.036 ( 0.047)	Loss 5.1221e-01 (4.1021e-01)	Acc@1  89.00 ( 91.19)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.036 ( 0.043)	Loss 3.9502e-01 (4.3443e-01)	Acc@1  91.00 ( 91.26)	Acc@5  99.00 ( 99.52)
Test: [ 40/100]	Time  0.035 ( 0.042)	Loss 3.1006e-01 (4.2942e-01)	Acc@1  92.00 ( 91.15)	Acc@5  99.00 ( 99.51)
Test: [ 50/100]	Time  0.036 ( 0.041)	Loss 2.8931e-01 (4.2197e-01)	Acc@1  92.00 ( 91.27)	Acc@5 100.00 ( 99.53)
Test: [ 60/100]	Time  0.037 ( 0.040)	Loss 5.8545e-01 (4.1510e-01)	Acc@1  93.00 ( 91.36)	Acc@5  99.00 ( 99.56)
Test: [ 70/100]	Time  0.037 ( 0.040)	Loss 6.3477e-01 (4.0839e-01)	Acc@1  89.00 ( 91.32)	Acc@5  99.00 ( 99.59)
Test: [ 80/100]	Time  0.039 ( 0.039)	Loss 2.8931e-01 (4.0749e-01)	Acc@1  92.00 ( 91.32)	Acc@5 100.00 ( 99.59)
Test: [ 90/100]	Time  0.036 ( 0.039)	Loss 3.1030e-01 (4.0096e-01)	Acc@1  93.00 ( 91.31)	Acc@5 100.00 ( 99.59)
 * Acc@1 91.330 Acc@5 99.620
### epoch[77] execution time: 46.60264849662781
EPOCH 78
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [78][  0/391]	Time  0.272 ( 0.272)	Data  0.175 ( 0.175)	Loss 3.3020e-02 (3.3020e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [78][ 10/391]	Time  0.102 ( 0.122)	Data  0.001 ( 0.019)	Loss 4.5319e-02 (2.4017e-02)	Acc@1  98.44 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [78][ 20/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.012)	Loss 1.6830e-02 (2.5119e-02)	Acc@1  99.22 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [78][ 30/391]	Time  0.112 ( 0.113)	Data  0.001 ( 0.009)	Loss 3.2104e-02 (2.3590e-02)	Acc@1  97.66 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [78][ 40/391]	Time  0.106 ( 0.112)	Data  0.001 ( 0.008)	Loss 2.3590e-02 (2.3605e-02)	Acc@1  98.44 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [78][ 50/391]	Time  0.106 ( 0.111)	Data  0.001 ( 0.007)	Loss 7.0381e-03 (2.3431e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [78][ 60/391]	Time  0.106 ( 0.111)	Data  0.001 ( 0.007)	Loss 3.1235e-02 (2.4343e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [78][ 70/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.006)	Loss 6.7291e-03 (2.3991e-02)	Acc@1 100.00 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [78][ 80/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.006)	Loss 5.9395e-03 (2.3300e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [78][ 90/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.006)	Loss 6.5651e-03 (2.3591e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [78][100/391]	Time  0.102 ( 0.110)	Data  0.001 ( 0.006)	Loss 6.1920e-02 (2.3965e-02)	Acc@1  97.66 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [78][110/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.006)	Loss 7.6714e-03 (2.3739e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [78][120/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.006)	Loss 4.5547e-03 (2.3807e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [78][130/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.2239e-03 (2.4081e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [78][140/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.1484e-02 (2.3356e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [78][150/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.4454e-02 (2.3824e-02)	Acc@1  97.66 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [78][160/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2535e-02 (2.3736e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [78][170/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.1100e-03 (2.3631e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [78][180/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.5248e-02 (2.3447e-02)	Acc@1  98.44 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [78][190/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1070e-02 (2.3273e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [78][200/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.5116e-02 (2.3562e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [78][210/391]	Time  0.106 ( 0.109)	Data  0.002 ( 0.005)	Loss 1.7822e-02 (2.3625e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [78][220/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.9929e-03 (2.3360e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [78][230/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5152e-02 (2.3239e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [78][240/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2306e-02 (2.2986e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [78][250/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.7057e-03 (2.2938e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [78][260/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2375e-02 (2.2658e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [78][270/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.7204e-03 (2.2970e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [78][280/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.9012e-02 (2.2933e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [78][290/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.2751e-02 (2.2730e-02)	Acc@1  97.66 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [78][300/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.1462e-02 (2.2929e-02)	Acc@1  98.44 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [78][310/391]	Time  0.103 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.4114e-02 (2.2871e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [78][320/391]	Time  0.105 ( 0.108)	Data  0.001 ( 0.005)	Loss 2.7191e-02 (2.2809e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [78][330/391]	Time  0.108 ( 0.108)	Data  0.001 ( 0.005)	Loss 7.5264e-03 (2.2789e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [78][340/391]	Time  0.104 ( 0.108)	Data  0.001 ( 0.005)	Loss 5.6824e-02 (2.3172e-02)	Acc@1  97.66 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [78][350/391]	Time  0.106 ( 0.108)	Data  0.001 ( 0.005)	Loss 2.4582e-02 (2.3234e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [78][360/391]	Time  0.106 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.8448e-02 (2.3355e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [78][370/391]	Time  0.106 ( 0.108)	Data  0.001 ( 0.005)	Loss 5.9296e-02 (2.3386e-02)	Acc@1  97.66 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [78][380/391]	Time  0.102 ( 0.108)	Data  0.001 ( 0.005)	Loss 6.4758e-02 (2.3719e-02)	Acc@1  98.44 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [78][390/391]	Time  0.102 ( 0.108)	Data  0.001 ( 0.005)	Loss 4.9286e-02 (2.3888e-02)	Acc@1  96.25 ( 99.28)	Acc@5 100.00 (100.00)
## e[78] optimizer.zero_grad (sum) time: 0.35135483741760254
## e[78]       loss.backward (sum) time: 13.661795616149902
## e[78]      optimizer.step (sum) time: 2.7106406688690186
## epoch[78] training(only) time: 42.51379442214966
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 3.5083e-01 (3.5083e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.036 ( 0.052)	Loss 2.6172e-01 (3.9360e-01)	Acc@1  94.00 ( 91.36)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.037 ( 0.045)	Loss 5.3125e-01 (4.1628e-01)	Acc@1  90.00 ( 91.19)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.037 ( 0.042)	Loss 4.1406e-01 (4.3692e-01)	Acc@1  91.00 ( 91.32)	Acc@5  99.00 ( 99.45)
Test: [ 40/100]	Time  0.039 ( 0.041)	Loss 3.3130e-01 (4.3424e-01)	Acc@1  92.00 ( 91.27)	Acc@5  99.00 ( 99.49)
Test: [ 50/100]	Time  0.037 ( 0.040)	Loss 2.9053e-01 (4.2621e-01)	Acc@1  91.00 ( 91.25)	Acc@5 100.00 ( 99.53)
Test: [ 60/100]	Time  0.036 ( 0.040)	Loss 5.6982e-01 (4.1889e-01)	Acc@1  93.00 ( 91.41)	Acc@5  99.00 ( 99.59)
Test: [ 70/100]	Time  0.036 ( 0.039)	Loss 6.4551e-01 (4.1276e-01)	Acc@1  88.00 ( 91.35)	Acc@5  99.00 ( 99.62)
Test: [ 80/100]	Time  0.036 ( 0.039)	Loss 2.6538e-01 (4.1111e-01)	Acc@1  93.00 ( 91.42)	Acc@5 100.00 ( 99.62)
Test: [ 90/100]	Time  0.037 ( 0.039)	Loss 3.0103e-01 (4.0438e-01)	Acc@1  94.00 ( 91.43)	Acc@5 100.00 ( 99.60)
 * Acc@1 91.460 Acc@5 99.630
### epoch[78] execution time: 46.48615598678589
EPOCH 79
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [79][  0/391]	Time  0.319 ( 0.319)	Data  0.209 ( 0.209)	Loss 3.0380e-02 (3.0380e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [79][ 10/391]	Time  0.105 ( 0.127)	Data  0.001 ( 0.022)	Loss 2.6962e-02 (1.9317e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [79][ 20/391]	Time  0.107 ( 0.118)	Data  0.001 ( 0.014)	Loss 1.3092e-02 (2.5537e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [79][ 30/391]	Time  0.109 ( 0.115)	Data  0.001 ( 0.011)	Loss 1.0742e-02 (2.4710e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [79][ 40/391]	Time  0.108 ( 0.113)	Data  0.002 ( 0.009)	Loss 3.4119e-02 (2.4022e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [79][ 50/391]	Time  0.104 ( 0.112)	Data  0.001 ( 0.008)	Loss 2.7527e-02 (2.2675e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [79][ 60/391]	Time  0.106 ( 0.112)	Data  0.001 ( 0.008)	Loss 2.3407e-02 (2.3660e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [79][ 70/391]	Time  0.107 ( 0.111)	Data  0.001 ( 0.007)	Loss 9.9792e-03 (2.2885e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [79][ 80/391]	Time  0.104 ( 0.111)	Data  0.001 ( 0.007)	Loss 3.3417e-02 (2.3042e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [79][ 90/391]	Time  0.100 ( 0.110)	Data  0.001 ( 0.006)	Loss 2.1835e-02 (2.3231e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [79][100/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.006)	Loss 9.6130e-03 (2.3186e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [79][110/391]	Time  0.104 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.1627e-02 (2.2422e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [79][120/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 8.0490e-03 (2.2298e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [79][130/391]	Time  0.109 ( 0.110)	Data  0.001 ( 0.006)	Loss 2.7924e-02 (2.2127e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [79][140/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 3.0579e-02 (2.2229e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [79][150/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.006)	Loss 2.3804e-02 (2.2002e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [79][160/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.7924e-02 (2.2171e-02)	Acc@1  98.44 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [79][170/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5869e-02 (2.2195e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [79][180/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.0812e-02 (2.2155e-02)	Acc@1  98.44 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [79][190/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.5635e-02 (2.2378e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [79][200/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.5746e-02 (2.2745e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [79][210/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.3162e-02 (2.2701e-02)	Acc@1  98.44 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [79][220/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.7028e-02 (2.2794e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [79][230/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.8168e-02 (2.2914e-02)	Acc@1  98.44 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [79][240/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4328e-02 (2.2572e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [79][250/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.8366e-02 (2.2626e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [79][260/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.8043e-03 (2.2610e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [79][270/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.7881e-02 (2.2685e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [79][280/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2810e-02 (2.2552e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [79][290/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.4468e-03 (2.2398e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [79][300/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.8448e-02 (2.2182e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [79][310/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.2349e-02 (2.2110e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [79][320/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2665e-02 (2.1912e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [79][330/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.0182e-02 (2.1985e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [79][340/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.8295e-02 (2.2087e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [79][350/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.7556e-02 (2.2263e-02)	Acc@1  97.66 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [79][360/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.6122e-02 (2.2445e-02)	Acc@1  98.44 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [79][370/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2589e-02 (2.2353e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [79][380/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4870e-02 (2.2428e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [79][390/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.5983e-03 (2.2365e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
## e[79] optimizer.zero_grad (sum) time: 0.3478212356567383
## e[79]       loss.backward (sum) time: 13.667325735092163
## e[79]      optimizer.step (sum) time: 2.7614147663116455
## epoch[79] training(only) time: 42.580172061920166
# Switched to evaluate mode...
Test: [  0/100]	Time  0.192 ( 0.192)	Loss 3.4570e-01 (3.4570e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.036 ( 0.051)	Loss 2.6318e-01 (3.9167e-01)	Acc@1  94.00 ( 91.36)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.037 ( 0.044)	Loss 5.1465e-01 (4.1848e-01)	Acc@1  90.00 ( 91.24)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.037 ( 0.042)	Loss 4.1602e-01 (4.3678e-01)	Acc@1  91.00 ( 91.32)	Acc@5  99.00 ( 99.45)
Test: [ 40/100]	Time  0.037 ( 0.041)	Loss 3.1152e-01 (4.3343e-01)	Acc@1  92.00 ( 91.24)	Acc@5  99.00 ( 99.49)
Test: [ 50/100]	Time  0.036 ( 0.040)	Loss 2.8613e-01 (4.2489e-01)	Acc@1  92.00 ( 91.25)	Acc@5 100.00 ( 99.53)
Test: [ 60/100]	Time  0.037 ( 0.040)	Loss 5.7910e-01 (4.1790e-01)	Acc@1  93.00 ( 91.51)	Acc@5  99.00 ( 99.57)
Test: [ 70/100]	Time  0.036 ( 0.039)	Loss 6.3672e-01 (4.1192e-01)	Acc@1  88.00 ( 91.46)	Acc@5  99.00 ( 99.61)
Test: [ 80/100]	Time  0.038 ( 0.039)	Loss 2.9004e-01 (4.1075e-01)	Acc@1  92.00 ( 91.52)	Acc@5 100.00 ( 99.60)
Test: [ 90/100]	Time  0.037 ( 0.039)	Loss 3.2788e-01 (4.0392e-01)	Acc@1  94.00 ( 91.51)	Acc@5 100.00 ( 99.59)
 * Acc@1 91.490 Acc@5 99.610
### epoch[79] execution time: 46.540297746658325
EPOCH 80
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [80][  0/391]	Time  0.274 ( 0.274)	Data  0.177 ( 0.177)	Loss 1.2520e-02 (1.2520e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [80][ 10/391]	Time  0.105 ( 0.122)	Data  0.001 ( 0.020)	Loss 1.2756e-02 (2.4973e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [80][ 20/391]	Time  0.106 ( 0.115)	Data  0.001 ( 0.012)	Loss 1.5358e-02 (2.1621e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [80][ 30/391]	Time  0.107 ( 0.113)	Data  0.001 ( 0.010)	Loss 1.0506e-02 (2.1788e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [80][ 40/391]	Time  0.107 ( 0.112)	Data  0.001 ( 0.008)	Loss 1.4549e-02 (2.2626e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [80][ 50/391]	Time  0.106 ( 0.111)	Data  0.001 ( 0.007)	Loss 3.7109e-02 (2.3137e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [80][ 60/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.007)	Loss 4.6417e-02 (2.5550e-02)	Acc@1  98.44 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [80][ 70/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.007)	Loss 9.7580e-03 (2.4229e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [80][ 80/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.006)	Loss 2.1393e-02 (2.3998e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [80][ 90/391]	Time  0.112 ( 0.110)	Data  0.001 ( 0.006)	Loss 2.2522e-02 (2.3943e-02)	Acc@1  99.22 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [80][100/391]	Time  0.101 ( 0.109)	Data  0.001 ( 0.006)	Loss 2.9877e-02 (2.4571e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [80][110/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.006)	Loss 1.4099e-02 (2.4151e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [80][120/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.006)	Loss 4.0405e-02 (2.4353e-02)	Acc@1  98.44 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [80][130/391]	Time  0.111 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.8799e-02 (2.4285e-02)	Acc@1  99.22 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [80][140/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.3304e-02 (2.4331e-02)	Acc@1  98.44 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [80][150/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.1382e-02 (2.4103e-02)	Acc@1  98.44 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [80][160/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.7496e-02 (2.3783e-02)	Acc@1  98.44 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [80][170/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.3965e-02 (2.3848e-02)	Acc@1  96.88 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [80][180/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.2926e-03 (2.3888e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [80][190/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.0142e-02 (2.3303e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [80][200/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.9324e-03 (2.2888e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [80][210/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.0233e-02 (2.2945e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [80][220/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.7264e-03 (2.2606e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [80][230/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.9276e-02 (2.2921e-02)	Acc@1  98.44 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [80][240/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.0948e-03 (2.3024e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [80][250/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.9335e-03 (2.2650e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [80][260/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.2550e-03 (2.2692e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [80][270/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.8272e-03 (2.2590e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [80][280/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.0110e-02 (2.2821e-02)	Acc@1  97.66 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [80][290/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.9856e-02 (2.2941e-02)	Acc@1  98.44 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [80][300/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.0538e-02 (2.2842e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [80][310/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.9785e-02 (2.2693e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [80][320/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1543e-02 (2.2772e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [80][330/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5930e-02 (2.2825e-02)	Acc@1  98.44 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [80][340/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.2480e-02 (2.2735e-02)	Acc@1  98.44 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [80][350/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.8976e-02 (2.2741e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [80][360/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.0005e-03 (2.2721e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [80][370/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.7395e-02 (2.2722e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [80][380/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4854e-02 (2.2854e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [80][390/391]	Time  0.100 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1642e-02 (2.2823e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
## e[80] optimizer.zero_grad (sum) time: 0.3478877544403076
## e[80]       loss.backward (sum) time: 13.697513580322266
## e[80]      optimizer.step (sum) time: 2.8201332092285156
## epoch[80] training(only) time: 42.57307839393616
# Switched to evaluate mode...
Test: [  0/100]	Time  0.196 ( 0.196)	Loss 3.6523e-01 (3.6523e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.040 ( 0.052)	Loss 2.6880e-01 (3.9035e-01)	Acc@1  93.00 ( 91.09)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.036 ( 0.045)	Loss 5.1904e-01 (4.2031e-01)	Acc@1  89.00 ( 90.81)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.036 ( 0.042)	Loss 4.2139e-01 (4.4396e-01)	Acc@1  91.00 ( 91.03)	Acc@5  99.00 ( 99.48)
Test: [ 40/100]	Time  0.036 ( 0.041)	Loss 3.0811e-01 (4.3744e-01)	Acc@1  92.00 ( 91.05)	Acc@5  99.00 ( 99.49)
Test: [ 50/100]	Time  0.036 ( 0.040)	Loss 3.0493e-01 (4.2981e-01)	Acc@1  92.00 ( 91.10)	Acc@5 100.00 ( 99.47)
Test: [ 60/100]	Time  0.036 ( 0.040)	Loss 5.9131e-01 (4.2284e-01)	Acc@1  93.00 ( 91.20)	Acc@5  99.00 ( 99.51)
Test: [ 70/100]	Time  0.036 ( 0.039)	Loss 7.1094e-01 (4.1803e-01)	Acc@1  88.00 ( 91.20)	Acc@5  99.00 ( 99.54)
Test: [ 80/100]	Time  0.038 ( 0.039)	Loss 3.1152e-01 (4.1726e-01)	Acc@1  92.00 ( 91.25)	Acc@5 100.00 ( 99.54)
Test: [ 90/100]	Time  0.038 ( 0.039)	Loss 3.0371e-01 (4.1062e-01)	Acc@1  94.00 ( 91.27)	Acc@5 100.00 ( 99.55)
 * Acc@1 91.280 Acc@5 99.580
### epoch[80] execution time: 46.53012442588806
EPOCH 81
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [81][  0/391]	Time  0.271 ( 0.271)	Data  0.174 ( 0.174)	Loss 8.9798e-03 (8.9798e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [81][ 10/391]	Time  0.105 ( 0.122)	Data  0.001 ( 0.019)	Loss 7.1526e-03 (2.0476e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [81][ 20/391]	Time  0.107 ( 0.115)	Data  0.001 ( 0.012)	Loss 3.4504e-03 (1.7491e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [81][ 30/391]	Time  0.103 ( 0.113)	Data  0.001 ( 0.009)	Loss 1.5121e-02 (2.0903e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [81][ 40/391]	Time  0.108 ( 0.112)	Data  0.001 ( 0.008)	Loss 1.6586e-02 (2.3503e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [81][ 50/391]	Time  0.107 ( 0.111)	Data  0.001 ( 0.007)	Loss 3.0334e-02 (2.2098e-02)	Acc@1  98.44 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [81][ 60/391]	Time  0.107 ( 0.111)	Data  0.001 ( 0.007)	Loss 4.3297e-03 (2.0796e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [81][ 70/391]	Time  0.104 ( 0.110)	Data  0.001 ( 0.006)	Loss 6.3591e-03 (2.0243e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [81][ 80/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 4.5197e-02 (2.1295e-02)	Acc@1  97.66 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [81][ 90/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.3077e-02 (2.0420e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [81][100/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.006)	Loss 1.2459e-02 (1.9906e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [81][110/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.006)	Loss 3.3539e-02 (2.0295e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [81][120/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.6478e-02 (2.0637e-02)	Acc@1  98.44 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [81][130/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.8223e-03 (2.0907e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [81][140/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.2837e-02 (2.1242e-02)	Acc@1  98.44 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [81][150/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.9479e-02 (2.1080e-02)	Acc@1  98.44 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [81][160/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.9119e-02 (2.1352e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [81][170/391]	Time  0.111 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.8671e-02 (2.1638e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [81][180/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.9241e-02 (2.1704e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [81][190/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.6907e-03 (2.1320e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [81][200/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3252e-02 (2.1500e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [81][210/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.1042e-02 (2.1355e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [81][220/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.6413e-02 (2.1487e-02)	Acc@1  98.44 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [81][230/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.7363e-02 (2.1753e-02)	Acc@1  98.44 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [81][240/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3039e-02 (2.1379e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [81][250/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.2378e-02 (2.1739e-02)	Acc@1  98.44 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [81][260/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.3008e-03 (2.1319e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [81][270/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.0659e-02 (2.1451e-02)	Acc@1  98.44 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [81][280/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1726e-02 (2.1332e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [81][290/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4717e-02 (2.1183e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [81][300/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.3361e-02 (2.1246e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [81][310/391]	Time  0.099 ( 0.108)	Data  0.001 ( 0.005)	Loss 2.5620e-02 (2.1164e-02)	Acc@1  98.44 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [81][320/391]	Time  0.109 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.0857e-02 (2.0987e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [81][330/391]	Time  0.108 ( 0.108)	Data  0.001 ( 0.005)	Loss 4.8431e-02 (2.1422e-02)	Acc@1  98.44 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [81][340/391]	Time  0.104 ( 0.108)	Data  0.001 ( 0.005)	Loss 2.8503e-02 (2.1370e-02)	Acc@1  97.66 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [81][350/391]	Time  0.107 ( 0.108)	Data  0.001 ( 0.005)	Loss 6.2752e-03 (2.1374e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [81][360/391]	Time  0.105 ( 0.108)	Data  0.001 ( 0.005)	Loss 7.4097e-02 (2.1619e-02)	Acc@1  97.66 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [81][370/391]	Time  0.105 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.6083e-02 (2.1627e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [81][380/391]	Time  0.102 ( 0.108)	Data  0.001 ( 0.005)	Loss 6.0516e-02 (2.1727e-02)	Acc@1  98.44 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [81][390/391]	Time  0.099 ( 0.108)	Data  0.001 ( 0.005)	Loss 9.4910e-03 (2.1606e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
## e[81] optimizer.zero_grad (sum) time: 0.357327938079834
## e[81]       loss.backward (sum) time: 13.672957181930542
## e[81]      optimizer.step (sum) time: 2.7638039588928223
## epoch[81] training(only) time: 42.4908401966095
# Switched to evaluate mode...
Test: [  0/100]	Time  0.201 ( 0.201)	Loss 3.5327e-01 (3.5327e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.037 ( 0.053)	Loss 2.8149e-01 (3.9167e-01)	Acc@1  94.00 ( 91.36)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.037 ( 0.045)	Loss 4.9316e-01 (4.1754e-01)	Acc@1  89.00 ( 91.24)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.037 ( 0.043)	Loss 4.0381e-01 (4.4540e-01)	Acc@1  92.00 ( 91.32)	Acc@5  99.00 ( 99.48)
Test: [ 40/100]	Time  0.038 ( 0.041)	Loss 2.8979e-01 (4.3916e-01)	Acc@1  93.00 ( 91.22)	Acc@5  99.00 ( 99.51)
Test: [ 50/100]	Time  0.037 ( 0.040)	Loss 2.7979e-01 (4.3160e-01)	Acc@1  92.00 ( 91.24)	Acc@5 100.00 ( 99.49)
Test: [ 60/100]	Time  0.039 ( 0.040)	Loss 5.8350e-01 (4.2520e-01)	Acc@1  93.00 ( 91.34)	Acc@5  99.00 ( 99.52)
Test: [ 70/100]	Time  0.036 ( 0.039)	Loss 7.2900e-01 (4.2032e-01)	Acc@1  88.00 ( 91.30)	Acc@5  99.00 ( 99.55)
Test: [ 80/100]	Time  0.036 ( 0.039)	Loss 2.7124e-01 (4.2010e-01)	Acc@1  92.00 ( 91.36)	Acc@5 100.00 ( 99.56)
Test: [ 90/100]	Time  0.037 ( 0.039)	Loss 3.0615e-01 (4.1293e-01)	Acc@1  94.00 ( 91.38)	Acc@5 100.00 ( 99.56)
 * Acc@1 91.340 Acc@5 99.590
### epoch[81] execution time: 46.471092224121094
EPOCH 82
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [82][  0/391]	Time  0.277 ( 0.277)	Data  0.178 ( 0.178)	Loss 5.5023e-02 (5.5023e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [82][ 10/391]	Time  0.106 ( 0.123)	Data  0.001 ( 0.020)	Loss 3.4363e-02 (2.6477e-02)	Acc@1  99.22 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [82][ 20/391]	Time  0.107 ( 0.116)	Data  0.001 ( 0.012)	Loss 1.9104e-02 (2.2233e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [82][ 30/391]	Time  0.104 ( 0.113)	Data  0.001 ( 0.010)	Loss 1.4465e-02 (2.5361e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [82][ 40/391]	Time  0.106 ( 0.112)	Data  0.001 ( 0.008)	Loss 1.3405e-02 (2.4675e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [82][ 50/391]	Time  0.106 ( 0.111)	Data  0.001 ( 0.008)	Loss 1.2520e-02 (2.3730e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [82][ 60/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.007)	Loss 1.5717e-02 (2.4032e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [82][ 70/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.007)	Loss 4.9835e-02 (2.4392e-02)	Acc@1  98.44 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [82][ 80/391]	Time  0.100 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.4267e-02 (2.4061e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [82][ 90/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.006)	Loss 7.6790e-03 (2.3275e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [82][100/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.006)	Loss 3.1555e-02 (2.3098e-02)	Acc@1  98.44 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [82][110/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.006)	Loss 1.7334e-02 (2.3125e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [82][120/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.006)	Loss 3.6499e-02 (2.2788e-02)	Acc@1  98.44 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [82][130/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.6825e-02 (2.2400e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [82][140/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.0649e-02 (2.2358e-02)	Acc@1  98.44 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [82][150/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.7262e-02 (2.2213e-02)	Acc@1  97.66 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [82][160/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.5736e-02 (2.2074e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [82][170/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.2163e-03 (2.1917e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [82][180/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.9165e-02 (2.2054e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [82][190/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.4332e-02 (2.1997e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [82][200/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.1329e-03 (2.2009e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [82][210/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4175e-02 (2.1976e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [82][220/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2871e-02 (2.1845e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [82][230/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.7654e-02 (2.1951e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [82][240/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.3691e-02 (2.1854e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [82][250/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.8372e-02 (2.1702e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [82][260/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.8398e-03 (2.1691e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [82][270/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.6588e-03 (2.1839e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [82][280/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.9738e-03 (2.1571e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [82][290/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.3956e-02 (2.1743e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [82][300/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.6987e-02 (2.1661e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [82][310/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.6108e-02 (2.1545e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [82][320/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.7079e-02 (2.1763e-02)	Acc@1  97.66 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [82][330/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3710e-02 (2.1984e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [82][340/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1398e-02 (2.2036e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [82][350/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.8219e-02 (2.2047e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [82][360/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.8043e-03 (2.1855e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [82][370/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.2990e-02 (2.1962e-02)	Acc@1  98.44 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [82][380/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.4429e-02 (2.1795e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [82][390/391]	Time  0.098 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.4414e-02 (2.1655e-02)	Acc@1  98.75 ( 99.36)	Acc@5 100.00 (100.00)
## e[82] optimizer.zero_grad (sum) time: 0.3473525047302246
## e[82]       loss.backward (sum) time: 13.69308090209961
## e[82]      optimizer.step (sum) time: 2.796888589859009
## epoch[82] training(only) time: 42.620808601379395
# Switched to evaluate mode...
Test: [  0/100]	Time  0.192 ( 0.192)	Loss 3.5522e-01 (3.5522e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.037 ( 0.052)	Loss 2.4487e-01 (3.9889e-01)	Acc@1  94.00 ( 91.18)	Acc@5 100.00 ( 99.55)
Test: [ 20/100]	Time  0.038 ( 0.044)	Loss 5.2002e-01 (4.2150e-01)	Acc@1  90.00 ( 91.24)	Acc@5 100.00 ( 99.48)
Test: [ 30/100]	Time  0.037 ( 0.042)	Loss 4.2310e-01 (4.4485e-01)	Acc@1  91.00 ( 91.32)	Acc@5  99.00 ( 99.42)
Test: [ 40/100]	Time  0.044 ( 0.041)	Loss 3.3008e-01 (4.3832e-01)	Acc@1  91.00 ( 91.24)	Acc@5  99.00 ( 99.46)
Test: [ 50/100]	Time  0.038 ( 0.040)	Loss 2.9565e-01 (4.3102e-01)	Acc@1  91.00 ( 91.31)	Acc@5 100.00 ( 99.47)
Test: [ 60/100]	Time  0.037 ( 0.040)	Loss 5.7520e-01 (4.2395e-01)	Acc@1  93.00 ( 91.36)	Acc@5  99.00 ( 99.51)
Test: [ 70/100]	Time  0.039 ( 0.039)	Loss 6.9287e-01 (4.1767e-01)	Acc@1  88.00 ( 91.31)	Acc@5  99.00 ( 99.55)
Test: [ 80/100]	Time  0.036 ( 0.039)	Loss 2.9736e-01 (4.1755e-01)	Acc@1  92.00 ( 91.36)	Acc@5 100.00 ( 99.56)
Test: [ 90/100]	Time  0.036 ( 0.039)	Loss 3.0737e-01 (4.1088e-01)	Acc@1  94.00 ( 91.36)	Acc@5 100.00 ( 99.55)
 * Acc@1 91.340 Acc@5 99.580
### epoch[82] execution time: 46.57833409309387
EPOCH 83
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [83][  0/391]	Time  0.269 ( 0.269)	Data  0.174 ( 0.174)	Loss 8.7891e-03 (8.7891e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [83][ 10/391]	Time  0.104 ( 0.122)	Data  0.002 ( 0.019)	Loss 5.1117e-02 (2.2667e-02)	Acc@1  97.66 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [83][ 20/391]	Time  0.108 ( 0.115)	Data  0.001 ( 0.012)	Loss 1.8799e-02 (1.7286e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [83][ 30/391]	Time  0.104 ( 0.113)	Data  0.001 ( 0.009)	Loss 2.3392e-02 (1.9540e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [83][ 40/391]	Time  0.106 ( 0.112)	Data  0.001 ( 0.008)	Loss 2.1271e-02 (1.9660e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [83][ 50/391]	Time  0.108 ( 0.111)	Data  0.001 ( 0.007)	Loss 3.0609e-02 (2.0292e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [83][ 60/391]	Time  0.106 ( 0.111)	Data  0.001 ( 0.007)	Loss 1.7212e-02 (2.0477e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [83][ 70/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 2.1255e-02 (2.1634e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [83][ 80/391]	Time  0.104 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.6113e-02 (2.1287e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [83][ 90/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.006)	Loss 4.6143e-02 (2.1655e-02)	Acc@1  98.44 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [83][100/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.7563e-02 (2.2043e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [83][110/391]	Time  0.109 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.5762e-02 (2.2396e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [83][120/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.3234e-02 (2.2294e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [83][130/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5190e-02 (2.1992e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [83][140/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.4983e-02 (2.1852e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [83][150/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.0771e-02 (2.2088e-02)	Acc@1  98.44 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [83][160/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.0477e-02 (2.2761e-02)	Acc@1  98.44 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [83][170/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.9089e-02 (2.2974e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [83][180/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.8732e-02 (2.3092e-02)	Acc@1  98.44 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [83][190/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.0081e-03 (2.2867e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [83][200/391]	Time  0.105 ( 0.109)	Data  0.002 ( 0.005)	Loss 1.6281e-02 (2.2778e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [83][210/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.5879e-02 (2.2624e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [83][220/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.0381e-03 (2.2403e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [83][230/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.8677e-02 (2.2351e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [83][240/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.6693e-02 (2.1969e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [83][250/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.7197e-02 (2.2055e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [83][260/391]	Time  0.100 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.0201e-02 (2.2321e-02)	Acc@1  98.44 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [83][270/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.4241e-02 (2.2592e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [83][280/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.7303e-02 (2.2551e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [83][290/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4145e-02 (2.2306e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [83][300/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.7654e-02 (2.2494e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [83][310/391]	Time  0.100 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.0294e-02 (2.2264e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [83][320/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2474e-02 (2.2310e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [83][330/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.0233e-02 (2.2283e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [83][340/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.2623e-02 (2.2410e-02)	Acc@1  97.66 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [83][350/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.0343e-03 (2.2213e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [83][360/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.4201e-03 (2.2044e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [83][370/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.5389e-02 (2.2122e-02)	Acc@1  97.66 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [83][380/391]	Time  0.106 ( 0.108)	Data  0.001 ( 0.005)	Loss 7.7400e-03 (2.1917e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [83][390/391]	Time  0.102 ( 0.108)	Data  0.001 ( 0.005)	Loss 9.3460e-03 (2.1990e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
## e[83] optimizer.zero_grad (sum) time: 0.352794885635376
## e[83]       loss.backward (sum) time: 13.66967511177063
## e[83]      optimizer.step (sum) time: 2.7545480728149414
## epoch[83] training(only) time: 42.54781436920166
# Switched to evaluate mode...
Test: [  0/100]	Time  0.234 ( 0.234)	Loss 3.3618e-01 (3.3618e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.037 ( 0.055)	Loss 2.6855e-01 (3.8681e-01)	Acc@1  94.00 ( 91.82)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.039 ( 0.047)	Loss 5.4785e-01 (4.1714e-01)	Acc@1  90.00 ( 91.48)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.036 ( 0.043)	Loss 3.9819e-01 (4.3918e-01)	Acc@1  91.00 ( 91.52)	Acc@5  99.00 ( 99.52)
Test: [ 40/100]	Time  0.038 ( 0.042)	Loss 3.2397e-01 (4.3543e-01)	Acc@1  92.00 ( 91.41)	Acc@5  99.00 ( 99.51)
Test: [ 50/100]	Time  0.037 ( 0.041)	Loss 3.0371e-01 (4.2869e-01)	Acc@1  91.00 ( 91.41)	Acc@5 100.00 ( 99.49)
Test: [ 60/100]	Time  0.037 ( 0.040)	Loss 5.5615e-01 (4.2134e-01)	Acc@1  93.00 ( 91.49)	Acc@5  99.00 ( 99.52)
Test: [ 70/100]	Time  0.036 ( 0.040)	Loss 6.8848e-01 (4.1691e-01)	Acc@1  89.00 ( 91.42)	Acc@5  99.00 ( 99.55)
Test: [ 80/100]	Time  0.036 ( 0.039)	Loss 2.8735e-01 (4.1639e-01)	Acc@1  92.00 ( 91.47)	Acc@5 100.00 ( 99.56)
Test: [ 90/100]	Time  0.036 ( 0.039)	Loss 2.9492e-01 (4.0960e-01)	Acc@1  94.00 ( 91.45)	Acc@5 100.00 ( 99.56)
 * Acc@1 91.430 Acc@5 99.590
### epoch[83] execution time: 46.539644718170166
EPOCH 84
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [84][  0/391]	Time  0.265 ( 0.265)	Data  0.166 ( 0.166)	Loss 1.2268e-02 (1.2268e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [84][ 10/391]	Time  0.110 ( 0.123)	Data  0.001 ( 0.019)	Loss 8.2474e-03 (2.6409e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [84][ 20/391]	Time  0.105 ( 0.115)	Data  0.001 ( 0.012)	Loss 1.9577e-02 (2.3654e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [84][ 30/391]	Time  0.104 ( 0.113)	Data  0.001 ( 0.009)	Loss 2.3422e-02 (2.5710e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [84][ 40/391]	Time  0.110 ( 0.112)	Data  0.001 ( 0.008)	Loss 2.1973e-02 (2.4042e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [84][ 50/391]	Time  0.106 ( 0.111)	Data  0.001 ( 0.007)	Loss 1.6006e-02 (2.3516e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [84][ 60/391]	Time  0.103 ( 0.111)	Data  0.001 ( 0.007)	Loss 3.4027e-02 (2.2685e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [84][ 70/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 2.5696e-02 (2.2677e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [84][ 80/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.5930e-02 (2.3168e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [84][ 90/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 2.0035e-02 (2.2892e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [84][100/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.2413e-02 (2.3093e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [84][110/391]	Time  0.104 ( 0.110)	Data  0.001 ( 0.006)	Loss 4.1931e-02 (2.3550e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [84][120/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.2532e-02 (2.3791e-02)	Acc@1  97.66 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [84][130/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.0561e-03 (2.3537e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [84][140/391]	Time  0.111 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.6539e-02 (2.4043e-02)	Acc@1  98.44 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [84][150/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.8485e-03 (2.3548e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [84][160/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.4698e-02 (2.3797e-02)	Acc@1  98.44 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [84][170/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.6661e-02 (2.3923e-02)	Acc@1  98.44 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [84][180/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.6321e-02 (2.3899e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [84][190/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2367e-02 (2.3596e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [84][200/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.7466e-02 (2.3614e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [84][210/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.9411e-03 (2.3465e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [84][220/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.8112e-02 (2.3420e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [84][230/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.1067e-02 (2.3479e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [84][240/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.9001e-02 (2.3487e-02)	Acc@1  98.44 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [84][250/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.3232e-02 (2.3510e-02)	Acc@1  97.66 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [84][260/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0231e-02 (2.3583e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [84][270/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5091e-02 (2.3308e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [84][280/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.7291e-03 (2.3205e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [84][290/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.1150e-03 (2.3052e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [84][300/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.2847e-02 (2.3232e-02)	Acc@1  98.44 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [84][310/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.5208e-02 (2.3365e-02)	Acc@1  98.44 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [84][320/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2512e-02 (2.3305e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [84][330/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.7166e-02 (2.3144e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [84][340/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.8503e-02 (2.3070e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [84][350/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.0261e-03 (2.3220e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [84][360/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5137e-02 (2.3300e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [84][370/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.7729e-02 (2.3303e-02)	Acc@1  98.44 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [84][380/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0910e-02 (2.3312e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [84][390/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.9695e-03 (2.3151e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
## e[84] optimizer.zero_grad (sum) time: 0.3533174991607666
## e[84]       loss.backward (sum) time: 13.730396509170532
## e[84]      optimizer.step (sum) time: 2.7979040145874023
## epoch[84] training(only) time: 42.65928077697754
# Switched to evaluate mode...
Test: [  0/100]	Time  0.192 ( 0.192)	Loss 3.5815e-01 (3.5815e-01)	Acc@1  87.00 ( 87.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.039 ( 0.052)	Loss 2.6392e-01 (3.8300e-01)	Acc@1  94.00 ( 91.36)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.036 ( 0.045)	Loss 5.3125e-01 (4.1417e-01)	Acc@1  90.00 ( 91.43)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.037 ( 0.042)	Loss 4.0601e-01 (4.3728e-01)	Acc@1  91.00 ( 91.45)	Acc@5  99.00 ( 99.48)
Test: [ 40/100]	Time  0.045 ( 0.041)	Loss 2.9541e-01 (4.3203e-01)	Acc@1  92.00 ( 91.24)	Acc@5  99.00 ( 99.51)
Test: [ 50/100]	Time  0.036 ( 0.040)	Loss 2.9297e-01 (4.2502e-01)	Acc@1  92.00 ( 91.29)	Acc@5 100.00 ( 99.49)
Test: [ 60/100]	Time  0.037 ( 0.040)	Loss 5.7617e-01 (4.1839e-01)	Acc@1  93.00 ( 91.43)	Acc@5  99.00 ( 99.52)
Test: [ 70/100]	Time  0.039 ( 0.039)	Loss 6.8115e-01 (4.1288e-01)	Acc@1  88.00 ( 91.41)	Acc@5  99.00 ( 99.55)
Test: [ 80/100]	Time  0.037 ( 0.039)	Loss 2.9443e-01 (4.1180e-01)	Acc@1  92.00 ( 91.48)	Acc@5 100.00 ( 99.56)
Test: [ 90/100]	Time  0.038 ( 0.039)	Loss 3.0713e-01 (4.0523e-01)	Acc@1  94.00 ( 91.48)	Acc@5 100.00 ( 99.56)
 * Acc@1 91.450 Acc@5 99.580
### epoch[84] execution time: 46.624502420425415
EPOCH 85
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [85][  0/391]	Time  0.278 ( 0.278)	Data  0.177 ( 0.177)	Loss 7.5035e-03 (7.5035e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [85][ 10/391]	Time  0.110 ( 0.124)	Data  0.002 ( 0.019)	Loss 1.2161e-02 (1.6248e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [85][ 20/391]	Time  0.105 ( 0.116)	Data  0.001 ( 0.012)	Loss 2.0599e-02 (1.9247e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [85][ 30/391]	Time  0.105 ( 0.114)	Data  0.001 ( 0.010)	Loss 2.8671e-02 (1.8657e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [85][ 40/391]	Time  0.106 ( 0.112)	Data  0.001 ( 0.008)	Loss 1.9363e-02 (1.8704e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [85][ 50/391]	Time  0.108 ( 0.112)	Data  0.001 ( 0.007)	Loss 3.6255e-02 (1.9887e-02)	Acc@1  98.44 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [85][ 60/391]	Time  0.106 ( 0.111)	Data  0.001 ( 0.007)	Loss 1.6922e-02 (1.9671e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [85][ 70/391]	Time  0.101 ( 0.111)	Data  0.001 ( 0.007)	Loss 2.0370e-02 (1.9725e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [85][ 80/391]	Time  0.104 ( 0.110)	Data  0.001 ( 0.006)	Loss 5.8365e-03 (2.0319e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [85][ 90/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 3.5004e-02 (2.0404e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [85][100/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 2.5604e-02 (2.0302e-02)	Acc@1  98.44 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [85][110/391]	Time  0.103 ( 0.110)	Data  0.001 ( 0.006)	Loss 3.6316e-02 (1.9770e-02)	Acc@1  98.44 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [85][120/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.006)	Loss 1.3763e-02 (2.0203e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [85][130/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.7575e-03 (2.0151e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [85][140/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.3997e-02 (2.0025e-02)	Acc@1  98.44 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [85][150/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5640e-02 (2.0122e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [85][160/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3016e-02 (2.0406e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [85][170/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.5654e-02 (2.0336e-02)	Acc@1  97.66 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [85][180/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0849e-02 (2.0395e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [85][190/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.0554e-02 (2.0396e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [85][200/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.7283e-02 (2.0559e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [85][210/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.6459e-02 (2.0923e-02)	Acc@1  98.44 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [85][220/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.9791e-02 (2.0694e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [85][230/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.0588e-02 (2.0580e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [85][240/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5015e-02 (2.0407e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [85][250/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.9028e-02 (2.0428e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [85][260/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.6047e-02 (2.0316e-02)	Acc@1  98.44 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [85][270/391]	Time  0.103 ( 0.109)	Data  0.002 ( 0.005)	Loss 2.2003e-02 (2.0177e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [85][280/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.7567e-02 (2.0294e-02)	Acc@1  97.66 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [85][290/391]	Time  0.099 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.0935e-02 (2.0279e-02)	Acc@1  98.44 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [85][300/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.9803e-03 (2.0212e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [85][310/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.9028e-02 (2.0161e-02)	Acc@1  98.44 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [85][320/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.6430e-03 (2.0259e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [85][330/391]	Time  0.107 ( 0.108)	Data  0.001 ( 0.005)	Loss 5.6671e-02 (2.0470e-02)	Acc@1  97.66 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [85][340/391]	Time  0.108 ( 0.108)	Data  0.001 ( 0.005)	Loss 5.4703e-03 (2.0277e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [85][350/391]	Time  0.103 ( 0.108)	Data  0.001 ( 0.005)	Loss 2.5513e-02 (2.0340e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [85][360/391]	Time  0.105 ( 0.108)	Data  0.001 ( 0.005)	Loss 3.4119e-02 (2.0507e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [85][370/391]	Time  0.108 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.6037e-02 (2.0509e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [85][380/391]	Time  0.104 ( 0.108)	Data  0.001 ( 0.005)	Loss 2.0996e-02 (2.0420e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [85][390/391]	Time  0.100 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.1719e-02 (2.0505e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
## e[85] optimizer.zero_grad (sum) time: 0.34674620628356934
## e[85]       loss.backward (sum) time: 13.676486492156982
## e[85]      optimizer.step (sum) time: 2.744880199432373
## epoch[85] training(only) time: 42.53977704048157
# Switched to evaluate mode...
Test: [  0/100]	Time  0.225 ( 0.225)	Loss 3.4131e-01 (3.4131e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.038 ( 0.055)	Loss 2.7002e-01 (3.8971e-01)	Acc@1  94.00 ( 91.45)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.036 ( 0.046)	Loss 5.2539e-01 (4.1945e-01)	Acc@1  89.00 ( 91.38)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.037 ( 0.043)	Loss 4.2505e-01 (4.4476e-01)	Acc@1  90.00 ( 91.32)	Acc@5  99.00 ( 99.48)
Test: [ 40/100]	Time  0.037 ( 0.042)	Loss 2.9248e-01 (4.3845e-01)	Acc@1  92.00 ( 91.22)	Acc@5  99.00 ( 99.51)
Test: [ 50/100]	Time  0.036 ( 0.041)	Loss 3.0371e-01 (4.3078e-01)	Acc@1  92.00 ( 91.29)	Acc@5 100.00 ( 99.49)
Test: [ 60/100]	Time  0.036 ( 0.040)	Loss 5.8447e-01 (4.2411e-01)	Acc@1  93.00 ( 91.39)	Acc@5  99.00 ( 99.52)
Test: [ 70/100]	Time  0.038 ( 0.040)	Loss 6.7578e-01 (4.1877e-01)	Acc@1  89.00 ( 91.38)	Acc@5  99.00 ( 99.55)
Test: [ 80/100]	Time  0.036 ( 0.039)	Loss 3.0615e-01 (4.1820e-01)	Acc@1  92.00 ( 91.46)	Acc@5 100.00 ( 99.54)
Test: [ 90/100]	Time  0.036 ( 0.039)	Loss 3.1641e-01 (4.1093e-01)	Acc@1  94.00 ( 91.44)	Acc@5 100.00 ( 99.55)
 * Acc@1 91.440 Acc@5 99.580
### epoch[85] execution time: 46.52819752693176
EPOCH 86
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [86][  0/391]	Time  0.277 ( 0.277)	Data  0.170 ( 0.170)	Loss 7.1716e-03 (7.1716e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [86][ 10/391]	Time  0.106 ( 0.123)	Data  0.001 ( 0.019)	Loss 2.7466e-02 (1.9035e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [86][ 20/391]	Time  0.107 ( 0.116)	Data  0.001 ( 0.012)	Loss 2.6886e-02 (1.7460e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [86][ 30/391]	Time  0.108 ( 0.113)	Data  0.001 ( 0.009)	Loss 4.1260e-02 (2.1229e-02)	Acc@1  97.66 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [86][ 40/391]	Time  0.105 ( 0.112)	Data  0.001 ( 0.008)	Loss 2.8229e-02 (2.1664e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [86][ 50/391]	Time  0.109 ( 0.111)	Data  0.001 ( 0.007)	Loss 2.0096e-02 (2.1239e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [86][ 60/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.007)	Loss 4.4952e-02 (2.4127e-02)	Acc@1  98.44 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [86][ 70/391]	Time  0.105 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.9287e-02 (2.3532e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [86][ 80/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.0017e-02 (2.4185e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [86][ 90/391]	Time  0.110 ( 0.110)	Data  0.001 ( 0.006)	Loss 3.4454e-02 (2.4141e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [86][100/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 3.2135e-02 (2.3721e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [86][110/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.006)	Loss 1.1276e-02 (2.3777e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [86][120/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.8654e-03 (2.3157e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [86][130/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.2751e-02 (2.3279e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [86][140/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.8422e-02 (2.3261e-02)	Acc@1  98.44 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [86][150/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5022e-02 (2.2634e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [86][160/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.2049e-02 (2.2455e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [86][170/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5236e-02 (2.2238e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [86][180/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.9498e-03 (2.1934e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [86][190/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.5959e-02 (2.2279e-02)	Acc@1  97.66 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [86][200/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0696e-02 (2.1955e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [86][210/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.6062e-02 (2.1705e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [86][220/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2535e-02 (2.1713e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [86][230/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.4535e-02 (2.1822e-02)	Acc@1  96.88 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [86][240/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.2684e-02 (2.1630e-02)	Acc@1  98.44 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [86][250/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4702e-02 (2.1579e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [86][260/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.5526e-03 (2.1522e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [86][270/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.2135e-02 (2.1512e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [86][280/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.3765e-03 (2.1527e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [86][290/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.1708e-02 (2.1654e-02)	Acc@1  98.44 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [86][300/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.2714e-03 (2.1552e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [86][310/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.2863e-02 (2.1796e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [86][320/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.3010e-02 (2.2068e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [86][330/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0414e-02 (2.1995e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [86][340/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.5685e-02 (2.1869e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [86][350/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3420e-02 (2.1689e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [86][360/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.6459e-02 (2.1799e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [86][370/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.8890e-02 (2.1626e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [86][380/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.1362e-02 (2.1555e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [86][390/391]	Time  0.101 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.2474e-03 (2.1377e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
## e[86] optimizer.zero_grad (sum) time: 0.34941959381103516
## e[86]       loss.backward (sum) time: 13.755331039428711
## e[86]      optimizer.step (sum) time: 2.797919988632202
## epoch[86] training(only) time: 42.58272862434387
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 3.5474e-01 (3.5474e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.036 ( 0.051)	Loss 2.6245e-01 (3.9089e-01)	Acc@1  94.00 ( 91.27)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.045 ( 0.044)	Loss 5.2637e-01 (4.1811e-01)	Acc@1  90.00 ( 91.38)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.037 ( 0.042)	Loss 4.3481e-01 (4.4292e-01)	Acc@1  90.00 ( 91.39)	Acc@5  99.00 ( 99.48)
Test: [ 40/100]	Time  0.038 ( 0.041)	Loss 3.0957e-01 (4.3786e-01)	Acc@1  91.00 ( 91.27)	Acc@5  99.00 ( 99.51)
Test: [ 50/100]	Time  0.037 ( 0.040)	Loss 2.8955e-01 (4.2969e-01)	Acc@1  92.00 ( 91.29)	Acc@5 100.00 ( 99.51)
Test: [ 60/100]	Time  0.036 ( 0.040)	Loss 5.6396e-01 (4.2210e-01)	Acc@1  93.00 ( 91.34)	Acc@5  99.00 ( 99.54)
Test: [ 70/100]	Time  0.036 ( 0.039)	Loss 6.7285e-01 (4.1493e-01)	Acc@1  88.00 ( 91.35)	Acc@5  99.00 ( 99.56)
Test: [ 80/100]	Time  0.036 ( 0.039)	Loss 3.0200e-01 (4.1455e-01)	Acc@1  92.00 ( 91.41)	Acc@5 100.00 ( 99.57)
Test: [ 90/100]	Time  0.036 ( 0.039)	Loss 3.1226e-01 (4.0749e-01)	Acc@1  94.00 ( 91.41)	Acc@5 100.00 ( 99.57)
 * Acc@1 91.410 Acc@5 99.600
### epoch[86] execution time: 46.56930685043335
EPOCH 87
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [87][  0/391]	Time  0.304 ( 0.304)	Data  0.199 ( 0.199)	Loss 2.9266e-02 (2.9266e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [87][ 10/391]	Time  0.106 ( 0.125)	Data  0.001 ( 0.022)	Loss 1.4221e-02 (2.4091e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [87][ 20/391]	Time  0.105 ( 0.117)	Data  0.001 ( 0.013)	Loss 1.3412e-02 (2.3409e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [87][ 30/391]	Time  0.105 ( 0.114)	Data  0.001 ( 0.010)	Loss 2.0615e-02 (2.2313e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [87][ 40/391]	Time  0.107 ( 0.113)	Data  0.001 ( 0.009)	Loss 3.5126e-02 (2.3034e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [87][ 50/391]	Time  0.108 ( 0.112)	Data  0.001 ( 0.008)	Loss 1.0345e-02 (2.2275e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [87][ 60/391]	Time  0.104 ( 0.111)	Data  0.001 ( 0.007)	Loss 9.9869e-03 (2.1911e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [87][ 70/391]	Time  0.107 ( 0.111)	Data  0.001 ( 0.007)	Loss 1.9745e-02 (2.2456e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [87][ 80/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.007)	Loss 3.0548e-02 (2.1767e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [87][ 90/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.1681e-02 (2.1521e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [87][100/391]	Time  0.108 ( 0.110)	Data  0.001 ( 0.006)	Loss 2.3636e-02 (2.1382e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [87][110/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.8234e-02 (2.1398e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [87][120/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 9.7733e-03 (2.1406e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [87][130/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.006)	Loss 1.8188e-02 (2.1417e-02)	Acc@1  98.44 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [87][140/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.006)	Loss 4.2358e-02 (2.1500e-02)	Acc@1  98.44 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [87][150/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.6556e-02 (2.1186e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [87][160/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.3020e-02 (2.1265e-02)	Acc@1  98.44 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [87][170/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.3340e-03 (2.1011e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [87][180/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.0615e-02 (2.0762e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [87][190/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.3468e-02 (2.0934e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [87][200/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0117e-02 (2.0524e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [87][210/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1833e-02 (2.0325e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [87][220/391]	Time  0.112 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.2016e-03 (2.0204e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [87][230/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.3936e-02 (2.0001e-02)	Acc@1  98.44 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [87][240/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.2084e-02 (2.0133e-02)	Acc@1  97.66 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [87][250/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.9640e-03 (2.0228e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [87][260/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3206e-02 (2.0155e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [87][270/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.9241e-02 (2.0076e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [87][280/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.1925e-02 (2.0105e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [87][290/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.8027e-03 (2.0007e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [87][300/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.1052e-02 (1.9929e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [87][310/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.7720e-02 (1.9940e-02)	Acc@1  97.66 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [87][320/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.6144e-02 (2.0006e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [87][330/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.7654e-02 (2.0079e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [87][340/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.3779e-02 (2.0124e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [87][350/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.0765e-02 (2.0153e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [87][360/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.7705e-03 (2.0010e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [87][370/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.6708e-02 (2.0387e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [87][380/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.8681e-03 (2.0297e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [87][390/391]	Time  0.098 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.0304e-02 (2.0231e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
## e[87] optimizer.zero_grad (sum) time: 0.3515279293060303
## e[87]       loss.backward (sum) time: 13.724123001098633
## e[87]      optimizer.step (sum) time: 2.8428173065185547
## epoch[87] training(only) time: 42.72420358657837
# Switched to evaluate mode...
Test: [  0/100]	Time  0.191 ( 0.191)	Loss 3.1226e-01 (3.1226e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.036 ( 0.052)	Loss 2.5098e-01 (3.7724e-01)	Acc@1  94.00 ( 91.55)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.037 ( 0.045)	Loss 5.5273e-01 (4.1343e-01)	Acc@1  89.00 ( 91.29)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.037 ( 0.043)	Loss 4.1382e-01 (4.3945e-01)	Acc@1  91.00 ( 91.26)	Acc@5  99.00 ( 99.52)
Test: [ 40/100]	Time  0.036 ( 0.041)	Loss 2.9346e-01 (4.3371e-01)	Acc@1  92.00 ( 91.17)	Acc@5  99.00 ( 99.51)
Test: [ 50/100]	Time  0.036 ( 0.041)	Loss 2.8271e-01 (4.2585e-01)	Acc@1  91.00 ( 91.22)	Acc@5 100.00 ( 99.51)
Test: [ 60/100]	Time  0.038 ( 0.040)	Loss 5.5566e-01 (4.1927e-01)	Acc@1  93.00 ( 91.28)	Acc@5  99.00 ( 99.54)
Test: [ 70/100]	Time  0.036 ( 0.040)	Loss 6.7188e-01 (4.1302e-01)	Acc@1  88.00 ( 91.27)	Acc@5  99.00 ( 99.56)
Test: [ 80/100]	Time  0.037 ( 0.039)	Loss 2.8101e-01 (4.1230e-01)	Acc@1  92.00 ( 91.30)	Acc@5 100.00 ( 99.56)
Test: [ 90/100]	Time  0.037 ( 0.039)	Loss 2.9565e-01 (4.0545e-01)	Acc@1  94.00 ( 91.30)	Acc@5 100.00 ( 99.56)
 * Acc@1 91.330 Acc@5 99.590
### epoch[87] execution time: 46.74341297149658
EPOCH 88
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [88][  0/391]	Time  0.267 ( 0.267)	Data  0.162 ( 0.162)	Loss 9.7198e-03 (9.7198e-03)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [88][ 10/391]	Time  0.105 ( 0.121)	Data  0.001 ( 0.018)	Loss 1.4702e-02 (1.9017e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [88][ 20/391]	Time  0.106 ( 0.115)	Data  0.001 ( 0.011)	Loss 1.4771e-02 (2.1936e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [88][ 30/391]	Time  0.105 ( 0.113)	Data  0.001 ( 0.009)	Loss 2.2293e-02 (2.0392e-02)	Acc@1  98.44 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [88][ 40/391]	Time  0.104 ( 0.112)	Data  0.001 ( 0.008)	Loss 2.4521e-02 (1.9858e-02)	Acc@1  98.44 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [88][ 50/391]	Time  0.107 ( 0.111)	Data  0.001 ( 0.007)	Loss 3.0518e-02 (1.9742e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [88][ 60/391]	Time  0.106 ( 0.111)	Data  0.001 ( 0.007)	Loss 1.0094e-02 (1.9613e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [88][ 70/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.4076e-02 (1.9578e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [88][ 80/391]	Time  0.103 ( 0.110)	Data  0.001 ( 0.006)	Loss 2.4734e-02 (1.9048e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [88][ 90/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.006)	Loss 1.4145e-02 (1.8597e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [88][100/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.006)	Loss 6.1340e-03 (1.8645e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [88][110/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.006)	Loss 1.6190e-02 (1.9470e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [88][120/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.1310e-02 (2.0112e-02)	Acc@1  97.66 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [88][130/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.5330e-02 (1.9969e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [88][140/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 6.2347e-02 (2.0251e-02)	Acc@1  97.66 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [88][150/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.1988e-02 (1.9917e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [88][160/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.2684e-02 (1.9986e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [88][170/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.0798e-02 (2.0145e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [88][180/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.9917e-02 (2.0403e-02)	Acc@1  97.66 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [88][190/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.3987e-02 (2.0600e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [88][200/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.5248e-02 (2.0700e-02)	Acc@1  98.44 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [88][210/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.6072e-02 (2.0951e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [88][220/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.4180e-02 (2.0933e-02)	Acc@1  98.44 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [88][230/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 4.6997e-02 (2.0999e-02)	Acc@1  97.66 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [88][240/391]	Time  0.107 ( 0.108)	Data  0.001 ( 0.005)	Loss 2.9648e-02 (2.0833e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [88][250/391]	Time  0.104 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.2741e-02 (2.0739e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [88][260/391]	Time  0.105 ( 0.108)	Data  0.001 ( 0.005)	Loss 4.0833e-02 (2.0537e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [88][270/391]	Time  0.105 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.1841e-02 (2.0480e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [88][280/391]	Time  0.108 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.2321e-02 (2.0478e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [88][290/391]	Time  0.105 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.8585e-02 (2.0463e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [88][300/391]	Time  0.106 ( 0.108)	Data  0.001 ( 0.005)	Loss 2.7832e-02 (2.0453e-02)	Acc@1  98.44 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [88][310/391]	Time  0.109 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.0513e-02 (2.0580e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [88][320/391]	Time  0.108 ( 0.108)	Data  0.001 ( 0.005)	Loss 8.5907e-03 (2.0524e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [88][330/391]	Time  0.106 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.0826e-02 (2.0426e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [88][340/391]	Time  0.107 ( 0.108)	Data  0.001 ( 0.005)	Loss 5.4474e-02 (2.0521e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [88][350/391]	Time  0.104 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.1139e-02 (2.0478e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [88][360/391]	Time  0.109 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.2787e-02 (2.0433e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [88][370/391]	Time  0.108 ( 0.108)	Data  0.001 ( 0.005)	Loss 1.8814e-02 (2.0537e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [88][380/391]	Time  0.106 ( 0.108)	Data  0.001 ( 0.005)	Loss 2.7390e-02 (2.0706e-02)	Acc@1  98.44 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [88][390/391]	Time  0.100 ( 0.108)	Data  0.001 ( 0.005)	Loss 4.0092e-03 (2.0851e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
## e[88] optimizer.zero_grad (sum) time: 0.35056138038635254
## e[88]       loss.backward (sum) time: 13.716191530227661
## e[88]      optimizer.step (sum) time: 2.7855191230773926
## epoch[88] training(only) time: 42.48927164077759
# Switched to evaluate mode...
Test: [  0/100]	Time  0.197 ( 0.197)	Loss 3.4351e-01 (3.4351e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.037 ( 0.052)	Loss 2.5415e-01 (3.8608e-01)	Acc@1  94.00 ( 91.36)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.036 ( 0.045)	Loss 5.2979e-01 (4.1961e-01)	Acc@1  88.00 ( 91.33)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.036 ( 0.042)	Loss 4.4019e-01 (4.4906e-01)	Acc@1  91.00 ( 91.23)	Acc@5  99.00 ( 99.48)
Test: [ 40/100]	Time  0.036 ( 0.041)	Loss 2.8491e-01 (4.4282e-01)	Acc@1  92.00 ( 91.15)	Acc@5  99.00 ( 99.51)
Test: [ 50/100]	Time  0.040 ( 0.040)	Loss 2.9297e-01 (4.3534e-01)	Acc@1  92.00 ( 91.14)	Acc@5 100.00 ( 99.47)
Test: [ 60/100]	Time  0.036 ( 0.040)	Loss 5.9424e-01 (4.2863e-01)	Acc@1  93.00 ( 91.28)	Acc@5  99.00 ( 99.51)
Test: [ 70/100]	Time  0.036 ( 0.039)	Loss 6.8457e-01 (4.2234e-01)	Acc@1  89.00 ( 91.30)	Acc@5  99.00 ( 99.54)
Test: [ 80/100]	Time  0.036 ( 0.039)	Loss 2.8833e-01 (4.2083e-01)	Acc@1  92.00 ( 91.33)	Acc@5 100.00 ( 99.54)
Test: [ 90/100]	Time  0.037 ( 0.039)	Loss 3.1006e-01 (4.1376e-01)	Acc@1  94.00 ( 91.34)	Acc@5 100.00 ( 99.54)
 * Acc@1 91.360 Acc@5 99.570
### epoch[88] execution time: 46.43767690658569
EPOCH 89
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [89][  0/391]	Time  0.264 ( 0.264)	Data  0.165 ( 0.165)	Loss 3.3752e-02 (3.3752e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [89][ 10/391]	Time  0.104 ( 0.121)	Data  0.001 ( 0.018)	Loss 4.1275e-03 (2.3200e-02)	Acc@1 100.00 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [89][ 20/391]	Time  0.105 ( 0.114)	Data  0.001 ( 0.012)	Loss 2.2186e-02 (2.1471e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [89][ 30/391]	Time  0.106 ( 0.113)	Data  0.001 ( 0.009)	Loss 1.0864e-02 (1.9788e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [89][ 40/391]	Time  0.107 ( 0.112)	Data  0.001 ( 0.008)	Loss 2.7863e-02 (1.9485e-02)	Acc@1  98.44 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [89][ 50/391]	Time  0.108 ( 0.111)	Data  0.001 ( 0.007)	Loss 1.5732e-02 (1.8315e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [89][ 60/391]	Time  0.108 ( 0.111)	Data  0.001 ( 0.007)	Loss 5.8655e-02 (2.0132e-02)	Acc@1  97.66 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [89][ 70/391]	Time  0.103 ( 0.110)	Data  0.001 ( 0.006)	Loss 3.7750e-02 (2.1191e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [89][ 80/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.2650e-02 (2.1969e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [89][ 90/391]	Time  0.107 ( 0.110)	Data  0.001 ( 0.006)	Loss 8.1177e-02 (2.2522e-02)	Acc@1  97.66 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [89][100/391]	Time  0.109 ( 0.110)	Data  0.001 ( 0.006)	Loss 1.3680e-02 (2.2675e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [89][110/391]	Time  0.106 ( 0.110)	Data  0.001 ( 0.006)	Loss 9.2087e-03 (2.2026e-02)	Acc@1 100.00 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [89][120/391]	Time  0.102 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.7349e-02 (2.1725e-02)	Acc@1  99.22 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [89][130/391]	Time  0.111 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.4932e-02 (2.1473e-02)	Acc@1  97.66 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [89][140/391]	Time  0.107 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.5144e-03 (2.1046e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [89][150/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 8.0566e-03 (2.0945e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [89][160/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.5640e-02 (2.0978e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [89][170/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.2110e-02 (2.1261e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [89][180/391]	Time  0.104 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.8696e-02 (2.1286e-02)	Acc@1  98.44 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [89][190/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.2690e-02 (2.1673e-02)	Acc@1  98.44 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [89][200/391]	Time  0.110 ( 0.109)	Data  0.001 ( 0.005)	Loss 5.3345e-02 (2.1723e-02)	Acc@1  98.44 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [89][210/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.3148e-02 (2.1671e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [89][220/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.9734e-02 (2.2142e-02)	Acc@1  96.88 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [89][230/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.7954e-02 (2.2260e-02)	Acc@1  99.22 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [89][240/391]	Time  0.108 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.4234e-03 (2.2264e-02)	Acc@1 100.00 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [89][250/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.9806e-02 (2.2050e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [89][260/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.4099e-02 (2.2101e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [89][270/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.7075e-02 (2.2297e-02)	Acc@1  99.22 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [89][280/391]	Time  0.105 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.5950e-02 (2.2353e-02)	Acc@1  98.44 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [89][290/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.8114e-03 (2.2281e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [89][300/391]	Time  0.109 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.5238e-02 (2.2266e-02)	Acc@1  99.22 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [89][310/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 9.8419e-03 (2.2196e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [89][320/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 1.8784e-02 (2.2158e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [89][330/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.0981e-02 (2.2075e-02)	Acc@1  98.44 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [89][340/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.7924e-02 (2.1983e-02)	Acc@1  98.44 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [89][350/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.5035e-03 (2.1950e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [89][360/391]	Time  0.099 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.8824e-02 (2.2114e-02)	Acc@1  98.44 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [89][370/391]	Time  0.103 ( 0.109)	Data  0.001 ( 0.005)	Loss 7.2708e-03 (2.2065e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [89][380/391]	Time  0.106 ( 0.109)	Data  0.001 ( 0.005)	Loss 3.7476e-02 (2.2059e-02)	Acc@1  98.44 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [89][390/391]	Time  0.100 ( 0.109)	Data  0.001 ( 0.005)	Loss 2.5620e-02 (2.2355e-02)	Acc@1  98.75 ( 99.24)	Acc@5 100.00 (100.00)
## e[89] optimizer.zero_grad (sum) time: 0.34790658950805664
## e[89]       loss.backward (sum) time: 13.72839903831482
## e[89]      optimizer.step (sum) time: 2.8272016048431396
## epoch[89] training(only) time: 42.63555598258972
# Switched to evaluate mode...
Test: [  0/100]	Time  0.241 ( 0.241)	Loss 3.3984e-01 (3.3984e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.037 ( 0.056)	Loss 2.4280e-01 (3.8900e-01)	Acc@1  94.00 ( 91.09)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.036 ( 0.047)	Loss 5.3857e-01 (4.1867e-01)	Acc@1  90.00 ( 91.10)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.036 ( 0.044)	Loss 4.2896e-01 (4.4182e-01)	Acc@1  91.00 ( 91.10)	Acc@5  99.00 ( 99.48)
Test: [ 40/100]	Time  0.039 ( 0.042)	Loss 3.2495e-01 (4.3908e-01)	Acc@1  92.00 ( 91.00)	Acc@5  99.00 ( 99.51)
Test: [ 50/100]	Time  0.037 ( 0.041)	Loss 2.7832e-01 (4.3047e-01)	Acc@1  91.00 ( 91.08)	Acc@5 100.00 ( 99.49)
Test: [ 60/100]	Time  0.036 ( 0.040)	Loss 5.8398e-01 (4.2419e-01)	Acc@1  93.00 ( 91.16)	Acc@5  99.00 ( 99.52)
Test: [ 70/100]	Time  0.036 ( 0.040)	Loss 6.6895e-01 (4.1773e-01)	Acc@1  88.00 ( 91.20)	Acc@5  99.00 ( 99.55)
Test: [ 80/100]	Time  0.037 ( 0.039)	Loss 2.7612e-01 (4.1634e-01)	Acc@1  92.00 ( 91.26)	Acc@5 100.00 ( 99.56)
Test: [ 90/100]	Time  0.038 ( 0.039)	Loss 3.1494e-01 (4.0925e-01)	Acc@1  94.00 ( 91.27)	Acc@5 100.00 ( 99.55)
 * Acc@1 91.280 Acc@5 99.580
### epoch[89] execution time: 46.633753538131714
### Training complete:
#### total training(only) time: 3840.1418886184692
##### Total run time: 4202.597126960754
