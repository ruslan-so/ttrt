# Model: squeezenet
# Dataset: cifardecem
# Batch size: 128
# Freezeout: False
# Low precision: False
# Epochs: 90
# Initial learning rate: 0.1
# module_name: models_cifardecem.squeezenet
<function squeezenet at 0x7fa8a7bbef28>
# model requested: 'squeezenet'
# printing out the model
SqueezeNet(
  (stem): Sequential(
    (0): Conv2d(3, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fire2): Fire(
    (squeeze): Sequential(
      (0): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (expand_1x1): Sequential(
      (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (expand_3x3): Sequential(
      (0): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
  )
  (fire3): Fire(
    (squeeze): Sequential(
      (0): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (expand_1x1): Sequential(
      (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (expand_3x3): Sequential(
      (0): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
  )
  (fire4): Fire(
    (squeeze): Sequential(
      (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (expand_1x1): Sequential(
      (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (expand_3x3): Sequential(
      (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
  )
  (fire5): Fire(
    (squeeze): Sequential(
      (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (expand_1x1): Sequential(
      (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (expand_3x3): Sequential(
      (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
  )
  (fire6): Fire(
    (squeeze): Sequential(
      (0): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (expand_1x1): Sequential(
      (0): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (expand_3x3): Sequential(
      (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
  )
  (fire7): Fire(
    (squeeze): Sequential(
      (0): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (expand_1x1): Sequential(
      (0): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (expand_3x3): Sequential(
      (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
  )
  (fire8): Fire(
    (squeeze): Sequential(
      (0): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (expand_1x1): Sequential(
      (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (expand_3x3): Sequential(
      (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
  )
  (fire9): Fire(
    (squeeze): Sequential(
      (0): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (expand_1x1): Sequential(
      (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (expand_3x3): Sequential(
      (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
  )
  (conv10): Conv2d(512, 10, kernel_size=(1, 1), stride=(1, 1))
  (avg): AdaptiveAvgPool2d(output_size=1)
  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
# model is full precision
# Model: squeezenet
# Dataset: cifardecem
# Freezeout: False
# Low precision: False
# Initial learning rate: 0.1
# Criterion: CrossEntropyLoss()
# Optimizer: SGD
#	lr 0.1
#	momentum 0.9
#	dampening 0
#	weight_decay 0.0001
#	nesterov False
CIFAR10 loading and normalization
==> Preparing data..
# CIFAR10 / full precision
Files already downloaded and verified
Files already downloaded and verified
#--> Training data: <--#
#-->>
EPOCH 0
# current learning rate: 0.1
# Switched to train mode...
Epoch: [0][  0/391]	Time  3.509 ( 3.509)	Data  0.103 ( 0.103)	Loss 2.3214e+00 (2.3214e+00)	Acc@1   5.47 (  5.47)	Acc@5  50.00 ( 50.00)
Epoch: [0][ 10/391]	Time  0.041 ( 0.357)	Data  0.001 ( 0.010)	Loss 2.2419e+00 (2.2093e+00)	Acc@1  24.22 ( 18.11)	Acc@5  71.88 ( 68.47)
Epoch: [0][ 20/391]	Time  0.039 ( 0.206)	Data  0.001 ( 0.006)	Loss 1.9392e+00 (2.1530e+00)	Acc@1  30.47 ( 19.61)	Acc@5  78.12 ( 72.73)
Epoch: [0][ 30/391]	Time  0.043 ( 0.153)	Data  0.002 ( 0.004)	Loss 1.8292e+00 (2.0740e+00)	Acc@1  25.78 ( 22.00)	Acc@5  82.81 ( 75.93)
Epoch: [0][ 40/391]	Time  0.038 ( 0.125)	Data  0.001 ( 0.004)	Loss 1.9063e+00 (2.0328e+00)	Acc@1  23.44 ( 23.49)	Acc@5  87.50 ( 77.95)
Epoch: [0][ 50/391]	Time  0.043 ( 0.109)	Data  0.001 ( 0.003)	Loss 1.9351e+00 (2.0120e+00)	Acc@1  33.59 ( 24.23)	Acc@5  81.25 ( 78.63)
Epoch: [0][ 60/391]	Time  0.041 ( 0.098)	Data  0.001 ( 0.003)	Loss 1.9043e+00 (1.9902e+00)	Acc@1  30.47 ( 24.85)	Acc@5  82.03 ( 79.64)
Epoch: [0][ 70/391]	Time  0.041 ( 0.090)	Data  0.001 ( 0.003)	Loss 1.9068e+00 (1.9837e+00)	Acc@1  32.03 ( 24.93)	Acc@5  79.69 ( 79.91)
Epoch: [0][ 80/391]	Time  0.041 ( 0.083)	Data  0.001 ( 0.002)	Loss 1.8159e+00 (1.9636e+00)	Acc@1  26.56 ( 25.68)	Acc@5  89.84 ( 80.60)
Epoch: [0][ 90/391]	Time  0.048 ( 0.079)	Data  0.001 ( 0.002)	Loss 1.6892e+00 (1.9412e+00)	Acc@1  37.50 ( 26.61)	Acc@5  87.50 ( 81.27)
Epoch: [0][100/391]	Time  0.041 ( 0.075)	Data  0.002 ( 0.002)	Loss 1.8034e+00 (1.9261e+00)	Acc@1  28.91 ( 27.27)	Acc@5  85.94 ( 81.75)
Epoch: [0][110/391]	Time  0.041 ( 0.072)	Data  0.001 ( 0.002)	Loss 1.8165e+00 (1.9137e+00)	Acc@1  31.25 ( 27.91)	Acc@5  84.38 ( 82.14)
Epoch: [0][120/391]	Time  0.041 ( 0.069)	Data  0.001 ( 0.002)	Loss 1.6339e+00 (1.8961e+00)	Acc@1  39.84 ( 28.42)	Acc@5  89.06 ( 82.57)
Epoch: [0][130/391]	Time  0.043 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5665e+00 (1.8768e+00)	Acc@1  42.19 ( 29.29)	Acc@5  92.19 ( 83.10)
Epoch: [0][140/391]	Time  0.042 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6125e+00 (1.8634e+00)	Acc@1  34.38 ( 29.75)	Acc@5  90.62 ( 83.49)
Epoch: [0][150/391]	Time  0.039 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.5509e+00 (1.8463e+00)	Acc@1  44.53 ( 30.51)	Acc@5  91.41 ( 83.82)
Epoch: [0][160/391]	Time  0.041 ( 0.062)	Data  0.001 ( 0.002)	Loss 1.5902e+00 (1.8313e+00)	Acc@1  36.72 ( 31.07)	Acc@5  92.97 ( 84.28)
Epoch: [0][170/391]	Time  0.043 ( 0.061)	Data  0.001 ( 0.002)	Loss 1.7128e+00 (1.8218e+00)	Acc@1  35.16 ( 31.41)	Acc@5  92.19 ( 84.62)
Epoch: [0][180/391]	Time  0.040 ( 0.060)	Data  0.001 ( 0.002)	Loss 1.4390e+00 (1.8063e+00)	Acc@1  45.31 ( 31.99)	Acc@5  89.84 ( 84.99)
Epoch: [0][190/391]	Time  0.041 ( 0.059)	Data  0.001 ( 0.002)	Loss 1.5697e+00 (1.7946e+00)	Acc@1  38.28 ( 32.44)	Acc@5  90.62 ( 85.30)
Epoch: [0][200/391]	Time  0.040 ( 0.058)	Data  0.001 ( 0.002)	Loss 1.6017e+00 (1.7860e+00)	Acc@1  36.72 ( 32.78)	Acc@5  89.06 ( 85.49)
Epoch: [0][210/391]	Time  0.041 ( 0.057)	Data  0.001 ( 0.002)	Loss 1.5147e+00 (1.7774e+00)	Acc@1  42.19 ( 33.18)	Acc@5  89.84 ( 85.71)
Epoch: [0][220/391]	Time  0.039 ( 0.056)	Data  0.001 ( 0.002)	Loss 1.5925e+00 (1.7646e+00)	Acc@1  46.09 ( 33.70)	Acc@5  90.62 ( 85.97)
Epoch: [0][230/391]	Time  0.039 ( 0.056)	Data  0.001 ( 0.001)	Loss 1.5269e+00 (1.7543e+00)	Acc@1  46.88 ( 34.07)	Acc@5  95.31 ( 86.23)
Epoch: [0][240/391]	Time  0.042 ( 0.055)	Data  0.001 ( 0.001)	Loss 1.4203e+00 (1.7450e+00)	Acc@1  43.75 ( 34.32)	Acc@5  91.41 ( 86.41)
Epoch: [0][250/391]	Time  0.040 ( 0.055)	Data  0.001 ( 0.001)	Loss 1.4992e+00 (1.7365e+00)	Acc@1  44.53 ( 34.75)	Acc@5  91.41 ( 86.57)
Epoch: [0][260/391]	Time  0.038 ( 0.054)	Data  0.001 ( 0.001)	Loss 1.4375e+00 (1.7271e+00)	Acc@1  44.53 ( 35.17)	Acc@5  93.75 ( 86.77)
Epoch: [0][270/391]	Time  0.041 ( 0.054)	Data  0.001 ( 0.001)	Loss 1.4388e+00 (1.7155e+00)	Acc@1  48.44 ( 35.61)	Acc@5  93.75 ( 86.95)
Epoch: [0][280/391]	Time  0.042 ( 0.053)	Data  0.001 ( 0.001)	Loss 1.4344e+00 (1.7048e+00)	Acc@1  46.09 ( 36.07)	Acc@5  92.19 ( 87.15)
Epoch: [0][290/391]	Time  0.040 ( 0.053)	Data  0.001 ( 0.001)	Loss 1.4319e+00 (1.6950e+00)	Acc@1  46.09 ( 36.39)	Acc@5  92.19 ( 87.37)
Epoch: [0][300/391]	Time  0.039 ( 0.052)	Data  0.001 ( 0.001)	Loss 1.3067e+00 (1.6864e+00)	Acc@1  43.75 ( 36.70)	Acc@5  97.66 ( 87.55)
Epoch: [0][310/391]	Time  0.041 ( 0.052)	Data  0.001 ( 0.001)	Loss 1.4788e+00 (1.6773e+00)	Acc@1  42.19 ( 37.02)	Acc@5  94.53 ( 87.70)
Epoch: [0][320/391]	Time  0.050 ( 0.052)	Data  0.002 ( 0.001)	Loss 1.2717e+00 (1.6689e+00)	Acc@1  51.56 ( 37.37)	Acc@5  95.31 ( 87.84)
Epoch: [0][330/391]	Time  0.039 ( 0.051)	Data  0.001 ( 0.001)	Loss 1.2190e+00 (1.6602e+00)	Acc@1  56.25 ( 37.75)	Acc@5  93.75 ( 87.98)
Epoch: [0][340/391]	Time  0.041 ( 0.051)	Data  0.002 ( 0.001)	Loss 1.4657e+00 (1.6520e+00)	Acc@1  51.56 ( 38.13)	Acc@5  90.62 ( 88.12)
Epoch: [0][350/391]	Time  0.041 ( 0.051)	Data  0.002 ( 0.001)	Loss 1.4635e+00 (1.6457e+00)	Acc@1  44.53 ( 38.40)	Acc@5  91.41 ( 88.22)
Epoch: [0][360/391]	Time  0.041 ( 0.050)	Data  0.002 ( 0.001)	Loss 1.3892e+00 (1.6391e+00)	Acc@1  45.31 ( 38.65)	Acc@5  91.41 ( 88.35)
Epoch: [0][370/391]	Time  0.040 ( 0.050)	Data  0.001 ( 0.001)	Loss 1.3049e+00 (1.6318e+00)	Acc@1  45.31 ( 38.96)	Acc@5  96.09 ( 88.47)
Epoch: [0][380/391]	Time  0.041 ( 0.050)	Data  0.001 ( 0.001)	Loss 1.3800e+00 (1.6252e+00)	Acc@1  46.09 ( 39.22)	Acc@5  95.31 ( 88.61)
Epoch: [0][390/391]	Time  0.248 ( 0.050)	Data  0.001 ( 0.001)	Loss 1.4441e+00 (1.6189e+00)	Acc@1  47.50 ( 39.48)	Acc@5  92.50 ( 88.74)
## e[0] optimizer.zero_grad (sum) time: 0.26705408096313477
## e[0]       loss.backward (sum) time: 4.288426399230957
## e[0]      optimizer.step (sum) time: 1.7335126399993896
## epoch[0] training(only) time: 19.599501132965088
# Switched to evaluate mode...
Test: [  0/100]	Time  0.245 ( 0.245)	Loss 1.2724e+00 (1.2724e+00)	Acc@1  55.00 ( 55.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.021 ( 0.041)	Loss 1.1198e+00 (1.4083e+00)	Acc@1  57.00 ( 48.27)	Acc@5  94.00 ( 92.09)
Test: [ 20/100]	Time  0.017 ( 0.032)	Loss 1.5445e+00 (1.3790e+00)	Acc@1  41.00 ( 49.43)	Acc@5  94.00 ( 93.05)
Test: [ 30/100]	Time  0.023 ( 0.028)	Loss 1.2972e+00 (1.4161e+00)	Acc@1  52.00 ( 49.03)	Acc@5  94.00 ( 92.81)
Test: [ 40/100]	Time  0.018 ( 0.027)	Loss 1.4433e+00 (1.4146e+00)	Acc@1  51.00 ( 49.32)	Acc@5  93.00 ( 92.90)
Test: [ 50/100]	Time  0.024 ( 0.026)	Loss 1.2435e+00 (1.4072e+00)	Acc@1  58.00 ( 49.63)	Acc@5  93.00 ( 93.08)
Test: [ 60/100]	Time  0.024 ( 0.025)	Loss 1.5115e+00 (1.4121e+00)	Acc@1  40.00 ( 49.31)	Acc@5  94.00 ( 93.08)
Test: [ 70/100]	Time  0.017 ( 0.024)	Loss 1.3207e+00 (1.4219e+00)	Acc@1  44.00 ( 48.92)	Acc@5  97.00 ( 93.07)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.3955e+00 (1.4225e+00)	Acc@1  47.00 ( 48.99)	Acc@5  93.00 ( 93.06)
Test: [ 90/100]	Time  0.021 ( 0.023)	Loss 1.2735e+00 (1.4253e+00)	Acc@1  55.00 ( 48.91)	Acc@5  96.00 ( 92.99)
 * Acc@1 48.840 Acc@5 92.920
### epoch[0] execution time: 22.017961263656616
EPOCH 1
# current learning rate: 0.1
# Switched to train mode...
Epoch: [1][  0/391]	Time  0.225 ( 0.225)	Data  0.175 ( 0.175)	Loss 1.3282e+00 (1.3282e+00)	Acc@1  50.78 ( 50.78)	Acc@5  93.75 ( 93.75)
Epoch: [1][ 10/391]	Time  0.038 ( 0.057)	Data  0.001 ( 0.017)	Loss 1.2267e+00 (1.3534e+00)	Acc@1  53.12 ( 49.36)	Acc@5  93.75 ( 92.40)
Epoch: [1][ 20/391]	Time  0.041 ( 0.049)	Data  0.001 ( 0.009)	Loss 1.2403e+00 (1.3406e+00)	Acc@1  57.03 ( 50.71)	Acc@5  94.53 ( 93.23)
Epoch: [1][ 30/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.007)	Loss 1.1977e+00 (1.3343e+00)	Acc@1  56.25 ( 51.18)	Acc@5  95.31 ( 93.50)
Epoch: [1][ 40/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.005)	Loss 1.4153e+00 (1.3390e+00)	Acc@1  51.56 ( 51.22)	Acc@5  92.97 ( 93.54)
Epoch: [1][ 50/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.3393e+00 (1.3301e+00)	Acc@1  53.91 ( 51.39)	Acc@5  93.75 ( 93.66)
Epoch: [1][ 60/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.2757e+00 (1.3252e+00)	Acc@1  51.56 ( 51.41)	Acc@5  92.19 ( 93.78)
Epoch: [1][ 70/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.1087e+00 (1.3164e+00)	Acc@1  56.25 ( 51.64)	Acc@5  94.53 ( 93.85)
Epoch: [1][ 80/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.2785e+00 (1.3106e+00)	Acc@1  59.38 ( 51.99)	Acc@5  91.41 ( 93.88)
Epoch: [1][ 90/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.3018e+00 (1.3040e+00)	Acc@1  53.12 ( 52.41)	Acc@5  91.41 ( 93.95)
Epoch: [1][100/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.4240e+00 (1.3008e+00)	Acc@1  54.69 ( 52.59)	Acc@5  90.62 ( 93.97)
Epoch: [1][110/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 1.3607e+00 (1.3021e+00)	Acc@1  52.34 ( 52.48)	Acc@5  92.19 ( 93.94)
Epoch: [1][120/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0313e+00 (1.2972e+00)	Acc@1  61.72 ( 52.65)	Acc@5  97.66 ( 94.05)
Epoch: [1][130/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1923e+00 (1.2879e+00)	Acc@1  56.25 ( 53.07)	Acc@5  95.31 ( 94.09)
Epoch: [1][140/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1764e+00 (1.2836e+00)	Acc@1  58.59 ( 53.29)	Acc@5  95.31 ( 94.14)
Epoch: [1][150/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2266e+00 (1.2800e+00)	Acc@1  55.47 ( 53.41)	Acc@5  95.31 ( 94.13)
Epoch: [1][160/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2285e+00 (1.2751e+00)	Acc@1  57.03 ( 53.55)	Acc@5  92.97 ( 94.17)
Epoch: [1][170/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0391e+00 (1.2695e+00)	Acc@1  67.19 ( 53.71)	Acc@5  97.66 ( 94.22)
Epoch: [1][180/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3242e+00 (1.2658e+00)	Acc@1  50.78 ( 53.86)	Acc@5  95.31 ( 94.24)
Epoch: [1][190/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4044e+00 (1.2623e+00)	Acc@1  50.00 ( 53.95)	Acc@5  96.09 ( 94.26)
Epoch: [1][200/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3036e+00 (1.2611e+00)	Acc@1  50.00 ( 54.02)	Acc@5  92.97 ( 94.25)
Epoch: [1][210/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1058e+00 (1.2566e+00)	Acc@1  61.72 ( 54.24)	Acc@5  96.09 ( 94.26)
Epoch: [1][220/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0330e+00 (1.2523e+00)	Acc@1  61.72 ( 54.41)	Acc@5  97.66 ( 94.32)
Epoch: [1][230/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1852e+00 (1.2448e+00)	Acc@1  55.47 ( 54.72)	Acc@5  94.53 ( 94.39)
Epoch: [1][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1563e+00 (1.2434e+00)	Acc@1  57.81 ( 54.78)	Acc@5  96.09 ( 94.45)
Epoch: [1][250/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1222e+00 (1.2406e+00)	Acc@1  55.47 ( 54.86)	Acc@5  96.88 ( 94.49)
Epoch: [1][260/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1891e+00 (1.2383e+00)	Acc@1  58.59 ( 55.03)	Acc@5  92.97 ( 94.50)
Epoch: [1][270/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0679e+00 (1.2342e+00)	Acc@1  63.28 ( 55.30)	Acc@5  92.97 ( 94.50)
Epoch: [1][280/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1053e+00 (1.2296e+00)	Acc@1  58.59 ( 55.46)	Acc@5  99.22 ( 94.55)
Epoch: [1][290/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1909e+00 (1.2241e+00)	Acc@1  61.72 ( 55.68)	Acc@5  95.31 ( 94.57)
Epoch: [1][300/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0935e+00 (1.2187e+00)	Acc@1  64.06 ( 55.89)	Acc@5  96.88 ( 94.64)
Epoch: [1][310/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.9439e-01 (1.2167e+00)	Acc@1  63.28 ( 56.02)	Acc@5  96.88 ( 94.65)
Epoch: [1][320/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0456e+00 (1.2146e+00)	Acc@1  68.75 ( 56.11)	Acc@5  92.97 ( 94.68)
Epoch: [1][330/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 9.7895e-01 (1.2112e+00)	Acc@1  63.28 ( 56.29)	Acc@5  96.09 ( 94.70)
Epoch: [1][340/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.0349e+00 (1.2082e+00)	Acc@1  58.59 ( 56.37)	Acc@5  95.31 ( 94.72)
Epoch: [1][350/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.0977e+00 (1.2050e+00)	Acc@1  63.28 ( 56.50)	Acc@5  96.09 ( 94.74)
Epoch: [1][360/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.1864e+00 (1.2007e+00)	Acc@1  55.47 ( 56.63)	Acc@5  97.66 ( 94.81)
Epoch: [1][370/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.0088e+00 (1.1966e+00)	Acc@1  67.19 ( 56.79)	Acc@5  97.66 ( 94.86)
Epoch: [1][380/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.0614e+00 (1.1937e+00)	Acc@1  62.50 ( 56.91)	Acc@5  97.66 ( 94.86)
Epoch: [1][390/391]	Time  0.027 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.2616e+00 (1.1922e+00)	Acc@1  60.00 ( 56.97)	Acc@5  92.50 ( 94.88)
## e[1] optimizer.zero_grad (sum) time: 0.2646026611328125
## e[1]       loss.backward (sum) time: 3.9952025413513184
## e[1]      optimizer.step (sum) time: 1.822671890258789
## epoch[1] training(only) time: 16.052438020706177
# Switched to evaluate mode...
Test: [  0/100]	Time  0.223 ( 0.223)	Loss 1.1481e+00 (1.1481e+00)	Acc@1  61.00 ( 61.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.020 ( 0.040)	Loss 9.5689e-01 (1.3324e+00)	Acc@1  69.00 ( 56.82)	Acc@5  97.00 ( 95.00)
Test: [ 20/100]	Time  0.016 ( 0.031)	Loss 1.3187e+00 (1.3775e+00)	Acc@1  57.00 ( 55.90)	Acc@5  99.00 ( 94.76)
Test: [ 30/100]	Time  0.022 ( 0.028)	Loss 1.4880e+00 (1.4381e+00)	Acc@1  57.00 ( 54.81)	Acc@5  94.00 ( 94.32)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.5468e+00 (1.4437e+00)	Acc@1  53.00 ( 54.66)	Acc@5  92.00 ( 94.10)
Test: [ 50/100]	Time  0.017 ( 0.025)	Loss 1.4455e+00 (1.4364e+00)	Acc@1  54.00 ( 54.49)	Acc@5  90.00 ( 94.22)
Test: [ 60/100]	Time  0.022 ( 0.024)	Loss 1.5983e+00 (1.4405e+00)	Acc@1  52.00 ( 54.48)	Acc@5  96.00 ( 94.08)
Test: [ 70/100]	Time  0.021 ( 0.024)	Loss 1.4581e+00 (1.4424e+00)	Acc@1  56.00 ( 54.32)	Acc@5  92.00 ( 94.17)
Test: [ 80/100]	Time  0.022 ( 0.023)	Loss 1.2841e+00 (1.4359e+00)	Acc@1  58.00 ( 54.51)	Acc@5  96.00 ( 94.31)
Test: [ 90/100]	Time  0.018 ( 0.023)	Loss 1.5370e+00 (1.4438e+00)	Acc@1  47.00 ( 54.26)	Acc@5  96.00 ( 94.21)
 * Acc@1 54.430 Acc@5 94.210
### epoch[1] execution time: 18.43163561820984
EPOCH 2
# current learning rate: 0.1
# Switched to train mode...
Epoch: [2][  0/391]	Time  0.215 ( 0.215)	Data  0.169 ( 0.169)	Loss 1.1709e+00 (1.1709e+00)	Acc@1  61.72 ( 61.72)	Acc@5  95.31 ( 95.31)
Epoch: [2][ 10/391]	Time  0.040 ( 0.057)	Data  0.001 ( 0.016)	Loss 1.0163e+00 (1.0386e+00)	Acc@1  66.41 ( 63.14)	Acc@5  96.09 ( 96.09)
Epoch: [2][ 20/391]	Time  0.039 ( 0.049)	Data  0.001 ( 0.009)	Loss 1.0407e+00 (1.0324e+00)	Acc@1  64.84 ( 62.43)	Acc@5  95.31 ( 96.32)
Epoch: [2][ 30/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.006)	Loss 1.1628e+00 (1.0459e+00)	Acc@1  60.94 ( 62.50)	Acc@5  96.88 ( 96.24)
Epoch: [2][ 40/391]	Time  0.039 ( 0.045)	Data  0.001 ( 0.005)	Loss 9.1925e-01 (1.0311e+00)	Acc@1  61.72 ( 63.38)	Acc@5  95.31 ( 96.38)
Epoch: [2][ 50/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.1992e+00 (1.0270e+00)	Acc@1  58.59 ( 63.73)	Acc@5  96.09 ( 96.40)
Epoch: [2][ 60/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.0377e+00 (1.0279e+00)	Acc@1  61.72 ( 63.67)	Acc@5  93.75 ( 96.43)
Epoch: [2][ 70/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 9.4167e-01 (1.0326e+00)	Acc@1  69.53 ( 63.46)	Acc@5  96.88 ( 96.26)
Epoch: [2][ 80/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.1713e+00 (1.0304e+00)	Acc@1  57.81 ( 63.40)	Acc@5  93.75 ( 96.26)
Epoch: [2][ 90/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 9.1417e-01 (1.0215e+00)	Acc@1  67.97 ( 63.72)	Acc@5  96.09 ( 96.21)
Epoch: [2][100/391]	Time  0.051 ( 0.042)	Data  0.001 ( 0.003)	Loss 8.3390e-01 (1.0165e+00)	Acc@1  67.19 ( 63.85)	Acc@5  98.44 ( 96.30)
Epoch: [2][110/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.1552e+00 (1.0197e+00)	Acc@1  62.50 ( 63.79)	Acc@5  95.31 ( 96.27)
Epoch: [2][120/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.0509e+00 (1.0243e+00)	Acc@1  64.06 ( 63.66)	Acc@5  99.22 ( 96.23)
Epoch: [2][130/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.2225e+00 (1.0316e+00)	Acc@1  55.47 ( 63.50)	Acc@5  92.19 ( 96.16)
Epoch: [2][140/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.1394e+00 (1.0331e+00)	Acc@1  64.06 ( 63.52)	Acc@5  92.97 ( 96.10)
Epoch: [2][150/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1371e+00 (1.0318e+00)	Acc@1  60.94 ( 63.59)	Acc@5  95.31 ( 96.13)
Epoch: [2][160/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.9246e-01 (1.0287e+00)	Acc@1  71.88 ( 63.71)	Acc@5  98.44 ( 96.15)
Epoch: [2][170/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1074e+00 (1.0281e+00)	Acc@1  62.50 ( 63.72)	Acc@5  96.88 ( 96.15)
Epoch: [2][180/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0592e+00 (1.0261e+00)	Acc@1  60.94 ( 63.77)	Acc@5  96.09 ( 96.19)
Epoch: [2][190/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.2156e-01 (1.0240e+00)	Acc@1  70.31 ( 63.77)	Acc@5  96.09 ( 96.22)
Epoch: [2][200/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0195e+00 (1.0212e+00)	Acc@1  67.97 ( 63.88)	Acc@5  97.66 ( 96.23)
Epoch: [2][210/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.3750e-01 (1.0174e+00)	Acc@1  69.53 ( 64.04)	Acc@5  99.22 ( 96.24)
Epoch: [2][220/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0098e+00 (1.0162e+00)	Acc@1  66.41 ( 64.09)	Acc@5  96.09 ( 96.25)
Epoch: [2][230/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.3630e-01 (1.0123e+00)	Acc@1  67.19 ( 64.24)	Acc@5  98.44 ( 96.33)
Epoch: [2][240/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1208e+00 (1.0099e+00)	Acc@1  60.94 ( 64.32)	Acc@5  96.88 ( 96.37)
Epoch: [2][250/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.6656e-01 (1.0061e+00)	Acc@1  60.94 ( 64.43)	Acc@5  98.44 ( 96.43)
Epoch: [2][260/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.6890e-01 (1.0023e+00)	Acc@1  71.09 ( 64.56)	Acc@5  98.44 ( 96.46)
Epoch: [2][270/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0843e+00 (1.0029e+00)	Acc@1  62.50 ( 64.57)	Acc@5  92.97 ( 96.45)
Epoch: [2][280/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1740e+00 (1.0006e+00)	Acc@1  56.25 ( 64.66)	Acc@5  96.88 ( 96.49)
Epoch: [2][290/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.8833e-01 (9.9696e-01)	Acc@1  68.75 ( 64.81)	Acc@5  96.88 ( 96.54)
Epoch: [2][300/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.9219e-01 (9.9391e-01)	Acc@1  65.62 ( 64.92)	Acc@5  99.22 ( 96.57)
Epoch: [2][310/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.1665e-01 (9.9263e-01)	Acc@1  71.09 ( 64.96)	Acc@5  95.31 ( 96.56)
Epoch: [2][320/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.0251e+00 (9.9097e-01)	Acc@1  64.84 ( 64.99)	Acc@5  96.09 ( 96.58)
Epoch: [2][330/391]	Time  0.053 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.9933e-01 (9.8884e-01)	Acc@1  74.22 ( 65.08)	Acc@5  98.44 ( 96.59)
Epoch: [2][340/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 8.6090e-01 (9.8780e-01)	Acc@1  71.09 ( 65.13)	Acc@5  96.88 ( 96.60)
Epoch: [2][350/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.6023e-01 (9.8475e-01)	Acc@1  74.22 ( 65.21)	Acc@5  99.22 ( 96.62)
Epoch: [2][360/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 9.1848e-01 (9.8476e-01)	Acc@1  64.06 ( 65.20)	Acc@5  99.22 ( 96.63)
Epoch: [2][370/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 8.7922e-01 (9.8172e-01)	Acc@1  64.06 ( 65.25)	Acc@5  96.88 ( 96.65)
Epoch: [2][380/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.7034e-01 (9.7976e-01)	Acc@1  71.88 ( 65.31)	Acc@5  99.22 ( 96.67)
Epoch: [2][390/391]	Time  0.025 ( 0.041)	Data  0.001 ( 0.001)	Loss 8.8152e-01 (9.7896e-01)	Acc@1  66.25 ( 65.32)	Acc@5  98.75 ( 96.68)
## e[2] optimizer.zero_grad (sum) time: 0.2631709575653076
## e[2]       loss.backward (sum) time: 3.9841179847717285
## e[2]      optimizer.step (sum) time: 1.853963851928711
## epoch[2] training(only) time: 16.014827489852905
# Switched to evaluate mode...
Test: [  0/100]	Time  0.236 ( 0.236)	Loss 8.7849e-01 (8.7849e-01)	Acc@1  71.00 ( 71.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.021 ( 0.039)	Loss 6.8508e-01 (9.4949e-01)	Acc@1  76.00 ( 66.55)	Acc@5  99.00 ( 98.00)
Test: [ 20/100]	Time  0.022 ( 0.031)	Loss 1.1570e+00 (1.0004e+00)	Acc@1  61.00 ( 66.05)	Acc@5  96.00 ( 97.19)
Test: [ 30/100]	Time  0.022 ( 0.028)	Loss 9.9432e-01 (1.0106e+00)	Acc@1  68.00 ( 66.52)	Acc@5  98.00 ( 97.06)
Test: [ 40/100]	Time  0.024 ( 0.027)	Loss 1.0652e+00 (1.0085e+00)	Acc@1  68.00 ( 66.63)	Acc@5  95.00 ( 96.90)
Test: [ 50/100]	Time  0.016 ( 0.025)	Loss 8.9400e-01 (9.9967e-01)	Acc@1  65.00 ( 66.80)	Acc@5  99.00 ( 97.14)
Test: [ 60/100]	Time  0.024 ( 0.025)	Loss 1.0188e+00 (1.0150e+00)	Acc@1  60.00 ( 66.31)	Acc@5  97.00 ( 97.03)
Test: [ 70/100]	Time  0.018 ( 0.024)	Loss 1.0602e+00 (1.0164e+00)	Acc@1  68.00 ( 66.13)	Acc@5  97.00 ( 97.08)
Test: [ 80/100]	Time  0.021 ( 0.024)	Loss 7.9297e-01 (1.0081e+00)	Acc@1  71.00 ( 66.46)	Acc@5  96.00 ( 97.00)
Test: [ 90/100]	Time  0.022 ( 0.023)	Loss 8.3141e-01 (1.0094e+00)	Acc@1  71.00 ( 66.33)	Acc@5  99.00 ( 96.99)
 * Acc@1 66.250 Acc@5 97.000
### epoch[2] execution time: 18.441673040390015
EPOCH 3
# current learning rate: 0.1
# Switched to train mode...
Epoch: [3][  0/391]	Time  0.253 ( 0.253)	Data  0.212 ( 0.212)	Loss 9.5290e-01 (9.5290e-01)	Acc@1  66.41 ( 66.41)	Acc@5  97.66 ( 97.66)
Epoch: [3][ 10/391]	Time  0.040 ( 0.060)	Data  0.001 ( 0.020)	Loss 1.0643e+00 (9.0763e-01)	Acc@1  63.28 ( 67.83)	Acc@5  94.53 ( 96.88)
Epoch: [3][ 20/391]	Time  0.042 ( 0.050)	Data  0.001 ( 0.011)	Loss 7.9828e-01 (8.9923e-01)	Acc@1  71.88 ( 68.19)	Acc@5  99.22 ( 97.36)
Epoch: [3][ 30/391]	Time  0.038 ( 0.046)	Data  0.001 ( 0.008)	Loss 9.6017e-01 (8.9286e-01)	Acc@1  63.28 ( 68.17)	Acc@5  97.66 ( 97.35)
Epoch: [3][ 40/391]	Time  0.038 ( 0.045)	Data  0.001 ( 0.006)	Loss 8.2754e-01 (8.8555e-01)	Acc@1  71.88 ( 68.64)	Acc@5  99.22 ( 97.37)
Epoch: [3][ 50/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.005)	Loss 8.8781e-01 (8.7909e-01)	Acc@1  70.31 ( 68.92)	Acc@5  98.44 ( 97.43)
Epoch: [3][ 60/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.004)	Loss 8.7400e-01 (8.7523e-01)	Acc@1  72.66 ( 68.93)	Acc@5  96.09 ( 97.49)
Epoch: [3][ 70/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.004)	Loss 9.0067e-01 (8.7865e-01)	Acc@1  69.53 ( 68.72)	Acc@5  96.88 ( 97.47)
Epoch: [3][ 80/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 9.2383e-01 (8.7478e-01)	Acc@1  68.75 ( 68.99)	Acc@5  96.09 ( 97.41)
Epoch: [3][ 90/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 9.1430e-01 (8.7821e-01)	Acc@1  66.41 ( 68.88)	Acc@5  96.09 ( 97.37)
Epoch: [3][100/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 8.4633e-01 (8.7513e-01)	Acc@1  71.88 ( 68.90)	Acc@5  97.66 ( 97.43)
Epoch: [3][110/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 8.2465e-01 (8.6990e-01)	Acc@1  69.53 ( 69.02)	Acc@5  99.22 ( 97.46)
Epoch: [3][120/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.0386e+00 (8.6816e-01)	Acc@1  65.62 ( 69.07)	Acc@5  94.53 ( 97.48)
Epoch: [3][130/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 8.8238e-01 (8.6709e-01)	Acc@1  71.09 ( 69.14)	Acc@5  96.09 ( 97.47)
Epoch: [3][140/391]	Time  0.047 ( 0.042)	Data  0.001 ( 0.002)	Loss 9.8496e-01 (8.6931e-01)	Acc@1  64.06 ( 69.07)	Acc@5  95.31 ( 97.41)
Epoch: [3][150/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 9.3256e-01 (8.6619e-01)	Acc@1  70.31 ( 69.26)	Acc@5  98.44 ( 97.45)
Epoch: [3][160/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 8.4425e-01 (8.6529e-01)	Acc@1  71.88 ( 69.38)	Acc@5  96.09 ( 97.42)
Epoch: [3][170/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 8.5900e-01 (8.6616e-01)	Acc@1  70.31 ( 69.37)	Acc@5  96.88 ( 97.40)
Epoch: [3][180/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.5552e-01 (8.6280e-01)	Acc@1  71.09 ( 69.51)	Acc@5 100.00 ( 97.45)
Epoch: [3][190/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.3699e-01 (8.6499e-01)	Acc@1  67.19 ( 69.41)	Acc@5  98.44 ( 97.45)
Epoch: [3][200/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.9939e-01 (8.6113e-01)	Acc@1  81.25 ( 69.62)	Acc@5  99.22 ( 97.47)
Epoch: [3][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.8588e-01 (8.5966e-01)	Acc@1  78.12 ( 69.74)	Acc@5  99.22 ( 97.51)
Epoch: [3][220/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.9321e-01 (8.5789e-01)	Acc@1  67.19 ( 69.81)	Acc@5  97.66 ( 97.53)
Epoch: [3][230/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.4562e-01 (8.5475e-01)	Acc@1  74.22 ( 69.89)	Acc@5  98.44 ( 97.53)
Epoch: [3][240/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.7661e-01 (8.5442e-01)	Acc@1  64.06 ( 69.88)	Acc@5  96.88 ( 97.56)
Epoch: [3][250/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.8885e-01 (8.5295e-01)	Acc@1  75.00 ( 69.94)	Acc@5  99.22 ( 97.55)
Epoch: [3][260/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.0129e-01 (8.5212e-01)	Acc@1  69.53 ( 69.96)	Acc@5  98.44 ( 97.55)
Epoch: [3][270/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.4262e-01 (8.5167e-01)	Acc@1  71.88 ( 69.99)	Acc@5  98.44 ( 97.55)
Epoch: [3][280/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.8760e-01 (8.5097e-01)	Acc@1  64.84 ( 69.98)	Acc@5  98.44 ( 97.59)
Epoch: [3][290/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.1337e-01 (8.4980e-01)	Acc@1  74.22 ( 70.03)	Acc@5  98.44 ( 97.60)
Epoch: [3][300/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.8495e-01 (8.5008e-01)	Acc@1  69.53 ( 70.01)	Acc@5  99.22 ( 97.60)
Epoch: [3][310/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.4327e-01 (8.4914e-01)	Acc@1  75.78 ( 70.08)	Acc@5  96.09 ( 97.60)
Epoch: [3][320/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.8282e-01 (8.4740e-01)	Acc@1  75.00 ( 70.15)	Acc@5  98.44 ( 97.60)
Epoch: [3][330/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.0829e-01 (8.4573e-01)	Acc@1  66.41 ( 70.23)	Acc@5  98.44 ( 97.61)
Epoch: [3][340/391]	Time  0.045 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.8658e-01 (8.4414e-01)	Acc@1  71.09 ( 70.30)	Acc@5  98.44 ( 97.62)
Epoch: [3][350/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.9557e-01 (8.4410e-01)	Acc@1  68.75 ( 70.30)	Acc@5  96.09 ( 97.62)
Epoch: [3][360/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.2059e-01 (8.4258e-01)	Acc@1  70.31 ( 70.35)	Acc@5  99.22 ( 97.65)
Epoch: [3][370/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.7168e-01 (8.4201e-01)	Acc@1  73.44 ( 70.37)	Acc@5  96.88 ( 97.65)
Epoch: [3][380/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.2217e-01 (8.4103e-01)	Acc@1  76.56 ( 70.42)	Acc@5  97.66 ( 97.65)
Epoch: [3][390/391]	Time  0.027 ( 0.041)	Data  0.001 ( 0.001)	Loss 8.0538e-01 (8.3945e-01)	Acc@1  63.75 ( 70.43)	Acc@5  97.50 ( 97.67)
## e[3] optimizer.zero_grad (sum) time: 0.2629687786102295
## e[3]       loss.backward (sum) time: 4.018957138061523
## e[3]      optimizer.step (sum) time: 1.7960636615753174
## epoch[3] training(only) time: 16.134535789489746
# Switched to evaluate mode...
Test: [  0/100]	Time  0.206 ( 0.206)	Loss 1.0323e+00 (1.0323e+00)	Acc@1  65.00 ( 65.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.017 ( 0.035)	Loss 8.5970e-01 (9.8335e-01)	Acc@1  67.00 ( 66.45)	Acc@5  98.00 ( 97.55)
Test: [ 20/100]	Time  0.021 ( 0.028)	Loss 1.0315e+00 (1.0059e+00)	Acc@1  61.00 ( 65.90)	Acc@5 100.00 ( 97.19)
Test: [ 30/100]	Time  0.023 ( 0.026)	Loss 9.7440e-01 (9.9340e-01)	Acc@1  69.00 ( 66.90)	Acc@5  94.00 ( 96.94)
Test: [ 40/100]	Time  0.024 ( 0.025)	Loss 9.4694e-01 (9.9224e-01)	Acc@1  72.00 ( 67.20)	Acc@5  96.00 ( 96.85)
Test: [ 50/100]	Time  0.016 ( 0.024)	Loss 7.5890e-01 (9.7784e-01)	Acc@1  72.00 ( 67.75)	Acc@5  99.00 ( 96.90)
Test: [ 60/100]	Time  0.019 ( 0.024)	Loss 1.0690e+00 (9.8968e-01)	Acc@1  68.00 ( 67.69)	Acc@5  94.00 ( 96.82)
Test: [ 70/100]	Time  0.018 ( 0.023)	Loss 7.2766e-01 (9.9141e-01)	Acc@1  74.00 ( 67.59)	Acc@5  96.00 ( 96.80)
Test: [ 80/100]	Time  0.018 ( 0.023)	Loss 7.9527e-01 (9.8881e-01)	Acc@1  74.00 ( 67.59)	Acc@5  96.00 ( 96.80)
Test: [ 90/100]	Time  0.022 ( 0.022)	Loss 8.5142e-01 (9.9659e-01)	Acc@1  69.00 ( 67.24)	Acc@5  97.00 ( 96.76)
 * Acc@1 67.360 Acc@5 96.790
### epoch[3] execution time: 18.456689834594727
EPOCH 4
# current learning rate: 0.1
# Switched to train mode...
Epoch: [4][  0/391]	Time  0.228 ( 0.228)	Data  0.183 ( 0.183)	Loss 7.9277e-01 (7.9277e-01)	Acc@1  75.00 ( 75.00)	Acc@5  98.44 ( 98.44)
Epoch: [4][ 10/391]	Time  0.039 ( 0.059)	Data  0.001 ( 0.017)	Loss 6.8056e-01 (8.0227e-01)	Acc@1  77.34 ( 72.37)	Acc@5 100.00 ( 98.51)
Epoch: [4][ 20/391]	Time  0.041 ( 0.050)	Data  0.001 ( 0.010)	Loss 7.6969e-01 (7.7953e-01)	Acc@1  72.66 ( 72.02)	Acc@5  98.44 ( 98.44)
Epoch: [4][ 30/391]	Time  0.039 ( 0.047)	Data  0.001 ( 0.007)	Loss 7.4178e-01 (7.9258e-01)	Acc@1  75.78 ( 71.45)	Acc@5  98.44 ( 98.21)
Epoch: [4][ 40/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.005)	Loss 7.8133e-01 (7.8946e-01)	Acc@1  69.53 ( 71.78)	Acc@5  98.44 ( 98.08)
Epoch: [4][ 50/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.005)	Loss 7.8985e-01 (7.7723e-01)	Acc@1  75.00 ( 72.38)	Acc@5  97.66 ( 98.13)
Epoch: [4][ 60/391]	Time  0.040 ( 0.043)	Data  0.002 ( 0.004)	Loss 7.5734e-01 (7.7999e-01)	Acc@1  70.31 ( 72.35)	Acc@5  99.22 ( 98.09)
Epoch: [4][ 70/391]	Time  0.045 ( 0.042)	Data  0.001 ( 0.004)	Loss 7.0569e-01 (7.8185e-01)	Acc@1  75.00 ( 72.35)	Acc@5 100.00 ( 98.13)
Epoch: [4][ 80/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.4813e-01 (7.7548e-01)	Acc@1  71.88 ( 72.46)	Acc@5 100.00 ( 98.23)
Epoch: [4][ 90/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.2442e-01 (7.6946e-01)	Acc@1  78.91 ( 72.87)	Acc@5 100.00 ( 98.21)
Epoch: [4][100/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.0240e+00 (7.7222e-01)	Acc@1  65.62 ( 72.96)	Acc@5  96.09 ( 98.15)
Epoch: [4][110/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.8207e-01 (7.7331e-01)	Acc@1  75.78 ( 72.94)	Acc@5  99.22 ( 98.13)
Epoch: [4][120/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.003)	Loss 8.2221e-01 (7.7122e-01)	Acc@1  75.78 ( 73.10)	Acc@5  96.88 ( 98.16)
Epoch: [4][130/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.5260e-01 (7.7013e-01)	Acc@1  68.75 ( 73.15)	Acc@5  99.22 ( 98.16)
Epoch: [4][140/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.4977e-01 (7.6582e-01)	Acc@1  73.44 ( 73.28)	Acc@5  96.88 ( 98.19)
Epoch: [4][150/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.7094e-01 (7.6638e-01)	Acc@1  72.66 ( 73.27)	Acc@5  97.66 ( 98.17)
Epoch: [4][160/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.0330e-01 (7.6452e-01)	Acc@1  68.75 ( 73.23)	Acc@5  99.22 ( 98.20)
Epoch: [4][170/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.3061e-01 (7.6387e-01)	Acc@1  75.78 ( 73.23)	Acc@5  99.22 ( 98.20)
Epoch: [4][180/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.1402e-01 (7.6127e-01)	Acc@1  78.12 ( 73.33)	Acc@5  93.75 ( 98.16)
Epoch: [4][190/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.4283e-01 (7.5751e-01)	Acc@1  82.81 ( 73.48)	Acc@5  99.22 ( 98.16)
Epoch: [4][200/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.2442e-01 (7.5313e-01)	Acc@1  73.44 ( 73.65)	Acc@5  98.44 ( 98.19)
Epoch: [4][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.7132e-01 (7.5284e-01)	Acc@1  67.97 ( 73.67)	Acc@5  98.44 ( 98.20)
Epoch: [4][220/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.8033e-01 (7.5285e-01)	Acc@1  73.44 ( 73.66)	Acc@5  98.44 ( 98.21)
Epoch: [4][230/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.2184e-01 (7.5151e-01)	Acc@1  73.44 ( 73.65)	Acc@5  99.22 ( 98.21)
Epoch: [4][240/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.6482e-01 (7.4938e-01)	Acc@1  65.62 ( 73.69)	Acc@5  97.66 ( 98.25)
Epoch: [4][250/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.3375e-01 (7.5013e-01)	Acc@1  72.66 ( 73.66)	Acc@5  98.44 ( 98.23)
Epoch: [4][260/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.2687e-01 (7.5272e-01)	Acc@1  70.31 ( 73.61)	Acc@5  97.66 ( 98.20)
Epoch: [4][270/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.5836e-01 (7.5217e-01)	Acc@1  71.09 ( 73.55)	Acc@5 100.00 ( 98.22)
Epoch: [4][280/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.4797e-01 (7.5317e-01)	Acc@1  72.66 ( 73.49)	Acc@5 100.00 ( 98.23)
Epoch: [4][290/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.0860e-01 (7.5331e-01)	Acc@1  71.88 ( 73.48)	Acc@5  99.22 ( 98.24)
Epoch: [4][300/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.0672e-01 (7.5169e-01)	Acc@1  71.88 ( 73.51)	Acc@5  96.88 ( 98.25)
Epoch: [4][310/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.0203e-01 (7.5228e-01)	Acc@1  67.19 ( 73.49)	Acc@5  97.66 ( 98.27)
Epoch: [4][320/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.2324e-01 (7.5117e-01)	Acc@1  75.00 ( 73.53)	Acc@5  98.44 ( 98.27)
Epoch: [4][330/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.7093e-01 (7.4797e-01)	Acc@1  74.22 ( 73.67)	Acc@5  96.88 ( 98.29)
Epoch: [4][340/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.7699e-01 (7.4502e-01)	Acc@1  83.59 ( 73.82)	Acc@5  98.44 ( 98.30)
Epoch: [4][350/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.8362e-01 (7.4553e-01)	Acc@1  73.44 ( 73.83)	Acc@5  99.22 ( 98.30)
Epoch: [4][360/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.001)	Loss 8.0893e-01 (7.4550e-01)	Acc@1  72.66 ( 73.83)	Acc@5  97.66 ( 98.30)
Epoch: [4][370/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.3180e-01 (7.4523e-01)	Acc@1  78.12 ( 73.84)	Acc@5 100.00 ( 98.30)
Epoch: [4][380/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.7610e-01 (7.4580e-01)	Acc@1  76.56 ( 73.86)	Acc@5  98.44 ( 98.29)
Epoch: [4][390/391]	Time  0.027 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.2707e-01 (7.4505e-01)	Acc@1  75.00 ( 73.88)	Acc@5  97.50 ( 98.30)
## e[4] optimizer.zero_grad (sum) time: 0.2630181312561035
## e[4]       loss.backward (sum) time: 3.992281436920166
## e[4]      optimizer.step (sum) time: 1.8386061191558838
## epoch[4] training(only) time: 16.058212757110596
# Switched to evaluate mode...
Test: [  0/100]	Time  0.180 ( 0.180)	Loss 9.5500e-01 (9.5500e-01)	Acc@1  66.00 ( 66.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.018 ( 0.036)	Loss 9.5591e-01 (9.3588e-01)	Acc@1  66.00 ( 67.55)	Acc@5  97.00 ( 97.91)
Test: [ 20/100]	Time  0.018 ( 0.028)	Loss 8.6259e-01 (9.5376e-01)	Acc@1  68.00 ( 66.95)	Acc@5  99.00 ( 97.95)
Test: [ 30/100]	Time  0.017 ( 0.025)	Loss 1.0388e+00 (9.5719e-01)	Acc@1  65.00 ( 67.32)	Acc@5  98.00 ( 97.68)
Test: [ 40/100]	Time  0.017 ( 0.024)	Loss 8.9728e-01 (9.6062e-01)	Acc@1  67.00 ( 67.29)	Acc@5  98.00 ( 97.54)
Test: [ 50/100]	Time  0.017 ( 0.023)	Loss 9.1878e-01 (9.5382e-01)	Acc@1  71.00 ( 67.61)	Acc@5  97.00 ( 97.63)
Test: [ 60/100]	Time  0.023 ( 0.022)	Loss 8.2398e-01 (9.6234e-01)	Acc@1  71.00 ( 67.43)	Acc@5  99.00 ( 97.66)
Test: [ 70/100]	Time  0.021 ( 0.022)	Loss 9.4969e-01 (9.6710e-01)	Acc@1  70.00 ( 67.23)	Acc@5  98.00 ( 97.76)
Test: [ 80/100]	Time  0.020 ( 0.022)	Loss 8.2666e-01 (9.7105e-01)	Acc@1  71.00 ( 67.07)	Acc@5  99.00 ( 97.84)
Test: [ 90/100]	Time  0.022 ( 0.022)	Loss 8.2385e-01 (9.8161e-01)	Acc@1  73.00 ( 66.68)	Acc@5  99.00 ( 97.69)
 * Acc@1 66.650 Acc@5 97.640
### epoch[4] execution time: 18.311280727386475
EPOCH 5
# current learning rate: 0.1
# Switched to train mode...
Epoch: [5][  0/391]	Time  0.218 ( 0.218)	Data  0.172 ( 0.172)	Loss 6.9874e-01 (6.9874e-01)	Acc@1  73.44 ( 73.44)	Acc@5  98.44 ( 98.44)
Epoch: [5][ 10/391]	Time  0.041 ( 0.057)	Data  0.001 ( 0.017)	Loss 7.6905e-01 (6.9084e-01)	Acc@1  78.12 ( 76.21)	Acc@5  97.66 ( 98.30)
Epoch: [5][ 20/391]	Time  0.038 ( 0.049)	Data  0.001 ( 0.009)	Loss 6.0792e-01 (6.9445e-01)	Acc@1  75.00 ( 75.74)	Acc@5 100.00 ( 98.62)
Epoch: [5][ 30/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.007)	Loss 8.5955e-01 (7.1205e-01)	Acc@1  73.44 ( 75.35)	Acc@5  96.88 ( 98.64)
Epoch: [5][ 40/391]	Time  0.043 ( 0.045)	Data  0.001 ( 0.005)	Loss 6.3299e-01 (7.0498e-01)	Acc@1  76.56 ( 75.51)	Acc@5  99.22 ( 98.61)
Epoch: [5][ 50/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.004)	Loss 6.4940e-01 (6.9776e-01)	Acc@1  78.91 ( 75.77)	Acc@5  98.44 ( 98.51)
Epoch: [5][ 60/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.004)	Loss 6.7161e-01 (6.9780e-01)	Acc@1  76.56 ( 75.79)	Acc@5 100.00 ( 98.50)
Epoch: [5][ 70/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.003)	Loss 6.2171e-01 (6.9890e-01)	Acc@1  81.25 ( 75.98)	Acc@5  99.22 ( 98.55)
Epoch: [5][ 80/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.9492e-01 (6.9578e-01)	Acc@1  78.12 ( 76.08)	Acc@5  99.22 ( 98.54)
Epoch: [5][ 90/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.0896e-01 (7.0087e-01)	Acc@1  79.69 ( 75.87)	Acc@5  99.22 ( 98.48)
Epoch: [5][100/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.8956e-01 (6.9857e-01)	Acc@1  74.22 ( 75.89)	Acc@5  98.44 ( 98.45)
Epoch: [5][110/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.2932e-01 (6.9755e-01)	Acc@1  70.31 ( 75.82)	Acc@5  97.66 ( 98.45)
Epoch: [5][120/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.0459e-01 (6.9629e-01)	Acc@1  81.25 ( 75.88)	Acc@5  98.44 ( 98.47)
Epoch: [5][130/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.002)	Loss 7.1772e-01 (6.9905e-01)	Acc@1  78.12 ( 75.80)	Acc@5  97.66 ( 98.38)
Epoch: [5][140/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.2226e-01 (6.9770e-01)	Acc@1  81.25 ( 75.84)	Acc@5 100.00 ( 98.39)
Epoch: [5][150/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.6468e-01 (6.9652e-01)	Acc@1  74.22 ( 75.93)	Acc@5  99.22 ( 98.39)
Epoch: [5][160/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.9927e-01 (6.9361e-01)	Acc@1  81.25 ( 76.04)	Acc@5  97.66 ( 98.43)
Epoch: [5][170/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.8760e-01 (6.9317e-01)	Acc@1  74.22 ( 75.99)	Acc@5  99.22 ( 98.45)
Epoch: [5][180/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.7198e-01 (6.9040e-01)	Acc@1  75.00 ( 76.04)	Acc@5  97.66 ( 98.46)
Epoch: [5][190/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.9921e-01 (6.8877e-01)	Acc@1  80.47 ( 76.18)	Acc@5  99.22 ( 98.45)
Epoch: [5][200/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.8685e-01 (6.8730e-01)	Acc@1  80.47 ( 76.22)	Acc@5 100.00 ( 98.47)
Epoch: [5][210/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.7061e-01 (6.8677e-01)	Acc@1  71.09 ( 76.26)	Acc@5  97.66 ( 98.47)
Epoch: [5][220/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.4504e-01 (6.8444e-01)	Acc@1  72.66 ( 76.32)	Acc@5  98.44 ( 98.48)
Epoch: [5][230/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.1135e-01 (6.8437e-01)	Acc@1  71.09 ( 76.30)	Acc@5  99.22 ( 98.48)
Epoch: [5][240/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.5429e-01 (6.8549e-01)	Acc@1  74.22 ( 76.26)	Acc@5  98.44 ( 98.47)
Epoch: [5][250/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.5395e-01 (6.8521e-01)	Acc@1  75.78 ( 76.29)	Acc@5  99.22 ( 98.48)
Epoch: [5][260/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.6657e-01 (6.8417e-01)	Acc@1  79.69 ( 76.34)	Acc@5  99.22 ( 98.48)
Epoch: [5][270/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.0263e-01 (6.8458e-01)	Acc@1  78.12 ( 76.31)	Acc@5  97.66 ( 98.48)
Epoch: [5][280/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.3758e-01 (6.8374e-01)	Acc@1  78.12 ( 76.27)	Acc@5  98.44 ( 98.51)
Epoch: [5][290/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.1997e-01 (6.8366e-01)	Acc@1  74.22 ( 76.24)	Acc@5  98.44 ( 98.52)
Epoch: [5][300/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.8067e-01 (6.8363e-01)	Acc@1  76.56 ( 76.24)	Acc@5  97.66 ( 98.51)
Epoch: [5][310/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.5360e-01 (6.8262e-01)	Acc@1  74.22 ( 76.30)	Acc@5 100.00 ( 98.51)
Epoch: [5][320/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.6929e-01 (6.8174e-01)	Acc@1  76.56 ( 76.31)	Acc@5  99.22 ( 98.51)
Epoch: [5][330/391]	Time  0.045 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.2572e-01 (6.8320e-01)	Acc@1  71.09 ( 76.29)	Acc@5  98.44 ( 98.48)
Epoch: [5][340/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.6627e-01 (6.8316e-01)	Acc@1  78.91 ( 76.32)	Acc@5  99.22 ( 98.47)
Epoch: [5][350/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.6968e-01 (6.8267e-01)	Acc@1  73.44 ( 76.34)	Acc@5  98.44 ( 98.48)
Epoch: [5][360/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.4460e-01 (6.8195e-01)	Acc@1  76.56 ( 76.38)	Acc@5 100.00 ( 98.49)
Epoch: [5][370/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 8.3650e-01 (6.8286e-01)	Acc@1  72.66 ( 76.36)	Acc@5  99.22 ( 98.48)
Epoch: [5][380/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.8894e-01 (6.8108e-01)	Acc@1  75.00 ( 76.43)	Acc@5 100.00 ( 98.49)
Epoch: [5][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.6958e-01 (6.8028e-01)	Acc@1  76.25 ( 76.44)	Acc@5  98.75 ( 98.49)
## e[5] optimizer.zero_grad (sum) time: 0.2638370990753174
## e[5]       loss.backward (sum) time: 3.9336166381835938
## e[5]      optimizer.step (sum) time: 1.8713245391845703
## epoch[5] training(only) time: 15.970430374145508
# Switched to evaluate mode...
Test: [  0/100]	Time  0.210 ( 0.210)	Loss 7.8711e-01 (7.8711e-01)	Acc@1  74.00 ( 74.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.018 ( 0.039)	Loss 6.6114e-01 (7.8360e-01)	Acc@1  77.00 ( 73.09)	Acc@5 100.00 ( 99.09)
Test: [ 20/100]	Time  0.018 ( 0.030)	Loss 1.0732e+00 (8.0038e-01)	Acc@1  66.00 ( 73.14)	Acc@5  98.00 ( 98.57)
Test: [ 30/100]	Time  0.020 ( 0.027)	Loss 8.2634e-01 (8.2398e-01)	Acc@1  73.00 ( 73.10)	Acc@5  98.00 ( 98.29)
Test: [ 40/100]	Time  0.018 ( 0.026)	Loss 6.1449e-01 (8.2384e-01)	Acc@1  81.00 ( 73.37)	Acc@5  98.00 ( 98.22)
Test: [ 50/100]	Time  0.020 ( 0.025)	Loss 7.0149e-01 (8.1448e-01)	Acc@1  74.00 ( 73.57)	Acc@5  98.00 ( 98.24)
Test: [ 60/100]	Time  0.021 ( 0.024)	Loss 8.0550e-01 (8.1187e-01)	Acc@1  65.00 ( 73.38)	Acc@5  98.00 ( 98.30)
Test: [ 70/100]	Time  0.026 ( 0.024)	Loss 8.6257e-01 (8.1288e-01)	Acc@1  74.00 ( 73.20)	Acc@5  98.00 ( 98.38)
Test: [ 80/100]	Time  0.021 ( 0.024)	Loss 5.9965e-01 (8.1599e-01)	Acc@1  76.00 ( 73.09)	Acc@5  99.00 ( 98.36)
Test: [ 90/100]	Time  0.022 ( 0.023)	Loss 6.5711e-01 (8.1845e-01)	Acc@1  77.00 ( 72.85)	Acc@5 100.00 ( 98.33)
 * Acc@1 73.010 Acc@5 98.340
### epoch[5] execution time: 18.373821020126343
EPOCH 6
# current learning rate: 0.1
# Switched to train mode...
Epoch: [6][  0/391]	Time  0.230 ( 0.230)	Data  0.184 ( 0.184)	Loss 6.9809e-01 (6.9809e-01)	Acc@1  77.34 ( 77.34)	Acc@5  96.88 ( 96.88)
Epoch: [6][ 10/391]	Time  0.041 ( 0.058)	Data  0.001 ( 0.018)	Loss 7.2021e-01 (6.2473e-01)	Acc@1  71.88 ( 78.12)	Acc@5  98.44 ( 98.44)
Epoch: [6][ 20/391]	Time  0.039 ( 0.050)	Data  0.001 ( 0.010)	Loss 6.9419e-01 (6.2548e-01)	Acc@1  75.00 ( 78.20)	Acc@5  98.44 ( 98.62)
Epoch: [6][ 30/391]	Time  0.038 ( 0.047)	Data  0.001 ( 0.007)	Loss 6.7048e-01 (6.2639e-01)	Acc@1  77.34 ( 77.90)	Acc@5  98.44 ( 98.84)
Epoch: [6][ 40/391]	Time  0.039 ( 0.045)	Data  0.001 ( 0.005)	Loss 6.4536e-01 (6.2944e-01)	Acc@1  75.78 ( 77.97)	Acc@5  98.44 ( 98.78)
Epoch: [6][ 50/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.005)	Loss 6.9548e-01 (6.3508e-01)	Acc@1  74.22 ( 77.48)	Acc@5  98.44 ( 98.76)
Epoch: [6][ 60/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.004)	Loss 6.3641e-01 (6.3364e-01)	Acc@1  79.69 ( 77.70)	Acc@5  98.44 ( 98.77)
Epoch: [6][ 70/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 6.6047e-01 (6.3681e-01)	Acc@1  76.56 ( 77.72)	Acc@5  98.44 ( 98.78)
Epoch: [6][ 80/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 6.9800e-01 (6.3458e-01)	Acc@1  71.09 ( 77.80)	Acc@5  99.22 ( 98.78)
Epoch: [6][ 90/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.3901e-01 (6.3052e-01)	Acc@1  74.22 ( 77.78)	Acc@5  98.44 ( 98.81)
Epoch: [6][100/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.5081e-01 (6.2878e-01)	Acc@1  75.00 ( 77.82)	Acc@5  98.44 ( 98.83)
Epoch: [6][110/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.1391e-01 (6.3134e-01)	Acc@1  74.22 ( 77.65)	Acc@5  96.88 ( 98.82)
Epoch: [6][120/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 7.3543e-01 (6.3280e-01)	Acc@1  75.00 ( 77.69)	Acc@5  96.09 ( 98.77)
Epoch: [6][130/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.5695e-01 (6.3250e-01)	Acc@1  80.47 ( 77.70)	Acc@5  99.22 ( 98.80)
Epoch: [6][140/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.3556e-01 (6.3095e-01)	Acc@1  80.47 ( 77.73)	Acc@5  99.22 ( 98.79)
Epoch: [6][150/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7305e-01 (6.2697e-01)	Acc@1  85.16 ( 77.91)	Acc@5  98.44 ( 98.75)
Epoch: [6][160/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.4735e-01 (6.2526e-01)	Acc@1  81.25 ( 77.93)	Acc@5 100.00 ( 98.77)
Epoch: [6][170/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.1040e-01 (6.2360e-01)	Acc@1  72.66 ( 78.00)	Acc@5  97.66 ( 98.79)
Epoch: [6][180/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.7726e-01 (6.2509e-01)	Acc@1  75.00 ( 77.96)	Acc@5 100.00 ( 98.77)
Epoch: [6][190/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9402e-01 (6.2362e-01)	Acc@1  82.81 ( 78.10)	Acc@5 100.00 ( 98.76)
Epoch: [6][200/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.8169e-01 (6.2167e-01)	Acc@1  77.34 ( 78.17)	Acc@5  98.44 ( 98.78)
Epoch: [6][210/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2438e-01 (6.2068e-01)	Acc@1  83.59 ( 78.18)	Acc@5  99.22 ( 98.79)
Epoch: [6][220/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.6100e-01 (6.2109e-01)	Acc@1  75.78 ( 78.19)	Acc@5  98.44 ( 98.78)
Epoch: [6][230/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.3245e-01 (6.1985e-01)	Acc@1  78.91 ( 78.24)	Acc@5  98.44 ( 98.78)
Epoch: [6][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.2713e-01 (6.1953e-01)	Acc@1  75.00 ( 78.26)	Acc@5  99.22 ( 98.79)
Epoch: [6][250/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.7060e-01 (6.1821e-01)	Acc@1  82.03 ( 78.36)	Acc@5  99.22 ( 98.81)
Epoch: [6][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.1202e-01 (6.1821e-01)	Acc@1  76.56 ( 78.38)	Acc@5 100.00 ( 98.82)
Epoch: [6][270/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.3359e-01 (6.1812e-01)	Acc@1  80.47 ( 78.44)	Acc@5  99.22 ( 98.82)
Epoch: [6][280/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.0415e-01 (6.1901e-01)	Acc@1  78.12 ( 78.41)	Acc@5  98.44 ( 98.81)
Epoch: [6][290/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.1092e-01 (6.1811e-01)	Acc@1  79.69 ( 78.46)	Acc@5  99.22 ( 98.81)
Epoch: [6][300/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.0385e-01 (6.1755e-01)	Acc@1  83.59 ( 78.49)	Acc@5  99.22 ( 98.81)
Epoch: [6][310/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2377e-01 (6.1631e-01)	Acc@1  84.38 ( 78.52)	Acc@5  98.44 ( 98.82)
Epoch: [6][320/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.9658e-01 (6.1618e-01)	Acc@1  77.34 ( 78.50)	Acc@5  97.66 ( 98.82)
Epoch: [6][330/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.0358e-01 (6.1490e-01)	Acc@1  78.12 ( 78.57)	Acc@5  96.88 ( 98.81)
Epoch: [6][340/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.3078e-01 (6.1507e-01)	Acc@1  79.69 ( 78.57)	Acc@5  99.22 ( 98.81)
Epoch: [6][350/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.6418e-01 (6.1552e-01)	Acc@1  74.22 ( 78.58)	Acc@5  96.88 ( 98.81)
Epoch: [6][360/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.5043e-01 (6.1589e-01)	Acc@1  78.91 ( 78.56)	Acc@5  97.66 ( 98.82)
Epoch: [6][370/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.6683e-01 (6.1677e-01)	Acc@1  78.12 ( 78.53)	Acc@5  98.44 ( 98.81)
Epoch: [6][380/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.1844e-01 (6.1683e-01)	Acc@1  81.25 ( 78.52)	Acc@5  99.22 ( 98.81)
Epoch: [6][390/391]	Time  0.029 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.5603e-01 (6.1653e-01)	Acc@1  78.75 ( 78.53)	Acc@5  98.75 ( 98.82)
## e[6] optimizer.zero_grad (sum) time: 0.2649047374725342
## e[6]       loss.backward (sum) time: 4.01066780090332
## e[6]      optimizer.step (sum) time: 1.8204264640808105
## epoch[6] training(only) time: 15.990337610244751
# Switched to evaluate mode...
Test: [  0/100]	Time  0.213 ( 0.213)	Loss 6.4052e-01 (6.4052e-01)	Acc@1  79.00 ( 79.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.018 ( 0.039)	Loss 6.1180e-01 (6.9381e-01)	Acc@1  79.00 ( 76.73)	Acc@5  99.00 ( 98.55)
Test: [ 20/100]	Time  0.024 ( 0.031)	Loss 8.8648e-01 (7.2091e-01)	Acc@1  71.00 ( 75.95)	Acc@5  99.00 ( 98.52)
Test: [ 30/100]	Time  0.024 ( 0.028)	Loss 6.7344e-01 (7.3129e-01)	Acc@1  78.00 ( 76.42)	Acc@5  98.00 ( 98.29)
Test: [ 40/100]	Time  0.021 ( 0.027)	Loss 6.0887e-01 (7.4323e-01)	Acc@1  82.00 ( 76.00)	Acc@5  99.00 ( 98.29)
Test: [ 50/100]	Time  0.023 ( 0.026)	Loss 6.4348e-01 (7.3230e-01)	Acc@1  76.00 ( 76.22)	Acc@5  98.00 ( 98.27)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 9.1318e-01 (7.3644e-01)	Acc@1  68.00 ( 75.84)	Acc@5 100.00 ( 98.36)
Test: [ 70/100]	Time  0.024 ( 0.025)	Loss 5.8633e-01 (7.3799e-01)	Acc@1  81.00 ( 75.66)	Acc@5  99.00 ( 98.38)
Test: [ 80/100]	Time  0.017 ( 0.024)	Loss 7.7410e-01 (7.3962e-01)	Acc@1  74.00 ( 75.52)	Acc@5  97.00 ( 98.35)
Test: [ 90/100]	Time  0.021 ( 0.024)	Loss 8.1790e-01 (7.4341e-01)	Acc@1  68.00 ( 75.45)	Acc@5 100.00 ( 98.35)
 * Acc@1 75.490 Acc@5 98.360
### epoch[6] execution time: 18.477352142333984
EPOCH 7
# current learning rate: 0.1
# Switched to train mode...
Epoch: [7][  0/391]	Time  0.199 ( 0.199)	Data  0.151 ( 0.151)	Loss 5.3537e-01 (5.3537e-01)	Acc@1  81.25 ( 81.25)	Acc@5  99.22 ( 99.22)
Epoch: [7][ 10/391]	Time  0.038 ( 0.057)	Data  0.001 ( 0.015)	Loss 5.9958e-01 (5.9472e-01)	Acc@1  81.25 ( 79.83)	Acc@5  97.66 ( 99.01)
Epoch: [7][ 20/391]	Time  0.041 ( 0.049)	Data  0.001 ( 0.008)	Loss 6.7298e-01 (6.0777e-01)	Acc@1  78.91 ( 79.13)	Acc@5  96.88 ( 99.00)
Epoch: [7][ 30/391]	Time  0.043 ( 0.046)	Data  0.001 ( 0.006)	Loss 6.1133e-01 (6.1829e-01)	Acc@1  82.03 ( 78.81)	Acc@5  97.66 ( 98.79)
Epoch: [7][ 40/391]	Time  0.039 ( 0.045)	Data  0.001 ( 0.005)	Loss 4.7782e-01 (6.1155e-01)	Acc@1  82.03 ( 79.08)	Acc@5 100.00 ( 98.88)
Epoch: [7][ 50/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.004)	Loss 5.2728e-01 (6.0523e-01)	Acc@1  82.03 ( 79.30)	Acc@5  99.22 ( 98.88)
Epoch: [7][ 60/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 6.1370e-01 (5.9724e-01)	Acc@1  77.34 ( 79.60)	Acc@5  98.44 ( 98.82)
Epoch: [7][ 70/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.003)	Loss 6.6761e-01 (5.9145e-01)	Acc@1  74.22 ( 79.71)	Acc@5  99.22 ( 98.82)
Epoch: [7][ 80/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.8062e-01 (5.8542e-01)	Acc@1  83.59 ( 79.87)	Acc@5  98.44 ( 98.83)
Epoch: [7][ 90/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.6686e-01 (5.8620e-01)	Acc@1  73.44 ( 79.91)	Acc@5  98.44 ( 98.82)
Epoch: [7][100/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.4906e-01 (5.8817e-01)	Acc@1  79.69 ( 79.70)	Acc@5  99.22 ( 98.85)
Epoch: [7][110/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.6750e-01 (5.8859e-01)	Acc@1  76.56 ( 79.63)	Acc@5  96.88 ( 98.83)
Epoch: [7][120/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.5571e-01 (5.8691e-01)	Acc@1  83.59 ( 79.76)	Acc@5 100.00 ( 98.82)
Epoch: [7][130/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.3640e-01 (5.8583e-01)	Acc@1  82.03 ( 79.78)	Acc@5  99.22 ( 98.84)
Epoch: [7][140/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.1164e-01 (5.8449e-01)	Acc@1  81.25 ( 79.84)	Acc@5 100.00 ( 98.87)
Epoch: [7][150/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.4586e-01 (5.8489e-01)	Acc@1  78.91 ( 79.81)	Acc@5  99.22 ( 98.86)
Epoch: [7][160/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.1518e-01 (5.8353e-01)	Acc@1  75.78 ( 79.89)	Acc@5  98.44 ( 98.86)
Epoch: [7][170/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.2952e-01 (5.8314e-01)	Acc@1  82.03 ( 79.87)	Acc@5  98.44 ( 98.89)
Epoch: [7][180/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2446e-01 (5.8466e-01)	Acc@1  79.69 ( 79.73)	Acc@5  99.22 ( 98.91)
Epoch: [7][190/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.1615e-01 (5.8614e-01)	Acc@1  66.41 ( 79.71)	Acc@5  96.09 ( 98.87)
Epoch: [7][200/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.9674e-01 (5.8614e-01)	Acc@1  75.00 ( 79.74)	Acc@5  99.22 ( 98.88)
Epoch: [7][210/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.6192e-01 (5.8518e-01)	Acc@1  81.25 ( 79.74)	Acc@5  99.22 ( 98.88)
Epoch: [7][220/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7183e-01 (5.8531e-01)	Acc@1  84.38 ( 79.77)	Acc@5 100.00 ( 98.88)
Epoch: [7][230/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.4578e-01 (5.8431e-01)	Acc@1  82.03 ( 79.80)	Acc@5 100.00 ( 98.89)
Epoch: [7][240/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0992e-01 (5.8502e-01)	Acc@1  82.81 ( 79.80)	Acc@5 100.00 ( 98.88)
Epoch: [7][250/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.6629e-01 (5.8272e-01)	Acc@1  84.38 ( 79.86)	Acc@5 100.00 ( 98.90)
Epoch: [7][260/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.3213e-01 (5.8171e-01)	Acc@1  80.47 ( 79.93)	Acc@5 100.00 ( 98.88)
Epoch: [7][270/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.2571e-01 (5.8200e-01)	Acc@1  77.34 ( 79.90)	Acc@5 100.00 ( 98.89)
Epoch: [7][280/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.0891e-01 (5.8244e-01)	Acc@1  76.56 ( 79.91)	Acc@5  97.66 ( 98.88)
Epoch: [7][290/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.5273e-01 (5.8166e-01)	Acc@1  78.91 ( 79.92)	Acc@5  99.22 ( 98.87)
Epoch: [7][300/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.0357e-01 (5.8032e-01)	Acc@1  74.22 ( 79.97)	Acc@5  97.66 ( 98.88)
Epoch: [7][310/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.8925e-01 (5.8004e-01)	Acc@1  79.69 ( 80.00)	Acc@5 100.00 ( 98.89)
Epoch: [7][320/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.6416e-01 (5.7870e-01)	Acc@1  83.59 ( 80.07)	Acc@5  98.44 ( 98.90)
Epoch: [7][330/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.3488e-01 (5.7821e-01)	Acc@1  84.38 ( 80.07)	Acc@5 100.00 ( 98.90)
Epoch: [7][340/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.2142e-01 (5.7682e-01)	Acc@1  82.03 ( 80.11)	Acc@5  99.22 ( 98.91)
Epoch: [7][350/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.5916e-01 (5.7773e-01)	Acc@1  77.34 ( 80.11)	Acc@5 100.00 ( 98.90)
Epoch: [7][360/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.0223e-01 (5.7713e-01)	Acc@1  75.00 ( 80.09)	Acc@5 100.00 ( 98.90)
Epoch: [7][370/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.4991e-01 (5.7731e-01)	Acc@1  80.47 ( 80.10)	Acc@5  97.66 ( 98.90)
Epoch: [7][380/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.6746e-01 (5.7684e-01)	Acc@1  88.28 ( 80.11)	Acc@5 100.00 ( 98.90)
Epoch: [7][390/391]	Time  0.028 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.4547e-01 (5.7592e-01)	Acc@1  85.00 ( 80.13)	Acc@5 100.00 ( 98.91)
## e[7] optimizer.zero_grad (sum) time: 0.2628805637359619
## e[7]       loss.backward (sum) time: 3.9964473247528076
## e[7]      optimizer.step (sum) time: 1.7689237594604492
## epoch[7] training(only) time: 16.057020664215088
# Switched to evaluate mode...
Test: [  0/100]	Time  0.177 ( 0.177)	Loss 7.7023e-01 (7.7023e-01)	Acc@1  76.00 ( 76.00)	Acc@5  97.00 ( 97.00)
Test: [ 10/100]	Time  0.018 ( 0.036)	Loss 6.7781e-01 (6.6603e-01)	Acc@1  77.00 ( 77.00)	Acc@5  99.00 ( 98.55)
Test: [ 20/100]	Time  0.021 ( 0.029)	Loss 8.3571e-01 (6.9103e-01)	Acc@1  72.00 ( 75.71)	Acc@5  98.00 ( 98.52)
Test: [ 30/100]	Time  0.017 ( 0.026)	Loss 6.5034e-01 (6.8292e-01)	Acc@1  78.00 ( 76.45)	Acc@5  99.00 ( 98.55)
Test: [ 40/100]	Time  0.021 ( 0.024)	Loss 7.3551e-01 (6.8043e-01)	Acc@1  79.00 ( 76.80)	Acc@5  99.00 ( 98.56)
Test: [ 50/100]	Time  0.018 ( 0.023)	Loss 6.0573e-01 (6.7825e-01)	Acc@1  81.00 ( 77.06)	Acc@5  98.00 ( 98.51)
Test: [ 60/100]	Time  0.018 ( 0.023)	Loss 5.7230e-01 (6.8068e-01)	Acc@1  77.00 ( 76.89)	Acc@5 100.00 ( 98.52)
Test: [ 70/100]	Time  0.020 ( 0.023)	Loss 7.3554e-01 (6.8541e-01)	Acc@1  76.00 ( 76.61)	Acc@5  97.00 ( 98.54)
Test: [ 80/100]	Time  0.018 ( 0.022)	Loss 4.4948e-01 (6.7843e-01)	Acc@1  82.00 ( 76.72)	Acc@5 100.00 ( 98.56)
Test: [ 90/100]	Time  0.022 ( 0.022)	Loss 4.1240e-01 (6.8055e-01)	Acc@1  85.00 ( 76.79)	Acc@5  99.00 ( 98.52)
 * Acc@1 76.850 Acc@5 98.430
### epoch[7] execution time: 18.350451946258545
EPOCH 8
# current learning rate: 0.1
# Switched to train mode...
Epoch: [8][  0/391]	Time  0.258 ( 0.258)	Data  0.217 ( 0.217)	Loss 4.9383e-01 (4.9383e-01)	Acc@1  82.81 ( 82.81)	Acc@5  99.22 ( 99.22)
Epoch: [8][ 10/391]	Time  0.038 ( 0.060)	Data  0.001 ( 0.021)	Loss 6.2899e-01 (5.3175e-01)	Acc@1  78.12 ( 82.88)	Acc@5  98.44 ( 99.22)
Epoch: [8][ 20/391]	Time  0.040 ( 0.051)	Data  0.001 ( 0.011)	Loss 5.0518e-01 (5.5015e-01)	Acc@1  79.69 ( 81.14)	Acc@5  98.44 ( 99.03)
Epoch: [8][ 30/391]	Time  0.039 ( 0.048)	Data  0.001 ( 0.008)	Loss 5.4009e-01 (5.5305e-01)	Acc@1  81.25 ( 81.22)	Acc@5  99.22 ( 98.84)
Epoch: [8][ 40/391]	Time  0.038 ( 0.046)	Data  0.001 ( 0.006)	Loss 4.9096e-01 (5.6059e-01)	Acc@1  82.81 ( 81.14)	Acc@5  99.22 ( 98.84)
Epoch: [8][ 50/391]	Time  0.039 ( 0.045)	Data  0.001 ( 0.005)	Loss 5.9197e-01 (5.5674e-01)	Acc@1  78.91 ( 81.16)	Acc@5  98.44 ( 98.90)
Epoch: [8][ 60/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.004)	Loss 5.3977e-01 (5.5342e-01)	Acc@1  83.59 ( 81.13)	Acc@5 100.00 ( 98.96)
Epoch: [8][ 70/391]	Time  0.043 ( 0.043)	Data  0.001 ( 0.004)	Loss 6.6250e-01 (5.5205e-01)	Acc@1  74.22 ( 81.05)	Acc@5  97.66 ( 99.03)
Epoch: [8][ 80/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.004)	Loss 6.6553e-01 (5.4732e-01)	Acc@1  73.44 ( 81.19)	Acc@5  98.44 ( 99.07)
Epoch: [8][ 90/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 6.4554e-01 (5.4784e-01)	Acc@1  76.56 ( 81.22)	Acc@5  99.22 ( 99.08)
Epoch: [8][100/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.3184e-01 (5.4856e-01)	Acc@1  78.12 ( 81.23)	Acc@5  95.31 ( 99.07)
Epoch: [8][110/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.4296e-01 (5.5076e-01)	Acc@1  82.81 ( 81.24)	Acc@5  99.22 ( 99.06)
Epoch: [8][120/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.1595e-01 (5.5073e-01)	Acc@1  79.69 ( 81.19)	Acc@5 100.00 ( 99.06)
Epoch: [8][130/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.0782e-01 (5.4933e-01)	Acc@1  78.12 ( 81.17)	Acc@5  99.22 ( 99.07)
Epoch: [8][140/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.0640e-01 (5.5012e-01)	Acc@1  78.91 ( 81.15)	Acc@5  98.44 ( 99.06)
Epoch: [8][150/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 7.0295e-01 (5.4911e-01)	Acc@1  75.00 ( 81.14)	Acc@5  97.66 ( 99.05)
Epoch: [8][160/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.9520e-01 (5.4887e-01)	Acc@1  86.72 ( 81.24)	Acc@5  97.66 ( 99.05)
Epoch: [8][170/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0210e-01 (5.4779e-01)	Acc@1  82.03 ( 81.32)	Acc@5 100.00 ( 99.05)
Epoch: [8][180/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4278e-01 (5.4810e-01)	Acc@1  85.94 ( 81.23)	Acc@5  99.22 ( 99.05)
Epoch: [8][190/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0323e-01 (5.4824e-01)	Acc@1  82.03 ( 81.23)	Acc@5 100.00 ( 99.05)
Epoch: [8][200/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.4393e-01 (5.4690e-01)	Acc@1  79.69 ( 81.29)	Acc@5  98.44 ( 99.05)
Epoch: [8][210/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.8122e-01 (5.4714e-01)	Acc@1  81.25 ( 81.30)	Acc@5  97.66 ( 99.04)
Epoch: [8][220/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.2280e-01 (5.4619e-01)	Acc@1  82.03 ( 81.33)	Acc@5  99.22 ( 99.05)
Epoch: [8][230/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.6379e-01 (5.4581e-01)	Acc@1  80.47 ( 81.31)	Acc@5 100.00 ( 99.07)
Epoch: [8][240/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.3429e-01 (5.4620e-01)	Acc@1  76.56 ( 81.27)	Acc@5  99.22 ( 99.06)
Epoch: [8][250/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2148e-01 (5.4563e-01)	Acc@1  86.72 ( 81.24)	Acc@5 100.00 ( 99.06)
Epoch: [8][260/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.8106e-01 (5.4658e-01)	Acc@1  80.47 ( 81.16)	Acc@5 100.00 ( 99.08)
Epoch: [8][270/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4989e-01 (5.4540e-01)	Acc@1  88.28 ( 81.20)	Acc@5 100.00 ( 99.07)
Epoch: [8][280/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2327e-01 (5.4512e-01)	Acc@1  80.47 ( 81.19)	Acc@5  99.22 ( 99.09)
Epoch: [8][290/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.7333e-01 (5.4419e-01)	Acc@1  77.34 ( 81.21)	Acc@5  99.22 ( 99.08)
Epoch: [8][300/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4362e-01 (5.4361e-01)	Acc@1  83.59 ( 81.23)	Acc@5  99.22 ( 99.09)
Epoch: [8][310/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.0036e-01 (5.4368e-01)	Acc@1  72.66 ( 81.18)	Acc@5 100.00 ( 99.09)
Epoch: [8][320/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2373e-01 (5.4235e-01)	Acc@1  85.16 ( 81.19)	Acc@5  99.22 ( 99.10)
Epoch: [8][330/391]	Time  0.040 ( 0.041)	Data  0.002 ( 0.002)	Loss 6.6925e-01 (5.4357e-01)	Acc@1  73.44 ( 81.11)	Acc@5  99.22 ( 99.10)
Epoch: [8][340/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.5117e-01 (5.4258e-01)	Acc@1  82.03 ( 81.17)	Acc@5  99.22 ( 99.10)
Epoch: [8][350/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.8246e-01 (5.4300e-01)	Acc@1  78.91 ( 81.15)	Acc@5  98.44 ( 99.11)
Epoch: [8][360/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4135e-01 (5.4232e-01)	Acc@1  82.81 ( 81.17)	Acc@5  99.22 ( 99.11)
Epoch: [8][370/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9267e-01 (5.4179e-01)	Acc@1  83.59 ( 81.17)	Acc@5  99.22 ( 99.12)
Epoch: [8][380/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.4230e-01 (5.4051e-01)	Acc@1  76.56 ( 81.20)	Acc@5 100.00 ( 99.12)
Epoch: [8][390/391]	Time  0.028 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9801e-01 (5.4112e-01)	Acc@1  86.25 ( 81.18)	Acc@5 100.00 ( 99.12)
## e[8] optimizer.zero_grad (sum) time: 0.2647974491119385
## e[8]       loss.backward (sum) time: 4.002844333648682
## e[8]      optimizer.step (sum) time: 1.8483552932739258
## epoch[8] training(only) time: 16.03217077255249
# Switched to evaluate mode...
Test: [  0/100]	Time  0.172 ( 0.172)	Loss 6.8437e-01 (6.8437e-01)	Acc@1  80.00 ( 80.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.020 ( 0.034)	Loss 6.6668e-01 (6.8794e-01)	Acc@1  81.00 ( 78.82)	Acc@5  99.00 ( 98.82)
Test: [ 20/100]	Time  0.018 ( 0.027)	Loss 1.0893e+00 (7.0213e-01)	Acc@1  68.00 ( 77.95)	Acc@5  99.00 ( 98.76)
Test: [ 30/100]	Time  0.021 ( 0.025)	Loss 7.6038e-01 (7.0161e-01)	Acc@1  77.00 ( 77.87)	Acc@5  96.00 ( 98.55)
Test: [ 40/100]	Time  0.022 ( 0.024)	Loss 6.5865e-01 (7.0412e-01)	Acc@1  84.00 ( 77.56)	Acc@5  99.00 ( 98.51)
Test: [ 50/100]	Time  0.020 ( 0.023)	Loss 5.1557e-01 (6.9672e-01)	Acc@1  84.00 ( 78.08)	Acc@5  99.00 ( 98.47)
Test: [ 60/100]	Time  0.018 ( 0.023)	Loss 6.2951e-01 (6.9445e-01)	Acc@1  77.00 ( 78.05)	Acc@5  98.00 ( 98.48)
Test: [ 70/100]	Time  0.022 ( 0.022)	Loss 5.7744e-01 (6.9575e-01)	Acc@1  80.00 ( 78.00)	Acc@5 100.00 ( 98.54)
Test: [ 80/100]	Time  0.023 ( 0.022)	Loss 5.3475e-01 (6.9212e-01)	Acc@1  82.00 ( 77.94)	Acc@5 100.00 ( 98.56)
Test: [ 90/100]	Time  0.023 ( 0.022)	Loss 5.3817e-01 (6.9670e-01)	Acc@1  83.00 ( 77.74)	Acc@5  99.00 ( 98.52)
 * Acc@1 77.810 Acc@5 98.590
### epoch[8] execution time: 18.393336057662964
EPOCH 9
# current learning rate: 0.1
# Switched to train mode...
Epoch: [9][  0/391]	Time  0.212 ( 0.212)	Data  0.171 ( 0.171)	Loss 5.1050e-01 (5.1050e-01)	Acc@1  82.81 ( 82.81)	Acc@5  99.22 ( 99.22)
Epoch: [9][ 10/391]	Time  0.041 ( 0.056)	Data  0.001 ( 0.016)	Loss 4.0423e-01 (5.2889e-01)	Acc@1  88.28 ( 82.88)	Acc@5 100.00 ( 98.93)
Epoch: [9][ 20/391]	Time  0.039 ( 0.049)	Data  0.001 ( 0.009)	Loss 4.8605e-01 (5.1905e-01)	Acc@1  82.03 ( 82.85)	Acc@5  99.22 ( 99.00)
Epoch: [9][ 30/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.006)	Loss 6.1419e-01 (5.1536e-01)	Acc@1  78.12 ( 82.61)	Acc@5  98.44 ( 99.04)
Epoch: [9][ 40/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.005)	Loss 2.8297e-01 (5.1430e-01)	Acc@1  91.41 ( 82.83)	Acc@5 100.00 ( 99.05)
Epoch: [9][ 50/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.004)	Loss 5.6967e-01 (5.0673e-01)	Acc@1  79.69 ( 83.01)	Acc@5  98.44 ( 99.14)
Epoch: [9][ 60/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.004)	Loss 5.9036e-01 (5.0442e-01)	Acc@1  79.69 ( 82.99)	Acc@5  98.44 ( 99.14)
Epoch: [9][ 70/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 5.8862e-01 (5.0266e-01)	Acc@1  78.91 ( 82.89)	Acc@5  97.66 ( 99.15)
Epoch: [9][ 80/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.9819e-01 (4.9876e-01)	Acc@1  81.25 ( 82.94)	Acc@5  99.22 ( 99.12)
Epoch: [9][ 90/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.2514e-01 (5.0257e-01)	Acc@1  82.81 ( 82.81)	Acc@5  99.22 ( 99.13)
Epoch: [9][100/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.9294e-01 (5.0071e-01)	Acc@1  90.62 ( 82.86)	Acc@5 100.00 ( 99.16)
Epoch: [9][110/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.3625e-01 (5.0017e-01)	Acc@1  81.25 ( 82.79)	Acc@5 100.00 ( 99.20)
Epoch: [9][120/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.4261e-01 (5.0374e-01)	Acc@1  82.81 ( 82.66)	Acc@5  98.44 ( 99.18)
Epoch: [9][130/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.9335e-01 (5.0907e-01)	Acc@1  83.59 ( 82.48)	Acc@5  99.22 ( 99.17)
Epoch: [9][140/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.4327e-01 (5.0734e-01)	Acc@1  82.81 ( 82.59)	Acc@5 100.00 ( 99.17)
Epoch: [9][150/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2342e-01 (5.0821e-01)	Acc@1  83.59 ( 82.54)	Acc@5  98.44 ( 99.19)
Epoch: [9][160/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.7149e-01 (5.0908e-01)	Acc@1  83.59 ( 82.54)	Acc@5  97.66 ( 99.19)
Epoch: [9][170/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.6472e-01 (5.0958e-01)	Acc@1  84.38 ( 82.52)	Acc@5 100.00 ( 99.21)
Epoch: [9][180/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2929e-01 (5.0795e-01)	Acc@1  87.50 ( 82.53)	Acc@5 100.00 ( 99.21)
Epoch: [9][190/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.6285e-01 (5.0819e-01)	Acc@1  78.91 ( 82.56)	Acc@5  98.44 ( 99.20)
Epoch: [9][200/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1259e-01 (5.0983e-01)	Acc@1  81.25 ( 82.50)	Acc@5 100.00 ( 99.20)
Epoch: [9][210/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.5998e-01 (5.0918e-01)	Acc@1  82.03 ( 82.52)	Acc@5  99.22 ( 99.21)
Epoch: [9][220/391]	Time  0.047 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.3650e-01 (5.1009e-01)	Acc@1  83.59 ( 82.51)	Acc@5  98.44 ( 99.18)
Epoch: [9][230/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.9705e-01 (5.1086e-01)	Acc@1  77.34 ( 82.46)	Acc@5  99.22 ( 99.18)
Epoch: [9][240/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.7642e-01 (5.1006e-01)	Acc@1  77.34 ( 82.45)	Acc@5  98.44 ( 99.17)
Epoch: [9][250/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.9350e-01 (5.1011e-01)	Acc@1  78.91 ( 82.45)	Acc@5 100.00 ( 99.18)
Epoch: [9][260/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.6672e-01 (5.1055e-01)	Acc@1  79.69 ( 82.45)	Acc@5 100.00 ( 99.18)
Epoch: [9][270/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.2243e-01 (5.1151e-01)	Acc@1  77.34 ( 82.42)	Acc@5  96.88 ( 99.16)
Epoch: [9][280/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3009e-01 (5.1167e-01)	Acc@1  83.59 ( 82.40)	Acc@5  99.22 ( 99.17)
Epoch: [9][290/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0232e-01 (5.1152e-01)	Acc@1  82.81 ( 82.40)	Acc@5  98.44 ( 99.16)
Epoch: [9][300/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.1698e-01 (5.1208e-01)	Acc@1  79.69 ( 82.38)	Acc@5  99.22 ( 99.17)
Epoch: [9][310/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.5667e-01 (5.1235e-01)	Acc@1  77.34 ( 82.37)	Acc@5  99.22 ( 99.16)
Epoch: [9][320/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.7082e-01 (5.1241e-01)	Acc@1  85.94 ( 82.38)	Acc@5 100.00 ( 99.16)
Epoch: [9][330/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.2913e-01 (5.1254e-01)	Acc@1  85.16 ( 82.39)	Acc@5  98.44 ( 99.16)
Epoch: [9][340/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.7850e-01 (5.1267e-01)	Acc@1  81.25 ( 82.35)	Acc@5 100.00 ( 99.16)
Epoch: [9][350/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.4922e-01 (5.1309e-01)	Acc@1  81.25 ( 82.34)	Acc@5  96.09 ( 99.15)
Epoch: [9][360/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.6554e-01 (5.1312e-01)	Acc@1  79.69 ( 82.35)	Acc@5  98.44 ( 99.15)
Epoch: [9][370/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.1749e-01 (5.1374e-01)	Acc@1  82.03 ( 82.33)	Acc@5  97.66 ( 99.15)
Epoch: [9][380/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.0446e-01 (5.1346e-01)	Acc@1  80.47 ( 82.32)	Acc@5 100.00 ( 99.16)
Epoch: [9][390/391]	Time  0.026 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.7719e-01 (5.1368e-01)	Acc@1  82.50 ( 82.30)	Acc@5 100.00 ( 99.16)
## e[9] optimizer.zero_grad (sum) time: 0.26226377487182617
## e[9]       loss.backward (sum) time: 3.9754226207733154
## e[9]      optimizer.step (sum) time: 1.8156917095184326
## epoch[9] training(only) time: 16.082892179489136
# Switched to evaluate mode...
Test: [  0/100]	Time  0.194 ( 0.194)	Loss 7.5032e-01 (7.5032e-01)	Acc@1  69.00 ( 69.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.019 ( 0.036)	Loss 7.2435e-01 (6.5787e-01)	Acc@1  79.00 ( 77.00)	Acc@5  99.00 ( 99.27)
Test: [ 20/100]	Time  0.021 ( 0.029)	Loss 6.2608e-01 (6.3249e-01)	Acc@1  76.00 ( 77.86)	Acc@5 100.00 ( 99.00)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 6.1298e-01 (6.4833e-01)	Acc@1  77.00 ( 77.97)	Acc@5  98.00 ( 98.71)
Test: [ 40/100]	Time  0.021 ( 0.026)	Loss 5.6960e-01 (6.5543e-01)	Acc@1  81.00 ( 78.02)	Acc@5  99.00 ( 98.71)
Test: [ 50/100]	Time  0.018 ( 0.025)	Loss 4.9511e-01 (6.4774e-01)	Acc@1  81.00 ( 78.43)	Acc@5  98.00 ( 98.61)
Test: [ 60/100]	Time  0.021 ( 0.024)	Loss 7.5250e-01 (6.5184e-01)	Acc@1  79.00 ( 78.43)	Acc@5 100.00 ( 98.70)
Test: [ 70/100]	Time  0.018 ( 0.023)	Loss 6.1138e-01 (6.5150e-01)	Acc@1  79.00 ( 78.38)	Acc@5  99.00 ( 98.66)
Test: [ 80/100]	Time  0.021 ( 0.023)	Loss 5.1682e-01 (6.4984e-01)	Acc@1  78.00 ( 78.47)	Acc@5 100.00 ( 98.69)
Test: [ 90/100]	Time  0.017 ( 0.023)	Loss 5.1066e-01 (6.5062e-01)	Acc@1  78.00 ( 78.52)	Acc@5 100.00 ( 98.70)
 * Acc@1 78.690 Acc@5 98.720
### epoch[9] execution time: 18.47204303741455
EPOCH 10
# current learning rate: 0.1
# Switched to train mode...
Epoch: [10][  0/391]	Time  0.211 ( 0.211)	Data  0.169 ( 0.169)	Loss 4.7484e-01 (4.7484e-01)	Acc@1  84.38 ( 84.38)	Acc@5 100.00 (100.00)
Epoch: [10][ 10/391]	Time  0.040 ( 0.056)	Data  0.001 ( 0.016)	Loss 5.0708e-01 (4.9561e-01)	Acc@1  84.38 ( 82.88)	Acc@5  99.22 ( 99.64)
Epoch: [10][ 20/391]	Time  0.038 ( 0.049)	Data  0.001 ( 0.009)	Loss 5.3307e-01 (4.8942e-01)	Acc@1  75.78 ( 82.85)	Acc@5  99.22 ( 99.52)
Epoch: [10][ 30/391]	Time  0.044 ( 0.046)	Data  0.001 ( 0.006)	Loss 4.8751e-01 (4.8968e-01)	Acc@1  83.59 ( 82.99)	Acc@5  99.22 ( 99.37)
Epoch: [10][ 40/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.005)	Loss 6.0907e-01 (4.8038e-01)	Acc@1  81.25 ( 83.27)	Acc@5  99.22 ( 99.33)
Epoch: [10][ 50/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.004)	Loss 5.0013e-01 (4.8012e-01)	Acc@1  78.12 ( 83.04)	Acc@5 100.00 ( 99.36)
Epoch: [10][ 60/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 6.6885e-01 (4.7841e-01)	Acc@1  76.56 ( 83.18)	Acc@5  97.66 ( 99.30)
Epoch: [10][ 70/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.2122e-01 (4.7517e-01)	Acc@1  84.38 ( 83.44)	Acc@5  99.22 ( 99.30)
Epoch: [10][ 80/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.0204e-01 (4.6670e-01)	Acc@1  88.28 ( 83.69)	Acc@5 100.00 ( 99.31)
Epoch: [10][ 90/391]	Time  0.045 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.8395e-01 (4.6895e-01)	Acc@1  76.56 ( 83.59)	Acc@5  99.22 ( 99.30)
Epoch: [10][100/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.9247e-01 (4.7691e-01)	Acc@1  76.56 ( 83.44)	Acc@5 100.00 ( 99.30)
Epoch: [10][110/391]	Time  0.054 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.1404e-01 (4.7668e-01)	Acc@1  83.59 ( 83.47)	Acc@5  99.22 ( 99.32)
Epoch: [10][120/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.5131e-01 (4.7959e-01)	Acc@1  81.25 ( 83.32)	Acc@5  99.22 ( 99.32)
Epoch: [10][130/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.6667e-01 (4.8031e-01)	Acc@1  85.94 ( 83.39)	Acc@5 100.00 ( 99.28)
Epoch: [10][140/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7329e-01 (4.7997e-01)	Acc@1  85.94 ( 83.39)	Acc@5 100.00 ( 99.29)
Epoch: [10][150/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0023e-01 (4.7879e-01)	Acc@1  88.28 ( 83.43)	Acc@5  99.22 ( 99.30)
Epoch: [10][160/391]	Time  0.039 ( 0.041)	Data  0.002 ( 0.002)	Loss 5.9846e-01 (4.7953e-01)	Acc@1  77.34 ( 83.35)	Acc@5  99.22 ( 99.31)
Epoch: [10][170/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.4109e-01 (4.7917e-01)	Acc@1  82.03 ( 83.40)	Acc@5 100.00 ( 99.29)
Epoch: [10][180/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.0344e-01 (4.8241e-01)	Acc@1  78.12 ( 83.36)	Acc@5  98.44 ( 99.27)
Epoch: [10][190/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8399e-01 (4.8293e-01)	Acc@1  81.25 ( 83.36)	Acc@5 100.00 ( 99.25)
Epoch: [10][200/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5059e-01 (4.8449e-01)	Acc@1  88.28 ( 83.28)	Acc@5 100.00 ( 99.24)
Epoch: [10][210/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.8248e-01 (4.8606e-01)	Acc@1  73.44 ( 83.22)	Acc@5 100.00 ( 99.22)
Epoch: [10][220/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1238e-01 (4.8823e-01)	Acc@1  88.28 ( 83.15)	Acc@5 100.00 ( 99.23)
Epoch: [10][230/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8348e-01 (4.8833e-01)	Acc@1  89.84 ( 83.16)	Acc@5  99.22 ( 99.23)
Epoch: [10][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.6784e-01 (4.8880e-01)	Acc@1  77.34 ( 83.12)	Acc@5 100.00 ( 99.23)
Epoch: [10][250/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.6596e-01 (4.8832e-01)	Acc@1  81.25 ( 83.16)	Acc@5  98.44 ( 99.24)
Epoch: [10][260/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7159e-01 (4.8684e-01)	Acc@1  86.72 ( 83.22)	Acc@5  98.44 ( 99.24)
Epoch: [10][270/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3925e-01 (4.8609e-01)	Acc@1  86.72 ( 83.19)	Acc@5 100.00 ( 99.25)
Epoch: [10][280/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8315e-01 (4.8575e-01)	Acc@1  88.28 ( 83.19)	Acc@5 100.00 ( 99.25)
Epoch: [10][290/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.5609e-01 (4.8579e-01)	Acc@1  85.94 ( 83.17)	Acc@5 100.00 ( 99.26)
Epoch: [10][300/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.5626e-01 (4.8780e-01)	Acc@1  84.38 ( 83.11)	Acc@5 100.00 ( 99.25)
Epoch: [10][310/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.2624e-01 (4.8925e-01)	Acc@1  82.03 ( 83.08)	Acc@5  99.22 ( 99.23)
Epoch: [10][320/391]	Time  0.040 ( 0.041)	Data  0.002 ( 0.001)	Loss 5.4427e-01 (4.8977e-01)	Acc@1  79.69 ( 83.06)	Acc@5  99.22 ( 99.21)
Epoch: [10][330/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.5742e-01 (4.8995e-01)	Acc@1  81.25 ( 83.03)	Acc@5  97.66 ( 99.22)
Epoch: [10][340/391]	Time  0.048 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.2239e-01 (4.8924e-01)	Acc@1  84.38 ( 83.06)	Acc@5 100.00 ( 99.23)
Epoch: [10][350/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.1579e-01 (4.8913e-01)	Acc@1  85.94 ( 83.07)	Acc@5  99.22 ( 99.23)
Epoch: [10][360/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.6218e-01 (4.8906e-01)	Acc@1  83.59 ( 83.07)	Acc@5  99.22 ( 99.23)
Epoch: [10][370/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.3807e-01 (4.8805e-01)	Acc@1  84.38 ( 83.11)	Acc@5 100.00 ( 99.23)
Epoch: [10][380/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.3230e-01 (4.8837e-01)	Acc@1  80.47 ( 83.10)	Acc@5 100.00 ( 99.23)
Epoch: [10][390/391]	Time  0.028 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.7581e-01 (4.8937e-01)	Acc@1  78.75 ( 83.05)	Acc@5  97.50 ( 99.23)
## e[10] optimizer.zero_grad (sum) time: 0.2605559825897217
## e[10]       loss.backward (sum) time: 3.9851112365722656
## e[10]      optimizer.step (sum) time: 1.8036868572235107
## epoch[10] training(only) time: 16.055303812026978
# Switched to evaluate mode...
Test: [  0/100]	Time  0.172 ( 0.172)	Loss 6.0034e-01 (6.0034e-01)	Acc@1  80.00 ( 80.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 5.7131e-01 (5.6095e-01)	Acc@1  80.00 ( 81.73)	Acc@5 100.00 ( 99.18)
Test: [ 20/100]	Time  0.016 ( 0.028)	Loss 6.5821e-01 (5.6700e-01)	Acc@1  79.00 ( 80.81)	Acc@5  99.00 ( 99.19)
Test: [ 30/100]	Time  0.023 ( 0.025)	Loss 6.0581e-01 (5.6840e-01)	Acc@1  74.00 ( 80.48)	Acc@5  98.00 ( 99.13)
Test: [ 40/100]	Time  0.022 ( 0.024)	Loss 5.2206e-01 (5.8276e-01)	Acc@1  84.00 ( 80.22)	Acc@5  99.00 ( 99.00)
Test: [ 50/100]	Time  0.022 ( 0.024)	Loss 4.9083e-01 (5.6814e-01)	Acc@1  81.00 ( 80.78)	Acc@5 100.00 ( 99.06)
Test: [ 60/100]	Time  0.017 ( 0.023)	Loss 6.0808e-01 (5.7727e-01)	Acc@1  81.00 ( 80.41)	Acc@5  99.00 ( 99.05)
Test: [ 70/100]	Time  0.023 ( 0.023)	Loss 4.9644e-01 (5.7179e-01)	Acc@1  83.00 ( 80.45)	Acc@5  99.00 ( 99.04)
Test: [ 80/100]	Time  0.016 ( 0.022)	Loss 5.4458e-01 (5.7048e-01)	Acc@1  82.00 ( 80.42)	Acc@5  98.00 ( 99.06)
Test: [ 90/100]	Time  0.016 ( 0.022)	Loss 4.4604e-01 (5.7616e-01)	Acc@1  79.00 ( 80.16)	Acc@5 100.00 ( 99.01)
 * Acc@1 80.300 Acc@5 99.040
### epoch[10] execution time: 18.390249729156494
EPOCH 11
# current learning rate: 0.1
# Switched to train mode...
Epoch: [11][  0/391]	Time  0.225 ( 0.225)	Data  0.183 ( 0.183)	Loss 4.8619e-01 (4.8619e-01)	Acc@1  84.38 ( 84.38)	Acc@5  99.22 ( 99.22)
Epoch: [11][ 10/391]	Time  0.038 ( 0.057)	Data  0.001 ( 0.017)	Loss 5.0242e-01 (4.4425e-01)	Acc@1  83.59 ( 84.23)	Acc@5  97.66 ( 99.36)
Epoch: [11][ 20/391]	Time  0.042 ( 0.049)	Data  0.001 ( 0.010)	Loss 5.0895e-01 (4.5259e-01)	Acc@1  83.59 ( 84.49)	Acc@5  99.22 ( 99.33)
Epoch: [11][ 30/391]	Time  0.040 ( 0.046)	Data  0.001 ( 0.007)	Loss 4.7763e-01 (4.6408e-01)	Acc@1  83.59 ( 84.02)	Acc@5  99.22 ( 99.24)
Epoch: [11][ 40/391]	Time  0.040 ( 0.045)	Data  0.001 ( 0.005)	Loss 5.1116e-01 (4.7193e-01)	Acc@1  85.16 ( 83.71)	Acc@5  98.44 ( 99.28)
Epoch: [11][ 50/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.005)	Loss 4.3185e-01 (4.6111e-01)	Acc@1  85.16 ( 84.01)	Acc@5  99.22 ( 99.30)
Epoch: [11][ 60/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.004)	Loss 4.1067e-01 (4.6291e-01)	Acc@1  85.94 ( 84.03)	Acc@5  99.22 ( 99.33)
Epoch: [11][ 70/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 4.7002e-01 (4.5949e-01)	Acc@1  82.81 ( 84.00)	Acc@5  99.22 ( 99.36)
Epoch: [11][ 80/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.6602e-01 (4.6492e-01)	Acc@1  82.81 ( 83.83)	Acc@5 100.00 ( 99.37)
Epoch: [11][ 90/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.3425e-01 (4.6700e-01)	Acc@1  80.47 ( 83.75)	Acc@5  99.22 ( 99.35)
Epoch: [11][100/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.1386e-01 (4.6943e-01)	Acc@1  88.28 ( 83.70)	Acc@5  99.22 ( 99.36)
Epoch: [11][110/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.0811e-01 (4.6596e-01)	Acc@1  86.72 ( 83.88)	Acc@5  99.22 ( 99.39)
Epoch: [11][120/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.8629e-01 (4.6586e-01)	Acc@1  85.94 ( 83.88)	Acc@5  99.22 ( 99.37)
Epoch: [11][130/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.0647e-01 (4.6478e-01)	Acc@1  85.94 ( 83.93)	Acc@5 100.00 ( 99.39)
Epoch: [11][140/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.6335e-01 (4.6290e-01)	Acc@1  82.81 ( 84.00)	Acc@5 100.00 ( 99.41)
Epoch: [11][150/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1336e-01 (4.6533e-01)	Acc@1  85.94 ( 83.91)	Acc@5 100.00 ( 99.40)
Epoch: [11][160/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8121e-01 (4.6324e-01)	Acc@1  82.81 ( 83.93)	Acc@5 100.00 ( 99.40)
Epoch: [11][170/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2484e-01 (4.6390e-01)	Acc@1  83.59 ( 83.87)	Acc@5 100.00 ( 99.38)
Epoch: [11][180/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.5436e-01 (4.6706e-01)	Acc@1  85.94 ( 83.81)	Acc@5  98.44 ( 99.37)
Epoch: [11][190/391]	Time  0.039 ( 0.041)	Data  0.002 ( 0.002)	Loss 3.4110e-01 (4.6510e-01)	Acc@1  87.50 ( 83.85)	Acc@5 100.00 ( 99.36)
Epoch: [11][200/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8048e-01 (4.6600e-01)	Acc@1  85.16 ( 83.85)	Acc@5  97.66 ( 99.34)
Epoch: [11][210/391]	Time  0.046 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9676e-01 (4.6772e-01)	Acc@1  82.81 ( 83.76)	Acc@5  97.66 ( 99.34)
Epoch: [11][220/391]	Time  0.035 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2349e-01 (4.6695e-01)	Acc@1  89.06 ( 83.79)	Acc@5 100.00 ( 99.35)
Epoch: [11][230/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8619e-01 (4.6665e-01)	Acc@1  89.06 ( 83.82)	Acc@5  99.22 ( 99.35)
Epoch: [11][240/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.6776e-01 (4.6721e-01)	Acc@1  79.69 ( 83.77)	Acc@5  99.22 ( 99.35)
Epoch: [11][250/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1187e-01 (4.6621e-01)	Acc@1  82.03 ( 83.76)	Acc@5  98.44 ( 99.35)
Epoch: [11][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4261e-01 (4.6723e-01)	Acc@1  86.72 ( 83.71)	Acc@5  99.22 ( 99.34)
Epoch: [11][270/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6061e-01 (4.6703e-01)	Acc@1  88.28 ( 83.71)	Acc@5 100.00 ( 99.35)
Epoch: [11][280/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.7335e-01 (4.6690e-01)	Acc@1  78.12 ( 83.70)	Acc@5 100.00 ( 99.35)
Epoch: [11][290/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3941e-01 (4.6743e-01)	Acc@1  82.81 ( 83.66)	Acc@5  99.22 ( 99.34)
Epoch: [11][300/391]	Time  0.045 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.9987e-01 (4.6849e-01)	Acc@1  78.12 ( 83.65)	Acc@5  99.22 ( 99.35)
Epoch: [11][310/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9420e-01 (4.6830e-01)	Acc@1  83.59 ( 83.65)	Acc@5  99.22 ( 99.35)
Epoch: [11][320/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7257e-01 (4.6775e-01)	Acc@1  85.16 ( 83.69)	Acc@5 100.00 ( 99.35)
Epoch: [11][330/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 4.1945e-01 (4.6590e-01)	Acc@1  85.16 ( 83.77)	Acc@5  99.22 ( 99.36)
Epoch: [11][340/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 4.1989e-01 (4.6576e-01)	Acc@1  85.94 ( 83.75)	Acc@5  99.22 ( 99.35)
Epoch: [11][350/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 4.2321e-01 (4.6618e-01)	Acc@1  86.72 ( 83.74)	Acc@5  99.22 ( 99.34)
Epoch: [11][360/391]	Time  0.046 ( 0.040)	Data  0.001 ( 0.002)	Loss 5.1555e-01 (4.6542e-01)	Acc@1  80.47 ( 83.78)	Acc@5  99.22 ( 99.34)
Epoch: [11][370/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 5.3857e-01 (4.6649e-01)	Acc@1  80.47 ( 83.72)	Acc@5  98.44 ( 99.34)
Epoch: [11][380/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 4.4179e-01 (4.6718e-01)	Acc@1  83.59 ( 83.70)	Acc@5 100.00 ( 99.34)
Epoch: [11][390/391]	Time  0.027 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.9895e-01 (4.6746e-01)	Acc@1  80.00 ( 83.69)	Acc@5  98.75 ( 99.34)
## e[11] optimizer.zero_grad (sum) time: 0.2620079517364502
## e[11]       loss.backward (sum) time: 3.9408044815063477
## e[11]      optimizer.step (sum) time: 1.8325738906860352
## epoch[11] training(only) time: 15.922290325164795
# Switched to evaluate mode...
Test: [  0/100]	Time  0.165 ( 0.165)	Loss 7.0080e-01 (7.0080e-01)	Acc@1  75.00 ( 75.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.018 ( 0.033)	Loss 6.5838e-01 (6.3020e-01)	Acc@1  79.00 ( 79.55)	Acc@5  99.00 ( 99.00)
Test: [ 20/100]	Time  0.023 ( 0.027)	Loss 5.4699e-01 (6.4359e-01)	Acc@1  78.00 ( 78.71)	Acc@5 100.00 ( 98.81)
Test: [ 30/100]	Time  0.017 ( 0.025)	Loss 6.5874e-01 (6.4982e-01)	Acc@1  84.00 ( 78.94)	Acc@5  98.00 ( 98.65)
Test: [ 40/100]	Time  0.018 ( 0.024)	Loss 7.0197e-01 (6.5828e-01)	Acc@1  81.00 ( 78.80)	Acc@5  99.00 ( 98.63)
Test: [ 50/100]	Time  0.021 ( 0.023)	Loss 4.1375e-01 (6.4179e-01)	Acc@1  83.00 ( 79.27)	Acc@5  98.00 ( 98.65)
Test: [ 60/100]	Time  0.021 ( 0.023)	Loss 4.1542e-01 (6.4298e-01)	Acc@1  86.00 ( 79.28)	Acc@5  99.00 ( 98.59)
Test: [ 70/100]	Time  0.018 ( 0.022)	Loss 6.4354e-01 (6.3952e-01)	Acc@1  80.00 ( 79.32)	Acc@5  99.00 ( 98.58)
Test: [ 80/100]	Time  0.015 ( 0.022)	Loss 4.8629e-01 (6.3542e-01)	Acc@1  82.00 ( 79.47)	Acc@5  99.00 ( 98.64)
Test: [ 90/100]	Time  0.018 ( 0.022)	Loss 5.3540e-01 (6.3445e-01)	Acc@1  82.00 ( 79.43)	Acc@5 100.00 ( 98.65)
 * Acc@1 79.560 Acc@5 98.630
### epoch[11] execution time: 18.167810916900635
EPOCH 12
# current learning rate: 0.1
# Switched to train mode...
Epoch: [12][  0/391]	Time  0.215 ( 0.215)	Data  0.174 ( 0.174)	Loss 4.3700e-01 (4.3700e-01)	Acc@1  85.16 ( 85.16)	Acc@5 100.00 (100.00)
Epoch: [12][ 10/391]	Time  0.040 ( 0.056)	Data  0.001 ( 0.017)	Loss 4.6325e-01 (4.4348e-01)	Acc@1  85.94 ( 84.94)	Acc@5 100.00 ( 99.57)
Epoch: [12][ 20/391]	Time  0.043 ( 0.049)	Data  0.001 ( 0.009)	Loss 3.2456e-01 (4.3900e-01)	Acc@1  89.06 ( 85.12)	Acc@5 100.00 ( 99.55)
Epoch: [12][ 30/391]	Time  0.038 ( 0.046)	Data  0.001 ( 0.007)	Loss 4.6407e-01 (4.3765e-01)	Acc@1  84.38 ( 85.28)	Acc@5  99.22 ( 99.42)
Epoch: [12][ 40/391]	Time  0.042 ( 0.045)	Data  0.001 ( 0.005)	Loss 4.5554e-01 (4.3527e-01)	Acc@1  82.03 ( 85.19)	Acc@5 100.00 ( 99.39)
Epoch: [12][ 50/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.004)	Loss 4.9538e-01 (4.4747e-01)	Acc@1  78.91 ( 84.50)	Acc@5 100.00 ( 99.40)
Epoch: [12][ 60/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.004)	Loss 4.1674e-01 (4.3892e-01)	Acc@1  86.72 ( 84.95)	Acc@5 100.00 ( 99.47)
Epoch: [12][ 70/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.1916e-01 (4.3741e-01)	Acc@1  82.03 ( 84.79)	Acc@5 100.00 ( 99.48)
Epoch: [12][ 80/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 6.3002e-01 (4.3586e-01)	Acc@1  75.78 ( 84.81)	Acc@5  98.44 ( 99.48)
Epoch: [12][ 90/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.8082e-01 (4.3701e-01)	Acc@1  85.94 ( 84.75)	Acc@5  99.22 ( 99.45)
Epoch: [12][100/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.8462e-01 (4.3855e-01)	Acc@1  80.47 ( 84.72)	Acc@5  97.66 ( 99.43)
Epoch: [12][110/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.2931e-01 (4.3985e-01)	Acc@1  85.16 ( 84.64)	Acc@5  99.22 ( 99.46)
Epoch: [12][120/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.7331e-01 (4.3775e-01)	Acc@1  85.94 ( 84.61)	Acc@5 100.00 ( 99.46)
Epoch: [12][130/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.2095e-01 (4.3869e-01)	Acc@1  87.50 ( 84.67)	Acc@5  99.22 ( 99.43)
Epoch: [12][140/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.2207e-01 (4.4072e-01)	Acc@1  85.94 ( 84.62)	Acc@5  99.22 ( 99.43)
Epoch: [12][150/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.0210e-01 (4.3923e-01)	Acc@1  81.25 ( 84.66)	Acc@5  99.22 ( 99.45)
Epoch: [12][160/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.7844e-01 (4.4102e-01)	Acc@1  86.72 ( 84.56)	Acc@5 100.00 ( 99.46)
Epoch: [12][170/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.1015e-01 (4.4178e-01)	Acc@1  81.25 ( 84.55)	Acc@5  99.22 ( 99.45)
Epoch: [12][180/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2411e-01 (4.4098e-01)	Acc@1  85.94 ( 84.61)	Acc@5  98.44 ( 99.45)
Epoch: [12][190/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2122e-01 (4.4237e-01)	Acc@1  83.59 ( 84.55)	Acc@5 100.00 ( 99.44)
Epoch: [12][200/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7833e-01 (4.4250e-01)	Acc@1  82.81 ( 84.55)	Acc@5  98.44 ( 99.42)
Epoch: [12][210/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0454e-01 (4.4181e-01)	Acc@1  85.94 ( 84.52)	Acc@5  99.22 ( 99.43)
Epoch: [12][220/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8821e-01 (4.4285e-01)	Acc@1  84.38 ( 84.47)	Acc@5  97.66 ( 99.41)
Epoch: [12][230/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7631e-01 (4.4479e-01)	Acc@1  83.59 ( 84.41)	Acc@5  98.44 ( 99.39)
Epoch: [12][240/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.1322e-01 (4.4699e-01)	Acc@1  81.25 ( 84.34)	Acc@5  97.66 ( 99.39)
Epoch: [12][250/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.0348e-01 (4.4703e-01)	Acc@1  87.50 ( 84.36)	Acc@5 100.00 ( 99.39)
Epoch: [12][260/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6217e-01 (4.4747e-01)	Acc@1  87.50 ( 84.39)	Acc@5 100.00 ( 99.40)
Epoch: [12][270/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4117e-01 (4.4776e-01)	Acc@1  87.50 ( 84.38)	Acc@5  99.22 ( 99.39)
Epoch: [12][280/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8203e-01 (4.4752e-01)	Acc@1  80.47 ( 84.37)	Acc@5  97.66 ( 99.39)
Epoch: [12][290/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2603e-01 (4.4674e-01)	Acc@1  83.59 ( 84.41)	Acc@5 100.00 ( 99.39)
Epoch: [12][300/391]	Time  0.039 ( 0.041)	Data  0.002 ( 0.002)	Loss 3.3461e-01 (4.4674e-01)	Acc@1  85.94 ( 84.38)	Acc@5 100.00 ( 99.40)
Epoch: [12][310/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9430e-01 (4.4717e-01)	Acc@1  84.38 ( 84.37)	Acc@5  98.44 ( 99.39)
Epoch: [12][320/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4935e-01 (4.4771e-01)	Acc@1  88.28 ( 84.36)	Acc@5 100.00 ( 99.40)
Epoch: [12][330/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3588e-01 (4.4710e-01)	Acc@1  84.38 ( 84.39)	Acc@5  99.22 ( 99.41)
Epoch: [12][340/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4795e-01 (4.4768e-01)	Acc@1  84.38 ( 84.36)	Acc@5 100.00 ( 99.40)
Epoch: [12][350/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.9934e-01 (4.4857e-01)	Acc@1  82.03 ( 84.34)	Acc@5  99.22 ( 99.40)
Epoch: [12][360/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.7159e-01 (4.4905e-01)	Acc@1  78.91 ( 84.33)	Acc@5  99.22 ( 99.40)
Epoch: [12][370/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.6010e-01 (4.4970e-01)	Acc@1  87.50 ( 84.34)	Acc@5 100.00 ( 99.39)
Epoch: [12][380/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.1251e-01 (4.5025e-01)	Acc@1  85.16 ( 84.35)	Acc@5 100.00 ( 99.38)
Epoch: [12][390/391]	Time  0.034 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.2110e-01 (4.5125e-01)	Acc@1  86.25 ( 84.31)	Acc@5  98.75 ( 99.38)
## e[12] optimizer.zero_grad (sum) time: 0.2642967700958252
## e[12]       loss.backward (sum) time: 4.00077223777771
## e[12]      optimizer.step (sum) time: 1.8219361305236816
## epoch[12] training(only) time: 16.091408252716064
# Switched to evaluate mode...
Test: [  0/100]	Time  0.164 ( 0.164)	Loss 5.2505e-01 (5.2505e-01)	Acc@1  81.00 ( 81.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.019 ( 0.035)	Loss 5.7641e-01 (5.5640e-01)	Acc@1  82.00 ( 81.45)	Acc@5  99.00 ( 99.36)
Test: [ 20/100]	Time  0.022 ( 0.027)	Loss 5.5495e-01 (5.5707e-01)	Acc@1  80.00 ( 81.62)	Acc@5 100.00 ( 99.14)
Test: [ 30/100]	Time  0.021 ( 0.025)	Loss 5.9372e-01 (5.5387e-01)	Acc@1  81.00 ( 81.61)	Acc@5  98.00 ( 99.00)
Test: [ 40/100]	Time  0.024 ( 0.024)	Loss 5.3144e-01 (5.6102e-01)	Acc@1  84.00 ( 81.17)	Acc@5 100.00 ( 98.98)
Test: [ 50/100]	Time  0.018 ( 0.023)	Loss 4.7144e-01 (5.5387e-01)	Acc@1  83.00 ( 81.22)	Acc@5 100.00 ( 99.04)
Test: [ 60/100]	Time  0.018 ( 0.023)	Loss 4.8036e-01 (5.5078e-01)	Acc@1  84.00 ( 81.30)	Acc@5 100.00 ( 99.03)
Test: [ 70/100]	Time  0.020 ( 0.022)	Loss 5.6831e-01 (5.5131e-01)	Acc@1  84.00 ( 81.39)	Acc@5 100.00 ( 99.07)
Test: [ 80/100]	Time  0.024 ( 0.022)	Loss 5.1951e-01 (5.4882e-01)	Acc@1  79.00 ( 81.21)	Acc@5 100.00 ( 99.11)
Test: [ 90/100]	Time  0.019 ( 0.022)	Loss 3.6644e-01 (5.4643e-01)	Acc@1  87.00 ( 81.42)	Acc@5 100.00 ( 99.18)
 * Acc@1 81.420 Acc@5 99.180
### epoch[12] execution time: 18.415984630584717
EPOCH 13
# current learning rate: 0.1
# Switched to train mode...
Epoch: [13][  0/391]	Time  0.263 ( 0.263)	Data  0.219 ( 0.219)	Loss 3.4735e-01 (3.4735e-01)	Acc@1  90.62 ( 90.62)	Acc@5 100.00 (100.00)
Epoch: [13][ 10/391]	Time  0.038 ( 0.061)	Data  0.001 ( 0.021)	Loss 4.8162e-01 (4.3098e-01)	Acc@1  85.16 ( 85.09)	Acc@5  98.44 ( 99.29)
Epoch: [13][ 20/391]	Time  0.043 ( 0.051)	Data  0.001 ( 0.011)	Loss 3.8884e-01 (4.5614e-01)	Acc@1  89.06 ( 84.41)	Acc@5 100.00 ( 99.26)
Epoch: [13][ 30/391]	Time  0.039 ( 0.047)	Data  0.001 ( 0.008)	Loss 3.8064e-01 (4.4893e-01)	Acc@1  84.38 ( 84.35)	Acc@5 100.00 ( 99.34)
Epoch: [13][ 40/391]	Time  0.042 ( 0.046)	Data  0.001 ( 0.006)	Loss 3.9248e-01 (4.3518e-01)	Acc@1  84.38 ( 84.57)	Acc@5 100.00 ( 99.45)
Epoch: [13][ 50/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.005)	Loss 4.4493e-01 (4.3723e-01)	Acc@1  82.03 ( 84.33)	Acc@5  98.44 ( 99.39)
Epoch: [13][ 60/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.005)	Loss 4.1395e-01 (4.3213e-01)	Acc@1  84.38 ( 84.50)	Acc@5  98.44 ( 99.35)
Epoch: [13][ 70/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.004)	Loss 5.7858e-01 (4.3144e-01)	Acc@1  82.03 ( 84.61)	Acc@5  98.44 ( 99.35)
Epoch: [13][ 80/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.004)	Loss 3.7839e-01 (4.3299e-01)	Acc@1  86.72 ( 84.69)	Acc@5 100.00 ( 99.35)
Epoch: [13][ 90/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.5293e-01 (4.3123e-01)	Acc@1  90.62 ( 84.86)	Acc@5 100.00 ( 99.35)
Epoch: [13][100/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.8088e-01 (4.3133e-01)	Acc@1  82.03 ( 84.85)	Acc@5 100.00 ( 99.36)
Epoch: [13][110/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.0095e-01 (4.3060e-01)	Acc@1  85.16 ( 84.96)	Acc@5  99.22 ( 99.38)
Epoch: [13][120/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.2836e-01 (4.3037e-01)	Acc@1  84.38 ( 84.92)	Acc@5 100.00 ( 99.41)
Epoch: [13][130/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.0231e-01 (4.2924e-01)	Acc@1  85.16 ( 84.95)	Acc@5 100.00 ( 99.43)
Epoch: [13][140/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.4834e-01 (4.2763e-01)	Acc@1  87.50 ( 85.01)	Acc@5 100.00 ( 99.45)
Epoch: [13][150/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.7214e-01 (4.2764e-01)	Acc@1  87.50 ( 85.04)	Acc@5  97.66 ( 99.45)
Epoch: [13][160/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.7681e-01 (4.2898e-01)	Acc@1  86.72 ( 85.00)	Acc@5  97.66 ( 99.43)
Epoch: [13][170/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1258e-01 (4.2821e-01)	Acc@1  85.94 ( 85.01)	Acc@5  99.22 ( 99.44)
Epoch: [13][180/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4378e-01 (4.2807e-01)	Acc@1  89.84 ( 85.00)	Acc@5  99.22 ( 99.44)
Epoch: [13][190/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7712e-01 (4.2892e-01)	Acc@1  78.91 ( 84.94)	Acc@5 100.00 ( 99.43)
Epoch: [13][200/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0135e-01 (4.2697e-01)	Acc@1  82.81 ( 85.01)	Acc@5  99.22 ( 99.44)
Epoch: [13][210/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.5889e-01 (4.2691e-01)	Acc@1  86.72 ( 85.02)	Acc@5  98.44 ( 99.44)
Epoch: [13][220/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7461e-01 (4.2771e-01)	Acc@1  81.25 ( 85.02)	Acc@5  99.22 ( 99.45)
Epoch: [13][230/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2552e-01 (4.2613e-01)	Acc@1  92.19 ( 85.09)	Acc@5  99.22 ( 99.46)
Epoch: [13][240/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2593e-01 (4.2868e-01)	Acc@1  88.28 ( 85.02)	Acc@5 100.00 ( 99.46)
Epoch: [13][250/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9835e-01 (4.2790e-01)	Acc@1  83.59 ( 85.06)	Acc@5  99.22 ( 99.47)
Epoch: [13][260/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9030e-01 (4.2903e-01)	Acc@1  82.03 ( 84.99)	Acc@5 100.00 ( 99.47)
Epoch: [13][270/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2875e-01 (4.3025e-01)	Acc@1  82.03 ( 84.97)	Acc@5 100.00 ( 99.46)
Epoch: [13][280/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.0121e-01 (4.2956e-01)	Acc@1  92.97 ( 85.02)	Acc@5 100.00 ( 99.47)
Epoch: [13][290/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1824e-01 (4.2819e-01)	Acc@1  85.16 ( 85.07)	Acc@5  98.44 ( 99.47)
Epoch: [13][300/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9588e-01 (4.2748e-01)	Acc@1  88.28 ( 85.07)	Acc@5  99.22 ( 99.47)
Epoch: [13][310/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2989e-01 (4.2665e-01)	Acc@1  80.47 ( 85.10)	Acc@5  99.22 ( 99.47)
Epoch: [13][320/391]	Time  0.035 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9834e-01 (4.2656e-01)	Acc@1  82.81 ( 85.09)	Acc@5  99.22 ( 99.46)
Epoch: [13][330/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7547e-01 (4.2551e-01)	Acc@1  89.84 ( 85.15)	Acc@5 100.00 ( 99.46)
Epoch: [13][340/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7687e-01 (4.2624e-01)	Acc@1  86.72 ( 85.17)	Acc@5  99.22 ( 99.46)
Epoch: [13][350/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4931e-01 (4.2665e-01)	Acc@1  85.16 ( 85.18)	Acc@5  98.44 ( 99.46)
Epoch: [13][360/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.2138e-01 (4.2710e-01)	Acc@1  82.03 ( 85.18)	Acc@5  98.44 ( 99.46)
Epoch: [13][370/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2678e-01 (4.2718e-01)	Acc@1  83.59 ( 85.18)	Acc@5 100.00 ( 99.45)
Epoch: [13][380/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.5630e-01 (4.2800e-01)	Acc@1  81.25 ( 85.15)	Acc@5 100.00 ( 99.45)
Epoch: [13][390/391]	Time  0.030 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.0733e-01 (4.2853e-01)	Acc@1  87.50 ( 85.14)	Acc@5  97.50 ( 99.45)
## e[13] optimizer.zero_grad (sum) time: 0.2700767517089844
## e[13]       loss.backward (sum) time: 3.980428695678711
## e[13]      optimizer.step (sum) time: 1.8323986530303955
## epoch[13] training(only) time: 15.982922315597534
# Switched to evaluate mode...
Test: [  0/100]	Time  0.177 ( 0.177)	Loss 6.8168e-01 (6.8168e-01)	Acc@1  77.00 ( 77.00)	Acc@5  97.00 ( 97.00)
Test: [ 10/100]	Time  0.021 ( 0.035)	Loss 5.0461e-01 (5.5470e-01)	Acc@1  83.00 ( 81.00)	Acc@5 100.00 ( 99.27)
Test: [ 20/100]	Time  0.018 ( 0.027)	Loss 5.3436e-01 (5.3435e-01)	Acc@1  83.00 ( 82.05)	Acc@5  99.00 ( 98.95)
Test: [ 30/100]	Time  0.016 ( 0.025)	Loss 6.0634e-01 (5.3774e-01)	Acc@1  81.00 ( 82.06)	Acc@5  99.00 ( 98.90)
Test: [ 40/100]	Time  0.017 ( 0.024)	Loss 4.4535e-01 (5.4342e-01)	Acc@1  87.00 ( 82.12)	Acc@5  99.00 ( 98.90)
Test: [ 50/100]	Time  0.016 ( 0.023)	Loss 5.8821e-01 (5.3916e-01)	Acc@1  80.00 ( 82.25)	Acc@5  99.00 ( 98.96)
Test: [ 60/100]	Time  0.021 ( 0.023)	Loss 4.1233e-01 (5.3376e-01)	Acc@1  85.00 ( 82.43)	Acc@5 100.00 ( 99.05)
Test: [ 70/100]	Time  0.020 ( 0.023)	Loss 5.6259e-01 (5.2850e-01)	Acc@1  84.00 ( 82.59)	Acc@5  98.00 ( 99.07)
Test: [ 80/100]	Time  0.024 ( 0.023)	Loss 4.7664e-01 (5.2730e-01)	Acc@1  83.00 ( 82.59)	Acc@5 100.00 ( 99.11)
Test: [ 90/100]	Time  0.023 ( 0.023)	Loss 3.6861e-01 (5.3223e-01)	Acc@1  89.00 ( 82.46)	Acc@5 100.00 ( 99.11)
 * Acc@1 82.490 Acc@5 99.140
### epoch[13] execution time: 18.35351538658142
EPOCH 14
# current learning rate: 0.1
# Switched to train mode...
Epoch: [14][  0/391]	Time  0.251 ( 0.251)	Data  0.210 ( 0.210)	Loss 3.7430e-01 (3.7430e-01)	Acc@1  88.28 ( 88.28)	Acc@5 100.00 (100.00)
Epoch: [14][ 10/391]	Time  0.038 ( 0.059)	Data  0.001 ( 0.020)	Loss 3.5963e-01 (3.6712e-01)	Acc@1  87.50 ( 86.72)	Acc@5 100.00 ( 99.72)
Epoch: [14][ 20/391]	Time  0.039 ( 0.051)	Data  0.001 ( 0.011)	Loss 3.4475e-01 (3.9237e-01)	Acc@1  85.94 ( 86.09)	Acc@5  99.22 ( 99.52)
Epoch: [14][ 30/391]	Time  0.038 ( 0.047)	Data  0.001 ( 0.008)	Loss 4.8569e-01 (4.0138e-01)	Acc@1  83.59 ( 86.14)	Acc@5  99.22 ( 99.45)
Epoch: [14][ 40/391]	Time  0.042 ( 0.046)	Data  0.001 ( 0.006)	Loss 3.1749e-01 (4.0137e-01)	Acc@1  90.62 ( 86.22)	Acc@5 100.00 ( 99.43)
Epoch: [14][ 50/391]	Time  0.037 ( 0.044)	Data  0.001 ( 0.005)	Loss 4.8145e-01 (4.1215e-01)	Acc@1  81.25 ( 86.08)	Acc@5  99.22 ( 99.34)
Epoch: [14][ 60/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.004)	Loss 3.3120e-01 (4.1134e-01)	Acc@1  89.06 ( 86.18)	Acc@5  99.22 ( 99.32)
Epoch: [14][ 70/391]	Time  0.037 ( 0.043)	Data  0.001 ( 0.004)	Loss 4.5388e-01 (4.1782e-01)	Acc@1  83.59 ( 85.89)	Acc@5 100.00 ( 99.35)
Epoch: [14][ 80/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.004)	Loss 4.9125e-01 (4.2106e-01)	Acc@1  83.59 ( 85.76)	Acc@5  98.44 ( 99.33)
Epoch: [14][ 90/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.0189e-01 (4.2105e-01)	Acc@1  88.28 ( 85.65)	Acc@5  99.22 ( 99.36)
Epoch: [14][100/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.9401e-01 (4.2298e-01)	Acc@1  88.28 ( 85.58)	Acc@5  99.22 ( 99.37)
Epoch: [14][110/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.5366e-01 (4.2102e-01)	Acc@1  86.72 ( 85.53)	Acc@5  99.22 ( 99.39)
Epoch: [14][120/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.3503e-01 (4.2214e-01)	Acc@1  89.06 ( 85.49)	Acc@5 100.00 ( 99.38)
Epoch: [14][130/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.9954e-01 (4.2216e-01)	Acc@1  85.16 ( 85.45)	Acc@5  98.44 ( 99.37)
Epoch: [14][140/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.3911e-01 (4.1813e-01)	Acc@1  88.28 ( 85.56)	Acc@5  99.22 ( 99.38)
Epoch: [14][150/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7245e-01 (4.1783e-01)	Acc@1  85.94 ( 85.51)	Acc@5  97.66 ( 99.36)
Epoch: [14][160/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.5253e-01 (4.1989e-01)	Acc@1  82.81 ( 85.46)	Acc@5 100.00 ( 99.37)
Epoch: [14][170/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9355e-01 (4.1943e-01)	Acc@1  85.16 ( 85.45)	Acc@5 100.00 ( 99.38)
Epoch: [14][180/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5898e-01 (4.1648e-01)	Acc@1  86.72 ( 85.55)	Acc@5 100.00 ( 99.40)
Epoch: [14][190/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1772e-01 (4.1470e-01)	Acc@1  84.38 ( 85.63)	Acc@5  99.22 ( 99.42)
Epoch: [14][200/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4536e-01 (4.1547e-01)	Acc@1  89.06 ( 85.60)	Acc@5 100.00 ( 99.42)
Epoch: [14][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0265e-01 (4.1717e-01)	Acc@1  85.94 ( 85.55)	Acc@5  99.22 ( 99.41)
Epoch: [14][220/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5712e-01 (4.1783e-01)	Acc@1  89.06 ( 85.51)	Acc@5  99.22 ( 99.43)
Epoch: [14][230/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3214e-01 (4.1775e-01)	Acc@1  85.16 ( 85.51)	Acc@5  99.22 ( 99.44)
Epoch: [14][240/391]	Time  0.045 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2900e-01 (4.1724e-01)	Acc@1  82.03 ( 85.52)	Acc@5  98.44 ( 99.44)
Epoch: [14][250/391]	Time  0.035 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4662e-01 (4.1701e-01)	Acc@1  85.16 ( 85.50)	Acc@5 100.00 ( 99.44)
Epoch: [14][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.9314e-01 (4.1572e-01)	Acc@1  90.62 ( 85.54)	Acc@5 100.00 ( 99.44)
Epoch: [14][270/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0844e-01 (4.1517e-01)	Acc@1  83.59 ( 85.56)	Acc@5 100.00 ( 99.45)
Epoch: [14][280/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8506e-01 (4.1603e-01)	Acc@1  87.50 ( 85.57)	Acc@5  99.22 ( 99.45)
Epoch: [14][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1058e-01 (4.1544e-01)	Acc@1  84.38 ( 85.61)	Acc@5  99.22 ( 99.46)
Epoch: [14][300/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.5203e-01 (4.1679e-01)	Acc@1  83.59 ( 85.56)	Acc@5 100.00 ( 99.46)
Epoch: [14][310/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9106e-01 (4.1750e-01)	Acc@1  86.72 ( 85.54)	Acc@5  97.66 ( 99.44)
Epoch: [14][320/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.4549e-01 (4.1917e-01)	Acc@1  79.69 ( 85.49)	Acc@5 100.00 ( 99.43)
Epoch: [14][330/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.5787e-01 (4.2046e-01)	Acc@1  78.91 ( 85.42)	Acc@5  99.22 ( 99.42)
Epoch: [14][340/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7939e-01 (4.2056e-01)	Acc@1  82.03 ( 85.42)	Acc@5  99.22 ( 99.42)
Epoch: [14][350/391]	Time  0.034 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7274e-01 (4.1929e-01)	Acc@1  83.59 ( 85.47)	Acc@5 100.00 ( 99.42)
Epoch: [14][360/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9107e-01 (4.2102e-01)	Acc@1  82.81 ( 85.44)	Acc@5  99.22 ( 99.42)
Epoch: [14][370/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2699e-01 (4.2144e-01)	Acc@1  88.28 ( 85.40)	Acc@5  99.22 ( 99.41)
Epoch: [14][380/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9029e-01 (4.2140e-01)	Acc@1  82.03 ( 85.39)	Acc@5 100.00 ( 99.42)
Epoch: [14][390/391]	Time  0.027 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.8230e-01 (4.2251e-01)	Acc@1  73.75 ( 85.34)	Acc@5  98.75 ( 99.41)
## e[14] optimizer.zero_grad (sum) time: 0.26189088821411133
## e[14]       loss.backward (sum) time: 3.945648670196533
## e[14]      optimizer.step (sum) time: 1.8199076652526855
## epoch[14] training(only) time: 16.003461837768555
# Switched to evaluate mode...
Test: [  0/100]	Time  0.221 ( 0.221)	Loss 7.0211e-01 (7.0211e-01)	Acc@1  73.00 ( 73.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.021 ( 0.040)	Loss 5.8020e-01 (5.9687e-01)	Acc@1  83.00 ( 80.00)	Acc@5  99.00 ( 99.45)
Test: [ 20/100]	Time  0.018 ( 0.031)	Loss 8.2476e-01 (6.3151e-01)	Acc@1  73.00 ( 79.29)	Acc@5 100.00 ( 98.76)
Test: [ 30/100]	Time  0.016 ( 0.027)	Loss 6.3388e-01 (6.2490e-01)	Acc@1  77.00 ( 79.77)	Acc@5  96.00 ( 98.65)
Test: [ 40/100]	Time  0.023 ( 0.025)	Loss 6.1184e-01 (6.2634e-01)	Acc@1  81.00 ( 79.78)	Acc@5  99.00 ( 98.63)
Test: [ 50/100]	Time  0.019 ( 0.024)	Loss 6.4783e-01 (6.3054e-01)	Acc@1  77.00 ( 79.73)	Acc@5  98.00 ( 98.53)
Test: [ 60/100]	Time  0.021 ( 0.024)	Loss 5.7205e-01 (6.3605e-01)	Acc@1  83.00 ( 79.56)	Acc@5  98.00 ( 98.57)
Test: [ 70/100]	Time  0.023 ( 0.023)	Loss 7.1170e-01 (6.3666e-01)	Acc@1  77.00 ( 79.42)	Acc@5  98.00 ( 98.62)
Test: [ 80/100]	Time  0.022 ( 0.023)	Loss 3.6702e-01 (6.3174e-01)	Acc@1  84.00 ( 79.40)	Acc@5 100.00 ( 98.65)
Test: [ 90/100]	Time  0.017 ( 0.023)	Loss 3.9387e-01 (6.3089e-01)	Acc@1  88.00 ( 79.46)	Acc@5 100.00 ( 98.69)
 * Acc@1 79.510 Acc@5 98.680
### epoch[14] execution time: 18.35962200164795
EPOCH 15
# current learning rate: 0.1
# Switched to train mode...
Epoch: [15][  0/391]	Time  0.215 ( 0.215)	Data  0.175 ( 0.175)	Loss 4.1237e-01 (4.1237e-01)	Acc@1  88.28 ( 88.28)	Acc@5 100.00 (100.00)
Epoch: [15][ 10/391]	Time  0.039 ( 0.056)	Data  0.001 ( 0.017)	Loss 4.8331e-01 (4.4565e-01)	Acc@1  83.59 ( 85.58)	Acc@5  99.22 ( 99.36)
Epoch: [15][ 20/391]	Time  0.041 ( 0.049)	Data  0.001 ( 0.009)	Loss 3.5303e-01 (4.0234e-01)	Acc@1  90.62 ( 86.76)	Acc@5 100.00 ( 99.55)
Epoch: [15][ 30/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.007)	Loss 4.2917e-01 (4.0158e-01)	Acc@1  88.28 ( 86.87)	Acc@5  99.22 ( 99.45)
Epoch: [15][ 40/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.005)	Loss 3.6361e-01 (3.9082e-01)	Acc@1  87.50 ( 87.08)	Acc@5  98.44 ( 99.41)
Epoch: [15][ 50/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.004)	Loss 2.9530e-01 (3.9676e-01)	Acc@1  89.84 ( 86.78)	Acc@5 100.00 ( 99.40)
Epoch: [15][ 60/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.004)	Loss 3.0535e-01 (3.9132e-01)	Acc@1  86.72 ( 86.76)	Acc@5 100.00 ( 99.47)
Epoch: [15][ 70/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.8333e-01 (3.9374e-01)	Acc@1  85.16 ( 86.67)	Acc@5  99.22 ( 99.46)
Epoch: [15][ 80/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.8610e-01 (3.9523e-01)	Acc@1  85.16 ( 86.54)	Acc@5 100.00 ( 99.50)
Epoch: [15][ 90/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.7519e-01 (3.9732e-01)	Acc@1  84.38 ( 86.46)	Acc@5  98.44 ( 99.47)
Epoch: [15][100/391]	Time  0.035 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.0129e-01 (3.9225e-01)	Acc@1  86.72 ( 86.57)	Acc@5 100.00 ( 99.49)
Epoch: [15][110/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 3.0189e-01 (3.9392e-01)	Acc@1  88.28 ( 86.46)	Acc@5 100.00 ( 99.49)
Epoch: [15][120/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0477e-01 (3.9870e-01)	Acc@1  82.03 ( 86.31)	Acc@5  98.44 ( 99.48)
Epoch: [15][130/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.2190e-01 (4.0144e-01)	Acc@1  80.47 ( 86.18)	Acc@5  96.88 ( 99.47)
Epoch: [15][140/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8089e-01 (4.0190e-01)	Acc@1  85.16 ( 86.25)	Acc@5 100.00 ( 99.47)
Epoch: [15][150/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1468e-01 (4.0361e-01)	Acc@1  82.81 ( 86.14)	Acc@5  99.22 ( 99.45)
Epoch: [15][160/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3834e-01 (4.0239e-01)	Acc@1  89.06 ( 86.22)	Acc@5  99.22 ( 99.45)
Epoch: [15][170/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0602e-01 (4.0219e-01)	Acc@1  81.25 ( 86.19)	Acc@5  99.22 ( 99.43)
Epoch: [15][180/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6532e-01 (4.0203e-01)	Acc@1  86.72 ( 86.16)	Acc@5  99.22 ( 99.43)
Epoch: [15][190/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.5886e-01 (4.0322e-01)	Acc@1  85.16 ( 86.14)	Acc@5 100.00 ( 99.44)
Epoch: [15][200/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1184e-01 (4.0457e-01)	Acc@1  77.34 ( 86.05)	Acc@5 100.00 ( 99.45)
Epoch: [15][210/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1286e-01 (4.0631e-01)	Acc@1  82.81 ( 85.95)	Acc@5  99.22 ( 99.44)
Epoch: [15][220/391]	Time  0.055 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3129e-01 (4.0684e-01)	Acc@1  84.38 ( 85.93)	Acc@5 100.00 ( 99.43)
Epoch: [15][230/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4483e-01 (4.0779e-01)	Acc@1  82.81 ( 85.90)	Acc@5  98.44 ( 99.43)
Epoch: [15][240/391]	Time  0.046 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4551e-01 (4.0759e-01)	Acc@1  85.16 ( 85.87)	Acc@5 100.00 ( 99.42)
Epoch: [15][250/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3789e-01 (4.0693e-01)	Acc@1  81.25 ( 85.87)	Acc@5 100.00 ( 99.43)
Epoch: [15][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9055e-01 (4.0525e-01)	Acc@1  83.59 ( 85.90)	Acc@5  99.22 ( 99.44)
Epoch: [15][270/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2000e-01 (4.0510e-01)	Acc@1  89.06 ( 85.87)	Acc@5  97.66 ( 99.43)
Epoch: [15][280/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8323e-01 (4.0754e-01)	Acc@1  84.38 ( 85.80)	Acc@5  99.22 ( 99.43)
Epoch: [15][290/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2069e-01 (4.0811e-01)	Acc@1  78.91 ( 85.75)	Acc@5  99.22 ( 99.43)
Epoch: [15][300/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3420e-01 (4.0705e-01)	Acc@1  86.72 ( 85.78)	Acc@5  98.44 ( 99.43)
Epoch: [15][310/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.2619e-01 (4.0670e-01)	Acc@1  83.59 ( 85.79)	Acc@5  99.22 ( 99.44)
Epoch: [15][320/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.3567e-01 (4.0672e-01)	Acc@1  82.81 ( 85.78)	Acc@5  99.22 ( 99.44)
Epoch: [15][330/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.2372e-01 (4.0562e-01)	Acc@1  90.62 ( 85.81)	Acc@5  99.22 ( 99.43)
Epoch: [15][340/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.0961e-01 (4.0546e-01)	Acc@1  85.16 ( 85.80)	Acc@5  98.44 ( 99.44)
Epoch: [15][350/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.2838e-01 (4.0537e-01)	Acc@1  85.16 ( 85.80)	Acc@5  98.44 ( 99.43)
Epoch: [15][360/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.4272e-01 (4.0509e-01)	Acc@1  84.38 ( 85.80)	Acc@5 100.00 ( 99.44)
Epoch: [15][370/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.6184e-01 (4.0517e-01)	Acc@1  83.59 ( 85.81)	Acc@5 100.00 ( 99.45)
Epoch: [15][380/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.1818e-01 (4.0518e-01)	Acc@1  84.38 ( 85.81)	Acc@5  98.44 ( 99.45)
Epoch: [15][390/391]	Time  0.027 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.9448e-01 (4.0533e-01)	Acc@1  82.50 ( 85.80)	Acc@5  98.75 ( 99.45)
## e[15] optimizer.zero_grad (sum) time: 0.2649819850921631
## e[15]       loss.backward (sum) time: 3.9700586795806885
## e[15]      optimizer.step (sum) time: 1.8337516784667969
## epoch[15] training(only) time: 15.970873832702637
# Switched to evaluate mode...
Test: [  0/100]	Time  0.182 ( 0.182)	Loss 6.8487e-01 (6.8487e-01)	Acc@1  75.00 ( 75.00)	Acc@5  97.00 ( 97.00)
Test: [ 10/100]	Time  0.018 ( 0.037)	Loss 6.0944e-01 (6.1391e-01)	Acc@1  83.00 ( 80.82)	Acc@5  99.00 ( 99.00)
Test: [ 20/100]	Time  0.017 ( 0.030)	Loss 6.8611e-01 (6.1440e-01)	Acc@1  75.00 ( 80.24)	Acc@5 100.00 ( 99.00)
Test: [ 30/100]	Time  0.021 ( 0.027)	Loss 7.3964e-01 (6.3224e-01)	Acc@1  79.00 ( 79.74)	Acc@5  98.00 ( 99.03)
Test: [ 40/100]	Time  0.016 ( 0.025)	Loss 6.7532e-01 (6.5001e-01)	Acc@1  79.00 ( 79.46)	Acc@5 100.00 ( 98.90)
Test: [ 50/100]	Time  0.021 ( 0.024)	Loss 7.2219e-01 (6.3550e-01)	Acc@1  81.00 ( 79.92)	Acc@5  98.00 ( 98.94)
Test: [ 60/100]	Time  0.021 ( 0.024)	Loss 7.6937e-01 (6.4536e-01)	Acc@1  79.00 ( 79.61)	Acc@5  99.00 ( 98.90)
Test: [ 70/100]	Time  0.021 ( 0.024)	Loss 6.2860e-01 (6.3340e-01)	Acc@1  76.00 ( 79.80)	Acc@5  99.00 ( 98.93)
Test: [ 80/100]	Time  0.017 ( 0.023)	Loss 7.3725e-01 (6.3199e-01)	Acc@1  78.00 ( 79.89)	Acc@5  99.00 ( 98.95)
Test: [ 90/100]	Time  0.024 ( 0.023)	Loss 7.0395e-01 (6.3607e-01)	Acc@1  78.00 ( 79.69)	Acc@5  98.00 ( 98.92)
 * Acc@1 79.960 Acc@5 98.950
### epoch[15] execution time: 18.39962673187256
EPOCH 16
# current learning rate: 0.1
# Switched to train mode...
Epoch: [16][  0/391]	Time  0.214 ( 0.214)	Data  0.166 ( 0.166)	Loss 4.0824e-01 (4.0824e-01)	Acc@1  86.72 ( 86.72)	Acc@5 100.00 (100.00)
Epoch: [16][ 10/391]	Time  0.039 ( 0.057)	Data  0.001 ( 0.016)	Loss 3.5203e-01 (3.9983e-01)	Acc@1  87.50 ( 85.65)	Acc@5 100.00 ( 99.50)
Epoch: [16][ 20/391]	Time  0.035 ( 0.049)	Data  0.001 ( 0.009)	Loss 4.6353e-01 (3.9288e-01)	Acc@1  83.59 ( 86.09)	Acc@5 100.00 ( 99.55)
Epoch: [16][ 30/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.006)	Loss 3.5660e-01 (3.9284e-01)	Acc@1  88.28 ( 86.24)	Acc@5 100.00 ( 99.55)
Epoch: [16][ 40/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.005)	Loss 3.8554e-01 (4.0272e-01)	Acc@1  86.72 ( 86.05)	Acc@5 100.00 ( 99.49)
Epoch: [16][ 50/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 3.1319e-01 (4.0058e-01)	Acc@1  88.28 ( 86.08)	Acc@5 100.00 ( 99.43)
Epoch: [16][ 60/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.004)	Loss 4.1810e-01 (3.9042e-01)	Acc@1  87.50 ( 86.49)	Acc@5  99.22 ( 99.46)
Epoch: [16][ 70/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.4226e-01 (3.9173e-01)	Acc@1  87.50 ( 86.45)	Acc@5 100.00 ( 99.46)
Epoch: [16][ 80/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.5484e-01 (3.8736e-01)	Acc@1  84.38 ( 86.63)	Acc@5  99.22 ( 99.47)
Epoch: [16][ 90/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.7174e-01 (3.8163e-01)	Acc@1  86.72 ( 86.85)	Acc@5  99.22 ( 99.48)
Epoch: [16][100/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.0610e-01 (3.8222e-01)	Acc@1  85.94 ( 86.78)	Acc@5  99.22 ( 99.51)
Epoch: [16][110/391]	Time  0.052 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.2682e-01 (3.8138e-01)	Acc@1  84.38 ( 86.78)	Acc@5 100.00 ( 99.52)
Epoch: [16][120/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3581e-01 (3.8278e-01)	Acc@1  82.03 ( 86.71)	Acc@5 100.00 ( 99.52)
Epoch: [16][130/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4633e-01 (3.8700e-01)	Acc@1  80.47 ( 86.52)	Acc@5 100.00 ( 99.51)
Epoch: [16][140/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2137e-01 (3.8569e-01)	Acc@1  85.16 ( 86.53)	Acc@5 100.00 ( 99.50)
Epoch: [16][150/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9710e-01 (3.8607e-01)	Acc@1  80.47 ( 86.49)	Acc@5 100.00 ( 99.50)
Epoch: [16][160/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9679e-01 (3.8834e-01)	Acc@1  82.81 ( 86.38)	Acc@5  98.44 ( 99.50)
Epoch: [16][170/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7052e-01 (3.8963e-01)	Acc@1  88.28 ( 86.39)	Acc@5  98.44 ( 99.47)
Epoch: [16][180/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2607e-01 (3.9042e-01)	Acc@1  85.16 ( 86.39)	Acc@5  99.22 ( 99.46)
Epoch: [16][190/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7037e-01 (3.9110e-01)	Acc@1  85.94 ( 86.41)	Acc@5  99.22 ( 99.46)
Epoch: [16][200/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3466e-01 (3.9116e-01)	Acc@1  89.06 ( 86.44)	Acc@5 100.00 ( 99.46)
Epoch: [16][210/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4109e-01 (3.9037e-01)	Acc@1  85.94 ( 86.46)	Acc@5  99.22 ( 99.46)
Epoch: [16][220/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.7584e-01 (3.9039e-01)	Acc@1  92.97 ( 86.48)	Acc@5 100.00 ( 99.46)
Epoch: [16][230/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.3008e-01 (3.8860e-01)	Acc@1  93.75 ( 86.56)	Acc@5  99.22 ( 99.48)
Epoch: [16][240/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1549e-01 (3.8821e-01)	Acc@1  85.94 ( 86.60)	Acc@5 100.00 ( 99.47)
Epoch: [16][250/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0927e-01 (3.8652e-01)	Acc@1  85.94 ( 86.63)	Acc@5 100.00 ( 99.49)
Epoch: [16][260/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7817e-01 (3.8639e-01)	Acc@1  85.94 ( 86.63)	Acc@5 100.00 ( 99.49)
Epoch: [16][270/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5202e-01 (3.8733e-01)	Acc@1  89.06 ( 86.59)	Acc@5 100.00 ( 99.48)
Epoch: [16][280/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.4082e-01 (3.8792e-01)	Acc@1  83.59 ( 86.59)	Acc@5  96.88 ( 99.47)
Epoch: [16][290/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7541e-01 (3.9013e-01)	Acc@1  82.03 ( 86.46)	Acc@5 100.00 ( 99.48)
Epoch: [16][300/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.6772e-01 (3.9251e-01)	Acc@1  81.25 ( 86.35)	Acc@5  97.66 ( 99.48)
Epoch: [16][310/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1006e-01 (3.9381e-01)	Acc@1  83.59 ( 86.31)	Acc@5 100.00 ( 99.47)
Epoch: [16][320/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.8699e-01 (3.9419e-01)	Acc@1  77.34 ( 86.26)	Acc@5  99.22 ( 99.47)
Epoch: [16][330/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.9139e-01 (3.9226e-01)	Acc@1  87.50 ( 86.35)	Acc@5  98.44 ( 99.48)
Epoch: [16][340/391]	Time  0.053 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.0112e-01 (3.9295e-01)	Acc@1  89.84 ( 86.32)	Acc@5  99.22 ( 99.47)
Epoch: [16][350/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.9217e-01 (3.9184e-01)	Acc@1  85.16 ( 86.32)	Acc@5 100.00 ( 99.48)
Epoch: [16][360/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.1678e-01 (3.9157e-01)	Acc@1  84.38 ( 86.32)	Acc@5  99.22 ( 99.49)
Epoch: [16][370/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.1764e-01 (3.9199e-01)	Acc@1  87.50 ( 86.31)	Acc@5  97.66 ( 99.48)
Epoch: [16][380/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.5010e-01 (3.9206e-01)	Acc@1  84.38 ( 86.31)	Acc@5 100.00 ( 99.48)
Epoch: [16][390/391]	Time  0.029 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.2432e-01 (3.9279e-01)	Acc@1  80.00 ( 86.27)	Acc@5  98.75 ( 99.48)
## e[16] optimizer.zero_grad (sum) time: 0.2631714344024658
## e[16]       loss.backward (sum) time: 4.037344694137573
## e[16]      optimizer.step (sum) time: 1.7859482765197754
## epoch[16] training(only) time: 16.019733905792236
# Switched to evaluate mode...
Test: [  0/100]	Time  0.221 ( 0.221)	Loss 7.3941e-01 (7.3941e-01)	Acc@1  76.00 ( 76.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.022 ( 0.039)	Loss 6.4711e-01 (5.9864e-01)	Acc@1  80.00 ( 80.64)	Acc@5  99.00 ( 99.18)
Test: [ 20/100]	Time  0.020 ( 0.029)	Loss 8.1643e-01 (5.8565e-01)	Acc@1  71.00 ( 80.81)	Acc@5  99.00 ( 99.00)
Test: [ 30/100]	Time  0.024 ( 0.027)	Loss 6.0390e-01 (5.8348e-01)	Acc@1  81.00 ( 80.87)	Acc@5  98.00 ( 98.87)
Test: [ 40/100]	Time  0.021 ( 0.026)	Loss 5.5783e-01 (5.8053e-01)	Acc@1  83.00 ( 80.98)	Acc@5  99.00 ( 98.80)
Test: [ 50/100]	Time  0.019 ( 0.025)	Loss 4.8904e-01 (5.6450e-01)	Acc@1  84.00 ( 81.41)	Acc@5  99.00 ( 98.92)
Test: [ 60/100]	Time  0.021 ( 0.024)	Loss 6.2968e-01 (5.6839e-01)	Acc@1  84.00 ( 81.39)	Acc@5  99.00 ( 98.97)
Test: [ 70/100]	Time  0.016 ( 0.023)	Loss 4.3561e-01 (5.6675e-01)	Acc@1  87.00 ( 81.46)	Acc@5  98.00 ( 99.01)
Test: [ 80/100]	Time  0.024 ( 0.023)	Loss 3.9895e-01 (5.6262e-01)	Acc@1  86.00 ( 81.48)	Acc@5 100.00 ( 99.05)
Test: [ 90/100]	Time  0.022 ( 0.023)	Loss 5.6059e-01 (5.6568e-01)	Acc@1  84.00 ( 81.45)	Acc@5  98.00 ( 99.01)
 * Acc@1 81.540 Acc@5 99.010
### epoch[16] execution time: 18.40160298347473
EPOCH 17
# current learning rate: 0.1
# Switched to train mode...
Epoch: [17][  0/391]	Time  0.255 ( 0.255)	Data  0.206 ( 0.206)	Loss 3.9089e-01 (3.9089e-01)	Acc@1  83.59 ( 83.59)	Acc@5 100.00 (100.00)
Epoch: [17][ 10/391]	Time  0.042 ( 0.060)	Data  0.001 ( 0.020)	Loss 4.1385e-01 (3.7276e-01)	Acc@1  91.41 ( 87.50)	Acc@5  98.44 ( 99.50)
Epoch: [17][ 20/391]	Time  0.040 ( 0.051)	Data  0.001 ( 0.011)	Loss 4.3412e-01 (3.7282e-01)	Acc@1  84.38 ( 86.94)	Acc@5  99.22 ( 99.44)
Epoch: [17][ 30/391]	Time  0.040 ( 0.047)	Data  0.001 ( 0.008)	Loss 3.5850e-01 (3.8520e-01)	Acc@1  89.06 ( 86.82)	Acc@5 100.00 ( 99.45)
Epoch: [17][ 40/391]	Time  0.045 ( 0.046)	Data  0.001 ( 0.006)	Loss 3.4459e-01 (3.8829e-01)	Acc@1  88.28 ( 86.79)	Acc@5  98.44 ( 99.31)
Epoch: [17][ 50/391]	Time  0.038 ( 0.045)	Data  0.001 ( 0.005)	Loss 3.4238e-01 (3.8097e-01)	Acc@1  89.84 ( 87.09)	Acc@5  99.22 ( 99.36)
Epoch: [17][ 60/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.004)	Loss 2.7003e-01 (3.8150e-01)	Acc@1  92.97 ( 87.27)	Acc@5 100.00 ( 99.37)
Epoch: [17][ 70/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.004)	Loss 3.0955e-01 (3.8353e-01)	Acc@1  92.97 ( 87.22)	Acc@5  99.22 ( 99.42)
Epoch: [17][ 80/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.2234e-01 (3.8471e-01)	Acc@1  87.50 ( 87.06)	Acc@5  99.22 ( 99.42)
Epoch: [17][ 90/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.0703e-01 (3.8415e-01)	Acc@1  89.06 ( 86.96)	Acc@5 100.00 ( 99.45)
Epoch: [17][100/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.7891e-01 (3.8481e-01)	Acc@1  83.59 ( 86.91)	Acc@5 100.00 ( 99.43)
Epoch: [17][110/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.2285e-01 (3.8110e-01)	Acc@1  89.06 ( 87.02)	Acc@5 100.00 ( 99.46)
Epoch: [17][120/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.4002e-01 (3.7863e-01)	Acc@1  87.50 ( 87.11)	Acc@5  99.22 ( 99.47)
Epoch: [17][130/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.1002e-01 (3.7886e-01)	Acc@1  86.72 ( 87.04)	Acc@5  97.66 ( 99.48)
Epoch: [17][140/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.7903e-01 (3.7786e-01)	Acc@1  87.50 ( 87.07)	Acc@5  99.22 ( 99.49)
Epoch: [17][150/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.3688e-01 (3.7697e-01)	Acc@1  85.94 ( 87.14)	Acc@5 100.00 ( 99.51)
Epoch: [17][160/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.0150e-01 (3.7639e-01)	Acc@1  92.19 ( 87.16)	Acc@5 100.00 ( 99.50)
Epoch: [17][170/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.9754e-01 (3.7819e-01)	Acc@1  89.84 ( 87.14)	Acc@5 100.00 ( 99.50)
Epoch: [17][180/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7692e-01 (3.8014e-01)	Acc@1  87.50 ( 87.07)	Acc@5 100.00 ( 99.52)
Epoch: [17][190/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8149e-01 (3.7981e-01)	Acc@1  86.72 ( 87.12)	Acc@5 100.00 ( 99.53)
Epoch: [17][200/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9045e-01 (3.8182e-01)	Acc@1  80.47 ( 87.01)	Acc@5 100.00 ( 99.51)
Epoch: [17][210/391]	Time  0.039 ( 0.041)	Data  0.002 ( 0.002)	Loss 4.1456e-01 (3.8404e-01)	Acc@1  85.94 ( 86.94)	Acc@5 100.00 ( 99.51)
Epoch: [17][220/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9176e-01 (3.8515e-01)	Acc@1  87.50 ( 86.89)	Acc@5 100.00 ( 99.51)
Epoch: [17][230/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3567e-01 (3.8306e-01)	Acc@1  87.50 ( 86.94)	Acc@5 100.00 ( 99.53)
Epoch: [17][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7940e-01 (3.8486e-01)	Acc@1  87.50 ( 86.87)	Acc@5 100.00 ( 99.52)
Epoch: [17][250/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4641e-01 (3.8510e-01)	Acc@1  86.72 ( 86.82)	Acc@5 100.00 ( 99.53)
Epoch: [17][260/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0443e-01 (3.8360e-01)	Acc@1  88.28 ( 86.91)	Acc@5  99.22 ( 99.53)
Epoch: [17][270/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1531e-01 (3.8267e-01)	Acc@1  89.84 ( 86.94)	Acc@5 100.00 ( 99.53)
Epoch: [17][280/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0874e-01 (3.8181e-01)	Acc@1  88.28 ( 86.96)	Acc@5  99.22 ( 99.53)
Epoch: [17][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.7721e-01 (3.8244e-01)	Acc@1  89.84 ( 86.95)	Acc@5  99.22 ( 99.52)
Epoch: [17][300/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.9827e-01 (3.8203e-01)	Acc@1  85.94 ( 86.95)	Acc@5 100.00 ( 99.51)
Epoch: [17][310/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.5748e-01 (3.8226e-01)	Acc@1  85.94 ( 86.94)	Acc@5 100.00 ( 99.52)
Epoch: [17][320/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.0515e-01 (3.8122e-01)	Acc@1  89.06 ( 86.98)	Acc@5 100.00 ( 99.52)
Epoch: [17][330/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0117e-01 (3.8160e-01)	Acc@1  84.38 ( 86.96)	Acc@5  99.22 ( 99.52)
Epoch: [17][340/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8059e-01 (3.8227e-01)	Acc@1  82.81 ( 86.91)	Acc@5 100.00 ( 99.51)
Epoch: [17][350/391]	Time  0.039 ( 0.041)	Data  0.002 ( 0.002)	Loss 3.8066e-01 (3.8234e-01)	Acc@1  87.50 ( 86.88)	Acc@5 100.00 ( 99.52)
Epoch: [17][360/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8042e-01 (3.8386e-01)	Acc@1  85.94 ( 86.83)	Acc@5  99.22 ( 99.51)
Epoch: [17][370/391]	Time  0.038 ( 0.041)	Data  0.002 ( 0.002)	Loss 4.1030e-01 (3.8325e-01)	Acc@1  83.59 ( 86.84)	Acc@5  98.44 ( 99.52)
Epoch: [17][380/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0258e-01 (3.8331e-01)	Acc@1  89.06 ( 86.87)	Acc@5  98.44 ( 99.52)
Epoch: [17][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.002)	Loss 2.6956e-01 (3.8266e-01)	Acc@1  87.50 ( 86.88)	Acc@5 100.00 ( 99.52)
## e[17] optimizer.zero_grad (sum) time: 0.26257753372192383
## e[17]       loss.backward (sum) time: 3.932623863220215
## e[17]      optimizer.step (sum) time: 1.8309123516082764
## epoch[17] training(only) time: 15.984940528869629
# Switched to evaluate mode...
Test: [  0/100]	Time  0.177 ( 0.177)	Loss 5.2458e-01 (5.2458e-01)	Acc@1  84.00 ( 84.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.022 ( 0.036)	Loss 6.1555e-01 (4.6942e-01)	Acc@1  80.00 ( 84.36)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.016 ( 0.027)	Loss 7.2659e-01 (4.7306e-01)	Acc@1  79.00 ( 84.29)	Acc@5  99.00 ( 99.38)
Test: [ 30/100]	Time  0.018 ( 0.025)	Loss 4.6819e-01 (4.8372e-01)	Acc@1  83.00 ( 84.52)	Acc@5  99.00 ( 99.29)
Test: [ 40/100]	Time  0.024 ( 0.023)	Loss 6.1456e-01 (5.0751e-01)	Acc@1  79.00 ( 83.63)	Acc@5  98.00 ( 99.17)
Test: [ 50/100]	Time  0.024 ( 0.023)	Loss 4.1666e-01 (4.9918e-01)	Acc@1  85.00 ( 83.98)	Acc@5  99.00 ( 99.14)
Test: [ 60/100]	Time  0.020 ( 0.023)	Loss 5.1937e-01 (4.9680e-01)	Acc@1  81.00 ( 83.84)	Acc@5 100.00 ( 99.16)
Test: [ 70/100]	Time  0.018 ( 0.022)	Loss 5.1504e-01 (4.9438e-01)	Acc@1  82.00 ( 83.85)	Acc@5  99.00 ( 99.17)
Test: [ 80/100]	Time  0.017 ( 0.022)	Loss 3.3120e-01 (4.9086e-01)	Acc@1  89.00 ( 83.84)	Acc@5 100.00 ( 99.22)
Test: [ 90/100]	Time  0.024 ( 0.022)	Loss 2.9774e-01 (4.8959e-01)	Acc@1  88.00 ( 83.78)	Acc@5 100.00 ( 99.21)
 * Acc@1 83.710 Acc@5 99.230
### epoch[17] execution time: 18.287752866744995
EPOCH 18
# current learning rate: 0.1
# Switched to train mode...
Epoch: [18][  0/391]	Time  0.213 ( 0.213)	Data  0.170 ( 0.170)	Loss 3.7849e-01 (3.7849e-01)	Acc@1  90.62 ( 90.62)	Acc@5  98.44 ( 98.44)
Epoch: [18][ 10/391]	Time  0.042 ( 0.056)	Data  0.001 ( 0.016)	Loss 2.2517e-01 (3.5653e-01)	Acc@1  92.19 ( 88.35)	Acc@5 100.00 ( 99.29)
Epoch: [18][ 20/391]	Time  0.039 ( 0.049)	Data  0.001 ( 0.009)	Loss 2.9678e-01 (3.5641e-01)	Acc@1  86.72 ( 87.72)	Acc@5  99.22 ( 99.40)
Epoch: [18][ 30/391]	Time  0.042 ( 0.046)	Data  0.001 ( 0.006)	Loss 4.4747e-01 (3.5954e-01)	Acc@1  85.94 ( 87.63)	Acc@5 100.00 ( 99.47)
Epoch: [18][ 40/391]	Time  0.042 ( 0.045)	Data  0.001 ( 0.005)	Loss 3.0954e-01 (3.5606e-01)	Acc@1  89.84 ( 87.69)	Acc@5  99.22 ( 99.54)
Epoch: [18][ 50/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.004)	Loss 3.0757e-01 (3.6579e-01)	Acc@1  90.62 ( 87.36)	Acc@5 100.00 ( 99.56)
Epoch: [18][ 60/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 3.0156e-01 (3.6614e-01)	Acc@1  89.84 ( 87.33)	Acc@5  98.44 ( 99.53)
Epoch: [18][ 70/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.1515e-01 (3.6675e-01)	Acc@1  88.28 ( 87.27)	Acc@5  99.22 ( 99.49)
Epoch: [18][ 80/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.1955e-01 (3.6677e-01)	Acc@1  86.72 ( 87.28)	Acc@5 100.00 ( 99.52)
Epoch: [18][ 90/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.8988e-01 (3.6832e-01)	Acc@1  85.16 ( 87.17)	Acc@5 100.00 ( 99.54)
Epoch: [18][100/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.8186e-01 (3.6960e-01)	Acc@1  92.19 ( 87.15)	Acc@5  99.22 ( 99.53)
Epoch: [18][110/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.1428e-01 (3.6730e-01)	Acc@1  87.50 ( 87.23)	Acc@5 100.00 ( 99.54)
Epoch: [18][120/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.0705e-01 (3.6987e-01)	Acc@1  85.94 ( 87.09)	Acc@5  99.22 ( 99.55)
Epoch: [18][130/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.4810e-01 (3.7362e-01)	Acc@1  85.16 ( 86.92)	Acc@5  99.22 ( 99.52)
Epoch: [18][140/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6767e-01 (3.7417e-01)	Acc@1  85.94 ( 86.91)	Acc@5 100.00 ( 99.51)
Epoch: [18][150/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.4611e-01 (3.7597e-01)	Acc@1  80.47 ( 86.87)	Acc@5  99.22 ( 99.51)
Epoch: [18][160/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1368e-01 (3.7467e-01)	Acc@1  85.94 ( 86.93)	Acc@5 100.00 ( 99.51)
Epoch: [18][170/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4304e-01 (3.7519e-01)	Acc@1  85.16 ( 86.97)	Acc@5 100.00 ( 99.51)
Epoch: [18][180/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4574e-01 (3.7392e-01)	Acc@1  89.06 ( 87.01)	Acc@5  99.22 ( 99.50)
Epoch: [18][190/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4997e-01 (3.7545e-01)	Acc@1  82.81 ( 86.89)	Acc@5  99.22 ( 99.50)
Epoch: [18][200/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2522e-01 (3.7611e-01)	Acc@1  83.59 ( 86.80)	Acc@5 100.00 ( 99.51)
Epoch: [18][210/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8124e-01 (3.7602e-01)	Acc@1  85.16 ( 86.84)	Acc@5 100.00 ( 99.51)
Epoch: [18][220/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4719e-01 (3.7755e-01)	Acc@1  81.25 ( 86.80)	Acc@5 100.00 ( 99.50)
Epoch: [18][230/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7341e-01 (3.7626e-01)	Acc@1  88.28 ( 86.82)	Acc@5  99.22 ( 99.51)
Epoch: [18][240/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9362e-01 (3.7606e-01)	Acc@1  83.59 ( 86.79)	Acc@5  99.22 ( 99.51)
Epoch: [18][250/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.0113e-01 (3.7586e-01)	Acc@1  89.84 ( 86.80)	Acc@5 100.00 ( 99.53)
Epoch: [18][260/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0406e-01 (3.7645e-01)	Acc@1  89.06 ( 86.80)	Acc@5 100.00 ( 99.53)
Epoch: [18][270/391]	Time  0.035 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.6585e-01 (3.7718e-01)	Acc@1  84.38 ( 86.75)	Acc@5 100.00 ( 99.53)
Epoch: [18][280/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.8108e-01 (3.7908e-01)	Acc@1  79.69 ( 86.70)	Acc@5  96.88 ( 99.52)
Epoch: [18][290/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1943e-01 (3.8010e-01)	Acc@1  85.16 ( 86.66)	Acc@5  97.66 ( 99.52)
Epoch: [18][300/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.6377e-01 (3.8036e-01)	Acc@1  87.50 ( 86.68)	Acc@5 100.00 ( 99.52)
Epoch: [18][310/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.0581e-01 (3.7918e-01)	Acc@1  89.06 ( 86.72)	Acc@5 100.00 ( 99.53)
Epoch: [18][320/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.7189e-01 (3.7846e-01)	Acc@1  91.41 ( 86.75)	Acc@5 100.00 ( 99.53)
Epoch: [18][330/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.5213e-01 (3.7769e-01)	Acc@1  84.38 ( 86.78)	Acc@5 100.00 ( 99.53)
Epoch: [18][340/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.3562e-01 (3.7752e-01)	Acc@1  89.84 ( 86.82)	Acc@5 100.00 ( 99.52)
Epoch: [18][350/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0982e-01 (3.7706e-01)	Acc@1  85.94 ( 86.85)	Acc@5 100.00 ( 99.53)
Epoch: [18][360/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.7632e-01 (3.7562e-01)	Acc@1  90.62 ( 86.88)	Acc@5  99.22 ( 99.53)
Epoch: [18][370/391]	Time  0.042 ( 0.041)	Data  0.002 ( 0.001)	Loss 3.8223e-01 (3.7427e-01)	Acc@1  89.06 ( 86.94)	Acc@5  99.22 ( 99.53)
Epoch: [18][380/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.2016e-01 (3.7463e-01)	Acc@1  86.72 ( 86.93)	Acc@5  99.22 ( 99.53)
Epoch: [18][390/391]	Time  0.026 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.2086e-01 (3.7461e-01)	Acc@1  86.25 ( 86.92)	Acc@5  97.50 ( 99.52)
## e[18] optimizer.zero_grad (sum) time: 0.2634005546569824
## e[18]       loss.backward (sum) time: 3.940490245819092
## e[18]      optimizer.step (sum) time: 1.7936019897460938
## epoch[18] training(only) time: 15.977185487747192
# Switched to evaluate mode...
Test: [  0/100]	Time  0.177 ( 0.177)	Loss 4.7563e-01 (4.7563e-01)	Acc@1  83.00 ( 83.00)	Acc@5  97.00 ( 97.00)
Test: [ 10/100]	Time  0.018 ( 0.035)	Loss 5.0024e-01 (4.5693e-01)	Acc@1  83.00 ( 84.45)	Acc@5  99.00 ( 99.36)
Test: [ 20/100]	Time  0.018 ( 0.028)	Loss 5.0935e-01 (4.5185e-01)	Acc@1  84.00 ( 84.43)	Acc@5 100.00 ( 99.33)
Test: [ 30/100]	Time  0.023 ( 0.026)	Loss 5.5197e-01 (4.5159e-01)	Acc@1  82.00 ( 84.65)	Acc@5  99.00 ( 99.26)
Test: [ 40/100]	Time  0.018 ( 0.024)	Loss 4.9954e-01 (4.5563e-01)	Acc@1  87.00 ( 84.49)	Acc@5 100.00 ( 99.29)
Test: [ 50/100]	Time  0.018 ( 0.023)	Loss 3.9721e-01 (4.4921e-01)	Acc@1  88.00 ( 84.73)	Acc@5  99.00 ( 99.33)
Test: [ 60/100]	Time  0.023 ( 0.023)	Loss 4.5541e-01 (4.5210e-01)	Acc@1  86.00 ( 84.62)	Acc@5 100.00 ( 99.43)
Test: [ 70/100]	Time  0.021 ( 0.022)	Loss 4.6951e-01 (4.4994e-01)	Acc@1  85.00 ( 84.73)	Acc@5  99.00 ( 99.39)
Test: [ 80/100]	Time  0.024 ( 0.022)	Loss 3.4271e-01 (4.5050e-01)	Acc@1  87.00 ( 84.84)	Acc@5  99.00 ( 99.35)
Test: [ 90/100]	Time  0.023 ( 0.022)	Loss 4.8618e-01 (4.5481e-01)	Acc@1  85.00 ( 84.78)	Acc@5  99.00 ( 99.36)
 * Acc@1 84.790 Acc@5 99.410
### epoch[18] execution time: 18.32151174545288
EPOCH 19
# current learning rate: 0.1
# Switched to train mode...
Epoch: [19][  0/391]	Time  0.213 ( 0.213)	Data  0.171 ( 0.171)	Loss 2.9794e-01 (2.9794e-01)	Acc@1  89.84 ( 89.84)	Acc@5 100.00 (100.00)
Epoch: [19][ 10/391]	Time  0.042 ( 0.056)	Data  0.001 ( 0.016)	Loss 2.9955e-01 (3.7531e-01)	Acc@1  89.84 ( 86.79)	Acc@5 100.00 ( 99.50)
Epoch: [19][ 20/391]	Time  0.039 ( 0.049)	Data  0.001 ( 0.009)	Loss 2.3322e-01 (3.4797e-01)	Acc@1  92.19 ( 88.02)	Acc@5 100.00 ( 99.55)
Epoch: [19][ 30/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.006)	Loss 2.1305e-01 (3.4381e-01)	Acc@1  92.19 ( 88.41)	Acc@5 100.00 ( 99.55)
Epoch: [19][ 40/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.005)	Loss 3.5789e-01 (3.4232e-01)	Acc@1  88.28 ( 88.30)	Acc@5  99.22 ( 99.52)
Epoch: [19][ 50/391]	Time  0.037 ( 0.043)	Data  0.001 ( 0.004)	Loss 4.2822e-01 (3.4211e-01)	Acc@1  87.50 ( 88.34)	Acc@5 100.00 ( 99.56)
Epoch: [19][ 60/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 4.0766e-01 (3.4589e-01)	Acc@1  84.38 ( 88.09)	Acc@5  98.44 ( 99.54)
Epoch: [19][ 70/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.8773e-01 (3.4409e-01)	Acc@1  92.97 ( 88.12)	Acc@5 100.00 ( 99.58)
Epoch: [19][ 80/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.6041e-01 (3.4425e-01)	Acc@1  89.84 ( 88.19)	Acc@5 100.00 ( 99.56)
Epoch: [19][ 90/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.0622e-01 (3.4989e-01)	Acc@1  91.41 ( 88.13)	Acc@5 100.00 ( 99.54)
Epoch: [19][100/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.6323e-01 (3.5087e-01)	Acc@1  89.84 ( 88.10)	Acc@5 100.00 ( 99.52)
Epoch: [19][110/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.8100e-01 (3.5263e-01)	Acc@1  84.38 ( 87.90)	Acc@5 100.00 ( 99.52)
Epoch: [19][120/391]	Time  0.035 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.0271e-01 (3.5578e-01)	Acc@1  89.84 ( 87.82)	Acc@5 100.00 ( 99.51)
Epoch: [19][130/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4685e-01 (3.5538e-01)	Acc@1  85.94 ( 87.80)	Acc@5 100.00 ( 99.53)
Epoch: [19][140/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5116e-01 (3.5550e-01)	Acc@1  89.06 ( 87.82)	Acc@5 100.00 ( 99.56)
Epoch: [19][150/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.5132e-01 (3.5628e-01)	Acc@1  91.41 ( 87.76)	Acc@5 100.00 ( 99.56)
Epoch: [19][160/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4517e-01 (3.5789e-01)	Acc@1  82.81 ( 87.70)	Acc@5  99.22 ( 99.56)
Epoch: [19][170/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6259e-01 (3.6056e-01)	Acc@1  85.94 ( 87.64)	Acc@5 100.00 ( 99.58)
Epoch: [19][180/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8018e-01 (3.6240e-01)	Acc@1  89.84 ( 87.57)	Acc@5 100.00 ( 99.60)
Epoch: [19][190/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2580e-01 (3.6489e-01)	Acc@1  82.81 ( 87.47)	Acc@5  99.22 ( 99.59)
Epoch: [19][200/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.0439e-01 (3.6574e-01)	Acc@1  89.84 ( 87.45)	Acc@5 100.00 ( 99.60)
Epoch: [19][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9464e-01 (3.6627e-01)	Acc@1  85.94 ( 87.43)	Acc@5  99.22 ( 99.60)
Epoch: [19][220/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8672e-01 (3.6670e-01)	Acc@1  86.72 ( 87.42)	Acc@5 100.00 ( 99.60)
Epoch: [19][230/391]	Time  0.045 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8587e-01 (3.6659e-01)	Acc@1  85.94 ( 87.43)	Acc@5  99.22 ( 99.59)
Epoch: [19][240/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8674e-01 (3.6569e-01)	Acc@1  85.94 ( 87.43)	Acc@5 100.00 ( 99.60)
Epoch: [19][250/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.7414e-01 (3.6494e-01)	Acc@1  92.19 ( 87.43)	Acc@5 100.00 ( 99.60)
Epoch: [19][260/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.7862e-01 (3.6453e-01)	Acc@1  89.06 ( 87.45)	Acc@5 100.00 ( 99.60)
Epoch: [19][270/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1022e-01 (3.6394e-01)	Acc@1  84.38 ( 87.45)	Acc@5  99.22 ( 99.59)
Epoch: [19][280/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1518e-01 (3.6289e-01)	Acc@1  89.06 ( 87.47)	Acc@5 100.00 ( 99.60)
Epoch: [19][290/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8570e-01 (3.6326e-01)	Acc@1  86.72 ( 87.44)	Acc@5 100.00 ( 99.59)
Epoch: [19][300/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4287e-01 (3.6390e-01)	Acc@1  82.03 ( 87.41)	Acc@5  99.22 ( 99.59)
Epoch: [19][310/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2987e-01 (3.6370e-01)	Acc@1  88.28 ( 87.41)	Acc@5  98.44 ( 99.59)
Epoch: [19][320/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.7182e-01 (3.6435e-01)	Acc@1  85.16 ( 87.42)	Acc@5 100.00 ( 99.58)
Epoch: [19][330/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.7057e-01 (3.6517e-01)	Acc@1  86.72 ( 87.37)	Acc@5  99.22 ( 99.59)
Epoch: [19][340/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.2639e-01 (3.6608e-01)	Acc@1  92.19 ( 87.34)	Acc@5 100.00 ( 99.59)
Epoch: [19][350/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.3707e-01 (3.6613e-01)	Acc@1  85.94 ( 87.33)	Acc@5 100.00 ( 99.59)
Epoch: [19][360/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.9914e-01 (3.6661e-01)	Acc@1  83.59 ( 87.30)	Acc@5  99.22 ( 99.59)
Epoch: [19][370/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.1919e-01 (3.6664e-01)	Acc@1  89.84 ( 87.31)	Acc@5 100.00 ( 99.58)
Epoch: [19][380/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.9892e-01 (3.6777e-01)	Acc@1  88.28 ( 87.26)	Acc@5  99.22 ( 99.57)
Epoch: [19][390/391]	Time  0.033 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.4587e-01 (3.6796e-01)	Acc@1  85.00 ( 87.26)	Acc@5 100.00 ( 99.57)
## e[19] optimizer.zero_grad (sum) time: 0.26394033432006836
## e[19]       loss.backward (sum) time: 3.9395382404327393
## e[19]      optimizer.step (sum) time: 1.822242021560669
## epoch[19] training(only) time: 15.957166194915771
# Switched to evaluate mode...
Test: [  0/100]	Time  0.168 ( 0.168)	Loss 4.5166e-01 (4.5166e-01)	Acc@1  85.00 ( 85.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.021 ( 0.033)	Loss 5.8227e-01 (4.3927e-01)	Acc@1  80.00 ( 85.55)	Acc@5  99.00 ( 99.45)
Test: [ 20/100]	Time  0.024 ( 0.028)	Loss 6.2790e-01 (4.5424e-01)	Acc@1  81.00 ( 84.76)	Acc@5  98.00 ( 99.29)
Test: [ 30/100]	Time  0.021 ( 0.026)	Loss 4.5073e-01 (4.5365e-01)	Acc@1  89.00 ( 85.45)	Acc@5  99.00 ( 99.23)
Test: [ 40/100]	Time  0.018 ( 0.025)	Loss 4.4555e-01 (4.5410e-01)	Acc@1  84.00 ( 84.98)	Acc@5  99.00 ( 99.22)
Test: [ 50/100]	Time  0.024 ( 0.024)	Loss 4.2792e-01 (4.5053e-01)	Acc@1  85.00 ( 84.98)	Acc@5  99.00 ( 99.22)
Test: [ 60/100]	Time  0.021 ( 0.024)	Loss 4.1515e-01 (4.4951e-01)	Acc@1  86.00 ( 84.93)	Acc@5 100.00 ( 99.23)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 4.3417e-01 (4.4653e-01)	Acc@1  85.00 ( 85.04)	Acc@5  99.00 ( 99.23)
Test: [ 80/100]	Time  0.021 ( 0.023)	Loss 3.1213e-01 (4.4323e-01)	Acc@1  88.00 ( 85.06)	Acc@5 100.00 ( 99.22)
Test: [ 90/100]	Time  0.017 ( 0.023)	Loss 2.4599e-01 (4.4256e-01)	Acc@1  90.00 ( 85.05)	Acc@5 100.00 ( 99.24)
 * Acc@1 85.080 Acc@5 99.280
### epoch[19] execution time: 18.337888956069946
EPOCH 20
# current learning rate: 0.1
# Switched to train mode...
Epoch: [20][  0/391]	Time  0.258 ( 0.258)	Data  0.217 ( 0.217)	Loss 4.2535e-01 (4.2535e-01)	Acc@1  87.50 ( 87.50)	Acc@5 100.00 (100.00)
Epoch: [20][ 10/391]	Time  0.039 ( 0.060)	Data  0.001 ( 0.021)	Loss 3.8168e-01 (3.6385e-01)	Acc@1  85.16 ( 87.29)	Acc@5 100.00 ( 99.64)
Epoch: [20][ 20/391]	Time  0.040 ( 0.051)	Data  0.001 ( 0.011)	Loss 5.2121e-01 (3.5544e-01)	Acc@1  83.59 ( 87.91)	Acc@5  99.22 ( 99.48)
Epoch: [20][ 30/391]	Time  0.040 ( 0.048)	Data  0.001 ( 0.008)	Loss 3.6268e-01 (3.5206e-01)	Acc@1  85.16 ( 87.78)	Acc@5 100.00 ( 99.50)
Epoch: [20][ 40/391]	Time  0.042 ( 0.046)	Data  0.001 ( 0.006)	Loss 3.5740e-01 (3.4533e-01)	Acc@1  89.06 ( 87.90)	Acc@5 100.00 ( 99.60)
Epoch: [20][ 50/391]	Time  0.042 ( 0.045)	Data  0.001 ( 0.005)	Loss 3.3974e-01 (3.4421e-01)	Acc@1  84.38 ( 88.11)	Acc@5  99.22 ( 99.59)
Epoch: [20][ 60/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.004)	Loss 2.2526e-01 (3.3753e-01)	Acc@1  92.19 ( 88.27)	Acc@5 100.00 ( 99.59)
Epoch: [20][ 70/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.004)	Loss 3.7459e-01 (3.3837e-01)	Acc@1  85.94 ( 88.24)	Acc@5  99.22 ( 99.59)
Epoch: [20][ 80/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.004)	Loss 3.6414e-01 (3.4050e-01)	Acc@1  85.16 ( 88.21)	Acc@5 100.00 ( 99.59)
Epoch: [20][ 90/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.0171e-01 (3.3902e-01)	Acc@1  90.62 ( 88.26)	Acc@5 100.00 ( 99.61)
Epoch: [20][100/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.4121e-01 (3.4539e-01)	Acc@1  88.28 ( 88.10)	Acc@5 100.00 ( 99.57)
Epoch: [20][110/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.4076e-01 (3.4687e-01)	Acc@1  89.06 ( 88.11)	Acc@5  99.22 ( 99.60)
Epoch: [20][120/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.0758e-01 (3.4590e-01)	Acc@1  86.72 ( 88.11)	Acc@5  98.44 ( 99.61)
Epoch: [20][130/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.5147e-01 (3.4543e-01)	Acc@1  89.84 ( 88.24)	Acc@5 100.00 ( 99.61)
Epoch: [20][140/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.8453e-01 (3.4681e-01)	Acc@1  90.62 ( 88.13)	Acc@5 100.00 ( 99.62)
Epoch: [20][150/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.0375e-01 (3.4930e-01)	Acc@1  85.16 ( 88.04)	Acc@5  99.22 ( 99.60)
Epoch: [20][160/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.1059e-01 (3.4898e-01)	Acc@1  88.28 ( 88.00)	Acc@5  99.22 ( 99.60)
Epoch: [20][170/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1768e-01 (3.4965e-01)	Acc@1  85.94 ( 88.00)	Acc@5  98.44 ( 99.61)
Epoch: [20][180/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.4026e-01 (3.4879e-01)	Acc@1  89.84 ( 88.04)	Acc@5 100.00 ( 99.62)
Epoch: [20][190/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3171e-01 (3.5024e-01)	Acc@1  87.50 ( 87.99)	Acc@5 100.00 ( 99.62)
Epoch: [20][200/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3972e-01 (3.4923e-01)	Acc@1  89.84 ( 88.06)	Acc@5  99.22 ( 99.62)
Epoch: [20][210/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.0137e-01 (3.5023e-01)	Acc@1  93.75 ( 88.08)	Acc@5 100.00 ( 99.61)
Epoch: [20][220/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0926e-01 (3.5237e-01)	Acc@1  85.16 ( 87.99)	Acc@5 100.00 ( 99.61)
Epoch: [20][230/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2734e-01 (3.5442e-01)	Acc@1  86.72 ( 87.88)	Acc@5 100.00 ( 99.61)
Epoch: [20][240/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.6373e-01 (3.5493e-01)	Acc@1  84.38 ( 87.86)	Acc@5 100.00 ( 99.62)
Epoch: [20][250/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2930e-01 (3.5491e-01)	Acc@1  86.72 ( 87.85)	Acc@5  99.22 ( 99.61)
Epoch: [20][260/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8659e-01 (3.5526e-01)	Acc@1  85.94 ( 87.82)	Acc@5  99.22 ( 99.62)
Epoch: [20][270/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3908e-01 (3.5584e-01)	Acc@1  86.72 ( 87.83)	Acc@5 100.00 ( 99.61)
Epoch: [20][280/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.9945e-01 (3.5677e-01)	Acc@1  91.41 ( 87.79)	Acc@5  99.22 ( 99.61)
Epoch: [20][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2888e-01 (3.5577e-01)	Acc@1  90.62 ( 87.80)	Acc@5  99.22 ( 99.61)
Epoch: [20][300/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5917e-01 (3.5641e-01)	Acc@1  88.28 ( 87.80)	Acc@5 100.00 ( 99.61)
Epoch: [20][310/391]	Time  0.045 ( 0.041)	Data  0.002 ( 0.002)	Loss 4.3348e-01 (3.5635e-01)	Acc@1  86.72 ( 87.81)	Acc@5  99.22 ( 99.61)
Epoch: [20][320/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6818e-01 (3.5648e-01)	Acc@1  85.16 ( 87.79)	Acc@5 100.00 ( 99.61)
Epoch: [20][330/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.6526e-01 (3.5641e-01)	Acc@1  89.06 ( 87.80)	Acc@5 100.00 ( 99.61)
Epoch: [20][340/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5245e-01 (3.5549e-01)	Acc@1  88.28 ( 87.82)	Acc@5  98.44 ( 99.61)
Epoch: [20][350/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1306e-01 (3.5620e-01)	Acc@1  89.84 ( 87.83)	Acc@5  99.22 ( 99.61)
Epoch: [20][360/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.6490e-01 (3.5684e-01)	Acc@1  93.75 ( 87.81)	Acc@5  99.22 ( 99.60)
Epoch: [20][370/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8845e-01 (3.5681e-01)	Acc@1  84.38 ( 87.81)	Acc@5 100.00 ( 99.60)
Epoch: [20][380/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.1346e-01 (3.5720e-01)	Acc@1  92.19 ( 87.81)	Acc@5 100.00 ( 99.59)
Epoch: [20][390/391]	Time  0.028 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.3999e-01 (3.5788e-01)	Acc@1  83.75 ( 87.79)	Acc@5  98.75 ( 99.59)
## e[20] optimizer.zero_grad (sum) time: 0.26224446296691895
## e[20]       loss.backward (sum) time: 3.9589595794677734
## e[20]      optimizer.step (sum) time: 1.7874259948730469
## epoch[20] training(only) time: 16.06963038444519
# Switched to evaluate mode...
Test: [  0/100]	Time  0.173 ( 0.173)	Loss 5.8052e-01 (5.8052e-01)	Acc@1  86.00 ( 86.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.024 ( 0.034)	Loss 5.7960e-01 (4.7388e-01)	Acc@1  79.00 ( 84.27)	Acc@5 100.00 ( 99.55)
Test: [ 20/100]	Time  0.018 ( 0.028)	Loss 5.9824e-01 (4.9607e-01)	Acc@1  79.00 ( 83.67)	Acc@5 100.00 ( 99.24)
Test: [ 30/100]	Time  0.023 ( 0.025)	Loss 5.2492e-01 (5.1222e-01)	Acc@1  79.00 ( 83.42)	Acc@5  99.00 ( 99.35)
Test: [ 40/100]	Time  0.017 ( 0.024)	Loss 5.4730e-01 (5.2983e-01)	Acc@1  84.00 ( 83.29)	Acc@5  99.00 ( 99.29)
Test: [ 50/100]	Time  0.018 ( 0.023)	Loss 5.8118e-01 (5.1626e-01)	Acc@1  79.00 ( 83.65)	Acc@5  98.00 ( 99.27)
Test: [ 60/100]	Time  0.022 ( 0.023)	Loss 3.6406e-01 (5.1593e-01)	Acc@1  88.00 ( 83.52)	Acc@5 100.00 ( 99.28)
Test: [ 70/100]	Time  0.022 ( 0.023)	Loss 5.3132e-01 (5.0944e-01)	Acc@1  83.00 ( 83.70)	Acc@5  98.00 ( 99.30)
Test: [ 80/100]	Time  0.022 ( 0.023)	Loss 4.2097e-01 (5.0903e-01)	Acc@1  83.00 ( 83.59)	Acc@5  99.00 ( 99.31)
Test: [ 90/100]	Time  0.021 ( 0.023)	Loss 4.0036e-01 (5.1107e-01)	Acc@1  86.00 ( 83.51)	Acc@5 100.00 ( 99.29)
 * Acc@1 83.720 Acc@5 99.320
### epoch[20] execution time: 18.407801866531372
EPOCH 21
# current learning rate: 0.1
# Switched to train mode...
Epoch: [21][  0/391]	Time  0.210 ( 0.210)	Data  0.161 ( 0.161)	Loss 2.6231e-01 (2.6231e-01)	Acc@1  89.06 ( 89.06)	Acc@5 100.00 (100.00)
Epoch: [21][ 10/391]	Time  0.039 ( 0.057)	Data  0.001 ( 0.015)	Loss 3.3495e-01 (3.2505e-01)	Acc@1  85.94 ( 89.42)	Acc@5 100.00 ( 99.57)
Epoch: [21][ 20/391]	Time  0.041 ( 0.049)	Data  0.001 ( 0.008)	Loss 2.6802e-01 (3.2773e-01)	Acc@1  90.62 ( 88.80)	Acc@5 100.00 ( 99.74)
Epoch: [21][ 30/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.006)	Loss 2.9306e-01 (3.2317e-01)	Acc@1  89.84 ( 88.99)	Acc@5 100.00 ( 99.72)
Epoch: [21][ 40/391]	Time  0.039 ( 0.045)	Data  0.001 ( 0.005)	Loss 3.7133e-01 (3.2199e-01)	Acc@1  85.94 ( 88.80)	Acc@5 100.00 ( 99.75)
Epoch: [21][ 50/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.004)	Loss 4.7272e-01 (3.2890e-01)	Acc@1  82.81 ( 88.60)	Acc@5 100.00 ( 99.74)
Epoch: [21][ 60/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 4.2842e-01 (3.3573e-01)	Acc@1  85.94 ( 88.35)	Acc@5  99.22 ( 99.69)
Epoch: [21][ 70/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.7374e-01 (3.3868e-01)	Acc@1  86.72 ( 88.30)	Acc@5  98.44 ( 99.65)
Epoch: [21][ 80/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.5232e-01 (3.3921e-01)	Acc@1  91.41 ( 88.24)	Acc@5 100.00 ( 99.65)
Epoch: [21][ 90/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.7896e-01 (3.4029e-01)	Acc@1  92.19 ( 88.28)	Acc@5  99.22 ( 99.66)
Epoch: [21][100/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.003)	Loss 4.3059e-01 (3.4175e-01)	Acc@1  86.72 ( 88.23)	Acc@5  98.44 ( 99.64)
Epoch: [21][110/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1351e-01 (3.4350e-01)	Acc@1  91.41 ( 88.16)	Acc@5 100.00 ( 99.64)
Epoch: [21][120/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3219e-01 (3.4606e-01)	Acc@1  84.38 ( 88.08)	Acc@5 100.00 ( 99.62)
Epoch: [21][130/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1377e-01 (3.4871e-01)	Acc@1  89.06 ( 87.95)	Acc@5  99.22 ( 99.62)
Epoch: [21][140/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3170e-01 (3.4792e-01)	Acc@1  87.50 ( 87.94)	Acc@5  99.22 ( 99.63)
Epoch: [21][150/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8525e-01 (3.4549e-01)	Acc@1  86.72 ( 88.03)	Acc@5 100.00 ( 99.64)
Epoch: [21][160/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.8387e-01 (3.4440e-01)	Acc@1  90.62 ( 88.15)	Acc@5 100.00 ( 99.64)
Epoch: [21][170/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1635e-01 (3.4508e-01)	Acc@1  83.59 ( 88.05)	Acc@5 100.00 ( 99.64)
Epoch: [21][180/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8729e-01 (3.4797e-01)	Acc@1  87.50 ( 88.00)	Acc@5  99.22 ( 99.63)
Epoch: [21][190/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9987e-01 (3.4874e-01)	Acc@1  89.06 ( 87.95)	Acc@5  99.22 ( 99.63)
Epoch: [21][200/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1213e-01 (3.4843e-01)	Acc@1  89.84 ( 87.96)	Acc@5  99.22 ( 99.62)
Epoch: [21][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.6264e-01 (3.5041e-01)	Acc@1  82.81 ( 87.89)	Acc@5  98.44 ( 99.60)
Epoch: [21][220/391]	Time  0.060 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6194e-01 (3.4983e-01)	Acc@1  83.59 ( 87.89)	Acc@5 100.00 ( 99.62)
Epoch: [21][230/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3195e-01 (3.5091e-01)	Acc@1  89.06 ( 87.88)	Acc@5 100.00 ( 99.62)
Epoch: [21][240/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.5674e-01 (3.4961e-01)	Acc@1  92.97 ( 87.93)	Acc@5  99.22 ( 99.62)
Epoch: [21][250/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.0080e-01 (3.4982e-01)	Acc@1  89.06 ( 87.90)	Acc@5 100.00 ( 99.63)
Epoch: [21][260/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9297e-01 (3.4981e-01)	Acc@1  84.38 ( 87.87)	Acc@5 100.00 ( 99.63)
Epoch: [21][270/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4639e-01 (3.5136e-01)	Acc@1  86.72 ( 87.77)	Acc@5  99.22 ( 99.64)
Epoch: [21][280/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4721e-01 (3.5286e-01)	Acc@1  84.38 ( 87.68)	Acc@5  99.22 ( 99.63)
Epoch: [21][290/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3185e-01 (3.5380e-01)	Acc@1  89.84 ( 87.62)	Acc@5  99.22 ( 99.63)
Epoch: [21][300/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6631e-01 (3.5310e-01)	Acc@1  87.50 ( 87.63)	Acc@5  99.22 ( 99.63)
Epoch: [21][310/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.7969e-01 (3.5470e-01)	Acc@1  78.91 ( 87.58)	Acc@5  99.22 ( 99.63)
Epoch: [21][320/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1765e-01 (3.5470e-01)	Acc@1  84.38 ( 87.58)	Acc@5 100.00 ( 99.63)
Epoch: [21][330/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.5101e-01 (3.5557e-01)	Acc@1  90.62 ( 87.55)	Acc@5 100.00 ( 99.63)
Epoch: [21][340/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.9036e-01 (3.5602e-01)	Acc@1  84.38 ( 87.52)	Acc@5  99.22 ( 99.62)
Epoch: [21][350/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.6638e-01 (3.5562e-01)	Acc@1  87.50 ( 87.53)	Acc@5  99.22 ( 99.61)
Epoch: [21][360/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.2400e-01 (3.5567e-01)	Acc@1  87.50 ( 87.53)	Acc@5  99.22 ( 99.62)
Epoch: [21][370/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.7616e-01 (3.5513e-01)	Acc@1  89.06 ( 87.57)	Acc@5  99.22 ( 99.62)
Epoch: [21][380/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.6056e-01 (3.5529e-01)	Acc@1  84.38 ( 87.56)	Acc@5  98.44 ( 99.62)
Epoch: [21][390/391]	Time  0.030 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.2411e-01 (3.5478e-01)	Acc@1  88.75 ( 87.59)	Acc@5  97.50 ( 99.62)
## e[21] optimizer.zero_grad (sum) time: 0.26199841499328613
## e[21]       loss.backward (sum) time: 3.9228575229644775
## e[21]      optimizer.step (sum) time: 1.8156473636627197
## epoch[21] training(only) time: 15.948296785354614
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 4.2689e-01 (4.2689e-01)	Acc@1  86.00 ( 86.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.020 ( 0.036)	Loss 5.8781e-01 (4.6853e-01)	Acc@1  83.00 ( 84.45)	Acc@5 100.00 ( 99.55)
Test: [ 20/100]	Time  0.018 ( 0.028)	Loss 4.9850e-01 (4.6766e-01)	Acc@1  85.00 ( 84.00)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.024 ( 0.025)	Loss 5.4991e-01 (4.8041e-01)	Acc@1  81.00 ( 84.19)	Acc@5  98.00 ( 99.39)
Test: [ 40/100]	Time  0.021 ( 0.024)	Loss 4.1398e-01 (4.7934e-01)	Acc@1  86.00 ( 84.17)	Acc@5  99.00 ( 99.34)
Test: [ 50/100]	Time  0.021 ( 0.024)	Loss 3.3194e-01 (4.6279e-01)	Acc@1  88.00 ( 84.61)	Acc@5 100.00 ( 99.31)
Test: [ 60/100]	Time  0.020 ( 0.023)	Loss 4.5596e-01 (4.7431e-01)	Acc@1  82.00 ( 84.11)	Acc@5  99.00 ( 99.31)
Test: [ 70/100]	Time  0.022 ( 0.023)	Loss 4.0399e-01 (4.7356e-01)	Acc@1  85.00 ( 84.08)	Acc@5 100.00 ( 99.31)
Test: [ 80/100]	Time  0.021 ( 0.023)	Loss 5.0166e-01 (4.7248e-01)	Acc@1  84.00 ( 84.10)	Acc@5 100.00 ( 99.36)
Test: [ 90/100]	Time  0.018 ( 0.022)	Loss 4.0119e-01 (4.7603e-01)	Acc@1  82.00 ( 83.95)	Acc@5 100.00 ( 99.37)
 * Acc@1 84.010 Acc@5 99.410
### epoch[21] execution time: 18.237140655517578
EPOCH 22
# current learning rate: 0.1
# Switched to train mode...
Epoch: [22][  0/391]	Time  0.215 ( 0.215)	Data  0.172 ( 0.172)	Loss 4.1375e-01 (4.1375e-01)	Acc@1  80.47 ( 80.47)	Acc@5  99.22 ( 99.22)
Epoch: [22][ 10/391]	Time  0.040 ( 0.056)	Data  0.001 ( 0.016)	Loss 2.1780e-01 (3.1958e-01)	Acc@1  90.62 ( 88.00)	Acc@5 100.00 ( 99.50)
Epoch: [22][ 20/391]	Time  0.039 ( 0.049)	Data  0.001 ( 0.009)	Loss 3.9036e-01 (3.3393e-01)	Acc@1  85.94 ( 88.10)	Acc@5 100.00 ( 99.70)
Epoch: [22][ 30/391]	Time  0.042 ( 0.046)	Data  0.001 ( 0.006)	Loss 3.4728e-01 (3.3304e-01)	Acc@1  86.72 ( 88.41)	Acc@5  99.22 ( 99.67)
Epoch: [22][ 40/391]	Time  0.038 ( 0.045)	Data  0.001 ( 0.005)	Loss 5.7091e-01 (3.4042e-01)	Acc@1  80.47 ( 88.15)	Acc@5  98.44 ( 99.60)
Epoch: [22][ 50/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.004)	Loss 2.8491e-01 (3.3722e-01)	Acc@1  89.84 ( 88.27)	Acc@5 100.00 ( 99.65)
Epoch: [22][ 60/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.004)	Loss 3.2365e-01 (3.3549e-01)	Acc@1  85.94 ( 88.31)	Acc@5  99.22 ( 99.64)
Epoch: [22][ 70/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 2.5159e-01 (3.3009e-01)	Acc@1  92.19 ( 88.39)	Acc@5 100.00 ( 99.67)
Epoch: [22][ 80/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.3697e-01 (3.3286e-01)	Acc@1  92.97 ( 88.38)	Acc@5 100.00 ( 99.67)
Epoch: [22][ 90/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.4941e-01 (3.3750e-01)	Acc@1  87.50 ( 88.33)	Acc@5  99.22 ( 99.62)
Epoch: [22][100/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.9629e-01 (3.3688e-01)	Acc@1  89.84 ( 88.34)	Acc@5  99.22 ( 99.62)
Epoch: [22][110/391]	Time  0.051 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.2339e-01 (3.3803e-01)	Acc@1  89.84 ( 88.30)	Acc@5  99.22 ( 99.62)
Epoch: [22][120/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.6533e-01 (3.3744e-01)	Acc@1  86.72 ( 88.35)	Acc@5  99.22 ( 99.63)
Epoch: [22][130/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.1972e-01 (3.3943e-01)	Acc@1  79.69 ( 88.29)	Acc@5  99.22 ( 99.62)
Epoch: [22][140/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.3882e-01 (3.3832e-01)	Acc@1  90.62 ( 88.28)	Acc@5 100.00 ( 99.62)
Epoch: [22][150/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6620e-01 (3.3800e-01)	Acc@1  85.94 ( 88.29)	Acc@5 100.00 ( 99.61)
Epoch: [22][160/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5004e-01 (3.3900e-01)	Acc@1  87.50 ( 88.26)	Acc@5 100.00 ( 99.59)
Epoch: [22][170/391]	Time  0.046 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1752e-01 (3.3835e-01)	Acc@1  82.03 ( 88.24)	Acc@5  99.22 ( 99.60)
Epoch: [22][180/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8688e-01 (3.3898e-01)	Acc@1  82.03 ( 88.22)	Acc@5  99.22 ( 99.60)
Epoch: [22][190/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.9290e-01 (3.4006e-01)	Acc@1  91.41 ( 88.13)	Acc@5  99.22 ( 99.60)
Epoch: [22][200/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4033e-01 (3.4179e-01)	Acc@1  85.94 ( 88.06)	Acc@5 100.00 ( 99.60)
Epoch: [22][210/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4012e-01 (3.4454e-01)	Acc@1  85.94 ( 87.98)	Acc@5 100.00 ( 99.59)
Epoch: [22][220/391]	Time  0.040 ( 0.041)	Data  0.002 ( 0.002)	Loss 2.8367e-01 (3.4363e-01)	Acc@1  86.72 ( 87.99)	Acc@5 100.00 ( 99.59)
Epoch: [22][230/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8300e-01 (3.4465e-01)	Acc@1  82.81 ( 87.96)	Acc@5  99.22 ( 99.60)
Epoch: [22][240/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.5231e-01 (3.4601e-01)	Acc@1  83.59 ( 87.93)	Acc@5 100.00 ( 99.60)
Epoch: [22][250/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.6934e-01 (3.4602e-01)	Acc@1  92.97 ( 87.97)	Acc@5 100.00 ( 99.61)
Epoch: [22][260/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4584e-01 (3.4818e-01)	Acc@1  88.28 ( 87.91)	Acc@5 100.00 ( 99.59)
Epoch: [22][270/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.5126e-01 (3.4783e-01)	Acc@1  89.84 ( 87.92)	Acc@5 100.00 ( 99.59)
Epoch: [22][280/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.4605e-01 (3.4643e-01)	Acc@1  93.75 ( 87.96)	Acc@5 100.00 ( 99.59)
Epoch: [22][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.9134e-01 (3.4593e-01)	Acc@1  94.53 ( 87.96)	Acc@5 100.00 ( 99.60)
Epoch: [22][300/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2740e-01 (3.4672e-01)	Acc@1  89.84 ( 87.96)	Acc@5 100.00 ( 99.59)
Epoch: [22][310/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.8074e-01 (3.4623e-01)	Acc@1  92.19 ( 88.02)	Acc@5  99.22 ( 99.60)
Epoch: [22][320/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.9824e-01 (3.4671e-01)	Acc@1  89.06 ( 88.01)	Acc@5  99.22 ( 99.59)
Epoch: [22][330/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2158e-01 (3.4590e-01)	Acc@1  85.94 ( 88.02)	Acc@5 100.00 ( 99.58)
Epoch: [22][340/391]	Time  0.054 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.5807e-01 (3.4581e-01)	Acc@1  87.50 ( 88.04)	Acc@5  99.22 ( 99.58)
Epoch: [22][350/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.2055e-01 (3.4627e-01)	Acc@1  88.28 ( 88.03)	Acc@5 100.00 ( 99.58)
Epoch: [22][360/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.6208e-01 (3.4536e-01)	Acc@1  88.28 ( 88.06)	Acc@5 100.00 ( 99.59)
Epoch: [22][370/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.4831e-01 (3.4515e-01)	Acc@1  86.72 ( 88.07)	Acc@5 100.00 ( 99.58)
Epoch: [22][380/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.3322e-01 (3.4514e-01)	Acc@1  87.50 ( 88.06)	Acc@5 100.00 ( 99.59)
Epoch: [22][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.3925e-01 (3.4572e-01)	Acc@1  87.50 ( 88.04)	Acc@5  98.75 ( 99.59)
## e[22] optimizer.zero_grad (sum) time: 0.2628359794616699
## e[22]       loss.backward (sum) time: 3.962320327758789
## e[22]      optimizer.step (sum) time: 1.8280136585235596
## epoch[22] training(only) time: 15.943363666534424
# Switched to evaluate mode...
Test: [  0/100]	Time  0.182 ( 0.182)	Loss 3.2102e-01 (3.2102e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.018 ( 0.034)	Loss 5.6701e-01 (4.0548e-01)	Acc@1  86.00 ( 86.73)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.018 ( 0.027)	Loss 4.8530e-01 (4.2592e-01)	Acc@1  85.00 ( 85.76)	Acc@5 100.00 ( 99.52)
Test: [ 30/100]	Time  0.021 ( 0.025)	Loss 5.0751e-01 (4.5015e-01)	Acc@1  83.00 ( 85.29)	Acc@5  98.00 ( 99.45)
Test: [ 40/100]	Time  0.020 ( 0.024)	Loss 5.3557e-01 (4.5016e-01)	Acc@1  83.00 ( 85.12)	Acc@5  99.00 ( 99.41)
Test: [ 50/100]	Time  0.021 ( 0.023)	Loss 3.5711e-01 (4.4343e-01)	Acc@1  86.00 ( 85.41)	Acc@5 100.00 ( 99.45)
Test: [ 60/100]	Time  0.022 ( 0.023)	Loss 4.4636e-01 (4.5023e-01)	Acc@1  88.00 ( 85.10)	Acc@5 100.00 ( 99.52)
Test: [ 70/100]	Time  0.022 ( 0.023)	Loss 3.5995e-01 (4.4391e-01)	Acc@1  86.00 ( 85.21)	Acc@5 100.00 ( 99.55)
Test: [ 80/100]	Time  0.021 ( 0.023)	Loss 4.0736e-01 (4.4173e-01)	Acc@1  88.00 ( 85.30)	Acc@5  98.00 ( 99.47)
Test: [ 90/100]	Time  0.017 ( 0.022)	Loss 4.4784e-01 (4.4237e-01)	Acc@1  82.00 ( 85.35)	Acc@5  99.00 ( 99.43)
 * Acc@1 85.430 Acc@5 99.450
### epoch[22] execution time: 18.277002811431885
EPOCH 23
# current learning rate: 0.1
# Switched to train mode...
Epoch: [23][  0/391]	Time  0.212 ( 0.212)	Data  0.172 ( 0.172)	Loss 3.0400e-01 (3.0400e-01)	Acc@1  89.06 ( 89.06)	Acc@5 100.00 (100.00)
Epoch: [23][ 10/391]	Time  0.042 ( 0.056)	Data  0.001 ( 0.017)	Loss 3.2900e-01 (3.1692e-01)	Acc@1  89.84 ( 89.42)	Acc@5 100.00 ( 99.72)
Epoch: [23][ 20/391]	Time  0.040 ( 0.049)	Data  0.001 ( 0.009)	Loss 2.7436e-01 (3.2241e-01)	Acc@1  92.19 ( 88.91)	Acc@5 100.00 ( 99.63)
Epoch: [23][ 30/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.007)	Loss 2.5272e-01 (3.2005e-01)	Acc@1  91.41 ( 88.61)	Acc@5  99.22 ( 99.62)
Epoch: [23][ 40/391]	Time  0.039 ( 0.045)	Data  0.001 ( 0.005)	Loss 1.7409e-01 (3.1528e-01)	Acc@1  96.09 ( 89.02)	Acc@5 100.00 ( 99.64)
Epoch: [23][ 50/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.004)	Loss 2.5493e-01 (3.0926e-01)	Acc@1  92.97 ( 89.26)	Acc@5  98.44 ( 99.59)
Epoch: [23][ 60/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.004)	Loss 5.3081e-01 (3.1998e-01)	Acc@1  81.25 ( 88.88)	Acc@5  99.22 ( 99.58)
Epoch: [23][ 70/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.003)	Loss 2.8139e-01 (3.2143e-01)	Acc@1  89.84 ( 88.83)	Acc@5 100.00 ( 99.57)
Epoch: [23][ 80/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 2.7379e-01 (3.2121e-01)	Acc@1  89.06 ( 88.71)	Acc@5 100.00 ( 99.61)
Epoch: [23][ 90/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.7160e-01 (3.2077e-01)	Acc@1  92.19 ( 88.74)	Acc@5  99.22 ( 99.62)
Epoch: [23][100/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.2672e-01 (3.1990e-01)	Acc@1  89.84 ( 88.69)	Acc@5 100.00 ( 99.65)
Epoch: [23][110/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.1267e-01 (3.1790e-01)	Acc@1  89.06 ( 88.84)	Acc@5 100.00 ( 99.66)
Epoch: [23][120/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.9492e-01 (3.2172e-01)	Acc@1  89.06 ( 88.82)	Acc@5  98.44 ( 99.63)
Epoch: [23][130/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1802e-01 (3.2394e-01)	Acc@1  85.94 ( 88.79)	Acc@5  99.22 ( 99.61)
Epoch: [23][140/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.7663e-01 (3.2413e-01)	Acc@1  89.84 ( 88.82)	Acc@5  99.22 ( 99.62)
Epoch: [23][150/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7521e-01 (3.2517e-01)	Acc@1  87.50 ( 88.83)	Acc@5 100.00 ( 99.61)
Epoch: [23][160/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4021e-01 (3.2860e-01)	Acc@1  86.72 ( 88.67)	Acc@5 100.00 ( 99.61)
Epoch: [23][170/391]	Time  0.036 ( 0.041)	Data  0.002 ( 0.002)	Loss 3.4185e-01 (3.2926e-01)	Acc@1  87.50 ( 88.67)	Acc@5 100.00 ( 99.62)
Epoch: [23][180/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1426e-01 (3.2660e-01)	Acc@1  89.06 ( 88.74)	Acc@5  99.22 ( 99.62)
Epoch: [23][190/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.8175e-01 (3.2749e-01)	Acc@1  90.62 ( 88.71)	Acc@5 100.00 ( 99.63)
Epoch: [23][200/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.0262e-01 (3.2700e-01)	Acc@1  92.19 ( 88.74)	Acc@5 100.00 ( 99.64)
Epoch: [23][210/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.6428e-01 (3.2724e-01)	Acc@1  85.94 ( 88.69)	Acc@5 100.00 ( 99.64)
Epoch: [23][220/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0393e-01 (3.2909e-01)	Acc@1  85.94 ( 88.61)	Acc@5 100.00 ( 99.63)
Epoch: [23][230/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5466e-01 (3.2958e-01)	Acc@1  86.72 ( 88.60)	Acc@5  99.22 ( 99.63)
Epoch: [23][240/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.7062e-01 (3.2994e-01)	Acc@1  90.62 ( 88.59)	Acc@5 100.00 ( 99.63)
Epoch: [23][250/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0130e-01 (3.3048e-01)	Acc@1  84.38 ( 88.54)	Acc@5 100.00 ( 99.62)
Epoch: [23][260/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0351e-01 (3.3038e-01)	Acc@1  86.72 ( 88.56)	Acc@5 100.00 ( 99.63)
Epoch: [23][270/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1801e-01 (3.3023e-01)	Acc@1  89.84 ( 88.58)	Acc@5 100.00 ( 99.64)
Epoch: [23][280/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 4.4315e-01 (3.3097e-01)	Acc@1  82.03 ( 88.55)	Acc@5 100.00 ( 99.64)
Epoch: [23][290/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 4.0004e-01 (3.3132e-01)	Acc@1  88.28 ( 88.56)	Acc@5  97.66 ( 99.63)
Epoch: [23][300/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 2.7830e-01 (3.3114e-01)	Acc@1  91.41 ( 88.56)	Acc@5 100.00 ( 99.62)
Epoch: [23][310/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 2.8903e-01 (3.3039e-01)	Acc@1  89.84 ( 88.59)	Acc@5 100.00 ( 99.62)
Epoch: [23][320/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.4354e-01 (3.2946e-01)	Acc@1  82.81 ( 88.60)	Acc@5  99.22 ( 99.63)
Epoch: [23][330/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.9812e-01 (3.3043e-01)	Acc@1  82.81 ( 88.57)	Acc@5 100.00 ( 99.62)
Epoch: [23][340/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.2924e-01 (3.3109e-01)	Acc@1  86.72 ( 88.56)	Acc@5 100.00 ( 99.61)
Epoch: [23][350/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.3668e-01 (3.3193e-01)	Acc@1  89.84 ( 88.51)	Acc@5 100.00 ( 99.61)
Epoch: [23][360/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.6471e-01 (3.3151e-01)	Acc@1  86.72 ( 88.52)	Acc@5 100.00 ( 99.62)
Epoch: [23][370/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.7086e-01 (3.3165e-01)	Acc@1  90.62 ( 88.52)	Acc@5 100.00 ( 99.62)
Epoch: [23][380/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.0641e-01 (3.3192e-01)	Acc@1  82.03 ( 88.49)	Acc@5 100.00 ( 99.62)
Epoch: [23][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.6341e-01 (3.3250e-01)	Acc@1  88.75 ( 88.47)	Acc@5 100.00 ( 99.62)
## e[23] optimizer.zero_grad (sum) time: 0.26371192932128906
## e[23]       loss.backward (sum) time: 3.8642044067382812
## e[23]      optimizer.step (sum) time: 1.865182638168335
## epoch[23] training(only) time: 15.856343030929565
# Switched to evaluate mode...
Test: [  0/100]	Time  0.167 ( 0.167)	Loss 5.0350e-01 (5.0350e-01)	Acc@1  86.00 ( 86.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.021 ( 0.035)	Loss 5.5673e-01 (4.6106e-01)	Acc@1  85.00 ( 85.27)	Acc@5  99.00 ( 99.55)
Test: [ 20/100]	Time  0.024 ( 0.029)	Loss 6.0445e-01 (4.7064e-01)	Acc@1  78.00 ( 85.05)	Acc@5  98.00 ( 99.43)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 4.6619e-01 (4.7940e-01)	Acc@1  83.00 ( 84.97)	Acc@5  98.00 ( 99.29)
Test: [ 40/100]	Time  0.022 ( 0.025)	Loss 4.7155e-01 (4.8301e-01)	Acc@1  86.00 ( 84.71)	Acc@5  98.00 ( 99.17)
Test: [ 50/100]	Time  0.020 ( 0.024)	Loss 4.5877e-01 (4.7379e-01)	Acc@1  85.00 ( 84.88)	Acc@5  98.00 ( 99.20)
Test: [ 60/100]	Time  0.023 ( 0.024)	Loss 4.3108e-01 (4.7074e-01)	Acc@1  87.00 ( 84.82)	Acc@5 100.00 ( 99.28)
Test: [ 70/100]	Time  0.018 ( 0.023)	Loss 5.5569e-01 (4.6643e-01)	Acc@1  83.00 ( 84.96)	Acc@5  98.00 ( 99.28)
Test: [ 80/100]	Time  0.023 ( 0.023)	Loss 4.2343e-01 (4.6369e-01)	Acc@1  85.00 ( 84.99)	Acc@5 100.00 ( 99.27)
Test: [ 90/100]	Time  0.022 ( 0.023)	Loss 3.3061e-01 (4.6223e-01)	Acc@1  88.00 ( 85.04)	Acc@5 100.00 ( 99.31)
 * Acc@1 84.930 Acc@5 99.340
### epoch[23] execution time: 18.20997714996338
EPOCH 24
# current learning rate: 0.1
# Switched to train mode...
Epoch: [24][  0/391]	Time  0.206 ( 0.206)	Data  0.165 ( 0.165)	Loss 3.0875e-01 (3.0875e-01)	Acc@1  89.06 ( 89.06)	Acc@5 100.00 (100.00)
Epoch: [24][ 10/391]	Time  0.042 ( 0.056)	Data  0.001 ( 0.016)	Loss 2.3089e-01 (3.2781e-01)	Acc@1  92.97 ( 88.28)	Acc@5 100.00 ( 99.72)
Epoch: [24][ 20/391]	Time  0.041 ( 0.049)	Data  0.001 ( 0.009)	Loss 2.5669e-01 (3.3435e-01)	Acc@1  88.28 ( 88.02)	Acc@5 100.00 ( 99.63)
Epoch: [24][ 30/391]	Time  0.040 ( 0.046)	Data  0.001 ( 0.006)	Loss 3.6261e-01 (3.2299e-01)	Acc@1  87.50 ( 88.43)	Acc@5 100.00 ( 99.70)
Epoch: [24][ 40/391]	Time  0.044 ( 0.045)	Data  0.001 ( 0.005)	Loss 2.0781e-01 (3.2792e-01)	Acc@1  93.75 ( 88.40)	Acc@5 100.00 ( 99.64)
Epoch: [24][ 50/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.004)	Loss 3.6232e-01 (3.2845e-01)	Acc@1  84.38 ( 88.22)	Acc@5 100.00 ( 99.68)
Epoch: [24][ 60/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.004)	Loss 2.7043e-01 (3.2794e-01)	Acc@1  90.62 ( 88.20)	Acc@5  99.22 ( 99.67)
Epoch: [24][ 70/391]	Time  0.036 ( 0.043)	Data  0.001 ( 0.003)	Loss 2.2957e-01 (3.2517e-01)	Acc@1  89.84 ( 88.28)	Acc@5 100.00 ( 99.68)
Epoch: [24][ 80/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.2745e-01 (3.2350e-01)	Acc@1  85.16 ( 88.31)	Acc@5  99.22 ( 99.67)
Epoch: [24][ 90/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.1490e-01 (3.2188e-01)	Acc@1  85.94 ( 88.40)	Acc@5  98.44 ( 99.67)
Epoch: [24][100/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.5274e-01 (3.2453e-01)	Acc@1  93.75 ( 88.38)	Acc@5 100.00 ( 99.61)
Epoch: [24][110/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.2201e-01 (3.2801e-01)	Acc@1  86.72 ( 88.24)	Acc@5  99.22 ( 99.61)
Epoch: [24][120/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.7119e-01 (3.3051e-01)	Acc@1  91.41 ( 88.14)	Acc@5 100.00 ( 99.59)
Epoch: [24][130/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.9145e-01 (3.2888e-01)	Acc@1  87.50 ( 88.25)	Acc@5 100.00 ( 99.61)
Epoch: [24][140/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.1585e-01 (3.2668e-01)	Acc@1  89.84 ( 88.33)	Acc@5  99.22 ( 99.61)
Epoch: [24][150/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1840e-01 (3.2799e-01)	Acc@1  84.38 ( 88.35)	Acc@5 100.00 ( 99.62)
Epoch: [24][160/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.7369e-01 (3.3061e-01)	Acc@1  92.19 ( 88.21)	Acc@5  99.22 ( 99.62)
Epoch: [24][170/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4648e-01 (3.3180e-01)	Acc@1  87.50 ( 88.14)	Acc@5 100.00 ( 99.63)
Epoch: [24][180/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.8212e-01 (3.3138e-01)	Acc@1  89.84 ( 88.14)	Acc@5 100.00 ( 99.63)
Epoch: [24][190/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.7469e-01 (3.3097e-01)	Acc@1  89.84 ( 88.21)	Acc@5 100.00 ( 99.62)
Epoch: [24][200/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.8054e-01 (3.3191e-01)	Acc@1  93.75 ( 88.17)	Acc@5 100.00 ( 99.62)
Epoch: [24][210/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.0917e-01 (3.3232e-01)	Acc@1  86.72 ( 88.23)	Acc@5 100.00 ( 99.61)
Epoch: [24][220/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.5381e-01 (3.3039e-01)	Acc@1  90.62 ( 88.30)	Acc@5 100.00 ( 99.61)
Epoch: [24][230/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.6615e-01 (3.3166e-01)	Acc@1  89.84 ( 88.24)	Acc@5  99.22 ( 99.61)
Epoch: [24][240/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3851e-01 (3.3068e-01)	Acc@1  87.50 ( 88.26)	Acc@5  99.22 ( 99.61)
Epoch: [24][250/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7443e-01 (3.3156e-01)	Acc@1  89.06 ( 88.23)	Acc@5  98.44 ( 99.60)
Epoch: [24][260/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.0803e-01 (3.3178e-01)	Acc@1  89.84 ( 88.23)	Acc@5 100.00 ( 99.60)
Epoch: [24][270/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4014e-01 (3.3236e-01)	Acc@1  87.50 ( 88.21)	Acc@5 100.00 ( 99.60)
Epoch: [24][280/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.9172e-01 (3.3347e-01)	Acc@1  91.41 ( 88.21)	Acc@5  99.22 ( 99.60)
Epoch: [24][290/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.1325e-01 (3.3379e-01)	Acc@1  89.06 ( 88.21)	Acc@5 100.00 ( 99.60)
Epoch: [24][300/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.1914e-01 (3.3429e-01)	Acc@1  87.50 ( 88.21)	Acc@5 100.00 ( 99.61)
Epoch: [24][310/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.2664e-01 (3.3410e-01)	Acc@1  85.94 ( 88.24)	Acc@5  99.22 ( 99.60)
Epoch: [24][320/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.8281e-01 (3.3408e-01)	Acc@1  86.72 ( 88.27)	Acc@5 100.00 ( 99.61)
Epoch: [24][330/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.9947e-01 (3.3546e-01)	Acc@1  89.84 ( 88.26)	Acc@5  99.22 ( 99.60)
Epoch: [24][340/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.5464e-01 (3.3695e-01)	Acc@1  85.94 ( 88.21)	Acc@5 100.00 ( 99.59)
Epoch: [24][350/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.2526e-01 (3.3683e-01)	Acc@1  88.28 ( 88.20)	Acc@5  98.44 ( 99.60)
Epoch: [24][360/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.5947e-01 (3.3658e-01)	Acc@1  87.50 ( 88.20)	Acc@5 100.00 ( 99.60)
Epoch: [24][370/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.9383e-01 (3.3682e-01)	Acc@1  86.72 ( 88.17)	Acc@5 100.00 ( 99.60)
Epoch: [24][380/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.0203e-01 (3.3646e-01)	Acc@1  93.75 ( 88.17)	Acc@5 100.00 ( 99.61)
Epoch: [24][390/391]	Time  0.029 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.0987e-01 (3.3648e-01)	Acc@1  86.25 ( 88.17)	Acc@5 100.00 ( 99.62)
## e[24] optimizer.zero_grad (sum) time: 0.26197052001953125
## e[24]       loss.backward (sum) time: 3.9197163581848145
## e[24]      optimizer.step (sum) time: 1.8274731636047363
## epoch[24] training(only) time: 16.014124631881714
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 4.1853e-01 (4.1853e-01)	Acc@1  86.00 ( 86.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.021 ( 0.036)	Loss 4.0368e-01 (4.2731e-01)	Acc@1  86.00 ( 85.27)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.017 ( 0.029)	Loss 4.9803e-01 (4.1926e-01)	Acc@1  84.00 ( 85.71)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.024 ( 0.027)	Loss 5.1039e-01 (4.2631e-01)	Acc@1  86.00 ( 85.68)	Acc@5  99.00 ( 99.58)
Test: [ 40/100]	Time  0.021 ( 0.025)	Loss 3.8011e-01 (4.3193e-01)	Acc@1  85.00 ( 85.73)	Acc@5  99.00 ( 99.51)
Test: [ 50/100]	Time  0.021 ( 0.025)	Loss 4.1339e-01 (4.2339e-01)	Acc@1  85.00 ( 86.06)	Acc@5  99.00 ( 99.53)
Test: [ 60/100]	Time  0.022 ( 0.024)	Loss 4.7621e-01 (4.2335e-01)	Acc@1  83.00 ( 86.08)	Acc@5  98.00 ( 99.54)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 5.5823e-01 (4.2284e-01)	Acc@1  82.00 ( 86.04)	Acc@5 100.00 ( 99.54)
Test: [ 80/100]	Time  0.024 ( 0.024)	Loss 4.3163e-01 (4.2303e-01)	Acc@1  83.00 ( 86.01)	Acc@5  99.00 ( 99.46)
Test: [ 90/100]	Time  0.023 ( 0.023)	Loss 2.4311e-01 (4.2316e-01)	Acc@1  90.00 ( 85.98)	Acc@5 100.00 ( 99.48)
 * Acc@1 86.030 Acc@5 99.480
### epoch[24] execution time: 18.43262815475464
EPOCH 25
# current learning rate: 0.1
# Switched to train mode...
Epoch: [25][  0/391]	Time  0.216 ( 0.216)	Data  0.175 ( 0.175)	Loss 2.7016e-01 (2.7016e-01)	Acc@1  90.62 ( 90.62)	Acc@5  99.22 ( 99.22)
Epoch: [25][ 10/391]	Time  0.041 ( 0.056)	Data  0.001 ( 0.017)	Loss 3.0903e-01 (3.2271e-01)	Acc@1  90.62 ( 89.35)	Acc@5 100.00 ( 99.64)
Epoch: [25][ 20/391]	Time  0.039 ( 0.048)	Data  0.001 ( 0.009)	Loss 1.8894e-01 (3.2016e-01)	Acc@1  94.53 ( 89.40)	Acc@5 100.00 ( 99.63)
Epoch: [25][ 30/391]	Time  0.038 ( 0.046)	Data  0.001 ( 0.007)	Loss 3.1194e-01 (3.1580e-01)	Acc@1  89.84 ( 89.39)	Acc@5 100.00 ( 99.62)
Epoch: [25][ 40/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.005)	Loss 3.1812e-01 (3.1416e-01)	Acc@1  87.50 ( 89.27)	Acc@5 100.00 ( 99.64)
Epoch: [25][ 50/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.004)	Loss 3.2208e-01 (3.1506e-01)	Acc@1  89.84 ( 89.22)	Acc@5  99.22 ( 99.65)
Epoch: [25][ 60/391]	Time  0.043 ( 0.043)	Data  0.001 ( 0.004)	Loss 2.6329e-01 (3.1218e-01)	Acc@1  90.62 ( 89.31)	Acc@5 100.00 ( 99.69)
Epoch: [25][ 70/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.5610e-01 (3.1461e-01)	Acc@1  88.28 ( 89.16)	Acc@5  99.22 ( 99.69)
Epoch: [25][ 80/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.8908e-01 (3.1562e-01)	Acc@1  91.41 ( 89.12)	Acc@5 100.00 ( 99.72)
Epoch: [25][ 90/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.8779e-01 (3.1251e-01)	Acc@1  89.84 ( 89.13)	Acc@5 100.00 ( 99.73)
Epoch: [25][100/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.8918e-01 (3.1331e-01)	Acc@1  88.28 ( 89.16)	Acc@5 100.00 ( 99.74)
Epoch: [25][110/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.8515e-01 (3.1211e-01)	Acc@1  89.06 ( 89.21)	Acc@5  99.22 ( 99.73)
Epoch: [25][120/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2926e-01 (3.1178e-01)	Acc@1  84.38 ( 89.20)	Acc@5  99.22 ( 99.74)
Epoch: [25][130/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.9550e-01 (3.1030e-01)	Acc@1  89.06 ( 89.28)	Acc@5 100.00 ( 99.74)
Epoch: [25][140/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.5203e-01 (3.1440e-01)	Acc@1  82.81 ( 89.08)	Acc@5  99.22 ( 99.75)
Epoch: [25][150/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4700e-01 (3.1618e-01)	Acc@1  87.50 ( 89.04)	Acc@5 100.00 ( 99.74)
Epoch: [25][160/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3103e-01 (3.1797e-01)	Acc@1  86.72 ( 88.98)	Acc@5 100.00 ( 99.73)
Epoch: [25][170/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2482e-01 (3.1653e-01)	Acc@1  88.28 ( 89.02)	Acc@5  99.22 ( 99.73)
Epoch: [25][180/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7453e-01 (3.1619e-01)	Acc@1  93.75 ( 89.05)	Acc@5 100.00 ( 99.73)
Epoch: [25][190/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.0548e-01 (3.1483e-01)	Acc@1  92.97 ( 89.11)	Acc@5 100.00 ( 99.74)
Epoch: [25][200/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7823e-01 (3.1469e-01)	Acc@1  84.38 ( 89.09)	Acc@5 100.00 ( 99.75)
Epoch: [25][210/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4999e-01 (3.1439e-01)	Acc@1  85.94 ( 89.07)	Acc@5  99.22 ( 99.74)
Epoch: [25][220/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8716e-01 (3.1499e-01)	Acc@1  87.50 ( 89.08)	Acc@5 100.00 ( 99.75)
Epoch: [25][230/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2500e-01 (3.1773e-01)	Acc@1  90.62 ( 88.99)	Acc@5  99.22 ( 99.76)
Epoch: [25][240/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2598e-01 (3.1806e-01)	Acc@1  91.41 ( 89.00)	Acc@5 100.00 ( 99.76)
Epoch: [25][250/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6968e-01 (3.1795e-01)	Acc@1  87.50 ( 88.98)	Acc@5  99.22 ( 99.76)
Epoch: [25][260/391]	Time  0.045 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2070e-01 (3.1781e-01)	Acc@1  92.19 ( 89.01)	Acc@5  99.22 ( 99.75)
Epoch: [25][270/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3587e-01 (3.1792e-01)	Acc@1  89.06 ( 88.95)	Acc@5 100.00 ( 99.75)
Epoch: [25][280/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5879e-01 (3.1816e-01)	Acc@1  87.50 ( 88.95)	Acc@5 100.00 ( 99.75)
Epoch: [25][290/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.5010e-01 (3.1881e-01)	Acc@1  86.72 ( 88.95)	Acc@5  99.22 ( 99.74)
Epoch: [25][300/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4146e-01 (3.1989e-01)	Acc@1  87.50 ( 88.92)	Acc@5 100.00 ( 99.74)
Epoch: [25][310/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.4045e-01 (3.2074e-01)	Acc@1  95.31 ( 88.90)	Acc@5 100.00 ( 99.74)
Epoch: [25][320/391]	Time  0.035 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9319e-01 (3.2065e-01)	Acc@1  89.06 ( 88.88)	Acc@5  99.22 ( 99.74)
Epoch: [25][330/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.7739e-01 (3.2105e-01)	Acc@1  94.53 ( 88.89)	Acc@5  99.22 ( 99.74)
Epoch: [25][340/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.7575e-01 (3.2148e-01)	Acc@1  90.62 ( 88.90)	Acc@5 100.00 ( 99.74)
Epoch: [25][350/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.0103e-01 (3.2216e-01)	Acc@1  92.19 ( 88.86)	Acc@5 100.00 ( 99.73)
Epoch: [25][360/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 3.9319e-01 (3.2320e-01)	Acc@1  86.72 ( 88.80)	Acc@5 100.00 ( 99.73)
Epoch: [25][370/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.0010e-01 (3.2328e-01)	Acc@1  82.81 ( 88.79)	Acc@5  99.22 ( 99.72)
Epoch: [25][380/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.7973e-01 (3.2480e-01)	Acc@1  88.28 ( 88.75)	Acc@5 100.00 ( 99.72)
Epoch: [25][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.1719e-01 (3.2509e-01)	Acc@1  86.25 ( 88.75)	Acc@5 100.00 ( 99.72)
## e[25] optimizer.zero_grad (sum) time: 0.26177430152893066
## e[25]       loss.backward (sum) time: 3.9037604331970215
## e[25]      optimizer.step (sum) time: 1.81528639793396
## epoch[25] training(only) time: 15.896612644195557
# Switched to evaluate mode...
Test: [  0/100]	Time  0.178 ( 0.178)	Loss 5.0325e-01 (5.0325e-01)	Acc@1  83.00 ( 83.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 5.0728e-01 (4.2843e-01)	Acc@1  80.00 ( 84.55)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.024 ( 0.028)	Loss 3.9394e-01 (4.2108e-01)	Acc@1  84.00 ( 85.00)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.024 ( 0.026)	Loss 4.2822e-01 (4.2906e-01)	Acc@1  83.00 ( 84.90)	Acc@5  99.00 ( 99.48)
Test: [ 40/100]	Time  0.022 ( 0.025)	Loss 4.4989e-01 (4.3150e-01)	Acc@1  88.00 ( 85.07)	Acc@5  98.00 ( 99.37)
Test: [ 50/100]	Time  0.018 ( 0.024)	Loss 4.1806e-01 (4.2444e-01)	Acc@1  86.00 ( 85.29)	Acc@5  99.00 ( 99.39)
Test: [ 60/100]	Time  0.022 ( 0.024)	Loss 3.4080e-01 (4.2415e-01)	Acc@1  89.00 ( 85.25)	Acc@5 100.00 ( 99.43)
Test: [ 70/100]	Time  0.018 ( 0.023)	Loss 5.5155e-01 (4.2325e-01)	Acc@1  82.00 ( 85.30)	Acc@5  98.00 ( 99.41)
Test: [ 80/100]	Time  0.017 ( 0.023)	Loss 3.5865e-01 (4.2040e-01)	Acc@1  89.00 ( 85.47)	Acc@5 100.00 ( 99.44)
Test: [ 90/100]	Time  0.025 ( 0.022)	Loss 2.9574e-01 (4.1610e-01)	Acc@1  89.00 ( 85.59)	Acc@5 100.00 ( 99.49)
 * Acc@1 85.580 Acc@5 99.500
### epoch[25] execution time: 18.241379022598267
EPOCH 26
# current learning rate: 0.1
# Switched to train mode...
Epoch: [26][  0/391]	Time  0.208 ( 0.208)	Data  0.166 ( 0.166)	Loss 3.8021e-01 (3.8021e-01)	Acc@1  87.50 ( 87.50)	Acc@5  98.44 ( 98.44)
Epoch: [26][ 10/391]	Time  0.039 ( 0.056)	Data  0.001 ( 0.016)	Loss 3.1108e-01 (3.0252e-01)	Acc@1  85.16 ( 89.91)	Acc@5 100.00 ( 99.50)
Epoch: [26][ 20/391]	Time  0.039 ( 0.049)	Data  0.001 ( 0.009)	Loss 2.9437e-01 (3.0813e-01)	Acc@1  87.50 ( 89.40)	Acc@5 100.00 ( 99.40)
Epoch: [26][ 30/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.006)	Loss 5.0857e-01 (3.2186e-01)	Acc@1  85.94 ( 88.81)	Acc@5  99.22 ( 99.52)
Epoch: [26][ 40/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.005)	Loss 3.1804e-01 (3.1586e-01)	Acc@1  87.50 ( 89.10)	Acc@5 100.00 ( 99.60)
Epoch: [26][ 50/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.004)	Loss 2.7782e-01 (3.2264e-01)	Acc@1  89.84 ( 88.92)	Acc@5 100.00 ( 99.62)
Epoch: [26][ 60/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.004)	Loss 4.0332e-01 (3.2393e-01)	Acc@1  87.50 ( 88.76)	Acc@5 100.00 ( 99.63)
Epoch: [26][ 70/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.1487e-01 (3.2226e-01)	Acc@1  86.72 ( 88.77)	Acc@5 100.00 ( 99.68)
Epoch: [26][ 80/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.4183e-01 (3.1747e-01)	Acc@1  91.41 ( 88.96)	Acc@5 100.00 ( 99.70)
Epoch: [26][ 90/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.7763e-01 (3.1696e-01)	Acc@1  93.75 ( 88.95)	Acc@5 100.00 ( 99.68)
Epoch: [26][100/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.2431e-01 (3.1979e-01)	Acc@1  84.38 ( 88.87)	Acc@5  99.22 ( 99.68)
Epoch: [26][110/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.9644e-01 (3.1939e-01)	Acc@1  88.28 ( 88.83)	Acc@5 100.00 ( 99.68)
Epoch: [26][120/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.4204e-01 (3.2082e-01)	Acc@1  92.19 ( 88.73)	Acc@5 100.00 ( 99.68)
Epoch: [26][130/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.9643e-01 (3.2146e-01)	Acc@1  89.84 ( 88.69)	Acc@5  99.22 ( 99.68)
Epoch: [26][140/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.9407e-01 (3.2199e-01)	Acc@1  89.84 ( 88.72)	Acc@5 100.00 ( 99.68)
Epoch: [26][150/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1730e-01 (3.2081e-01)	Acc@1  89.06 ( 88.73)	Acc@5 100.00 ( 99.68)
Epoch: [26][160/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8368e-01 (3.2015e-01)	Acc@1  84.38 ( 88.77)	Acc@5  99.22 ( 99.70)
Epoch: [26][170/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5527e-01 (3.1742e-01)	Acc@1  96.09 ( 88.91)	Acc@5 100.00 ( 99.70)
Epoch: [26][180/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.7928e-01 (3.1769e-01)	Acc@1  85.94 ( 88.85)	Acc@5 100.00 ( 99.69)
Epoch: [26][190/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.6901e-01 (3.2086e-01)	Acc@1  91.41 ( 88.80)	Acc@5 100.00 ( 99.70)
Epoch: [26][200/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4145e-01 (3.2119e-01)	Acc@1  91.41 ( 88.81)	Acc@5 100.00 ( 99.70)
Epoch: [26][210/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.6846e-01 (3.2170e-01)	Acc@1  91.41 ( 88.78)	Acc@5  99.22 ( 99.69)
Epoch: [26][220/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0145e-01 (3.2130e-01)	Acc@1  88.28 ( 88.84)	Acc@5 100.00 ( 99.69)
Epoch: [26][230/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3548e-01 (3.2094e-01)	Acc@1  88.28 ( 88.83)	Acc@5 100.00 ( 99.69)
Epoch: [26][240/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2941e-01 (3.2153e-01)	Acc@1  89.06 ( 88.83)	Acc@5 100.00 ( 99.69)
Epoch: [26][250/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2415e-01 (3.2179e-01)	Acc@1  89.84 ( 88.82)	Acc@5  99.22 ( 99.69)
Epoch: [26][260/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.8632e-01 (3.2169e-01)	Acc@1  87.50 ( 88.84)	Acc@5 100.00 ( 99.68)
Epoch: [26][270/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.8685e-01 (3.2149e-01)	Acc@1  89.84 ( 88.86)	Acc@5  99.22 ( 99.69)
Epoch: [26][280/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2788e-01 (3.2115e-01)	Acc@1  86.72 ( 88.89)	Acc@5  99.22 ( 99.69)
Epoch: [26][290/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7504e-01 (3.2028e-01)	Acc@1  89.06 ( 88.94)	Acc@5  98.44 ( 99.69)
Epoch: [26][300/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2583e-01 (3.1969e-01)	Acc@1  88.28 ( 88.98)	Acc@5 100.00 ( 99.68)
Epoch: [26][310/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2267e-01 (3.1908e-01)	Acc@1  89.06 ( 89.01)	Acc@5 100.00 ( 99.68)
Epoch: [26][320/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7659e-01 (3.1979e-01)	Acc@1  85.16 ( 88.97)	Acc@5 100.00 ( 99.68)
Epoch: [26][330/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.1946e-01 (3.1950e-01)	Acc@1  93.75 ( 88.99)	Acc@5 100.00 ( 99.69)
Epoch: [26][340/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.1632e-01 (3.1951e-01)	Acc@1  92.97 ( 88.98)	Acc@5 100.00 ( 99.69)
Epoch: [26][350/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.0018e-01 (3.1870e-01)	Acc@1  89.84 ( 89.00)	Acc@5  99.22 ( 99.69)
Epoch: [26][360/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.0887e-01 (3.1827e-01)	Acc@1  89.06 ( 89.00)	Acc@5 100.00 ( 99.69)
Epoch: [26][370/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.6635e-01 (3.1859e-01)	Acc@1  85.16 ( 88.98)	Acc@5  99.22 ( 99.70)
Epoch: [26][380/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.3596e-01 (3.1874e-01)	Acc@1  89.84 ( 88.99)	Acc@5  99.22 ( 99.69)
Epoch: [26][390/391]	Time  0.034 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.9311e-01 (3.1854e-01)	Acc@1  91.25 ( 89.01)	Acc@5  98.75 ( 99.68)
## e[26] optimizer.zero_grad (sum) time: 0.2637641429901123
## e[26]       loss.backward (sum) time: 4.047864198684692
## e[26]      optimizer.step (sum) time: 1.7945022583007812
## epoch[26] training(only) time: 16.021124362945557
# Switched to evaluate mode...
Test: [  0/100]	Time  0.175 ( 0.175)	Loss 5.1190e-01 (5.1190e-01)	Acc@1  85.00 ( 85.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.024 ( 0.035)	Loss 4.7680e-01 (5.2472e-01)	Acc@1  84.00 ( 82.82)	Acc@5 100.00 ( 99.36)
Test: [ 20/100]	Time  0.023 ( 0.028)	Loss 6.8654e-01 (5.3595e-01)	Acc@1  78.00 ( 82.29)	Acc@5  99.00 ( 99.19)
Test: [ 30/100]	Time  0.017 ( 0.025)	Loss 5.4556e-01 (5.3099e-01)	Acc@1  81.00 ( 82.77)	Acc@5  99.00 ( 99.10)
Test: [ 40/100]	Time  0.018 ( 0.025)	Loss 7.4137e-01 (5.3585e-01)	Acc@1  75.00 ( 82.63)	Acc@5 100.00 ( 99.10)
Test: [ 50/100]	Time  0.018 ( 0.024)	Loss 5.4144e-01 (5.3327e-01)	Acc@1  85.00 ( 82.78)	Acc@5  99.00 ( 99.06)
Test: [ 60/100]	Time  0.023 ( 0.024)	Loss 5.2196e-01 (5.3788e-01)	Acc@1  85.00 ( 82.62)	Acc@5 100.00 ( 99.11)
Test: [ 70/100]	Time  0.020 ( 0.023)	Loss 5.0235e-01 (5.3748e-01)	Acc@1  83.00 ( 82.51)	Acc@5 100.00 ( 99.14)
Test: [ 80/100]	Time  0.025 ( 0.023)	Loss 4.5615e-01 (5.3809e-01)	Acc@1  86.00 ( 82.60)	Acc@5 100.00 ( 99.11)
Test: [ 90/100]	Time  0.019 ( 0.023)	Loss 2.5407e-01 (5.3810e-01)	Acc@1  93.00 ( 82.60)	Acc@5 100.00 ( 99.05)
 * Acc@1 82.660 Acc@5 99.090
### epoch[26] execution time: 18.408299684524536
EPOCH 27
# current learning rate: 0.1
# Switched to train mode...
Epoch: [27][  0/391]	Time  0.213 ( 0.213)	Data  0.172 ( 0.172)	Loss 5.0635e-01 (5.0635e-01)	Acc@1  81.25 ( 81.25)	Acc@5  99.22 ( 99.22)
Epoch: [27][ 10/391]	Time  0.041 ( 0.057)	Data  0.001 ( 0.017)	Loss 2.9869e-01 (3.0742e-01)	Acc@1  88.28 ( 89.06)	Acc@5 100.00 ( 99.79)
Epoch: [27][ 20/391]	Time  0.040 ( 0.049)	Data  0.001 ( 0.009)	Loss 3.5850e-01 (3.0912e-01)	Acc@1  86.72 ( 88.80)	Acc@5  99.22 ( 99.67)
Epoch: [27][ 30/391]	Time  0.040 ( 0.046)	Data  0.001 ( 0.007)	Loss 2.6886e-01 (2.9957e-01)	Acc@1  90.62 ( 89.11)	Acc@5  99.22 ( 99.72)
Epoch: [27][ 40/391]	Time  0.042 ( 0.045)	Data  0.001 ( 0.005)	Loss 3.2236e-01 (3.0274e-01)	Acc@1  87.50 ( 89.18)	Acc@5 100.00 ( 99.75)
Epoch: [27][ 50/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.004)	Loss 3.2633e-01 (3.0047e-01)	Acc@1  91.41 ( 89.38)	Acc@5 100.00 ( 99.77)
Epoch: [27][ 60/391]	Time  0.043 ( 0.043)	Data  0.001 ( 0.004)	Loss 3.2186e-01 (3.0687e-01)	Acc@1  85.94 ( 89.14)	Acc@5 100.00 ( 99.73)
Epoch: [27][ 70/391]	Time  0.044 ( 0.043)	Data  0.001 ( 0.003)	Loss 2.8351e-01 (3.0726e-01)	Acc@1  88.28 ( 89.12)	Acc@5 100.00 ( 99.72)
Epoch: [27][ 80/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.5073e-01 (3.0987e-01)	Acc@1  85.16 ( 89.04)	Acc@5 100.00 ( 99.71)
Epoch: [27][ 90/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.4917e-01 (3.1010e-01)	Acc@1  91.41 ( 89.10)	Acc@5  99.22 ( 99.69)
Epoch: [27][100/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.2836e-01 (3.1425e-01)	Acc@1  87.50 ( 88.85)	Acc@5 100.00 ( 99.71)
Epoch: [27][110/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.5433e-01 (3.1350e-01)	Acc@1  89.84 ( 88.91)	Acc@5 100.00 ( 99.73)
Epoch: [27][120/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.0151e-01 (3.1301e-01)	Acc@1  90.62 ( 88.91)	Acc@5 100.00 ( 99.73)
Epoch: [27][130/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.1841e-01 (3.1200e-01)	Acc@1  87.50 ( 88.94)	Acc@5 100.00 ( 99.74)
Epoch: [27][140/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5617e-01 (3.1578e-01)	Acc@1  89.06 ( 88.80)	Acc@5 100.00 ( 99.73)
Epoch: [27][150/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8274e-01 (3.1449e-01)	Acc@1  89.06 ( 88.91)	Acc@5  99.22 ( 99.72)
Epoch: [27][160/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1527e-01 (3.1519e-01)	Acc@1  90.62 ( 88.91)	Acc@5 100.00 ( 99.72)
Epoch: [27][170/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4671e-01 (3.1518e-01)	Acc@1  87.50 ( 88.96)	Acc@5  99.22 ( 99.72)
Epoch: [27][180/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2661e-01 (3.1543e-01)	Acc@1  83.59 ( 88.93)	Acc@5 100.00 ( 99.72)
Epoch: [27][190/391]	Time  0.043 ( 0.041)	Data  0.002 ( 0.002)	Loss 1.8622e-01 (3.1573e-01)	Acc@1  92.19 ( 88.93)	Acc@5 100.00 ( 99.72)
Epoch: [27][200/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.0299e-01 (3.1519e-01)	Acc@1  91.41 ( 88.98)	Acc@5 100.00 ( 99.70)
Epoch: [27][210/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7306e-01 (3.1435e-01)	Acc@1  86.72 ( 89.01)	Acc@5 100.00 ( 99.70)
Epoch: [27][220/391]	Time  0.046 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.8046e-01 (3.1408e-01)	Acc@1  89.84 ( 89.03)	Acc@5 100.00 ( 99.71)
Epoch: [27][230/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4293e-01 (3.1354e-01)	Acc@1  90.62 ( 89.07)	Acc@5 100.00 ( 99.72)
Epoch: [27][240/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6065e-01 (3.1408e-01)	Acc@1  86.72 ( 89.08)	Acc@5 100.00 ( 99.72)
Epoch: [27][250/391]	Time  0.040 ( 0.041)	Data  0.002 ( 0.002)	Loss 3.2731e-01 (3.1502e-01)	Acc@1  89.84 ( 89.07)	Acc@5 100.00 ( 99.72)
Epoch: [27][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2557e-01 (3.1412e-01)	Acc@1  90.62 ( 89.13)	Acc@5  99.22 ( 99.72)
Epoch: [27][270/391]	Time  0.046 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.5126e-01 (3.1434e-01)	Acc@1  82.81 ( 89.12)	Acc@5  98.44 ( 99.71)
Epoch: [27][280/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3986e-01 (3.1528e-01)	Acc@1  85.94 ( 89.05)	Acc@5  99.22 ( 99.71)
Epoch: [27][290/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8883e-01 (3.1688e-01)	Acc@1  86.72 ( 89.02)	Acc@5 100.00 ( 99.71)
Epoch: [27][300/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.0315e-01 (3.1612e-01)	Acc@1  89.84 ( 89.04)	Acc@5 100.00 ( 99.72)
Epoch: [27][310/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.8714e-01 (3.1680e-01)	Acc@1  90.62 ( 89.03)	Acc@5 100.00 ( 99.72)
Epoch: [27][320/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.8089e-01 (3.1764e-01)	Acc@1  89.06 ( 88.99)	Acc@5 100.00 ( 99.72)
Epoch: [27][330/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.2220e-01 (3.2030e-01)	Acc@1  83.59 ( 88.92)	Acc@5 100.00 ( 99.71)
Epoch: [27][340/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.0713e-01 (3.2017e-01)	Acc@1  82.03 ( 88.92)	Acc@5 100.00 ( 99.71)
Epoch: [27][350/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.9527e-01 (3.1893e-01)	Acc@1  89.84 ( 88.98)	Acc@5 100.00 ( 99.72)
Epoch: [27][360/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.1373e-01 (3.1909e-01)	Acc@1  89.06 ( 88.97)	Acc@5  99.22 ( 99.71)
Epoch: [27][370/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.0333e-01 (3.2078e-01)	Acc@1  85.16 ( 88.90)	Acc@5  99.22 ( 99.71)
Epoch: [27][380/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.0751e-01 (3.2176e-01)	Acc@1  84.38 ( 88.88)	Acc@5 100.00 ( 99.71)
Epoch: [27][390/391]	Time  0.028 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.8211e-01 (3.2164e-01)	Acc@1  90.00 ( 88.88)	Acc@5 100.00 ( 99.71)
## e[27] optimizer.zero_grad (sum) time: 0.26296448707580566
## e[27]       loss.backward (sum) time: 4.012349605560303
## e[27]      optimizer.step (sum) time: 1.7863738536834717
## epoch[27] training(only) time: 16.05517029762268
# Switched to evaluate mode...
Test: [  0/100]	Time  0.170 ( 0.170)	Loss 4.4770e-01 (4.4770e-01)	Acc@1  84.00 ( 84.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.021 ( 0.033)	Loss 6.0506e-01 (4.9981e-01)	Acc@1  78.00 ( 83.18)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.020 ( 0.026)	Loss 5.1307e-01 (4.9065e-01)	Acc@1  83.00 ( 83.48)	Acc@5 100.00 ( 99.52)
Test: [ 30/100]	Time  0.021 ( 0.024)	Loss 4.3918e-01 (4.9252e-01)	Acc@1  86.00 ( 83.94)	Acc@5  99.00 ( 99.45)
Test: [ 40/100]	Time  0.023 ( 0.023)	Loss 4.7686e-01 (4.9751e-01)	Acc@1  86.00 ( 83.83)	Acc@5  97.00 ( 99.27)
Test: [ 50/100]	Time  0.020 ( 0.023)	Loss 3.6890e-01 (4.9024e-01)	Acc@1  87.00 ( 83.98)	Acc@5  99.00 ( 99.24)
Test: [ 60/100]	Time  0.019 ( 0.022)	Loss 5.8670e-01 (4.9647e-01)	Acc@1  78.00 ( 83.75)	Acc@5 100.00 ( 99.28)
Test: [ 70/100]	Time  0.021 ( 0.022)	Loss 3.5811e-01 (4.8538e-01)	Acc@1  88.00 ( 83.92)	Acc@5 100.00 ( 99.34)
Test: [ 80/100]	Time  0.018 ( 0.022)	Loss 3.1396e-01 (4.8090e-01)	Acc@1  89.00 ( 84.07)	Acc@5  99.00 ( 99.36)
Test: [ 90/100]	Time  0.024 ( 0.022)	Loss 4.4842e-01 (4.8143e-01)	Acc@1  84.00 ( 84.03)	Acc@5 100.00 ( 99.38)
 * Acc@1 84.080 Acc@5 99.380
### epoch[27] execution time: 18.318561792373657
EPOCH 28
# current learning rate: 0.1
# Switched to train mode...
Epoch: [28][  0/391]	Time  0.213 ( 0.213)	Data  0.169 ( 0.169)	Loss 4.0965e-01 (4.0965e-01)	Acc@1  85.94 ( 85.94)	Acc@5 100.00 (100.00)
Epoch: [28][ 10/391]	Time  0.039 ( 0.057)	Data  0.001 ( 0.016)	Loss 4.3215e-01 (3.1290e-01)	Acc@1  85.16 ( 89.49)	Acc@5 100.00 ( 99.64)
Epoch: [28][ 20/391]	Time  0.042 ( 0.049)	Data  0.001 ( 0.009)	Loss 3.9501e-01 (3.1922e-01)	Acc@1  89.06 ( 89.21)	Acc@5 100.00 ( 99.63)
Epoch: [28][ 30/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.006)	Loss 2.4020e-01 (3.1323e-01)	Acc@1  92.97 ( 89.34)	Acc@5 100.00 ( 99.72)
Epoch: [28][ 40/391]	Time  0.038 ( 0.045)	Data  0.001 ( 0.005)	Loss 2.7292e-01 (3.0918e-01)	Acc@1  89.06 ( 89.39)	Acc@5 100.00 ( 99.75)
Epoch: [28][ 50/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.004)	Loss 4.1136e-01 (3.1185e-01)	Acc@1  86.72 ( 89.38)	Acc@5  99.22 ( 99.77)
Epoch: [28][ 60/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.004)	Loss 4.3912e-01 (3.1601e-01)	Acc@1  87.50 ( 89.41)	Acc@5 100.00 ( 99.78)
Epoch: [28][ 70/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 2.4035e-01 (3.0956e-01)	Acc@1  92.97 ( 89.61)	Acc@5 100.00 ( 99.74)
Epoch: [28][ 80/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.8160e-01 (3.0953e-01)	Acc@1  85.16 ( 89.67)	Acc@5 100.00 ( 99.75)
Epoch: [28][ 90/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.2228e-01 (3.1199e-01)	Acc@1  85.94 ( 89.44)	Acc@5 100.00 ( 99.73)
Epoch: [28][100/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.5144e-01 (3.0945e-01)	Acc@1  88.28 ( 89.50)	Acc@5 100.00 ( 99.74)
Epoch: [28][110/391]	Time  0.053 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.0112e-01 (3.0868e-01)	Acc@1  89.06 ( 89.54)	Acc@5 100.00 ( 99.75)
Epoch: [28][120/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.6566e-01 (3.1018e-01)	Acc@1  90.62 ( 89.41)	Acc@5 100.00 ( 99.75)
Epoch: [28][130/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.2009e-01 (3.0932e-01)	Acc@1  92.97 ( 89.41)	Acc@5 100.00 ( 99.76)
Epoch: [28][140/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.8724e-01 (3.0963e-01)	Acc@1  91.41 ( 89.40)	Acc@5 100.00 ( 99.74)
Epoch: [28][150/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4701e-01 (3.0857e-01)	Acc@1  87.50 ( 89.44)	Acc@5  99.22 ( 99.74)
Epoch: [28][160/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.0085e-01 (3.1089e-01)	Acc@1  89.84 ( 89.40)	Acc@5 100.00 ( 99.72)
Epoch: [28][170/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8210e-01 (3.1082e-01)	Acc@1  85.94 ( 89.36)	Acc@5  99.22 ( 99.72)
Epoch: [28][180/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3498e-01 (3.1145e-01)	Acc@1  89.06 ( 89.32)	Acc@5 100.00 ( 99.72)
Epoch: [28][190/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3656e-01 (3.1299e-01)	Acc@1  86.72 ( 89.22)	Acc@5 100.00 ( 99.71)
Epoch: [28][200/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5639e-01 (3.1339e-01)	Acc@1  94.53 ( 89.20)	Acc@5 100.00 ( 99.71)
Epoch: [28][210/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6309e-01 (3.1426e-01)	Acc@1  89.06 ( 89.14)	Acc@5 100.00 ( 99.70)
Epoch: [28][220/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7457e-01 (3.1326e-01)	Acc@1  90.62 ( 89.21)	Acc@5  98.44 ( 99.70)
Epoch: [28][230/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8010e-01 (3.1411e-01)	Acc@1  86.72 ( 89.20)	Acc@5 100.00 ( 99.70)
Epoch: [28][240/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2532e-01 (3.1542e-01)	Acc@1  89.06 ( 89.18)	Acc@5 100.00 ( 99.70)
Epoch: [28][250/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0425e-01 (3.1595e-01)	Acc@1  87.50 ( 89.14)	Acc@5  98.44 ( 99.70)
Epoch: [28][260/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.0571e-01 (3.1634e-01)	Acc@1  89.06 ( 89.09)	Acc@5 100.00 ( 99.70)
Epoch: [28][270/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.2236e-01 (3.1613e-01)	Acc@1  90.62 ( 89.09)	Acc@5  99.22 ( 99.70)
Epoch: [28][280/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.8170e-01 (3.1595e-01)	Acc@1  89.06 ( 89.09)	Acc@5 100.00 ( 99.70)
Epoch: [28][290/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0338e-01 (3.1588e-01)	Acc@1  84.38 ( 89.08)	Acc@5 100.00 ( 99.70)
Epoch: [28][300/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0979e-01 (3.1530e-01)	Acc@1  85.16 ( 89.08)	Acc@5  98.44 ( 99.71)
Epoch: [28][310/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4183e-01 (3.1611e-01)	Acc@1  88.28 ( 89.05)	Acc@5  98.44 ( 99.71)
Epoch: [28][320/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.3636e-01 (3.1564e-01)	Acc@1  92.97 ( 89.08)	Acc@5 100.00 ( 99.71)
Epoch: [28][330/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5060e-01 (3.1610e-01)	Acc@1  87.50 ( 89.06)	Acc@5 100.00 ( 99.71)
Epoch: [28][340/391]	Time  0.056 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.2090e-01 (3.1542e-01)	Acc@1  92.19 ( 89.10)	Acc@5  98.44 ( 99.70)
Epoch: [28][350/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.8739e-01 (3.1541e-01)	Acc@1  86.72 ( 89.06)	Acc@5 100.00 ( 99.71)
Epoch: [28][360/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.5652e-01 (3.1630e-01)	Acc@1  88.28 ( 89.03)	Acc@5 100.00 ( 99.71)
Epoch: [28][370/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.0388e-01 (3.1676e-01)	Acc@1  84.38 ( 89.01)	Acc@5 100.00 ( 99.70)
Epoch: [28][380/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.0954e-01 (3.1646e-01)	Acc@1  87.50 ( 88.98)	Acc@5  99.22 ( 99.70)
Epoch: [28][390/391]	Time  0.028 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.0162e-01 (3.1550e-01)	Acc@1  88.75 ( 89.03)	Acc@5 100.00 ( 99.70)
## e[28] optimizer.zero_grad (sum) time: 0.2630143165588379
## e[28]       loss.backward (sum) time: 3.94337797164917
## e[28]      optimizer.step (sum) time: 1.8051626682281494
## epoch[28] training(only) time: 16.038445711135864
# Switched to evaluate mode...
Test: [  0/100]	Time  0.179 ( 0.179)	Loss 3.9829e-01 (3.9829e-01)	Acc@1  87.00 ( 87.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.018 ( 0.035)	Loss 5.5461e-01 (4.4298e-01)	Acc@1  84.00 ( 85.45)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 6.1182e-01 (4.5760e-01)	Acc@1  80.00 ( 85.19)	Acc@5 100.00 ( 99.48)
Test: [ 30/100]	Time  0.020 ( 0.026)	Loss 5.0530e-01 (4.6765e-01)	Acc@1  88.00 ( 85.19)	Acc@5  99.00 ( 99.39)
Test: [ 40/100]	Time  0.019 ( 0.024)	Loss 3.9243e-01 (4.6707e-01)	Acc@1  89.00 ( 85.20)	Acc@5 100.00 ( 99.32)
Test: [ 50/100]	Time  0.021 ( 0.024)	Loss 2.5953e-01 (4.5869e-01)	Acc@1  90.00 ( 85.24)	Acc@5 100.00 ( 99.37)
Test: [ 60/100]	Time  0.020 ( 0.023)	Loss 5.2218e-01 (4.6347e-01)	Acc@1  85.00 ( 85.00)	Acc@5  99.00 ( 99.36)
Test: [ 70/100]	Time  0.023 ( 0.023)	Loss 3.8063e-01 (4.5830e-01)	Acc@1  88.00 ( 85.07)	Acc@5 100.00 ( 99.38)
Test: [ 80/100]	Time  0.020 ( 0.023)	Loss 3.6043e-01 (4.5494e-01)	Acc@1  87.00 ( 85.09)	Acc@5 100.00 ( 99.42)
Test: [ 90/100]	Time  0.021 ( 0.022)	Loss 3.0736e-01 (4.5498e-01)	Acc@1  87.00 ( 85.10)	Acc@5 100.00 ( 99.40)
 * Acc@1 85.150 Acc@5 99.400
### epoch[28] execution time: 18.393903493881226
EPOCH 29
# current learning rate: 0.1
# Switched to train mode...
Epoch: [29][  0/391]	Time  0.208 ( 0.208)	Data  0.164 ( 0.164)	Loss 2.6254e-01 (2.6254e-01)	Acc@1  92.97 ( 92.97)	Acc@5 100.00 (100.00)
Epoch: [29][ 10/391]	Time  0.038 ( 0.056)	Data  0.001 ( 0.016)	Loss 4.2027e-01 (2.8954e-01)	Acc@1  85.16 ( 90.13)	Acc@5  99.22 ( 99.79)
Epoch: [29][ 20/391]	Time  0.043 ( 0.049)	Data  0.001 ( 0.009)	Loss 3.3981e-01 (2.8829e-01)	Acc@1  90.62 ( 90.07)	Acc@5 100.00 ( 99.78)
Epoch: [29][ 30/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.006)	Loss 3.1265e-01 (2.8925e-01)	Acc@1  89.84 ( 89.97)	Acc@5 100.00 ( 99.80)
Epoch: [29][ 40/391]	Time  0.038 ( 0.045)	Data  0.001 ( 0.005)	Loss 2.4385e-01 (2.8134e-01)	Acc@1  91.41 ( 90.34)	Acc@5 100.00 ( 99.79)
Epoch: [29][ 50/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.004)	Loss 2.6170e-01 (2.7973e-01)	Acc@1  89.84 ( 90.35)	Acc@5 100.00 ( 99.80)
Epoch: [29][ 60/391]	Time  0.037 ( 0.043)	Data  0.001 ( 0.004)	Loss 3.3664e-01 (2.8841e-01)	Acc@1  88.28 ( 90.11)	Acc@5 100.00 ( 99.74)
Epoch: [29][ 70/391]	Time  0.036 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.0822e-01 (2.9628e-01)	Acc@1  89.06 ( 89.68)	Acc@5 100.00 ( 99.72)
Epoch: [29][ 80/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.2541e-01 (3.0028e-01)	Acc@1  87.50 ( 89.60)	Acc@5 100.00 ( 99.70)
Epoch: [29][ 90/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.9309e-01 (2.9712e-01)	Acc@1  87.50 ( 89.71)	Acc@5 100.00 ( 99.72)
Epoch: [29][100/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.8924e-01 (2.9570e-01)	Acc@1  89.84 ( 89.76)	Acc@5 100.00 ( 99.74)
Epoch: [29][110/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.3348e-01 (2.9838e-01)	Acc@1  83.59 ( 89.65)	Acc@5 100.00 ( 99.74)
Epoch: [29][120/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4530e-01 (2.9734e-01)	Acc@1  96.88 ( 89.64)	Acc@5 100.00 ( 99.75)
Epoch: [29][130/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4783e-01 (2.9952e-01)	Acc@1  87.50 ( 89.61)	Acc@5 100.00 ( 99.74)
Epoch: [29][140/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.5067e-01 (2.9764e-01)	Acc@1  90.62 ( 89.63)	Acc@5  99.22 ( 99.73)
Epoch: [29][150/391]	Time  0.038 ( 0.041)	Data  0.002 ( 0.002)	Loss 3.9723e-01 (2.9872e-01)	Acc@1  88.28 ( 89.60)	Acc@5 100.00 ( 99.74)
Epoch: [29][160/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.1859e-01 (3.0031e-01)	Acc@1  92.97 ( 89.55)	Acc@5 100.00 ( 99.73)
Epoch: [29][170/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.7471e-01 (3.0088e-01)	Acc@1  90.62 ( 89.49)	Acc@5 100.00 ( 99.74)
Epoch: [29][180/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5974e-01 (3.0085e-01)	Acc@1  85.94 ( 89.46)	Acc@5 100.00 ( 99.73)
Epoch: [29][190/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3399e-01 (2.9911e-01)	Acc@1  88.28 ( 89.53)	Acc@5 100.00 ( 99.73)
Epoch: [29][200/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.9359e-01 (3.0010e-01)	Acc@1  90.62 ( 89.47)	Acc@5 100.00 ( 99.74)
Epoch: [29][210/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2836e-01 (3.0145e-01)	Acc@1  83.59 ( 89.43)	Acc@5 100.00 ( 99.74)
Epoch: [29][220/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1867e-01 (3.0527e-01)	Acc@1  85.16 ( 89.29)	Acc@5  99.22 ( 99.73)
Epoch: [29][230/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.8346e-01 (3.0468e-01)	Acc@1  92.97 ( 89.31)	Acc@5 100.00 ( 99.74)
Epoch: [29][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.0681e-01 (3.0622e-01)	Acc@1  87.50 ( 89.24)	Acc@5 100.00 ( 99.72)
Epoch: [29][250/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7793e-01 (3.0693e-01)	Acc@1  86.72 ( 89.19)	Acc@5  99.22 ( 99.73)
Epoch: [29][260/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8176e-01 (3.0758e-01)	Acc@1  89.06 ( 89.22)	Acc@5  99.22 ( 99.73)
Epoch: [29][270/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.8723e-01 (3.0849e-01)	Acc@1  89.06 ( 89.20)	Acc@5  99.22 ( 99.72)
Epoch: [29][280/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6682e-01 (3.0970e-01)	Acc@1  84.38 ( 89.13)	Acc@5 100.00 ( 99.72)
Epoch: [29][290/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.3385e-01 (3.1011e-01)	Acc@1  90.62 ( 89.13)	Acc@5 100.00 ( 99.70)
Epoch: [29][300/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.3676e-01 (3.0921e-01)	Acc@1  92.19 ( 89.16)	Acc@5  99.22 ( 99.70)
Epoch: [29][310/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.7296e-01 (3.0922e-01)	Acc@1  92.97 ( 89.15)	Acc@5 100.00 ( 99.71)
Epoch: [29][320/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.5078e-01 (3.0951e-01)	Acc@1  91.41 ( 89.15)	Acc@5  99.22 ( 99.71)
Epoch: [29][330/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4740e-01 (3.0961e-01)	Acc@1  89.06 ( 89.13)	Acc@5 100.00 ( 99.71)
Epoch: [29][340/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 4.6059e-01 (3.1074e-01)	Acc@1  85.94 ( 89.10)	Acc@5 100.00 ( 99.71)
Epoch: [29][350/391]	Time  0.043 ( 0.040)	Data  0.001 ( 0.002)	Loss 3.4813e-01 (3.1111e-01)	Acc@1  88.28 ( 89.09)	Acc@5  98.44 ( 99.72)
Epoch: [29][360/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.4382e-01 (3.1173e-01)	Acc@1  91.41 ( 89.08)	Acc@5 100.00 ( 99.71)
Epoch: [29][370/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.7372e-01 (3.1175e-01)	Acc@1  89.84 ( 89.08)	Acc@5 100.00 ( 99.71)
Epoch: [29][380/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.7595e-01 (3.1158e-01)	Acc@1  94.53 ( 89.07)	Acc@5  98.44 ( 99.70)
Epoch: [29][390/391]	Time  0.025 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.9702e-01 (3.1240e-01)	Acc@1  86.25 ( 89.05)	Acc@5 100.00 ( 99.71)
## e[29] optimizer.zero_grad (sum) time: 0.2621920108795166
## e[29]       loss.backward (sum) time: 3.9102394580841064
## e[29]      optimizer.step (sum) time: 1.8531537055969238
## epoch[29] training(only) time: 15.886478662490845
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 3.7562e-01 (3.7562e-01)	Acc@1  87.00 ( 87.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.019 ( 0.035)	Loss 3.8154e-01 (4.1055e-01)	Acc@1  88.00 ( 87.00)	Acc@5 100.00 ( 99.36)
Test: [ 20/100]	Time  0.018 ( 0.028)	Loss 4.3722e-01 (4.1859e-01)	Acc@1  87.00 ( 86.71)	Acc@5 100.00 ( 99.38)
Test: [ 30/100]	Time  0.021 ( 0.025)	Loss 4.3145e-01 (4.2655e-01)	Acc@1  85.00 ( 86.61)	Acc@5  98.00 ( 99.29)
Test: [ 40/100]	Time  0.018 ( 0.024)	Loss 5.3939e-01 (4.3138e-01)	Acc@1  82.00 ( 86.37)	Acc@5  99.00 ( 99.34)
Test: [ 50/100]	Time  0.021 ( 0.023)	Loss 3.5675e-01 (4.2219e-01)	Acc@1  89.00 ( 86.53)	Acc@5  99.00 ( 99.35)
Test: [ 60/100]	Time  0.023 ( 0.023)	Loss 5.5211e-01 (4.2327e-01)	Acc@1  84.00 ( 86.51)	Acc@5  99.00 ( 99.39)
Test: [ 70/100]	Time  0.016 ( 0.023)	Loss 5.1971e-01 (4.2138e-01)	Acc@1  86.00 ( 86.51)	Acc@5 100.00 ( 99.41)
Test: [ 80/100]	Time  0.018 ( 0.023)	Loss 4.2271e-01 (4.1928e-01)	Acc@1  85.00 ( 86.59)	Acc@5 100.00 ( 99.38)
Test: [ 90/100]	Time  0.017 ( 0.022)	Loss 2.5671e-01 (4.2163e-01)	Acc@1  89.00 ( 86.40)	Acc@5 100.00 ( 99.40)
 * Acc@1 86.380 Acc@5 99.410
### epoch[29] execution time: 18.231755256652832
EPOCH 30
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [30][  0/391]	Time  0.218 ( 0.218)	Data  0.175 ( 0.175)	Loss 2.9725e-01 (2.9725e-01)	Acc@1  87.50 ( 87.50)	Acc@5 100.00 (100.00)
Epoch: [30][ 10/391]	Time  0.042 ( 0.056)	Data  0.001 ( 0.017)	Loss 2.3213e-01 (2.9159e-01)	Acc@1  92.97 ( 89.63)	Acc@5  99.22 ( 99.64)
Epoch: [30][ 20/391]	Time  0.044 ( 0.049)	Data  0.001 ( 0.009)	Loss 2.6834e-01 (2.6893e-01)	Acc@1  89.84 ( 90.29)	Acc@5 100.00 ( 99.52)
Epoch: [30][ 30/391]	Time  0.040 ( 0.046)	Data  0.001 ( 0.007)	Loss 1.9177e-01 (2.5707e-01)	Acc@1  94.53 ( 90.88)	Acc@5  99.22 ( 99.57)
Epoch: [30][ 40/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.005)	Loss 1.8675e-01 (2.4779e-01)	Acc@1  94.53 ( 91.06)	Acc@5 100.00 ( 99.68)
Epoch: [30][ 50/391]	Time  0.042 ( 0.044)	Data  0.002 ( 0.004)	Loss 1.6248e-01 (2.4424e-01)	Acc@1  93.75 ( 91.27)	Acc@5 100.00 ( 99.68)
Epoch: [30][ 60/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.8425e-01 (2.4034e-01)	Acc@1  94.53 ( 91.44)	Acc@5 100.00 ( 99.71)
Epoch: [30][ 70/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.003)	Loss 2.6255e-01 (2.3687e-01)	Acc@1  91.41 ( 91.59)	Acc@5 100.00 ( 99.75)
Epoch: [30][ 80/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.1874e-01 (2.3769e-01)	Acc@1  92.19 ( 91.58)	Acc@5 100.00 ( 99.71)
Epoch: [30][ 90/391]	Time  0.045 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.6801e-01 (2.3304e-01)	Acc@1  93.75 ( 91.79)	Acc@5  99.22 ( 99.73)
Epoch: [30][100/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.5978e-01 (2.2996e-01)	Acc@1  89.06 ( 91.92)	Acc@5 100.00 ( 99.75)
Epoch: [30][110/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.3702e-01 (2.2753e-01)	Acc@1  91.41 ( 92.03)	Acc@5 100.00 ( 99.75)
Epoch: [30][120/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.3890e-01 (2.2612e-01)	Acc@1  90.62 ( 92.03)	Acc@5 100.00 ( 99.77)
Epoch: [30][130/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3437e-01 (2.2446e-01)	Acc@1  97.66 ( 92.15)	Acc@5 100.00 ( 99.77)
Epoch: [30][140/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.6820e-01 (2.2402e-01)	Acc@1  89.84 ( 92.20)	Acc@5 100.00 ( 99.77)
Epoch: [30][150/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7443e-01 (2.2204e-01)	Acc@1  96.88 ( 92.25)	Acc@5 100.00 ( 99.78)
Epoch: [30][160/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.5317e-01 (2.2067e-01)	Acc@1  91.41 ( 92.33)	Acc@5 100.00 ( 99.80)
Epoch: [30][170/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.1615e-01 (2.2043e-01)	Acc@1  93.75 ( 92.36)	Acc@5 100.00 ( 99.80)
Epoch: [30][180/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.5303e-01 (2.2002e-01)	Acc@1  90.62 ( 92.36)	Acc@5 100.00 ( 99.81)
Epoch: [30][190/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.2135e-01 (2.1997e-01)	Acc@1  92.19 ( 92.35)	Acc@5 100.00 ( 99.82)
Epoch: [30][200/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4856e-01 (2.1980e-01)	Acc@1  94.53 ( 92.37)	Acc@5 100.00 ( 99.82)
Epoch: [30][210/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7690e-01 (2.1851e-01)	Acc@1  94.53 ( 92.44)	Acc@5 100.00 ( 99.82)
Epoch: [30][220/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6955e-01 (2.1671e-01)	Acc@1  95.31 ( 92.53)	Acc@5 100.00 ( 99.83)
Epoch: [30][230/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.8106e-01 (2.1505e-01)	Acc@1  93.75 ( 92.59)	Acc@5 100.00 ( 99.83)
Epoch: [30][240/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.9635e-01 (2.1457e-01)	Acc@1  96.09 ( 92.62)	Acc@5 100.00 ( 99.82)
Epoch: [30][250/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1213e-01 (2.1511e-01)	Acc@1  87.50 ( 92.61)	Acc@5  99.22 ( 99.83)
Epoch: [30][260/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.5863e-01 (2.1451e-01)	Acc@1  91.41 ( 92.63)	Acc@5 100.00 ( 99.83)
Epoch: [30][270/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.9561e-01 (2.1303e-01)	Acc@1  93.75 ( 92.68)	Acc@5 100.00 ( 99.83)
Epoch: [30][280/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5513e-01 (2.1346e-01)	Acc@1  96.09 ( 92.66)	Acc@5  99.22 ( 99.83)
Epoch: [30][290/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.1391e-01 (2.1320e-01)	Acc@1  92.19 ( 92.67)	Acc@5 100.00 ( 99.83)
Epoch: [30][300/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.2813e-01 (2.1329e-01)	Acc@1  92.97 ( 92.67)	Acc@5 100.00 ( 99.84)
Epoch: [30][310/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.8583e-01 (2.1304e-01)	Acc@1  92.97 ( 92.68)	Acc@5  99.22 ( 99.84)
Epoch: [30][320/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.5766e-01 (2.1282e-01)	Acc@1  92.97 ( 92.71)	Acc@5 100.00 ( 99.84)
Epoch: [30][330/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5021e-01 (2.1188e-01)	Acc@1  95.31 ( 92.75)	Acc@5 100.00 ( 99.84)
Epoch: [30][340/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.5562e-02 (2.1192e-01)	Acc@1  98.44 ( 92.75)	Acc@5 100.00 ( 99.84)
Epoch: [30][350/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.2345e-01 (2.1148e-01)	Acc@1  90.62 ( 92.76)	Acc@5 100.00 ( 99.84)
Epoch: [30][360/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3142e-01 (2.1072e-01)	Acc@1  96.88 ( 92.80)	Acc@5 100.00 ( 99.84)
Epoch: [30][370/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.6623e-01 (2.0984e-01)	Acc@1  94.53 ( 92.83)	Acc@5 100.00 ( 99.84)
Epoch: [30][380/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.4874e-01 (2.0988e-01)	Acc@1  89.84 ( 92.82)	Acc@5 100.00 ( 99.85)
Epoch: [30][390/391]	Time  0.030 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.6184e-01 (2.0925e-01)	Acc@1  90.00 ( 92.84)	Acc@5 100.00 ( 99.84)
## e[30] optimizer.zero_grad (sum) time: 0.2628209590911865
## e[30]       loss.backward (sum) time: 3.980215549468994
## e[30]      optimizer.step (sum) time: 1.8294620513916016
## epoch[30] training(only) time: 16.000855207443237
# Switched to evaluate mode...
Test: [  0/100]	Time  0.217 ( 0.217)	Loss 2.4446e-01 (2.4446e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.022 ( 0.039)	Loss 3.2988e-01 (2.8182e-01)	Acc@1  88.00 ( 90.09)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.018 ( 0.030)	Loss 4.2244e-01 (2.8955e-01)	Acc@1  86.00 ( 90.05)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 3.3372e-01 (3.0488e-01)	Acc@1  90.00 ( 90.06)	Acc@5  99.00 ( 99.65)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 3.8385e-01 (3.1156e-01)	Acc@1  89.00 ( 89.88)	Acc@5  99.00 ( 99.63)
Test: [ 50/100]	Time  0.021 ( 0.025)	Loss 2.4635e-01 (3.0728e-01)	Acc@1  93.00 ( 90.14)	Acc@5 100.00 ( 99.65)
Test: [ 60/100]	Time  0.017 ( 0.024)	Loss 3.7486e-01 (3.1021e-01)	Acc@1  89.00 ( 89.85)	Acc@5 100.00 ( 99.70)
Test: [ 70/100]	Time  0.021 ( 0.024)	Loss 2.8605e-01 (3.0610e-01)	Acc@1  88.00 ( 89.76)	Acc@5 100.00 ( 99.69)
Test: [ 80/100]	Time  0.021 ( 0.024)	Loss 2.0393e-01 (3.0262e-01)	Acc@1  92.00 ( 89.89)	Acc@5 100.00 ( 99.68)
Test: [ 90/100]	Time  0.018 ( 0.023)	Loss 1.5686e-01 (3.0135e-01)	Acc@1  93.00 ( 89.96)	Acc@5 100.00 ( 99.69)
 * Acc@1 90.020 Acc@5 99.700
### epoch[30] execution time: 18.40291118621826
EPOCH 31
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [31][  0/391]	Time  0.217 ( 0.217)	Data  0.172 ( 0.172)	Loss 1.5701e-01 (1.5701e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [31][ 10/391]	Time  0.040 ( 0.057)	Data  0.001 ( 0.017)	Loss 1.9148e-01 (1.7178e-01)	Acc@1  94.53 ( 94.39)	Acc@5 100.00 (100.00)
Epoch: [31][ 20/391]	Time  0.038 ( 0.049)	Data  0.001 ( 0.009)	Loss 1.2300e-01 (1.7025e-01)	Acc@1  96.09 ( 94.46)	Acc@5 100.00 (100.00)
Epoch: [31][ 30/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.006)	Loss 2.3348e-01 (1.8625e-01)	Acc@1  92.97 ( 93.90)	Acc@5 100.00 ( 99.95)
Epoch: [31][ 40/391]	Time  0.042 ( 0.045)	Data  0.001 ( 0.005)	Loss 2.4609e-01 (1.7953e-01)	Acc@1  90.62 ( 94.09)	Acc@5  99.22 ( 99.94)
Epoch: [31][ 50/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.9195e-01 (1.8395e-01)	Acc@1  94.53 ( 93.80)	Acc@5  99.22 ( 99.92)
Epoch: [31][ 60/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.004)	Loss 2.1179e-01 (1.8448e-01)	Acc@1  92.19 ( 93.70)	Acc@5 100.00 ( 99.92)
Epoch: [31][ 70/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.6581e-01 (1.8327e-01)	Acc@1  92.19 ( 93.72)	Acc@5 100.00 ( 99.93)
Epoch: [31][ 80/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.4046e-01 (1.8077e-01)	Acc@1  95.31 ( 93.75)	Acc@5 100.00 ( 99.93)
Epoch: [31][ 90/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.3467e-01 (1.7984e-01)	Acc@1  95.31 ( 93.82)	Acc@5 100.00 ( 99.94)
Epoch: [31][100/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.6791e-01 (1.7984e-01)	Acc@1  92.19 ( 93.84)	Acc@5 100.00 ( 99.93)
Epoch: [31][110/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.1543e-01 (1.7949e-01)	Acc@1  92.19 ( 93.89)	Acc@5 100.00 ( 99.91)
Epoch: [31][120/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.2092e-01 (1.8100e-01)	Acc@1  90.62 ( 93.82)	Acc@5 100.00 ( 99.90)
Epoch: [31][130/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.8750e-01 (1.8190e-01)	Acc@1  94.53 ( 93.85)	Acc@5 100.00 ( 99.90)
Epoch: [31][140/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.9380e-01 (1.8076e-01)	Acc@1  94.53 ( 93.86)	Acc@5 100.00 ( 99.91)
Epoch: [31][150/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6501e-01 (1.7939e-01)	Acc@1  95.31 ( 93.90)	Acc@5 100.00 ( 99.91)
Epoch: [31][160/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2264e-01 (1.7924e-01)	Acc@1  96.09 ( 93.90)	Acc@5 100.00 ( 99.90)
Epoch: [31][170/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4062e-01 (1.7899e-01)	Acc@1  96.09 ( 93.89)	Acc@5 100.00 ( 99.91)
Epoch: [31][180/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.2305e-01 (1.7916e-01)	Acc@1  93.75 ( 93.90)	Acc@5  99.22 ( 99.91)
Epoch: [31][190/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3554e-01 (1.7821e-01)	Acc@1  95.31 ( 93.93)	Acc@5 100.00 ( 99.91)
Epoch: [31][200/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5968e-01 (1.7811e-01)	Acc@1  93.75 ( 93.92)	Acc@5 100.00 ( 99.91)
Epoch: [31][210/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.9690e-01 (1.7837e-01)	Acc@1  92.97 ( 93.91)	Acc@5 100.00 ( 99.90)
Epoch: [31][220/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4762e-01 (1.7796e-01)	Acc@1  96.88 ( 93.94)	Acc@5 100.00 ( 99.90)
Epoch: [31][230/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.5524e-01 (1.7829e-01)	Acc@1  90.62 ( 93.92)	Acc@5 100.00 ( 99.91)
Epoch: [31][240/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7026e-01 (1.7857e-01)	Acc@1  96.09 ( 93.90)	Acc@5 100.00 ( 99.91)
Epoch: [31][250/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5180e-01 (1.7854e-01)	Acc@1  88.28 ( 93.90)	Acc@5 100.00 ( 99.90)
Epoch: [31][260/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.8536e-01 (1.7804e-01)	Acc@1  93.75 ( 93.90)	Acc@5 100.00 ( 99.91)
Epoch: [31][270/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7986e-01 (1.7857e-01)	Acc@1  94.53 ( 93.90)	Acc@5 100.00 ( 99.90)
Epoch: [31][280/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.7313e-01 (1.7844e-01)	Acc@1  89.84 ( 93.91)	Acc@5 100.00 ( 99.91)
Epoch: [31][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3491e-01 (1.7810e-01)	Acc@1  96.09 ( 93.90)	Acc@5 100.00 ( 99.91)
Epoch: [31][300/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.0228e-01 (1.7789e-01)	Acc@1  94.53 ( 93.90)	Acc@5 100.00 ( 99.91)
Epoch: [31][310/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.3708e-01 (1.7753e-01)	Acc@1  95.31 ( 93.93)	Acc@5 100.00 ( 99.91)
Epoch: [31][320/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.6442e-01 (1.7804e-01)	Acc@1  89.06 ( 93.92)	Acc@5  99.22 ( 99.91)
Epoch: [31][330/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.8816e-01 (1.7810e-01)	Acc@1  94.53 ( 93.91)	Acc@5 100.00 ( 99.91)
Epoch: [31][340/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.1196e-01 (1.7821e-01)	Acc@1  96.09 ( 93.90)	Acc@5 100.00 ( 99.91)
Epoch: [31][350/391]	Time  0.035 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.9107e-01 (1.7810e-01)	Acc@1  91.41 ( 93.91)	Acc@5 100.00 ( 99.91)
Epoch: [31][360/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.7832e-01 (1.7701e-01)	Acc@1  96.09 ( 93.95)	Acc@5 100.00 ( 99.91)
Epoch: [31][370/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.2701e-01 (1.7700e-01)	Acc@1  90.62 ( 93.94)	Acc@5  99.22 ( 99.91)
Epoch: [31][380/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.5435e-01 (1.7708e-01)	Acc@1  92.97 ( 93.92)	Acc@5  99.22 ( 99.90)
Epoch: [31][390/391]	Time  0.027 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.1197e-01 (1.7679e-01)	Acc@1  96.25 ( 93.93)	Acc@5 100.00 ( 99.90)
## e[31] optimizer.zero_grad (sum) time: 0.2653329372406006
## e[31]       loss.backward (sum) time: 4.046959400177002
## e[31]      optimizer.step (sum) time: 1.7878036499023438
## epoch[31] training(only) time: 16.033799648284912
# Switched to evaluate mode...
Test: [  0/100]	Time  0.176 ( 0.176)	Loss 2.5373e-01 (2.5373e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.016 ( 0.035)	Loss 3.2339e-01 (2.7816e-01)	Acc@1  91.00 ( 90.45)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.024 ( 0.029)	Loss 4.2032e-01 (2.8292e-01)	Acc@1  88.00 ( 90.19)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.021 ( 0.027)	Loss 3.6265e-01 (2.9958e-01)	Acc@1  89.00 ( 90.16)	Acc@5  98.00 ( 99.61)
Test: [ 40/100]	Time  0.021 ( 0.026)	Loss 3.6741e-01 (3.0640e-01)	Acc@1  89.00 ( 89.98)	Acc@5 100.00 ( 99.59)
Test: [ 50/100]	Time  0.021 ( 0.025)	Loss 2.6924e-01 (3.0099e-01)	Acc@1  92.00 ( 90.43)	Acc@5 100.00 ( 99.61)
Test: [ 60/100]	Time  0.019 ( 0.024)	Loss 3.6211e-01 (3.0486e-01)	Acc@1  87.00 ( 90.13)	Acc@5 100.00 ( 99.64)
Test: [ 70/100]	Time  0.020 ( 0.024)	Loss 3.4894e-01 (3.0022e-01)	Acc@1  87.00 ( 90.18)	Acc@5 100.00 ( 99.63)
Test: [ 80/100]	Time  0.021 ( 0.024)	Loss 2.0910e-01 (2.9699e-01)	Acc@1  92.00 ( 90.28)	Acc@5 100.00 ( 99.65)
Test: [ 90/100]	Time  0.017 ( 0.023)	Loss 1.6547e-01 (2.9588e-01)	Acc@1  93.00 ( 90.31)	Acc@5 100.00 ( 99.68)
 * Acc@1 90.350 Acc@5 99.690
### epoch[31] execution time: 18.43549394607544
EPOCH 32
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [32][  0/391]	Time  0.214 ( 0.214)	Data  0.170 ( 0.170)	Loss 1.6452e-01 (1.6452e-01)	Acc@1  92.97 ( 92.97)	Acc@5 100.00 (100.00)
Epoch: [32][ 10/391]	Time  0.041 ( 0.057)	Data  0.001 ( 0.016)	Loss 1.2239e-01 (1.7343e-01)	Acc@1  95.31 ( 94.53)	Acc@5 100.00 ( 99.86)
Epoch: [32][ 20/391]	Time  0.040 ( 0.050)	Data  0.001 ( 0.009)	Loss 1.9977e-01 (1.7174e-01)	Acc@1  93.75 ( 94.31)	Acc@5  99.22 ( 99.78)
Epoch: [32][ 30/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.006)	Loss 1.5691e-01 (1.7088e-01)	Acc@1  93.75 ( 94.00)	Acc@5 100.00 ( 99.85)
Epoch: [32][ 40/391]	Time  0.038 ( 0.045)	Data  0.001 ( 0.005)	Loss 1.8905e-01 (1.6662e-01)	Acc@1  94.53 ( 94.19)	Acc@5  99.22 ( 99.87)
Epoch: [32][ 50/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.4743e-01 (1.6416e-01)	Acc@1  92.97 ( 94.24)	Acc@5 100.00 ( 99.86)
Epoch: [32][ 60/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.1640e-01 (1.6330e-01)	Acc@1  96.09 ( 94.33)	Acc@5 100.00 ( 99.88)
Epoch: [32][ 70/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.9129e-01 (1.6549e-01)	Acc@1  92.19 ( 94.21)	Acc@5 100.00 ( 99.89)
Epoch: [32][ 80/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.8232e-01 (1.6752e-01)	Acc@1  95.31 ( 94.14)	Acc@5  99.22 ( 99.87)
Epoch: [32][ 90/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.3558e-01 (1.6666e-01)	Acc@1  91.41 ( 94.24)	Acc@5 100.00 ( 99.89)
Epoch: [32][100/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.1113e-01 (1.6666e-01)	Acc@1  92.19 ( 94.23)	Acc@5 100.00 ( 99.90)
Epoch: [32][110/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.6019e-01 (1.6738e-01)	Acc@1  93.75 ( 94.26)	Acc@5  99.22 ( 99.89)
Epoch: [32][120/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.7563e-01 (1.6963e-01)	Acc@1  93.75 ( 94.12)	Acc@5 100.00 ( 99.89)
Epoch: [32][130/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7096e-01 (1.6805e-01)	Acc@1  93.75 ( 94.20)	Acc@5 100.00 ( 99.90)
Epoch: [32][140/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.3772e-01 (1.7090e-01)	Acc@1  92.19 ( 94.09)	Acc@5 100.00 ( 99.89)
Epoch: [32][150/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4471e-01 (1.6985e-01)	Acc@1  94.53 ( 94.11)	Acc@5  99.22 ( 99.89)
Epoch: [32][160/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.9273e-01 (1.6861e-01)	Acc@1  94.53 ( 94.16)	Acc@5 100.00 ( 99.89)
Epoch: [32][170/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6606e-01 (1.6862e-01)	Acc@1  94.53 ( 94.17)	Acc@5 100.00 ( 99.89)
Epoch: [32][180/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.0753e-01 (1.6965e-01)	Acc@1  92.19 ( 94.17)	Acc@5 100.00 ( 99.89)
Epoch: [32][190/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7493e-01 (1.6994e-01)	Acc@1  96.09 ( 94.17)	Acc@5  99.22 ( 99.89)
Epoch: [32][200/391]	Time  0.045 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0585e-01 (1.7101e-01)	Acc@1  96.09 ( 94.12)	Acc@5 100.00 ( 99.89)
Epoch: [32][210/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.1024e-01 (1.7202e-01)	Acc@1  91.41 ( 94.09)	Acc@5 100.00 ( 99.88)
Epoch: [32][220/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1434e-01 (1.7252e-01)	Acc@1  96.88 ( 94.09)	Acc@5 100.00 ( 99.88)
Epoch: [32][230/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6722e-01 (1.7172e-01)	Acc@1  95.31 ( 94.13)	Acc@5 100.00 ( 99.88)
Epoch: [32][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7240e-01 (1.7098e-01)	Acc@1  96.09 ( 94.14)	Acc@5 100.00 ( 99.89)
Epoch: [32][250/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7002e-01 (1.7035e-01)	Acc@1  92.97 ( 94.15)	Acc@5 100.00 ( 99.89)
Epoch: [32][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0498e-01 (1.6897e-01)	Acc@1  96.88 ( 94.20)	Acc@5 100.00 ( 99.89)
Epoch: [32][270/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.4146e-01 (1.6940e-01)	Acc@1  90.62 ( 94.17)	Acc@5  99.22 ( 99.89)
Epoch: [32][280/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5828e-01 (1.6916e-01)	Acc@1  93.75 ( 94.18)	Acc@5 100.00 ( 99.89)
Epoch: [32][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5936e-01 (1.6942e-01)	Acc@1  95.31 ( 94.17)	Acc@5 100.00 ( 99.88)
Epoch: [32][300/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.9170e-01 (1.6905e-01)	Acc@1  92.97 ( 94.20)	Acc@5 100.00 ( 99.89)
Epoch: [32][310/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2223e-01 (1.6911e-01)	Acc@1  96.09 ( 94.19)	Acc@5 100.00 ( 99.89)
Epoch: [32][320/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.7437e-01 (1.6835e-01)	Acc@1  92.97 ( 94.21)	Acc@5 100.00 ( 99.89)
Epoch: [32][330/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.0607e-01 (1.6753e-01)	Acc@1  93.75 ( 94.25)	Acc@5 100.00 ( 99.89)
Epoch: [32][340/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.1309e-01 (1.6703e-01)	Acc@1  96.09 ( 94.27)	Acc@5 100.00 ( 99.89)
Epoch: [32][350/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.2500e-01 (1.6657e-01)	Acc@1  98.44 ( 94.28)	Acc@5 100.00 ( 99.89)
Epoch: [32][360/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.4889e-01 (1.6657e-01)	Acc@1  92.97 ( 94.27)	Acc@5  99.22 ( 99.89)
Epoch: [32][370/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.1642e-01 (1.6656e-01)	Acc@1  92.19 ( 94.27)	Acc@5 100.00 ( 99.89)
Epoch: [32][380/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.4131e-01 (1.6644e-01)	Acc@1  94.53 ( 94.26)	Acc@5 100.00 ( 99.89)
Epoch: [32][390/391]	Time  0.027 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.9713e-01 (1.6619e-01)	Acc@1  90.00 ( 94.27)	Acc@5 100.00 ( 99.89)
## e[32] optimizer.zero_grad (sum) time: 0.26317811012268066
## e[32]       loss.backward (sum) time: 4.013659477233887
## e[32]      optimizer.step (sum) time: 1.8192007541656494
## epoch[32] training(only) time: 16.035496711730957
# Switched to evaluate mode...
Test: [  0/100]	Time  0.207 ( 0.207)	Loss 2.5539e-01 (2.5539e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.021 ( 0.037)	Loss 4.1470e-01 (2.7458e-01)	Acc@1  87.00 ( 90.55)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.021 ( 0.029)	Loss 4.6913e-01 (2.8808e-01)	Acc@1  86.00 ( 90.29)	Acc@5  99.00 ( 99.86)
Test: [ 30/100]	Time  0.021 ( 0.026)	Loss 3.7120e-01 (3.0131e-01)	Acc@1  89.00 ( 90.23)	Acc@5  98.00 ( 99.77)
Test: [ 40/100]	Time  0.021 ( 0.025)	Loss 3.9941e-01 (3.1290e-01)	Acc@1  88.00 ( 89.98)	Acc@5 100.00 ( 99.76)
Test: [ 50/100]	Time  0.018 ( 0.024)	Loss 2.7904e-01 (3.0791e-01)	Acc@1  92.00 ( 90.22)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.018 ( 0.023)	Loss 3.2943e-01 (3.0808e-01)	Acc@1  89.00 ( 90.07)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.021 ( 0.023)	Loss 3.6202e-01 (3.0278e-01)	Acc@1  88.00 ( 90.08)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.021 ( 0.022)	Loss 2.3682e-01 (2.9973e-01)	Acc@1  92.00 ( 90.25)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.021 ( 0.022)	Loss 1.4362e-01 (2.9781e-01)	Acc@1  94.00 ( 90.27)	Acc@5 100.00 ( 99.77)
 * Acc@1 90.340 Acc@5 99.770
### epoch[32] execution time: 18.34205913543701
EPOCH 33
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [33][  0/391]	Time  0.228 ( 0.228)	Data  0.186 ( 0.186)	Loss 7.0606e-02 (7.0606e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [33][ 10/391]	Time  0.039 ( 0.057)	Data  0.001 ( 0.018)	Loss 1.6521e-01 (1.3447e-01)	Acc@1  94.53 ( 95.88)	Acc@5 100.00 ( 99.93)
Epoch: [33][ 20/391]	Time  0.038 ( 0.049)	Data  0.001 ( 0.010)	Loss 2.2585e-01 (1.4888e-01)	Acc@1  91.41 ( 95.24)	Acc@5  99.22 ( 99.93)
Epoch: [33][ 30/391]	Time  0.040 ( 0.046)	Data  0.001 ( 0.007)	Loss 1.7840e-01 (1.4622e-01)	Acc@1  93.75 ( 95.16)	Acc@5 100.00 ( 99.95)
Epoch: [33][ 40/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.005)	Loss 2.4470e-01 (1.5236e-01)	Acc@1  88.28 ( 94.99)	Acc@5 100.00 ( 99.96)
Epoch: [33][ 50/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.005)	Loss 1.8010e-01 (1.4925e-01)	Acc@1  95.31 ( 95.14)	Acc@5 100.00 ( 99.95)
Epoch: [33][ 60/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.004)	Loss 2.4315e-01 (1.4984e-01)	Acc@1  92.19 ( 95.16)	Acc@5 100.00 ( 99.96)
Epoch: [33][ 70/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.6181e-01 (1.4970e-01)	Acc@1  92.97 ( 95.11)	Acc@5 100.00 ( 99.97)
Epoch: [33][ 80/391]	Time  0.043 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.7006e-01 (1.4822e-01)	Acc@1  92.19 ( 95.15)	Acc@5 100.00 ( 99.97)
Epoch: [33][ 90/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.7551e-02 (1.4999e-01)	Acc@1  99.22 ( 95.04)	Acc@5 100.00 ( 99.95)
Epoch: [33][100/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.003)	Loss 9.1585e-02 (1.5191e-01)	Acc@1  96.88 ( 94.97)	Acc@5 100.00 ( 99.95)
Epoch: [33][110/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.0636e-01 (1.5290e-01)	Acc@1  92.97 ( 94.86)	Acc@5 100.00 ( 99.95)
Epoch: [33][120/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.1728e-01 (1.5074e-01)	Acc@1  97.66 ( 94.92)	Acc@5 100.00 ( 99.95)
Epoch: [33][130/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.5338e-01 (1.5096e-01)	Acc@1  95.31 ( 94.90)	Acc@5 100.00 ( 99.94)
Epoch: [33][140/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3343e-01 (1.5125e-01)	Acc@1  96.09 ( 94.87)	Acc@5 100.00 ( 99.94)
Epoch: [33][150/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.7887e-02 (1.5159e-01)	Acc@1  98.44 ( 94.85)	Acc@5 100.00 ( 99.94)
Epoch: [33][160/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.2628e-01 (1.5094e-01)	Acc@1  92.97 ( 94.88)	Acc@5 100.00 ( 99.94)
Epoch: [33][170/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.9087e-01 (1.5231e-01)	Acc@1  93.75 ( 94.85)	Acc@5 100.00 ( 99.93)
Epoch: [33][180/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.4131e-01 (1.5485e-01)	Acc@1  92.97 ( 94.76)	Acc@5 100.00 ( 99.92)
Epoch: [33][190/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1394e-01 (1.5460e-01)	Acc@1  96.09 ( 94.76)	Acc@5 100.00 ( 99.92)
Epoch: [33][200/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6093e-01 (1.5457e-01)	Acc@1  93.75 ( 94.76)	Acc@5 100.00 ( 99.92)
Epoch: [33][210/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2126e-01 (1.5426e-01)	Acc@1  97.66 ( 94.76)	Acc@5 100.00 ( 99.92)
Epoch: [33][220/391]	Time  0.045 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.4505e-02 (1.5414e-01)	Acc@1  96.09 ( 94.75)	Acc@5 100.00 ( 99.93)
Epoch: [33][230/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1317e-01 (1.5350e-01)	Acc@1  96.88 ( 94.79)	Acc@5 100.00 ( 99.93)
Epoch: [33][240/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.2246e-01 (1.5365e-01)	Acc@1  92.19 ( 94.80)	Acc@5 100.00 ( 99.93)
Epoch: [33][250/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.2578e-01 (1.5333e-01)	Acc@1  92.19 ( 94.80)	Acc@5 100.00 ( 99.93)
Epoch: [33][260/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.3751e-02 (1.5337e-01)	Acc@1  96.88 ( 94.79)	Acc@5 100.00 ( 99.93)
Epoch: [33][270/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2863e-01 (1.5414e-01)	Acc@1  95.31 ( 94.75)	Acc@5 100.00 ( 99.93)
Epoch: [33][280/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3918e-01 (1.5412e-01)	Acc@1  93.75 ( 94.74)	Acc@5 100.00 ( 99.93)
Epoch: [33][290/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5586e-01 (1.5442e-01)	Acc@1  92.19 ( 94.72)	Acc@5 100.00 ( 99.93)
Epoch: [33][300/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.9458e-01 (1.5474e-01)	Acc@1  92.97 ( 94.69)	Acc@5 100.00 ( 99.93)
Epoch: [33][310/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1112e-01 (1.5473e-01)	Acc@1  96.88 ( 94.68)	Acc@5 100.00 ( 99.93)
Epoch: [33][320/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4772e-01 (1.5478e-01)	Acc@1  96.09 ( 94.69)	Acc@5 100.00 ( 99.93)
Epoch: [33][330/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.9596e-01 (1.5580e-01)	Acc@1  92.97 ( 94.66)	Acc@5 100.00 ( 99.92)
Epoch: [33][340/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.2891e-01 (1.5528e-01)	Acc@1  95.31 ( 94.68)	Acc@5 100.00 ( 99.93)
Epoch: [33][350/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 9.4952e-02 (1.5480e-01)	Acc@1  97.66 ( 94.70)	Acc@5 100.00 ( 99.92)
Epoch: [33][360/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.5804e-01 (1.5568e-01)	Acc@1  91.41 ( 94.65)	Acc@5  99.22 ( 99.92)
Epoch: [33][370/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.4874e-01 (1.5685e-01)	Acc@1  92.97 ( 94.61)	Acc@5 100.00 ( 99.92)
Epoch: [33][380/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.1057e-01 (1.5696e-01)	Acc@1  89.84 ( 94.60)	Acc@5 100.00 ( 99.92)
Epoch: [33][390/391]	Time  0.028 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.5136e-01 (1.5641e-01)	Acc@1  96.25 ( 94.62)	Acc@5 100.00 ( 99.93)
## e[33] optimizer.zero_grad (sum) time: 0.2630496025085449
## e[33]       loss.backward (sum) time: 3.9857335090637207
## e[33]      optimizer.step (sum) time: 1.7842226028442383
## epoch[33] training(only) time: 16.03598427772522
# Switched to evaluate mode...
Test: [  0/100]	Time  0.174 ( 0.174)	Loss 2.4356e-01 (2.4356e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.036)	Loss 3.9167e-01 (2.8005e-01)	Acc@1  90.00 ( 91.18)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.020 ( 0.029)	Loss 4.0646e-01 (2.8930e-01)	Acc@1  87.00 ( 90.52)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.017 ( 0.025)	Loss 3.6773e-01 (3.0761e-01)	Acc@1  89.00 ( 90.48)	Acc@5  99.00 ( 99.74)
Test: [ 40/100]	Time  0.021 ( 0.024)	Loss 3.7395e-01 (3.1794e-01)	Acc@1  88.00 ( 90.07)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.020 ( 0.023)	Loss 2.9870e-01 (3.1219e-01)	Acc@1  92.00 ( 90.31)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.017 ( 0.022)	Loss 3.6037e-01 (3.1197e-01)	Acc@1  88.00 ( 90.10)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.022 ( 0.022)	Loss 3.3629e-01 (3.0554e-01)	Acc@1  87.00 ( 90.14)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.023 ( 0.022)	Loss 2.4620e-01 (3.0372e-01)	Acc@1  93.00 ( 90.28)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.023 ( 0.022)	Loss 1.8462e-01 (3.0263e-01)	Acc@1  93.00 ( 90.30)	Acc@5 100.00 ( 99.79)
 * Acc@1 90.320 Acc@5 99.790
### epoch[33] execution time: 18.326171159744263
EPOCH 34
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [34][  0/391]	Time  0.205 ( 0.205)	Data  0.160 ( 0.160)	Loss 1.4807e-01 (1.4807e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [34][ 10/391]	Time  0.042 ( 0.055)	Data  0.001 ( 0.015)	Loss 1.0906e-01 (1.3497e-01)	Acc@1  96.09 ( 95.60)	Acc@5 100.00 ( 99.93)
Epoch: [34][ 20/391]	Time  0.044 ( 0.049)	Data  0.001 ( 0.009)	Loss 1.6510e-01 (1.4019e-01)	Acc@1  92.97 ( 95.05)	Acc@5 100.00 ( 99.96)
Epoch: [34][ 30/391]	Time  0.042 ( 0.047)	Data  0.001 ( 0.006)	Loss 1.6740e-01 (1.4304e-01)	Acc@1  94.53 ( 95.01)	Acc@5 100.00 ( 99.95)
Epoch: [34][ 40/391]	Time  0.038 ( 0.045)	Data  0.001 ( 0.005)	Loss 1.3009e-01 (1.4138e-01)	Acc@1  96.88 ( 95.12)	Acc@5 100.00 ( 99.92)
Epoch: [34][ 50/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.004)	Loss 2.2926e-01 (1.4954e-01)	Acc@1  92.97 ( 94.96)	Acc@5 100.00 ( 99.89)
Epoch: [34][ 60/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.1363e-01 (1.4796e-01)	Acc@1  96.09 ( 94.98)	Acc@5 100.00 ( 99.91)
Epoch: [34][ 70/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 2.2888e-01 (1.5145e-01)	Acc@1  92.19 ( 94.85)	Acc@5 100.00 ( 99.90)
Epoch: [34][ 80/391]	Time  0.043 ( 0.043)	Data  0.001 ( 0.003)	Loss 7.8956e-02 (1.4977e-01)	Acc@1  98.44 ( 94.91)	Acc@5 100.00 ( 99.90)
Epoch: [34][ 90/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.9693e-01 (1.5114e-01)	Acc@1  92.19 ( 94.81)	Acc@5  99.22 ( 99.91)
Epoch: [34][100/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.4318e-01 (1.4993e-01)	Acc@1  92.97 ( 94.86)	Acc@5 100.00 ( 99.91)
Epoch: [34][110/391]	Time  0.047 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.4134e-01 (1.5059e-01)	Acc@1  92.19 ( 94.88)	Acc@5 100.00 ( 99.92)
Epoch: [34][120/391]	Time  0.035 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.5741e-01 (1.4976e-01)	Acc@1  94.53 ( 94.94)	Acc@5  99.22 ( 99.92)
Epoch: [34][130/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.2598e-01 (1.5021e-01)	Acc@1  95.31 ( 94.86)	Acc@5 100.00 ( 99.90)
Epoch: [34][140/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.1905e-01 (1.5024e-01)	Acc@1  96.09 ( 94.86)	Acc@5 100.00 ( 99.91)
Epoch: [34][150/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2157e-01 (1.5025e-01)	Acc@1  96.09 ( 94.86)	Acc@5 100.00 ( 99.91)
Epoch: [34][160/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.7975e-02 (1.4968e-01)	Acc@1  96.09 ( 94.83)	Acc@5 100.00 ( 99.91)
Epoch: [34][170/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4125e-01 (1.5005e-01)	Acc@1  94.53 ( 94.84)	Acc@5 100.00 ( 99.90)
Epoch: [34][180/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4488e-01 (1.4966e-01)	Acc@1  94.53 ( 94.89)	Acc@5 100.00 ( 99.90)
Epoch: [34][190/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2316e-01 (1.4936e-01)	Acc@1  94.53 ( 94.86)	Acc@5 100.00 ( 99.90)
Epoch: [34][200/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6561e-01 (1.4904e-01)	Acc@1  93.75 ( 94.85)	Acc@5 100.00 ( 99.91)
Epoch: [34][210/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0126e-01 (1.4862e-01)	Acc@1  97.66 ( 94.88)	Acc@5 100.00 ( 99.91)
Epoch: [34][220/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5304e-01 (1.4852e-01)	Acc@1  94.53 ( 94.89)	Acc@5 100.00 ( 99.91)
Epoch: [34][230/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6064e-01 (1.4762e-01)	Acc@1  92.97 ( 94.92)	Acc@5 100.00 ( 99.91)
Epoch: [34][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4105e-01 (1.4830e-01)	Acc@1  95.31 ( 94.87)	Acc@5 100.00 ( 99.91)
Epoch: [34][250/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3008e-01 (1.4917e-01)	Acc@1  96.88 ( 94.81)	Acc@5 100.00 ( 99.91)
Epoch: [34][260/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5593e-01 (1.4874e-01)	Acc@1  94.53 ( 94.82)	Acc@5 100.00 ( 99.91)
Epoch: [34][270/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7761e-01 (1.4880e-01)	Acc@1  95.31 ( 94.84)	Acc@5  99.22 ( 99.91)
Epoch: [34][280/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.8709e-01 (1.4902e-01)	Acc@1  93.75 ( 94.84)	Acc@5 100.00 ( 99.91)
Epoch: [34][290/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.4772e-01 (1.4904e-01)	Acc@1  92.19 ( 94.85)	Acc@5  99.22 ( 99.91)
Epoch: [34][300/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0417e-01 (1.4918e-01)	Acc@1  96.09 ( 94.84)	Acc@5 100.00 ( 99.91)
Epoch: [34][310/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6932e-01 (1.4911e-01)	Acc@1  95.31 ( 94.84)	Acc@5  99.22 ( 99.91)
Epoch: [34][320/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2407e-01 (1.4873e-01)	Acc@1  95.31 ( 94.85)	Acc@5 100.00 ( 99.91)
Epoch: [34][330/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.8945e-01 (1.4942e-01)	Acc@1  92.97 ( 94.83)	Acc@5 100.00 ( 99.91)
Epoch: [34][340/391]	Time  0.049 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.7298e-01 (1.4920e-01)	Acc@1  92.97 ( 94.82)	Acc@5 100.00 ( 99.91)
Epoch: [34][350/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.2743e-01 (1.4924e-01)	Acc@1  96.09 ( 94.83)	Acc@5 100.00 ( 99.91)
Epoch: [34][360/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.7761e-01 (1.4965e-01)	Acc@1  93.75 ( 94.81)	Acc@5 100.00 ( 99.92)
Epoch: [34][370/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.0719e-01 (1.4890e-01)	Acc@1  95.31 ( 94.84)	Acc@5 100.00 ( 99.92)
Epoch: [34][380/391]	Time  0.047 ( 0.041)	Data  0.002 ( 0.001)	Loss 1.8609e-01 (1.4861e-01)	Acc@1  93.75 ( 94.85)	Acc@5 100.00 ( 99.92)
Epoch: [34][390/391]	Time  0.027 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.1804e-01 (1.4852e-01)	Acc@1  95.00 ( 94.86)	Acc@5 100.00 ( 99.92)
## e[34] optimizer.zero_grad (sum) time: 0.2625110149383545
## e[34]       loss.backward (sum) time: 3.9582579135894775
## e[34]      optimizer.step (sum) time: 1.8378543853759766
## epoch[34] training(only) time: 16.06461524963379
# Switched to evaluate mode...
Test: [  0/100]	Time  0.215 ( 0.215)	Loss 2.8622e-01 (2.8622e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.017 ( 0.037)	Loss 3.8389e-01 (2.8283e-01)	Acc@1  90.00 ( 90.27)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.024 ( 0.029)	Loss 3.8289e-01 (2.8856e-01)	Acc@1  89.00 ( 90.14)	Acc@5 100.00 ( 99.90)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 3.8161e-01 (3.0957e-01)	Acc@1  90.00 ( 90.26)	Acc@5  98.00 ( 99.74)
Test: [ 40/100]	Time  0.021 ( 0.026)	Loss 3.5401e-01 (3.1778e-01)	Acc@1  88.00 ( 89.85)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 2.9064e-01 (3.1102e-01)	Acc@1  92.00 ( 90.31)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.020 ( 0.024)	Loss 3.3333e-01 (3.1149e-01)	Acc@1  89.00 ( 90.16)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.018 ( 0.023)	Loss 2.9685e-01 (3.0438e-01)	Acc@1  89.00 ( 90.25)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.021 ( 0.023)	Loss 2.3758e-01 (3.0071e-01)	Acc@1  93.00 ( 90.43)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.021 ( 0.023)	Loss 1.8139e-01 (3.0011e-01)	Acc@1  93.00 ( 90.35)	Acc@5 100.00 ( 99.78)
 * Acc@1 90.440 Acc@5 99.770
### epoch[34] execution time: 18.447813034057617
EPOCH 35
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [35][  0/391]	Time  0.218 ( 0.218)	Data  0.178 ( 0.178)	Loss 1.2901e-01 (1.2901e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
Epoch: [35][ 10/391]	Time  0.042 ( 0.057)	Data  0.001 ( 0.017)	Loss 9.1415e-02 (1.2934e-01)	Acc@1  96.88 ( 95.45)	Acc@5 100.00 (100.00)
Epoch: [35][ 20/391]	Time  0.042 ( 0.049)	Data  0.001 ( 0.009)	Loss 8.5917e-02 (1.2481e-01)	Acc@1  96.09 ( 95.65)	Acc@5 100.00 (100.00)
Epoch: [35][ 30/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.007)	Loss 1.8219e-01 (1.3370e-01)	Acc@1  93.75 ( 95.41)	Acc@5 100.00 ( 99.95)
Epoch: [35][ 40/391]	Time  0.042 ( 0.045)	Data  0.001 ( 0.005)	Loss 1.3878e-01 (1.3107e-01)	Acc@1  93.75 ( 95.37)	Acc@5 100.00 ( 99.96)
Epoch: [35][ 50/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.2582e-01 (1.3105e-01)	Acc@1  96.09 ( 95.47)	Acc@5 100.00 ( 99.95)
Epoch: [35][ 60/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.7630e-01 (1.3218e-01)	Acc@1  94.53 ( 95.36)	Acc@5 100.00 ( 99.96)
Epoch: [35][ 70/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 9.6363e-02 (1.3308e-01)	Acc@1  96.88 ( 95.32)	Acc@5 100.00 ( 99.97)
Epoch: [35][ 80/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 7.5445e-02 (1.3506e-01)	Acc@1  98.44 ( 95.28)	Acc@5 100.00 ( 99.95)
Epoch: [35][ 90/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.2050e-01 (1.3611e-01)	Acc@1  96.09 ( 95.33)	Acc@5 100.00 ( 99.96)
Epoch: [35][100/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.2659e-01 (1.3563e-01)	Acc@1  95.31 ( 95.34)	Acc@5 100.00 ( 99.96)
Epoch: [35][110/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.5538e-01 (1.3578e-01)	Acc@1  96.09 ( 95.33)	Acc@5  99.22 ( 99.95)
Epoch: [35][120/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.9213e-01 (1.3549e-01)	Acc@1  94.53 ( 95.39)	Acc@5 100.00 ( 99.95)
Epoch: [35][130/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.7720e-01 (1.3607e-01)	Acc@1  91.41 ( 95.39)	Acc@5 100.00 ( 99.95)
Epoch: [35][140/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.4803e-01 (1.3630e-01)	Acc@1  95.31 ( 95.39)	Acc@5 100.00 ( 99.94)
Epoch: [35][150/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.2435e-01 (1.3710e-01)	Acc@1  95.31 ( 95.38)	Acc@5 100.00 ( 99.95)
Epoch: [35][160/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.1105e-01 (1.3587e-01)	Acc@1  95.31 ( 95.43)	Acc@5 100.00 ( 99.95)
Epoch: [35][170/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.9208e-02 (1.3585e-01)	Acc@1  98.44 ( 95.43)	Acc@5 100.00 ( 99.95)
Epoch: [35][180/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.3248e-01 (1.3609e-01)	Acc@1  93.75 ( 95.39)	Acc@5 100.00 ( 99.95)
Epoch: [35][190/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7241e-01 (1.3753e-01)	Acc@1  92.97 ( 95.35)	Acc@5 100.00 ( 99.94)
Epoch: [35][200/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1867e-01 (1.3818e-01)	Acc@1  96.88 ( 95.33)	Acc@5 100.00 ( 99.95)
Epoch: [35][210/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.1596e-01 (1.3949e-01)	Acc@1  93.75 ( 95.26)	Acc@5 100.00 ( 99.94)
Epoch: [35][220/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.6234e-01 (1.4111e-01)	Acc@1  93.75 ( 95.22)	Acc@5  99.22 ( 99.94)
Epoch: [35][230/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4851e-01 (1.4008e-01)	Acc@1  95.31 ( 95.22)	Acc@5 100.00 ( 99.94)
Epoch: [35][240/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.4569e-02 (1.3976e-01)	Acc@1  97.66 ( 95.25)	Acc@5 100.00 ( 99.94)
Epoch: [35][250/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2557e-01 (1.3971e-01)	Acc@1  95.31 ( 95.23)	Acc@5 100.00 ( 99.94)
Epoch: [35][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5911e-01 (1.3999e-01)	Acc@1  94.53 ( 95.23)	Acc@5 100.00 ( 99.94)
Epoch: [35][270/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1890e-01 (1.3994e-01)	Acc@1  96.88 ( 95.23)	Acc@5 100.00 ( 99.95)
Epoch: [35][280/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.6380e-02 (1.3944e-01)	Acc@1  99.22 ( 95.23)	Acc@5 100.00 ( 99.95)
Epoch: [35][290/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.0003e-02 (1.3898e-01)	Acc@1  99.22 ( 95.25)	Acc@5 100.00 ( 99.95)
Epoch: [35][300/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.5752e-01 (1.3973e-01)	Acc@1  94.53 ( 95.24)	Acc@5 100.00 ( 99.95)
Epoch: [35][310/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.1004e-01 (1.4018e-01)	Acc@1  96.09 ( 95.25)	Acc@5 100.00 ( 99.95)
Epoch: [35][320/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.6435e-01 (1.4060e-01)	Acc@1  95.31 ( 95.23)	Acc@5 100.00 ( 99.95)
Epoch: [35][330/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.5758e-01 (1.4072e-01)	Acc@1  92.97 ( 95.23)	Acc@5 100.00 ( 99.96)
Epoch: [35][340/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.3157e-01 (1.4084e-01)	Acc@1  92.19 ( 95.23)	Acc@5 100.00 ( 99.95)
Epoch: [35][350/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.8546e-01 (1.4047e-01)	Acc@1  92.19 ( 95.23)	Acc@5 100.00 ( 99.95)
Epoch: [35][360/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.3777e-01 (1.4029e-01)	Acc@1  95.31 ( 95.24)	Acc@5 100.00 ( 99.95)
Epoch: [35][370/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.9941e-01 (1.4082e-01)	Acc@1  92.97 ( 95.23)	Acc@5 100.00 ( 99.95)
Epoch: [35][380/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.8377e-01 (1.4085e-01)	Acc@1  92.97 ( 95.23)	Acc@5 100.00 ( 99.95)
Epoch: [35][390/391]	Time  0.029 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.1654e-01 (1.4033e-01)	Acc@1  96.25 ( 95.25)	Acc@5 100.00 ( 99.95)
## e[35] optimizer.zero_grad (sum) time: 0.26200413703918457
## e[35]       loss.backward (sum) time: 4.004690408706665
## e[35]      optimizer.step (sum) time: 1.799950361251831
## epoch[35] training(only) time: 16.09542942047119
# Switched to evaluate mode...
Test: [  0/100]	Time  0.168 ( 0.168)	Loss 2.8641e-01 (2.8641e-01)	Acc@1  93.00 ( 93.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.024 ( 0.033)	Loss 3.8232e-01 (2.8083e-01)	Acc@1  90.00 ( 91.27)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.022 ( 0.027)	Loss 3.8615e-01 (2.8818e-01)	Acc@1  87.00 ( 90.67)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.018 ( 0.024)	Loss 3.7109e-01 (3.0879e-01)	Acc@1  90.00 ( 90.39)	Acc@5  99.00 ( 99.74)
Test: [ 40/100]	Time  0.018 ( 0.023)	Loss 3.6329e-01 (3.1718e-01)	Acc@1  89.00 ( 90.17)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.026 ( 0.023)	Loss 2.7194e-01 (3.1137e-01)	Acc@1  94.00 ( 90.45)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.018 ( 0.022)	Loss 3.2167e-01 (3.1293e-01)	Acc@1  89.00 ( 90.21)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.018 ( 0.022)	Loss 3.6641e-01 (3.0621e-01)	Acc@1  87.00 ( 90.25)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.021 ( 0.021)	Loss 2.5359e-01 (3.0397e-01)	Acc@1  93.00 ( 90.44)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.019 ( 0.021)	Loss 2.0442e-01 (3.0333e-01)	Acc@1  93.00 ( 90.43)	Acc@5 100.00 ( 99.76)
 * Acc@1 90.500 Acc@5 99.760
### epoch[35] execution time: 18.310734272003174
EPOCH 36
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [36][  0/391]	Time  0.257 ( 0.257)	Data  0.216 ( 0.216)	Loss 1.2426e-01 (1.2426e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [36][ 10/391]	Time  0.038 ( 0.060)	Data  0.001 ( 0.020)	Loss 1.2447e-01 (1.4784e-01)	Acc@1  96.09 ( 95.03)	Acc@5 100.00 ( 99.86)
Epoch: [36][ 20/391]	Time  0.041 ( 0.051)	Data  0.001 ( 0.011)	Loss 1.8301e-01 (1.4722e-01)	Acc@1  93.75 ( 94.79)	Acc@5 100.00 ( 99.93)
Epoch: [36][ 30/391]	Time  0.041 ( 0.047)	Data  0.001 ( 0.008)	Loss 1.0401e-01 (1.4132e-01)	Acc@1  96.09 ( 95.09)	Acc@5 100.00 ( 99.92)
Epoch: [36][ 40/391]	Time  0.039 ( 0.045)	Data  0.001 ( 0.006)	Loss 1.0092e-01 (1.3964e-01)	Acc@1  96.88 ( 95.14)	Acc@5 100.00 ( 99.94)
Epoch: [36][ 50/391]	Time  0.044 ( 0.045)	Data  0.001 ( 0.005)	Loss 1.8786e-01 (1.4151e-01)	Acc@1  92.97 ( 94.96)	Acc@5 100.00 ( 99.94)
Epoch: [36][ 60/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.6774e-01 (1.4211e-01)	Acc@1  96.09 ( 94.95)	Acc@5 100.00 ( 99.94)
Epoch: [36][ 70/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.1335e-01 (1.4045e-01)	Acc@1  96.09 ( 95.09)	Acc@5 100.00 ( 99.93)
Epoch: [36][ 80/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.0347e-01 (1.4116e-01)	Acc@1  98.44 ( 95.14)	Acc@5 100.00 ( 99.93)
Epoch: [36][ 90/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.8779e-01 (1.4104e-01)	Acc@1  93.75 ( 95.16)	Acc@5 100.00 ( 99.93)
Epoch: [36][100/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.0233e-01 (1.3968e-01)	Acc@1  95.31 ( 95.24)	Acc@5 100.00 ( 99.94)
Epoch: [36][110/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.6749e-01 (1.4216e-01)	Acc@1  93.75 ( 95.09)	Acc@5 100.00 ( 99.94)
Epoch: [36][120/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 9.0396e-02 (1.4074e-01)	Acc@1  97.66 ( 95.13)	Acc@5 100.00 ( 99.94)
Epoch: [36][130/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.8536e-01 (1.4240e-01)	Acc@1  92.97 ( 95.09)	Acc@5 100.00 ( 99.94)
Epoch: [36][140/391]	Time  0.035 ( 0.042)	Data  0.001 ( 0.002)	Loss 8.5818e-02 (1.4174e-01)	Acc@1  97.66 ( 95.14)	Acc@5 100.00 ( 99.94)
Epoch: [36][150/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4117e-01 (1.4220e-01)	Acc@1  94.53 ( 95.14)	Acc@5 100.00 ( 99.93)
Epoch: [36][160/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6284e-01 (1.4276e-01)	Acc@1  94.53 ( 95.10)	Acc@5 100.00 ( 99.93)
Epoch: [36][170/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.4439e-02 (1.4110e-01)	Acc@1  98.44 ( 95.15)	Acc@5 100.00 ( 99.93)
Epoch: [36][180/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3392e-01 (1.4018e-01)	Acc@1  92.97 ( 95.19)	Acc@5 100.00 ( 99.94)
Epoch: [36][190/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.6799e-02 (1.3986e-01)	Acc@1  95.31 ( 95.17)	Acc@5 100.00 ( 99.94)
Epoch: [36][200/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3381e-01 (1.3997e-01)	Acc@1  94.53 ( 95.15)	Acc@5 100.00 ( 99.94)
Epoch: [36][210/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.2633e-02 (1.3992e-01)	Acc@1  96.88 ( 95.16)	Acc@5 100.00 ( 99.94)
Epoch: [36][220/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1205e-01 (1.4000e-01)	Acc@1  95.31 ( 95.15)	Acc@5  99.22 ( 99.94)
Epoch: [36][230/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5342e-01 (1.3987e-01)	Acc@1  93.75 ( 95.14)	Acc@5 100.00 ( 99.95)
Epoch: [36][240/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4937e-01 (1.4034e-01)	Acc@1  95.31 ( 95.13)	Acc@5 100.00 ( 99.94)
Epoch: [36][250/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1523e-01 (1.3958e-01)	Acc@1  96.09 ( 95.14)	Acc@5 100.00 ( 99.94)
Epoch: [36][260/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7076e-01 (1.4015e-01)	Acc@1  92.97 ( 95.10)	Acc@5 100.00 ( 99.95)
Epoch: [36][270/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1807e-01 (1.4005e-01)	Acc@1  96.09 ( 95.10)	Acc@5 100.00 ( 99.94)
Epoch: [36][280/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4978e-01 (1.4081e-01)	Acc@1  93.75 ( 95.06)	Acc@5 100.00 ( 99.94)
Epoch: [36][290/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1159e-01 (1.4066e-01)	Acc@1  96.88 ( 95.08)	Acc@5 100.00 ( 99.94)
Epoch: [36][300/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3972e-01 (1.3953e-01)	Acc@1  96.09 ( 95.13)	Acc@5 100.00 ( 99.95)
Epoch: [36][310/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0244e-01 (1.4069e-01)	Acc@1  95.31 ( 95.10)	Acc@5 100.00 ( 99.95)
Epoch: [36][320/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7901e-01 (1.4104e-01)	Acc@1  96.88 ( 95.10)	Acc@5 100.00 ( 99.95)
Epoch: [36][330/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4487e-01 (1.4065e-01)	Acc@1  94.53 ( 95.11)	Acc@5 100.00 ( 99.95)
Epoch: [36][340/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.6200e-02 (1.4059e-01)	Acc@1  98.44 ( 95.12)	Acc@5  99.22 ( 99.95)
Epoch: [36][350/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4048e-01 (1.4067e-01)	Acc@1  94.53 ( 95.10)	Acc@5 100.00 ( 99.94)
Epoch: [36][360/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6036e-01 (1.4019e-01)	Acc@1  92.97 ( 95.12)	Acc@5 100.00 ( 99.94)
Epoch: [36][370/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1852e-01 (1.3994e-01)	Acc@1  94.53 ( 95.12)	Acc@5 100.00 ( 99.94)
Epoch: [36][380/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6575e-01 (1.4005e-01)	Acc@1  94.53 ( 95.10)	Acc@5  99.22 ( 99.94)
Epoch: [36][390/391]	Time  0.027 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.3046e-01 (1.3983e-01)	Acc@1  95.00 ( 95.11)	Acc@5 100.00 ( 99.94)
## e[36] optimizer.zero_grad (sum) time: 0.2626786231994629
## e[36]       loss.backward (sum) time: 3.918045997619629
## e[36]      optimizer.step (sum) time: 1.8124322891235352
## epoch[36] training(only) time: 15.965621709823608
# Switched to evaluate mode...
Test: [  0/100]	Time  0.164 ( 0.164)	Loss 2.7496e-01 (2.7496e-01)	Acc@1  93.00 ( 93.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 3.9483e-01 (2.8041e-01)	Acc@1  88.00 ( 90.91)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.018 ( 0.027)	Loss 4.4489e-01 (2.8804e-01)	Acc@1  86.00 ( 90.33)	Acc@5  99.00 ( 99.76)
Test: [ 30/100]	Time  0.018 ( 0.025)	Loss 3.9210e-01 (3.0842e-01)	Acc@1  88.00 ( 90.35)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.019 ( 0.025)	Loss 4.1298e-01 (3.1932e-01)	Acc@1  86.00 ( 89.95)	Acc@5 100.00 ( 99.66)
Test: [ 50/100]	Time  0.021 ( 0.024)	Loss 2.4531e-01 (3.1175e-01)	Acc@1  94.00 ( 90.39)	Acc@5 100.00 ( 99.65)
Test: [ 60/100]	Time  0.018 ( 0.023)	Loss 3.5145e-01 (3.1371e-01)	Acc@1  89.00 ( 90.20)	Acc@5 100.00 ( 99.69)
Test: [ 70/100]	Time  0.020 ( 0.023)	Loss 3.1090e-01 (3.0573e-01)	Acc@1  90.00 ( 90.32)	Acc@5 100.00 ( 99.70)
Test: [ 80/100]	Time  0.024 ( 0.023)	Loss 2.0908e-01 (3.0282e-01)	Acc@1  92.00 ( 90.48)	Acc@5 100.00 ( 99.72)
Test: [ 90/100]	Time  0.016 ( 0.022)	Loss 1.6538e-01 (3.0362e-01)	Acc@1  92.00 ( 90.42)	Acc@5 100.00 ( 99.74)
 * Acc@1 90.500 Acc@5 99.740
### epoch[36] execution time: 18.294048309326172
EPOCH 37
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [37][  0/391]	Time  0.217 ( 0.217)	Data  0.176 ( 0.176)	Loss 7.3837e-02 (7.3837e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [37][ 10/391]	Time  0.039 ( 0.056)	Data  0.001 ( 0.017)	Loss 1.6787e-01 (1.3659e-01)	Acc@1  95.31 ( 94.96)	Acc@5  99.22 ( 99.86)
Epoch: [37][ 20/391]	Time  0.041 ( 0.048)	Data  0.001 ( 0.009)	Loss 9.4353e-02 (1.2461e-01)	Acc@1  98.44 ( 95.61)	Acc@5 100.00 ( 99.81)
Epoch: [37][ 30/391]	Time  0.043 ( 0.046)	Data  0.001 ( 0.007)	Loss 1.1878e-01 (1.3159e-01)	Acc@1  97.66 ( 95.44)	Acc@5  99.22 ( 99.82)
Epoch: [37][ 40/391]	Time  0.040 ( 0.045)	Data  0.001 ( 0.005)	Loss 1.4632e-01 (1.2966e-01)	Acc@1  96.88 ( 95.48)	Acc@5 100.00 ( 99.87)
Epoch: [37][ 50/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.004)	Loss 9.6367e-02 (1.2789e-01)	Acc@1  96.09 ( 95.44)	Acc@5 100.00 ( 99.89)
Epoch: [37][ 60/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.0090e-01 (1.2783e-01)	Acc@1  96.88 ( 95.48)	Acc@5 100.00 ( 99.90)
Epoch: [37][ 70/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.2861e-01 (1.2896e-01)	Acc@1  92.97 ( 95.42)	Acc@5 100.00 ( 99.91)
Epoch: [37][ 80/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.7308e-01 (1.3010e-01)	Acc@1  94.53 ( 95.32)	Acc@5 100.00 ( 99.91)
Epoch: [37][ 90/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.8457e-01 (1.3177e-01)	Acc@1  95.31 ( 95.30)	Acc@5  99.22 ( 99.91)
Epoch: [37][100/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.9399e-01 (1.3023e-01)	Acc@1  90.62 ( 95.35)	Acc@5 100.00 ( 99.91)
Epoch: [37][110/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.4642e-01 (1.2914e-01)	Acc@1  92.19 ( 95.36)	Acc@5 100.00 ( 99.92)
Epoch: [37][120/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.4590e-01 (1.2935e-01)	Acc@1  95.31 ( 95.33)	Acc@5 100.00 ( 99.92)
Epoch: [37][130/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.6852e-01 (1.3095e-01)	Acc@1  92.19 ( 95.28)	Acc@5 100.00 ( 99.92)
Epoch: [37][140/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.1628e-01 (1.3029e-01)	Acc@1  95.31 ( 95.30)	Acc@5 100.00 ( 99.92)
Epoch: [37][150/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.0959e-01 (1.2963e-01)	Acc@1  96.09 ( 95.31)	Acc@5 100.00 ( 99.93)
Epoch: [37][160/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.4439e-01 (1.2931e-01)	Acc@1  96.09 ( 95.35)	Acc@5 100.00 ( 99.93)
Epoch: [37][170/391]	Time  0.046 ( 0.042)	Data  0.001 ( 0.002)	Loss 8.6337e-02 (1.3036e-01)	Acc@1  96.09 ( 95.29)	Acc@5 100.00 ( 99.94)
Epoch: [37][180/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6873e-01 (1.3112e-01)	Acc@1  94.53 ( 95.30)	Acc@5 100.00 ( 99.94)
Epoch: [37][190/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0878e-01 (1.3112e-01)	Acc@1  96.88 ( 95.30)	Acc@5 100.00 ( 99.93)
Epoch: [37][200/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4401e-01 (1.3112e-01)	Acc@1  93.75 ( 95.30)	Acc@5 100.00 ( 99.93)
Epoch: [37][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6091e-01 (1.3096e-01)	Acc@1  96.09 ( 95.32)	Acc@5 100.00 ( 99.93)
Epoch: [37][220/391]	Time  0.035 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.4800e-02 (1.2993e-01)	Acc@1  95.31 ( 95.38)	Acc@5 100.00 ( 99.92)
Epoch: [37][230/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.9502e-01 (1.3041e-01)	Acc@1  91.41 ( 95.37)	Acc@5 100.00 ( 99.92)
Epoch: [37][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7892e-01 (1.3006e-01)	Acc@1  96.09 ( 95.40)	Acc@5 100.00 ( 99.92)
Epoch: [37][250/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2392e-01 (1.3006e-01)	Acc@1  96.09 ( 95.39)	Acc@5 100.00 ( 99.93)
Epoch: [37][260/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0583e-01 (1.3096e-01)	Acc@1  95.31 ( 95.33)	Acc@5 100.00 ( 99.93)
Epoch: [37][270/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3194e-01 (1.3058e-01)	Acc@1  95.31 ( 95.35)	Acc@5 100.00 ( 99.93)
Epoch: [37][280/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5875e-01 (1.3042e-01)	Acc@1  95.31 ( 95.37)	Acc@5 100.00 ( 99.93)
Epoch: [37][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4783e-01 (1.3075e-01)	Acc@1  95.31 ( 95.36)	Acc@5 100.00 ( 99.93)
Epoch: [37][300/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.2069e-02 (1.3051e-01)	Acc@1  96.09 ( 95.37)	Acc@5 100.00 ( 99.93)
Epoch: [37][310/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1676e-01 (1.3081e-01)	Acc@1  96.09 ( 95.36)	Acc@5 100.00 ( 99.93)
Epoch: [37][320/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0804e-01 (1.3174e-01)	Acc@1  95.31 ( 95.33)	Acc@5 100.00 ( 99.93)
Epoch: [37][330/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.0606e-01 (1.3168e-01)	Acc@1  96.88 ( 95.34)	Acc@5 100.00 ( 99.93)
Epoch: [37][340/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.0176e-01 (1.3181e-01)	Acc@1  91.41 ( 95.33)	Acc@5  99.22 ( 99.93)
Epoch: [37][350/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.5238e-01 (1.3241e-01)	Acc@1  94.53 ( 95.32)	Acc@5 100.00 ( 99.93)
Epoch: [37][360/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.3286e-01 (1.3262e-01)	Acc@1  96.09 ( 95.31)	Acc@5 100.00 ( 99.93)
Epoch: [37][370/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.7902e-02 (1.3209e-01)	Acc@1  98.44 ( 95.33)	Acc@5 100.00 ( 99.93)
Epoch: [37][380/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.2839e-01 (1.3220e-01)	Acc@1  96.09 ( 95.31)	Acc@5 100.00 ( 99.93)
Epoch: [37][390/391]	Time  0.028 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.2695e-02 (1.3228e-01)	Acc@1 100.00 ( 95.32)	Acc@5 100.00 ( 99.93)
## e[37] optimizer.zero_grad (sum) time: 0.2616081237792969
## e[37]       loss.backward (sum) time: 3.9842307567596436
## e[37]      optimizer.step (sum) time: 1.7936370372772217
## epoch[37] training(only) time: 16.04081892967224
# Switched to evaluate mode...
Test: [  0/100]	Time  0.212 ( 0.212)	Loss 2.9021e-01 (2.9021e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.038)	Loss 4.3969e-01 (2.7677e-01)	Acc@1  89.00 ( 90.27)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.016 ( 0.029)	Loss 4.2727e-01 (2.8887e-01)	Acc@1  87.00 ( 90.14)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.018 ( 0.027)	Loss 4.3065e-01 (3.1213e-01)	Acc@1  89.00 ( 90.26)	Acc@5  99.00 ( 99.74)
Test: [ 40/100]	Time  0.018 ( 0.025)	Loss 3.5977e-01 (3.1877e-01)	Acc@1  88.00 ( 90.00)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.018 ( 0.024)	Loss 2.7933e-01 (3.1145e-01)	Acc@1  94.00 ( 90.41)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.016 ( 0.023)	Loss 3.3965e-01 (3.1472e-01)	Acc@1  91.00 ( 90.18)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.022 ( 0.023)	Loss 3.7287e-01 (3.0720e-01)	Acc@1  89.00 ( 90.31)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.025 ( 0.023)	Loss 2.1754e-01 (3.0434e-01)	Acc@1  93.00 ( 90.48)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.021 ( 0.023)	Loss 2.0248e-01 (3.0240e-01)	Acc@1  94.00 ( 90.49)	Acc@5 100.00 ( 99.78)
 * Acc@1 90.500 Acc@5 99.770
### epoch[37] execution time: 18.383596897125244
EPOCH 38
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [38][  0/391]	Time  0.228 ( 0.228)	Data  0.186 ( 0.186)	Loss 1.8118e-01 (1.8118e-01)	Acc@1  92.97 ( 92.97)	Acc@5 100.00 (100.00)
Epoch: [38][ 10/391]	Time  0.038 ( 0.057)	Data  0.001 ( 0.018)	Loss 1.4527e-01 (1.4353e-01)	Acc@1  93.75 ( 94.67)	Acc@5 100.00 ( 99.86)
Epoch: [38][ 20/391]	Time  0.039 ( 0.049)	Data  0.001 ( 0.010)	Loss 1.8397e-01 (1.4501e-01)	Acc@1  96.09 ( 94.75)	Acc@5 100.00 ( 99.93)
Epoch: [38][ 30/391]	Time  0.038 ( 0.046)	Data  0.001 ( 0.007)	Loss 6.9215e-02 (1.3738e-01)	Acc@1  98.44 ( 95.01)	Acc@5 100.00 ( 99.92)
Epoch: [38][ 40/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.005)	Loss 1.1066e-01 (1.3652e-01)	Acc@1  96.88 ( 95.18)	Acc@5 100.00 ( 99.94)
Epoch: [38][ 50/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.005)	Loss 1.5986e-01 (1.3573e-01)	Acc@1  95.31 ( 95.16)	Acc@5 100.00 ( 99.94)
Epoch: [38][ 60/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.3097e-01 (1.3488e-01)	Acc@1  94.53 ( 95.20)	Acc@5 100.00 ( 99.94)
Epoch: [38][ 70/391]	Time  0.043 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.0068e-01 (1.3296e-01)	Acc@1  95.31 ( 95.33)	Acc@5 100.00 ( 99.93)
Epoch: [38][ 80/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.5900e-01 (1.3476e-01)	Acc@1  96.09 ( 95.33)	Acc@5 100.00 ( 99.94)
Epoch: [38][ 90/391]	Time  0.035 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.3174e-01 (1.3389e-01)	Acc@1  96.09 ( 95.36)	Acc@5 100.00 ( 99.95)
Epoch: [38][100/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.5436e-01 (1.3304e-01)	Acc@1  94.53 ( 95.37)	Acc@5 100.00 ( 99.95)
Epoch: [38][110/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.003)	Loss 1.2399e-01 (1.3213e-01)	Acc@1  96.09 ( 95.45)	Acc@5 100.00 ( 99.94)
Epoch: [38][120/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.9127e-01 (1.3206e-01)	Acc@1  94.53 ( 95.45)	Acc@5 100.00 ( 99.94)
Epoch: [38][130/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.7713e-02 (1.3082e-01)	Acc@1  96.88 ( 95.45)	Acc@5 100.00 ( 99.95)
Epoch: [38][140/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.4812e-02 (1.3044e-01)	Acc@1  96.09 ( 95.46)	Acc@5 100.00 ( 99.95)
Epoch: [38][150/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.9236e-02 (1.2936e-01)	Acc@1  96.88 ( 95.48)	Acc@5 100.00 ( 99.95)
Epoch: [38][160/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.4645e-02 (1.2840e-01)	Acc@1  97.66 ( 95.53)	Acc@5 100.00 ( 99.95)
Epoch: [38][170/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2915e-01 (1.2875e-01)	Acc@1  95.31 ( 95.53)	Acc@5 100.00 ( 99.95)
Epoch: [38][180/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3398e-01 (1.2858e-01)	Acc@1  96.09 ( 95.54)	Acc@5 100.00 ( 99.95)
Epoch: [38][190/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1711e-01 (1.2858e-01)	Acc@1  94.53 ( 95.55)	Acc@5 100.00 ( 99.95)
Epoch: [38][200/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6544e-01 (1.2946e-01)	Acc@1  94.53 ( 95.51)	Acc@5 100.00 ( 99.95)
Epoch: [38][210/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0935e-01 (1.2905e-01)	Acc@1  97.66 ( 95.53)	Acc@5 100.00 ( 99.96)
Epoch: [38][220/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3183e-01 (1.2964e-01)	Acc@1  95.31 ( 95.50)	Acc@5  99.22 ( 99.95)
Epoch: [38][230/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.4818e-02 (1.2975e-01)	Acc@1  94.53 ( 95.49)	Acc@5 100.00 ( 99.95)
Epoch: [38][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6185e-01 (1.3000e-01)	Acc@1  98.44 ( 95.47)	Acc@5 100.00 ( 99.95)
Epoch: [38][250/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3515e-01 (1.2911e-01)	Acc@1  94.53 ( 95.52)	Acc@5 100.00 ( 99.95)
Epoch: [38][260/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6127e-01 (1.2929e-01)	Acc@1  95.31 ( 95.54)	Acc@5 100.00 ( 99.95)
Epoch: [38][270/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2028e-01 (1.2974e-01)	Acc@1  94.53 ( 95.51)	Acc@5 100.00 ( 99.95)
Epoch: [38][280/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0206e-01 (1.2921e-01)	Acc@1  96.88 ( 95.53)	Acc@5 100.00 ( 99.96)
Epoch: [38][290/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2044e-01 (1.2919e-01)	Acc@1  92.97 ( 95.51)	Acc@5 100.00 ( 99.96)
Epoch: [38][300/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.3508e-01 (1.3027e-01)	Acc@1  91.41 ( 95.47)	Acc@5 100.00 ( 99.95)
Epoch: [38][310/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.9328e-01 (1.3074e-01)	Acc@1  94.53 ( 95.46)	Acc@5 100.00 ( 99.95)
Epoch: [38][320/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2513e-01 (1.3119e-01)	Acc@1  95.31 ( 95.43)	Acc@5 100.00 ( 99.95)
Epoch: [38][330/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.1264e-02 (1.3132e-01)	Acc@1  97.66 ( 95.41)	Acc@5 100.00 ( 99.95)
Epoch: [38][340/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.9058e-01 (1.3119e-01)	Acc@1  94.53 ( 95.41)	Acc@5 100.00 ( 99.95)
Epoch: [38][350/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.4998e-01 (1.3061e-01)	Acc@1  93.75 ( 95.43)	Acc@5  99.22 ( 99.95)
Epoch: [38][360/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.4337e-01 (1.3085e-01)	Acc@1  95.31 ( 95.43)	Acc@5 100.00 ( 99.95)
Epoch: [38][370/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.8395e-02 (1.3033e-01)	Acc@1  96.88 ( 95.46)	Acc@5 100.00 ( 99.95)
Epoch: [38][380/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.3317e-02 (1.3053e-01)	Acc@1  98.44 ( 95.46)	Acc@5 100.00 ( 99.95)
Epoch: [38][390/391]	Time  0.025 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.0829e-01 (1.3040e-01)	Acc@1  93.75 ( 95.48)	Acc@5 100.00 ( 99.94)
## e[38] optimizer.zero_grad (sum) time: 0.26151466369628906
## e[38]       loss.backward (sum) time: 3.9545884132385254
## e[38]      optimizer.step (sum) time: 1.8319470882415771
## epoch[38] training(only) time: 15.98302412033081
# Switched to evaluate mode...
Test: [  0/100]	Time  0.177 ( 0.177)	Loss 2.9471e-01 (2.9471e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.018 ( 0.034)	Loss 4.4106e-01 (2.7654e-01)	Acc@1  88.00 ( 91.27)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.021 ( 0.028)	Loss 3.9871e-01 (2.9009e-01)	Acc@1  87.00 ( 90.67)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.020 ( 0.026)	Loss 4.3953e-01 (3.1509e-01)	Acc@1  89.00 ( 90.48)	Acc@5  99.00 ( 99.68)
Test: [ 40/100]	Time  0.017 ( 0.025)	Loss 3.7000e-01 (3.2313e-01)	Acc@1  90.00 ( 90.37)	Acc@5 100.00 ( 99.66)
Test: [ 50/100]	Time  0.019 ( 0.025)	Loss 2.8925e-01 (3.1670e-01)	Acc@1  93.00 ( 90.71)	Acc@5 100.00 ( 99.69)
Test: [ 60/100]	Time  0.024 ( 0.024)	Loss 3.8624e-01 (3.1901e-01)	Acc@1  88.00 ( 90.46)	Acc@5  99.00 ( 99.72)
Test: [ 70/100]	Time  0.024 ( 0.023)	Loss 3.5019e-01 (3.0984e-01)	Acc@1  89.00 ( 90.46)	Acc@5 100.00 ( 99.72)
Test: [ 80/100]	Time  0.022 ( 0.023)	Loss 2.2228e-01 (3.0765e-01)	Acc@1  94.00 ( 90.51)	Acc@5 100.00 ( 99.73)
Test: [ 90/100]	Time  0.024 ( 0.023)	Loss 2.3683e-01 (3.0637e-01)	Acc@1  93.00 ( 90.52)	Acc@5 100.00 ( 99.74)
 * Acc@1 90.610 Acc@5 99.730
### epoch[38] execution time: 18.34235119819641
EPOCH 39
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [39][  0/391]	Time  0.217 ( 0.217)	Data  0.176 ( 0.176)	Loss 1.3369e-01 (1.3369e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [39][ 10/391]	Time  0.038 ( 0.057)	Data  0.001 ( 0.017)	Loss 1.7050e-01 (1.5166e-01)	Acc@1  92.19 ( 94.46)	Acc@5 100.00 ( 99.86)
Epoch: [39][ 20/391]	Time  0.041 ( 0.049)	Data  0.001 ( 0.009)	Loss 1.2956e-01 (1.3380e-01)	Acc@1  93.75 ( 95.13)	Acc@5 100.00 ( 99.93)
Epoch: [39][ 30/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.007)	Loss 1.6091e-01 (1.3322e-01)	Acc@1  95.31 ( 95.29)	Acc@5 100.00 ( 99.95)
Epoch: [39][ 40/391]	Time  0.042 ( 0.045)	Data  0.001 ( 0.005)	Loss 8.3077e-02 (1.3305e-01)	Acc@1  97.66 ( 95.41)	Acc@5 100.00 ( 99.96)
Epoch: [39][ 50/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.0624e-01 (1.3061e-01)	Acc@1  96.09 ( 95.53)	Acc@5 100.00 ( 99.95)
Epoch: [39][ 60/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.5922e-01 (1.3108e-01)	Acc@1  92.19 ( 95.49)	Acc@5 100.00 ( 99.96)
Epoch: [39][ 70/391]	Time  0.037 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.8194e-01 (1.3061e-01)	Acc@1  92.97 ( 95.54)	Acc@5 100.00 ( 99.96)
Epoch: [39][ 80/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.1505e-01 (1.3092e-01)	Acc@1  96.88 ( 95.53)	Acc@5 100.00 ( 99.96)
Epoch: [39][ 90/391]	Time  0.035 ( 0.042)	Data  0.001 ( 0.003)	Loss 9.2356e-02 (1.2780e-01)	Acc@1  95.31 ( 95.56)	Acc@5 100.00 ( 99.97)
Epoch: [39][100/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.003)	Loss 8.8278e-02 (1.2813e-01)	Acc@1  98.44 ( 95.54)	Acc@5 100.00 ( 99.96)
Epoch: [39][110/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 1.5678e-01 (1.2805e-01)	Acc@1  92.97 ( 95.52)	Acc@5 100.00 ( 99.96)
Epoch: [39][120/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5718e-01 (1.2806e-01)	Acc@1  93.75 ( 95.55)	Acc@5 100.00 ( 99.96)
Epoch: [39][130/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2923e-01 (1.2853e-01)	Acc@1  95.31 ( 95.57)	Acc@5 100.00 ( 99.96)
Epoch: [39][140/391]	Time  0.046 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0671e-01 (1.2782e-01)	Acc@1  96.09 ( 95.59)	Acc@5 100.00 ( 99.96)
Epoch: [39][150/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6260e-01 (1.2848e-01)	Acc@1  95.31 ( 95.59)	Acc@5 100.00 ( 99.96)
Epoch: [39][160/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0983e-01 (1.2760e-01)	Acc@1  96.88 ( 95.62)	Acc@5 100.00 ( 99.96)
Epoch: [39][170/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.0526e-02 (1.2831e-01)	Acc@1  98.44 ( 95.59)	Acc@5 100.00 ( 99.96)
Epoch: [39][180/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4761e-01 (1.2734e-01)	Acc@1  94.53 ( 95.62)	Acc@5 100.00 ( 99.96)
Epoch: [39][190/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.5982e-02 (1.2690e-01)	Acc@1  97.66 ( 95.63)	Acc@5 100.00 ( 99.96)
Epoch: [39][200/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.9091e-02 (1.2643e-01)	Acc@1  97.66 ( 95.64)	Acc@5 100.00 ( 99.97)
Epoch: [39][210/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1988e-01 (1.2584e-01)	Acc@1  97.66 ( 95.67)	Acc@5 100.00 ( 99.97)
Epoch: [39][220/391]	Time  0.054 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3890e-01 (1.2654e-01)	Acc@1  95.31 ( 95.62)	Acc@5 100.00 ( 99.96)
Epoch: [39][230/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1996e-01 (1.2701e-01)	Acc@1  96.09 ( 95.62)	Acc@5 100.00 ( 99.96)
Epoch: [39][240/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7447e-01 (1.2777e-01)	Acc@1  94.53 ( 95.61)	Acc@5 100.00 ( 99.96)
Epoch: [39][250/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.6670e-02 (1.2774e-01)	Acc@1  98.44 ( 95.63)	Acc@5 100.00 ( 99.96)
Epoch: [39][260/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1825e-01 (1.2803e-01)	Acc@1  96.09 ( 95.61)	Acc@5  99.22 ( 99.96)
Epoch: [39][270/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.8071e-02 (1.2728e-01)	Acc@1  96.09 ( 95.64)	Acc@5 100.00 ( 99.95)
Epoch: [39][280/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.4082e-02 (1.2642e-01)	Acc@1  99.22 ( 95.65)	Acc@5 100.00 ( 99.96)
Epoch: [39][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3274e-01 (1.2594e-01)	Acc@1  96.09 ( 95.67)	Acc@5 100.00 ( 99.96)
Epoch: [39][300/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6067e-01 (1.2524e-01)	Acc@1  96.09 ( 95.70)	Acc@5 100.00 ( 99.96)
Epoch: [39][310/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.5705e-01 (1.2569e-01)	Acc@1  96.09 ( 95.69)	Acc@5 100.00 ( 99.96)
Epoch: [39][320/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.4249e-01 (1.2510e-01)	Acc@1  96.09 ( 95.71)	Acc@5 100.00 ( 99.96)
Epoch: [39][330/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 9.1123e-02 (1.2486e-01)	Acc@1  96.88 ( 95.73)	Acc@5 100.00 ( 99.96)
Epoch: [39][340/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.4566e-01 (1.2496e-01)	Acc@1  95.31 ( 95.73)	Acc@5  99.22 ( 99.96)
Epoch: [39][350/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.3633e-01 (1.2512e-01)	Acc@1  95.31 ( 95.71)	Acc@5 100.00 ( 99.96)
Epoch: [39][360/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.0815e-01 (1.2507e-01)	Acc@1  96.88 ( 95.70)	Acc@5 100.00 ( 99.96)
Epoch: [39][370/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.6470e-01 (1.2571e-01)	Acc@1  95.31 ( 95.69)	Acc@5 100.00 ( 99.96)
Epoch: [39][380/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.1815e-01 (1.2573e-01)	Acc@1  96.09 ( 95.69)	Acc@5 100.00 ( 99.95)
Epoch: [39][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.1619e-01 (1.2623e-01)	Acc@1  90.00 ( 95.68)	Acc@5  98.75 ( 99.95)
## e[39] optimizer.zero_grad (sum) time: 0.2622041702270508
## e[39]       loss.backward (sum) time: 3.975141763687134
## e[39]      optimizer.step (sum) time: 1.8215200901031494
## epoch[39] training(only) time: 15.99333667755127
# Switched to evaluate mode...
Test: [  0/100]	Time  0.194 ( 0.194)	Loss 2.7856e-01 (2.7856e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.020 ( 0.037)	Loss 4.3190e-01 (2.9100e-01)	Acc@1  87.00 ( 90.64)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.026 ( 0.028)	Loss 4.2441e-01 (3.0144e-01)	Acc@1  87.00 ( 90.29)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.018 ( 0.026)	Loss 4.3124e-01 (3.2246e-01)	Acc@1  89.00 ( 90.45)	Acc@5  99.00 ( 99.65)
Test: [ 40/100]	Time  0.022 ( 0.024)	Loss 4.0281e-01 (3.2926e-01)	Acc@1  89.00 ( 89.93)	Acc@5 100.00 ( 99.66)
Test: [ 50/100]	Time  0.023 ( 0.024)	Loss 2.9183e-01 (3.2041e-01)	Acc@1  94.00 ( 90.31)	Acc@5 100.00 ( 99.65)
Test: [ 60/100]	Time  0.021 ( 0.023)	Loss 3.6911e-01 (3.2366e-01)	Acc@1  87.00 ( 89.98)	Acc@5 100.00 ( 99.69)
Test: [ 70/100]	Time  0.019 ( 0.023)	Loss 3.8914e-01 (3.1506e-01)	Acc@1  88.00 ( 90.17)	Acc@5 100.00 ( 99.72)
Test: [ 80/100]	Time  0.018 ( 0.023)	Loss 2.4415e-01 (3.1284e-01)	Acc@1  92.00 ( 90.30)	Acc@5 100.00 ( 99.72)
Test: [ 90/100]	Time  0.016 ( 0.022)	Loss 2.1409e-01 (3.1113e-01)	Acc@1  94.00 ( 90.43)	Acc@5 100.00 ( 99.74)
 * Acc@1 90.480 Acc@5 99.720
### epoch[39] execution time: 18.340450286865234
EPOCH 40
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [40][  0/391]	Time  0.210 ( 0.210)	Data  0.168 ( 0.168)	Loss 1.5026e-01 (1.5026e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
Epoch: [40][ 10/391]	Time  0.038 ( 0.056)	Data  0.001 ( 0.016)	Loss 1.2754e-01 (1.0931e-01)	Acc@1  96.09 ( 96.02)	Acc@5  99.22 ( 99.93)
Epoch: [40][ 20/391]	Time  0.039 ( 0.049)	Data  0.001 ( 0.009)	Loss 1.2490e-01 (1.1947e-01)	Acc@1  94.53 ( 95.39)	Acc@5 100.00 ( 99.89)
Epoch: [40][ 30/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.006)	Loss 8.8998e-02 (1.2601e-01)	Acc@1  96.88 ( 95.31)	Acc@5 100.00 ( 99.92)
Epoch: [40][ 40/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.005)	Loss 1.4036e-01 (1.2506e-01)	Acc@1  96.09 ( 95.48)	Acc@5 100.00 ( 99.94)
Epoch: [40][ 50/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.0350e-01 (1.2883e-01)	Acc@1  95.31 ( 95.25)	Acc@5 100.00 ( 99.92)
Epoch: [40][ 60/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 9.2636e-02 (1.2920e-01)	Acc@1  96.88 ( 95.33)	Acc@5 100.00 ( 99.91)
Epoch: [40][ 70/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 9.3868e-02 (1.2731e-01)	Acc@1  96.09 ( 95.40)	Acc@5 100.00 ( 99.91)
Epoch: [40][ 80/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 8.4972e-02 (1.2337e-01)	Acc@1  96.09 ( 95.56)	Acc@5 100.00 ( 99.91)
Epoch: [40][ 90/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.1899e-01 (1.2348e-01)	Acc@1  94.53 ( 95.52)	Acc@5 100.00 ( 99.92)
Epoch: [40][100/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.1345e-02 (1.2391e-01)	Acc@1  96.09 ( 95.51)	Acc@5 100.00 ( 99.92)
Epoch: [40][110/391]	Time  0.048 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.0959e-01 (1.2356e-01)	Acc@1  94.53 ( 95.51)	Acc@5 100.00 ( 99.93)
Epoch: [40][120/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.6863e-01 (1.2439e-01)	Acc@1  94.53 ( 95.50)	Acc@5 100.00 ( 99.94)
Epoch: [40][130/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4672e-01 (1.2487e-01)	Acc@1  94.53 ( 95.50)	Acc@5  99.22 ( 99.93)
Epoch: [40][140/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2837e-01 (1.2498e-01)	Acc@1  94.53 ( 95.50)	Acc@5 100.00 ( 99.92)
Epoch: [40][150/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4397e-01 (1.2636e-01)	Acc@1  94.53 ( 95.43)	Acc@5 100.00 ( 99.92)
Epoch: [40][160/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3130e-01 (1.2601e-01)	Acc@1  96.88 ( 95.47)	Acc@5 100.00 ( 99.92)
Epoch: [40][170/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.0728e-02 (1.2606e-01)	Acc@1  97.66 ( 95.49)	Acc@5 100.00 ( 99.92)
Epoch: [40][180/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.5005e-02 (1.2546e-01)	Acc@1  96.88 ( 95.55)	Acc@5 100.00 ( 99.92)
Epoch: [40][190/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.9455e-01 (1.2481e-01)	Acc@1  95.31 ( 95.57)	Acc@5  99.22 ( 99.92)
Epoch: [40][200/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.7528e-02 (1.2408e-01)	Acc@1  98.44 ( 95.62)	Acc@5 100.00 ( 99.93)
Epoch: [40][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.4184e-02 (1.2375e-01)	Acc@1  96.09 ( 95.65)	Acc@5 100.00 ( 99.93)
Epoch: [40][220/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.6227e-02 (1.2387e-01)	Acc@1  99.22 ( 95.67)	Acc@5 100.00 ( 99.93)
Epoch: [40][230/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.7431e-02 (1.2345e-01)	Acc@1  95.31 ( 95.68)	Acc@5 100.00 ( 99.93)
Epoch: [40][240/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.9408e-01 (1.2406e-01)	Acc@1  92.97 ( 95.68)	Acc@5 100.00 ( 99.93)
Epoch: [40][250/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.9612e-02 (1.2385e-01)	Acc@1  99.22 ( 95.68)	Acc@5 100.00 ( 99.93)
Epoch: [40][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.0819e-01 (1.2467e-01)	Acc@1  92.19 ( 95.65)	Acc@5  99.22 ( 99.93)
Epoch: [40][270/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.8903e-01 (1.2468e-01)	Acc@1  92.97 ( 95.66)	Acc@5 100.00 ( 99.93)
Epoch: [40][280/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.8257e-01 (1.2519e-01)	Acc@1  89.84 ( 95.65)	Acc@5 100.00 ( 99.93)
Epoch: [40][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2911e-01 (1.2488e-01)	Acc@1  97.66 ( 95.66)	Acc@5 100.00 ( 99.93)
Epoch: [40][300/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.6605e-02 (1.2450e-01)	Acc@1  98.44 ( 95.67)	Acc@5 100.00 ( 99.94)
Epoch: [40][310/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4151e-01 (1.2506e-01)	Acc@1  96.09 ( 95.65)	Acc@5 100.00 ( 99.93)
Epoch: [40][320/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0223e-01 (1.2514e-01)	Acc@1  96.88 ( 95.63)	Acc@5 100.00 ( 99.94)
Epoch: [40][330/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.3627e-01 (1.2561e-01)	Acc@1  94.53 ( 95.60)	Acc@5 100.00 ( 99.94)
Epoch: [40][340/391]	Time  0.049 ( 0.041)	Data  0.001 ( 0.001)	Loss 8.5197e-02 (1.2548e-01)	Acc@1  97.66 ( 95.60)	Acc@5 100.00 ( 99.94)
Epoch: [40][350/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.6079e-01 (1.2479e-01)	Acc@1  95.31 ( 95.63)	Acc@5 100.00 ( 99.94)
Epoch: [40][360/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.0827e-01 (1.2504e-01)	Acc@1  94.53 ( 95.61)	Acc@5 100.00 ( 99.94)
Epoch: [40][370/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.1796e-01 (1.2573e-01)	Acc@1  95.31 ( 95.60)	Acc@5 100.00 ( 99.94)
Epoch: [40][380/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 8.9563e-02 (1.2537e-01)	Acc@1  96.09 ( 95.60)	Acc@5 100.00 ( 99.94)
Epoch: [40][390/391]	Time  0.027 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.2174e-01 (1.2558e-01)	Acc@1  97.50 ( 95.61)	Acc@5 100.00 ( 99.94)
## e[40] optimizer.zero_grad (sum) time: 0.26262640953063965
## e[40]       loss.backward (sum) time: 3.938324213027954
## e[40]      optimizer.step (sum) time: 1.8068957328796387
## epoch[40] training(only) time: 15.938139915466309
# Switched to evaluate mode...
Test: [  0/100]	Time  0.165 ( 0.165)	Loss 3.2736e-01 (3.2736e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.021 ( 0.033)	Loss 4.2007e-01 (2.8951e-01)	Acc@1  89.00 ( 91.45)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.018 ( 0.026)	Loss 4.2217e-01 (3.0000e-01)	Acc@1  89.00 ( 90.90)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.022 ( 0.024)	Loss 4.3035e-01 (3.2374e-01)	Acc@1  88.00 ( 90.58)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.017 ( 0.024)	Loss 3.6066e-01 (3.2908e-01)	Acc@1  90.00 ( 90.46)	Acc@5  99.00 ( 99.71)
Test: [ 50/100]	Time  0.018 ( 0.023)	Loss 2.9300e-01 (3.2167e-01)	Acc@1  94.00 ( 90.82)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.023 ( 0.022)	Loss 3.2609e-01 (3.2174e-01)	Acc@1  89.00 ( 90.51)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.020 ( 0.022)	Loss 3.6722e-01 (3.1320e-01)	Acc@1  89.00 ( 90.70)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.017 ( 0.022)	Loss 2.3362e-01 (3.1055e-01)	Acc@1  93.00 ( 90.79)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.018 ( 0.022)	Loss 2.1897e-01 (3.0979e-01)	Acc@1  92.00 ( 90.63)	Acc@5 100.00 ( 99.77)
 * Acc@1 90.680 Acc@5 99.770
### epoch[40] execution time: 18.219561100006104
EPOCH 41
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [41][  0/391]	Time  0.249 ( 0.249)	Data  0.209 ( 0.209)	Loss 1.1135e-01 (1.1135e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [41][ 10/391]	Time  0.038 ( 0.060)	Data  0.001 ( 0.020)	Loss 1.2656e-01 (1.2086e-01)	Acc@1  94.53 ( 95.67)	Acc@5 100.00 (100.00)
Epoch: [41][ 20/391]	Time  0.041 ( 0.050)	Data  0.001 ( 0.011)	Loss 1.3130e-01 (1.1765e-01)	Acc@1  96.09 ( 95.65)	Acc@5 100.00 ( 99.96)
Epoch: [41][ 30/391]	Time  0.040 ( 0.047)	Data  0.001 ( 0.008)	Loss 1.1331e-01 (1.2002e-01)	Acc@1  95.31 ( 95.56)	Acc@5 100.00 ( 99.97)
Epoch: [41][ 40/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.006)	Loss 1.0374e-01 (1.1753e-01)	Acc@1  96.88 ( 95.71)	Acc@5 100.00 ( 99.96)
Epoch: [41][ 50/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.005)	Loss 1.2369e-01 (1.1873e-01)	Acc@1  96.88 ( 95.62)	Acc@5  99.22 ( 99.94)
Epoch: [41][ 60/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.004)	Loss 9.3140e-02 (1.1474e-01)	Acc@1  96.88 ( 95.85)	Acc@5 100.00 ( 99.95)
Epoch: [41][ 70/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.004)	Loss 6.8870e-02 (1.1305e-01)	Acc@1  98.44 ( 95.92)	Acc@5 100.00 ( 99.96)
Epoch: [41][ 80/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 7.1788e-02 (1.1244e-01)	Acc@1  97.66 ( 95.99)	Acc@5 100.00 ( 99.96)
Epoch: [41][ 90/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.1829e-01 (1.1423e-01)	Acc@1  96.09 ( 95.95)	Acc@5 100.00 ( 99.97)
Epoch: [41][100/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.1931e-01 (1.1345e-01)	Acc@1  95.31 ( 95.99)	Acc@5 100.00 ( 99.97)
Epoch: [41][110/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.7266e-01 (1.1358e-01)	Acc@1  95.31 ( 96.00)	Acc@5 100.00 ( 99.96)
Epoch: [41][120/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.3682e-01 (1.1582e-01)	Acc@1  94.53 ( 95.91)	Acc@5 100.00 ( 99.97)
Epoch: [41][130/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.0634e-01 (1.1440e-01)	Acc@1  96.88 ( 96.00)	Acc@5 100.00 ( 99.96)
Epoch: [41][140/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.1697e-01 (1.1401e-01)	Acc@1  95.31 ( 95.98)	Acc@5 100.00 ( 99.97)
Epoch: [41][150/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.4452e-01 (1.1545e-01)	Acc@1  93.75 ( 95.95)	Acc@5 100.00 ( 99.96)
Epoch: [41][160/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.2492e-01 (1.1513e-01)	Acc@1  94.53 ( 95.97)	Acc@5 100.00 ( 99.97)
Epoch: [41][170/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.6423e-02 (1.1463e-01)	Acc@1  95.31 ( 95.98)	Acc@5 100.00 ( 99.97)
Epoch: [41][180/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.7488e-02 (1.1395e-01)	Acc@1  96.09 ( 96.00)	Acc@5 100.00 ( 99.97)
Epoch: [41][190/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.1635e-02 (1.1378e-01)	Acc@1  98.44 ( 96.02)	Acc@5 100.00 ( 99.97)
Epoch: [41][200/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0114e-01 (1.1367e-01)	Acc@1  97.66 ( 96.03)	Acc@5 100.00 ( 99.97)
Epoch: [41][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6723e-01 (1.1430e-01)	Acc@1  94.53 ( 96.01)	Acc@5 100.00 ( 99.97)
Epoch: [41][220/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4969e-01 (1.1454e-01)	Acc@1  94.53 ( 96.01)	Acc@5 100.00 ( 99.97)
Epoch: [41][230/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.8584e-01 (1.1527e-01)	Acc@1  94.53 ( 96.01)	Acc@5 100.00 ( 99.97)
Epoch: [41][240/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1058e-01 (1.1519e-01)	Acc@1  96.09 ( 96.00)	Acc@5 100.00 ( 99.97)
Epoch: [41][250/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4250e-01 (1.1550e-01)	Acc@1  93.75 ( 96.02)	Acc@5 100.00 ( 99.97)
Epoch: [41][260/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6199e-01 (1.1596e-01)	Acc@1  95.31 ( 96.02)	Acc@5  99.22 ( 99.97)
Epoch: [41][270/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.9464e-02 (1.1588e-01)	Acc@1  98.44 ( 96.00)	Acc@5 100.00 ( 99.97)
Epoch: [41][280/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2733e-01 (1.1616e-01)	Acc@1  96.88 ( 95.99)	Acc@5 100.00 ( 99.97)
Epoch: [41][290/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2954e-01 (1.1699e-01)	Acc@1  91.41 ( 95.96)	Acc@5 100.00 ( 99.96)
Epoch: [41][300/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1853e-01 (1.1745e-01)	Acc@1  93.75 ( 95.93)	Acc@5 100.00 ( 99.96)
Epoch: [41][310/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.1723e-01 (1.1796e-01)	Acc@1  92.97 ( 95.92)	Acc@5 100.00 ( 99.96)
Epoch: [41][320/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0627e-01 (1.1797e-01)	Acc@1  96.09 ( 95.93)	Acc@5 100.00 ( 99.96)
Epoch: [41][330/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2318e-01 (1.1758e-01)	Acc@1  95.31 ( 95.94)	Acc@5 100.00 ( 99.96)
Epoch: [41][340/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.3540e-02 (1.1745e-01)	Acc@1  96.88 ( 95.94)	Acc@5 100.00 ( 99.96)
Epoch: [41][350/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.6224e-01 (1.1858e-01)	Acc@1  89.84 ( 95.90)	Acc@5 100.00 ( 99.96)
Epoch: [41][360/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.5913e-01 (1.1927e-01)	Acc@1  91.41 ( 95.90)	Acc@5  99.22 ( 99.96)
Epoch: [41][370/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 9.9077e-02 (1.1913e-01)	Acc@1  98.44 ( 95.91)	Acc@5 100.00 ( 99.96)
Epoch: [41][380/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 9.2455e-02 (1.1944e-01)	Acc@1  95.31 ( 95.89)	Acc@5 100.00 ( 99.96)
Epoch: [41][390/391]	Time  0.028 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.2905e-01 (1.1967e-01)	Acc@1  95.00 ( 95.89)	Acc@5 100.00 ( 99.96)
## e[41] optimizer.zero_grad (sum) time: 0.2627129554748535
## e[41]       loss.backward (sum) time: 4.0158021450042725
## e[41]      optimizer.step (sum) time: 1.79301118850708
## epoch[41] training(only) time: 16.10272526741028
# Switched to evaluate mode...
Test: [  0/100]	Time  0.172 ( 0.172)	Loss 2.9934e-01 (2.9934e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.034)	Loss 4.4915e-01 (2.9278e-01)	Acc@1  88.00 ( 90.55)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.020 ( 0.028)	Loss 4.5210e-01 (3.0235e-01)	Acc@1  86.00 ( 90.48)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.023 ( 0.026)	Loss 3.4785e-01 (3.2144e-01)	Acc@1  88.00 ( 90.42)	Acc@5  99.00 ( 99.65)
Test: [ 40/100]	Time  0.021 ( 0.024)	Loss 3.6597e-01 (3.2823e-01)	Acc@1  89.00 ( 90.07)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.021 ( 0.024)	Loss 2.9972e-01 (3.2159e-01)	Acc@1  94.00 ( 90.63)	Acc@5 100.00 ( 99.69)
Test: [ 60/100]	Time  0.021 ( 0.023)	Loss 3.7415e-01 (3.2621e-01)	Acc@1  91.00 ( 90.34)	Acc@5  99.00 ( 99.69)
Test: [ 70/100]	Time  0.022 ( 0.023)	Loss 3.5573e-01 (3.1796e-01)	Acc@1  89.00 ( 90.31)	Acc@5 100.00 ( 99.70)
Test: [ 80/100]	Time  0.020 ( 0.022)	Loss 2.0767e-01 (3.1375e-01)	Acc@1  92.00 ( 90.43)	Acc@5 100.00 ( 99.72)
Test: [ 90/100]	Time  0.017 ( 0.022)	Loss 1.9101e-01 (3.1373e-01)	Acc@1  92.00 ( 90.40)	Acc@5 100.00 ( 99.73)
 * Acc@1 90.490 Acc@5 99.730
### epoch[41] execution time: 18.39599061012268
EPOCH 42
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [42][  0/391]	Time  0.215 ( 0.215)	Data  0.171 ( 0.171)	Loss 1.1443e-01 (1.1443e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [42][ 10/391]	Time  0.039 ( 0.055)	Data  0.001 ( 0.017)	Loss 8.2246e-02 (1.2136e-01)	Acc@1  97.66 ( 95.45)	Acc@5 100.00 ( 99.93)
Epoch: [42][ 20/391]	Time  0.038 ( 0.049)	Data  0.001 ( 0.009)	Loss 3.9640e-02 (1.1341e-01)	Acc@1  99.22 ( 95.80)	Acc@5 100.00 ( 99.96)
Epoch: [42][ 30/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.007)	Loss 1.0218e-01 (1.0822e-01)	Acc@1  99.22 ( 96.27)	Acc@5 100.00 ( 99.97)
Epoch: [42][ 40/391]	Time  0.043 ( 0.045)	Data  0.001 ( 0.005)	Loss 1.1427e-01 (1.1130e-01)	Acc@1  94.53 ( 96.07)	Acc@5 100.00 ( 99.98)
Epoch: [42][ 50/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.004)	Loss 5.4808e-02 (1.1254e-01)	Acc@1  98.44 ( 96.03)	Acc@5 100.00 ( 99.98)
Epoch: [42][ 60/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.0336e-01 (1.1449e-01)	Acc@1  95.31 ( 95.95)	Acc@5 100.00 ( 99.96)
Epoch: [42][ 70/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 5.9023e-02 (1.1292e-01)	Acc@1  98.44 ( 95.94)	Acc@5 100.00 ( 99.97)
Epoch: [42][ 80/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.7944e-01 (1.1173e-01)	Acc@1  93.75 ( 96.05)	Acc@5 100.00 ( 99.97)
Epoch: [42][ 90/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 9.6240e-02 (1.1101e-01)	Acc@1  96.88 ( 96.11)	Acc@5 100.00 ( 99.97)
Epoch: [42][100/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.2462e-01 (1.1262e-01)	Acc@1  96.09 ( 96.04)	Acc@5 100.00 ( 99.98)
Epoch: [42][110/391]	Time  0.040 ( 0.042)	Data  0.002 ( 0.002)	Loss 1.2497e-01 (1.1291e-01)	Acc@1  95.31 ( 96.06)	Acc@5 100.00 ( 99.98)
Epoch: [42][120/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.002)	Loss 9.9764e-02 (1.1189e-01)	Acc@1  94.53 ( 96.08)	Acc@5 100.00 ( 99.98)
Epoch: [42][130/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.0231e-02 (1.1078e-01)	Acc@1  97.66 ( 96.12)	Acc@5 100.00 ( 99.98)
Epoch: [42][140/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5021e-01 (1.1138e-01)	Acc@1  95.31 ( 96.08)	Acc@5 100.00 ( 99.98)
Epoch: [42][150/391]	Time  0.034 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1697e-01 (1.1142e-01)	Acc@1  96.09 ( 96.12)	Acc@5 100.00 ( 99.98)
Epoch: [42][160/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.7818e-02 (1.1108e-01)	Acc@1  97.66 ( 96.17)	Acc@5 100.00 ( 99.98)
Epoch: [42][170/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2661e-01 (1.1128e-01)	Acc@1  97.66 ( 96.16)	Acc@5  99.22 ( 99.97)
Epoch: [42][180/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0118e-01 (1.1138e-01)	Acc@1  96.09 ( 96.15)	Acc@5 100.00 ( 99.97)
Epoch: [42][190/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0642e-01 (1.1154e-01)	Acc@1  97.66 ( 96.15)	Acc@5 100.00 ( 99.97)
Epoch: [42][200/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7215e-01 (1.1303e-01)	Acc@1  93.75 ( 96.08)	Acc@5  99.22 ( 99.97)
Epoch: [42][210/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.8132e-01 (1.1301e-01)	Acc@1  92.97 ( 96.04)	Acc@5 100.00 ( 99.97)
Epoch: [42][220/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.5630e-02 (1.1302e-01)	Acc@1  96.09 ( 96.05)	Acc@5 100.00 ( 99.97)
Epoch: [42][230/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2067e-01 (1.1315e-01)	Acc@1  96.09 ( 96.05)	Acc@5 100.00 ( 99.97)
Epoch: [42][240/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4402e-01 (1.1316e-01)	Acc@1  95.31 ( 96.05)	Acc@5 100.00 ( 99.97)
Epoch: [42][250/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.9947e-02 (1.1413e-01)	Acc@1  97.66 ( 96.02)	Acc@5 100.00 ( 99.97)
Epoch: [42][260/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2748e-01 (1.1410e-01)	Acc@1  95.31 ( 96.03)	Acc@5 100.00 ( 99.97)
Epoch: [42][270/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2521e-02 (1.1343e-01)	Acc@1  98.44 ( 96.06)	Acc@5 100.00 ( 99.97)
Epoch: [42][280/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.1951e-02 (1.1400e-01)	Acc@1  96.88 ( 96.03)	Acc@5 100.00 ( 99.97)
Epoch: [42][290/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.3096e-02 (1.1425e-01)	Acc@1  97.66 ( 96.02)	Acc@5 100.00 ( 99.97)
Epoch: [42][300/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5918e-01 (1.1449e-01)	Acc@1  92.97 ( 96.00)	Acc@5 100.00 ( 99.97)
Epoch: [42][310/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.0804e-01 (1.1435e-01)	Acc@1  93.75 ( 96.01)	Acc@5  99.22 ( 99.96)
Epoch: [42][320/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2030e-01 (1.1371e-01)	Acc@1  95.31 ( 96.02)	Acc@5 100.00 ( 99.96)
Epoch: [42][330/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5894e-01 (1.1352e-01)	Acc@1  92.97 ( 96.02)	Acc@5  99.22 ( 99.96)
Epoch: [42][340/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.6879e-01 (1.1408e-01)	Acc@1  95.31 ( 96.01)	Acc@5 100.00 ( 99.96)
Epoch: [42][350/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.3067e-01 (1.1474e-01)	Acc@1  96.09 ( 96.00)	Acc@5 100.00 ( 99.96)
Epoch: [42][360/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.7644e-01 (1.1465e-01)	Acc@1  95.31 ( 96.00)	Acc@5 100.00 ( 99.96)
Epoch: [42][370/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.0349e-01 (1.1499e-01)	Acc@1  94.53 ( 95.98)	Acc@5 100.00 ( 99.96)
Epoch: [42][380/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.9462e-01 (1.1528e-01)	Acc@1  92.97 ( 95.96)	Acc@5 100.00 ( 99.96)
Epoch: [42][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.1134e-01 (1.1530e-01)	Acc@1  93.75 ( 95.96)	Acc@5 100.00 ( 99.96)
## e[42] optimizer.zero_grad (sum) time: 0.2631688117980957
## e[42]       loss.backward (sum) time: 3.9446632862091064
## e[42]      optimizer.step (sum) time: 1.8475944995880127
## epoch[42] training(only) time: 15.965080261230469
# Switched to evaluate mode...
Test: [  0/100]	Time  0.177 ( 0.177)	Loss 2.9210e-01 (2.9210e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.022 ( 0.036)	Loss 4.3011e-01 (2.8984e-01)	Acc@1  89.00 ( 90.45)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.021 ( 0.029)	Loss 4.4475e-01 (2.9984e-01)	Acc@1  88.00 ( 90.57)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.017 ( 0.025)	Loss 3.7310e-01 (3.2383e-01)	Acc@1  89.00 ( 90.58)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.017 ( 0.024)	Loss 3.4130e-01 (3.3364e-01)	Acc@1  90.00 ( 90.41)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.024 ( 0.024)	Loss 3.0571e-01 (3.2604e-01)	Acc@1  94.00 ( 90.69)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.018 ( 0.023)	Loss 3.2906e-01 (3.2811e-01)	Acc@1  90.00 ( 90.26)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.022 ( 0.023)	Loss 3.7265e-01 (3.2013e-01)	Acc@1  89.00 ( 90.25)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.021 ( 0.022)	Loss 2.3417e-01 (3.1758e-01)	Acc@1  92.00 ( 90.37)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.024 ( 0.022)	Loss 2.2129e-01 (3.1546e-01)	Acc@1  94.00 ( 90.42)	Acc@5 100.00 ( 99.77)
 * Acc@1 90.520 Acc@5 99.750
### epoch[42] execution time: 18.31333875656128
EPOCH 43
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [43][  0/391]	Time  0.212 ( 0.212)	Data  0.172 ( 0.172)	Loss 8.6844e-02 (8.6844e-02)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [43][ 10/391]	Time  0.042 ( 0.057)	Data  0.001 ( 0.017)	Loss 8.6063e-02 (1.0715e-01)	Acc@1  96.88 ( 96.02)	Acc@5 100.00 (100.00)
Epoch: [43][ 20/391]	Time  0.040 ( 0.049)	Data  0.001 ( 0.009)	Loss 8.6958e-02 (1.0998e-01)	Acc@1  95.31 ( 95.91)	Acc@5 100.00 ( 99.96)
Epoch: [43][ 30/391]	Time  0.042 ( 0.046)	Data  0.001 ( 0.006)	Loss 1.2819e-01 (1.1383e-01)	Acc@1  93.75 ( 95.74)	Acc@5 100.00 ( 99.95)
Epoch: [43][ 40/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.005)	Loss 1.5951e-01 (1.1207e-01)	Acc@1  92.97 ( 95.87)	Acc@5 100.00 ( 99.94)
Epoch: [43][ 50/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.1999e-01 (1.1314e-01)	Acc@1  93.75 ( 95.86)	Acc@5 100.00 ( 99.95)
Epoch: [43][ 60/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.1823e-01 (1.1033e-01)	Acc@1  95.31 ( 95.99)	Acc@5 100.00 ( 99.96)
Epoch: [43][ 70/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 5.0128e-02 (1.1131e-01)	Acc@1 100.00 ( 96.06)	Acc@5 100.00 ( 99.96)
Epoch: [43][ 80/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.1885e-01 (1.1000e-01)	Acc@1  96.09 ( 96.13)	Acc@5 100.00 ( 99.96)
Epoch: [43][ 90/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.1168e-01 (1.0906e-01)	Acc@1  96.09 ( 96.15)	Acc@5 100.00 ( 99.96)
Epoch: [43][100/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.5306e-01 (1.1162e-01)	Acc@1  94.53 ( 96.08)	Acc@5 100.00 ( 99.95)
Epoch: [43][110/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.1884e-01 (1.1052e-01)	Acc@1  95.31 ( 96.10)	Acc@5 100.00 ( 99.95)
Epoch: [43][120/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 8.2603e-02 (1.1000e-01)	Acc@1  97.66 ( 96.16)	Acc@5 100.00 ( 99.95)
Epoch: [43][130/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.3745e-01 (1.1060e-01)	Acc@1  94.53 ( 96.17)	Acc@5 100.00 ( 99.95)
Epoch: [43][140/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.2232e-01 (1.1036e-01)	Acc@1  94.53 ( 96.17)	Acc@5 100.00 ( 99.95)
Epoch: [43][150/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.8537e-01 (1.1001e-01)	Acc@1  92.19 ( 96.19)	Acc@5 100.00 ( 99.95)
Epoch: [43][160/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 8.6696e-02 (1.0967e-01)	Acc@1  98.44 ( 96.24)	Acc@5 100.00 ( 99.95)
Epoch: [43][170/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.3558e-01 (1.0988e-01)	Acc@1  95.31 ( 96.22)	Acc@5 100.00 ( 99.95)
Epoch: [43][180/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.9253e-02 (1.1009e-01)	Acc@1  95.31 ( 96.23)	Acc@5 100.00 ( 99.95)
Epoch: [43][190/391]	Time  0.045 ( 0.041)	Data  0.002 ( 0.002)	Loss 1.2587e-01 (1.1005e-01)	Acc@1  94.53 ( 96.23)	Acc@5 100.00 ( 99.96)
Epoch: [43][200/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2222e-01 (1.0999e-01)	Acc@1  94.53 ( 96.19)	Acc@5 100.00 ( 99.95)
Epoch: [43][210/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1267e-01 (1.1049e-01)	Acc@1  97.66 ( 96.20)	Acc@5 100.00 ( 99.96)
Epoch: [43][220/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2453e-01 (1.1007e-01)	Acc@1  96.88 ( 96.21)	Acc@5 100.00 ( 99.96)
Epoch: [43][230/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.7960e-02 (1.0977e-01)	Acc@1  97.66 ( 96.23)	Acc@5 100.00 ( 99.96)
Epoch: [43][240/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0800e-01 (1.1030e-01)	Acc@1  96.09 ( 96.23)	Acc@5 100.00 ( 99.95)
Epoch: [43][250/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.2244e-01 (1.1196e-01)	Acc@1  92.19 ( 96.18)	Acc@5  99.22 ( 99.94)
Epoch: [43][260/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1943e-01 (1.1198e-01)	Acc@1  95.31 ( 96.16)	Acc@5 100.00 ( 99.94)
Epoch: [43][270/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7397e-01 (1.1197e-01)	Acc@1  92.19 ( 96.14)	Acc@5 100.00 ( 99.95)
Epoch: [43][280/391]	Time  0.040 ( 0.041)	Data  0.002 ( 0.002)	Loss 7.0282e-02 (1.1268e-01)	Acc@1  96.88 ( 96.11)	Acc@5 100.00 ( 99.94)
Epoch: [43][290/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6869e-01 (1.1332e-01)	Acc@1  95.31 ( 96.09)	Acc@5 100.00 ( 99.95)
Epoch: [43][300/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6195e-01 (1.1376e-01)	Acc@1  95.31 ( 96.05)	Acc@5 100.00 ( 99.95)
Epoch: [43][310/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2858e-01 (1.1380e-01)	Acc@1  96.09 ( 96.07)	Acc@5 100.00 ( 99.95)
Epoch: [43][320/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3154e-01 (1.1360e-01)	Acc@1  94.53 ( 96.06)	Acc@5 100.00 ( 99.95)
Epoch: [43][330/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.7115e-02 (1.1358e-01)	Acc@1  98.44 ( 96.06)	Acc@5 100.00 ( 99.95)
Epoch: [43][340/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1187e-01 (1.1368e-01)	Acc@1  96.88 ( 96.06)	Acc@5 100.00 ( 99.95)
Epoch: [43][350/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.0055e-02 (1.1311e-01)	Acc@1  98.44 ( 96.07)	Acc@5 100.00 ( 99.95)
Epoch: [43][360/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.9384e-02 (1.1343e-01)	Acc@1  96.09 ( 96.05)	Acc@5 100.00 ( 99.95)
Epoch: [43][370/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.1738e-01 (1.1358e-01)	Acc@1  96.88 ( 96.05)	Acc@5 100.00 ( 99.95)
Epoch: [43][380/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.0503e-01 (1.1343e-01)	Acc@1  95.31 ( 96.04)	Acc@5 100.00 ( 99.95)
Epoch: [43][390/391]	Time  0.030 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.4450e-01 (1.1313e-01)	Acc@1  96.25 ( 96.05)	Acc@5 100.00 ( 99.96)
## e[43] optimizer.zero_grad (sum) time: 0.2612464427947998
## e[43]       loss.backward (sum) time: 3.8961026668548584
## e[43]      optimizer.step (sum) time: 1.861665964126587
## epoch[43] training(only) time: 15.929939031600952
# Switched to evaluate mode...
Test: [  0/100]	Time  0.173 ( 0.173)	Loss 3.2707e-01 (3.2707e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 4.6814e-01 (2.9546e-01)	Acc@1  89.00 ( 89.82)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.016 ( 0.027)	Loss 4.9902e-01 (3.0363e-01)	Acc@1  86.00 ( 90.05)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.021 ( 0.025)	Loss 3.9681e-01 (3.2949e-01)	Acc@1  89.00 ( 89.94)	Acc@5 100.00 ( 99.77)
Test: [ 40/100]	Time  0.023 ( 0.024)	Loss 3.7819e-01 (3.3819e-01)	Acc@1  89.00 ( 89.71)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.019 ( 0.023)	Loss 3.1809e-01 (3.3097e-01)	Acc@1  93.00 ( 90.02)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.022 ( 0.023)	Loss 3.2905e-01 (3.3291e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.024 ( 0.022)	Loss 4.2848e-01 (3.2354e-01)	Acc@1  88.00 ( 90.08)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.023 ( 0.022)	Loss 1.9229e-01 (3.1954e-01)	Acc@1  93.00 ( 90.26)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.024 ( 0.022)	Loss 2.5551e-01 (3.1854e-01)	Acc@1  92.00 ( 90.32)	Acc@5  99.00 ( 99.76)
 * Acc@1 90.420 Acc@5 99.760
### epoch[43] execution time: 18.245698928833008
EPOCH 44
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [44][  0/391]	Time  0.217 ( 0.217)	Data  0.175 ( 0.175)	Loss 1.0704e-01 (1.0704e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [44][ 10/391]	Time  0.038 ( 0.056)	Data  0.001 ( 0.017)	Loss 9.6873e-02 (1.0041e-01)	Acc@1  96.88 ( 96.80)	Acc@5 100.00 ( 99.93)
Epoch: [44][ 20/391]	Time  0.037 ( 0.049)	Data  0.001 ( 0.009)	Loss 7.1146e-02 (1.0481e-01)	Acc@1  97.66 ( 96.69)	Acc@5 100.00 ( 99.93)
Epoch: [44][ 30/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.007)	Loss 7.2068e-02 (1.0345e-01)	Acc@1  98.44 ( 96.60)	Acc@5 100.00 ( 99.92)
Epoch: [44][ 40/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.005)	Loss 5.7071e-02 (9.9911e-02)	Acc@1  99.22 ( 96.70)	Acc@5 100.00 ( 99.94)
Epoch: [44][ 50/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.6399e-01 (1.0481e-01)	Acc@1  95.31 ( 96.49)	Acc@5 100.00 ( 99.95)
Epoch: [44][ 60/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.6060e-01 (1.0589e-01)	Acc@1  92.97 ( 96.41)	Acc@5 100.00 ( 99.96)
Epoch: [44][ 70/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.1846e-01 (1.0667e-01)	Acc@1  94.53 ( 96.34)	Acc@5 100.00 ( 99.97)
Epoch: [44][ 80/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.4568e-02 (1.0755e-01)	Acc@1 100.00 ( 96.27)	Acc@5 100.00 ( 99.96)
Epoch: [44][ 90/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.0836e-01 (1.0678e-01)	Acc@1  95.31 ( 96.26)	Acc@5 100.00 ( 99.97)
Epoch: [44][100/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 8.5545e-02 (1.0713e-01)	Acc@1  97.66 ( 96.22)	Acc@5 100.00 ( 99.97)
Epoch: [44][110/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 9.1490e-02 (1.0692e-01)	Acc@1  96.88 ( 96.26)	Acc@5 100.00 ( 99.96)
Epoch: [44][120/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.6011e-01 (1.0826e-01)	Acc@1  96.09 ( 96.26)	Acc@5 100.00 ( 99.97)
Epoch: [44][130/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0114e-01 (1.0741e-01)	Acc@1  96.09 ( 96.28)	Acc@5 100.00 ( 99.97)
Epoch: [44][140/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.6883e-02 (1.0830e-01)	Acc@1  96.88 ( 96.28)	Acc@5 100.00 ( 99.97)
Epoch: [44][150/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.5746e-02 (1.0905e-01)	Acc@1  96.09 ( 96.22)	Acc@5 100.00 ( 99.96)
Epoch: [44][160/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7558e-01 (1.1009e-01)	Acc@1  93.75 ( 96.20)	Acc@5 100.00 ( 99.96)
Epoch: [44][170/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0161e-01 (1.0977e-01)	Acc@1  97.66 ( 96.20)	Acc@5 100.00 ( 99.95)
Epoch: [44][180/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2387e-01 (1.0991e-01)	Acc@1  96.09 ( 96.22)	Acc@5 100.00 ( 99.96)
Epoch: [44][190/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.8417e-01 (1.1076e-01)	Acc@1  94.53 ( 96.21)	Acc@5 100.00 ( 99.96)
Epoch: [44][200/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.3464e-02 (1.1105e-01)	Acc@1  96.09 ( 96.19)	Acc@5 100.00 ( 99.96)
Epoch: [44][210/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.4945e-02 (1.0977e-01)	Acc@1  97.66 ( 96.25)	Acc@5 100.00 ( 99.96)
Epoch: [44][220/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0672e-01 (1.0990e-01)	Acc@1  96.88 ( 96.24)	Acc@5 100.00 ( 99.96)
Epoch: [44][230/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.2107e-01 (1.1007e-01)	Acc@1  91.41 ( 96.23)	Acc@5 100.00 ( 99.96)
Epoch: [44][240/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0656e-01 (1.1043e-01)	Acc@1  96.09 ( 96.22)	Acc@5 100.00 ( 99.95)
Epoch: [44][250/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0255e-01 (1.0987e-01)	Acc@1  97.66 ( 96.24)	Acc@5 100.00 ( 99.96)
Epoch: [44][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0517e-01 (1.0997e-01)	Acc@1  96.88 ( 96.25)	Acc@5 100.00 ( 99.96)
Epoch: [44][270/391]	Time  0.045 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.8533e-01 (1.1074e-01)	Acc@1  93.75 ( 96.22)	Acc@5  99.22 ( 99.96)
Epoch: [44][280/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6587e-01 (1.1125e-01)	Acc@1  95.31 ( 96.19)	Acc@5 100.00 ( 99.96)
Epoch: [44][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1654e-02 (1.1053e-01)	Acc@1  99.22 ( 96.22)	Acc@5 100.00 ( 99.96)
Epoch: [44][300/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2334e-01 (1.1025e-01)	Acc@1  96.09 ( 96.23)	Acc@5 100.00 ( 99.96)
Epoch: [44][310/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.4138e-02 (1.0992e-01)	Acc@1  97.66 ( 96.24)	Acc@5 100.00 ( 99.96)
Epoch: [44][320/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5938e-01 (1.0972e-01)	Acc@1  96.09 ( 96.25)	Acc@5 100.00 ( 99.96)
Epoch: [44][330/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5953e-01 (1.0946e-01)	Acc@1  92.19 ( 96.24)	Acc@5 100.00 ( 99.96)
Epoch: [44][340/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0906e-01 (1.0934e-01)	Acc@1  96.88 ( 96.25)	Acc@5 100.00 ( 99.96)
Epoch: [44][350/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.8613e-01 (1.0982e-01)	Acc@1  92.97 ( 96.22)	Acc@5 100.00 ( 99.96)
Epoch: [44][360/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.2088e-01 (1.0985e-01)	Acc@1  96.09 ( 96.22)	Acc@5 100.00 ( 99.96)
Epoch: [44][370/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.6839e-01 (1.0998e-01)	Acc@1  96.09 ( 96.22)	Acc@5 100.00 ( 99.96)
Epoch: [44][380/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 9.2595e-02 (1.0988e-01)	Acc@1  95.31 ( 96.22)	Acc@5 100.00 ( 99.96)
Epoch: [44][390/391]	Time  0.027 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.9071e-01 (1.0970e-01)	Acc@1  93.75 ( 96.23)	Acc@5 100.00 ( 99.96)
## e[44] optimizer.zero_grad (sum) time: 0.2624173164367676
## e[44]       loss.backward (sum) time: 3.9097585678100586
## e[44]      optimizer.step (sum) time: 1.8227465152740479
## epoch[44] training(only) time: 15.966721773147583
# Switched to evaluate mode...
Test: [  0/100]	Time  0.181 ( 0.181)	Loss 3.1561e-01 (3.1561e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.023 ( 0.036)	Loss 5.1688e-01 (3.1674e-01)	Acc@1  89.00 ( 90.18)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.021 ( 0.029)	Loss 4.5258e-01 (3.1619e-01)	Acc@1  89.00 ( 90.62)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.021 ( 0.026)	Loss 4.0261e-01 (3.3106e-01)	Acc@1  88.00 ( 90.48)	Acc@5  99.00 ( 99.77)
Test: [ 40/100]	Time  0.023 ( 0.024)	Loss 3.2687e-01 (3.3680e-01)	Acc@1  88.00 ( 90.12)	Acc@5 100.00 ( 99.76)
Test: [ 50/100]	Time  0.019 ( 0.023)	Loss 2.5969e-01 (3.3093e-01)	Acc@1  94.00 ( 90.49)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.021 ( 0.023)	Loss 3.3660e-01 (3.3176e-01)	Acc@1  90.00 ( 90.31)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.019 ( 0.022)	Loss 4.1176e-01 (3.2364e-01)	Acc@1  87.00 ( 90.39)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.022 ( 0.022)	Loss 2.3493e-01 (3.2019e-01)	Acc@1  91.00 ( 90.51)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.024 ( 0.022)	Loss 2.3754e-01 (3.1928e-01)	Acc@1  94.00 ( 90.52)	Acc@5  99.00 ( 99.77)
 * Acc@1 90.640 Acc@5 99.770
### epoch[44] execution time: 18.27653193473816
EPOCH 45
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [45][  0/391]	Time  0.223 ( 0.223)	Data  0.181 ( 0.181)	Loss 1.2855e-01 (1.2855e-01)	Acc@1  94.53 ( 94.53)	Acc@5  99.22 ( 99.22)
Epoch: [45][ 10/391]	Time  0.039 ( 0.057)	Data  0.001 ( 0.017)	Loss 5.9650e-02 (9.8825e-02)	Acc@1  97.66 ( 96.73)	Acc@5 100.00 ( 99.93)
Epoch: [45][ 20/391]	Time  0.041 ( 0.049)	Data  0.001 ( 0.010)	Loss 7.3256e-02 (1.0629e-01)	Acc@1  97.66 ( 96.43)	Acc@5 100.00 ( 99.96)
Epoch: [45][ 30/391]	Time  0.042 ( 0.046)	Data  0.001 ( 0.007)	Loss 1.6100e-01 (1.0312e-01)	Acc@1  96.09 ( 96.55)	Acc@5 100.00 ( 99.97)
Epoch: [45][ 40/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.005)	Loss 1.0499e-01 (1.0276e-01)	Acc@1  94.53 ( 96.51)	Acc@5 100.00 ( 99.96)
Epoch: [45][ 50/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.3557e-01 (1.0258e-01)	Acc@1  96.88 ( 96.51)	Acc@5  99.22 ( 99.94)
Epoch: [45][ 60/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.004)	Loss 9.7780e-02 (1.0399e-01)	Acc@1  98.44 ( 96.48)	Acc@5  99.22 ( 99.94)
Epoch: [45][ 70/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.1278e-01 (1.0304e-01)	Acc@1  95.31 ( 96.49)	Acc@5 100.00 ( 99.94)
Epoch: [45][ 80/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.0651e-01 (9.9662e-02)	Acc@1  95.31 ( 96.62)	Acc@5 100.00 ( 99.95)
Epoch: [45][ 90/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.0708e-01 (1.0153e-01)	Acc@1  96.88 ( 96.58)	Acc@5 100.00 ( 99.96)
Epoch: [45][100/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.1409e-01 (1.0187e-01)	Acc@1  94.53 ( 96.57)	Acc@5 100.00 ( 99.96)
Epoch: [45][110/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.1511e-01 (1.0102e-01)	Acc@1  95.31 ( 96.62)	Acc@5 100.00 ( 99.96)
Epoch: [45][120/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 8.4838e-02 (1.0179e-01)	Acc@1  96.88 ( 96.57)	Acc@5 100.00 ( 99.97)
Epoch: [45][130/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.0022e-01 (1.0180e-01)	Acc@1  96.09 ( 96.58)	Acc@5 100.00 ( 99.97)
Epoch: [45][140/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.5746e-02 (1.0180e-01)	Acc@1  96.09 ( 96.58)	Acc@5 100.00 ( 99.97)
Epoch: [45][150/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.1764e-02 (1.0210e-01)	Acc@1  96.09 ( 96.54)	Acc@5 100.00 ( 99.97)
Epoch: [45][160/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.5426e-02 (1.0136e-01)	Acc@1  96.88 ( 96.56)	Acc@5 100.00 ( 99.97)
Epoch: [45][170/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.5119e-02 (1.0226e-01)	Acc@1  97.66 ( 96.52)	Acc@5 100.00 ( 99.97)
Epoch: [45][180/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.3985e-02 (1.0149e-01)	Acc@1  98.44 ( 96.53)	Acc@5  99.22 ( 99.97)
Epoch: [45][190/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.6884e-02 (1.0099e-01)	Acc@1  98.44 ( 96.55)	Acc@5 100.00 ( 99.97)
Epoch: [45][200/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0727e-01 (1.0162e-01)	Acc@1  96.09 ( 96.51)	Acc@5 100.00 ( 99.97)
Epoch: [45][210/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.7501e-02 (1.0108e-01)	Acc@1  99.22 ( 96.54)	Acc@5 100.00 ( 99.97)
Epoch: [45][220/391]	Time  0.045 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.8865e-02 (1.0129e-01)	Acc@1  95.31 ( 96.54)	Acc@5 100.00 ( 99.96)
Epoch: [45][230/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3115e-01 (1.0208e-01)	Acc@1  97.66 ( 96.52)	Acc@5 100.00 ( 99.97)
Epoch: [45][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.3048e-02 (1.0236e-01)	Acc@1  97.66 ( 96.48)	Acc@5 100.00 ( 99.97)
Epoch: [45][250/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0810e-01 (1.0216e-01)	Acc@1  94.53 ( 96.48)	Acc@5 100.00 ( 99.97)
Epoch: [45][260/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1705e-02 (1.0213e-01)	Acc@1  99.22 ( 96.47)	Acc@5 100.00 ( 99.96)
Epoch: [45][270/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.8457e-02 (1.0263e-01)	Acc@1  98.44 ( 96.46)	Acc@5  99.22 ( 99.96)
Epoch: [45][280/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.5985e-02 (1.0306e-01)	Acc@1  96.09 ( 96.44)	Acc@5 100.00 ( 99.96)
Epoch: [45][290/391]	Time  0.035 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2112e-01 (1.0357e-01)	Acc@1  96.09 ( 96.42)	Acc@5 100.00 ( 99.97)
Epoch: [45][300/391]	Time  0.040 ( 0.041)	Data  0.002 ( 0.002)	Loss 1.1908e-01 (1.0414e-01)	Acc@1  96.09 ( 96.39)	Acc@5 100.00 ( 99.96)
Epoch: [45][310/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.9052e-02 (1.0370e-01)	Acc@1  98.44 ( 96.41)	Acc@5 100.00 ( 99.96)
Epoch: [45][320/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.2115e-02 (1.0377e-01)	Acc@1  98.44 ( 96.41)	Acc@5 100.00 ( 99.97)
Epoch: [45][330/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.1188e-01 (1.0354e-01)	Acc@1  91.41 ( 96.43)	Acc@5 100.00 ( 99.96)
Epoch: [45][340/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.6483e-01 (1.0410e-01)	Acc@1  95.31 ( 96.42)	Acc@5 100.00 ( 99.97)
Epoch: [45][350/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.0761e-01 (1.0451e-01)	Acc@1  92.97 ( 96.42)	Acc@5 100.00 ( 99.96)
Epoch: [45][360/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.1990e-01 (1.0446e-01)	Acc@1  94.53 ( 96.42)	Acc@5 100.00 ( 99.97)
Epoch: [45][370/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.001)	Loss 9.7963e-02 (1.0479e-01)	Acc@1  96.88 ( 96.41)	Acc@5 100.00 ( 99.97)
Epoch: [45][380/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.1890e-01 (1.0527e-01)	Acc@1  95.31 ( 96.39)	Acc@5 100.00 ( 99.97)
Epoch: [45][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.6675e-01 (1.0602e-01)	Acc@1  95.00 ( 96.37)	Acc@5 100.00 ( 99.97)
## e[45] optimizer.zero_grad (sum) time: 0.26496362686157227
## e[45]       loss.backward (sum) time: 3.9478583335876465
## e[45]      optimizer.step (sum) time: 1.8298914432525635
## epoch[45] training(only) time: 15.894999504089355
# Switched to evaluate mode...
Test: [  0/100]	Time  0.180 ( 0.180)	Loss 2.8113e-01 (2.8113e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 4.9383e-01 (2.9700e-01)	Acc@1  88.00 ( 91.00)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.022 ( 0.028)	Loss 4.3108e-01 (3.0828e-01)	Acc@1  88.00 ( 90.67)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.016 ( 0.025)	Loss 4.0728e-01 (3.3152e-01)	Acc@1  89.00 ( 90.45)	Acc@5  99.00 ( 99.71)
Test: [ 40/100]	Time  0.018 ( 0.024)	Loss 4.0801e-01 (3.4200e-01)	Acc@1  87.00 ( 89.83)	Acc@5 100.00 ( 99.66)
Test: [ 50/100]	Time  0.021 ( 0.024)	Loss 2.9964e-01 (3.3468e-01)	Acc@1  94.00 ( 90.31)	Acc@5 100.00 ( 99.69)
Test: [ 60/100]	Time  0.022 ( 0.024)	Loss 3.0445e-01 (3.3443e-01)	Acc@1  92.00 ( 90.21)	Acc@5 100.00 ( 99.70)
Test: [ 70/100]	Time  0.023 ( 0.023)	Loss 3.9184e-01 (3.2625e-01)	Acc@1  89.00 ( 90.28)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.023 ( 0.023)	Loss 1.9905e-01 (3.2335e-01)	Acc@1  95.00 ( 90.42)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.024 ( 0.023)	Loss 2.1247e-01 (3.2300e-01)	Acc@1  92.00 ( 90.35)	Acc@5 100.00 ( 99.75)
 * Acc@1 90.470 Acc@5 99.740
### epoch[45] execution time: 18.293978214263916
EPOCH 46
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [46][  0/391]	Time  0.229 ( 0.229)	Data  0.187 ( 0.187)	Loss 1.1997e-01 (1.1997e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
Epoch: [46][ 10/391]	Time  0.041 ( 0.057)	Data  0.001 ( 0.018)	Loss 2.0338e-01 (1.1209e-01)	Acc@1  92.97 ( 96.31)	Acc@5 100.00 (100.00)
Epoch: [46][ 20/391]	Time  0.040 ( 0.050)	Data  0.001 ( 0.010)	Loss 7.7755e-02 (1.0414e-01)	Acc@1  96.88 ( 96.54)	Acc@5 100.00 ( 99.96)
Epoch: [46][ 30/391]	Time  0.041 ( 0.047)	Data  0.001 ( 0.007)	Loss 5.8243e-02 (1.0073e-01)	Acc@1  99.22 ( 96.55)	Acc@5 100.00 ( 99.95)
Epoch: [46][ 40/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.005)	Loss 9.4966e-02 (1.0327e-01)	Acc@1  97.66 ( 96.44)	Acc@5 100.00 ( 99.96)
Epoch: [46][ 50/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.005)	Loss 5.4704e-02 (1.0421e-01)	Acc@1  99.22 ( 96.48)	Acc@5 100.00 ( 99.97)
Epoch: [46][ 60/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.004)	Loss 8.5578e-02 (1.0260e-01)	Acc@1  97.66 ( 96.50)	Acc@5 100.00 ( 99.97)
Epoch: [46][ 70/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.2741e-01 (1.0051e-01)	Acc@1  93.75 ( 96.53)	Acc@5 100.00 ( 99.97)
Epoch: [46][ 80/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 7.4733e-02 (9.9423e-02)	Acc@1  97.66 ( 96.61)	Acc@5  99.22 ( 99.95)
Epoch: [46][ 90/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.1250e-01 (1.0225e-01)	Acc@1  97.66 ( 96.50)	Acc@5 100.00 ( 99.96)
Epoch: [46][100/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.9225e-02 (1.0220e-01)	Acc@1  96.88 ( 96.47)	Acc@5 100.00 ( 99.96)
Epoch: [46][110/391]	Time  0.053 ( 0.042)	Data  0.002 ( 0.003)	Loss 8.3360e-02 (1.0183e-01)	Acc@1  98.44 ( 96.49)	Acc@5 100.00 ( 99.96)
Epoch: [46][120/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.0965e-01 (1.0345e-01)	Acc@1  95.31 ( 96.44)	Acc@5 100.00 ( 99.96)
Epoch: [46][130/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.9151e-02 (1.0330e-01)	Acc@1  98.44 ( 96.45)	Acc@5 100.00 ( 99.96)
Epoch: [46][140/391]	Time  0.053 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.4872e-01 (1.0333e-01)	Acc@1  92.97 ( 96.42)	Acc@5 100.00 ( 99.97)
Epoch: [46][150/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 7.9218e-02 (1.0423e-01)	Acc@1  98.44 ( 96.39)	Acc@5 100.00 ( 99.97)
Epoch: [46][160/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.4910e-01 (1.0464e-01)	Acc@1  93.75 ( 96.37)	Acc@5  99.22 ( 99.97)
Epoch: [46][170/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3739e-01 (1.0502e-01)	Acc@1  95.31 ( 96.35)	Acc@5  99.22 ( 99.96)
Epoch: [46][180/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.6837e-02 (1.0449e-01)	Acc@1  98.44 ( 96.39)	Acc@5 100.00 ( 99.96)
Epoch: [46][190/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.4161e-02 (1.0371e-01)	Acc@1  96.88 ( 96.40)	Acc@5 100.00 ( 99.96)
Epoch: [46][200/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.5962e-02 (1.0307e-01)	Acc@1  97.66 ( 96.43)	Acc@5 100.00 ( 99.96)
Epoch: [46][210/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.4916e-02 (1.0293e-01)	Acc@1  97.66 ( 96.43)	Acc@5 100.00 ( 99.96)
Epoch: [46][220/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3318e-01 (1.0227e-01)	Acc@1  96.88 ( 96.47)	Acc@5  99.22 ( 99.96)
Epoch: [46][230/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.5655e-02 (1.0200e-01)	Acc@1  97.66 ( 96.48)	Acc@5 100.00 ( 99.96)
Epoch: [46][240/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.8615e-01 (1.0209e-01)	Acc@1  95.31 ( 96.49)	Acc@5  99.22 ( 99.95)
Epoch: [46][250/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.9313e-02 (1.0148e-01)	Acc@1  96.88 ( 96.52)	Acc@5 100.00 ( 99.96)
Epoch: [46][260/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.6104e-02 (1.0084e-01)	Acc@1  98.44 ( 96.54)	Acc@5 100.00 ( 99.96)
Epoch: [46][270/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3024e-01 (1.0099e-01)	Acc@1  96.09 ( 96.53)	Acc@5  99.22 ( 99.96)
Epoch: [46][280/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.5074e-02 (1.0086e-01)	Acc@1  96.88 ( 96.53)	Acc@5 100.00 ( 99.96)
Epoch: [46][290/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0221e-01 (1.0060e-01)	Acc@1  96.09 ( 96.54)	Acc@5 100.00 ( 99.96)
Epoch: [46][300/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7055e-01 (1.0116e-01)	Acc@1  94.53 ( 96.52)	Acc@5 100.00 ( 99.96)
Epoch: [46][310/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2656e-01 (1.0153e-01)	Acc@1  94.53 ( 96.50)	Acc@5 100.00 ( 99.96)
Epoch: [46][320/391]	Time  0.040 ( 0.041)	Data  0.002 ( 0.002)	Loss 2.4691e-01 (1.0209e-01)	Acc@1  89.84 ( 96.48)	Acc@5 100.00 ( 99.96)
Epoch: [46][330/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.4944e-01 (1.0188e-01)	Acc@1  92.97 ( 96.48)	Acc@5 100.00 ( 99.96)
Epoch: [46][340/391]	Time  0.053 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.5703e-02 (1.0246e-01)	Acc@1  97.66 ( 96.45)	Acc@5 100.00 ( 99.96)
Epoch: [46][350/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.0168e-01 (1.0265e-01)	Acc@1  96.09 ( 96.45)	Acc@5 100.00 ( 99.96)
Epoch: [46][360/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.0535e-01 (1.0245e-01)	Acc@1  95.31 ( 96.45)	Acc@5 100.00 ( 99.96)
Epoch: [46][370/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.1099e-01 (1.0236e-01)	Acc@1  95.31 ( 96.44)	Acc@5 100.00 ( 99.96)
Epoch: [46][380/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.3702e-01 (1.0231e-01)	Acc@1  94.53 ( 96.43)	Acc@5 100.00 ( 99.96)
Epoch: [46][390/391]	Time  0.028 ( 0.041)	Data  0.001 ( 0.001)	Loss 8.2566e-02 (1.0258e-01)	Acc@1  97.50 ( 96.42)	Acc@5 100.00 ( 99.96)
## e[46] optimizer.zero_grad (sum) time: 0.26320505142211914
## e[46]       loss.backward (sum) time: 4.024081468582153
## e[46]      optimizer.step (sum) time: 1.7857980728149414
## epoch[46] training(only) time: 16.10957622528076
# Switched to evaluate mode...
Test: [  0/100]	Time  0.178 ( 0.178)	Loss 3.0815e-01 (3.0815e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.023 ( 0.034)	Loss 5.0621e-01 (2.9529e-01)	Acc@1  90.00 ( 90.73)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.020 ( 0.028)	Loss 4.5575e-01 (3.0419e-01)	Acc@1  88.00 ( 90.57)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.020 ( 0.025)	Loss 3.7993e-01 (3.2709e-01)	Acc@1  89.00 ( 90.61)	Acc@5 100.00 ( 99.84)
Test: [ 40/100]	Time  0.022 ( 0.024)	Loss 3.4097e-01 (3.4004e-01)	Acc@1  90.00 ( 90.24)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.020 ( 0.023)	Loss 2.6898e-01 (3.3304e-01)	Acc@1  94.00 ( 90.63)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.017 ( 0.022)	Loss 3.2545e-01 (3.3373e-01)	Acc@1  91.00 ( 90.41)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.021 ( 0.022)	Loss 4.4104e-01 (3.2865e-01)	Acc@1  86.00 ( 90.48)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.022 ( 0.022)	Loss 2.5370e-01 (3.2674e-01)	Acc@1  92.00 ( 90.54)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.021 ( 0.022)	Loss 1.9911e-01 (3.2541e-01)	Acc@1  94.00 ( 90.52)	Acc@5  99.00 ( 99.73)
 * Acc@1 90.500 Acc@5 99.730
### epoch[46] execution time: 18.396239042282104
EPOCH 47
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [47][  0/391]	Time  0.220 ( 0.220)	Data  0.174 ( 0.174)	Loss 1.4647e-01 (1.4647e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [47][ 10/391]	Time  0.042 ( 0.057)	Data  0.001 ( 0.017)	Loss 9.0201e-02 (8.8280e-02)	Acc@1  97.66 ( 97.37)	Acc@5 100.00 (100.00)
Epoch: [47][ 20/391]	Time  0.041 ( 0.049)	Data  0.001 ( 0.009)	Loss 3.1368e-02 (9.2899e-02)	Acc@1  99.22 ( 97.06)	Acc@5 100.00 ( 99.96)
Epoch: [47][ 30/391]	Time  0.038 ( 0.046)	Data  0.001 ( 0.007)	Loss 8.7152e-02 (9.5743e-02)	Acc@1  96.09 ( 96.90)	Acc@5 100.00 ( 99.97)
Epoch: [47][ 40/391]	Time  0.040 ( 0.045)	Data  0.001 ( 0.005)	Loss 8.8128e-02 (9.3701e-02)	Acc@1  97.66 ( 96.95)	Acc@5 100.00 ( 99.96)
Epoch: [47][ 50/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.004)	Loss 5.7160e-02 (9.5545e-02)	Acc@1  98.44 ( 96.84)	Acc@5 100.00 ( 99.97)
Epoch: [47][ 60/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 6.7926e-02 (9.3331e-02)	Acc@1  96.88 ( 96.91)	Acc@5 100.00 ( 99.97)
Epoch: [47][ 70/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.1460e-01 (9.4176e-02)	Acc@1  95.31 ( 96.82)	Acc@5 100.00 ( 99.97)
Epoch: [47][ 80/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.1409e-01 (9.5613e-02)	Acc@1  96.09 ( 96.80)	Acc@5 100.00 ( 99.96)
Epoch: [47][ 90/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 8.8173e-02 (9.7622e-02)	Acc@1  96.88 ( 96.74)	Acc@5 100.00 ( 99.97)
Epoch: [47][100/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.2192e-01 (9.7950e-02)	Acc@1  96.88 ( 96.70)	Acc@5 100.00 ( 99.97)
Epoch: [47][110/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 8.3078e-02 (9.7765e-02)	Acc@1  95.31 ( 96.65)	Acc@5 100.00 ( 99.96)
Epoch: [47][120/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.1863e-01 (9.8501e-02)	Acc@1  92.19 ( 96.60)	Acc@5 100.00 ( 99.96)
Epoch: [47][130/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0275e-01 (9.8172e-02)	Acc@1  95.31 ( 96.59)	Acc@5 100.00 ( 99.96)
Epoch: [47][140/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7766e-01 (9.9346e-02)	Acc@1  93.75 ( 96.55)	Acc@5 100.00 ( 99.97)
Epoch: [47][150/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.8168e-02 (9.9596e-02)	Acc@1  96.09 ( 96.50)	Acc@5 100.00 ( 99.97)
Epoch: [47][160/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.3210e-02 (9.9482e-02)	Acc@1  97.66 ( 96.51)	Acc@5 100.00 ( 99.97)
Epoch: [47][170/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.9176e-02 (1.0020e-01)	Acc@1  98.44 ( 96.48)	Acc@5 100.00 ( 99.97)
Epoch: [47][180/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.2394e-02 (1.0035e-01)	Acc@1  95.31 ( 96.46)	Acc@5 100.00 ( 99.97)
Epoch: [47][190/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6984e-01 (1.0096e-01)	Acc@1  96.88 ( 96.44)	Acc@5 100.00 ( 99.98)
Epoch: [47][200/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.8988e-02 (1.0055e-01)	Acc@1  96.88 ( 96.46)	Acc@5 100.00 ( 99.97)
Epoch: [47][210/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.1954e-02 (1.0063e-01)	Acc@1  96.88 ( 96.48)	Acc@5 100.00 ( 99.97)
Epoch: [47][220/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4803e-01 (1.0068e-01)	Acc@1  98.44 ( 96.50)	Acc@5  99.22 ( 99.96)
Epoch: [47][230/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.5265e-02 (1.0079e-01)	Acc@1  96.09 ( 96.50)	Acc@5 100.00 ( 99.97)
Epoch: [47][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.5884e-02 (1.0065e-01)	Acc@1  97.66 ( 96.50)	Acc@5 100.00 ( 99.97)
Epoch: [47][250/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3304e-01 (1.0099e-01)	Acc@1  96.09 ( 96.49)	Acc@5 100.00 ( 99.97)
Epoch: [47][260/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0580e-01 (1.0057e-01)	Acc@1  96.09 ( 96.50)	Acc@5 100.00 ( 99.97)
Epoch: [47][270/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4753e-01 (1.0106e-01)	Acc@1  93.75 ( 96.49)	Acc@5 100.00 ( 99.97)
Epoch: [47][280/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4191e-02 (1.0064e-01)	Acc@1 100.00 ( 96.49)	Acc@5 100.00 ( 99.97)
Epoch: [47][290/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0781e-01 (1.0152e-01)	Acc@1  96.88 ( 96.45)	Acc@5 100.00 ( 99.97)
Epoch: [47][300/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9530e-02 (1.0168e-01)	Acc@1  97.66 ( 96.45)	Acc@5 100.00 ( 99.97)
Epoch: [47][310/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1567e-01 (1.0095e-01)	Acc@1  94.53 ( 96.47)	Acc@5 100.00 ( 99.97)
Epoch: [47][320/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.7193e-01 (1.0159e-01)	Acc@1  93.75 ( 96.43)	Acc@5  99.22 ( 99.97)
Epoch: [47][330/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.4603e-01 (1.0243e-01)	Acc@1  93.75 ( 96.39)	Acc@5 100.00 ( 99.97)
Epoch: [47][340/391]	Time  0.040 ( 0.041)	Data  0.002 ( 0.001)	Loss 1.0534e-01 (1.0235e-01)	Acc@1  96.09 ( 96.40)	Acc@5 100.00 ( 99.97)
Epoch: [47][350/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.1451e-01 (1.0195e-01)	Acc@1  95.31 ( 96.41)	Acc@5 100.00 ( 99.97)
Epoch: [47][360/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.1150e-01 (1.0226e-01)	Acc@1  95.31 ( 96.39)	Acc@5 100.00 ( 99.97)
Epoch: [47][370/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.5488e-02 (1.0187e-01)	Acc@1  98.44 ( 96.41)	Acc@5 100.00 ( 99.97)
Epoch: [47][380/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 8.1304e-02 (1.0205e-01)	Acc@1  97.66 ( 96.40)	Acc@5 100.00 ( 99.97)
Epoch: [47][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 8.9183e-02 (1.0219e-01)	Acc@1  97.50 ( 96.38)	Acc@5 100.00 ( 99.97)
## e[47] optimizer.zero_grad (sum) time: 0.26358580589294434
## e[47]       loss.backward (sum) time: 3.997086763381958
## e[47]      optimizer.step (sum) time: 1.8183329105377197
## epoch[47] training(only) time: 15.956886529922485
# Switched to evaluate mode...
Test: [  0/100]	Time  0.223 ( 0.223)	Loss 3.8901e-01 (3.8901e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.018 ( 0.038)	Loss 3.9834e-01 (3.2093e-01)	Acc@1  87.00 ( 89.73)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.018 ( 0.029)	Loss 4.5706e-01 (3.3474e-01)	Acc@1  86.00 ( 89.43)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.016 ( 0.026)	Loss 4.0668e-01 (3.4327e-01)	Acc@1  88.00 ( 89.74)	Acc@5  99.00 ( 99.81)
Test: [ 40/100]	Time  0.024 ( 0.025)	Loss 3.5280e-01 (3.5269e-01)	Acc@1  92.00 ( 89.76)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.022 ( 0.024)	Loss 3.4698e-01 (3.4674e-01)	Acc@1  91.00 ( 90.12)	Acc@5 100.00 ( 99.69)
Test: [ 60/100]	Time  0.021 ( 0.024)	Loss 3.7396e-01 (3.5159e-01)	Acc@1  88.00 ( 89.97)	Acc@5 100.00 ( 99.70)
Test: [ 70/100]	Time  0.024 ( 0.024)	Loss 4.4272e-01 (3.4306e-01)	Acc@1  86.00 ( 90.03)	Acc@5 100.00 ( 99.70)
Test: [ 80/100]	Time  0.018 ( 0.023)	Loss 2.4622e-01 (3.4091e-01)	Acc@1  93.00 ( 90.11)	Acc@5 100.00 ( 99.69)
Test: [ 90/100]	Time  0.018 ( 0.023)	Loss 2.0301e-01 (3.3947e-01)	Acc@1  94.00 ( 90.12)	Acc@5 100.00 ( 99.71)
 * Acc@1 90.240 Acc@5 99.730
### epoch[47] execution time: 18.32179021835327
EPOCH 48
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [48][  0/391]	Time  0.257 ( 0.257)	Data  0.214 ( 0.214)	Loss 6.8893e-02 (6.8893e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [48][ 10/391]	Time  0.041 ( 0.060)	Data  0.001 ( 0.020)	Loss 4.2182e-02 (8.2602e-02)	Acc@1  98.44 ( 97.30)	Acc@5 100.00 (100.00)
Epoch: [48][ 20/391]	Time  0.042 ( 0.051)	Data  0.001 ( 0.011)	Loss 1.0318e-01 (9.1374e-02)	Acc@1  96.09 ( 96.69)	Acc@5 100.00 ( 99.96)
Epoch: [48][ 30/391]	Time  0.041 ( 0.048)	Data  0.001 ( 0.008)	Loss 7.1416e-02 (9.0242e-02)	Acc@1  96.88 ( 96.72)	Acc@5 100.00 ( 99.97)
Epoch: [48][ 40/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.006)	Loss 1.4883e-01 (9.2372e-02)	Acc@1  96.09 ( 96.70)	Acc@5 100.00 ( 99.98)
Epoch: [48][ 50/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.005)	Loss 1.4582e-01 (9.3776e-02)	Acc@1  95.31 ( 96.71)	Acc@5 100.00 ( 99.98)
Epoch: [48][ 60/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.004)	Loss 9.4755e-02 (9.4867e-02)	Acc@1  94.53 ( 96.62)	Acc@5 100.00 ( 99.99)
Epoch: [48][ 70/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.1799e-01 (9.2541e-02)	Acc@1  96.88 ( 96.75)	Acc@5 100.00 ( 99.99)
Epoch: [48][ 80/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.004)	Loss 9.8944e-02 (9.2664e-02)	Acc@1  97.66 ( 96.78)	Acc@5 100.00 ( 99.98)
Epoch: [48][ 90/391]	Time  0.037 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.1377e-02 (9.1051e-02)	Acc@1  99.22 ( 96.88)	Acc@5 100.00 ( 99.98)
Epoch: [48][100/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.003)	Loss 8.4358e-02 (9.0863e-02)	Acc@1  96.09 ( 96.88)	Acc@5 100.00 ( 99.98)
Epoch: [48][110/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.2164e-01 (9.2514e-02)	Acc@1  96.88 ( 96.87)	Acc@5 100.00 ( 99.99)
Epoch: [48][120/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 9.1238e-02 (9.2826e-02)	Acc@1  96.88 ( 96.86)	Acc@5 100.00 ( 99.99)
Epoch: [48][130/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.003)	Loss 1.1682e-01 (9.3045e-02)	Acc@1  96.09 ( 96.88)	Acc@5 100.00 ( 99.99)
Epoch: [48][140/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.003)	Loss 1.2219e-01 (9.3822e-02)	Acc@1  94.53 ( 96.83)	Acc@5 100.00 ( 99.99)
Epoch: [48][150/391]	Time  0.039 ( 0.041)	Data  0.002 ( 0.002)	Loss 1.6317e-01 (9.4707e-02)	Acc@1  94.53 ( 96.80)	Acc@5  99.22 ( 99.98)
Epoch: [48][160/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.6606e-02 (9.4424e-02)	Acc@1  96.88 ( 96.77)	Acc@5 100.00 ( 99.99)
Epoch: [48][170/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4699e-01 (9.4505e-02)	Acc@1  93.75 ( 96.73)	Acc@5 100.00 ( 99.99)
Epoch: [48][180/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.8293e-02 (9.5126e-02)	Acc@1  97.66 ( 96.68)	Acc@5 100.00 ( 99.98)
Epoch: [48][190/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.8569e-02 (9.4756e-02)	Acc@1  97.66 ( 96.70)	Acc@5 100.00 ( 99.98)
Epoch: [48][200/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.8188e-02 (9.5164e-02)	Acc@1  96.09 ( 96.66)	Acc@5 100.00 ( 99.98)
Epoch: [48][210/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.4505e-02 (9.5628e-02)	Acc@1  97.66 ( 96.65)	Acc@5 100.00 ( 99.98)
Epoch: [48][220/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.1144e-02 (9.5569e-02)	Acc@1  98.44 ( 96.66)	Acc@5 100.00 ( 99.98)
Epoch: [48][230/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.1295e-01 (9.5486e-02)	Acc@1  96.09 ( 96.65)	Acc@5 100.00 ( 99.98)
Epoch: [48][240/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.1405e-01 (9.5220e-02)	Acc@1  96.88 ( 96.64)	Acc@5 100.00 ( 99.98)
Epoch: [48][250/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.002)	Loss 3.8215e-02 (9.5024e-02)	Acc@1  98.44 ( 96.66)	Acc@5 100.00 ( 99.98)
Epoch: [48][260/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.2095e-01 (9.5573e-02)	Acc@1  96.88 ( 96.63)	Acc@5 100.00 ( 99.98)
Epoch: [48][270/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.5649e-01 (9.5898e-02)	Acc@1  93.75 ( 96.61)	Acc@5 100.00 ( 99.98)
Epoch: [48][280/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.4556e-01 (9.6544e-02)	Acc@1  92.97 ( 96.57)	Acc@5 100.00 ( 99.98)
Epoch: [48][290/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 9.0670e-02 (9.6527e-02)	Acc@1  95.31 ( 96.58)	Acc@5 100.00 ( 99.98)
Epoch: [48][300/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 8.1908e-02 (9.6353e-02)	Acc@1  96.88 ( 96.59)	Acc@5 100.00 ( 99.98)
Epoch: [48][310/391]	Time  0.038 ( 0.040)	Data  0.000 ( 0.002)	Loss 5.1769e-02 (9.6243e-02)	Acc@1  98.44 ( 96.59)	Acc@5 100.00 ( 99.98)
Epoch: [48][320/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.2854e-01 (9.6567e-02)	Acc@1  95.31 ( 96.56)	Acc@5 100.00 ( 99.98)
Epoch: [48][330/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.1155e-01 (9.6765e-02)	Acc@1  96.09 ( 96.55)	Acc@5 100.00 ( 99.98)
Epoch: [48][340/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 4.5630e-02 (9.7415e-02)	Acc@1  97.66 ( 96.52)	Acc@5 100.00 ( 99.98)
Epoch: [48][350/391]	Time  0.046 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.2407e-01 (9.7280e-02)	Acc@1  95.31 ( 96.53)	Acc@5 100.00 ( 99.98)
Epoch: [48][360/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.5841e-01 (9.7585e-02)	Acc@1  95.31 ( 96.53)	Acc@5 100.00 ( 99.98)
Epoch: [48][370/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 7.1162e-02 (9.7662e-02)	Acc@1  97.66 ( 96.53)	Acc@5 100.00 ( 99.98)
Epoch: [48][380/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.4045e-01 (9.7773e-02)	Acc@1  95.31 ( 96.53)	Acc@5 100.00 ( 99.98)
Epoch: [48][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.1545e-01 (9.7938e-02)	Acc@1  95.00 ( 96.52)	Acc@5 100.00 ( 99.98)
## e[48] optimizer.zero_grad (sum) time: 0.2633705139160156
## e[48]       loss.backward (sum) time: 3.868025064468384
## e[48]      optimizer.step (sum) time: 1.8825428485870361
## epoch[48] training(only) time: 15.86171817779541
# Switched to evaluate mode...
Test: [  0/100]	Time  0.176 ( 0.176)	Loss 3.3965e-01 (3.3965e-01)	Acc@1  93.00 ( 93.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.023 ( 0.034)	Loss 4.4596e-01 (3.2018e-01)	Acc@1  88.00 ( 91.09)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.024 ( 0.027)	Loss 4.6017e-01 (3.2314e-01)	Acc@1  87.00 ( 90.81)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.021 ( 0.025)	Loss 4.0634e-01 (3.3865e-01)	Acc@1  89.00 ( 90.74)	Acc@5  99.00 ( 99.68)
Test: [ 40/100]	Time  0.021 ( 0.024)	Loss 3.3370e-01 (3.4430e-01)	Acc@1  91.00 ( 90.41)	Acc@5  99.00 ( 99.66)
Test: [ 50/100]	Time  0.018 ( 0.024)	Loss 2.5333e-01 (3.3587e-01)	Acc@1  96.00 ( 90.71)	Acc@5 100.00 ( 99.67)
Test: [ 60/100]	Time  0.020 ( 0.023)	Loss 3.5433e-01 (3.4069e-01)	Acc@1  92.00 ( 90.51)	Acc@5  99.00 ( 99.69)
Test: [ 70/100]	Time  0.023 ( 0.022)	Loss 4.6811e-01 (3.3221e-01)	Acc@1  87.00 ( 90.42)	Acc@5 100.00 ( 99.70)
Test: [ 80/100]	Time  0.019 ( 0.022)	Loss 1.9980e-01 (3.2968e-01)	Acc@1  95.00 ( 90.51)	Acc@5 100.00 ( 99.69)
Test: [ 90/100]	Time  0.019 ( 0.022)	Loss 2.5427e-01 (3.3037e-01)	Acc@1  93.00 ( 90.46)	Acc@5 100.00 ( 99.71)
 * Acc@1 90.540 Acc@5 99.720
### epoch[48] execution time: 18.145846128463745
EPOCH 49
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [49][  0/391]	Time  0.218 ( 0.218)	Data  0.175 ( 0.175)	Loss 8.5845e-02 (8.5845e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [49][ 10/391]	Time  0.042 ( 0.056)	Data  0.001 ( 0.017)	Loss 7.9871e-02 (1.0440e-01)	Acc@1  98.44 ( 96.59)	Acc@5 100.00 (100.00)
Epoch: [49][ 20/391]	Time  0.039 ( 0.048)	Data  0.001 ( 0.009)	Loss 7.8746e-02 (9.8519e-02)	Acc@1  97.66 ( 96.61)	Acc@5 100.00 (100.00)
Epoch: [49][ 30/391]	Time  0.042 ( 0.046)	Data  0.001 ( 0.007)	Loss 7.3972e-02 (9.4673e-02)	Acc@1  96.88 ( 96.67)	Acc@5 100.00 ( 99.97)
Epoch: [49][ 40/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.005)	Loss 7.6609e-02 (9.1543e-02)	Acc@1  97.66 ( 96.84)	Acc@5 100.00 ( 99.98)
Epoch: [49][ 50/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.6231e-01 (9.4544e-02)	Acc@1  95.31 ( 96.75)	Acc@5  99.22 ( 99.97)
Epoch: [49][ 60/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.0168e-01 (9.0874e-02)	Acc@1  95.31 ( 96.86)	Acc@5 100.00 ( 99.97)
Epoch: [49][ 70/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 7.2875e-02 (8.8484e-02)	Acc@1  98.44 ( 96.96)	Acc@5 100.00 ( 99.98)
Epoch: [49][ 80/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.0010e-01 (8.8164e-02)	Acc@1  96.09 ( 96.93)	Acc@5 100.00 ( 99.97)
Epoch: [49][ 90/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 9.8019e-02 (8.7654e-02)	Acc@1  96.88 ( 96.94)	Acc@5  99.22 ( 99.97)
Epoch: [49][100/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.0006e-02 (8.6885e-02)	Acc@1  97.66 ( 96.95)	Acc@5 100.00 ( 99.97)
Epoch: [49][110/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.2889e-02 (8.7906e-02)	Acc@1  98.44 ( 96.88)	Acc@5 100.00 ( 99.97)
Epoch: [49][120/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.7014e-01 (8.9179e-02)	Acc@1  95.31 ( 96.86)	Acc@5  99.22 ( 99.97)
Epoch: [49][130/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.9902e-02 (9.0887e-02)	Acc@1  96.88 ( 96.79)	Acc@5 100.00 ( 99.96)
Epoch: [49][140/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0612e-01 (9.1437e-02)	Acc@1  95.31 ( 96.76)	Acc@5 100.00 ( 99.96)
Epoch: [49][150/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0703e-01 (9.1321e-02)	Acc@1  97.66 ( 96.79)	Acc@5 100.00 ( 99.96)
Epoch: [49][160/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.1246e-02 (9.1298e-02)	Acc@1  96.88 ( 96.79)	Acc@5 100.00 ( 99.97)
Epoch: [49][170/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4513e-01 (9.1932e-02)	Acc@1  96.88 ( 96.79)	Acc@5 100.00 ( 99.97)
Epoch: [49][180/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0263e-01 (9.2519e-02)	Acc@1  95.31 ( 96.78)	Acc@5 100.00 ( 99.97)
Epoch: [49][190/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1017e-01 (9.3083e-02)	Acc@1  95.31 ( 96.72)	Acc@5 100.00 ( 99.97)
Epoch: [49][200/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6398e-01 (9.3801e-02)	Acc@1  94.53 ( 96.72)	Acc@5 100.00 ( 99.97)
Epoch: [49][210/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.3633e-02 (9.3154e-02)	Acc@1  98.44 ( 96.73)	Acc@5 100.00 ( 99.97)
Epoch: [49][220/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4112e-01 (9.3092e-02)	Acc@1  95.31 ( 96.72)	Acc@5 100.00 ( 99.97)
Epoch: [49][230/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1299e-01 (9.3557e-02)	Acc@1  96.09 ( 96.72)	Acc@5 100.00 ( 99.97)
Epoch: [49][240/391]	Time  0.041 ( 0.041)	Data  0.002 ( 0.002)	Loss 5.7504e-02 (9.3099e-02)	Acc@1  97.66 ( 96.74)	Acc@5 100.00 ( 99.97)
Epoch: [49][250/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3760e-01 (9.3146e-02)	Acc@1  94.53 ( 96.75)	Acc@5 100.00 ( 99.97)
Epoch: [49][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.6993e-02 (9.3371e-02)	Acc@1  96.88 ( 96.73)	Acc@5 100.00 ( 99.97)
Epoch: [49][270/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1188e-01 (9.3530e-02)	Acc@1  95.31 ( 96.72)	Acc@5 100.00 ( 99.97)
Epoch: [49][280/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 8.6469e-02 (9.2570e-02)	Acc@1  97.66 ( 96.76)	Acc@5 100.00 ( 99.97)
Epoch: [49][290/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.3441e-01 (9.2617e-02)	Acc@1  96.09 ( 96.75)	Acc@5 100.00 ( 99.97)
Epoch: [49][300/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 7.4364e-02 (9.2849e-02)	Acc@1  96.88 ( 96.75)	Acc@5 100.00 ( 99.97)
Epoch: [49][310/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 6.4890e-02 (9.3494e-02)	Acc@1  98.44 ( 96.74)	Acc@5 100.00 ( 99.97)
Epoch: [49][320/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 7.6475e-02 (9.3474e-02)	Acc@1  97.66 ( 96.74)	Acc@5 100.00 ( 99.97)
Epoch: [49][330/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 8.4266e-02 (9.4018e-02)	Acc@1  98.44 ( 96.74)	Acc@5 100.00 ( 99.97)
Epoch: [49][340/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.3981e-01 (9.4112e-02)	Acc@1  96.09 ( 96.74)	Acc@5  99.22 ( 99.97)
Epoch: [49][350/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.0195e-01 (9.4491e-02)	Acc@1  96.09 ( 96.73)	Acc@5 100.00 ( 99.97)
Epoch: [49][360/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.5312e-02 (9.4072e-02)	Acc@1  96.88 ( 96.74)	Acc@5 100.00 ( 99.97)
Epoch: [49][370/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.6192e-02 (9.4057e-02)	Acc@1  97.66 ( 96.74)	Acc@5 100.00 ( 99.97)
Epoch: [49][380/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.4635e-01 (9.4264e-02)	Acc@1  93.75 ( 96.73)	Acc@5 100.00 ( 99.97)
Epoch: [49][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.5367e-02 (9.4136e-02)	Acc@1  97.50 ( 96.73)	Acc@5 100.00 ( 99.97)
## e[49] optimizer.zero_grad (sum) time: 0.26244664192199707
## e[49]       loss.backward (sum) time: 3.942542552947998
## e[49]      optimizer.step (sum) time: 1.8623275756835938
## epoch[49] training(only) time: 15.893303871154785
# Switched to evaluate mode...
Test: [  0/100]	Time  0.177 ( 0.177)	Loss 3.3275e-01 (3.3275e-01)	Acc@1  91.00 ( 91.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.025 ( 0.037)	Loss 3.2985e-01 (2.9714e-01)	Acc@1  91.00 ( 91.55)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.018 ( 0.029)	Loss 4.7474e-01 (3.1194e-01)	Acc@1  88.00 ( 90.95)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.018 ( 0.026)	Loss 4.4395e-01 (3.3284e-01)	Acc@1  87.00 ( 90.77)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.024 ( 0.025)	Loss 3.8443e-01 (3.4957e-01)	Acc@1  86.00 ( 90.39)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.018 ( 0.024)	Loss 2.9301e-01 (3.4095e-01)	Acc@1  94.00 ( 90.45)	Acc@5 100.00 ( 99.69)
Test: [ 60/100]	Time  0.021 ( 0.024)	Loss 4.1361e-01 (3.4339e-01)	Acc@1  89.00 ( 90.25)	Acc@5 100.00 ( 99.69)
Test: [ 70/100]	Time  0.022 ( 0.023)	Loss 3.6015e-01 (3.3538e-01)	Acc@1  88.00 ( 90.31)	Acc@5 100.00 ( 99.70)
Test: [ 80/100]	Time  0.021 ( 0.023)	Loss 2.3524e-01 (3.3452e-01)	Acc@1  94.00 ( 90.32)	Acc@5 100.00 ( 99.69)
Test: [ 90/100]	Time  0.021 ( 0.023)	Loss 2.6940e-01 (3.3470e-01)	Acc@1  93.00 ( 90.33)	Acc@5 100.00 ( 99.70)
 * Acc@1 90.380 Acc@5 99.720
### epoch[49] execution time: 18.24827480316162
EPOCH 50
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [50][  0/391]	Time  0.226 ( 0.226)	Data  0.185 ( 0.185)	Loss 9.8266e-02 (9.8266e-02)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [50][ 10/391]	Time  0.042 ( 0.058)	Data  0.001 ( 0.018)	Loss 9.9052e-02 (8.8211e-02)	Acc@1  95.31 ( 96.66)	Acc@5 100.00 (100.00)
Epoch: [50][ 20/391]	Time  0.038 ( 0.050)	Data  0.001 ( 0.010)	Loss 1.1663e-01 (9.2453e-02)	Acc@1  93.75 ( 96.61)	Acc@5 100.00 ( 99.96)
Epoch: [50][ 30/391]	Time  0.039 ( 0.047)	Data  0.001 ( 0.007)	Loss 1.1870e-01 (8.9977e-02)	Acc@1  93.75 ( 96.70)	Acc@5 100.00 ( 99.97)
Epoch: [50][ 40/391]	Time  0.039 ( 0.045)	Data  0.001 ( 0.005)	Loss 4.5902e-02 (8.8648e-02)	Acc@1  99.22 ( 96.93)	Acc@5 100.00 ( 99.98)
Epoch: [50][ 50/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.005)	Loss 1.1033e-01 (9.0555e-02)	Acc@1  96.09 ( 96.88)	Acc@5  99.22 ( 99.95)
Epoch: [50][ 60/391]	Time  0.043 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.0814e-01 (9.1305e-02)	Acc@1  98.44 ( 96.84)	Acc@5 100.00 ( 99.96)
Epoch: [50][ 70/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.004)	Loss 4.9180e-02 (9.0668e-02)	Acc@1  99.22 ( 96.83)	Acc@5 100.00 ( 99.97)
Epoch: [50][ 80/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.0413e-02 (9.2098e-02)	Acc@1  98.44 ( 96.85)	Acc@5 100.00 ( 99.96)
Epoch: [50][ 90/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.7620e-02 (9.4289e-02)	Acc@1 100.00 ( 96.80)	Acc@5 100.00 ( 99.96)
Epoch: [50][100/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.1123e-02 (9.3347e-02)	Acc@1  96.88 ( 96.73)	Acc@5 100.00 ( 99.96)
Epoch: [50][110/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.1491e-01 (9.5148e-02)	Acc@1  96.09 ( 96.66)	Acc@5 100.00 ( 99.96)
Epoch: [50][120/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 8.5885e-02 (9.4929e-02)	Acc@1  96.88 ( 96.64)	Acc@5 100.00 ( 99.97)
Epoch: [50][130/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.4499e-02 (9.4041e-02)	Acc@1  98.44 ( 96.70)	Acc@5 100.00 ( 99.96)
Epoch: [50][140/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8525e-02 (9.2944e-02)	Acc@1  99.22 ( 96.73)	Acc@5 100.00 ( 99.97)
Epoch: [50][150/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.7493e-02 (9.3426e-02)	Acc@1  96.88 ( 96.73)	Acc@5 100.00 ( 99.96)
Epoch: [50][160/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3100e-01 (9.3410e-02)	Acc@1  94.53 ( 96.74)	Acc@5 100.00 ( 99.97)
Epoch: [50][170/391]	Time  0.041 ( 0.041)	Data  0.002 ( 0.002)	Loss 5.2765e-02 (9.2648e-02)	Acc@1  98.44 ( 96.77)	Acc@5 100.00 ( 99.97)
Epoch: [50][180/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.3584e-02 (9.3229e-02)	Acc@1  96.09 ( 96.72)	Acc@5 100.00 ( 99.97)
Epoch: [50][190/391]	Time  0.045 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2791e-01 (9.3709e-02)	Acc@1  95.31 ( 96.70)	Acc@5 100.00 ( 99.97)
Epoch: [50][200/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.9263e-02 (9.3870e-02)	Acc@1  97.66 ( 96.68)	Acc@5 100.00 ( 99.97)
Epoch: [50][210/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7041e-02 (9.4206e-02)	Acc@1  99.22 ( 96.65)	Acc@5 100.00 ( 99.97)
Epoch: [50][220/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.7793e-02 (9.4352e-02)	Acc@1  96.88 ( 96.65)	Acc@5 100.00 ( 99.98)
Epoch: [50][230/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5589e-01 (9.5479e-02)	Acc@1  94.53 ( 96.61)	Acc@5 100.00 ( 99.98)
Epoch: [50][240/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.4093e-02 (9.5893e-02)	Acc@1  97.66 ( 96.62)	Acc@5 100.00 ( 99.97)
Epoch: [50][250/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.9301e-02 (9.6145e-02)	Acc@1  96.09 ( 96.61)	Acc@5 100.00 ( 99.98)
Epoch: [50][260/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.6391e-02 (9.6524e-02)	Acc@1  97.66 ( 96.59)	Acc@5 100.00 ( 99.98)
Epoch: [50][270/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.5723e-02 (9.6590e-02)	Acc@1  96.88 ( 96.59)	Acc@5 100.00 ( 99.98)
Epoch: [50][280/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5004e-01 (9.5968e-02)	Acc@1  93.75 ( 96.60)	Acc@5 100.00 ( 99.98)
Epoch: [50][290/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.2511e-02 (9.6044e-02)	Acc@1  97.66 ( 96.59)	Acc@5 100.00 ( 99.98)
Epoch: [50][300/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.8715e-02 (9.5476e-02)	Acc@1  96.88 ( 96.63)	Acc@5 100.00 ( 99.98)
Epoch: [50][310/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1311e-01 (9.4757e-02)	Acc@1  94.53 ( 96.65)	Acc@5 100.00 ( 99.98)
Epoch: [50][320/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.2946e-02 (9.4229e-02)	Acc@1  97.66 ( 96.67)	Acc@5 100.00 ( 99.98)
Epoch: [50][330/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2756e-02 (9.4059e-02)	Acc@1  99.22 ( 96.68)	Acc@5 100.00 ( 99.98)
Epoch: [50][340/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1124e-01 (9.4189e-02)	Acc@1  94.53 ( 96.68)	Acc@5 100.00 ( 99.98)
Epoch: [50][350/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1523e-01 (9.3969e-02)	Acc@1  95.31 ( 96.68)	Acc@5 100.00 ( 99.98)
Epoch: [50][360/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3261e-01 (9.4222e-02)	Acc@1  97.66 ( 96.68)	Acc@5  99.22 ( 99.98)
Epoch: [50][370/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.0830e-02 (9.4573e-02)	Acc@1  95.31 ( 96.67)	Acc@5 100.00 ( 99.98)
Epoch: [50][380/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.5116e-02 (9.4322e-02)	Acc@1  99.22 ( 96.68)	Acc@5 100.00 ( 99.98)
Epoch: [50][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.2772e-01 (9.4415e-02)	Acc@1  95.00 ( 96.67)	Acc@5 100.00 ( 99.98)
## e[50] optimizer.zero_grad (sum) time: 0.26224184036254883
## e[50]       loss.backward (sum) time: 3.9443602561950684
## e[50]      optimizer.step (sum) time: 1.8542420864105225
## epoch[50] training(only) time: 15.946183681488037
# Switched to evaluate mode...
Test: [  0/100]	Time  0.173 ( 0.173)	Loss 3.0497e-01 (3.0497e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.021 ( 0.034)	Loss 3.9290e-01 (3.0418e-01)	Acc@1  90.00 ( 90.55)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.017 ( 0.028)	Loss 6.1200e-01 (3.2300e-01)	Acc@1  85.00 ( 90.57)	Acc@5  99.00 ( 99.81)
Test: [ 30/100]	Time  0.024 ( 0.025)	Loss 4.5878e-01 (3.4697e-01)	Acc@1  88.00 ( 90.52)	Acc@5 100.00 ( 99.77)
Test: [ 40/100]	Time  0.018 ( 0.024)	Loss 3.8586e-01 (3.5800e-01)	Acc@1  88.00 ( 90.17)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.018 ( 0.023)	Loss 2.7127e-01 (3.5324e-01)	Acc@1  95.00 ( 90.47)	Acc@5 100.00 ( 99.69)
Test: [ 60/100]	Time  0.019 ( 0.022)	Loss 3.6484e-01 (3.5685e-01)	Acc@1  90.00 ( 90.11)	Acc@5  99.00 ( 99.69)
Test: [ 70/100]	Time  0.017 ( 0.022)	Loss 3.9595e-01 (3.4564e-01)	Acc@1  87.00 ( 90.21)	Acc@5 100.00 ( 99.70)
Test: [ 80/100]	Time  0.022 ( 0.022)	Loss 2.1350e-01 (3.4099e-01)	Acc@1  94.00 ( 90.31)	Acc@5 100.00 ( 99.72)
Test: [ 90/100]	Time  0.024 ( 0.022)	Loss 2.0459e-01 (3.4142e-01)	Acc@1  93.00 ( 90.32)	Acc@5 100.00 ( 99.73)
 * Acc@1 90.430 Acc@5 99.730
### epoch[50] execution time: 18.218143463134766
EPOCH 51
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [51][  0/391]	Time  0.210 ( 0.210)	Data  0.166 ( 0.166)	Loss 1.2675e-01 (1.2675e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [51][ 10/391]	Time  0.041 ( 0.055)	Data  0.001 ( 0.016)	Loss 3.5033e-02 (9.8566e-02)	Acc@1  99.22 ( 96.24)	Acc@5 100.00 (100.00)
Epoch: [51][ 20/391]	Time  0.042 ( 0.048)	Data  0.001 ( 0.009)	Loss 7.6484e-02 (8.7614e-02)	Acc@1  96.09 ( 96.61)	Acc@5 100.00 (100.00)
Epoch: [51][ 30/391]	Time  0.042 ( 0.046)	Data  0.001 ( 0.006)	Loss 6.3033e-02 (8.7125e-02)	Acc@1  98.44 ( 96.70)	Acc@5 100.00 (100.00)
Epoch: [51][ 40/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.005)	Loss 1.6551e-01 (9.0054e-02)	Acc@1  95.31 ( 96.72)	Acc@5 100.00 (100.00)
Epoch: [51][ 50/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.004)	Loss 4.9880e-02 (8.6792e-02)	Acc@1  98.44 ( 96.86)	Acc@5 100.00 ( 99.98)
Epoch: [51][ 60/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 6.5543e-02 (8.7553e-02)	Acc@1  98.44 ( 96.86)	Acc@5 100.00 ( 99.99)
Epoch: [51][ 70/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.2660e-01 (8.9407e-02)	Acc@1  93.75 ( 96.82)	Acc@5 100.00 ( 99.98)
Epoch: [51][ 80/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.2662e-02 (8.8660e-02)	Acc@1  98.44 ( 96.84)	Acc@5 100.00 ( 99.98)
Epoch: [51][ 90/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.6096e-01 (8.6792e-02)	Acc@1  96.88 ( 96.92)	Acc@5  99.22 ( 99.97)
Epoch: [51][100/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.4259e-02 (8.7674e-02)	Acc@1  99.22 ( 96.91)	Acc@5 100.00 ( 99.98)
Epoch: [51][110/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.4131e-01 (8.8871e-02)	Acc@1  94.53 ( 96.88)	Acc@5 100.00 ( 99.98)
Epoch: [51][120/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.0271e-02 (8.9639e-02)	Acc@1  97.66 ( 96.84)	Acc@5 100.00 ( 99.98)
Epoch: [51][130/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0720e-01 (9.0362e-02)	Acc@1  96.88 ( 96.77)	Acc@5 100.00 ( 99.98)
Epoch: [51][140/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.9252e-01 (9.0834e-02)	Acc@1  92.97 ( 96.75)	Acc@5 100.00 ( 99.98)
Epoch: [51][150/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.5081e-02 (9.1420e-02)	Acc@1  97.66 ( 96.72)	Acc@5 100.00 ( 99.98)
Epoch: [51][160/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1771e-01 (9.0779e-02)	Acc@1  96.09 ( 96.75)	Acc@5 100.00 ( 99.99)
Epoch: [51][170/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.6872e-02 (8.9831e-02)	Acc@1  96.88 ( 96.81)	Acc@5 100.00 ( 99.99)
Epoch: [51][180/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.5679e-02 (9.0782e-02)	Acc@1  98.44 ( 96.73)	Acc@5 100.00 ( 99.99)
Epoch: [51][190/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.0008e-02 (9.0374e-02)	Acc@1  97.66 ( 96.76)	Acc@5 100.00 ( 99.99)
Epoch: [51][200/391]	Time  0.035 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.7472e-02 (9.0291e-02)	Acc@1  96.88 ( 96.75)	Acc@5 100.00 ( 99.99)
Epoch: [51][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.6355e-02 (9.0592e-02)	Acc@1  95.31 ( 96.74)	Acc@5 100.00 ( 99.99)
Epoch: [51][220/391]	Time  0.053 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.4382e-02 (9.1082e-02)	Acc@1  96.88 ( 96.73)	Acc@5 100.00 ( 99.99)
Epoch: [51][230/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.7160e-02 (9.1265e-02)	Acc@1  97.66 ( 96.72)	Acc@5 100.00 ( 99.99)
Epoch: [51][240/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1234e-01 (9.1083e-02)	Acc@1  96.09 ( 96.74)	Acc@5 100.00 ( 99.99)
Epoch: [51][250/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3993e-01 (9.1183e-02)	Acc@1  93.75 ( 96.73)	Acc@5 100.00 ( 99.98)
Epoch: [51][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.7243e-02 (9.1715e-02)	Acc@1  96.88 ( 96.72)	Acc@5 100.00 ( 99.99)
Epoch: [51][270/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0310e-01 (9.2035e-02)	Acc@1  97.66 ( 96.72)	Acc@5 100.00 ( 99.98)
Epoch: [51][280/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.8084e-01 (9.2476e-02)	Acc@1  93.75 ( 96.71)	Acc@5  99.22 ( 99.98)
Epoch: [51][290/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.7275e-02 (9.2635e-02)	Acc@1  98.44 ( 96.70)	Acc@5 100.00 ( 99.98)
Epoch: [51][300/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.8765e-02 (9.2744e-02)	Acc@1  97.66 ( 96.71)	Acc@5 100.00 ( 99.98)
Epoch: [51][310/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.7263e-02 (9.2239e-02)	Acc@1  98.44 ( 96.75)	Acc@5 100.00 ( 99.98)
Epoch: [51][320/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.6623e-01 (9.2590e-02)	Acc@1  95.31 ( 96.75)	Acc@5  99.22 ( 99.98)
Epoch: [51][330/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.2950e-01 (9.2339e-02)	Acc@1  96.88 ( 96.76)	Acc@5 100.00 ( 99.98)
Epoch: [51][340/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 9.2293e-02 (9.3517e-02)	Acc@1  96.88 ( 96.69)	Acc@5  99.22 ( 99.97)
Epoch: [51][350/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.0723e-01 (9.3850e-02)	Acc@1  96.88 ( 96.67)	Acc@5  98.44 ( 99.97)
Epoch: [51][360/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.1269e-02 (9.3378e-02)	Acc@1  98.44 ( 96.68)	Acc@5 100.00 ( 99.97)
Epoch: [51][370/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.2067e-02 (9.3389e-02)	Acc@1  97.66 ( 96.67)	Acc@5 100.00 ( 99.96)
Epoch: [51][380/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 8.8640e-02 (9.3493e-02)	Acc@1  96.09 ( 96.66)	Acc@5 100.00 ( 99.97)
Epoch: [51][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.2719e-02 (9.3188e-02)	Acc@1  98.75 ( 96.67)	Acc@5 100.00 ( 99.97)
## e[51] optimizer.zero_grad (sum) time: 0.26208996772766113
## e[51]       loss.backward (sum) time: 3.9681291580200195
## e[51]      optimizer.step (sum) time: 1.8209624290466309
## epoch[51] training(only) time: 15.957195281982422
# Switched to evaluate mode...
Test: [  0/100]	Time  0.178 ( 0.178)	Loss 3.3662e-01 (3.3662e-01)	Acc@1  91.00 ( 91.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.024 ( 0.037)	Loss 4.1603e-01 (3.2123e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.021 ( 0.030)	Loss 5.2370e-01 (3.2757e-01)	Acc@1  85.00 ( 90.29)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.021 ( 0.027)	Loss 3.6761e-01 (3.4888e-01)	Acc@1  90.00 ( 90.39)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.024 ( 0.026)	Loss 4.0157e-01 (3.6339e-01)	Acc@1  89.00 ( 89.93)	Acc@5 100.00 ( 99.61)
Test: [ 50/100]	Time  0.021 ( 0.025)	Loss 2.5616e-01 (3.5245e-01)	Acc@1  94.00 ( 90.24)	Acc@5 100.00 ( 99.63)
Test: [ 60/100]	Time  0.023 ( 0.024)	Loss 3.4118e-01 (3.5626e-01)	Acc@1  89.00 ( 90.13)	Acc@5 100.00 ( 99.66)
Test: [ 70/100]	Time  0.024 ( 0.024)	Loss 5.1418e-01 (3.4658e-01)	Acc@1  86.00 ( 90.21)	Acc@5 100.00 ( 99.69)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.6211e-01 (3.4183e-01)	Acc@1  94.00 ( 90.32)	Acc@5 100.00 ( 99.69)
Test: [ 90/100]	Time  0.019 ( 0.024)	Loss 2.3611e-01 (3.4239e-01)	Acc@1  94.00 ( 90.33)	Acc@5 100.00 ( 99.70)
 * Acc@1 90.450 Acc@5 99.710
### epoch[51] execution time: 18.402177810668945
EPOCH 52
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [52][  0/391]	Time  0.223 ( 0.223)	Data  0.180 ( 0.180)	Loss 3.3228e-02 (3.3228e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [52][ 10/391]	Time  0.042 ( 0.057)	Data  0.001 ( 0.017)	Loss 6.6163e-02 (9.4737e-02)	Acc@1  97.66 ( 97.09)	Acc@5 100.00 ( 99.93)
Epoch: [52][ 20/391]	Time  0.040 ( 0.050)	Data  0.001 ( 0.010)	Loss 7.2001e-02 (8.5678e-02)	Acc@1  96.88 ( 97.32)	Acc@5 100.00 ( 99.93)
Epoch: [52][ 30/391]	Time  0.039 ( 0.047)	Data  0.001 ( 0.007)	Loss 8.6617e-02 (8.4722e-02)	Acc@1  94.53 ( 97.23)	Acc@5 100.00 ( 99.95)
Epoch: [52][ 40/391]	Time  0.037 ( 0.045)	Data  0.001 ( 0.005)	Loss 1.2932e-01 (8.7781e-02)	Acc@1  94.53 ( 97.05)	Acc@5 100.00 ( 99.96)
Epoch: [52][ 50/391]	Time  0.037 ( 0.044)	Data  0.001 ( 0.005)	Loss 1.6099e-01 (8.9854e-02)	Acc@1  92.97 ( 96.88)	Acc@5 100.00 ( 99.95)
Epoch: [52][ 60/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.0596e-01 (8.9928e-02)	Acc@1  96.09 ( 96.80)	Acc@5 100.00 ( 99.96)
Epoch: [52][ 70/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.004)	Loss 9.5099e-02 (9.0588e-02)	Acc@1  96.09 ( 96.82)	Acc@5 100.00 ( 99.97)
Epoch: [52][ 80/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.6880e-02 (9.4305e-02)	Acc@1  99.22 ( 96.71)	Acc@5 100.00 ( 99.97)
Epoch: [52][ 90/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.2860e-01 (9.5833e-02)	Acc@1  96.09 ( 96.65)	Acc@5 100.00 ( 99.97)
Epoch: [52][100/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.003)	Loss 8.3871e-02 (9.6332e-02)	Acc@1  96.09 ( 96.70)	Acc@5 100.00 ( 99.97)
Epoch: [52][110/391]	Time  0.054 ( 0.042)	Data  0.001 ( 0.003)	Loss 8.9431e-02 (9.5642e-02)	Acc@1  96.88 ( 96.69)	Acc@5 100.00 ( 99.97)
Epoch: [52][120/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 9.6516e-02 (9.5033e-02)	Acc@1  95.31 ( 96.70)	Acc@5 100.00 ( 99.97)
Epoch: [52][130/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 8.6830e-02 (9.4065e-02)	Acc@1  97.66 ( 96.74)	Acc@5 100.00 ( 99.98)
Epoch: [52][140/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.7499e-02 (9.3260e-02)	Acc@1  96.88 ( 96.75)	Acc@5 100.00 ( 99.98)
Epoch: [52][150/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.2373e-02 (9.2250e-02)	Acc@1  97.66 ( 96.81)	Acc@5 100.00 ( 99.98)
Epoch: [52][160/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5495e-01 (9.1041e-02)	Acc@1  94.53 ( 96.88)	Acc@5 100.00 ( 99.98)
Epoch: [52][170/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0302e-01 (9.1826e-02)	Acc@1  96.88 ( 96.81)	Acc@5 100.00 ( 99.98)
Epoch: [52][180/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.8095e-02 (9.1087e-02)	Acc@1  96.88 ( 96.82)	Acc@5 100.00 ( 99.98)
Epoch: [52][190/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2014e-02 (8.9882e-02)	Acc@1  98.44 ( 96.88)	Acc@5 100.00 ( 99.98)
Epoch: [52][200/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.5382e-02 (8.9725e-02)	Acc@1  98.44 ( 96.88)	Acc@5 100.00 ( 99.98)
Epoch: [52][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.6166e-02 (8.9169e-02)	Acc@1  96.88 ( 96.90)	Acc@5 100.00 ( 99.99)
Epoch: [52][220/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1549e-01 (8.9046e-02)	Acc@1  96.09 ( 96.90)	Acc@5 100.00 ( 99.99)
Epoch: [52][230/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.7270e-02 (8.9243e-02)	Acc@1  96.88 ( 96.89)	Acc@5 100.00 ( 99.99)
Epoch: [52][240/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6062e-01 (8.9443e-02)	Acc@1  94.53 ( 96.89)	Acc@5 100.00 ( 99.99)
Epoch: [52][250/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.2933e-02 (8.9323e-02)	Acc@1  96.88 ( 96.91)	Acc@5 100.00 ( 99.99)
Epoch: [52][260/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3230e-01 (8.9853e-02)	Acc@1  96.09 ( 96.88)	Acc@5 100.00 ( 99.99)
Epoch: [52][270/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.3182e-02 (8.9885e-02)	Acc@1  98.44 ( 96.88)	Acc@5 100.00 ( 99.99)
Epoch: [52][280/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.5471e-02 (8.9889e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 ( 99.99)
Epoch: [52][290/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3039e-01 (9.0410e-02)	Acc@1  95.31 ( 96.87)	Acc@5 100.00 ( 99.99)
Epoch: [52][300/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5546e-01 (9.0286e-02)	Acc@1  94.53 ( 96.87)	Acc@5 100.00 ( 99.99)
Epoch: [52][310/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1516e-01 (9.0405e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 ( 99.99)
Epoch: [52][320/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 4.3060e-02 (8.9925e-02)	Acc@1  97.66 ( 96.89)	Acc@5 100.00 ( 99.99)
Epoch: [52][330/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.1929e-01 (9.0205e-02)	Acc@1  96.09 ( 96.88)	Acc@5 100.00 ( 99.99)
Epoch: [52][340/391]	Time  0.052 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.4092e-02 (9.0191e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 ( 99.99)
Epoch: [52][350/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.1135e-01 (9.0072e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 ( 99.99)
Epoch: [52][360/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.1559e-01 (8.9876e-02)	Acc@1  97.66 ( 96.89)	Acc@5 100.00 ( 99.99)
Epoch: [52][370/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.2070e-01 (9.0470e-02)	Acc@1  93.75 ( 96.86)	Acc@5 100.00 ( 99.99)
Epoch: [52][380/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.2006e-01 (9.0787e-02)	Acc@1  97.66 ( 96.85)	Acc@5 100.00 ( 99.99)
Epoch: [52][390/391]	Time  0.027 ( 0.040)	Data  0.001 ( 0.001)	Loss 8.3489e-02 (9.0721e-02)	Acc@1  96.25 ( 96.85)	Acc@5 100.00 ( 99.99)
## e[52] optimizer.zero_grad (sum) time: 0.2638258934020996
## e[52]       loss.backward (sum) time: 3.941394805908203
## e[52]      optimizer.step (sum) time: 1.8310127258300781
## epoch[52] training(only) time: 15.90414023399353
# Switched to evaluate mode...
Test: [  0/100]	Time  0.176 ( 0.176)	Loss 3.0552e-01 (3.0552e-01)	Acc@1  91.00 ( 91.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.021 ( 0.037)	Loss 4.5154e-01 (2.9863e-01)	Acc@1  88.00 ( 91.00)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.017 ( 0.028)	Loss 5.2614e-01 (3.0563e-01)	Acc@1  85.00 ( 90.71)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.022 ( 0.026)	Loss 4.1565e-01 (3.3823e-01)	Acc@1  88.00 ( 90.26)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.024 ( 0.025)	Loss 4.1306e-01 (3.4700e-01)	Acc@1  91.00 ( 90.20)	Acc@5 100.00 ( 99.63)
Test: [ 50/100]	Time  0.017 ( 0.024)	Loss 2.7981e-01 (3.3648e-01)	Acc@1  93.00 ( 90.43)	Acc@5 100.00 ( 99.63)
Test: [ 60/100]	Time  0.018 ( 0.023)	Loss 2.6218e-01 (3.3738e-01)	Acc@1  91.00 ( 90.41)	Acc@5 100.00 ( 99.67)
Test: [ 70/100]	Time  0.019 ( 0.022)	Loss 4.2787e-01 (3.2787e-01)	Acc@1  89.00 ( 90.55)	Acc@5 100.00 ( 99.68)
Test: [ 80/100]	Time  0.017 ( 0.022)	Loss 1.8011e-01 (3.2541e-01)	Acc@1  94.00 ( 90.69)	Acc@5 100.00 ( 99.68)
Test: [ 90/100]	Time  0.020 ( 0.022)	Loss 2.4414e-01 (3.2738e-01)	Acc@1  94.00 ( 90.60)	Acc@5 100.00 ( 99.69)
 * Acc@1 90.640 Acc@5 99.700
### epoch[52] execution time: 18.21290612220764
EPOCH 53
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [53][  0/391]	Time  0.212 ( 0.212)	Data  0.166 ( 0.166)	Loss 5.7494e-02 (5.7494e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [53][ 10/391]	Time  0.047 ( 0.056)	Data  0.001 ( 0.016)	Loss 8.3906e-02 (8.8135e-02)	Acc@1  96.88 ( 96.80)	Acc@5 100.00 (100.00)
Epoch: [53][ 20/391]	Time  0.041 ( 0.049)	Data  0.001 ( 0.009)	Loss 5.9451e-02 (9.8074e-02)	Acc@1  97.66 ( 96.54)	Acc@5 100.00 (100.00)
Epoch: [53][ 30/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.006)	Loss 9.6868e-02 (9.3525e-02)	Acc@1  96.88 ( 96.65)	Acc@5 100.00 (100.00)
Epoch: [53][ 40/391]	Time  0.038 ( 0.045)	Data  0.001 ( 0.005)	Loss 5.7602e-02 (9.1790e-02)	Acc@1  98.44 ( 96.86)	Acc@5 100.00 (100.00)
Epoch: [53][ 50/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.004)	Loss 7.2343e-02 (9.3776e-02)	Acc@1  96.88 ( 96.72)	Acc@5 100.00 (100.00)
Epoch: [53][ 60/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.004)	Loss 5.0023e-02 (9.1208e-02)	Acc@1 100.00 ( 96.87)	Acc@5 100.00 ( 99.99)
Epoch: [53][ 70/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 6.1871e-02 (9.1226e-02)	Acc@1  99.22 ( 96.81)	Acc@5 100.00 ( 99.99)
Epoch: [53][ 80/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.6639e-01 (9.0287e-02)	Acc@1  92.19 ( 96.80)	Acc@5 100.00 ( 99.99)
Epoch: [53][ 90/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 9.0760e-02 (9.0515e-02)	Acc@1  96.88 ( 96.79)	Acc@5 100.00 ( 99.99)
Epoch: [53][100/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.3585e-01 (8.9952e-02)	Acc@1  96.09 ( 96.82)	Acc@5 100.00 ( 99.98)
Epoch: [53][110/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.0630e-02 (8.9650e-02)	Acc@1  99.22 ( 96.84)	Acc@5 100.00 ( 99.98)
Epoch: [53][120/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.8721e-02 (8.7899e-02)	Acc@1  99.22 ( 96.91)	Acc@5 100.00 ( 99.98)
Epoch: [53][130/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.0203e-02 (8.7781e-02)	Acc@1  98.44 ( 96.91)	Acc@5 100.00 ( 99.97)
Epoch: [53][140/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.0342e-01 (8.8475e-02)	Acc@1  95.31 ( 96.86)	Acc@5 100.00 ( 99.97)
Epoch: [53][150/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.0862e-02 (8.7907e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 ( 99.97)
Epoch: [53][160/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.1988e-02 (8.8671e-02)	Acc@1  96.09 ( 96.87)	Acc@5 100.00 ( 99.98)
Epoch: [53][170/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.1710e-02 (8.9909e-02)	Acc@1  98.44 ( 96.83)	Acc@5 100.00 ( 99.98)
Epoch: [53][180/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.8612e-02 (9.0086e-02)	Acc@1  99.22 ( 96.83)	Acc@5 100.00 ( 99.98)
Epoch: [53][190/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4456e-01 (9.0154e-02)	Acc@1  94.53 ( 96.84)	Acc@5  99.22 ( 99.98)
Epoch: [53][200/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7381e-01 (9.0085e-02)	Acc@1  92.97 ( 96.84)	Acc@5 100.00 ( 99.98)
Epoch: [53][210/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.5660e-02 (8.8631e-02)	Acc@1  99.22 ( 96.90)	Acc@5 100.00 ( 99.98)
Epoch: [53][220/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7521e-02 (8.8324e-02)	Acc@1  99.22 ( 96.92)	Acc@5 100.00 ( 99.98)
Epoch: [53][230/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0245e-01 (8.7963e-02)	Acc@1  96.09 ( 96.93)	Acc@5 100.00 ( 99.98)
Epoch: [53][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0079e-01 (8.7751e-02)	Acc@1  96.09 ( 96.93)	Acc@5  99.22 ( 99.98)
Epoch: [53][250/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.0865e-02 (8.7528e-02)	Acc@1  96.88 ( 96.95)	Acc@5 100.00 ( 99.98)
Epoch: [53][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.6809e-02 (8.7957e-02)	Acc@1  98.44 ( 96.94)	Acc@5 100.00 ( 99.98)
Epoch: [53][270/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.6265e-02 (8.8789e-02)	Acc@1  98.44 ( 96.90)	Acc@5 100.00 ( 99.98)
Epoch: [53][280/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.8317e-02 (8.8880e-02)	Acc@1  96.09 ( 96.89)	Acc@5 100.00 ( 99.98)
Epoch: [53][290/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.0168e-02 (8.8714e-02)	Acc@1  99.22 ( 96.91)	Acc@5 100.00 ( 99.97)
Epoch: [53][300/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2451e-01 (8.8619e-02)	Acc@1  96.88 ( 96.91)	Acc@5 100.00 ( 99.97)
Epoch: [53][310/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.8234e-02 (8.8263e-02)	Acc@1  97.66 ( 96.92)	Acc@5 100.00 ( 99.97)
Epoch: [53][320/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.7195e-02 (8.8759e-02)	Acc@1  97.66 ( 96.90)	Acc@5 100.00 ( 99.98)
Epoch: [53][330/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.2243e-01 (8.8709e-02)	Acc@1  96.88 ( 96.91)	Acc@5 100.00 ( 99.98)
Epoch: [53][340/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.5992e-02 (8.8847e-02)	Acc@1  96.88 ( 96.91)	Acc@5 100.00 ( 99.97)
Epoch: [53][350/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.4884e-02 (8.8669e-02)	Acc@1  98.44 ( 96.92)	Acc@5 100.00 ( 99.97)
Epoch: [53][360/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.3135e-01 (8.9030e-02)	Acc@1  94.53 ( 96.91)	Acc@5 100.00 ( 99.97)
Epoch: [53][370/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 9.2383e-02 (8.8634e-02)	Acc@1  97.66 ( 96.92)	Acc@5 100.00 ( 99.97)
Epoch: [53][380/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.9920e-02 (8.9298e-02)	Acc@1  97.66 ( 96.88)	Acc@5 100.00 ( 99.98)
Epoch: [53][390/391]	Time  0.028 ( 0.041)	Data  0.001 ( 0.001)	Loss 8.9859e-02 (8.9180e-02)	Acc@1  96.25 ( 96.88)	Acc@5 100.00 ( 99.98)
## e[53] optimizer.zero_grad (sum) time: 0.26310014724731445
## e[53]       loss.backward (sum) time: 4.036630153656006
## e[53]      optimizer.step (sum) time: 1.7883105278015137
## epoch[53] training(only) time: 16.04584503173828
# Switched to evaluate mode...
Test: [  0/100]	Time  0.166 ( 0.166)	Loss 3.4654e-01 (3.4654e-01)	Acc@1  90.00 ( 90.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.024 ( 0.034)	Loss 4.4981e-01 (2.9959e-01)	Acc@1  88.00 ( 91.45)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.018 ( 0.027)	Loss 4.7585e-01 (3.1804e-01)	Acc@1  90.00 ( 91.05)	Acc@5  99.00 ( 99.81)
Test: [ 30/100]	Time  0.018 ( 0.024)	Loss 4.6872e-01 (3.4210e-01)	Acc@1  87.00 ( 90.68)	Acc@5 100.00 ( 99.81)
Test: [ 40/100]	Time  0.019 ( 0.023)	Loss 4.0762e-01 (3.5953e-01)	Acc@1  89.00 ( 90.37)	Acc@5 100.00 ( 99.76)
Test: [ 50/100]	Time  0.022 ( 0.023)	Loss 2.6096e-01 (3.5065e-01)	Acc@1  94.00 ( 90.55)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.023 ( 0.023)	Loss 3.3617e-01 (3.5430e-01)	Acc@1  89.00 ( 90.33)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.024 ( 0.023)	Loss 4.1576e-01 (3.4573e-01)	Acc@1  91.00 ( 90.34)	Acc@5 100.00 ( 99.80)
Test: [ 80/100]	Time  0.020 ( 0.022)	Loss 2.1785e-01 (3.4139e-01)	Acc@1  94.00 ( 90.48)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.023 ( 0.022)	Loss 2.4898e-01 (3.4287e-01)	Acc@1  93.00 ( 90.47)	Acc@5 100.00 ( 99.78)
 * Acc@1 90.540 Acc@5 99.770
### epoch[53] execution time: 18.349578857421875
EPOCH 54
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [54][  0/391]	Time  0.255 ( 0.255)	Data  0.212 ( 0.212)	Loss 6.5185e-02 (6.5185e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [54][ 10/391]	Time  0.040 ( 0.061)	Data  0.001 ( 0.020)	Loss 3.6294e-02 (8.6736e-02)	Acc@1  98.44 ( 96.80)	Acc@5 100.00 (100.00)
Epoch: [54][ 20/391]	Time  0.041 ( 0.051)	Data  0.001 ( 0.011)	Loss 6.7598e-02 (8.7786e-02)	Acc@1  96.88 ( 96.95)	Acc@5 100.00 (100.00)
Epoch: [54][ 30/391]	Time  0.041 ( 0.047)	Data  0.001 ( 0.008)	Loss 1.3454e-01 (9.0957e-02)	Acc@1  96.88 ( 96.93)	Acc@5 100.00 ( 99.97)
Epoch: [54][ 40/391]	Time  0.042 ( 0.046)	Data  0.001 ( 0.006)	Loss 9.0502e-02 (8.8941e-02)	Acc@1  96.09 ( 96.97)	Acc@5 100.00 ( 99.94)
Epoch: [54][ 50/391]	Time  0.043 ( 0.045)	Data  0.001 ( 0.005)	Loss 1.5911e-01 (8.8695e-02)	Acc@1  95.31 ( 97.03)	Acc@5 100.00 ( 99.95)
Epoch: [54][ 60/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.004)	Loss 9.2645e-02 (8.9518e-02)	Acc@1  96.09 ( 96.86)	Acc@5 100.00 ( 99.96)
Epoch: [54][ 70/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 7.7268e-02 (9.1636e-02)	Acc@1  96.88 ( 96.78)	Acc@5 100.00 ( 99.97)
Epoch: [54][ 80/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 7.5373e-02 (9.2616e-02)	Acc@1  97.66 ( 96.67)	Acc@5 100.00 ( 99.97)
Epoch: [54][ 90/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.3042e-02 (9.1727e-02)	Acc@1  96.09 ( 96.70)	Acc@5 100.00 ( 99.97)
Epoch: [54][100/391]	Time  0.035 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.2438e-01 (9.1442e-02)	Acc@1  95.31 ( 96.67)	Acc@5 100.00 ( 99.98)
Epoch: [54][110/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.1846e-01 (9.1103e-02)	Acc@1  96.09 ( 96.68)	Acc@5 100.00 ( 99.98)
Epoch: [54][120/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 9.1303e-02 (9.1845e-02)	Acc@1  96.88 ( 96.70)	Acc@5 100.00 ( 99.98)
Epoch: [54][130/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.7400e-02 (9.1062e-02)	Acc@1  96.88 ( 96.74)	Acc@5 100.00 ( 99.98)
Epoch: [54][140/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.7751e-02 (8.9470e-02)	Acc@1  99.22 ( 96.85)	Acc@5 100.00 ( 99.98)
Epoch: [54][150/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1537e-02 (8.9414e-02)	Acc@1  98.44 ( 96.84)	Acc@5 100.00 ( 99.98)
Epoch: [54][160/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8376e-02 (8.9154e-02)	Acc@1  99.22 ( 96.87)	Acc@5 100.00 ( 99.98)
Epoch: [54][170/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8216e-02 (8.8289e-02)	Acc@1  99.22 ( 96.90)	Acc@5 100.00 ( 99.98)
Epoch: [54][180/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.8345e-02 (8.7851e-02)	Acc@1  97.66 ( 96.90)	Acc@5 100.00 ( 99.98)
Epoch: [54][190/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.1182e-02 (8.7860e-02)	Acc@1  97.66 ( 96.89)	Acc@5 100.00 ( 99.98)
Epoch: [54][200/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1247e-01 (8.7928e-02)	Acc@1  96.88 ( 96.89)	Acc@5 100.00 ( 99.98)
Epoch: [54][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.3854e-02 (8.8088e-02)	Acc@1  96.88 ( 96.87)	Acc@5 100.00 ( 99.99)
Epoch: [54][220/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7790e-01 (8.8056e-02)	Acc@1  93.75 ( 96.86)	Acc@5 100.00 ( 99.99)
Epoch: [54][230/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4352e-01 (8.7120e-02)	Acc@1  94.53 ( 96.89)	Acc@5 100.00 ( 99.99)
Epoch: [54][240/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0830e-02 (8.6778e-02)	Acc@1  98.44 ( 96.89)	Acc@5 100.00 ( 99.99)
Epoch: [54][250/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.6866e-02 (8.7274e-02)	Acc@1  99.22 ( 96.88)	Acc@5 100.00 ( 99.99)
Epoch: [54][260/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.7975e-02 (8.7284e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 ( 99.99)
Epoch: [54][270/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.5272e-02 (8.7602e-02)	Acc@1  97.66 ( 96.88)	Acc@5 100.00 ( 99.98)
Epoch: [54][280/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.4267e-02 (8.7089e-02)	Acc@1  97.66 ( 96.90)	Acc@5 100.00 ( 99.98)
Epoch: [54][290/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4958e-01 (8.7310e-02)	Acc@1  95.31 ( 96.89)	Acc@5 100.00 ( 99.98)
Epoch: [54][300/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0403e-01 (8.8152e-02)	Acc@1  95.31 ( 96.85)	Acc@5 100.00 ( 99.98)
Epoch: [54][310/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.7463e-02 (8.8207e-02)	Acc@1  98.44 ( 96.85)	Acc@5 100.00 ( 99.98)
Epoch: [54][320/391]	Time  0.050 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.8968e-02 (8.7977e-02)	Acc@1  97.66 ( 96.87)	Acc@5 100.00 ( 99.99)
Epoch: [54][330/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2259e-01 (8.8121e-02)	Acc@1  94.53 ( 96.86)	Acc@5 100.00 ( 99.99)
Epoch: [54][340/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1195e-02 (8.8294e-02)	Acc@1  97.66 ( 96.86)	Acc@5 100.00 ( 99.99)
Epoch: [54][350/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.3605e-02 (8.8135e-02)	Acc@1  97.66 ( 96.87)	Acc@5 100.00 ( 99.99)
Epoch: [54][360/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.5395e-02 (8.7598e-02)	Acc@1  97.66 ( 96.89)	Acc@5 100.00 ( 99.99)
Epoch: [54][370/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.7001e-02 (8.7399e-02)	Acc@1  96.88 ( 96.90)	Acc@5 100.00 ( 99.99)
Epoch: [54][380/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.4945e-02 (8.7041e-02)	Acc@1  96.88 ( 96.91)	Acc@5 100.00 ( 99.99)
Epoch: [54][390/391]	Time  0.035 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.5009e-02 (8.7385e-02)	Acc@1  95.00 ( 96.90)	Acc@5 100.00 ( 99.99)
## e[54] optimizer.zero_grad (sum) time: 0.2620842456817627
## e[54]       loss.backward (sum) time: 3.9641940593719482
## e[54]      optimizer.step (sum) time: 1.8210909366607666
## epoch[54] training(only) time: 16.008384704589844
# Switched to evaluate mode...
Test: [  0/100]	Time  0.173 ( 0.173)	Loss 2.7725e-01 (2.7725e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.019 ( 0.034)	Loss 4.1573e-01 (2.9009e-01)	Acc@1  90.00 ( 91.36)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.022 ( 0.028)	Loss 5.0769e-01 (3.2253e-01)	Acc@1  89.00 ( 90.81)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.023 ( 0.026)	Loss 4.2270e-01 (3.3701e-01)	Acc@1  87.00 ( 90.58)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.021 ( 0.025)	Loss 3.8776e-01 (3.4344e-01)	Acc@1  89.00 ( 90.34)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.020 ( 0.024)	Loss 2.5372e-01 (3.3981e-01)	Acc@1  95.00 ( 90.65)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.017 ( 0.023)	Loss 3.5992e-01 (3.4881e-01)	Acc@1  89.00 ( 90.26)	Acc@5  99.00 ( 99.74)
Test: [ 70/100]	Time  0.022 ( 0.023)	Loss 3.6027e-01 (3.3790e-01)	Acc@1  89.00 ( 90.56)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.019 ( 0.023)	Loss 2.8433e-01 (3.3574e-01)	Acc@1  92.00 ( 90.68)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.021 ( 0.023)	Loss 2.6895e-01 (3.3664e-01)	Acc@1  94.00 ( 90.57)	Acc@5 100.00 ( 99.76)
 * Acc@1 90.620 Acc@5 99.760
### epoch[54] execution time: 18.3817195892334
EPOCH 55
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [55][  0/391]	Time  0.221 ( 0.221)	Data  0.180 ( 0.180)	Loss 7.9059e-02 (7.9059e-02)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [55][ 10/391]	Time  0.038 ( 0.057)	Data  0.001 ( 0.017)	Loss 8.0132e-02 (8.3949e-02)	Acc@1  98.44 ( 97.16)	Acc@5 100.00 (100.00)
Epoch: [55][ 20/391]	Time  0.039 ( 0.049)	Data  0.001 ( 0.009)	Loss 6.7755e-02 (7.6011e-02)	Acc@1  96.09 ( 97.28)	Acc@5 100.00 (100.00)
Epoch: [55][ 30/391]	Time  0.038 ( 0.046)	Data  0.001 ( 0.007)	Loss 9.7195e-02 (7.8564e-02)	Acc@1  96.09 ( 97.05)	Acc@5 100.00 ( 99.97)
Epoch: [55][ 40/391]	Time  0.038 ( 0.045)	Data  0.001 ( 0.005)	Loss 1.1807e-01 (7.8878e-02)	Acc@1  96.09 ( 97.03)	Acc@5  99.22 ( 99.96)
Epoch: [55][ 50/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.004)	Loss 7.9981e-02 (7.7064e-02)	Acc@1  96.88 ( 97.12)	Acc@5 100.00 ( 99.97)
Epoch: [55][ 60/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.004)	Loss 7.2658e-02 (7.9506e-02)	Acc@1  97.66 ( 97.13)	Acc@5 100.00 ( 99.97)
Epoch: [55][ 70/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 2.0710e-01 (8.2061e-02)	Acc@1  92.97 ( 97.06)	Acc@5  98.44 ( 99.96)
Epoch: [55][ 80/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.003)	Loss 6.6223e-02 (8.0753e-02)	Acc@1  97.66 ( 97.08)	Acc@5 100.00 ( 99.95)
Epoch: [55][ 90/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.8447e-02 (7.9858e-02)	Acc@1  96.88 ( 97.13)	Acc@5 100.00 ( 99.96)
Epoch: [55][100/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.6901e-02 (7.7994e-02)	Acc@1 100.00 ( 97.24)	Acc@5 100.00 ( 99.96)
Epoch: [55][110/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.2957e-01 (7.8542e-02)	Acc@1  96.09 ( 97.21)	Acc@5 100.00 ( 99.96)
Epoch: [55][120/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.7412e-02 (7.8771e-02)	Acc@1  99.22 ( 97.24)	Acc@5 100.00 ( 99.97)
Epoch: [55][130/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4596e-02 (8.0212e-02)	Acc@1  97.66 ( 97.17)	Acc@5 100.00 ( 99.97)
Epoch: [55][140/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.8842e-02 (8.1160e-02)	Acc@1  96.88 ( 97.16)	Acc@5 100.00 ( 99.97)
Epoch: [55][150/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.1582e-02 (8.0535e-02)	Acc@1  96.09 ( 97.20)	Acc@5 100.00 ( 99.97)
Epoch: [55][160/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.0136e-02 (8.1664e-02)	Acc@1  97.66 ( 97.16)	Acc@5 100.00 ( 99.97)
Epoch: [55][170/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5424e-01 (8.1933e-02)	Acc@1  96.88 ( 97.12)	Acc@5 100.00 ( 99.97)
Epoch: [55][180/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.1461e-02 (8.0878e-02)	Acc@1  97.66 ( 97.18)	Acc@5 100.00 ( 99.97)
Epoch: [55][190/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.2702e-02 (8.1831e-02)	Acc@1  97.66 ( 97.15)	Acc@5 100.00 ( 99.97)
Epoch: [55][200/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.1735e-02 (8.1664e-02)	Acc@1  97.66 ( 97.17)	Acc@5 100.00 ( 99.97)
Epoch: [55][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.6779e-02 (8.1331e-02)	Acc@1  94.53 ( 97.19)	Acc@5 100.00 ( 99.97)
Epoch: [55][220/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.9909e-02 (8.1748e-02)	Acc@1  97.66 ( 97.18)	Acc@5 100.00 ( 99.98)
Epoch: [55][230/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.6750e-02 (8.2077e-02)	Acc@1  98.44 ( 97.16)	Acc@5 100.00 ( 99.98)
Epoch: [55][240/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6988e-01 (8.2292e-02)	Acc@1  92.97 ( 97.14)	Acc@5 100.00 ( 99.98)
Epoch: [55][250/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3422e-02 (8.2548e-02)	Acc@1 100.00 ( 97.12)	Acc@5 100.00 ( 99.98)
Epoch: [55][260/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0531e-01 (8.2687e-02)	Acc@1  96.09 ( 97.11)	Acc@5 100.00 ( 99.98)
Epoch: [55][270/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2627e-01 (8.2286e-02)	Acc@1  95.31 ( 97.12)	Acc@5 100.00 ( 99.98)
Epoch: [55][280/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.7653e-02 (8.2587e-02)	Acc@1  99.22 ( 97.13)	Acc@5 100.00 ( 99.98)
Epoch: [55][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7304e-02 (8.2364e-02)	Acc@1  99.22 ( 97.14)	Acc@5 100.00 ( 99.98)
Epoch: [55][300/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.9653e-02 (8.2254e-02)	Acc@1  96.88 ( 97.14)	Acc@5 100.00 ( 99.98)
Epoch: [55][310/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2972e-01 (8.2160e-02)	Acc@1  95.31 ( 97.13)	Acc@5  99.22 ( 99.98)
Epoch: [55][320/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.7718e-02 (8.2006e-02)	Acc@1  97.66 ( 97.14)	Acc@5 100.00 ( 99.98)
Epoch: [55][330/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.5173e-02 (8.2415e-02)	Acc@1  96.88 ( 97.12)	Acc@5 100.00 ( 99.98)
Epoch: [55][340/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.4001e-01 (8.2434e-02)	Acc@1  94.53 ( 97.12)	Acc@5 100.00 ( 99.98)
Epoch: [55][350/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.2184e-02 (8.2167e-02)	Acc@1  96.88 ( 97.12)	Acc@5 100.00 ( 99.98)
Epoch: [55][360/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.3086e-02 (8.2283e-02)	Acc@1  97.66 ( 97.10)	Acc@5 100.00 ( 99.98)
Epoch: [55][370/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 8.8978e-02 (8.2479e-02)	Acc@1  98.44 ( 97.10)	Acc@5 100.00 ( 99.98)
Epoch: [55][380/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.0936e-01 (8.2790e-02)	Acc@1  96.09 ( 97.09)	Acc@5 100.00 ( 99.98)
Epoch: [55][390/391]	Time  0.028 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.5708e-01 (8.2837e-02)	Acc@1  93.75 ( 97.08)	Acc@5 100.00 ( 99.98)
## e[55] optimizer.zero_grad (sum) time: 0.26291561126708984
## e[55]       loss.backward (sum) time: 4.003329038619995
## e[55]      optimizer.step (sum) time: 1.7711219787597656
## epoch[55] training(only) time: 16.005629777908325
# Switched to evaluate mode...
Test: [  0/100]	Time  0.173 ( 0.173)	Loss 3.9935e-01 (3.9935e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.036)	Loss 4.7464e-01 (3.3379e-01)	Acc@1  89.00 ( 90.73)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.022 ( 0.028)	Loss 5.4380e-01 (3.4311e-01)	Acc@1  88.00 ( 90.62)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.024 ( 0.027)	Loss 4.1542e-01 (3.5724e-01)	Acc@1  88.00 ( 90.48)	Acc@5 100.00 ( 99.84)
Test: [ 40/100]	Time  0.022 ( 0.025)	Loss 3.6181e-01 (3.6643e-01)	Acc@1  89.00 ( 90.12)	Acc@5 100.00 ( 99.78)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 2.9766e-01 (3.5483e-01)	Acc@1  95.00 ( 90.51)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.021 ( 0.024)	Loss 3.6916e-01 (3.5615e-01)	Acc@1  88.00 ( 90.28)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.022 ( 0.023)	Loss 4.4285e-01 (3.4585e-01)	Acc@1  86.00 ( 90.45)	Acc@5 100.00 ( 99.80)
Test: [ 80/100]	Time  0.018 ( 0.023)	Loss 2.1600e-01 (3.4119e-01)	Acc@1  95.00 ( 90.54)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.022 ( 0.023)	Loss 2.9228e-01 (3.4494e-01)	Acc@1  94.00 ( 90.49)	Acc@5 100.00 ( 99.79)
 * Acc@1 90.650 Acc@5 99.780
### epoch[55] execution time: 18.359421730041504
EPOCH 56
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [56][  0/391]	Time  0.219 ( 0.219)	Data  0.176 ( 0.176)	Loss 6.7953e-02 (6.7953e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [56][ 10/391]	Time  0.040 ( 0.057)	Data  0.001 ( 0.017)	Loss 5.2513e-02 (8.2915e-02)	Acc@1  98.44 ( 97.23)	Acc@5 100.00 (100.00)
Epoch: [56][ 20/391]	Time  0.039 ( 0.049)	Data  0.001 ( 0.009)	Loss 3.4401e-02 (8.3085e-02)	Acc@1  99.22 ( 97.21)	Acc@5 100.00 (100.00)
Epoch: [56][ 30/391]	Time  0.042 ( 0.046)	Data  0.001 ( 0.007)	Loss 6.8773e-02 (8.1397e-02)	Acc@1  96.88 ( 97.13)	Acc@5 100.00 (100.00)
Epoch: [56][ 40/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.005)	Loss 7.6157e-02 (8.0384e-02)	Acc@1  98.44 ( 97.22)	Acc@5 100.00 (100.00)
Epoch: [56][ 50/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.004)	Loss 5.0917e-02 (8.1697e-02)	Acc@1  97.66 ( 97.14)	Acc@5 100.00 ( 99.98)
Epoch: [56][ 60/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 6.2370e-02 (8.3381e-02)	Acc@1  98.44 ( 97.14)	Acc@5 100.00 ( 99.99)
Epoch: [56][ 70/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 6.6942e-02 (8.2851e-02)	Acc@1  96.88 ( 97.14)	Acc@5 100.00 ( 99.98)
Epoch: [56][ 80/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 6.3396e-02 (8.1407e-02)	Acc@1  97.66 ( 97.19)	Acc@5 100.00 ( 99.98)
Epoch: [56][ 90/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.1353e-01 (8.3093e-02)	Acc@1  96.88 ( 97.14)	Acc@5 100.00 ( 99.98)
Epoch: [56][100/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.1826e-02 (8.2336e-02)	Acc@1  98.44 ( 97.15)	Acc@5 100.00 ( 99.98)
Epoch: [56][110/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.7685e-01 (8.3421e-02)	Acc@1  92.97 ( 97.07)	Acc@5 100.00 ( 99.99)
Epoch: [56][120/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 7.6473e-02 (8.3215e-02)	Acc@1  97.66 ( 97.06)	Acc@5 100.00 ( 99.98)
Epoch: [56][130/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.7292e-02 (8.3040e-02)	Acc@1  97.66 ( 97.10)	Acc@5 100.00 ( 99.98)
Epoch: [56][140/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.9513e-02 (8.2829e-02)	Acc@1  98.44 ( 97.10)	Acc@5 100.00 ( 99.98)
Epoch: [56][150/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 8.6964e-02 (8.3125e-02)	Acc@1  97.66 ( 97.12)	Acc@5 100.00 ( 99.98)
Epoch: [56][160/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 8.0400e-02 (8.2372e-02)	Acc@1  96.09 ( 97.14)	Acc@5 100.00 ( 99.99)
Epoch: [56][170/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.2065e-02 (8.1957e-02)	Acc@1  96.88 ( 97.17)	Acc@5 100.00 ( 99.98)
Epoch: [56][180/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.5084e-02 (8.2304e-02)	Acc@1  96.09 ( 97.13)	Acc@5 100.00 ( 99.98)
Epoch: [56][190/391]	Time  0.039 ( 0.041)	Data  0.002 ( 0.002)	Loss 8.9169e-02 (8.2739e-02)	Acc@1  96.88 ( 97.12)	Acc@5 100.00 ( 99.98)
Epoch: [56][200/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.6372e-02 (8.2828e-02)	Acc@1  97.66 ( 97.10)	Acc@5 100.00 ( 99.98)
Epoch: [56][210/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.0448e-02 (8.3687e-02)	Acc@1  96.88 ( 97.07)	Acc@5 100.00 ( 99.98)
Epoch: [56][220/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0507e-01 (8.3858e-02)	Acc@1  96.88 ( 97.07)	Acc@5 100.00 ( 99.98)
Epoch: [56][230/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.6356e-02 (8.3990e-02)	Acc@1  96.88 ( 97.07)	Acc@5 100.00 ( 99.97)
Epoch: [56][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0267e-01 (8.4589e-02)	Acc@1  95.31 ( 97.03)	Acc@5 100.00 ( 99.97)
Epoch: [56][250/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3427e-01 (8.4787e-02)	Acc@1  95.31 ( 97.01)	Acc@5 100.00 ( 99.98)
Epoch: [56][260/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.6474e-02 (8.4367e-02)	Acc@1  96.88 ( 97.02)	Acc@5 100.00 ( 99.98)
Epoch: [56][270/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.2767e-02 (8.4229e-02)	Acc@1  97.66 ( 97.02)	Acc@5 100.00 ( 99.98)
Epoch: [56][280/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1251e-01 (8.4692e-02)	Acc@1  96.09 ( 97.00)	Acc@5 100.00 ( 99.97)
Epoch: [56][290/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2403e-01 (8.4866e-02)	Acc@1  95.31 ( 97.01)	Acc@5 100.00 ( 99.98)
Epoch: [56][300/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.5585e-02 (8.5430e-02)	Acc@1  96.88 ( 96.97)	Acc@5 100.00 ( 99.98)
Epoch: [56][310/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.1795e-02 (8.5518e-02)	Acc@1  96.88 ( 96.95)	Acc@5 100.00 ( 99.98)
Epoch: [56][320/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.5744e-02 (8.5741e-02)	Acc@1  96.09 ( 96.94)	Acc@5 100.00 ( 99.98)
Epoch: [56][330/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.7927e-02 (8.5397e-02)	Acc@1  97.66 ( 96.93)	Acc@5 100.00 ( 99.98)
Epoch: [56][340/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.5755e-02 (8.5095e-02)	Acc@1  98.44 ( 96.96)	Acc@5 100.00 ( 99.98)
Epoch: [56][350/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.0433e-01 (8.5125e-02)	Acc@1  95.31 ( 96.96)	Acc@5 100.00 ( 99.98)
Epoch: [56][360/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.6624e-02 (8.4922e-02)	Acc@1  99.22 ( 96.97)	Acc@5 100.00 ( 99.98)
Epoch: [56][370/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.5639e-01 (8.5172e-02)	Acc@1  96.09 ( 96.98)	Acc@5 100.00 ( 99.98)
Epoch: [56][380/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.9848e-02 (8.5142e-02)	Acc@1  96.88 ( 96.98)	Acc@5 100.00 ( 99.98)
Epoch: [56][390/391]	Time  0.028 ( 0.041)	Data  0.001 ( 0.001)	Loss 8.3686e-02 (8.4808e-02)	Acc@1  96.25 ( 96.98)	Acc@5 100.00 ( 99.98)
## e[56] optimizer.zero_grad (sum) time: 0.2624013423919678
## e[56]       loss.backward (sum) time: 4.019109487533569
## e[56]      optimizer.step (sum) time: 1.7913999557495117
## epoch[56] training(only) time: 16.060895919799805
# Switched to evaluate mode...
Test: [  0/100]	Time  0.207 ( 0.207)	Loss 2.9010e-01 (2.9010e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.022 ( 0.038)	Loss 4.0672e-01 (3.0407e-01)	Acc@1  87.00 ( 90.91)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.018 ( 0.029)	Loss 5.6379e-01 (3.2666e-01)	Acc@1  87.00 ( 90.48)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.021 ( 0.026)	Loss 3.8886e-01 (3.4572e-01)	Acc@1  89.00 ( 90.58)	Acc@5  99.00 ( 99.74)
Test: [ 40/100]	Time  0.021 ( 0.025)	Loss 3.7443e-01 (3.5311e-01)	Acc@1  92.00 ( 90.34)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 3.3560e-01 (3.4761e-01)	Acc@1  93.00 ( 90.45)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.022 ( 0.024)	Loss 3.9405e-01 (3.4993e-01)	Acc@1  89.00 ( 90.20)	Acc@5  99.00 ( 99.72)
Test: [ 70/100]	Time  0.024 ( 0.024)	Loss 3.6525e-01 (3.4126e-01)	Acc@1  88.00 ( 90.37)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.023 ( 0.023)	Loss 2.6613e-01 (3.3651e-01)	Acc@1  94.00 ( 90.56)	Acc@5 100.00 ( 99.72)
Test: [ 90/100]	Time  0.018 ( 0.023)	Loss 2.1133e-01 (3.3547e-01)	Acc@1  95.00 ( 90.59)	Acc@5 100.00 ( 99.74)
 * Acc@1 90.700 Acc@5 99.740
### epoch[56] execution time: 18.44189739227295
EPOCH 57
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [57][  0/391]	Time  0.214 ( 0.214)	Data  0.172 ( 0.172)	Loss 6.1873e-02 (6.1873e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [57][ 10/391]	Time  0.040 ( 0.055)	Data  0.001 ( 0.017)	Loss 5.0771e-02 (7.5911e-02)	Acc@1  97.66 ( 97.37)	Acc@5 100.00 (100.00)
Epoch: [57][ 20/391]	Time  0.042 ( 0.048)	Data  0.001 ( 0.009)	Loss 5.0385e-02 (7.1630e-02)	Acc@1  97.66 ( 97.54)	Acc@5 100.00 (100.00)
Epoch: [57][ 30/391]	Time  0.042 ( 0.046)	Data  0.001 ( 0.006)	Loss 2.4901e-02 (7.1179e-02)	Acc@1 100.00 ( 97.58)	Acc@5 100.00 (100.00)
Epoch: [57][ 40/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.005)	Loss 5.5480e-02 (6.8876e-02)	Acc@1  98.44 ( 97.77)	Acc@5 100.00 (100.00)
Epoch: [57][ 50/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.004)	Loss 8.8130e-02 (6.8441e-02)	Acc@1  96.88 ( 97.79)	Acc@5 100.00 (100.00)
Epoch: [57][ 60/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.004)	Loss 9.3170e-02 (7.1825e-02)	Acc@1  96.88 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [57][ 70/391]	Time  0.034 ( 0.043)	Data  0.001 ( 0.003)	Loss 2.0169e-02 (7.1121e-02)	Acc@1 100.00 ( 97.67)	Acc@5 100.00 ( 99.99)
Epoch: [57][ 80/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.4244e-01 (7.5206e-02)	Acc@1  96.88 ( 97.56)	Acc@5 100.00 ( 99.97)
Epoch: [57][ 90/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.003)	Loss 9.7495e-02 (7.5121e-02)	Acc@1  96.09 ( 97.56)	Acc@5 100.00 ( 99.97)
Epoch: [57][100/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.003)	Loss 9.7762e-02 (7.6282e-02)	Acc@1  96.09 ( 97.47)	Acc@5 100.00 ( 99.97)
Epoch: [57][110/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 6.8696e-02 (7.6330e-02)	Acc@1  97.66 ( 97.44)	Acc@5 100.00 ( 99.97)
Epoch: [57][120/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7052e-02 (7.7333e-02)	Acc@1  98.44 ( 97.42)	Acc@5 100.00 ( 99.97)
Epoch: [57][130/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.3859e-02 (7.6903e-02)	Acc@1  97.66 ( 97.43)	Acc@5 100.00 ( 99.98)
Epoch: [57][140/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.8127e-02 (7.6039e-02)	Acc@1  96.88 ( 97.46)	Acc@5 100.00 ( 99.98)
Epoch: [57][150/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1248e-02 (7.6092e-02)	Acc@1  99.22 ( 97.44)	Acc@5 100.00 ( 99.98)
Epoch: [57][160/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.4485e-02 (7.7155e-02)	Acc@1  97.66 ( 97.36)	Acc@5 100.00 ( 99.98)
Epoch: [57][170/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.0611e-02 (7.7648e-02)	Acc@1  97.66 ( 97.31)	Acc@5 100.00 ( 99.98)
Epoch: [57][180/391]	Time  0.049 ( 0.041)	Data  0.003 ( 0.002)	Loss 1.0933e-01 (7.8489e-02)	Acc@1  93.75 ( 97.27)	Acc@5 100.00 ( 99.98)
Epoch: [57][190/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.0934e-02 (7.8092e-02)	Acc@1  97.66 ( 97.28)	Acc@5 100.00 ( 99.98)
Epoch: [57][200/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2346e-01 (7.9000e-02)	Acc@1  95.31 ( 97.24)	Acc@5 100.00 ( 99.98)
Epoch: [57][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.9529e-02 (7.8838e-02)	Acc@1  97.66 ( 97.26)	Acc@5 100.00 ( 99.98)
Epoch: [57][220/391]	Time  0.056 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.9120e-02 (7.8439e-02)	Acc@1  96.09 ( 97.28)	Acc@5 100.00 ( 99.98)
Epoch: [57][230/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.7728e-02 (7.8801e-02)	Acc@1  98.44 ( 97.28)	Acc@5 100.00 ( 99.98)
Epoch: [57][240/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.6190e-02 (7.9117e-02)	Acc@1  96.09 ( 97.25)	Acc@5 100.00 ( 99.98)
Epoch: [57][250/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.2057e-02 (7.9778e-02)	Acc@1  96.88 ( 97.21)	Acc@5 100.00 ( 99.98)
Epoch: [57][260/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8695e-02 (7.9872e-02)	Acc@1  99.22 ( 97.20)	Acc@5 100.00 ( 99.98)
Epoch: [57][270/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.4253e-02 (8.0154e-02)	Acc@1  98.44 ( 97.17)	Acc@5 100.00 ( 99.98)
Epoch: [57][280/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.0955e-02 (7.9621e-02)	Acc@1  97.66 ( 97.21)	Acc@5 100.00 ( 99.98)
Epoch: [57][290/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.3190e-02 (7.9672e-02)	Acc@1  97.66 ( 97.19)	Acc@5 100.00 ( 99.98)
Epoch: [57][300/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.6640e-02 (8.0308e-02)	Acc@1  97.66 ( 97.17)	Acc@5 100.00 ( 99.98)
Epoch: [57][310/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.7200e-02 (8.0821e-02)	Acc@1  96.09 ( 97.16)	Acc@5 100.00 ( 99.98)
Epoch: [57][320/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2211e-01 (8.0744e-02)	Acc@1  95.31 ( 97.16)	Acc@5 100.00 ( 99.98)
Epoch: [57][330/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.6799e-02 (8.0598e-02)	Acc@1  98.44 ( 97.17)	Acc@5 100.00 ( 99.98)
Epoch: [57][340/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.3917e-02 (8.0404e-02)	Acc@1  96.88 ( 97.18)	Acc@5 100.00 ( 99.98)
Epoch: [57][350/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 8.8765e-02 (8.0715e-02)	Acc@1  95.31 ( 97.16)	Acc@5 100.00 ( 99.98)
Epoch: [57][360/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.0135e-01 (8.1100e-02)	Acc@1  96.88 ( 97.15)	Acc@5 100.00 ( 99.98)
Epoch: [57][370/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.5357e-02 (8.1115e-02)	Acc@1  98.44 ( 97.15)	Acc@5 100.00 ( 99.99)
Epoch: [57][380/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.1702e-01 (8.1751e-02)	Acc@1  95.31 ( 97.12)	Acc@5 100.00 ( 99.98)
Epoch: [57][390/391]	Time  0.028 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.8715e-01 (8.2092e-02)	Acc@1  92.50 ( 97.11)	Acc@5 100.00 ( 99.98)
## e[57] optimizer.zero_grad (sum) time: 0.26222848892211914
## e[57]       loss.backward (sum) time: 3.960667610168457
## e[57]      optimizer.step (sum) time: 1.7955121994018555
## epoch[57] training(only) time: 16.001707315444946
# Switched to evaluate mode...
Test: [  0/100]	Time  0.169 ( 0.169)	Loss 3.5458e-01 (3.5458e-01)	Acc@1  89.00 ( 89.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.017 ( 0.032)	Loss 4.2007e-01 (3.4659e-01)	Acc@1  90.00 ( 89.64)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.019 ( 0.026)	Loss 6.0694e-01 (3.5819e-01)	Acc@1  84.00 ( 89.57)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.021 ( 0.024)	Loss 3.5745e-01 (3.7056e-01)	Acc@1  90.00 ( 89.90)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.018 ( 0.023)	Loss 3.8074e-01 (3.7821e-01)	Acc@1  91.00 ( 89.83)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.021 ( 0.022)	Loss 2.3857e-01 (3.6865e-01)	Acc@1  95.00 ( 90.10)	Acc@5 100.00 ( 99.69)
Test: [ 60/100]	Time  0.023 ( 0.022)	Loss 4.4765e-01 (3.7031e-01)	Acc@1  86.00 ( 90.02)	Acc@5  99.00 ( 99.70)
Test: [ 70/100]	Time  0.021 ( 0.022)	Loss 3.9120e-01 (3.6159e-01)	Acc@1  88.00 ( 90.03)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.021 ( 0.022)	Loss 2.4088e-01 (3.5920e-01)	Acc@1  93.00 ( 90.06)	Acc@5 100.00 ( 99.72)
Test: [ 90/100]	Time  0.024 ( 0.022)	Loss 2.4990e-01 (3.6200e-01)	Acc@1  93.00 ( 90.07)	Acc@5 100.00 ( 99.73)
 * Acc@1 90.250 Acc@5 99.730
### epoch[57] execution time: 18.257767915725708
EPOCH 58
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [58][  0/391]	Time  0.211 ( 0.211)	Data  0.169 ( 0.169)	Loss 7.9985e-02 (7.9985e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [58][ 10/391]	Time  0.040 ( 0.056)	Data  0.001 ( 0.016)	Loss 5.4397e-02 (6.5698e-02)	Acc@1  98.44 ( 97.94)	Acc@5 100.00 (100.00)
Epoch: [58][ 20/391]	Time  0.041 ( 0.050)	Data  0.001 ( 0.009)	Loss 4.9585e-02 (7.5256e-02)	Acc@1  97.66 ( 97.73)	Acc@5 100.00 (100.00)
Epoch: [58][ 30/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.006)	Loss 1.1351e-01 (7.6453e-02)	Acc@1  95.31 ( 97.61)	Acc@5 100.00 (100.00)
Epoch: [58][ 40/391]	Time  0.043 ( 0.045)	Data  0.001 ( 0.005)	Loss 7.9648e-02 (7.1332e-02)	Acc@1  96.09 ( 97.73)	Acc@5 100.00 (100.00)
Epoch: [58][ 50/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.004)	Loss 8.3867e-02 (7.0977e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [58][ 60/391]	Time  0.043 ( 0.043)	Data  0.001 ( 0.004)	Loss 7.3992e-02 (7.0081e-02)	Acc@1  96.88 ( 97.68)	Acc@5 100.00 (100.00)
Epoch: [58][ 70/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.1384e-02 (7.0453e-02)	Acc@1  97.66 ( 97.62)	Acc@5 100.00 (100.00)
Epoch: [58][ 80/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 9.5311e-02 (7.2761e-02)	Acc@1  96.88 ( 97.56)	Acc@5 100.00 (100.00)
Epoch: [58][ 90/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 9.9138e-02 (7.3810e-02)	Acc@1  96.88 ( 97.51)	Acc@5 100.00 ( 99.99)
Epoch: [58][100/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.6898e-02 (7.3314e-02)	Acc@1  96.88 ( 97.54)	Acc@5 100.00 ( 99.99)
Epoch: [58][110/391]	Time  0.046 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.7096e-01 (7.5600e-02)	Acc@1  93.75 ( 97.42)	Acc@5 100.00 ( 99.99)
Epoch: [58][120/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.0389e-01 (7.5510e-02)	Acc@1  96.09 ( 97.45)	Acc@5 100.00 ( 99.99)
Epoch: [58][130/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.2246e-02 (7.5212e-02)	Acc@1  96.09 ( 97.45)	Acc@5 100.00 ( 99.99)
Epoch: [58][140/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0904e-02 (7.5576e-02)	Acc@1  98.44 ( 97.43)	Acc@5 100.00 ( 99.99)
Epoch: [58][150/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.3066e-02 (7.6581e-02)	Acc@1  96.09 ( 97.36)	Acc@5 100.00 ( 99.99)
Epoch: [58][160/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.7105e-02 (7.7272e-02)	Acc@1  96.88 ( 97.34)	Acc@5 100.00 (100.00)
Epoch: [58][170/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.6615e-02 (7.6792e-02)	Acc@1  98.44 ( 97.36)	Acc@5 100.00 ( 99.99)
Epoch: [58][180/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.7068e-02 (7.6558e-02)	Acc@1  96.09 ( 97.36)	Acc@5 100.00 ( 99.99)
Epoch: [58][190/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.4761e-02 (7.6910e-02)	Acc@1  96.88 ( 97.35)	Acc@5 100.00 ( 99.99)
Epoch: [58][200/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.3334e-02 (7.6888e-02)	Acc@1  97.66 ( 97.35)	Acc@5 100.00 ( 99.99)
Epoch: [58][210/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.0331e-01 (7.7924e-02)	Acc@1  92.97 ( 97.31)	Acc@5 100.00 ( 99.99)
Epoch: [58][220/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9477e-02 (7.7843e-02)	Acc@1  98.44 ( 97.31)	Acc@5 100.00 ( 99.99)
Epoch: [58][230/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.9025e-02 (7.8437e-02)	Acc@1  96.88 ( 97.29)	Acc@5 100.00 ( 99.99)
Epoch: [58][240/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1218e-01 (7.8037e-02)	Acc@1  96.88 ( 97.30)	Acc@5 100.00 ( 99.99)
Epoch: [58][250/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.5780e-02 (7.8241e-02)	Acc@1  98.44 ( 97.31)	Acc@5 100.00 ( 99.99)
Epoch: [58][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2750e-02 (7.7348e-02)	Acc@1  98.44 ( 97.34)	Acc@5 100.00 ( 99.99)
Epoch: [58][270/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.2804e-02 (7.7325e-02)	Acc@1  96.88 ( 97.34)	Acc@5 100.00 ( 99.99)
Epoch: [58][280/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.5743e-02 (7.7067e-02)	Acc@1  97.66 ( 97.33)	Acc@5 100.00 ( 99.99)
Epoch: [58][290/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.5504e-02 (7.6973e-02)	Acc@1  96.88 ( 97.32)	Acc@5 100.00 ( 99.99)
Epoch: [58][300/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0648e-02 (7.7207e-02)	Acc@1  97.66 ( 97.31)	Acc@5 100.00 ( 99.99)
Epoch: [58][310/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.7699e-02 (7.7410e-02)	Acc@1  97.66 ( 97.30)	Acc@5 100.00 ( 99.99)
Epoch: [58][320/391]	Time  0.035 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1865e-01 (7.7764e-02)	Acc@1  95.31 ( 97.30)	Acc@5 100.00 ( 99.99)
Epoch: [58][330/391]	Time  0.045 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0090e-01 (7.8320e-02)	Acc@1  96.88 ( 97.30)	Acc@5 100.00 ( 99.99)
Epoch: [58][340/391]	Time  0.045 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.3602e-01 (7.8444e-02)	Acc@1  96.09 ( 97.29)	Acc@5 100.00 ( 99.99)
Epoch: [58][350/391]	Time  0.045 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.2639e-02 (7.8781e-02)	Acc@1  99.22 ( 97.28)	Acc@5 100.00 ( 99.99)
Epoch: [58][360/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.8204e-01 (7.9570e-02)	Acc@1  93.75 ( 97.25)	Acc@5  99.22 ( 99.99)
Epoch: [58][370/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.9682e-02 (7.9458e-02)	Acc@1  98.44 ( 97.25)	Acc@5 100.00 ( 99.99)
Epoch: [58][380/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.2528e-01 (7.9886e-02)	Acc@1  95.31 ( 97.24)	Acc@5 100.00 ( 99.99)
Epoch: [58][390/391]	Time  0.030 ( 0.040)	Data  0.001 ( 0.001)	Loss 8.6775e-02 (8.0038e-02)	Acc@1  95.00 ( 97.24)	Acc@5 100.00 ( 99.99)
## e[58] optimizer.zero_grad (sum) time: 0.26300787925720215
## e[58]       loss.backward (sum) time: 3.9685003757476807
## e[58]      optimizer.step (sum) time: 1.8499162197113037
## epoch[58] training(only) time: 15.901344060897827
# Switched to evaluate mode...
Test: [  0/100]	Time  0.171 ( 0.171)	Loss 2.8145e-01 (2.8145e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.021 ( 0.034)	Loss 4.2908e-01 (3.3296e-01)	Acc@1  90.00 ( 91.09)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.018 ( 0.027)	Loss 5.5284e-01 (3.4477e-01)	Acc@1  86.00 ( 90.43)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.019 ( 0.024)	Loss 3.7798e-01 (3.6374e-01)	Acc@1  90.00 ( 90.32)	Acc@5  99.00 ( 99.65)
Test: [ 40/100]	Time  0.023 ( 0.023)	Loss 3.8830e-01 (3.7346e-01)	Acc@1  90.00 ( 89.93)	Acc@5  99.00 ( 99.59)
Test: [ 50/100]	Time  0.020 ( 0.023)	Loss 2.0542e-01 (3.5797e-01)	Acc@1  93.00 ( 90.27)	Acc@5 100.00 ( 99.63)
Test: [ 60/100]	Time  0.021 ( 0.022)	Loss 4.1858e-01 (3.5906e-01)	Acc@1  89.00 ( 90.21)	Acc@5  98.00 ( 99.64)
Test: [ 70/100]	Time  0.018 ( 0.022)	Loss 4.0507e-01 (3.5253e-01)	Acc@1  88.00 ( 90.21)	Acc@5 100.00 ( 99.66)
Test: [ 80/100]	Time  0.023 ( 0.022)	Loss 2.2290e-01 (3.4884e-01)	Acc@1  95.00 ( 90.32)	Acc@5 100.00 ( 99.64)
Test: [ 90/100]	Time  0.022 ( 0.022)	Loss 2.3050e-01 (3.5279e-01)	Acc@1  93.00 ( 90.24)	Acc@5 100.00 ( 99.65)
 * Acc@1 90.350 Acc@5 99.660
### epoch[58] execution time: 18.159937381744385
EPOCH 59
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [59][  0/391]	Time  0.211 ( 0.211)	Data  0.170 ( 0.170)	Loss 6.2393e-02 (6.2393e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [59][ 10/391]	Time  0.039 ( 0.056)	Data  0.001 ( 0.016)	Loss 7.4992e-02 (8.5217e-02)	Acc@1  97.66 ( 96.80)	Acc@5 100.00 (100.00)
Epoch: [59][ 20/391]	Time  0.039 ( 0.048)	Data  0.001 ( 0.009)	Loss 8.3883e-02 (7.8694e-02)	Acc@1  96.88 ( 97.17)	Acc@5 100.00 (100.00)
Epoch: [59][ 30/391]	Time  0.040 ( 0.045)	Data  0.001 ( 0.006)	Loss 6.6106e-02 (7.5858e-02)	Acc@1  98.44 ( 97.38)	Acc@5 100.00 (100.00)
Epoch: [59][ 40/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.005)	Loss 4.4725e-02 (7.6741e-02)	Acc@1  99.22 ( 97.52)	Acc@5 100.00 ( 99.96)
Epoch: [59][ 50/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.0315e-01 (7.7059e-02)	Acc@1  96.09 ( 97.55)	Acc@5 100.00 ( 99.97)
Epoch: [59][ 60/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.004)	Loss 7.4625e-02 (7.6019e-02)	Acc@1  96.88 ( 97.59)	Acc@5 100.00 ( 99.97)
Epoch: [59][ 70/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.003)	Loss 9.2298e-02 (7.7699e-02)	Acc@1  96.88 ( 97.47)	Acc@5 100.00 ( 99.98)
Epoch: [59][ 80/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.5756e-02 (7.7491e-02)	Acc@1  99.22 ( 97.48)	Acc@5 100.00 ( 99.97)
Epoch: [59][ 90/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.1896e-01 (7.7801e-02)	Acc@1  96.09 ( 97.45)	Acc@5 100.00 ( 99.97)
Epoch: [59][100/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.4967e-02 (7.7778e-02)	Acc@1 100.00 ( 97.50)	Acc@5 100.00 ( 99.98)
Epoch: [59][110/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.3604e-02 (7.6692e-02)	Acc@1  99.22 ( 97.54)	Acc@5 100.00 ( 99.98)
Epoch: [59][120/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.8653e-02 (7.5886e-02)	Acc@1  98.44 ( 97.55)	Acc@5 100.00 ( 99.98)
Epoch: [59][130/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.7284e-02 (7.4970e-02)	Acc@1  97.66 ( 97.57)	Acc@5 100.00 ( 99.98)
Epoch: [59][140/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.3272e-02 (7.6609e-02)	Acc@1  98.44 ( 97.48)	Acc@5 100.00 ( 99.98)
Epoch: [59][150/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0846e-01 (7.6950e-02)	Acc@1  96.09 ( 97.46)	Acc@5 100.00 ( 99.98)
Epoch: [59][160/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4351e-01 (7.6220e-02)	Acc@1  95.31 ( 97.49)	Acc@5 100.00 ( 99.99)
Epoch: [59][170/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5182e-01 (7.6741e-02)	Acc@1  96.09 ( 97.46)	Acc@5 100.00 ( 99.99)
Epoch: [59][180/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.6576e-02 (7.6545e-02)	Acc@1  96.88 ( 97.46)	Acc@5 100.00 ( 99.99)
Epoch: [59][190/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.1468e-02 (7.7259e-02)	Acc@1  98.44 ( 97.46)	Acc@5 100.00 ( 99.98)
Epoch: [59][200/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.7966e-02 (7.7747e-02)	Acc@1  96.88 ( 97.44)	Acc@5 100.00 ( 99.98)
Epoch: [59][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0228e-02 (7.7896e-02)	Acc@1  98.44 ( 97.43)	Acc@5 100.00 ( 99.99)
Epoch: [59][220/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.1730e-02 (7.7666e-02)	Acc@1  96.88 ( 97.42)	Acc@5 100.00 ( 99.99)
Epoch: [59][230/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.4864e-02 (7.6737e-02)	Acc@1  97.66 ( 97.46)	Acc@5 100.00 ( 99.99)
Epoch: [59][240/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.6676e-02 (7.7174e-02)	Acc@1  96.09 ( 97.43)	Acc@5 100.00 ( 99.99)
Epoch: [59][250/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.7251e-02 (7.7202e-02)	Acc@1  96.88 ( 97.44)	Acc@5 100.00 ( 99.99)
Epoch: [59][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2376e-02 (7.6968e-02)	Acc@1 100.00 ( 97.45)	Acc@5 100.00 ( 99.99)
Epoch: [59][270/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.8290e-02 (7.7049e-02)	Acc@1  99.22 ( 97.44)	Acc@5 100.00 ( 99.99)
Epoch: [59][280/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6130e-01 (7.7426e-02)	Acc@1  95.31 ( 97.43)	Acc@5 100.00 ( 99.99)
Epoch: [59][290/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.4284e-02 (7.8171e-02)	Acc@1  98.44 ( 97.38)	Acc@5 100.00 ( 99.99)
Epoch: [59][300/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.0288e-02 (7.8893e-02)	Acc@1  98.44 ( 97.34)	Acc@5 100.00 ( 99.98)
Epoch: [59][310/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0027e-01 (7.9080e-02)	Acc@1  97.66 ( 97.32)	Acc@5 100.00 ( 99.98)
Epoch: [59][320/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.5348e-02 (7.8886e-02)	Acc@1  99.22 ( 97.33)	Acc@5 100.00 ( 99.99)
Epoch: [59][330/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.5574e-02 (7.8858e-02)	Acc@1  94.53 ( 97.32)	Acc@5 100.00 ( 99.99)
Epoch: [59][340/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.0872e-01 (7.9589e-02)	Acc@1  97.66 ( 97.29)	Acc@5 100.00 ( 99.99)
Epoch: [59][350/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.5526e-02 (7.9286e-02)	Acc@1  98.44 ( 97.29)	Acc@5 100.00 ( 99.98)
Epoch: [59][360/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.2924e-02 (7.9106e-02)	Acc@1  98.44 ( 97.31)	Acc@5 100.00 ( 99.98)
Epoch: [59][370/391]	Time  0.043 ( 0.041)	Data  0.002 ( 0.001)	Loss 9.9156e-02 (7.8729e-02)	Acc@1  96.09 ( 97.32)	Acc@5 100.00 ( 99.98)
Epoch: [59][380/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.8071e-02 (7.8507e-02)	Acc@1  97.66 ( 97.34)	Acc@5 100.00 ( 99.98)
Epoch: [59][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 8.4315e-02 (7.8220e-02)	Acc@1  97.50 ( 97.35)	Acc@5 100.00 ( 99.98)
## e[59] optimizer.zero_grad (sum) time: 0.2631955146789551
## e[59]       loss.backward (sum) time: 3.95527720451355
## e[59]      optimizer.step (sum) time: 1.8072052001953125
## epoch[59] training(only) time: 15.985179424285889
# Switched to evaluate mode...
Test: [  0/100]	Time  0.223 ( 0.223)	Loss 3.4295e-01 (3.4295e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.017 ( 0.040)	Loss 3.7587e-01 (3.1964e-01)	Acc@1  90.00 ( 90.91)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.023 ( 0.032)	Loss 5.1618e-01 (3.3350e-01)	Acc@1  87.00 ( 90.86)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.021 ( 0.028)	Loss 3.5635e-01 (3.4483e-01)	Acc@1  88.00 ( 90.71)	Acc@5 100.00 ( 99.81)
Test: [ 40/100]	Time  0.022 ( 0.027)	Loss 4.0069e-01 (3.5388e-01)	Acc@1  90.00 ( 90.49)	Acc@5  99.00 ( 99.76)
Test: [ 50/100]	Time  0.017 ( 0.026)	Loss 2.7938e-01 (3.4300e-01)	Acc@1  92.00 ( 90.82)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 4.1672e-01 (3.5184e-01)	Acc@1  89.00 ( 90.48)	Acc@5  99.00 ( 99.74)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 3.4978e-01 (3.3992e-01)	Acc@1  92.00 ( 90.66)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 2.1606e-01 (3.3648e-01)	Acc@1  94.00 ( 90.78)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.021 ( 0.023)	Loss 2.0602e-01 (3.3858e-01)	Acc@1  95.00 ( 90.73)	Acc@5 100.00 ( 99.78)
 * Acc@1 90.790 Acc@5 99.780
### epoch[59] execution time: 18.38684344291687
EPOCH 60
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [60][  0/391]	Time  0.260 ( 0.260)	Data  0.219 ( 0.219)	Loss 6.9142e-02 (6.9142e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [60][ 10/391]	Time  0.041 ( 0.061)	Data  0.001 ( 0.021)	Loss 1.0748e-01 (7.9035e-02)	Acc@1  96.88 ( 97.51)	Acc@5 100.00 (100.00)
Epoch: [60][ 20/391]	Time  0.040 ( 0.052)	Data  0.002 ( 0.011)	Loss 8.2019e-02 (7.7424e-02)	Acc@1  97.66 ( 97.51)	Acc@5 100.00 (100.00)
Epoch: [60][ 30/391]	Time  0.039 ( 0.048)	Data  0.001 ( 0.008)	Loss 4.7705e-02 (7.5565e-02)	Acc@1  98.44 ( 97.61)	Acc@5 100.00 (100.00)
Epoch: [60][ 40/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.006)	Loss 1.0808e-01 (7.5505e-02)	Acc@1  95.31 ( 97.60)	Acc@5 100.00 (100.00)
Epoch: [60][ 50/391]	Time  0.044 ( 0.045)	Data  0.001 ( 0.005)	Loss 4.8024e-02 (7.5067e-02)	Acc@1  98.44 ( 97.55)	Acc@5 100.00 (100.00)
Epoch: [60][ 60/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.005)	Loss 4.3257e-02 (7.4455e-02)	Acc@1  98.44 ( 97.59)	Acc@5 100.00 ( 99.99)
Epoch: [60][ 70/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.004)	Loss 7.2126e-02 (7.4529e-02)	Acc@1  98.44 ( 97.56)	Acc@5 100.00 ( 99.99)
Epoch: [60][ 80/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.004)	Loss 7.7717e-02 (7.3818e-02)	Acc@1  96.88 ( 97.60)	Acc@5  99.22 ( 99.98)
Epoch: [60][ 90/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 8.6437e-02 (7.3608e-02)	Acc@1  96.09 ( 97.59)	Acc@5 100.00 ( 99.97)
Epoch: [60][100/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.9680e-02 (7.2800e-02)	Acc@1  99.22 ( 97.59)	Acc@5 100.00 ( 99.98)
Epoch: [60][110/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.7526e-02 (7.3537e-02)	Acc@1  96.88 ( 97.54)	Acc@5 100.00 ( 99.98)
Epoch: [60][120/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.6945e-02 (7.2128e-02)	Acc@1  97.66 ( 97.59)	Acc@5 100.00 ( 99.98)
Epoch: [60][130/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.5793e-02 (7.2777e-02)	Acc@1  97.66 ( 97.60)	Acc@5 100.00 ( 99.98)
Epoch: [60][140/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 8.1418e-02 (7.2005e-02)	Acc@1  98.44 ( 97.66)	Acc@5 100.00 ( 99.98)
Epoch: [60][150/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 7.0940e-02 (7.1641e-02)	Acc@1  97.66 ( 97.68)	Acc@5 100.00 ( 99.98)
Epoch: [60][160/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.4595e-02 (7.1548e-02)	Acc@1  98.44 ( 97.71)	Acc@5 100.00 ( 99.98)
Epoch: [60][170/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.0357e-02 (7.1890e-02)	Acc@1  98.44 ( 97.69)	Acc@5 100.00 ( 99.98)
Epoch: [60][180/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9660e-02 (7.2042e-02)	Acc@1  98.44 ( 97.70)	Acc@5 100.00 ( 99.98)
Epoch: [60][190/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.5260e-02 (7.1001e-02)	Acc@1  98.44 ( 97.73)	Acc@5 100.00 ( 99.98)
Epoch: [60][200/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1470e-02 (7.0055e-02)	Acc@1  99.22 ( 97.78)	Acc@5 100.00 ( 99.98)
Epoch: [60][210/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.8233e-02 (6.9834e-02)	Acc@1  94.53 ( 97.76)	Acc@5 100.00 ( 99.98)
Epoch: [60][220/391]	Time  0.039 ( 0.041)	Data  0.002 ( 0.002)	Loss 7.8344e-02 (6.9699e-02)	Acc@1  98.44 ( 97.77)	Acc@5 100.00 ( 99.98)
Epoch: [60][230/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.5272e-02 (6.9782e-02)	Acc@1  96.09 ( 97.76)	Acc@5 100.00 ( 99.98)
Epoch: [60][240/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2586e-02 (6.9677e-02)	Acc@1  98.44 ( 97.75)	Acc@5 100.00 ( 99.98)
Epoch: [60][250/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2919e-02 (6.9673e-02)	Acc@1  98.44 ( 97.75)	Acc@5 100.00 ( 99.98)
Epoch: [60][260/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.8209e-02 (6.9828e-02)	Acc@1  97.66 ( 97.73)	Acc@5 100.00 ( 99.99)
Epoch: [60][270/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9343e-02 (6.9682e-02)	Acc@1  98.44 ( 97.74)	Acc@5 100.00 ( 99.99)
Epoch: [60][280/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.7308e-02 (6.9830e-02)	Acc@1  97.66 ( 97.73)	Acc@5 100.00 ( 99.98)
Epoch: [60][290/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.5040e-02 (6.9122e-02)	Acc@1  97.66 ( 97.76)	Acc@5 100.00 ( 99.98)
Epoch: [60][300/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.4781e-02 (6.8685e-02)	Acc@1  96.09 ( 97.77)	Acc@5 100.00 ( 99.98)
Epoch: [60][310/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.5653e-02 (6.8269e-02)	Acc@1  99.22 ( 97.78)	Acc@5 100.00 ( 99.98)
Epoch: [60][320/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3152e-01 (6.8218e-02)	Acc@1  95.31 ( 97.79)	Acc@5 100.00 ( 99.99)
Epoch: [60][330/391]	Time  0.051 ( 0.041)	Data  0.002 ( 0.002)	Loss 9.0906e-02 (6.7944e-02)	Acc@1  97.66 ( 97.80)	Acc@5 100.00 ( 99.99)
Epoch: [60][340/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7440e-02 (6.8285e-02)	Acc@1  98.44 ( 97.78)	Acc@5 100.00 ( 99.99)
Epoch: [60][350/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4046e-02 (6.8235e-02)	Acc@1  97.66 ( 97.78)	Acc@5 100.00 ( 99.99)
Epoch: [60][360/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.3243e-02 (6.7868e-02)	Acc@1  99.22 ( 97.80)	Acc@5 100.00 ( 99.98)
Epoch: [60][370/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2796e-02 (6.7389e-02)	Acc@1  98.44 ( 97.82)	Acc@5 100.00 ( 99.99)
Epoch: [60][380/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6101e-02 (6.7192e-02)	Acc@1  98.44 ( 97.83)	Acc@5 100.00 ( 99.99)
Epoch: [60][390/391]	Time  0.028 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.2327e-02 (6.6962e-02)	Acc@1  96.25 ( 97.84)	Acc@5 100.00 ( 99.99)
## e[60] optimizer.zero_grad (sum) time: 0.26343822479248047
## e[60]       loss.backward (sum) time: 3.94785737991333
## e[60]      optimizer.step (sum) time: 1.8231639862060547
## epoch[60] training(only) time: 15.975722789764404
# Switched to evaluate mode...
Test: [  0/100]	Time  0.174 ( 0.174)	Loss 3.1589e-01 (3.1589e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.018 ( 0.033)	Loss 3.6758e-01 (3.0651e-01)	Acc@1  90.00 ( 90.36)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.022 ( 0.027)	Loss 5.1885e-01 (3.1623e-01)	Acc@1  88.00 ( 90.81)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.018 ( 0.025)	Loss 3.5818e-01 (3.3375e-01)	Acc@1  89.00 ( 90.68)	Acc@5 100.00 ( 99.81)
Test: [ 40/100]	Time  0.018 ( 0.024)	Loss 3.5867e-01 (3.4591e-01)	Acc@1  92.00 ( 90.49)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.020 ( 0.023)	Loss 2.4866e-01 (3.3456e-01)	Acc@1  95.00 ( 90.92)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.019 ( 0.022)	Loss 3.9824e-01 (3.4156e-01)	Acc@1  90.00 ( 90.67)	Acc@5  99.00 ( 99.74)
Test: [ 70/100]	Time  0.019 ( 0.022)	Loss 3.3397e-01 (3.2892e-01)	Acc@1  90.00 ( 90.80)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.017 ( 0.022)	Loss 2.1658e-01 (3.2529e-01)	Acc@1  94.00 ( 90.98)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.017 ( 0.022)	Loss 2.1313e-01 (3.2589e-01)	Acc@1  95.00 ( 90.97)	Acc@5 100.00 ( 99.75)
 * Acc@1 91.010 Acc@5 99.750
### epoch[60] execution time: 18.220815420150757
EPOCH 61
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [61][  0/391]	Time  0.254 ( 0.254)	Data  0.213 ( 0.213)	Loss 4.7066e-02 (4.7066e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [61][ 10/391]	Time  0.040 ( 0.060)	Data  0.001 ( 0.020)	Loss 5.5016e-02 (4.4063e-02)	Acc@1  97.66 ( 98.86)	Acc@5 100.00 (100.00)
Epoch: [61][ 20/391]	Time  0.038 ( 0.051)	Data  0.001 ( 0.011)	Loss 1.1157e-01 (5.8212e-02)	Acc@1  97.66 ( 98.33)	Acc@5 100.00 (100.00)
Epoch: [61][ 30/391]	Time  0.038 ( 0.047)	Data  0.001 ( 0.008)	Loss 7.4291e-02 (5.7504e-02)	Acc@1  96.09 ( 98.19)	Acc@5 100.00 (100.00)
Epoch: [61][ 40/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.006)	Loss 2.8969e-02 (6.0387e-02)	Acc@1 100.00 ( 97.98)	Acc@5 100.00 (100.00)
Epoch: [61][ 50/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.005)	Loss 9.0854e-02 (6.0156e-02)	Acc@1  97.66 ( 98.04)	Acc@5 100.00 (100.00)
Epoch: [61][ 60/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.004)	Loss 5.6003e-02 (6.1630e-02)	Acc@1  99.22 ( 98.04)	Acc@5 100.00 (100.00)
Epoch: [61][ 70/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.004)	Loss 5.7756e-02 (6.1704e-02)	Acc@1  97.66 ( 98.00)	Acc@5 100.00 (100.00)
Epoch: [61][ 80/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.004)	Loss 8.0842e-02 (6.1747e-02)	Acc@1  96.09 ( 97.98)	Acc@5 100.00 (100.00)
Epoch: [61][ 90/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.7989e-02 (6.1008e-02)	Acc@1 100.00 ( 97.99)	Acc@5 100.00 (100.00)
Epoch: [61][100/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.0452e-02 (5.9627e-02)	Acc@1  98.44 ( 98.05)	Acc@5 100.00 (100.00)
Epoch: [61][110/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.1514e-01 (5.9713e-02)	Acc@1  96.09 ( 98.06)	Acc@5 100.00 (100.00)
Epoch: [61][120/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.5223e-02 (5.9586e-02)	Acc@1  97.66 ( 98.06)	Acc@5 100.00 (100.00)
Epoch: [61][130/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.6951e-02 (5.9213e-02)	Acc@1  99.22 ( 98.08)	Acc@5 100.00 (100.00)
Epoch: [61][140/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.0373e-01 (5.9568e-02)	Acc@1  96.09 ( 98.07)	Acc@5 100.00 (100.00)
Epoch: [61][150/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 8.8593e-02 (5.9865e-02)	Acc@1  96.88 ( 98.05)	Acc@5 100.00 (100.00)
Epoch: [61][160/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.2369e-02 (5.9881e-02)	Acc@1  99.22 ( 98.05)	Acc@5 100.00 (100.00)
Epoch: [61][170/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.4529e-02 (6.0919e-02)	Acc@1  98.44 ( 98.02)	Acc@5 100.00 (100.00)
Epoch: [61][180/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.3648e-02 (6.1272e-02)	Acc@1  98.44 ( 97.99)	Acc@5 100.00 (100.00)
Epoch: [61][190/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2424e-01 (6.1857e-02)	Acc@1  96.09 ( 97.95)	Acc@5 100.00 (100.00)
Epoch: [61][200/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.3055e-02 (6.1563e-02)	Acc@1  99.22 ( 97.99)	Acc@5 100.00 (100.00)
Epoch: [61][210/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0686e-01 (6.1835e-02)	Acc@1  96.88 ( 97.99)	Acc@5 100.00 (100.00)
Epoch: [61][220/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.6123e-02 (6.1794e-02)	Acc@1  97.66 ( 98.00)	Acc@5 100.00 (100.00)
Epoch: [61][230/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.0315e-02 (6.1910e-02)	Acc@1  97.66 ( 97.99)	Acc@5 100.00 (100.00)
Epoch: [61][240/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.9806e-02 (6.1913e-02)	Acc@1  96.09 ( 97.99)	Acc@5 100.00 (100.00)
Epoch: [61][250/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.7593e-02 (6.2229e-02)	Acc@1  96.09 ( 97.98)	Acc@5 100.00 ( 99.99)
Epoch: [61][260/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.4560e-02 (6.1955e-02)	Acc@1  98.44 ( 97.98)	Acc@5 100.00 ( 99.99)
Epoch: [61][270/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.0277e-02 (6.2191e-02)	Acc@1  98.44 ( 97.98)	Acc@5 100.00 ( 99.99)
Epoch: [61][280/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7251e-02 (6.2494e-02)	Acc@1  99.22 ( 97.96)	Acc@5 100.00 ( 99.99)
Epoch: [61][290/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.2003e-02 (6.2731e-02)	Acc@1  96.09 ( 97.95)	Acc@5 100.00 ( 99.99)
Epoch: [61][300/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9497e-02 (6.2507e-02)	Acc@1  98.44 ( 97.95)	Acc@5 100.00 ( 99.99)
Epoch: [61][310/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4782e-02 (6.2383e-02)	Acc@1  99.22 ( 97.95)	Acc@5 100.00 ( 99.99)
Epoch: [61][320/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3960e-02 (6.2546e-02)	Acc@1 100.00 ( 97.95)	Acc@5 100.00 ( 99.99)
Epoch: [61][330/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.4545e-02 (6.2522e-02)	Acc@1  96.09 ( 97.94)	Acc@5 100.00 ( 99.99)
Epoch: [61][340/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.6461e-02 (6.2585e-02)	Acc@1  97.66 ( 97.93)	Acc@5 100.00 ( 99.99)
Epoch: [61][350/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.3384e-02 (6.2672e-02)	Acc@1  97.66 ( 97.93)	Acc@5 100.00 ( 99.99)
Epoch: [61][360/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.5141e-02 (6.2973e-02)	Acc@1  98.44 ( 97.92)	Acc@5 100.00 ( 99.99)
Epoch: [61][370/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.5371e-02 (6.2650e-02)	Acc@1  99.22 ( 97.94)	Acc@5 100.00 ( 99.99)
Epoch: [61][380/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.3817e-02 (6.2443e-02)	Acc@1  98.44 ( 97.96)	Acc@5 100.00 ( 99.99)
Epoch: [61][390/391]	Time  0.033 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5173e-01 (6.2729e-02)	Acc@1  93.75 ( 97.94)	Acc@5 100.00 ( 99.99)
## e[61] optimizer.zero_grad (sum) time: 0.26105475425720215
## e[61]       loss.backward (sum) time: 4.003350734710693
## e[61]      optimizer.step (sum) time: 1.7773475646972656
## epoch[61] training(only) time: 16.077116012573242
# Switched to evaluate mode...
Test: [  0/100]	Time  0.167 ( 0.167)	Loss 3.2408e-01 (3.2408e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.018 ( 0.032)	Loss 3.6755e-01 (3.0735e-01)	Acc@1  91.00 ( 90.45)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.021 ( 0.027)	Loss 5.2781e-01 (3.2047e-01)	Acc@1  87.00 ( 90.57)	Acc@5 100.00 ( 99.90)
Test: [ 30/100]	Time  0.021 ( 0.025)	Loss 3.5042e-01 (3.3564e-01)	Acc@1  89.00 ( 90.68)	Acc@5 100.00 ( 99.87)
Test: [ 40/100]	Time  0.024 ( 0.024)	Loss 3.6858e-01 (3.4740e-01)	Acc@1  91.00 ( 90.39)	Acc@5 100.00 ( 99.80)
Test: [ 50/100]	Time  0.021 ( 0.023)	Loss 2.4407e-01 (3.3588e-01)	Acc@1  95.00 ( 90.73)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.023 ( 0.023)	Loss 3.8967e-01 (3.4202e-01)	Acc@1  90.00 ( 90.49)	Acc@5  99.00 ( 99.79)
Test: [ 70/100]	Time  0.022 ( 0.023)	Loss 3.5901e-01 (3.3049e-01)	Acc@1  88.00 ( 90.56)	Acc@5 100.00 ( 99.80)
Test: [ 80/100]	Time  0.021 ( 0.023)	Loss 2.1754e-01 (3.2676e-01)	Acc@1  94.00 ( 90.75)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.018 ( 0.023)	Loss 2.0972e-01 (3.2837e-01)	Acc@1  95.00 ( 90.80)	Acc@5 100.00 ( 99.78)
 * Acc@1 90.890 Acc@5 99.780
### epoch[61] execution time: 18.432575225830078
EPOCH 62
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [62][  0/391]	Time  0.222 ( 0.222)	Data  0.180 ( 0.180)	Loss 4.2988e-02 (4.2988e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [62][ 10/391]	Time  0.041 ( 0.056)	Data  0.001 ( 0.017)	Loss 4.9117e-02 (5.2417e-02)	Acc@1  97.66 ( 98.22)	Acc@5 100.00 (100.00)
Epoch: [62][ 20/391]	Time  0.040 ( 0.049)	Data  0.001 ( 0.009)	Loss 4.4249e-02 (5.4119e-02)	Acc@1  98.44 ( 98.18)	Acc@5 100.00 (100.00)
Epoch: [62][ 30/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.007)	Loss 8.2481e-02 (5.5865e-02)	Acc@1  96.09 ( 98.01)	Acc@5 100.00 (100.00)
Epoch: [62][ 40/391]	Time  0.038 ( 0.045)	Data  0.001 ( 0.005)	Loss 7.5543e-02 (5.7782e-02)	Acc@1  96.09 ( 98.08)	Acc@5 100.00 (100.00)
Epoch: [62][ 50/391]	Time  0.044 ( 0.044)	Data  0.001 ( 0.004)	Loss 5.0399e-02 (5.5050e-02)	Acc@1  98.44 ( 98.21)	Acc@5 100.00 (100.00)
Epoch: [62][ 60/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.004)	Loss 2.6393e-02 (5.4070e-02)	Acc@1 100.00 ( 98.25)	Acc@5 100.00 ( 99.99)
Epoch: [62][ 70/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.9561e-02 (5.6411e-02)	Acc@1  99.22 ( 98.10)	Acc@5 100.00 ( 99.99)
Epoch: [62][ 80/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.3194e-02 (5.5994e-02)	Acc@1  99.22 ( 98.12)	Acc@5 100.00 ( 99.99)
Epoch: [62][ 90/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 8.2412e-02 (5.6578e-02)	Acc@1  96.09 ( 98.09)	Acc@5 100.00 ( 99.99)
Epoch: [62][100/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.0927e-02 (5.6283e-02)	Acc@1  97.66 ( 98.07)	Acc@5 100.00 ( 99.99)
Epoch: [62][110/391]	Time  0.039 ( 0.042)	Data  0.002 ( 0.003)	Loss 6.5584e-02 (5.7789e-02)	Acc@1  98.44 ( 98.00)	Acc@5 100.00 ( 99.99)
Epoch: [62][120/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 7.1186e-02 (5.7604e-02)	Acc@1  98.44 ( 98.03)	Acc@5 100.00 ( 99.99)
Epoch: [62][130/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.3209e-02 (5.7961e-02)	Acc@1 100.00 ( 98.02)	Acc@5 100.00 ( 99.99)
Epoch: [62][140/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 9.5746e-02 (5.8704e-02)	Acc@1  96.88 ( 97.99)	Acc@5 100.00 ( 99.99)
Epoch: [62][150/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.002)	Loss 9.0436e-02 (5.9248e-02)	Acc@1  96.88 ( 97.97)	Acc@5 100.00 ( 99.99)
Epoch: [62][160/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.1491e-02 (5.9081e-02)	Acc@1 100.00 ( 97.99)	Acc@5 100.00 (100.00)
Epoch: [62][170/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.2906e-02 (5.9263e-02)	Acc@1  98.44 ( 97.99)	Acc@5 100.00 (100.00)
Epoch: [62][180/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.2470e-02 (5.9747e-02)	Acc@1  96.88 ( 97.96)	Acc@5 100.00 (100.00)
Epoch: [62][190/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.8774e-02 (6.0024e-02)	Acc@1  98.44 ( 97.93)	Acc@5 100.00 (100.00)
Epoch: [62][200/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5693e-02 (5.9969e-02)	Acc@1  98.44 ( 97.95)	Acc@5 100.00 (100.00)
Epoch: [62][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7825e-02 (6.0117e-02)	Acc@1 100.00 ( 97.93)	Acc@5 100.00 (100.00)
Epoch: [62][220/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.9037e-02 (6.0290e-02)	Acc@1  97.66 ( 97.92)	Acc@5 100.00 (100.00)
Epoch: [62][230/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2771e-01 (6.0746e-02)	Acc@1  94.53 ( 97.89)	Acc@5 100.00 (100.00)
Epoch: [62][240/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1293e-01 (6.1185e-02)	Acc@1  96.88 ( 97.90)	Acc@5 100.00 ( 99.99)
Epoch: [62][250/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2486e-01 (6.1497e-02)	Acc@1  94.53 ( 97.88)	Acc@5 100.00 ( 99.99)
Epoch: [62][260/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4203e-02 (6.0837e-02)	Acc@1  98.44 ( 97.91)	Acc@5 100.00 ( 99.99)
Epoch: [62][270/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1789e-02 (6.0962e-02)	Acc@1  98.44 ( 97.90)	Acc@5 100.00 ( 99.99)
Epoch: [62][280/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.6448e-02 (6.1806e-02)	Acc@1  98.44 ( 97.86)	Acc@5 100.00 ( 99.99)
Epoch: [62][290/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4934e-02 (6.1840e-02)	Acc@1  98.44 ( 97.87)	Acc@5 100.00 ( 99.99)
Epoch: [62][300/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7540e-02 (6.1614e-02)	Acc@1  97.66 ( 97.88)	Acc@5 100.00 ( 99.99)
Epoch: [62][310/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.9166e-02 (6.2000e-02)	Acc@1  95.31 ( 97.87)	Acc@5 100.00 ( 99.99)
Epoch: [62][320/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.7091e-02 (6.2312e-02)	Acc@1  98.44 ( 97.86)	Acc@5 100.00 ( 99.99)
Epoch: [62][330/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 8.4400e-02 (6.2469e-02)	Acc@1  96.88 ( 97.86)	Acc@5 100.00 ( 99.99)
Epoch: [62][340/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.2871e-02 (6.2410e-02)	Acc@1  96.09 ( 97.86)	Acc@5 100.00 ( 99.99)
Epoch: [62][350/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.3579e-02 (6.2313e-02)	Acc@1  99.22 ( 97.87)	Acc@5 100.00 ( 99.99)
Epoch: [62][360/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.1652e-02 (6.1999e-02)	Acc@1  98.44 ( 97.89)	Acc@5 100.00 ( 99.99)
Epoch: [62][370/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.9104e-02 (6.1899e-02)	Acc@1  97.66 ( 97.90)	Acc@5 100.00 ( 99.99)
Epoch: [62][380/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.5731e-02 (6.1760e-02)	Acc@1  99.22 ( 97.90)	Acc@5 100.00 ( 99.99)
Epoch: [62][390/391]	Time  0.029 ( 0.041)	Data  0.001 ( 0.001)	Loss 9.0395e-02 (6.1828e-02)	Acc@1  97.50 ( 97.91)	Acc@5 100.00 ( 99.99)
## e[62] optimizer.zero_grad (sum) time: 0.26257991790771484
## e[62]       loss.backward (sum) time: 4.000760555267334
## e[62]      optimizer.step (sum) time: 1.792607069015503
## epoch[62] training(only) time: 15.987862348556519
# Switched to evaluate mode...
Test: [  0/100]	Time  0.220 ( 0.220)	Loss 3.4272e-01 (3.4272e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.021 ( 0.039)	Loss 4.0854e-01 (3.1314e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.017 ( 0.031)	Loss 5.4123e-01 (3.2498e-01)	Acc@1  88.00 ( 91.05)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.021 ( 0.027)	Loss 3.6682e-01 (3.4060e-01)	Acc@1  88.00 ( 90.87)	Acc@5 100.00 ( 99.84)
Test: [ 40/100]	Time  0.021 ( 0.026)	Loss 3.5183e-01 (3.5192e-01)	Acc@1  92.00 ( 90.51)	Acc@5 100.00 ( 99.78)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 2.6355e-01 (3.4128e-01)	Acc@1  95.00 ( 90.80)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.021 ( 0.024)	Loss 4.0290e-01 (3.4895e-01)	Acc@1  90.00 ( 90.56)	Acc@5  99.00 ( 99.77)
Test: [ 70/100]	Time  0.021 ( 0.024)	Loss 3.6281e-01 (3.3644e-01)	Acc@1  88.00 ( 90.70)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.021 ( 0.024)	Loss 2.1493e-01 (3.3280e-01)	Acc@1  94.00 ( 90.86)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.017 ( 0.023)	Loss 2.2825e-01 (3.3425e-01)	Acc@1  94.00 ( 90.90)	Acc@5 100.00 ( 99.79)
 * Acc@1 91.000 Acc@5 99.790
### epoch[62] execution time: 18.432848691940308
EPOCH 63
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [63][  0/391]	Time  0.234 ( 0.234)	Data  0.191 ( 0.191)	Loss 6.1248e-02 (6.1248e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [63][ 10/391]	Time  0.042 ( 0.058)	Data  0.001 ( 0.018)	Loss 3.4961e-02 (5.0340e-02)	Acc@1  99.22 ( 98.58)	Acc@5 100.00 (100.00)
Epoch: [63][ 20/391]	Time  0.039 ( 0.050)	Data  0.001 ( 0.010)	Loss 5.3798e-02 (5.0255e-02)	Acc@1  97.66 ( 98.47)	Acc@5 100.00 (100.00)
Epoch: [63][ 30/391]	Time  0.041 ( 0.047)	Data  0.001 ( 0.007)	Loss 5.9681e-02 (5.4463e-02)	Acc@1  97.66 ( 98.26)	Acc@5 100.00 (100.00)
Epoch: [63][ 40/391]	Time  0.038 ( 0.045)	Data  0.001 ( 0.006)	Loss 5.3969e-02 (5.3920e-02)	Acc@1  97.66 ( 98.29)	Acc@5 100.00 (100.00)
Epoch: [63][ 50/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.005)	Loss 6.9500e-02 (5.5232e-02)	Acc@1  97.66 ( 98.21)	Acc@5 100.00 (100.00)
Epoch: [63][ 60/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.004)	Loss 8.5275e-02 (5.6396e-02)	Acc@1  96.88 ( 98.17)	Acc@5 100.00 (100.00)
Epoch: [63][ 70/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.2391e-01 (5.8567e-02)	Acc@1  95.31 ( 98.10)	Acc@5 100.00 (100.00)
Epoch: [63][ 80/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 7.5804e-02 (5.8330e-02)	Acc@1  96.88 ( 98.06)	Acc@5 100.00 (100.00)
Epoch: [63][ 90/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.0421e-01 (5.7843e-02)	Acc@1  96.88 ( 98.09)	Acc@5 100.00 (100.00)
Epoch: [63][100/391]	Time  0.035 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.0770e-02 (5.7661e-02)	Acc@1  98.44 ( 98.07)	Acc@5 100.00 (100.00)
Epoch: [63][110/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.3369e-01 (5.8697e-02)	Acc@1  94.53 ( 98.02)	Acc@5 100.00 (100.00)
Epoch: [63][120/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.6277e-02 (5.8515e-02)	Acc@1 100.00 ( 98.02)	Acc@5 100.00 (100.00)
Epoch: [63][130/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3278e-02 (5.8849e-02)	Acc@1  98.44 ( 97.98)	Acc@5 100.00 (100.00)
Epoch: [63][140/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.1345e-02 (5.7803e-02)	Acc@1  96.09 ( 98.01)	Acc@5 100.00 (100.00)
Epoch: [63][150/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.6451e-02 (5.8160e-02)	Acc@1  98.44 ( 98.00)	Acc@5 100.00 ( 99.99)
Epoch: [63][160/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.3087e-02 (5.8479e-02)	Acc@1  96.09 ( 97.98)	Acc@5 100.00 (100.00)
Epoch: [63][170/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1225e-02 (5.9469e-02)	Acc@1  97.66 ( 97.93)	Acc@5 100.00 (100.00)
Epoch: [63][180/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.6797e-02 (5.9559e-02)	Acc@1  98.44 ( 97.93)	Acc@5 100.00 (100.00)
Epoch: [63][190/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.2757e-02 (5.9580e-02)	Acc@1  96.09 ( 97.93)	Acc@5 100.00 (100.00)
Epoch: [63][200/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.9162e-02 (5.9248e-02)	Acc@1  97.66 ( 97.94)	Acc@5 100.00 (100.00)
Epoch: [63][210/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.7381e-02 (5.9648e-02)	Acc@1  96.09 ( 97.91)	Acc@5 100.00 (100.00)
Epoch: [63][220/391]	Time  0.049 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.6686e-02 (5.9663e-02)	Acc@1  96.88 ( 97.91)	Acc@5 100.00 (100.00)
Epoch: [63][230/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.1240e-02 (5.9440e-02)	Acc@1  96.09 ( 97.92)	Acc@5 100.00 (100.00)
Epoch: [63][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.8791e-02 (5.9606e-02)	Acc@1  96.09 ( 97.92)	Acc@5 100.00 (100.00)
Epoch: [63][250/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3381e-02 (5.9380e-02)	Acc@1  98.44 ( 97.94)	Acc@5 100.00 (100.00)
Epoch: [63][260/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.8249e-02 (6.0299e-02)	Acc@1  96.88 ( 97.91)	Acc@5 100.00 (100.00)
Epoch: [63][270/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.6580e-02 (6.0641e-02)	Acc@1  97.66 ( 97.90)	Acc@5 100.00 (100.00)
Epoch: [63][280/391]	Time  0.046 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8531e-02 (6.0689e-02)	Acc@1  99.22 ( 97.90)	Acc@5 100.00 (100.00)
Epoch: [63][290/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1290e-02 (6.1175e-02)	Acc@1  98.44 ( 97.86)	Acc@5 100.00 (100.00)
Epoch: [63][300/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.5007e-02 (6.1152e-02)	Acc@1  99.22 ( 97.87)	Acc@5 100.00 (100.00)
Epoch: [63][310/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9534e-02 (6.1420e-02)	Acc@1  98.44 ( 97.86)	Acc@5 100.00 (100.00)
Epoch: [63][320/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2769e-02 (6.1077e-02)	Acc@1  99.22 ( 97.88)	Acc@5 100.00 (100.00)
Epoch: [63][330/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5876e-02 (6.0862e-02)	Acc@1  98.44 ( 97.88)	Acc@5 100.00 (100.00)
Epoch: [63][340/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.6369e-02 (6.0796e-02)	Acc@1  96.88 ( 97.90)	Acc@5 100.00 (100.00)
Epoch: [63][350/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.9720e-02 (6.0669e-02)	Acc@1  98.44 ( 97.91)	Acc@5 100.00 (100.00)
Epoch: [63][360/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.001)	Loss 9.2334e-02 (6.0852e-02)	Acc@1  96.88 ( 97.90)	Acc@5 100.00 (100.00)
Epoch: [63][370/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 9.7322e-02 (6.0671e-02)	Acc@1  97.66 ( 97.91)	Acc@5 100.00 (100.00)
Epoch: [63][380/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.2333e-02 (6.0671e-02)	Acc@1  96.88 ( 97.91)	Acc@5 100.00 (100.00)
Epoch: [63][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.2655e-02 (6.0539e-02)	Acc@1  98.75 ( 97.92)	Acc@5 100.00 (100.00)
## e[63] optimizer.zero_grad (sum) time: 0.26215410232543945
## e[63]       loss.backward (sum) time: 3.9602322578430176
## e[63]      optimizer.step (sum) time: 1.8134222030639648
## epoch[63] training(only) time: 15.979368209838867
# Switched to evaluate mode...
Test: [  0/100]	Time  0.196 ( 0.196)	Loss 3.4462e-01 (3.4462e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.022 ( 0.037)	Loss 3.8181e-01 (3.1582e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.021 ( 0.029)	Loss 5.0576e-01 (3.2561e-01)	Acc@1  87.00 ( 90.81)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.023 ( 0.026)	Loss 3.8037e-01 (3.4315e-01)	Acc@1  90.00 ( 90.65)	Acc@5  99.00 ( 99.81)
Test: [ 40/100]	Time  0.021 ( 0.025)	Loss 3.6957e-01 (3.5203e-01)	Acc@1  90.00 ( 90.44)	Acc@5 100.00 ( 99.76)
Test: [ 50/100]	Time  0.020 ( 0.024)	Loss 2.6339e-01 (3.4067e-01)	Acc@1  95.00 ( 90.80)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.020 ( 0.024)	Loss 3.9564e-01 (3.4777e-01)	Acc@1  91.00 ( 90.59)	Acc@5  99.00 ( 99.74)
Test: [ 70/100]	Time  0.021 ( 0.023)	Loss 3.9731e-01 (3.3558e-01)	Acc@1  88.00 ( 90.62)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.021 ( 0.023)	Loss 2.3811e-01 (3.3238e-01)	Acc@1  94.00 ( 90.75)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.021 ( 0.023)	Loss 2.3344e-01 (3.3313e-01)	Acc@1  94.00 ( 90.78)	Acc@5 100.00 ( 99.76)
 * Acc@1 90.880 Acc@5 99.760
### epoch[63] execution time: 18.353419303894043
EPOCH 64
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [64][  0/391]	Time  0.218 ( 0.218)	Data  0.173 ( 0.173)	Loss 6.9456e-02 (6.9456e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [64][ 10/391]	Time  0.041 ( 0.057)	Data  0.001 ( 0.017)	Loss 5.4091e-02 (5.9893e-02)	Acc@1  98.44 ( 98.15)	Acc@5 100.00 (100.00)
Epoch: [64][ 20/391]	Time  0.041 ( 0.050)	Data  0.001 ( 0.009)	Loss 9.5245e-02 (6.9629e-02)	Acc@1  96.09 ( 97.51)	Acc@5 100.00 (100.00)
Epoch: [64][ 30/391]	Time  0.038 ( 0.047)	Data  0.001 ( 0.007)	Loss 5.2732e-02 (6.7007e-02)	Acc@1  97.66 ( 97.68)	Acc@5 100.00 (100.00)
Epoch: [64][ 40/391]	Time  0.039 ( 0.045)	Data  0.001 ( 0.005)	Loss 6.8277e-02 (6.2759e-02)	Acc@1  97.66 ( 97.90)	Acc@5 100.00 (100.00)
Epoch: [64][ 50/391]	Time  0.039 ( 0.044)	Data  0.002 ( 0.004)	Loss 3.0189e-02 (5.9930e-02)	Acc@1 100.00 ( 98.05)	Acc@5 100.00 (100.00)
Epoch: [64][ 60/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.004)	Loss 5.2312e-02 (6.0811e-02)	Acc@1  97.66 ( 98.07)	Acc@5 100.00 (100.00)
Epoch: [64][ 70/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.7056e-02 (5.9717e-02)	Acc@1 100.00 ( 98.12)	Acc@5 100.00 (100.00)
Epoch: [64][ 80/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.003)	Loss 8.1184e-02 (5.9333e-02)	Acc@1  96.88 ( 98.15)	Acc@5 100.00 (100.00)
Epoch: [64][ 90/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.0103e-01 (5.9017e-02)	Acc@1  96.88 ( 98.18)	Acc@5  99.22 ( 99.99)
Epoch: [64][100/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.8897e-02 (5.9268e-02)	Acc@1  98.44 ( 98.20)	Acc@5 100.00 ( 99.99)
Epoch: [64][110/391]	Time  0.046 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.2994e-02 (5.9859e-02)	Acc@1  98.44 ( 98.14)	Acc@5 100.00 ( 99.99)
Epoch: [64][120/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.6828e-02 (5.8895e-02)	Acc@1  98.44 ( 98.17)	Acc@5 100.00 ( 99.99)
Epoch: [64][130/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 9.9467e-02 (5.9304e-02)	Acc@1  96.09 ( 98.17)	Acc@5 100.00 ( 99.99)
Epoch: [64][140/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 7.1342e-02 (5.8917e-02)	Acc@1  96.88 ( 98.18)	Acc@5 100.00 ( 99.99)
Epoch: [64][150/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.3748e-02 (5.8670e-02)	Acc@1  98.44 ( 98.17)	Acc@5 100.00 ( 99.99)
Epoch: [64][160/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1483e-02 (5.8501e-02)	Acc@1  98.44 ( 98.15)	Acc@5 100.00 ( 99.99)
Epoch: [64][170/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.3455e-02 (5.8844e-02)	Acc@1  97.66 ( 98.11)	Acc@5 100.00 ( 99.99)
Epoch: [64][180/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.4697e-02 (5.9327e-02)	Acc@1  98.44 ( 98.08)	Acc@5 100.00 ( 99.99)
Epoch: [64][190/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.0698e-02 (5.9641e-02)	Acc@1  96.88 ( 98.08)	Acc@5 100.00 ( 99.99)
Epoch: [64][200/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.9578e-02 (5.9491e-02)	Acc@1  96.09 ( 98.07)	Acc@5 100.00 ( 99.99)
Epoch: [64][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1409e-02 (5.9420e-02)	Acc@1  99.22 ( 98.05)	Acc@5 100.00 ( 99.99)
Epoch: [64][220/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0161e-02 (5.9661e-02)	Acc@1  99.22 ( 98.02)	Acc@5 100.00 ( 99.99)
Epoch: [64][230/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.5599e-02 (6.0139e-02)	Acc@1  97.66 ( 98.00)	Acc@5 100.00 ( 99.99)
Epoch: [64][240/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1314e-02 (5.9740e-02)	Acc@1  99.22 ( 98.03)	Acc@5 100.00 ( 99.99)
Epoch: [64][250/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3832e-01 (6.0045e-02)	Acc@1  94.53 ( 98.01)	Acc@5 100.00 ( 99.99)
Epoch: [64][260/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7143e-02 (6.0385e-02)	Acc@1  99.22 ( 98.00)	Acc@5 100.00 ( 99.99)
Epoch: [64][270/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.4405e-02 (6.0014e-02)	Acc@1  98.44 ( 98.00)	Acc@5 100.00 ( 99.99)
Epoch: [64][280/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.1846e-02 (5.9620e-02)	Acc@1  98.44 ( 98.01)	Acc@5 100.00 ( 99.99)
Epoch: [64][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.8605e-02 (5.9158e-02)	Acc@1  97.66 ( 98.03)	Acc@5 100.00 ( 99.99)
Epoch: [64][300/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2976e-02 (5.9405e-02)	Acc@1  99.22 ( 98.02)	Acc@5 100.00 ( 99.99)
Epoch: [64][310/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.3852e-02 (5.9135e-02)	Acc@1  97.66 ( 98.02)	Acc@5 100.00 ( 99.99)
Epoch: [64][320/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.3506e-02 (5.9251e-02)	Acc@1  97.66 ( 98.01)	Acc@5 100.00 (100.00)
Epoch: [64][330/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8021e-02 (5.9313e-02)	Acc@1  99.22 ( 98.02)	Acc@5 100.00 (100.00)
Epoch: [64][340/391]	Time  0.054 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.5522e-02 (5.9287e-02)	Acc@1  97.66 ( 98.01)	Acc@5 100.00 (100.00)
Epoch: [64][350/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.0175e-02 (5.9139e-02)	Acc@1  97.66 ( 98.01)	Acc@5 100.00 (100.00)
Epoch: [64][360/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.0287e-02 (5.9107e-02)	Acc@1  97.66 ( 98.00)	Acc@5 100.00 (100.00)
Epoch: [64][370/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.5769e-02 (5.9057e-02)	Acc@1  98.44 ( 98.00)	Acc@5 100.00 (100.00)
Epoch: [64][380/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.3745e-02 (5.8834e-02)	Acc@1  97.66 ( 98.01)	Acc@5 100.00 (100.00)
Epoch: [64][390/391]	Time  0.028 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.3605e-02 (5.8919e-02)	Acc@1 100.00 ( 98.01)	Acc@5 100.00 (100.00)
## e[64] optimizer.zero_grad (sum) time: 0.2632145881652832
## e[64]       loss.backward (sum) time: 4.0090718269348145
## e[64]      optimizer.step (sum) time: 1.8035552501678467
## epoch[64] training(only) time: 16.038944721221924
# Switched to evaluate mode...
Test: [  0/100]	Time  0.176 ( 0.176)	Loss 3.4322e-01 (3.4322e-01)	Acc@1  91.00 ( 91.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.023 ( 0.036)	Loss 3.9839e-01 (3.1511e-01)	Acc@1  91.00 ( 90.82)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.018 ( 0.028)	Loss 5.3942e-01 (3.2374e-01)	Acc@1  87.00 ( 91.00)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.023 ( 0.026)	Loss 3.7665e-01 (3.4247e-01)	Acc@1  89.00 ( 90.81)	Acc@5  99.00 ( 99.71)
Test: [ 40/100]	Time  0.021 ( 0.025)	Loss 3.3924e-01 (3.5178e-01)	Acc@1  91.00 ( 90.51)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.019 ( 0.024)	Loss 2.8034e-01 (3.4133e-01)	Acc@1  95.00 ( 90.84)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.017 ( 0.023)	Loss 4.0382e-01 (3.4859e-01)	Acc@1  89.00 ( 90.67)	Acc@5  99.00 ( 99.70)
Test: [ 70/100]	Time  0.018 ( 0.023)	Loss 3.6256e-01 (3.3622e-01)	Acc@1  89.00 ( 90.73)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.023 ( 0.022)	Loss 2.4327e-01 (3.3365e-01)	Acc@1  93.00 ( 90.84)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.021 ( 0.022)	Loss 2.5220e-01 (3.3467e-01)	Acc@1  94.00 ( 90.81)	Acc@5  99.00 ( 99.73)
 * Acc@1 90.940 Acc@5 99.730
### epoch[64] execution time: 18.364609241485596
EPOCH 65
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [65][  0/391]	Time  0.215 ( 0.215)	Data  0.172 ( 0.172)	Loss 3.0791e-02 (3.0791e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [65][ 10/391]	Time  0.040 ( 0.057)	Data  0.001 ( 0.017)	Loss 4.4122e-02 (5.4835e-02)	Acc@1  99.22 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [65][ 20/391]	Time  0.041 ( 0.049)	Data  0.001 ( 0.009)	Loss 4.7974e-02 (5.2008e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [65][ 30/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.007)	Loss 5.2794e-02 (5.5812e-02)	Acc@1  98.44 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [65][ 40/391]	Time  0.040 ( 0.045)	Data  0.001 ( 0.005)	Loss 6.5300e-02 (6.0830e-02)	Acc@1  96.88 ( 98.04)	Acc@5 100.00 (100.00)
Epoch: [65][ 50/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.004)	Loss 6.1005e-02 (6.0786e-02)	Acc@1  98.44 ( 98.02)	Acc@5 100.00 (100.00)
Epoch: [65][ 60/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.004)	Loss 5.8343e-02 (6.0490e-02)	Acc@1  98.44 ( 98.04)	Acc@5 100.00 (100.00)
Epoch: [65][ 70/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 8.8860e-02 (6.1862e-02)	Acc@1  96.09 ( 98.00)	Acc@5 100.00 (100.00)
Epoch: [65][ 80/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 7.2776e-02 (6.1865e-02)	Acc@1  96.88 ( 98.00)	Acc@5 100.00 (100.00)
Epoch: [65][ 90/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.5106e-02 (6.0456e-02)	Acc@1  98.44 ( 98.04)	Acc@5 100.00 (100.00)
Epoch: [65][100/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.1347e-02 (5.9893e-02)	Acc@1  98.44 ( 98.04)	Acc@5 100.00 (100.00)
Epoch: [65][110/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 7.1647e-02 (5.9741e-02)	Acc@1  98.44 ( 98.09)	Acc@5 100.00 ( 99.99)
Epoch: [65][120/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.3294e-02 (6.1719e-02)	Acc@1  98.44 ( 98.00)	Acc@5 100.00 ( 99.99)
Epoch: [65][130/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.2457e-02 (6.1592e-02)	Acc@1  99.22 ( 98.01)	Acc@5 100.00 ( 99.99)
Epoch: [65][140/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.6281e-02 (6.1119e-02)	Acc@1  97.66 ( 98.01)	Acc@5 100.00 ( 99.99)
Epoch: [65][150/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.0734e-02 (6.1531e-02)	Acc@1  98.44 ( 98.00)	Acc@5 100.00 ( 99.99)
Epoch: [65][160/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.7300e-02 (6.0765e-02)	Acc@1  98.44 ( 98.02)	Acc@5 100.00 (100.00)
Epoch: [65][170/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.1173e-02 (6.0155e-02)	Acc@1  97.66 ( 98.05)	Acc@5 100.00 (100.00)
Epoch: [65][180/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.3558e-02 (6.0291e-02)	Acc@1  96.88 ( 98.02)	Acc@5 100.00 (100.00)
Epoch: [65][190/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5687e-02 (5.9741e-02)	Acc@1  99.22 ( 98.03)	Acc@5 100.00 (100.00)
Epoch: [65][200/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.4865e-02 (5.9770e-02)	Acc@1  99.22 ( 98.01)	Acc@5 100.00 (100.00)
Epoch: [65][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.7206e-02 (5.9882e-02)	Acc@1  96.09 ( 98.00)	Acc@5 100.00 (100.00)
Epoch: [65][220/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4463e-02 (5.9972e-02)	Acc@1  99.22 ( 98.00)	Acc@5 100.00 (100.00)
Epoch: [65][230/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1421e-02 (5.9522e-02)	Acc@1  99.22 ( 98.03)	Acc@5 100.00 (100.00)
Epoch: [65][240/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3083e-02 (5.9258e-02)	Acc@1 100.00 ( 98.03)	Acc@5 100.00 (100.00)
Epoch: [65][250/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2301e-02 (5.9092e-02)	Acc@1  99.22 ( 98.04)	Acc@5 100.00 (100.00)
Epoch: [65][260/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9187e-02 (5.8885e-02)	Acc@1  98.44 ( 98.05)	Acc@5 100.00 ( 99.99)
Epoch: [65][270/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2569e-01 (5.8988e-02)	Acc@1  96.09 ( 98.05)	Acc@5 100.00 ( 99.99)
Epoch: [65][280/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.2858e-02 (5.9233e-02)	Acc@1  98.44 ( 98.05)	Acc@5 100.00 ( 99.99)
Epoch: [65][290/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.7602e-02 (5.8837e-02)	Acc@1  98.44 ( 98.08)	Acc@5 100.00 ( 99.99)
Epoch: [65][300/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.4040e-02 (5.9195e-02)	Acc@1  97.66 ( 98.05)	Acc@5 100.00 ( 99.99)
Epoch: [65][310/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0695e-02 (5.9138e-02)	Acc@1  99.22 ( 98.05)	Acc@5 100.00 ( 99.99)
Epoch: [65][320/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.8589e-02 (5.9130e-02)	Acc@1  98.44 ( 98.05)	Acc@5 100.00 ( 99.99)
Epoch: [65][330/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 8.9802e-02 (5.9219e-02)	Acc@1  96.09 ( 98.04)	Acc@5 100.00 ( 99.99)
Epoch: [65][340/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.9902e-02 (5.9423e-02)	Acc@1  98.44 ( 98.03)	Acc@5 100.00 ( 99.99)
Epoch: [65][350/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.9459e-02 (5.9023e-02)	Acc@1 100.00 ( 98.05)	Acc@5 100.00 ( 99.99)
Epoch: [65][360/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.2628e-02 (5.8735e-02)	Acc@1  98.44 ( 98.06)	Acc@5 100.00 ( 99.99)
Epoch: [65][370/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 9.7039e-02 (5.8832e-02)	Acc@1  96.88 ( 98.05)	Acc@5 100.00 ( 99.99)
Epoch: [65][380/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 8.5405e-02 (5.8699e-02)	Acc@1  96.88 ( 98.05)	Acc@5 100.00 ( 99.99)
Epoch: [65][390/391]	Time  0.028 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.7285e-02 (5.8846e-02)	Acc@1  98.75 ( 98.06)	Acc@5 100.00 ( 99.99)
## e[65] optimizer.zero_grad (sum) time: 0.26355433464050293
## e[65]       loss.backward (sum) time: 3.9977242946624756
## e[65]      optimizer.step (sum) time: 1.7793607711791992
## epoch[65] training(only) time: 16.052332401275635
# Switched to evaluate mode...
Test: [  0/100]	Time  0.179 ( 0.179)	Loss 3.3677e-01 (3.3677e-01)	Acc@1  91.00 ( 91.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 3.8652e-01 (3.1000e-01)	Acc@1  91.00 ( 91.09)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 5.4129e-01 (3.2202e-01)	Acc@1  86.00 ( 90.95)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.020 ( 0.026)	Loss 3.7105e-01 (3.3917e-01)	Acc@1  89.00 ( 90.94)	Acc@5  99.00 ( 99.81)
Test: [ 40/100]	Time  0.025 ( 0.025)	Loss 3.8195e-01 (3.5168e-01)	Acc@1  91.00 ( 90.59)	Acc@5  98.00 ( 99.71)
Test: [ 50/100]	Time  0.018 ( 0.024)	Loss 2.6629e-01 (3.4006e-01)	Acc@1  95.00 ( 90.88)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.021 ( 0.023)	Loss 4.0450e-01 (3.4655e-01)	Acc@1  89.00 ( 90.70)	Acc@5  99.00 ( 99.72)
Test: [ 70/100]	Time  0.024 ( 0.023)	Loss 3.8859e-01 (3.3419e-01)	Acc@1  89.00 ( 90.73)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.016 ( 0.022)	Loss 2.0543e-01 (3.3010e-01)	Acc@1  93.00 ( 90.89)	Acc@5 100.00 ( 99.73)
Test: [ 90/100]	Time  0.018 ( 0.022)	Loss 2.2417e-01 (3.3176e-01)	Acc@1  95.00 ( 90.88)	Acc@5 100.00 ( 99.73)
 * Acc@1 90.950 Acc@5 99.730
### epoch[65] execution time: 18.325878381729126
EPOCH 66
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [66][  0/391]	Time  0.219 ( 0.219)	Data  0.178 ( 0.178)	Loss 5.7469e-02 (5.7469e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [66][ 10/391]	Time  0.043 ( 0.057)	Data  0.001 ( 0.017)	Loss 3.4965e-02 (5.7640e-02)	Acc@1  98.44 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [66][ 20/391]	Time  0.041 ( 0.050)	Data  0.001 ( 0.009)	Loss 1.1267e-01 (6.2128e-02)	Acc@1  96.88 ( 97.81)	Acc@5 100.00 (100.00)
Epoch: [66][ 30/391]	Time  0.042 ( 0.047)	Data  0.001 ( 0.007)	Loss 2.2592e-02 (6.0920e-02)	Acc@1 100.00 ( 98.01)	Acc@5 100.00 (100.00)
Epoch: [66][ 40/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.005)	Loss 5.1903e-02 (6.1135e-02)	Acc@1  99.22 ( 98.02)	Acc@5 100.00 (100.00)
Epoch: [66][ 50/391]	Time  0.037 ( 0.044)	Data  0.001 ( 0.004)	Loss 6.2461e-02 (5.9706e-02)	Acc@1  96.88 ( 98.02)	Acc@5 100.00 (100.00)
Epoch: [66][ 60/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.004)	Loss 7.3538e-02 (6.0719e-02)	Acc@1  97.66 ( 97.94)	Acc@5 100.00 (100.00)
Epoch: [66][ 70/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 8.3093e-02 (6.0399e-02)	Acc@1  95.31 ( 97.90)	Acc@5 100.00 (100.00)
Epoch: [66][ 80/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.2178e-02 (6.0040e-02)	Acc@1  97.66 ( 97.92)	Acc@5 100.00 (100.00)
Epoch: [66][ 90/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.2310e-02 (5.9205e-02)	Acc@1  97.66 ( 97.97)	Acc@5 100.00 (100.00)
Epoch: [66][100/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.1009e-01 (6.0746e-02)	Acc@1  95.31 ( 97.91)	Acc@5 100.00 (100.00)
Epoch: [66][110/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.9141e-02 (6.0253e-02)	Acc@1  97.66 ( 97.94)	Acc@5 100.00 (100.00)
Epoch: [66][120/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.9554e-02 (5.9988e-02)	Acc@1 100.00 ( 97.95)	Acc@5 100.00 ( 99.99)
Epoch: [66][130/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.1292e-02 (5.9320e-02)	Acc@1  95.31 ( 97.96)	Acc@5 100.00 ( 99.99)
Epoch: [66][140/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.1867e-02 (5.8955e-02)	Acc@1  96.88 ( 97.97)	Acc@5 100.00 ( 99.99)
Epoch: [66][150/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2000e-02 (5.8746e-02)	Acc@1  99.22 ( 98.00)	Acc@5 100.00 ( 99.99)
Epoch: [66][160/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.9811e-02 (5.8398e-02)	Acc@1  96.09 ( 97.99)	Acc@5 100.00 ( 99.99)
Epoch: [66][170/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.3558e-02 (5.7996e-02)	Acc@1  95.31 ( 98.01)	Acc@5 100.00 ( 99.99)
Epoch: [66][180/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9877e-02 (5.7519e-02)	Acc@1  99.22 ( 98.02)	Acc@5 100.00 ( 99.99)
Epoch: [66][190/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0110e-01 (5.7962e-02)	Acc@1  93.75 ( 97.98)	Acc@5 100.00 ( 99.99)
Epoch: [66][200/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.6457e-02 (5.8407e-02)	Acc@1  96.09 ( 97.96)	Acc@5 100.00 ( 99.99)
Epoch: [66][210/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7650e-02 (5.8530e-02)	Acc@1  99.22 ( 97.97)	Acc@5 100.00 ( 99.99)
Epoch: [66][220/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.9785e-02 (5.8238e-02)	Acc@1  97.66 ( 98.00)	Acc@5 100.00 ( 99.99)
Epoch: [66][230/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4125e-02 (5.8003e-02)	Acc@1  99.22 ( 98.02)	Acc@5 100.00 ( 99.99)
Epoch: [66][240/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2539e-02 (5.7968e-02)	Acc@1  98.44 ( 98.04)	Acc@5 100.00 ( 99.99)
Epoch: [66][250/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.7616e-02 (5.7920e-02)	Acc@1  98.44 ( 98.04)	Acc@5 100.00 ( 99.99)
Epoch: [66][260/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1918e-02 (5.7869e-02)	Acc@1  99.22 ( 98.02)	Acc@5 100.00 ( 99.99)
Epoch: [66][270/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.4427e-02 (5.8119e-02)	Acc@1  95.31 ( 98.00)	Acc@5 100.00 ( 99.99)
Epoch: [66][280/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.1105e-02 (5.7983e-02)	Acc@1  96.88 ( 98.01)	Acc@5 100.00 ( 99.99)
Epoch: [66][290/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.6118e-02 (5.7850e-02)	Acc@1  99.22 ( 98.02)	Acc@5 100.00 ( 99.99)
Epoch: [66][300/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3812e-02 (5.7968e-02)	Acc@1 100.00 ( 98.03)	Acc@5 100.00 ( 99.99)
Epoch: [66][310/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.3324e-02 (5.8280e-02)	Acc@1  98.44 ( 98.02)	Acc@5 100.00 ( 99.99)
Epoch: [66][320/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.0777e-02 (5.9096e-02)	Acc@1  99.22 ( 97.99)	Acc@5 100.00 ( 99.99)
Epoch: [66][330/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.9558e-02 (5.9372e-02)	Acc@1  97.66 ( 97.98)	Acc@5 100.00 ( 99.99)
Epoch: [66][340/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1260e-02 (5.9347e-02)	Acc@1  98.44 ( 97.99)	Acc@5 100.00 ( 99.99)
Epoch: [66][350/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.8038e-02 (5.9221e-02)	Acc@1  97.66 ( 97.99)	Acc@5 100.00 ( 99.99)
Epoch: [66][360/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.7924e-02 (5.9352e-02)	Acc@1  96.09 ( 97.98)	Acc@5 100.00 ( 99.99)
Epoch: [66][370/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.0875e-02 (5.9232e-02)	Acc@1  96.88 ( 97.98)	Acc@5 100.00 ( 99.99)
Epoch: [66][380/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.6111e-02 (5.9006e-02)	Acc@1 100.00 ( 98.00)	Acc@5 100.00 ( 99.99)
Epoch: [66][390/391]	Time  0.028 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.0106e-02 (5.9137e-02)	Acc@1  98.75 ( 97.98)	Acc@5 100.00 ( 99.99)
## e[66] optimizer.zero_grad (sum) time: 0.26203322410583496
## e[66]       loss.backward (sum) time: 3.9311392307281494
## e[66]      optimizer.step (sum) time: 1.8187026977539062
## epoch[66] training(only) time: 16.00922417640686
# Switched to evaluate mode...
Test: [  0/100]	Time  0.173 ( 0.173)	Loss 3.3741e-01 (3.3741e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.022 ( 0.036)	Loss 4.0217e-01 (3.1417e-01)	Acc@1  91.00 ( 91.18)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.021 ( 0.029)	Loss 5.4566e-01 (3.2717e-01)	Acc@1  85.00 ( 90.71)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.016 ( 0.026)	Loss 3.5260e-01 (3.4211e-01)	Acc@1  89.00 ( 90.84)	Acc@5  99.00 ( 99.77)
Test: [ 40/100]	Time  0.018 ( 0.025)	Loss 3.7065e-01 (3.5338e-01)	Acc@1  92.00 ( 90.61)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.024 ( 0.024)	Loss 2.6136e-01 (3.4084e-01)	Acc@1  94.00 ( 90.88)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.016 ( 0.023)	Loss 4.0668e-01 (3.4789e-01)	Acc@1  88.00 ( 90.61)	Acc@5  99.00 ( 99.72)
Test: [ 70/100]	Time  0.017 ( 0.023)	Loss 4.1368e-01 (3.3617e-01)	Acc@1  88.00 ( 90.72)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.024 ( 0.023)	Loss 2.0987e-01 (3.3218e-01)	Acc@1  94.00 ( 90.85)	Acc@5 100.00 ( 99.73)
Test: [ 90/100]	Time  0.021 ( 0.023)	Loss 2.2234e-01 (3.3356e-01)	Acc@1  94.00 ( 90.82)	Acc@5 100.00 ( 99.74)
 * Acc@1 90.940 Acc@5 99.740
### epoch[66] execution time: 18.37772798538208
EPOCH 67
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [67][  0/391]	Time  0.264 ( 0.264)	Data  0.220 ( 0.220)	Loss 9.8041e-02 (9.8041e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [67][ 10/391]	Time  0.041 ( 0.060)	Data  0.001 ( 0.021)	Loss 7.0518e-02 (5.5502e-02)	Acc@1  98.44 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [67][ 20/391]	Time  0.040 ( 0.051)	Data  0.001 ( 0.011)	Loss 2.4997e-02 (5.4028e-02)	Acc@1  99.22 ( 98.10)	Acc@5 100.00 (100.00)
Epoch: [67][ 30/391]	Time  0.041 ( 0.047)	Data  0.001 ( 0.008)	Loss 6.1145e-02 (5.6382e-02)	Acc@1  97.66 ( 98.14)	Acc@5 100.00 (100.00)
Epoch: [67][ 40/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.006)	Loss 5.4983e-02 (5.7170e-02)	Acc@1  98.44 ( 98.06)	Acc@5 100.00 (100.00)
Epoch: [67][ 50/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.005)	Loss 7.5942e-02 (5.8507e-02)	Acc@1  97.66 ( 98.04)	Acc@5 100.00 (100.00)
Epoch: [67][ 60/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.005)	Loss 2.9353e-02 (5.7976e-02)	Acc@1  98.44 ( 98.10)	Acc@5 100.00 (100.00)
Epoch: [67][ 70/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 4.5663e-02 (5.7069e-02)	Acc@1  99.22 ( 98.13)	Acc@5 100.00 (100.00)
Epoch: [67][ 80/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 4.5742e-02 (5.6962e-02)	Acc@1  97.66 ( 98.17)	Acc@5 100.00 (100.00)
Epoch: [67][ 90/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.2286e-01 (5.6682e-02)	Acc@1  95.31 ( 98.18)	Acc@5 100.00 (100.00)
Epoch: [67][100/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.4487e-02 (5.6252e-02)	Acc@1  99.22 ( 98.21)	Acc@5 100.00 (100.00)
Epoch: [67][110/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.4038e-02 (5.6107e-02)	Acc@1  97.66 ( 98.18)	Acc@5 100.00 (100.00)
Epoch: [67][120/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.6239e-02 (5.6892e-02)	Acc@1 100.00 ( 98.13)	Acc@5 100.00 (100.00)
Epoch: [67][130/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.7378e-02 (5.6742e-02)	Acc@1  98.44 ( 98.12)	Acc@5 100.00 (100.00)
Epoch: [67][140/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.3638e-02 (5.7613e-02)	Acc@1 100.00 ( 98.10)	Acc@5 100.00 ( 99.99)
Epoch: [67][150/391]	Time  0.040 ( 0.041)	Data  0.000 ( 0.002)	Loss 6.1953e-02 (5.7335e-02)	Acc@1  97.66 ( 98.11)	Acc@5 100.00 ( 99.99)
Epoch: [67][160/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2879e-01 (5.7358e-02)	Acc@1  96.09 ( 98.11)	Acc@5 100.00 (100.00)
Epoch: [67][170/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8096e-02 (5.7680e-02)	Acc@1  98.44 ( 98.11)	Acc@5 100.00 (100.00)
Epoch: [67][180/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.1523e-02 (5.7562e-02)	Acc@1  96.09 ( 98.13)	Acc@5 100.00 (100.00)
Epoch: [67][190/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.3347e-02 (5.7838e-02)	Acc@1  97.66 ( 98.13)	Acc@5 100.00 (100.00)
Epoch: [67][200/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2411e-01 (5.7767e-02)	Acc@1  94.53 ( 98.12)	Acc@5 100.00 (100.00)
Epoch: [67][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7896e-02 (5.7322e-02)	Acc@1  99.22 ( 98.14)	Acc@5 100.00 (100.00)
Epoch: [67][220/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.0579e-02 (5.7391e-02)	Acc@1  99.22 ( 98.14)	Acc@5 100.00 (100.00)
Epoch: [67][230/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.6339e-02 (5.7082e-02)	Acc@1 100.00 ( 98.16)	Acc@5 100.00 (100.00)
Epoch: [67][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.1481e-02 (5.7543e-02)	Acc@1  94.53 ( 98.12)	Acc@5 100.00 (100.00)
Epoch: [67][250/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.1408e-02 (5.7114e-02)	Acc@1 100.00 ( 98.14)	Acc@5 100.00 (100.00)
Epoch: [67][260/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.5850e-02 (5.7014e-02)	Acc@1  98.44 ( 98.13)	Acc@5 100.00 (100.00)
Epoch: [67][270/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.1789e-02 (5.6909e-02)	Acc@1  97.66 ( 98.13)	Acc@5 100.00 (100.00)
Epoch: [67][280/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.5055e-02 (5.6675e-02)	Acc@1  98.44 ( 98.13)	Acc@5 100.00 (100.00)
Epoch: [67][290/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.5406e-02 (5.7048e-02)	Acc@1  97.66 ( 98.11)	Acc@5 100.00 (100.00)
Epoch: [67][300/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.0890e-02 (5.6809e-02)	Acc@1  99.22 ( 98.11)	Acc@5 100.00 (100.00)
Epoch: [67][310/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.0883e-02 (5.6666e-02)	Acc@1  97.66 ( 98.13)	Acc@5 100.00 (100.00)
Epoch: [67][320/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4063e-02 (5.6842e-02)	Acc@1  99.22 ( 98.12)	Acc@5 100.00 (100.00)
Epoch: [67][330/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.8641e-02 (5.6969e-02)	Acc@1  99.22 ( 98.10)	Acc@5 100.00 (100.00)
Epoch: [67][340/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.0465e-02 (5.6793e-02)	Acc@1  98.44 ( 98.11)	Acc@5 100.00 (100.00)
Epoch: [67][350/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.2590e-02 (5.6931e-02)	Acc@1  98.44 ( 98.11)	Acc@5 100.00 (100.00)
Epoch: [67][360/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.7200e-02 (5.6898e-02)	Acc@1  97.66 ( 98.12)	Acc@5 100.00 (100.00)
Epoch: [67][370/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.4162e-02 (5.7165e-02)	Acc@1  98.44 ( 98.10)	Acc@5 100.00 (100.00)
Epoch: [67][380/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3484e-02 (5.6957e-02)	Acc@1 100.00 ( 98.11)	Acc@5 100.00 (100.00)
Epoch: [67][390/391]	Time  0.027 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.5499e-02 (5.6830e-02)	Acc@1  95.00 ( 98.12)	Acc@5 100.00 (100.00)
## e[67] optimizer.zero_grad (sum) time: 0.26343250274658203
## e[67]       loss.backward (sum) time: 3.989823579788208
## e[67]      optimizer.step (sum) time: 1.806403636932373
## epoch[67] training(only) time: 15.997660636901855
# Switched to evaluate mode...
Test: [  0/100]	Time  0.191 ( 0.191)	Loss 3.3423e-01 (3.3423e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.021 ( 0.038)	Loss 3.7709e-01 (3.1139e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.022 ( 0.031)	Loss 5.4067e-01 (3.2684e-01)	Acc@1  86.00 ( 90.67)	Acc@5 100.00 ( 99.90)
Test: [ 30/100]	Time  0.024 ( 0.028)	Loss 3.5977e-01 (3.4050e-01)	Acc@1  90.00 ( 90.68)	Acc@5  99.00 ( 99.81)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 3.9930e-01 (3.5562e-01)	Acc@1  92.00 ( 90.37)	Acc@5  99.00 ( 99.73)
Test: [ 50/100]	Time  0.018 ( 0.025)	Loss 2.6235e-01 (3.4321e-01)	Acc@1  95.00 ( 90.71)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.016 ( 0.024)	Loss 4.0864e-01 (3.4938e-01)	Acc@1  89.00 ( 90.48)	Acc@5  99.00 ( 99.74)
Test: [ 70/100]	Time  0.018 ( 0.023)	Loss 4.0353e-01 (3.3752e-01)	Acc@1  88.00 ( 90.61)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.021 ( 0.023)	Loss 1.9974e-01 (3.3341e-01)	Acc@1  94.00 ( 90.78)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.022 ( 0.023)	Loss 2.1972e-01 (3.3505e-01)	Acc@1  94.00 ( 90.78)	Acc@5 100.00 ( 99.76)
 * Acc@1 90.890 Acc@5 99.760
### epoch[67] execution time: 18.353045225143433
EPOCH 68
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [68][  0/391]	Time  0.249 ( 0.249)	Data  0.209 ( 0.209)	Loss 6.0136e-02 (6.0136e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [68][ 10/391]	Time  0.041 ( 0.059)	Data  0.001 ( 0.020)	Loss 1.0942e-01 (5.8458e-02)	Acc@1  94.53 ( 98.01)	Acc@5 100.00 (100.00)
Epoch: [68][ 20/391]	Time  0.043 ( 0.051)	Data  0.001 ( 0.011)	Loss 4.2186e-02 (5.7668e-02)	Acc@1  98.44 ( 98.14)	Acc@5 100.00 (100.00)
Epoch: [68][ 30/391]	Time  0.043 ( 0.047)	Data  0.001 ( 0.008)	Loss 7.8575e-02 (6.8747e-02)	Acc@1  96.88 ( 97.68)	Acc@5 100.00 (100.00)
Epoch: [68][ 40/391]	Time  0.040 ( 0.046)	Data  0.001 ( 0.006)	Loss 6.2899e-02 (6.7144e-02)	Acc@1  97.66 ( 97.85)	Acc@5 100.00 (100.00)
Epoch: [68][ 50/391]	Time  0.039 ( 0.045)	Data  0.001 ( 0.005)	Loss 3.4843e-02 (6.4693e-02)	Acc@1  99.22 ( 97.95)	Acc@5 100.00 (100.00)
Epoch: [68][ 60/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.004)	Loss 6.6981e-02 (6.3442e-02)	Acc@1  96.09 ( 97.98)	Acc@5 100.00 (100.00)
Epoch: [68][ 70/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.004)	Loss 6.6636e-02 (6.3443e-02)	Acc@1  98.44 ( 97.93)	Acc@5 100.00 (100.00)
Epoch: [68][ 80/391]	Time  0.043 ( 0.043)	Data  0.001 ( 0.004)	Loss 6.0638e-02 (6.2421e-02)	Acc@1  97.66 ( 97.92)	Acc@5 100.00 (100.00)
Epoch: [68][ 90/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 8.4887e-02 (6.2658e-02)	Acc@1  97.66 ( 97.91)	Acc@5 100.00 (100.00)
Epoch: [68][100/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.003)	Loss 5.4587e-02 (6.1470e-02)	Acc@1  99.22 ( 97.95)	Acc@5 100.00 (100.00)
Epoch: [68][110/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.2550e-02 (6.0909e-02)	Acc@1  97.66 ( 97.99)	Acc@5 100.00 (100.00)
Epoch: [68][120/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.7081e-02 (6.0654e-02)	Acc@1  98.44 ( 97.98)	Acc@5 100.00 (100.00)
Epoch: [68][130/391]	Time  0.046 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.5868e-02 (6.0115e-02)	Acc@1  98.44 ( 97.99)	Acc@5 100.00 (100.00)
Epoch: [68][140/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.7138e-02 (5.9275e-02)	Acc@1  99.22 ( 98.03)	Acc@5 100.00 (100.00)
Epoch: [68][150/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.8702e-02 (5.8967e-02)	Acc@1  97.66 ( 98.05)	Acc@5 100.00 (100.00)
Epoch: [68][160/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.6412e-02 (5.8631e-02)	Acc@1  98.44 ( 98.06)	Acc@5 100.00 (100.00)
Epoch: [68][170/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.2182e-02 (5.8378e-02)	Acc@1  99.22 ( 98.09)	Acc@5 100.00 (100.00)
Epoch: [68][180/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.4638e-02 (5.8114e-02)	Acc@1 100.00 ( 98.11)	Acc@5 100.00 (100.00)
Epoch: [68][190/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.3268e-02 (5.8229e-02)	Acc@1  97.66 ( 98.09)	Acc@5 100.00 (100.00)
Epoch: [68][200/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0195e-01 (5.7931e-02)	Acc@1  95.31 ( 98.10)	Acc@5 100.00 (100.00)
Epoch: [68][210/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6829e-02 (5.8413e-02)	Acc@1 100.00 ( 98.08)	Acc@5 100.00 (100.00)
Epoch: [68][220/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.6260e-02 (5.8320e-02)	Acc@1  98.44 ( 98.09)	Acc@5 100.00 (100.00)
Epoch: [68][230/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3799e-02 (5.7846e-02)	Acc@1  99.22 ( 98.11)	Acc@5 100.00 (100.00)
Epoch: [68][240/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.9453e-02 (5.8298e-02)	Acc@1  98.44 ( 98.10)	Acc@5 100.00 (100.00)
Epoch: [68][250/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.3319e-02 (5.7907e-02)	Acc@1 100.00 ( 98.11)	Acc@5 100.00 (100.00)
Epoch: [68][260/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.0236e-02 (5.7546e-02)	Acc@1  97.66 ( 98.14)	Acc@5 100.00 (100.00)
Epoch: [68][270/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9794e-02 (5.7097e-02)	Acc@1  98.44 ( 98.15)	Acc@5 100.00 (100.00)
Epoch: [68][280/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.8568e-02 (5.6663e-02)	Acc@1  99.22 ( 98.17)	Acc@5 100.00 (100.00)
Epoch: [68][290/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.6175e-02 (5.6511e-02)	Acc@1  98.44 ( 98.18)	Acc@5 100.00 (100.00)
Epoch: [68][300/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.8373e-02 (5.6395e-02)	Acc@1  96.09 ( 98.18)	Acc@5 100.00 (100.00)
Epoch: [68][310/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.0090e-02 (5.6388e-02)	Acc@1 100.00 ( 98.18)	Acc@5 100.00 (100.00)
Epoch: [68][320/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.9835e-02 (5.7000e-02)	Acc@1  96.88 ( 98.16)	Acc@5 100.00 (100.00)
Epoch: [68][330/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1730e-02 (5.7182e-02)	Acc@1  98.44 ( 98.15)	Acc@5 100.00 (100.00)
Epoch: [68][340/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6878e-02 (5.6806e-02)	Acc@1 100.00 ( 98.17)	Acc@5 100.00 (100.00)
Epoch: [68][350/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0669e-01 (5.6794e-02)	Acc@1  96.88 ( 98.16)	Acc@5 100.00 (100.00)
Epoch: [68][360/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7875e-02 (5.7042e-02)	Acc@1  99.22 ( 98.15)	Acc@5 100.00 (100.00)
Epoch: [68][370/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.3188e-02 (5.6955e-02)	Acc@1  98.44 ( 98.15)	Acc@5 100.00 (100.00)
Epoch: [68][380/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.0326e-02 (5.7258e-02)	Acc@1  96.88 ( 98.13)	Acc@5 100.00 (100.00)
Epoch: [68][390/391]	Time  0.029 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.5110e-02 (5.7258e-02)	Acc@1  98.75 ( 98.13)	Acc@5 100.00 (100.00)
## e[68] optimizer.zero_grad (sum) time: 0.2622346878051758
## e[68]       loss.backward (sum) time: 4.0029332637786865
## e[68]      optimizer.step (sum) time: 1.810417652130127
## epoch[68] training(only) time: 16.050467014312744
# Switched to evaluate mode...
Test: [  0/100]	Time  0.165 ( 0.165)	Loss 3.5998e-01 (3.5998e-01)	Acc@1  91.00 ( 91.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.020 ( 0.036)	Loss 4.2680e-01 (3.2248e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 5.2845e-01 (3.3315e-01)	Acc@1  87.00 ( 90.81)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 3.8975e-01 (3.4929e-01)	Acc@1  90.00 ( 90.71)	Acc@5  99.00 ( 99.77)
Test: [ 40/100]	Time  0.020 ( 0.025)	Loss 3.7016e-01 (3.5781e-01)	Acc@1  92.00 ( 90.54)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.022 ( 0.024)	Loss 2.6330e-01 (3.4510e-01)	Acc@1  93.00 ( 90.88)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.017 ( 0.024)	Loss 3.8818e-01 (3.5206e-01)	Acc@1  89.00 ( 90.67)	Acc@5  99.00 ( 99.70)
Test: [ 70/100]	Time  0.017 ( 0.023)	Loss 4.0839e-01 (3.4028e-01)	Acc@1  87.00 ( 90.80)	Acc@5 100.00 ( 99.72)
Test: [ 80/100]	Time  0.024 ( 0.023)	Loss 2.1795e-01 (3.3678e-01)	Acc@1  93.00 ( 90.89)	Acc@5 100.00 ( 99.72)
Test: [ 90/100]	Time  0.018 ( 0.023)	Loss 2.3374e-01 (3.3825e-01)	Acc@1  94.00 ( 90.85)	Acc@5 100.00 ( 99.73)
 * Acc@1 90.930 Acc@5 99.730
### epoch[68] execution time: 18.460026502609253
EPOCH 69
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [69][  0/391]	Time  0.262 ( 0.262)	Data  0.213 ( 0.213)	Loss 5.5790e-02 (5.5790e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [69][ 10/391]	Time  0.040 ( 0.061)	Data  0.001 ( 0.020)	Loss 3.4453e-02 (5.4579e-02)	Acc@1  99.22 ( 98.08)	Acc@5 100.00 (100.00)
Epoch: [69][ 20/391]	Time  0.041 ( 0.051)	Data  0.001 ( 0.011)	Loss 8.4394e-02 (5.2652e-02)	Acc@1  96.88 ( 98.33)	Acc@5 100.00 (100.00)
Epoch: [69][ 30/391]	Time  0.041 ( 0.048)	Data  0.001 ( 0.008)	Loss 5.2463e-02 (5.0623e-02)	Acc@1  98.44 ( 98.34)	Acc@5 100.00 (100.00)
Epoch: [69][ 40/391]	Time  0.038 ( 0.046)	Data  0.001 ( 0.006)	Loss 4.1800e-02 (5.3038e-02)	Acc@1  97.66 ( 98.21)	Acc@5 100.00 (100.00)
Epoch: [69][ 50/391]	Time  0.038 ( 0.045)	Data  0.001 ( 0.005)	Loss 4.9362e-02 (5.1363e-02)	Acc@1  98.44 ( 98.24)	Acc@5 100.00 (100.00)
Epoch: [69][ 60/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.004)	Loss 3.8289e-02 (5.2148e-02)	Acc@1  98.44 ( 98.23)	Acc@5 100.00 (100.00)
Epoch: [69][ 70/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.004)	Loss 3.4911e-02 (5.2378e-02)	Acc@1  99.22 ( 98.23)	Acc@5 100.00 (100.00)
Epoch: [69][ 80/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 6.5905e-02 (5.4066e-02)	Acc@1  97.66 ( 98.13)	Acc@5 100.00 (100.00)
Epoch: [69][ 90/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.8274e-02 (5.5136e-02)	Acc@1  99.22 ( 98.12)	Acc@5 100.00 (100.00)
Epoch: [69][100/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.4926e-02 (5.5362e-02)	Acc@1 100.00 ( 98.11)	Acc@5 100.00 (100.00)
Epoch: [69][110/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 9.4420e-02 (5.4931e-02)	Acc@1  97.66 ( 98.16)	Acc@5 100.00 (100.00)
Epoch: [69][120/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.2650e-02 (5.4455e-02)	Acc@1  96.09 ( 98.15)	Acc@5 100.00 (100.00)
Epoch: [69][130/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.2263e-01 (5.5432e-02)	Acc@1  96.09 ( 98.13)	Acc@5 100.00 (100.00)
Epoch: [69][140/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.5782e-02 (5.5204e-02)	Acc@1  99.22 ( 98.14)	Acc@5 100.00 (100.00)
Epoch: [69][150/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.6299e-02 (5.5590e-02)	Acc@1  97.66 ( 98.13)	Acc@5 100.00 ( 99.99)
Epoch: [69][160/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 9.4232e-02 (5.4934e-02)	Acc@1  96.88 ( 98.17)	Acc@5 100.00 ( 99.99)
Epoch: [69][170/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.2904e-01 (5.5886e-02)	Acc@1  96.09 ( 98.16)	Acc@5 100.00 ( 99.99)
Epoch: [69][180/391]	Time  0.046 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.1633e-01 (5.6362e-02)	Acc@1  95.31 ( 98.14)	Acc@5 100.00 ( 99.99)
Epoch: [69][190/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.3484e-02 (5.5910e-02)	Acc@1  96.88 ( 98.16)	Acc@5 100.00 ( 99.99)
Epoch: [69][200/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1155e-02 (5.5878e-02)	Acc@1  99.22 ( 98.16)	Acc@5 100.00 ( 99.99)
Epoch: [69][210/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0887e-01 (5.6890e-02)	Acc@1  95.31 ( 98.09)	Acc@5 100.00 ( 99.99)
Epoch: [69][220/391]	Time  0.052 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1241e-01 (5.7234e-02)	Acc@1  93.75 ( 98.04)	Acc@5 100.00 ( 99.99)
Epoch: [69][230/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.8119e-02 (5.7123e-02)	Acc@1  97.66 ( 98.06)	Acc@5 100.00 ( 99.99)
Epoch: [69][240/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.1571e-02 (5.6925e-02)	Acc@1  97.66 ( 98.05)	Acc@5 100.00 ( 99.99)
Epoch: [69][250/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.4965e-02 (5.6793e-02)	Acc@1  97.66 ( 98.06)	Acc@5 100.00 ( 99.99)
Epoch: [69][260/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.4088e-02 (5.6699e-02)	Acc@1  97.66 ( 98.07)	Acc@5 100.00 ( 99.99)
Epoch: [69][270/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1542e-02 (5.6240e-02)	Acc@1  99.22 ( 98.08)	Acc@5 100.00 ( 99.99)
Epoch: [69][280/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.6890e-02 (5.5931e-02)	Acc@1  98.44 ( 98.09)	Acc@5 100.00 ( 99.99)
Epoch: [69][290/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1170e-02 (5.5891e-02)	Acc@1  99.22 ( 98.09)	Acc@5 100.00 ( 99.99)
Epoch: [69][300/391]	Time  0.040 ( 0.041)	Data  0.002 ( 0.002)	Loss 9.1276e-02 (5.6105e-02)	Acc@1  96.88 ( 98.10)	Acc@5 100.00 ( 99.99)
Epoch: [69][310/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0220e-01 (5.6064e-02)	Acc@1  96.88 ( 98.11)	Acc@5 100.00 ( 99.99)
Epoch: [69][320/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1921e-02 (5.5801e-02)	Acc@1  98.44 ( 98.12)	Acc@5 100.00 ( 99.99)
Epoch: [69][330/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0924e-02 (5.5913e-02)	Acc@1  98.44 ( 98.13)	Acc@5 100.00 ( 99.99)
Epoch: [69][340/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2936e-02 (5.5981e-02)	Acc@1  98.44 ( 98.13)	Acc@5 100.00 ( 99.99)
Epoch: [69][350/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.4678e-02 (5.6240e-02)	Acc@1  97.66 ( 98.12)	Acc@5 100.00 ( 99.99)
Epoch: [69][360/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2999e-02 (5.6398e-02)	Acc@1  98.44 ( 98.12)	Acc@5 100.00 ( 99.99)
Epoch: [69][370/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1829e-02 (5.6288e-02)	Acc@1  98.44 ( 98.12)	Acc@5 100.00 ( 99.99)
Epoch: [69][380/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0153e-01 (5.6564e-02)	Acc@1  98.44 ( 98.11)	Acc@5 100.00 ( 99.99)
Epoch: [69][390/391]	Time  0.031 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.0252e-02 (5.6933e-02)	Acc@1  96.25 ( 98.10)	Acc@5 100.00 ( 99.99)
## e[69] optimizer.zero_grad (sum) time: 0.2623887062072754
## e[69]       loss.backward (sum) time: 4.041092872619629
## e[69]      optimizer.step (sum) time: 1.7929973602294922
## epoch[69] training(only) time: 16.101240396499634
# Switched to evaluate mode...
Test: [  0/100]	Time  0.171 ( 0.171)	Loss 3.7134e-01 (3.7134e-01)	Acc@1  89.00 ( 89.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.018 ( 0.035)	Loss 4.4059e-01 (3.2590e-01)	Acc@1  91.00 ( 90.73)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.018 ( 0.029)	Loss 5.4590e-01 (3.3122e-01)	Acc@1  87.00 ( 90.71)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.017 ( 0.026)	Loss 3.7364e-01 (3.4794e-01)	Acc@1  89.00 ( 90.84)	Acc@5  99.00 ( 99.74)
Test: [ 40/100]	Time  0.017 ( 0.024)	Loss 3.7708e-01 (3.5755e-01)	Acc@1  89.00 ( 90.44)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.023 ( 0.023)	Loss 2.6231e-01 (3.4571e-01)	Acc@1  95.00 ( 90.71)	Acc@5 100.00 ( 99.67)
Test: [ 60/100]	Time  0.020 ( 0.023)	Loss 3.9379e-01 (3.5180e-01)	Acc@1  90.00 ( 90.56)	Acc@5  99.00 ( 99.69)
Test: [ 70/100]	Time  0.018 ( 0.022)	Loss 4.3157e-01 (3.4174e-01)	Acc@1  87.00 ( 90.58)	Acc@5 100.00 ( 99.70)
Test: [ 80/100]	Time  0.021 ( 0.022)	Loss 2.2383e-01 (3.3794e-01)	Acc@1  93.00 ( 90.75)	Acc@5 100.00 ( 99.70)
Test: [ 90/100]	Time  0.022 ( 0.022)	Loss 2.2880e-01 (3.3945e-01)	Acc@1  94.00 ( 90.77)	Acc@5 100.00 ( 99.71)
 * Acc@1 90.870 Acc@5 99.720
### epoch[69] execution time: 18.41534924507141
EPOCH 70
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [70][  0/391]	Time  0.219 ( 0.219)	Data  0.177 ( 0.177)	Loss 6.3978e-02 (6.3978e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [70][ 10/391]	Time  0.039 ( 0.058)	Data  0.001 ( 0.017)	Loss 3.1704e-02 (4.3810e-02)	Acc@1 100.00 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [70][ 20/391]	Time  0.041 ( 0.050)	Data  0.001 ( 0.009)	Loss 7.2619e-02 (4.7602e-02)	Acc@1  98.44 ( 98.51)	Acc@5 100.00 (100.00)
Epoch: [70][ 30/391]	Time  0.039 ( 0.047)	Data  0.001 ( 0.007)	Loss 3.6298e-02 (4.7716e-02)	Acc@1  97.66 ( 98.34)	Acc@5 100.00 (100.00)
Epoch: [70][ 40/391]	Time  0.040 ( 0.045)	Data  0.001 ( 0.005)	Loss 3.8848e-02 (5.0803e-02)	Acc@1  98.44 ( 98.19)	Acc@5 100.00 (100.00)
Epoch: [70][ 50/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.004)	Loss 3.9054e-02 (5.2239e-02)	Acc@1  98.44 ( 98.16)	Acc@5 100.00 (100.00)
Epoch: [70][ 60/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.004)	Loss 2.1975e-02 (5.3612e-02)	Acc@1  99.22 ( 98.09)	Acc@5 100.00 (100.00)
Epoch: [70][ 70/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.003)	Loss 9.7489e-02 (5.5994e-02)	Acc@1  96.09 ( 98.05)	Acc@5 100.00 (100.00)
Epoch: [70][ 80/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 5.1013e-02 (5.5720e-02)	Acc@1  98.44 ( 98.06)	Acc@5 100.00 (100.00)
Epoch: [70][ 90/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.0870e-02 (5.7037e-02)	Acc@1  98.44 ( 98.06)	Acc@5 100.00 (100.00)
Epoch: [70][100/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.8439e-02 (5.7759e-02)	Acc@1  96.88 ( 98.07)	Acc@5 100.00 (100.00)
Epoch: [70][110/391]	Time  0.045 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.3702e-02 (5.8362e-02)	Acc@1  99.22 ( 98.04)	Acc@5 100.00 (100.00)
Epoch: [70][120/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.0293e-02 (5.7190e-02)	Acc@1  99.22 ( 98.06)	Acc@5 100.00 (100.00)
Epoch: [70][130/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.7566e-02 (5.7902e-02)	Acc@1  96.88 ( 98.06)	Acc@5 100.00 (100.00)
Epoch: [70][140/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 7.2439e-02 (5.7705e-02)	Acc@1  97.66 ( 98.06)	Acc@5 100.00 (100.00)
Epoch: [70][150/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.9546e-02 (5.7170e-02)	Acc@1 100.00 ( 98.09)	Acc@5 100.00 ( 99.99)
Epoch: [70][160/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0434e-02 (5.7015e-02)	Acc@1  99.22 ( 98.11)	Acc@5 100.00 ( 99.99)
Epoch: [70][170/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0994e-02 (5.7281e-02)	Acc@1  97.66 ( 98.12)	Acc@5 100.00 ( 99.99)
Epoch: [70][180/391]	Time  0.056 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1257e-02 (5.6683e-02)	Acc@1  98.44 ( 98.14)	Acc@5 100.00 ( 99.99)
Epoch: [70][190/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.6608e-02 (5.6962e-02)	Acc@1  98.44 ( 98.13)	Acc@5 100.00 ( 99.99)
Epoch: [70][200/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.8908e-02 (5.6794e-02)	Acc@1  97.66 ( 98.13)	Acc@5 100.00 ( 99.99)
Epoch: [70][210/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.0618e-02 (5.6707e-02)	Acc@1  97.66 ( 98.12)	Acc@5 100.00 ( 99.99)
Epoch: [70][220/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.9718e-02 (5.6927e-02)	Acc@1  99.22 ( 98.11)	Acc@5 100.00 ( 99.99)
Epoch: [70][230/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9597e-02 (5.6808e-02)	Acc@1 100.00 ( 98.11)	Acc@5 100.00 ( 99.99)
Epoch: [70][240/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7799e-02 (5.6429e-02)	Acc@1 100.00 ( 98.15)	Acc@5 100.00 ( 99.99)
Epoch: [70][250/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.6204e-02 (5.5885e-02)	Acc@1 100.00 ( 98.16)	Acc@5 100.00 ( 99.99)
Epoch: [70][260/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4369e-02 (5.5965e-02)	Acc@1  98.44 ( 98.15)	Acc@5 100.00 ( 99.99)
Epoch: [70][270/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.9816e-02 (5.5743e-02)	Acc@1 100.00 ( 98.17)	Acc@5 100.00 ( 99.99)
Epoch: [70][280/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.5065e-02 (5.5616e-02)	Acc@1 100.00 ( 98.17)	Acc@5 100.00 ( 99.99)
Epoch: [70][290/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.9785e-02 (5.6118e-02)	Acc@1  96.09 ( 98.15)	Acc@5 100.00 ( 99.99)
Epoch: [70][300/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9868e-02 (5.6031e-02)	Acc@1  98.44 ( 98.16)	Acc@5 100.00 ( 99.99)
Epoch: [70][310/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7529e-02 (5.6053e-02)	Acc@1  98.44 ( 98.16)	Acc@5 100.00 ( 99.99)
Epoch: [70][320/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.3358e-02 (5.5702e-02)	Acc@1  99.22 ( 98.17)	Acc@5 100.00 ( 99.99)
Epoch: [70][330/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.3284e-02 (5.5895e-02)	Acc@1  98.44 ( 98.16)	Acc@5 100.00 ( 99.99)
Epoch: [70][340/391]	Time  0.046 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.6205e-02 (5.5931e-02)	Acc@1  98.44 ( 98.16)	Acc@5 100.00 ( 99.99)
Epoch: [70][350/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 9.7089e-02 (5.5922e-02)	Acc@1  96.09 ( 98.15)	Acc@5 100.00 ( 99.99)
Epoch: [70][360/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.4896e-02 (5.6202e-02)	Acc@1  96.88 ( 98.15)	Acc@5 100.00 ( 99.99)
Epoch: [70][370/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.5100e-02 (5.5969e-02)	Acc@1  99.22 ( 98.16)	Acc@5 100.00 ( 99.99)
Epoch: [70][380/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.3649e-02 (5.5922e-02)	Acc@1  96.88 ( 98.16)	Acc@5 100.00 ( 99.99)
Epoch: [70][390/391]	Time  0.035 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.5430e-01 (5.6006e-02)	Acc@1  92.50 ( 98.16)	Acc@5 100.00 ( 99.99)
## e[70] optimizer.zero_grad (sum) time: 0.26332545280456543
## e[70]       loss.backward (sum) time: 4.010682821273804
## e[70]      optimizer.step (sum) time: 1.7941844463348389
## epoch[70] training(only) time: 16.066945552825928
# Switched to evaluate mode...
Test: [  0/100]	Time  0.177 ( 0.177)	Loss 3.4421e-01 (3.4421e-01)	Acc@1  90.00 ( 90.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.019 ( 0.033)	Loss 3.9539e-01 (3.2120e-01)	Acc@1  91.00 ( 90.55)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.024 ( 0.027)	Loss 5.5549e-01 (3.2411e-01)	Acc@1  86.00 ( 90.48)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.020 ( 0.026)	Loss 3.8534e-01 (3.4142e-01)	Acc@1  88.00 ( 90.58)	Acc@5  99.00 ( 99.77)
Test: [ 40/100]	Time  0.020 ( 0.024)	Loss 3.5918e-01 (3.5303e-01)	Acc@1  91.00 ( 90.32)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.019 ( 0.024)	Loss 2.8456e-01 (3.4230e-01)	Acc@1  94.00 ( 90.67)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.018 ( 0.023)	Loss 4.1863e-01 (3.4835e-01)	Acc@1  88.00 ( 90.48)	Acc@5  99.00 ( 99.70)
Test: [ 70/100]	Time  0.020 ( 0.023)	Loss 3.8674e-01 (3.3648e-01)	Acc@1  88.00 ( 90.58)	Acc@5 100.00 ( 99.70)
Test: [ 80/100]	Time  0.023 ( 0.023)	Loss 2.0719e-01 (3.3276e-01)	Acc@1  94.00 ( 90.74)	Acc@5 100.00 ( 99.72)
Test: [ 90/100]	Time  0.018 ( 0.023)	Loss 2.4511e-01 (3.3377e-01)	Acc@1  94.00 ( 90.76)	Acc@5 100.00 ( 99.71)
 * Acc@1 90.890 Acc@5 99.720
### epoch[70] execution time: 18.42423176765442
EPOCH 71
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [71][  0/391]	Time  0.213 ( 0.213)	Data  0.171 ( 0.171)	Loss 8.3381e-02 (8.3381e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [71][ 10/391]	Time  0.039 ( 0.056)	Data  0.001 ( 0.016)	Loss 4.1189e-02 (4.6288e-02)	Acc@1  99.22 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [71][ 20/391]	Time  0.041 ( 0.049)	Data  0.001 ( 0.009)	Loss 4.7760e-02 (5.2897e-02)	Acc@1  97.66 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [71][ 30/391]	Time  0.040 ( 0.046)	Data  0.001 ( 0.006)	Loss 2.6500e-02 (5.5042e-02)	Acc@1 100.00 ( 98.39)	Acc@5 100.00 ( 99.97)
Epoch: [71][ 40/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.005)	Loss 4.0489e-02 (5.2270e-02)	Acc@1  99.22 ( 98.48)	Acc@5 100.00 ( 99.98)
Epoch: [71][ 50/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.0666e-01 (5.5460e-02)	Acc@1  95.31 ( 98.38)	Acc@5 100.00 ( 99.98)
Epoch: [71][ 60/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.004)	Loss 7.0379e-02 (5.6764e-02)	Acc@1  96.88 ( 98.25)	Acc@5 100.00 ( 99.99)
Epoch: [71][ 70/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.0909e-02 (5.4808e-02)	Acc@1 100.00 ( 98.33)	Acc@5 100.00 ( 99.99)
Epoch: [71][ 80/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.003)	Loss 9.6218e-02 (5.6518e-02)	Acc@1  96.88 ( 98.23)	Acc@5 100.00 ( 99.99)
Epoch: [71][ 90/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.2353e-01 (5.6771e-02)	Acc@1  95.31 ( 98.21)	Acc@5 100.00 ( 99.99)
Epoch: [71][100/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.9560e-02 (5.5250e-02)	Acc@1  97.66 ( 98.23)	Acc@5 100.00 ( 99.99)
Epoch: [71][110/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.8877e-02 (5.5952e-02)	Acc@1 100.00 ( 98.21)	Acc@5 100.00 ( 99.99)
Epoch: [71][120/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.3852e-02 (5.6426e-02)	Acc@1 100.00 ( 98.23)	Acc@5 100.00 ( 99.99)
Epoch: [71][130/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3959e-02 (5.5803e-02)	Acc@1  99.22 ( 98.26)	Acc@5 100.00 ( 99.99)
Epoch: [71][140/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7368e-02 (5.6144e-02)	Acc@1  98.44 ( 98.25)	Acc@5 100.00 ( 99.99)
Epoch: [71][150/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.8748e-02 (5.5358e-02)	Acc@1 100.00 ( 98.28)	Acc@5 100.00 ( 99.99)
Epoch: [71][160/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.6203e-02 (5.5217e-02)	Acc@1  98.44 ( 98.28)	Acc@5 100.00 ( 99.99)
Epoch: [71][170/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0428e-02 (5.4883e-02)	Acc@1  98.44 ( 98.29)	Acc@5 100.00 ( 99.99)
Epoch: [71][180/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.3489e-02 (5.4385e-02)	Acc@1  98.44 ( 98.31)	Acc@5 100.00 ( 99.99)
Epoch: [71][190/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2306e-02 (5.3846e-02)	Acc@1  97.66 ( 98.33)	Acc@5 100.00 ( 99.99)
Epoch: [71][200/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8702e-02 (5.3794e-02)	Acc@1  98.44 ( 98.33)	Acc@5 100.00 ( 99.99)
Epoch: [71][210/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.5667e-02 (5.3884e-02)	Acc@1  98.44 ( 98.31)	Acc@5 100.00 ( 99.99)
Epoch: [71][220/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.3025e-02 (5.3970e-02)	Acc@1  98.44 ( 98.30)	Acc@5 100.00 ( 99.99)
Epoch: [71][230/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.9535e-02 (5.4367e-02)	Acc@1  98.44 ( 98.28)	Acc@5 100.00 ( 99.99)
Epoch: [71][240/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0142e-01 (5.4982e-02)	Acc@1  96.88 ( 98.25)	Acc@5 100.00 ( 99.99)
Epoch: [71][250/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0610e-01 (5.5175e-02)	Acc@1  95.31 ( 98.24)	Acc@5 100.00 ( 99.99)
Epoch: [71][260/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0367e-01 (5.5265e-02)	Acc@1  95.31 ( 98.23)	Acc@5 100.00 ( 99.99)
Epoch: [71][270/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.2470e-02 (5.5501e-02)	Acc@1  97.66 ( 98.20)	Acc@5 100.00 ( 99.99)
Epoch: [71][280/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.8774e-02 (5.5396e-02)	Acc@1  97.66 ( 98.20)	Acc@5 100.00 ( 99.99)
Epoch: [71][290/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9262e-02 (5.5332e-02)	Acc@1  98.44 ( 98.20)	Acc@5 100.00 ( 99.99)
Epoch: [71][300/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.4340e-02 (5.6081e-02)	Acc@1  96.88 ( 98.18)	Acc@5 100.00 ( 99.99)
Epoch: [71][310/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.9672e-02 (5.6058e-02)	Acc@1  98.44 ( 98.19)	Acc@5 100.00 ( 99.99)
Epoch: [71][320/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.2484e-01 (5.6457e-02)	Acc@1  96.88 ( 98.18)	Acc@5 100.00 ( 99.99)
Epoch: [71][330/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.3387e-02 (5.6136e-02)	Acc@1  98.44 ( 98.20)	Acc@5 100.00 ( 99.99)
Epoch: [71][340/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.5018e-02 (5.5895e-02)	Acc@1  99.22 ( 98.21)	Acc@5 100.00 ( 99.99)
Epoch: [71][350/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.2555e-02 (5.5709e-02)	Acc@1  98.44 ( 98.22)	Acc@5 100.00 ( 99.99)
Epoch: [71][360/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.0133e-02 (5.5651e-02)	Acc@1  99.22 ( 98.22)	Acc@5 100.00 ( 99.99)
Epoch: [71][370/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.4297e-02 (5.5710e-02)	Acc@1  97.66 ( 98.22)	Acc@5 100.00 ( 99.99)
Epoch: [71][380/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.4304e-02 (5.5894e-02)	Acc@1  97.66 ( 98.21)	Acc@5 100.00 ( 99.99)
Epoch: [71][390/391]	Time  0.028 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.6079e-02 (5.6201e-02)	Acc@1  98.75 ( 98.20)	Acc@5 100.00 ( 99.99)
## e[71] optimizer.zero_grad (sum) time: 0.2625396251678467
## e[71]       loss.backward (sum) time: 4.022696018218994
## e[71]      optimizer.step (sum) time: 1.7687838077545166
## epoch[71] training(only) time: 16.066927433013916
# Switched to evaluate mode...
Test: [  0/100]	Time  0.215 ( 0.215)	Loss 3.4903e-01 (3.4903e-01)	Acc@1  91.00 ( 91.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.022 ( 0.039)	Loss 4.2558e-01 (3.2293e-01)	Acc@1  90.00 ( 91.00)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.018 ( 0.029)	Loss 5.5086e-01 (3.2920e-01)	Acc@1  87.00 ( 90.86)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.022 ( 0.026)	Loss 3.9117e-01 (3.4359e-01)	Acc@1  89.00 ( 90.77)	Acc@5  99.00 ( 99.77)
Test: [ 40/100]	Time  0.022 ( 0.025)	Loss 3.6134e-01 (3.5260e-01)	Acc@1  91.00 ( 90.46)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.018 ( 0.024)	Loss 2.8043e-01 (3.4257e-01)	Acc@1  94.00 ( 90.78)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.021 ( 0.023)	Loss 4.1695e-01 (3.5007e-01)	Acc@1  88.00 ( 90.56)	Acc@5  99.00 ( 99.70)
Test: [ 70/100]	Time  0.022 ( 0.023)	Loss 4.1736e-01 (3.3878e-01)	Acc@1  87.00 ( 90.59)	Acc@5 100.00 ( 99.70)
Test: [ 80/100]	Time  0.017 ( 0.023)	Loss 2.1586e-01 (3.3542e-01)	Acc@1  94.00 ( 90.77)	Acc@5 100.00 ( 99.70)
Test: [ 90/100]	Time  0.022 ( 0.022)	Loss 2.5232e-01 (3.3646e-01)	Acc@1  94.00 ( 90.81)	Acc@5 100.00 ( 99.71)
 * Acc@1 90.900 Acc@5 99.720
### epoch[71] execution time: 18.381608724594116
EPOCH 72
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [72][  0/391]	Time  0.249 ( 0.249)	Data  0.209 ( 0.209)	Loss 5.2705e-02 (5.2705e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [72][ 10/391]	Time  0.039 ( 0.059)	Data  0.001 ( 0.020)	Loss 4.0808e-02 (5.5968e-02)	Acc@1  98.44 ( 98.01)	Acc@5 100.00 (100.00)
Epoch: [72][ 20/391]	Time  0.042 ( 0.051)	Data  0.001 ( 0.011)	Loss 5.3037e-02 (5.7127e-02)	Acc@1  98.44 ( 98.14)	Acc@5 100.00 (100.00)
Epoch: [72][ 30/391]	Time  0.041 ( 0.047)	Data  0.001 ( 0.008)	Loss 8.8821e-02 (5.4752e-02)	Acc@1  96.88 ( 98.21)	Acc@5 100.00 (100.00)
Epoch: [72][ 40/391]	Time  0.043 ( 0.046)	Data  0.001 ( 0.006)	Loss 3.2033e-02 (5.4228e-02)	Acc@1  99.22 ( 98.25)	Acc@5 100.00 (100.00)
Epoch: [72][ 50/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.005)	Loss 2.9436e-02 (5.5077e-02)	Acc@1  99.22 ( 98.35)	Acc@5 100.00 (100.00)
Epoch: [72][ 60/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.004)	Loss 3.8761e-02 (5.4558e-02)	Acc@1  98.44 ( 98.31)	Acc@5 100.00 (100.00)
Epoch: [72][ 70/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.8049e-02 (5.3136e-02)	Acc@1 100.00 ( 98.42)	Acc@5 100.00 (100.00)
Epoch: [72][ 80/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.004)	Loss 9.6631e-03 (5.2180e-02)	Acc@1 100.00 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [72][ 90/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.4514e-02 (5.2348e-02)	Acc@1  99.22 ( 98.38)	Acc@5 100.00 (100.00)
Epoch: [72][100/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.9801e-02 (5.2655e-02)	Acc@1  98.44 ( 98.31)	Acc@5 100.00 (100.00)
Epoch: [72][110/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.8474e-02 (5.3015e-02)	Acc@1  98.44 ( 98.32)	Acc@5 100.00 (100.00)
Epoch: [72][120/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.4983e-02 (5.3117e-02)	Acc@1  99.22 ( 98.32)	Acc@5 100.00 (100.00)
Epoch: [72][130/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.9548e-02 (5.2883e-02)	Acc@1  99.22 ( 98.34)	Acc@5 100.00 ( 99.99)
Epoch: [72][140/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.6975e-02 (5.2973e-02)	Acc@1  97.66 ( 98.32)	Acc@5 100.00 ( 99.99)
Epoch: [72][150/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.6695e-02 (5.2371e-02)	Acc@1  97.66 ( 98.33)	Acc@5 100.00 ( 99.99)
Epoch: [72][160/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.0306e-02 (5.2512e-02)	Acc@1  99.22 ( 98.31)	Acc@5 100.00 (100.00)
Epoch: [72][170/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.2365e-02 (5.2569e-02)	Acc@1  98.44 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [72][180/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 8.2137e-02 (5.2348e-02)	Acc@1  96.88 ( 98.33)	Acc@5 100.00 (100.00)
Epoch: [72][190/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.3331e-02 (5.2423e-02)	Acc@1  97.66 ( 98.32)	Acc@5 100.00 (100.00)
Epoch: [72][200/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.3463e-02 (5.1956e-02)	Acc@1  98.44 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [72][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9270e-02 (5.1837e-02)	Acc@1  98.44 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [72][220/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1992e-02 (5.1843e-02)	Acc@1  98.44 ( 98.38)	Acc@5 100.00 (100.00)
Epoch: [72][230/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.0037e-02 (5.1889e-02)	Acc@1  98.44 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [72][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1180e-02 (5.2001e-02)	Acc@1  98.44 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [72][250/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.9770e-02 (5.2327e-02)	Acc@1  98.44 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [72][260/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.8108e-02 (5.2318e-02)	Acc@1  96.09 ( 98.35)	Acc@5 100.00 (100.00)
Epoch: [72][270/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.4160e-02 (5.2155e-02)	Acc@1  98.44 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [72][280/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4666e-02 (5.2077e-02)	Acc@1  99.22 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [72][290/391]	Time  0.045 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1513e-02 (5.2042e-02)	Acc@1  99.22 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [72][300/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5896e-02 (5.1730e-02)	Acc@1  99.22 ( 98.39)	Acc@5 100.00 (100.00)
Epoch: [72][310/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.6238e-02 (5.1484e-02)	Acc@1  97.66 ( 98.39)	Acc@5 100.00 (100.00)
Epoch: [72][320/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0048e-01 (5.1965e-02)	Acc@1  96.88 ( 98.39)	Acc@5 100.00 (100.00)
Epoch: [72][330/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9108e-02 (5.1660e-02)	Acc@1  99.22 ( 98.41)	Acc@5 100.00 (100.00)
Epoch: [72][340/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.8572e-02 (5.1951e-02)	Acc@1  97.66 ( 98.40)	Acc@5 100.00 (100.00)
Epoch: [72][350/391]	Time  0.035 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4247e-02 (5.2110e-02)	Acc@1  98.44 ( 98.38)	Acc@5 100.00 (100.00)
Epoch: [72][360/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.6129e-02 (5.2192e-02)	Acc@1  97.66 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [72][370/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.0080e-02 (5.2151e-02)	Acc@1  99.22 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [72][380/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0674e-02 (5.2298e-02)	Acc@1  98.44 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [72][390/391]	Time  0.028 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1202e-02 (5.2509e-02)	Acc@1  98.75 ( 98.34)	Acc@5 100.00 (100.00)
## e[72] optimizer.zero_grad (sum) time: 0.2639799118041992
## e[72]       loss.backward (sum) time: 3.9537606239318848
## e[72]      optimizer.step (sum) time: 1.8175122737884521
## epoch[72] training(only) time: 15.983915328979492
# Switched to evaluate mode...
Test: [  0/100]	Time  0.173 ( 0.173)	Loss 3.4044e-01 (3.4044e-01)	Acc@1  91.00 ( 91.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.022 ( 0.036)	Loss 4.0139e-01 (3.1872e-01)	Acc@1  91.00 ( 91.18)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.021 ( 0.029)	Loss 5.4740e-01 (3.2776e-01)	Acc@1  86.00 ( 90.86)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.022 ( 0.026)	Loss 3.9724e-01 (3.4282e-01)	Acc@1  89.00 ( 90.81)	Acc@5  99.00 ( 99.71)
Test: [ 40/100]	Time  0.020 ( 0.024)	Loss 3.7969e-01 (3.5403e-01)	Acc@1  90.00 ( 90.46)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.020 ( 0.023)	Loss 2.6089e-01 (3.4370e-01)	Acc@1  95.00 ( 90.82)	Acc@5 100.00 ( 99.67)
Test: [ 60/100]	Time  0.021 ( 0.023)	Loss 4.3734e-01 (3.5034e-01)	Acc@1  87.00 ( 90.56)	Acc@5  99.00 ( 99.67)
Test: [ 70/100]	Time  0.022 ( 0.023)	Loss 4.2550e-01 (3.3890e-01)	Acc@1  88.00 ( 90.61)	Acc@5 100.00 ( 99.68)
Test: [ 80/100]	Time  0.019 ( 0.022)	Loss 2.1760e-01 (3.3503e-01)	Acc@1  94.00 ( 90.75)	Acc@5 100.00 ( 99.69)
Test: [ 90/100]	Time  0.023 ( 0.022)	Loss 2.4935e-01 (3.3608e-01)	Acc@1  94.00 ( 90.78)	Acc@5  99.00 ( 99.69)
 * Acc@1 90.920 Acc@5 99.700
### epoch[72] execution time: 18.326487064361572
EPOCH 73
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [73][  0/391]	Time  0.225 ( 0.225)	Data  0.185 ( 0.185)	Loss 7.3586e-02 (7.3586e-02)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [73][ 10/391]	Time  0.039 ( 0.057)	Data  0.001 ( 0.018)	Loss 7.4180e-02 (4.8118e-02)	Acc@1  96.88 ( 98.01)	Acc@5 100.00 (100.00)
Epoch: [73][ 20/391]	Time  0.040 ( 0.049)	Data  0.001 ( 0.010)	Loss 6.3995e-02 (5.0354e-02)	Acc@1  97.66 ( 98.07)	Acc@5 100.00 (100.00)
Epoch: [73][ 30/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.007)	Loss 5.5576e-02 (5.2096e-02)	Acc@1  97.66 ( 97.98)	Acc@5 100.00 (100.00)
Epoch: [73][ 40/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.005)	Loss 5.1872e-02 (5.8257e-02)	Acc@1  97.66 ( 97.88)	Acc@5 100.00 (100.00)
Epoch: [73][ 50/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.005)	Loss 4.3934e-02 (6.0504e-02)	Acc@1  98.44 ( 97.86)	Acc@5 100.00 (100.00)
Epoch: [73][ 60/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.004)	Loss 9.5985e-02 (5.9924e-02)	Acc@1  96.09 ( 97.85)	Acc@5 100.00 (100.00)
Epoch: [73][ 70/391]	Time  0.043 ( 0.043)	Data  0.001 ( 0.004)	Loss 4.9914e-02 (5.8271e-02)	Acc@1  97.66 ( 97.93)	Acc@5 100.00 (100.00)
Epoch: [73][ 80/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.8429e-02 (5.6844e-02)	Acc@1  98.44 ( 97.97)	Acc@5 100.00 (100.00)
Epoch: [73][ 90/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.2154e-02 (5.6429e-02)	Acc@1  98.44 ( 97.97)	Acc@5 100.00 (100.00)
Epoch: [73][100/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.8951e-02 (5.5952e-02)	Acc@1  97.66 ( 97.97)	Acc@5 100.00 (100.00)
Epoch: [73][110/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.7017e-02 (5.4756e-02)	Acc@1  99.22 ( 98.07)	Acc@5 100.00 (100.00)
Epoch: [73][120/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.0006e-02 (5.4694e-02)	Acc@1  98.44 ( 98.08)	Acc@5 100.00 (100.00)
Epoch: [73][130/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.1951e-02 (5.4087e-02)	Acc@1  97.66 ( 98.10)	Acc@5 100.00 ( 99.99)
Epoch: [73][140/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 8.8756e-02 (5.3563e-02)	Acc@1  95.31 ( 98.12)	Acc@5 100.00 ( 99.99)
Epoch: [73][150/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.0372e-02 (5.3481e-02)	Acc@1 100.00 ( 98.13)	Acc@5 100.00 ( 99.99)
Epoch: [73][160/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.5403e-02 (5.3053e-02)	Acc@1  98.44 ( 98.15)	Acc@5 100.00 (100.00)
Epoch: [73][170/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 8.6537e-02 (5.3112e-02)	Acc@1  95.31 ( 98.17)	Acc@5 100.00 (100.00)
Epoch: [73][180/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.3529e-02 (5.2898e-02)	Acc@1  96.09 ( 98.18)	Acc@5 100.00 (100.00)
Epoch: [73][190/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.8686e-02 (5.3294e-02)	Acc@1  97.66 ( 98.16)	Acc@5 100.00 (100.00)
Epoch: [73][200/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3906e-02 (5.2773e-02)	Acc@1  99.22 ( 98.19)	Acc@5 100.00 (100.00)
Epoch: [73][210/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7158e-02 (5.2925e-02)	Acc@1  98.44 ( 98.18)	Acc@5 100.00 (100.00)
Epoch: [73][220/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.7121e-02 (5.2694e-02)	Acc@1  97.66 ( 98.19)	Acc@5 100.00 (100.00)
Epoch: [73][230/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9605e-02 (5.3132e-02)	Acc@1  98.44 ( 98.17)	Acc@5 100.00 (100.00)
Epoch: [73][240/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.8507e-02 (5.3495e-02)	Acc@1 100.00 ( 98.16)	Acc@5 100.00 (100.00)
Epoch: [73][250/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0909e-02 (5.3277e-02)	Acc@1  99.22 ( 98.17)	Acc@5 100.00 (100.00)
Epoch: [73][260/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.4032e-02 (5.3952e-02)	Acc@1  96.88 ( 98.15)	Acc@5 100.00 (100.00)
Epoch: [73][270/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.8572e-02 (5.4071e-02)	Acc@1  98.44 ( 98.15)	Acc@5 100.00 (100.00)
Epoch: [73][280/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3285e-02 (5.4420e-02)	Acc@1  99.22 ( 98.13)	Acc@5 100.00 (100.00)
Epoch: [73][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.0481e-02 (5.4767e-02)	Acc@1  98.44 ( 98.12)	Acc@5 100.00 (100.00)
Epoch: [73][300/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4049e-02 (5.4472e-02)	Acc@1  99.22 ( 98.13)	Acc@5 100.00 (100.00)
Epoch: [73][310/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.6211e-02 (5.4132e-02)	Acc@1 100.00 ( 98.15)	Acc@5 100.00 (100.00)
Epoch: [73][320/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3292e-02 (5.4546e-02)	Acc@1  99.22 ( 98.13)	Acc@5 100.00 (100.00)
Epoch: [73][330/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5677e-02 (5.4542e-02)	Acc@1  98.44 ( 98.14)	Acc@5 100.00 (100.00)
Epoch: [73][340/391]	Time  0.038 ( 0.041)	Data  0.002 ( 0.002)	Loss 6.7543e-02 (5.4571e-02)	Acc@1  96.88 ( 98.14)	Acc@5 100.00 (100.00)
Epoch: [73][350/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0088e-01 (5.4839e-02)	Acc@1  95.31 ( 98.12)	Acc@5 100.00 (100.00)
Epoch: [73][360/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9308e-02 (5.5046e-02)	Acc@1  98.44 ( 98.11)	Acc@5 100.00 (100.00)
Epoch: [73][370/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.6196e-02 (5.5024e-02)	Acc@1  98.44 ( 98.10)	Acc@5 100.00 (100.00)
Epoch: [73][380/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.7177e-02 (5.5006e-02)	Acc@1  96.88 ( 98.10)	Acc@5 100.00 (100.00)
Epoch: [73][390/391]	Time  0.028 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.1390e-02 (5.4890e-02)	Acc@1  98.75 ( 98.10)	Acc@5 100.00 (100.00)
## e[73] optimizer.zero_grad (sum) time: 0.2634913921356201
## e[73]       loss.backward (sum) time: 3.9846036434173584
## e[73]      optimizer.step (sum) time: 1.8275418281555176
## epoch[73] training(only) time: 16.04110026359558
# Switched to evaluate mode...
Test: [  0/100]	Time  0.172 ( 0.172)	Loss 3.4103e-01 (3.4103e-01)	Acc@1  91.00 ( 91.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.018 ( 0.034)	Loss 3.9293e-01 (3.2111e-01)	Acc@1  91.00 ( 90.91)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.021 ( 0.028)	Loss 5.3720e-01 (3.2657e-01)	Acc@1  88.00 ( 90.86)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.024 ( 0.026)	Loss 3.7510e-01 (3.4173e-01)	Acc@1  88.00 ( 90.71)	Acc@5  99.00 ( 99.81)
Test: [ 40/100]	Time  0.024 ( 0.025)	Loss 3.5549e-01 (3.5440e-01)	Acc@1  91.00 ( 90.37)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.023 ( 0.024)	Loss 2.8997e-01 (3.4445e-01)	Acc@1  95.00 ( 90.76)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.018 ( 0.023)	Loss 4.2183e-01 (3.5042e-01)	Acc@1  89.00 ( 90.56)	Acc@5  99.00 ( 99.75)
Test: [ 70/100]	Time  0.021 ( 0.022)	Loss 3.9824e-01 (3.3821e-01)	Acc@1  88.00 ( 90.72)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.020 ( 0.022)	Loss 2.0963e-01 (3.3455e-01)	Acc@1  95.00 ( 90.89)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.024 ( 0.022)	Loss 2.5563e-01 (3.3520e-01)	Acc@1  94.00 ( 90.91)	Acc@5 100.00 ( 99.77)
 * Acc@1 91.010 Acc@5 99.770
### epoch[73] execution time: 18.321717977523804
EPOCH 74
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [74][  0/391]	Time  0.221 ( 0.221)	Data  0.179 ( 0.179)	Loss 6.2854e-02 (6.2854e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [74][ 10/391]	Time  0.042 ( 0.057)	Data  0.001 ( 0.017)	Loss 4.5169e-02 (4.8009e-02)	Acc@1  99.22 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [74][ 20/391]	Time  0.040 ( 0.050)	Data  0.001 ( 0.009)	Loss 2.7713e-02 (5.0839e-02)	Acc@1  99.22 ( 98.33)	Acc@5 100.00 (100.00)
Epoch: [74][ 30/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.007)	Loss 2.2125e-02 (4.9369e-02)	Acc@1 100.00 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [74][ 40/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.005)	Loss 6.8841e-02 (5.1247e-02)	Acc@1  98.44 ( 98.27)	Acc@5 100.00 (100.00)
Epoch: [74][ 50/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.004)	Loss 2.8852e-02 (5.2382e-02)	Acc@1  99.22 ( 98.22)	Acc@5 100.00 (100.00)
Epoch: [74][ 60/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.004)	Loss 6.3192e-02 (5.6114e-02)	Acc@1  97.66 ( 98.10)	Acc@5 100.00 (100.00)
Epoch: [74][ 70/391]	Time  0.043 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.8624e-02 (5.3851e-02)	Acc@1  99.22 ( 98.24)	Acc@5 100.00 (100.00)
Epoch: [74][ 80/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.3336e-02 (5.3552e-02)	Acc@1  99.22 ( 98.26)	Acc@5 100.00 (100.00)
Epoch: [74][ 90/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.5016e-02 (5.2374e-02)	Acc@1  99.22 ( 98.31)	Acc@5 100.00 (100.00)
Epoch: [74][100/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.8412e-02 (5.1307e-02)	Acc@1  98.44 ( 98.34)	Acc@5 100.00 (100.00)
Epoch: [74][110/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.2358e-02 (5.2182e-02)	Acc@1  98.44 ( 98.29)	Acc@5 100.00 (100.00)
Epoch: [74][120/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.7262e-02 (5.1698e-02)	Acc@1  96.88 ( 98.29)	Acc@5 100.00 (100.00)
Epoch: [74][130/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.6034e-02 (5.2116e-02)	Acc@1  99.22 ( 98.27)	Acc@5 100.00 (100.00)
Epoch: [74][140/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 8.2753e-02 (5.2090e-02)	Acc@1  96.09 ( 98.27)	Acc@5 100.00 (100.00)
Epoch: [74][150/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8228e-02 (5.2579e-02)	Acc@1  98.44 ( 98.23)	Acc@5 100.00 (100.00)
Epoch: [74][160/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9786e-02 (5.2786e-02)	Acc@1  98.44 ( 98.24)	Acc@5 100.00 (100.00)
Epoch: [74][170/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1928e-02 (5.3085e-02)	Acc@1  98.44 ( 98.20)	Acc@5 100.00 (100.00)
Epoch: [74][180/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.6344e-02 (5.3437e-02)	Acc@1  96.88 ( 98.20)	Acc@5 100.00 (100.00)
Epoch: [74][190/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.8813e-02 (5.3623e-02)	Acc@1  96.88 ( 98.19)	Acc@5 100.00 (100.00)
Epoch: [74][200/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9800e-02 (5.3636e-02)	Acc@1  98.44 ( 98.22)	Acc@5 100.00 (100.00)
Epoch: [74][210/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7142e-02 (5.3668e-02)	Acc@1  98.44 ( 98.22)	Acc@5 100.00 (100.00)
Epoch: [74][220/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6079e-02 (5.4045e-02)	Acc@1  98.44 ( 98.21)	Acc@5 100.00 ( 99.99)
Epoch: [74][230/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.8494e-02 (5.3901e-02)	Acc@1  98.44 ( 98.21)	Acc@5 100.00 ( 99.99)
Epoch: [74][240/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.4583e-02 (5.3860e-02)	Acc@1  97.66 ( 98.21)	Acc@5 100.00 ( 99.99)
Epoch: [74][250/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.6708e-02 (5.4357e-02)	Acc@1  96.88 ( 98.17)	Acc@5 100.00 ( 99.99)
Epoch: [74][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4415e-02 (5.4290e-02)	Acc@1  98.44 ( 98.17)	Acc@5 100.00 ( 99.99)
Epoch: [74][270/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2044e-02 (5.4007e-02)	Acc@1  97.66 ( 98.17)	Acc@5 100.00 ( 99.99)
Epoch: [74][280/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2387e-02 (5.3657e-02)	Acc@1  99.22 ( 98.19)	Acc@5 100.00 ( 99.99)
Epoch: [74][290/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1378e-01 (5.3494e-02)	Acc@1  95.31 ( 98.19)	Acc@5 100.00 ( 99.99)
Epoch: [74][300/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.6815e-02 (5.3618e-02)	Acc@1  96.88 ( 98.19)	Acc@5 100.00 ( 99.99)
Epoch: [74][310/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1810e-02 (5.3709e-02)	Acc@1  97.66 ( 98.18)	Acc@5 100.00 ( 99.99)
Epoch: [74][320/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8595e-02 (5.3714e-02)	Acc@1  98.44 ( 98.18)	Acc@5 100.00 (100.00)
Epoch: [74][330/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.4279e-02 (5.3487e-02)	Acc@1 100.00 ( 98.19)	Acc@5 100.00 (100.00)
Epoch: [74][340/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 8.6321e-02 (5.3564e-02)	Acc@1  97.66 ( 98.19)	Acc@5 100.00 (100.00)
Epoch: [74][350/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.3807e-02 (5.3404e-02)	Acc@1 100.00 ( 98.20)	Acc@5 100.00 (100.00)
Epoch: [74][360/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.5836e-02 (5.3365e-02)	Acc@1  98.44 ( 98.20)	Acc@5 100.00 (100.00)
Epoch: [74][370/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.2243e-02 (5.3214e-02)	Acc@1  98.44 ( 98.21)	Acc@5 100.00 (100.00)
Epoch: [74][380/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 8.6814e-02 (5.3358e-02)	Acc@1  96.88 ( 98.20)	Acc@5 100.00 (100.00)
Epoch: [74][390/391]	Time  0.028 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.2917e-02 (5.3367e-02)	Acc@1  97.50 ( 98.20)	Acc@5 100.00 (100.00)
## e[74] optimizer.zero_grad (sum) time: 0.2627735137939453
## e[74]       loss.backward (sum) time: 3.998448133468628
## e[74]      optimizer.step (sum) time: 1.803501844406128
## epoch[74] training(only) time: 15.990777254104614
# Switched to evaluate mode...
Test: [  0/100]	Time  0.176 ( 0.176)	Loss 3.2877e-01 (3.2877e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.021 ( 0.033)	Loss 3.7896e-01 (3.1608e-01)	Acc@1  91.00 ( 91.27)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.021 ( 0.027)	Loss 5.8906e-01 (3.2740e-01)	Acc@1  86.00 ( 91.00)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.021 ( 0.026)	Loss 3.9538e-01 (3.4347e-01)	Acc@1  89.00 ( 90.77)	Acc@5  99.00 ( 99.74)
Test: [ 40/100]	Time  0.022 ( 0.024)	Loss 3.7526e-01 (3.5565e-01)	Acc@1  90.00 ( 90.54)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.018 ( 0.023)	Loss 2.8057e-01 (3.4546e-01)	Acc@1  94.00 ( 90.86)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.018 ( 0.023)	Loss 4.2499e-01 (3.5230e-01)	Acc@1  88.00 ( 90.59)	Acc@5  99.00 ( 99.72)
Test: [ 70/100]	Time  0.022 ( 0.022)	Loss 4.1599e-01 (3.4050e-01)	Acc@1  88.00 ( 90.63)	Acc@5 100.00 ( 99.72)
Test: [ 80/100]	Time  0.020 ( 0.022)	Loss 2.0048e-01 (3.3704e-01)	Acc@1  95.00 ( 90.78)	Acc@5 100.00 ( 99.73)
Test: [ 90/100]	Time  0.017 ( 0.022)	Loss 2.5229e-01 (3.3863e-01)	Acc@1  94.00 ( 90.79)	Acc@5 100.00 ( 99.73)
 * Acc@1 90.880 Acc@5 99.730
### epoch[74] execution time: 18.261979579925537
EPOCH 75
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [75][  0/391]	Time  0.228 ( 0.228)	Data  0.185 ( 0.185)	Loss 1.5598e-02 (1.5598e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [75][ 10/391]	Time  0.040 ( 0.057)	Data  0.001 ( 0.018)	Loss 6.4235e-02 (4.7579e-02)	Acc@1  97.66 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [75][ 20/391]	Time  0.042 ( 0.049)	Data  0.001 ( 0.010)	Loss 4.2331e-02 (5.0072e-02)	Acc@1  99.22 ( 98.47)	Acc@5 100.00 (100.00)
Epoch: [75][ 30/391]	Time  0.038 ( 0.046)	Data  0.001 ( 0.007)	Loss 6.1307e-02 (5.0084e-02)	Acc@1  98.44 ( 98.41)	Acc@5 100.00 (100.00)
Epoch: [75][ 40/391]	Time  0.040 ( 0.045)	Data  0.001 ( 0.005)	Loss 4.1721e-02 (4.8655e-02)	Acc@1  99.22 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [75][ 50/391]	Time  0.044 ( 0.044)	Data  0.001 ( 0.005)	Loss 9.7795e-02 (4.8970e-02)	Acc@1  96.88 ( 98.47)	Acc@5 100.00 (100.00)
Epoch: [75][ 60/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.004)	Loss 3.4254e-02 (5.1132e-02)	Acc@1  99.22 ( 98.40)	Acc@5 100.00 (100.00)
Epoch: [75][ 70/391]	Time  0.036 ( 0.043)	Data  0.001 ( 0.004)	Loss 6.1670e-02 (5.2641e-02)	Acc@1  98.44 ( 98.32)	Acc@5 100.00 (100.00)
Epoch: [75][ 80/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.5518e-02 (5.2001e-02)	Acc@1  98.44 ( 98.32)	Acc@5 100.00 (100.00)
Epoch: [75][ 90/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.5995e-02 (5.1621e-02)	Acc@1  99.22 ( 98.34)	Acc@5 100.00 (100.00)
Epoch: [75][100/391]	Time  0.035 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.9268e-02 (5.1984e-02)	Acc@1  96.88 ( 98.32)	Acc@5 100.00 (100.00)
Epoch: [75][110/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.0209e-01 (5.2178e-02)	Acc@1  96.88 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [75][120/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8077e-02 (5.2875e-02)	Acc@1  99.22 ( 98.29)	Acc@5 100.00 (100.00)
Epoch: [75][130/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1891e-02 (5.3680e-02)	Acc@1  98.44 ( 98.25)	Acc@5 100.00 (100.00)
Epoch: [75][140/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8956e-02 (5.3903e-02)	Acc@1  97.66 ( 98.25)	Acc@5 100.00 (100.00)
Epoch: [75][150/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3963e-02 (5.4467e-02)	Acc@1  98.44 ( 98.25)	Acc@5 100.00 (100.00)
Epoch: [75][160/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8324e-02 (5.4144e-02)	Acc@1  99.22 ( 98.27)	Acc@5 100.00 (100.00)
Epoch: [75][170/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.6173e-02 (5.3879e-02)	Acc@1 100.00 ( 98.28)	Acc@5 100.00 (100.00)
Epoch: [75][180/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1659e-02 (5.3827e-02)	Acc@1  99.22 ( 98.26)	Acc@5 100.00 (100.00)
Epoch: [75][190/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3138e-02 (5.3601e-02)	Acc@1  98.44 ( 98.26)	Acc@5 100.00 (100.00)
Epoch: [75][200/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1018e-02 (5.3955e-02)	Acc@1  98.44 ( 98.23)	Acc@5 100.00 (100.00)
Epoch: [75][210/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.4448e-02 (5.4333e-02)	Acc@1  97.66 ( 98.20)	Acc@5 100.00 (100.00)
Epoch: [75][220/391]	Time  0.049 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.8236e-02 (5.3878e-02)	Acc@1 100.00 ( 98.21)	Acc@5 100.00 (100.00)
Epoch: [75][230/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.7483e-02 (5.3265e-02)	Acc@1 100.00 ( 98.24)	Acc@5 100.00 (100.00)
Epoch: [75][240/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.4333e-02 (5.3585e-02)	Acc@1  97.66 ( 98.22)	Acc@5 100.00 (100.00)
Epoch: [75][250/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3067e-02 (5.3259e-02)	Acc@1 100.00 ( 98.23)	Acc@5 100.00 (100.00)
Epoch: [75][260/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.8581e-02 (5.3117e-02)	Acc@1 100.00 ( 98.24)	Acc@5 100.00 (100.00)
Epoch: [75][270/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.9506e-02 (5.2981e-02)	Acc@1  98.44 ( 98.25)	Acc@5 100.00 (100.00)
Epoch: [75][280/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0964e-02 (5.2740e-02)	Acc@1  97.66 ( 98.26)	Acc@5 100.00 (100.00)
Epoch: [75][290/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2921e-02 (5.2730e-02)	Acc@1  97.66 ( 98.25)	Acc@5 100.00 (100.00)
Epoch: [75][300/391]	Time  0.040 ( 0.041)	Data  0.002 ( 0.002)	Loss 3.5191e-02 (5.2815e-02)	Acc@1 100.00 ( 98.27)	Acc@5 100.00 (100.00)
Epoch: [75][310/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.4621e-02 (5.3003e-02)	Acc@1  96.88 ( 98.25)	Acc@5 100.00 (100.00)
Epoch: [75][320/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.9388e-02 (5.3195e-02)	Acc@1  99.22 ( 98.24)	Acc@5 100.00 (100.00)
Epoch: [75][330/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4103e-02 (5.3278e-02)	Acc@1  98.44 ( 98.24)	Acc@5 100.00 (100.00)
Epoch: [75][340/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.0969e-02 (5.3521e-02)	Acc@1  99.22 ( 98.22)	Acc@5 100.00 (100.00)
Epoch: [75][350/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.7974e-02 (5.3806e-02)	Acc@1  97.66 ( 98.21)	Acc@5 100.00 (100.00)
Epoch: [75][360/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 9.6447e-02 (5.4090e-02)	Acc@1  96.88 ( 98.20)	Acc@5 100.00 (100.00)
Epoch: [75][370/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.9625e-02 (5.4229e-02)	Acc@1  97.66 ( 98.19)	Acc@5 100.00 (100.00)
Epoch: [75][380/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.8076e-02 (5.4164e-02)	Acc@1  98.44 ( 98.19)	Acc@5 100.00 (100.00)
Epoch: [75][390/391]	Time  0.029 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.2015e-01 (5.4238e-02)	Acc@1  96.25 ( 98.18)	Acc@5 100.00 (100.00)
## e[75] optimizer.zero_grad (sum) time: 0.26386022567749023
## e[75]       loss.backward (sum) time: 3.998074769973755
## e[75]      optimizer.step (sum) time: 1.8323962688446045
## epoch[75] training(only) time: 16.00437068939209
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 3.3978e-01 (3.3978e-01)	Acc@1  90.00 ( 90.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.020 ( 0.037)	Loss 4.1754e-01 (3.2504e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.016 ( 0.029)	Loss 5.6214e-01 (3.3311e-01)	Acc@1  87.00 ( 90.90)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.020 ( 0.026)	Loss 3.6238e-01 (3.4989e-01)	Acc@1  90.00 ( 90.90)	Acc@5  99.00 ( 99.77)
Test: [ 40/100]	Time  0.020 ( 0.025)	Loss 3.6177e-01 (3.6168e-01)	Acc@1  91.00 ( 90.41)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.021 ( 0.024)	Loss 2.6289e-01 (3.5011e-01)	Acc@1  93.00 ( 90.73)	Acc@5 100.00 ( 99.69)
Test: [ 60/100]	Time  0.021 ( 0.023)	Loss 4.0998e-01 (3.5569e-01)	Acc@1  89.00 ( 90.54)	Acc@5  99.00 ( 99.70)
Test: [ 70/100]	Time  0.023 ( 0.023)	Loss 4.0498e-01 (3.4464e-01)	Acc@1  89.00 ( 90.56)	Acc@5 100.00 ( 99.70)
Test: [ 80/100]	Time  0.021 ( 0.023)	Loss 2.1518e-01 (3.4152e-01)	Acc@1  93.00 ( 90.69)	Acc@5 100.00 ( 99.70)
Test: [ 90/100]	Time  0.022 ( 0.023)	Loss 2.4803e-01 (3.4309e-01)	Acc@1  95.00 ( 90.77)	Acc@5 100.00 ( 99.71)
 * Acc@1 90.910 Acc@5 99.720
### epoch[75] execution time: 18.349819660186768
EPOCH 76
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [76][  0/391]	Time  0.259 ( 0.259)	Data  0.215 ( 0.215)	Loss 3.7071e-02 (3.7071e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [76][ 10/391]	Time  0.041 ( 0.061)	Data  0.001 ( 0.020)	Loss 7.4307e-02 (5.5462e-02)	Acc@1  96.88 ( 98.08)	Acc@5 100.00 (100.00)
Epoch: [76][ 20/391]	Time  0.041 ( 0.051)	Data  0.001 ( 0.011)	Loss 5.1774e-02 (4.9205e-02)	Acc@1  97.66 ( 98.40)	Acc@5 100.00 (100.00)
Epoch: [76][ 30/391]	Time  0.044 ( 0.048)	Data  0.001 ( 0.008)	Loss 6.9276e-02 (5.3299e-02)	Acc@1  97.66 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [76][ 40/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.006)	Loss 4.5521e-02 (5.5120e-02)	Acc@1  97.66 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [76][ 50/391]	Time  0.042 ( 0.045)	Data  0.001 ( 0.005)	Loss 3.4030e-02 (5.3793e-02)	Acc@1  99.22 ( 98.38)	Acc@5 100.00 (100.00)
Epoch: [76][ 60/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.005)	Loss 5.0266e-02 (5.2960e-02)	Acc@1  98.44 ( 98.42)	Acc@5 100.00 (100.00)
Epoch: [76][ 70/391]	Time  0.043 ( 0.043)	Data  0.001 ( 0.004)	Loss 5.8449e-02 (5.4799e-02)	Acc@1  96.88 ( 98.28)	Acc@5 100.00 (100.00)
Epoch: [76][ 80/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.004)	Loss 4.8129e-02 (5.3730e-02)	Acc@1  99.22 ( 98.33)	Acc@5 100.00 (100.00)
Epoch: [76][ 90/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 5.4757e-02 (5.3616e-02)	Acc@1  98.44 ( 98.33)	Acc@5 100.00 (100.00)
Epoch: [76][100/391]	Time  0.035 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.5393e-02 (5.3358e-02)	Acc@1  99.22 ( 98.31)	Acc@5 100.00 (100.00)
Epoch: [76][110/391]	Time  0.047 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.4366e-02 (5.2933e-02)	Acc@1  97.66 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [76][120/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.3669e-02 (5.2335e-02)	Acc@1  98.44 ( 98.33)	Acc@5 100.00 (100.00)
Epoch: [76][130/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.7811e-02 (5.2976e-02)	Acc@1  99.22 ( 98.31)	Acc@5 100.00 (100.00)
Epoch: [76][140/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.9486e-02 (5.2950e-02)	Acc@1  99.22 ( 98.32)	Acc@5 100.00 (100.00)
Epoch: [76][150/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.1518e-02 (5.3020e-02)	Acc@1  98.44 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [76][160/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.2176e-02 (5.3224e-02)	Acc@1  99.22 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [76][170/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7607e-02 (5.3340e-02)	Acc@1  98.44 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [76][180/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.6690e-02 (5.2995e-02)	Acc@1  97.66 ( 98.32)	Acc@5 100.00 (100.00)
Epoch: [76][190/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.8125e-02 (5.2639e-02)	Acc@1  97.66 ( 98.32)	Acc@5 100.00 (100.00)
Epoch: [76][200/391]	Time  0.039 ( 0.041)	Data  0.002 ( 0.002)	Loss 5.2881e-02 (5.2634e-02)	Acc@1  99.22 ( 98.34)	Acc@5 100.00 (100.00)
Epoch: [76][210/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4407e-02 (5.2887e-02)	Acc@1  99.22 ( 98.32)	Acc@5 100.00 (100.00)
Epoch: [76][220/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.8227e-02 (5.3196e-02)	Acc@1  98.44 ( 98.31)	Acc@5 100.00 (100.00)
Epoch: [76][230/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.8136e-02 (5.3129e-02)	Acc@1  97.66 ( 98.32)	Acc@5 100.00 (100.00)
Epoch: [76][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6799e-02 (5.2741e-02)	Acc@1  99.22 ( 98.32)	Acc@5 100.00 (100.00)
Epoch: [76][250/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0430e-01 (5.2858e-02)	Acc@1  96.09 ( 98.31)	Acc@5 100.00 (100.00)
Epoch: [76][260/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0335e-02 (5.2972e-02)	Acc@1  97.66 ( 98.29)	Acc@5 100.00 (100.00)
Epoch: [76][270/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3356e-02 (5.2457e-02)	Acc@1  98.44 ( 98.31)	Acc@5 100.00 (100.00)
Epoch: [76][280/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3541e-02 (5.2364e-02)	Acc@1  98.44 ( 98.32)	Acc@5 100.00 (100.00)
Epoch: [76][290/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3369e-02 (5.2352e-02)	Acc@1  99.22 ( 98.31)	Acc@5 100.00 (100.00)
Epoch: [76][300/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.7564e-02 (5.2039e-02)	Acc@1  98.44 ( 98.33)	Acc@5 100.00 (100.00)
Epoch: [76][310/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.5305e-02 (5.2181e-02)	Acc@1  97.66 ( 98.32)	Acc@5 100.00 (100.00)
Epoch: [76][320/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.8552e-02 (5.2213e-02)	Acc@1  99.22 ( 98.32)	Acc@5 100.00 (100.00)
Epoch: [76][330/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7775e-02 (5.2287e-02)	Acc@1  99.22 ( 98.31)	Acc@5 100.00 (100.00)
Epoch: [76][340/391]	Time  0.052 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.9020e-02 (5.2103e-02)	Acc@1  96.88 ( 98.32)	Acc@5 100.00 (100.00)
Epoch: [76][350/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0483e-02 (5.1951e-02)	Acc@1  98.44 ( 98.33)	Acc@5 100.00 (100.00)
Epoch: [76][360/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.5943e-02 (5.2031e-02)	Acc@1  99.22 ( 98.34)	Acc@5 100.00 (100.00)
Epoch: [76][370/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7125e-02 (5.2116e-02)	Acc@1  99.22 ( 98.34)	Acc@5 100.00 (100.00)
Epoch: [76][380/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.9174e-02 (5.1972e-02)	Acc@1  99.22 ( 98.35)	Acc@5 100.00 (100.00)
Epoch: [76][390/391]	Time  0.027 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3488e-02 (5.2107e-02)	Acc@1  98.75 ( 98.35)	Acc@5 100.00 (100.00)
## e[76] optimizer.zero_grad (sum) time: 0.26189756393432617
## e[76]       loss.backward (sum) time: 3.9720065593719482
## e[76]      optimizer.step (sum) time: 1.802903413772583
## epoch[76] training(only) time: 16.139046907424927
# Switched to evaluate mode...
Test: [  0/100]	Time  0.182 ( 0.182)	Loss 3.4788e-01 (3.4788e-01)	Acc@1  90.00 ( 90.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.021 ( 0.036)	Loss 3.8585e-01 (3.2481e-01)	Acc@1  91.00 ( 90.45)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.025 ( 0.029)	Loss 5.8388e-01 (3.2872e-01)	Acc@1  86.00 ( 90.62)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.024 ( 0.026)	Loss 3.9178e-01 (3.4488e-01)	Acc@1  90.00 ( 90.58)	Acc@5  99.00 ( 99.77)
Test: [ 40/100]	Time  0.023 ( 0.025)	Loss 3.7776e-01 (3.5723e-01)	Acc@1  91.00 ( 90.41)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.021 ( 0.024)	Loss 2.7041e-01 (3.4528e-01)	Acc@1  95.00 ( 90.82)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.019 ( 0.024)	Loss 4.3649e-01 (3.5202e-01)	Acc@1  90.00 ( 90.59)	Acc@5  99.00 ( 99.70)
Test: [ 70/100]	Time  0.017 ( 0.023)	Loss 4.2864e-01 (3.4111e-01)	Acc@1  87.00 ( 90.62)	Acc@5 100.00 ( 99.70)
Test: [ 80/100]	Time  0.020 ( 0.023)	Loss 2.0057e-01 (3.3754e-01)	Acc@1  95.00 ( 90.75)	Acc@5 100.00 ( 99.70)
Test: [ 90/100]	Time  0.017 ( 0.022)	Loss 2.4951e-01 (3.3827e-01)	Acc@1  94.00 ( 90.79)	Acc@5 100.00 ( 99.70)
 * Acc@1 90.850 Acc@5 99.710
### epoch[76] execution time: 18.444483757019043
EPOCH 77
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [77][  0/391]	Time  0.212 ( 0.212)	Data  0.166 ( 0.166)	Loss 4.0827e-02 (4.0827e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [77][ 10/391]	Time  0.042 ( 0.056)	Data  0.001 ( 0.016)	Loss 3.8394e-02 (4.9374e-02)	Acc@1  99.22 ( 98.65)	Acc@5 100.00 (100.00)
Epoch: [77][ 20/391]	Time  0.039 ( 0.048)	Data  0.001 ( 0.009)	Loss 4.6050e-02 (4.7370e-02)	Acc@1  99.22 ( 98.55)	Acc@5 100.00 (100.00)
Epoch: [77][ 30/391]	Time  0.040 ( 0.046)	Data  0.001 ( 0.006)	Loss 6.3083e-02 (5.0016e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [77][ 40/391]	Time  0.043 ( 0.044)	Data  0.001 ( 0.005)	Loss 4.7281e-02 (5.0197e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [77][ 50/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.6738e-02 (4.9748e-02)	Acc@1 100.00 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [77][ 60/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.004)	Loss 7.5870e-02 (4.9946e-02)	Acc@1  96.88 ( 98.46)	Acc@5 100.00 (100.00)
Epoch: [77][ 70/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.7029e-02 (5.0124e-02)	Acc@1 100.00 ( 98.50)	Acc@5 100.00 (100.00)
Epoch: [77][ 80/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.9719e-02 (4.9390e-02)	Acc@1 100.00 ( 98.47)	Acc@5 100.00 (100.00)
Epoch: [77][ 90/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.5234e-02 (4.8792e-02)	Acc@1  99.22 ( 98.50)	Acc@5 100.00 (100.00)
Epoch: [77][100/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.3446e-02 (4.9421e-02)	Acc@1  97.66 ( 98.47)	Acc@5 100.00 (100.00)
Epoch: [77][110/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.0041e-01 (4.9946e-02)	Acc@1  96.88 ( 98.42)	Acc@5 100.00 (100.00)
Epoch: [77][120/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.2560e-02 (5.0513e-02)	Acc@1  98.44 ( 98.42)	Acc@5 100.00 (100.00)
Epoch: [77][130/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.4461e-02 (5.1736e-02)	Acc@1  99.22 ( 98.38)	Acc@5 100.00 (100.00)
Epoch: [77][140/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.4861e-02 (5.2081e-02)	Acc@1  99.22 ( 98.38)	Acc@5 100.00 (100.00)
Epoch: [77][150/391]	Time  0.035 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7992e-02 (5.2284e-02)	Acc@1  98.44 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [77][160/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.5553e-02 (5.2418e-02)	Acc@1  96.88 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [77][170/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.2286e-02 (5.2915e-02)	Acc@1  98.44 ( 98.33)	Acc@5 100.00 (100.00)
Epoch: [77][180/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.3542e-02 (5.3147e-02)	Acc@1 100.00 ( 98.33)	Acc@5 100.00 (100.00)
Epoch: [77][190/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6060e-02 (5.2849e-02)	Acc@1  99.22 ( 98.34)	Acc@5 100.00 (100.00)
Epoch: [77][200/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6514e-02 (5.3415e-02)	Acc@1  98.44 ( 98.33)	Acc@5 100.00 (100.00)
Epoch: [77][210/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.8743e-02 (5.2959e-02)	Acc@1  99.22 ( 98.35)	Acc@5 100.00 (100.00)
Epoch: [77][220/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.6768e-02 (5.3164e-02)	Acc@1  97.66 ( 98.34)	Acc@5 100.00 (100.00)
Epoch: [77][230/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6029e-02 (5.3178e-02)	Acc@1  99.22 ( 98.34)	Acc@5 100.00 (100.00)
Epoch: [77][240/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4506e-02 (5.3446e-02)	Acc@1  97.66 ( 98.34)	Acc@5 100.00 (100.00)
Epoch: [77][250/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4069e-02 (5.3198e-02)	Acc@1  99.22 ( 98.35)	Acc@5 100.00 (100.00)
Epoch: [77][260/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.0295e-02 (5.3686e-02)	Acc@1  96.88 ( 98.33)	Acc@5 100.00 (100.00)
Epoch: [77][270/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.8262e-02 (5.3979e-02)	Acc@1  97.66 ( 98.31)	Acc@5 100.00 ( 99.99)
Epoch: [77][280/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2479e-02 (5.3405e-02)	Acc@1  98.44 ( 98.33)	Acc@5 100.00 ( 99.99)
Epoch: [77][290/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1248e-02 (5.3744e-02)	Acc@1  98.44 ( 98.29)	Acc@5 100.00 ( 99.99)
Epoch: [77][300/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.7847e-02 (5.3592e-02)	Acc@1  97.66 ( 98.30)	Acc@5 100.00 ( 99.99)
Epoch: [77][310/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.8343e-02 (5.3460e-02)	Acc@1  97.66 ( 98.30)	Acc@5 100.00 ( 99.99)
Epoch: [77][320/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.5108e-02 (5.3103e-02)	Acc@1  98.44 ( 98.31)	Acc@5 100.00 (100.00)
Epoch: [77][330/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.4338e-02 (5.3252e-02)	Acc@1  98.44 ( 98.29)	Acc@5 100.00 (100.00)
Epoch: [77][340/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.6373e-02 (5.3234e-02)	Acc@1  98.44 ( 98.29)	Acc@5 100.00 (100.00)
Epoch: [77][350/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.1437e-02 (5.3159e-02)	Acc@1  98.44 ( 98.29)	Acc@5 100.00 (100.00)
Epoch: [77][360/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.7595e-02 (5.3586e-02)	Acc@1 100.00 ( 98.27)	Acc@5 100.00 ( 99.99)
Epoch: [77][370/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.8992e-02 (5.3320e-02)	Acc@1  97.66 ( 98.27)	Acc@5 100.00 ( 99.99)
Epoch: [77][380/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.4531e-02 (5.3368e-02)	Acc@1  99.22 ( 98.27)	Acc@5 100.00 ( 99.99)
Epoch: [77][390/391]	Time  0.028 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.0983e-02 (5.3242e-02)	Acc@1 100.00 ( 98.27)	Acc@5 100.00 ( 99.99)
## e[77] optimizer.zero_grad (sum) time: 0.2622661590576172
## e[77]       loss.backward (sum) time: 3.986466884613037
## e[77]      optimizer.step (sum) time: 1.8108901977539062
## epoch[77] training(only) time: 16.003398418426514
# Switched to evaluate mode...
Test: [  0/100]	Time  0.172 ( 0.172)	Loss 3.5533e-01 (3.5533e-01)	Acc@1  90.00 ( 90.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.023 ( 0.036)	Loss 3.9964e-01 (3.2915e-01)	Acc@1  91.00 ( 90.82)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.024 ( 0.030)	Loss 5.6027e-01 (3.3365e-01)	Acc@1  86.00 ( 90.86)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.020 ( 0.027)	Loss 3.7595e-01 (3.5151e-01)	Acc@1  90.00 ( 90.81)	Acc@5  99.00 ( 99.77)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 3.7768e-01 (3.6354e-01)	Acc@1  91.00 ( 90.41)	Acc@5 100.00 ( 99.76)
Test: [ 50/100]	Time  0.017 ( 0.025)	Loss 2.6403e-01 (3.5197e-01)	Acc@1  95.00 ( 90.73)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 4.1510e-01 (3.5588e-01)	Acc@1  89.00 ( 90.56)	Acc@5  99.00 ( 99.75)
Test: [ 70/100]	Time  0.021 ( 0.024)	Loss 4.1860e-01 (3.4458e-01)	Acc@1  88.00 ( 90.59)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.021 ( 0.024)	Loss 2.1637e-01 (3.4071e-01)	Acc@1  94.00 ( 90.70)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.021 ( 0.023)	Loss 2.4793e-01 (3.4116e-01)	Acc@1  94.00 ( 90.74)	Acc@5 100.00 ( 99.77)
 * Acc@1 90.810 Acc@5 99.770
### epoch[77] execution time: 18.416786909103394
EPOCH 78
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [78][  0/391]	Time  0.223 ( 0.223)	Data  0.180 ( 0.180)	Loss 5.0847e-02 (5.0847e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [78][ 10/391]	Time  0.044 ( 0.057)	Data  0.001 ( 0.017)	Loss 7.7658e-02 (4.9277e-02)	Acc@1  96.09 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [78][ 20/391]	Time  0.037 ( 0.049)	Data  0.001 ( 0.010)	Loss 6.1589e-02 (4.5521e-02)	Acc@1  96.88 ( 98.47)	Acc@5 100.00 (100.00)
Epoch: [78][ 30/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.007)	Loss 4.4069e-02 (4.6386e-02)	Acc@1  97.66 ( 98.49)	Acc@5 100.00 (100.00)
Epoch: [78][ 40/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.005)	Loss 5.2555e-02 (5.0000e-02)	Acc@1  98.44 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [78][ 50/391]	Time  0.037 ( 0.043)	Data  0.001 ( 0.005)	Loss 8.5291e-02 (5.3252e-02)	Acc@1  95.31 ( 98.24)	Acc@5 100.00 (100.00)
Epoch: [78][ 60/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.004)	Loss 6.2523e-02 (5.1100e-02)	Acc@1  98.44 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [78][ 70/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.004)	Loss 3.7542e-02 (5.0019e-02)	Acc@1  98.44 ( 98.38)	Acc@5 100.00 (100.00)
Epoch: [78][ 80/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.5436e-02 (5.0115e-02)	Acc@1  97.66 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [78][ 90/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 9.8371e-02 (5.1233e-02)	Acc@1  96.09 ( 98.33)	Acc@5 100.00 (100.00)
Epoch: [78][100/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.6159e-02 (5.1407e-02)	Acc@1 100.00 ( 98.34)	Acc@5 100.00 (100.00)
Epoch: [78][110/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.003)	Loss 2.9941e-02 (5.0589e-02)	Acc@1 100.00 ( 98.38)	Acc@5 100.00 (100.00)
Epoch: [78][120/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 3.5578e-02 (5.0354e-02)	Acc@1  99.22 ( 98.39)	Acc@5 100.00 (100.00)
Epoch: [78][130/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.1533e-02 (4.9721e-02)	Acc@1  96.88 ( 98.39)	Acc@5 100.00 (100.00)
Epoch: [78][140/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5215e-02 (4.9681e-02)	Acc@1 100.00 ( 98.41)	Acc@5 100.00 (100.00)
Epoch: [78][150/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7230e-02 (4.9261e-02)	Acc@1  97.66 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [78][160/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.9950e-02 (4.9072e-02)	Acc@1 100.00 ( 98.46)	Acc@5 100.00 (100.00)
Epoch: [78][170/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.5488e-02 (5.0070e-02)	Acc@1  97.66 ( 98.42)	Acc@5 100.00 (100.00)
Epoch: [78][180/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.9263e-02 (5.0308e-02)	Acc@1  96.88 ( 98.39)	Acc@5 100.00 (100.00)
Epoch: [78][190/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.5354e-02 (5.0554e-02)	Acc@1  97.66 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [78][200/391]	Time  0.047 ( 0.041)	Data  0.002 ( 0.002)	Loss 4.6754e-02 (5.0482e-02)	Acc@1  98.44 ( 98.35)	Acc@5 100.00 (100.00)
Epoch: [78][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.6685e-02 (5.0505e-02)	Acc@1  96.88 ( 98.35)	Acc@5 100.00 (100.00)
Epoch: [78][220/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.5350e-02 (5.0663e-02)	Acc@1  97.66 ( 98.35)	Acc@5 100.00 (100.00)
Epoch: [78][230/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.6607e-02 (5.0476e-02)	Acc@1  98.44 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [78][240/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.4612e-02 (5.0389e-02)	Acc@1  99.22 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [78][250/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6363e-02 (4.9876e-02)	Acc@1  99.22 ( 98.38)	Acc@5 100.00 (100.00)
Epoch: [78][260/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.4364e-02 (4.9965e-02)	Acc@1  96.88 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [78][270/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.6429e-02 (5.0159e-02)	Acc@1  99.22 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [78][280/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.5518e-02 (5.0361e-02)	Acc@1  96.88 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [78][290/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.2452e-02 (5.0826e-02)	Acc@1  97.66 ( 98.35)	Acc@5 100.00 (100.00)
Epoch: [78][300/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.6568e-02 (5.0657e-02)	Acc@1  98.44 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [78][310/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.1849e-02 (5.1061e-02)	Acc@1  96.88 ( 98.36)	Acc@5 100.00 ( 99.99)
Epoch: [78][320/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.7687e-02 (5.1085e-02)	Acc@1 100.00 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [78][330/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1947e-01 (5.1368e-02)	Acc@1  95.31 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [78][340/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.9338e-02 (5.1560e-02)	Acc@1  96.09 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [78][350/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1935e-02 (5.2214e-02)	Acc@1  98.44 ( 98.33)	Acc@5 100.00 (100.00)
Epoch: [78][360/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.0899e-02 (5.2234e-02)	Acc@1 100.00 ( 98.33)	Acc@5 100.00 (100.00)
Epoch: [78][370/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.5726e-02 (5.2120e-02)	Acc@1  99.22 ( 98.34)	Acc@5 100.00 (100.00)
Epoch: [78][380/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.4317e-02 (5.2433e-02)	Acc@1  97.66 ( 98.32)	Acc@5 100.00 (100.00)
Epoch: [78][390/391]	Time  0.028 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.1686e-02 (5.2405e-02)	Acc@1  97.50 ( 98.32)	Acc@5 100.00 (100.00)
## e[78] optimizer.zero_grad (sum) time: 0.261934757232666
## e[78]       loss.backward (sum) time: 3.96535325050354
## e[78]      optimizer.step (sum) time: 1.8153793811798096
## epoch[78] training(only) time: 15.993119478225708
# Switched to evaluate mode...
Test: [  0/100]	Time  0.178 ( 0.178)	Loss 3.6302e-01 (3.6302e-01)	Acc@1  91.00 ( 91.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.021 ( 0.035)	Loss 4.2229e-01 (3.2381e-01)	Acc@1  90.00 ( 91.09)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 5.3831e-01 (3.2964e-01)	Acc@1  87.00 ( 90.76)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.018 ( 0.026)	Loss 3.7195e-01 (3.4623e-01)	Acc@1  91.00 ( 90.81)	Acc@5  99.00 ( 99.77)
Test: [ 40/100]	Time  0.022 ( 0.025)	Loss 3.9551e-01 (3.5890e-01)	Acc@1  90.00 ( 90.46)	Acc@5  99.00 ( 99.71)
Test: [ 50/100]	Time  0.020 ( 0.024)	Loss 2.6961e-01 (3.4799e-01)	Acc@1  93.00 ( 90.73)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.018 ( 0.024)	Loss 4.2669e-01 (3.5384e-01)	Acc@1  89.00 ( 90.51)	Acc@5  99.00 ( 99.74)
Test: [ 70/100]	Time  0.019 ( 0.023)	Loss 4.1107e-01 (3.4353e-01)	Acc@1  88.00 ( 90.52)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.016 ( 0.023)	Loss 2.1698e-01 (3.3969e-01)	Acc@1  93.00 ( 90.64)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.024 ( 0.023)	Loss 2.5492e-01 (3.4147e-01)	Acc@1  95.00 ( 90.71)	Acc@5 100.00 ( 99.76)
 * Acc@1 90.810 Acc@5 99.760
### epoch[78] execution time: 18.38444757461548
EPOCH 79
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [79][  0/391]	Time  0.223 ( 0.223)	Data  0.178 ( 0.178)	Loss 4.6784e-02 (4.6784e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [79][ 10/391]	Time  0.042 ( 0.056)	Data  0.001 ( 0.017)	Loss 8.7507e-02 (5.5725e-02)	Acc@1  98.44 ( 98.22)	Acc@5 100.00 ( 99.93)
Epoch: [79][ 20/391]	Time  0.042 ( 0.049)	Data  0.001 ( 0.009)	Loss 3.7573e-02 (5.3194e-02)	Acc@1  99.22 ( 98.36)	Acc@5 100.00 ( 99.96)
Epoch: [79][ 30/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.007)	Loss 9.9399e-02 (5.2501e-02)	Acc@1  96.88 ( 98.29)	Acc@5 100.00 ( 99.97)
Epoch: [79][ 40/391]	Time  0.042 ( 0.045)	Data  0.001 ( 0.005)	Loss 7.8340e-02 (5.3475e-02)	Acc@1  96.09 ( 98.23)	Acc@5 100.00 ( 99.98)
Epoch: [79][ 50/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.004)	Loss 4.3683e-02 (5.3833e-02)	Acc@1  98.44 ( 98.21)	Acc@5 100.00 ( 99.98)
Epoch: [79][ 60/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.004)	Loss 6.5169e-02 (5.2459e-02)	Acc@1  96.88 ( 98.28)	Acc@5 100.00 ( 99.99)
Epoch: [79][ 70/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 5.1342e-02 (5.2726e-02)	Acc@1  97.66 ( 98.31)	Acc@5 100.00 ( 99.99)
Epoch: [79][ 80/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.2621e-02 (5.1454e-02)	Acc@1 100.00 ( 98.32)	Acc@5 100.00 ( 99.99)
Epoch: [79][ 90/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 7.7443e-02 (5.1108e-02)	Acc@1  96.88 ( 98.31)	Acc@5 100.00 ( 99.99)
Epoch: [79][100/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.7665e-02 (5.1576e-02)	Acc@1  97.66 ( 98.31)	Acc@5 100.00 ( 99.99)
Epoch: [79][110/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 8.2682e-02 (5.1113e-02)	Acc@1  97.66 ( 98.32)	Acc@5 100.00 ( 99.99)
Epoch: [79][120/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.6379e-02 (5.0679e-02)	Acc@1 100.00 ( 98.35)	Acc@5 100.00 ( 99.99)
Epoch: [79][130/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.4524e-02 (5.0163e-02)	Acc@1  96.88 ( 98.37)	Acc@5 100.00 ( 99.99)
Epoch: [79][140/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.3879e-02 (5.0278e-02)	Acc@1  97.66 ( 98.36)	Acc@5 100.00 ( 99.99)
Epoch: [79][150/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6308e-02 (5.0498e-02)	Acc@1  98.44 ( 98.34)	Acc@5 100.00 ( 99.99)
Epoch: [79][160/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4966e-02 (5.0575e-02)	Acc@1  99.22 ( 98.34)	Acc@5 100.00 (100.00)
Epoch: [79][170/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.4826e-02 (5.0357e-02)	Acc@1 100.00 ( 98.33)	Acc@5 100.00 (100.00)
Epoch: [79][180/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2699e-01 (5.0245e-02)	Acc@1  95.31 ( 98.33)	Acc@5 100.00 (100.00)
Epoch: [79][190/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1181e-02 (5.0818e-02)	Acc@1 100.00 ( 98.29)	Acc@5 100.00 (100.00)
Epoch: [79][200/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.9227e-02 (5.1628e-02)	Acc@1  96.88 ( 98.25)	Acc@5 100.00 (100.00)
Epoch: [79][210/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4577e-02 (5.1736e-02)	Acc@1  99.22 ( 98.26)	Acc@5 100.00 (100.00)
Epoch: [79][220/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3099e-02 (5.2171e-02)	Acc@1  98.44 ( 98.24)	Acc@5 100.00 (100.00)
Epoch: [79][230/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6506e-02 (5.2017e-02)	Acc@1 100.00 ( 98.24)	Acc@5 100.00 (100.00)
Epoch: [79][240/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.1439e-02 (5.1950e-02)	Acc@1  97.66 ( 98.26)	Acc@5 100.00 (100.00)
Epoch: [79][250/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.7741e-02 (5.1852e-02)	Acc@1  96.88 ( 98.27)	Acc@5 100.00 (100.00)
Epoch: [79][260/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8698e-02 (5.2014e-02)	Acc@1  98.44 ( 98.27)	Acc@5 100.00 (100.00)
Epoch: [79][270/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.3973e-02 (5.2187e-02)	Acc@1  97.66 ( 98.27)	Acc@5 100.00 (100.00)
Epoch: [79][280/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.9464e-02 (5.2131e-02)	Acc@1  97.66 ( 98.28)	Acc@5 100.00 (100.00)
Epoch: [79][290/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.9678e-02 (5.1841e-02)	Acc@1  97.66 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [79][300/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.4058e-02 (5.1617e-02)	Acc@1  96.88 ( 98.31)	Acc@5 100.00 (100.00)
Epoch: [79][310/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.4413e-02 (5.1204e-02)	Acc@1  98.44 ( 98.33)	Acc@5 100.00 (100.00)
Epoch: [79][320/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0184e-02 (5.1208e-02)	Acc@1  99.22 ( 98.33)	Acc@5 100.00 (100.00)
Epoch: [79][330/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.7591e-02 (5.1070e-02)	Acc@1  99.22 ( 98.32)	Acc@5 100.00 (100.00)
Epoch: [79][340/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.2730e-02 (5.1203e-02)	Acc@1  99.22 ( 98.31)	Acc@5 100.00 (100.00)
Epoch: [79][350/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.0772e-02 (5.1052e-02)	Acc@1  98.44 ( 98.32)	Acc@5 100.00 (100.00)
Epoch: [79][360/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.6619e-02 (5.1118e-02)	Acc@1  96.88 ( 98.31)	Acc@5 100.00 (100.00)
Epoch: [79][370/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.0980e-02 (5.1292e-02)	Acc@1  98.44 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [79][380/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.7708e-02 (5.1520e-02)	Acc@1  98.44 ( 98.29)	Acc@5 100.00 (100.00)
Epoch: [79][390/391]	Time  0.030 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.0632e-01 (5.1650e-02)	Acc@1  96.25 ( 98.28)	Acc@5 100.00 (100.00)
## e[79] optimizer.zero_grad (sum) time: 0.26263904571533203
## e[79]       loss.backward (sum) time: 3.9839141368865967
## e[79]      optimizer.step (sum) time: 1.8314330577850342
## epoch[79] training(only) time: 15.98580551147461
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 3.6807e-01 (3.6807e-01)	Acc@1  91.00 ( 91.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.020 ( 0.034)	Loss 4.0856e-01 (3.2903e-01)	Acc@1  90.00 ( 90.82)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.020 ( 0.028)	Loss 5.6017e-01 (3.3405e-01)	Acc@1  87.00 ( 90.67)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.018 ( 0.025)	Loss 3.7305e-01 (3.4907e-01)	Acc@1  90.00 ( 90.65)	Acc@5  99.00 ( 99.74)
Test: [ 40/100]	Time  0.021 ( 0.024)	Loss 4.1604e-01 (3.6244e-01)	Acc@1  89.00 ( 90.37)	Acc@5  99.00 ( 99.68)
Test: [ 50/100]	Time  0.023 ( 0.023)	Loss 2.7220e-01 (3.5143e-01)	Acc@1  95.00 ( 90.71)	Acc@5 100.00 ( 99.69)
Test: [ 60/100]	Time  0.024 ( 0.023)	Loss 4.1334e-01 (3.5691e-01)	Acc@1  89.00 ( 90.49)	Acc@5  99.00 ( 99.70)
Test: [ 70/100]	Time  0.016 ( 0.022)	Loss 4.3522e-01 (3.4606e-01)	Acc@1  87.00 ( 90.52)	Acc@5 100.00 ( 99.72)
Test: [ 80/100]	Time  0.021 ( 0.022)	Loss 1.9178e-01 (3.4147e-01)	Acc@1  95.00 ( 90.67)	Acc@5 100.00 ( 99.72)
Test: [ 90/100]	Time  0.022 ( 0.022)	Loss 2.4151e-01 (3.4324e-01)	Acc@1  94.00 ( 90.68)	Acc@5 100.00 ( 99.73)
 * Acc@1 90.800 Acc@5 99.730
### epoch[79] execution time: 18.31978464126587
EPOCH 80
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [80][  0/391]	Time  0.220 ( 0.220)	Data  0.177 ( 0.177)	Loss 5.2847e-02 (5.2847e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [80][ 10/391]	Time  0.038 ( 0.057)	Data  0.001 ( 0.017)	Loss 2.0296e-02 (4.9202e-02)	Acc@1  99.22 ( 98.15)	Acc@5 100.00 (100.00)
Epoch: [80][ 20/391]	Time  0.041 ( 0.050)	Data  0.001 ( 0.009)	Loss 1.9471e-02 (4.7275e-02)	Acc@1 100.00 ( 98.40)	Acc@5 100.00 (100.00)
Epoch: [80][ 30/391]	Time  0.041 ( 0.047)	Data  0.001 ( 0.007)	Loss 5.1767e-02 (5.1606e-02)	Acc@1  98.44 ( 98.14)	Acc@5 100.00 (100.00)
Epoch: [80][ 40/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.005)	Loss 5.1472e-02 (4.9594e-02)	Acc@1  98.44 ( 98.27)	Acc@5 100.00 (100.00)
Epoch: [80][ 50/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.004)	Loss 6.4513e-02 (4.8704e-02)	Acc@1  96.88 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [80][ 60/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.004)	Loss 5.0696e-02 (4.8416e-02)	Acc@1  98.44 ( 98.35)	Acc@5 100.00 (100.00)
Epoch: [80][ 70/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 5.6873e-02 (4.8622e-02)	Acc@1  98.44 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [80][ 80/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 6.6061e-02 (4.9905e-02)	Acc@1  96.09 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [80][ 90/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.5865e-02 (4.9594e-02)	Acc@1  98.44 ( 98.33)	Acc@5 100.00 (100.00)
Epoch: [80][100/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 9.7651e-02 (5.0574e-02)	Acc@1  96.09 ( 98.28)	Acc@5 100.00 (100.00)
Epoch: [80][110/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.1091e-02 (4.9738e-02)	Acc@1  98.44 ( 98.31)	Acc@5 100.00 (100.00)
Epoch: [80][120/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.002)	Loss 7.4780e-02 (4.9951e-02)	Acc@1  96.88 ( 98.28)	Acc@5 100.00 (100.00)
Epoch: [80][130/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.8278e-02 (5.0701e-02)	Acc@1  98.44 ( 98.27)	Acc@5 100.00 (100.00)
Epoch: [80][140/391]	Time  0.055 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.1118e-02 (5.0153e-02)	Acc@1  99.22 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [80][150/391]	Time  0.034 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0227e-02 (4.9854e-02)	Acc@1  99.22 ( 98.33)	Acc@5 100.00 (100.00)
Epoch: [80][160/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3475e-02 (4.9736e-02)	Acc@1  99.22 ( 98.35)	Acc@5 100.00 (100.00)
Epoch: [80][170/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9806e-02 (5.0371e-02)	Acc@1  98.44 ( 98.32)	Acc@5 100.00 (100.00)
Epoch: [80][180/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.5304e-02 (5.0888e-02)	Acc@1  96.88 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [80][190/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.0996e-02 (5.0858e-02)	Acc@1  96.88 ( 98.31)	Acc@5 100.00 (100.00)
Epoch: [80][200/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3455e-02 (5.0472e-02)	Acc@1  98.44 ( 98.33)	Acc@5 100.00 (100.00)
Epoch: [80][210/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7223e-02 (5.0171e-02)	Acc@1  99.22 ( 98.34)	Acc@5 100.00 (100.00)
Epoch: [80][220/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1724e-02 (5.0366e-02)	Acc@1  99.22 ( 98.33)	Acc@5 100.00 (100.00)
Epoch: [80][230/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8225e-02 (4.9845e-02)	Acc@1  98.44 ( 98.35)	Acc@5 100.00 (100.00)
Epoch: [80][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.9071e-02 (4.9609e-02)	Acc@1  98.44 ( 98.35)	Acc@5 100.00 (100.00)
Epoch: [80][250/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3738e-01 (4.9951e-02)	Acc@1  96.09 ( 98.34)	Acc@5  99.22 (100.00)
Epoch: [80][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4947e-02 (5.0353e-02)	Acc@1 100.00 ( 98.33)	Acc@5 100.00 (100.00)
Epoch: [80][270/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2044e-02 (5.0036e-02)	Acc@1  98.44 ( 98.34)	Acc@5 100.00 (100.00)
Epoch: [80][280/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.7436e-02 (5.0191e-02)	Acc@1  97.66 ( 98.33)	Acc@5 100.00 (100.00)
Epoch: [80][290/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3784e-02 (5.0529e-02)	Acc@1  97.66 ( 98.31)	Acc@5 100.00 (100.00)
Epoch: [80][300/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.2103e-02 (5.0585e-02)	Acc@1  98.44 ( 98.31)	Acc@5 100.00 (100.00)
Epoch: [80][310/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8187e-02 (5.0804e-02)	Acc@1  96.88 ( 98.28)	Acc@5 100.00 (100.00)
Epoch: [80][320/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8664e-02 (5.0707e-02)	Acc@1  99.22 ( 98.29)	Acc@5 100.00 (100.00)
Epoch: [80][330/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7482e-02 (5.0820e-02)	Acc@1  98.44 ( 98.27)	Acc@5 100.00 (100.00)
Epoch: [80][340/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3319e-02 (5.0723e-02)	Acc@1  99.22 ( 98.29)	Acc@5 100.00 (100.00)
Epoch: [80][350/391]	Time  0.035 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9531e-02 (5.0991e-02)	Acc@1  98.44 ( 98.28)	Acc@5 100.00 (100.00)
Epoch: [80][360/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.0028e-02 (5.1120e-02)	Acc@1  97.66 ( 98.26)	Acc@5 100.00 (100.00)
Epoch: [80][370/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4977e-02 (5.1044e-02)	Acc@1  99.22 ( 98.26)	Acc@5 100.00 (100.00)
Epoch: [80][380/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.1338e-02 (5.0784e-02)	Acc@1  97.66 ( 98.28)	Acc@5 100.00 (100.00)
Epoch: [80][390/391]	Time  0.027 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.1938e-01 (5.0563e-02)	Acc@1  95.00 ( 98.29)	Acc@5 100.00 (100.00)
## e[80] optimizer.zero_grad (sum) time: 0.26291465759277344
## e[80]       loss.backward (sum) time: 3.9456872940063477
## e[80]      optimizer.step (sum) time: 1.8605024814605713
## epoch[80] training(only) time: 15.979631423950195
# Switched to evaluate mode...
Test: [  0/100]	Time  0.174 ( 0.174)	Loss 3.7242e-01 (3.7242e-01)	Acc@1  91.00 ( 91.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 4.2238e-01 (3.2947e-01)	Acc@1  90.00 ( 91.00)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.020 ( 0.028)	Loss 5.6261e-01 (3.3450e-01)	Acc@1  87.00 ( 90.86)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.018 ( 0.025)	Loss 3.8195e-01 (3.4951e-01)	Acc@1  88.00 ( 90.61)	Acc@5  99.00 ( 99.81)
Test: [ 40/100]	Time  0.021 ( 0.025)	Loss 4.0227e-01 (3.6237e-01)	Acc@1  89.00 ( 90.27)	Acc@5 100.00 ( 99.76)
Test: [ 50/100]	Time  0.018 ( 0.024)	Loss 2.8452e-01 (3.5164e-01)	Acc@1  95.00 ( 90.59)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.019 ( 0.023)	Loss 4.1788e-01 (3.5795e-01)	Acc@1  88.00 ( 90.36)	Acc@5  99.00 ( 99.75)
Test: [ 70/100]	Time  0.019 ( 0.023)	Loss 4.1838e-01 (3.4663e-01)	Acc@1  88.00 ( 90.39)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.016 ( 0.022)	Loss 1.9006e-01 (3.4216e-01)	Acc@1  94.00 ( 90.53)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.023 ( 0.022)	Loss 2.3593e-01 (3.4363e-01)	Acc@1  95.00 ( 90.62)	Acc@5 100.00 ( 99.77)
 * Acc@1 90.730 Acc@5 99.770
### epoch[80] execution time: 18.29252028465271
EPOCH 81
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [81][  0/391]	Time  0.211 ( 0.211)	Data  0.168 ( 0.168)	Loss 2.8382e-02 (2.8382e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [81][ 10/391]	Time  0.041 ( 0.056)	Data  0.001 ( 0.016)	Loss 4.1438e-02 (5.0851e-02)	Acc@1  99.22 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [81][ 20/391]	Time  0.041 ( 0.049)	Data  0.001 ( 0.009)	Loss 6.4714e-02 (4.7624e-02)	Acc@1  98.44 ( 98.62)	Acc@5 100.00 (100.00)
Epoch: [81][ 30/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.006)	Loss 4.3552e-02 (4.9491e-02)	Acc@1  98.44 ( 98.46)	Acc@5 100.00 (100.00)
Epoch: [81][ 40/391]	Time  0.039 ( 0.045)	Data  0.001 ( 0.005)	Loss 3.4252e-02 (4.9111e-02)	Acc@1  99.22 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [81][ 50/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.004)	Loss 3.7100e-02 (4.9483e-02)	Acc@1  99.22 ( 98.42)	Acc@5 100.00 (100.00)
Epoch: [81][ 60/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.0034e-01 (5.3333e-02)	Acc@1  95.31 ( 98.32)	Acc@5 100.00 (100.00)
Epoch: [81][ 70/391]	Time  0.037 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.2563e-02 (5.2317e-02)	Acc@1  97.66 ( 98.31)	Acc@5 100.00 (100.00)
Epoch: [81][ 80/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.6149e-02 (5.2238e-02)	Acc@1  98.44 ( 98.30)	Acc@5 100.00 ( 99.99)
Epoch: [81][ 90/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.2049e-01 (5.2651e-02)	Acc@1  96.09 ( 98.32)	Acc@5 100.00 ( 99.99)
Epoch: [81][100/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.3517e-02 (5.2628e-02)	Acc@1  96.88 ( 98.31)	Acc@5 100.00 ( 99.99)
Epoch: [81][110/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.8023e-02 (5.3749e-02)	Acc@1  99.22 ( 98.28)	Acc@5 100.00 ( 99.99)
Epoch: [81][120/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.0076e-02 (5.3958e-02)	Acc@1  99.22 ( 98.28)	Acc@5 100.00 ( 99.99)
Epoch: [81][130/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.9317e-02 (5.4013e-02)	Acc@1  99.22 ( 98.29)	Acc@5 100.00 ( 99.99)
Epoch: [81][140/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.5504e-02 (5.3502e-02)	Acc@1  98.44 ( 98.29)	Acc@5 100.00 ( 99.99)
Epoch: [81][150/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.9997e-02 (5.3392e-02)	Acc@1  96.88 ( 98.31)	Acc@5 100.00 ( 99.99)
Epoch: [81][160/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0752e-02 (5.3258e-02)	Acc@1  98.44 ( 98.29)	Acc@5 100.00 (100.00)
Epoch: [81][170/391]	Time  0.036 ( 0.041)	Data  0.002 ( 0.002)	Loss 6.1961e-02 (5.3013e-02)	Acc@1  98.44 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [81][180/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.5552e-02 (5.3096e-02)	Acc@1  98.44 ( 98.28)	Acc@5 100.00 (100.00)
Epoch: [81][190/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6831e-02 (5.2590e-02)	Acc@1 100.00 ( 98.28)	Acc@5 100.00 (100.00)
Epoch: [81][200/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7560e-02 (5.2775e-02)	Acc@1  99.22 ( 98.25)	Acc@5 100.00 (100.00)
Epoch: [81][210/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8478e-02 (5.2717e-02)	Acc@1  98.44 ( 98.24)	Acc@5 100.00 (100.00)
Epoch: [81][220/391]	Time  0.045 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.8736e-02 (5.2298e-02)	Acc@1  97.66 ( 98.26)	Acc@5 100.00 (100.00)
Epoch: [81][230/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.4980e-02 (5.2012e-02)	Acc@1  99.22 ( 98.28)	Acc@5 100.00 (100.00)
Epoch: [81][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.9547e-02 (5.1899e-02)	Acc@1  96.88 ( 98.28)	Acc@5 100.00 (100.00)
Epoch: [81][250/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.7891e-02 (5.2025e-02)	Acc@1  99.22 ( 98.27)	Acc@5 100.00 (100.00)
Epoch: [81][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.8884e-02 (5.2693e-02)	Acc@1  95.31 ( 98.23)	Acc@5 100.00 (100.00)
Epoch: [81][270/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2833e-02 (5.2942e-02)	Acc@1  97.66 ( 98.23)	Acc@5 100.00 (100.00)
Epoch: [81][280/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1710e-02 (5.2728e-02)	Acc@1  99.22 ( 98.25)	Acc@5 100.00 (100.00)
Epoch: [81][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.9253e-02 (5.2344e-02)	Acc@1  99.22 ( 98.26)	Acc@5 100.00 (100.00)
Epoch: [81][300/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0460e-02 (5.2322e-02)	Acc@1  98.44 ( 98.26)	Acc@5 100.00 (100.00)
Epoch: [81][310/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.1292e-02 (5.2324e-02)	Acc@1  96.88 ( 98.26)	Acc@5 100.00 (100.00)
Epoch: [81][320/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.1191e-02 (5.2730e-02)	Acc@1  94.53 ( 98.23)	Acc@5 100.00 (100.00)
Epoch: [81][330/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9861e-02 (5.2440e-02)	Acc@1  98.44 ( 98.24)	Acc@5 100.00 (100.00)
Epoch: [81][340/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1026e-02 (5.2237e-02)	Acc@1  98.44 ( 98.24)	Acc@5 100.00 (100.00)
Epoch: [81][350/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0861e-01 (5.2150e-02)	Acc@1  96.88 ( 98.25)	Acc@5 100.00 (100.00)
Epoch: [81][360/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2912e-02 (5.2372e-02)	Acc@1  99.22 ( 98.24)	Acc@5 100.00 (100.00)
Epoch: [81][370/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 7.6921e-02 (5.2312e-02)	Acc@1  96.88 ( 98.24)	Acc@5 100.00 (100.00)
Epoch: [81][380/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 7.4873e-02 (5.2564e-02)	Acc@1  97.66 ( 98.23)	Acc@5 100.00 (100.00)
Epoch: [81][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.0140e-01 (5.2685e-02)	Acc@1  95.00 ( 98.23)	Acc@5 100.00 (100.00)
## e[81] optimizer.zero_grad (sum) time: 0.26322102546691895
## e[81]       loss.backward (sum) time: 3.891538381576538
## e[81]      optimizer.step (sum) time: 1.8707704544067383
## epoch[81] training(only) time: 15.903303384780884
# Switched to evaluate mode...
Test: [  0/100]	Time  0.167 ( 0.167)	Loss 3.6401e-01 (3.6401e-01)	Acc@1  90.00 ( 90.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.018 ( 0.033)	Loss 4.0022e-01 (3.2616e-01)	Acc@1  91.00 ( 90.82)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.021 ( 0.027)	Loss 5.6947e-01 (3.3418e-01)	Acc@1  87.00 ( 90.81)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.021 ( 0.025)	Loss 3.7294e-01 (3.5104e-01)	Acc@1  90.00 ( 90.71)	Acc@5  99.00 ( 99.71)
Test: [ 40/100]	Time  0.017 ( 0.024)	Loss 3.8633e-01 (3.6267e-01)	Acc@1  90.00 ( 90.37)	Acc@5  99.00 ( 99.66)
Test: [ 50/100]	Time  0.024 ( 0.023)	Loss 2.7554e-01 (3.5141e-01)	Acc@1  93.00 ( 90.71)	Acc@5 100.00 ( 99.65)
Test: [ 60/100]	Time  0.017 ( 0.023)	Loss 4.2394e-01 (3.5797e-01)	Acc@1  89.00 ( 90.56)	Acc@5  99.00 ( 99.67)
Test: [ 70/100]	Time  0.017 ( 0.022)	Loss 4.0479e-01 (3.4651e-01)	Acc@1  88.00 ( 90.68)	Acc@5 100.00 ( 99.68)
Test: [ 80/100]	Time  0.020 ( 0.022)	Loss 2.0195e-01 (3.4276e-01)	Acc@1  94.00 ( 90.79)	Acc@5 100.00 ( 99.68)
Test: [ 90/100]	Time  0.017 ( 0.022)	Loss 2.6039e-01 (3.4501e-01)	Acc@1  94.00 ( 90.80)	Acc@5 100.00 ( 99.69)
 * Acc@1 90.900 Acc@5 99.700
### epoch[81] execution time: 18.201537370681763
EPOCH 82
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [82][  0/391]	Time  0.218 ( 0.218)	Data  0.171 ( 0.171)	Loss 6.0889e-02 (6.0889e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [82][ 10/391]	Time  0.039 ( 0.057)	Data  0.001 ( 0.016)	Loss 5.7428e-02 (5.3934e-02)	Acc@1  97.66 ( 98.08)	Acc@5 100.00 (100.00)
Epoch: [82][ 20/391]	Time  0.035 ( 0.049)	Data  0.001 ( 0.009)	Loss 4.3941e-02 (5.1663e-02)	Acc@1  98.44 ( 98.21)	Acc@5 100.00 (100.00)
Epoch: [82][ 30/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.007)	Loss 5.0936e-02 (5.1610e-02)	Acc@1  98.44 ( 98.24)	Acc@5 100.00 (100.00)
Epoch: [82][ 40/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.005)	Loss 3.5735e-02 (5.2008e-02)	Acc@1  99.22 ( 98.25)	Acc@5 100.00 (100.00)
Epoch: [82][ 50/391]	Time  0.037 ( 0.043)	Data  0.001 ( 0.004)	Loss 4.3017e-02 (5.1096e-02)	Acc@1  99.22 ( 98.28)	Acc@5 100.00 (100.00)
Epoch: [82][ 60/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.004)	Loss 4.1225e-02 (5.0272e-02)	Acc@1  98.44 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [82][ 70/391]	Time  0.035 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.0217e-01 (5.1170e-02)	Acc@1  96.88 ( 98.31)	Acc@5 100.00 (100.00)
Epoch: [82][ 80/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.2128e-02 (5.1512e-02)	Acc@1  99.22 ( 98.29)	Acc@5 100.00 (100.00)
Epoch: [82][ 90/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.003)	Loss 5.6744e-02 (5.1649e-02)	Acc@1  99.22 ( 98.29)	Acc@5 100.00 (100.00)
Epoch: [82][100/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.003)	Loss 4.4262e-02 (5.1070e-02)	Acc@1  99.22 ( 98.31)	Acc@5 100.00 (100.00)
Epoch: [82][110/391]	Time  0.050 ( 0.041)	Data  0.001 ( 0.003)	Loss 5.7299e-02 (5.0641e-02)	Acc@1  98.44 ( 98.32)	Acc@5 100.00 (100.00)
Epoch: [82][120/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.1971e-02 (5.1195e-02)	Acc@1 100.00 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [82][130/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5012e-01 (5.2234e-02)	Acc@1  96.88 ( 98.28)	Acc@5 100.00 (100.00)
Epoch: [82][140/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.7660e-02 (5.1924e-02)	Acc@1  99.22 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [82][150/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.0227e-02 (5.1435e-02)	Acc@1  99.22 ( 98.31)	Acc@5 100.00 (100.00)
Epoch: [82][160/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.8118e-02 (5.1547e-02)	Acc@1  98.44 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [82][170/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1216e-01 (5.1607e-02)	Acc@1  94.53 ( 98.28)	Acc@5 100.00 (100.00)
Epoch: [82][180/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.6666e-02 (5.1667e-02)	Acc@1  96.88 ( 98.26)	Acc@5 100.00 (100.00)
Epoch: [82][190/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9724e-02 (5.1366e-02)	Acc@1  98.44 ( 98.27)	Acc@5 100.00 (100.00)
Epoch: [82][200/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7166e-02 (5.1062e-02)	Acc@1  97.66 ( 98.28)	Acc@5 100.00 (100.00)
Epoch: [82][210/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.2783e-02 (5.1135e-02)	Acc@1  96.09 ( 98.26)	Acc@5 100.00 (100.00)
Epoch: [82][220/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.9430e-02 (5.1136e-02)	Acc@1  98.44 ( 98.25)	Acc@5 100.00 (100.00)
Epoch: [82][230/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.3358e-02 (5.1085e-02)	Acc@1  98.44 ( 98.25)	Acc@5 100.00 (100.00)
Epoch: [82][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.8245e-02 (5.1452e-02)	Acc@1  99.22 ( 98.25)	Acc@5 100.00 (100.00)
Epoch: [82][250/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7799e-02 (5.1364e-02)	Acc@1  99.22 ( 98.26)	Acc@5 100.00 (100.00)
Epoch: [82][260/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1261e-02 (5.1564e-02)	Acc@1  99.22 ( 98.25)	Acc@5 100.00 (100.00)
Epoch: [82][270/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.1027e-02 (5.1562e-02)	Acc@1  98.44 ( 98.26)	Acc@5 100.00 (100.00)
Epoch: [82][280/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.6505e-02 (5.1290e-02)	Acc@1  98.44 ( 98.27)	Acc@5 100.00 (100.00)
Epoch: [82][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.7310e-02 (5.1491e-02)	Acc@1  96.09 ( 98.26)	Acc@5 100.00 ( 99.99)
Epoch: [82][300/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3133e-02 (5.1370e-02)	Acc@1  98.44 ( 98.26)	Acc@5 100.00 ( 99.99)
Epoch: [82][310/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.9618e-02 (5.1498e-02)	Acc@1  97.66 ( 98.25)	Acc@5 100.00 ( 99.99)
Epoch: [82][320/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.6588e-02 (5.1868e-02)	Acc@1  98.44 ( 98.24)	Acc@5 100.00 ( 99.99)
Epoch: [82][330/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.4006e-02 (5.1883e-02)	Acc@1  99.22 ( 98.25)	Acc@5 100.00 ( 99.99)
Epoch: [82][340/391]	Time  0.051 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.8534e-02 (5.1905e-02)	Acc@1 100.00 ( 98.25)	Acc@5 100.00 ( 99.99)
Epoch: [82][350/391]	Time  0.058 ( 0.041)	Data  0.002 ( 0.001)	Loss 2.5443e-02 (5.1867e-02)	Acc@1  99.22 ( 98.24)	Acc@5 100.00 ( 99.99)
Epoch: [82][360/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.3790e-02 (5.1533e-02)	Acc@1 100.00 ( 98.26)	Acc@5 100.00 ( 99.99)
Epoch: [82][370/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 8.1028e-02 (5.1718e-02)	Acc@1  96.88 ( 98.25)	Acc@5 100.00 ( 99.99)
Epoch: [82][380/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.8715e-02 (5.2021e-02)	Acc@1  99.22 ( 98.24)	Acc@5 100.00 ( 99.99)
Epoch: [82][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.4739e-02 (5.1880e-02)	Acc@1  98.75 ( 98.25)	Acc@5 100.00 ( 99.99)
## e[82] optimizer.zero_grad (sum) time: 0.26056456565856934
## e[82]       loss.backward (sum) time: 3.893360137939453
## e[82]      optimizer.step (sum) time: 1.8446846008300781
## epoch[82] training(only) time: 15.967262744903564
# Switched to evaluate mode...
Test: [  0/100]	Time  0.211 ( 0.211)	Loss 3.5850e-01 (3.5850e-01)	Acc@1  90.00 ( 90.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.022 ( 0.038)	Loss 4.0645e-01 (3.2860e-01)	Acc@1  91.00 ( 90.64)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.021 ( 0.030)	Loss 6.1847e-01 (3.3609e-01)	Acc@1  87.00 ( 90.81)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.018 ( 0.027)	Loss 3.7317e-01 (3.4775e-01)	Acc@1  89.00 ( 90.90)	Acc@5  99.00 ( 99.74)
Test: [ 40/100]	Time  0.025 ( 0.025)	Loss 4.1010e-01 (3.6155e-01)	Acc@1  90.00 ( 90.49)	Acc@5  99.00 ( 99.68)
Test: [ 50/100]	Time  0.025 ( 0.024)	Loss 2.6907e-01 (3.4955e-01)	Acc@1  93.00 ( 90.80)	Acc@5 100.00 ( 99.69)
Test: [ 60/100]	Time  0.022 ( 0.024)	Loss 4.2577e-01 (3.5567e-01)	Acc@1  89.00 ( 90.57)	Acc@5  99.00 ( 99.70)
Test: [ 70/100]	Time  0.025 ( 0.024)	Loss 4.1165e-01 (3.4400e-01)	Acc@1  88.00 ( 90.69)	Acc@5 100.00 ( 99.72)
Test: [ 80/100]	Time  0.017 ( 0.023)	Loss 1.9903e-01 (3.3941e-01)	Acc@1  94.00 ( 90.83)	Acc@5 100.00 ( 99.73)
Test: [ 90/100]	Time  0.018 ( 0.023)	Loss 2.5091e-01 (3.4161e-01)	Acc@1  94.00 ( 90.85)	Acc@5 100.00 ( 99.73)
 * Acc@1 90.970 Acc@5 99.730
### epoch[82] execution time: 18.342888832092285
EPOCH 83
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [83][  0/391]	Time  0.207 ( 0.207)	Data  0.166 ( 0.166)	Loss 5.0433e-02 (5.0433e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [83][ 10/391]	Time  0.041 ( 0.055)	Data  0.001 ( 0.016)	Loss 4.0718e-02 (5.0529e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [83][ 20/391]	Time  0.042 ( 0.048)	Data  0.001 ( 0.009)	Loss 3.7879e-02 (5.2148e-02)	Acc@1  98.44 ( 98.21)	Acc@5 100.00 (100.00)
Epoch: [83][ 30/391]	Time  0.042 ( 0.045)	Data  0.001 ( 0.006)	Loss 3.5016e-02 (5.1917e-02)	Acc@1  97.66 ( 98.24)	Acc@5 100.00 (100.00)
Epoch: [83][ 40/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.005)	Loss 4.1893e-02 (5.0368e-02)	Acc@1  99.22 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [83][ 50/391]	Time  0.043 ( 0.043)	Data  0.001 ( 0.004)	Loss 5.3441e-02 (4.8373e-02)	Acc@1  98.44 ( 98.48)	Acc@5 100.00 (100.00)
Epoch: [83][ 60/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.004)	Loss 6.3116e-02 (4.9639e-02)	Acc@1  97.66 ( 98.45)	Acc@5 100.00 (100.00)
Epoch: [83][ 70/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.003)	Loss 6.7573e-02 (5.1149e-02)	Acc@1  97.66 ( 98.45)	Acc@5 100.00 (100.00)
Epoch: [83][ 80/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.4802e-02 (5.2043e-02)	Acc@1  97.66 ( 98.40)	Acc@5 100.00 (100.00)
Epoch: [83][ 90/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.2034e-02 (5.3476e-02)	Acc@1  99.22 ( 98.34)	Acc@5 100.00 ( 99.99)
Epoch: [83][100/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.2974e-02 (5.2496e-02)	Acc@1 100.00 ( 98.39)	Acc@5 100.00 ( 99.99)
Epoch: [83][110/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.6072e-02 (5.2637e-02)	Acc@1  97.66 ( 98.34)	Acc@5 100.00 ( 99.99)
Epoch: [83][120/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.2563e-02 (5.3106e-02)	Acc@1  97.66 ( 98.31)	Acc@5 100.00 ( 99.99)
Epoch: [83][130/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.3165e-02 (5.2064e-02)	Acc@1  98.44 ( 98.34)	Acc@5 100.00 ( 99.99)
Epoch: [83][140/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9159e-02 (5.1585e-02)	Acc@1  99.22 ( 98.37)	Acc@5 100.00 ( 99.99)
Epoch: [83][150/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.4083e-02 (5.1812e-02)	Acc@1  98.44 ( 98.35)	Acc@5 100.00 ( 99.99)
Epoch: [83][160/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4797e-02 (5.2200e-02)	Acc@1  99.22 ( 98.32)	Acc@5 100.00 (100.00)
Epoch: [83][170/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6789e-02 (5.2082e-02)	Acc@1  98.44 ( 98.32)	Acc@5 100.00 (100.00)
Epoch: [83][180/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5875e-02 (5.2340e-02)	Acc@1  98.44 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [83][190/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.5114e-02 (5.2133e-02)	Acc@1  97.66 ( 98.31)	Acc@5 100.00 (100.00)
Epoch: [83][200/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1517e-02 (5.2224e-02)	Acc@1  98.44 ( 98.29)	Acc@5 100.00 (100.00)
Epoch: [83][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.8733e-02 (5.2172e-02)	Acc@1  96.09 ( 98.30)	Acc@5 100.00 ( 99.99)
Epoch: [83][220/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.8660e-02 (5.2359e-02)	Acc@1  96.88 ( 98.28)	Acc@5 100.00 ( 99.99)
Epoch: [83][230/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.2683e-02 (5.2447e-02)	Acc@1  99.22 ( 98.28)	Acc@5 100.00 ( 99.99)
Epoch: [83][240/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.6177e-02 (5.2759e-02)	Acc@1  97.66 ( 98.27)	Acc@5 100.00 ( 99.99)
Epoch: [83][250/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.5283e-02 (5.2718e-02)	Acc@1  96.09 ( 98.26)	Acc@5 100.00 ( 99.99)
Epoch: [83][260/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.7873e-02 (5.2433e-02)	Acc@1  98.44 ( 98.28)	Acc@5 100.00 ( 99.99)
Epoch: [83][270/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.9365e-02 (5.2151e-02)	Acc@1 100.00 ( 98.29)	Acc@5 100.00 ( 99.99)
Epoch: [83][280/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1493e-02 (5.2431e-02)	Acc@1  99.22 ( 98.28)	Acc@5 100.00 ( 99.99)
Epoch: [83][290/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.0684e-02 (5.2475e-02)	Acc@1 100.00 ( 98.26)	Acc@5 100.00 ( 99.99)
Epoch: [83][300/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 8.2594e-02 (5.2389e-02)	Acc@1  96.88 ( 98.26)	Acc@5 100.00 ( 99.99)
Epoch: [83][310/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.3435e-02 (5.2170e-02)	Acc@1  98.44 ( 98.26)	Acc@5 100.00 ( 99.99)
Epoch: [83][320/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.9345e-02 (5.1788e-02)	Acc@1 100.00 ( 98.28)	Acc@5 100.00 (100.00)
Epoch: [83][330/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.0374e-02 (5.1898e-02)	Acc@1  99.22 ( 98.28)	Acc@5 100.00 (100.00)
Epoch: [83][340/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.2941e-02 (5.1888e-02)	Acc@1  97.66 ( 98.28)	Acc@5 100.00 (100.00)
Epoch: [83][350/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.3900e-02 (5.1828e-02)	Acc@1  99.22 ( 98.29)	Acc@5 100.00 (100.00)
Epoch: [83][360/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.5756e-02 (5.1635e-02)	Acc@1 100.00 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [83][370/391]	Time  0.039 ( 0.041)	Data  0.002 ( 0.001)	Loss 7.9263e-02 (5.1849e-02)	Acc@1  96.88 ( 98.28)	Acc@5 100.00 (100.00)
Epoch: [83][380/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.9592e-02 (5.1571e-02)	Acc@1  99.22 ( 98.31)	Acc@5 100.00 (100.00)
Epoch: [83][390/391]	Time  0.027 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.0079e-01 (5.1727e-02)	Acc@1  96.25 ( 98.30)	Acc@5 100.00 (100.00)
## e[83] optimizer.zero_grad (sum) time: 0.26230430603027344
## e[83]       loss.backward (sum) time: 3.970940351486206
## e[83]      optimizer.step (sum) time: 1.8129165172576904
## epoch[83] training(only) time: 15.982985258102417
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 3.5100e-01 (3.5100e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.017 ( 0.034)	Loss 4.0960e-01 (3.2463e-01)	Acc@1  91.00 ( 91.18)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.018 ( 0.028)	Loss 6.1357e-01 (3.3536e-01)	Acc@1  86.00 ( 91.00)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.025 ( 0.025)	Loss 3.8047e-01 (3.5163e-01)	Acc@1  89.00 ( 90.90)	Acc@5  99.00 ( 99.71)
Test: [ 40/100]	Time  0.023 ( 0.024)	Loss 3.9837e-01 (3.6427e-01)	Acc@1  89.00 ( 90.54)	Acc@5  99.00 ( 99.66)
Test: [ 50/100]	Time  0.024 ( 0.024)	Loss 2.8616e-01 (3.5293e-01)	Acc@1  93.00 ( 90.80)	Acc@5 100.00 ( 99.61)
Test: [ 60/100]	Time  0.019 ( 0.023)	Loss 4.1854e-01 (3.6027e-01)	Acc@1  90.00 ( 90.70)	Acc@5  99.00 ( 99.64)
Test: [ 70/100]	Time  0.023 ( 0.023)	Loss 4.2576e-01 (3.4858e-01)	Acc@1  88.00 ( 90.76)	Acc@5 100.00 ( 99.66)
Test: [ 80/100]	Time  0.024 ( 0.023)	Loss 2.0865e-01 (3.4513e-01)	Acc@1  95.00 ( 90.91)	Acc@5 100.00 ( 99.67)
Test: [ 90/100]	Time  0.020 ( 0.023)	Loss 2.5666e-01 (3.4749e-01)	Acc@1  95.00 ( 90.91)	Acc@5 100.00 ( 99.68)
 * Acc@1 90.990 Acc@5 99.690
### epoch[83] execution time: 18.331915855407715
EPOCH 84
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [84][  0/391]	Time  0.209 ( 0.209)	Data  0.165 ( 0.165)	Loss 2.0456e-02 (2.0456e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [84][ 10/391]	Time  0.038 ( 0.055)	Data  0.001 ( 0.016)	Loss 4.3335e-02 (6.3503e-02)	Acc@1  99.22 ( 98.22)	Acc@5 100.00 (100.00)
Epoch: [84][ 20/391]	Time  0.041 ( 0.049)	Data  0.001 ( 0.009)	Loss 3.2935e-02 (5.8127e-02)	Acc@1  99.22 ( 98.33)	Acc@5 100.00 (100.00)
Epoch: [84][ 30/391]	Time  0.040 ( 0.046)	Data  0.001 ( 0.006)	Loss 3.6613e-02 (5.5234e-02)	Acc@1  99.22 ( 98.29)	Acc@5 100.00 (100.00)
Epoch: [84][ 40/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.005)	Loss 2.2966e-02 (5.4439e-02)	Acc@1  99.22 ( 98.25)	Acc@5 100.00 (100.00)
Epoch: [84][ 50/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.004)	Loss 8.6275e-02 (5.5420e-02)	Acc@1  96.09 ( 98.07)	Acc@5 100.00 (100.00)
Epoch: [84][ 60/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.004)	Loss 2.1121e-02 (5.6002e-02)	Acc@1 100.00 ( 98.00)	Acc@5 100.00 (100.00)
Epoch: [84][ 70/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.1710e-02 (5.6944e-02)	Acc@1  98.44 ( 97.99)	Acc@5 100.00 (100.00)
Epoch: [84][ 80/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.0734e-02 (5.6539e-02)	Acc@1  99.22 ( 97.98)	Acc@5 100.00 (100.00)
Epoch: [84][ 90/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.6505e-02 (5.6721e-02)	Acc@1  98.44 ( 98.00)	Acc@5 100.00 (100.00)
Epoch: [84][100/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.2039e-02 (5.6529e-02)	Acc@1  96.09 ( 97.99)	Acc@5 100.00 (100.00)
Epoch: [84][110/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.4928e-02 (5.5526e-02)	Acc@1  99.22 ( 98.04)	Acc@5 100.00 (100.00)
Epoch: [84][120/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.8292e-02 (5.5947e-02)	Acc@1  98.44 ( 98.02)	Acc@5 100.00 (100.00)
Epoch: [84][130/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.8411e-02 (5.4978e-02)	Acc@1 100.00 ( 98.07)	Acc@5 100.00 (100.00)
Epoch: [84][140/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0349e-02 (5.4061e-02)	Acc@1  99.22 ( 98.11)	Acc@5 100.00 (100.00)
Epoch: [84][150/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.1186e-02 (5.4162e-02)	Acc@1  96.09 ( 98.08)	Acc@5 100.00 (100.00)
Epoch: [84][160/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5092e-02 (5.4697e-02)	Acc@1 100.00 ( 98.09)	Acc@5 100.00 (100.00)
Epoch: [84][170/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.8939e-02 (5.4346e-02)	Acc@1  98.44 ( 98.11)	Acc@5 100.00 (100.00)
Epoch: [84][180/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.7957e-02 (5.3896e-02)	Acc@1  98.44 ( 98.13)	Acc@5 100.00 (100.00)
Epoch: [84][190/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9424e-02 (5.3487e-02)	Acc@1  98.44 ( 98.16)	Acc@5 100.00 (100.00)
Epoch: [84][200/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.4943e-02 (5.3261e-02)	Acc@1  98.44 ( 98.17)	Acc@5 100.00 (100.00)
Epoch: [84][210/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2082e-02 (5.3511e-02)	Acc@1  97.66 ( 98.14)	Acc@5 100.00 (100.00)
Epoch: [84][220/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.8660e-02 (5.3370e-02)	Acc@1  96.09 ( 98.14)	Acc@5 100.00 (100.00)
Epoch: [84][230/391]	Time  0.041 ( 0.041)	Data  0.002 ( 0.002)	Loss 6.3012e-02 (5.3229e-02)	Acc@1  98.44 ( 98.14)	Acc@5 100.00 (100.00)
Epoch: [84][240/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4742e-02 (5.3205e-02)	Acc@1  99.22 ( 98.14)	Acc@5 100.00 (100.00)
Epoch: [84][250/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.9264e-02 (5.3286e-02)	Acc@1  97.66 ( 98.14)	Acc@5 100.00 (100.00)
Epoch: [84][260/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8652e-02 (5.3258e-02)	Acc@1  98.44 ( 98.15)	Acc@5 100.00 (100.00)
Epoch: [84][270/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.6260e-02 (5.3064e-02)	Acc@1  99.22 ( 98.15)	Acc@5 100.00 (100.00)
Epoch: [84][280/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.6502e-02 (5.3031e-02)	Acc@1  99.22 ( 98.15)	Acc@5 100.00 (100.00)
Epoch: [84][290/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.0603e-02 (5.3065e-02)	Acc@1  98.44 ( 98.14)	Acc@5 100.00 (100.00)
Epoch: [84][300/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4702e-02 (5.2892e-02)	Acc@1  98.44 ( 98.16)	Acc@5 100.00 (100.00)
Epoch: [84][310/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2797e-02 (5.2737e-02)	Acc@1  98.44 ( 98.17)	Acc@5 100.00 ( 99.99)
Epoch: [84][320/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.2456e-02 (5.2620e-02)	Acc@1  99.22 ( 98.17)	Acc@5 100.00 (100.00)
Epoch: [84][330/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.7588e-02 (5.2278e-02)	Acc@1  99.22 ( 98.19)	Acc@5 100.00 (100.00)
Epoch: [84][340/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.2726e-02 (5.2023e-02)	Acc@1 100.00 ( 98.20)	Acc@5 100.00 (100.00)
Epoch: [84][350/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.7893e-02 (5.1947e-02)	Acc@1  99.22 ( 98.20)	Acc@5 100.00 (100.00)
Epoch: [84][360/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.5649e-02 (5.1851e-02)	Acc@1  97.66 ( 98.21)	Acc@5 100.00 (100.00)
Epoch: [84][370/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.9352e-02 (5.2062e-02)	Acc@1  99.22 ( 98.20)	Acc@5 100.00 ( 99.99)
Epoch: [84][380/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.7886e-02 (5.1746e-02)	Acc@1  97.66 ( 98.22)	Acc@5 100.00 ( 99.99)
Epoch: [84][390/391]	Time  0.033 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.3352e-02 (5.1902e-02)	Acc@1 100.00 ( 98.22)	Acc@5 100.00 ( 99.99)
## e[84] optimizer.zero_grad (sum) time: 0.262373685836792
## e[84]       loss.backward (sum) time: 3.9875617027282715
## e[84]      optimizer.step (sum) time: 1.788465976715088
## epoch[84] training(only) time: 16.01011824607849
# Switched to evaluate mode...
Test: [  0/100]	Time  0.174 ( 0.174)	Loss 3.6916e-01 (3.6916e-01)	Acc@1  91.00 ( 91.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.023 ( 0.035)	Loss 4.4027e-01 (3.2938e-01)	Acc@1  90.00 ( 90.73)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.021 ( 0.029)	Loss 5.8571e-01 (3.3457e-01)	Acc@1  86.00 ( 90.76)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.018 ( 0.025)	Loss 4.0409e-01 (3.5046e-01)	Acc@1  88.00 ( 90.81)	Acc@5  99.00 ( 99.74)
Test: [ 40/100]	Time  0.017 ( 0.024)	Loss 4.0960e-01 (3.6434e-01)	Acc@1  89.00 ( 90.41)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.022 ( 0.023)	Loss 2.9991e-01 (3.5266e-01)	Acc@1  94.00 ( 90.75)	Acc@5 100.00 ( 99.69)
Test: [ 60/100]	Time  0.022 ( 0.023)	Loss 4.0011e-01 (3.5980e-01)	Acc@1  88.00 ( 90.57)	Acc@5  99.00 ( 99.69)
Test: [ 70/100]	Time  0.020 ( 0.022)	Loss 4.2481e-01 (3.4877e-01)	Acc@1  88.00 ( 90.73)	Acc@5 100.00 ( 99.72)
Test: [ 80/100]	Time  0.021 ( 0.022)	Loss 2.1820e-01 (3.4500e-01)	Acc@1  95.00 ( 90.91)	Acc@5 100.00 ( 99.72)
Test: [ 90/100]	Time  0.024 ( 0.022)	Loss 2.5377e-01 (3.4700e-01)	Acc@1  94.00 ( 90.87)	Acc@5 100.00 ( 99.73)
 * Acc@1 90.920 Acc@5 99.730
### epoch[84] execution time: 18.305005311965942
EPOCH 85
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [85][  0/391]	Time  0.216 ( 0.216)	Data  0.174 ( 0.174)	Loss 2.4806e-02 (2.4806e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [85][ 10/391]	Time  0.041 ( 0.057)	Data  0.001 ( 0.017)	Loss 5.1981e-02 (5.1831e-02)	Acc@1  97.66 ( 97.94)	Acc@5 100.00 (100.00)
Epoch: [85][ 20/391]	Time  0.042 ( 0.049)	Data  0.001 ( 0.009)	Loss 6.5135e-02 (5.4831e-02)	Acc@1  96.88 ( 97.84)	Acc@5 100.00 (100.00)
Epoch: [85][ 30/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.007)	Loss 5.1275e-02 (5.2302e-02)	Acc@1  98.44 ( 98.14)	Acc@5 100.00 (100.00)
Epoch: [85][ 40/391]	Time  0.042 ( 0.045)	Data  0.001 ( 0.005)	Loss 6.9130e-02 (5.4686e-02)	Acc@1  95.31 ( 98.06)	Acc@5 100.00 (100.00)
Epoch: [85][ 50/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.004)	Loss 4.5456e-02 (5.4080e-02)	Acc@1  97.66 ( 98.09)	Acc@5 100.00 (100.00)
Epoch: [85][ 60/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.004)	Loss 8.3046e-02 (5.3341e-02)	Acc@1  96.88 ( 98.13)	Acc@5 100.00 (100.00)
Epoch: [85][ 70/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.003)	Loss 6.9343e-02 (5.3547e-02)	Acc@1  96.09 ( 98.17)	Acc@5 100.00 (100.00)
Epoch: [85][ 80/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 5.7306e-02 (5.2466e-02)	Acc@1  96.09 ( 98.19)	Acc@5 100.00 (100.00)
Epoch: [85][ 90/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.6245e-02 (5.2386e-02)	Acc@1  99.22 ( 98.21)	Acc@5 100.00 (100.00)
Epoch: [85][100/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.3439e-02 (5.2869e-02)	Acc@1  97.66 ( 98.15)	Acc@5 100.00 (100.00)
Epoch: [85][110/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.8665e-02 (5.2041e-02)	Acc@1 100.00 ( 98.21)	Acc@5 100.00 (100.00)
Epoch: [85][120/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.1661e-02 (5.2069e-02)	Acc@1 100.00 ( 98.21)	Acc@5 100.00 (100.00)
Epoch: [85][130/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.3935e-02 (5.2105e-02)	Acc@1  98.44 ( 98.25)	Acc@5 100.00 (100.00)
Epoch: [85][140/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4610e-02 (5.1657e-02)	Acc@1  97.66 ( 98.25)	Acc@5 100.00 (100.00)
Epoch: [85][150/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6486e-02 (5.1597e-02)	Acc@1 100.00 ( 98.26)	Acc@5 100.00 (100.00)
Epoch: [85][160/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.9010e-02 (5.1725e-02)	Acc@1 100.00 ( 98.25)	Acc@5 100.00 (100.00)
Epoch: [85][170/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2352e-02 (5.1393e-02)	Acc@1  97.66 ( 98.26)	Acc@5 100.00 (100.00)
Epoch: [85][180/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.2554e-02 (5.1656e-02)	Acc@1  97.66 ( 98.26)	Acc@5 100.00 (100.00)
Epoch: [85][190/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.0435e-02 (5.2222e-02)	Acc@1  96.88 ( 98.25)	Acc@5 100.00 (100.00)
Epoch: [85][200/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2413e-02 (5.2424e-02)	Acc@1  99.22 ( 98.26)	Acc@5 100.00 (100.00)
Epoch: [85][210/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.3155e-02 (5.2634e-02)	Acc@1  96.88 ( 98.24)	Acc@5 100.00 (100.00)
Epoch: [85][220/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.9234e-02 (5.2229e-02)	Acc@1  98.44 ( 98.25)	Acc@5 100.00 (100.00)
Epoch: [85][230/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.9675e-02 (5.1771e-02)	Acc@1  99.22 ( 98.27)	Acc@5 100.00 (100.00)
Epoch: [85][240/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.7788e-02 (5.1902e-02)	Acc@1  96.88 ( 98.26)	Acc@5 100.00 (100.00)
Epoch: [85][250/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4780e-02 (5.1513e-02)	Acc@1 100.00 ( 98.27)	Acc@5 100.00 (100.00)
Epoch: [85][260/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3106e-02 (5.1410e-02)	Acc@1  99.22 ( 98.27)	Acc@5 100.00 (100.00)
Epoch: [85][270/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3738e-02 (5.1809e-02)	Acc@1 100.00 ( 98.28)	Acc@5 100.00 (100.00)
Epoch: [85][280/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8664e-02 (5.1771e-02)	Acc@1  98.44 ( 98.28)	Acc@5 100.00 (100.00)
Epoch: [85][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9314e-02 (5.1761e-02)	Acc@1  98.44 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [85][300/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 2.5977e-02 (5.1697e-02)	Acc@1  99.22 ( 98.29)	Acc@5 100.00 (100.00)
Epoch: [85][310/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1934e-01 (5.1790e-02)	Acc@1  96.88 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [85][320/391]	Time  0.045 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.2347e-02 (5.1624e-02)	Acc@1 100.00 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [85][330/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 3.1502e-02 (5.1337e-02)	Acc@1  99.22 ( 98.32)	Acc@5 100.00 (100.00)
Epoch: [85][340/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3274e-02 (5.1282e-02)	Acc@1  98.44 ( 98.33)	Acc@5 100.00 (100.00)
Epoch: [85][350/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.1229e-02 (5.1474e-02)	Acc@1  98.44 ( 98.32)	Acc@5 100.00 (100.00)
Epoch: [85][360/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.4347e-02 (5.1714e-02)	Acc@1  97.66 ( 98.31)	Acc@5 100.00 (100.00)
Epoch: [85][370/391]	Time  0.043 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.1909e-02 (5.1533e-02)	Acc@1  98.44 ( 98.33)	Acc@5 100.00 (100.00)
Epoch: [85][380/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.2198e-02 (5.1495e-02)	Acc@1  99.22 ( 98.33)	Acc@5 100.00 (100.00)
Epoch: [85][390/391]	Time  0.025 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.5819e-02 (5.1483e-02)	Acc@1  97.50 ( 98.33)	Acc@5 100.00 (100.00)
## e[85] optimizer.zero_grad (sum) time: 0.26336193084716797
## e[85]       loss.backward (sum) time: 3.94126558303833
## e[85]      optimizer.step (sum) time: 1.832545280456543
## epoch[85] training(only) time: 15.914861917495728
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 3.3725e-01 (3.3725e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.023 ( 0.036)	Loss 3.8885e-01 (3.2388e-01)	Acc@1  91.00 ( 90.73)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.017 ( 0.029)	Loss 5.9360e-01 (3.2773e-01)	Acc@1  86.00 ( 90.86)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.021 ( 0.026)	Loss 3.6264e-01 (3.4471e-01)	Acc@1  89.00 ( 90.97)	Acc@5  99.00 ( 99.77)
Test: [ 40/100]	Time  0.026 ( 0.025)	Loss 3.8019e-01 (3.6124e-01)	Acc@1  91.00 ( 90.56)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.018 ( 0.024)	Loss 2.9173e-01 (3.5158e-01)	Acc@1  94.00 ( 90.84)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.022 ( 0.023)	Loss 4.0842e-01 (3.5851e-01)	Acc@1  90.00 ( 90.69)	Acc@5  99.00 ( 99.74)
Test: [ 70/100]	Time  0.017 ( 0.023)	Loss 4.0722e-01 (3.4638e-01)	Acc@1  87.00 ( 90.77)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.021 ( 0.022)	Loss 1.9832e-01 (3.4313e-01)	Acc@1  95.00 ( 90.88)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.021 ( 0.022)	Loss 2.4698e-01 (3.4471e-01)	Acc@1  94.00 ( 90.85)	Acc@5 100.00 ( 99.74)
 * Acc@1 90.920 Acc@5 99.740
### epoch[85] execution time: 18.24783754348755
EPOCH 86
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [86][  0/391]	Time  0.208 ( 0.208)	Data  0.167 ( 0.167)	Loss 3.1951e-02 (3.1951e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [86][ 10/391]	Time  0.040 ( 0.056)	Data  0.001 ( 0.016)	Loss 1.8534e-02 (3.9514e-02)	Acc@1  99.22 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [86][ 20/391]	Time  0.041 ( 0.049)	Data  0.001 ( 0.009)	Loss 9.0933e-02 (4.5988e-02)	Acc@1  96.88 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [86][ 30/391]	Time  0.038 ( 0.046)	Data  0.001 ( 0.006)	Loss 3.3596e-02 (4.6711e-02)	Acc@1  99.22 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [86][ 40/391]	Time  0.040 ( 0.045)	Data  0.001 ( 0.005)	Loss 8.0257e-02 (4.4901e-02)	Acc@1  96.09 ( 98.70)	Acc@5 100.00 (100.00)
Epoch: [86][ 50/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 3.5041e-02 (4.4020e-02)	Acc@1  99.22 ( 98.64)	Acc@5 100.00 (100.00)
Epoch: [86][ 60/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.004)	Loss 2.4352e-02 (4.4528e-02)	Acc@1 100.00 ( 98.62)	Acc@5 100.00 (100.00)
Epoch: [86][ 70/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.9314e-02 (4.4703e-02)	Acc@1  98.44 ( 98.61)	Acc@5 100.00 ( 99.99)
Epoch: [86][ 80/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.2173e-01 (4.5544e-02)	Acc@1  95.31 ( 98.55)	Acc@5 100.00 ( 99.99)
Epoch: [86][ 90/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.6627e-02 (4.6462e-02)	Acc@1  99.22 ( 98.50)	Acc@5 100.00 ( 99.99)
Epoch: [86][100/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.3984e-02 (4.5356e-02)	Acc@1 100.00 ( 98.56)	Acc@5 100.00 ( 99.99)
Epoch: [86][110/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.2151e-02 (4.5707e-02)	Acc@1  96.88 ( 98.56)	Acc@5 100.00 ( 99.99)
Epoch: [86][120/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.1621e-02 (4.6771e-02)	Acc@1  98.44 ( 98.51)	Acc@5 100.00 ( 99.99)
Epoch: [86][130/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.6064e-02 (4.6412e-02)	Acc@1  99.22 ( 98.52)	Acc@5 100.00 ( 99.99)
Epoch: [86][140/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 9.2929e-02 (4.7009e-02)	Acc@1  94.53 ( 98.49)	Acc@5 100.00 ( 99.99)
Epoch: [86][150/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.8401e-02 (4.7253e-02)	Acc@1 100.00 ( 98.48)	Acc@5 100.00 ( 99.99)
Epoch: [86][160/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9130e-02 (4.7222e-02)	Acc@1  99.22 ( 98.49)	Acc@5 100.00 (100.00)
Epoch: [86][170/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5119e-02 (4.6761e-02)	Acc@1  99.22 ( 98.51)	Acc@5 100.00 (100.00)
Epoch: [86][180/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.9034e-02 (4.7860e-02)	Acc@1  96.88 ( 98.47)	Acc@5 100.00 ( 99.99)
Epoch: [86][190/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.5789e-02 (4.7529e-02)	Acc@1 100.00 ( 98.49)	Acc@5 100.00 ( 99.99)
Epoch: [86][200/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5548e-02 (4.7386e-02)	Acc@1  98.44 ( 98.50)	Acc@5 100.00 ( 99.99)
Epoch: [86][210/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.6738e-02 (4.7268e-02)	Acc@1  98.44 ( 98.51)	Acc@5 100.00 ( 99.99)
Epoch: [86][220/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.4845e-02 (4.7856e-02)	Acc@1  96.09 ( 98.50)	Acc@5 100.00 ( 99.99)
Epoch: [86][230/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4298e-02 (4.7510e-02)	Acc@1  99.22 ( 98.52)	Acc@5 100.00 ( 99.99)
Epoch: [86][240/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.9750e-02 (4.7080e-02)	Acc@1  99.22 ( 98.54)	Acc@5 100.00 ( 99.99)
Epoch: [86][250/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7940e-02 (4.7346e-02)	Acc@1  99.22 ( 98.53)	Acc@5 100.00 ( 99.99)
Epoch: [86][260/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.3230e-02 (4.7849e-02)	Acc@1  98.44 ( 98.52)	Acc@5 100.00 ( 99.99)
Epoch: [86][270/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.7464e-02 (4.7400e-02)	Acc@1  96.88 ( 98.53)	Acc@5 100.00 ( 99.99)
Epoch: [86][280/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.9942e-02 (4.7823e-02)	Acc@1  99.22 ( 98.51)	Acc@5 100.00 ( 99.99)
Epoch: [86][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0778e-01 (4.7987e-02)	Acc@1  96.09 ( 98.50)	Acc@5 100.00 ( 99.99)
Epoch: [86][300/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.0028e-02 (4.8498e-02)	Acc@1  96.88 ( 98.48)	Acc@5 100.00 ( 99.99)
Epoch: [86][310/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4532e-02 (4.8590e-02)	Acc@1 100.00 ( 98.47)	Acc@5 100.00 ( 99.99)
Epoch: [86][320/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1226e-02 (4.8974e-02)	Acc@1  99.22 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [86][330/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.3780e-02 (4.8346e-02)	Acc@1 100.00 ( 98.47)	Acc@5 100.00 (100.00)
Epoch: [86][340/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.9774e-02 (4.8291e-02)	Acc@1  98.44 ( 98.46)	Acc@5 100.00 (100.00)
Epoch: [86][350/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.4315e-02 (4.8582e-02)	Acc@1  99.22 ( 98.46)	Acc@5 100.00 (100.00)
Epoch: [86][360/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.5673e-02 (4.8617e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [86][370/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.2165e-02 (4.8618e-02)	Acc@1  99.22 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [86][380/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.2472e-02 (4.8706e-02)	Acc@1 100.00 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [86][390/391]	Time  0.029 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.2154e-02 (4.8434e-02)	Acc@1  97.50 ( 98.46)	Acc@5 100.00 (100.00)
## e[86] optimizer.zero_grad (sum) time: 0.2636985778808594
## e[86]       loss.backward (sum) time: 3.9628708362579346
## e[86]      optimizer.step (sum) time: 1.8211731910705566
## epoch[86] training(only) time: 15.951703786849976
# Switched to evaluate mode...
Test: [  0/100]	Time  0.173 ( 0.173)	Loss 3.4020e-01 (3.4020e-01)	Acc@1  91.00 ( 91.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.017 ( 0.033)	Loss 3.9870e-01 (3.2827e-01)	Acc@1  90.00 ( 90.55)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.017 ( 0.027)	Loss 5.8526e-01 (3.3228e-01)	Acc@1  87.00 ( 90.67)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.018 ( 0.024)	Loss 3.8390e-01 (3.4634e-01)	Acc@1  90.00 ( 90.68)	Acc@5  99.00 ( 99.77)
Test: [ 40/100]	Time  0.021 ( 0.023)	Loss 4.0461e-01 (3.6182e-01)	Acc@1  90.00 ( 90.37)	Acc@5  99.00 ( 99.71)
Test: [ 50/100]	Time  0.023 ( 0.023)	Loss 2.7293e-01 (3.5151e-01)	Acc@1  95.00 ( 90.67)	Acc@5 100.00 ( 99.69)
Test: [ 60/100]	Time  0.017 ( 0.023)	Loss 4.3430e-01 (3.5849e-01)	Acc@1  89.00 ( 90.48)	Acc@5  99.00 ( 99.70)
Test: [ 70/100]	Time  0.021 ( 0.023)	Loss 4.3661e-01 (3.4690e-01)	Acc@1  87.00 ( 90.59)	Acc@5 100.00 ( 99.72)
Test: [ 80/100]	Time  0.021 ( 0.022)	Loss 2.0707e-01 (3.4278e-01)	Acc@1  95.00 ( 90.72)	Acc@5 100.00 ( 99.73)
Test: [ 90/100]	Time  0.023 ( 0.022)	Loss 2.2952e-01 (3.4449e-01)	Acc@1  94.00 ( 90.74)	Acc@5 100.00 ( 99.73)
 * Acc@1 90.810 Acc@5 99.730
### epoch[86] execution time: 18.300365447998047
EPOCH 87
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [87][  0/391]	Time  0.222 ( 0.222)	Data  0.175 ( 0.175)	Loss 3.7484e-02 (3.7484e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [87][ 10/391]	Time  0.040 ( 0.056)	Data  0.001 ( 0.017)	Loss 7.0120e-02 (4.5985e-02)	Acc@1  96.09 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [87][ 20/391]	Time  0.038 ( 0.049)	Data  0.001 ( 0.009)	Loss 2.7302e-02 (4.1318e-02)	Acc@1  99.22 ( 98.74)	Acc@5 100.00 (100.00)
Epoch: [87][ 30/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.007)	Loss 4.9867e-02 (4.3484e-02)	Acc@1  97.66 ( 98.54)	Acc@5 100.00 (100.00)
Epoch: [87][ 40/391]	Time  0.040 ( 0.045)	Data  0.001 ( 0.005)	Loss 4.1607e-02 (4.5092e-02)	Acc@1  98.44 ( 98.48)	Acc@5 100.00 (100.00)
Epoch: [87][ 50/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.004)	Loss 3.0588e-02 (4.7262e-02)	Acc@1 100.00 ( 98.38)	Acc@5 100.00 (100.00)
Epoch: [87][ 60/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 7.7075e-02 (4.9017e-02)	Acc@1  96.09 ( 98.34)	Acc@5 100.00 (100.00)
Epoch: [87][ 70/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.3439e-02 (4.7963e-02)	Acc@1  98.44 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [87][ 80/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.003)	Loss 9.8735e-02 (4.8910e-02)	Acc@1  98.44 ( 98.39)	Acc@5 100.00 (100.00)
Epoch: [87][ 90/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.9839e-02 (4.8985e-02)	Acc@1  97.66 ( 98.39)	Acc@5 100.00 (100.00)
Epoch: [87][100/391]	Time  0.045 ( 0.042)	Data  0.002 ( 0.003)	Loss 5.2696e-02 (4.9863e-02)	Acc@1  98.44 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [87][110/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.2815e-02 (4.9911e-02)	Acc@1  99.22 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [87][120/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5423e-02 (4.9436e-02)	Acc@1 100.00 ( 98.39)	Acc@5 100.00 (100.00)
Epoch: [87][130/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.6986e-02 (4.9671e-02)	Acc@1  99.22 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [87][140/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.1579e-02 (4.9558e-02)	Acc@1  98.44 ( 98.38)	Acc@5 100.00 (100.00)
Epoch: [87][150/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.3369e-02 (4.9226e-02)	Acc@1  99.22 ( 98.42)	Acc@5 100.00 (100.00)
Epoch: [87][160/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.8904e-02 (4.9305e-02)	Acc@1  99.22 ( 98.40)	Acc@5 100.00 (100.00)
Epoch: [87][170/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8140e-02 (4.8830e-02)	Acc@1  98.44 ( 98.43)	Acc@5 100.00 (100.00)
Epoch: [87][180/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7167e-02 (4.9649e-02)	Acc@1  99.22 ( 98.40)	Acc@5 100.00 (100.00)
Epoch: [87][190/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2774e-02 (4.9945e-02)	Acc@1  97.66 ( 98.38)	Acc@5 100.00 (100.00)
Epoch: [87][200/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.9196e-02 (5.0106e-02)	Acc@1  96.09 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [87][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.5385e-02 (4.9855e-02)	Acc@1  99.22 ( 98.39)	Acc@5 100.00 (100.00)
Epoch: [87][220/391]	Time  0.047 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.7285e-02 (4.9852e-02)	Acc@1  98.44 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [87][230/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.5208e-02 (5.0544e-02)	Acc@1  97.66 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [87][240/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9759e-02 (5.0302e-02)	Acc@1  99.22 ( 98.39)	Acc@5 100.00 (100.00)
Epoch: [87][250/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.0356e-02 (5.0577e-02)	Acc@1 100.00 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [87][260/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.8505e-02 (5.0420e-02)	Acc@1  98.44 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [87][270/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.4045e-02 (5.0399e-02)	Acc@1  96.88 ( 98.38)	Acc@5 100.00 (100.00)
Epoch: [87][280/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7893e-02 (5.0044e-02)	Acc@1  99.22 ( 98.38)	Acc@5 100.00 (100.00)
Epoch: [87][290/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2981e-02 (5.0183e-02)	Acc@1  97.66 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [87][300/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7853e-02 (4.9998e-02)	Acc@1  99.22 ( 98.38)	Acc@5 100.00 (100.00)
Epoch: [87][310/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.2761e-02 (4.9876e-02)	Acc@1  97.66 ( 98.39)	Acc@5 100.00 (100.00)
Epoch: [87][320/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0910e-02 (4.9806e-02)	Acc@1  97.66 ( 98.40)	Acc@5 100.00 (100.00)
Epoch: [87][330/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.8573e-02 (4.9790e-02)	Acc@1 100.00 ( 98.40)	Acc@5 100.00 (100.00)
Epoch: [87][340/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.5808e-02 (4.9721e-02)	Acc@1 100.00 ( 98.39)	Acc@5 100.00 (100.00)
Epoch: [87][350/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.4609e-02 (4.9729e-02)	Acc@1  97.66 ( 98.39)	Acc@5 100.00 (100.00)
Epoch: [87][360/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.9034e-02 (4.9914e-02)	Acc@1  96.88 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [87][370/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.3565e-02 (5.0164e-02)	Acc@1  99.22 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [87][380/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.1218e-02 (5.0149e-02)	Acc@1  96.88 ( 98.35)	Acc@5 100.00 (100.00)
Epoch: [87][390/391]	Time  0.028 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.7574e-02 (5.0076e-02)	Acc@1  97.50 ( 98.35)	Acc@5 100.00 (100.00)
## e[87] optimizer.zero_grad (sum) time: 0.26235127449035645
## e[87]       loss.backward (sum) time: 3.989664316177368
## e[87]      optimizer.step (sum) time: 1.8073208332061768
## epoch[87] training(only) time: 15.96226167678833
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 3.3166e-01 (3.3166e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.021 ( 0.035)	Loss 4.0773e-01 (3.3155e-01)	Acc@1  90.00 ( 90.36)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 6.1727e-01 (3.3436e-01)	Acc@1  86.00 ( 90.62)	Acc@5 100.00 ( 99.90)
Test: [ 30/100]	Time  0.022 ( 0.026)	Loss 3.8624e-01 (3.5158e-01)	Acc@1  89.00 ( 90.61)	Acc@5  99.00 ( 99.81)
Test: [ 40/100]	Time  0.018 ( 0.024)	Loss 3.9393e-01 (3.6737e-01)	Acc@1  91.00 ( 90.27)	Acc@5  99.00 ( 99.73)
Test: [ 50/100]	Time  0.022 ( 0.024)	Loss 2.7810e-01 (3.5516e-01)	Acc@1  94.00 ( 90.63)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.022 ( 0.023)	Loss 4.2400e-01 (3.6148e-01)	Acc@1  90.00 ( 90.48)	Acc@5  99.00 ( 99.74)
Test: [ 70/100]	Time  0.017 ( 0.023)	Loss 4.3959e-01 (3.5007e-01)	Acc@1  87.00 ( 90.49)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.024 ( 0.023)	Loss 1.8445e-01 (3.4482e-01)	Acc@1  95.00 ( 90.69)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.019 ( 0.023)	Loss 2.2145e-01 (3.4607e-01)	Acc@1  94.00 ( 90.67)	Acc@5 100.00 ( 99.76)
 * Acc@1 90.780 Acc@5 99.760
### epoch[87] execution time: 18.338027238845825
EPOCH 88
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [88][  0/391]	Time  0.223 ( 0.223)	Data  0.181 ( 0.181)	Loss 3.4760e-02 (3.4760e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [88][ 10/391]	Time  0.042 ( 0.057)	Data  0.001 ( 0.017)	Loss 9.5939e-02 (4.5464e-02)	Acc@1  98.44 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [88][ 20/391]	Time  0.043 ( 0.050)	Data  0.001 ( 0.010)	Loss 8.6534e-02 (4.7690e-02)	Acc@1  97.66 ( 98.51)	Acc@5 100.00 (100.00)
Epoch: [88][ 30/391]	Time  0.039 ( 0.047)	Data  0.001 ( 0.007)	Loss 2.9442e-02 (4.7987e-02)	Acc@1 100.00 ( 98.61)	Acc@5 100.00 (100.00)
Epoch: [88][ 40/391]	Time  0.042 ( 0.045)	Data  0.001 ( 0.005)	Loss 3.5203e-02 (4.8814e-02)	Acc@1  99.22 ( 98.51)	Acc@5 100.00 (100.00)
Epoch: [88][ 50/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.005)	Loss 4.9830e-02 (4.9817e-02)	Acc@1  99.22 ( 98.47)	Acc@5 100.00 (100.00)
Epoch: [88][ 60/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.004)	Loss 6.1627e-02 (4.9380e-02)	Acc@1  97.66 ( 98.49)	Acc@5 100.00 (100.00)
Epoch: [88][ 70/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.004)	Loss 8.7929e-02 (5.1327e-02)	Acc@1  96.88 ( 98.42)	Acc@5 100.00 (100.00)
Epoch: [88][ 80/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 6.1848e-02 (5.0912e-02)	Acc@1  96.88 ( 98.43)	Acc@5 100.00 (100.00)
Epoch: [88][ 90/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.2921e-02 (5.0878e-02)	Acc@1  99.22 ( 98.42)	Acc@5 100.00 (100.00)
Epoch: [88][100/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.8302e-02 (4.9070e-02)	Acc@1  99.22 ( 98.50)	Acc@5 100.00 (100.00)
Epoch: [88][110/391]	Time  0.056 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.1462e-02 (4.9111e-02)	Acc@1  99.22 ( 98.49)	Acc@5 100.00 (100.00)
Epoch: [88][120/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.3519e-02 (4.8489e-02)	Acc@1  99.22 ( 98.53)	Acc@5 100.00 (100.00)
Epoch: [88][130/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.1787e-02 (4.8932e-02)	Acc@1  97.66 ( 98.46)	Acc@5 100.00 (100.00)
Epoch: [88][140/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.3854e-02 (4.8974e-02)	Acc@1  98.44 ( 98.45)	Acc@5 100.00 (100.00)
Epoch: [88][150/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.4483e-02 (4.9446e-02)	Acc@1  96.88 ( 98.38)	Acc@5 100.00 ( 99.99)
Epoch: [88][160/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.5975e-02 (4.9082e-02)	Acc@1  99.22 ( 98.39)	Acc@5 100.00 (100.00)
Epoch: [88][170/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3611e-02 (4.8236e-02)	Acc@1 100.00 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [88][180/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.7372e-02 (4.8586e-02)	Acc@1  96.88 ( 98.41)	Acc@5 100.00 ( 99.99)
Epoch: [88][190/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8562e-02 (4.8459e-02)	Acc@1 100.00 ( 98.42)	Acc@5 100.00 ( 99.99)
Epoch: [88][200/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4265e-02 (4.8244e-02)	Acc@1  98.44 ( 98.42)	Acc@5 100.00 ( 99.99)
Epoch: [88][210/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.4027e-02 (4.7909e-02)	Acc@1 100.00 ( 98.43)	Acc@5 100.00 ( 99.99)
Epoch: [88][220/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2466e-02 (4.7858e-02)	Acc@1  99.22 ( 98.45)	Acc@5 100.00 ( 99.99)
Epoch: [88][230/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.7675e-02 (4.8037e-02)	Acc@1  99.22 ( 98.44)	Acc@5 100.00 ( 99.99)
Epoch: [88][240/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.3543e-02 (4.8131e-02)	Acc@1  97.66 ( 98.43)	Acc@5 100.00 ( 99.99)
Epoch: [88][250/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.3645e-02 (4.8082e-02)	Acc@1  96.88 ( 98.43)	Acc@5 100.00 ( 99.99)
Epoch: [88][260/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5132e-02 (4.7810e-02)	Acc@1 100.00 ( 98.44)	Acc@5 100.00 ( 99.99)
Epoch: [88][270/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.3018e-02 (4.8272e-02)	Acc@1  97.66 ( 98.43)	Acc@5 100.00 ( 99.99)
Epoch: [88][280/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.0404e-02 (4.8052e-02)	Acc@1  96.88 ( 98.43)	Acc@5 100.00 ( 99.99)
Epoch: [88][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7174e-02 (4.7991e-02)	Acc@1 100.00 ( 98.44)	Acc@5 100.00 ( 99.99)
Epoch: [88][300/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0597e-01 (4.7630e-02)	Acc@1  97.66 ( 98.46)	Acc@5 100.00 ( 99.99)
Epoch: [88][310/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5146e-02 (4.7436e-02)	Acc@1  99.22 ( 98.46)	Acc@5 100.00 ( 99.99)
Epoch: [88][320/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.4988e-02 (4.7627e-02)	Acc@1  97.66 ( 98.45)	Acc@5 100.00 ( 99.99)
Epoch: [88][330/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.5383e-02 (4.7755e-02)	Acc@1  98.44 ( 98.46)	Acc@5 100.00 ( 99.99)
Epoch: [88][340/391]	Time  0.050 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.6500e-02 (4.7835e-02)	Acc@1  98.44 ( 98.45)	Acc@5 100.00 ( 99.99)
Epoch: [88][350/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.1611e-02 (4.7816e-02)	Acc@1  98.44 ( 98.45)	Acc@5 100.00 ( 99.99)
Epoch: [88][360/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.0679e-02 (4.8142e-02)	Acc@1  97.66 ( 98.43)	Acc@5 100.00 ( 99.99)
Epoch: [88][370/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.0641e-02 (4.8220e-02)	Acc@1  99.22 ( 98.42)	Acc@5 100.00 ( 99.99)
Epoch: [88][380/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.1311e-02 (4.8183e-02)	Acc@1 100.00 ( 98.43)	Acc@5 100.00 ( 99.99)
Epoch: [88][390/391]	Time  0.028 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.4609e-02 (4.8164e-02)	Acc@1  98.75 ( 98.43)	Acc@5 100.00 ( 99.99)
## e[88] optimizer.zero_grad (sum) time: 0.2611429691314697
## e[88]       loss.backward (sum) time: 3.969283103942871
## e[88]      optimizer.step (sum) time: 1.787790060043335
## epoch[88] training(only) time: 16.007716417312622
# Switched to evaluate mode...
Test: [  0/100]	Time  0.181 ( 0.181)	Loss 3.5946e-01 (3.5946e-01)	Acc@1  91.00 ( 91.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.019 ( 0.034)	Loss 4.0998e-01 (3.3307e-01)	Acc@1  90.00 ( 90.36)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.021 ( 0.028)	Loss 5.8955e-01 (3.3206e-01)	Acc@1  87.00 ( 90.81)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.021 ( 0.025)	Loss 4.0406e-01 (3.5179e-01)	Acc@1  89.00 ( 90.68)	Acc@5  99.00 ( 99.74)
Test: [ 40/100]	Time  0.021 ( 0.024)	Loss 3.8383e-01 (3.6343e-01)	Acc@1  91.00 ( 90.34)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.023 ( 0.024)	Loss 2.7455e-01 (3.5290e-01)	Acc@1  95.00 ( 90.69)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.019 ( 0.023)	Loss 4.1868e-01 (3.5982e-01)	Acc@1  89.00 ( 90.46)	Acc@5  99.00 ( 99.72)
Test: [ 70/100]	Time  0.023 ( 0.022)	Loss 4.1977e-01 (3.4758e-01)	Acc@1  88.00 ( 90.59)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.021 ( 0.022)	Loss 1.9659e-01 (3.4315e-01)	Acc@1  95.00 ( 90.79)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.022 ( 0.022)	Loss 2.6121e-01 (3.4414e-01)	Acc@1  94.00 ( 90.79)	Acc@5 100.00 ( 99.75)
 * Acc@1 90.890 Acc@5 99.750
### epoch[88] execution time: 18.31855082511902
EPOCH 89
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [89][  0/391]	Time  0.226 ( 0.226)	Data  0.180 ( 0.180)	Loss 3.0554e-02 (3.0554e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [89][ 10/391]	Time  0.038 ( 0.058)	Data  0.001 ( 0.017)	Loss 1.9155e-02 (4.8233e-02)	Acc@1 100.00 ( 98.51)	Acc@5 100.00 (100.00)
Epoch: [89][ 20/391]	Time  0.039 ( 0.049)	Data  0.001 ( 0.010)	Loss 2.7123e-02 (4.4659e-02)	Acc@1  99.22 ( 98.59)	Acc@5 100.00 (100.00)
Epoch: [89][ 30/391]	Time  0.041 ( 0.047)	Data  0.001 ( 0.007)	Loss 3.9492e-02 (4.4540e-02)	Acc@1  99.22 ( 98.56)	Acc@5 100.00 (100.00)
Epoch: [89][ 40/391]	Time  0.042 ( 0.045)	Data  0.001 ( 0.005)	Loss 2.3640e-02 (4.5699e-02)	Acc@1  99.22 ( 98.49)	Acc@5 100.00 (100.00)
Epoch: [89][ 50/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.005)	Loss 7.2549e-02 (4.5865e-02)	Acc@1  97.66 ( 98.53)	Acc@5 100.00 (100.00)
Epoch: [89][ 60/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.004)	Loss 8.3566e-02 (4.7102e-02)	Acc@1  96.88 ( 98.46)	Acc@5 100.00 (100.00)
Epoch: [89][ 70/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.004)	Loss 6.1180e-02 (4.7209e-02)	Acc@1  97.66 ( 98.47)	Acc@5 100.00 (100.00)
Epoch: [89][ 80/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.5255e-02 (4.6134e-02)	Acc@1  98.44 ( 98.51)	Acc@5 100.00 (100.00)
Epoch: [89][ 90/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.0210e-01 (4.6689e-02)	Acc@1  96.09 ( 98.47)	Acc@5 100.00 (100.00)
Epoch: [89][100/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.7775e-02 (4.8261e-02)	Acc@1  99.22 ( 98.41)	Acc@5 100.00 (100.00)
Epoch: [89][110/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.8663e-02 (4.8873e-02)	Acc@1  99.22 ( 98.42)	Acc@5 100.00 ( 99.99)
Epoch: [89][120/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.3184e-02 (4.8904e-02)	Acc@1  97.66 ( 98.46)	Acc@5 100.00 ( 99.99)
Epoch: [89][130/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.3743e-02 (4.8793e-02)	Acc@1  98.44 ( 98.46)	Acc@5 100.00 ( 99.99)
Epoch: [89][140/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.4680e-02 (4.8579e-02)	Acc@1 100.00 ( 98.46)	Acc@5 100.00 ( 99.99)
Epoch: [89][150/391]	Time  0.035 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.2734e-02 (4.8628e-02)	Acc@1  99.22 ( 98.45)	Acc@5 100.00 ( 99.99)
Epoch: [89][160/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8618e-02 (4.8679e-02)	Acc@1  98.44 ( 98.45)	Acc@5 100.00 (100.00)
Epoch: [89][170/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.7177e-02 (4.8737e-02)	Acc@1  98.44 ( 98.43)	Acc@5 100.00 (100.00)
Epoch: [89][180/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.9085e-02 (4.8272e-02)	Acc@1 100.00 ( 98.45)	Acc@5 100.00 (100.00)
Epoch: [89][190/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7428e-02 (4.7597e-02)	Acc@1  97.66 ( 98.47)	Acc@5 100.00 (100.00)
Epoch: [89][200/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1606e-02 (4.7989e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [89][210/391]	Time  0.042 ( 0.041)	Data  0.002 ( 0.002)	Loss 3.8735e-02 (4.7953e-02)	Acc@1  99.22 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [89][220/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4042e-01 (4.8243e-02)	Acc@1  95.31 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [89][230/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1817e-02 (4.8066e-02)	Acc@1  99.22 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [89][240/391]	Time  0.035 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0988e-02 (4.8639e-02)	Acc@1  97.66 ( 98.43)	Acc@5 100.00 (100.00)
Epoch: [89][250/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.5331e-02 (4.8362e-02)	Acc@1  99.22 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [89][260/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8835e-02 (4.7726e-02)	Acc@1  98.44 ( 98.46)	Acc@5 100.00 (100.00)
Epoch: [89][270/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.5128e-02 (4.8096e-02)	Acc@1  97.66 ( 98.45)	Acc@5 100.00 (100.00)
Epoch: [89][280/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.5492e-02 (4.8222e-02)	Acc@1  97.66 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [89][290/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9754e-02 (4.8517e-02)	Acc@1  99.22 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [89][300/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.7963e-02 (4.9094e-02)	Acc@1  99.22 ( 98.41)	Acc@5 100.00 (100.00)
Epoch: [89][310/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.0201e-02 (4.9300e-02)	Acc@1  96.88 ( 98.40)	Acc@5 100.00 (100.00)
Epoch: [89][320/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1736e-02 (4.9185e-02)	Acc@1  99.22 ( 98.40)	Acc@5 100.00 (100.00)
Epoch: [89][330/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.5406e-02 (4.9089e-02)	Acc@1  99.22 ( 98.40)	Acc@5 100.00 (100.00)
Epoch: [89][340/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.0454e-02 (4.9133e-02)	Acc@1  98.44 ( 98.40)	Acc@5 100.00 (100.00)
Epoch: [89][350/391]	Time  0.035 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.5581e-02 (4.9222e-02)	Acc@1  99.22 ( 98.40)	Acc@5 100.00 (100.00)
Epoch: [89][360/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.6506e-02 (4.9055e-02)	Acc@1  99.22 ( 98.41)	Acc@5 100.00 (100.00)
Epoch: [89][370/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.0982e-02 (4.8861e-02)	Acc@1  99.22 ( 98.42)	Acc@5 100.00 (100.00)
Epoch: [89][380/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.3854e-02 (4.8780e-02)	Acc@1  97.66 ( 98.41)	Acc@5 100.00 (100.00)
Epoch: [89][390/391]	Time  0.028 ( 0.041)	Data  0.001 ( 0.001)	Loss 8.0771e-02 (4.9271e-02)	Acc@1  97.50 ( 98.40)	Acc@5 100.00 (100.00)
## e[89] optimizer.zero_grad (sum) time: 0.26134586334228516
## e[89]       loss.backward (sum) time: 3.973961353302002
## e[89]      optimizer.step (sum) time: 1.8172798156738281
## epoch[89] training(only) time: 15.992558479309082
# Switched to evaluate mode...
Test: [  0/100]	Time  0.218 ( 0.218)	Loss 3.5683e-01 (3.5683e-01)	Acc@1  90.00 ( 90.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.022 ( 0.040)	Loss 4.1220e-01 (3.2968e-01)	Acc@1  90.00 ( 90.55)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.025 ( 0.031)	Loss 6.2512e-01 (3.3380e-01)	Acc@1  86.00 ( 90.71)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.022 ( 0.028)	Loss 3.9096e-01 (3.5117e-01)	Acc@1  89.00 ( 90.65)	Acc@5  99.00 ( 99.77)
Test: [ 40/100]	Time  0.021 ( 0.026)	Loss 3.9743e-01 (3.6387e-01)	Acc@1  91.00 ( 90.34)	Acc@5  99.00 ( 99.71)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 2.8262e-01 (3.5380e-01)	Acc@1  95.00 ( 90.75)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.017 ( 0.024)	Loss 4.2737e-01 (3.6200e-01)	Acc@1  90.00 ( 90.51)	Acc@5  99.00 ( 99.72)
Test: [ 70/100]	Time  0.022 ( 0.023)	Loss 4.4213e-01 (3.4947e-01)	Acc@1  88.00 ( 90.65)	Acc@5 100.00 ( 99.72)
Test: [ 80/100]	Time  0.022 ( 0.023)	Loss 1.9943e-01 (3.4522e-01)	Acc@1  95.00 ( 90.86)	Acc@5 100.00 ( 99.73)
Test: [ 90/100]	Time  0.021 ( 0.023)	Loss 2.4707e-01 (3.4601e-01)	Acc@1  94.00 ( 90.90)	Acc@5 100.00 ( 99.74)
 * Acc@1 91.010 Acc@5 99.740
### epoch[89] execution time: 18.34939193725586
### Training complete:
#### total training(only) time: 1443.5895702838898
##### Total run time: 1658.0712587833405
