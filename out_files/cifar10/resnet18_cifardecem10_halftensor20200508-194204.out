# Model: resnet18
# Dataset: cifardecem
# Batch size: 128
# Freezeout: False
# Low precision: True
# Epochs: 90
# Initial learning rate: 0.1
# module_name: models_cifardecem.resnet
<function resnet18 at 0x7fc8557acf28>
# model requested: 'resnet18'
# printing out the model
ResNet(
  (conv1): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (conv2_x): Sequential(
    (0): BasicBlock(
      (residual_function): Sequential(
        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
    (1): BasicBlock(
      (residual_function): Sequential(
        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
  )
  (conv3_x): Sequential(
    (0): BasicBlock(
      (residual_function): Sequential(
        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (residual_function): Sequential(
        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
  )
  (conv4_x): Sequential(
    (0): BasicBlock(
      (residual_function): Sequential(
        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (residual_function): Sequential(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
  )
  (conv5_x): Sequential(
    (0): BasicBlock(
      (residual_function): Sequential(
        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (residual_function): Sequential(
        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
  )
  (avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
# model is low precision
# Model: resnet18
# Dataset: cifardecem
# Freezeout: False
# Low precision: True
# Initial learning rate: 0.1
# Criterion: CrossEntropyLoss()
# Optimizer: SGD
#	lr 0.1
#	momentum 0.9
#	dampening 0
#	weight_decay 0.0001
#	nesterov False
CIFAR10 loading and normalization
==> Preparing data..
# CIFAR10 / low precision
Files already downloaded and verified
Files already downloaded and verified
#--> Training data: <--#
#-->>
EPOCH 0
# current learning rate: 0.1
# Switched to train mode...
Epoch: [0][  0/391]	Time  3.511 ( 3.511)	Data  0.106 ( 0.106)	Loss 2.4785e+00 (2.4785e+00)	Acc@1  10.94 ( 10.94)	Acc@5  49.22 ( 49.22)
Epoch: [0][ 10/391]	Time  0.036 ( 0.350)	Data  0.001 ( 0.011)	Loss 3.2363e+00 (4.6818e+00)	Acc@1   5.47 ( 10.87)	Acc@5  39.06 ( 50.71)
Epoch: [0][ 20/391]	Time  0.032 ( 0.199)	Data  0.001 ( 0.006)	Loss 2.3418e+00 (3.7775e+00)	Acc@1  14.06 ( 10.94)	Acc@5  64.06 ( 51.71)
Epoch: [0][ 30/391]	Time  0.032 ( 0.145)	Data  0.001 ( 0.005)	Loss 2.3672e+00 (3.3056e+00)	Acc@1  16.41 ( 11.64)	Acc@5  57.03 ( 55.07)
Epoch: [0][ 40/391]	Time  0.032 ( 0.118)	Data  0.001 ( 0.004)	Loss 2.2207e+00 (3.0526e+00)	Acc@1  16.41 ( 12.86)	Acc@5  70.31 ( 57.93)
Epoch: [0][ 50/391]	Time  0.033 ( 0.102)	Data  0.001 ( 0.003)	Loss 2.1719e+00 (2.8782e+00)	Acc@1  22.66 ( 14.06)	Acc@5  82.81 ( 61.55)
Epoch: [0][ 60/391]	Time  0.032 ( 0.091)	Data  0.001 ( 0.003)	Loss 2.0918e+00 (2.7566e+00)	Acc@1  22.66 ( 15.22)	Acc@5  82.03 ( 64.20)
Epoch: [0][ 70/391]	Time  0.032 ( 0.083)	Data  0.001 ( 0.003)	Loss 2.1641e+00 (2.6681e+00)	Acc@1  21.88 ( 16.25)	Acc@5  75.78 ( 66.00)
Epoch: [0][ 80/391]	Time  0.037 ( 0.076)	Data  0.001 ( 0.003)	Loss 2.0820e+00 (2.6033e+00)	Acc@1  23.44 ( 16.74)	Acc@5  77.34 ( 67.23)
Epoch: [0][ 90/391]	Time  0.032 ( 0.072)	Data  0.001 ( 0.003)	Loss 2.0020e+00 (2.5426e+00)	Acc@1  21.88 ( 17.48)	Acc@5  84.38 ( 68.62)
Epoch: [0][100/391]	Time  0.033 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.9463e+00 (2.4879e+00)	Acc@1  20.31 ( 18.12)	Acc@5  79.69 ( 69.84)
Epoch: [0][110/391]	Time  0.028 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9980e+00 (2.4401e+00)	Acc@1  19.53 ( 18.74)	Acc@5  78.91 ( 70.89)
Epoch: [0][120/391]	Time  0.034 ( 0.062)	Data  0.001 ( 0.002)	Loss 1.8721e+00 (2.3995e+00)	Acc@1  33.59 ( 19.43)	Acc@5  87.50 ( 71.79)
Epoch: [0][130/391]	Time  0.034 ( 0.060)	Data  0.001 ( 0.002)	Loss 1.9043e+00 (2.3642e+00)	Acc@1  29.69 ( 20.10)	Acc@5  88.28 ( 72.66)
Epoch: [0][140/391]	Time  0.042 ( 0.058)	Data  0.001 ( 0.002)	Loss 1.9453e+00 (2.3323e+00)	Acc@1  25.78 ( 20.70)	Acc@5  77.34 ( 73.29)
Epoch: [0][150/391]	Time  0.032 ( 0.056)	Data  0.001 ( 0.002)	Loss 1.7676e+00 (2.3035e+00)	Acc@1  34.38 ( 21.30)	Acc@5  90.62 ( 73.95)
Epoch: [0][160/391]	Time  0.034 ( 0.055)	Data  0.001 ( 0.002)	Loss 1.8486e+00 (2.2748e+00)	Acc@1  33.59 ( 21.90)	Acc@5  86.72 ( 74.60)
Epoch: [0][170/391]	Time  0.032 ( 0.054)	Data  0.001 ( 0.002)	Loss 1.9180e+00 (2.2489e+00)	Acc@1  28.12 ( 22.44)	Acc@5  85.94 ( 75.26)
Epoch: [0][180/391]	Time  0.035 ( 0.053)	Data  0.001 ( 0.002)	Loss 1.8848e+00 (2.2268e+00)	Acc@1  36.72 ( 22.90)	Acc@5  79.69 ( 75.76)
Epoch: [0][190/391]	Time  0.030 ( 0.052)	Data  0.001 ( 0.002)	Loss 1.8936e+00 (2.2031e+00)	Acc@1  25.78 ( 23.42)	Acc@5  89.84 ( 76.37)
Epoch: [0][200/391]	Time  0.034 ( 0.051)	Data  0.000 ( 0.002)	Loss 1.8594e+00 (2.1828e+00)	Acc@1  31.25 ( 23.89)	Acc@5  87.50 ( 76.92)
Epoch: [0][210/391]	Time  0.032 ( 0.050)	Data  0.001 ( 0.002)	Loss 1.7305e+00 (2.1656e+00)	Acc@1  31.25 ( 24.21)	Acc@5  91.41 ( 77.32)
Epoch: [0][220/391]	Time  0.030 ( 0.049)	Data  0.001 ( 0.002)	Loss 1.7070e+00 (2.1498e+00)	Acc@1  38.28 ( 24.58)	Acc@5  88.28 ( 77.69)
Epoch: [0][230/391]	Time  0.034 ( 0.049)	Data  0.001 ( 0.002)	Loss 1.7275e+00 (2.1336e+00)	Acc@1  28.91 ( 24.90)	Acc@5  90.62 ( 78.05)
Epoch: [0][240/391]	Time  0.032 ( 0.048)	Data  0.001 ( 0.002)	Loss 1.8496e+00 (2.1181e+00)	Acc@1  32.81 ( 25.22)	Acc@5  83.59 ( 78.43)
Epoch: [0][250/391]	Time  0.034 ( 0.047)	Data  0.001 ( 0.002)	Loss 1.7197e+00 (2.1044e+00)	Acc@1  37.50 ( 25.63)	Acc@5  87.50 ( 78.63)
Epoch: [0][260/391]	Time  0.030 ( 0.047)	Data  0.001 ( 0.002)	Loss 1.7275e+00 (2.0919e+00)	Acc@1  36.72 ( 25.89)	Acc@5  89.84 ( 78.94)
Epoch: [0][270/391]	Time  0.034 ( 0.046)	Data  0.001 ( 0.002)	Loss 1.6621e+00 (2.0767e+00)	Acc@1  35.94 ( 26.25)	Acc@5  90.62 ( 79.28)
Epoch: [0][280/391]	Time  0.029 ( 0.046)	Data  0.001 ( 0.002)	Loss 1.7744e+00 (2.0641e+00)	Acc@1  31.25 ( 26.58)	Acc@5  85.16 ( 79.58)
Epoch: [0][290/391]	Time  0.032 ( 0.045)	Data  0.001 ( 0.002)	Loss 1.7871e+00 (2.0521e+00)	Acc@1  25.78 ( 26.85)	Acc@5  90.62 ( 79.89)
Epoch: [0][300/391]	Time  0.037 ( 0.045)	Data  0.001 ( 0.002)	Loss 1.7764e+00 (2.0410e+00)	Acc@1  33.59 ( 27.13)	Acc@5  86.72 ( 80.20)
Epoch: [0][310/391]	Time  0.029 ( 0.045)	Data  0.001 ( 0.002)	Loss 1.7520e+00 (2.0305e+00)	Acc@1  32.03 ( 27.41)	Acc@5  85.16 ( 80.48)
Epoch: [0][320/391]	Time  0.034 ( 0.044)	Data  0.001 ( 0.002)	Loss 1.5439e+00 (2.0206e+00)	Acc@1  40.62 ( 27.71)	Acc@5  91.41 ( 80.70)
Epoch: [0][330/391]	Time  0.034 ( 0.044)	Data  0.001 ( 0.002)	Loss 1.5977e+00 (2.0103e+00)	Acc@1  34.38 ( 27.98)	Acc@5  92.97 ( 80.96)
Epoch: [0][340/391]	Time  0.032 ( 0.044)	Data  0.001 ( 0.002)	Loss 1.7041e+00 (2.0014e+00)	Acc@1  40.62 ( 28.25)	Acc@5  89.84 ( 81.19)
Epoch: [0][350/391]	Time  0.032 ( 0.043)	Data  0.001 ( 0.002)	Loss 1.5615e+00 (1.9925e+00)	Acc@1  46.09 ( 28.49)	Acc@5  93.75 ( 81.41)
Epoch: [0][360/391]	Time  0.032 ( 0.043)	Data  0.001 ( 0.002)	Loss 1.6650e+00 (1.9830e+00)	Acc@1  37.50 ( 28.80)	Acc@5  90.62 ( 81.64)
Epoch: [0][370/391]	Time  0.032 ( 0.043)	Data  0.001 ( 0.002)	Loss 1.5859e+00 (1.9728e+00)	Acc@1  42.97 ( 29.11)	Acc@5  85.94 ( 81.86)
Epoch: [0][380/391]	Time  0.032 ( 0.043)	Data  0.001 ( 0.002)	Loss 1.5352e+00 (1.9623e+00)	Acc@1  40.62 ( 29.45)	Acc@5  91.41 ( 82.09)
Epoch: [0][390/391]	Time  0.299 ( 0.043)	Data  0.001 ( 0.002)	Loss 1.7090e+00 (1.9544e+00)	Acc@1  42.50 ( 29.68)	Acc@5  90.00 ( 82.31)
## e[0] optimizer.zero_grad (sum) time: 0.1712172031402588
## e[0]       loss.backward (sum) time: 4.182436466217041
## e[0]      optimizer.step (sum) time: 1.2818496227264404
## epoch[0] training(only) time: 16.86024570465088
# Switched to evaluate mode...
Test: [  0/100]	Time  0.263 ( 0.263)	Loss 1.6260e+00 (1.6260e+00)	Acc@1  42.00 ( 42.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.014 ( 0.040)	Loss 1.6992e+00 (1.6453e+00)	Acc@1  38.00 ( 39.27)	Acc@5  88.00 ( 89.36)
Test: [ 20/100]	Time  0.014 ( 0.029)	Loss 1.5049e+00 (1.6271e+00)	Acc@1  44.00 ( 40.52)	Acc@5  91.00 ( 89.24)
Test: [ 30/100]	Time  0.013 ( 0.025)	Loss 1.4502e+00 (1.6391e+00)	Acc@1  45.00 ( 40.35)	Acc@5  90.00 ( 88.71)
Test: [ 40/100]	Time  0.020 ( 0.023)	Loss 1.6309e+00 (1.6420e+00)	Acc@1  42.00 ( 40.29)	Acc@5  93.00 ( 88.61)
Test: [ 50/100]	Time  0.013 ( 0.022)	Loss 1.5859e+00 (1.6319e+00)	Acc@1  40.00 ( 40.25)	Acc@5  89.00 ( 88.94)
Test: [ 60/100]	Time  0.019 ( 0.021)	Loss 1.7373e+00 (1.6404e+00)	Acc@1  34.00 ( 39.75)	Acc@5  86.00 ( 88.97)
Test: [ 70/100]	Time  0.016 ( 0.020)	Loss 1.6943e+00 (1.6445e+00)	Acc@1  33.00 ( 39.44)	Acc@5  83.00 ( 88.94)
Test: [ 80/100]	Time  0.015 ( 0.020)	Loss 1.5791e+00 (1.6420e+00)	Acc@1  44.00 ( 39.59)	Acc@5  89.00 ( 88.98)
Test: [ 90/100]	Time  0.019 ( 0.020)	Loss 1.6094e+00 (1.6471e+00)	Acc@1  33.00 ( 39.19)	Acc@5  87.00 ( 88.78)
 * Acc@1 39.300 Acc@5 88.820
### epoch[0] execution time: 18.91154384613037
EPOCH 1
# current learning rate: 0.1
# Switched to train mode...
Epoch: [1][  0/391]	Time  0.223 ( 0.223)	Data  0.183 ( 0.183)	Loss 1.6924e+00 (1.6924e+00)	Acc@1  37.50 ( 37.50)	Acc@5  89.06 ( 89.06)
Epoch: [1][ 10/391]	Time  0.032 ( 0.050)	Data  0.001 ( 0.018)	Loss 1.6914e+00 (1.6576e+00)	Acc@1  35.16 ( 37.71)	Acc@5  86.72 ( 88.49)
Epoch: [1][ 20/391]	Time  0.032 ( 0.042)	Data  0.001 ( 0.010)	Loss 1.5635e+00 (1.6306e+00)	Acc@1  41.41 ( 38.99)	Acc@5  93.75 ( 89.43)
Epoch: [1][ 30/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.007)	Loss 1.6738e+00 (1.6257e+00)	Acc@1  37.50 ( 39.49)	Acc@5  83.59 ( 89.24)
Epoch: [1][ 40/391]	Time  0.046 ( 0.038)	Data  0.001 ( 0.006)	Loss 1.5967e+00 (1.6184e+00)	Acc@1  46.09 ( 39.71)	Acc@5  89.06 ( 89.52)
Epoch: [1][ 50/391]	Time  0.030 ( 0.037)	Data  0.001 ( 0.005)	Loss 1.6084e+00 (1.6124e+00)	Acc@1  39.06 ( 40.29)	Acc@5  87.50 ( 89.60)
Epoch: [1][ 60/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.005)	Loss 1.4375e+00 (1.6032e+00)	Acc@1  49.22 ( 40.78)	Acc@5  89.84 ( 89.75)
Epoch: [1][ 70/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 1.3789e+00 (1.5956e+00)	Acc@1  46.09 ( 41.08)	Acc@5  91.41 ( 89.96)
Epoch: [1][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.3604e+00 (1.5884e+00)	Acc@1  50.78 ( 41.31)	Acc@5  90.62 ( 90.12)
Epoch: [1][ 90/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.5518e+00 (1.5828e+00)	Acc@1  41.41 ( 41.40)	Acc@5  92.97 ( 90.22)
Epoch: [1][100/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.6025e+00 (1.5760e+00)	Acc@1  36.72 ( 41.37)	Acc@5  89.84 ( 90.35)
Epoch: [1][110/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.5342e+00 (1.5688e+00)	Acc@1  42.97 ( 41.58)	Acc@5  91.41 ( 90.55)
Epoch: [1][120/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.4336e+00 (1.5638e+00)	Acc@1  45.31 ( 41.70)	Acc@5  95.31 ( 90.65)
Epoch: [1][130/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.5479e+00 (1.5612e+00)	Acc@1  46.09 ( 41.81)	Acc@5  90.62 ( 90.73)
Epoch: [1][140/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.6309e+00 (1.5577e+00)	Acc@1  34.38 ( 41.87)	Acc@5  88.28 ( 90.77)
Epoch: [1][150/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.4590e+00 (1.5546e+00)	Acc@1  42.97 ( 42.01)	Acc@5  92.97 ( 90.79)
Epoch: [1][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.4746e+00 (1.5515e+00)	Acc@1  49.22 ( 42.27)	Acc@5  90.62 ( 90.77)
Epoch: [1][170/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.3662e+00 (1.5495e+00)	Acc@1  46.09 ( 42.39)	Acc@5  93.75 ( 90.82)
Epoch: [1][180/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5146e+00 (1.5432e+00)	Acc@1  45.31 ( 42.68)	Acc@5  89.84 ( 90.91)
Epoch: [1][190/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4229e+00 (1.5403e+00)	Acc@1  46.88 ( 42.82)	Acc@5  90.62 ( 90.89)
Epoch: [1][200/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2480e+00 (1.5341e+00)	Acc@1  47.66 ( 42.99)	Acc@5  96.88 ( 91.01)
Epoch: [1][210/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4170e+00 (1.5313e+00)	Acc@1  48.44 ( 43.16)	Acc@5  89.06 ( 91.01)
Epoch: [1][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4463e+00 (1.5292e+00)	Acc@1  44.53 ( 43.19)	Acc@5  92.19 ( 91.06)
Epoch: [1][230/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3379e+00 (1.5261e+00)	Acc@1  53.12 ( 43.36)	Acc@5  96.09 ( 91.08)
Epoch: [1][240/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4951e+00 (1.5242e+00)	Acc@1  45.31 ( 43.40)	Acc@5  88.28 ( 91.10)
Epoch: [1][250/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3350e+00 (1.5213e+00)	Acc@1  50.00 ( 43.43)	Acc@5  94.53 ( 91.18)
Epoch: [1][260/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5000e+00 (1.5170e+00)	Acc@1  44.53 ( 43.53)	Acc@5  93.75 ( 91.24)
Epoch: [1][270/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4238e+00 (1.5134e+00)	Acc@1  49.22 ( 43.72)	Acc@5  93.75 ( 91.31)
Epoch: [1][280/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4346e+00 (1.5107e+00)	Acc@1  50.78 ( 43.78)	Acc@5  92.19 ( 91.37)
Epoch: [1][290/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.6104e+00 (1.5083e+00)	Acc@1  43.75 ( 43.88)	Acc@5  88.28 ( 91.41)
Epoch: [1][300/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3770e+00 (1.5048e+00)	Acc@1  49.22 ( 44.02)	Acc@5  89.84 ( 91.45)
Epoch: [1][310/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5566e+00 (1.5042e+00)	Acc@1  48.44 ( 44.08)	Acc@5  92.19 ( 91.49)
Epoch: [1][320/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3223e+00 (1.5001e+00)	Acc@1  56.25 ( 44.26)	Acc@5  95.31 ( 91.53)
Epoch: [1][330/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3281e+00 (1.4968e+00)	Acc@1  44.53 ( 44.38)	Acc@5  94.53 ( 91.59)
Epoch: [1][340/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3398e+00 (1.4922e+00)	Acc@1  52.34 ( 44.60)	Acc@5  94.53 ( 91.64)
Epoch: [1][350/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4102e+00 (1.4881e+00)	Acc@1  46.88 ( 44.77)	Acc@5  96.09 ( 91.71)
Epoch: [1][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4775e+00 (1.4848e+00)	Acc@1  49.22 ( 44.91)	Acc@5  92.97 ( 91.78)
Epoch: [1][370/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0732e+00 (1.4808e+00)	Acc@1  60.94 ( 45.08)	Acc@5  97.66 ( 91.87)
Epoch: [1][380/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5361e+00 (1.4780e+00)	Acc@1  44.53 ( 45.19)	Acc@5  93.75 ( 91.91)
Epoch: [1][390/391]	Time  0.022 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.7188e+00 (1.4761e+00)	Acc@1  37.50 ( 45.23)	Acc@5  90.00 ( 91.94)
## e[1] optimizer.zero_grad (sum) time: 0.16752314567565918
## e[1]       loss.backward (sum) time: 3.903355836868286
## e[1]      optimizer.step (sum) time: 1.2631721496582031
## epoch[1] training(only) time: 13.282690048217773
# Switched to evaluate mode...
Test: [  0/100]	Time  0.192 ( 0.192)	Loss 1.3838e+00 (1.3838e+00)	Acc@1  52.00 ( 52.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.014 ( 0.033)	Loss 1.1826e+00 (1.3236e+00)	Acc@1  60.00 ( 51.00)	Acc@5  95.00 ( 93.82)
Test: [ 20/100]	Time  0.014 ( 0.025)	Loss 1.2578e+00 (1.3341e+00)	Acc@1  53.00 ( 50.71)	Acc@5  96.00 ( 93.86)
Test: [ 30/100]	Time  0.027 ( 0.023)	Loss 1.2295e+00 (1.3377e+00)	Acc@1  56.00 ( 50.90)	Acc@5  95.00 ( 93.65)
Test: [ 40/100]	Time  0.017 ( 0.021)	Loss 1.3730e+00 (1.3377e+00)	Acc@1  47.00 ( 51.15)	Acc@5  94.00 ( 93.66)
Test: [ 50/100]	Time  0.014 ( 0.020)	Loss 1.4346e+00 (1.3341e+00)	Acc@1  46.00 ( 50.88)	Acc@5  92.00 ( 93.92)
Test: [ 60/100]	Time  0.014 ( 0.020)	Loss 1.2510e+00 (1.3341e+00)	Acc@1  44.00 ( 50.57)	Acc@5  98.00 ( 94.03)
Test: [ 70/100]	Time  0.015 ( 0.019)	Loss 1.3838e+00 (1.3387e+00)	Acc@1  44.00 ( 50.08)	Acc@5  95.00 ( 94.14)
Test: [ 80/100]	Time  0.016 ( 0.019)	Loss 1.2705e+00 (1.3332e+00)	Acc@1  54.00 ( 50.32)	Acc@5  90.00 ( 94.21)
Test: [ 90/100]	Time  0.024 ( 0.019)	Loss 1.2871e+00 (1.3409e+00)	Acc@1  50.00 ( 50.01)	Acc@5  98.00 ( 94.14)
 * Acc@1 50.240 Acc@5 94.170
### epoch[1] execution time: 15.259301900863647
EPOCH 2
# current learning rate: 0.1
# Switched to train mode...
Epoch: [2][  0/391]	Time  0.216 ( 0.216)	Data  0.181 ( 0.181)	Loss 1.2900e+00 (1.2900e+00)	Acc@1  51.56 ( 51.56)	Acc@5  95.31 ( 95.31)
Epoch: [2][ 10/391]	Time  0.032 ( 0.050)	Data  0.001 ( 0.018)	Loss 1.3076e+00 (1.3041e+00)	Acc@1  55.47 ( 52.34)	Acc@5  93.75 ( 94.18)
Epoch: [2][ 20/391]	Time  0.032 ( 0.042)	Data  0.001 ( 0.010)	Loss 1.3145e+00 (1.3015e+00)	Acc@1  52.34 ( 53.09)	Acc@5  92.97 ( 94.20)
Epoch: [2][ 30/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.007)	Loss 1.1904e+00 (1.2985e+00)	Acc@1  64.84 ( 53.20)	Acc@5  95.31 ( 93.85)
Epoch: [2][ 40/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.006)	Loss 1.2871e+00 (1.3112e+00)	Acc@1  53.12 ( 52.76)	Acc@5  94.53 ( 93.79)
Epoch: [2][ 50/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 1.2480e+00 (1.3003e+00)	Acc@1  56.25 ( 53.23)	Acc@5  94.53 ( 93.98)
Epoch: [2][ 60/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.004)	Loss 1.3145e+00 (1.2999e+00)	Acc@1  46.88 ( 52.82)	Acc@5  95.31 ( 94.04)
Epoch: [2][ 70/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.2412e+00 (1.3032e+00)	Acc@1  54.69 ( 52.53)	Acc@5  95.31 ( 94.11)
Epoch: [2][ 80/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.2432e+00 (1.3055e+00)	Acc@1  56.25 ( 52.51)	Acc@5  92.97 ( 94.03)
Epoch: [2][ 90/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.2627e+00 (1.3015e+00)	Acc@1  53.12 ( 52.88)	Acc@5  95.31 ( 93.99)
Epoch: [2][100/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.3896e+00 (1.2971e+00)	Acc@1  45.31 ( 52.92)	Acc@5  92.19 ( 94.04)
Epoch: [2][110/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.2959e+00 (1.2939e+00)	Acc@1  53.12 ( 53.07)	Acc@5  88.28 ( 93.95)
Epoch: [2][120/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.4258e+00 (1.2920e+00)	Acc@1  51.56 ( 53.18)	Acc@5  91.41 ( 93.92)
Epoch: [2][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.2051e+00 (1.2859e+00)	Acc@1  55.47 ( 53.38)	Acc@5  96.88 ( 94.09)
Epoch: [2][140/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.3301e+00 (1.2819e+00)	Acc@1  53.12 ( 53.59)	Acc@5  95.31 ( 94.13)
Epoch: [2][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.2666e+00 (1.2780e+00)	Acc@1  50.78 ( 53.71)	Acc@5  95.31 ( 94.19)
Epoch: [2][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.3164e+00 (1.2785e+00)	Acc@1  52.34 ( 53.63)	Acc@5  93.75 ( 94.25)
Epoch: [2][170/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1562e+00 (1.2738e+00)	Acc@1  60.16 ( 53.77)	Acc@5  94.53 ( 94.28)
Epoch: [2][180/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1348e+00 (1.2706e+00)	Acc@1  62.50 ( 53.96)	Acc@5  98.44 ( 94.30)
Epoch: [2][190/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3408e+00 (1.2692e+00)	Acc@1  48.44 ( 54.02)	Acc@5  95.31 ( 94.34)
Epoch: [2][200/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1729e+00 (1.2650e+00)	Acc@1  53.91 ( 54.18)	Acc@5  98.44 ( 94.40)
Epoch: [2][210/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2373e+00 (1.2625e+00)	Acc@1  54.69 ( 54.26)	Acc@5  93.75 ( 94.44)
Epoch: [2][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2637e+00 (1.2608e+00)	Acc@1  57.81 ( 54.39)	Acc@5  93.75 ( 94.44)
Epoch: [2][230/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1494e+00 (1.2579e+00)	Acc@1  56.25 ( 54.51)	Acc@5  97.66 ( 94.47)
Epoch: [2][240/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0557e+00 (1.2550e+00)	Acc@1  58.59 ( 54.57)	Acc@5  96.88 ( 94.55)
Epoch: [2][250/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1582e+00 (1.2526e+00)	Acc@1  57.03 ( 54.64)	Acc@5  94.53 ( 94.60)
Epoch: [2][260/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1807e+00 (1.2483e+00)	Acc@1  57.03 ( 54.81)	Acc@5  96.09 ( 94.63)
Epoch: [2][270/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1777e+00 (1.2452e+00)	Acc@1  57.03 ( 54.90)	Acc@5  96.09 ( 94.70)
Epoch: [2][280/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4355e+00 (1.2426e+00)	Acc@1  50.00 ( 54.96)	Acc@5  92.19 ( 94.72)
Epoch: [2][290/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1377e+00 (1.2395e+00)	Acc@1  59.38 ( 55.06)	Acc@5  97.66 ( 94.73)
Epoch: [2][300/391]	Time  0.031 ( 0.034)	Data  0.000 ( 0.002)	Loss 1.2051e+00 (1.2370e+00)	Acc@1  56.25 ( 55.18)	Acc@5  97.66 ( 94.75)
Epoch: [2][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1963e+00 (1.2338e+00)	Acc@1  57.81 ( 55.36)	Acc@5  99.22 ( 94.79)
Epoch: [2][320/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1797e+00 (1.2304e+00)	Acc@1  62.50 ( 55.50)	Acc@5  95.31 ( 94.81)
Epoch: [2][330/391]	Time  0.037 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2207e+00 (1.2285e+00)	Acc@1  57.03 ( 55.58)	Acc@5  96.09 ( 94.82)
Epoch: [2][340/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0117e+00 (1.2264e+00)	Acc@1  67.19 ( 55.67)	Acc@5  96.09 ( 94.82)
Epoch: [2][350/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0957e+00 (1.2225e+00)	Acc@1  59.38 ( 55.83)	Acc@5  94.53 ( 94.85)
Epoch: [2][360/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.9072e-01 (1.2194e+00)	Acc@1  62.50 ( 55.94)	Acc@5  96.88 ( 94.87)
Epoch: [2][370/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1582e+00 (1.2172e+00)	Acc@1  62.50 ( 56.01)	Acc@5  93.75 ( 94.89)
Epoch: [2][380/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1133e+00 (1.2150e+00)	Acc@1  63.28 ( 56.11)	Acc@5  95.31 ( 94.89)
Epoch: [2][390/391]	Time  0.028 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0703e+00 (1.2127e+00)	Acc@1  63.75 ( 56.21)	Acc@5  93.75 ( 94.90)
## e[2] optimizer.zero_grad (sum) time: 0.16881394386291504
## e[2]       loss.backward (sum) time: 3.9965603351593018
## e[2]      optimizer.step (sum) time: 1.2237708568572998
## epoch[2] training(only) time: 13.294233798980713
# Switched to evaluate mode...
Test: [  0/100]	Time  0.177 ( 0.177)	Loss 9.4482e-01 (9.4482e-01)	Acc@1  67.00 ( 67.00)	Acc@5  97.00 ( 97.00)
Test: [ 10/100]	Time  0.016 ( 0.031)	Loss 8.2520e-01 (1.0432e+00)	Acc@1  71.00 ( 61.09)	Acc@5  97.00 ( 97.09)
Test: [ 20/100]	Time  0.015 ( 0.024)	Loss 1.0225e+00 (1.0689e+00)	Acc@1  62.00 ( 60.29)	Acc@5  98.00 ( 96.86)
Test: [ 30/100]	Time  0.020 ( 0.022)	Loss 1.0537e+00 (1.0800e+00)	Acc@1  65.00 ( 60.74)	Acc@5  98.00 ( 96.90)
Test: [ 40/100]	Time  0.018 ( 0.020)	Loss 1.1074e+00 (1.0853e+00)	Acc@1  67.00 ( 60.78)	Acc@5  95.00 ( 96.54)
Test: [ 50/100]	Time  0.022 ( 0.020)	Loss 1.0684e+00 (1.0742e+00)	Acc@1  65.00 ( 61.22)	Acc@5  95.00 ( 96.65)
Test: [ 60/100]	Time  0.013 ( 0.019)	Loss 1.0918e+00 (1.0785e+00)	Acc@1  58.00 ( 61.16)	Acc@5  98.00 ( 96.54)
Test: [ 70/100]	Time  0.016 ( 0.019)	Loss 9.7021e-01 (1.0841e+00)	Acc@1  62.00 ( 60.79)	Acc@5  97.00 ( 96.61)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.0674e+00 (1.0800e+00)	Acc@1  60.00 ( 60.90)	Acc@5  94.00 ( 96.54)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 1.1133e+00 (1.0859e+00)	Acc@1  60.00 ( 60.68)	Acc@5  98.00 ( 96.51)
 * Acc@1 60.830 Acc@5 96.470
### epoch[2] execution time: 15.259985208511353
EPOCH 3
# current learning rate: 0.1
# Switched to train mode...
Epoch: [3][  0/391]	Time  0.209 ( 0.209)	Data  0.176 ( 0.176)	Loss 9.4482e-01 (9.4482e-01)	Acc@1  67.19 ( 67.19)	Acc@5  98.44 ( 98.44)
Epoch: [3][ 10/391]	Time  0.032 ( 0.050)	Data  0.001 ( 0.018)	Loss 8.9648e-01 (1.0225e+00)	Acc@1  70.31 ( 63.71)	Acc@5  98.44 ( 96.59)
Epoch: [3][ 20/391]	Time  0.033 ( 0.042)	Data  0.001 ( 0.010)	Loss 9.6143e-01 (1.0370e+00)	Acc@1  67.97 ( 63.62)	Acc@5  97.66 ( 96.06)
Epoch: [3][ 30/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.007)	Loss 1.0889e+00 (1.0622e+00)	Acc@1  57.81 ( 62.58)	Acc@5  96.09 ( 96.14)
Epoch: [3][ 40/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.006)	Loss 1.0439e+00 (1.0519e+00)	Acc@1  60.16 ( 62.58)	Acc@5  94.53 ( 96.36)
Epoch: [3][ 50/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 1.1973e+00 (1.0741e+00)	Acc@1  60.94 ( 61.89)	Acc@5  97.66 ( 96.11)
Epoch: [3][ 60/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 9.2090e-01 (1.0691e+00)	Acc@1  73.44 ( 62.21)	Acc@5  96.88 ( 96.18)
Epoch: [3][ 70/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 8.3350e-01 (1.0686e+00)	Acc@1  67.97 ( 62.06)	Acc@5  99.22 ( 96.21)
Epoch: [3][ 80/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.004)	Loss 9.3994e-01 (1.0642e+00)	Acc@1  68.75 ( 62.42)	Acc@5  95.31 ( 96.17)
Epoch: [3][ 90/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.003)	Loss 9.4043e-01 (1.0610e+00)	Acc@1  64.84 ( 62.54)	Acc@5 100.00 ( 96.20)
Epoch: [3][100/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.0098e+00 (1.0590e+00)	Acc@1  61.72 ( 62.55)	Acc@5  96.88 ( 96.12)
Epoch: [3][110/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 9.7803e-01 (1.0563e+00)	Acc@1  60.94 ( 62.71)	Acc@5  96.88 ( 96.16)
Epoch: [3][120/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.0938e+00 (1.0552e+00)	Acc@1  62.50 ( 62.79)	Acc@5  96.09 ( 96.16)
Epoch: [3][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.0029e+00 (1.0514e+00)	Acc@1  68.75 ( 63.05)	Acc@5  96.88 ( 96.17)
Epoch: [3][140/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.0254e+00 (1.0520e+00)	Acc@1  68.75 ( 63.11)	Acc@5  96.88 ( 96.15)
Epoch: [3][150/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.3584e+00 (1.0559e+00)	Acc@1  52.34 ( 62.84)	Acc@5  92.19 ( 96.16)
Epoch: [3][160/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 9.1309e-01 (1.0535e+00)	Acc@1  67.97 ( 62.92)	Acc@5  97.66 ( 96.18)
Epoch: [3][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.7988e-01 (1.0536e+00)	Acc@1  67.97 ( 62.88)	Acc@5  97.66 ( 96.19)
Epoch: [3][180/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1436e+00 (1.0522e+00)	Acc@1  57.03 ( 62.95)	Acc@5  95.31 ( 96.20)
Epoch: [3][190/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0928e+00 (1.0523e+00)	Acc@1  64.06 ( 63.02)	Acc@5  97.66 ( 96.22)
Epoch: [3][200/391]	Time  0.040 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0996e+00 (1.0519e+00)	Acc@1  61.72 ( 63.10)	Acc@5  96.09 ( 96.26)
Epoch: [3][210/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0303e+00 (1.0493e+00)	Acc@1  65.62 ( 63.19)	Acc@5  97.66 ( 96.30)
Epoch: [3][220/391]	Time  0.037 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.8389e-01 (1.0462e+00)	Acc@1  61.72 ( 63.23)	Acc@5  95.31 ( 96.32)
Epoch: [3][230/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.6240e-01 (1.0419e+00)	Acc@1  64.06 ( 63.39)	Acc@5  96.88 ( 96.36)
Epoch: [3][240/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0557e+00 (1.0415e+00)	Acc@1  62.50 ( 63.42)	Acc@5  94.53 ( 96.36)
Epoch: [3][250/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0176e+00 (1.0395e+00)	Acc@1  64.84 ( 63.52)	Acc@5  96.09 ( 96.37)
Epoch: [3][260/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.5703e-01 (1.0394e+00)	Acc@1  69.53 ( 63.49)	Acc@5  96.09 ( 96.40)
Epoch: [3][270/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.8428e-01 (1.0366e+00)	Acc@1  67.19 ( 63.58)	Acc@5  96.88 ( 96.43)
Epoch: [3][280/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.6533e-01 (1.0338e+00)	Acc@1  64.06 ( 63.63)	Acc@5  98.44 ( 96.47)
Epoch: [3][290/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.6533e-01 (1.0314e+00)	Acc@1  69.53 ( 63.67)	Acc@5  93.75 ( 96.48)
Epoch: [3][300/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.8379e-01 (1.0284e+00)	Acc@1  62.50 ( 63.75)	Acc@5  96.09 ( 96.50)
Epoch: [3][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0537e+00 (1.0247e+00)	Acc@1  62.50 ( 63.86)	Acc@5  94.53 ( 96.52)
Epoch: [3][320/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1553e+00 (1.0216e+00)	Acc@1  62.50 ( 63.93)	Acc@5  93.75 ( 96.55)
Epoch: [3][330/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.2480e-01 (1.0183e+00)	Acc@1  63.28 ( 64.02)	Acc@5  95.31 ( 96.56)
Epoch: [3][340/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0254e+00 (1.0157e+00)	Acc@1  63.28 ( 64.12)	Acc@5  93.75 ( 96.57)
Epoch: [3][350/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.4971e-01 (1.0127e+00)	Acc@1  64.84 ( 64.20)	Acc@5  96.09 ( 96.59)
Epoch: [3][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0000e+00 (1.0111e+00)	Acc@1  65.62 ( 64.25)	Acc@5  96.09 ( 96.59)
Epoch: [3][370/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0469e+00 (1.0092e+00)	Acc@1  65.62 ( 64.36)	Acc@5  99.22 ( 96.62)
Epoch: [3][380/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1455e+00 (1.0078e+00)	Acc@1  60.16 ( 64.39)	Acc@5  99.22 ( 96.65)
Epoch: [3][390/391]	Time  0.027 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.3340e-01 (1.0070e+00)	Acc@1  75.00 ( 64.42)	Acc@5  98.75 ( 96.65)
## e[3] optimizer.zero_grad (sum) time: 0.17060184478759766
## e[3]       loss.backward (sum) time: 3.994947910308838
## e[3]      optimizer.step (sum) time: 1.2369976043701172
## epoch[3] training(only) time: 13.322646617889404
# Switched to evaluate mode...
Test: [  0/100]	Time  0.175 ( 0.175)	Loss 7.9395e-01 (7.9395e-01)	Acc@1  73.00 ( 73.00)	Acc@5  97.00 ( 97.00)
Test: [ 10/100]	Time  0.016 ( 0.031)	Loss 7.2168e-01 (9.0008e-01)	Acc@1  72.00 ( 68.18)	Acc@5  98.00 ( 97.18)
Test: [ 20/100]	Time  0.021 ( 0.025)	Loss 8.7500e-01 (9.1990e-01)	Acc@1  66.00 ( 66.76)	Acc@5  98.00 ( 97.52)
Test: [ 30/100]	Time  0.023 ( 0.023)	Loss 9.4043e-01 (9.1776e-01)	Acc@1  69.00 ( 67.19)	Acc@5  95.00 ( 97.35)
Test: [ 40/100]	Time  0.014 ( 0.021)	Loss 9.2236e-01 (9.1896e-01)	Acc@1  65.00 ( 67.20)	Acc@5  97.00 ( 97.15)
Test: [ 50/100]	Time  0.014 ( 0.020)	Loss 8.9551e-01 (9.1550e-01)	Acc@1  67.00 ( 67.45)	Acc@5  98.00 ( 97.12)
Test: [ 60/100]	Time  0.017 ( 0.020)	Loss 9.6387e-01 (9.2021e-01)	Acc@1  61.00 ( 67.02)	Acc@5  99.00 ( 97.25)
Test: [ 70/100]	Time  0.022 ( 0.019)	Loss 9.1064e-01 (9.2238e-01)	Acc@1  67.00 ( 66.90)	Acc@5  96.00 ( 97.31)
Test: [ 80/100]	Time  0.016 ( 0.019)	Loss 8.7012e-01 (9.1954e-01)	Acc@1  67.00 ( 66.99)	Acc@5  97.00 ( 97.26)
Test: [ 90/100]	Time  0.015 ( 0.019)	Loss 7.4170e-01 (9.2068e-01)	Acc@1  75.00 ( 66.98)	Acc@5 100.00 ( 97.26)
 * Acc@1 67.010 Acc@5 97.310
### epoch[3] execution time: 15.295230865478516
EPOCH 4
# current learning rate: 0.1
# Switched to train mode...
Epoch: [4][  0/391]	Time  0.203 ( 0.203)	Data  0.169 ( 0.169)	Loss 1.0605e+00 (1.0605e+00)	Acc@1  65.62 ( 65.62)	Acc@5  92.19 ( 92.19)
Epoch: [4][ 10/391]	Time  0.032 ( 0.050)	Data  0.001 ( 0.017)	Loss 8.7158e-01 (9.0545e-01)	Acc@1  71.09 ( 68.68)	Acc@5 100.00 ( 97.09)
Epoch: [4][ 20/391]	Time  0.032 ( 0.042)	Data  0.001 ( 0.010)	Loss 8.6475e-01 (9.0055e-01)	Acc@1  75.00 ( 68.64)	Acc@5  96.88 ( 97.40)
Epoch: [4][ 30/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.007)	Loss 9.2822e-01 (9.0239e-01)	Acc@1  65.62 ( 68.45)	Acc@5  96.88 ( 97.53)
Epoch: [4][ 40/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.006)	Loss 9.5361e-01 (8.9991e-01)	Acc@1  68.75 ( 67.97)	Acc@5  95.31 ( 97.54)
Epoch: [4][ 50/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 7.6758e-01 (8.9087e-01)	Acc@1  71.88 ( 68.18)	Acc@5  99.22 ( 97.69)
Epoch: [4][ 60/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.004)	Loss 9.0479e-01 (8.9599e-01)	Acc@1  68.75 ( 68.21)	Acc@5  98.44 ( 97.71)
Epoch: [4][ 70/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.004)	Loss 8.6230e-01 (8.9984e-01)	Acc@1  71.88 ( 68.05)	Acc@5  96.09 ( 97.69)
Epoch: [4][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 8.6572e-01 (9.0186e-01)	Acc@1  71.09 ( 67.88)	Acc@5  98.44 ( 97.78)
Epoch: [4][ 90/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 8.8574e-01 (8.9159e-01)	Acc@1  67.19 ( 68.24)	Acc@5  96.88 ( 97.76)
Epoch: [4][100/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.0391e+00 (8.9156e-01)	Acc@1  65.62 ( 68.26)	Acc@5  95.31 ( 97.66)
Epoch: [4][110/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.003)	Loss 9.0332e-01 (8.9208e-01)	Acc@1  70.31 ( 68.32)	Acc@5  98.44 ( 97.65)
Epoch: [4][120/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 8.1494e-01 (8.9100e-01)	Acc@1  74.22 ( 68.45)	Acc@5  97.66 ( 97.62)
Epoch: [4][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 8.6670e-01 (8.9152e-01)	Acc@1  67.97 ( 68.55)	Acc@5  99.22 ( 97.53)
Epoch: [4][140/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 8.8867e-01 (8.8800e-01)	Acc@1  68.75 ( 68.67)	Acc@5  96.88 ( 97.53)
Epoch: [4][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 8.4814e-01 (8.8623e-01)	Acc@1  66.41 ( 68.70)	Acc@5  99.22 ( 97.53)
Epoch: [4][160/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.9688e-01 (8.8735e-01)	Acc@1  69.53 ( 68.69)	Acc@5  98.44 ( 97.54)
Epoch: [4][170/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.2900e-01 (8.8404e-01)	Acc@1  72.66 ( 68.68)	Acc@5  99.22 ( 97.59)
Epoch: [4][180/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.6768e-01 (8.8065e-01)	Acc@1  69.53 ( 68.80)	Acc@5  95.31 ( 97.60)
Epoch: [4][190/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.4580e-01 (8.8146e-01)	Acc@1  68.75 ( 68.76)	Acc@5  96.09 ( 97.60)
Epoch: [4][200/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0176e+00 (8.8136e-01)	Acc@1  66.41 ( 68.79)	Acc@5  96.88 ( 97.60)
Epoch: [4][210/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.8770e-01 (8.8139e-01)	Acc@1  70.31 ( 68.75)	Acc@5  99.22 ( 97.63)
Epoch: [4][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.3555e-01 (8.8079e-01)	Acc@1  65.62 ( 68.82)	Acc@5  96.88 ( 97.59)
Epoch: [4][230/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.3945e-01 (8.8178e-01)	Acc@1  69.53 ( 68.82)	Acc@5  97.66 ( 97.58)
Epoch: [4][240/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.1992e-01 (8.8027e-01)	Acc@1  74.22 ( 68.92)	Acc@5  96.88 ( 97.58)
Epoch: [4][250/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.6758e-01 (8.8049e-01)	Acc@1  75.00 ( 68.94)	Acc@5 100.00 ( 97.59)
Epoch: [4][260/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.1211e-01 (8.7972e-01)	Acc@1  71.88 ( 68.99)	Acc@5  97.66 ( 97.58)
Epoch: [4][270/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.7939e-01 (8.8156e-01)	Acc@1  70.31 ( 68.96)	Acc@5  95.31 ( 97.54)
Epoch: [4][280/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.1748e-01 (8.7990e-01)	Acc@1  66.41 ( 69.02)	Acc@5  96.88 ( 97.55)
Epoch: [4][290/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0195e+00 (8.7764e-01)	Acc@1  67.97 ( 69.11)	Acc@5  96.09 ( 97.55)
Epoch: [4][300/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.1973e-01 (8.7735e-01)	Acc@1  75.00 ( 69.12)	Acc@5  98.44 ( 97.54)
Epoch: [4][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.8271e-01 (8.7600e-01)	Acc@1  71.09 ( 69.17)	Acc@5  99.22 ( 97.53)
Epoch: [4][320/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.4766e-01 (8.7500e-01)	Acc@1  67.19 ( 69.21)	Acc@5  97.66 ( 97.52)
Epoch: [4][330/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.2070e-01 (8.7184e-01)	Acc@1  75.78 ( 69.32)	Acc@5  97.66 ( 97.51)
Epoch: [4][340/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.7725e-01 (8.6897e-01)	Acc@1  75.00 ( 69.41)	Acc@5  98.44 ( 97.53)
Epoch: [4][350/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.9385e-01 (8.6565e-01)	Acc@1  70.31 ( 69.53)	Acc@5  99.22 ( 97.54)
Epoch: [4][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.9102e-01 (8.6268e-01)	Acc@1  74.22 ( 69.67)	Acc@5  96.09 ( 97.55)
Epoch: [4][370/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.0771e-01 (8.6198e-01)	Acc@1  62.50 ( 69.68)	Acc@5  97.66 ( 97.57)
Epoch: [4][380/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.9443e-01 (8.5976e-01)	Acc@1  71.88 ( 69.80)	Acc@5  97.66 ( 97.58)
Epoch: [4][390/391]	Time  0.023 ( 0.034)	Data  0.000 ( 0.002)	Loss 7.9590e-01 (8.5840e-01)	Acc@1  73.75 ( 69.85)	Acc@5  98.75 ( 97.59)
## e[4] optimizer.zero_grad (sum) time: 0.17056512832641602
## e[4]       loss.backward (sum) time: 4.006414890289307
## e[4]      optimizer.step (sum) time: 1.2251145839691162
## epoch[4] training(only) time: 13.28175973892212
# Switched to evaluate mode...
Test: [  0/100]	Time  0.208 ( 0.208)	Loss 8.5791e-01 (8.5791e-01)	Acc@1  76.00 ( 76.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.014 ( 0.034)	Loss 9.1357e-01 (9.4531e-01)	Acc@1  70.00 ( 68.64)	Acc@5  98.00 ( 97.36)
Test: [ 20/100]	Time  0.017 ( 0.026)	Loss 9.3799e-01 (9.6529e-01)	Acc@1  69.00 ( 67.67)	Acc@5  99.00 ( 97.57)
Test: [ 30/100]	Time  0.016 ( 0.024)	Loss 8.9209e-01 (9.5785e-01)	Acc@1  69.00 ( 68.03)	Acc@5  98.00 ( 97.48)
Test: [ 40/100]	Time  0.022 ( 0.022)	Loss 8.1152e-01 (9.5859e-01)	Acc@1  77.00 ( 67.98)	Acc@5  98.00 ( 97.49)
Test: [ 50/100]	Time  0.021 ( 0.021)	Loss 8.2520e-01 (9.4976e-01)	Acc@1  73.00 ( 68.12)	Acc@5  98.00 ( 97.61)
Test: [ 60/100]	Time  0.016 ( 0.020)	Loss 9.0527e-01 (9.5164e-01)	Acc@1  67.00 ( 68.11)	Acc@5  99.00 ( 97.72)
Test: [ 70/100]	Time  0.022 ( 0.020)	Loss 9.5361e-01 (9.5621e-01)	Acc@1  68.00 ( 67.76)	Acc@5  99.00 ( 97.73)
Test: [ 80/100]	Time  0.016 ( 0.020)	Loss 9.7217e-01 (9.6058e-01)	Acc@1  72.00 ( 67.93)	Acc@5  97.00 ( 97.80)
Test: [ 90/100]	Time  0.020 ( 0.019)	Loss 8.5156e-01 (9.6418e-01)	Acc@1  73.00 ( 67.79)	Acc@5  98.00 ( 97.71)
 * Acc@1 67.890 Acc@5 97.660
### epoch[4] execution time: 15.29888105392456
EPOCH 5
# current learning rate: 0.1
# Switched to train mode...
Epoch: [5][  0/391]	Time  0.221 ( 0.221)	Data  0.174 ( 0.174)	Loss 8.7793e-01 (8.7793e-01)	Acc@1  68.75 ( 68.75)	Acc@5  96.88 ( 96.88)
Epoch: [5][ 10/391]	Time  0.033 ( 0.050)	Data  0.001 ( 0.017)	Loss 7.8320e-01 (7.7468e-01)	Acc@1  72.66 ( 73.15)	Acc@5  95.31 ( 98.15)
Epoch: [5][ 20/391]	Time  0.032 ( 0.042)	Data  0.001 ( 0.010)	Loss 7.7881e-01 (7.7065e-01)	Acc@1  74.22 ( 72.92)	Acc@5  95.31 ( 98.33)
Epoch: [5][ 30/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.007)	Loss 9.7900e-01 (7.6788e-01)	Acc@1  66.41 ( 72.98)	Acc@5  97.66 ( 98.29)
Epoch: [5][ 40/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.006)	Loss 9.2822e-01 (7.7146e-01)	Acc@1  64.84 ( 72.92)	Acc@5  98.44 ( 98.25)
Epoch: [5][ 50/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 7.5781e-01 (7.7026e-01)	Acc@1  73.44 ( 73.12)	Acc@5  95.31 ( 98.21)
Epoch: [5][ 60/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 7.6221e-01 (7.6490e-01)	Acc@1  72.66 ( 73.10)	Acc@5 100.00 ( 98.21)
Epoch: [5][ 70/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 7.4854e-01 (7.6321e-01)	Acc@1  72.66 ( 73.07)	Acc@5  98.44 ( 98.26)
Epoch: [5][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 8.6670e-01 (7.6278e-01)	Acc@1  67.97 ( 73.09)	Acc@5  96.09 ( 98.22)
Epoch: [5][ 90/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 6.9189e-01 (7.6384e-01)	Acc@1  75.00 ( 72.98)	Acc@5  98.44 ( 98.18)
Epoch: [5][100/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 7.3242e-01 (7.6046e-01)	Acc@1  76.56 ( 73.12)	Acc@5  99.22 ( 98.21)
Epoch: [5][110/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 7.9590e-01 (7.5978e-01)	Acc@1  71.09 ( 73.14)	Acc@5  96.09 ( 98.18)
Epoch: [5][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 8.8818e-01 (7.6186e-01)	Acc@1  69.53 ( 73.02)	Acc@5  95.31 ( 98.17)
Epoch: [5][130/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.4355e-01 (7.5489e-01)	Acc@1  72.66 ( 73.17)	Acc@5  99.22 ( 98.22)
Epoch: [5][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 7.4463e-01 (7.5537e-01)	Acc@1  73.44 ( 73.22)	Acc@5  99.22 ( 98.20)
Epoch: [5][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 7.2217e-01 (7.5289e-01)	Acc@1  75.00 ( 73.29)	Acc@5  99.22 ( 98.24)
Epoch: [5][160/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 8.7842e-01 (7.5778e-01)	Acc@1  67.19 ( 73.17)	Acc@5  96.88 ( 98.20)
Epoch: [5][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.6953e-01 (7.5778e-01)	Acc@1  72.66 ( 73.18)	Acc@5 100.00 ( 98.21)
Epoch: [5][180/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.9092e-01 (7.5652e-01)	Acc@1  76.56 ( 73.22)	Acc@5  96.09 ( 98.21)
Epoch: [5][190/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.6699e-01 (7.5397e-01)	Acc@1  83.59 ( 73.31)	Acc@5  96.88 ( 98.22)
Epoch: [5][200/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.4697e-01 (7.5184e-01)	Acc@1  78.12 ( 73.32)	Acc@5  97.66 ( 98.26)
Epoch: [5][210/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.2129e-01 (7.5237e-01)	Acc@1  72.66 ( 73.33)	Acc@5  97.66 ( 98.24)
Epoch: [5][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.5049e-01 (7.4962e-01)	Acc@1  78.12 ( 73.44)	Acc@5  97.66 ( 98.26)
Epoch: [5][230/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.9521e-01 (7.4684e-01)	Acc@1  78.91 ( 73.56)	Acc@5  99.22 ( 98.28)
Epoch: [5][240/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.8213e-01 (7.4631e-01)	Acc@1  70.31 ( 73.54)	Acc@5  98.44 ( 98.28)
Epoch: [5][250/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.2422e-01 (7.4597e-01)	Acc@1  64.06 ( 73.54)	Acc@5 100.00 ( 98.27)
Epoch: [5][260/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.7812e-01 (7.4447e-01)	Acc@1  82.03 ( 73.62)	Acc@5  97.66 ( 98.27)
Epoch: [5][270/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.1426e-01 (7.4274e-01)	Acc@1  76.56 ( 73.66)	Acc@5 100.00 ( 98.28)
Epoch: [5][280/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.0938e-01 (7.4214e-01)	Acc@1  79.69 ( 73.72)	Acc@5  99.22 ( 98.28)
Epoch: [5][290/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.6123e-01 (7.4282e-01)	Acc@1  67.19 ( 73.74)	Acc@5  97.66 ( 98.27)
Epoch: [5][300/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.3438e-01 (7.4092e-01)	Acc@1  72.66 ( 73.83)	Acc@5  99.22 ( 98.27)
Epoch: [5][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.7891e-01 (7.4020e-01)	Acc@1  63.28 ( 73.85)	Acc@5  99.22 ( 98.27)
Epoch: [5][320/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.4277e-01 (7.3768e-01)	Acc@1  71.09 ( 73.95)	Acc@5  99.22 ( 98.29)
Epoch: [5][330/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.2109e-01 (7.3608e-01)	Acc@1  76.56 ( 74.01)	Acc@5  99.22 ( 98.29)
Epoch: [5][340/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.5332e-01 (7.3247e-01)	Acc@1  76.56 ( 74.15)	Acc@5 100.00 ( 98.32)
Epoch: [5][350/391]	Time  0.040 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.1709e-01 (7.3073e-01)	Acc@1  85.16 ( 74.22)	Acc@5 100.00 ( 98.33)
Epoch: [5][360/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.2168e-01 (7.2985e-01)	Acc@1  76.56 ( 74.26)	Acc@5  98.44 ( 98.34)
Epoch: [5][370/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.2715e-01 (7.3084e-01)	Acc@1  71.88 ( 74.21)	Acc@5  97.66 ( 98.34)
Epoch: [5][380/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.8643e-01 (7.2965e-01)	Acc@1  83.59 ( 74.27)	Acc@5  98.44 ( 98.35)
Epoch: [5][390/391]	Time  0.024 ( 0.033)	Data  0.001 ( 0.002)	Loss 6.8652e-01 (7.2945e-01)	Acc@1  76.25 ( 74.29)	Acc@5 100.00 ( 98.34)
## e[5] optimizer.zero_grad (sum) time: 0.16916418075561523
## e[5]       loss.backward (sum) time: 4.036847114562988
## e[5]      optimizer.step (sum) time: 1.217543363571167
## epoch[5] training(only) time: 13.206030130386353
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 5.7031e-01 (5.7031e-01)	Acc@1  81.00 ( 81.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.020 ( 0.030)	Loss 6.3574e-01 (7.1276e-01)	Acc@1  75.00 ( 75.91)	Acc@5  98.00 ( 98.82)
Test: [ 20/100]	Time  0.014 ( 0.024)	Loss 7.1875e-01 (7.4556e-01)	Acc@1  74.00 ( 74.71)	Acc@5  99.00 ( 98.38)
Test: [ 30/100]	Time  0.014 ( 0.022)	Loss 7.2314e-01 (7.5329e-01)	Acc@1  79.00 ( 74.68)	Acc@5 100.00 ( 98.10)
Test: [ 40/100]	Time  0.014 ( 0.020)	Loss 6.9727e-01 (7.5924e-01)	Acc@1  79.00 ( 74.56)	Acc@5  99.00 ( 98.12)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 6.3574e-01 (7.5623e-01)	Acc@1  81.00 ( 74.57)	Acc@5  99.00 ( 98.22)
Test: [ 60/100]	Time  0.014 ( 0.019)	Loss 7.4561e-01 (7.5780e-01)	Acc@1  71.00 ( 74.36)	Acc@5  98.00 ( 98.25)
Test: [ 70/100]	Time  0.015 ( 0.019)	Loss 6.9678e-01 (7.5858e-01)	Acc@1  74.00 ( 74.14)	Acc@5  99.00 ( 98.27)
Test: [ 80/100]	Time  0.026 ( 0.019)	Loss 7.1582e-01 (7.6207e-01)	Acc@1  74.00 ( 73.90)	Acc@5  97.00 ( 98.28)
Test: [ 90/100]	Time  0.016 ( 0.018)	Loss 7.0850e-01 (7.6751e-01)	Acc@1  75.00 ( 73.63)	Acc@5  99.00 ( 98.25)
 * Acc@1 73.680 Acc@5 98.250
### epoch[5] execution time: 15.160164833068848
EPOCH 6
# current learning rate: 0.1
# Switched to train mode...
Epoch: [6][  0/391]	Time  0.216 ( 0.216)	Data  0.181 ( 0.181)	Loss 5.8984e-01 (5.8984e-01)	Acc@1  79.69 ( 79.69)	Acc@5  98.44 ( 98.44)
Epoch: [6][ 10/391]	Time  0.033 ( 0.051)	Data  0.001 ( 0.018)	Loss 7.5830e-01 (6.4893e-01)	Acc@1  77.34 ( 78.91)	Acc@5  97.66 ( 98.37)
Epoch: [6][ 20/391]	Time  0.033 ( 0.042)	Data  0.001 ( 0.010)	Loss 5.5615e-01 (6.6483e-01)	Acc@1  78.91 ( 77.46)	Acc@5  99.22 ( 98.51)
Epoch: [6][ 30/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.007)	Loss 6.7139e-01 (6.5422e-01)	Acc@1  75.00 ( 77.65)	Acc@5 100.00 ( 98.61)
Epoch: [6][ 40/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.006)	Loss 5.3809e-01 (6.5807e-01)	Acc@1  83.59 ( 77.38)	Acc@5  98.44 ( 98.70)
Epoch: [6][ 50/391]	Time  0.036 ( 0.036)	Data  0.004 ( 0.005)	Loss 6.3770e-01 (6.5990e-01)	Acc@1  78.12 ( 77.21)	Acc@5  98.44 ( 98.73)
Epoch: [6][ 60/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.005)	Loss 5.8252e-01 (6.5673e-01)	Acc@1  78.91 ( 77.23)	Acc@5 100.00 ( 98.71)
Epoch: [6][ 70/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 6.7041e-01 (6.5596e-01)	Acc@1  75.78 ( 77.13)	Acc@5  96.88 ( 98.68)
Epoch: [6][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 7.0117e-01 (6.5547e-01)	Acc@1  73.44 ( 77.21)	Acc@5  99.22 ( 98.71)
Epoch: [6][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 6.2549e-01 (6.5802e-01)	Acc@1  76.56 ( 77.14)	Acc@5 100.00 ( 98.70)
Epoch: [6][100/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 5.7471e-01 (6.5422e-01)	Acc@1  79.69 ( 77.16)	Acc@5  97.66 ( 98.76)
Epoch: [6][110/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 5.4834e-01 (6.5012e-01)	Acc@1  78.12 ( 77.40)	Acc@5  97.66 ( 98.76)
Epoch: [6][120/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.1475e-01 (6.5216e-01)	Acc@1  80.47 ( 77.40)	Acc@5 100.00 ( 98.74)
Epoch: [6][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.4688e-01 (6.5296e-01)	Acc@1  82.03 ( 77.35)	Acc@5  98.44 ( 98.74)
Epoch: [6][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 7.9932e-01 (6.5341e-01)	Acc@1  73.44 ( 77.39)	Acc@5  98.44 ( 98.71)
Epoch: [6][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.4648e-01 (6.5217e-01)	Acc@1  71.88 ( 77.39)	Acc@5 100.00 ( 98.74)
Epoch: [6][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 7.5684e-01 (6.4805e-01)	Acc@1  72.66 ( 77.51)	Acc@5 100.00 ( 98.79)
Epoch: [6][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.9658e-01 (6.4595e-01)	Acc@1  82.03 ( 77.58)	Acc@5 100.00 ( 98.81)
Epoch: [6][180/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.6895e-01 (6.4460e-01)	Acc@1  75.00 ( 77.61)	Acc@5  97.66 ( 98.81)
Epoch: [6][190/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.0762e-01 (6.4609e-01)	Acc@1  75.00 ( 77.61)	Acc@5  98.44 ( 98.81)
Epoch: [6][200/391]	Time  0.035 ( 0.034)	Data  0.000 ( 0.002)	Loss 5.8105e-01 (6.4548e-01)	Acc@1  82.81 ( 77.65)	Acc@5  99.22 ( 98.81)
Epoch: [6][210/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.5273e-01 (6.4460e-01)	Acc@1  81.25 ( 77.66)	Acc@5  99.22 ( 98.82)
Epoch: [6][220/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.4365e-01 (6.4303e-01)	Acc@1  75.00 ( 77.65)	Acc@5  96.09 ( 98.84)
Epoch: [6][230/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.9082e-01 (6.4088e-01)	Acc@1  78.12 ( 77.73)	Acc@5  98.44 ( 98.82)
Epoch: [6][240/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.5625e-01 (6.4095e-01)	Acc@1  81.25 ( 77.73)	Acc@5  98.44 ( 98.83)
Epoch: [6][250/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.1045e-01 (6.4096e-01)	Acc@1  74.22 ( 77.70)	Acc@5  97.66 ( 98.84)
Epoch: [6][260/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.6260e-01 (6.4099e-01)	Acc@1  76.56 ( 77.70)	Acc@5  99.22 ( 98.84)
Epoch: [6][270/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.7803e-01 (6.3946e-01)	Acc@1  84.38 ( 77.76)	Acc@5  98.44 ( 98.83)
Epoch: [6][280/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.5723e-01 (6.3773e-01)	Acc@1  78.91 ( 77.82)	Acc@5  98.44 ( 98.82)
Epoch: [6][290/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.9541e-01 (6.3777e-01)	Acc@1  74.22 ( 77.85)	Acc@5  98.44 ( 98.81)
Epoch: [6][300/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.2207e-01 (6.3731e-01)	Acc@1  81.25 ( 77.89)	Acc@5  99.22 ( 98.79)
Epoch: [6][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.0264e-01 (6.3853e-01)	Acc@1  74.22 ( 77.84)	Acc@5  97.66 ( 98.78)
Epoch: [6][320/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.3818e-01 (6.3833e-01)	Acc@1  76.56 ( 77.84)	Acc@5  99.22 ( 98.78)
Epoch: [6][330/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.0449e-01 (6.3727e-01)	Acc@1  78.91 ( 77.86)	Acc@5  99.22 ( 98.79)
Epoch: [6][340/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.6738e-01 (6.3674e-01)	Acc@1  82.03 ( 77.85)	Acc@5  98.44 ( 98.78)
Epoch: [6][350/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.4053e-01 (6.3442e-01)	Acc@1  84.38 ( 77.94)	Acc@5  98.44 ( 98.79)
Epoch: [6][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.7041e-01 (6.3323e-01)	Acc@1  76.56 ( 78.00)	Acc@5  99.22 ( 98.79)
Epoch: [6][370/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.1768e-01 (6.3284e-01)	Acc@1  79.69 ( 78.00)	Acc@5  98.44 ( 98.80)
Epoch: [6][380/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.8789e-01 (6.3219e-01)	Acc@1  75.78 ( 78.03)	Acc@5 100.00 ( 98.80)
Epoch: [6][390/391]	Time  0.024 ( 0.033)	Data  0.001 ( 0.002)	Loss 9.2578e-01 (6.3103e-01)	Acc@1  72.50 ( 78.08)	Acc@5  95.00 ( 98.80)
## e[6] optimizer.zero_grad (sum) time: 0.1697063446044922
## e[6]       loss.backward (sum) time: 4.110069751739502
## e[6]      optimizer.step (sum) time: 1.2047700881958008
## epoch[6] training(only) time: 13.213841438293457
# Switched to evaluate mode...
Test: [  0/100]	Time  0.162 ( 0.162)	Loss 6.8359e-01 (6.8359e-01)	Acc@1  75.00 ( 75.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.016 ( 0.030)	Loss 7.0410e-01 (7.1105e-01)	Acc@1  79.00 ( 76.64)	Acc@5  97.00 ( 98.64)
Test: [ 20/100]	Time  0.014 ( 0.024)	Loss 7.9395e-01 (7.1590e-01)	Acc@1  72.00 ( 75.95)	Acc@5  99.00 ( 98.57)
Test: [ 30/100]	Time  0.014 ( 0.022)	Loss 6.6162e-01 (7.2359e-01)	Acc@1  77.00 ( 75.97)	Acc@5  98.00 ( 98.45)
Test: [ 40/100]	Time  0.014 ( 0.020)	Loss 6.1328e-01 (7.1991e-01)	Acc@1  80.00 ( 75.71)	Acc@5  99.00 ( 98.49)
Test: [ 50/100]	Time  0.013 ( 0.020)	Loss 6.1572e-01 (7.0744e-01)	Acc@1  79.00 ( 76.04)	Acc@5  99.00 ( 98.61)
Test: [ 60/100]	Time  0.018 ( 0.019)	Loss 6.7041e-01 (7.0552e-01)	Acc@1  80.00 ( 76.28)	Acc@5  99.00 ( 98.67)
Test: [ 70/100]	Time  0.018 ( 0.019)	Loss 8.5449e-01 (7.0716e-01)	Acc@1  74.00 ( 76.18)	Acc@5  98.00 ( 98.72)
Test: [ 80/100]	Time  0.014 ( 0.019)	Loss 7.0312e-01 (7.0505e-01)	Acc@1  77.00 ( 76.27)	Acc@5  99.00 ( 98.73)
Test: [ 90/100]	Time  0.017 ( 0.018)	Loss 7.1631e-01 (7.0876e-01)	Acc@1  77.00 ( 76.18)	Acc@5 100.00 ( 98.75)
 * Acc@1 76.370 Acc@5 98.750
### epoch[6] execution time: 15.156610012054443
EPOCH 7
# current learning rate: 0.1
# Switched to train mode...
Epoch: [7][  0/391]	Time  0.220 ( 0.220)	Data  0.185 ( 0.185)	Loss 5.0781e-01 (5.0781e-01)	Acc@1  78.91 ( 78.91)	Acc@5  99.22 ( 99.22)
Epoch: [7][ 10/391]	Time  0.035 ( 0.050)	Data  0.001 ( 0.018)	Loss 4.7534e-01 (6.1879e-01)	Acc@1  81.25 ( 78.05)	Acc@5 100.00 ( 99.08)
Epoch: [7][ 20/391]	Time  0.032 ( 0.042)	Data  0.001 ( 0.010)	Loss 5.0146e-01 (6.1249e-01)	Acc@1  85.16 ( 78.01)	Acc@5  99.22 ( 99.00)
Epoch: [7][ 30/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.007)	Loss 4.9976e-01 (5.9263e-01)	Acc@1  84.38 ( 79.11)	Acc@5  99.22 ( 99.02)
Epoch: [7][ 40/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.006)	Loss 7.4854e-01 (6.0380e-01)	Acc@1  78.91 ( 79.13)	Acc@5  96.09 ( 98.91)
Epoch: [7][ 50/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.005)	Loss 4.6338e-01 (5.9847e-01)	Acc@1  85.16 ( 79.30)	Acc@5  99.22 ( 98.91)
Epoch: [7][ 60/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.004)	Loss 7.0703e-01 (5.9727e-01)	Acc@1  79.69 ( 79.51)	Acc@5  96.88 ( 98.86)
Epoch: [7][ 70/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 6.7676e-01 (5.9854e-01)	Acc@1  78.12 ( 79.52)	Acc@5  97.66 ( 98.80)
Epoch: [7][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 6.5381e-01 (5.9741e-01)	Acc@1  78.91 ( 79.64)	Acc@5  99.22 ( 98.83)
Epoch: [7][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 6.8457e-01 (5.9207e-01)	Acc@1  73.44 ( 79.96)	Acc@5  99.22 ( 98.87)
Epoch: [7][100/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.7031e-01 (5.9091e-01)	Acc@1  78.91 ( 79.91)	Acc@5  98.44 ( 98.89)
Epoch: [7][110/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.5605e-01 (5.8958e-01)	Acc@1  84.38 ( 79.84)	Acc@5  97.66 ( 98.88)
Epoch: [7][120/391]	Time  0.039 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.7666e-01 (5.8483e-01)	Acc@1  78.91 ( 80.00)	Acc@5  98.44 ( 98.91)
Epoch: [7][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.1230e-01 (5.8427e-01)	Acc@1  82.03 ( 80.04)	Acc@5  97.66 ( 98.90)
Epoch: [7][140/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.7412e-01 (5.8105e-01)	Acc@1  82.03 ( 80.03)	Acc@5 100.00 ( 98.92)
Epoch: [7][150/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 7.3535e-01 (5.7948e-01)	Acc@1  72.66 ( 80.10)	Acc@5  98.44 ( 98.92)
Epoch: [7][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.2686e-01 (5.7814e-01)	Acc@1  79.69 ( 80.09)	Acc@5 100.00 ( 98.96)
Epoch: [7][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.1729e-01 (5.7626e-01)	Acc@1  75.00 ( 80.14)	Acc@5  97.66 ( 98.97)
Epoch: [7][180/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.6748e-01 (5.7461e-01)	Acc@1  77.34 ( 80.19)	Acc@5 100.00 ( 99.01)
Epoch: [7][190/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.8252e-01 (5.7540e-01)	Acc@1  81.25 ( 80.16)	Acc@5  99.22 ( 99.02)
Epoch: [7][200/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.3750e-01 (5.7381e-01)	Acc@1  83.59 ( 80.25)	Acc@5 100.00 ( 99.02)
Epoch: [7][210/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.5142e-01 (5.7211e-01)	Acc@1  85.16 ( 80.32)	Acc@5  99.22 ( 99.02)
Epoch: [7][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.6152e-01 (5.7174e-01)	Acc@1  79.69 ( 80.33)	Acc@5 100.00 ( 99.02)
Epoch: [7][230/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.0596e-01 (5.7198e-01)	Acc@1  79.69 ( 80.28)	Acc@5  97.66 ( 99.02)
Epoch: [7][240/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.4980e-01 (5.7295e-01)	Acc@1  83.59 ( 80.26)	Acc@5 100.00 ( 99.02)
Epoch: [7][250/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.9717e-01 (5.7163e-01)	Acc@1  82.81 ( 80.31)	Acc@5  98.44 ( 99.03)
Epoch: [7][260/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.9678e-01 (5.7169e-01)	Acc@1  76.56 ( 80.34)	Acc@5  97.66 ( 99.02)
Epoch: [7][270/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.4893e-01 (5.7186e-01)	Acc@1  79.69 ( 80.33)	Acc@5  98.44 ( 99.00)
Epoch: [7][280/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.8999e-01 (5.7129e-01)	Acc@1  81.25 ( 80.35)	Acc@5 100.00 ( 99.00)
Epoch: [7][290/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.8555e-01 (5.7179e-01)	Acc@1  80.47 ( 80.35)	Acc@5  96.88 ( 98.99)
Epoch: [7][300/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.0596e-01 (5.7097e-01)	Acc@1  78.91 ( 80.39)	Acc@5 100.00 ( 98.99)
Epoch: [7][310/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.6201e-01 (5.7153e-01)	Acc@1  83.59 ( 80.34)	Acc@5  99.22 ( 98.99)
Epoch: [7][320/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.3916e-01 (5.7262e-01)	Acc@1  76.56 ( 80.32)	Acc@5 100.00 ( 98.98)
Epoch: [7][330/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.3320e-01 (5.7251e-01)	Acc@1  77.34 ( 80.35)	Acc@5 100.00 ( 98.99)
Epoch: [7][340/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.2109e-01 (5.7153e-01)	Acc@1  75.00 ( 80.36)	Acc@5  98.44 ( 98.99)
Epoch: [7][350/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.6289e-01 (5.7183e-01)	Acc@1  83.59 ( 80.32)	Acc@5  98.44 ( 98.98)
Epoch: [7][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.3916e-01 (5.7182e-01)	Acc@1  75.78 ( 80.31)	Acc@5  99.22 ( 98.98)
Epoch: [7][370/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 6.4697e-01 (5.7118e-01)	Acc@1  75.00 ( 80.31)	Acc@5  98.44 ( 98.98)
Epoch: [7][380/391]	Time  0.033 ( 0.033)	Data  0.000 ( 0.002)	Loss 6.1426e-01 (5.7071e-01)	Acc@1  78.12 ( 80.32)	Acc@5 100.00 ( 98.99)
Epoch: [7][390/391]	Time  0.022 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.7256e-01 (5.6936e-01)	Acc@1  87.50 ( 80.36)	Acc@5  98.75 ( 98.99)
## e[7] optimizer.zero_grad (sum) time: 0.1692790985107422
## e[7]       loss.backward (sum) time: 4.0927464962005615
## e[7]      optimizer.step (sum) time: 1.2190582752227783
## epoch[7] training(only) time: 13.202717542648315
# Switched to evaluate mode...
Test: [  0/100]	Time  0.165 ( 0.165)	Loss 6.6211e-01 (6.6211e-01)	Acc@1  75.00 ( 75.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.015 ( 0.030)	Loss 6.2500e-01 (6.7057e-01)	Acc@1  78.00 ( 76.45)	Acc@5  99.00 ( 98.91)
Test: [ 20/100]	Time  0.024 ( 0.024)	Loss 8.0078e-01 (6.6779e-01)	Acc@1  73.00 ( 76.62)	Acc@5  98.00 ( 98.67)
Test: [ 30/100]	Time  0.016 ( 0.022)	Loss 6.4453e-01 (6.7700e-01)	Acc@1  76.00 ( 76.74)	Acc@5  97.00 ( 98.45)
Test: [ 40/100]	Time  0.016 ( 0.021)	Loss 7.7344e-01 (6.8107e-01)	Acc@1  79.00 ( 76.73)	Acc@5  99.00 ( 98.56)
Test: [ 50/100]	Time  0.018 ( 0.020)	Loss 5.8398e-01 (6.8032e-01)	Acc@1  85.00 ( 77.14)	Acc@5  98.00 ( 98.61)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 5.7666e-01 (6.8476e-01)	Acc@1  77.00 ( 77.00)	Acc@5  99.00 ( 98.56)
Test: [ 70/100]	Time  0.018 ( 0.019)	Loss 6.5527e-01 (6.9323e-01)	Acc@1  78.00 ( 76.89)	Acc@5  99.00 ( 98.62)
Test: [ 80/100]	Time  0.014 ( 0.019)	Loss 5.7861e-01 (6.9060e-01)	Acc@1  78.00 ( 76.86)	Acc@5 100.00 ( 98.72)
Test: [ 90/100]	Time  0.021 ( 0.019)	Loss 5.2637e-01 (6.9513e-01)	Acc@1  80.00 ( 76.80)	Acc@5 100.00 ( 98.78)
 * Acc@1 76.860 Acc@5 98.790
### epoch[7] execution time: 15.173923969268799
EPOCH 8
# current learning rate: 0.1
# Switched to train mode...
Epoch: [8][  0/391]	Time  0.213 ( 0.213)	Data  0.176 ( 0.176)	Loss 6.7773e-01 (6.7773e-01)	Acc@1  78.91 ( 78.91)	Acc@5  99.22 ( 99.22)
Epoch: [8][ 10/391]	Time  0.032 ( 0.050)	Data  0.001 ( 0.017)	Loss 4.2651e-01 (5.2563e-01)	Acc@1  83.59 ( 81.25)	Acc@5 100.00 ( 99.22)
Epoch: [8][ 20/391]	Time  0.032 ( 0.042)	Data  0.001 ( 0.010)	Loss 6.3184e-01 (5.3405e-01)	Acc@1  74.22 ( 81.55)	Acc@5  99.22 ( 99.18)
Epoch: [8][ 30/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.007)	Loss 5.0879e-01 (5.3779e-01)	Acc@1  82.03 ( 80.90)	Acc@5  97.66 ( 99.14)
Epoch: [8][ 40/391]	Time  0.032 ( 0.038)	Data  0.001 ( 0.006)	Loss 3.6719e-01 (5.3195e-01)	Acc@1  84.38 ( 81.17)	Acc@5 100.00 ( 99.22)
Epoch: [8][ 50/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 4.8315e-01 (5.2889e-01)	Acc@1  82.81 ( 81.40)	Acc@5 100.00 ( 99.25)
Epoch: [8][ 60/391]	Time  0.029 ( 0.036)	Data  0.001 ( 0.004)	Loss 5.1709e-01 (5.2330e-01)	Acc@1  82.03 ( 81.62)	Acc@5  99.22 ( 99.28)
Epoch: [8][ 70/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 5.5762e-01 (5.2133e-01)	Acc@1  82.81 ( 81.73)	Acc@5 100.00 ( 99.26)
Epoch: [8][ 80/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 6.4502e-01 (5.2821e-01)	Acc@1  79.69 ( 81.53)	Acc@5  99.22 ( 99.24)
Epoch: [8][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 4.7632e-01 (5.2603e-01)	Acc@1  85.16 ( 81.71)	Acc@5 100.00 ( 99.27)
Epoch: [8][100/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 4.4434e-01 (5.2482e-01)	Acc@1  85.16 ( 81.85)	Acc@5  99.22 ( 99.27)
Epoch: [8][110/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 4.1113e-01 (5.1702e-01)	Acc@1  86.72 ( 82.16)	Acc@5 100.00 ( 99.30)
Epoch: [8][120/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 4.9805e-01 (5.1586e-01)	Acc@1  84.38 ( 82.17)	Acc@5  98.44 ( 99.26)
Epoch: [8][130/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.5957e-01 (5.1819e-01)	Acc@1  85.94 ( 82.13)	Acc@5  99.22 ( 99.25)
Epoch: [8][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.6689e-01 (5.1763e-01)	Acc@1  82.81 ( 82.20)	Acc@5  98.44 ( 99.25)
Epoch: [8][150/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.2944e-01 (5.1923e-01)	Acc@1  85.94 ( 82.12)	Acc@5 100.00 ( 99.22)
Epoch: [8][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.9722e-01 (5.1917e-01)	Acc@1  87.50 ( 82.08)	Acc@5 100.00 ( 99.22)
Epoch: [8][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.6948e-01 (5.2086e-01)	Acc@1  83.59 ( 82.05)	Acc@5  99.22 ( 99.20)
Epoch: [8][180/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.8853e-01 (5.2057e-01)	Acc@1  82.81 ( 82.07)	Acc@5 100.00 ( 99.18)
Epoch: [8][190/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.4980e-01 (5.1986e-01)	Acc@1  80.47 ( 82.04)	Acc@5  99.22 ( 99.18)
Epoch: [8][200/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.4648e-01 (5.2171e-01)	Acc@1  77.34 ( 82.02)	Acc@5  98.44 ( 99.16)
Epoch: [8][210/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.3027e-01 (5.2028e-01)	Acc@1  82.81 ( 82.09)	Acc@5  99.22 ( 99.15)
Epoch: [8][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.8984e-01 (5.1927e-01)	Acc@1  78.12 ( 82.15)	Acc@5 100.00 ( 99.15)
Epoch: [8][230/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.0454e-01 (5.2074e-01)	Acc@1  85.16 ( 82.13)	Acc@5 100.00 ( 99.16)
Epoch: [8][240/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.1187e-01 (5.1982e-01)	Acc@1  84.38 ( 82.13)	Acc@5 100.00 ( 99.16)
Epoch: [8][250/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.1621e-01 (5.2022e-01)	Acc@1  78.91 ( 82.12)	Acc@5  98.44 ( 99.18)
Epoch: [8][260/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.8779e-01 (5.1862e-01)	Acc@1  80.47 ( 82.16)	Acc@5  99.22 ( 99.19)
Epoch: [8][270/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.0010e-01 (5.1852e-01)	Acc@1  77.34 ( 82.18)	Acc@5  99.22 ( 99.19)
Epoch: [8][280/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.1025e-01 (5.1944e-01)	Acc@1  82.03 ( 82.13)	Acc@5  97.66 ( 99.19)
Epoch: [8][290/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.8486e-01 (5.1873e-01)	Acc@1  84.38 ( 82.16)	Acc@5  99.22 ( 99.20)
Epoch: [8][300/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.4521e-01 (5.1865e-01)	Acc@1  88.28 ( 82.18)	Acc@5  99.22 ( 99.19)
Epoch: [8][310/391]	Time  0.044 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.6631e-01 (5.1905e-01)	Acc@1  83.59 ( 82.18)	Acc@5  99.22 ( 99.19)
Epoch: [8][320/391]	Time  0.042 ( 0.034)	Data  0.007 ( 0.002)	Loss 4.6851e-01 (5.1865e-01)	Acc@1  83.59 ( 82.19)	Acc@5 100.00 ( 99.19)
Epoch: [8][330/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.5322e-01 (5.1718e-01)	Acc@1  77.34 ( 82.19)	Acc@5  99.22 ( 99.19)
Epoch: [8][340/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.7388e-01 (5.1586e-01)	Acc@1  82.03 ( 82.23)	Acc@5  99.22 ( 99.19)
Epoch: [8][350/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.8779e-01 (5.1595e-01)	Acc@1  84.38 ( 82.23)	Acc@5  99.22 ( 99.19)
Epoch: [8][360/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.3174e-01 (5.1499e-01)	Acc@1  82.03 ( 82.26)	Acc@5  98.44 ( 99.19)
Epoch: [8][370/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.1431e-01 (5.1469e-01)	Acc@1  84.38 ( 82.26)	Acc@5 100.00 ( 99.20)
Epoch: [8][380/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.2637e-01 (5.1558e-01)	Acc@1  78.12 ( 82.21)	Acc@5  99.22 ( 99.20)
Epoch: [8][390/391]	Time  0.023 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.5122e-01 (5.1518e-01)	Acc@1  93.75 ( 82.23)	Acc@5 100.00 ( 99.20)
## e[8] optimizer.zero_grad (sum) time: 0.16927361488342285
## e[8]       loss.backward (sum) time: 4.012934684753418
## e[8]      optimizer.step (sum) time: 1.230893611907959
## epoch[8] training(only) time: 13.244258642196655
# Switched to evaluate mode...
Test: [  0/100]	Time  0.174 ( 0.174)	Loss 5.0635e-01 (5.0635e-01)	Acc@1  84.00 ( 84.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.018 ( 0.031)	Loss 4.4800e-01 (4.7157e-01)	Acc@1  82.00 ( 82.73)	Acc@5  99.00 ( 99.18)
Test: [ 20/100]	Time  0.022 ( 0.025)	Loss 5.4248e-01 (4.8593e-01)	Acc@1  81.00 ( 82.71)	Acc@5 100.00 ( 99.10)
Test: [ 30/100]	Time  0.015 ( 0.022)	Loss 5.0781e-01 (4.9283e-01)	Acc@1  85.00 ( 83.10)	Acc@5  98.00 ( 99.13)
Test: [ 40/100]	Time  0.016 ( 0.021)	Loss 5.2148e-01 (4.9778e-01)	Acc@1  83.00 ( 82.88)	Acc@5  99.00 ( 99.12)
Test: [ 50/100]	Time  0.021 ( 0.020)	Loss 4.7144e-01 (4.9738e-01)	Acc@1  87.00 ( 83.08)	Acc@5  99.00 ( 99.14)
Test: [ 60/100]	Time  0.018 ( 0.019)	Loss 4.0527e-01 (4.9934e-01)	Acc@1  87.00 ( 83.03)	Acc@5 100.00 ( 99.20)
Test: [ 70/100]	Time  0.016 ( 0.019)	Loss 6.8311e-01 (5.0050e-01)	Acc@1  76.00 ( 82.93)	Acc@5  99.00 ( 99.21)
Test: [ 80/100]	Time  0.018 ( 0.019)	Loss 5.5127e-01 (5.0199e-01)	Acc@1  81.00 ( 82.81)	Acc@5  99.00 ( 99.22)
Test: [ 90/100]	Time  0.016 ( 0.018)	Loss 3.5034e-01 (5.0097e-01)	Acc@1  91.00 ( 82.85)	Acc@5 100.00 ( 99.21)
 * Acc@1 82.950 Acc@5 99.180
### epoch[8] execution time: 15.185839891433716
EPOCH 9
# current learning rate: 0.1
# Switched to train mode...
Epoch: [9][  0/391]	Time  0.215 ( 0.215)	Data  0.181 ( 0.181)	Loss 4.1479e-01 (4.1479e-01)	Acc@1  85.16 ( 85.16)	Acc@5  99.22 ( 99.22)
Epoch: [9][ 10/391]	Time  0.033 ( 0.051)	Data  0.001 ( 0.018)	Loss 4.9023e-01 (4.6014e-01)	Acc@1  82.81 ( 83.24)	Acc@5  99.22 ( 99.50)
Epoch: [9][ 20/391]	Time  0.033 ( 0.042)	Data  0.001 ( 0.010)	Loss 4.9023e-01 (4.5826e-01)	Acc@1  78.91 ( 83.48)	Acc@5  99.22 ( 99.33)
Epoch: [9][ 30/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.007)	Loss 4.0405e-01 (4.5457e-01)	Acc@1  85.94 ( 83.74)	Acc@5  98.44 ( 99.37)
Epoch: [9][ 40/391]	Time  0.031 ( 0.037)	Data  0.001 ( 0.006)	Loss 4.5361e-01 (4.6539e-01)	Acc@1  84.38 ( 83.40)	Acc@5  99.22 ( 99.31)
Epoch: [9][ 50/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 4.9976e-01 (4.7395e-01)	Acc@1  82.81 ( 83.43)	Acc@5  99.22 ( 99.28)
Epoch: [9][ 60/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 4.8267e-01 (4.7598e-01)	Acc@1  82.03 ( 83.26)	Acc@5 100.00 ( 99.26)
Epoch: [9][ 70/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.004)	Loss 3.2178e-01 (4.7249e-01)	Acc@1  88.28 ( 83.31)	Acc@5  99.22 ( 99.26)
Epoch: [9][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 5.7031e-01 (4.7896e-01)	Acc@1  80.47 ( 83.06)	Acc@5 100.00 ( 99.27)
Epoch: [9][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 4.3042e-01 (4.8067e-01)	Acc@1  82.81 ( 82.97)	Acc@5 100.00 ( 99.29)
Epoch: [9][100/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 3.8647e-01 (4.7942e-01)	Acc@1  85.94 ( 83.10)	Acc@5 100.00 ( 99.30)
Epoch: [9][110/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 5.0098e-01 (4.7469e-01)	Acc@1  84.38 ( 83.31)	Acc@5 100.00 ( 99.32)
Epoch: [9][120/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 7.0605e-01 (4.7996e-01)	Acc@1  76.56 ( 83.20)	Acc@5  96.09 ( 99.26)
Epoch: [9][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.9136e-01 (4.7720e-01)	Acc@1  85.94 ( 83.28)	Acc@5 100.00 ( 99.31)
Epoch: [9][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.5508e-01 (4.7366e-01)	Acc@1  80.47 ( 83.40)	Acc@5 100.00 ( 99.29)
Epoch: [9][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.6768e-01 (4.7181e-01)	Acc@1  90.62 ( 83.50)	Acc@5 100.00 ( 99.29)
Epoch: [9][160/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.4395e-01 (4.6986e-01)	Acc@1  82.03 ( 83.61)	Acc@5  96.88 ( 99.29)
Epoch: [9][170/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.7656e-01 (4.7101e-01)	Acc@1  82.03 ( 83.56)	Acc@5 100.00 ( 99.26)
Epoch: [9][180/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.9927e-01 (4.7061e-01)	Acc@1  81.25 ( 83.57)	Acc@5  98.44 ( 99.25)
Epoch: [9][190/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.8535e-01 (4.6961e-01)	Acc@1  80.47 ( 83.60)	Acc@5  99.22 ( 99.26)
Epoch: [9][200/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.5371e-01 (4.7016e-01)	Acc@1  85.94 ( 83.58)	Acc@5 100.00 ( 99.28)
Epoch: [9][210/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.0830e-01 (4.7116e-01)	Acc@1  85.94 ( 83.59)	Acc@5  99.22 ( 99.29)
Epoch: [9][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.5664e-01 (4.7194e-01)	Acc@1  80.47 ( 83.55)	Acc@5  99.22 ( 99.28)
Epoch: [9][230/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.5132e-01 (4.7276e-01)	Acc@1  86.72 ( 83.52)	Acc@5 100.00 ( 99.28)
Epoch: [9][240/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.1431e-01 (4.7180e-01)	Acc@1  85.94 ( 83.54)	Acc@5 100.00 ( 99.29)
Epoch: [9][250/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.0942e-01 (4.7231e-01)	Acc@1  86.72 ( 83.58)	Acc@5  99.22 ( 99.28)
Epoch: [9][260/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.2393e-01 (4.7257e-01)	Acc@1  79.69 ( 83.58)	Acc@5  99.22 ( 99.28)
Epoch: [9][270/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.4297e-01 (4.7389e-01)	Acc@1  81.25 ( 83.52)	Acc@5 100.00 ( 99.29)
Epoch: [9][280/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.5020e-01 (4.7466e-01)	Acc@1  85.16 ( 83.50)	Acc@5  99.22 ( 99.29)
Epoch: [9][290/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.3516e-01 (4.7485e-01)	Acc@1  81.25 ( 83.47)	Acc@5  98.44 ( 99.29)
Epoch: [9][300/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.9761e-01 (4.7375e-01)	Acc@1  90.62 ( 83.51)	Acc@5  99.22 ( 99.29)
Epoch: [9][310/391]	Time  0.044 ( 0.034)	Data  0.012 ( 0.002)	Loss 4.0747e-01 (4.7416e-01)	Acc@1  87.50 ( 83.47)	Acc@5 100.00 ( 99.29)
Epoch: [9][320/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.4946e-01 (4.7339e-01)	Acc@1  86.72 ( 83.49)	Acc@5  98.44 ( 99.29)
Epoch: [9][330/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.7461e-01 (4.7267e-01)	Acc@1  78.91 ( 83.49)	Acc@5 100.00 ( 99.31)
Epoch: [9][340/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.6118e-01 (4.7303e-01)	Acc@1  89.06 ( 83.51)	Acc@5 100.00 ( 99.31)
Epoch: [9][350/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.2256e-01 (4.7452e-01)	Acc@1  74.22 ( 83.46)	Acc@5  99.22 ( 99.29)
Epoch: [9][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.3818e-01 (4.7502e-01)	Acc@1  76.56 ( 83.44)	Acc@5  98.44 ( 99.28)
Epoch: [9][370/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.5898e-01 (4.7438e-01)	Acc@1  82.03 ( 83.47)	Acc@5  99.22 ( 99.29)
Epoch: [9][380/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.7754e-01 (4.7376e-01)	Acc@1  79.69 ( 83.47)	Acc@5  98.44 ( 99.29)
Epoch: [9][390/391]	Time  0.024 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.4883e-01 (4.7382e-01)	Acc@1  80.00 ( 83.47)	Acc@5 100.00 ( 99.28)
## e[9] optimizer.zero_grad (sum) time: 0.1705617904663086
## e[9]       loss.backward (sum) time: 4.009249210357666
## e[9]      optimizer.step (sum) time: 1.211993932723999
## epoch[9] training(only) time: 13.239598512649536
# Switched to evaluate mode...
Test: [  0/100]	Time  0.173 ( 0.173)	Loss 5.1709e-01 (5.1709e-01)	Acc@1  81.00 ( 81.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.021 ( 0.031)	Loss 6.4648e-01 (5.7149e-01)	Acc@1  79.00 ( 80.18)	Acc@5  98.00 ( 98.55)
Test: [ 20/100]	Time  0.019 ( 0.025)	Loss 6.6113e-01 (6.0476e-01)	Acc@1  77.00 ( 79.14)	Acc@5 100.00 ( 98.67)
Test: [ 30/100]	Time  0.014 ( 0.022)	Loss 7.1436e-01 (6.1794e-01)	Acc@1  73.00 ( 79.03)	Acc@5  98.00 ( 98.74)
Test: [ 40/100]	Time  0.014 ( 0.020)	Loss 6.1279e-01 (6.1504e-01)	Acc@1  81.00 ( 79.27)	Acc@5  99.00 ( 98.90)
Test: [ 50/100]	Time  0.016 ( 0.020)	Loss 4.6973e-01 (6.0785e-01)	Acc@1  85.00 ( 79.57)	Acc@5  98.00 ( 98.82)
Test: [ 60/100]	Time  0.014 ( 0.019)	Loss 6.2354e-01 (6.0863e-01)	Acc@1  74.00 ( 79.52)	Acc@5 100.00 ( 98.89)
Test: [ 70/100]	Time  0.015 ( 0.019)	Loss 6.2256e-01 (6.0581e-01)	Acc@1  84.00 ( 79.58)	Acc@5  99.00 ( 98.89)
Test: [ 80/100]	Time  0.014 ( 0.019)	Loss 5.7910e-01 (6.0292e-01)	Acc@1  80.00 ( 79.69)	Acc@5  99.00 ( 98.91)
Test: [ 90/100]	Time  0.015 ( 0.018)	Loss 5.3418e-01 (5.9849e-01)	Acc@1  83.00 ( 79.74)	Acc@5  99.00 ( 98.92)
 * Acc@1 80.050 Acc@5 98.910
### epoch[9] execution time: 15.194310665130615
EPOCH 10
# current learning rate: 0.1
# Switched to train mode...
Epoch: [10][  0/391]	Time  0.213 ( 0.213)	Data  0.179 ( 0.179)	Loss 3.1543e-01 (3.1543e-01)	Acc@1  89.06 ( 89.06)	Acc@5 100.00 (100.00)
Epoch: [10][ 10/391]	Time  0.034 ( 0.049)	Data  0.001 ( 0.018)	Loss 4.1064e-01 (4.3779e-01)	Acc@1  81.25 ( 85.65)	Acc@5 100.00 ( 99.29)
Epoch: [10][ 20/391]	Time  0.032 ( 0.041)	Data  0.001 ( 0.010)	Loss 3.8208e-01 (4.3350e-01)	Acc@1  86.72 ( 85.49)	Acc@5 100.00 ( 99.26)
Epoch: [10][ 30/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.007)	Loss 4.2700e-01 (4.3710e-01)	Acc@1  85.94 ( 85.33)	Acc@5 100.00 ( 99.29)
Epoch: [10][ 40/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.006)	Loss 4.4482e-01 (4.4095e-01)	Acc@1  83.59 ( 84.93)	Acc@5 100.00 ( 99.22)
Epoch: [10][ 50/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.005)	Loss 4.7021e-01 (4.4437e-01)	Acc@1  82.03 ( 84.71)	Acc@5 100.00 ( 99.28)
Epoch: [10][ 60/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 3.3081e-01 (4.4150e-01)	Acc@1  85.94 ( 84.90)	Acc@5 100.00 ( 99.32)
Epoch: [10][ 70/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 5.5225e-01 (4.4287e-01)	Acc@1  79.69 ( 84.97)	Acc@5  99.22 ( 99.33)
Epoch: [10][ 80/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.004)	Loss 5.0439e-01 (4.4195e-01)	Acc@1  83.59 ( 85.00)	Acc@5 100.00 ( 99.36)
Epoch: [10][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 4.8828e-01 (4.3960e-01)	Acc@1  82.03 ( 85.03)	Acc@5  99.22 ( 99.39)
Epoch: [10][100/391]	Time  0.039 ( 0.035)	Data  0.001 ( 0.003)	Loss 4.7705e-01 (4.3919e-01)	Acc@1  85.16 ( 85.02)	Acc@5  99.22 ( 99.37)
Epoch: [10][110/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 4.9316e-01 (4.4131e-01)	Acc@1  81.25 ( 84.92)	Acc@5 100.00 ( 99.40)
Epoch: [10][120/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 4.5093e-01 (4.3945e-01)	Acc@1  83.59 ( 84.94)	Acc@5  98.44 ( 99.40)
Epoch: [10][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.4834e-01 (4.4011e-01)	Acc@1  78.91 ( 84.86)	Acc@5 100.00 ( 99.42)
Epoch: [10][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.1494e-01 (4.4275e-01)	Acc@1  88.28 ( 84.76)	Acc@5 100.00 ( 99.38)
Epoch: [10][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.5620e-01 (4.4286e-01)	Acc@1  88.28 ( 84.79)	Acc@5  98.44 ( 99.36)
Epoch: [10][160/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.6670e-01 (4.3961e-01)	Acc@1  85.16 ( 84.93)	Acc@5 100.00 ( 99.36)
Epoch: [10][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.1040e-01 (4.4054e-01)	Acc@1  84.38 ( 84.87)	Acc@5 100.00 ( 99.37)
Epoch: [10][180/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.6094e-01 (4.4270e-01)	Acc@1  85.16 ( 84.78)	Acc@5  99.22 ( 99.37)
Epoch: [10][190/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.0796e-01 (4.4211e-01)	Acc@1  85.94 ( 84.79)	Acc@5 100.00 ( 99.35)
Epoch: [10][200/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.3496e-01 (4.4090e-01)	Acc@1  92.19 ( 84.79)	Acc@5 100.00 ( 99.37)
Epoch: [10][210/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.5645e-01 (4.3992e-01)	Acc@1  88.28 ( 84.79)	Acc@5 100.00 ( 99.39)
Epoch: [10][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.4155e-01 (4.4013e-01)	Acc@1  89.84 ( 84.79)	Acc@5 100.00 ( 99.37)
Epoch: [10][230/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.5996e-01 (4.4098e-01)	Acc@1  82.03 ( 84.75)	Acc@5 100.00 ( 99.38)
Epoch: [10][240/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.7500e-01 (4.4073e-01)	Acc@1  86.72 ( 84.76)	Acc@5  98.44 ( 99.38)
Epoch: [10][250/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.9429e-01 (4.3929e-01)	Acc@1  83.59 ( 84.79)	Acc@5 100.00 ( 99.39)
Epoch: [10][260/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.1870e-01 (4.3869e-01)	Acc@1  83.59 ( 84.80)	Acc@5  99.22 ( 99.40)
Epoch: [10][270/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.5337e-01 (4.3992e-01)	Acc@1  82.81 ( 84.75)	Acc@5  99.22 ( 99.39)
Epoch: [10][280/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.4277e-01 (4.3937e-01)	Acc@1  86.72 ( 84.74)	Acc@5 100.00 ( 99.39)
Epoch: [10][290/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.2715e-01 (4.3783e-01)	Acc@1  89.06 ( 84.82)	Acc@5  99.22 ( 99.40)
Epoch: [10][300/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.5874e-01 (4.3686e-01)	Acc@1  85.16 ( 84.88)	Acc@5 100.00 ( 99.41)
Epoch: [10][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.3115e-01 (4.3659e-01)	Acc@1  85.16 ( 84.90)	Acc@5  99.22 ( 99.41)
Epoch: [10][320/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.6621e-01 (4.3579e-01)	Acc@1  85.94 ( 84.90)	Acc@5 100.00 ( 99.42)
Epoch: [10][330/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.4263e-01 (4.3501e-01)	Acc@1  82.81 ( 84.90)	Acc@5  99.22 ( 99.42)
Epoch: [10][340/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.1465e-01 (4.3503e-01)	Acc@1  81.25 ( 84.90)	Acc@5 100.00 ( 99.43)
Epoch: [10][350/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.8550e-01 (4.3400e-01)	Acc@1  85.94 ( 84.93)	Acc@5  99.22 ( 99.43)
Epoch: [10][360/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.9600e-01 (4.3457e-01)	Acc@1  85.94 ( 84.93)	Acc@5  99.22 ( 99.42)
Epoch: [10][370/391]	Time  0.033 ( 0.034)	Data  0.002 ( 0.002)	Loss 4.1772e-01 (4.3475e-01)	Acc@1  85.16 ( 84.91)	Acc@5  99.22 ( 99.41)
Epoch: [10][380/391]	Time  0.034 ( 0.034)	Data  0.002 ( 0.002)	Loss 5.7617e-01 (4.3572e-01)	Acc@1  82.81 ( 84.88)	Acc@5  99.22 ( 99.41)
Epoch: [10][390/391]	Time  0.022 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.6597e-01 (4.3554e-01)	Acc@1  86.25 ( 84.88)	Acc@5 100.00 ( 99.41)
## e[10] optimizer.zero_grad (sum) time: 0.16950464248657227
## e[10]       loss.backward (sum) time: 4.0955657958984375
## e[10]      optimizer.step (sum) time: 1.2120304107666016
## epoch[10] training(only) time: 13.270510911941528
# Switched to evaluate mode...
Test: [  0/100]	Time  0.171 ( 0.171)	Loss 4.2871e-01 (4.2871e-01)	Acc@1  86.00 ( 86.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.018 ( 0.031)	Loss 5.6836e-01 (5.1372e-01)	Acc@1  81.00 ( 81.45)	Acc@5  98.00 ( 99.18)
Test: [ 20/100]	Time  0.016 ( 0.025)	Loss 4.8120e-01 (5.3412e-01)	Acc@1  81.00 ( 81.29)	Acc@5  99.00 ( 99.00)
Test: [ 30/100]	Time  0.013 ( 0.021)	Loss 5.0342e-01 (5.4158e-01)	Acc@1  79.00 ( 81.42)	Acc@5  98.00 ( 99.03)
Test: [ 40/100]	Time  0.016 ( 0.020)	Loss 5.2002e-01 (5.3320e-01)	Acc@1  82.00 ( 81.63)	Acc@5  99.00 ( 99.15)
Test: [ 50/100]	Time  0.021 ( 0.020)	Loss 3.9429e-01 (5.3252e-01)	Acc@1  86.00 ( 81.92)	Acc@5 100.00 ( 99.14)
Test: [ 60/100]	Time  0.014 ( 0.019)	Loss 5.2197e-01 (5.3306e-01)	Acc@1  80.00 ( 82.05)	Acc@5 100.00 ( 99.18)
Test: [ 70/100]	Time  0.020 ( 0.019)	Loss 7.3633e-01 (5.2960e-01)	Acc@1  77.00 ( 82.14)	Acc@5  98.00 ( 99.13)
Test: [ 80/100]	Time  0.016 ( 0.019)	Loss 5.0146e-01 (5.3089e-01)	Acc@1  84.00 ( 82.20)	Acc@5  99.00 ( 99.16)
Test: [ 90/100]	Time  0.016 ( 0.019)	Loss 5.2930e-01 (5.3228e-01)	Acc@1  84.00 ( 82.19)	Acc@5 100.00 ( 99.19)
 * Acc@1 82.290 Acc@5 99.190
### epoch[10] execution time: 15.241381406784058
EPOCH 11
# current learning rate: 0.1
# Switched to train mode...
Epoch: [11][  0/391]	Time  0.211 ( 0.211)	Data  0.178 ( 0.178)	Loss 4.0063e-01 (4.0063e-01)	Acc@1  86.72 ( 86.72)	Acc@5  99.22 ( 99.22)
Epoch: [11][ 10/391]	Time  0.033 ( 0.049)	Data  0.001 ( 0.018)	Loss 4.3945e-01 (3.9407e-01)	Acc@1  83.59 ( 86.29)	Acc@5 100.00 ( 99.43)
Epoch: [11][ 20/391]	Time  0.032 ( 0.041)	Data  0.001 ( 0.010)	Loss 4.3237e-01 (3.7860e-01)	Acc@1  85.16 ( 86.79)	Acc@5  99.22 ( 99.44)
Epoch: [11][ 30/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.007)	Loss 4.3579e-01 (3.8639e-01)	Acc@1  87.50 ( 86.67)	Acc@5  98.44 ( 99.27)
Epoch: [11][ 40/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.006)	Loss 3.4839e-01 (3.8326e-01)	Acc@1  87.50 ( 86.99)	Acc@5  99.22 ( 99.28)
Epoch: [11][ 50/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.005)	Loss 4.2407e-01 (3.7900e-01)	Acc@1  84.38 ( 86.96)	Acc@5 100.00 ( 99.39)
Epoch: [11][ 60/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 3.1104e-01 (3.8226e-01)	Acc@1  89.06 ( 86.94)	Acc@5 100.00 ( 99.44)
Epoch: [11][ 70/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 3.4839e-01 (3.8031e-01)	Acc@1  88.28 ( 87.07)	Acc@5 100.00 ( 99.45)
Epoch: [11][ 80/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.004)	Loss 3.1274e-01 (3.8085e-01)	Acc@1  92.19 ( 87.09)	Acc@5  99.22 ( 99.43)
Epoch: [11][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.4158e-01 (3.8262e-01)	Acc@1  94.53 ( 87.05)	Acc@5 100.00 ( 99.44)
Epoch: [11][100/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 3.8208e-01 (3.8587e-01)	Acc@1  82.03 ( 86.84)	Acc@5  99.22 ( 99.44)
Epoch: [11][110/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 6.1230e-01 (3.9209e-01)	Acc@1  80.47 ( 86.61)	Acc@5  99.22 ( 99.43)
Epoch: [11][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.6313e-01 (3.9531e-01)	Acc@1  84.38 ( 86.54)	Acc@5  98.44 ( 99.44)
Epoch: [11][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.3140e-01 (3.9713e-01)	Acc@1  83.59 ( 86.43)	Acc@5  99.22 ( 99.45)
Epoch: [11][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.2344e-01 (3.9952e-01)	Acc@1  81.25 ( 86.37)	Acc@5  98.44 ( 99.45)
Epoch: [11][150/391]	Time  0.041 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.5654e-01 (4.0159e-01)	Acc@1  83.59 ( 86.33)	Acc@5 100.00 ( 99.45)
Epoch: [11][160/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.9697e-01 (4.0184e-01)	Acc@1  85.94 ( 86.32)	Acc@5  99.22 ( 99.47)
Epoch: [11][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.7607e-01 (4.0343e-01)	Acc@1  83.59 ( 86.18)	Acc@5  98.44 ( 99.46)
Epoch: [11][180/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.0869e-01 (4.0321e-01)	Acc@1  83.59 ( 86.19)	Acc@5 100.00 ( 99.46)
Epoch: [11][190/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.9321e-01 (4.0345e-01)	Acc@1  88.28 ( 86.13)	Acc@5 100.00 ( 99.46)
Epoch: [11][200/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.3896e-01 (4.0385e-01)	Acc@1  86.72 ( 86.08)	Acc@5 100.00 ( 99.48)
Epoch: [11][210/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.2148e-01 (4.0665e-01)	Acc@1  82.03 ( 85.96)	Acc@5  99.22 ( 99.47)
Epoch: [11][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.9419e-01 (4.0487e-01)	Acc@1  90.62 ( 86.03)	Acc@5 100.00 ( 99.47)
Epoch: [11][230/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.3809e-01 (4.0439e-01)	Acc@1  80.47 ( 86.06)	Acc@5 100.00 ( 99.49)
Epoch: [11][240/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.4492e-01 (4.0435e-01)	Acc@1  84.38 ( 86.05)	Acc@5  99.22 ( 99.49)
Epoch: [11][250/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.9478e-01 (4.0458e-01)	Acc@1  86.72 ( 86.04)	Acc@5  97.66 ( 99.48)
Epoch: [11][260/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.8892e-01 (4.0557e-01)	Acc@1  87.50 ( 86.01)	Acc@5  99.22 ( 99.49)
Epoch: [11][270/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.5181e-01 (4.0423e-01)	Acc@1  86.72 ( 86.06)	Acc@5 100.00 ( 99.49)
Epoch: [11][280/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.0420e-01 (4.0389e-01)	Acc@1  89.06 ( 86.07)	Acc@5 100.00 ( 99.51)
Epoch: [11][290/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.7598e-01 (4.0454e-01)	Acc@1  84.38 ( 86.08)	Acc@5 100.00 ( 99.50)
Epoch: [11][300/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.0908e-01 (4.0534e-01)	Acc@1  89.06 ( 86.03)	Acc@5 100.00 ( 99.50)
Epoch: [11][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.2822e-01 (4.0525e-01)	Acc@1  83.59 ( 85.99)	Acc@5 100.00 ( 99.51)
Epoch: [11][320/391]	Time  0.047 ( 0.034)	Data  0.005 ( 0.002)	Loss 5.2881e-01 (4.0504e-01)	Acc@1  77.34 ( 85.98)	Acc@5  99.22 ( 99.51)
Epoch: [11][330/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.6558e-01 (4.0398e-01)	Acc@1  82.03 ( 85.99)	Acc@5  99.22 ( 99.52)
Epoch: [11][340/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.0479e-01 (4.0501e-01)	Acc@1  85.16 ( 85.96)	Acc@5 100.00 ( 99.51)
Epoch: [11][350/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.5771e-01 (4.0579e-01)	Acc@1  85.16 ( 85.95)	Acc@5  98.44 ( 99.51)
Epoch: [11][360/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.4092e-01 (4.0590e-01)	Acc@1  83.59 ( 85.93)	Acc@5  97.66 ( 99.50)
Epoch: [11][370/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.0503e-01 (4.0753e-01)	Acc@1  85.94 ( 85.91)	Acc@5  98.44 ( 99.49)
Epoch: [11][380/391]	Time  0.035 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.8584e-01 (4.0831e-01)	Acc@1  82.03 ( 85.88)	Acc@5 100.00 ( 99.48)
Epoch: [11][390/391]	Time  0.024 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.9463e-01 (4.0884e-01)	Acc@1  81.25 ( 85.86)	Acc@5  96.25 ( 99.48)
## e[11] optimizer.zero_grad (sum) time: 0.16932225227355957
## e[11]       loss.backward (sum) time: 4.073211431503296
## e[11]      optimizer.step (sum) time: 1.2133398056030273
## epoch[11] training(only) time: 13.198758125305176
# Switched to evaluate mode...
Test: [  0/100]	Time  0.164 ( 0.164)	Loss 3.5962e-01 (3.5962e-01)	Acc@1  84.00 ( 84.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.017 ( 0.030)	Loss 4.1455e-01 (4.6740e-01)	Acc@1  82.00 ( 83.36)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.014 ( 0.023)	Loss 5.0098e-01 (4.7098e-01)	Acc@1  82.00 ( 83.57)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.014 ( 0.021)	Loss 4.2505e-01 (4.8830e-01)	Acc@1  83.00 ( 83.06)	Acc@5 100.00 ( 99.42)
Test: [ 40/100]	Time  0.014 ( 0.020)	Loss 5.3711e-01 (4.9100e-01)	Acc@1  82.00 ( 82.95)	Acc@5 100.00 ( 99.37)
Test: [ 50/100]	Time  0.014 ( 0.019)	Loss 4.5752e-01 (4.8421e-01)	Acc@1  83.00 ( 83.29)	Acc@5  99.00 ( 99.35)
Test: [ 60/100]	Time  0.014 ( 0.019)	Loss 3.7451e-01 (4.8261e-01)	Acc@1  88.00 ( 83.54)	Acc@5 100.00 ( 99.38)
Test: [ 70/100]	Time  0.018 ( 0.019)	Loss 5.2148e-01 (4.8446e-01)	Acc@1  83.00 ( 83.58)	Acc@5 100.00 ( 99.38)
Test: [ 80/100]	Time  0.020 ( 0.018)	Loss 4.0186e-01 (4.8258e-01)	Acc@1  87.00 ( 83.60)	Acc@5  99.00 ( 99.38)
Test: [ 90/100]	Time  0.014 ( 0.018)	Loss 3.3521e-01 (4.8322e-01)	Acc@1  90.00 ( 83.52)	Acc@5 100.00 ( 99.37)
 * Acc@1 83.500 Acc@5 99.390
### epoch[11] execution time: 15.117633819580078
EPOCH 12
# current learning rate: 0.1
# Switched to train mode...
Epoch: [12][  0/391]	Time  0.227 ( 0.227)	Data  0.190 ( 0.190)	Loss 3.4448e-01 (3.4448e-01)	Acc@1  89.84 ( 89.84)	Acc@5  99.22 ( 99.22)
Epoch: [12][ 10/391]	Time  0.033 ( 0.051)	Data  0.001 ( 0.019)	Loss 4.6729e-01 (3.4844e-01)	Acc@1  79.69 ( 87.57)	Acc@5 100.00 ( 99.64)
Epoch: [12][ 20/391]	Time  0.034 ( 0.042)	Data  0.001 ( 0.010)	Loss 4.3091e-01 (3.6102e-01)	Acc@1  83.59 ( 86.87)	Acc@5  99.22 ( 99.70)
Epoch: [12][ 30/391]	Time  0.032 ( 0.040)	Data  0.001 ( 0.008)	Loss 3.3350e-01 (3.6836e-01)	Acc@1  89.84 ( 86.87)	Acc@5  99.22 ( 99.52)
Epoch: [12][ 40/391]	Time  0.032 ( 0.038)	Data  0.001 ( 0.006)	Loss 4.4971e-01 (3.7468e-01)	Acc@1  84.38 ( 86.70)	Acc@5 100.00 ( 99.45)
Epoch: [12][ 50/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.005)	Loss 4.1821e-01 (3.7200e-01)	Acc@1  86.72 ( 86.67)	Acc@5 100.00 ( 99.49)
Epoch: [12][ 60/391]	Time  0.040 ( 0.037)	Data  0.001 ( 0.005)	Loss 4.0674e-01 (3.6810e-01)	Acc@1  85.94 ( 86.67)	Acc@5  99.22 ( 99.55)
Epoch: [12][ 70/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 3.7524e-01 (3.7197e-01)	Acc@1  88.28 ( 86.63)	Acc@5 100.00 ( 99.56)
Epoch: [12][ 80/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.004)	Loss 4.6729e-01 (3.7086e-01)	Acc@1  85.16 ( 86.74)	Acc@5  99.22 ( 99.58)
Epoch: [12][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 3.4741e-01 (3.6919e-01)	Acc@1  88.28 ( 86.74)	Acc@5 100.00 ( 99.59)
Epoch: [12][100/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 5.0000e-01 (3.7384e-01)	Acc@1  84.38 ( 86.62)	Acc@5 100.00 ( 99.60)
Epoch: [12][110/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 4.1870e-01 (3.7441e-01)	Acc@1  85.16 ( 86.64)	Acc@5 100.00 ( 99.62)
Epoch: [12][120/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 3.8477e-01 (3.7547e-01)	Acc@1  89.06 ( 86.63)	Acc@5 100.00 ( 99.60)
Epoch: [12][130/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 3.9160e-01 (3.7789e-01)	Acc@1  86.72 ( 86.61)	Acc@5 100.00 ( 99.60)
Epoch: [12][140/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.003)	Loss 3.6719e-01 (3.7714e-01)	Acc@1  85.16 ( 86.64)	Acc@5 100.00 ( 99.58)
Epoch: [12][150/391]	Time  0.031 ( 0.034)	Data  0.002 ( 0.003)	Loss 2.7246e-01 (3.7680e-01)	Acc@1  89.06 ( 86.67)	Acc@5 100.00 ( 99.59)
Epoch: [12][160/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.4182e-01 (3.7423e-01)	Acc@1  93.75 ( 86.80)	Acc@5 100.00 ( 99.60)
Epoch: [12][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.3447e-01 (3.7151e-01)	Acc@1  89.06 ( 86.92)	Acc@5 100.00 ( 99.61)
Epoch: [12][180/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.0527e-01 (3.7209e-01)	Acc@1  86.72 ( 86.96)	Acc@5 100.00 ( 99.61)
Epoch: [12][190/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.9976e-01 (3.7314e-01)	Acc@1  84.38 ( 86.98)	Acc@5 100.00 ( 99.61)
Epoch: [12][200/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.8730e-01 (3.7387e-01)	Acc@1  82.03 ( 86.94)	Acc@5 100.00 ( 99.61)
Epoch: [12][210/391]	Time  0.034 ( 0.034)	Data  0.002 ( 0.002)	Loss 3.3960e-01 (3.7509e-01)	Acc@1  85.16 ( 86.87)	Acc@5  99.22 ( 99.60)
Epoch: [12][220/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.9502e-01 (3.7639e-01)	Acc@1  85.94 ( 86.84)	Acc@5  98.44 ( 99.59)
Epoch: [12][230/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.6084e-01 (3.7741e-01)	Acc@1  89.84 ( 86.80)	Acc@5 100.00 ( 99.60)
Epoch: [12][240/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.6880e-01 (3.7649e-01)	Acc@1  91.41 ( 86.82)	Acc@5  99.22 ( 99.60)
Epoch: [12][250/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.1396e-01 (3.7747e-01)	Acc@1  89.06 ( 86.75)	Acc@5 100.00 ( 99.60)
Epoch: [12][260/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.0454e-01 (3.7891e-01)	Acc@1  85.16 ( 86.70)	Acc@5  99.22 ( 99.59)
Epoch: [12][270/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.6904e-01 (3.7796e-01)	Acc@1  89.84 ( 86.74)	Acc@5 100.00 ( 99.58)
Epoch: [12][280/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.8354e-01 (3.7986e-01)	Acc@1  89.84 ( 86.71)	Acc@5  99.22 ( 99.57)
Epoch: [12][290/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.3813e-01 (3.7947e-01)	Acc@1  85.94 ( 86.72)	Acc@5 100.00 ( 99.58)
Epoch: [12][300/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.0283e-01 (3.7996e-01)	Acc@1  86.72 ( 86.73)	Acc@5  99.22 ( 99.57)
Epoch: [12][310/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.9526e-01 (3.8001e-01)	Acc@1  85.94 ( 86.74)	Acc@5 100.00 ( 99.57)
Epoch: [12][320/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.1787e-01 (3.7974e-01)	Acc@1  88.28 ( 86.76)	Acc@5 100.00 ( 99.57)
Epoch: [12][330/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.1104e-01 (3.7946e-01)	Acc@1  89.84 ( 86.78)	Acc@5 100.00 ( 99.57)
Epoch: [12][340/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.2017e-01 (3.8022e-01)	Acc@1  87.50 ( 86.76)	Acc@5  98.44 ( 99.56)
Epoch: [12][350/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.2886e-01 (3.7988e-01)	Acc@1  91.41 ( 86.78)	Acc@5  99.22 ( 99.56)
Epoch: [12][360/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.3613e-01 (3.7919e-01)	Acc@1  83.59 ( 86.80)	Acc@5  99.22 ( 99.57)
Epoch: [12][370/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.7402e-01 (3.7940e-01)	Acc@1  85.16 ( 86.81)	Acc@5  99.22 ( 99.57)
Epoch: [12][380/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.4036e-01 (3.7851e-01)	Acc@1  92.19 ( 86.85)	Acc@5 100.00 ( 99.57)
Epoch: [12][390/391]	Time  0.022 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.3125e-01 (3.7813e-01)	Acc@1  80.00 ( 86.88)	Acc@5  97.50 ( 99.56)
## e[12] optimizer.zero_grad (sum) time: 0.1682584285736084
## e[12]       loss.backward (sum) time: 3.9844024181365967
## e[12]      optimizer.step (sum) time: 1.2304356098175049
## epoch[12] training(only) time: 13.29990816116333
# Switched to evaluate mode...
Test: [  0/100]	Time  0.175 ( 0.175)	Loss 3.2300e-01 (3.2300e-01)	Acc@1  87.00 ( 87.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.020 ( 0.032)	Loss 4.1553e-01 (4.1642e-01)	Acc@1  85.00 ( 85.55)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.014 ( 0.024)	Loss 5.5078e-01 (4.4385e-01)	Acc@1  80.00 ( 84.71)	Acc@5 100.00 ( 99.38)
Test: [ 30/100]	Time  0.015 ( 0.022)	Loss 5.8301e-01 (4.6346e-01)	Acc@1  82.00 ( 84.68)	Acc@5  99.00 ( 99.35)
Test: [ 40/100]	Time  0.015 ( 0.020)	Loss 4.8828e-01 (4.6656e-01)	Acc@1  83.00 ( 84.51)	Acc@5  99.00 ( 99.32)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 4.1724e-01 (4.7107e-01)	Acc@1  89.00 ( 84.65)	Acc@5  97.00 ( 99.22)
Test: [ 60/100]	Time  0.022 ( 0.019)	Loss 3.7842e-01 (4.6921e-01)	Acc@1  86.00 ( 84.70)	Acc@5 100.00 ( 99.28)
Test: [ 70/100]	Time  0.020 ( 0.019)	Loss 5.0537e-01 (4.7016e-01)	Acc@1  85.00 ( 84.75)	Acc@5 100.00 ( 99.31)
Test: [ 80/100]	Time  0.016 ( 0.019)	Loss 4.0381e-01 (4.7202e-01)	Acc@1  84.00 ( 84.46)	Acc@5 100.00 ( 99.35)
Test: [ 90/100]	Time  0.023 ( 0.019)	Loss 2.6001e-01 (4.7211e-01)	Acc@1  91.00 ( 84.42)	Acc@5 100.00 ( 99.38)
 * Acc@1 84.420 Acc@5 99.380
### epoch[12] execution time: 15.248435735702515
EPOCH 13
# current learning rate: 0.1
# Switched to train mode...
Epoch: [13][  0/391]	Time  0.216 ( 0.216)	Data  0.183 ( 0.183)	Loss 3.8867e-01 (3.8867e-01)	Acc@1  84.38 ( 84.38)	Acc@5  99.22 ( 99.22)
Epoch: [13][ 10/391]	Time  0.032 ( 0.050)	Data  0.001 ( 0.018)	Loss 3.4985e-01 (3.5387e-01)	Acc@1  86.72 ( 87.36)	Acc@5  99.22 ( 99.64)
Epoch: [13][ 20/391]	Time  0.032 ( 0.042)	Data  0.001 ( 0.010)	Loss 2.8442e-01 (3.5531e-01)	Acc@1  88.28 ( 87.31)	Acc@5  99.22 ( 99.63)
Epoch: [13][ 30/391]	Time  0.033 ( 0.039)	Data  0.001 ( 0.007)	Loss 4.5093e-01 (3.5918e-01)	Acc@1  83.59 ( 87.32)	Acc@5  98.44 ( 99.67)
Epoch: [13][ 40/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.006)	Loss 4.3018e-01 (3.6626e-01)	Acc@1  85.94 ( 87.04)	Acc@5  99.22 ( 99.66)
Epoch: [13][ 50/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.005)	Loss 2.7441e-01 (3.5781e-01)	Acc@1  91.41 ( 87.44)	Acc@5  99.22 ( 99.65)
Epoch: [13][ 60/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.004)	Loss 4.0649e-01 (3.5619e-01)	Acc@1  85.16 ( 87.37)	Acc@5 100.00 ( 99.71)
Epoch: [13][ 70/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.004)	Loss 2.9004e-01 (3.5121e-01)	Acc@1  90.62 ( 87.51)	Acc@5 100.00 ( 99.71)
Epoch: [13][ 80/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.004)	Loss 3.0225e-01 (3.4911e-01)	Acc@1  90.62 ( 87.60)	Acc@5  99.22 ( 99.68)
Epoch: [13][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 4.1895e-01 (3.5102e-01)	Acc@1  87.50 ( 87.59)	Acc@5  99.22 ( 99.67)
Epoch: [13][100/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.003)	Loss 3.3545e-01 (3.5447e-01)	Acc@1  89.06 ( 87.45)	Acc@5  99.22 ( 99.67)
Epoch: [13][110/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 3.6523e-01 (3.5652e-01)	Acc@1  85.94 ( 87.42)	Acc@5  99.22 ( 99.66)
Epoch: [13][120/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 3.4082e-01 (3.5555e-01)	Acc@1  82.03 ( 87.44)	Acc@5 100.00 ( 99.64)
Epoch: [13][130/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.7612e-01 (3.5778e-01)	Acc@1  91.41 ( 87.46)	Acc@5  99.22 ( 99.61)
Epoch: [13][140/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.4277e-01 (3.5870e-01)	Acc@1  88.28 ( 87.45)	Acc@5 100.00 ( 99.60)
Epoch: [13][150/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.6196e-01 (3.5850e-01)	Acc@1  92.97 ( 87.47)	Acc@5 100.00 ( 99.62)
Epoch: [13][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.1577e-01 (3.5814e-01)	Acc@1  89.06 ( 87.49)	Acc@5  98.44 ( 99.62)
Epoch: [13][170/391]	Time  0.033 ( 0.034)	Data  0.000 ( 0.003)	Loss 2.5732e-01 (3.5825e-01)	Acc@1  92.97 ( 87.50)	Acc@5 100.00 ( 99.60)
Epoch: [13][180/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.1465e-01 (3.5962e-01)	Acc@1  82.81 ( 87.51)	Acc@5  99.22 ( 99.61)
Epoch: [13][190/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.5806e-01 (3.5948e-01)	Acc@1  90.62 ( 87.56)	Acc@5  99.22 ( 99.59)
Epoch: [13][200/391]	Time  0.041 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.7515e-01 (3.5871e-01)	Acc@1  91.41 ( 87.59)	Acc@5 100.00 ( 99.58)
Epoch: [13][210/391]	Time  0.037 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.5205e-01 (3.5956e-01)	Acc@1  88.28 ( 87.53)	Acc@5 100.00 ( 99.59)
Epoch: [13][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.2300e-01 (3.5891e-01)	Acc@1  89.84 ( 87.56)	Acc@5  98.44 ( 99.59)
Epoch: [13][230/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.2163e-01 (3.6112e-01)	Acc@1  88.28 ( 87.52)	Acc@5  98.44 ( 99.58)
Epoch: [13][240/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.9517e-01 (3.5995e-01)	Acc@1  89.84 ( 87.57)	Acc@5  99.22 ( 99.59)
Epoch: [13][250/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.4839e-01 (3.5917e-01)	Acc@1  84.38 ( 87.56)	Acc@5  99.22 ( 99.59)
Epoch: [13][260/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.8159e-01 (3.5945e-01)	Acc@1  85.16 ( 87.51)	Acc@5 100.00 ( 99.59)
Epoch: [13][270/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.1152e-01 (3.6002e-01)	Acc@1  90.62 ( 87.50)	Acc@5 100.00 ( 99.59)
Epoch: [13][280/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.9307e-01 (3.5926e-01)	Acc@1  86.72 ( 87.50)	Acc@5  99.22 ( 99.59)
Epoch: [13][290/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.4985e-01 (3.6000e-01)	Acc@1  85.94 ( 87.48)	Acc@5 100.00 ( 99.59)
Epoch: [13][300/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.2871e-01 (3.6001e-01)	Acc@1  83.59 ( 87.49)	Acc@5  99.22 ( 99.58)
Epoch: [13][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.2651e-01 (3.6110e-01)	Acc@1  87.50 ( 87.47)	Acc@5  99.22 ( 99.58)
Epoch: [13][320/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.2642e-01 (3.6050e-01)	Acc@1  85.16 ( 87.45)	Acc@5 100.00 ( 99.58)
Epoch: [13][330/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.6440e-01 (3.6023e-01)	Acc@1  92.97 ( 87.47)	Acc@5 100.00 ( 99.58)
Epoch: [13][340/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.0786e-01 (3.5975e-01)	Acc@1  85.94 ( 87.46)	Acc@5 100.00 ( 99.59)
Epoch: [13][350/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.2080e-01 (3.5971e-01)	Acc@1  89.84 ( 87.47)	Acc@5  99.22 ( 99.58)
Epoch: [13][360/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.4912e-01 (3.5959e-01)	Acc@1  91.41 ( 87.48)	Acc@5 100.00 ( 99.58)
Epoch: [13][370/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.5083e-01 (3.5899e-01)	Acc@1  85.94 ( 87.49)	Acc@5 100.00 ( 99.59)
Epoch: [13][380/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.0063e-01 (3.5900e-01)	Acc@1  85.94 ( 87.50)	Acc@5  99.22 ( 99.58)
Epoch: [13][390/391]	Time  0.026 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.5684e-01 (3.5895e-01)	Acc@1  91.25 ( 87.50)	Acc@5 100.00 ( 99.58)
## e[13] optimizer.zero_grad (sum) time: 0.16867852210998535
## e[13]       loss.backward (sum) time: 3.9891440868377686
## e[13]      optimizer.step (sum) time: 1.2181122303009033
## epoch[13] training(only) time: 13.332545518875122
# Switched to evaluate mode...
Test: [  0/100]	Time  0.165 ( 0.165)	Loss 4.2188e-01 (4.2188e-01)	Acc@1  83.00 ( 83.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.014 ( 0.030)	Loss 4.4434e-01 (4.2729e-01)	Acc@1  85.00 ( 84.36)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.014 ( 0.024)	Loss 5.2637e-01 (4.4602e-01)	Acc@1  78.00 ( 84.24)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.019 ( 0.021)	Loss 3.7671e-01 (4.6930e-01)	Acc@1  86.00 ( 84.03)	Acc@5  99.00 ( 99.48)
Test: [ 40/100]	Time  0.014 ( 0.020)	Loss 3.2812e-01 (4.6684e-01)	Acc@1  89.00 ( 84.27)	Acc@5 100.00 ( 99.39)
Test: [ 50/100]	Time  0.014 ( 0.020)	Loss 3.2202e-01 (4.6164e-01)	Acc@1  86.00 ( 84.43)	Acc@5 100.00 ( 99.37)
Test: [ 60/100]	Time  0.015 ( 0.019)	Loss 4.0063e-01 (4.6235e-01)	Acc@1  84.00 ( 84.39)	Acc@5  98.00 ( 99.38)
Test: [ 70/100]	Time  0.014 ( 0.019)	Loss 4.2847e-01 (4.6264e-01)	Acc@1  86.00 ( 84.25)	Acc@5 100.00 ( 99.42)
Test: [ 80/100]	Time  0.029 ( 0.019)	Loss 4.1016e-01 (4.6604e-01)	Acc@1  86.00 ( 84.28)	Acc@5 100.00 ( 99.42)
Test: [ 90/100]	Time  0.015 ( 0.018)	Loss 4.5093e-01 (4.6539e-01)	Acc@1  82.00 ( 84.36)	Acc@5 100.00 ( 99.45)
 * Acc@1 84.370 Acc@5 99.480
### epoch[13] execution time: 15.30780029296875
EPOCH 14
# current learning rate: 0.1
# Switched to train mode...
Epoch: [14][  0/391]	Time  0.209 ( 0.209)	Data  0.174 ( 0.174)	Loss 3.1104e-01 (3.1104e-01)	Acc@1  91.41 ( 91.41)	Acc@5 100.00 (100.00)
Epoch: [14][ 10/391]	Time  0.032 ( 0.049)	Data  0.001 ( 0.017)	Loss 3.6450e-01 (3.0975e-01)	Acc@1  86.72 ( 89.28)	Acc@5 100.00 ( 99.86)
Epoch: [14][ 20/391]	Time  0.032 ( 0.041)	Data  0.001 ( 0.010)	Loss 2.5562e-01 (3.3816e-01)	Acc@1  89.06 ( 88.06)	Acc@5 100.00 ( 99.70)
Epoch: [14][ 30/391]	Time  0.032 ( 0.038)	Data  0.001 ( 0.007)	Loss 2.7417e-01 (3.3143e-01)	Acc@1  91.41 ( 88.51)	Acc@5 100.00 ( 99.70)
Epoch: [14][ 40/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.006)	Loss 3.4961e-01 (3.2756e-01)	Acc@1  85.94 ( 88.59)	Acc@5 100.00 ( 99.70)
Epoch: [14][ 50/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.005)	Loss 3.6841e-01 (3.3124e-01)	Acc@1  87.50 ( 88.60)	Acc@5  99.22 ( 99.65)
Epoch: [14][ 60/391]	Time  0.037 ( 0.036)	Data  0.001 ( 0.004)	Loss 2.2241e-01 (3.2957e-01)	Acc@1  92.19 ( 88.72)	Acc@5  99.22 ( 99.65)
Epoch: [14][ 70/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 2.6074e-01 (3.2461e-01)	Acc@1  89.06 ( 88.84)	Acc@5 100.00 ( 99.65)
Epoch: [14][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 3.6792e-01 (3.2803e-01)	Acc@1  89.06 ( 88.73)	Acc@5 100.00 ( 99.67)
Epoch: [14][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 3.2324e-01 (3.2491e-01)	Acc@1  88.28 ( 88.85)	Acc@5  99.22 ( 99.68)
Epoch: [14][100/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 4.1553e-01 (3.2881e-01)	Acc@1  83.59 ( 88.61)	Acc@5  99.22 ( 99.68)
Epoch: [14][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.6758e-01 (3.2931e-01)	Acc@1  90.62 ( 88.64)	Acc@5  99.22 ( 99.68)
Epoch: [14][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.7368e-01 (3.3197e-01)	Acc@1  92.97 ( 88.55)	Acc@5  99.22 ( 99.69)
Epoch: [14][130/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.9885e-01 (3.3047e-01)	Acc@1  92.97 ( 88.57)	Acc@5 100.00 ( 99.70)
Epoch: [14][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.4497e-01 (3.3090e-01)	Acc@1  89.84 ( 88.56)	Acc@5  99.22 ( 99.70)
Epoch: [14][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.2690e-01 (3.3300e-01)	Acc@1  86.72 ( 88.42)	Acc@5 100.00 ( 99.72)
Epoch: [14][160/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.6353e-01 (3.3635e-01)	Acc@1  86.72 ( 88.29)	Acc@5  99.22 ( 99.69)
Epoch: [14][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.8965e-01 (3.3725e-01)	Acc@1  88.28 ( 88.27)	Acc@5  99.22 ( 99.68)
Epoch: [14][180/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.7930e-01 (3.3610e-01)	Acc@1  89.84 ( 88.33)	Acc@5 100.00 ( 99.68)
Epoch: [14][190/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.0469e-01 (3.3637e-01)	Acc@1  89.84 ( 88.32)	Acc@5 100.00 ( 99.69)
Epoch: [14][200/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.2139e-01 (3.3658e-01)	Acc@1  86.72 ( 88.32)	Acc@5 100.00 ( 99.69)
Epoch: [14][210/391]	Time  0.038 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.0688e-01 (3.3601e-01)	Acc@1  91.41 ( 88.36)	Acc@5  99.22 ( 99.70)
Epoch: [14][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.3804e-01 (3.3444e-01)	Acc@1  92.97 ( 88.42)	Acc@5 100.00 ( 99.71)
Epoch: [14][230/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.3350e-01 (3.3384e-01)	Acc@1  88.28 ( 88.45)	Acc@5 100.00 ( 99.71)
Epoch: [14][240/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.5264e-01 (3.3401e-01)	Acc@1  82.03 ( 88.44)	Acc@5 100.00 ( 99.71)
Epoch: [14][250/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.7461e-01 (3.3606e-01)	Acc@1  86.72 ( 88.39)	Acc@5 100.00 ( 99.70)
Epoch: [14][260/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.0259e-01 (3.3930e-01)	Acc@1  86.72 ( 88.28)	Acc@5  99.22 ( 99.68)
Epoch: [14][270/391]	Time  0.037 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.0664e-01 (3.4044e-01)	Acc@1  89.06 ( 88.20)	Acc@5 100.00 ( 99.69)
Epoch: [14][280/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.3823e-01 (3.4057e-01)	Acc@1  83.59 ( 88.20)	Acc@5  99.22 ( 99.69)
Epoch: [14][290/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.2163e-01 (3.3999e-01)	Acc@1  85.16 ( 88.21)	Acc@5 100.00 ( 99.69)
Epoch: [14][300/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.7988e-01 (3.4012e-01)	Acc@1  86.72 ( 88.18)	Acc@5  99.22 ( 99.69)
Epoch: [14][310/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.1521e-01 (3.4006e-01)	Acc@1  93.75 ( 88.20)	Acc@5 100.00 ( 99.70)
Epoch: [14][320/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.3691e-01 (3.4033e-01)	Acc@1  88.28 ( 88.20)	Acc@5 100.00 ( 99.69)
Epoch: [14][330/391]	Time  0.046 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.1274e-01 (3.4082e-01)	Acc@1  89.84 ( 88.19)	Acc@5  99.22 ( 99.68)
Epoch: [14][340/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.5132e-01 (3.4100e-01)	Acc@1  89.84 ( 88.21)	Acc@5  99.22 ( 99.67)
Epoch: [14][350/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.6035e-01 (3.4035e-01)	Acc@1  88.28 ( 88.23)	Acc@5 100.00 ( 99.67)
Epoch: [14][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.8013e-01 (3.4049e-01)	Acc@1  86.72 ( 88.24)	Acc@5  99.22 ( 99.67)
Epoch: [14][370/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.2334e-01 (3.4014e-01)	Acc@1  87.50 ( 88.23)	Acc@5 100.00 ( 99.67)
Epoch: [14][380/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.3901e-01 (3.4041e-01)	Acc@1  90.62 ( 88.22)	Acc@5 100.00 ( 99.67)
Epoch: [14][390/391]	Time  0.026 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.9526e-01 (3.4081e-01)	Acc@1  86.25 ( 88.20)	Acc@5 100.00 ( 99.67)
## e[14] optimizer.zero_grad (sum) time: 0.16933226585388184
## e[14]       loss.backward (sum) time: 4.041656017303467
## e[14]      optimizer.step (sum) time: 1.2186508178710938
## epoch[14] training(only) time: 13.230303525924683
# Switched to evaluate mode...
Test: [  0/100]	Time  0.171 ( 0.171)	Loss 3.0322e-01 (3.0322e-01)	Acc@1  89.00 ( 89.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.016 ( 0.032)	Loss 4.1431e-01 (4.3530e-01)	Acc@1  87.00 ( 85.73)	Acc@5  99.00 ( 99.09)
Test: [ 20/100]	Time  0.017 ( 0.025)	Loss 5.3369e-01 (4.5260e-01)	Acc@1  78.00 ( 84.95)	Acc@5 100.00 ( 99.10)
Test: [ 30/100]	Time  0.020 ( 0.022)	Loss 4.3530e-01 (4.6006e-01)	Acc@1  85.00 ( 85.13)	Acc@5  99.00 ( 99.00)
Test: [ 40/100]	Time  0.018 ( 0.021)	Loss 3.9624e-01 (4.5657e-01)	Acc@1  83.00 ( 85.12)	Acc@5 100.00 ( 99.02)
Test: [ 50/100]	Time  0.021 ( 0.020)	Loss 3.6816e-01 (4.5249e-01)	Acc@1  88.00 ( 85.24)	Acc@5 100.00 ( 99.08)
Test: [ 60/100]	Time  0.014 ( 0.020)	Loss 3.1030e-01 (4.4380e-01)	Acc@1  91.00 ( 85.34)	Acc@5 100.00 ( 99.13)
Test: [ 70/100]	Time  0.013 ( 0.019)	Loss 5.5176e-01 (4.4577e-01)	Acc@1  83.00 ( 85.18)	Acc@5 100.00 ( 99.20)
Test: [ 80/100]	Time  0.015 ( 0.019)	Loss 3.0615e-01 (4.4438e-01)	Acc@1  90.00 ( 85.15)	Acc@5  99.00 ( 99.23)
Test: [ 90/100]	Time  0.014 ( 0.019)	Loss 4.3359e-01 (4.4409e-01)	Acc@1  86.00 ( 85.03)	Acc@5 100.00 ( 99.29)
 * Acc@1 85.110 Acc@5 99.330
### epoch[14] execution time: 15.210010528564453
EPOCH 15
# current learning rate: 0.1
# Switched to train mode...
Epoch: [15][  0/391]	Time  0.214 ( 0.214)	Data  0.181 ( 0.181)	Loss 4.4580e-01 (4.4580e-01)	Acc@1  85.16 ( 85.16)	Acc@5 100.00 (100.00)
Epoch: [15][ 10/391]	Time  0.032 ( 0.050)	Data  0.001 ( 0.018)	Loss 3.7451e-01 (3.1551e-01)	Acc@1  85.94 ( 89.84)	Acc@5 100.00 ( 99.57)
Epoch: [15][ 20/391]	Time  0.035 ( 0.042)	Data  0.001 ( 0.010)	Loss 3.7915e-01 (3.2550e-01)	Acc@1  85.94 ( 89.17)	Acc@5 100.00 ( 99.52)
Epoch: [15][ 30/391]	Time  0.032 ( 0.040)	Data  0.001 ( 0.007)	Loss 3.6694e-01 (3.1741e-01)	Acc@1  86.72 ( 89.26)	Acc@5  98.44 ( 99.60)
Epoch: [15][ 40/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.006)	Loss 4.0552e-01 (3.3396e-01)	Acc@1  83.59 ( 88.57)	Acc@5  99.22 ( 99.58)
Epoch: [15][ 50/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 2.1106e-01 (3.2533e-01)	Acc@1  92.97 ( 88.71)	Acc@5 100.00 ( 99.62)
Epoch: [15][ 60/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 2.7588e-01 (3.2347e-01)	Acc@1  90.62 ( 88.72)	Acc@5 100.00 ( 99.62)
Epoch: [15][ 70/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 2.2314e-01 (3.2149e-01)	Acc@1  90.62 ( 88.73)	Acc@5 100.00 ( 99.59)
Epoch: [15][ 80/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.004)	Loss 3.0762e-01 (3.2538e-01)	Acc@1  89.06 ( 88.55)	Acc@5 100.00 ( 99.62)
Epoch: [15][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 3.2886e-01 (3.3008e-01)	Acc@1  89.84 ( 88.35)	Acc@5  98.44 ( 99.62)
Epoch: [15][100/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.5293e-01 (3.3129e-01)	Acc@1  92.19 ( 88.34)	Acc@5  99.22 ( 99.62)
Epoch: [15][110/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.8809e-01 (3.2740e-01)	Acc@1  94.53 ( 88.54)	Acc@5 100.00 ( 99.63)
Epoch: [15][120/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.3767e-01 (3.2656e-01)	Acc@1  93.75 ( 88.65)	Acc@5 100.00 ( 99.65)
Epoch: [15][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.5400e-01 (3.2667e-01)	Acc@1  88.28 ( 88.63)	Acc@5 100.00 ( 99.64)
Epoch: [15][140/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.2129e-01 (3.2239e-01)	Acc@1  86.72 ( 88.82)	Acc@5 100.00 ( 99.64)
Epoch: [15][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.2156e-01 (3.1941e-01)	Acc@1  92.97 ( 88.94)	Acc@5  99.22 ( 99.63)
Epoch: [15][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.0786e-01 (3.1979e-01)	Acc@1  88.28 ( 88.94)	Acc@5 100.00 ( 99.64)
Epoch: [15][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.1543e-01 (3.2060e-01)	Acc@1  89.06 ( 88.88)	Acc@5 100.00 ( 99.65)
Epoch: [15][180/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.1543e-01 (3.1963e-01)	Acc@1  89.06 ( 88.91)	Acc@5 100.00 ( 99.65)
Epoch: [15][190/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.1863e-01 (3.2021e-01)	Acc@1  90.62 ( 88.84)	Acc@5 100.00 ( 99.66)
Epoch: [15][200/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.6611e-01 (3.2035e-01)	Acc@1  91.41 ( 88.87)	Acc@5 100.00 ( 99.66)
Epoch: [15][210/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.3691e-01 (3.2218e-01)	Acc@1  88.28 ( 88.80)	Acc@5 100.00 ( 99.65)
Epoch: [15][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.4731e-01 (3.2136e-01)	Acc@1  92.97 ( 88.86)	Acc@5 100.00 ( 99.65)
Epoch: [15][230/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.0981e-01 (3.1998e-01)	Acc@1  89.06 ( 88.87)	Acc@5  99.22 ( 99.65)
Epoch: [15][240/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.8281e-01 (3.2069e-01)	Acc@1  84.38 ( 88.85)	Acc@5 100.00 ( 99.65)
Epoch: [15][250/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.7109e-01 (3.2075e-01)	Acc@1  86.72 ( 88.84)	Acc@5 100.00 ( 99.65)
Epoch: [15][260/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.7256e-01 (3.2103e-01)	Acc@1  85.16 ( 88.81)	Acc@5 100.00 ( 99.66)
Epoch: [15][270/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.4961e-01 (3.2178e-01)	Acc@1  86.72 ( 88.78)	Acc@5 100.00 ( 99.66)
Epoch: [15][280/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.5977e-01 (3.2192e-01)	Acc@1  89.84 ( 88.80)	Acc@5 100.00 ( 99.66)
Epoch: [15][290/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.1494e-01 (3.2179e-01)	Acc@1  89.84 ( 88.78)	Acc@5  99.22 ( 99.66)
Epoch: [15][300/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.6541e-01 (3.2051e-01)	Acc@1  96.09 ( 88.83)	Acc@5 100.00 ( 99.67)
Epoch: [15][310/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.8550e-01 (3.2149e-01)	Acc@1  85.16 ( 88.81)	Acc@5 100.00 ( 99.67)
Epoch: [15][320/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.2080e-01 (3.2075e-01)	Acc@1  89.84 ( 88.83)	Acc@5 100.00 ( 99.67)
Epoch: [15][330/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.6777e-01 (3.2183e-01)	Acc@1  85.94 ( 88.79)	Acc@5  99.22 ( 99.67)
Epoch: [15][340/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.4888e-01 (3.2226e-01)	Acc@1  89.06 ( 88.78)	Acc@5  99.22 ( 99.67)
Epoch: [15][350/391]	Time  0.055 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.2520e-01 (3.2236e-01)	Acc@1  85.16 ( 88.76)	Acc@5 100.00 ( 99.67)
Epoch: [15][360/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.0029e-01 (3.2148e-01)	Acc@1  90.62 ( 88.80)	Acc@5  99.22 ( 99.67)
Epoch: [15][370/391]	Time  0.028 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.6450e-01 (3.2030e-01)	Acc@1  87.50 ( 88.85)	Acc@5 100.00 ( 99.68)
Epoch: [15][380/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.1812e-01 (3.2015e-01)	Acc@1  88.28 ( 88.86)	Acc@5  98.44 ( 99.68)
Epoch: [15][390/391]	Time  0.026 ( 0.034)	Data  0.000 ( 0.002)	Loss 2.7588e-01 (3.2024e-01)	Acc@1  90.00 ( 88.86)	Acc@5 100.00 ( 99.68)
## e[15] optimizer.zero_grad (sum) time: 0.1686406135559082
## e[15]       loss.backward (sum) time: 3.8809523582458496
## e[15]      optimizer.step (sum) time: 1.2679650783538818
## epoch[15] training(only) time: 13.266429901123047
# Switched to evaluate mode...
Test: [  0/100]	Time  0.167 ( 0.167)	Loss 3.4888e-01 (3.4888e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.014 ( 0.030)	Loss 3.0078e-01 (3.4645e-01)	Acc@1  89.00 ( 87.91)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.020 ( 0.024)	Loss 5.0928e-01 (3.7768e-01)	Acc@1  84.00 ( 87.14)	Acc@5  99.00 ( 99.62)
Test: [ 30/100]	Time  0.015 ( 0.022)	Loss 4.6167e-01 (3.9684e-01)	Acc@1  85.00 ( 87.13)	Acc@5  99.00 ( 99.52)
Test: [ 40/100]	Time  0.014 ( 0.020)	Loss 4.2578e-01 (4.0501e-01)	Acc@1  84.00 ( 86.59)	Acc@5  99.00 ( 99.54)
Test: [ 50/100]	Time  0.016 ( 0.020)	Loss 3.1616e-01 (4.0473e-01)	Acc@1  88.00 ( 86.80)	Acc@5  98.00 ( 99.43)
Test: [ 60/100]	Time  0.013 ( 0.019)	Loss 3.3081e-01 (4.0193e-01)	Acc@1  90.00 ( 86.75)	Acc@5 100.00 ( 99.48)
Test: [ 70/100]	Time  0.015 ( 0.019)	Loss 5.7715e-01 (4.0579e-01)	Acc@1  84.00 ( 86.72)	Acc@5 100.00 ( 99.51)
Test: [ 80/100]	Time  0.015 ( 0.019)	Loss 3.1274e-01 (4.0939e-01)	Acc@1  89.00 ( 86.44)	Acc@5 100.00 ( 99.51)
Test: [ 90/100]	Time  0.014 ( 0.018)	Loss 2.2180e-01 (4.0745e-01)	Acc@1  91.00 ( 86.49)	Acc@5 100.00 ( 99.53)
 * Acc@1 86.590 Acc@5 99.540
### epoch[15] execution time: 15.193909645080566
EPOCH 16
# current learning rate: 0.1
# Switched to train mode...
Epoch: [16][  0/391]	Time  0.217 ( 0.217)	Data  0.182 ( 0.182)	Loss 3.1299e-01 (3.1299e-01)	Acc@1  88.28 ( 88.28)	Acc@5 100.00 (100.00)
Epoch: [16][ 10/391]	Time  0.032 ( 0.050)	Data  0.001 ( 0.018)	Loss 3.0664e-01 (2.6692e-01)	Acc@1  89.84 ( 90.84)	Acc@5 100.00 ( 99.93)
Epoch: [16][ 20/391]	Time  0.033 ( 0.042)	Data  0.001 ( 0.010)	Loss 3.5376e-01 (2.7668e-01)	Acc@1  87.50 ( 90.10)	Acc@5  99.22 ( 99.93)
Epoch: [16][ 30/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.007)	Loss 2.7490e-01 (2.7993e-01)	Acc@1  90.62 ( 90.20)	Acc@5  99.22 ( 99.90)
Epoch: [16][ 40/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.006)	Loss 3.9136e-01 (2.8352e-01)	Acc@1  85.94 ( 89.96)	Acc@5 100.00 ( 99.89)
Epoch: [16][ 50/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 4.1772e-01 (2.9368e-01)	Acc@1  83.59 ( 89.49)	Acc@5  99.22 ( 99.86)
Epoch: [16][ 60/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 3.2642e-01 (2.9421e-01)	Acc@1  89.84 ( 89.36)	Acc@5 100.00 ( 99.85)
Epoch: [16][ 70/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 3.5864e-01 (2.9605e-01)	Acc@1  88.28 ( 89.27)	Acc@5 100.00 ( 99.85)
Epoch: [16][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 3.4131e-01 (2.9542e-01)	Acc@1  87.50 ( 89.23)	Acc@5 100.00 ( 99.86)
Epoch: [16][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.9321e-01 (2.9614e-01)	Acc@1  89.06 ( 89.17)	Acc@5 100.00 ( 99.84)
Epoch: [16][100/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 3.4839e-01 (2.9912e-01)	Acc@1  89.06 ( 89.15)	Acc@5 100.00 ( 99.83)
Epoch: [16][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.8965e-01 (3.0386e-01)	Acc@1  86.72 ( 89.03)	Acc@5 100.00 ( 99.82)
Epoch: [16][120/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.1494e-01 (3.0644e-01)	Acc@1  87.50 ( 89.02)	Acc@5 100.00 ( 99.80)
Epoch: [16][130/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.6279e-01 (3.0561e-01)	Acc@1  85.94 ( 89.09)	Acc@5 100.00 ( 99.79)
Epoch: [16][140/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.8394e-01 (3.0371e-01)	Acc@1  89.06 ( 89.16)	Acc@5 100.00 ( 99.79)
Epoch: [16][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.3130e-01 (3.0226e-01)	Acc@1  89.06 ( 89.23)	Acc@5 100.00 ( 99.79)
Epoch: [16][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.5645e-01 (3.0274e-01)	Acc@1  86.72 ( 89.18)	Acc@5  99.22 ( 99.79)
Epoch: [16][170/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.6367e-01 (3.0236e-01)	Acc@1  87.50 ( 89.19)	Acc@5 100.00 ( 99.76)
Epoch: [16][180/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.8003e-01 (3.0153e-01)	Acc@1  90.62 ( 89.18)	Acc@5 100.00 ( 99.77)
Epoch: [16][190/391]	Time  0.028 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.6401e-01 (3.0373e-01)	Acc@1  85.94 ( 89.06)	Acc@5 100.00 ( 99.78)
Epoch: [16][200/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.6548e-01 (3.0544e-01)	Acc@1  82.03 ( 88.98)	Acc@5 100.00 ( 99.77)
Epoch: [16][210/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.8027e-01 (3.0580e-01)	Acc@1  88.28 ( 88.93)	Acc@5 100.00 ( 99.77)
Epoch: [16][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.8711e-01 (3.0563e-01)	Acc@1  89.06 ( 88.96)	Acc@5 100.00 ( 99.77)
Epoch: [16][230/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.6587e-01 (3.0582e-01)	Acc@1  88.28 ( 88.96)	Acc@5 100.00 ( 99.77)
Epoch: [16][240/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.8760e-01 (3.0464e-01)	Acc@1  89.84 ( 89.00)	Acc@5 100.00 ( 99.77)
Epoch: [16][250/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.4312e-01 (3.0471e-01)	Acc@1  84.38 ( 89.02)	Acc@5 100.00 ( 99.77)
Epoch: [16][260/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.6514e-01 (3.0412e-01)	Acc@1  91.41 ( 89.08)	Acc@5 100.00 ( 99.75)
Epoch: [16][270/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.0020e-01 (3.0367e-01)	Acc@1  92.19 ( 89.11)	Acc@5 100.00 ( 99.76)
Epoch: [16][280/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.3643e-01 (3.0360e-01)	Acc@1  89.84 ( 89.14)	Acc@5  98.44 ( 99.75)
Epoch: [16][290/391]	Time  0.027 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.0837e-01 (3.0392e-01)	Acc@1  94.53 ( 89.16)	Acc@5 100.00 ( 99.75)
Epoch: [16][300/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.2593e-01 (3.0420e-01)	Acc@1  87.50 ( 89.18)	Acc@5  99.22 ( 99.74)
Epoch: [16][310/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.6270e-01 (3.0394e-01)	Acc@1  91.41 ( 89.21)	Acc@5 100.00 ( 99.74)
Epoch: [16][320/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.6318e-01 (3.0309e-01)	Acc@1  92.19 ( 89.26)	Acc@5  99.22 ( 99.73)
Epoch: [16][330/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.6802e-01 (3.0422e-01)	Acc@1  85.94 ( 89.26)	Acc@5  99.22 ( 99.73)
Epoch: [16][340/391]	Time  0.043 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.0884e-01 (3.0428e-01)	Acc@1  90.62 ( 89.27)	Acc@5  98.44 ( 99.72)
Epoch: [16][350/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.0396e-01 (3.0305e-01)	Acc@1  88.28 ( 89.33)	Acc@5 100.00 ( 99.73)
Epoch: [16][360/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.2739e-01 (3.0196e-01)	Acc@1  89.06 ( 89.38)	Acc@5 100.00 ( 99.74)
Epoch: [16][370/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.4524e-01 (3.0177e-01)	Acc@1  90.62 ( 89.39)	Acc@5 100.00 ( 99.74)
Epoch: [16][380/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.0625e-01 (3.0321e-01)	Acc@1  87.50 ( 89.36)	Acc@5 100.00 ( 99.74)
Epoch: [16][390/391]	Time  0.025 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.0244e-01 (3.0329e-01)	Acc@1  82.50 ( 89.35)	Acc@5 100.00 ( 99.74)
## e[16] optimizer.zero_grad (sum) time: 0.1724228858947754
## e[16]       loss.backward (sum) time: 3.954828977584839
## e[16]      optimizer.step (sum) time: 1.2192180156707764
## epoch[16] training(only) time: 13.215951919555664
# Switched to evaluate mode...
Test: [  0/100]	Time  0.175 ( 0.175)	Loss 3.8867e-01 (3.8867e-01)	Acc@1  88.00 ( 88.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.014 ( 0.031)	Loss 3.1226e-01 (4.4209e-01)	Acc@1  88.00 ( 85.00)	Acc@5 100.00 ( 99.45)
Test: [ 20/100]	Time  0.020 ( 0.025)	Loss 4.7192e-01 (4.3708e-01)	Acc@1  85.00 ( 85.52)	Acc@5  99.00 ( 99.29)
Test: [ 30/100]	Time  0.016 ( 0.022)	Loss 5.2051e-01 (4.5594e-01)	Acc@1  81.00 ( 85.19)	Acc@5 100.00 ( 99.23)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 4.5508e-01 (4.4500e-01)	Acc@1  82.00 ( 85.17)	Acc@5  99.00 ( 99.22)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 3.7476e-01 (4.4346e-01)	Acc@1  87.00 ( 85.43)	Acc@5 100.00 ( 99.24)
Test: [ 60/100]	Time  0.020 ( 0.019)	Loss 3.7305e-01 (4.4512e-01)	Acc@1  86.00 ( 85.28)	Acc@5 100.00 ( 99.26)
Test: [ 70/100]	Time  0.027 ( 0.019)	Loss 5.4785e-01 (4.4218e-01)	Acc@1  82.00 ( 85.41)	Acc@5 100.00 ( 99.30)
Test: [ 80/100]	Time  0.014 ( 0.019)	Loss 4.0039e-01 (4.3859e-01)	Acc@1  87.00 ( 85.46)	Acc@5  99.00 ( 99.32)
Test: [ 90/100]	Time  0.019 ( 0.018)	Loss 4.2725e-01 (4.4391e-01)	Acc@1  85.00 ( 85.16)	Acc@5 100.00 ( 99.36)
 * Acc@1 85.180 Acc@5 99.400
### epoch[16] execution time: 15.185748815536499
EPOCH 17
# current learning rate: 0.1
# Switched to train mode...
Epoch: [17][  0/391]	Time  0.212 ( 0.212)	Data  0.179 ( 0.179)	Loss 1.2170e-01 (1.2170e-01)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [17][ 10/391]	Time  0.037 ( 0.051)	Data  0.001 ( 0.018)	Loss 2.9346e-01 (2.6842e-01)	Acc@1  90.62 ( 91.19)	Acc@5  99.22 ( 99.86)
Epoch: [17][ 20/391]	Time  0.032 ( 0.042)	Data  0.001 ( 0.010)	Loss 2.1399e-01 (2.8219e-01)	Acc@1  92.19 ( 90.51)	Acc@5 100.00 ( 99.81)
Epoch: [17][ 30/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.007)	Loss 2.2144e-01 (2.7245e-01)	Acc@1  92.97 ( 90.73)	Acc@5  99.22 ( 99.82)
Epoch: [17][ 40/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.006)	Loss 3.0200e-01 (2.8535e-01)	Acc@1  87.50 ( 89.79)	Acc@5 100.00 ( 99.85)
Epoch: [17][ 50/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 3.2178e-01 (2.8597e-01)	Acc@1  89.06 ( 89.69)	Acc@5 100.00 ( 99.83)
Epoch: [17][ 60/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.004)	Loss 3.6523e-01 (2.8673e-01)	Acc@1  88.28 ( 89.82)	Acc@5  99.22 ( 99.77)
Epoch: [17][ 70/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 2.4170e-01 (2.8473e-01)	Acc@1  89.84 ( 89.82)	Acc@5 100.00 ( 99.79)
Epoch: [17][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.1017e-01 (2.8054e-01)	Acc@1  98.44 ( 90.08)	Acc@5 100.00 ( 99.78)
Epoch: [17][ 90/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 3.1250e-01 (2.8103e-01)	Acc@1  88.28 ( 90.10)	Acc@5  99.22 ( 99.78)
Epoch: [17][100/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.6074e-01 (2.8403e-01)	Acc@1  92.97 ( 89.99)	Acc@5 100.00 ( 99.75)
Epoch: [17][110/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 3.0322e-01 (2.8460e-01)	Acc@1  89.84 ( 89.96)	Acc@5  99.22 ( 99.73)
Epoch: [17][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.6035e-01 (2.8381e-01)	Acc@1  86.72 ( 90.02)	Acc@5 100.00 ( 99.75)
Epoch: [17][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.3506e-01 (2.8671e-01)	Acc@1  86.72 ( 89.96)	Acc@5 100.00 ( 99.76)
Epoch: [17][140/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.2690e-01 (2.8817e-01)	Acc@1  88.28 ( 89.93)	Acc@5 100.00 ( 99.74)
Epoch: [17][150/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.9478e-01 (2.8900e-01)	Acc@1  88.28 ( 89.91)	Acc@5 100.00 ( 99.73)
Epoch: [17][160/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.8467e-01 (2.8819e-01)	Acc@1  90.62 ( 89.95)	Acc@5 100.00 ( 99.73)
Epoch: [17][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.3303e-01 (2.8943e-01)	Acc@1  90.62 ( 89.90)	Acc@5 100.00 ( 99.72)
Epoch: [17][180/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.7930e-01 (2.8878e-01)	Acc@1  92.19 ( 89.97)	Acc@5  99.22 ( 99.72)
Epoch: [17][190/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.2056e-01 (2.8830e-01)	Acc@1  89.84 ( 90.00)	Acc@5 100.00 ( 99.73)
Epoch: [17][200/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.0054e-01 (2.8671e-01)	Acc@1  92.19 ( 90.07)	Acc@5  99.22 ( 99.72)
Epoch: [17][210/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.2788e-01 (2.8597e-01)	Acc@1  89.84 ( 90.10)	Acc@5 100.00 ( 99.73)
Epoch: [17][220/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.5620e-01 (2.8712e-01)	Acc@1  90.62 ( 90.06)	Acc@5  97.66 ( 99.72)
Epoch: [17][230/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.8516e-01 (2.8600e-01)	Acc@1  90.62 ( 90.08)	Acc@5  99.22 ( 99.73)
Epoch: [17][240/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.4976e-01 (2.8628e-01)	Acc@1  92.19 ( 90.08)	Acc@5 100.00 ( 99.73)
Epoch: [17][250/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.7539e-01 (2.8665e-01)	Acc@1  87.50 ( 90.08)	Acc@5 100.00 ( 99.73)
Epoch: [17][260/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.8140e-01 (2.8650e-01)	Acc@1  92.97 ( 90.09)	Acc@5 100.00 ( 99.72)
Epoch: [17][270/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.5830e-01 (2.8670e-01)	Acc@1  89.84 ( 90.05)	Acc@5 100.00 ( 99.72)
Epoch: [17][280/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.4377e-01 (2.8694e-01)	Acc@1  91.41 ( 90.02)	Acc@5 100.00 ( 99.72)
Epoch: [17][290/391]	Time  0.037 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.7759e-01 (2.8713e-01)	Acc@1  91.41 ( 90.03)	Acc@5  99.22 ( 99.72)
Epoch: [17][300/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.6025e-01 (2.8885e-01)	Acc@1  90.62 ( 89.99)	Acc@5 100.00 ( 99.72)
Epoch: [17][310/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.1543e-01 (2.8949e-01)	Acc@1  88.28 ( 89.98)	Acc@5 100.00 ( 99.71)
Epoch: [17][320/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.6353e-01 (2.9015e-01)	Acc@1  89.06 ( 89.97)	Acc@5 100.00 ( 99.71)
Epoch: [17][330/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3379e-01 (2.8982e-01)	Acc@1  96.09 ( 89.97)	Acc@5 100.00 ( 99.71)
Epoch: [17][340/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.6416e-01 (2.8912e-01)	Acc@1  90.62 ( 89.98)	Acc@5 100.00 ( 99.72)
Epoch: [17][350/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.2642e-01 (2.8884e-01)	Acc@1  92.19 ( 90.00)	Acc@5 100.00 ( 99.72)
Epoch: [17][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.8540e-01 (2.8944e-01)	Acc@1  91.41 ( 89.98)	Acc@5 100.00 ( 99.73)
Epoch: [17][370/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.6489e-01 (2.9029e-01)	Acc@1  92.19 ( 89.96)	Acc@5 100.00 ( 99.73)
Epoch: [17][380/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.7124e-01 (2.9023e-01)	Acc@1  90.62 ( 89.96)	Acc@5 100.00 ( 99.73)
Epoch: [17][390/391]	Time  0.030 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.8013e-01 (2.8979e-01)	Acc@1  87.50 ( 90.01)	Acc@5  98.75 ( 99.73)
## e[17] optimizer.zero_grad (sum) time: 0.1698894500732422
## e[17]       loss.backward (sum) time: 4.006805896759033
## e[17]      optimizer.step (sum) time: 1.2365832328796387
## epoch[17] training(only) time: 13.226318120956421
# Switched to evaluate mode...
Test: [  0/100]	Time  0.170 ( 0.170)	Loss 4.2822e-01 (4.2822e-01)	Acc@1  85.00 ( 85.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.019 ( 0.031)	Loss 4.1187e-01 (3.8012e-01)	Acc@1  84.00 ( 87.27)	Acc@5 100.00 ( 99.45)
Test: [ 20/100]	Time  0.014 ( 0.024)	Loss 4.2407e-01 (4.0161e-01)	Acc@1  83.00 ( 86.24)	Acc@5 100.00 ( 99.52)
Test: [ 30/100]	Time  0.018 ( 0.022)	Loss 4.8389e-01 (4.1091e-01)	Acc@1  85.00 ( 86.26)	Acc@5  99.00 ( 99.58)
Test: [ 40/100]	Time  0.014 ( 0.021)	Loss 3.6963e-01 (4.0259e-01)	Acc@1  89.00 ( 86.37)	Acc@5  98.00 ( 99.51)
Test: [ 50/100]	Time  0.014 ( 0.020)	Loss 3.2642e-01 (4.0172e-01)	Acc@1  90.00 ( 86.45)	Acc@5 100.00 ( 99.53)
Test: [ 60/100]	Time  0.019 ( 0.020)	Loss 3.0469e-01 (3.9730e-01)	Acc@1  89.00 ( 86.54)	Acc@5 100.00 ( 99.56)
Test: [ 70/100]	Time  0.015 ( 0.019)	Loss 5.4199e-01 (3.9557e-01)	Acc@1  83.00 ( 86.65)	Acc@5 100.00 ( 99.58)
Test: [ 80/100]	Time  0.014 ( 0.019)	Loss 4.1797e-01 (3.9884e-01)	Acc@1  82.00 ( 86.47)	Acc@5  99.00 ( 99.60)
Test: [ 90/100]	Time  0.014 ( 0.018)	Loss 3.3301e-01 (3.9754e-01)	Acc@1  91.00 ( 86.58)	Acc@5 100.00 ( 99.63)
 * Acc@1 86.590 Acc@5 99.620
### epoch[17] execution time: 15.182672500610352
EPOCH 18
# current learning rate: 0.1
# Switched to train mode...
Epoch: [18][  0/391]	Time  0.213 ( 0.213)	Data  0.178 ( 0.178)	Loss 3.1030e-01 (3.1030e-01)	Acc@1  89.06 ( 89.06)	Acc@5 100.00 (100.00)
Epoch: [18][ 10/391]	Time  0.032 ( 0.051)	Data  0.001 ( 0.018)	Loss 2.3865e-01 (2.8197e-01)	Acc@1  93.75 ( 90.20)	Acc@5  99.22 ( 99.86)
Epoch: [18][ 20/391]	Time  0.033 ( 0.042)	Data  0.001 ( 0.010)	Loss 2.2119e-01 (2.6456e-01)	Acc@1  93.75 ( 91.07)	Acc@5  99.22 ( 99.89)
Epoch: [18][ 30/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.007)	Loss 2.4438e-01 (2.6787e-01)	Acc@1  90.62 ( 90.73)	Acc@5 100.00 ( 99.90)
Epoch: [18][ 40/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.006)	Loss 2.1680e-01 (2.6422e-01)	Acc@1  92.97 ( 90.85)	Acc@5 100.00 ( 99.89)
Epoch: [18][ 50/391]	Time  0.031 ( 0.037)	Data  0.001 ( 0.005)	Loss 3.2349e-01 (2.6557e-01)	Acc@1  86.72 ( 90.81)	Acc@5  99.22 ( 99.85)
Epoch: [18][ 60/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 2.7686e-01 (2.6558e-01)	Acc@1  92.19 ( 90.75)	Acc@5  99.22 ( 99.85)
Epoch: [18][ 70/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 2.4207e-01 (2.6104e-01)	Acc@1  92.19 ( 90.97)	Acc@5 100.00 ( 99.83)
Epoch: [18][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 3.0469e-01 (2.5814e-01)	Acc@1  88.28 ( 91.04)	Acc@5  99.22 ( 99.84)
Epoch: [18][ 90/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 3.6035e-01 (2.6238e-01)	Acc@1  86.72 ( 90.88)	Acc@5 100.00 ( 99.84)
Epoch: [18][100/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 3.4106e-01 (2.6849e-01)	Acc@1  92.19 ( 90.62)	Acc@5 100.00 ( 99.83)
Epoch: [18][110/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.4866e-01 (2.6616e-01)	Acc@1  89.84 ( 90.67)	Acc@5 100.00 ( 99.84)
Epoch: [18][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.7881e-01 (2.6915e-01)	Acc@1  89.84 ( 90.50)	Acc@5 100.00 ( 99.84)
Epoch: [18][130/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.4829e-01 (2.7203e-01)	Acc@1  90.62 ( 90.42)	Acc@5 100.00 ( 99.82)
Epoch: [18][140/391]	Time  0.042 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.3499e-01 (2.7136e-01)	Acc@1  93.75 ( 90.46)	Acc@5 100.00 ( 99.81)
Epoch: [18][150/391]	Time  0.037 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.7236e-01 (2.7130e-01)	Acc@1  95.31 ( 90.46)	Acc@5 100.00 ( 99.80)
Epoch: [18][160/391]	Time  0.037 ( 0.034)	Data  0.000 ( 0.003)	Loss 3.4082e-01 (2.7064e-01)	Acc@1  88.28 ( 90.51)	Acc@5 100.00 ( 99.81)
Epoch: [18][170/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.6196e-01 (2.6988e-01)	Acc@1  89.06 ( 90.53)	Acc@5 100.00 ( 99.81)
Epoch: [18][180/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.7417e-01 (2.6867e-01)	Acc@1  88.28 ( 90.52)	Acc@5 100.00 ( 99.81)
Epoch: [18][190/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.4888e-01 (2.6811e-01)	Acc@1  86.72 ( 90.53)	Acc@5 100.00 ( 99.80)
Epoch: [18][200/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.5303e-01 (2.7016e-01)	Acc@1  85.94 ( 90.50)	Acc@5  99.22 ( 99.79)
Epoch: [18][210/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.6758e-01 (2.7049e-01)	Acc@1  92.19 ( 90.49)	Acc@5 100.00 ( 99.80)
Epoch: [18][220/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.4302e-01 (2.7270e-01)	Acc@1  87.50 ( 90.36)	Acc@5 100.00 ( 99.81)
Epoch: [18][230/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.6987e-01 (2.7327e-01)	Acc@1  85.94 ( 90.33)	Acc@5  98.44 ( 99.78)
Epoch: [18][240/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.7368e-01 (2.7404e-01)	Acc@1  91.41 ( 90.31)	Acc@5 100.00 ( 99.78)
Epoch: [18][250/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.4434e-01 (2.7436e-01)	Acc@1  85.94 ( 90.34)	Acc@5  99.22 ( 99.77)
Epoch: [18][260/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.0618e-01 (2.7377e-01)	Acc@1  91.41 ( 90.37)	Acc@5  99.22 ( 99.78)
Epoch: [18][270/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.5767e-01 (2.7506e-01)	Acc@1  89.84 ( 90.35)	Acc@5 100.00 ( 99.77)
Epoch: [18][280/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.6182e-01 (2.7502e-01)	Acc@1  88.28 ( 90.37)	Acc@5  99.22 ( 99.77)
Epoch: [18][290/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.4204e-01 (2.7612e-01)	Acc@1  85.94 ( 90.31)	Acc@5 100.00 ( 99.77)
Epoch: [18][300/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.2815e-01 (2.7583e-01)	Acc@1  90.62 ( 90.32)	Acc@5 100.00 ( 99.77)
Epoch: [18][310/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.1948e-01 (2.7595e-01)	Acc@1  91.41 ( 90.34)	Acc@5 100.00 ( 99.77)
Epoch: [18][320/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.3462e-01 (2.7551e-01)	Acc@1  92.97 ( 90.35)	Acc@5 100.00 ( 99.77)
Epoch: [18][330/391]	Time  0.035 ( 0.034)	Data  0.000 ( 0.002)	Loss 2.6611e-01 (2.7601e-01)	Acc@1  93.75 ( 90.36)	Acc@5 100.00 ( 99.78)
Epoch: [18][340/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.6172e-01 (2.7584e-01)	Acc@1  90.62 ( 90.37)	Acc@5 100.00 ( 99.78)
Epoch: [18][350/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.8467e-01 (2.7481e-01)	Acc@1  89.84 ( 90.40)	Acc@5  98.44 ( 99.78)
Epoch: [18][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.1494e-01 (2.7495e-01)	Acc@1  88.28 ( 90.41)	Acc@5 100.00 ( 99.77)
Epoch: [18][370/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.7808e-01 (2.7515e-01)	Acc@1  90.62 ( 90.39)	Acc@5 100.00 ( 99.76)
Epoch: [18][380/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1969e-01 (2.7415e-01)	Acc@1  96.09 ( 90.43)	Acc@5 100.00 ( 99.77)
Epoch: [18][390/391]	Time  0.028 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.6587e-01 (2.7494e-01)	Acc@1  88.75 ( 90.40)	Acc@5 100.00 ( 99.77)
## e[18] optimizer.zero_grad (sum) time: 0.169386625289917
## e[18]       loss.backward (sum) time: 3.9500229358673096
## e[18]      optimizer.step (sum) time: 1.2316093444824219
## epoch[18] training(only) time: 13.223868131637573
# Switched to evaluate mode...
Test: [  0/100]	Time  0.164 ( 0.164)	Loss 1.9897e-01 (1.9897e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.017 ( 0.031)	Loss 4.2920e-01 (3.8046e-01)	Acc@1  86.00 ( 86.82)	Acc@5  99.00 ( 99.55)
Test: [ 20/100]	Time  0.014 ( 0.024)	Loss 3.5278e-01 (3.9846e-01)	Acc@1  84.00 ( 86.43)	Acc@5 100.00 ( 99.43)
Test: [ 30/100]	Time  0.014 ( 0.021)	Loss 4.6924e-01 (4.1595e-01)	Acc@1  81.00 ( 86.00)	Acc@5 100.00 ( 99.45)
Test: [ 40/100]	Time  0.020 ( 0.020)	Loss 4.8242e-01 (4.1076e-01)	Acc@1  82.00 ( 86.37)	Acc@5  99.00 ( 99.41)
Test: [ 50/100]	Time  0.016 ( 0.020)	Loss 3.2642e-01 (4.1962e-01)	Acc@1  90.00 ( 86.39)	Acc@5 100.00 ( 99.47)
Test: [ 60/100]	Time  0.020 ( 0.019)	Loss 4.1699e-01 (4.1732e-01)	Acc@1  89.00 ( 86.43)	Acc@5 100.00 ( 99.54)
Test: [ 70/100]	Time  0.016 ( 0.019)	Loss 5.6836e-01 (4.2188e-01)	Acc@1  84.00 ( 86.25)	Acc@5 100.00 ( 99.52)
Test: [ 80/100]	Time  0.014 ( 0.019)	Loss 2.3621e-01 (4.2484e-01)	Acc@1  93.00 ( 86.30)	Acc@5 100.00 ( 99.52)
Test: [ 90/100]	Time  0.014 ( 0.018)	Loss 2.5024e-01 (4.2640e-01)	Acc@1  89.00 ( 86.13)	Acc@5 100.00 ( 99.54)
 * Acc@1 86.200 Acc@5 99.560
### epoch[18] execution time: 15.163329839706421
EPOCH 19
# current learning rate: 0.1
# Switched to train mode...
Epoch: [19][  0/391]	Time  0.213 ( 0.213)	Data  0.180 ( 0.180)	Loss 3.1689e-01 (3.1689e-01)	Acc@1  87.50 ( 87.50)	Acc@5 100.00 (100.00)
Epoch: [19][ 10/391]	Time  0.034 ( 0.050)	Data  0.001 ( 0.018)	Loss 2.0703e-01 (2.5643e-01)	Acc@1  92.97 ( 90.62)	Acc@5 100.00 ( 99.86)
Epoch: [19][ 20/391]	Time  0.032 ( 0.042)	Data  0.001 ( 0.010)	Loss 2.9370e-01 (2.5247e-01)	Acc@1  88.28 ( 90.96)	Acc@5 100.00 ( 99.93)
Epoch: [19][ 30/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.007)	Loss 4.3286e-01 (2.6210e-01)	Acc@1  85.16 ( 90.88)	Acc@5 100.00 ( 99.87)
Epoch: [19][ 40/391]	Time  0.032 ( 0.038)	Data  0.001 ( 0.006)	Loss 2.6025e-01 (2.7071e-01)	Acc@1  90.62 ( 90.78)	Acc@5 100.00 ( 99.85)
Epoch: [19][ 50/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 1.7310e-01 (2.6607e-01)	Acc@1  93.75 ( 90.93)	Acc@5 100.00 ( 99.85)
Epoch: [19][ 60/391]	Time  0.027 ( 0.036)	Data  0.001 ( 0.004)	Loss 2.5317e-01 (2.6140e-01)	Acc@1  88.28 ( 91.14)	Acc@5 100.00 ( 99.86)
Epoch: [19][ 70/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 2.4133e-01 (2.6109e-01)	Acc@1  92.97 ( 91.13)	Acc@5 100.00 ( 99.85)
Epoch: [19][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 2.4902e-01 (2.6189e-01)	Acc@1  91.41 ( 91.14)	Acc@5  99.22 ( 99.84)
Epoch: [19][ 90/391]	Time  0.030 ( 0.035)	Data  0.001 ( 0.003)	Loss 3.7598e-01 (2.6258e-01)	Acc@1  85.94 ( 91.14)	Acc@5 100.00 ( 99.82)
Epoch: [19][100/391]	Time  0.028 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.8860e-01 (2.6087e-01)	Acc@1  91.41 ( 91.19)	Acc@5 100.00 ( 99.81)
Epoch: [19][110/391]	Time  0.026 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.3110e-01 (2.5903e-01)	Acc@1  95.31 ( 91.20)	Acc@5 100.00 ( 99.82)
Epoch: [19][120/391]	Time  0.028 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.3291e-01 (2.5923e-01)	Acc@1  95.31 ( 91.17)	Acc@5 100.00 ( 99.82)
Epoch: [19][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.8906e-01 (2.5850e-01)	Acc@1  89.84 ( 91.13)	Acc@5 100.00 ( 99.82)
Epoch: [19][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.8467e-01 (2.5968e-01)	Acc@1  91.41 ( 91.11)	Acc@5 100.00 ( 99.82)
Epoch: [19][150/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.7344e-01 (2.6110e-01)	Acc@1  89.84 ( 91.08)	Acc@5  99.22 ( 99.81)
Epoch: [19][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.3831e-01 (2.6008e-01)	Acc@1  96.09 ( 91.10)	Acc@5 100.00 ( 99.81)
Epoch: [19][170/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.3203e-01 (2.5957e-01)	Acc@1  86.72 ( 91.05)	Acc@5 100.00 ( 99.81)
Epoch: [19][180/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.9102e-01 (2.5926e-01)	Acc@1  89.84 ( 91.10)	Acc@5  99.22 ( 99.81)
Epoch: [19][190/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.8369e-01 (2.5974e-01)	Acc@1  91.41 ( 91.09)	Acc@5 100.00 ( 99.82)
Epoch: [19][200/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.1436e-01 (2.5862e-01)	Acc@1  90.62 ( 91.10)	Acc@5 100.00 ( 99.82)
Epoch: [19][210/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.5742e-01 (2.6066e-01)	Acc@1  90.62 ( 91.04)	Acc@5  98.44 ( 99.81)
Epoch: [19][220/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.9956e-01 (2.6061e-01)	Acc@1  88.28 ( 91.01)	Acc@5  99.22 ( 99.81)
Epoch: [19][230/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.4500e-01 (2.6114e-01)	Acc@1  92.19 ( 91.00)	Acc@5  99.22 ( 99.81)
Epoch: [19][240/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.5977e-01 (2.6068e-01)	Acc@1  92.19 ( 91.01)	Acc@5  99.22 ( 99.81)
Epoch: [19][250/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.8867e-01 (2.6052e-01)	Acc@1  87.50 ( 91.03)	Acc@5  98.44 ( 99.80)
Epoch: [19][260/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.1494e-01 (2.6084e-01)	Acc@1  86.72 ( 90.96)	Acc@5 100.00 ( 99.81)
Epoch: [19][270/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.1960e-01 (2.6072e-01)	Acc@1  91.41 ( 90.98)	Acc@5 100.00 ( 99.80)
Epoch: [19][280/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.4744e-01 (2.6067e-01)	Acc@1  90.62 ( 90.98)	Acc@5 100.00 ( 99.81)
Epoch: [19][290/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.3975e-01 (2.6159e-01)	Acc@1  89.84 ( 90.93)	Acc@5 100.00 ( 99.81)
Epoch: [19][300/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.4231e-01 (2.6204e-01)	Acc@1  91.41 ( 90.88)	Acc@5 100.00 ( 99.81)
Epoch: [19][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.1626e-01 (2.6259e-01)	Acc@1  87.50 ( 90.86)	Acc@5 100.00 ( 99.81)
Epoch: [19][320/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.7856e-01 (2.6251e-01)	Acc@1  89.84 ( 90.84)	Acc@5 100.00 ( 99.81)
Epoch: [19][330/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.1958e-01 (2.6177e-01)	Acc@1  87.50 ( 90.87)	Acc@5  99.22 ( 99.81)
Epoch: [19][340/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.0837e-01 (2.6058e-01)	Acc@1  91.41 ( 90.92)	Acc@5 100.00 ( 99.81)
Epoch: [19][350/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.0908e-01 (2.6067e-01)	Acc@1  86.72 ( 90.92)	Acc@5 100.00 ( 99.81)
Epoch: [19][360/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.7588e-01 (2.5964e-01)	Acc@1  89.84 ( 90.94)	Acc@5 100.00 ( 99.81)
Epoch: [19][370/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.4473e-01 (2.6038e-01)	Acc@1  88.28 ( 90.90)	Acc@5 100.00 ( 99.82)
Epoch: [19][380/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.3840e-01 (2.5988e-01)	Acc@1  92.97 ( 90.93)	Acc@5 100.00 ( 99.82)
Epoch: [19][390/391]	Time  0.022 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.8657e-01 (2.6089e-01)	Acc@1  85.00 ( 90.89)	Acc@5  98.75 ( 99.82)
## e[19] optimizer.zero_grad (sum) time: 0.16854095458984375
## e[19]       loss.backward (sum) time: 3.8828182220458984
## e[19]      optimizer.step (sum) time: 1.2505466938018799
## epoch[19] training(only) time: 13.25475287437439
# Switched to evaluate mode...
Test: [  0/100]	Time  0.169 ( 0.169)	Loss 3.2397e-01 (3.2397e-01)	Acc@1  89.00 ( 89.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.014 ( 0.030)	Loss 5.2344e-01 (3.9385e-01)	Acc@1  85.00 ( 86.18)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.017 ( 0.024)	Loss 5.1270e-01 (4.2452e-01)	Acc@1  84.00 ( 85.76)	Acc@5  99.00 ( 99.33)
Test: [ 30/100]	Time  0.015 ( 0.022)	Loss 5.4883e-01 (4.4576e-01)	Acc@1  84.00 ( 85.58)	Acc@5 100.00 ( 99.39)
Test: [ 40/100]	Time  0.021 ( 0.021)	Loss 3.7793e-01 (4.4005e-01)	Acc@1  88.00 ( 85.66)	Acc@5  99.00 ( 99.39)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 3.5156e-01 (4.3770e-01)	Acc@1  89.00 ( 85.75)	Acc@5  99.00 ( 99.35)
Test: [ 60/100]	Time  0.014 ( 0.019)	Loss 3.7158e-01 (4.3616e-01)	Acc@1  87.00 ( 85.72)	Acc@5 100.00 ( 99.41)
Test: [ 70/100]	Time  0.020 ( 0.019)	Loss 5.9131e-01 (4.3731e-01)	Acc@1  82.00 ( 85.66)	Acc@5 100.00 ( 99.42)
Test: [ 80/100]	Time  0.020 ( 0.019)	Loss 2.3694e-01 (4.3638e-01)	Acc@1  90.00 ( 85.69)	Acc@5 100.00 ( 99.44)
Test: [ 90/100]	Time  0.016 ( 0.019)	Loss 2.9004e-01 (4.3163e-01)	Acc@1  90.00 ( 85.84)	Acc@5 100.00 ( 99.51)
 * Acc@1 85.960 Acc@5 99.510
### epoch[19] execution time: 15.236566066741943
EPOCH 20
# current learning rate: 0.1
# Switched to train mode...
Epoch: [20][  0/391]	Time  0.205 ( 0.205)	Data  0.168 ( 0.168)	Loss 1.4832e-01 (1.4832e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
Epoch: [20][ 10/391]	Time  0.032 ( 0.049)	Data  0.001 ( 0.017)	Loss 2.3462e-01 (2.2250e-01)	Acc@1  88.28 ( 91.83)	Acc@5 100.00 ( 99.93)
Epoch: [20][ 20/391]	Time  0.034 ( 0.042)	Data  0.001 ( 0.010)	Loss 2.0227e-01 (2.3926e-01)	Acc@1  93.75 ( 91.70)	Acc@5 100.00 ( 99.93)
Epoch: [20][ 30/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.007)	Loss 3.6914e-01 (2.4030e-01)	Acc@1  88.28 ( 91.61)	Acc@5  99.22 ( 99.92)
Epoch: [20][ 40/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.006)	Loss 2.7222e-01 (2.4031e-01)	Acc@1  89.84 ( 91.44)	Acc@5 100.00 ( 99.94)
Epoch: [20][ 50/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 1.7261e-01 (2.4000e-01)	Acc@1  94.53 ( 91.54)	Acc@5 100.00 ( 99.94)
Epoch: [20][ 60/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 2.6416e-01 (2.4034e-01)	Acc@1  89.06 ( 91.56)	Acc@5 100.00 ( 99.92)
Epoch: [20][ 70/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.004)	Loss 3.0396e-01 (2.4148e-01)	Acc@1  90.62 ( 91.66)	Acc@5 100.00 ( 99.88)
Epoch: [20][ 80/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 2.5854e-01 (2.4212e-01)	Acc@1  89.84 ( 91.58)	Acc@5  98.44 ( 99.86)
Epoch: [20][ 90/391]	Time  0.029 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.9910e-01 (2.3922e-01)	Acc@1  92.19 ( 91.68)	Acc@5  99.22 ( 99.84)
Epoch: [20][100/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.2168e-01 (2.3719e-01)	Acc@1  92.97 ( 91.76)	Acc@5 100.00 ( 99.82)
Epoch: [20][110/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.0312e-01 (2.3783e-01)	Acc@1  92.97 ( 91.81)	Acc@5 100.00 ( 99.81)
Epoch: [20][120/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.1765e-01 (2.4076e-01)	Acc@1  91.41 ( 91.65)	Acc@5 100.00 ( 99.79)
Epoch: [20][130/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.3193e-01 (2.3832e-01)	Acc@1  92.97 ( 91.69)	Acc@5 100.00 ( 99.80)
Epoch: [20][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.3083e-01 (2.3972e-01)	Acc@1  93.75 ( 91.66)	Acc@5  98.44 ( 99.79)
Epoch: [20][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.1128e-01 (2.4029e-01)	Acc@1  91.41 ( 91.67)	Acc@5 100.00 ( 99.79)
Epoch: [20][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.9189e-01 (2.4103e-01)	Acc@1  92.97 ( 91.66)	Acc@5 100.00 ( 99.77)
Epoch: [20][170/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.9492e-01 (2.4114e-01)	Acc@1  91.41 ( 91.65)	Acc@5 100.00 ( 99.78)
Epoch: [20][180/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.8811e-01 (2.4118e-01)	Acc@1  95.31 ( 91.67)	Acc@5  99.22 ( 99.77)
Epoch: [20][190/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.4170e-01 (2.4174e-01)	Acc@1  89.84 ( 91.61)	Acc@5 100.00 ( 99.77)
Epoch: [20][200/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.6768e-01 (2.4451e-01)	Acc@1  86.72 ( 91.55)	Acc@5  99.22 ( 99.76)
Epoch: [20][210/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.7441e-01 (2.4555e-01)	Acc@1  88.28 ( 91.50)	Acc@5 100.00 ( 99.76)
Epoch: [20][220/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.1265e-01 (2.4656e-01)	Acc@1  93.75 ( 91.48)	Acc@5 100.00 ( 99.75)
Epoch: [20][230/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.0654e-01 (2.4627e-01)	Acc@1  90.62 ( 91.44)	Acc@5 100.00 ( 99.75)
Epoch: [20][240/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.3010e-01 (2.4690e-01)	Acc@1  91.41 ( 91.42)	Acc@5 100.00 ( 99.75)
Epoch: [20][250/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.5635e-01 (2.4841e-01)	Acc@1  88.28 ( 91.41)	Acc@5  99.22 ( 99.75)
Epoch: [20][260/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.4695e-01 (2.4887e-01)	Acc@1  91.41 ( 91.36)	Acc@5 100.00 ( 99.75)
Epoch: [20][270/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.5327e-01 (2.4893e-01)	Acc@1  88.28 ( 91.35)	Acc@5 100.00 ( 99.76)
Epoch: [20][280/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.2627e-01 (2.4934e-01)	Acc@1  85.94 ( 91.32)	Acc@5  99.22 ( 99.76)
Epoch: [20][290/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.1143e-01 (2.4976e-01)	Acc@1  91.41 ( 91.29)	Acc@5 100.00 ( 99.76)
Epoch: [20][300/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.1934e-01 (2.5009e-01)	Acc@1  88.28 ( 91.27)	Acc@5 100.00 ( 99.77)
Epoch: [20][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.4768e-01 (2.5039e-01)	Acc@1  92.19 ( 91.27)	Acc@5 100.00 ( 99.77)
Epoch: [20][320/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5894e-01 (2.5034e-01)	Acc@1  93.75 ( 91.28)	Acc@5 100.00 ( 99.77)
Epoch: [20][330/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.3914e-01 (2.5001e-01)	Acc@1  92.19 ( 91.27)	Acc@5  99.22 ( 99.78)
Epoch: [20][340/391]	Time  0.038 ( 0.034)	Data  0.000 ( 0.002)	Loss 1.8652e-01 (2.4927e-01)	Acc@1  94.53 ( 91.29)	Acc@5  99.22 ( 99.78)
Epoch: [20][350/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5686e-01 (2.4807e-01)	Acc@1  96.09 ( 91.33)	Acc@5 100.00 ( 99.78)
Epoch: [20][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.5415e-01 (2.4736e-01)	Acc@1  89.84 ( 91.36)	Acc@5 100.00 ( 99.78)
Epoch: [20][370/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.3828e-01 (2.4840e-01)	Acc@1  92.19 ( 91.33)	Acc@5 100.00 ( 99.79)
Epoch: [20][380/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.7881e-01 (2.4869e-01)	Acc@1  89.06 ( 91.32)	Acc@5 100.00 ( 99.79)
Epoch: [20][390/391]	Time  0.022 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.0542e-01 (2.4927e-01)	Acc@1  91.25 ( 91.31)	Acc@5 100.00 ( 99.79)
## e[20] optimizer.zero_grad (sum) time: 0.16896295547485352
## e[20]       loss.backward (sum) time: 3.870049476623535
## e[20]      optimizer.step (sum) time: 1.2583401203155518
## epoch[20] training(only) time: 13.317005157470703
# Switched to evaluate mode...
Test: [  0/100]	Time  0.164 ( 0.164)	Loss 3.7500e-01 (3.7500e-01)	Acc@1  87.00 ( 87.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.015 ( 0.031)	Loss 4.3042e-01 (3.8837e-01)	Acc@1  83.00 ( 87.18)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.022 ( 0.024)	Loss 3.1543e-01 (3.7152e-01)	Acc@1  90.00 ( 87.43)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.013 ( 0.022)	Loss 4.0015e-01 (3.7929e-01)	Acc@1  88.00 ( 87.16)	Acc@5 100.00 ( 99.45)
Test: [ 40/100]	Time  0.019 ( 0.021)	Loss 3.6279e-01 (3.8444e-01)	Acc@1  90.00 ( 87.12)	Acc@5  98.00 ( 99.37)
Test: [ 50/100]	Time  0.015 ( 0.020)	Loss 2.6538e-01 (3.8199e-01)	Acc@1  89.00 ( 87.43)	Acc@5 100.00 ( 99.35)
Test: [ 60/100]	Time  0.014 ( 0.019)	Loss 2.9932e-01 (3.7586e-01)	Acc@1  93.00 ( 87.77)	Acc@5 100.00 ( 99.41)
Test: [ 70/100]	Time  0.019 ( 0.019)	Loss 3.9941e-01 (3.8283e-01)	Acc@1  87.00 ( 87.52)	Acc@5 100.00 ( 99.41)
Test: [ 80/100]	Time  0.015 ( 0.019)	Loss 3.2861e-01 (3.7964e-01)	Acc@1  89.00 ( 87.74)	Acc@5 100.00 ( 99.46)
Test: [ 90/100]	Time  0.014 ( 0.019)	Loss 2.6025e-01 (3.8406e-01)	Acc@1  89.00 ( 87.53)	Acc@5 100.00 ( 99.49)
 * Acc@1 87.510 Acc@5 99.530
### epoch[20] execution time: 15.260767936706543
EPOCH 21
# current learning rate: 0.1
# Switched to train mode...
Epoch: [21][  0/391]	Time  0.211 ( 0.211)	Data  0.171 ( 0.171)	Loss 1.9336e-01 (1.9336e-01)	Acc@1  92.97 ( 92.97)	Acc@5 100.00 (100.00)
Epoch: [21][ 10/391]	Time  0.034 ( 0.051)	Data  0.001 ( 0.017)	Loss 2.1533e-01 (2.1721e-01)	Acc@1  90.62 ( 92.40)	Acc@5 100.00 ( 99.86)
Epoch: [21][ 20/391]	Time  0.033 ( 0.042)	Data  0.001 ( 0.010)	Loss 2.3401e-01 (2.1723e-01)	Acc@1  92.19 ( 92.41)	Acc@5 100.00 ( 99.89)
Epoch: [21][ 30/391]	Time  0.033 ( 0.040)	Data  0.001 ( 0.007)	Loss 2.4475e-01 (2.2404e-01)	Acc@1  93.75 ( 92.31)	Acc@5  99.22 ( 99.82)
Epoch: [21][ 40/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.006)	Loss 3.5913e-01 (2.2112e-01)	Acc@1  88.28 ( 92.44)	Acc@5  99.22 ( 99.83)
Epoch: [21][ 50/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 1.9421e-01 (2.1995e-01)	Acc@1  92.19 ( 92.42)	Acc@5 100.00 ( 99.86)
Epoch: [21][ 60/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 2.6538e-01 (2.1945e-01)	Acc@1  89.06 ( 92.49)	Acc@5 100.00 ( 99.87)
Epoch: [21][ 70/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 2.6562e-01 (2.2792e-01)	Acc@1  89.06 ( 92.10)	Acc@5  99.22 ( 99.87)
Epoch: [21][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 2.1765e-01 (2.2820e-01)	Acc@1  89.84 ( 92.08)	Acc@5 100.00 ( 99.86)
Epoch: [21][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.6162e-01 (2.2726e-01)	Acc@1  92.97 ( 92.07)	Acc@5 100.00 ( 99.87)
Epoch: [21][100/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.4282e-01 (2.2564e-01)	Acc@1  94.53 ( 92.09)	Acc@5 100.00 ( 99.86)
Epoch: [21][110/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.2607e-01 (2.2696e-01)	Acc@1  93.75 ( 92.03)	Acc@5  98.44 ( 99.86)
Epoch: [21][120/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.9299e-01 (2.2702e-01)	Acc@1  92.97 ( 92.05)	Acc@5 100.00 ( 99.86)
Epoch: [21][130/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.2778e-01 (2.2808e-01)	Acc@1  92.19 ( 92.01)	Acc@5 100.00 ( 99.85)
Epoch: [21][140/391]	Time  0.028 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.0154e-01 (2.2857e-01)	Acc@1  93.75 ( 91.99)	Acc@5 100.00 ( 99.84)
Epoch: [21][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.4341e-01 (2.2890e-01)	Acc@1  89.84 ( 91.97)	Acc@5 100.00 ( 99.83)
Epoch: [21][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.9429e-01 (2.2958e-01)	Acc@1  87.50 ( 91.95)	Acc@5 100.00 ( 99.84)
Epoch: [21][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.8809e-01 (2.2972e-01)	Acc@1  87.50 ( 91.95)	Acc@5 100.00 ( 99.85)
Epoch: [21][180/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.5464e-01 (2.3064e-01)	Acc@1  92.97 ( 91.94)	Acc@5  99.22 ( 99.84)
Epoch: [21][190/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.3301e-01 (2.3176e-01)	Acc@1  85.94 ( 91.87)	Acc@5 100.00 ( 99.85)
Epoch: [21][200/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.7188e-01 (2.3278e-01)	Acc@1  93.75 ( 91.85)	Acc@5 100.00 ( 99.83)
Epoch: [21][210/391]	Time  0.026 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.7783e-01 (2.3269e-01)	Acc@1  88.28 ( 91.86)	Acc@5 100.00 ( 99.83)
Epoch: [21][220/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4954e-01 (2.3273e-01)	Acc@1  93.75 ( 91.88)	Acc@5 100.00 ( 99.83)
Epoch: [21][230/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.7729e-01 (2.3321e-01)	Acc@1  82.81 ( 91.85)	Acc@5  99.22 ( 99.84)
Epoch: [21][240/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9641e-01 (2.3298e-01)	Acc@1  93.75 ( 91.85)	Acc@5 100.00 ( 99.84)
Epoch: [21][250/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.1470e-01 (2.3310e-01)	Acc@1  89.84 ( 91.87)	Acc@5 100.00 ( 99.84)
Epoch: [21][260/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.0203e-01 (2.3369e-01)	Acc@1  91.41 ( 91.87)	Acc@5 100.00 ( 99.84)
Epoch: [21][270/391]	Time  0.037 ( 0.034)	Data  0.002 ( 0.002)	Loss 2.4487e-01 (2.3447e-01)	Acc@1  92.97 ( 91.85)	Acc@5 100.00 ( 99.84)
Epoch: [21][280/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.1106e-01 (2.3483e-01)	Acc@1  94.53 ( 91.86)	Acc@5 100.00 ( 99.83)
Epoch: [21][290/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.1958e-01 (2.3580e-01)	Acc@1  87.50 ( 91.83)	Acc@5 100.00 ( 99.83)
Epoch: [21][300/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.1936e-01 (2.3541e-01)	Acc@1  92.97 ( 91.85)	Acc@5 100.00 ( 99.83)
Epoch: [21][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.3865e-01 (2.3582e-01)	Acc@1  90.62 ( 91.82)	Acc@5 100.00 ( 99.83)
Epoch: [21][320/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.1885e-01 (2.3502e-01)	Acc@1  89.06 ( 91.83)	Acc@5 100.00 ( 99.83)
Epoch: [21][330/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9189e-01 (2.3548e-01)	Acc@1  95.31 ( 91.82)	Acc@5 100.00 ( 99.84)
Epoch: [21][340/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.7090e-01 (2.3547e-01)	Acc@1  93.75 ( 91.82)	Acc@5 100.00 ( 99.84)
Epoch: [21][350/391]	Time  0.036 ( 0.034)	Data  0.005 ( 0.002)	Loss 2.0447e-01 (2.3612e-01)	Acc@1  93.75 ( 91.80)	Acc@5 100.00 ( 99.84)
Epoch: [21][360/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.0862e-01 (2.3730e-01)	Acc@1  91.41 ( 91.76)	Acc@5 100.00 ( 99.84)
Epoch: [21][370/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.1030e-01 (2.3796e-01)	Acc@1  88.28 ( 91.73)	Acc@5 100.00 ( 99.84)
Epoch: [21][380/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.5098e-01 (2.3858e-01)	Acc@1  90.62 ( 91.70)	Acc@5  99.22 ( 99.84)
Epoch: [21][390/391]	Time  0.023 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.1594e-01 (2.3853e-01)	Acc@1  93.75 ( 91.70)	Acc@5 100.00 ( 99.84)
## e[21] optimizer.zero_grad (sum) time: 0.16983723640441895
## e[21]       loss.backward (sum) time: 3.88043212890625
## e[21]      optimizer.step (sum) time: 1.2535409927368164
## epoch[21] training(only) time: 13.305999755859375
# Switched to evaluate mode...
Test: [  0/100]	Time  0.167 ( 0.167)	Loss 4.7217e-01 (4.7217e-01)	Acc@1  89.00 ( 89.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.020 ( 0.032)	Loss 5.0830e-01 (3.6538e-01)	Acc@1  85.00 ( 88.55)	Acc@5 100.00 ( 99.36)
Test: [ 20/100]	Time  0.017 ( 0.025)	Loss 3.7866e-01 (3.7082e-01)	Acc@1  86.00 ( 88.05)	Acc@5  99.00 ( 99.52)
Test: [ 30/100]	Time  0.016 ( 0.022)	Loss 3.5815e-01 (3.8661e-01)	Acc@1  85.00 ( 87.68)	Acc@5 100.00 ( 99.48)
Test: [ 40/100]	Time  0.020 ( 0.021)	Loss 3.4766e-01 (3.8402e-01)	Acc@1  92.00 ( 88.00)	Acc@5  99.00 ( 99.44)
Test: [ 50/100]	Time  0.015 ( 0.020)	Loss 3.5156e-01 (3.7849e-01)	Acc@1  91.00 ( 88.20)	Acc@5 100.00 ( 99.49)
Test: [ 60/100]	Time  0.021 ( 0.020)	Loss 2.4048e-01 (3.7470e-01)	Acc@1  92.00 ( 88.16)	Acc@5 100.00 ( 99.52)
Test: [ 70/100]	Time  0.016 ( 0.019)	Loss 6.0010e-01 (3.7864e-01)	Acc@1  83.00 ( 87.97)	Acc@5  99.00 ( 99.55)
Test: [ 80/100]	Time  0.021 ( 0.019)	Loss 3.0444e-01 (3.7771e-01)	Acc@1  88.00 ( 87.95)	Acc@5 100.00 ( 99.57)
Test: [ 90/100]	Time  0.020 ( 0.019)	Loss 3.4204e-01 (3.7869e-01)	Acc@1  88.00 ( 87.79)	Acc@5 100.00 ( 99.59)
 * Acc@1 87.720 Acc@5 99.610
### epoch[21] execution time: 15.293336868286133
EPOCH 22
# current learning rate: 0.1
# Switched to train mode...
Epoch: [22][  0/391]	Time  0.229 ( 0.229)	Data  0.194 ( 0.194)	Loss 2.6978e-01 (2.6978e-01)	Acc@1  88.28 ( 88.28)	Acc@5 100.00 (100.00)
Epoch: [22][ 10/391]	Time  0.033 ( 0.052)	Data  0.001 ( 0.019)	Loss 1.4526e-01 (2.2892e-01)	Acc@1  94.53 ( 92.12)	Acc@5 100.00 ( 99.79)
Epoch: [22][ 20/391]	Time  0.037 ( 0.043)	Data  0.001 ( 0.011)	Loss 2.5854e-01 (2.1824e-01)	Acc@1  92.97 ( 92.41)	Acc@5  99.22 ( 99.81)
Epoch: [22][ 30/391]	Time  0.032 ( 0.040)	Data  0.001 ( 0.008)	Loss 1.8921e-01 (2.1247e-01)	Acc@1  92.97 ( 92.49)	Acc@5 100.00 ( 99.87)
Epoch: [22][ 40/391]	Time  0.032 ( 0.038)	Data  0.001 ( 0.006)	Loss 2.3145e-01 (2.1886e-01)	Acc@1  91.41 ( 92.34)	Acc@5  99.22 ( 99.85)
Epoch: [22][ 50/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 1.8262e-01 (2.1426e-01)	Acc@1  93.75 ( 92.60)	Acc@5 100.00 ( 99.83)
Epoch: [22][ 60/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.005)	Loss 2.1936e-01 (2.1345e-01)	Acc@1  92.19 ( 92.70)	Acc@5 100.00 ( 99.81)
Epoch: [22][ 70/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.004)	Loss 2.0459e-01 (2.1238e-01)	Acc@1  94.53 ( 92.72)	Acc@5 100.00 ( 99.79)
Epoch: [22][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 2.5830e-01 (2.1449e-01)	Acc@1  90.62 ( 92.65)	Acc@5 100.00 ( 99.80)
Epoch: [22][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 3.4058e-01 (2.1439e-01)	Acc@1  86.72 ( 92.61)	Acc@5 100.00 ( 99.80)
Epoch: [22][100/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.5137e-01 (2.1470e-01)	Acc@1  92.97 ( 92.53)	Acc@5 100.00 ( 99.81)
Epoch: [22][110/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.9031e-01 (2.1435e-01)	Acc@1  92.19 ( 92.48)	Acc@5 100.00 ( 99.82)
Epoch: [22][120/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.5430e-01 (2.1571e-01)	Acc@1  95.31 ( 92.43)	Acc@5 100.00 ( 99.82)
Epoch: [22][130/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.3877e-01 (2.1753e-01)	Acc@1  92.19 ( 92.43)	Acc@5 100.00 ( 99.83)
Epoch: [22][140/391]	Time  0.037 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.2646e-01 (2.1816e-01)	Acc@1  96.09 ( 92.50)	Acc@5 100.00 ( 99.82)
Epoch: [22][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.3828e-01 (2.2219e-01)	Acc@1  89.06 ( 92.34)	Acc@5 100.00 ( 99.83)
Epoch: [22][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.0850e-01 (2.2002e-01)	Acc@1  91.41 ( 92.42)	Acc@5 100.00 ( 99.84)
Epoch: [22][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.8784e-01 (2.2056e-01)	Acc@1  89.06 ( 92.37)	Acc@5 100.00 ( 99.85)
Epoch: [22][180/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.9751e-01 (2.1947e-01)	Acc@1  94.53 ( 92.42)	Acc@5 100.00 ( 99.85)
Epoch: [22][190/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.7905e-01 (2.2092e-01)	Acc@1  89.84 ( 92.36)	Acc@5 100.00 ( 99.84)
Epoch: [22][200/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.2485e-01 (2.2414e-01)	Acc@1  95.31 ( 92.27)	Acc@5 100.00 ( 99.84)
Epoch: [22][210/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.5220e-01 (2.2588e-01)	Acc@1  93.75 ( 92.21)	Acc@5  99.22 ( 99.84)
Epoch: [22][220/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.6638e-01 (2.2530e-01)	Acc@1  95.31 ( 92.22)	Acc@5  99.22 ( 99.84)
Epoch: [22][230/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.2083e-01 (2.2611e-01)	Acc@1  90.62 ( 92.17)	Acc@5 100.00 ( 99.84)
Epoch: [22][240/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9580e-01 (2.2518e-01)	Acc@1  93.75 ( 92.21)	Acc@5 100.00 ( 99.84)
Epoch: [22][250/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.1985e-01 (2.2605e-01)	Acc@1  92.97 ( 92.18)	Acc@5 100.00 ( 99.84)
Epoch: [22][260/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.3657e-01 (2.2572e-01)	Acc@1  89.84 ( 92.20)	Acc@5 100.00 ( 99.84)
Epoch: [22][270/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.6611e-01 (2.2607e-01)	Acc@1  89.84 ( 92.16)	Acc@5 100.00 ( 99.85)
Epoch: [22][280/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.9565e-01 (2.2648e-01)	Acc@1  90.62 ( 92.16)	Acc@5  99.22 ( 99.85)
Epoch: [22][290/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.2668e-01 (2.2592e-01)	Acc@1  92.97 ( 92.19)	Acc@5  99.22 ( 99.85)
Epoch: [22][300/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9543e-01 (2.2561e-01)	Acc@1  94.53 ( 92.21)	Acc@5 100.00 ( 99.85)
Epoch: [22][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.6416e-01 (2.2553e-01)	Acc@1  89.06 ( 92.21)	Acc@5 100.00 ( 99.85)
Epoch: [22][320/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.8372e-01 (2.2617e-01)	Acc@1  93.75 ( 92.20)	Acc@5 100.00 ( 99.85)
Epoch: [22][330/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.1055e-01 (2.2691e-01)	Acc@1  87.50 ( 92.15)	Acc@5  99.22 ( 99.85)
Epoch: [22][340/391]	Time  0.037 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.9858e-01 (2.2773e-01)	Acc@1  91.41 ( 92.14)	Acc@5  99.22 ( 99.85)
Epoch: [22][350/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.3083e-01 (2.2716e-01)	Acc@1  92.19 ( 92.15)	Acc@5 100.00 ( 99.85)
Epoch: [22][360/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.0593e-01 (2.2673e-01)	Acc@1  92.19 ( 92.18)	Acc@5 100.00 ( 99.84)
Epoch: [22][370/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9055e-01 (2.2655e-01)	Acc@1  92.97 ( 92.17)	Acc@5 100.00 ( 99.85)
Epoch: [22][380/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.6602e-01 (2.2667e-01)	Acc@1  94.53 ( 92.16)	Acc@5  99.22 ( 99.84)
Epoch: [22][390/391]	Time  0.026 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.5000e-01 (2.2653e-01)	Acc@1  93.75 ( 92.17)	Acc@5 100.00 ( 99.84)
## e[22] optimizer.zero_grad (sum) time: 0.1693735122680664
## e[22]       loss.backward (sum) time: 4.073801279067993
## e[22]      optimizer.step (sum) time: 1.1987473964691162
## epoch[22] training(only) time: 13.268822193145752
# Switched to evaluate mode...
Test: [  0/100]	Time  0.173 ( 0.173)	Loss 3.1519e-01 (3.1519e-01)	Acc@1  90.00 ( 90.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.015 ( 0.030)	Loss 4.5312e-01 (3.4330e-01)	Acc@1  81.00 ( 87.18)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.015 ( 0.024)	Loss 4.2749e-01 (3.5337e-01)	Acc@1  84.00 ( 87.43)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.014 ( 0.022)	Loss 3.5010e-01 (3.6949e-01)	Acc@1  90.00 ( 87.48)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.015 ( 0.021)	Loss 2.9224e-01 (3.7068e-01)	Acc@1  90.00 ( 87.78)	Acc@5 100.00 ( 99.59)
Test: [ 50/100]	Time  0.016 ( 0.020)	Loss 3.2715e-01 (3.7575e-01)	Acc@1  89.00 ( 87.80)	Acc@5 100.00 ( 99.61)
Test: [ 60/100]	Time  0.014 ( 0.019)	Loss 2.8931e-01 (3.6976e-01)	Acc@1  91.00 ( 87.89)	Acc@5 100.00 ( 99.64)
Test: [ 70/100]	Time  0.015 ( 0.019)	Loss 4.3774e-01 (3.7091e-01)	Acc@1  87.00 ( 87.89)	Acc@5 100.00 ( 99.65)
Test: [ 80/100]	Time  0.019 ( 0.019)	Loss 2.9297e-01 (3.7097e-01)	Acc@1  91.00 ( 88.00)	Acc@5 100.00 ( 99.65)
Test: [ 90/100]	Time  0.026 ( 0.018)	Loss 2.8711e-01 (3.7443e-01)	Acc@1  90.00 ( 88.00)	Acc@5 100.00 ( 99.64)
 * Acc@1 88.000 Acc@5 99.640
### epoch[22] execution time: 15.210337400436401
EPOCH 23
# current learning rate: 0.1
# Switched to train mode...
Epoch: [23][  0/391]	Time  0.201 ( 0.201)	Data  0.165 ( 0.165)	Loss 1.5955e-01 (1.5955e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
Epoch: [23][ 10/391]	Time  0.032 ( 0.048)	Data  0.001 ( 0.016)	Loss 2.4365e-01 (2.1199e-01)	Acc@1  91.41 ( 92.19)	Acc@5 100.00 ( 99.93)
Epoch: [23][ 20/391]	Time  0.033 ( 0.042)	Data  0.001 ( 0.009)	Loss 1.7932e-01 (2.0210e-01)	Acc@1  92.19 ( 92.86)	Acc@5 100.00 ( 99.85)
Epoch: [23][ 30/391]	Time  0.030 ( 0.039)	Data  0.001 ( 0.007)	Loss 1.6455e-01 (2.0203e-01)	Acc@1  94.53 ( 92.72)	Acc@5 100.00 ( 99.90)
Epoch: [23][ 40/391]	Time  0.029 ( 0.037)	Data  0.001 ( 0.005)	Loss 2.0129e-01 (2.0339e-01)	Acc@1  93.75 ( 92.66)	Acc@5 100.00 ( 99.90)
Epoch: [23][ 50/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.005)	Loss 1.6748e-01 (2.0815e-01)	Acc@1  93.75 ( 92.43)	Acc@5 100.00 ( 99.88)
Epoch: [23][ 60/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.004)	Loss 2.8345e-01 (2.1187e-01)	Acc@1  89.06 ( 92.34)	Acc@5 100.00 ( 99.88)
Epoch: [23][ 70/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.8298e-01 (2.1167e-01)	Acc@1  92.97 ( 92.40)	Acc@5 100.00 ( 99.90)
Epoch: [23][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.4744e-01 (2.0885e-01)	Acc@1  92.97 ( 92.58)	Acc@5 100.00 ( 99.91)
Epoch: [23][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.6440e-01 (2.1035e-01)	Acc@1  88.28 ( 92.46)	Acc@5 100.00 ( 99.87)
Epoch: [23][100/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.7319e-01 (2.1048e-01)	Acc@1  89.06 ( 92.47)	Acc@5 100.00 ( 99.87)
Epoch: [23][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.4783e-01 (2.1076e-01)	Acc@1  96.09 ( 92.46)	Acc@5 100.00 ( 99.87)
Epoch: [23][120/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.8442e-01 (2.1404e-01)	Acc@1  88.28 ( 92.37)	Acc@5 100.00 ( 99.87)
Epoch: [23][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.1021e-01 (2.1452e-01)	Acc@1  91.41 ( 92.28)	Acc@5 100.00 ( 99.87)
Epoch: [23][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.6489e-01 (2.1379e-01)	Acc@1  90.62 ( 92.31)	Acc@5 100.00 ( 99.88)
Epoch: [23][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.9531e-01 (2.1513e-01)	Acc@1  95.31 ( 92.25)	Acc@5 100.00 ( 99.87)
Epoch: [23][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9873e-01 (2.1667e-01)	Acc@1  91.41 ( 92.17)	Acc@5 100.00 ( 99.87)
Epoch: [23][170/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.7798e-01 (2.1793e-01)	Acc@1  94.53 ( 92.15)	Acc@5 100.00 ( 99.86)
Epoch: [23][180/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.8735e-01 (2.1787e-01)	Acc@1  89.84 ( 92.20)	Acc@5 100.00 ( 99.86)
Epoch: [23][190/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.3328e-01 (2.1762e-01)	Acc@1  89.84 ( 92.21)	Acc@5 100.00 ( 99.86)
Epoch: [23][200/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.6907e-01 (2.1655e-01)	Acc@1  92.97 ( 92.27)	Acc@5 100.00 ( 99.86)
Epoch: [23][210/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.3608e-01 (2.1778e-01)	Acc@1  92.97 ( 92.21)	Acc@5  99.22 ( 99.86)
Epoch: [23][220/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4233e-01 (2.1766e-01)	Acc@1  93.75 ( 92.20)	Acc@5 100.00 ( 99.87)
Epoch: [23][230/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.1155e-01 (2.1898e-01)	Acc@1  91.41 ( 92.19)	Acc@5 100.00 ( 99.86)
Epoch: [23][240/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9739e-01 (2.2030e-01)	Acc@1  90.62 ( 92.13)	Acc@5 100.00 ( 99.86)
Epoch: [23][250/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.1533e-01 (2.2175e-01)	Acc@1  93.75 ( 92.08)	Acc@5 100.00 ( 99.87)
Epoch: [23][260/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.4475e-01 (2.2285e-01)	Acc@1  92.19 ( 92.05)	Acc@5 100.00 ( 99.87)
Epoch: [23][270/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.9844e-01 (2.2367e-01)	Acc@1  88.28 ( 92.01)	Acc@5  99.22 ( 99.86)
Epoch: [23][280/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.2510e-01 (2.2355e-01)	Acc@1  92.97 ( 92.02)	Acc@5 100.00 ( 99.87)
Epoch: [23][290/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.1252e-01 (2.2209e-01)	Acc@1  92.97 ( 92.07)	Acc@5 100.00 ( 99.87)
Epoch: [23][300/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9177e-01 (2.2185e-01)	Acc@1  94.53 ( 92.09)	Acc@5 100.00 ( 99.87)
Epoch: [23][310/391]	Time  0.033 ( 0.034)	Data  0.000 ( 0.002)	Loss 2.6733e-01 (2.2116e-01)	Acc@1  88.28 ( 92.10)	Acc@5 100.00 ( 99.87)
Epoch: [23][320/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.5132e-01 (2.2166e-01)	Acc@1  87.50 ( 92.10)	Acc@5 100.00 ( 99.87)
Epoch: [23][330/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.8882e-01 (2.2181e-01)	Acc@1  91.41 ( 92.10)	Acc@5 100.00 ( 99.87)
Epoch: [23][340/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.4451e-01 (2.2117e-01)	Acc@1  91.41 ( 92.14)	Acc@5  99.22 ( 99.87)
Epoch: [23][350/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.1057e-01 (2.2070e-01)	Acc@1  93.75 ( 92.13)	Acc@5 100.00 ( 99.87)
Epoch: [23][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.8616e-01 (2.2092e-01)	Acc@1  93.75 ( 92.14)	Acc@5  99.22 ( 99.87)
Epoch: [23][370/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9763e-01 (2.2185e-01)	Acc@1  89.84 ( 92.11)	Acc@5 100.00 ( 99.87)
Epoch: [23][380/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2878e-01 (2.2174e-01)	Acc@1  95.31 ( 92.13)	Acc@5 100.00 ( 99.87)
Epoch: [23][390/391]	Time  0.024 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.9907e-01 (2.2232e-01)	Acc@1  87.50 ( 92.12)	Acc@5 100.00 ( 99.87)
## e[23] optimizer.zero_grad (sum) time: 0.17048120498657227
## e[23]       loss.backward (sum) time: 4.035923004150391
## e[23]      optimizer.step (sum) time: 1.2214791774749756
## epoch[23] training(only) time: 13.240497827529907
# Switched to evaluate mode...
Test: [  0/100]	Time  0.162 ( 0.162)	Loss 3.7061e-01 (3.7061e-01)	Acc@1  91.00 ( 91.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.014 ( 0.030)	Loss 3.3472e-01 (3.6855e-01)	Acc@1  90.00 ( 88.82)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.014 ( 0.023)	Loss 5.1855e-01 (4.0130e-01)	Acc@1  80.00 ( 87.71)	Acc@5 100.00 ( 99.52)
Test: [ 30/100]	Time  0.014 ( 0.021)	Loss 4.7583e-01 (4.3422e-01)	Acc@1  86.00 ( 87.35)	Acc@5 100.00 ( 99.45)
Test: [ 40/100]	Time  0.014 ( 0.020)	Loss 4.1724e-01 (4.3476e-01)	Acc@1  91.00 ( 87.41)	Acc@5  99.00 ( 99.46)
Test: [ 50/100]	Time  0.021 ( 0.020)	Loss 3.1030e-01 (4.3432e-01)	Acc@1  91.00 ( 87.55)	Acc@5 100.00 ( 99.45)
Test: [ 60/100]	Time  0.020 ( 0.019)	Loss 3.0444e-01 (4.2936e-01)	Acc@1  91.00 ( 87.49)	Acc@5 100.00 ( 99.48)
Test: [ 70/100]	Time  0.020 ( 0.019)	Loss 7.1387e-01 (4.2506e-01)	Acc@1  84.00 ( 87.51)	Acc@5 100.00 ( 99.55)
Test: [ 80/100]	Time  0.020 ( 0.019)	Loss 3.5010e-01 (4.2324e-01)	Acc@1  88.00 ( 87.58)	Acc@5 100.00 ( 99.57)
Test: [ 90/100]	Time  0.013 ( 0.019)	Loss 2.8271e-01 (4.2473e-01)	Acc@1  91.00 ( 87.47)	Acc@5 100.00 ( 99.59)
 * Acc@1 87.410 Acc@5 99.600
### epoch[23] execution time: 15.199232339859009
EPOCH 24
# current learning rate: 0.1
# Switched to train mode...
Epoch: [24][  0/391]	Time  0.217 ( 0.217)	Data  0.182 ( 0.182)	Loss 2.2729e-01 (2.2729e-01)	Acc@1  91.41 ( 91.41)	Acc@5 100.00 (100.00)
Epoch: [24][ 10/391]	Time  0.032 ( 0.051)	Data  0.001 ( 0.018)	Loss 1.5002e-01 (2.1093e-01)	Acc@1  95.31 ( 92.12)	Acc@5 100.00 ( 99.86)
Epoch: [24][ 20/391]	Time  0.032 ( 0.042)	Data  0.001 ( 0.010)	Loss 1.5588e-01 (2.0338e-01)	Acc@1  96.09 ( 92.82)	Acc@5 100.00 ( 99.89)
Epoch: [24][ 30/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.007)	Loss 1.4600e-01 (1.8943e-01)	Acc@1  92.97 ( 93.20)	Acc@5 100.00 ( 99.90)
Epoch: [24][ 40/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.006)	Loss 1.3220e-01 (1.8456e-01)	Acc@1  97.66 ( 93.46)	Acc@5 100.00 ( 99.90)
Epoch: [24][ 50/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.005)	Loss 1.7834e-01 (1.8810e-01)	Acc@1  91.41 ( 93.47)	Acc@5 100.00 ( 99.89)
Epoch: [24][ 60/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.004)	Loss 1.5100e-01 (1.9025e-01)	Acc@1  92.97 ( 93.34)	Acc@5 100.00 ( 99.88)
Epoch: [24][ 70/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 2.2168e-01 (1.8955e-01)	Acc@1  93.75 ( 93.34)	Acc@5  99.22 ( 99.88)
Epoch: [24][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.9067e-01 (1.9295e-01)	Acc@1  95.31 ( 93.25)	Acc@5 100.00 ( 99.89)
Epoch: [24][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.8933e-01 (1.9449e-01)	Acc@1  92.97 ( 93.24)	Acc@5 100.00 ( 99.89)
Epoch: [24][100/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.8982e-01 (1.9652e-01)	Acc@1  95.31 ( 93.28)	Acc@5 100.00 ( 99.88)
Epoch: [24][110/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 3.4595e-01 (1.9956e-01)	Acc@1  87.50 ( 93.17)	Acc@5 100.00 ( 99.87)
Epoch: [24][120/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.1558e-01 (1.9973e-01)	Acc@1  91.41 ( 93.16)	Acc@5 100.00 ( 99.88)
Epoch: [24][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.0386e-01 (1.9899e-01)	Acc@1  93.75 ( 93.20)	Acc@5 100.00 ( 99.88)
Epoch: [24][140/391]	Time  0.039 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.2437e-01 (1.9861e-01)	Acc@1  92.19 ( 93.24)	Acc@5 100.00 ( 99.88)
Epoch: [24][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.0347e-01 (2.0071e-01)	Acc@1  89.06 ( 93.16)	Acc@5 100.00 ( 99.88)
Epoch: [24][160/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.7395e-01 (2.0065e-01)	Acc@1  93.75 ( 93.09)	Acc@5 100.00 ( 99.89)
Epoch: [24][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.8054e-01 (2.0195e-01)	Acc@1  94.53 ( 93.03)	Acc@5 100.00 ( 99.89)
Epoch: [24][180/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.1777e-01 (2.0255e-01)	Acc@1  92.19 ( 92.95)	Acc@5 100.00 ( 99.88)
Epoch: [24][190/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.6426e-01 (2.0547e-01)	Acc@1  91.41 ( 92.84)	Acc@5 100.00 ( 99.89)
Epoch: [24][200/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.8223e-01 (2.0557e-01)	Acc@1  92.19 ( 92.86)	Acc@5 100.00 ( 99.89)
Epoch: [24][210/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.4829e-01 (2.0568e-01)	Acc@1  90.62 ( 92.86)	Acc@5 100.00 ( 99.89)
Epoch: [24][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.9712e-01 (2.0749e-01)	Acc@1  89.84 ( 92.81)	Acc@5 100.00 ( 99.89)
Epoch: [24][230/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.4182e-01 (2.0814e-01)	Acc@1  91.41 ( 92.77)	Acc@5 100.00 ( 99.89)
Epoch: [24][240/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2744e-01 (2.0877e-01)	Acc@1  96.09 ( 92.74)	Acc@5 100.00 ( 99.89)
Epoch: [24][250/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.1826e-01 (2.0982e-01)	Acc@1  92.19 ( 92.70)	Acc@5 100.00 ( 99.89)
Epoch: [24][260/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.1191e-01 (2.1029e-01)	Acc@1  90.62 ( 92.69)	Acc@5 100.00 ( 99.90)
Epoch: [24][270/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.8589e-01 (2.1030e-01)	Acc@1  90.62 ( 92.67)	Acc@5 100.00 ( 99.90)
Epoch: [24][280/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3867e-01 (2.0931e-01)	Acc@1  92.97 ( 92.67)	Acc@5 100.00 ( 99.90)
Epoch: [24][290/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5723e-01 (2.0966e-01)	Acc@1  93.75 ( 92.66)	Acc@5 100.00 ( 99.90)
Epoch: [24][300/391]	Time  0.044 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.7603e-01 (2.1024e-01)	Acc@1  93.75 ( 92.65)	Acc@5 100.00 ( 99.89)
Epoch: [24][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.7834e-01 (2.1158e-01)	Acc@1  93.75 ( 92.61)	Acc@5 100.00 ( 99.88)
Epoch: [24][320/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.8955e-01 (2.1198e-01)	Acc@1  92.19 ( 92.63)	Acc@5 100.00 ( 99.88)
Epoch: [24][330/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9580e-01 (2.1174e-01)	Acc@1  94.53 ( 92.63)	Acc@5 100.00 ( 99.88)
Epoch: [24][340/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.7578e-01 (2.1133e-01)	Acc@1  92.97 ( 92.62)	Acc@5 100.00 ( 99.88)
Epoch: [24][350/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.0029e-01 (2.1075e-01)	Acc@1  87.50 ( 92.63)	Acc@5  99.22 ( 99.88)
Epoch: [24][360/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.1387e-01 (2.1097e-01)	Acc@1  93.75 ( 92.63)	Acc@5  99.22 ( 99.88)
Epoch: [24][370/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.6882e-01 (2.1041e-01)	Acc@1  92.97 ( 92.63)	Acc@5 100.00 ( 99.88)
Epoch: [24][380/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.2351e-01 (2.1073e-01)	Acc@1  90.62 ( 92.60)	Acc@5 100.00 ( 99.88)
Epoch: [24][390/391]	Time  0.024 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.8787e-01 (2.1117e-01)	Acc@1  93.75 ( 92.60)	Acc@5 100.00 ( 99.88)
## e[24] optimizer.zero_grad (sum) time: 0.16936683654785156
## e[24]       loss.backward (sum) time: 4.054240465164185
## e[24]      optimizer.step (sum) time: 1.2330801486968994
## epoch[24] training(only) time: 13.238938808441162
# Switched to evaluate mode...
Test: [  0/100]	Time  0.159 ( 0.159)	Loss 2.8662e-01 (2.8662e-01)	Acc@1  91.00 ( 91.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.025 ( 0.030)	Loss 3.6987e-01 (3.3917e-01)	Acc@1  87.00 ( 88.27)	Acc@5 100.00 ( 99.55)
Test: [ 20/100]	Time  0.017 ( 0.023)	Loss 5.6104e-01 (3.6539e-01)	Acc@1  83.00 ( 88.19)	Acc@5 100.00 ( 99.43)
Test: [ 30/100]	Time  0.014 ( 0.021)	Loss 3.4082e-01 (3.7177e-01)	Acc@1  87.00 ( 87.90)	Acc@5 100.00 ( 99.45)
Test: [ 40/100]	Time  0.021 ( 0.020)	Loss 3.3398e-01 (3.6852e-01)	Acc@1  90.00 ( 87.95)	Acc@5  99.00 ( 99.44)
Test: [ 50/100]	Time  0.013 ( 0.020)	Loss 2.6221e-01 (3.6716e-01)	Acc@1  92.00 ( 88.16)	Acc@5  99.00 ( 99.43)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 2.9785e-01 (3.6311e-01)	Acc@1  90.00 ( 88.26)	Acc@5 100.00 ( 99.48)
Test: [ 70/100]	Time  0.016 ( 0.019)	Loss 4.4409e-01 (3.6002e-01)	Acc@1  86.00 ( 88.49)	Acc@5 100.00 ( 99.51)
Test: [ 80/100]	Time  0.019 ( 0.019)	Loss 3.3008e-01 (3.6018e-01)	Acc@1  90.00 ( 88.46)	Acc@5  99.00 ( 99.54)
Test: [ 90/100]	Time  0.023 ( 0.019)	Loss 2.0325e-01 (3.6150e-01)	Acc@1  91.00 ( 88.38)	Acc@5 100.00 ( 99.56)
 * Acc@1 88.410 Acc@5 99.580
### epoch[24] execution time: 15.189519882202148
EPOCH 25
# current learning rate: 0.1
# Switched to train mode...
Epoch: [25][  0/391]	Time  0.211 ( 0.211)	Data  0.178 ( 0.178)	Loss 1.7944e-01 (1.7944e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
Epoch: [25][ 10/391]	Time  0.034 ( 0.050)	Data  0.001 ( 0.018)	Loss 2.3547e-01 (1.8122e-01)	Acc@1  92.97 ( 93.68)	Acc@5  99.22 ( 99.79)
Epoch: [25][ 20/391]	Time  0.033 ( 0.042)	Data  0.001 ( 0.010)	Loss 1.4490e-01 (1.8557e-01)	Acc@1  96.09 ( 93.60)	Acc@5 100.00 ( 99.85)
Epoch: [25][ 30/391]	Time  0.031 ( 0.039)	Data  0.001 ( 0.007)	Loss 2.8564e-01 (1.8288e-01)	Acc@1  92.19 ( 93.65)	Acc@5  98.44 ( 99.85)
Epoch: [25][ 40/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.006)	Loss 2.5366e-01 (1.8790e-01)	Acc@1  89.84 ( 93.37)	Acc@5 100.00 ( 99.89)
Epoch: [25][ 50/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.005)	Loss 1.9275e-01 (1.8802e-01)	Acc@1  93.75 ( 93.34)	Acc@5 100.00 ( 99.91)
Epoch: [25][ 60/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.004)	Loss 2.9224e-01 (1.9071e-01)	Acc@1  89.84 ( 93.26)	Acc@5 100.00 ( 99.92)
Epoch: [25][ 70/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.004)	Loss 3.3423e-01 (1.9271e-01)	Acc@1  86.72 ( 93.16)	Acc@5 100.00 ( 99.93)
Epoch: [25][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 2.5415e-01 (1.8926e-01)	Acc@1  92.19 ( 93.30)	Acc@5  98.44 ( 99.91)
Epoch: [25][ 90/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.6113e-01 (1.8846e-01)	Acc@1  94.53 ( 93.29)	Acc@5 100.00 ( 99.92)
Epoch: [25][100/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.0728e-01 (1.8955e-01)	Acc@1  93.75 ( 93.29)	Acc@5 100.00 ( 99.91)
Epoch: [25][110/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.8054e-01 (1.9040e-01)	Acc@1  95.31 ( 93.29)	Acc@5  99.22 ( 99.90)
Epoch: [25][120/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.9824e-01 (1.9066e-01)	Acc@1  92.19 ( 93.30)	Acc@5 100.00 ( 99.90)
Epoch: [25][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.0789e-01 (1.9221e-01)	Acc@1  90.62 ( 93.26)	Acc@5 100.00 ( 99.89)
Epoch: [25][140/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.6016e-01 (1.9110e-01)	Acc@1  93.75 ( 93.26)	Acc@5 100.00 ( 99.88)
Epoch: [25][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.0020e-01 (1.9153e-01)	Acc@1  92.97 ( 93.30)	Acc@5 100.00 ( 99.89)
Epoch: [25][160/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.3523e-01 (1.9383e-01)	Acc@1  90.62 ( 93.20)	Acc@5 100.00 ( 99.89)
Epoch: [25][170/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.4402e-01 (1.9409e-01)	Acc@1  92.19 ( 93.19)	Acc@5 100.00 ( 99.89)
Epoch: [25][180/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.7834e-01 (1.9478e-01)	Acc@1  90.62 ( 93.18)	Acc@5 100.00 ( 99.88)
Epoch: [25][190/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3403e-01 (1.9589e-01)	Acc@1  93.75 ( 93.14)	Acc@5 100.00 ( 99.89)
Epoch: [25][200/391]	Time  0.037 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.3022e-01 (1.9684e-01)	Acc@1  92.97 ( 93.11)	Acc@5 100.00 ( 99.88)
Epoch: [25][210/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9348e-01 (1.9626e-01)	Acc@1  91.41 ( 93.10)	Acc@5 100.00 ( 99.89)
Epoch: [25][220/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4050e-01 (1.9569e-01)	Acc@1  93.75 ( 93.11)	Acc@5 100.00 ( 99.89)
Epoch: [25][230/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.3398e-01 (1.9670e-01)	Acc@1  89.06 ( 93.06)	Acc@5 100.00 ( 99.89)
Epoch: [25][240/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.4329e-01 (1.9623e-01)	Acc@1  92.19 ( 93.07)	Acc@5 100.00 ( 99.89)
Epoch: [25][250/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5894e-01 (1.9502e-01)	Acc@1  93.75 ( 93.12)	Acc@5 100.00 ( 99.89)
Epoch: [25][260/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.3767e-01 (1.9480e-01)	Acc@1  91.41 ( 93.11)	Acc@5  99.22 ( 99.88)
Epoch: [25][270/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2903e-01 (1.9476e-01)	Acc@1  96.88 ( 93.10)	Acc@5 100.00 ( 99.89)
Epoch: [25][280/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.6248e-01 (1.9472e-01)	Acc@1  93.75 ( 93.12)	Acc@5 100.00 ( 99.89)
Epoch: [25][290/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.0923e-01 (1.9483e-01)	Acc@1  92.19 ( 93.11)	Acc@5 100.00 ( 99.89)
Epoch: [25][300/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.3706e-01 (1.9498e-01)	Acc@1  92.19 ( 93.14)	Acc@5 100.00 ( 99.89)
Epoch: [25][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.7883e-01 (1.9466e-01)	Acc@1  92.19 ( 93.14)	Acc@5  99.22 ( 99.89)
Epoch: [25][320/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5015e-01 (1.9626e-01)	Acc@1  95.31 ( 93.10)	Acc@5 100.00 ( 99.89)
Epoch: [25][330/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.4302e-01 (1.9686e-01)	Acc@1  89.06 ( 93.08)	Acc@5 100.00 ( 99.89)
Epoch: [25][340/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.4536e-01 (1.9810e-01)	Acc@1  90.62 ( 93.03)	Acc@5 100.00 ( 99.89)
Epoch: [25][350/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2305e-01 (1.9841e-01)	Acc@1  96.88 ( 93.03)	Acc@5 100.00 ( 99.88)
Epoch: [25][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.1326e-01 (1.9875e-01)	Acc@1  92.97 ( 93.04)	Acc@5 100.00 ( 99.88)
Epoch: [25][370/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4111e-01 (1.9869e-01)	Acc@1  94.53 ( 93.03)	Acc@5 100.00 ( 99.88)
Epoch: [25][380/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.3596e-01 (1.9912e-01)	Acc@1  91.41 ( 93.00)	Acc@5 100.00 ( 99.88)
Epoch: [25][390/391]	Time  0.028 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.8579e-01 (1.9922e-01)	Acc@1  92.50 ( 92.99)	Acc@5 100.00 ( 99.88)
## e[25] optimizer.zero_grad (sum) time: 0.1696159839630127
## e[25]       loss.backward (sum) time: 3.9853217601776123
## e[25]      optimizer.step (sum) time: 1.2467126846313477
## epoch[25] training(only) time: 13.226943254470825
# Switched to evaluate mode...
Test: [  0/100]	Time  0.173 ( 0.173)	Loss 3.5156e-01 (3.5156e-01)	Acc@1  88.00 ( 88.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.019 ( 0.031)	Loss 4.3823e-01 (3.7726e-01)	Acc@1  85.00 ( 87.27)	Acc@5 100.00 ( 99.45)
Test: [ 20/100]	Time  0.014 ( 0.025)	Loss 3.4058e-01 (3.6680e-01)	Acc@1  88.00 ( 87.62)	Acc@5  99.00 ( 99.43)
Test: [ 30/100]	Time  0.014 ( 0.022)	Loss 3.5327e-01 (3.7205e-01)	Acc@1  88.00 ( 87.77)	Acc@5 100.00 ( 99.45)
Test: [ 40/100]	Time  0.017 ( 0.021)	Loss 3.6548e-01 (3.6626e-01)	Acc@1  88.00 ( 87.98)	Acc@5 100.00 ( 99.46)
Test: [ 50/100]	Time  0.020 ( 0.020)	Loss 1.8933e-01 (3.6112e-01)	Acc@1  93.00 ( 88.33)	Acc@5 100.00 ( 99.49)
Test: [ 60/100]	Time  0.020 ( 0.020)	Loss 2.2961e-01 (3.6237e-01)	Acc@1  91.00 ( 88.41)	Acc@5 100.00 ( 99.52)
Test: [ 70/100]	Time  0.014 ( 0.019)	Loss 5.7861e-01 (3.6751e-01)	Acc@1  87.00 ( 88.41)	Acc@5 100.00 ( 99.52)
Test: [ 80/100]	Time  0.016 ( 0.019)	Loss 2.5659e-01 (3.6647e-01)	Acc@1  92.00 ( 88.44)	Acc@5 100.00 ( 99.53)
Test: [ 90/100]	Time  0.014 ( 0.019)	Loss 2.1118e-01 (3.6699e-01)	Acc@1  91.00 ( 88.33)	Acc@5 100.00 ( 99.54)
 * Acc@1 88.340 Acc@5 99.540
### epoch[25] execution time: 15.237164497375488
EPOCH 26
# current learning rate: 0.1
# Switched to train mode...
Epoch: [26][  0/391]	Time  0.206 ( 0.206)	Data  0.172 ( 0.172)	Loss 2.7710e-01 (2.7710e-01)	Acc@1  89.84 ( 89.84)	Acc@5 100.00 (100.00)
Epoch: [26][ 10/391]	Time  0.032 ( 0.049)	Data  0.001 ( 0.017)	Loss 1.2549e-01 (1.8746e-01)	Acc@1  95.31 ( 93.32)	Acc@5 100.00 (100.00)
Epoch: [26][ 20/391]	Time  0.034 ( 0.041)	Data  0.001 ( 0.010)	Loss 1.8127e-01 (1.7550e-01)	Acc@1  93.75 ( 93.82)	Acc@5 100.00 (100.00)
Epoch: [26][ 30/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.007)	Loss 2.1680e-01 (1.7345e-01)	Acc@1  92.97 ( 94.05)	Acc@5 100.00 (100.00)
Epoch: [26][ 40/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.006)	Loss 1.5808e-01 (1.8108e-01)	Acc@1  96.09 ( 93.73)	Acc@5 100.00 (100.00)
Epoch: [26][ 50/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.005)	Loss 2.0312e-01 (1.8571e-01)	Acc@1  93.75 ( 93.46)	Acc@5 100.00 ( 99.98)
Epoch: [26][ 60/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.004)	Loss 1.2183e-01 (1.8340e-01)	Acc@1  95.31 ( 93.53)	Acc@5 100.00 ( 99.97)
Epoch: [26][ 70/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.3525e-01 (1.8233e-01)	Acc@1  93.75 ( 93.55)	Acc@5 100.00 ( 99.96)
Epoch: [26][ 80/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.1847e-01 (1.8163e-01)	Acc@1  96.88 ( 93.58)	Acc@5 100.00 ( 99.95)
Epoch: [26][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.2988e-01 (1.7732e-01)	Acc@1  96.88 ( 93.78)	Acc@5 100.00 ( 99.96)
Epoch: [26][100/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.5659e-01 (1.7972e-01)	Acc@1  92.97 ( 93.70)	Acc@5 100.00 ( 99.95)
Epoch: [26][110/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.6528e-01 (1.8019e-01)	Acc@1  90.62 ( 93.64)	Acc@5 100.00 ( 99.94)
Epoch: [26][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.2666e-01 (1.8569e-01)	Acc@1  88.28 ( 93.45)	Acc@5 100.00 ( 99.95)
Epoch: [26][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.4233e-01 (1.8820e-01)	Acc@1  93.75 ( 93.33)	Acc@5 100.00 ( 99.94)
Epoch: [26][140/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.7808e-01 (1.9137e-01)	Acc@1  92.97 ( 93.22)	Acc@5 100.00 ( 99.94)
Epoch: [26][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.9373e-01 (1.9396e-01)	Acc@1  91.41 ( 93.10)	Acc@5 100.00 ( 99.93)
Epoch: [26][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.1753e-01 (1.9536e-01)	Acc@1  92.97 ( 93.11)	Acc@5 100.00 ( 99.93)
Epoch: [26][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.8884e-01 (1.9762e-01)	Acc@1  94.53 ( 93.06)	Acc@5 100.00 ( 99.93)
Epoch: [26][180/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.7334e-01 (1.9665e-01)	Acc@1  92.97 ( 93.09)	Acc@5  99.22 ( 99.92)
Epoch: [26][190/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.8079e-01 (1.9589e-01)	Acc@1  92.97 ( 93.10)	Acc@5 100.00 ( 99.93)
Epoch: [26][200/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.7930e-01 (1.9602e-01)	Acc@1  89.06 ( 93.11)	Acc@5 100.00 ( 99.93)
Epoch: [26][210/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.2607e-01 (1.9670e-01)	Acc@1  91.41 ( 93.13)	Acc@5 100.00 ( 99.92)
Epoch: [26][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5503e-01 (1.9567e-01)	Acc@1  92.19 ( 93.15)	Acc@5 100.00 ( 99.92)
Epoch: [26][230/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2421e-01 (1.9472e-01)	Acc@1  96.09 ( 93.20)	Acc@5 100.00 ( 99.92)
Epoch: [26][240/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.6968e-01 (1.9371e-01)	Acc@1  94.53 ( 93.23)	Acc@5 100.00 ( 99.92)
Epoch: [26][250/391]	Time  0.026 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.7468e-01 (1.9447e-01)	Acc@1  90.62 ( 93.20)	Acc@5 100.00 ( 99.92)
Epoch: [26][260/391]	Time  0.037 ( 0.034)	Data  0.005 ( 0.002)	Loss 2.0605e-01 (1.9511e-01)	Acc@1  92.97 ( 93.18)	Acc@5 100.00 ( 99.93)
Epoch: [26][270/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5247e-01 (1.9542e-01)	Acc@1  95.31 ( 93.20)	Acc@5 100.00 ( 99.93)
Epoch: [26][280/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.7075e-01 (1.9604e-01)	Acc@1  92.19 ( 93.18)	Acc@5  99.22 ( 99.92)
Epoch: [26][290/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.3657e-01 (1.9697e-01)	Acc@1  91.41 ( 93.14)	Acc@5  99.22 ( 99.92)
Epoch: [26][300/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.0776e-01 (1.9679e-01)	Acc@1  92.19 ( 93.13)	Acc@5 100.00 ( 99.92)
Epoch: [26][310/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.7139e-01 (1.9679e-01)	Acc@1  93.75 ( 93.14)	Acc@5 100.00 ( 99.92)
Epoch: [26][320/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.6392e-01 (1.9691e-01)	Acc@1  88.28 ( 93.14)	Acc@5 100.00 ( 99.92)
Epoch: [26][330/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.0225e-01 (1.9723e-01)	Acc@1  91.41 ( 93.14)	Acc@5 100.00 ( 99.92)
Epoch: [26][340/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.0776e-01 (1.9767e-01)	Acc@1  93.75 ( 93.15)	Acc@5 100.00 ( 99.92)
Epoch: [26][350/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.4670e-01 (1.9772e-01)	Acc@1  91.41 ( 93.16)	Acc@5  99.22 ( 99.91)
Epoch: [26][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9617e-01 (1.9783e-01)	Acc@1  94.53 ( 93.14)	Acc@5 100.00 ( 99.92)
Epoch: [26][370/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.2205e-01 (1.9814e-01)	Acc@1  91.41 ( 93.11)	Acc@5 100.00 ( 99.91)
Epoch: [26][380/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5942e-01 (1.9831e-01)	Acc@1  93.75 ( 93.10)	Acc@5 100.00 ( 99.92)
Epoch: [26][390/391]	Time  0.023 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.2842e-01 (1.9900e-01)	Acc@1  93.75 ( 93.08)	Acc@5 100.00 ( 99.92)
## e[26] optimizer.zero_grad (sum) time: 0.16975069046020508
## e[26]       loss.backward (sum) time: 4.01354718208313
## e[26]      optimizer.step (sum) time: 1.234400987625122
## epoch[26] training(only) time: 13.229052543640137
# Switched to evaluate mode...
Test: [  0/100]	Time  0.159 ( 0.159)	Loss 4.6582e-01 (4.6582e-01)	Acc@1  84.00 ( 84.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.021 ( 0.032)	Loss 3.5522e-01 (4.3953e-01)	Acc@1  85.00 ( 86.45)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.021 ( 0.025)	Loss 6.1719e-01 (4.7317e-01)	Acc@1  79.00 ( 85.57)	Acc@5  98.00 ( 99.52)
Test: [ 30/100]	Time  0.017 ( 0.022)	Loss 5.6299e-01 (4.7610e-01)	Acc@1  83.00 ( 85.90)	Acc@5 100.00 ( 99.61)
Test: [ 40/100]	Time  0.020 ( 0.021)	Loss 4.9194e-01 (4.8508e-01)	Acc@1  86.00 ( 85.95)	Acc@5  99.00 ( 99.49)
Test: [ 50/100]	Time  0.014 ( 0.020)	Loss 2.2827e-01 (4.7550e-01)	Acc@1  89.00 ( 86.14)	Acc@5 100.00 ( 99.47)
Test: [ 60/100]	Time  0.014 ( 0.019)	Loss 3.7329e-01 (4.7248e-01)	Acc@1  88.00 ( 86.08)	Acc@5 100.00 ( 99.49)
Test: [ 70/100]	Time  0.016 ( 0.019)	Loss 7.4512e-01 (4.7008e-01)	Acc@1  83.00 ( 86.10)	Acc@5 100.00 ( 99.52)
Test: [ 80/100]	Time  0.024 ( 0.019)	Loss 5.0879e-01 (4.7796e-01)	Acc@1  87.00 ( 86.02)	Acc@5 100.00 ( 99.53)
Test: [ 90/100]	Time  0.018 ( 0.018)	Loss 3.3374e-01 (4.7841e-01)	Acc@1  90.00 ( 85.95)	Acc@5 100.00 ( 99.55)
 * Acc@1 85.960 Acc@5 99.580
### epoch[26] execution time: 15.176496982574463
EPOCH 27
# current learning rate: 0.1
# Switched to train mode...
Epoch: [27][  0/391]	Time  0.206 ( 0.206)	Data  0.172 ( 0.172)	Loss 1.9617e-01 (1.9617e-01)	Acc@1  92.97 ( 92.97)	Acc@5 100.00 (100.00)
Epoch: [27][ 10/391]	Time  0.032 ( 0.051)	Data  0.001 ( 0.017)	Loss 1.4673e-01 (1.7189e-01)	Acc@1  96.88 ( 94.39)	Acc@5 100.00 ( 99.93)
Epoch: [27][ 20/391]	Time  0.035 ( 0.043)	Data  0.001 ( 0.010)	Loss 1.3770e-01 (1.7655e-01)	Acc@1  96.09 ( 94.12)	Acc@5 100.00 ( 99.93)
Epoch: [27][ 30/391]	Time  0.032 ( 0.040)	Data  0.001 ( 0.007)	Loss 1.2524e-01 (1.8069e-01)	Acc@1  97.66 ( 93.83)	Acc@5 100.00 ( 99.92)
Epoch: [27][ 40/391]	Time  0.032 ( 0.038)	Data  0.001 ( 0.006)	Loss 1.6138e-01 (1.8234e-01)	Acc@1  92.19 ( 93.67)	Acc@5 100.00 ( 99.89)
Epoch: [27][ 50/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 2.8271e-01 (1.8459e-01)	Acc@1  88.28 ( 93.57)	Acc@5  99.22 ( 99.88)
Epoch: [27][ 60/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.004)	Loss 1.8384e-01 (1.8836e-01)	Acc@1  93.75 ( 93.47)	Acc@5 100.00 ( 99.87)
Epoch: [27][ 70/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.004)	Loss 1.6199e-01 (1.8663e-01)	Acc@1  94.53 ( 93.53)	Acc@5 100.00 ( 99.86)
Epoch: [27][ 80/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.004)	Loss 1.4050e-01 (1.8507e-01)	Acc@1  94.53 ( 93.58)	Acc@5 100.00 ( 99.87)
Epoch: [27][ 90/391]	Time  0.040 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.3135e-01 (1.8446e-01)	Acc@1  93.75 ( 93.58)	Acc@5 100.00 ( 99.89)
Epoch: [27][100/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.7661e-01 (1.8468e-01)	Acc@1  89.06 ( 93.62)	Acc@5 100.00 ( 99.89)
Epoch: [27][110/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.5747e-01 (1.8653e-01)	Acc@1  92.97 ( 93.52)	Acc@5 100.00 ( 99.89)
Epoch: [27][120/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.3328e-01 (1.8708e-01)	Acc@1  91.41 ( 93.43)	Acc@5 100.00 ( 99.88)
Epoch: [27][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.0947e-01 (1.8653e-01)	Acc@1  90.62 ( 93.45)	Acc@5 100.00 ( 99.89)
Epoch: [27][140/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.9104e-01 (1.8639e-01)	Acc@1  92.97 ( 93.44)	Acc@5 100.00 ( 99.90)
Epoch: [27][150/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.6333e-01 (1.8736e-01)	Acc@1  95.31 ( 93.38)	Acc@5 100.00 ( 99.90)
Epoch: [27][160/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.5796e-01 (1.8791e-01)	Acc@1  92.19 ( 93.37)	Acc@5 100.00 ( 99.91)
Epoch: [27][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.7639e-01 (1.8850e-01)	Acc@1  92.97 ( 93.31)	Acc@5 100.00 ( 99.91)
Epoch: [27][180/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9824e-01 (1.9011e-01)	Acc@1  92.19 ( 93.24)	Acc@5 100.00 ( 99.92)
Epoch: [27][190/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3354e-01 (1.8849e-01)	Acc@1  96.09 ( 93.32)	Acc@5 100.00 ( 99.92)
Epoch: [27][200/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4392e-01 (1.8874e-01)	Acc@1  96.88 ( 93.31)	Acc@5 100.00 ( 99.92)
Epoch: [27][210/391]	Time  0.028 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.8591e-01 (1.8882e-01)	Acc@1  91.41 ( 93.28)	Acc@5 100.00 ( 99.92)
Epoch: [27][220/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1835e-01 (1.8882e-01)	Acc@1  93.75 ( 93.29)	Acc@5 100.00 ( 99.92)
Epoch: [27][230/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.1509e-01 (1.8794e-01)	Acc@1  92.19 ( 93.31)	Acc@5 100.00 ( 99.93)
Epoch: [27][240/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5479e-01 (1.8714e-01)	Acc@1  93.75 ( 93.36)	Acc@5  99.22 ( 99.92)
Epoch: [27][250/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0870e-01 (1.8667e-01)	Acc@1  96.09 ( 93.39)	Acc@5 100.00 ( 99.92)
Epoch: [27][260/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.7637e-01 (1.8748e-01)	Acc@1  87.50 ( 93.37)	Acc@5 100.00 ( 99.92)
Epoch: [27][270/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.6943e-01 (1.8832e-01)	Acc@1  92.97 ( 93.37)	Acc@5 100.00 ( 99.92)
Epoch: [27][280/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9482e-01 (1.8907e-01)	Acc@1  94.53 ( 93.32)	Acc@5 100.00 ( 99.91)
Epoch: [27][290/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.4792e-01 (1.9023e-01)	Acc@1  90.62 ( 93.29)	Acc@5 100.00 ( 99.92)
Epoch: [27][300/391]	Time  0.040 ( 0.034)	Data  0.002 ( 0.002)	Loss 2.0691e-01 (1.9007e-01)	Acc@1  95.31 ( 93.31)	Acc@5 100.00 ( 99.91)
Epoch: [27][310/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.7102e-01 (1.8967e-01)	Acc@1  93.75 ( 93.33)	Acc@5 100.00 ( 99.91)
Epoch: [27][320/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.6577e-01 (1.9006e-01)	Acc@1  94.53 ( 93.30)	Acc@5 100.00 ( 99.91)
Epoch: [27][330/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.6709e-01 (1.9039e-01)	Acc@1  89.84 ( 93.27)	Acc@5 100.00 ( 99.92)
Epoch: [27][340/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4441e-01 (1.9030e-01)	Acc@1  96.88 ( 93.30)	Acc@5 100.00 ( 99.92)
Epoch: [27][350/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.0166e-01 (1.8996e-01)	Acc@1  94.53 ( 93.30)	Acc@5 100.00 ( 99.92)
Epoch: [27][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9055e-01 (1.8989e-01)	Acc@1  95.31 ( 93.32)	Acc@5 100.00 ( 99.92)
Epoch: [27][370/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.0251e-01 (1.8989e-01)	Acc@1  90.62 ( 93.30)	Acc@5  99.22 ( 99.92)
Epoch: [27][380/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.3999e-01 (1.9058e-01)	Acc@1  92.19 ( 93.28)	Acc@5 100.00 ( 99.92)
Epoch: [27][390/391]	Time  0.029 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.6357e-01 (1.9073e-01)	Acc@1  91.25 ( 93.26)	Acc@5 100.00 ( 99.92)
## e[27] optimizer.zero_grad (sum) time: 0.16837739944458008
## e[27]       loss.backward (sum) time: 3.917191505432129
## e[27]      optimizer.step (sum) time: 1.2524185180664062
## epoch[27] training(only) time: 13.249940395355225
# Switched to evaluate mode...
Test: [  0/100]	Time  0.175 ( 0.175)	Loss 4.4336e-01 (4.4336e-01)	Acc@1  85.00 ( 85.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.018 ( 0.032)	Loss 4.5605e-01 (3.9431e-01)	Acc@1  89.00 ( 88.18)	Acc@5  98.00 ( 99.64)
Test: [ 20/100]	Time  0.015 ( 0.024)	Loss 3.5034e-01 (4.0510e-01)	Acc@1  90.00 ( 87.62)	Acc@5  99.00 ( 99.48)
Test: [ 30/100]	Time  0.013 ( 0.022)	Loss 3.8379e-01 (4.0076e-01)	Acc@1  88.00 ( 87.87)	Acc@5  99.00 ( 99.48)
Test: [ 40/100]	Time  0.014 ( 0.021)	Loss 3.7720e-01 (4.0086e-01)	Acc@1  88.00 ( 88.00)	Acc@5  99.00 ( 99.44)
Test: [ 50/100]	Time  0.016 ( 0.020)	Loss 3.1201e-01 (3.8934e-01)	Acc@1  89.00 ( 88.24)	Acc@5 100.00 ( 99.49)
Test: [ 60/100]	Time  0.016 ( 0.019)	Loss 2.4634e-01 (3.7957e-01)	Acc@1  92.00 ( 88.21)	Acc@5 100.00 ( 99.56)
Test: [ 70/100]	Time  0.014 ( 0.019)	Loss 4.2358e-01 (3.8153e-01)	Acc@1  90.00 ( 88.07)	Acc@5 100.00 ( 99.61)
Test: [ 80/100]	Time  0.022 ( 0.019)	Loss 3.8965e-01 (3.8216e-01)	Acc@1  88.00 ( 88.09)	Acc@5 100.00 ( 99.62)
Test: [ 90/100]	Time  0.016 ( 0.018)	Loss 3.0518e-01 (3.8077e-01)	Acc@1  91.00 ( 87.98)	Acc@5 100.00 ( 99.58)
 * Acc@1 88.120 Acc@5 99.620
### epoch[27] execution time: 15.191692352294922
EPOCH 28
# current learning rate: 0.1
# Switched to train mode...
Epoch: [28][  0/391]	Time  0.209 ( 0.209)	Data  0.173 ( 0.173)	Loss 1.6565e-01 (1.6565e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
Epoch: [28][ 10/391]	Time  0.032 ( 0.049)	Data  0.001 ( 0.017)	Loss 2.2705e-01 (1.7940e-01)	Acc@1  94.53 ( 93.96)	Acc@5  99.22 ( 99.93)
Epoch: [28][ 20/391]	Time  0.032 ( 0.041)	Data  0.001 ( 0.010)	Loss 2.0593e-01 (1.8576e-01)	Acc@1  92.19 ( 93.75)	Acc@5 100.00 ( 99.93)
Epoch: [28][ 30/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.007)	Loss 7.2571e-02 (1.8223e-01)	Acc@1  97.66 ( 93.85)	Acc@5 100.00 ( 99.90)
Epoch: [28][ 40/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.006)	Loss 1.3635e-01 (1.7993e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 ( 99.90)
Epoch: [28][ 50/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.005)	Loss 2.9492e-01 (1.8009e-01)	Acc@1  90.62 ( 93.80)	Acc@5 100.00 ( 99.92)
Epoch: [28][ 60/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.004)	Loss 2.9688e-01 (1.8149e-01)	Acc@1  92.19 ( 93.81)	Acc@5 100.00 ( 99.92)
Epoch: [28][ 70/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.004)	Loss 1.3098e-01 (1.8184e-01)	Acc@1  94.53 ( 93.78)	Acc@5 100.00 ( 99.92)
Epoch: [28][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.4905e-01 (1.8139e-01)	Acc@1  96.09 ( 93.80)	Acc@5 100.00 ( 99.91)
Epoch: [28][ 90/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.3254e-01 (1.8105e-01)	Acc@1  91.41 ( 93.78)	Acc@5 100.00 ( 99.92)
Epoch: [28][100/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.3474e-01 (1.8166e-01)	Acc@1  91.41 ( 93.77)	Acc@5 100.00 ( 99.92)
Epoch: [28][110/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.6367e-01 (1.8074e-01)	Acc@1  92.97 ( 93.76)	Acc@5 100.00 ( 99.92)
Epoch: [28][120/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.1804e-01 (1.7869e-01)	Acc@1  96.88 ( 93.85)	Acc@5 100.00 ( 99.92)
Epoch: [28][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.8335e-01 (1.7921e-01)	Acc@1  92.97 ( 93.84)	Acc@5 100.00 ( 99.92)
Epoch: [28][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.8054e-01 (1.7755e-01)	Acc@1  94.53 ( 93.92)	Acc@5  99.22 ( 99.90)
Epoch: [28][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.4146e-01 (1.7867e-01)	Acc@1  93.75 ( 93.89)	Acc@5 100.00 ( 99.90)
Epoch: [28][160/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1377e-01 (1.7830e-01)	Acc@1  96.88 ( 93.91)	Acc@5 100.00 ( 99.90)
Epoch: [28][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.6150e-01 (1.8016e-01)	Acc@1  94.53 ( 93.89)	Acc@5 100.00 ( 99.89)
Epoch: [28][180/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5332e-01 (1.8165e-01)	Acc@1  92.97 ( 93.84)	Acc@5 100.00 ( 99.88)
Epoch: [28][190/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9397e-01 (1.8170e-01)	Acc@1  92.97 ( 93.84)	Acc@5 100.00 ( 99.89)
Epoch: [28][200/391]	Time  0.038 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2769e-01 (1.8365e-01)	Acc@1  96.88 ( 93.77)	Acc@5 100.00 ( 99.89)
Epoch: [28][210/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.1350e-01 (1.8325e-01)	Acc@1  92.19 ( 93.78)	Acc@5  99.22 ( 99.89)
Epoch: [28][220/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.8206e-02 (1.8220e-01)	Acc@1  96.09 ( 93.83)	Acc@5 100.00 ( 99.89)
Epoch: [28][230/391]	Time  0.028 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.5171e-01 (1.8269e-01)	Acc@1  92.97 ( 93.81)	Acc@5  98.44 ( 99.89)
Epoch: [28][240/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.0508e-01 (1.8272e-01)	Acc@1  92.97 ( 93.80)	Acc@5 100.00 ( 99.89)
Epoch: [28][250/391]	Time  0.043 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9055e-01 (1.8268e-01)	Acc@1  90.62 ( 93.77)	Acc@5 100.00 ( 99.89)
Epoch: [28][260/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.8931e-01 (1.8358e-01)	Acc@1  89.06 ( 93.73)	Acc@5 100.00 ( 99.90)
Epoch: [28][270/391]	Time  0.028 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.5024e-01 (1.8335e-01)	Acc@1  92.97 ( 93.76)	Acc@5 100.00 ( 99.90)
Epoch: [28][280/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.3389e-01 (1.8426e-01)	Acc@1  91.41 ( 93.72)	Acc@5 100.00 ( 99.89)
Epoch: [28][290/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.8164e-01 (1.8524e-01)	Acc@1  92.97 ( 93.67)	Acc@5 100.00 ( 99.90)
Epoch: [28][300/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.5488e-01 (1.8482e-01)	Acc@1  91.41 ( 93.66)	Acc@5 100.00 ( 99.90)
Epoch: [28][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9385e-01 (1.8426e-01)	Acc@1  95.31 ( 93.70)	Acc@5 100.00 ( 99.90)
Epoch: [28][320/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1108e-01 (1.8407e-01)	Acc@1  96.09 ( 93.69)	Acc@5 100.00 ( 99.90)
Epoch: [28][330/391]	Time  0.048 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.2351e-01 (1.8511e-01)	Acc@1  92.97 ( 93.66)	Acc@5 100.00 ( 99.89)
Epoch: [28][340/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.8677e-01 (1.8588e-01)	Acc@1  94.53 ( 93.62)	Acc@5 100.00 ( 99.89)
Epoch: [28][350/391]	Time  0.028 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.8271e-01 (1.8673e-01)	Acc@1  89.84 ( 93.59)	Acc@5 100.00 ( 99.90)
Epoch: [28][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.2180e-01 (1.8799e-01)	Acc@1  91.41 ( 93.54)	Acc@5 100.00 ( 99.89)
Epoch: [28][370/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.9365e-02 (1.8834e-01)	Acc@1  96.88 ( 93.53)	Acc@5 100.00 ( 99.89)
Epoch: [28][380/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.7830e-02 (1.8874e-01)	Acc@1  97.66 ( 93.50)	Acc@5 100.00 ( 99.89)
Epoch: [28][390/391]	Time  0.027 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.0542e-01 (1.8900e-01)	Acc@1  86.25 ( 93.50)	Acc@5 100.00 ( 99.89)
## e[28] optimizer.zero_grad (sum) time: 0.16926217079162598
## e[28]       loss.backward (sum) time: 4.065376281738281
## e[28]      optimizer.step (sum) time: 1.2136292457580566
## epoch[28] training(only) time: 13.249258995056152
# Switched to evaluate mode...
Test: [  0/100]	Time  0.170 ( 0.170)	Loss 2.2107e-01 (2.2107e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.016 ( 0.031)	Loss 2.5732e-01 (3.2852e-01)	Acc@1  93.00 ( 89.73)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.024 ( 0.024)	Loss 5.3027e-01 (3.5970e-01)	Acc@1  80.00 ( 88.38)	Acc@5  99.00 ( 99.62)
Test: [ 30/100]	Time  0.019 ( 0.022)	Loss 3.6792e-01 (3.8243e-01)	Acc@1  86.00 ( 87.94)	Acc@5 100.00 ( 99.61)
Test: [ 40/100]	Time  0.027 ( 0.021)	Loss 3.0884e-01 (3.8198e-01)	Acc@1  90.00 ( 88.07)	Acc@5  99.00 ( 99.61)
Test: [ 50/100]	Time  0.020 ( 0.020)	Loss 3.6694e-01 (3.7307e-01)	Acc@1  88.00 ( 88.24)	Acc@5  99.00 ( 99.57)
Test: [ 60/100]	Time  0.020 ( 0.020)	Loss 2.6538e-01 (3.7146e-01)	Acc@1  89.00 ( 88.25)	Acc@5 100.00 ( 99.61)
Test: [ 70/100]	Time  0.018 ( 0.019)	Loss 5.6738e-01 (3.7059e-01)	Acc@1  85.00 ( 88.25)	Acc@5 100.00 ( 99.59)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 2.7905e-01 (3.6846e-01)	Acc@1  89.00 ( 88.25)	Acc@5 100.00 ( 99.59)
Test: [ 90/100]	Time  0.019 ( 0.019)	Loss 2.7124e-01 (3.7143e-01)	Acc@1  88.00 ( 88.19)	Acc@5 100.00 ( 99.60)
 * Acc@1 88.290 Acc@5 99.610
### epoch[28] execution time: 15.237240076065063
EPOCH 29
# current learning rate: 0.1
# Switched to train mode...
Epoch: [29][  0/391]	Time  0.221 ( 0.221)	Data  0.188 ( 0.188)	Loss 1.2622e-01 (1.2622e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [29][ 10/391]	Time  0.032 ( 0.051)	Data  0.001 ( 0.019)	Loss 2.1204e-01 (1.6463e-01)	Acc@1  90.62 ( 93.89)	Acc@5 100.00 ( 99.86)
Epoch: [29][ 20/391]	Time  0.032 ( 0.042)	Data  0.001 ( 0.010)	Loss 1.2354e-01 (1.6582e-01)	Acc@1  96.09 ( 93.79)	Acc@5 100.00 ( 99.93)
Epoch: [29][ 30/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.007)	Loss 2.4438e-01 (1.6709e-01)	Acc@1  90.62 ( 93.70)	Acc@5 100.00 ( 99.95)
Epoch: [29][ 40/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.006)	Loss 1.3391e-01 (1.6843e-01)	Acc@1  96.88 ( 93.77)	Acc@5 100.00 ( 99.92)
Epoch: [29][ 50/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.005)	Loss 2.2668e-01 (1.6943e-01)	Acc@1  92.97 ( 93.84)	Acc@5 100.00 ( 99.94)
Epoch: [29][ 60/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.005)	Loss 1.4807e-01 (1.6904e-01)	Acc@1  94.53 ( 93.78)	Acc@5 100.00 ( 99.95)
Epoch: [29][ 70/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 2.0374e-01 (1.7159e-01)	Acc@1  89.84 ( 93.74)	Acc@5 100.00 ( 99.93)
Epoch: [29][ 80/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 9.6130e-02 (1.7182e-01)	Acc@1  97.66 ( 93.70)	Acc@5 100.00 ( 99.94)
Epoch: [29][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.1924e-01 (1.7145e-01)	Acc@1  92.19 ( 93.74)	Acc@5 100.00 ( 99.93)
Epoch: [29][100/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.7539e-01 (1.6956e-01)	Acc@1  89.84 ( 93.86)	Acc@5 100.00 ( 99.94)
Epoch: [29][110/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.9324e-01 (1.6926e-01)	Acc@1  92.19 ( 93.83)	Acc@5 100.00 ( 99.94)
Epoch: [29][120/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.7639e-01 (1.7100e-01)	Acc@1  94.53 ( 93.80)	Acc@5 100.00 ( 99.94)
Epoch: [29][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.5601e-01 (1.7182e-01)	Acc@1  95.31 ( 93.81)	Acc@5 100.00 ( 99.94)
Epoch: [29][140/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.6858e-01 (1.7170e-01)	Acc@1  94.53 ( 93.85)	Acc@5 100.00 ( 99.94)
Epoch: [29][150/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.0154e-01 (1.7338e-01)	Acc@1  91.41 ( 93.82)	Acc@5 100.00 ( 99.94)
Epoch: [29][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.6113e-01 (1.7456e-01)	Acc@1  96.09 ( 93.77)	Acc@5  99.22 ( 99.94)
Epoch: [29][170/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.7688e-01 (1.7581e-01)	Acc@1  92.97 ( 93.76)	Acc@5 100.00 ( 99.95)
Epoch: [29][180/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.6992e-01 (1.7688e-01)	Acc@1  95.31 ( 93.70)	Acc@5  99.22 ( 99.93)
Epoch: [29][190/391]	Time  0.036 ( 0.034)	Data  0.002 ( 0.002)	Loss 1.8250e-01 (1.7727e-01)	Acc@1  92.97 ( 93.71)	Acc@5 100.00 ( 99.93)
Epoch: [29][200/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.4915e-01 (1.7689e-01)	Acc@1  89.06 ( 93.71)	Acc@5 100.00 ( 99.93)
Epoch: [29][210/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2866e-01 (1.7694e-01)	Acc@1  94.53 ( 93.68)	Acc@5 100.00 ( 99.94)
Epoch: [29][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.2717e-01 (1.7751e-01)	Acc@1  94.53 ( 93.69)	Acc@5  99.22 ( 99.93)
Epoch: [29][230/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.5244e-01 (1.7802e-01)	Acc@1  89.84 ( 93.69)	Acc@5 100.00 ( 99.94)
Epoch: [29][240/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.3425e-01 (1.7926e-01)	Acc@1  92.97 ( 93.64)	Acc@5 100.00 ( 99.94)
Epoch: [29][250/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.8003e-01 (1.7993e-01)	Acc@1  91.41 ( 93.64)	Acc@5  99.22 ( 99.93)
Epoch: [29][260/391]	Time  0.037 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.0129e-01 (1.7898e-01)	Acc@1  90.62 ( 93.67)	Acc@5 100.00 ( 99.93)
Epoch: [29][270/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1084e-01 (1.7874e-01)	Acc@1  96.88 ( 93.68)	Acc@5 100.00 ( 99.93)
Epoch: [29][280/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5625e-01 (1.7906e-01)	Acc@1  95.31 ( 93.67)	Acc@5 100.00 ( 99.93)
Epoch: [29][290/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.9272e-01 (1.7974e-01)	Acc@1  90.62 ( 93.64)	Acc@5 100.00 ( 99.93)
Epoch: [29][300/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3440e-01 (1.7912e-01)	Acc@1  93.75 ( 93.65)	Acc@5 100.00 ( 99.93)
Epoch: [29][310/391]	Time  0.037 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5649e-01 (1.7891e-01)	Acc@1  94.53 ( 93.65)	Acc@5  99.22 ( 99.92)
Epoch: [29][320/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5723e-01 (1.7876e-01)	Acc@1  96.09 ( 93.67)	Acc@5 100.00 ( 99.92)
Epoch: [29][330/391]	Time  0.028 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.8662e-01 (1.7939e-01)	Acc@1  90.62 ( 93.66)	Acc@5 100.00 ( 99.93)
Epoch: [29][340/391]	Time  0.040 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.2949e-01 (1.7937e-01)	Acc@1  89.84 ( 93.66)	Acc@5 100.00 ( 99.93)
Epoch: [29][350/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3318e-01 (1.7926e-01)	Acc@1  97.66 ( 93.68)	Acc@5 100.00 ( 99.92)
Epoch: [29][360/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.4438e-01 (1.8008e-01)	Acc@1  92.97 ( 93.66)	Acc@5 100.00 ( 99.92)
Epoch: [29][370/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.7285e-01 (1.8058e-01)	Acc@1  93.75 ( 93.64)	Acc@5  99.22 ( 99.92)
Epoch: [29][380/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.5024e-01 (1.8064e-01)	Acc@1  92.97 ( 93.63)	Acc@5 100.00 ( 99.92)
Epoch: [29][390/391]	Time  0.024 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9580e-01 (1.8071e-01)	Acc@1  93.75 ( 93.62)	Acc@5 100.00 ( 99.92)
## e[29] optimizer.zero_grad (sum) time: 0.16960501670837402
## e[29]       loss.backward (sum) time: 3.9933905601501465
## e[29]      optimizer.step (sum) time: 1.2254078388214111
## epoch[29] training(only) time: 13.35281229019165
# Switched to evaluate mode...
Test: [  0/100]	Time  0.170 ( 0.170)	Loss 3.0322e-01 (3.0322e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.014 ( 0.031)	Loss 3.0322e-01 (3.4770e-01)	Acc@1  89.00 ( 89.91)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.017 ( 0.024)	Loss 4.5312e-01 (3.5983e-01)	Acc@1  85.00 ( 89.43)	Acc@5  99.00 ( 99.62)
Test: [ 30/100]	Time  0.015 ( 0.022)	Loss 3.7109e-01 (3.7959e-01)	Acc@1  81.00 ( 89.00)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.020 ( 0.021)	Loss 3.3887e-01 (3.8137e-01)	Acc@1  88.00 ( 88.80)	Acc@5 100.00 ( 99.63)
Test: [ 50/100]	Time  0.020 ( 0.020)	Loss 2.3010e-01 (3.7764e-01)	Acc@1  94.00 ( 88.82)	Acc@5 100.00 ( 99.57)
Test: [ 60/100]	Time  0.016 ( 0.020)	Loss 3.7207e-01 (3.7711e-01)	Acc@1  90.00 ( 88.77)	Acc@5 100.00 ( 99.57)
Test: [ 70/100]	Time  0.016 ( 0.020)	Loss 5.6787e-01 (3.7561e-01)	Acc@1  86.00 ( 88.79)	Acc@5 100.00 ( 99.61)
Test: [ 80/100]	Time  0.014 ( 0.019)	Loss 3.2324e-01 (3.7480e-01)	Acc@1  91.00 ( 88.89)	Acc@5 100.00 ( 99.62)
Test: [ 90/100]	Time  0.014 ( 0.019)	Loss 2.5977e-01 (3.7201e-01)	Acc@1  92.00 ( 88.92)	Acc@5 100.00 ( 99.62)
 * Acc@1 88.960 Acc@5 99.620
### epoch[29] execution time: 15.353869676589966
EPOCH 30
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [30][  0/391]	Time  0.213 ( 0.213)	Data  0.179 ( 0.179)	Loss 2.1204e-01 (2.1204e-01)	Acc@1  92.97 ( 92.97)	Acc@5 100.00 (100.00)
Epoch: [30][ 10/391]	Time  0.032 ( 0.050)	Data  0.001 ( 0.018)	Loss 1.0626e-01 (1.4916e-01)	Acc@1  96.09 ( 95.31)	Acc@5 100.00 ( 99.93)
Epoch: [30][ 20/391]	Time  0.032 ( 0.042)	Data  0.001 ( 0.010)	Loss 1.1230e-01 (1.5434e-01)	Acc@1  96.09 ( 94.75)	Acc@5 100.00 ( 99.96)
Epoch: [30][ 30/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.007)	Loss 1.4246e-01 (1.4339e-01)	Acc@1  95.31 ( 95.16)	Acc@5 100.00 ( 99.95)
Epoch: [30][ 40/391]	Time  0.032 ( 0.038)	Data  0.001 ( 0.006)	Loss 1.9702e-01 (1.3979e-01)	Acc@1  93.75 ( 95.33)	Acc@5  99.22 ( 99.94)
Epoch: [30][ 50/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.005)	Loss 1.0089e-01 (1.3535e-01)	Acc@1  94.53 ( 95.31)	Acc@5 100.00 ( 99.95)
Epoch: [30][ 60/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.004)	Loss 1.0223e-01 (1.3312e-01)	Acc@1  95.31 ( 95.48)	Acc@5 100.00 ( 99.95)
Epoch: [30][ 70/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 7.4280e-02 (1.2952e-01)	Acc@1  98.44 ( 95.62)	Acc@5 100.00 ( 99.96)
Epoch: [30][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.5149e-01 (1.2993e-01)	Acc@1  95.31 ( 95.53)	Acc@5 100.00 ( 99.96)
Epoch: [30][ 90/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.0101e-01 (1.2690e-01)	Acc@1  96.88 ( 95.65)	Acc@5 100.00 ( 99.97)
Epoch: [30][100/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.8750e-01 (1.2505e-01)	Acc@1  92.19 ( 95.71)	Acc@5 100.00 ( 99.96)
Epoch: [30][110/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.4587e-01 (1.2357e-01)	Acc@1  93.75 ( 95.72)	Acc@5 100.00 ( 99.96)
Epoch: [30][120/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.0022e-01 (1.2134e-01)	Acc@1  97.66 ( 95.80)	Acc@5 100.00 ( 99.97)
Epoch: [30][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.0022e-01 (1.2146e-01)	Acc@1  94.53 ( 95.82)	Acc@5 100.00 ( 99.96)
Epoch: [30][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.4851e-02 (1.2010e-01)	Acc@1 100.00 ( 95.87)	Acc@5 100.00 ( 99.97)
Epoch: [30][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 7.0251e-02 (1.1951e-01)	Acc@1  98.44 ( 95.88)	Acc@5 100.00 ( 99.97)
Epoch: [30][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.1041e-01 (1.2007e-01)	Acc@1  96.88 ( 95.89)	Acc@5 100.00 ( 99.97)
Epoch: [30][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4062e-01 (1.1932e-01)	Acc@1  96.88 ( 95.92)	Acc@5 100.00 ( 99.97)
Epoch: [30][180/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.8989e-02 (1.1802e-01)	Acc@1  96.09 ( 95.97)	Acc@5 100.00 ( 99.97)
Epoch: [30][190/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1719e-01 (1.1646e-01)	Acc@1  92.97 ( 96.05)	Acc@5 100.00 ( 99.97)
Epoch: [30][200/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.1838e-02 (1.1571e-01)	Acc@1  96.88 ( 96.08)	Acc@5 100.00 ( 99.97)
Epoch: [30][210/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.0812e-02 (1.1407e-01)	Acc@1  99.22 ( 96.13)	Acc@5 100.00 ( 99.97)
Epoch: [30][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0632e-01 (1.1348e-01)	Acc@1  97.66 ( 96.16)	Acc@5 100.00 ( 99.97)
Epoch: [30][230/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1548e-01 (1.1268e-01)	Acc@1  94.53 ( 96.18)	Acc@5 100.00 ( 99.97)
Epoch: [30][240/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.0027e-02 (1.1189e-01)	Acc@1  97.66 ( 96.22)	Acc@5 100.00 ( 99.97)
Epoch: [30][250/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.4910e-02 (1.1046e-01)	Acc@1  96.88 ( 96.28)	Acc@5 100.00 ( 99.98)
Epoch: [30][260/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4270e-01 (1.1000e-01)	Acc@1  96.88 ( 96.31)	Acc@5 100.00 ( 99.98)
Epoch: [30][270/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5369e-01 (1.1042e-01)	Acc@1  95.31 ( 96.29)	Acc@5 100.00 ( 99.97)
Epoch: [30][280/391]	Time  0.028 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4392e-01 (1.0944e-01)	Acc@1  94.53 ( 96.33)	Acc@5 100.00 ( 99.97)
Epoch: [30][290/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.3232e-02 (1.0835e-01)	Acc@1  98.44 ( 96.36)	Acc@5 100.00 ( 99.98)
Epoch: [30][300/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.6488e-02 (1.0791e-01)	Acc@1  98.44 ( 96.38)	Acc@5 100.00 ( 99.98)
Epoch: [30][310/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.2214e-02 (1.0752e-01)	Acc@1  96.88 ( 96.39)	Acc@5 100.00 ( 99.98)
Epoch: [30][320/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1700e-01 (1.0719e-01)	Acc@1  95.31 ( 96.39)	Acc@5 100.00 ( 99.98)
Epoch: [30][330/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.0852e-02 (1.0660e-01)	Acc@1  97.66 ( 96.42)	Acc@5 100.00 ( 99.98)
Epoch: [30][340/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1584e-01 (1.0662e-01)	Acc@1  99.22 ( 96.43)	Acc@5 100.00 ( 99.97)
Epoch: [30][350/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1389e-01 (1.0587e-01)	Acc@1  96.09 ( 96.47)	Acc@5 100.00 ( 99.98)
Epoch: [30][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1224e-01 (1.0543e-01)	Acc@1  96.09 ( 96.48)	Acc@5 100.00 ( 99.97)
Epoch: [30][370/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0785e-01 (1.0483e-01)	Acc@1  96.09 ( 96.50)	Acc@5 100.00 ( 99.97)
Epoch: [30][380/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3342e-01 (1.0408e-01)	Acc@1  96.09 ( 96.54)	Acc@5 100.00 ( 99.98)
Epoch: [30][390/391]	Time  0.023 ( 0.033)	Data  0.001 ( 0.002)	Loss 7.5684e-02 (1.0375e-01)	Acc@1  97.50 ( 96.56)	Acc@5 100.00 ( 99.98)
## e[30] optimizer.zero_grad (sum) time: 0.17044448852539062
## e[30]       loss.backward (sum) time: 4.04750657081604
## e[30]      optimizer.step (sum) time: 1.212315320968628
## epoch[30] training(only) time: 13.241716384887695
# Switched to evaluate mode...
Test: [  0/100]	Time  0.181 ( 0.181)	Loss 1.2512e-01 (1.2512e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.014 ( 0.031)	Loss 1.9177e-01 (2.3059e-01)	Acc@1  93.00 ( 92.45)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.021 ( 0.024)	Loss 3.7549e-01 (2.5603e-01)	Acc@1  89.00 ( 92.05)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.015 ( 0.022)	Loss 2.8540e-01 (2.7051e-01)	Acc@1  88.00 ( 91.48)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.014 ( 0.021)	Loss 2.2351e-01 (2.6943e-01)	Acc@1  94.00 ( 91.54)	Acc@5  99.00 ( 99.71)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.6138e-01 (2.6492e-01)	Acc@1  93.00 ( 91.78)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.016 ( 0.020)	Loss 2.5879e-01 (2.5882e-01)	Acc@1  94.00 ( 91.84)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.021 ( 0.019)	Loss 4.0210e-01 (2.5568e-01)	Acc@1  89.00 ( 91.92)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.020 ( 0.019)	Loss 2.1741e-01 (2.5546e-01)	Acc@1  92.00 ( 91.90)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.014 ( 0.019)	Loss 1.9238e-01 (2.5547e-01)	Acc@1  93.00 ( 91.84)	Acc@5 100.00 ( 99.77)
 * Acc@1 91.880 Acc@5 99.780
### epoch[30] execution time: 15.22160005569458
EPOCH 31
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [31][  0/391]	Time  0.211 ( 0.211)	Data  0.177 ( 0.177)	Loss 3.8849e-02 (3.8849e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [31][ 10/391]	Time  0.032 ( 0.050)	Data  0.001 ( 0.017)	Loss 7.5378e-02 (6.7327e-02)	Acc@1  98.44 ( 97.94)	Acc@5 100.00 (100.00)
Epoch: [31][ 20/391]	Time  0.032 ( 0.042)	Data  0.001 ( 0.010)	Loss 5.7434e-02 (7.0828e-02)	Acc@1  98.44 ( 97.62)	Acc@5 100.00 (100.00)
Epoch: [31][ 30/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.007)	Loss 4.8065e-02 (7.1397e-02)	Acc@1  98.44 ( 97.73)	Acc@5 100.00 (100.00)
Epoch: [31][ 40/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.006)	Loss 3.6957e-02 (7.5947e-02)	Acc@1 100.00 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [31][ 50/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 1.1243e-01 (7.5082e-02)	Acc@1  95.31 ( 97.64)	Acc@5 100.00 (100.00)
Epoch: [31][ 60/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 6.1859e-02 (7.6882e-02)	Acc@1  97.66 ( 97.54)	Acc@5 100.00 (100.00)
Epoch: [31][ 70/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 7.5562e-02 (7.6762e-02)	Acc@1  96.88 ( 97.55)	Acc@5 100.00 (100.00)
Epoch: [31][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 6.0913e-02 (7.7398e-02)	Acc@1  98.44 ( 97.54)	Acc@5 100.00 ( 99.99)
Epoch: [31][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 5.1270e-02 (7.6969e-02)	Acc@1  99.22 ( 97.54)	Acc@5 100.00 ( 99.99)
Epoch: [31][100/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 8.4534e-02 (7.7159e-02)	Acc@1  96.09 ( 97.54)	Acc@5 100.00 ( 99.99)
Epoch: [31][110/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 6.1493e-02 (7.7020e-02)	Acc@1  98.44 ( 97.53)	Acc@5 100.00 ( 99.99)
Epoch: [31][120/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 8.4351e-02 (7.6283e-02)	Acc@1  97.66 ( 97.55)	Acc@5 100.00 ( 99.99)
Epoch: [31][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 7.3730e-02 (7.5716e-02)	Acc@1  96.88 ( 97.59)	Acc@5 100.00 ( 99.99)
Epoch: [31][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 7.8369e-02 (7.5489e-02)	Acc@1  96.09 ( 97.57)	Acc@5 100.00 ( 99.99)
Epoch: [31][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.5125e-02 (7.4986e-02)	Acc@1  98.44 ( 97.59)	Acc@5 100.00 ( 99.99)
Epoch: [31][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.3599e-02 (7.5449e-02)	Acc@1  97.66 ( 97.56)	Acc@5 100.00 ( 99.99)
Epoch: [31][170/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.1909e-02 (7.5234e-02)	Acc@1  96.09 ( 97.58)	Acc@5 100.00 ( 99.99)
Epoch: [31][180/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.1980e-02 (7.5110e-02)	Acc@1  96.09 ( 97.56)	Acc@5 100.00 ( 99.99)
Epoch: [31][190/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.0751e-02 (7.4753e-02)	Acc@1  99.22 ( 97.60)	Acc@5 100.00 ( 99.99)
Epoch: [31][200/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0394e-01 (7.5500e-02)	Acc@1  96.09 ( 97.55)	Acc@5 100.00 ( 99.99)
Epoch: [31][210/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.3721e-02 (7.5935e-02)	Acc@1  97.66 ( 97.54)	Acc@5 100.00 ( 99.99)
Epoch: [31][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0303e-01 (7.5935e-02)	Acc@1  96.88 ( 97.54)	Acc@5 100.00 ( 99.99)
Epoch: [31][230/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.8767e-02 (7.6060e-02)	Acc@1  98.44 ( 97.51)	Acc@5 100.00 ( 99.99)
Epoch: [31][240/391]	Time  0.027 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.1655e-02 (7.6454e-02)	Acc@1  97.66 ( 97.50)	Acc@5 100.00 ( 99.99)
Epoch: [31][250/391]	Time  0.031 ( 0.034)	Data  0.000 ( 0.002)	Loss 6.3904e-02 (7.6408e-02)	Acc@1  98.44 ( 97.52)	Acc@5 100.00 ( 99.99)
Epoch: [31][260/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.8369e-02 (7.6304e-02)	Acc@1  96.09 ( 97.52)	Acc@5 100.00 ( 99.99)
Epoch: [31][270/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1224e-01 (7.6764e-02)	Acc@1  96.09 ( 97.50)	Acc@5 100.00 ( 99.99)
Epoch: [31][280/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.7351e-02 (7.7827e-02)	Acc@1  94.53 ( 97.44)	Acc@5 100.00 ( 99.99)
Epoch: [31][290/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.3618e-02 (7.8286e-02)	Acc@1  97.66 ( 97.41)	Acc@5 100.00 ( 99.99)
Epoch: [31][300/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.1951e-02 (7.7790e-02)	Acc@1  96.88 ( 97.43)	Acc@5 100.00 ( 99.99)
Epoch: [31][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.2397e-02 (7.7346e-02)	Acc@1  97.66 ( 97.44)	Acc@5 100.00 ( 99.99)
Epoch: [31][320/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1407e-01 (7.7387e-02)	Acc@1  95.31 ( 97.43)	Acc@5 100.00 ( 99.99)
Epoch: [31][330/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.2266e-02 (7.7162e-02)	Acc@1  97.66 ( 97.44)	Acc@5 100.00 ( 99.99)
Epoch: [31][340/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.8167e-02 (7.6770e-02)	Acc@1  97.66 ( 97.45)	Acc@5 100.00 ( 99.99)
Epoch: [31][350/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.2317e-02 (7.6990e-02)	Acc@1  97.66 ( 97.44)	Acc@5 100.00 ( 99.99)
Epoch: [31][360/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.002)	Loss 9.1431e-02 (7.6808e-02)	Acc@1  97.66 ( 97.46)	Acc@5 100.00 ( 99.99)
Epoch: [31][370/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.9251e-02 (7.7006e-02)	Acc@1  99.22 ( 97.45)	Acc@5 100.00 ( 99.99)
Epoch: [31][380/391]	Time  0.036 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.6946e-02 (7.6745e-02)	Acc@1  98.44 ( 97.47)	Acc@5 100.00 ( 99.99)
Epoch: [31][390/391]	Time  0.029 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.7546e-02 (7.6525e-02)	Acc@1  98.75 ( 97.48)	Acc@5 100.00 ( 99.99)
## e[31] optimizer.zero_grad (sum) time: 0.16904592514038086
## e[31]       loss.backward (sum) time: 4.108630418777466
## e[31]      optimizer.step (sum) time: 1.2092669010162354
## epoch[31] training(only) time: 13.202149629592896
# Switched to evaluate mode...
Test: [  0/100]	Time  0.172 ( 0.172)	Loss 1.3831e-01 (1.3831e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.017 ( 0.032)	Loss 1.5942e-01 (2.2183e-01)	Acc@1  94.00 ( 92.27)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.017 ( 0.025)	Loss 3.3887e-01 (2.4518e-01)	Acc@1  91.00 ( 92.14)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.014 ( 0.022)	Loss 2.5903e-01 (2.6517e-01)	Acc@1  91.00 ( 91.84)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.023 ( 0.021)	Loss 2.2217e-01 (2.6469e-01)	Acc@1  92.00 ( 91.80)	Acc@5  99.00 ( 99.63)
Test: [ 50/100]	Time  0.012 ( 0.020)	Loss 1.6333e-01 (2.6072e-01)	Acc@1  94.00 ( 91.96)	Acc@5 100.00 ( 99.67)
Test: [ 60/100]	Time  0.015 ( 0.020)	Loss 2.5000e-01 (2.5467e-01)	Acc@1  93.00 ( 91.90)	Acc@5 100.00 ( 99.72)
Test: [ 70/100]	Time  0.020 ( 0.019)	Loss 4.0332e-01 (2.5039e-01)	Acc@1  90.00 ( 92.10)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.020 ( 0.019)	Loss 2.2095e-01 (2.5026e-01)	Acc@1  94.00 ( 92.10)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.015 ( 0.019)	Loss 1.7603e-01 (2.4950e-01)	Acc@1  93.00 ( 92.04)	Acc@5 100.00 ( 99.76)
 * Acc@1 92.090 Acc@5 99.760
### epoch[31] execution time: 15.17669415473938
EPOCH 32
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [32][  0/391]	Time  0.210 ( 0.210)	Data  0.175 ( 0.175)	Loss 7.5806e-02 (7.5806e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [32][ 10/391]	Time  0.032 ( 0.050)	Data  0.001 ( 0.017)	Loss 5.6396e-02 (6.2881e-02)	Acc@1  99.22 ( 98.08)	Acc@5 100.00 (100.00)
Epoch: [32][ 20/391]	Time  0.032 ( 0.042)	Data  0.001 ( 0.010)	Loss 9.1797e-02 (6.9325e-02)	Acc@1  96.09 ( 97.69)	Acc@5 100.00 (100.00)
Epoch: [32][ 30/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.007)	Loss 4.1840e-02 (7.0142e-02)	Acc@1  98.44 ( 97.71)	Acc@5 100.00 (100.00)
Epoch: [32][ 40/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.006)	Loss 7.9041e-02 (7.2078e-02)	Acc@1  97.66 ( 97.60)	Acc@5 100.00 (100.00)
Epoch: [32][ 50/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.005)	Loss 6.7566e-02 (7.1216e-02)	Acc@1  97.66 ( 97.61)	Acc@5 100.00 (100.00)
Epoch: [32][ 60/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 3.0884e-02 (7.1428e-02)	Acc@1 100.00 ( 97.64)	Acc@5 100.00 (100.00)
Epoch: [32][ 70/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 4.7333e-02 (7.1300e-02)	Acc@1  99.22 ( 97.68)	Acc@5 100.00 (100.00)
Epoch: [32][ 80/391]	Time  0.040 ( 0.035)	Data  0.001 ( 0.004)	Loss 4.1290e-02 (7.1284e-02)	Acc@1  98.44 ( 97.69)	Acc@5 100.00 (100.00)
Epoch: [32][ 90/391]	Time  0.029 ( 0.035)	Data  0.001 ( 0.003)	Loss 4.9042e-02 (7.2132e-02)	Acc@1  97.66 ( 97.64)	Acc@5 100.00 ( 99.98)
Epoch: [32][100/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 6.1249e-02 (7.1817e-02)	Acc@1  97.66 ( 97.63)	Acc@5 100.00 ( 99.98)
Epoch: [32][110/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.1475e-01 (7.2054e-02)	Acc@1  95.31 ( 97.64)	Acc@5 100.00 ( 99.99)
Epoch: [32][120/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.003)	Loss 8.1848e-02 (7.1846e-02)	Acc@1  96.88 ( 97.62)	Acc@5 100.00 ( 99.98)
Epoch: [32][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.2185e-02 (7.1059e-02)	Acc@1  98.44 ( 97.63)	Acc@5 100.00 ( 99.98)
Epoch: [32][140/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.7587e-02 (7.1396e-02)	Acc@1  98.44 ( 97.63)	Acc@5 100.00 ( 99.98)
Epoch: [32][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.2603e-02 (7.0411e-02)	Acc@1  99.22 ( 97.68)	Acc@5 100.00 ( 99.98)
Epoch: [32][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.4760e-02 (7.1463e-02)	Acc@1  99.22 ( 97.63)	Acc@5 100.00 ( 99.99)
Epoch: [32][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.6886e-02 (7.1145e-02)	Acc@1  99.22 ( 97.63)	Acc@5 100.00 ( 99.99)
Epoch: [32][180/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.8755e-02 (7.0960e-02)	Acc@1  95.31 ( 97.61)	Acc@5 100.00 ( 99.99)
Epoch: [32][190/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.2042e-02 (7.0798e-02)	Acc@1  99.22 ( 97.64)	Acc@5 100.00 ( 99.99)
Epoch: [32][200/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.4565e-02 (7.0153e-02)	Acc@1 100.00 ( 97.67)	Acc@5 100.00 ( 99.99)
Epoch: [32][210/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.2339e-02 (6.8832e-02)	Acc@1 100.00 ( 97.73)	Acc@5 100.00 ( 99.99)
Epoch: [32][220/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.3941e-02 (6.8779e-02)	Acc@1  99.22 ( 97.73)	Acc@5 100.00 ( 99.99)
Epoch: [32][230/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.0710e-02 (6.8930e-02)	Acc@1  98.44 ( 97.72)	Acc@5 100.00 ( 99.99)
Epoch: [32][240/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.9050e-02 (6.9020e-02)	Acc@1  96.88 ( 97.73)	Acc@5 100.00 ( 99.99)
Epoch: [32][250/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.8502e-02 (6.8677e-02)	Acc@1  96.88 ( 97.72)	Acc@5 100.00 ( 99.99)
Epoch: [32][260/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.3651e-02 (6.8665e-02)	Acc@1 100.00 ( 97.75)	Acc@5 100.00 ( 99.99)
Epoch: [32][270/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.0017e-02 (6.9014e-02)	Acc@1  96.09 ( 97.72)	Acc@5 100.00 ( 99.99)
Epoch: [32][280/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.2632e-02 (6.8866e-02)	Acc@1  97.66 ( 97.73)	Acc@5 100.00 ( 99.99)
Epoch: [32][290/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.4790e-02 (6.8364e-02)	Acc@1  99.22 ( 97.75)	Acc@5 100.00 ( 99.99)
Epoch: [32][300/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1993e-01 (6.8378e-02)	Acc@1  95.31 ( 97.75)	Acc@5 100.00 ( 99.99)
Epoch: [32][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.6304e-02 (6.8592e-02)	Acc@1  98.44 ( 97.74)	Acc@5 100.00 ( 99.99)
Epoch: [32][320/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.0149e-02 (6.8421e-02)	Acc@1  95.31 ( 97.75)	Acc@5 100.00 ( 99.99)
Epoch: [32][330/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.6702e-02 (6.8747e-02)	Acc@1  98.44 ( 97.75)	Acc@5 100.00 ( 99.99)
Epoch: [32][340/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.2317e-02 (6.8836e-02)	Acc@1  98.44 ( 97.75)	Acc@5 100.00 ( 99.99)
Epoch: [32][350/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.0312e-02 (6.8737e-02)	Acc@1  96.88 ( 97.76)	Acc@5 100.00 ( 99.99)
Epoch: [32][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.5552e-02 (6.8988e-02)	Acc@1  97.66 ( 97.74)	Acc@5 100.00 ( 99.99)
Epoch: [32][370/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.5125e-02 (6.8622e-02)	Acc@1  96.88 ( 97.76)	Acc@5 100.00 ( 99.99)
Epoch: [32][380/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0852e-01 (6.8576e-02)	Acc@1  95.31 ( 97.75)	Acc@5 100.00 ( 99.99)
Epoch: [32][390/391]	Time  0.023 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.1351e-02 (6.8827e-02)	Acc@1  98.75 ( 97.74)	Acc@5 100.00 ( 99.99)
## e[32] optimizer.zero_grad (sum) time: 0.16992616653442383
## e[32]       loss.backward (sum) time: 4.088141441345215
## e[32]      optimizer.step (sum) time: 1.227675437927246
## epoch[32] training(only) time: 13.240561485290527
# Switched to evaluate mode...
Test: [  0/100]	Time  0.170 ( 0.170)	Loss 1.5137e-01 (1.5137e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.015 ( 0.031)	Loss 1.4600e-01 (2.2590e-01)	Acc@1  95.00 ( 92.55)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.020 ( 0.024)	Loss 3.4521e-01 (2.4637e-01)	Acc@1  93.00 ( 92.81)	Acc@5  99.00 ( 99.67)
Test: [ 30/100]	Time  0.021 ( 0.022)	Loss 2.7588e-01 (2.6585e-01)	Acc@1  90.00 ( 92.13)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.012 ( 0.020)	Loss 2.3291e-01 (2.6539e-01)	Acc@1  92.00 ( 92.10)	Acc@5  99.00 ( 99.63)
Test: [ 50/100]	Time  0.014 ( 0.020)	Loss 1.6760e-01 (2.6121e-01)	Acc@1  94.00 ( 92.20)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.015 ( 0.019)	Loss 2.4194e-01 (2.5439e-01)	Acc@1  92.00 ( 92.26)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.022 ( 0.019)	Loss 4.2871e-01 (2.5091e-01)	Acc@1  89.00 ( 92.34)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.018 ( 0.019)	Loss 1.9641e-01 (2.5126e-01)	Acc@1  94.00 ( 92.33)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.018 ( 0.019)	Loss 1.9238e-01 (2.5109e-01)	Acc@1  93.00 ( 92.22)	Acc@5 100.00 ( 99.75)
 * Acc@1 92.300 Acc@5 99.750
### epoch[32] execution time: 15.183512687683105
EPOCH 33
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [33][  0/391]	Time  0.212 ( 0.212)	Data  0.178 ( 0.178)	Loss 6.9702e-02 (6.9702e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [33][ 10/391]	Time  0.033 ( 0.051)	Data  0.001 ( 0.018)	Loss 7.7332e-02 (6.2759e-02)	Acc@1  96.88 ( 98.08)	Acc@5 100.00 (100.00)
Epoch: [33][ 20/391]	Time  0.032 ( 0.042)	Data  0.001 ( 0.010)	Loss 8.4351e-02 (6.0072e-02)	Acc@1  96.09 ( 98.03)	Acc@5 100.00 (100.00)
Epoch: [33][ 30/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.007)	Loss 3.1235e-02 (6.1030e-02)	Acc@1  99.22 ( 98.08)	Acc@5 100.00 (100.00)
Epoch: [33][ 40/391]	Time  0.032 ( 0.038)	Data  0.001 ( 0.006)	Loss 5.4138e-02 (6.2918e-02)	Acc@1  98.44 ( 97.98)	Acc@5 100.00 (100.00)
Epoch: [33][ 50/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 6.2286e-02 (6.3422e-02)	Acc@1  97.66 ( 98.02)	Acc@5 100.00 (100.00)
Epoch: [33][ 60/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.004)	Loss 1.1560e-01 (6.4227e-02)	Acc@1  95.31 ( 97.94)	Acc@5 100.00 (100.00)
Epoch: [33][ 70/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 6.7200e-02 (6.4005e-02)	Acc@1  96.88 ( 97.94)	Acc@5 100.00 (100.00)
Epoch: [33][ 80/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.004)	Loss 2.3819e-02 (6.3660e-02)	Acc@1  99.22 ( 97.92)	Acc@5 100.00 (100.00)
Epoch: [33][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 5.4626e-02 (6.4214e-02)	Acc@1  97.66 ( 97.88)	Acc@5 100.00 (100.00)
Epoch: [33][100/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 8.0261e-02 (6.4668e-02)	Acc@1  96.88 ( 97.86)	Acc@5 100.00 ( 99.99)
Epoch: [33][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.3864e-02 (6.3782e-02)	Acc@1  98.44 ( 97.91)	Acc@5 100.00 ( 99.99)
Epoch: [33][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.1636e-02 (6.2389e-02)	Acc@1  97.66 ( 97.97)	Acc@5 100.00 ( 99.99)
Epoch: [33][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.4830e-02 (6.2953e-02)	Acc@1  98.44 ( 97.97)	Acc@5 100.00 ( 99.98)
Epoch: [33][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 9.0637e-02 (6.3381e-02)	Acc@1  96.09 ( 97.96)	Acc@5 100.00 ( 99.97)
Epoch: [33][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.0558e-02 (6.2575e-02)	Acc@1  99.22 ( 98.01)	Acc@5 100.00 ( 99.97)
Epoch: [33][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 7.2388e-02 (6.3418e-02)	Acc@1  98.44 ( 97.97)	Acc@5 100.00 ( 99.98)
Epoch: [33][170/391]	Time  0.035 ( 0.034)	Data  0.002 ( 0.002)	Loss 9.1797e-02 (6.3611e-02)	Acc@1  97.66 ( 97.97)	Acc@5 100.00 ( 99.98)
Epoch: [33][180/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.4067e-02 (6.3647e-02)	Acc@1  99.22 ( 97.96)	Acc@5 100.00 ( 99.98)
Epoch: [33][190/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.3651e-02 (6.3294e-02)	Acc@1  99.22 ( 97.96)	Acc@5 100.00 ( 99.98)
Epoch: [33][200/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.6355e-02 (6.4472e-02)	Acc@1  96.88 ( 97.92)	Acc@5 100.00 ( 99.98)
Epoch: [33][210/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.1523e-02 (6.4133e-02)	Acc@1  97.66 ( 97.95)	Acc@5 100.00 ( 99.98)
Epoch: [33][220/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.9129e-02 (6.4120e-02)	Acc@1 100.00 ( 97.96)	Acc@5 100.00 ( 99.98)
Epoch: [33][230/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.0466e-02 (6.4044e-02)	Acc@1  99.22 ( 97.96)	Acc@5 100.00 ( 99.98)
Epoch: [33][240/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.8879e-02 (6.3515e-02)	Acc@1  98.44 ( 97.97)	Acc@5 100.00 ( 99.98)
Epoch: [33][250/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.0995e-02 (6.3360e-02)	Acc@1  99.22 ( 97.97)	Acc@5 100.00 ( 99.98)
Epoch: [33][260/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.1309e-02 (6.3012e-02)	Acc@1  96.09 ( 97.97)	Acc@5 100.00 ( 99.99)
Epoch: [33][270/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.2572e-02 (6.2863e-02)	Acc@1  97.66 ( 97.97)	Acc@5 100.00 ( 99.99)
Epoch: [33][280/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.2969e-02 (6.2648e-02)	Acc@1  99.22 ( 97.97)	Acc@5 100.00 ( 99.99)
Epoch: [33][290/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.7444e-02 (6.2563e-02)	Acc@1  96.88 ( 97.98)	Acc@5 100.00 ( 99.99)
Epoch: [33][300/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.5040e-02 (6.2573e-02)	Acc@1  99.22 ( 97.97)	Acc@5 100.00 ( 99.99)
Epoch: [33][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.7098e-02 (6.2781e-02)	Acc@1  96.88 ( 97.97)	Acc@5 100.00 ( 99.99)
Epoch: [33][320/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.1980e-02 (6.2560e-02)	Acc@1  96.88 ( 97.98)	Acc@5 100.00 ( 99.99)
Epoch: [33][330/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.9479e-02 (6.2424e-02)	Acc@1  96.88 ( 97.99)	Acc@5 100.00 ( 99.99)
Epoch: [33][340/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.2225e-02 (6.2432e-02)	Acc@1  98.44 ( 97.98)	Acc@5 100.00 ( 99.99)
Epoch: [33][350/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.9835e-02 (6.2534e-02)	Acc@1  98.44 ( 97.97)	Acc@5 100.00 ( 99.99)
Epoch: [33][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.8909e-02 (6.2279e-02)	Acc@1  98.44 ( 97.98)	Acc@5 100.00 ( 99.99)
Epoch: [33][370/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.5614e-02 (6.1828e-02)	Acc@1  99.22 ( 98.00)	Acc@5 100.00 ( 99.99)
Epoch: [33][380/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.1228e-02 (6.1830e-02)	Acc@1  96.88 ( 98.00)	Acc@5 100.00 ( 99.99)
Epoch: [33][390/391]	Time  0.022 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.0352e-01 (6.1673e-02)	Acc@1  95.00 ( 98.00)	Acc@5 100.00 ( 99.99)
## e[33] optimizer.zero_grad (sum) time: 0.17073726654052734
## e[33]       loss.backward (sum) time: 4.081367015838623
## e[33]      optimizer.step (sum) time: 1.2065725326538086
## epoch[33] training(only) time: 13.203110456466675
# Switched to evaluate mode...
Test: [  0/100]	Time  0.168 ( 0.168)	Loss 1.5796e-01 (1.5796e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.016 ( 0.031)	Loss 1.6797e-01 (2.1924e-01)	Acc@1  93.00 ( 92.45)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.016 ( 0.024)	Loss 3.5596e-01 (2.4121e-01)	Acc@1  93.00 ( 92.48)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.014 ( 0.022)	Loss 2.3816e-01 (2.5904e-01)	Acc@1  91.00 ( 92.10)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.016 ( 0.021)	Loss 2.3645e-01 (2.5855e-01)	Acc@1  92.00 ( 92.05)	Acc@5  99.00 ( 99.71)
Test: [ 50/100]	Time  0.020 ( 0.020)	Loss 1.6467e-01 (2.5798e-01)	Acc@1  93.00 ( 92.10)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.015 ( 0.019)	Loss 2.7783e-01 (2.5299e-01)	Acc@1  92.00 ( 92.11)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.029 ( 0.019)	Loss 4.4482e-01 (2.4990e-01)	Acc@1  90.00 ( 92.24)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.016 ( 0.019)	Loss 2.0728e-01 (2.5006e-01)	Acc@1  95.00 ( 92.27)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.021 ( 0.019)	Loss 1.7395e-01 (2.4878e-01)	Acc@1  93.00 ( 92.23)	Acc@5 100.00 ( 99.78)
 * Acc@1 92.280 Acc@5 99.780
### epoch[33] execution time: 15.174115180969238
EPOCH 34
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [34][  0/391]	Time  0.219 ( 0.219)	Data  0.182 ( 0.182)	Loss 4.0619e-02 (4.0619e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [34][ 10/391]	Time  0.032 ( 0.051)	Data  0.001 ( 0.018)	Loss 9.0881e-02 (5.3295e-02)	Acc@1  97.66 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [34][ 20/391]	Time  0.032 ( 0.042)	Data  0.001 ( 0.010)	Loss 3.7018e-02 (5.6387e-02)	Acc@1  99.22 ( 98.21)	Acc@5 100.00 (100.00)
Epoch: [34][ 30/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.007)	Loss 4.0588e-02 (5.1511e-02)	Acc@1  97.66 ( 98.34)	Acc@5 100.00 (100.00)
Epoch: [34][ 40/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.006)	Loss 6.0669e-02 (5.3690e-02)	Acc@1  98.44 ( 98.32)	Acc@5 100.00 (100.00)
Epoch: [34][ 50/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.005)	Loss 5.0690e-02 (5.6017e-02)	Acc@1  99.22 ( 98.28)	Acc@5 100.00 (100.00)
Epoch: [34][ 60/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 5.1086e-02 (5.7352e-02)	Acc@1  97.66 ( 98.23)	Acc@5 100.00 (100.00)
Epoch: [34][ 70/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 8.5754e-02 (5.8468e-02)	Acc@1  96.09 ( 98.17)	Acc@5 100.00 (100.00)
Epoch: [34][ 80/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.004)	Loss 8.6243e-02 (5.7105e-02)	Acc@1  96.09 ( 98.21)	Acc@5 100.00 (100.00)
Epoch: [34][ 90/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.6382e-02 (5.7171e-02)	Acc@1 100.00 ( 98.21)	Acc@5 100.00 (100.00)
Epoch: [34][100/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 6.0669e-02 (5.6519e-02)	Acc@1  96.88 ( 98.20)	Acc@5 100.00 (100.00)
Epoch: [34][110/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 4.4250e-02 (5.7070e-02)	Acc@1  99.22 ( 98.18)	Acc@5 100.00 (100.00)
Epoch: [34][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.5959e-02 (5.7055e-02)	Acc@1  98.44 ( 98.20)	Acc@5 100.00 (100.00)
Epoch: [34][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.3131e-02 (5.7565e-02)	Acc@1  99.22 ( 98.18)	Acc@5 100.00 ( 99.99)
Epoch: [34][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 7.9529e-02 (5.8714e-02)	Acc@1  96.88 ( 98.13)	Acc@5 100.00 ( 99.99)
Epoch: [34][150/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.9824e-02 (5.7951e-02)	Acc@1  98.44 ( 98.16)	Acc@5 100.00 ( 99.99)
Epoch: [34][160/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.9978e-02 (5.7262e-02)	Acc@1  98.44 ( 98.18)	Acc@5 100.00 (100.00)
Epoch: [34][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.8176e-02 (5.7188e-02)	Acc@1  96.88 ( 98.17)	Acc@5 100.00 (100.00)
Epoch: [34][180/391]	Time  0.038 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.5309e-02 (5.7031e-02)	Acc@1 100.00 ( 98.18)	Acc@5 100.00 (100.00)
Epoch: [34][190/391]	Time  0.037 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.9561e-02 (5.6882e-02)	Acc@1  98.44 ( 98.18)	Acc@5 100.00 (100.00)
Epoch: [34][200/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.9011e-02 (5.6871e-02)	Acc@1  98.44 ( 98.17)	Acc@5 100.00 (100.00)
Epoch: [34][210/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.5023e-02 (5.6903e-02)	Acc@1  97.66 ( 98.17)	Acc@5 100.00 (100.00)
Epoch: [34][220/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0065e-01 (5.7548e-02)	Acc@1  97.66 ( 98.15)	Acc@5 100.00 (100.00)
Epoch: [34][230/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3214e-02 (5.7289e-02)	Acc@1 100.00 ( 98.16)	Acc@5 100.00 (100.00)
Epoch: [34][240/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.2023e-02 (5.7530e-02)	Acc@1  97.66 ( 98.13)	Acc@5 100.00 (100.00)
Epoch: [34][250/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.6030e-02 (5.7385e-02)	Acc@1  98.44 ( 98.13)	Acc@5 100.00 (100.00)
Epoch: [34][260/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.1321e-02 (5.6993e-02)	Acc@1  99.22 ( 98.15)	Acc@5 100.00 (100.00)
Epoch: [34][270/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.8269e-02 (5.6916e-02)	Acc@1  99.22 ( 98.15)	Acc@5 100.00 (100.00)
Epoch: [34][280/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.3854e-02 (5.6537e-02)	Acc@1  99.22 ( 98.18)	Acc@5 100.00 (100.00)
Epoch: [34][290/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.0863e-02 (5.6203e-02)	Acc@1  99.22 ( 98.20)	Acc@5 100.00 (100.00)
Epoch: [34][300/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.2347e-02 (5.5795e-02)	Acc@1  97.66 ( 98.21)	Acc@5 100.00 (100.00)
Epoch: [34][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.4535e-02 (5.5387e-02)	Acc@1  97.66 ( 98.22)	Acc@5 100.00 (100.00)
Epoch: [34][320/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0248e-01 (5.5916e-02)	Acc@1  96.88 ( 98.20)	Acc@5 100.00 (100.00)
Epoch: [34][330/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.9591e-02 (5.5716e-02)	Acc@1  98.44 ( 98.21)	Acc@5 100.00 (100.00)
Epoch: [34][340/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.4849e-02 (5.5595e-02)	Acc@1  96.09 ( 98.21)	Acc@5 100.00 (100.00)
Epoch: [34][350/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.2632e-02 (5.5715e-02)	Acc@1  96.88 ( 98.20)	Acc@5 100.00 (100.00)
Epoch: [34][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.3110e-02 (5.5879e-02)	Acc@1  98.44 ( 98.20)	Acc@5 100.00 (100.00)
Epoch: [34][370/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.9570e-02 (5.6033e-02)	Acc@1  97.66 ( 98.20)	Acc@5 100.00 (100.00)
Epoch: [34][380/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.9285e-02 (5.5765e-02)	Acc@1  96.88 ( 98.21)	Acc@5 100.00 (100.00)
Epoch: [34][390/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.0933e-02 (5.5883e-02)	Acc@1  96.25 ( 98.21)	Acc@5 100.00 ( 99.99)
## e[34] optimizer.zero_grad (sum) time: 0.16922879219055176
## e[34]       loss.backward (sum) time: 4.079255819320679
## e[34]      optimizer.step (sum) time: 1.1978893280029297
## epoch[34] training(only) time: 13.281200408935547
# Switched to evaluate mode...
Test: [  0/100]	Time  0.171 ( 0.171)	Loss 1.6370e-01 (1.6370e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.014 ( 0.032)	Loss 1.5857e-01 (2.2222e-01)	Acc@1  93.00 ( 92.36)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.014 ( 0.024)	Loss 3.5083e-01 (2.4385e-01)	Acc@1  91.00 ( 92.48)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.014 ( 0.022)	Loss 2.4524e-01 (2.5908e-01)	Acc@1  92.00 ( 92.35)	Acc@5 100.00 ( 99.77)
Test: [ 40/100]	Time  0.016 ( 0.021)	Loss 2.2046e-01 (2.5840e-01)	Acc@1  92.00 ( 92.34)	Acc@5  99.00 ( 99.73)
Test: [ 50/100]	Time  0.016 ( 0.020)	Loss 1.5234e-01 (2.5650e-01)	Acc@1  94.00 ( 92.39)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.015 ( 0.020)	Loss 2.4426e-01 (2.4990e-01)	Acc@1  95.00 ( 92.46)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.018 ( 0.019)	Loss 4.2236e-01 (2.4557e-01)	Acc@1  90.00 ( 92.63)	Acc@5 100.00 ( 99.80)
Test: [ 80/100]	Time  0.018 ( 0.019)	Loss 2.0300e-01 (2.4476e-01)	Acc@1  95.00 ( 92.60)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.019 ( 0.019)	Loss 1.9812e-01 (2.4362e-01)	Acc@1  92.00 ( 92.54)	Acc@5 100.00 ( 99.81)
 * Acc@1 92.580 Acc@5 99.810
### epoch[34] execution time: 15.257111549377441
EPOCH 35
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [35][  0/391]	Time  0.210 ( 0.210)	Data  0.173 ( 0.173)	Loss 5.4230e-02 (5.4230e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [35][ 10/391]	Time  0.032 ( 0.050)	Data  0.001 ( 0.017)	Loss 1.1768e-01 (5.9283e-02)	Acc@1  95.31 ( 98.08)	Acc@5 100.00 (100.00)
Epoch: [35][ 20/391]	Time  0.032 ( 0.042)	Data  0.001 ( 0.010)	Loss 3.2043e-02 (5.2670e-02)	Acc@1  99.22 ( 98.40)	Acc@5 100.00 (100.00)
Epoch: [35][ 30/391]	Time  0.031 ( 0.039)	Data  0.001 ( 0.007)	Loss 3.6133e-02 (4.7768e-02)	Acc@1  98.44 ( 98.51)	Acc@5 100.00 (100.00)
Epoch: [35][ 40/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.006)	Loss 4.0161e-02 (5.0093e-02)	Acc@1  98.44 ( 98.46)	Acc@5 100.00 (100.00)
Epoch: [35][ 50/391]	Time  0.033 ( 0.037)	Data  0.002 ( 0.005)	Loss 6.7810e-02 (4.8976e-02)	Acc@1  96.88 ( 98.48)	Acc@5 100.00 (100.00)
Epoch: [35][ 60/391]	Time  0.030 ( 0.036)	Data  0.001 ( 0.004)	Loss 3.8025e-02 (4.6698e-02)	Acc@1  98.44 ( 98.53)	Acc@5 100.00 (100.00)
Epoch: [35][ 70/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 5.7190e-02 (4.8365e-02)	Acc@1  98.44 ( 98.42)	Acc@5 100.00 (100.00)
Epoch: [35][ 80/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.004)	Loss 3.2745e-02 (4.9205e-02)	Acc@1  99.22 ( 98.38)	Acc@5 100.00 (100.00)
Epoch: [35][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 3.0930e-02 (4.9073e-02)	Acc@1 100.00 ( 98.39)	Acc@5 100.00 (100.00)
Epoch: [35][100/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 3.6133e-02 (4.8877e-02)	Acc@1  99.22 ( 98.41)	Acc@5 100.00 (100.00)
Epoch: [35][110/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 6.0181e-02 (4.9340e-02)	Acc@1  97.66 ( 98.40)	Acc@5 100.00 (100.00)
Epoch: [35][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.8228e-02 (4.9899e-02)	Acc@1  99.22 ( 98.41)	Acc@5 100.00 (100.00)
Epoch: [35][130/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.003)	Loss 7.4036e-02 (5.0677e-02)	Acc@1  97.66 ( 98.38)	Acc@5 100.00 (100.00)
Epoch: [35][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.2542e-02 (5.0205e-02)	Acc@1 100.00 ( 98.40)	Acc@5 100.00 (100.00)
Epoch: [35][150/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.0140e-02 (4.9835e-02)	Acc@1  99.22 ( 98.42)	Acc@5 100.00 (100.00)
Epoch: [35][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.6895e-02 (5.0110e-02)	Acc@1  96.88 ( 98.38)	Acc@5 100.00 (100.00)
Epoch: [35][170/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.2744e-02 (5.0560e-02)	Acc@1  96.88 ( 98.35)	Acc@5 100.00 (100.00)
Epoch: [35][180/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.7627e-02 (5.0596e-02)	Acc@1  97.66 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [35][190/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.5461e-02 (5.0440e-02)	Acc@1  98.44 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [35][200/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.2369e-02 (5.0347e-02)	Acc@1 100.00 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [35][210/391]	Time  0.034 ( 0.034)	Data  0.002 ( 0.002)	Loss 3.2837e-02 (5.0704e-02)	Acc@1  99.22 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [35][220/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.5406e-02 (5.0506e-02)	Acc@1  99.22 ( 98.38)	Acc@5 100.00 (100.00)
Epoch: [35][230/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.0568e-02 (5.0702e-02)	Acc@1  97.66 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [35][240/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.5959e-02 (5.0303e-02)	Acc@1  98.44 ( 98.40)	Acc@5 100.00 (100.00)
Epoch: [35][250/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.4668e-02 (5.0028e-02)	Acc@1  98.44 ( 98.40)	Acc@5 100.00 (100.00)
Epoch: [35][260/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.0690e-02 (4.9966e-02)	Acc@1  97.66 ( 98.42)	Acc@5 100.00 (100.00)
Epoch: [35][270/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.0380e-02 (5.0260e-02)	Acc@1 100.00 ( 98.41)	Acc@5 100.00 (100.00)
Epoch: [35][280/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.4810e-02 (5.0398e-02)	Acc@1  97.66 ( 98.41)	Acc@5 100.00 (100.00)
Epoch: [35][290/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.9652e-02 (5.0821e-02)	Acc@1  97.66 ( 98.39)	Acc@5 100.00 (100.00)
Epoch: [35][300/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.6040e-02 (5.0534e-02)	Acc@1  97.66 ( 98.41)	Acc@5 100.00 (100.00)
Epoch: [35][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.8082e-02 (5.0393e-02)	Acc@1 100.00 ( 98.42)	Acc@5 100.00 (100.00)
Epoch: [35][320/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.8239e-02 (5.0062e-02)	Acc@1  99.22 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [35][330/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.4323e-02 (5.0199e-02)	Acc@1  99.22 ( 98.43)	Acc@5 100.00 (100.00)
Epoch: [35][340/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.5450e-02 (5.0105e-02)	Acc@1  99.22 ( 98.42)	Acc@5 100.00 (100.00)
Epoch: [35][350/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.4027e-02 (5.0011e-02)	Acc@1  99.22 ( 98.42)	Acc@5 100.00 (100.00)
Epoch: [35][360/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.1788e-02 (4.9811e-02)	Acc@1  98.44 ( 98.43)	Acc@5 100.00 (100.00)
Epoch: [35][370/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.6306e-02 (5.0162e-02)	Acc@1  99.22 ( 98.42)	Acc@5 100.00 (100.00)
Epoch: [35][380/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.5925e-02 (5.0287e-02)	Acc@1 100.00 ( 98.42)	Acc@5 100.00 (100.00)
Epoch: [35][390/391]	Time  0.021 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.0497e-02 (5.0363e-02)	Acc@1  98.75 ( 98.41)	Acc@5 100.00 (100.00)
## e[35] optimizer.zero_grad (sum) time: 0.16998696327209473
## e[35]       loss.backward (sum) time: 4.042643308639526
## e[35]      optimizer.step (sum) time: 1.2222886085510254
## epoch[35] training(only) time: 13.220062971115112
# Switched to evaluate mode...
Test: [  0/100]	Time  0.168 ( 0.168)	Loss 1.5759e-01 (1.5759e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.014 ( 0.030)	Loss 1.4807e-01 (2.1528e-01)	Acc@1  94.00 ( 92.91)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.017 ( 0.024)	Loss 3.6230e-01 (2.4089e-01)	Acc@1  91.00 ( 92.81)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.014 ( 0.022)	Loss 2.5000e-01 (2.6008e-01)	Acc@1  93.00 ( 92.26)	Acc@5 100.00 ( 99.81)
Test: [ 40/100]	Time  0.024 ( 0.021)	Loss 2.3730e-01 (2.6199e-01)	Acc@1  92.00 ( 92.20)	Acc@5  99.00 ( 99.78)
Test: [ 50/100]	Time  0.015 ( 0.020)	Loss 1.4319e-01 (2.5948e-01)	Acc@1  93.00 ( 92.25)	Acc@5 100.00 ( 99.80)
Test: [ 60/100]	Time  0.018 ( 0.019)	Loss 2.3340e-01 (2.5316e-01)	Acc@1  93.00 ( 92.36)	Acc@5 100.00 ( 99.82)
Test: [ 70/100]	Time  0.014 ( 0.019)	Loss 4.0625e-01 (2.4909e-01)	Acc@1  89.00 ( 92.49)	Acc@5 100.00 ( 99.82)
Test: [ 80/100]	Time  0.019 ( 0.019)	Loss 1.9592e-01 (2.4889e-01)	Acc@1  95.00 ( 92.49)	Acc@5 100.00 ( 99.81)
Test: [ 90/100]	Time  0.014 ( 0.018)	Loss 1.6345e-01 (2.4764e-01)	Acc@1  92.00 ( 92.41)	Acc@5 100.00 ( 99.82)
 * Acc@1 92.500 Acc@5 99.820
### epoch[35] execution time: 15.156813621520996
EPOCH 36
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [36][  0/391]	Time  0.211 ( 0.211)	Data  0.170 ( 0.170)	Loss 6.2927e-02 (6.2927e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [36][ 10/391]	Time  0.032 ( 0.051)	Data  0.001 ( 0.017)	Loss 2.8000e-02 (5.0184e-02)	Acc@1 100.00 ( 98.65)	Acc@5 100.00 (100.00)
Epoch: [36][ 20/391]	Time  0.032 ( 0.042)	Data  0.001 ( 0.010)	Loss 9.7656e-02 (4.5921e-02)	Acc@1  96.09 ( 98.70)	Acc@5 100.00 (100.00)
Epoch: [36][ 30/391]	Time  0.032 ( 0.040)	Data  0.001 ( 0.007)	Loss 3.4119e-02 (4.3464e-02)	Acc@1  98.44 ( 98.74)	Acc@5 100.00 (100.00)
Epoch: [36][ 40/391]	Time  0.032 ( 0.038)	Data  0.001 ( 0.006)	Loss 6.6833e-02 (4.6242e-02)	Acc@1  98.44 ( 98.65)	Acc@5 100.00 (100.00)
Epoch: [36][ 50/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 8.0322e-02 (4.6427e-02)	Acc@1  97.66 ( 98.62)	Acc@5 100.00 (100.00)
Epoch: [36][ 60/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.004)	Loss 3.3997e-02 (4.6775e-02)	Acc@1  99.22 ( 98.66)	Acc@5 100.00 ( 99.99)
Epoch: [36][ 70/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 5.8136e-02 (4.6308e-02)	Acc@1  97.66 ( 98.69)	Acc@5 100.00 ( 99.99)
Epoch: [36][ 80/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.004)	Loss 2.4612e-02 (4.7317e-02)	Acc@1  99.22 ( 98.66)	Acc@5 100.00 ( 99.99)
Epoch: [36][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.2858e-02 (4.6669e-02)	Acc@1 100.00 ( 98.71)	Acc@5 100.00 ( 99.99)
Epoch: [36][100/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.3346e-02 (4.6129e-02)	Acc@1  99.22 ( 98.73)	Acc@5 100.00 ( 99.99)
Epoch: [36][110/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.1637e-02 (4.5808e-02)	Acc@1 100.00 ( 98.74)	Acc@5 100.00 ( 99.99)
Epoch: [36][120/391]	Time  0.031 ( 0.035)	Data  0.003 ( 0.003)	Loss 3.0228e-02 (4.5818e-02)	Acc@1 100.00 ( 98.72)	Acc@5 100.00 ( 99.99)
Epoch: [36][130/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.8585e-02 (4.5047e-02)	Acc@1 100.00 ( 98.74)	Acc@5 100.00 ( 99.99)
Epoch: [36][140/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.5497e-02 (4.4818e-02)	Acc@1  99.22 ( 98.73)	Acc@5 100.00 ( 99.99)
Epoch: [36][150/391]	Time  0.026 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.7079e-02 (4.5409e-02)	Acc@1  99.22 ( 98.71)	Acc@5 100.00 ( 99.99)
Epoch: [36][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.6234e-02 (4.5619e-02)	Acc@1  99.22 ( 98.71)	Acc@5 100.00 (100.00)
Epoch: [36][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.9907e-02 (4.5791e-02)	Acc@1  99.22 ( 98.68)	Acc@5 100.00 (100.00)
Epoch: [36][180/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.2795e-02 (4.5749e-02)	Acc@1  97.66 ( 98.67)	Acc@5 100.00 (100.00)
Epoch: [36][190/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.2397e-02 (4.6363e-02)	Acc@1  96.09 ( 98.63)	Acc@5 100.00 (100.00)
Epoch: [36][200/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.7323e-02 (4.6233e-02)	Acc@1  99.22 ( 98.64)	Acc@5 100.00 (100.00)
Epoch: [36][210/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.0251e-02 (4.6101e-02)	Acc@1  97.66 ( 98.64)	Acc@5 100.00 (100.00)
Epoch: [36][220/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.4392e-02 (4.6347e-02)	Acc@1  98.44 ( 98.62)	Acc@5 100.00 (100.00)
Epoch: [36][230/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0461e-01 (4.6505e-02)	Acc@1  95.31 ( 98.61)	Acc@5 100.00 (100.00)
Epoch: [36][240/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.3140e-02 (4.6608e-02)	Acc@1  97.66 ( 98.61)	Acc@5 100.00 (100.00)
Epoch: [36][250/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.6906e-02 (4.6853e-02)	Acc@1  99.22 ( 98.60)	Acc@5 100.00 (100.00)
Epoch: [36][260/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.8495e-03 (4.6571e-02)	Acc@1 100.00 ( 98.60)	Acc@5 100.00 (100.00)
Epoch: [36][270/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.1683e-02 (4.6034e-02)	Acc@1  99.22 ( 98.62)	Acc@5 100.00 (100.00)
Epoch: [36][280/391]	Time  0.040 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.2327e-02 (4.6565e-02)	Acc@1  97.66 ( 98.60)	Acc@5 100.00 (100.00)
Epoch: [36][290/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.2938e-02 (4.6666e-02)	Acc@1  99.22 ( 98.60)	Acc@5 100.00 (100.00)
Epoch: [36][300/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2054e-02 (4.6524e-02)	Acc@1 100.00 ( 98.60)	Acc@5 100.00 (100.00)
Epoch: [36][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.9459e-02 (4.6333e-02)	Acc@1  98.44 ( 98.61)	Acc@5 100.00 (100.00)
Epoch: [36][320/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.1769e-02 (4.6164e-02)	Acc@1 100.00 ( 98.61)	Acc@5 100.00 (100.00)
Epoch: [36][330/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.1439e-02 (4.6386e-02)	Acc@1  99.22 ( 98.60)	Acc@5 100.00 (100.00)
Epoch: [36][340/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.8615e-02 (4.6609e-02)	Acc@1  98.44 ( 98.57)	Acc@5 100.00 (100.00)
Epoch: [36][350/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9516e-02 (4.6423e-02)	Acc@1  99.22 ( 98.58)	Acc@5 100.00 (100.00)
Epoch: [36][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.0253e-02 (4.6572e-02)	Acc@1  99.22 ( 98.58)	Acc@5 100.00 (100.00)
Epoch: [36][370/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1108e-02 (4.6477e-02)	Acc@1 100.00 ( 98.58)	Acc@5 100.00 (100.00)
Epoch: [36][380/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1589e-02 (4.6423e-02)	Acc@1 100.00 ( 98.57)	Acc@5 100.00 ( 99.99)
Epoch: [36][390/391]	Time  0.025 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.6396e-02 (4.6459e-02)	Acc@1  98.75 ( 98.57)	Acc@5 100.00 ( 99.99)
## e[36] optimizer.zero_grad (sum) time: 0.16866350173950195
## e[36]       loss.backward (sum) time: 3.992141008377075
## e[36]      optimizer.step (sum) time: 1.2274751663208008
## epoch[36] training(only) time: 13.300888776779175
# Switched to evaluate mode...
Test: [  0/100]	Time  0.165 ( 0.165)	Loss 1.7676e-01 (1.7676e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.021 ( 0.031)	Loss 1.4929e-01 (2.1963e-01)	Acc@1  94.00 ( 92.18)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.018 ( 0.024)	Loss 3.3472e-01 (2.4209e-01)	Acc@1  93.00 ( 92.43)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.014 ( 0.021)	Loss 2.3779e-01 (2.6024e-01)	Acc@1  92.00 ( 92.32)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.014 ( 0.020)	Loss 2.4231e-01 (2.6060e-01)	Acc@1  94.00 ( 92.29)	Acc@5  99.00 ( 99.73)
Test: [ 50/100]	Time  0.015 ( 0.020)	Loss 1.5955e-01 (2.5815e-01)	Acc@1  95.00 ( 92.31)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.019 ( 0.019)	Loss 2.2571e-01 (2.5301e-01)	Acc@1  95.00 ( 92.36)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.034 ( 0.019)	Loss 4.4702e-01 (2.4852e-01)	Acc@1  91.00 ( 92.54)	Acc@5 100.00 ( 99.80)
Test: [ 80/100]	Time  0.015 ( 0.019)	Loss 2.0300e-01 (2.4932e-01)	Acc@1  94.00 ( 92.51)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.015 ( 0.019)	Loss 1.6614e-01 (2.4758e-01)	Acc@1  92.00 ( 92.43)	Acc@5 100.00 ( 99.80)
 * Acc@1 92.490 Acc@5 99.800
### epoch[36] execution time: 15.25974702835083
EPOCH 37
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [37][  0/391]	Time  0.215 ( 0.215)	Data  0.177 ( 0.177)	Loss 9.5459e-02 (9.5459e-02)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [37][ 10/391]	Time  0.032 ( 0.049)	Data  0.001 ( 0.018)	Loss 4.5166e-02 (4.3178e-02)	Acc@1  97.66 ( 98.79)	Acc@5 100.00 (100.00)
Epoch: [37][ 20/391]	Time  0.034 ( 0.042)	Data  0.001 ( 0.010)	Loss 2.6978e-02 (3.9780e-02)	Acc@1 100.00 ( 98.85)	Acc@5 100.00 (100.00)
Epoch: [37][ 30/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.007)	Loss 8.0078e-02 (4.1701e-02)	Acc@1  96.88 ( 98.71)	Acc@5 100.00 (100.00)
Epoch: [37][ 40/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.006)	Loss 2.3758e-02 (4.1224e-02)	Acc@1 100.00 ( 98.69)	Acc@5 100.00 (100.00)
Epoch: [37][ 50/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 3.0457e-02 (4.3088e-02)	Acc@1 100.00 ( 98.67)	Acc@5 100.00 (100.00)
Epoch: [37][ 60/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 3.1860e-02 (4.3648e-02)	Acc@1  99.22 ( 98.68)	Acc@5 100.00 (100.00)
Epoch: [37][ 70/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.004)	Loss 3.0701e-02 (4.3797e-02)	Acc@1  99.22 ( 98.67)	Acc@5 100.00 (100.00)
Epoch: [37][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 2.9694e-02 (4.4230e-02)	Acc@1  99.22 ( 98.68)	Acc@5 100.00 (100.00)
Epoch: [37][ 90/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.003)	Loss 8.9355e-02 (4.3940e-02)	Acc@1  96.88 ( 98.70)	Acc@5 100.00 (100.00)
Epoch: [37][100/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 3.3508e-02 (4.3473e-02)	Acc@1  99.22 ( 98.69)	Acc@5 100.00 (100.00)
Epoch: [37][110/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 5.5054e-02 (4.3378e-02)	Acc@1  98.44 ( 98.70)	Acc@5 100.00 (100.00)
Epoch: [37][120/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.6495e-02 (4.3069e-02)	Acc@1 100.00 ( 98.71)	Acc@5 100.00 (100.00)
Epoch: [37][130/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.8889e-02 (4.3256e-02)	Acc@1  97.66 ( 98.70)	Acc@5 100.00 (100.00)
Epoch: [37][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.9327e-02 (4.3036e-02)	Acc@1  99.22 ( 98.70)	Acc@5 100.00 (100.00)
Epoch: [37][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.9440e-02 (4.3005e-02)	Acc@1 100.00 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [37][160/391]	Time  0.032 ( 0.034)	Data  0.002 ( 0.003)	Loss 3.4149e-02 (4.3245e-02)	Acc@1  99.22 ( 98.71)	Acc@5 100.00 (100.00)
Epoch: [37][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.0842e-02 (4.3808e-02)	Acc@1  98.44 ( 98.69)	Acc@5 100.00 (100.00)
Epoch: [37][180/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.8391e-02 (4.3692e-02)	Acc@1  98.44 ( 98.68)	Acc@5 100.00 (100.00)
Epoch: [37][190/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.7302e-02 (4.3982e-02)	Acc@1  99.22 ( 98.69)	Acc@5 100.00 (100.00)
Epoch: [37][200/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.2358e-02 (4.4152e-02)	Acc@1  98.44 ( 98.67)	Acc@5 100.00 (100.00)
Epoch: [37][210/391]	Time  0.030 ( 0.034)	Data  0.002 ( 0.002)	Loss 4.9316e-02 (4.4245e-02)	Acc@1  98.44 ( 98.65)	Acc@5 100.00 (100.00)
Epoch: [37][220/391]	Time  0.032 ( 0.034)	Data  0.002 ( 0.002)	Loss 3.8269e-02 (4.4039e-02)	Acc@1  99.22 ( 98.67)	Acc@5 100.00 (100.00)
Epoch: [37][230/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9226e-02 (4.3808e-02)	Acc@1  99.22 ( 98.67)	Acc@5 100.00 (100.00)
Epoch: [37][240/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.3335e-02 (4.3875e-02)	Acc@1  98.44 ( 98.66)	Acc@5 100.00 (100.00)
Epoch: [37][250/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.7750e-02 (4.3894e-02)	Acc@1  98.44 ( 98.66)	Acc@5 100.00 (100.00)
Epoch: [37][260/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.3284e-02 (4.3632e-02)	Acc@1  97.66 ( 98.67)	Acc@5 100.00 (100.00)
Epoch: [37][270/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.5481e-02 (4.3460e-02)	Acc@1  98.44 ( 98.67)	Acc@5 100.00 (100.00)
Epoch: [37][280/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.2562e-02 (4.3489e-02)	Acc@1  98.44 ( 98.66)	Acc@5 100.00 (100.00)
Epoch: [37][290/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.5864e-02 (4.3537e-02)	Acc@1  98.44 ( 98.65)	Acc@5 100.00 (100.00)
Epoch: [37][300/391]	Time  0.038 ( 0.034)	Data  0.002 ( 0.002)	Loss 3.0014e-02 (4.3486e-02)	Acc@1  99.22 ( 98.66)	Acc@5 100.00 (100.00)
Epoch: [37][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.1143e-02 (4.3533e-02)	Acc@1  98.44 ( 98.65)	Acc@5 100.00 (100.00)
Epoch: [37][320/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1279e-01 (4.3635e-02)	Acc@1  96.09 ( 98.63)	Acc@5 100.00 (100.00)
Epoch: [37][330/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.8631e-02 (4.3543e-02)	Acc@1  99.22 ( 98.64)	Acc@5 100.00 (100.00)
Epoch: [37][340/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.7648e-02 (4.3852e-02)	Acc@1  97.66 ( 98.62)	Acc@5 100.00 (100.00)
Epoch: [37][350/391]	Time  0.027 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.0314e-02 (4.3780e-02)	Acc@1 100.00 ( 98.63)	Acc@5 100.00 (100.00)
Epoch: [37][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.8778e-02 (4.3667e-02)	Acc@1  99.22 ( 98.64)	Acc@5 100.00 (100.00)
Epoch: [37][370/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.5614e-02 (4.3647e-02)	Acc@1  99.22 ( 98.64)	Acc@5 100.00 (100.00)
Epoch: [37][380/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.1270e-02 (4.3426e-02)	Acc@1  98.44 ( 98.65)	Acc@5 100.00 (100.00)
Epoch: [37][390/391]	Time  0.022 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.5645e-02 (4.3452e-02)	Acc@1  98.75 ( 98.65)	Acc@5 100.00 (100.00)
## e[37] optimizer.zero_grad (sum) time: 0.16839194297790527
## e[37]       loss.backward (sum) time: 4.037642955780029
## e[37]      optimizer.step (sum) time: 1.2226979732513428
## epoch[37] training(only) time: 13.208752393722534
# Switched to evaluate mode...
Test: [  0/100]	Time  0.166 ( 0.166)	Loss 1.5698e-01 (1.5698e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.017 ( 0.030)	Loss 1.7993e-01 (2.1983e-01)	Acc@1  90.00 ( 92.36)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.016 ( 0.024)	Loss 3.2593e-01 (2.4101e-01)	Acc@1  91.00 ( 92.48)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.014 ( 0.022)	Loss 2.0520e-01 (2.5814e-01)	Acc@1  94.00 ( 92.29)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.016 ( 0.020)	Loss 2.0850e-01 (2.5783e-01)	Acc@1  93.00 ( 92.37)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.020 ( 0.020)	Loss 1.6858e-01 (2.5698e-01)	Acc@1  93.00 ( 92.37)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.014 ( 0.019)	Loss 2.5244e-01 (2.5107e-01)	Acc@1  94.00 ( 92.51)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.015 ( 0.019)	Loss 4.0405e-01 (2.4666e-01)	Acc@1  92.00 ( 92.70)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.019 ( 0.019)	Loss 2.1582e-01 (2.4663e-01)	Acc@1  93.00 ( 92.67)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.014 ( 0.018)	Loss 1.7664e-01 (2.4486e-01)	Acc@1  92.00 ( 92.58)	Acc@5 100.00 ( 99.80)
 * Acc@1 92.650 Acc@5 99.800
### epoch[37] execution time: 15.155393362045288
EPOCH 38
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [38][  0/391]	Time  0.210 ( 0.210)	Data  0.174 ( 0.174)	Loss 2.7802e-02 (2.7802e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [38][ 10/391]	Time  0.041 ( 0.050)	Data  0.001 ( 0.017)	Loss 3.3875e-02 (3.7989e-02)	Acc@1  99.22 ( 98.79)	Acc@5 100.00 (100.00)
Epoch: [38][ 20/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.010)	Loss 1.8677e-02 (3.6608e-02)	Acc@1 100.00 ( 98.81)	Acc@5 100.00 (100.00)
Epoch: [38][ 30/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.007)	Loss 4.1016e-02 (3.8999e-02)	Acc@1  99.22 ( 98.71)	Acc@5 100.00 (100.00)
Epoch: [38][ 40/391]	Time  0.032 ( 0.038)	Data  0.001 ( 0.006)	Loss 2.9114e-02 (3.7649e-02)	Acc@1 100.00 ( 98.88)	Acc@5 100.00 (100.00)
Epoch: [38][ 50/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.005)	Loss 6.8115e-02 (3.9148e-02)	Acc@1  96.09 ( 98.82)	Acc@5 100.00 (100.00)
Epoch: [38][ 60/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.004)	Loss 3.6377e-02 (4.0965e-02)	Acc@1  99.22 ( 98.71)	Acc@5 100.00 (100.00)
Epoch: [38][ 70/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 2.9129e-02 (4.0066e-02)	Acc@1  99.22 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [38][ 80/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.004)	Loss 4.9133e-02 (3.9978e-02)	Acc@1  98.44 ( 98.69)	Acc@5 100.00 (100.00)
Epoch: [38][ 90/391]	Time  0.043 ( 0.035)	Data  0.001 ( 0.003)	Loss 5.4169e-02 (3.9561e-02)	Acc@1  98.44 ( 98.70)	Acc@5 100.00 (100.00)
Epoch: [38][100/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 6.4941e-02 (4.1475e-02)	Acc@1  98.44 ( 98.73)	Acc@5 100.00 (100.00)
Epoch: [38][110/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 3.4760e-02 (4.2617e-02)	Acc@1  98.44 ( 98.67)	Acc@5 100.00 (100.00)
Epoch: [38][120/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.6417e-02 (4.1775e-02)	Acc@1  96.88 ( 98.68)	Acc@5 100.00 (100.00)
Epoch: [38][130/391]	Time  0.038 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.8158e-02 (4.0678e-02)	Acc@1  99.22 ( 98.74)	Acc@5 100.00 (100.00)
Epoch: [38][140/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.9825e-02 (4.1253e-02)	Acc@1  99.22 ( 98.71)	Acc@5 100.00 (100.00)
Epoch: [38][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 7.7019e-03 (4.1560e-02)	Acc@1 100.00 ( 98.71)	Acc@5 100.00 (100.00)
Epoch: [38][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.6957e-02 (4.1815e-02)	Acc@1 100.00 ( 98.70)	Acc@5 100.00 (100.00)
Epoch: [38][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.1952e-02 (4.1701e-02)	Acc@1  99.22 ( 98.69)	Acc@5 100.00 (100.00)
Epoch: [38][180/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9455e-02 (4.1545e-02)	Acc@1  99.22 ( 98.69)	Acc@5 100.00 (100.00)
Epoch: [38][190/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.8431e-02 (4.1669e-02)	Acc@1  96.88 ( 98.67)	Acc@5 100.00 (100.00)
Epoch: [38][200/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.6474e-02 (4.1365e-02)	Acc@1 100.00 ( 98.68)	Acc@5 100.00 (100.00)
Epoch: [38][210/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.7933e-02 (4.1251e-02)	Acc@1  98.44 ( 98.70)	Acc@5 100.00 (100.00)
Epoch: [38][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.1838e-02 (4.1710e-02)	Acc@1  96.09 ( 98.68)	Acc@5 100.00 (100.00)
Epoch: [38][230/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.3427e-02 (4.1563e-02)	Acc@1  98.44 ( 98.69)	Acc@5 100.00 (100.00)
Epoch: [38][240/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.6885e-02 (4.1962e-02)	Acc@1  97.66 ( 98.67)	Acc@5 100.00 (100.00)
Epoch: [38][250/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.6642e-02 (4.1845e-02)	Acc@1  99.22 ( 98.67)	Acc@5 100.00 (100.00)
Epoch: [38][260/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.4607e-02 (4.1807e-02)	Acc@1  98.44 ( 98.67)	Acc@5 100.00 (100.00)
Epoch: [38][270/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.7985e-02 (4.1302e-02)	Acc@1  99.22 ( 98.70)	Acc@5 100.00 (100.00)
Epoch: [38][280/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.5450e-02 (4.1072e-02)	Acc@1  97.66 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [38][290/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.4302e-02 (4.1371e-02)	Acc@1 100.00 ( 98.70)	Acc@5 100.00 (100.00)
Epoch: [38][300/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.8105e-02 (4.1296e-02)	Acc@1  97.66 ( 98.70)	Acc@5 100.00 (100.00)
Epoch: [38][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.8828e-02 (4.1088e-02)	Acc@1  97.66 ( 98.71)	Acc@5 100.00 (100.00)
Epoch: [38][320/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.8229e-02 (4.1039e-02)	Acc@1  99.22 ( 98.71)	Acc@5 100.00 (100.00)
Epoch: [38][330/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.7252e-02 (4.0905e-02)	Acc@1  99.22 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [38][340/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.1616e-02 (4.0837e-02)	Acc@1  99.22 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [38][350/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.4829e-02 (4.0608e-02)	Acc@1  98.44 ( 98.73)	Acc@5 100.00 (100.00)
Epoch: [38][360/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.0109e-02 (4.0382e-02)	Acc@1 100.00 ( 98.74)	Acc@5 100.00 (100.00)
Epoch: [38][370/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.1647e-02 (4.0221e-02)	Acc@1  99.22 ( 98.75)	Acc@5 100.00 (100.00)
Epoch: [38][380/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.9500e-02 (4.0293e-02)	Acc@1  97.66 ( 98.76)	Acc@5 100.00 (100.00)
Epoch: [38][390/391]	Time  0.023 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.2405e-02 (4.0344e-02)	Acc@1 100.00 ( 98.76)	Acc@5 100.00 (100.00)
## e[38] optimizer.zero_grad (sum) time: 0.16918683052062988
## e[38]       loss.backward (sum) time: 4.070245027542114
## e[38]      optimizer.step (sum) time: 1.2196214199066162
## epoch[38] training(only) time: 13.244044542312622
# Switched to evaluate mode...
Test: [  0/100]	Time  0.177 ( 0.177)	Loss 1.8787e-01 (1.8787e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.018 ( 0.032)	Loss 1.5002e-01 (2.2256e-01)	Acc@1  93.00 ( 92.55)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.014 ( 0.025)	Loss 3.2739e-01 (2.4259e-01)	Acc@1  94.00 ( 92.90)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.025 ( 0.022)	Loss 2.1130e-01 (2.5653e-01)	Acc@1  95.00 ( 92.74)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.016 ( 0.021)	Loss 2.2375e-01 (2.6106e-01)	Acc@1  93.00 ( 92.61)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.5698e-01 (2.5809e-01)	Acc@1  95.00 ( 92.51)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.015 ( 0.019)	Loss 2.2754e-01 (2.5302e-01)	Acc@1  94.00 ( 92.51)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.014 ( 0.019)	Loss 4.3188e-01 (2.4981e-01)	Acc@1  91.00 ( 92.63)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.014 ( 0.019)	Loss 1.8140e-01 (2.4946e-01)	Acc@1  93.00 ( 92.58)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.017 ( 0.018)	Loss 1.6614e-01 (2.4814e-01)	Acc@1  92.00 ( 92.51)	Acc@5 100.00 ( 99.80)
 * Acc@1 92.560 Acc@5 99.810
### epoch[38] execution time: 15.194512605667114
EPOCH 39
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [39][  0/391]	Time  0.214 ( 0.214)	Data  0.178 ( 0.178)	Loss 5.7465e-02 (5.7465e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [39][ 10/391]	Time  0.032 ( 0.049)	Data  0.001 ( 0.018)	Loss 2.7878e-02 (4.0763e-02)	Acc@1  98.44 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [39][ 20/391]	Time  0.029 ( 0.041)	Data  0.001 ( 0.010)	Loss 2.6199e-02 (3.6309e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [39][ 30/391]	Time  0.031 ( 0.038)	Data  0.005 ( 0.007)	Loss 3.6957e-02 (3.6667e-02)	Acc@1  98.44 ( 98.89)	Acc@5 100.00 (100.00)
Epoch: [39][ 40/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.006)	Loss 3.7689e-02 (3.6776e-02)	Acc@1  98.44 ( 98.88)	Acc@5 100.00 (100.00)
Epoch: [39][ 50/391]	Time  0.041 ( 0.037)	Data  0.001 ( 0.005)	Loss 1.5610e-02 (3.6457e-02)	Acc@1 100.00 ( 98.84)	Acc@5 100.00 (100.00)
Epoch: [39][ 60/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.005)	Loss 1.9958e-02 (3.8310e-02)	Acc@1 100.00 ( 98.81)	Acc@5 100.00 (100.00)
Epoch: [39][ 70/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 5.4565e-02 (3.9123e-02)	Acc@1  98.44 ( 98.81)	Acc@5 100.00 (100.00)
Epoch: [39][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.4626e-02 (3.8284e-02)	Acc@1  99.22 ( 98.82)	Acc@5 100.00 (100.00)
Epoch: [39][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 6.4209e-02 (3.9674e-02)	Acc@1  97.66 ( 98.71)	Acc@5 100.00 (100.00)
Epoch: [39][100/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.4628e-02 (3.8594e-02)	Acc@1 100.00 ( 98.78)	Acc@5 100.00 (100.00)
Epoch: [39][110/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 4.6509e-02 (3.9052e-02)	Acc@1  98.44 ( 98.76)	Acc@5 100.00 (100.00)
Epoch: [39][120/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.003)	Loss 5.5695e-02 (3.9391e-02)	Acc@1  97.66 ( 98.75)	Acc@5 100.00 (100.00)
Epoch: [39][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.8706e-02 (3.9005e-02)	Acc@1  98.44 ( 98.76)	Acc@5 100.00 (100.00)
Epoch: [39][140/391]	Time  0.037 ( 0.034)	Data  0.001 ( 0.003)	Loss 9.6283e-03 (3.8103e-02)	Acc@1 100.00 ( 98.81)	Acc@5 100.00 (100.00)
Epoch: [39][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.1666e-02 (3.9000e-02)	Acc@1  97.66 ( 98.78)	Acc@5 100.00 (100.00)
Epoch: [39][160/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.6469e-02 (3.9342e-02)	Acc@1  98.44 ( 98.76)	Acc@5 100.00 (100.00)
Epoch: [39][170/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.3232e-01 (4.0011e-02)	Acc@1  95.31 ( 98.73)	Acc@5  99.22 (100.00)
Epoch: [39][180/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9257e-02 (3.9417e-02)	Acc@1  99.22 ( 98.75)	Acc@5 100.00 (100.00)
Epoch: [39][190/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.1016e-02 (3.9460e-02)	Acc@1  98.44 ( 98.74)	Acc@5 100.00 (100.00)
Epoch: [39][200/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.7068e-02 (3.9303e-02)	Acc@1  96.88 ( 98.75)	Acc@5 100.00 (100.00)
Epoch: [39][210/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.0798e-02 (3.9211e-02)	Acc@1  99.22 ( 98.76)	Acc@5 100.00 (100.00)
Epoch: [39][220/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.2633e-02 (3.9650e-02)	Acc@1  98.44 ( 98.73)	Acc@5 100.00 ( 99.99)
Epoch: [39][230/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.3356e-02 (3.9605e-02)	Acc@1  98.44 ( 98.73)	Acc@5 100.00 ( 99.99)
Epoch: [39][240/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.6580e-02 (3.9359e-02)	Acc@1  98.44 ( 98.74)	Acc@5 100.00 ( 99.99)
Epoch: [39][250/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.9388e-02 (3.9470e-02)	Acc@1 100.00 ( 98.73)	Acc@5 100.00 ( 99.99)
Epoch: [39][260/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.3819e-02 (3.9630e-02)	Acc@1  99.22 ( 98.72)	Acc@5 100.00 ( 99.99)
Epoch: [39][270/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9653e-02 (3.9662e-02)	Acc@1  99.22 ( 98.71)	Acc@5 100.00 ( 99.99)
Epoch: [39][280/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.0698e-02 (3.9801e-02)	Acc@1  97.66 ( 98.71)	Acc@5 100.00 ( 99.99)
Epoch: [39][290/391]	Time  0.028 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.7078e-02 (3.9856e-02)	Acc@1  97.66 ( 98.70)	Acc@5 100.00 ( 99.99)
Epoch: [39][300/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.7863e-02 (3.9671e-02)	Acc@1  98.44 ( 98.71)	Acc@5 100.00 ( 99.99)
Epoch: [39][310/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.7384e-02 (3.9767e-02)	Acc@1  98.44 ( 98.70)	Acc@5 100.00 ( 99.99)
Epoch: [39][320/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.8920e-02 (3.9802e-02)	Acc@1  98.44 ( 98.69)	Acc@5 100.00 (100.00)
Epoch: [39][330/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.7130e-02 (3.9628e-02)	Acc@1 100.00 ( 98.71)	Acc@5 100.00 (100.00)
Epoch: [39][340/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.8605e-02 (3.9707e-02)	Acc@1  99.22 ( 98.71)	Acc@5 100.00 (100.00)
Epoch: [39][350/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.9343e-02 (3.9536e-02)	Acc@1 100.00 ( 98.73)	Acc@5 100.00 (100.00)
Epoch: [39][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1711e-02 (3.9612e-02)	Acc@1 100.00 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [39][370/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.5787e-02 (3.9697e-02)	Acc@1  99.22 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [39][380/391]	Time  0.038 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.3721e-02 (3.9516e-02)	Acc@1  97.66 ( 98.73)	Acc@5 100.00 (100.00)
Epoch: [39][390/391]	Time  0.021 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.9739e-02 (3.9357e-02)	Acc@1  98.75 ( 98.74)	Acc@5 100.00 (100.00)
## e[39] optimizer.zero_grad (sum) time: 0.1703190803527832
## e[39]       loss.backward (sum) time: 4.036165237426758
## e[39]      optimizer.step (sum) time: 1.2198054790496826
## epoch[39] training(only) time: 13.258656740188599
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 2.0911e-01 (2.0911e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.014 ( 0.030)	Loss 1.5576e-01 (2.3268e-01)	Acc@1  93.00 ( 92.55)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.013 ( 0.024)	Loss 3.3643e-01 (2.5189e-01)	Acc@1  92.00 ( 92.62)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.018 ( 0.022)	Loss 2.1948e-01 (2.6356e-01)	Acc@1  95.00 ( 92.48)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.019 ( 0.021)	Loss 2.0349e-01 (2.6474e-01)	Acc@1  92.00 ( 92.39)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.013 ( 0.020)	Loss 1.7712e-01 (2.6371e-01)	Acc@1  93.00 ( 92.33)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 2.4744e-01 (2.5907e-01)	Acc@1  92.00 ( 92.44)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.015 ( 0.019)	Loss 4.3701e-01 (2.5571e-01)	Acc@1  88.00 ( 92.51)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.016 ( 0.019)	Loss 2.2473e-01 (2.5535e-01)	Acc@1  93.00 ( 92.54)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.018 ( 0.019)	Loss 1.8311e-01 (2.5325e-01)	Acc@1  92.00 ( 92.47)	Acc@5 100.00 ( 99.81)
 * Acc@1 92.520 Acc@5 99.820
### epoch[39] execution time: 15.225850820541382
EPOCH 40
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [40][  0/391]	Time  0.213 ( 0.213)	Data  0.182 ( 0.182)	Loss 3.1143e-02 (3.1143e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [40][ 10/391]	Time  0.032 ( 0.050)	Data  0.001 ( 0.018)	Loss 2.3651e-02 (3.2706e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [40][ 20/391]	Time  0.032 ( 0.042)	Data  0.001 ( 0.010)	Loss 4.1107e-02 (3.3071e-02)	Acc@1  97.66 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [40][ 30/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.007)	Loss 1.1787e-02 (3.2595e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [40][ 40/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.006)	Loss 5.5634e-02 (3.4880e-02)	Acc@1  97.66 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [40][ 50/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 3.7262e-02 (3.3868e-02)	Acc@1  98.44 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [40][ 60/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 2.4414e-02 (3.4263e-02)	Acc@1 100.00 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [40][ 70/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.004)	Loss 8.0627e-02 (3.4735e-02)	Acc@1  98.44 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [40][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 5.4230e-02 (3.4111e-02)	Acc@1  98.44 ( 99.01)	Acc@5  99.22 ( 99.99)
Epoch: [40][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 4.4525e-02 (3.4680e-02)	Acc@1  98.44 ( 99.00)	Acc@5 100.00 ( 99.99)
Epoch: [40][100/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.8336e-02 (3.5489e-02)	Acc@1  98.44 ( 98.95)	Acc@5 100.00 ( 99.99)
Epoch: [40][110/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.8198e-02 (3.5653e-02)	Acc@1  99.22 ( 98.97)	Acc@5 100.00 ( 99.99)
Epoch: [40][120/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.4719e-02 (3.5075e-02)	Acc@1 100.00 ( 98.98)	Acc@5 100.00 ( 99.99)
Epoch: [40][130/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.8748e-02 (3.4652e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [40][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 7.1167e-02 (3.4979e-02)	Acc@1  96.88 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [40][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.0201e-02 (3.5275e-02)	Acc@1  97.66 ( 98.98)	Acc@5 100.00 ( 99.99)
Epoch: [40][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.4821e-02 (3.5693e-02)	Acc@1  99.22 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [40][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.7791e-02 (3.6469e-02)	Acc@1  99.22 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [40][180/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.2236e-02 (3.6365e-02)	Acc@1  97.66 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [40][190/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.5502e-02 (3.6593e-02)	Acc@1  98.44 ( 98.91)	Acc@5 100.00 (100.00)
Epoch: [40][200/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5320e-02 (3.6800e-02)	Acc@1 100.00 ( 98.90)	Acc@5 100.00 (100.00)
Epoch: [40][210/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.7170e-02 (3.6185e-02)	Acc@1  98.44 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [40][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.7689e-02 (3.6760e-02)	Acc@1  99.22 ( 98.90)	Acc@5 100.00 (100.00)
Epoch: [40][230/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.6825e-02 (3.6590e-02)	Acc@1  99.22 ( 98.91)	Acc@5 100.00 (100.00)
Epoch: [40][240/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.6169e-02 (3.6725e-02)	Acc@1  98.44 ( 98.90)	Acc@5 100.00 (100.00)
Epoch: [40][250/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.8849e-02 (3.6755e-02)	Acc@1  98.44 ( 98.90)	Acc@5 100.00 (100.00)
Epoch: [40][260/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.0527e-02 (3.6432e-02)	Acc@1  98.44 ( 98.90)	Acc@5 100.00 (100.00)
Epoch: [40][270/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.7069e-02 (3.6523e-02)	Acc@1  99.22 ( 98.89)	Acc@5 100.00 (100.00)
Epoch: [40][280/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.7624e-02 (3.6247e-02)	Acc@1 100.00 ( 98.91)	Acc@5 100.00 (100.00)
Epoch: [40][290/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.6596e-02 (3.6204e-02)	Acc@1  99.22 ( 98.91)	Acc@5 100.00 (100.00)
Epoch: [40][300/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.8580e-02 (3.6041e-02)	Acc@1  99.22 ( 98.90)	Acc@5 100.00 (100.00)
Epoch: [40][310/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1299e-02 (3.5815e-02)	Acc@1 100.00 ( 98.92)	Acc@5 100.00 (100.00)
Epoch: [40][320/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.1830e-02 (3.5714e-02)	Acc@1  98.44 ( 98.92)	Acc@5 100.00 (100.00)
Epoch: [40][330/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.4729e-02 (3.6051e-02)	Acc@1  99.22 ( 98.90)	Acc@5 100.00 (100.00)
Epoch: [40][340/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.2440e-02 (3.5995e-02)	Acc@1  98.44 ( 98.90)	Acc@5 100.00 (100.00)
Epoch: [40][350/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.2084e-02 (3.5976e-02)	Acc@1  99.22 ( 98.90)	Acc@5 100.00 (100.00)
Epoch: [40][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.0364e-02 (3.6031e-02)	Acc@1  96.88 ( 98.90)	Acc@5 100.00 (100.00)
Epoch: [40][370/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.4006e-02 (3.6101e-02)	Acc@1  97.66 ( 98.89)	Acc@5 100.00 (100.00)
Epoch: [40][380/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.4536e-02 (3.6182e-02)	Acc@1 100.00 ( 98.90)	Acc@5 100.00 (100.00)
Epoch: [40][390/391]	Time  0.027 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.7415e-02 (3.6158e-02)	Acc@1  98.75 ( 98.91)	Acc@5 100.00 (100.00)
## e[40] optimizer.zero_grad (sum) time: 0.16889119148254395
## e[40]       loss.backward (sum) time: 4.087828874588013
## e[40]      optimizer.step (sum) time: 1.1879401206970215
## epoch[40] training(only) time: 13.236595630645752
# Switched to evaluate mode...
Test: [  0/100]	Time  0.171 ( 0.171)	Loss 1.5576e-01 (1.5576e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.015 ( 0.031)	Loss 1.5271e-01 (2.0985e-01)	Acc@1  94.00 ( 93.27)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.020 ( 0.024)	Loss 3.6157e-01 (2.4452e-01)	Acc@1  91.00 ( 93.14)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.021 ( 0.022)	Loss 2.2192e-01 (2.6110e-01)	Acc@1  93.00 ( 92.68)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.015 ( 0.020)	Loss 2.0325e-01 (2.6285e-01)	Acc@1  92.00 ( 92.56)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.020 ( 0.020)	Loss 1.4978e-01 (2.6219e-01)	Acc@1  94.00 ( 92.47)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.014 ( 0.019)	Loss 2.7417e-01 (2.5719e-01)	Acc@1  92.00 ( 92.46)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.021 ( 0.019)	Loss 4.2310e-01 (2.5261e-01)	Acc@1  91.00 ( 92.56)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.020 ( 0.019)	Loss 2.1472e-01 (2.5368e-01)	Acc@1  95.00 ( 92.57)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.014 ( 0.019)	Loss 1.5491e-01 (2.5181e-01)	Acc@1  94.00 ( 92.49)	Acc@5 100.00 ( 99.76)
 * Acc@1 92.560 Acc@5 99.760
### epoch[40] execution time: 15.22914171218872
EPOCH 41
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [41][  0/391]	Time  0.212 ( 0.212)	Data  0.181 ( 0.181)	Loss 1.8982e-02 (1.8982e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [41][ 10/391]	Time  0.032 ( 0.050)	Data  0.001 ( 0.018)	Loss 1.9928e-02 (3.8828e-02)	Acc@1 100.00 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [41][ 20/391]	Time  0.032 ( 0.041)	Data  0.001 ( 0.010)	Loss 1.5289e-02 (3.6388e-02)	Acc@1 100.00 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [41][ 30/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.007)	Loss 5.9662e-02 (3.6305e-02)	Acc@1  98.44 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [41][ 40/391]	Time  0.032 ( 0.038)	Data  0.001 ( 0.006)	Loss 2.5848e-02 (3.7318e-02)	Acc@1  98.44 ( 98.89)	Acc@5 100.00 (100.00)
Epoch: [41][ 50/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.005)	Loss 2.2308e-02 (3.4273e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [41][ 60/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.004)	Loss 1.8860e-02 (3.3529e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [41][ 70/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 5.1849e-02 (3.5163e-02)	Acc@1  98.44 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [41][ 80/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.004)	Loss 3.5370e-02 (3.4318e-02)	Acc@1  98.44 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [41][ 90/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 4.3152e-02 (3.4018e-02)	Acc@1  97.66 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [41][100/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 4.5319e-02 (3.5369e-02)	Acc@1  99.22 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [41][110/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 3.5339e-02 (3.5607e-02)	Acc@1  98.44 ( 98.94)	Acc@5 100.00 ( 99.99)
Epoch: [41][120/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 3.1204e-02 (3.5319e-02)	Acc@1  99.22 ( 98.95)	Acc@5 100.00 ( 99.99)
Epoch: [41][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.9449e-02 (3.5287e-02)	Acc@1  99.22 ( 98.93)	Acc@5 100.00 ( 99.99)
Epoch: [41][140/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.4291e-02 (3.4864e-02)	Acc@1  99.22 ( 98.95)	Acc@5 100.00 ( 99.99)
Epoch: [41][150/391]	Time  0.028 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.7739e-02 (3.5407e-02)	Acc@1  99.22 ( 98.95)	Acc@5 100.00 ( 99.99)
Epoch: [41][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.1391e-02 (3.4978e-02)	Acc@1 100.00 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [41][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.6154e-02 (3.4552e-02)	Acc@1  99.22 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [41][180/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.4861e-02 (3.4144e-02)	Acc@1  98.44 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [41][190/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0963e-02 (3.3947e-02)	Acc@1 100.00 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [41][200/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.9103e-02 (3.4323e-02)	Acc@1  98.44 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [41][210/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.1086e-02 (3.4293e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [41][220/391]	Time  0.034 ( 0.034)	Data  0.002 ( 0.002)	Loss 4.4312e-02 (3.4162e-02)	Acc@1 100.00 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [41][230/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.9831e-02 (3.4245e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [41][240/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0872e-02 (3.4104e-02)	Acc@1 100.00 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [41][250/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5289e-02 (3.4100e-02)	Acc@1 100.00 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [41][260/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4511e-02 (3.4248e-02)	Acc@1 100.00 ( 98.99)	Acc@5 100.00 ( 99.99)
Epoch: [41][270/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.7882e-02 (3.4495e-02)	Acc@1  99.22 ( 98.99)	Acc@5 100.00 ( 99.99)
Epoch: [41][280/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.6917e-02 (3.4336e-02)	Acc@1  99.22 ( 98.99)	Acc@5 100.00 ( 99.99)
Epoch: [41][290/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.6133e-02 (3.4441e-02)	Acc@1  98.44 ( 98.97)	Acc@5 100.00 ( 99.99)
Epoch: [41][300/391]	Time  0.028 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.8237e-02 (3.4387e-02)	Acc@1  96.88 ( 98.97)	Acc@5 100.00 ( 99.99)
Epoch: [41][310/391]	Time  0.037 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.6438e-02 (3.4610e-02)	Acc@1  99.22 ( 98.97)	Acc@5 100.00 ( 99.99)
Epoch: [41][320/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.7964e-02 (3.4750e-02)	Acc@1  99.22 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [41][330/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.2239e-03 (3.4483e-02)	Acc@1 100.00 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [41][340/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.6785e-02 (3.4294e-02)	Acc@1  99.22 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [41][350/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.6611e-02 (3.4060e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [41][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.9316e-02 (3.3874e-02)	Acc@1  99.22 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [41][370/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.7219e-02 (3.4261e-02)	Acc@1  96.88 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [41][380/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.3539e-02 (3.4246e-02)	Acc@1  98.44 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [41][390/391]	Time  0.023 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.2103e-03 (3.4182e-02)	Acc@1 100.00 ( 99.01)	Acc@5 100.00 (100.00)
## e[41] optimizer.zero_grad (sum) time: 0.16919279098510742
## e[41]       loss.backward (sum) time: 3.9840176105499268
## e[41]      optimizer.step (sum) time: 1.2237911224365234
## epoch[41] training(only) time: 13.360753059387207
# Switched to evaluate mode...
Test: [  0/100]	Time  0.166 ( 0.166)	Loss 1.6614e-01 (1.6614e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.013 ( 0.030)	Loss 1.7676e-01 (2.2096e-01)	Acc@1  91.00 ( 93.00)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.021 ( 0.024)	Loss 3.2983e-01 (2.4563e-01)	Acc@1  92.00 ( 92.90)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.019 ( 0.022)	Loss 2.2095e-01 (2.6306e-01)	Acc@1  94.00 ( 92.65)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 1.9263e-01 (2.6429e-01)	Acc@1  93.00 ( 92.61)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.016 ( 0.020)	Loss 1.5430e-01 (2.6318e-01)	Acc@1  94.00 ( 92.53)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 2.5537e-01 (2.5771e-01)	Acc@1  95.00 ( 92.66)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.013 ( 0.019)	Loss 4.5337e-01 (2.5228e-01)	Acc@1  89.00 ( 92.72)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.021 ( 0.019)	Loss 1.9031e-01 (2.5318e-01)	Acc@1  94.00 ( 92.72)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.014 ( 0.019)	Loss 1.6577e-01 (2.5132e-01)	Acc@1  93.00 ( 92.65)	Acc@5 100.00 ( 99.77)
 * Acc@1 92.700 Acc@5 99.780
### epoch[41] execution time: 15.311431169509888
EPOCH 42
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [42][  0/391]	Time  0.217 ( 0.217)	Data  0.182 ( 0.182)	Loss 7.3120e-02 (7.3120e-02)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [42][ 10/391]	Time  0.032 ( 0.050)	Data  0.001 ( 0.018)	Loss 3.4088e-02 (2.7038e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [42][ 20/391]	Time  0.033 ( 0.042)	Data  0.001 ( 0.010)	Loss 1.7090e-02 (2.9669e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [42][ 30/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.007)	Loss 2.2324e-02 (2.9499e-02)	Acc@1 100.00 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [42][ 40/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.006)	Loss 5.6213e-02 (3.2581e-02)	Acc@1  98.44 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [42][ 50/391]	Time  0.030 ( 0.037)	Data  0.001 ( 0.005)	Loss 4.8340e-02 (3.3779e-02)	Acc@1  98.44 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [42][ 60/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 2.6260e-02 (3.3627e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [42][ 70/391]	Time  0.034 ( 0.036)	Data  0.002 ( 0.004)	Loss 4.0009e-02 (3.3115e-02)	Acc@1  98.44 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [42][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 2.8809e-02 (3.2562e-02)	Acc@1 100.00 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [42][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.9724e-02 (3.1874e-02)	Acc@1  99.22 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [42][100/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.1108e-02 (3.2011e-02)	Acc@1 100.00 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [42][110/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.3468e-02 (3.2141e-02)	Acc@1 100.00 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [42][120/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.4403e-02 (3.3206e-02)	Acc@1  97.66 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [42][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.9377e-02 (3.3131e-02)	Acc@1  97.66 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [42][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.0060e-02 (3.3209e-02)	Acc@1  97.66 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [42][150/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.1860e-02 (3.2596e-02)	Acc@1  98.44 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [42][160/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.6144e-02 (3.2645e-02)	Acc@1 100.00 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [42][170/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.4495e-02 (3.3651e-02)	Acc@1  97.66 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [42][180/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.7288e-02 (3.3334e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [42][190/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5335e-02 (3.3069e-02)	Acc@1 100.00 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [42][200/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.1820e-02 (3.2970e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [42][210/391]	Time  0.035 ( 0.034)	Data  0.000 ( 0.002)	Loss 2.0691e-02 (3.3379e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [42][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.4017e-02 (3.3386e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [42][230/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1482e-02 (3.2979e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [42][240/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.9886e-02 (3.2884e-02)	Acc@1  98.44 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [42][250/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.0599e-02 (3.2798e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [42][260/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.7261e-02 (3.2927e-02)	Acc@1  97.66 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [42][270/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.0508e-02 (3.2883e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [42][280/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.6000e-02 (3.3166e-02)	Acc@1  97.66 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [42][290/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.4307e-02 (3.3470e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [42][300/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.4098e-02 (3.3465e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [42][310/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4275e-02 (3.3503e-02)	Acc@1 100.00 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [42][320/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.7990e-02 (3.3520e-02)	Acc@1 100.00 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [42][330/391]	Time  0.028 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.5492e-02 (3.3600e-02)	Acc@1  98.44 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [42][340/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.4453e-02 (3.3697e-02)	Acc@1  97.66 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [42][350/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2543e-02 (3.3649e-02)	Acc@1 100.00 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [42][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.6305e-02 (3.3613e-02)	Acc@1  97.66 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [42][370/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.6251e-02 (3.3610e-02)	Acc@1 100.00 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [42][380/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.6260e-02 (3.3448e-02)	Acc@1  99.22 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [42][390/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.3422e-02 (3.3236e-02)	Acc@1 100.00 ( 99.03)	Acc@5 100.00 (100.00)
## e[42] optimizer.zero_grad (sum) time: 0.17014575004577637
## e[42]       loss.backward (sum) time: 4.023969411849976
## e[42]      optimizer.step (sum) time: 1.223433017730713
## epoch[42] training(only) time: 13.278431177139282
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 1.8140e-01 (1.8140e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.016 ( 0.033)	Loss 1.8713e-01 (2.3145e-01)	Acc@1  91.00 ( 92.45)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.017 ( 0.026)	Loss 3.2495e-01 (2.5472e-01)	Acc@1  90.00 ( 92.52)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.018 ( 0.023)	Loss 2.1460e-01 (2.6691e-01)	Acc@1  94.00 ( 92.35)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.014 ( 0.022)	Loss 2.1375e-01 (2.6822e-01)	Acc@1  91.00 ( 92.27)	Acc@5  99.00 ( 99.68)
Test: [ 50/100]	Time  0.014 ( 0.021)	Loss 1.3953e-01 (2.6556e-01)	Acc@1  93.00 ( 92.27)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.015 ( 0.020)	Loss 2.4780e-01 (2.5909e-01)	Acc@1  92.00 ( 92.38)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.016 ( 0.020)	Loss 4.4385e-01 (2.5543e-01)	Acc@1  90.00 ( 92.52)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.9983e-01 (2.5667e-01)	Acc@1  95.00 ( 92.53)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.014 ( 0.019)	Loss 1.8164e-01 (2.5417e-01)	Acc@1  93.00 ( 92.46)	Acc@5 100.00 ( 99.79)
 * Acc@1 92.520 Acc@5 99.790
### epoch[42] execution time: 15.288426399230957
EPOCH 43
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [43][  0/391]	Time  0.214 ( 0.214)	Data  0.180 ( 0.180)	Loss 4.6967e-02 (4.6967e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [43][ 10/391]	Time  0.035 ( 0.050)	Data  0.001 ( 0.018)	Loss 3.1128e-02 (4.0282e-02)	Acc@1  99.22 ( 98.72)	Acc@5 100.00 ( 99.93)
Epoch: [43][ 20/391]	Time  0.032 ( 0.042)	Data  0.001 ( 0.010)	Loss 2.4536e-02 (4.1207e-02)	Acc@1  99.22 ( 98.66)	Acc@5 100.00 ( 99.96)
Epoch: [43][ 30/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.007)	Loss 1.2878e-02 (3.5826e-02)	Acc@1 100.00 ( 98.92)	Acc@5 100.00 ( 99.97)
Epoch: [43][ 40/391]	Time  0.032 ( 0.038)	Data  0.001 ( 0.006)	Loss 1.6830e-02 (3.5109e-02)	Acc@1 100.00 ( 98.91)	Acc@5 100.00 ( 99.98)
Epoch: [43][ 50/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 3.3600e-02 (3.5912e-02)	Acc@1  99.22 ( 98.90)	Acc@5 100.00 ( 99.98)
Epoch: [43][ 60/391]	Time  0.037 ( 0.036)	Data  0.001 ( 0.004)	Loss 3.3081e-02 (3.4152e-02)	Acc@1  98.44 ( 98.94)	Acc@5 100.00 ( 99.99)
Epoch: [43][ 70/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 4.0558e-02 (3.4002e-02)	Acc@1  98.44 ( 98.95)	Acc@5 100.00 ( 99.99)
Epoch: [43][ 80/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 3.5645e-02 (3.4197e-02)	Acc@1  99.22 ( 98.96)	Acc@5 100.00 ( 99.99)
Epoch: [43][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 3.0762e-02 (3.3663e-02)	Acc@1  98.44 ( 98.96)	Acc@5 100.00 ( 99.99)
Epoch: [43][100/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.003)	Loss 6.4148e-02 (3.3181e-02)	Acc@1  96.88 ( 98.96)	Acc@5 100.00 ( 99.99)
Epoch: [43][110/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 5.3802e-02 (3.3147e-02)	Acc@1  97.66 ( 98.96)	Acc@5 100.00 ( 99.99)
Epoch: [43][120/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.5900e-02 (3.3550e-02)	Acc@1  99.22 ( 98.94)	Acc@5 100.00 ( 99.99)
Epoch: [43][130/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 5.2979e-02 (3.2884e-02)	Acc@1  98.44 ( 98.98)	Acc@5 100.00 ( 99.99)
Epoch: [43][140/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.5572e-02 (3.2330e-02)	Acc@1 100.00 ( 99.02)	Acc@5 100.00 ( 99.99)
Epoch: [43][150/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 3.0457e-02 (3.2088e-02)	Acc@1  99.22 ( 99.02)	Acc@5 100.00 ( 99.99)
Epoch: [43][160/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.1403e-02 (3.2370e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [43][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.7231e-02 (3.2367e-02)	Acc@1  99.22 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [43][180/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.3295e-02 (3.2256e-02)	Acc@1  97.66 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [43][190/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.8433e-02 (3.2237e-02)	Acc@1 100.00 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [43][200/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.8177e-02 (3.2312e-02)	Acc@1  97.66 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [43][210/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.4839e-02 (3.2884e-02)	Acc@1  97.66 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [43][220/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.4811e-02 (3.2558e-02)	Acc@1 100.00 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [43][230/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.0823e-02 (3.2635e-02)	Acc@1 100.00 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [43][240/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.8738e-02 (3.2331e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [43][250/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.3727e-02 (3.2310e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [43][260/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.1494e-02 (3.2288e-02)	Acc@1  98.44 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [43][270/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0422e-02 (3.1867e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [43][280/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.7435e-02 (3.1693e-02)	Acc@1  99.22 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [43][290/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.8849e-02 (3.1750e-02)	Acc@1  99.22 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [43][300/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.8361e-02 (3.1787e-02)	Acc@1  97.66 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [43][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.2694e-02 (3.1805e-02)	Acc@1  99.22 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [43][320/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.8046e-02 (3.1701e-02)	Acc@1 100.00 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [43][330/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.2815e-02 (3.1595e-02)	Acc@1  97.66 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [43][340/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2207e-02 (3.1339e-02)	Acc@1 100.00 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [43][350/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.0050e-02 (3.1163e-02)	Acc@1 100.00 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [43][360/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.8706e-02 (3.1073e-02)	Acc@1  99.22 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [43][370/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.9007e-02 (3.1048e-02)	Acc@1  99.22 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [43][380/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.3486e-02 (3.1044e-02)	Acc@1  96.88 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [43][390/391]	Time  0.023 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.8509e-02 (3.0845e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 (100.00)
## e[43] optimizer.zero_grad (sum) time: 0.1683812141418457
## e[43]       loss.backward (sum) time: 3.852543830871582
## e[43]      optimizer.step (sum) time: 1.224076509475708
## epoch[43] training(only) time: 13.352540016174316
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 1.9568e-01 (1.9568e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.014 ( 0.030)	Loss 2.1838e-01 (2.3663e-01)	Acc@1  91.00 ( 92.27)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.015 ( 0.024)	Loss 3.4497e-01 (2.5746e-01)	Acc@1  91.00 ( 92.43)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.021 ( 0.022)	Loss 1.9604e-01 (2.7147e-01)	Acc@1  96.00 ( 92.42)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.018 ( 0.021)	Loss 2.1936e-01 (2.7271e-01)	Acc@1  92.00 ( 92.29)	Acc@5  99.00 ( 99.73)
Test: [ 50/100]	Time  0.014 ( 0.020)	Loss 1.2323e-01 (2.7123e-01)	Acc@1  96.00 ( 92.35)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.019 ( 0.020)	Loss 2.7710e-01 (2.6586e-01)	Acc@1  91.00 ( 92.38)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.016 ( 0.019)	Loss 4.1284e-01 (2.5964e-01)	Acc@1  90.00 ( 92.54)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.013 ( 0.019)	Loss 1.7493e-01 (2.5961e-01)	Acc@1  92.00 ( 92.56)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.018 ( 0.019)	Loss 1.9836e-01 (2.5743e-01)	Acc@1  93.00 ( 92.49)	Acc@5 100.00 ( 99.79)
 * Acc@1 92.560 Acc@5 99.800
### epoch[43] execution time: 15.344012260437012
EPOCH 44
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [44][  0/391]	Time  0.224 ( 0.224)	Data  0.179 ( 0.179)	Loss 2.4796e-02 (2.4796e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [44][ 10/391]	Time  0.037 ( 0.052)	Data  0.001 ( 0.018)	Loss 3.3417e-02 (2.8738e-02)	Acc@1 100.00 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [44][ 20/391]	Time  0.032 ( 0.043)	Data  0.001 ( 0.010)	Loss 5.2002e-02 (2.9772e-02)	Acc@1  97.66 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [44][ 30/391]	Time  0.032 ( 0.040)	Data  0.001 ( 0.007)	Loss 2.3041e-02 (2.7925e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [44][ 40/391]	Time  0.032 ( 0.038)	Data  0.001 ( 0.006)	Loss 2.5940e-02 (2.9285e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [44][ 50/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 3.4424e-02 (2.8978e-02)	Acc@1  98.44 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [44][ 60/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 1.8982e-02 (2.7931e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [44][ 70/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.004)	Loss 2.4292e-02 (2.9050e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [44][ 80/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 2.2568e-02 (2.8575e-02)	Acc@1 100.00 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [44][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 7.9651e-02 (2.9921e-02)	Acc@1  97.66 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [44][100/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.3392e-02 (2.9573e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [44][110/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 5.0262e-02 (2.9491e-02)	Acc@1  98.44 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [44][120/391]	Time  0.029 ( 0.035)	Data  0.001 ( 0.003)	Loss 9.9030e-03 (2.9991e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [44][130/391]	Time  0.028 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.3499e-02 (2.9645e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [44][140/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.1103e-02 (2.9256e-02)	Acc@1 100.00 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [44][150/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.1612e-02 (2.9003e-02)	Acc@1 100.00 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [44][160/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 3.3295e-02 (2.8921e-02)	Acc@1  98.44 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [44][170/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.4027e-02 (2.9344e-02)	Acc@1  98.44 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [44][180/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5358e-02 (2.9479e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [44][190/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.8244e-02 (2.9657e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [44][200/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3535e-02 (2.9384e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [44][210/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.7566e-02 (2.9448e-02)	Acc@1  97.66 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [44][220/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.8229e-02 (2.9662e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [44][230/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.2664e-02 (2.9470e-02)	Acc@1  98.44 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [44][240/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.6213e-02 (2.9609e-02)	Acc@1  97.66 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [44][250/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.2003e-02 (2.9421e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [44][260/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.6052e-02 (2.9391e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [44][270/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.3193e-02 (2.9199e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [44][280/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.6804e-02 (2.9224e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [44][290/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.0457e-02 (2.9411e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [44][300/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.2471e-02 (2.9386e-02)	Acc@1  98.44 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [44][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.2562e-02 (2.9470e-02)	Acc@1  98.44 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [44][320/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.7023e-02 (2.9313e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [44][330/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.2776e-02 (2.9222e-02)	Acc@1  98.44 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [44][340/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1139e-02 (2.9140e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [44][350/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.7319e-02 (2.9250e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [44][360/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.6922e-02 (2.9035e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [44][370/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5411e-02 (2.9007e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [44][380/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.5467e-02 (2.9016e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [44][390/391]	Time  0.021 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0849e-02 (2.9030e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
## e[44] optimizer.zero_grad (sum) time: 0.16928434371948242
## e[44]       loss.backward (sum) time: 4.036302328109741
## e[44]      optimizer.step (sum) time: 1.2045786380767822
## epoch[44] training(only) time: 13.314000606536865
# Switched to evaluate mode...
Test: [  0/100]	Time  0.176 ( 0.176)	Loss 1.7017e-01 (1.7017e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.013 ( 0.031)	Loss 2.1533e-01 (2.3257e-01)	Acc@1  92.00 ( 92.64)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.020 ( 0.025)	Loss 3.3130e-01 (2.5719e-01)	Acc@1  92.00 ( 92.57)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.015 ( 0.022)	Loss 1.8262e-01 (2.6914e-01)	Acc@1  95.00 ( 92.45)	Acc@5 100.00 ( 99.77)
Test: [ 40/100]	Time  0.021 ( 0.021)	Loss 2.4329e-01 (2.6661e-01)	Acc@1  90.00 ( 92.39)	Acc@5  99.00 ( 99.76)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.4038e-01 (2.6614e-01)	Acc@1  96.00 ( 92.39)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.029 ( 0.020)	Loss 2.7905e-01 (2.6136e-01)	Acc@1  92.00 ( 92.49)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.020 ( 0.019)	Loss 4.5190e-01 (2.5675e-01)	Acc@1  90.00 ( 92.63)	Acc@5 100.00 ( 99.80)
Test: [ 80/100]	Time  0.014 ( 0.019)	Loss 1.9568e-01 (2.5811e-01)	Acc@1  95.00 ( 92.63)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.016 ( 0.019)	Loss 1.9885e-01 (2.5599e-01)	Acc@1  92.00 ( 92.56)	Acc@5 100.00 ( 99.80)
 * Acc@1 92.600 Acc@5 99.810
### epoch[44] execution time: 15.313906192779541
EPOCH 45
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [45][  0/391]	Time  0.215 ( 0.215)	Data  0.186 ( 0.186)	Loss 3.0685e-02 (3.0685e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [45][ 10/391]	Time  0.034 ( 0.050)	Data  0.001 ( 0.019)	Loss 1.5991e-02 (2.3763e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [45][ 20/391]	Time  0.032 ( 0.042)	Data  0.001 ( 0.011)	Loss 2.5406e-02 (2.7890e-02)	Acc@1  98.44 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [45][ 30/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.008)	Loss 6.5552e-02 (2.9763e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [45][ 40/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.006)	Loss 3.0991e-02 (2.8276e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [45][ 50/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.005)	Loss 1.8661e-02 (2.7069e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [45][ 60/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.005)	Loss 3.2593e-02 (2.7340e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [45][ 70/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 1.4290e-02 (2.5869e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [45][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.2596e-02 (2.6517e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [45][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 2.5406e-02 (2.7324e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [45][100/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.1520e-02 (2.7488e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [45][110/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.0178e-02 (2.7033e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [45][120/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 3.3081e-02 (2.7190e-02)	Acc@1  98.44 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [45][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.3732e-02 (2.7131e-02)	Acc@1  97.66 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [45][140/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.4076e-02 (2.6953e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [45][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.1626e-02 (2.6697e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [45][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.0710e-02 (2.6619e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [45][170/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.5808e-02 (2.6449e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [45][180/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.8107e-02 (2.6584e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [45][190/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.9957e-02 (2.6514e-02)	Acc@1  98.44 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [45][200/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.1313e-03 (2.6658e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [45][210/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.8707e-02 (2.6847e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [45][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.8076e-02 (2.6938e-02)	Acc@1  98.44 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [45][230/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.0629e-02 (2.7204e-02)	Acc@1  98.44 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [45][240/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.6647e-02 (2.7366e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [45][250/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1299e-02 (2.7558e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [45][260/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.4231e-02 (2.7377e-02)	Acc@1  99.22 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [45][270/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.6846e-02 (2.7337e-02)	Acc@1 100.00 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [45][280/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.0029e-02 (2.7209e-02)	Acc@1  99.22 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [45][290/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.3295e-02 (2.7274e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [45][300/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.1677e-02 (2.7044e-02)	Acc@1  99.22 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [45][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.5635e-02 (2.7139e-02)	Acc@1  99.22 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [45][320/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2169e-02 (2.7032e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [45][330/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3329e-02 (2.7088e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [45][340/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9241e-02 (2.7026e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [45][350/391]	Time  0.044 ( 0.034)	Data  0.003 ( 0.002)	Loss 9.5139e-03 (2.6952e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [45][360/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.5319e-02 (2.6793e-02)	Acc@1  97.66 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [45][370/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.6907e-02 (2.6846e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [45][380/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.6510e-02 (2.6855e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [45][390/391]	Time  0.027 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.7670e-02 (2.7017e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
## e[45] optimizer.zero_grad (sum) time: 0.16877365112304688
## e[45]       loss.backward (sum) time: 4.020935535430908
## e[45]      optimizer.step (sum) time: 1.208341360092163
## epoch[45] training(only) time: 13.259384155273438
# Switched to evaluate mode...
Test: [  0/100]	Time  0.169 ( 0.169)	Loss 1.8665e-01 (1.8665e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.019 ( 0.030)	Loss 1.8103e-01 (2.3806e-01)	Acc@1  92.00 ( 92.55)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.019 ( 0.024)	Loss 3.3667e-01 (2.5857e-01)	Acc@1  90.00 ( 92.67)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.019 ( 0.021)	Loss 2.1252e-01 (2.7189e-01)	Acc@1  94.00 ( 92.55)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 2.2351e-01 (2.7313e-01)	Acc@1  92.00 ( 92.37)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.016 ( 0.020)	Loss 1.5369e-01 (2.7058e-01)	Acc@1  95.00 ( 92.39)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.014 ( 0.019)	Loss 2.4695e-01 (2.6408e-01)	Acc@1  93.00 ( 92.56)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.018 ( 0.019)	Loss 4.4336e-01 (2.5935e-01)	Acc@1  90.00 ( 92.69)	Acc@5 100.00 ( 99.82)
Test: [ 80/100]	Time  0.014 ( 0.018)	Loss 2.0508e-01 (2.6047e-01)	Acc@1  93.00 ( 92.67)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.014 ( 0.018)	Loss 1.9873e-01 (2.5811e-01)	Acc@1  90.00 ( 92.54)	Acc@5 100.00 ( 99.81)
 * Acc@1 92.550 Acc@5 99.820
### epoch[45] execution time: 15.187640190124512
EPOCH 46
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [46][  0/391]	Time  0.219 ( 0.219)	Data  0.183 ( 0.183)	Loss 1.8112e-02 (1.8112e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [46][ 10/391]	Time  0.032 ( 0.050)	Data  0.001 ( 0.018)	Loss 2.5818e-02 (2.3662e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [46][ 20/391]	Time  0.032 ( 0.042)	Data  0.001 ( 0.010)	Loss 5.3253e-02 (2.4204e-02)	Acc@1  97.66 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [46][ 30/391]	Time  0.033 ( 0.039)	Data  0.001 ( 0.007)	Loss 1.2787e-02 (2.3122e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [46][ 40/391]	Time  0.032 ( 0.038)	Data  0.001 ( 0.006)	Loss 6.5994e-03 (2.4516e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [46][ 50/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 2.2156e-02 (2.6386e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [46][ 60/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.004)	Loss 2.5192e-02 (2.6158e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [46][ 70/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 1.4069e-02 (2.7013e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [46][ 80/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.9333e-02 (2.6730e-02)	Acc@1  99.22 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [46][ 90/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.003)	Loss 3.6591e-02 (2.6652e-02)	Acc@1  99.22 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [46][100/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.3697e-02 (2.6350e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [46][110/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.0582e-02 (2.6154e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [46][120/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 7.8201e-03 (2.5996e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [46][130/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 3.5217e-02 (2.5868e-02)	Acc@1  98.44 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [46][140/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 8.5678e-03 (2.5678e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [46][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.5044e-02 (2.6055e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [46][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.4017e-02 (2.6123e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [46][170/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.0889e-02 (2.6133e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [46][180/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.8204e-02 (2.6184e-02)	Acc@1  99.22 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [46][190/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.1067e-02 (2.6402e-02)	Acc@1  98.44 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [46][200/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.3234e-02 (2.6355e-02)	Acc@1  98.44 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [46][210/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.5391e-02 (2.6246e-02)	Acc@1  99.22 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [46][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3344e-02 (2.6119e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [46][230/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.3727e-02 (2.5902e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [46][240/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.4277e-02 (2.5978e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [46][250/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.2720e-02 (2.5966e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [46][260/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.8506e-03 (2.5965e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [46][270/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.2349e-02 (2.6095e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [46][280/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3115e-02 (2.6174e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [46][290/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.9861e-02 (2.6097e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [46][300/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.0716e-02 (2.6391e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [46][310/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9913e-02 (2.6182e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [46][320/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.0714e-03 (2.6377e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [46][330/391]	Time  0.035 ( 0.034)	Data  0.000 ( 0.002)	Loss 3.2715e-02 (2.6330e-02)	Acc@1  98.44 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [46][340/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.1708e-02 (2.6337e-02)	Acc@1  98.44 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [46][350/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.7659e-02 (2.6307e-02)	Acc@1  98.44 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [46][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.4830e-02 (2.6624e-02)	Acc@1  97.66 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [46][370/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4221e-02 (2.6486e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [46][380/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.6337e-02 (2.6566e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [46][390/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1322e-02 (2.6693e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
## e[46] optimizer.zero_grad (sum) time: 0.1691899299621582
## e[46]       loss.backward (sum) time: 4.067693710327148
## e[46]      optimizer.step (sum) time: 1.2159743309020996
## epoch[46] training(only) time: 13.37969160079956
# Switched to evaluate mode...
Test: [  0/100]	Time  0.177 ( 0.177)	Loss 1.7029e-01 (1.7029e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.013 ( 0.031)	Loss 1.9983e-01 (2.3538e-01)	Acc@1  92.00 ( 92.36)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.018 ( 0.025)	Loss 3.6646e-01 (2.6300e-01)	Acc@1  90.00 ( 92.57)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.022 ( 0.022)	Loss 1.9495e-01 (2.7483e-01)	Acc@1  95.00 ( 92.61)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.018 ( 0.021)	Loss 2.3535e-01 (2.7434e-01)	Acc@1  93.00 ( 92.39)	Acc@5 100.00 ( 99.76)
Test: [ 50/100]	Time  0.014 ( 0.020)	Loss 1.3586e-01 (2.7261e-01)	Acc@1  95.00 ( 92.33)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.018 ( 0.019)	Loss 2.6147e-01 (2.6574e-01)	Acc@1  92.00 ( 92.43)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.014 ( 0.019)	Loss 4.4922e-01 (2.6164e-01)	Acc@1  90.00 ( 92.56)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.016 ( 0.019)	Loss 1.8555e-01 (2.6174e-01)	Acc@1  93.00 ( 92.47)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.014 ( 0.019)	Loss 1.8970e-01 (2.5917e-01)	Acc@1  93.00 ( 92.47)	Acc@5 100.00 ( 99.79)
 * Acc@1 92.540 Acc@5 99.800
### epoch[46] execution time: 15.373084306716919
EPOCH 47
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [47][  0/391]	Time  0.221 ( 0.221)	Data  0.185 ( 0.185)	Loss 3.3844e-02 (3.3844e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [47][ 10/391]	Time  0.033 ( 0.051)	Data  0.001 ( 0.019)	Loss 1.5228e-02 (2.3298e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [47][ 20/391]	Time  0.032 ( 0.043)	Data  0.001 ( 0.010)	Loss 2.8778e-02 (2.4609e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [47][ 30/391]	Time  0.032 ( 0.040)	Data  0.001 ( 0.007)	Loss 1.9791e-02 (2.5354e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [47][ 40/391]	Time  0.032 ( 0.038)	Data  0.001 ( 0.006)	Loss 2.9770e-02 (2.6433e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [47][ 50/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 6.9237e-03 (2.6883e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [47][ 60/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 4.7699e-02 (2.7366e-02)	Acc@1  98.44 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [47][ 70/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 5.0415e-02 (2.7195e-02)	Acc@1  98.44 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [47][ 80/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 2.1774e-02 (2.7476e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [47][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 3.4027e-02 (2.7550e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [47][100/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.4610e-02 (2.7557e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [47][110/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.7039e-02 (2.6956e-02)	Acc@1  98.44 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [47][120/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.003)	Loss 4.7302e-02 (2.6802e-02)	Acc@1  98.44 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [47][130/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.3972e-02 (2.6893e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [47][140/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.5116e-02 (2.6227e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [47][150/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 4.1870e-02 (2.6150e-02)	Acc@1  98.44 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [47][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.2166e-02 (2.6462e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [47][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.1088e-02 (2.5916e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [47][180/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.6220e-02 (2.5682e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [47][190/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.3661e-02 (2.5697e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [47][200/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2260e-02 (2.5239e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [47][210/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2947e-02 (2.4892e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [47][220/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.5187e-02 (2.5192e-02)	Acc@1  98.44 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [47][230/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.6817e-03 (2.4905e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [47][240/391]	Time  0.034 ( 0.034)	Data  0.002 ( 0.002)	Loss 3.1494e-02 (2.4799e-02)	Acc@1  98.44 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [47][250/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4854e-02 (2.4748e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [47][260/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.1046e-02 (2.5132e-02)	Acc@1  98.44 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [47][270/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.8387e-02 (2.5011e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [47][280/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.8473e-02 (2.4827e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [47][290/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.4403e-02 (2.4635e-02)	Acc@1  97.66 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [47][300/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.8631e-02 (2.4658e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [47][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3916e-02 (2.4707e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [47][320/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.8997e-02 (2.4920e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [47][330/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.1554e-02 (2.4956e-02)	Acc@1  96.09 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [47][340/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5045e-02 (2.4908e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [47][350/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.1960e-02 (2.4876e-02)	Acc@1  96.88 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [47][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.7136e-02 (2.4916e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [47][370/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.5912e-03 (2.4722e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [47][380/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.6093e-02 (2.4710e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [47][390/391]	Time  0.025 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0602e-01 (2.4864e-02)	Acc@1  96.25 ( 99.32)	Acc@5 100.00 (100.00)
## e[47] optimizer.zero_grad (sum) time: 0.16845393180847168
## e[47]       loss.backward (sum) time: 4.053650379180908
## e[47]      optimizer.step (sum) time: 1.2152512073516846
## epoch[47] training(only) time: 13.31315803527832
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 2.1899e-01 (2.1899e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.014 ( 0.030)	Loss 2.0142e-01 (2.4313e-01)	Acc@1  92.00 ( 92.64)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.016 ( 0.024)	Loss 3.4790e-01 (2.6527e-01)	Acc@1  90.00 ( 92.52)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.014 ( 0.022)	Loss 2.1313e-01 (2.7617e-01)	Acc@1  93.00 ( 92.32)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.021 ( 0.021)	Loss 2.3938e-01 (2.7725e-01)	Acc@1  92.00 ( 92.27)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.014 ( 0.020)	Loss 1.5698e-01 (2.7483e-01)	Acc@1  95.00 ( 92.33)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.014 ( 0.020)	Loss 2.6465e-01 (2.6944e-01)	Acc@1  93.00 ( 92.48)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.014 ( 0.019)	Loss 4.2041e-01 (2.6325e-01)	Acc@1  92.00 ( 92.69)	Acc@5 100.00 ( 99.80)
Test: [ 80/100]	Time  0.016 ( 0.019)	Loss 1.9019e-01 (2.6331e-01)	Acc@1  93.00 ( 92.63)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.014 ( 0.019)	Loss 2.0532e-01 (2.5989e-01)	Acc@1  92.00 ( 92.56)	Acc@5 100.00 ( 99.81)
 * Acc@1 92.580 Acc@5 99.820
### epoch[47] execution time: 15.296288967132568
EPOCH 48
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [48][  0/391]	Time  0.216 ( 0.216)	Data  0.183 ( 0.183)	Loss 2.8625e-02 (2.8625e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [48][ 10/391]	Time  0.033 ( 0.051)	Data  0.001 ( 0.018)	Loss 3.3264e-02 (2.7591e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [48][ 20/391]	Time  0.032 ( 0.043)	Data  0.001 ( 0.010)	Loss 3.1128e-02 (2.5718e-02)	Acc@1  98.44 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [48][ 30/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.007)	Loss 1.3916e-02 (2.3983e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [48][ 40/391]	Time  0.032 ( 0.038)	Data  0.001 ( 0.006)	Loss 5.1361e-02 (2.5642e-02)	Acc@1  97.66 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [48][ 50/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 2.6184e-02 (2.4913e-02)	Acc@1  98.44 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [48][ 60/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.004)	Loss 1.4130e-02 (2.4750e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [48][ 70/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 1.2566e-02 (2.4556e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [48][ 80/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 3.6011e-02 (2.4016e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [48][ 90/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.2405e-02 (2.3768e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [48][100/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 3.4515e-02 (2.4196e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [48][110/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 4.7974e-02 (2.4350e-02)	Acc@1  98.44 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [48][120/391]	Time  0.041 ( 0.035)	Data  0.001 ( 0.003)	Loss 9.6817e-03 (2.4074e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [48][130/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.0727e-02 (2.4192e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [48][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.1185e-02 (2.4083e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [48][150/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 8.4686e-03 (2.4258e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [48][160/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.2680e-02 (2.4159e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [48][170/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.2703e-02 (2.4453e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [48][180/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.0935e-02 (2.4552e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [48][190/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.7319e-02 (2.4296e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [48][200/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.0813e-02 (2.4537e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [48][210/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.6495e-02 (2.4527e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [48][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.2155e-02 (2.4652e-02)	Acc@1  97.66 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [48][230/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.7451e-02 (2.4740e-02)	Acc@1  98.44 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [48][240/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.0416e-02 (2.4476e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [48][250/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2810e-02 (2.4700e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [48][260/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5511e-02 (2.4579e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [48][270/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.0691e-02 (2.4517e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [48][280/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.2766e-02 (2.4399e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [48][290/391]	Time  0.031 ( 0.034)	Data  0.000 ( 0.002)	Loss 4.7302e-02 (2.4302e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [48][300/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.1210e-02 (2.4553e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [48][310/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.9373e-02 (2.4750e-02)	Acc@1  98.44 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [48][320/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.4686e-03 (2.4627e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [48][330/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.8737e-02 (2.4437e-02)	Acc@1  97.66 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [48][340/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.6769e-02 (2.4244e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [48][350/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1101e-02 (2.4209e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [48][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0887e-02 (2.4144e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [48][370/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.0547e-02 (2.4185e-02)	Acc@1  96.09 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [48][380/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.6377e-02 (2.4294e-02)	Acc@1  98.44 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [48][390/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9089e-02 (2.4357e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
## e[48] optimizer.zero_grad (sum) time: 0.16846919059753418
## e[48]       loss.backward (sum) time: 4.007563591003418
## e[48]      optimizer.step (sum) time: 1.2288484573364258
## epoch[48] training(only) time: 13.314807415008545
# Switched to evaluate mode...
Test: [  0/100]	Time  0.169 ( 0.169)	Loss 1.8958e-01 (1.8958e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.014 ( 0.032)	Loss 2.0593e-01 (2.3447e-01)	Acc@1  92.00 ( 92.36)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.019 ( 0.025)	Loss 3.2812e-01 (2.6024e-01)	Acc@1  92.00 ( 92.62)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.021 ( 0.022)	Loss 2.1448e-01 (2.7529e-01)	Acc@1  93.00 ( 92.26)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.014 ( 0.021)	Loss 2.2351e-01 (2.7567e-01)	Acc@1  93.00 ( 92.32)	Acc@5 100.00 ( 99.76)
Test: [ 50/100]	Time  0.018 ( 0.020)	Loss 1.8018e-01 (2.7523e-01)	Acc@1  93.00 ( 92.20)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.021 ( 0.019)	Loss 2.7441e-01 (2.6855e-01)	Acc@1  93.00 ( 92.36)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.016 ( 0.019)	Loss 4.2822e-01 (2.6180e-01)	Acc@1  90.00 ( 92.51)	Acc@5 100.00 ( 99.80)
Test: [ 80/100]	Time  0.016 ( 0.019)	Loss 1.6797e-01 (2.6175e-01)	Acc@1  94.00 ( 92.52)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.016 ( 0.019)	Loss 2.2644e-01 (2.5903e-01)	Acc@1  92.00 ( 92.49)	Acc@5 100.00 ( 99.81)
 * Acc@1 92.500 Acc@5 99.820
### epoch[48] execution time: 15.31059718132019
EPOCH 49
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [49][  0/391]	Time  0.207 ( 0.207)	Data  0.170 ( 0.170)	Loss 1.5404e-02 (1.5404e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [49][ 10/391]	Time  0.032 ( 0.049)	Data  0.001 ( 0.017)	Loss 1.4091e-02 (2.9185e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [49][ 20/391]	Time  0.032 ( 0.041)	Data  0.001 ( 0.010)	Loss 1.0857e-02 (2.6268e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [49][ 30/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.007)	Loss 1.1597e-02 (2.3442e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [49][ 40/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.006)	Loss 1.3931e-02 (2.2194e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [49][ 50/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.005)	Loss 3.0106e-02 (2.2395e-02)	Acc@1  98.44 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [49][ 60/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.004)	Loss 1.0345e-02 (2.2264e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [49][ 70/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 6.0921e-03 (2.3273e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [49][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.4893e-02 (2.3517e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [49][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 4.5624e-03 (2.2887e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [49][100/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.5253e-02 (2.2824e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [49][110/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.4618e-02 (2.2689e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [49][120/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.4809e-02 (2.3008e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [49][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.8055e-02 (2.2992e-02)	Acc@1  98.44 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [49][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.2720e-02 (2.3182e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [49][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.6077e-02 (2.3094e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [49][160/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.0343e-03 (2.2555e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [49][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0948e-02 (2.2178e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [49][180/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.9337e-02 (2.2197e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [49][190/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.6967e-02 (2.2746e-02)	Acc@1  97.66 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [49][200/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.4918e-02 (2.2967e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [49][210/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.6174e-02 (2.2909e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [49][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3908e-02 (2.2664e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [49][230/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.6975e-03 (2.2468e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [49][240/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0078e-02 (2.2670e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [49][250/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.2257e-02 (2.2716e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [49][260/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.4139e-02 (2.3058e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [49][270/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.7281e-02 (2.3274e-02)	Acc@1  97.66 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [49][280/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.7585e-03 (2.3525e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [49][290/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.1988e-02 (2.3750e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [49][300/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.7792e-02 (2.3754e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [49][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5915e-02 (2.4000e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [49][320/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5823e-02 (2.3986e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [49][330/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.5345e-02 (2.4188e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [49][340/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.2354e-02 (2.4096e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [49][350/391]	Time  0.038 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.9733e-03 (2.4041e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [49][360/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.8646e-02 (2.3866e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [49][370/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3779e-02 (2.3845e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [49][380/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.0232e-02 (2.4013e-02)	Acc@1  98.44 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [49][390/391]	Time  0.023 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.7008e-02 (2.3990e-02)	Acc@1  98.75 ( 99.32)	Acc@5 100.00 (100.00)
## e[49] optimizer.zero_grad (sum) time: 0.17236661911010742
## e[49]       loss.backward (sum) time: 4.081156015396118
## e[49]      optimizer.step (sum) time: 1.1946020126342773
## epoch[49] training(only) time: 13.242372512817383
# Switched to evaluate mode...
Test: [  0/100]	Time  0.171 ( 0.171)	Loss 1.8713e-01 (1.8713e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.021 ( 0.032)	Loss 1.9092e-01 (2.4768e-01)	Acc@1  92.00 ( 92.73)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.020 ( 0.025)	Loss 3.5229e-01 (2.7121e-01)	Acc@1  90.00 ( 92.67)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.016 ( 0.022)	Loss 2.0068e-01 (2.8512e-01)	Acc@1  94.00 ( 92.35)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.022 ( 0.021)	Loss 2.1729e-01 (2.8250e-01)	Acc@1  92.00 ( 92.37)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.014 ( 0.020)	Loss 1.8420e-01 (2.8075e-01)	Acc@1  93.00 ( 92.37)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.014 ( 0.020)	Loss 2.7710e-01 (2.7361e-01)	Acc@1  92.00 ( 92.49)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.016 ( 0.019)	Loss 4.1870e-01 (2.6749e-01)	Acc@1  90.00 ( 92.66)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.020 ( 0.019)	Loss 2.0105e-01 (2.6729e-01)	Acc@1  93.00 ( 92.67)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.014 ( 0.019)	Loss 2.4438e-01 (2.6561e-01)	Acc@1  92.00 ( 92.58)	Acc@5 100.00 ( 99.78)
 * Acc@1 92.640 Acc@5 99.790
### epoch[49] execution time: 15.238409042358398
EPOCH 50
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [50][  0/391]	Time  0.213 ( 0.213)	Data  0.180 ( 0.180)	Loss 2.8442e-02 (2.8442e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [50][ 10/391]	Time  0.032 ( 0.049)	Data  0.001 ( 0.018)	Loss 1.7807e-02 (2.1625e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [50][ 20/391]	Time  0.032 ( 0.041)	Data  0.001 ( 0.010)	Loss 2.5284e-02 (2.1670e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [50][ 30/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.007)	Loss 1.7197e-02 (2.2658e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [50][ 40/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.006)	Loss 1.2192e-02 (2.2610e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [50][ 50/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 2.0233e-02 (2.1327e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [50][ 60/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.004)	Loss 3.5339e-02 (2.1450e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [50][ 70/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 2.0477e-02 (2.1668e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [50][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 3.6743e-02 (2.1690e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [50][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.8458e-02 (2.3096e-02)	Acc@1  98.44 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [50][100/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 4.1168e-02 (2.3538e-02)	Acc@1  98.44 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [50][110/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.3504e-02 (2.3548e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [50][120/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.5121e-02 (2.3297e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [50][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.6464e-02 (2.3111e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [50][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.9043e-02 (2.2919e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [50][150/391]	Time  0.033 ( 0.034)	Data  0.000 ( 0.003)	Loss 3.0746e-02 (2.3119e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [50][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.3819e-02 (2.2978e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [50][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.9974e-02 (2.2958e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [50][180/391]	Time  0.039 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3725e-02 (2.2863e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [50][190/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3496e-02 (2.2475e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [50][200/391]	Time  0.037 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3550e-02 (2.2435e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [50][210/391]	Time  0.033 ( 0.034)	Data  0.005 ( 0.002)	Loss 1.3947e-02 (2.2184e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [50][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.8188e-02 (2.2322e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [50][230/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.7150e-02 (2.2287e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [50][240/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.1253e-03 (2.2042e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [50][250/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.7059e-02 (2.1918e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [50][260/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3649e-02 (2.1683e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [50][270/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.3376e-02 (2.1721e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [50][280/391]	Time  0.037 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.6245e-02 (2.1853e-02)	Acc@1  98.44 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [50][290/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.7689e-02 (2.1808e-02)	Acc@1  98.44 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [50][300/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1810e-02 (2.1914e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [50][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.1738e-02 (2.1865e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [50][320/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.3864e-02 (2.1982e-02)	Acc@1  96.88 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [50][330/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.0584e-02 (2.1971e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [50][340/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.0386e-02 (2.1915e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [50][350/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.9575e-03 (2.1949e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [50][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.7983e-03 (2.2035e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [50][370/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2917e-02 (2.2150e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [50][380/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5686e-02 (2.1995e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [50][390/391]	Time  0.025 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.0375e-02 (2.2020e-02)	Acc@1  98.75 ( 99.42)	Acc@5 100.00 (100.00)
## e[50] optimizer.zero_grad (sum) time: 0.16800236701965332
## e[50]       loss.backward (sum) time: 3.946824312210083
## e[50]      optimizer.step (sum) time: 1.2426276206970215
## epoch[50] training(only) time: 13.273737668991089
# Switched to evaluate mode...
Test: [  0/100]	Time  0.175 ( 0.175)	Loss 1.7151e-01 (1.7151e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.022 ( 0.032)	Loss 1.7590e-01 (2.3467e-01)	Acc@1  93.00 ( 92.73)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.018 ( 0.025)	Loss 3.3765e-01 (2.6653e-01)	Acc@1  92.00 ( 92.62)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.013 ( 0.023)	Loss 1.9128e-01 (2.7740e-01)	Acc@1  95.00 ( 92.32)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.016 ( 0.021)	Loss 2.1326e-01 (2.7816e-01)	Acc@1  92.00 ( 92.24)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.015 ( 0.020)	Loss 1.6089e-01 (2.7612e-01)	Acc@1  94.00 ( 92.22)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.014 ( 0.020)	Loss 2.6001e-01 (2.6916e-01)	Acc@1  92.00 ( 92.44)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.018 ( 0.019)	Loss 4.3433e-01 (2.6468e-01)	Acc@1  91.00 ( 92.55)	Acc@5 100.00 ( 99.80)
Test: [ 80/100]	Time  0.021 ( 0.019)	Loss 1.7383e-01 (2.6424e-01)	Acc@1  95.00 ( 92.59)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.020 ( 0.019)	Loss 2.3853e-01 (2.6334e-01)	Acc@1  91.00 ( 92.54)	Acc@5 100.00 ( 99.81)
 * Acc@1 92.550 Acc@5 99.820
### epoch[50] execution time: 15.257352113723755
EPOCH 51
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [51][  0/391]	Time  0.197 ( 0.197)	Data  0.165 ( 0.165)	Loss 1.9104e-02 (1.9104e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [51][ 10/391]	Time  0.032 ( 0.048)	Data  0.001 ( 0.017)	Loss 9.4147e-03 (2.0692e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [51][ 20/391]	Time  0.032 ( 0.041)	Data  0.001 ( 0.009)	Loss 9.3002e-03 (2.0708e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [51][ 30/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.007)	Loss 1.6190e-02 (2.2542e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [51][ 40/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.006)	Loss 7.0457e-03 (2.0723e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [51][ 50/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.005)	Loss 2.4094e-02 (2.2191e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [51][ 60/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 4.3030e-02 (2.1163e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [51][ 70/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.8906e-02 (2.2474e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [51][ 80/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.7700e-02 (2.2568e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [51][ 90/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.2736e-02 (2.2427e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [51][100/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.8387e-02 (2.1814e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [51][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.0628e-02 (2.1596e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [51][120/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.0050e-02 (2.1627e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [51][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.5961e-02 (2.1698e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [51][140/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.3962e-02 (2.1177e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [51][150/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.7384e-02 (2.1323e-02)	Acc@1  98.44 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [51][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9043e-02 (2.1419e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [51][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.4261e-02 (2.1652e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [51][180/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.1290e-02 (2.1618e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [51][190/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.1455e-03 (2.1407e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [51][200/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0841e-02 (2.1373e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [51][210/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0246e-02 (2.1238e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [51][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.7105e-02 (2.1262e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [51][230/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5717e-02 (2.1085e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [51][240/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.1150e-03 (2.0838e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [51][250/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.4536e-02 (2.0762e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [51][260/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0590e-02 (2.0609e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [51][270/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0353e-02 (2.0596e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [51][280/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.8893e-03 (2.0759e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [51][290/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.3478e-02 (2.0841e-02)	Acc@1  98.44 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [51][300/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.3453e-02 (2.0701e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [51][310/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.3956e-02 (2.0584e-02)	Acc@1  97.66 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [51][320/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2787e-02 (2.0888e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [51][330/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.7945e-03 (2.0843e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [51][340/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9897e-02 (2.0801e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [51][350/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.2476e-02 (2.0793e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [51][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.2468e-03 (2.0624e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [51][370/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0979e-02 (2.0675e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [51][380/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.3793e-02 (2.0731e-02)	Acc@1  98.44 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [51][390/391]	Time  0.023 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.2354e-02 (2.0628e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
## e[51] optimizer.zero_grad (sum) time: 0.16955900192260742
## e[51]       loss.backward (sum) time: 4.100033283233643
## e[51]      optimizer.step (sum) time: 1.2072179317474365
## epoch[51] training(only) time: 13.258917331695557
# Switched to evaluate mode...
Test: [  0/100]	Time  0.170 ( 0.170)	Loss 1.8091e-01 (1.8091e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.014 ( 0.030)	Loss 1.9385e-01 (2.4086e-01)	Acc@1  93.00 ( 92.64)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.019 ( 0.024)	Loss 3.3447e-01 (2.6694e-01)	Acc@1  93.00 ( 92.71)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.013 ( 0.021)	Loss 1.8945e-01 (2.8168e-01)	Acc@1  96.00 ( 92.45)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.015 ( 0.020)	Loss 2.1790e-01 (2.7955e-01)	Acc@1  91.00 ( 92.39)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.014 ( 0.019)	Loss 1.3574e-01 (2.7630e-01)	Acc@1  93.00 ( 92.35)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.020 ( 0.019)	Loss 2.5220e-01 (2.6844e-01)	Acc@1  92.00 ( 92.54)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.018 ( 0.019)	Loss 4.2822e-01 (2.6237e-01)	Acc@1  91.00 ( 92.72)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.014 ( 0.018)	Loss 1.7383e-01 (2.6215e-01)	Acc@1  93.00 ( 92.72)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.019 ( 0.018)	Loss 2.1655e-01 (2.6054e-01)	Acc@1  92.00 ( 92.66)	Acc@5 100.00 ( 99.80)
 * Acc@1 92.710 Acc@5 99.810
### epoch[51] execution time: 15.192959785461426
EPOCH 52
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [52][  0/391]	Time  0.216 ( 0.216)	Data  0.175 ( 0.175)	Loss 5.1537e-03 (5.1537e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [52][ 10/391]	Time  0.032 ( 0.051)	Data  0.001 ( 0.017)	Loss 1.5076e-02 (1.9046e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [52][ 20/391]	Time  0.032 ( 0.042)	Data  0.001 ( 0.010)	Loss 1.2787e-02 (1.7898e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [52][ 30/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.007)	Loss 1.2474e-02 (1.9306e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [52][ 40/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.006)	Loss 3.0624e-02 (2.1410e-02)	Acc@1  98.44 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [52][ 50/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 2.1500e-02 (2.0535e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [52][ 60/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 1.2489e-02 (1.9589e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [52][ 70/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 2.2888e-02 (2.0243e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [52][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 3.8574e-02 (2.0194e-02)	Acc@1  98.44 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [52][ 90/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 3.1342e-02 (1.9779e-02)	Acc@1  98.44 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [52][100/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 9.4910e-03 (1.9784e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [52][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.3657e-02 (1.9334e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [52][120/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.3621e-02 (1.9005e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [52][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.0597e-02 (1.9161e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [52][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 9.9487e-03 (1.8907e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [52][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.0777e-02 (1.9192e-02)	Acc@1  98.44 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [52][160/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.7609e-02 (1.9549e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [52][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.3529e-02 (1.9625e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [52][180/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.5055e-02 (1.9803e-02)	Acc@1  98.44 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [52][190/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5526e-02 (1.9701e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [52][200/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.8290e-02 (2.0138e-02)	Acc@1  98.44 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [52][210/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.2932e-03 (1.9884e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [52][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.5833e-02 (1.9717e-02)	Acc@1  98.44 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [52][230/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.8671e-02 (1.9666e-02)	Acc@1  98.44 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [52][240/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.1863e-03 (1.9756e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [52][250/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.7495e-02 (1.9729e-02)	Acc@1  98.44 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [52][260/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.0009e-02 (1.9789e-02)	Acc@1  98.44 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [52][270/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.4017e-02 (1.9997e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [52][280/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.3834e-02 (2.0135e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [52][290/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.1342e-02 (2.0179e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [52][300/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.2166e-02 (2.0066e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [52][310/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.0309e-02 (2.0180e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [52][320/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.8066e-02 (2.0188e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [52][330/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.2932e-03 (2.0128e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [52][340/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5533e-02 (2.0093e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [52][350/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4053e-02 (2.0109e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [52][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.8539e-02 (2.0273e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [52][370/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.9429e-02 (2.0273e-02)	Acc@1  98.44 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [52][380/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.8767e-02 (2.0583e-02)	Acc@1  98.44 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [52][390/391]	Time  0.023 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.6474e-02 (2.0635e-02)	Acc@1  98.75 ( 99.38)	Acc@5 100.00 (100.00)
## e[52] optimizer.zero_grad (sum) time: 0.17058825492858887
## e[52]       loss.backward (sum) time: 4.06125283241272
## e[52]      optimizer.step (sum) time: 1.2129549980163574
## epoch[52] training(only) time: 13.288600206375122
# Switched to evaluate mode...
Test: [  0/100]	Time  0.178 ( 0.178)	Loss 1.7590e-01 (1.7590e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.015 ( 0.030)	Loss 2.0007e-01 (2.5274e-01)	Acc@1  92.00 ( 92.55)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.023 ( 0.024)	Loss 3.8623e-01 (2.7592e-01)	Acc@1  90.00 ( 92.38)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.019 ( 0.022)	Loss 2.3157e-01 (2.8970e-01)	Acc@1  94.00 ( 92.16)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.014 ( 0.021)	Loss 2.2021e-01 (2.8594e-01)	Acc@1  94.00 ( 92.29)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.014 ( 0.020)	Loss 1.4185e-01 (2.8494e-01)	Acc@1  94.00 ( 92.20)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.014 ( 0.019)	Loss 2.8027e-01 (2.8100e-01)	Acc@1  92.00 ( 92.34)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.025 ( 0.019)	Loss 3.9453e-01 (2.7323e-01)	Acc@1  92.00 ( 92.54)	Acc@5 100.00 ( 99.80)
Test: [ 80/100]	Time  0.016 ( 0.019)	Loss 1.6687e-01 (2.7093e-01)	Acc@1  93.00 ( 92.58)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.028 ( 0.018)	Loss 1.9714e-01 (2.6797e-01)	Acc@1  91.00 ( 92.48)	Acc@5 100.00 ( 99.81)
 * Acc@1 92.560 Acc@5 99.820
### epoch[52] execution time: 15.237996101379395
EPOCH 53
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [53][  0/391]	Time  0.214 ( 0.214)	Data  0.173 ( 0.173)	Loss 3.6499e-02 (3.6499e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [53][ 10/391]	Time  0.032 ( 0.051)	Data  0.001 ( 0.017)	Loss 2.3376e-02 (2.7264e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [53][ 20/391]	Time  0.032 ( 0.042)	Data  0.001 ( 0.010)	Loss 3.1555e-02 (2.5464e-02)	Acc@1  98.44 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [53][ 30/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.007)	Loss 2.4719e-02 (2.5562e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [53][ 40/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.006)	Loss 4.9133e-02 (2.4798e-02)	Acc@1  97.66 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [53][ 50/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 9.4452e-03 (2.2830e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [53][ 60/391]	Time  0.027 ( 0.036)	Data  0.001 ( 0.004)	Loss 8.0490e-03 (2.2154e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [53][ 70/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 6.3538e-02 (2.2124e-02)	Acc@1  97.66 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [53][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 4.9553e-03 (2.1154e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [53][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 9.4223e-03 (2.1133e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [53][100/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.3893e-02 (2.1123e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [53][110/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.9333e-02 (2.1260e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [53][120/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.3529e-02 (2.0906e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [53][130/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 5.3215e-03 (2.0545e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [53][140/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.7487e-02 (2.0471e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [53][150/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.1337e-02 (2.0074e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [53][160/391]	Time  0.037 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.1841e-02 (1.9885e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [53][170/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.8433e-02 (1.9923e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [53][180/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.0608e-02 (2.0261e-02)	Acc@1  98.44 ( 99.46)	Acc@5  99.22 (100.00)
Epoch: [53][190/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1124e-02 (2.0350e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [53][200/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.0419e-03 (2.0513e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [53][210/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.5909e-02 (2.0501e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [53][220/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0826e-02 (2.0203e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [53][230/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.4893e-03 (2.0159e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [53][240/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.2196e-02 (2.0079e-02)	Acc@1  97.66 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [53][250/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.8433e-02 (2.0029e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [53][260/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.8666e-02 (2.0027e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [53][270/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.3703e-03 (1.9921e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [53][280/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.8458e-02 (2.0207e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [53][290/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.5497e-02 (2.0081e-02)	Acc@1  98.44 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [53][300/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2993e-02 (2.0169e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [53][310/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.4261e-02 (2.0171e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [53][320/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.1534e-02 (2.0242e-02)	Acc@1  97.66 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [53][330/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.6657e-02 (2.0334e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [53][340/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.0487e-02 (2.0181e-02)	Acc@1  98.44 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [53][350/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.6469e-02 (2.0320e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [53][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.8000e-02 (2.0167e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [53][370/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1925e-02 (2.0229e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [53][380/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.6517e-03 (2.0274e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [53][390/391]	Time  0.023 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.6926e-02 (2.0396e-02)	Acc@1  98.75 ( 99.43)	Acc@5 100.00 (100.00)
## e[53] optimizer.zero_grad (sum) time: 0.16910243034362793
## e[53]       loss.backward (sum) time: 4.098439455032349
## e[53]      optimizer.step (sum) time: 1.2112784385681152
## epoch[53] training(only) time: 13.31973147392273
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 1.7712e-01 (1.7712e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.014 ( 0.030)	Loss 2.1497e-01 (2.4739e-01)	Acc@1  91.00 ( 92.73)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.016 ( 0.024)	Loss 3.5229e-01 (2.6735e-01)	Acc@1  90.00 ( 92.71)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.014 ( 0.021)	Loss 1.9592e-01 (2.7882e-01)	Acc@1  94.00 ( 92.52)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.014 ( 0.020)	Loss 1.9958e-01 (2.7839e-01)	Acc@1  92.00 ( 92.37)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.015 ( 0.020)	Loss 1.5796e-01 (2.7677e-01)	Acc@1  94.00 ( 92.35)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.014 ( 0.019)	Loss 2.4902e-01 (2.7370e-01)	Acc@1  93.00 ( 92.43)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.016 ( 0.019)	Loss 4.7827e-01 (2.6750e-01)	Acc@1  90.00 ( 92.59)	Acc@5 100.00 ( 99.82)
Test: [ 80/100]	Time  0.014 ( 0.019)	Loss 1.8494e-01 (2.6770e-01)	Acc@1  93.00 ( 92.59)	Acc@5 100.00 ( 99.81)
Test: [ 90/100]	Time  0.014 ( 0.018)	Loss 1.9617e-01 (2.6543e-01)	Acc@1  92.00 ( 92.56)	Acc@5 100.00 ( 99.82)
 * Acc@1 92.620 Acc@5 99.830
### epoch[53] execution time: 15.292716264724731
EPOCH 54
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [54][  0/391]	Time  0.218 ( 0.218)	Data  0.182 ( 0.182)	Loss 3.2623e-02 (3.2623e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [54][ 10/391]	Time  0.032 ( 0.050)	Data  0.001 ( 0.018)	Loss 1.7639e-02 (1.7668e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [54][ 20/391]	Time  0.032 ( 0.042)	Data  0.001 ( 0.010)	Loss 1.0101e-02 (1.6064e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [54][ 30/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.007)	Loss 2.8244e-02 (1.5455e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [54][ 40/391]	Time  0.032 ( 0.038)	Data  0.001 ( 0.006)	Loss 2.8534e-02 (1.6632e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [54][ 50/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 1.8570e-02 (1.6478e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [54][ 60/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 4.9095e-03 (1.7186e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [54][ 70/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 3.5156e-02 (1.7381e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [54][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 2.5940e-02 (1.7860e-02)	Acc@1  98.44 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [54][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.8036e-02 (1.7930e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [54][100/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.6144e-02 (1.8053e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [54][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.4435e-02 (1.7766e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [54][120/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.8570e-02 (1.7538e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [54][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.5610e-02 (1.8079e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [54][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 7.7820e-03 (1.8157e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [54][150/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.8778e-02 (1.7885e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [54][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.1727e-03 (1.7880e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [54][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.6288e-03 (1.7960e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [54][180/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1459e-02 (1.7958e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [54][190/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4343e-02 (1.7796e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [54][200/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.7537e-02 (1.8119e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [54][210/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1772e-02 (1.7979e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [54][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.6512e-03 (1.8133e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [54][230/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.2087e-03 (1.8186e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [54][240/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.3793e-03 (1.8182e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [54][250/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.6627e-02 (1.8289e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [54][260/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.2400e-02 (1.8575e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [54][270/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5221e-02 (1.8540e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [54][280/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.1199e-02 (1.8508e-02)	Acc@1  98.44 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [54][290/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.0354e-03 (1.8377e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [54][300/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.6749e-02 (1.8231e-02)	Acc@1  98.44 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [54][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.6388e-02 (1.8207e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [54][320/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.9188e-03 (1.8162e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [54][330/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3527e-02 (1.8228e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [54][340/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3779e-02 (1.8203e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [54][350/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2634e-02 (1.8154e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [54][360/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.3283e-03 (1.8180e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [54][370/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.7044e-02 (1.8153e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [54][380/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.8239e-02 (1.8140e-02)	Acc@1  98.44 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [54][390/391]	Time  0.024 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.7212e-02 (1.8042e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
## e[54] optimizer.zero_grad (sum) time: 0.16980981826782227
## e[54]       loss.backward (sum) time: 4.045344352722168
## e[54]      optimizer.step (sum) time: 1.212695837020874
## epoch[54] training(only) time: 13.229657649993896
# Switched to evaluate mode...
Test: [  0/100]	Time  0.170 ( 0.170)	Loss 1.9116e-01 (1.9116e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.018 ( 0.031)	Loss 2.3083e-01 (2.4993e-01)	Acc@1  92.00 ( 92.45)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.015 ( 0.024)	Loss 3.1616e-01 (2.6771e-01)	Acc@1  92.00 ( 92.57)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.016 ( 0.022)	Loss 2.0361e-01 (2.8039e-01)	Acc@1  95.00 ( 92.42)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.014 ( 0.021)	Loss 2.1021e-01 (2.7927e-01)	Acc@1  92.00 ( 92.34)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.013 ( 0.020)	Loss 1.6687e-01 (2.7782e-01)	Acc@1  95.00 ( 92.27)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.015 ( 0.020)	Loss 2.6245e-01 (2.7315e-01)	Acc@1  93.00 ( 92.41)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.022 ( 0.019)	Loss 4.6460e-01 (2.6738e-01)	Acc@1  90.00 ( 92.65)	Acc@5 100.00 ( 99.80)
Test: [ 80/100]	Time  0.022 ( 0.019)	Loss 1.7981e-01 (2.6787e-01)	Acc@1  93.00 ( 92.65)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.014 ( 0.019)	Loss 2.3291e-01 (2.6429e-01)	Acc@1  92.00 ( 92.66)	Acc@5 100.00 ( 99.81)
 * Acc@1 92.730 Acc@5 99.810
### epoch[54] execution time: 15.215126276016235
EPOCH 55
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [55][  0/391]	Time  0.213 ( 0.213)	Data  0.178 ( 0.178)	Loss 4.6043e-03 (4.6043e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [55][ 10/391]	Time  0.032 ( 0.050)	Data  0.001 ( 0.018)	Loss 1.4290e-02 (1.4208e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [55][ 20/391]	Time  0.032 ( 0.042)	Data  0.001 ( 0.010)	Loss 1.4435e-02 (1.2689e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [55][ 30/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.007)	Loss 2.2125e-02 (1.3435e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [55][ 40/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.006)	Loss 1.4816e-02 (1.4099e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [55][ 50/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 1.4587e-02 (1.5635e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [55][ 60/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 2.3087e-02 (1.5673e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [55][ 70/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.0017e-02 (1.5629e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [55][ 80/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.7242e-02 (1.5949e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [55][ 90/391]	Time  0.029 ( 0.035)	Data  0.001 ( 0.003)	Loss 6.0158e-03 (1.5769e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [55][100/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.1744e-02 (1.5961e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [55][110/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.4534e-02 (1.6616e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [55][120/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 3.6316e-02 (1.6569e-02)	Acc@1  98.44 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [55][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.4704e-02 (1.6641e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [55][140/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.7406e-03 (1.6359e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [55][150/391]	Time  0.028 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.5122e-03 (1.6658e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [55][160/391]	Time  0.028 ( 0.034)	Data  0.001 ( 0.003)	Loss 7.4921e-03 (1.6829e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [55][170/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.1204e-02 (1.6773e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [55][180/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5656e-02 (1.6819e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [55][190/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3351e-02 (1.6610e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [55][200/391]	Time  0.027 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4999e-02 (1.6565e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [55][210/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3924e-02 (1.6633e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [55][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3351e-02 (1.6847e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [55][230/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.7624e-02 (1.6877e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [55][240/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.6071e-03 (1.6787e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [55][250/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.5319e-02 (1.6786e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [55][260/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.0340e-02 (1.6975e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [55][270/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.7438e-03 (1.7110e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [55][280/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.6256e-03 (1.7255e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [55][290/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.2623e-02 (1.7271e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [55][300/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9913e-02 (1.7305e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [55][310/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4725e-02 (1.7262e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [55][320/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.8201e-03 (1.7143e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [55][330/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.5258e-02 (1.7327e-02)	Acc@1  98.44 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [55][340/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.4485e-02 (1.7350e-02)	Acc@1  97.66 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [55][350/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1787e-02 (1.7469e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [55][360/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.8723e-02 (1.7553e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [55][370/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2314e-02 (1.7546e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [55][380/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.6352e-02 (1.7503e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [55][390/391]	Time  0.027 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.3536e-03 (1.7576e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
## e[55] optimizer.zero_grad (sum) time: 0.1680748462677002
## e[55]       loss.backward (sum) time: 3.899667263031006
## e[55]      optimizer.step (sum) time: 1.2418291568756104
## epoch[55] training(only) time: 13.292523384094238
# Switched to evaluate mode...
Test: [  0/100]	Time  0.165 ( 0.165)	Loss 1.8994e-01 (1.8994e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.025 ( 0.031)	Loss 1.9751e-01 (2.4486e-01)	Acc@1  91.00 ( 92.55)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.016 ( 0.024)	Loss 3.5034e-01 (2.7312e-01)	Acc@1  93.00 ( 92.57)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.014 ( 0.022)	Loss 2.0740e-01 (2.8496e-01)	Acc@1  93.00 ( 92.35)	Acc@5  99.00 ( 99.61)
Test: [ 40/100]	Time  0.019 ( 0.021)	Loss 2.1289e-01 (2.8389e-01)	Acc@1  90.00 ( 92.24)	Acc@5 100.00 ( 99.66)
Test: [ 50/100]	Time  0.015 ( 0.020)	Loss 1.5332e-01 (2.8414e-01)	Acc@1  96.00 ( 92.20)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 2.4072e-01 (2.7575e-01)	Acc@1  93.00 ( 92.48)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.014 ( 0.019)	Loss 4.3628e-01 (2.6964e-01)	Acc@1  91.00 ( 92.65)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.014 ( 0.019)	Loss 2.0947e-01 (2.7187e-01)	Acc@1  94.00 ( 92.65)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.019 ( 0.019)	Loss 2.2009e-01 (2.6878e-01)	Acc@1  93.00 ( 92.65)	Acc@5 100.00 ( 99.77)
 * Acc@1 92.680 Acc@5 99.780
### epoch[55] execution time: 15.247971773147583
EPOCH 56
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [56][  0/391]	Time  0.204 ( 0.204)	Data  0.172 ( 0.172)	Loss 2.4094e-02 (2.4094e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [56][ 10/391]	Time  0.032 ( 0.049)	Data  0.001 ( 0.017)	Loss 1.2787e-02 (1.2500e-02)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [56][ 20/391]	Time  0.033 ( 0.041)	Data  0.001 ( 0.010)	Loss 2.3514e-02 (1.3755e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [56][ 30/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.007)	Loss 7.7744e-03 (1.3950e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [56][ 40/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.006)	Loss 2.5925e-02 (1.4695e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [56][ 50/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.005)	Loss 2.8946e-02 (1.6506e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [56][ 60/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.004)	Loss 7.2899e-03 (1.5839e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [56][ 70/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.9577e-02 (1.6074e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [56][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 7.2632e-03 (1.5410e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [56][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.4473e-02 (1.6055e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [56][100/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.5701e-02 (1.5968e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [56][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.2383e-02 (1.5963e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [56][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.1475e-02 (1.5790e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [56][130/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.6877e-03 (1.5924e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [56][140/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.6846e-02 (1.5959e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [56][150/391]	Time  0.044 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.9516e-02 (1.5929e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [56][160/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3084e-02 (1.6159e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [56][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.9095e-03 (1.6127e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [56][180/391]	Time  0.034 ( 0.034)	Data  0.000 ( 0.002)	Loss 1.6693e-02 (1.6165e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [56][190/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.9487e-03 (1.5946e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [56][200/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.8583e-03 (1.5925e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [56][210/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1276e-02 (1.5961e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [56][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.2016e-03 (1.5841e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [56][230/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.4078e-02 (1.5904e-02)	Acc@1  98.44 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [56][240/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1993e-02 (1.5842e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [56][250/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.0243e-02 (1.5965e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [56][260/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.0741e-03 (1.5892e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [56][270/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.7171e-03 (1.5784e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [56][280/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.8158e-02 (1.5747e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [56][290/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4771e-02 (1.5889e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [56][300/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.6670e-03 (1.5930e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [56][310/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.4088e-02 (1.6010e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [56][320/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.7069e-02 (1.6060e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [56][330/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.4292e-02 (1.6204e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [56][340/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.7838e-02 (1.6198e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [56][350/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.3171e-03 (1.6211e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [56][360/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2672e-02 (1.6207e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [56][370/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9653e-02 (1.6047e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [56][380/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.6915e-03 (1.6071e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [56][390/391]	Time  0.023 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.8341e-02 (1.6019e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
## e[56] optimizer.zero_grad (sum) time: 0.1701974868774414
## e[56]       loss.backward (sum) time: 4.043750047683716
## e[56]      optimizer.step (sum) time: 1.2108242511749268
## epoch[56] training(only) time: 13.268234729766846
# Switched to evaluate mode...
Test: [  0/100]	Time  0.167 ( 0.167)	Loss 2.0105e-01 (2.0105e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.020 ( 0.032)	Loss 2.2314e-01 (2.5155e-01)	Acc@1  92.00 ( 92.64)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.020 ( 0.025)	Loss 3.4521e-01 (2.7240e-01)	Acc@1  91.00 ( 92.57)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.019 ( 0.022)	Loss 2.0654e-01 (2.8803e-01)	Acc@1  94.00 ( 92.32)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.014 ( 0.021)	Loss 2.2559e-01 (2.8627e-01)	Acc@1  93.00 ( 92.34)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.016 ( 0.020)	Loss 1.6272e-01 (2.8634e-01)	Acc@1  93.00 ( 92.27)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.020 ( 0.019)	Loss 3.1689e-01 (2.8039e-01)	Acc@1  93.00 ( 92.46)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.014 ( 0.019)	Loss 4.1748e-01 (2.7392e-01)	Acc@1  90.00 ( 92.61)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.016 ( 0.019)	Loss 1.7639e-01 (2.7349e-01)	Acc@1  94.00 ( 92.69)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.015 ( 0.019)	Loss 2.3523e-01 (2.7078e-01)	Acc@1  92.00 ( 92.63)	Acc@5 100.00 ( 99.79)
 * Acc@1 92.680 Acc@5 99.800
### epoch[56] execution time: 15.223977327346802
EPOCH 57
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [57][  0/391]	Time  0.209 ( 0.209)	Data  0.173 ( 0.173)	Loss 2.2003e-02 (2.2003e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [57][ 10/391]	Time  0.032 ( 0.050)	Data  0.001 ( 0.017)	Loss 7.7972e-03 (1.5350e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [57][ 20/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.010)	Loss 3.8757e-02 (1.6140e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [57][ 30/391]	Time  0.033 ( 0.040)	Data  0.001 ( 0.007)	Loss 2.6001e-02 (1.8535e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [57][ 40/391]	Time  0.032 ( 0.038)	Data  0.001 ( 0.006)	Loss 3.4058e-02 (1.8628e-02)	Acc@1  98.44 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [57][ 50/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 9.1553e-03 (1.7562e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [57][ 60/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.004)	Loss 2.1759e-02 (1.8213e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [57][ 70/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 9.9411e-03 (1.8060e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [57][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 3.0380e-02 (1.8452e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [57][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 3.3875e-02 (1.8237e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [57][100/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 7.1907e-03 (1.7850e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [57][110/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.4565e-02 (1.7683e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [57][120/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 3.4241e-02 (1.7450e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [57][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.1002e-02 (1.7220e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [57][140/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.3344e-02 (1.7127e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [57][150/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.1581e-02 (1.7485e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [57][160/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.1194e-02 (1.7878e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [57][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3130e-02 (1.7761e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [57][180/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.8198e-02 (1.7585e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [57][190/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0139e-02 (1.7573e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [57][200/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.7044e-02 (1.7452e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [57][210/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.7531e-03 (1.7224e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [57][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.0334e-02 (1.7182e-02)	Acc@1  98.44 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [57][230/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.8000e-02 (1.7168e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [57][240/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5091e-02 (1.7249e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [57][250/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1604e-02 (1.7316e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [57][260/391]	Time  0.037 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9287e-02 (1.7404e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [57][270/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5137e-02 (1.7272e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [57][280/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.9035e-03 (1.7294e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [57][290/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.7166e-02 (1.7225e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [57][300/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.6769e-02 (1.7105e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [57][310/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4626e-02 (1.7161e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [57][320/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.7740e-02 (1.7168e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [57][330/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.6801e-03 (1.7187e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [57][340/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1322e-02 (1.7179e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [57][350/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.7558e-03 (1.7409e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [57][360/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.7351e-03 (1.7530e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [57][370/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.9358e-02 (1.7498e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [57][380/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 8.5678e-03 (1.7530e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [57][390/391]	Time  0.024 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.1240e-02 (1.7726e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
## e[57] optimizer.zero_grad (sum) time: 0.16972112655639648
## e[57]       loss.backward (sum) time: 4.039586544036865
## e[57]      optimizer.step (sum) time: 1.2137861251831055
## epoch[57] training(only) time: 13.238811254501343
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 2.0703e-01 (2.0703e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.014 ( 0.031)	Loss 2.5854e-01 (2.5691e-01)	Acc@1  91.00 ( 92.27)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.014 ( 0.024)	Loss 3.4033e-01 (2.7596e-01)	Acc@1  92.00 ( 92.38)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.015 ( 0.022)	Loss 1.9141e-01 (2.8846e-01)	Acc@1  96.00 ( 92.23)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.017 ( 0.021)	Loss 2.1631e-01 (2.8598e-01)	Acc@1  93.00 ( 92.17)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.6907e-01 (2.8563e-01)	Acc@1  95.00 ( 92.25)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.020 ( 0.019)	Loss 2.6611e-01 (2.7850e-01)	Acc@1  93.00 ( 92.49)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.020 ( 0.019)	Loss 4.1235e-01 (2.7227e-01)	Acc@1  92.00 ( 92.69)	Acc@5 100.00 ( 99.82)
Test: [ 80/100]	Time  0.016 ( 0.019)	Loss 1.9702e-01 (2.7243e-01)	Acc@1  91.00 ( 92.73)	Acc@5 100.00 ( 99.81)
Test: [ 90/100]	Time  0.020 ( 0.019)	Loss 2.1704e-01 (2.6922e-01)	Acc@1  93.00 ( 92.70)	Acc@5 100.00 ( 99.82)
 * Acc@1 92.730 Acc@5 99.820
### epoch[57] execution time: 15.215238332748413
EPOCH 58
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [58][  0/391]	Time  0.220 ( 0.220)	Data  0.182 ( 0.182)	Loss 2.1561e-02 (2.1561e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [58][ 10/391]	Time  0.032 ( 0.051)	Data  0.001 ( 0.018)	Loss 4.6356e-02 (1.9323e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [58][ 20/391]	Time  0.032 ( 0.043)	Data  0.001 ( 0.010)	Loss 1.1543e-02 (1.7305e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [58][ 30/391]	Time  0.033 ( 0.040)	Data  0.001 ( 0.007)	Loss 1.5350e-02 (1.6927e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [58][ 40/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.006)	Loss 1.8570e-02 (1.7839e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [58][ 50/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 1.8188e-02 (1.6924e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [58][ 60/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 2.0615e-02 (1.6687e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [58][ 70/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 1.3680e-02 (1.7119e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [58][ 80/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.004)	Loss 1.7761e-02 (1.7651e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [58][ 90/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.0719e-02 (1.7587e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [58][100/391]	Time  0.028 ( 0.035)	Data  0.001 ( 0.003)	Loss 3.7292e-02 (1.8070e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [58][110/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.0056e-02 (1.7542e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [58][120/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.9770e-02 (1.7226e-02)	Acc@1  98.44 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [58][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.7609e-02 (1.7109e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [58][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.9427e-03 (1.6824e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [58][150/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.3875e-02 (1.6774e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [58][160/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.5526e-02 (1.6603e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [58][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.0425e-03 (1.6520e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [58][180/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.0414e-03 (1.6351e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [58][190/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.7481e-02 (1.6489e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [58][200/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1734e-02 (1.6466e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [58][210/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4420e-02 (1.6229e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [58][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3443e-02 (1.6456e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [58][230/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5671e-02 (1.6534e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [58][240/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9501e-02 (1.6472e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [58][250/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.1760e-03 (1.6611e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [58][260/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.6588e-03 (1.6547e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [58][270/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.8409e-03 (1.6652e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [58][280/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2627e-02 (1.6638e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [58][290/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.0894e-03 (1.6589e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [58][300/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9211e-02 (1.6521e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [58][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.1713e-02 (1.6830e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [58][320/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1749e-02 (1.6738e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [58][330/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2039e-02 (1.6797e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [58][340/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.5460e-03 (1.6851e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [58][350/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.2995e-02 (1.6945e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [58][360/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4679e-02 (1.6842e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [58][370/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.8275e-02 (1.6621e-02)	Acc@1  98.44 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [58][380/391]	Time  0.034 ( 0.034)	Data  0.000 ( 0.002)	Loss 3.8574e-02 (1.6587e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [58][390/391]	Time  0.022 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.8631e-02 (1.6552e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
## e[58] optimizer.zero_grad (sum) time: 0.16890478134155273
## e[58]       loss.backward (sum) time: 4.002878904342651
## e[58]      optimizer.step (sum) time: 1.2185797691345215
## epoch[58] training(only) time: 13.368015050888062
# Switched to evaluate mode...
Test: [  0/100]	Time  0.173 ( 0.173)	Loss 1.7688e-01 (1.7688e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.020 ( 0.031)	Loss 2.4988e-01 (2.4945e-01)	Acc@1  91.00 ( 92.73)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.014 ( 0.024)	Loss 3.7036e-01 (2.7539e-01)	Acc@1  91.00 ( 92.71)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.020 ( 0.022)	Loss 1.8042e-01 (2.8720e-01)	Acc@1  96.00 ( 92.45)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.021 ( 0.021)	Loss 2.3193e-01 (2.8757e-01)	Acc@1  92.00 ( 92.34)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.022 ( 0.020)	Loss 1.4307e-01 (2.8788e-01)	Acc@1  95.00 ( 92.27)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.020 ( 0.020)	Loss 2.5952e-01 (2.8027e-01)	Acc@1  92.00 ( 92.46)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.020 ( 0.019)	Loss 4.4263e-01 (2.7408e-01)	Acc@1  90.00 ( 92.58)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.014 ( 0.019)	Loss 1.4417e-01 (2.7475e-01)	Acc@1  94.00 ( 92.60)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.016 ( 0.019)	Loss 2.0117e-01 (2.7284e-01)	Acc@1  94.00 ( 92.58)	Acc@5 100.00 ( 99.76)
 * Acc@1 92.620 Acc@5 99.770
### epoch[58] execution time: 15.347064733505249
EPOCH 59
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [59][  0/391]	Time  0.212 ( 0.212)	Data  0.178 ( 0.178)	Loss 6.4392e-03 (6.4392e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [59][ 10/391]	Time  0.032 ( 0.050)	Data  0.001 ( 0.018)	Loss 1.3298e-02 (1.6796e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [59][ 20/391]	Time  0.032 ( 0.042)	Data  0.001 ( 0.010)	Loss 1.7410e-02 (1.6428e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [59][ 30/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.007)	Loss 1.1337e-02 (1.5879e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [59][ 40/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.006)	Loss 2.1572e-03 (1.6077e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [59][ 50/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.005)	Loss 8.3542e-03 (1.5842e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [59][ 60/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 8.1100e-03 (1.5869e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [59][ 70/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.2001e-02 (1.5280e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [59][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.1040e-02 (1.5802e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [59][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.6800e-02 (1.5955e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [59][100/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.3359e-02 (1.5857e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [59][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.0844e-03 (1.5957e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [59][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.4580e-02 (1.5591e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [59][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.3168e-02 (1.5337e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [59][140/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.3640e-03 (1.5152e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [59][150/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 9.0408e-03 (1.5392e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [59][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.1880e-03 (1.5291e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [59][170/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.1787e-03 (1.5370e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [59][180/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.1270e-03 (1.5236e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [59][190/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.6370e-03 (1.5380e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [59][200/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.0240e-03 (1.5434e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [59][210/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.9733e-03 (1.5229e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [59][220/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.8234e-02 (1.5299e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [59][230/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.4271e-02 (1.5360e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [59][240/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.5776e-02 (1.5756e-02)	Acc@1  97.66 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [59][250/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1375e-02 (1.5746e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [59][260/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.1881e-02 (1.5876e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [59][270/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.8402e-02 (1.5800e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [59][280/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.2539e-03 (1.5759e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [59][290/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.7711e-03 (1.5865e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [59][300/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.5596e-03 (1.5700e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [59][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.1460e-03 (1.5787e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [59][320/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4450e-02 (1.5682e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [59][330/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4297e-02 (1.5659e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [59][340/391]	Time  0.037 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.8051e-02 (1.5630e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [59][350/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0933e-02 (1.5531e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [59][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.6052e-02 (1.5438e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [59][370/391]	Time  0.037 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.1150e-03 (1.5409e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [59][380/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.1831e-03 (1.5473e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [59][390/391]	Time  0.026 ( 0.033)	Data  0.001 ( 0.002)	Loss 7.1716e-03 (1.5486e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
## e[59] optimizer.zero_grad (sum) time: 0.16918230056762695
## e[59]       loss.backward (sum) time: 4.086670398712158
## e[59]      optimizer.step (sum) time: 1.2053892612457275
## epoch[59] training(only) time: 13.231619596481323
# Switched to evaluate mode...
Test: [  0/100]	Time  0.181 ( 0.181)	Loss 2.1143e-01 (2.1143e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.015 ( 0.031)	Loss 1.8481e-01 (2.5254e-01)	Acc@1  92.00 ( 92.64)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.024 ( 0.025)	Loss 3.7524e-01 (2.7949e-01)	Acc@1  91.00 ( 92.71)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.014 ( 0.022)	Loss 1.8237e-01 (2.8921e-01)	Acc@1  94.00 ( 92.58)	Acc@5  99.00 ( 99.65)
Test: [ 40/100]	Time  0.016 ( 0.021)	Loss 2.2717e-01 (2.8923e-01)	Acc@1  92.00 ( 92.51)	Acc@5 100.00 ( 99.66)
Test: [ 50/100]	Time  0.020 ( 0.020)	Loss 1.3953e-01 (2.8889e-01)	Acc@1  95.00 ( 92.47)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.016 ( 0.020)	Loss 2.5952e-01 (2.8082e-01)	Acc@1  93.00 ( 92.61)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 4.4214e-01 (2.7432e-01)	Acc@1  90.00 ( 92.69)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.5039e-01 (2.7470e-01)	Acc@1  95.00 ( 92.73)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.016 ( 0.019)	Loss 2.1179e-01 (2.7147e-01)	Acc@1  93.00 ( 92.67)	Acc@5 100.00 ( 99.78)
 * Acc@1 92.740 Acc@5 99.780
### epoch[59] execution time: 15.216478109359741
EPOCH 60
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [60][  0/391]	Time  0.214 ( 0.214)	Data  0.180 ( 0.180)	Loss 1.5732e-02 (1.5732e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [60][ 10/391]	Time  0.032 ( 0.050)	Data  0.001 ( 0.018)	Loss 4.9210e-03 (1.0697e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [60][ 20/391]	Time  0.032 ( 0.042)	Data  0.001 ( 0.010)	Loss 1.0315e-02 (1.1987e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [60][ 30/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.007)	Loss 2.1225e-02 (1.1988e-02)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [60][ 40/391]	Time  0.032 ( 0.038)	Data  0.001 ( 0.006)	Loss 1.3641e-02 (1.3666e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [60][ 50/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 7.9498e-03 (1.3821e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [60][ 60/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 1.0742e-02 (1.3426e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [60][ 70/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.3565e-02 (1.3605e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [60][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.0124e-02 (1.3340e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [60][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.1070e-02 (1.3483e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [60][100/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.5360e-02 (1.4185e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [60][110/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 8.0338e-03 (1.4398e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [60][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.0658e-02 (1.4319e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [60][130/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.1643e-03 (1.4288e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [60][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.6978e-02 (1.4617e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [60][150/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.0828e-02 (1.4650e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [60][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.2806e-02 (1.5066e-02)	Acc@1  98.44 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [60][170/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.1569e-03 (1.4754e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [60][180/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.0408e-03 (1.4675e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [60][190/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.6182e-03 (1.4504e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [60][200/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.0926e-03 (1.4284e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [60][210/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.5452e-02 (1.4201e-02)	Acc@1  98.44 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [60][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.9199e-03 (1.4141e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [60][230/391]	Time  0.038 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.1874e-03 (1.4289e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [60][240/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1086e-02 (1.4388e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [60][250/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.3902e-03 (1.4450e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [60][260/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5747e-02 (1.4233e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [60][270/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.6828e-03 (1.4126e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [60][280/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1681e-02 (1.4019e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [60][290/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.1454e-02 (1.3925e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [60][300/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.4468e-03 (1.3847e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [60][310/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.7502e-02 (1.3788e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [60][320/391]	Time  0.046 ( 0.034)	Data  0.002 ( 0.002)	Loss 2.9984e-02 (1.4036e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [60][330/391]	Time  0.027 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.4686e-03 (1.4023e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [60][340/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.6495e-02 (1.4049e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [60][350/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.5689e-03 (1.4033e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [60][360/391]	Time  0.042 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.0157e-02 (1.4033e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [60][370/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1925e-02 (1.3976e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [60][380/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.9602e-02 (1.4115e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [60][390/391]	Time  0.024 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.5040e-02 (1.4051e-02)	Acc@1  98.75 ( 99.68)	Acc@5 100.00 (100.00)
## e[60] optimizer.zero_grad (sum) time: 0.17021727561950684
## e[60]       loss.backward (sum) time: 3.9812259674072266
## e[60]      optimizer.step (sum) time: 1.2184607982635498
## epoch[60] training(only) time: 13.274278163909912
# Switched to evaluate mode...
Test: [  0/100]	Time  0.166 ( 0.166)	Loss 2.3730e-01 (2.3730e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.023 ( 0.031)	Loss 1.8530e-01 (2.5967e-01)	Acc@1  92.00 ( 92.55)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.016 ( 0.024)	Loss 3.4692e-01 (2.7968e-01)	Acc@1  91.00 ( 92.67)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.017 ( 0.022)	Loss 1.7627e-01 (2.8858e-01)	Acc@1  95.00 ( 92.29)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.016 ( 0.021)	Loss 2.2498e-01 (2.8828e-01)	Acc@1  92.00 ( 92.27)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.014 ( 0.020)	Loss 1.5295e-01 (2.8635e-01)	Acc@1  95.00 ( 92.24)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.015 ( 0.019)	Loss 2.4170e-01 (2.7863e-01)	Acc@1  93.00 ( 92.46)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.014 ( 0.019)	Loss 4.6045e-01 (2.7230e-01)	Acc@1  89.00 ( 92.58)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.013 ( 0.019)	Loss 1.6626e-01 (2.7288e-01)	Acc@1  94.00 ( 92.62)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.018 ( 0.019)	Loss 2.1021e-01 (2.6913e-01)	Acc@1  94.00 ( 92.57)	Acc@5 100.00 ( 99.80)
 * Acc@1 92.610 Acc@5 99.800
### epoch[60] execution time: 15.222326040267944
EPOCH 61
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [61][  0/391]	Time  0.211 ( 0.211)	Data  0.178 ( 0.178)	Loss 6.3820e-03 (6.3820e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [61][ 10/391]	Time  0.032 ( 0.049)	Data  0.001 ( 0.018)	Loss 9.4681e-03 (1.1468e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [61][ 20/391]	Time  0.032 ( 0.042)	Data  0.001 ( 0.010)	Loss 4.5242e-03 (1.5217e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 ( 99.96)
Epoch: [61][ 30/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.007)	Loss 4.1046e-02 (1.6003e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 ( 99.97)
Epoch: [61][ 40/391]	Time  0.032 ( 0.038)	Data  0.001 ( 0.006)	Loss 6.1607e-03 (1.6366e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 ( 99.98)
Epoch: [61][ 50/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 4.8218e-03 (1.4698e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 ( 99.98)
Epoch: [61][ 60/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.004)	Loss 2.6722e-03 (1.5183e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 ( 99.99)
Epoch: [61][ 70/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 2.6291e-02 (1.5566e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 ( 99.99)
Epoch: [61][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.1749e-02 (1.5802e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 ( 99.99)
Epoch: [61][ 90/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.9150e-02 (1.5785e-02)	Acc@1  98.44 ( 99.60)	Acc@5 100.00 ( 99.99)
Epoch: [61][100/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 5.9700e-03 (1.5536e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 ( 99.99)
Epoch: [61][110/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.2018e-02 (1.5584e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 ( 99.99)
Epoch: [61][120/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 6.8398e-03 (1.5653e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 ( 99.99)
Epoch: [61][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.7596e-03 (1.5274e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 ( 99.99)
Epoch: [61][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 8.8730e-03 (1.5043e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 ( 99.99)
Epoch: [61][150/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.0033e-02 (1.4779e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 ( 99.99)
Epoch: [61][160/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.3712e-02 (1.4837e-02)	Acc@1  98.44 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [61][170/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.1787e-03 (1.4666e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [61][180/391]	Time  0.040 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1360e-02 (1.4445e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [61][190/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.7656e-03 (1.4285e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [61][200/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.5210e-03 (1.4014e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [61][210/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.6185e-03 (1.3915e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [61][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.4124e-02 (1.3883e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [61][230/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.1382e-02 (1.3917e-02)	Acc@1  98.44 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [61][240/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.2310e-03 (1.3745e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [61][250/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.5891e-03 (1.3711e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [61][260/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.7756e-02 (1.3797e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [61][270/391]	Time  0.033 ( 0.034)	Data  0.002 ( 0.002)	Loss 3.5492e-02 (1.3984e-02)	Acc@1  98.44 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [61][280/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0796e-02 (1.4057e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [61][290/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.1324e-03 (1.3961e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [61][300/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.4637e-03 (1.4069e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [61][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.5370e-02 (1.4158e-02)	Acc@1  97.66 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [61][320/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.8976e-02 (1.4266e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [61][330/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.0430e-03 (1.4311e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [61][340/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3824e-02 (1.4350e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [61][350/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.3155e-03 (1.4298e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [61][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.7493e-03 (1.4317e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [61][370/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.6205e-02 (1.4324e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [61][380/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.7019e-03 (1.4371e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [61][390/391]	Time  0.027 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.8305e-02 (1.4418e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
## e[61] optimizer.zero_grad (sum) time: 0.1690828800201416
## e[61]       loss.backward (sum) time: 4.018326282501221
## e[61]      optimizer.step (sum) time: 1.2124969959259033
## epoch[61] training(only) time: 13.259427309036255
# Switched to evaluate mode...
Test: [  0/100]	Time  0.172 ( 0.172)	Loss 2.0813e-01 (2.0813e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.021 ( 0.032)	Loss 2.3010e-01 (2.6131e-01)	Acc@1  91.00 ( 92.73)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.017 ( 0.025)	Loss 3.7329e-01 (2.8243e-01)	Acc@1  91.00 ( 92.81)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.021 ( 0.023)	Loss 1.8323e-01 (2.9316e-01)	Acc@1  95.00 ( 92.58)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.014 ( 0.021)	Loss 2.2583e-01 (2.9143e-01)	Acc@1  92.00 ( 92.49)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.020 ( 0.020)	Loss 1.6296e-01 (2.9120e-01)	Acc@1  95.00 ( 92.39)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.015 ( 0.020)	Loss 2.7124e-01 (2.8381e-01)	Acc@1  92.00 ( 92.51)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.015 ( 0.019)	Loss 4.4141e-01 (2.7752e-01)	Acc@1  89.00 ( 92.61)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.014 ( 0.019)	Loss 1.5759e-01 (2.7675e-01)	Acc@1  95.00 ( 92.70)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.021 ( 0.019)	Loss 2.0422e-01 (2.7299e-01)	Acc@1  94.00 ( 92.65)	Acc@5 100.00 ( 99.78)
 * Acc@1 92.760 Acc@5 99.780
### epoch[61] execution time: 15.244592189788818
EPOCH 62
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [62][  0/391]	Time  0.212 ( 0.212)	Data  0.178 ( 0.178)	Loss 1.6190e-02 (1.6190e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [62][ 10/391]	Time  0.032 ( 0.049)	Data  0.001 ( 0.018)	Loss 1.9608e-02 (1.2267e-02)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [62][ 20/391]	Time  0.032 ( 0.042)	Data  0.001 ( 0.010)	Loss 1.9699e-02 (1.2572e-02)	Acc@1  98.44 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [62][ 30/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.007)	Loss 1.1627e-02 (1.2437e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [62][ 40/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.006)	Loss 7.7705e-03 (1.2576e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [62][ 50/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.005)	Loss 2.4033e-03 (1.1607e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [62][ 60/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 9.6359e-03 (1.2060e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [62][ 70/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.9730e-02 (1.2898e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [62][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 7.5951e-03 (1.4071e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [62][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.4277e-02 (1.3902e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [62][100/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.003)	Loss 6.8359e-03 (1.3656e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [62][110/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.1421e-02 (1.3637e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [62][120/391]	Time  0.027 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.0863e-02 (1.4260e-02)	Acc@1  97.66 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [62][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.7441e-02 (1.4510e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [62][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.5472e-02 (1.4420e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [62][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 7.4921e-03 (1.4077e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [62][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.7112e-03 (1.3761e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [62][170/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.3002e-03 (1.3677e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [62][180/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.1851e-02 (1.3664e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [62][190/391]	Time  0.037 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.5978e-03 (1.3858e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [62][200/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2100e-02 (1.3932e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [62][210/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5671e-02 (1.4062e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [62][220/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3168e-02 (1.4032e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [62][230/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.3286e-03 (1.4052e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [62][240/391]	Time  0.032 ( 0.034)	Data  0.000 ( 0.002)	Loss 7.5455e-03 (1.4102e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [62][250/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.8882e-03 (1.4041e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [62][260/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2642e-02 (1.3976e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [62][270/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3634e-02 (1.3960e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [62][280/391]	Time  0.042 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.1226e-03 (1.3918e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [62][290/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.6147e-03 (1.3904e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [62][300/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1040e-02 (1.3945e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [62][310/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.7122e-03 (1.3850e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [62][320/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.4986e-03 (1.3683e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [62][330/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5030e-02 (1.3757e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [62][340/391]	Time  0.033 ( 0.034)	Data  0.004 ( 0.002)	Loss 8.7585e-03 (1.3731e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [62][350/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9104e-02 (1.3797e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [62][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.1526e-03 (1.3841e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [62][370/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.7359e-02 (1.3939e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [62][380/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.9776e-03 (1.4138e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [62][390/391]	Time  0.024 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.2633e-02 (1.4019e-02)	Acc@1  98.75 ( 99.66)	Acc@5 100.00 (100.00)
## e[62] optimizer.zero_grad (sum) time: 0.1685338020324707
## e[62]       loss.backward (sum) time: 3.9789562225341797
## e[62]      optimizer.step (sum) time: 1.228224515914917
## epoch[62] training(only) time: 13.221870422363281
# Switched to evaluate mode...
Test: [  0/100]	Time  0.173 ( 0.173)	Loss 2.2400e-01 (2.2400e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.019 ( 0.032)	Loss 2.1765e-01 (2.5708e-01)	Acc@1  92.00 ( 92.27)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.016 ( 0.025)	Loss 3.6816e-01 (2.7968e-01)	Acc@1  91.00 ( 92.62)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.017 ( 0.023)	Loss 1.8835e-01 (2.8807e-01)	Acc@1  95.00 ( 92.45)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.015 ( 0.021)	Loss 2.2852e-01 (2.8851e-01)	Acc@1  92.00 ( 92.46)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.4160e-01 (2.8754e-01)	Acc@1  95.00 ( 92.41)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.016 ( 0.020)	Loss 2.7051e-01 (2.7959e-01)	Acc@1  92.00 ( 92.54)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.016 ( 0.019)	Loss 4.3945e-01 (2.7300e-01)	Acc@1  90.00 ( 92.70)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.016 ( 0.019)	Loss 1.5918e-01 (2.7362e-01)	Acc@1  95.00 ( 92.72)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.016 ( 0.019)	Loss 2.0178e-01 (2.7015e-01)	Acc@1  94.00 ( 92.67)	Acc@5 100.00 ( 99.79)
 * Acc@1 92.750 Acc@5 99.790
### epoch[62] execution time: 15.219724416732788
EPOCH 63
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [63][  0/391]	Time  0.221 ( 0.221)	Data  0.182 ( 0.182)	Loss 1.6632e-02 (1.6632e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [63][ 10/391]	Time  0.032 ( 0.051)	Data  0.001 ( 0.018)	Loss 1.0971e-02 (1.4101e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [63][ 20/391]	Time  0.033 ( 0.043)	Data  0.001 ( 0.010)	Loss 8.8501e-03 (1.3140e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [63][ 30/391]	Time  0.033 ( 0.039)	Data  0.001 ( 0.007)	Loss 8.3313e-03 (1.4088e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [63][ 40/391]	Time  0.032 ( 0.038)	Data  0.001 ( 0.006)	Loss 2.3972e-02 (1.6061e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [63][ 50/391]	Time  0.031 ( 0.037)	Data  0.001 ( 0.005)	Loss 4.0054e-03 (1.4978e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [63][ 60/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 2.2278e-02 (1.4629e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [63][ 70/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 1.2169e-02 (1.4364e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [63][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 3.8815e-03 (1.3724e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [63][ 90/391]	Time  0.027 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.6937e-02 (1.3388e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [63][100/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 5.5504e-03 (1.3263e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [63][110/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 7.0038e-03 (1.3396e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [63][120/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.2489e-02 (1.3968e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [63][130/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.003)	Loss 8.7662e-03 (1.3561e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [63][140/391]	Time  0.045 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.4241e-02 (1.4195e-02)	Acc@1  98.44 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [63][150/391]	Time  0.038 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.4992e-02 (1.4021e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [63][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.9638e-02 (1.4024e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [63][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.0029e-02 (1.4014e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [63][180/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.4414e-02 (1.3950e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [63][190/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2558e-02 (1.3863e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [63][200/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.4830e-02 (1.4206e-02)	Acc@1  98.44 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [63][210/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.4528e-03 (1.4178e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [63][220/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.6098e-02 (1.4084e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [63][230/391]	Time  0.039 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5366e-02 (1.4046e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [63][240/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.2686e-03 (1.3994e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [63][250/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.6387e-02 (1.4079e-02)	Acc@1  97.66 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [63][260/391]	Time  0.027 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.4681e-03 (1.3994e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [63][270/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.8814e-02 (1.3949e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [63][280/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2215e-02 (1.3998e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [63][290/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.2474e-03 (1.3940e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [63][300/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.1068e-03 (1.3875e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [63][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.3773e-02 (1.4210e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [63][320/391]	Time  0.028 ( 0.034)	Data  0.000 ( 0.002)	Loss 6.2065e-03 (1.4231e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [63][330/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9150e-02 (1.4252e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [63][340/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.0493e-02 (1.4298e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [63][350/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.8290e-02 (1.4356e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [63][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0902e-02 (1.4354e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [63][370/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.3313e-03 (1.4345e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [63][380/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.0921e-03 (1.4296e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [63][390/391]	Time  0.023 ( 0.033)	Data  0.001 ( 0.002)	Loss 7.4348e-03 (1.4206e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
## e[63] optimizer.zero_grad (sum) time: 0.16811323165893555
## e[63]       loss.backward (sum) time: 3.885368824005127
## e[63]      optimizer.step (sum) time: 1.2589244842529297
## epoch[63] training(only) time: 13.231615781784058
# Switched to evaluate mode...
Test: [  0/100]	Time  0.171 ( 0.171)	Loss 1.9836e-01 (1.9836e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.014 ( 0.030)	Loss 2.1240e-01 (2.5279e-01)	Acc@1  92.00 ( 92.45)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.014 ( 0.024)	Loss 3.6279e-01 (2.7604e-01)	Acc@1  91.00 ( 92.57)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.025 ( 0.022)	Loss 1.7090e-01 (2.8599e-01)	Acc@1  95.00 ( 92.39)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.018 ( 0.021)	Loss 2.2095e-01 (2.8548e-01)	Acc@1  93.00 ( 92.41)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.025 ( 0.020)	Loss 1.5125e-01 (2.8453e-01)	Acc@1  94.00 ( 92.35)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.016 ( 0.020)	Loss 2.5586e-01 (2.7754e-01)	Acc@1  92.00 ( 92.52)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.016 ( 0.019)	Loss 4.6167e-01 (2.7149e-01)	Acc@1  89.00 ( 92.66)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.016 ( 0.019)	Loss 1.5833e-01 (2.7170e-01)	Acc@1  95.00 ( 92.69)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.013 ( 0.018)	Loss 2.0239e-01 (2.6827e-01)	Acc@1  94.00 ( 92.65)	Acc@5 100.00 ( 99.78)
 * Acc@1 92.740 Acc@5 99.780
### epoch[63] execution time: 15.167817831039429
EPOCH 64
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [64][  0/391]	Time  0.215 ( 0.215)	Data  0.178 ( 0.178)	Loss 3.9368e-02 (3.9368e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [64][ 10/391]	Time  0.032 ( 0.050)	Data  0.001 ( 0.018)	Loss 2.6665e-03 (1.1260e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [64][ 20/391]	Time  0.032 ( 0.042)	Data  0.001 ( 0.010)	Loss 1.7258e-02 (1.1586e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [64][ 30/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.007)	Loss 2.0248e-02 (1.3365e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [64][ 40/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.006)	Loss 1.1436e-02 (1.2953e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [64][ 50/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 7.8812e-03 (1.2617e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [64][ 60/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 2.1027e-02 (1.2621e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [64][ 70/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.004)	Loss 1.2405e-02 (1.3078e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [64][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 6.2065e-03 (1.3242e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [64][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.7954e-02 (1.3224e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [64][100/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.2354e-02 (1.3613e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [64][110/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.2405e-02 (1.3364e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [64][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.4740e-02 (1.3461e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [64][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.4389e-02 (1.3569e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [64][140/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.0670e-03 (1.3368e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [64][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 7.2441e-03 (1.3410e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [64][160/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.1760e-03 (1.3559e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [64][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0292e-02 (1.3477e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [64][180/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5236e-02 (1.3804e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [64][190/391]	Time  0.038 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.3520e-03 (1.3708e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [64][200/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.5068e-03 (1.3843e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [64][210/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.3254e-02 (1.3857e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [64][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2405e-02 (1.4067e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [64][230/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2993e-02 (1.3927e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [64][240/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.3536e-03 (1.3957e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [64][250/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0033e-02 (1.4015e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [64][260/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.3661e-02 (1.4152e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [64][270/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.7994e-03 (1.4098e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [64][280/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.4577e-03 (1.3988e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [64][290/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.6276e-02 (1.4027e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [64][300/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2405e-02 (1.4026e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [64][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.7978e-03 (1.4030e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [64][320/391]	Time  0.034 ( 0.034)	Data  0.002 ( 0.002)	Loss 1.6495e-02 (1.4123e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [64][330/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.6288e-03 (1.4089e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [64][340/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.5493e-03 (1.4163e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [64][350/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5671e-02 (1.4245e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [64][360/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2848e-02 (1.4212e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [64][370/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.9340e-03 (1.4288e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [64][380/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2825e-02 (1.4317e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [64][390/391]	Time  0.028 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.9248e-03 (1.4220e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
## e[64] optimizer.zero_grad (sum) time: 0.16924023628234863
## e[64]       loss.backward (sum) time: 4.076679706573486
## e[64]      optimizer.step (sum) time: 1.206113338470459
## epoch[64] training(only) time: 13.234659433364868
# Switched to evaluate mode...
Test: [  0/100]	Time  0.176 ( 0.176)	Loss 2.2473e-01 (2.2473e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.018 ( 0.031)	Loss 1.7065e-01 (2.5282e-01)	Acc@1  93.00 ( 92.73)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.013 ( 0.024)	Loss 3.5034e-01 (2.7707e-01)	Acc@1  92.00 ( 92.81)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.018 ( 0.022)	Loss 1.8518e-01 (2.8547e-01)	Acc@1  94.00 ( 92.52)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.014 ( 0.021)	Loss 2.3950e-01 (2.8658e-01)	Acc@1  91.00 ( 92.49)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.3074e-01 (2.8503e-01)	Acc@1  96.00 ( 92.45)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.018 ( 0.019)	Loss 2.4207e-01 (2.7653e-01)	Acc@1  94.00 ( 92.69)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.015 ( 0.019)	Loss 4.5459e-01 (2.7029e-01)	Acc@1  89.00 ( 92.82)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.019 ( 0.019)	Loss 1.6052e-01 (2.7174e-01)	Acc@1  95.00 ( 92.81)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.015 ( 0.018)	Loss 2.0105e-01 (2.6865e-01)	Acc@1  93.00 ( 92.74)	Acc@5 100.00 ( 99.80)
 * Acc@1 92.760 Acc@5 99.800
### epoch[64] execution time: 15.20757532119751
EPOCH 65
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [65][  0/391]	Time  0.213 ( 0.213)	Data  0.180 ( 0.180)	Loss 2.7809e-03 (2.7809e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [65][ 10/391]	Time  0.032 ( 0.050)	Data  0.001 ( 0.018)	Loss 8.0643e-03 (1.0390e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [65][ 20/391]	Time  0.032 ( 0.042)	Data  0.001 ( 0.010)	Loss 1.1932e-02 (1.2865e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [65][ 30/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.007)	Loss 5.1384e-03 (1.2010e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [65][ 40/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.006)	Loss 1.1368e-02 (1.2282e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [65][ 50/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.005)	Loss 1.1078e-02 (1.2577e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [65][ 60/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 9.5596e-03 (1.2923e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [65][ 70/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.004)	Loss 1.5381e-02 (1.3143e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [65][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.1612e-02 (1.3263e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [65][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.9150e-02 (1.3319e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [65][100/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.4572e-02 (1.3549e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [65][110/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.003)	Loss 9.1171e-03 (1.3305e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [65][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 8.5678e-03 (1.3173e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [65][130/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.3453e-02 (1.3294e-02)	Acc@1  98.44 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [65][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.3260e-02 (1.3388e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [65][150/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 7.8201e-03 (1.3607e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [65][160/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0193e-02 (1.3651e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [65][170/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.6907e-02 (1.3487e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [65][180/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.6647e-02 (1.3410e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [65][190/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.3705e-03 (1.3582e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [65][200/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.6541e-02 (1.3760e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [65][210/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.6970e-03 (1.3728e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [65][220/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.2904e-03 (1.3862e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [65][230/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.0798e-02 (1.3852e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [65][240/391]	Time  0.037 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.4850e-03 (1.4189e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [65][250/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.8173e-02 (1.4084e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [65][260/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2794e-02 (1.3966e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [65][270/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.2990e-02 (1.3975e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [65][280/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1665e-02 (1.3957e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [65][290/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.6757e-03 (1.3736e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [65][300/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.8539e-02 (1.3772e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [65][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1246e-02 (1.3818e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [65][320/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2077e-02 (1.3758e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [65][330/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5297e-02 (1.3739e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [65][340/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.1106e-03 (1.3803e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [65][350/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.5526e-03 (1.3766e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [65][360/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.0197e-02 (1.3748e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [65][370/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4076e-02 (1.3707e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [65][380/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.7258e-02 (1.3837e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [65][390/391]	Time  0.027 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.3234e-02 (1.3921e-02)	Acc@1  98.75 ( 99.69)	Acc@5 100.00 (100.00)
## e[65] optimizer.zero_grad (sum) time: 0.17094683647155762
## e[65]       loss.backward (sum) time: 4.103614807128906
## e[65]      optimizer.step (sum) time: 1.1935598850250244
## epoch[65] training(only) time: 13.26848554611206
# Switched to evaluate mode...
Test: [  0/100]	Time  0.174 ( 0.174)	Loss 2.1448e-01 (2.1448e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.017 ( 0.031)	Loss 1.7542e-01 (2.5261e-01)	Acc@1  92.00 ( 92.73)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.019 ( 0.025)	Loss 3.3496e-01 (2.7668e-01)	Acc@1  92.00 ( 92.71)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.015 ( 0.022)	Loss 1.7639e-01 (2.8653e-01)	Acc@1  94.00 ( 92.35)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.017 ( 0.021)	Loss 1.9629e-01 (2.8633e-01)	Acc@1  91.00 ( 92.32)	Acc@5 100.00 ( 99.66)
Test: [ 50/100]	Time  0.018 ( 0.020)	Loss 1.3330e-01 (2.8513e-01)	Acc@1  95.00 ( 92.33)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.017 ( 0.020)	Loss 2.5366e-01 (2.7695e-01)	Acc@1  92.00 ( 92.52)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.014 ( 0.019)	Loss 4.4702e-01 (2.7069e-01)	Acc@1  90.00 ( 92.69)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.018 ( 0.019)	Loss 1.7358e-01 (2.7190e-01)	Acc@1  94.00 ( 92.75)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 2.1729e-01 (2.6871e-01)	Acc@1  91.00 ( 92.67)	Acc@5 100.00 ( 99.77)
 * Acc@1 92.750 Acc@5 99.770
### epoch[65] execution time: 15.287739276885986
EPOCH 66
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [66][  0/391]	Time  0.209 ( 0.209)	Data  0.174 ( 0.174)	Loss 7.6981e-03 (7.6981e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [66][ 10/391]	Time  0.040 ( 0.050)	Data  0.001 ( 0.017)	Loss 5.1788e-02 (1.1559e-02)	Acc@1  98.44 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [66][ 20/391]	Time  0.032 ( 0.042)	Data  0.001 ( 0.010)	Loss 1.4122e-02 (1.0760e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [66][ 30/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.007)	Loss 1.2756e-02 (1.1538e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [66][ 40/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.006)	Loss 1.4793e-02 (1.1638e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [66][ 50/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.005)	Loss 4.8332e-03 (1.2303e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [66][ 60/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.004)	Loss 7.7209e-03 (1.3512e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [66][ 70/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.004)	Loss 1.7349e-02 (1.2976e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [66][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.7685e-02 (1.3128e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [66][ 90/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.1375e-02 (1.3165e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [66][100/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.3191e-02 (1.2874e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [66][110/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.8738e-02 (1.3038e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [66][120/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.5884e-02 (1.3123e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [66][130/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 7.9193e-03 (1.3009e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [66][140/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.3212e-03 (1.2736e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [66][150/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.0361e-02 (1.2622e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [66][160/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 7.7209e-03 (1.2496e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [66][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.4332e-02 (1.2572e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [66][180/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.8136e-03 (1.2525e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [66][190/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.1698e-02 (1.2757e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [66][200/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4183e-02 (1.2746e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [66][210/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.7039e-02 (1.2914e-02)	Acc@1  98.44 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [66][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5732e-02 (1.2766e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [66][230/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.6632e-02 (1.2751e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [66][240/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5015e-02 (1.2859e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [66][250/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.9809e-03 (1.3072e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [66][260/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4244e-02 (1.2995e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [66][270/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.7482e-03 (1.3059e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [66][280/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.4937e-03 (1.2889e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [66][290/391]	Time  0.027 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.1561e-02 (1.2976e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [66][300/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.9053e-02 (1.3083e-02)	Acc@1  98.44 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [66][310/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9226e-02 (1.3223e-02)	Acc@1  98.44 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [66][320/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9745e-02 (1.3316e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [66][330/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.0792e-02 (1.3396e-02)	Acc@1  98.44 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [66][340/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3443e-02 (1.3388e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [66][350/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.3148e-02 (1.3409e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [66][360/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2947e-02 (1.3389e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [66][370/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.5008e-03 (1.3349e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [66][380/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.6224e-02 (1.3620e-02)	Acc@1  97.66 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [66][390/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.7481e-02 (1.3687e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
## e[66] optimizer.zero_grad (sum) time: 0.16852283477783203
## e[66]       loss.backward (sum) time: 3.972036600112915
## e[66]      optimizer.step (sum) time: 1.2188639640808105
## epoch[66] training(only) time: 13.381578922271729
# Switched to evaluate mode...
Test: [  0/100]	Time  0.172 ( 0.172)	Loss 1.9885e-01 (1.9885e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.016 ( 0.031)	Loss 1.8237e-01 (2.4780e-01)	Acc@1  92.00 ( 92.73)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.014 ( 0.024)	Loss 3.5254e-01 (2.7583e-01)	Acc@1  92.00 ( 92.76)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.015 ( 0.022)	Loss 1.8347e-01 (2.8690e-01)	Acc@1  94.00 ( 92.55)	Acc@5  99.00 ( 99.65)
Test: [ 40/100]	Time  0.018 ( 0.021)	Loss 2.1863e-01 (2.8765e-01)	Acc@1  91.00 ( 92.46)	Acc@5 100.00 ( 99.66)
Test: [ 50/100]	Time  0.018 ( 0.020)	Loss 1.3208e-01 (2.8646e-01)	Acc@1  94.00 ( 92.43)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.016 ( 0.019)	Loss 2.4084e-01 (2.7784e-01)	Acc@1  94.00 ( 92.67)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.014 ( 0.019)	Loss 4.3750e-01 (2.7117e-01)	Acc@1  90.00 ( 92.82)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.015 ( 0.019)	Loss 1.6321e-01 (2.7227e-01)	Acc@1  94.00 ( 92.80)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.019 ( 0.019)	Loss 1.9092e-01 (2.6942e-01)	Acc@1  93.00 ( 92.73)	Acc@5 100.00 ( 99.76)
 * Acc@1 92.760 Acc@5 99.760
### epoch[66] execution time: 15.346947193145752
EPOCH 67
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [67][  0/391]	Time  0.193 ( 0.193)	Data  0.153 ( 0.153)	Loss 4.3297e-03 (4.3297e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [67][ 10/391]	Time  0.032 ( 0.049)	Data  0.001 ( 0.016)	Loss 1.3100e-02 (1.2960e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [67][ 20/391]	Time  0.032 ( 0.041)	Data  0.001 ( 0.009)	Loss 1.4076e-02 (1.2269e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [67][ 30/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.006)	Loss 9.2926e-03 (1.1981e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [67][ 40/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 9.3460e-03 (1.2252e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [67][ 50/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.004)	Loss 1.9592e-02 (1.2209e-02)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [67][ 60/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.004)	Loss 9.2392e-03 (1.2615e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [67][ 70/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.004)	Loss 1.6830e-02 (1.3575e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [67][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 8.2321e-03 (1.3213e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [67][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 5.6877e-03 (1.3047e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [67][100/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.0681e-02 (1.3132e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [67][110/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 7.1297e-03 (1.3179e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [67][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.3544e-02 (1.3301e-02)	Acc@1  98.44 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [67][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.0391e-02 (1.3414e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [67][140/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.9501e-02 (1.3277e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [67][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.6517e-03 (1.3247e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [67][160/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9714e-02 (1.3238e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [67][170/391]	Time  0.035 ( 0.034)	Data  0.002 ( 0.002)	Loss 9.7351e-03 (1.3267e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [67][180/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5869e-02 (1.3616e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [67][190/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.0645e-02 (1.3964e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [67][200/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.5367e-03 (1.3785e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [67][210/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1581e-02 (1.3598e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [67][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.6616e-03 (1.3740e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [67][230/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.4256e-03 (1.3744e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [67][240/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.4887e-02 (1.3772e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [67][250/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.6068e-03 (1.3676e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [67][260/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2665e-02 (1.3766e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [67][270/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.1193e-03 (1.3811e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [67][280/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.7787e-03 (1.3818e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [67][290/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.3406e-03 (1.3707e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [67][300/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.6032e-03 (1.3645e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [67][310/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.5727e-03 (1.3570e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [67][320/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.7913e-03 (1.3452e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [67][330/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.5395e-03 (1.3569e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [67][340/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5625e-02 (1.3644e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [67][350/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0925e-02 (1.3572e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [67][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.9351e-03 (1.3531e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [67][370/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.3743e-02 (1.3532e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [67][380/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5564e-02 (1.3466e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [67][390/391]	Time  0.021 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.2476e-02 (1.3511e-02)	Acc@1  98.75 ( 99.68)	Acc@5 100.00 (100.00)
## e[67] optimizer.zero_grad (sum) time: 0.17040491104125977
## e[67]       loss.backward (sum) time: 4.1122589111328125
## e[67]      optimizer.step (sum) time: 1.1957371234893799
## epoch[67] training(only) time: 13.235911846160889
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 2.0300e-01 (2.0300e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.014 ( 0.031)	Loss 1.9824e-01 (2.5084e-01)	Acc@1  92.00 ( 92.64)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.018 ( 0.024)	Loss 3.7329e-01 (2.7645e-01)	Acc@1  92.00 ( 92.57)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.015 ( 0.021)	Loss 1.7444e-01 (2.8536e-01)	Acc@1  95.00 ( 92.42)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.013 ( 0.020)	Loss 2.4072e-01 (2.8690e-01)	Acc@1  92.00 ( 92.39)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.014 ( 0.019)	Loss 1.4429e-01 (2.8599e-01)	Acc@1  94.00 ( 92.27)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.020 ( 0.019)	Loss 2.4902e-01 (2.7784e-01)	Acc@1  93.00 ( 92.44)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.020 ( 0.019)	Loss 4.4678e-01 (2.7242e-01)	Acc@1  89.00 ( 92.55)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.020 ( 0.018)	Loss 1.5442e-01 (2.7307e-01)	Acc@1  95.00 ( 92.62)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.014 ( 0.018)	Loss 1.9177e-01 (2.7046e-01)	Acc@1  94.00 ( 92.59)	Acc@5 100.00 ( 99.79)
 * Acc@1 92.660 Acc@5 99.800
### epoch[67] execution time: 15.17586898803711
EPOCH 68
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [68][  0/391]	Time  0.212 ( 0.212)	Data  0.179 ( 0.179)	Loss 1.3695e-02 (1.3695e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [68][ 10/391]	Time  0.032 ( 0.050)	Data  0.001 ( 0.018)	Loss 1.0704e-02 (1.2986e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [68][ 20/391]	Time  0.035 ( 0.043)	Data  0.001 ( 0.010)	Loss 3.5629e-03 (1.1945e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [68][ 30/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.007)	Loss 9.1858e-03 (1.1498e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [68][ 40/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.006)	Loss 1.6617e-02 (1.1353e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [68][ 50/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 1.2886e-02 (1.2412e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [68][ 60/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.004)	Loss 3.4046e-03 (1.2749e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [68][ 70/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.004)	Loss 7.3662e-03 (1.3181e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [68][ 80/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.004)	Loss 4.0359e-03 (1.2783e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [68][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.2962e-02 (1.3187e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [68][100/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 9.5215e-03 (1.2934e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [68][110/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.9221e-02 (1.3546e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [68][120/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 4.7638e-02 (1.3816e-02)	Acc@1  97.66 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [68][130/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.003)	Loss 7.7515e-03 (1.3565e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [68][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.5041e-03 (1.3593e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [68][150/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.0475e-02 (1.3497e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [68][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.5160e-02 (1.3557e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [68][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.6305e-03 (1.3382e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [68][180/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.5373e-03 (1.3283e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [68][190/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.9124e-02 (1.3449e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [68][200/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.7150e-03 (1.3429e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [68][210/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.1545e-02 (1.3693e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [68][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3069e-02 (1.3955e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [68][230/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.7197e-02 (1.3809e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [68][240/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0742e-02 (1.3684e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [68][250/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.9542e-03 (1.3623e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [68][260/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1658e-02 (1.3667e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [68][270/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4832e-02 (1.3731e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [68][280/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9592e-02 (1.3723e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [68][290/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4931e-02 (1.3652e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [68][300/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2947e-02 (1.3483e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [68][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5182e-02 (1.3404e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [68][320/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.4948e-02 (1.3485e-02)	Acc@1  97.66 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [68][330/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.2027e-03 (1.3415e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [68][340/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.6643e-03 (1.3494e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [68][350/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3763e-02 (1.3422e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [68][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5335e-02 (1.3354e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [68][370/391]	Time  0.028 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.2801e-03 (1.3392e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [68][380/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.1727e-03 (1.3316e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [68][390/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.7106e-03 (1.3261e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
## e[68] optimizer.zero_grad (sum) time: 0.16932225227355957
## e[68]       loss.backward (sum) time: 4.023339509963989
## e[68]      optimizer.step (sum) time: 1.2043285369873047
## epoch[68] training(only) time: 13.300679922103882
# Switched to evaluate mode...
Test: [  0/100]	Time  0.178 ( 0.178)	Loss 2.1643e-01 (2.1643e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.015 ( 0.032)	Loss 2.2424e-01 (2.5223e-01)	Acc@1  92.00 ( 92.55)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.015 ( 0.024)	Loss 3.5840e-01 (2.7428e-01)	Acc@1  92.00 ( 92.76)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.014 ( 0.022)	Loss 1.8250e-01 (2.8361e-01)	Acc@1  95.00 ( 92.55)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.014 ( 0.021)	Loss 2.4438e-01 (2.8346e-01)	Acc@1  92.00 ( 92.49)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.023 ( 0.020)	Loss 1.4026e-01 (2.8249e-01)	Acc@1  96.00 ( 92.43)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.016 ( 0.020)	Loss 2.5830e-01 (2.7518e-01)	Acc@1  92.00 ( 92.51)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.018 ( 0.019)	Loss 4.4971e-01 (2.6957e-01)	Acc@1  89.00 ( 92.61)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.018 ( 0.019)	Loss 1.6711e-01 (2.7068e-01)	Acc@1  96.00 ( 92.72)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.018 ( 0.019)	Loss 1.9299e-01 (2.6760e-01)	Acc@1  94.00 ( 92.66)	Acc@5 100.00 ( 99.78)
 * Acc@1 92.750 Acc@5 99.790
### epoch[68] execution time: 15.264024019241333
EPOCH 69
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [69][  0/391]	Time  0.214 ( 0.214)	Data  0.181 ( 0.181)	Loss 1.5778e-02 (1.5778e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [69][ 10/391]	Time  0.032 ( 0.050)	Data  0.001 ( 0.018)	Loss 2.0248e-02 (1.2325e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [69][ 20/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.010)	Loss 4.0741e-03 (1.1355e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [69][ 30/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.007)	Loss 2.1622e-02 (1.0855e-02)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [69][ 40/391]	Time  0.032 ( 0.038)	Data  0.001 ( 0.006)	Loss 5.0659e-03 (1.3010e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [69][ 50/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.005)	Loss 1.1360e-02 (1.2782e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [69][ 60/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 2.0844e-02 (1.2832e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [69][ 70/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 2.3453e-02 (1.3020e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [69][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.2871e-02 (1.4185e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [69][ 90/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 7.2937e-03 (1.4033e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [69][100/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 7.3509e-03 (1.3921e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [69][110/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 6.1798e-03 (1.3663e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [69][120/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.0353e-02 (1.3450e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [69][130/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.3588e-02 (1.3354e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [69][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.3705e-03 (1.3121e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [69][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 7.4158e-02 (1.3372e-02)	Acc@1  98.44 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [69][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 9.3307e-03 (1.3240e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [69][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1543e-02 (1.2974e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [69][180/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.4915e-03 (1.2897e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [69][190/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.6430e-03 (1.2888e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [69][200/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3786e-02 (1.2859e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [69][210/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0002e-02 (1.2764e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [69][220/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.2903e-02 (1.2933e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [69][230/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0651e-02 (1.2944e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [69][240/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9012e-02 (1.2860e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [69][250/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2924e-02 (1.2805e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [69][260/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.8806e-03 (1.2648e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [69][270/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.6586e-02 (1.2548e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [69][280/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3191e-02 (1.2530e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [69][290/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4038e-02 (1.2630e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [69][300/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.5954e-03 (1.2649e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [69][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3191e-02 (1.2763e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [69][320/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.5754e-03 (1.2971e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [69][330/391]	Time  0.027 ( 0.034)	Data  0.000 ( 0.002)	Loss 7.9651e-03 (1.2875e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [69][340/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.0703e-03 (1.2815e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [69][350/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.0539e-03 (1.2940e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [69][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.3987e-02 (1.2896e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [69][370/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.6113e-02 (1.2935e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [69][380/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.7349e-02 (1.2950e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [69][390/391]	Time  0.029 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.0986e-02 (1.2973e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
## e[69] optimizer.zero_grad (sum) time: 0.16925311088562012
## e[69]       loss.backward (sum) time: 4.053685665130615
## e[69]      optimizer.step (sum) time: 1.2115538120269775
## epoch[69] training(only) time: 13.228835582733154
# Switched to evaluate mode...
Test: [  0/100]	Time  0.176 ( 0.176)	Loss 2.1216e-01 (2.1216e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.018 ( 0.031)	Loss 1.8237e-01 (2.5312e-01)	Acc@1  92.00 ( 92.73)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.021 ( 0.025)	Loss 3.5059e-01 (2.7852e-01)	Acc@1  92.00 ( 92.62)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.020 ( 0.022)	Loss 1.8005e-01 (2.8726e-01)	Acc@1  95.00 ( 92.42)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.017 ( 0.021)	Loss 2.2498e-01 (2.8781e-01)	Acc@1  91.00 ( 92.37)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.025 ( 0.020)	Loss 1.3635e-01 (2.8652e-01)	Acc@1  96.00 ( 92.33)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.016 ( 0.020)	Loss 2.4719e-01 (2.7839e-01)	Acc@1  93.00 ( 92.57)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.014 ( 0.019)	Loss 4.4897e-01 (2.7196e-01)	Acc@1  89.00 ( 92.69)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.014 ( 0.019)	Loss 1.6968e-01 (2.7360e-01)	Acc@1  94.00 ( 92.73)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.018 ( 0.019)	Loss 2.0154e-01 (2.7048e-01)	Acc@1  93.00 ( 92.69)	Acc@5 100.00 ( 99.79)
 * Acc@1 92.730 Acc@5 99.790
### epoch[69] execution time: 15.21262264251709
EPOCH 70
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [70][  0/391]	Time  0.212 ( 0.212)	Data  0.178 ( 0.178)	Loss 2.5848e-02 (2.5848e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [70][ 10/391]	Time  0.034 ( 0.051)	Data  0.001 ( 0.018)	Loss 2.2659e-02 (1.5515e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [70][ 20/391]	Time  0.033 ( 0.042)	Data  0.001 ( 0.010)	Loss 2.5970e-02 (1.5689e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [70][ 30/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.007)	Loss 1.0193e-02 (1.3233e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [70][ 40/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.006)	Loss 2.8687e-02 (1.4367e-02)	Acc@1  98.44 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [70][ 50/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 4.6692e-03 (1.4106e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [70][ 60/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 5.0850e-03 (1.3807e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [70][ 70/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 1.9073e-02 (1.3833e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [70][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.6678e-02 (1.4404e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [70][ 90/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.3390e-02 (1.4416e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [70][100/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.003)	Loss 4.7607e-02 (1.5285e-02)	Acc@1  98.44 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [70][110/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.003)	Loss 8.1635e-03 (1.4984e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [70][120/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.4429e-02 (1.4748e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [70][130/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 5.4016e-03 (1.4530e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [70][140/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 4.6936e-02 (1.4968e-02)	Acc@1  98.44 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [70][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.5991e-02 (1.4806e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [70][160/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.1226e-03 (1.5514e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [70][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.3213e-02 (1.5628e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [70][180/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.4454e-02 (1.5640e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [70][190/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0582e-02 (1.5449e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [70][200/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4656e-02 (1.5262e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [70][210/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1559e-02 (1.5164e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [70][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.3062e-03 (1.5123e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [70][230/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.1705e-03 (1.4881e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [70][240/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.2381e-03 (1.4833e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [70][250/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1871e-02 (1.4896e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [70][260/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.8517e-03 (1.4706e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [70][270/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.4810e-02 (1.4835e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [70][280/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.3056e-02 (1.4805e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [70][290/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0414e-02 (1.4613e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [70][300/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1887e-02 (1.4518e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [70][310/391]	Time  0.028 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.2664e-02 (1.4521e-02)	Acc@1  98.44 ( 99.69)	Acc@5  99.22 (100.00)
Epoch: [70][320/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.7395e-02 (1.4500e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [70][330/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.8753e-02 (1.4553e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [70][340/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.9498e-03 (1.4533e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [70][350/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.5596e-03 (1.4386e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [70][360/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.9171e-03 (1.4451e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [70][370/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2871e-02 (1.4297e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [70][380/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.1286e-02 (1.4482e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [70][390/391]	Time  0.025 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.6987e-02 (1.4508e-02)	Acc@1  98.75 ( 99.68)	Acc@5 100.00 (100.00)
## e[70] optimizer.zero_grad (sum) time: 0.1700727939605713
## e[70]       loss.backward (sum) time: 4.052617073059082
## e[70]      optimizer.step (sum) time: 1.1975715160369873
## epoch[70] training(only) time: 13.324437141418457
# Switched to evaluate mode...
Test: [  0/100]	Time  0.171 ( 0.171)	Loss 2.3462e-01 (2.3462e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.022 ( 0.031)	Loss 1.9006e-01 (2.5819e-01)	Acc@1  93.00 ( 92.55)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.024 ( 0.024)	Loss 3.5400e-01 (2.7864e-01)	Acc@1  91.00 ( 92.67)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.014 ( 0.022)	Loss 1.8018e-01 (2.8621e-01)	Acc@1  95.00 ( 92.48)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.018 ( 0.021)	Loss 2.3169e-01 (2.8804e-01)	Acc@1  91.00 ( 92.37)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.015 ( 0.020)	Loss 1.1633e-01 (2.8609e-01)	Acc@1  97.00 ( 92.37)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.021 ( 0.019)	Loss 2.4219e-01 (2.7668e-01)	Acc@1  92.00 ( 92.64)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 4.4702e-01 (2.7072e-01)	Acc@1  91.00 ( 92.76)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.014 ( 0.019)	Loss 1.5491e-01 (2.7194e-01)	Acc@1  93.00 ( 92.75)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.021 ( 0.019)	Loss 2.0679e-01 (2.6855e-01)	Acc@1  93.00 ( 92.71)	Acc@5 100.00 ( 99.80)
 * Acc@1 92.750 Acc@5 99.800
### epoch[70] execution time: 15.316391229629517
EPOCH 71
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [71][  0/391]	Time  0.223 ( 0.223)	Data  0.184 ( 0.184)	Loss 1.0078e-02 (1.0078e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [71][ 10/391]	Time  0.035 ( 0.052)	Data  0.001 ( 0.018)	Loss 6.1111e-03 (1.1898e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [71][ 20/391]	Time  0.033 ( 0.043)	Data  0.001 ( 0.010)	Loss 6.5041e-03 (1.1581e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [71][ 30/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.007)	Loss 2.4628e-02 (1.2421e-02)	Acc@1  98.44 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [71][ 40/391]	Time  0.032 ( 0.038)	Data  0.001 ( 0.006)	Loss 1.4603e-02 (1.3378e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [71][ 50/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.005)	Loss 3.5210e-03 (1.2918e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [71][ 60/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 1.2489e-02 (1.3098e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [71][ 70/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.004)	Loss 4.7546e-02 (1.3463e-02)	Acc@1  97.66 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [71][ 80/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.004)	Loss 3.7098e-03 (1.3319e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [71][ 90/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.003)	Loss 9.1476e-03 (1.3023e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [71][100/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.6357e-02 (1.2799e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [71][110/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 5.3253e-03 (1.2982e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [71][120/391]	Time  0.029 ( 0.035)	Data  0.001 ( 0.003)	Loss 8.5831e-03 (1.2940e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [71][130/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.9541e-02 (1.3094e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [71][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.4507e-03 (1.2905e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [71][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.0704e-02 (1.2877e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [71][160/391]	Time  0.043 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.3565e-02 (1.2713e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [71][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.0114e-03 (1.2646e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [71][180/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.7441e-02 (1.2702e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [71][190/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1993e-02 (1.2849e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [71][200/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1185e-02 (1.2933e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [71][210/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.1469e-02 (1.2919e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [71][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.6657e-02 (1.2973e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [71][230/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.9166e-03 (1.2901e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [71][240/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.4000e-03 (1.2803e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [71][250/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4061e-02 (1.2884e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [71][260/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.1542e-03 (1.3042e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [71][270/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.2715e-02 (1.3010e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [71][280/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.2328e-02 (1.3078e-02)	Acc@1  98.44 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [71][290/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.3087e-02 (1.3016e-02)	Acc@1  98.44 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [71][300/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3824e-02 (1.3143e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [71][310/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.4893e-03 (1.3059e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [71][320/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.8066e-02 (1.3104e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [71][330/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.1164e-02 (1.3017e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [71][340/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.3558e-03 (1.2907e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [71][350/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.3373e-03 (1.2871e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [71][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0376e-02 (1.2875e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [71][370/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.3596e-03 (1.2840e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [71][380/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.3148e-02 (1.2781e-02)	Acc@1  98.44 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [71][390/391]	Time  0.027 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.3782e-03 (1.2923e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
## e[71] optimizer.zero_grad (sum) time: 0.16941356658935547
## e[71]       loss.backward (sum) time: 4.087451934814453
## e[71]      optimizer.step (sum) time: 1.2021727561950684
## epoch[71] training(only) time: 13.315702199935913
# Switched to evaluate mode...
Test: [  0/100]	Time  0.174 ( 0.174)	Loss 2.1313e-01 (2.1313e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.015 ( 0.031)	Loss 2.0813e-01 (2.5538e-01)	Acc@1  92.00 ( 92.64)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.019 ( 0.024)	Loss 3.5571e-01 (2.7551e-01)	Acc@1  92.00 ( 92.81)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.023 ( 0.022)	Loss 1.7786e-01 (2.8494e-01)	Acc@1  95.00 ( 92.61)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.014 ( 0.020)	Loss 2.4646e-01 (2.8485e-01)	Acc@1  92.00 ( 92.54)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.013 ( 0.020)	Loss 1.3831e-01 (2.8408e-01)	Acc@1  96.00 ( 92.45)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 2.6270e-01 (2.7655e-01)	Acc@1  92.00 ( 92.62)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.020 ( 0.019)	Loss 4.7339e-01 (2.7088e-01)	Acc@1  89.00 ( 92.69)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.019 ( 0.019)	Loss 1.6162e-01 (2.7209e-01)	Acc@1  94.00 ( 92.70)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.014 ( 0.019)	Loss 2.0093e-01 (2.6909e-01)	Acc@1  94.00 ( 92.68)	Acc@5 100.00 ( 99.77)
 * Acc@1 92.760 Acc@5 99.780
### epoch[71] execution time: 15.285567998886108
EPOCH 72
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [72][  0/391]	Time  0.205 ( 0.205)	Data  0.167 ( 0.167)	Loss 3.6640e-03 (3.6640e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [72][ 10/391]	Time  0.032 ( 0.049)	Data  0.001 ( 0.017)	Loss 2.1164e-02 (1.2657e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [72][ 20/391]	Time  0.032 ( 0.042)	Data  0.001 ( 0.009)	Loss 1.1574e-02 (1.2074e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [72][ 30/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.007)	Loss 7.9498e-03 (1.2149e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [72][ 40/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.006)	Loss 1.0269e-02 (1.1948e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [72][ 50/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.005)	Loss 3.0640e-02 (1.2500e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [72][ 60/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 5.6686e-03 (1.2045e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [72][ 70/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 2.0828e-02 (1.1954e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [72][ 80/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.003)	Loss 6.0463e-03 (1.1771e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [72][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 4.0283e-03 (1.1723e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [72][100/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 8.7128e-03 (1.1891e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [72][110/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.5991e-02 (1.1942e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [72][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 9.0027e-03 (1.1768e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [72][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 9.1476e-03 (1.1767e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [72][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.1360e-02 (1.2003e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [72][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.8714e-03 (1.1850e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [72][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3863e-02 (1.1808e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [72][170/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.6447e-03 (1.1696e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [72][180/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.2016e-03 (1.1671e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [72][190/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.6769e-02 (1.1732e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [72][200/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0139e-02 (1.1704e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [72][210/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.6184e-02 (1.2235e-02)	Acc@1  98.44 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [72][220/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2611e-02 (1.2318e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [72][230/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.0323e-02 (1.2630e-02)	Acc@1  98.44 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [72][240/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.5291e-03 (1.2606e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [72][250/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.1860e-02 (1.2816e-02)	Acc@1  98.44 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [72][260/391]	Time  0.037 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.0261e-03 (1.2785e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [72][270/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1452e-02 (1.2749e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [72][280/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4885e-02 (1.2764e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [72][290/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5198e-02 (1.2620e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [72][300/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2451e-02 (1.2573e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [72][310/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.8648e-03 (1.2583e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [72][320/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.0714e-03 (1.2548e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [72][330/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.9174e-02 (1.2667e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [72][340/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.7914e-02 (1.2647e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [72][350/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3657e-02 (1.2767e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [72][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.0637e-03 (1.2829e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [72][370/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.3773e-02 (1.2917e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [72][380/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4030e-02 (1.3145e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [72][390/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.1796e-03 (1.3055e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
## e[72] optimizer.zero_grad (sum) time: 0.16936731338500977
## e[72]       loss.backward (sum) time: 4.033440113067627
## e[72]      optimizer.step (sum) time: 1.2303364276885986
## epoch[72] training(only) time: 13.242395401000977
# Switched to evaluate mode...
Test: [  0/100]	Time  0.166 ( 0.166)	Loss 2.0337e-01 (2.0337e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.014 ( 0.031)	Loss 1.9519e-01 (2.5398e-01)	Acc@1  92.00 ( 92.73)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.017 ( 0.024)	Loss 3.3667e-01 (2.7658e-01)	Acc@1  92.00 ( 92.86)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.020 ( 0.022)	Loss 1.8420e-01 (2.8641e-01)	Acc@1  94.00 ( 92.65)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.017 ( 0.021)	Loss 2.3254e-01 (2.8595e-01)	Acc@1  91.00 ( 92.56)	Acc@5 100.00 ( 99.66)
Test: [ 50/100]	Time  0.014 ( 0.020)	Loss 1.3416e-01 (2.8577e-01)	Acc@1  95.00 ( 92.51)	Acc@5 100.00 ( 99.69)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 2.6245e-01 (2.7761e-01)	Acc@1  92.00 ( 92.74)	Acc@5 100.00 ( 99.72)
Test: [ 70/100]	Time  0.024 ( 0.019)	Loss 4.7144e-01 (2.7149e-01)	Acc@1  89.00 ( 92.86)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.018 ( 0.019)	Loss 1.7395e-01 (2.7302e-01)	Acc@1  94.00 ( 92.88)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.020 ( 0.019)	Loss 2.0325e-01 (2.7045e-01)	Acc@1  94.00 ( 92.80)	Acc@5 100.00 ( 99.76)
 * Acc@1 92.850 Acc@5 99.760
### epoch[72] execution time: 15.205492734909058
EPOCH 73
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [73][  0/391]	Time  0.214 ( 0.214)	Data  0.180 ( 0.180)	Loss 1.5839e-02 (1.5839e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [73][ 10/391]	Time  0.033 ( 0.051)	Data  0.001 ( 0.018)	Loss 1.6983e-02 (1.1937e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [73][ 20/391]	Time  0.032 ( 0.042)	Data  0.001 ( 0.010)	Loss 5.5351e-03 (1.2216e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [73][ 30/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.007)	Loss 3.2520e-03 (1.2658e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [73][ 40/391]	Time  0.032 ( 0.038)	Data  0.001 ( 0.006)	Loss 1.0857e-02 (1.2814e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [73][ 50/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 1.6342e-02 (1.2058e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [73][ 60/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 8.3542e-03 (1.2510e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [73][ 70/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.004)	Loss 3.3569e-02 (1.3601e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [73][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 3.0167e-02 (1.4074e-02)	Acc@1  98.44 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [73][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 7.1564e-03 (1.3670e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [73][100/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 5.9547e-03 (1.3460e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [73][110/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.7715e-02 (1.3225e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [73][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.4607e-02 (1.3229e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [73][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.5400e-02 (1.3645e-02)	Acc@1  98.44 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [73][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 7.0152e-03 (1.3795e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [73][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.3344e-02 (1.3787e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [73][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.1978e-02 (1.3625e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [73][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.4305e-03 (1.3585e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [73][180/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4442e-02 (1.3651e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [73][190/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2054e-02 (1.3776e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [73][200/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.3468e-02 (1.4043e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [73][210/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.8572e-03 (1.3972e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [73][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.1019e-03 (1.3979e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [73][230/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4816e-02 (1.3985e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [73][240/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2909e-02 (1.3999e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [73][250/391]	Time  0.041 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.4963e-02 (1.3917e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [73][260/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4328e-02 (1.3856e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [73][270/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.9880e-03 (1.3754e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [73][280/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.4899e-03 (1.3627e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [73][290/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.5155e-03 (1.3665e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [73][300/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9394e-02 (1.3820e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [73][310/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.9973e-03 (1.3717e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [73][320/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.3171e-03 (1.3649e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [73][330/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.5880e-03 (1.3557e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [73][340/391]	Time  0.041 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.3242e-03 (1.3611e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [73][350/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.5197e-02 (1.3735e-02)	Acc@1  98.44 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [73][360/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0498e-02 (1.3962e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [73][370/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0941e-02 (1.4016e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [73][380/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1917e-02 (1.4178e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [73][390/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3916e-02 (1.4134e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
## e[73] optimizer.zero_grad (sum) time: 0.17041468620300293
## e[73]       loss.backward (sum) time: 3.9648630619049072
## e[73]      optimizer.step (sum) time: 1.2161383628845215
## epoch[73] training(only) time: 13.234962701797485
# Switched to evaluate mode...
Test: [  0/100]	Time  0.180 ( 0.180)	Loss 2.1643e-01 (2.1643e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.016 ( 0.032)	Loss 2.0923e-01 (2.5659e-01)	Acc@1  92.00 ( 92.45)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.025 ( 0.025)	Loss 3.6987e-01 (2.7874e-01)	Acc@1  91.00 ( 92.52)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.020 ( 0.023)	Loss 1.7615e-01 (2.8536e-01)	Acc@1  95.00 ( 92.35)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.030 ( 0.021)	Loss 2.3596e-01 (2.8593e-01)	Acc@1  92.00 ( 92.29)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.013 ( 0.020)	Loss 1.4941e-01 (2.8431e-01)	Acc@1  94.00 ( 92.27)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.016 ( 0.020)	Loss 2.6050e-01 (2.7723e-01)	Acc@1  92.00 ( 92.46)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.016 ( 0.020)	Loss 4.4678e-01 (2.7199e-01)	Acc@1  90.00 ( 92.59)	Acc@5 100.00 ( 99.82)
Test: [ 80/100]	Time  0.015 ( 0.019)	Loss 1.6235e-01 (2.7279e-01)	Acc@1  94.00 ( 92.63)	Acc@5 100.00 ( 99.81)
Test: [ 90/100]	Time  0.014 ( 0.019)	Loss 2.1021e-01 (2.6899e-01)	Acc@1  94.00 ( 92.59)	Acc@5 100.00 ( 99.82)
 * Acc@1 92.680 Acc@5 99.830
### epoch[73] execution time: 15.271163702011108
EPOCH 74
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [74][  0/391]	Time  0.220 ( 0.220)	Data  0.187 ( 0.187)	Loss 1.1894e-02 (1.1894e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [74][ 10/391]	Time  0.034 ( 0.051)	Data  0.001 ( 0.019)	Loss 7.3318e-03 (1.3396e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [74][ 20/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.010)	Loss 3.2593e-02 (1.2873e-02)	Acc@1  98.44 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [74][ 30/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.008)	Loss 1.3329e-02 (1.2938e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [74][ 40/391]	Time  0.032 ( 0.038)	Data  0.001 ( 0.006)	Loss 1.5602e-02 (1.3292e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [74][ 50/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 1.1139e-02 (1.2668e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [74][ 60/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.005)	Loss 1.1810e-02 (1.2501e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [74][ 70/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 1.7822e-02 (1.2479e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [74][ 80/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.4816e-02 (1.2380e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [74][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.4000e-02 (1.2398e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [74][100/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 7.6752e-03 (1.2266e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [74][110/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 9.0027e-03 (1.2194e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [74][120/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.003)	Loss 9.0942e-03 (1.2329e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [74][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.4427e-02 (1.2395e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [74][140/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.0340e-02 (1.2296e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [74][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.8665e-03 (1.2704e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [74][160/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.2644e-02 (1.2806e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [74][170/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.6327e-02 (1.2786e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [74][180/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.5929e-03 (1.2804e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [74][190/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.6212e-03 (1.2951e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [74][200/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.5111e-03 (1.2873e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [74][210/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.7400e-03 (1.2762e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [74][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.8043e-03 (1.2634e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [74][230/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.3825e-03 (1.2476e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [74][240/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.7051e-03 (1.2391e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [74][250/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1116e-02 (1.2418e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [74][260/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.1651e-03 (1.2351e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [74][270/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.7291e-03 (1.2259e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [74][280/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.5689e-03 (1.2288e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [74][290/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1322e-02 (1.2151e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [74][300/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0262e-02 (1.2222e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [74][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2589e-02 (1.2146e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [74][320/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.9030e-03 (1.2162e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [74][330/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.2779e-03 (1.2206e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [74][340/391]	Time  0.038 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.6212e-03 (1.2098e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [74][350/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4328e-02 (1.2094e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [74][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.6082e-02 (1.2222e-02)	Acc@1  98.44 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [74][370/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.1271e-02 (1.2257e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [74][380/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.1027e-02 (1.2329e-02)	Acc@1  98.44 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [74][390/391]	Time  0.022 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.5951e-03 (1.2279e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
## e[74] optimizer.zero_grad (sum) time: 0.17000579833984375
## e[74]       loss.backward (sum) time: 4.1137590408325195
## e[74]      optimizer.step (sum) time: 1.191563367843628
## epoch[74] training(only) time: 13.291589498519897
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 1.9482e-01 (1.9482e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.018 ( 0.033)	Loss 2.1399e-01 (2.4866e-01)	Acc@1  92.00 ( 92.91)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.022 ( 0.026)	Loss 3.6182e-01 (2.7293e-01)	Acc@1  91.00 ( 92.86)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.014 ( 0.022)	Loss 1.8005e-01 (2.8303e-01)	Acc@1  95.00 ( 92.74)	Acc@5  99.00 ( 99.68)
Test: [ 40/100]	Time  0.021 ( 0.021)	Loss 2.4011e-01 (2.8475e-01)	Acc@1  92.00 ( 92.63)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.014 ( 0.020)	Loss 1.3611e-01 (2.8393e-01)	Acc@1  95.00 ( 92.59)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.017 ( 0.020)	Loss 2.5439e-01 (2.7570e-01)	Acc@1  92.00 ( 92.75)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.020 ( 0.019)	Loss 4.5044e-01 (2.6996e-01)	Acc@1  89.00 ( 92.86)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.023 ( 0.019)	Loss 1.5369e-01 (2.7124e-01)	Acc@1  95.00 ( 92.84)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.014 ( 0.019)	Loss 1.8372e-01 (2.6851e-01)	Acc@1  95.00 ( 92.78)	Acc@5 100.00 ( 99.77)
 * Acc@1 92.810 Acc@5 99.780
### epoch[74] execution time: 15.283182144165039
EPOCH 75
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [75][  0/391]	Time  0.214 ( 0.214)	Data  0.180 ( 0.180)	Loss 1.8097e-02 (1.8097e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [75][ 10/391]	Time  0.034 ( 0.050)	Data  0.001 ( 0.018)	Loss 7.5073e-03 (1.4587e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [75][ 20/391]	Time  0.032 ( 0.042)	Data  0.001 ( 0.010)	Loss 2.7069e-02 (1.3426e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [75][ 30/391]	Time  0.033 ( 0.039)	Data  0.001 ( 0.007)	Loss 3.1586e-02 (1.5036e-02)	Acc@1  98.44 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [75][ 40/391]	Time  0.032 ( 0.038)	Data  0.001 ( 0.006)	Loss 1.8463e-02 (1.4634e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [75][ 50/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 1.5579e-02 (1.5919e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [75][ 60/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 1.3550e-02 (1.5374e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [75][ 70/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.3680e-02 (1.5061e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [75][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 8.5068e-03 (1.5208e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [75][ 90/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 5.8708e-03 (1.4780e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [75][100/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 4.4220e-02 (1.5032e-02)	Acc@1  98.44 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [75][110/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.3880e-02 (1.4877e-02)	Acc@1  98.44 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [75][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.3046e-02 (1.4920e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [75][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.7242e-02 (1.4896e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [75][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 8.1863e-03 (1.4874e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [75][150/391]	Time  0.031 ( 0.034)	Data  0.000 ( 0.003)	Loss 1.8921e-02 (1.4822e-02)	Acc@1  98.44 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [75][160/391]	Time  0.032 ( 0.034)	Data  0.002 ( 0.003)	Loss 1.3214e-02 (1.4454e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [75][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.2034e-02 (1.4323e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [75][180/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.4087e-03 (1.4113e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [75][190/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.7913e-03 (1.4121e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [75][200/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3565e-02 (1.4062e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [75][210/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.7596e-03 (1.3917e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [75][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9424e-02 (1.3956e-02)	Acc@1  98.44 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [75][230/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.3972e-02 (1.3960e-02)	Acc@1  98.44 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [75][240/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.1542e-03 (1.3903e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [75][250/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.7624e-02 (1.3798e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [75][260/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.6108e-02 (1.4149e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [75][270/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3794e-02 (1.3997e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [75][280/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2360e-02 (1.3963e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [75][290/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5511e-02 (1.3907e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [75][300/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.8776e-03 (1.3782e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [75][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9135e-02 (1.3785e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [75][320/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0231e-02 (1.3777e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [75][330/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9684e-02 (1.3647e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [75][340/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0651e-02 (1.3607e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [75][350/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.2458e-03 (1.3473e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [75][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.7792e-02 (1.3479e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [75][370/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2718e-02 (1.3410e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [75][380/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2207e-02 (1.3345e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [75][390/391]	Time  0.023 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.8130e-03 (1.3286e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
## e[75] optimizer.zero_grad (sum) time: 0.16995549201965332
## e[75]       loss.backward (sum) time: 4.067940950393677
## e[75]      optimizer.step (sum) time: 1.1949610710144043
## epoch[75] training(only) time: 13.269871711730957
# Switched to evaluate mode...
Test: [  0/100]	Time  0.175 ( 0.175)	Loss 2.1326e-01 (2.1326e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.021 ( 0.031)	Loss 1.8408e-01 (2.5411e-01)	Acc@1  92.00 ( 92.82)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.014 ( 0.024)	Loss 3.5059e-01 (2.7795e-01)	Acc@1  91.00 ( 92.62)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.017 ( 0.021)	Loss 1.7834e-01 (2.8621e-01)	Acc@1  95.00 ( 92.48)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.019 ( 0.021)	Loss 2.1997e-01 (2.8577e-01)	Acc@1  91.00 ( 92.41)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.014 ( 0.020)	Loss 1.3806e-01 (2.8426e-01)	Acc@1  94.00 ( 92.37)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 2.4475e-01 (2.7593e-01)	Acc@1  92.00 ( 92.62)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.014 ( 0.019)	Loss 4.5190e-01 (2.6992e-01)	Acc@1  90.00 ( 92.77)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.014 ( 0.019)	Loss 1.7114e-01 (2.7114e-01)	Acc@1  94.00 ( 92.80)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.018 ( 0.018)	Loss 2.0569e-01 (2.6785e-01)	Acc@1  93.00 ( 92.75)	Acc@5 100.00 ( 99.80)
 * Acc@1 92.810 Acc@5 99.800
### epoch[75] execution time: 15.215011835098267
EPOCH 76
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [76][  0/391]	Time  0.216 ( 0.216)	Data  0.183 ( 0.183)	Loss 1.3748e-02 (1.3748e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [76][ 10/391]	Time  0.032 ( 0.050)	Data  0.001 ( 0.018)	Loss 2.6016e-02 (1.4709e-02)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [76][ 20/391]	Time  0.034 ( 0.042)	Data  0.001 ( 0.010)	Loss 8.4000e-03 (1.3754e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [76][ 30/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.007)	Loss 3.2928e-02 (1.3021e-02)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [76][ 40/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.006)	Loss 1.2398e-02 (1.3301e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [76][ 50/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 2.7283e-02 (1.3353e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [76][ 60/391]	Time  0.029 ( 0.036)	Data  0.001 ( 0.004)	Loss 1.2650e-02 (1.2936e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [76][ 70/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.004)	Loss 7.0457e-03 (1.2495e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [76][ 80/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.2611e-02 (1.2485e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [76][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 8.8425e-03 (1.2542e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [76][100/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.7715e-02 (1.2572e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [76][110/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.3351e-02 (1.2382e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [76][120/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.4526e-02 (1.2562e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [76][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.2756e-02 (1.2539e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [76][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.0889e-02 (1.2695e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [76][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.1885e-03 (1.2606e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [76][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.7258e-02 (1.2859e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [76][170/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.7715e-02 (1.3195e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [76][180/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9165e-02 (1.3054e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [76][190/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4221e-02 (1.2873e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [76][200/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.7602e-03 (1.2910e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [76][210/391]	Time  0.030 ( 0.034)	Data  0.002 ( 0.002)	Loss 5.0087e-03 (1.2776e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [76][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.9030e-03 (1.2637e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [76][230/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.8267e-03 (1.2564e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [76][240/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4137e-02 (1.2607e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [76][250/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.0304e-02 (1.2794e-02)	Acc@1  98.44 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [76][260/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.3529e-02 (1.2794e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [76][270/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.3911e-02 (1.2787e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [76][280/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0406e-02 (1.2636e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [76][290/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.6234e-03 (1.2637e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [76][300/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.3782e-03 (1.2804e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [76][310/391]	Time  0.039 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.7441e-03 (1.2744e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [76][320/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.4686e-03 (1.2926e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [76][330/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.1255e-02 (1.3037e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [76][340/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.0935e-02 (1.3077e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [76][350/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.2392e-03 (1.3001e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [76][360/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2428e-02 (1.3036e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [76][370/391]	Time  0.039 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5396e-02 (1.3183e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [76][380/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3695e-02 (1.3170e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [76][390/391]	Time  0.023 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5617e-02 (1.3111e-02)	Acc@1  98.75 ( 99.68)	Acc@5 100.00 (100.00)
## e[76] optimizer.zero_grad (sum) time: 0.1698315143585205
## e[76]       loss.backward (sum) time: 4.028887987136841
## e[76]      optimizer.step (sum) time: 1.2153973579406738
## epoch[76] training(only) time: 13.278794527053833
# Switched to evaluate mode...
Test: [  0/100]	Time  0.171 ( 0.171)	Loss 2.2375e-01 (2.2375e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.015 ( 0.032)	Loss 1.9519e-01 (2.5494e-01)	Acc@1  92.00 ( 92.64)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.015 ( 0.025)	Loss 3.4766e-01 (2.7518e-01)	Acc@1  91.00 ( 92.76)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.021 ( 0.022)	Loss 1.8481e-01 (2.8433e-01)	Acc@1  94.00 ( 92.45)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.016 ( 0.021)	Loss 2.3816e-01 (2.8485e-01)	Acc@1  91.00 ( 92.41)	Acc@5 100.00 ( 99.66)
Test: [ 50/100]	Time  0.014 ( 0.020)	Loss 1.2482e-01 (2.8364e-01)	Acc@1  95.00 ( 92.35)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.014 ( 0.020)	Loss 2.5342e-01 (2.7555e-01)	Acc@1  92.00 ( 92.56)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.014 ( 0.019)	Loss 4.5483e-01 (2.6958e-01)	Acc@1  90.00 ( 92.68)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.021 ( 0.019)	Loss 1.5906e-01 (2.7089e-01)	Acc@1  94.00 ( 92.68)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.014 ( 0.019)	Loss 1.9434e-01 (2.6767e-01)	Acc@1  94.00 ( 92.67)	Acc@5 100.00 ( 99.78)
 * Acc@1 92.740 Acc@5 99.780
### epoch[76] execution time: 15.242396593093872
EPOCH 77
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [77][  0/391]	Time  0.212 ( 0.212)	Data  0.179 ( 0.179)	Loss 1.0872e-02 (1.0872e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [77][ 10/391]	Time  0.034 ( 0.050)	Data  0.001 ( 0.018)	Loss 1.3786e-02 (1.3326e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [77][ 20/391]	Time  0.033 ( 0.042)	Data  0.001 ( 0.010)	Loss 7.6332e-03 (1.2250e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [77][ 30/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.007)	Loss 9.0027e-03 (1.2756e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [77][ 40/391]	Time  0.032 ( 0.038)	Data  0.001 ( 0.006)	Loss 3.1891e-03 (1.3645e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [77][ 50/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 1.0178e-02 (1.3953e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [77][ 60/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 4.6768e-03 (1.4369e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [77][ 70/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.004)	Loss 1.3466e-02 (1.4559e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [77][ 80/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.3176e-02 (1.4168e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [77][ 90/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 6.4163e-03 (1.4346e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [77][100/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.1795e-02 (1.4357e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [77][110/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.3229e-02 (1.5016e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [77][120/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.0590e-02 (1.5012e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [77][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.8632e-03 (1.5132e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [77][140/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.4397e-02 (1.5143e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [77][150/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 8.6899e-03 (1.5080e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [77][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.2482e-02 (1.5035e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [77][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.8204e-03 (1.4638e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [77][180/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9012e-02 (1.4420e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [77][190/391]	Time  0.037 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.4970e-03 (1.4576e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [77][200/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.7569e-03 (1.4611e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [77][210/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4534e-02 (1.4390e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [77][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5381e-02 (1.4213e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [77][230/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.4384e-02 (1.4162e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [77][240/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.8198e-02 (1.4179e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [77][250/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.3106e-03 (1.4263e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [77][260/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.0477e-02 (1.4195e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [77][270/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9745e-02 (1.4171e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [77][280/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9257e-02 (1.4112e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [77][290/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0811e-02 (1.4040e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [77][300/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9318e-02 (1.4012e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [77][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.2714e-03 (1.3952e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [77][320/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.2079e-02 (1.3982e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [77][330/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3130e-02 (1.3906e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [77][340/391]	Time  0.042 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0857e-02 (1.3833e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [77][350/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.6305e-03 (1.3866e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [77][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.9956e-03 (1.3794e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [77][370/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.6060e-03 (1.3736e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [77][380/391]	Time  0.038 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.7820e-03 (1.3733e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [77][390/391]	Time  0.024 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.8806e-02 (1.3873e-02)	Acc@1  96.25 ( 99.69)	Acc@5 100.00 (100.00)
## e[77] optimizer.zero_grad (sum) time: 0.16830778121948242
## e[77]       loss.backward (sum) time: 4.0928285121917725
## e[77]      optimizer.step (sum) time: 1.194392442703247
## epoch[77] training(only) time: 13.252519845962524
# Switched to evaluate mode...
Test: [  0/100]	Time  0.172 ( 0.172)	Loss 2.3840e-01 (2.3840e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.017 ( 0.032)	Loss 1.9202e-01 (2.5975e-01)	Acc@1  92.00 ( 92.82)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.020 ( 0.025)	Loss 3.5205e-01 (2.8056e-01)	Acc@1  91.00 ( 92.71)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.016 ( 0.023)	Loss 1.8604e-01 (2.8809e-01)	Acc@1  95.00 ( 92.52)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.014 ( 0.021)	Loss 2.4097e-01 (2.8928e-01)	Acc@1  90.00 ( 92.39)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.018 ( 0.020)	Loss 1.2695e-01 (2.8675e-01)	Acc@1  96.00 ( 92.35)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.017 ( 0.020)	Loss 2.2656e-01 (2.7761e-01)	Acc@1  94.00 ( 92.59)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.023 ( 0.020)	Loss 4.5410e-01 (2.7181e-01)	Acc@1  90.00 ( 92.75)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.018 ( 0.019)	Loss 1.5698e-01 (2.7316e-01)	Acc@1  94.00 ( 92.72)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.014 ( 0.019)	Loss 2.0081e-01 (2.6984e-01)	Acc@1  93.00 ( 92.68)	Acc@5 100.00 ( 99.80)
 * Acc@1 92.710 Acc@5 99.800
### epoch[77] execution time: 15.297158241271973
EPOCH 78
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [78][  0/391]	Time  0.209 ( 0.209)	Data  0.172 ( 0.172)	Loss 9.5444e-03 (9.5444e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [78][ 10/391]	Time  0.032 ( 0.050)	Data  0.001 ( 0.017)	Loss 2.5513e-02 (1.8945e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [78][ 20/391]	Time  0.032 ( 0.042)	Data  0.001 ( 0.010)	Loss 1.8723e-02 (1.6603e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [78][ 30/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.007)	Loss 3.1830e-02 (1.5137e-02)	Acc@1  98.44 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [78][ 40/391]	Time  0.032 ( 0.038)	Data  0.001 ( 0.006)	Loss 1.6891e-02 (1.3828e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [78][ 50/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.005)	Loss 3.4962e-03 (1.3569e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [78][ 60/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 3.6652e-02 (1.4384e-02)	Acc@1  98.44 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [78][ 70/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 1.2947e-02 (1.3761e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [78][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.8539e-03 (1.3599e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [78][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 3.2196e-02 (1.3576e-02)	Acc@1  98.44 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [78][100/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 8.4610e-03 (1.3403e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [78][110/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 4.5280e-03 (1.2950e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [78][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.7170e-02 (1.3102e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [78][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.1513e-02 (1.3067e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [78][140/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.8692e-02 (1.3193e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [78][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.9190e-02 (1.3081e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [78][160/391]	Time  0.028 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0780e-02 (1.3048e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [78][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.9803e-03 (1.3356e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [78][180/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.6234e-03 (1.3316e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [78][190/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.0806e-03 (1.3299e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [78][200/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.8471e-03 (1.3142e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [78][210/391]	Time  0.027 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2352e-02 (1.3076e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [78][220/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.0763e-03 (1.2979e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [78][230/391]	Time  0.037 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.1182e-03 (1.2969e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [78][240/391]	Time  0.046 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.7340e-03 (1.2896e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [78][250/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.9236e-02 (1.3012e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [78][260/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.3755e-03 (1.2847e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [78][270/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.7504e-03 (1.2785e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [78][280/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.5803e-02 (1.2755e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [78][290/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.6060e-03 (1.2743e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [78][300/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.4008e-03 (1.2734e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [78][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.6953e-03 (1.2772e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [78][320/391]	Time  0.036 ( 0.034)	Data  0.002 ( 0.002)	Loss 9.9869e-03 (1.2814e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [78][330/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1803e-02 (1.2748e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [78][340/391]	Time  0.037 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.7456e-02 (1.2731e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [78][350/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.9166e-03 (1.2711e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [78][360/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.0823e-03 (1.2610e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [78][370/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.8419e-03 (1.2584e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [78][380/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.6733e-02 (1.2522e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [78][390/391]	Time  0.022 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.3245e-02 (1.2576e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
## e[78] optimizer.zero_grad (sum) time: 0.16870999336242676
## e[78]       loss.backward (sum) time: 3.9851651191711426
## e[78]      optimizer.step (sum) time: 1.2410264015197754
## epoch[78] training(only) time: 13.23633599281311
# Switched to evaluate mode...
Test: [  0/100]	Time  0.176 ( 0.176)	Loss 2.3511e-01 (2.3511e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.014 ( 0.032)	Loss 1.8958e-01 (2.5985e-01)	Acc@1  92.00 ( 92.55)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.016 ( 0.025)	Loss 3.4839e-01 (2.7951e-01)	Acc@1  93.00 ( 92.62)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.016 ( 0.022)	Loss 1.8701e-01 (2.8811e-01)	Acc@1  94.00 ( 92.26)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.018 ( 0.021)	Loss 2.3120e-01 (2.8755e-01)	Acc@1  90.00 ( 92.20)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.013 ( 0.020)	Loss 1.3745e-01 (2.8491e-01)	Acc@1  96.00 ( 92.20)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.021 ( 0.020)	Loss 2.3987e-01 (2.7712e-01)	Acc@1  93.00 ( 92.49)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.015 ( 0.019)	Loss 4.5654e-01 (2.7087e-01)	Acc@1  90.00 ( 92.63)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.013 ( 0.019)	Loss 1.7542e-01 (2.7186e-01)	Acc@1  93.00 ( 92.60)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.014 ( 0.018)	Loss 2.0703e-01 (2.6805e-01)	Acc@1  93.00 ( 92.58)	Acc@5 100.00 ( 99.80)
 * Acc@1 92.630 Acc@5 99.800
### epoch[78] execution time: 15.18451189994812
EPOCH 79
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [79][  0/391]	Time  0.211 ( 0.211)	Data  0.178 ( 0.178)	Loss 1.7487e-02 (1.7487e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [79][ 10/391]	Time  0.032 ( 0.051)	Data  0.001 ( 0.018)	Loss 5.2147e-03 (1.3609e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [79][ 20/391]	Time  0.032 ( 0.042)	Data  0.001 ( 0.010)	Loss 2.4368e-02 (1.2584e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [79][ 30/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.007)	Loss 7.0267e-03 (1.1958e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [79][ 40/391]	Time  0.032 ( 0.038)	Data  0.001 ( 0.006)	Loss 9.8724e-03 (1.2566e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [79][ 50/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 5.4970e-03 (1.2761e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [79][ 60/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.004)	Loss 1.8372e-02 (1.3577e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [79][ 70/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 1.4206e-02 (1.3555e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [79][ 80/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.004)	Loss 3.9043e-03 (1.3338e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [79][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.6505e-02 (1.3105e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [79][100/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 6.4087e-03 (1.2768e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [79][110/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.9210e-03 (1.2581e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [79][120/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.3178e-02 (1.3363e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [79][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.1284e-02 (1.3881e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [79][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 7.1297e-03 (1.3735e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [79][150/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 9.3994e-03 (1.3514e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [79][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.8637e-03 (1.3355e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [79][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.2697e-03 (1.3385e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [79][180/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4656e-02 (1.3376e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [79][190/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1642e-02 (1.3332e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [79][200/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.0850e-03 (1.3292e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [79][210/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.3373e-03 (1.3292e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [79][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0696e-02 (1.3268e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [79][230/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.3084e-03 (1.3375e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [79][240/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9394e-02 (1.3439e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [79][250/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.0926e-03 (1.3296e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [79][260/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.9673e-02 (1.3296e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [79][270/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.0953e-03 (1.3321e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [79][280/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.7755e-03 (1.3232e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [79][290/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.6180e-03 (1.3346e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [79][300/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.8981e-03 (1.3137e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [79][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0040e-02 (1.3275e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [79][320/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.3155e-03 (1.3169e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [79][330/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.3515e-03 (1.3160e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [79][340/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.4806e-03 (1.3267e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [79][350/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 7.0038e-03 (1.3355e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [79][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.1342e-02 (1.3410e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [79][370/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.8654e-03 (1.3343e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [79][380/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1497e-02 (1.3362e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [79][390/391]	Time  0.023 ( 0.033)	Data  0.001 ( 0.002)	Loss 9.5901e-03 (1.3361e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
## e[79] optimizer.zero_grad (sum) time: 0.16999554634094238
## e[79]       loss.backward (sum) time: 4.088501930236816
## e[79]      optimizer.step (sum) time: 1.2063679695129395
## epoch[79] training(only) time: 13.211758136749268
# Switched to evaluate mode...
Test: [  0/100]	Time  0.174 ( 0.174)	Loss 2.3621e-01 (2.3621e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.020 ( 0.031)	Loss 1.7285e-01 (2.5882e-01)	Acc@1  93.00 ( 92.55)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.018 ( 0.025)	Loss 3.4277e-01 (2.7858e-01)	Acc@1  93.00 ( 92.71)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.016 ( 0.022)	Loss 1.8994e-01 (2.8726e-01)	Acc@1  94.00 ( 92.35)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.018 ( 0.021)	Loss 2.3413e-01 (2.8756e-01)	Acc@1  90.00 ( 92.24)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.021 ( 0.020)	Loss 1.2695e-01 (2.8503e-01)	Acc@1  96.00 ( 92.22)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.014 ( 0.020)	Loss 2.3242e-01 (2.7663e-01)	Acc@1  93.00 ( 92.49)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.013 ( 0.019)	Loss 4.5435e-01 (2.7049e-01)	Acc@1  90.00 ( 92.66)	Acc@5 100.00 ( 99.80)
Test: [ 80/100]	Time  0.016 ( 0.019)	Loss 1.6418e-01 (2.7179e-01)	Acc@1  93.00 ( 92.64)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.021 ( 0.019)	Loss 2.1313e-01 (2.6838e-01)	Acc@1  92.00 ( 92.60)	Acc@5 100.00 ( 99.81)
 * Acc@1 92.640 Acc@5 99.810
### epoch[79] execution time: 15.211793184280396
EPOCH 80
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [80][  0/391]	Time  0.208 ( 0.208)	Data  0.180 ( 0.180)	Loss 1.4931e-02 (1.4931e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [80][ 10/391]	Time  0.032 ( 0.050)	Data  0.001 ( 0.018)	Loss 1.3763e-02 (1.1952e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [80][ 20/391]	Time  0.032 ( 0.042)	Data  0.001 ( 0.010)	Loss 1.3702e-02 (1.4445e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [80][ 30/391]	Time  0.031 ( 0.040)	Data  0.001 ( 0.007)	Loss 8.9951e-03 (1.3589e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [80][ 40/391]	Time  0.032 ( 0.038)	Data  0.001 ( 0.006)	Loss 1.2238e-02 (1.4719e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [80][ 50/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.005)	Loss 1.0742e-02 (1.4248e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [80][ 60/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 8.3694e-03 (1.4172e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [80][ 70/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 1.1559e-02 (1.4081e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [80][ 80/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.004)	Loss 1.3252e-02 (1.4071e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [80][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 3.5172e-03 (1.4618e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 ( 99.99)
Epoch: [80][100/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 4.8904e-03 (1.4565e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 ( 99.99)
Epoch: [80][110/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.3474e-02 (1.4265e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 ( 99.99)
Epoch: [80][120/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 7.2975e-03 (1.4131e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 ( 99.99)
Epoch: [80][130/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 9.7046e-03 (1.4079e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 ( 99.99)
Epoch: [80][140/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.5583e-02 (1.4060e-02)	Acc@1  98.44 ( 99.70)	Acc@5 100.00 ( 99.99)
Epoch: [80][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.4496e-02 (1.4027e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 ( 99.99)
Epoch: [80][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.1620e-02 (1.4047e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [80][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.1403e-02 (1.4245e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [80][180/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.5085e-02 (1.4393e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [80][190/391]	Time  0.031 ( 0.034)	Data  0.002 ( 0.002)	Loss 1.2924e-02 (1.4192e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [80][200/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.4359e-03 (1.4144e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [80][210/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.5513e-02 (1.4189e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [80][220/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.3694e-03 (1.3967e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [80][230/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5747e-02 (1.3733e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [80][240/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.6525e-02 (1.3608e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [80][250/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.2784e-03 (1.3545e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [80][260/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.4180e-02 (1.3598e-02)	Acc@1  98.44 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [80][270/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.3694e-03 (1.3725e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [80][280/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.6953e-02 (1.3670e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [80][290/391]	Time  0.040 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1620e-02 (1.3663e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [80][300/391]	Time  0.040 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.5152e-03 (1.3591e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [80][310/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.5269e-03 (1.3549e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [80][320/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.6458e-03 (1.3676e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [80][330/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9287e-02 (1.3852e-02)	Acc@1  98.44 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [80][340/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4999e-02 (1.3848e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [80][350/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.7603e-02 (1.3835e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [80][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3245e-02 (1.3977e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [80][370/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.2239e-03 (1.3937e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [80][380/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.4504e-03 (1.3823e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [80][390/391]	Time  0.024 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9028e-02 (1.3929e-02)	Acc@1  98.75 ( 99.67)	Acc@5 100.00 (100.00)
## e[80] optimizer.zero_grad (sum) time: 0.16950249671936035
## e[80]       loss.backward (sum) time: 3.9640161991119385
## e[80]      optimizer.step (sum) time: 1.2239515781402588
## epoch[80] training(only) time: 13.255208969116211
# Switched to evaluate mode...
Test: [  0/100]	Time  0.174 ( 0.174)	Loss 2.0984e-01 (2.0984e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.018 ( 0.031)	Loss 1.9434e-01 (2.5070e-01)	Acc@1  92.00 ( 92.55)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.019 ( 0.025)	Loss 3.4668e-01 (2.7291e-01)	Acc@1  92.00 ( 92.62)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.014 ( 0.022)	Loss 1.8286e-01 (2.8166e-01)	Acc@1  95.00 ( 92.45)	Acc@5  99.00 ( 99.68)
Test: [ 40/100]	Time  0.017 ( 0.021)	Loss 2.3633e-01 (2.8288e-01)	Acc@1  91.00 ( 92.41)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.3062e-01 (2.8178e-01)	Acc@1  95.00 ( 92.37)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.020 ( 0.020)	Loss 2.4976e-01 (2.7427e-01)	Acc@1  93.00 ( 92.69)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.014 ( 0.019)	Loss 4.5239e-01 (2.6881e-01)	Acc@1  90.00 ( 92.80)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.014 ( 0.019)	Loss 1.6479e-01 (2.7014e-01)	Acc@1  93.00 ( 92.79)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.014 ( 0.019)	Loss 1.9592e-01 (2.6706e-01)	Acc@1  95.00 ( 92.75)	Acc@5 100.00 ( 99.80)
 * Acc@1 92.790 Acc@5 99.810
### epoch[80] execution time: 15.213028907775879
EPOCH 81
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [81][  0/391]	Time  0.194 ( 0.194)	Data  0.158 ( 0.158)	Loss 3.9825e-03 (3.9825e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [81][ 10/391]	Time  0.032 ( 0.049)	Data  0.001 ( 0.016)	Loss 2.6657e-02 (1.1945e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [81][ 20/391]	Time  0.029 ( 0.041)	Data  0.001 ( 0.009)	Loss 2.7512e-02 (1.3466e-02)	Acc@1  98.44 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [81][ 30/391]	Time  0.042 ( 0.038)	Data  0.002 ( 0.007)	Loss 2.2293e-02 (1.2766e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [81][ 40/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.006)	Loss 8.2169e-03 (1.2508e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [81][ 50/391]	Time  0.029 ( 0.036)	Data  0.001 ( 0.005)	Loss 1.6632e-02 (1.3454e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [81][ 60/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.004)	Loss 9.6359e-03 (1.3015e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [81][ 70/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.4473e-02 (1.2907e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [81][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 2.0401e-02 (1.2956e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [81][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 5.7182e-03 (1.2681e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [81][100/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.1790e-02 (1.3178e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [81][110/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.4275e-02 (1.3217e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [81][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.8082e-02 (1.3000e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [81][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.2562e-02 (1.3112e-02)	Acc@1  98.44 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [81][140/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.1760e-03 (1.3214e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [81][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 8.2932e-03 (1.2898e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [81][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.8488e-02 (1.3090e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [81][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.8204e-03 (1.3201e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [81][180/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.7740e-02 (1.3406e-02)	Acc@1  98.44 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [81][190/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0551e-02 (1.3330e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [81][200/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.4763e-03 (1.3231e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [81][210/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.1847e-03 (1.3177e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [81][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2390e-02 (1.3169e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [81][230/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2749e-02 (1.3260e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [81][240/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.2212e-03 (1.3145e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [81][250/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.1389e-03 (1.3184e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [81][260/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0460e-02 (1.3208e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [81][270/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.2637e-03 (1.3108e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [81][280/391]	Time  0.040 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.9635e-03 (1.3241e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [81][290/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.6001e-02 (1.3323e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [81][300/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.5765e-03 (1.3302e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [81][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0078e-02 (1.3171e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [81][320/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9745e-02 (1.3123e-02)	Acc@1  98.44 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [81][330/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2222e-02 (1.3080e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [81][340/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0612e-02 (1.3193e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [81][350/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.4070e-03 (1.3180e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [81][360/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0559e-02 (1.3184e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [81][370/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.5833e-02 (1.3203e-02)	Acc@1  98.44 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [81][380/391]	Time  0.037 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.0155e-02 (1.3359e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [81][390/391]	Time  0.024 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.6891e-02 (1.3318e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
## e[81] optimizer.zero_grad (sum) time: 0.16872668266296387
## e[81]       loss.backward (sum) time: 3.9932849407196045
## e[81]      optimizer.step (sum) time: 1.2216262817382812
## epoch[81] training(only) time: 13.22641634941101
# Switched to evaluate mode...
Test: [  0/100]	Time  0.166 ( 0.166)	Loss 2.1594e-01 (2.1594e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.022 ( 0.030)	Loss 2.1545e-01 (2.5891e-01)	Acc@1  92.00 ( 92.55)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.017 ( 0.024)	Loss 3.4717e-01 (2.7781e-01)	Acc@1  91.00 ( 92.67)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.014 ( 0.021)	Loss 1.8274e-01 (2.8794e-01)	Acc@1  95.00 ( 92.45)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.020 ( 0.020)	Loss 2.2205e-01 (2.8646e-01)	Acc@1  93.00 ( 92.41)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.014 ( 0.019)	Loss 1.3965e-01 (2.8471e-01)	Acc@1  96.00 ( 92.41)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.018 ( 0.019)	Loss 2.4451e-01 (2.7670e-01)	Acc@1  92.00 ( 92.56)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.028 ( 0.019)	Loss 4.5483e-01 (2.7049e-01)	Acc@1  90.00 ( 92.70)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.021 ( 0.019)	Loss 1.6492e-01 (2.7059e-01)	Acc@1  94.00 ( 92.73)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.014 ( 0.019)	Loss 2.0691e-01 (2.6735e-01)	Acc@1  93.00 ( 92.68)	Acc@5 100.00 ( 99.79)
 * Acc@1 92.790 Acc@5 99.800
### epoch[81] execution time: 15.183608293533325
EPOCH 82
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [82][  0/391]	Time  0.209 ( 0.209)	Data  0.170 ( 0.170)	Loss 1.3176e-02 (1.3176e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [82][ 10/391]	Time  0.032 ( 0.050)	Data  0.001 ( 0.017)	Loss 1.6724e-02 (1.5639e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [82][ 20/391]	Time  0.032 ( 0.042)	Data  0.001 ( 0.010)	Loss 8.9111e-03 (1.3258e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [82][ 30/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.007)	Loss 7.0190e-03 (1.4890e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [82][ 40/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.006)	Loss 5.4741e-03 (1.4555e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [82][ 50/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 1.0696e-02 (1.3547e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [82][ 60/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 2.1248e-03 (1.3084e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [82][ 70/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.2482e-02 (1.3204e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [82][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.9547e-02 (1.3375e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [82][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.1284e-02 (1.3173e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [82][100/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.0752e-02 (1.3451e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [82][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.0169e-03 (1.3234e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [82][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.7373e-03 (1.2994e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [82][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.6769e-02 (1.3140e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [82][140/391]	Time  0.042 ( 0.034)	Data  0.001 ( 0.003)	Loss 9.0027e-03 (1.3238e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [82][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.4908e-02 (1.3307e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [82][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2291e-02 (1.3117e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [82][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.3744e-03 (1.2941e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [82][180/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.1725e-03 (1.2704e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [82][190/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.6588e-03 (1.2637e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [82][200/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.2981e-03 (1.2604e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [82][210/391]	Time  0.037 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.7380e-02 (1.2570e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [82][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4969e-02 (1.2518e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [82][230/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.6993e-02 (1.2693e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [82][240/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.8982e-02 (1.2541e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [82][250/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.5367e-03 (1.2573e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [82][260/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3084e-02 (1.2583e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [82][270/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2901e-02 (1.2600e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [82][280/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.9433e-03 (1.2508e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [82][290/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1452e-02 (1.2552e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [82][300/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.1869e-03 (1.2610e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [82][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.2908e-02 (1.2696e-02)	Acc@1  97.66 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [82][320/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1047e-02 (1.2655e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [82][330/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.8311e-02 (1.2583e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [82][340/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3695e-02 (1.2588e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [82][350/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.8398e-03 (1.2633e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [82][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.0408e-03 (1.2727e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [82][370/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.4523e-03 (1.2798e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [82][380/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.9373e-02 (1.2840e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [82][390/391]	Time  0.023 ( 0.033)	Data  0.001 ( 0.002)	Loss 8.2932e-03 (1.2860e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
## e[82] optimizer.zero_grad (sum) time: 0.16931796073913574
## e[82]       loss.backward (sum) time: 4.074394702911377
## e[82]      optimizer.step (sum) time: 1.2094087600708008
## epoch[82] training(only) time: 13.21481990814209
# Switched to evaluate mode...
Test: [  0/100]	Time  0.174 ( 0.174)	Loss 2.2595e-01 (2.2595e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.019 ( 0.031)	Loss 2.0642e-01 (2.5773e-01)	Acc@1  92.00 ( 92.64)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.014 ( 0.025)	Loss 3.3594e-01 (2.7550e-01)	Acc@1  91.00 ( 92.71)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.014 ( 0.022)	Loss 1.8005e-01 (2.8447e-01)	Acc@1  95.00 ( 92.35)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.020 ( 0.021)	Loss 2.3425e-01 (2.8521e-01)	Acc@1  92.00 ( 92.32)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.014 ( 0.020)	Loss 1.3074e-01 (2.8369e-01)	Acc@1  96.00 ( 92.31)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.014 ( 0.020)	Loss 2.4805e-01 (2.7541e-01)	Acc@1  92.00 ( 92.49)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.016 ( 0.019)	Loss 4.6753e-01 (2.6997e-01)	Acc@1  90.00 ( 92.62)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.019 ( 0.019)	Loss 1.5930e-01 (2.7100e-01)	Acc@1  94.00 ( 92.65)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.015 ( 0.019)	Loss 2.0459e-01 (2.6816e-01)	Acc@1  94.00 ( 92.63)	Acc@5 100.00 ( 99.79)
 * Acc@1 92.720 Acc@5 99.800
### epoch[82] execution time: 15.175810098648071
EPOCH 83
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [83][  0/391]	Time  0.214 ( 0.214)	Data  0.180 ( 0.180)	Loss 2.3102e-02 (2.3102e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [83][ 10/391]	Time  0.041 ( 0.051)	Data  0.001 ( 0.018)	Loss 6.5956e-03 (1.1797e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [83][ 20/391]	Time  0.032 ( 0.043)	Data  0.001 ( 0.010)	Loss 9.0866e-03 (1.0838e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [83][ 30/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.007)	Loss 3.7048e-02 (1.2072e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [83][ 40/391]	Time  0.032 ( 0.038)	Data  0.001 ( 0.006)	Loss 1.9028e-02 (1.2804e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [83][ 50/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 2.4109e-02 (1.2320e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [83][ 60/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.004)	Loss 6.6833e-03 (1.2844e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [83][ 70/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 2.9541e-02 (1.2943e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [83][ 80/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 9.0408e-03 (1.2685e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [83][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 9.3079e-03 (1.2543e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [83][100/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 7.6180e-03 (1.2594e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [83][110/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.6724e-02 (1.2782e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [83][120/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 7.8888e-03 (1.2711e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [83][130/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 7.6828e-03 (1.2628e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [83][140/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 9.1934e-03 (1.2558e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [83][150/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.003)	Loss 8.9340e-03 (1.2414e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [83][160/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.4049e-03 (1.2265e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [83][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.7596e-03 (1.2162e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [83][180/391]	Time  0.027 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4755e-02 (1.2086e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [83][190/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.2523e-03 (1.2402e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [83][200/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.5444e-03 (1.2596e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [83][210/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.7433e-03 (1.2630e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [83][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.4932e-03 (1.2682e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [83][230/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.2217e-02 (1.2784e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [83][240/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.4866e-03 (1.2735e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [83][250/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.5284e-02 (1.3011e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [83][260/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.4528e-03 (1.3019e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [83][270/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.9892e-02 (1.3167e-02)	Acc@1  98.44 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [83][280/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.3853e-03 (1.3157e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [83][290/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.0790e-03 (1.3133e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [83][300/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.1024e-03 (1.3043e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [83][310/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1780e-02 (1.3003e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [83][320/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2337e-02 (1.2948e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [83][330/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.0654e-03 (1.2914e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [83][340/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2527e-02 (1.2935e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [83][350/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1086e-02 (1.2875e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [83][360/391]	Time  0.028 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4732e-02 (1.2888e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [83][370/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4732e-02 (1.2888e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [83][380/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.2447e-03 (1.2891e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [83][390/391]	Time  0.027 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.4603e-02 (1.2901e-02)	Acc@1  98.75 ( 99.69)	Acc@5 100.00 (100.00)
## e[83] optimizer.zero_grad (sum) time: 0.1684708595275879
## e[83]       loss.backward (sum) time: 3.8686983585357666
## e[83]      optimizer.step (sum) time: 1.245990514755249
## epoch[83] training(only) time: 13.221959352493286
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 2.1240e-01 (2.1240e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.020 ( 0.030)	Loss 1.9287e-01 (2.5326e-01)	Acc@1  91.00 ( 92.36)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.019 ( 0.024)	Loss 3.5767e-01 (2.7496e-01)	Acc@1  93.00 ( 92.52)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.020 ( 0.021)	Loss 1.7822e-01 (2.8403e-01)	Acc@1  95.00 ( 92.42)	Acc@5  99.00 ( 99.65)
Test: [ 40/100]	Time  0.019 ( 0.020)	Loss 2.7734e-01 (2.8518e-01)	Acc@1  91.00 ( 92.39)	Acc@5 100.00 ( 99.66)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.3770e-01 (2.8356e-01)	Acc@1  96.00 ( 92.31)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 2.3145e-01 (2.7574e-01)	Acc@1  93.00 ( 92.56)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.014 ( 0.019)	Loss 4.9585e-01 (2.7074e-01)	Acc@1  89.00 ( 92.66)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.5674e-01 (2.7229e-01)	Acc@1  95.00 ( 92.65)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.015 ( 0.019)	Loss 1.8799e-01 (2.6973e-01)	Acc@1  95.00 ( 92.66)	Acc@5 100.00 ( 99.78)
 * Acc@1 92.700 Acc@5 99.790
### epoch[83] execution time: 15.178070545196533
EPOCH 84
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [84][  0/391]	Time  0.214 ( 0.214)	Data  0.169 ( 0.169)	Loss 1.0262e-02 (1.0262e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [84][ 10/391]	Time  0.034 ( 0.050)	Data  0.001 ( 0.017)	Loss 4.6349e-03 (1.1205e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [84][ 20/391]	Time  0.033 ( 0.042)	Data  0.001 ( 0.010)	Loss 1.5549e-02 (1.4207e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [84][ 30/391]	Time  0.033 ( 0.039)	Data  0.001 ( 0.007)	Loss 8.4763e-03 (1.3445e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [84][ 40/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.006)	Loss 1.0727e-02 (1.3520e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [84][ 50/391]	Time  0.041 ( 0.037)	Data  0.001 ( 0.005)	Loss 8.8577e-03 (1.3459e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [84][ 60/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.004)	Loss 7.3013e-03 (1.3076e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [84][ 70/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.004)	Loss 1.1436e-02 (1.2321e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [84][ 80/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 8.4610e-03 (1.2674e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [84][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.6724e-02 (1.2252e-02)	Acc@1  98.44 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [84][100/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.2840e-02 (1.2321e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [84][110/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.2840e-02 (1.2579e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [84][120/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.003)	Loss 3.2928e-02 (1.2900e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [84][130/391]	Time  0.027 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.0742e-02 (1.2871e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [84][140/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.8234e-02 (1.2807e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [84][150/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.003)	Loss 9.6741e-03 (1.2830e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [84][160/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.4546e-02 (1.2883e-02)	Acc@1  98.44 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [84][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.2185e-03 (1.3009e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [84][180/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.4043e-03 (1.3038e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [84][190/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.6724e-02 (1.3180e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [84][200/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.9417e-03 (1.3214e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [84][210/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.7128e-03 (1.3143e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [84][220/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.7814e-03 (1.3181e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [84][230/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.6752e-03 (1.3234e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [84][240/391]	Time  0.037 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.6755e-03 (1.3037e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [84][250/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2306e-02 (1.3021e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [84][260/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.2948e-03 (1.2890e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [84][270/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9592e-02 (1.2908e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [84][280/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5060e-02 (1.3023e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [84][290/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.8961e-02 (1.3074e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [84][300/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.2010e-03 (1.2991e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [84][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.4741e-03 (1.3041e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [84][320/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.6037e-02 (1.3076e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [84][330/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1559e-02 (1.3046e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [84][340/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.7359e-02 (1.3269e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [84][350/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.1079e-03 (1.3231e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [84][360/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.0828e-02 (1.3154e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [84][370/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9058e-02 (1.3282e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [84][380/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0483e-02 (1.3383e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [84][390/391]	Time  0.025 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.7517e-02 (1.3333e-02)	Acc@1  98.75 ( 99.71)	Acc@5 100.00 (100.00)
## e[84] optimizer.zero_grad (sum) time: 0.16789674758911133
## e[84]       loss.backward (sum) time: 3.9464285373687744
## e[84]      optimizer.step (sum) time: 1.241558313369751
## epoch[84] training(only) time: 13.297685623168945
# Switched to evaluate mode...
Test: [  0/100]	Time  0.174 ( 0.174)	Loss 1.8530e-01 (1.8530e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.013 ( 0.030)	Loss 1.9080e-01 (2.4973e-01)	Acc@1  92.00 ( 92.73)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.014 ( 0.024)	Loss 3.6475e-01 (2.7692e-01)	Acc@1  92.00 ( 92.81)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.014 ( 0.022)	Loss 1.7725e-01 (2.8723e-01)	Acc@1  96.00 ( 92.65)	Acc@5  99.00 ( 99.68)
Test: [ 40/100]	Time  0.014 ( 0.021)	Loss 2.3840e-01 (2.8808e-01)	Acc@1  91.00 ( 92.51)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.014 ( 0.020)	Loss 1.2622e-01 (2.8710e-01)	Acc@1  95.00 ( 92.47)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.016 ( 0.020)	Loss 2.4329e-01 (2.7840e-01)	Acc@1  92.00 ( 92.72)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.014 ( 0.019)	Loss 4.4531e-01 (2.7228e-01)	Acc@1  90.00 ( 92.79)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.020 ( 0.019)	Loss 1.4978e-01 (2.7310e-01)	Acc@1  93.00 ( 92.78)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.015 ( 0.019)	Loss 1.9653e-01 (2.7006e-01)	Acc@1  94.00 ( 92.74)	Acc@5 100.00 ( 99.79)
 * Acc@1 92.780 Acc@5 99.800
### epoch[84] execution time: 15.273398399353027
EPOCH 85
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [85][  0/391]	Time  0.210 ( 0.210)	Data  0.177 ( 0.177)	Loss 9.1248e-03 (9.1248e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [85][ 10/391]	Time  0.032 ( 0.049)	Data  0.001 ( 0.018)	Loss 4.4479e-03 (1.2077e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [85][ 20/391]	Time  0.032 ( 0.041)	Data  0.001 ( 0.010)	Loss 8.9798e-03 (1.5993e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [85][ 30/391]	Time  0.033 ( 0.039)	Data  0.001 ( 0.007)	Loss 8.3160e-03 (1.5211e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [85][ 40/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.006)	Loss 3.9368e-02 (1.4616e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [85][ 50/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.005)	Loss 8.3466e-03 (1.4205e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [85][ 60/391]	Time  0.028 ( 0.036)	Data  0.001 ( 0.004)	Loss 1.6464e-02 (1.3364e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [85][ 70/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 4.2534e-03 (1.3419e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [85][ 80/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.2321e-02 (1.3377e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [85][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.0071e-02 (1.3425e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [85][100/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 4.1771e-03 (1.3234e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [85][110/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.3420e-02 (1.3291e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [85][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.9785e-02 (1.3404e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [85][130/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.8768e-02 (1.3781e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [85][140/391]	Time  0.037 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.6452e-03 (1.3533e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [85][150/391]	Time  0.037 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.6605e-03 (1.3446e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [85][160/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.003)	Loss 7.8125e-03 (1.3212e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [85][170/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.3657e-02 (1.3113e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [85][180/391]	Time  0.040 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.1100e-03 (1.3187e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [85][190/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.1607e-03 (1.3389e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [85][200/391]	Time  0.039 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.6986e-03 (1.3146e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [85][210/391]	Time  0.027 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1505e-02 (1.3106e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [85][220/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.6846e-02 (1.3149e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [85][230/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.7365e-02 (1.3189e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [85][240/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.0685e-02 (1.3163e-02)	Acc@1  98.44 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [85][250/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.1253e-03 (1.3288e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [85][260/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.4599e-03 (1.3213e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [85][270/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.7427e-03 (1.3126e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [85][280/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.8943e-03 (1.3271e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [85][290/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.1177e-03 (1.3216e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [85][300/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.6795e-03 (1.3154e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [85][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.8515e-03 (1.3165e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [85][320/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4038e-02 (1.3275e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [85][330/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.9556e-02 (1.3247e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [85][340/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.5610e-03 (1.3190e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [85][350/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.2768e-03 (1.3096e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [85][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0986e-02 (1.2973e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [85][370/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0239e-02 (1.3048e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [85][380/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.3270e-02 (1.3154e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [85][390/391]	Time  0.025 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.6381e-03 (1.3060e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
## e[85] optimizer.zero_grad (sum) time: 0.16777658462524414
## e[85]       loss.backward (sum) time: 3.8917312622070312
## e[85]      optimizer.step (sum) time: 1.2556536197662354
## epoch[85] training(only) time: 13.188042163848877
# Switched to evaluate mode...
Test: [  0/100]	Time  0.173 ( 0.173)	Loss 2.0496e-01 (2.0496e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.017 ( 0.031)	Loss 2.0825e-01 (2.5271e-01)	Acc@1  92.00 ( 92.82)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.014 ( 0.024)	Loss 3.4888e-01 (2.7725e-01)	Acc@1  91.00 ( 92.86)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.022 ( 0.022)	Loss 1.9653e-01 (2.8682e-01)	Acc@1  95.00 ( 92.55)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.019 ( 0.021)	Loss 2.4353e-01 (2.8585e-01)	Acc@1  92.00 ( 92.49)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.013 ( 0.020)	Loss 1.3635e-01 (2.8485e-01)	Acc@1  97.00 ( 92.43)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.013 ( 0.020)	Loss 2.5928e-01 (2.7808e-01)	Acc@1  93.00 ( 92.61)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.015 ( 0.019)	Loss 4.6802e-01 (2.7144e-01)	Acc@1  89.00 ( 92.72)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.020 ( 0.019)	Loss 1.7078e-01 (2.7281e-01)	Acc@1  95.00 ( 92.77)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.020 ( 0.019)	Loss 2.0435e-01 (2.7026e-01)	Acc@1  94.00 ( 92.71)	Acc@5 100.00 ( 99.79)
 * Acc@1 92.790 Acc@5 99.800
### epoch[85] execution time: 15.185700416564941
EPOCH 86
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [86][  0/391]	Time  0.206 ( 0.206)	Data  0.172 ( 0.172)	Loss 1.0254e-02 (1.0254e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [86][ 10/391]	Time  0.032 ( 0.049)	Data  0.001 ( 0.017)	Loss 1.2413e-02 (1.0588e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [86][ 20/391]	Time  0.032 ( 0.041)	Data  0.001 ( 0.010)	Loss 1.8036e-02 (1.2884e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [86][ 30/391]	Time  0.032 ( 0.038)	Data  0.001 ( 0.007)	Loss 1.3634e-02 (1.2671e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [86][ 40/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.006)	Loss 8.8120e-03 (1.4041e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [86][ 50/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.005)	Loss 8.3847e-03 (1.3700e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [86][ 60/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.004)	Loss 7.6294e-03 (1.3897e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [86][ 70/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.004)	Loss 2.1301e-02 (1.3916e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [86][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.6220e-02 (1.3924e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [86][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.5541e-02 (1.3857e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [86][100/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.4938e-02 (1.3677e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [86][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.5509e-03 (1.3366e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [86][120/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 7.9193e-03 (1.3232e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [86][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.9885e-03 (1.3319e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [86][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.0925e-02 (1.3313e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [86][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.5007e-02 (1.3197e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [86][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 8.3694e-03 (1.3100e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [86][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2970e-02 (1.3329e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [86][180/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.0436e-02 (1.3330e-02)	Acc@1  98.44 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [86][190/391]	Time  0.037 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.1498e-03 (1.3152e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [86][200/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.6602e-02 (1.3143e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [86][210/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0361e-02 (1.3134e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [86][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.3234e-02 (1.3167e-02)	Acc@1  98.44 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [86][230/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2924e-02 (1.3154e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [86][240/391]	Time  0.028 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1505e-02 (1.3036e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [86][250/391]	Time  0.044 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.7482e-03 (1.3122e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [86][260/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.9083e-02 (1.3358e-02)	Acc@1  98.44 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [86][270/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.2299e-03 (1.3317e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [86][280/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.8387e-02 (1.3337e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [86][290/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.3869e-03 (1.3316e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [86][300/391]	Time  0.035 ( 0.033)	Data  0.001 ( 0.002)	Loss 8.1787e-03 (1.3372e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [86][310/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.4450e-02 (1.3430e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [86][320/391]	Time  0.035 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.3531e-03 (1.3379e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [86][330/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.2207e-02 (1.3365e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [86][340/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.0345e-02 (1.3237e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [86][350/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.8054e-03 (1.3364e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [86][360/391]	Time  0.039 ( 0.034)	Data  0.005 ( 0.002)	Loss 1.4313e-02 (1.3507e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [86][370/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 8.0109e-03 (1.3480e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [86][380/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 7.6599e-03 (1.3430e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [86][390/391]	Time  0.027 ( 0.033)	Data  0.000 ( 0.002)	Loss 3.3665e-03 (1.3460e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
## e[86] optimizer.zero_grad (sum) time: 0.1702730655670166
## e[86]       loss.backward (sum) time: 4.081364154815674
## e[86]      optimizer.step (sum) time: 1.203500747680664
## epoch[86] training(only) time: 13.180302858352661
# Switched to evaluate mode...
Test: [  0/100]	Time  0.172 ( 0.172)	Loss 2.1997e-01 (2.1997e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.015 ( 0.031)	Loss 2.0447e-01 (2.5451e-01)	Acc@1  92.00 ( 93.00)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.021 ( 0.025)	Loss 3.5791e-01 (2.7817e-01)	Acc@1  91.00 ( 92.95)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.014 ( 0.022)	Loss 1.9019e-01 (2.8631e-01)	Acc@1  95.00 ( 92.68)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.020 ( 0.021)	Loss 2.3315e-01 (2.8696e-01)	Acc@1  91.00 ( 92.59)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.013 ( 0.020)	Loss 1.3330e-01 (2.8554e-01)	Acc@1  95.00 ( 92.51)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.023 ( 0.020)	Loss 2.4255e-01 (2.7753e-01)	Acc@1  93.00 ( 92.70)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.015 ( 0.019)	Loss 4.4531e-01 (2.7145e-01)	Acc@1  90.00 ( 92.83)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.014 ( 0.019)	Loss 1.6064e-01 (2.7223e-01)	Acc@1  94.00 ( 92.84)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.014 ( 0.019)	Loss 2.0105e-01 (2.6892e-01)	Acc@1  93.00 ( 92.76)	Acc@5 100.00 ( 99.79)
 * Acc@1 92.810 Acc@5 99.800
### epoch[86] execution time: 15.143391609191895
EPOCH 87
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [87][  0/391]	Time  0.212 ( 0.212)	Data  0.177 ( 0.177)	Loss 1.8799e-02 (1.8799e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [87][ 10/391]	Time  0.036 ( 0.051)	Data  0.001 ( 0.018)	Loss 9.3307e-03 (1.1921e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [87][ 20/391]	Time  0.036 ( 0.043)	Data  0.001 ( 0.010)	Loss 3.6449e-03 (1.0046e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [87][ 30/391]	Time  0.032 ( 0.040)	Data  0.001 ( 0.007)	Loss 9.7504e-03 (1.1389e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [87][ 40/391]	Time  0.032 ( 0.038)	Data  0.001 ( 0.006)	Loss 2.2766e-02 (1.2017e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [87][ 50/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 8.2855e-03 (1.1824e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [87][ 60/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 9.5749e-03 (1.1642e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [87][ 70/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 1.0948e-02 (1.1832e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [87][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 9.3536e-03 (1.2090e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [87][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.8768e-02 (1.2954e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [87][100/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.1719e-02 (1.3241e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [87][110/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.5177e-02 (1.3277e-02)	Acc@1  98.44 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [87][120/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.2108e-02 (1.3195e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [87][130/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.2321e-02 (1.3348e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [87][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.6338e-03 (1.3032e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [87][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.2415e-02 (1.3168e-02)	Acc@1  98.44 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [87][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.7319e-02 (1.3202e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [87][170/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0658e-02 (1.3297e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [87][180/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.0411e-02 (1.3460e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [87][190/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2688e-02 (1.3466e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [87][200/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.6643e-03 (1.3525e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [87][210/391]	Time  0.038 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0536e-02 (1.3455e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [87][220/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.3994e-03 (1.3527e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [87][230/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.8370e-03 (1.3313e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [87][240/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.4550e-03 (1.3348e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [87][250/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0971e-02 (1.3422e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [87][260/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5289e-02 (1.3276e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [87][270/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4206e-02 (1.3137e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [87][280/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.7019e-03 (1.3124e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [87][290/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2703e-02 (1.3216e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [87][300/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3123e-02 (1.3322e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [87][310/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.8814e-02 (1.3405e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [87][320/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.1248e-03 (1.3522e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [87][330/391]	Time  0.042 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4946e-02 (1.3405e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [87][340/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.9814e-03 (1.3450e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [87][350/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3313e-02 (1.3405e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [87][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.7662e-03 (1.3435e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [87][370/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.3376e-02 (1.3644e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [87][380/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.1545e-02 (1.3698e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [87][390/391]	Time  0.022 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.4485e-03 (1.3663e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
## e[87] optimizer.zero_grad (sum) time: 0.16850996017456055
## e[87]       loss.backward (sum) time: 4.0229198932647705
## e[87]      optimizer.step (sum) time: 1.220008134841919
## epoch[87] training(only) time: 13.296342611312866
# Switched to evaluate mode...
Test: [  0/100]	Time  0.178 ( 0.178)	Loss 2.1948e-01 (2.1948e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.018 ( 0.032)	Loss 2.0850e-01 (2.5453e-01)	Acc@1  92.00 ( 92.73)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.013 ( 0.024)	Loss 3.4717e-01 (2.7715e-01)	Acc@1  92.00 ( 92.81)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.014 ( 0.022)	Loss 1.8591e-01 (2.8671e-01)	Acc@1  95.00 ( 92.45)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.023 ( 0.021)	Loss 2.3096e-01 (2.8695e-01)	Acc@1  90.00 ( 92.34)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.020 ( 0.020)	Loss 1.2720e-01 (2.8471e-01)	Acc@1  95.00 ( 92.29)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 2.4194e-01 (2.7639e-01)	Acc@1  93.00 ( 92.57)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.016 ( 0.019)	Loss 4.4238e-01 (2.6976e-01)	Acc@1  90.00 ( 92.72)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.014 ( 0.019)	Loss 1.5576e-01 (2.7029e-01)	Acc@1  94.00 ( 92.78)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.013 ( 0.019)	Loss 2.0276e-01 (2.6719e-01)	Acc@1  92.00 ( 92.71)	Acc@5 100.00 ( 99.79)
 * Acc@1 92.760 Acc@5 99.790
### epoch[87] execution time: 15.25740933418274
EPOCH 88
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [88][  0/391]	Time  0.216 ( 0.216)	Data  0.179 ( 0.179)	Loss 1.3489e-02 (1.3489e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [88][ 10/391]	Time  0.032 ( 0.050)	Data  0.001 ( 0.018)	Loss 2.1103e-02 (1.2019e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [88][ 20/391]	Time  0.032 ( 0.042)	Data  0.001 ( 0.010)	Loss 6.0463e-03 (9.6638e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [88][ 30/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.007)	Loss 2.3911e-02 (1.0751e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [88][ 40/391]	Time  0.032 ( 0.038)	Data  0.001 ( 0.006)	Loss 2.8782e-03 (1.0702e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [88][ 50/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 9.4452e-03 (1.0676e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [88][ 60/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 8.3542e-03 (1.0619e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [88][ 70/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.004)	Loss 1.7181e-02 (1.0974e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [88][ 80/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.004)	Loss 1.0445e-02 (1.1004e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [88][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 4.2076e-03 (1.1083e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [88][100/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.4216e-02 (1.1803e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [88][110/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.6174e-02 (1.1792e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [88][120/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 8.9569e-03 (1.1763e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [88][130/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.2705e-02 (1.1689e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [88][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.7059e-02 (1.2162e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [88][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.0597e-02 (1.2164e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [88][160/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.9302e-02 (1.2328e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [88][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4793e-02 (1.2367e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [88][180/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.6123e-02 (1.2305e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [88][190/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.6414e-03 (1.2510e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [88][200/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4290e-02 (1.2675e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [88][210/391]	Time  0.043 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.8982e-02 (1.2707e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [88][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.1250e-02 (1.3165e-02)	Acc@1  98.44 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [88][230/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.1698e-02 (1.3158e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [88][240/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1749e-02 (1.3071e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [88][250/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.2572e-02 (1.3421e-02)	Acc@1  98.44 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [88][260/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.6182e-03 (1.3287e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [88][270/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0406e-02 (1.3201e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [88][280/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.9049e-03 (1.3206e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [88][290/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.0027e-03 (1.3281e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [88][300/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2665e-02 (1.3165e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [88][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.8278e-03 (1.3216e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [88][320/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2131e-02 (1.3228e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [88][330/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.3091e-02 (1.3375e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [88][340/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.9193e-03 (1.3517e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [88][350/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.1757e-03 (1.3365e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [88][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.6693e-02 (1.3336e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [88][370/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5625e-02 (1.3442e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [88][380/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2703e-02 (1.3372e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [88][390/391]	Time  0.024 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.0279e-02 (1.3264e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
## e[88] optimizer.zero_grad (sum) time: 0.16995930671691895
## e[88]       loss.backward (sum) time: 4.008485317230225
## e[88]      optimizer.step (sum) time: 1.2265067100524902
## epoch[88] training(only) time: 13.232745885848999
# Switched to evaluate mode...
Test: [  0/100]	Time  0.173 ( 0.173)	Loss 2.2400e-01 (2.2400e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.019 ( 0.032)	Loss 2.1814e-01 (2.5799e-01)	Acc@1  92.00 ( 92.45)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.014 ( 0.025)	Loss 3.4839e-01 (2.7625e-01)	Acc@1  92.00 ( 92.57)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.022 ( 0.022)	Loss 1.9421e-01 (2.8640e-01)	Acc@1  95.00 ( 92.32)	Acc@5  99.00 ( 99.68)
Test: [ 40/100]	Time  0.014 ( 0.021)	Loss 2.2168e-01 (2.8451e-01)	Acc@1  92.00 ( 92.34)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.016 ( 0.020)	Loss 1.4026e-01 (2.8308e-01)	Acc@1  95.00 ( 92.29)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.015 ( 0.020)	Loss 2.4292e-01 (2.7553e-01)	Acc@1  93.00 ( 92.51)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.015 ( 0.019)	Loss 4.5483e-01 (2.6864e-01)	Acc@1  90.00 ( 92.66)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.7346e-01 (2.6956e-01)	Acc@1  93.00 ( 92.68)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.015 ( 0.019)	Loss 2.0044e-01 (2.6606e-01)	Acc@1  93.00 ( 92.64)	Acc@5 100.00 ( 99.77)
 * Acc@1 92.700 Acc@5 99.780
### epoch[88] execution time: 15.206717729568481
EPOCH 89
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [89][  0/391]	Time  0.210 ( 0.210)	Data  0.178 ( 0.178)	Loss 8.4610e-03 (8.4610e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [89][ 10/391]	Time  0.035 ( 0.050)	Data  0.001 ( 0.018)	Loss 7.8812e-03 (1.4581e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [89][ 20/391]	Time  0.032 ( 0.042)	Data  0.001 ( 0.010)	Loss 1.4931e-02 (1.2502e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [89][ 30/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.007)	Loss 1.6373e-02 (1.2685e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [89][ 40/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.006)	Loss 8.7204e-03 (1.2066e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [89][ 50/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 1.7807e-02 (1.2914e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [89][ 60/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 1.7456e-02 (1.3060e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [89][ 70/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 3.1921e-02 (1.3918e-02)	Acc@1  97.66 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [89][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 7.2289e-03 (1.3281e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [89][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.5793e-02 (1.3465e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [89][100/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.8799e-02 (1.3266e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [89][110/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.5259e-02 (1.3116e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [89][120/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.1027e-02 (1.3099e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [89][130/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.2970e-02 (1.3101e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [89][140/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.3924e-02 (1.2884e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [89][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.5116e-02 (1.2920e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [89][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.1177e-02 (1.2751e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [89][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.1760e-03 (1.2613e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [89][180/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.0103e-03 (1.2680e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [89][190/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.7809e-03 (1.2692e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [89][200/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.0692e-03 (1.2619e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [89][210/391]	Time  0.039 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.0386e-02 (1.2560e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [89][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.2385e-02 (1.2632e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [89][230/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.2136e-03 (1.2777e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [89][240/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1108e-02 (1.2967e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [89][250/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4946e-02 (1.2820e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [89][260/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.7847e-02 (1.2866e-02)	Acc@1  98.44 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [89][270/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.9253e-03 (1.2976e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [89][280/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3145e-02 (1.2958e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [89][290/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.1569e-03 (1.3086e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [89][300/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2077e-02 (1.3087e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [89][310/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.9028e-02 (1.3035e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [89][320/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.0349e-03 (1.2937e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [89][330/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.8376e-03 (1.2873e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [89][340/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4229e-02 (1.2858e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [89][350/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.2686e-03 (1.2876e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [89][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.6283e-03 (1.2841e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [89][370/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2291e-02 (1.2848e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [89][380/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.6120e-03 (1.2826e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [89][390/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.7817e-02 (1.2866e-02)	Acc@1  97.50 ( 99.75)	Acc@5 100.00 (100.00)
## e[89] optimizer.zero_grad (sum) time: 0.16990089416503906
## e[89]       loss.backward (sum) time: 4.074648857116699
## e[89]      optimizer.step (sum) time: 1.206718921661377
## epoch[89] training(only) time: 13.336844682693481
# Switched to evaluate mode...
Test: [  0/100]	Time  0.174 ( 0.174)	Loss 2.1790e-01 (2.1790e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.020 ( 0.031)	Loss 1.9348e-01 (2.5647e-01)	Acc@1  92.00 ( 92.91)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.014 ( 0.024)	Loss 3.4180e-01 (2.7777e-01)	Acc@1  91.00 ( 92.81)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.017 ( 0.021)	Loss 1.8506e-01 (2.8503e-01)	Acc@1  96.00 ( 92.65)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.013 ( 0.020)	Loss 2.5098e-01 (2.8661e-01)	Acc@1  89.00 ( 92.51)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.019 ( 0.019)	Loss 1.2189e-01 (2.8515e-01)	Acc@1  96.00 ( 92.45)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.014 ( 0.019)	Loss 2.5806e-01 (2.7705e-01)	Acc@1  92.00 ( 92.70)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.014 ( 0.019)	Loss 4.4409e-01 (2.7108e-01)	Acc@1  90.00 ( 92.82)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.026 ( 0.019)	Loss 1.6528e-01 (2.7244e-01)	Acc@1  93.00 ( 92.81)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.014 ( 0.019)	Loss 1.9336e-01 (2.6939e-01)	Acc@1  94.00 ( 92.76)	Acc@5 100.00 ( 99.79)
 * Acc@1 92.800 Acc@5 99.800
### epoch[89] execution time: 15.288659811019897
### Training complete:
#### total training(only) time: 1197.3154454231262
##### Total run time: 1378.483410358429
