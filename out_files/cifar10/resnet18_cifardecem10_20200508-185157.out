# Model: resnet18
# Dataset: cifardecem
# Batch size: 128
# Freezeout: False
# Low precision: False
# Epochs: 90
# Initial learning rate: 0.1
# module_name: models_cifardecem.resnet
<function resnet18 at 0x7fb0b022af28>
# model requested: 'resnet18'
# printing out the model
ResNet(
  (conv1): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (conv2_x): Sequential(
    (0): BasicBlock(
      (residual_function): Sequential(
        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
    (1): BasicBlock(
      (residual_function): Sequential(
        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
  )
  (conv3_x): Sequential(
    (0): BasicBlock(
      (residual_function): Sequential(
        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (residual_function): Sequential(
        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
  )
  (conv4_x): Sequential(
    (0): BasicBlock(
      (residual_function): Sequential(
        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (residual_function): Sequential(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
  )
  (conv5_x): Sequential(
    (0): BasicBlock(
      (residual_function): Sequential(
        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (residual_function): Sequential(
        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
  )
  (avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
# model is full precision
# Model: resnet18
# Dataset: cifardecem
# Freezeout: False
# Low precision: False
# Initial learning rate: 0.1
# Criterion: CrossEntropyLoss()
# Optimizer: SGD
#	lr 0.1
#	momentum 0.9
#	dampening 0
#	weight_decay 0.0001
#	nesterov False
CIFAR10 loading and normalization
==> Preparing data..
# CIFAR10 / full precision
Files already downloaded and verified
Files already downloaded and verified
#--> Training data: <--#
#-->>
EPOCH 0
# current learning rate: 0.1
# Switched to train mode...
Epoch: [0][  0/391]	Time  3.734 ( 3.734)	Data  0.110 ( 0.110)	Loss 2.3919e+00 (2.3919e+00)	Acc@1  14.06 ( 14.06)	Acc@5  47.66 ( 47.66)
Epoch: [0][ 10/391]	Time  0.037 ( 0.373)	Data  0.001 ( 0.011)	Loss 2.7226e+00 (3.4679e+00)	Acc@1  16.41 ( 13.78)	Acc@5  52.34 ( 56.18)
Epoch: [0][ 20/391]	Time  0.037 ( 0.213)	Data  0.001 ( 0.007)	Loss 2.8016e+00 (2.9911e+00)	Acc@1  16.41 ( 15.81)	Acc@5  69.53 ( 62.43)
Epoch: [0][ 30/391]	Time  0.038 ( 0.157)	Data  0.001 ( 0.005)	Loss 2.2559e+00 (2.7451e+00)	Acc@1  20.31 ( 18.20)	Acc@5  78.12 ( 66.66)
Epoch: [0][ 40/391]	Time  0.036 ( 0.128)	Data  0.001 ( 0.005)	Loss 2.3082e+00 (2.6116e+00)	Acc@1  21.88 ( 19.21)	Acc@5  77.34 ( 68.90)
Epoch: [0][ 50/391]	Time  0.037 ( 0.110)	Data  0.001 ( 0.004)	Loss 2.0080e+00 (2.5248e+00)	Acc@1  32.03 ( 19.55)	Acc@5  85.16 ( 70.57)
Epoch: [0][ 60/391]	Time  0.036 ( 0.098)	Data  0.001 ( 0.004)	Loss 2.0606e+00 (2.4545e+00)	Acc@1  21.88 ( 19.85)	Acc@5  85.16 ( 71.99)
Epoch: [0][ 70/391]	Time  0.038 ( 0.090)	Data  0.001 ( 0.004)	Loss 1.9351e+00 (2.3828e+00)	Acc@1  25.78 ( 20.76)	Acc@5  85.94 ( 73.33)
Epoch: [0][ 80/391]	Time  0.034 ( 0.083)	Data  0.001 ( 0.003)	Loss 1.8256e+00 (2.3235e+00)	Acc@1  30.47 ( 21.64)	Acc@5  83.59 ( 74.55)
Epoch: [0][ 90/391]	Time  0.035 ( 0.078)	Data  0.001 ( 0.003)	Loss 1.8134e+00 (2.2738e+00)	Acc@1  32.81 ( 22.52)	Acc@5  84.38 ( 75.47)
Epoch: [0][100/391]	Time  0.037 ( 0.074)	Data  0.001 ( 0.003)	Loss 1.8839e+00 (2.2304e+00)	Acc@1  32.03 ( 23.45)	Acc@5  79.69 ( 76.36)
Epoch: [0][110/391]	Time  0.037 ( 0.071)	Data  0.001 ( 0.003)	Loss 1.8718e+00 (2.1971e+00)	Acc@1  30.47 ( 24.07)	Acc@5  88.28 ( 77.14)
Epoch: [0][120/391]	Time  0.036 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.8941e+00 (2.1704e+00)	Acc@1  32.03 ( 24.61)	Acc@5  82.81 ( 77.66)
Epoch: [0][130/391]	Time  0.034 ( 0.066)	Data  0.002 ( 0.003)	Loss 1.6926e+00 (2.1427e+00)	Acc@1  33.59 ( 25.10)	Acc@5  88.28 ( 78.21)
Epoch: [0][140/391]	Time  0.038 ( 0.064)	Data  0.001 ( 0.003)	Loss 1.7378e+00 (2.1170e+00)	Acc@1  28.91 ( 25.61)	Acc@5  87.50 ( 78.75)
Epoch: [0][150/391]	Time  0.034 ( 0.062)	Data  0.001 ( 0.003)	Loss 1.7717e+00 (2.0964e+00)	Acc@1  26.56 ( 25.97)	Acc@5  84.38 ( 79.18)
Epoch: [0][160/391]	Time  0.038 ( 0.060)	Data  0.001 ( 0.003)	Loss 1.6814e+00 (2.0743e+00)	Acc@1  35.94 ( 26.29)	Acc@5  89.06 ( 79.76)
Epoch: [0][170/391]	Time  0.035 ( 0.059)	Data  0.001 ( 0.003)	Loss 1.7357e+00 (2.0552e+00)	Acc@1  39.84 ( 26.82)	Acc@5  89.06 ( 80.23)
Epoch: [0][180/391]	Time  0.037 ( 0.058)	Data  0.001 ( 0.003)	Loss 1.6838e+00 (2.0338e+00)	Acc@1  33.59 ( 27.46)	Acc@5  93.75 ( 80.70)
Epoch: [0][190/391]	Time  0.036 ( 0.057)	Data  0.001 ( 0.003)	Loss 1.7895e+00 (2.0216e+00)	Acc@1  35.94 ( 27.69)	Acc@5  85.94 ( 80.97)
Epoch: [0][200/391]	Time  0.036 ( 0.056)	Data  0.001 ( 0.003)	Loss 1.8482e+00 (2.0086e+00)	Acc@1  26.56 ( 28.04)	Acc@5  84.38 ( 81.29)
Epoch: [0][210/391]	Time  0.036 ( 0.055)	Data  0.001 ( 0.003)	Loss 1.5775e+00 (1.9942e+00)	Acc@1  40.62 ( 28.48)	Acc@5  92.19 ( 81.62)
Epoch: [0][220/391]	Time  0.039 ( 0.054)	Data  0.001 ( 0.003)	Loss 1.6295e+00 (1.9801e+00)	Acc@1  33.59 ( 28.80)	Acc@5  91.41 ( 81.96)
Epoch: [0][230/391]	Time  0.035 ( 0.053)	Data  0.001 ( 0.003)	Loss 1.8535e+00 (1.9685e+00)	Acc@1  28.91 ( 29.13)	Acc@5  84.38 ( 82.25)
Epoch: [0][240/391]	Time  0.035 ( 0.053)	Data  0.002 ( 0.003)	Loss 1.6016e+00 (1.9552e+00)	Acc@1  34.38 ( 29.51)	Acc@5  92.19 ( 82.50)
Epoch: [0][250/391]	Time  0.037 ( 0.052)	Data  0.002 ( 0.003)	Loss 1.6804e+00 (1.9433e+00)	Acc@1  37.50 ( 29.82)	Acc@5  88.28 ( 82.77)
Epoch: [0][260/391]	Time  0.037 ( 0.051)	Data  0.001 ( 0.003)	Loss 1.6808e+00 (1.9336e+00)	Acc@1  38.28 ( 30.12)	Acc@5  86.72 ( 82.97)
Epoch: [0][270/391]	Time  0.036 ( 0.051)	Data  0.001 ( 0.003)	Loss 1.8020e+00 (1.9225e+00)	Acc@1  31.25 ( 30.53)	Acc@5  88.28 ( 83.25)
Epoch: [0][280/391]	Time  0.035 ( 0.050)	Data  0.001 ( 0.003)	Loss 1.5588e+00 (1.9115e+00)	Acc@1  45.31 ( 30.87)	Acc@5  91.41 ( 83.46)
Epoch: [0][290/391]	Time  0.039 ( 0.050)	Data  0.004 ( 0.003)	Loss 1.6635e+00 (1.9008e+00)	Acc@1  37.50 ( 31.16)	Acc@5  86.72 ( 83.70)
Epoch: [0][300/391]	Time  0.036 ( 0.050)	Data  0.001 ( 0.003)	Loss 1.5975e+00 (1.8900e+00)	Acc@1  36.72 ( 31.49)	Acc@5  86.72 ( 83.90)
Epoch: [0][310/391]	Time  0.035 ( 0.049)	Data  0.001 ( 0.003)	Loss 1.6892e+00 (1.8802e+00)	Acc@1  38.28 ( 31.81)	Acc@5  89.84 ( 84.07)
Epoch: [0][320/391]	Time  0.036 ( 0.049)	Data  0.001 ( 0.003)	Loss 1.5879e+00 (1.8722e+00)	Acc@1  34.38 ( 32.00)	Acc@5  88.28 ( 84.24)
Epoch: [0][330/391]	Time  0.038 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.6062e+00 (1.8643e+00)	Acc@1  37.50 ( 32.23)	Acc@5  92.19 ( 84.43)
Epoch: [0][340/391]	Time  0.038 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.7732e+00 (1.8555e+00)	Acc@1  35.16 ( 32.52)	Acc@5  85.94 ( 84.58)
Epoch: [0][350/391]	Time  0.036 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.3744e+00 (1.8467e+00)	Acc@1  46.09 ( 32.79)	Acc@5  94.53 ( 84.76)
Epoch: [0][360/391]	Time  0.035 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.3931e+00 (1.8378e+00)	Acc@1  46.09 ( 33.12)	Acc@5  93.75 ( 84.93)
Epoch: [0][370/391]	Time  0.037 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4914e+00 (1.8291e+00)	Acc@1  40.62 ( 33.35)	Acc@5  92.19 ( 85.11)
Epoch: [0][380/391]	Time  0.037 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4915e+00 (1.8209e+00)	Acc@1  42.19 ( 33.66)	Acc@5  93.75 ( 85.23)
Epoch: [0][390/391]	Time  0.307 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.7081e+00 (1.8122e+00)	Acc@1  36.25 ( 33.97)	Acc@5  85.00 ( 85.38)
## e[0] optimizer.zero_grad (sum) time: 0.1804335117340088
## e[0]       loss.backward (sum) time: 3.361433744430542
## e[0]      optimizer.step (sum) time: 1.0638587474822998
## epoch[0] training(only) time: 18.66057300567627
# Switched to evaluate mode...
Test: [  0/100]	Time  0.266 ( 0.266)	Loss 1.4045e+00 (1.4045e+00)	Acc@1  47.00 ( 47.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.017 ( 0.042)	Loss 1.4579e+00 (1.4719e+00)	Acc@1  45.00 ( 45.18)	Acc@5  94.00 ( 93.45)
Test: [ 20/100]	Time  0.016 ( 0.031)	Loss 1.3406e+00 (1.4806e+00)	Acc@1  48.00 ( 43.95)	Acc@5  93.00 ( 93.10)
Test: [ 30/100]	Time  0.021 ( 0.027)	Loss 1.3456e+00 (1.4899e+00)	Acc@1  46.00 ( 43.26)	Acc@5  93.00 ( 92.84)
Test: [ 40/100]	Time  0.015 ( 0.024)	Loss 1.4780e+00 (1.4884e+00)	Acc@1  44.00 ( 43.59)	Acc@5  94.00 ( 92.63)
Test: [ 50/100]	Time  0.016 ( 0.023)	Loss 1.4724e+00 (1.4808e+00)	Acc@1  45.00 ( 43.94)	Acc@5  94.00 ( 92.78)
Test: [ 60/100]	Time  0.017 ( 0.022)	Loss 1.4187e+00 (1.4817e+00)	Acc@1  44.00 ( 44.11)	Acc@5  91.00 ( 92.79)
Test: [ 70/100]	Time  0.015 ( 0.022)	Loss 1.5149e+00 (1.4854e+00)	Acc@1  42.00 ( 43.96)	Acc@5  91.00 ( 92.72)
Test: [ 80/100]	Time  0.016 ( 0.021)	Loss 1.3067e+00 (1.4756e+00)	Acc@1  50.00 ( 44.17)	Acc@5  92.00 ( 92.77)
Test: [ 90/100]	Time  0.016 ( 0.021)	Loss 1.4009e+00 (1.4809e+00)	Acc@1  44.00 ( 43.92)	Acc@5  94.00 ( 92.58)
 * Acc@1 43.990 Acc@5 92.560
### epoch[0] execution time: 20.87337636947632
EPOCH 1
# current learning rate: 0.1
# Switched to train mode...
Epoch: [1][  0/391]	Time  0.219 ( 0.219)	Data  0.179 ( 0.179)	Loss 1.5736e+00 (1.5736e+00)	Acc@1  46.09 ( 46.09)	Acc@5  89.06 ( 89.06)
Epoch: [1][ 10/391]	Time  0.039 ( 0.054)	Data  0.001 ( 0.018)	Loss 1.5680e+00 (1.5021e+00)	Acc@1  45.31 ( 44.96)	Acc@5  89.84 ( 91.41)
Epoch: [1][ 20/391]	Time  0.036 ( 0.046)	Data  0.001 ( 0.010)	Loss 1.4178e+00 (1.4351e+00)	Acc@1  46.88 ( 46.91)	Acc@5  93.75 ( 92.78)
Epoch: [1][ 30/391]	Time  0.036 ( 0.043)	Data  0.001 ( 0.008)	Loss 1.5218e+00 (1.4669e+00)	Acc@1  42.97 ( 45.49)	Acc@5  89.06 ( 92.39)
Epoch: [1][ 40/391]	Time  0.049 ( 0.042)	Data  0.001 ( 0.006)	Loss 1.3948e+00 (1.4647e+00)	Acc@1  50.78 ( 45.71)	Acc@5  92.97 ( 92.44)
Epoch: [1][ 50/391]	Time  0.034 ( 0.041)	Data  0.001 ( 0.006)	Loss 1.4440e+00 (1.4750e+00)	Acc@1  46.09 ( 45.48)	Acc@5  93.75 ( 92.26)
Epoch: [1][ 60/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.005)	Loss 1.2517e+00 (1.4656e+00)	Acc@1  50.78 ( 45.65)	Acc@5  95.31 ( 92.41)
Epoch: [1][ 70/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.005)	Loss 1.2258e+00 (1.4596e+00)	Acc@1  50.78 ( 45.88)	Acc@5  96.88 ( 92.69)
Epoch: [1][ 80/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.4165e+00 (1.4560e+00)	Acc@1  52.34 ( 46.17)	Acc@5  89.84 ( 92.66)
Epoch: [1][ 90/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.5770e+00 (1.4558e+00)	Acc@1  39.84 ( 46.27)	Acc@5  89.06 ( 92.63)
Epoch: [1][100/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.3625e+00 (1.4489e+00)	Acc@1  50.00 ( 46.58)	Acc@5  92.19 ( 92.63)
Epoch: [1][110/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.2154e+00 (1.4421e+00)	Acc@1  59.38 ( 46.85)	Acc@5  95.31 ( 92.69)
Epoch: [1][120/391]	Time  0.035 ( 0.039)	Data  0.002 ( 0.004)	Loss 1.4057e+00 (1.4343e+00)	Acc@1  50.78 ( 47.16)	Acc@5  93.75 ( 92.78)
Epoch: [1][130/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.3152e+00 (1.4269e+00)	Acc@1  51.56 ( 47.51)	Acc@5  95.31 ( 92.86)
Epoch: [1][140/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.4696e+00 (1.4222e+00)	Acc@1  45.31 ( 47.67)	Acc@5  89.06 ( 92.84)
Epoch: [1][150/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.3248e+00 (1.4200e+00)	Acc@1  48.44 ( 47.87)	Acc@5  93.75 ( 92.85)
Epoch: [1][160/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.2804e+00 (1.4136e+00)	Acc@1  57.03 ( 48.16)	Acc@5  93.75 ( 92.92)
Epoch: [1][170/391]	Time  0.040 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.3917e+00 (1.4064e+00)	Acc@1  50.78 ( 48.57)	Acc@5  92.19 ( 92.94)
Epoch: [1][180/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.2095e+00 (1.4015e+00)	Acc@1  61.72 ( 48.83)	Acc@5  93.75 ( 93.02)
Epoch: [1][190/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.3640e+00 (1.3985e+00)	Acc@1  50.00 ( 48.92)	Acc@5  93.75 ( 93.05)
Epoch: [1][200/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.4082e+00 (1.3951e+00)	Acc@1  50.78 ( 49.09)	Acc@5  89.84 ( 93.06)
Epoch: [1][210/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.1757e+00 (1.3898e+00)	Acc@1  54.69 ( 49.33)	Acc@5  94.53 ( 93.02)
Epoch: [1][220/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.3148e+00 (1.3827e+00)	Acc@1  55.47 ( 49.57)	Acc@5  94.53 ( 93.14)
Epoch: [1][230/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.3207e+00 (1.3781e+00)	Acc@1  46.88 ( 49.64)	Acc@5  96.09 ( 93.18)
Epoch: [1][240/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.1052e+00 (1.3730e+00)	Acc@1  60.16 ( 49.86)	Acc@5  94.53 ( 93.20)
Epoch: [1][250/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.1578e+00 (1.3687e+00)	Acc@1  57.81 ( 50.09)	Acc@5  96.09 ( 93.23)
Epoch: [1][260/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.3779e+00 (1.3646e+00)	Acc@1  45.31 ( 50.26)	Acc@5  96.88 ( 93.28)
Epoch: [1][270/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.2313e+00 (1.3603e+00)	Acc@1  53.91 ( 50.39)	Acc@5  98.44 ( 93.34)
Epoch: [1][280/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.2218e+00 (1.3550e+00)	Acc@1  57.03 ( 50.63)	Acc@5  94.53 ( 93.40)
Epoch: [1][290/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.2222e+00 (1.3516e+00)	Acc@1  53.91 ( 50.78)	Acc@5  96.88 ( 93.46)
Epoch: [1][300/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.2026e+00 (1.3456e+00)	Acc@1  57.81 ( 50.96)	Acc@5  92.97 ( 93.54)
Epoch: [1][310/391]	Time  0.041 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.2084e+00 (1.3398e+00)	Acc@1  57.03 ( 51.16)	Acc@5  97.66 ( 93.62)
Epoch: [1][320/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.1208e+00 (1.3367e+00)	Acc@1  57.81 ( 51.26)	Acc@5  97.66 ( 93.65)
Epoch: [1][330/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.2548e+00 (1.3355e+00)	Acc@1  51.56 ( 51.34)	Acc@5  96.88 ( 93.66)
Epoch: [1][340/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.3500e+00 (1.3329e+00)	Acc@1  50.00 ( 51.44)	Acc@5  90.62 ( 93.65)
Epoch: [1][350/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.2839e+00 (1.3293e+00)	Acc@1  55.47 ( 51.56)	Acc@5  95.31 ( 93.70)
Epoch: [1][360/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.0694e+00 (1.3254e+00)	Acc@1  58.59 ( 51.71)	Acc@5  95.31 ( 93.73)
Epoch: [1][370/391]	Time  0.041 ( 0.038)	Data  0.002 ( 0.003)	Loss 1.0580e+00 (1.3230e+00)	Acc@1  64.84 ( 51.86)	Acc@5  98.44 ( 93.76)
Epoch: [1][380/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.1104e+00 (1.3191e+00)	Acc@1  63.28 ( 52.05)	Acc@5  92.19 ( 93.78)
Epoch: [1][390/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.2913e+00 (1.3159e+00)	Acc@1  53.75 ( 52.20)	Acc@5  98.75 ( 93.81)
## e[1] optimizer.zero_grad (sum) time: 0.18225765228271484
## e[1]       loss.backward (sum) time: 2.9882380962371826
## e[1]      optimizer.step (sum) time: 1.0431411266326904
## epoch[1] training(only) time: 15.00204348564148
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 1.0599e+00 (1.0599e+00)	Acc@1  63.00 ( 63.00)	Acc@5  97.00 ( 97.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.1405e+00 (1.1691e+00)	Acc@1  62.00 ( 57.45)	Acc@5  96.00 ( 96.55)
Test: [ 20/100]	Time  0.016 ( 0.028)	Loss 1.2256e+00 (1.1936e+00)	Acc@1  51.00 ( 56.52)	Acc@5  95.00 ( 95.57)
Test: [ 30/100]	Time  0.020 ( 0.025)	Loss 1.1253e+00 (1.1969e+00)	Acc@1  66.00 ( 56.90)	Acc@5  91.00 ( 95.23)
Test: [ 40/100]	Time  0.017 ( 0.023)	Loss 1.2034e+00 (1.1993e+00)	Acc@1  59.00 ( 57.12)	Acc@5  94.00 ( 95.07)
Test: [ 50/100]	Time  0.023 ( 0.022)	Loss 1.1812e+00 (1.1896e+00)	Acc@1  63.00 ( 57.49)	Acc@5  95.00 ( 95.22)
Test: [ 60/100]	Time  0.016 ( 0.022)	Loss 1.2642e+00 (1.1945e+00)	Acc@1  54.00 ( 57.25)	Acc@5  95.00 ( 95.11)
Test: [ 70/100]	Time  0.016 ( 0.021)	Loss 1.1452e+00 (1.1958e+00)	Acc@1  59.00 ( 57.17)	Acc@5  95.00 ( 95.13)
Test: [ 80/100]	Time  0.017 ( 0.021)	Loss 1.0187e+00 (1.1890e+00)	Acc@1  62.00 ( 57.48)	Acc@5  96.00 ( 95.27)
Test: [ 90/100]	Time  0.023 ( 0.021)	Loss 1.0561e+00 (1.1921e+00)	Acc@1  59.00 ( 57.27)	Acc@5  98.00 ( 95.33)
 * Acc@1 57.150 Acc@5 95.310
### epoch[1] execution time: 17.1894474029541
EPOCH 2
# current learning rate: 0.1
# Switched to train mode...
Epoch: [2][  0/391]	Time  0.202 ( 0.202)	Data  0.171 ( 0.171)	Loss 1.1252e+00 (1.1252e+00)	Acc@1  62.50 ( 62.50)	Acc@5  96.09 ( 96.09)
Epoch: [2][ 10/391]	Time  0.037 ( 0.053)	Data  0.001 ( 0.018)	Loss 1.2323e+00 (1.1752e+00)	Acc@1  53.12 ( 59.09)	Acc@5  94.53 ( 95.45)
Epoch: [2][ 20/391]	Time  0.034 ( 0.045)	Data  0.001 ( 0.010)	Loss 1.1384e+00 (1.1459e+00)	Acc@1  57.81 ( 59.34)	Acc@5  96.88 ( 95.87)
Epoch: [2][ 30/391]	Time  0.037 ( 0.043)	Data  0.001 ( 0.008)	Loss 1.1201e+00 (1.1388e+00)	Acc@1  60.16 ( 59.20)	Acc@5  96.88 ( 96.12)
Epoch: [2][ 40/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.006)	Loss 1.1060e+00 (1.1439e+00)	Acc@1  57.81 ( 58.75)	Acc@5  97.66 ( 95.90)
Epoch: [2][ 50/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.005)	Loss 1.0235e+00 (1.1397e+00)	Acc@1  62.50 ( 59.44)	Acc@5  96.88 ( 95.73)
Epoch: [2][ 60/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.005)	Loss 1.1588e+00 (1.1409e+00)	Acc@1  57.03 ( 59.23)	Acc@5  94.53 ( 95.77)
Epoch: [2][ 70/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.005)	Loss 1.0813e+00 (1.1257e+00)	Acc@1  55.47 ( 59.69)	Acc@5  99.22 ( 95.79)
Epoch: [2][ 80/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.1396e+00 (1.1225e+00)	Acc@1  63.28 ( 59.67)	Acc@5  95.31 ( 95.85)
Epoch: [2][ 90/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.0956e+00 (1.1194e+00)	Acc@1  62.50 ( 59.72)	Acc@5  97.66 ( 95.96)
Epoch: [2][100/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.0498e+00 (1.1110e+00)	Acc@1  60.94 ( 59.99)	Acc@5  97.66 ( 96.07)
Epoch: [2][110/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.0188e+00 (1.1087e+00)	Acc@1  64.06 ( 60.24)	Acc@5  95.31 ( 96.03)
Epoch: [2][120/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.2803e+00 (1.1044e+00)	Acc@1  57.03 ( 60.38)	Acc@5  95.31 ( 96.12)
Epoch: [2][130/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.0230e+00 (1.1010e+00)	Acc@1  64.06 ( 60.51)	Acc@5  97.66 ( 96.11)
Epoch: [2][140/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 9.2811e-01 (1.0953e+00)	Acc@1  66.41 ( 60.76)	Acc@5  96.88 ( 96.15)
Epoch: [2][150/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.0066e+00 (1.0979e+00)	Acc@1  59.38 ( 60.57)	Acc@5  98.44 ( 96.17)
Epoch: [2][160/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.1480e+00 (1.0957e+00)	Acc@1  60.16 ( 60.68)	Acc@5  96.09 ( 96.12)
Epoch: [2][170/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 9.8021e-01 (1.0900e+00)	Acc@1  64.06 ( 60.89)	Acc@5  97.66 ( 96.17)
Epoch: [2][180/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 9.4552e-01 (1.0862e+00)	Acc@1  65.62 ( 61.05)	Acc@5  97.66 ( 96.18)
Epoch: [2][190/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.1454e+00 (1.0902e+00)	Acc@1  60.16 ( 60.97)	Acc@5  97.66 ( 96.20)
Epoch: [2][200/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.1736e-01 (1.0874e+00)	Acc@1  70.31 ( 61.05)	Acc@5  97.66 ( 96.22)
Epoch: [2][210/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.2786e+00 (1.0851e+00)	Acc@1  54.69 ( 61.15)	Acc@5  96.88 ( 96.25)
Epoch: [2][220/391]	Time  0.042 ( 0.038)	Data  0.005 ( 0.003)	Loss 9.5754e-01 (1.0832e+00)	Acc@1  65.62 ( 61.18)	Acc@5  99.22 ( 96.23)
Epoch: [2][230/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.0487e+00 (1.0787e+00)	Acc@1  67.97 ( 61.32)	Acc@5  95.31 ( 96.27)
Epoch: [2][240/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.0160e+00 (1.0768e+00)	Acc@1  65.62 ( 61.34)	Acc@5  94.53 ( 96.28)
Epoch: [2][250/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 9.9006e-01 (1.0743e+00)	Acc@1  60.16 ( 61.43)	Acc@5  96.09 ( 96.27)
Epoch: [2][260/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.0906e+00 (1.0710e+00)	Acc@1  61.72 ( 61.62)	Acc@5  97.66 ( 96.30)
Epoch: [2][270/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.1710e+00 (1.0708e+00)	Acc@1  57.81 ( 61.68)	Acc@5  92.97 ( 96.28)
Epoch: [2][280/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.9863e-01 (1.0690e+00)	Acc@1  72.66 ( 61.81)	Acc@5  97.66 ( 96.29)
Epoch: [2][290/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.0242e+00 (1.0674e+00)	Acc@1  64.84 ( 61.88)	Acc@5  96.09 ( 96.28)
Epoch: [2][300/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 9.5877e-01 (1.0641e+00)	Acc@1  62.50 ( 62.01)	Acc@5  98.44 ( 96.31)
Epoch: [2][310/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.0573e+00 (1.0614e+00)	Acc@1  65.62 ( 62.11)	Acc@5  98.44 ( 96.33)
Epoch: [2][320/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.8860e-01 (1.0573e+00)	Acc@1  67.97 ( 62.25)	Acc@5  99.22 ( 96.35)
Epoch: [2][330/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.0188e+00 (1.0541e+00)	Acc@1  64.84 ( 62.37)	Acc@5  98.44 ( 96.37)
Epoch: [2][340/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.9789e-01 (1.0520e+00)	Acc@1  71.09 ( 62.50)	Acc@5  97.66 ( 96.38)
Epoch: [2][350/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.9083e-01 (1.0490e+00)	Acc@1  65.62 ( 62.62)	Acc@5  96.88 ( 96.41)
Epoch: [2][360/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.0037e+00 (1.0462e+00)	Acc@1  62.50 ( 62.75)	Acc@5  96.88 ( 96.43)
Epoch: [2][370/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.2053e-01 (1.0435e+00)	Acc@1  73.44 ( 62.85)	Acc@5  96.88 ( 96.44)
Epoch: [2][380/391]	Time  0.042 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.8655e-01 (1.0402e+00)	Acc@1  66.41 ( 62.96)	Acc@5  97.66 ( 96.46)
Epoch: [2][390/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.1613e+00 (1.0373e+00)	Acc@1  62.50 ( 63.08)	Acc@5  96.25 ( 96.47)
## e[2] optimizer.zero_grad (sum) time: 0.17636775970458984
## e[2]       loss.backward (sum) time: 2.961200714111328
## e[2]      optimizer.step (sum) time: 1.050626277923584
## epoch[2] training(only) time: 15.000736713409424
# Switched to evaluate mode...
Test: [  0/100]	Time  0.192 ( 0.192)	Loss 9.3002e-01 (9.3002e-01)	Acc@1  61.00 ( 61.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.019 ( 0.035)	Loss 8.2429e-01 (9.7336e-01)	Acc@1  69.00 ( 64.00)	Acc@5  99.00 ( 98.00)
Test: [ 20/100]	Time  0.024 ( 0.027)	Loss 9.9981e-01 (9.9665e-01)	Acc@1  59.00 ( 63.76)	Acc@5  98.00 ( 97.52)
Test: [ 30/100]	Time  0.016 ( 0.024)	Loss 9.2364e-01 (1.0101e+00)	Acc@1  65.00 ( 63.87)	Acc@5  99.00 ( 97.35)
Test: [ 40/100]	Time  0.019 ( 0.023)	Loss 9.6957e-01 (1.0166e+00)	Acc@1  65.00 ( 63.59)	Acc@5  99.00 ( 97.20)
Test: [ 50/100]	Time  0.016 ( 0.022)	Loss 9.0938e-01 (9.9991e-01)	Acc@1  70.00 ( 64.08)	Acc@5  98.00 ( 97.31)
Test: [ 60/100]	Time  0.017 ( 0.022)	Loss 9.1665e-01 (1.0004e+00)	Acc@1  66.00 ( 64.08)	Acc@5  99.00 ( 97.38)
Test: [ 70/100]	Time  0.022 ( 0.022)	Loss 1.0888e+00 (9.9686e-01)	Acc@1  60.00 ( 64.41)	Acc@5  97.00 ( 97.41)
Test: [ 80/100]	Time  0.018 ( 0.021)	Loss 8.6836e-01 (9.9740e-01)	Acc@1  66.00 ( 64.62)	Acc@5  96.00 ( 97.38)
Test: [ 90/100]	Time  0.016 ( 0.021)	Loss 8.4420e-01 (9.9932e-01)	Acc@1  74.00 ( 64.64)	Acc@5  99.00 ( 97.27)
 * Acc@1 64.710 Acc@5 97.290
### epoch[2] execution time: 17.19537091255188
EPOCH 3
# current learning rate: 0.1
# Switched to train mode...
Epoch: [3][  0/391]	Time  0.214 ( 0.214)	Data  0.179 ( 0.179)	Loss 8.8187e-01 (8.8187e-01)	Acc@1  68.75 ( 68.75)	Acc@5  94.53 ( 94.53)
Epoch: [3][ 10/391]	Time  0.036 ( 0.053)	Data  0.001 ( 0.018)	Loss 9.0508e-01 (8.8582e-01)	Acc@1  73.44 ( 68.82)	Acc@5  97.66 ( 97.23)
Epoch: [3][ 20/391]	Time  0.037 ( 0.046)	Data  0.001 ( 0.010)	Loss 9.2824e-01 (8.9509e-01)	Acc@1  69.53 ( 68.68)	Acc@5  96.88 ( 97.77)
Epoch: [3][ 30/391]	Time  0.036 ( 0.043)	Data  0.001 ( 0.008)	Loss 1.0611e+00 (8.9818e-01)	Acc@1  64.06 ( 68.42)	Acc@5  96.88 ( 97.66)
Epoch: [3][ 40/391]	Time  0.036 ( 0.042)	Data  0.002 ( 0.006)	Loss 9.2515e-01 (8.9189e-01)	Acc@1  72.66 ( 69.04)	Acc@5  96.09 ( 97.56)
Epoch: [3][ 50/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.006)	Loss 8.6977e-01 (8.9665e-01)	Acc@1  71.88 ( 68.96)	Acc@5  98.44 ( 97.46)
Epoch: [3][ 60/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.005)	Loss 8.2035e-01 (9.0431e-01)	Acc@1  68.75 ( 68.40)	Acc@5  96.09 ( 97.32)
Epoch: [3][ 70/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.005)	Loss 8.4424e-01 (9.0326e-01)	Acc@1  65.62 ( 68.31)	Acc@5  98.44 ( 97.34)
Epoch: [3][ 80/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.004)	Loss 9.2394e-01 (8.9801e-01)	Acc@1  67.97 ( 68.40)	Acc@5  92.97 ( 97.32)
Epoch: [3][ 90/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 8.3248e-01 (8.9774e-01)	Acc@1  71.88 ( 68.39)	Acc@5  98.44 ( 97.33)
Epoch: [3][100/391]	Time  0.037 ( 0.039)	Data  0.002 ( 0.004)	Loss 8.6790e-01 (8.9578e-01)	Acc@1  66.41 ( 68.43)	Acc@5  98.44 ( 97.33)
Epoch: [3][110/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 9.7118e-01 (9.0303e-01)	Acc@1  62.50 ( 68.26)	Acc@5  99.22 ( 97.33)
Epoch: [3][120/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 8.7309e-01 (9.0248e-01)	Acc@1  64.06 ( 68.25)	Acc@5  99.22 ( 97.33)
Epoch: [3][130/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 8.6486e-01 (9.0149e-01)	Acc@1  71.09 ( 68.31)	Acc@5  96.88 ( 97.31)
Epoch: [3][140/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 9.7569e-01 (8.9626e-01)	Acc@1  69.53 ( 68.56)	Acc@5  94.53 ( 97.36)
Epoch: [3][150/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.9362e-01 (8.9355e-01)	Acc@1  71.09 ( 68.68)	Acc@5  96.09 ( 97.32)
Epoch: [3][160/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.1117e-01 (8.8825e-01)	Acc@1  71.09 ( 68.85)	Acc@5  96.09 ( 97.35)
Epoch: [3][170/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 9.0226e-01 (8.8780e-01)	Acc@1  67.19 ( 68.84)	Acc@5  95.31 ( 97.36)
Epoch: [3][180/391]	Time  0.038 ( 0.039)	Data  0.002 ( 0.003)	Loss 8.3576e-01 (8.8790e-01)	Acc@1  70.31 ( 68.83)	Acc@5  97.66 ( 97.35)
Epoch: [3][190/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.2825e-01 (8.8710e-01)	Acc@1  74.22 ( 68.85)	Acc@5  96.88 ( 97.36)
Epoch: [3][200/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.2891e-01 (8.8630e-01)	Acc@1  75.78 ( 68.82)	Acc@5 100.00 ( 97.40)
Epoch: [3][210/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.5839e-01 (8.8376e-01)	Acc@1  71.88 ( 68.88)	Acc@5  98.44 ( 97.43)
Epoch: [3][220/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 9.7547e-01 (8.8089e-01)	Acc@1  64.84 ( 68.97)	Acc@5  96.88 ( 97.44)
Epoch: [3][230/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 9.7077e-01 (8.7981e-01)	Acc@1  62.50 ( 69.01)	Acc@5  97.66 ( 97.46)
Epoch: [3][240/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.4064e-01 (8.7709e-01)	Acc@1  72.66 ( 69.04)	Acc@5  98.44 ( 97.46)
Epoch: [3][250/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.5580e-01 (8.7495e-01)	Acc@1  73.44 ( 69.18)	Acc@5  98.44 ( 97.47)
Epoch: [3][260/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 9.5149e-01 (8.7173e-01)	Acc@1  66.41 ( 69.28)	Acc@5  98.44 ( 97.51)
Epoch: [3][270/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.1675e+00 (8.7196e-01)	Acc@1  57.81 ( 69.29)	Acc@5  96.09 ( 97.49)
Epoch: [3][280/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.7133e-01 (8.7108e-01)	Acc@1  71.09 ( 69.36)	Acc@5  98.44 ( 97.50)
Epoch: [3][290/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 9.9376e-01 (8.6997e-01)	Acc@1  66.41 ( 69.43)	Acc@5  96.88 ( 97.53)
Epoch: [3][300/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.7541e-01 (8.6717e-01)	Acc@1  72.66 ( 69.52)	Acc@5  98.44 ( 97.54)
Epoch: [3][310/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.6548e-01 (8.6665e-01)	Acc@1  67.19 ( 69.55)	Acc@5  96.88 ( 97.55)
Epoch: [3][320/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.9571e-01 (8.6387e-01)	Acc@1  69.53 ( 69.62)	Acc@5  98.44 ( 97.57)
Epoch: [3][330/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.6730e-01 (8.6357e-01)	Acc@1  71.88 ( 69.62)	Acc@5  96.88 ( 97.58)
Epoch: [3][340/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.5377e-01 (8.6195e-01)	Acc@1  76.56 ( 69.66)	Acc@5  99.22 ( 97.59)
Epoch: [3][350/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.3794e-01 (8.6173e-01)	Acc@1  71.88 ( 69.68)	Acc@5 100.00 ( 97.62)
Epoch: [3][360/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.1496e-01 (8.5963e-01)	Acc@1  73.44 ( 69.74)	Acc@5  97.66 ( 97.63)
Epoch: [3][370/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.9972e-01 (8.5710e-01)	Acc@1  75.00 ( 69.83)	Acc@5  98.44 ( 97.65)
Epoch: [3][380/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 9.5597e-01 (8.5580e-01)	Acc@1  65.62 ( 69.86)	Acc@5  96.09 ( 97.66)
Epoch: [3][390/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.1361e+00 (8.5409e-01)	Acc@1  66.25 ( 69.94)	Acc@5  96.25 ( 97.68)
## e[3] optimizer.zero_grad (sum) time: 0.17504501342773438
## e[3]       loss.backward (sum) time: 2.993642807006836
## e[3]      optimizer.step (sum) time: 1.0367484092712402
## epoch[3] training(only) time: 15.051979064941406
# Switched to evaluate mode...
Test: [  0/100]	Time  0.181 ( 0.181)	Loss 6.5409e-01 (6.5409e-01)	Acc@1  75.00 ( 75.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.016 ( 0.032)	Loss 8.9526e-01 (8.6142e-01)	Acc@1  66.00 ( 70.55)	Acc@5  99.00 ( 98.09)
Test: [ 20/100]	Time  0.020 ( 0.026)	Loss 9.2148e-01 (8.6408e-01)	Acc@1  62.00 ( 70.86)	Acc@5  98.00 ( 97.95)
Test: [ 30/100]	Time  0.025 ( 0.024)	Loss 8.0053e-01 (8.6647e-01)	Acc@1  73.00 ( 70.71)	Acc@5  97.00 ( 97.71)
Test: [ 40/100]	Time  0.019 ( 0.023)	Loss 7.3637e-01 (8.6418e-01)	Acc@1  75.00 ( 70.95)	Acc@5 100.00 ( 97.73)
Test: [ 50/100]	Time  0.016 ( 0.022)	Loss 7.7932e-01 (8.5760e-01)	Acc@1  77.00 ( 70.92)	Acc@5  97.00 ( 97.84)
Test: [ 60/100]	Time  0.022 ( 0.021)	Loss 8.0090e-01 (8.5746e-01)	Acc@1  71.00 ( 70.97)	Acc@5  98.00 ( 97.90)
Test: [ 70/100]	Time  0.020 ( 0.021)	Loss 8.6302e-01 (8.6213e-01)	Acc@1  74.00 ( 70.79)	Acc@5  98.00 ( 97.96)
Test: [ 80/100]	Time  0.019 ( 0.021)	Loss 7.2596e-01 (8.5694e-01)	Acc@1  72.00 ( 71.00)	Acc@5  97.00 ( 98.02)
Test: [ 90/100]	Time  0.018 ( 0.021)	Loss 6.8106e-01 (8.6073e-01)	Acc@1  71.00 ( 70.73)	Acc@5  99.00 ( 98.05)
 * Acc@1 70.880 Acc@5 98.010
### epoch[3] execution time: 17.210867881774902
EPOCH 4
# current learning rate: 0.1
# Switched to train mode...
Epoch: [4][  0/391]	Time  0.226 ( 0.226)	Data  0.191 ( 0.191)	Loss 7.5884e-01 (7.5884e-01)	Acc@1  72.66 ( 72.66)	Acc@5  99.22 ( 99.22)
Epoch: [4][ 10/391]	Time  0.037 ( 0.055)	Data  0.001 ( 0.019)	Loss 7.8127e-01 (7.8741e-01)	Acc@1  67.19 ( 70.95)	Acc@5  99.22 ( 98.79)
Epoch: [4][ 20/391]	Time  0.035 ( 0.047)	Data  0.001 ( 0.011)	Loss 8.0779e-01 (7.9967e-01)	Acc@1  67.97 ( 71.21)	Acc@5  99.22 ( 98.40)
Epoch: [4][ 30/391]	Time  0.036 ( 0.044)	Data  0.001 ( 0.008)	Loss 9.2461e-01 (7.8224e-01)	Acc@1  67.97 ( 72.15)	Acc@5  96.88 ( 98.36)
Epoch: [4][ 40/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.007)	Loss 8.0979e-01 (7.8021e-01)	Acc@1  73.44 ( 72.50)	Acc@5  96.09 ( 98.30)
Epoch: [4][ 50/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.006)	Loss 7.8214e-01 (7.7098e-01)	Acc@1  74.22 ( 72.92)	Acc@5  96.88 ( 98.30)
Epoch: [4][ 60/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.005)	Loss 7.5816e-01 (7.7140e-01)	Acc@1  73.44 ( 72.80)	Acc@5  98.44 ( 98.36)
Epoch: [4][ 70/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.005)	Loss 8.4099e-01 (7.7665e-01)	Acc@1  69.53 ( 72.67)	Acc@5  97.66 ( 98.34)
Epoch: [4][ 80/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.005)	Loss 7.1021e-01 (7.6700e-01)	Acc@1  75.00 ( 72.81)	Acc@5  99.22 ( 98.41)
Epoch: [4][ 90/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 9.3545e-01 (7.6078e-01)	Acc@1  64.84 ( 73.10)	Acc@5  96.09 ( 98.32)
Epoch: [4][100/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 7.7076e-01 (7.6099e-01)	Acc@1  74.22 ( 73.19)	Acc@5  96.09 ( 98.35)
Epoch: [4][110/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.004)	Loss 7.3547e-01 (7.5914e-01)	Acc@1  75.00 ( 73.36)	Acc@5  98.44 ( 98.32)
Epoch: [4][120/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 7.6513e-01 (7.5931e-01)	Acc@1  75.00 ( 73.53)	Acc@5  96.88 ( 98.22)
Epoch: [4][130/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.004)	Loss 6.9044e-01 (7.5255e-01)	Acc@1  75.00 ( 73.71)	Acc@5  99.22 ( 98.29)
Epoch: [4][140/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 9.9127e-01 (7.5400e-01)	Acc@1  63.28 ( 73.63)	Acc@5  94.53 ( 98.25)
Epoch: [4][150/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.1029e-01 (7.5234e-01)	Acc@1  74.22 ( 73.71)	Acc@5  98.44 ( 98.26)
Epoch: [4][160/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.3290e-01 (7.5118e-01)	Acc@1  76.56 ( 73.74)	Acc@5  96.09 ( 98.25)
Epoch: [4][170/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.8231e-01 (7.4622e-01)	Acc@1  78.12 ( 73.91)	Acc@5  98.44 ( 98.26)
Epoch: [4][180/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.7813e-01 (7.4380e-01)	Acc@1  77.34 ( 73.97)	Acc@5  96.88 ( 98.29)
Epoch: [4][190/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.8059e-01 (7.3973e-01)	Acc@1  77.34 ( 74.09)	Acc@5  98.44 ( 98.29)
Epoch: [4][200/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.6233e-01 (7.3914e-01)	Acc@1  75.00 ( 74.11)	Acc@5  97.66 ( 98.28)
Epoch: [4][210/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.6919e-01 (7.3647e-01)	Acc@1  78.12 ( 74.22)	Acc@5  98.44 ( 98.30)
Epoch: [4][220/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.5804e-01 (7.3708e-01)	Acc@1  74.22 ( 74.13)	Acc@5  99.22 ( 98.31)
Epoch: [4][230/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.5961e-01 (7.3694e-01)	Acc@1  75.00 ( 74.11)	Acc@5 100.00 ( 98.34)
Epoch: [4][240/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.6349e-01 (7.3519e-01)	Acc@1  68.75 ( 74.16)	Acc@5  99.22 ( 98.33)
Epoch: [4][250/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.3653e-01 (7.3351e-01)	Acc@1  81.25 ( 74.25)	Acc@5  97.66 ( 98.34)
Epoch: [4][260/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.6210e-01 (7.3102e-01)	Acc@1  75.00 ( 74.32)	Acc@5 100.00 ( 98.36)
Epoch: [4][270/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.3889e-01 (7.2955e-01)	Acc@1  80.47 ( 74.44)	Acc@5  97.66 ( 98.34)
Epoch: [4][280/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.6304e-01 (7.2953e-01)	Acc@1  76.56 ( 74.47)	Acc@5  99.22 ( 98.35)
Epoch: [4][290/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.2078e-01 (7.2877e-01)	Acc@1  70.31 ( 74.48)	Acc@5  98.44 ( 98.35)
Epoch: [4][300/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.0701e-01 (7.2887e-01)	Acc@1  75.78 ( 74.45)	Acc@5  96.88 ( 98.36)
Epoch: [4][310/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.3440e-01 (7.2813e-01)	Acc@1  73.44 ( 74.44)	Acc@5  99.22 ( 98.37)
Epoch: [4][320/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.5676e-01 (7.2803e-01)	Acc@1  75.00 ( 74.45)	Acc@5  99.22 ( 98.38)
Epoch: [4][330/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.0920e-01 (7.2420e-01)	Acc@1  81.25 ( 74.60)	Acc@5  98.44 ( 98.40)
Epoch: [4][340/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.3239e-01 (7.2326e-01)	Acc@1  77.34 ( 74.62)	Acc@5  99.22 ( 98.39)
Epoch: [4][350/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.9309e-01 (7.2195e-01)	Acc@1  75.78 ( 74.66)	Acc@5  99.22 ( 98.40)
Epoch: [4][360/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.7983e-01 (7.2106e-01)	Acc@1  73.44 ( 74.69)	Acc@5  98.44 ( 98.41)
Epoch: [4][370/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.7383e-01 (7.1945e-01)	Acc@1  76.56 ( 74.73)	Acc@5  99.22 ( 98.41)
Epoch: [4][380/391]	Time  0.038 ( 0.038)	Data  0.002 ( 0.003)	Loss 6.3771e-01 (7.1765e-01)	Acc@1  74.22 ( 74.76)	Acc@5  98.44 ( 98.41)
Epoch: [4][390/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.2994e-01 (7.1648e-01)	Acc@1  73.75 ( 74.80)	Acc@5  98.75 ( 98.42)
## e[4] optimizer.zero_grad (sum) time: 0.17443633079528809
## e[4]       loss.backward (sum) time: 2.9973278045654297
## e[4]      optimizer.step (sum) time: 1.0312154293060303
## epoch[4] training(only) time: 15.066400527954102
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 7.7021e-01 (7.7021e-01)	Acc@1  73.00 ( 73.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.020 ( 0.033)	Loss 1.0566e+00 (8.9895e-01)	Acc@1  70.00 ( 70.73)	Acc@5  93.00 ( 97.09)
Test: [ 20/100]	Time  0.019 ( 0.027)	Loss 8.1599e-01 (8.8609e-01)	Acc@1  68.00 ( 71.00)	Acc@5 100.00 ( 97.38)
Test: [ 30/100]	Time  0.017 ( 0.024)	Loss 8.8923e-01 (9.0373e-01)	Acc@1  71.00 ( 70.77)	Acc@5  98.00 ( 97.48)
Test: [ 40/100]	Time  0.017 ( 0.022)	Loss 8.3542e-01 (9.0143e-01)	Acc@1  74.00 ( 70.95)	Acc@5 100.00 ( 97.78)
Test: [ 50/100]	Time  0.017 ( 0.021)	Loss 7.8910e-01 (9.0016e-01)	Acc@1  73.00 ( 71.25)	Acc@5  99.00 ( 97.86)
Test: [ 60/100]	Time  0.016 ( 0.021)	Loss 7.8623e-01 (9.0176e-01)	Acc@1  73.00 ( 71.23)	Acc@5  99.00 ( 97.87)
Test: [ 70/100]	Time  0.017 ( 0.021)	Loss 9.2806e-01 (9.0807e-01)	Acc@1  71.00 ( 70.87)	Acc@5  97.00 ( 97.86)
Test: [ 80/100]	Time  0.019 ( 0.020)	Loss 6.8140e-01 (9.0574e-01)	Acc@1  76.00 ( 70.98)	Acc@5  99.00 ( 97.94)
Test: [ 90/100]	Time  0.016 ( 0.020)	Loss 1.0276e+00 (9.1798e-01)	Acc@1  68.00 ( 70.57)	Acc@5  98.00 ( 97.87)
 * Acc@1 70.500 Acc@5 97.810
### epoch[4] execution time: 17.191978693008423
EPOCH 5
# current learning rate: 0.1
# Switched to train mode...
Epoch: [5][  0/391]	Time  0.217 ( 0.217)	Data  0.184 ( 0.184)	Loss 6.3506e-01 (6.3506e-01)	Acc@1  79.69 ( 79.69)	Acc@5  98.44 ( 98.44)
Epoch: [5][ 10/391]	Time  0.036 ( 0.054)	Data  0.001 ( 0.019)	Loss 5.0545e-01 (6.2699e-01)	Acc@1  84.38 ( 78.34)	Acc@5  98.44 ( 98.72)
Epoch: [5][ 20/391]	Time  0.036 ( 0.046)	Data  0.001 ( 0.011)	Loss 7.8098e-01 (6.3544e-01)	Acc@1  74.22 ( 78.09)	Acc@5  98.44 ( 98.81)
Epoch: [5][ 30/391]	Time  0.036 ( 0.043)	Data  0.001 ( 0.008)	Loss 7.0801e-01 (6.4805e-01)	Acc@1  76.56 ( 77.47)	Acc@5  96.88 ( 98.69)
Epoch: [5][ 40/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.007)	Loss 5.0795e-01 (6.5391e-01)	Acc@1  83.59 ( 77.10)	Acc@5 100.00 ( 98.78)
Epoch: [5][ 50/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.006)	Loss 8.2504e-01 (6.5936e-01)	Acc@1  74.22 ( 76.95)	Acc@5  96.88 ( 98.73)
Epoch: [5][ 60/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.005)	Loss 7.9216e-01 (6.6203e-01)	Acc@1  75.78 ( 77.13)	Acc@5  97.66 ( 98.71)
Epoch: [5][ 70/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.005)	Loss 7.7730e-01 (6.6016e-01)	Acc@1  77.34 ( 77.09)	Acc@5  96.88 ( 98.73)
Epoch: [5][ 80/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.004)	Loss 4.9143e-01 (6.5348e-01)	Acc@1  82.81 ( 77.29)	Acc@5  99.22 ( 98.78)
Epoch: [5][ 90/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 7.1008e-01 (6.4533e-01)	Acc@1  72.66 ( 77.73)	Acc@5 100.00 ( 98.80)
Epoch: [5][100/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 5.6275e-01 (6.4477e-01)	Acc@1  79.69 ( 77.78)	Acc@5  98.44 ( 98.77)
Epoch: [5][110/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.004)	Loss 4.9298e-01 (6.3944e-01)	Acc@1  82.03 ( 77.98)	Acc@5  98.44 ( 98.78)
Epoch: [5][120/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 4.8880e-01 (6.3601e-01)	Acc@1  87.50 ( 78.15)	Acc@5  98.44 ( 98.79)
Epoch: [5][130/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 6.3715e-01 (6.3596e-01)	Acc@1  82.81 ( 78.14)	Acc@5  97.66 ( 98.80)
Epoch: [5][140/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.004)	Loss 5.1303e-01 (6.3284e-01)	Acc@1  83.59 ( 78.20)	Acc@5  97.66 ( 98.80)
Epoch: [5][150/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.7017e-01 (6.3239e-01)	Acc@1  78.12 ( 78.23)	Acc@5  96.88 ( 98.77)
Epoch: [5][160/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.9564e-01 (6.3259e-01)	Acc@1  74.22 ( 78.14)	Acc@5  98.44 ( 98.78)
Epoch: [5][170/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.3711e-01 (6.2984e-01)	Acc@1  76.56 ( 78.31)	Acc@5  99.22 ( 98.76)
Epoch: [5][180/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.5355e-01 (6.2848e-01)	Acc@1  75.78 ( 78.33)	Acc@5  99.22 ( 98.78)
Epoch: [5][190/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.9057e-01 (6.2902e-01)	Acc@1  78.12 ( 78.23)	Acc@5  98.44 ( 98.79)
Epoch: [5][200/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.1732e-01 (6.2895e-01)	Acc@1  78.91 ( 78.27)	Acc@5  99.22 ( 98.74)
Epoch: [5][210/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.3675e-01 (6.2964e-01)	Acc@1  74.22 ( 78.24)	Acc@5  99.22 ( 98.73)
Epoch: [5][220/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.0034e-01 (6.2806e-01)	Acc@1  82.03 ( 78.26)	Acc@5  99.22 ( 98.73)
Epoch: [5][230/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.7978e-01 (6.2750e-01)	Acc@1  75.00 ( 78.28)	Acc@5  99.22 ( 98.74)
Epoch: [5][240/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.2162e-01 (6.2621e-01)	Acc@1  81.25 ( 78.32)	Acc@5  98.44 ( 98.76)
Epoch: [5][250/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.5342e-01 (6.2433e-01)	Acc@1  75.00 ( 78.41)	Acc@5  98.44 ( 98.76)
Epoch: [5][260/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.2829e-01 (6.2196e-01)	Acc@1  75.78 ( 78.51)	Acc@5 100.00 ( 98.78)
Epoch: [5][270/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.9117e-01 (6.2192e-01)	Acc@1  75.00 ( 78.51)	Acc@5 100.00 ( 98.77)
Epoch: [5][280/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.7901e-01 (6.2104e-01)	Acc@1  78.12 ( 78.54)	Acc@5 100.00 ( 98.78)
Epoch: [5][290/391]	Time  0.040 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.2877e-01 (6.2074e-01)	Acc@1  76.56 ( 78.57)	Acc@5  98.44 ( 98.77)
Epoch: [5][300/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.0265e-01 (6.2000e-01)	Acc@1  80.47 ( 78.55)	Acc@5 100.00 ( 98.77)
Epoch: [5][310/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.1119e-01 (6.2129e-01)	Acc@1  82.03 ( 78.53)	Acc@5  99.22 ( 98.76)
Epoch: [5][320/391]	Time  0.040 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.3505e-01 (6.2125e-01)	Acc@1  81.25 ( 78.53)	Acc@5  98.44 ( 98.76)
Epoch: [5][330/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.1244e-01 (6.2053e-01)	Acc@1  80.47 ( 78.57)	Acc@5  99.22 ( 98.77)
Epoch: [5][340/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.7027e-01 (6.1887e-01)	Acc@1  79.69 ( 78.63)	Acc@5  98.44 ( 98.77)
Epoch: [5][350/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.2873e-01 (6.1833e-01)	Acc@1  78.12 ( 78.62)	Acc@5  99.22 ( 98.78)
Epoch: [5][360/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.1528e-01 (6.1744e-01)	Acc@1  72.66 ( 78.66)	Acc@5  96.88 ( 98.79)
Epoch: [5][370/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.5000e-01 (6.1666e-01)	Acc@1  75.00 ( 78.65)	Acc@5  99.22 ( 98.79)
Epoch: [5][380/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.2641e-01 (6.1755e-01)	Acc@1  75.78 ( 78.64)	Acc@5  96.88 ( 98.78)
Epoch: [5][390/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.4263e-01 (6.1756e-01)	Acc@1  81.25 ( 78.63)	Acc@5  98.75 ( 98.77)
## e[5] optimizer.zero_grad (sum) time: 0.1761608123779297
## e[5]       loss.backward (sum) time: 2.959510326385498
## e[5]      optimizer.step (sum) time: 1.026841402053833
## epoch[5] training(only) time: 15.095637321472168
# Switched to evaluate mode...
Test: [  0/100]	Time  0.175 ( 0.175)	Loss 5.5469e-01 (5.5469e-01)	Acc@1  82.00 ( 82.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.018 ( 0.031)	Loss 6.8204e-01 (5.9632e-01)	Acc@1  81.00 ( 80.27)	Acc@5  99.00 ( 99.18)
Test: [ 20/100]	Time  0.018 ( 0.026)	Loss 5.5739e-01 (5.9372e-01)	Acc@1  79.00 ( 79.90)	Acc@5 100.00 ( 98.86)
Test: [ 30/100]	Time  0.024 ( 0.023)	Loss 7.0113e-01 (6.0935e-01)	Acc@1  78.00 ( 79.81)	Acc@5  98.00 ( 98.77)
Test: [ 40/100]	Time  0.019 ( 0.022)	Loss 5.5648e-01 (6.1488e-01)	Acc@1  82.00 ( 79.39)	Acc@5  97.00 ( 98.76)
Test: [ 50/100]	Time  0.021 ( 0.022)	Loss 5.4208e-01 (6.1391e-01)	Acc@1  84.00 ( 79.53)	Acc@5  99.00 ( 98.75)
Test: [ 60/100]	Time  0.016 ( 0.021)	Loss 5.5437e-01 (6.1206e-01)	Acc@1  82.00 ( 79.48)	Acc@5 100.00 ( 98.84)
Test: [ 70/100]	Time  0.018 ( 0.021)	Loss 6.3182e-01 (6.0923e-01)	Acc@1  79.00 ( 79.51)	Acc@5  99.00 ( 98.87)
Test: [ 80/100]	Time  0.016 ( 0.020)	Loss 5.9324e-01 (6.1240e-01)	Acc@1  80.00 ( 79.54)	Acc@5  98.00 ( 98.88)
Test: [ 90/100]	Time  0.016 ( 0.020)	Loss 5.6015e-01 (6.1434e-01)	Acc@1  77.00 ( 79.38)	Acc@5 100.00 ( 98.89)
 * Acc@1 79.530 Acc@5 98.900
### epoch[5] execution time: 17.216055631637573
EPOCH 6
# current learning rate: 0.1
# Switched to train mode...
Epoch: [6][  0/391]	Time  0.213 ( 0.213)	Data  0.178 ( 0.178)	Loss 5.1822e-01 (5.1822e-01)	Acc@1  85.94 ( 85.94)	Acc@5  97.66 ( 97.66)
Epoch: [6][ 10/391]	Time  0.035 ( 0.054)	Data  0.001 ( 0.018)	Loss 6.1708e-01 (5.1010e-01)	Acc@1  83.59 ( 83.66)	Acc@5  97.66 ( 99.01)
Epoch: [6][ 20/391]	Time  0.038 ( 0.046)	Data  0.001 ( 0.011)	Loss 5.6055e-01 (5.2516e-01)	Acc@1  78.91 ( 81.92)	Acc@5  97.66 ( 98.88)
Epoch: [6][ 30/391]	Time  0.036 ( 0.043)	Data  0.001 ( 0.008)	Loss 6.6068e-01 (5.4833e-01)	Acc@1  72.66 ( 80.80)	Acc@5  99.22 ( 98.89)
Epoch: [6][ 40/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.007)	Loss 6.5227e-01 (5.5307e-01)	Acc@1  81.25 ( 80.85)	Acc@5  99.22 ( 98.99)
Epoch: [6][ 50/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.006)	Loss 5.6639e-01 (5.5598e-01)	Acc@1  77.34 ( 80.76)	Acc@5 100.00 ( 99.02)
Epoch: [6][ 60/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.005)	Loss 5.7704e-01 (5.5814e-01)	Acc@1  77.34 ( 80.52)	Acc@5 100.00 ( 98.99)
Epoch: [6][ 70/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.005)	Loss 5.4799e-01 (5.5533e-01)	Acc@1  82.03 ( 80.52)	Acc@5  99.22 ( 99.00)
Epoch: [6][ 80/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.004)	Loss 5.1380e-01 (5.5581e-01)	Acc@1  83.59 ( 80.48)	Acc@5  99.22 ( 98.97)
Epoch: [6][ 90/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.004)	Loss 5.5504e-01 (5.5954e-01)	Acc@1  80.47 ( 80.49)	Acc@5  99.22 ( 98.89)
Epoch: [6][100/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 4.1191e-01 (5.6171e-01)	Acc@1  87.50 ( 80.47)	Acc@5 100.00 ( 98.90)
Epoch: [6][110/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.004)	Loss 4.6654e-01 (5.6203e-01)	Acc@1  85.16 ( 80.48)	Acc@5  99.22 ( 98.92)
Epoch: [6][120/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.004)	Loss 6.5763e-01 (5.6297e-01)	Acc@1  76.56 ( 80.39)	Acc@5  99.22 ( 98.93)
Epoch: [6][130/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.004)	Loss 5.2342e-01 (5.6123e-01)	Acc@1  83.59 ( 80.46)	Acc@5 100.00 ( 98.93)
Epoch: [6][140/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 7.2146e-01 (5.6057e-01)	Acc@1  75.00 ( 80.41)	Acc@5  98.44 ( 98.93)
Epoch: [6][150/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.5120e-01 (5.6116e-01)	Acc@1  85.16 ( 80.43)	Acc@5  99.22 ( 98.94)
Epoch: [6][160/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.2310e-01 (5.6489e-01)	Acc@1  72.66 ( 80.31)	Acc@5  98.44 ( 98.93)
Epoch: [6][170/391]	Time  0.040 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.2765e-01 (5.6370e-01)	Acc@1  88.28 ( 80.35)	Acc@5  98.44 ( 98.94)
Epoch: [6][180/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.1206e-01 (5.6432e-01)	Acc@1  79.69 ( 80.30)	Acc@5 100.00 ( 98.96)
Epoch: [6][190/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.6567e-01 (5.6472e-01)	Acc@1  73.44 ( 80.24)	Acc@5 100.00 ( 98.97)
Epoch: [6][200/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.6820e-01 (5.6378e-01)	Acc@1  81.25 ( 80.28)	Acc@5  98.44 ( 98.99)
Epoch: [6][210/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.3861e-01 (5.6198e-01)	Acc@1  80.47 ( 80.34)	Acc@5  98.44 ( 98.97)
Epoch: [6][220/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.8452e-01 (5.6366e-01)	Acc@1  78.91 ( 80.29)	Acc@5  98.44 ( 98.96)
Epoch: [6][230/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.1173e-01 (5.6072e-01)	Acc@1  84.38 ( 80.45)	Acc@5 100.00 ( 98.96)
Epoch: [6][240/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.1070e-01 (5.6139e-01)	Acc@1  78.12 ( 80.42)	Acc@5 100.00 ( 98.96)
Epoch: [6][250/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.2370e-01 (5.6056e-01)	Acc@1  79.69 ( 80.48)	Acc@5  98.44 ( 98.97)
Epoch: [6][260/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.6183e-01 (5.6049e-01)	Acc@1  83.59 ( 80.44)	Acc@5 100.00 ( 98.97)
Epoch: [6][270/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.4657e-01 (5.6022e-01)	Acc@1  76.56 ( 80.42)	Acc@5  98.44 ( 98.96)
Epoch: [6][280/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.4916e-01 (5.5827e-01)	Acc@1  87.50 ( 80.50)	Acc@5 100.00 ( 98.97)
Epoch: [6][290/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.5592e-01 (5.5595e-01)	Acc@1  81.25 ( 80.56)	Acc@5  98.44 ( 98.98)
Epoch: [6][300/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.9519e-01 (5.5422e-01)	Acc@1  81.25 ( 80.63)	Acc@5  99.22 ( 98.97)
Epoch: [6][310/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.5749e-01 (5.5432e-01)	Acc@1  77.34 ( 80.61)	Acc@5 100.00 ( 98.97)
Epoch: [6][320/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.2566e-01 (5.5556e-01)	Acc@1  78.12 ( 80.57)	Acc@5  94.53 ( 98.96)
Epoch: [6][330/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.3877e-01 (5.5595e-01)	Acc@1  85.16 ( 80.56)	Acc@5  99.22 ( 98.97)
Epoch: [6][340/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.3984e-01 (5.5385e-01)	Acc@1  83.59 ( 80.64)	Acc@5  98.44 ( 98.99)
Epoch: [6][350/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.0153e-01 (5.5268e-01)	Acc@1  78.12 ( 80.67)	Acc@5  98.44 ( 98.99)
Epoch: [6][360/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.7912e-01 (5.5244e-01)	Acc@1  78.12 ( 80.66)	Acc@5  99.22 ( 99.00)
Epoch: [6][370/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.4886e-01 (5.5231e-01)	Acc@1  78.91 ( 80.67)	Acc@5  98.44 ( 99.00)
Epoch: [6][380/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.3220e-01 (5.5267e-01)	Acc@1  81.25 ( 80.68)	Acc@5  98.44 ( 99.00)
Epoch: [6][390/391]	Time  0.032 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.4525e-01 (5.5368e-01)	Acc@1  87.50 ( 80.65)	Acc@5  98.75 ( 98.99)
## e[6] optimizer.zero_grad (sum) time: 0.17826366424560547
## e[6]       loss.backward (sum) time: 2.9401421546936035
## e[6]      optimizer.step (sum) time: 1.0203468799591064
## epoch[6] training(only) time: 15.058820962905884
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 4.7147e-01 (4.7147e-01)	Acc@1  85.00 ( 85.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.020 ( 0.034)	Loss 6.7166e-01 (5.5801e-01)	Acc@1  81.00 ( 80.82)	Acc@5  99.00 ( 98.91)
Test: [ 20/100]	Time  0.017 ( 0.026)	Loss 5.2683e-01 (5.7264e-01)	Acc@1  82.00 ( 79.81)	Acc@5 100.00 ( 98.95)
Test: [ 30/100]	Time  0.017 ( 0.024)	Loss 6.2936e-01 (5.8070e-01)	Acc@1  82.00 ( 80.26)	Acc@5  98.00 ( 98.68)
Test: [ 40/100]	Time  0.023 ( 0.023)	Loss 5.9220e-01 (5.7382e-01)	Acc@1  82.00 ( 80.54)	Acc@5  99.00 ( 98.66)
Test: [ 50/100]	Time  0.022 ( 0.023)	Loss 4.2272e-01 (5.7188e-01)	Acc@1  82.00 ( 80.71)	Acc@5 100.00 ( 98.71)
Test: [ 60/100]	Time  0.024 ( 0.022)	Loss 5.1969e-01 (5.7054e-01)	Acc@1  80.00 ( 80.89)	Acc@5 100.00 ( 98.75)
Test: [ 70/100]	Time  0.021 ( 0.022)	Loss 6.7429e-01 (5.7057e-01)	Acc@1  84.00 ( 80.94)	Acc@5  99.00 ( 98.82)
Test: [ 80/100]	Time  0.016 ( 0.022)	Loss 4.3712e-01 (5.6990e-01)	Acc@1  82.00 ( 80.83)	Acc@5 100.00 ( 98.84)
Test: [ 90/100]	Time  0.016 ( 0.021)	Loss 5.4594e-01 (5.7137e-01)	Acc@1  83.00 ( 80.82)	Acc@5  99.00 ( 98.88)
 * Acc@1 80.800 Acc@5 98.860
### epoch[6] execution time: 17.3382568359375
EPOCH 7
# current learning rate: 0.1
# Switched to train mode...
Epoch: [7][  0/391]	Time  0.213 ( 0.213)	Data  0.175 ( 0.175)	Loss 5.8941e-01 (5.8941e-01)	Acc@1  78.12 ( 78.12)	Acc@5  97.66 ( 97.66)
Epoch: [7][ 10/391]	Time  0.037 ( 0.053)	Data  0.001 ( 0.018)	Loss 5.3150e-01 (4.9603e-01)	Acc@1  83.59 ( 82.95)	Acc@5  99.22 ( 99.08)
Epoch: [7][ 20/391]	Time  0.037 ( 0.046)	Data  0.001 ( 0.010)	Loss 3.7134e-01 (4.7059e-01)	Acc@1  87.50 ( 83.63)	Acc@5 100.00 ( 99.29)
Epoch: [7][ 30/391]	Time  0.035 ( 0.043)	Data  0.002 ( 0.008)	Loss 4.7139e-01 (4.8307e-01)	Acc@1  82.81 ( 83.19)	Acc@5 100.00 ( 99.29)
Epoch: [7][ 40/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.006)	Loss 5.4684e-01 (4.9464e-01)	Acc@1  78.91 ( 82.87)	Acc@5  99.22 ( 99.35)
Epoch: [7][ 50/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.006)	Loss 4.5624e-01 (4.9607e-01)	Acc@1  87.50 ( 82.89)	Acc@5  99.22 ( 99.30)
Epoch: [7][ 60/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.005)	Loss 5.5927e-01 (4.9938e-01)	Acc@1  78.12 ( 82.61)	Acc@5 100.00 ( 99.31)
Epoch: [7][ 70/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.005)	Loss 5.0485e-01 (4.9814e-01)	Acc@1  81.25 ( 82.70)	Acc@5  99.22 ( 99.32)
Epoch: [7][ 80/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 5.1395e-01 (4.9409e-01)	Acc@1  83.59 ( 82.78)	Acc@5  98.44 ( 99.33)
Epoch: [7][ 90/391]	Time  0.038 ( 0.040)	Data  0.002 ( 0.004)	Loss 6.0079e-01 (4.9703e-01)	Acc@1  77.34 ( 82.66)	Acc@5  99.22 ( 99.32)
Epoch: [7][100/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 3.7697e-01 (4.9897e-01)	Acc@1  87.50 ( 82.56)	Acc@5  98.44 ( 99.33)
Epoch: [7][110/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 3.9742e-01 (4.9748e-01)	Acc@1  87.50 ( 82.61)	Acc@5  99.22 ( 99.35)
Epoch: [7][120/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 5.5654e-01 (4.9738e-01)	Acc@1  81.25 ( 82.52)	Acc@5  99.22 ( 99.36)
Epoch: [7][130/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 4.6108e-01 (4.9673e-01)	Acc@1  85.16 ( 82.56)	Acc@5 100.00 ( 99.39)
Epoch: [7][140/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.3098e-01 (4.9715e-01)	Acc@1  82.03 ( 82.44)	Acc@5 100.00 ( 99.40)
Epoch: [7][150/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.0435e-01 (4.9829e-01)	Acc@1  85.16 ( 82.37)	Acc@5 100.00 ( 99.40)
Epoch: [7][160/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.2643e-01 (4.9930e-01)	Acc@1  88.28 ( 82.33)	Acc@5 100.00 ( 99.38)
Epoch: [7][170/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.6851e-01 (5.0200e-01)	Acc@1  78.12 ( 82.26)	Acc@5  99.22 ( 99.35)
Epoch: [7][180/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.2331e-01 (5.0355e-01)	Acc@1  80.47 ( 82.20)	Acc@5 100.00 ( 99.35)
Epoch: [7][190/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.7593e-01 (5.0265e-01)	Acc@1  82.03 ( 82.25)	Acc@5  98.44 ( 99.35)
Epoch: [7][200/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.0911e-01 (5.0310e-01)	Acc@1  83.59 ( 82.21)	Acc@5 100.00 ( 99.33)
Epoch: [7][210/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.0931e-01 (5.0492e-01)	Acc@1  88.28 ( 82.16)	Acc@5 100.00 ( 99.32)
Epoch: [7][220/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.2647e-01 (5.0768e-01)	Acc@1  79.69 ( 82.08)	Acc@5  97.66 ( 99.31)
Epoch: [7][230/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.6433e-01 (5.0602e-01)	Acc@1  84.38 ( 82.16)	Acc@5  98.44 ( 99.32)
Epoch: [7][240/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.5863e-01 (5.0314e-01)	Acc@1  85.94 ( 82.28)	Acc@5 100.00 ( 99.34)
Epoch: [7][250/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.7360e-01 (5.0388e-01)	Acc@1  85.16 ( 82.27)	Acc@5  99.22 ( 99.32)
Epoch: [7][260/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.2612e-01 (5.0401e-01)	Acc@1  82.03 ( 82.31)	Acc@5 100.00 ( 99.32)
Epoch: [7][270/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.0056e-01 (5.0286e-01)	Acc@1  78.91 ( 82.36)	Acc@5  99.22 ( 99.33)
Epoch: [7][280/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.6432e-01 (5.0199e-01)	Acc@1  88.28 ( 82.42)	Acc@5 100.00 ( 99.33)
Epoch: [7][290/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.0660e-01 (4.9986e-01)	Acc@1  89.84 ( 82.52)	Acc@5 100.00 ( 99.33)
Epoch: [7][300/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.0984e-01 (5.0016e-01)	Acc@1  84.38 ( 82.51)	Acc@5 100.00 ( 99.33)
Epoch: [7][310/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.1243e-01 (4.9941e-01)	Acc@1  84.38 ( 82.55)	Acc@5  98.44 ( 99.33)
Epoch: [7][320/391]	Time  0.037 ( 0.038)	Data  0.002 ( 0.003)	Loss 4.8105e-01 (4.9825e-01)	Acc@1  84.38 ( 82.58)	Acc@5 100.00 ( 99.33)
Epoch: [7][330/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.0275e-01 (4.9792e-01)	Acc@1  85.16 ( 82.59)	Acc@5  97.66 ( 99.32)
Epoch: [7][340/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.5604e-01 (4.9612e-01)	Acc@1  79.69 ( 82.65)	Acc@5  99.22 ( 99.34)
Epoch: [7][350/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.8834e-01 (4.9479e-01)	Acc@1  85.94 ( 82.72)	Acc@5 100.00 ( 99.32)
Epoch: [7][360/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.4388e-01 (4.9368e-01)	Acc@1  87.50 ( 82.75)	Acc@5 100.00 ( 99.32)
Epoch: [7][370/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.0290e-01 (4.9311e-01)	Acc@1  80.47 ( 82.72)	Acc@5  99.22 ( 99.33)
Epoch: [7][380/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.6480e-01 (4.9289e-01)	Acc@1  82.81 ( 82.73)	Acc@5 100.00 ( 99.32)
Epoch: [7][390/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.5286e-01 (4.9278e-01)	Acc@1  85.00 ( 82.76)	Acc@5 100.00 ( 99.32)
## e[7] optimizer.zero_grad (sum) time: 0.17598652839660645
## e[7]       loss.backward (sum) time: 2.9777772426605225
## e[7]      optimizer.step (sum) time: 1.0364596843719482
## epoch[7] training(only) time: 15.119544982910156
# Switched to evaluate mode...
Test: [  0/100]	Time  0.174 ( 0.174)	Loss 5.8956e-01 (5.8956e-01)	Acc@1  84.00 ( 84.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.016 ( 0.033)	Loss 6.9491e-01 (5.9102e-01)	Acc@1  78.00 ( 81.27)	Acc@5 100.00 ( 98.91)
Test: [ 20/100]	Time  0.017 ( 0.026)	Loss 6.3094e-01 (5.8362e-01)	Acc@1  81.00 ( 81.05)	Acc@5 100.00 ( 98.90)
Test: [ 30/100]	Time  0.016 ( 0.024)	Loss 6.3088e-01 (6.0790e-01)	Acc@1  80.00 ( 80.45)	Acc@5  97.00 ( 98.77)
Test: [ 40/100]	Time  0.022 ( 0.023)	Loss 4.4488e-01 (6.0117e-01)	Acc@1  86.00 ( 80.56)	Acc@5  99.00 ( 98.78)
Test: [ 50/100]	Time  0.020 ( 0.023)	Loss 4.7508e-01 (6.0122e-01)	Acc@1  84.00 ( 80.37)	Acc@5  98.00 ( 98.80)
Test: [ 60/100]	Time  0.018 ( 0.022)	Loss 5.5065e-01 (5.9740e-01)	Acc@1  83.00 ( 80.56)	Acc@5 100.00 ( 98.90)
Test: [ 70/100]	Time  0.021 ( 0.021)	Loss 5.3770e-01 (5.9820e-01)	Acc@1  82.00 ( 80.45)	Acc@5 100.00 ( 98.92)
Test: [ 80/100]	Time  0.016 ( 0.021)	Loss 6.1341e-01 (5.9958e-01)	Acc@1  77.00 ( 80.33)	Acc@5  99.00 ( 98.94)
Test: [ 90/100]	Time  0.017 ( 0.021)	Loss 6.3063e-01 (5.9804e-01)	Acc@1  80.00 ( 80.24)	Acc@5 100.00 ( 98.97)
 * Acc@1 80.370 Acc@5 99.000
### epoch[7] execution time: 17.302926063537598
EPOCH 8
# current learning rate: 0.1
# Switched to train mode...
Epoch: [8][  0/391]	Time  0.212 ( 0.212)	Data  0.179 ( 0.179)	Loss 3.9605e-01 (3.9605e-01)	Acc@1  85.94 ( 85.94)	Acc@5 100.00 (100.00)
Epoch: [8][ 10/391]	Time  0.036 ( 0.054)	Data  0.001 ( 0.018)	Loss 5.1681e-01 (4.8839e-01)	Acc@1  85.16 ( 83.10)	Acc@5 100.00 ( 99.57)
Epoch: [8][ 20/391]	Time  0.038 ( 0.046)	Data  0.001 ( 0.011)	Loss 5.4304e-01 (4.7540e-01)	Acc@1  83.59 ( 83.82)	Acc@5  99.22 ( 99.48)
Epoch: [8][ 30/391]	Time  0.036 ( 0.044)	Data  0.001 ( 0.008)	Loss 4.3841e-01 (4.6565e-01)	Acc@1  86.72 ( 84.63)	Acc@5  99.22 ( 99.32)
Epoch: [8][ 40/391]	Time  0.035 ( 0.042)	Data  0.001 ( 0.007)	Loss 5.1763e-01 (4.6193e-01)	Acc@1  84.38 ( 84.49)	Acc@5  96.88 ( 99.20)
Epoch: [8][ 50/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.006)	Loss 5.0710e-01 (4.6117e-01)	Acc@1  85.16 ( 84.51)	Acc@5 100.00 ( 99.25)
Epoch: [8][ 60/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.005)	Loss 5.1430e-01 (4.6309e-01)	Acc@1  78.91 ( 84.30)	Acc@5  98.44 ( 99.27)
Epoch: [8][ 70/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.005)	Loss 3.7885e-01 (4.5916e-01)	Acc@1  87.50 ( 84.32)	Acc@5 100.00 ( 99.30)
Epoch: [8][ 80/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 4.5885e-01 (4.5807e-01)	Acc@1  82.81 ( 84.33)	Acc@5  99.22 ( 99.30)
Epoch: [8][ 90/391]	Time  0.038 ( 0.040)	Data  0.002 ( 0.004)	Loss 4.0588e-01 (4.5886e-01)	Acc@1  85.94 ( 84.29)	Acc@5  99.22 ( 99.26)
Epoch: [8][100/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 3.5512e-01 (4.5857e-01)	Acc@1  86.72 ( 84.27)	Acc@5 100.00 ( 99.26)
Epoch: [8][110/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 4.6271e-01 (4.6132e-01)	Acc@1  85.94 ( 84.13)	Acc@5 100.00 ( 99.28)
Epoch: [8][120/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 5.2759e-01 (4.6390e-01)	Acc@1  78.91 ( 84.07)	Acc@5  99.22 ( 99.25)
Epoch: [8][130/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 4.3655e-01 (4.6383e-01)	Acc@1  85.16 ( 84.12)	Acc@5 100.00 ( 99.24)
Epoch: [8][140/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.2381e-01 (4.6155e-01)	Acc@1  87.50 ( 84.19)	Acc@5  99.22 ( 99.25)
Epoch: [8][150/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.4582e-01 (4.6352e-01)	Acc@1  82.03 ( 84.09)	Acc@5  98.44 ( 99.24)
Epoch: [8][160/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.3857e-01 (4.6692e-01)	Acc@1  72.66 ( 83.96)	Acc@5  96.88 ( 99.23)
Epoch: [8][170/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.2496e-01 (4.6678e-01)	Acc@1  77.34 ( 83.92)	Acc@5 100.00 ( 99.22)
Epoch: [8][180/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.1706e-01 (4.6486e-01)	Acc@1  82.81 ( 84.03)	Acc@5 100.00 ( 99.24)
Epoch: [8][190/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.1788e-01 (4.6365e-01)	Acc@1  84.38 ( 84.04)	Acc@5  99.22 ( 99.26)
Epoch: [8][200/391]	Time  0.040 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.3959e-01 (4.6278e-01)	Acc@1  79.69 ( 84.06)	Acc@5  99.22 ( 99.27)
Epoch: [8][210/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.4589e-01 (4.6336e-01)	Acc@1  82.81 ( 84.03)	Acc@5  99.22 ( 99.27)
Epoch: [8][220/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.3572e-01 (4.6234e-01)	Acc@1  83.59 ( 84.11)	Acc@5 100.00 ( 99.30)
Epoch: [8][230/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.7027e-01 (4.6383e-01)	Acc@1  79.69 ( 84.04)	Acc@5  98.44 ( 99.27)
Epoch: [8][240/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.9801e-01 (4.6365e-01)	Acc@1  82.81 ( 84.02)	Acc@5  99.22 ( 99.28)
Epoch: [8][250/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.3749e-01 (4.6316e-01)	Acc@1  81.25 ( 83.97)	Acc@5 100.00 ( 99.29)
Epoch: [8][260/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.3810e-01 (4.6225e-01)	Acc@1  84.38 ( 84.00)	Acc@5 100.00 ( 99.28)
Epoch: [8][270/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.9899e-01 (4.6190e-01)	Acc@1  84.38 ( 84.01)	Acc@5 100.00 ( 99.29)
Epoch: [8][280/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.8798e-01 (4.6099e-01)	Acc@1  81.25 ( 84.02)	Acc@5  99.22 ( 99.29)
Epoch: [8][290/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.1136e-01 (4.5928e-01)	Acc@1  89.06 ( 84.10)	Acc@5 100.00 ( 99.29)
Epoch: [8][300/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.0847e-01 (4.5781e-01)	Acc@1  85.94 ( 84.14)	Acc@5 100.00 ( 99.30)
Epoch: [8][310/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.5343e-01 (4.5764e-01)	Acc@1  91.41 ( 84.17)	Acc@5 100.00 ( 99.31)
Epoch: [8][320/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.7068e-01 (4.5574e-01)	Acc@1  85.94 ( 84.22)	Acc@5 100.00 ( 99.32)
Epoch: [8][330/391]	Time  0.036 ( 0.038)	Data  0.002 ( 0.003)	Loss 4.1436e-01 (4.5545e-01)	Acc@1  86.72 ( 84.22)	Acc@5  99.22 ( 99.33)
Epoch: [8][340/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.1972e-01 (4.5424e-01)	Acc@1  88.28 ( 84.29)	Acc@5 100.00 ( 99.34)
Epoch: [8][350/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.1394e-01 (4.5403e-01)	Acc@1  79.69 ( 84.30)	Acc@5 100.00 ( 99.33)
Epoch: [8][360/391]	Time  0.042 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.7199e-01 (4.5366e-01)	Acc@1  82.81 ( 84.31)	Acc@5 100.00 ( 99.34)
Epoch: [8][370/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.6380e-01 (4.5289e-01)	Acc@1  86.72 ( 84.32)	Acc@5 100.00 ( 99.35)
Epoch: [8][380/391]	Time  0.038 ( 0.038)	Data  0.002 ( 0.003)	Loss 4.1749e-01 (4.5179e-01)	Acc@1  85.94 ( 84.37)	Acc@5 100.00 ( 99.36)
Epoch: [8][390/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.9403e-01 (4.5115e-01)	Acc@1  82.50 ( 84.40)	Acc@5 100.00 ( 99.35)
## e[8] optimizer.zero_grad (sum) time: 0.175767183303833
## e[8]       loss.backward (sum) time: 2.9742534160614014
## e[8]      optimizer.step (sum) time: 1.054450511932373
## epoch[8] training(only) time: 15.106740236282349
# Switched to evaluate mode...
Test: [  0/100]	Time  0.179 ( 0.179)	Loss 3.9835e-01 (3.9835e-01)	Acc@1  86.00 ( 86.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.019 ( 0.034)	Loss 6.4876e-01 (4.3832e-01)	Acc@1  80.00 ( 85.18)	Acc@5  98.00 ( 99.36)
Test: [ 20/100]	Time  0.023 ( 0.027)	Loss 4.5042e-01 (4.5488e-01)	Acc@1  82.00 ( 84.43)	Acc@5 100.00 ( 99.10)
Test: [ 30/100]	Time  0.017 ( 0.024)	Loss 5.0765e-01 (4.8277e-01)	Acc@1  80.00 ( 83.87)	Acc@5  99.00 ( 99.10)
Test: [ 40/100]	Time  0.022 ( 0.022)	Loss 5.2779e-01 (4.8887e-01)	Acc@1  87.00 ( 83.46)	Acc@5  98.00 ( 98.98)
Test: [ 50/100]	Time  0.021 ( 0.021)	Loss 3.1096e-01 (4.8127e-01)	Acc@1  87.00 ( 83.53)	Acc@5 100.00 ( 99.08)
Test: [ 60/100]	Time  0.019 ( 0.021)	Loss 4.0655e-01 (4.8349e-01)	Acc@1  84.00 ( 83.52)	Acc@5 100.00 ( 99.13)
Test: [ 70/100]	Time  0.022 ( 0.021)	Loss 6.6609e-01 (4.8363e-01)	Acc@1  79.00 ( 83.49)	Acc@5  99.00 ( 99.17)
Test: [ 80/100]	Time  0.017 ( 0.020)	Loss 3.2124e-01 (4.8335e-01)	Acc@1  84.00 ( 83.36)	Acc@5 100.00 ( 99.19)
Test: [ 90/100]	Time  0.017 ( 0.020)	Loss 3.8725e-01 (4.8040e-01)	Acc@1  86.00 ( 83.43)	Acc@5 100.00 ( 99.21)
 * Acc@1 83.550 Acc@5 99.200
### epoch[8] execution time: 17.219895839691162
EPOCH 9
# current learning rate: 0.1
# Switched to train mode...
Epoch: [9][  0/391]	Time  0.214 ( 0.214)	Data  0.180 ( 0.180)	Loss 4.5359e-01 (4.5359e-01)	Acc@1  83.59 ( 83.59)	Acc@5  99.22 ( 99.22)
Epoch: [9][ 10/391]	Time  0.037 ( 0.054)	Data  0.001 ( 0.018)	Loss 5.3913e-01 (4.5404e-01)	Acc@1  82.03 ( 84.66)	Acc@5 100.00 ( 99.43)
Epoch: [9][ 20/391]	Time  0.036 ( 0.046)	Data  0.001 ( 0.011)	Loss 2.7715e-01 (4.4634e-01)	Acc@1  92.19 ( 84.52)	Acc@5  98.44 ( 99.44)
Epoch: [9][ 30/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.008)	Loss 4.2856e-01 (4.3446e-01)	Acc@1  82.81 ( 84.75)	Acc@5  98.44 ( 99.47)
Epoch: [9][ 40/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.007)	Loss 4.7554e-01 (4.3932e-01)	Acc@1  84.38 ( 84.68)	Acc@5  99.22 ( 99.43)
Epoch: [9][ 50/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.006)	Loss 3.0576e-01 (4.2558e-01)	Acc@1  85.94 ( 85.02)	Acc@5 100.00 ( 99.51)
Epoch: [9][ 60/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.005)	Loss 3.9486e-01 (4.1781e-01)	Acc@1  85.94 ( 85.25)	Acc@5  99.22 ( 99.50)
Epoch: [9][ 70/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.005)	Loss 5.1407e-01 (4.2060e-01)	Acc@1  85.16 ( 85.32)	Acc@5  98.44 ( 99.44)
Epoch: [9][ 80/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 4.5475e-01 (4.2183e-01)	Acc@1  83.59 ( 85.29)	Acc@5  99.22 ( 99.45)
Epoch: [9][ 90/391]	Time  0.036 ( 0.040)	Data  0.002 ( 0.004)	Loss 4.3003e-01 (4.2254e-01)	Acc@1  85.16 ( 85.24)	Acc@5  98.44 ( 99.45)
Epoch: [9][100/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 4.5606e-01 (4.2234e-01)	Acc@1  83.59 ( 85.33)	Acc@5  99.22 ( 99.45)
Epoch: [9][110/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.004)	Loss 5.0195e-01 (4.2346e-01)	Acc@1  83.59 ( 85.25)	Acc@5  97.66 ( 99.41)
Epoch: [9][120/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.7538e-01 (4.2530e-01)	Acc@1  92.97 ( 85.20)	Acc@5 100.00 ( 99.40)
Epoch: [9][130/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 4.3379e-01 (4.1850e-01)	Acc@1  85.94 ( 85.44)	Acc@5  99.22 ( 99.39)
Epoch: [9][140/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.3751e-01 (4.2192e-01)	Acc@1  85.16 ( 85.36)	Acc@5  98.44 ( 99.36)
Epoch: [9][150/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.0232e-01 (4.2296e-01)	Acc@1  83.59 ( 85.31)	Acc@5 100.00 ( 99.35)
Epoch: [9][160/391]	Time  0.040 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.9335e-01 (4.2248e-01)	Acc@1  88.28 ( 85.31)	Acc@5 100.00 ( 99.34)
Epoch: [9][170/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.3837e-01 (4.2242e-01)	Acc@1  87.50 ( 85.30)	Acc@5 100.00 ( 99.36)
Epoch: [9][180/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.4204e-01 (4.2285e-01)	Acc@1  82.03 ( 85.32)	Acc@5  99.22 ( 99.36)
Epoch: [9][190/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.9178e-01 (4.2505e-01)	Acc@1  87.50 ( 85.25)	Acc@5 100.00 ( 99.37)
Epoch: [9][200/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.9288e-01 (4.2404e-01)	Acc@1  84.38 ( 85.32)	Acc@5  99.22 ( 99.37)
Epoch: [9][210/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.7286e-01 (4.2216e-01)	Acc@1  82.81 ( 85.40)	Acc@5  98.44 ( 99.38)
Epoch: [9][220/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.3779e-01 (4.2277e-01)	Acc@1  85.16 ( 85.39)	Acc@5 100.00 ( 99.39)
Epoch: [9][230/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.3541e-01 (4.2468e-01)	Acc@1  89.84 ( 85.37)	Acc@5 100.00 ( 99.38)
Epoch: [9][240/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.4973e-01 (4.2487e-01)	Acc@1  85.94 ( 85.33)	Acc@5 100.00 ( 99.38)
Epoch: [9][250/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.1418e-01 (4.2437e-01)	Acc@1  86.72 ( 85.37)	Acc@5  99.22 ( 99.39)
Epoch: [9][260/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.0832e-01 (4.2569e-01)	Acc@1  86.72 ( 85.32)	Acc@5  99.22 ( 99.38)
Epoch: [9][270/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.5102e-01 (4.2556e-01)	Acc@1  87.50 ( 85.27)	Acc@5 100.00 ( 99.39)
Epoch: [9][280/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.0051e-01 (4.2488e-01)	Acc@1  85.16 ( 85.30)	Acc@5  98.44 ( 99.39)
Epoch: [9][290/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.8371e-01 (4.2601e-01)	Acc@1  82.81 ( 85.26)	Acc@5  98.44 ( 99.37)
Epoch: [9][300/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.9176e-01 (4.2661e-01)	Acc@1  85.16 ( 85.27)	Acc@5 100.00 ( 99.38)
Epoch: [9][310/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.9030e-01 (4.2688e-01)	Acc@1  84.38 ( 85.23)	Acc@5 100.00 ( 99.38)
Epoch: [9][320/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.8893e-01 (4.2533e-01)	Acc@1  88.28 ( 85.29)	Acc@5  99.22 ( 99.39)
Epoch: [9][330/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.5796e-01 (4.2444e-01)	Acc@1  90.62 ( 85.33)	Acc@5 100.00 ( 99.40)
Epoch: [9][340/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.8595e-01 (4.2254e-01)	Acc@1  86.72 ( 85.40)	Acc@5 100.00 ( 99.40)
Epoch: [9][350/391]	Time  0.039 ( 0.038)	Data  0.002 ( 0.003)	Loss 4.5968e-01 (4.2195e-01)	Acc@1  85.94 ( 85.43)	Acc@5 100.00 ( 99.41)
Epoch: [9][360/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.1732e-01 (4.2082e-01)	Acc@1  86.72 ( 85.47)	Acc@5 100.00 ( 99.41)
Epoch: [9][370/391]	Time  0.036 ( 0.038)	Data  0.002 ( 0.003)	Loss 3.4260e-01 (4.2136e-01)	Acc@1  87.50 ( 85.47)	Acc@5 100.00 ( 99.41)
Epoch: [9][380/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.2204e-01 (4.2190e-01)	Acc@1  84.38 ( 85.44)	Acc@5 100.00 ( 99.41)
Epoch: [9][390/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.8600e-01 (4.2064e-01)	Acc@1  87.50 ( 85.49)	Acc@5 100.00 ( 99.41)
## e[9] optimizer.zero_grad (sum) time: 0.17318010330200195
## e[9]       loss.backward (sum) time: 3.014768362045288
## e[9]      optimizer.step (sum) time: 1.0169355869293213
## epoch[9] training(only) time: 15.121174097061157
# Switched to evaluate mode...
Test: [  0/100]	Time  0.166 ( 0.166)	Loss 4.6378e-01 (4.6378e-01)	Acc@1  83.00 ( 83.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.023 ( 0.032)	Loss 5.1168e-01 (4.8812e-01)	Acc@1  81.00 ( 83.00)	Acc@5  99.00 ( 99.36)
Test: [ 20/100]	Time  0.017 ( 0.026)	Loss 5.4136e-01 (4.8459e-01)	Acc@1  79.00 ( 83.76)	Acc@5  99.00 ( 98.95)
Test: [ 30/100]	Time  0.019 ( 0.024)	Loss 7.2538e-01 (4.9871e-01)	Acc@1  76.00 ( 83.55)	Acc@5 100.00 ( 99.00)
Test: [ 40/100]	Time  0.016 ( 0.023)	Loss 4.7528e-01 (5.0086e-01)	Acc@1  83.00 ( 83.37)	Acc@5  98.00 ( 98.90)
Test: [ 50/100]	Time  0.016 ( 0.022)	Loss 4.8240e-01 (5.0235e-01)	Acc@1  81.00 ( 83.29)	Acc@5  99.00 ( 98.92)
Test: [ 60/100]	Time  0.016 ( 0.021)	Loss 4.3349e-01 (5.0403e-01)	Acc@1  85.00 ( 83.11)	Acc@5 100.00 ( 99.00)
Test: [ 70/100]	Time  0.020 ( 0.021)	Loss 4.2749e-01 (5.0600e-01)	Acc@1  83.00 ( 83.14)	Acc@5  99.00 ( 98.96)
Test: [ 80/100]	Time  0.023 ( 0.021)	Loss 4.7862e-01 (5.0765e-01)	Acc@1  84.00 ( 83.12)	Acc@5  99.00 ( 98.89)
Test: [ 90/100]	Time  0.022 ( 0.021)	Loss 4.0673e-01 (5.0624e-01)	Acc@1  86.00 ( 83.07)	Acc@5 100.00 ( 98.89)
 * Acc@1 83.140 Acc@5 98.920
### epoch[9] execution time: 17.29652500152588
EPOCH 10
# current learning rate: 0.1
# Switched to train mode...
Epoch: [10][  0/391]	Time  0.217 ( 0.217)	Data  0.183 ( 0.183)	Loss 3.5274e-01 (3.5274e-01)	Acc@1  91.41 ( 91.41)	Acc@5  99.22 ( 99.22)
Epoch: [10][ 10/391]	Time  0.036 ( 0.054)	Data  0.001 ( 0.019)	Loss 4.2298e-01 (3.7983e-01)	Acc@1  83.59 ( 87.71)	Acc@5  99.22 ( 99.15)
Epoch: [10][ 20/391]	Time  0.036 ( 0.046)	Data  0.001 ( 0.011)	Loss 3.7889e-01 (3.9086e-01)	Acc@1  85.94 ( 87.54)	Acc@5 100.00 ( 99.22)
Epoch: [10][ 30/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.008)	Loss 3.6757e-01 (3.8681e-01)	Acc@1  85.16 ( 87.15)	Acc@5 100.00 ( 99.32)
Epoch: [10][ 40/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.007)	Loss 3.8614e-01 (3.7703e-01)	Acc@1  89.06 ( 87.21)	Acc@5  99.22 ( 99.41)
Epoch: [10][ 50/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.006)	Loss 3.5919e-01 (3.7713e-01)	Acc@1  85.94 ( 87.24)	Acc@5 100.00 ( 99.43)
Epoch: [10][ 60/391]	Time  0.038 ( 0.041)	Data  0.002 ( 0.005)	Loss 2.9230e-01 (3.7187e-01)	Acc@1  90.62 ( 87.41)	Acc@5 100.00 ( 99.49)
Epoch: [10][ 70/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.005)	Loss 4.0417e-01 (3.7584e-01)	Acc@1  85.94 ( 87.36)	Acc@5 100.00 ( 99.50)
Epoch: [10][ 80/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.4146e-01 (3.7103e-01)	Acc@1  91.41 ( 87.50)	Acc@5 100.00 ( 99.54)
Epoch: [10][ 90/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.5354e-01 (3.7383e-01)	Acc@1  90.62 ( 87.35)	Acc@5 100.00 ( 99.48)
Epoch: [10][100/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 3.8158e-01 (3.7794e-01)	Acc@1  87.50 ( 87.17)	Acc@5 100.00 ( 99.50)
Epoch: [10][110/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 3.8506e-01 (3.8111e-01)	Acc@1  84.38 ( 87.07)	Acc@5 100.00 ( 99.51)
Epoch: [10][120/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 4.8339e-01 (3.8289e-01)	Acc@1  82.81 ( 87.02)	Acc@5  99.22 ( 99.52)
Epoch: [10][130/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 4.4399e-01 (3.8540e-01)	Acc@1  82.81 ( 86.96)	Acc@5 100.00 ( 99.52)
Epoch: [10][140/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 3.2005e-01 (3.8297e-01)	Acc@1  88.28 ( 87.07)	Acc@5 100.00 ( 99.53)
Epoch: [10][150/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.4284e-01 (3.8498e-01)	Acc@1  86.72 ( 86.96)	Acc@5  99.22 ( 99.56)
Epoch: [10][160/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.7324e-01 (3.8509e-01)	Acc@1  88.28 ( 86.92)	Acc@5 100.00 ( 99.55)
Epoch: [10][170/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.3116e-01 (3.8384e-01)	Acc@1  89.84 ( 86.98)	Acc@5  99.22 ( 99.55)
Epoch: [10][180/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.2673e-01 (3.8248e-01)	Acc@1  89.06 ( 87.00)	Acc@5 100.00 ( 99.57)
Epoch: [10][190/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.8219e-01 (3.8191e-01)	Acc@1  89.06 ( 86.97)	Acc@5 100.00 ( 99.57)
Epoch: [10][200/391]	Time  0.040 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.9800e-01 (3.8347e-01)	Acc@1  85.16 ( 86.92)	Acc@5  99.22 ( 99.57)
Epoch: [10][210/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.5675e-01 (3.8384e-01)	Acc@1  85.16 ( 86.86)	Acc@5 100.00 ( 99.59)
Epoch: [10][220/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.9152e-01 (3.8476e-01)	Acc@1  85.16 ( 86.81)	Acc@5  98.44 ( 99.60)
Epoch: [10][230/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.7755e-01 (3.8699e-01)	Acc@1  80.47 ( 86.71)	Acc@5 100.00 ( 99.57)
Epoch: [10][240/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.5296e-01 (3.8762e-01)	Acc@1  85.16 ( 86.69)	Acc@5  98.44 ( 99.56)
Epoch: [10][250/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.1322e-01 (3.8852e-01)	Acc@1  86.72 ( 86.63)	Acc@5 100.00 ( 99.57)
Epoch: [10][260/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.7065e-01 (3.8832e-01)	Acc@1  84.38 ( 86.61)	Acc@5 100.00 ( 99.58)
Epoch: [10][270/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.6098e-01 (3.8748e-01)	Acc@1  83.59 ( 86.65)	Acc@5  99.22 ( 99.58)
Epoch: [10][280/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.5791e-01 (3.8705e-01)	Acc@1  81.25 ( 86.64)	Acc@5  99.22 ( 99.58)
Epoch: [10][290/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.7012e-01 (3.8712e-01)	Acc@1  82.03 ( 86.65)	Acc@5  99.22 ( 99.58)
Epoch: [10][300/391]	Time  0.033 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.3152e-01 (3.8757e-01)	Acc@1  87.50 ( 86.65)	Acc@5 100.00 ( 99.58)
Epoch: [10][310/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.6099e-01 (3.8711e-01)	Acc@1  84.38 ( 86.65)	Acc@5  99.22 ( 99.59)
Epoch: [10][320/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.6810e-01 (3.8686e-01)	Acc@1  86.72 ( 86.65)	Acc@5  98.44 ( 99.57)
Epoch: [10][330/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.2497e-01 (3.8702e-01)	Acc@1  89.06 ( 86.65)	Acc@5 100.00 ( 99.57)
Epoch: [10][340/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.2740e-01 (3.8666e-01)	Acc@1  82.03 ( 86.68)	Acc@5 100.00 ( 99.57)
Epoch: [10][350/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.6169e-01 (3.8592e-01)	Acc@1  86.72 ( 86.72)	Acc@5 100.00 ( 99.57)
Epoch: [10][360/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.0019e-01 (3.8595e-01)	Acc@1  82.03 ( 86.71)	Acc@5  97.66 ( 99.57)
Epoch: [10][370/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.3968e-01 (3.8543e-01)	Acc@1  86.72 ( 86.72)	Acc@5 100.00 ( 99.56)
Epoch: [10][380/391]	Time  0.036 ( 0.038)	Data  0.002 ( 0.003)	Loss 4.3047e-01 (3.8509e-01)	Acc@1  87.50 ( 86.74)	Acc@5 100.00 ( 99.56)
Epoch: [10][390/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.2549e-01 (3.8656e-01)	Acc@1  88.75 ( 86.67)	Acc@5 100.00 ( 99.56)
## e[10] optimizer.zero_grad (sum) time: 0.17463064193725586
## e[10]       loss.backward (sum) time: 2.9722137451171875
## e[10]      optimizer.step (sum) time: 1.0253245830535889
## epoch[10] training(only) time: 15.189131021499634
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 3.8034e-01 (3.8034e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.017 ( 0.033)	Loss 5.5114e-01 (4.2205e-01)	Acc@1  83.00 ( 85.91)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.017 ( 0.026)	Loss 4.6744e-01 (4.4000e-01)	Acc@1  85.00 ( 85.67)	Acc@5 100.00 ( 99.29)
Test: [ 30/100]	Time  0.017 ( 0.024)	Loss 5.2631e-01 (4.4371e-01)	Acc@1  83.00 ( 85.61)	Acc@5  99.00 ( 99.35)
Test: [ 40/100]	Time  0.022 ( 0.023)	Loss 4.7892e-01 (4.4595e-01)	Acc@1  84.00 ( 85.41)	Acc@5  99.00 ( 99.37)
Test: [ 50/100]	Time  0.018 ( 0.022)	Loss 3.5828e-01 (4.4580e-01)	Acc@1  85.00 ( 85.43)	Acc@5 100.00 ( 99.35)
Test: [ 60/100]	Time  0.017 ( 0.021)	Loss 4.4860e-01 (4.4547e-01)	Acc@1  86.00 ( 85.36)	Acc@5 100.00 ( 99.36)
Test: [ 70/100]	Time  0.024 ( 0.021)	Loss 5.5029e-01 (4.4442e-01)	Acc@1  83.00 ( 85.45)	Acc@5 100.00 ( 99.38)
Test: [ 80/100]	Time  0.016 ( 0.021)	Loss 3.1920e-01 (4.4309e-01)	Acc@1  87.00 ( 85.42)	Acc@5 100.00 ( 99.43)
Test: [ 90/100]	Time  0.016 ( 0.021)	Loss 2.5127e-01 (4.3860e-01)	Acc@1  89.00 ( 85.36)	Acc@5 100.00 ( 99.43)
 * Acc@1 85.290 Acc@5 99.430
### epoch[10] execution time: 17.358540534973145
EPOCH 11
# current learning rate: 0.1
# Switched to train mode...
Epoch: [11][  0/391]	Time  0.207 ( 0.207)	Data  0.173 ( 0.173)	Loss 2.5671e-01 (2.5671e-01)	Acc@1  92.19 ( 92.19)	Acc@5 100.00 (100.00)
Epoch: [11][ 10/391]	Time  0.037 ( 0.053)	Data  0.001 ( 0.018)	Loss 3.1234e-01 (3.3837e-01)	Acc@1  90.62 ( 89.06)	Acc@5  99.22 ( 99.50)
Epoch: [11][ 20/391]	Time  0.035 ( 0.046)	Data  0.001 ( 0.010)	Loss 3.6368e-01 (3.3638e-01)	Acc@1  86.72 ( 88.54)	Acc@5 100.00 ( 99.59)
Epoch: [11][ 30/391]	Time  0.036 ( 0.043)	Data  0.001 ( 0.008)	Loss 5.0144e-01 (3.4363e-01)	Acc@1  82.03 ( 88.23)	Acc@5  98.44 ( 99.55)
Epoch: [11][ 40/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.006)	Loss 3.0935e-01 (3.4249e-01)	Acc@1  85.94 ( 88.21)	Acc@5  99.22 ( 99.58)
Epoch: [11][ 50/391]	Time  0.035 ( 0.041)	Data  0.001 ( 0.006)	Loss 4.3578e-01 (3.4922e-01)	Acc@1  84.38 ( 87.81)	Acc@5 100.00 ( 99.57)
Epoch: [11][ 60/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.005)	Loss 4.0476e-01 (3.5311e-01)	Acc@1  85.16 ( 87.63)	Acc@5 100.00 ( 99.59)
Epoch: [11][ 70/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.005)	Loss 5.0361e-01 (3.5553e-01)	Acc@1  79.69 ( 87.57)	Acc@5 100.00 ( 99.60)
Epoch: [11][ 80/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 4.7978e-01 (3.5772e-01)	Acc@1  81.25 ( 87.49)	Acc@5 100.00 ( 99.59)
Epoch: [11][ 90/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 3.8970e-01 (3.5654e-01)	Acc@1  83.59 ( 87.51)	Acc@5  99.22 ( 99.62)
Epoch: [11][100/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.004)	Loss 3.3416e-01 (3.5780e-01)	Acc@1  85.94 ( 87.46)	Acc@5 100.00 ( 99.64)
Epoch: [11][110/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.9056e-01 (3.5856e-01)	Acc@1  91.41 ( 87.47)	Acc@5 100.00 ( 99.62)
Epoch: [11][120/391]	Time  0.038 ( 0.039)	Data  0.003 ( 0.004)	Loss 3.3798e-01 (3.5867e-01)	Acc@1  88.28 ( 87.46)	Acc@5 100.00 ( 99.63)
Epoch: [11][130/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 4.1318e-01 (3.5926e-01)	Acc@1  89.06 ( 87.45)	Acc@5 100.00 ( 99.64)
Epoch: [11][140/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.2206e-01 (3.5859e-01)	Acc@1  86.72 ( 87.46)	Acc@5  99.22 ( 99.64)
Epoch: [11][150/391]	Time  0.043 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.1720e-01 (3.6085e-01)	Acc@1  87.50 ( 87.39)	Acc@5 100.00 ( 99.65)
Epoch: [11][160/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.2891e-01 (3.5846e-01)	Acc@1  90.62 ( 87.53)	Acc@5  98.44 ( 99.63)
Epoch: [11][170/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.6231e-01 (3.5941e-01)	Acc@1  89.06 ( 87.57)	Acc@5 100.00 ( 99.62)
Epoch: [11][180/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.4651e-01 (3.5894e-01)	Acc@1  84.38 ( 87.59)	Acc@5  98.44 ( 99.62)
Epoch: [11][190/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.8872e-01 (3.6098e-01)	Acc@1  87.50 ( 87.53)	Acc@5 100.00 ( 99.61)
Epoch: [11][200/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.2827e-01 (3.6066e-01)	Acc@1  89.06 ( 87.56)	Acc@5  99.22 ( 99.61)
Epoch: [11][210/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.6949e-01 (3.6019e-01)	Acc@1  85.94 ( 87.59)	Acc@5 100.00 ( 99.61)
Epoch: [11][220/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.5928e-01 (3.5879e-01)	Acc@1  86.72 ( 87.62)	Acc@5 100.00 ( 99.61)
Epoch: [11][230/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.2377e-01 (3.5789e-01)	Acc@1  89.06 ( 87.61)	Acc@5 100.00 ( 99.62)
Epoch: [11][240/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.2295e-01 (3.5767e-01)	Acc@1  88.28 ( 87.60)	Acc@5 100.00 ( 99.62)
Epoch: [11][250/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.0930e-01 (3.5754e-01)	Acc@1  90.62 ( 87.57)	Acc@5  99.22 ( 99.61)
Epoch: [11][260/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.4741e-01 (3.5849e-01)	Acc@1  88.28 ( 87.51)	Acc@5 100.00 ( 99.61)
Epoch: [11][270/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.0258e-01 (3.5928e-01)	Acc@1  88.28 ( 87.50)	Acc@5 100.00 ( 99.62)
Epoch: [11][280/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.6023e-01 (3.5957e-01)	Acc@1  87.50 ( 87.49)	Acc@5  99.22 ( 99.61)
Epoch: [11][290/391]	Time  0.038 ( 0.038)	Data  0.002 ( 0.003)	Loss 3.0936e-01 (3.5799e-01)	Acc@1  87.50 ( 87.56)	Acc@5 100.00 ( 99.62)
Epoch: [11][300/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.7937e-01 (3.5769e-01)	Acc@1  85.16 ( 87.56)	Acc@5 100.00 ( 99.63)
Epoch: [11][310/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.2010e-01 (3.5657e-01)	Acc@1  88.28 ( 87.56)	Acc@5 100.00 ( 99.64)
Epoch: [11][320/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.0904e-01 (3.5711e-01)	Acc@1  85.94 ( 87.57)	Acc@5  99.22 ( 99.63)
Epoch: [11][330/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.8734e-01 (3.5751e-01)	Acc@1  89.06 ( 87.57)	Acc@5 100.00 ( 99.62)
Epoch: [11][340/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.0410e-01 (3.5733e-01)	Acc@1  82.81 ( 87.57)	Acc@5  99.22 ( 99.62)
Epoch: [11][350/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.5634e-01 (3.5699e-01)	Acc@1  90.62 ( 87.60)	Acc@5  99.22 ( 99.61)
Epoch: [11][360/391]	Time  0.038 ( 0.038)	Data  0.002 ( 0.003)	Loss 4.2857e-01 (3.5773e-01)	Acc@1  87.50 ( 87.58)	Acc@5 100.00 ( 99.61)
Epoch: [11][370/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.8726e-01 (3.5992e-01)	Acc@1  83.59 ( 87.51)	Acc@5  98.44 ( 99.59)
Epoch: [11][380/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.4174e-01 (3.5972e-01)	Acc@1  91.41 ( 87.51)	Acc@5  99.22 ( 99.59)
Epoch: [11][390/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.3283e-01 (3.5988e-01)	Acc@1  88.75 ( 87.50)	Acc@5 100.00 ( 99.59)
## e[11] optimizer.zero_grad (sum) time: 0.17508864402770996
## e[11]       loss.backward (sum) time: 2.970702886581421
## e[11]      optimizer.step (sum) time: 0.9995627403259277
## epoch[11] training(only) time: 15.092746496200562
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 4.3264e-01 (4.3264e-01)	Acc@1  82.00 ( 82.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.016 ( 0.034)	Loss 5.4142e-01 (4.3202e-01)	Acc@1  82.00 ( 85.36)	Acc@5 100.00 ( 99.45)
Test: [ 20/100]	Time  0.018 ( 0.027)	Loss 4.7405e-01 (4.3382e-01)	Acc@1  83.00 ( 85.62)	Acc@5 100.00 ( 99.19)
Test: [ 30/100]	Time  0.023 ( 0.024)	Loss 3.7055e-01 (4.3809e-01)	Acc@1  85.00 ( 85.23)	Acc@5  98.00 ( 99.16)
Test: [ 40/100]	Time  0.016 ( 0.023)	Loss 4.5403e-01 (4.3886e-01)	Acc@1  85.00 ( 85.27)	Acc@5  99.00 ( 99.10)
Test: [ 50/100]	Time  0.016 ( 0.022)	Loss 2.6398e-01 (4.3539e-01)	Acc@1  91.00 ( 85.61)	Acc@5 100.00 ( 99.10)
Test: [ 60/100]	Time  0.020 ( 0.021)	Loss 3.8041e-01 (4.3239e-01)	Acc@1  88.00 ( 85.64)	Acc@5 100.00 ( 99.18)
Test: [ 70/100]	Time  0.020 ( 0.021)	Loss 4.0409e-01 (4.2638e-01)	Acc@1  85.00 ( 85.76)	Acc@5 100.00 ( 99.24)
Test: [ 80/100]	Time  0.016 ( 0.020)	Loss 3.0030e-01 (4.2502e-01)	Acc@1  90.00 ( 85.80)	Acc@5 100.00 ( 99.30)
Test: [ 90/100]	Time  0.016 ( 0.020)	Loss 4.7438e-01 (4.2796e-01)	Acc@1  88.00 ( 85.74)	Acc@5  98.00 ( 99.31)
 * Acc@1 85.740 Acc@5 99.320
### epoch[11] execution time: 17.22205924987793
EPOCH 12
# current learning rate: 0.1
# Switched to train mode...
Epoch: [12][  0/391]	Time  0.215 ( 0.215)	Data  0.181 ( 0.181)	Loss 3.6767e-01 (3.6767e-01)	Acc@1  88.28 ( 88.28)	Acc@5 100.00 (100.00)
Epoch: [12][ 10/391]	Time  0.034 ( 0.054)	Data  0.001 ( 0.018)	Loss 3.0039e-01 (3.1970e-01)	Acc@1  85.94 ( 88.92)	Acc@5 100.00 ( 99.86)
Epoch: [12][ 20/391]	Time  0.038 ( 0.046)	Data  0.001 ( 0.011)	Loss 3.7061e-01 (3.3378e-01)	Acc@1  87.50 ( 88.32)	Acc@5 100.00 ( 99.74)
Epoch: [12][ 30/391]	Time  0.037 ( 0.044)	Data  0.001 ( 0.008)	Loss 2.4166e-01 (3.2762e-01)	Acc@1  92.97 ( 88.43)	Acc@5 100.00 ( 99.77)
Epoch: [12][ 40/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.006)	Loss 3.7038e-01 (3.3134e-01)	Acc@1  87.50 ( 88.11)	Acc@5  99.22 ( 99.73)
Epoch: [12][ 50/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.006)	Loss 3.5344e-01 (3.3880e-01)	Acc@1  87.50 ( 88.14)	Acc@5  99.22 ( 99.71)
Epoch: [12][ 60/391]	Time  0.035 ( 0.041)	Data  0.001 ( 0.005)	Loss 3.1701e-01 (3.3785e-01)	Acc@1  88.28 ( 88.31)	Acc@5 100.00 ( 99.73)
Epoch: [12][ 70/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.005)	Loss 3.4661e-01 (3.3664e-01)	Acc@1  84.38 ( 88.30)	Acc@5  99.22 ( 99.72)
Epoch: [12][ 80/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 3.4310e-01 (3.3636e-01)	Acc@1  85.16 ( 88.18)	Acc@5  99.22 ( 99.73)
Epoch: [12][ 90/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 4.1173e-01 (3.3575e-01)	Acc@1  84.38 ( 88.13)	Acc@5  99.22 ( 99.73)
Epoch: [12][100/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.6589e-01 (3.2947e-01)	Acc@1  89.84 ( 88.26)	Acc@5 100.00 ( 99.72)
Epoch: [12][110/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.5751e-01 (3.3189e-01)	Acc@1  89.84 ( 88.22)	Acc@5 100.00 ( 99.70)
Epoch: [12][120/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 3.5328e-01 (3.3536e-01)	Acc@1  85.94 ( 88.04)	Acc@5 100.00 ( 99.70)
Epoch: [12][130/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.7632e-01 (3.3478e-01)	Acc@1  91.41 ( 88.12)	Acc@5 100.00 ( 99.70)
Epoch: [12][140/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.1987e-01 (3.3530e-01)	Acc@1  89.06 ( 88.13)	Acc@5 100.00 ( 99.70)
Epoch: [12][150/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.0279e-01 (3.3634e-01)	Acc@1  85.16 ( 88.11)	Acc@5  99.22 ( 99.67)
Epoch: [12][160/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.9017e-01 (3.3471e-01)	Acc@1  89.84 ( 88.16)	Acc@5  99.22 ( 99.67)
Epoch: [12][170/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.2296e-01 (3.3600e-01)	Acc@1  88.28 ( 88.11)	Acc@5 100.00 ( 99.67)
Epoch: [12][180/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.9660e-01 (3.3799e-01)	Acc@1  87.50 ( 88.01)	Acc@5  99.22 ( 99.67)
Epoch: [12][190/391]	Time  0.040 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.8800e-01 (3.3707e-01)	Acc@1  89.84 ( 88.03)	Acc@5 100.00 ( 99.69)
Epoch: [12][200/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.6214e-01 (3.3728e-01)	Acc@1  92.19 ( 88.05)	Acc@5 100.00 ( 99.69)
Epoch: [12][210/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.0906e-01 (3.3969e-01)	Acc@1  89.84 ( 87.95)	Acc@5 100.00 ( 99.68)
Epoch: [12][220/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.2090e-01 (3.4006e-01)	Acc@1  87.50 ( 87.96)	Acc@5 100.00 ( 99.67)
Epoch: [12][230/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.8237e-01 (3.4106e-01)	Acc@1  89.06 ( 87.96)	Acc@5 100.00 ( 99.68)
Epoch: [12][240/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.2510e-01 (3.4134e-01)	Acc@1  87.50 ( 87.98)	Acc@5 100.00 ( 99.68)
Epoch: [12][250/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.2326e-01 (3.4279e-01)	Acc@1  82.81 ( 87.93)	Acc@5  99.22 ( 99.68)
Epoch: [12][260/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.9511e-01 (3.4264e-01)	Acc@1  89.84 ( 87.94)	Acc@5  98.44 ( 99.67)
Epoch: [12][270/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.2637e-01 (3.4273e-01)	Acc@1  85.94 ( 87.96)	Acc@5 100.00 ( 99.67)
Epoch: [12][280/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.7559e-01 (3.4172e-01)	Acc@1  90.62 ( 87.99)	Acc@5  99.22 ( 99.68)
Epoch: [12][290/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.8271e-01 (3.4200e-01)	Acc@1  84.38 ( 87.97)	Acc@5 100.00 ( 99.68)
Epoch: [12][300/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.6964e-01 (3.4179e-01)	Acc@1  86.72 ( 87.98)	Acc@5 100.00 ( 99.69)
Epoch: [12][310/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.9753e-01 (3.4161e-01)	Acc@1  88.28 ( 87.98)	Acc@5  99.22 ( 99.69)
Epoch: [12][320/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.5686e-01 (3.4136e-01)	Acc@1  89.84 ( 88.02)	Acc@5 100.00 ( 99.68)
Epoch: [12][330/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.6693e-01 (3.4323e-01)	Acc@1  88.28 ( 87.96)	Acc@5  99.22 ( 99.67)
Epoch: [12][340/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.8600e-01 (3.4362e-01)	Acc@1  86.72 ( 87.96)	Acc@5  99.22 ( 99.66)
Epoch: [12][350/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.0876e-01 (3.4318e-01)	Acc@1  89.06 ( 87.99)	Acc@5  98.44 ( 99.66)
Epoch: [12][360/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.3002e-01 (3.4263e-01)	Acc@1  89.06 ( 87.98)	Acc@5  99.22 ( 99.65)
Epoch: [12][370/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.2004e-01 (3.4251e-01)	Acc@1  88.28 ( 87.99)	Acc@5 100.00 ( 99.65)
Epoch: [12][380/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.5417e-01 (3.4262e-01)	Acc@1  89.84 ( 88.01)	Acc@5 100.00 ( 99.65)
Epoch: [12][390/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.2174e-01 (3.4313e-01)	Acc@1  88.75 ( 88.01)	Acc@5 100.00 ( 99.65)
## e[12] optimizer.zero_grad (sum) time: 0.17370939254760742
## e[12]       loss.backward (sum) time: 2.9624853134155273
## e[12]      optimizer.step (sum) time: 1.04142427444458
## epoch[12] training(only) time: 15.13027048110962
# Switched to evaluate mode...
Test: [  0/100]	Time  0.178 ( 0.178)	Loss 4.1963e-01 (4.1963e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.019 ( 0.033)	Loss 3.6780e-01 (4.0267e-01)	Acc@1  89.00 ( 86.18)	Acc@5  99.00 ( 99.82)
Test: [ 20/100]	Time  0.019 ( 0.026)	Loss 4.9285e-01 (4.2019e-01)	Acc@1  82.00 ( 86.10)	Acc@5  99.00 ( 99.43)
Test: [ 30/100]	Time  0.016 ( 0.024)	Loss 4.0013e-01 (4.2169e-01)	Acc@1  85.00 ( 86.16)	Acc@5  99.00 ( 99.35)
Test: [ 40/100]	Time  0.016 ( 0.022)	Loss 4.4515e-01 (4.2653e-01)	Acc@1  87.00 ( 86.17)	Acc@5  98.00 ( 99.17)
Test: [ 50/100]	Time  0.024 ( 0.022)	Loss 4.4555e-01 (4.2293e-01)	Acc@1  86.00 ( 86.06)	Acc@5  99.00 ( 99.22)
Test: [ 60/100]	Time  0.019 ( 0.022)	Loss 3.6474e-01 (4.2894e-01)	Acc@1  88.00 ( 86.02)	Acc@5 100.00 ( 99.26)
Test: [ 70/100]	Time  0.024 ( 0.021)	Loss 5.6590e-01 (4.3061e-01)	Acc@1  83.00 ( 85.92)	Acc@5  99.00 ( 99.25)
Test: [ 80/100]	Time  0.016 ( 0.021)	Loss 3.6843e-01 (4.2813e-01)	Acc@1  86.00 ( 85.93)	Acc@5  99.00 ( 99.27)
Test: [ 90/100]	Time  0.016 ( 0.021)	Loss 2.4213e-01 (4.2481e-01)	Acc@1  92.00 ( 86.01)	Acc@5 100.00 ( 99.29)
 * Acc@1 85.980 Acc@5 99.300
### epoch[12] execution time: 17.3068106174469
EPOCH 13
# current learning rate: 0.1
# Switched to train mode...
Epoch: [13][  0/391]	Time  0.213 ( 0.213)	Data  0.176 ( 0.176)	Loss 2.8287e-01 (2.8287e-01)	Acc@1  90.62 ( 90.62)	Acc@5 100.00 (100.00)
Epoch: [13][ 10/391]	Time  0.036 ( 0.053)	Data  0.001 ( 0.018)	Loss 2.6840e-01 (3.1020e-01)	Acc@1  89.06 ( 89.42)	Acc@5 100.00 ( 99.79)
Epoch: [13][ 20/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.010)	Loss 2.5747e-01 (2.9378e-01)	Acc@1  91.41 ( 90.10)	Acc@5 100.00 ( 99.74)
Epoch: [13][ 30/391]	Time  0.037 ( 0.043)	Data  0.001 ( 0.008)	Loss 3.2926e-01 (3.0689e-01)	Acc@1  88.28 ( 89.67)	Acc@5  98.44 ( 99.67)
Epoch: [13][ 40/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.006)	Loss 3.0671e-01 (3.1262e-01)	Acc@1  89.06 ( 89.42)	Acc@5  99.22 ( 99.68)
Epoch: [13][ 50/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.006)	Loss 3.0526e-01 (3.1501e-01)	Acc@1  90.62 ( 89.32)	Acc@5 100.00 ( 99.65)
Epoch: [13][ 60/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.005)	Loss 2.6279e-01 (3.1437e-01)	Acc@1  93.75 ( 89.22)	Acc@5 100.00 ( 99.67)
Epoch: [13][ 70/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.005)	Loss 3.5549e-01 (3.1438e-01)	Acc@1  89.06 ( 89.26)	Acc@5 100.00 ( 99.66)
Epoch: [13][ 80/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 4.6122e-01 (3.1619e-01)	Acc@1  81.25 ( 89.09)	Acc@5  99.22 ( 99.66)
Epoch: [13][ 90/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 4.7858e-01 (3.1949e-01)	Acc@1  84.38 ( 88.95)	Acc@5 100.00 ( 99.67)
Epoch: [13][100/391]	Time  0.038 ( 0.040)	Data  0.002 ( 0.004)	Loss 2.9058e-01 (3.1961e-01)	Acc@1  91.41 ( 88.99)	Acc@5  99.22 ( 99.66)
Epoch: [13][110/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.004)	Loss 3.4182e-01 (3.1989e-01)	Acc@1  88.28 ( 88.98)	Acc@5 100.00 ( 99.68)
Epoch: [13][120/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 3.0522e-01 (3.1844e-01)	Acc@1  88.28 ( 89.02)	Acc@5 100.00 ( 99.66)
Epoch: [13][130/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 4.9242e-01 (3.1968e-01)	Acc@1  85.94 ( 89.01)	Acc@5 100.00 ( 99.66)
Epoch: [13][140/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.6489e-01 (3.2097e-01)	Acc@1  87.50 ( 89.00)	Acc@5  99.22 ( 99.66)
Epoch: [13][150/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.0134e-01 (3.2296e-01)	Acc@1  86.72 ( 88.87)	Acc@5 100.00 ( 99.67)
Epoch: [13][160/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.1313e-01 (3.2588e-01)	Acc@1  84.38 ( 88.77)	Acc@5  99.22 ( 99.65)
Epoch: [13][170/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.4084e-01 (3.2427e-01)	Acc@1  91.41 ( 88.82)	Acc@5 100.00 ( 99.66)
Epoch: [13][180/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.9823e-01 (3.2245e-01)	Acc@1  88.28 ( 88.87)	Acc@5  99.22 ( 99.67)
Epoch: [13][190/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.4542e-01 (3.2147e-01)	Acc@1  86.72 ( 88.94)	Acc@5  99.22 ( 99.67)
Epoch: [13][200/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.7577e-01 (3.2267e-01)	Acc@1  82.81 ( 88.88)	Acc@5 100.00 ( 99.69)
Epoch: [13][210/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.4970e-01 (3.2398e-01)	Acc@1  87.50 ( 88.81)	Acc@5 100.00 ( 99.69)
Epoch: [13][220/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.6312e-01 (3.2314e-01)	Acc@1  88.28 ( 88.84)	Acc@5 100.00 ( 99.69)
Epoch: [13][230/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.7537e-01 (3.2364e-01)	Acc@1  87.50 ( 88.85)	Acc@5  99.22 ( 99.68)
Epoch: [13][240/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.1395e-01 (3.2285e-01)	Acc@1  87.50 ( 88.88)	Acc@5 100.00 ( 99.69)
Epoch: [13][250/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.6510e-01 (3.2409e-01)	Acc@1  88.28 ( 88.83)	Acc@5 100.00 ( 99.69)
Epoch: [13][260/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.7519e-01 (3.2368e-01)	Acc@1  94.53 ( 88.84)	Acc@5 100.00 ( 99.69)
Epoch: [13][270/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.2864e-01 (3.2322e-01)	Acc@1  91.41 ( 88.85)	Acc@5 100.00 ( 99.68)
Epoch: [13][280/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.0294e-01 (3.2554e-01)	Acc@1  83.59 ( 88.79)	Acc@5 100.00 ( 99.68)
Epoch: [13][290/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.6688e-01 (3.2426e-01)	Acc@1  92.97 ( 88.86)	Acc@5 100.00 ( 99.68)
Epoch: [13][300/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.6977e-01 (3.2336e-01)	Acc@1  87.50 ( 88.87)	Acc@5  99.22 ( 99.68)
Epoch: [13][310/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.9198e-01 (3.2375e-01)	Acc@1  83.59 ( 88.86)	Acc@5 100.00 ( 99.68)
Epoch: [13][320/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.9313e-01 (3.2435e-01)	Acc@1  83.59 ( 88.81)	Acc@5  99.22 ( 99.68)
Epoch: [13][330/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.2305e-01 (3.2465e-01)	Acc@1  89.06 ( 88.80)	Acc@5 100.00 ( 99.68)
Epoch: [13][340/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.2683e-01 (3.2363e-01)	Acc@1  96.09 ( 88.83)	Acc@5 100.00 ( 99.69)
Epoch: [13][350/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.6492e-01 (3.2246e-01)	Acc@1  88.28 ( 88.87)	Acc@5 100.00 ( 99.70)
Epoch: [13][360/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.8948e-01 (3.2274e-01)	Acc@1  85.94 ( 88.87)	Acc@5  99.22 ( 99.69)
Epoch: [13][370/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.6010e-01 (3.2360e-01)	Acc@1  90.62 ( 88.85)	Acc@5 100.00 ( 99.69)
Epoch: [13][380/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.7232e-01 (3.2377e-01)	Acc@1  83.59 ( 88.83)	Acc@5 100.00 ( 99.70)
Epoch: [13][390/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.0031e-01 (3.2283e-01)	Acc@1  91.25 ( 88.87)	Acc@5 100.00 ( 99.70)
## e[13] optimizer.zero_grad (sum) time: 0.17408490180969238
## e[13]       loss.backward (sum) time: 2.955850601196289
## e[13]      optimizer.step (sum) time: 1.02378511428833
## epoch[13] training(only) time: 15.164743185043335
# Switched to evaluate mode...
Test: [  0/100]	Time  0.182 ( 0.182)	Loss 3.2722e-01 (3.2722e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.016 ( 0.034)	Loss 4.6622e-01 (4.3148e-01)	Acc@1  88.00 ( 86.18)	Acc@5  98.00 ( 99.55)
Test: [ 20/100]	Time  0.016 ( 0.027)	Loss 4.8836e-01 (4.4126e-01)	Acc@1  81.00 ( 85.67)	Acc@5 100.00 ( 99.52)
Test: [ 30/100]	Time  0.023 ( 0.024)	Loss 4.5031e-01 (4.4803e-01)	Acc@1  85.00 ( 85.42)	Acc@5 100.00 ( 99.48)
Test: [ 40/100]	Time  0.020 ( 0.023)	Loss 5.0823e-01 (4.4999e-01)	Acc@1  86.00 ( 85.59)	Acc@5  98.00 ( 99.39)
Test: [ 50/100]	Time  0.023 ( 0.022)	Loss 3.1101e-01 (4.4660e-01)	Acc@1  88.00 ( 85.65)	Acc@5  98.00 ( 99.35)
Test: [ 60/100]	Time  0.017 ( 0.022)	Loss 4.7246e-01 (4.4929e-01)	Acc@1  85.00 ( 85.64)	Acc@5 100.00 ( 99.36)
Test: [ 70/100]	Time  0.017 ( 0.022)	Loss 5.8439e-01 (4.5206e-01)	Acc@1  81.00 ( 85.42)	Acc@5 100.00 ( 99.41)
Test: [ 80/100]	Time  0.019 ( 0.021)	Loss 2.4374e-01 (4.5064e-01)	Acc@1  94.00 ( 85.36)	Acc@5 100.00 ( 99.43)
Test: [ 90/100]	Time  0.016 ( 0.021)	Loss 3.2963e-01 (4.5324e-01)	Acc@1  88.00 ( 85.24)	Acc@5 100.00 ( 99.45)
 * Acc@1 85.290 Acc@5 99.460
### epoch[13] execution time: 17.377729892730713
EPOCH 14
# current learning rate: 0.1
# Switched to train mode...
Epoch: [14][  0/391]	Time  0.214 ( 0.214)	Data  0.179 ( 0.179)	Loss 2.7300e-01 (2.7300e-01)	Acc@1  90.62 ( 90.62)	Acc@5 100.00 (100.00)
Epoch: [14][ 10/391]	Time  0.037 ( 0.054)	Data  0.001 ( 0.018)	Loss 3.1595e-01 (2.8461e-01)	Acc@1  90.62 ( 90.27)	Acc@5 100.00 ( 99.86)
Epoch: [14][ 20/391]	Time  0.037 ( 0.046)	Data  0.001 ( 0.011)	Loss 2.5502e-01 (2.8524e-01)	Acc@1  87.50 ( 90.18)	Acc@5 100.00 ( 99.81)
Epoch: [14][ 30/391]	Time  0.037 ( 0.044)	Data  0.001 ( 0.008)	Loss 2.5854e-01 (2.9296e-01)	Acc@1  89.84 ( 89.94)	Acc@5 100.00 ( 99.82)
Epoch: [14][ 40/391]	Time  0.035 ( 0.042)	Data  0.001 ( 0.007)	Loss 2.7992e-01 (2.9206e-01)	Acc@1  90.62 ( 89.88)	Acc@5 100.00 ( 99.77)
Epoch: [14][ 50/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.006)	Loss 3.4794e-01 (2.9297e-01)	Acc@1  88.28 ( 89.87)	Acc@5 100.00 ( 99.80)
Epoch: [14][ 60/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.005)	Loss 2.2761e-01 (2.8861e-01)	Acc@1  92.97 ( 90.09)	Acc@5 100.00 ( 99.80)
Epoch: [14][ 70/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.005)	Loss 4.0201e-01 (2.9070e-01)	Acc@1  86.72 ( 89.88)	Acc@5 100.00 ( 99.81)
Epoch: [14][ 80/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.8191e-01 (2.9103e-01)	Acc@1  92.19 ( 89.98)	Acc@5  99.22 ( 99.82)
Epoch: [14][ 90/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.5169e-01 (2.9377e-01)	Acc@1  96.09 ( 89.87)	Acc@5  99.22 ( 99.81)
Epoch: [14][100/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 3.0720e-01 (2.9353e-01)	Acc@1  89.84 ( 89.87)	Acc@5  99.22 ( 99.82)
Epoch: [14][110/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 4.5540e-01 (2.9714e-01)	Acc@1  86.72 ( 89.76)	Acc@5  99.22 ( 99.82)
Epoch: [14][120/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 3.3335e-01 (2.9702e-01)	Acc@1  87.50 ( 89.71)	Acc@5 100.00 ( 99.83)
Epoch: [14][130/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 3.4803e-01 (2.9672e-01)	Acc@1  86.72 ( 89.73)	Acc@5 100.00 ( 99.83)
Epoch: [14][140/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.7392e-01 (2.9693e-01)	Acc@1  88.28 ( 89.74)	Acc@5 100.00 ( 99.81)
Epoch: [14][150/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.2466e-01 (2.9709e-01)	Acc@1  90.62 ( 89.70)	Acc@5 100.00 ( 99.80)
Epoch: [14][160/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.4572e-01 (2.9457e-01)	Acc@1  87.50 ( 89.82)	Acc@5 100.00 ( 99.81)
Epoch: [14][170/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.4730e-01 (2.9479e-01)	Acc@1  89.06 ( 89.81)	Acc@5 100.00 ( 99.81)
Epoch: [14][180/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.0321e-01 (2.9723e-01)	Acc@1  84.38 ( 89.69)	Acc@5 100.00 ( 99.81)
Epoch: [14][190/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.1232e-01 (2.9907e-01)	Acc@1  90.62 ( 89.64)	Acc@5 100.00 ( 99.80)
Epoch: [14][200/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.0455e-01 (2.9922e-01)	Acc@1  89.06 ( 89.63)	Acc@5  99.22 ( 99.79)
Epoch: [14][210/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.5057e-01 (2.9883e-01)	Acc@1  89.06 ( 89.64)	Acc@5 100.00 ( 99.80)
Epoch: [14][220/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.4379e-01 (2.9861e-01)	Acc@1  92.97 ( 89.66)	Acc@5 100.00 ( 99.80)
Epoch: [14][230/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.4724e-01 (2.9890e-01)	Acc@1  88.28 ( 89.65)	Acc@5 100.00 ( 99.79)
Epoch: [14][240/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.6196e-01 (3.0076e-01)	Acc@1  88.28 ( 89.54)	Acc@5 100.00 ( 99.78)
Epoch: [14][250/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.8293e-01 (3.0032e-01)	Acc@1  89.84 ( 89.55)	Acc@5 100.00 ( 99.78)
Epoch: [14][260/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.4810e-01 (3.0032e-01)	Acc@1  82.81 ( 89.54)	Acc@5 100.00 ( 99.78)
Epoch: [14][270/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.3258e-01 (3.0048e-01)	Acc@1  88.28 ( 89.51)	Acc@5 100.00 ( 99.78)
Epoch: [14][280/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.5458e-01 (3.0009e-01)	Acc@1  89.06 ( 89.56)	Acc@5 100.00 ( 99.78)
Epoch: [14][290/391]	Time  0.040 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.8535e-01 (2.9995e-01)	Acc@1  92.19 ( 89.56)	Acc@5 100.00 ( 99.78)
Epoch: [14][300/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.4535e-01 (3.0008e-01)	Acc@1  91.41 ( 89.59)	Acc@5 100.00 ( 99.77)
Epoch: [14][310/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.3693e-01 (2.9951e-01)	Acc@1  90.62 ( 89.61)	Acc@5  99.22 ( 99.77)
Epoch: [14][320/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.4101e-01 (3.0069e-01)	Acc@1  92.19 ( 89.57)	Acc@5 100.00 ( 99.76)
Epoch: [14][330/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.0264e-01 (3.0127e-01)	Acc@1  89.84 ( 89.56)	Acc@5 100.00 ( 99.77)
Epoch: [14][340/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.4816e-01 (3.0163e-01)	Acc@1  90.62 ( 89.55)	Acc@5  99.22 ( 99.76)
Epoch: [14][350/391]	Time  0.036 ( 0.038)	Data  0.002 ( 0.003)	Loss 2.2777e-01 (3.0183e-01)	Acc@1  91.41 ( 89.55)	Acc@5 100.00 ( 99.76)
Epoch: [14][360/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.6330e-01 (3.0262e-01)	Acc@1  88.28 ( 89.51)	Acc@5 100.00 ( 99.75)
Epoch: [14][370/391]	Time  0.037 ( 0.038)	Data  0.004 ( 0.003)	Loss 2.0149e-01 (3.0267e-01)	Acc@1  92.97 ( 89.51)	Acc@5 100.00 ( 99.75)
Epoch: [14][380/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.6124e-01 (3.0260e-01)	Acc@1  89.06 ( 89.52)	Acc@5 100.00 ( 99.75)
Epoch: [14][390/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.5614e-01 (3.0138e-01)	Acc@1  85.00 ( 89.54)	Acc@5  98.75 ( 99.75)
## e[14] optimizer.zero_grad (sum) time: 0.1759810447692871
## e[14]       loss.backward (sum) time: 2.9930524826049805
## e[14]      optimizer.step (sum) time: 1.0301897525787354
## epoch[14] training(only) time: 15.127441167831421
# Switched to evaluate mode...
Test: [  0/100]	Time  0.180 ( 0.180)	Loss 3.0898e-01 (3.0898e-01)	Acc@1  91.00 ( 91.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.020 ( 0.033)	Loss 4.3108e-01 (3.6013e-01)	Acc@1  87.00 ( 87.27)	Acc@5 100.00 ( 99.55)
Test: [ 20/100]	Time  0.021 ( 0.026)	Loss 4.2412e-01 (3.7752e-01)	Acc@1  83.00 ( 86.76)	Acc@5  99.00 ( 99.29)
Test: [ 30/100]	Time  0.017 ( 0.024)	Loss 4.5998e-01 (3.9478e-01)	Acc@1  85.00 ( 86.68)	Acc@5  99.00 ( 99.35)
Test: [ 40/100]	Time  0.020 ( 0.022)	Loss 3.4182e-01 (3.9602e-01)	Acc@1  90.00 ( 86.88)	Acc@5  99.00 ( 99.29)
Test: [ 50/100]	Time  0.017 ( 0.021)	Loss 1.9808e-01 (3.9168e-01)	Acc@1  95.00 ( 87.00)	Acc@5 100.00 ( 99.29)
Test: [ 60/100]	Time  0.016 ( 0.021)	Loss 5.3788e-01 (3.9243e-01)	Acc@1  80.00 ( 86.74)	Acc@5 100.00 ( 99.38)
Test: [ 70/100]	Time  0.017 ( 0.021)	Loss 4.3653e-01 (3.9129e-01)	Acc@1  86.00 ( 86.82)	Acc@5  99.00 ( 99.38)
Test: [ 80/100]	Time  0.021 ( 0.020)	Loss 2.4532e-01 (3.8612e-01)	Acc@1  94.00 ( 86.98)	Acc@5  99.00 ( 99.37)
Test: [ 90/100]	Time  0.022 ( 0.020)	Loss 3.8017e-01 (3.8481e-01)	Acc@1  87.00 ( 87.08)	Acc@5 100.00 ( 99.36)
 * Acc@1 87.070 Acc@5 99.370
### epoch[14] execution time: 17.28687596321106
EPOCH 15
# current learning rate: 0.1
# Switched to train mode...
Epoch: [15][  0/391]	Time  0.209 ( 0.209)	Data  0.176 ( 0.176)	Loss 1.5105e-01 (1.5105e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
Epoch: [15][ 10/391]	Time  0.038 ( 0.054)	Data  0.001 ( 0.018)	Loss 2.5026e-01 (2.6504e-01)	Acc@1  89.84 ( 91.41)	Acc@5 100.00 ( 99.79)
Epoch: [15][ 20/391]	Time  0.037 ( 0.046)	Data  0.001 ( 0.011)	Loss 2.9275e-01 (2.6011e-01)	Acc@1  89.84 ( 91.33)	Acc@5 100.00 ( 99.81)
Epoch: [15][ 30/391]	Time  0.036 ( 0.043)	Data  0.001 ( 0.008)	Loss 3.9958e-01 (2.7209e-01)	Acc@1  86.72 ( 90.62)	Acc@5  99.22 ( 99.85)
Epoch: [15][ 40/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.007)	Loss 2.5068e-01 (2.7429e-01)	Acc@1  88.28 ( 90.57)	Acc@5 100.00 ( 99.85)
Epoch: [15][ 50/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.006)	Loss 3.4039e-01 (2.7623e-01)	Acc@1  85.16 ( 90.18)	Acc@5  99.22 ( 99.82)
Epoch: [15][ 60/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.005)	Loss 2.6652e-01 (2.7613e-01)	Acc@1  92.19 ( 90.22)	Acc@5 100.00 ( 99.80)
Epoch: [15][ 70/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.005)	Loss 2.6275e-01 (2.7885e-01)	Acc@1  88.28 ( 90.21)	Acc@5 100.00 ( 99.79)
Epoch: [15][ 80/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.4963e-01 (2.7461e-01)	Acc@1  90.62 ( 90.35)	Acc@5 100.00 ( 99.80)
Epoch: [15][ 90/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.004)	Loss 3.3896e-01 (2.7686e-01)	Acc@1  88.28 ( 90.32)	Acc@5  99.22 ( 99.78)
Epoch: [15][100/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.6435e-01 (2.7784e-01)	Acc@1  92.97 ( 90.28)	Acc@5  99.22 ( 99.76)
Epoch: [15][110/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.3448e-01 (2.7863e-01)	Acc@1  92.19 ( 90.29)	Acc@5 100.00 ( 99.76)
Epoch: [15][120/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.004)	Loss 3.9163e-01 (2.8171e-01)	Acc@1  86.72 ( 90.19)	Acc@5  99.22 ( 99.75)
Epoch: [15][130/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 3.1829e-01 (2.7992e-01)	Acc@1  90.62 ( 90.20)	Acc@5 100.00 ( 99.76)
Epoch: [15][140/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.6218e-01 (2.7954e-01)	Acc@1  92.97 ( 90.19)	Acc@5 100.00 ( 99.75)
Epoch: [15][150/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.7794e-01 (2.7971e-01)	Acc@1  90.62 ( 90.22)	Acc@5  99.22 ( 99.75)
Epoch: [15][160/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.3110e-01 (2.7984e-01)	Acc@1  84.38 ( 90.21)	Acc@5 100.00 ( 99.75)
Epoch: [15][170/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.0767e-01 (2.8044e-01)	Acc@1  92.97 ( 90.19)	Acc@5 100.00 ( 99.75)
Epoch: [15][180/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.8239e-01 (2.8053e-01)	Acc@1  89.06 ( 90.19)	Acc@5 100.00 ( 99.75)
Epoch: [15][190/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.4171e-01 (2.8441e-01)	Acc@1  90.62 ( 90.10)	Acc@5  99.22 ( 99.73)
Epoch: [15][200/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.4928e-01 (2.8355e-01)	Acc@1  89.84 ( 90.08)	Acc@5 100.00 ( 99.73)
Epoch: [15][210/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.3216e-01 (2.8353e-01)	Acc@1  97.66 ( 90.13)	Acc@5 100.00 ( 99.73)
Epoch: [15][220/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.2947e-01 (2.8418e-01)	Acc@1  91.41 ( 90.11)	Acc@5 100.00 ( 99.73)
Epoch: [15][230/391]	Time  0.035 ( 0.039)	Data  0.002 ( 0.003)	Loss 2.6518e-01 (2.8317e-01)	Acc@1  86.72 ( 90.14)	Acc@5 100.00 ( 99.74)
Epoch: [15][240/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.6917e-01 (2.8332e-01)	Acc@1  85.94 ( 90.14)	Acc@5 100.00 ( 99.74)
Epoch: [15][250/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.8621e-01 (2.8381e-01)	Acc@1  91.41 ( 90.14)	Acc@5 100.00 ( 99.74)
Epoch: [15][260/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.0232e-01 (2.8343e-01)	Acc@1  89.06 ( 90.14)	Acc@5 100.00 ( 99.75)
Epoch: [15][270/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.8665e-01 (2.8304e-01)	Acc@1  93.75 ( 90.17)	Acc@5 100.00 ( 99.75)
Epoch: [15][280/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.5390e-01 (2.8254e-01)	Acc@1  90.62 ( 90.17)	Acc@5  99.22 ( 99.75)
Epoch: [15][290/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.0984e-01 (2.8388e-01)	Acc@1  86.72 ( 90.13)	Acc@5 100.00 ( 99.75)
Epoch: [15][300/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.2228e-01 (2.8452e-01)	Acc@1  93.75 ( 90.10)	Acc@5 100.00 ( 99.75)
Epoch: [15][310/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.3948e-01 (2.8503e-01)	Acc@1  88.28 ( 90.07)	Acc@5 100.00 ( 99.75)
Epoch: [15][320/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.4821e-01 (2.8623e-01)	Acc@1  91.41 ( 90.03)	Acc@5 100.00 ( 99.76)
Epoch: [15][330/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.4286e-01 (2.8616e-01)	Acc@1  87.50 ( 90.03)	Acc@5 100.00 ( 99.76)
Epoch: [15][340/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.8381e-01 (2.8696e-01)	Acc@1  91.41 ( 89.99)	Acc@5 100.00 ( 99.75)
Epoch: [15][350/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.9082e-01 (2.8646e-01)	Acc@1  93.75 ( 90.01)	Acc@5 100.00 ( 99.75)
Epoch: [15][360/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.2573e-01 (2.8651e-01)	Acc@1  91.41 ( 90.00)	Acc@5  99.22 ( 99.75)
Epoch: [15][370/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.8423e-01 (2.8637e-01)	Acc@1  89.84 ( 90.02)	Acc@5 100.00 ( 99.76)
Epoch: [15][380/391]	Time  0.040 ( 0.038)	Data  0.002 ( 0.003)	Loss 1.8800e-01 (2.8629e-01)	Acc@1  92.19 ( 90.03)	Acc@5 100.00 ( 99.75)
Epoch: [15][390/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.9107e-01 (2.8708e-01)	Acc@1  93.75 ( 89.99)	Acc@5 100.00 ( 99.75)
## e[15] optimizer.zero_grad (sum) time: 0.17592287063598633
## e[15]       loss.backward (sum) time: 3.006211280822754
## e[15]      optimizer.step (sum) time: 1.0245070457458496
## epoch[15] training(only) time: 15.129836559295654
# Switched to evaluate mode...
Test: [  0/100]	Time  0.177 ( 0.177)	Loss 3.3003e-01 (3.3003e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.023 ( 0.033)	Loss 4.3216e-01 (3.5221e-01)	Acc@1  87.00 ( 88.09)	Acc@5  99.00 ( 99.73)
Test: [ 20/100]	Time  0.021 ( 0.027)	Loss 4.2334e-01 (3.9191e-01)	Acc@1  84.00 ( 87.81)	Acc@5 100.00 ( 99.48)
Test: [ 30/100]	Time  0.016 ( 0.024)	Loss 4.3781e-01 (3.9567e-01)	Acc@1  84.00 ( 87.39)	Acc@5  99.00 ( 99.58)
Test: [ 40/100]	Time  0.016 ( 0.023)	Loss 3.8469e-01 (3.9529e-01)	Acc@1  87.00 ( 87.22)	Acc@5  98.00 ( 99.46)
Test: [ 50/100]	Time  0.016 ( 0.022)	Loss 3.4133e-01 (4.0235e-01)	Acc@1  89.00 ( 86.98)	Acc@5 100.00 ( 99.43)
Test: [ 60/100]	Time  0.020 ( 0.021)	Loss 3.3255e-01 (3.9594e-01)	Acc@1  90.00 ( 87.11)	Acc@5 100.00 ( 99.51)
Test: [ 70/100]	Time  0.017 ( 0.021)	Loss 5.3147e-01 (3.9258e-01)	Acc@1  83.00 ( 87.25)	Acc@5 100.00 ( 99.52)
Test: [ 80/100]	Time  0.017 ( 0.021)	Loss 2.7980e-01 (3.9077e-01)	Acc@1  89.00 ( 87.30)	Acc@5 100.00 ( 99.53)
Test: [ 90/100]	Time  0.016 ( 0.020)	Loss 3.0951e-01 (3.9103e-01)	Acc@1  87.00 ( 87.20)	Acc@5 100.00 ( 99.54)
 * Acc@1 87.220 Acc@5 99.550
### epoch[15] execution time: 17.29252529144287
EPOCH 16
# current learning rate: 0.1
# Switched to train mode...
Epoch: [16][  0/391]	Time  0.232 ( 0.232)	Data  0.197 ( 0.197)	Loss 2.6185e-01 (2.6185e-01)	Acc@1  92.19 ( 92.19)	Acc@5 100.00 (100.00)
Epoch: [16][ 10/391]	Time  0.034 ( 0.054)	Data  0.001 ( 0.020)	Loss 4.0131e-01 (2.4644e-01)	Acc@1  86.72 ( 91.34)	Acc@5  98.44 ( 99.86)
Epoch: [16][ 20/391]	Time  0.036 ( 0.046)	Data  0.001 ( 0.011)	Loss 1.7115e-01 (2.4648e-01)	Acc@1  93.75 ( 91.41)	Acc@5 100.00 ( 99.70)
Epoch: [16][ 30/391]	Time  0.037 ( 0.044)	Data  0.001 ( 0.009)	Loss 1.8351e-01 (2.4501e-01)	Acc@1  92.97 ( 91.76)	Acc@5 100.00 ( 99.72)
Epoch: [16][ 40/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.007)	Loss 2.1032e-01 (2.4274e-01)	Acc@1  93.75 ( 91.88)	Acc@5 100.00 ( 99.75)
Epoch: [16][ 50/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.006)	Loss 1.7444e-01 (2.4819e-01)	Acc@1  93.75 ( 91.67)	Acc@5 100.00 ( 99.80)
Epoch: [16][ 60/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.005)	Loss 3.1696e-01 (2.5400e-01)	Acc@1  89.06 ( 91.48)	Acc@5  98.44 ( 99.77)
Epoch: [16][ 70/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.005)	Loss 2.9853e-01 (2.5209e-01)	Acc@1  87.50 ( 91.52)	Acc@5 100.00 ( 99.80)
Epoch: [16][ 80/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.005)	Loss 2.2522e-01 (2.5135e-01)	Acc@1  94.53 ( 91.65)	Acc@5  97.66 ( 99.74)
Epoch: [16][ 90/391]	Time  0.038 ( 0.040)	Data  0.002 ( 0.004)	Loss 1.5418e-01 (2.5184e-01)	Acc@1  95.31 ( 91.56)	Acc@5 100.00 ( 99.76)
Epoch: [16][100/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 3.1037e-01 (2.5415e-01)	Acc@1  89.06 ( 91.51)	Acc@5 100.00 ( 99.78)
Epoch: [16][110/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.7645e-01 (2.5814e-01)	Acc@1  91.41 ( 91.39)	Acc@5 100.00 ( 99.78)
Epoch: [16][120/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.8870e-01 (2.6158e-01)	Acc@1  92.19 ( 91.19)	Acc@5 100.00 ( 99.75)
Epoch: [16][130/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.3377e-01 (2.6284e-01)	Acc@1  90.62 ( 91.12)	Acc@5 100.00 ( 99.75)
Epoch: [16][140/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.4139e-01 (2.6303e-01)	Acc@1  91.41 ( 91.10)	Acc@5 100.00 ( 99.76)
Epoch: [16][150/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.2055e-01 (2.6361e-01)	Acc@1  91.41 ( 91.04)	Acc@5  99.22 ( 99.74)
Epoch: [16][160/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.0204e-01 (2.6374e-01)	Acc@1  90.62 ( 91.01)	Acc@5  99.22 ( 99.73)
Epoch: [16][170/391]	Time  0.035 ( 0.039)	Data  0.002 ( 0.003)	Loss 2.7332e-01 (2.6350e-01)	Acc@1  89.84 ( 90.98)	Acc@5 100.00 ( 99.74)
Epoch: [16][180/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.6926e-01 (2.6439e-01)	Acc@1  90.62 ( 90.94)	Acc@5  99.22 ( 99.74)
Epoch: [16][190/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.0515e-01 (2.6563e-01)	Acc@1  87.50 ( 90.86)	Acc@5 100.00 ( 99.74)
Epoch: [16][200/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.3533e-01 (2.6560e-01)	Acc@1  93.75 ( 90.87)	Acc@5  99.22 ( 99.75)
Epoch: [16][210/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.2545e-01 (2.6756e-01)	Acc@1  85.94 ( 90.82)	Acc@5  99.22 ( 99.75)
Epoch: [16][220/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.2537e-01 (2.6892e-01)	Acc@1  92.19 ( 90.77)	Acc@5 100.00 ( 99.75)
Epoch: [16][230/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.5236e-01 (2.7055e-01)	Acc@1  87.50 ( 90.74)	Acc@5 100.00 ( 99.74)
Epoch: [16][240/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.2290e-01 (2.7139e-01)	Acc@1  90.62 ( 90.76)	Acc@5  98.44 ( 99.74)
Epoch: [16][250/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.5591e-01 (2.7046e-01)	Acc@1  94.53 ( 90.79)	Acc@5 100.00 ( 99.74)
Epoch: [16][260/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.8003e-01 (2.6976e-01)	Acc@1  92.97 ( 90.83)	Acc@5 100.00 ( 99.75)
Epoch: [16][270/391]	Time  0.038 ( 0.039)	Data  0.002 ( 0.003)	Loss 3.1945e-01 (2.6858e-01)	Acc@1  89.06 ( 90.86)	Acc@5 100.00 ( 99.76)
Epoch: [16][280/391]	Time  0.040 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.8958e-01 (2.6734e-01)	Acc@1  89.84 ( 90.89)	Acc@5  99.22 ( 99.75)
Epoch: [16][290/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.6330e-01 (2.6805e-01)	Acc@1  86.72 ( 90.86)	Acc@5  98.44 ( 99.75)
Epoch: [16][300/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.3827e-01 (2.6751e-01)	Acc@1  96.09 ( 90.88)	Acc@5 100.00 ( 99.75)
Epoch: [16][310/391]	Time  0.039 ( 0.039)	Data  0.002 ( 0.003)	Loss 2.3187e-01 (2.6775e-01)	Acc@1  92.97 ( 90.88)	Acc@5 100.00 ( 99.75)
Epoch: [16][320/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.3126e-01 (2.6701e-01)	Acc@1  87.50 ( 90.89)	Acc@5 100.00 ( 99.76)
Epoch: [16][330/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.3995e-01 (2.6762e-01)	Acc@1  89.84 ( 90.87)	Acc@5 100.00 ( 99.77)
Epoch: [16][340/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.1797e-01 (2.6766e-01)	Acc@1  87.50 ( 90.86)	Acc@5 100.00 ( 99.77)
Epoch: [16][350/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.8439e-01 (2.6776e-01)	Acc@1  89.06 ( 90.86)	Acc@5 100.00 ( 99.77)
Epoch: [16][360/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.7916e-01 (2.6776e-01)	Acc@1  92.19 ( 90.87)	Acc@5  99.22 ( 99.77)
Epoch: [16][370/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.5999e-01 (2.6834e-01)	Acc@1  90.62 ( 90.85)	Acc@5 100.00 ( 99.77)
Epoch: [16][380/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.4350e-01 (2.6848e-01)	Acc@1  88.28 ( 90.84)	Acc@5 100.00 ( 99.77)
Epoch: [16][390/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.5515e-01 (2.6732e-01)	Acc@1  95.00 ( 90.90)	Acc@5 100.00 ( 99.77)
## e[16] optimizer.zero_grad (sum) time: 0.1751854419708252
## e[16]       loss.backward (sum) time: 2.978191375732422
## e[16]      optimizer.step (sum) time: 1.029754877090454
## epoch[16] training(only) time: 15.163012266159058
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 2.7151e-01 (2.7151e-01)	Acc@1  93.00 ( 93.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.019 ( 0.036)	Loss 4.1294e-01 (3.8621e-01)	Acc@1  87.00 ( 87.73)	Acc@5 100.00 ( 99.55)
Test: [ 20/100]	Time  0.016 ( 0.028)	Loss 4.9957e-01 (4.0507e-01)	Acc@1  83.00 ( 87.57)	Acc@5 100.00 ( 99.48)
Test: [ 30/100]	Time  0.021 ( 0.025)	Loss 5.2358e-01 (4.0871e-01)	Acc@1  82.00 ( 87.39)	Acc@5 100.00 ( 99.52)
Test: [ 40/100]	Time  0.017 ( 0.023)	Loss 3.8397e-01 (4.1506e-01)	Acc@1  87.00 ( 87.07)	Acc@5  99.00 ( 99.44)
Test: [ 50/100]	Time  0.016 ( 0.022)	Loss 2.9353e-01 (4.1037e-01)	Acc@1  91.00 ( 87.31)	Acc@5  99.00 ( 99.43)
Test: [ 60/100]	Time  0.019 ( 0.022)	Loss 4.3973e-01 (4.0401e-01)	Acc@1  86.00 ( 87.39)	Acc@5 100.00 ( 99.51)
Test: [ 70/100]	Time  0.016 ( 0.021)	Loss 3.3916e-01 (3.9569e-01)	Acc@1  89.00 ( 87.52)	Acc@5 100.00 ( 99.55)
Test: [ 80/100]	Time  0.023 ( 0.021)	Loss 2.6742e-01 (3.9813e-01)	Acc@1  93.00 ( 87.43)	Acc@5 100.00 ( 99.56)
Test: [ 90/100]	Time  0.016 ( 0.021)	Loss 3.0218e-01 (3.9751e-01)	Acc@1  89.00 ( 87.48)	Acc@5 100.00 ( 99.57)
 * Acc@1 87.540 Acc@5 99.580
### epoch[16] execution time: 17.34174418449402
EPOCH 17
# current learning rate: 0.1
# Switched to train mode...
Epoch: [17][  0/391]	Time  0.218 ( 0.218)	Data  0.183 ( 0.183)	Loss 2.0626e-01 (2.0626e-01)	Acc@1  92.97 ( 92.97)	Acc@5 100.00 (100.00)
Epoch: [17][ 10/391]	Time  0.039 ( 0.054)	Data  0.001 ( 0.018)	Loss 3.1502e-01 (2.6803e-01)	Acc@1  92.97 ( 91.26)	Acc@5  99.22 ( 99.64)
Epoch: [17][ 20/391]	Time  0.038 ( 0.046)	Data  0.001 ( 0.011)	Loss 2.1215e-01 (2.4495e-01)	Acc@1  91.41 ( 91.89)	Acc@5 100.00 ( 99.70)
Epoch: [17][ 30/391]	Time  0.037 ( 0.043)	Data  0.001 ( 0.008)	Loss 1.8426e-01 (2.3764e-01)	Acc@1  94.53 ( 91.99)	Acc@5 100.00 ( 99.75)
Epoch: [17][ 40/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.007)	Loss 1.2419e-01 (2.3801e-01)	Acc@1  96.09 ( 91.77)	Acc@5 100.00 ( 99.77)
Epoch: [17][ 50/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.006)	Loss 2.2857e-01 (2.3355e-01)	Acc@1  93.75 ( 92.00)	Acc@5 100.00 ( 99.80)
Epoch: [17][ 60/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.005)	Loss 1.7673e-01 (2.3230e-01)	Acc@1  92.19 ( 92.01)	Acc@5 100.00 ( 99.83)
Epoch: [17][ 70/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.005)	Loss 2.0403e-01 (2.3554e-01)	Acc@1  92.97 ( 91.66)	Acc@5 100.00 ( 99.85)
Epoch: [17][ 80/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.6471e-01 (2.3481e-01)	Acc@1  92.97 ( 91.68)	Acc@5 100.00 ( 99.84)
Epoch: [17][ 90/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.7830e-01 (2.3841e-01)	Acc@1  90.62 ( 91.60)	Acc@5 100.00 ( 99.82)
Epoch: [17][100/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 3.0258e-01 (2.4289e-01)	Acc@1  89.06 ( 91.39)	Acc@5 100.00 ( 99.81)
Epoch: [17][110/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.4276e-01 (2.4309e-01)	Acc@1  91.41 ( 91.37)	Acc@5  99.22 ( 99.82)
Epoch: [17][120/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.3086e-01 (2.4632e-01)	Acc@1  93.75 ( 91.25)	Acc@5 100.00 ( 99.81)
Epoch: [17][130/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.9403e-01 (2.4724e-01)	Acc@1  92.97 ( 91.25)	Acc@5 100.00 ( 99.80)
Epoch: [17][140/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.3581e-01 (2.4704e-01)	Acc@1  92.19 ( 91.28)	Acc@5  99.22 ( 99.78)
Epoch: [17][150/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.9911e-01 (2.4785e-01)	Acc@1  90.62 ( 91.30)	Acc@5  99.22 ( 99.77)
Epoch: [17][160/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.3123e-01 (2.4702e-01)	Acc@1  91.41 ( 91.34)	Acc@5 100.00 ( 99.77)
Epoch: [17][170/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.1962e-01 (2.4881e-01)	Acc@1  91.41 ( 91.27)	Acc@5 100.00 ( 99.76)
Epoch: [17][180/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.5441e-01 (2.5058e-01)	Acc@1  89.84 ( 91.22)	Acc@5 100.00 ( 99.77)
Epoch: [17][190/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.2918e-01 (2.5251e-01)	Acc@1  92.97 ( 91.19)	Acc@5 100.00 ( 99.76)
Epoch: [17][200/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.5561e-01 (2.5342e-01)	Acc@1  92.19 ( 91.14)	Acc@5 100.00 ( 99.76)
Epoch: [17][210/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.1553e-01 (2.5294e-01)	Acc@1  90.62 ( 91.18)	Acc@5 100.00 ( 99.77)
Epoch: [17][220/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.1320e-01 (2.5531e-01)	Acc@1  90.62 ( 91.11)	Acc@5  99.22 ( 99.76)
Epoch: [17][230/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.1346e-01 (2.5675e-01)	Acc@1  90.62 ( 91.07)	Acc@5 100.00 ( 99.76)
Epoch: [17][240/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.7120e-01 (2.5638e-01)	Acc@1  89.84 ( 91.07)	Acc@5 100.00 ( 99.76)
Epoch: [17][250/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.4281e-01 (2.5707e-01)	Acc@1  92.19 ( 91.04)	Acc@5 100.00 ( 99.76)
Epoch: [17][260/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.4017e-01 (2.5610e-01)	Acc@1  92.19 ( 91.07)	Acc@5 100.00 ( 99.77)
Epoch: [17][270/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.3228e-01 (2.5632e-01)	Acc@1  89.84 ( 91.08)	Acc@5 100.00 ( 99.77)
Epoch: [17][280/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.2349e-01 (2.5632e-01)	Acc@1  91.41 ( 91.07)	Acc@5 100.00 ( 99.76)
Epoch: [17][290/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.4889e-01 (2.5685e-01)	Acc@1  92.97 ( 91.08)	Acc@5 100.00 ( 99.77)
Epoch: [17][300/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.7938e-01 (2.5789e-01)	Acc@1  91.41 ( 91.05)	Acc@5  99.22 ( 99.77)
Epoch: [17][310/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.5745e-01 (2.5679e-01)	Acc@1  93.75 ( 91.07)	Acc@5  99.22 ( 99.77)
Epoch: [17][320/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.8113e-01 (2.5747e-01)	Acc@1  93.75 ( 91.06)	Acc@5 100.00 ( 99.77)
Epoch: [17][330/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.5448e-01 (2.5753e-01)	Acc@1  92.19 ( 91.08)	Acc@5  99.22 ( 99.77)
Epoch: [17][340/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.4887e-01 (2.5758e-01)	Acc@1  92.19 ( 91.06)	Acc@5 100.00 ( 99.77)
Epoch: [17][350/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.4410e-01 (2.5620e-01)	Acc@1  92.19 ( 91.10)	Acc@5 100.00 ( 99.78)
Epoch: [17][360/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.0008e-01 (2.5609e-01)	Acc@1  92.19 ( 91.10)	Acc@5 100.00 ( 99.77)
Epoch: [17][370/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.3744e-01 (2.5677e-01)	Acc@1  91.41 ( 91.08)	Acc@5 100.00 ( 99.78)
Epoch: [17][380/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.5443e-01 (2.5662e-01)	Acc@1  87.50 ( 91.10)	Acc@5 100.00 ( 99.78)
Epoch: [17][390/391]	Time  0.036 ( 0.038)	Data  0.000 ( 0.003)	Loss 4.1327e-01 (2.5679e-01)	Acc@1  88.75 ( 91.08)	Acc@5 100.00 ( 99.78)
## e[17] optimizer.zero_grad (sum) time: 0.1755051612854004
## e[17]       loss.backward (sum) time: 2.997358560562134
## e[17]      optimizer.step (sum) time: 1.0230767726898193
## epoch[17] training(only) time: 15.170287847518921
# Switched to evaluate mode...
Test: [  0/100]	Time  0.181 ( 0.181)	Loss 4.4710e-01 (4.4710e-01)	Acc@1  86.00 ( 86.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.019 ( 0.033)	Loss 4.0497e-01 (4.0050e-01)	Acc@1  85.00 ( 86.73)	Acc@5  99.00 ( 99.55)
Test: [ 20/100]	Time  0.023 ( 0.026)	Loss 4.4372e-01 (4.2801e-01)	Acc@1  85.00 ( 86.48)	Acc@5 100.00 ( 99.52)
Test: [ 30/100]	Time  0.021 ( 0.024)	Loss 5.7066e-01 (4.3748e-01)	Acc@1  81.00 ( 86.13)	Acc@5 100.00 ( 99.58)
Test: [ 40/100]	Time  0.016 ( 0.022)	Loss 5.7888e-01 (4.4827e-01)	Acc@1  84.00 ( 86.12)	Acc@5  99.00 ( 99.49)
Test: [ 50/100]	Time  0.023 ( 0.022)	Loss 5.1879e-01 (4.4542e-01)	Acc@1  87.00 ( 86.27)	Acc@5  98.00 ( 99.47)
Test: [ 60/100]	Time  0.016 ( 0.021)	Loss 4.5243e-01 (4.3915e-01)	Acc@1  83.00 ( 86.18)	Acc@5 100.00 ( 99.51)
Test: [ 70/100]	Time  0.017 ( 0.021)	Loss 5.4803e-01 (4.4528e-01)	Acc@1  84.00 ( 86.06)	Acc@5 100.00 ( 99.49)
Test: [ 80/100]	Time  0.021 ( 0.021)	Loss 3.2923e-01 (4.4019e-01)	Acc@1  90.00 ( 86.19)	Acc@5 100.00 ( 99.51)
Test: [ 90/100]	Time  0.020 ( 0.020)	Loss 2.5893e-01 (4.4306e-01)	Acc@1  91.00 ( 86.19)	Acc@5  99.00 ( 99.51)
 * Acc@1 86.340 Acc@5 99.490
### epoch[17] execution time: 17.351717948913574
EPOCH 18
# current learning rate: 0.1
# Switched to train mode...
Epoch: [18][  0/391]	Time  0.216 ( 0.216)	Data  0.182 ( 0.182)	Loss 3.2217e-01 (3.2217e-01)	Acc@1  87.50 ( 87.50)	Acc@5  99.22 ( 99.22)
Epoch: [18][ 10/391]	Time  0.038 ( 0.054)	Data  0.001 ( 0.018)	Loss 1.4009e-01 (2.4300e-01)	Acc@1  95.31 ( 91.41)	Acc@5 100.00 ( 99.86)
Epoch: [18][ 20/391]	Time  0.038 ( 0.047)	Data  0.001 ( 0.011)	Loss 3.0618e-01 (2.4268e-01)	Acc@1  86.72 ( 91.18)	Acc@5 100.00 ( 99.85)
Epoch: [18][ 30/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.008)	Loss 1.8955e-01 (2.4196e-01)	Acc@1  92.97 ( 91.46)	Acc@5  99.22 ( 99.77)
Epoch: [18][ 40/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.007)	Loss 2.8184e-01 (2.3589e-01)	Acc@1  88.28 ( 91.67)	Acc@5 100.00 ( 99.81)
Epoch: [18][ 50/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.006)	Loss 2.6853e-01 (2.3086e-01)	Acc@1  92.19 ( 91.84)	Acc@5 100.00 ( 99.83)
Epoch: [18][ 60/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.005)	Loss 3.4597e-01 (2.2883e-01)	Acc@1  88.28 ( 91.83)	Acc@5  99.22 ( 99.85)
Epoch: [18][ 70/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.005)	Loss 2.4408e-01 (2.2742e-01)	Acc@1  91.41 ( 91.85)	Acc@5 100.00 ( 99.87)
Epoch: [18][ 80/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.6031e-01 (2.3498e-01)	Acc@1  92.19 ( 91.70)	Acc@5 100.00 ( 99.88)
Epoch: [18][ 90/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.9151e-01 (2.3998e-01)	Acc@1  92.19 ( 91.61)	Acc@5 100.00 ( 99.87)
Epoch: [18][100/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.8817e-01 (2.3927e-01)	Acc@1  89.84 ( 91.62)	Acc@5  99.22 ( 99.85)
Epoch: [18][110/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 3.2126e-01 (2.3831e-01)	Acc@1  89.06 ( 91.70)	Acc@5 100.00 ( 99.84)
Epoch: [18][120/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.6799e-01 (2.3755e-01)	Acc@1  89.06 ( 91.66)	Acc@5 100.00 ( 99.83)
Epoch: [18][130/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.7912e-01 (2.3625e-01)	Acc@1  88.28 ( 91.68)	Acc@5 100.00 ( 99.84)
Epoch: [18][140/391]	Time  0.045 ( 0.039)	Data  0.001 ( 0.004)	Loss 3.5508e-01 (2.3929e-01)	Acc@1  88.28 ( 91.58)	Acc@5 100.00 ( 99.82)
Epoch: [18][150/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.3878e-01 (2.4070e-01)	Acc@1  92.19 ( 91.53)	Acc@5 100.00 ( 99.82)
Epoch: [18][160/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.7032e-01 (2.4026e-01)	Acc@1  95.31 ( 91.59)	Acc@5 100.00 ( 99.83)
Epoch: [18][170/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.2502e-01 (2.4180e-01)	Acc@1  92.19 ( 91.53)	Acc@5  99.22 ( 99.82)
Epoch: [18][180/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 9.7465e-02 (2.4147e-01)	Acc@1  96.88 ( 91.55)	Acc@5 100.00 ( 99.82)
Epoch: [18][190/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.9172e-01 (2.4212e-01)	Acc@1  95.31 ( 91.53)	Acc@5 100.00 ( 99.82)
Epoch: [18][200/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.0342e-01 (2.4081e-01)	Acc@1  89.84 ( 91.55)	Acc@5 100.00 ( 99.83)
Epoch: [18][210/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.4146e-01 (2.4158e-01)	Acc@1  92.97 ( 91.54)	Acc@5 100.00 ( 99.83)
Epoch: [18][220/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.4216e-01 (2.4140e-01)	Acc@1  90.62 ( 91.55)	Acc@5 100.00 ( 99.83)
Epoch: [18][230/391]	Time  0.040 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.1930e-01 (2.4111e-01)	Acc@1  94.53 ( 91.56)	Acc@5  98.44 ( 99.82)
Epoch: [18][240/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.3160e-01 (2.4218e-01)	Acc@1  91.41 ( 91.53)	Acc@5  99.22 ( 99.80)
Epoch: [18][250/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.0664e-01 (2.4117e-01)	Acc@1  92.97 ( 91.59)	Acc@5 100.00 ( 99.80)
Epoch: [18][260/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.2581e-01 (2.4049e-01)	Acc@1  89.06 ( 91.60)	Acc@5  98.44 ( 99.80)
Epoch: [18][270/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.7625e-01 (2.3985e-01)	Acc@1  92.97 ( 91.61)	Acc@5 100.00 ( 99.80)
Epoch: [18][280/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.8748e-01 (2.3974e-01)	Acc@1  89.06 ( 91.58)	Acc@5  99.22 ( 99.80)
Epoch: [18][290/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.7523e-01 (2.4120e-01)	Acc@1  89.06 ( 91.55)	Acc@5  99.22 ( 99.80)
Epoch: [18][300/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.9398e-01 (2.4248e-01)	Acc@1  85.94 ( 91.50)	Acc@5 100.00 ( 99.81)
Epoch: [18][310/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.9311e-01 (2.4256e-01)	Acc@1  89.84 ( 91.49)	Acc@5 100.00 ( 99.80)
Epoch: [18][320/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.0243e-01 (2.4359e-01)	Acc@1  89.84 ( 91.45)	Acc@5 100.00 ( 99.81)
Epoch: [18][330/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.0205e-01 (2.4291e-01)	Acc@1  94.53 ( 91.50)	Acc@5 100.00 ( 99.80)
Epoch: [18][340/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.6146e-01 (2.4344e-01)	Acc@1  89.84 ( 91.47)	Acc@5 100.00 ( 99.81)
Epoch: [18][350/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.4618e-01 (2.4282e-01)	Acc@1  96.88 ( 91.49)	Acc@5 100.00 ( 99.81)
Epoch: [18][360/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.0150e-01 (2.4338e-01)	Acc@1  91.41 ( 91.48)	Acc@5 100.00 ( 99.81)
Epoch: [18][370/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.2694e-01 (2.4440e-01)	Acc@1  88.28 ( 91.43)	Acc@5  97.66 ( 99.80)
Epoch: [18][380/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.8752e-01 (2.4465e-01)	Acc@1  94.53 ( 91.45)	Acc@5 100.00 ( 99.80)
Epoch: [18][390/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.2872e-01 (2.4461e-01)	Acc@1  92.50 ( 91.46)	Acc@5  98.75 ( 99.80)
## e[18] optimizer.zero_grad (sum) time: 0.17493343353271484
## e[18]       loss.backward (sum) time: 2.9773106575012207
## e[18]      optimizer.step (sum) time: 1.0331988334655762
## epoch[18] training(only) time: 15.141653537750244
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 2.9779e-01 (2.9779e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.023 ( 0.035)	Loss 3.9899e-01 (3.5291e-01)	Acc@1  88.00 ( 89.18)	Acc@5 100.00 ( 99.55)
Test: [ 20/100]	Time  0.020 ( 0.027)	Loss 3.7314e-01 (3.5252e-01)	Acc@1  86.00 ( 88.62)	Acc@5 100.00 ( 99.52)
Test: [ 30/100]	Time  0.020 ( 0.025)	Loss 5.0050e-01 (3.6831e-01)	Acc@1  79.00 ( 88.29)	Acc@5  99.00 ( 99.55)
Test: [ 40/100]	Time  0.018 ( 0.023)	Loss 2.5669e-01 (3.6626e-01)	Acc@1  91.00 ( 88.12)	Acc@5 100.00 ( 99.56)
Test: [ 50/100]	Time  0.020 ( 0.023)	Loss 2.9656e-01 (3.6944e-01)	Acc@1  88.00 ( 87.96)	Acc@5  99.00 ( 99.55)
Test: [ 60/100]	Time  0.018 ( 0.022)	Loss 2.9715e-01 (3.7466e-01)	Acc@1  90.00 ( 87.77)	Acc@5 100.00 ( 99.52)
Test: [ 70/100]	Time  0.020 ( 0.022)	Loss 3.9494e-01 (3.6964e-01)	Acc@1  87.00 ( 87.89)	Acc@5  99.00 ( 99.54)
Test: [ 80/100]	Time  0.023 ( 0.021)	Loss 2.6533e-01 (3.6906e-01)	Acc@1  91.00 ( 87.84)	Acc@5 100.00 ( 99.54)
Test: [ 90/100]	Time  0.020 ( 0.021)	Loss 2.3750e-01 (3.6618e-01)	Acc@1  92.00 ( 87.80)	Acc@5 100.00 ( 99.56)
 * Acc@1 87.880 Acc@5 99.580
### epoch[18] execution time: 17.353254556655884
EPOCH 19
# current learning rate: 0.1
# Switched to train mode...
Epoch: [19][  0/391]	Time  0.206 ( 0.206)	Data  0.165 ( 0.165)	Loss 1.7120e-01 (1.7120e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
Epoch: [19][ 10/391]	Time  0.036 ( 0.053)	Data  0.001 ( 0.017)	Loss 2.3657e-01 (2.5291e-01)	Acc@1  92.19 ( 91.97)	Acc@5  99.22 ( 99.64)
Epoch: [19][ 20/391]	Time  0.037 ( 0.046)	Data  0.001 ( 0.010)	Loss 2.6747e-01 (2.3148e-01)	Acc@1  92.19 ( 92.41)	Acc@5 100.00 ( 99.81)
Epoch: [19][ 30/391]	Time  0.037 ( 0.043)	Data  0.001 ( 0.007)	Loss 2.5382e-01 (2.3071e-01)	Acc@1  91.41 ( 92.57)	Acc@5 100.00 ( 99.77)
Epoch: [19][ 40/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.006)	Loss 2.0963e-01 (2.2985e-01)	Acc@1  93.75 ( 92.55)	Acc@5  99.22 ( 99.77)
Epoch: [19][ 50/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.005)	Loss 2.5756e-01 (2.2833e-01)	Acc@1  88.28 ( 92.51)	Acc@5  99.22 ( 99.79)
Epoch: [19][ 60/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.005)	Loss 1.9247e-01 (2.2644e-01)	Acc@1  92.97 ( 92.46)	Acc@5 100.00 ( 99.81)
Epoch: [19][ 70/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.9522e-01 (2.2279e-01)	Acc@1  92.97 ( 92.53)	Acc@5 100.00 ( 99.82)
Epoch: [19][ 80/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.7329e-01 (2.2261e-01)	Acc@1  92.97 ( 92.44)	Acc@5 100.00 ( 99.84)
Epoch: [19][ 90/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.4589e-01 (2.2318e-01)	Acc@1  92.19 ( 92.43)	Acc@5 100.00 ( 99.85)
Epoch: [19][100/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.6654e-01 (2.2411e-01)	Acc@1  91.41 ( 92.43)	Acc@5  99.22 ( 99.85)
Epoch: [19][110/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.2965e-01 (2.2486e-01)	Acc@1  91.41 ( 92.36)	Acc@5 100.00 ( 99.85)
Epoch: [19][120/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.1748e-01 (2.2527e-01)	Acc@1  90.62 ( 92.37)	Acc@5 100.00 ( 99.84)
Epoch: [19][130/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.1856e-01 (2.2373e-01)	Acc@1  92.97 ( 92.43)	Acc@5 100.00 ( 99.84)
Epoch: [19][140/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.2309e-01 (2.2768e-01)	Acc@1  92.97 ( 92.23)	Acc@5 100.00 ( 99.84)
Epoch: [19][150/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.4643e-01 (2.2763e-01)	Acc@1  90.62 ( 92.24)	Acc@5 100.00 ( 99.86)
Epoch: [19][160/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.9906e-01 (2.2522e-01)	Acc@1  94.53 ( 92.35)	Acc@5 100.00 ( 99.86)
Epoch: [19][170/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.3009e-01 (2.2788e-01)	Acc@1  93.75 ( 92.27)	Acc@5 100.00 ( 99.85)
Epoch: [19][180/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.7597e-01 (2.2844e-01)	Acc@1  96.09 ( 92.23)	Acc@5 100.00 ( 99.85)
Epoch: [19][190/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.6306e-01 (2.2854e-01)	Acc@1  95.31 ( 92.23)	Acc@5 100.00 ( 99.85)
Epoch: [19][200/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.0196e-01 (2.2950e-01)	Acc@1  93.75 ( 92.19)	Acc@5  99.22 ( 99.85)
Epoch: [19][210/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.5610e-01 (2.3152e-01)	Acc@1  93.75 ( 92.08)	Acc@5 100.00 ( 99.86)
Epoch: [19][220/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.7995e-01 (2.3115e-01)	Acc@1  90.62 ( 92.05)	Acc@5 100.00 ( 99.86)
Epoch: [19][230/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.7247e-01 (2.3224e-01)	Acc@1  90.62 ( 92.02)	Acc@5 100.00 ( 99.86)
Epoch: [19][240/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.9556e-01 (2.3240e-01)	Acc@1  93.75 ( 92.02)	Acc@5 100.00 ( 99.86)
Epoch: [19][250/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.1702e-01 (2.3306e-01)	Acc@1  92.19 ( 92.00)	Acc@5 100.00 ( 99.86)
Epoch: [19][260/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.8962e-01 (2.3260e-01)	Acc@1  92.19 ( 92.00)	Acc@5 100.00 ( 99.87)
Epoch: [19][270/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.7102e-01 (2.3265e-01)	Acc@1  92.19 ( 91.97)	Acc@5 100.00 ( 99.87)
Epoch: [19][280/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.0079e-01 (2.3242e-01)	Acc@1  93.75 ( 91.98)	Acc@5 100.00 ( 99.87)
Epoch: [19][290/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.0519e-01 (2.3172e-01)	Acc@1  94.53 ( 92.01)	Acc@5 100.00 ( 99.87)
Epoch: [19][300/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.8330e-01 (2.3064e-01)	Acc@1  94.53 ( 92.06)	Acc@5 100.00 ( 99.87)
Epoch: [19][310/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.0334e-01 (2.3020e-01)	Acc@1  92.97 ( 92.06)	Acc@5 100.00 ( 99.87)
Epoch: [19][320/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.4783e-01 (2.3140e-01)	Acc@1  89.84 ( 92.01)	Acc@5 100.00 ( 99.88)
Epoch: [19][330/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.4837e-01 (2.3170e-01)	Acc@1  93.75 ( 92.00)	Acc@5 100.00 ( 99.87)
Epoch: [19][340/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.0470e-01 (2.3240e-01)	Acc@1  88.28 ( 91.97)	Acc@5 100.00 ( 99.87)
Epoch: [19][350/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.6821e-01 (2.3291e-01)	Acc@1  91.41 ( 91.97)	Acc@5  99.22 ( 99.87)
Epoch: [19][360/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.9923e-01 (2.3280e-01)	Acc@1  92.97 ( 91.96)	Acc@5 100.00 ( 99.87)
Epoch: [19][370/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.1984e-01 (2.3288e-01)	Acc@1  91.41 ( 91.96)	Acc@5  99.22 ( 99.87)
Epoch: [19][380/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.6276e-01 (2.3385e-01)	Acc@1  91.41 ( 91.93)	Acc@5  99.22 ( 99.86)
Epoch: [19][390/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.2676e-01 (2.3453e-01)	Acc@1  90.00 ( 91.89)	Acc@5 100.00 ( 99.86)
## e[19] optimizer.zero_grad (sum) time: 0.17641520500183105
## e[19]       loss.backward (sum) time: 2.9920437335968018
## e[19]      optimizer.step (sum) time: 1.025752305984497
## epoch[19] training(only) time: 15.136840581893921
# Switched to evaluate mode...
Test: [  0/100]	Time  0.177 ( 0.177)	Loss 4.3520e-01 (4.3520e-01)	Acc@1  85.00 ( 85.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.018 ( 0.034)	Loss 3.6828e-01 (3.6684e-01)	Acc@1  90.00 ( 88.09)	Acc@5 100.00 ( 99.55)
Test: [ 20/100]	Time  0.016 ( 0.026)	Loss 4.5933e-01 (3.9032e-01)	Acc@1  83.00 ( 87.48)	Acc@5 100.00 ( 99.52)
Test: [ 30/100]	Time  0.024 ( 0.024)	Loss 4.2569e-01 (4.0352e-01)	Acc@1  85.00 ( 87.26)	Acc@5 100.00 ( 99.58)
Test: [ 40/100]	Time  0.017 ( 0.022)	Loss 3.7498e-01 (4.0171e-01)	Acc@1  94.00 ( 87.51)	Acc@5  99.00 ( 99.49)
Test: [ 50/100]	Time  0.022 ( 0.022)	Loss 2.2491e-01 (4.0078e-01)	Acc@1  92.00 ( 87.51)	Acc@5 100.00 ( 99.51)
Test: [ 60/100]	Time  0.016 ( 0.021)	Loss 4.1598e-01 (3.9405e-01)	Acc@1  88.00 ( 87.62)	Acc@5 100.00 ( 99.51)
Test: [ 70/100]	Time  0.021 ( 0.021)	Loss 4.8671e-01 (3.8818e-01)	Acc@1  86.00 ( 87.79)	Acc@5 100.00 ( 99.52)
Test: [ 80/100]	Time  0.023 ( 0.021)	Loss 3.5023e-01 (3.8584e-01)	Acc@1  85.00 ( 87.75)	Acc@5 100.00 ( 99.57)
Test: [ 90/100]	Time  0.017 ( 0.021)	Loss 2.4097e-01 (3.8683e-01)	Acc@1  93.00 ( 87.70)	Acc@5 100.00 ( 99.57)
 * Acc@1 87.700 Acc@5 99.570
### epoch[19] execution time: 17.31922698020935
EPOCH 20
# current learning rate: 0.1
# Switched to train mode...
Epoch: [20][  0/391]	Time  0.220 ( 0.220)	Data  0.183 ( 0.183)	Loss 2.2022e-01 (2.2022e-01)	Acc@1  93.75 ( 93.75)	Acc@5  99.22 ( 99.22)
Epoch: [20][ 10/391]	Time  0.038 ( 0.054)	Data  0.001 ( 0.018)	Loss 1.4917e-01 (1.7965e-01)	Acc@1  95.31 ( 93.47)	Acc@5 100.00 ( 99.93)
Epoch: [20][ 20/391]	Time  0.036 ( 0.046)	Data  0.001 ( 0.011)	Loss 1.5810e-01 (1.8323e-01)	Acc@1  94.53 ( 93.34)	Acc@5 100.00 ( 99.89)
Epoch: [20][ 30/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.008)	Loss 2.1095e-01 (1.9606e-01)	Acc@1  93.75 ( 92.94)	Acc@5 100.00 ( 99.92)
Epoch: [20][ 40/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.007)	Loss 1.6798e-01 (1.9413e-01)	Acc@1  94.53 ( 92.87)	Acc@5  99.22 ( 99.90)
Epoch: [20][ 50/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.006)	Loss 2.7738e-01 (2.0230e-01)	Acc@1  89.06 ( 92.59)	Acc@5 100.00 ( 99.92)
Epoch: [20][ 60/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.005)	Loss 2.6998e-01 (2.0303e-01)	Acc@1  91.41 ( 92.73)	Acc@5 100.00 ( 99.94)
Epoch: [20][ 70/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.005)	Loss 3.5743e-01 (2.1194e-01)	Acc@1  86.72 ( 92.43)	Acc@5 100.00 ( 99.92)
Epoch: [20][ 80/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.2054e-01 (2.1723e-01)	Acc@1  88.28 ( 92.21)	Acc@5 100.00 ( 99.90)
Epoch: [20][ 90/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.3723e-01 (2.1703e-01)	Acc@1  92.97 ( 92.20)	Acc@5 100.00 ( 99.89)
Epoch: [20][100/391]	Time  0.038 ( 0.040)	Data  0.003 ( 0.004)	Loss 1.4044e-01 (2.1624e-01)	Acc@1  93.75 ( 92.24)	Acc@5 100.00 ( 99.89)
Epoch: [20][110/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.4037e-01 (2.1397e-01)	Acc@1  92.19 ( 92.29)	Acc@5 100.00 ( 99.90)
Epoch: [20][120/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.004)	Loss 4.2030e-01 (2.1740e-01)	Acc@1  86.72 ( 92.20)	Acc@5  99.22 ( 99.88)
Epoch: [20][130/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.004)	Loss 3.2615e-01 (2.1989e-01)	Acc@1  88.28 ( 92.11)	Acc@5 100.00 ( 99.88)
Epoch: [20][140/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.4258e-01 (2.1966e-01)	Acc@1  91.41 ( 92.10)	Acc@5 100.00 ( 99.88)
Epoch: [20][150/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.8308e-01 (2.1813e-01)	Acc@1  92.97 ( 92.13)	Acc@5 100.00 ( 99.88)
Epoch: [20][160/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.0586e-01 (2.1799e-01)	Acc@1  91.41 ( 92.11)	Acc@5 100.00 ( 99.87)
Epoch: [20][170/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.1676e-01 (2.1869e-01)	Acc@1  90.62 ( 92.09)	Acc@5 100.00 ( 99.88)
Epoch: [20][180/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.6719e-01 (2.1864e-01)	Acc@1  89.84 ( 92.11)	Acc@5  99.22 ( 99.88)
Epoch: [20][190/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.6271e-01 (2.1952e-01)	Acc@1  96.09 ( 92.11)	Acc@5 100.00 ( 99.89)
Epoch: [20][200/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.3841e-01 (2.1910e-01)	Acc@1  92.19 ( 92.14)	Acc@5 100.00 ( 99.88)
Epoch: [20][210/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.3069e-01 (2.2166e-01)	Acc@1  89.84 ( 92.07)	Acc@5 100.00 ( 99.87)
Epoch: [20][220/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.7415e-01 (2.2307e-01)	Acc@1  89.84 ( 92.05)	Acc@5 100.00 ( 99.86)
Epoch: [20][230/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.1690e-01 (2.2439e-01)	Acc@1  89.84 ( 91.96)	Acc@5 100.00 ( 99.85)
Epoch: [20][240/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.3737e-01 (2.2530e-01)	Acc@1  88.28 ( 91.92)	Acc@5 100.00 ( 99.85)
Epoch: [20][250/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.5863e-01 (2.2601e-01)	Acc@1  92.19 ( 91.89)	Acc@5 100.00 ( 99.84)
Epoch: [20][260/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.0191e-01 (2.2694e-01)	Acc@1  90.62 ( 91.88)	Acc@5 100.00 ( 99.84)
Epoch: [20][270/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.1688e-01 (2.2725e-01)	Acc@1  94.53 ( 91.91)	Acc@5 100.00 ( 99.84)
Epoch: [20][280/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.2374e-01 (2.2733e-01)	Acc@1  92.97 ( 91.93)	Acc@5 100.00 ( 99.84)
Epoch: [20][290/391]	Time  0.040 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.4821e-01 (2.2813e-01)	Acc@1  89.84 ( 91.88)	Acc@5 100.00 ( 99.84)
Epoch: [20][300/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.4626e-01 (2.2735e-01)	Acc@1  90.62 ( 91.91)	Acc@5 100.00 ( 99.84)
Epoch: [20][310/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.9115e-01 (2.2761e-01)	Acc@1  91.41 ( 91.90)	Acc@5  99.22 ( 99.83)
Epoch: [20][320/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.3282e-01 (2.2805e-01)	Acc@1  90.62 ( 91.89)	Acc@5 100.00 ( 99.83)
Epoch: [20][330/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.1994e-01 (2.2861e-01)	Acc@1  86.72 ( 91.85)	Acc@5 100.00 ( 99.84)
Epoch: [20][340/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.0594e-01 (2.2960e-01)	Acc@1  90.62 ( 91.81)	Acc@5 100.00 ( 99.84)
Epoch: [20][350/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.7821e-01 (2.3097e-01)	Acc@1  92.97 ( 91.76)	Acc@5 100.00 ( 99.83)
Epoch: [20][360/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.6980e-01 (2.3207e-01)	Acc@1  95.31 ( 91.76)	Acc@5 100.00 ( 99.83)
Epoch: [20][370/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.3589e-01 (2.3227e-01)	Acc@1  85.94 ( 91.73)	Acc@5  99.22 ( 99.83)
Epoch: [20][380/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.5225e-01 (2.3166e-01)	Acc@1  92.97 ( 91.75)	Acc@5 100.00 ( 99.83)
Epoch: [20][390/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.9639e-01 (2.3179e-01)	Acc@1  92.50 ( 91.74)	Acc@5 100.00 ( 99.84)
## e[20] optimizer.zero_grad (sum) time: 0.17680883407592773
## e[20]       loss.backward (sum) time: 3.014012098312378
## e[20]      optimizer.step (sum) time: 1.0268886089324951
## epoch[20] training(only) time: 15.13610053062439
# Switched to evaluate mode...
Test: [  0/100]	Time  0.178 ( 0.178)	Loss 2.4951e-01 (2.4951e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.018 ( 0.032)	Loss 3.4225e-01 (3.3860e-01)	Acc@1  89.00 ( 88.09)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.016 ( 0.026)	Loss 4.1486e-01 (3.6761e-01)	Acc@1  85.00 ( 88.00)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.016 ( 0.024)	Loss 5.2228e-01 (3.9029e-01)	Acc@1  81.00 ( 87.58)	Acc@5 100.00 ( 99.55)
Test: [ 40/100]	Time  0.016 ( 0.022)	Loss 4.5579e-01 (3.9051e-01)	Acc@1  82.00 ( 87.41)	Acc@5 100.00 ( 99.46)
Test: [ 50/100]	Time  0.016 ( 0.022)	Loss 3.0700e-01 (3.8700e-01)	Acc@1  90.00 ( 87.65)	Acc@5 100.00 ( 99.45)
Test: [ 60/100]	Time  0.016 ( 0.021)	Loss 5.0812e-01 (3.8742e-01)	Acc@1  82.00 ( 87.39)	Acc@5 100.00 ( 99.52)
Test: [ 70/100]	Time  0.017 ( 0.021)	Loss 3.8182e-01 (3.8440e-01)	Acc@1  88.00 ( 87.39)	Acc@5 100.00 ( 99.54)
Test: [ 80/100]	Time  0.016 ( 0.020)	Loss 2.7280e-01 (3.7836e-01)	Acc@1  91.00 ( 87.67)	Acc@5 100.00 ( 99.52)
Test: [ 90/100]	Time  0.019 ( 0.020)	Loss 1.1923e-01 (3.7925e-01)	Acc@1  97.00 ( 87.58)	Acc@5 100.00 ( 99.54)
 * Acc@1 87.710 Acc@5 99.550
### epoch[20] execution time: 17.264774322509766
EPOCH 21
# current learning rate: 0.1
# Switched to train mode...
Epoch: [21][  0/391]	Time  0.217 ( 0.217)	Data  0.177 ( 0.177)	Loss 1.7330e-01 (1.7330e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
Epoch: [21][ 10/391]	Time  0.038 ( 0.054)	Data  0.001 ( 0.018)	Loss 2.4775e-01 (2.1562e-01)	Acc@1  90.62 ( 92.26)	Acc@5 100.00 ( 99.79)
Epoch: [21][ 20/391]	Time  0.038 ( 0.046)	Data  0.001 ( 0.010)	Loss 3.5440e-01 (2.0657e-01)	Acc@1  87.50 ( 92.49)	Acc@5  99.22 ( 99.78)
Epoch: [21][ 30/391]	Time  0.037 ( 0.044)	Data  0.001 ( 0.008)	Loss 2.5020e-01 (2.1433e-01)	Acc@1  89.84 ( 92.39)	Acc@5 100.00 ( 99.85)
Epoch: [21][ 40/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.006)	Loss 2.2654e-01 (2.1668e-01)	Acc@1  94.53 ( 92.44)	Acc@5  99.22 ( 99.85)
Epoch: [21][ 50/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.006)	Loss 1.3001e-01 (2.0581e-01)	Acc@1  95.31 ( 92.85)	Acc@5 100.00 ( 99.85)
Epoch: [21][ 60/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.005)	Loss 1.8325e-01 (2.0171e-01)	Acc@1  92.97 ( 92.93)	Acc@5 100.00 ( 99.87)
Epoch: [21][ 70/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.005)	Loss 1.5769e-01 (1.9958e-01)	Acc@1  94.53 ( 93.07)	Acc@5 100.00 ( 99.89)
Epoch: [21][ 80/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.9047e-01 (1.9896e-01)	Acc@1  87.50 ( 93.00)	Acc@5  99.22 ( 99.88)
Epoch: [21][ 90/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.5569e-01 (1.9931e-01)	Acc@1  92.97 ( 93.03)	Acc@5 100.00 ( 99.89)
Epoch: [21][100/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.5248e-01 (1.9758e-01)	Acc@1  92.19 ( 93.07)	Acc@5  99.22 ( 99.88)
Epoch: [21][110/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.3877e-01 (1.9905e-01)	Acc@1  91.41 ( 93.01)	Acc@5 100.00 ( 99.89)
Epoch: [21][120/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.0412e-01 (2.0072e-01)	Acc@1  92.19 ( 92.98)	Acc@5 100.00 ( 99.88)
Epoch: [21][130/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.2459e-01 (2.0030e-01)	Acc@1  89.06 ( 92.92)	Acc@5 100.00 ( 99.89)
Epoch: [21][140/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.5573e-01 (2.0220e-01)	Acc@1  96.09 ( 92.84)	Acc@5  99.22 ( 99.87)
Epoch: [21][150/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.6698e-01 (2.0342e-01)	Acc@1  93.75 ( 92.81)	Acc@5 100.00 ( 99.88)
Epoch: [21][160/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.0528e-01 (2.0466e-01)	Acc@1  92.97 ( 92.80)	Acc@5 100.00 ( 99.88)
Epoch: [21][170/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.8910e-01 (2.0491e-01)	Acc@1  94.53 ( 92.82)	Acc@5  99.22 ( 99.88)
Epoch: [21][180/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.4081e-01 (2.0495e-01)	Acc@1  90.62 ( 92.81)	Acc@5  98.44 ( 99.87)
Epoch: [21][190/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 9.9676e-02 (2.0499e-01)	Acc@1  96.09 ( 92.80)	Acc@5 100.00 ( 99.88)
Epoch: [21][200/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.0176e-01 (2.0530e-01)	Acc@1  90.62 ( 92.78)	Acc@5 100.00 ( 99.89)
Epoch: [21][210/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.4402e-01 (2.0541e-01)	Acc@1  91.41 ( 92.76)	Acc@5 100.00 ( 99.89)
Epoch: [21][220/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.1710e-01 (2.0659e-01)	Acc@1  91.41 ( 92.74)	Acc@5 100.00 ( 99.89)
Epoch: [21][230/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.5183e-01 (2.0740e-01)	Acc@1  89.06 ( 92.69)	Acc@5 100.00 ( 99.89)
Epoch: [21][240/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.9402e-01 (2.0835e-01)	Acc@1  90.62 ( 92.70)	Acc@5 100.00 ( 99.89)
Epoch: [21][250/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.3653e-01 (2.0801e-01)	Acc@1  92.19 ( 92.71)	Acc@5 100.00 ( 99.89)
Epoch: [21][260/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.4988e-01 (2.0829e-01)	Acc@1  92.97 ( 92.71)	Acc@5 100.00 ( 99.90)
Epoch: [21][270/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.8282e-01 (2.0866e-01)	Acc@1  94.53 ( 92.71)	Acc@5 100.00 ( 99.90)
Epoch: [21][280/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.6183e-01 (2.0926e-01)	Acc@1  95.31 ( 92.70)	Acc@5 100.00 ( 99.89)
Epoch: [21][290/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.8924e-01 (2.1053e-01)	Acc@1  90.62 ( 92.65)	Acc@5  99.22 ( 99.88)
Epoch: [21][300/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.4817e-01 (2.1169e-01)	Acc@1  92.97 ( 92.58)	Acc@5  99.22 ( 99.88)
Epoch: [21][310/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.6859e-01 (2.1236e-01)	Acc@1  90.62 ( 92.55)	Acc@5 100.00 ( 99.87)
Epoch: [21][320/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.3121e-01 (2.1182e-01)	Acc@1  95.31 ( 92.58)	Acc@5 100.00 ( 99.88)
Epoch: [21][330/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.8401e-01 (2.1210e-01)	Acc@1  89.84 ( 92.58)	Acc@5 100.00 ( 99.88)
Epoch: [21][340/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.4990e-01 (2.1250e-01)	Acc@1  96.09 ( 92.57)	Acc@5 100.00 ( 99.88)
Epoch: [21][350/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.8670e-01 (2.1349e-01)	Acc@1  91.41 ( 92.54)	Acc@5 100.00 ( 99.88)
Epoch: [21][360/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.0091e-01 (2.1499e-01)	Acc@1  92.97 ( 92.49)	Acc@5 100.00 ( 99.88)
Epoch: [21][370/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.6065e-01 (2.1484e-01)	Acc@1  93.75 ( 92.50)	Acc@5  99.22 ( 99.88)
Epoch: [21][380/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.5403e-01 (2.1507e-01)	Acc@1  93.75 ( 92.48)	Acc@5 100.00 ( 99.88)
Epoch: [21][390/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.8502e-01 (2.1536e-01)	Acc@1  90.00 ( 92.47)	Acc@5 100.00 ( 99.88)
## e[21] optimizer.zero_grad (sum) time: 0.1756916046142578
## e[21]       loss.backward (sum) time: 2.9743666648864746
## e[21]      optimizer.step (sum) time: 1.0231800079345703
## epoch[21] training(only) time: 15.122654438018799
# Switched to evaluate mode...
Test: [  0/100]	Time  0.179 ( 0.179)	Loss 3.3953e-01 (3.3953e-01)	Acc@1  89.00 ( 89.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.018 ( 0.033)	Loss 4.2065e-01 (3.4204e-01)	Acc@1  87.00 ( 87.82)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.020 ( 0.026)	Loss 5.5206e-01 (3.8019e-01)	Acc@1  85.00 ( 87.38)	Acc@5  99.00 ( 99.71)
Test: [ 30/100]	Time  0.017 ( 0.023)	Loss 5.6875e-01 (4.0335e-01)	Acc@1  86.00 ( 87.39)	Acc@5  98.00 ( 99.65)
Test: [ 40/100]	Time  0.023 ( 0.022)	Loss 5.7279e-01 (4.0214e-01)	Acc@1  85.00 ( 87.68)	Acc@5  99.00 ( 99.56)
Test: [ 50/100]	Time  0.021 ( 0.022)	Loss 3.0608e-01 (4.0004e-01)	Acc@1  90.00 ( 87.73)	Acc@5  98.00 ( 99.51)
Test: [ 60/100]	Time  0.019 ( 0.021)	Loss 3.1013e-01 (3.9826e-01)	Acc@1  94.00 ( 87.67)	Acc@5 100.00 ( 99.57)
Test: [ 70/100]	Time  0.018 ( 0.021)	Loss 5.2031e-01 (3.9744e-01)	Acc@1  85.00 ( 87.49)	Acc@5  99.00 ( 99.61)
Test: [ 80/100]	Time  0.017 ( 0.021)	Loss 3.2687e-01 (3.9454e-01)	Acc@1  92.00 ( 87.65)	Acc@5 100.00 ( 99.62)
Test: [ 90/100]	Time  0.016 ( 0.020)	Loss 3.2213e-01 (3.9326e-01)	Acc@1  90.00 ( 87.63)	Acc@5 100.00 ( 99.62)
 * Acc@1 87.640 Acc@5 99.630
### epoch[21] execution time: 17.268768787384033
EPOCH 22
# current learning rate: 0.1
# Switched to train mode...
Epoch: [22][  0/391]	Time  0.211 ( 0.211)	Data  0.175 ( 0.175)	Loss 9.3332e-02 (9.3332e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [22][ 10/391]	Time  0.035 ( 0.053)	Data  0.001 ( 0.018)	Loss 2.3730e-01 (1.7538e-01)	Acc@1  91.41 ( 93.82)	Acc@5  99.22 ( 99.93)
Epoch: [22][ 20/391]	Time  0.036 ( 0.045)	Data  0.001 ( 0.010)	Loss 1.7249e-01 (2.0204e-01)	Acc@1  94.53 ( 92.78)	Acc@5 100.00 ( 99.93)
Epoch: [22][ 30/391]	Time  0.036 ( 0.043)	Data  0.001 ( 0.008)	Loss 2.4745e-01 (2.0466e-01)	Acc@1  89.06 ( 92.49)	Acc@5  99.22 ( 99.85)
Epoch: [22][ 40/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.006)	Loss 1.0896e-01 (1.9921e-01)	Acc@1  96.88 ( 92.72)	Acc@5 100.00 ( 99.87)
Epoch: [22][ 50/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.006)	Loss 1.0697e-01 (1.9374e-01)	Acc@1  97.66 ( 93.06)	Acc@5 100.00 ( 99.85)
Epoch: [22][ 60/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.005)	Loss 1.6809e-01 (1.8915e-01)	Acc@1  96.88 ( 93.24)	Acc@5 100.00 ( 99.87)
Epoch: [22][ 70/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.005)	Loss 2.2553e-01 (1.8769e-01)	Acc@1  91.41 ( 93.34)	Acc@5 100.00 ( 99.87)
Epoch: [22][ 80/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.4097e-01 (1.8895e-01)	Acc@1  89.06 ( 93.25)	Acc@5 100.00 ( 99.88)
Epoch: [22][ 90/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.8537e-01 (1.9387e-01)	Acc@1  91.41 ( 93.18)	Acc@5  99.22 ( 99.83)
Epoch: [22][100/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.1184e-01 (1.9672e-01)	Acc@1  93.75 ( 93.04)	Acc@5  99.22 ( 99.82)
Epoch: [22][110/391]	Time  0.038 ( 0.039)	Data  0.002 ( 0.004)	Loss 3.7657e-01 (2.0101e-01)	Acc@1  88.28 ( 92.91)	Acc@5  98.44 ( 99.82)
Epoch: [22][120/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 9.9349e-02 (1.9955e-01)	Acc@1  96.88 ( 92.92)	Acc@5 100.00 ( 99.83)
Epoch: [22][130/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.2324e-01 (1.9997e-01)	Acc@1  93.75 ( 92.86)	Acc@5 100.00 ( 99.83)
Epoch: [22][140/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.0081e-01 (2.0087e-01)	Acc@1  89.84 ( 92.87)	Acc@5 100.00 ( 99.84)
Epoch: [22][150/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.7070e-01 (2.0022e-01)	Acc@1  96.09 ( 92.94)	Acc@5 100.00 ( 99.84)
Epoch: [22][160/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.0956e-01 (1.9928e-01)	Acc@1  93.75 ( 92.99)	Acc@5  99.22 ( 99.84)
Epoch: [22][170/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.4947e-01 (1.9911e-01)	Acc@1  89.06 ( 92.99)	Acc@5 100.00 ( 99.84)
Epoch: [22][180/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.9978e-01 (1.9867e-01)	Acc@1  92.97 ( 93.04)	Acc@5 100.00 ( 99.84)
Epoch: [22][190/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.4462e-01 (2.0057e-01)	Acc@1  90.62 ( 92.99)	Acc@5 100.00 ( 99.84)
Epoch: [22][200/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.7971e-01 (2.0119e-01)	Acc@1  92.97 ( 92.98)	Acc@5 100.00 ( 99.84)
Epoch: [22][210/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.1065e-01 (2.0165e-01)	Acc@1  92.19 ( 93.00)	Acc@5 100.00 ( 99.85)
Epoch: [22][220/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.1684e-01 (2.0231e-01)	Acc@1  96.09 ( 93.00)	Acc@5 100.00 ( 99.85)
Epoch: [22][230/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.4454e-01 (2.0261e-01)	Acc@1  92.97 ( 92.98)	Acc@5 100.00 ( 99.85)
Epoch: [22][240/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.3867e-01 (2.0235e-01)	Acc@1  95.31 ( 92.98)	Acc@5 100.00 ( 99.86)
Epoch: [22][250/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.7661e-01 (2.0297e-01)	Acc@1  89.06 ( 93.00)	Acc@5  98.44 ( 99.85)
Epoch: [22][260/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.9063e-01 (2.0335e-01)	Acc@1  90.62 ( 93.00)	Acc@5 100.00 ( 99.85)
Epoch: [22][270/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.0748e-01 (2.0309e-01)	Acc@1  96.88 ( 93.02)	Acc@5 100.00 ( 99.85)
Epoch: [22][280/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.9667e-01 (2.0295e-01)	Acc@1  88.28 ( 93.03)	Acc@5  99.22 ( 99.85)
Epoch: [22][290/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.4145e-01 (2.0433e-01)	Acc@1  90.62 ( 93.00)	Acc@5 100.00 ( 99.86)
Epoch: [22][300/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.8769e-01 (2.0528e-01)	Acc@1  90.62 ( 92.96)	Acc@5 100.00 ( 99.85)
Epoch: [22][310/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.2305e-01 (2.0600e-01)	Acc@1  92.19 ( 92.93)	Acc@5 100.00 ( 99.86)
Epoch: [22][320/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.6704e-01 (2.0793e-01)	Acc@1  90.62 ( 92.89)	Acc@5 100.00 ( 99.86)
Epoch: [22][330/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.5143e-01 (2.0831e-01)	Acc@1  93.75 ( 92.87)	Acc@5 100.00 ( 99.86)
Epoch: [22][340/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.3416e-01 (2.0861e-01)	Acc@1  88.28 ( 92.85)	Acc@5 100.00 ( 99.86)
Epoch: [22][350/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.0849e-01 (2.0910e-01)	Acc@1  92.97 ( 92.81)	Acc@5 100.00 ( 99.86)
Epoch: [22][360/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.7831e-01 (2.0888e-01)	Acc@1  92.19 ( 92.80)	Acc@5 100.00 ( 99.86)
Epoch: [22][370/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.7076e-01 (2.0890e-01)	Acc@1  92.97 ( 92.79)	Acc@5 100.00 ( 99.87)
Epoch: [22][380/391]	Time  0.043 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.9504e-01 (2.0959e-01)	Acc@1  94.53 ( 92.77)	Acc@5 100.00 ( 99.86)
Epoch: [22][390/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.9708e-01 (2.0914e-01)	Acc@1  91.25 ( 92.78)	Acc@5 100.00 ( 99.87)
## e[22] optimizer.zero_grad (sum) time: 0.17569756507873535
## e[22]       loss.backward (sum) time: 2.98516583442688
## e[22]      optimizer.step (sum) time: 1.031104564666748
## epoch[22] training(only) time: 15.109213829040527
# Switched to evaluate mode...
Test: [  0/100]	Time  0.168 ( 0.168)	Loss 5.6313e-01 (5.6313e-01)	Acc@1  84.00 ( 84.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.032)	Loss 6.5881e-01 (4.2787e-01)	Acc@1  81.00 ( 85.55)	Acc@5 100.00 ( 99.27)
Test: [ 20/100]	Time  0.023 ( 0.026)	Loss 6.2494e-01 (4.8722e-01)	Acc@1  84.00 ( 84.95)	Acc@5  99.00 ( 99.24)
Test: [ 30/100]	Time  0.015 ( 0.024)	Loss 5.4667e-01 (5.0456e-01)	Acc@1  84.00 ( 84.29)	Acc@5  97.00 ( 99.19)
Test: [ 40/100]	Time  0.016 ( 0.022)	Loss 5.1902e-01 (5.1184e-01)	Acc@1  82.00 ( 84.07)	Acc@5 100.00 ( 99.32)
Test: [ 50/100]	Time  0.016 ( 0.021)	Loss 3.6150e-01 (5.0544e-01)	Acc@1  90.00 ( 84.24)	Acc@5 100.00 ( 99.27)
Test: [ 60/100]	Time  0.016 ( 0.021)	Loss 4.6788e-01 (5.0428e-01)	Acc@1  88.00 ( 84.26)	Acc@5 100.00 ( 99.33)
Test: [ 70/100]	Time  0.018 ( 0.020)	Loss 7.4338e-01 (5.0223e-01)	Acc@1  81.00 ( 84.23)	Acc@5 100.00 ( 99.34)
Test: [ 80/100]	Time  0.016 ( 0.020)	Loss 2.9638e-01 (4.9617e-01)	Acc@1  91.00 ( 84.46)	Acc@5 100.00 ( 99.38)
Test: [ 90/100]	Time  0.020 ( 0.020)	Loss 5.5771e-01 (4.9663e-01)	Acc@1  84.00 ( 84.47)	Acc@5 100.00 ( 99.40)
 * Acc@1 84.480 Acc@5 99.430
### epoch[22] execution time: 17.228195667266846
EPOCH 23
# current learning rate: 0.1
# Switched to train mode...
Epoch: [23][  0/391]	Time  0.215 ( 0.215)	Data  0.181 ( 0.181)	Loss 2.0876e-01 (2.0876e-01)	Acc@1  94.53 ( 94.53)	Acc@5  99.22 ( 99.22)
Epoch: [23][ 10/391]	Time  0.039 ( 0.054)	Data  0.001 ( 0.018)	Loss 1.7385e-01 (1.8386e-01)	Acc@1  94.53 ( 93.32)	Acc@5 100.00 ( 99.93)
Epoch: [23][ 20/391]	Time  0.036 ( 0.046)	Data  0.001 ( 0.011)	Loss 1.4743e-01 (1.8815e-01)	Acc@1  94.53 ( 93.15)	Acc@5 100.00 ( 99.96)
Epoch: [23][ 30/391]	Time  0.035 ( 0.043)	Data  0.001 ( 0.008)	Loss 2.3170e-01 (1.8637e-01)	Acc@1  92.19 ( 93.37)	Acc@5 100.00 ( 99.97)
Epoch: [23][ 40/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.007)	Loss 2.2129e-01 (1.8185e-01)	Acc@1  91.41 ( 93.48)	Acc@5 100.00 ( 99.98)
Epoch: [23][ 50/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.006)	Loss 1.3614e-01 (1.8293e-01)	Acc@1  96.09 ( 93.57)	Acc@5 100.00 ( 99.95)
Epoch: [23][ 60/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.005)	Loss 2.0220e-01 (1.8390e-01)	Acc@1  92.97 ( 93.47)	Acc@5  99.22 ( 99.95)
Epoch: [23][ 70/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.005)	Loss 1.2187e-01 (1.8589e-01)	Acc@1  94.53 ( 93.45)	Acc@5 100.00 ( 99.94)
Epoch: [23][ 80/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.9067e-01 (1.8391e-01)	Acc@1  95.31 ( 93.52)	Acc@5  99.22 ( 99.94)
Epoch: [23][ 90/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.8948e-01 (1.8606e-01)	Acc@1  92.97 ( 93.34)	Acc@5 100.00 ( 99.95)
Epoch: [23][100/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.4343e-01 (1.8637e-01)	Acc@1  94.53 ( 93.36)	Acc@5 100.00 ( 99.95)
Epoch: [23][110/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.1185e-01 (1.8460e-01)	Acc@1  96.88 ( 93.43)	Acc@5 100.00 ( 99.94)
Epoch: [23][120/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.4963e-01 (1.8764e-01)	Acc@1  96.09 ( 93.36)	Acc@5 100.00 ( 99.94)
Epoch: [23][130/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.5418e-01 (1.8827e-01)	Acc@1  96.09 ( 93.34)	Acc@5 100.00 ( 99.92)
Epoch: [23][140/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.8126e-01 (1.8840e-01)	Acc@1  91.41 ( 93.32)	Acc@5 100.00 ( 99.93)
Epoch: [23][150/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.9543e-01 (1.8900e-01)	Acc@1  92.97 ( 93.32)	Acc@5 100.00 ( 99.93)
Epoch: [23][160/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.6798e-01 (1.8870e-01)	Acc@1  91.41 ( 93.30)	Acc@5 100.00 ( 99.94)
Epoch: [23][170/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.5800e-01 (1.9013e-01)	Acc@1  96.88 ( 93.27)	Acc@5 100.00 ( 99.93)
Epoch: [23][180/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.8578e-01 (1.9065e-01)	Acc@1  93.75 ( 93.29)	Acc@5 100.00 ( 99.93)
Epoch: [23][190/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.5819e-01 (1.8999e-01)	Acc@1  94.53 ( 93.32)	Acc@5 100.00 ( 99.93)
Epoch: [23][200/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.2046e-01 (1.9152e-01)	Acc@1  91.41 ( 93.25)	Acc@5 100.00 ( 99.93)
Epoch: [23][210/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.2073e-01 (1.9217e-01)	Acc@1  92.97 ( 93.27)	Acc@5 100.00 ( 99.93)
Epoch: [23][220/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.0168e-01 (1.9187e-01)	Acc@1  94.53 ( 93.27)	Acc@5  99.22 ( 99.93)
Epoch: [23][230/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.0685e-01 (1.9349e-01)	Acc@1  92.19 ( 93.22)	Acc@5  99.22 ( 99.92)
Epoch: [23][240/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.6276e-01 (1.9416e-01)	Acc@1  92.97 ( 93.18)	Acc@5 100.00 ( 99.92)
Epoch: [23][250/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.5625e-01 (1.9512e-01)	Acc@1  96.09 ( 93.17)	Acc@5 100.00 ( 99.92)
Epoch: [23][260/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.7083e-01 (1.9545e-01)	Acc@1  89.06 ( 93.16)	Acc@5 100.00 ( 99.93)
Epoch: [23][270/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.6801e-01 (1.9568e-01)	Acc@1  90.62 ( 93.16)	Acc@5  99.22 ( 99.92)
Epoch: [23][280/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.6724e-01 (1.9553e-01)	Acc@1  95.31 ( 93.17)	Acc@5  99.22 ( 99.91)
Epoch: [23][290/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.9252e-01 (1.9524e-01)	Acc@1  92.97 ( 93.17)	Acc@5 100.00 ( 99.91)
Epoch: [23][300/391]	Time  0.037 ( 0.038)	Data  0.002 ( 0.003)	Loss 2.6361e-01 (1.9671e-01)	Acc@1  89.84 ( 93.12)	Acc@5 100.00 ( 99.91)
Epoch: [23][310/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.0785e-01 (1.9662e-01)	Acc@1  96.09 ( 93.14)	Acc@5 100.00 ( 99.91)
Epoch: [23][320/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.4644e-01 (1.9825e-01)	Acc@1  92.19 ( 93.10)	Acc@5 100.00 ( 99.91)
Epoch: [23][330/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.3495e-01 (1.9782e-01)	Acc@1  90.62 ( 93.10)	Acc@5 100.00 ( 99.91)
Epoch: [23][340/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.9941e-01 (1.9749e-01)	Acc@1  92.19 ( 93.10)	Acc@5 100.00 ( 99.91)
Epoch: [23][350/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.3724e-01 (1.9775e-01)	Acc@1  89.84 ( 93.09)	Acc@5 100.00 ( 99.91)
Epoch: [23][360/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.8815e-01 (1.9827e-01)	Acc@1  89.84 ( 93.08)	Acc@5 100.00 ( 99.92)
Epoch: [23][370/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.6205e-01 (1.9911e-01)	Acc@1  89.84 ( 93.04)	Acc@5 100.00 ( 99.92)
Epoch: [23][380/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.3952e-01 (1.9842e-01)	Acc@1  92.97 ( 93.06)	Acc@5 100.00 ( 99.91)
Epoch: [23][390/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.4461e-01 (1.9822e-01)	Acc@1  90.00 ( 93.07)	Acc@5 100.00 ( 99.91)
## e[23] optimizer.zero_grad (sum) time: 0.17580175399780273
## e[23]       loss.backward (sum) time: 2.9869887828826904
## e[23]      optimizer.step (sum) time: 1.0301265716552734
## epoch[23] training(only) time: 15.159544706344604
# Switched to evaluate mode...
Test: [  0/100]	Time  0.180 ( 0.180)	Loss 6.1789e-01 (6.1789e-01)	Acc@1  86.00 ( 86.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.018 ( 0.032)	Loss 5.4204e-01 (4.0206e-01)	Acc@1  85.00 ( 87.55)	Acc@5 100.00 ( 99.55)
Test: [ 20/100]	Time  0.024 ( 0.027)	Loss 5.8423e-01 (4.2378e-01)	Acc@1  84.00 ( 87.19)	Acc@5 100.00 ( 99.48)
Test: [ 30/100]	Time  0.023 ( 0.024)	Loss 5.7032e-01 (4.5159e-01)	Acc@1  78.00 ( 86.84)	Acc@5 100.00 ( 99.48)
Test: [ 40/100]	Time  0.016 ( 0.023)	Loss 4.3753e-01 (4.5655e-01)	Acc@1  89.00 ( 86.85)	Acc@5  99.00 ( 99.44)
Test: [ 50/100]	Time  0.020 ( 0.022)	Loss 4.3330e-01 (4.5661e-01)	Acc@1  88.00 ( 86.86)	Acc@5  99.00 ( 99.47)
Test: [ 60/100]	Time  0.022 ( 0.022)	Loss 3.1567e-01 (4.4948e-01)	Acc@1  90.00 ( 86.62)	Acc@5 100.00 ( 99.54)
Test: [ 70/100]	Time  0.019 ( 0.021)	Loss 4.5927e-01 (4.3952e-01)	Acc@1  85.00 ( 86.85)	Acc@5 100.00 ( 99.56)
Test: [ 80/100]	Time  0.021 ( 0.021)	Loss 4.2390e-01 (4.3718e-01)	Acc@1  86.00 ( 86.88)	Acc@5 100.00 ( 99.57)
Test: [ 90/100]	Time  0.019 ( 0.021)	Loss 5.9319e-01 (4.3886e-01)	Acc@1  84.00 ( 86.78)	Acc@5 100.00 ( 99.59)
 * Acc@1 86.830 Acc@5 99.600
### epoch[23] execution time: 17.356263160705566
EPOCH 24
# current learning rate: 0.1
# Switched to train mode...
Epoch: [24][  0/391]	Time  0.223 ( 0.223)	Data  0.185 ( 0.185)	Loss 1.9454e-01 (1.9454e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
Epoch: [24][ 10/391]	Time  0.036 ( 0.055)	Data  0.001 ( 0.019)	Loss 1.9278e-01 (2.2264e-01)	Acc@1  92.97 ( 92.40)	Acc@5  99.22 ( 99.79)
Epoch: [24][ 20/391]	Time  0.037 ( 0.047)	Data  0.001 ( 0.011)	Loss 1.3568e-01 (1.9386e-01)	Acc@1  96.88 ( 93.27)	Acc@5 100.00 ( 99.85)
Epoch: [24][ 30/391]	Time  0.036 ( 0.044)	Data  0.001 ( 0.008)	Loss 2.0683e-01 (1.9256e-01)	Acc@1  93.75 ( 93.30)	Acc@5 100.00 ( 99.87)
Epoch: [24][ 40/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.007)	Loss 2.8525e-01 (1.9243e-01)	Acc@1  91.41 ( 93.33)	Acc@5  99.22 ( 99.87)
Epoch: [24][ 50/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.006)	Loss 1.3846e-01 (1.8561e-01)	Acc@1  93.75 ( 93.49)	Acc@5 100.00 ( 99.89)
Epoch: [24][ 60/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.005)	Loss 1.5981e-01 (1.8543e-01)	Acc@1  92.97 ( 93.52)	Acc@5 100.00 ( 99.88)
Epoch: [24][ 70/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.005)	Loss 2.0740e-01 (1.8977e-01)	Acc@1  94.53 ( 93.41)	Acc@5 100.00 ( 99.89)
Epoch: [24][ 80/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.004)	Loss 3.5143e-01 (1.8833e-01)	Acc@1  88.28 ( 93.49)	Acc@5 100.00 ( 99.89)
Epoch: [24][ 90/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 3.2594e-01 (1.9282e-01)	Acc@1  90.62 ( 93.36)	Acc@5 100.00 ( 99.91)
Epoch: [24][100/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.0264e-01 (1.9250e-01)	Acc@1  96.09 ( 93.35)	Acc@5 100.00 ( 99.91)
Epoch: [24][110/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.1534e-01 (1.9091e-01)	Acc@1  96.88 ( 93.44)	Acc@5 100.00 ( 99.91)
Epoch: [24][120/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.1914e-01 (1.9077e-01)	Acc@1  92.19 ( 93.47)	Acc@5 100.00 ( 99.90)
Epoch: [24][130/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 8.8938e-02 (1.9185e-01)	Acc@1  97.66 ( 93.43)	Acc@5 100.00 ( 99.89)
Epoch: [24][140/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.3059e-01 (1.9271e-01)	Acc@1  92.19 ( 93.41)	Acc@5 100.00 ( 99.89)
Epoch: [24][150/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.8280e-01 (1.9276e-01)	Acc@1  93.75 ( 93.38)	Acc@5 100.00 ( 99.90)
Epoch: [24][160/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.5804e-01 (1.9284e-01)	Acc@1  96.09 ( 93.41)	Acc@5 100.00 ( 99.91)
Epoch: [24][170/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.1093e-01 (1.9248e-01)	Acc@1  95.31 ( 93.41)	Acc@5 100.00 ( 99.91)
Epoch: [24][180/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.9474e-01 (1.9277e-01)	Acc@1  94.53 ( 93.40)	Acc@5  99.22 ( 99.90)
Epoch: [24][190/391]	Time  0.040 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.9017e-01 (1.9412e-01)	Acc@1  92.19 ( 93.34)	Acc@5 100.00 ( 99.89)
Epoch: [24][200/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.7264e-01 (1.9524e-01)	Acc@1  90.62 ( 93.28)	Acc@5 100.00 ( 99.88)
Epoch: [24][210/391]	Time  0.033 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.2436e-01 (1.9568e-01)	Acc@1  94.53 ( 93.28)	Acc@5 100.00 ( 99.89)
Epoch: [24][220/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.2166e-01 (1.9580e-01)	Acc@1  92.97 ( 93.30)	Acc@5 100.00 ( 99.88)
Epoch: [24][230/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.5679e-01 (1.9547e-01)	Acc@1  92.97 ( 93.31)	Acc@5 100.00 ( 99.88)
Epoch: [24][240/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.6699e-01 (1.9555e-01)	Acc@1  91.41 ( 93.31)	Acc@5 100.00 ( 99.88)
Epoch: [24][250/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.3609e-01 (1.9449e-01)	Acc@1  95.31 ( 93.32)	Acc@5 100.00 ( 99.88)
Epoch: [24][260/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.5738e-01 (1.9547e-01)	Acc@1  92.97 ( 93.30)	Acc@5 100.00 ( 99.89)
Epoch: [24][270/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.3657e-01 (1.9398e-01)	Acc@1  93.75 ( 93.35)	Acc@5 100.00 ( 99.89)
Epoch: [24][280/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.9679e-01 (1.9387e-01)	Acc@1  91.41 ( 93.33)	Acc@5 100.00 ( 99.89)
Epoch: [24][290/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.6996e-01 (1.9317e-01)	Acc@1  95.31 ( 93.36)	Acc@5  99.22 ( 99.88)
Epoch: [24][300/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.8897e-01 (1.9348e-01)	Acc@1  89.84 ( 93.34)	Acc@5 100.00 ( 99.89)
Epoch: [24][310/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.9623e-01 (1.9346e-01)	Acc@1  94.53 ( 93.34)	Acc@5 100.00 ( 99.89)
Epoch: [24][320/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.6838e-01 (1.9385e-01)	Acc@1  92.19 ( 93.31)	Acc@5 100.00 ( 99.89)
Epoch: [24][330/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.6631e-01 (1.9434e-01)	Acc@1  94.53 ( 93.29)	Acc@5  99.22 ( 99.89)
Epoch: [24][340/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.4039e-01 (1.9548e-01)	Acc@1  96.09 ( 93.26)	Acc@5 100.00 ( 99.90)
Epoch: [24][350/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.0919e-01 (1.9541e-01)	Acc@1  92.97 ( 93.24)	Acc@5  99.22 ( 99.90)
Epoch: [24][360/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.2699e-01 (1.9521e-01)	Acc@1  92.97 ( 93.25)	Acc@5 100.00 ( 99.90)
Epoch: [24][370/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.2515e-01 (1.9573e-01)	Acc@1  92.97 ( 93.22)	Acc@5 100.00 ( 99.90)
Epoch: [24][380/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.9261e-01 (1.9555e-01)	Acc@1  92.97 ( 93.22)	Acc@5  99.22 ( 99.90)
Epoch: [24][390/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.8733e-01 (1.9571e-01)	Acc@1  91.25 ( 93.21)	Acc@5 100.00 ( 99.90)
## e[24] optimizer.zero_grad (sum) time: 0.17769551277160645
## e[24]       loss.backward (sum) time: 2.9911210536956787
## e[24]      optimizer.step (sum) time: 1.0273559093475342
## epoch[24] training(only) time: 15.149723768234253
# Switched to evaluate mode...
Test: [  0/100]	Time  0.176 ( 0.176)	Loss 3.4297e-01 (3.4297e-01)	Acc@1  87.00 ( 87.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.020 ( 0.033)	Loss 3.7824e-01 (3.2979e-01)	Acc@1  86.00 ( 88.45)	Acc@5  99.00 ( 99.45)
Test: [ 20/100]	Time  0.023 ( 0.027)	Loss 5.2026e-01 (3.6052e-01)	Acc@1  86.00 ( 88.24)	Acc@5 100.00 ( 99.43)
Test: [ 30/100]	Time  0.017 ( 0.024)	Loss 4.3905e-01 (3.7668e-01)	Acc@1  85.00 ( 88.13)	Acc@5 100.00 ( 99.42)
Test: [ 40/100]	Time  0.021 ( 0.023)	Loss 2.7534e-01 (3.7655e-01)	Acc@1  91.00 ( 88.32)	Acc@5  99.00 ( 99.37)
Test: [ 50/100]	Time  0.017 ( 0.022)	Loss 2.6686e-01 (3.6951e-01)	Acc@1  88.00 ( 88.53)	Acc@5  99.00 ( 99.35)
Test: [ 60/100]	Time  0.018 ( 0.022)	Loss 3.6052e-01 (3.6897e-01)	Acc@1  86.00 ( 88.52)	Acc@5  99.00 ( 99.43)
Test: [ 70/100]	Time  0.017 ( 0.021)	Loss 3.9260e-01 (3.6861e-01)	Acc@1  88.00 ( 88.51)	Acc@5 100.00 ( 99.46)
Test: [ 80/100]	Time  0.021 ( 0.021)	Loss 1.9058e-01 (3.6263e-01)	Acc@1  95.00 ( 88.69)	Acc@5  99.00 ( 99.47)
Test: [ 90/100]	Time  0.016 ( 0.021)	Loss 2.3198e-01 (3.6360e-01)	Acc@1  93.00 ( 88.52)	Acc@5 100.00 ( 99.52)
 * Acc@1 88.650 Acc@5 99.520
### epoch[24] execution time: 17.32580804824829
EPOCH 25
# current learning rate: 0.1
# Switched to train mode...
Epoch: [25][  0/391]	Time  0.214 ( 0.214)	Data  0.179 ( 0.179)	Loss 5.1260e-02 (5.1260e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [25][ 10/391]	Time  0.037 ( 0.053)	Data  0.001 ( 0.018)	Loss 9.7147e-02 (1.8766e-01)	Acc@1  95.31 ( 93.47)	Acc@5 100.00 (100.00)
Epoch: [25][ 20/391]	Time  0.036 ( 0.046)	Data  0.001 ( 0.011)	Loss 2.1334e-01 (1.8872e-01)	Acc@1  89.84 ( 93.19)	Acc@5 100.00 ( 99.96)
Epoch: [25][ 30/391]	Time  0.037 ( 0.043)	Data  0.001 ( 0.008)	Loss 1.6296e-01 (1.7836e-01)	Acc@1  91.41 ( 93.42)	Acc@5 100.00 ( 99.95)
Epoch: [25][ 40/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.006)	Loss 1.7631e-01 (1.7262e-01)	Acc@1  94.53 ( 93.69)	Acc@5 100.00 ( 99.92)
Epoch: [25][ 50/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.006)	Loss 1.4939e-01 (1.6673e-01)	Acc@1  94.53 ( 93.90)	Acc@5 100.00 ( 99.94)
Epoch: [25][ 60/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.005)	Loss 1.8135e-01 (1.6780e-01)	Acc@1  91.41 ( 93.90)	Acc@5 100.00 ( 99.92)
Epoch: [25][ 70/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.005)	Loss 1.4835e-01 (1.6746e-01)	Acc@1  95.31 ( 93.87)	Acc@5 100.00 ( 99.91)
Epoch: [25][ 80/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.3092e-01 (1.6673e-01)	Acc@1  95.31 ( 93.89)	Acc@5 100.00 ( 99.90)
Epoch: [25][ 90/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.4702e-01 (1.6991e-01)	Acc@1  96.88 ( 93.82)	Acc@5 100.00 ( 99.91)
Epoch: [25][100/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.4204e-01 (1.7272e-01)	Acc@1  95.31 ( 93.77)	Acc@5 100.00 ( 99.91)
Epoch: [25][110/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.2202e-01 (1.7262e-01)	Acc@1  95.31 ( 93.86)	Acc@5 100.00 ( 99.92)
Epoch: [25][120/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.7910e-01 (1.7239e-01)	Acc@1  91.41 ( 93.86)	Acc@5 100.00 ( 99.93)
Epoch: [25][130/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.6083e-01 (1.7388e-01)	Acc@1  88.28 ( 93.79)	Acc@5 100.00 ( 99.93)
Epoch: [25][140/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.1024e-01 (1.7434e-01)	Acc@1  96.88 ( 93.76)	Acc@5 100.00 ( 99.93)
Epoch: [25][150/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.1256e-01 (1.7517e-01)	Acc@1  94.53 ( 93.73)	Acc@5  99.22 ( 99.93)
Epoch: [25][160/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.6470e-01 (1.7821e-01)	Acc@1  87.50 ( 93.60)	Acc@5 100.00 ( 99.93)
Epoch: [25][170/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.2963e-01 (1.7933e-01)	Acc@1  92.97 ( 93.61)	Acc@5 100.00 ( 99.93)
Epoch: [25][180/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.7793e-01 (1.8206e-01)	Acc@1  94.53 ( 93.58)	Acc@5 100.00 ( 99.93)
Epoch: [25][190/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.1149e-01 (1.8402e-01)	Acc@1  95.31 ( 93.53)	Acc@5 100.00 ( 99.93)
Epoch: [25][200/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.1829e-01 (1.8466e-01)	Acc@1  92.19 ( 93.49)	Acc@5  99.22 ( 99.93)
Epoch: [25][210/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.3174e-01 (1.8550e-01)	Acc@1  93.75 ( 93.49)	Acc@5 100.00 ( 99.92)
Epoch: [25][220/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.7152e-01 (1.8449e-01)	Acc@1  94.53 ( 93.51)	Acc@5 100.00 ( 99.92)
Epoch: [25][230/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.3150e-01 (1.8450e-01)	Acc@1  95.31 ( 93.51)	Acc@5 100.00 ( 99.91)
Epoch: [25][240/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.9319e-01 (1.8543e-01)	Acc@1  92.19 ( 93.51)	Acc@5 100.00 ( 99.91)
Epoch: [25][250/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.5982e-01 (1.8542e-01)	Acc@1  94.53 ( 93.52)	Acc@5 100.00 ( 99.91)
Epoch: [25][260/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.3965e-01 (1.8565e-01)	Acc@1  90.62 ( 93.52)	Acc@5 100.00 ( 99.91)
Epoch: [25][270/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.7112e-01 (1.8603e-01)	Acc@1  93.75 ( 93.52)	Acc@5 100.00 ( 99.90)
Epoch: [25][280/391]	Time  0.036 ( 0.038)	Data  0.002 ( 0.003)	Loss 2.1122e-01 (1.8845e-01)	Acc@1  91.41 ( 93.45)	Acc@5 100.00 ( 99.90)
Epoch: [25][290/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.6808e-01 (1.8881e-01)	Acc@1  92.97 ( 93.42)	Acc@5 100.00 ( 99.91)
Epoch: [25][300/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.3640e-01 (1.8964e-01)	Acc@1  91.41 ( 93.38)	Acc@5  99.22 ( 99.91)
Epoch: [25][310/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.4959e-01 (1.8890e-01)	Acc@1  95.31 ( 93.41)	Acc@5 100.00 ( 99.90)
Epoch: [25][320/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.1921e-01 (1.8849e-01)	Acc@1  93.75 ( 93.43)	Acc@5 100.00 ( 99.90)
Epoch: [25][330/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.4816e-01 (1.8924e-01)	Acc@1  86.72 ( 93.40)	Acc@5  99.22 ( 99.90)
Epoch: [25][340/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.6265e-01 (1.8961e-01)	Acc@1  92.19 ( 93.39)	Acc@5 100.00 ( 99.90)
Epoch: [25][350/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.2489e-01 (1.8979e-01)	Acc@1  92.19 ( 93.37)	Acc@5 100.00 ( 99.90)
Epoch: [25][360/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.3843e-01 (1.8979e-01)	Acc@1  95.31 ( 93.37)	Acc@5 100.00 ( 99.90)
Epoch: [25][370/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.8765e-01 (1.8980e-01)	Acc@1  90.62 ( 93.37)	Acc@5 100.00 ( 99.91)
Epoch: [25][380/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.5816e-01 (1.8999e-01)	Acc@1  85.94 ( 93.36)	Acc@5  99.22 ( 99.90)
Epoch: [25][390/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.6124e-01 (1.9019e-01)	Acc@1  83.75 ( 93.35)	Acc@5 100.00 ( 99.90)
## e[25] optimizer.zero_grad (sum) time: 0.17498111724853516
## e[25]       loss.backward (sum) time: 2.936030387878418
## e[25]      optimizer.step (sum) time: 1.0295486450195312
## epoch[25] training(only) time: 15.128148078918457
# Switched to evaluate mode...
Test: [  0/100]	Time  0.180 ( 0.180)	Loss 3.5294e-01 (3.5294e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.023 ( 0.033)	Loss 4.3876e-01 (3.2292e-01)	Acc@1  86.00 ( 89.18)	Acc@5 100.00 ( 99.55)
Test: [ 20/100]	Time  0.020 ( 0.027)	Loss 3.7364e-01 (3.4734e-01)	Acc@1  86.00 ( 88.76)	Acc@5 100.00 ( 99.48)
Test: [ 30/100]	Time  0.021 ( 0.025)	Loss 3.8567e-01 (3.5526e-01)	Acc@1  87.00 ( 88.74)	Acc@5 100.00 ( 99.45)
Test: [ 40/100]	Time  0.023 ( 0.023)	Loss 2.9635e-01 (3.6197e-01)	Acc@1  91.00 ( 88.88)	Acc@5  99.00 ( 99.41)
Test: [ 50/100]	Time  0.019 ( 0.023)	Loss 2.6322e-01 (3.5408e-01)	Acc@1  92.00 ( 89.08)	Acc@5 100.00 ( 99.45)
Test: [ 60/100]	Time  0.016 ( 0.022)	Loss 3.3404e-01 (3.5323e-01)	Acc@1  90.00 ( 89.16)	Acc@5 100.00 ( 99.49)
Test: [ 70/100]	Time  0.016 ( 0.021)	Loss 2.6663e-01 (3.4664e-01)	Acc@1  90.00 ( 89.35)	Acc@5 100.00 ( 99.52)
Test: [ 80/100]	Time  0.023 ( 0.021)	Loss 2.1552e-01 (3.4592e-01)	Acc@1  90.00 ( 89.32)	Acc@5 100.00 ( 99.56)
Test: [ 90/100]	Time  0.018 ( 0.021)	Loss 3.1896e-01 (3.4310e-01)	Acc@1  93.00 ( 89.38)	Acc@5 100.00 ( 99.58)
 * Acc@1 89.320 Acc@5 99.610
### epoch[25] execution time: 17.309001684188843
EPOCH 26
# current learning rate: 0.1
# Switched to train mode...
Epoch: [26][  0/391]	Time  0.214 ( 0.214)	Data  0.180 ( 0.180)	Loss 2.1382e-01 (2.1382e-01)	Acc@1  91.41 ( 91.41)	Acc@5 100.00 (100.00)
Epoch: [26][ 10/391]	Time  0.036 ( 0.053)	Data  0.001 ( 0.018)	Loss 1.6428e-01 (1.7556e-01)	Acc@1  93.75 ( 93.32)	Acc@5 100.00 ( 99.86)
Epoch: [26][ 20/391]	Time  0.036 ( 0.046)	Data  0.001 ( 0.011)	Loss 1.3440e-01 (1.7698e-01)	Acc@1  95.31 ( 93.75)	Acc@5 100.00 ( 99.85)
Epoch: [26][ 30/391]	Time  0.037 ( 0.043)	Data  0.001 ( 0.008)	Loss 1.2810e-01 (1.7804e-01)	Acc@1  95.31 ( 93.65)	Acc@5 100.00 ( 99.87)
Epoch: [26][ 40/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.007)	Loss 1.5777e-01 (1.7256e-01)	Acc@1  96.09 ( 94.04)	Acc@5 100.00 ( 99.89)
Epoch: [26][ 50/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.006)	Loss 9.9132e-02 (1.6711e-01)	Acc@1  96.88 ( 94.09)	Acc@5 100.00 ( 99.89)
Epoch: [26][ 60/391]	Time  0.034 ( 0.041)	Data  0.002 ( 0.005)	Loss 1.7297e-01 (1.6186e-01)	Acc@1  92.97 ( 94.33)	Acc@5 100.00 ( 99.91)
Epoch: [26][ 70/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.005)	Loss 1.0485e-01 (1.5982e-01)	Acc@1  95.31 ( 94.29)	Acc@5 100.00 ( 99.92)
Epoch: [26][ 80/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.004)	Loss 9.8376e-02 (1.5676e-01)	Acc@1  96.88 ( 94.42)	Acc@5 100.00 ( 99.93)
Epoch: [26][ 90/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.6000e-01 (1.5616e-01)	Acc@1  92.19 ( 94.45)	Acc@5 100.00 ( 99.93)
Epoch: [26][100/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.0629e-01 (1.5867e-01)	Acc@1  93.75 ( 94.40)	Acc@5  99.22 ( 99.93)
Epoch: [26][110/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.3886e-01 (1.6294e-01)	Acc@1  94.53 ( 94.26)	Acc@5  99.22 ( 99.92)
Epoch: [26][120/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.7747e-01 (1.6717e-01)	Acc@1  94.53 ( 94.16)	Acc@5 100.00 ( 99.91)
Epoch: [26][130/391]	Time  0.040 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.0460e-01 (1.6988e-01)	Acc@1  96.88 ( 94.11)	Acc@5 100.00 ( 99.90)
Epoch: [26][140/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.6404e-01 (1.7105e-01)	Acc@1  91.41 ( 94.04)	Acc@5  99.22 ( 99.90)
Epoch: [26][150/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.3471e-01 (1.7198e-01)	Acc@1  89.06 ( 93.98)	Acc@5 100.00 ( 99.89)
Epoch: [26][160/391]	Time  0.037 ( 0.039)	Data  0.002 ( 0.003)	Loss 1.3970e-01 (1.7146e-01)	Acc@1  94.53 ( 93.93)	Acc@5 100.00 ( 99.89)
Epoch: [26][170/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.3732e-01 (1.7417e-01)	Acc@1  93.75 ( 93.87)	Acc@5 100.00 ( 99.89)
Epoch: [26][180/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.7225e-01 (1.7491e-01)	Acc@1  89.84 ( 93.86)	Acc@5 100.00 ( 99.90)
Epoch: [26][190/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.8392e-01 (1.7668e-01)	Acc@1  94.53 ( 93.77)	Acc@5 100.00 ( 99.90)
Epoch: [26][200/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.4324e-01 (1.7753e-01)	Acc@1  92.19 ( 93.74)	Acc@5 100.00 ( 99.91)
Epoch: [26][210/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.4532e-01 (1.7910e-01)	Acc@1  96.09 ( 93.69)	Acc@5 100.00 ( 99.91)
Epoch: [26][220/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.6421e-01 (1.7968e-01)	Acc@1  90.62 ( 93.65)	Acc@5 100.00 ( 99.90)
Epoch: [26][230/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.0528e-01 (1.8103e-01)	Acc@1  94.53 ( 93.63)	Acc@5 100.00 ( 99.90)
Epoch: [26][240/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.0101e-01 (1.8138e-01)	Acc@1  92.97 ( 93.60)	Acc@5  99.22 ( 99.90)
Epoch: [26][250/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.5289e-01 (1.8161e-01)	Acc@1  90.62 ( 93.59)	Acc@5 100.00 ( 99.90)
Epoch: [26][260/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.7794e-01 (1.8238e-01)	Acc@1  93.75 ( 93.60)	Acc@5 100.00 ( 99.90)
Epoch: [26][270/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.8568e-01 (1.8204e-01)	Acc@1  92.97 ( 93.62)	Acc@5 100.00 ( 99.91)
Epoch: [26][280/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.5648e-01 (1.8264e-01)	Acc@1  92.97 ( 93.61)	Acc@5 100.00 ( 99.91)
Epoch: [26][290/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.5539e-01 (1.8182e-01)	Acc@1  93.75 ( 93.63)	Acc@5 100.00 ( 99.91)
Epoch: [26][300/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.2024e-01 (1.8135e-01)	Acc@1  96.88 ( 93.64)	Acc@5 100.00 ( 99.91)
Epoch: [26][310/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.5250e-01 (1.8205e-01)	Acc@1  93.75 ( 93.65)	Acc@5  99.22 ( 99.91)
Epoch: [26][320/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.5882e-01 (1.8175e-01)	Acc@1  95.31 ( 93.67)	Acc@5 100.00 ( 99.91)
Epoch: [26][330/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.8250e-01 (1.8294e-01)	Acc@1  87.50 ( 93.64)	Acc@5 100.00 ( 99.91)
Epoch: [26][340/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.9310e-01 (1.8347e-01)	Acc@1  89.06 ( 93.61)	Acc@5 100.00 ( 99.91)
Epoch: [26][350/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.5647e-01 (1.8487e-01)	Acc@1  96.09 ( 93.56)	Acc@5 100.00 ( 99.91)
Epoch: [26][360/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.7376e-01 (1.8482e-01)	Acc@1  94.53 ( 93.56)	Acc@5 100.00 ( 99.91)
Epoch: [26][370/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.8727e-01 (1.8471e-01)	Acc@1  92.19 ( 93.55)	Acc@5 100.00 ( 99.91)
Epoch: [26][380/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.5226e-01 (1.8464e-01)	Acc@1  95.31 ( 93.56)	Acc@5 100.00 ( 99.91)
Epoch: [26][390/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.8436e-01 (1.8490e-01)	Acc@1  91.25 ( 93.56)	Acc@5 100.00 ( 99.91)
## e[26] optimizer.zero_grad (sum) time: 0.1743762493133545
## e[26]       loss.backward (sum) time: 2.9796082973480225
## e[26]      optimizer.step (sum) time: 1.0172040462493896
## epoch[26] training(only) time: 15.129456281661987
# Switched to evaluate mode...
Test: [  0/100]	Time  0.176 ( 0.176)	Loss 3.5310e-01 (3.5310e-01)	Acc@1  87.00 ( 87.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.018 ( 0.032)	Loss 4.7857e-01 (3.0756e-01)	Acc@1  86.00 ( 89.82)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.017 ( 0.025)	Loss 5.2739e-01 (3.4810e-01)	Acc@1  87.00 ( 88.48)	Acc@5 100.00 ( 99.52)
Test: [ 30/100]	Time  0.021 ( 0.023)	Loss 4.5601e-01 (3.5812e-01)	Acc@1  85.00 ( 88.26)	Acc@5 100.00 ( 99.55)
Test: [ 40/100]	Time  0.023 ( 0.022)	Loss 3.8341e-01 (3.5435e-01)	Acc@1  87.00 ( 88.46)	Acc@5 100.00 ( 99.54)
Test: [ 50/100]	Time  0.016 ( 0.021)	Loss 2.4367e-01 (3.5528e-01)	Acc@1  93.00 ( 88.41)	Acc@5 100.00 ( 99.51)
Test: [ 60/100]	Time  0.023 ( 0.021)	Loss 3.2502e-01 (3.5154e-01)	Acc@1  90.00 ( 88.54)	Acc@5 100.00 ( 99.54)
Test: [ 70/100]	Time  0.023 ( 0.020)	Loss 4.6362e-01 (3.4586e-01)	Acc@1  84.00 ( 88.72)	Acc@5 100.00 ( 99.55)
Test: [ 80/100]	Time  0.020 ( 0.020)	Loss 2.8617e-01 (3.4263e-01)	Acc@1  91.00 ( 88.81)	Acc@5 100.00 ( 99.54)
Test: [ 90/100]	Time  0.020 ( 0.020)	Loss 1.7335e-01 (3.3925e-01)	Acc@1  95.00 ( 88.77)	Acc@5 100.00 ( 99.57)
 * Acc@1 88.850 Acc@5 99.610
### epoch[26] execution time: 17.253246068954468
EPOCH 27
# current learning rate: 0.1
# Switched to train mode...
Epoch: [27][  0/391]	Time  0.205 ( 0.205)	Data  0.171 ( 0.171)	Loss 1.3814e-01 (1.3814e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [27][ 10/391]	Time  0.037 ( 0.052)	Data  0.001 ( 0.017)	Loss 1.7423e-01 (1.5235e-01)	Acc@1  93.75 ( 94.96)	Acc@5 100.00 (100.00)
Epoch: [27][ 20/391]	Time  0.037 ( 0.046)	Data  0.001 ( 0.010)	Loss 1.3732e-01 (1.6074e-01)	Acc@1  94.53 ( 94.46)	Acc@5 100.00 ( 99.93)
Epoch: [27][ 30/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.008)	Loss 1.1475e-01 (1.6369e-01)	Acc@1  96.09 ( 94.20)	Acc@5 100.00 ( 99.95)
Epoch: [27][ 40/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.006)	Loss 2.0176e-01 (1.6096e-01)	Acc@1  92.19 ( 94.17)	Acc@5 100.00 ( 99.94)
Epoch: [27][ 50/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.005)	Loss 2.5550e-01 (1.6501e-01)	Acc@1  90.62 ( 94.06)	Acc@5 100.00 ( 99.95)
Epoch: [27][ 60/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.005)	Loss 1.4510e-01 (1.6885e-01)	Acc@1  94.53 ( 94.02)	Acc@5 100.00 ( 99.96)
Epoch: [27][ 70/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.8265e-01 (1.6848e-01)	Acc@1  92.97 ( 93.98)	Acc@5 100.00 ( 99.94)
Epoch: [27][ 80/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.9587e-01 (1.7246e-01)	Acc@1  91.41 ( 93.91)	Acc@5 100.00 ( 99.93)
Epoch: [27][ 90/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.7782e-01 (1.7441e-01)	Acc@1  95.31 ( 93.84)	Acc@5 100.00 ( 99.93)
Epoch: [27][100/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.0625e-01 (1.7406e-01)	Acc@1  92.97 ( 93.83)	Acc@5 100.00 ( 99.93)
Epoch: [27][110/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.7651e-01 (1.7305e-01)	Acc@1  92.97 ( 93.81)	Acc@5 100.00 ( 99.93)
Epoch: [27][120/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.7260e-01 (1.7316e-01)	Acc@1  94.53 ( 93.81)	Acc@5 100.00 ( 99.94)
Epoch: [27][130/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.3827e-01 (1.7285e-01)	Acc@1  96.09 ( 93.85)	Acc@5 100.00 ( 99.93)
Epoch: [27][140/391]	Time  0.037 ( 0.039)	Data  0.002 ( 0.003)	Loss 8.3183e-02 (1.7314e-01)	Acc@1  96.88 ( 93.87)	Acc@5 100.00 ( 99.94)
Epoch: [27][150/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.7264e-02 (1.7333e-01)	Acc@1  98.44 ( 93.88)	Acc@5 100.00 ( 99.94)
Epoch: [27][160/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.7161e-01 (1.7389e-01)	Acc@1  93.75 ( 93.86)	Acc@5 100.00 ( 99.94)
Epoch: [27][170/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.0421e-01 (1.7410e-01)	Acc@1  89.84 ( 93.82)	Acc@5 100.00 ( 99.94)
Epoch: [27][180/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.1550e-01 (1.7556e-01)	Acc@1  93.75 ( 93.81)	Acc@5 100.00 ( 99.94)
Epoch: [27][190/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 9.0702e-02 (1.7541e-01)	Acc@1  98.44 ( 93.88)	Acc@5 100.00 ( 99.94)
Epoch: [27][200/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.4691e-01 (1.7644e-01)	Acc@1  94.53 ( 93.82)	Acc@5 100.00 ( 99.94)
Epoch: [27][210/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.3509e-01 (1.7631e-01)	Acc@1  92.97 ( 93.79)	Acc@5 100.00 ( 99.94)
Epoch: [27][220/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.4682e-01 (1.7719e-01)	Acc@1  94.53 ( 93.77)	Acc@5  99.22 ( 99.93)
Epoch: [27][230/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.8517e-01 (1.7678e-01)	Acc@1  93.75 ( 93.80)	Acc@5 100.00 ( 99.93)
Epoch: [27][240/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.8984e-01 (1.7782e-01)	Acc@1  91.41 ( 93.77)	Acc@5 100.00 ( 99.93)
Epoch: [27][250/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.3123e-01 (1.7870e-01)	Acc@1  92.19 ( 93.72)	Acc@5 100.00 ( 99.93)
Epoch: [27][260/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.0660e-01 (1.7920e-01)	Acc@1  98.44 ( 93.72)	Acc@5  99.22 ( 99.93)
Epoch: [27][270/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.7506e-01 (1.7999e-01)	Acc@1  92.97 ( 93.68)	Acc@5 100.00 ( 99.93)
Epoch: [27][280/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.4042e-01 (1.8060e-01)	Acc@1  94.53 ( 93.66)	Acc@5 100.00 ( 99.92)
Epoch: [27][290/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.7224e-01 (1.8038e-01)	Acc@1  94.53 ( 93.66)	Acc@5 100.00 ( 99.92)
Epoch: [27][300/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.0431e-01 (1.8069e-01)	Acc@1  94.53 ( 93.66)	Acc@5 100.00 ( 99.92)
Epoch: [27][310/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.1754e-01 (1.8083e-01)	Acc@1  92.19 ( 93.68)	Acc@5 100.00 ( 99.92)
Epoch: [27][320/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.9956e-01 (1.8120e-01)	Acc@1  92.97 ( 93.68)	Acc@5 100.00 ( 99.91)
Epoch: [27][330/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.0494e-01 (1.8118e-01)	Acc@1  95.31 ( 93.67)	Acc@5 100.00 ( 99.91)
Epoch: [27][340/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.6009e-01 (1.8141e-01)	Acc@1  94.53 ( 93.66)	Acc@5 100.00 ( 99.91)
Epoch: [27][350/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.7078e-01 (1.8045e-01)	Acc@1  94.53 ( 93.69)	Acc@5 100.00 ( 99.91)
Epoch: [27][360/391]	Time  0.035 ( 0.038)	Data  0.002 ( 0.003)	Loss 1.4850e-01 (1.8042e-01)	Acc@1  92.97 ( 93.68)	Acc@5 100.00 ( 99.91)
Epoch: [27][370/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.2278e-01 (1.8042e-01)	Acc@1  90.62 ( 93.67)	Acc@5 100.00 ( 99.91)
Epoch: [27][380/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.9550e-01 (1.8048e-01)	Acc@1  90.62 ( 93.66)	Acc@5 100.00 ( 99.91)
Epoch: [27][390/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.5030e-01 (1.8062e-01)	Acc@1  86.25 ( 93.65)	Acc@5 100.00 ( 99.91)
## e[27] optimizer.zero_grad (sum) time: 0.17650341987609863
## e[27]       loss.backward (sum) time: 3.004871129989624
## e[27]      optimizer.step (sum) time: 1.0267443656921387
## epoch[27] training(only) time: 15.159297943115234
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 4.0111e-01 (4.0111e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.021 ( 0.033)	Loss 4.4569e-01 (4.0945e-01)	Acc@1  86.00 ( 87.36)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.017 ( 0.026)	Loss 5.1062e-01 (4.2403e-01)	Acc@1  83.00 ( 87.00)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.020 ( 0.024)	Loss 2.7588e-01 (4.2186e-01)	Acc@1  92.00 ( 87.35)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.017 ( 0.022)	Loss 4.0088e-01 (4.2449e-01)	Acc@1  89.00 ( 87.22)	Acc@5 100.00 ( 99.61)
Test: [ 50/100]	Time  0.017 ( 0.022)	Loss 2.4188e-01 (4.1565e-01)	Acc@1  93.00 ( 87.51)	Acc@5 100.00 ( 99.55)
Test: [ 60/100]	Time  0.017 ( 0.021)	Loss 3.7760e-01 (4.1560e-01)	Acc@1  89.00 ( 87.48)	Acc@5 100.00 ( 99.59)
Test: [ 70/100]	Time  0.023 ( 0.021)	Loss 4.2614e-01 (4.1024e-01)	Acc@1  86.00 ( 87.51)	Acc@5 100.00 ( 99.63)
Test: [ 80/100]	Time  0.016 ( 0.021)	Loss 2.9411e-01 (4.0868e-01)	Acc@1  91.00 ( 87.51)	Acc@5  99.00 ( 99.63)
Test: [ 90/100]	Time  0.016 ( 0.020)	Loss 2.0824e-01 (4.0973e-01)	Acc@1  92.00 ( 87.47)	Acc@5 100.00 ( 99.64)
 * Acc@1 87.620 Acc@5 99.640
### epoch[27] execution time: 17.337568044662476
EPOCH 28
# current learning rate: 0.1
# Switched to train mode...
Epoch: [28][  0/391]	Time  0.218 ( 0.218)	Data  0.183 ( 0.183)	Loss 9.5987e-02 (9.5987e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [28][ 10/391]	Time  0.037 ( 0.053)	Data  0.001 ( 0.018)	Loss 1.4461e-01 (1.5178e-01)	Acc@1  96.88 ( 95.17)	Acc@5 100.00 (100.00)
Epoch: [28][ 20/391]	Time  0.037 ( 0.046)	Data  0.001 ( 0.011)	Loss 1.7134e-01 (1.5481e-01)	Acc@1  92.97 ( 94.90)	Acc@5 100.00 (100.00)
Epoch: [28][ 30/391]	Time  0.037 ( 0.043)	Data  0.001 ( 0.008)	Loss 1.2343e-01 (1.5934e-01)	Acc@1  95.31 ( 94.91)	Acc@5 100.00 ( 99.95)
Epoch: [28][ 40/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.007)	Loss 1.0441e-01 (1.5957e-01)	Acc@1  96.88 ( 94.74)	Acc@5 100.00 ( 99.92)
Epoch: [28][ 50/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.006)	Loss 2.1851e-01 (1.6196e-01)	Acc@1  92.19 ( 94.58)	Acc@5 100.00 ( 99.92)
Epoch: [28][ 60/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.005)	Loss 1.5713e-01 (1.6068e-01)	Acc@1  93.75 ( 94.58)	Acc@5 100.00 ( 99.91)
Epoch: [28][ 70/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.005)	Loss 1.4473e-01 (1.6125e-01)	Acc@1  95.31 ( 94.59)	Acc@5 100.00 ( 99.90)
Epoch: [28][ 80/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.5588e-01 (1.5868e-01)	Acc@1  94.53 ( 94.67)	Acc@5 100.00 ( 99.91)
Epoch: [28][ 90/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.0509e-01 (1.5827e-01)	Acc@1  92.97 ( 94.57)	Acc@5 100.00 ( 99.91)
Epoch: [28][100/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.3790e-01 (1.5905e-01)	Acc@1  94.53 ( 94.50)	Acc@5 100.00 ( 99.90)
Epoch: [28][110/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.9326e-01 (1.6005e-01)	Acc@1  92.97 ( 94.45)	Acc@5 100.00 ( 99.89)
Epoch: [28][120/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.1740e-01 (1.6009e-01)	Acc@1  94.53 ( 94.38)	Acc@5 100.00 ( 99.90)
Epoch: [28][130/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.4875e-01 (1.6004e-01)	Acc@1  93.75 ( 94.41)	Acc@5 100.00 ( 99.91)
Epoch: [28][140/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.5663e-02 (1.6046e-01)	Acc@1  96.88 ( 94.40)	Acc@5 100.00 ( 99.91)
Epoch: [28][150/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.3829e-01 (1.6098e-01)	Acc@1  96.88 ( 94.43)	Acc@5 100.00 ( 99.91)
Epoch: [28][160/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.3719e-01 (1.6133e-01)	Acc@1  96.09 ( 94.41)	Acc@5 100.00 ( 99.92)
Epoch: [28][170/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.9420e-01 (1.6179e-01)	Acc@1  93.75 ( 94.42)	Acc@5 100.00 ( 99.92)
Epoch: [28][180/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.5100e-01 (1.6293e-01)	Acc@1  89.84 ( 94.36)	Acc@5 100.00 ( 99.92)
Epoch: [28][190/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.0906e-01 (1.6487e-01)	Acc@1  92.19 ( 94.27)	Acc@5 100.00 ( 99.91)
Epoch: [28][200/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.8591e-01 (1.6492e-01)	Acc@1  94.53 ( 94.26)	Acc@5 100.00 ( 99.92)
Epoch: [28][210/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.7342e-01 (1.6546e-01)	Acc@1  94.53 ( 94.24)	Acc@5 100.00 ( 99.92)
Epoch: [28][220/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.8392e-01 (1.6629e-01)	Acc@1  92.97 ( 94.18)	Acc@5 100.00 ( 99.92)
Epoch: [28][230/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.4846e-01 (1.6885e-01)	Acc@1  89.06 ( 94.08)	Acc@5 100.00 ( 99.92)
Epoch: [28][240/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.7919e-01 (1.7022e-01)	Acc@1  92.97 ( 94.06)	Acc@5 100.00 ( 99.92)
Epoch: [28][250/391]	Time  0.049 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.2458e-01 (1.7018e-01)	Acc@1  96.88 ( 94.11)	Acc@5 100.00 ( 99.92)
Epoch: [28][260/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.0913e-01 (1.7125e-01)	Acc@1  89.06 ( 94.07)	Acc@5  99.22 ( 99.91)
Epoch: [28][270/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.5767e-01 (1.7307e-01)	Acc@1  94.53 ( 94.02)	Acc@5  99.22 ( 99.91)
Epoch: [28][280/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.5114e-01 (1.7338e-01)	Acc@1  94.53 ( 94.00)	Acc@5 100.00 ( 99.91)
Epoch: [28][290/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.4866e-01 (1.7249e-01)	Acc@1  96.88 ( 94.04)	Acc@5 100.00 ( 99.91)
Epoch: [28][300/391]	Time  0.038 ( 0.038)	Data  0.002 ( 0.003)	Loss 2.0265e-01 (1.7352e-01)	Acc@1  91.41 ( 94.02)	Acc@5 100.00 ( 99.90)
Epoch: [28][310/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.1468e-01 (1.7363e-01)	Acc@1  96.09 ( 94.02)	Acc@5 100.00 ( 99.90)
Epoch: [28][320/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.9684e-01 (1.7462e-01)	Acc@1  92.97 ( 94.00)	Acc@5 100.00 ( 99.90)
Epoch: [28][330/391]	Time  0.036 ( 0.038)	Data  0.002 ( 0.003)	Loss 2.8519e-01 (1.7549e-01)	Acc@1  88.28 ( 93.95)	Acc@5 100.00 ( 99.91)
Epoch: [28][340/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.0748e-01 (1.7553e-01)	Acc@1  93.75 ( 93.93)	Acc@5 100.00 ( 99.90)
Epoch: [28][350/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.2442e-01 (1.7588e-01)	Acc@1  92.19 ( 93.92)	Acc@5 100.00 ( 99.90)
Epoch: [28][360/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.8974e-01 (1.7629e-01)	Acc@1  92.97 ( 93.90)	Acc@5 100.00 ( 99.90)
Epoch: [28][370/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.7733e-01 (1.7674e-01)	Acc@1  90.62 ( 93.92)	Acc@5 100.00 ( 99.91)
Epoch: [28][380/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.5125e-01 (1.7676e-01)	Acc@1  93.75 ( 93.91)	Acc@5 100.00 ( 99.91)
Epoch: [28][390/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.2378e-01 (1.7697e-01)	Acc@1  96.25 ( 93.89)	Acc@5 100.00 ( 99.91)
## e[28] optimizer.zero_grad (sum) time: 0.17600440979003906
## e[28]       loss.backward (sum) time: 3.000828266143799
## e[28]      optimizer.step (sum) time: 1.0347378253936768
## epoch[28] training(only) time: 15.107575178146362
# Switched to evaluate mode...
Test: [  0/100]	Time  0.162 ( 0.162)	Loss 3.3280e-01 (3.3280e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.018 ( 0.032)	Loss 5.3012e-01 (3.5294e-01)	Acc@1  85.00 ( 88.64)	Acc@5  99.00 ( 99.55)
Test: [ 20/100]	Time  0.016 ( 0.025)	Loss 4.6018e-01 (3.7240e-01)	Acc@1  85.00 ( 89.10)	Acc@5 100.00 ( 99.52)
Test: [ 30/100]	Time  0.023 ( 0.023)	Loss 4.6470e-01 (3.7201e-01)	Acc@1  87.00 ( 89.00)	Acc@5  99.00 ( 99.52)
Test: [ 40/100]	Time  0.028 ( 0.023)	Loss 3.3475e-01 (3.6662e-01)	Acc@1  90.00 ( 89.12)	Acc@5  99.00 ( 99.49)
Test: [ 50/100]	Time  0.017 ( 0.022)	Loss 2.7951e-01 (3.7214e-01)	Acc@1  87.00 ( 88.86)	Acc@5  99.00 ( 99.45)
Test: [ 60/100]	Time  0.020 ( 0.022)	Loss 3.1150e-01 (3.6748e-01)	Acc@1  89.00 ( 88.92)	Acc@5  99.00 ( 99.51)
Test: [ 70/100]	Time  0.025 ( 0.022)	Loss 5.6331e-01 (3.7009e-01)	Acc@1  85.00 ( 88.79)	Acc@5 100.00 ( 99.55)
Test: [ 80/100]	Time  0.017 ( 0.021)	Loss 2.6854e-01 (3.6936e-01)	Acc@1  90.00 ( 88.80)	Acc@5 100.00 ( 99.54)
Test: [ 90/100]	Time  0.017 ( 0.021)	Loss 1.5527e-01 (3.6719e-01)	Acc@1  94.00 ( 88.78)	Acc@5 100.00 ( 99.56)
 * Acc@1 88.950 Acc@5 99.560
### epoch[28] execution time: 17.354447841644287
EPOCH 29
# current learning rate: 0.1
# Switched to train mode...
Epoch: [29][  0/391]	Time  0.207 ( 0.207)	Data  0.173 ( 0.173)	Loss 1.4080e-01 (1.4080e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [29][ 10/391]	Time  0.039 ( 0.053)	Data  0.001 ( 0.017)	Loss 1.6303e-01 (1.5504e-01)	Acc@1  93.75 ( 94.25)	Acc@5 100.00 ( 99.93)
Epoch: [29][ 20/391]	Time  0.037 ( 0.046)	Data  0.001 ( 0.010)	Loss 1.0710e-01 (1.4137e-01)	Acc@1  96.09 ( 94.72)	Acc@5 100.00 ( 99.96)
Epoch: [29][ 30/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.008)	Loss 8.5889e-02 (1.4179e-01)	Acc@1  96.88 ( 94.76)	Acc@5 100.00 ( 99.95)
Epoch: [29][ 40/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.006)	Loss 1.1924e-01 (1.4566e-01)	Acc@1  95.31 ( 94.68)	Acc@5 100.00 ( 99.96)
Epoch: [29][ 50/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.006)	Loss 7.6796e-02 (1.4484e-01)	Acc@1  99.22 ( 94.72)	Acc@5 100.00 ( 99.97)
Epoch: [29][ 60/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.005)	Loss 1.3957e-01 (1.4357e-01)	Acc@1  94.53 ( 94.68)	Acc@5 100.00 ( 99.94)
Epoch: [29][ 70/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.005)	Loss 1.9618e-01 (1.4148e-01)	Acc@1  93.75 ( 94.84)	Acc@5 100.00 ( 99.94)
Epoch: [29][ 80/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.1206e-01 (1.4160e-01)	Acc@1  96.88 ( 94.85)	Acc@5 100.00 ( 99.95)
Epoch: [29][ 90/391]	Time  0.037 ( 0.040)	Data  0.002 ( 0.004)	Loss 1.2646e-01 (1.4113e-01)	Acc@1  95.31 ( 94.88)	Acc@5 100.00 ( 99.96)
Epoch: [29][100/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.4908e-01 (1.4099e-01)	Acc@1  95.31 ( 94.91)	Acc@5 100.00 ( 99.95)
Epoch: [29][110/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 8.1248e-02 (1.4298e-01)	Acc@1  98.44 ( 94.83)	Acc@5 100.00 ( 99.95)
Epoch: [29][120/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.8476e-01 (1.4610e-01)	Acc@1  92.19 ( 94.79)	Acc@5 100.00 ( 99.95)
Epoch: [29][130/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.7253e-01 (1.4655e-01)	Acc@1  92.19 ( 94.77)	Acc@5  99.22 ( 99.95)
Epoch: [29][140/391]	Time  0.036 ( 0.039)	Data  0.002 ( 0.003)	Loss 1.9095e-01 (1.4916e-01)	Acc@1  92.19 ( 94.68)	Acc@5 100.00 ( 99.94)
Epoch: [29][150/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.6364e-01 (1.5233e-01)	Acc@1  94.53 ( 94.59)	Acc@5 100.00 ( 99.94)
Epoch: [29][160/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.7291e-01 (1.5388e-01)	Acc@1  92.19 ( 94.54)	Acc@5 100.00 ( 99.94)
Epoch: [29][170/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.3396e-01 (1.5371e-01)	Acc@1  95.31 ( 94.56)	Acc@5 100.00 ( 99.94)
Epoch: [29][180/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.2550e-01 (1.5551e-01)	Acc@1  96.09 ( 94.50)	Acc@5 100.00 ( 99.94)
Epoch: [29][190/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.1170e-01 (1.5648e-01)	Acc@1  94.53 ( 94.48)	Acc@5 100.00 ( 99.94)
Epoch: [29][200/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.7373e-01 (1.5775e-01)	Acc@1  95.31 ( 94.45)	Acc@5  98.44 ( 99.93)
Epoch: [29][210/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.8561e-01 (1.5673e-01)	Acc@1  94.53 ( 94.48)	Acc@5 100.00 ( 99.93)
Epoch: [29][220/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.6235e-01 (1.5729e-01)	Acc@1  93.75 ( 94.47)	Acc@5 100.00 ( 99.93)
Epoch: [29][230/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.0665e-01 (1.5721e-01)	Acc@1  95.31 ( 94.47)	Acc@5 100.00 ( 99.94)
Epoch: [29][240/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.6932e-01 (1.5774e-01)	Acc@1  93.75 ( 94.45)	Acc@5 100.00 ( 99.93)
Epoch: [29][250/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 9.3959e-02 (1.5729e-01)	Acc@1  96.88 ( 94.51)	Acc@5 100.00 ( 99.93)
Epoch: [29][260/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.4907e-01 (1.5839e-01)	Acc@1  93.75 ( 94.48)	Acc@5 100.00 ( 99.93)
Epoch: [29][270/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.9980e-01 (1.5887e-01)	Acc@1  92.97 ( 94.44)	Acc@5 100.00 ( 99.93)
Epoch: [29][280/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.3068e-01 (1.5949e-01)	Acc@1  96.09 ( 94.42)	Acc@5 100.00 ( 99.92)
Epoch: [29][290/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.2370e-01 (1.6041e-01)	Acc@1  96.09 ( 94.42)	Acc@5 100.00 ( 99.92)
Epoch: [29][300/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.7439e-02 (1.6067e-01)	Acc@1  98.44 ( 94.41)	Acc@5 100.00 ( 99.93)
Epoch: [29][310/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.4056e-01 (1.6130e-01)	Acc@1  96.09 ( 94.37)	Acc@5 100.00 ( 99.93)
Epoch: [29][320/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.0156e-01 (1.6237e-01)	Acc@1  98.44 ( 94.35)	Acc@5 100.00 ( 99.93)
Epoch: [29][330/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.7542e-01 (1.6350e-01)	Acc@1  95.31 ( 94.31)	Acc@5 100.00 ( 99.92)
Epoch: [29][340/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.0484e-01 (1.6425e-01)	Acc@1  90.62 ( 94.29)	Acc@5 100.00 ( 99.91)
Epoch: [29][350/391]	Time  0.041 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.2240e-01 (1.6439e-01)	Acc@1  96.88 ( 94.30)	Acc@5 100.00 ( 99.92)
Epoch: [29][360/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.8961e-01 (1.6449e-01)	Acc@1  92.97 ( 94.27)	Acc@5 100.00 ( 99.92)
Epoch: [29][370/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.9598e-01 (1.6507e-01)	Acc@1  94.53 ( 94.24)	Acc@5 100.00 ( 99.91)
Epoch: [29][380/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.7619e-01 (1.6570e-01)	Acc@1  93.75 ( 94.20)	Acc@5 100.00 ( 99.91)
Epoch: [29][390/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.0988e-01 (1.6580e-01)	Acc@1  91.25 ( 94.19)	Acc@5 100.00 ( 99.91)
## e[29] optimizer.zero_grad (sum) time: 0.17590689659118652
## e[29]       loss.backward (sum) time: 2.9905076026916504
## e[29]      optimizer.step (sum) time: 1.0376057624816895
## epoch[29] training(only) time: 15.14661169052124
# Switched to evaluate mode...
Test: [  0/100]	Time  0.180 ( 0.180)	Loss 5.1948e-01 (5.1948e-01)	Acc@1  82.00 ( 82.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.021 ( 0.033)	Loss 4.2264e-01 (4.1940e-01)	Acc@1  88.00 ( 87.45)	Acc@5  98.00 ( 99.55)
Test: [ 20/100]	Time  0.025 ( 0.027)	Loss 3.4595e-01 (4.1852e-01)	Acc@1  88.00 ( 87.57)	Acc@5  99.00 ( 99.38)
Test: [ 30/100]	Time  0.020 ( 0.024)	Loss 3.6113e-01 (4.1428e-01)	Acc@1  87.00 ( 87.77)	Acc@5  98.00 ( 99.35)
Test: [ 40/100]	Time  0.019 ( 0.023)	Loss 5.6419e-01 (4.2381e-01)	Acc@1  85.00 ( 87.71)	Acc@5  99.00 ( 99.39)
Test: [ 50/100]	Time  0.016 ( 0.022)	Loss 2.2111e-01 (4.2512e-01)	Acc@1  93.00 ( 87.71)	Acc@5 100.00 ( 99.47)
Test: [ 60/100]	Time  0.019 ( 0.021)	Loss 3.0485e-01 (4.2025e-01)	Acc@1  90.00 ( 87.77)	Acc@5 100.00 ( 99.56)
Test: [ 70/100]	Time  0.016 ( 0.021)	Loss 7.3272e-01 (4.1396e-01)	Acc@1  79.00 ( 87.82)	Acc@5 100.00 ( 99.61)
Test: [ 80/100]	Time  0.016 ( 0.021)	Loss 3.3251e-01 (4.1531e-01)	Acc@1  88.00 ( 87.90)	Acc@5 100.00 ( 99.63)
Test: [ 90/100]	Time  0.016 ( 0.020)	Loss 3.3271e-01 (4.1455e-01)	Acc@1  87.00 ( 87.75)	Acc@5 100.00 ( 99.66)
 * Acc@1 87.860 Acc@5 99.680
### epoch[29] execution time: 17.31891369819641
EPOCH 30
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [30][  0/391]	Time  0.220 ( 0.220)	Data  0.182 ( 0.182)	Loss 1.5781e-01 (1.5781e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
Epoch: [30][ 10/391]	Time  0.036 ( 0.054)	Data  0.001 ( 0.018)	Loss 2.0749e-01 (1.7637e-01)	Acc@1  95.31 ( 93.96)	Acc@5 100.00 ( 99.93)
Epoch: [30][ 20/391]	Time  0.036 ( 0.046)	Data  0.001 ( 0.011)	Loss 9.1287e-02 (1.5657e-01)	Acc@1  97.66 ( 94.61)	Acc@5 100.00 ( 99.96)
Epoch: [30][ 30/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.008)	Loss 1.3304e-01 (1.4539e-01)	Acc@1  94.53 ( 94.86)	Acc@5 100.00 ( 99.97)
Epoch: [30][ 40/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.007)	Loss 8.2864e-02 (1.3665e-01)	Acc@1  97.66 ( 95.24)	Acc@5 100.00 ( 99.98)
Epoch: [30][ 50/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.006)	Loss 1.1457e-01 (1.3074e-01)	Acc@1  96.88 ( 95.45)	Acc@5 100.00 ( 99.98)
Epoch: [30][ 60/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.005)	Loss 7.4159e-02 (1.3003e-01)	Acc@1  98.44 ( 95.57)	Acc@5 100.00 ( 99.99)
Epoch: [30][ 70/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.005)	Loss 6.3651e-02 (1.2365e-01)	Acc@1  98.44 ( 95.87)	Acc@5 100.00 ( 99.99)
Epoch: [30][ 80/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.3075e-01 (1.2362e-01)	Acc@1  93.75 ( 95.85)	Acc@5 100.00 ( 99.99)
Epoch: [30][ 90/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.0561e-01 (1.2357e-01)	Acc@1  95.31 ( 95.79)	Acc@5 100.00 ( 99.98)
Epoch: [30][100/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.2183e-01 (1.2087e-01)	Acc@1  96.09 ( 95.90)	Acc@5 100.00 ( 99.98)
Epoch: [30][110/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.004)	Loss 8.6462e-02 (1.1845e-01)	Acc@1  96.09 ( 95.97)	Acc@5 100.00 ( 99.99)
Epoch: [30][120/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.4003e-01 (1.1679e-01)	Acc@1  93.75 ( 96.02)	Acc@5 100.00 ( 99.99)
Epoch: [30][130/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 8.8226e-02 (1.1526e-01)	Acc@1  95.31 ( 96.04)	Acc@5 100.00 ( 99.99)
Epoch: [30][140/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.1798e-02 (1.1351e-01)	Acc@1  96.88 ( 96.11)	Acc@5 100.00 ( 99.99)
Epoch: [30][150/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.3256e-01 (1.1201e-01)	Acc@1  96.09 ( 96.16)	Acc@5 100.00 ( 99.98)
Epoch: [30][160/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.3696e-01 (1.1127e-01)	Acc@1  95.31 ( 96.18)	Acc@5 100.00 ( 99.98)
Epoch: [30][170/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.0595e-02 (1.1037e-01)	Acc@1  96.09 ( 96.21)	Acc@5 100.00 ( 99.98)
Epoch: [30][180/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.2744e-02 (1.0845e-01)	Acc@1  97.66 ( 96.31)	Acc@5 100.00 ( 99.98)
Epoch: [30][190/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.3642e-02 (1.0742e-01)	Acc@1  99.22 ( 96.38)	Acc@5 100.00 ( 99.98)
Epoch: [30][200/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.1910e-02 (1.0610e-01)	Acc@1 100.00 ( 96.44)	Acc@5 100.00 ( 99.98)
Epoch: [30][210/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.2765e-01 (1.0506e-01)	Acc@1  95.31 ( 96.48)	Acc@5 100.00 ( 99.98)
Epoch: [30][220/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.4630e-02 (1.0447e-01)	Acc@1  96.09 ( 96.50)	Acc@5 100.00 ( 99.98)
Epoch: [30][230/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 9.0040e-02 (1.0358e-01)	Acc@1  96.88 ( 96.54)	Acc@5  99.22 ( 99.98)
Epoch: [30][240/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.1896e-01 (1.0347e-01)	Acc@1  96.09 ( 96.55)	Acc@5  99.22 ( 99.97)
Epoch: [30][250/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.5867e-02 (1.0232e-01)	Acc@1  96.88 ( 96.58)	Acc@5 100.00 ( 99.98)
Epoch: [30][260/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.2925e-02 (1.0180e-01)	Acc@1  96.09 ( 96.58)	Acc@5 100.00 ( 99.97)
Epoch: [30][270/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.6207e-02 (1.0186e-01)	Acc@1  97.66 ( 96.56)	Acc@5 100.00 ( 99.97)
Epoch: [30][280/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 9.7796e-02 (1.0135e-01)	Acc@1  97.66 ( 96.59)	Acc@5 100.00 ( 99.97)
Epoch: [30][290/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 9.6611e-02 (1.0065e-01)	Acc@1  95.31 ( 96.60)	Acc@5 100.00 ( 99.97)
Epoch: [30][300/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.1075e-02 (1.0011e-01)	Acc@1  99.22 ( 96.63)	Acc@5 100.00 ( 99.97)
Epoch: [30][310/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.8507e-02 (9.9200e-02)	Acc@1  98.44 ( 96.67)	Acc@5 100.00 ( 99.97)
Epoch: [30][320/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.7905e-02 (9.8366e-02)	Acc@1  99.22 ( 96.69)	Acc@5 100.00 ( 99.97)
Epoch: [30][330/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.2777e-02 (9.7527e-02)	Acc@1  98.44 ( 96.73)	Acc@5 100.00 ( 99.97)
Epoch: [30][340/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.3278e-02 (9.6766e-02)	Acc@1  97.66 ( 96.74)	Acc@5 100.00 ( 99.97)
Epoch: [30][350/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.6753e-02 (9.6064e-02)	Acc@1  96.88 ( 96.77)	Acc@5 100.00 ( 99.97)
Epoch: [30][360/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.5180e-01 (9.5579e-02)	Acc@1  96.09 ( 96.79)	Acc@5 100.00 ( 99.97)
Epoch: [30][370/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.0306e-02 (9.4782e-02)	Acc@1  97.66 ( 96.83)	Acc@5 100.00 ( 99.97)
Epoch: [30][380/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.3515e-02 (9.3945e-02)	Acc@1  97.66 ( 96.86)	Acc@5 100.00 ( 99.98)
Epoch: [30][390/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.0438e-01 (9.3282e-02)	Acc@1  97.50 ( 96.89)	Acc@5 100.00 ( 99.98)
## e[30] optimizer.zero_grad (sum) time: 0.17548584938049316
## e[30]       loss.backward (sum) time: 2.985053300857544
## e[30]      optimizer.step (sum) time: 1.0321993827819824
## epoch[30] training(only) time: 15.148300409317017
# Switched to evaluate mode...
Test: [  0/100]	Time  0.172 ( 0.172)	Loss 2.4644e-01 (2.4644e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.019 ( 0.033)	Loss 2.8660e-01 (2.2691e-01)	Acc@1  93.00 ( 92.82)	Acc@5  99.00 ( 99.82)
Test: [ 20/100]	Time  0.022 ( 0.026)	Loss 3.2168e-01 (2.4302e-01)	Acc@1  87.00 ( 92.19)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.024 ( 0.024)	Loss 2.3841e-01 (2.4610e-01)	Acc@1  92.00 ( 92.32)	Acc@5 100.00 ( 99.81)
Test: [ 40/100]	Time  0.018 ( 0.023)	Loss 2.3513e-01 (2.5059e-01)	Acc@1  93.00 ( 92.29)	Acc@5  99.00 ( 99.71)
Test: [ 50/100]	Time  0.018 ( 0.022)	Loss 1.2723e-01 (2.5299e-01)	Acc@1  96.00 ( 92.35)	Acc@5 100.00 ( 99.69)
Test: [ 60/100]	Time  0.022 ( 0.022)	Loss 2.1659e-01 (2.4570e-01)	Acc@1  93.00 ( 92.38)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.020 ( 0.021)	Loss 4.2108e-01 (2.4071e-01)	Acc@1  85.00 ( 92.37)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.020 ( 0.021)	Loss 1.3524e-01 (2.4243e-01)	Acc@1  95.00 ( 92.43)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.019 ( 0.021)	Loss 1.3944e-01 (2.3738e-01)	Acc@1  94.00 ( 92.47)	Acc@5 100.00 ( 99.79)
 * Acc@1 92.490 Acc@5 99.790
### epoch[30] execution time: 17.314905643463135
EPOCH 31
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [31][  0/391]	Time  0.223 ( 0.223)	Data  0.186 ( 0.186)	Loss 5.6605e-02 (5.6605e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [31][ 10/391]	Time  0.037 ( 0.054)	Data  0.001 ( 0.019)	Loss 6.2342e-02 (7.9019e-02)	Acc@1  96.88 ( 97.37)	Acc@5 100.00 ( 99.93)
Epoch: [31][ 20/391]	Time  0.038 ( 0.046)	Data  0.001 ( 0.011)	Loss 4.4683e-02 (6.9084e-02)	Acc@1  99.22 ( 97.77)	Acc@5 100.00 ( 99.96)
Epoch: [31][ 30/391]	Time  0.035 ( 0.044)	Data  0.001 ( 0.008)	Loss 1.2672e-01 (6.7514e-02)	Acc@1  96.09 ( 97.83)	Acc@5 100.00 ( 99.97)
Epoch: [31][ 40/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.007)	Loss 6.4541e-02 (6.9693e-02)	Acc@1  98.44 ( 97.79)	Acc@5 100.00 ( 99.98)
Epoch: [31][ 50/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.006)	Loss 4.1458e-02 (7.3189e-02)	Acc@1  99.22 ( 97.66)	Acc@5 100.00 ( 99.98)
Epoch: [31][ 60/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.005)	Loss 1.1879e-01 (7.2406e-02)	Acc@1  96.88 ( 97.71)	Acc@5 100.00 ( 99.99)
Epoch: [31][ 70/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.005)	Loss 9.2962e-02 (7.2928e-02)	Acc@1  96.09 ( 97.70)	Acc@5 100.00 ( 99.98)
Epoch: [31][ 80/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 5.6736e-02 (7.1937e-02)	Acc@1  98.44 ( 97.78)	Acc@5 100.00 ( 99.98)
Epoch: [31][ 90/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 6.2487e-02 (7.1279e-02)	Acc@1  97.66 ( 97.75)	Acc@5 100.00 ( 99.98)
Epoch: [31][100/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.004)	Loss 6.5021e-02 (7.1362e-02)	Acc@1  98.44 ( 97.76)	Acc@5 100.00 ( 99.98)
Epoch: [31][110/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.004)	Loss 6.4108e-02 (7.1667e-02)	Acc@1  98.44 ( 97.71)	Acc@5 100.00 ( 99.99)
Epoch: [31][120/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.1786e-02 (6.9479e-02)	Acc@1  99.22 ( 97.77)	Acc@5 100.00 ( 99.99)
Epoch: [31][130/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 5.0338e-02 (6.9917e-02)	Acc@1  99.22 ( 97.76)	Acc@5 100.00 ( 99.99)
Epoch: [31][140/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 5.9470e-02 (6.9392e-02)	Acc@1  99.22 ( 97.81)	Acc@5 100.00 ( 99.99)
Epoch: [31][150/391]	Time  0.037 ( 0.039)	Data  0.002 ( 0.003)	Loss 5.2203e-02 (6.8881e-02)	Acc@1  99.22 ( 97.86)	Acc@5 100.00 ( 99.98)
Epoch: [31][160/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.0264e-02 (6.7586e-02)	Acc@1  98.44 ( 97.90)	Acc@5 100.00 ( 99.99)
Epoch: [31][170/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.1118e-02 (6.7735e-02)	Acc@1  97.66 ( 97.90)	Acc@5 100.00 ( 99.99)
Epoch: [31][180/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.3728e-02 (6.7376e-02)	Acc@1  98.44 ( 97.90)	Acc@5 100.00 ( 99.99)
Epoch: [31][190/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.3570e-02 (6.7173e-02)	Acc@1  97.66 ( 97.89)	Acc@5 100.00 ( 99.99)
Epoch: [31][200/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.7392e-02 (6.6938e-02)	Acc@1  96.88 ( 97.89)	Acc@5 100.00 ( 99.99)
Epoch: [31][210/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.9611e-02 (6.6685e-02)	Acc@1  96.88 ( 97.90)	Acc@5 100.00 ( 99.99)
Epoch: [31][220/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.2052e-01 (6.6724e-02)	Acc@1  96.09 ( 97.88)	Acc@5 100.00 ( 99.99)
Epoch: [31][230/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.2076e-02 (6.7205e-02)	Acc@1  99.22 ( 97.88)	Acc@5 100.00 ( 99.99)
Epoch: [31][240/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.2266e-01 (6.7204e-02)	Acc@1  96.88 ( 97.89)	Acc@5 100.00 ( 99.99)
Epoch: [31][250/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.8716e-02 (6.7634e-02)	Acc@1  97.66 ( 97.86)	Acc@5 100.00 ( 99.99)
Epoch: [31][260/391]	Time  0.040 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.3351e-02 (6.7121e-02)	Acc@1  96.88 ( 97.86)	Acc@5 100.00 ( 99.99)
Epoch: [31][270/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.2003e-02 (6.6724e-02)	Acc@1  96.88 ( 97.88)	Acc@5 100.00 ( 99.99)
Epoch: [31][280/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.2855e-02 (6.6420e-02)	Acc@1  97.66 ( 97.89)	Acc@5 100.00 ( 99.99)
Epoch: [31][290/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.0465e-02 (6.6281e-02)	Acc@1  96.88 ( 97.90)	Acc@5 100.00 ( 99.99)
Epoch: [31][300/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.3047e-02 (6.6296e-02)	Acc@1  99.22 ( 97.90)	Acc@5 100.00 ( 99.99)
Epoch: [31][310/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 9.3256e-02 (6.6427e-02)	Acc@1  96.09 ( 97.88)	Acc@5 100.00 ( 99.99)
Epoch: [31][320/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.6702e-02 (6.6260e-02)	Acc@1  98.44 ( 97.88)	Acc@5 100.00 ( 99.99)
Epoch: [31][330/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.2055e-02 (6.6511e-02)	Acc@1  99.22 ( 97.87)	Acc@5 100.00 ( 99.99)
Epoch: [31][340/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.1387e-02 (6.6156e-02)	Acc@1  98.44 ( 97.88)	Acc@5 100.00 ( 99.99)
Epoch: [31][350/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.5091e-02 (6.5813e-02)	Acc@1  99.22 ( 97.89)	Acc@5 100.00 ( 99.99)
Epoch: [31][360/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.6263e-02 (6.5527e-02)	Acc@1  99.22 ( 97.91)	Acc@5 100.00 ( 99.99)
Epoch: [31][370/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.9375e-02 (6.5491e-02)	Acc@1  97.66 ( 97.90)	Acc@5 100.00 ( 99.99)
Epoch: [31][380/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.1820e-02 (6.5444e-02)	Acc@1  96.88 ( 97.90)	Acc@5 100.00 ( 99.99)
Epoch: [31][390/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.4974e-02 (6.5465e-02)	Acc@1  96.25 ( 97.90)	Acc@5 100.00 ( 99.99)
## e[31] optimizer.zero_grad (sum) time: 0.17584562301635742
## e[31]       loss.backward (sum) time: 2.99965763092041
## e[31]      optimizer.step (sum) time: 1.0363636016845703
## epoch[31] training(only) time: 15.177117347717285
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 2.1654e-01 (2.1654e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.017 ( 0.034)	Loss 2.7444e-01 (2.1719e-01)	Acc@1  90.00 ( 93.18)	Acc@5  99.00 ( 99.82)
Test: [ 20/100]	Time  0.016 ( 0.026)	Loss 3.1576e-01 (2.3672e-01)	Acc@1  87.00 ( 92.52)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.016 ( 0.024)	Loss 2.4299e-01 (2.4242e-01)	Acc@1  90.00 ( 92.52)	Acc@5 100.00 ( 99.81)
Test: [ 40/100]	Time  0.017 ( 0.022)	Loss 2.0301e-01 (2.4757e-01)	Acc@1  95.00 ( 92.49)	Acc@5  99.00 ( 99.80)
Test: [ 50/100]	Time  0.018 ( 0.022)	Loss 1.0040e-01 (2.4859e-01)	Acc@1  97.00 ( 92.63)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.020 ( 0.021)	Loss 2.3373e-01 (2.4210e-01)	Acc@1  94.00 ( 92.72)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.016 ( 0.021)	Loss 4.2150e-01 (2.3664e-01)	Acc@1  87.00 ( 92.72)	Acc@5  99.00 ( 99.80)
Test: [ 80/100]	Time  0.024 ( 0.021)	Loss 1.3025e-01 (2.3923e-01)	Acc@1  95.00 ( 92.74)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.016 ( 0.020)	Loss 1.2442e-01 (2.3390e-01)	Acc@1  93.00 ( 92.81)	Acc@5 100.00 ( 99.81)
 * Acc@1 92.900 Acc@5 99.810
### epoch[31] execution time: 17.318933725357056
EPOCH 32
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [32][  0/391]	Time  0.211 ( 0.211)	Data  0.175 ( 0.175)	Loss 7.7992e-02 (7.7992e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [32][ 10/391]	Time  0.038 ( 0.053)	Data  0.001 ( 0.018)	Loss 3.7054e-02 (5.4620e-02)	Acc@1  99.22 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [32][ 20/391]	Time  0.037 ( 0.046)	Data  0.001 ( 0.010)	Loss 2.9876e-02 (4.8562e-02)	Acc@1 100.00 ( 98.47)	Acc@5 100.00 (100.00)
Epoch: [32][ 30/391]	Time  0.036 ( 0.043)	Data  0.001 ( 0.008)	Loss 9.4966e-02 (4.6976e-02)	Acc@1  96.88 ( 98.64)	Acc@5 100.00 (100.00)
Epoch: [32][ 40/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.006)	Loss 4.3439e-02 (5.1897e-02)	Acc@1  98.44 ( 98.42)	Acc@5 100.00 ( 99.98)
Epoch: [32][ 50/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.006)	Loss 4.5009e-02 (4.9944e-02)	Acc@1  99.22 ( 98.51)	Acc@5 100.00 ( 99.98)
Epoch: [32][ 60/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.005)	Loss 2.9265e-02 (5.2189e-02)	Acc@1  99.22 ( 98.42)	Acc@5 100.00 ( 99.99)
Epoch: [32][ 70/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.005)	Loss 6.3493e-02 (5.2247e-02)	Acc@1  96.88 ( 98.42)	Acc@5 100.00 ( 99.99)
Epoch: [32][ 80/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 4.5868e-02 (5.1553e-02)	Acc@1  97.66 ( 98.44)	Acc@5 100.00 ( 99.99)
Epoch: [32][ 90/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.004)	Loss 4.3555e-02 (5.1903e-02)	Acc@1  99.22 ( 98.41)	Acc@5 100.00 ( 99.99)
Epoch: [32][100/391]	Time  0.040 ( 0.040)	Data  0.002 ( 0.004)	Loss 7.5282e-02 (5.2618e-02)	Acc@1  97.66 ( 98.40)	Acc@5 100.00 ( 99.99)
Epoch: [32][110/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.004)	Loss 3.7823e-02 (5.2574e-02)	Acc@1  99.22 ( 98.41)	Acc@5 100.00 ( 99.99)
Epoch: [32][120/391]	Time  0.036 ( 0.039)	Data  0.002 ( 0.004)	Loss 4.5729e-02 (5.2141e-02)	Acc@1  97.66 ( 98.42)	Acc@5 100.00 ( 99.99)
Epoch: [32][130/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 4.7930e-02 (5.2034e-02)	Acc@1  99.22 ( 98.41)	Acc@5 100.00 ( 99.99)
Epoch: [32][140/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.2202e-02 (5.1968e-02)	Acc@1  97.66 ( 98.39)	Acc@5 100.00 ( 99.99)
Epoch: [32][150/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.3495e-02 (5.1919e-02)	Acc@1  97.66 ( 98.39)	Acc@5 100.00 ( 99.99)
Epoch: [32][160/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.0415e-02 (5.1299e-02)	Acc@1  99.22 ( 98.40)	Acc@5 100.00 (100.00)
Epoch: [32][170/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.4779e-02 (5.1074e-02)	Acc@1  97.66 ( 98.41)	Acc@5 100.00 (100.00)
Epoch: [32][180/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.0040e-02 (5.1394e-02)	Acc@1  96.88 ( 98.40)	Acc@5 100.00 ( 99.99)
Epoch: [32][190/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.7176e-02 (5.0995e-02)	Acc@1  99.22 ( 98.40)	Acc@5 100.00 ( 99.99)
Epoch: [32][200/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.3919e-02 (5.1141e-02)	Acc@1  97.66 ( 98.39)	Acc@5 100.00 ( 99.99)
Epoch: [32][210/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.0278e-02 (5.0993e-02)	Acc@1  98.44 ( 98.40)	Acc@5 100.00 ( 99.99)
Epoch: [32][220/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.1937e-02 (5.0941e-02)	Acc@1  98.44 ( 98.39)	Acc@5 100.00 ( 99.99)
Epoch: [32][230/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.6638e-02 (5.1347e-02)	Acc@1  98.44 ( 98.35)	Acc@5 100.00 ( 99.99)
Epoch: [32][240/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.2576e-02 (5.1454e-02)	Acc@1  98.44 ( 98.36)	Acc@5 100.00 ( 99.99)
Epoch: [32][250/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.2601e-02 (5.1680e-02)	Acc@1  97.66 ( 98.35)	Acc@5  99.22 ( 99.99)
Epoch: [32][260/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.1046e-01 (5.2240e-02)	Acc@1  95.31 ( 98.32)	Acc@5 100.00 ( 99.99)
Epoch: [32][270/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.0296e-02 (5.2287e-02)	Acc@1  99.22 ( 98.32)	Acc@5 100.00 ( 99.99)
Epoch: [32][280/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.9439e-02 (5.2696e-02)	Acc@1  97.66 ( 98.30)	Acc@5 100.00 ( 99.99)
Epoch: [32][290/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.7307e-02 (5.2384e-02)	Acc@1  99.22 ( 98.32)	Acc@5 100.00 ( 99.99)
Epoch: [32][300/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.6722e-02 (5.2253e-02)	Acc@1 100.00 ( 98.33)	Acc@5 100.00 ( 99.99)
Epoch: [32][310/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.4528e-02 (5.2053e-02)	Acc@1  98.44 ( 98.34)	Acc@5 100.00 ( 99.99)
Epoch: [32][320/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.6343e-02 (5.2607e-02)	Acc@1  96.09 ( 98.32)	Acc@5 100.00 ( 99.99)
Epoch: [32][330/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.8532e-02 (5.2639e-02)	Acc@1  99.22 ( 98.32)	Acc@5 100.00 ( 99.99)
Epoch: [32][340/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.8855e-02 (5.2854e-02)	Acc@1  99.22 ( 98.31)	Acc@5 100.00 ( 99.99)
Epoch: [32][350/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.4588e-02 (5.3120e-02)	Acc@1  99.22 ( 98.30)	Acc@5 100.00 ( 99.99)
Epoch: [32][360/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.5084e-02 (5.3105e-02)	Acc@1  99.22 ( 98.31)	Acc@5 100.00 ( 99.99)
Epoch: [32][370/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.8321e-02 (5.3070e-02)	Acc@1 100.00 ( 98.31)	Acc@5 100.00 ( 99.99)
Epoch: [32][380/391]	Time  0.041 ( 0.038)	Data  0.002 ( 0.003)	Loss 3.2601e-02 (5.3171e-02)	Acc@1  99.22 ( 98.29)	Acc@5 100.00 ( 99.99)
Epoch: [32][390/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.0138e-02 (5.3023e-02)	Acc@1  98.75 ( 98.30)	Acc@5 100.00 ( 99.99)
## e[32] optimizer.zero_grad (sum) time: 0.1766822338104248
## e[32]       loss.backward (sum) time: 2.999786615371704
## e[32]      optimizer.step (sum) time: 1.0392167568206787
## epoch[32] training(only) time: 15.133763313293457
# Switched to evaluate mode...
Test: [  0/100]	Time  0.178 ( 0.178)	Loss 2.4081e-01 (2.4081e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.016 ( 0.033)	Loss 3.3089e-01 (2.2998e-01)	Acc@1  91.00 ( 93.45)	Acc@5  99.00 ( 99.82)
Test: [ 20/100]	Time  0.016 ( 0.026)	Loss 3.2494e-01 (2.4493e-01)	Acc@1  89.00 ( 92.67)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.019 ( 0.023)	Loss 2.2964e-01 (2.4716e-01)	Acc@1  91.00 ( 92.81)	Acc@5 100.00 ( 99.77)
Test: [ 40/100]	Time  0.016 ( 0.022)	Loss 2.0168e-01 (2.5064e-01)	Acc@1  94.00 ( 92.49)	Acc@5  99.00 ( 99.76)
Test: [ 50/100]	Time  0.023 ( 0.021)	Loss 1.1377e-01 (2.5087e-01)	Acc@1  97.00 ( 92.65)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.020 ( 0.021)	Loss 2.5641e-01 (2.4472e-01)	Acc@1  94.00 ( 92.77)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.017 ( 0.021)	Loss 4.1092e-01 (2.3932e-01)	Acc@1  90.00 ( 92.92)	Acc@5 100.00 ( 99.80)
Test: [ 80/100]	Time  0.016 ( 0.020)	Loss 1.4572e-01 (2.4190e-01)	Acc@1  95.00 ( 92.96)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.017 ( 0.020)	Loss 1.5117e-01 (2.3717e-01)	Acc@1  92.00 ( 92.99)	Acc@5 100.00 ( 99.79)
 * Acc@1 93.010 Acc@5 99.790
### epoch[32] execution time: 17.260505437850952
EPOCH 33
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [33][  0/391]	Time  0.214 ( 0.214)	Data  0.180 ( 0.180)	Loss 2.5302e-02 (2.5302e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [33][ 10/391]	Time  0.037 ( 0.054)	Data  0.001 ( 0.018)	Loss 8.7345e-02 (5.2272e-02)	Acc@1  97.66 ( 98.30)	Acc@5 100.00 ( 99.93)
Epoch: [33][ 20/391]	Time  0.037 ( 0.046)	Data  0.001 ( 0.011)	Loss 5.1343e-02 (5.5659e-02)	Acc@1  99.22 ( 98.40)	Acc@5 100.00 ( 99.96)
Epoch: [33][ 30/391]	Time  0.037 ( 0.043)	Data  0.001 ( 0.008)	Loss 5.3243e-02 (5.4456e-02)	Acc@1  98.44 ( 98.31)	Acc@5 100.00 ( 99.97)
Epoch: [33][ 40/391]	Time  0.037 ( 0.042)	Data  0.002 ( 0.006)	Loss 3.7849e-02 (5.2591e-02)	Acc@1  98.44 ( 98.40)	Acc@5 100.00 ( 99.98)
Epoch: [33][ 50/391]	Time  0.035 ( 0.041)	Data  0.001 ( 0.006)	Loss 3.7489e-02 (5.0604e-02)	Acc@1  98.44 ( 98.45)	Acc@5 100.00 ( 99.98)
Epoch: [33][ 60/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.005)	Loss 4.6757e-02 (5.0363e-02)	Acc@1  96.88 ( 98.39)	Acc@5 100.00 ( 99.99)
Epoch: [33][ 70/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.005)	Loss 5.5303e-02 (4.9541e-02)	Acc@1  97.66 ( 98.45)	Acc@5 100.00 ( 99.99)
Epoch: [33][ 80/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.5839e-02 (4.9878e-02)	Acc@1  99.22 ( 98.43)	Acc@5 100.00 ( 99.99)
Epoch: [33][ 90/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.9179e-02 (4.8828e-02)	Acc@1  99.22 ( 98.47)	Acc@5 100.00 ( 99.99)
Epoch: [33][100/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.6875e-02 (4.9208e-02)	Acc@1  99.22 ( 98.45)	Acc@5 100.00 ( 99.99)
Epoch: [33][110/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 3.8334e-02 (4.9175e-02)	Acc@1  98.44 ( 98.46)	Acc@5 100.00 ( 99.99)
Epoch: [33][120/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.8556e-02 (4.7756e-02)	Acc@1  99.22 ( 98.51)	Acc@5 100.00 ( 99.99)
Epoch: [33][130/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 5.4205e-02 (4.7953e-02)	Acc@1  96.09 ( 98.47)	Acc@5 100.00 ( 99.99)
Epoch: [33][140/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.8236e-02 (4.7768e-02)	Acc@1  99.22 ( 98.49)	Acc@5 100.00 ( 99.99)
Epoch: [33][150/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.1499e-02 (4.8082e-02)	Acc@1  99.22 ( 98.48)	Acc@5 100.00 ( 99.99)
Epoch: [33][160/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.7465e-02 (4.8559e-02)	Acc@1  97.66 ( 98.45)	Acc@5 100.00 (100.00)
Epoch: [33][170/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.6082e-02 (4.7921e-02)	Acc@1 100.00 ( 98.47)	Acc@5 100.00 (100.00)
Epoch: [33][180/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.7471e-02 (4.7826e-02)	Acc@1  99.22 ( 98.49)	Acc@5 100.00 (100.00)
Epoch: [33][190/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.2083e-02 (4.7592e-02)	Acc@1  97.66 ( 98.49)	Acc@5 100.00 (100.00)
Epoch: [33][200/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.6976e-02 (4.7738e-02)	Acc@1  99.22 ( 98.48)	Acc@5 100.00 (100.00)
Epoch: [33][210/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.9526e-02 (4.8048e-02)	Acc@1  98.44 ( 98.47)	Acc@5 100.00 (100.00)
Epoch: [33][220/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 9.9014e-02 (4.8345e-02)	Acc@1  96.88 ( 98.47)	Acc@5 100.00 (100.00)
Epoch: [33][230/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.0488e-02 (4.8393e-02)	Acc@1  97.66 ( 98.46)	Acc@5 100.00 (100.00)
Epoch: [33][240/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.2408e-02 (4.8094e-02)	Acc@1  96.09 ( 98.46)	Acc@5 100.00 (100.00)
Epoch: [33][250/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.7309e-02 (4.8136e-02)	Acc@1  98.44 ( 98.46)	Acc@5 100.00 (100.00)
Epoch: [33][260/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.6300e-02 (4.8275e-02)	Acc@1  98.44 ( 98.45)	Acc@5 100.00 (100.00)
Epoch: [33][270/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.1636e-02 (4.8765e-02)	Acc@1  98.44 ( 98.43)	Acc@5 100.00 (100.00)
Epoch: [33][280/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.2096e-02 (4.8716e-02)	Acc@1  98.44 ( 98.43)	Acc@5 100.00 (100.00)
Epoch: [33][290/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.8229e-02 (4.8501e-02)	Acc@1  99.22 ( 98.44)	Acc@5 100.00 ( 99.99)
Epoch: [33][300/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.7224e-02 (4.8712e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 ( 99.99)
Epoch: [33][310/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.2066e-02 (4.8695e-02)	Acc@1  99.22 ( 98.43)	Acc@5 100.00 ( 99.99)
Epoch: [33][320/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.7316e-02 (4.8661e-02)	Acc@1  97.66 ( 98.43)	Acc@5 100.00 (100.00)
Epoch: [33][330/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.1232e-02 (4.8684e-02)	Acc@1  97.66 ( 98.43)	Acc@5 100.00 (100.00)
Epoch: [33][340/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.0226e-02 (4.8462e-02)	Acc@1  98.44 ( 98.43)	Acc@5 100.00 ( 99.99)
Epoch: [33][350/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.2471e-02 (4.8116e-02)	Acc@1  99.22 ( 98.45)	Acc@5 100.00 ( 99.99)
Epoch: [33][360/391]	Time  0.046 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.8081e-02 (4.7889e-02)	Acc@1  99.22 ( 98.45)	Acc@5 100.00 ( 99.99)
Epoch: [33][370/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.9254e-02 (4.7904e-02)	Acc@1  97.66 ( 98.45)	Acc@5 100.00 ( 99.99)
Epoch: [33][380/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.9736e-02 (4.7966e-02)	Acc@1  96.88 ( 98.43)	Acc@5 100.00 ( 99.99)
Epoch: [33][390/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.0616e-01 (4.7844e-02)	Acc@1  93.75 ( 98.43)	Acc@5 100.00 ( 99.99)
## e[33] optimizer.zero_grad (sum) time: 0.17653203010559082
## e[33]       loss.backward (sum) time: 2.978926181793213
## e[33]      optimizer.step (sum) time: 1.0325672626495361
## epoch[33] training(only) time: 15.136818408966064
# Switched to evaluate mode...
Test: [  0/100]	Time  0.176 ( 0.176)	Loss 2.1359e-01 (2.1359e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.023 ( 0.034)	Loss 3.3376e-01 (2.2353e-01)	Acc@1  89.00 ( 93.00)	Acc@5  99.00 ( 99.82)
Test: [ 20/100]	Time  0.021 ( 0.027)	Loss 3.0793e-01 (2.4092e-01)	Acc@1  89.00 ( 92.71)	Acc@5 100.00 ( 99.90)
Test: [ 30/100]	Time  0.021 ( 0.024)	Loss 2.2080e-01 (2.4410e-01)	Acc@1  91.00 ( 92.68)	Acc@5 100.00 ( 99.84)
Test: [ 40/100]	Time  0.017 ( 0.023)	Loss 2.2654e-01 (2.5061e-01)	Acc@1  93.00 ( 92.63)	Acc@5  99.00 ( 99.80)
Test: [ 50/100]	Time  0.017 ( 0.022)	Loss 1.3225e-01 (2.5427e-01)	Acc@1  94.00 ( 92.59)	Acc@5 100.00 ( 99.80)
Test: [ 60/100]	Time  0.017 ( 0.022)	Loss 2.6470e-01 (2.4923e-01)	Acc@1  94.00 ( 92.67)	Acc@5 100.00 ( 99.84)
Test: [ 70/100]	Time  0.016 ( 0.021)	Loss 4.8909e-01 (2.4487e-01)	Acc@1  88.00 ( 92.76)	Acc@5  99.00 ( 99.83)
Test: [ 80/100]	Time  0.021 ( 0.021)	Loss 1.3140e-01 (2.4632e-01)	Acc@1  95.00 ( 92.81)	Acc@5 100.00 ( 99.81)
Test: [ 90/100]	Time  0.016 ( 0.021)	Loss 1.2098e-01 (2.4158e-01)	Acc@1  92.00 ( 92.87)	Acc@5 100.00 ( 99.82)
 * Acc@1 92.940 Acc@5 99.800
### epoch[33] execution time: 17.332117080688477
EPOCH 34
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [34][  0/391]	Time  0.218 ( 0.218)	Data  0.178 ( 0.178)	Loss 2.3526e-02 (2.3526e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [34][ 10/391]	Time  0.038 ( 0.055)	Data  0.001 ( 0.018)	Loss 3.5418e-02 (4.4491e-02)	Acc@1  99.22 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [34][ 20/391]	Time  0.039 ( 0.047)	Data  0.001 ( 0.011)	Loss 5.2285e-02 (4.6574e-02)	Acc@1  98.44 ( 98.40)	Acc@5 100.00 (100.00)
Epoch: [34][ 30/391]	Time  0.036 ( 0.044)	Data  0.001 ( 0.008)	Loss 5.1833e-02 (4.4904e-02)	Acc@1  98.44 ( 98.59)	Acc@5 100.00 ( 99.97)
Epoch: [34][ 40/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.006)	Loss 2.4251e-02 (4.5583e-02)	Acc@1 100.00 ( 98.55)	Acc@5 100.00 ( 99.96)
Epoch: [34][ 50/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.006)	Loss 5.8612e-02 (4.4471e-02)	Acc@1  98.44 ( 98.64)	Acc@5 100.00 ( 99.97)
Epoch: [34][ 60/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.005)	Loss 8.5104e-02 (4.4858e-02)	Acc@1  96.88 ( 98.62)	Acc@5 100.00 ( 99.97)
Epoch: [34][ 70/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.005)	Loss 3.0376e-02 (4.3839e-02)	Acc@1  99.22 ( 98.68)	Acc@5 100.00 ( 99.98)
Epoch: [34][ 80/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.004)	Loss 3.8418e-02 (4.4903e-02)	Acc@1  97.66 ( 98.66)	Acc@5 100.00 ( 99.98)
Epoch: [34][ 90/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.7035e-02 (4.3590e-02)	Acc@1  99.22 ( 98.69)	Acc@5 100.00 ( 99.98)
Epoch: [34][100/391]	Time  0.038 ( 0.040)	Data  0.002 ( 0.004)	Loss 3.9247e-02 (4.4446e-02)	Acc@1  98.44 ( 98.65)	Acc@5 100.00 ( 99.98)
Epoch: [34][110/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 4.0482e-02 (4.4504e-02)	Acc@1  98.44 ( 98.63)	Acc@5 100.00 ( 99.99)
Epoch: [34][120/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.9318e-02 (4.4235e-02)	Acc@1  98.44 ( 98.65)	Acc@5 100.00 ( 99.99)
Epoch: [34][130/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 6.3107e-02 (4.3663e-02)	Acc@1  97.66 ( 98.67)	Acc@5 100.00 ( 99.99)
Epoch: [34][140/391]	Time  0.039 ( 0.039)	Data  0.002 ( 0.003)	Loss 2.3258e-02 (4.3283e-02)	Acc@1  99.22 ( 98.67)	Acc@5 100.00 ( 99.99)
Epoch: [34][150/391]	Time  0.037 ( 0.039)	Data  0.002 ( 0.003)	Loss 9.8402e-03 (4.3392e-02)	Acc@1 100.00 ( 98.67)	Acc@5 100.00 ( 99.99)
Epoch: [34][160/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.1605e-02 (4.3022e-02)	Acc@1  98.44 ( 98.70)	Acc@5 100.00 ( 99.99)
Epoch: [34][170/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.1561e-02 (4.2802e-02)	Acc@1 100.00 ( 98.71)	Acc@5 100.00 ( 99.99)
Epoch: [34][180/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.4955e-02 (4.2244e-02)	Acc@1 100.00 ( 98.74)	Acc@5 100.00 ( 99.99)
Epoch: [34][190/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.1591e-02 (4.2293e-02)	Acc@1  99.22 ( 98.74)	Acc@5 100.00 ( 99.99)
Epoch: [34][200/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.6889e-02 (4.2367e-02)	Acc@1  99.22 ( 98.74)	Acc@5 100.00 ( 99.99)
Epoch: [34][210/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 9.4555e-03 (4.2043e-02)	Acc@1 100.00 ( 98.74)	Acc@5 100.00 ( 99.99)
Epoch: [34][220/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.6119e-02 (4.1745e-02)	Acc@1 100.00 ( 98.75)	Acc@5 100.00 ( 99.99)
Epoch: [34][230/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.6197e-02 (4.1971e-02)	Acc@1  97.66 ( 98.72)	Acc@5 100.00 ( 99.99)
Epoch: [34][240/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.4015e-02 (4.1928e-02)	Acc@1 100.00 ( 98.73)	Acc@5 100.00 ( 99.99)
Epoch: [34][250/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.9981e-02 (4.1282e-02)	Acc@1  98.44 ( 98.74)	Acc@5 100.00 ( 99.99)
Epoch: [34][260/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.5247e-02 (4.1569e-02)	Acc@1  97.66 ( 98.73)	Acc@5 100.00 ( 99.99)
Epoch: [34][270/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.5217e-02 (4.1579e-02)	Acc@1  98.44 ( 98.73)	Acc@5 100.00 ( 99.99)
Epoch: [34][280/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.4231e-02 (4.1552e-02)	Acc@1 100.00 ( 98.72)	Acc@5 100.00 ( 99.99)
Epoch: [34][290/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.9871e-02 (4.1018e-02)	Acc@1  99.22 ( 98.74)	Acc@5 100.00 ( 99.99)
Epoch: [34][300/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.3646e-02 (4.0987e-02)	Acc@1  99.22 ( 98.74)	Acc@5 100.00 ( 99.99)
Epoch: [34][310/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.0761e-02 (4.0968e-02)	Acc@1  97.66 ( 98.73)	Acc@5 100.00 ( 99.99)
Epoch: [34][320/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.7586e-02 (4.1182e-02)	Acc@1  97.66 ( 98.71)	Acc@5 100.00 ( 99.99)
Epoch: [34][330/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.0909e-02 (4.1065e-02)	Acc@1  99.22 ( 98.70)	Acc@5 100.00 ( 99.99)
Epoch: [34][340/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.9652e-02 (4.1239e-02)	Acc@1 100.00 ( 98.69)	Acc@5 100.00 ( 99.99)
Epoch: [34][350/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.7506e-02 (4.1103e-02)	Acc@1  97.66 ( 98.69)	Acc@5 100.00 ( 99.99)
Epoch: [34][360/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.2214e-02 (4.1293e-02)	Acc@1  98.44 ( 98.69)	Acc@5 100.00 ( 99.99)
Epoch: [34][370/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.4404e-02 (4.1239e-02)	Acc@1  99.22 ( 98.69)	Acc@5 100.00 ( 99.99)
Epoch: [34][380/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.0347e-02 (4.1206e-02)	Acc@1 100.00 ( 98.70)	Acc@5 100.00 ( 99.99)
Epoch: [34][390/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.4251e-02 (4.1406e-02)	Acc@1  97.50 ( 98.69)	Acc@5 100.00 ( 99.99)
## e[34] optimizer.zero_grad (sum) time: 0.1761922836303711
## e[34]       loss.backward (sum) time: 2.95379638671875
## e[34]      optimizer.step (sum) time: 1.0101253986358643
## epoch[34] training(only) time: 15.110836267471313
# Switched to evaluate mode...
Test: [  0/100]	Time  0.181 ( 0.181)	Loss 2.1650e-01 (2.1650e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.020 ( 0.034)	Loss 2.9694e-01 (2.2846e-01)	Acc@1  90.00 ( 93.09)	Acc@5  99.00 ( 99.82)
Test: [ 20/100]	Time  0.023 ( 0.027)	Loss 3.5617e-01 (2.4919e-01)	Acc@1  90.00 ( 92.67)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.016 ( 0.024)	Loss 2.0070e-01 (2.5076e-01)	Acc@1  92.00 ( 92.87)	Acc@5 100.00 ( 99.77)
Test: [ 40/100]	Time  0.023 ( 0.023)	Loss 2.2344e-01 (2.6001e-01)	Acc@1  91.00 ( 92.71)	Acc@5  99.00 ( 99.76)
Test: [ 50/100]	Time  0.020 ( 0.022)	Loss 1.1094e-01 (2.6201e-01)	Acc@1  96.00 ( 92.75)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.019 ( 0.022)	Loss 2.6088e-01 (2.5521e-01)	Acc@1  94.00 ( 92.82)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.017 ( 0.021)	Loss 4.8540e-01 (2.5068e-01)	Acc@1  89.00 ( 92.87)	Acc@5  99.00 ( 99.79)
Test: [ 80/100]	Time  0.022 ( 0.021)	Loss 1.1130e-01 (2.5185e-01)	Acc@1  95.00 ( 92.91)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.016 ( 0.020)	Loss 1.1288e-01 (2.4678e-01)	Acc@1  94.00 ( 92.95)	Acc@5 100.00 ( 99.80)
 * Acc@1 92.970 Acc@5 99.800
### epoch[34] execution time: 17.266594171524048
EPOCH 35
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [35][  0/391]	Time  0.223 ( 0.223)	Data  0.187 ( 0.187)	Loss 2.3904e-02 (2.3904e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [35][ 10/391]	Time  0.037 ( 0.055)	Data  0.001 ( 0.019)	Loss 4.1031e-02 (3.1238e-02)	Acc@1  99.22 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [35][ 20/391]	Time  0.039 ( 0.047)	Data  0.001 ( 0.011)	Loss 4.1937e-02 (3.4544e-02)	Acc@1  98.44 ( 98.85)	Acc@5 100.00 (100.00)
Epoch: [35][ 30/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.008)	Loss 6.8748e-02 (3.4931e-02)	Acc@1  96.88 ( 98.79)	Acc@5 100.00 (100.00)
Epoch: [35][ 40/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.007)	Loss 8.0852e-02 (3.9956e-02)	Acc@1  97.66 ( 98.63)	Acc@5 100.00 (100.00)
Epoch: [35][ 50/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.006)	Loss 4.6345e-02 (3.7576e-02)	Acc@1  99.22 ( 98.76)	Acc@5 100.00 (100.00)
Epoch: [35][ 60/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.005)	Loss 7.3937e-02 (3.7784e-02)	Acc@1  97.66 ( 98.76)	Acc@5 100.00 (100.00)
Epoch: [35][ 70/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.005)	Loss 3.9669e-02 (3.8958e-02)	Acc@1  98.44 ( 98.73)	Acc@5 100.00 (100.00)
Epoch: [35][ 80/391]	Time  0.037 ( 0.040)	Data  0.002 ( 0.005)	Loss 2.9661e-02 (3.8719e-02)	Acc@1  99.22 ( 98.75)	Acc@5 100.00 (100.00)
Epoch: [35][ 90/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.4715e-02 (3.8702e-02)	Acc@1  99.22 ( 98.74)	Acc@5 100.00 (100.00)
Epoch: [35][100/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 4.8240e-02 (3.9226e-02)	Acc@1  98.44 ( 98.73)	Acc@5 100.00 (100.00)
Epoch: [35][110/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.004)	Loss 4.3460e-02 (3.9217e-02)	Acc@1  97.66 ( 98.73)	Acc@5 100.00 (100.00)
Epoch: [35][120/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.7588e-02 (3.8721e-02)	Acc@1  99.22 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [35][130/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 4.3463e-02 (3.8636e-02)	Acc@1  99.22 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [35][140/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.0362e-02 (3.8469e-02)	Acc@1  99.22 ( 98.70)	Acc@5 100.00 (100.00)
Epoch: [35][150/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.1469e-02 (3.8310e-02)	Acc@1  98.44 ( 98.69)	Acc@5 100.00 (100.00)
Epoch: [35][160/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.3458e-02 (3.8196e-02)	Acc@1  99.22 ( 98.71)	Acc@5 100.00 (100.00)
Epoch: [35][170/391]	Time  0.038 ( 0.039)	Data  0.002 ( 0.003)	Loss 6.2220e-02 (3.7422e-02)	Acc@1  98.44 ( 98.74)	Acc@5 100.00 (100.00)
Epoch: [35][180/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.1131e-02 (3.7602e-02)	Acc@1  97.66 ( 98.75)	Acc@5 100.00 (100.00)
Epoch: [35][190/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 9.3595e-03 (3.7697e-02)	Acc@1 100.00 ( 98.75)	Acc@5 100.00 (100.00)
Epoch: [35][200/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.9528e-02 (3.7271e-02)	Acc@1  98.44 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [35][210/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.5491e-02 (3.6908e-02)	Acc@1  98.44 ( 98.79)	Acc@5 100.00 (100.00)
Epoch: [35][220/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.1646e-02 (3.6476e-02)	Acc@1 100.00 ( 98.81)	Acc@5 100.00 (100.00)
Epoch: [35][230/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.5835e-02 (3.6563e-02)	Acc@1  99.22 ( 98.80)	Acc@5 100.00 (100.00)
Epoch: [35][240/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.2659e-02 (3.7011e-02)	Acc@1  97.66 ( 98.79)	Acc@5 100.00 (100.00)
Epoch: [35][250/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.8620e-02 (3.6901e-02)	Acc@1  97.66 ( 98.80)	Acc@5 100.00 (100.00)
Epoch: [35][260/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.0834e-02 (3.6753e-02)	Acc@1  99.22 ( 98.80)	Acc@5 100.00 (100.00)
Epoch: [35][270/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.3730e-02 (3.7373e-02)	Acc@1  99.22 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [35][280/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.8553e-02 (3.7661e-02)	Acc@1  98.44 ( 98.78)	Acc@5 100.00 (100.00)
Epoch: [35][290/391]	Time  0.045 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.4082e-02 (3.7868e-02)	Acc@1  99.22 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [35][300/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.6760e-02 (3.7569e-02)	Acc@1  98.44 ( 98.78)	Acc@5 100.00 (100.00)
Epoch: [35][310/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.6644e-02 (3.7617e-02)	Acc@1  99.22 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [35][320/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.8265e-02 (3.7471e-02)	Acc@1  98.44 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [35][330/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.0952e-02 (3.7656e-02)	Acc@1  99.22 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [35][340/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.2580e-02 (3.7440e-02)	Acc@1 100.00 ( 98.79)	Acc@5 100.00 (100.00)
Epoch: [35][350/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.1778e-02 (3.7535e-02)	Acc@1  97.66 ( 98.78)	Acc@5 100.00 (100.00)
Epoch: [35][360/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.7139e-02 (3.7384e-02)	Acc@1  96.09 ( 98.78)	Acc@5 100.00 (100.00)
Epoch: [35][370/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.1036e-02 (3.7320e-02)	Acc@1 100.00 ( 98.78)	Acc@5 100.00 (100.00)
Epoch: [35][380/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.9473e-02 (3.7436e-02)	Acc@1  97.66 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [35][390/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.0920e-02 (3.7592e-02)	Acc@1  97.50 ( 98.77)	Acc@5 100.00 (100.00)
## e[35] optimizer.zero_grad (sum) time: 0.17619085311889648
## e[35]       loss.backward (sum) time: 2.9813802242279053
## e[35]      optimizer.step (sum) time: 1.018643856048584
## epoch[35] training(only) time: 15.189709424972534
# Switched to evaluate mode...
Test: [  0/100]	Time  0.166 ( 0.166)	Loss 2.4740e-01 (2.4740e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.019 ( 0.031)	Loss 3.5442e-01 (2.3556e-01)	Acc@1  89.00 ( 93.36)	Acc@5  99.00 ( 99.82)
Test: [ 20/100]	Time  0.016 ( 0.025)	Loss 3.1180e-01 (2.5435e-01)	Acc@1  89.00 ( 92.86)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.023 ( 0.023)	Loss 1.8929e-01 (2.5192e-01)	Acc@1  93.00 ( 93.03)	Acc@5 100.00 ( 99.77)
Test: [ 40/100]	Time  0.017 ( 0.022)	Loss 2.1020e-01 (2.5801e-01)	Acc@1  92.00 ( 92.85)	Acc@5  99.00 ( 99.76)
Test: [ 50/100]	Time  0.017 ( 0.021)	Loss 8.3196e-02 (2.6111e-01)	Acc@1  98.00 ( 92.82)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.016 ( 0.021)	Loss 2.3337e-01 (2.5318e-01)	Acc@1  93.00 ( 92.80)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.020 ( 0.021)	Loss 4.6582e-01 (2.4821e-01)	Acc@1  88.00 ( 92.87)	Acc@5  99.00 ( 99.79)
Test: [ 80/100]	Time  0.016 ( 0.020)	Loss 1.0524e-01 (2.4927e-01)	Acc@1  96.00 ( 92.94)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.018 ( 0.020)	Loss 1.2612e-01 (2.4479e-01)	Acc@1  95.00 ( 92.95)	Acc@5 100.00 ( 99.79)
 * Acc@1 93.020 Acc@5 99.800
### epoch[35] execution time: 17.340835332870483
EPOCH 36
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [36][  0/391]	Time  0.228 ( 0.228)	Data  0.191 ( 0.191)	Loss 2.5864e-02 (2.5864e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [36][ 10/391]	Time  0.036 ( 0.055)	Data  0.001 ( 0.019)	Loss 7.2685e-03 (3.5037e-02)	Acc@1 100.00 ( 98.58)	Acc@5 100.00 (100.00)
Epoch: [36][ 20/391]	Time  0.037 ( 0.047)	Data  0.001 ( 0.011)	Loss 5.2500e-02 (3.4108e-02)	Acc@1  97.66 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [36][ 30/391]	Time  0.037 ( 0.044)	Data  0.001 ( 0.008)	Loss 2.8162e-02 (3.3337e-02)	Acc@1  99.22 ( 98.94)	Acc@5 100.00 (100.00)
Epoch: [36][ 40/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.007)	Loss 5.7239e-02 (3.5775e-02)	Acc@1  97.66 ( 98.80)	Acc@5 100.00 (100.00)
Epoch: [36][ 50/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.006)	Loss 4.6428e-02 (3.5928e-02)	Acc@1  98.44 ( 98.85)	Acc@5 100.00 (100.00)
Epoch: [36][ 60/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.005)	Loss 1.9266e-02 (3.5675e-02)	Acc@1 100.00 ( 98.86)	Acc@5 100.00 (100.00)
Epoch: [36][ 70/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.005)	Loss 2.4828e-02 (3.5434e-02)	Acc@1  98.44 ( 98.89)	Acc@5 100.00 (100.00)
Epoch: [36][ 80/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.005)	Loss 3.5085e-02 (3.5161e-02)	Acc@1  98.44 ( 98.91)	Acc@5 100.00 (100.00)
Epoch: [36][ 90/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.7595e-02 (3.5648e-02)	Acc@1  98.44 ( 98.87)	Acc@5 100.00 (100.00)
Epoch: [36][100/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.4889e-02 (3.6451e-02)	Acc@1  99.22 ( 98.83)	Acc@5 100.00 (100.00)
Epoch: [36][110/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 4.4751e-02 (3.7206e-02)	Acc@1  98.44 ( 98.83)	Acc@5 100.00 (100.00)
Epoch: [36][120/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.3197e-02 (3.6457e-02)	Acc@1  99.22 ( 98.88)	Acc@5 100.00 (100.00)
Epoch: [36][130/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 3.8178e-02 (3.5973e-02)	Acc@1  98.44 ( 98.88)	Acc@5 100.00 (100.00)
Epoch: [36][140/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.004)	Loss 4.6511e-02 (3.6037e-02)	Acc@1  98.44 ( 98.87)	Acc@5 100.00 (100.00)
Epoch: [36][150/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.2897e-02 (3.6253e-02)	Acc@1 100.00 ( 98.87)	Acc@5 100.00 (100.00)
Epoch: [36][160/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.3032e-02 (3.6187e-02)	Acc@1 100.00 ( 98.87)	Acc@5 100.00 (100.00)
Epoch: [36][170/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.0002e-02 (3.5693e-02)	Acc@1 100.00 ( 98.89)	Acc@5 100.00 (100.00)
Epoch: [36][180/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.8546e-02 (3.4946e-02)	Acc@1  99.22 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [36][190/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.6954e-03 (3.4581e-02)	Acc@1 100.00 ( 98.92)	Acc@5 100.00 (100.00)
Epoch: [36][200/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.2275e-02 (3.4228e-02)	Acc@1 100.00 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [36][210/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.5897e-02 (3.4031e-02)	Acc@1 100.00 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [36][220/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.2642e-02 (3.4047e-02)	Acc@1 100.00 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [36][230/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.0275e-02 (3.3857e-02)	Acc@1  99.22 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [36][240/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.6553e-02 (3.3753e-02)	Acc@1 100.00 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [36][250/391]	Time  0.040 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.7449e-02 (3.3614e-02)	Acc@1  97.66 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [36][260/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.1814e-02 (3.3817e-02)	Acc@1  97.66 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [36][270/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.6500e-02 (3.3739e-02)	Acc@1  99.22 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [36][280/391]	Time  0.041 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.8457e-02 (3.3722e-02)	Acc@1  99.22 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [36][290/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.4962e-02 (3.3367e-02)	Acc@1  97.66 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [36][300/391]	Time  0.038 ( 0.039)	Data  0.002 ( 0.003)	Loss 2.9526e-02 (3.3484e-02)	Acc@1  99.22 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [36][310/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.5576e-02 (3.3247e-02)	Acc@1  98.44 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [36][320/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.6462e-02 (3.3434e-02)	Acc@1  99.22 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [36][330/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.7564e-02 (3.3712e-02)	Acc@1  97.66 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [36][340/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.8635e-02 (3.3834e-02)	Acc@1  96.88 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [36][350/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.6149e-02 (3.3865e-02)	Acc@1  96.09 ( 98.94)	Acc@5 100.00 (100.00)
Epoch: [36][360/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.2505e-02 (3.3954e-02)	Acc@1 100.00 ( 98.94)	Acc@5 100.00 (100.00)
Epoch: [36][370/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.5103e-02 (3.3847e-02)	Acc@1  97.66 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [36][380/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.4934e-03 (3.3763e-02)	Acc@1 100.00 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [36][390/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.2555e-01 (3.3682e-02)	Acc@1  96.25 ( 98.94)	Acc@5 100.00 (100.00)
## e[36] optimizer.zero_grad (sum) time: 0.17665553092956543
## e[36]       loss.backward (sum) time: 3.0204033851623535
## e[36]      optimizer.step (sum) time: 1.0248231887817383
## epoch[36] training(only) time: 15.16038703918457
# Switched to evaluate mode...
Test: [  0/100]	Time  0.165 ( 0.165)	Loss 3.1800e-01 (3.1800e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.019 ( 0.031)	Loss 2.9192e-01 (2.4004e-01)	Acc@1  90.00 ( 93.36)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.019 ( 0.025)	Loss 3.1998e-01 (2.6066e-01)	Acc@1  90.00 ( 92.76)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.016 ( 0.023)	Loss 1.9110e-01 (2.6257e-01)	Acc@1  93.00 ( 92.90)	Acc@5 100.00 ( 99.81)
Test: [ 40/100]	Time  0.016 ( 0.022)	Loss 2.0325e-01 (2.6638e-01)	Acc@1  94.00 ( 92.71)	Acc@5  99.00 ( 99.76)
Test: [ 50/100]	Time  0.022 ( 0.021)	Loss 1.1863e-01 (2.6953e-01)	Acc@1  96.00 ( 92.65)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.017 ( 0.021)	Loss 2.7328e-01 (2.6193e-01)	Acc@1  92.00 ( 92.69)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.028 ( 0.021)	Loss 4.7527e-01 (2.5519e-01)	Acc@1  91.00 ( 92.86)	Acc@5  99.00 ( 99.79)
Test: [ 80/100]	Time  0.018 ( 0.020)	Loss 1.6512e-01 (2.5864e-01)	Acc@1  95.00 ( 92.89)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.016 ( 0.020)	Loss 1.5228e-01 (2.5348e-01)	Acc@1  94.00 ( 92.92)	Acc@5 100.00 ( 99.78)
 * Acc@1 92.980 Acc@5 99.770
### epoch[36] execution time: 17.28199553489685
EPOCH 37
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [37][  0/391]	Time  0.220 ( 0.220)	Data  0.181 ( 0.181)	Loss 2.9140e-02 (2.9140e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [37][ 10/391]	Time  0.036 ( 0.054)	Data  0.001 ( 0.018)	Loss 4.7950e-02 (3.4698e-02)	Acc@1  99.22 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [37][ 20/391]	Time  0.037 ( 0.046)	Data  0.001 ( 0.011)	Loss 4.6858e-02 (3.2255e-02)	Acc@1  98.44 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [37][ 30/391]	Time  0.036 ( 0.043)	Data  0.001 ( 0.008)	Loss 5.1844e-02 (3.1154e-02)	Acc@1  97.66 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [37][ 40/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.007)	Loss 4.9999e-02 (3.1079e-02)	Acc@1  98.44 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [37][ 50/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.006)	Loss 4.5529e-02 (3.1709e-02)	Acc@1  98.44 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [37][ 60/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.005)	Loss 1.6083e-02 (3.0838e-02)	Acc@1 100.00 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [37][ 70/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.005)	Loss 1.4981e-02 (3.0989e-02)	Acc@1  99.22 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [37][ 80/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.4400e-02 (3.1001e-02)	Acc@1  99.22 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [37][ 90/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.0101e-02 (3.0961e-02)	Acc@1 100.00 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [37][100/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.5433e-02 (2.9955e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [37][110/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.6379e-02 (3.0488e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [37][120/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 6.3316e-02 (3.0842e-02)	Acc@1  96.88 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [37][130/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 4.3245e-02 (3.0378e-02)	Acc@1  98.44 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [37][140/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.1363e-02 (3.0090e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [37][150/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.4473e-02 (2.9921e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [37][160/391]	Time  0.040 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.9751e-02 (2.9927e-02)	Acc@1 100.00 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [37][170/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.1691e-02 (3.0054e-02)	Acc@1  98.44 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [37][180/391]	Time  0.033 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.1525e-02 (3.0411e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [37][190/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.8770e-02 (3.0559e-02)	Acc@1  98.44 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [37][200/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.1042e-02 (3.0510e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [37][210/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.8345e-02 (3.0341e-02)	Acc@1 100.00 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [37][220/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.5113e-02 (3.0244e-02)	Acc@1  98.44 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [37][230/391]	Time  0.041 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.3965e-02 (3.0300e-02)	Acc@1 100.00 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [37][240/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.4019e-02 (3.0425e-02)	Acc@1  98.44 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [37][250/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.4254e-02 (3.0107e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [37][260/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.6548e-02 (2.9988e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [37][270/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.9427e-02 (3.0102e-02)	Acc@1 100.00 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [37][280/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.0191e-02 (3.0191e-02)	Acc@1  98.44 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [37][290/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.5755e-02 (3.0425e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [37][300/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.6924e-02 (3.0406e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [37][310/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.9958e-02 (3.0369e-02)	Acc@1  97.66 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [37][320/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.5448e-02 (3.0651e-02)	Acc@1  96.88 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [37][330/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.1603e-02 (3.0631e-02)	Acc@1 100.00 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [37][340/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.9037e-02 (3.0471e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [37][350/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.9338e-02 (3.0843e-02)	Acc@1  98.44 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [37][360/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.0796e-02 (3.1080e-02)	Acc@1  99.22 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [37][370/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.7172e-02 (3.1073e-02)	Acc@1  98.44 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [37][380/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.3993e-02 (3.1013e-02)	Acc@1  98.44 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [37][390/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.0378e-02 (3.0949e-02)	Acc@1  97.50 ( 98.99)	Acc@5 100.00 (100.00)
## e[37] optimizer.zero_grad (sum) time: 0.17751002311706543
## e[37]       loss.backward (sum) time: 3.008807420730591
## e[37]      optimizer.step (sum) time: 1.0287139415740967
## epoch[37] training(only) time: 15.14221978187561
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 2.7933e-01 (2.7933e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.020 ( 0.033)	Loss 3.0990e-01 (2.3669e-01)	Acc@1  92.00 ( 93.36)	Acc@5  99.00 ( 99.82)
Test: [ 20/100]	Time  0.019 ( 0.026)	Loss 3.2467e-01 (2.5664e-01)	Acc@1  90.00 ( 92.76)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.017 ( 0.023)	Loss 1.9853e-01 (2.6070e-01)	Acc@1  93.00 ( 92.87)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.017 ( 0.022)	Loss 1.8508e-01 (2.6385e-01)	Acc@1  94.00 ( 92.80)	Acc@5  99.00 ( 99.73)
Test: [ 50/100]	Time  0.016 ( 0.021)	Loss 8.5605e-02 (2.6995e-01)	Acc@1  98.00 ( 92.76)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.017 ( 0.021)	Loss 2.3651e-01 (2.6346e-01)	Acc@1  95.00 ( 92.82)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.023 ( 0.021)	Loss 4.5806e-01 (2.5651e-01)	Acc@1  89.00 ( 92.92)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.019 ( 0.020)	Loss 1.3405e-01 (2.5927e-01)	Acc@1  96.00 ( 92.99)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.020 ( 0.020)	Loss 1.2149e-01 (2.5433e-01)	Acc@1  94.00 ( 93.04)	Acc@5 100.00 ( 99.79)
 * Acc@1 93.040 Acc@5 99.790
### epoch[37] execution time: 17.27684998512268
EPOCH 38
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [38][  0/391]	Time  0.212 ( 0.212)	Data  0.176 ( 0.176)	Loss 1.4266e-02 (1.4266e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [38][ 10/391]	Time  0.046 ( 0.054)	Data  0.001 ( 0.018)	Loss 2.5114e-02 (3.1724e-02)	Acc@1  99.22 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [38][ 20/391]	Time  0.036 ( 0.046)	Data  0.001 ( 0.010)	Loss 6.3792e-02 (2.7450e-02)	Acc@1  96.88 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [38][ 30/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.008)	Loss 1.6960e-02 (2.9466e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [38][ 40/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.006)	Loss 5.4683e-02 (3.1352e-02)	Acc@1  99.22 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [38][ 50/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.006)	Loss 1.1062e-02 (3.1602e-02)	Acc@1 100.00 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [38][ 60/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.005)	Loss 2.0128e-02 (3.1006e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [38][ 70/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.005)	Loss 3.5706e-02 (3.0653e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [38][ 80/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.004)	Loss 3.2878e-02 (3.1138e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [38][ 90/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.4196e-02 (3.1101e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [38][100/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 3.1613e-02 (2.9941e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [38][110/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 3.1611e-02 (2.9715e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [38][120/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 4.6996e-02 (2.9830e-02)	Acc@1  97.66 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [38][130/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.8343e-02 (3.0185e-02)	Acc@1  98.44 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [38][140/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.5022e-02 (3.0363e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [38][150/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.2291e-02 (3.0498e-02)	Acc@1 100.00 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [38][160/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.2110e-02 (3.1093e-02)	Acc@1 100.00 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [38][170/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.8213e-02 (3.1040e-02)	Acc@1  97.66 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [38][180/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.7073e-02 (3.1107e-02)	Acc@1  98.44 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [38][190/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.7635e-02 (3.1082e-02)	Acc@1  98.44 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [38][200/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.9188e-02 (3.1183e-02)	Acc@1 100.00 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [38][210/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.0470e-01 (3.0959e-02)	Acc@1  96.09 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [38][220/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.8059e-02 (3.0902e-02)	Acc@1  99.22 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [38][230/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.8454e-02 (3.1176e-02)	Acc@1  98.44 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [38][240/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.3695e-02 (3.1091e-02)	Acc@1 100.00 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [38][250/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.5794e-02 (3.1181e-02)	Acc@1 100.00 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [38][260/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.3522e-03 (3.0984e-02)	Acc@1 100.00 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [38][270/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.3516e-02 (3.0787e-02)	Acc@1  99.22 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [38][280/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.5365e-03 (3.0323e-02)	Acc@1 100.00 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [38][290/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.8640e-02 (3.0027e-02)	Acc@1  98.44 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [38][300/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.5210e-02 (2.9796e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [38][310/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.0908e-02 (2.9912e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [38][320/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.3406e-02 (3.0089e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [38][330/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.3631e-02 (2.9848e-02)	Acc@1 100.00 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [38][340/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.9283e-02 (2.9834e-02)	Acc@1  98.44 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [38][350/391]	Time  0.038 ( 0.038)	Data  0.002 ( 0.003)	Loss 6.9748e-02 (2.9753e-02)	Acc@1  97.66 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [38][360/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.0941e-02 (2.9745e-02)	Acc@1  98.44 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [38][370/391]	Time  0.040 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.3215e-02 (2.9637e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [38][380/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.8105e-02 (2.9688e-02)	Acc@1  98.44 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [38][390/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.3666e-02 (3.0021e-02)	Acc@1  97.50 ( 99.03)	Acc@5 100.00 (100.00)
## e[38] optimizer.zero_grad (sum) time: 0.177138090133667
## e[38]       loss.backward (sum) time: 3.008821725845337
## e[38]      optimizer.step (sum) time: 1.0351712703704834
## epoch[38] training(only) time: 15.174598217010498
# Switched to evaluate mode...
Test: [  0/100]	Time  0.178 ( 0.178)	Loss 2.7572e-01 (2.7572e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.033)	Loss 3.1689e-01 (2.4501e-01)	Acc@1  90.00 ( 93.09)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.020 ( 0.026)	Loss 3.4230e-01 (2.6637e-01)	Acc@1  90.00 ( 92.90)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.016 ( 0.024)	Loss 2.2222e-01 (2.6847e-01)	Acc@1  91.00 ( 92.97)	Acc@5 100.00 ( 99.77)
Test: [ 40/100]	Time  0.017 ( 0.023)	Loss 2.2034e-01 (2.7203e-01)	Acc@1  91.00 ( 92.78)	Acc@5  99.00 ( 99.73)
Test: [ 50/100]	Time  0.017 ( 0.022)	Loss 1.0090e-01 (2.7213e-01)	Acc@1  98.00 ( 92.84)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.018 ( 0.021)	Loss 3.1895e-01 (2.6754e-01)	Acc@1  93.00 ( 92.92)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.019 ( 0.021)	Loss 5.1298e-01 (2.5946e-01)	Acc@1  89.00 ( 93.03)	Acc@5  99.00 ( 99.79)
Test: [ 80/100]	Time  0.017 ( 0.020)	Loss 1.4084e-01 (2.6168e-01)	Acc@1  95.00 ( 93.06)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.018 ( 0.020)	Loss 1.1219e-01 (2.5652e-01)	Acc@1  94.00 ( 93.07)	Acc@5 100.00 ( 99.79)
 * Acc@1 93.070 Acc@5 99.790
### epoch[38] execution time: 17.35289168357849
EPOCH 39
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [39][  0/391]	Time  0.219 ( 0.219)	Data  0.186 ( 0.186)	Loss 2.9854e-02 (2.9854e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [39][ 10/391]	Time  0.039 ( 0.054)	Data  0.001 ( 0.019)	Loss 1.8213e-02 (3.3055e-02)	Acc@1  99.22 ( 98.79)	Acc@5 100.00 (100.00)
Epoch: [39][ 20/391]	Time  0.036 ( 0.046)	Data  0.001 ( 0.011)	Loss 7.4069e-02 (3.4550e-02)	Acc@1  97.66 ( 98.81)	Acc@5 100.00 (100.00)
Epoch: [39][ 30/391]	Time  0.038 ( 0.044)	Data  0.002 ( 0.008)	Loss 5.1115e-02 (3.1640e-02)	Acc@1  98.44 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [39][ 40/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.007)	Loss 8.7168e-03 (2.9716e-02)	Acc@1 100.00 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [39][ 50/391]	Time  0.046 ( 0.041)	Data  0.001 ( 0.006)	Loss 2.6770e-02 (2.9246e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [39][ 60/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.005)	Loss 4.9541e-02 (2.8763e-02)	Acc@1  98.44 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [39][ 70/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.005)	Loss 5.2886e-02 (2.9772e-02)	Acc@1  99.22 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [39][ 80/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 3.4529e-02 (2.8961e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [39][ 90/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.0732e-02 (2.7670e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [39][100/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 3.5657e-02 (2.7302e-02)	Acc@1  99.22 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [39][110/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.0512e-02 (2.7255e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [39][120/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.5140e-02 (2.6843e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [39][130/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 5.3800e-02 (2.7008e-02)	Acc@1  98.44 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [39][140/391]	Time  0.045 ( 0.039)	Data  0.002 ( 0.004)	Loss 4.1284e-02 (2.6601e-02)	Acc@1  98.44 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [39][150/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.5947e-02 (2.6453e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [39][160/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.8042e-02 (2.6245e-02)	Acc@1  99.22 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [39][170/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.6997e-02 (2.5938e-02)	Acc@1  98.44 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [39][180/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.3534e-02 (2.5515e-02)	Acc@1  98.44 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [39][190/391]	Time  0.036 ( 0.039)	Data  0.002 ( 0.003)	Loss 1.5177e-02 (2.6078e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [39][200/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.6230e-02 (2.5804e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [39][210/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.5878e-02 (2.5587e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [39][220/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.8884e-02 (2.5556e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [39][230/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.7396e-02 (2.5340e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [39][240/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.8804e-02 (2.5108e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [39][250/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.7554e-02 (2.4867e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [39][260/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.0194e-02 (2.4881e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [39][270/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.1702e-02 (2.5038e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [39][280/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.3418e-02 (2.5218e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [39][290/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.1876e-02 (2.5276e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [39][300/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.5683e-02 (2.5084e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [39][310/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.1262e-02 (2.5021e-02)	Acc@1  97.66 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [39][320/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.9429e-03 (2.4957e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [39][330/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.1481e-02 (2.4860e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [39][340/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 9.0153e-03 (2.4870e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [39][350/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.1371e-02 (2.5004e-02)	Acc@1  98.44 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [39][360/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.8342e-02 (2.4826e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [39][370/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 9.6813e-03 (2.4814e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [39][380/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.9746e-02 (2.4601e-02)	Acc@1  97.66 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [39][390/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.6413e-02 (2.4695e-02)	Acc@1  98.75 ( 99.28)	Acc@5 100.00 (100.00)
## e[39] optimizer.zero_grad (sum) time: 0.17453956604003906
## e[39]       loss.backward (sum) time: 2.9813199043273926
## e[39]      optimizer.step (sum) time: 1.0234050750732422
## epoch[39] training(only) time: 15.194030284881592
# Switched to evaluate mode...
Test: [  0/100]	Time  0.177 ( 0.177)	Loss 2.3912e-01 (2.3912e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.016 ( 0.034)	Loss 3.2397e-01 (2.4132e-01)	Acc@1  92.00 ( 93.45)	Acc@5  99.00 ( 99.82)
Test: [ 20/100]	Time  0.018 ( 0.027)	Loss 3.3104e-01 (2.5920e-01)	Acc@1  90.00 ( 93.05)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.017 ( 0.024)	Loss 1.7550e-01 (2.6084e-01)	Acc@1  94.00 ( 93.16)	Acc@5  99.00 ( 99.71)
Test: [ 40/100]	Time  0.023 ( 0.023)	Loss 1.9603e-01 (2.6563e-01)	Acc@1  95.00 ( 93.07)	Acc@5 100.00 ( 99.76)
Test: [ 50/100]	Time  0.015 ( 0.022)	Loss 6.1879e-02 (2.6856e-01)	Acc@1  98.00 ( 93.12)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.016 ( 0.022)	Loss 2.9592e-01 (2.6427e-01)	Acc@1  93.00 ( 93.07)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.017 ( 0.021)	Loss 4.3904e-01 (2.5781e-01)	Acc@1  90.00 ( 93.17)	Acc@5  99.00 ( 99.77)
Test: [ 80/100]	Time  0.017 ( 0.021)	Loss 1.5826e-01 (2.6053e-01)	Acc@1  95.00 ( 93.23)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.017 ( 0.021)	Loss 9.7579e-02 (2.5603e-01)	Acc@1  93.00 ( 93.22)	Acc@5 100.00 ( 99.79)
 * Acc@1 93.330 Acc@5 99.790
### epoch[39] execution time: 17.36892557144165
EPOCH 40
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [40][  0/391]	Time  0.233 ( 0.233)	Data  0.197 ( 0.197)	Loss 1.2465e-02 (1.2465e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [40][ 10/391]	Time  0.037 ( 0.055)	Data  0.001 ( 0.020)	Loss 2.5223e-02 (2.1101e-02)	Acc@1  98.44 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [40][ 20/391]	Time  0.037 ( 0.047)	Data  0.001 ( 0.011)	Loss 2.1103e-02 (2.0904e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [40][ 30/391]	Time  0.035 ( 0.044)	Data  0.001 ( 0.008)	Loss 1.2490e-02 (2.1428e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [40][ 40/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.007)	Loss 1.7613e-02 (2.3068e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [40][ 50/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.006)	Loss 1.7352e-02 (2.3954e-02)	Acc@1  99.22 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [40][ 60/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.005)	Loss 1.4192e-02 (2.4042e-02)	Acc@1  99.22 ( 99.23)	Acc@5 100.00 ( 99.99)
Epoch: [40][ 70/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.005)	Loss 9.0134e-03 (2.3523e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 ( 99.99)
Epoch: [40][ 80/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.005)	Loss 8.3600e-03 (2.3102e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 ( 99.99)
Epoch: [40][ 90/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.5912e-02 (2.2365e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 ( 99.99)
Epoch: [40][100/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.004)	Loss 3.8450e-02 (2.2504e-02)	Acc@1  98.44 ( 99.30)	Acc@5 100.00 ( 99.99)
Epoch: [40][110/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 5.8753e-02 (2.2369e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 ( 99.99)
Epoch: [40][120/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.7068e-02 (2.2753e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 ( 99.99)
Epoch: [40][130/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.8401e-02 (2.2588e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 ( 99.99)
Epoch: [40][140/391]	Time  0.035 ( 0.039)	Data  0.002 ( 0.004)	Loss 1.5857e-02 (2.2697e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 ( 99.99)
Epoch: [40][150/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.6454e-02 (2.2376e-02)	Acc@1  98.44 ( 99.29)	Acc@5 100.00 ( 99.99)
Epoch: [40][160/391]	Time  0.039 ( 0.039)	Data  0.002 ( 0.003)	Loss 1.1795e-02 (2.2093e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [40][170/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.7131e-02 (2.1764e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [40][180/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.1105e-02 (2.1896e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [40][190/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.2065e-02 (2.1643e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [40][200/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.6117e-02 (2.2162e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [40][210/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 9.0840e-03 (2.2761e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [40][220/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 9.8577e-03 (2.2727e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [40][230/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 9.0815e-03 (2.2621e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [40][240/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.3331e-02 (2.3026e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [40][250/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.9420e-02 (2.2809e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [40][260/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.3545e-02 (2.2799e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [40][270/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.8859e-02 (2.2759e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [40][280/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.9831e-02 (2.2565e-02)	Acc@1  97.66 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [40][290/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 9.2068e-02 (2.2769e-02)	Acc@1  96.88 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [40][300/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.8837e-03 (2.2623e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [40][310/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.8443e-02 (2.2891e-02)	Acc@1  98.44 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [40][320/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.9136e-02 (2.3016e-02)	Acc@1  96.88 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [40][330/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.3731e-02 (2.3086e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [40][340/391]	Time  0.041 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.2451e-02 (2.3317e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [40][350/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.1182e-02 (2.3351e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [40][360/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.2304e-02 (2.3525e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [40][370/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.5415e-02 (2.3845e-02)	Acc@1  97.66 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [40][380/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.8700e-02 (2.3958e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [40][390/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.1913e-02 (2.3969e-02)	Acc@1  98.75 ( 99.24)	Acc@5 100.00 (100.00)
## e[40] optimizer.zero_grad (sum) time: 0.17661213874816895
## e[40]       loss.backward (sum) time: 3.0020804405212402
## e[40]      optimizer.step (sum) time: 1.0195503234863281
## epoch[40] training(only) time: 15.11434292793274
# Switched to evaluate mode...
Test: [  0/100]	Time  0.171 ( 0.171)	Loss 2.4296e-01 (2.4296e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.016 ( 0.033)	Loss 3.7948e-01 (2.5128e-01)	Acc@1  90.00 ( 93.55)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.020 ( 0.026)	Loss 3.5012e-01 (2.7137e-01)	Acc@1  90.00 ( 93.10)	Acc@5 100.00 ( 99.90)
Test: [ 30/100]	Time  0.020 ( 0.023)	Loss 2.2923e-01 (2.7362e-01)	Acc@1  91.00 ( 93.06)	Acc@5 100.00 ( 99.84)
Test: [ 40/100]	Time  0.016 ( 0.022)	Loss 2.3928e-01 (2.7988e-01)	Acc@1  92.00 ( 92.78)	Acc@5 100.00 ( 99.80)
Test: [ 50/100]	Time  0.017 ( 0.021)	Loss 1.0389e-01 (2.7943e-01)	Acc@1  97.00 ( 92.86)	Acc@5 100.00 ( 99.82)
Test: [ 60/100]	Time  0.020 ( 0.021)	Loss 3.0733e-01 (2.7333e-01)	Acc@1  93.00 ( 92.95)	Acc@5 100.00 ( 99.85)
Test: [ 70/100]	Time  0.018 ( 0.021)	Loss 5.2156e-01 (2.6701e-01)	Acc@1  89.00 ( 92.99)	Acc@5 100.00 ( 99.86)
Test: [ 80/100]	Time  0.020 ( 0.021)	Loss 1.4923e-01 (2.6857e-01)	Acc@1  95.00 ( 93.05)	Acc@5 100.00 ( 99.84)
Test: [ 90/100]	Time  0.016 ( 0.021)	Loss 1.1001e-01 (2.6366e-01)	Acc@1  93.00 ( 93.09)	Acc@5 100.00 ( 99.84)
 * Acc@1 93.140 Acc@5 99.840
### epoch[40] execution time: 17.296524047851562
EPOCH 41
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [41][  0/391]	Time  0.213 ( 0.213)	Data  0.178 ( 0.178)	Loss 3.0686e-02 (3.0686e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [41][ 10/391]	Time  0.035 ( 0.053)	Data  0.001 ( 0.018)	Loss 2.8082e-02 (1.6530e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [41][ 20/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.011)	Loss 1.8356e-02 (1.8325e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [41][ 30/391]	Time  0.036 ( 0.044)	Data  0.001 ( 0.008)	Loss 2.1263e-02 (1.8822e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [41][ 40/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.007)	Loss 3.4011e-02 (1.9817e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [41][ 50/391]	Time  0.035 ( 0.041)	Data  0.001 ( 0.006)	Loss 1.0124e-02 (1.8623e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [41][ 60/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.005)	Loss 5.2460e-03 (1.8708e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [41][ 70/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.005)	Loss 1.5168e-02 (1.8319e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [41][ 80/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 5.7487e-02 (1.9242e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [41][ 90/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.5679e-02 (1.9641e-02)	Acc@1  98.44 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [41][100/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 6.7427e-03 (1.9540e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [41][110/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.0733e-02 (1.9618e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [41][120/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 9.2234e-03 (1.9698e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [41][130/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 4.2755e-02 (1.9723e-02)	Acc@1  98.44 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [41][140/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 4.1126e-02 (1.9698e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [41][150/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.7544e-03 (2.0207e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [41][160/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.6513e-03 (1.9872e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [41][170/391]	Time  0.035 ( 0.039)	Data  0.002 ( 0.003)	Loss 7.2069e-03 (2.0022e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [41][180/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.9049e-02 (2.0046e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [41][190/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.4301e-02 (2.0245e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [41][200/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.2734e-02 (2.0530e-02)	Acc@1  98.44 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [41][210/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.5592e-02 (2.0542e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [41][220/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.1779e-02 (2.0606e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [41][230/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.7922e-02 (2.0755e-02)	Acc@1  97.66 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [41][240/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.3555e-03 (2.0717e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [41][250/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.2060e-02 (2.0593e-02)	Acc@1  98.44 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [41][260/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.0410e-02 (2.0706e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [41][270/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.2434e-02 (2.0892e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [41][280/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.6233e-02 (2.0816e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [41][290/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.7545e-02 (2.0731e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [41][300/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.2225e-03 (2.0742e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [41][310/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.2927e-02 (2.0768e-02)	Acc@1  97.66 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [41][320/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.8867e-02 (2.1009e-02)	Acc@1  98.44 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [41][330/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.6280e-02 (2.1386e-02)	Acc@1  98.44 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [41][340/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.5027e-02 (2.1522e-02)	Acc@1  97.66 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [41][350/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.3956e-02 (2.1618e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [41][360/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.1222e-03 (2.1522e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [41][370/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.9106e-02 (2.1417e-02)	Acc@1  98.44 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [41][380/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.8508e-02 (2.1510e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [41][390/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.0401e-02 (2.1584e-02)	Acc@1  98.75 ( 99.37)	Acc@5 100.00 (100.00)
## e[41] optimizer.zero_grad (sum) time: 0.17479205131530762
## e[41]       loss.backward (sum) time: 2.9732675552368164
## e[41]      optimizer.step (sum) time: 1.0174612998962402
## epoch[41] training(only) time: 15.096560001373291
# Switched to evaluate mode...
Test: [  0/100]	Time  0.180 ( 0.180)	Loss 2.6828e-01 (2.6828e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.018 ( 0.034)	Loss 3.5658e-01 (2.4708e-01)	Acc@1  89.00 ( 93.00)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.018 ( 0.027)	Loss 3.4389e-01 (2.6648e-01)	Acc@1  90.00 ( 92.95)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.018 ( 0.025)	Loss 2.3528e-01 (2.7072e-01)	Acc@1  93.00 ( 93.03)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.019 ( 0.023)	Loss 1.9260e-01 (2.7533e-01)	Acc@1  95.00 ( 93.02)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.017 ( 0.022)	Loss 1.0981e-01 (2.7612e-01)	Acc@1  97.00 ( 93.04)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.016 ( 0.022)	Loss 2.6377e-01 (2.6842e-01)	Acc@1  93.00 ( 93.03)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.019 ( 0.021)	Loss 5.2379e-01 (2.6328e-01)	Acc@1  89.00 ( 93.01)	Acc@5  99.00 ( 99.79)
Test: [ 80/100]	Time  0.016 ( 0.021)	Loss 1.3929e-01 (2.6549e-01)	Acc@1  95.00 ( 92.99)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.023 ( 0.021)	Loss 1.4561e-01 (2.6139e-01)	Acc@1  94.00 ( 93.03)	Acc@5 100.00 ( 99.79)
 * Acc@1 93.130 Acc@5 99.790
### epoch[41] execution time: 17.27094078063965
EPOCH 42
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [42][  0/391]	Time  0.218 ( 0.218)	Data  0.183 ( 0.183)	Loss 2.0210e-02 (2.0210e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [42][ 10/391]	Time  0.037 ( 0.055)	Data  0.001 ( 0.019)	Loss 5.7035e-03 (2.1092e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [42][ 20/391]	Time  0.039 ( 0.047)	Data  0.001 ( 0.011)	Loss 7.6503e-03 (2.0395e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 ( 99.96)
Epoch: [42][ 30/391]	Time  0.037 ( 0.044)	Data  0.001 ( 0.008)	Loss 1.8945e-02 (2.0881e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 ( 99.97)
Epoch: [42][ 40/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.007)	Loss 7.0845e-03 (1.9569e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 ( 99.98)
Epoch: [42][ 50/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.006)	Loss 3.4719e-02 (1.9865e-02)	Acc@1  98.44 ( 99.46)	Acc@5 100.00 ( 99.98)
Epoch: [42][ 60/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.005)	Loss 4.7668e-02 (2.0001e-02)	Acc@1  97.66 ( 99.46)	Acc@5 100.00 ( 99.99)
Epoch: [42][ 70/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.005)	Loss 2.5472e-02 (2.1123e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 ( 99.99)
Epoch: [42][ 80/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.2493e-02 (2.0706e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 ( 99.99)
Epoch: [42][ 90/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 3.0680e-02 (2.1305e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 ( 99.99)
Epoch: [42][100/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.8524e-02 (2.1706e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 ( 99.99)
Epoch: [42][110/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.2178e-02 (2.0937e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 ( 99.99)
Epoch: [42][120/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.0701e-02 (2.1198e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 ( 99.99)
Epoch: [42][130/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 5.3426e-03 (2.0926e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 ( 99.99)
Epoch: [42][140/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 8.3646e-03 (2.0682e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 ( 99.99)
Epoch: [42][150/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.0881e-02 (2.0758e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 ( 99.99)
Epoch: [42][160/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.1769e-02 (2.0623e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [42][170/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.5080e-02 (2.1026e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [42][180/391]	Time  0.038 ( 0.039)	Data  0.002 ( 0.003)	Loss 2.3770e-02 (2.0750e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [42][190/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.5750e-02 (2.0633e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [42][200/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.4819e-02 (2.0868e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [42][210/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.5608e-02 (2.0911e-02)	Acc@1  98.44 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [42][220/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.1992e-02 (2.0781e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [42][230/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.2391e-02 (2.1050e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [42][240/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.6126e-02 (2.0932e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [42][250/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.9180e-03 (2.1265e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [42][260/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.1399e-02 (2.1559e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [42][270/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.4451e-02 (2.1627e-02)	Acc@1  98.44 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [42][280/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.9399e-02 (2.1670e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [42][290/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.4423e-02 (2.1579e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [42][300/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.5952e-02 (2.1532e-02)	Acc@1  98.44 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [42][310/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.7773e-03 (2.1820e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [42][320/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.3540e-02 (2.1818e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [42][330/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.2997e-02 (2.1747e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [42][340/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.3452e-03 (2.1736e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [42][350/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.8156e-03 (2.1864e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [42][360/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.1923e-02 (2.1787e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [42][370/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.7380e-02 (2.1745e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [42][380/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.9747e-02 (2.1789e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [42][390/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.9632e-02 (2.1787e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
## e[42] optimizer.zero_grad (sum) time: 0.17332077026367188
## e[42]       loss.backward (sum) time: 2.95094895362854
## e[42]      optimizer.step (sum) time: 1.0222442150115967
## epoch[42] training(only) time: 15.171022653579712
# Switched to evaluate mode...
Test: [  0/100]	Time  0.173 ( 0.173)	Loss 2.7479e-01 (2.7479e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.020 ( 0.033)	Loss 3.7887e-01 (2.5660e-01)	Acc@1  88.00 ( 93.27)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.016 ( 0.026)	Loss 3.7452e-01 (2.6722e-01)	Acc@1  90.00 ( 93.10)	Acc@5 100.00 ( 99.90)
Test: [ 30/100]	Time  0.016 ( 0.023)	Loss 2.1568e-01 (2.7405e-01)	Acc@1  93.00 ( 93.10)	Acc@5 100.00 ( 99.84)
Test: [ 40/100]	Time  0.017 ( 0.022)	Loss 1.8223e-01 (2.7397e-01)	Acc@1  93.00 ( 93.02)	Acc@5 100.00 ( 99.80)
Test: [ 50/100]	Time  0.021 ( 0.021)	Loss 9.6035e-02 (2.7440e-01)	Acc@1  96.00 ( 93.02)	Acc@5 100.00 ( 99.80)
Test: [ 60/100]	Time  0.023 ( 0.021)	Loss 2.9792e-01 (2.6731e-01)	Acc@1  93.00 ( 92.98)	Acc@5 100.00 ( 99.84)
Test: [ 70/100]	Time  0.016 ( 0.021)	Loss 5.2194e-01 (2.6377e-01)	Acc@1  90.00 ( 93.07)	Acc@5 100.00 ( 99.83)
Test: [ 80/100]	Time  0.024 ( 0.020)	Loss 1.7599e-01 (2.6713e-01)	Acc@1  95.00 ( 93.12)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.016 ( 0.020)	Loss 1.1031e-01 (2.6247e-01)	Acc@1  93.00 ( 93.09)	Acc@5 100.00 ( 99.81)
 * Acc@1 93.170 Acc@5 99.810
### epoch[42] execution time: 17.332247972488403
EPOCH 43
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [43][  0/391]	Time  0.215 ( 0.215)	Data  0.181 ( 0.181)	Loss 6.3019e-03 (6.3019e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [43][ 10/391]	Time  0.037 ( 0.054)	Data  0.001 ( 0.018)	Loss 5.5404e-03 (1.4751e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [43][ 20/391]	Time  0.037 ( 0.046)	Data  0.001 ( 0.011)	Loss 2.1289e-02 (1.6146e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [43][ 30/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.008)	Loss 1.0288e-02 (1.5456e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [43][ 40/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.007)	Loss 7.1784e-03 (1.5459e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [43][ 50/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.006)	Loss 4.5703e-02 (1.6757e-02)	Acc@1  98.44 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [43][ 60/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.005)	Loss 1.6172e-02 (1.8078e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [43][ 70/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.005)	Loss 2.1681e-02 (1.7817e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [43][ 80/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.4318e-02 (1.7908e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [43][ 90/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.004)	Loss 6.4211e-03 (1.8082e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [43][100/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.4305e-02 (1.7854e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [43][110/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.0314e-02 (1.8328e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [43][120/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.5225e-02 (1.8087e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [43][130/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.5916e-02 (1.7977e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 ( 99.99)
Epoch: [43][140/391]	Time  0.040 ( 0.039)	Data  0.007 ( 0.004)	Loss 3.4944e-03 (1.8573e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 ( 99.99)
Epoch: [43][150/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.6605e-03 (1.8462e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 ( 99.99)
Epoch: [43][160/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.5828e-02 (1.8409e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 ( 99.99)
Epoch: [43][170/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.0900e-02 (1.8622e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 ( 99.99)
Epoch: [43][180/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.2024e-02 (1.8867e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 ( 99.99)
Epoch: [43][190/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.0146e-02 (1.9150e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [43][200/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.3603e-02 (1.8971e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [43][210/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.2488e-02 (1.8939e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [43][220/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.1702e-02 (1.8846e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [43][230/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.8290e-03 (1.8685e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 ( 99.99)
Epoch: [43][240/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.6914e-02 (1.8569e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 ( 99.99)
Epoch: [43][250/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.9523e-03 (1.8493e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 ( 99.99)
Epoch: [43][260/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.8455e-02 (1.8964e-02)	Acc@1  98.44 ( 99.46)	Acc@5 100.00 ( 99.99)
Epoch: [43][270/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.2387e-02 (1.8742e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 ( 99.99)
Epoch: [43][280/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.0897e-02 (1.8753e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 ( 99.99)
Epoch: [43][290/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.2888e-02 (1.8942e-02)	Acc@1  97.66 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [43][300/391]	Time  0.038 ( 0.038)	Data  0.002 ( 0.003)	Loss 7.9593e-03 (1.8730e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [43][310/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.5759e-03 (1.8533e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 ( 99.99)
Epoch: [43][320/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.5835e-02 (1.8534e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [43][330/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.0333e-02 (1.8739e-02)	Acc@1  98.44 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [43][340/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.5628e-02 (1.8681e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [43][350/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.3967e-02 (1.8558e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [43][360/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 9.2640e-03 (1.8525e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [43][370/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.0880e-02 (1.8650e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [43][380/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.0605e-02 (1.8681e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [43][390/391]	Time  0.041 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.7785e-02 (1.8568e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
## e[43] optimizer.zero_grad (sum) time: 0.17482781410217285
## e[43]       loss.backward (sum) time: 2.970621109008789
## e[43]      optimizer.step (sum) time: 1.0090656280517578
## epoch[43] training(only) time: 15.098201990127563
# Switched to evaluate mode...
Test: [  0/100]	Time  0.179 ( 0.179)	Loss 2.7129e-01 (2.7129e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.033)	Loss 3.4029e-01 (2.4549e-01)	Acc@1  91.00 ( 93.45)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.016 ( 0.026)	Loss 4.2856e-01 (2.6916e-01)	Acc@1  88.00 ( 92.81)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.020 ( 0.023)	Loss 2.2985e-01 (2.7495e-01)	Acc@1  91.00 ( 92.87)	Acc@5 100.00 ( 99.87)
Test: [ 40/100]	Time  0.019 ( 0.023)	Loss 1.8103e-01 (2.7465e-01)	Acc@1  95.00 ( 92.95)	Acc@5  99.00 ( 99.83)
Test: [ 50/100]	Time  0.019 ( 0.022)	Loss 8.2496e-02 (2.8083e-01)	Acc@1  96.00 ( 92.92)	Acc@5 100.00 ( 99.82)
Test: [ 60/100]	Time  0.016 ( 0.021)	Loss 3.6753e-01 (2.7494e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 ( 99.85)
Test: [ 70/100]	Time  0.019 ( 0.021)	Loss 6.2513e-01 (2.7155e-01)	Acc@1  89.00 ( 93.03)	Acc@5  99.00 ( 99.85)
Test: [ 80/100]	Time  0.020 ( 0.021)	Loss 1.9161e-01 (2.7538e-01)	Acc@1  95.00 ( 93.04)	Acc@5 100.00 ( 99.83)
Test: [ 90/100]	Time  0.017 ( 0.020)	Loss 1.4353e-01 (2.7092e-01)	Acc@1  93.00 ( 93.08)	Acc@5 100.00 ( 99.84)
 * Acc@1 93.120 Acc@5 99.840
### epoch[43] execution time: 17.295839071273804
EPOCH 44
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [44][  0/391]	Time  0.222 ( 0.222)	Data  0.187 ( 0.187)	Loss 2.1407e-02 (2.1407e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [44][ 10/391]	Time  0.037 ( 0.054)	Data  0.001 ( 0.019)	Loss 1.7314e-02 (1.7304e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [44][ 20/391]	Time  0.038 ( 0.047)	Data  0.001 ( 0.011)	Loss 1.5264e-02 (1.5811e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [44][ 30/391]	Time  0.037 ( 0.044)	Data  0.002 ( 0.008)	Loss 6.3283e-02 (1.6577e-02)	Acc@1  98.44 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [44][ 40/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.007)	Loss 1.5756e-02 (1.7365e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [44][ 50/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.006)	Loss 7.0411e-03 (1.6585e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [44][ 60/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.005)	Loss 2.1595e-02 (1.7353e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [44][ 70/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.005)	Loss 2.4264e-02 (1.6881e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [44][ 80/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.005)	Loss 1.4483e-02 (1.6828e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [44][ 90/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.5431e-02 (1.6666e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [44][100/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 7.3326e-03 (1.6439e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [44][110/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.7342e-02 (1.7112e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [44][120/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 4.1486e-03 (1.6842e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [44][130/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 6.0041e-02 (1.6958e-02)	Acc@1  96.88 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [44][140/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.004)	Loss 4.5179e-02 (1.7357e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [44][150/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.7870e-02 (1.7493e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [44][160/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.7048e-02 (1.7676e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [44][170/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.3914e-02 (1.7861e-02)	Acc@1  98.44 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [44][180/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.4062e-02 (1.7803e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [44][190/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.8046e-03 (1.8161e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [44][200/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.8796e-02 (1.8078e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [44][210/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.6030e-02 (1.7968e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [44][220/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.7504e-02 (1.7920e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [44][230/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.8359e-02 (1.8126e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [44][240/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.7884e-03 (1.7991e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [44][250/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.3846e-02 (1.7903e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [44][260/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.7602e-02 (1.8196e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [44][270/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.0507e-02 (1.8307e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [44][280/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.9139e-02 (1.8537e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [44][290/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.7553e-02 (1.8534e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [44][300/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.6832e-02 (1.8691e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [44][310/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.1507e-02 (1.8810e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [44][320/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.3791e-02 (1.8829e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [44][330/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.0180e-02 (1.8810e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [44][340/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.5612e-02 (1.8764e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [44][350/391]	Time  0.040 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.2682e-02 (1.8754e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [44][360/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.1798e-02 (1.8922e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [44][370/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.3162e-02 (1.8866e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [44][380/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.2729e-03 (1.8825e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [44][390/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.5995e-03 (1.8827e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
## e[44] optimizer.zero_grad (sum) time: 0.17526745796203613
## e[44]       loss.backward (sum) time: 2.9888296127319336
## e[44]      optimizer.step (sum) time: 1.023153305053711
## epoch[44] training(only) time: 15.139160871505737
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 2.3276e-01 (2.3276e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.016 ( 0.034)	Loss 2.8706e-01 (2.4035e-01)	Acc@1  90.00 ( 92.82)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.016 ( 0.027)	Loss 4.3004e-01 (2.6851e-01)	Acc@1  89.00 ( 92.62)	Acc@5 100.00 ( 99.90)
Test: [ 30/100]	Time  0.022 ( 0.024)	Loss 2.3081e-01 (2.7734e-01)	Acc@1  92.00 ( 92.97)	Acc@5 100.00 ( 99.84)
Test: [ 40/100]	Time  0.020 ( 0.023)	Loss 2.3023e-01 (2.8249e-01)	Acc@1  92.00 ( 92.88)	Acc@5 100.00 ( 99.83)
Test: [ 50/100]	Time  0.016 ( 0.022)	Loss 9.2135e-02 (2.8690e-01)	Acc@1  96.00 ( 92.92)	Acc@5 100.00 ( 99.82)
Test: [ 60/100]	Time  0.019 ( 0.021)	Loss 2.9922e-01 (2.7542e-01)	Acc@1  91.00 ( 93.02)	Acc@5 100.00 ( 99.85)
Test: [ 70/100]	Time  0.023 ( 0.021)	Loss 6.1437e-01 (2.7137e-01)	Acc@1  89.00 ( 93.00)	Acc@5  99.00 ( 99.83)
Test: [ 80/100]	Time  0.019 ( 0.020)	Loss 1.8872e-01 (2.7474e-01)	Acc@1  95.00 ( 93.10)	Acc@5 100.00 ( 99.81)
Test: [ 90/100]	Time  0.020 ( 0.021)	Loss 1.9686e-01 (2.7227e-01)	Acc@1  93.00 ( 93.03)	Acc@5 100.00 ( 99.80)
 * Acc@1 93.110 Acc@5 99.800
### epoch[44] execution time: 17.3366596698761
EPOCH 45
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [45][  0/391]	Time  0.206 ( 0.206)	Data  0.169 ( 0.169)	Loss 1.0838e-02 (1.0838e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [45][ 10/391]	Time  0.037 ( 0.053)	Data  0.001 ( 0.017)	Loss 3.4855e-03 (1.2509e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [45][ 20/391]	Time  0.037 ( 0.046)	Data  0.001 ( 0.010)	Loss 4.2359e-03 (1.7960e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [45][ 30/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.007)	Loss 9.1034e-03 (1.8832e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [45][ 40/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.006)	Loss 1.5529e-02 (1.9171e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [45][ 50/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.005)	Loss 6.7396e-03 (1.8200e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [45][ 60/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.005)	Loss 2.0669e-02 (1.7323e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [45][ 70/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.005)	Loss 4.3015e-03 (1.7943e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [45][ 80/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 5.9156e-03 (1.8050e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [45][ 90/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.7354e-02 (1.8305e-02)	Acc@1  98.44 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [45][100/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 7.9396e-03 (1.7669e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [45][110/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 4.0695e-02 (1.7678e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [45][120/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.7631e-02 (1.7225e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [45][130/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.9747e-03 (1.6851e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [45][140/391]	Time  0.040 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.6234e-02 (1.7217e-02)	Acc@1  98.44 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [45][150/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.5094e-02 (1.7430e-02)	Acc@1  98.44 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [45][160/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.2352e-02 (1.7471e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [45][170/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.6002e-02 (1.7204e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [45][180/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.5587e-03 (1.7377e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [45][190/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.6314e-03 (1.7227e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [45][200/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.1561e-02 (1.7370e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [45][210/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.7245e-03 (1.7352e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [45][220/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.4567e-03 (1.7075e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [45][230/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.0975e-02 (1.6857e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [45][240/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.3930e-03 (1.6724e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [45][250/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.5137e-02 (1.6781e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [45][260/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.2538e-02 (1.6749e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [45][270/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.3078e-02 (1.6888e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [45][280/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.3899e-03 (1.6841e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [45][290/391]	Time  0.040 ( 0.039)	Data  0.002 ( 0.003)	Loss 1.0360e-02 (1.6831e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [45][300/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.5658e-02 (1.7153e-02)	Acc@1  98.44 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [45][310/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.3424e-02 (1.7205e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [45][320/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.4114e-03 (1.7183e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [45][330/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.3948e-02 (1.7446e-02)	Acc@1  98.44 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [45][340/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.0109e-02 (1.7377e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [45][350/391]	Time  0.040 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.2573e-02 (1.7277e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [45][360/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.3921e-03 (1.7194e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [45][370/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.2823e-02 (1.7110e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [45][380/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.0394e-02 (1.7287e-02)	Acc@1  98.44 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [45][390/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.8322e-02 (1.7282e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
## e[45] optimizer.zero_grad (sum) time: 0.17610979080200195
## e[45]       loss.backward (sum) time: 3.018376350402832
## e[45]      optimizer.step (sum) time: 1.0245773792266846
## epoch[45] training(only) time: 15.12052035331726
# Switched to evaluate mode...
Test: [  0/100]	Time  0.178 ( 0.178)	Loss 2.5340e-01 (2.5340e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.021 ( 0.033)	Loss 3.1890e-01 (2.4777e-01)	Acc@1  91.00 ( 93.36)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.016 ( 0.026)	Loss 3.4451e-01 (2.7227e-01)	Acc@1  90.00 ( 93.10)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.021 ( 0.024)	Loss 2.5779e-01 (2.7675e-01)	Acc@1  91.00 ( 93.13)	Acc@5 100.00 ( 99.87)
Test: [ 40/100]	Time  0.022 ( 0.023)	Loss 2.2232e-01 (2.8113e-01)	Acc@1  94.00 ( 93.12)	Acc@5  99.00 ( 99.83)
Test: [ 50/100]	Time  0.024 ( 0.022)	Loss 6.4077e-02 (2.8520e-01)	Acc@1  97.00 ( 93.16)	Acc@5 100.00 ( 99.82)
Test: [ 60/100]	Time  0.023 ( 0.021)	Loss 2.9942e-01 (2.7799e-01)	Acc@1  93.00 ( 93.15)	Acc@5 100.00 ( 99.85)
Test: [ 70/100]	Time  0.020 ( 0.021)	Loss 5.8677e-01 (2.7431e-01)	Acc@1  88.00 ( 93.18)	Acc@5  99.00 ( 99.82)
Test: [ 80/100]	Time  0.023 ( 0.021)	Loss 1.9485e-01 (2.7623e-01)	Acc@1  94.00 ( 93.20)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.017 ( 0.020)	Loss 1.6963e-01 (2.7160e-01)	Acc@1  93.00 ( 93.16)	Acc@5 100.00 ( 99.81)
 * Acc@1 93.160 Acc@5 99.820
### epoch[45] execution time: 17.270206689834595
EPOCH 46
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [46][  0/391]	Time  0.208 ( 0.208)	Data  0.173 ( 0.173)	Loss 1.2939e-02 (1.2939e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [46][ 10/391]	Time  0.037 ( 0.053)	Data  0.001 ( 0.017)	Loss 1.5340e-02 (8.4043e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [46][ 20/391]	Time  0.037 ( 0.046)	Data  0.001 ( 0.010)	Loss 3.8315e-02 (1.1550e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [46][ 30/391]	Time  0.036 ( 0.044)	Data  0.001 ( 0.008)	Loss 1.4891e-02 (1.2940e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [46][ 40/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.006)	Loss 2.5913e-02 (1.4254e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [46][ 50/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.005)	Loss 1.0948e-02 (1.4523e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [46][ 60/391]	Time  0.035 ( 0.041)	Data  0.001 ( 0.005)	Loss 5.6728e-03 (1.4308e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [46][ 70/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.005)	Loss 1.2533e-02 (1.3777e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [46][ 80/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.7470e-03 (1.4210e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [46][ 90/391]	Time  0.038 ( 0.040)	Data  0.002 ( 0.004)	Loss 1.9444e-02 (1.4366e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [46][100/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 3.2954e-02 (1.4076e-02)	Acc@1  98.44 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [46][110/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.2481e-02 (1.4161e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [46][120/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.3933e-02 (1.4573e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [46][130/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.7885e-02 (1.4993e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [46][140/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.2305e-02 (1.5284e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [46][150/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.6867e-02 (1.5719e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [46][160/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.0702e-02 (1.5548e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [46][170/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.5706e-03 (1.5546e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [46][180/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.8359e-03 (1.5270e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [46][190/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.8220e-02 (1.5487e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [46][200/391]	Time  0.043 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.9915e-02 (1.5619e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [46][210/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.8686e-02 (1.5999e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [46][220/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.3420e-02 (1.6207e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [46][230/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.6932e-02 (1.6277e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [46][240/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.1501e-02 (1.6207e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [46][250/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.0407e-02 (1.5907e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [46][260/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.2858e-03 (1.5840e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [46][270/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.1376e-03 (1.5750e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [46][280/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.3721e-02 (1.5660e-02)	Acc@1  98.44 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [46][290/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.3451e-03 (1.5747e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [46][300/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 9.8329e-03 (1.5645e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [46][310/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.2139e-03 (1.5709e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [46][320/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.7426e-03 (1.5635e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [46][330/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.2317e-02 (1.5525e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [46][340/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.7841e-03 (1.5633e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [46][350/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.5356e-02 (1.5759e-02)	Acc@1  98.44 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [46][360/391]	Time  0.041 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.7407e-02 (1.5789e-02)	Acc@1  97.66 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [46][370/391]	Time  0.037 ( 0.038)	Data  0.004 ( 0.003)	Loss 1.7788e-02 (1.5836e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [46][380/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 9.4262e-03 (1.5790e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [46][390/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.0159e-02 (1.5912e-02)	Acc@1  97.50 ( 99.52)	Acc@5 100.00 (100.00)
## e[46] optimizer.zero_grad (sum) time: 0.1752917766571045
## e[46]       loss.backward (sum) time: 3.008230447769165
## e[46]      optimizer.step (sum) time: 1.0167021751403809
## epoch[46] training(only) time: 15.14861536026001
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 2.6170e-01 (2.6170e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.017 ( 0.033)	Loss 3.8083e-01 (2.6050e-01)	Acc@1  90.00 ( 93.45)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.017 ( 0.027)	Loss 3.8665e-01 (2.7533e-01)	Acc@1  88.00 ( 92.86)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.020 ( 0.024)	Loss 2.7885e-01 (2.7998e-01)	Acc@1  93.00 ( 93.13)	Acc@5 100.00 ( 99.81)
Test: [ 40/100]	Time  0.020 ( 0.023)	Loss 2.0270e-01 (2.7967e-01)	Acc@1  95.00 ( 93.24)	Acc@5  99.00 ( 99.78)
Test: [ 50/100]	Time  0.016 ( 0.022)	Loss 7.5696e-02 (2.8650e-01)	Acc@1  98.00 ( 93.24)	Acc@5 100.00 ( 99.80)
Test: [ 60/100]	Time  0.021 ( 0.021)	Loss 2.8068e-01 (2.7814e-01)	Acc@1  95.00 ( 93.28)	Acc@5 100.00 ( 99.82)
Test: [ 70/100]	Time  0.021 ( 0.021)	Loss 5.4244e-01 (2.7399e-01)	Acc@1  90.00 ( 93.30)	Acc@5  99.00 ( 99.80)
Test: [ 80/100]	Time  0.020 ( 0.021)	Loss 2.3213e-01 (2.7792e-01)	Acc@1  95.00 ( 93.22)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.020 ( 0.020)	Loss 2.1191e-01 (2.7479e-01)	Acc@1  93.00 ( 93.21)	Acc@5 100.00 ( 99.80)
 * Acc@1 93.200 Acc@5 99.800
### epoch[46] execution time: 17.303600788116455
EPOCH 47
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [47][  0/391]	Time  0.215 ( 0.215)	Data  0.181 ( 0.181)	Loss 1.3663e-02 (1.3663e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [47][ 10/391]	Time  0.040 ( 0.055)	Data  0.001 ( 0.018)	Loss 4.4169e-03 (9.1820e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [47][ 20/391]	Time  0.036 ( 0.047)	Data  0.001 ( 0.011)	Loss 2.7139e-02 (1.1917e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [47][ 30/391]	Time  0.037 ( 0.044)	Data  0.001 ( 0.008)	Loss 3.3709e-02 (1.4216e-02)	Acc@1  98.44 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [47][ 40/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.007)	Loss 8.8725e-03 (1.4350e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [47][ 50/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.006)	Loss 9.6076e-03 (1.4248e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 ( 99.98)
Epoch: [47][ 60/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.005)	Loss 1.5909e-02 (1.4867e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 ( 99.99)
Epoch: [47][ 70/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.005)	Loss 6.2044e-03 (1.4392e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 ( 99.99)
Epoch: [47][ 80/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.2892e-02 (1.3921e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 ( 99.99)
Epoch: [47][ 90/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 5.2994e-03 (1.4116e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 ( 99.99)
Epoch: [47][100/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 5.1809e-03 (1.3940e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 ( 99.99)
Epoch: [47][110/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 7.9976e-03 (1.3625e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 ( 99.99)
Epoch: [47][120/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 7.0371e-03 (1.3997e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 ( 99.99)
Epoch: [47][130/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.6005e-02 (1.4402e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 ( 99.99)
Epoch: [47][140/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.7062e-03 (1.4164e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 ( 99.99)
Epoch: [47][150/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.3913e-02 (1.4447e-02)	Acc@1  98.44 ( 99.61)	Acc@5 100.00 ( 99.99)
Epoch: [47][160/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 9.2626e-03 (1.4650e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [47][170/391]	Time  0.036 ( 0.039)	Data  0.002 ( 0.003)	Loss 7.9727e-03 (1.4886e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [47][180/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.3511e-02 (1.4945e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [47][190/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.6138e-02 (1.4797e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [47][200/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.2058e-02 (1.4770e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [47][210/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.7255e-02 (1.4991e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [47][220/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.8831e-03 (1.4827e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [47][230/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.4052e-02 (1.4784e-02)	Acc@1  98.44 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [47][240/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.7086e-03 (1.4629e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [47][250/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.8146e-02 (1.4548e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [47][260/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 9.0596e-03 (1.4481e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [47][270/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.2003e-02 (1.4359e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [47][280/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.4180e-03 (1.4344e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [47][290/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.4833e-03 (1.4195e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [47][300/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.2303e-02 (1.4432e-02)	Acc@1  97.66 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [47][310/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.5719e-02 (1.4438e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [47][320/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.1789e-02 (1.4464e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [47][330/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.1777e-03 (1.4323e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [47][340/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.1514e-02 (1.4554e-02)	Acc@1  98.44 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [47][350/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.9680e-03 (1.4761e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [47][360/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.1360e-02 (1.4791e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [47][370/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.9320e-03 (1.4727e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [47][380/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.4034e-03 (1.4722e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [47][390/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.0167e-02 (1.4728e-02)	Acc@1  97.50 ( 99.58)	Acc@5 100.00 (100.00)
## e[47] optimizer.zero_grad (sum) time: 0.17723917961120605
## e[47]       loss.backward (sum) time: 3.019998788833618
## e[47]      optimizer.step (sum) time: 1.0347979068756104
## epoch[47] training(only) time: 15.15943169593811
# Switched to evaluate mode...
Test: [  0/100]	Time  0.181 ( 0.181)	Loss 2.1719e-01 (2.1719e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.016 ( 0.034)	Loss 3.6377e-01 (2.4122e-01)	Acc@1  90.00 ( 93.27)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.019 ( 0.026)	Loss 3.8257e-01 (2.6990e-01)	Acc@1  90.00 ( 93.10)	Acc@5 100.00 ( 99.90)
Test: [ 30/100]	Time  0.018 ( 0.024)	Loss 2.2300e-01 (2.7989e-01)	Acc@1  94.00 ( 93.10)	Acc@5 100.00 ( 99.84)
Test: [ 40/100]	Time  0.020 ( 0.023)	Loss 2.3295e-01 (2.8415e-01)	Acc@1  94.00 ( 93.07)	Acc@5  99.00 ( 99.80)
Test: [ 50/100]	Time  0.016 ( 0.022)	Loss 1.2993e-01 (2.8875e-01)	Acc@1  96.00 ( 93.04)	Acc@5 100.00 ( 99.80)
Test: [ 60/100]	Time  0.020 ( 0.022)	Loss 2.9600e-01 (2.8115e-01)	Acc@1  91.00 ( 93.00)	Acc@5 100.00 ( 99.84)
Test: [ 70/100]	Time  0.023 ( 0.021)	Loss 5.3976e-01 (2.7344e-01)	Acc@1  90.00 ( 93.17)	Acc@5  99.00 ( 99.82)
Test: [ 80/100]	Time  0.023 ( 0.021)	Loss 1.7160e-01 (2.7618e-01)	Acc@1  95.00 ( 93.22)	Acc@5 100.00 ( 99.81)
Test: [ 90/100]	Time  0.017 ( 0.021)	Loss 1.6233e-01 (2.7106e-01)	Acc@1  94.00 ( 93.24)	Acc@5 100.00 ( 99.82)
 * Acc@1 93.290 Acc@5 99.800
### epoch[47] execution time: 17.349278211593628
EPOCH 48
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [48][  0/391]	Time  0.222 ( 0.222)	Data  0.187 ( 0.187)	Loss 1.0841e-02 (1.0841e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [48][ 10/391]	Time  0.037 ( 0.054)	Data  0.001 ( 0.019)	Loss 2.4943e-02 (1.6012e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [48][ 20/391]	Time  0.038 ( 0.046)	Data  0.001 ( 0.011)	Loss 9.1740e-03 (1.2758e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [48][ 30/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.008)	Loss 1.2897e-02 (1.3867e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [48][ 40/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.007)	Loss 7.5595e-03 (1.3842e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [48][ 50/391]	Time  0.035 ( 0.041)	Data  0.001 ( 0.006)	Loss 1.6168e-02 (1.3630e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [48][ 60/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.005)	Loss 1.9181e-02 (1.4189e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [48][ 70/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.005)	Loss 9.6383e-03 (1.3958e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [48][ 80/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.005)	Loss 1.0721e-02 (1.3942e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [48][ 90/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.0750e-03 (1.3684e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [48][100/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.004)	Loss 5.0807e-03 (1.4746e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [48][110/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.8937e-02 (1.4440e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [48][120/391]	Time  0.043 ( 0.039)	Data  0.001 ( 0.004)	Loss 8.9251e-03 (1.4614e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [48][130/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.8771e-02 (1.5212e-02)	Acc@1  97.66 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [48][140/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.3685e-02 (1.4791e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [48][150/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.7584e-03 (1.4722e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [48][160/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.4906e-02 (1.4657e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [48][170/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.5510e-02 (1.4783e-02)	Acc@1  98.44 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [48][180/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.5188e-03 (1.4579e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [48][190/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.2428e-02 (1.4516e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [48][200/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.2958e-02 (1.4607e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [48][210/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.6586e-02 (1.4723e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [48][220/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.2377e-02 (1.4500e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [48][230/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.0678e-02 (1.4371e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [48][240/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.7040e-02 (1.4320e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [48][250/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.9648e-03 (1.4384e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [48][260/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.5876e-03 (1.4337e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [48][270/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.9557e-03 (1.4307e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [48][280/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.7092e-03 (1.4173e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [48][290/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.1047e-03 (1.4124e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [48][300/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 9.6914e-03 (1.4103e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [48][310/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.1108e-02 (1.4197e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [48][320/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.8756e-03 (1.4241e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [48][330/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.1675e-03 (1.4125e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [48][340/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.6588e-02 (1.4073e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [48][350/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.8027e-03 (1.4031e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [48][360/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.1877e-03 (1.4083e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [48][370/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.2903e-02 (1.4081e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [48][380/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.5497e-02 (1.4159e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [48][390/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.0243e-02 (1.4252e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
## e[48] optimizer.zero_grad (sum) time: 0.1761629581451416
## e[48]       loss.backward (sum) time: 3.000361442565918
## e[48]      optimizer.step (sum) time: 1.038926362991333
## epoch[48] training(only) time: 15.117135286331177
# Switched to evaluate mode...
Test: [  0/100]	Time  0.178 ( 0.178)	Loss 2.3439e-01 (2.3439e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.021 ( 0.033)	Loss 3.5621e-01 (2.4862e-01)	Acc@1  90.00 ( 93.18)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.016 ( 0.027)	Loss 3.5565e-01 (2.7440e-01)	Acc@1  90.00 ( 93.05)	Acc@5 100.00 ( 99.90)
Test: [ 30/100]	Time  0.021 ( 0.025)	Loss 2.6329e-01 (2.8142e-01)	Acc@1  91.00 ( 92.84)	Acc@5 100.00 ( 99.87)
Test: [ 40/100]	Time  0.018 ( 0.024)	Loss 2.2516e-01 (2.8514e-01)	Acc@1  94.00 ( 92.85)	Acc@5  99.00 ( 99.83)
Test: [ 50/100]	Time  0.016 ( 0.023)	Loss 1.3272e-01 (2.9216e-01)	Acc@1  95.00 ( 92.80)	Acc@5 100.00 ( 99.80)
Test: [ 60/100]	Time  0.017 ( 0.022)	Loss 2.8020e-01 (2.8358e-01)	Acc@1  92.00 ( 92.93)	Acc@5 100.00 ( 99.82)
Test: [ 70/100]	Time  0.021 ( 0.022)	Loss 5.7159e-01 (2.7863e-01)	Acc@1  88.00 ( 93.04)	Acc@5  98.00 ( 99.79)
Test: [ 80/100]	Time  0.016 ( 0.021)	Loss 2.0890e-01 (2.8326e-01)	Acc@1  95.00 ( 93.04)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.023 ( 0.021)	Loss 1.8114e-01 (2.7945e-01)	Acc@1  94.00 ( 93.03)	Acc@5 100.00 ( 99.77)
 * Acc@1 93.110 Acc@5 99.760
### epoch[48] execution time: 17.325934648513794
EPOCH 49
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [49][  0/391]	Time  0.208 ( 0.208)	Data  0.172 ( 0.172)	Loss 6.3774e-03 (6.3774e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [49][ 10/391]	Time  0.036 ( 0.053)	Data  0.001 ( 0.018)	Loss 8.3672e-03 (1.0644e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [49][ 20/391]	Time  0.036 ( 0.046)	Data  0.001 ( 0.010)	Loss 8.8137e-03 (1.0771e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [49][ 30/391]	Time  0.035 ( 0.043)	Data  0.001 ( 0.008)	Loss 1.0110e-02 (1.3176e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [49][ 40/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.006)	Loss 3.4691e-02 (1.5465e-02)	Acc@1  98.44 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [49][ 50/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.006)	Loss 1.1487e-02 (1.5825e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [49][ 60/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.005)	Loss 5.1915e-03 (1.4631e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [49][ 70/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.005)	Loss 1.4948e-02 (1.4775e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [49][ 80/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 8.1702e-03 (1.5072e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [49][ 90/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.004)	Loss 8.0099e-03 (1.5504e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [49][100/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.3711e-02 (1.5615e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [49][110/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.0468e-02 (1.5832e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [49][120/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.6041e-02 (1.5650e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [49][130/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.6771e-02 (1.5210e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [49][140/391]	Time  0.041 ( 0.039)	Data  0.002 ( 0.003)	Loss 1.0206e-02 (1.5035e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [49][150/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.3237e-03 (1.4816e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [49][160/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.2107e-03 (1.4675e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [49][170/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 9.1136e-03 (1.4660e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [49][180/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.8455e-03 (1.4630e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [49][190/391]	Time  0.039 ( 0.039)	Data  0.002 ( 0.003)	Loss 1.8811e-02 (1.4441e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [49][200/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.6791e-02 (1.4423e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [49][210/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.8519e-03 (1.4295e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [49][220/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.8817e-03 (1.4280e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [49][230/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.2982e-02 (1.4175e-02)	Acc@1  98.44 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [49][240/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.2460e-03 (1.4093e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [49][250/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.5130e-03 (1.3895e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [49][260/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.0685e-03 (1.3938e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [49][270/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.6666e-03 (1.3775e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [49][280/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 9.7714e-03 (1.3636e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [49][290/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.2524e-03 (1.3516e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [49][300/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.3443e-02 (1.3382e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [49][310/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.9679e-02 (1.3540e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [49][320/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.7584e-03 (1.3436e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [49][330/391]	Time  0.036 ( 0.038)	Data  0.002 ( 0.003)	Loss 1.9909e-02 (1.3506e-02)	Acc@1  98.44 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [49][340/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.3298e-02 (1.3542e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [49][350/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.3037e-02 (1.3584e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [49][360/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 9.2758e-03 (1.3480e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [49][370/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.0932e-02 (1.3464e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [49][380/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.0686e-02 (1.3528e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [49][390/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.4856e-02 (1.3647e-02)	Acc@1  97.50 ( 99.60)	Acc@5 100.00 (100.00)
## e[49] optimizer.zero_grad (sum) time: 0.1753690242767334
## e[49]       loss.backward (sum) time: 2.939098596572876
## e[49]      optimizer.step (sum) time: 1.01664137840271
## epoch[49] training(only) time: 15.13452696800232
# Switched to evaluate mode...
Test: [  0/100]	Time  0.169 ( 0.169)	Loss 2.0924e-01 (2.0924e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.022 ( 0.033)	Loss 3.6765e-01 (2.5539e-01)	Acc@1  88.00 ( 92.91)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.023 ( 0.026)	Loss 3.6869e-01 (2.7865e-01)	Acc@1  89.00 ( 92.86)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.016 ( 0.024)	Loss 3.3743e-01 (2.8653e-01)	Acc@1  91.00 ( 92.87)	Acc@5 100.00 ( 99.87)
Test: [ 40/100]	Time  0.017 ( 0.023)	Loss 1.9691e-01 (2.9134e-01)	Acc@1  94.00 ( 92.95)	Acc@5  99.00 ( 99.83)
Test: [ 50/100]	Time  0.020 ( 0.022)	Loss 7.9681e-02 (2.9951e-01)	Acc@1  97.00 ( 92.92)	Acc@5 100.00 ( 99.82)
Test: [ 60/100]	Time  0.016 ( 0.021)	Loss 2.6829e-01 (2.8916e-01)	Acc@1  92.00 ( 92.95)	Acc@5 100.00 ( 99.85)
Test: [ 70/100]	Time  0.016 ( 0.021)	Loss 5.8815e-01 (2.8515e-01)	Acc@1  89.00 ( 93.01)	Acc@5  99.00 ( 99.83)
Test: [ 80/100]	Time  0.020 ( 0.021)	Loss 1.6516e-01 (2.8791e-01)	Acc@1  96.00 ( 93.09)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.016 ( 0.020)	Loss 1.8638e-01 (2.8458e-01)	Acc@1  94.00 ( 93.08)	Acc@5 100.00 ( 99.80)
 * Acc@1 93.100 Acc@5 99.800
### epoch[49] execution time: 17.282670974731445
EPOCH 50
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [50][  0/391]	Time  0.231 ( 0.231)	Data  0.197 ( 0.197)	Loss 1.1266e-02 (1.1266e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [50][ 10/391]	Time  0.037 ( 0.055)	Data  0.001 ( 0.020)	Loss 2.9631e-03 (1.3112e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [50][ 20/391]	Time  0.036 ( 0.047)	Data  0.001 ( 0.012)	Loss 4.8491e-03 (1.0499e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [50][ 30/391]	Time  0.036 ( 0.044)	Data  0.001 ( 0.009)	Loss 4.2886e-02 (1.2641e-02)	Acc@1  98.44 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [50][ 40/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.007)	Loss 1.7374e-02 (1.2703e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [50][ 50/391]	Time  0.035 ( 0.042)	Data  0.001 ( 0.006)	Loss 7.0968e-03 (1.3438e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [50][ 60/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.005)	Loss 2.6350e-02 (1.4468e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [50][ 70/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.005)	Loss 3.8267e-03 (1.3778e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [50][ 80/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.005)	Loss 5.2602e-03 (1.4017e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [50][ 90/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 3.0234e-03 (1.4113e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [50][100/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.0067e-02 (1.3733e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [50][110/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 6.0669e-03 (1.3810e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [50][120/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.6235e-02 (1.3770e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [50][130/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.1958e-02 (1.3981e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [50][140/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.004)	Loss 3.4390e-03 (1.3864e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [50][150/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 3.7385e-03 (1.3728e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [50][160/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.0256e-02 (1.4103e-02)	Acc@1  98.44 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [50][170/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.3809e-03 (1.3888e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [50][180/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.4179e-02 (1.3785e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [50][190/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.9685e-02 (1.4035e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [50][200/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.2665e-02 (1.3785e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [50][210/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.1726e-03 (1.4016e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [50][220/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.9006e-03 (1.4018e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [50][230/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.9618e-02 (1.4267e-02)	Acc@1  98.44 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [50][240/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.5292e-03 (1.4114e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [50][250/391]	Time  0.037 ( 0.038)	Data  0.002 ( 0.003)	Loss 1.4384e-02 (1.4107e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [50][260/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.7194e-02 (1.4119e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [50][270/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.9442e-03 (1.4067e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [50][280/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.0407e-03 (1.3929e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [50][290/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.7979e-02 (1.3866e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [50][300/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.6811e-02 (1.3690e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [50][310/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.4195e-02 (1.3781e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [50][320/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.3294e-03 (1.3731e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [50][330/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.0606e-02 (1.3840e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [50][340/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.9399e-03 (1.3867e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [50][350/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.9650e-02 (1.3930e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [50][360/391]	Time  0.041 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.5940e-02 (1.3954e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [50][370/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.7254e-02 (1.4067e-02)	Acc@1  98.44 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [50][380/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.0597e-02 (1.3998e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [50][390/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.1001e-02 (1.3933e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
## e[50] optimizer.zero_grad (sum) time: 0.17553114891052246
## e[50]       loss.backward (sum) time: 2.983222484588623
## e[50]      optimizer.step (sum) time: 1.0357778072357178
## epoch[50] training(only) time: 15.09312105178833
# Switched to evaluate mode...
Test: [  0/100]	Time  0.178 ( 0.178)	Loss 2.1315e-01 (2.1315e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.017 ( 0.034)	Loss 3.5542e-01 (2.5380e-01)	Acc@1  90.00 ( 92.91)	Acc@5  99.00 ( 99.82)
Test: [ 20/100]	Time  0.016 ( 0.027)	Loss 3.7712e-01 (2.7251e-01)	Acc@1  89.00 ( 92.81)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.022 ( 0.024)	Loss 3.8786e-01 (2.8489e-01)	Acc@1  91.00 ( 92.87)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.023 ( 0.023)	Loss 2.0716e-01 (2.8426e-01)	Acc@1  94.00 ( 93.07)	Acc@5  99.00 ( 99.73)
Test: [ 50/100]	Time  0.021 ( 0.022)	Loss 7.5456e-02 (2.9283e-01)	Acc@1  98.00 ( 93.08)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.016 ( 0.021)	Loss 2.7805e-01 (2.8492e-01)	Acc@1  92.00 ( 93.10)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.020 ( 0.021)	Loss 6.0956e-01 (2.8016e-01)	Acc@1  90.00 ( 93.15)	Acc@5  99.00 ( 99.75)
Test: [ 80/100]	Time  0.019 ( 0.021)	Loss 1.9330e-01 (2.8305e-01)	Acc@1  95.00 ( 93.25)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.021 ( 0.021)	Loss 2.0770e-01 (2.7920e-01)	Acc@1  92.00 ( 93.26)	Acc@5 100.00 ( 99.77)
 * Acc@1 93.280 Acc@5 99.760
### epoch[50] execution time: 17.266933917999268
EPOCH 51
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [51][  0/391]	Time  0.209 ( 0.209)	Data  0.172 ( 0.172)	Loss 1.5795e-02 (1.5795e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [51][ 10/391]	Time  0.037 ( 0.053)	Data  0.001 ( 0.017)	Loss 4.1129e-03 (1.5608e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [51][ 20/391]	Time  0.038 ( 0.046)	Data  0.001 ( 0.010)	Loss 4.3913e-02 (1.4848e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [51][ 30/391]	Time  0.035 ( 0.043)	Data  0.001 ( 0.008)	Loss 2.3859e-02 (1.3590e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [51][ 40/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.006)	Loss 3.7778e-03 (1.2825e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [51][ 50/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.005)	Loss 4.4437e-03 (1.3227e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [51][ 60/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.005)	Loss 1.4500e-02 (1.3937e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [51][ 70/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.005)	Loss 7.1616e-03 (1.3648e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [51][ 80/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.004)	Loss 3.0838e-02 (1.3594e-02)	Acc@1  98.44 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [51][ 90/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.0856e-02 (1.3285e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [51][100/391]	Time  0.038 ( 0.040)	Data  0.002 ( 0.004)	Loss 2.3389e-02 (1.3277e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [51][110/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 6.6058e-03 (1.3033e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [51][120/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.004)	Loss 9.9591e-03 (1.2741e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [51][130/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.0642e-03 (1.2356e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [51][140/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 9.1401e-04 (1.2098e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [51][150/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.2173e-03 (1.1997e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [51][160/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.3554e-03 (1.2000e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [51][170/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.2772e-03 (1.2155e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [51][180/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.3315e-02 (1.2392e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [51][190/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.4327e-02 (1.2274e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [51][200/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 9.8530e-03 (1.2100e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [51][210/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.2676e-03 (1.2085e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [51][220/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.9152e-03 (1.1889e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [51][230/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.0901e-02 (1.1924e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [51][240/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.5151e-02 (1.1836e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [51][250/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.1797e-03 (1.1840e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [51][260/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.7621e-02 (1.2010e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [51][270/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.5963e-03 (1.1963e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [51][280/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.3648e-03 (1.1911e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [51][290/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.3060e-02 (1.1992e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [51][300/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.7335e-03 (1.1980e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [51][310/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.8868e-03 (1.1854e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [51][320/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.3990e-03 (1.1700e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [51][330/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.5398e-03 (1.1721e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [51][340/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.5274e-02 (1.1761e-02)	Acc@1  97.66 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [51][350/391]	Time  0.038 ( 0.038)	Data  0.002 ( 0.003)	Loss 5.7214e-03 (1.1874e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [51][360/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.9110e-02 (1.2056e-02)	Acc@1  98.44 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [51][370/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.0437e-02 (1.2080e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [51][380/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.7590e-03 (1.2062e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [51][390/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.7112e-02 (1.2170e-02)	Acc@1  97.50 ( 99.64)	Acc@5 100.00 (100.00)
## e[51] optimizer.zero_grad (sum) time: 0.17607522010803223
## e[51]       loss.backward (sum) time: 2.9631435871124268
## e[51]      optimizer.step (sum) time: 1.0127038955688477
## epoch[51] training(only) time: 15.14492917060852
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 2.5874e-01 (2.5874e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.025 ( 0.034)	Loss 3.6159e-01 (2.6281e-01)	Acc@1  91.00 ( 93.73)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.022 ( 0.027)	Loss 3.4967e-01 (2.8593e-01)	Acc@1  88.00 ( 93.05)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.017 ( 0.025)	Loss 3.1350e-01 (2.9242e-01)	Acc@1  91.00 ( 92.87)	Acc@5 100.00 ( 99.81)
Test: [ 40/100]	Time  0.023 ( 0.024)	Loss 2.1646e-01 (2.9260e-01)	Acc@1  95.00 ( 93.05)	Acc@5  99.00 ( 99.78)
Test: [ 50/100]	Time  0.017 ( 0.023)	Loss 7.5781e-02 (2.9688e-01)	Acc@1  98.00 ( 93.06)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.022 ( 0.022)	Loss 3.0740e-01 (2.8882e-01)	Acc@1  92.00 ( 93.05)	Acc@5 100.00 ( 99.82)
Test: [ 70/100]	Time  0.020 ( 0.022)	Loss 5.6927e-01 (2.8256e-01)	Acc@1  91.00 ( 93.18)	Acc@5 100.00 ( 99.82)
Test: [ 80/100]	Time  0.020 ( 0.021)	Loss 1.9870e-01 (2.8707e-01)	Acc@1  94.00 ( 93.20)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.017 ( 0.021)	Loss 1.9106e-01 (2.8360e-01)	Acc@1  93.00 ( 93.20)	Acc@5 100.00 ( 99.81)
 * Acc@1 93.290 Acc@5 99.820
### epoch[51] execution time: 17.36347770690918
EPOCH 52
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [52][  0/391]	Time  0.218 ( 0.218)	Data  0.182 ( 0.182)	Loss 7.6145e-03 (7.6145e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [52][ 10/391]	Time  0.037 ( 0.054)	Data  0.001 ( 0.019)	Loss 1.7769e-03 (1.0926e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [52][ 20/391]	Time  0.036 ( 0.047)	Data  0.001 ( 0.011)	Loss 8.3719e-03 (1.3611e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [52][ 30/391]	Time  0.037 ( 0.044)	Data  0.001 ( 0.008)	Loss 1.8111e-02 (1.4077e-02)	Acc@1  98.44 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [52][ 40/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.007)	Loss 3.8960e-03 (1.2599e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [52][ 50/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.006)	Loss 1.7980e-02 (1.2051e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [52][ 60/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.005)	Loss 2.0788e-03 (1.1085e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [52][ 70/391]	Time  0.035 ( 0.040)	Data  0.002 ( 0.005)	Loss 5.7392e-03 (1.1091e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [52][ 80/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.2484e-02 (1.1943e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [52][ 90/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.5796e-02 (1.2311e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [52][100/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 6.2419e-03 (1.2354e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [52][110/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.6463e-02 (1.2395e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [52][120/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.004)	Loss 5.5105e-03 (1.2531e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [52][130/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.3924e-03 (1.2229e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [52][140/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.0275e-02 (1.1948e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [52][150/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.3058e-02 (1.1921e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [52][160/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.5163e-03 (1.1781e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [52][170/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.9850e-03 (1.1783e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [52][180/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.1987e-03 (1.1579e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [52][190/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.0966e-03 (1.1515e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [52][200/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.8779e-02 (1.1457e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [52][210/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.5282e-02 (1.1562e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [52][220/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.0458e-02 (1.2048e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [52][230/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.5059e-03 (1.1929e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [52][240/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.2621e-02 (1.2126e-02)	Acc@1  98.44 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [52][250/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.1290e-02 (1.2321e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [52][260/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.9705e-02 (1.2257e-02)	Acc@1  98.44 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [52][270/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.0102e-03 (1.2443e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [52][280/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.9189e-03 (1.2345e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [52][290/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.2752e-03 (1.2474e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [52][300/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.5149e-03 (1.2494e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [52][310/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.4909e-03 (1.2407e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [52][320/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.6270e-02 (1.2356e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [52][330/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.5796e-03 (1.2244e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [52][340/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.1786e-03 (1.2253e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [52][350/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.7818e-02 (1.2321e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [52][360/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.2461e-02 (1.2278e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [52][370/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.8513e-03 (1.2204e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [52][380/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.7594e-02 (1.2159e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [52][390/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.3139e-02 (1.2045e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
## e[52] optimizer.zero_grad (sum) time: 0.17763733863830566
## e[52]       loss.backward (sum) time: 3.013113498687744
## e[52]      optimizer.step (sum) time: 1.0261104106903076
## epoch[52] training(only) time: 15.143841981887817
# Switched to evaluate mode...
Test: [  0/100]	Time  0.179 ( 0.179)	Loss 2.4456e-01 (2.4456e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.018 ( 0.034)	Loss 3.2593e-01 (2.6291e-01)	Acc@1  91.00 ( 93.45)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.020 ( 0.026)	Loss 3.7256e-01 (2.8274e-01)	Acc@1  89.00 ( 93.10)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.020 ( 0.024)	Loss 3.3763e-01 (2.9371e-01)	Acc@1  90.00 ( 93.03)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.016 ( 0.023)	Loss 2.5114e-01 (2.9469e-01)	Acc@1  93.00 ( 93.00)	Acc@5  99.00 ( 99.73)
Test: [ 50/100]	Time  0.022 ( 0.022)	Loss 1.0045e-01 (2.9998e-01)	Acc@1  97.00 ( 92.96)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.023 ( 0.021)	Loss 3.2483e-01 (2.9196e-01)	Acc@1  91.00 ( 92.93)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.018 ( 0.021)	Loss 5.8285e-01 (2.8528e-01)	Acc@1  90.00 ( 93.15)	Acc@5  99.00 ( 99.76)
Test: [ 80/100]	Time  0.022 ( 0.021)	Loss 2.0007e-01 (2.8762e-01)	Acc@1  95.00 ( 93.19)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.018 ( 0.021)	Loss 2.0918e-01 (2.8378e-01)	Acc@1  93.00 ( 93.21)	Acc@5 100.00 ( 99.76)
 * Acc@1 93.280 Acc@5 99.770
### epoch[52] execution time: 17.315207958221436
EPOCH 53
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [53][  0/391]	Time  0.218 ( 0.218)	Data  0.189 ( 0.189)	Loss 1.0085e-02 (1.0085e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [53][ 10/391]	Time  0.037 ( 0.056)	Data  0.001 ( 0.019)	Loss 1.0198e-02 (1.2585e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [53][ 20/391]	Time  0.038 ( 0.047)	Data  0.001 ( 0.011)	Loss 1.3036e-02 (1.1809e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [53][ 30/391]	Time  0.036 ( 0.044)	Data  0.001 ( 0.008)	Loss 3.4217e-02 (1.2126e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [53][ 40/391]	Time  0.037 ( 0.043)	Data  0.001 ( 0.007)	Loss 7.2232e-03 (1.1540e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [53][ 50/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.006)	Loss 4.9240e-03 (1.1471e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [53][ 60/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.005)	Loss 5.9024e-03 (1.1155e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [53][ 70/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.005)	Loss 1.6461e-02 (1.1023e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [53][ 80/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.005)	Loss 8.9151e-03 (1.0492e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [53][ 90/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.7529e-03 (1.0201e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [53][100/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 9.1498e-03 (9.9029e-03)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [53][110/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 8.7635e-03 (9.6341e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [53][120/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.7247e-02 (9.5626e-03)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [53][130/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 3.7484e-03 (9.8095e-03)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [53][140/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.004)	Loss 5.7106e-03 (9.7703e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [53][150/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 3.6082e-02 (9.8138e-03)	Acc@1  98.44 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [53][160/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.2815e-02 (9.7453e-03)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [53][170/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.2337e-02 (9.9954e-03)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [53][180/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.6563e-03 (9.7090e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [53][190/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.7102e-03 (1.0023e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [53][200/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.8648e-03 (1.0008e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [53][210/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.1453e-03 (9.9107e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [53][220/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.3201e-03 (1.0102e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [53][230/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.7466e-03 (1.0150e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [53][240/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.6000e-03 (1.0247e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [53][250/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.2480e-02 (1.0326e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [53][260/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.2218e-02 (1.0308e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [53][270/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.1791e-02 (1.0222e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [53][280/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.1670e-02 (1.0218e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [53][290/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.9024e-03 (1.0068e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [53][300/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 9.6683e-03 (1.0025e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [53][310/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.4932e-03 (9.8831e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [53][320/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.4606e-02 (9.8520e-03)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [53][330/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.4598e-03 (9.8006e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [53][340/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.3594e-03 (9.7700e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [53][350/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.9216e-03 (9.7982e-03)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [53][360/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 9.6873e-03 (9.8015e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [53][370/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.2155e-03 (9.8306e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [53][380/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 9.2486e-03 (9.7850e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [53][390/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.6235e-03 (9.8010e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
## e[53] optimizer.zero_grad (sum) time: 0.17601275444030762
## e[53]       loss.backward (sum) time: 3.0058062076568604
## e[53]      optimizer.step (sum) time: 1.0342488288879395
## epoch[53] training(only) time: 15.163390398025513
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 2.2141e-01 (2.2141e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.023 ( 0.034)	Loss 3.9063e-01 (2.6823e-01)	Acc@1  91.00 ( 93.73)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.017 ( 0.026)	Loss 4.0664e-01 (2.8751e-01)	Acc@1  90.00 ( 93.52)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.017 ( 0.024)	Loss 2.5830e-01 (2.9496e-01)	Acc@1  93.00 ( 93.39)	Acc@5 100.00 ( 99.87)
Test: [ 40/100]	Time  0.017 ( 0.023)	Loss 2.1243e-01 (2.9625e-01)	Acc@1  93.00 ( 93.32)	Acc@5 100.00 ( 99.85)
Test: [ 50/100]	Time  0.016 ( 0.022)	Loss 8.2150e-02 (3.0190e-01)	Acc@1  98.00 ( 93.35)	Acc@5 100.00 ( 99.82)
Test: [ 60/100]	Time  0.024 ( 0.021)	Loss 3.2086e-01 (2.9540e-01)	Acc@1  91.00 ( 93.31)	Acc@5 100.00 ( 99.85)
Test: [ 70/100]	Time  0.023 ( 0.021)	Loss 6.1317e-01 (2.8879e-01)	Acc@1  90.00 ( 93.38)	Acc@5  99.00 ( 99.83)
Test: [ 80/100]	Time  0.016 ( 0.020)	Loss 1.7162e-01 (2.9071e-01)	Acc@1  96.00 ( 93.38)	Acc@5 100.00 ( 99.81)
Test: [ 90/100]	Time  0.019 ( 0.020)	Loss 1.6027e-01 (2.8819e-01)	Acc@1  95.00 ( 93.35)	Acc@5 100.00 ( 99.81)
 * Acc@1 93.380 Acc@5 99.810
### epoch[53] execution time: 17.329132556915283
EPOCH 54
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [54][  0/391]	Time  0.219 ( 0.219)	Data  0.182 ( 0.182)	Loss 2.0221e-02 (2.0221e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [54][ 10/391]	Time  0.035 ( 0.053)	Data  0.001 ( 0.018)	Loss 6.3529e-03 (1.2719e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [54][ 20/391]	Time  0.036 ( 0.046)	Data  0.001 ( 0.011)	Loss 5.3830e-03 (9.7249e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [54][ 30/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.008)	Loss 1.2937e-02 (1.1494e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [54][ 40/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.006)	Loss 1.8249e-02 (1.1492e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [54][ 50/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.006)	Loss 2.4065e-02 (1.1726e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [54][ 60/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.005)	Loss 7.1535e-03 (1.1433e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [54][ 70/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.005)	Loss 1.2475e-02 (1.1091e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [54][ 80/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.004)	Loss 7.5846e-03 (1.0772e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [54][ 90/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 4.3559e-03 (1.1173e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [54][100/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.004)	Loss 9.3080e-04 (1.0853e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [54][110/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.7057e-02 (1.1234e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [54][120/391]	Time  0.038 ( 0.039)	Data  0.003 ( 0.004)	Loss 2.6981e-03 (1.1090e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [54][130/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 3.5491e-03 (1.0818e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [54][140/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 9.5189e-03 (1.1171e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [54][150/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.7933e-03 (1.1300e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [54][160/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.0319e-02 (1.1563e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [54][170/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.0174e-03 (1.1382e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [54][180/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.4571e-02 (1.1590e-02)	Acc@1  98.44 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [54][190/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.0650e-03 (1.1553e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [54][200/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.7067e-03 (1.1457e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [54][210/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.6817e-02 (1.1660e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [54][220/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.2247e-03 (1.1571e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [54][230/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.4951e-03 (1.1469e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [54][240/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 9.2502e-03 (1.1260e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [54][250/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.7588e-02 (1.1180e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [54][260/391]	Time  0.037 ( 0.038)	Data  0.002 ( 0.003)	Loss 6.5523e-04 (1.1142e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [54][270/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.1782e-03 (1.1088e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [54][280/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.5780e-03 (1.1009e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [54][290/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.0747e-02 (1.1041e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [54][300/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.8647e-02 (1.1001e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [54][310/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.8953e-03 (1.1068e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [54][320/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.1770e-03 (1.1073e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [54][330/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.3607e-03 (1.1010e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [54][340/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.3764e-02 (1.0931e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [54][350/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.2234e-03 (1.0815e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [54][360/391]	Time  0.042 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.0137e-02 (1.0831e-02)	Acc@1  98.44 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [54][370/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.1075e-02 (1.0814e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [54][380/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.4695e-02 (1.0978e-02)	Acc@1  97.66 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [54][390/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.0565e-02 (1.0932e-02)	Acc@1  98.75 ( 99.68)	Acc@5 100.00 (100.00)
## e[54] optimizer.zero_grad (sum) time: 0.1748800277709961
## e[54]       loss.backward (sum) time: 2.9943604469299316
## e[54]      optimizer.step (sum) time: 1.0228850841522217
## epoch[54] training(only) time: 15.150415420532227
# Switched to evaluate mode...
Test: [  0/100]	Time  0.190 ( 0.190)	Loss 2.5371e-01 (2.5371e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.020 ( 0.034)	Loss 4.2892e-01 (2.7437e-01)	Acc@1  89.00 ( 93.00)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.019 ( 0.027)	Loss 4.4917e-01 (2.9841e-01)	Acc@1  89.00 ( 92.67)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.020 ( 0.024)	Loss 3.2072e-01 (3.0532e-01)	Acc@1  90.00 ( 92.77)	Acc@5  99.00 ( 99.77)
Test: [ 40/100]	Time  0.016 ( 0.023)	Loss 2.0147e-01 (3.0278e-01)	Acc@1  92.00 ( 92.85)	Acc@5 100.00 ( 99.76)
Test: [ 50/100]	Time  0.016 ( 0.022)	Loss 5.3507e-02 (3.0830e-01)	Acc@1  98.00 ( 92.82)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.020 ( 0.021)	Loss 3.5811e-01 (3.0297e-01)	Acc@1  92.00 ( 92.84)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.017 ( 0.021)	Loss 6.1506e-01 (2.9546e-01)	Acc@1  89.00 ( 92.96)	Acc@5  98.00 ( 99.73)
Test: [ 80/100]	Time  0.023 ( 0.021)	Loss 1.6924e-01 (2.9975e-01)	Acc@1  97.00 ( 92.98)	Acc@5 100.00 ( 99.73)
Test: [ 90/100]	Time  0.019 ( 0.021)	Loss 1.5164e-01 (2.9459e-01)	Acc@1  95.00 ( 93.09)	Acc@5 100.00 ( 99.74)
 * Acc@1 93.190 Acc@5 99.750
### epoch[54] execution time: 17.374124765396118
EPOCH 55
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [55][  0/391]	Time  0.221 ( 0.221)	Data  0.185 ( 0.185)	Loss 1.1464e-02 (1.1464e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [55][ 10/391]	Time  0.036 ( 0.054)	Data  0.001 ( 0.019)	Loss 1.2839e-02 (1.4244e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [55][ 20/391]	Time  0.037 ( 0.046)	Data  0.001 ( 0.011)	Loss 2.3681e-03 (1.1506e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [55][ 30/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.008)	Loss 1.0758e-02 (1.1305e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [55][ 40/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.007)	Loss 2.8681e-02 (1.0470e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [55][ 50/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.006)	Loss 5.8896e-03 (1.0340e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [55][ 60/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.005)	Loss 4.2217e-03 (9.8655e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [55][ 70/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.005)	Loss 5.8672e-03 (9.5961e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [55][ 80/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.5661e-02 (9.5153e-03)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [55][ 90/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 4.0339e-03 (9.4176e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [55][100/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.2871e-03 (9.3904e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [55][110/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 7.1522e-03 (9.3046e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [55][120/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 4.0847e-03 (9.2664e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [55][130/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.2214e-02 (9.3105e-03)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [55][140/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.0354e-02 (9.5772e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [55][150/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.5013e-03 (9.4158e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [55][160/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.1355e-03 (9.7296e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [55][170/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.1773e-03 (9.8187e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [55][180/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.5111e-02 (9.7866e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [55][190/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.1193e-03 (9.6457e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [55][200/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.1358e-02 (9.8844e-03)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [55][210/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.2196e-03 (9.7734e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [55][220/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.6492e-03 (9.8754e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [55][230/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.1622e-02 (9.9266e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [55][240/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.0967e-03 (1.0008e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [55][250/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.4507e-03 (9.9713e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [55][260/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.8167e-03 (1.0005e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [55][270/391]	Time  0.043 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.6866e-03 (9.9110e-03)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [55][280/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.0926e-02 (9.7927e-03)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [55][290/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.3767e-02 (9.6991e-03)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [55][300/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.6809e-02 (9.7683e-03)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [55][310/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.1364e-02 (9.7400e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [55][320/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.7826e-03 (9.8031e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [55][330/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.5922e-03 (9.7493e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [55][340/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.2583e-03 (9.6432e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [55][350/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.3482e-03 (9.5363e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [55][360/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.1002e-02 (9.5929e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [55][370/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 9.4208e-03 (9.5548e-03)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [55][380/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.2868e-02 (9.6402e-03)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [55][390/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.4275e-03 (9.6140e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
## e[55] optimizer.zero_grad (sum) time: 0.17530012130737305
## e[55]       loss.backward (sum) time: 3.0337610244750977
## e[55]      optimizer.step (sum) time: 1.0315783023834229
## epoch[55] training(only) time: 15.170231580734253
# Switched to evaluate mode...
Test: [  0/100]	Time  0.176 ( 0.176)	Loss 2.4565e-01 (2.4565e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.034)	Loss 3.9288e-01 (2.6961e-01)	Acc@1  91.00 ( 93.91)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.017 ( 0.027)	Loss 4.2114e-01 (2.9392e-01)	Acc@1  90.00 ( 93.57)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.017 ( 0.024)	Loss 2.9025e-01 (2.9877e-01)	Acc@1  92.00 ( 93.48)	Acc@5 100.00 ( 99.84)
Test: [ 40/100]	Time  0.023 ( 0.023)	Loss 2.4169e-01 (2.9729e-01)	Acc@1  92.00 ( 93.46)	Acc@5 100.00 ( 99.83)
Test: [ 50/100]	Time  0.016 ( 0.022)	Loss 7.1639e-02 (3.0734e-01)	Acc@1  98.00 ( 93.37)	Acc@5 100.00 ( 99.80)
Test: [ 60/100]	Time  0.016 ( 0.022)	Loss 3.3650e-01 (3.0111e-01)	Acc@1  91.00 ( 93.28)	Acc@5 100.00 ( 99.82)
Test: [ 70/100]	Time  0.017 ( 0.021)	Loss 6.3080e-01 (2.9387e-01)	Acc@1  89.00 ( 93.39)	Acc@5  99.00 ( 99.80)
Test: [ 80/100]	Time  0.021 ( 0.021)	Loss 1.5695e-01 (2.9552e-01)	Acc@1  95.00 ( 93.44)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.023 ( 0.021)	Loss 1.5222e-01 (2.9089e-01)	Acc@1  95.00 ( 93.38)	Acc@5 100.00 ( 99.80)
 * Acc@1 93.450 Acc@5 99.810
### epoch[55] execution time: 17.343075275421143
EPOCH 56
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [56][  0/391]	Time  0.211 ( 0.211)	Data  0.176 ( 0.176)	Loss 3.2046e-03 (3.2046e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [56][ 10/391]	Time  0.036 ( 0.053)	Data  0.001 ( 0.018)	Loss 1.7968e-03 (1.4973e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [56][ 20/391]	Time  0.037 ( 0.046)	Data  0.001 ( 0.010)	Loss 4.4753e-03 (1.4356e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [56][ 30/391]	Time  0.036 ( 0.043)	Data  0.001 ( 0.008)	Loss 1.2125e-02 (1.4062e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [56][ 40/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.006)	Loss 9.4111e-03 (1.4650e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [56][ 50/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.006)	Loss 1.0305e-02 (1.3282e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [56][ 60/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.005)	Loss 2.9352e-03 (1.2959e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [56][ 70/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.005)	Loss 7.0858e-03 (1.2083e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [56][ 80/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 3.0672e-03 (1.1348e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [56][ 90/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.004)	Loss 7.3249e-03 (1.1110e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [56][100/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.004)	Loss 4.2029e-03 (1.0639e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [56][110/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.1828e-02 (1.0817e-02)	Acc@1  98.44 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [56][120/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 3.2435e-03 (1.0561e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [56][130/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.2991e-02 (1.0733e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [56][140/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 4.2666e-03 (1.0516e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [56][150/391]	Time  0.048 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.1528e-03 (1.0222e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [56][160/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.3845e-04 (1.0197e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [56][170/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.6473e-02 (1.0136e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [56][180/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.7208e-03 (1.0084e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [56][190/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.0173e-02 (9.9922e-03)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [56][200/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.1449e-02 (9.8951e-03)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [56][210/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.7622e-02 (1.0173e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [56][220/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 9.6563e-03 (1.0011e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [56][230/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.9950e-03 (9.8601e-03)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [56][240/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.6671e-03 (9.7631e-03)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [56][250/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.2436e-03 (9.6245e-03)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [56][260/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.7024e-03 (9.5359e-03)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [56][270/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.0065e-03 (9.4804e-03)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [56][280/391]	Time  0.040 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.5485e-03 (9.4814e-03)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [56][290/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.4817e-02 (9.4597e-03)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [56][300/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.3626e-03 (9.4386e-03)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [56][310/391]	Time  0.039 ( 0.039)	Data  0.002 ( 0.003)	Loss 1.6000e-03 (9.3008e-03)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [56][320/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 9.3810e-03 (9.2445e-03)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [56][330/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.2733e-02 (9.2435e-03)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [56][340/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.7022e-03 (9.4025e-03)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [56][350/391]	Time  0.038 ( 0.038)	Data  0.002 ( 0.003)	Loss 3.4065e-03 (9.2776e-03)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [56][360/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.9576e-03 (9.3420e-03)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [56][370/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.2114e-02 (9.4555e-03)	Acc@1  98.44 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [56][380/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.5833e-03 (9.4416e-03)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [56][390/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.7020e-03 (9.4091e-03)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
## e[56] optimizer.zero_grad (sum) time: 0.17766451835632324
## e[56]       loss.backward (sum) time: 2.985323667526245
## e[56]      optimizer.step (sum) time: 1.0234661102294922
## epoch[56] training(only) time: 15.20250916481018
# Switched to evaluate mode...
Test: [  0/100]	Time  0.180 ( 0.180)	Loss 2.4092e-01 (2.4092e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.021 ( 0.034)	Loss 3.8568e-01 (2.6901e-01)	Acc@1  92.00 ( 93.55)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.016 ( 0.026)	Loss 4.1212e-01 (2.9056e-01)	Acc@1  90.00 ( 93.43)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.017 ( 0.024)	Loss 3.0828e-01 (3.0098e-01)	Acc@1  92.00 ( 93.29)	Acc@5  99.00 ( 99.81)
Test: [ 40/100]	Time  0.019 ( 0.023)	Loss 2.6962e-01 (3.0034e-01)	Acc@1  91.00 ( 93.22)	Acc@5  99.00 ( 99.76)
Test: [ 50/100]	Time  0.016 ( 0.022)	Loss 1.0893e-01 (3.0724e-01)	Acc@1  97.00 ( 93.20)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.017 ( 0.021)	Loss 3.2354e-01 (2.9957e-01)	Acc@1  91.00 ( 93.16)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.016 ( 0.021)	Loss 5.8223e-01 (2.9158e-01)	Acc@1  89.00 ( 93.27)	Acc@5 100.00 ( 99.80)
Test: [ 80/100]	Time  0.018 ( 0.020)	Loss 2.1562e-01 (2.9433e-01)	Acc@1  95.00 ( 93.30)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.018 ( 0.020)	Loss 1.9902e-01 (2.9171e-01)	Acc@1  94.00 ( 93.23)	Acc@5 100.00 ( 99.80)
 * Acc@1 93.310 Acc@5 99.820
### epoch[56] execution time: 17.3320574760437
EPOCH 57
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [57][  0/391]	Time  0.216 ( 0.216)	Data  0.180 ( 0.180)	Loss 8.9259e-03 (8.9259e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [57][ 10/391]	Time  0.038 ( 0.054)	Data  0.001 ( 0.018)	Loss 2.2094e-02 (1.5829e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [57][ 20/391]	Time  0.036 ( 0.046)	Data  0.001 ( 0.010)	Loss 1.1051e-03 (1.1412e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [57][ 30/391]	Time  0.036 ( 0.044)	Data  0.001 ( 0.008)	Loss 7.5353e-03 (1.0463e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [57][ 40/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.006)	Loss 1.3969e-02 (9.3938e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [57][ 50/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.006)	Loss 5.2885e-03 (9.8189e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [57][ 60/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.005)	Loss 1.2363e-02 (9.5589e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [57][ 70/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.005)	Loss 1.5485e-02 (9.6236e-03)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [57][ 80/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.004)	Loss 3.3502e-03 (9.3102e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [57][ 90/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.7612e-02 (9.5856e-03)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [57][100/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.1214e-02 (9.6794e-03)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [57][110/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 6.1499e-03 (9.4698e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [57][120/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 3.6762e-03 (9.5620e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [57][130/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 9.6853e-03 (9.4034e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [57][140/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.3632e-02 (9.5478e-03)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [57][150/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.1477e-03 (9.8069e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [57][160/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.3329e-02 (9.9240e-03)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [57][170/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.2193e-02 (9.7743e-03)	Acc@1  98.44 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [57][180/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.7272e-03 (9.5400e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [57][190/391]	Time  0.041 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.3238e-03 (9.5630e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [57][200/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.0924e-03 (9.6921e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [57][210/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.2590e-03 (9.6161e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [57][220/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.8897e-03 (9.5961e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [57][230/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.6524e-03 (9.5366e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [57][240/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.5726e-03 (9.5692e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [57][250/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.0915e-03 (9.5574e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [57][260/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.1162e-03 (9.4554e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [57][270/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.5364e-02 (9.5487e-03)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [57][280/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.9364e-04 (9.6606e-03)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [57][290/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.8925e-03 (9.5968e-03)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [57][300/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.0232e-03 (9.5988e-03)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [57][310/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.0880e-02 (9.5424e-03)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [57][320/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.0997e-02 (9.5072e-03)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [57][330/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.4210e-03 (9.5111e-03)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [57][340/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.2987e-03 (9.4732e-03)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [57][350/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.1079e-03 (9.3812e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [57][360/391]	Time  0.045 ( 0.038)	Data  0.002 ( 0.003)	Loss 5.1338e-03 (9.4153e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [57][370/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.5635e-03 (9.3216e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [57][380/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.6451e-03 (9.2947e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [57][390/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.3713e-02 (9.2400e-03)	Acc@1  98.75 ( 99.75)	Acc@5 100.00 (100.00)
## e[57] optimizer.zero_grad (sum) time: 0.17751431465148926
## e[57]       loss.backward (sum) time: 2.9554967880249023
## e[57]      optimizer.step (sum) time: 1.0355300903320312
## epoch[57] training(only) time: 15.109673023223877
# Switched to evaluate mode...
Test: [  0/100]	Time  0.171 ( 0.171)	Loss 2.4160e-01 (2.4160e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.023 ( 0.033)	Loss 4.1619e-01 (2.7829e-01)	Acc@1  89.00 ( 92.64)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.020 ( 0.027)	Loss 3.7611e-01 (2.9636e-01)	Acc@1  92.00 ( 92.81)	Acc@5 100.00 ( 99.90)
Test: [ 30/100]	Time  0.016 ( 0.024)	Loss 3.1989e-01 (2.9871e-01)	Acc@1  92.00 ( 93.13)	Acc@5  99.00 ( 99.81)
Test: [ 40/100]	Time  0.016 ( 0.022)	Loss 2.3828e-01 (2.9986e-01)	Acc@1  94.00 ( 93.15)	Acc@5 100.00 ( 99.80)
Test: [ 50/100]	Time  0.016 ( 0.022)	Loss 9.8843e-02 (3.0586e-01)	Acc@1  95.00 ( 93.02)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.022 ( 0.021)	Loss 3.0941e-01 (2.9969e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 ( 99.82)
Test: [ 70/100]	Time  0.018 ( 0.021)	Loss 6.5446e-01 (2.9195e-01)	Acc@1  88.00 ( 93.08)	Acc@5  99.00 ( 99.80)
Test: [ 80/100]	Time  0.023 ( 0.021)	Loss 1.8890e-01 (2.9428e-01)	Acc@1  93.00 ( 93.12)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.017 ( 0.021)	Loss 1.4841e-01 (2.9166e-01)	Acc@1  94.00 ( 93.11)	Acc@5 100.00 ( 99.81)
 * Acc@1 93.180 Acc@5 99.830
### epoch[57] execution time: 17.26972460746765
EPOCH 58
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [58][  0/391]	Time  0.212 ( 0.212)	Data  0.178 ( 0.178)	Loss 6.2481e-03 (6.2481e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [58][ 10/391]	Time  0.037 ( 0.053)	Data  0.001 ( 0.018)	Loss 8.1620e-03 (7.8393e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [58][ 20/391]	Time  0.038 ( 0.046)	Data  0.001 ( 0.010)	Loss 8.0127e-04 (6.9080e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [58][ 30/391]	Time  0.037 ( 0.043)	Data  0.001 ( 0.008)	Loss 9.3345e-04 (7.4008e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [58][ 40/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.006)	Loss 3.5758e-03 (6.9390e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [58][ 50/391]	Time  0.037 ( 0.041)	Data  0.002 ( 0.006)	Loss 6.1651e-02 (9.4447e-03)	Acc@1  98.44 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [58][ 60/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.005)	Loss 2.7343e-03 (9.0006e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [58][ 70/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.005)	Loss 3.9055e-03 (8.8079e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [58][ 80/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 6.8362e-03 (8.6558e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [58][ 90/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.004)	Loss 3.3468e-03 (8.3799e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [58][100/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.0546e-02 (8.3825e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [58][110/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.004)	Loss 6.3026e-03 (8.5748e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [58][120/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 9.4471e-03 (8.5803e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [58][130/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.9342e-03 (9.1362e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [58][140/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.8916e-03 (9.2217e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [58][150/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.0125e-02 (9.2710e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [58][160/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.5785e-03 (9.3665e-03)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [58][170/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.6716e-02 (9.6584e-03)	Acc@1  97.66 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [58][180/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.8703e-03 (9.4408e-03)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [58][190/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.5947e-03 (9.3656e-03)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [58][200/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.2642e-02 (9.3528e-03)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [58][210/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.3132e-03 (9.3869e-03)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [58][220/391]	Time  0.040 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.4456e-02 (9.5039e-03)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [58][230/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.9337e-03 (9.5483e-03)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [58][240/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.7248e-03 (9.3560e-03)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [58][250/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.1870e-02 (9.2746e-03)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [58][260/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.5754e-03 (9.1491e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [58][270/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.5888e-03 (9.0883e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [58][280/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.8200e-03 (8.9684e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [58][290/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.6650e-03 (8.9211e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [58][300/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.0005e-03 (8.9662e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [58][310/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.9374e-03 (8.8360e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [58][320/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.1928e-03 (8.8305e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [58][330/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.1203e-03 (8.7655e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [58][340/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 9.4528e-03 (8.8133e-03)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [58][350/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.4751e-03 (8.8978e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [58][360/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.5441e-02 (8.9028e-03)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [58][370/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.7879e-02 (8.9132e-03)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [58][380/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.1357e-03 (8.8755e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [58][390/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.3571e-03 (8.7670e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
## e[58] optimizer.zero_grad (sum) time: 0.1763312816619873
## e[58]       loss.backward (sum) time: 2.975834369659424
## e[58]      optimizer.step (sum) time: 1.0359172821044922
## epoch[58] training(only) time: 15.116335153579712
# Switched to evaluate mode...
Test: [  0/100]	Time  0.182 ( 0.182)	Loss 2.3985e-01 (2.3985e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.016 ( 0.034)	Loss 4.4841e-01 (2.8064e-01)	Acc@1  89.00 ( 93.36)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.020 ( 0.027)	Loss 4.1678e-01 (2.9075e-01)	Acc@1  90.00 ( 93.05)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.016 ( 0.024)	Loss 3.2709e-01 (2.9970e-01)	Acc@1  91.00 ( 93.00)	Acc@5 100.00 ( 99.87)
Test: [ 40/100]	Time  0.021 ( 0.022)	Loss 2.4200e-01 (3.0351e-01)	Acc@1  93.00 ( 93.07)	Acc@5 100.00 ( 99.83)
Test: [ 50/100]	Time  0.023 ( 0.022)	Loss 1.0106e-01 (3.0894e-01)	Acc@1  97.00 ( 93.10)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.020 ( 0.022)	Loss 2.9084e-01 (3.0123e-01)	Acc@1  93.00 ( 93.10)	Acc@5 100.00 ( 99.82)
Test: [ 70/100]	Time  0.018 ( 0.021)	Loss 6.5487e-01 (2.9227e-01)	Acc@1  88.00 ( 93.17)	Acc@5  99.00 ( 99.82)
Test: [ 80/100]	Time  0.024 ( 0.021)	Loss 1.7774e-01 (2.9361e-01)	Acc@1  96.00 ( 93.27)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.021 ( 0.021)	Loss 1.6861e-01 (2.9010e-01)	Acc@1  95.00 ( 93.24)	Acc@5 100.00 ( 99.81)
 * Acc@1 93.300 Acc@5 99.810
### epoch[58] execution time: 17.303155660629272
EPOCH 59
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [59][  0/391]	Time  0.214 ( 0.214)	Data  0.179 ( 0.179)	Loss 6.4555e-03 (6.4555e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [59][ 10/391]	Time  0.037 ( 0.053)	Data  0.001 ( 0.018)	Loss 1.2701e-02 (6.5312e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [59][ 20/391]	Time  0.038 ( 0.046)	Data  0.001 ( 0.010)	Loss 7.8689e-03 (8.6235e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [59][ 30/391]	Time  0.037 ( 0.043)	Data  0.001 ( 0.008)	Loss 7.6356e-03 (8.6000e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [59][ 40/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.006)	Loss 3.4505e-03 (7.9983e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [59][ 50/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.006)	Loss 1.4083e-03 (8.7862e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [59][ 60/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.005)	Loss 5.8356e-04 (8.2913e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [59][ 70/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.005)	Loss 1.0412e-03 (8.8542e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [59][ 80/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.004)	Loss 7.2826e-03 (8.7772e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [59][ 90/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 3.0248e-03 (8.4461e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [59][100/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 5.1783e-03 (7.9907e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [59][110/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.7677e-02 (8.2196e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [59][120/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.004)	Loss 3.9397e-03 (8.2918e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [59][130/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.1948e-03 (8.3027e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [59][140/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.7529e-03 (8.3045e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [59][150/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.9895e-02 (8.3731e-03)	Acc@1  98.44 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [59][160/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.9705e-03 (8.2994e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [59][170/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.1033e-02 (8.4299e-03)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [59][180/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.7203e-03 (8.3119e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [59][190/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.7072e-03 (8.3055e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [59][200/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.0356e-03 (8.2500e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [59][210/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.3215e-03 (8.0529e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [59][220/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.0109e-02 (8.2301e-03)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [59][230/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.6722e-03 (8.2431e-03)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [59][240/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.9703e-03 (8.4429e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [59][250/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.6787e-03 (8.5469e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [59][260/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.1351e-03 (8.5283e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [59][270/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.7674e-03 (8.5609e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [59][280/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.5645e-03 (8.4057e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [59][290/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.3684e-03 (8.3203e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [59][300/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.8788e-02 (8.4368e-03)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [59][310/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.0625e-02 (8.4457e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [59][320/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.7931e-03 (8.3690e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [59][330/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.8204e-03 (8.3266e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [59][340/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.7759e-03 (8.3221e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [59][350/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.1747e-03 (8.4068e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [59][360/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.8421e-03 (8.3355e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [59][370/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.2881e-03 (8.3529e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [59][380/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.6560e-03 (8.3315e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [59][390/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.0734e-02 (8.3964e-03)	Acc@1  98.75 ( 99.77)	Acc@5 100.00 (100.00)
## e[59] optimizer.zero_grad (sum) time: 0.17567873001098633
## e[59]       loss.backward (sum) time: 2.9835102558135986
## e[59]      optimizer.step (sum) time: 1.0217788219451904
## epoch[59] training(only) time: 15.084923028945923
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 2.4816e-01 (2.4816e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.017 ( 0.034)	Loss 4.5845e-01 (2.9725e-01)	Acc@1  89.00 ( 93.36)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.023 ( 0.027)	Loss 4.5792e-01 (3.1080e-01)	Acc@1  89.00 ( 93.00)	Acc@5 100.00 ( 99.90)
Test: [ 30/100]	Time  0.021 ( 0.024)	Loss 3.4036e-01 (3.1026e-01)	Acc@1  92.00 ( 93.03)	Acc@5 100.00 ( 99.90)
Test: [ 40/100]	Time  0.019 ( 0.023)	Loss 2.3138e-01 (3.0943e-01)	Acc@1  94.00 ( 93.17)	Acc@5 100.00 ( 99.85)
Test: [ 50/100]	Time  0.016 ( 0.022)	Loss 9.2661e-02 (3.1511e-01)	Acc@1  97.00 ( 93.18)	Acc@5 100.00 ( 99.82)
Test: [ 60/100]	Time  0.021 ( 0.021)	Loss 3.0503e-01 (3.0754e-01)	Acc@1  93.00 ( 93.18)	Acc@5 100.00 ( 99.82)
Test: [ 70/100]	Time  0.017 ( 0.021)	Loss 6.3843e-01 (3.0086e-01)	Acc@1  90.00 ( 93.25)	Acc@5  98.00 ( 99.79)
Test: [ 80/100]	Time  0.016 ( 0.021)	Loss 2.3011e-01 (3.0526e-01)	Acc@1  94.00 ( 93.20)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.016 ( 0.020)	Loss 2.0937e-01 (3.0128e-01)	Acc@1  94.00 ( 93.19)	Acc@5 100.00 ( 99.79)
 * Acc@1 93.210 Acc@5 99.790
### epoch[59] execution time: 17.245775938034058
EPOCH 60
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [60][  0/391]	Time  0.211 ( 0.211)	Data  0.172 ( 0.172)	Loss 1.1397e-02 (1.1397e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [60][ 10/391]	Time  0.035 ( 0.054)	Data  0.001 ( 0.017)	Loss 1.1937e-03 (5.6838e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [60][ 20/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.010)	Loss 4.3809e-03 (5.9060e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [60][ 30/391]	Time  0.036 ( 0.043)	Data  0.001 ( 0.008)	Loss 4.2213e-03 (7.3927e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [60][ 40/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.006)	Loss 6.4902e-03 (7.0837e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [60][ 50/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.006)	Loss 1.9927e-02 (7.4325e-03)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [60][ 60/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.005)	Loss 2.0764e-03 (7.6899e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [60][ 70/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.005)	Loss 8.3983e-04 (7.8336e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [60][ 80/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.9442e-03 (7.7358e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [60][ 90/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 3.7543e-03 (7.5200e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [60][100/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.2303e-02 (7.5598e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [60][110/391]	Time  0.036 ( 0.039)	Data  0.003 ( 0.004)	Loss 6.9580e-03 (7.5640e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [60][120/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.2159e-02 (7.6357e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [60][130/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.9511e-03 (7.5573e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [60][140/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.7824e-03 (7.4244e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [60][150/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.3329e-03 (7.4090e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [60][160/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.2168e-02 (7.7318e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [60][170/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.9824e-02 (7.6670e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [60][180/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.2225e-02 (7.9394e-03)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [60][190/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.3966e-03 (8.1349e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [60][200/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.2215e-02 (8.0638e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [60][210/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.2743e-03 (8.0012e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [60][220/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.8941e-03 (8.0343e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [60][230/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.5484e-03 (7.9388e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [60][240/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.6364e-03 (7.7492e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [60][250/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.0016e-03 (7.6314e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [60][260/391]	Time  0.043 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.2480e-02 (7.6290e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [60][270/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.3563e-03 (7.6725e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [60][280/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.2015e-03 (7.5756e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [60][290/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.4169e-03 (7.6345e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [60][300/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.3072e-02 (7.5946e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [60][310/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.6942e-03 (7.5173e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [60][320/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.2885e-02 (7.6630e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [60][330/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.7653e-03 (7.6554e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [60][340/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.7335e-03 (7.6322e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [60][350/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.8147e-03 (7.6645e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [60][360/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.1852e-03 (7.5749e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [60][370/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.4272e-03 (7.5519e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [60][380/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.9069e-03 (7.5446e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [60][390/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.0178e-02 (7.5767e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
## e[60] optimizer.zero_grad (sum) time: 0.17508506774902344
## e[60]       loss.backward (sum) time: 2.9910309314727783
## e[60]      optimizer.step (sum) time: 1.0470778942108154
## epoch[60] training(only) time: 15.12984824180603
# Switched to evaluate mode...
Test: [  0/100]	Time  0.177 ( 0.177)	Loss 2.3838e-01 (2.3838e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.023 ( 0.033)	Loss 4.4839e-01 (2.9302e-01)	Acc@1  89.00 ( 93.09)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.020 ( 0.026)	Loss 4.2960e-01 (3.0759e-01)	Acc@1  91.00 ( 93.24)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.017 ( 0.024)	Loss 3.0285e-01 (3.0695e-01)	Acc@1  93.00 ( 93.29)	Acc@5  99.00 ( 99.81)
Test: [ 40/100]	Time  0.020 ( 0.023)	Loss 2.3582e-01 (3.0607e-01)	Acc@1  94.00 ( 93.37)	Acc@5 100.00 ( 99.78)
Test: [ 50/100]	Time  0.020 ( 0.022)	Loss 1.0362e-01 (3.1279e-01)	Acc@1  97.00 ( 93.33)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.017 ( 0.022)	Loss 3.1578e-01 (3.0525e-01)	Acc@1  93.00 ( 93.33)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.019 ( 0.021)	Loss 6.0988e-01 (2.9763e-01)	Acc@1  90.00 ( 93.38)	Acc@5  98.00 ( 99.77)
Test: [ 80/100]	Time  0.016 ( 0.021)	Loss 2.0466e-01 (3.0033e-01)	Acc@1  95.00 ( 93.37)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.026 ( 0.021)	Loss 1.9417e-01 (2.9562e-01)	Acc@1  95.00 ( 93.41)	Acc@5 100.00 ( 99.79)
 * Acc@1 93.410 Acc@5 99.790
### epoch[60] execution time: 17.319945096969604
EPOCH 61
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [61][  0/391]	Time  0.220 ( 0.220)	Data  0.187 ( 0.187)	Loss 3.1258e-03 (3.1258e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [61][ 10/391]	Time  0.037 ( 0.054)	Data  0.001 ( 0.019)	Loss 1.0572e-02 (6.9958e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [61][ 20/391]	Time  0.037 ( 0.046)	Data  0.001 ( 0.011)	Loss 2.8228e-03 (9.5226e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [61][ 30/391]	Time  0.036 ( 0.044)	Data  0.001 ( 0.008)	Loss 7.8981e-03 (9.2452e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [61][ 40/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.007)	Loss 1.7945e-02 (8.4376e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [61][ 50/391]	Time  0.035 ( 0.041)	Data  0.001 ( 0.006)	Loss 4.4209e-03 (7.7711e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [61][ 60/391]	Time  0.035 ( 0.041)	Data  0.001 ( 0.005)	Loss 2.4316e-03 (7.5820e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [61][ 70/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.005)	Loss 6.1987e-03 (7.0177e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [61][ 80/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.005)	Loss 3.5716e-02 (7.0874e-03)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [61][ 90/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 7.4325e-03 (7.1900e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [61][100/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.3482e-02 (7.2401e-03)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [61][110/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 6.4149e-03 (7.5977e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [61][120/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.1358e-03 (8.1453e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [61][130/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.3884e-03 (8.0251e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [61][140/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.0449e-02 (7.8526e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [61][150/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.0626e-03 (7.6153e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [61][160/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.8748e-03 (7.6051e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [61][170/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.5925e-03 (7.5564e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [61][180/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.0380e-02 (7.4136e-03)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [61][190/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.2831e-03 (7.3823e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [61][200/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.6182e-03 (7.3718e-03)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [61][210/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.9770e-03 (7.3499e-03)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [61][220/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.4519e-03 (7.2018e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [61][230/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.3839e-02 (7.2256e-03)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [61][240/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.7048e-02 (7.1980e-03)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [61][250/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.5565e-03 (7.0912e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [61][260/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.1488e-03 (7.0822e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [61][270/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.7264e-03 (6.9648e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [61][280/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.0989e-02 (6.9664e-03)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [61][290/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.3932e-03 (6.9658e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [61][300/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.5305e-03 (6.9500e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [61][310/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.0194e-03 (6.9450e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [61][320/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.8149e-03 (7.0151e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [61][330/391]	Time  0.036 ( 0.038)	Data  0.002 ( 0.003)	Loss 2.2752e-03 (6.9065e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [61][340/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.5336e-03 (6.8581e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [61][350/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.9898e-03 (6.8323e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [61][360/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.1603e-02 (7.0173e-03)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [61][370/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.8440e-03 (7.0221e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [61][380/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.5368e-03 (6.9888e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [61][390/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.7491e-02 (6.9698e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
## e[61] optimizer.zero_grad (sum) time: 0.17571449279785156
## e[61]       loss.backward (sum) time: 2.985469102859497
## e[61]      optimizer.step (sum) time: 1.019665241241455
## epoch[61] training(only) time: 15.123401880264282
# Switched to evaluate mode...
Test: [  0/100]	Time  0.171 ( 0.171)	Loss 2.4982e-01 (2.4982e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.017 ( 0.033)	Loss 4.0936e-01 (2.8464e-01)	Acc@1  90.00 ( 93.27)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.016 ( 0.026)	Loss 3.9224e-01 (2.9890e-01)	Acc@1  91.00 ( 93.29)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.024 ( 0.023)	Loss 3.0225e-01 (2.9729e-01)	Acc@1  92.00 ( 93.29)	Acc@5 100.00 ( 99.87)
Test: [ 40/100]	Time  0.019 ( 0.023)	Loss 2.4444e-01 (2.9741e-01)	Acc@1  93.00 ( 93.39)	Acc@5 100.00 ( 99.83)
Test: [ 50/100]	Time  0.019 ( 0.022)	Loss 9.6793e-02 (3.0283e-01)	Acc@1  97.00 ( 93.41)	Acc@5 100.00 ( 99.82)
Test: [ 60/100]	Time  0.016 ( 0.021)	Loss 3.3657e-01 (2.9686e-01)	Acc@1  91.00 ( 93.33)	Acc@5 100.00 ( 99.85)
Test: [ 70/100]	Time  0.024 ( 0.021)	Loss 6.1673e-01 (2.8998e-01)	Acc@1  90.00 ( 93.34)	Acc@5  99.00 ( 99.83)
Test: [ 80/100]	Time  0.016 ( 0.020)	Loss 2.1681e-01 (2.9315e-01)	Acc@1  94.00 ( 93.35)	Acc@5 100.00 ( 99.81)
Test: [ 90/100]	Time  0.021 ( 0.020)	Loss 1.9725e-01 (2.8895e-01)	Acc@1  95.00 ( 93.37)	Acc@5 100.00 ( 99.84)
 * Acc@1 93.440 Acc@5 99.830
### epoch[61] execution time: 17.24686884880066
EPOCH 62
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [62][  0/391]	Time  0.219 ( 0.219)	Data  0.183 ( 0.183)	Loss 3.0709e-03 (3.0709e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [62][ 10/391]	Time  0.038 ( 0.054)	Data  0.001 ( 0.018)	Loss 5.4951e-03 (4.0323e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [62][ 20/391]	Time  0.037 ( 0.046)	Data  0.001 ( 0.011)	Loss 3.8446e-03 (3.8923e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [62][ 30/391]	Time  0.036 ( 0.044)	Data  0.001 ( 0.008)	Loss 3.6979e-03 (4.8629e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [62][ 40/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.006)	Loss 2.1368e-02 (5.4272e-03)	Acc@1  99.22 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [62][ 50/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.006)	Loss 7.3476e-03 (5.3708e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [62][ 60/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.005)	Loss 7.4177e-03 (5.2840e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [62][ 70/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.005)	Loss 1.2389e-02 (5.5705e-03)	Acc@1  99.22 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [62][ 80/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.8064e-03 (5.8439e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [62][ 90/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 9.0115e-03 (6.2316e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [62][100/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.7654e-03 (6.5632e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [62][110/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 6.3821e-03 (6.6740e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [62][120/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.5869e-03 (6.9124e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [62][130/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 8.4992e-04 (6.7744e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [62][140/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.0658e-02 (6.7931e-03)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [62][150/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.0345e-04 (6.8425e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [62][160/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.3410e-03 (6.7066e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [62][170/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.8595e-03 (6.6350e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [62][180/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.9872e-03 (6.5914e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [62][190/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.2682e-02 (6.5068e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [62][200/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.7657e-03 (6.5704e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [62][210/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.4764e-02 (6.6008e-03)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [62][220/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.1856e-03 (6.5145e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [62][230/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.1876e-03 (6.4967e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [62][240/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.5365e-03 (6.5044e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [62][250/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.9829e-03 (6.4364e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [62][260/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.0858e-02 (6.4509e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [62][270/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.9076e-03 (6.4770e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [62][280/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.2463e-03 (6.4718e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [62][290/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.3106e-03 (6.3779e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [62][300/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.6367e-03 (6.3544e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [62][310/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.8279e-03 (6.4424e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [62][320/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.1049e-02 (6.4814e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [62][330/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.2141e-03 (6.4405e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [62][340/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.2171e-03 (6.4614e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [62][350/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.5449e-03 (6.4883e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [62][360/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.5504e-03 (6.4357e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [62][370/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.7809e-03 (6.5193e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [62][380/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.4371e-03 (6.4608e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [62][390/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.1656e-03 (6.4423e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
## e[62] optimizer.zero_grad (sum) time: 0.17688274383544922
## e[62]       loss.backward (sum) time: 2.95194411277771
## e[62]      optimizer.step (sum) time: 1.0309944152832031
## epoch[62] training(only) time: 15.103825807571411
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 2.5330e-01 (2.5330e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.016 ( 0.033)	Loss 4.1837e-01 (2.8495e-01)	Acc@1  90.00 ( 93.55)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.018 ( 0.025)	Loss 4.0471e-01 (3.0171e-01)	Acc@1  91.00 ( 93.33)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.020 ( 0.023)	Loss 2.8794e-01 (3.0170e-01)	Acc@1  93.00 ( 93.39)	Acc@5  99.00 ( 99.81)
Test: [ 40/100]	Time  0.017 ( 0.023)	Loss 2.4009e-01 (3.0188e-01)	Acc@1  93.00 ( 93.46)	Acc@5 100.00 ( 99.78)
Test: [ 50/100]	Time  0.017 ( 0.022)	Loss 9.2818e-02 (3.0779e-01)	Acc@1  97.00 ( 93.43)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.024 ( 0.022)	Loss 3.2050e-01 (3.0106e-01)	Acc@1  92.00 ( 93.41)	Acc@5 100.00 ( 99.82)
Test: [ 70/100]	Time  0.024 ( 0.021)	Loss 6.0013e-01 (2.9326e-01)	Acc@1  91.00 ( 93.46)	Acc@5  98.00 ( 99.79)
Test: [ 80/100]	Time  0.016 ( 0.021)	Loss 2.0072e-01 (2.9577e-01)	Acc@1  94.00 ( 93.44)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.016 ( 0.021)	Loss 1.8804e-01 (2.9141e-01)	Acc@1  95.00 ( 93.46)	Acc@5 100.00 ( 99.79)
 * Acc@1 93.510 Acc@5 99.790
### epoch[62] execution time: 17.253828048706055
EPOCH 63
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [63][  0/391]	Time  0.216 ( 0.216)	Data  0.180 ( 0.180)	Loss 1.5664e-02 (1.5664e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [63][ 10/391]	Time  0.036 ( 0.053)	Data  0.001 ( 0.018)	Loss 5.3913e-03 (9.2440e-03)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [63][ 20/391]	Time  0.038 ( 0.046)	Data  0.001 ( 0.011)	Loss 1.5334e-03 (8.0249e-03)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [63][ 30/391]	Time  0.037 ( 0.043)	Data  0.001 ( 0.008)	Loss 5.2874e-03 (7.4024e-03)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [63][ 40/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.007)	Loss 5.5036e-03 (6.6834e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [63][ 50/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.006)	Loss 1.5491e-02 (7.1155e-03)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [63][ 60/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.005)	Loss 6.2584e-03 (7.1227e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [63][ 70/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.005)	Loss 1.1491e-03 (6.9015e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [63][ 80/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 3.4214e-03 (6.6978e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [63][ 90/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.004)	Loss 7.5084e-03 (6.5061e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [63][100/391]	Time  0.037 ( 0.039)	Data  0.003 ( 0.004)	Loss 3.8134e-03 (6.5866e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [63][110/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.0174e-02 (6.7493e-03)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [63][120/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 6.7625e-03 (7.0105e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [63][130/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.4627e-03 (7.1969e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [63][140/391]	Time  0.045 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.3609e-02 (7.4743e-03)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [63][150/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.7286e-03 (7.3267e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [63][160/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.2957e-02 (7.4672e-03)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [63][170/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.7658e-03 (7.3975e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [63][180/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.4461e-03 (7.3784e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [63][190/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.1880e-03 (7.2377e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [63][200/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.5732e-03 (7.2832e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [63][210/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.9551e-03 (7.3267e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [63][220/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.6550e-03 (7.2186e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [63][230/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.5893e-03 (7.1120e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [63][240/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.1922e-03 (7.2968e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [63][250/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.8187e-03 (7.4338e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [63][260/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.3017e-03 (7.3383e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [63][270/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.7456e-03 (7.2923e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [63][280/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.0575e-03 (7.2565e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [63][290/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.7809e-03 (7.1689e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [63][300/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.9820e-03 (7.0436e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [63][310/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.2859e-03 (7.0046e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [63][320/391]	Time  0.037 ( 0.038)	Data  0.002 ( 0.003)	Loss 2.9066e-03 (6.9729e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [63][330/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.5646e-03 (7.0398e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [63][340/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.6839e-03 (7.0305e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [63][350/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.0859e-03 (7.1350e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [63][360/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.1500e-03 (7.0589e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [63][370/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.7962e-03 (6.9956e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [63][380/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.6519e-03 (6.9528e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [63][390/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.7049e-02 (6.9404e-03)	Acc@1  98.75 ( 99.82)	Acc@5 100.00 (100.00)
## e[63] optimizer.zero_grad (sum) time: 0.17502689361572266
## e[63]       loss.backward (sum) time: 3.0071027278900146
## e[63]      optimizer.step (sum) time: 1.0244076251983643
## epoch[63] training(only) time: 15.128906965255737
# Switched to evaluate mode...
Test: [  0/100]	Time  0.170 ( 0.170)	Loss 2.6265e-01 (2.6265e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.016 ( 0.032)	Loss 3.9249e-01 (2.8380e-01)	Acc@1  90.00 ( 93.45)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.019 ( 0.026)	Loss 3.9647e-01 (3.0109e-01)	Acc@1  89.00 ( 93.19)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.021 ( 0.024)	Loss 3.0625e-01 (3.0392e-01)	Acc@1  91.00 ( 93.16)	Acc@5 100.00 ( 99.84)
Test: [ 40/100]	Time  0.023 ( 0.023)	Loss 2.4206e-01 (3.0348e-01)	Acc@1  93.00 ( 93.24)	Acc@5 100.00 ( 99.83)
Test: [ 50/100]	Time  0.016 ( 0.022)	Loss 1.0605e-01 (3.0798e-01)	Acc@1  97.00 ( 93.33)	Acc@5 100.00 ( 99.82)
Test: [ 60/100]	Time  0.016 ( 0.021)	Loss 2.9613e-01 (3.0070e-01)	Acc@1  92.00 ( 93.26)	Acc@5 100.00 ( 99.85)
Test: [ 70/100]	Time  0.016 ( 0.021)	Loss 5.8998e-01 (2.9290e-01)	Acc@1  90.00 ( 93.30)	Acc@5  99.00 ( 99.83)
Test: [ 80/100]	Time  0.019 ( 0.021)	Loss 2.2017e-01 (2.9649e-01)	Acc@1  94.00 ( 93.26)	Acc@5 100.00 ( 99.81)
Test: [ 90/100]	Time  0.021 ( 0.021)	Loss 2.1461e-01 (2.9210e-01)	Acc@1  94.00 ( 93.26)	Acc@5 100.00 ( 99.82)
 * Acc@1 93.310 Acc@5 99.830
### epoch[63] execution time: 17.301444053649902
EPOCH 64
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [64][  0/391]	Time  0.215 ( 0.215)	Data  0.181 ( 0.181)	Loss 4.9724e-03 (4.9724e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [64][ 10/391]	Time  0.036 ( 0.054)	Data  0.001 ( 0.018)	Loss 7.3152e-03 (5.0785e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [64][ 20/391]	Time  0.036 ( 0.046)	Data  0.001 ( 0.011)	Loss 2.6413e-03 (4.5708e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [64][ 30/391]	Time  0.037 ( 0.044)	Data  0.001 ( 0.008)	Loss 6.1682e-03 (7.1349e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [64][ 40/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.006)	Loss 6.8596e-03 (6.8839e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [64][ 50/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.006)	Loss 8.4919e-03 (6.4913e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [64][ 60/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.005)	Loss 6.6189e-03 (6.5807e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [64][ 70/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.005)	Loss 7.0848e-03 (6.7387e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [64][ 80/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.1099e-03 (6.7773e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [64][ 90/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.3901e-03 (6.6845e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [64][100/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.004)	Loss 3.1302e-03 (6.7932e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [64][110/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.6267e-03 (6.4826e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [64][120/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 7.9875e-04 (6.4769e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [64][130/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 4.5357e-03 (6.3206e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [64][140/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 9.7708e-03 (6.3689e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [64][150/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.6418e-03 (6.5572e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [64][160/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.5034e-03 (6.4347e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [64][170/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.2188e-03 (6.3130e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [64][180/391]	Time  0.046 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.2131e-03 (6.3379e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [64][190/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.9458e-03 (6.2717e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [64][200/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.8798e-03 (6.2796e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [64][210/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.0476e-02 (6.2612e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [64][220/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.7290e-03 (6.1576e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [64][230/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.5372e-02 (6.1876e-03)	Acc@1  98.44 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [64][240/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.2843e-03 (6.1567e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [64][250/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.4946e-03 (6.1050e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [64][260/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.8073e-04 (6.0797e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [64][270/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.6715e-03 (6.0673e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [64][280/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.8589e-03 (6.0567e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [64][290/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.0524e-02 (6.0203e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [64][300/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.2015e-03 (6.0020e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [64][310/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.5160e-03 (6.1061e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [64][320/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.3035e-03 (6.0277e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [64][330/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.6651e-03 (6.0082e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [64][340/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.8276e-03 (6.0435e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [64][350/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.6033e-03 (6.0280e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [64][360/391]	Time  0.038 ( 0.038)	Data  0.002 ( 0.003)	Loss 3.0761e-03 (6.0482e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [64][370/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.1468e-03 (6.0100e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [64][380/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.6380e-03 (6.0096e-03)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [64][390/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 9.7874e-03 (6.0581e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
## e[64] optimizer.zero_grad (sum) time: 0.17747259140014648
## e[64]       loss.backward (sum) time: 2.9913527965545654
## e[64]      optimizer.step (sum) time: 1.023805856704712
## epoch[64] training(only) time: 15.115947484970093
# Switched to evaluate mode...
Test: [  0/100]	Time  0.165 ( 0.165)	Loss 2.3645e-01 (2.3645e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.016 ( 0.031)	Loss 4.1436e-01 (2.8422e-01)	Acc@1  89.00 ( 93.45)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.021 ( 0.025)	Loss 3.8653e-01 (2.9407e-01)	Acc@1  91.00 ( 93.38)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.020 ( 0.023)	Loss 3.0426e-01 (2.9507e-01)	Acc@1  92.00 ( 93.42)	Acc@5  99.00 ( 99.84)
Test: [ 40/100]	Time  0.017 ( 0.022)	Loss 2.3657e-01 (2.9582e-01)	Acc@1  93.00 ( 93.46)	Acc@5 100.00 ( 99.80)
Test: [ 50/100]	Time  0.023 ( 0.021)	Loss 9.5617e-02 (3.0249e-01)	Acc@1  96.00 ( 93.39)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.025 ( 0.021)	Loss 3.1097e-01 (2.9707e-01)	Acc@1  92.00 ( 93.33)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.017 ( 0.021)	Loss 6.1201e-01 (2.9023e-01)	Acc@1  90.00 ( 93.37)	Acc@5  99.00 ( 99.79)
Test: [ 80/100]	Time  0.020 ( 0.021)	Loss 2.0087e-01 (2.9323e-01)	Acc@1  95.00 ( 93.37)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.019 ( 0.021)	Loss 1.8967e-01 (2.8868e-01)	Acc@1  93.00 ( 93.40)	Acc@5 100.00 ( 99.80)
 * Acc@1 93.430 Acc@5 99.800
### epoch[64] execution time: 17.31472897529602
EPOCH 65
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [65][  0/391]	Time  0.203 ( 0.203)	Data  0.169 ( 0.169)	Loss 5.3466e-03 (5.3466e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [65][ 10/391]	Time  0.039 ( 0.053)	Data  0.001 ( 0.017)	Loss 1.0471e-02 (9.0584e-03)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [65][ 20/391]	Time  0.037 ( 0.045)	Data  0.001 ( 0.010)	Loss 7.0200e-03 (7.5753e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [65][ 30/391]	Time  0.037 ( 0.043)	Data  0.001 ( 0.008)	Loss 1.8018e-02 (8.0797e-03)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [65][ 40/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.006)	Loss 1.5687e-02 (8.0209e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [65][ 50/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.005)	Loss 2.9949e-03 (7.6519e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [65][ 60/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.005)	Loss 4.0322e-03 (7.1244e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [65][ 70/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.005)	Loss 4.4258e-03 (7.0552e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [65][ 80/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.7980e-02 (7.3406e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [65][ 90/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.5406e-03 (7.2298e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [65][100/391]	Time  0.037 ( 0.039)	Data  0.003 ( 0.004)	Loss 3.0945e-03 (7.0618e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [65][110/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.8736e-03 (6.9225e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [65][120/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.1652e-02 (6.7775e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [65][130/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.8842e-03 (6.6755e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [65][140/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.9179e-02 (6.5163e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [65][150/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.7865e-03 (6.3333e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [65][160/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.1516e-02 (6.5516e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [65][170/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.3327e-03 (6.6096e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [65][180/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.4823e-03 (6.6316e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [65][190/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.2261e-03 (6.6224e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [65][200/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.0107e-02 (6.6218e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [65][210/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.2017e-03 (6.5824e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [65][220/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.2108e-03 (6.7370e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [65][230/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.3842e-03 (6.7296e-03)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [65][240/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.3289e-03 (6.6778e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [65][250/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.1773e-03 (6.6492e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [65][260/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.1991e-03 (6.5732e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [65][270/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.3848e-03 (6.7147e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [65][280/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.6394e-03 (6.7391e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [65][290/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.7265e-03 (6.6841e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [65][300/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.4576e-03 (6.6696e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [65][310/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.3430e-03 (6.7811e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [65][320/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.3367e-03 (6.6918e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [65][330/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.0755e-03 (6.6169e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [65][340/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.1744e-03 (6.6753e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [65][350/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.1269e-03 (6.7172e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [65][360/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.8627e-03 (6.7129e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [65][370/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.8184e-03 (6.7270e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [65][380/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 9.8034e-03 (6.7944e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [65][390/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.4872e-03 (6.8129e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
## e[65] optimizer.zero_grad (sum) time: 0.17633962631225586
## e[65]       loss.backward (sum) time: 2.9881937503814697
## e[65]      optimizer.step (sum) time: 1.0470044612884521
## epoch[65] training(only) time: 15.139755487442017
# Switched to evaluate mode...
Test: [  0/100]	Time  0.180 ( 0.180)	Loss 2.2787e-01 (2.2787e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.033)	Loss 3.9409e-01 (2.8284e-01)	Acc@1  89.00 ( 93.55)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.017 ( 0.027)	Loss 4.0934e-01 (2.9890e-01)	Acc@1  90.00 ( 93.33)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.024 ( 0.025)	Loss 2.7491e-01 (2.9933e-01)	Acc@1  93.00 ( 93.39)	Acc@5  99.00 ( 99.77)
Test: [ 40/100]	Time  0.016 ( 0.023)	Loss 2.3592e-01 (2.9914e-01)	Acc@1  92.00 ( 93.46)	Acc@5 100.00 ( 99.78)
Test: [ 50/100]	Time  0.019 ( 0.022)	Loss 8.5856e-02 (3.0470e-01)	Acc@1  97.00 ( 93.47)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.017 ( 0.021)	Loss 2.9812e-01 (2.9779e-01)	Acc@1  92.00 ( 93.41)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.020 ( 0.021)	Loss 6.0544e-01 (2.9029e-01)	Acc@1  90.00 ( 93.48)	Acc@5  99.00 ( 99.79)
Test: [ 80/100]	Time  0.016 ( 0.021)	Loss 2.0093e-01 (2.9380e-01)	Acc@1  95.00 ( 93.46)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.016 ( 0.021)	Loss 1.8658e-01 (2.8964e-01)	Acc@1  93.00 ( 93.46)	Acc@5 100.00 ( 99.79)
 * Acc@1 93.490 Acc@5 99.790
### epoch[65] execution time: 17.324608325958252
EPOCH 66
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [66][  0/391]	Time  0.216 ( 0.216)	Data  0.181 ( 0.181)	Loss 5.1045e-03 (5.1045e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [66][ 10/391]	Time  0.038 ( 0.054)	Data  0.001 ( 0.018)	Loss 1.1391e-02 (1.1193e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [66][ 20/391]	Time  0.036 ( 0.046)	Data  0.001 ( 0.011)	Loss 2.1738e-03 (7.3723e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [66][ 30/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.008)	Loss 1.5290e-03 (6.1564e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [66][ 40/391]	Time  0.034 ( 0.042)	Data  0.001 ( 0.006)	Loss 3.3037e-03 (6.4441e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [66][ 50/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.006)	Loss 2.2404e-02 (6.0984e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [66][ 60/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.005)	Loss 9.6881e-04 (6.0446e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [66][ 70/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.005)	Loss 1.0415e-02 (6.1401e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [66][ 80/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.8880e-03 (5.8431e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [66][ 90/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.9042e-03 (5.5749e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [66][100/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.0895e-03 (5.6499e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [66][110/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.8079e-03 (5.4050e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [66][120/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 8.7546e-03 (5.5110e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [66][130/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.5665e-03 (5.5582e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [66][140/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.5517e-03 (5.8228e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [66][150/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.3265e-03 (5.8991e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [66][160/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.9737e-03 (5.8551e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [66][170/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 9.5139e-04 (5.7672e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [66][180/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.3122e-02 (5.7058e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [66][190/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.4489e-03 (5.6061e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [66][200/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 9.4490e-03 (5.6996e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [66][210/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.2575e-03 (5.8209e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [66][220/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 9.2466e-03 (5.9231e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [66][230/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.6870e-03 (5.8877e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [66][240/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.9399e-03 (5.8509e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [66][250/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.3482e-02 (5.8656e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [66][260/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.7434e-03 (5.8150e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [66][270/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.1353e-03 (6.0442e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [66][280/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.2446e-03 (6.1126e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [66][290/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.7757e-02 (6.0911e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [66][300/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.6795e-02 (6.1323e-03)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [66][310/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.6133e-03 (6.0900e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [66][320/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.1846e-03 (6.0419e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [66][330/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.6986e-03 (5.9909e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [66][340/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.4033e-03 (5.9560e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [66][350/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.7359e-03 (5.9953e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [66][360/391]	Time  0.037 ( 0.038)	Data  0.000 ( 0.003)	Loss 2.4609e-03 (6.0166e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [66][370/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.6814e-03 (5.9752e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [66][380/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.9582e-03 (5.9555e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [66][390/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.2248e-03 (5.9242e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
## e[66] optimizer.zero_grad (sum) time: 0.1741352081298828
## e[66]       loss.backward (sum) time: 3.000755548477173
## e[66]      optimizer.step (sum) time: 1.029731273651123
## epoch[66] training(only) time: 15.19746470451355
# Switched to evaluate mode...
Test: [  0/100]	Time  0.176 ( 0.176)	Loss 2.4321e-01 (2.4321e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.023 ( 0.033)	Loss 3.9956e-01 (2.8379e-01)	Acc@1  88.00 ( 93.09)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.020 ( 0.027)	Loss 3.8754e-01 (2.9738e-01)	Acc@1  91.00 ( 92.95)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.016 ( 0.024)	Loss 3.1487e-01 (2.9918e-01)	Acc@1  93.00 ( 93.16)	Acc@5  99.00 ( 99.84)
Test: [ 40/100]	Time  0.018 ( 0.023)	Loss 2.3486e-01 (2.9940e-01)	Acc@1  93.00 ( 93.32)	Acc@5  99.00 ( 99.78)
Test: [ 50/100]	Time  0.023 ( 0.022)	Loss 9.3972e-02 (3.0464e-01)	Acc@1  97.00 ( 93.29)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.016 ( 0.021)	Loss 3.0122e-01 (2.9833e-01)	Acc@1  92.00 ( 93.30)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.016 ( 0.021)	Loss 6.0231e-01 (2.9040e-01)	Acc@1  90.00 ( 93.30)	Acc@5  98.00 ( 99.76)
Test: [ 80/100]	Time  0.016 ( 0.021)	Loss 2.0833e-01 (2.9424e-01)	Acc@1  95.00 ( 93.30)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.024 ( 0.020)	Loss 2.0012e-01 (2.8980e-01)	Acc@1  93.00 ( 93.36)	Acc@5 100.00 ( 99.77)
 * Acc@1 93.390 Acc@5 99.770
### epoch[66] execution time: 17.357380867004395
EPOCH 67
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [67][  0/391]	Time  0.218 ( 0.218)	Data  0.181 ( 0.181)	Loss 4.9344e-03 (4.9344e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [67][ 10/391]	Time  0.037 ( 0.054)	Data  0.001 ( 0.018)	Loss 3.6788e-03 (4.0930e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [67][ 20/391]	Time  0.036 ( 0.046)	Data  0.001 ( 0.011)	Loss 3.1668e-03 (4.1527e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [67][ 30/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.008)	Loss 5.4319e-03 (4.2808e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [67][ 40/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.006)	Loss 2.3700e-03 (4.6041e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [67][ 50/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.006)	Loss 2.5935e-03 (4.4727e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [67][ 60/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.005)	Loss 2.4340e-03 (4.6226e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [67][ 70/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.005)	Loss 2.0859e-02 (5.1042e-03)	Acc@1  99.22 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [67][ 80/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.004)	Loss 7.5553e-03 (5.0755e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [67][ 90/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 5.3038e-03 (5.1365e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [67][100/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.1005e-03 (5.2117e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [67][110/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 3.6229e-03 (5.2495e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [67][120/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 4.7126e-02 (5.8061e-03)	Acc@1  98.44 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [67][130/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 8.1202e-03 (5.7006e-03)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [67][140/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.7080e-03 (5.6559e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [67][150/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.8278e-03 (5.7421e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [67][160/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.5374e-03 (5.7869e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [67][170/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.2399e-03 (5.7717e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [67][180/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.2993e-03 (5.6930e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [67][190/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.9580e-02 (5.7672e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [67][200/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.9912e-03 (5.6519e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [67][210/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.4273e-03 (5.6343e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [67][220/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.4639e-03 (5.8448e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [67][230/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.2257e-02 (5.8161e-03)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [67][240/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.4678e-03 (5.7479e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [67][250/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.9624e-02 (5.7545e-03)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [67][260/391]	Time  0.040 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.5007e-03 (5.7203e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [67][270/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.7774e-03 (5.8162e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [67][280/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.0838e-03 (5.8246e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [67][290/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 9.7510e-04 (5.7079e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [67][300/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.6149e-03 (5.6751e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [67][310/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.3172e-03 (5.7244e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [67][320/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.4125e-03 (5.7640e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [67][330/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.0117e-03 (5.7188e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [67][340/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.8039e-03 (5.7074e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [67][350/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.4322e-02 (5.6962e-03)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [67][360/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.3233e-04 (5.5982e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [67][370/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.6848e-03 (5.6221e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [67][380/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.5505e-03 (5.7276e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [67][390/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.4657e-03 (5.6674e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
## e[67] optimizer.zero_grad (sum) time: 0.17517805099487305
## e[67]       loss.backward (sum) time: 2.9718494415283203
## e[67]      optimizer.step (sum) time: 1.0369811058044434
## epoch[67] training(only) time: 15.088084936141968
# Switched to evaluate mode...
Test: [  0/100]	Time  0.180 ( 0.180)	Loss 2.2532e-01 (2.2532e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.020 ( 0.033)	Loss 3.9980e-01 (2.8722e-01)	Acc@1  88.00 ( 93.09)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.017 ( 0.027)	Loss 4.0348e-01 (3.0007e-01)	Acc@1  91.00 ( 93.00)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.021 ( 0.024)	Loss 2.8975e-01 (3.0066e-01)	Acc@1  93.00 ( 93.16)	Acc@5  99.00 ( 99.81)
Test: [ 40/100]	Time  0.016 ( 0.023)	Loss 2.1434e-01 (3.0102e-01)	Acc@1  93.00 ( 93.22)	Acc@5 100.00 ( 99.78)
Test: [ 50/100]	Time  0.016 ( 0.022)	Loss 9.1269e-02 (3.0748e-01)	Acc@1  97.00 ( 93.18)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.016 ( 0.022)	Loss 3.2958e-01 (3.0011e-01)	Acc@1  92.00 ( 93.25)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.016 ( 0.021)	Loss 5.9145e-01 (2.9256e-01)	Acc@1  90.00 ( 93.28)	Acc@5  99.00 ( 99.79)
Test: [ 80/100]	Time  0.024 ( 0.021)	Loss 1.9351e-01 (2.9526e-01)	Acc@1  95.00 ( 93.30)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.020 ( 0.021)	Loss 2.0439e-01 (2.9073e-01)	Acc@1  93.00 ( 93.31)	Acc@5 100.00 ( 99.80)
 * Acc@1 93.350 Acc@5 99.800
### epoch[67] execution time: 17.257533311843872
EPOCH 68
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [68][  0/391]	Time  0.206 ( 0.206)	Data  0.170 ( 0.170)	Loss 2.6884e-03 (2.6884e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [68][ 10/391]	Time  0.037 ( 0.053)	Data  0.001 ( 0.018)	Loss 4.8372e-03 (5.3221e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [68][ 20/391]	Time  0.038 ( 0.047)	Data  0.001 ( 0.010)	Loss 3.3726e-03 (5.4336e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [68][ 30/391]	Time  0.037 ( 0.044)	Data  0.001 ( 0.008)	Loss 1.7237e-02 (5.7871e-03)	Acc@1  99.22 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [68][ 40/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.006)	Loss 3.9452e-03 (6.2397e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [68][ 50/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.006)	Loss 2.6305e-03 (6.4899e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [68][ 60/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.005)	Loss 1.0769e-02 (6.2030e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [68][ 70/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.005)	Loss 4.8588e-03 (6.2813e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [68][ 80/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.5847e-03 (6.6471e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [68][ 90/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.004)	Loss 3.9321e-03 (6.4354e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [68][100/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 9.1372e-04 (6.1712e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [68][110/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 5.8050e-03 (6.0842e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [68][120/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 3.9346e-03 (5.9018e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [68][130/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 5.2673e-03 (5.7365e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [68][140/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.8171e-03 (5.6251e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [68][150/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.5989e-03 (5.5428e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [68][160/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.9235e-03 (5.5181e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [68][170/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.0522e-03 (5.5639e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [68][180/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.3694e-03 (5.6333e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [68][190/391]	Time  0.038 ( 0.039)	Data  0.002 ( 0.003)	Loss 2.9780e-02 (5.7432e-03)	Acc@1  97.66 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [68][200/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.5230e-03 (5.6955e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [68][210/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.7443e-02 (5.7395e-03)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [68][220/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.5588e-03 (5.7897e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [68][230/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.7666e-03 (5.7350e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [68][240/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.0919e-02 (5.7931e-03)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [68][250/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.1097e-03 (5.6587e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [68][260/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.4534e-02 (5.7085e-03)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [68][270/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.1465e-02 (5.8816e-03)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [68][280/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.9791e-03 (5.9944e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [68][290/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.0742e-03 (5.9086e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [68][300/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.5443e-03 (5.8803e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [68][310/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.7086e-03 (5.9304e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [68][320/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.2633e-03 (5.9312e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [68][330/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.3961e-03 (5.8666e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [68][340/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.5341e-03 (5.9185e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [68][350/391]	Time  0.037 ( 0.038)	Data  0.002 ( 0.003)	Loss 1.9314e-03 (5.9839e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [68][360/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.9918e-03 (5.9226e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [68][370/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.8115e-03 (5.8882e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [68][380/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.4223e-03 (5.9086e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [68][390/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.8842e-02 (5.9503e-03)	Acc@1  98.75 ( 99.86)	Acc@5 100.00 (100.00)
## e[68] optimizer.zero_grad (sum) time: 0.1743009090423584
## e[68]       loss.backward (sum) time: 2.9847207069396973
## e[68]      optimizer.step (sum) time: 1.0338499546051025
## epoch[68] training(only) time: 15.1541006565094
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 2.5761e-01 (2.5761e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.017 ( 0.033)	Loss 3.7977e-01 (2.8507e-01)	Acc@1  89.00 ( 93.18)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.020 ( 0.026)	Loss 4.0291e-01 (2.9969e-01)	Acc@1  90.00 ( 93.05)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.020 ( 0.023)	Loss 2.9462e-01 (3.0186e-01)	Acc@1  93.00 ( 93.10)	Acc@5 100.00 ( 99.87)
Test: [ 40/100]	Time  0.020 ( 0.022)	Loss 2.1983e-01 (3.0036e-01)	Acc@1  93.00 ( 93.27)	Acc@5 100.00 ( 99.85)
Test: [ 50/100]	Time  0.023 ( 0.021)	Loss 8.5830e-02 (3.0503e-01)	Acc@1  97.00 ( 93.29)	Acc@5 100.00 ( 99.84)
Test: [ 60/100]	Time  0.020 ( 0.021)	Loss 3.1306e-01 (2.9746e-01)	Acc@1  92.00 ( 93.34)	Acc@5 100.00 ( 99.87)
Test: [ 70/100]	Time  0.017 ( 0.021)	Loss 5.8150e-01 (2.8980e-01)	Acc@1  90.00 ( 93.35)	Acc@5  99.00 ( 99.85)
Test: [ 80/100]	Time  0.019 ( 0.020)	Loss 2.1296e-01 (2.9323e-01)	Acc@1  94.00 ( 93.37)	Acc@5 100.00 ( 99.83)
Test: [ 90/100]	Time  0.019 ( 0.020)	Loss 2.2646e-01 (2.8902e-01)	Acc@1  93.00 ( 93.40)	Acc@5 100.00 ( 99.84)
 * Acc@1 93.410 Acc@5 99.830
### epoch[68] execution time: 17.314301252365112
EPOCH 69
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [69][  0/391]	Time  0.212 ( 0.212)	Data  0.175 ( 0.175)	Loss 3.9192e-03 (3.9192e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [69][ 10/391]	Time  0.036 ( 0.053)	Data  0.001 ( 0.018)	Loss 7.0431e-03 (7.7866e-03)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [69][ 20/391]	Time  0.037 ( 0.046)	Data  0.001 ( 0.010)	Loss 9.2008e-03 (5.7790e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [69][ 30/391]	Time  0.035 ( 0.043)	Data  0.001 ( 0.008)	Loss 3.0361e-03 (6.1765e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [69][ 40/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.006)	Loss 2.8314e-02 (6.3626e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [69][ 50/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.005)	Loss 1.4578e-03 (6.2898e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [69][ 60/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.005)	Loss 1.4456e-03 (6.1368e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [69][ 70/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.005)	Loss 3.0754e-03 (5.7837e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [69][ 80/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 8.9940e-03 (5.8567e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [69][ 90/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.2373e-03 (5.4860e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [69][100/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 4.2669e-03 (5.3378e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [69][110/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 4.7357e-03 (5.0953e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [69][120/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 3.5052e-03 (5.2301e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [69][130/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 3.0492e-03 (5.2146e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [69][140/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.4861e-03 (5.1259e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [69][150/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.2941e-02 (5.2590e-03)	Acc@1  99.22 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [69][160/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.4981e-03 (5.2429e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [69][170/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.4334e-03 (5.2593e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [69][180/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.8150e-03 (5.3062e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [69][190/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.4454e-02 (5.2748e-03)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [69][200/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.3978e-03 (5.1944e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [69][210/391]	Time  0.037 ( 0.039)	Data  0.002 ( 0.003)	Loss 1.6842e-03 (5.1403e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [69][220/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.0730e-03 (5.2341e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [69][230/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.1515e-03 (5.1380e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [69][240/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.2586e-03 (5.1257e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [69][250/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.6643e-03 (5.0952e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [69][260/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.1317e-03 (5.1035e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [69][270/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.1280e-02 (5.1104e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [69][280/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.3613e-03 (5.0865e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [69][290/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.2166e-02 (5.2114e-03)	Acc@1  98.44 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [69][300/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.9131e-03 (5.2321e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [69][310/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.3190e-03 (5.2539e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [69][320/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.9303e-02 (5.3119e-03)	Acc@1  98.44 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [69][330/391]	Time  0.037 ( 0.038)	Data  0.002 ( 0.003)	Loss 1.6020e-02 (5.3393e-03)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [69][340/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.5807e-03 (5.3838e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [69][350/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.5613e-03 (5.5012e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [69][360/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.1378e-03 (5.4849e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [69][370/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.7055e-02 (5.4733e-03)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [69][380/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.6944e-03 (5.4562e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [69][390/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.4624e-04 (5.4771e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
## e[69] optimizer.zero_grad (sum) time: 0.17519688606262207
## e[69]       loss.backward (sum) time: 2.9626169204711914
## e[69]      optimizer.step (sum) time: 1.0151093006134033
## epoch[69] training(only) time: 15.107327461242676
# Switched to evaluate mode...
Test: [  0/100]	Time  0.194 ( 0.194)	Loss 2.7323e-01 (2.7323e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.020 ( 0.035)	Loss 3.9489e-01 (2.9357e-01)	Acc@1  90.00 ( 93.18)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.016 ( 0.027)	Loss 4.2479e-01 (3.0653e-01)	Acc@1  90.00 ( 92.86)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.016 ( 0.025)	Loss 2.7360e-01 (3.0427e-01)	Acc@1  93.00 ( 93.03)	Acc@5 100.00 ( 99.81)
Test: [ 40/100]	Time  0.016 ( 0.023)	Loss 2.2084e-01 (3.0323e-01)	Acc@1  92.00 ( 93.12)	Acc@5 100.00 ( 99.76)
Test: [ 50/100]	Time  0.021 ( 0.022)	Loss 9.0360e-02 (3.0722e-01)	Acc@1  97.00 ( 93.27)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.016 ( 0.022)	Loss 3.0158e-01 (2.9839e-01)	Acc@1  93.00 ( 93.33)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.020 ( 0.021)	Loss 5.4879e-01 (2.9005e-01)	Acc@1  91.00 ( 93.39)	Acc@5  99.00 ( 99.79)
Test: [ 80/100]	Time  0.016 ( 0.021)	Loss 1.9982e-01 (2.9276e-01)	Acc@1  94.00 ( 93.38)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.017 ( 0.021)	Loss 2.2565e-01 (2.8830e-01)	Acc@1  93.00 ( 93.38)	Acc@5 100.00 ( 99.79)
 * Acc@1 93.410 Acc@5 99.790
### epoch[69] execution time: 17.30415987968445
EPOCH 70
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [70][  0/391]	Time  0.215 ( 0.215)	Data  0.180 ( 0.180)	Loss 8.4517e-03 (8.4517e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [70][ 10/391]	Time  0.038 ( 0.054)	Data  0.001 ( 0.018)	Loss 4.8054e-03 (4.6702e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [70][ 20/391]	Time  0.036 ( 0.046)	Data  0.001 ( 0.011)	Loss 2.2694e-03 (4.4926e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [70][ 30/391]	Time  0.037 ( 0.043)	Data  0.001 ( 0.008)	Loss 5.5078e-03 (4.9235e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [70][ 40/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.007)	Loss 4.2620e-03 (5.4196e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [70][ 50/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.006)	Loss 3.5909e-03 (5.6215e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [70][ 60/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.005)	Loss 3.0813e-03 (5.3318e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [70][ 70/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.005)	Loss 1.5727e-03 (5.2771e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [70][ 80/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 7.5428e-03 (5.5716e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [70][ 90/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 6.4519e-03 (5.5678e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [70][100/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 3.5025e-03 (5.4025e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [70][110/391]	Time  0.040 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.1326e-02 (5.3506e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [70][120/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.1206e-02 (5.7027e-03)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [70][130/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 6.1736e-03 (5.8741e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [70][140/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 9.8969e-03 (6.2263e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [70][150/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.6109e-03 (6.2176e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [70][160/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.5628e-03 (6.1350e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [70][170/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.8187e-03 (5.9944e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [70][180/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.3309e-03 (6.2043e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [70][190/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.2486e-03 (6.3662e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [70][200/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.7583e-03 (6.4880e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [70][210/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.0118e-03 (6.4035e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [70][220/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.6313e-03 (6.5164e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [70][230/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.4223e-02 (6.4774e-03)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [70][240/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.2838e-03 (6.4151e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [70][250/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.0591e-03 (6.2885e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [70][260/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.2301e-03 (6.2732e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [70][270/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.6313e-03 (6.2113e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [70][280/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.4216e-03 (6.1897e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [70][290/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.8729e-03 (6.1566e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [70][300/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.7340e-03 (6.0686e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [70][310/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.9061e-03 (5.9861e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [70][320/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.6385e-03 (6.0009e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [70][330/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.0472e-03 (6.0039e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [70][340/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.0899e-02 (5.9673e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [70][350/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.0007e-02 (5.9679e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [70][360/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.5710e-02 (6.0074e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [70][370/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.1347e-03 (6.0449e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [70][380/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.9379e-03 (6.1195e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [70][390/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.7730e-02 (6.1438e-03)	Acc@1  98.75 ( 99.86)	Acc@5 100.00 (100.00)
## e[70] optimizer.zero_grad (sum) time: 0.17495155334472656
## e[70]       loss.backward (sum) time: 2.995069980621338
## e[70]      optimizer.step (sum) time: 1.0161364078521729
## epoch[70] training(only) time: 15.151279211044312
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 2.4563e-01 (2.4563e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.016 ( 0.033)	Loss 4.0364e-01 (2.9135e-01)	Acc@1  89.00 ( 93.18)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.017 ( 0.025)	Loss 3.9233e-01 (3.0072e-01)	Acc@1  90.00 ( 93.05)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.017 ( 0.023)	Loss 2.6772e-01 (3.0176e-01)	Acc@1  93.00 ( 93.10)	Acc@5 100.00 ( 99.81)
Test: [ 40/100]	Time  0.017 ( 0.022)	Loss 2.1685e-01 (3.0053e-01)	Acc@1  94.00 ( 93.20)	Acc@5 100.00 ( 99.80)
Test: [ 50/100]	Time  0.024 ( 0.021)	Loss 9.7544e-02 (3.0504e-01)	Acc@1  97.00 ( 93.22)	Acc@5 100.00 ( 99.80)
Test: [ 60/100]	Time  0.018 ( 0.021)	Loss 3.0263e-01 (2.9652e-01)	Acc@1  91.00 ( 93.23)	Acc@5 100.00 ( 99.84)
Test: [ 70/100]	Time  0.017 ( 0.020)	Loss 5.7030e-01 (2.8818e-01)	Acc@1  91.00 ( 93.31)	Acc@5  99.00 ( 99.82)
Test: [ 80/100]	Time  0.020 ( 0.020)	Loss 1.9550e-01 (2.9056e-01)	Acc@1  94.00 ( 93.32)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.016 ( 0.020)	Loss 2.2566e-01 (2.8638e-01)	Acc@1  93.00 ( 93.38)	Acc@5 100.00 ( 99.82)
 * Acc@1 93.420 Acc@5 99.820
### epoch[70] execution time: 17.254292249679565
EPOCH 71
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [71][  0/391]	Time  0.205 ( 0.205)	Data  0.172 ( 0.172)	Loss 4.9502e-04 (4.9502e-04)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [71][ 10/391]	Time  0.037 ( 0.053)	Data  0.001 ( 0.018)	Loss 1.7064e-03 (5.5556e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [71][ 20/391]	Time  0.037 ( 0.046)	Data  0.001 ( 0.010)	Loss 1.0891e-03 (6.1048e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [71][ 30/391]	Time  0.036 ( 0.043)	Data  0.001 ( 0.008)	Loss 1.6098e-02 (6.5172e-03)	Acc@1  98.44 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [71][ 40/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.006)	Loss 2.9769e-03 (6.0129e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [71][ 50/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.006)	Loss 2.1574e-03 (5.6742e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [71][ 60/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.005)	Loss 4.6805e-03 (6.3175e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [71][ 70/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.005)	Loss 2.9720e-03 (6.3504e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [71][ 80/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.1212e-03 (6.0479e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [71][ 90/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 7.1097e-03 (6.0797e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [71][100/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 7.2370e-03 (5.8051e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [71][110/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.7839e-03 (6.0187e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [71][120/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.0451e-02 (6.0770e-03)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [71][130/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.8791e-03 (6.1688e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [71][140/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.0105e-03 (6.0026e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [71][150/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.2043e-03 (5.9799e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [71][160/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.8059e-03 (5.8841e-03)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [71][170/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.8214e-02 (5.9301e-03)	Acc@1  98.44 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [71][180/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.3028e-02 (6.0121e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [71][190/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.0965e-03 (5.8515e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [71][200/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.3149e-03 (5.9441e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [71][210/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.1029e-03 (5.8791e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [71][220/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.2818e-03 (5.9097e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [71][230/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.6551e-03 (5.8860e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [71][240/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.8657e-02 (6.0106e-03)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [71][250/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.0805e-03 (5.9984e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [71][260/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.2260e-03 (5.9309e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [71][270/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.8209e-03 (5.8916e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [71][280/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.4766e-03 (6.0070e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [71][290/391]	Time  0.038 ( 0.038)	Data  0.002 ( 0.003)	Loss 2.9605e-03 (6.0585e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [71][300/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.3769e-02 (6.1166e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [71][310/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.8146e-03 (6.1609e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [71][320/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.8394e-03 (6.1457e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [71][330/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.4676e-03 (6.1470e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [71][340/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.7083e-03 (6.1843e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [71][350/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.9319e-03 (6.1890e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [71][360/391]	Time  0.038 ( 0.038)	Data  0.002 ( 0.003)	Loss 4.6890e-03 (6.1674e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [71][370/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.1858e-03 (6.0974e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [71][380/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.3703e-03 (6.0581e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [71][390/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.0661e-02 (6.0053e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
## e[71] optimizer.zero_grad (sum) time: 0.17379188537597656
## e[71]       loss.backward (sum) time: 3.0074753761291504
## e[71]      optimizer.step (sum) time: 1.0202727317810059
## epoch[71] training(only) time: 15.12437915802002
# Switched to evaluate mode...
Test: [  0/100]	Time  0.177 ( 0.177)	Loss 2.3173e-01 (2.3173e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.020 ( 0.033)	Loss 4.0154e-01 (2.8645e-01)	Acc@1  90.00 ( 93.27)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.016 ( 0.027)	Loss 4.0357e-01 (2.9883e-01)	Acc@1  91.00 ( 93.14)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.023 ( 0.024)	Loss 2.6770e-01 (2.9906e-01)	Acc@1  93.00 ( 93.16)	Acc@5  99.00 ( 99.77)
Test: [ 40/100]	Time  0.017 ( 0.023)	Loss 2.2920e-01 (2.9761e-01)	Acc@1  93.00 ( 93.24)	Acc@5 100.00 ( 99.78)
Test: [ 50/100]	Time  0.020 ( 0.022)	Loss 8.6695e-02 (3.0354e-01)	Acc@1  96.00 ( 93.22)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.019 ( 0.022)	Loss 3.1470e-01 (2.9528e-01)	Acc@1  93.00 ( 93.26)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.017 ( 0.021)	Loss 5.8334e-01 (2.8804e-01)	Acc@1  90.00 ( 93.34)	Acc@5  99.00 ( 99.79)
Test: [ 80/100]	Time  0.024 ( 0.021)	Loss 1.9605e-01 (2.9025e-01)	Acc@1  94.00 ( 93.35)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.019 ( 0.021)	Loss 2.0283e-01 (2.8649e-01)	Acc@1  93.00 ( 93.38)	Acc@5 100.00 ( 99.79)
 * Acc@1 93.400 Acc@5 99.790
### epoch[71] execution time: 17.327147245407104
EPOCH 72
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [72][  0/391]	Time  0.218 ( 0.218)	Data  0.181 ( 0.181)	Loss 4.4173e-03 (4.4173e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [72][ 10/391]	Time  0.036 ( 0.054)	Data  0.001 ( 0.018)	Loss 3.0098e-03 (4.1921e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [72][ 20/391]	Time  0.037 ( 0.046)	Data  0.001 ( 0.011)	Loss 6.0647e-03 (3.8808e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [72][ 30/391]	Time  0.037 ( 0.044)	Data  0.001 ( 0.008)	Loss 2.1645e-03 (4.9192e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [72][ 40/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.007)	Loss 1.9268e-03 (4.9974e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [72][ 50/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.006)	Loss 2.7879e-03 (4.9425e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [72][ 60/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.005)	Loss 2.7615e-03 (5.1923e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [72][ 70/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.005)	Loss 2.5348e-03 (5.0044e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [72][ 80/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.004)	Loss 9.0554e-03 (5.1664e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [72][ 90/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.4410e-03 (5.0352e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [72][100/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.6896e-03 (4.9293e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [72][110/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.6523e-03 (5.0269e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [72][120/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.004)	Loss 3.9040e-03 (5.3975e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [72][130/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.9500e-02 (5.5522e-03)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [72][140/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.0795e-03 (5.4877e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [72][150/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.9445e-03 (5.5852e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [72][160/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.0847e-02 (5.6558e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [72][170/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.2987e-04 (5.7748e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [72][180/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.3759e-03 (5.7011e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [72][190/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.2384e-03 (5.6390e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [72][200/391]	Time  0.034 ( 0.039)	Data  0.000 ( 0.003)	Loss 3.0341e-03 (5.7567e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [72][210/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.4777e-03 (5.8024e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [72][220/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.1091e-03 (5.8513e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [72][230/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.0851e-03 (5.9258e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [72][240/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.8965e-03 (5.9878e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [72][250/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.3057e-02 (6.0697e-03)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [72][260/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.0887e-03 (6.1272e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [72][270/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.3011e-03 (6.0790e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [72][280/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.6292e-02 (6.0946e-03)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [72][290/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.8581e-03 (6.0845e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [72][300/391]	Time  0.038 ( 0.038)	Data  0.002 ( 0.003)	Loss 5.7147e-03 (6.0909e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [72][310/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.0705e-03 (6.1586e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [72][320/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.0151e-03 (6.0829e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [72][330/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.6255e-03 (6.0412e-03)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [72][340/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.6029e-03 (5.9667e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [72][350/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.9187e-03 (5.9489e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [72][360/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.7134e-03 (5.9350e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [72][370/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.3242e-02 (5.9443e-03)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [72][380/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.0300e-02 (5.9649e-03)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [72][390/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.0416e-03 (6.0142e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
## e[72] optimizer.zero_grad (sum) time: 0.17655158042907715
## e[72]       loss.backward (sum) time: 2.9995622634887695
## e[72]      optimizer.step (sum) time: 1.0284476280212402
## epoch[72] training(only) time: 15.143398523330688
# Switched to evaluate mode...
Test: [  0/100]	Time  0.182 ( 0.182)	Loss 2.6419e-01 (2.6419e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.019 ( 0.033)	Loss 4.0191e-01 (2.8367e-01)	Acc@1  89.00 ( 93.36)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.023 ( 0.026)	Loss 3.8027e-01 (2.9727e-01)	Acc@1  90.00 ( 93.29)	Acc@5 100.00 ( 99.90)
Test: [ 30/100]	Time  0.019 ( 0.024)	Loss 2.8258e-01 (2.9984e-01)	Acc@1  92.00 ( 93.29)	Acc@5  99.00 ( 99.87)
Test: [ 40/100]	Time  0.019 ( 0.023)	Loss 2.5019e-01 (2.9905e-01)	Acc@1  94.00 ( 93.44)	Acc@5 100.00 ( 99.85)
Test: [ 50/100]	Time  0.020 ( 0.022)	Loss 1.0722e-01 (3.0339e-01)	Acc@1  97.00 ( 93.55)	Acc@5 100.00 ( 99.82)
Test: [ 60/100]	Time  0.017 ( 0.021)	Loss 3.0858e-01 (2.9591e-01)	Acc@1  91.00 ( 93.46)	Acc@5 100.00 ( 99.85)
Test: [ 70/100]	Time  0.021 ( 0.021)	Loss 6.0100e-01 (2.8828e-01)	Acc@1  90.00 ( 93.51)	Acc@5  98.00 ( 99.82)
Test: [ 80/100]	Time  0.021 ( 0.021)	Loss 2.0788e-01 (2.9213e-01)	Acc@1  94.00 ( 93.48)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.020 ( 0.021)	Loss 1.9394e-01 (2.8811e-01)	Acc@1  94.00 ( 93.47)	Acc@5 100.00 ( 99.81)
 * Acc@1 93.490 Acc@5 99.810
### epoch[72] execution time: 17.371835947036743
EPOCH 73
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [73][  0/391]	Time  0.219 ( 0.219)	Data  0.185 ( 0.185)	Loss 2.4011e-03 (2.4011e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [73][ 10/391]	Time  0.037 ( 0.054)	Data  0.001 ( 0.019)	Loss 2.8845e-03 (4.2738e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [73][ 20/391]	Time  0.037 ( 0.046)	Data  0.001 ( 0.011)	Loss 1.0335e-02 (4.2896e-03)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [73][ 30/391]	Time  0.035 ( 0.043)	Data  0.001 ( 0.008)	Loss 1.8217e-03 (3.9055e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [73][ 40/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.007)	Loss 1.2984e-03 (5.0582e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [73][ 50/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.006)	Loss 8.9864e-03 (4.9652e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [73][ 60/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.005)	Loss 8.0219e-04 (5.2312e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [73][ 70/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.005)	Loss 4.9109e-03 (5.2699e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [73][ 80/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 3.5826e-03 (5.3321e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [73][ 90/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.2734e-02 (5.2571e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [73][100/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.5045e-03 (5.4630e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [73][110/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 8.4761e-04 (5.3182e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [73][120/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.3979e-03 (5.2532e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [73][130/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.4811e-03 (5.4351e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [73][140/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.0643e-02 (5.5293e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [73][150/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.9776e-03 (5.4025e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [73][160/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.7513e-03 (5.4137e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [73][170/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 9.2128e-04 (5.3284e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [73][180/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.9799e-03 (5.3836e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [73][190/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.3152e-03 (5.3343e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [73][200/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.0535e-02 (5.6169e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [73][210/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.4775e-02 (5.5925e-03)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [73][220/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.1096e-02 (5.5779e-03)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [73][230/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.5736e-03 (5.5139e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [73][240/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.7024e-02 (5.5732e-03)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [73][250/391]	Time  0.043 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.5320e-03 (5.5281e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [73][260/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.4403e-03 (5.5395e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [73][270/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.3569e-03 (5.5865e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [73][280/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.0214e-03 (5.5278e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [73][290/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.7890e-03 (5.5092e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [73][300/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.9085e-03 (5.5429e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [73][310/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.6973e-03 (5.5388e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [73][320/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.0967e-03 (5.5361e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [73][330/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.8491e-02 (5.5926e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [73][340/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.7307e-03 (5.5476e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [73][350/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.1280e-03 (5.4824e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [73][360/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.1124e-03 (5.4812e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [73][370/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.8628e-03 (5.4518e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [73][380/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.3912e-03 (5.5693e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [73][390/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.0983e-02 (5.5455e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
## e[73] optimizer.zero_grad (sum) time: 0.17510676383972168
## e[73]       loss.backward (sum) time: 2.987931489944458
## e[73]      optimizer.step (sum) time: 1.0291674137115479
## epoch[73] training(only) time: 15.197192907333374
# Switched to evaluate mode...
Test: [  0/100]	Time  0.175 ( 0.175)	Loss 2.3843e-01 (2.3843e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.016 ( 0.032)	Loss 4.1071e-01 (2.8332e-01)	Acc@1  88.00 ( 93.00)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.016 ( 0.025)	Loss 3.8014e-01 (2.9629e-01)	Acc@1  91.00 ( 93.10)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.021 ( 0.023)	Loss 2.9149e-01 (2.9939e-01)	Acc@1  92.00 ( 93.16)	Acc@5  99.00 ( 99.81)
Test: [ 40/100]	Time  0.030 ( 0.022)	Loss 2.2478e-01 (2.9698e-01)	Acc@1  93.00 ( 93.29)	Acc@5 100.00 ( 99.80)
Test: [ 50/100]	Time  0.020 ( 0.022)	Loss 9.0292e-02 (3.0352e-01)	Acc@1  96.00 ( 93.25)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.016 ( 0.021)	Loss 3.4861e-01 (2.9618e-01)	Acc@1  92.00 ( 93.26)	Acc@5 100.00 ( 99.82)
Test: [ 70/100]	Time  0.018 ( 0.021)	Loss 6.2168e-01 (2.8874e-01)	Acc@1  90.00 ( 93.34)	Acc@5  99.00 ( 99.80)
Test: [ 80/100]	Time  0.018 ( 0.021)	Loss 2.1534e-01 (2.9253e-01)	Acc@1  94.00 ( 93.35)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.019 ( 0.020)	Loss 1.9548e-01 (2.8877e-01)	Acc@1  93.00 ( 93.38)	Acc@5 100.00 ( 99.81)
 * Acc@1 93.420 Acc@5 99.810
### epoch[73] execution time: 17.353865385055542
EPOCH 74
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [74][  0/391]	Time  0.219 ( 0.219)	Data  0.182 ( 0.182)	Loss 2.2857e-02 (2.2857e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [74][ 10/391]	Time  0.037 ( 0.054)	Data  0.001 ( 0.018)	Loss 2.6722e-03 (6.2844e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [74][ 20/391]	Time  0.038 ( 0.046)	Data  0.001 ( 0.010)	Loss 2.1909e-03 (5.0449e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [74][ 30/391]	Time  0.037 ( 0.044)	Data  0.001 ( 0.008)	Loss 1.0457e-02 (6.5611e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [74][ 40/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.006)	Loss 7.7382e-03 (6.2932e-03)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [74][ 50/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.006)	Loss 5.0857e-03 (5.6260e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [74][ 60/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.005)	Loss 3.6611e-03 (5.7702e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [74][ 70/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.005)	Loss 8.0453e-04 (5.3358e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [74][ 80/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 4.4445e-03 (5.3484e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [74][ 90/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.0716e-03 (5.5825e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [74][100/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.004)	Loss 3.1350e-03 (5.5763e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [74][110/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.5836e-03 (5.5566e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [74][120/391]	Time  0.040 ( 0.039)	Data  0.001 ( 0.004)	Loss 9.7388e-04 (5.3112e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [74][130/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.7933e-02 (5.3136e-03)	Acc@1  99.22 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [74][140/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.3182e-03 (5.1983e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [74][150/391]	Time  0.044 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.7705e-03 (5.1214e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [74][160/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 9.9050e-04 (5.1613e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [74][170/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.1781e-04 (5.2270e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [74][180/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.5051e-03 (5.3839e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [74][190/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.8826e-03 (5.5248e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [74][200/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.7334e-03 (5.5313e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [74][210/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.9480e-03 (5.4402e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [74][220/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.4862e-03 (5.3321e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [74][230/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.8838e-03 (5.2956e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [74][240/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.5297e-02 (5.4351e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [74][250/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.1826e-02 (5.3988e-03)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [74][260/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.8614e-03 (5.3825e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [74][270/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.4145e-03 (5.3572e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [74][280/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.7649e-03 (5.3236e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [74][290/391]	Time  0.038 ( 0.038)	Data  0.002 ( 0.003)	Loss 2.7044e-03 (5.3701e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [74][300/391]	Time  0.036 ( 0.038)	Data  0.002 ( 0.003)	Loss 2.5475e-03 (5.3573e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [74][310/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.5914e-03 (5.2995e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [74][320/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.3919e-03 (5.2936e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [74][330/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.7930e-03 (5.3119e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [74][340/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.4540e-02 (5.3711e-03)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [74][350/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.5992e-03 (5.3789e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [74][360/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.0682e-02 (5.5008e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [74][370/391]	Time  0.037 ( 0.038)	Data  0.002 ( 0.003)	Loss 4.0191e-03 (5.5038e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [74][380/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.5380e-03 (5.4538e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [74][390/391]	Time  0.034 ( 0.038)	Data  0.000 ( 0.003)	Loss 2.4085e-03 (5.4456e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
## e[74] optimizer.zero_grad (sum) time: 0.17701029777526855
## e[74]       loss.backward (sum) time: 3.0219926834106445
## e[74]      optimizer.step (sum) time: 1.046767234802246
## epoch[74] training(only) time: 15.160998344421387
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 2.4464e-01 (2.4464e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.023 ( 0.034)	Loss 3.9770e-01 (2.8703e-01)	Acc@1  89.00 ( 93.36)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.019 ( 0.026)	Loss 4.1124e-01 (2.9937e-01)	Acc@1  90.00 ( 93.29)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.016 ( 0.024)	Loss 2.7651e-01 (2.9997e-01)	Acc@1  91.00 ( 93.32)	Acc@5 100.00 ( 99.87)
Test: [ 40/100]	Time  0.017 ( 0.023)	Loss 2.3950e-01 (2.9900e-01)	Acc@1  94.00 ( 93.39)	Acc@5 100.00 ( 99.85)
Test: [ 50/100]	Time  0.020 ( 0.022)	Loss 8.4403e-02 (3.0488e-01)	Acc@1  96.00 ( 93.39)	Acc@5 100.00 ( 99.84)
Test: [ 60/100]	Time  0.022 ( 0.022)	Loss 3.1270e-01 (2.9661e-01)	Acc@1  92.00 ( 93.33)	Acc@5 100.00 ( 99.87)
Test: [ 70/100]	Time  0.016 ( 0.021)	Loss 6.1574e-01 (2.8938e-01)	Acc@1  90.00 ( 93.41)	Acc@5  99.00 ( 99.85)
Test: [ 80/100]	Time  0.018 ( 0.021)	Loss 2.0905e-01 (2.9327e-01)	Acc@1  94.00 ( 93.41)	Acc@5 100.00 ( 99.83)
Test: [ 90/100]	Time  0.018 ( 0.020)	Loss 2.0490e-01 (2.8916e-01)	Acc@1  93.00 ( 93.43)	Acc@5 100.00 ( 99.85)
 * Acc@1 93.430 Acc@5 99.840
### epoch[74] execution time: 17.33306646347046
EPOCH 75
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [75][  0/391]	Time  0.213 ( 0.213)	Data  0.177 ( 0.177)	Loss 2.0158e-03 (2.0158e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [75][ 10/391]	Time  0.037 ( 0.053)	Data  0.001 ( 0.018)	Loss 5.1717e-03 (3.8707e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [75][ 20/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.010)	Loss 1.1826e-03 (6.0383e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [75][ 30/391]	Time  0.037 ( 0.043)	Data  0.001 ( 0.008)	Loss 1.6343e-02 (5.6880e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [75][ 40/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.006)	Loss 1.3650e-02 (5.8183e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [75][ 50/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.006)	Loss 2.7894e-03 (6.0394e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [75][ 60/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.005)	Loss 1.0009e-02 (6.0326e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [75][ 70/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.005)	Loss 9.5997e-03 (5.9158e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [75][ 80/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.7399e-03 (6.0106e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [75][ 90/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 5.3070e-03 (6.1547e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [75][100/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.9198e-03 (6.1271e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [75][110/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 9.2366e-04 (5.9745e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [75][120/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.2737e-03 (5.8346e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [75][130/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.004)	Loss 9.5299e-04 (5.8130e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [75][140/391]	Time  0.036 ( 0.039)	Data  0.002 ( 0.003)	Loss 1.0620e-02 (5.7580e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [75][150/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.2862e-03 (5.7905e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [75][160/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.3674e-03 (5.8697e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [75][170/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.8900e-03 (5.7623e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [75][180/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.9245e-03 (5.8310e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [75][190/391]	Time  0.044 ( 0.039)	Data  0.002 ( 0.003)	Loss 4.8916e-03 (5.8515e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [75][200/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.0747e-03 (5.8693e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [75][210/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.4659e-03 (5.8184e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [75][220/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.5009e-03 (5.7764e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [75][230/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.5207e-03 (5.6851e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [75][240/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.0949e-03 (5.5804e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [75][250/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.0403e-02 (5.6529e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [75][260/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.9152e-03 (5.6202e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [75][270/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.8039e-03 (5.5926e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [75][280/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.7121e-03 (5.5220e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [75][290/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.2630e-03 (5.5283e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [75][300/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.4344e-03 (5.5282e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [75][310/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.3836e-03 (5.5627e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [75][320/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.3902e-03 (5.5931e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [75][330/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.3713e-03 (5.5526e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [75][340/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.0103e-03 (5.5016e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [75][350/391]	Time  0.038 ( 0.038)	Data  0.002 ( 0.003)	Loss 2.8576e-03 (5.4927e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [75][360/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 9.2104e-03 (5.4672e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [75][370/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.6964e-03 (5.4061e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [75][380/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.3973e-03 (5.4216e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [75][390/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.6335e-03 (5.4102e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
## e[75] optimizer.zero_grad (sum) time: 0.17479586601257324
## e[75]       loss.backward (sum) time: 2.976262092590332
## e[75]      optimizer.step (sum) time: 1.0260281562805176
## epoch[75] training(only) time: 15.145561695098877
# Switched to evaluate mode...
Test: [  0/100]	Time  0.174 ( 0.174)	Loss 2.2164e-01 (2.2164e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.017 ( 0.032)	Loss 3.9765e-01 (2.8098e-01)	Acc@1  89.00 ( 93.18)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.029 ( 0.026)	Loss 3.8577e-01 (2.9417e-01)	Acc@1  91.00 ( 93.19)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.017 ( 0.024)	Loss 2.7214e-01 (2.9768e-01)	Acc@1  91.00 ( 93.29)	Acc@5  99.00 ( 99.81)
Test: [ 40/100]	Time  0.016 ( 0.022)	Loss 2.1872e-01 (2.9668e-01)	Acc@1  94.00 ( 93.44)	Acc@5 100.00 ( 99.80)
Test: [ 50/100]	Time  0.019 ( 0.022)	Loss 8.2485e-02 (3.0511e-01)	Acc@1  96.00 ( 93.35)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.016 ( 0.021)	Loss 3.3913e-01 (2.9784e-01)	Acc@1  92.00 ( 93.34)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.016 ( 0.021)	Loss 6.5130e-01 (2.9080e-01)	Acc@1  90.00 ( 93.39)	Acc@5  99.00 ( 99.77)
Test: [ 80/100]	Time  0.021 ( 0.021)	Loss 2.0042e-01 (2.9514e-01)	Acc@1  94.00 ( 93.37)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.021 ( 0.020)	Loss 1.9235e-01 (2.9112e-01)	Acc@1  93.00 ( 93.40)	Acc@5 100.00 ( 99.79)
 * Acc@1 93.470 Acc@5 99.790
### epoch[75] execution time: 17.31526494026184
EPOCH 76
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [76][  0/391]	Time  0.216 ( 0.216)	Data  0.182 ( 0.182)	Loss 4.2508e-03 (4.2508e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [76][ 10/391]	Time  0.037 ( 0.054)	Data  0.001 ( 0.018)	Loss 5.8973e-03 (7.7542e-03)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [76][ 20/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.011)	Loss 1.4267e-02 (7.5189e-03)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [76][ 30/391]	Time  0.037 ( 0.044)	Data  0.002 ( 0.008)	Loss 2.3611e-03 (6.6098e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [76][ 40/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.007)	Loss 1.8834e-02 (6.8173e-03)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [76][ 50/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.006)	Loss 3.9150e-03 (6.9415e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [76][ 60/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.005)	Loss 1.0481e-03 (6.5244e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [76][ 70/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.005)	Loss 1.5135e-03 (5.8504e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [76][ 80/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.9430e-02 (5.8071e-03)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [76][ 90/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.1302e-03 (5.4547e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [76][100/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.004)	Loss 8.6104e-03 (5.6110e-03)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [76][110/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.2517e-03 (5.5698e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [76][120/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.8895e-03 (5.5557e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [76][130/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 3.5291e-03 (5.5683e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [76][140/391]	Time  0.036 ( 0.039)	Data  0.002 ( 0.003)	Loss 9.1998e-03 (5.5280e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [76][150/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.1476e-03 (5.6510e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [76][160/391]	Time  0.035 ( 0.039)	Data  0.002 ( 0.003)	Loss 2.2563e-03 (5.6288e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [76][170/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 9.0664e-03 (5.6099e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [76][180/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.3398e-03 (5.5087e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [76][190/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.4491e-04 (5.5492e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [76][200/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.3547e-03 (5.4404e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [76][210/391]	Time  0.040 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.1027e-03 (5.3418e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [76][220/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.6089e-03 (5.2921e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [76][230/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.3139e-03 (5.2072e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [76][240/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.1081e-03 (5.1923e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [76][250/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.2367e-03 (5.0888e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [76][260/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.5929e-02 (5.1604e-03)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [76][270/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.1422e-04 (5.0638e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [76][280/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.8432e-03 (5.0853e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [76][290/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.5423e-03 (5.1512e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [76][300/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.5041e-03 (5.1665e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [76][310/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.4398e-02 (5.1842e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [76][320/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.0453e-03 (5.1563e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [76][330/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.5199e-03 (5.2345e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [76][340/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.6822e-03 (5.2176e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [76][350/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.2414e-03 (5.2619e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [76][360/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.0748e-03 (5.2030e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [76][370/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.5906e-03 (5.2135e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [76][380/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.4229e-02 (5.1899e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [76][390/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.3071e-02 (5.2599e-03)	Acc@1  97.50 ( 99.90)	Acc@5 100.00 (100.00)
## e[76] optimizer.zero_grad (sum) time: 0.17490077018737793
## e[76]       loss.backward (sum) time: 2.98354434967041
## e[76]      optimizer.step (sum) time: 1.0203776359558105
## epoch[76] training(only) time: 15.108045101165771
# Switched to evaluate mode...
Test: [  0/100]	Time  0.179 ( 0.179)	Loss 2.2062e-01 (2.2062e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.019 ( 0.035)	Loss 4.1724e-01 (2.8027e-01)	Acc@1  88.00 ( 93.09)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.026 ( 0.027)	Loss 4.1070e-01 (2.9542e-01)	Acc@1  91.00 ( 93.10)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.020 ( 0.024)	Loss 2.6855e-01 (2.9970e-01)	Acc@1  93.00 ( 93.26)	Acc@5  99.00 ( 99.81)
Test: [ 40/100]	Time  0.017 ( 0.023)	Loss 2.2343e-01 (3.0029e-01)	Acc@1  93.00 ( 93.27)	Acc@5 100.00 ( 99.80)
Test: [ 50/100]	Time  0.019 ( 0.022)	Loss 9.4277e-02 (3.0648e-01)	Acc@1  95.00 ( 93.25)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.016 ( 0.021)	Loss 3.2167e-01 (2.9856e-01)	Acc@1  92.00 ( 93.33)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.018 ( 0.021)	Loss 6.4049e-01 (2.9147e-01)	Acc@1  90.00 ( 93.38)	Acc@5  99.00 ( 99.77)
Test: [ 80/100]	Time  0.023 ( 0.021)	Loss 1.7803e-01 (2.9436e-01)	Acc@1  95.00 ( 93.40)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.020 ( 0.021)	Loss 1.8059e-01 (2.9019e-01)	Acc@1  95.00 ( 93.44)	Acc@5 100.00 ( 99.78)
 * Acc@1 93.520 Acc@5 99.780
### epoch[76] execution time: 17.26925778388977
EPOCH 77
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [77][  0/391]	Time  0.222 ( 0.222)	Data  0.187 ( 0.187)	Loss 6.4696e-03 (6.4696e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [77][ 10/391]	Time  0.038 ( 0.055)	Data  0.001 ( 0.019)	Loss 2.9524e-03 (2.9074e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [77][ 20/391]	Time  0.036 ( 0.047)	Data  0.001 ( 0.011)	Loss 9.4471e-04 (3.3456e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [77][ 30/391]	Time  0.036 ( 0.044)	Data  0.001 ( 0.008)	Loss 1.9460e-03 (3.5303e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [77][ 40/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.007)	Loss 6.3467e-03 (3.6606e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [77][ 50/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.006)	Loss 3.6367e-03 (4.3143e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [77][ 60/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.005)	Loss 5.1670e-03 (4.2268e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [77][ 70/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.005)	Loss 8.7491e-04 (4.4097e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [77][ 80/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.5032e-03 (5.2098e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [77][ 90/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.5397e-03 (5.0668e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [77][100/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.4728e-03 (4.8232e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [77][110/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.004)	Loss 6.3610e-03 (5.0441e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [77][120/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.2856e-02 (5.5123e-03)	Acc@1  98.44 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [77][130/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.1771e-03 (5.3054e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [77][140/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.004)	Loss 5.7453e-03 (5.2161e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [77][150/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 9.3394e-03 (5.3166e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [77][160/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.0671e-03 (5.1736e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [77][170/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 9.1396e-03 (5.2066e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [77][180/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.4891e-03 (5.1158e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [77][190/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.7652e-03 (5.1444e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [77][200/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.5788e-03 (5.1268e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [77][210/391]	Time  0.040 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.1445e-04 (5.0220e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [77][220/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.1878e-04 (4.9924e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [77][230/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.4543e-03 (4.9671e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [77][240/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.2141e-03 (4.9443e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [77][250/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.6573e-03 (4.9473e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [77][260/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.4078e-03 (5.0783e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [77][270/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 9.6375e-04 (5.0749e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [77][280/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.3887e-03 (5.1260e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [77][290/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.0933e-03 (5.0778e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [77][300/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.6899e-02 (5.1005e-03)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [77][310/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.3522e-02 (5.0740e-03)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [77][320/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.6841e-03 (5.1210e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [77][330/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.7645e-03 (5.0823e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [77][340/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.5310e-03 (5.0249e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [77][350/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.1924e-03 (5.0425e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [77][360/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.2320e-03 (5.0054e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [77][370/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.2848e-02 (5.0831e-03)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [77][380/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.9356e-03 (5.0798e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [77][390/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.5432e-03 (5.1152e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
## e[77] optimizer.zero_grad (sum) time: 0.1733708381652832
## e[77]       loss.backward (sum) time: 2.99114727973938
## e[77]      optimizer.step (sum) time: 1.0186407566070557
## epoch[77] training(only) time: 15.162298202514648
# Switched to evaluate mode...
Test: [  0/100]	Time  0.175 ( 0.175)	Loss 2.3935e-01 (2.3935e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.018 ( 0.034)	Loss 4.1640e-01 (2.8348e-01)	Acc@1  88.00 ( 93.09)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.020 ( 0.027)	Loss 3.9542e-01 (2.9712e-01)	Acc@1  91.00 ( 93.19)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.016 ( 0.024)	Loss 2.7729e-01 (3.0072e-01)	Acc@1  93.00 ( 93.23)	Acc@5  99.00 ( 99.81)
Test: [ 40/100]	Time  0.019 ( 0.023)	Loss 2.3123e-01 (2.9995e-01)	Acc@1  94.00 ( 93.37)	Acc@5 100.00 ( 99.80)
Test: [ 50/100]	Time  0.017 ( 0.022)	Loss 8.6192e-02 (3.0629e-01)	Acc@1  97.00 ( 93.41)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.016 ( 0.022)	Loss 3.2210e-01 (2.9777e-01)	Acc@1  92.00 ( 93.34)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.016 ( 0.021)	Loss 6.3421e-01 (2.9030e-01)	Acc@1  90.00 ( 93.41)	Acc@5  99.00 ( 99.76)
Test: [ 80/100]	Time  0.020 ( 0.021)	Loss 1.9993e-01 (2.9377e-01)	Acc@1  94.00 ( 93.40)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.019 ( 0.021)	Loss 2.0725e-01 (2.9024e-01)	Acc@1  93.00 ( 93.42)	Acc@5 100.00 ( 99.77)
 * Acc@1 93.470 Acc@5 99.770
### epoch[77] execution time: 17.355732440948486
EPOCH 78
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [78][  0/391]	Time  0.224 ( 0.224)	Data  0.185 ( 0.185)	Loss 1.2044e-02 (1.2044e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [78][ 10/391]	Time  0.039 ( 0.055)	Data  0.001 ( 0.018)	Loss 1.5491e-03 (3.8764e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [78][ 20/391]	Time  0.038 ( 0.047)	Data  0.001 ( 0.011)	Loss 2.7225e-03 (5.5881e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [78][ 30/391]	Time  0.037 ( 0.044)	Data  0.001 ( 0.008)	Loss 1.1315e-02 (6.0215e-03)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [78][ 40/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.007)	Loss 1.6724e-03 (5.5666e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [78][ 50/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.006)	Loss 2.5029e-03 (5.6386e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [78][ 60/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.005)	Loss 1.9488e-03 (5.4602e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][ 70/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.005)	Loss 1.9406e-03 (5.5154e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][ 80/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.4226e-03 (5.1890e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [78][ 90/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.4074e-02 (5.1664e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [78][100/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 9.1846e-03 (5.1503e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [78][110/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 3.3126e-03 (5.3619e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [78][120/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 3.9137e-03 (5.3968e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [78][130/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.1823e-02 (5.3916e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][140/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.5126e-03 (5.4379e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][150/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.9266e-03 (5.5111e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [78][160/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 9.6759e-03 (5.4836e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [78][170/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.4158e-03 (5.3852e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [78][180/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.9273e-03 (5.5271e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [78][190/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.7787e-03 (5.4310e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [78][200/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.6516e-03 (5.3654e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][210/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.7678e-02 (5.4337e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [78][220/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.4816e-03 (5.3442e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [78][230/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.5215e-03 (5.2958e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [78][240/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.5873e-03 (5.2190e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][250/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.3968e-03 (5.2202e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][260/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.5675e-03 (5.2152e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][270/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.4905e-03 (5.2364e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][280/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.6246e-03 (5.1886e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][290/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 9.7872e-03 (5.2792e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][300/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.9836e-03 (5.2917e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][310/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.5756e-03 (5.2063e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][320/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.2237e-03 (5.1600e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [78][330/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.6585e-03 (5.1846e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][340/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.9408e-03 (5.1922e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][350/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.6492e-03 (5.1941e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [78][360/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 9.8587e-03 (5.2066e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][370/391]	Time  0.043 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.5717e-03 (5.1702e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][380/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.0309e-03 (5.1247e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [78][390/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.1457e-03 (5.2048e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
## e[78] optimizer.zero_grad (sum) time: 0.17436814308166504
## e[78]       loss.backward (sum) time: 2.9762790203094482
## e[78]      optimizer.step (sum) time: 1.019141435623169
## epoch[78] training(only) time: 15.134963989257812
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 2.6172e-01 (2.6172e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.016 ( 0.034)	Loss 3.8327e-01 (2.8894e-01)	Acc@1  90.00 ( 93.27)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.016 ( 0.026)	Loss 3.9115e-01 (3.0004e-01)	Acc@1  91.00 ( 93.33)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.023 ( 0.024)	Loss 2.9135e-01 (3.0278e-01)	Acc@1  92.00 ( 93.35)	Acc@5  99.00 ( 99.84)
Test: [ 40/100]	Time  0.021 ( 0.023)	Loss 2.4129e-01 (3.0229e-01)	Acc@1  93.00 ( 93.41)	Acc@5 100.00 ( 99.83)
Test: [ 50/100]	Time  0.023 ( 0.022)	Loss 1.0100e-01 (3.0730e-01)	Acc@1  96.00 ( 93.41)	Acc@5 100.00 ( 99.82)
Test: [ 60/100]	Time  0.015 ( 0.022)	Loss 3.0235e-01 (2.9968e-01)	Acc@1  92.00 ( 93.41)	Acc@5 100.00 ( 99.84)
Test: [ 70/100]	Time  0.020 ( 0.021)	Loss 6.3620e-01 (2.9221e-01)	Acc@1  90.00 ( 93.51)	Acc@5  99.00 ( 99.82)
Test: [ 80/100]	Time  0.020 ( 0.021)	Loss 1.9291e-01 (2.9613e-01)	Acc@1  95.00 ( 93.51)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.016 ( 0.021)	Loss 2.0105e-01 (2.9215e-01)	Acc@1  93.00 ( 93.53)	Acc@5 100.00 ( 99.81)
 * Acc@1 93.580 Acc@5 99.810
### epoch[78] execution time: 17.30921983718872
EPOCH 79
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [79][  0/391]	Time  0.214 ( 0.214)	Data  0.175 ( 0.175)	Loss 1.8490e-03 (1.8490e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [79][ 10/391]	Time  0.036 ( 0.054)	Data  0.001 ( 0.018)	Loss 6.0371e-04 (4.7393e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [79][ 20/391]	Time  0.037 ( 0.046)	Data  0.001 ( 0.010)	Loss 8.6371e-03 (4.8243e-03)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [79][ 30/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.008)	Loss 1.4217e-02 (5.7902e-03)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [79][ 40/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.006)	Loss 3.0146e-03 (6.1101e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [79][ 50/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.006)	Loss 1.6714e-03 (5.8507e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [79][ 60/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.005)	Loss 3.1731e-03 (5.4330e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [79][ 70/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.005)	Loss 2.0926e-03 (5.4221e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [79][ 80/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 4.6345e-03 (5.2579e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [79][ 90/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 3.7450e-03 (5.6064e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [79][100/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.004)	Loss 6.9557e-03 (5.5347e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [79][110/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 3.5768e-03 (5.3017e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [79][120/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 8.5602e-03 (5.4874e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [79][130/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 5.9568e-03 (5.3516e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [79][140/391]	Time  0.040 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.8083e-03 (5.1793e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [79][150/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.0439e-02 (5.2576e-03)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [79][160/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.6394e-03 (5.3164e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [79][170/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.4837e-03 (5.3439e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [79][180/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.7483e-03 (5.2630e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [79][190/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.8587e-03 (5.3787e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [79][200/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.7441e-03 (5.3890e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [79][210/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.8101e-03 (5.2979e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [79][220/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.2900e-02 (5.2886e-03)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [79][230/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.8546e-03 (5.2547e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [79][240/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.4722e-03 (5.3675e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [79][250/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.1204e-03 (5.3304e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [79][260/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.4215e-03 (5.3044e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [79][270/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.0679e-03 (5.4173e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [79][280/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.6770e-03 (5.3752e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [79][290/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.3833e-03 (5.4562e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [79][300/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.9032e-04 (5.3504e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [79][310/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.5901e-03 (5.3485e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [79][320/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.7929e-03 (5.2990e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [79][330/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.6865e-04 (5.1973e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [79][340/391]	Time  0.041 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.8914e-03 (5.2037e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [79][350/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.8048e-03 (5.1915e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [79][360/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.8111e-03 (5.1805e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [79][370/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.2348e-03 (5.1979e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [79][380/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.3732e-02 (5.2585e-03)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [79][390/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.0008e-02 (5.2344e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
## e[79] optimizer.zero_grad (sum) time: 0.1748671531677246
## e[79]       loss.backward (sum) time: 2.967989683151245
## e[79]      optimizer.step (sum) time: 1.0220212936401367
## epoch[79] training(only) time: 15.124433040618896
# Switched to evaluate mode...
Test: [  0/100]	Time  0.171 ( 0.171)	Loss 2.6797e-01 (2.6797e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.022 ( 0.033)	Loss 4.1205e-01 (2.8765e-01)	Acc@1  90.00 ( 93.00)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.016 ( 0.026)	Loss 3.6816e-01 (2.9768e-01)	Acc@1  91.00 ( 93.00)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.015 ( 0.023)	Loss 2.9361e-01 (3.0157e-01)	Acc@1  93.00 ( 93.32)	Acc@5  99.00 ( 99.81)
Test: [ 40/100]	Time  0.017 ( 0.022)	Loss 2.3337e-01 (3.0114e-01)	Acc@1  93.00 ( 93.34)	Acc@5 100.00 ( 99.78)
Test: [ 50/100]	Time  0.018 ( 0.022)	Loss 1.0490e-01 (3.0664e-01)	Acc@1  97.00 ( 93.37)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.023 ( 0.021)	Loss 3.3844e-01 (3.0035e-01)	Acc@1  92.00 ( 93.39)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.016 ( 0.021)	Loss 6.3810e-01 (2.9303e-01)	Acc@1  90.00 ( 93.54)	Acc@5  98.00 ( 99.76)
Test: [ 80/100]	Time  0.016 ( 0.020)	Loss 1.9841e-01 (2.9621e-01)	Acc@1  95.00 ( 93.52)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.020 ( 0.020)	Loss 1.6881e-01 (2.9202e-01)	Acc@1  94.00 ( 93.52)	Acc@5 100.00 ( 99.78)
 * Acc@1 93.620 Acc@5 99.780
### epoch[79] execution time: 17.25761604309082
EPOCH 80
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [80][  0/391]	Time  0.216 ( 0.216)	Data  0.180 ( 0.180)	Loss 5.3762e-03 (5.3762e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [80][ 10/391]	Time  0.038 ( 0.053)	Data  0.001 ( 0.018)	Loss 1.3180e-03 (5.5349e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [80][ 20/391]	Time  0.035 ( 0.046)	Data  0.001 ( 0.011)	Loss 5.7948e-03 (5.2370e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [80][ 30/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.008)	Loss 1.8461e-03 (4.5013e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [80][ 40/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.007)	Loss 2.8383e-03 (4.7819e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [80][ 50/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.006)	Loss 4.1515e-03 (4.5865e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [80][ 60/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.005)	Loss 3.9993e-03 (4.4951e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [80][ 70/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.005)	Loss 1.9502e-03 (4.3880e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [80][ 80/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.004)	Loss 8.8701e-04 (4.6116e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [80][ 90/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 4.3270e-03 (4.7098e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [80][100/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 5.9159e-03 (4.6193e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [80][110/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.5499e-03 (5.0837e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [80][120/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 3.9105e-03 (5.1497e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [80][130/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 7.7786e-03 (5.1188e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [80][140/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 3.0553e-03 (5.0627e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [80][150/391]	Time  0.042 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.2421e-03 (5.0930e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [80][160/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.6020e-03 (5.0150e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [80][170/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.0045e-03 (5.0321e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [80][180/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.8953e-03 (4.9177e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [80][190/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.8780e-03 (4.9171e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [80][200/391]	Time  0.036 ( 0.039)	Data  0.002 ( 0.003)	Loss 6.6862e-03 (5.0680e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [80][210/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.4628e-04 (5.0134e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [80][220/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.5790e-03 (4.9311e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [80][230/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.4714e-03 (4.8510e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [80][240/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.6849e-03 (4.8579e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [80][250/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.1077e-03 (4.8031e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [80][260/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.3804e-02 (4.9115e-03)	Acc@1  99.22 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [80][270/391]	Time  0.040 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.6156e-03 (4.9473e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [80][280/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.0570e-03 (4.8710e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [80][290/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.6022e-02 (4.8549e-03)	Acc@1  99.22 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [80][300/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 9.3948e-04 (4.8671e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [80][310/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.5772e-03 (4.8667e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [80][320/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.0575e-03 (4.8441e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [80][330/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.3227e-03 (4.8233e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [80][340/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.9157e-03 (4.8441e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [80][350/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.6083e-03 (4.8731e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [80][360/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.9695e-03 (4.8518e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [80][370/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.1426e-03 (4.8517e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [80][380/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.9087e-03 (4.9323e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [80][390/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.0854e-04 (4.9065e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
## e[80] optimizer.zero_grad (sum) time: 0.17517638206481934
## e[80]       loss.backward (sum) time: 2.964155912399292
## e[80]      optimizer.step (sum) time: 1.0362417697906494
## epoch[80] training(only) time: 15.116563558578491
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 2.6024e-01 (2.6024e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.016 ( 0.034)	Loss 4.0713e-01 (2.8119e-01)	Acc@1  88.00 ( 93.27)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.023 ( 0.027)	Loss 3.6923e-01 (2.9326e-01)	Acc@1  91.00 ( 93.33)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.016 ( 0.024)	Loss 2.8749e-01 (2.9815e-01)	Acc@1  93.00 ( 93.32)	Acc@5  99.00 ( 99.84)
Test: [ 40/100]	Time  0.020 ( 0.023)	Loss 2.3464e-01 (2.9839e-01)	Acc@1  93.00 ( 93.39)	Acc@5 100.00 ( 99.83)
Test: [ 50/100]	Time  0.018 ( 0.022)	Loss 9.8166e-02 (3.0435e-01)	Acc@1  96.00 ( 93.41)	Acc@5 100.00 ( 99.80)
Test: [ 60/100]	Time  0.019 ( 0.021)	Loss 3.3126e-01 (2.9724e-01)	Acc@1  91.00 ( 93.39)	Acc@5 100.00 ( 99.82)
Test: [ 70/100]	Time  0.017 ( 0.021)	Loss 6.3232e-01 (2.9002e-01)	Acc@1  90.00 ( 93.52)	Acc@5  98.00 ( 99.79)
Test: [ 80/100]	Time  0.020 ( 0.021)	Loss 2.1079e-01 (2.9374e-01)	Acc@1  95.00 ( 93.53)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.020 ( 0.020)	Loss 1.7716e-01 (2.9019e-01)	Acc@1  94.00 ( 93.54)	Acc@5 100.00 ( 99.80)
 * Acc@1 93.610 Acc@5 99.800
### epoch[80] execution time: 17.26096534729004
EPOCH 81
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [81][  0/391]	Time  0.218 ( 0.218)	Data  0.182 ( 0.182)	Loss 3.9646e-03 (3.9646e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [81][ 10/391]	Time  0.037 ( 0.054)	Data  0.001 ( 0.019)	Loss 9.0479e-03 (4.9203e-03)	Acc@1  99.22 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [81][ 20/391]	Time  0.038 ( 0.046)	Data  0.001 ( 0.011)	Loss 3.9986e-03 (5.0737e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [81][ 30/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.008)	Loss 7.6279e-03 (4.3150e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [81][ 40/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.007)	Loss 1.0851e-02 (4.4279e-03)	Acc@1  99.22 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [81][ 50/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.006)	Loss 3.6731e-03 (4.0610e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [81][ 60/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.005)	Loss 1.2251e-03 (4.7329e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [81][ 70/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.005)	Loss 1.0469e-03 (4.7434e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [81][ 80/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.4958e-03 (4.9514e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [81][ 90/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.7309e-03 (4.9239e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [81][100/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.5342e-03 (5.1503e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [81][110/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 9.0405e-04 (5.1644e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [81][120/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 3.9732e-03 (5.2021e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [81][130/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.3396e-03 (5.0605e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [81][140/391]	Time  0.037 ( 0.039)	Data  0.002 ( 0.003)	Loss 1.5779e-03 (4.9140e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [81][150/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.8911e-03 (4.9563e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [81][160/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.2935e-02 (4.9201e-03)	Acc@1  99.22 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [81][170/391]	Time  0.040 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.1339e-03 (4.8199e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [81][180/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.1874e-02 (4.8345e-03)	Acc@1  99.22 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [81][190/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.6539e-03 (4.9139e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [81][200/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.7019e-04 (4.8301e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [81][210/391]	Time  0.040 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.3014e-03 (4.7584e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [81][220/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.6736e-03 (4.6836e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [81][230/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.6868e-03 (4.6041e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [81][240/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.3667e-03 (4.7348e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [81][250/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.6536e-03 (4.7989e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [81][260/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.1675e-03 (4.7822e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [81][270/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.1810e-03 (4.7834e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [81][280/391]	Time  0.043 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.3811e-03 (4.7301e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [81][290/391]	Time  0.039 ( 0.038)	Data  0.002 ( 0.003)	Loss 1.1694e-02 (4.7274e-03)	Acc@1  99.22 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [81][300/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.3058e-03 (4.6898e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [81][310/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.1077e-03 (4.6457e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [81][320/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.2139e-03 (4.7229e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [81][330/391]	Time  0.036 ( 0.038)	Data  0.002 ( 0.003)	Loss 4.9075e-04 (4.6374e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [81][340/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.6118e-03 (4.6101e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [81][350/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.4620e-03 (4.6890e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [81][360/391]	Time  0.039 ( 0.038)	Data  0.003 ( 0.003)	Loss 9.0261e-03 (4.6956e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [81][370/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.3617e-02 (4.7222e-03)	Acc@1  99.22 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [81][380/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.9056e-03 (4.6879e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [81][390/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.1934e-02 (4.7646e-03)	Acc@1  98.75 ( 99.90)	Acc@5 100.00 (100.00)
## e[81] optimizer.zero_grad (sum) time: 0.1751854419708252
## e[81]       loss.backward (sum) time: 2.9854559898376465
## e[81]      optimizer.step (sum) time: 1.0261735916137695
## epoch[81] training(only) time: 15.130316257476807
# Switched to evaluate mode...
Test: [  0/100]	Time  0.178 ( 0.178)	Loss 2.6077e-01 (2.6077e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.018 ( 0.033)	Loss 3.9451e-01 (2.8789e-01)	Acc@1  89.00 ( 93.09)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.024 ( 0.027)	Loss 3.9791e-01 (3.0028e-01)	Acc@1  91.00 ( 93.24)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.025 ( 0.024)	Loss 2.9159e-01 (3.0259e-01)	Acc@1  93.00 ( 93.32)	Acc@5  99.00 ( 99.81)
Test: [ 40/100]	Time  0.022 ( 0.023)	Loss 2.4327e-01 (3.0261e-01)	Acc@1  92.00 ( 93.37)	Acc@5 100.00 ( 99.80)
Test: [ 50/100]	Time  0.021 ( 0.022)	Loss 7.5862e-02 (3.0769e-01)	Acc@1  97.00 ( 93.37)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.021 ( 0.022)	Loss 3.2345e-01 (2.9918e-01)	Acc@1  92.00 ( 93.36)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.028 ( 0.022)	Loss 5.9986e-01 (2.9110e-01)	Acc@1  90.00 ( 93.42)	Acc@5  99.00 ( 99.77)
Test: [ 80/100]	Time  0.021 ( 0.021)	Loss 1.9403e-01 (2.9384e-01)	Acc@1  95.00 ( 93.44)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.020 ( 0.021)	Loss 2.0915e-01 (2.9037e-01)	Acc@1  93.00 ( 93.46)	Acc@5 100.00 ( 99.78)
 * Acc@1 93.530 Acc@5 99.780
### epoch[81] execution time: 17.332025051116943
EPOCH 82
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [82][  0/391]	Time  0.214 ( 0.214)	Data  0.180 ( 0.180)	Loss 7.7716e-03 (7.7716e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [82][ 10/391]	Time  0.037 ( 0.053)	Data  0.001 ( 0.018)	Loss 1.6546e-03 (3.8281e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [82][ 20/391]	Time  0.036 ( 0.046)	Data  0.001 ( 0.011)	Loss 4.7709e-03 (3.9219e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [82][ 30/391]	Time  0.037 ( 0.043)	Data  0.001 ( 0.008)	Loss 2.0539e-03 (4.3438e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [82][ 40/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.007)	Loss 3.2536e-04 (4.7113e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [82][ 50/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.006)	Loss 1.3293e-02 (4.8815e-03)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [82][ 60/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.005)	Loss 1.0879e-03 (5.1089e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [82][ 70/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.005)	Loss 1.7070e-03 (5.0709e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [82][ 80/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.004)	Loss 6.8593e-03 (4.8090e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [82][ 90/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.004)	Loss 3.4581e-03 (4.7389e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [82][100/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.004)	Loss 3.6886e-03 (4.6676e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [82][110/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.004)	Loss 3.7456e-03 (4.6126e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [82][120/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.1946e-03 (4.5331e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [82][130/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.0902e-03 (4.6438e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [82][140/391]	Time  0.045 ( 0.039)	Data  0.002 ( 0.003)	Loss 3.0076e-03 (4.5098e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [82][150/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.1862e-02 (4.5821e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [82][160/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.0227e-03 (4.5687e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [82][170/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.6343e-03 (4.5683e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [82][180/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.0234e-03 (4.6388e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [82][190/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.0412e-03 (4.6020e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [82][200/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.4500e-03 (4.6109e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [82][210/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.9481e-02 (4.7182e-03)	Acc@1  98.44 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [82][220/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.9925e-03 (4.8052e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [82][230/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.4387e-02 (4.9252e-03)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [82][240/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.0788e-03 (4.9236e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [82][250/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.6685e-03 (4.8227e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [82][260/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.4469e-03 (4.8692e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [82][270/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.1808e-04 (4.8716e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [82][280/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.4164e-03 (5.0795e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [82][290/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.3608e-03 (5.2180e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [82][300/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.0524e-02 (5.2085e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [82][310/391]	Time  0.043 ( 0.039)	Data  0.004 ( 0.003)	Loss 1.0442e-03 (5.1345e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [82][320/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.0037e-03 (5.1518e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [82][330/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.4749e-03 (5.1809e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [82][340/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.4425e-03 (5.1993e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [82][350/391]	Time  0.040 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.2708e-03 (5.1918e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [82][360/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.0361e-03 (5.1224e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [82][370/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.2400e-03 (5.1402e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [82][380/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.6016e-03 (5.0623e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [82][390/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.1655e-03 (5.0108e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
## e[82] optimizer.zero_grad (sum) time: 0.1752924919128418
## e[82]       loss.backward (sum) time: 2.9905176162719727
## e[82]      optimizer.step (sum) time: 1.0317749977111816
## epoch[82] training(only) time: 15.220003128051758
# Switched to evaluate mode...
Test: [  0/100]	Time  0.177 ( 0.177)	Loss 2.7354e-01 (2.7354e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.016 ( 0.033)	Loss 3.8844e-01 (2.8510e-01)	Acc@1  89.00 ( 93.27)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.023 ( 0.026)	Loss 3.8857e-01 (2.9812e-01)	Acc@1  91.00 ( 93.33)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.019 ( 0.024)	Loss 2.9269e-01 (3.0283e-01)	Acc@1  93.00 ( 93.32)	Acc@5 100.00 ( 99.87)
Test: [ 40/100]	Time  0.021 ( 0.022)	Loss 2.4053e-01 (3.0313e-01)	Acc@1  94.00 ( 93.44)	Acc@5  99.00 ( 99.80)
Test: [ 50/100]	Time  0.021 ( 0.022)	Loss 9.2686e-02 (3.0974e-01)	Acc@1  97.00 ( 93.43)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.016 ( 0.021)	Loss 3.1734e-01 (3.0074e-01)	Acc@1  91.00 ( 93.36)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.023 ( 0.021)	Loss 6.4864e-01 (2.9241e-01)	Acc@1  90.00 ( 93.44)	Acc@5  98.00 ( 99.77)
Test: [ 80/100]	Time  0.016 ( 0.020)	Loss 2.0253e-01 (2.9613e-01)	Acc@1  94.00 ( 93.44)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.016 ( 0.020)	Loss 2.1973e-01 (2.9219e-01)	Acc@1  93.00 ( 93.46)	Acc@5 100.00 ( 99.79)
 * Acc@1 93.510 Acc@5 99.790
### epoch[82] execution time: 17.388822078704834
EPOCH 83
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [83][  0/391]	Time  0.217 ( 0.217)	Data  0.176 ( 0.176)	Loss 5.2655e-03 (5.2655e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [83][ 10/391]	Time  0.044 ( 0.054)	Data  0.001 ( 0.018)	Loss 1.7904e-02 (4.4489e-03)	Acc@1  99.22 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [83][ 20/391]	Time  0.036 ( 0.046)	Data  0.001 ( 0.010)	Loss 4.5622e-03 (4.7855e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [83][ 30/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.008)	Loss 2.7428e-03 (5.7378e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [83][ 40/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.006)	Loss 7.5435e-03 (5.2907e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [83][ 50/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.006)	Loss 1.6637e-02 (5.2640e-03)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [83][ 60/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.005)	Loss 3.1815e-03 (5.6136e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [83][ 70/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.005)	Loss 3.7607e-03 (5.2654e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [83][ 80/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 9.9709e-03 (5.4768e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [83][ 90/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.3395e-03 (5.8820e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [83][100/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.9363e-03 (5.8464e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [83][110/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.1070e-03 (5.5387e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [83][120/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.0571e-02 (5.3347e-03)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [83][130/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.8205e-03 (5.3706e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [83][140/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.7174e-03 (5.2679e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [83][150/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.7178e-03 (5.1497e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [83][160/391]	Time  0.049 ( 0.039)	Data  0.002 ( 0.003)	Loss 1.0266e-03 (5.0455e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [83][170/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.0345e-03 (5.0626e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [83][180/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.0513e-03 (4.8889e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [83][190/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.5780e-03 (4.8488e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [83][200/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.1038e-03 (4.8688e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [83][210/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.8363e-03 (4.8658e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [83][220/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.2451e-03 (4.7968e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [83][230/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.5034e-03 (4.8780e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [83][240/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.4655e-03 (4.8564e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [83][250/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.7230e-03 (4.8561e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [83][260/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.5832e-04 (4.8173e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [83][270/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.0292e-03 (4.7443e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [83][280/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.0871e-02 (4.8492e-03)	Acc@1  99.22 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [83][290/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.4497e-03 (5.0208e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [83][300/391]	Time  0.040 ( 0.038)	Data  0.001 ( 0.003)	Loss 9.4214e-04 (5.0024e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [83][310/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.9556e-03 (4.9825e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [83][320/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.9655e-04 (4.9099e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [83][330/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.0497e-03 (4.9102e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [83][340/391]	Time  0.040 ( 0.038)	Data  0.002 ( 0.003)	Loss 4.0369e-03 (4.8806e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [83][350/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.4355e-03 (4.8548e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [83][360/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.1316e-02 (4.8699e-03)	Acc@1  99.22 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [83][370/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.4341e-03 (4.8599e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [83][380/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.5569e-03 (4.8090e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [83][390/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.8489e-04 (4.8088e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
## e[83] optimizer.zero_grad (sum) time: 0.1772747039794922
## e[83]       loss.backward (sum) time: 3.0000102519989014
## e[83]      optimizer.step (sum) time: 1.031531810760498
## epoch[83] training(only) time: 15.162328481674194
# Switched to evaluate mode...
Test: [  0/100]	Time  0.180 ( 0.180)	Loss 2.7396e-01 (2.7396e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.017 ( 0.032)	Loss 3.8637e-01 (2.8064e-01)	Acc@1  89.00 ( 93.55)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.021 ( 0.026)	Loss 3.7099e-01 (2.9446e-01)	Acc@1  91.00 ( 93.43)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.021 ( 0.024)	Loss 3.0356e-01 (2.9894e-01)	Acc@1  92.00 ( 93.35)	Acc@5  99.00 ( 99.84)
Test: [ 40/100]	Time  0.016 ( 0.022)	Loss 2.5971e-01 (2.9845e-01)	Acc@1  92.00 ( 93.41)	Acc@5 100.00 ( 99.83)
Test: [ 50/100]	Time  0.017 ( 0.022)	Loss 1.0410e-01 (3.0406e-01)	Acc@1  97.00 ( 93.47)	Acc@5 100.00 ( 99.80)
Test: [ 60/100]	Time  0.021 ( 0.021)	Loss 3.4343e-01 (2.9717e-01)	Acc@1  90.00 ( 93.39)	Acc@5 100.00 ( 99.82)
Test: [ 70/100]	Time  0.016 ( 0.021)	Loss 6.4183e-01 (2.8994e-01)	Acc@1  90.00 ( 93.46)	Acc@5  98.00 ( 99.79)
Test: [ 80/100]	Time  0.022 ( 0.021)	Loss 2.0958e-01 (2.9354e-01)	Acc@1  94.00 ( 93.46)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.025 ( 0.021)	Loss 1.9462e-01 (2.8970e-01)	Acc@1  94.00 ( 93.45)	Acc@5 100.00 ( 99.80)
 * Acc@1 93.480 Acc@5 99.800
### epoch[83] execution time: 17.346524953842163
EPOCH 84
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [84][  0/391]	Time  0.206 ( 0.206)	Data  0.171 ( 0.171)	Loss 2.8803e-03 (2.8803e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [84][ 10/391]	Time  0.038 ( 0.053)	Data  0.001 ( 0.017)	Loss 4.7807e-03 (3.1419e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [84][ 20/391]	Time  0.038 ( 0.046)	Data  0.001 ( 0.010)	Loss 6.0945e-03 (3.3609e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [84][ 30/391]	Time  0.036 ( 0.043)	Data  0.001 ( 0.008)	Loss 3.6560e-03 (3.4743e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [84][ 40/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.006)	Loss 1.0519e-02 (4.0546e-03)	Acc@1  99.22 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [84][ 50/391]	Time  0.050 ( 0.041)	Data  0.001 ( 0.006)	Loss 1.7364e-03 (3.8840e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [84][ 60/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.005)	Loss 1.8235e-02 (4.2168e-03)	Acc@1  99.22 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [84][ 70/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.005)	Loss 1.2726e-02 (4.7988e-03)	Acc@1  99.22 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [84][ 80/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.9204e-03 (4.5935e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [84][ 90/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.1479e-02 (4.6922e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [84][100/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.004)	Loss 7.9017e-04 (4.6604e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [84][110/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.7696e-03 (4.4840e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [84][120/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.9684e-02 (4.5357e-03)	Acc@1  99.22 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [84][130/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.7770e-03 (4.8313e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [84][140/391]	Time  0.042 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.7498e-02 (4.8071e-03)	Acc@1  99.22 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [84][150/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.4206e-03 (4.7081e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [84][160/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.9673e-03 (4.8700e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [84][170/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.1003e-03 (4.7768e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [84][180/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.6217e-03 (4.7195e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [84][190/391]	Time  0.040 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.3932e-03 (4.6695e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [84][200/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.5745e-02 (4.8093e-03)	Acc@1  99.22 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [84][210/391]	Time  0.037 ( 0.039)	Data  0.002 ( 0.003)	Loss 6.9845e-03 (4.7871e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [84][220/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.1178e-04 (4.7835e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [84][230/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.7566e-02 (4.8303e-03)	Acc@1  99.22 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [84][240/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 9.2812e-04 (4.8260e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [84][250/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.5854e-03 (4.7854e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [84][260/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.3676e-03 (4.7443e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [84][270/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.3380e-04 (4.7568e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [84][280/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.2724e-03 (4.7064e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [84][290/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.6056e-02 (4.6944e-03)	Acc@1  99.22 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [84][300/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.4081e-02 (4.7187e-03)	Acc@1  99.22 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [84][310/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.8597e-03 (4.7157e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [84][320/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.0442e-03 (4.6982e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [84][330/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.8641e-03 (4.7302e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [84][340/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.0717e-03 (4.7179e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [84][350/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.0525e-03 (4.7194e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [84][360/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.0241e-03 (4.6849e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [84][370/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.0209e-03 (4.6663e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [84][380/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.3581e-03 (4.7236e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [84][390/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.4782e-02 (4.7490e-03)	Acc@1  98.75 ( 99.91)	Acc@5 100.00 (100.00)
## e[84] optimizer.zero_grad (sum) time: 0.175123929977417
## e[84]       loss.backward (sum) time: 2.9950599670410156
## e[84]      optimizer.step (sum) time: 1.024949073791504
## epoch[84] training(only) time: 15.175350904464722
# Switched to evaluate mode...
Test: [  0/100]	Time  0.180 ( 0.180)	Loss 2.3864e-01 (2.3864e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.020 ( 0.034)	Loss 3.8586e-01 (2.8293e-01)	Acc@1  90.00 ( 93.36)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.017 ( 0.026)	Loss 3.9023e-01 (2.9475e-01)	Acc@1  91.00 ( 93.29)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.018 ( 0.025)	Loss 2.8876e-01 (2.9810e-01)	Acc@1  92.00 ( 93.29)	Acc@5  99.00 ( 99.81)
Test: [ 40/100]	Time  0.020 ( 0.023)	Loss 2.3384e-01 (2.9867e-01)	Acc@1  93.00 ( 93.29)	Acc@5 100.00 ( 99.78)
Test: [ 50/100]	Time  0.016 ( 0.022)	Loss 9.1859e-02 (3.0537e-01)	Acc@1  95.00 ( 93.24)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.017 ( 0.022)	Loss 3.3957e-01 (2.9816e-01)	Acc@1  92.00 ( 93.25)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.016 ( 0.021)	Loss 6.2363e-01 (2.9138e-01)	Acc@1  90.00 ( 93.30)	Acc@5  99.00 ( 99.77)
Test: [ 80/100]	Time  0.016 ( 0.021)	Loss 1.8744e-01 (2.9374e-01)	Acc@1  95.00 ( 93.32)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.016 ( 0.020)	Loss 1.8420e-01 (2.8969e-01)	Acc@1  94.00 ( 93.36)	Acc@5 100.00 ( 99.79)
 * Acc@1 93.410 Acc@5 99.790
### epoch[84] execution time: 17.322113275527954
EPOCH 85
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [85][  0/391]	Time  0.219 ( 0.219)	Data  0.182 ( 0.182)	Loss 1.4053e-03 (1.4053e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [85][ 10/391]	Time  0.036 ( 0.054)	Data  0.001 ( 0.018)	Loss 3.3165e-03 (5.0895e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [85][ 20/391]	Time  0.038 ( 0.046)	Data  0.001 ( 0.011)	Loss 7.1007e-03 (5.0056e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [85][ 30/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.008)	Loss 3.7980e-02 (5.2287e-03)	Acc@1  98.44 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [85][ 40/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.007)	Loss 7.1625e-03 (5.6080e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [85][ 50/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.006)	Loss 6.2537e-03 (5.2627e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [85][ 60/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.005)	Loss 2.1898e-03 (5.0468e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [85][ 70/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.005)	Loss 2.6824e-03 (5.2845e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [85][ 80/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.1930e-03 (4.9519e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [85][ 90/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.004)	Loss 4.5833e-03 (5.1144e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [85][100/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.2651e-03 (5.0682e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [85][110/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 6.9590e-03 (5.0795e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [85][120/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 4.5977e-03 (5.1311e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [85][130/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.2039e-02 (5.1420e-03)	Acc@1  99.22 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [85][140/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.2020e-03 (5.1806e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [85][150/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.2601e-03 (5.2185e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [85][160/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.7497e-03 (5.2769e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [85][170/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.8148e-03 (5.1987e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [85][180/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.2626e-03 (5.1485e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [85][190/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.2149e-03 (5.0214e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [85][200/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.5654e-03 (4.9550e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [85][210/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.9329e-03 (4.9648e-03)	Acc@1  99.22 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [85][220/391]	Time  0.040 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.3726e-03 (5.0199e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [85][230/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.8156e-03 (5.0319e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [85][240/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.8754e-03 (4.9690e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [85][250/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.3841e-03 (4.9596e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [85][260/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.1690e-03 (4.9498e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [85][270/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.0719e-03 (5.0381e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [85][280/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.1690e-03 (5.0275e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [85][290/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.5332e-03 (4.9671e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [85][300/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.3988e-04 (5.0160e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [85][310/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.7525e-03 (5.0240e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [85][320/391]	Time  0.040 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.6583e-03 (4.9970e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [85][330/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.4940e-03 (5.0503e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [85][340/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.2532e-03 (4.9961e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [85][350/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.1538e-03 (4.9983e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [85][360/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.0394e-03 (5.0496e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [85][370/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.1947e-03 (4.9970e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [85][380/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.7492e-03 (4.9337e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [85][390/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.7035e-03 (4.9090e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
## e[85] optimizer.zero_grad (sum) time: 0.17649483680725098
## e[85]       loss.backward (sum) time: 2.9824583530426025
## e[85]      optimizer.step (sum) time: 1.018594741821289
## epoch[85] training(only) time: 15.122220039367676
# Switched to evaluate mode...
Test: [  0/100]	Time  0.193 ( 0.193)	Loss 2.4746e-01 (2.4746e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.017 ( 0.034)	Loss 3.7327e-01 (2.8408e-01)	Acc@1  91.00 ( 93.36)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.016 ( 0.027)	Loss 3.8926e-01 (2.9531e-01)	Acc@1  91.00 ( 93.38)	Acc@5 100.00 ( 99.90)
Test: [ 30/100]	Time  0.022 ( 0.024)	Loss 2.7195e-01 (2.9930e-01)	Acc@1  92.00 ( 93.39)	Acc@5 100.00 ( 99.84)
Test: [ 40/100]	Time  0.018 ( 0.023)	Loss 2.2464e-01 (2.9988e-01)	Acc@1  94.00 ( 93.41)	Acc@5 100.00 ( 99.83)
Test: [ 50/100]	Time  0.023 ( 0.022)	Loss 8.2017e-02 (3.0744e-01)	Acc@1  97.00 ( 93.39)	Acc@5 100.00 ( 99.80)
Test: [ 60/100]	Time  0.017 ( 0.022)	Loss 2.9891e-01 (2.9791e-01)	Acc@1  92.00 ( 93.30)	Acc@5 100.00 ( 99.84)
Test: [ 70/100]	Time  0.020 ( 0.021)	Loss 6.2597e-01 (2.8970e-01)	Acc@1  90.00 ( 93.37)	Acc@5  99.00 ( 99.82)
Test: [ 80/100]	Time  0.016 ( 0.021)	Loss 1.9812e-01 (2.9310e-01)	Acc@1  95.00 ( 93.38)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.016 ( 0.021)	Loss 2.2650e-01 (2.8971e-01)	Acc@1  93.00 ( 93.41)	Acc@5 100.00 ( 99.81)
 * Acc@1 93.430 Acc@5 99.810
### epoch[85] execution time: 17.30355739593506
EPOCH 86
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [86][  0/391]	Time  0.212 ( 0.212)	Data  0.179 ( 0.179)	Loss 5.3615e-03 (5.3615e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [86][ 10/391]	Time  0.036 ( 0.054)	Data  0.001 ( 0.018)	Loss 2.2076e-03 (4.2375e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [86][ 20/391]	Time  0.038 ( 0.046)	Data  0.001 ( 0.011)	Loss 6.7414e-03 (4.0630e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [86][ 30/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.008)	Loss 2.5500e-03 (4.0231e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [86][ 40/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.007)	Loss 1.9819e-03 (4.0947e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [86][ 50/391]	Time  0.035 ( 0.041)	Data  0.001 ( 0.006)	Loss 2.2553e-03 (4.0201e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [86][ 60/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.005)	Loss 4.2752e-03 (4.5596e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [86][ 70/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.005)	Loss 5.5828e-03 (4.4609e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [86][ 80/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 5.8567e-03 (4.3024e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [86][ 90/391]	Time  0.039 ( 0.040)	Data  0.003 ( 0.004)	Loss 4.9034e-03 (4.2078e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [86][100/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.6730e-03 (4.1112e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [86][110/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 6.7833e-03 (4.2845e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [86][120/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.5021e-03 (4.1453e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [86][130/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.3384e-03 (4.1610e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [86][140/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.7397e-03 (4.3905e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [86][150/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.0162e-02 (4.3598e-03)	Acc@1  99.22 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [86][160/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.2542e-03 (4.3521e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [86][170/391]	Time  0.038 ( 0.039)	Data  0.002 ( 0.003)	Loss 2.1810e-03 (4.3956e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [86][180/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.8854e-02 (4.5572e-03)	Acc@1  99.22 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [86][190/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.8632e-03 (4.7621e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [86][200/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.7992e-03 (4.7390e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [86][210/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.2892e-03 (4.6956e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [86][220/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.8856e-03 (4.6352e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [86][230/391]	Time  0.039 ( 0.038)	Data  0.002 ( 0.003)	Loss 8.8777e-03 (4.5946e-03)	Acc@1  99.22 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [86][240/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.4109e-03 (4.5355e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [86][250/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.6796e-03 (4.5315e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [86][260/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.3651e-03 (4.5598e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [86][270/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.3694e-03 (4.6968e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [86][280/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.8985e-03 (4.6944e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [86][290/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.3140e-03 (4.6892e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [86][300/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.6451e-03 (4.7341e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [86][310/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.3485e-03 (4.7622e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [86][320/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.9877e-03 (4.7824e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [86][330/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.2270e-03 (4.7233e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [86][340/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.1860e-02 (4.7117e-03)	Acc@1  99.22 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [86][350/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.7780e-03 (4.6934e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [86][360/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.2777e-03 (4.6752e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [86][370/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.0090e-03 (4.6785e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [86][380/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.8684e-03 (4.6964e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [86][390/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.4424e-03 (4.7065e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
## e[86] optimizer.zero_grad (sum) time: 0.17596030235290527
## e[86]       loss.backward (sum) time: 2.9508275985717773
## e[86]      optimizer.step (sum) time: 1.040773630142212
## epoch[86] training(only) time: 15.086055040359497
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 2.5243e-01 (2.5243e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.021 ( 0.034)	Loss 4.0046e-01 (2.8740e-01)	Acc@1  90.00 ( 93.45)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.020 ( 0.027)	Loss 3.8373e-01 (2.9806e-01)	Acc@1  91.00 ( 93.29)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.016 ( 0.024)	Loss 2.8987e-01 (3.0064e-01)	Acc@1  92.00 ( 93.39)	Acc@5  99.00 ( 99.81)
Test: [ 40/100]	Time  0.016 ( 0.023)	Loss 2.5528e-01 (3.0182e-01)	Acc@1  93.00 ( 93.39)	Acc@5 100.00 ( 99.80)
Test: [ 50/100]	Time  0.016 ( 0.022)	Loss 1.0111e-01 (3.0795e-01)	Acc@1  96.00 ( 93.37)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.016 ( 0.022)	Loss 3.3175e-01 (3.0103e-01)	Acc@1  92.00 ( 93.41)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.020 ( 0.021)	Loss 6.3279e-01 (2.9359e-01)	Acc@1  90.00 ( 93.49)	Acc@5  99.00 ( 99.77)
Test: [ 80/100]	Time  0.022 ( 0.021)	Loss 1.9078e-01 (2.9620e-01)	Acc@1  95.00 ( 93.49)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.017 ( 0.021)	Loss 1.7630e-01 (2.9176e-01)	Acc@1  94.00 ( 93.55)	Acc@5 100.00 ( 99.79)
 * Acc@1 93.630 Acc@5 99.790
### epoch[86] execution time: 17.282273054122925
EPOCH 87
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [87][  0/391]	Time  0.217 ( 0.217)	Data  0.183 ( 0.183)	Loss 2.2629e-03 (2.2629e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [87][ 10/391]	Time  0.035 ( 0.054)	Data  0.001 ( 0.018)	Loss 6.7944e-03 (5.8707e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [87][ 20/391]	Time  0.034 ( 0.046)	Data  0.001 ( 0.011)	Loss 6.3493e-03 (5.1147e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [87][ 30/391]	Time  0.036 ( 0.043)	Data  0.001 ( 0.008)	Loss 1.5512e-03 (5.5792e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [87][ 40/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.007)	Loss 4.2783e-03 (5.4251e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [87][ 50/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.006)	Loss 4.8986e-03 (5.5566e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [87][ 60/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.005)	Loss 1.5266e-03 (5.8877e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [87][ 70/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.005)	Loss 4.4985e-03 (5.9970e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [87][ 80/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 5.7937e-03 (5.9878e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [87][ 90/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 2.7746e-03 (5.6558e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [87][100/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.1913e-03 (5.4100e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [87][110/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.9627e-03 (5.2907e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [87][120/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 8.7485e-04 (5.1467e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [87][130/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.004)	Loss 5.8200e-03 (5.0777e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [87][140/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.9909e-03 (5.0561e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [87][150/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.0111e-03 (4.9807e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [87][160/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.2537e-02 (5.0178e-03)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [87][170/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.1244e-02 (5.1634e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [87][180/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.8476e-03 (5.3260e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [87][190/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.0061e-03 (5.2231e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [87][200/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.3662e-03 (5.1004e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [87][210/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.6093e-04 (5.0735e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [87][220/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 9.5350e-03 (5.1425e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [87][230/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.1944e-03 (5.0485e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [87][240/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.3089e-04 (5.0106e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [87][250/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.7443e-04 (5.1259e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [87][260/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.8758e-03 (5.0742e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [87][270/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.1491e-03 (5.0385e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [87][280/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.1778e-03 (5.0139e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [87][290/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.0996e-03 (5.1574e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [87][300/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.8102e-03 (5.1168e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [87][310/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.0294e-03 (5.1925e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [87][320/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.4138e-03 (5.1794e-03)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [87][330/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.1022e-03 (5.1591e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [87][340/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 9.7106e-04 (5.1215e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [87][350/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.7831e-03 (5.0920e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [87][360/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.6962e-03 (5.0230e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [87][370/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.6558e-03 (4.9795e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [87][380/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.8017e-03 (4.9400e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [87][390/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.7818e-03 (4.9211e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
## e[87] optimizer.zero_grad (sum) time: 0.17575979232788086
## e[87]       loss.backward (sum) time: 3.000683307647705
## e[87]      optimizer.step (sum) time: 1.0333976745605469
## epoch[87] training(only) time: 15.12026333808899
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 2.5984e-01 (2.5984e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.021 ( 0.034)	Loss 3.6451e-01 (2.8614e-01)	Acc@1  91.00 ( 93.27)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.024 ( 0.027)	Loss 3.8377e-01 (2.9857e-01)	Acc@1  91.00 ( 93.24)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.023 ( 0.025)	Loss 2.8078e-01 (2.9980e-01)	Acc@1  92.00 ( 93.23)	Acc@5  99.00 ( 99.81)
Test: [ 40/100]	Time  0.017 ( 0.023)	Loss 2.3798e-01 (2.9991e-01)	Acc@1  93.00 ( 93.27)	Acc@5 100.00 ( 99.80)
Test: [ 50/100]	Time  0.019 ( 0.022)	Loss 8.7734e-02 (3.0570e-01)	Acc@1  96.00 ( 93.29)	Acc@5 100.00 ( 99.80)
Test: [ 60/100]	Time  0.024 ( 0.021)	Loss 3.3719e-01 (2.9801e-01)	Acc@1  91.00 ( 93.30)	Acc@5 100.00 ( 99.82)
Test: [ 70/100]	Time  0.020 ( 0.021)	Loss 5.9561e-01 (2.9029e-01)	Acc@1  90.00 ( 93.38)	Acc@5  99.00 ( 99.80)
Test: [ 80/100]	Time  0.017 ( 0.021)	Loss 2.0053e-01 (2.9297e-01)	Acc@1  96.00 ( 93.44)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.016 ( 0.020)	Loss 2.1448e-01 (2.8895e-01)	Acc@1  93.00 ( 93.46)	Acc@5 100.00 ( 99.80)
 * Acc@1 93.520 Acc@5 99.800
### epoch[87] execution time: 17.263351917266846
EPOCH 88
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [88][  0/391]	Time  0.215 ( 0.215)	Data  0.177 ( 0.177)	Loss 6.1470e-03 (6.1470e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [88][ 10/391]	Time  0.038 ( 0.053)	Data  0.001 ( 0.018)	Loss 2.5725e-04 (2.8236e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [88][ 20/391]	Time  0.038 ( 0.046)	Data  0.001 ( 0.010)	Loss 1.5247e-03 (4.1719e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [88][ 30/391]	Time  0.036 ( 0.043)	Data  0.001 ( 0.008)	Loss 7.1286e-03 (4.5546e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [88][ 40/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.006)	Loss 6.2215e-03 (4.6954e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [88][ 50/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.006)	Loss 1.0677e-02 (4.6259e-03)	Acc@1  99.22 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [88][ 60/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.005)	Loss 1.1574e-03 (5.0432e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [88][ 70/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.005)	Loss 3.1691e-03 (4.7661e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [88][ 80/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.8869e-02 (5.1282e-03)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [88][ 90/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 9.4662e-03 (5.4615e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [88][100/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.4595e-02 (5.3525e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [88][110/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.1710e-03 (5.2171e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [88][120/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.1717e-03 (5.0373e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [88][130/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.4753e-03 (5.2676e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [88][140/391]	Time  0.038 ( 0.039)	Data  0.003 ( 0.003)	Loss 2.3208e-02 (5.3587e-03)	Acc@1  98.44 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [88][150/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.9312e-03 (5.2842e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [88][160/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.1681e-03 (5.3434e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [88][170/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.8962e-03 (5.4131e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [88][180/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.0316e-03 (5.3494e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [88][190/391]	Time  0.044 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.1884e-02 (5.4241e-03)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [88][200/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.4103e-03 (5.3313e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [88][210/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.3371e-02 (5.3910e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [88][220/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.5033e-03 (5.3962e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [88][230/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.4281e-03 (5.3779e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [88][240/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.6781e-03 (5.4725e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [88][250/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.2194e-03 (5.4688e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [88][260/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.5440e-03 (5.4750e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [88][270/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.6641e-03 (5.4292e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [88][280/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.5846e-03 (5.3228e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [88][290/391]	Time  0.038 ( 0.038)	Data  0.002 ( 0.003)	Loss 2.8912e-03 (5.2898e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [88][300/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.5603e-03 (5.2942e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [88][310/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.9028e-04 (5.2432e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [88][320/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.7509e-03 (5.2753e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [88][330/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.0053e-03 (5.2365e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [88][340/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.5735e-03 (5.1622e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [88][350/391]	Time  0.039 ( 0.038)	Data  0.002 ( 0.003)	Loss 6.1685e-03 (5.1713e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [88][360/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.4707e-03 (5.0977e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [88][370/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.8116e-03 (5.0331e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [88][380/391]	Time  0.038 ( 0.038)	Data  0.002 ( 0.003)	Loss 4.2961e-03 (5.0295e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [88][390/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.7403e-03 (5.0067e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
## e[88] optimizer.zero_grad (sum) time: 0.17536401748657227
## e[88]       loss.backward (sum) time: 2.9619202613830566
## e[88]      optimizer.step (sum) time: 1.0260272026062012
## epoch[88] training(only) time: 15.121245861053467
# Switched to evaluate mode...
Test: [  0/100]	Time  0.166 ( 0.166)	Loss 2.4837e-01 (2.4837e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.033)	Loss 3.9168e-01 (2.8426e-01)	Acc@1  89.00 ( 93.09)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.016 ( 0.026)	Loss 3.7346e-01 (2.9522e-01)	Acc@1  91.00 ( 93.05)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.020 ( 0.024)	Loss 2.8918e-01 (2.9880e-01)	Acc@1  92.00 ( 93.06)	Acc@5  99.00 ( 99.77)
Test: [ 40/100]	Time  0.023 ( 0.024)	Loss 2.3559e-01 (2.9904e-01)	Acc@1  93.00 ( 93.20)	Acc@5  99.00 ( 99.73)
Test: [ 50/100]	Time  0.016 ( 0.023)	Loss 8.7555e-02 (3.0683e-01)	Acc@1  97.00 ( 93.18)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.021 ( 0.023)	Loss 3.4601e-01 (2.9937e-01)	Acc@1  91.00 ( 93.25)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.017 ( 0.022)	Loss 6.6500e-01 (2.9184e-01)	Acc@1  90.00 ( 93.31)	Acc@5  99.00 ( 99.73)
Test: [ 80/100]	Time  0.022 ( 0.022)	Loss 1.9787e-01 (2.9514e-01)	Acc@1  95.00 ( 93.31)	Acc@5 100.00 ( 99.73)
Test: [ 90/100]	Time  0.019 ( 0.022)	Loss 2.0375e-01 (2.9174e-01)	Acc@1  93.00 ( 93.36)	Acc@5 100.00 ( 99.75)
 * Acc@1 93.440 Acc@5 99.750
### epoch[88] execution time: 17.406248807907104
EPOCH 89
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [89][  0/391]	Time  0.210 ( 0.210)	Data  0.175 ( 0.175)	Loss 1.9712e-03 (1.9712e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [89][ 10/391]	Time  0.036 ( 0.053)	Data  0.001 ( 0.018)	Loss 1.9702e-03 (2.2125e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [89][ 20/391]	Time  0.037 ( 0.046)	Data  0.001 ( 0.010)	Loss 4.1061e-03 (3.1293e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [89][ 30/391]	Time  0.037 ( 0.043)	Data  0.001 ( 0.008)	Loss 2.4113e-03 (3.1302e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [89][ 40/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.006)	Loss 1.0853e-03 (3.3848e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [89][ 50/391]	Time  0.034 ( 0.041)	Data  0.001 ( 0.006)	Loss 5.2376e-03 (4.2148e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [89][ 60/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.005)	Loss 2.3766e-03 (4.5569e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [89][ 70/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.005)	Loss 2.7680e-02 (5.5458e-03)	Acc@1  98.44 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [89][ 80/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.3643e-03 (5.4730e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [89][ 90/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.2912e-03 (5.9014e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [89][100/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 8.5386e-03 (5.6719e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [89][110/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.0377e-03 (5.3646e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [89][120/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.3753e-03 (5.1752e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [89][130/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 4.1668e-03 (5.1467e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [89][140/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.6707e-03 (4.9938e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [89][150/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.2010e-03 (5.0169e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [89][160/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.8043e-02 (5.0726e-03)	Acc@1  99.22 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [89][170/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.2762e-03 (5.0637e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [89][180/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.2740e-03 (5.0338e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [89][190/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.0453e-03 (5.0467e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [89][200/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.1759e-03 (5.0460e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [89][210/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.2060e-04 (5.2239e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [89][220/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.0089e-03 (5.2550e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [89][230/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.5397e-03 (5.2358e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [89][240/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.8485e-03 (5.1846e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [89][250/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.3499e-03 (5.2256e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [89][260/391]	Time  0.040 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.8219e-03 (5.1467e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [89][270/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.1465e-04 (5.0898e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [89][280/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.9889e-03 (5.0350e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [89][290/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.6801e-03 (4.9384e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [89][300/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.5679e-03 (4.8881e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [89][310/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.6234e-03 (4.8374e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [89][320/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 9.0408e-03 (4.9854e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [89][330/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.1051e-02 (5.0267e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [89][340/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.5325e-02 (5.0189e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [89][350/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.8586e-02 (5.0717e-03)	Acc@1  99.22 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [89][360/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.7498e-03 (5.0356e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [89][370/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.5900e-03 (4.9736e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [89][380/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.0088e-03 (4.9646e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [89][390/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.0595e-03 (4.9089e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
## e[89] optimizer.zero_grad (sum) time: 0.17476725578308105
## e[89]       loss.backward (sum) time: 3.006934642791748
## e[89]      optimizer.step (sum) time: 1.026130199432373
## epoch[89] training(only) time: 15.197874784469604
# Switched to evaluate mode...
Test: [  0/100]	Time  0.182 ( 0.182)	Loss 2.3636e-01 (2.3636e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.023 ( 0.035)	Loss 3.8466e-01 (2.8342e-01)	Acc@1  89.00 ( 93.36)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.024 ( 0.027)	Loss 3.9255e-01 (2.9463e-01)	Acc@1  91.00 ( 93.38)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.017 ( 0.024)	Loss 2.8172e-01 (2.9707e-01)	Acc@1  92.00 ( 93.35)	Acc@5  99.00 ( 99.81)
Test: [ 40/100]	Time  0.019 ( 0.023)	Loss 2.5301e-01 (2.9902e-01)	Acc@1  93.00 ( 93.41)	Acc@5 100.00 ( 99.78)
Test: [ 50/100]	Time  0.021 ( 0.022)	Loss 9.4162e-02 (3.0583e-01)	Acc@1  97.00 ( 93.31)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.021 ( 0.021)	Loss 3.2980e-01 (2.9897e-01)	Acc@1  92.00 ( 93.34)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.017 ( 0.021)	Loss 6.7351e-01 (2.9193e-01)	Acc@1  90.00 ( 93.42)	Acc@5  99.00 ( 99.76)
Test: [ 80/100]	Time  0.017 ( 0.020)	Loss 1.7626e-01 (2.9454e-01)	Acc@1  95.00 ( 93.43)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.016 ( 0.020)	Loss 1.8361e-01 (2.9113e-01)	Acc@1  95.00 ( 93.45)	Acc@5 100.00 ( 99.78)
 * Acc@1 93.500 Acc@5 99.780
### epoch[89] execution time: 17.343603134155273
### Training complete:
#### total training(only) time: 1365.518043756485
##### Total run time: 1564.8298890590668
