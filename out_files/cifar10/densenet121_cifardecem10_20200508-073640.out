# Model: densenet121
# Dataset: cifardecem
# Batch size: 128
# Freezeout: False
# Low precision: False
# Epochs: 90
# Initial learning rate: 0.1
# module_name: models_cifardecem.densenet
<function densenet121 at 0x7fc1af689f28>
# model requested: 'densenet121'
# printing out the model
DenseNet(
  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (features): Sequential(
    (dense_block_layer_0): Sequential(
      (bottle_neck_layer_0): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_1): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_2): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_3): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_4): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_5): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
    )
    (transition_layer_0): Transition(
      (down_sample): Sequential(
        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): AvgPool2d(kernel_size=2, stride=2, padding=0)
      )
    )
    (dense_block_layer_1): Sequential(
      (bottle_neck_layer_0): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_1): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_2): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_3): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_4): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_5): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_6): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_7): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_8): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_9): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_10): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_11): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
    )
    (transition_layer_1): Transition(
      (down_sample): Sequential(
        (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): AvgPool2d(kernel_size=2, stride=2, padding=0)
      )
    )
    (dense_block_layer_2): Sequential(
      (bottle_neck_layer_0): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_1): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_2): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_3): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_4): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_5): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_6): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_7): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_8): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_9): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_10): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_11): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_12): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_13): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_14): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_15): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_16): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_17): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_18): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_19): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_20): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_21): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_22): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_23): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
    )
    (transition_layer_2): Transition(
      (down_sample): Sequential(
        (0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): AvgPool2d(kernel_size=2, stride=2, padding=0)
      )
    )
    (dense_block3): Sequential(
      (bottle_neck_layer_0): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_1): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_2): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_3): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_4): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_5): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_6): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_7): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_8): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_9): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_10): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_11): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_12): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_13): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_14): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_15): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
    )
    (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (linear): Linear(in_features=1024, out_features=10, bias=True)
)
# model is full precision
# Model: densenet121
# Dataset: cifardecem
# Freezeout: False
# Low precision: False
# Initial learning rate: 0.1
# Criterion: CrossEntropyLoss()
# Optimizer: SGD
#	lr 0.1
#	momentum 0.9
#	dampening 0
#	weight_decay 0.0001
#	nesterov False
CIFAR10 loading and normalization
==> Preparing data..
# CIFAR10 / full precision
Files already downloaded and verified
Files already downloaded and verified
#--> Training data: <--#
#-->>
EPOCH 0
# current learning rate: 0.1
# Switched to train mode...
Epoch: [0][  0/391]	Time  4.363 ( 4.363)	Data  0.101 ( 0.101)	Loss 2.3195e+00 (2.3195e+00)	Acc@1  14.84 ( 14.84)	Acc@5  53.12 ( 53.12)
Epoch: [0][ 10/391]	Time  0.133 ( 0.523)	Data  0.001 ( 0.010)	Loss 2.7395e+00 (2.4469e+00)	Acc@1  19.53 ( 18.96)	Acc@5  57.03 ( 65.13)
Epoch: [0][ 20/391]	Time  0.134 ( 0.338)	Data  0.001 ( 0.006)	Loss 2.4003e+00 (2.4809e+00)	Acc@1  14.84 ( 18.42)	Acc@5  73.44 ( 67.00)
Epoch: [0][ 30/391]	Time  0.137 ( 0.272)	Data  0.001 ( 0.004)	Loss 2.3485e+00 (2.4388e+00)	Acc@1  21.09 ( 19.51)	Acc@5  85.16 ( 69.15)
Epoch: [0][ 40/391]	Time  0.131 ( 0.238)	Data  0.001 ( 0.003)	Loss 2.3571e+00 (2.3755e+00)	Acc@1  14.84 ( 20.03)	Acc@5  80.47 ( 71.74)
Epoch: [0][ 50/391]	Time  0.130 ( 0.218)	Data  0.001 ( 0.003)	Loss 2.5215e+00 (2.3333e+00)	Acc@1  23.44 ( 20.31)	Acc@5  82.81 ( 73.38)
Epoch: [0][ 60/391]	Time  0.132 ( 0.204)	Data  0.001 ( 0.003)	Loss 1.9455e+00 (2.2937e+00)	Acc@1  25.78 ( 21.08)	Acc@5  78.91 ( 74.53)
Epoch: [0][ 70/391]	Time  0.133 ( 0.195)	Data  0.001 ( 0.002)	Loss 1.9743e+00 (2.2400e+00)	Acc@1  28.12 ( 21.93)	Acc@5  82.81 ( 75.83)
Epoch: [0][ 80/391]	Time  0.131 ( 0.187)	Data  0.001 ( 0.002)	Loss 1.8422e+00 (2.2005e+00)	Acc@1  26.56 ( 22.46)	Acc@5  84.38 ( 76.78)
Epoch: [0][ 90/391]	Time  0.131 ( 0.181)	Data  0.001 ( 0.002)	Loss 1.8965e+00 (2.1636e+00)	Acc@1  31.25 ( 23.27)	Acc@5  82.81 ( 77.73)
Epoch: [0][100/391]	Time  0.133 ( 0.176)	Data  0.001 ( 0.002)	Loss 1.7470e+00 (2.1330e+00)	Acc@1  37.50 ( 24.25)	Acc@5  85.16 ( 78.36)
Epoch: [0][110/391]	Time  0.131 ( 0.173)	Data  0.001 ( 0.002)	Loss 1.7215e+00 (2.1012e+00)	Acc@1  37.50 ( 25.06)	Acc@5  87.50 ( 79.03)
Epoch: [0][120/391]	Time  0.136 ( 0.170)	Data  0.001 ( 0.002)	Loss 1.7374e+00 (2.0736e+00)	Acc@1  41.41 ( 26.02)	Acc@5  87.50 ( 79.64)
Epoch: [0][130/391]	Time  0.137 ( 0.167)	Data  0.001 ( 0.002)	Loss 1.7618e+00 (2.0489e+00)	Acc@1  32.03 ( 26.72)	Acc@5  88.28 ( 80.24)
Epoch: [0][140/391]	Time  0.132 ( 0.165)	Data  0.001 ( 0.002)	Loss 1.8766e+00 (2.0242e+00)	Acc@1  32.03 ( 27.50)	Acc@5  83.59 ( 80.72)
Epoch: [0][150/391]	Time  0.133 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6622e+00 (2.0036e+00)	Acc@1  39.06 ( 28.00)	Acc@5  90.62 ( 81.25)
Epoch: [0][160/391]	Time  0.133 ( 0.161)	Data  0.001 ( 0.002)	Loss 1.6056e+00 (1.9810e+00)	Acc@1  41.41 ( 28.64)	Acc@5  91.41 ( 81.86)
Epoch: [0][170/391]	Time  0.130 ( 0.159)	Data  0.001 ( 0.002)	Loss 1.7401e+00 (1.9612e+00)	Acc@1  35.94 ( 29.24)	Acc@5  86.72 ( 82.29)
Epoch: [0][180/391]	Time  0.135 ( 0.158)	Data  0.001 ( 0.002)	Loss 1.6544e+00 (1.9431e+00)	Acc@1  39.84 ( 29.83)	Acc@5  89.84 ( 82.72)
Epoch: [0][190/391]	Time  0.134 ( 0.157)	Data  0.001 ( 0.002)	Loss 1.4848e+00 (1.9268e+00)	Acc@1  44.53 ( 30.24)	Acc@5  95.31 ( 83.12)
Epoch: [0][200/391]	Time  0.134 ( 0.155)	Data  0.001 ( 0.002)	Loss 1.7331e+00 (1.9114e+00)	Acc@1  38.28 ( 30.81)	Acc@5  89.84 ( 83.45)
Epoch: [0][210/391]	Time  0.133 ( 0.154)	Data  0.001 ( 0.002)	Loss 1.6166e+00 (1.8950e+00)	Acc@1  42.19 ( 31.38)	Acc@5  89.84 ( 83.79)
Epoch: [0][220/391]	Time  0.147 ( 0.154)	Data  0.001 ( 0.002)	Loss 1.6671e+00 (1.8825e+00)	Acc@1  32.03 ( 31.73)	Acc@5  91.41 ( 84.05)
Epoch: [0][230/391]	Time  0.132 ( 0.153)	Data  0.001 ( 0.002)	Loss 1.6413e+00 (1.8689e+00)	Acc@1  42.19 ( 32.17)	Acc@5  85.16 ( 84.29)
Epoch: [0][240/391]	Time  0.132 ( 0.152)	Data  0.001 ( 0.002)	Loss 1.3328e+00 (1.8552e+00)	Acc@1  51.56 ( 32.66)	Acc@5  94.53 ( 84.59)
Epoch: [0][250/391]	Time  0.143 ( 0.151)	Data  0.001 ( 0.001)	Loss 1.6284e+00 (1.8452e+00)	Acc@1  39.06 ( 33.04)	Acc@5  89.84 ( 84.81)
Epoch: [0][260/391]	Time  0.133 ( 0.151)	Data  0.001 ( 0.001)	Loss 1.4599e+00 (1.8335e+00)	Acc@1  48.44 ( 33.46)	Acc@5  89.84 ( 85.06)
Epoch: [0][270/391]	Time  0.132 ( 0.150)	Data  0.001 ( 0.001)	Loss 1.4570e+00 (1.8236e+00)	Acc@1  47.66 ( 33.83)	Acc@5  93.75 ( 85.27)
Epoch: [0][280/391]	Time  0.135 ( 0.149)	Data  0.001 ( 0.001)	Loss 1.4356e+00 (1.8142e+00)	Acc@1  46.88 ( 34.19)	Acc@5  91.41 ( 85.44)
Epoch: [0][290/391]	Time  0.133 ( 0.149)	Data  0.001 ( 0.001)	Loss 1.4212e+00 (1.8028e+00)	Acc@1  44.53 ( 34.48)	Acc@5  94.53 ( 85.66)
Epoch: [0][300/391]	Time  0.130 ( 0.148)	Data  0.001 ( 0.001)	Loss 1.5437e+00 (1.7957e+00)	Acc@1  42.19 ( 34.66)	Acc@5  88.28 ( 85.80)
Epoch: [0][310/391]	Time  0.131 ( 0.148)	Data  0.001 ( 0.001)	Loss 1.4347e+00 (1.7863e+00)	Acc@1  47.66 ( 34.99)	Acc@5  90.62 ( 85.98)
Epoch: [0][320/391]	Time  0.136 ( 0.148)	Data  0.001 ( 0.001)	Loss 1.7743e+00 (1.7776e+00)	Acc@1  34.38 ( 35.26)	Acc@5  82.81 ( 86.14)
Epoch: [0][330/391]	Time  0.133 ( 0.147)	Data  0.001 ( 0.001)	Loss 1.3899e+00 (1.7700e+00)	Acc@1  52.34 ( 35.55)	Acc@5  94.53 ( 86.32)
Epoch: [0][340/391]	Time  0.133 ( 0.147)	Data  0.001 ( 0.001)	Loss 1.4136e+00 (1.7621e+00)	Acc@1  46.09 ( 35.80)	Acc@5  92.97 ( 86.47)
Epoch: [0][350/391]	Time  0.135 ( 0.146)	Data  0.001 ( 0.001)	Loss 1.5043e+00 (1.7540e+00)	Acc@1  45.31 ( 36.04)	Acc@5  93.75 ( 86.64)
Epoch: [0][360/391]	Time  0.140 ( 0.146)	Data  0.001 ( 0.001)	Loss 1.4612e+00 (1.7450e+00)	Acc@1  47.66 ( 36.37)	Acc@5  92.97 ( 86.80)
Epoch: [0][370/391]	Time  0.134 ( 0.146)	Data  0.001 ( 0.001)	Loss 1.3351e+00 (1.7365e+00)	Acc@1  48.44 ( 36.66)	Acc@5  92.19 ( 86.97)
Epoch: [0][380/391]	Time  0.134 ( 0.146)	Data  0.001 ( 0.001)	Loss 1.2829e+00 (1.7290e+00)	Acc@1  51.56 ( 36.91)	Acc@5  96.09 ( 87.12)
Epoch: [0][390/391]	Time  1.456 ( 0.149)	Data  0.001 ( 0.001)	Loss 1.3347e+00 (1.7213e+00)	Acc@1  53.75 ( 37.17)	Acc@5  90.00 ( 87.26)
## e[0] optimizer.zero_grad (sum) time: 0.6798763275146484
## e[0]       loss.backward (sum) time: 15.417519092559814
## e[0]      optimizer.step (sum) time: 14.17214560508728
## epoch[0] training(only) time: 58.16302466392517
# Switched to evaluate mode...
Test: [  0/100]	Time  0.690 ( 0.690)	Loss 1.4025e+00 (1.4025e+00)	Acc@1  51.00 ( 51.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.045 ( 0.107)	Loss 1.3436e+00 (1.4011e+00)	Acc@1  53.00 ( 51.45)	Acc@5  93.00 ( 92.36)
Test: [ 20/100]	Time  0.049 ( 0.078)	Loss 1.2825e+00 (1.3923e+00)	Acc@1  59.00 ( 50.67)	Acc@5  96.00 ( 92.76)
Test: [ 30/100]	Time  0.046 ( 0.068)	Loss 1.2097e+00 (1.3842e+00)	Acc@1  58.00 ( 50.45)	Acc@5  97.00 ( 93.29)
Test: [ 40/100]	Time  0.045 ( 0.063)	Loss 1.5718e+00 (1.3921e+00)	Acc@1  49.00 ( 49.59)	Acc@5  94.00 ( 93.27)
Test: [ 50/100]	Time  0.045 ( 0.059)	Loss 1.4321e+00 (1.3825e+00)	Acc@1  50.00 ( 49.80)	Acc@5  92.00 ( 93.43)
Test: [ 60/100]	Time  0.045 ( 0.057)	Loss 1.5004e+00 (1.3967e+00)	Acc@1  43.00 ( 49.64)	Acc@5  94.00 ( 93.31)
Test: [ 70/100]	Time  0.047 ( 0.056)	Loss 1.3494e+00 (1.3988e+00)	Acc@1  53.00 ( 49.44)	Acc@5  93.00 ( 93.24)
Test: [ 80/100]	Time  0.060 ( 0.055)	Loss 1.3349e+00 (1.3944e+00)	Acc@1  47.00 ( 49.26)	Acc@5  90.00 ( 93.14)
Test: [ 90/100]	Time  0.047 ( 0.054)	Loss 1.2564e+00 (1.3994e+00)	Acc@1  48.00 ( 49.04)	Acc@5  95.00 ( 93.21)
 * Acc@1 49.100 Acc@5 93.230
### epoch[0] execution time: 63.61326217651367
EPOCH 1
# current learning rate: 0.1
# Switched to train mode...
Epoch: [1][  0/391]	Time  0.353 ( 0.353)	Data  0.144 ( 0.144)	Loss 1.3450e+00 (1.3450e+00)	Acc@1  50.00 ( 50.00)	Acc@5  94.53 ( 94.53)
Epoch: [1][ 10/391]	Time  0.136 ( 0.156)	Data  0.001 ( 0.014)	Loss 1.2598e+00 (1.4191e+00)	Acc@1  50.78 ( 48.01)	Acc@5  94.53 ( 92.61)
Epoch: [1][ 20/391]	Time  0.136 ( 0.146)	Data  0.001 ( 0.008)	Loss 1.3289e+00 (1.3959e+00)	Acc@1  51.56 ( 49.07)	Acc@5  96.88 ( 93.08)
Epoch: [1][ 30/391]	Time  0.138 ( 0.143)	Data  0.001 ( 0.006)	Loss 1.3169e+00 (1.3838e+00)	Acc@1  46.09 ( 49.37)	Acc@5  95.31 ( 93.35)
Epoch: [1][ 40/391]	Time  0.134 ( 0.141)	Data  0.001 ( 0.005)	Loss 1.1340e+00 (1.3724e+00)	Acc@1  61.72 ( 49.75)	Acc@5  96.88 ( 93.65)
Epoch: [1][ 50/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.004)	Loss 1.3754e+00 (1.3653e+00)	Acc@1  48.44 ( 50.23)	Acc@5  94.53 ( 93.40)
Epoch: [1][ 60/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.004)	Loss 1.2112e+00 (1.3628e+00)	Acc@1  57.03 ( 50.53)	Acc@5  95.31 ( 93.42)
Epoch: [1][ 70/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.3380e+00 (1.3645e+00)	Acc@1  46.88 ( 50.46)	Acc@5  97.66 ( 93.44)
Epoch: [1][ 80/391]	Time  0.148 ( 0.138)	Data  0.001 ( 0.003)	Loss 1.1614e+00 (1.3579e+00)	Acc@1  57.81 ( 50.61)	Acc@5  96.09 ( 93.60)
Epoch: [1][ 90/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.003)	Loss 1.3146e+00 (1.3541e+00)	Acc@1  56.25 ( 50.87)	Acc@5  90.62 ( 93.62)
Epoch: [1][100/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.003)	Loss 1.1760e+00 (1.3434e+00)	Acc@1  54.69 ( 51.16)	Acc@5  94.53 ( 93.86)
Epoch: [1][110/391]	Time  0.146 ( 0.137)	Data  0.001 ( 0.002)	Loss 1.3428e+00 (1.3425e+00)	Acc@1  54.69 ( 51.40)	Acc@5  92.19 ( 93.83)
Epoch: [1][120/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.002)	Loss 1.2164e+00 (1.3392e+00)	Acc@1  57.81 ( 51.49)	Acc@5  95.31 ( 93.85)
Epoch: [1][130/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.002)	Loss 1.3460e+00 (1.3377e+00)	Acc@1  50.78 ( 51.46)	Acc@5  90.62 ( 93.80)
Epoch: [1][140/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.002)	Loss 1.1225e+00 (1.3343e+00)	Acc@1  60.94 ( 51.61)	Acc@5  96.09 ( 93.89)
Epoch: [1][150/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.002)	Loss 1.1562e+00 (1.3293e+00)	Acc@1  60.94 ( 51.81)	Acc@5  92.19 ( 93.85)
Epoch: [1][160/391]	Time  0.132 ( 0.136)	Data  0.001 ( 0.002)	Loss 1.1289e+00 (1.3219e+00)	Acc@1  57.03 ( 52.01)	Acc@5  96.09 ( 93.92)
Epoch: [1][170/391]	Time  0.135 ( 0.136)	Data  0.001 ( 0.002)	Loss 1.2830e+00 (1.3158e+00)	Acc@1  57.81 ( 52.34)	Acc@5  95.31 ( 94.01)
Epoch: [1][180/391]	Time  0.138 ( 0.136)	Data  0.001 ( 0.002)	Loss 1.2529e+00 (1.3124e+00)	Acc@1  55.47 ( 52.49)	Acc@5  93.75 ( 94.03)
Epoch: [1][190/391]	Time  0.132 ( 0.136)	Data  0.001 ( 0.002)	Loss 1.3549e+00 (1.3068e+00)	Acc@1  49.22 ( 52.68)	Acc@5  92.97 ( 94.10)
Epoch: [1][200/391]	Time  0.133 ( 0.136)	Data  0.001 ( 0.002)	Loss 1.2550e+00 (1.3010e+00)	Acc@1  54.69 ( 52.90)	Acc@5  94.53 ( 94.13)
Epoch: [1][210/391]	Time  0.132 ( 0.136)	Data  0.001 ( 0.002)	Loss 1.2232e+00 (1.2970e+00)	Acc@1  53.12 ( 53.07)	Acc@5  96.88 ( 94.18)
Epoch: [1][220/391]	Time  0.132 ( 0.136)	Data  0.001 ( 0.002)	Loss 1.1510e+00 (1.2914e+00)	Acc@1  60.94 ( 53.28)	Acc@5  98.44 ( 94.23)
Epoch: [1][230/391]	Time  0.132 ( 0.136)	Data  0.001 ( 0.002)	Loss 1.0650e+00 (1.2864e+00)	Acc@1  60.94 ( 53.43)	Acc@5  98.44 ( 94.34)
Epoch: [1][240/391]	Time  0.136 ( 0.136)	Data  0.001 ( 0.002)	Loss 1.2602e+00 (1.2823e+00)	Acc@1  53.12 ( 53.60)	Acc@5  93.75 ( 94.39)
Epoch: [1][250/391]	Time  0.133 ( 0.136)	Data  0.001 ( 0.002)	Loss 1.1347e+00 (1.2793e+00)	Acc@1  53.91 ( 53.67)	Acc@5  96.88 ( 94.46)
Epoch: [1][260/391]	Time  0.132 ( 0.136)	Data  0.001 ( 0.002)	Loss 1.2519e+00 (1.2732e+00)	Acc@1  53.91 ( 53.87)	Acc@5  93.75 ( 94.51)
Epoch: [1][270/391]	Time  0.137 ( 0.136)	Data  0.001 ( 0.002)	Loss 1.0276e+00 (1.2701e+00)	Acc@1  63.28 ( 53.95)	Acc@5  98.44 ( 94.55)
Epoch: [1][280/391]	Time  0.132 ( 0.136)	Data  0.001 ( 0.002)	Loss 1.2046e+00 (1.2668e+00)	Acc@1  53.12 ( 54.03)	Acc@5  96.88 ( 94.60)
Epoch: [1][290/391]	Time  0.131 ( 0.136)	Data  0.001 ( 0.002)	Loss 1.1381e+00 (1.2639e+00)	Acc@1  60.94 ( 54.16)	Acc@5  92.97 ( 94.61)
Epoch: [1][300/391]	Time  0.132 ( 0.136)	Data  0.001 ( 0.002)	Loss 1.1545e+00 (1.2617e+00)	Acc@1  57.03 ( 54.25)	Acc@5  96.88 ( 94.67)
Epoch: [1][310/391]	Time  0.134 ( 0.136)	Data  0.001 ( 0.002)	Loss 1.1971e+00 (1.2581e+00)	Acc@1  58.59 ( 54.40)	Acc@5  95.31 ( 94.68)
Epoch: [1][320/391]	Time  0.138 ( 0.136)	Data  0.001 ( 0.002)	Loss 1.2588e+00 (1.2542e+00)	Acc@1  55.47 ( 54.52)	Acc@5  94.53 ( 94.72)
Epoch: [1][330/391]	Time  0.132 ( 0.136)	Data  0.001 ( 0.002)	Loss 1.2823e+00 (1.2496e+00)	Acc@1  50.78 ( 54.68)	Acc@5  92.19 ( 94.78)
Epoch: [1][340/391]	Time  0.134 ( 0.136)	Data  0.001 ( 0.002)	Loss 1.0921e+00 (1.2459e+00)	Acc@1  57.81 ( 54.84)	Acc@5  97.66 ( 94.78)
Epoch: [1][350/391]	Time  0.135 ( 0.136)	Data  0.001 ( 0.002)	Loss 1.0837e+00 (1.2420e+00)	Acc@1  61.72 ( 55.02)	Acc@5  92.19 ( 94.78)
Epoch: [1][360/391]	Time  0.134 ( 0.136)	Data  0.001 ( 0.002)	Loss 1.0888e+00 (1.2368e+00)	Acc@1  63.28 ( 55.21)	Acc@5  97.66 ( 94.83)
Epoch: [1][370/391]	Time  0.136 ( 0.136)	Data  0.001 ( 0.002)	Loss 1.0286e+00 (1.2320e+00)	Acc@1  57.81 ( 55.38)	Acc@5  97.66 ( 94.87)
Epoch: [1][380/391]	Time  0.135 ( 0.136)	Data  0.001 ( 0.002)	Loss 1.0196e+00 (1.2287e+00)	Acc@1  60.94 ( 55.52)	Acc@5  96.09 ( 94.91)
Epoch: [1][390/391]	Time  0.112 ( 0.135)	Data  0.001 ( 0.002)	Loss 1.0578e+00 (1.2249e+00)	Acc@1  66.25 ( 55.65)	Acc@5  95.00 ( 94.94)
## e[1] optimizer.zero_grad (sum) time: 0.6829361915588379
## e[1]       loss.backward (sum) time: 13.786210536956787
## e[1]      optimizer.step (sum) time: 14.462273120880127
## epoch[1] training(only) time: 53.051851987838745
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 9.5436e-01 (9.5436e-01)	Acc@1  62.00 ( 62.00)	Acc@5  97.00 ( 97.00)
Test: [ 10/100]	Time  0.046 ( 0.059)	Loss 1.0589e+00 (1.0583e+00)	Acc@1  59.00 ( 61.91)	Acc@5  97.00 ( 97.36)
Test: [ 20/100]	Time  0.045 ( 0.054)	Loss 1.1648e+00 (1.0755e+00)	Acc@1  55.00 ( 62.14)	Acc@5  96.00 ( 96.90)
Test: [ 30/100]	Time  0.045 ( 0.051)	Loss 9.5144e-01 (1.0889e+00)	Acc@1  65.00 ( 61.52)	Acc@5  95.00 ( 96.58)
Test: [ 40/100]	Time  0.048 ( 0.050)	Loss 9.9203e-01 (1.0904e+00)	Acc@1  67.00 ( 61.20)	Acc@5  94.00 ( 96.34)
Test: [ 50/100]	Time  0.045 ( 0.050)	Loss 8.3534e-01 (1.0763e+00)	Acc@1  72.00 ( 62.10)	Acc@5  99.00 ( 96.57)
Test: [ 60/100]	Time  0.045 ( 0.049)	Loss 1.1069e+00 (1.0738e+00)	Acc@1  57.00 ( 62.03)	Acc@5  99.00 ( 96.72)
Test: [ 70/100]	Time  0.045 ( 0.049)	Loss 1.0274e+00 (1.0803e+00)	Acc@1  63.00 ( 61.66)	Acc@5  94.00 ( 96.70)
Test: [ 80/100]	Time  0.044 ( 0.048)	Loss 8.8458e-01 (1.0758e+00)	Acc@1  68.00 ( 61.78)	Acc@5  98.00 ( 96.78)
Test: [ 90/100]	Time  0.045 ( 0.048)	Loss 1.0992e+00 (1.0779e+00)	Acc@1  62.00 ( 61.45)	Acc@5  97.00 ( 96.84)
 * Acc@1 61.610 Acc@5 96.840
### epoch[1] execution time: 57.92282247543335
EPOCH 2
# current learning rate: 0.1
# Switched to train mode...
Epoch: [2][  0/391]	Time  0.306 ( 0.306)	Data  0.164 ( 0.164)	Loss 8.1981e-01 (8.1981e-01)	Acc@1  71.09 ( 71.09)	Acc@5  98.44 ( 98.44)
Epoch: [2][ 10/391]	Time  0.131 ( 0.150)	Data  0.001 ( 0.016)	Loss 9.4247e-01 (1.0217e+00)	Acc@1  64.84 ( 62.57)	Acc@5  96.09 ( 96.80)
Epoch: [2][ 20/391]	Time  0.131 ( 0.144)	Data  0.001 ( 0.009)	Loss 1.1356e+00 (1.0119e+00)	Acc@1  59.38 ( 63.47)	Acc@5  93.75 ( 96.35)
Epoch: [2][ 30/391]	Time  0.141 ( 0.141)	Data  0.001 ( 0.006)	Loss 9.2252e-01 (1.0090e+00)	Acc@1  63.28 ( 63.33)	Acc@5  97.66 ( 96.93)
Epoch: [2][ 40/391]	Time  0.133 ( 0.140)	Data  0.001 ( 0.005)	Loss 9.0700e-01 (1.0196e+00)	Acc@1  68.75 ( 63.15)	Acc@5  96.88 ( 96.76)
Epoch: [2][ 50/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.004)	Loss 8.1823e-01 (1.0186e+00)	Acc@1  67.97 ( 63.17)	Acc@5  98.44 ( 96.72)
Epoch: [2][ 60/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.004)	Loss 1.0642e+00 (1.0153e+00)	Acc@1  59.38 ( 63.41)	Acc@5  96.88 ( 96.68)
Epoch: [2][ 70/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.003)	Loss 8.4784e-01 (1.0161e+00)	Acc@1  71.09 ( 63.46)	Acc@5  96.88 ( 96.62)
Epoch: [2][ 80/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.003)	Loss 9.4727e-01 (1.0205e+00)	Acc@1  65.62 ( 63.36)	Acc@5  97.66 ( 96.64)
Epoch: [2][ 90/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.003)	Loss 9.8325e-01 (1.0191e+00)	Acc@1  64.84 ( 63.48)	Acc@5  93.75 ( 96.55)
Epoch: [2][100/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.003)	Loss 8.7163e-01 (1.0151e+00)	Acc@1  70.31 ( 63.58)	Acc@5  98.44 ( 96.62)
Epoch: [2][110/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.003)	Loss 8.8166e-01 (1.0098e+00)	Acc@1  68.75 ( 63.70)	Acc@5  96.88 ( 96.61)
Epoch: [2][120/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.002)	Loss 1.1064e+00 (1.0104e+00)	Acc@1  61.72 ( 63.67)	Acc@5  92.97 ( 96.58)
Epoch: [2][130/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.002)	Loss 1.0093e+00 (1.0108e+00)	Acc@1  67.19 ( 63.65)	Acc@5  93.75 ( 96.57)
Epoch: [2][140/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.002)	Loss 8.5317e-01 (1.0059e+00)	Acc@1  74.22 ( 63.92)	Acc@5  96.09 ( 96.60)
Epoch: [2][150/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.002)	Loss 9.7991e-01 (1.0033e+00)	Acc@1  64.06 ( 64.06)	Acc@5  97.66 ( 96.67)
Epoch: [2][160/391]	Time  0.132 ( 0.136)	Data  0.001 ( 0.002)	Loss 1.1119e+00 (1.0044e+00)	Acc@1  57.81 ( 64.02)	Acc@5  96.09 ( 96.67)
Epoch: [2][170/391]	Time  0.132 ( 0.136)	Data  0.001 ( 0.002)	Loss 1.0202e+00 (1.0021e+00)	Acc@1  64.84 ( 64.09)	Acc@5  95.31 ( 96.67)
Epoch: [2][180/391]	Time  0.134 ( 0.136)	Data  0.001 ( 0.002)	Loss 9.7906e-01 (9.9956e-01)	Acc@1  66.41 ( 64.12)	Acc@5  98.44 ( 96.73)
Epoch: [2][190/391]	Time  0.139 ( 0.136)	Data  0.001 ( 0.002)	Loss 9.6342e-01 (9.9949e-01)	Acc@1  64.06 ( 64.14)	Acc@5  96.09 ( 96.70)
Epoch: [2][200/391]	Time  0.132 ( 0.136)	Data  0.001 ( 0.002)	Loss 9.2620e-01 (9.9318e-01)	Acc@1  66.41 ( 64.36)	Acc@5  96.09 ( 96.75)
Epoch: [2][210/391]	Time  0.141 ( 0.136)	Data  0.001 ( 0.002)	Loss 9.9794e-01 (9.8978e-01)	Acc@1  65.62 ( 64.54)	Acc@5  99.22 ( 96.78)
Epoch: [2][220/391]	Time  0.136 ( 0.136)	Data  0.001 ( 0.002)	Loss 1.0353e+00 (9.8903e-01)	Acc@1  66.41 ( 64.55)	Acc@5  94.53 ( 96.79)
Epoch: [2][230/391]	Time  0.138 ( 0.136)	Data  0.001 ( 0.002)	Loss 1.0128e+00 (9.8707e-01)	Acc@1  61.72 ( 64.62)	Acc@5  96.09 ( 96.80)
Epoch: [2][240/391]	Time  0.135 ( 0.136)	Data  0.001 ( 0.002)	Loss 9.4015e-01 (9.8406e-01)	Acc@1  71.88 ( 64.75)	Acc@5  96.09 ( 96.82)
Epoch: [2][250/391]	Time  0.150 ( 0.136)	Data  0.001 ( 0.002)	Loss 9.2006e-01 (9.8064e-01)	Acc@1  71.88 ( 64.89)	Acc@5  96.09 ( 96.85)
Epoch: [2][260/391]	Time  0.134 ( 0.136)	Data  0.001 ( 0.002)	Loss 8.7112e-01 (9.7918e-01)	Acc@1  68.75 ( 64.95)	Acc@5  96.09 ( 96.86)
Epoch: [2][270/391]	Time  0.134 ( 0.136)	Data  0.001 ( 0.002)	Loss 8.3645e-01 (9.7638e-01)	Acc@1  68.75 ( 65.05)	Acc@5  97.66 ( 96.88)
Epoch: [2][280/391]	Time  0.145 ( 0.136)	Data  0.001 ( 0.002)	Loss 9.4753e-01 (9.7432e-01)	Acc@1  67.19 ( 65.11)	Acc@5 100.00 ( 96.93)
Epoch: [2][290/391]	Time  0.134 ( 0.136)	Data  0.001 ( 0.002)	Loss 1.0934e+00 (9.7176e-01)	Acc@1  60.94 ( 65.20)	Acc@5  93.75 ( 96.94)
Epoch: [2][300/391]	Time  0.132 ( 0.136)	Data  0.001 ( 0.002)	Loss 9.4710e-01 (9.7131e-01)	Acc@1  65.62 ( 65.19)	Acc@5  97.66 ( 96.94)
Epoch: [2][310/391]	Time  0.132 ( 0.136)	Data  0.001 ( 0.002)	Loss 9.2196e-01 (9.6854e-01)	Acc@1  67.97 ( 65.33)	Acc@5  97.66 ( 96.97)
Epoch: [2][320/391]	Time  0.133 ( 0.136)	Data  0.001 ( 0.002)	Loss 7.6558e-01 (9.6626e-01)	Acc@1  73.44 ( 65.43)	Acc@5  98.44 ( 96.98)
Epoch: [2][330/391]	Time  0.136 ( 0.136)	Data  0.001 ( 0.002)	Loss 8.9598e-01 (9.6327e-01)	Acc@1  70.31 ( 65.56)	Acc@5  99.22 ( 97.00)
Epoch: [2][340/391]	Time  0.136 ( 0.136)	Data  0.001 ( 0.002)	Loss 9.3723e-01 (9.6307e-01)	Acc@1  67.19 ( 65.60)	Acc@5  99.22 ( 96.99)
Epoch: [2][350/391]	Time  0.135 ( 0.136)	Data  0.001 ( 0.002)	Loss 7.8818e-01 (9.6077e-01)	Acc@1  67.97 ( 65.67)	Acc@5  99.22 ( 97.00)
Epoch: [2][360/391]	Time  0.134 ( 0.136)	Data  0.001 ( 0.002)	Loss 7.9416e-01 (9.5868e-01)	Acc@1  71.09 ( 65.74)	Acc@5  98.44 ( 97.02)
Epoch: [2][370/391]	Time  0.137 ( 0.136)	Data  0.001 ( 0.002)	Loss 9.1027e-01 (9.5814e-01)	Acc@1  67.19 ( 65.75)	Acc@5  98.44 ( 97.03)
Epoch: [2][380/391]	Time  0.136 ( 0.136)	Data  0.001 ( 0.002)	Loss 9.4461e-01 (9.5741e-01)	Acc@1  70.31 ( 65.79)	Acc@5 100.00 ( 97.04)
Epoch: [2][390/391]	Time  0.107 ( 0.136)	Data  0.001 ( 0.002)	Loss 8.1515e-01 (9.5587e-01)	Acc@1  70.00 ( 65.83)	Acc@5  95.00 ( 97.06)
## e[2] optimizer.zero_grad (sum) time: 0.6916816234588623
## e[2]       loss.backward (sum) time: 13.799654006958008
## e[2]      optimizer.step (sum) time: 14.558077335357666
## epoch[2] training(only) time: 53.24087595939636
# Switched to evaluate mode...
Test: [  0/100]	Time  0.190 ( 0.190)	Loss 7.8407e-01 (7.8407e-01)	Acc@1  75.00 ( 75.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.045 ( 0.062)	Loss 6.8429e-01 (8.4675e-01)	Acc@1  79.00 ( 70.82)	Acc@5  97.00 ( 98.09)
Test: [ 20/100]	Time  0.048 ( 0.056)	Loss 9.8172e-01 (8.6189e-01)	Acc@1  63.00 ( 70.19)	Acc@5  98.00 ( 97.95)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 7.4960e-01 (8.7524e-01)	Acc@1  75.00 ( 69.68)	Acc@5  99.00 ( 97.77)
Test: [ 40/100]	Time  0.046 ( 0.052)	Loss 8.2977e-01 (8.7636e-01)	Acc@1  73.00 ( 69.54)	Acc@5  97.00 ( 97.61)
Test: [ 50/100]	Time  0.045 ( 0.050)	Loss 8.0405e-01 (8.7037e-01)	Acc@1  66.00 ( 69.86)	Acc@5  98.00 ( 97.67)
Test: [ 60/100]	Time  0.045 ( 0.050)	Loss 8.9108e-01 (8.7209e-01)	Acc@1  59.00 ( 69.62)	Acc@5 100.00 ( 97.75)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 7.5597e-01 (8.7791e-01)	Acc@1  73.00 ( 69.39)	Acc@5 100.00 ( 97.76)
Test: [ 80/100]	Time  0.045 ( 0.049)	Loss 7.7672e-01 (8.7644e-01)	Acc@1  73.00 ( 69.36)	Acc@5  99.00 ( 97.81)
Test: [ 90/100]	Time  0.045 ( 0.049)	Loss 8.2359e-01 (8.7859e-01)	Acc@1  70.00 ( 69.14)	Acc@5 100.00 ( 97.79)
 * Acc@1 69.300 Acc@5 97.780
### epoch[2] execution time: 58.216817140579224
EPOCH 3
# current learning rate: 0.1
# Switched to train mode...
Epoch: [3][  0/391]	Time  0.295 ( 0.295)	Data  0.150 ( 0.150)	Loss 8.3645e-01 (8.3645e-01)	Acc@1  72.66 ( 72.66)	Acc@5  97.66 ( 97.66)
Epoch: [3][ 10/391]	Time  0.142 ( 0.151)	Data  0.001 ( 0.015)	Loss 1.1219e+00 (8.7697e-01)	Acc@1  62.50 ( 69.25)	Acc@5  92.97 ( 97.51)
Epoch: [3][ 20/391]	Time  0.132 ( 0.143)	Data  0.001 ( 0.008)	Loss 8.0325e-01 (8.8901e-01)	Acc@1  68.75 ( 68.68)	Acc@5  96.88 ( 97.32)
Epoch: [3][ 30/391]	Time  0.133 ( 0.141)	Data  0.001 ( 0.006)	Loss 7.5759e-01 (8.6406e-01)	Acc@1  77.34 ( 69.35)	Acc@5  97.66 ( 97.45)
Epoch: [3][ 40/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.005)	Loss 9.6806e-01 (8.5391e-01)	Acc@1  64.06 ( 69.70)	Acc@5  96.88 ( 97.62)
Epoch: [3][ 50/391]	Time  0.132 ( 0.139)	Data  0.001 ( 0.004)	Loss 1.0360e+00 (8.5663e-01)	Acc@1  64.84 ( 69.41)	Acc@5  96.09 ( 97.58)
Epoch: [3][ 60/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.004)	Loss 9.2210e-01 (8.4814e-01)	Acc@1  66.41 ( 69.62)	Acc@5  95.31 ( 97.71)
Epoch: [3][ 70/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.003)	Loss 7.3558e-01 (8.4858e-01)	Acc@1  76.56 ( 69.59)	Acc@5  99.22 ( 97.66)
Epoch: [3][ 80/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.003)	Loss 8.0242e-01 (8.5156e-01)	Acc@1  70.31 ( 69.56)	Acc@5  99.22 ( 97.68)
Epoch: [3][ 90/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.003)	Loss 8.3187e-01 (8.4771e-01)	Acc@1  69.53 ( 69.65)	Acc@5  99.22 ( 97.71)
Epoch: [3][100/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.003)	Loss 1.1467e+00 (8.4988e-01)	Acc@1  56.25 ( 69.56)	Acc@5  96.09 ( 97.66)
Epoch: [3][110/391]	Time  0.144 ( 0.137)	Data  0.001 ( 0.002)	Loss 8.3111e-01 (8.4686e-01)	Acc@1  71.09 ( 69.93)	Acc@5  98.44 ( 97.71)
Epoch: [3][120/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.002)	Loss 7.7389e-01 (8.4642e-01)	Acc@1  69.53 ( 69.96)	Acc@5  96.88 ( 97.69)
Epoch: [3][130/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.002)	Loss 9.1420e-01 (8.4731e-01)	Acc@1  66.41 ( 69.91)	Acc@5  98.44 ( 97.66)
Epoch: [3][140/391]	Time  0.148 ( 0.137)	Data  0.001 ( 0.002)	Loss 7.3480e-01 (8.4500e-01)	Acc@1  72.66 ( 69.92)	Acc@5  99.22 ( 97.71)
Epoch: [3][150/391]	Time  0.131 ( 0.137)	Data  0.001 ( 0.002)	Loss 6.3146e-01 (8.4351e-01)	Acc@1  79.69 ( 70.00)	Acc@5  99.22 ( 97.73)
Epoch: [3][160/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.002)	Loss 6.9726e-01 (8.4277e-01)	Acc@1  75.00 ( 70.01)	Acc@5  97.66 ( 97.75)
Epoch: [3][170/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.002)	Loss 8.3434e-01 (8.4010e-01)	Acc@1  70.31 ( 70.10)	Acc@5  96.09 ( 97.77)
Epoch: [3][180/391]	Time  0.135 ( 0.136)	Data  0.001 ( 0.002)	Loss 7.7954e-01 (8.3884e-01)	Acc@1  73.44 ( 70.14)	Acc@5  97.66 ( 97.78)
Epoch: [3][190/391]	Time  0.134 ( 0.136)	Data  0.001 ( 0.002)	Loss 6.2863e-01 (8.3727e-01)	Acc@1  76.56 ( 70.16)	Acc@5 100.00 ( 97.79)
Epoch: [3][200/391]	Time  0.135 ( 0.136)	Data  0.001 ( 0.002)	Loss 7.2384e-01 (8.3537e-01)	Acc@1  75.00 ( 70.21)	Acc@5 100.00 ( 97.83)
Epoch: [3][210/391]	Time  0.132 ( 0.136)	Data  0.001 ( 0.002)	Loss 7.5115e-01 (8.3197e-01)	Acc@1  70.31 ( 70.28)	Acc@5  98.44 ( 97.83)
Epoch: [3][220/391]	Time  0.138 ( 0.136)	Data  0.001 ( 0.002)	Loss 8.2449e-01 (8.3120e-01)	Acc@1  72.66 ( 70.32)	Acc@5  98.44 ( 97.83)
Epoch: [3][230/391]	Time  0.134 ( 0.136)	Data  0.001 ( 0.002)	Loss 8.7978e-01 (8.3219e-01)	Acc@1  67.97 ( 70.27)	Acc@5 100.00 ( 97.87)
Epoch: [3][240/391]	Time  0.134 ( 0.136)	Data  0.001 ( 0.002)	Loss 7.4852e-01 (8.3030e-01)	Acc@1  71.88 ( 70.28)	Acc@5  98.44 ( 97.88)
Epoch: [3][250/391]	Time  0.145 ( 0.136)	Data  0.001 ( 0.002)	Loss 8.6880e-01 (8.2938e-01)	Acc@1  69.53 ( 70.33)	Acc@5  98.44 ( 97.86)
Epoch: [3][260/391]	Time  0.133 ( 0.136)	Data  0.001 ( 0.002)	Loss 9.8067e-01 (8.2880e-01)	Acc@1  64.84 ( 70.37)	Acc@5  98.44 ( 97.88)
Epoch: [3][270/391]	Time  0.138 ( 0.136)	Data  0.001 ( 0.002)	Loss 6.7309e-01 (8.2645e-01)	Acc@1  76.56 ( 70.49)	Acc@5  96.09 ( 97.88)
Epoch: [3][280/391]	Time  0.133 ( 0.136)	Data  0.001 ( 0.002)	Loss 7.6538e-01 (8.2461e-01)	Acc@1  76.56 ( 70.56)	Acc@5  98.44 ( 97.89)
Epoch: [3][290/391]	Time  0.135 ( 0.136)	Data  0.001 ( 0.002)	Loss 7.3324e-01 (8.2098e-01)	Acc@1  75.00 ( 70.74)	Acc@5 100.00 ( 97.92)
Epoch: [3][300/391]	Time  0.131 ( 0.136)	Data  0.001 ( 0.002)	Loss 7.4248e-01 (8.1844e-01)	Acc@1  72.66 ( 70.82)	Acc@5  98.44 ( 97.92)
Epoch: [3][310/391]	Time  0.135 ( 0.136)	Data  0.001 ( 0.002)	Loss 9.0632e-01 (8.1658e-01)	Acc@1  67.19 ( 70.87)	Acc@5  97.66 ( 97.94)
Epoch: [3][320/391]	Time  0.132 ( 0.136)	Data  0.001 ( 0.002)	Loss 8.0083e-01 (8.1396e-01)	Acc@1  71.88 ( 70.94)	Acc@5  98.44 ( 97.95)
Epoch: [3][330/391]	Time  0.136 ( 0.136)	Data  0.001 ( 0.002)	Loss 7.5562e-01 (8.1339e-01)	Acc@1  74.22 ( 70.95)	Acc@5  96.88 ( 97.96)
Epoch: [3][340/391]	Time  0.136 ( 0.136)	Data  0.001 ( 0.002)	Loss 7.2399e-01 (8.1171e-01)	Acc@1  75.78 ( 70.98)	Acc@5  97.66 ( 97.97)
Epoch: [3][350/391]	Time  0.136 ( 0.136)	Data  0.001 ( 0.002)	Loss 7.3485e-01 (8.0982e-01)	Acc@1  75.78 ( 71.07)	Acc@5  98.44 ( 97.97)
Epoch: [3][360/391]	Time  0.137 ( 0.136)	Data  0.001 ( 0.002)	Loss 6.6493e-01 (8.0978e-01)	Acc@1  75.78 ( 71.13)	Acc@5 100.00 ( 97.98)
Epoch: [3][370/391]	Time  0.135 ( 0.136)	Data  0.001 ( 0.002)	Loss 7.7778e-01 (8.0935e-01)	Acc@1  74.22 ( 71.15)	Acc@5  97.66 ( 97.99)
Epoch: [3][380/391]	Time  0.137 ( 0.136)	Data  0.001 ( 0.002)	Loss 7.4610e-01 (8.0905e-01)	Acc@1  76.56 ( 71.18)	Acc@5  96.88 ( 98.00)
Epoch: [3][390/391]	Time  0.104 ( 0.136)	Data  0.001 ( 0.002)	Loss 8.6626e-01 (8.0805e-01)	Acc@1  67.50 ( 71.22)	Acc@5 100.00 ( 97.99)
## e[3] optimizer.zero_grad (sum) time: 0.6869959831237793
## e[3]       loss.backward (sum) time: 13.779710054397583
## e[3]      optimizer.step (sum) time: 14.618895769119263
## epoch[3] training(only) time: 53.24649906158447
# Switched to evaluate mode...
Test: [  0/100]	Time  0.202 ( 0.202)	Loss 7.8650e-01 (7.8650e-01)	Acc@1  71.00 ( 71.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.045 ( 0.061)	Loss 8.6506e-01 (9.0640e-01)	Acc@1  69.00 ( 69.73)	Acc@5  97.00 ( 98.36)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 8.6368e-01 (9.0611e-01)	Acc@1  69.00 ( 69.10)	Acc@5  97.00 ( 98.05)
Test: [ 30/100]	Time  0.048 ( 0.052)	Loss 7.9524e-01 (9.0950e-01)	Acc@1  70.00 ( 68.61)	Acc@5  97.00 ( 97.84)
Test: [ 40/100]	Time  0.045 ( 0.051)	Loss 7.9554e-01 (9.1149e-01)	Acc@1  72.00 ( 68.44)	Acc@5  99.00 ( 97.85)
Test: [ 50/100]	Time  0.054 ( 0.050)	Loss 7.6711e-01 (9.0775e-01)	Acc@1  74.00 ( 68.90)	Acc@5  97.00 ( 97.75)
Test: [ 60/100]	Time  0.045 ( 0.050)	Loss 9.4824e-01 (9.0462e-01)	Acc@1  66.00 ( 69.07)	Acc@5  98.00 ( 97.87)
Test: [ 70/100]	Time  0.044 ( 0.050)	Loss 8.0916e-01 (9.0784e-01)	Acc@1  72.00 ( 68.79)	Acc@5 100.00 ( 97.83)
Test: [ 80/100]	Time  0.045 ( 0.049)	Loss 8.4624e-01 (9.0472e-01)	Acc@1  71.00 ( 68.88)	Acc@5  97.00 ( 97.78)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 9.6072e-01 (9.0538e-01)	Acc@1  65.00 ( 68.82)	Acc@5  98.00 ( 97.73)
 * Acc@1 69.010 Acc@5 97.760
### epoch[3] execution time: 58.19878959655762
EPOCH 4
# current learning rate: 0.1
# Switched to train mode...
Epoch: [4][  0/391]	Time  0.301 ( 0.301)	Data  0.144 ( 0.144)	Loss 7.2268e-01 (7.2268e-01)	Acc@1  74.22 ( 74.22)	Acc@5  98.44 ( 98.44)
Epoch: [4][ 10/391]	Time  0.135 ( 0.150)	Data  0.001 ( 0.014)	Loss 7.0114e-01 (7.6607e-01)	Acc@1  75.00 ( 72.09)	Acc@5 100.00 ( 98.65)
Epoch: [4][ 20/391]	Time  0.135 ( 0.143)	Data  0.001 ( 0.008)	Loss 7.9628e-01 (7.4484e-01)	Acc@1  72.66 ( 73.21)	Acc@5 100.00 ( 98.51)
Epoch: [4][ 30/391]	Time  0.135 ( 0.141)	Data  0.001 ( 0.006)	Loss 7.2331e-01 (7.2303e-01)	Acc@1  70.31 ( 74.04)	Acc@5 100.00 ( 98.46)
Epoch: [4][ 40/391]	Time  0.132 ( 0.139)	Data  0.001 ( 0.005)	Loss 6.5819e-01 (7.2152e-01)	Acc@1  71.88 ( 74.18)	Acc@5  99.22 ( 98.48)
Epoch: [4][ 50/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.004)	Loss 5.7576e-01 (7.1567e-01)	Acc@1  77.34 ( 74.28)	Acc@5  99.22 ( 98.50)
Epoch: [4][ 60/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.004)	Loss 7.4317e-01 (7.1645e-01)	Acc@1  68.75 ( 74.31)	Acc@5  98.44 ( 98.53)
Epoch: [4][ 70/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.003)	Loss 8.2305e-01 (7.1824e-01)	Acc@1  75.00 ( 74.27)	Acc@5  99.22 ( 98.49)
Epoch: [4][ 80/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.003)	Loss 6.7538e-01 (7.2026e-01)	Acc@1  76.56 ( 74.25)	Acc@5  98.44 ( 98.46)
Epoch: [4][ 90/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.003)	Loss 6.9699e-01 (7.1559e-01)	Acc@1  73.44 ( 74.35)	Acc@5 100.00 ( 98.51)
Epoch: [4][100/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.003)	Loss 9.0669e-01 (7.1827e-01)	Acc@1  64.06 ( 74.20)	Acc@5 100.00 ( 98.51)
Epoch: [4][110/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.002)	Loss 6.9363e-01 (7.1736e-01)	Acc@1  77.34 ( 74.29)	Acc@5  99.22 ( 98.52)
Epoch: [4][120/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.002)	Loss 8.0944e-01 (7.2138e-01)	Acc@1  74.22 ( 74.15)	Acc@5  97.66 ( 98.53)
Epoch: [4][130/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.002)	Loss 7.4475e-01 (7.1831e-01)	Acc@1  71.09 ( 74.24)	Acc@5  99.22 ( 98.53)
Epoch: [4][140/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.002)	Loss 7.0836e-01 (7.1837e-01)	Acc@1  75.00 ( 74.33)	Acc@5  98.44 ( 98.49)
Epoch: [4][150/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.002)	Loss 8.0645e-01 (7.1400e-01)	Acc@1  70.31 ( 74.47)	Acc@5  99.22 ( 98.51)
Epoch: [4][160/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.002)	Loss 5.8917e-01 (7.1223e-01)	Acc@1  78.91 ( 74.60)	Acc@5 100.00 ( 98.53)
Epoch: [4][170/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.002)	Loss 6.6606e-01 (7.1257e-01)	Acc@1  72.66 ( 74.54)	Acc@5 100.00 ( 98.54)
Epoch: [4][180/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.002)	Loss 6.9170e-01 (7.1217e-01)	Acc@1  78.91 ( 74.59)	Acc@5 100.00 ( 98.53)
Epoch: [4][190/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.002)	Loss 7.6396e-01 (7.1357e-01)	Acc@1  73.44 ( 74.59)	Acc@5  97.66 ( 98.53)
Epoch: [4][200/391]	Time  0.134 ( 0.136)	Data  0.001 ( 0.002)	Loss 6.7689e-01 (7.1117e-01)	Acc@1  75.00 ( 74.69)	Acc@5  98.44 ( 98.51)
Epoch: [4][210/391]	Time  0.135 ( 0.136)	Data  0.001 ( 0.002)	Loss 7.0801e-01 (7.1076e-01)	Acc@1  74.22 ( 74.69)	Acc@5  98.44 ( 98.53)
Epoch: [4][220/391]	Time  0.132 ( 0.136)	Data  0.001 ( 0.002)	Loss 6.2069e-01 (7.0951e-01)	Acc@1  82.03 ( 74.81)	Acc@5  99.22 ( 98.53)
Epoch: [4][230/391]	Time  0.146 ( 0.137)	Data  0.001 ( 0.002)	Loss 6.2764e-01 (7.0728e-01)	Acc@1  75.78 ( 74.86)	Acc@5  99.22 ( 98.56)
Epoch: [4][240/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.002)	Loss 9.5804e-01 (7.0783e-01)	Acc@1  67.97 ( 74.81)	Acc@5  95.31 ( 98.55)
Epoch: [4][250/391]	Time  0.135 ( 0.136)	Data  0.001 ( 0.002)	Loss 6.9568e-01 (7.0739e-01)	Acc@1  77.34 ( 74.79)	Acc@5  98.44 ( 98.56)
Epoch: [4][260/391]	Time  0.133 ( 0.136)	Data  0.001 ( 0.002)	Loss 6.8320e-01 (7.0423e-01)	Acc@1  72.66 ( 74.88)	Acc@5 100.00 ( 98.58)
Epoch: [4][270/391]	Time  0.134 ( 0.136)	Data  0.001 ( 0.002)	Loss 6.9294e-01 (7.0389e-01)	Acc@1  78.91 ( 74.92)	Acc@5  96.88 ( 98.58)
Epoch: [4][280/391]	Time  0.141 ( 0.136)	Data  0.001 ( 0.002)	Loss 5.8837e-01 (7.0326e-01)	Acc@1  80.47 ( 74.93)	Acc@5  99.22 ( 98.58)
Epoch: [4][290/391]	Time  0.133 ( 0.136)	Data  0.001 ( 0.002)	Loss 8.2046e-01 (7.0331e-01)	Acc@1  71.09 ( 74.95)	Acc@5  99.22 ( 98.58)
Epoch: [4][300/391]	Time  0.133 ( 0.136)	Data  0.001 ( 0.002)	Loss 6.5909e-01 (7.0414e-01)	Acc@1  75.78 ( 74.92)	Acc@5 100.00 ( 98.58)
Epoch: [4][310/391]	Time  0.145 ( 0.136)	Data  0.001 ( 0.002)	Loss 7.6259e-01 (7.0419e-01)	Acc@1  68.75 ( 74.91)	Acc@5  99.22 ( 98.58)
Epoch: [4][320/391]	Time  0.137 ( 0.136)	Data  0.001 ( 0.002)	Loss 6.5444e-01 (7.0337e-01)	Acc@1  77.34 ( 74.97)	Acc@5  99.22 ( 98.58)
Epoch: [4][330/391]	Time  0.135 ( 0.136)	Data  0.001 ( 0.002)	Loss 6.8180e-01 (7.0376e-01)	Acc@1  77.34 ( 74.97)	Acc@5  98.44 ( 98.58)
Epoch: [4][340/391]	Time  0.143 ( 0.136)	Data  0.001 ( 0.002)	Loss 7.7856e-01 (7.0134e-01)	Acc@1  72.66 ( 75.06)	Acc@5 100.00 ( 98.60)
Epoch: [4][350/391]	Time  0.133 ( 0.136)	Data  0.001 ( 0.002)	Loss 6.9856e-01 (7.0029e-01)	Acc@1  77.34 ( 75.11)	Acc@5  99.22 ( 98.61)
Epoch: [4][360/391]	Time  0.139 ( 0.136)	Data  0.001 ( 0.002)	Loss 7.2955e-01 (6.9976e-01)	Acc@1  78.91 ( 75.12)	Acc@5 100.00 ( 98.62)
Epoch: [4][370/391]	Time  0.135 ( 0.136)	Data  0.001 ( 0.002)	Loss 8.2774e-01 (6.9932e-01)	Acc@1  70.31 ( 75.12)	Acc@5  96.88 ( 98.61)
Epoch: [4][380/391]	Time  0.133 ( 0.136)	Data  0.001 ( 0.002)	Loss 6.8484e-01 (6.9912e-01)	Acc@1  75.78 ( 75.13)	Acc@5  97.66 ( 98.60)
Epoch: [4][390/391]	Time  0.111 ( 0.136)	Data  0.001 ( 0.002)	Loss 6.2873e-01 (6.9908e-01)	Acc@1  82.50 ( 75.12)	Acc@5  98.75 ( 98.61)
## e[4] optimizer.zero_grad (sum) time: 0.6858196258544922
## e[4]       loss.backward (sum) time: 13.841621160507202
## e[4]      optimizer.step (sum) time: 14.619575262069702
## epoch[4] training(only) time: 53.37562608718872
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 6.8977e-01 (6.8977e-01)	Acc@1  74.00 ( 74.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.048 ( 0.061)	Loss 6.6832e-01 (7.4587e-01)	Acc@1  75.00 ( 73.27)	Acc@5  98.00 ( 98.09)
Test: [ 20/100]	Time  0.045 ( 0.055)	Loss 7.8659e-01 (7.8781e-01)	Acc@1  76.00 ( 72.19)	Acc@5  99.00 ( 98.00)
Test: [ 30/100]	Time  0.045 ( 0.052)	Loss 7.4964e-01 (7.9479e-01)	Acc@1  72.00 ( 72.48)	Acc@5  97.00 ( 97.94)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 6.9045e-01 (7.9118e-01)	Acc@1  76.00 ( 72.76)	Acc@5  97.00 ( 97.95)
Test: [ 50/100]	Time  0.045 ( 0.051)	Loss 6.5129e-01 (7.8380e-01)	Acc@1  77.00 ( 72.92)	Acc@5  99.00 ( 98.12)
Test: [ 60/100]	Time  0.045 ( 0.050)	Loss 7.2371e-01 (7.8310e-01)	Acc@1  73.00 ( 73.16)	Acc@5  99.00 ( 98.20)
Test: [ 70/100]	Time  0.045 ( 0.049)	Loss 8.5577e-01 (7.8544e-01)	Acc@1  67.00 ( 72.89)	Acc@5  99.00 ( 98.24)
Test: [ 80/100]	Time  0.050 ( 0.049)	Loss 7.8641e-01 (7.8203e-01)	Acc@1  75.00 ( 73.02)	Acc@5  99.00 ( 98.30)
Test: [ 90/100]	Time  0.045 ( 0.049)	Loss 7.4455e-01 (7.7795e-01)	Acc@1  76.00 ( 73.09)	Acc@5  98.00 ( 98.31)
 * Acc@1 73.260 Acc@5 98.340
### epoch[4] execution time: 58.36326718330383
EPOCH 5
# current learning rate: 0.1
# Switched to train mode...
Epoch: [5][  0/391]	Time  0.294 ( 0.294)	Data  0.152 ( 0.152)	Loss 6.9947e-01 (6.9947e-01)	Acc@1  78.12 ( 78.12)	Acc@5  98.44 ( 98.44)
Epoch: [5][ 10/391]	Time  0.135 ( 0.151)	Data  0.001 ( 0.015)	Loss 6.4375e-01 (6.4565e-01)	Acc@1  78.12 ( 77.56)	Acc@5  99.22 ( 98.72)
Epoch: [5][ 20/391]	Time  0.135 ( 0.145)	Data  0.001 ( 0.008)	Loss 6.0842e-01 (6.3819e-01)	Acc@1  78.12 ( 78.12)	Acc@5  99.22 ( 98.62)
Epoch: [5][ 30/391]	Time  0.137 ( 0.142)	Data  0.001 ( 0.006)	Loss 6.4408e-01 (6.3577e-01)	Acc@1  78.12 ( 78.55)	Acc@5  99.22 ( 98.69)
Epoch: [5][ 40/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.005)	Loss 9.6492e-01 (6.4309e-01)	Acc@1  64.84 ( 77.97)	Acc@5  96.09 ( 98.70)
Epoch: [5][ 50/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.004)	Loss 5.1649e-01 (6.4403e-01)	Acc@1  86.72 ( 77.93)	Acc@5  98.44 ( 98.64)
Epoch: [5][ 60/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.004)	Loss 6.0222e-01 (6.4265e-01)	Acc@1  75.78 ( 77.89)	Acc@5  99.22 ( 98.72)
Epoch: [5][ 70/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.003)	Loss 6.1706e-01 (6.4531e-01)	Acc@1  78.12 ( 77.82)	Acc@5  98.44 ( 98.64)
Epoch: [5][ 80/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.003)	Loss 6.8007e-01 (6.4477e-01)	Acc@1  73.44 ( 77.66)	Acc@5  99.22 ( 98.58)
Epoch: [5][ 90/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.003)	Loss 6.6739e-01 (6.4009e-01)	Acc@1  75.78 ( 77.64)	Acc@5  99.22 ( 98.69)
Epoch: [5][100/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.003)	Loss 5.9388e-01 (6.3298e-01)	Acc@1  84.38 ( 77.85)	Acc@5  97.66 ( 98.74)
Epoch: [5][110/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.003)	Loss 6.3209e-01 (6.3225e-01)	Acc@1  75.00 ( 77.93)	Acc@5  98.44 ( 98.75)
Epoch: [5][120/391]	Time  0.148 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.4706e-01 (6.3233e-01)	Acc@1  78.91 ( 77.87)	Acc@5  99.22 ( 98.80)
Epoch: [5][130/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.1968e-01 (6.3608e-01)	Acc@1  76.56 ( 77.73)	Acc@5  97.66 ( 98.77)
Epoch: [5][140/391]	Time  0.145 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.2619e-01 (6.3567e-01)	Acc@1  81.25 ( 77.73)	Acc@5  99.22 ( 98.79)
Epoch: [5][150/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.4523e-01 (6.3330e-01)	Acc@1  80.47 ( 77.88)	Acc@5 100.00 ( 98.80)
Epoch: [5][160/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.2717e-01 (6.3223e-01)	Acc@1  79.69 ( 77.86)	Acc@5 100.00 ( 98.82)
Epoch: [5][170/391]	Time  0.149 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.0201e-01 (6.3226e-01)	Acc@1  75.78 ( 77.87)	Acc@5  98.44 ( 98.83)
Epoch: [5][180/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.002)	Loss 4.9632e-01 (6.2897e-01)	Acc@1  83.59 ( 78.00)	Acc@5 100.00 ( 98.85)
Epoch: [5][190/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.002)	Loss 6.6809e-01 (6.2785e-01)	Acc@1  76.56 ( 78.06)	Acc@5  96.88 ( 98.84)
Epoch: [5][200/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.002)	Loss 6.7998e-01 (6.2794e-01)	Acc@1  71.88 ( 78.03)	Acc@5  98.44 ( 98.86)
Epoch: [5][210/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.002)	Loss 5.8270e-01 (6.2872e-01)	Acc@1  79.69 ( 77.96)	Acc@5  99.22 ( 98.84)
Epoch: [5][220/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.002)	Loss 6.3590e-01 (6.2936e-01)	Acc@1  81.25 ( 77.92)	Acc@5  98.44 ( 98.84)
Epoch: [5][230/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.002)	Loss 5.8673e-01 (6.2686e-01)	Acc@1  78.91 ( 78.00)	Acc@5 100.00 ( 98.87)
Epoch: [5][240/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.002)	Loss 8.4174e-01 (6.2510e-01)	Acc@1  71.88 ( 78.06)	Acc@5  97.66 ( 98.86)
Epoch: [5][250/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.002)	Loss 6.8973e-01 (6.2429e-01)	Acc@1  78.12 ( 78.06)	Acc@5  98.44 ( 98.85)
Epoch: [5][260/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.002)	Loss 5.0088e-01 (6.2208e-01)	Acc@1  81.25 ( 78.12)	Acc@5  98.44 ( 98.86)
Epoch: [5][270/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.002)	Loss 6.4395e-01 (6.2123e-01)	Acc@1  76.56 ( 78.21)	Acc@5  98.44 ( 98.86)
Epoch: [5][280/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.002)	Loss 7.3391e-01 (6.2124e-01)	Acc@1  78.91 ( 78.21)	Acc@5  99.22 ( 98.87)
Epoch: [5][290/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.002)	Loss 5.1490e-01 (6.2101e-01)	Acc@1  82.03 ( 78.24)	Acc@5 100.00 ( 98.86)
Epoch: [5][300/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.002)	Loss 5.8364e-01 (6.2012e-01)	Acc@1  80.47 ( 78.29)	Acc@5  97.66 ( 98.87)
Epoch: [5][310/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.002)	Loss 5.8729e-01 (6.1839e-01)	Acc@1  77.34 ( 78.35)	Acc@5  97.66 ( 98.87)
Epoch: [5][320/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.002)	Loss 5.5668e-01 (6.1858e-01)	Acc@1  78.91 ( 78.33)	Acc@5  98.44 ( 98.88)
Epoch: [5][330/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.002)	Loss 6.9543e-01 (6.1832e-01)	Acc@1  71.88 ( 78.32)	Acc@5  99.22 ( 98.88)
Epoch: [5][340/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.002)	Loss 6.0169e-01 (6.1738e-01)	Acc@1  78.12 ( 78.33)	Acc@5  98.44 ( 98.88)
Epoch: [5][350/391]	Time  0.149 ( 0.137)	Data  0.001 ( 0.002)	Loss 5.9373e-01 (6.1723e-01)	Acc@1  78.12 ( 78.33)	Acc@5  99.22 ( 98.87)
Epoch: [5][360/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.002)	Loss 4.7137e-01 (6.1523e-01)	Acc@1  80.47 ( 78.40)	Acc@5  99.22 ( 98.88)
Epoch: [5][370/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.002)	Loss 6.3496e-01 (6.1520e-01)	Acc@1  77.34 ( 78.42)	Acc@5  99.22 ( 98.88)
Epoch: [5][380/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.002)	Loss 4.5171e-01 (6.1292e-01)	Acc@1  85.16 ( 78.49)	Acc@5  99.22 ( 98.89)
Epoch: [5][390/391]	Time  0.109 ( 0.137)	Data  0.001 ( 0.002)	Loss 8.1938e-01 (6.1219e-01)	Acc@1  68.75 ( 78.52)	Acc@5  98.75 ( 98.90)
## e[5] optimizer.zero_grad (sum) time: 0.69565749168396
## e[5]       loss.backward (sum) time: 13.914487838745117
## e[5]      optimizer.step (sum) time: 14.675983428955078
## epoch[5] training(only) time: 53.56303000450134
# Switched to evaluate mode...
Test: [  0/100]	Time  0.191 ( 0.191)	Loss 5.7939e-01 (5.7939e-01)	Acc@1  80.00 ( 80.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.049 ( 0.061)	Loss 5.6735e-01 (6.7165e-01)	Acc@1  80.00 ( 77.91)	Acc@5  97.00 ( 98.27)
Test: [ 20/100]	Time  0.046 ( 0.056)	Loss 7.7859e-01 (6.8704e-01)	Acc@1  74.00 ( 77.29)	Acc@5 100.00 ( 98.19)
Test: [ 30/100]	Time  0.045 ( 0.052)	Loss 7.2998e-01 (7.0207e-01)	Acc@1  81.00 ( 76.97)	Acc@5  98.00 ( 98.00)
Test: [ 40/100]	Time  0.048 ( 0.051)	Loss 6.8526e-01 (6.9163e-01)	Acc@1  81.00 ( 77.29)	Acc@5  97.00 ( 97.90)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 5.8680e-01 (6.9011e-01)	Acc@1  76.00 ( 77.27)	Acc@5  99.00 ( 97.88)
Test: [ 60/100]	Time  0.045 ( 0.050)	Loss 6.2601e-01 (6.8657e-01)	Acc@1  76.00 ( 77.26)	Acc@5 100.00 ( 97.97)
Test: [ 70/100]	Time  0.045 ( 0.049)	Loss 8.8101e-01 (6.9879e-01)	Acc@1  77.00 ( 77.00)	Acc@5  98.00 ( 98.00)
Test: [ 80/100]	Time  0.048 ( 0.049)	Loss 4.7216e-01 (6.9539e-01)	Acc@1  84.00 ( 77.04)	Acc@5 100.00 ( 98.11)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 5.5670e-01 (6.9717e-01)	Acc@1  77.00 ( 77.10)	Acc@5 100.00 ( 98.18)
 * Acc@1 77.120 Acc@5 98.190
### epoch[5] execution time: 58.52035403251648
EPOCH 6
# current learning rate: 0.1
# Switched to train mode...
Epoch: [6][  0/391]	Time  0.306 ( 0.306)	Data  0.153 ( 0.153)	Loss 6.7786e-01 (6.7786e-01)	Acc@1  77.34 ( 77.34)	Acc@5  98.44 ( 98.44)
Epoch: [6][ 10/391]	Time  0.139 ( 0.150)	Data  0.001 ( 0.015)	Loss 5.6013e-01 (5.8022e-01)	Acc@1  80.47 ( 80.97)	Acc@5  98.44 ( 98.79)
Epoch: [6][ 20/391]	Time  0.138 ( 0.144)	Data  0.001 ( 0.008)	Loss 4.9181e-01 (5.5921e-01)	Acc@1  81.25 ( 80.92)	Acc@5  98.44 ( 98.85)
Epoch: [6][ 30/391]	Time  0.146 ( 0.142)	Data  0.001 ( 0.006)	Loss 4.9938e-01 (5.6333e-01)	Acc@1  82.81 ( 80.67)	Acc@5  99.22 ( 98.97)
Epoch: [6][ 40/391]	Time  0.134 ( 0.141)	Data  0.001 ( 0.005)	Loss 7.5208e-01 (5.6618e-01)	Acc@1  71.09 ( 80.16)	Acc@5  99.22 ( 98.95)
Epoch: [6][ 50/391]	Time  0.132 ( 0.140)	Data  0.001 ( 0.004)	Loss 5.5563e-01 (5.5643e-01)	Acc@1  79.69 ( 80.58)	Acc@5  98.44 ( 98.96)
Epoch: [6][ 60/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.004)	Loss 5.1223e-01 (5.5611e-01)	Acc@1  82.81 ( 80.65)	Acc@5  99.22 ( 98.99)
Epoch: [6][ 70/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.003)	Loss 5.3740e-01 (5.5702e-01)	Acc@1  78.91 ( 80.48)	Acc@5 100.00 ( 98.95)
Epoch: [6][ 80/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.003)	Loss 5.3795e-01 (5.5255e-01)	Acc@1  80.47 ( 80.60)	Acc@5  99.22 ( 98.99)
Epoch: [6][ 90/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.003)	Loss 5.3471e-01 (5.4936e-01)	Acc@1  77.34 ( 80.81)	Acc@5 100.00 ( 99.00)
Epoch: [6][100/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.003)	Loss 4.5000e-01 (5.4746e-01)	Acc@1  82.81 ( 80.82)	Acc@5 100.00 ( 99.03)
Epoch: [6][110/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.4005e-01 (5.4667e-01)	Acc@1  82.03 ( 80.86)	Acc@5  98.44 ( 99.04)
Epoch: [6][120/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.5939e-01 (5.5150e-01)	Acc@1  78.91 ( 80.71)	Acc@5  98.44 ( 99.06)
Epoch: [6][130/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.002)	Loss 5.0685e-01 (5.5051e-01)	Acc@1  83.59 ( 80.80)	Acc@5  99.22 ( 99.06)
Epoch: [6][140/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.002)	Loss 6.9816e-01 (5.4900e-01)	Acc@1  72.66 ( 80.82)	Acc@5  99.22 ( 99.10)
Epoch: [6][150/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.002)	Loss 5.7988e-01 (5.4745e-01)	Acc@1  82.03 ( 80.89)	Acc@5  97.66 ( 99.10)
Epoch: [6][160/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.002)	Loss 5.9268e-01 (5.4789e-01)	Acc@1  78.12 ( 80.84)	Acc@5  98.44 ( 99.10)
Epoch: [6][170/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.002)	Loss 8.4010e-01 (5.4904e-01)	Acc@1  71.88 ( 80.76)	Acc@5  97.66 ( 99.10)
Epoch: [6][180/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.002)	Loss 4.4444e-01 (5.5183e-01)	Acc@1  87.50 ( 80.70)	Acc@5  98.44 ( 99.11)
Epoch: [6][190/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.002)	Loss 6.4400e-01 (5.5353e-01)	Acc@1  75.78 ( 80.63)	Acc@5  97.66 ( 99.10)
Epoch: [6][200/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.002)	Loss 5.9828e-01 (5.5339e-01)	Acc@1  78.91 ( 80.61)	Acc@5  98.44 ( 99.09)
Epoch: [6][210/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.002)	Loss 5.5093e-01 (5.5427e-01)	Acc@1  79.69 ( 80.66)	Acc@5  99.22 ( 99.10)
Epoch: [6][220/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.002)	Loss 5.6801e-01 (5.5389e-01)	Acc@1  81.25 ( 80.73)	Acc@5  99.22 ( 99.11)
Epoch: [6][230/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.002)	Loss 4.7962e-01 (5.5232e-01)	Acc@1  83.59 ( 80.75)	Acc@5  98.44 ( 99.12)
Epoch: [6][240/391]	Time  0.145 ( 0.137)	Data  0.001 ( 0.002)	Loss 4.6912e-01 (5.5241e-01)	Acc@1  82.81 ( 80.76)	Acc@5 100.00 ( 99.11)
Epoch: [6][250/391]	Time  0.143 ( 0.137)	Data  0.001 ( 0.002)	Loss 6.6189e-01 (5.5397e-01)	Acc@1  80.47 ( 80.71)	Acc@5  98.44 ( 99.10)
Epoch: [6][260/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.002)	Loss 5.2442e-01 (5.5323e-01)	Acc@1  82.03 ( 80.76)	Acc@5  99.22 ( 99.11)
Epoch: [6][270/391]	Time  0.142 ( 0.137)	Data  0.001 ( 0.002)	Loss 5.1579e-01 (5.5155e-01)	Acc@1  83.59 ( 80.84)	Acc@5  99.22 ( 99.11)
Epoch: [6][280/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.002)	Loss 5.1504e-01 (5.4988e-01)	Acc@1  82.03 ( 80.87)	Acc@5 100.00 ( 99.12)
Epoch: [6][290/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.002)	Loss 5.5131e-01 (5.4678e-01)	Acc@1  79.69 ( 80.97)	Acc@5  99.22 ( 99.14)
Epoch: [6][300/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.002)	Loss 5.4162e-01 (5.4788e-01)	Acc@1  82.81 ( 80.97)	Acc@5 100.00 ( 99.14)
Epoch: [6][310/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.002)	Loss 5.1097e-01 (5.4672e-01)	Acc@1  82.03 ( 81.01)	Acc@5 100.00 ( 99.15)
Epoch: [6][320/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.002)	Loss 4.2267e-01 (5.4666e-01)	Acc@1  83.59 ( 81.01)	Acc@5 100.00 ( 99.15)
Epoch: [6][330/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.002)	Loss 4.2201e-01 (5.4577e-01)	Acc@1  89.06 ( 81.04)	Acc@5  99.22 ( 99.14)
Epoch: [6][340/391]	Time  0.146 ( 0.137)	Data  0.001 ( 0.002)	Loss 5.0830e-01 (5.4493e-01)	Acc@1  78.12 ( 81.06)	Acc@5  99.22 ( 99.15)
Epoch: [6][350/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.002)	Loss 5.7202e-01 (5.4531e-01)	Acc@1  82.03 ( 81.03)	Acc@5  99.22 ( 99.16)
Epoch: [6][360/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.002)	Loss 5.1299e-01 (5.4484e-01)	Acc@1  83.59 ( 81.04)	Acc@5  98.44 ( 99.16)
Epoch: [6][370/391]	Time  0.145 ( 0.137)	Data  0.001 ( 0.002)	Loss 4.5475e-01 (5.4380e-01)	Acc@1  85.16 ( 81.07)	Acc@5  99.22 ( 99.17)
Epoch: [6][380/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.002)	Loss 6.5194e-01 (5.4571e-01)	Acc@1  75.78 ( 81.01)	Acc@5 100.00 ( 99.17)
Epoch: [6][390/391]	Time  0.107 ( 0.137)	Data  0.001 ( 0.002)	Loss 5.1082e-01 (5.4511e-01)	Acc@1  82.50 ( 81.02)	Acc@5  98.75 ( 99.17)
## e[6] optimizer.zero_grad (sum) time: 0.6837520599365234
## e[6]       loss.backward (sum) time: 13.887011289596558
## e[6]      optimizer.step (sum) time: 14.65426254272461
## epoch[6] training(only) time: 53.56272888183594
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 4.4827e-01 (4.4827e-01)	Acc@1  84.00 ( 84.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 4.3106e-01 (5.1374e-01)	Acc@1  86.00 ( 83.09)	Acc@5 100.00 ( 99.09)
Test: [ 20/100]	Time  0.045 ( 0.054)	Loss 6.0933e-01 (5.2946e-01)	Acc@1  77.00 ( 81.86)	Acc@5 100.00 ( 99.05)
Test: [ 30/100]	Time  0.045 ( 0.052)	Loss 6.3687e-01 (5.4065e-01)	Acc@1  77.00 ( 81.97)	Acc@5  98.00 ( 98.87)
Test: [ 40/100]	Time  0.045 ( 0.050)	Loss 4.7144e-01 (5.3397e-01)	Acc@1  86.00 ( 81.83)	Acc@5 100.00 ( 99.02)
Test: [ 50/100]	Time  0.045 ( 0.050)	Loss 4.3564e-01 (5.3060e-01)	Acc@1  86.00 ( 82.16)	Acc@5 100.00 ( 99.06)
Test: [ 60/100]	Time  0.045 ( 0.050)	Loss 4.5347e-01 (5.3015e-01)	Acc@1  86.00 ( 82.26)	Acc@5 100.00 ( 99.13)
Test: [ 70/100]	Time  0.045 ( 0.049)	Loss 5.6666e-01 (5.3443e-01)	Acc@1  84.00 ( 82.24)	Acc@5 100.00 ( 99.20)
Test: [ 80/100]	Time  0.049 ( 0.049)	Loss 5.0135e-01 (5.3456e-01)	Acc@1  83.00 ( 82.25)	Acc@5  99.00 ( 99.22)
Test: [ 90/100]	Time  0.045 ( 0.049)	Loss 3.6846e-01 (5.3241e-01)	Acc@1  86.00 ( 82.20)	Acc@5 100.00 ( 99.25)
 * Acc@1 82.240 Acc@5 99.270
### epoch[6] execution time: 58.49418044090271
EPOCH 7
# current learning rate: 0.1
# Switched to train mode...
Epoch: [7][  0/391]	Time  0.292 ( 0.292)	Data  0.148 ( 0.148)	Loss 7.0710e-01 (7.0710e-01)	Acc@1  79.69 ( 79.69)	Acc@5  98.44 ( 98.44)
Epoch: [7][ 10/391]	Time  0.133 ( 0.150)	Data  0.001 ( 0.014)	Loss 5.1421e-01 (5.2902e-01)	Acc@1  82.81 ( 81.96)	Acc@5 100.00 ( 99.57)
Epoch: [7][ 20/391]	Time  0.133 ( 0.143)	Data  0.001 ( 0.008)	Loss 4.2660e-01 (5.3691e-01)	Acc@1  85.16 ( 81.21)	Acc@5 100.00 ( 99.48)
Epoch: [7][ 30/391]	Time  0.138 ( 0.142)	Data  0.001 ( 0.006)	Loss 4.8494e-01 (5.3386e-01)	Acc@1  82.03 ( 81.35)	Acc@5  99.22 ( 99.42)
Epoch: [7][ 40/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.005)	Loss 4.5761e-01 (5.2312e-01)	Acc@1  85.94 ( 81.76)	Acc@5 100.00 ( 99.43)
Epoch: [7][ 50/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.004)	Loss 4.1724e-01 (5.0429e-01)	Acc@1  85.94 ( 82.37)	Acc@5 100.00 ( 99.48)
Epoch: [7][ 60/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.003)	Loss 6.2336e-01 (5.0134e-01)	Acc@1  79.69 ( 82.62)	Acc@5 100.00 ( 99.46)
Epoch: [7][ 70/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 6.2754e-01 (5.0506e-01)	Acc@1  73.44 ( 82.35)	Acc@5  98.44 ( 99.35)
Epoch: [7][ 80/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.003)	Loss 6.0817e-01 (5.0447e-01)	Acc@1  77.34 ( 82.26)	Acc@5  99.22 ( 99.37)
Epoch: [7][ 90/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.003)	Loss 4.9963e-01 (5.0522e-01)	Acc@1  82.81 ( 82.36)	Acc@5  98.44 ( 99.34)
Epoch: [7][100/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.003)	Loss 6.0774e-01 (5.0972e-01)	Acc@1  78.91 ( 82.26)	Acc@5 100.00 ( 99.30)
Epoch: [7][110/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.2979e-01 (5.0915e-01)	Acc@1  76.56 ( 82.27)	Acc@5 100.00 ( 99.30)
Epoch: [7][120/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.6997e-01 (5.0915e-01)	Acc@1  78.12 ( 82.30)	Acc@5 100.00 ( 99.32)
Epoch: [7][130/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.002)	Loss 5.4187e-01 (5.0693e-01)	Acc@1  76.56 ( 82.35)	Acc@5 100.00 ( 99.32)
Epoch: [7][140/391]	Time  0.144 ( 0.137)	Data  0.001 ( 0.002)	Loss 5.2833e-01 (5.0474e-01)	Acc@1  82.03 ( 82.45)	Acc@5  98.44 ( 99.32)
Epoch: [7][150/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.002)	Loss 6.4226e-01 (5.0548e-01)	Acc@1  80.47 ( 82.45)	Acc@5  99.22 ( 99.31)
Epoch: [7][160/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.002)	Loss 3.7896e-01 (5.0625e-01)	Acc@1  88.28 ( 82.36)	Acc@5 100.00 ( 99.31)
Epoch: [7][170/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.002)	Loss 5.0931e-01 (5.0555e-01)	Acc@1  81.25 ( 82.41)	Acc@5  99.22 ( 99.32)
Epoch: [7][180/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.002)	Loss 4.9038e-01 (5.0227e-01)	Acc@1  82.81 ( 82.56)	Acc@5  99.22 ( 99.32)
Epoch: [7][190/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.002)	Loss 5.0886e-01 (5.0102e-01)	Acc@1  82.81 ( 82.57)	Acc@5  99.22 ( 99.32)
Epoch: [7][200/391]	Time  0.144 ( 0.137)	Data  0.001 ( 0.002)	Loss 3.6920e-01 (5.0004e-01)	Acc@1  87.50 ( 82.62)	Acc@5 100.00 ( 99.33)
Epoch: [7][210/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.002)	Loss 4.4345e-01 (4.9908e-01)	Acc@1  82.03 ( 82.65)	Acc@5  99.22 ( 99.32)
Epoch: [7][220/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.002)	Loss 4.1962e-01 (4.9954e-01)	Acc@1  85.94 ( 82.63)	Acc@5  99.22 ( 99.31)
Epoch: [7][230/391]	Time  0.149 ( 0.137)	Data  0.001 ( 0.002)	Loss 5.3180e-01 (4.9887e-01)	Acc@1  78.12 ( 82.61)	Acc@5 100.00 ( 99.32)
Epoch: [7][240/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.002)	Loss 5.7911e-01 (4.9789e-01)	Acc@1  79.69 ( 82.63)	Acc@5  99.22 ( 99.33)
Epoch: [7][250/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.002)	Loss 7.2944e-01 (4.9684e-01)	Acc@1  71.88 ( 82.66)	Acc@5  99.22 ( 99.34)
Epoch: [7][260/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.002)	Loss 5.6965e-01 (4.9779e-01)	Acc@1  78.12 ( 82.66)	Acc@5  99.22 ( 99.32)
Epoch: [7][270/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.002)	Loss 4.8010e-01 (4.9635e-01)	Acc@1  82.03 ( 82.71)	Acc@5  99.22 ( 99.33)
Epoch: [7][280/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.002)	Loss 5.8031e-01 (4.9823e-01)	Acc@1  79.69 ( 82.63)	Acc@5  99.22 ( 99.32)
Epoch: [7][290/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.002)	Loss 4.1769e-01 (4.9658e-01)	Acc@1  88.28 ( 82.70)	Acc@5  98.44 ( 99.32)
Epoch: [7][300/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.002)	Loss 5.1462e-01 (4.9600e-01)	Acc@1  83.59 ( 82.74)	Acc@5 100.00 ( 99.33)
Epoch: [7][310/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.002)	Loss 5.2511e-01 (4.9651e-01)	Acc@1  79.69 ( 82.74)	Acc@5  97.66 ( 99.31)
Epoch: [7][320/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.002)	Loss 4.0871e-01 (4.9618e-01)	Acc@1  84.38 ( 82.75)	Acc@5  99.22 ( 99.31)
Epoch: [7][330/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.002)	Loss 5.9705e-01 (4.9608e-01)	Acc@1  76.56 ( 82.75)	Acc@5  99.22 ( 99.30)
Epoch: [7][340/391]	Time  0.150 ( 0.137)	Data  0.001 ( 0.002)	Loss 6.3159e-01 (4.9538e-01)	Acc@1  77.34 ( 82.75)	Acc@5  98.44 ( 99.30)
Epoch: [7][350/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.002)	Loss 5.7541e-01 (4.9577e-01)	Acc@1  80.47 ( 82.75)	Acc@5  99.22 ( 99.31)
Epoch: [7][360/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.002)	Loss 3.1727e-01 (4.9577e-01)	Acc@1  89.84 ( 82.78)	Acc@5 100.00 ( 99.30)
Epoch: [7][370/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.002)	Loss 5.2494e-01 (4.9537e-01)	Acc@1  83.59 ( 82.82)	Acc@5 100.00 ( 99.30)
Epoch: [7][380/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.002)	Loss 4.4117e-01 (4.9419e-01)	Acc@1  82.03 ( 82.86)	Acc@5 100.00 ( 99.31)
Epoch: [7][390/391]	Time  0.108 ( 0.137)	Data  0.001 ( 0.002)	Loss 3.8625e-01 (4.9333e-01)	Acc@1  90.00 ( 82.88)	Acc@5 100.00 ( 99.32)
## e[7] optimizer.zero_grad (sum) time: 0.6920545101165771
## e[7]       loss.backward (sum) time: 13.903598308563232
## e[7]      optimizer.step (sum) time: 14.648123264312744
## epoch[7] training(only) time: 53.54068970680237
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 4.8616e-01 (4.8616e-01)	Acc@1  80.00 ( 80.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.049 ( 0.062)	Loss 5.9091e-01 (5.7565e-01)	Acc@1  81.00 ( 80.45)	Acc@5  98.00 ( 98.91)
Test: [ 20/100]	Time  0.048 ( 0.056)	Loss 5.6437e-01 (5.7554e-01)	Acc@1  82.00 ( 80.29)	Acc@5 100.00 ( 98.95)
Test: [ 30/100]	Time  0.048 ( 0.053)	Loss 6.5015e-01 (5.9599e-01)	Acc@1  80.00 ( 80.42)	Acc@5  99.00 ( 98.90)
Test: [ 40/100]	Time  0.045 ( 0.052)	Loss 5.1341e-01 (5.9311e-01)	Acc@1  81.00 ( 80.22)	Acc@5  99.00 ( 98.83)
Test: [ 50/100]	Time  0.062 ( 0.051)	Loss 3.4626e-01 (5.8595e-01)	Acc@1  86.00 ( 80.51)	Acc@5 100.00 ( 98.88)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 5.8509e-01 (5.7589e-01)	Acc@1  84.00 ( 81.00)	Acc@5  99.00 ( 98.95)
Test: [ 70/100]	Time  0.045 ( 0.050)	Loss 5.4743e-01 (5.7738e-01)	Acc@1  82.00 ( 81.07)	Acc@5  99.00 ( 98.97)
Test: [ 80/100]	Time  0.051 ( 0.050)	Loss 5.2175e-01 (5.7475e-01)	Acc@1  86.00 ( 81.19)	Acc@5  99.00 ( 99.06)
Test: [ 90/100]	Time  0.046 ( 0.050)	Loss 5.7915e-01 (5.7414e-01)	Acc@1  80.00 ( 81.10)	Acc@5 100.00 ( 99.09)
 * Acc@1 81.060 Acc@5 99.100
### epoch[7] execution time: 58.56859564781189
EPOCH 8
# current learning rate: 0.1
# Switched to train mode...
Epoch: [8][  0/391]	Time  0.298 ( 0.298)	Data  0.158 ( 0.158)	Loss 4.9564e-01 (4.9564e-01)	Acc@1  81.25 ( 81.25)	Acc@5 100.00 (100.00)
Epoch: [8][ 10/391]	Time  0.140 ( 0.152)	Data  0.001 ( 0.015)	Loss 5.0083e-01 (4.4191e-01)	Acc@1  82.81 ( 84.23)	Acc@5  99.22 ( 99.50)
Epoch: [8][ 20/391]	Time  0.134 ( 0.145)	Data  0.001 ( 0.009)	Loss 5.5669e-01 (4.4579e-01)	Acc@1  77.34 ( 83.78)	Acc@5  99.22 ( 99.55)
Epoch: [8][ 30/391]	Time  0.143 ( 0.142)	Data  0.001 ( 0.006)	Loss 3.5293e-01 (4.4006e-01)	Acc@1  88.28 ( 83.87)	Acc@5  99.22 ( 99.50)
Epoch: [8][ 40/391]	Time  0.135 ( 0.141)	Data  0.001 ( 0.005)	Loss 4.1565e-01 (4.3899e-01)	Acc@1  83.59 ( 84.17)	Acc@5 100.00 ( 99.50)
Epoch: [8][ 50/391]	Time  0.132 ( 0.140)	Data  0.001 ( 0.004)	Loss 5.1446e-01 (4.4135e-01)	Acc@1  83.59 ( 84.30)	Acc@5 100.00 ( 99.53)
Epoch: [8][ 60/391]	Time  0.146 ( 0.139)	Data  0.001 ( 0.004)	Loss 4.8130e-01 (4.4482e-01)	Acc@1  83.59 ( 84.00)	Acc@5 100.00 ( 99.59)
Epoch: [8][ 70/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.003)	Loss 3.8774e-01 (4.4882e-01)	Acc@1  86.72 ( 83.92)	Acc@5 100.00 ( 99.56)
Epoch: [8][ 80/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.003)	Loss 3.6246e-01 (4.5092e-01)	Acc@1  87.50 ( 83.80)	Acc@5 100.00 ( 99.55)
Epoch: [8][ 90/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.003)	Loss 3.4398e-01 (4.4969e-01)	Acc@1  87.50 ( 84.00)	Acc@5 100.00 ( 99.50)
Epoch: [8][100/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.003)	Loss 5.5382e-01 (4.4770e-01)	Acc@1  81.25 ( 83.99)	Acc@5  99.22 ( 99.50)
Epoch: [8][110/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.003)	Loss 5.0724e-01 (4.4882e-01)	Acc@1  80.47 ( 84.01)	Acc@5 100.00 ( 99.47)
Epoch: [8][120/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.6218e-01 (4.4494e-01)	Acc@1  86.72 ( 84.16)	Acc@5  99.22 ( 99.46)
Epoch: [8][130/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.8251e-01 (4.4604e-01)	Acc@1  84.38 ( 84.15)	Acc@5  99.22 ( 99.45)
Epoch: [8][140/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.4520e-01 (4.5004e-01)	Acc@1  83.59 ( 84.05)	Acc@5 100.00 ( 99.46)
Epoch: [8][150/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.4317e-01 (4.5052e-01)	Acc@1  87.50 ( 84.14)	Acc@5  99.22 ( 99.45)
Epoch: [8][160/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.2591e-01 (4.5262e-01)	Acc@1  86.72 ( 84.06)	Acc@5  98.44 ( 99.44)
Epoch: [8][170/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.1068e-01 (4.5392e-01)	Acc@1  85.94 ( 84.01)	Acc@5 100.00 ( 99.45)
Epoch: [8][180/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.9828e-01 (4.5128e-01)	Acc@1  87.50 ( 84.11)	Acc@5 100.00 ( 99.46)
Epoch: [8][190/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.002)	Loss 3.7240e-01 (4.4933e-01)	Acc@1  89.84 ( 84.20)	Acc@5  99.22 ( 99.47)
Epoch: [8][200/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.002)	Loss 4.2511e-01 (4.4838e-01)	Acc@1  85.94 ( 84.22)	Acc@5 100.00 ( 99.47)
Epoch: [8][210/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.002)	Loss 5.1035e-01 (4.4774e-01)	Acc@1  82.81 ( 84.24)	Acc@5 100.00 ( 99.48)
Epoch: [8][220/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.002)	Loss 5.4095e-01 (4.4780e-01)	Acc@1  83.59 ( 84.31)	Acc@5 100.00 ( 99.47)
Epoch: [8][230/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.002)	Loss 5.1481e-01 (4.4870e-01)	Acc@1  81.25 ( 84.32)	Acc@5 100.00 ( 99.46)
Epoch: [8][240/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.002)	Loss 4.5800e-01 (4.4873e-01)	Acc@1  81.25 ( 84.28)	Acc@5 100.00 ( 99.46)
Epoch: [8][250/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.002)	Loss 4.4964e-01 (4.4902e-01)	Acc@1  85.94 ( 84.24)	Acc@5  99.22 ( 99.46)
Epoch: [8][260/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.002)	Loss 3.3875e-01 (4.4978e-01)	Acc@1  86.72 ( 84.23)	Acc@5 100.00 ( 99.44)
Epoch: [8][270/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.002)	Loss 3.7640e-01 (4.4861e-01)	Acc@1  85.16 ( 84.27)	Acc@5 100.00 ( 99.45)
Epoch: [8][280/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.002)	Loss 4.4282e-01 (4.4777e-01)	Acc@1  83.59 ( 84.33)	Acc@5 100.00 ( 99.45)
Epoch: [8][290/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.002)	Loss 4.4899e-01 (4.4764e-01)	Acc@1  82.03 ( 84.32)	Acc@5 100.00 ( 99.44)
Epoch: [8][300/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.002)	Loss 4.6600e-01 (4.4635e-01)	Acc@1  85.16 ( 84.38)	Acc@5 100.00 ( 99.45)
Epoch: [8][310/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.002)	Loss 3.9663e-01 (4.4785e-01)	Acc@1  89.06 ( 84.34)	Acc@5  99.22 ( 99.45)
Epoch: [8][320/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.002)	Loss 4.1665e-01 (4.4805e-01)	Acc@1  85.94 ( 84.32)	Acc@5  99.22 ( 99.46)
Epoch: [8][330/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.002)	Loss 4.5212e-01 (4.4799e-01)	Acc@1  85.94 ( 84.34)	Acc@5 100.00 ( 99.46)
Epoch: [8][340/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.002)	Loss 4.2968e-01 (4.4754e-01)	Acc@1  84.38 ( 84.38)	Acc@5  99.22 ( 99.45)
Epoch: [8][350/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.002)	Loss 4.8221e-01 (4.4742e-01)	Acc@1  82.81 ( 84.39)	Acc@5 100.00 ( 99.44)
Epoch: [8][360/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.002)	Loss 5.7452e-01 (4.4741e-01)	Acc@1  82.81 ( 84.37)	Acc@5  99.22 ( 99.45)
Epoch: [8][370/391]	Time  0.145 ( 0.137)	Data  0.001 ( 0.002)	Loss 3.1634e-01 (4.4769e-01)	Acc@1  87.50 ( 84.33)	Acc@5 100.00 ( 99.45)
Epoch: [8][380/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.002)	Loss 4.1788e-01 (4.4786e-01)	Acc@1  85.16 ( 84.35)	Acc@5 100.00 ( 99.44)
Epoch: [8][390/391]	Time  0.110 ( 0.137)	Data  0.001 ( 0.002)	Loss 5.8245e-01 (4.4748e-01)	Acc@1  81.25 ( 84.38)	Acc@5 100.00 ( 99.45)
## e[8] optimizer.zero_grad (sum) time: 0.6877214908599854
## e[8]       loss.backward (sum) time: 13.934566259384155
## e[8]      optimizer.step (sum) time: 14.644434213638306
## epoch[8] training(only) time: 53.64548087120056
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 5.5713e-01 (5.5713e-01)	Acc@1  78.00 ( 78.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.059 ( 0.060)	Loss 4.7370e-01 (5.1332e-01)	Acc@1  88.00 ( 83.64)	Acc@5  99.00 ( 99.18)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 4.6417e-01 (5.2410e-01)	Acc@1  82.00 ( 82.29)	Acc@5 100.00 ( 99.33)
Test: [ 30/100]	Time  0.045 ( 0.052)	Loss 7.0019e-01 (5.3520e-01)	Acc@1  79.00 ( 82.03)	Acc@5  99.00 ( 99.19)
Test: [ 40/100]	Time  0.045 ( 0.051)	Loss 5.2960e-01 (5.3162e-01)	Acc@1  83.00 ( 81.90)	Acc@5 100.00 ( 99.12)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 3.9864e-01 (5.2724e-01)	Acc@1  86.00 ( 82.14)	Acc@5  99.00 ( 99.16)
Test: [ 60/100]	Time  0.057 ( 0.050)	Loss 4.4355e-01 (5.3077e-01)	Acc@1  87.00 ( 82.07)	Acc@5 100.00 ( 99.23)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 4.9001e-01 (5.2729e-01)	Acc@1  79.00 ( 82.25)	Acc@5 100.00 ( 99.28)
Test: [ 80/100]	Time  0.045 ( 0.049)	Loss 4.8482e-01 (5.2936e-01)	Acc@1  86.00 ( 82.15)	Acc@5  99.00 ( 99.26)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 4.9946e-01 (5.2938e-01)	Acc@1  84.00 ( 82.20)	Acc@5 100.00 ( 99.26)
 * Acc@1 82.240 Acc@5 99.280
### epoch[8] execution time: 58.59569549560547
EPOCH 9
# current learning rate: 0.1
# Switched to train mode...
Epoch: [9][  0/391]	Time  0.303 ( 0.303)	Data  0.156 ( 0.156)	Loss 5.5920e-01 (5.5920e-01)	Acc@1  80.47 ( 80.47)	Acc@5  95.31 ( 95.31)
Epoch: [9][ 10/391]	Time  0.138 ( 0.151)	Data  0.001 ( 0.015)	Loss 5.4723e-01 (4.4548e-01)	Acc@1  82.03 ( 84.80)	Acc@5 100.00 ( 99.01)
Epoch: [9][ 20/391]	Time  0.137 ( 0.145)	Data  0.001 ( 0.009)	Loss 3.4895e-01 (4.3173e-01)	Acc@1  89.06 ( 85.53)	Acc@5 100.00 ( 99.18)
Epoch: [9][ 30/391]	Time  0.139 ( 0.142)	Data  0.001 ( 0.006)	Loss 3.6616e-01 (4.2073e-01)	Acc@1  86.72 ( 85.51)	Acc@5  98.44 ( 99.29)
Epoch: [9][ 40/391]	Time  0.135 ( 0.141)	Data  0.001 ( 0.005)	Loss 3.6114e-01 (4.1160e-01)	Acc@1  88.28 ( 85.88)	Acc@5  99.22 ( 99.28)
Epoch: [9][ 50/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.004)	Loss 3.4292e-01 (4.1337e-01)	Acc@1  88.28 ( 85.92)	Acc@5 100.00 ( 99.25)
Epoch: [9][ 60/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.004)	Loss 3.1768e-01 (4.0887e-01)	Acc@1  89.06 ( 86.10)	Acc@5 100.00 ( 99.33)
Epoch: [9][ 70/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 4.6437e-01 (4.1041e-01)	Acc@1  89.06 ( 86.08)	Acc@5  99.22 ( 99.35)
Epoch: [9][ 80/391]	Time  0.143 ( 0.139)	Data  0.001 ( 0.003)	Loss 3.1214e-01 (4.0857e-01)	Acc@1  90.62 ( 86.19)	Acc@5 100.00 ( 99.36)
Epoch: [9][ 90/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.003)	Loss 4.4109e-01 (4.1430e-01)	Acc@1  83.59 ( 85.90)	Acc@5  99.22 ( 99.36)
Epoch: [9][100/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.003)	Loss 4.7423e-01 (4.1396e-01)	Acc@1  80.47 ( 85.84)	Acc@5 100.00 ( 99.39)
Epoch: [9][110/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.003)	Loss 4.4826e-01 (4.1747e-01)	Acc@1  82.81 ( 85.68)	Acc@5 100.00 ( 99.39)
Epoch: [9][120/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.1228e-01 (4.1624e-01)	Acc@1  88.28 ( 85.67)	Acc@5 100.00 ( 99.41)
Epoch: [9][130/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.3795e-01 (4.1714e-01)	Acc@1  85.16 ( 85.60)	Acc@5  98.44 ( 99.39)
Epoch: [9][140/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.3516e-01 (4.1749e-01)	Acc@1  80.47 ( 85.57)	Acc@5 100.00 ( 99.41)
Epoch: [9][150/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.7580e-01 (4.1707e-01)	Acc@1  85.94 ( 85.56)	Acc@5 100.00 ( 99.41)
Epoch: [9][160/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.2210e-01 (4.1569e-01)	Acc@1  82.03 ( 85.58)	Acc@5  97.66 ( 99.41)
Epoch: [9][170/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.2821e-01 (4.1446e-01)	Acc@1  86.72 ( 85.69)	Acc@5  99.22 ( 99.41)
Epoch: [9][180/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.002)	Loss 5.3199e-01 (4.1411e-01)	Acc@1  82.81 ( 85.70)	Acc@5  99.22 ( 99.39)
Epoch: [9][190/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.002)	Loss 3.0818e-01 (4.1152e-01)	Acc@1  92.19 ( 85.84)	Acc@5 100.00 ( 99.39)
Epoch: [9][200/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.002)	Loss 5.2299e-01 (4.1141e-01)	Acc@1  81.25 ( 85.84)	Acc@5  99.22 ( 99.40)
Epoch: [9][210/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.002)	Loss 3.0301e-01 (4.1146e-01)	Acc@1  92.97 ( 85.83)	Acc@5  99.22 ( 99.40)
Epoch: [9][220/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.002)	Loss 3.6294e-01 (4.1241e-01)	Acc@1  89.06 ( 85.79)	Acc@5  99.22 ( 99.39)
Epoch: [9][230/391]	Time  0.144 ( 0.137)	Data  0.001 ( 0.002)	Loss 3.3797e-01 (4.1312e-01)	Acc@1  89.06 ( 85.74)	Acc@5  99.22 ( 99.39)
Epoch: [9][240/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.002)	Loss 4.5261e-01 (4.1135e-01)	Acc@1  85.16 ( 85.80)	Acc@5  98.44 ( 99.40)
Epoch: [9][250/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.002)	Loss 4.0002e-01 (4.1071e-01)	Acc@1  86.72 ( 85.90)	Acc@5  99.22 ( 99.39)
Epoch: [9][260/391]	Time  0.144 ( 0.137)	Data  0.001 ( 0.002)	Loss 4.3417e-01 (4.0970e-01)	Acc@1  87.50 ( 85.97)	Acc@5 100.00 ( 99.40)
Epoch: [9][270/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.002)	Loss 6.4143e-01 (4.1078e-01)	Acc@1  74.22 ( 85.92)	Acc@5 100.00 ( 99.39)
Epoch: [9][280/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.002)	Loss 4.1052e-01 (4.1123e-01)	Acc@1  86.72 ( 85.89)	Acc@5  99.22 ( 99.39)
Epoch: [9][290/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.002)	Loss 3.3176e-01 (4.1193e-01)	Acc@1  89.84 ( 85.85)	Acc@5 100.00 ( 99.40)
Epoch: [9][300/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.002)	Loss 5.4774e-01 (4.1233e-01)	Acc@1  82.81 ( 85.84)	Acc@5  99.22 ( 99.40)
Epoch: [9][310/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.002)	Loss 3.4710e-01 (4.1223e-01)	Acc@1  86.72 ( 85.84)	Acc@5 100.00 ( 99.40)
Epoch: [9][320/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.002)	Loss 3.3304e-01 (4.1163e-01)	Acc@1  85.94 ( 85.86)	Acc@5 100.00 ( 99.41)
Epoch: [9][330/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.002)	Loss 4.4096e-01 (4.1218e-01)	Acc@1  84.38 ( 85.85)	Acc@5 100.00 ( 99.41)
Epoch: [9][340/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.002)	Loss 3.7230e-01 (4.1122e-01)	Acc@1  88.28 ( 85.89)	Acc@5 100.00 ( 99.42)
Epoch: [9][350/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.002)	Loss 3.7190e-01 (4.1062e-01)	Acc@1  91.41 ( 85.91)	Acc@5  99.22 ( 99.43)
Epoch: [9][360/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.002)	Loss 3.9785e-01 (4.0949e-01)	Acc@1  87.50 ( 85.95)	Acc@5  98.44 ( 99.43)
Epoch: [9][370/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.002)	Loss 3.6609e-01 (4.0885e-01)	Acc@1  85.94 ( 85.96)	Acc@5 100.00 ( 99.44)
Epoch: [9][380/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.002)	Loss 3.8823e-01 (4.0930e-01)	Acc@1  86.72 ( 85.94)	Acc@5 100.00 ( 99.43)
Epoch: [9][390/391]	Time  0.110 ( 0.137)	Data  0.001 ( 0.002)	Loss 3.3129e-01 (4.0901e-01)	Acc@1  91.25 ( 85.97)	Acc@5 100.00 ( 99.44)
## e[9] optimizer.zero_grad (sum) time: 0.6836528778076172
## e[9]       loss.backward (sum) time: 13.882076501846313
## e[9]      optimizer.step (sum) time: 14.656877279281616
## epoch[9] training(only) time: 53.73749017715454
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 4.3666e-01 (4.3666e-01)	Acc@1  84.00 ( 84.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 4.2685e-01 (4.4759e-01)	Acc@1  89.00 ( 84.82)	Acc@5  99.00 ( 99.36)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 4.8263e-01 (4.8637e-01)	Acc@1  80.00 ( 83.48)	Acc@5 100.00 ( 99.19)
Test: [ 30/100]	Time  0.048 ( 0.052)	Loss 5.5726e-01 (5.0004e-01)	Acc@1  83.00 ( 83.35)	Acc@5 100.00 ( 99.23)
Test: [ 40/100]	Time  0.049 ( 0.051)	Loss 5.2523e-01 (4.9835e-01)	Acc@1  84.00 ( 83.59)	Acc@5  99.00 ( 99.17)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 3.7768e-01 (4.9542e-01)	Acc@1  88.00 ( 83.73)	Acc@5 100.00 ( 99.16)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 3.3682e-01 (4.9512e-01)	Acc@1  88.00 ( 83.66)	Acc@5 100.00 ( 99.23)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 6.0315e-01 (4.9424e-01)	Acc@1  81.00 ( 83.68)	Acc@5 100.00 ( 99.28)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 4.1415e-01 (4.9813e-01)	Acc@1  86.00 ( 83.63)	Acc@5 100.00 ( 99.33)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 3.6000e-01 (4.9791e-01)	Acc@1  88.00 ( 83.64)	Acc@5 100.00 ( 99.33)
 * Acc@1 83.580 Acc@5 99.370
### epoch[9] execution time: 58.66934514045715
EPOCH 10
# current learning rate: 0.1
# Switched to train mode...
Epoch: [10][  0/391]	Time  0.289 ( 0.289)	Data  0.141 ( 0.141)	Loss 6.3612e-01 (6.3612e-01)	Acc@1  77.34 ( 77.34)	Acc@5  99.22 ( 99.22)
Epoch: [10][ 10/391]	Time  0.135 ( 0.151)	Data  0.001 ( 0.014)	Loss 3.3259e-01 (3.9128e-01)	Acc@1  86.72 ( 85.65)	Acc@5 100.00 ( 99.64)
Epoch: [10][ 20/391]	Time  0.135 ( 0.145)	Data  0.001 ( 0.008)	Loss 2.2735e-01 (3.6847e-01)	Acc@1  92.19 ( 86.83)	Acc@5 100.00 ( 99.63)
Epoch: [10][ 30/391]	Time  0.135 ( 0.142)	Data  0.001 ( 0.006)	Loss 2.9047e-01 (3.6280e-01)	Acc@1  88.28 ( 86.97)	Acc@5 100.00 ( 99.67)
Epoch: [10][ 40/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.004)	Loss 3.6370e-01 (3.6773e-01)	Acc@1  84.38 ( 86.78)	Acc@5 100.00 ( 99.68)
Epoch: [10][ 50/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.004)	Loss 3.7569e-01 (3.7556e-01)	Acc@1  85.94 ( 86.66)	Acc@5  99.22 ( 99.71)
Epoch: [10][ 60/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.003)	Loss 4.4179e-01 (3.8227e-01)	Acc@1  83.59 ( 86.37)	Acc@5  99.22 ( 99.67)
Epoch: [10][ 70/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 4.7362e-01 (3.8694e-01)	Acc@1  87.50 ( 86.29)	Acc@5  98.44 ( 99.60)
Epoch: [10][ 80/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.5706e-01 (3.8635e-01)	Acc@1  90.62 ( 86.48)	Acc@5 100.00 ( 99.62)
Epoch: [10][ 90/391]	Time  0.142 ( 0.139)	Data  0.001 ( 0.003)	Loss 3.2677e-01 (3.8557e-01)	Acc@1  85.94 ( 86.38)	Acc@5 100.00 ( 99.64)
Epoch: [10][100/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.3684e-01 (3.8105e-01)	Acc@1  89.84 ( 86.56)	Acc@5  99.22 ( 99.64)
Epoch: [10][110/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.4259e-01 (3.7844e-01)	Acc@1  88.28 ( 86.68)	Acc@5 100.00 ( 99.63)
Epoch: [10][120/391]	Time  0.144 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.9780e-01 (3.7883e-01)	Acc@1  89.84 ( 86.68)	Acc@5  98.44 ( 99.62)
Epoch: [10][130/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.8191e-01 (3.8108e-01)	Acc@1  88.28 ( 86.65)	Acc@5  99.22 ( 99.62)
Epoch: [10][140/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.7298e-01 (3.8326e-01)	Acc@1  85.94 ( 86.62)	Acc@5  98.44 ( 99.60)
Epoch: [10][150/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.3889e-01 (3.8220e-01)	Acc@1  82.03 ( 86.71)	Acc@5  99.22 ( 99.59)
Epoch: [10][160/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.1294e-01 (3.8566e-01)	Acc@1  85.94 ( 86.66)	Acc@5 100.00 ( 99.58)
Epoch: [10][170/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.2257e-01 (3.8760e-01)	Acc@1  83.59 ( 86.57)	Acc@5  98.44 ( 99.57)
Epoch: [10][180/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.0565e-01 (3.8973e-01)	Acc@1  84.38 ( 86.50)	Acc@5 100.00 ( 99.57)
Epoch: [10][190/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.8227e-01 (3.8854e-01)	Acc@1  85.16 ( 86.51)	Acc@5 100.00 ( 99.58)
Epoch: [10][200/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.2028e-01 (3.8696e-01)	Acc@1  83.59 ( 86.57)	Acc@5 100.00 ( 99.59)
Epoch: [10][210/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.9809e-01 (3.8683e-01)	Acc@1  84.38 ( 86.60)	Acc@5 100.00 ( 99.58)
Epoch: [10][220/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.9079e-01 (3.8512e-01)	Acc@1  92.19 ( 86.68)	Acc@5  99.22 ( 99.58)
Epoch: [10][230/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.6851e-01 (3.8424e-01)	Acc@1  86.72 ( 86.72)	Acc@5 100.00 ( 99.58)
Epoch: [10][240/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.4636e-01 (3.8340e-01)	Acc@1  87.50 ( 86.74)	Acc@5 100.00 ( 99.59)
Epoch: [10][250/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.6594e-01 (3.8351e-01)	Acc@1  89.06 ( 86.76)	Acc@5  98.44 ( 99.59)
Epoch: [10][260/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.8264e-01 (3.8217e-01)	Acc@1  89.84 ( 86.80)	Acc@5  99.22 ( 99.60)
Epoch: [10][270/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0884e-01 (3.8260e-01)	Acc@1  93.75 ( 86.79)	Acc@5 100.00 ( 99.59)
Epoch: [10][280/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.0980e-01 (3.8270e-01)	Acc@1  82.81 ( 86.77)	Acc@5  99.22 ( 99.60)
Epoch: [10][290/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.8320e-01 (3.8425e-01)	Acc@1  83.59 ( 86.70)	Acc@5  98.44 ( 99.59)
Epoch: [10][300/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.3637e-01 (3.8466e-01)	Acc@1  90.62 ( 86.70)	Acc@5  99.22 ( 99.60)
Epoch: [10][310/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.6396e-01 (3.8470e-01)	Acc@1  83.59 ( 86.67)	Acc@5 100.00 ( 99.59)
Epoch: [10][320/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.0318e-01 (3.8499e-01)	Acc@1  82.03 ( 86.65)	Acc@5 100.00 ( 99.58)
Epoch: [10][330/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.9674e-01 (3.8475e-01)	Acc@1  88.28 ( 86.67)	Acc@5  97.66 ( 99.58)
Epoch: [10][340/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.4443e-01 (3.8474e-01)	Acc@1  82.03 ( 86.66)	Acc@5  98.44 ( 99.58)
Epoch: [10][350/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.001)	Loss 4.0297e-01 (3.8436e-01)	Acc@1  84.38 ( 86.69)	Acc@5 100.00 ( 99.58)
Epoch: [10][360/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.001)	Loss 4.5687e-01 (3.8451e-01)	Acc@1  87.50 ( 86.68)	Acc@5  97.66 ( 99.58)
Epoch: [10][370/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.001)	Loss 3.8953e-01 (3.8390e-01)	Acc@1  85.16 ( 86.70)	Acc@5  99.22 ( 99.59)
Epoch: [10][380/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.001)	Loss 3.6229e-01 (3.8396e-01)	Acc@1  83.59 ( 86.65)	Acc@5 100.00 ( 99.59)
Epoch: [10][390/391]	Time  0.106 ( 0.137)	Data  0.001 ( 0.001)	Loss 2.9482e-01 (3.8377e-01)	Acc@1  90.00 ( 86.65)	Acc@5 100.00 ( 99.59)
## e[10] optimizer.zero_grad (sum) time: 0.6840729713439941
## e[10]       loss.backward (sum) time: 13.911331415176392
## e[10]      optimizer.step (sum) time: 14.647147417068481
## epoch[10] training(only) time: 53.79135227203369
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 3.4373e-01 (3.4373e-01)	Acc@1  89.00 ( 89.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.047 ( 0.060)	Loss 4.3307e-01 (4.4167e-01)	Acc@1  91.00 ( 85.82)	Acc@5  99.00 ( 99.09)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 3.7017e-01 (4.5129e-01)	Acc@1  85.00 ( 84.57)	Acc@5  99.00 ( 99.24)
Test: [ 30/100]	Time  0.050 ( 0.052)	Loss 5.3864e-01 (4.7472e-01)	Acc@1  84.00 ( 84.42)	Acc@5  99.00 ( 99.06)
Test: [ 40/100]	Time  0.050 ( 0.051)	Loss 4.3389e-01 (4.6700e-01)	Acc@1  83.00 ( 84.54)	Acc@5  99.00 ( 99.07)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 3.4123e-01 (4.6055e-01)	Acc@1  88.00 ( 84.80)	Acc@5 100.00 ( 99.14)
Test: [ 60/100]	Time  0.056 ( 0.050)	Loss 3.5335e-01 (4.6230e-01)	Acc@1  84.00 ( 84.61)	Acc@5 100.00 ( 99.25)
Test: [ 70/100]	Time  0.051 ( 0.049)	Loss 5.6477e-01 (4.6312e-01)	Acc@1  83.00 ( 84.62)	Acc@5  98.00 ( 99.23)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 4.6576e-01 (4.6391e-01)	Acc@1  85.00 ( 84.54)	Acc@5  99.00 ( 99.20)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 3.0528e-01 (4.6434e-01)	Acc@1  86.00 ( 84.49)	Acc@5 100.00 ( 99.20)
 * Acc@1 84.750 Acc@5 99.250
### epoch[10] execution time: 58.762768268585205
EPOCH 11
# current learning rate: 0.1
# Switched to train mode...
Epoch: [11][  0/391]	Time  0.290 ( 0.290)	Data  0.149 ( 0.149)	Loss 4.2931e-01 (4.2931e-01)	Acc@1  85.16 ( 85.16)	Acc@5  99.22 ( 99.22)
Epoch: [11][ 10/391]	Time  0.138 ( 0.151)	Data  0.001 ( 0.014)	Loss 3.0621e-01 (3.6901e-01)	Acc@1  90.62 ( 86.22)	Acc@5 100.00 ( 99.72)
Epoch: [11][ 20/391]	Time  0.140 ( 0.144)	Data  0.001 ( 0.008)	Loss 2.3605e-01 (3.4459e-01)	Acc@1  89.06 ( 87.39)	Acc@5 100.00 ( 99.67)
Epoch: [11][ 30/391]	Time  0.133 ( 0.142)	Data  0.001 ( 0.006)	Loss 3.7387e-01 (3.3577e-01)	Acc@1  84.38 ( 87.85)	Acc@5 100.00 ( 99.67)
Epoch: [11][ 40/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.005)	Loss 4.1931e-01 (3.4093e-01)	Acc@1  85.16 ( 87.75)	Acc@5 100.00 ( 99.71)
Epoch: [11][ 50/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.004)	Loss 3.3659e-01 (3.3925e-01)	Acc@1  90.62 ( 87.94)	Acc@5  99.22 ( 99.66)
Epoch: [11][ 60/391]	Time  0.133 ( 0.140)	Data  0.001 ( 0.004)	Loss 3.5613e-01 (3.3616e-01)	Acc@1  85.94 ( 87.97)	Acc@5 100.00 ( 99.69)
Epoch: [11][ 70/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.003)	Loss 3.2868e-01 (3.3383e-01)	Acc@1  89.06 ( 88.05)	Acc@5 100.00 ( 99.71)
Epoch: [11][ 80/391]	Time  0.142 ( 0.139)	Data  0.001 ( 0.003)	Loss 3.2612e-01 (3.3036e-01)	Acc@1  88.28 ( 88.17)	Acc@5 100.00 ( 99.72)
Epoch: [11][ 90/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.003)	Loss 3.8496e-01 (3.3162e-01)	Acc@1  86.72 ( 88.14)	Acc@5 100.00 ( 99.70)
Epoch: [11][100/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.3104e-01 (3.3410e-01)	Acc@1  91.41 ( 88.08)	Acc@5 100.00 ( 99.70)
Epoch: [11][110/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.5313e-01 (3.3430e-01)	Acc@1  87.50 ( 88.18)	Acc@5 100.00 ( 99.69)
Epoch: [11][120/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.9484e-01 (3.3189e-01)	Acc@1  90.62 ( 88.25)	Acc@5 100.00 ( 99.71)
Epoch: [11][130/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.2566e-01 (3.3422e-01)	Acc@1  87.50 ( 88.19)	Acc@5 100.00 ( 99.71)
Epoch: [11][140/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.0399e-01 (3.3731e-01)	Acc@1  84.38 ( 88.05)	Acc@5 100.00 ( 99.70)
Epoch: [11][150/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.1249e-01 (3.3977e-01)	Acc@1  86.72 ( 88.05)	Acc@5  99.22 ( 99.67)
Epoch: [11][160/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.9822e-01 (3.4281e-01)	Acc@1  86.72 ( 87.93)	Acc@5 100.00 ( 99.68)
Epoch: [11][170/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.5625e-01 (3.4273e-01)	Acc@1  87.50 ( 87.93)	Acc@5 100.00 ( 99.68)
Epoch: [11][180/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.0635e-01 (3.4213e-01)	Acc@1  89.06 ( 87.94)	Acc@5 100.00 ( 99.68)
Epoch: [11][190/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.4201e-01 (3.4257e-01)	Acc@1  86.72 ( 87.97)	Acc@5 100.00 ( 99.68)
Epoch: [11][200/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.6469e-01 (3.4448e-01)	Acc@1  88.28 ( 87.94)	Acc@5  98.44 ( 99.68)
Epoch: [11][210/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.4507e-01 (3.4411e-01)	Acc@1  87.50 ( 87.95)	Acc@5  99.22 ( 99.67)
Epoch: [11][220/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.5584e-01 (3.4477e-01)	Acc@1  91.41 ( 87.96)	Acc@5  99.22 ( 99.67)
Epoch: [11][230/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.4243e-01 (3.4646e-01)	Acc@1  88.28 ( 87.95)	Acc@5  99.22 ( 99.67)
Epoch: [11][240/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.1922e-01 (3.4690e-01)	Acc@1  84.38 ( 87.90)	Acc@5 100.00 ( 99.66)
Epoch: [11][250/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.002)	Loss 3.2608e-01 (3.4683e-01)	Acc@1  89.06 ( 87.91)	Acc@5 100.00 ( 99.65)
Epoch: [11][260/391]	Time  0.146 ( 0.137)	Data  0.001 ( 0.002)	Loss 3.6795e-01 (3.4717e-01)	Acc@1  84.38 ( 87.90)	Acc@5 100.00 ( 99.65)
Epoch: [11][270/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.002)	Loss 4.0296e-01 (3.4798e-01)	Acc@1  84.38 ( 87.87)	Acc@5 100.00 ( 99.63)
Epoch: [11][280/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.002)	Loss 3.5596e-01 (3.4867e-01)	Acc@1  89.06 ( 87.86)	Acc@5 100.00 ( 99.64)
Epoch: [11][290/391]	Time  0.147 ( 0.137)	Data  0.001 ( 0.002)	Loss 3.7760e-01 (3.4909e-01)	Acc@1  85.94 ( 87.81)	Acc@5  99.22 ( 99.64)
Epoch: [11][300/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.002)	Loss 3.3469e-01 (3.4947e-01)	Acc@1  89.06 ( 87.82)	Acc@5  98.44 ( 99.63)
Epoch: [11][310/391]	Time  0.143 ( 0.137)	Data  0.001 ( 0.002)	Loss 4.6887e-01 (3.4972e-01)	Acc@1  84.38 ( 87.81)	Acc@5 100.00 ( 99.63)
Epoch: [11][320/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.002)	Loss 3.3956e-01 (3.5061e-01)	Acc@1  86.72 ( 87.77)	Acc@5 100.00 ( 99.62)
Epoch: [11][330/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.002)	Loss 3.5586e-01 (3.5161e-01)	Acc@1  88.28 ( 87.72)	Acc@5 100.00 ( 99.63)
Epoch: [11][340/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.002)	Loss 3.6218e-01 (3.5225e-01)	Acc@1  88.28 ( 87.71)	Acc@5  99.22 ( 99.63)
Epoch: [11][350/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.002)	Loss 3.4016e-01 (3.5277e-01)	Acc@1  89.06 ( 87.68)	Acc@5 100.00 ( 99.63)
Epoch: [11][360/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.002)	Loss 2.1762e-01 (3.5164e-01)	Acc@1  93.75 ( 87.71)	Acc@5 100.00 ( 99.63)
Epoch: [11][370/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.002)	Loss 4.0039e-01 (3.5123e-01)	Acc@1  85.94 ( 87.73)	Acc@5 100.00 ( 99.64)
Epoch: [11][380/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.002)	Loss 2.8650e-01 (3.5125e-01)	Acc@1  89.06 ( 87.73)	Acc@5 100.00 ( 99.64)
Epoch: [11][390/391]	Time  0.109 ( 0.137)	Data  0.001 ( 0.001)	Loss 3.7580e-01 (3.5163e-01)	Acc@1  87.50 ( 87.72)	Acc@5 100.00 ( 99.63)
## e[11] optimizer.zero_grad (sum) time: 0.6865653991699219
## e[11]       loss.backward (sum) time: 13.937353610992432
## e[11]      optimizer.step (sum) time: 14.636985540390015
## epoch[11] training(only) time: 53.76575541496277
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 4.3938e-01 (4.3938e-01)	Acc@1  86.00 ( 86.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.048 ( 0.062)	Loss 4.1298e-01 (3.8934e-01)	Acc@1  84.00 ( 86.09)	Acc@5 100.00 ( 99.45)
Test: [ 20/100]	Time  0.046 ( 0.056)	Loss 5.1039e-01 (4.0679e-01)	Acc@1  82.00 ( 85.43)	Acc@5 100.00 ( 99.38)
Test: [ 30/100]	Time  0.046 ( 0.054)	Loss 5.6119e-01 (4.2587e-01)	Acc@1  80.00 ( 85.23)	Acc@5  99.00 ( 99.29)
Test: [ 40/100]	Time  0.045 ( 0.052)	Loss 4.4066e-01 (4.2782e-01)	Acc@1  83.00 ( 85.44)	Acc@5  98.00 ( 99.27)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 3.7915e-01 (4.2834e-01)	Acc@1  85.00 ( 85.37)	Acc@5 100.00 ( 99.29)
Test: [ 60/100]	Time  0.046 ( 0.051)	Loss 3.7764e-01 (4.3173e-01)	Acc@1  86.00 ( 85.18)	Acc@5  99.00 ( 99.36)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 5.3500e-01 (4.2939e-01)	Acc@1  84.00 ( 85.37)	Acc@5  99.00 ( 99.37)
Test: [ 80/100]	Time  0.045 ( 0.049)	Loss 3.2413e-01 (4.2978e-01)	Acc@1  89.00 ( 85.37)	Acc@5 100.00 ( 99.41)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 2.3415e-01 (4.2682e-01)	Acc@1  94.00 ( 85.51)	Acc@5 100.00 ( 99.43)
 * Acc@1 85.590 Acc@5 99.460
### epoch[11] execution time: 58.7474901676178
EPOCH 12
# current learning rate: 0.1
# Switched to train mode...
Epoch: [12][  0/391]	Time  0.298 ( 0.298)	Data  0.154 ( 0.154)	Loss 3.9209e-01 (3.9209e-01)	Acc@1  85.16 ( 85.16)	Acc@5  99.22 ( 99.22)
Epoch: [12][ 10/391]	Time  0.142 ( 0.152)	Data  0.001 ( 0.015)	Loss 4.7464e-01 (3.2249e-01)	Acc@1  83.59 ( 89.13)	Acc@5 100.00 ( 99.64)
Epoch: [12][ 20/391]	Time  0.135 ( 0.145)	Data  0.001 ( 0.008)	Loss 2.2888e-01 (3.0945e-01)	Acc@1  91.41 ( 89.32)	Acc@5 100.00 ( 99.74)
Epoch: [12][ 30/391]	Time  0.137 ( 0.143)	Data  0.001 ( 0.006)	Loss 2.1071e-01 (2.9955e-01)	Acc@1  92.19 ( 89.59)	Acc@5 100.00 ( 99.82)
Epoch: [12][ 40/391]	Time  0.136 ( 0.141)	Data  0.001 ( 0.005)	Loss 4.2043e-01 (3.0347e-01)	Acc@1  84.38 ( 89.56)	Acc@5 100.00 ( 99.83)
Epoch: [12][ 50/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.004)	Loss 3.1828e-01 (3.0329e-01)	Acc@1  91.41 ( 89.55)	Acc@5 100.00 ( 99.83)
Epoch: [12][ 60/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.004)	Loss 2.6589e-01 (3.0554e-01)	Acc@1  91.41 ( 89.38)	Acc@5 100.00 ( 99.83)
Epoch: [12][ 70/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 3.9952e-01 (3.1016e-01)	Acc@1  87.50 ( 89.16)	Acc@5 100.00 ( 99.80)
Epoch: [12][ 80/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.6903e-01 (3.1270e-01)	Acc@1  92.97 ( 89.13)	Acc@5 100.00 ( 99.80)
Epoch: [12][ 90/391]	Time  0.145 ( 0.139)	Data  0.001 ( 0.003)	Loss 5.9783e-01 (3.1829e-01)	Acc@1  79.69 ( 88.87)	Acc@5  99.22 ( 99.79)
Epoch: [12][100/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.003)	Loss 3.1468e-01 (3.2231e-01)	Acc@1  88.28 ( 88.71)	Acc@5  99.22 ( 99.79)
Epoch: [12][110/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.003)	Loss 3.6713e-01 (3.2419e-01)	Acc@1  91.41 ( 88.74)	Acc@5  98.44 ( 99.79)
Epoch: [12][120/391]	Time  0.148 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.3423e-01 (3.2349e-01)	Acc@1  88.28 ( 88.73)	Acc@5 100.00 ( 99.79)
Epoch: [12][130/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.1239e-01 (3.2282e-01)	Acc@1  89.06 ( 88.70)	Acc@5  99.22 ( 99.79)
Epoch: [12][140/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.3105e-01 (3.2521e-01)	Acc@1  87.50 ( 88.61)	Acc@5  99.22 ( 99.79)
Epoch: [12][150/391]	Time  0.144 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.3617e-01 (3.2415e-01)	Acc@1  92.19 ( 88.68)	Acc@5 100.00 ( 99.79)
Epoch: [12][160/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.4575e-01 (3.2755e-01)	Acc@1  89.06 ( 88.57)	Acc@5  99.22 ( 99.79)
Epoch: [12][170/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.4252e-01 (3.2923e-01)	Acc@1  84.38 ( 88.45)	Acc@5  99.22 ( 99.79)
Epoch: [12][180/391]	Time  0.132 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.5931e-01 (3.2850e-01)	Acc@1  86.72 ( 88.48)	Acc@5  98.44 ( 99.78)
Epoch: [12][190/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.3332e-01 (3.2537e-01)	Acc@1  85.94 ( 88.60)	Acc@5 100.00 ( 99.79)
Epoch: [12][200/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2030e-01 (3.2531e-01)	Acc@1  92.19 ( 88.60)	Acc@5 100.00 ( 99.78)
Epoch: [12][210/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.9112e-01 (3.2512e-01)	Acc@1  87.50 ( 88.59)	Acc@5 100.00 ( 99.78)
Epoch: [12][220/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.9359e-01 (3.2589e-01)	Acc@1  89.06 ( 88.55)	Acc@5 100.00 ( 99.78)
Epoch: [12][230/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.4959e-01 (3.2609e-01)	Acc@1  87.50 ( 88.54)	Acc@5 100.00 ( 99.79)
Epoch: [12][240/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.8950e-01 (3.2630e-01)	Acc@1  88.28 ( 88.52)	Acc@5  98.44 ( 99.78)
Epoch: [12][250/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.1590e-01 (3.2689e-01)	Acc@1  89.84 ( 88.53)	Acc@5 100.00 ( 99.78)
Epoch: [12][260/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.7151e-01 (3.2753e-01)	Acc@1  91.41 ( 88.51)	Acc@5 100.00 ( 99.77)
Epoch: [12][270/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1930e-01 (3.2765e-01)	Acc@1  90.62 ( 88.48)	Acc@5 100.00 ( 99.78)
Epoch: [12][280/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.2989e-01 (3.2737e-01)	Acc@1  87.50 ( 88.51)	Acc@5  99.22 ( 99.78)
Epoch: [12][290/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.4892e-01 (3.2850e-01)	Acc@1  82.03 ( 88.43)	Acc@5 100.00 ( 99.77)
Epoch: [12][300/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.6442e-01 (3.2925e-01)	Acc@1  88.28 ( 88.44)	Acc@5  97.66 ( 99.76)
Epoch: [12][310/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.9246e-01 (3.2948e-01)	Acc@1  89.84 ( 88.41)	Acc@5 100.00 ( 99.76)
Epoch: [12][320/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.6649e-01 (3.3039e-01)	Acc@1  89.06 ( 88.41)	Acc@5  99.22 ( 99.75)
Epoch: [12][330/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.5030e-01 (3.3040e-01)	Acc@1  89.84 ( 88.40)	Acc@5  99.22 ( 99.75)
Epoch: [12][340/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6775e-01 (3.3055e-01)	Acc@1  94.53 ( 88.43)	Acc@5 100.00 ( 99.75)
Epoch: [12][350/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0640e-01 (3.2965e-01)	Acc@1  91.41 ( 88.44)	Acc@5 100.00 ( 99.75)
Epoch: [12][360/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1265e-01 (3.3007e-01)	Acc@1  95.31 ( 88.43)	Acc@5  99.22 ( 99.75)
Epoch: [12][370/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.7338e-01 (3.2995e-01)	Acc@1  90.62 ( 88.45)	Acc@5 100.00 ( 99.74)
Epoch: [12][380/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.6361e-01 (3.3070e-01)	Acc@1  85.16 ( 88.42)	Acc@5  99.22 ( 99.73)
Epoch: [12][390/391]	Time  0.107 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.6864e-01 (3.3020e-01)	Acc@1  90.00 ( 88.45)	Acc@5  98.75 ( 99.72)
## e[12] optimizer.zero_grad (sum) time: 0.6858909130096436
## e[12]       loss.backward (sum) time: 13.960694551467896
## e[12]      optimizer.step (sum) time: 14.640007257461548
## epoch[12] training(only) time: 53.87607502937317
# Switched to evaluate mode...
Test: [  0/100]	Time  0.194 ( 0.194)	Loss 3.1222e-01 (3.1222e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.060)	Loss 3.8184e-01 (3.7438e-01)	Acc@1  88.00 ( 87.18)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 4.8907e-01 (3.7928e-01)	Acc@1  81.00 ( 86.76)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.047 ( 0.053)	Loss 3.7991e-01 (3.8687e-01)	Acc@1  87.00 ( 86.87)	Acc@5  99.00 ( 99.61)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 3.2760e-01 (3.8512e-01)	Acc@1  85.00 ( 86.85)	Acc@5 100.00 ( 99.63)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 2.4400e-01 (3.8478e-01)	Acc@1  91.00 ( 86.88)	Acc@5 100.00 ( 99.61)
Test: [ 60/100]	Time  0.048 ( 0.050)	Loss 3.5054e-01 (3.9065e-01)	Acc@1  90.00 ( 86.70)	Acc@5  99.00 ( 99.64)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 4.5267e-01 (3.9301e-01)	Acc@1  83.00 ( 86.56)	Acc@5 100.00 ( 99.62)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 3.1205e-01 (3.9542e-01)	Acc@1  90.00 ( 86.52)	Acc@5 100.00 ( 99.64)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 2.1129e-01 (3.9575e-01)	Acc@1  95.00 ( 86.56)	Acc@5 100.00 ( 99.64)
 * Acc@1 86.650 Acc@5 99.660
### epoch[12] execution time: 58.859638690948486
EPOCH 13
# current learning rate: 0.1
# Switched to train mode...
Epoch: [13][  0/391]	Time  0.290 ( 0.290)	Data  0.135 ( 0.135)	Loss 3.5970e-01 (3.5970e-01)	Acc@1  88.28 ( 88.28)	Acc@5  99.22 ( 99.22)
Epoch: [13][ 10/391]	Time  0.143 ( 0.150)	Data  0.001 ( 0.013)	Loss 2.6804e-01 (2.9475e-01)	Acc@1  89.06 ( 90.06)	Acc@5 100.00 ( 99.57)
Epoch: [13][ 20/391]	Time  0.134 ( 0.143)	Data  0.001 ( 0.007)	Loss 3.5768e-01 (3.0931e-01)	Acc@1  85.94 ( 89.14)	Acc@5 100.00 ( 99.74)
Epoch: [13][ 30/391]	Time  0.133 ( 0.141)	Data  0.001 ( 0.005)	Loss 2.8833e-01 (2.9127e-01)	Acc@1  89.84 ( 89.99)	Acc@5 100.00 ( 99.72)
Epoch: [13][ 40/391]	Time  0.140 ( 0.141)	Data  0.001 ( 0.004)	Loss 2.3938e-01 (2.8846e-01)	Acc@1  89.84 ( 90.11)	Acc@5 100.00 ( 99.71)
Epoch: [13][ 50/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.004)	Loss 2.1294e-01 (2.8815e-01)	Acc@1  92.97 ( 90.10)	Acc@5 100.00 ( 99.74)
Epoch: [13][ 60/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 3.2779e-01 (2.9509e-01)	Acc@1  88.28 ( 89.86)	Acc@5  99.22 ( 99.67)
Epoch: [13][ 70/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.003)	Loss 4.1002e-01 (2.9639e-01)	Acc@1  85.94 ( 89.84)	Acc@5  99.22 ( 99.64)
Epoch: [13][ 80/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.6245e-01 (2.9714e-01)	Acc@1  90.62 ( 89.73)	Acc@5 100.00 ( 99.65)
Epoch: [13][ 90/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 3.0873e-01 (2.9312e-01)	Acc@1  88.28 ( 89.86)	Acc@5 100.00 ( 99.69)
Epoch: [13][100/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.9556e-01 (2.9276e-01)	Acc@1  86.72 ( 89.87)	Acc@5 100.00 ( 99.71)
Epoch: [13][110/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.9941e-01 (2.9411e-01)	Acc@1  94.53 ( 89.88)	Acc@5 100.00 ( 99.71)
Epoch: [13][120/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.2972e-01 (2.9649e-01)	Acc@1  89.06 ( 89.74)	Acc@5 100.00 ( 99.72)
Epoch: [13][130/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.2673e-01 (2.9872e-01)	Acc@1  86.72 ( 89.59)	Acc@5 100.00 ( 99.73)
Epoch: [13][140/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.5599e-01 (2.9959e-01)	Acc@1  92.97 ( 89.60)	Acc@5 100.00 ( 99.74)
Epoch: [13][150/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.3760e-01 (2.9946e-01)	Acc@1  89.84 ( 89.53)	Acc@5 100.00 ( 99.74)
Epoch: [13][160/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.2524e-01 (3.0307e-01)	Acc@1  90.62 ( 89.47)	Acc@5 100.00 ( 99.73)
Epoch: [13][170/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.3230e-01 (3.0554e-01)	Acc@1  92.19 ( 89.39)	Acc@5 100.00 ( 99.73)
Epoch: [13][180/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1643e-01 (3.0567e-01)	Acc@1  91.41 ( 89.39)	Acc@5 100.00 ( 99.72)
Epoch: [13][190/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.4972e-01 (3.0706e-01)	Acc@1  90.62 ( 89.34)	Acc@5 100.00 ( 99.72)
Epoch: [13][200/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.0468e-01 (3.0696e-01)	Acc@1  89.84 ( 89.34)	Acc@5 100.00 ( 99.72)
Epoch: [13][210/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.9912e-01 (3.0909e-01)	Acc@1  89.06 ( 89.22)	Acc@5  98.44 ( 99.72)
Epoch: [13][220/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.0348e-01 (3.0980e-01)	Acc@1  85.94 ( 89.16)	Acc@5 100.00 ( 99.73)
Epoch: [13][230/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1927e-01 (3.0926e-01)	Acc@1  91.41 ( 89.18)	Acc@5 100.00 ( 99.72)
Epoch: [13][240/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.9358e-01 (3.0930e-01)	Acc@1  85.94 ( 89.16)	Acc@5 100.00 ( 99.72)
Epoch: [13][250/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.9340e-01 (3.1102e-01)	Acc@1  87.50 ( 89.11)	Acc@5  99.22 ( 99.70)
Epoch: [13][260/391]	Time  0.147 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.0285e-01 (3.1203e-01)	Acc@1  89.84 ( 89.13)	Acc@5  99.22 ( 99.71)
Epoch: [13][270/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.5081e-01 (3.1161e-01)	Acc@1  87.50 ( 89.15)	Acc@5 100.00 ( 99.71)
Epoch: [13][280/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.0184e-01 (3.1246e-01)	Acc@1  89.06 ( 89.10)	Acc@5 100.00 ( 99.71)
Epoch: [13][290/391]	Time  0.147 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.3783e-01 (3.1318e-01)	Acc@1  86.72 ( 89.06)	Acc@5  99.22 ( 99.71)
Epoch: [13][300/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.8518e-01 (3.1419e-01)	Acc@1  92.19 ( 89.05)	Acc@5 100.00 ( 99.71)
Epoch: [13][310/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2870e-01 (3.1421e-01)	Acc@1  91.41 ( 89.07)	Acc@5 100.00 ( 99.71)
Epoch: [13][320/391]	Time  0.148 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.8012e-01 (3.1334e-01)	Acc@1  92.19 ( 89.12)	Acc@5 100.00 ( 99.71)
Epoch: [13][330/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.3265e-01 (3.1449e-01)	Acc@1  91.41 ( 89.08)	Acc@5 100.00 ( 99.71)
Epoch: [13][340/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.2379e-01 (3.1499e-01)	Acc@1  82.03 ( 89.06)	Acc@5  99.22 ( 99.71)
Epoch: [13][350/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.0936e-01 (3.1482e-01)	Acc@1  92.19 ( 89.05)	Acc@5  98.44 ( 99.70)
Epoch: [13][360/391]	Time  0.144 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.5076e-01 (3.1374e-01)	Acc@1  86.72 ( 89.08)	Acc@5  99.22 ( 99.71)
Epoch: [13][370/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.6139e-01 (3.1445e-01)	Acc@1  90.62 ( 89.05)	Acc@5  99.22 ( 99.70)
Epoch: [13][380/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1275e-01 (3.1425e-01)	Acc@1  92.97 ( 89.06)	Acc@5 100.00 ( 99.70)
Epoch: [13][390/391]	Time  0.107 ( 0.138)	Data  0.001 ( 0.001)	Loss 4.2625e-01 (3.1542e-01)	Acc@1  86.25 ( 89.00)	Acc@5 100.00 ( 99.70)
## e[13] optimizer.zero_grad (sum) time: 0.6821980476379395
## e[13]       loss.backward (sum) time: 13.913342952728271
## e[13]      optimizer.step (sum) time: 14.660978555679321
## epoch[13] training(only) time: 53.890812397003174
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 4.6459e-01 (4.6459e-01)	Acc@1  86.00 ( 86.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.049 ( 0.062)	Loss 4.1335e-01 (4.1611e-01)	Acc@1  85.00 ( 85.64)	Acc@5 100.00 ( 99.45)
Test: [ 20/100]	Time  0.058 ( 0.056)	Loss 5.4352e-01 (4.2542e-01)	Acc@1  80.00 ( 85.62)	Acc@5 100.00 ( 99.52)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 4.1192e-01 (4.2249e-01)	Acc@1  83.00 ( 85.77)	Acc@5 100.00 ( 99.55)
Test: [ 40/100]	Time  0.049 ( 0.052)	Loss 3.5527e-01 (4.1878e-01)	Acc@1  88.00 ( 86.05)	Acc@5 100.00 ( 99.41)
Test: [ 50/100]	Time  0.045 ( 0.051)	Loss 2.1841e-01 (4.2197e-01)	Acc@1  95.00 ( 86.02)	Acc@5 100.00 ( 99.39)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 3.6285e-01 (4.1935e-01)	Acc@1  85.00 ( 86.02)	Acc@5 100.00 ( 99.41)
Test: [ 70/100]	Time  0.057 ( 0.050)	Loss 4.4680e-01 (4.1975e-01)	Acc@1  86.00 ( 86.04)	Acc@5  99.00 ( 99.39)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 4.0411e-01 (4.2365e-01)	Acc@1  86.00 ( 85.95)	Acc@5 100.00 ( 99.41)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 3.8719e-01 (4.2353e-01)	Acc@1  83.00 ( 85.86)	Acc@5 100.00 ( 99.42)
 * Acc@1 85.980 Acc@5 99.440
### epoch[13] execution time: 58.854148387908936
EPOCH 14
# current learning rate: 0.1
# Switched to train mode...
Epoch: [14][  0/391]	Time  0.295 ( 0.295)	Data  0.155 ( 0.155)	Loss 2.6669e-01 (2.6669e-01)	Acc@1  89.84 ( 89.84)	Acc@5 100.00 (100.00)
Epoch: [14][ 10/391]	Time  0.136 ( 0.150)	Data  0.001 ( 0.015)	Loss 2.3797e-01 (2.5431e-01)	Acc@1  89.84 ( 90.34)	Acc@5 100.00 ( 99.72)
Epoch: [14][ 20/391]	Time  0.137 ( 0.144)	Data  0.001 ( 0.008)	Loss 3.4643e-01 (2.7810e-01)	Acc@1  89.06 ( 90.14)	Acc@5  99.22 ( 99.74)
Epoch: [14][ 30/391]	Time  0.138 ( 0.142)	Data  0.001 ( 0.006)	Loss 2.9693e-01 (2.8153e-01)	Acc@1  89.84 ( 89.97)	Acc@5 100.00 ( 99.75)
Epoch: [14][ 40/391]	Time  0.138 ( 0.141)	Data  0.001 ( 0.005)	Loss 2.3306e-01 (2.8024e-01)	Acc@1  93.75 ( 90.19)	Acc@5 100.00 ( 99.73)
Epoch: [14][ 50/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.004)	Loss 2.8942e-01 (2.8171e-01)	Acc@1  89.84 ( 90.07)	Acc@5 100.00 ( 99.79)
Epoch: [14][ 60/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.004)	Loss 2.1816e-01 (2.7792e-01)	Acc@1  92.97 ( 90.15)	Acc@5 100.00 ( 99.82)
Epoch: [14][ 70/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 3.6872e-01 (2.8150e-01)	Acc@1  87.50 ( 90.04)	Acc@5  99.22 ( 99.81)
Epoch: [14][ 80/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.8439e-01 (2.8597e-01)	Acc@1  90.62 ( 89.89)	Acc@5  99.22 ( 99.82)
Epoch: [14][ 90/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.5924e-01 (2.8529e-01)	Acc@1  89.84 ( 89.94)	Acc@5 100.00 ( 99.82)
Epoch: [14][100/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.1198e-01 (2.8282e-01)	Acc@1  93.75 ( 90.00)	Acc@5 100.00 ( 99.82)
Epoch: [14][110/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.0593e-01 (2.8473e-01)	Acc@1  88.28 ( 89.95)	Acc@5  98.44 ( 99.81)
Epoch: [14][120/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1508e-01 (2.8458e-01)	Acc@1  92.19 ( 89.95)	Acc@5 100.00 ( 99.81)
Epoch: [14][130/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.6617e-01 (2.8629e-01)	Acc@1  90.62 ( 89.88)	Acc@5 100.00 ( 99.81)
Epoch: [14][140/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.9585e-01 (2.8702e-01)	Acc@1  92.19 ( 89.87)	Acc@5 100.00 ( 99.82)
Epoch: [14][150/391]	Time  0.152 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.4278e-01 (2.8615e-01)	Acc@1  85.16 ( 89.82)	Acc@5  99.22 ( 99.81)
Epoch: [14][160/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.2282e-01 (2.8677e-01)	Acc@1  90.62 ( 89.85)	Acc@5 100.00 ( 99.82)
Epoch: [14][170/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.4714e-01 (2.8970e-01)	Acc@1  87.50 ( 89.74)	Acc@5 100.00 ( 99.80)
Epoch: [14][180/391]	Time  0.145 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.6051e-01 (2.9217e-01)	Acc@1  90.62 ( 89.72)	Acc@5  99.22 ( 99.80)
Epoch: [14][190/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1742e-01 (2.9202e-01)	Acc@1  93.75 ( 89.77)	Acc@5 100.00 ( 99.80)
Epoch: [14][200/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.8853e-01 (2.9540e-01)	Acc@1  85.16 ( 89.66)	Acc@5 100.00 ( 99.79)
Epoch: [14][210/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.8692e-01 (2.9423e-01)	Acc@1  89.84 ( 89.71)	Acc@5  99.22 ( 99.79)
Epoch: [14][220/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.4194e-01 (2.9529e-01)	Acc@1  89.84 ( 89.67)	Acc@5  99.22 ( 99.79)
Epoch: [14][230/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.5927e-01 (2.9523e-01)	Acc@1  89.84 ( 89.66)	Acc@5 100.00 ( 99.78)
Epoch: [14][240/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.9076e-01 (2.9542e-01)	Acc@1  89.06 ( 89.65)	Acc@5 100.00 ( 99.78)
Epoch: [14][250/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1185e-01 (2.9479e-01)	Acc@1  91.41 ( 89.66)	Acc@5 100.00 ( 99.78)
Epoch: [14][260/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.7329e-01 (2.9452e-01)	Acc@1  92.97 ( 89.69)	Acc@5 100.00 ( 99.79)
Epoch: [14][270/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.2700e-01 (2.9482e-01)	Acc@1  92.97 ( 89.69)	Acc@5 100.00 ( 99.79)
Epoch: [14][280/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.0627e-01 (2.9541e-01)	Acc@1  88.28 ( 89.67)	Acc@5 100.00 ( 99.79)
Epoch: [14][290/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.8694e-01 (2.9535e-01)	Acc@1  88.28 ( 89.66)	Acc@5 100.00 ( 99.79)
Epoch: [14][300/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.6071e-01 (2.9613e-01)	Acc@1  85.16 ( 89.62)	Acc@5 100.00 ( 99.79)
Epoch: [14][310/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.0895e-01 (2.9690e-01)	Acc@1  89.84 ( 89.60)	Acc@5  99.22 ( 99.79)
Epoch: [14][320/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2037e-01 (2.9641e-01)	Acc@1  92.97 ( 89.63)	Acc@5 100.00 ( 99.80)
Epoch: [14][330/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.3598e-01 (2.9621e-01)	Acc@1  90.62 ( 89.66)	Acc@5 100.00 ( 99.80)
Epoch: [14][340/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.6598e-01 (2.9696e-01)	Acc@1  89.06 ( 89.62)	Acc@5 100.00 ( 99.80)
Epoch: [14][350/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2555e-01 (2.9800e-01)	Acc@1  92.97 ( 89.58)	Acc@5 100.00 ( 99.79)
Epoch: [14][360/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.2875e-01 (2.9807e-01)	Acc@1  82.03 ( 89.57)	Acc@5 100.00 ( 99.79)
Epoch: [14][370/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.3599e-01 (2.9770e-01)	Acc@1  91.41 ( 89.60)	Acc@5 100.00 ( 99.79)
Epoch: [14][380/391]	Time  0.144 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.4452e-01 (2.9751e-01)	Acc@1  92.19 ( 89.64)	Acc@5  99.22 ( 99.79)
Epoch: [14][390/391]	Time  0.110 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.0918e-01 (2.9770e-01)	Acc@1  91.25 ( 89.64)	Acc@5 100.00 ( 99.78)
## e[14] optimizer.zero_grad (sum) time: 0.688500165939331
## e[14]       loss.backward (sum) time: 13.945151090621948
## e[14]      optimizer.step (sum) time: 14.650550365447998
## epoch[14] training(only) time: 53.98232817649841
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 3.5760e-01 (3.5760e-01)	Acc@1  91.00 ( 91.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 3.2138e-01 (3.6011e-01)	Acc@1  89.00 ( 87.64)	Acc@5 100.00 ( 99.55)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 4.2036e-01 (3.7197e-01)	Acc@1  84.00 ( 87.33)	Acc@5  99.00 ( 99.38)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 5.2685e-01 (3.9317e-01)	Acc@1  82.00 ( 86.90)	Acc@5  99.00 ( 99.48)
Test: [ 40/100]	Time  0.048 ( 0.051)	Loss 4.2225e-01 (3.9763e-01)	Acc@1  83.00 ( 86.85)	Acc@5 100.00 ( 99.44)
Test: [ 50/100]	Time  0.048 ( 0.051)	Loss 1.7857e-01 (3.9189e-01)	Acc@1  96.00 ( 87.18)	Acc@5 100.00 ( 99.45)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 3.6378e-01 (3.9801e-01)	Acc@1  87.00 ( 86.93)	Acc@5 100.00 ( 99.48)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 4.2525e-01 (3.9675e-01)	Acc@1  85.00 ( 86.99)	Acc@5 100.00 ( 99.54)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 3.4174e-01 (3.9783e-01)	Acc@1  86.00 ( 86.89)	Acc@5 100.00 ( 99.54)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 4.0538e-01 (3.9521e-01)	Acc@1  87.00 ( 86.99)	Acc@5  99.00 ( 99.55)
 * Acc@1 87.100 Acc@5 99.560
### epoch[14] execution time: 58.96558856964111
EPOCH 15
# current learning rate: 0.1
# Switched to train mode...
Epoch: [15][  0/391]	Time  0.295 ( 0.295)	Data  0.152 ( 0.152)	Loss 2.7061e-01 (2.7061e-01)	Acc@1  92.19 ( 92.19)	Acc@5  99.22 ( 99.22)
Epoch: [15][ 10/391]	Time  0.147 ( 0.151)	Data  0.001 ( 0.015)	Loss 2.6317e-01 (2.5585e-01)	Acc@1  89.06 ( 90.77)	Acc@5 100.00 ( 99.64)
Epoch: [15][ 20/391]	Time  0.136 ( 0.145)	Data  0.001 ( 0.008)	Loss 2.6533e-01 (2.5867e-01)	Acc@1  89.84 ( 90.74)	Acc@5 100.00 ( 99.78)
Epoch: [15][ 30/391]	Time  0.137 ( 0.143)	Data  0.001 ( 0.006)	Loss 2.6176e-01 (2.5805e-01)	Acc@1  89.84 ( 90.85)	Acc@5 100.00 ( 99.82)
Epoch: [15][ 40/391]	Time  0.148 ( 0.142)	Data  0.001 ( 0.005)	Loss 2.2358e-01 (2.5390e-01)	Acc@1  91.41 ( 91.18)	Acc@5 100.00 ( 99.81)
Epoch: [15][ 50/391]	Time  0.135 ( 0.141)	Data  0.001 ( 0.004)	Loss 2.5865e-01 (2.5573e-01)	Acc@1  91.41 ( 91.28)	Acc@5  99.22 ( 99.80)
Epoch: [15][ 60/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.004)	Loss 3.1444e-01 (2.5976e-01)	Acc@1  89.06 ( 91.03)	Acc@5 100.00 ( 99.78)
Epoch: [15][ 70/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.9433e-01 (2.6400e-01)	Acc@1  92.97 ( 90.91)	Acc@5 100.00 ( 99.80)
Epoch: [15][ 80/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.5701e-01 (2.6510e-01)	Acc@1  90.62 ( 90.77)	Acc@5 100.00 ( 99.81)
Epoch: [15][ 90/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.7147e-01 (2.7074e-01)	Acc@1  92.97 ( 90.62)	Acc@5 100.00 ( 99.79)
Epoch: [15][100/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.9679e-01 (2.7034e-01)	Acc@1  92.19 ( 90.64)	Acc@5 100.00 ( 99.80)
Epoch: [15][110/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.4008e-01 (2.7075e-01)	Acc@1  91.41 ( 90.57)	Acc@5 100.00 ( 99.81)
Epoch: [15][120/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0770e-01 (2.7271e-01)	Acc@1  94.53 ( 90.46)	Acc@5 100.00 ( 99.80)
Epoch: [15][130/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.8670e-01 (2.7286e-01)	Acc@1  89.84 ( 90.46)	Acc@5 100.00 ( 99.81)
Epoch: [15][140/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.0273e-01 (2.7100e-01)	Acc@1  88.28 ( 90.53)	Acc@5 100.00 ( 99.81)
Epoch: [15][150/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1066e-01 (2.7319e-01)	Acc@1  92.97 ( 90.49)	Acc@5 100.00 ( 99.80)
Epoch: [15][160/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.5139e-01 (2.7243e-01)	Acc@1  89.06 ( 90.47)	Acc@5  99.22 ( 99.81)
Epoch: [15][170/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.1446e-01 (2.7288e-01)	Acc@1  90.62 ( 90.49)	Acc@5 100.00 ( 99.80)
Epoch: [15][180/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.5520e-01 (2.7352e-01)	Acc@1  89.06 ( 90.46)	Acc@5 100.00 ( 99.80)
Epoch: [15][190/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2950e-01 (2.7400e-01)	Acc@1  92.97 ( 90.44)	Acc@5 100.00 ( 99.80)
Epoch: [15][200/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.4169e-01 (2.7597e-01)	Acc@1  85.94 ( 90.33)	Acc@5 100.00 ( 99.80)
Epoch: [15][210/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.5671e-01 (2.7432e-01)	Acc@1  89.84 ( 90.35)	Acc@5 100.00 ( 99.79)
Epoch: [15][220/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.9319e-01 (2.7536e-01)	Acc@1  88.28 ( 90.34)	Acc@5 100.00 ( 99.78)
Epoch: [15][230/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1598e-01 (2.7564e-01)	Acc@1  91.41 ( 90.30)	Acc@5  99.22 ( 99.78)
Epoch: [15][240/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.8917e-01 (2.7755e-01)	Acc@1  91.41 ( 90.27)	Acc@5 100.00 ( 99.79)
Epoch: [15][250/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.8850e-02 (2.7547e-01)	Acc@1  96.88 ( 90.36)	Acc@5 100.00 ( 99.79)
Epoch: [15][260/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.5345e-01 (2.7579e-01)	Acc@1  89.06 ( 90.33)	Acc@5 100.00 ( 99.79)
Epoch: [15][270/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.8049e-01 (2.7622e-01)	Acc@1  89.06 ( 90.29)	Acc@5  99.22 ( 99.79)
Epoch: [15][280/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.5576e-01 (2.7652e-01)	Acc@1  85.94 ( 90.27)	Acc@5  99.22 ( 99.79)
Epoch: [15][290/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0740e-01 (2.7599e-01)	Acc@1  91.41 ( 90.26)	Acc@5 100.00 ( 99.79)
Epoch: [15][300/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9982e-01 (2.7466e-01)	Acc@1  92.97 ( 90.32)	Acc@5 100.00 ( 99.79)
Epoch: [15][310/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.0749e-01 (2.7437e-01)	Acc@1  88.28 ( 90.33)	Acc@5 100.00 ( 99.79)
Epoch: [15][320/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.4413e-01 (2.7553e-01)	Acc@1  89.06 ( 90.32)	Acc@5  98.44 ( 99.79)
Epoch: [15][330/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.9851e-01 (2.7615e-01)	Acc@1  89.84 ( 90.31)	Acc@5 100.00 ( 99.79)
Epoch: [15][340/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9313e-01 (2.7654e-01)	Acc@1  95.31 ( 90.32)	Acc@5 100.00 ( 99.78)
Epoch: [15][350/391]	Time  0.146 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2398e-01 (2.7752e-01)	Acc@1  89.84 ( 90.26)	Acc@5 100.00 ( 99.78)
Epoch: [15][360/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.5383e-01 (2.7724e-01)	Acc@1  86.72 ( 90.27)	Acc@5 100.00 ( 99.79)
Epoch: [15][370/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.5238e-01 (2.7765e-01)	Acc@1  94.53 ( 90.27)	Acc@5 100.00 ( 99.79)
Epoch: [15][380/391]	Time  0.145 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.5097e-01 (2.7661e-01)	Acc@1  92.97 ( 90.31)	Acc@5 100.00 ( 99.80)
Epoch: [15][390/391]	Time  0.111 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.7795e-01 (2.7658e-01)	Acc@1  85.00 ( 90.31)	Acc@5 100.00 ( 99.80)
## e[15] optimizer.zero_grad (sum) time: 0.6832184791564941
## e[15]       loss.backward (sum) time: 13.948055028915405
## e[15]      optimizer.step (sum) time: 14.643265962600708
## epoch[15] training(only) time: 53.94450283050537
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 4.2833e-01 (4.2833e-01)	Acc@1  88.00 ( 88.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.047 ( 0.062)	Loss 4.6439e-01 (4.0941e-01)	Acc@1  88.00 ( 87.09)	Acc@5  97.00 ( 99.27)
Test: [ 20/100]	Time  0.057 ( 0.056)	Loss 5.0751e-01 (4.3204e-01)	Acc@1  80.00 ( 85.76)	Acc@5 100.00 ( 99.33)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 4.2359e-01 (4.2997e-01)	Acc@1  85.00 ( 86.13)	Acc@5  99.00 ( 99.29)
Test: [ 40/100]	Time  0.049 ( 0.052)	Loss 3.2961e-01 (4.2456e-01)	Acc@1  89.00 ( 86.46)	Acc@5  99.00 ( 99.27)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 2.4875e-01 (4.2287e-01)	Acc@1  88.00 ( 86.37)	Acc@5 100.00 ( 99.33)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 4.5613e-01 (4.2093e-01)	Acc@1  87.00 ( 86.41)	Acc@5 100.00 ( 99.38)
Test: [ 70/100]	Time  0.056 ( 0.049)	Loss 6.7020e-01 (4.2160e-01)	Acc@1  80.00 ( 86.45)	Acc@5 100.00 ( 99.42)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 3.7096e-01 (4.2376e-01)	Acc@1  86.00 ( 86.28)	Acc@5 100.00 ( 99.47)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 2.6925e-01 (4.1972e-01)	Acc@1  89.00 ( 86.41)	Acc@5 100.00 ( 99.47)
 * Acc@1 86.440 Acc@5 99.500
### epoch[15] execution time: 58.92790985107422
EPOCH 16
# current learning rate: 0.1
# Switched to train mode...
Epoch: [16][  0/391]	Time  0.299 ( 0.299)	Data  0.154 ( 0.154)	Loss 2.5092e-01 (2.5092e-01)	Acc@1  92.97 ( 92.97)	Acc@5 100.00 (100.00)
Epoch: [16][ 10/391]	Time  0.135 ( 0.152)	Data  0.001 ( 0.015)	Loss 1.9841e-01 (2.2671e-01)	Acc@1  91.41 ( 92.05)	Acc@5 100.00 ( 99.93)
Epoch: [16][ 20/391]	Time  0.138 ( 0.145)	Data  0.001 ( 0.008)	Loss 1.7964e-01 (2.2910e-01)	Acc@1  94.53 ( 92.19)	Acc@5 100.00 ( 99.93)
Epoch: [16][ 30/391]	Time  0.139 ( 0.142)	Data  0.001 ( 0.006)	Loss 2.9343e-01 (2.3109e-01)	Acc@1  87.50 ( 91.96)	Acc@5 100.00 ( 99.95)
Epoch: [16][ 40/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.005)	Loss 1.4309e-01 (2.2643e-01)	Acc@1  96.09 ( 91.98)	Acc@5 100.00 ( 99.94)
Epoch: [16][ 50/391]	Time  0.138 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.9910e-01 (2.2997e-01)	Acc@1  93.75 ( 91.96)	Acc@5  98.44 ( 99.88)
Epoch: [16][ 60/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.004)	Loss 2.3360e-01 (2.3002e-01)	Acc@1  90.62 ( 91.91)	Acc@5 100.00 ( 99.86)
Epoch: [16][ 70/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.7183e-01 (2.3765e-01)	Acc@1  91.41 ( 91.71)	Acc@5 100.00 ( 99.82)
Epoch: [16][ 80/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 3.7495e-01 (2.3726e-01)	Acc@1  86.72 ( 91.66)	Acc@5 100.00 ( 99.84)
Epoch: [16][ 90/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.003)	Loss 3.1929e-01 (2.4091e-01)	Acc@1  90.62 ( 91.50)	Acc@5 100.00 ( 99.83)
Epoch: [16][100/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.003)	Loss 3.4992e-01 (2.4026e-01)	Acc@1  88.28 ( 91.55)	Acc@5 100.00 ( 99.83)
Epoch: [16][110/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.8365e-01 (2.4164e-01)	Acc@1  89.06 ( 91.50)	Acc@5 100.00 ( 99.85)
Epoch: [16][120/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6180e-01 (2.4323e-01)	Acc@1  95.31 ( 91.44)	Acc@5 100.00 ( 99.85)
Epoch: [16][130/391]	Time  0.142 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.6972e-01 (2.4232e-01)	Acc@1  89.84 ( 91.51)	Acc@5 100.00 ( 99.85)
Epoch: [16][140/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2580e-01 (2.4447e-01)	Acc@1  90.62 ( 91.44)	Acc@5 100.00 ( 99.84)
Epoch: [16][150/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.5573e-01 (2.4518e-01)	Acc@1  92.97 ( 91.40)	Acc@5 100.00 ( 99.84)
Epoch: [16][160/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.3336e-01 (2.4809e-01)	Acc@1  89.06 ( 91.27)	Acc@5 100.00 ( 99.84)
Epoch: [16][170/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.3246e-01 (2.5018e-01)	Acc@1  90.62 ( 91.18)	Acc@5  98.44 ( 99.82)
Epoch: [16][180/391]	Time  0.148 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.1997e-01 (2.5241e-01)	Acc@1  87.50 ( 91.03)	Acc@5  99.22 ( 99.82)
Epoch: [16][190/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.8034e-01 (2.5376e-01)	Acc@1  90.62 ( 91.02)	Acc@5 100.00 ( 99.82)
Epoch: [16][200/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.2930e-01 (2.5476e-01)	Acc@1  91.41 ( 90.98)	Acc@5 100.00 ( 99.82)
Epoch: [16][210/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.7511e-01 (2.5651e-01)	Acc@1  90.62 ( 90.97)	Acc@5 100.00 ( 99.81)
Epoch: [16][220/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.3072e-01 (2.5918e-01)	Acc@1  91.41 ( 90.91)	Acc@5 100.00 ( 99.81)
Epoch: [16][230/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.5058e-01 (2.6062e-01)	Acc@1  89.84 ( 90.86)	Acc@5 100.00 ( 99.81)
Epoch: [16][240/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.4344e-01 (2.5946e-01)	Acc@1  91.41 ( 90.90)	Acc@5 100.00 ( 99.81)
Epoch: [16][250/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.7317e-01 (2.6176e-01)	Acc@1  86.72 ( 90.85)	Acc@5 100.00 ( 99.81)
Epoch: [16][260/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.1478e-01 (2.6171e-01)	Acc@1  89.84 ( 90.86)	Acc@5  98.44 ( 99.81)
Epoch: [16][270/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.2254e-01 (2.6213e-01)	Acc@1  81.25 ( 90.83)	Acc@5 100.00 ( 99.81)
Epoch: [16][280/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.4139e-01 (2.6183e-01)	Acc@1  92.19 ( 90.84)	Acc@5 100.00 ( 99.81)
Epoch: [16][290/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.1727e-01 (2.6264e-01)	Acc@1  89.84 ( 90.82)	Acc@5  99.22 ( 99.81)
Epoch: [16][300/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0694e-01 (2.6334e-01)	Acc@1  91.41 ( 90.80)	Acc@5 100.00 ( 99.81)
Epoch: [16][310/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.5443e-01 (2.6301e-01)	Acc@1  86.72 ( 90.83)	Acc@5 100.00 ( 99.81)
Epoch: [16][320/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2518e-01 (2.6232e-01)	Acc@1  92.19 ( 90.84)	Acc@5 100.00 ( 99.81)
Epoch: [16][330/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.2418e-01 (2.6289e-01)	Acc@1  89.84 ( 90.81)	Acc@5  99.22 ( 99.80)
Epoch: [16][340/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.9795e-01 (2.6307e-01)	Acc@1  89.06 ( 90.79)	Acc@5 100.00 ( 99.80)
Epoch: [16][350/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.3820e-01 (2.6259e-01)	Acc@1  87.50 ( 90.83)	Acc@5 100.00 ( 99.80)
Epoch: [16][360/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5163e-01 (2.6341e-01)	Acc@1  94.53 ( 90.83)	Acc@5 100.00 ( 99.80)
Epoch: [16][370/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.0033e-01 (2.6301e-01)	Acc@1  89.84 ( 90.82)	Acc@5  99.22 ( 99.80)
Epoch: [16][380/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.3220e-01 (2.6353e-01)	Acc@1  84.38 ( 90.79)	Acc@5 100.00 ( 99.79)
Epoch: [16][390/391]	Time  0.106 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.0795e-01 (2.6455e-01)	Acc@1  95.00 ( 90.77)	Acc@5 100.00 ( 99.79)
## e[16] optimizer.zero_grad (sum) time: 0.6910874843597412
## e[16]       loss.backward (sum) time: 13.958467245101929
## e[16]      optimizer.step (sum) time: 14.622791290283203
## epoch[16] training(only) time: 53.971883058547974
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 5.0722e-01 (5.0722e-01)	Acc@1  81.00 ( 81.00)	Acc@5  97.00 ( 97.00)
Test: [ 10/100]	Time  0.047 ( 0.060)	Loss 3.9478e-01 (4.2132e-01)	Acc@1  89.00 ( 86.64)	Acc@5  99.00 ( 99.09)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 4.2392e-01 (4.4760e-01)	Acc@1  81.00 ( 85.62)	Acc@5 100.00 ( 99.14)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 5.2500e-01 (4.7022e-01)	Acc@1  84.00 ( 85.16)	Acc@5  99.00 ( 99.16)
Test: [ 40/100]	Time  0.046 ( 0.052)	Loss 4.9424e-01 (4.6626e-01)	Acc@1  84.00 ( 85.22)	Acc@5 100.00 ( 99.17)
Test: [ 50/100]	Time  0.049 ( 0.051)	Loss 3.1237e-01 (4.6188e-01)	Acc@1  90.00 ( 85.25)	Acc@5 100.00 ( 99.20)
Test: [ 60/100]	Time  0.048 ( 0.050)	Loss 4.5098e-01 (4.6731e-01)	Acc@1  85.00 ( 85.15)	Acc@5 100.00 ( 99.18)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 6.6713e-01 (4.6773e-01)	Acc@1  84.00 ( 85.25)	Acc@5  99.00 ( 99.17)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 3.4989e-01 (4.6259e-01)	Acc@1  87.00 ( 85.27)	Acc@5  99.00 ( 99.16)
Test: [ 90/100]	Time  0.049 ( 0.050)	Loss 4.0885e-01 (4.5939e-01)	Acc@1  86.00 ( 85.46)	Acc@5 100.00 ( 99.20)
 * Acc@1 85.580 Acc@5 99.260
### epoch[16] execution time: 59.01486921310425
EPOCH 17
# current learning rate: 0.1
# Switched to train mode...
Epoch: [17][  0/391]	Time  0.292 ( 0.292)	Data  0.145 ( 0.145)	Loss 2.3007e-01 (2.3007e-01)	Acc@1  91.41 ( 91.41)	Acc@5 100.00 (100.00)
Epoch: [17][ 10/391]	Time  0.135 ( 0.151)	Data  0.001 ( 0.014)	Loss 1.9593e-01 (2.4539e-01)	Acc@1  93.75 ( 91.55)	Acc@5 100.00 ( 99.93)
Epoch: [17][ 20/391]	Time  0.136 ( 0.144)	Data  0.001 ( 0.008)	Loss 2.8569e-01 (2.4978e-01)	Acc@1  89.84 ( 91.26)	Acc@5  99.22 ( 99.85)
Epoch: [17][ 30/391]	Time  0.136 ( 0.142)	Data  0.001 ( 0.006)	Loss 2.0847e-01 (2.5837e-01)	Acc@1  93.75 ( 91.18)	Acc@5  99.22 ( 99.80)
Epoch: [17][ 40/391]	Time  0.148 ( 0.141)	Data  0.001 ( 0.005)	Loss 1.9599e-01 (2.5772e-01)	Acc@1  92.97 ( 91.03)	Acc@5  99.22 ( 99.79)
Epoch: [17][ 50/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.004)	Loss 2.5186e-01 (2.5201e-01)	Acc@1  91.41 ( 91.18)	Acc@5 100.00 ( 99.80)
Epoch: [17][ 60/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.004)	Loss 3.1575e-01 (2.5043e-01)	Acc@1  90.62 ( 91.33)	Acc@5  99.22 ( 99.80)
Epoch: [17][ 70/391]	Time  0.142 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.6543e-01 (2.4702e-01)	Acc@1  95.31 ( 91.53)	Acc@5 100.00 ( 99.81)
Epoch: [17][ 80/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.5020e-01 (2.4407e-01)	Acc@1  92.97 ( 91.62)	Acc@5 100.00 ( 99.83)
Epoch: [17][ 90/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.1899e-01 (2.4648e-01)	Acc@1  92.19 ( 91.47)	Acc@5 100.00 ( 99.84)
Epoch: [17][100/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.2613e-01 (2.4848e-01)	Acc@1  93.75 ( 91.36)	Acc@5 100.00 ( 99.84)
Epoch: [17][110/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.4568e-01 (2.5059e-01)	Acc@1  92.97 ( 91.31)	Acc@5 100.00 ( 99.82)
Epoch: [17][120/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1324e-01 (2.5040e-01)	Acc@1  91.41 ( 91.27)	Acc@5 100.00 ( 99.83)
Epoch: [17][130/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.7527e-01 (2.5066e-01)	Acc@1  88.28 ( 91.28)	Acc@5 100.00 ( 99.83)
Epoch: [17][140/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2224e-01 (2.4986e-01)	Acc@1  92.19 ( 91.29)	Acc@5  99.22 ( 99.83)
Epoch: [17][150/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.1307e-01 (2.5030e-01)	Acc@1  85.94 ( 91.27)	Acc@5 100.00 ( 99.84)
Epoch: [17][160/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.3922e-01 (2.5145e-01)	Acc@1  90.62 ( 91.24)	Acc@5 100.00 ( 99.84)
Epoch: [17][170/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.5009e-01 (2.5183e-01)	Acc@1  92.19 ( 91.23)	Acc@5 100.00 ( 99.83)
Epoch: [17][180/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0239e-01 (2.5282e-01)	Acc@1  90.62 ( 91.24)	Acc@5 100.00 ( 99.82)
Epoch: [17][190/391]	Time  0.145 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7206e-01 (2.5223e-01)	Acc@1  93.75 ( 91.29)	Acc@5  99.22 ( 99.82)
Epoch: [17][200/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.7008e-01 (2.5338e-01)	Acc@1  91.41 ( 91.20)	Acc@5 100.00 ( 99.83)
Epoch: [17][210/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.8165e-01 (2.5359e-01)	Acc@1  89.84 ( 91.19)	Acc@5 100.00 ( 99.82)
Epoch: [17][220/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.9851e-01 (2.5538e-01)	Acc@1  89.06 ( 91.11)	Acc@5 100.00 ( 99.82)
Epoch: [17][230/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.5295e-01 (2.5398e-01)	Acc@1  90.62 ( 91.16)	Acc@5 100.00 ( 99.83)
Epoch: [17][240/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.3212e-01 (2.5320e-01)	Acc@1  94.53 ( 91.23)	Acc@5  99.22 ( 99.83)
Epoch: [17][250/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.2889e-01 (2.5255e-01)	Acc@1  88.28 ( 91.24)	Acc@5 100.00 ( 99.83)
Epoch: [17][260/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3816e-01 (2.5121e-01)	Acc@1  93.75 ( 91.24)	Acc@5 100.00 ( 99.83)
Epoch: [17][270/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2773e-01 (2.5077e-01)	Acc@1  92.19 ( 91.23)	Acc@5 100.00 ( 99.83)
Epoch: [17][280/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.8683e-01 (2.5089e-01)	Acc@1  90.62 ( 91.26)	Acc@5 100.00 ( 99.83)
Epoch: [17][290/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0250e-01 (2.5078e-01)	Acc@1  92.19 ( 91.25)	Acc@5 100.00 ( 99.84)
Epoch: [17][300/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.5048e-01 (2.5141e-01)	Acc@1  89.84 ( 91.22)	Acc@5 100.00 ( 99.83)
Epoch: [17][310/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.3855e-01 (2.5151e-01)	Acc@1  89.84 ( 91.18)	Acc@5 100.00 ( 99.83)
Epoch: [17][320/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.5343e-01 (2.5243e-01)	Acc@1  89.06 ( 91.12)	Acc@5 100.00 ( 99.82)
Epoch: [17][330/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.4082e-01 (2.5291e-01)	Acc@1  89.84 ( 91.13)	Acc@5  99.22 ( 99.82)
Epoch: [17][340/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.8273e-01 (2.5253e-01)	Acc@1  87.50 ( 91.16)	Acc@5  99.22 ( 99.82)
Epoch: [17][350/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.3287e-01 (2.5211e-01)	Acc@1  92.19 ( 91.18)	Acc@5 100.00 ( 99.82)
Epoch: [17][360/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8142e-01 (2.5219e-01)	Acc@1  94.53 ( 91.18)	Acc@5  99.22 ( 99.82)
Epoch: [17][370/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.4808e-01 (2.5199e-01)	Acc@1  92.19 ( 91.19)	Acc@5 100.00 ( 99.82)
Epoch: [17][380/391]	Time  0.154 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2728e-01 (2.5224e-01)	Acc@1  92.19 ( 91.18)	Acc@5 100.00 ( 99.82)
Epoch: [17][390/391]	Time  0.114 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.9816e-01 (2.5229e-01)	Acc@1  88.75 ( 91.16)	Acc@5  98.75 ( 99.82)
## e[17] optimizer.zero_grad (sum) time: 0.6822795867919922
## e[17]       loss.backward (sum) time: 13.966986656188965
## e[17]      optimizer.step (sum) time: 14.66532015800476
## epoch[17] training(only) time: 54.035332679748535
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 3.7540e-01 (3.7540e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 2.7860e-01 (3.5238e-01)	Acc@1  92.00 ( 89.55)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 3.6197e-01 (3.6165e-01)	Acc@1  85.00 ( 88.43)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 3.8993e-01 (3.7211e-01)	Acc@1  88.00 ( 88.42)	Acc@5 100.00 ( 99.61)
Test: [ 40/100]	Time  0.052 ( 0.052)	Loss 4.8198e-01 (3.7397e-01)	Acc@1  86.00 ( 88.29)	Acc@5  98.00 ( 99.56)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 2.6840e-01 (3.7665e-01)	Acc@1  90.00 ( 88.04)	Acc@5 100.00 ( 99.57)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 2.7219e-01 (3.7255e-01)	Acc@1  89.00 ( 87.92)	Acc@5 100.00 ( 99.61)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 5.6769e-01 (3.7174e-01)	Acc@1  86.00 ( 87.89)	Acc@5 100.00 ( 99.65)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 3.0135e-01 (3.7438e-01)	Acc@1  89.00 ( 87.79)	Acc@5 100.00 ( 99.67)
Test: [ 90/100]	Time  0.048 ( 0.049)	Loss 2.1433e-01 (3.7008e-01)	Acc@1  95.00 ( 88.08)	Acc@5 100.00 ( 99.69)
 * Acc@1 88.030 Acc@5 99.710
### epoch[17] execution time: 59.047215938568115
EPOCH 18
# current learning rate: 0.1
# Switched to train mode...
Epoch: [18][  0/391]	Time  0.300 ( 0.300)	Data  0.157 ( 0.157)	Loss 2.0954e-01 (2.0954e-01)	Acc@1  89.84 ( 89.84)	Acc@5 100.00 (100.00)
Epoch: [18][ 10/391]	Time  0.139 ( 0.153)	Data  0.001 ( 0.015)	Loss 2.3450e-01 (2.0768e-01)	Acc@1  91.41 ( 92.33)	Acc@5 100.00 (100.00)
Epoch: [18][ 20/391]	Time  0.138 ( 0.146)	Data  0.001 ( 0.009)	Loss 2.0060e-01 (2.1076e-01)	Acc@1  90.62 ( 92.60)	Acc@5 100.00 ( 99.85)
Epoch: [18][ 30/391]	Time  0.140 ( 0.144)	Data  0.001 ( 0.006)	Loss 1.5987e-01 (2.0511e-01)	Acc@1  94.53 ( 92.67)	Acc@5 100.00 ( 99.85)
Epoch: [18][ 40/391]	Time  0.136 ( 0.143)	Data  0.001 ( 0.005)	Loss 1.7013e-01 (2.1255e-01)	Acc@1  93.75 ( 92.28)	Acc@5 100.00 ( 99.85)
Epoch: [18][ 50/391]	Time  0.136 ( 0.142)	Data  0.001 ( 0.004)	Loss 2.1878e-01 (2.0961e-01)	Acc@1  92.97 ( 92.40)	Acc@5 100.00 ( 99.86)
Epoch: [18][ 60/391]	Time  0.141 ( 0.141)	Data  0.001 ( 0.004)	Loss 2.0060e-01 (2.0547e-01)	Acc@1  91.41 ( 92.64)	Acc@5  99.22 ( 99.86)
Epoch: [18][ 70/391]	Time  0.139 ( 0.141)	Data  0.001 ( 0.003)	Loss 3.3703e-01 (2.0915e-01)	Acc@1  90.62 ( 92.46)	Acc@5 100.00 ( 99.87)
Epoch: [18][ 80/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.2900e-01 (2.0836e-01)	Acc@1  94.53 ( 92.53)	Acc@5  99.22 ( 99.86)
Epoch: [18][ 90/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.5221e-01 (2.1184e-01)	Acc@1  95.31 ( 92.51)	Acc@5 100.00 ( 99.87)
Epoch: [18][100/391]	Time  0.144 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.1480e-01 (2.1211e-01)	Acc@1  92.97 ( 92.50)	Acc@5 100.00 ( 99.88)
Epoch: [18][110/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.1558e-01 (2.1192e-01)	Acc@1  91.41 ( 92.47)	Acc@5 100.00 ( 99.89)
Epoch: [18][120/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.3898e-01 (2.1507e-01)	Acc@1  92.19 ( 92.36)	Acc@5 100.00 ( 99.88)
Epoch: [18][130/391]	Time  0.146 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.5727e-01 (2.1922e-01)	Acc@1  86.72 ( 92.19)	Acc@5  99.22 ( 99.87)
Epoch: [18][140/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.9427e-01 (2.2009e-01)	Acc@1  92.97 ( 92.17)	Acc@5  99.22 ( 99.87)
Epoch: [18][150/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.3885e-01 (2.2131e-01)	Acc@1  89.84 ( 92.12)	Acc@5  99.22 ( 99.88)
Epoch: [18][160/391]	Time  0.142 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.2756e-01 (2.2236e-01)	Acc@1  86.72 ( 92.12)	Acc@5  99.22 ( 99.87)
Epoch: [18][170/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.5954e-01 (2.2279e-01)	Acc@1  91.41 ( 92.13)	Acc@5 100.00 ( 99.86)
Epoch: [18][180/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.8948e-01 (2.2387e-01)	Acc@1  89.06 ( 92.08)	Acc@5 100.00 ( 99.87)
Epoch: [18][190/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1204e-01 (2.2476e-01)	Acc@1  92.19 ( 92.07)	Acc@5 100.00 ( 99.86)
Epoch: [18][200/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1662e-01 (2.2597e-01)	Acc@1  91.41 ( 92.03)	Acc@5 100.00 ( 99.85)
Epoch: [18][210/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.3011e-01 (2.2723e-01)	Acc@1  89.06 ( 92.01)	Acc@5 100.00 ( 99.85)
Epoch: [18][220/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.1123e-01 (2.2821e-01)	Acc@1  88.28 ( 91.98)	Acc@5  99.22 ( 99.84)
Epoch: [18][230/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0894e-01 (2.2889e-01)	Acc@1  89.84 ( 91.93)	Acc@5 100.00 ( 99.85)
Epoch: [18][240/391]	Time  0.146 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2149e-01 (2.2918e-01)	Acc@1  93.75 ( 91.95)	Acc@5  99.22 ( 99.85)
Epoch: [18][250/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.1134e-01 (2.3063e-01)	Acc@1  89.84 ( 91.92)	Acc@5 100.00 ( 99.84)
Epoch: [18][260/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9090e-01 (2.3094e-01)	Acc@1  90.62 ( 91.88)	Acc@5 100.00 ( 99.84)
Epoch: [18][270/391]	Time  0.144 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0101e-01 (2.3174e-01)	Acc@1  94.53 ( 91.87)	Acc@5  99.22 ( 99.83)
Epoch: [18][280/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1401e-01 (2.3201e-01)	Acc@1  91.41 ( 91.84)	Acc@5 100.00 ( 99.84)
Epoch: [18][290/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0339e-01 (2.3305e-01)	Acc@1  91.41 ( 91.83)	Acc@5 100.00 ( 99.83)
Epoch: [18][300/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.3890e-01 (2.3480e-01)	Acc@1  90.62 ( 91.74)	Acc@5 100.00 ( 99.83)
Epoch: [18][310/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.4657e-01 (2.3556e-01)	Acc@1  87.50 ( 91.71)	Acc@5 100.00 ( 99.83)
Epoch: [18][320/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1663e-01 (2.3598e-01)	Acc@1  92.97 ( 91.73)	Acc@5 100.00 ( 99.83)
Epoch: [18][330/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.3083e-01 (2.3611e-01)	Acc@1  88.28 ( 91.74)	Acc@5 100.00 ( 99.83)
Epoch: [18][340/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.2666e-01 (2.3607e-01)	Acc@1  88.28 ( 91.74)	Acc@5 100.00 ( 99.83)
Epoch: [18][350/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9250e-01 (2.3549e-01)	Acc@1  93.75 ( 91.75)	Acc@5 100.00 ( 99.83)
Epoch: [18][360/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2091e-01 (2.3628e-01)	Acc@1  92.19 ( 91.74)	Acc@5  99.22 ( 99.83)
Epoch: [18][370/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9516e-01 (2.3646e-01)	Acc@1  93.75 ( 91.73)	Acc@5 100.00 ( 99.83)
Epoch: [18][380/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0577e-01 (2.3660e-01)	Acc@1  93.75 ( 91.73)	Acc@5 100.00 ( 99.84)
Epoch: [18][390/391]	Time  0.109 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.5814e-01 (2.3609e-01)	Acc@1  92.50 ( 91.75)	Acc@5 100.00 ( 99.84)
## e[18] optimizer.zero_grad (sum) time: 0.6864666938781738
## e[18]       loss.backward (sum) time: 14.021810054779053
## e[18]      optimizer.step (sum) time: 14.624109506607056
## epoch[18] training(only) time: 54.04269862174988
# Switched to evaluate mode...
Test: [  0/100]	Time  0.197 ( 0.197)	Loss 4.7626e-01 (4.7626e-01)	Acc@1  87.00 ( 87.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.046 ( 0.062)	Loss 3.9566e-01 (3.7691e-01)	Acc@1  91.00 ( 88.27)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.048 ( 0.055)	Loss 4.3666e-01 (3.9232e-01)	Acc@1  83.00 ( 87.29)	Acc@5 100.00 ( 99.48)
Test: [ 30/100]	Time  0.055 ( 0.053)	Loss 4.4926e-01 (4.0086e-01)	Acc@1  87.00 ( 86.97)	Acc@5  99.00 ( 99.48)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 2.8699e-01 (3.9721e-01)	Acc@1  90.00 ( 87.34)	Acc@5 100.00 ( 99.49)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 3.8022e-01 (3.9482e-01)	Acc@1  86.00 ( 87.22)	Acc@5 100.00 ( 99.53)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 3.4533e-01 (3.9761e-01)	Acc@1  89.00 ( 87.08)	Acc@5 100.00 ( 99.59)
Test: [ 70/100]	Time  0.045 ( 0.049)	Loss 5.3277e-01 (3.9703e-01)	Acc@1  82.00 ( 87.21)	Acc@5 100.00 ( 99.56)
Test: [ 80/100]	Time  0.053 ( 0.049)	Loss 5.2179e-01 (3.9557e-01)	Acc@1  82.00 ( 87.21)	Acc@5 100.00 ( 99.58)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.4606e-01 (3.9246e-01)	Acc@1  93.00 ( 87.25)	Acc@5 100.00 ( 99.63)
 * Acc@1 87.230 Acc@5 99.640
### epoch[18] execution time: 59.01695227622986
EPOCH 19
# current learning rate: 0.1
# Switched to train mode...
Epoch: [19][  0/391]	Time  0.292 ( 0.292)	Data  0.150 ( 0.150)	Loss 4.1559e-01 (4.1559e-01)	Acc@1  86.72 ( 86.72)	Acc@5  98.44 ( 98.44)
Epoch: [19][ 10/391]	Time  0.140 ( 0.151)	Data  0.001 ( 0.015)	Loss 2.4115e-01 (2.5580e-01)	Acc@1  90.62 ( 90.77)	Acc@5 100.00 ( 99.72)
Epoch: [19][ 20/391]	Time  0.138 ( 0.145)	Data  0.001 ( 0.008)	Loss 2.4304e-01 (2.4362e-01)	Acc@1  92.19 ( 91.52)	Acc@5 100.00 ( 99.81)
Epoch: [19][ 30/391]	Time  0.135 ( 0.143)	Data  0.001 ( 0.006)	Loss 1.8128e-01 (2.3503e-01)	Acc@1  92.19 ( 91.86)	Acc@5 100.00 ( 99.82)
Epoch: [19][ 40/391]	Time  0.138 ( 0.142)	Data  0.001 ( 0.005)	Loss 1.6937e-01 (2.3212e-01)	Acc@1  92.19 ( 91.88)	Acc@5 100.00 ( 99.85)
Epoch: [19][ 50/391]	Time  0.138 ( 0.141)	Data  0.001 ( 0.004)	Loss 2.0029e-01 (2.3123e-01)	Acc@1  92.97 ( 91.99)	Acc@5 100.00 ( 99.85)
Epoch: [19][ 60/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.4695e-01 (2.3066e-01)	Acc@1  92.19 ( 91.98)	Acc@5 100.00 ( 99.82)
Epoch: [19][ 70/391]	Time  0.147 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.5416e-01 (2.2888e-01)	Acc@1  91.41 ( 91.97)	Acc@5  98.44 ( 99.82)
Epoch: [19][ 80/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.0291e-01 (2.2827e-01)	Acc@1  92.19 ( 91.96)	Acc@5 100.00 ( 99.83)
Epoch: [19][ 90/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.9998e-01 (2.2667e-01)	Acc@1  92.97 ( 91.97)	Acc@5 100.00 ( 99.84)
Epoch: [19][100/391]	Time  0.148 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.7172e-01 (2.2826e-01)	Acc@1  95.31 ( 91.94)	Acc@5 100.00 ( 99.83)
Epoch: [19][110/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.9458e-01 (2.2744e-01)	Acc@1  94.53 ( 91.99)	Acc@5  99.22 ( 99.82)
Epoch: [19][120/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8889e-01 (2.2482e-01)	Acc@1  90.62 ( 92.11)	Acc@5 100.00 ( 99.84)
Epoch: [19][130/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.5541e-01 (2.2612e-01)	Acc@1  92.97 ( 92.07)	Acc@5 100.00 ( 99.83)
Epoch: [19][140/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.3955e-01 (2.2611e-01)	Acc@1  89.84 ( 92.04)	Acc@5  99.22 ( 99.82)
Epoch: [19][150/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1610e-01 (2.2742e-01)	Acc@1  91.41 ( 91.92)	Acc@5 100.00 ( 99.83)
Epoch: [19][160/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.3323e-01 (2.2669e-01)	Acc@1  92.19 ( 91.95)	Acc@5 100.00 ( 99.84)
Epoch: [19][170/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0015e-01 (2.2693e-01)	Acc@1  93.75 ( 91.98)	Acc@5 100.00 ( 99.84)
Epoch: [19][180/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.5805e-01 (2.2838e-01)	Acc@1  92.19 ( 91.96)	Acc@5 100.00 ( 99.84)
Epoch: [19][190/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2682e-01 (2.2883e-01)	Acc@1  90.62 ( 91.94)	Acc@5 100.00 ( 99.83)
Epoch: [19][200/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5308e-01 (2.2751e-01)	Acc@1  94.53 ( 91.98)	Acc@5 100.00 ( 99.84)
Epoch: [19][210/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.9716e-01 (2.2759e-01)	Acc@1  92.97 ( 91.98)	Acc@5 100.00 ( 99.84)
Epoch: [19][220/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6103e-01 (2.2674e-01)	Acc@1  94.53 ( 92.00)	Acc@5 100.00 ( 99.84)
Epoch: [19][230/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.7128e-02 (2.2625e-01)	Acc@1  98.44 ( 92.01)	Acc@5 100.00 ( 99.85)
Epoch: [19][240/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4815e-01 (2.2613e-01)	Acc@1  95.31 ( 92.02)	Acc@5 100.00 ( 99.86)
Epoch: [19][250/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.7279e-01 (2.2468e-01)	Acc@1  89.84 ( 92.06)	Acc@5 100.00 ( 99.86)
Epoch: [19][260/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9897e-01 (2.2445e-01)	Acc@1  94.53 ( 92.06)	Acc@5 100.00 ( 99.86)
Epoch: [19][270/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.4345e-01 (2.2471e-01)	Acc@1  90.62 ( 92.04)	Acc@5 100.00 ( 99.86)
Epoch: [19][280/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4988e-01 (2.2503e-01)	Acc@1  95.31 ( 92.04)	Acc@5 100.00 ( 99.87)
Epoch: [19][290/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.5743e-01 (2.2596e-01)	Acc@1  90.62 ( 91.99)	Acc@5 100.00 ( 99.87)
Epoch: [19][300/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9712e-01 (2.2633e-01)	Acc@1  92.97 ( 91.97)	Acc@5 100.00 ( 99.87)
Epoch: [19][310/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0585e-01 (2.2666e-01)	Acc@1  92.19 ( 91.97)	Acc@5 100.00 ( 99.87)
Epoch: [19][320/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.6907e-01 (2.2748e-01)	Acc@1  85.94 ( 91.97)	Acc@5  97.66 ( 99.86)
Epoch: [19][330/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.9630e-01 (2.2742e-01)	Acc@1  90.62 ( 91.99)	Acc@5  99.22 ( 99.86)
Epoch: [19][340/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.6973e-01 (2.2729e-01)	Acc@1  89.84 ( 92.00)	Acc@5 100.00 ( 99.86)
Epoch: [19][350/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.0946e-01 (2.2841e-01)	Acc@1  84.38 ( 91.97)	Acc@5  99.22 ( 99.86)
Epoch: [19][360/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.1424e-01 (2.2848e-01)	Acc@1  90.62 ( 91.98)	Acc@5  99.22 ( 99.85)
Epoch: [19][370/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4568e-01 (2.2884e-01)	Acc@1  94.53 ( 91.96)	Acc@5 100.00 ( 99.85)
Epoch: [19][380/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.4614e-01 (2.2912e-01)	Acc@1  89.84 ( 91.94)	Acc@5 100.00 ( 99.86)
Epoch: [19][390/391]	Time  0.112 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7032e-01 (2.3012e-01)	Acc@1  96.25 ( 91.93)	Acc@5 100.00 ( 99.86)
## e[19] optimizer.zero_grad (sum) time: 0.6847825050354004
## e[19]       loss.backward (sum) time: 13.950985670089722
## e[19]      optimizer.step (sum) time: 14.664564847946167
## epoch[19] training(only) time: 54.06898236274719
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 3.9514e-01 (3.9514e-01)	Acc@1  89.00 ( 89.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.046 ( 0.062)	Loss 4.4992e-01 (4.0338e-01)	Acc@1  87.00 ( 86.82)	Acc@5  99.00 ( 99.27)
Test: [ 20/100]	Time  0.047 ( 0.056)	Loss 4.7404e-01 (4.1034e-01)	Acc@1  84.00 ( 86.48)	Acc@5  99.00 ( 99.24)
Test: [ 30/100]	Time  0.046 ( 0.054)	Loss 6.2196e-01 (4.2289e-01)	Acc@1  80.00 ( 86.29)	Acc@5  98.00 ( 99.35)
Test: [ 40/100]	Time  0.046 ( 0.052)	Loss 3.9579e-01 (4.0978e-01)	Acc@1  87.00 ( 86.39)	Acc@5  98.00 ( 99.32)
Test: [ 50/100]	Time  0.048 ( 0.051)	Loss 2.4492e-01 (4.0086e-01)	Acc@1  92.00 ( 86.69)	Acc@5 100.00 ( 99.35)
Test: [ 60/100]	Time  0.048 ( 0.051)	Loss 2.7246e-01 (3.9629e-01)	Acc@1  92.00 ( 86.77)	Acc@5 100.00 ( 99.43)
Test: [ 70/100]	Time  0.046 ( 0.051)	Loss 3.5844e-01 (3.9366e-01)	Acc@1  85.00 ( 86.92)	Acc@5 100.00 ( 99.41)
Test: [ 80/100]	Time  0.049 ( 0.050)	Loss 4.6906e-01 (3.9474e-01)	Acc@1  86.00 ( 86.91)	Acc@5  99.00 ( 99.44)
Test: [ 90/100]	Time  0.046 ( 0.050)	Loss 2.4303e-01 (3.8954e-01)	Acc@1  91.00 ( 87.05)	Acc@5 100.00 ( 99.46)
 * Acc@1 87.080 Acc@5 99.490
### epoch[19] execution time: 59.14142823219299
EPOCH 20
# current learning rate: 0.1
# Switched to train mode...
Epoch: [20][  0/391]	Time  0.301 ( 0.301)	Data  0.152 ( 0.152)	Loss 2.2847e-01 (2.2847e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
Epoch: [20][ 10/391]	Time  0.137 ( 0.154)	Data  0.001 ( 0.015)	Loss 1.5351e-01 (2.4326e-01)	Acc@1  94.53 ( 91.90)	Acc@5 100.00 ( 99.86)
Epoch: [20][ 20/391]	Time  0.140 ( 0.147)	Data  0.001 ( 0.008)	Loss 2.5203e-01 (2.1704e-01)	Acc@1  92.97 ( 92.60)	Acc@5 100.00 ( 99.89)
Epoch: [20][ 30/391]	Time  0.134 ( 0.144)	Data  0.001 ( 0.006)	Loss 1.2568e-01 (2.1647e-01)	Acc@1  96.88 ( 92.41)	Acc@5  99.22 ( 99.85)
Epoch: [20][ 40/391]	Time  0.134 ( 0.142)	Data  0.001 ( 0.005)	Loss 2.8553e-01 (2.1436e-01)	Acc@1  92.97 ( 92.66)	Acc@5  99.22 ( 99.85)
Epoch: [20][ 50/391]	Time  0.138 ( 0.141)	Data  0.001 ( 0.004)	Loss 2.8156e-01 (2.1397e-01)	Acc@1  88.28 ( 92.43)	Acc@5 100.00 ( 99.86)
Epoch: [20][ 60/391]	Time  0.135 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.8502e-01 (2.1050e-01)	Acc@1  92.97 ( 92.46)	Acc@5 100.00 ( 99.87)
Epoch: [20][ 70/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.3261e-01 (2.0494e-01)	Acc@1  96.09 ( 92.67)	Acc@5 100.00 ( 99.88)
Epoch: [20][ 80/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 3.0490e-01 (2.1000e-01)	Acc@1  89.84 ( 92.55)	Acc@5  99.22 ( 99.88)
Epoch: [20][ 90/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.8136e-01 (2.1249e-01)	Acc@1  93.75 ( 92.42)	Acc@5 100.00 ( 99.86)
Epoch: [20][100/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 3.4649e-01 (2.1497e-01)	Acc@1  87.50 ( 92.32)	Acc@5 100.00 ( 99.86)
Epoch: [20][110/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.8704e-01 (2.1595e-01)	Acc@1  92.97 ( 92.31)	Acc@5 100.00 ( 99.87)
Epoch: [20][120/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5410e-01 (2.1601e-01)	Acc@1  95.31 ( 92.31)	Acc@5  99.22 ( 99.86)
Epoch: [20][130/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2927e-01 (2.1700e-01)	Acc@1  92.97 ( 92.27)	Acc@5  99.22 ( 99.86)
Epoch: [20][140/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.5205e-01 (2.1534e-01)	Acc@1  89.84 ( 92.34)	Acc@5 100.00 ( 99.87)
Epoch: [20][150/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2658e-01 (2.1423e-01)	Acc@1  96.09 ( 92.37)	Acc@5 100.00 ( 99.88)
Epoch: [20][160/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.0387e-01 (2.1333e-01)	Acc@1  90.62 ( 92.40)	Acc@5 100.00 ( 99.88)
Epoch: [20][170/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4021e-01 (2.1347e-01)	Acc@1  96.09 ( 92.43)	Acc@5 100.00 ( 99.88)
Epoch: [20][180/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.7524e-01 (2.1463e-01)	Acc@1  89.84 ( 92.40)	Acc@5 100.00 ( 99.87)
Epoch: [20][190/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.4392e-01 (2.1444e-01)	Acc@1  91.41 ( 92.41)	Acc@5 100.00 ( 99.87)
Epoch: [20][200/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6235e-01 (2.1430e-01)	Acc@1  96.09 ( 92.40)	Acc@5 100.00 ( 99.88)
Epoch: [20][210/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.6336e-01 (2.1628e-01)	Acc@1  89.84 ( 92.35)	Acc@5 100.00 ( 99.87)
Epoch: [20][220/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.3082e-01 (2.1755e-01)	Acc@1  92.97 ( 92.33)	Acc@5  99.22 ( 99.86)
Epoch: [20][230/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2808e-01 (2.1827e-01)	Acc@1  90.62 ( 92.32)	Acc@5 100.00 ( 99.86)
Epoch: [20][240/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7867e-01 (2.2068e-01)	Acc@1  92.97 ( 92.24)	Acc@5 100.00 ( 99.86)
Epoch: [20][250/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1888e-01 (2.2020e-01)	Acc@1  91.41 ( 92.27)	Acc@5 100.00 ( 99.86)
Epoch: [20][260/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.6545e-01 (2.1981e-01)	Acc@1  90.62 ( 92.30)	Acc@5 100.00 ( 99.87)
Epoch: [20][270/391]	Time  0.147 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.0424e-01 (2.2065e-01)	Acc@1  89.84 ( 92.26)	Acc@5 100.00 ( 99.87)
Epoch: [20][280/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4822e-01 (2.2101e-01)	Acc@1  94.53 ( 92.24)	Acc@5 100.00 ( 99.87)
Epoch: [20][290/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1516e-01 (2.2196e-01)	Acc@1  92.19 ( 92.20)	Acc@5 100.00 ( 99.87)
Epoch: [20][300/391]	Time  0.146 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.4729e-01 (2.2207e-01)	Acc@1  89.06 ( 92.18)	Acc@5 100.00 ( 99.87)
Epoch: [20][310/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2729e-01 (2.2154e-01)	Acc@1  91.41 ( 92.21)	Acc@5 100.00 ( 99.87)
Epoch: [20][320/391]	Time  0.143 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6639e-01 (2.2227e-01)	Acc@1  96.09 ( 92.20)	Acc@5 100.00 ( 99.86)
Epoch: [20][330/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.7983e-01 (2.2310e-01)	Acc@1  89.06 ( 92.16)	Acc@5 100.00 ( 99.87)
Epoch: [20][340/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.3303e-01 (2.2454e-01)	Acc@1  85.94 ( 92.10)	Acc@5 100.00 ( 99.86)
Epoch: [20][350/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0765e-01 (2.2590e-01)	Acc@1  90.62 ( 92.07)	Acc@5 100.00 ( 99.86)
Epoch: [20][360/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.8459e-02 (2.2569e-01)	Acc@1  97.66 ( 92.08)	Acc@5 100.00 ( 99.86)
Epoch: [20][370/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4551e-01 (2.2524e-01)	Acc@1  96.09 ( 92.09)	Acc@5 100.00 ( 99.86)
Epoch: [20][380/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1715e-01 (2.2510e-01)	Acc@1  92.19 ( 92.10)	Acc@5 100.00 ( 99.86)
Epoch: [20][390/391]	Time  0.110 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.4238e-01 (2.2454e-01)	Acc@1  88.75 ( 92.14)	Acc@5 100.00 ( 99.86)
## e[20] optimizer.zero_grad (sum) time: 0.6873757839202881
## e[20]       loss.backward (sum) time: 13.964635133743286
## e[20]      optimizer.step (sum) time: 14.643409967422485
## epoch[20] training(only) time: 54.109278202056885
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 2.7556e-01 (2.7556e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.048 ( 0.060)	Loss 3.6118e-01 (3.4071e-01)	Acc@1  89.00 ( 88.09)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 4.0555e-01 (3.7686e-01)	Acc@1  85.00 ( 87.38)	Acc@5  99.00 ( 99.57)
Test: [ 30/100]	Time  0.056 ( 0.052)	Loss 4.6199e-01 (3.7411e-01)	Acc@1  85.00 ( 87.94)	Acc@5  99.00 ( 99.58)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 3.2845e-01 (3.6033e-01)	Acc@1  90.00 ( 88.39)	Acc@5  99.00 ( 99.59)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 2.7282e-01 (3.6249e-01)	Acc@1  91.00 ( 88.37)	Acc@5 100.00 ( 99.59)
Test: [ 60/100]	Time  0.048 ( 0.050)	Loss 2.8959e-01 (3.6020e-01)	Acc@1  90.00 ( 88.13)	Acc@5 100.00 ( 99.64)
Test: [ 70/100]	Time  0.050 ( 0.050)	Loss 2.8207e-01 (3.5773e-01)	Acc@1  90.00 ( 88.23)	Acc@5 100.00 ( 99.63)
Test: [ 80/100]	Time  0.060 ( 0.050)	Loss 2.4410e-01 (3.5749e-01)	Acc@1  90.00 ( 88.16)	Acc@5 100.00 ( 99.68)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 2.6844e-01 (3.5587e-01)	Acc@1  89.00 ( 88.22)	Acc@5 100.00 ( 99.69)
 * Acc@1 88.350 Acc@5 99.710
### epoch[20] execution time: 59.116472482681274
EPOCH 21
# current learning rate: 0.1
# Switched to train mode...
Epoch: [21][  0/391]	Time  0.295 ( 0.295)	Data  0.147 ( 0.147)	Loss 2.5081e-01 (2.5081e-01)	Acc@1  88.28 ( 88.28)	Acc@5 100.00 (100.00)
Epoch: [21][ 10/391]	Time  0.134 ( 0.151)	Data  0.001 ( 0.014)	Loss 2.4162e-01 (2.0275e-01)	Acc@1  89.84 ( 92.83)	Acc@5 100.00 (100.00)
Epoch: [21][ 20/391]	Time  0.138 ( 0.145)	Data  0.001 ( 0.008)	Loss 2.0534e-01 (2.0225e-01)	Acc@1  92.19 ( 92.86)	Acc@5 100.00 (100.00)
Epoch: [21][ 30/391]	Time  0.137 ( 0.143)	Data  0.001 ( 0.006)	Loss 1.9163e-01 (1.9716e-01)	Acc@1  95.31 ( 92.77)	Acc@5 100.00 (100.00)
Epoch: [21][ 40/391]	Time  0.140 ( 0.142)	Data  0.001 ( 0.005)	Loss 2.2042e-01 (2.0453e-01)	Acc@1  91.41 ( 92.53)	Acc@5 100.00 ( 99.94)
Epoch: [21][ 50/391]	Time  0.139 ( 0.141)	Data  0.001 ( 0.004)	Loss 2.0228e-01 (1.9881e-01)	Acc@1  93.75 ( 92.77)	Acc@5 100.00 ( 99.95)
Epoch: [21][ 60/391]	Time  0.139 ( 0.141)	Data  0.001 ( 0.004)	Loss 2.5480e-01 (1.9758e-01)	Acc@1  92.19 ( 92.85)	Acc@5 100.00 ( 99.94)
Epoch: [21][ 70/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.6852e-01 (1.9857e-01)	Acc@1  90.62 ( 92.85)	Acc@5 100.00 ( 99.92)
Epoch: [21][ 80/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.8775e-01 (1.9997e-01)	Acc@1  93.75 ( 92.78)	Acc@5 100.00 ( 99.91)
Epoch: [21][ 90/391]	Time  0.140 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.6643e-01 (2.0029e-01)	Acc@1  93.75 ( 92.81)	Acc@5  99.22 ( 99.90)
Epoch: [21][100/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.7320e-01 (2.0072e-01)	Acc@1  92.97 ( 92.81)	Acc@5 100.00 ( 99.91)
Epoch: [21][110/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.2347e-01 (2.0402e-01)	Acc@1  88.28 ( 92.66)	Acc@5  99.22 ( 99.90)
Epoch: [21][120/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1769e-01 (2.0654e-01)	Acc@1  91.41 ( 92.51)	Acc@5 100.00 ( 99.91)
Epoch: [21][130/391]	Time  0.150 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.7454e-01 (2.0809e-01)	Acc@1  88.28 ( 92.47)	Acc@5 100.00 ( 99.92)
Epoch: [21][140/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5276e-01 (2.0855e-01)	Acc@1  94.53 ( 92.45)	Acc@5 100.00 ( 99.91)
Epoch: [21][150/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.9645e-01 (2.1069e-01)	Acc@1  89.84 ( 92.36)	Acc@5 100.00 ( 99.90)
Epoch: [21][160/391]	Time  0.148 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.6279e-01 (2.0953e-01)	Acc@1  92.19 ( 92.43)	Acc@5 100.00 ( 99.90)
Epoch: [21][170/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2162e-01 (2.0953e-01)	Acc@1  92.97 ( 92.43)	Acc@5 100.00 ( 99.89)
Epoch: [21][180/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.3891e-01 (2.0927e-01)	Acc@1  90.62 ( 92.42)	Acc@5 100.00 ( 99.90)
Epoch: [21][190/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.5363e-01 (2.1060e-01)	Acc@1  91.41 ( 92.43)	Acc@5  99.22 ( 99.89)
Epoch: [21][200/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1148e-01 (2.1035e-01)	Acc@1  93.75 ( 92.46)	Acc@5 100.00 ( 99.90)
Epoch: [21][210/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.4903e-02 (2.0938e-01)	Acc@1  96.88 ( 92.51)	Acc@5 100.00 ( 99.89)
Epoch: [21][220/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.4874e-01 (2.0989e-01)	Acc@1  90.62 ( 92.51)	Acc@5 100.00 ( 99.89)
Epoch: [21][230/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.3529e-01 (2.1101e-01)	Acc@1  88.28 ( 92.44)	Acc@5 100.00 ( 99.89)
Epoch: [21][240/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.9711e-01 (2.1322e-01)	Acc@1  89.84 ( 92.38)	Acc@5 100.00 ( 99.89)
Epoch: [21][250/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2237e-01 (2.1333e-01)	Acc@1  93.75 ( 92.36)	Acc@5 100.00 ( 99.89)
Epoch: [21][260/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7726e-01 (2.1314e-01)	Acc@1  94.53 ( 92.39)	Acc@5 100.00 ( 99.89)
Epoch: [21][270/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9109e-01 (2.1259e-01)	Acc@1  91.41 ( 92.43)	Acc@5 100.00 ( 99.89)
Epoch: [21][280/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.3359e-01 (2.1257e-01)	Acc@1  92.19 ( 92.44)	Acc@5  99.22 ( 99.89)
Epoch: [21][290/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4476e-01 (2.1273e-01)	Acc@1  93.75 ( 92.45)	Acc@5 100.00 ( 99.89)
Epoch: [21][300/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.8632e-01 (2.1301e-01)	Acc@1  92.19 ( 92.45)	Acc@5 100.00 ( 99.89)
Epoch: [21][310/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.9137e-01 (2.1335e-01)	Acc@1  89.06 ( 92.42)	Acc@5 100.00 ( 99.89)
Epoch: [21][320/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.3466e-01 (2.1350e-01)	Acc@1  92.97 ( 92.44)	Acc@5 100.00 ( 99.89)
Epoch: [21][330/391]	Time  0.144 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0847e-01 (2.1371e-01)	Acc@1  97.66 ( 92.42)	Acc@5 100.00 ( 99.89)
Epoch: [21][340/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6252e-01 (2.1405e-01)	Acc@1  92.19 ( 92.41)	Acc@5 100.00 ( 99.89)
Epoch: [21][350/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.6860e-01 (2.1420e-01)	Acc@1  85.94 ( 92.40)	Acc@5 100.00 ( 99.89)
Epoch: [21][360/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.7158e-01 (2.1412e-01)	Acc@1  91.41 ( 92.42)	Acc@5 100.00 ( 99.89)
Epoch: [21][370/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9435e-01 (2.1356e-01)	Acc@1  93.75 ( 92.43)	Acc@5 100.00 ( 99.90)
Epoch: [21][380/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1826e-01 (2.1371e-01)	Acc@1  90.62 ( 92.41)	Acc@5 100.00 ( 99.90)
Epoch: [21][390/391]	Time  0.106 ( 0.138)	Data  0.001 ( 0.001)	Loss 1.9708e-01 (2.1356e-01)	Acc@1  92.50 ( 92.42)	Acc@5 100.00 ( 99.90)
## e[21] optimizer.zero_grad (sum) time: 0.6914727687835693
## e[21]       loss.backward (sum) time: 13.99279236793518
## e[21]      optimizer.step (sum) time: 14.607070207595825
## epoch[21] training(only) time: 54.0539391040802
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 3.1996e-01 (3.1996e-01)	Acc@1  91.00 ( 91.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.047 ( 0.060)	Loss 3.6755e-01 (3.3739e-01)	Acc@1  87.00 ( 89.18)	Acc@5 100.00 ( 99.45)
Test: [ 20/100]	Time  0.048 ( 0.054)	Loss 4.1154e-01 (3.4854e-01)	Acc@1  91.00 ( 88.95)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.048 ( 0.052)	Loss 3.5748e-01 (3.5934e-01)	Acc@1  90.00 ( 88.87)	Acc@5  99.00 ( 99.48)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 3.4786e-01 (3.6139e-01)	Acc@1  85.00 ( 88.61)	Acc@5  99.00 ( 99.51)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.7632e-01 (3.6119e-01)	Acc@1  95.00 ( 88.63)	Acc@5 100.00 ( 99.47)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 4.6254e-01 (3.6122e-01)	Acc@1  84.00 ( 88.44)	Acc@5 100.00 ( 99.51)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 4.3431e-01 (3.6707e-01)	Acc@1  87.00 ( 88.28)	Acc@5 100.00 ( 99.51)
Test: [ 80/100]	Time  0.048 ( 0.049)	Loss 2.4148e-01 (3.7178e-01)	Acc@1  91.00 ( 88.07)	Acc@5 100.00 ( 99.54)
Test: [ 90/100]	Time  0.048 ( 0.049)	Loss 1.8741e-01 (3.7171e-01)	Acc@1  92.00 ( 88.08)	Acc@5 100.00 ( 99.54)
 * Acc@1 88.200 Acc@5 99.570
### epoch[21] execution time: 59.05717349052429
EPOCH 22
# current learning rate: 0.1
# Switched to train mode...
Epoch: [22][  0/391]	Time  0.302 ( 0.302)	Data  0.131 ( 0.131)	Loss 1.5293e-01 (1.5293e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [22][ 10/391]	Time  0.137 ( 0.153)	Data  0.001 ( 0.013)	Loss 1.6055e-01 (1.6437e-01)	Acc@1  92.97 ( 94.18)	Acc@5 100.00 ( 99.93)
Epoch: [22][ 20/391]	Time  0.134 ( 0.145)	Data  0.001 ( 0.007)	Loss 1.6081e-01 (1.6660e-01)	Acc@1  92.19 ( 93.97)	Acc@5 100.00 ( 99.93)
Epoch: [22][ 30/391]	Time  0.135 ( 0.142)	Data  0.001 ( 0.005)	Loss 1.5064e-01 (1.7011e-01)	Acc@1  93.75 ( 93.83)	Acc@5 100.00 ( 99.90)
Epoch: [22][ 40/391]	Time  0.136 ( 0.141)	Data  0.001 ( 0.004)	Loss 2.6492e-01 (1.7669e-01)	Acc@1  91.41 ( 93.73)	Acc@5 100.00 ( 99.87)
Epoch: [22][ 50/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.004)	Loss 1.8299e-01 (1.7998e-01)	Acc@1  91.41 ( 93.67)	Acc@5 100.00 ( 99.86)
Epoch: [22][ 60/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.4862e-01 (1.8293e-01)	Acc@1  90.62 ( 93.62)	Acc@5 100.00 ( 99.87)
Epoch: [22][ 70/391]	Time  0.145 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.5662e-01 (1.7983e-01)	Acc@1  96.09 ( 93.79)	Acc@5 100.00 ( 99.87)
Epoch: [22][ 80/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.4235e-01 (1.8069e-01)	Acc@1  89.84 ( 93.67)	Acc@5 100.00 ( 99.88)
Epoch: [22][ 90/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.6000e-01 (1.8123e-01)	Acc@1  92.19 ( 93.66)	Acc@5 100.00 ( 99.90)
Epoch: [22][100/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.3058e-01 (1.8729e-01)	Acc@1  91.41 ( 93.43)	Acc@5 100.00 ( 99.89)
Epoch: [22][110/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.4328e-01 (1.8841e-01)	Acc@1  92.19 ( 93.42)	Acc@5 100.00 ( 99.89)
Epoch: [22][120/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0354e-01 (1.8782e-01)	Acc@1  92.97 ( 93.49)	Acc@5 100.00 ( 99.88)
Epoch: [22][130/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2061e-01 (1.8835e-01)	Acc@1  91.41 ( 93.45)	Acc@5 100.00 ( 99.88)
Epoch: [22][140/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6126e-01 (1.9023e-01)	Acc@1  92.97 ( 93.36)	Acc@5 100.00 ( 99.88)
Epoch: [22][150/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.3601e-01 (1.8946e-01)	Acc@1  93.75 ( 93.42)	Acc@5 100.00 ( 99.88)
Epoch: [22][160/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7175e-01 (1.8912e-01)	Acc@1  93.75 ( 93.42)	Acc@5 100.00 ( 99.88)
Epoch: [22][170/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8179e-01 (1.8920e-01)	Acc@1  94.53 ( 93.44)	Acc@5 100.00 ( 99.89)
Epoch: [22][180/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1669e-01 (1.9077e-01)	Acc@1  92.19 ( 93.36)	Acc@5 100.00 ( 99.89)
Epoch: [22][190/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1960e-01 (1.9185e-01)	Acc@1  93.75 ( 93.34)	Acc@5 100.00 ( 99.89)
Epoch: [22][200/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.3213e-01 (1.9300e-01)	Acc@1  85.94 ( 93.26)	Acc@5 100.00 ( 99.90)
Epoch: [22][210/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2579e-01 (1.9327e-01)	Acc@1  90.62 ( 93.26)	Acc@5 100.00 ( 99.89)
Epoch: [22][220/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4535e-01 (1.9406e-01)	Acc@1  96.88 ( 93.24)	Acc@5 100.00 ( 99.88)
Epoch: [22][230/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1791e-01 (1.9441e-01)	Acc@1  96.09 ( 93.25)	Acc@5 100.00 ( 99.89)
Epoch: [22][240/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2734e-01 (1.9470e-01)	Acc@1  96.09 ( 93.24)	Acc@5 100.00 ( 99.88)
Epoch: [22][250/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8873e-01 (1.9430e-01)	Acc@1  95.31 ( 93.29)	Acc@5  99.22 ( 99.88)
Epoch: [22][260/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8138e-01 (1.9436e-01)	Acc@1  92.97 ( 93.26)	Acc@5 100.00 ( 99.89)
Epoch: [22][270/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.6470e-01 (1.9459e-01)	Acc@1  86.72 ( 93.23)	Acc@5 100.00 ( 99.89)
Epoch: [22][280/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0316e-01 (1.9470e-01)	Acc@1  92.97 ( 93.22)	Acc@5 100.00 ( 99.89)
Epoch: [22][290/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9015e-01 (1.9528e-01)	Acc@1  92.97 ( 93.19)	Acc@5 100.00 ( 99.89)
Epoch: [22][300/391]	Time  0.149 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.5115e-01 (1.9626e-01)	Acc@1  89.84 ( 93.14)	Acc@5 100.00 ( 99.89)
Epoch: [22][310/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.5968e-01 (1.9689e-01)	Acc@1  89.84 ( 93.12)	Acc@5 100.00 ( 99.89)
Epoch: [22][320/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2992e-01 (1.9830e-01)	Acc@1  90.62 ( 93.07)	Acc@5 100.00 ( 99.88)
Epoch: [22][330/391]	Time  0.147 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0369e-01 (1.9922e-01)	Acc@1  92.97 ( 93.05)	Acc@5  99.22 ( 99.88)
Epoch: [22][340/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9085e-01 (1.9963e-01)	Acc@1  93.75 ( 93.06)	Acc@5 100.00 ( 99.88)
Epoch: [22][350/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0678e-01 (1.9987e-01)	Acc@1  92.97 ( 93.04)	Acc@5 100.00 ( 99.88)
Epoch: [22][360/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0535e-01 (2.0079e-01)	Acc@1  92.19 ( 93.01)	Acc@5 100.00 ( 99.88)
Epoch: [22][370/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0547e-01 (2.0140e-01)	Acc@1  93.75 ( 93.00)	Acc@5 100.00 ( 99.88)
Epoch: [22][380/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7470e-01 (2.0225e-01)	Acc@1  95.31 ( 92.98)	Acc@5 100.00 ( 99.88)
Epoch: [22][390/391]	Time  0.109 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0555e-01 (2.0286e-01)	Acc@1  92.50 ( 92.98)	Acc@5 100.00 ( 99.88)
## e[22] optimizer.zero_grad (sum) time: 0.6852247714996338
## e[22]       loss.backward (sum) time: 13.93474292755127
## e[22]      optimizer.step (sum) time: 14.638408899307251
## epoch[22] training(only) time: 53.99002242088318
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 4.1839e-01 (4.1839e-01)	Acc@1  90.00 ( 90.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 4.0520e-01 (3.9872e-01)	Acc@1  86.00 ( 87.45)	Acc@5 100.00 ( 99.55)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 4.7281e-01 (4.1583e-01)	Acc@1  83.00 ( 86.81)	Acc@5 100.00 ( 99.52)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 5.3291e-01 (4.2873e-01)	Acc@1  84.00 ( 86.74)	Acc@5 100.00 ( 99.55)
Test: [ 40/100]	Time  0.047 ( 0.052)	Loss 2.6800e-01 (4.3113e-01)	Acc@1  89.00 ( 86.61)	Acc@5 100.00 ( 99.56)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 3.2498e-01 (4.2216e-01)	Acc@1  88.00 ( 86.86)	Acc@5 100.00 ( 99.53)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 3.1026e-01 (4.1378e-01)	Acc@1  88.00 ( 86.84)	Acc@5 100.00 ( 99.56)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 3.8719e-01 (4.1595e-01)	Acc@1  87.00 ( 86.80)	Acc@5 100.00 ( 99.56)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 3.4138e-01 (4.1669e-01)	Acc@1  88.00 ( 86.68)	Acc@5 100.00 ( 99.57)
Test: [ 90/100]	Time  0.050 ( 0.050)	Loss 2.7970e-01 (4.1269e-01)	Acc@1  90.00 ( 86.85)	Acc@5 100.00 ( 99.56)
 * Acc@1 87.070 Acc@5 99.590
### epoch[22] execution time: 59.02771496772766
EPOCH 23
# current learning rate: 0.1
# Switched to train mode...
Epoch: [23][  0/391]	Time  0.296 ( 0.296)	Data  0.154 ( 0.154)	Loss 1.7798e-01 (1.7798e-01)	Acc@1  92.19 ( 92.19)	Acc@5 100.00 (100.00)
Epoch: [23][ 10/391]	Time  0.140 ( 0.152)	Data  0.001 ( 0.015)	Loss 1.4861e-01 (1.6800e-01)	Acc@1  92.97 ( 93.96)	Acc@5 100.00 ( 99.93)
Epoch: [23][ 20/391]	Time  0.140 ( 0.146)	Data  0.001 ( 0.008)	Loss 1.4027e-01 (1.6916e-01)	Acc@1  96.88 ( 94.23)	Acc@5 100.00 ( 99.96)
Epoch: [23][ 30/391]	Time  0.139 ( 0.144)	Data  0.001 ( 0.006)	Loss 1.5392e-01 (1.6535e-01)	Acc@1  93.75 ( 94.30)	Acc@5 100.00 ( 99.97)
Epoch: [23][ 40/391]	Time  0.134 ( 0.142)	Data  0.001 ( 0.005)	Loss 1.0528e-01 (1.6792e-01)	Acc@1  96.88 ( 94.13)	Acc@5 100.00 ( 99.96)
Epoch: [23][ 50/391]	Time  0.134 ( 0.141)	Data  0.001 ( 0.004)	Loss 2.4677e-01 (1.6460e-01)	Acc@1  92.19 ( 94.29)	Acc@5 100.00 ( 99.97)
Epoch: [23][ 60/391]	Time  0.139 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.1969e-01 (1.6377e-01)	Acc@1  95.31 ( 94.29)	Acc@5 100.00 ( 99.96)
Epoch: [23][ 70/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.4694e-01 (1.6539e-01)	Acc@1  93.75 ( 94.22)	Acc@5 100.00 ( 99.94)
Epoch: [23][ 80/391]	Time  0.133 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.6209e-01 (1.6621e-01)	Acc@1  96.09 ( 94.22)	Acc@5  99.22 ( 99.94)
Epoch: [23][ 90/391]	Time  0.140 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.9293e-01 (1.6771e-01)	Acc@1  92.97 ( 94.10)	Acc@5 100.00 ( 99.95)
Epoch: [23][100/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.7361e-01 (1.7042e-01)	Acc@1  90.62 ( 94.07)	Acc@5  99.22 ( 99.95)
Epoch: [23][110/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.002)	Loss 2.1265e-01 (1.7063e-01)	Acc@1  92.19 ( 94.00)	Acc@5 100.00 ( 99.94)
Epoch: [23][120/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.002)	Loss 2.4454e-01 (1.7360e-01)	Acc@1  91.41 ( 93.87)	Acc@5 100.00 ( 99.95)
Epoch: [23][130/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.5182e-01 (1.7731e-01)	Acc@1  88.28 ( 93.68)	Acc@5 100.00 ( 99.95)
Epoch: [23][140/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5763e-01 (1.8003e-01)	Acc@1  94.53 ( 93.60)	Acc@5 100.00 ( 99.93)
Epoch: [23][150/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0568e-01 (1.8375e-01)	Acc@1  96.88 ( 93.46)	Acc@5 100.00 ( 99.93)
Epoch: [23][160/391]	Time  0.149 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5581e-01 (1.8469e-01)	Acc@1  93.75 ( 93.41)	Acc@5 100.00 ( 99.93)
Epoch: [23][170/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5468e-01 (1.8649e-01)	Acc@1  93.75 ( 93.33)	Acc@5 100.00 ( 99.94)
Epoch: [23][180/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7786e-01 (1.8807e-01)	Acc@1  94.53 ( 93.30)	Acc@5 100.00 ( 99.93)
Epoch: [23][190/391]	Time  0.145 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1597e-01 (1.8879e-01)	Acc@1  92.97 ( 93.27)	Acc@5 100.00 ( 99.93)
Epoch: [23][200/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.0469e-01 (1.9110e-01)	Acc@1  89.06 ( 93.14)	Acc@5 100.00 ( 99.92)
Epoch: [23][210/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0671e-01 (1.9130e-01)	Acc@1  93.75 ( 93.14)	Acc@5 100.00 ( 99.92)
Epoch: [23][220/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.3026e-01 (1.9223e-01)	Acc@1  93.75 ( 93.10)	Acc@5 100.00 ( 99.92)
Epoch: [23][230/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8981e-01 (1.9324e-01)	Acc@1  94.53 ( 93.12)	Acc@5  99.22 ( 99.92)
Epoch: [23][240/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1762e-01 (1.9343e-01)	Acc@1  94.53 ( 93.11)	Acc@5 100.00 ( 99.92)
Epoch: [23][250/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5661e-01 (1.9438e-01)	Acc@1  94.53 ( 93.10)	Acc@5 100.00 ( 99.91)
Epoch: [23][260/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8418e-01 (1.9555e-01)	Acc@1  90.62 ( 93.06)	Acc@5 100.00 ( 99.91)
Epoch: [23][270/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0006e-01 (1.9621e-01)	Acc@1  92.19 ( 93.01)	Acc@5 100.00 ( 99.91)
Epoch: [23][280/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.9413e-01 (1.9555e-01)	Acc@1  92.19 ( 93.04)	Acc@5 100.00 ( 99.91)
Epoch: [23][290/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.4820e-01 (1.9582e-01)	Acc@1  92.19 ( 93.04)	Acc@5 100.00 ( 99.91)
Epoch: [23][300/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1616e-01 (1.9544e-01)	Acc@1  97.66 ( 93.05)	Acc@5 100.00 ( 99.90)
Epoch: [23][310/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6226e-01 (1.9549e-01)	Acc@1  94.53 ( 93.07)	Acc@5 100.00 ( 99.90)
Epoch: [23][320/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0351e-01 (1.9628e-01)	Acc@1  94.53 ( 93.04)	Acc@5 100.00 ( 99.90)
Epoch: [23][330/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.8393e-01 (1.9641e-01)	Acc@1  91.41 ( 93.05)	Acc@5 100.00 ( 99.91)
Epoch: [23][340/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2131e-01 (1.9662e-01)	Acc@1  91.41 ( 93.04)	Acc@5 100.00 ( 99.91)
Epoch: [23][350/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7564e-01 (1.9683e-01)	Acc@1  93.75 ( 93.04)	Acc@5  99.22 ( 99.91)
Epoch: [23][360/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.1590e-02 (1.9703e-01)	Acc@1  97.66 ( 93.03)	Acc@5 100.00 ( 99.90)
Epoch: [23][370/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.4588e-01 (1.9678e-01)	Acc@1  92.19 ( 93.04)	Acc@5 100.00 ( 99.91)
Epoch: [23][380/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1140e-01 (1.9686e-01)	Acc@1  94.53 ( 93.03)	Acc@5 100.00 ( 99.90)
Epoch: [23][390/391]	Time  0.114 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6825e-01 (1.9733e-01)	Acc@1  92.50 ( 93.02)	Acc@5 100.00 ( 99.90)
## e[23] optimizer.zero_grad (sum) time: 0.6841673851013184
## e[23]       loss.backward (sum) time: 13.98403024673462
## e[23]      optimizer.step (sum) time: 14.669872522354126
## epoch[23] training(only) time: 54.158923625946045
# Switched to evaluate mode...
Test: [  0/100]	Time  0.197 ( 0.197)	Loss 3.7393e-01 (3.7393e-01)	Acc@1  87.00 ( 87.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.046 ( 0.062)	Loss 4.4912e-01 (3.7353e-01)	Acc@1  86.00 ( 88.18)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 3.5085e-01 (3.8347e-01)	Acc@1  84.00 ( 87.76)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 3.5431e-01 (3.8494e-01)	Acc@1  88.00 ( 87.84)	Acc@5  99.00 ( 99.52)
Test: [ 40/100]	Time  0.053 ( 0.052)	Loss 2.8822e-01 (3.8067e-01)	Acc@1  93.00 ( 88.07)	Acc@5  99.00 ( 99.51)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 2.2878e-01 (3.7373e-01)	Acc@1  93.00 ( 88.29)	Acc@5 100.00 ( 99.55)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 3.6995e-01 (3.6723e-01)	Acc@1  88.00 ( 88.49)	Acc@5  99.00 ( 99.56)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 3.6334e-01 (3.6180e-01)	Acc@1  88.00 ( 88.69)	Acc@5 100.00 ( 99.59)
Test: [ 80/100]	Time  0.059 ( 0.050)	Loss 2.6545e-01 (3.6478e-01)	Acc@1  90.00 ( 88.58)	Acc@5 100.00 ( 99.59)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 2.3962e-01 (3.5901e-01)	Acc@1  93.00 ( 88.69)	Acc@5 100.00 ( 99.60)
 * Acc@1 88.820 Acc@5 99.620
### epoch[23] execution time: 59.15449619293213
EPOCH 24
# current learning rate: 0.1
# Switched to train mode...
Epoch: [24][  0/391]	Time  0.307 ( 0.307)	Data  0.162 ( 0.162)	Loss 1.1654e-01 (1.1654e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [24][ 10/391]	Time  0.137 ( 0.154)	Data  0.001 ( 0.016)	Loss 2.3142e-01 (1.7357e-01)	Acc@1  88.28 ( 93.96)	Acc@5 100.00 ( 99.93)
Epoch: [24][ 20/391]	Time  0.142 ( 0.147)	Data  0.001 ( 0.009)	Loss 2.4145e-01 (1.8623e-01)	Acc@1  91.41 ( 93.42)	Acc@5 100.00 ( 99.93)
Epoch: [24][ 30/391]	Time  0.137 ( 0.143)	Data  0.001 ( 0.006)	Loss 2.3385e-01 (1.8471e-01)	Acc@1  92.19 ( 93.47)	Acc@5 100.00 ( 99.95)
Epoch: [24][ 40/391]	Time  0.136 ( 0.142)	Data  0.001 ( 0.005)	Loss 1.3967e-01 (1.7990e-01)	Acc@1  94.53 ( 93.62)	Acc@5 100.00 ( 99.96)
Epoch: [24][ 50/391]	Time  0.144 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.8264e-01 (1.8322e-01)	Acc@1  93.75 ( 93.43)	Acc@5 100.00 ( 99.94)
Epoch: [24][ 60/391]	Time  0.136 ( 0.141)	Data  0.001 ( 0.004)	Loss 2.7652e-01 (1.8645e-01)	Acc@1  87.50 ( 93.26)	Acc@5 100.00 ( 99.95)
Epoch: [24][ 70/391]	Time  0.133 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.3659e-01 (1.8329e-01)	Acc@1  94.53 ( 93.41)	Acc@5 100.00 ( 99.94)
Epoch: [24][ 80/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.9228e-01 (1.8306e-01)	Acc@1  91.41 ( 93.45)	Acc@5 100.00 ( 99.94)
Epoch: [24][ 90/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.0081e-01 (1.8104e-01)	Acc@1  95.31 ( 93.54)	Acc@5  99.22 ( 99.93)
Epoch: [24][100/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.6252e-01 (1.8366e-01)	Acc@1  88.28 ( 93.41)	Acc@5 100.00 ( 99.94)
Epoch: [24][110/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.6407e-01 (1.8250e-01)	Acc@1  93.75 ( 93.48)	Acc@5 100.00 ( 99.94)
Epoch: [24][120/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.3877e-01 (1.8089e-01)	Acc@1  92.19 ( 93.58)	Acc@5 100.00 ( 99.94)
Epoch: [24][130/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6170e-01 (1.8032e-01)	Acc@1  94.53 ( 93.57)	Acc@5 100.00 ( 99.95)
Epoch: [24][140/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.2395e-01 (1.8172e-01)	Acc@1  89.84 ( 93.52)	Acc@5  99.22 ( 99.93)
Epoch: [24][150/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.9961e-01 (1.8154e-01)	Acc@1  91.41 ( 93.49)	Acc@5 100.00 ( 99.94)
Epoch: [24][160/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6321e-01 (1.8244e-01)	Acc@1  92.97 ( 93.45)	Acc@5 100.00 ( 99.94)
Epoch: [24][170/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.9252e-01 (1.8307e-01)	Acc@1  94.53 ( 93.42)	Acc@5 100.00 ( 99.94)
Epoch: [24][180/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5894e-01 (1.8462e-01)	Acc@1  92.19 ( 93.36)	Acc@5 100.00 ( 99.93)
Epoch: [24][190/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.9543e-01 (1.8786e-01)	Acc@1  88.28 ( 93.28)	Acc@5 100.00 ( 99.92)
Epoch: [24][200/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5989e-01 (1.8933e-01)	Acc@1  94.53 ( 93.24)	Acc@5 100.00 ( 99.92)
Epoch: [24][210/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0431e-01 (1.9077e-01)	Acc@1  90.62 ( 93.19)	Acc@5  99.22 ( 99.91)
Epoch: [24][220/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.9706e-02 (1.9066e-01)	Acc@1  96.09 ( 93.20)	Acc@5 100.00 ( 99.90)
Epoch: [24][230/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3523e-01 (1.9038e-01)	Acc@1  96.09 ( 93.22)	Acc@5 100.00 ( 99.90)
Epoch: [24][240/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6647e-01 (1.9155e-01)	Acc@1  93.75 ( 93.19)	Acc@5 100.00 ( 99.90)
Epoch: [24][250/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.6651e-02 (1.9094e-01)	Acc@1  95.31 ( 93.22)	Acc@5 100.00 ( 99.91)
Epoch: [24][260/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9139e-01 (1.9035e-01)	Acc@1  92.97 ( 93.25)	Acc@5 100.00 ( 99.90)
Epoch: [24][270/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0904e-01 (1.9100e-01)	Acc@1  92.19 ( 93.25)	Acc@5 100.00 ( 99.90)
Epoch: [24][280/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2634e-01 (1.9002e-01)	Acc@1  94.53 ( 93.31)	Acc@5 100.00 ( 99.90)
Epoch: [24][290/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1766e-01 (1.8973e-01)	Acc@1  90.62 ( 93.31)	Acc@5 100.00 ( 99.90)
Epoch: [24][300/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.3382e-01 (1.8949e-01)	Acc@1  92.97 ( 93.32)	Acc@5 100.00 ( 99.90)
Epoch: [24][310/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2663e-01 (1.8992e-01)	Acc@1  91.41 ( 93.31)	Acc@5  99.22 ( 99.90)
Epoch: [24][320/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.4559e-01 (1.9019e-01)	Acc@1  89.06 ( 93.28)	Acc@5  99.22 ( 99.90)
Epoch: [24][330/391]	Time  0.144 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.4839e-01 (1.8911e-01)	Acc@1  90.62 ( 93.31)	Acc@5 100.00 ( 99.90)
Epoch: [24][340/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4281e-01 (1.8964e-01)	Acc@1  94.53 ( 93.28)	Acc@5 100.00 ( 99.89)
Epoch: [24][350/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7344e-01 (1.8951e-01)	Acc@1  93.75 ( 93.30)	Acc@5 100.00 ( 99.90)
Epoch: [24][360/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4193e-01 (1.9039e-01)	Acc@1  94.53 ( 93.26)	Acc@5 100.00 ( 99.89)
Epoch: [24][370/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.6067e-01 (1.9120e-01)	Acc@1  89.06 ( 93.23)	Acc@5  99.22 ( 99.89)
Epoch: [24][380/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.3907e-01 (1.9119e-01)	Acc@1  90.62 ( 93.24)	Acc@5 100.00 ( 99.89)
Epoch: [24][390/391]	Time  0.113 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.5058e-02 (1.9164e-01)	Acc@1  98.75 ( 93.22)	Acc@5 100.00 ( 99.89)
## e[24] optimizer.zero_grad (sum) time: 0.6879513263702393
## e[24]       loss.backward (sum) time: 13.956974029541016
## e[24]      optimizer.step (sum) time: 14.630610227584839
## epoch[24] training(only) time: 54.06584882736206
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 2.4872e-01 (2.4872e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 3.8418e-01 (3.2538e-01)	Acc@1  87.00 ( 88.64)	Acc@5  99.00 ( 99.73)
Test: [ 20/100]	Time  0.049 ( 0.055)	Loss 3.5529e-01 (3.2416e-01)	Acc@1  85.00 ( 88.24)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 2.8075e-01 (3.3180e-01)	Acc@1  91.00 ( 88.77)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 4.5199e-01 (3.3360e-01)	Acc@1  88.00 ( 88.78)	Acc@5  99.00 ( 99.68)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 3.2737e-01 (3.3734e-01)	Acc@1  88.00 ( 88.80)	Acc@5 100.00 ( 99.67)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 2.9869e-01 (3.3266e-01)	Acc@1  94.00 ( 88.95)	Acc@5 100.00 ( 99.69)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 3.6829e-01 (3.2649e-01)	Acc@1  87.00 ( 89.23)	Acc@5 100.00 ( 99.70)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 2.7902e-01 (3.3389e-01)	Acc@1  91.00 ( 89.10)	Acc@5 100.00 ( 99.72)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 2.5240e-01 (3.3335e-01)	Acc@1  89.00 ( 89.03)	Acc@5 100.00 ( 99.73)
 * Acc@1 89.120 Acc@5 99.720
### epoch[24] execution time: 59.073806285858154
EPOCH 25
# current learning rate: 0.1
# Switched to train mode...
Epoch: [25][  0/391]	Time  0.299 ( 0.299)	Data  0.158 ( 0.158)	Loss 1.6896e-01 (1.6896e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
Epoch: [25][ 10/391]	Time  0.144 ( 0.153)	Data  0.001 ( 0.015)	Loss 2.4338e-01 (1.6357e-01)	Acc@1  92.19 ( 94.03)	Acc@5  99.22 ( 99.86)
Epoch: [25][ 20/391]	Time  0.136 ( 0.146)	Data  0.001 ( 0.009)	Loss 1.8865e-01 (1.7029e-01)	Acc@1  92.19 ( 93.79)	Acc@5 100.00 ( 99.93)
Epoch: [25][ 30/391]	Time  0.136 ( 0.144)	Data  0.001 ( 0.006)	Loss 1.5401e-01 (1.6575e-01)	Acc@1  95.31 ( 94.00)	Acc@5 100.00 ( 99.92)
Epoch: [25][ 40/391]	Time  0.139 ( 0.142)	Data  0.001 ( 0.005)	Loss 1.0955e-01 (1.6954e-01)	Acc@1  96.88 ( 93.73)	Acc@5 100.00 ( 99.94)
Epoch: [25][ 50/391]	Time  0.138 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.1892e-01 (1.6726e-01)	Acc@1  96.09 ( 93.77)	Acc@5 100.00 ( 99.94)
Epoch: [25][ 60/391]	Time  0.135 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.2576e-01 (1.6877e-01)	Acc@1  93.75 ( 93.80)	Acc@5 100.00 ( 99.95)
Epoch: [25][ 70/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.5091e-01 (1.6555e-01)	Acc@1  92.19 ( 93.86)	Acc@5 100.00 ( 99.96)
Epoch: [25][ 80/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.6903e-01 (1.6663e-01)	Acc@1  93.75 ( 93.89)	Acc@5 100.00 ( 99.95)
Epoch: [25][ 90/391]	Time  0.140 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.0997e-01 (1.6833e-01)	Acc@1  92.97 ( 93.84)	Acc@5  99.22 ( 99.95)
Epoch: [25][100/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.6032e-01 (1.7232e-01)	Acc@1  95.31 ( 93.70)	Acc@5 100.00 ( 99.95)
Epoch: [25][110/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.8889e-01 (1.7342e-01)	Acc@1  92.19 ( 93.72)	Acc@5 100.00 ( 99.94)
Epoch: [25][120/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7071e-01 (1.7335e-01)	Acc@1  89.84 ( 93.72)	Acc@5 100.00 ( 99.94)
Epoch: [25][130/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4818e-01 (1.7375e-01)	Acc@1  96.09 ( 93.71)	Acc@5 100.00 ( 99.94)
Epoch: [25][140/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.7266e-01 (1.7477e-01)	Acc@1  90.62 ( 93.70)	Acc@5 100.00 ( 99.94)
Epoch: [25][150/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5289e-01 (1.7350e-01)	Acc@1  96.88 ( 93.76)	Acc@5 100.00 ( 99.94)
Epoch: [25][160/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6188e-01 (1.7308e-01)	Acc@1  95.31 ( 93.78)	Acc@5 100.00 ( 99.94)
Epoch: [25][170/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2883e-01 (1.7296e-01)	Acc@1  92.19 ( 93.74)	Acc@5 100.00 ( 99.94)
Epoch: [25][180/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5421e-01 (1.7385e-01)	Acc@1  95.31 ( 93.72)	Acc@5 100.00 ( 99.94)
Epoch: [25][190/391]	Time  0.148 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4648e-01 (1.7306e-01)	Acc@1  95.31 ( 93.79)	Acc@5 100.00 ( 99.95)
Epoch: [25][200/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.7050e-01 (1.7437e-01)	Acc@1  89.06 ( 93.72)	Acc@5 100.00 ( 99.95)
Epoch: [25][210/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2259e-01 (1.7513e-01)	Acc@1  96.88 ( 93.75)	Acc@5 100.00 ( 99.95)
Epoch: [25][220/391]	Time  0.148 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2825e-01 (1.7469e-01)	Acc@1  96.09 ( 93.76)	Acc@5 100.00 ( 99.95)
Epoch: [25][230/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4493e-01 (1.7521e-01)	Acc@1  94.53 ( 93.76)	Acc@5 100.00 ( 99.95)
Epoch: [25][240/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1282e-01 (1.7676e-01)	Acc@1  92.97 ( 93.70)	Acc@5 100.00 ( 99.94)
Epoch: [25][250/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9390e-01 (1.7705e-01)	Acc@1  92.19 ( 93.69)	Acc@5 100.00 ( 99.94)
Epoch: [25][260/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6140e-01 (1.7710e-01)	Acc@1  96.09 ( 93.69)	Acc@5 100.00 ( 99.94)
Epoch: [25][270/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.3553e-01 (1.7803e-01)	Acc@1  90.62 ( 93.64)	Acc@5 100.00 ( 99.93)
Epoch: [25][280/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.3605e-01 (1.7845e-01)	Acc@1  92.19 ( 93.63)	Acc@5 100.00 ( 99.93)
Epoch: [25][290/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7017e-01 (1.7801e-01)	Acc@1  93.75 ( 93.65)	Acc@5 100.00 ( 99.93)
Epoch: [25][300/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7210e-01 (1.7836e-01)	Acc@1  92.97 ( 93.62)	Acc@5 100.00 ( 99.93)
Epoch: [25][310/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.5184e-01 (1.7923e-01)	Acc@1  88.28 ( 93.59)	Acc@5 100.00 ( 99.93)
Epoch: [25][320/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7764e-01 (1.7907e-01)	Acc@1  93.75 ( 93.60)	Acc@5  99.22 ( 99.93)
Epoch: [25][330/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7728e-01 (1.7910e-01)	Acc@1  94.53 ( 93.59)	Acc@5  99.22 ( 99.92)
Epoch: [25][340/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6653e-01 (1.7919e-01)	Acc@1  96.09 ( 93.60)	Acc@5 100.00 ( 99.93)
Epoch: [25][350/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.4328e-01 (1.7924e-01)	Acc@1  91.41 ( 93.61)	Acc@5 100.00 ( 99.92)
Epoch: [25][360/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7898e-01 (1.7995e-01)	Acc@1  93.75 ( 93.58)	Acc@5 100.00 ( 99.92)
Epoch: [25][370/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9863e-01 (1.7987e-01)	Acc@1  94.53 ( 93.59)	Acc@5 100.00 ( 99.92)
Epoch: [25][380/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9607e-01 (1.8095e-01)	Acc@1  92.19 ( 93.55)	Acc@5 100.00 ( 99.91)
Epoch: [25][390/391]	Time  0.110 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4988e-01 (1.8233e-01)	Acc@1  95.00 ( 93.51)	Acc@5 100.00 ( 99.91)
## e[25] optimizer.zero_grad (sum) time: 0.6853156089782715
## e[25]       loss.backward (sum) time: 13.991937637329102
## e[25]      optimizer.step (sum) time: 14.6280996799469
## epoch[25] training(only) time: 54.08918070793152
# Switched to evaluate mode...
Test: [  0/100]	Time  0.191 ( 0.191)	Loss 2.4633e-01 (2.4633e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.062)	Loss 3.7971e-01 (3.4188e-01)	Acc@1  89.00 ( 88.91)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 3.4196e-01 (3.6112e-01)	Acc@1  91.00 ( 88.62)	Acc@5  99.00 ( 99.57)
Test: [ 30/100]	Time  0.050 ( 0.053)	Loss 4.5932e-01 (3.7904e-01)	Acc@1  89.00 ( 88.71)	Acc@5  97.00 ( 99.42)
Test: [ 40/100]	Time  0.063 ( 0.052)	Loss 3.9098e-01 (3.7661e-01)	Acc@1  89.00 ( 88.80)	Acc@5  99.00 ( 99.49)
Test: [ 50/100]	Time  0.048 ( 0.051)	Loss 2.2183e-01 (3.7787e-01)	Acc@1  93.00 ( 88.84)	Acc@5 100.00 ( 99.45)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 3.6279e-01 (3.7197e-01)	Acc@1  89.00 ( 88.82)	Acc@5 100.00 ( 99.52)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 5.4316e-01 (3.7887e-01)	Acc@1  86.00 ( 88.76)	Acc@5 100.00 ( 99.51)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 2.3393e-01 (3.8006e-01)	Acc@1  92.00 ( 88.74)	Acc@5 100.00 ( 99.54)
Test: [ 90/100]	Time  0.057 ( 0.050)	Loss 2.5359e-01 (3.7607e-01)	Acc@1  93.00 ( 88.80)	Acc@5 100.00 ( 99.56)
 * Acc@1 88.780 Acc@5 99.560
### epoch[25] execution time: 59.09944820404053
EPOCH 26
# current learning rate: 0.1
# Switched to train mode...
Epoch: [26][  0/391]	Time  0.299 ( 0.299)	Data  0.154 ( 0.154)	Loss 1.2866e-01 (1.2866e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [26][ 10/391]	Time  0.138 ( 0.152)	Data  0.001 ( 0.015)	Loss 2.1896e-01 (1.7494e-01)	Acc@1  93.75 ( 93.96)	Acc@5  99.22 ( 99.86)
Epoch: [26][ 20/391]	Time  0.140 ( 0.145)	Data  0.001 ( 0.008)	Loss 1.0237e-01 (1.5483e-01)	Acc@1  96.09 ( 94.64)	Acc@5 100.00 ( 99.89)
Epoch: [26][ 30/391]	Time  0.138 ( 0.143)	Data  0.001 ( 0.006)	Loss 1.2115e-01 (1.6505e-01)	Acc@1  96.88 ( 94.43)	Acc@5 100.00 ( 99.92)
Epoch: [26][ 40/391]	Time  0.140 ( 0.142)	Data  0.001 ( 0.005)	Loss 1.7728e-01 (1.6143e-01)	Acc@1  96.09 ( 94.66)	Acc@5 100.00 ( 99.94)
Epoch: [26][ 50/391]	Time  0.151 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.4739e-01 (1.6219e-01)	Acc@1  94.53 ( 94.58)	Acc@5 100.00 ( 99.92)
Epoch: [26][ 60/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.2031e-01 (1.6339e-01)	Acc@1  95.31 ( 94.45)	Acc@5 100.00 ( 99.94)
Epoch: [26][ 70/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.4714e-01 (1.6569e-01)	Acc@1  96.09 ( 94.44)	Acc@5 100.00 ( 99.93)
Epoch: [26][ 80/391]	Time  0.145 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.1991e-01 (1.6459e-01)	Acc@1  96.88 ( 94.50)	Acc@5 100.00 ( 99.91)
Epoch: [26][ 90/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.3343e-01 (1.6448e-01)	Acc@1  94.53 ( 94.47)	Acc@5 100.00 ( 99.92)
Epoch: [26][100/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.0841e-01 (1.6378e-01)	Acc@1  96.09 ( 94.47)	Acc@5 100.00 ( 99.93)
Epoch: [26][110/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.6330e-01 (1.6341e-01)	Acc@1  95.31 ( 94.50)	Acc@5 100.00 ( 99.94)
Epoch: [26][120/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1735e-01 (1.6368e-01)	Acc@1  91.41 ( 94.46)	Acc@5 100.00 ( 99.94)
Epoch: [26][130/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.9357e-01 (1.6493e-01)	Acc@1  94.53 ( 94.42)	Acc@5 100.00 ( 99.95)
Epoch: [26][140/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.8771e-02 (1.6498e-01)	Acc@1  96.88 ( 94.39)	Acc@5 100.00 ( 99.94)
Epoch: [26][150/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8536e-01 (1.6532e-01)	Acc@1  94.53 ( 94.39)	Acc@5 100.00 ( 99.95)
Epoch: [26][160/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4248e-01 (1.6762e-01)	Acc@1  95.31 ( 94.32)	Acc@5 100.00 ( 99.94)
Epoch: [26][170/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.1082e-01 (1.6898e-01)	Acc@1  92.19 ( 94.31)	Acc@5 100.00 ( 99.95)
Epoch: [26][180/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2192e-01 (1.6858e-01)	Acc@1  94.53 ( 94.29)	Acc@5 100.00 ( 99.95)
Epoch: [26][190/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.7243e-01 (1.7083e-01)	Acc@1  92.97 ( 94.20)	Acc@5  99.22 ( 99.95)
Epoch: [26][200/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1097e-01 (1.7120e-01)	Acc@1  90.62 ( 94.17)	Acc@5 100.00 ( 99.94)
Epoch: [26][210/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3754e-01 (1.7121e-01)	Acc@1  93.75 ( 94.17)	Acc@5 100.00 ( 99.94)
Epoch: [26][220/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1345e-01 (1.7203e-01)	Acc@1  92.97 ( 94.14)	Acc@5  99.22 ( 99.94)
Epoch: [26][230/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.8016e-02 (1.7172e-01)	Acc@1  97.66 ( 94.14)	Acc@5 100.00 ( 99.94)
Epoch: [26][240/391]	Time  0.144 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2285e-01 (1.7251e-01)	Acc@1  92.19 ( 94.10)	Acc@5 100.00 ( 99.94)
Epoch: [26][250/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.3138e-01 (1.7253e-01)	Acc@1  90.62 ( 94.07)	Acc@5 100.00 ( 99.94)
Epoch: [26][260/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7440e-01 (1.7369e-01)	Acc@1  95.31 ( 94.04)	Acc@5 100.00 ( 99.94)
Epoch: [26][270/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6079e-01 (1.7428e-01)	Acc@1  95.31 ( 94.00)	Acc@5 100.00 ( 99.94)
Epoch: [26][280/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7945e-01 (1.7500e-01)	Acc@1  95.31 ( 93.95)	Acc@5 100.00 ( 99.94)
Epoch: [26][290/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5393e-01 (1.7525e-01)	Acc@1  93.75 ( 93.92)	Acc@5 100.00 ( 99.94)
Epoch: [26][300/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0704e-01 (1.7594e-01)	Acc@1  92.97 ( 93.87)	Acc@5 100.00 ( 99.95)
Epoch: [26][310/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9857e-01 (1.7599e-01)	Acc@1  92.19 ( 93.87)	Acc@5 100.00 ( 99.95)
Epoch: [26][320/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7035e-01 (1.7638e-01)	Acc@1  93.75 ( 93.84)	Acc@5 100.00 ( 99.95)
Epoch: [26][330/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.1294e-02 (1.7686e-01)	Acc@1  97.66 ( 93.81)	Acc@5 100.00 ( 99.95)
Epoch: [26][340/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4559e-01 (1.7740e-01)	Acc@1  96.88 ( 93.81)	Acc@5 100.00 ( 99.94)
Epoch: [26][350/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.9860e-02 (1.7673e-01)	Acc@1  98.44 ( 93.85)	Acc@5 100.00 ( 99.94)
Epoch: [26][360/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6480e-01 (1.7684e-01)	Acc@1  94.53 ( 93.85)	Acc@5 100.00 ( 99.94)
Epoch: [26][370/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0888e-01 (1.7686e-01)	Acc@1  92.97 ( 93.86)	Acc@5  99.22 ( 99.93)
Epoch: [26][380/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7482e-01 (1.7684e-01)	Acc@1  93.75 ( 93.85)	Acc@5 100.00 ( 99.93)
Epoch: [26][390/391]	Time  0.116 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.0871e-01 (1.7781e-01)	Acc@1  86.25 ( 93.81)	Acc@5 100.00 ( 99.94)
## e[26] optimizer.zero_grad (sum) time: 0.6870372295379639
## e[26]       loss.backward (sum) time: 13.969313383102417
## e[26]      optimizer.step (sum) time: 14.61078405380249
## epoch[26] training(only) time: 54.06271409988403
# Switched to evaluate mode...
Test: [  0/100]	Time  0.191 ( 0.191)	Loss 2.8186e-01 (2.8186e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 3.3365e-01 (3.4826e-01)	Acc@1  88.00 ( 89.09)	Acc@5 100.00 ( 99.55)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 4.4002e-01 (3.7740e-01)	Acc@1  85.00 ( 88.05)	Acc@5  99.00 ( 99.43)
Test: [ 30/100]	Time  0.047 ( 0.053)	Loss 4.0328e-01 (3.7980e-01)	Acc@1  85.00 ( 87.90)	Acc@5  99.00 ( 99.48)
Test: [ 40/100]	Time  0.048 ( 0.052)	Loss 3.8888e-01 (3.7777e-01)	Acc@1  88.00 ( 88.20)	Acc@5  99.00 ( 99.49)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 2.1816e-01 (3.7537e-01)	Acc@1  91.00 ( 88.22)	Acc@5 100.00 ( 99.43)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 3.4854e-01 (3.7187e-01)	Acc@1  92.00 ( 88.30)	Acc@5 100.00 ( 99.49)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 5.7014e-01 (3.7480e-01)	Acc@1  84.00 ( 88.07)	Acc@5 100.00 ( 99.46)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 3.1197e-01 (3.7610e-01)	Acc@1  87.00 ( 88.07)	Acc@5 100.00 ( 99.49)
Test: [ 90/100]	Time  0.049 ( 0.049)	Loss 3.0905e-01 (3.7647e-01)	Acc@1  90.00 ( 88.07)	Acc@5 100.00 ( 99.55)
 * Acc@1 88.060 Acc@5 99.570
### epoch[26] execution time: 59.06473422050476
EPOCH 27
# current learning rate: 0.1
# Switched to train mode...
Epoch: [27][  0/391]	Time  0.298 ( 0.298)	Data  0.151 ( 0.151)	Loss 1.9037e-01 (1.9037e-01)	Acc@1  95.31 ( 95.31)	Acc@5  99.22 ( 99.22)
Epoch: [27][ 10/391]	Time  0.136 ( 0.152)	Data  0.001 ( 0.015)	Loss 1.2418e-01 (1.4658e-01)	Acc@1  94.53 ( 95.24)	Acc@5 100.00 ( 99.86)
Epoch: [27][ 20/391]	Time  0.136 ( 0.145)	Data  0.001 ( 0.008)	Loss 2.8330e-01 (1.5596e-01)	Acc@1  89.84 ( 94.72)	Acc@5 100.00 ( 99.85)
Epoch: [27][ 30/391]	Time  0.137 ( 0.143)	Data  0.001 ( 0.006)	Loss 1.6540e-01 (1.5718e-01)	Acc@1  92.97 ( 94.58)	Acc@5 100.00 ( 99.90)
Epoch: [27][ 40/391]	Time  0.140 ( 0.141)	Data  0.001 ( 0.005)	Loss 2.4755e-01 (1.5689e-01)	Acc@1  89.06 ( 94.44)	Acc@5  99.22 ( 99.89)
Epoch: [27][ 50/391]	Time  0.136 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.4456e-01 (1.5781e-01)	Acc@1  93.75 ( 94.35)	Acc@5 100.00 ( 99.88)
Epoch: [27][ 60/391]	Time  0.133 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.6007e-01 (1.5933e-01)	Acc@1  95.31 ( 94.35)	Acc@5 100.00 ( 99.90)
Epoch: [27][ 70/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.0835e-01 (1.6352e-01)	Acc@1  96.09 ( 94.21)	Acc@5 100.00 ( 99.90)
Epoch: [27][ 80/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.5216e-01 (1.6313e-01)	Acc@1  93.75 ( 94.27)	Acc@5 100.00 ( 99.88)
Epoch: [27][ 90/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.5592e-01 (1.6488e-01)	Acc@1  92.19 ( 94.20)	Acc@5  99.22 ( 99.88)
Epoch: [27][100/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.003)	Loss 3.1733e-01 (1.6496e-01)	Acc@1  86.72 ( 94.13)	Acc@5 100.00 ( 99.89)
Epoch: [27][110/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2422e-01 (1.6835e-01)	Acc@1  96.09 ( 94.03)	Acc@5 100.00 ( 99.90)
Epoch: [27][120/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1076e-01 (1.6934e-01)	Acc@1  92.97 ( 93.99)	Acc@5 100.00 ( 99.89)
Epoch: [27][130/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5748e-01 (1.7025e-01)	Acc@1  93.75 ( 93.95)	Acc@5 100.00 ( 99.90)
Epoch: [27][140/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6886e-01 (1.7015e-01)	Acc@1  93.75 ( 93.94)	Acc@5 100.00 ( 99.89)
Epoch: [27][150/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5934e-01 (1.7150e-01)	Acc@1  94.53 ( 93.86)	Acc@5 100.00 ( 99.90)
Epoch: [27][160/391]	Time  0.145 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4149e-01 (1.7032e-01)	Acc@1  95.31 ( 93.90)	Acc@5 100.00 ( 99.91)
Epoch: [27][170/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5101e-01 (1.7053e-01)	Acc@1  96.09 ( 93.89)	Acc@5 100.00 ( 99.91)
Epoch: [27][180/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3101e-01 (1.6858e-01)	Acc@1  96.88 ( 93.98)	Acc@5 100.00 ( 99.91)
Epoch: [27][190/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3094e-01 (1.6863e-01)	Acc@1  94.53 ( 93.98)	Acc@5 100.00 ( 99.92)
Epoch: [27][200/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4432e-01 (1.6846e-01)	Acc@1  95.31 ( 93.98)	Acc@5 100.00 ( 99.92)
Epoch: [27][210/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1792e-01 (1.6723e-01)	Acc@1  97.66 ( 94.05)	Acc@5 100.00 ( 99.92)
Epoch: [27][220/391]	Time  0.146 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2878e-01 (1.6695e-01)	Acc@1  94.53 ( 94.04)	Acc@5 100.00 ( 99.92)
Epoch: [27][230/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0864e-01 (1.6669e-01)	Acc@1  91.41 ( 94.02)	Acc@5 100.00 ( 99.92)
Epoch: [27][240/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1123e-01 (1.6669e-01)	Acc@1  94.53 ( 94.04)	Acc@5  98.44 ( 99.92)
Epoch: [27][250/391]	Time  0.145 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2573e-01 (1.6624e-01)	Acc@1  95.31 ( 94.07)	Acc@5 100.00 ( 99.92)
Epoch: [27][260/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.9082e-01 (1.6673e-01)	Acc@1  91.41 ( 94.07)	Acc@5  99.22 ( 99.92)
Epoch: [27][270/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.1825e-02 (1.6602e-01)	Acc@1  97.66 ( 94.11)	Acc@5 100.00 ( 99.92)
Epoch: [27][280/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3394e-01 (1.6684e-01)	Acc@1  96.88 ( 94.09)	Acc@5 100.00 ( 99.91)
Epoch: [27][290/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.7996e-02 (1.6648e-01)	Acc@1  98.44 ( 94.11)	Acc@5  99.22 ( 99.91)
Epoch: [27][300/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2955e-01 (1.6618e-01)	Acc@1  94.53 ( 94.13)	Acc@5 100.00 ( 99.91)
Epoch: [27][310/391]	Time  0.144 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9177e-01 (1.6677e-01)	Acc@1  92.19 ( 94.10)	Acc@5 100.00 ( 99.91)
Epoch: [27][320/391]	Time  0.143 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5436e-01 (1.6751e-01)	Acc@1  92.97 ( 94.08)	Acc@5 100.00 ( 99.91)
Epoch: [27][330/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3706e-01 (1.6735e-01)	Acc@1  95.31 ( 94.11)	Acc@5 100.00 ( 99.91)
Epoch: [27][340/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.4649e-01 (1.6830e-01)	Acc@1  92.19 ( 94.08)	Acc@5 100.00 ( 99.91)
Epoch: [27][350/391]	Time  0.149 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.6526e-01 (1.6987e-01)	Acc@1  90.62 ( 94.01)	Acc@5 100.00 ( 99.91)
Epoch: [27][360/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7221e-01 (1.7101e-01)	Acc@1  92.97 ( 93.96)	Acc@5 100.00 ( 99.91)
Epoch: [27][370/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.7236e-01 (1.7215e-01)	Acc@1  89.84 ( 93.91)	Acc@5  99.22 ( 99.91)
Epoch: [27][380/391]	Time  0.148 ( 0.138)	Data  0.001 ( 0.001)	Loss 2.0216e-01 (1.7245e-01)	Acc@1  94.53 ( 93.89)	Acc@5 100.00 ( 99.90)
Epoch: [27][390/391]	Time  0.114 ( 0.138)	Data  0.001 ( 0.001)	Loss 9.8092e-02 (1.7221e-01)	Acc@1  98.75 ( 93.91)	Acc@5 100.00 ( 99.90)
## e[27] optimizer.zero_grad (sum) time: 0.6867556571960449
## e[27]       loss.backward (sum) time: 13.983592987060547
## e[27]      optimizer.step (sum) time: 14.642884016036987
## epoch[27] training(only) time: 54.167338371276855
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 3.6560e-01 (3.6560e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.062)	Loss 4.0530e-01 (3.5503e-01)	Acc@1  89.00 ( 88.91)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 4.4050e-01 (3.7157e-01)	Acc@1  86.00 ( 88.95)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.047 ( 0.053)	Loss 3.2055e-01 (3.7543e-01)	Acc@1  86.00 ( 88.94)	Acc@5 100.00 ( 99.77)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 3.6785e-01 (3.7773e-01)	Acc@1  89.00 ( 88.88)	Acc@5  99.00 ( 99.68)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 2.2103e-01 (3.7642e-01)	Acc@1  95.00 ( 89.00)	Acc@5 100.00 ( 99.65)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 3.4363e-01 (3.6993e-01)	Acc@1  89.00 ( 88.90)	Acc@5 100.00 ( 99.66)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 4.4259e-01 (3.6694e-01)	Acc@1  87.00 ( 89.00)	Acc@5 100.00 ( 99.63)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 2.9042e-01 (3.6915e-01)	Acc@1  91.00 ( 88.98)	Acc@5  99.00 ( 99.64)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 2.6518e-01 (3.6853e-01)	Acc@1  90.00 ( 88.93)	Acc@5 100.00 ( 99.63)
 * Acc@1 88.980 Acc@5 99.630
### epoch[27] execution time: 59.179380893707275
EPOCH 28
# current learning rate: 0.1
# Switched to train mode...
Epoch: [28][  0/391]	Time  0.300 ( 0.300)	Data  0.157 ( 0.157)	Loss 1.0448e-01 (1.0448e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [28][ 10/391]	Time  0.140 ( 0.153)	Data  0.001 ( 0.015)	Loss 1.8584e-01 (1.5764e-01)	Acc@1  92.19 ( 94.11)	Acc@5 100.00 (100.00)
Epoch: [28][ 20/391]	Time  0.136 ( 0.146)	Data  0.001 ( 0.008)	Loss 1.9626e-01 (1.5548e-01)	Acc@1  92.97 ( 94.49)	Acc@5 100.00 ( 99.96)
Epoch: [28][ 30/391]	Time  0.146 ( 0.144)	Data  0.001 ( 0.006)	Loss 1.3225e-01 (1.5314e-01)	Acc@1  96.88 ( 94.81)	Acc@5 100.00 ( 99.92)
Epoch: [28][ 40/391]	Time  0.136 ( 0.143)	Data  0.001 ( 0.005)	Loss 1.3570e-01 (1.5257e-01)	Acc@1  95.31 ( 94.74)	Acc@5 100.00 ( 99.94)
Epoch: [28][ 50/391]	Time  0.139 ( 0.142)	Data  0.001 ( 0.004)	Loss 1.8096e-01 (1.5093e-01)	Acc@1  96.09 ( 94.84)	Acc@5 100.00 ( 99.92)
Epoch: [28][ 60/391]	Time  0.138 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.2995e-01 (1.5290e-01)	Acc@1  93.75 ( 94.71)	Acc@5 100.00 ( 99.94)
Epoch: [28][ 70/391]	Time  0.138 ( 0.141)	Data  0.001 ( 0.003)	Loss 1.1658e-01 (1.5872e-01)	Acc@1  96.09 ( 94.47)	Acc@5 100.00 ( 99.93)
Epoch: [28][ 80/391]	Time  0.143 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.8287e-01 (1.6171e-01)	Acc@1  94.53 ( 94.36)	Acc@5 100.00 ( 99.92)
Epoch: [28][ 90/391]	Time  0.140 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.7138e-01 (1.6073e-01)	Acc@1  92.97 ( 94.28)	Acc@5 100.00 ( 99.92)
Epoch: [28][100/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.5152e-01 (1.6057e-01)	Acc@1  94.53 ( 94.28)	Acc@5 100.00 ( 99.92)
Epoch: [28][110/391]	Time  0.148 ( 0.140)	Data  0.001 ( 0.002)	Loss 1.1934e-01 (1.5946e-01)	Acc@1  94.53 ( 94.27)	Acc@5 100.00 ( 99.93)
Epoch: [28][120/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7603e-01 (1.5759e-01)	Acc@1  92.19 ( 94.37)	Acc@5 100.00 ( 99.94)
Epoch: [28][130/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4747e-01 (1.5800e-01)	Acc@1  94.53 ( 94.38)	Acc@5 100.00 ( 99.94)
Epoch: [28][140/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7254e-01 (1.5818e-01)	Acc@1  92.97 ( 94.40)	Acc@5 100.00 ( 99.94)
Epoch: [28][150/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3974e-01 (1.5899e-01)	Acc@1  93.75 ( 94.36)	Acc@5 100.00 ( 99.94)
Epoch: [28][160/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.9288e-01 (1.5885e-01)	Acc@1  92.19 ( 94.38)	Acc@5 100.00 ( 99.95)
Epoch: [28][170/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2570e-01 (1.5914e-01)	Acc@1  95.31 ( 94.39)	Acc@5 100.00 ( 99.95)
Epoch: [28][180/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2345e-01 (1.6012e-01)	Acc@1  95.31 ( 94.37)	Acc@5 100.00 ( 99.95)
Epoch: [28][190/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0504e-01 (1.5985e-01)	Acc@1  92.97 ( 94.40)	Acc@5 100.00 ( 99.95)
Epoch: [28][200/391]	Time  0.145 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2166e-01 (1.6157e-01)	Acc@1  96.88 ( 94.34)	Acc@5 100.00 ( 99.95)
Epoch: [28][210/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5005e-01 (1.6098e-01)	Acc@1  94.53 ( 94.35)	Acc@5 100.00 ( 99.95)
Epoch: [28][220/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1856e-01 (1.6151e-01)	Acc@1  96.88 ( 94.33)	Acc@5 100.00 ( 99.95)
Epoch: [28][230/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7874e-01 (1.6262e-01)	Acc@1  91.41 ( 94.29)	Acc@5 100.00 ( 99.95)
Epoch: [28][240/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6281e-01 (1.6344e-01)	Acc@1  95.31 ( 94.28)	Acc@5 100.00 ( 99.95)
Epoch: [28][250/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0867e-01 (1.6363e-01)	Acc@1  92.97 ( 94.25)	Acc@5  99.22 ( 99.94)
Epoch: [28][260/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5320e-01 (1.6414e-01)	Acc@1  94.53 ( 94.22)	Acc@5 100.00 ( 99.95)
Epoch: [28][270/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2684e-01 (1.6443e-01)	Acc@1  96.09 ( 94.21)	Acc@5 100.00 ( 99.95)
Epoch: [28][280/391]	Time  0.143 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0763e-01 (1.6471e-01)	Acc@1  96.09 ( 94.20)	Acc@5 100.00 ( 99.94)
Epoch: [28][290/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2820e-01 (1.6543e-01)	Acc@1  92.19 ( 94.18)	Acc@5  99.22 ( 99.94)
Epoch: [28][300/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0126e-01 (1.6616e-01)	Acc@1  93.75 ( 94.16)	Acc@5 100.00 ( 99.94)
Epoch: [28][310/391]	Time  0.144 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1947e-01 (1.6612e-01)	Acc@1  96.09 ( 94.16)	Acc@5 100.00 ( 99.94)
Epoch: [28][320/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3130e-01 (1.6624e-01)	Acc@1  94.53 ( 94.16)	Acc@5 100.00 ( 99.95)
Epoch: [28][330/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1214e-01 (1.6655e-01)	Acc@1  95.31 ( 94.13)	Acc@5 100.00 ( 99.95)
Epoch: [28][340/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2941e-01 (1.6700e-01)	Acc@1  94.53 ( 94.12)	Acc@5 100.00 ( 99.95)
Epoch: [28][350/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7628e-01 (1.6759e-01)	Acc@1  94.53 ( 94.10)	Acc@5 100.00 ( 99.95)
Epoch: [28][360/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7777e-01 (1.6788e-01)	Acc@1  95.31 ( 94.11)	Acc@5 100.00 ( 99.95)
Epoch: [28][370/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.8659e-02 (1.6762e-01)	Acc@1  95.31 ( 94.12)	Acc@5 100.00 ( 99.95)
Epoch: [28][380/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.001)	Loss 1.5639e-01 (1.6726e-01)	Acc@1  94.53 ( 94.14)	Acc@5 100.00 ( 99.94)
Epoch: [28][390/391]	Time  0.109 ( 0.138)	Data  0.001 ( 0.001)	Loss 1.6493e-01 (1.6738e-01)	Acc@1  97.50 ( 94.13)	Acc@5 100.00 ( 99.94)
## e[28] optimizer.zero_grad (sum) time: 0.6910455226898193
## e[28]       loss.backward (sum) time: 14.035711288452148
## e[28]      optimizer.step (sum) time: 14.69556212425232
## epoch[28] training(only) time: 54.23684000968933
# Switched to evaluate mode...
Test: [  0/100]	Time  0.199 ( 0.199)	Loss 2.3544e-01 (2.3544e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 3.1323e-01 (3.1233e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 3.4051e-01 (3.2231e-01)	Acc@1  90.00 ( 89.71)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.047 ( 0.053)	Loss 4.3782e-01 (3.4523e-01)	Acc@1  87.00 ( 89.23)	Acc@5  99.00 ( 99.71)
Test: [ 40/100]	Time  0.047 ( 0.052)	Loss 4.3538e-01 (3.4861e-01)	Acc@1  87.00 ( 89.49)	Acc@5  99.00 ( 99.61)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 2.2817e-01 (3.4606e-01)	Acc@1  93.00 ( 89.69)	Acc@5  99.00 ( 99.57)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 4.2978e-01 (3.4179e-01)	Acc@1  88.00 ( 89.67)	Acc@5 100.00 ( 99.64)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 5.2608e-01 (3.4233e-01)	Acc@1  85.00 ( 89.63)	Acc@5 100.00 ( 99.66)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.5650e-01 (3.4417e-01)	Acc@1  93.00 ( 89.54)	Acc@5 100.00 ( 99.65)
Test: [ 90/100]	Time  0.050 ( 0.050)	Loss 1.8508e-01 (3.3903e-01)	Acc@1  92.00 ( 89.64)	Acc@5 100.00 ( 99.65)
 * Acc@1 89.750 Acc@5 99.670
### epoch[28] execution time: 59.26347327232361
EPOCH 29
# current learning rate: 0.1
# Switched to train mode...
Epoch: [29][  0/391]	Time  0.292 ( 0.292)	Data  0.149 ( 0.149)	Loss 2.5770e-01 (2.5770e-01)	Acc@1  89.84 ( 89.84)	Acc@5  99.22 ( 99.22)
Epoch: [29][ 10/391]	Time  0.137 ( 0.151)	Data  0.001 ( 0.014)	Loss 2.0520e-01 (1.3665e-01)	Acc@1  94.53 ( 95.31)	Acc@5 100.00 ( 99.93)
Epoch: [29][ 20/391]	Time  0.137 ( 0.145)	Data  0.001 ( 0.008)	Loss 1.8957e-01 (1.3477e-01)	Acc@1  92.97 ( 95.50)	Acc@5 100.00 ( 99.96)
Epoch: [29][ 30/391]	Time  0.139 ( 0.143)	Data  0.001 ( 0.006)	Loss 1.7705e-01 (1.4388e-01)	Acc@1  94.53 ( 95.01)	Acc@5 100.00 ( 99.97)
Epoch: [29][ 40/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.005)	Loss 1.6732e-01 (1.4679e-01)	Acc@1  93.75 ( 94.78)	Acc@5 100.00 ( 99.98)
Epoch: [29][ 50/391]	Time  0.140 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.3729e-01 (1.4452e-01)	Acc@1  95.31 ( 94.91)	Acc@5 100.00 ( 99.97)
Epoch: [29][ 60/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.1453e-01 (1.4525e-01)	Acc@1  92.19 ( 94.88)	Acc@5 100.00 ( 99.97)
Epoch: [29][ 70/391]	Time  0.140 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.5908e-01 (1.4681e-01)	Acc@1  95.31 ( 94.81)	Acc@5 100.00 ( 99.98)
Epoch: [29][ 80/391]	Time  0.133 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.6852e-01 (1.4461e-01)	Acc@1  96.88 ( 94.89)	Acc@5 100.00 ( 99.98)
Epoch: [29][ 90/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.6477e-01 (1.4627e-01)	Acc@1  91.41 ( 94.79)	Acc@5 100.00 ( 99.98)
Epoch: [29][100/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5738e-01 (1.4539e-01)	Acc@1  93.75 ( 94.80)	Acc@5 100.00 ( 99.98)
Epoch: [29][110/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.4357e-02 (1.4473e-01)	Acc@1  98.44 ( 94.81)	Acc@5 100.00 ( 99.99)
Epoch: [29][120/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3538e-01 (1.4662e-01)	Acc@1  94.53 ( 94.76)	Acc@5 100.00 ( 99.97)
Epoch: [29][130/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8838e-01 (1.4740e-01)	Acc@1  95.31 ( 94.79)	Acc@5 100.00 ( 99.97)
Epoch: [29][140/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8410e-01 (1.4791e-01)	Acc@1  93.75 ( 94.78)	Acc@5 100.00 ( 99.96)
Epoch: [29][150/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4765e-01 (1.4849e-01)	Acc@1  96.88 ( 94.77)	Acc@5 100.00 ( 99.96)
Epoch: [29][160/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1611e-01 (1.4964e-01)	Acc@1  96.09 ( 94.76)	Acc@5 100.00 ( 99.95)
Epoch: [29][170/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2156e-01 (1.4988e-01)	Acc@1  96.09 ( 94.77)	Acc@5 100.00 ( 99.95)
Epoch: [29][180/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3660e-01 (1.4873e-01)	Acc@1  96.09 ( 94.81)	Acc@5 100.00 ( 99.96)
Epoch: [29][190/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4248e-01 (1.4928e-01)	Acc@1  92.19 ( 94.77)	Acc@5 100.00 ( 99.96)
Epoch: [29][200/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8212e-01 (1.5048e-01)	Acc@1  93.75 ( 94.74)	Acc@5  99.22 ( 99.95)
Epoch: [29][210/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1042e-01 (1.5140e-01)	Acc@1  93.75 ( 94.69)	Acc@5 100.00 ( 99.94)
Epoch: [29][220/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.4825e-02 (1.5128e-01)	Acc@1  96.88 ( 94.70)	Acc@5 100.00 ( 99.94)
Epoch: [29][230/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0522e-01 (1.5038e-01)	Acc@1  95.31 ( 94.73)	Acc@5 100.00 ( 99.94)
Epoch: [29][240/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1493e-01 (1.5116e-01)	Acc@1  96.88 ( 94.70)	Acc@5 100.00 ( 99.94)
Epoch: [29][250/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.5459e-01 (1.5193e-01)	Acc@1  91.41 ( 94.69)	Acc@5 100.00 ( 99.94)
Epoch: [29][260/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5910e-01 (1.5247e-01)	Acc@1  95.31 ( 94.69)	Acc@5 100.00 ( 99.94)
Epoch: [29][270/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9273e-01 (1.5269e-01)	Acc@1  94.53 ( 94.68)	Acc@5  99.22 ( 99.94)
Epoch: [29][280/391]	Time  0.148 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1456e-01 (1.5306e-01)	Acc@1  93.75 ( 94.69)	Acc@5 100.00 ( 99.94)
Epoch: [29][290/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5049e-01 (1.5354e-01)	Acc@1  93.75 ( 94.66)	Acc@5 100.00 ( 99.94)
Epoch: [29][300/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8659e-01 (1.5417e-01)	Acc@1  92.19 ( 94.62)	Acc@5 100.00 ( 99.94)
Epoch: [29][310/391]	Time  0.147 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0094e-01 (1.5494e-01)	Acc@1  95.31 ( 94.60)	Acc@5 100.00 ( 99.93)
Epoch: [29][320/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.6596e-01 (1.5664e-01)	Acc@1  92.97 ( 94.53)	Acc@5 100.00 ( 99.94)
Epoch: [29][330/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2227e-01 (1.5810e-01)	Acc@1  92.19 ( 94.49)	Acc@5 100.00 ( 99.94)
Epoch: [29][340/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7131e-01 (1.5787e-01)	Acc@1  95.31 ( 94.51)	Acc@5 100.00 ( 99.94)
Epoch: [29][350/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.001)	Loss 1.6341e-01 (1.5873e-01)	Acc@1  93.75 ( 94.48)	Acc@5 100.00 ( 99.94)
Epoch: [29][360/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.001)	Loss 1.6988e-01 (1.5968e-01)	Acc@1  92.97 ( 94.44)	Acc@5 100.00 ( 99.94)
Epoch: [29][370/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.001)	Loss 2.8875e-01 (1.6134e-01)	Acc@1  92.97 ( 94.41)	Acc@5 100.00 ( 99.94)
Epoch: [29][380/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.001)	Loss 1.9316e-01 (1.6176e-01)	Acc@1  93.75 ( 94.41)	Acc@5 100.00 ( 99.94)
Epoch: [29][390/391]	Time  0.111 ( 0.138)	Data  0.001 ( 0.001)	Loss 2.0718e-01 (1.6227e-01)	Acc@1  95.00 ( 94.39)	Acc@5  98.75 ( 99.94)
## e[29] optimizer.zero_grad (sum) time: 0.686730146408081
## e[29]       loss.backward (sum) time: 13.931706428527832
## e[29]      optimizer.step (sum) time: 14.63861870765686
## epoch[29] training(only) time: 54.062532901763916
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 2.7601e-01 (2.7601e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 3.7927e-01 (3.8142e-01)	Acc@1  90.00 ( 88.45)	Acc@5  99.00 ( 99.45)
Test: [ 20/100]	Time  0.048 ( 0.055)	Loss 3.9334e-01 (4.0925e-01)	Acc@1  88.00 ( 88.33)	Acc@5 100.00 ( 99.38)
Test: [ 30/100]	Time  0.050 ( 0.053)	Loss 4.2166e-01 (4.2624e-01)	Acc@1  89.00 ( 88.39)	Acc@5 100.00 ( 99.35)
Test: [ 40/100]	Time  0.046 ( 0.052)	Loss 4.1560e-01 (4.1937e-01)	Acc@1  90.00 ( 88.24)	Acc@5 100.00 ( 99.34)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.5671e-01 (4.1487e-01)	Acc@1  94.00 ( 88.25)	Acc@5  99.00 ( 99.29)
Test: [ 60/100]	Time  0.046 ( 0.051)	Loss 4.3037e-01 (4.0469e-01)	Acc@1  88.00 ( 88.39)	Acc@5  99.00 ( 99.36)
Test: [ 70/100]	Time  0.051 ( 0.050)	Loss 4.8635e-01 (3.9479e-01)	Acc@1  81.00 ( 88.48)	Acc@5 100.00 ( 99.41)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.9066e-01 (3.9273e-01)	Acc@1  93.00 ( 88.47)	Acc@5 100.00 ( 99.44)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 2.6881e-01 (3.8802e-01)	Acc@1  93.00 ( 88.64)	Acc@5 100.00 ( 99.49)
 * Acc@1 88.660 Acc@5 99.500
### epoch[29] execution time: 59.08795666694641
EPOCH 30
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [30][  0/391]	Time  0.302 ( 0.302)	Data  0.155 ( 0.155)	Loss 1.9067e-01 (1.9067e-01)	Acc@1  92.19 ( 92.19)	Acc@5 100.00 (100.00)
Epoch: [30][ 10/391]	Time  0.140 ( 0.153)	Data  0.001 ( 0.015)	Loss 9.2771e-02 (1.3203e-01)	Acc@1  97.66 ( 94.82)	Acc@5 100.00 (100.00)
Epoch: [30][ 20/391]	Time  0.139 ( 0.145)	Data  0.001 ( 0.008)	Loss 5.7592e-02 (1.2671e-01)	Acc@1  98.44 ( 95.65)	Acc@5 100.00 (100.00)
Epoch: [30][ 30/391]	Time  0.138 ( 0.143)	Data  0.001 ( 0.006)	Loss 1.0968e-01 (1.2535e-01)	Acc@1  96.09 ( 95.72)	Acc@5 100.00 (100.00)
Epoch: [30][ 40/391]	Time  0.140 ( 0.142)	Data  0.001 ( 0.005)	Loss 8.3862e-02 (1.1774e-01)	Acc@1  97.66 ( 96.02)	Acc@5 100.00 (100.00)
Epoch: [30][ 50/391]	Time  0.145 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.5351e-01 (1.1460e-01)	Acc@1  94.53 ( 96.06)	Acc@5 100.00 (100.00)
Epoch: [30][ 60/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.6399e-01 (1.1144e-01)	Acc@1  93.75 ( 96.20)	Acc@5 100.00 (100.00)
Epoch: [30][ 70/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 7.5571e-02 (1.1017e-01)	Acc@1  98.44 ( 96.29)	Acc@5 100.00 ( 99.99)
Epoch: [30][ 80/391]	Time  0.144 ( 0.140)	Data  0.001 ( 0.003)	Loss 7.0821e-02 (1.0832e-01)	Acc@1  98.44 ( 96.35)	Acc@5 100.00 ( 99.98)
Epoch: [30][ 90/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 7.7910e-02 (1.0692e-01)	Acc@1  96.88 ( 96.43)	Acc@5 100.00 ( 99.98)
Epoch: [30][100/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.2448e-01 (1.0533e-01)	Acc@1  96.88 ( 96.47)	Acc@5 100.00 ( 99.98)
Epoch: [30][110/391]	Time  0.144 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1277e-01 (1.0400e-01)	Acc@1  96.09 ( 96.52)	Acc@5 100.00 ( 99.99)
Epoch: [30][120/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.5641e-02 (1.0220e-01)	Acc@1  96.09 ( 96.55)	Acc@5 100.00 ( 99.99)
Epoch: [30][130/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.9850e-02 (1.0053e-01)	Acc@1  99.22 ( 96.63)	Acc@5 100.00 ( 99.99)
Epoch: [30][140/391]	Time  0.143 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.1011e-02 (9.9953e-02)	Acc@1  96.88 ( 96.64)	Acc@5 100.00 ( 99.99)
Epoch: [30][150/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.7344e-02 (9.9092e-02)	Acc@1  97.66 ( 96.70)	Acc@5 100.00 ( 99.98)
Epoch: [30][160/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.9430e-02 (9.7922e-02)	Acc@1  98.44 ( 96.74)	Acc@5 100.00 ( 99.99)
Epoch: [30][170/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.8740e-02 (9.6031e-02)	Acc@1  97.66 ( 96.82)	Acc@5 100.00 ( 99.99)
Epoch: [30][180/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.6478e-02 (9.4344e-02)	Acc@1  99.22 ( 96.89)	Acc@5 100.00 ( 99.99)
Epoch: [30][190/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.2230e-02 (9.3094e-02)	Acc@1  97.66 ( 96.94)	Acc@5 100.00 ( 99.99)
Epoch: [30][200/391]	Time  0.143 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5430e-01 (9.2840e-02)	Acc@1  94.53 ( 96.96)	Acc@5 100.00 ( 99.98)
Epoch: [30][210/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1411e-01 (9.1971e-02)	Acc@1  96.09 ( 96.99)	Acc@5 100.00 ( 99.99)
Epoch: [30][220/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.7485e-02 (9.1714e-02)	Acc@1  97.66 ( 97.00)	Acc@5 100.00 ( 99.99)
Epoch: [30][230/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.6005e-02 (9.0949e-02)	Acc@1  98.44 ( 97.03)	Acc@5 100.00 ( 99.99)
Epoch: [30][240/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4061e-01 (8.9935e-02)	Acc@1  95.31 ( 97.06)	Acc@5 100.00 ( 99.99)
Epoch: [30][250/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.5123e-02 (8.9569e-02)	Acc@1  98.44 ( 97.07)	Acc@5 100.00 ( 99.98)
Epoch: [30][260/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.3891e-02 (8.8963e-02)	Acc@1  97.66 ( 97.10)	Acc@5 100.00 ( 99.98)
Epoch: [30][270/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.6723e-02 (8.8237e-02)	Acc@1  98.44 ( 97.12)	Acc@5 100.00 ( 99.98)
Epoch: [30][280/391]	Time  0.143 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.1263e-02 (8.7737e-02)	Acc@1  98.44 ( 97.15)	Acc@5 100.00 ( 99.98)
Epoch: [30][290/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.7911e-02 (8.6900e-02)	Acc@1  96.88 ( 97.18)	Acc@5 100.00 ( 99.98)
Epoch: [30][300/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.4782e-02 (8.6020e-02)	Acc@1  97.66 ( 97.20)	Acc@5 100.00 ( 99.98)
Epoch: [30][310/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.5256e-02 (8.5162e-02)	Acc@1  96.09 ( 97.23)	Acc@5 100.00 ( 99.98)
Epoch: [30][320/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.5608e-02 (8.4499e-02)	Acc@1  97.66 ( 97.24)	Acc@5 100.00 ( 99.99)
Epoch: [30][330/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.2517e-02 (8.3680e-02)	Acc@1  96.88 ( 97.28)	Acc@5 100.00 ( 99.99)
Epoch: [30][340/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.5650e-02 (8.3343e-02)	Acc@1  99.22 ( 97.29)	Acc@5 100.00 ( 99.99)
Epoch: [30][350/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.7502e-02 (8.2835e-02)	Acc@1 100.00 ( 97.32)	Acc@5 100.00 ( 99.99)
Epoch: [30][360/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.001)	Loss 9.0635e-02 (8.2755e-02)	Acc@1  97.66 ( 97.33)	Acc@5 100.00 ( 99.99)
Epoch: [30][370/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.001)	Loss 8.7677e-02 (8.2213e-02)	Acc@1  96.88 ( 97.35)	Acc@5 100.00 ( 99.99)
Epoch: [30][380/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.001)	Loss 7.5199e-02 (8.1950e-02)	Acc@1  97.66 ( 97.36)	Acc@5 100.00 ( 99.99)
Epoch: [30][390/391]	Time  0.106 ( 0.138)	Data  0.001 ( 0.001)	Loss 5.8461e-02 (8.1643e-02)	Acc@1  97.50 ( 97.37)	Acc@5 100.00 ( 99.99)
## e[30] optimizer.zero_grad (sum) time: 0.6853578090667725
## e[30]       loss.backward (sum) time: 13.972792625427246
## e[30]      optimizer.step (sum) time: 14.646929740905762
## epoch[30] training(only) time: 54.09097766876221
# Switched to evaluate mode...
Test: [  0/100]	Time  0.204 ( 0.204)	Loss 2.1607e-01 (2.1607e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.062)	Loss 2.3414e-01 (2.2474e-01)	Acc@1  92.00 ( 92.91)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 2.5399e-01 (2.5097e-01)	Acc@1  90.00 ( 92.24)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.050 ( 0.053)	Loss 2.3427e-01 (2.5873e-01)	Acc@1  91.00 ( 92.16)	Acc@5 100.00 ( 99.81)
Test: [ 40/100]	Time  0.046 ( 0.052)	Loss 2.3878e-01 (2.5405e-01)	Acc@1  93.00 ( 92.27)	Acc@5 100.00 ( 99.80)
Test: [ 50/100]	Time  0.056 ( 0.051)	Loss 1.8044e-01 (2.4921e-01)	Acc@1  93.00 ( 92.37)	Acc@5 100.00 ( 99.80)
Test: [ 60/100]	Time  0.050 ( 0.050)	Loss 2.4638e-01 (2.4258e-01)	Acc@1  91.00 ( 92.41)	Acc@5 100.00 ( 99.84)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 3.6188e-01 (2.3934e-01)	Acc@1  91.00 ( 92.46)	Acc@5 100.00 ( 99.83)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.2417e-01 (2.3810e-01)	Acc@1  96.00 ( 92.42)	Acc@5 100.00 ( 99.84)
Test: [ 90/100]	Time  0.049 ( 0.049)	Loss 1.5327e-01 (2.3230e-01)	Acc@1  93.00 ( 92.65)	Acc@5 100.00 ( 99.85)
 * Acc@1 92.680 Acc@5 99.860
### epoch[30] execution time: 59.08934426307678
EPOCH 31
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [31][  0/391]	Time  0.309 ( 0.309)	Data  0.154 ( 0.154)	Loss 6.0394e-02 (6.0394e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [31][ 10/391]	Time  0.140 ( 0.153)	Data  0.001 ( 0.015)	Loss 5.3352e-02 (6.9430e-02)	Acc@1  97.66 ( 97.59)	Acc@5 100.00 ( 99.93)
Epoch: [31][ 20/391]	Time  0.145 ( 0.146)	Data  0.001 ( 0.008)	Loss 6.3997e-02 (5.8708e-02)	Acc@1  98.44 ( 98.21)	Acc@5 100.00 ( 99.96)
Epoch: [31][ 30/391]	Time  0.138 ( 0.144)	Data  0.001 ( 0.006)	Loss 3.9238e-02 (5.7091e-02)	Acc@1  99.22 ( 98.36)	Acc@5 100.00 ( 99.97)
Epoch: [31][ 40/391]	Time  0.134 ( 0.142)	Data  0.001 ( 0.005)	Loss 5.3245e-02 (5.7863e-02)	Acc@1  99.22 ( 98.44)	Acc@5 100.00 ( 99.98)
Epoch: [31][ 50/391]	Time  0.136 ( 0.142)	Data  0.001 ( 0.004)	Loss 4.8955e-02 (5.8300e-02)	Acc@1  98.44 ( 98.30)	Acc@5 100.00 ( 99.98)
Epoch: [31][ 60/391]	Time  0.145 ( 0.141)	Data  0.001 ( 0.004)	Loss 3.4871e-02 (5.8397e-02)	Acc@1  99.22 ( 98.32)	Acc@5 100.00 ( 99.99)
Epoch: [31][ 70/391]	Time  0.140 ( 0.141)	Data  0.001 ( 0.003)	Loss 3.4782e-02 (5.6894e-02)	Acc@1 100.00 ( 98.38)	Acc@5 100.00 ( 99.99)
Epoch: [31][ 80/391]	Time  0.140 ( 0.140)	Data  0.001 ( 0.003)	Loss 6.7561e-02 (5.5018e-02)	Acc@1  98.44 ( 98.47)	Acc@5 100.00 ( 99.99)
Epoch: [31][ 90/391]	Time  0.133 ( 0.140)	Data  0.001 ( 0.003)	Loss 5.7574e-02 (5.4889e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 ( 99.99)
Epoch: [31][100/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.003)	Loss 3.8363e-02 (5.5163e-02)	Acc@1  97.66 ( 98.41)	Acc@5 100.00 ( 99.99)
Epoch: [31][110/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.002)	Loss 3.4833e-02 (5.5192e-02)	Acc@1  99.22 ( 98.37)	Acc@5 100.00 ( 99.99)
Epoch: [31][120/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.3532e-02 (5.6082e-02)	Acc@1  98.44 ( 98.31)	Acc@5 100.00 ( 99.99)
Epoch: [31][130/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.1849e-02 (5.6246e-02)	Acc@1  99.22 ( 98.31)	Acc@5 100.00 ( 99.99)
Epoch: [31][140/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.4437e-02 (5.6619e-02)	Acc@1  97.66 ( 98.29)	Acc@5 100.00 ( 99.99)
Epoch: [31][150/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1868e-02 (5.5981e-02)	Acc@1 100.00 ( 98.32)	Acc@5 100.00 ( 99.99)
Epoch: [31][160/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.3558e-02 (5.5353e-02)	Acc@1  96.88 ( 98.34)	Acc@5 100.00 (100.00)
Epoch: [31][170/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.1885e-02 (5.5422e-02)	Acc@1  98.44 ( 98.33)	Acc@5 100.00 (100.00)
Epoch: [31][180/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.4167e-02 (5.5624e-02)	Acc@1 100.00 ( 98.33)	Acc@5 100.00 (100.00)
Epoch: [31][190/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.6152e-02 (5.5304e-02)	Acc@1 100.00 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [31][200/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.9093e-02 (5.5500e-02)	Acc@1  97.66 ( 98.34)	Acc@5 100.00 (100.00)
Epoch: [31][210/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2166e-01 (5.5839e-02)	Acc@1  94.53 ( 98.32)	Acc@5 100.00 (100.00)
Epoch: [31][220/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.4959e-02 (5.5401e-02)	Acc@1  99.22 ( 98.35)	Acc@5 100.00 (100.00)
Epoch: [31][230/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.0037e-02 (5.5507e-02)	Acc@1  97.66 ( 98.33)	Acc@5 100.00 (100.00)
Epoch: [31][240/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.5500e-02 (5.5301e-02)	Acc@1  98.44 ( 98.33)	Acc@5 100.00 (100.00)
Epoch: [31][250/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.9393e-02 (5.4966e-02)	Acc@1  99.22 ( 98.34)	Acc@5 100.00 (100.00)
Epoch: [31][260/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.3942e-02 (5.5492e-02)	Acc@1  96.09 ( 98.31)	Acc@5 100.00 ( 99.99)
Epoch: [31][270/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.5714e-02 (5.5449e-02)	Acc@1  97.66 ( 98.30)	Acc@5 100.00 ( 99.99)
Epoch: [31][280/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.5001e-02 (5.5349e-02)	Acc@1  97.66 ( 98.30)	Acc@5 100.00 ( 99.99)
Epoch: [31][290/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.3449e-02 (5.4968e-02)	Acc@1  97.66 ( 98.31)	Acc@5 100.00 ( 99.99)
Epoch: [31][300/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.3878e-02 (5.5067e-02)	Acc@1 100.00 ( 98.32)	Acc@5 100.00 ( 99.99)
Epoch: [31][310/391]	Time  0.151 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.1086e-02 (5.5003e-02)	Acc@1  97.66 ( 98.32)	Acc@5 100.00 ( 99.99)
Epoch: [31][320/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.9511e-02 (5.4364e-02)	Acc@1  99.22 ( 98.35)	Acc@5 100.00 (100.00)
Epoch: [31][330/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.9688e-02 (5.4107e-02)	Acc@1  98.44 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [31][340/391]	Time  0.151 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.0131e-02 (5.3826e-02)	Acc@1  96.88 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [31][350/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0617e-02 (5.3734e-02)	Acc@1  99.22 ( 98.38)	Acc@5 100.00 (100.00)
Epoch: [31][360/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.2002e-02 (5.3633e-02)	Acc@1  99.22 ( 98.38)	Acc@5 100.00 (100.00)
Epoch: [31][370/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.4681e-02 (5.3476e-02)	Acc@1 100.00 ( 98.40)	Acc@5 100.00 (100.00)
Epoch: [31][380/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.001)	Loss 7.2629e-02 (5.3479e-02)	Acc@1  97.66 ( 98.39)	Acc@5 100.00 (100.00)
Epoch: [31][390/391]	Time  0.108 ( 0.138)	Data  0.001 ( 0.001)	Loss 4.4475e-02 (5.3125e-02)	Acc@1  98.75 ( 98.40)	Acc@5 100.00 (100.00)
## e[31] optimizer.zero_grad (sum) time: 0.6834537982940674
## e[31]       loss.backward (sum) time: 13.979216814041138
## e[31]      optimizer.step (sum) time: 14.61247968673706
## epoch[31] training(only) time: 54.105241537094116
# Switched to evaluate mode...
Test: [  0/100]	Time  0.179 ( 0.179)	Loss 2.3594e-01 (2.3594e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.049 ( 0.060)	Loss 2.1385e-01 (2.2583e-01)	Acc@1  93.00 ( 93.09)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 2.8382e-01 (2.5563e-01)	Acc@1  90.00 ( 92.52)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.048 ( 0.052)	Loss 2.5057e-01 (2.6242e-01)	Acc@1  93.00 ( 92.39)	Acc@5 100.00 ( 99.81)
Test: [ 40/100]	Time  0.049 ( 0.051)	Loss 2.5715e-01 (2.6229e-01)	Acc@1  93.00 ( 92.41)	Acc@5 100.00 ( 99.78)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.5191e-01 (2.5596e-01)	Acc@1  93.00 ( 92.45)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 2.6116e-01 (2.4848e-01)	Acc@1  92.00 ( 92.56)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 3.5337e-01 (2.4466e-01)	Acc@1  93.00 ( 92.61)	Acc@5 100.00 ( 99.80)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 1.3219e-01 (2.4251e-01)	Acc@1  96.00 ( 92.67)	Acc@5 100.00 ( 99.81)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.7472e-01 (2.3715e-01)	Acc@1  94.00 ( 92.80)	Acc@5 100.00 ( 99.82)
 * Acc@1 92.770 Acc@5 99.840
### epoch[31] execution time: 59.08554673194885
EPOCH 32
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [32][  0/391]	Time  0.296 ( 0.296)	Data  0.151 ( 0.151)	Loss 9.4603e-02 (9.4603e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [32][ 10/391]	Time  0.139 ( 0.152)	Data  0.001 ( 0.015)	Loss 7.3126e-02 (5.6386e-02)	Acc@1  96.09 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [32][ 20/391]	Time  0.140 ( 0.145)	Data  0.001 ( 0.008)	Loss 1.0440e-01 (5.6685e-02)	Acc@1  97.66 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [32][ 30/391]	Time  0.134 ( 0.143)	Data  0.001 ( 0.006)	Loss 5.7992e-02 (5.2113e-02)	Acc@1  98.44 ( 98.49)	Acc@5 100.00 (100.00)
Epoch: [32][ 40/391]	Time  0.140 ( 0.142)	Data  0.001 ( 0.005)	Loss 3.9802e-02 (4.8962e-02)	Acc@1  98.44 ( 98.61)	Acc@5 100.00 (100.00)
Epoch: [32][ 50/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.004)	Loss 5.5818e-02 (4.6690e-02)	Acc@1  97.66 ( 98.71)	Acc@5 100.00 (100.00)
Epoch: [32][ 60/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.004)	Loss 4.9281e-02 (4.5254e-02)	Acc@1  99.22 ( 98.74)	Acc@5 100.00 (100.00)
Epoch: [32][ 70/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 4.8264e-02 (4.3919e-02)	Acc@1  98.44 ( 98.78)	Acc@5 100.00 (100.00)
Epoch: [32][ 80/391]	Time  0.143 ( 0.140)	Data  0.001 ( 0.003)	Loss 6.2562e-02 (4.3479e-02)	Acc@1  98.44 ( 98.79)	Acc@5 100.00 (100.00)
Epoch: [32][ 90/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.6733e-02 (4.4578e-02)	Acc@1 100.00 ( 98.75)	Acc@5 100.00 (100.00)
Epoch: [32][100/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.5221e-02 (4.4545e-02)	Acc@1  99.22 ( 98.73)	Acc@5 100.00 (100.00)
Epoch: [32][110/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.002)	Loss 7.0561e-02 (4.4235e-02)	Acc@1  96.88 ( 98.74)	Acc@5 100.00 (100.00)
Epoch: [32][120/391]	Time  0.140 ( 0.140)	Data  0.001 ( 0.002)	Loss 2.8168e-02 (4.4759e-02)	Acc@1  99.22 ( 98.70)	Acc@5 100.00 (100.00)
Epoch: [32][130/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.9677e-02 (4.5142e-02)	Acc@1 100.00 ( 98.69)	Acc@5 100.00 (100.00)
Epoch: [32][140/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.9340e-02 (4.4259e-02)	Acc@1  97.66 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [32][150/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.5862e-02 (4.3937e-02)	Acc@1  98.44 ( 98.73)	Acc@5 100.00 (100.00)
Epoch: [32][160/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.5988e-02 (4.3732e-02)	Acc@1  99.22 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [32][170/391]	Time  0.144 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2917e-02 (4.3571e-02)	Acc@1 100.00 ( 98.74)	Acc@5 100.00 (100.00)
Epoch: [32][180/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.2444e-02 (4.3649e-02)	Acc@1  99.22 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [32][190/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.0369e-02 (4.3858e-02)	Acc@1  99.22 ( 98.70)	Acc@5 100.00 (100.00)
Epoch: [32][200/391]	Time  0.143 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2563e-02 (4.3466e-02)	Acc@1  99.22 ( 98.71)	Acc@5 100.00 (100.00)
Epoch: [32][210/391]	Time  0.145 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.9929e-02 (4.3321e-02)	Acc@1 100.00 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [32][220/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.6737e-02 (4.3241e-02)	Acc@1  96.88 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [32][230/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.9969e-02 (4.3079e-02)	Acc@1 100.00 ( 98.73)	Acc@5 100.00 (100.00)
Epoch: [32][240/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.4450e-02 (4.2739e-02)	Acc@1  99.22 ( 98.74)	Acc@5 100.00 (100.00)
Epoch: [32][250/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.1894e-02 (4.2523e-02)	Acc@1  99.22 ( 98.75)	Acc@5 100.00 (100.00)
Epoch: [32][260/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.6433e-02 (4.2261e-02)	Acc@1  99.22 ( 98.76)	Acc@5 100.00 (100.00)
Epoch: [32][270/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.1365e-02 (4.2042e-02)	Acc@1  98.44 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [32][280/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.0322e-02 (4.1671e-02)	Acc@1  99.22 ( 98.78)	Acc@5 100.00 (100.00)
Epoch: [32][290/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.3215e-02 (4.1714e-02)	Acc@1 100.00 ( 98.78)	Acc@5 100.00 (100.00)
Epoch: [32][300/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.6383e-02 (4.1695e-02)	Acc@1  97.66 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [32][310/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.2793e-02 (4.1538e-02)	Acc@1  98.44 ( 98.78)	Acc@5 100.00 (100.00)
Epoch: [32][320/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.8540e-02 (4.1541e-02)	Acc@1  96.88 ( 98.76)	Acc@5 100.00 (100.00)
Epoch: [32][330/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.7139e-02 (4.1471e-02)	Acc@1  99.22 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [32][340/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.2434e-02 (4.1557e-02)	Acc@1  98.44 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [32][350/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5699e-02 (4.1567e-02)	Acc@1  99.22 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [32][360/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.001)	Loss 3.6849e-02 (4.1664e-02)	Acc@1  99.22 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [32][370/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.001)	Loss 6.2331e-02 (4.1597e-02)	Acc@1  99.22 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [32][380/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.001)	Loss 2.2800e-02 (4.1485e-02)	Acc@1  99.22 ( 98.78)	Acc@5 100.00 (100.00)
Epoch: [32][390/391]	Time  0.111 ( 0.138)	Data  0.001 ( 0.001)	Loss 6.4448e-02 (4.1787e-02)	Acc@1  98.75 ( 98.77)	Acc@5 100.00 (100.00)
## e[32] optimizer.zero_grad (sum) time: 0.6870155334472656
## e[32]       loss.backward (sum) time: 13.990846872329712
## e[32]      optimizer.step (sum) time: 14.653079271316528
## epoch[32] training(only) time: 54.172034740448
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 2.2817e-01 (2.2817e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.060)	Loss 2.0932e-01 (2.1713e-01)	Acc@1  92.00 ( 93.09)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 3.0304e-01 (2.4941e-01)	Acc@1  89.00 ( 92.62)	Acc@5 100.00 ( 99.90)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 2.2907e-01 (2.5787e-01)	Acc@1  93.00 ( 92.58)	Acc@5 100.00 ( 99.87)
Test: [ 40/100]	Time  0.046 ( 0.052)	Loss 2.6190e-01 (2.5733e-01)	Acc@1  90.00 ( 92.56)	Acc@5 100.00 ( 99.85)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.7191e-01 (2.5265e-01)	Acc@1  93.00 ( 92.67)	Acc@5 100.00 ( 99.84)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 2.5964e-01 (2.4486e-01)	Acc@1  92.00 ( 92.82)	Acc@5 100.00 ( 99.87)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 3.8366e-01 (2.3994e-01)	Acc@1  92.00 ( 92.92)	Acc@5 100.00 ( 99.87)
Test: [ 80/100]	Time  0.050 ( 0.050)	Loss 1.2929e-01 (2.3859e-01)	Acc@1  95.00 ( 92.98)	Acc@5 100.00 ( 99.89)
Test: [ 90/100]	Time  0.046 ( 0.050)	Loss 1.4161e-01 (2.3445e-01)	Acc@1  95.00 ( 93.07)	Acc@5 100.00 ( 99.89)
 * Acc@1 93.080 Acc@5 99.900
### epoch[32] execution time: 59.22609829902649
EPOCH 33
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [33][  0/391]	Time  0.310 ( 0.310)	Data  0.150 ( 0.150)	Loss 2.2433e-02 (2.2433e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [33][ 10/391]	Time  0.139 ( 0.152)	Data  0.001 ( 0.015)	Loss 1.4559e-02 (3.2243e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [33][ 20/391]	Time  0.133 ( 0.145)	Data  0.001 ( 0.008)	Loss 2.6400e-02 (2.8529e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [33][ 30/391]	Time  0.145 ( 0.143)	Data  0.001 ( 0.006)	Loss 4.8163e-02 (3.4157e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [33][ 40/391]	Time  0.136 ( 0.142)	Data  0.001 ( 0.005)	Loss 3.9149e-02 (3.3618e-02)	Acc@1  99.22 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [33][ 50/391]	Time  0.134 ( 0.141)	Data  0.001 ( 0.004)	Loss 4.0463e-02 (3.3352e-02)	Acc@1  98.44 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [33][ 60/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.004)	Loss 4.0683e-02 (3.4559e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [33][ 70/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.6808e-02 (3.4829e-02)	Acc@1 100.00 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [33][ 80/391]	Time  0.142 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.6383e-02 (3.3840e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [33][ 90/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.7617e-02 (3.4081e-02)	Acc@1  99.22 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [33][100/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.003)	Loss 4.3284e-02 (3.4155e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [33][110/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.8433e-02 (3.3807e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [33][120/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.5946e-02 (3.4307e-02)	Acc@1  97.66 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [33][130/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.2418e-02 (3.4233e-02)	Acc@1  97.66 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [33][140/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.8192e-02 (3.4722e-02)	Acc@1  98.44 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [33][150/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.1703e-02 (3.4933e-02)	Acc@1  98.44 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [33][160/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.3348e-02 (3.5377e-02)	Acc@1 100.00 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [33][170/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5166e-02 (3.5268e-02)	Acc@1 100.00 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [33][180/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.0147e-02 (3.4873e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [33][190/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.3319e-02 (3.4726e-02)	Acc@1 100.00 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [33][200/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.9836e-02 (3.4942e-02)	Acc@1  98.44 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [33][210/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.7736e-02 (3.5571e-02)	Acc@1  99.22 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [33][220/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.5012e-02 (3.6204e-02)	Acc@1  99.22 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [33][230/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.2814e-03 (3.6138e-02)	Acc@1 100.00 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [33][240/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.0970e-02 (3.6243e-02)	Acc@1 100.00 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [33][250/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.4607e-02 (3.6685e-02)	Acc@1  98.44 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [33][260/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9036e-02 (3.6252e-02)	Acc@1 100.00 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [33][270/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.8510e-02 (3.6812e-02)	Acc@1  98.44 ( 98.92)	Acc@5 100.00 (100.00)
Epoch: [33][280/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.6827e-02 (3.6821e-02)	Acc@1  98.44 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [33][290/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.9593e-02 (3.6794e-02)	Acc@1  99.22 ( 98.92)	Acc@5 100.00 (100.00)
Epoch: [33][300/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.6854e-02 (3.6746e-02)	Acc@1  99.22 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [33][310/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1102e-02 (3.6918e-02)	Acc@1 100.00 ( 98.91)	Acc@5 100.00 (100.00)
Epoch: [33][320/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.0562e-02 (3.7178e-02)	Acc@1  98.44 ( 98.90)	Acc@5 100.00 (100.00)
Epoch: [33][330/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.4512e-02 (3.7064e-02)	Acc@1  99.22 ( 98.91)	Acc@5 100.00 (100.00)
Epoch: [33][340/391]	Time  0.148 ( 0.138)	Data  0.001 ( 0.001)	Loss 3.2775e-02 (3.7012e-02)	Acc@1  99.22 ( 98.91)	Acc@5 100.00 (100.00)
Epoch: [33][350/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.001)	Loss 6.9927e-02 (3.6958e-02)	Acc@1  97.66 ( 98.92)	Acc@5 100.00 (100.00)
Epoch: [33][360/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.001)	Loss 4.7003e-02 (3.6931e-02)	Acc@1  98.44 ( 98.92)	Acc@5 100.00 (100.00)
Epoch: [33][370/391]	Time  0.147 ( 0.138)	Data  0.001 ( 0.001)	Loss 2.2763e-02 (3.6889e-02)	Acc@1 100.00 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [33][380/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.001)	Loss 4.6831e-02 (3.6711e-02)	Acc@1  97.66 ( 98.94)	Acc@5 100.00 (100.00)
Epoch: [33][390/391]	Time  0.110 ( 0.138)	Data  0.001 ( 0.001)	Loss 1.8419e-02 (3.7092e-02)	Acc@1 100.00 ( 98.93)	Acc@5 100.00 (100.00)
## e[33] optimizer.zero_grad (sum) time: 0.6820635795593262
## e[33]       loss.backward (sum) time: 13.986956596374512
## e[33]      optimizer.step (sum) time: 14.601089715957642
## epoch[33] training(only) time: 54.109429597854614
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 1.9159e-01 (1.9159e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.050 ( 0.061)	Loss 2.1251e-01 (2.1944e-01)	Acc@1  93.00 ( 93.09)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 2.9417e-01 (2.5150e-01)	Acc@1  91.00 ( 92.48)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.049 ( 0.053)	Loss 2.2479e-01 (2.6303e-01)	Acc@1  93.00 ( 92.39)	Acc@5 100.00 ( 99.81)
Test: [ 40/100]	Time  0.050 ( 0.052)	Loss 2.5684e-01 (2.6268e-01)	Acc@1  91.00 ( 92.44)	Acc@5 100.00 ( 99.78)
Test: [ 50/100]	Time  0.049 ( 0.051)	Loss 1.5692e-01 (2.5941e-01)	Acc@1  94.00 ( 92.47)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 2.8749e-01 (2.5244e-01)	Acc@1  92.00 ( 92.52)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.049 ( 0.050)	Loss 4.3954e-01 (2.4744e-01)	Acc@1  91.00 ( 92.62)	Acc@5 100.00 ( 99.80)
Test: [ 80/100]	Time  0.049 ( 0.050)	Loss 1.3994e-01 (2.4636e-01)	Acc@1  96.00 ( 92.64)	Acc@5 100.00 ( 99.81)
Test: [ 90/100]	Time  0.046 ( 0.050)	Loss 1.8255e-01 (2.4070e-01)	Acc@1  93.00 ( 92.82)	Acc@5 100.00 ( 99.82)
 * Acc@1 92.850 Acc@5 99.840
### epoch[33] execution time: 59.134883642196655
EPOCH 34
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [34][  0/391]	Time  0.305 ( 0.305)	Data  0.158 ( 0.158)	Loss 6.0786e-02 (6.0786e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [34][ 10/391]	Time  0.138 ( 0.153)	Data  0.001 ( 0.015)	Loss 3.7374e-02 (3.7357e-02)	Acc@1  98.44 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [34][ 20/391]	Time  0.136 ( 0.146)	Data  0.001 ( 0.009)	Loss 2.9983e-02 (3.2363e-02)	Acc@1  98.44 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [34][ 30/391]	Time  0.138 ( 0.143)	Data  0.001 ( 0.006)	Loss 1.7092e-02 (3.1775e-02)	Acc@1 100.00 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [34][ 40/391]	Time  0.140 ( 0.142)	Data  0.001 ( 0.005)	Loss 1.9252e-02 (3.1613e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [34][ 50/391]	Time  0.138 ( 0.141)	Data  0.001 ( 0.004)	Loss 3.3587e-02 (3.2387e-02)	Acc@1  98.44 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [34][ 60/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.9995e-02 (3.2585e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [34][ 70/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.3412e-02 (3.2175e-02)	Acc@1 100.00 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [34][ 80/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.8517e-02 (3.1196e-02)	Acc@1  98.44 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [34][ 90/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 4.9310e-02 (3.2795e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [34][100/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.2430e-02 (3.2524e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [34][110/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.2822e-02 (3.2780e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [34][120/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.3013e-02 (3.2919e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [34][130/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.3931e-02 (3.2873e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [34][140/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1856e-02 (3.2916e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [34][150/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.7887e-03 (3.3116e-02)	Acc@1 100.00 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [34][160/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.0254e-02 (3.2569e-02)	Acc@1  98.44 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [34][170/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.2260e-02 (3.2370e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [34][180/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.1454e-02 (3.2295e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [34][190/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1062e-02 (3.2296e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [34][200/391]	Time  0.146 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8189e-02 (3.2328e-02)	Acc@1 100.00 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [34][210/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6436e-02 (3.2293e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [34][220/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.7503e-02 (3.2402e-02)	Acc@1  97.66 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [34][230/391]	Time  0.143 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.0217e-03 (3.2403e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [34][240/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.4855e-02 (3.2638e-02)	Acc@1  97.66 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [34][250/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.3672e-02 (3.2409e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [34][260/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.1278e-02 (3.2598e-02)	Acc@1  97.66 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [34][270/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.7647e-02 (3.2788e-02)	Acc@1  99.22 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [34][280/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.9322e-02 (3.2936e-02)	Acc@1  97.66 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [34][290/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6972e-02 (3.2725e-02)	Acc@1 100.00 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [34][300/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.5902e-02 (3.2679e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [34][310/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.7966e-02 (3.2932e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [34][320/391]	Time  0.146 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.4163e-02 (3.3003e-02)	Acc@1  96.88 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [34][330/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.1618e-02 (3.2873e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [34][340/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.6306e-02 (3.2797e-02)	Acc@1  98.44 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [34][350/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.5270e-03 (3.2725e-02)	Acc@1 100.00 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [34][360/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0702e-02 (3.2537e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [34][370/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.2896e-02 (3.2572e-02)	Acc@1  98.44 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [34][380/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2804e-02 (3.2464e-02)	Acc@1 100.00 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [34][390/391]	Time  0.110 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.6989e-02 (3.2710e-02)	Acc@1  98.75 ( 99.02)	Acc@5 100.00 (100.00)
## e[34] optimizer.zero_grad (sum) time: 0.6901626586914062
## e[34]       loss.backward (sum) time: 13.966902017593384
## e[34]      optimizer.step (sum) time: 14.6143057346344
## epoch[34] training(only) time: 54.0529625415802
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 2.4762e-01 (2.4762e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 2.1585e-01 (2.1563e-01)	Acc@1  93.00 ( 93.36)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 3.1334e-01 (2.5659e-01)	Acc@1  91.00 ( 92.67)	Acc@5 100.00 ( 99.90)
Test: [ 30/100]	Time  0.050 ( 0.053)	Loss 1.8960e-01 (2.6568e-01)	Acc@1  95.00 ( 92.68)	Acc@5 100.00 ( 99.84)
Test: [ 40/100]	Time  0.046 ( 0.052)	Loss 2.5243e-01 (2.6642e-01)	Acc@1  91.00 ( 92.71)	Acc@5 100.00 ( 99.80)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.6405e-01 (2.6513e-01)	Acc@1  94.00 ( 92.61)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.049 ( 0.051)	Loss 2.4943e-01 (2.5717e-01)	Acc@1  92.00 ( 92.72)	Acc@5 100.00 ( 99.82)
Test: [ 70/100]	Time  0.051 ( 0.050)	Loss 4.2191e-01 (2.5242e-01)	Acc@1  91.00 ( 92.76)	Acc@5 100.00 ( 99.82)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.3710e-01 (2.5349e-01)	Acc@1  96.00 ( 92.68)	Acc@5 100.00 ( 99.83)
Test: [ 90/100]	Time  0.046 ( 0.050)	Loss 1.9942e-01 (2.4821e-01)	Acc@1  93.00 ( 92.81)	Acc@5 100.00 ( 99.84)
 * Acc@1 92.880 Acc@5 99.850
### epoch[34] execution time: 59.10074234008789
EPOCH 35
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [35][  0/391]	Time  0.299 ( 0.299)	Data  0.158 ( 0.158)	Loss 3.0030e-02 (3.0030e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [35][ 10/391]	Time  0.134 ( 0.153)	Data  0.001 ( 0.015)	Loss 4.1345e-02 (3.0585e-02)	Acc@1  98.44 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [35][ 20/391]	Time  0.136 ( 0.146)	Data  0.001 ( 0.009)	Loss 1.7941e-02 (3.0053e-02)	Acc@1  99.22 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [35][ 30/391]	Time  0.137 ( 0.144)	Data  0.001 ( 0.006)	Loss 2.7005e-02 (2.8189e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [35][ 40/391]	Time  0.137 ( 0.142)	Data  0.001 ( 0.005)	Loss 1.1310e-02 (2.5916e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [35][ 50/391]	Time  0.135 ( 0.141)	Data  0.001 ( 0.004)	Loss 6.4993e-02 (2.8564e-02)	Acc@1  98.44 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [35][ 60/391]	Time  0.146 ( 0.141)	Data  0.001 ( 0.004)	Loss 6.6356e-03 (2.7981e-02)	Acc@1 100.00 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [35][ 70/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.5743e-02 (2.8002e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [35][ 80/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 8.6433e-03 (2.7842e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [35][ 90/391]	Time  0.147 ( 0.140)	Data  0.001 ( 0.003)	Loss 4.9503e-02 (2.8075e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [35][100/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.3741e-02 (2.7780e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [35][110/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.9901e-02 (2.8008e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [35][120/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.9300e-02 (2.8543e-02)	Acc@1  98.44 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [35][130/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.0327e-02 (2.8725e-02)	Acc@1  96.88 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [35][140/391]	Time  0.143 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.4154e-02 (2.9297e-02)	Acc@1  97.66 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [35][150/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1102e-02 (2.9198e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [35][160/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.7973e-03 (2.9169e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [35][170/391]	Time  0.143 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.4477e-02 (2.9088e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [35][180/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7866e-02 (2.8854e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [35][190/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.0229e-02 (2.8741e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [35][200/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.2655e-02 (2.8875e-02)	Acc@1  97.66 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [35][210/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1460e-02 (2.8805e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [35][220/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.2587e-02 (2.8528e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [35][230/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.7013e-02 (2.8498e-02)	Acc@1  98.44 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [35][240/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.9251e-02 (2.8152e-02)	Acc@1  99.22 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [35][250/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.3779e-02 (2.8058e-02)	Acc@1  98.44 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [35][260/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.2353e-02 (2.8236e-02)	Acc@1  98.44 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [35][270/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0348e-02 (2.8005e-02)	Acc@1 100.00 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [35][280/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1876e-02 (2.7883e-02)	Acc@1 100.00 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [35][290/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5388e-02 (2.7990e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [35][300/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.7580e-02 (2.8344e-02)	Acc@1  98.44 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [35][310/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2942e-02 (2.8176e-02)	Acc@1 100.00 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [35][320/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.0498e-02 (2.8216e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [35][330/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1971e-02 (2.8203e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [35][340/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.5370e-03 (2.8225e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [35][350/391]	Time  0.144 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3682e-02 (2.7885e-02)	Acc@1  99.22 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [35][360/391]	Time  0.143 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.7694e-03 (2.7952e-02)	Acc@1 100.00 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [35][370/391]	Time  0.149 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0015e-02 (2.7963e-02)	Acc@1 100.00 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [35][380/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.5279e-02 (2.8145e-02)	Acc@1  97.66 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [35][390/391]	Time  0.106 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6849e-02 (2.8179e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
## e[35] optimizer.zero_grad (sum) time: 0.6830272674560547
## e[35]       loss.backward (sum) time: 13.999878168106079
## e[35]      optimizer.step (sum) time: 14.599005937576294
## epoch[35] training(only) time: 54.11304950714111
# Switched to evaluate mode...
Test: [  0/100]	Time  0.192 ( 0.192)	Loss 2.3232e-01 (2.3232e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.057 ( 0.062)	Loss 2.0125e-01 (2.1874e-01)	Acc@1  93.00 ( 93.18)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 3.2929e-01 (2.5774e-01)	Acc@1  91.00 ( 92.71)	Acc@5 100.00 ( 99.90)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 2.5791e-01 (2.6610e-01)	Acc@1  94.00 ( 92.65)	Acc@5 100.00 ( 99.84)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 2.4313e-01 (2.6875e-01)	Acc@1  92.00 ( 92.63)	Acc@5 100.00 ( 99.80)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 1.6748e-01 (2.6581e-01)	Acc@1  96.00 ( 92.67)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.056 ( 0.050)	Loss 2.7908e-01 (2.5905e-01)	Acc@1  93.00 ( 92.84)	Acc@5 100.00 ( 99.82)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 4.3758e-01 (2.5361e-01)	Acc@1  91.00 ( 92.93)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.2297e-01 (2.5320e-01)	Acc@1  97.00 ( 92.93)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.049 ( 0.049)	Loss 1.9996e-01 (2.4818e-01)	Acc@1  93.00 ( 93.01)	Acc@5 100.00 ( 99.80)
 * Acc@1 93.040 Acc@5 99.820
### epoch[35] execution time: 59.11703968048096
EPOCH 36
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [36][  0/391]	Time  0.303 ( 0.303)	Data  0.152 ( 0.152)	Loss 1.7075e-02 (1.7075e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [36][ 10/391]	Time  0.138 ( 0.154)	Data  0.001 ( 0.015)	Loss 5.4869e-02 (2.8386e-02)	Acc@1  97.66 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [36][ 20/391]	Time  0.135 ( 0.146)	Data  0.001 ( 0.008)	Loss 3.7393e-02 (2.7030e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [36][ 30/391]	Time  0.136 ( 0.143)	Data  0.001 ( 0.006)	Loss 2.0307e-02 (2.4948e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [36][ 40/391]	Time  0.139 ( 0.142)	Data  0.001 ( 0.005)	Loss 9.2656e-03 (2.3458e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [36][ 50/391]	Time  0.140 ( 0.141)	Data  0.001 ( 0.004)	Loss 2.3636e-02 (2.4847e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [36][ 60/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.7402e-02 (2.5402e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [36][ 70/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.003)	Loss 8.0011e-03 (2.4006e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [36][ 80/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 6.3608e-03 (2.4503e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [36][ 90/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.6306e-02 (2.3877e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [36][100/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 9.2629e-03 (2.3604e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [36][110/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.6608e-02 (2.3389e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [36][120/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6355e-02 (2.3079e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [36][130/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0697e-02 (2.3028e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [36][140/391]	Time  0.143 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.3084e-02 (2.3910e-02)	Acc@1  98.44 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [36][150/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5573e-02 (2.4109e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [36][160/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.5344e-02 (2.4183e-02)	Acc@1  98.44 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [36][170/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.3085e-02 (2.4496e-02)	Acc@1  97.66 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [36][180/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.2370e-02 (2.4876e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [36][190/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.2669e-02 (2.4934e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [36][200/391]	Time  0.142 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.9071e-02 (2.4961e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [36][210/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.6876e-02 (2.4991e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [36][220/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1359e-02 (2.4921e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [36][230/391]	Time  0.144 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.5498e-02 (2.5032e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [36][240/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7821e-02 (2.5341e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [36][250/391]	Time  0.149 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7481e-02 (2.5675e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [36][260/391]	Time  0.145 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.9077e-02 (2.5881e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [36][270/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.0018e-02 (2.6278e-02)	Acc@1  97.66 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [36][280/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.4319e-03 (2.6232e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [36][290/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.4705e-02 (2.6334e-02)	Acc@1  98.44 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [36][300/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.8089e-02 (2.6174e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [36][310/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7635e-02 (2.6065e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [36][320/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0642e-02 (2.5840e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [36][330/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8643e-02 (2.5837e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [36][340/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0395e-02 (2.6090e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [36][350/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1639e-02 (2.6112e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [36][360/391]	Time  0.147 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.6226e-03 (2.5944e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [36][370/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7886e-02 (2.5773e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [36][380/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.2592e-03 (2.5769e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [36][390/391]	Time  0.110 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0071e-02 (2.5667e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
## e[36] optimizer.zero_grad (sum) time: 0.6847541332244873
## e[36]       loss.backward (sum) time: 13.970735788345337
## e[36]      optimizer.step (sum) time: 14.620052814483643
## epoch[36] training(only) time: 54.08752083778381
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 2.1479e-01 (2.1479e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.049 ( 0.063)	Loss 1.8579e-01 (2.1861e-01)	Acc@1  94.00 ( 93.36)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.047 ( 0.056)	Loss 2.7262e-01 (2.5690e-01)	Acc@1  92.00 ( 92.57)	Acc@5 100.00 ( 99.90)
Test: [ 30/100]	Time  0.050 ( 0.054)	Loss 2.1806e-01 (2.6398e-01)	Acc@1  93.00 ( 92.55)	Acc@5 100.00 ( 99.77)
Test: [ 40/100]	Time  0.046 ( 0.052)	Loss 2.6403e-01 (2.6773e-01)	Acc@1  90.00 ( 92.49)	Acc@5 100.00 ( 99.78)
Test: [ 50/100]	Time  0.049 ( 0.052)	Loss 1.6665e-01 (2.6451e-01)	Acc@1  95.00 ( 92.61)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.046 ( 0.051)	Loss 2.9985e-01 (2.5761e-01)	Acc@1  93.00 ( 92.69)	Acc@5 100.00 ( 99.82)
Test: [ 70/100]	Time  0.046 ( 0.051)	Loss 4.5618e-01 (2.5391e-01)	Acc@1  90.00 ( 92.75)	Acc@5 100.00 ( 99.80)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.5459e-01 (2.5215e-01)	Acc@1  96.00 ( 92.75)	Acc@5 100.00 ( 99.81)
Test: [ 90/100]	Time  0.046 ( 0.050)	Loss 1.8966e-01 (2.4572e-01)	Acc@1  93.00 ( 92.91)	Acc@5 100.00 ( 99.82)
 * Acc@1 92.960 Acc@5 99.840
### epoch[36] execution time: 59.163376808166504
EPOCH 37
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [37][  0/391]	Time  0.293 ( 0.293)	Data  0.150 ( 0.150)	Loss 1.1338e-02 (1.1338e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [37][ 10/391]	Time  0.136 ( 0.151)	Data  0.001 ( 0.015)	Loss 1.3995e-02 (1.7138e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [37][ 20/391]	Time  0.138 ( 0.145)	Data  0.001 ( 0.008)	Loss 2.1654e-02 (1.8472e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [37][ 30/391]	Time  0.137 ( 0.143)	Data  0.001 ( 0.006)	Loss 4.4886e-02 (2.0863e-02)	Acc@1  98.44 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [37][ 40/391]	Time  0.137 ( 0.142)	Data  0.001 ( 0.005)	Loss 3.2739e-02 (2.1299e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [37][ 50/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.004)	Loss 2.8322e-02 (2.1177e-02)	Acc@1  98.44 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [37][ 60/391]	Time  0.134 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.3555e-02 (2.1858e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [37][ 70/391]	Time  0.139 ( 0.141)	Data  0.001 ( 0.003)	Loss 3.3475e-02 (2.1578e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 ( 99.99)
Epoch: [37][ 80/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.6773e-02 (2.1061e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 ( 99.99)
Epoch: [37][ 90/391]	Time  0.147 ( 0.140)	Data  0.001 ( 0.003)	Loss 8.4905e-03 (2.1508e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 ( 99.99)
Epoch: [37][100/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.7912e-02 (2.1421e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 ( 99.99)
Epoch: [37][110/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.002)	Loss 2.9044e-02 (2.1381e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [37][120/391]	Time  0.145 ( 0.140)	Data  0.001 ( 0.002)	Loss 4.2765e-02 (2.1586e-02)	Acc@1  97.66 ( 99.41)	Acc@5 100.00 ( 99.99)
Epoch: [37][130/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1504e-02 (2.1296e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 ( 99.99)
Epoch: [37][140/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.8228e-02 (2.1476e-02)	Acc@1  98.44 ( 99.43)	Acc@5 100.00 ( 99.99)
Epoch: [37][150/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0893e-02 (2.1516e-02)	Acc@1  98.44 ( 99.41)	Acc@5 100.00 ( 99.99)
Epoch: [37][160/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.4215e-02 (2.1371e-02)	Acc@1  97.66 ( 99.40)	Acc@5 100.00 ( 99.99)
Epoch: [37][170/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0360e-02 (2.1196e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 ( 99.99)
Epoch: [37][180/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3515e-02 (2.1056e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 ( 99.99)
Epoch: [37][190/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5895e-02 (2.0971e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 ( 99.99)
Epoch: [37][200/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.9684e-02 (2.1033e-02)	Acc@1  98.44 ( 99.41)	Acc@5 100.00 ( 99.99)
Epoch: [37][210/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0666e-02 (2.1295e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 ( 99.99)
Epoch: [37][220/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.7043e-03 (2.1029e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 ( 99.99)
Epoch: [37][230/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.2769e-02 (2.1228e-02)	Acc@1  97.66 ( 99.40)	Acc@5 100.00 ( 99.99)
Epoch: [37][240/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.8345e-03 (2.1358e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 ( 99.99)
Epoch: [37][250/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0604e-02 (2.1641e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 ( 99.99)
Epoch: [37][260/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.0207e-02 (2.1819e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 ( 99.99)
Epoch: [37][270/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.6605e-02 (2.1738e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 ( 99.99)
Epoch: [37][280/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.6805e-02 (2.1549e-02)	Acc@1  96.88 ( 99.40)	Acc@5 100.00 ( 99.99)
Epoch: [37][290/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9071e-02 (2.1386e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 ( 99.99)
Epoch: [37][300/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.8372e-02 (2.1285e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 ( 99.99)
Epoch: [37][310/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9947e-02 (2.1236e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 ( 99.99)
Epoch: [37][320/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.2105e-03 (2.1106e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 ( 99.99)
Epoch: [37][330/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.5900e-03 (2.0967e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 ( 99.99)
Epoch: [37][340/391]	Time  0.149 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.8899e-02 (2.1195e-02)	Acc@1  97.66 ( 99.41)	Acc@5 100.00 ( 99.99)
Epoch: [37][350/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.6259e-02 (2.1309e-02)	Acc@1  98.44 ( 99.41)	Acc@5 100.00 ( 99.99)
Epoch: [37][360/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0634e-02 (2.1323e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 ( 99.99)
Epoch: [37][370/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3485e-02 (2.1310e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 ( 99.99)
Epoch: [37][380/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.5600e-02 (2.1265e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 ( 99.99)
Epoch: [37][390/391]	Time  0.111 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.0580e-02 (2.1163e-02)	Acc@1  98.75 ( 99.43)	Acc@5 100.00 ( 99.99)
## e[37] optimizer.zero_grad (sum) time: 0.6835367679595947
## e[37]       loss.backward (sum) time: 13.969985961914062
## e[37]      optimizer.step (sum) time: 14.605529308319092
## epoch[37] training(only) time: 54.09599280357361
# Switched to evaluate mode...
Test: [  0/100]	Time  0.194 ( 0.194)	Loss 2.2744e-01 (2.2744e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 2.1323e-01 (2.3054e-01)	Acc@1  95.00 ( 93.36)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 3.2252e-01 (2.6644e-01)	Acc@1  91.00 ( 92.81)	Acc@5 100.00 ( 99.90)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 2.3200e-01 (2.7344e-01)	Acc@1  95.00 ( 92.87)	Acc@5 100.00 ( 99.84)
Test: [ 40/100]	Time  0.049 ( 0.052)	Loss 2.2863e-01 (2.7710e-01)	Acc@1  91.00 ( 92.63)	Acc@5 100.00 ( 99.83)
Test: [ 50/100]	Time  0.049 ( 0.051)	Loss 1.7668e-01 (2.7559e-01)	Acc@1  96.00 ( 92.51)	Acc@5 100.00 ( 99.82)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 2.7360e-01 (2.6809e-01)	Acc@1  94.00 ( 92.59)	Acc@5 100.00 ( 99.85)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 4.7901e-01 (2.6399e-01)	Acc@1  91.00 ( 92.69)	Acc@5 100.00 ( 99.82)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.5424e-01 (2.6351e-01)	Acc@1  97.00 ( 92.67)	Acc@5 100.00 ( 99.81)
Test: [ 90/100]	Time  0.046 ( 0.050)	Loss 1.9515e-01 (2.5715e-01)	Acc@1  93.00 ( 92.89)	Acc@5 100.00 ( 99.84)
 * Acc@1 92.930 Acc@5 99.850
### epoch[37] execution time: 59.14493656158447
EPOCH 38
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [38][  0/391]	Time  0.303 ( 0.303)	Data  0.162 ( 0.162)	Loss 4.0754e-02 (4.0754e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [38][ 10/391]	Time  0.136 ( 0.153)	Data  0.001 ( 0.016)	Loss 1.3925e-02 (1.7264e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [38][ 20/391]	Time  0.137 ( 0.145)	Data  0.001 ( 0.009)	Loss 2.0184e-02 (1.4733e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [38][ 30/391]	Time  0.139 ( 0.143)	Data  0.001 ( 0.006)	Loss 1.3922e-02 (1.6744e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [38][ 40/391]	Time  0.140 ( 0.142)	Data  0.001 ( 0.005)	Loss 1.2232e-02 (1.8218e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [38][ 50/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.004)	Loss 2.5811e-02 (1.8977e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [38][ 60/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.004)	Loss 6.7634e-03 (1.7933e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [38][ 70/391]	Time  0.140 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.5221e-02 (1.7477e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [38][ 80/391]	Time  0.140 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.3208e-02 (1.7318e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [38][ 90/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 3.1864e-02 (1.8285e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [38][100/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.003)	Loss 3.3855e-02 (1.8638e-02)	Acc@1  98.44 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [38][110/391]	Time  0.146 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.5431e-02 (1.8827e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [38][120/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.8876e-03 (1.8976e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 ( 99.99)
Epoch: [38][130/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.4281e-02 (1.9390e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 ( 99.99)
Epoch: [38][140/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.9036e-03 (1.9468e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 ( 99.99)
Epoch: [38][150/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.9240e-02 (1.9594e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 ( 99.99)
Epoch: [38][160/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.5058e-02 (1.9414e-02)	Acc@1  97.66 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [38][170/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.3981e-02 (1.9660e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [38][180/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.3218e-03 (1.9594e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [38][190/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2388e-02 (1.9484e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [38][200/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6202e-02 (1.9395e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [38][210/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.6470e-02 (1.9442e-02)	Acc@1  98.44 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [38][220/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.3200e-03 (1.9224e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [38][230/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1433e-02 (1.8898e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [38][240/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.8083e-03 (1.8887e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [38][250/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3078e-02 (1.9091e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [38][260/391]	Time  0.149 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.3169e-03 (1.9119e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [38][270/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.2585e-03 (1.9180e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [38][280/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.5478e-02 (1.9213e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [38][290/391]	Time  0.148 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.5030e-02 (1.9356e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [38][300/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4100e-02 (1.9419e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [38][310/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.5744e-03 (1.9197e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [38][320/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.1717e-02 (1.9356e-02)	Acc@1  98.44 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [38][330/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.7532e-02 (1.9657e-02)	Acc@1  97.66 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [38][340/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.4554e-02 (1.9772e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [38][350/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6859e-02 (1.9994e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [38][360/391]	Time  0.143 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.2825e-02 (1.9897e-02)	Acc@1  98.44 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [38][370/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0712e-02 (1.9789e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [38][380/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.2439e-03 (1.9953e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [38][390/391]	Time  0.112 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.8102e-02 (2.0070e-02)	Acc@1  98.75 ( 99.46)	Acc@5 100.00 (100.00)
## e[38] optimizer.zero_grad (sum) time: 0.6859323978424072
## e[38]       loss.backward (sum) time: 13.949243068695068
## e[38]      optimizer.step (sum) time: 14.628864765167236
## epoch[38] training(only) time: 54.1007194519043
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 2.0009e-01 (2.0009e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 2.3988e-01 (2.3630e-01)	Acc@1  94.00 ( 93.18)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.047 ( 0.056)	Loss 3.1782e-01 (2.7121e-01)	Acc@1  92.00 ( 92.52)	Acc@5 100.00 ( 99.90)
Test: [ 30/100]	Time  0.052 ( 0.053)	Loss 2.4989e-01 (2.7986e-01)	Acc@1  93.00 ( 92.39)	Acc@5 100.00 ( 99.84)
Test: [ 40/100]	Time  0.048 ( 0.052)	Loss 2.6092e-01 (2.8079e-01)	Acc@1  93.00 ( 92.49)	Acc@5 100.00 ( 99.85)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.9837e-01 (2.8012e-01)	Acc@1  95.00 ( 92.55)	Acc@5 100.00 ( 99.82)
Test: [ 60/100]	Time  0.049 ( 0.050)	Loss 3.3441e-01 (2.7788e-01)	Acc@1  94.00 ( 92.62)	Acc@5 100.00 ( 99.85)
Test: [ 70/100]	Time  0.045 ( 0.050)	Loss 5.1040e-01 (2.7319e-01)	Acc@1  92.00 ( 92.68)	Acc@5 100.00 ( 99.83)
Test: [ 80/100]	Time  0.047 ( 0.050)	Loss 1.3802e-01 (2.7117e-01)	Acc@1  96.00 ( 92.67)	Acc@5 100.00 ( 99.85)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.7907e-01 (2.6424e-01)	Acc@1  94.00 ( 92.82)	Acc@5 100.00 ( 99.87)
 * Acc@1 92.860 Acc@5 99.870
### epoch[38] execution time: 59.11365365982056
EPOCH 39
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [39][  0/391]	Time  0.304 ( 0.304)	Data  0.159 ( 0.159)	Loss 2.9622e-02 (2.9622e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [39][ 10/391]	Time  0.134 ( 0.152)	Data  0.001 ( 0.015)	Loss 5.0582e-02 (2.4056e-02)	Acc@1  98.44 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [39][ 20/391]	Time  0.139 ( 0.146)	Data  0.001 ( 0.009)	Loss 1.5390e-02 (2.3276e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [39][ 30/391]	Time  0.138 ( 0.143)	Data  0.001 ( 0.006)	Loss 1.0293e-02 (2.0825e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [39][ 40/391]	Time  0.137 ( 0.142)	Data  0.001 ( 0.005)	Loss 1.4145e-02 (2.0830e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [39][ 50/391]	Time  0.134 ( 0.141)	Data  0.001 ( 0.004)	Loss 2.3498e-02 (2.2077e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [39][ 60/391]	Time  0.139 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.4161e-02 (2.0921e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [39][ 70/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.7580e-02 (2.0385e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [39][ 80/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 8.7010e-03 (1.9727e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [39][ 90/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.3275e-02 (2.0141e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [39][100/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.6842e-02 (2.0300e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [39][110/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.2755e-02 (1.9851e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [39][120/391]	Time  0.148 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.9013e-02 (2.0010e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [39][130/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.4017e-02 (1.9683e-02)	Acc@1  98.44 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [39][140/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6679e-02 (1.9590e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [39][150/391]	Time  0.147 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.5356e-02 (1.9375e-02)	Acc@1  97.66 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [39][160/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.4082e-02 (1.9091e-02)	Acc@1  97.66 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [39][170/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.3098e-03 (1.9590e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [39][180/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.6430e-03 (1.9380e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [39][190/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.9713e-02 (1.9128e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [39][200/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2191e-02 (1.9236e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [39][210/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0610e-02 (1.9278e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [39][220/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.9097e-03 (1.9179e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [39][230/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.9367e-03 (1.9113e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [39][240/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.9078e-02 (1.9389e-02)	Acc@1  98.44 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [39][250/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6905e-02 (1.9352e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [39][260/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.8847e-02 (1.9394e-02)	Acc@1  98.44 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [39][270/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.8784e-03 (1.9542e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [39][280/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6067e-02 (1.9388e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [39][290/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.7247e-02 (1.9082e-02)	Acc@1  98.44 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [39][300/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1197e-02 (1.9277e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [39][310/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0271e-02 (1.9234e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [39][320/391]	Time  0.147 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.7874e-02 (1.9015e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [39][330/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9495e-02 (1.8902e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [39][340/391]	Time  0.144 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.1517e-02 (1.8837e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [39][350/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.6696e-02 (1.8703e-02)	Acc@1  98.44 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [39][360/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.2382e-03 (1.8631e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [39][370/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.5779e-02 (1.8848e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [39][380/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.8094e-02 (1.8883e-02)	Acc@1  98.44 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [39][390/391]	Time  0.111 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7180e-02 (1.8893e-02)	Acc@1  98.75 ( 99.49)	Acc@5 100.00 (100.00)
## e[39] optimizer.zero_grad (sum) time: 0.6834962368011475
## e[39]       loss.backward (sum) time: 13.959739208221436
## e[39]      optimizer.step (sum) time: 14.615802526473999
## epoch[39] training(only) time: 54.12810015678406
# Switched to evaluate mode...
Test: [  0/100]	Time  0.190 ( 0.190)	Loss 2.7150e-01 (2.7150e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 2.5853e-01 (2.3842e-01)	Acc@1  94.00 ( 93.45)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.049 ( 0.056)	Loss 3.7156e-01 (2.7590e-01)	Acc@1  91.00 ( 93.00)	Acc@5 100.00 ( 99.90)
Test: [ 30/100]	Time  0.048 ( 0.054)	Loss 2.6686e-01 (2.8317e-01)	Acc@1  93.00 ( 92.94)	Acc@5 100.00 ( 99.84)
Test: [ 40/100]	Time  0.049 ( 0.052)	Loss 2.5151e-01 (2.8583e-01)	Acc@1  91.00 ( 92.85)	Acc@5 100.00 ( 99.80)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.9480e-01 (2.8383e-01)	Acc@1  95.00 ( 92.86)	Acc@5 100.00 ( 99.84)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 3.3155e-01 (2.7912e-01)	Acc@1  94.00 ( 92.93)	Acc@5 100.00 ( 99.87)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 4.6900e-01 (2.7197e-01)	Acc@1  92.00 ( 93.00)	Acc@5 100.00 ( 99.83)
Test: [ 80/100]	Time  0.047 ( 0.050)	Loss 1.3354e-01 (2.7023e-01)	Acc@1  97.00 ( 92.95)	Acc@5 100.00 ( 99.84)
Test: [ 90/100]	Time  0.048 ( 0.050)	Loss 2.0435e-01 (2.6403e-01)	Acc@1  94.00 ( 93.07)	Acc@5 100.00 ( 99.85)
 * Acc@1 93.080 Acc@5 99.860
### epoch[39] execution time: 59.173335552215576
EPOCH 40
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [40][  0/391]	Time  0.304 ( 0.304)	Data  0.159 ( 0.159)	Loss 2.6670e-02 (2.6670e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [40][ 10/391]	Time  0.143 ( 0.151)	Data  0.001 ( 0.015)	Loss 2.2763e-02 (2.3119e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [40][ 20/391]	Time  0.134 ( 0.144)	Data  0.001 ( 0.009)	Loss 1.1703e-02 (2.3411e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [40][ 30/391]	Time  0.134 ( 0.142)	Data  0.001 ( 0.006)	Loss 8.0690e-03 (2.0158e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [40][ 40/391]	Time  0.136 ( 0.141)	Data  0.001 ( 0.005)	Loss 9.9241e-03 (1.9508e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [40][ 50/391]	Time  0.141 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.6925e-02 (1.8555e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [40][ 60/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.004)	Loss 1.1402e-02 (1.8590e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [40][ 70/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.003)	Loss 8.3401e-03 (1.8176e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [40][ 80/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 8.7348e-03 (1.7752e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [40][ 90/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.003)	Loss 7.2251e-03 (1.7291e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [40][100/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.003)	Loss 3.4740e-02 (1.7419e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [40][110/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 7.0254e-03 (1.7062e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [40][120/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.7531e-03 (1.6692e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [40][130/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0027e-02 (1.6441e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [40][140/391]	Time  0.144 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4129e-02 (1.6728e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [40][150/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1273e-02 (1.6862e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [40][160/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.3181e-02 (1.6960e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [40][170/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3917e-02 (1.6875e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [40][180/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1157e-02 (1.6837e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [40][190/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0036e-02 (1.7033e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [40][200/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.5755e-03 (1.7231e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [40][210/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0575e-02 (1.7251e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [40][220/391]	Time  0.145 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.2942e-03 (1.7221e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [40][230/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.0525e-03 (1.6896e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [40][240/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8960e-02 (1.7132e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [40][250/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9457e-02 (1.6984e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [40][260/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.5928e-03 (1.6869e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [40][270/391]	Time  0.147 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2191e-02 (1.7375e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [40][280/391]	Time  0.143 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3329e-02 (1.7297e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [40][290/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5022e-02 (1.7393e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [40][300/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7104e-02 (1.7538e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [40][310/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9505e-02 (1.7706e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [40][320/391]	Time  0.143 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.3935e-02 (1.7759e-02)	Acc@1  98.44 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [40][330/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9566e-02 (1.7568e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [40][340/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.8521e-02 (1.7718e-02)	Acc@1  98.44 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [40][350/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.6395e-02 (1.7726e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [40][360/391]	Time  0.145 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1278e-02 (1.7715e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [40][370/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9029e-02 (1.7643e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [40][380/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.9978e-03 (1.7564e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [40][390/391]	Time  0.108 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7327e-02 (1.7520e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
## e[40] optimizer.zero_grad (sum) time: 0.6884808540344238
## e[40]       loss.backward (sum) time: 13.962656736373901
## e[40]      optimizer.step (sum) time: 14.618924617767334
## epoch[40] training(only) time: 54.09494233131409
# Switched to evaluate mode...
Test: [  0/100]	Time  0.190 ( 0.190)	Loss 2.4860e-01 (2.4860e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.063)	Loss 2.6241e-01 (2.4516e-01)	Acc@1  95.00 ( 93.36)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.054 ( 0.056)	Loss 3.3413e-01 (2.7362e-01)	Acc@1  93.00 ( 92.81)	Acc@5 100.00 ( 99.90)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 2.5872e-01 (2.8128e-01)	Acc@1  94.00 ( 92.74)	Acc@5  99.00 ( 99.81)
Test: [ 40/100]	Time  0.049 ( 0.052)	Loss 2.6236e-01 (2.8255e-01)	Acc@1  93.00 ( 92.73)	Acc@5 100.00 ( 99.80)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.7524e-01 (2.8404e-01)	Acc@1  93.00 ( 92.75)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 3.3511e-01 (2.8062e-01)	Acc@1  93.00 ( 92.77)	Acc@5 100.00 ( 99.82)
Test: [ 70/100]	Time  0.055 ( 0.050)	Loss 4.9631e-01 (2.7447e-01)	Acc@1  92.00 ( 92.90)	Acc@5 100.00 ( 99.80)
Test: [ 80/100]	Time  0.047 ( 0.050)	Loss 1.0182e-01 (2.7226e-01)	Acc@1  97.00 ( 92.96)	Acc@5 100.00 ( 99.81)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.6719e-01 (2.6604e-01)	Acc@1  93.00 ( 93.04)	Acc@5 100.00 ( 99.84)
 * Acc@1 93.070 Acc@5 99.850
### epoch[40] execution time: 59.09915781021118
EPOCH 41
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [41][  0/391]	Time  0.302 ( 0.302)	Data  0.158 ( 0.158)	Loss 1.3535e-02 (1.3535e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [41][ 10/391]	Time  0.137 ( 0.153)	Data  0.001 ( 0.015)	Loss 8.6391e-03 (1.1790e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [41][ 20/391]	Time  0.139 ( 0.146)	Data  0.001 ( 0.009)	Loss 7.9429e-03 (1.2054e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [41][ 30/391]	Time  0.137 ( 0.143)	Data  0.001 ( 0.006)	Loss 1.2689e-02 (1.3259e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [41][ 40/391]	Time  0.138 ( 0.142)	Data  0.001 ( 0.005)	Loss 3.6897e-03 (1.2526e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [41][ 50/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.7783e-02 (1.2572e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [41][ 60/391]	Time  0.139 ( 0.141)	Data  0.001 ( 0.004)	Loss 2.8137e-02 (1.4734e-02)	Acc@1  98.44 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [41][ 70/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 8.6191e-03 (1.4141e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [41][ 80/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 3.9285e-03 (1.4170e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [41][ 90/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.3208e-02 (1.5236e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [41][100/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.3510e-02 (1.5487e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [41][110/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.3064e-02 (1.5708e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [41][120/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.6890e-02 (1.5662e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [41][130/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3441e-02 (1.6068e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [41][140/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1903e-02 (1.6007e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [41][150/391]	Time  0.148 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3047e-02 (1.6186e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [41][160/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0693e-02 (1.6206e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [41][170/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.3242e-02 (1.6490e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [41][180/391]	Time  0.145 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0801e-02 (1.6463e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [41][190/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.8723e-03 (1.6477e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [41][200/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1901e-02 (1.6479e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [41][210/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.4347e-02 (1.6446e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [41][220/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0902e-02 (1.6576e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [41][230/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0277e-02 (1.6535e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [41][240/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8683e-02 (1.6568e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [41][250/391]	Time  0.149 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3437e-02 (1.6679e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [41][260/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1243e-02 (1.6608e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [41][270/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0128e-02 (1.6703e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [41][280/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2463e-02 (1.6617e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [41][290/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.1533e-03 (1.6666e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [41][300/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.5868e-03 (1.6801e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [41][310/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4044e-02 (1.6830e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [41][320/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7265e-02 (1.6818e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [41][330/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.6896e-03 (1.6884e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [41][340/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.5018e-02 (1.6916e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [41][350/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.4186e-03 (1.6776e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [41][360/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1419e-02 (1.6722e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [41][370/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.7623e-03 (1.6639e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [41][380/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.9725e-02 (1.6630e-02)	Acc@1  98.44 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [41][390/391]	Time  0.110 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.6062e-02 (1.6669e-02)	Acc@1  97.50 ( 99.53)	Acc@5 100.00 (100.00)
## e[41] optimizer.zero_grad (sum) time: 0.6835644245147705
## e[41]       loss.backward (sum) time: 13.990180730819702
## e[41]      optimizer.step (sum) time: 14.63864278793335
## epoch[41] training(only) time: 54.13450241088867
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 2.2311e-01 (2.2311e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 2.4856e-01 (2.4865e-01)	Acc@1  95.00 ( 93.45)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.047 ( 0.056)	Loss 3.3914e-01 (2.7900e-01)	Acc@1  92.00 ( 93.10)	Acc@5 100.00 ( 99.90)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 2.7777e-01 (2.8624e-01)	Acc@1  92.00 ( 92.81)	Acc@5 100.00 ( 99.84)
Test: [ 40/100]	Time  0.046 ( 0.052)	Loss 2.6093e-01 (2.8952e-01)	Acc@1  90.00 ( 92.66)	Acc@5 100.00 ( 99.83)
Test: [ 50/100]	Time  0.050 ( 0.052)	Loss 1.7670e-01 (2.8985e-01)	Acc@1  95.00 ( 92.65)	Acc@5 100.00 ( 99.80)
Test: [ 60/100]	Time  0.046 ( 0.051)	Loss 3.1736e-01 (2.8181e-01)	Acc@1  94.00 ( 92.84)	Acc@5 100.00 ( 99.84)
Test: [ 70/100]	Time  0.048 ( 0.051)	Loss 5.7201e-01 (2.7773e-01)	Acc@1  90.00 ( 92.90)	Acc@5 100.00 ( 99.83)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.4539e-01 (2.7788e-01)	Acc@1  95.00 ( 92.81)	Acc@5 100.00 ( 99.84)
Test: [ 90/100]	Time  0.046 ( 0.050)	Loss 2.0817e-01 (2.7244e-01)	Acc@1  94.00 ( 92.92)	Acc@5 100.00 ( 99.85)
 * Acc@1 92.940 Acc@5 99.860
### epoch[41] execution time: 59.187378883361816
EPOCH 42
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [42][  0/391]	Time  0.305 ( 0.305)	Data  0.157 ( 0.157)	Loss 1.0954e-02 (1.0954e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [42][ 10/391]	Time  0.150 ( 0.155)	Data  0.001 ( 0.015)	Loss 2.8124e-02 (1.8831e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [42][ 20/391]	Time  0.135 ( 0.147)	Data  0.001 ( 0.008)	Loss 7.4439e-03 (1.5684e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [42][ 30/391]	Time  0.136 ( 0.144)	Data  0.001 ( 0.006)	Loss 1.6487e-02 (1.5682e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [42][ 40/391]	Time  0.146 ( 0.143)	Data  0.001 ( 0.005)	Loss 2.6071e-02 (1.7808e-02)	Acc@1  97.66 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [42][ 50/391]	Time  0.139 ( 0.142)	Data  0.001 ( 0.004)	Loss 6.9202e-03 (1.6836e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [42][ 60/391]	Time  0.146 ( 0.142)	Data  0.001 ( 0.004)	Loss 9.2277e-03 (1.6976e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [42][ 70/391]	Time  0.134 ( 0.141)	Data  0.001 ( 0.003)	Loss 9.7340e-03 (1.6801e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [42][ 80/391]	Time  0.138 ( 0.141)	Data  0.001 ( 0.003)	Loss 8.1075e-03 (1.6800e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [42][ 90/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.2086e-02 (1.7149e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [42][100/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.0015e-02 (1.6684e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [42][110/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.2599e-02 (1.6337e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [42][120/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.002)	Loss 4.5219e-02 (1.6284e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [42][130/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.002)	Loss 1.7538e-02 (1.5957e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [42][140/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4214e-02 (1.5942e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [42][150/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0995e-02 (1.5708e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [42][160/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8965e-02 (1.5635e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [42][170/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.0654e-03 (1.5648e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [42][180/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.9046e-03 (1.5529e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [42][190/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4444e-02 (1.5388e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [42][200/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.2408e-03 (1.5188e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [42][210/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1944e-02 (1.5023e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [42][220/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.9850e-03 (1.5005e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [42][230/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0624e-02 (1.4956e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [42][240/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.0637e-03 (1.4839e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [42][250/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.6500e-03 (1.5097e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [42][260/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0080e-02 (1.5122e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [42][270/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4902e-02 (1.5104e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [42][280/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.9193e-02 (1.5029e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [42][290/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1406e-02 (1.4901e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [42][300/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.5748e-03 (1.4985e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [42][310/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.8435e-02 (1.5347e-02)	Acc@1  97.66 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [42][320/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.0479e-03 (1.5450e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [42][330/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.8254e-03 (1.5355e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [42][340/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.7680e-03 (1.5398e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [42][350/391]	Time  0.144 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2166e-02 (1.5334e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [42][360/391]	Time  0.145 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4691e-02 (1.5288e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [42][370/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.7118e-03 (1.5202e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [42][380/391]	Time  0.147 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8321e-02 (1.5472e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [42][390/391]	Time  0.114 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.4531e-03 (1.5432e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
## e[42] optimizer.zero_grad (sum) time: 0.6869502067565918
## e[42]       loss.backward (sum) time: 14.058639764785767
## e[42]      optimizer.step (sum) time: 14.64746618270874
## epoch[42] training(only) time: 54.33517289161682
# Switched to evaluate mode...
Test: [  0/100]	Time  0.196 ( 0.196)	Loss 2.7154e-01 (2.7154e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.062)	Loss 2.1135e-01 (2.5867e-01)	Acc@1  94.00 ( 93.09)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 3.6160e-01 (2.8793e-01)	Acc@1  91.00 ( 92.67)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 2.6585e-01 (2.9710e-01)	Acc@1  93.00 ( 92.65)	Acc@5 100.00 ( 99.77)
Test: [ 40/100]	Time  0.050 ( 0.052)	Loss 2.3448e-01 (2.9842e-01)	Acc@1  92.00 ( 92.63)	Acc@5 100.00 ( 99.78)
Test: [ 50/100]	Time  0.048 ( 0.051)	Loss 1.6518e-01 (2.9648e-01)	Acc@1  96.00 ( 92.61)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.046 ( 0.051)	Loss 3.2931e-01 (2.8960e-01)	Acc@1  93.00 ( 92.69)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.056 ( 0.050)	Loss 5.1840e-01 (2.8464e-01)	Acc@1  91.00 ( 92.89)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.5580e-01 (2.8521e-01)	Acc@1  96.00 ( 92.83)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.046 ( 0.050)	Loss 2.2007e-01 (2.7860e-01)	Acc@1  94.00 ( 92.88)	Acc@5 100.00 ( 99.82)
 * Acc@1 92.930 Acc@5 99.840
### epoch[42] execution time: 59.37643504142761
EPOCH 43
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [43][  0/391]	Time  0.311 ( 0.311)	Data  0.162 ( 0.162)	Loss 1.0784e-02 (1.0784e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [43][ 10/391]	Time  0.139 ( 0.155)	Data  0.001 ( 0.016)	Loss 1.2951e-02 (1.1348e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [43][ 20/391]	Time  0.135 ( 0.147)	Data  0.001 ( 0.009)	Loss 3.1125e-03 (1.1348e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [43][ 30/391]	Time  0.140 ( 0.144)	Data  0.001 ( 0.006)	Loss 6.7818e-03 (1.2104e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [43][ 40/391]	Time  0.136 ( 0.143)	Data  0.001 ( 0.005)	Loss 1.2183e-02 (1.2424e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [43][ 50/391]	Time  0.135 ( 0.142)	Data  0.001 ( 0.004)	Loss 1.1349e-02 (1.2408e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [43][ 60/391]	Time  0.135 ( 0.141)	Data  0.001 ( 0.004)	Loss 5.6132e-03 (1.1867e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [43][ 70/391]	Time  0.140 ( 0.141)	Data  0.001 ( 0.003)	Loss 1.2902e-02 (1.1776e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [43][ 80/391]	Time  0.140 ( 0.141)	Data  0.001 ( 0.003)	Loss 1.2120e-02 (1.1440e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [43][ 90/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.5782e-03 (1.1542e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [43][100/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 6.1544e-03 (1.2016e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [43][110/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 8.4429e-03 (1.2093e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [43][120/391]	Time  0.140 ( 0.140)	Data  0.001 ( 0.002)	Loss 1.5595e-02 (1.2143e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [43][130/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.6531e-02 (1.2289e-02)	Acc@1  98.44 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [43][140/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.2156e-03 (1.2092e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [43][150/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.5563e-02 (1.2186e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [43][160/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.4569e-03 (1.2094e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [43][170/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.3239e-03 (1.2287e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [43][180/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.6761e-03 (1.2168e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [43][190/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0977e-02 (1.2406e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [43][200/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5136e-02 (1.2392e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [43][210/391]	Time  0.149 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2193e-02 (1.2540e-02)	Acc@1  98.44 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [43][220/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6641e-02 (1.2387e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [43][230/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.8918e-03 (1.2274e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [43][240/391]	Time  0.147 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.4851e-03 (1.2201e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [43][250/391]	Time  0.143 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3330e-02 (1.2078e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [43][260/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.4513e-02 (1.2069e-02)	Acc@1  98.44 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [43][270/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1726e-02 (1.2152e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [43][280/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1190e-02 (1.2245e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [43][290/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.6503e-03 (1.2341e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [43][300/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7007e-02 (1.2555e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [43][310/391]	Time  0.144 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8661e-02 (1.2461e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [43][320/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.7904e-03 (1.2386e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [43][330/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.5530e-03 (1.2357e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [43][340/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2998e-02 (1.2363e-02)	Acc@1  98.44 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [43][350/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.9528e-02 (1.2389e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [43][360/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5595e-02 (1.2433e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [43][370/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.7576e-02 (1.2511e-02)	Acc@1  97.66 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [43][380/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.6255e-03 (1.2560e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [43][390/391]	Time  0.108 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2054e-02 (1.2702e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
## e[43] optimizer.zero_grad (sum) time: 0.6843922138214111
## e[43]       loss.backward (sum) time: 14.001741409301758
## e[43]      optimizer.step (sum) time: 14.648534536361694
## epoch[43] training(only) time: 54.20467662811279
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 2.7371e-01 (2.7371e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 2.5319e-01 (2.5413e-01)	Acc@1  94.00 ( 93.36)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.050 ( 0.056)	Loss 3.9772e-01 (2.8752e-01)	Acc@1  91.00 ( 92.71)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.047 ( 0.053)	Loss 2.5457e-01 (2.9544e-01)	Acc@1  93.00 ( 92.68)	Acc@5 100.00 ( 99.81)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 2.6531e-01 (2.9693e-01)	Acc@1  92.00 ( 92.59)	Acc@5 100.00 ( 99.83)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 1.6338e-01 (2.9461e-01)	Acc@1  96.00 ( 92.59)	Acc@5 100.00 ( 99.82)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 3.1192e-01 (2.8679e-01)	Acc@1  93.00 ( 92.70)	Acc@5 100.00 ( 99.85)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 5.6113e-01 (2.8304e-01)	Acc@1  91.00 ( 92.80)	Acc@5 100.00 ( 99.82)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.6158e-01 (2.8378e-01)	Acc@1  96.00 ( 92.77)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.050 ( 0.049)	Loss 2.1406e-01 (2.7815e-01)	Acc@1  94.00 ( 92.89)	Acc@5 100.00 ( 99.82)
 * Acc@1 92.950 Acc@5 99.840
### epoch[43] execution time: 59.23714995384216
EPOCH 44
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [44][  0/391]	Time  0.295 ( 0.295)	Data  0.152 ( 0.152)	Loss 1.5587e-02 (1.5587e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [44][ 10/391]	Time  0.136 ( 0.151)	Data  0.001 ( 0.015)	Loss 4.6356e-02 (1.2232e-02)	Acc@1  98.44 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [44][ 20/391]	Time  0.138 ( 0.145)	Data  0.001 ( 0.008)	Loss 4.7360e-03 (1.0031e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [44][ 30/391]	Time  0.137 ( 0.143)	Data  0.001 ( 0.006)	Loss 7.1100e-03 (1.0231e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [44][ 40/391]	Time  0.149 ( 0.142)	Data  0.001 ( 0.005)	Loss 2.7047e-03 (9.5466e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [44][ 50/391]	Time  0.139 ( 0.141)	Data  0.001 ( 0.004)	Loss 4.9317e-02 (1.0479e-02)	Acc@1  97.66 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [44][ 60/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.004)	Loss 1.1247e-02 (1.0285e-02)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [44][ 70/391]	Time  0.149 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.4667e-03 (1.0411e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [44][ 80/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 3.0476e-03 (1.0642e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [44][ 90/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 5.1453e-03 (1.0695e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [44][100/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.3448e-02 (1.0783e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [44][110/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.003)	Loss 8.9312e-03 (1.1017e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [44][120/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.3464e-03 (1.1136e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [44][130/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.7098e-03 (1.0969e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [44][140/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.8254e-03 (1.1041e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [44][150/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2777e-02 (1.1278e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [44][160/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5146e-02 (1.1363e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [44][170/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3169e-02 (1.1574e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [44][180/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.9606e-03 (1.1475e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [44][190/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2306e-02 (1.1287e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [44][200/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.0271e-02 (1.1255e-02)	Acc@1  98.44 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [44][210/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.2290e-02 (1.1369e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [44][220/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.3776e-03 (1.1520e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [44][230/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7681e-02 (1.1742e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [44][240/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7728e-02 (1.1801e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [44][250/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1721e-02 (1.1781e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [44][260/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.3123e-03 (1.1737e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [44][270/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8548e-02 (1.1705e-02)	Acc@1  98.44 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [44][280/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.0279e-03 (1.1611e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [44][290/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0729e-02 (1.1651e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [44][300/391]	Time  0.149 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.5068e-02 (1.1938e-02)	Acc@1  98.44 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [44][310/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1276e-02 (1.2231e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [44][320/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6240e-02 (1.2176e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [44][330/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.0742e-03 (1.2155e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [44][340/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.6462e-03 (1.2037e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [44][350/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.2267e-02 (1.2189e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [44][360/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7492e-02 (1.2154e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [44][370/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7804e-02 (1.2219e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [44][380/391]	Time  0.144 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.4987e-02 (1.2281e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [44][390/391]	Time  0.111 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.9658e-02 (1.2341e-02)	Acc@1  97.50 ( 99.71)	Acc@5 100.00 (100.00)
## e[44] optimizer.zero_grad (sum) time: 0.6844806671142578
## e[44]       loss.backward (sum) time: 14.025465488433838
## e[44]      optimizer.step (sum) time: 14.590585231781006
## epoch[44] training(only) time: 54.130958557128906
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 3.0731e-01 (3.0731e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.062)	Loss 2.2590e-01 (2.5725e-01)	Acc@1  95.00 ( 93.55)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 3.3970e-01 (2.7912e-01)	Acc@1  92.00 ( 93.00)	Acc@5 100.00 ( 99.90)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 2.5960e-01 (2.9294e-01)	Acc@1  93.00 ( 92.87)	Acc@5 100.00 ( 99.81)
Test: [ 40/100]	Time  0.051 ( 0.052)	Loss 2.2635e-01 (2.9894e-01)	Acc@1  93.00 ( 92.68)	Acc@5 100.00 ( 99.83)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.8656e-01 (2.9787e-01)	Acc@1  94.00 ( 92.63)	Acc@5 100.00 ( 99.82)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 3.3830e-01 (2.9154e-01)	Acc@1  94.00 ( 92.80)	Acc@5 100.00 ( 99.85)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 5.3060e-01 (2.8809e-01)	Acc@1  91.00 ( 92.89)	Acc@5 100.00 ( 99.82)
Test: [ 80/100]	Time  0.050 ( 0.050)	Loss 1.9466e-01 (2.8880e-01)	Acc@1  94.00 ( 92.80)	Acc@5 100.00 ( 99.83)
Test: [ 90/100]	Time  0.050 ( 0.050)	Loss 2.3795e-01 (2.8168e-01)	Acc@1  92.00 ( 92.89)	Acc@5 100.00 ( 99.85)
 * Acc@1 92.910 Acc@5 99.860
### epoch[44] execution time: 59.14414310455322
EPOCH 45
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [45][  0/391]	Time  0.307 ( 0.307)	Data  0.156 ( 0.156)	Loss 5.5472e-03 (5.5472e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [45][ 10/391]	Time  0.135 ( 0.154)	Data  0.001 ( 0.015)	Loss 7.2735e-03 (9.6405e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [45][ 20/391]	Time  0.141 ( 0.146)	Data  0.001 ( 0.009)	Loss 1.2800e-02 (1.3020e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [45][ 30/391]	Time  0.137 ( 0.143)	Data  0.001 ( 0.006)	Loss 1.1374e-02 (1.4579e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [45][ 40/391]	Time  0.134 ( 0.142)	Data  0.001 ( 0.005)	Loss 6.9473e-03 (1.4058e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [45][ 50/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.004)	Loss 5.6484e-03 (1.2707e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [45][ 60/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.2926e-02 (1.2216e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [45][ 70/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 8.8614e-03 (1.2497e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [45][ 80/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 5.3353e-03 (1.1947e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [45][ 90/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 7.8627e-03 (1.2276e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [45][100/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.6353e-02 (1.2272e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [45][110/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 3.6972e-02 (1.2652e-02)	Acc@1  98.44 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [45][120/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.1702e-02 (1.2553e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [45][130/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2705e-02 (1.2568e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [45][140/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.7051e-03 (1.2181e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [45][150/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8004e-03 (1.2224e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [45][160/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.5129e-03 (1.1939e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [45][170/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.8641e-03 (1.2027e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [45][180/391]	Time  0.144 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.8660e-03 (1.1988e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [45][190/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.2342e-03 (1.1906e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [45][200/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1903e-02 (1.1983e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [45][210/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2376e-02 (1.2242e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [45][220/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.9998e-03 (1.2326e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [45][230/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5619e-02 (1.2217e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [45][240/391]	Time  0.144 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.4545e-02 (1.2179e-02)	Acc@1  98.44 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [45][250/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.3414e-02 (1.2295e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [45][260/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.4240e-03 (1.2188e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [45][270/391]	Time  0.147 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.7515e-03 (1.2132e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [45][280/391]	Time  0.144 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.7612e-03 (1.2206e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [45][290/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2604e-02 (1.2142e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [45][300/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.4816e-03 (1.1961e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [45][310/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.3654e-03 (1.1933e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [45][320/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9742e-02 (1.2147e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [45][330/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9709e-02 (1.2270e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [45][340/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.0273e-03 (1.2233e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [45][350/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.4062e-03 (1.2302e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [45][360/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.8708e-03 (1.2508e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [45][370/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.6398e-03 (1.2521e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [45][380/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5258e-02 (1.2532e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [45][390/391]	Time  0.108 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3316e-02 (1.2651e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
## e[45] optimizer.zero_grad (sum) time: 0.6840353012084961
## e[45]       loss.backward (sum) time: 13.987190246582031
## e[45]      optimizer.step (sum) time: 14.623291492462158
## epoch[45] training(only) time: 54.135993003845215
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 3.3625e-01 (3.3625e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 2.4374e-01 (2.6852e-01)	Acc@1  94.00 ( 93.18)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.049 ( 0.055)	Loss 4.2765e-01 (2.9850e-01)	Acc@1  90.00 ( 92.86)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.056 ( 0.053)	Loss 2.8416e-01 (3.0117e-01)	Acc@1  93.00 ( 92.90)	Acc@5  99.00 ( 99.77)
Test: [ 40/100]	Time  0.048 ( 0.051)	Loss 2.2894e-01 (3.0234e-01)	Acc@1  91.00 ( 92.71)	Acc@5 100.00 ( 99.78)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.9694e-01 (3.0345e-01)	Acc@1  95.00 ( 92.76)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 3.6865e-01 (2.9414e-01)	Acc@1  93.00 ( 92.95)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.051 ( 0.050)	Loss 5.0468e-01 (2.8712e-01)	Acc@1  91.00 ( 93.03)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.059 ( 0.050)	Loss 1.8633e-01 (2.8719e-01)	Acc@1  95.00 ( 92.98)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 2.6920e-01 (2.8110e-01)	Acc@1  92.00 ( 93.08)	Acc@5 100.00 ( 99.81)
 * Acc@1 93.120 Acc@5 99.830
### epoch[45] execution time: 59.16337776184082
EPOCH 46
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [46][  0/391]	Time  0.305 ( 0.305)	Data  0.158 ( 0.158)	Loss 1.6984e-02 (1.6984e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [46][ 10/391]	Time  0.136 ( 0.152)	Data  0.001 ( 0.015)	Loss 8.2123e-03 (1.0523e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [46][ 20/391]	Time  0.145 ( 0.146)	Data  0.001 ( 0.009)	Loss 1.2840e-02 (9.3443e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [46][ 30/391]	Time  0.137 ( 0.144)	Data  0.001 ( 0.006)	Loss 3.8274e-03 (8.6063e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [46][ 40/391]	Time  0.137 ( 0.142)	Data  0.001 ( 0.005)	Loss 3.4508e-03 (8.4549e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [46][ 50/391]	Time  0.141 ( 0.141)	Data  0.001 ( 0.004)	Loss 5.4480e-03 (8.3046e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [46][ 60/391]	Time  0.139 ( 0.141)	Data  0.001 ( 0.004)	Loss 9.7524e-03 (8.4487e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [46][ 70/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.5189e-02 (9.1539e-03)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [46][ 80/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 9.4781e-03 (9.2795e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [46][ 90/391]	Time  0.143 ( 0.140)	Data  0.001 ( 0.003)	Loss 5.4190e-03 (8.9050e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [46][100/391]	Time  0.144 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.8910e-02 (9.2435e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [46][110/391]	Time  0.143 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.4913e-03 (9.0298e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [46][120/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.4815e-03 (8.9782e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [46][130/391]	Time  0.147 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3296e-02 (9.1427e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [46][140/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2684e-02 (9.3194e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [46][150/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.5448e-02 (9.6116e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [46][160/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.6316e-02 (9.7564e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [46][170/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5108e-02 (1.0075e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [46][180/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.2903e-03 (1.0190e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [46][190/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0533e-02 (1.0329e-02)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [46][200/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1392e-02 (1.0287e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [46][210/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.4368e-03 (1.0271e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [46][220/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7351e-02 (1.0335e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [46][230/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2488e-02 (1.0308e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [46][240/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0222e-02 (1.0332e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [46][250/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.7810e-03 (1.0500e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [46][260/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.9001e-02 (1.0608e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [46][270/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.9319e-02 (1.0744e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [46][280/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.0699e-03 (1.0975e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [46][290/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2307e-02 (1.0944e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [46][300/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0473e-02 (1.0863e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [46][310/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.2938e-03 (1.1014e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [46][320/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.7057e-03 (1.0977e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [46][330/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.9748e-03 (1.1114e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [46][340/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0668e-02 (1.1103e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [46][350/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.001)	Loss 1.5316e-02 (1.1318e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [46][360/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.001)	Loss 6.0803e-03 (1.1524e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [46][370/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.001)	Loss 4.1531e-03 (1.1579e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [46][380/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.001)	Loss 3.0889e-03 (1.1733e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [46][390/391]	Time  0.107 ( 0.138)	Data  0.001 ( 0.001)	Loss 5.6312e-03 (1.1659e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
## e[46] optimizer.zero_grad (sum) time: 0.6821169853210449
## e[46]       loss.backward (sum) time: 13.959009408950806
## e[46]      optimizer.step (sum) time: 14.623874425888062
## epoch[46] training(only) time: 54.18217182159424
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 2.9256e-01 (2.9256e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 2.6987e-01 (2.6591e-01)	Acc@1  94.00 ( 93.09)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.052 ( 0.055)	Loss 3.8268e-01 (3.0245e-01)	Acc@1  92.00 ( 92.57)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 2.7088e-01 (3.0855e-01)	Acc@1  93.00 ( 92.55)	Acc@5 100.00 ( 99.81)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 2.6469e-01 (3.0764e-01)	Acc@1  90.00 ( 92.46)	Acc@5 100.00 ( 99.80)
Test: [ 50/100]	Time  0.051 ( 0.051)	Loss 1.7160e-01 (3.0992e-01)	Acc@1  95.00 ( 92.49)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 3.4345e-01 (3.0158e-01)	Acc@1  93.00 ( 92.62)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 5.3240e-01 (2.9455e-01)	Acc@1  91.00 ( 92.77)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.051 ( 0.049)	Loss 1.5814e-01 (2.9343e-01)	Acc@1  97.00 ( 92.83)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 2.5916e-01 (2.8664e-01)	Acc@1  95.00 ( 92.97)	Acc@5 100.00 ( 99.80)
 * Acc@1 93.060 Acc@5 99.820
### epoch[46] execution time: 59.20426535606384
EPOCH 47
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [47][  0/391]	Time  0.301 ( 0.301)	Data  0.158 ( 0.158)	Loss 1.9664e-02 (1.9664e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [47][ 10/391]	Time  0.136 ( 0.153)	Data  0.001 ( 0.015)	Loss 4.0528e-03 (1.3224e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [47][ 20/391]	Time  0.137 ( 0.146)	Data  0.001 ( 0.009)	Loss 9.3460e-03 (1.0722e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [47][ 30/391]	Time  0.138 ( 0.144)	Data  0.001 ( 0.006)	Loss 7.3681e-03 (9.7411e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [47][ 40/391]	Time  0.139 ( 0.143)	Data  0.001 ( 0.005)	Loss 1.9936e-02 (9.6242e-03)	Acc@1  98.44 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [47][ 50/391]	Time  0.137 ( 0.142)	Data  0.001 ( 0.004)	Loss 2.0242e-02 (9.5507e-03)	Acc@1  98.44 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [47][ 60/391]	Time  0.140 ( 0.141)	Data  0.001 ( 0.004)	Loss 4.9254e-03 (9.7113e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [47][ 70/391]	Time  0.139 ( 0.141)	Data  0.001 ( 0.003)	Loss 5.4564e-03 (9.5017e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [47][ 80/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.003)	Loss 1.6439e-02 (1.0034e-02)	Acc@1  98.44 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [47][ 90/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 7.0225e-03 (1.0091e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [47][100/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.8810e-03 (9.6618e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [47][110/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.6993e-02 (9.9369e-03)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [47][120/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.002)	Loss 2.5102e-03 (9.7517e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [47][130/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.002)	Loss 1.3272e-02 (9.9408e-03)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [47][140/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.002)	Loss 1.6587e-02 (1.0360e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [47][150/391]	Time  0.140 ( 0.140)	Data  0.001 ( 0.002)	Loss 1.1970e-02 (1.0287e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [47][160/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.4173e-03 (1.0127e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [47][170/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.2898e-03 (1.0362e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [47][180/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2228e-02 (1.0274e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [47][190/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.4048e-02 (1.0307e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [47][200/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6382e-02 (1.0288e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [47][210/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7083e-02 (1.0404e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [47][220/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.7024e-02 (1.0938e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [47][230/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2209e-02 (1.1091e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [47][240/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.9276e-03 (1.1177e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [47][250/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.5736e-03 (1.1134e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [47][260/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.3620e-03 (1.0984e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [47][270/391]	Time  0.147 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6070e-02 (1.0863e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [47][280/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.8699e-03 (1.0904e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [47][290/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.4428e-03 (1.1085e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [47][300/391]	Time  0.143 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.0891e-03 (1.0984e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [47][310/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.2584e-03 (1.0967e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [47][320/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2867e-02 (1.1011e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [47][330/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2761e-02 (1.0996e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [47][340/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3561e-02 (1.0932e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [47][350/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4758e-02 (1.0847e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [47][360/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.4305e-02 (1.0918e-02)	Acc@1  98.44 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [47][370/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.3058e-03 (1.0861e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [47][380/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.3896e-03 (1.0939e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [47][390/391]	Time  0.107 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7673e-02 (1.0865e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
## e[47] optimizer.zero_grad (sum) time: 0.6870729923248291
## e[47]       loss.backward (sum) time: 14.055191278457642
## e[47]      optimizer.step (sum) time: 14.63474440574646
## epoch[47] training(only) time: 54.24239110946655
# Switched to evaluate mode...
Test: [  0/100]	Time  0.190 ( 0.190)	Loss 3.1915e-01 (3.1915e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 2.8808e-01 (2.6382e-01)	Acc@1  95.00 ( 93.36)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.046 ( 0.056)	Loss 3.3681e-01 (2.9734e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 ( 99.90)
Test: [ 30/100]	Time  0.058 ( 0.053)	Loss 2.4076e-01 (3.0548e-01)	Acc@1  94.00 ( 92.94)	Acc@5 100.00 ( 99.81)
Test: [ 40/100]	Time  0.047 ( 0.052)	Loss 2.4101e-01 (3.0289e-01)	Acc@1  92.00 ( 93.05)	Acc@5 100.00 ( 99.80)
Test: [ 50/100]	Time  0.046 ( 0.052)	Loss 2.1962e-01 (3.0330e-01)	Acc@1  94.00 ( 93.02)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.047 ( 0.051)	Loss 3.5038e-01 (2.9719e-01)	Acc@1  93.00 ( 93.08)	Acc@5 100.00 ( 99.82)
Test: [ 70/100]	Time  0.046 ( 0.051)	Loss 4.9101e-01 (2.9204e-01)	Acc@1  91.00 ( 93.13)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.058 ( 0.050)	Loss 1.5703e-01 (2.9017e-01)	Acc@1  96.00 ( 93.11)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.048 ( 0.050)	Loss 2.6970e-01 (2.8353e-01)	Acc@1  93.00 ( 93.23)	Acc@5 100.00 ( 99.82)
 * Acc@1 93.300 Acc@5 99.830
### epoch[47] execution time: 59.32370376586914
EPOCH 48
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [48][  0/391]	Time  0.309 ( 0.309)	Data  0.154 ( 0.154)	Loss 4.2245e-03 (4.2245e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [48][ 10/391]	Time  0.137 ( 0.153)	Data  0.001 ( 0.015)	Loss 5.2722e-03 (7.3595e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [48][ 20/391]	Time  0.142 ( 0.146)	Data  0.001 ( 0.008)	Loss 5.6287e-03 (7.9115e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [48][ 30/391]	Time  0.137 ( 0.143)	Data  0.001 ( 0.006)	Loss 8.7236e-03 (8.7249e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [48][ 40/391]	Time  0.137 ( 0.142)	Data  0.001 ( 0.005)	Loss 1.4749e-02 (8.2327e-03)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [48][ 50/391]	Time  0.135 ( 0.141)	Data  0.001 ( 0.004)	Loss 6.2248e-03 (8.7562e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [48][ 60/391]	Time  0.139 ( 0.141)	Data  0.001 ( 0.004)	Loss 3.9318e-03 (8.4093e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [48][ 70/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.003)	Loss 5.5415e-03 (8.4639e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [48][ 80/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 7.3234e-03 (8.7113e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [48][ 90/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.8111e-02 (9.1120e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [48][100/391]	Time  0.141 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.6683e-02 (9.3604e-03)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [48][110/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.003)	Loss 9.2330e-04 (9.0399e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [48][120/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.7274e-02 (9.4904e-03)	Acc@1  98.44 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [48][130/391]	Time  0.143 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4856e-02 (9.4204e-03)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [48][140/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.6346e-02 (9.3495e-03)	Acc@1  98.44 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [48][150/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.8244e-03 (9.2683e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [48][160/391]	Time  0.143 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.7437e-03 (9.4487e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [48][170/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4064e-02 (9.4591e-03)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [48][180/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2774e-02 (9.6642e-03)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [48][190/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5991e-02 (9.6366e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [48][200/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0039e-02 (9.9410e-03)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [48][210/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.6454e-03 (9.9840e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [48][220/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1344e-02 (9.9111e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [48][230/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7250e-02 (1.0110e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [48][240/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1648e-03 (9.9773e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [48][250/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.6996e-03 (9.8390e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [48][260/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5542e-02 (9.9777e-03)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [48][270/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.5815e-02 (1.0032e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [48][280/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.6267e-03 (1.0046e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [48][290/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.9122e-03 (1.0123e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [48][300/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.1512e-03 (1.0198e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [48][310/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.7974e-03 (1.0153e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [48][320/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.3949e-03 (1.0062e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [48][330/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4901e-02 (1.0155e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [48][340/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.4832e-02 (1.0243e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [48][350/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.3430e-03 (1.0241e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [48][360/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.4517e-03 (1.0197e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [48][370/391]	Time  0.147 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.8103e-03 (1.0130e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [48][380/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.8426e-03 (1.0005e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [48][390/391]	Time  0.108 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.4911e-03 (9.9393e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
## e[48] optimizer.zero_grad (sum) time: 0.6858878135681152
## e[48]       loss.backward (sum) time: 14.004218101501465
## e[48]      optimizer.step (sum) time: 14.61732268333435
## epoch[48] training(only) time: 54.17455840110779
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 3.0608e-01 (3.0608e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.060)	Loss 2.6301e-01 (2.6168e-01)	Acc@1  95.00 ( 93.82)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.048 ( 0.054)	Loss 3.7005e-01 (3.0441e-01)	Acc@1  91.00 ( 93.19)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 2.6916e-01 (3.0852e-01)	Acc@1  94.00 ( 93.00)	Acc@5  99.00 ( 99.77)
Test: [ 40/100]	Time  0.050 ( 0.051)	Loss 2.8643e-01 (3.0694e-01)	Acc@1  92.00 ( 92.93)	Acc@5 100.00 ( 99.78)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 2.0382e-01 (3.0955e-01)	Acc@1  93.00 ( 92.88)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 3.4740e-01 (2.9918e-01)	Acc@1  94.00 ( 93.03)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 5.3644e-01 (2.9319e-01)	Acc@1  91.00 ( 93.08)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.6859e-01 (2.9362e-01)	Acc@1  96.00 ( 93.01)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.051 ( 0.049)	Loss 2.4672e-01 (2.8802e-01)	Acc@1  94.00 ( 93.08)	Acc@5 100.00 ( 99.79)
 * Acc@1 93.120 Acc@5 99.810
### epoch[48] execution time: 59.212374687194824
EPOCH 49
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [49][  0/391]	Time  0.294 ( 0.294)	Data  0.151 ( 0.151)	Loss 3.3493e-03 (3.3493e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [49][ 10/391]	Time  0.139 ( 0.152)	Data  0.001 ( 0.015)	Loss 3.5068e-03 (7.7358e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [49][ 20/391]	Time  0.143 ( 0.145)	Data  0.001 ( 0.008)	Loss 3.0124e-03 (8.8425e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [49][ 30/391]	Time  0.147 ( 0.143)	Data  0.001 ( 0.006)	Loss 7.4984e-03 (9.9439e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [49][ 40/391]	Time  0.139 ( 0.142)	Data  0.001 ( 0.005)	Loss 3.3332e-02 (1.0713e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [49][ 50/391]	Time  0.136 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.5507e-02 (9.7540e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [49][ 60/391]	Time  0.134 ( 0.141)	Data  0.001 ( 0.004)	Loss 4.9021e-03 (9.5971e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [49][ 70/391]	Time  0.138 ( 0.141)	Data  0.001 ( 0.003)	Loss 2.6318e-02 (9.8057e-03)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [49][ 80/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.0610e-03 (9.2450e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [49][ 90/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 9.0110e-03 (9.6533e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [49][100/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.003)	Loss 8.9548e-03 (9.5114e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [49][110/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.003)	Loss 9.3529e-03 (9.3010e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [49][120/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.0800e-03 (9.3592e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [49][130/391]	Time  0.143 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.9147e-03 (9.1813e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [49][140/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.5249e-03 (9.0809e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [49][150/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.0315e-03 (9.0278e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [49][160/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8951e-02 (9.3019e-03)	Acc@1  98.44 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [49][170/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.2687e-02 (9.2596e-03)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [49][180/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5162e-02 (9.6071e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [49][190/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1769e-03 (9.7957e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [49][200/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.5297e-03 (9.6258e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [49][210/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0198e-02 (9.5604e-03)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [49][220/391]	Time  0.145 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.5841e-03 (9.3872e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [49][230/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.7541e-03 (9.5347e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [49][240/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.8707e-03 (9.4898e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [49][250/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.2384e-03 (9.3420e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [49][260/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.0292e-03 (9.3492e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [49][270/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.8781e-03 (9.4385e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [49][280/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.5004e-03 (9.5341e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [49][290/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.5620e-03 (9.5611e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [49][300/391]	Time  0.150 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1300e-02 (9.6488e-03)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [49][310/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3828e-02 (9.6943e-03)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [49][320/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.7772e-03 (9.6043e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [49][330/391]	Time  0.147 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.8348e-03 (9.4639e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [49][340/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.8207e-03 (9.4212e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [49][350/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.0126e-03 (9.5197e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [49][360/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0884e-02 (9.5355e-03)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [49][370/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.3496e-03 (9.5407e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [49][380/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.9833e-03 (9.5934e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [49][390/391]	Time  0.110 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.7364e-03 (9.5638e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
## e[49] optimizer.zero_grad (sum) time: 0.6867160797119141
## e[49]       loss.backward (sum) time: 13.981454133987427
## e[49]      optimizer.step (sum) time: 14.621999740600586
## epoch[49] training(only) time: 54.125874757766724
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 2.9774e-01 (2.9774e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 2.6974e-01 (2.6188e-01)	Acc@1  94.00 ( 93.73)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 4.3895e-01 (3.0206e-01)	Acc@1  90.00 ( 92.90)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 2.7340e-01 (3.0998e-01)	Acc@1  92.00 ( 92.77)	Acc@5  98.00 ( 99.71)
Test: [ 40/100]	Time  0.047 ( 0.052)	Loss 3.3669e-01 (3.1525e-01)	Acc@1  89.00 ( 92.61)	Acc@5 100.00 ( 99.78)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 2.2160e-01 (3.1590e-01)	Acc@1  93.00 ( 92.67)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.046 ( 0.051)	Loss 3.5977e-01 (3.0939e-01)	Acc@1  93.00 ( 92.74)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 5.4230e-01 (3.0447e-01)	Acc@1  91.00 ( 92.80)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.047 ( 0.050)	Loss 1.5885e-01 (3.0216e-01)	Acc@1  97.00 ( 92.81)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.049 ( 0.050)	Loss 2.5102e-01 (2.9628e-01)	Acc@1  94.00 ( 92.92)	Acc@5 100.00 ( 99.80)
 * Acc@1 93.010 Acc@5 99.810
### epoch[49] execution time: 59.18271517753601
EPOCH 50
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [50][  0/391]	Time  0.306 ( 0.306)	Data  0.158 ( 0.158)	Loss 4.7830e-03 (4.7830e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [50][ 10/391]	Time  0.139 ( 0.153)	Data  0.001 ( 0.015)	Loss 5.3641e-03 (8.6408e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [50][ 20/391]	Time  0.136 ( 0.145)	Data  0.001 ( 0.009)	Loss 6.9485e-03 (7.3210e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [50][ 30/391]	Time  0.134 ( 0.143)	Data  0.001 ( 0.006)	Loss 1.5865e-02 (9.0947e-03)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [50][ 40/391]	Time  0.140 ( 0.142)	Data  0.001 ( 0.005)	Loss 3.2500e-03 (8.1885e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [50][ 50/391]	Time  0.138 ( 0.141)	Data  0.001 ( 0.004)	Loss 5.7083e-03 (9.0134e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [50][ 60/391]	Time  0.139 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.3666e-02 (8.8112e-03)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [50][ 70/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.003)	Loss 8.1831e-03 (9.0201e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [50][ 80/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.0762e-02 (8.9974e-03)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [50][ 90/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.003)	Loss 3.7416e-03 (8.7012e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [50][100/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.7111e-02 (8.7694e-03)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [50][110/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 5.1897e-03 (8.7579e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [50][120/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.002)	Loss 5.7747e-03 (8.7579e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [50][130/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.2808e-03 (8.7552e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [50][140/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1607e-02 (8.6398e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [50][150/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1869e-03 (8.4871e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [50][160/391]	Time  0.145 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8036e-03 (8.6255e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [50][170/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.9393e-03 (8.4971e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [50][180/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.5997e-03 (8.4430e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [50][190/391]	Time  0.146 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.6546e-03 (8.3200e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [50][200/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.0684e-03 (8.6276e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [50][210/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1869e-02 (8.6282e-03)	Acc@1  98.44 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [50][220/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.7373e-03 (8.6596e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [50][230/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.4670e-03 (8.5650e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [50][240/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2116e-03 (8.7075e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [50][250/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2875e-02 (8.6863e-03)	Acc@1  98.44 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [50][260/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.3975e-03 (8.7679e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [50][270/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.6445e-03 (8.8740e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [50][280/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.7073e-03 (8.7921e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [50][290/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4595e-02 (8.7907e-03)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [50][300/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.6170e-03 (8.8242e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [50][310/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8076e-03 (8.7995e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [50][320/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.2096e-03 (8.8081e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [50][330/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.1365e-03 (8.7459e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [50][340/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3728e-02 (8.6799e-03)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [50][350/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.8274e-03 (8.8035e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [50][360/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3880e-02 (8.9698e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [50][370/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.1392e-03 (8.8810e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [50][380/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1295e-02 (8.9032e-03)	Acc@1  98.44 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [50][390/391]	Time  0.112 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1221e-02 (8.9132e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
## e[50] optimizer.zero_grad (sum) time: 0.685701847076416
## e[50]       loss.backward (sum) time: 13.964884757995605
## e[50]      optimizer.step (sum) time: 14.629236936569214
## epoch[50] training(only) time: 54.15430569648743
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 3.0229e-01 (3.0229e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 2.2353e-01 (2.7626e-01)	Acc@1  95.00 ( 93.18)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 3.7311e-01 (3.1092e-01)	Acc@1  91.00 ( 93.00)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 2.9170e-01 (3.1910e-01)	Acc@1  93.00 ( 92.65)	Acc@5  99.00 ( 99.77)
Test: [ 40/100]	Time  0.054 ( 0.052)	Loss 3.1261e-01 (3.2348e-01)	Acc@1  89.00 ( 92.44)	Acc@5 100.00 ( 99.76)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.7674e-01 (3.2213e-01)	Acc@1  95.00 ( 92.49)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 2.6584e-01 (3.1329e-01)	Acc@1  93.00 ( 92.59)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 5.4831e-01 (3.0431e-01)	Acc@1  89.00 ( 92.65)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.050 ( 0.050)	Loss 1.7029e-01 (3.0192e-01)	Acc@1  96.00 ( 92.67)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.054 ( 0.049)	Loss 3.1331e-01 (2.9658e-01)	Acc@1  93.00 ( 92.82)	Acc@5 100.00 ( 99.77)
 * Acc@1 92.890 Acc@5 99.790
### epoch[50] execution time: 59.16793775558472
EPOCH 51
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [51][  0/391]	Time  0.300 ( 0.300)	Data  0.157 ( 0.157)	Loss 5.1649e-03 (5.1649e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [51][ 10/391]	Time  0.137 ( 0.153)	Data  0.001 ( 0.015)	Loss 4.5914e-03 (7.8335e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [51][ 20/391]	Time  0.147 ( 0.146)	Data  0.001 ( 0.009)	Loss 1.1370e-02 (8.7689e-03)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [51][ 30/391]	Time  0.138 ( 0.143)	Data  0.001 ( 0.006)	Loss 4.4372e-03 (7.4823e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [51][ 40/391]	Time  0.138 ( 0.142)	Data  0.001 ( 0.005)	Loss 6.4924e-03 (7.9943e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [51][ 50/391]	Time  0.143 ( 0.142)	Data  0.001 ( 0.004)	Loss 1.0963e-02 (7.8322e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [51][ 60/391]	Time  0.138 ( 0.141)	Data  0.001 ( 0.004)	Loss 3.8791e-02 (9.0036e-03)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [51][ 70/391]	Time  0.139 ( 0.141)	Data  0.001 ( 0.003)	Loss 1.5681e-02 (8.7716e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [51][ 80/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.3492e-03 (8.2233e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [51][ 90/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.2954e-03 (8.2572e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [51][100/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.6717e-03 (8.7142e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [51][110/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.002)	Loss 5.1763e-03 (8.5544e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [51][120/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7233e-03 (8.4567e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [51][130/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.4141e-03 (8.6064e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [51][140/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.1841e-03 (8.6938e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [51][150/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.8990e-03 (8.6959e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [51][160/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.5273e-03 (8.5587e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [51][170/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.9936e-03 (8.2617e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [51][180/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4850e-02 (8.2848e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [51][190/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.8180e-03 (8.5272e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [51][200/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.5297e-03 (8.5127e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [51][210/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.2698e-03 (8.4049e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [51][220/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.5750e-03 (8.4159e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [51][230/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.2101e-03 (8.4930e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [51][240/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.4524e-03 (8.4042e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [51][250/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.2905e-03 (8.3818e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [51][260/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.3594e-03 (8.2705e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [51][270/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1497e-03 (8.2737e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [51][280/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0572e-03 (8.2656e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [51][290/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.5284e-03 (8.1674e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [51][300/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.4109e-02 (8.1896e-03)	Acc@1  98.44 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [51][310/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1306e-03 (8.1208e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [51][320/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.5338e-03 (8.1357e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [51][330/391]	Time  0.145 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.7561e-02 (8.1083e-03)	Acc@1  98.44 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [51][340/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.9586e-03 (8.0625e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [51][350/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.5343e-02 (8.1395e-03)	Acc@1  97.66 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [51][360/391]	Time  0.146 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.0247e-03 (8.1956e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [51][370/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.001)	Loss 5.3678e-03 (8.3921e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [51][380/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.001)	Loss 7.5640e-03 (8.5402e-03)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [51][390/391]	Time  0.113 ( 0.138)	Data  0.001 ( 0.001)	Loss 4.5403e-03 (8.6371e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
## e[51] optimizer.zero_grad (sum) time: 0.6836152076721191
## e[51]       loss.backward (sum) time: 13.980395317077637
## e[51]      optimizer.step (sum) time: 14.631077766418457
## epoch[51] training(only) time: 54.16176223754883
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 3.2489e-01 (3.2489e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 2.8630e-01 (2.9068e-01)	Acc@1  95.00 ( 93.36)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.048 ( 0.055)	Loss 3.5269e-01 (3.1452e-01)	Acc@1  91.00 ( 93.24)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.049 ( 0.053)	Loss 2.8206e-01 (3.1630e-01)	Acc@1  94.00 ( 93.06)	Acc@5  99.00 ( 99.74)
Test: [ 40/100]	Time  0.050 ( 0.052)	Loss 3.3721e-01 (3.2561e-01)	Acc@1  90.00 ( 92.80)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 2.1444e-01 (3.2495e-01)	Acc@1  94.00 ( 92.88)	Acc@5 100.00 ( 99.69)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 3.6685e-01 (3.1625e-01)	Acc@1  92.00 ( 92.90)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.051 ( 0.050)	Loss 5.4625e-01 (3.0838e-01)	Acc@1  91.00 ( 92.97)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.051 ( 0.050)	Loss 1.6852e-01 (3.0637e-01)	Acc@1  97.00 ( 92.93)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.048 ( 0.050)	Loss 2.9756e-01 (2.9953e-01)	Acc@1  93.00 ( 92.99)	Acc@5 100.00 ( 99.78)
 * Acc@1 92.980 Acc@5 99.800
### epoch[51] execution time: 59.19737505912781
EPOCH 52
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [52][  0/391]	Time  0.308 ( 0.308)	Data  0.159 ( 0.159)	Loss 5.0638e-03 (5.0638e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [52][ 10/391]	Time  0.140 ( 0.154)	Data  0.001 ( 0.015)	Loss 6.4896e-03 (1.1945e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [52][ 20/391]	Time  0.146 ( 0.147)	Data  0.001 ( 0.009)	Loss 3.3398e-03 (9.6116e-03)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [52][ 30/391]	Time  0.138 ( 0.144)	Data  0.001 ( 0.006)	Loss 3.3925e-03 (9.8577e-03)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [52][ 40/391]	Time  0.139 ( 0.142)	Data  0.001 ( 0.005)	Loss 3.3497e-03 (1.0649e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [52][ 50/391]	Time  0.140 ( 0.142)	Data  0.001 ( 0.004)	Loss 1.0232e-02 (1.0215e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [52][ 60/391]	Time  0.140 ( 0.141)	Data  0.001 ( 0.004)	Loss 4.3978e-03 (9.8262e-03)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [52][ 70/391]	Time  0.141 ( 0.141)	Data  0.001 ( 0.003)	Loss 5.1471e-03 (9.6994e-03)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [52][ 80/391]	Time  0.136 ( 0.141)	Data  0.001 ( 0.003)	Loss 1.0291e-02 (9.9266e-03)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [52][ 90/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.003)	Loss 6.4566e-03 (9.7467e-03)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [52][100/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.1828e-02 (9.9229e-03)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [52][110/391]	Time  0.141 ( 0.140)	Data  0.001 ( 0.003)	Loss 5.5648e-03 (1.0252e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [52][120/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.002)	Loss 6.6798e-03 (1.0109e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [52][130/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.8947e-03 (9.7845e-03)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [52][140/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1180e-03 (9.4913e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [52][150/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0149e-02 (9.9285e-03)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [52][160/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.6488e-02 (9.9591e-03)	Acc@1  98.44 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [52][170/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.2594e-03 (9.8909e-03)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [52][180/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.5234e-03 (9.7742e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [52][190/391]	Time  0.152 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3175e-02 (9.6877e-03)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [52][200/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.5519e-03 (9.6791e-03)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [52][210/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0339e-02 (9.5865e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [52][220/391]	Time  0.147 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.8311e-03 (9.6728e-03)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [52][230/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.5152e-03 (9.5686e-03)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [52][240/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.2099e-02 (9.6183e-03)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [52][250/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2234e-03 (9.4795e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [52][260/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.4708e-03 (9.4196e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [52][270/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.7223e-03 (9.3519e-03)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [52][280/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7463e-02 (9.2153e-03)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [52][290/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.5988e-03 (9.2167e-03)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [52][300/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.6443e-03 (9.1557e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [52][310/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1259e-02 (9.1000e-03)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [52][320/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5136e-02 (9.1027e-03)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [52][330/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0312e-02 (9.1055e-03)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [52][340/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5193e-02 (9.0938e-03)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [52][350/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.8123e-03 (9.1412e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [52][360/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.8550e-03 (9.0569e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [52][370/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4197e-03 (8.9379e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [52][380/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9953e-03 (8.8261e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [52][390/391]	Time  0.106 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4045e-02 (8.7850e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
## e[52] optimizer.zero_grad (sum) time: 0.6858222484588623
## e[52]       loss.backward (sum) time: 14.055222749710083
## e[52]      optimizer.step (sum) time: 14.580449342727661
## epoch[52] training(only) time: 54.16150641441345
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 3.1497e-01 (3.1497e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.062)	Loss 2.4610e-01 (2.6144e-01)	Acc@1  95.00 ( 93.27)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 3.8799e-01 (2.9746e-01)	Acc@1  91.00 ( 92.76)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 2.9692e-01 (3.0397e-01)	Acc@1  92.00 ( 92.71)	Acc@5  99.00 ( 99.77)
Test: [ 40/100]	Time  0.056 ( 0.052)	Loss 2.5668e-01 (3.1292e-01)	Acc@1  90.00 ( 92.63)	Acc@5 100.00 ( 99.80)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 2.6284e-01 (3.1491e-01)	Acc@1  92.00 ( 92.69)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.046 ( 0.051)	Loss 3.1022e-01 (3.0602e-01)	Acc@1  93.00 ( 92.82)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 5.9330e-01 (3.0229e-01)	Acc@1  90.00 ( 92.92)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.7920e-01 (3.0233e-01)	Acc@1  96.00 ( 92.86)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.058 ( 0.050)	Loss 3.0340e-01 (2.9761e-01)	Acc@1  93.00 ( 93.03)	Acc@5 100.00 ( 99.81)
 * Acc@1 93.090 Acc@5 99.830
### epoch[52] execution time: 59.19134068489075
EPOCH 53
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [53][  0/391]	Time  0.298 ( 0.298)	Data  0.155 ( 0.155)	Loss 8.6991e-03 (8.6991e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [53][ 10/391]	Time  0.135 ( 0.153)	Data  0.001 ( 0.015)	Loss 3.5143e-03 (7.8383e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [53][ 20/391]	Time  0.139 ( 0.145)	Data  0.001 ( 0.008)	Loss 1.5905e-03 (9.0709e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [53][ 30/391]	Time  0.138 ( 0.143)	Data  0.001 ( 0.006)	Loss 3.4832e-03 (8.2608e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [53][ 40/391]	Time  0.140 ( 0.142)	Data  0.001 ( 0.005)	Loss 3.1849e-02 (8.7266e-03)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [53][ 50/391]	Time  0.144 ( 0.141)	Data  0.001 ( 0.004)	Loss 4.5743e-03 (8.1833e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [53][ 60/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.004)	Loss 7.8987e-03 (8.1443e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [53][ 70/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.9963e-03 (8.5491e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [53][ 80/391]	Time  0.143 ( 0.140)	Data  0.001 ( 0.003)	Loss 5.3516e-03 (8.9467e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [53][ 90/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.003)	Loss 5.5759e-03 (9.0476e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [53][100/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.003)	Loss 3.9612e-02 (9.1277e-03)	Acc@1  98.44 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [53][110/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.7906e-02 (9.1530e-03)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [53][120/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.6672e-03 (8.8219e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [53][130/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.5342e-03 (9.1423e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [53][140/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.5525e-03 (8.9045e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [53][150/391]	Time  0.143 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.9936e-03 (8.7211e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [53][160/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1516e-02 (8.5457e-03)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [53][170/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.3786e-02 (8.6092e-03)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [53][180/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.6170e-03 (8.6227e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [53][190/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0144e-02 (8.5100e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [53][200/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.6212e-03 (8.5459e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [53][210/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.6223e-02 (8.6033e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [53][220/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5357e-02 (8.5818e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [53][230/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.9385e-03 (8.6954e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [53][240/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.5639e-03 (8.6522e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [53][250/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.8527e-03 (8.6223e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [53][260/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.6310e-03 (8.4530e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [53][270/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9510e-02 (8.3974e-03)	Acc@1  98.44 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [53][280/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.1897e-03 (8.3559e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [53][290/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.0401e-03 (8.3461e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [53][300/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.7603e-02 (8.3413e-03)	Acc@1  98.44 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [53][310/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.4910e-03 (8.2898e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [53][320/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0584e-02 (8.3699e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [53][330/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.8873e-03 (8.4003e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [53][340/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.4869e-03 (8.2984e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [53][350/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.3467e-03 (8.3683e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [53][360/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.4122e-03 (8.4042e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [53][370/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.7884e-03 (8.3738e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [53][380/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0534e-02 (8.3073e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [53][390/391]	Time  0.118 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.9647e-03 (8.3035e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
## e[53] optimizer.zero_grad (sum) time: 0.6842312812805176
## e[53]       loss.backward (sum) time: 13.91044569015503
## e[53]      optimizer.step (sum) time: 14.62430739402771
## epoch[53] training(only) time: 54.11566686630249
# Switched to evaluate mode...
Test: [  0/100]	Time  0.194 ( 0.194)	Loss 3.2473e-01 (3.2473e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 2.9673e-01 (2.8698e-01)	Acc@1  96.00 ( 93.73)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 4.8393e-01 (3.1798e-01)	Acc@1  91.00 ( 93.05)	Acc@5 100.00 ( 99.86)
Test: [ 30/100]	Time  0.050 ( 0.053)	Loss 2.9035e-01 (3.2193e-01)	Acc@1  92.00 ( 92.90)	Acc@5  99.00 ( 99.81)
Test: [ 40/100]	Time  0.049 ( 0.052)	Loss 2.8903e-01 (3.2543e-01)	Acc@1  92.00 ( 92.78)	Acc@5 100.00 ( 99.80)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 1.8247e-01 (3.2025e-01)	Acc@1  96.00 ( 92.90)	Acc@5 100.00 ( 99.80)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 3.2878e-01 (3.0575e-01)	Acc@1  93.00 ( 93.02)	Acc@5 100.00 ( 99.84)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 5.1901e-01 (2.9909e-01)	Acc@1  91.00 ( 93.11)	Acc@5 100.00 ( 99.80)
Test: [ 80/100]	Time  0.049 ( 0.050)	Loss 1.9388e-01 (2.9783e-01)	Acc@1  96.00 ( 93.12)	Acc@5 100.00 ( 99.81)
Test: [ 90/100]	Time  0.046 ( 0.050)	Loss 2.7911e-01 (2.9373e-01)	Acc@1  95.00 ( 93.19)	Acc@5 100.00 ( 99.82)
 * Acc@1 93.210 Acc@5 99.840
### epoch[53] execution time: 59.1594033241272
EPOCH 54
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [54][  0/391]	Time  0.300 ( 0.300)	Data  0.151 ( 0.151)	Loss 2.9806e-03 (2.9806e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [54][ 10/391]	Time  0.139 ( 0.151)	Data  0.001 ( 0.015)	Loss 6.9516e-03 (9.0583e-03)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [54][ 20/391]	Time  0.138 ( 0.145)	Data  0.001 ( 0.008)	Loss 2.5733e-03 (6.9471e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [54][ 30/391]	Time  0.140 ( 0.143)	Data  0.001 ( 0.006)	Loss 1.8126e-03 (7.1085e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [54][ 40/391]	Time  0.134 ( 0.142)	Data  0.001 ( 0.005)	Loss 1.2624e-02 (7.4510e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [54][ 50/391]	Time  0.144 ( 0.141)	Data  0.001 ( 0.004)	Loss 2.9961e-02 (7.8549e-03)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [54][ 60/391]	Time  0.139 ( 0.141)	Data  0.001 ( 0.004)	Loss 4.2952e-03 (7.5021e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [54][ 70/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 3.5527e-03 (7.2404e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [54][ 80/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.2086e-02 (8.0811e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [54][ 90/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.8095e-02 (8.1752e-03)	Acc@1  98.44 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [54][100/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.8136e-03 (8.0779e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [54][110/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.0525e-03 (7.6509e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [54][120/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.7831e-03 (7.6020e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [54][130/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.8280e-03 (7.4753e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [54][140/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.4772e-04 (7.3211e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [54][150/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1208e-03 (7.1919e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [54][160/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.8013e-03 (7.0877e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [54][170/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.7716e-03 (7.0033e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [54][180/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.0396e-03 (6.9299e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [54][190/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.6091e-03 (6.9706e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [54][200/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.7629e-03 (6.9433e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [54][210/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.4302e-03 (7.1181e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [54][220/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.1400e-03 (7.1489e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [54][230/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.3406e-03 (7.1632e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [54][240/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.3219e-03 (7.1751e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [54][250/391]	Time  0.146 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.1022e-03 (7.3110e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [54][260/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.7349e-03 (7.3386e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [54][270/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7856e-02 (7.5506e-03)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [54][280/391]	Time  0.148 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.4988e-03 (7.5128e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [54][290/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.6382e-03 (7.5646e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [54][300/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.4383e-03 (7.5246e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [54][310/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.4502e-02 (7.7000e-03)	Acc@1  98.44 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [54][320/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.3565e-03 (7.7187e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [54][330/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.1690e-03 (7.7073e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [54][340/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.9710e-03 (7.7695e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [54][350/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.1035e-03 (7.7688e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [54][360/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.4099e-03 (7.7945e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [54][370/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5977e-03 (7.7949e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [54][380/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.001)	Loss 2.1555e-02 (7.8097e-03)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [54][390/391]	Time  0.110 ( 0.138)	Data  0.001 ( 0.001)	Loss 3.5323e-02 (7.8723e-03)	Acc@1  98.75 ( 99.81)	Acc@5 100.00 (100.00)
## e[54] optimizer.zero_grad (sum) time: 0.6885826587677002
## e[54]       loss.backward (sum) time: 13.972558975219727
## e[54]      optimizer.step (sum) time: 14.610613822937012
## epoch[54] training(only) time: 54.13737964630127
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 3.0596e-01 (3.0596e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 2.9833e-01 (2.8830e-01)	Acc@1  95.00 ( 93.45)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 4.6199e-01 (3.1743e-01)	Acc@1  90.00 ( 92.95)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.048 ( 0.053)	Loss 2.7362e-01 (3.1869e-01)	Acc@1  92.00 ( 92.90)	Acc@5  99.00 ( 99.77)
Test: [ 40/100]	Time  0.046 ( 0.052)	Loss 2.8422e-01 (3.2560e-01)	Acc@1  91.00 ( 92.66)	Acc@5 100.00 ( 99.76)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 2.0353e-01 (3.2454e-01)	Acc@1  94.00 ( 92.67)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.046 ( 0.051)	Loss 3.2742e-01 (3.1285e-01)	Acc@1  93.00 ( 92.79)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 5.2242e-01 (3.0635e-01)	Acc@1  91.00 ( 92.86)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.048 ( 0.050)	Loss 1.7643e-01 (3.0378e-01)	Acc@1  96.00 ( 92.84)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 2.5614e-01 (2.9964e-01)	Acc@1  93.00 ( 92.92)	Acc@5 100.00 ( 99.79)
 * Acc@1 92.980 Acc@5 99.810
### epoch[54] execution time: 59.16993761062622
EPOCH 55
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [55][  0/391]	Time  0.302 ( 0.302)	Data  0.159 ( 0.159)	Loss 7.7594e-03 (7.7594e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [55][ 10/391]	Time  0.138 ( 0.153)	Data  0.001 ( 0.015)	Loss 1.0656e-02 (4.2726e-03)	Acc@1  99.22 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [55][ 20/391]	Time  0.139 ( 0.146)	Data  0.001 ( 0.009)	Loss 2.1169e-02 (7.4750e-03)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [55][ 30/391]	Time  0.137 ( 0.143)	Data  0.001 ( 0.006)	Loss 4.2791e-03 (7.5310e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [55][ 40/391]	Time  0.138 ( 0.142)	Data  0.001 ( 0.005)	Loss 1.4890e-02 (8.3729e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [55][ 50/391]	Time  0.135 ( 0.141)	Data  0.001 ( 0.004)	Loss 5.5741e-03 (7.8994e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [55][ 60/391]	Time  0.136 ( 0.141)	Data  0.001 ( 0.004)	Loss 9.3336e-03 (8.3941e-03)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [55][ 70/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.003)	Loss 3.9399e-03 (8.5429e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [55][ 80/391]	Time  0.152 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.5885e-03 (8.4058e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [55][ 90/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 3.3722e-03 (8.0757e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [55][100/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 3.6425e-03 (8.1143e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [55][110/391]	Time  0.152 ( 0.139)	Data  0.001 ( 0.003)	Loss 7.3931e-03 (7.9979e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [55][120/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.8085e-03 (7.9451e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [55][130/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.8144e-03 (8.0795e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [55][140/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4735e-02 (8.0255e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [55][150/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.4067e-03 (8.1156e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [55][160/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.8817e-03 (7.9387e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [55][170/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.7016e-03 (8.0213e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [55][180/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5455e-02 (8.2504e-03)	Acc@1  98.44 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [55][190/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5039e-02 (8.3078e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [55][200/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2580e-02 (8.2111e-03)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [55][210/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2531e-02 (8.3004e-03)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [55][220/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.4483e-04 (8.1158e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [55][230/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.6070e-03 (8.3638e-03)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [55][240/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.8695e-03 (8.1896e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [55][250/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.6706e-04 (8.0690e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [55][260/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.6598e-03 (8.0990e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [55][270/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.8590e-03 (7.9813e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [55][280/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.1917e-03 (7.8543e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [55][290/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5127e-03 (7.9053e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [55][300/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.0132e-03 (7.8882e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [55][310/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8057e-03 (7.8526e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [55][320/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.3862e-03 (7.8607e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [55][330/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1660e-02 (7.8840e-03)	Acc@1  98.44 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [55][340/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.7435e-03 (7.9555e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [55][350/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.3991e-03 (8.0711e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [55][360/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.8714e-02 (8.1229e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [55][370/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.9322e-02 (8.1822e-03)	Acc@1  98.44 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [55][380/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.6125e-03 (8.1483e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [55][390/391]	Time  0.109 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.9026e-03 (8.1267e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
## e[55] optimizer.zero_grad (sum) time: 0.6841650009155273
## e[55]       loss.backward (sum) time: 13.970130205154419
## e[55]      optimizer.step (sum) time: 14.612382173538208
## epoch[55] training(only) time: 54.1277756690979
# Switched to evaluate mode...
Test: [  0/100]	Time  0.202 ( 0.202)	Loss 3.4145e-01 (3.4145e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 2.6248e-01 (2.9382e-01)	Acc@1  95.00 ( 92.91)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 3.9078e-01 (3.1890e-01)	Acc@1  92.00 ( 92.57)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 2.9152e-01 (3.1874e-01)	Acc@1  91.00 ( 92.61)	Acc@5  99.00 ( 99.74)
Test: [ 40/100]	Time  0.047 ( 0.052)	Loss 2.2735e-01 (3.2304e-01)	Acc@1  92.00 ( 92.59)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.056 ( 0.051)	Loss 2.1531e-01 (3.2156e-01)	Acc@1  94.00 ( 92.63)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 3.2232e-01 (3.1139e-01)	Acc@1  92.00 ( 92.66)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 5.8149e-01 (3.0636e-01)	Acc@1  89.00 ( 92.80)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.049 ( 0.049)	Loss 1.8458e-01 (3.0596e-01)	Acc@1  96.00 ( 92.81)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.047 ( 0.050)	Loss 2.9707e-01 (2.9795e-01)	Acc@1  94.00 ( 92.98)	Acc@5 100.00 ( 99.77)
 * Acc@1 92.990 Acc@5 99.780
### epoch[55] execution time: 59.155256271362305
EPOCH 56
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [56][  0/391]	Time  0.299 ( 0.299)	Data  0.155 ( 0.155)	Loss 1.2879e-03 (1.2879e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [56][ 10/391]	Time  0.140 ( 0.153)	Data  0.001 ( 0.015)	Loss 1.1686e-02 (6.8376e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [56][ 20/391]	Time  0.137 ( 0.147)	Data  0.001 ( 0.008)	Loss 1.8069e-03 (5.6251e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [56][ 30/391]	Time  0.140 ( 0.144)	Data  0.001 ( 0.006)	Loss 5.4172e-03 (5.4320e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [56][ 40/391]	Time  0.139 ( 0.143)	Data  0.001 ( 0.005)	Loss 3.9932e-03 (5.1957e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [56][ 50/391]	Time  0.139 ( 0.142)	Data  0.001 ( 0.004)	Loss 5.4169e-03 (5.9093e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [56][ 60/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.004)	Loss 3.2271e-03 (6.1421e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [56][ 70/391]	Time  0.133 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.4273e-02 (6.1653e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [56][ 80/391]	Time  0.140 ( 0.140)	Data  0.001 ( 0.003)	Loss 6.7161e-03 (6.4817e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [56][ 90/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 9.3010e-03 (7.0065e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [56][100/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.0157e-03 (6.9918e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [56][110/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.003)	Loss 9.5482e-03 (6.7310e-03)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [56][120/391]	Time  0.141 ( 0.140)	Data  0.001 ( 0.002)	Loss 4.3704e-03 (6.6740e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [56][130/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.002)	Loss 5.2586e-03 (6.4927e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [56][140/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.002)	Loss 7.8981e-03 (6.4305e-03)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [56][150/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.002)	Loss 4.5398e-03 (6.6065e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [56][160/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7936e-02 (6.7417e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [56][170/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.6034e-03 (6.7606e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [56][180/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.4848e-03 (6.7309e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [56][190/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4298e-02 (6.7526e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [56][200/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.9723e-03 (6.8620e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [56][210/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8141e-03 (6.8002e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [56][220/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0322e-03 (6.9111e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [56][230/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.6089e-03 (6.9429e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [56][240/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.6498e-03 (6.8868e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [56][250/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3176e-02 (6.9371e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [56][260/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.4027e-03 (6.8925e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [56][270/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5461e-02 (6.9859e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [56][280/391]	Time  0.144 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.1978e-03 (6.9312e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [56][290/391]	Time  0.142 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2139e-02 (6.9026e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [56][300/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4065e-03 (6.8721e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [56][310/391]	Time  0.149 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5630e-02 (6.8390e-03)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [56][320/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6815e-03 (7.0949e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [56][330/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0996e-02 (7.0746e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [56][340/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3681e-03 (6.9989e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [56][350/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2019e-03 (7.1087e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [56][360/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.5883e-03 (7.0383e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [56][370/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.1175e-03 (7.0311e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [56][380/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4289e-02 (7.0070e-03)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [56][390/391]	Time  0.108 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.9309e-02 (7.0462e-03)	Acc@1  98.75 ( 99.84)	Acc@5 100.00 (100.00)
## e[56] optimizer.zero_grad (sum) time: 0.6835765838623047
## e[56]       loss.backward (sum) time: 14.029678583145142
## e[56]      optimizer.step (sum) time: 14.622206687927246
## epoch[56] training(only) time: 54.26860070228577
# Switched to evaluate mode...
Test: [  0/100]	Time  0.192 ( 0.192)	Loss 3.5152e-01 (3.5152e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.048 ( 0.061)	Loss 3.2791e-01 (2.9198e-01)	Acc@1  94.00 ( 92.73)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 3.7658e-01 (3.2249e-01)	Acc@1  92.00 ( 92.43)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.048 ( 0.053)	Loss 3.5422e-01 (3.2899e-01)	Acc@1  92.00 ( 92.55)	Acc@5  99.00 ( 99.74)
Test: [ 40/100]	Time  0.046 ( 0.052)	Loss 2.6676e-01 (3.3189e-01)	Acc@1  93.00 ( 92.51)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 2.1221e-01 (3.2825e-01)	Acc@1  95.00 ( 92.51)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.047 ( 0.051)	Loss 3.3107e-01 (3.2009e-01)	Acc@1  93.00 ( 92.57)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 6.3569e-01 (3.1545e-01)	Acc@1  90.00 ( 92.63)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.9014e-01 (3.1330e-01)	Acc@1  96.00 ( 92.63)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.046 ( 0.050)	Loss 3.2505e-01 (3.0522e-01)	Acc@1  94.00 ( 92.82)	Acc@5 100.00 ( 99.78)
 * Acc@1 92.900 Acc@5 99.790
### epoch[56] execution time: 59.29859781265259
EPOCH 57
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [57][  0/391]	Time  0.297 ( 0.297)	Data  0.150 ( 0.150)	Loss 1.3222e-03 (1.3222e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [57][ 10/391]	Time  0.134 ( 0.154)	Data  0.001 ( 0.015)	Loss 2.5438e-03 (7.4944e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [57][ 20/391]	Time  0.134 ( 0.146)	Data  0.001 ( 0.008)	Loss 4.3138e-03 (7.1163e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [57][ 30/391]	Time  0.142 ( 0.144)	Data  0.001 ( 0.006)	Loss 1.5575e-03 (8.3728e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [57][ 40/391]	Time  0.139 ( 0.142)	Data  0.001 ( 0.005)	Loss 4.5408e-03 (7.8581e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [57][ 50/391]	Time  0.135 ( 0.142)	Data  0.001 ( 0.004)	Loss 1.3502e-02 (7.2958e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [57][ 60/391]	Time  0.139 ( 0.141)	Data  0.001 ( 0.003)	Loss 1.2198e-03 (6.8720e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [57][ 70/391]	Time  0.139 ( 0.141)	Data  0.001 ( 0.003)	Loss 5.0976e-03 (6.9271e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [57][ 80/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.003)	Loss 9.0232e-04 (6.8479e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [57][ 90/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 5.9696e-03 (7.1059e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [57][100/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.002)	Loss 4.1261e-03 (7.0287e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [57][110/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.002)	Loss 1.1851e-03 (6.7865e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [57][120/391]	Time  0.142 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3346e-03 (6.5611e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [57][130/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.7172e-03 (6.4246e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [57][140/391]	Time  0.147 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.2174e-03 (6.4136e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [57][150/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.1929e-03 (6.5054e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [57][160/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.0945e-03 (6.5484e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [57][170/391]	Time  0.146 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.5886e-03 (6.6240e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [57][180/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.5515e-02 (6.6470e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [57][190/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.8577e-03 (6.6168e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [57][200/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.1395e-03 (6.5592e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [57][210/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.3771e-03 (6.5208e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [57][220/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.5272e-03 (6.7125e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [57][230/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0169e-03 (6.6454e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [57][240/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0527e-02 (6.6809e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [57][250/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.5587e-03 (6.7291e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [57][260/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.2831e-02 (6.8845e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [57][270/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.3397e-03 (6.8675e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [57][280/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.0660e-03 (6.7562e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [57][290/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.6709e-03 (6.7670e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [57][300/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.9636e-03 (6.7949e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [57][310/391]	Time  0.146 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3011e-02 (6.8258e-03)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [57][320/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3838e-03 (6.8466e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [57][330/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5823e-03 (6.8280e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [57][340/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3207e-02 (6.9356e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [57][350/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.9293e-03 (6.9794e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [57][360/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.001)	Loss 3.4976e-03 (7.1039e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [57][370/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.001)	Loss 1.1329e-02 (7.0721e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [57][380/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.001)	Loss 1.4034e-02 (7.0818e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [57][390/391]	Time  0.111 ( 0.138)	Data  0.001 ( 0.001)	Loss 5.3599e-03 (7.0157e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
## e[57] optimizer.zero_grad (sum) time: 0.6877355575561523
## e[57]       loss.backward (sum) time: 13.945436000823975
## e[57]      optimizer.step (sum) time: 14.611934423446655
## epoch[57] training(only) time: 54.12207746505737
# Switched to evaluate mode...
Test: [  0/100]	Time  0.196 ( 0.196)	Loss 3.6319e-01 (3.6319e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 2.7833e-01 (2.7491e-01)	Acc@1  94.00 ( 93.64)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.048 ( 0.055)	Loss 3.1732e-01 (3.1074e-01)	Acc@1  92.00 ( 93.14)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.051 ( 0.053)	Loss 2.7563e-01 (3.1688e-01)	Acc@1  94.00 ( 93.13)	Acc@5  99.00 ( 99.68)
Test: [ 40/100]	Time  0.048 ( 0.051)	Loss 3.2374e-01 (3.2392e-01)	Acc@1  90.00 ( 93.00)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.056 ( 0.051)	Loss 1.6350e-01 (3.2820e-01)	Acc@1  96.00 ( 92.80)	Acc@5 100.00 ( 99.63)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 2.9840e-01 (3.1610e-01)	Acc@1  94.00 ( 92.95)	Acc@5 100.00 ( 99.69)
Test: [ 70/100]	Time  0.050 ( 0.050)	Loss 6.1786e-01 (3.0997e-01)	Acc@1  90.00 ( 93.04)	Acc@5 100.00 ( 99.68)
Test: [ 80/100]	Time  0.047 ( 0.050)	Loss 1.6089e-01 (3.0941e-01)	Acc@1  96.00 ( 93.02)	Acc@5 100.00 ( 99.69)
Test: [ 90/100]	Time  0.048 ( 0.049)	Loss 3.3103e-01 (3.0330e-01)	Acc@1  93.00 ( 93.15)	Acc@5 100.00 ( 99.73)
 * Acc@1 93.180 Acc@5 99.750
### epoch[57] execution time: 59.14041709899902
EPOCH 58
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [58][  0/391]	Time  0.308 ( 0.308)	Data  0.155 ( 0.155)	Loss 2.5287e-03 (2.5287e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [58][ 10/391]	Time  0.145 ( 0.152)	Data  0.001 ( 0.015)	Loss 6.0421e-03 (4.6594e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [58][ 20/391]	Time  0.135 ( 0.145)	Data  0.001 ( 0.008)	Loss 5.0453e-03 (4.5313e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [58][ 30/391]	Time  0.135 ( 0.143)	Data  0.001 ( 0.006)	Loss 4.2184e-03 (4.8363e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [58][ 40/391]	Time  0.138 ( 0.142)	Data  0.001 ( 0.005)	Loss 9.3469e-03 (4.7197e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [58][ 50/391]	Time  0.140 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.7283e-03 (5.4331e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [58][ 60/391]	Time  0.136 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.5953e-02 (5.7832e-03)	Acc@1  99.22 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [58][ 70/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.003)	Loss 6.3146e-03 (5.8234e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [58][ 80/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 5.9142e-03 (5.6631e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [58][ 90/391]	Time  0.140 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.5920e-02 (5.8957e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [58][100/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.003)	Loss 9.9453e-04 (5.8253e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [58][110/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 3.1009e-03 (5.7390e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [58][120/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2478e-03 (5.5270e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [58][130/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0832e-03 (5.5365e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [58][140/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.7708e-03 (5.7914e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [58][150/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8595e-03 (6.0974e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [58][160/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.8977e-03 (6.1136e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [58][170/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1003e-03 (6.1698e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [58][180/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.8731e-03 (6.1498e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [58][190/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.1769e-03 (6.1592e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [58][200/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.5526e-03 (6.3024e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [58][210/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.7065e-03 (6.1911e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [58][220/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.1686e-03 (6.1949e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [58][230/391]	Time  0.144 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3067e-03 (6.1301e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [58][240/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0911e-03 (6.0633e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [58][250/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5851e-02 (6.1369e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [58][260/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2935e-03 (6.1890e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [58][270/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.6566e-03 (6.1474e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [58][280/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.6017e-03 (6.1017e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [58][290/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2095e-03 (6.1505e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [58][300/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.9303e-03 (6.0842e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [58][310/391]	Time  0.145 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.8821e-03 (6.0118e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [58][320/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3080e-02 (6.1262e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [58][330/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.4595e-02 (6.1762e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [58][340/391]	Time  0.146 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4780e-03 (6.1867e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [58][350/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.3676e-03 (6.2987e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [58][360/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3278e-03 (6.2669e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [58][370/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.7843e-02 (6.2881e-03)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [58][380/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.5108e-03 (6.3058e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [58][390/391]	Time  0.110 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.4564e-03 (6.3390e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
## e[58] optimizer.zero_grad (sum) time: 0.6854188442230225
## e[58]       loss.backward (sum) time: 14.003535270690918
## e[58]      optimizer.step (sum) time: 14.639334917068481
## epoch[58] training(only) time: 54.25633907318115
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 3.8175e-01 (3.8175e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.062)	Loss 2.8182e-01 (2.9903e-01)	Acc@1  94.00 ( 92.73)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 4.0334e-01 (3.2645e-01)	Acc@1  90.00 ( 92.57)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 2.8776e-01 (3.2427e-01)	Acc@1  94.00 ( 92.94)	Acc@5  99.00 ( 99.71)
Test: [ 40/100]	Time  0.046 ( 0.052)	Loss 2.9718e-01 (3.2684e-01)	Acc@1  90.00 ( 92.73)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 2.1365e-01 (3.2862e-01)	Acc@1  96.00 ( 92.76)	Acc@5 100.00 ( 99.65)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 2.5898e-01 (3.1424e-01)	Acc@1  94.00 ( 93.03)	Acc@5 100.00 ( 99.70)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 5.3492e-01 (3.0895e-01)	Acc@1  91.00 ( 93.04)	Acc@5 100.00 ( 99.69)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.4917e-01 (3.0759e-01)	Acc@1  96.00 ( 93.01)	Acc@5 100.00 ( 99.72)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 2.9545e-01 (3.0064e-01)	Acc@1  92.00 ( 93.12)	Acc@5 100.00 ( 99.74)
 * Acc@1 93.140 Acc@5 99.750
### epoch[58] execution time: 59.272300720214844
EPOCH 59
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [59][  0/391]	Time  0.308 ( 0.308)	Data  0.164 ( 0.164)	Loss 5.3350e-03 (5.3350e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [59][ 10/391]	Time  0.140 ( 0.153)	Data  0.001 ( 0.016)	Loss 8.6018e-03 (7.7269e-03)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [59][ 20/391]	Time  0.138 ( 0.146)	Data  0.001 ( 0.009)	Loss 1.7225e-02 (6.5113e-03)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [59][ 30/391]	Time  0.137 ( 0.143)	Data  0.001 ( 0.006)	Loss 9.3109e-04 (7.3477e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [59][ 40/391]	Time  0.136 ( 0.142)	Data  0.001 ( 0.005)	Loss 4.2661e-03 (6.4413e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [59][ 50/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.004)	Loss 3.4007e-03 (6.3585e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [59][ 60/391]	Time  0.136 ( 0.141)	Data  0.001 ( 0.004)	Loss 2.4962e-03 (6.1177e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [59][ 70/391]	Time  0.139 ( 0.141)	Data  0.001 ( 0.003)	Loss 4.8865e-03 (7.0042e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [59][ 80/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.2555e-02 (6.8936e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [59][ 90/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.7113e-03 (6.6405e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [59][100/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.9825e-02 (6.8109e-03)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [59][110/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.0962e-02 (6.8402e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [59][120/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4968e-03 (6.6778e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [59][130/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.5000e-03 (7.2445e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [59][140/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.2407e-03 (7.0747e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [59][150/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.0018e-03 (6.9274e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [59][160/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.9818e-03 (6.9464e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [59][170/391]	Time  0.142 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.4914e-03 (7.0724e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [59][180/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.2526e-03 (7.0984e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [59][190/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.5028e-03 (7.1657e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [59][200/391]	Time  0.149 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.6361e-03 (7.1178e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [59][210/391]	Time  0.145 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.0215e-03 (7.0024e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [59][220/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.8745e-03 (6.9354e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [59][230/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2869e-02 (6.9252e-03)	Acc@1  98.44 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [59][240/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.4885e-03 (6.8132e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [59][250/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.3126e-04 (6.7122e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [59][260/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.6553e-02 (6.8266e-03)	Acc@1  97.66 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [59][270/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.3706e-03 (6.7136e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [59][280/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.5073e-03 (6.6281e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [59][290/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.4453e-02 (6.7124e-03)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [59][300/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.7583e-03 (6.8150e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [59][310/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4071e-02 (6.9019e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [59][320/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.1449e-03 (6.8791e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [59][330/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.7898e-04 (6.9438e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [59][340/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.1594e-03 (6.8536e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [59][350/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0781e-02 (6.8222e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [59][360/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.8896e-03 (6.8496e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [59][370/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6928e-02 (6.9646e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [59][380/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.4522e-03 (6.9298e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [59][390/391]	Time  0.109 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.4344e-02 (7.0795e-03)	Acc@1  98.75 ( 99.81)	Acc@5 100.00 (100.00)
## e[59] optimizer.zero_grad (sum) time: 0.6850020885467529
## e[59]       loss.backward (sum) time: 14.024929523468018
## e[59]      optimizer.step (sum) time: 14.580872058868408
## epoch[59] training(only) time: 54.23990845680237
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 3.4024e-01 (3.4024e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 2.5510e-01 (2.8234e-01)	Acc@1  95.00 ( 93.82)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 4.3031e-01 (3.1563e-01)	Acc@1  91.00 ( 93.14)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.050 ( 0.053)	Loss 3.1115e-01 (3.2233e-01)	Acc@1  93.00 ( 93.10)	Acc@5  99.00 ( 99.71)
Test: [ 40/100]	Time  0.048 ( 0.052)	Loss 2.9406e-01 (3.2472e-01)	Acc@1  93.00 ( 92.95)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.9653e-01 (3.2835e-01)	Acc@1  97.00 ( 92.90)	Acc@5 100.00 ( 99.65)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 3.6743e-01 (3.1781e-01)	Acc@1  94.00 ( 93.07)	Acc@5 100.00 ( 99.70)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 6.4773e-01 (3.1284e-01)	Acc@1  90.00 ( 93.17)	Acc@5 100.00 ( 99.69)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.7530e-01 (3.1351e-01)	Acc@1  95.00 ( 93.06)	Acc@5 100.00 ( 99.72)
Test: [ 90/100]	Time  0.047 ( 0.050)	Loss 2.7916e-01 (3.0584e-01)	Acc@1  94.00 ( 93.21)	Acc@5 100.00 ( 99.75)
 * Acc@1 93.260 Acc@5 99.760
### epoch[59] execution time: 59.25644326210022
EPOCH 60
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [60][  0/391]	Time  0.301 ( 0.301)	Data  0.153 ( 0.153)	Loss 1.5536e-03 (1.5536e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [60][ 10/391]	Time  0.136 ( 0.152)	Data  0.001 ( 0.015)	Loss 9.0670e-03 (9.1721e-03)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [60][ 20/391]	Time  0.135 ( 0.146)	Data  0.001 ( 0.008)	Loss 2.2728e-03 (6.8698e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [60][ 30/391]	Time  0.150 ( 0.143)	Data  0.001 ( 0.006)	Loss 1.2633e-03 (6.1376e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [60][ 40/391]	Time  0.138 ( 0.142)	Data  0.001 ( 0.005)	Loss 3.0276e-03 (6.9270e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [60][ 50/391]	Time  0.139 ( 0.142)	Data  0.001 ( 0.004)	Loss 1.7034e-03 (7.7196e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [60][ 60/391]	Time  0.147 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.3226e-02 (7.8313e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [60][ 70/391]	Time  0.139 ( 0.141)	Data  0.001 ( 0.003)	Loss 2.3109e-03 (7.7667e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [60][ 80/391]	Time  0.140 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.1565e-03 (7.3855e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [60][ 90/391]	Time  0.141 ( 0.140)	Data  0.001 ( 0.003)	Loss 7.2664e-03 (7.0511e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [60][100/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.3979e-03 (7.5125e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [60][110/391]	Time  0.140 ( 0.140)	Data  0.001 ( 0.002)	Loss 1.5954e-03 (7.4182e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [60][120/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.9112e-03 (7.4161e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [60][130/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3896e-02 (7.2872e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [60][140/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.3256e-04 (7.3183e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [60][150/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.7438e-03 (7.1797e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [60][160/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2777e-03 (6.9038e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [60][170/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.0779e-03 (6.8458e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [60][180/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2007e-03 (6.6475e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [60][190/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0563e-02 (6.5656e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [60][200/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.9461e-03 (6.6744e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [60][210/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.6560e-03 (6.6269e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [60][220/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.3907e-03 (6.5709e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [60][230/391]	Time  0.143 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.5115e-03 (6.5198e-03)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [60][240/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.0032e-03 (6.4335e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [60][250/391]	Time  0.145 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2847e-03 (6.3439e-03)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [60][260/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.0577e-03 (6.3203e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [60][270/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4547e-03 (6.2369e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [60][280/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.5636e-03 (6.2539e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [60][290/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.8605e-03 (6.1781e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [60][300/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.4841e-03 (6.1466e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [60][310/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.3382e-03 (6.1257e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [60][320/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.0399e-03 (6.0882e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [60][330/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4243e-03 (6.0670e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [60][340/391]	Time  0.148 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.3889e-03 (6.0717e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [60][350/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7633e-02 (6.1405e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [60][360/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7424e-03 (6.2006e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [60][370/391]	Time  0.146 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.3594e-03 (6.1432e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [60][380/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8970e-03 (6.1479e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [60][390/391]	Time  0.109 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.5261e-04 (6.1000e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
## e[60] optimizer.zero_grad (sum) time: 0.6872494220733643
## e[60]       loss.backward (sum) time: 13.998303413391113
## e[60]      optimizer.step (sum) time: 14.6241455078125
## epoch[60] training(only) time: 54.23780560493469
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 3.5430e-01 (3.5430e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.063 ( 0.062)	Loss 2.6152e-01 (2.7787e-01)	Acc@1  95.00 ( 93.55)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 4.1425e-01 (3.1167e-01)	Acc@1  91.00 ( 93.05)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.047 ( 0.053)	Loss 3.0793e-01 (3.1762e-01)	Acc@1  94.00 ( 93.10)	Acc@5  99.00 ( 99.74)
Test: [ 40/100]	Time  0.048 ( 0.052)	Loss 2.8821e-01 (3.2266e-01)	Acc@1  94.00 ( 92.93)	Acc@5 100.00 ( 99.76)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.9349e-01 (3.2433e-01)	Acc@1  97.00 ( 92.94)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.046 ( 0.051)	Loss 3.5092e-01 (3.1369e-01)	Acc@1  94.00 ( 93.10)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 5.9956e-01 (3.0843e-01)	Acc@1  90.00 ( 93.15)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.047 ( 0.050)	Loss 1.6314e-01 (3.0831e-01)	Acc@1  95.00 ( 93.07)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.046 ( 0.050)	Loss 2.6551e-01 (3.0085e-01)	Acc@1  93.00 ( 93.16)	Acc@5 100.00 ( 99.79)
 * Acc@1 93.220 Acc@5 99.800
### epoch[60] execution time: 59.29903292655945
EPOCH 61
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [61][  0/391]	Time  0.298 ( 0.298)	Data  0.155 ( 0.155)	Loss 6.1649e-03 (6.1649e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [61][ 10/391]	Time  0.134 ( 0.153)	Data  0.001 ( 0.015)	Loss 1.6171e-03 (7.7878e-03)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [61][ 20/391]	Time  0.138 ( 0.146)	Data  0.001 ( 0.008)	Loss 1.1172e-02 (7.5899e-03)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [61][ 30/391]	Time  0.140 ( 0.143)	Data  0.001 ( 0.006)	Loss 3.5986e-03 (6.8518e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [61][ 40/391]	Time  0.140 ( 0.142)	Data  0.001 ( 0.005)	Loss 1.3371e-03 (6.6569e-03)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [61][ 50/391]	Time  0.134 ( 0.141)	Data  0.001 ( 0.004)	Loss 2.8439e-03 (6.3366e-03)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [61][ 60/391]	Time  0.134 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.8014e-03 (5.9719e-03)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [61][ 70/391]	Time  0.140 ( 0.141)	Data  0.001 ( 0.003)	Loss 3.3810e-03 (5.5369e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [61][ 80/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.2805e-03 (5.2906e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [61][ 90/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 8.6126e-03 (5.2851e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [61][100/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.4807e-03 (5.0816e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [61][110/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.0373e-02 (5.1158e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [61][120/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.002)	Loss 4.5705e-03 (4.9381e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [61][130/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.7212e-03 (5.0303e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [61][140/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3317e-02 (5.1320e-03)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [61][150/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.3497e-03 (5.0528e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [61][160/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.5900e-04 (5.0803e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [61][170/391]	Time  0.143 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2487e-02 (5.1326e-03)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [61][180/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.7642e-03 (5.0008e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [61][190/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.0148e-03 (4.9836e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [61][200/391]	Time  0.151 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2821e-03 (5.1180e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [61][210/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.9077e-03 (5.1467e-03)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [61][220/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.4411e-03 (5.1339e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [61][230/391]	Time  0.148 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.4447e-03 (5.1320e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [61][240/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8280e-03 (5.0424e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [61][250/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.9248e-03 (4.9416e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [61][260/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.1389e-03 (4.9073e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [61][270/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5932e-02 (4.9188e-03)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [61][280/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0100e-02 (4.9559e-03)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [61][290/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.6602e-03 (4.9444e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [61][300/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2643e-03 (5.0557e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [61][310/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.8863e-04 (4.9782e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [61][320/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.4198e-03 (5.0173e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [61][330/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.3834e-03 (5.0193e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [61][340/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.7396e-03 (5.0348e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [61][350/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.6869e-03 (5.0640e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [61][360/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.5199e-03 (5.0536e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [61][370/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0277e-03 (4.9894e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [61][380/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.2407e-03 (5.0131e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [61][390/391]	Time  0.110 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.5169e-03 (4.9818e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
## e[61] optimizer.zero_grad (sum) time: 0.6896655559539795
## e[61]       loss.backward (sum) time: 13.967576503753662
## e[61]      optimizer.step (sum) time: 14.643781900405884
## epoch[61] training(only) time: 54.239917039871216
# Switched to evaluate mode...
Test: [  0/100]	Time  0.194 ( 0.194)	Loss 3.3716e-01 (3.3716e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.062)	Loss 2.6328e-01 (2.7450e-01)	Acc@1  94.00 ( 93.64)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.049 ( 0.056)	Loss 4.5913e-01 (3.1196e-01)	Acc@1  91.00 ( 93.05)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.047 ( 0.053)	Loss 3.0894e-01 (3.1867e-01)	Acc@1  94.00 ( 93.16)	Acc@5  99.00 ( 99.74)
Test: [ 40/100]	Time  0.046 ( 0.052)	Loss 2.6614e-01 (3.2412e-01)	Acc@1  93.00 ( 92.95)	Acc@5 100.00 ( 99.76)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 2.1214e-01 (3.2583e-01)	Acc@1  97.00 ( 92.92)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.047 ( 0.051)	Loss 3.1768e-01 (3.1486e-01)	Acc@1  93.00 ( 92.93)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 5.9609e-01 (3.0985e-01)	Acc@1  90.00 ( 93.06)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.6410e-01 (3.1020e-01)	Acc@1  96.00 ( 93.02)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.046 ( 0.050)	Loss 2.8032e-01 (3.0241e-01)	Acc@1  95.00 ( 93.14)	Acc@5 100.00 ( 99.78)
 * Acc@1 93.200 Acc@5 99.790
### epoch[61] execution time: 59.28119134902954
EPOCH 62
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [62][  0/391]	Time  0.300 ( 0.300)	Data  0.149 ( 0.149)	Loss 1.6339e-03 (1.6339e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [62][ 10/391]	Time  0.138 ( 0.153)	Data  0.001 ( 0.014)	Loss 3.6503e-04 (6.0787e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [62][ 20/391]	Time  0.138 ( 0.146)	Data  0.001 ( 0.008)	Loss 2.7883e-03 (4.8115e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [62][ 30/391]	Time  0.135 ( 0.143)	Data  0.001 ( 0.006)	Loss 9.8361e-03 (5.9177e-03)	Acc@1  99.22 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [62][ 40/391]	Time  0.137 ( 0.142)	Data  0.001 ( 0.005)	Loss 1.0215e-02 (5.6438e-03)	Acc@1  99.22 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [62][ 50/391]	Time  0.134 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.3120e-03 (5.4551e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [62][ 60/391]	Time  0.146 ( 0.141)	Data  0.001 ( 0.004)	Loss 2.1446e-02 (5.8836e-03)	Acc@1  98.44 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [62][ 70/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 3.6220e-03 (5.5077e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [62][ 80/391]	Time  0.140 ( 0.140)	Data  0.001 ( 0.003)	Loss 7.5452e-03 (5.3353e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [62][ 90/391]	Time  0.144 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.0769e-03 (5.3770e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [62][100/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.5768e-03 (5.3490e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [62][110/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.0442e-03 (5.1828e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [62][120/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.9060e-03 (5.2316e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [62][130/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0839e-03 (5.2258e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [62][140/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4732e-03 (5.0585e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [62][150/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5614e-03 (5.0461e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [62][160/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.5652e-02 (5.1604e-03)	Acc@1  98.44 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [62][170/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0257e-02 (5.3265e-03)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [62][180/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.6583e-03 (5.2299e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [62][190/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.6473e-03 (5.1095e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [62][200/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2531e-03 (5.1287e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [62][210/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.3803e-03 (5.0957e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [62][220/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3125e-02 (5.0794e-03)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [62][230/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4770e-03 (5.1058e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [62][240/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.8588e-04 (5.0340e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [62][250/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.1625e-03 (5.0355e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [62][260/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.0385e-03 (5.0334e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [62][270/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.8143e-04 (5.0253e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [62][280/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0625e-03 (5.0100e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [62][290/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.6274e-03 (5.0644e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [62][300/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.8941e-03 (5.1999e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [62][310/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1056e-03 (5.2071e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [62][320/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2095e-03 (5.1986e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [62][330/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.0526e-03 (5.2501e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [62][340/391]	Time  0.145 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4570e-03 (5.3109e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [62][350/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4956e-03 (5.2898e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [62][360/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.7870e-03 (5.2496e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [62][370/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5700e-03 (5.2541e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [62][380/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.6014e-04 (5.2151e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [62][390/391]	Time  0.109 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9108e-02 (5.2519e-03)	Acc@1  98.75 ( 99.88)	Acc@5 100.00 (100.00)
## e[62] optimizer.zero_grad (sum) time: 0.6867198944091797
## e[62]       loss.backward (sum) time: 13.957556009292603
## e[62]      optimizer.step (sum) time: 14.60455322265625
## epoch[62] training(only) time: 54.115580797195435
# Switched to evaluate mode...
Test: [  0/100]	Time  0.192 ( 0.192)	Loss 3.6569e-01 (3.6569e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.057 ( 0.061)	Loss 2.4686e-01 (2.7530e-01)	Acc@1  95.00 ( 93.64)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.049 ( 0.054)	Loss 4.2316e-01 (3.0973e-01)	Acc@1  91.00 ( 93.14)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 3.1814e-01 (3.1519e-01)	Acc@1  93.00 ( 93.16)	Acc@5  99.00 ( 99.71)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 2.8662e-01 (3.2260e-01)	Acc@1  93.00 ( 92.95)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.9885e-01 (3.2385e-01)	Acc@1  97.00 ( 92.98)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.059 ( 0.050)	Loss 3.4982e-01 (3.1313e-01)	Acc@1  94.00 ( 93.11)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.050 ( 0.050)	Loss 5.6388e-01 (3.0760e-01)	Acc@1  90.00 ( 93.20)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.050 ( 0.050)	Loss 1.7194e-01 (3.0697e-01)	Acc@1  95.00 ( 93.15)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.046 ( 0.050)	Loss 2.7905e-01 (2.9890e-01)	Acc@1  94.00 ( 93.29)	Acc@5 100.00 ( 99.79)
 * Acc@1 93.320 Acc@5 99.810
### epoch[62] execution time: 59.14270782470703
EPOCH 63
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [63][  0/391]	Time  0.301 ( 0.301)	Data  0.159 ( 0.159)	Loss 1.9540e-03 (1.9540e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [63][ 10/391]	Time  0.135 ( 0.153)	Data  0.001 ( 0.015)	Loss 6.6038e-03 (3.6766e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [63][ 20/391]	Time  0.137 ( 0.146)	Data  0.001 ( 0.009)	Loss 1.2066e-03 (3.2270e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [63][ 30/391]	Time  0.134 ( 0.143)	Data  0.001 ( 0.006)	Loss 2.0889e-03 (3.5520e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [63][ 40/391]	Time  0.140 ( 0.142)	Data  0.001 ( 0.005)	Loss 4.0475e-03 (3.5459e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [63][ 50/391]	Time  0.134 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.7584e-03 (3.7628e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [63][ 60/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.004)	Loss 3.6854e-03 (3.7653e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [63][ 70/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.8764e-02 (4.5986e-03)	Acc@1  99.22 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [63][ 80/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.8223e-03 (4.8622e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [63][ 90/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.9043e-03 (4.8715e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [63][100/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 3.5238e-03 (4.8828e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [63][110/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.9806e-03 (4.9546e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [63][120/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.7973e-03 (4.8756e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [63][130/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7944e-03 (4.8584e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [63][140/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.3568e-03 (5.1185e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [63][150/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0136e-03 (5.0075e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [63][160/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0767e-03 (5.0403e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [63][170/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.3081e-03 (5.0423e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [63][180/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.4527e-03 (4.9853e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [63][190/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.7250e-04 (4.9265e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [63][200/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.6206e-03 (4.9108e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [63][210/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1023e-03 (4.9696e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [63][220/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6924e-03 (4.8968e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [63][230/391]	Time  0.149 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5382e-03 (4.8355e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [63][240/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.6772e-03 (4.7801e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [63][250/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.9496e-03 (4.7218e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [63][260/391]	Time  0.152 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5692e-03 (4.7316e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [63][270/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7774e-03 (4.7008e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [63][280/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3753e-03 (4.6265e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [63][290/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0908e-03 (4.5718e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [63][300/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3782e-02 (4.6095e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [63][310/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2501e-02 (4.6868e-03)	Acc@1  99.22 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [63][320/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5538e-02 (4.6937e-03)	Acc@1  99.22 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [63][330/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.5311e-03 (4.6856e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [63][340/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.7937e-03 (4.6964e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [63][350/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4989e-03 (4.7040e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [63][360/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.3955e-03 (4.7241e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [63][370/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.0301e-03 (4.8049e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [63][380/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.8046e-03 (4.8351e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [63][390/391]	Time  0.104 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.7058e-03 (4.8222e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
## e[63] optimizer.zero_grad (sum) time: 0.6874639987945557
## e[63]       loss.backward (sum) time: 13.995277166366577
## e[63]      optimizer.step (sum) time: 14.64384913444519
## epoch[63] training(only) time: 54.17398262023926
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 3.7051e-01 (3.7051e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.050 ( 0.062)	Loss 2.6837e-01 (2.7306e-01)	Acc@1  94.00 ( 93.73)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.047 ( 0.056)	Loss 4.2337e-01 (3.0717e-01)	Acc@1  91.00 ( 93.05)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 3.0770e-01 (3.1491e-01)	Acc@1  94.00 ( 93.16)	Acc@5  99.00 ( 99.71)
Test: [ 40/100]	Time  0.049 ( 0.052)	Loss 2.7879e-01 (3.2098e-01)	Acc@1  92.00 ( 92.95)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 1.8501e-01 (3.2161e-01)	Acc@1  97.00 ( 92.94)	Acc@5 100.00 ( 99.69)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 3.4474e-01 (3.1121e-01)	Acc@1  93.00 ( 93.02)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 5.8458e-01 (3.0539e-01)	Acc@1  90.00 ( 93.17)	Acc@5 100.00 ( 99.72)
Test: [ 80/100]	Time  0.047 ( 0.050)	Loss 1.6291e-01 (3.0573e-01)	Acc@1  96.00 ( 93.09)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 2.8783e-01 (2.9822e-01)	Acc@1  93.00 ( 93.21)	Acc@5 100.00 ( 99.77)
 * Acc@1 93.220 Acc@5 99.780
### epoch[63] execution time: 59.17584800720215
EPOCH 64
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [64][  0/391]	Time  0.305 ( 0.305)	Data  0.156 ( 0.156)	Loss 3.0308e-03 (3.0308e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [64][ 10/391]	Time  0.138 ( 0.153)	Data  0.001 ( 0.015)	Loss 1.9505e-03 (5.5843e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [64][ 20/391]	Time  0.140 ( 0.146)	Data  0.001 ( 0.009)	Loss 1.6345e-02 (5.3797e-03)	Acc@1  99.22 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [64][ 30/391]	Time  0.136 ( 0.143)	Data  0.001 ( 0.006)	Loss 4.8407e-03 (5.4497e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [64][ 40/391]	Time  0.133 ( 0.142)	Data  0.001 ( 0.005)	Loss 8.5691e-04 (5.1557e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [64][ 50/391]	Time  0.136 ( 0.142)	Data  0.001 ( 0.004)	Loss 2.9423e-03 (5.1533e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [64][ 60/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.2402e-03 (4.8003e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [64][ 70/391]	Time  0.134 ( 0.141)	Data  0.001 ( 0.003)	Loss 6.4840e-03 (4.6045e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [64][ 80/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.003)	Loss 9.9535e-04 (4.5972e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [64][ 90/391]	Time  0.149 ( 0.140)	Data  0.001 ( 0.003)	Loss 7.2402e-03 (5.0941e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [64][100/391]	Time  0.140 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.3733e-03 (4.9970e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [64][110/391]	Time  0.140 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.6365e-03 (5.0105e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [64][120/391]	Time  0.145 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.2264e-03 (5.1094e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [64][130/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1890e-03 (4.9976e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [64][140/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.4484e-03 (5.0491e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [64][150/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.4714e-03 (5.0652e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [64][160/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3695e-03 (4.9256e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [64][170/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4721e-03 (4.8170e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [64][180/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4806e-03 (4.7359e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [64][190/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6028e-03 (4.6348e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [64][200/391]	Time  0.146 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.8282e-03 (4.6896e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [64][210/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.5709e-04 (4.6680e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [64][220/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.1094e-03 (4.8421e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [64][230/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.1728e-03 (4.8125e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [64][240/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.9301e-03 (4.7061e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [64][250/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.4102e-03 (4.6948e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [64][260/391]	Time  0.144 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.4173e-03 (4.6947e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [64][270/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6976e-03 (4.8017e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [64][280/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4443e-02 (4.9519e-03)	Acc@1  99.22 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [64][290/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.2701e-03 (4.8981e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [64][300/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2041e-03 (4.8626e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [64][310/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.8875e-03 (4.8560e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [64][320/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.0174e-03 (4.8785e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [64][330/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.4865e-03 (4.8301e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [64][340/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5441e-03 (4.9118e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [64][350/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.4130e-04 (4.9662e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [64][360/391]	Time  0.146 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.3838e-03 (4.9034e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [64][370/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.0407e-03 (4.8583e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [64][380/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.1070e-03 (4.8539e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [64][390/391]	Time  0.108 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0617e-02 (4.8580e-03)	Acc@1  98.75 ( 99.92)	Acc@5 100.00 (100.00)
## e[64] optimizer.zero_grad (sum) time: 0.6825416088104248
## e[64]       loss.backward (sum) time: 14.015427350997925
## e[64]      optimizer.step (sum) time: 14.6516592502594
## epoch[64] training(only) time: 54.27202606201172
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 3.2845e-01 (3.2845e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 2.5403e-01 (2.6757e-01)	Acc@1  95.00 ( 94.00)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 4.6386e-01 (3.0657e-01)	Acc@1  91.00 ( 93.33)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.048 ( 0.053)	Loss 2.9940e-01 (3.1090e-01)	Acc@1  93.00 ( 93.26)	Acc@5  99.00 ( 99.71)
Test: [ 40/100]	Time  0.048 ( 0.052)	Loss 2.8365e-01 (3.1929e-01)	Acc@1  91.00 ( 93.02)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 2.0692e-01 (3.2130e-01)	Acc@1  97.00 ( 93.00)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 3.1171e-01 (3.0943e-01)	Acc@1  94.00 ( 93.13)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.049 ( 0.050)	Loss 5.7709e-01 (3.0395e-01)	Acc@1  91.00 ( 93.25)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.047 ( 0.050)	Loss 1.5551e-01 (3.0354e-01)	Acc@1  96.00 ( 93.23)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 2.6408e-01 (2.9618e-01)	Acc@1  94.00 ( 93.32)	Acc@5 100.00 ( 99.79)
 * Acc@1 93.350 Acc@5 99.800
### epoch[64] execution time: 59.279518127441406
EPOCH 65
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [65][  0/391]	Time  0.307 ( 0.307)	Data  0.159 ( 0.159)	Loss 1.0678e-02 (1.0678e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [65][ 10/391]	Time  0.136 ( 0.153)	Data  0.001 ( 0.015)	Loss 8.6157e-04 (2.9887e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [65][ 20/391]	Time  0.139 ( 0.146)	Data  0.001 ( 0.009)	Loss 1.2281e-03 (5.3312e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [65][ 30/391]	Time  0.140 ( 0.144)	Data  0.001 ( 0.006)	Loss 2.2660e-03 (4.9216e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [65][ 40/391]	Time  0.137 ( 0.142)	Data  0.001 ( 0.005)	Loss 1.7208e-03 (4.9824e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [65][ 50/391]	Time  0.141 ( 0.141)	Data  0.001 ( 0.004)	Loss 3.4305e-03 (4.7719e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [65][ 60/391]	Time  0.138 ( 0.141)	Data  0.001 ( 0.004)	Loss 3.6624e-03 (4.8427e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [65][ 70/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.003)	Loss 4.2305e-03 (5.0805e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [65][ 80/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.6626e-03 (4.8795e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [65][ 90/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.003)	Loss 7.1829e-03 (4.7341e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [65][100/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.3863e-03 (4.6248e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [65][110/391]	Time  0.146 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.5496e-03 (4.4540e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [65][120/391]	Time  0.143 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7039e-02 (4.4835e-03)	Acc@1  99.22 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [65][130/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0412e-03 (4.4619e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [65][140/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.1232e-03 (4.3898e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [65][150/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1346e-03 (4.4666e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [65][160/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0401e-03 (4.3994e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [65][170/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0300e-02 (4.5659e-03)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [65][180/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3566e-03 (4.5006e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [65][190/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1011e-02 (4.5237e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [65][200/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.5783e-03 (4.5139e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [65][210/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.9120e-03 (4.5334e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [65][220/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.9956e-03 (4.5231e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [65][230/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.8875e-03 (4.4812e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [65][240/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.4638e-03 (4.5774e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [65][250/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.9563e-03 (4.5694e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [65][260/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.5759e-03 (4.5170e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [65][270/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7570e-03 (4.4440e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [65][280/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2489e-03 (4.4488e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [65][290/391]	Time  0.147 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4340e-03 (4.3806e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [65][300/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.6873e-03 (4.4019e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [65][310/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9216e-03 (4.3930e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [65][320/391]	Time  0.148 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.5966e-03 (4.3552e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [65][330/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.6955e-03 (4.3720e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [65][340/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2107e-03 (4.3522e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [65][350/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0346e-03 (4.4724e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [65][360/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.3087e-03 (4.5876e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [65][370/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.4459e-02 (4.6125e-03)	Acc@1  99.22 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [65][380/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.4857e-04 (4.5988e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [65][390/391]	Time  0.107 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.3622e-03 (4.6192e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
## e[65] optimizer.zero_grad (sum) time: 0.6864380836486816
## e[65]       loss.backward (sum) time: 13.937746286392212
## e[65]      optimizer.step (sum) time: 14.6189603805542
## epoch[65] training(only) time: 54.12187886238098
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 3.5757e-01 (3.5757e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 2.4690e-01 (2.7888e-01)	Acc@1  95.00 ( 93.91)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.047 ( 0.056)	Loss 4.5607e-01 (3.1625e-01)	Acc@1  91.00 ( 93.29)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.049 ( 0.053)	Loss 2.9584e-01 (3.1835e-01)	Acc@1  93.00 ( 93.23)	Acc@5  99.00 ( 99.74)
Test: [ 40/100]	Time  0.047 ( 0.052)	Loss 2.4857e-01 (3.2576e-01)	Acc@1  93.00 ( 92.98)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.8474e-01 (3.2881e-01)	Acc@1  97.00 ( 92.98)	Acc@5 100.00 ( 99.67)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 2.9179e-01 (3.1493e-01)	Acc@1  94.00 ( 93.07)	Acc@5 100.00 ( 99.72)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 5.9860e-01 (3.0832e-01)	Acc@1  90.00 ( 93.14)	Acc@5 100.00 ( 99.70)
Test: [ 80/100]	Time  0.050 ( 0.050)	Loss 1.5506e-01 (3.0959e-01)	Acc@1  96.00 ( 93.10)	Acc@5 100.00 ( 99.73)
Test: [ 90/100]	Time  0.047 ( 0.050)	Loss 3.0104e-01 (3.0286e-01)	Acc@1  93.00 ( 93.23)	Acc@5 100.00 ( 99.76)
 * Acc@1 93.280 Acc@5 99.780
### epoch[65] execution time: 59.14988327026367
EPOCH 66
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [66][  0/391]	Time  0.312 ( 0.312)	Data  0.163 ( 0.163)	Loss 2.1116e-02 (2.1116e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [66][ 10/391]	Time  0.135 ( 0.154)	Data  0.001 ( 0.016)	Loss 4.2877e-03 (5.7877e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [66][ 20/391]	Time  0.138 ( 0.147)	Data  0.001 ( 0.009)	Loss 6.5694e-03 (6.5080e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [66][ 30/391]	Time  0.136 ( 0.144)	Data  0.001 ( 0.006)	Loss 7.1496e-03 (5.5015e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [66][ 40/391]	Time  0.137 ( 0.143)	Data  0.001 ( 0.005)	Loss 3.1783e-03 (5.1476e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [66][ 50/391]	Time  0.133 ( 0.141)	Data  0.001 ( 0.004)	Loss 6.9727e-03 (4.9196e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [66][ 60/391]	Time  0.139 ( 0.141)	Data  0.001 ( 0.004)	Loss 3.9888e-03 (4.8056e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [66][ 70/391]	Time  0.136 ( 0.141)	Data  0.001 ( 0.003)	Loss 1.8027e-03 (4.5648e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [66][ 80/391]	Time  0.140 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.5850e-03 (4.4019e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [66][ 90/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.3567e-03 (4.1891e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [66][100/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.7095e-03 (4.1048e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [66][110/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.3652e-03 (4.1607e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [66][120/391]	Time  0.147 ( 0.139)	Data  0.001 ( 0.003)	Loss 4.2226e-03 (4.1897e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [66][130/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6561e-03 (4.1983e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [66][140/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.7833e-03 (4.2318e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [66][150/391]	Time  0.148 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.9510e-03 (4.1849e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [66][160/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.2243e-03 (4.1264e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [66][170/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.2301e-03 (4.0531e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [66][180/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.1357e-04 (3.9481e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [66][190/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3390e-02 (3.9306e-03)	Acc@1  99.22 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [66][200/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.8997e-03 (3.9481e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [66][210/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.9470e-03 (3.9974e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [66][220/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5867e-02 (4.1235e-03)	Acc@1  99.22 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [66][230/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2663e-03 (4.1977e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [66][240/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0939e-03 (4.1554e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [66][250/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.5503e-03 (4.1596e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [66][260/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.9915e-03 (4.1461e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [66][270/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.7538e-03 (4.1358e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [66][280/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.4775e-03 (4.1886e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [66][290/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8076e-03 (4.2344e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [66][300/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0067e-03 (4.1753e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [66][310/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.2929e-03 (4.1910e-03)	Acc@1  99.22 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [66][320/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.6963e-03 (4.1747e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [66][330/391]	Time  0.142 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4147e-03 (4.1585e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [66][340/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7124e-03 (4.1447e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [66][350/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5730e-03 (4.2076e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [66][360/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.6998e-03 (4.1866e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [66][370/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.1525e-03 (4.1387e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [66][380/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.2773e-03 (4.1326e-03)	Acc@1  99.22 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [66][390/391]	Time  0.110 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.7579e-03 (4.1362e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
## e[66] optimizer.zero_grad (sum) time: 0.6876311302185059
## e[66]       loss.backward (sum) time: 13.953782796859741
## e[66]      optimizer.step (sum) time: 14.643346309661865
## epoch[66] training(only) time: 54.19078016281128
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 3.3655e-01 (3.3655e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 2.6051e-01 (2.7111e-01)	Acc@1  95.00 ( 93.82)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 4.1869e-01 (3.0889e-01)	Acc@1  91.00 ( 93.14)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 3.1154e-01 (3.1457e-01)	Acc@1  93.00 ( 93.10)	Acc@5  99.00 ( 99.74)
Test: [ 40/100]	Time  0.049 ( 0.051)	Loss 2.4803e-01 (3.1995e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 ( 99.76)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.7551e-01 (3.2065e-01)	Acc@1  96.00 ( 93.02)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 3.3498e-01 (3.0997e-01)	Acc@1  94.00 ( 93.13)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 5.7193e-01 (3.0433e-01)	Acc@1  91.00 ( 93.24)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.050 ( 0.050)	Loss 1.5968e-01 (3.0426e-01)	Acc@1  95.00 ( 93.16)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.049 ( 0.049)	Loss 2.8920e-01 (2.9666e-01)	Acc@1  93.00 ( 93.26)	Acc@5 100.00 ( 99.78)
 * Acc@1 93.290 Acc@5 99.790
### epoch[66] execution time: 59.21553564071655
EPOCH 67
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [67][  0/391]	Time  0.309 ( 0.309)	Data  0.159 ( 0.159)	Loss 1.6904e-03 (1.6904e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [67][ 10/391]	Time  0.144 ( 0.153)	Data  0.001 ( 0.015)	Loss 2.2123e-03 (3.8344e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [67][ 20/391]	Time  0.137 ( 0.146)	Data  0.001 ( 0.009)	Loss 6.7109e-03 (3.4536e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [67][ 30/391]	Time  0.140 ( 0.143)	Data  0.001 ( 0.006)	Loss 6.2303e-04 (3.6212e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [67][ 40/391]	Time  0.142 ( 0.142)	Data  0.001 ( 0.005)	Loss 4.6003e-03 (3.8990e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [67][ 50/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.9516e-02 (4.1137e-03)	Acc@1  99.22 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [67][ 60/391]	Time  0.138 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.0046e-02 (4.4533e-03)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [67][ 70/391]	Time  0.140 ( 0.141)	Data  0.001 ( 0.003)	Loss 2.2300e-02 (4.6220e-03)	Acc@1  98.44 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [67][ 80/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.9996e-03 (4.5685e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [67][ 90/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.003)	Loss 6.7049e-03 (4.6503e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [67][100/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 4.2883e-03 (4.5041e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [67][110/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 3.8100e-03 (4.4660e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [67][120/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.8654e-04 (4.4345e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [67][130/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0434e-03 (4.5299e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [67][140/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.7376e-03 (4.5260e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [67][150/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.0809e-03 (4.5298e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [67][160/391]	Time  0.139 ( 0.139)	Data  0.003 ( 0.002)	Loss 2.5033e-03 (4.5934e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [67][170/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.5518e-03 (4.4974e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [67][180/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.9078e-03 (4.5608e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [67][190/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.9271e-03 (4.5193e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [67][200/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7242e-03 (4.4856e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [67][210/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6417e-03 (4.5032e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [67][220/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.1047e-03 (4.4424e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [67][230/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.1565e-03 (4.3722e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [67][240/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2726e-02 (4.4419e-03)	Acc@1  99.22 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [67][250/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.1906e-03 (4.3909e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [67][260/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0342e-02 (4.3580e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [67][270/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6681e-03 (4.3771e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [67][280/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2578e-02 (4.4137e-03)	Acc@1  99.22 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [67][290/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.9988e-03 (4.3972e-03)	Acc@1  99.22 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [67][300/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.3993e-03 (4.3599e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [67][310/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.5784e-03 (4.3840e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [67][320/391]	Time  0.143 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4555e-02 (4.4781e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [67][330/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2125e-02 (4.5792e-03)	Acc@1  99.22 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [67][340/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.0987e-03 (4.5397e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [67][350/391]	Time  0.146 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.9262e-03 (4.5866e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [67][360/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.4711e-03 (4.5852e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [67][370/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.7589e-03 (4.5795e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [67][380/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.0417e-03 (4.5345e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [67][390/391]	Time  0.107 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.5514e-03 (4.5246e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
## e[67] optimizer.zero_grad (sum) time: 0.6872241497039795
## e[67]       loss.backward (sum) time: 13.976857423782349
## e[67]      optimizer.step (sum) time: 14.635126829147339
## epoch[67] training(only) time: 54.16824817657471
# Switched to evaluate mode...
Test: [  0/100]	Time  0.194 ( 0.194)	Loss 3.4539e-01 (3.4539e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 2.5678e-01 (2.7216e-01)	Acc@1  95.00 ( 93.91)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.060 ( 0.056)	Loss 4.3221e-01 (3.0717e-01)	Acc@1  91.00 ( 93.14)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 2.9994e-01 (3.1032e-01)	Acc@1  93.00 ( 93.23)	Acc@5  99.00 ( 99.74)
Test: [ 40/100]	Time  0.048 ( 0.051)	Loss 2.6810e-01 (3.1876e-01)	Acc@1  92.00 ( 92.93)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.8430e-01 (3.2099e-01)	Acc@1  97.00 ( 92.94)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 3.2230e-01 (3.0848e-01)	Acc@1  94.00 ( 93.10)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.056 ( 0.050)	Loss 6.0272e-01 (3.0254e-01)	Acc@1  90.00 ( 93.20)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.049 ( 0.050)	Loss 1.5270e-01 (3.0231e-01)	Acc@1  96.00 ( 93.12)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 2.8897e-01 (2.9524e-01)	Acc@1  93.00 ( 93.29)	Acc@5 100.00 ( 99.78)
 * Acc@1 93.320 Acc@5 99.790
### epoch[67] execution time: 59.16816568374634
EPOCH 68
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [68][  0/391]	Time  0.304 ( 0.304)	Data  0.161 ( 0.161)	Loss 7.4698e-03 (7.4698e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [68][ 10/391]	Time  0.137 ( 0.152)	Data  0.001 ( 0.016)	Loss 1.0590e-02 (5.3812e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [68][ 20/391]	Time  0.138 ( 0.145)	Data  0.001 ( 0.009)	Loss 5.1490e-04 (4.9375e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [68][ 30/391]	Time  0.138 ( 0.143)	Data  0.001 ( 0.006)	Loss 1.0831e-02 (5.2461e-03)	Acc@1  99.22 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [68][ 40/391]	Time  0.139 ( 0.142)	Data  0.001 ( 0.005)	Loss 3.1070e-03 (5.0220e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [68][ 50/391]	Time  0.139 ( 0.141)	Data  0.001 ( 0.004)	Loss 2.2102e-03 (5.2533e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [68][ 60/391]	Time  0.136 ( 0.141)	Data  0.001 ( 0.004)	Loss 6.6083e-04 (4.9667e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [68][ 70/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 9.6042e-04 (4.8289e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [68][ 80/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.003)	Loss 6.8911e-03 (4.6646e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [68][ 90/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.8973e-03 (4.4897e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [68][100/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 3.3284e-03 (4.9280e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [68][110/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.7233e-03 (4.9448e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [68][120/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 5.2519e-04 (5.0552e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [68][130/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.4064e-03 (4.9784e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [68][140/391]	Time  0.146 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.2022e-03 (4.8305e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [68][150/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.7803e-03 (4.7660e-03)	Acc@1  99.22 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [68][160/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.5737e-02 (4.9147e-03)	Acc@1  99.22 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [68][170/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.8264e-03 (4.9107e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [68][180/391]	Time  0.147 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.6406e-03 (4.8574e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [68][190/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0447e-02 (5.0154e-03)	Acc@1  99.22 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [68][200/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.6699e-03 (5.0124e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [68][210/391]	Time  0.149 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4042e-03 (4.9076e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [68][220/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1911e-02 (4.9408e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [68][230/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2541e-03 (4.9573e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [68][240/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.3855e-03 (4.9160e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [68][250/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.2479e-03 (4.8866e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [68][260/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0959e-03 (4.8940e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [68][270/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.2141e-03 (4.8430e-03)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [68][280/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.4669e-03 (4.8492e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [68][290/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.5242e-03 (4.8370e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [68][300/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8416e-03 (4.8499e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [68][310/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.1818e-03 (4.9543e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [68][320/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4079e-03 (4.9237e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [68][330/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1658e-02 (4.9318e-03)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [68][340/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.2209e-03 (4.9269e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [68][350/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.8595e-03 (4.9530e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [68][360/391]	Time  0.148 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5155e-03 (4.9398e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [68][370/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.1688e-03 (4.8621e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [68][380/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.6782e-04 (4.8465e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [68][390/391]	Time  0.111 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5377e-03 (4.8536e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
## e[68] optimizer.zero_grad (sum) time: 0.6940526962280273
## e[68]       loss.backward (sum) time: 13.92827582359314
## e[68]      optimizer.step (sum) time: 14.66010046005249
## epoch[68] training(only) time: 54.141072511672974
# Switched to evaluate mode...
Test: [  0/100]	Time  0.182 ( 0.182)	Loss 3.3321e-01 (3.3321e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.050 ( 0.060)	Loss 2.7735e-01 (2.7568e-01)	Acc@1  95.00 ( 93.91)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.050 ( 0.055)	Loss 4.5995e-01 (3.1327e-01)	Acc@1  91.00 ( 93.19)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 3.0809e-01 (3.1853e-01)	Acc@1  93.00 ( 93.19)	Acc@5  99.00 ( 99.74)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 2.7265e-01 (3.2658e-01)	Acc@1  93.00 ( 93.10)	Acc@5 100.00 ( 99.76)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 2.0253e-01 (3.2744e-01)	Acc@1  96.00 ( 93.02)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 3.3813e-01 (3.1619e-01)	Acc@1  93.00 ( 93.15)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.049 ( 0.050)	Loss 5.7547e-01 (3.1033e-01)	Acc@1  91.00 ( 93.28)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.050 ( 0.049)	Loss 1.5250e-01 (3.0938e-01)	Acc@1  96.00 ( 93.23)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 2.8140e-01 (3.0162e-01)	Acc@1  94.00 ( 93.33)	Acc@5 100.00 ( 99.79)
 * Acc@1 93.360 Acc@5 99.800
### epoch[68] execution time: 59.14645767211914
EPOCH 69
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [69][  0/391]	Time  0.307 ( 0.307)	Data  0.160 ( 0.160)	Loss 1.9710e-03 (1.9710e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [69][ 10/391]	Time  0.151 ( 0.154)	Data  0.001 ( 0.016)	Loss 1.6513e-03 (5.2860e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [69][ 20/391]	Time  0.143 ( 0.146)	Data  0.001 ( 0.009)	Loss 9.2215e-04 (5.5512e-03)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [69][ 30/391]	Time  0.137 ( 0.144)	Data  0.001 ( 0.006)	Loss 3.1596e-03 (4.9334e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [69][ 40/391]	Time  0.144 ( 0.142)	Data  0.001 ( 0.005)	Loss 1.2181e-03 (4.3384e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [69][ 50/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.004)	Loss 3.1783e-03 (4.0664e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [69][ 60/391]	Time  0.136 ( 0.141)	Data  0.001 ( 0.004)	Loss 6.3642e-03 (4.2941e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [69][ 70/391]	Time  0.136 ( 0.141)	Data  0.001 ( 0.003)	Loss 6.1295e-03 (4.3607e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [69][ 80/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.003)	Loss 3.7669e-03 (4.2379e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [69][ 90/391]	Time  0.144 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.4590e-03 (4.1998e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [69][100/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 7.7727e-03 (4.2364e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [69][110/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.1645e-02 (4.3591e-03)	Acc@1  99.22 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [69][120/391]	Time  0.140 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.5209e-03 (4.1388e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [69][130/391]	Time  0.144 ( 0.140)	Data  0.001 ( 0.002)	Loss 3.1827e-03 (3.9796e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [69][140/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.5322e-03 (3.9062e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [69][150/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1790e-03 (3.9810e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [69][160/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.9826e-03 (3.9391e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [69][170/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.8955e-03 (3.8437e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [69][180/391]	Time  0.145 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.6294e-02 (4.1116e-03)	Acc@1  99.22 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [69][190/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3488e-03 (4.1468e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [69][200/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.6030e-03 (4.1177e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [69][210/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.3620e-03 (4.0977e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [69][220/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.7872e-03 (4.0465e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [69][230/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.9871e-03 (3.9713e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [69][240/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2598e-03 (3.9598e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [69][250/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.9978e-03 (3.9235e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [69][260/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.6755e-04 (3.8868e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [69][270/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5943e-02 (3.9648e-03)	Acc@1  99.22 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [69][280/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.9202e-04 (3.9786e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [69][290/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.5130e-03 (4.0913e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [69][300/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.4926e-04 (4.0863e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [69][310/391]	Time  0.143 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.6858e-03 (4.0917e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [69][320/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6970e-03 (4.2546e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [69][330/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.5901e-03 (4.2333e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [69][340/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.9388e-03 (4.2339e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [69][350/391]	Time  0.148 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0495e-03 (4.1891e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [69][360/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.0710e-03 (4.1707e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [69][370/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0674e-03 (4.1619e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [69][380/391]	Time  0.146 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.9657e-03 (4.1363e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [69][390/391]	Time  0.105 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.3998e-03 (4.1661e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
## e[69] optimizer.zero_grad (sum) time: 0.6855058670043945
## e[69]       loss.backward (sum) time: 13.986426591873169
## e[69]      optimizer.step (sum) time: 14.64536738395691
## epoch[69] training(only) time: 54.191813230514526
# Switched to evaluate mode...
Test: [  0/100]	Time  0.194 ( 0.194)	Loss 3.7056e-01 (3.7056e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.063)	Loss 2.6276e-01 (2.7974e-01)	Acc@1  95.00 ( 93.64)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.050 ( 0.055)	Loss 4.6827e-01 (3.1757e-01)	Acc@1  91.00 ( 93.14)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.046 ( 0.054)	Loss 3.2558e-01 (3.2201e-01)	Acc@1  93.00 ( 93.13)	Acc@5  99.00 ( 99.74)
Test: [ 40/100]	Time  0.048 ( 0.053)	Loss 2.5074e-01 (3.2913e-01)	Acc@1  94.00 ( 92.98)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.048 ( 0.052)	Loss 2.1564e-01 (3.2887e-01)	Acc@1  96.00 ( 92.98)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.046 ( 0.052)	Loss 3.2366e-01 (3.1738e-01)	Acc@1  93.00 ( 93.11)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.047 ( 0.051)	Loss 5.4227e-01 (3.1051e-01)	Acc@1  91.00 ( 93.23)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.050 ( 0.051)	Loss 1.5223e-01 (3.0963e-01)	Acc@1  96.00 ( 93.19)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.047 ( 0.050)	Loss 3.0105e-01 (3.0151e-01)	Acc@1  93.00 ( 93.31)	Acc@5 100.00 ( 99.79)
 * Acc@1 93.330 Acc@5 99.810
### epoch[69] execution time: 59.310274600982666
EPOCH 70
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [70][  0/391]	Time  0.312 ( 0.312)	Data  0.163 ( 0.163)	Loss 1.9520e-03 (1.9520e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [70][ 10/391]	Time  0.140 ( 0.154)	Data  0.001 ( 0.016)	Loss 1.9518e-03 (5.2973e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [70][ 20/391]	Time  0.138 ( 0.147)	Data  0.001 ( 0.009)	Loss 4.9453e-03 (4.3911e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [70][ 30/391]	Time  0.137 ( 0.144)	Data  0.001 ( 0.006)	Loss 5.7536e-03 (4.1206e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [70][ 40/391]	Time  0.137 ( 0.143)	Data  0.001 ( 0.005)	Loss 2.9634e-03 (4.1163e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [70][ 50/391]	Time  0.137 ( 0.142)	Data  0.001 ( 0.004)	Loss 1.9667e-03 (4.5039e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [70][ 60/391]	Time  0.135 ( 0.141)	Data  0.001 ( 0.004)	Loss 2.3555e-03 (4.3351e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [70][ 70/391]	Time  0.135 ( 0.141)	Data  0.001 ( 0.003)	Loss 9.3375e-03 (4.6863e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [70][ 80/391]	Time  0.140 ( 0.141)	Data  0.001 ( 0.003)	Loss 1.9968e-02 (5.1700e-03)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [70][ 90/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.4761e-03 (4.9019e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [70][100/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.7156e-03 (4.6924e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [70][110/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.7271e-03 (4.5766e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [70][120/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.7180e-03 (4.4769e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [70][130/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1258e-03 (4.3882e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [70][140/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1600e-02 (4.4271e-03)	Acc@1  99.22 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [70][150/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0763e-03 (4.4114e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [70][160/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1376e-03 (4.3112e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [70][170/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.2359e-03 (4.2352e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [70][180/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.4182e-03 (4.3111e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [70][190/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1712e-03 (4.3127e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [70][200/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.8624e-03 (4.3194e-03)	Acc@1  99.22 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [70][210/391]	Time  0.143 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.7422e-03 (4.3199e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [70][220/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.1233e-03 (4.2899e-03)	Acc@1  99.22 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [70][230/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.3882e-03 (4.3550e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [70][240/391]	Time  0.142 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.4189e-03 (4.3673e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [70][250/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.7110e-03 (4.3588e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [70][260/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.8630e-04 (4.3643e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [70][270/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.6304e-04 (4.3380e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [70][280/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1010e-03 (4.3979e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [70][290/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0097e-03 (4.3766e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [70][300/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.9547e-03 (4.3179e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [70][310/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5889e-02 (4.3666e-03)	Acc@1  99.22 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [70][320/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.3558e-03 (4.3386e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [70][330/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.7568e-03 (4.3447e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [70][340/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9542e-03 (4.3649e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [70][350/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.0897e-03 (4.3824e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [70][360/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.7269e-03 (4.3938e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [70][370/391]	Time  0.146 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.3416e-03 (4.3829e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [70][380/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1179e-02 (4.3656e-03)	Acc@1  99.22 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [70][390/391]	Time  0.109 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.0518e-04 (4.4018e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
## e[70] optimizer.zero_grad (sum) time: 0.6868150234222412
## e[70]       loss.backward (sum) time: 13.941193103790283
## e[70]      optimizer.step (sum) time: 14.616819143295288
## epoch[70] training(only) time: 54.15846085548401
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 3.3769e-01 (3.3769e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 2.5845e-01 (2.7756e-01)	Acc@1  95.00 ( 93.82)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 4.5325e-01 (3.1399e-01)	Acc@1  91.00 ( 93.14)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 3.1811e-01 (3.1843e-01)	Acc@1  93.00 ( 93.16)	Acc@5  99.00 ( 99.74)
Test: [ 40/100]	Time  0.051 ( 0.051)	Loss 2.4600e-01 (3.2406e-01)	Acc@1  94.00 ( 93.02)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.9334e-01 (3.2526e-01)	Acc@1  96.00 ( 92.98)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 3.0051e-01 (3.1309e-01)	Acc@1  94.00 ( 93.11)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.049 ( 0.050)	Loss 5.9154e-01 (3.0710e-01)	Acc@1  91.00 ( 93.20)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.048 ( 0.049)	Loss 1.6438e-01 (3.0713e-01)	Acc@1  96.00 ( 93.14)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 3.0852e-01 (2.9930e-01)	Acc@1  93.00 ( 93.24)	Acc@5 100.00 ( 99.78)
 * Acc@1 93.280 Acc@5 99.800
### epoch[70] execution time: 59.179250955581665
EPOCH 71
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [71][  0/391]	Time  0.289 ( 0.289)	Data  0.137 ( 0.137)	Loss 1.0431e-02 (1.0431e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [71][ 10/391]	Time  0.136 ( 0.151)	Data  0.001 ( 0.013)	Loss 2.1466e-03 (4.4113e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [71][ 20/391]	Time  0.137 ( 0.146)	Data  0.001 ( 0.008)	Loss 7.2105e-04 (3.8065e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [71][ 30/391]	Time  0.142 ( 0.144)	Data  0.001 ( 0.006)	Loss 1.4457e-02 (3.8660e-03)	Acc@1  99.22 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [71][ 40/391]	Time  0.135 ( 0.142)	Data  0.001 ( 0.004)	Loss 3.6072e-03 (3.6903e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [71][ 50/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.0462e-03 (3.7473e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [71][ 60/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.1787e-03 (3.6373e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [71][ 70/391]	Time  0.148 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.3200e-03 (3.5642e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [71][ 80/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.0008e-03 (3.5311e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [71][ 90/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.7732e-03 (3.5393e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [71][100/391]	Time  0.147 ( 0.140)	Data  0.001 ( 0.002)	Loss 6.4348e-03 (3.6152e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [71][110/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.002)	Loss 2.7394e-03 (3.6908e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [71][120/391]	Time  0.146 ( 0.140)	Data  0.001 ( 0.002)	Loss 1.0090e-02 (3.6785e-03)	Acc@1  99.22 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [71][130/391]	Time  0.140 ( 0.140)	Data  0.001 ( 0.002)	Loss 4.5591e-03 (3.8330e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [71][140/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2887e-03 (3.8092e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [71][150/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.5170e-03 (3.7759e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [71][160/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.4144e-03 (3.9460e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [71][170/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.4771e-03 (3.9482e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [71][180/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1680e-03 (3.8457e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [71][190/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.3629e-03 (4.0224e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [71][200/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.3355e-03 (3.9785e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [71][210/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8846e-03 (4.0221e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [71][220/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.5143e-04 (3.9509e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [71][230/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.2460e-03 (3.9554e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [71][240/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.9033e-03 (3.9777e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [71][250/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.2242e-03 (4.0248e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [71][260/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.4750e-03 (4.0010e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [71][270/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.0749e-03 (4.0095e-03)	Acc@1  99.22 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [71][280/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8576e-03 (4.0031e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [71][290/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.9106e-03 (4.0170e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [71][300/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.7284e-03 (4.0469e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [71][310/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.5267e-03 (4.0638e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [71][320/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1329e-03 (4.0071e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [71][330/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8718e-03 (4.0541e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [71][340/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.4800e-03 (4.0221e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [71][350/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.1847e-03 (4.0190e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [71][360/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.6029e-03 (4.0563e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [71][370/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.6378e-03 (4.0434e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [71][380/391]	Time  0.144 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.3490e-03 (4.0572e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [71][390/391]	Time  0.106 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.0645e-03 (4.0597e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
## e[71] optimizer.zero_grad (sum) time: 0.6851003170013428
## e[71]       loss.backward (sum) time: 14.00172209739685
## e[71]      optimizer.step (sum) time: 14.627584218978882
## epoch[71] training(only) time: 54.171568870544434
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 3.3745e-01 (3.3745e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 2.5437e-01 (2.7545e-01)	Acc@1  96.00 ( 94.18)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.048 ( 0.055)	Loss 4.4177e-01 (3.1345e-01)	Acc@1  91.00 ( 93.43)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.050 ( 0.052)	Loss 3.2137e-01 (3.1726e-01)	Acc@1  93.00 ( 93.32)	Acc@5  99.00 ( 99.71)
Test: [ 40/100]	Time  0.049 ( 0.052)	Loss 2.5635e-01 (3.2414e-01)	Acc@1  92.00 ( 93.07)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.050 ( 0.051)	Loss 2.0120e-01 (3.2435e-01)	Acc@1  96.00 ( 93.12)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 3.3773e-01 (3.1362e-01)	Acc@1  94.00 ( 93.25)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 5.3731e-01 (3.0723e-01)	Acc@1  91.00 ( 93.32)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 1.4520e-01 (3.0573e-01)	Acc@1  96.00 ( 93.30)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.051 ( 0.049)	Loss 3.0399e-01 (2.9800e-01)	Acc@1  93.00 ( 93.43)	Acc@5 100.00 ( 99.79)
 * Acc@1 93.430 Acc@5 99.810
### epoch[71] execution time: 59.167869329452515
EPOCH 72
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [72][  0/391]	Time  0.329 ( 0.329)	Data  0.180 ( 0.180)	Loss 3.9351e-03 (3.9351e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [72][ 10/391]	Time  0.133 ( 0.156)	Data  0.001 ( 0.017)	Loss 1.4794e-03 (4.9141e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [72][ 20/391]	Time  0.145 ( 0.148)	Data  0.001 ( 0.010)	Loss 3.2174e-03 (4.5040e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [72][ 30/391]	Time  0.138 ( 0.145)	Data  0.001 ( 0.007)	Loss 1.2326e-03 (4.5021e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [72][ 40/391]	Time  0.137 ( 0.143)	Data  0.001 ( 0.006)	Loss 2.3700e-03 (4.9034e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [72][ 50/391]	Time  0.135 ( 0.142)	Data  0.001 ( 0.005)	Loss 1.5223e-02 (4.7954e-03)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [72][ 60/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.004)	Loss 7.2162e-03 (5.1291e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [72][ 70/391]	Time  0.139 ( 0.141)	Data  0.001 ( 0.004)	Loss 5.1781e-03 (4.8965e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [72][ 80/391]	Time  0.139 ( 0.141)	Data  0.001 ( 0.003)	Loss 3.6519e-03 (4.6712e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [72][ 90/391]	Time  0.138 ( 0.141)	Data  0.001 ( 0.003)	Loss 1.6635e-03 (4.4844e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [72][100/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 3.1390e-03 (4.3567e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [72][110/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 4.3622e-03 (4.4058e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [72][120/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.8497e-03 (4.4568e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [72][130/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.002)	Loss 1.9107e-03 (4.4868e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [72][140/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.002)	Loss 7.2557e-03 (4.4967e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [72][150/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2222e-02 (4.3763e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [72][160/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2073e-02 (4.4228e-03)	Acc@1  99.22 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [72][170/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1235e-03 (4.4528e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [72][180/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.0499e-03 (4.4045e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [72][190/391]	Time  0.147 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5423e-03 (4.3864e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [72][200/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.0181e-03 (4.3910e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [72][210/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.7240e-03 (4.3065e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [72][220/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.1873e-03 (4.2803e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [72][230/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3375e-03 (4.2381e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [72][240/391]	Time  0.143 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.8147e-03 (4.2026e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [72][250/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.7492e-04 (4.1984e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [72][260/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6598e-03 (4.1361e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [72][270/391]	Time  0.149 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.9596e-03 (4.1063e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [72][280/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2717e-03 (4.1325e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [72][290/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5991e-03 (4.0807e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [72][300/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.6878e-03 (4.1007e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [72][310/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2297e-03 (4.0592e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [72][320/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0834e-03 (4.0280e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [72][330/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0866e-03 (4.1120e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [72][340/391]	Time  0.142 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0518e-03 (4.1050e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [72][350/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.9542e-03 (4.1177e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [72][360/391]	Time  0.143 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.9506e-03 (4.0912e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [72][370/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.9744e-03 (4.0694e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [72][380/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.6548e-04 (4.0316e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [72][390/391]	Time  0.110 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3337e-02 (4.0415e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
## e[72] optimizer.zero_grad (sum) time: 0.687605619430542
## e[72]       loss.backward (sum) time: 13.97452974319458
## e[72]      optimizer.step (sum) time: 14.642547130584717
## epoch[72] training(only) time: 54.243568897247314
# Switched to evaluate mode...
Test: [  0/100]	Time  0.194 ( 0.194)	Loss 3.4351e-01 (3.4351e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.060)	Loss 2.5848e-01 (2.7519e-01)	Acc@1  95.00 ( 94.18)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.049 ( 0.055)	Loss 4.2853e-01 (3.1361e-01)	Acc@1  91.00 ( 93.38)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.056 ( 0.053)	Loss 3.1586e-01 (3.1595e-01)	Acc@1  93.00 ( 93.26)	Acc@5  99.00 ( 99.71)
Test: [ 40/100]	Time  0.048 ( 0.051)	Loss 2.7287e-01 (3.2386e-01)	Acc@1  92.00 ( 93.07)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.049 ( 0.051)	Loss 1.9622e-01 (3.2430e-01)	Acc@1  96.00 ( 93.12)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 3.2488e-01 (3.1335e-01)	Acc@1  94.00 ( 93.21)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 5.6338e-01 (3.0648e-01)	Acc@1  92.00 ( 93.28)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.055 ( 0.049)	Loss 1.3930e-01 (3.0484e-01)	Acc@1  96.00 ( 93.22)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 2.9547e-01 (2.9735e-01)	Acc@1  93.00 ( 93.37)	Acc@5 100.00 ( 99.78)
 * Acc@1 93.370 Acc@5 99.790
### epoch[72] execution time: 59.251492738723755
EPOCH 73
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [73][  0/391]	Time  0.308 ( 0.308)	Data  0.157 ( 0.157)	Loss 4.4640e-03 (4.4640e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [73][ 10/391]	Time  0.136 ( 0.153)	Data  0.001 ( 0.015)	Loss 1.3913e-03 (5.3100e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [73][ 20/391]	Time  0.137 ( 0.145)	Data  0.001 ( 0.009)	Loss 1.3326e-03 (4.3590e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [73][ 30/391]	Time  0.135 ( 0.143)	Data  0.001 ( 0.006)	Loss 4.8896e-03 (4.0096e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [73][ 40/391]	Time  0.140 ( 0.142)	Data  0.001 ( 0.005)	Loss 1.8430e-03 (4.4591e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [73][ 50/391]	Time  0.136 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.7066e-03 (4.1880e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [73][ 60/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.004)	Loss 3.2866e-03 (3.9776e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [73][ 70/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 3.2980e-03 (4.3242e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [73][ 80/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 5.3437e-03 (4.2924e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [73][ 90/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.2629e-02 (4.4045e-03)	Acc@1  99.22 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [73][100/391]	Time  0.155 ( 0.139)	Data  0.001 ( 0.003)	Loss 9.5120e-04 (4.3545e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [73][110/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.9565e-03 (4.2235e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [73][120/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8026e-03 (4.3547e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [73][130/391]	Time  0.147 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7819e-03 (4.5487e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [73][140/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.2364e-03 (4.6057e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [73][150/391]	Time  0.145 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.0290e-03 (4.5347e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [73][160/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1898e-03 (4.4306e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [73][170/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.3134e-03 (4.4079e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [73][180/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1270e-03 (4.3570e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [73][190/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2401e-03 (4.2671e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [73][200/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2732e-03 (4.2276e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [73][210/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5337e-03 (4.2000e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [73][220/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8624e-03 (4.1824e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [73][230/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.2575e-04 (4.1590e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [73][240/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.7326e-03 (4.2901e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [73][250/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4228e-03 (4.2770e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [73][260/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.3649e-03 (4.3032e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [73][270/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.5539e-03 (4.2677e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [73][280/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0694e-03 (4.2392e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [73][290/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6369e-03 (4.2084e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [73][300/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.8513e-03 (4.2033e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [73][310/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2626e-02 (4.2709e-03)	Acc@1  99.22 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [73][320/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5889e-03 (4.2207e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [73][330/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0277e-03 (4.1944e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [73][340/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.8068e-03 (4.1711e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [73][350/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9659e-03 (4.1438e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [73][360/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.6678e-04 (4.1746e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [73][370/391]	Time  0.145 ( 0.138)	Data  0.001 ( 0.001)	Loss 6.8919e-04 (4.1283e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [73][380/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.001)	Loss 3.7503e-03 (4.0921e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [73][390/391]	Time  0.113 ( 0.138)	Data  0.001 ( 0.001)	Loss 1.8521e-02 (4.0919e-03)	Acc@1  98.75 ( 99.93)	Acc@5 100.00 (100.00)
## e[73] optimizer.zero_grad (sum) time: 0.6864538192749023
## e[73]       loss.backward (sum) time: 13.953150272369385
## e[73]      optimizer.step (sum) time: 14.59695291519165
## epoch[73] training(only) time: 54.096142292022705
# Switched to evaluate mode...
Test: [  0/100]	Time  0.194 ( 0.194)	Loss 3.3902e-01 (3.3902e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.062)	Loss 2.7063e-01 (2.7269e-01)	Acc@1  94.00 ( 93.91)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 4.6376e-01 (3.1164e-01)	Acc@1  91.00 ( 93.19)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.048 ( 0.053)	Loss 3.2242e-01 (3.1417e-01)	Acc@1  93.00 ( 93.32)	Acc@5  99.00 ( 99.71)
Test: [ 40/100]	Time  0.046 ( 0.052)	Loss 2.7713e-01 (3.2283e-01)	Acc@1  92.00 ( 93.07)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 2.0742e-01 (3.2378e-01)	Acc@1  96.00 ( 93.06)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.046 ( 0.051)	Loss 3.0518e-01 (3.1177e-01)	Acc@1  94.00 ( 93.21)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 5.8799e-01 (3.0541e-01)	Acc@1  91.00 ( 93.30)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.5550e-01 (3.0508e-01)	Acc@1  96.00 ( 93.25)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.046 ( 0.050)	Loss 2.9961e-01 (2.9805e-01)	Acc@1  93.00 ( 93.33)	Acc@5 100.00 ( 99.78)
 * Acc@1 93.350 Acc@5 99.790
### epoch[73] execution time: 59.11167335510254
EPOCH 74
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [74][  0/391]	Time  0.297 ( 0.297)	Data  0.148 ( 0.148)	Loss 3.7250e-03 (3.7250e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [74][ 10/391]	Time  0.134 ( 0.154)	Data  0.001 ( 0.014)	Loss 3.7736e-03 (5.0796e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [74][ 20/391]	Time  0.136 ( 0.147)	Data  0.001 ( 0.008)	Loss 2.7163e-03 (4.2520e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [74][ 30/391]	Time  0.138 ( 0.143)	Data  0.001 ( 0.006)	Loss 2.1920e-03 (3.4677e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [74][ 40/391]	Time  0.134 ( 0.142)	Data  0.001 ( 0.005)	Loss 1.3612e-02 (4.0324e-03)	Acc@1  99.22 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [74][ 50/391]	Time  0.137 ( 0.142)	Data  0.001 ( 0.004)	Loss 2.6826e-03 (3.8715e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [74][ 60/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.003)	Loss 3.5939e-02 (4.8941e-03)	Acc@1  98.44 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [74][ 70/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.5448e-03 (4.9191e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [74][ 80/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 5.2723e-03 (4.7944e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [74][ 90/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 5.4248e-03 (4.5997e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [74][100/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 3.4098e-03 (4.6644e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [74][110/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.002)	Loss 2.3112e-03 (4.5465e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [74][120/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4761e-03 (4.4340e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [74][130/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.0240e-03 (4.5839e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [74][140/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.1061e-03 (4.4754e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [74][150/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.5312e-03 (4.4525e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [74][160/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8375e-03 (4.3441e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [74][170/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.9675e-04 (4.3028e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [74][180/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.4548e-03 (4.1941e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [74][190/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.2586e-04 (4.1985e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [74][200/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3016e-03 (4.1560e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [74][210/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1109e-03 (4.1910e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [74][220/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.2366e-03 (4.2792e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [74][230/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.9020e-03 (4.1761e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [74][240/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.8723e-03 (4.1385e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [74][250/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0586e-02 (4.0987e-03)	Acc@1  99.22 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [74][260/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1110e-02 (4.1855e-03)	Acc@1  99.22 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [74][270/391]	Time  0.143 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2925e-03 (4.1289e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [74][280/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0326e-03 (4.1130e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [74][290/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.9985e-03 (4.1117e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [74][300/391]	Time  0.152 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4141e-03 (4.2440e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [74][310/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1853e-03 (4.2669e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [74][320/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5706e-03 (4.2124e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [74][330/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2310e-03 (4.1965e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [74][340/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.7170e-03 (4.2328e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [74][350/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9259e-03 (4.1814e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [74][360/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3642e-03 (4.2169e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [74][370/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4078e-03 (4.1988e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [74][380/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.4203e-03 (4.2435e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [74][390/391]	Time  0.112 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5706e-02 (4.2477e-03)	Acc@1  98.75 ( 99.92)	Acc@5 100.00 (100.00)
## e[74] optimizer.zero_grad (sum) time: 0.682917594909668
## e[74]       loss.backward (sum) time: 13.993628978729248
## e[74]      optimizer.step (sum) time: 14.597975730895996
## epoch[74] training(only) time: 54.15225839614868
# Switched to evaluate mode...
Test: [  0/100]	Time  0.192 ( 0.192)	Loss 3.4294e-01 (3.4294e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.062)	Loss 2.5660e-01 (2.7906e-01)	Acc@1  95.00 ( 94.00)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.047 ( 0.056)	Loss 4.5492e-01 (3.1522e-01)	Acc@1  91.00 ( 93.19)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 2.9578e-01 (3.1682e-01)	Acc@1  93.00 ( 93.10)	Acc@5  99.00 ( 99.71)
Test: [ 40/100]	Time  0.046 ( 0.052)	Loss 2.6333e-01 (3.2756e-01)	Acc@1  91.00 ( 92.85)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 2.1519e-01 (3.2729e-01)	Acc@1  96.00 ( 93.00)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 2.9863e-01 (3.1418e-01)	Acc@1  94.00 ( 93.11)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 5.5466e-01 (3.0722e-01)	Acc@1  91.00 ( 93.21)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.057 ( 0.050)	Loss 1.4848e-01 (3.0543e-01)	Acc@1  96.00 ( 93.22)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.046 ( 0.050)	Loss 2.7697e-01 (2.9774e-01)	Acc@1  93.00 ( 93.32)	Acc@5 100.00 ( 99.79)
 * Acc@1 93.340 Acc@5 99.810
### epoch[74] execution time: 59.201929807662964
EPOCH 75
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [75][  0/391]	Time  0.295 ( 0.295)	Data  0.151 ( 0.151)	Loss 2.0133e-02 (2.0133e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [75][ 10/391]	Time  0.136 ( 0.152)	Data  0.001 ( 0.015)	Loss 1.1881e-03 (5.6937e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [75][ 20/391]	Time  0.134 ( 0.146)	Data  0.001 ( 0.008)	Loss 1.2796e-02 (6.0312e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [75][ 30/391]	Time  0.134 ( 0.143)	Data  0.001 ( 0.006)	Loss 4.0831e-03 (5.1233e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [75][ 40/391]	Time  0.143 ( 0.142)	Data  0.001 ( 0.005)	Loss 1.3417e-03 (4.3926e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [75][ 50/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.004)	Loss 9.2790e-04 (4.4336e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [75][ 60/391]	Time  0.139 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.5077e-03 (6.3957e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [75][ 70/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.8139e-03 (6.0637e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [75][ 80/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.1484e-03 (5.8588e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [75][ 90/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 4.7946e-03 (5.5206e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [75][100/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.2496e-03 (5.3048e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [75][110/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 6.2327e-03 (5.1695e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [75][120/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0287e-03 (5.0161e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [75][130/391]	Time  0.146 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1718e-02 (4.9500e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [75][140/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.8255e-03 (4.8274e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [75][150/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.6330e-04 (4.8312e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [75][160/391]	Time  0.148 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.7375e-03 (4.7120e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [75][170/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2322e-03 (4.7109e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [75][180/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.3564e-03 (4.5972e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [75][190/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.2133e-03 (4.5463e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [75][200/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.4688e-03 (4.4657e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [75][210/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7545e-03 (4.4771e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [75][220/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5734e-03 (4.3928e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [75][230/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.6367e-03 (4.3526e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [75][240/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.5927e-03 (4.2944e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [75][250/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.9126e-03 (4.3740e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [75][260/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.5346e-03 (4.3142e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [75][270/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.9511e-03 (4.3625e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [75][280/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.0671e-03 (4.4077e-03)	Acc@1  99.22 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [75][290/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3356e-03 (4.3686e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [75][300/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.5795e-04 (4.3330e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [75][310/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.5428e-02 (4.3780e-03)	Acc@1  99.22 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [75][320/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.1673e-03 (4.4286e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [75][330/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7752e-02 (4.4045e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [75][340/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.6557e-03 (4.3329e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [75][350/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.6843e-03 (4.3016e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [75][360/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.7205e-03 (4.2536e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [75][370/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.5043e-03 (4.3078e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [75][380/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.7936e-03 (4.3934e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [75][390/391]	Time  0.108 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.8009e-03 (4.4309e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
## e[75] optimizer.zero_grad (sum) time: 0.684567928314209
## e[75]       loss.backward (sum) time: 13.921021461486816
## e[75]      optimizer.step (sum) time: 14.627474069595337
## epoch[75] training(only) time: 54.09413290023804
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 3.3402e-01 (3.3402e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.062)	Loss 2.6877e-01 (2.7976e-01)	Acc@1  95.00 ( 93.82)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.053 ( 0.056)	Loss 4.5276e-01 (3.1328e-01)	Acc@1  91.00 ( 93.33)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 2.9229e-01 (3.1596e-01)	Acc@1  93.00 ( 93.29)	Acc@5  99.00 ( 99.71)
Test: [ 40/100]	Time  0.047 ( 0.052)	Loss 2.5897e-01 (3.2273e-01)	Acc@1  92.00 ( 93.07)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.051 ( 0.051)	Loss 1.9597e-01 (3.2437e-01)	Acc@1  97.00 ( 93.10)	Acc@5 100.00 ( 99.69)
Test: [ 60/100]	Time  0.046 ( 0.051)	Loss 2.9811e-01 (3.1203e-01)	Acc@1  94.00 ( 93.23)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 5.9196e-01 (3.0538e-01)	Acc@1  91.00 ( 93.32)	Acc@5 100.00 ( 99.72)
Test: [ 80/100]	Time  0.047 ( 0.050)	Loss 1.4247e-01 (3.0523e-01)	Acc@1  96.00 ( 93.23)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.048 ( 0.050)	Loss 2.9145e-01 (2.9808e-01)	Acc@1  93.00 ( 93.33)	Acc@5 100.00 ( 99.77)
 * Acc@1 93.390 Acc@5 99.780
### epoch[75] execution time: 59.154465198516846
EPOCH 76
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [76][  0/391]	Time  0.302 ( 0.302)	Data  0.159 ( 0.159)	Loss 1.5826e-03 (1.5826e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [76][ 10/391]	Time  0.133 ( 0.153)	Data  0.001 ( 0.015)	Loss 2.3062e-03 (3.0729e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [76][ 20/391]	Time  0.142 ( 0.147)	Data  0.001 ( 0.009)	Loss 3.5489e-03 (2.9957e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [76][ 30/391]	Time  0.140 ( 0.144)	Data  0.001 ( 0.006)	Loss 7.5898e-03 (3.2557e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [76][ 40/391]	Time  0.135 ( 0.142)	Data  0.001 ( 0.005)	Loss 7.2946e-04 (3.1177e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [76][ 50/391]	Time  0.140 ( 0.142)	Data  0.001 ( 0.004)	Loss 1.1663e-03 (3.0128e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [76][ 60/391]	Time  0.135 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.6969e-03 (3.2482e-03)	Acc@1 100.00 ( 99.99)	Acc@5 100.00 (100.00)
Epoch: [76][ 70/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 6.3746e-04 (3.1655e-03)	Acc@1 100.00 ( 99.98)	Acc@5 100.00 (100.00)
Epoch: [76][ 80/391]	Time  0.142 ( 0.140)	Data  0.001 ( 0.003)	Loss 4.8625e-03 (3.1119e-03)	Acc@1 100.00 ( 99.98)	Acc@5 100.00 (100.00)
Epoch: [76][ 90/391]	Time  0.141 ( 0.140)	Data  0.001 ( 0.003)	Loss 3.0886e-03 (3.0483e-03)	Acc@1 100.00 ( 99.98)	Acc@5 100.00 (100.00)
Epoch: [76][100/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.3267e-03 (2.9874e-03)	Acc@1 100.00 ( 99.98)	Acc@5 100.00 (100.00)
Epoch: [76][110/391]	Time  0.140 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.4759e-02 (3.2684e-03)	Acc@1  99.22 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [76][120/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.3297e-03 (3.2458e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [76][130/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7753e-03 (3.3503e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [76][140/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.7202e-03 (3.4603e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [76][150/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.7395e-03 (3.4397e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [76][160/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.2836e-03 (3.3871e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [76][170/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.5750e-03 (3.4856e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [76][180/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.5954e-03 (3.4724e-03)	Acc@1  99.22 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [76][190/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1247e-03 (3.4740e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [76][200/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.9659e-03 (3.4323e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [76][210/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6577e-03 (3.3873e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [76][220/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.8719e-03 (3.3820e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [76][230/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.9347e-03 (3.4022e-03)	Acc@1  99.22 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [76][240/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6394e-03 (3.4253e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [76][250/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6897e-03 (3.4500e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [76][260/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.4933e-04 (3.4649e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [76][270/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0073e-03 (3.4944e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [76][280/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.3368e-03 (3.4848e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [76][290/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.5169e-03 (3.5680e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [76][300/391]	Time  0.143 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2950e-03 (3.6194e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [76][310/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0733e-03 (3.6530e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [76][320/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.3304e-04 (3.6431e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [76][330/391]	Time  0.147 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8305e-03 (3.6363e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [76][340/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.6557e-03 (3.6511e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [76][350/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.1162e-03 (3.6373e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [76][360/391]	Time  0.146 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4817e-03 (3.6300e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [76][370/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.6958e-04 (3.5874e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [76][380/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.6946e-03 (3.5590e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [76][390/391]	Time  0.110 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.5344e-03 (3.5941e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
## e[76] optimizer.zero_grad (sum) time: 0.6837825775146484
## e[76]       loss.backward (sum) time: 14.042386054992676
## e[76]      optimizer.step (sum) time: 14.62554383277893
## epoch[76] training(only) time: 54.1884503364563
# Switched to evaluate mode...
Test: [  0/100]	Time  0.191 ( 0.191)	Loss 3.5523e-01 (3.5523e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 2.7023e-01 (2.7659e-01)	Acc@1  94.00 ( 93.64)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.049 ( 0.055)	Loss 4.4952e-01 (3.1441e-01)	Acc@1  91.00 ( 93.29)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 3.1227e-01 (3.1813e-01)	Acc@1  93.00 ( 93.32)	Acc@5  99.00 ( 99.74)
Test: [ 40/100]	Time  0.046 ( 0.052)	Loss 2.5133e-01 (3.2370e-01)	Acc@1  93.00 ( 93.15)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.9231e-01 (3.2395e-01)	Acc@1  96.00 ( 93.08)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 3.1808e-01 (3.1286e-01)	Acc@1  94.00 ( 93.20)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 5.6151e-01 (3.0640e-01)	Acc@1  91.00 ( 93.30)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.6683e-01 (3.0585e-01)	Acc@1  96.00 ( 93.25)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 3.0999e-01 (2.9860e-01)	Acc@1  93.00 ( 93.36)	Acc@5 100.00 ( 99.79)
 * Acc@1 93.400 Acc@5 99.810
### epoch[76] execution time: 59.19569945335388
EPOCH 77
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [77][  0/391]	Time  0.304 ( 0.304)	Data  0.162 ( 0.162)	Loss 2.5567e-03 (2.5567e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [77][ 10/391]	Time  0.137 ( 0.153)	Data  0.001 ( 0.016)	Loss 2.3734e-03 (2.7171e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [77][ 20/391]	Time  0.138 ( 0.146)	Data  0.001 ( 0.009)	Loss 1.0399e-03 (3.0859e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [77][ 30/391]	Time  0.139 ( 0.144)	Data  0.001 ( 0.006)	Loss 2.3199e-03 (3.8027e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [77][ 40/391]	Time  0.137 ( 0.142)	Data  0.001 ( 0.005)	Loss 1.2270e-02 (3.9177e-03)	Acc@1  99.22 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [77][ 50/391]	Time  0.140 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.4452e-03 (4.0408e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [77][ 60/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.9363e-03 (4.4159e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [77][ 70/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.0680e-03 (4.4157e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [77][ 80/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 7.4873e-03 (4.4983e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [77][ 90/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.2758e-03 (4.3067e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [77][100/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.003)	Loss 4.3170e-03 (4.3007e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [77][110/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.003)	Loss 9.4659e-04 (4.4551e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [77][120/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 9.2229e-04 (4.2582e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [77][130/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1793e-03 (4.4328e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [77][140/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.4546e-03 (4.5106e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [77][150/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0071e-02 (4.5485e-03)	Acc@1  98.44 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [77][160/391]	Time  0.148 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6218e-03 (4.5159e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [77][170/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.9767e-03 (4.4380e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [77][180/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0035e-02 (4.4869e-03)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [77][190/391]	Time  0.145 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.0457e-04 (4.3838e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [77][200/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.7588e-03 (4.3478e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [77][210/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.6852e-03 (4.4612e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [77][220/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.0636e-03 (4.4584e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [77][230/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4787e-02 (4.4009e-03)	Acc@1  99.22 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [77][240/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.1383e-03 (4.4420e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [77][250/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.3604e-03 (4.4234e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [77][260/391]	Time  0.144 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.6451e-03 (4.5410e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [77][270/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4774e-03 (4.4403e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [77][280/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8421e-03 (4.3938e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [77][290/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6417e-02 (4.3899e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [77][300/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.5371e-04 (4.4401e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [77][310/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.2029e-03 (4.4158e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [77][320/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7087e-03 (4.3810e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [77][330/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.4088e-03 (4.3634e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [77][340/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.5245e-04 (4.2915e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [77][350/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5773e-02 (4.2848e-03)	Acc@1  99.22 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [77][360/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.0631e-03 (4.3011e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [77][370/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2492e-03 (4.2860e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [77][380/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9104e-03 (4.3112e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [77][390/391]	Time  0.115 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.4625e-03 (4.3101e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
## e[77] optimizer.zero_grad (sum) time: 0.6833059787750244
## e[77]       loss.backward (sum) time: 13.974835395812988
## e[77]      optimizer.step (sum) time: 14.614629030227661
## epoch[77] training(only) time: 54.12213706970215
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 3.4217e-01 (3.4217e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 2.7295e-01 (2.7802e-01)	Acc@1  94.00 ( 93.55)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.047 ( 0.054)	Loss 4.7927e-01 (3.1609e-01)	Acc@1  91.00 ( 93.10)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.048 ( 0.053)	Loss 2.9011e-01 (3.1608e-01)	Acc@1  93.00 ( 93.13)	Acc@5  99.00 ( 99.74)
Test: [ 40/100]	Time  0.055 ( 0.052)	Loss 2.4804e-01 (3.2387e-01)	Acc@1  92.00 ( 92.93)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 2.0692e-01 (3.2555e-01)	Acc@1  96.00 ( 92.92)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 2.9407e-01 (3.1249e-01)	Acc@1  94.00 ( 93.05)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 5.9537e-01 (3.0559e-01)	Acc@1  91.00 ( 93.18)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.4772e-01 (3.0613e-01)	Acc@1  96.00 ( 93.11)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.058 ( 0.049)	Loss 2.9683e-01 (2.9931e-01)	Acc@1  93.00 ( 93.22)	Acc@5 100.00 ( 99.78)
 * Acc@1 93.270 Acc@5 99.790
### epoch[77] execution time: 59.138213872909546
EPOCH 78
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [78][  0/391]	Time  0.289 ( 0.289)	Data  0.147 ( 0.147)	Loss 2.1317e-03 (2.1317e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [78][ 10/391]	Time  0.137 ( 0.149)	Data  0.001 ( 0.014)	Loss 8.9540e-04 (2.2612e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [78][ 20/391]	Time  0.151 ( 0.144)	Data  0.001 ( 0.008)	Loss 4.2534e-03 (2.6197e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [78][ 30/391]	Time  0.136 ( 0.142)	Data  0.001 ( 0.006)	Loss 1.9670e-03 (2.9623e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [78][ 40/391]	Time  0.139 ( 0.141)	Data  0.001 ( 0.005)	Loss 7.7042e-03 (3.6346e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [78][ 50/391]	Time  0.145 ( 0.140)	Data  0.001 ( 0.004)	Loss 2.5811e-03 (3.8424e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [78][ 60/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.004)	Loss 2.1582e-03 (3.7694e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [78][ 70/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.3585e-03 (3.5282e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [78][ 80/391]	Time  0.141 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.9042e-03 (3.5665e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [78][ 90/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.5864e-03 (3.5236e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [78][100/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.003)	Loss 6.7021e-03 (3.6455e-03)	Acc@1  99.22 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [78][110/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.2580e-04 (3.9281e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [78][120/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.9243e-03 (3.8694e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [78][130/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0994e-03 (3.8120e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [78][140/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0188e-02 (3.9316e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [78][150/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.1888e-03 (3.8458e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [78][160/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.2225e-03 (3.8327e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [78][170/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0555e-03 (3.7467e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [78][180/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.0570e-03 (3.7280e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [78][190/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1600e-03 (3.7364e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [78][200/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.3780e-03 (3.6670e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [78][210/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.7039e-03 (3.6488e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [78][220/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8726e-02 (3.7046e-03)	Acc@1  99.22 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [78][230/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.7959e-03 (3.7649e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [78][240/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2178e-03 (3.7281e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [78][250/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.2912e-03 (3.6811e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [78][260/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.3673e-03 (3.7474e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [78][270/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0432e-03 (3.7406e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [78][280/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.5766e-04 (3.7467e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [78][290/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2762e-03 (3.7422e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [78][300/391]	Time  0.144 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.0925e-03 (3.7414e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [78][310/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.3812e-03 (3.7235e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [78][320/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.0318e-03 (3.8175e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [78][330/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2547e-03 (3.7725e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [78][340/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7358e-03 (3.8258e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [78][350/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.6674e-03 (3.8219e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [78][360/391]	Time  0.147 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.5726e-03 (3.8013e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [78][370/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0925e-02 (3.8682e-03)	Acc@1  99.22 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [78][380/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2327e-03 (3.8503e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [78][390/391]	Time  0.115 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0857e-02 (3.8794e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
## e[78] optimizer.zero_grad (sum) time: 0.6838672161102295
## e[78]       loss.backward (sum) time: 13.990598678588867
## e[78]      optimizer.step (sum) time: 14.566457748413086
## epoch[78] training(only) time: 54.10967779159546
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 3.4045e-01 (3.4045e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 2.6875e-01 (2.7181e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.048 ( 0.055)	Loss 4.5667e-01 (3.1168e-01)	Acc@1  91.00 ( 93.33)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.047 ( 0.053)	Loss 3.1124e-01 (3.1448e-01)	Acc@1  93.00 ( 93.29)	Acc@5  99.00 ( 99.71)
Test: [ 40/100]	Time  0.046 ( 0.052)	Loss 2.7487e-01 (3.2225e-01)	Acc@1  92.00 ( 93.05)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.9644e-01 (3.2311e-01)	Acc@1  96.00 ( 93.06)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.047 ( 0.051)	Loss 3.2864e-01 (3.1156e-01)	Acc@1  94.00 ( 93.16)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 5.7140e-01 (3.0490e-01)	Acc@1  91.00 ( 93.25)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.3960e-01 (3.0400e-01)	Acc@1  96.00 ( 93.23)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.049 ( 0.049)	Loss 2.8879e-01 (2.9714e-01)	Acc@1  93.00 ( 93.32)	Acc@5 100.00 ( 99.78)
 * Acc@1 93.340 Acc@5 99.790
### epoch[78] execution time: 59.13994574546814
EPOCH 79
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [79][  0/391]	Time  0.300 ( 0.300)	Data  0.156 ( 0.156)	Loss 4.6222e-03 (4.6222e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [79][ 10/391]	Time  0.140 ( 0.152)	Data  0.001 ( 0.015)	Loss 1.1191e-03 (4.7397e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [79][ 20/391]	Time  0.134 ( 0.145)	Data  0.001 ( 0.009)	Loss 4.4485e-03 (4.1683e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [79][ 30/391]	Time  0.135 ( 0.143)	Data  0.001 ( 0.006)	Loss 1.6013e-03 (3.8701e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [79][ 40/391]	Time  0.137 ( 0.142)	Data  0.001 ( 0.005)	Loss 2.6373e-03 (3.7273e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [79][ 50/391]	Time  0.139 ( 0.141)	Data  0.001 ( 0.004)	Loss 2.5340e-03 (3.4485e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [79][ 60/391]	Time  0.136 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.6056e-03 (3.6552e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [79][ 70/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.7900e-03 (3.4345e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [79][ 80/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 9.7367e-04 (3.5641e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [79][ 90/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.8249e-03 (3.5017e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [79][100/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.0818e-02 (3.5584e-03)	Acc@1  99.22 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [79][110/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.1361e-03 (3.4140e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [79][120/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6582e-03 (3.5659e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [79][130/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3511e-03 (3.6304e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [79][140/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2240e-03 (3.5877e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [79][150/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1823e-02 (3.6289e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [79][160/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.5525e-03 (3.7396e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [79][170/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0209e-02 (3.8188e-03)	Acc@1  99.22 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [79][180/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0357e-03 (3.7680e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [79][190/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.6243e-04 (3.7108e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [79][200/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.4917e-03 (3.7697e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [79][210/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.5777e-03 (3.7198e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [79][220/391]	Time  0.148 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3458e-03 (3.7747e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [79][230/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.1800e-03 (3.7504e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [79][240/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.7431e-03 (3.7619e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [79][250/391]	Time  0.145 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.2218e-04 (3.8341e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [79][260/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.3755e-03 (3.8305e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [79][270/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.9743e-03 (3.8830e-03)	Acc@1  99.22 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [79][280/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0553e-03 (3.8744e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [79][290/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0479e-03 (3.8301e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [79][300/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1178e-03 (3.8153e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [79][310/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6642e-03 (3.8952e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [79][320/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.2095e-03 (3.8787e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [79][330/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2552e-03 (3.8801e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [79][340/391]	Time  0.143 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3251e-02 (3.8967e-03)	Acc@1  99.22 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [79][350/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.9448e-03 (3.9504e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [79][360/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.6227e-03 (3.9781e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [79][370/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.0131e-03 (3.9442e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [79][380/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.0701e-03 (3.9261e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [79][390/391]	Time  0.109 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.9829e-04 (3.9504e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
## e[79] optimizer.zero_grad (sum) time: 0.6829395294189453
## e[79]       loss.backward (sum) time: 13.97133207321167
## e[79]      optimizer.step (sum) time: 14.6227445602417
## epoch[79] training(only) time: 54.17882466316223
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 3.5746e-01 (3.5746e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.049 ( 0.062)	Loss 2.5401e-01 (2.7610e-01)	Acc@1  95.00 ( 94.18)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 4.5515e-01 (3.1396e-01)	Acc@1  91.00 ( 93.38)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.047 ( 0.053)	Loss 3.1203e-01 (3.1569e-01)	Acc@1  93.00 ( 93.35)	Acc@5  99.00 ( 99.71)
Test: [ 40/100]	Time  0.055 ( 0.052)	Loss 2.7691e-01 (3.2360e-01)	Acc@1  91.00 ( 93.12)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.8581e-01 (3.2310e-01)	Acc@1  97.00 ( 93.14)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 3.2666e-01 (3.1146e-01)	Acc@1  94.00 ( 93.25)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 5.5610e-01 (3.0475e-01)	Acc@1  91.00 ( 93.27)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.047 ( 0.050)	Loss 1.4602e-01 (3.0357e-01)	Acc@1  96.00 ( 93.26)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.061 ( 0.049)	Loss 2.9908e-01 (2.9709e-01)	Acc@1  93.00 ( 93.33)	Acc@5 100.00 ( 99.78)
 * Acc@1 93.350 Acc@5 99.800
### epoch[79] execution time: 59.19401526451111
EPOCH 80
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [80][  0/391]	Time  0.297 ( 0.297)	Data  0.155 ( 0.155)	Loss 8.2775e-03 (8.2775e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [80][ 10/391]	Time  0.136 ( 0.153)	Data  0.001 ( 0.015)	Loss 1.9556e-03 (4.3398e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [80][ 20/391]	Time  0.137 ( 0.145)	Data  0.001 ( 0.008)	Loss 5.2326e-03 (4.2503e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [80][ 30/391]	Time  0.137 ( 0.143)	Data  0.001 ( 0.006)	Loss 2.6474e-03 (3.3965e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [80][ 40/391]	Time  0.135 ( 0.142)	Data  0.001 ( 0.005)	Loss 5.9195e-04 (3.6405e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [80][ 50/391]	Time  0.150 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.0390e-02 (3.6783e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [80][ 60/391]	Time  0.139 ( 0.141)	Data  0.001 ( 0.004)	Loss 8.1992e-03 (4.1719e-03)	Acc@1  99.22 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [80][ 70/391]	Time  0.139 ( 0.141)	Data  0.001 ( 0.003)	Loss 1.3572e-03 (4.0246e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [80][ 80/391]	Time  0.142 ( 0.140)	Data  0.001 ( 0.003)	Loss 4.4834e-03 (3.8222e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [80][ 90/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.5611e-03 (4.0559e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [80][100/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 4.1081e-03 (4.0872e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [80][110/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.002)	Loss 1.0190e-03 (4.1975e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [80][120/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6337e-03 (4.1788e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [80][130/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.9663e-02 (4.3548e-03)	Acc@1  99.22 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [80][140/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.5086e-03 (4.5974e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [80][150/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.7145e-03 (4.5029e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [80][160/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.8629e-04 (4.4245e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [80][170/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.8688e-03 (4.3474e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [80][180/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5466e-03 (4.3338e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [80][190/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.0475e-03 (4.2517e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [80][200/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.6015e-03 (4.1551e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [80][210/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.5886e-03 (4.1356e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [80][220/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5948e-03 (4.0814e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [80][230/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1090e-03 (4.0325e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [80][240/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.4311e-03 (4.0131e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [80][250/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.5026e-03 (3.9728e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [80][260/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0189e-02 (4.0119e-03)	Acc@1  99.22 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [80][270/391]	Time  0.145 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.1074e-03 (3.9431e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [80][280/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0905e-03 (3.8731e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [80][290/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.2179e-04 (3.8634e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [80][300/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.7963e-03 (3.8299e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [80][310/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.3999e-03 (3.8199e-03)	Acc@1  99.22 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [80][320/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.4514e-03 (3.9731e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [80][330/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.7618e-03 (3.9214e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [80][340/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.6772e-03 (3.9534e-03)	Acc@1  99.22 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [80][350/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1844e-03 (3.9133e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [80][360/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.9708e-03 (3.8766e-03)	Acc@1  99.22 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [80][370/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7861e-03 (3.8354e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [80][380/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.3298e-03 (3.8256e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [80][390/391]	Time  0.125 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3297e-02 (3.8068e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
## e[80] optimizer.zero_grad (sum) time: 0.6884973049163818
## e[80]       loss.backward (sum) time: 14.000210523605347
## e[80]      optimizer.step (sum) time: 14.604361534118652
## epoch[80] training(only) time: 54.21443033218384
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 3.3375e-01 (3.3375e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 2.6097e-01 (2.7892e-01)	Acc@1  94.00 ( 93.64)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.049 ( 0.055)	Loss 4.6965e-01 (3.1465e-01)	Acc@1  91.00 ( 93.24)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.047 ( 0.053)	Loss 2.9071e-01 (3.1671e-01)	Acc@1  93.00 ( 93.29)	Acc@5  99.00 ( 99.71)
Test: [ 40/100]	Time  0.046 ( 0.052)	Loss 2.6875e-01 (3.2392e-01)	Acc@1  92.00 ( 93.05)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.9527e-01 (3.2390e-01)	Acc@1  97.00 ( 93.10)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.048 ( 0.050)	Loss 2.9966e-01 (3.1229e-01)	Acc@1  94.00 ( 93.15)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 5.8834e-01 (3.0605e-01)	Acc@1  91.00 ( 93.24)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.049 ( 0.049)	Loss 1.5832e-01 (3.0523e-01)	Acc@1  96.00 ( 93.22)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.050 ( 0.049)	Loss 2.8654e-01 (2.9786e-01)	Acc@1  93.00 ( 93.32)	Acc@5 100.00 ( 99.78)
 * Acc@1 93.350 Acc@5 99.800
### epoch[80] execution time: 59.236172676086426
EPOCH 81
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [81][  0/391]	Time  0.301 ( 0.301)	Data  0.157 ( 0.157)	Loss 2.2606e-03 (2.2606e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [81][ 10/391]	Time  0.138 ( 0.152)	Data  0.001 ( 0.015)	Loss 1.2480e-03 (5.4431e-03)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [81][ 20/391]	Time  0.135 ( 0.146)	Data  0.001 ( 0.009)	Loss 1.1819e-03 (5.0045e-03)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [81][ 30/391]	Time  0.139 ( 0.143)	Data  0.001 ( 0.006)	Loss 4.7932e-03 (4.0561e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [81][ 40/391]	Time  0.138 ( 0.141)	Data  0.001 ( 0.005)	Loss 9.3413e-04 (3.5358e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [81][ 50/391]	Time  0.140 ( 0.141)	Data  0.001 ( 0.004)	Loss 3.7289e-03 (3.4292e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [81][ 60/391]	Time  0.138 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.0165e-02 (3.2781e-03)	Acc@1  99.22 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [81][ 70/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.6972e-03 (3.4156e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [81][ 80/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.6780e-03 (3.3798e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [81][ 90/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 4.7346e-03 (3.4096e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [81][100/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 4.2745e-03 (3.5167e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [81][110/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.0273e-03 (3.7757e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [81][120/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6458e-03 (3.7696e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [81][130/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2295e-03 (3.7080e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [81][140/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2060e-03 (3.7393e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [81][150/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.6654e-04 (3.6562e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [81][160/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.9266e-03 (3.6257e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [81][170/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3998e-03 (3.6800e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [81][180/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.6132e-03 (3.6537e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [81][190/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3338e-02 (3.6276e-03)	Acc@1  99.22 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [81][200/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.9897e-03 (3.5836e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [81][210/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4008e-03 (3.6137e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [81][220/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.7753e-03 (3.5558e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [81][230/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.5602e-03 (3.5859e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [81][240/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.3048e-03 (3.6140e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [81][250/391]	Time  0.148 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.7961e-03 (3.6445e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [81][260/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.2755e-03 (3.6109e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [81][270/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3750e-03 (3.5509e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [81][280/391]	Time  0.150 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1319e-03 (3.5687e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [81][290/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.3662e-03 (3.5558e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [81][300/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.0335e-03 (3.5196e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [81][310/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2567e-03 (3.5244e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [81][320/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0780e-02 (3.5790e-03)	Acc@1  99.22 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [81][330/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7271e-03 (3.5887e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [81][340/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.9695e-03 (3.6841e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [81][350/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0610e-03 (3.7243e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [81][360/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.7858e-04 (3.7558e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [81][370/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.0192e-04 (3.7049e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [81][380/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1859e-03 (3.6847e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [81][390/391]	Time  0.112 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.2465e-03 (3.6777e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
## e[81] optimizer.zero_grad (sum) time: 0.6812047958374023
## e[81]       loss.backward (sum) time: 14.008522272109985
## e[81]      optimizer.step (sum) time: 14.598456859588623
## epoch[81] training(only) time: 54.15534424781799
# Switched to evaluate mode...
Test: [  0/100]	Time  0.193 ( 0.193)	Loss 3.5707e-01 (3.5707e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.064)	Loss 2.6548e-01 (2.8266e-01)	Acc@1  95.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.047 ( 0.057)	Loss 4.6885e-01 (3.2174e-01)	Acc@1  91.00 ( 93.24)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.048 ( 0.054)	Loss 3.1631e-01 (3.2518e-01)	Acc@1  93.00 ( 93.16)	Acc@5  99.00 ( 99.74)
Test: [ 40/100]	Time  0.046 ( 0.053)	Loss 2.5186e-01 (3.2864e-01)	Acc@1  93.00 ( 93.07)	Acc@5 100.00 ( 99.76)
Test: [ 50/100]	Time  0.046 ( 0.052)	Loss 1.7854e-01 (3.2828e-01)	Acc@1  96.00 ( 93.10)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.046 ( 0.051)	Loss 3.1065e-01 (3.1635e-01)	Acc@1  93.00 ( 93.23)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.050 ( 0.050)	Loss 5.7818e-01 (3.0895e-01)	Acc@1  91.00 ( 93.31)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.6830e-01 (3.0840e-01)	Acc@1  96.00 ( 93.25)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.046 ( 0.050)	Loss 3.1553e-01 (3.0104e-01)	Acc@1  93.00 ( 93.32)	Acc@5 100.00 ( 99.78)
 * Acc@1 93.360 Acc@5 99.800
### epoch[81] execution time: 59.20073199272156
EPOCH 82
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [82][  0/391]	Time  0.304 ( 0.304)	Data  0.155 ( 0.155)	Loss 7.5619e-04 (7.5619e-04)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [82][ 10/391]	Time  0.136 ( 0.153)	Data  0.001 ( 0.015)	Loss 2.0401e-03 (3.2956e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [82][ 20/391]	Time  0.140 ( 0.146)	Data  0.001 ( 0.008)	Loss 3.4998e-03 (4.5921e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [82][ 30/391]	Time  0.136 ( 0.144)	Data  0.001 ( 0.006)	Loss 2.6377e-03 (4.1006e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [82][ 40/391]	Time  0.138 ( 0.143)	Data  0.001 ( 0.005)	Loss 1.8624e-02 (4.3283e-03)	Acc@1  99.22 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [82][ 50/391]	Time  0.140 ( 0.142)	Data  0.001 ( 0.004)	Loss 2.9401e-03 (4.1100e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [82][ 60/391]	Time  0.138 ( 0.141)	Data  0.001 ( 0.004)	Loss 2.2939e-03 (4.2013e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [82][ 70/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.003)	Loss 7.7266e-03 (4.1659e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [82][ 80/391]	Time  0.145 ( 0.140)	Data  0.001 ( 0.003)	Loss 5.5900e-03 (4.1036e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [82][ 90/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 3.0093e-03 (3.9323e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [82][100/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.0636e-03 (3.8519e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [82][110/391]	Time  0.147 ( 0.140)	Data  0.001 ( 0.002)	Loss 1.7240e-03 (3.8990e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [82][120/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.2056e-03 (4.0032e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [82][130/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.002)	Loss 1.1391e-03 (4.1760e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [82][140/391]	Time  0.147 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.3193e-04 (4.2310e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [82][150/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.2180e-04 (4.1599e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [82][160/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1243e-03 (4.1280e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [82][170/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6124e-03 (4.0582e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [82][180/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.2937e-03 (3.9760e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [82][190/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4091e-03 (3.9289e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [82][200/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.1854e-04 (3.8765e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [82][210/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.4843e-03 (3.8517e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [82][220/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0757e-03 (3.8183e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [82][230/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2084e-03 (3.7813e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [82][240/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.3207e-03 (3.7671e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [82][250/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4520e-02 (3.9077e-03)	Acc@1  99.22 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [82][260/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.6385e-03 (3.8972e-03)	Acc@1  99.22 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [82][270/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0488e-02 (3.9083e-03)	Acc@1  99.22 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [82][280/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.8170e-03 (3.9141e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [82][290/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.8866e-03 (3.9588e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [82][300/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4562e-03 (3.9714e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [82][310/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.3633e-03 (3.9962e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [82][320/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.8960e-03 (4.0165e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [82][330/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.5291e-03 (4.0288e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [82][340/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.6284e-03 (4.0579e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [82][350/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.8147e-03 (4.0191e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [82][360/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.9257e-04 (3.9713e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [82][370/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.4609e-03 (3.9605e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [82][380/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1423e-03 (3.9426e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [82][390/391]	Time  0.106 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5407e-03 (3.9081e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
## e[82] optimizer.zero_grad (sum) time: 0.6908071041107178
## e[82]       loss.backward (sum) time: 14.018162488937378
## e[82]      optimizer.step (sum) time: 14.604261636734009
## epoch[82] training(only) time: 54.26754093170166
# Switched to evaluate mode...
Test: [  0/100]	Time  0.201 ( 0.201)	Loss 3.5208e-01 (3.5208e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 2.8128e-01 (2.7936e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.052 ( 0.055)	Loss 4.7526e-01 (3.1676e-01)	Acc@1  92.00 ( 93.29)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 3.1255e-01 (3.1918e-01)	Acc@1  93.00 ( 93.29)	Acc@5  99.00 ( 99.74)
Test: [ 40/100]	Time  0.046 ( 0.052)	Loss 2.9044e-01 (3.2679e-01)	Acc@1  92.00 ( 93.07)	Acc@5 100.00 ( 99.76)
Test: [ 50/100]	Time  0.055 ( 0.051)	Loss 1.9013e-01 (3.2644e-01)	Acc@1  96.00 ( 93.10)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 3.3234e-01 (3.1528e-01)	Acc@1  93.00 ( 93.20)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 5.5830e-01 (3.0918e-01)	Acc@1  91.00 ( 93.28)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.052 ( 0.050)	Loss 1.5653e-01 (3.0789e-01)	Acc@1  96.00 ( 93.23)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.048 ( 0.050)	Loss 2.8084e-01 (3.0045e-01)	Acc@1  93.00 ( 93.30)	Acc@5 100.00 ( 99.79)
 * Acc@1 93.350 Acc@5 99.810
### epoch[82] execution time: 59.332762002944946
EPOCH 83
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [83][  0/391]	Time  0.298 ( 0.298)	Data  0.151 ( 0.151)	Loss 1.2037e-03 (1.2037e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [83][ 10/391]	Time  0.134 ( 0.151)	Data  0.001 ( 0.015)	Loss 6.0938e-03 (3.2431e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [83][ 20/391]	Time  0.134 ( 0.146)	Data  0.001 ( 0.008)	Loss 1.2894e-03 (3.3939e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [83][ 30/391]	Time  0.138 ( 0.143)	Data  0.001 ( 0.006)	Loss 1.6115e-03 (3.3903e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [83][ 40/391]	Time  0.135 ( 0.141)	Data  0.001 ( 0.005)	Loss 1.3578e-03 (3.1480e-03)	Acc@1 100.00 ( 99.98)	Acc@5 100.00 (100.00)
Epoch: [83][ 50/391]	Time  0.143 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.9239e-03 (3.6832e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [83][ 60/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.004)	Loss 8.6104e-04 (3.7158e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [83][ 70/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.8822e-03 (3.7659e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [83][ 80/391]	Time  0.150 ( 0.140)	Data  0.001 ( 0.003)	Loss 3.6867e-03 (3.8269e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [83][ 90/391]	Time  0.140 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.7317e-03 (3.7592e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [83][100/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 9.0039e-04 (3.6991e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [83][110/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.1303e-03 (3.6492e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [83][120/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.1367e-03 (3.7871e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [83][130/391]	Time  0.143 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.9377e-03 (3.8766e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [83][140/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.3856e-03 (3.9256e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [83][150/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6935e-03 (3.7909e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [83][160/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.0034e-03 (3.8616e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [83][170/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.0277e-03 (3.7813e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [83][180/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1140e-03 (3.9127e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [83][190/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.0739e-03 (3.9685e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [83][200/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.0785e-04 (3.9157e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [83][210/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.8173e-02 (3.9761e-03)	Acc@1  98.44 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [83][220/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3711e-03 (4.0342e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [83][230/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.6025e-04 (4.0194e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [83][240/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.0416e-03 (4.0343e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [83][250/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.3725e-03 (4.0614e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [83][260/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.9186e-03 (4.0023e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [83][270/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6693e-03 (3.9908e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [83][280/391]	Time  0.150 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1751e-03 (3.9654e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [83][290/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3221e-03 (3.9001e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [83][300/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.8933e-03 (3.9444e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [83][310/391]	Time  0.144 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0270e-03 (3.8884e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [83][320/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8401e-03 (3.9359e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [83][330/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.0673e-04 (3.9060e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [83][340/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2617e-02 (3.9957e-03)	Acc@1  99.22 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [83][350/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.9357e-03 (3.9525e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [83][360/391]	Time  0.146 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7784e-03 (4.0109e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [83][370/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.2151e-03 (4.0165e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [83][380/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.9824e-03 (3.9909e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [83][390/391]	Time  0.108 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.2424e-03 (3.9717e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
## e[83] optimizer.zero_grad (sum) time: 0.68772292137146
## e[83]       loss.backward (sum) time: 14.079782962799072
## e[83]      optimizer.step (sum) time: 14.605803966522217
## epoch[83] training(only) time: 54.24697017669678
# Switched to evaluate mode...
Test: [  0/100]	Time  0.191 ( 0.191)	Loss 3.5966e-01 (3.5966e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.051 ( 0.062)	Loss 2.8621e-01 (2.7719e-01)	Acc@1  94.00 ( 93.45)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 4.5438e-01 (3.1411e-01)	Acc@1  91.00 ( 93.10)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.050 ( 0.053)	Loss 3.0617e-01 (3.1625e-01)	Acc@1  94.00 ( 93.23)	Acc@5  99.00 ( 99.71)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 2.7665e-01 (3.2345e-01)	Acc@1  92.00 ( 93.02)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 2.0479e-01 (3.2359e-01)	Acc@1  96.00 ( 93.04)	Acc@5 100.00 ( 99.67)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 3.5129e-01 (3.1422e-01)	Acc@1  94.00 ( 93.16)	Acc@5 100.00 ( 99.72)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 5.6869e-01 (3.0863e-01)	Acc@1  91.00 ( 93.27)	Acc@5 100.00 ( 99.72)
Test: [ 80/100]	Time  0.045 ( 0.050)	Loss 1.6804e-01 (3.0794e-01)	Acc@1  96.00 ( 93.23)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.051 ( 0.049)	Loss 2.8366e-01 (3.0028e-01)	Acc@1  93.00 ( 93.31)	Acc@5 100.00 ( 99.77)
 * Acc@1 93.370 Acc@5 99.790
### epoch[83] execution time: 59.26281404495239
EPOCH 84
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [84][  0/391]	Time  0.301 ( 0.301)	Data  0.159 ( 0.159)	Loss 5.5374e-03 (5.5374e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [84][ 10/391]	Time  0.134 ( 0.155)	Data  0.001 ( 0.015)	Loss 2.0553e-03 (5.1315e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [84][ 20/391]	Time  0.134 ( 0.146)	Data  0.001 ( 0.009)	Loss 1.4624e-03 (4.4220e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [84][ 30/391]	Time  0.138 ( 0.144)	Data  0.001 ( 0.006)	Loss 6.4962e-03 (5.0942e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [84][ 40/391]	Time  0.139 ( 0.143)	Data  0.001 ( 0.005)	Loss 4.7172e-03 (4.5053e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [84][ 50/391]	Time  0.137 ( 0.142)	Data  0.001 ( 0.004)	Loss 8.1605e-03 (4.2751e-03)	Acc@1  99.22 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [84][ 60/391]	Time  0.137 ( 0.142)	Data  0.001 ( 0.004)	Loss 1.1010e-03 (4.1097e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [84][ 70/391]	Time  0.142 ( 0.141)	Data  0.001 ( 0.003)	Loss 2.2059e-03 (3.8693e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [84][ 80/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.003)	Loss 3.5425e-03 (4.5152e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [84][ 90/391]	Time  0.140 ( 0.141)	Data  0.001 ( 0.003)	Loss 7.3908e-04 (4.3943e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [84][100/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 3.5022e-03 (4.2608e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [84][110/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.002)	Loss 7.6911e-03 (4.4498e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [84][120/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.002)	Loss 1.4595e-03 (4.2622e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [84][130/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.002)	Loss 6.6757e-03 (4.3166e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [84][140/391]	Time  0.149 ( 0.140)	Data  0.001 ( 0.002)	Loss 9.0293e-03 (4.4058e-03)	Acc@1  99.22 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [84][150/391]	Time  0.133 ( 0.140)	Data  0.001 ( 0.002)	Loss 2.1558e-03 (4.3101e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [84][160/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.002)	Loss 2.0332e-03 (4.2620e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [84][170/391]	Time  0.150 ( 0.140)	Data  0.001 ( 0.002)	Loss 3.4462e-03 (4.1348e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [84][180/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0939e-03 (4.0329e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [84][190/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.5749e-04 (3.9399e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [84][200/391]	Time  0.143 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0144e-03 (3.8839e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [84][210/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6555e-03 (3.8480e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [84][220/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.3946e-03 (3.9044e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [84][230/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2480e-03 (3.9340e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [84][240/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.5215e-03 (3.9256e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [84][250/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5920e-03 (3.8946e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [84][260/391]	Time  0.143 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.2447e-04 (3.9151e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [84][270/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6963e-02 (3.9346e-03)	Acc@1  99.22 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [84][280/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.7473e-04 (3.9592e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [84][290/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7729e-02 (3.9844e-03)	Acc@1  99.22 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [84][300/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.5303e-03 (3.9961e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [84][310/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.8474e-03 (4.0008e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [84][320/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2715e-02 (3.9859e-03)	Acc@1  99.22 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [84][330/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.8588e-03 (3.9780e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [84][340/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.1699e-04 (3.9682e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [84][350/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.001)	Loss 1.8362e-03 (4.0006e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [84][360/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.001)	Loss 1.2546e-03 (3.9559e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [84][370/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.001)	Loss 1.8884e-03 (3.9648e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [84][380/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.001)	Loss 4.0393e-04 (3.9281e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [84][390/391]	Time  0.109 ( 0.139)	Data  0.001 ( 0.001)	Loss 3.6960e-03 (3.9264e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
## e[84] optimizer.zero_grad (sum) time: 0.6833956241607666
## e[84]       loss.backward (sum) time: 14.033546447753906
## e[84]      optimizer.step (sum) time: 14.616745471954346
## epoch[84] training(only) time: 54.26975464820862
# Switched to evaluate mode...
Test: [  0/100]	Time  0.198 ( 0.198)	Loss 2.9648e-01 (2.9648e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 2.8812e-01 (2.7112e-01)	Acc@1  95.00 ( 93.91)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 5.0256e-01 (3.1402e-01)	Acc@1  91.00 ( 93.19)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 2.9594e-01 (3.1636e-01)	Acc@1  93.00 ( 93.19)	Acc@5  99.00 ( 99.74)
Test: [ 40/100]	Time  0.049 ( 0.052)	Loss 3.0923e-01 (3.2506e-01)	Acc@1  91.00 ( 93.05)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.058 ( 0.051)	Loss 2.0644e-01 (3.2804e-01)	Acc@1  95.00 ( 93.00)	Acc@5 100.00 ( 99.69)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 3.6483e-01 (3.1829e-01)	Acc@1  93.00 ( 93.07)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 6.0307e-01 (3.1252e-01)	Acc@1  91.00 ( 93.18)	Acc@5 100.00 ( 99.72)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.5702e-01 (3.1111e-01)	Acc@1  96.00 ( 93.14)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.048 ( 0.049)	Loss 2.6947e-01 (3.0444e-01)	Acc@1  94.00 ( 93.22)	Acc@5 100.00 ( 99.77)
 * Acc@1 93.250 Acc@5 99.780
### epoch[84] execution time: 59.28415131568909
EPOCH 85
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [85][  0/391]	Time  0.305 ( 0.305)	Data  0.151 ( 0.151)	Loss 7.4895e-03 (7.4895e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [85][ 10/391]	Time  0.139 ( 0.153)	Data  0.001 ( 0.015)	Loss 2.7760e-03 (3.8143e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [85][ 20/391]	Time  0.137 ( 0.146)	Data  0.001 ( 0.008)	Loss 4.0824e-03 (3.9048e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [85][ 30/391]	Time  0.135 ( 0.143)	Data  0.001 ( 0.006)	Loss 1.4933e-03 (3.8083e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [85][ 40/391]	Time  0.134 ( 0.142)	Data  0.001 ( 0.005)	Loss 2.8723e-03 (3.5388e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [85][ 50/391]	Time  0.138 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.1708e-03 (3.5582e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [85][ 60/391]	Time  0.139 ( 0.141)	Data  0.001 ( 0.004)	Loss 4.0306e-04 (3.6039e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [85][ 70/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 8.5249e-03 (3.7001e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [85][ 80/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 4.6914e-03 (3.7124e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [85][ 90/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.0164e-03 (3.5256e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [85][100/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 9.6963e-04 (3.3732e-03)	Acc@1 100.00 ( 99.98)	Acc@5 100.00 (100.00)
Epoch: [85][110/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.002)	Loss 1.0705e-03 (3.4277e-03)	Acc@1 100.00 ( 99.98)	Acc@5 100.00 (100.00)
Epoch: [85][120/391]	Time  0.140 ( 0.140)	Data  0.001 ( 0.002)	Loss 1.3159e-02 (3.6762e-03)	Acc@1  99.22 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [85][130/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.6723e-03 (3.8415e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [85][140/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.4542e-04 (3.8668e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [85][150/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4266e-02 (3.8923e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [85][160/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2553e-03 (3.7804e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [85][170/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.0446e-03 (3.9290e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [85][180/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.3649e-03 (3.8884e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [85][190/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.5468e-03 (3.8974e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [85][200/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.4538e-03 (3.8578e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [85][210/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8998e-03 (3.8743e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [85][220/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.8546e-04 (3.8384e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [85][230/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.6569e-03 (3.8077e-03)	Acc@1  99.22 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [85][240/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.7247e-03 (3.7920e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [85][250/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.1736e-03 (3.7548e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [85][260/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.4020e-03 (3.7282e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [85][270/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0519e-03 (3.8226e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [85][280/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2163e-03 (3.8322e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [85][290/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.6939e-03 (3.8921e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [85][300/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.6360e-04 (3.8533e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [85][310/391]	Time  0.148 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.6890e-03 (3.8225e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [85][320/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.6903e-03 (3.7711e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [85][330/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2233e-03 (3.7381e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [85][340/391]	Time  0.147 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.0014e-03 (3.8151e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [85][350/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1050e-03 (3.7696e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [85][360/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.4641e-03 (3.7361e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [85][370/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.0665e-03 (3.8338e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [85][380/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.2027e-03 (3.8134e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [85][390/391]	Time  0.107 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.2845e-02 (3.8526e-03)	Acc@1  98.75 ( 99.93)	Acc@5 100.00 (100.00)
## e[85] optimizer.zero_grad (sum) time: 0.687640905380249
## e[85]       loss.backward (sum) time: 13.944249868392944
## e[85]      optimizer.step (sum) time: 14.67559289932251
## epoch[85] training(only) time: 54.17002081871033
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 3.5695e-01 (3.5695e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.048 ( 0.062)	Loss 2.8243e-01 (2.7859e-01)	Acc@1  94.00 ( 93.82)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 4.8348e-01 (3.1672e-01)	Acc@1  91.00 ( 93.29)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 2.9089e-01 (3.1685e-01)	Acc@1  94.00 ( 93.35)	Acc@5  99.00 ( 99.74)
Test: [ 40/100]	Time  0.049 ( 0.052)	Loss 2.8764e-01 (3.2518e-01)	Acc@1  92.00 ( 93.07)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.9916e-01 (3.2625e-01)	Acc@1  96.00 ( 93.06)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 3.2014e-01 (3.1408e-01)	Acc@1  94.00 ( 93.18)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 5.8231e-01 (3.0715e-01)	Acc@1  91.00 ( 93.34)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.5665e-01 (3.0576e-01)	Acc@1  96.00 ( 93.26)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 2.7412e-01 (2.9907e-01)	Acc@1  93.00 ( 93.35)	Acc@5 100.00 ( 99.78)
 * Acc@1 93.400 Acc@5 99.790
### epoch[85] execution time: 59.20750617980957
EPOCH 86
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [86][  0/391]	Time  0.300 ( 0.300)	Data  0.156 ( 0.156)	Loss 6.1511e-03 (6.1511e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [86][ 10/391]	Time  0.134 ( 0.153)	Data  0.001 ( 0.015)	Loss 9.1923e-04 (4.3032e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [86][ 20/391]	Time  0.135 ( 0.145)	Data  0.001 ( 0.008)	Loss 8.4494e-04 (4.0092e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [86][ 30/391]	Time  0.137 ( 0.143)	Data  0.001 ( 0.006)	Loss 6.6286e-03 (3.7905e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [86][ 40/391]	Time  0.141 ( 0.142)	Data  0.001 ( 0.005)	Loss 1.6437e-03 (3.8608e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [86][ 50/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.1828e-03 (3.8444e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [86][ 60/391]	Time  0.139 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.0208e-03 (3.5456e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [86][ 70/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 3.9480e-03 (3.5137e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [86][ 80/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.2476e-03 (3.5748e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [86][ 90/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.2695e-03 (3.8898e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [86][100/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.2513e-03 (3.6618e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [86][110/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 7.4078e-03 (3.5791e-03)	Acc@1  99.22 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [86][120/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5461e-03 (3.6739e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [86][130/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.3343e-04 (3.6424e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [86][140/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.1482e-03 (3.6309e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [86][150/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.9471e-03 (3.7292e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [86][160/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0575e-03 (3.6585e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [86][170/391]	Time  0.154 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.1409e-03 (3.5990e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [86][180/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8725e-03 (3.5083e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [86][190/391]	Time  0.147 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8465e-03 (3.5095e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [86][200/391]	Time  0.146 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.4115e-03 (3.5509e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [86][210/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.7091e-03 (3.7054e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [86][220/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7652e-03 (3.6524e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [86][230/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.8051e-04 (3.7050e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [86][240/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.8245e-03 (3.7297e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [86][250/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.5204e-04 (3.6929e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [86][260/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.6406e-04 (3.6629e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [86][270/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9028e-03 (3.6471e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [86][280/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.6524e-03 (3.6680e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [86][290/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.3768e-03 (3.7922e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [86][300/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.0147e-04 (3.8272e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [86][310/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.5777e-03 (3.8219e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [86][320/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6744e-03 (3.8802e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [86][330/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.3390e-03 (3.9386e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [86][340/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.2663e-03 (3.9283e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [86][350/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4892e-03 (3.9035e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [86][360/391]	Time  0.149 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1500e-03 (3.8833e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [86][370/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.6199e-03 (3.8508e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [86][380/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5895e-03 (3.8831e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [86][390/391]	Time  0.112 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.1531e-03 (3.9219e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
## e[86] optimizer.zero_grad (sum) time: 0.6858348846435547
## e[86]       loss.backward (sum) time: 14.010056972503662
## e[86]      optimizer.step (sum) time: 14.601962327957153
## epoch[86] training(only) time: 54.174848556518555
# Switched to evaluate mode...
Test: [  0/100]	Time  0.199 ( 0.199)	Loss 3.7055e-01 (3.7055e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.063)	Loss 2.6150e-01 (2.8177e-01)	Acc@1  94.00 ( 93.73)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.049 ( 0.056)	Loss 4.9297e-01 (3.2173e-01)	Acc@1  91.00 ( 93.19)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.048 ( 0.053)	Loss 3.1288e-01 (3.2232e-01)	Acc@1  93.00 ( 93.16)	Acc@5  99.00 ( 99.74)
Test: [ 40/100]	Time  0.046 ( 0.052)	Loss 2.8595e-01 (3.2819e-01)	Acc@1  93.00 ( 93.12)	Acc@5 100.00 ( 99.76)
Test: [ 50/100]	Time  0.048 ( 0.051)	Loss 1.9255e-01 (3.2832e-01)	Acc@1  97.00 ( 93.16)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.049 ( 0.051)	Loss 3.1587e-01 (3.1562e-01)	Acc@1  94.00 ( 93.26)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 5.8725e-01 (3.0816e-01)	Acc@1  91.00 ( 93.35)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.6153e-01 (3.0717e-01)	Acc@1  96.00 ( 93.28)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.047 ( 0.050)	Loss 3.0851e-01 (3.0057e-01)	Acc@1  93.00 ( 93.35)	Acc@5 100.00 ( 99.80)
 * Acc@1 93.410 Acc@5 99.810
### epoch[86] execution time: 59.211275577545166
EPOCH 87
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [87][  0/391]	Time  0.298 ( 0.298)	Data  0.155 ( 0.155)	Loss 3.9192e-03 (3.9192e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [87][ 10/391]	Time  0.137 ( 0.152)	Data  0.001 ( 0.015)	Loss 1.0048e-03 (3.2794e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [87][ 20/391]	Time  0.136 ( 0.145)	Data  0.001 ( 0.008)	Loss 2.5280e-03 (3.1898e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [87][ 30/391]	Time  0.148 ( 0.143)	Data  0.001 ( 0.006)	Loss 1.0733e-03 (2.9501e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [87][ 40/391]	Time  0.142 ( 0.141)	Data  0.001 ( 0.005)	Loss 3.7132e-03 (2.9827e-03)	Acc@1 100.00 ( 99.98)	Acc@5 100.00 (100.00)
Epoch: [87][ 50/391]	Time  0.136 ( 0.141)	Data  0.001 ( 0.004)	Loss 9.9556e-04 (2.9756e-03)	Acc@1 100.00 ( 99.98)	Acc@5 100.00 (100.00)
Epoch: [87][ 60/391]	Time  0.149 ( 0.141)	Data  0.001 ( 0.004)	Loss 3.6438e-03 (3.0573e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [87][ 70/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 7.4696e-03 (3.0644e-03)	Acc@1 100.00 ( 99.98)	Acc@5 100.00 (100.00)
Epoch: [87][ 80/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.3523e-03 (3.1185e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [87][ 90/391]	Time  0.144 ( 0.140)	Data  0.001 ( 0.003)	Loss 5.9441e-03 (3.0781e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [87][100/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.003)	Loss 9.6043e-04 (3.0732e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [87][110/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1694e-03 (3.0277e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [87][120/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.4028e-03 (3.1456e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [87][130/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6684e-03 (3.0699e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [87][140/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.5132e-03 (3.0864e-03)	Acc@1  99.22 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [87][150/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.8928e-03 (3.2156e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [87][160/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.0716e-04 (3.2744e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [87][170/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6430e-02 (3.2966e-03)	Acc@1  99.22 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [87][180/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.3890e-03 (3.2161e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [87][190/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.7021e-04 (3.2959e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [87][200/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.7273e-03 (3.2340e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [87][210/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.3885e-03 (3.2400e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [87][220/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.8939e-03 (3.1975e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [87][230/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.9587e-03 (3.2158e-03)	Acc@1  99.22 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [87][240/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6843e-03 (3.2549e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [87][250/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.6294e-03 (3.2271e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [87][260/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.6385e-04 (3.2170e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [87][270/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1622e-02 (3.2481e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [87][280/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.6516e-04 (3.2505e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [87][290/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2834e-03 (3.2097e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [87][300/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.4350e-03 (3.2120e-03)	Acc@1  99.22 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [87][310/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3575e-03 (3.1829e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [87][320/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.7225e-04 (3.1679e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [87][330/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6130e-02 (3.2933e-03)	Acc@1  99.22 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [87][340/391]	Time  0.150 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.9858e-03 (3.2786e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [87][350/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3306e-02 (3.3082e-03)	Acc@1  99.22 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [87][360/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5292e-02 (3.3378e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [87][370/391]	Time  0.146 ( 0.138)	Data  0.001 ( 0.001)	Loss 9.8174e-03 (3.4101e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [87][380/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.001)	Loss 8.6888e-04 (3.4090e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [87][390/391]	Time  0.106 ( 0.138)	Data  0.001 ( 0.001)	Loss 2.2079e-03 (3.3862e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
## e[87] optimizer.zero_grad (sum) time: 0.6846513748168945
## e[87]       loss.backward (sum) time: 14.02046012878418
## e[87]      optimizer.step (sum) time: 14.58578610420227
## epoch[87] training(only) time: 54.17765426635742
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 3.7253e-01 (3.7253e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.063 ( 0.062)	Loss 2.6426e-01 (2.8424e-01)	Acc@1  94.00 ( 93.64)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 4.8209e-01 (3.2262e-01)	Acc@1  91.00 ( 93.10)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 3.0301e-01 (3.2357e-01)	Acc@1  93.00 ( 93.10)	Acc@5  99.00 ( 99.74)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 2.7315e-01 (3.3017e-01)	Acc@1  94.00 ( 93.02)	Acc@5 100.00 ( 99.76)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.9437e-01 (3.2850e-01)	Acc@1  96.00 ( 93.04)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 3.0109e-01 (3.1579e-01)	Acc@1  94.00 ( 93.16)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 5.6858e-01 (3.0798e-01)	Acc@1  91.00 ( 93.24)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.5890e-01 (3.0683e-01)	Acc@1  96.00 ( 93.21)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.049 ( 0.049)	Loss 2.9764e-01 (2.9972e-01)	Acc@1  94.00 ( 93.31)	Acc@5 100.00 ( 99.80)
 * Acc@1 93.370 Acc@5 99.800
### epoch[87] execution time: 59.15272545814514
EPOCH 88
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [88][  0/391]	Time  0.287 ( 0.287)	Data  0.144 ( 0.144)	Loss 2.2462e-03 (2.2462e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [88][ 10/391]	Time  0.140 ( 0.153)	Data  0.001 ( 0.014)	Loss 2.7523e-03 (4.1552e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [88][ 20/391]	Time  0.141 ( 0.146)	Data  0.001 ( 0.008)	Loss 1.0746e-03 (3.3274e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [88][ 30/391]	Time  0.133 ( 0.144)	Data  0.001 ( 0.006)	Loss 2.1848e-03 (3.0132e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [88][ 40/391]	Time  0.136 ( 0.143)	Data  0.001 ( 0.004)	Loss 1.0920e-03 (2.9924e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [88][ 50/391]	Time  0.139 ( 0.142)	Data  0.001 ( 0.004)	Loss 3.5810e-03 (3.1136e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [88][ 60/391]	Time  0.133 ( 0.141)	Data  0.001 ( 0.003)	Loss 1.7197e-03 (2.9775e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [88][ 70/391]	Time  0.140 ( 0.141)	Data  0.001 ( 0.003)	Loss 5.1808e-03 (3.0336e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [88][ 80/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.3922e-03 (3.1928e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [88][ 90/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.4778e-03 (3.1107e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [88][100/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.002)	Loss 1.8855e-03 (3.0679e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [88][110/391]	Time  0.140 ( 0.140)	Data  0.001 ( 0.002)	Loss 8.7691e-03 (3.1192e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [88][120/391]	Time  0.141 ( 0.140)	Data  0.001 ( 0.002)	Loss 4.5826e-03 (3.2427e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [88][130/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.002)	Loss 5.2385e-03 (3.4207e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [88][140/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.002)	Loss 4.9548e-04 (3.5723e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [88][150/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.3560e-03 (3.5783e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [88][160/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.5215e-03 (3.6132e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [88][170/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.6700e-03 (3.6037e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [88][180/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.7707e-03 (3.5724e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [88][190/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2683e-03 (3.7206e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [88][200/391]	Time  0.145 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.4563e-03 (3.6708e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [88][210/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2573e-03 (3.6184e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [88][220/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.6851e-04 (3.5891e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [88][230/391]	Time  0.148 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.4355e-04 (3.6036e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [88][240/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.1850e-03 (3.6127e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [88][250/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.4237e-03 (3.6537e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [88][260/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.4530e-03 (3.7342e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [88][270/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.4445e-03 (3.7274e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [88][280/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7467e-03 (3.6876e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [88][290/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8261e-03 (3.6806e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [88][300/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.001)	Loss 3.5506e-03 (3.6331e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [88][310/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.001)	Loss 2.4854e-02 (3.7173e-03)	Acc@1  99.22 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [88][320/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.001)	Loss 6.4163e-03 (3.7167e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [88][330/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.001)	Loss 3.4665e-03 (3.6951e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [88][340/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.001)	Loss 2.2814e-03 (3.7016e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [88][350/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.001)	Loss 3.2625e-03 (3.7196e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [88][360/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.001)	Loss 7.5187e-04 (3.6819e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [88][370/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.001)	Loss 1.8164e-03 (3.6435e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [88][380/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.001)	Loss 7.5850e-03 (3.6773e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [88][390/391]	Time  0.110 ( 0.139)	Data  0.001 ( 0.001)	Loss 5.9983e-03 (3.6704e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
## e[88] optimizer.zero_grad (sum) time: 0.6830871105194092
## e[88]       loss.backward (sum) time: 14.01641058921814
## e[88]      optimizer.step (sum) time: 14.59076452255249
## epoch[88] training(only) time: 54.29256796836853
# Switched to evaluate mode...
Test: [  0/100]	Time  0.190 ( 0.190)	Loss 3.5529e-01 (3.5529e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.062)	Loss 2.5725e-01 (2.7609e-01)	Acc@1  95.00 ( 94.09)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 4.4947e-01 (3.1470e-01)	Acc@1  91.00 ( 93.29)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 3.0085e-01 (3.1559e-01)	Acc@1  93.00 ( 93.19)	Acc@5  99.00 ( 99.74)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 2.7176e-01 (3.2384e-01)	Acc@1  92.00 ( 93.05)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.050 ( 0.051)	Loss 1.9448e-01 (3.2415e-01)	Acc@1  96.00 ( 93.08)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 3.4398e-01 (3.1258e-01)	Acc@1  94.00 ( 93.26)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 5.5681e-01 (3.0569e-01)	Acc@1  91.00 ( 93.30)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4653e-01 (3.0405e-01)	Acc@1  96.00 ( 93.26)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 2.9664e-01 (2.9718e-01)	Acc@1  93.00 ( 93.36)	Acc@5 100.00 ( 99.78)
 * Acc@1 93.430 Acc@5 99.790
### epoch[88] execution time: 59.28721904754639
EPOCH 89
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [89][  0/391]	Time  0.302 ( 0.302)	Data  0.153 ( 0.153)	Loss 2.2976e-03 (2.2976e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [89][ 10/391]	Time  0.138 ( 0.154)	Data  0.001 ( 0.015)	Loss 1.7717e-03 (4.1159e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [89][ 20/391]	Time  0.139 ( 0.146)	Data  0.001 ( 0.008)	Loss 8.4652e-04 (3.4716e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [89][ 30/391]	Time  0.134 ( 0.143)	Data  0.001 ( 0.006)	Loss 1.6036e-03 (3.4097e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [89][ 40/391]	Time  0.138 ( 0.142)	Data  0.001 ( 0.005)	Loss 2.3830e-03 (3.4134e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [89][ 50/391]	Time  0.136 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.5996e-03 (3.3436e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [89][ 60/391]	Time  0.146 ( 0.141)	Data  0.001 ( 0.004)	Loss 4.1232e-03 (3.2945e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [89][ 70/391]	Time  0.140 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.1154e-03 (3.3546e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [89][ 80/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 7.3407e-03 (3.4316e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [89][ 90/391]	Time  0.145 ( 0.140)	Data  0.001 ( 0.003)	Loss 5.7210e-04 (3.6118e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [89][100/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.4907e-03 (3.5089e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [89][110/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.9354e-03 (3.5253e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [89][120/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.4112e-04 (3.5039e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [89][130/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1337e-03 (3.4530e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [89][140/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5661e-02 (3.5464e-03)	Acc@1  99.22 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [89][150/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0780e-03 (3.4416e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [89][160/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5063e-03 (3.4359e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [89][170/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.2312e-03 (3.3699e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [89][180/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.6697e-03 (3.3124e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [89][190/391]	Time  0.144 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.7356e-02 (3.4876e-03)	Acc@1  99.22 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [89][200/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2821e-03 (3.5004e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [89][210/391]	Time  0.145 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.2933e-03 (3.5424e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [89][220/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.8542e-03 (3.5149e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [89][230/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2021e-03 (3.5295e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [89][240/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.2324e-04 (3.4898e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [89][250/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.5744e-03 (3.4362e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [89][260/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0275e-02 (3.5185e-03)	Acc@1  99.22 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [89][270/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.7031e-03 (3.4963e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [89][280/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0474e-03 (3.5150e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [89][290/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.9635e-03 (3.5155e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [89][300/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4120e-03 (3.5860e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [89][310/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.1041e-03 (3.6158e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [89][320/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.5591e-03 (3.5948e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [89][330/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.3194e-04 (3.5667e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [89][340/391]	Time  0.142 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.0950e-04 (3.5457e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [89][350/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.2757e-04 (3.5306e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [89][360/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6220e-03 (3.5012e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [89][370/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.2120e-03 (3.5020e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [89][380/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.3561e-03 (3.5366e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [89][390/391]	Time  0.111 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.9556e-03 (3.5532e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
## e[89] optimizer.zero_grad (sum) time: 0.688239574432373
## e[89]       loss.backward (sum) time: 14.062858819961548
## e[89]      optimizer.step (sum) time: 14.60668158531189
## epoch[89] training(only) time: 54.27625322341919
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 3.4013e-01 (3.4013e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.058 ( 0.062)	Loss 2.9835e-01 (2.7901e-01)	Acc@1  94.00 ( 93.55)	Acc@5 100.00 (100.00)
Test: [ 20/100]	Time  0.051 ( 0.055)	Loss 4.7489e-01 (3.1864e-01)	Acc@1  91.00 ( 93.24)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 3.0086e-01 (3.1929e-01)	Acc@1  94.00 ( 93.26)	Acc@5  99.00 ( 99.74)
Test: [ 40/100]	Time  0.047 ( 0.052)	Loss 2.8742e-01 (3.2644e-01)	Acc@1  93.00 ( 93.15)	Acc@5 100.00 ( 99.73)
Test: [ 50/100]	Time  0.063 ( 0.052)	Loss 2.0527e-01 (3.2719e-01)	Acc@1  96.00 ( 93.12)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.055 ( 0.051)	Loss 3.3683e-01 (3.1651e-01)	Acc@1  94.00 ( 93.26)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 6.1695e-01 (3.1014e-01)	Acc@1  92.00 ( 93.42)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.048 ( 0.050)	Loss 1.5839e-01 (3.0917e-01)	Acc@1  96.00 ( 93.37)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.050 ( 0.050)	Loss 2.7741e-01 (3.0224e-01)	Acc@1  94.00 ( 93.43)	Acc@5 100.00 ( 99.78)
 * Acc@1 93.470 Acc@5 99.780
### epoch[89] execution time: 59.34830832481384
### Training complete:
#### total training(only) time: 4869.669943809509
##### Total run time: 5325.456835508347
