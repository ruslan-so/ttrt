# Model: mobilenet2
# Dataset: cifardecem
# Batch size: 128
# Freezeout: False
# Low precision: False
# Epochs: 90
# Initial learning rate: 0.1
# module_name: models_cifardecem.mobilenet
<function mobilenet2 at 0x7f618ae9bf28>
# model requested: 'mobilenet2'
# printing out the model
MobileNetV2(
  (pre): Sequential(
    (0): Conv2d(3, 32, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (stage1): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage2): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96)
        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144)
        (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage3): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144)
        (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage4): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192)
        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage5): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576)
        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576)
        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage6): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576)
        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960)
        (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960)
        (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage7): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960)
      (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (conv1): Sequential(
    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (conv2): Conv2d(1280, 10, kernel_size=(1, 1), stride=(1, 1))
)
# model is full precision
# Model: mobilenet2
# Dataset: cifardecem
# Freezeout: False
# Low precision: False
# Initial learning rate: 0.1
# Criterion: CrossEntropyLoss()
# Optimizer: SGD
#	lr 0.1
#	momentum 0.9
#	dampening 0
#	weight_decay 0.0001
#	nesterov False
CIFAR10 loading and normalization
==> Preparing data..
# CIFAR10 / full precision
Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data-decem/cifar-10-python.tar.gz
Extracting ./data-decem/cifar-10-python.tar.gz to ./data-decem
Files already downloaded and verified
#--> Training data: <--#
#-->>
EPOCH 0
# current learning rate: 0.1
# Switched to train mode...
Epoch: [0][  0/391]	Time  3.581 ( 3.581)	Data  0.137 ( 0.137)	Loss 2.3085e+00 (2.3085e+00)	Acc@1  10.94 ( 10.94)	Acc@5  49.22 ( 49.22)
Epoch: [0][ 10/391]	Time  0.062 ( 0.383)	Data  0.001 ( 0.013)	Loss 2.5134e+00 (3.0522e+00)	Acc@1  17.97 ( 11.29)	Acc@5  59.38 ( 52.84)
Epoch: [0][ 20/391]	Time  0.062 ( 0.232)	Data  0.001 ( 0.008)	Loss 2.6735e+00 (2.9634e+00)	Acc@1  20.31 ( 13.21)	Acc@5  65.62 ( 56.96)
Epoch: [0][ 30/391]	Time  0.062 ( 0.178)	Data  0.001 ( 0.005)	Loss 2.2864e+00 (2.8236e+00)	Acc@1  14.84 ( 14.54)	Acc@5  75.78 ( 61.01)
Epoch: [0][ 40/391]	Time  0.061 ( 0.150)	Data  0.001 ( 0.004)	Loss 1.9133e+00 (2.6630e+00)	Acc@1  31.25 ( 16.18)	Acc@5  85.16 ( 65.30)
Epoch: [0][ 50/391]	Time  0.078 ( 0.133)	Data  0.001 ( 0.004)	Loss 1.9790e+00 (2.5448e+00)	Acc@1  20.31 ( 17.78)	Acc@5  81.25 ( 68.01)
Epoch: [0][ 60/391]	Time  0.064 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.9011e+00 (2.4480e+00)	Acc@1  34.38 ( 19.04)	Acc@5  80.47 ( 69.93)
Epoch: [0][ 70/391]	Time  0.063 ( 0.114)	Data  0.001 ( 0.003)	Loss 1.8783e+00 (2.3744e+00)	Acc@1  28.91 ( 19.97)	Acc@5  80.47 ( 71.62)
Epoch: [0][ 80/391]	Time  0.064 ( 0.108)	Data  0.001 ( 0.003)	Loss 1.7696e+00 (2.3078e+00)	Acc@1  34.38 ( 21.31)	Acc@5  85.94 ( 73.33)
Epoch: [0][ 90/391]	Time  0.062 ( 0.103)	Data  0.001 ( 0.003)	Loss 1.9033e+00 (2.2560e+00)	Acc@1  24.22 ( 22.13)	Acc@5  83.59 ( 74.73)
Epoch: [0][100/391]	Time  0.062 ( 0.099)	Data  0.001 ( 0.002)	Loss 1.7238e+00 (2.2118e+00)	Acc@1  35.16 ( 22.86)	Acc@5  85.94 ( 75.91)
Epoch: [0][110/391]	Time  0.063 ( 0.096)	Data  0.001 ( 0.002)	Loss 1.8518e+00 (2.1714e+00)	Acc@1  22.66 ( 23.58)	Acc@5  83.59 ( 77.01)
Epoch: [0][120/391]	Time  0.062 ( 0.093)	Data  0.001 ( 0.002)	Loss 1.8675e+00 (2.1376e+00)	Acc@1  32.81 ( 24.37)	Acc@5  86.72 ( 77.89)
Epoch: [0][130/391]	Time  0.065 ( 0.091)	Data  0.001 ( 0.002)	Loss 1.7272e+00 (2.1083e+00)	Acc@1  32.03 ( 24.91)	Acc@5  89.06 ( 78.60)
Epoch: [0][140/391]	Time  0.063 ( 0.089)	Data  0.001 ( 0.002)	Loss 1.6087e+00 (2.0828e+00)	Acc@1  36.72 ( 25.45)	Acc@5  90.62 ( 79.19)
Epoch: [0][150/391]	Time  0.065 ( 0.087)	Data  0.001 ( 0.002)	Loss 1.6918e+00 (2.0584e+00)	Acc@1  32.81 ( 26.10)	Acc@5  89.06 ( 79.75)
Epoch: [0][160/391]	Time  0.063 ( 0.086)	Data  0.001 ( 0.002)	Loss 1.6475e+00 (2.0369e+00)	Acc@1  39.84 ( 26.64)	Acc@5  88.28 ( 80.22)
Epoch: [0][170/391]	Time  0.062 ( 0.085)	Data  0.001 ( 0.002)	Loss 1.6980e+00 (2.0152e+00)	Acc@1  37.50 ( 27.22)	Acc@5  89.06 ( 80.71)
Epoch: [0][180/391]	Time  0.069 ( 0.084)	Data  0.001 ( 0.002)	Loss 1.6051e+00 (1.9947e+00)	Acc@1  37.50 ( 27.70)	Acc@5  91.41 ( 81.20)
Epoch: [0][190/391]	Time  0.062 ( 0.083)	Data  0.001 ( 0.002)	Loss 1.8327e+00 (1.9779e+00)	Acc@1  31.25 ( 28.08)	Acc@5  85.16 ( 81.60)
Epoch: [0][200/391]	Time  0.063 ( 0.082)	Data  0.001 ( 0.002)	Loss 1.6163e+00 (1.9584e+00)	Acc@1  39.06 ( 28.56)	Acc@5  89.84 ( 82.05)
Epoch: [0][210/391]	Time  0.064 ( 0.081)	Data  0.001 ( 0.002)	Loss 1.6768e+00 (1.9423e+00)	Acc@1  33.59 ( 29.02)	Acc@5  86.72 ( 82.41)
Epoch: [0][220/391]	Time  0.060 ( 0.080)	Data  0.001 ( 0.002)	Loss 1.5761e+00 (1.9304e+00)	Acc@1  46.09 ( 29.48)	Acc@5  92.97 ( 82.70)
Epoch: [0][230/391]	Time  0.064 ( 0.079)	Data  0.001 ( 0.002)	Loss 1.5402e+00 (1.9166e+00)	Acc@1  42.97 ( 29.85)	Acc@5  91.41 ( 83.03)
Epoch: [0][240/391]	Time  0.067 ( 0.079)	Data  0.001 ( 0.002)	Loss 1.6753e+00 (1.9039e+00)	Acc@1  39.84 ( 30.30)	Acc@5  85.94 ( 83.34)
Epoch: [0][250/391]	Time  0.064 ( 0.078)	Data  0.001 ( 0.002)	Loss 1.7898e+00 (1.8932e+00)	Acc@1  30.47 ( 30.61)	Acc@5  91.41 ( 83.58)
Epoch: [0][260/391]	Time  0.063 ( 0.078)	Data  0.001 ( 0.002)	Loss 1.5754e+00 (1.8824e+00)	Acc@1  38.28 ( 30.89)	Acc@5  90.62 ( 83.82)
Epoch: [0][270/391]	Time  0.078 ( 0.077)	Data  0.001 ( 0.002)	Loss 1.4303e+00 (1.8696e+00)	Acc@1  44.53 ( 31.27)	Acc@5  93.75 ( 84.12)
Epoch: [0][280/391]	Time  0.069 ( 0.077)	Data  0.001 ( 0.002)	Loss 1.5055e+00 (1.8569e+00)	Acc@1  46.09 ( 31.73)	Acc@5  91.41 ( 84.40)
Epoch: [0][290/391]	Time  0.060 ( 0.076)	Data  0.001 ( 0.002)	Loss 1.5348e+00 (1.8473e+00)	Acc@1  39.06 ( 32.08)	Acc@5  92.19 ( 84.58)
Epoch: [0][300/391]	Time  0.062 ( 0.076)	Data  0.001 ( 0.002)	Loss 1.5326e+00 (1.8359e+00)	Acc@1  44.53 ( 32.45)	Acc@5  91.41 ( 84.83)
Epoch: [0][310/391]	Time  0.062 ( 0.075)	Data  0.001 ( 0.002)	Loss 1.4755e+00 (1.8261e+00)	Acc@1  46.88 ( 32.85)	Acc@5  92.97 ( 85.01)
Epoch: [0][320/391]	Time  0.065 ( 0.075)	Data  0.001 ( 0.002)	Loss 1.5688e+00 (1.8155e+00)	Acc@1  38.28 ( 33.20)	Acc@5  92.19 ( 85.26)
Epoch: [0][330/391]	Time  0.065 ( 0.075)	Data  0.001 ( 0.002)	Loss 1.4098e+00 (1.8065e+00)	Acc@1  45.31 ( 33.47)	Acc@5  93.75 ( 85.46)
Epoch: [0][340/391]	Time  0.064 ( 0.074)	Data  0.001 ( 0.002)	Loss 1.4289e+00 (1.7977e+00)	Acc@1  46.09 ( 33.80)	Acc@5  92.97 ( 85.63)
Epoch: [0][350/391]	Time  0.066 ( 0.074)	Data  0.001 ( 0.001)	Loss 1.4916e+00 (1.7883e+00)	Acc@1  41.41 ( 34.08)	Acc@5  91.41 ( 85.84)
Epoch: [0][360/391]	Time  0.060 ( 0.074)	Data  0.001 ( 0.001)	Loss 1.6346e+00 (1.7805e+00)	Acc@1  39.06 ( 34.34)	Acc@5  88.28 ( 85.97)
Epoch: [0][370/391]	Time  0.058 ( 0.074)	Data  0.001 ( 0.001)	Loss 1.4941e+00 (1.7722e+00)	Acc@1  47.66 ( 34.65)	Acc@5  90.62 ( 86.14)
Epoch: [0][380/391]	Time  0.063 ( 0.073)	Data  0.001 ( 0.001)	Loss 1.3826e+00 (1.7624e+00)	Acc@1  49.22 ( 35.04)	Acc@5  94.53 ( 86.31)
Epoch: [0][390/391]	Time  0.432 ( 0.074)	Data  0.001 ( 0.001)	Loss 1.5252e+00 (1.7535e+00)	Acc@1  40.00 ( 35.37)	Acc@5  92.50 ( 86.48)
## e[0] optimizer.zero_grad (sum) time: 0.398256778717041
## e[0]       loss.backward (sum) time: 7.4754478931427
## e[0]      optimizer.step (sum) time: 3.431650400161743
## epoch[0] training(only) time: 28.947071075439453
# Switched to evaluate mode...
Test: [  0/100]	Time  0.336 ( 0.336)	Loss 1.6509e+00 (1.6509e+00)	Acc@1  43.00 ( 43.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.024 ( 0.053)	Loss 1.6238e+00 (1.6663e+00)	Acc@1  42.00 ( 42.82)	Acc@5  88.00 ( 88.45)
Test: [ 20/100]	Time  0.024 ( 0.041)	Loss 1.5735e+00 (1.6780e+00)	Acc@1  48.00 ( 43.10)	Acc@5  89.00 ( 88.43)
Test: [ 30/100]	Time  0.025 ( 0.036)	Loss 1.5288e+00 (1.6884e+00)	Acc@1  53.00 ( 42.81)	Acc@5  88.00 ( 88.13)
Test: [ 40/100]	Time  0.024 ( 0.034)	Loss 1.8147e+00 (1.6920e+00)	Acc@1  35.00 ( 42.27)	Acc@5  87.00 ( 88.56)
Test: [ 50/100]	Time  0.028 ( 0.032)	Loss 1.6976e+00 (1.6767e+00)	Acc@1  39.00 ( 42.59)	Acc@5  90.00 ( 88.86)
Test: [ 60/100]	Time  0.024 ( 0.031)	Loss 1.7558e+00 (1.6854e+00)	Acc@1  40.00 ( 42.38)	Acc@5  84.00 ( 88.79)
Test: [ 70/100]	Time  0.028 ( 0.030)	Loss 1.7912e+00 (1.6828e+00)	Acc@1  47.00 ( 42.31)	Acc@5  87.00 ( 88.75)
Test: [ 80/100]	Time  0.026 ( 0.030)	Loss 1.5504e+00 (1.6805e+00)	Acc@1  47.00 ( 42.43)	Acc@5  87.00 ( 88.79)
Test: [ 90/100]	Time  0.024 ( 0.029)	Loss 1.6386e+00 (1.6770e+00)	Acc@1  45.00 ( 42.56)	Acc@5  91.00 ( 88.81)
 * Acc@1 42.680 Acc@5 88.880
### epoch[0] execution time: 31.96021294593811
EPOCH 1
# current learning rate: 0.1
# Switched to train mode...
Epoch: [1][  0/391]	Time  0.246 ( 0.246)	Data  0.172 ( 0.172)	Loss 1.5470e+00 (1.5470e+00)	Acc@1  41.41 ( 41.41)	Acc@5  89.84 ( 89.84)
Epoch: [1][ 10/391]	Time  0.062 ( 0.081)	Data  0.001 ( 0.017)	Loss 1.3948e+00 (1.4240e+00)	Acc@1  50.00 ( 48.51)	Acc@5  92.97 ( 92.33)
Epoch: [1][ 20/391]	Time  0.061 ( 0.073)	Data  0.001 ( 0.009)	Loss 1.3112e+00 (1.4105e+00)	Acc@1  52.34 ( 48.36)	Acc@5  92.19 ( 92.56)
Epoch: [1][ 30/391]	Time  0.063 ( 0.070)	Data  0.001 ( 0.007)	Loss 1.6298e+00 (1.4178e+00)	Acc@1  40.62 ( 47.73)	Acc@5  85.16 ( 92.21)
Epoch: [1][ 40/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.005)	Loss 1.1544e+00 (1.4036e+00)	Acc@1  57.03 ( 47.96)	Acc@5  96.88 ( 92.49)
Epoch: [1][ 50/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.5228e+00 (1.4089e+00)	Acc@1  46.88 ( 48.01)	Acc@5  93.75 ( 92.46)
Epoch: [1][ 60/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.5298e+00 (1.4127e+00)	Acc@1  50.00 ( 47.94)	Acc@5  93.75 ( 92.48)
Epoch: [1][ 70/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.4003e+00 (1.4142e+00)	Acc@1  41.41 ( 47.77)	Acc@5  93.75 ( 92.54)
Epoch: [1][ 80/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.4646e+00 (1.4086e+00)	Acc@1  46.88 ( 48.07)	Acc@5  90.62 ( 92.56)
Epoch: [1][ 90/391]	Time  0.071 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.2261e+00 (1.3950e+00)	Acc@1  56.25 ( 48.70)	Acc@5  96.09 ( 92.75)
Epoch: [1][100/391]	Time  0.072 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.3838e+00 (1.3820e+00)	Acc@1  44.53 ( 49.20)	Acc@5  92.19 ( 92.91)
Epoch: [1][110/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.3352e+00 (1.3807e+00)	Acc@1  50.00 ( 49.25)	Acc@5  94.53 ( 92.91)
Epoch: [1][120/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3286e+00 (1.3758e+00)	Acc@1  52.34 ( 49.51)	Acc@5  93.75 ( 92.96)
Epoch: [1][130/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4801e+00 (1.3732e+00)	Acc@1  51.56 ( 49.68)	Acc@5  90.62 ( 93.05)
Epoch: [1][140/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3055e+00 (1.3716e+00)	Acc@1  53.91 ( 49.79)	Acc@5  96.09 ( 93.02)
Epoch: [1][150/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2288e+00 (1.3672e+00)	Acc@1  56.25 ( 49.99)	Acc@5  96.09 ( 93.02)
Epoch: [1][160/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2473e+00 (1.3649e+00)	Acc@1  52.34 ( 50.11)	Acc@5  96.88 ( 93.04)
Epoch: [1][170/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2294e+00 (1.3620e+00)	Acc@1  51.56 ( 50.21)	Acc@5  95.31 ( 93.06)
Epoch: [1][180/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5406e+00 (1.3591e+00)	Acc@1  50.00 ( 50.37)	Acc@5  90.62 ( 93.09)
Epoch: [1][190/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1217e+00 (1.3539e+00)	Acc@1  57.81 ( 50.58)	Acc@5  97.66 ( 93.16)
Epoch: [1][200/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3202e+00 (1.3499e+00)	Acc@1  52.34 ( 50.72)	Acc@5  92.97 ( 93.17)
Epoch: [1][210/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3404e+00 (1.3456e+00)	Acc@1  53.91 ( 50.85)	Acc@5  91.41 ( 93.28)
Epoch: [1][220/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2126e+00 (1.3401e+00)	Acc@1  54.69 ( 51.03)	Acc@5  94.53 ( 93.36)
Epoch: [1][230/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3782e+00 (1.3378e+00)	Acc@1  44.53 ( 51.17)	Acc@5  94.53 ( 93.39)
Epoch: [1][240/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1412e+00 (1.3309e+00)	Acc@1  56.25 ( 51.43)	Acc@5  96.09 ( 93.46)
Epoch: [1][250/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1837e+00 (1.3274e+00)	Acc@1  55.47 ( 51.57)	Acc@5  96.09 ( 93.49)
Epoch: [1][260/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1239e+00 (1.3222e+00)	Acc@1  60.94 ( 51.81)	Acc@5  93.75 ( 93.51)
Epoch: [1][270/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1705e+00 (1.3178e+00)	Acc@1  53.91 ( 52.02)	Acc@5  97.66 ( 93.57)
Epoch: [1][280/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2106e+00 (1.3139e+00)	Acc@1  60.16 ( 52.18)	Acc@5  96.09 ( 93.63)
Epoch: [1][290/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2585e+00 (1.3110e+00)	Acc@1  52.34 ( 52.27)	Acc@5  92.97 ( 93.65)
Epoch: [1][300/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3526e+00 (1.3076e+00)	Acc@1  50.78 ( 52.40)	Acc@5  92.19 ( 93.67)
Epoch: [1][310/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2688e+00 (1.3027e+00)	Acc@1  62.50 ( 52.61)	Acc@5  94.53 ( 93.73)
Epoch: [1][320/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2257e+00 (1.2989e+00)	Acc@1  56.25 ( 52.75)	Acc@5  94.53 ( 93.78)
Epoch: [1][330/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2253e+00 (1.2956e+00)	Acc@1  56.25 ( 52.86)	Acc@5  93.75 ( 93.80)
Epoch: [1][340/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3098e+00 (1.2921e+00)	Acc@1  51.56 ( 53.04)	Acc@5  90.62 ( 93.83)
Epoch: [1][350/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3132e+00 (1.2901e+00)	Acc@1  47.66 ( 53.12)	Acc@5  91.41 ( 93.84)
Epoch: [1][360/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2125e+00 (1.2880e+00)	Acc@1  60.16 ( 53.21)	Acc@5  94.53 ( 93.86)
Epoch: [1][370/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.0075e+00 (1.2852e+00)	Acc@1  69.53 ( 53.33)	Acc@5  95.31 ( 93.91)
Epoch: [1][380/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.1190e+00 (1.2814e+00)	Acc@1  66.41 ( 53.49)	Acc@5  96.09 ( 93.98)
Epoch: [1][390/391]	Time  0.057 ( 0.064)	Data  0.001 ( 0.001)	Loss 1.1587e+00 (1.2777e+00)	Acc@1  62.50 ( 53.65)	Acc@5  96.25 ( 94.02)
## e[1] optimizer.zero_grad (sum) time: 0.41008424758911133
## e[1]       loss.backward (sum) time: 6.926292181015015
## e[1]      optimizer.step (sum) time: 3.403512477874756
## epoch[1] training(only) time: 25.28573703765869
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 1.1657e+00 (1.1657e+00)	Acc@1  56.00 ( 56.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.024 ( 0.041)	Loss 9.6681e-01 (1.2210e+00)	Acc@1  60.00 ( 56.82)	Acc@5  96.00 ( 94.82)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 1.0181e+00 (1.2205e+00)	Acc@1  64.00 ( 56.71)	Acc@5  95.00 ( 94.38)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 1.0502e+00 (1.2264e+00)	Acc@1  65.00 ( 56.74)	Acc@5  97.00 ( 94.42)
Test: [ 40/100]	Time  0.028 ( 0.030)	Loss 1.1816e+00 (1.2260e+00)	Acc@1  59.00 ( 56.78)	Acc@5  96.00 ( 94.46)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 1.4844e+00 (1.2229e+00)	Acc@1  47.00 ( 56.57)	Acc@5  91.00 ( 94.43)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 1.2279e+00 (1.2242e+00)	Acc@1  56.00 ( 56.44)	Acc@5  94.00 ( 94.39)
Test: [ 70/100]	Time  0.027 ( 0.028)	Loss 1.2840e+00 (1.2230e+00)	Acc@1  55.00 ( 56.37)	Acc@5  96.00 ( 94.48)
Test: [ 80/100]	Time  0.024 ( 0.028)	Loss 1.1590e+00 (1.2230e+00)	Acc@1  54.00 ( 56.30)	Acc@5  98.00 ( 94.57)
Test: [ 90/100]	Time  0.027 ( 0.028)	Loss 1.1963e+00 (1.2269e+00)	Acc@1  51.00 ( 56.09)	Acc@5  99.00 ( 94.57)
 * Acc@1 56.220 Acc@5 94.630
### epoch[1] execution time: 28.192052364349365
EPOCH 2
# current learning rate: 0.1
# Switched to train mode...
Epoch: [2][  0/391]	Time  0.256 ( 0.256)	Data  0.191 ( 0.191)	Loss 1.2506e+00 (1.2506e+00)	Acc@1  51.56 ( 51.56)	Acc@5  92.97 ( 92.97)
Epoch: [2][ 10/391]	Time  0.064 ( 0.082)	Data  0.001 ( 0.018)	Loss 1.0672e+00 (1.1246e+00)	Acc@1  61.72 ( 58.74)	Acc@5  97.66 ( 95.95)
Epoch: [2][ 20/391]	Time  0.061 ( 0.074)	Data  0.001 ( 0.010)	Loss 1.1612e+00 (1.1375e+00)	Acc@1  63.28 ( 59.56)	Acc@5  94.53 ( 95.28)
Epoch: [2][ 30/391]	Time  0.064 ( 0.071)	Data  0.001 ( 0.007)	Loss 1.2481e+00 (1.1357e+00)	Acc@1  57.81 ( 59.53)	Acc@5  94.53 ( 95.36)
Epoch: [2][ 40/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.006)	Loss 1.0965e+00 (1.1436e+00)	Acc@1  62.50 ( 59.07)	Acc@5  96.88 ( 95.45)
Epoch: [2][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.005)	Loss 1.2464e+00 (1.1319e+00)	Acc@1  57.03 ( 59.47)	Acc@5  96.09 ( 95.66)
Epoch: [2][ 60/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.1754e+00 (1.1303e+00)	Acc@1  58.59 ( 59.46)	Acc@5  94.53 ( 95.57)
Epoch: [2][ 70/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.1613e+00 (1.1239e+00)	Acc@1  54.69 ( 59.67)	Acc@5  97.66 ( 95.60)
Epoch: [2][ 80/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.1493e+00 (1.1239e+00)	Acc@1  63.28 ( 59.77)	Acc@5  94.53 ( 95.60)
Epoch: [2][ 90/391]	Time  0.065 ( 0.066)	Data  0.002 ( 0.003)	Loss 1.0890e+00 (1.1252e+00)	Acc@1  61.72 ( 59.66)	Acc@5  94.53 ( 95.53)
Epoch: [2][100/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.2523e+00 (1.1244e+00)	Acc@1  60.94 ( 59.75)	Acc@5  91.41 ( 95.49)
Epoch: [2][110/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.003)	Loss 8.8295e-01 (1.1174e+00)	Acc@1  71.09 ( 60.12)	Acc@5  97.66 ( 95.58)
Epoch: [2][120/391]	Time  0.060 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.2283e+00 (1.1184e+00)	Acc@1  58.59 ( 60.18)	Acc@5  91.41 ( 95.59)
Epoch: [2][130/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.0755e+00 (1.1176e+00)	Acc@1  57.03 ( 60.24)	Acc@5  96.09 ( 95.59)
Epoch: [2][140/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2900e+00 (1.1177e+00)	Acc@1  55.47 ( 60.21)	Acc@5  93.75 ( 95.65)
Epoch: [2][150/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0945e+00 (1.1176e+00)	Acc@1  59.38 ( 60.25)	Acc@5  96.88 ( 95.65)
Epoch: [2][160/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0178e+00 (1.1141e+00)	Acc@1  61.72 ( 60.36)	Acc@5  96.09 ( 95.69)
Epoch: [2][170/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1613e+00 (1.1128e+00)	Acc@1  57.81 ( 60.41)	Acc@5  92.19 ( 95.69)
Epoch: [2][180/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0547e+00 (1.1110e+00)	Acc@1  64.84 ( 60.48)	Acc@5  97.66 ( 95.70)
Epoch: [2][190/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1612e+00 (1.1106e+00)	Acc@1  53.91 ( 60.44)	Acc@5  98.44 ( 95.65)
Epoch: [2][200/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0274e+00 (1.1071e+00)	Acc@1  63.28 ( 60.59)	Acc@5  96.09 ( 95.66)
Epoch: [2][210/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0988e+00 (1.1030e+00)	Acc@1  59.38 ( 60.72)	Acc@5  95.31 ( 95.72)
Epoch: [2][220/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0281e+00 (1.1008e+00)	Acc@1  62.50 ( 60.76)	Acc@5  98.44 ( 95.75)
Epoch: [2][230/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0837e+00 (1.0986e+00)	Acc@1  60.94 ( 60.83)	Acc@5  96.88 ( 95.78)
Epoch: [2][240/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.8311e-01 (1.0947e+00)	Acc@1  66.41 ( 61.00)	Acc@5  96.88 ( 95.82)
Epoch: [2][250/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.7917e-01 (1.0908e+00)	Acc@1  64.84 ( 61.11)	Acc@5  96.88 ( 95.88)
Epoch: [2][260/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0553e+00 (1.0890e+00)	Acc@1  64.06 ( 61.11)	Acc@5  93.75 ( 95.92)
Epoch: [2][270/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.8061e-01 (1.0852e+00)	Acc@1  66.41 ( 61.32)	Acc@5  94.53 ( 95.92)
Epoch: [2][280/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0444e+00 (1.0846e+00)	Acc@1  60.94 ( 61.31)	Acc@5  96.09 ( 95.93)
Epoch: [2][290/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.6136e-01 (1.0840e+00)	Acc@1  66.41 ( 61.36)	Acc@5  97.66 ( 95.95)
Epoch: [2][300/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.1946e-01 (1.0832e+00)	Acc@1  67.19 ( 61.40)	Acc@5  96.09 ( 95.97)
Epoch: [2][310/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.2951e-01 (1.0802e+00)	Acc@1  67.97 ( 61.50)	Acc@5  96.09 ( 95.99)
Epoch: [2][320/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1217e+00 (1.0797e+00)	Acc@1  58.59 ( 61.53)	Acc@5  95.31 ( 95.98)
Epoch: [2][330/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0473e+00 (1.0766e+00)	Acc@1  65.62 ( 61.63)	Acc@5  96.09 ( 96.01)
Epoch: [2][340/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0321e+00 (1.0735e+00)	Acc@1  66.41 ( 61.77)	Acc@5  95.31 ( 96.02)
Epoch: [2][350/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.8953e-01 (1.0714e+00)	Acc@1  60.94 ( 61.85)	Acc@5  99.22 ( 96.04)
Epoch: [2][360/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.7778e-01 (1.0693e+00)	Acc@1  67.19 ( 61.91)	Acc@5  98.44 ( 96.05)
Epoch: [2][370/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.5597e-01 (1.0661e+00)	Acc@1  75.00 ( 62.04)	Acc@5  98.44 ( 96.06)
Epoch: [2][380/391]	Time  0.070 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1011e+00 (1.0663e+00)	Acc@1  59.38 ( 62.08)	Acc@5  96.09 ( 96.04)
Epoch: [2][390/391]	Time  0.046 ( 0.064)	Data  0.001 ( 0.002)	Loss 8.9765e-01 (1.0650e+00)	Acc@1  68.75 ( 62.11)	Acc@5  97.50 ( 96.05)
## e[2] optimizer.zero_grad (sum) time: 0.4036242961883545
## e[2]       loss.backward (sum) time: 6.94497013092041
## e[2]      optimizer.step (sum) time: 3.364854097366333
## epoch[2] training(only) time: 25.294406414031982
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 1.0901e+00 (1.0901e+00)	Acc@1  63.00 ( 63.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.024 ( 0.041)	Loss 1.2360e+00 (1.1635e+00)	Acc@1  61.00 ( 57.73)	Acc@5  94.00 ( 95.82)
Test: [ 20/100]	Time  0.027 ( 0.034)	Loss 1.2068e+00 (1.1760e+00)	Acc@1  57.00 ( 57.52)	Acc@5  97.00 ( 95.57)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 1.1811e+00 (1.1993e+00)	Acc@1  58.00 ( 56.71)	Acc@5  97.00 ( 95.26)
Test: [ 40/100]	Time  0.024 ( 0.030)	Loss 1.0209e+00 (1.1988e+00)	Acc@1  67.00 ( 57.00)	Acc@5  94.00 ( 95.17)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 1.1840e+00 (1.1977e+00)	Acc@1  56.00 ( 57.39)	Acc@5  94.00 ( 95.12)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 1.1746e+00 (1.1957e+00)	Acc@1  56.00 ( 57.61)	Acc@5  96.00 ( 95.30)
Test: [ 70/100]	Time  0.027 ( 0.028)	Loss 1.1019e+00 (1.2020e+00)	Acc@1  60.00 ( 57.35)	Acc@5  94.00 ( 95.15)
Test: [ 80/100]	Time  0.028 ( 0.028)	Loss 1.2555e+00 (1.2056e+00)	Acc@1  59.00 ( 57.20)	Acc@5  95.00 ( 95.14)
Test: [ 90/100]	Time  0.024 ( 0.028)	Loss 1.2687e+00 (1.2087e+00)	Acc@1  53.00 ( 57.30)	Acc@5  99.00 ( 95.08)
 * Acc@1 57.410 Acc@5 95.110
### epoch[2] execution time: 28.140383005142212
EPOCH 3
# current learning rate: 0.1
# Switched to train mode...
Epoch: [3][  0/391]	Time  0.245 ( 0.245)	Data  0.180 ( 0.180)	Loss 9.1858e-01 (9.1858e-01)	Acc@1  67.19 ( 67.19)	Acc@5  98.44 ( 98.44)
Epoch: [3][ 10/391]	Time  0.063 ( 0.081)	Data  0.001 ( 0.017)	Loss 1.0280e+00 (9.6536e-01)	Acc@1  64.06 ( 64.91)	Acc@5  96.09 ( 96.80)
Epoch: [3][ 20/391]	Time  0.062 ( 0.073)	Data  0.001 ( 0.010)	Loss 9.8873e-01 (9.7599e-01)	Acc@1  65.62 ( 65.55)	Acc@5  94.53 ( 96.65)
Epoch: [3][ 30/391]	Time  0.063 ( 0.070)	Data  0.001 ( 0.007)	Loss 9.4157e-01 (9.7048e-01)	Acc@1  67.19 ( 65.73)	Acc@5  98.44 ( 96.80)
Epoch: [3][ 40/391]	Time  0.061 ( 0.068)	Data  0.001 ( 0.005)	Loss 9.6940e-01 (9.7016e-01)	Acc@1  64.84 ( 65.64)	Acc@5  96.09 ( 96.80)
Epoch: [3][ 50/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.005)	Loss 1.0772e+00 (9.8323e-01)	Acc@1  61.72 ( 65.10)	Acc@5  97.66 ( 96.58)
Epoch: [3][ 60/391]	Time  0.059 ( 0.066)	Data  0.001 ( 0.004)	Loss 9.0560e-01 (9.8244e-01)	Acc@1  71.88 ( 65.20)	Acc@5 100.00 ( 96.72)
Epoch: [3][ 70/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.004)	Loss 9.5967e-01 (9.7582e-01)	Acc@1  67.97 ( 65.72)	Acc@5  96.88 ( 96.64)
Epoch: [3][ 80/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 6.5169e-01 (9.7566e-01)	Acc@1  78.91 ( 65.66)	Acc@5  99.22 ( 96.72)
Epoch: [3][ 90/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 8.7061e-01 (9.7297e-01)	Acc@1  64.84 ( 65.67)	Acc@5  98.44 ( 96.75)
Epoch: [3][100/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.003)	Loss 8.0563e-01 (9.6713e-01)	Acc@1  71.88 ( 65.98)	Acc@5  99.22 ( 96.80)
Epoch: [3][110/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.003)	Loss 9.5513e-01 (9.6207e-01)	Acc@1  63.28 ( 66.16)	Acc@5  96.88 ( 96.78)
Epoch: [3][120/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.003)	Loss 9.1759e-01 (9.5845e-01)	Acc@1  69.53 ( 66.35)	Acc@5  96.88 ( 96.81)
Epoch: [3][130/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0637e+00 (9.5630e-01)	Acc@1  64.84 ( 66.40)	Acc@5  96.09 ( 96.81)
Epoch: [3][140/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.3782e-01 (9.5878e-01)	Acc@1  69.53 ( 66.47)	Acc@5  95.31 ( 96.71)
Epoch: [3][150/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.5291e-01 (9.5807e-01)	Acc@1  75.00 ( 66.53)	Acc@5  98.44 ( 96.71)
Epoch: [3][160/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.3881e-01 (9.5893e-01)	Acc@1  67.19 ( 66.45)	Acc@5  99.22 ( 96.71)
Epoch: [3][170/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.6336e-01 (9.5629e-01)	Acc@1  68.75 ( 66.50)	Acc@5  98.44 ( 96.76)
Epoch: [3][180/391]	Time  0.070 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.1806e-01 (9.5288e-01)	Acc@1  74.22 ( 66.62)	Acc@5  99.22 ( 96.78)
Epoch: [3][190/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.8652e-01 (9.4924e-01)	Acc@1  70.31 ( 66.64)	Acc@5  98.44 ( 96.79)
Epoch: [3][200/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.9816e-01 (9.5002e-01)	Acc@1  66.41 ( 66.58)	Acc@5  95.31 ( 96.81)
Epoch: [3][210/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.4523e-01 (9.4674e-01)	Acc@1  67.19 ( 66.70)	Acc@5  98.44 ( 96.84)
Epoch: [3][220/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.6614e-01 (9.4406e-01)	Acc@1  68.75 ( 66.77)	Acc@5  99.22 ( 96.87)
Epoch: [3][230/391]	Time  0.066 ( 0.065)	Data  0.003 ( 0.002)	Loss 9.6575e-01 (9.4540e-01)	Acc@1  66.41 ( 66.69)	Acc@5  96.88 ( 96.85)
Epoch: [3][240/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.6786e-01 (9.4281e-01)	Acc@1  67.97 ( 66.75)	Acc@5  98.44 ( 96.89)
Epoch: [3][250/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.4339e-01 (9.4269e-01)	Acc@1  71.09 ( 66.75)	Acc@5  96.88 ( 96.88)
Epoch: [3][260/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0023e+00 (9.4330e-01)	Acc@1  63.28 ( 66.70)	Acc@5  95.31 ( 96.90)
Epoch: [3][270/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.2440e-01 (9.4240e-01)	Acc@1  66.41 ( 66.75)	Acc@5  99.22 ( 96.92)
Epoch: [3][280/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.0825e-01 (9.4049e-01)	Acc@1  65.62 ( 66.77)	Acc@5  99.22 ( 96.97)
Epoch: [3][290/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.7042e-01 (9.4148e-01)	Acc@1  63.28 ( 66.73)	Acc@5  98.44 ( 96.97)
Epoch: [3][300/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.9576e-01 (9.4196e-01)	Acc@1  70.31 ( 66.73)	Acc@5  97.66 ( 96.95)
Epoch: [3][310/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.5739e-01 (9.4004e-01)	Acc@1  68.75 ( 66.78)	Acc@5  97.66 ( 96.99)
Epoch: [3][320/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.5402e-01 (9.3856e-01)	Acc@1  67.97 ( 66.85)	Acc@5  97.66 ( 97.00)
Epoch: [3][330/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.7982e-01 (9.3678e-01)	Acc@1  69.53 ( 66.95)	Acc@5  96.09 ( 97.00)
Epoch: [3][340/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.5243e-01 (9.3709e-01)	Acc@1  67.19 ( 66.99)	Acc@5  97.66 ( 97.00)
Epoch: [3][350/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.5704e-01 (9.3521e-01)	Acc@1  67.19 ( 67.06)	Acc@5  94.53 ( 97.00)
Epoch: [3][360/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 8.6893e-01 (9.3447e-01)	Acc@1  72.66 ( 67.14)	Acc@5  97.66 ( 97.01)
Epoch: [3][370/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 8.5590e-01 (9.3294e-01)	Acc@1  71.88 ( 67.23)	Acc@5  98.44 ( 97.02)
Epoch: [3][380/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 8.5439e-01 (9.3045e-01)	Acc@1  70.31 ( 67.32)	Acc@5  99.22 ( 97.05)
Epoch: [3][390/391]	Time  0.046 ( 0.064)	Data  0.001 ( 0.002)	Loss 9.5069e-01 (9.2924e-01)	Acc@1  70.00 ( 67.39)	Acc@5  97.50 ( 97.05)
## e[3] optimizer.zero_grad (sum) time: 0.40371274948120117
## e[3]       loss.backward (sum) time: 6.956078290939331
## e[3]      optimizer.step (sum) time: 3.3916287422180176
## epoch[3] training(only) time: 25.323485612869263
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 8.1965e-01 (8.1965e-01)	Acc@1  69.00 ( 69.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.041)	Loss 8.8621e-01 (8.4679e-01)	Acc@1  65.00 ( 70.09)	Acc@5  96.00 ( 98.00)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 8.3074e-01 (8.5480e-01)	Acc@1  70.00 ( 69.67)	Acc@5  98.00 ( 98.10)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 9.0957e-01 (8.6468e-01)	Acc@1  69.00 ( 69.35)	Acc@5  96.00 ( 97.81)
Test: [ 40/100]	Time  0.024 ( 0.030)	Loss 8.6572e-01 (8.6951e-01)	Acc@1  71.00 ( 69.17)	Acc@5  95.00 ( 97.54)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 8.2182e-01 (8.6522e-01)	Acc@1  68.00 ( 69.45)	Acc@5  96.00 ( 97.57)
Test: [ 60/100]	Time  0.032 ( 0.028)	Loss 8.6525e-01 (8.7147e-01)	Acc@1  65.00 ( 69.31)	Acc@5  98.00 ( 97.64)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 8.6542e-01 (8.7130e-01)	Acc@1  69.00 ( 69.38)	Acc@5  97.00 ( 97.70)
Test: [ 80/100]	Time  0.028 ( 0.028)	Loss 7.9147e-01 (8.6664e-01)	Acc@1  67.00 ( 69.42)	Acc@5  99.00 ( 97.73)
Test: [ 90/100]	Time  0.028 ( 0.028)	Loss 7.6620e-01 (8.6951e-01)	Acc@1  73.00 ( 69.27)	Acc@5 100.00 ( 97.74)
 * Acc@1 69.410 Acc@5 97.760
### epoch[3] execution time: 28.22427010536194
EPOCH 4
# current learning rate: 0.1
# Switched to train mode...
Epoch: [4][  0/391]	Time  0.263 ( 0.263)	Data  0.197 ( 0.197)	Loss 8.4579e-01 (8.4579e-01)	Acc@1  70.31 ( 70.31)	Acc@5  97.66 ( 97.66)
Epoch: [4][ 10/391]	Time  0.063 ( 0.082)	Data  0.001 ( 0.019)	Loss 9.6476e-01 (8.7874e-01)	Acc@1  62.50 ( 68.61)	Acc@5  97.66 ( 97.23)
Epoch: [4][ 20/391]	Time  0.063 ( 0.074)	Data  0.001 ( 0.011)	Loss 8.1933e-01 (8.6775e-01)	Acc@1  74.22 ( 68.97)	Acc@5  98.44 ( 97.43)
Epoch: [4][ 30/391]	Time  0.067 ( 0.071)	Data  0.001 ( 0.008)	Loss 7.1800e-01 (8.5633e-01)	Acc@1  72.66 ( 69.58)	Acc@5  98.44 ( 97.53)
Epoch: [4][ 40/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.006)	Loss 1.0859e+00 (8.5170e-01)	Acc@1  59.38 ( 69.47)	Acc@5  96.88 ( 97.62)
Epoch: [4][ 50/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.005)	Loss 9.0895e-01 (8.5848e-01)	Acc@1  67.97 ( 69.27)	Acc@5  96.88 ( 97.56)
Epoch: [4][ 60/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 8.6106e-01 (8.5904e-01)	Acc@1  68.75 ( 69.45)	Acc@5  97.66 ( 97.53)
Epoch: [4][ 70/391]	Time  0.059 ( 0.067)	Data  0.001 ( 0.004)	Loss 9.3079e-01 (8.5937e-01)	Acc@1  70.31 ( 69.59)	Acc@5  92.97 ( 97.45)
Epoch: [4][ 80/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.004)	Loss 7.6162e-01 (8.5831e-01)	Acc@1  70.31 ( 69.63)	Acc@5 100.00 ( 97.53)
Epoch: [4][ 90/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 8.7960e-01 (8.5191e-01)	Acc@1  71.09 ( 69.88)	Acc@5  96.09 ( 97.60)
Epoch: [4][100/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.003)	Loss 7.1875e-01 (8.4357e-01)	Acc@1  76.56 ( 70.19)	Acc@5  98.44 ( 97.66)
Epoch: [4][110/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 8.2052e-01 (8.4264e-01)	Acc@1  71.88 ( 70.19)	Acc@5  97.66 ( 97.65)
Epoch: [4][120/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 8.7176e-01 (8.4425e-01)	Acc@1  66.41 ( 70.08)	Acc@5  97.66 ( 97.60)
Epoch: [4][130/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.003)	Loss 7.8102e-01 (8.4637e-01)	Acc@1  69.53 ( 70.06)	Acc@5  96.88 ( 97.56)
Epoch: [4][140/391]	Time  0.070 ( 0.066)	Data  0.001 ( 0.003)	Loss 8.7347e-01 (8.4555e-01)	Acc@1  73.44 ( 70.16)	Acc@5  97.66 ( 97.61)
Epoch: [4][150/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.8757e-01 (8.4457e-01)	Acc@1  67.19 ( 70.17)	Acc@5  98.44 ( 97.62)
Epoch: [4][160/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.9254e-01 (8.4462e-01)	Acc@1  71.88 ( 70.17)	Acc@5  96.88 ( 97.63)
Epoch: [4][170/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.9850e-01 (8.4246e-01)	Acc@1  67.19 ( 70.23)	Acc@5  96.09 ( 97.63)
Epoch: [4][180/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.0814e-01 (8.3860e-01)	Acc@1  76.56 ( 70.36)	Acc@5  97.66 ( 97.65)
Epoch: [4][190/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.8468e-01 (8.3771e-01)	Acc@1  76.56 ( 70.33)	Acc@5  99.22 ( 97.68)
Epoch: [4][200/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.8602e-01 (8.3866e-01)	Acc@1  69.53 ( 70.30)	Acc@5  98.44 ( 97.69)
Epoch: [4][210/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.6273e-01 (8.3615e-01)	Acc@1  67.19 ( 70.43)	Acc@5  98.44 ( 97.73)
Epoch: [4][220/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0312e+00 (8.3561e-01)	Acc@1  62.50 ( 70.46)	Acc@5  98.44 ( 97.72)
Epoch: [4][230/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.5424e-01 (8.3568e-01)	Acc@1  73.44 ( 70.47)	Acc@5  97.66 ( 97.73)
Epoch: [4][240/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0299e+00 (8.3531e-01)	Acc@1  67.19 ( 70.52)	Acc@5  94.53 ( 97.72)
Epoch: [4][250/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0027e+00 (8.3290e-01)	Acc@1  62.50 ( 70.62)	Acc@5  96.88 ( 97.72)
Epoch: [4][260/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.1280e-01 (8.3280e-01)	Acc@1  72.66 ( 70.65)	Acc@5  97.66 ( 97.73)
Epoch: [4][270/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.6857e-01 (8.3160e-01)	Acc@1  75.00 ( 70.73)	Acc@5  96.88 ( 97.75)
Epoch: [4][280/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.7287e-01 (8.2971e-01)	Acc@1  72.66 ( 70.80)	Acc@5  98.44 ( 97.78)
Epoch: [4][290/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.9816e-01 (8.2912e-01)	Acc@1  72.66 ( 70.90)	Acc@5  96.88 ( 97.77)
Epoch: [4][300/391]	Time  0.058 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.1772e-01 (8.2767e-01)	Acc@1  71.09 ( 70.94)	Acc@5  98.44 ( 97.78)
Epoch: [4][310/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.2245e-01 (8.2662e-01)	Acc@1  70.31 ( 70.96)	Acc@5  97.66 ( 97.78)
Epoch: [4][320/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.1340e-01 (8.2430e-01)	Acc@1  74.22 ( 71.06)	Acc@5  96.88 ( 97.79)
Epoch: [4][330/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.8891e-01 (8.2245e-01)	Acc@1  71.09 ( 71.14)	Acc@5  99.22 ( 97.79)
Epoch: [4][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.7139e-01 (8.2079e-01)	Acc@1  78.12 ( 71.16)	Acc@5  99.22 ( 97.82)
Epoch: [4][350/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.4435e-01 (8.1993e-01)	Acc@1  71.88 ( 71.18)	Acc@5  96.88 ( 97.82)
Epoch: [4][360/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.2986e-01 (8.1890e-01)	Acc@1  77.34 ( 71.23)	Acc@5  96.88 ( 97.83)
Epoch: [4][370/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.7083e-01 (8.1914e-01)	Acc@1  78.12 ( 71.23)	Acc@5  98.44 ( 97.84)
Epoch: [4][380/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 8.4164e-01 (8.1686e-01)	Acc@1  70.31 ( 71.33)	Acc@5  97.66 ( 97.86)
Epoch: [4][390/391]	Time  0.046 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.0915e+00 (8.1591e-01)	Acc@1  60.00 ( 71.37)	Acc@5  96.25 ( 97.87)
## e[4] optimizer.zero_grad (sum) time: 0.4068264961242676
## e[4]       loss.backward (sum) time: 6.948453903198242
## e[4]      optimizer.step (sum) time: 3.3434407711029053
## epoch[4] training(only) time: 25.279821395874023
# Switched to evaluate mode...
Test: [  0/100]	Time  0.196 ( 0.196)	Loss 8.2456e-01 (8.2456e-01)	Acc@1  77.00 ( 77.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.024 ( 0.041)	Loss 7.9134e-01 (8.5703e-01)	Acc@1  75.00 ( 70.91)	Acc@5  97.00 ( 98.27)
Test: [ 20/100]	Time  0.024 ( 0.034)	Loss 7.5461e-01 (8.7971e-01)	Acc@1  70.00 ( 69.52)	Acc@5  98.00 ( 98.10)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 8.3196e-01 (8.9026e-01)	Acc@1  74.00 ( 69.61)	Acc@5  99.00 ( 97.97)
Test: [ 40/100]	Time  0.029 ( 0.030)	Loss 8.7389e-01 (8.9665e-01)	Acc@1  72.00 ( 69.66)	Acc@5  98.00 ( 97.78)
Test: [ 50/100]	Time  0.028 ( 0.029)	Loss 9.0126e-01 (8.9391e-01)	Acc@1  68.00 ( 69.96)	Acc@5  98.00 ( 97.84)
Test: [ 60/100]	Time  0.024 ( 0.029)	Loss 8.6381e-01 (9.0013e-01)	Acc@1  69.00 ( 69.64)	Acc@5 100.00 ( 97.95)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 9.4931e-01 (8.9543e-01)	Acc@1  68.00 ( 69.55)	Acc@5  97.00 ( 97.87)
Test: [ 80/100]	Time  0.024 ( 0.028)	Loss 8.9799e-01 (8.8982e-01)	Acc@1  73.00 ( 69.62)	Acc@5  96.00 ( 97.88)
Test: [ 90/100]	Time  0.026 ( 0.028)	Loss 8.9486e-01 (8.9517e-01)	Acc@1  71.00 ( 69.55)	Acc@5  98.00 ( 97.82)
 * Acc@1 69.660 Acc@5 97.900
### epoch[4] execution time: 28.145963191986084
EPOCH 5
# current learning rate: 0.1
# Switched to train mode...
Epoch: [5][  0/391]	Time  0.248 ( 0.248)	Data  0.180 ( 0.180)	Loss 7.3753e-01 (7.3753e-01)	Acc@1  75.00 ( 75.00)	Acc@5  99.22 ( 99.22)
Epoch: [5][ 10/391]	Time  0.064 ( 0.081)	Data  0.001 ( 0.017)	Loss 7.4875e-01 (7.6410e-01)	Acc@1  73.44 ( 72.66)	Acc@5  96.88 ( 97.94)
Epoch: [5][ 20/391]	Time  0.059 ( 0.073)	Data  0.001 ( 0.010)	Loss 6.9421e-01 (7.7904e-01)	Acc@1  74.22 ( 72.92)	Acc@5  98.44 ( 98.18)
Epoch: [5][ 30/391]	Time  0.065 ( 0.070)	Data  0.001 ( 0.007)	Loss 9.7317e-01 (7.8375e-01)	Acc@1  67.97 ( 72.73)	Acc@5  93.75 ( 98.01)
Epoch: [5][ 40/391]	Time  0.060 ( 0.068)	Data  0.001 ( 0.005)	Loss 7.9201e-01 (7.8160e-01)	Acc@1  66.41 ( 72.52)	Acc@5  98.44 ( 98.11)
Epoch: [5][ 50/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.005)	Loss 6.8886e-01 (7.7615e-01)	Acc@1  75.00 ( 72.78)	Acc@5  98.44 ( 98.09)
Epoch: [5][ 60/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 7.8427e-01 (7.7573e-01)	Acc@1  71.09 ( 72.93)	Acc@5  98.44 ( 98.00)
Epoch: [5][ 70/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.004)	Loss 6.0619e-01 (7.7349e-01)	Acc@1  79.69 ( 73.11)	Acc@5  98.44 ( 98.05)
Epoch: [5][ 80/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 6.7855e-01 (7.6521e-01)	Acc@1  74.22 ( 73.28)	Acc@5  99.22 ( 98.10)
Epoch: [5][ 90/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.003)	Loss 8.3535e-01 (7.5786e-01)	Acc@1  74.22 ( 73.63)	Acc@5  97.66 ( 98.14)
Epoch: [5][100/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 6.9227e-01 (7.5889e-01)	Acc@1  74.22 ( 73.54)	Acc@5  97.66 ( 98.12)
Epoch: [5][110/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 8.2463e-01 (7.5564e-01)	Acc@1  68.75 ( 73.70)	Acc@5  98.44 ( 98.17)
Epoch: [5][120/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 6.6741e-01 (7.5182e-01)	Acc@1  80.47 ( 73.82)	Acc@5  99.22 ( 98.21)
Epoch: [5][130/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.5523e-01 (7.5300e-01)	Acc@1  75.00 ( 73.72)	Acc@5  98.44 ( 98.20)
Epoch: [5][140/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.8340e-01 (7.5073e-01)	Acc@1  72.66 ( 73.74)	Acc@5  99.22 ( 98.23)
Epoch: [5][150/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.5866e-01 (7.5649e-01)	Acc@1  69.53 ( 73.54)	Acc@5  98.44 ( 98.20)
Epoch: [5][160/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.4775e-01 (7.5532e-01)	Acc@1  75.00 ( 73.62)	Acc@5  97.66 ( 98.19)
Epoch: [5][170/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.5410e-01 (7.5400e-01)	Acc@1  69.53 ( 73.64)	Acc@5  97.66 ( 98.18)
Epoch: [5][180/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.5129e-01 (7.5295e-01)	Acc@1  75.78 ( 73.76)	Acc@5  98.44 ( 98.14)
Epoch: [5][190/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.4125e-01 (7.4955e-01)	Acc@1  76.56 ( 73.83)	Acc@5  97.66 ( 98.14)
Epoch: [5][200/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.7527e-01 (7.4613e-01)	Acc@1  81.25 ( 73.96)	Acc@5  99.22 ( 98.16)
Epoch: [5][210/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.8399e-01 (7.4562e-01)	Acc@1  72.66 ( 73.92)	Acc@5 100.00 ( 98.16)
Epoch: [5][220/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.2888e-01 (7.4636e-01)	Acc@1  82.81 ( 73.90)	Acc@5 100.00 ( 98.17)
Epoch: [5][230/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.7304e-01 (7.4492e-01)	Acc@1  75.00 ( 73.93)	Acc@5  99.22 ( 98.20)
Epoch: [5][240/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.9234e-01 (7.4619e-01)	Acc@1  67.97 ( 73.92)	Acc@5  99.22 ( 98.19)
Epoch: [5][250/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.9345e-01 (7.4674e-01)	Acc@1  70.31 ( 73.93)	Acc@5  96.09 ( 98.18)
Epoch: [5][260/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.4888e-01 (7.4650e-01)	Acc@1  74.22 ( 73.98)	Acc@5  97.66 ( 98.17)
Epoch: [5][270/391]	Time  0.067 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.3204e-01 (7.4477e-01)	Acc@1  75.78 ( 74.03)	Acc@5  99.22 ( 98.16)
Epoch: [5][280/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.5766e-01 (7.4508e-01)	Acc@1  70.31 ( 74.03)	Acc@5  97.66 ( 98.18)
Epoch: [5][290/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.4553e-01 (7.4300e-01)	Acc@1  81.25 ( 74.14)	Acc@5  97.66 ( 98.19)
Epoch: [5][300/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.2281e-01 (7.4266e-01)	Acc@1  77.34 ( 74.19)	Acc@5  98.44 ( 98.19)
Epoch: [5][310/391]	Time  0.067 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.4053e-01 (7.4295e-01)	Acc@1  73.44 ( 74.18)	Acc@5  99.22 ( 98.19)
Epoch: [5][320/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.5473e-01 (7.4245e-01)	Acc@1  80.47 ( 74.21)	Acc@5  99.22 ( 98.20)
Epoch: [5][330/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.7431e-01 (7.4090e-01)	Acc@1  78.91 ( 74.28)	Acc@5  97.66 ( 98.20)
Epoch: [5][340/391]	Time  0.059 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.0965e-01 (7.4093e-01)	Acc@1  76.56 ( 74.27)	Acc@5  98.44 ( 98.22)
Epoch: [5][350/391]	Time  0.060 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.0424e-01 (7.4230e-01)	Acc@1  77.34 ( 74.23)	Acc@5  96.88 ( 98.20)
Epoch: [5][360/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.6382e-01 (7.4085e-01)	Acc@1  71.88 ( 74.29)	Acc@5  98.44 ( 98.22)
Epoch: [5][370/391]	Time  0.068 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.7627e-01 (7.4024e-01)	Acc@1  77.34 ( 74.27)	Acc@5  98.44 ( 98.22)
Epoch: [5][380/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.8753e-01 (7.3885e-01)	Acc@1  72.66 ( 74.30)	Acc@5  96.88 ( 98.22)
Epoch: [5][390/391]	Time  0.045 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.8621e-01 (7.3894e-01)	Acc@1  72.50 ( 74.25)	Acc@5 100.00 ( 98.24)
## e[5] optimizer.zero_grad (sum) time: 0.40277838706970215
## e[5]       loss.backward (sum) time: 6.981062889099121
## e[5]      optimizer.step (sum) time: 3.357353448867798
## epoch[5] training(only) time: 25.262635707855225
# Switched to evaluate mode...
Test: [  0/100]	Time  0.196 ( 0.196)	Loss 8.4207e-01 (8.4207e-01)	Acc@1  75.00 ( 75.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.024 ( 0.041)	Loss 6.5894e-01 (7.0314e-01)	Acc@1  81.00 ( 75.82)	Acc@5  98.00 ( 98.82)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 5.9976e-01 (6.9600e-01)	Acc@1  78.00 ( 75.71)	Acc@5 100.00 ( 98.71)
Test: [ 30/100]	Time  0.024 ( 0.030)	Loss 7.3703e-01 (7.0248e-01)	Acc@1  76.00 ( 75.61)	Acc@5  98.00 ( 98.58)
Test: [ 40/100]	Time  0.027 ( 0.029)	Loss 6.4363e-01 (7.0534e-01)	Acc@1  80.00 ( 75.71)	Acc@5 100.00 ( 98.46)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 5.8881e-01 (7.0399e-01)	Acc@1  80.00 ( 76.14)	Acc@5  98.00 ( 98.41)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 7.4038e-01 (7.0981e-01)	Acc@1  73.00 ( 75.87)	Acc@5  98.00 ( 98.41)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 7.1855e-01 (7.0657e-01)	Acc@1  81.00 ( 75.89)	Acc@5  98.00 ( 98.46)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 6.0942e-01 (7.0257e-01)	Acc@1  81.00 ( 75.91)	Acc@5  98.00 ( 98.47)
Test: [ 90/100]	Time  0.028 ( 0.027)	Loss 6.2049e-01 (7.0333e-01)	Acc@1  76.00 ( 75.76)	Acc@5 100.00 ( 98.43)
 * Acc@1 75.680 Acc@5 98.460
### epoch[5] execution time: 28.094237565994263
EPOCH 6
# current learning rate: 0.1
# Switched to train mode...
Epoch: [6][  0/391]	Time  0.238 ( 0.238)	Data  0.175 ( 0.175)	Loss 6.2805e-01 (6.2805e-01)	Acc@1  75.00 ( 75.00)	Acc@5  98.44 ( 98.44)
Epoch: [6][ 10/391]	Time  0.062 ( 0.079)	Data  0.001 ( 0.017)	Loss 7.4494e-01 (6.6186e-01)	Acc@1  71.88 ( 76.92)	Acc@5  98.44 ( 98.51)
Epoch: [6][ 20/391]	Time  0.058 ( 0.071)	Data  0.001 ( 0.009)	Loss 1.0227e+00 (6.8382e-01)	Acc@1  70.31 ( 76.60)	Acc@5  94.53 ( 98.33)
Epoch: [6][ 30/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.007)	Loss 6.4409e-01 (6.9131e-01)	Acc@1  82.03 ( 76.31)	Acc@5  96.88 ( 98.24)
Epoch: [6][ 40/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.005)	Loss 7.6889e-01 (7.0358e-01)	Acc@1  75.00 ( 75.30)	Acc@5  99.22 ( 98.19)
Epoch: [6][ 50/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.004)	Loss 8.0208e-01 (7.0602e-01)	Acc@1  68.75 ( 75.17)	Acc@5  98.44 ( 98.27)
Epoch: [6][ 60/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.004)	Loss 6.7679e-01 (7.0246e-01)	Acc@1  76.56 ( 75.42)	Acc@5  98.44 ( 98.37)
Epoch: [6][ 70/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.004)	Loss 5.9922e-01 (6.9419e-01)	Acc@1  79.69 ( 75.69)	Acc@5  99.22 ( 98.39)
Epoch: [6][ 80/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 6.4696e-01 (6.8833e-01)	Acc@1  77.34 ( 75.95)	Acc@5  96.09 ( 98.41)
Epoch: [6][ 90/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 6.8308e-01 (6.9018e-01)	Acc@1  75.78 ( 75.94)	Acc@5 100.00 ( 98.41)
Epoch: [6][100/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 7.2066e-01 (6.9439e-01)	Acc@1  73.44 ( 75.74)	Acc@5  98.44 ( 98.39)
Epoch: [6][110/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.003)	Loss 7.0694e-01 (6.9419e-01)	Acc@1  75.00 ( 75.81)	Acc@5  97.66 ( 98.42)
Epoch: [6][120/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 8.2392e-01 (6.9599e-01)	Acc@1  75.00 ( 75.84)	Acc@5  96.09 ( 98.42)
Epoch: [6][130/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.3489e-01 (7.0005e-01)	Acc@1  71.09 ( 75.83)	Acc@5  97.66 ( 98.41)
Epoch: [6][140/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.5330e-01 (6.9770e-01)	Acc@1  73.44 ( 75.95)	Acc@5  99.22 ( 98.46)
Epoch: [6][150/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.2123e-01 (6.9836e-01)	Acc@1  74.22 ( 75.94)	Acc@5  96.88 ( 98.49)
Epoch: [6][160/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.7922e-01 (6.9723e-01)	Acc@1  81.25 ( 75.96)	Acc@5  99.22 ( 98.52)
Epoch: [6][170/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.8660e-01 (6.9713e-01)	Acc@1  77.34 ( 75.94)	Acc@5  98.44 ( 98.51)
Epoch: [6][180/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.6141e-01 (6.9804e-01)	Acc@1  73.44 ( 75.91)	Acc@5  98.44 ( 98.49)
Epoch: [6][190/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.8814e-01 (6.9944e-01)	Acc@1  75.00 ( 75.85)	Acc@5  99.22 ( 98.45)
Epoch: [6][200/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.6881e-01 (6.9496e-01)	Acc@1  82.81 ( 75.98)	Acc@5  99.22 ( 98.48)
Epoch: [6][210/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.7416e-01 (6.9407e-01)	Acc@1  79.69 ( 76.03)	Acc@5 100.00 ( 98.47)
Epoch: [6][220/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.1972e-01 (6.9436e-01)	Acc@1  73.44 ( 76.08)	Acc@5  96.88 ( 98.43)
Epoch: [6][230/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.3219e-01 (6.9278e-01)	Acc@1  81.25 ( 76.15)	Acc@5  98.44 ( 98.44)
Epoch: [6][240/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.9217e-01 (6.9197e-01)	Acc@1  75.78 ( 76.10)	Acc@5  97.66 ( 98.44)
Epoch: [6][250/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.4592e-01 (6.9238e-01)	Acc@1  75.78 ( 76.08)	Acc@5  96.09 ( 98.40)
Epoch: [6][260/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.1450e-01 (6.9060e-01)	Acc@1  71.88 ( 76.11)	Acc@5 100.00 ( 98.43)
Epoch: [6][270/391]	Time  0.068 ( 0.065)	Data  0.002 ( 0.002)	Loss 6.7384e-01 (6.8962e-01)	Acc@1  73.44 ( 76.12)	Acc@5  97.66 ( 98.43)
Epoch: [6][280/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.7169e-01 (6.8937e-01)	Acc@1  74.22 ( 76.10)	Acc@5  97.66 ( 98.44)
Epoch: [6][290/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.7958e-01 (6.8921e-01)	Acc@1  71.09 ( 76.12)	Acc@5  99.22 ( 98.44)
Epoch: [6][300/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.0420e-01 (6.8962e-01)	Acc@1  75.00 ( 76.08)	Acc@5  98.44 ( 98.45)
Epoch: [6][310/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.9548e-01 (6.8917e-01)	Acc@1  73.44 ( 76.11)	Acc@5 100.00 ( 98.45)
Epoch: [6][320/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.6875e-01 (6.9161e-01)	Acc@1  72.66 ( 76.03)	Acc@5  95.31 ( 98.44)
Epoch: [6][330/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.2519e-01 (6.9145e-01)	Acc@1  72.66 ( 76.00)	Acc@5  96.88 ( 98.44)
Epoch: [6][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.9337e-01 (6.9190e-01)	Acc@1  73.44 ( 76.02)	Acc@5  96.88 ( 98.44)
Epoch: [6][350/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.2907e-01 (6.9261e-01)	Acc@1  78.12 ( 75.96)	Acc@5  99.22 ( 98.44)
Epoch: [6][360/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.6084e-01 (6.9258e-01)	Acc@1  74.22 ( 75.98)	Acc@5  99.22 ( 98.42)
Epoch: [6][370/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.3255e-01 (6.9071e-01)	Acc@1  70.31 ( 76.01)	Acc@5 100.00 ( 98.44)
Epoch: [6][380/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.5843e-01 (6.8959e-01)	Acc@1  69.53 ( 76.07)	Acc@5  97.66 ( 98.45)
Epoch: [6][390/391]	Time  0.056 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.2568e-01 (6.8863e-01)	Acc@1  75.00 ( 76.11)	Acc@5  98.75 ( 98.45)
## e[6] optimizer.zero_grad (sum) time: 0.4101278781890869
## e[6]       loss.backward (sum) time: 6.96586012840271
## e[6]      optimizer.step (sum) time: 3.3371331691741943
## epoch[6] training(only) time: 25.279823780059814
# Switched to evaluate mode...
Test: [  0/100]	Time  0.196 ( 0.196)	Loss 7.1301e-01 (7.1301e-01)	Acc@1  74.00 ( 74.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.026 ( 0.041)	Loss 6.8843e-01 (6.7721e-01)	Acc@1  79.00 ( 76.91)	Acc@5  98.00 ( 98.45)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 7.8018e-01 (6.7236e-01)	Acc@1  75.00 ( 77.19)	Acc@5  98.00 ( 98.48)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 7.1418e-01 (6.8157e-01)	Acc@1  70.00 ( 76.90)	Acc@5  99.00 ( 98.42)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 5.9190e-01 (6.7514e-01)	Acc@1  83.00 ( 77.07)	Acc@5  97.00 ( 98.29)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 7.1799e-01 (6.7623e-01)	Acc@1  77.00 ( 77.14)	Acc@5  96.00 ( 98.29)
Test: [ 60/100]	Time  0.027 ( 0.028)	Loss 7.9640e-01 (6.8130e-01)	Acc@1  72.00 ( 77.02)	Acc@5  99.00 ( 98.36)
Test: [ 70/100]	Time  0.027 ( 0.028)	Loss 6.5597e-01 (6.7994e-01)	Acc@1  79.00 ( 76.90)	Acc@5  98.00 ( 98.46)
Test: [ 80/100]	Time  0.027 ( 0.028)	Loss 5.1485e-01 (6.7981e-01)	Acc@1  78.00 ( 76.90)	Acc@5  99.00 ( 98.48)
Test: [ 90/100]	Time  0.024 ( 0.028)	Loss 6.7578e-01 (6.8207e-01)	Acc@1  80.00 ( 77.02)	Acc@5  99.00 ( 98.46)
 * Acc@1 77.140 Acc@5 98.460
### epoch[6] execution time: 28.171640396118164
EPOCH 7
# current learning rate: 0.1
# Switched to train mode...
Epoch: [7][  0/391]	Time  0.264 ( 0.264)	Data  0.204 ( 0.204)	Loss 5.9624e-01 (5.9624e-01)	Acc@1  81.25 ( 81.25)	Acc@5  98.44 ( 98.44)
Epoch: [7][ 10/391]	Time  0.063 ( 0.082)	Data  0.001 ( 0.020)	Loss 4.8989e-01 (6.3665e-01)	Acc@1  81.25 ( 76.56)	Acc@5 100.00 ( 99.01)
Epoch: [7][ 20/391]	Time  0.066 ( 0.073)	Data  0.001 ( 0.011)	Loss 6.9964e-01 (6.3755e-01)	Acc@1  75.00 ( 77.27)	Acc@5 100.00 ( 98.88)
Epoch: [7][ 30/391]	Time  0.061 ( 0.070)	Data  0.001 ( 0.008)	Loss 5.2032e-01 (6.1943e-01)	Acc@1  84.38 ( 78.07)	Acc@5  97.66 ( 98.77)
Epoch: [7][ 40/391]	Time  0.063 ( 0.069)	Data  0.001 ( 0.006)	Loss 5.7363e-01 (6.3761e-01)	Acc@1  78.12 ( 77.40)	Acc@5 100.00 ( 98.82)
Epoch: [7][ 50/391]	Time  0.071 ( 0.068)	Data  0.001 ( 0.005)	Loss 6.1812e-01 (6.3508e-01)	Acc@1  77.34 ( 77.80)	Acc@5  99.22 ( 98.82)
Epoch: [7][ 60/391]	Time  0.062 ( 0.068)	Data  0.001 ( 0.004)	Loss 5.9479e-01 (6.3634e-01)	Acc@1  80.47 ( 77.78)	Acc@5  97.66 ( 98.78)
Epoch: [7][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 7.6117e-01 (6.4896e-01)	Acc@1  74.22 ( 77.54)	Acc@5  96.88 ( 98.71)
Epoch: [7][ 80/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.004)	Loss 4.9318e-01 (6.4180e-01)	Acc@1  81.25 ( 77.79)	Acc@5  99.22 ( 98.70)
Epoch: [7][ 90/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 9.2751e-01 (6.4124e-01)	Acc@1  73.44 ( 77.93)	Acc@5  94.53 ( 98.69)
Epoch: [7][100/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 8.3424e-01 (6.4177e-01)	Acc@1  74.22 ( 77.97)	Acc@5  97.66 ( 98.67)
Epoch: [7][110/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 6.3549e-01 (6.4595e-01)	Acc@1  75.00 ( 77.65)	Acc@5  98.44 ( 98.66)
Epoch: [7][120/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 7.4405e-01 (6.4442e-01)	Acc@1  75.00 ( 77.67)	Acc@5  99.22 ( 98.68)
Epoch: [7][130/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 7.1952e-01 (6.4845e-01)	Acc@1  73.44 ( 77.59)	Acc@5  98.44 ( 98.64)
Epoch: [7][140/391]	Time  0.071 ( 0.066)	Data  0.001 ( 0.003)	Loss 7.4865e-01 (6.5467e-01)	Acc@1  73.44 ( 77.27)	Acc@5  98.44 ( 98.61)
Epoch: [7][150/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.1722e-01 (6.5331e-01)	Acc@1  78.12 ( 77.35)	Acc@5  97.66 ( 98.60)
Epoch: [7][160/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.4403e-01 (6.5097e-01)	Acc@1  81.25 ( 77.46)	Acc@5  98.44 ( 98.60)
Epoch: [7][170/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.3346e-01 (6.5053e-01)	Acc@1  81.25 ( 77.44)	Acc@5  99.22 ( 98.62)
Epoch: [7][180/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.4061e-01 (6.5167e-01)	Acc@1  78.12 ( 77.42)	Acc@5  98.44 ( 98.58)
Epoch: [7][190/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.8592e-01 (6.5216e-01)	Acc@1  81.25 ( 77.46)	Acc@5  99.22 ( 98.61)
Epoch: [7][200/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.6108e-01 (6.4930e-01)	Acc@1  80.47 ( 77.58)	Acc@5  99.22 ( 98.61)
Epoch: [7][210/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.9060e-01 (6.4784e-01)	Acc@1  82.81 ( 77.67)	Acc@5  99.22 ( 98.61)
Epoch: [7][220/391]	Time  0.071 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.2077e-01 (6.4657e-01)	Acc@1  81.25 ( 77.68)	Acc@5  99.22 ( 98.63)
Epoch: [7][230/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.3047e-01 (6.4870e-01)	Acc@1  79.69 ( 77.63)	Acc@5  98.44 ( 98.61)
Epoch: [7][240/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.6141e-01 (6.4984e-01)	Acc@1  82.03 ( 77.62)	Acc@5 100.00 ( 98.62)
Epoch: [7][250/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.8642e-01 (6.5117e-01)	Acc@1  82.03 ( 77.59)	Acc@5  99.22 ( 98.61)
Epoch: [7][260/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.3880e-01 (6.5048e-01)	Acc@1  85.94 ( 77.65)	Acc@5  98.44 ( 98.61)
Epoch: [7][270/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.0514e-01 (6.4911e-01)	Acc@1  79.69 ( 77.72)	Acc@5  99.22 ( 98.62)
Epoch: [7][280/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.0175e-01 (6.5108e-01)	Acc@1  82.03 ( 77.64)	Acc@5  99.22 ( 98.61)
Epoch: [7][290/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.4994e-01 (6.5085e-01)	Acc@1  78.12 ( 77.64)	Acc@5  99.22 ( 98.61)
Epoch: [7][300/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.7573e-01 (6.5002e-01)	Acc@1  74.22 ( 77.70)	Acc@5  93.75 ( 98.61)
Epoch: [7][310/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.9875e-01 (6.5008e-01)	Acc@1  68.75 ( 77.69)	Acc@5  96.88 ( 98.60)
Epoch: [7][320/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.9384e-01 (6.5005e-01)	Acc@1  78.12 ( 77.68)	Acc@5  98.44 ( 98.61)
Epoch: [7][330/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.3469e-01 (6.4919e-01)	Acc@1  78.91 ( 77.73)	Acc@5  97.66 ( 98.61)
Epoch: [7][340/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.2465e-01 (6.4801e-01)	Acc@1  75.78 ( 77.77)	Acc@5  99.22 ( 98.61)
Epoch: [7][350/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.8714e-01 (6.4750e-01)	Acc@1  78.91 ( 77.80)	Acc@5 100.00 ( 98.61)
Epoch: [7][360/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.3393e-01 (6.4878e-01)	Acc@1  76.56 ( 77.75)	Acc@5  97.66 ( 98.60)
Epoch: [7][370/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.7885e-01 (6.4823e-01)	Acc@1  77.34 ( 77.80)	Acc@5  98.44 ( 98.60)
Epoch: [7][380/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.3445e-01 (6.4632e-01)	Acc@1  78.91 ( 77.86)	Acc@5 100.00 ( 98.61)
Epoch: [7][390/391]	Time  0.046 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.2895e-01 (6.4520e-01)	Acc@1  78.75 ( 77.90)	Acc@5  98.75 ( 98.61)
## e[7] optimizer.zero_grad (sum) time: 0.40693187713623047
## e[7]       loss.backward (sum) time: 6.9704365730285645
## e[7]      optimizer.step (sum) time: 3.3595855236053467
## epoch[7] training(only) time: 25.303584575653076
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 8.2435e-01 (8.2435e-01)	Acc@1  72.00 ( 72.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.040)	Loss 8.8433e-01 (7.4517e-01)	Acc@1  72.00 ( 74.55)	Acc@5  98.00 ( 98.64)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 7.5310e-01 (7.4542e-01)	Acc@1  72.00 ( 74.05)	Acc@5  99.00 ( 98.62)
Test: [ 30/100]	Time  0.024 ( 0.030)	Loss 7.3445e-01 (7.5488e-01)	Acc@1  78.00 ( 74.06)	Acc@5  99.00 ( 98.55)
Test: [ 40/100]	Time  0.027 ( 0.029)	Loss 5.8010e-01 (7.5102e-01)	Acc@1  82.00 ( 74.29)	Acc@5  98.00 ( 98.49)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 7.6320e-01 (7.4932e-01)	Acc@1  73.00 ( 74.49)	Acc@5  99.00 ( 98.47)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 7.9164e-01 (7.5566e-01)	Acc@1  74.00 ( 73.95)	Acc@5 100.00 ( 98.52)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 8.2102e-01 (7.5198e-01)	Acc@1  71.00 ( 74.13)	Acc@5  98.00 ( 98.56)
Test: [ 80/100]	Time  0.024 ( 0.028)	Loss 6.0643e-01 (7.4942e-01)	Acc@1  79.00 ( 74.09)	Acc@5  99.00 ( 98.60)
Test: [ 90/100]	Time  0.027 ( 0.027)	Loss 5.6224e-01 (7.5743e-01)	Acc@1  79.00 ( 73.89)	Acc@5 100.00 ( 98.59)
 * Acc@1 73.800 Acc@5 98.540
### epoch[7] execution time: 28.17976975440979
EPOCH 8
# current learning rate: 0.1
# Switched to train mode...
Epoch: [8][  0/391]	Time  0.241 ( 0.241)	Data  0.178 ( 0.178)	Loss 4.4940e-01 (4.4940e-01)	Acc@1  83.59 ( 83.59)	Acc@5  99.22 ( 99.22)
Epoch: [8][ 10/391]	Time  0.063 ( 0.078)	Data  0.001 ( 0.017)	Loss 7.0845e-01 (5.6585e-01)	Acc@1  73.44 ( 78.98)	Acc@5  97.66 ( 99.22)
Epoch: [8][ 20/391]	Time  0.061 ( 0.071)	Data  0.001 ( 0.010)	Loss 5.1703e-01 (5.8937e-01)	Acc@1  82.03 ( 78.35)	Acc@5  99.22 ( 99.07)
Epoch: [8][ 30/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.007)	Loss 6.4684e-01 (6.1509e-01)	Acc@1  79.69 ( 78.23)	Acc@5  97.66 ( 98.71)
Epoch: [8][ 40/391]	Time  0.062 ( 0.068)	Data  0.001 ( 0.005)	Loss 6.3795e-01 (6.1306e-01)	Acc@1  78.12 ( 78.20)	Acc@5  99.22 ( 98.82)
Epoch: [8][ 50/391]	Time  0.075 ( 0.067)	Data  0.001 ( 0.005)	Loss 6.2763e-01 (6.2117e-01)	Acc@1  78.12 ( 77.82)	Acc@5  99.22 ( 98.93)
Epoch: [8][ 60/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 5.6973e-01 (6.1538e-01)	Acc@1  81.25 ( 78.25)	Acc@5  99.22 ( 98.92)
Epoch: [8][ 70/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.004)	Loss 4.6634e-01 (6.0858e-01)	Acc@1  80.47 ( 78.41)	Acc@5 100.00 ( 98.97)
Epoch: [8][ 80/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 7.0487e-01 (6.0454e-01)	Acc@1  78.91 ( 78.69)	Acc@5  97.66 ( 98.97)
Epoch: [8][ 90/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 7.1228e-01 (6.1124e-01)	Acc@1  78.91 ( 78.44)	Acc@5  96.88 ( 98.91)
Epoch: [8][100/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 5.3001e-01 (6.1288e-01)	Acc@1  78.91 ( 78.27)	Acc@5 100.00 ( 98.91)
Epoch: [8][110/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.003)	Loss 7.8337e-01 (6.1816e-01)	Acc@1  69.53 ( 78.07)	Acc@5  98.44 ( 98.89)
Epoch: [8][120/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.003)	Loss 5.1702e-01 (6.1617e-01)	Acc@1  85.94 ( 78.29)	Acc@5  98.44 ( 98.88)
Epoch: [8][130/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.4634e-01 (6.1991e-01)	Acc@1  71.88 ( 78.17)	Acc@5  98.44 ( 98.87)
Epoch: [8][140/391]	Time  0.076 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.0684e-01 (6.1971e-01)	Acc@1  80.47 ( 78.17)	Acc@5  96.88 ( 98.88)
Epoch: [8][150/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.7847e-01 (6.1610e-01)	Acc@1  82.03 ( 78.40)	Acc@5  99.22 ( 98.89)
Epoch: [8][160/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.0933e-01 (6.1745e-01)	Acc@1  75.00 ( 78.34)	Acc@5  97.66 ( 98.88)
Epoch: [8][170/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.5147e-01 (6.1867e-01)	Acc@1  71.88 ( 78.26)	Acc@5  98.44 ( 98.89)
Epoch: [8][180/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.5379e-01 (6.1712e-01)	Acc@1  85.94 ( 78.30)	Acc@5  99.22 ( 98.88)
Epoch: [8][190/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.5587e-01 (6.1495e-01)	Acc@1  76.56 ( 78.38)	Acc@5  99.22 ( 98.88)
Epoch: [8][200/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.0776e-01 (6.1335e-01)	Acc@1  79.69 ( 78.46)	Acc@5  99.22 ( 98.87)
Epoch: [8][210/391]	Time  0.058 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.6968e-01 (6.1209e-01)	Acc@1  78.12 ( 78.57)	Acc@5  99.22 ( 98.87)
Epoch: [8][220/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.0427e-01 (6.1153e-01)	Acc@1  82.03 ( 78.62)	Acc@5  98.44 ( 98.83)
Epoch: [8][230/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.1387e-01 (6.1128e-01)	Acc@1  77.34 ( 78.60)	Acc@5  99.22 ( 98.84)
Epoch: [8][240/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.0679e-01 (6.1360e-01)	Acc@1  85.94 ( 78.56)	Acc@5  98.44 ( 98.81)
Epoch: [8][250/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.8804e-01 (6.1267e-01)	Acc@1  75.78 ( 78.60)	Acc@5  99.22 ( 98.81)
Epoch: [8][260/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.4122e-01 (6.1137e-01)	Acc@1  78.91 ( 78.65)	Acc@5 100.00 ( 98.82)
Epoch: [8][270/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.3080e-01 (6.1059e-01)	Acc@1  82.81 ( 78.66)	Acc@5  99.22 ( 98.83)
Epoch: [8][280/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.1090e-01 (6.1035e-01)	Acc@1  78.12 ( 78.69)	Acc@5 100.00 ( 98.84)
Epoch: [8][290/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.1008e-01 (6.0948e-01)	Acc@1  78.12 ( 78.70)	Acc@5 100.00 ( 98.85)
Epoch: [8][300/391]	Time  0.067 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.9411e-01 (6.0758e-01)	Acc@1  78.12 ( 78.78)	Acc@5  97.66 ( 98.83)
Epoch: [8][310/391]	Time  0.068 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.4730e-01 (6.0666e-01)	Acc@1  80.47 ( 78.87)	Acc@5  99.22 ( 98.83)
Epoch: [8][320/391]	Time  0.075 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.6653e-01 (6.0640e-01)	Acc@1  76.56 ( 78.90)	Acc@5  97.66 ( 98.81)
Epoch: [8][330/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.3149e-01 (6.0660e-01)	Acc@1  76.56 ( 78.93)	Acc@5  99.22 ( 98.80)
Epoch: [8][340/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.5818e-01 (6.0804e-01)	Acc@1  71.88 ( 78.89)	Acc@5  97.66 ( 98.80)
Epoch: [8][350/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.1013e-01 (6.0905e-01)	Acc@1  75.78 ( 78.88)	Acc@5  97.66 ( 98.80)
Epoch: [8][360/391]	Time  0.058 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.2119e-01 (6.0781e-01)	Acc@1  80.47 ( 78.90)	Acc@5  99.22 ( 98.81)
Epoch: [8][370/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.8271e-01 (6.0787e-01)	Acc@1  82.81 ( 78.92)	Acc@5 100.00 ( 98.81)
Epoch: [8][380/391]	Time  0.070 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.8500e-01 (6.0809e-01)	Acc@1  77.34 ( 78.89)	Acc@5  99.22 ( 98.82)
Epoch: [8][390/391]	Time  0.049 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.9107e-01 (6.0819e-01)	Acc@1  77.50 ( 78.90)	Acc@5  98.75 ( 98.82)
## e[8] optimizer.zero_grad (sum) time: 0.3981897830963135
## e[8]       loss.backward (sum) time: 6.940683126449585
## e[8]      optimizer.step (sum) time: 3.361628293991089
## epoch[8] training(only) time: 25.220296144485474
# Switched to evaluate mode...
Test: [  0/100]	Time  0.190 ( 0.190)	Loss 7.3897e-01 (7.3897e-01)	Acc@1  76.00 ( 76.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.024 ( 0.040)	Loss 7.7100e-01 (7.1297e-01)	Acc@1  76.00 ( 75.82)	Acc@5  98.00 ( 98.55)
Test: [ 20/100]	Time  0.027 ( 0.033)	Loss 6.8512e-01 (7.0612e-01)	Acc@1  74.00 ( 75.67)	Acc@5 100.00 ( 98.52)
Test: [ 30/100]	Time  0.027 ( 0.031)	Loss 6.8083e-01 (7.1612e-01)	Acc@1  75.00 ( 75.35)	Acc@5  98.00 ( 98.48)
Test: [ 40/100]	Time  0.028 ( 0.030)	Loss 5.4020e-01 (7.1357e-01)	Acc@1  83.00 ( 75.44)	Acc@5  99.00 ( 98.46)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 6.7336e-01 (7.0973e-01)	Acc@1  76.00 ( 75.59)	Acc@5  97.00 ( 98.37)
Test: [ 60/100]	Time  0.028 ( 0.029)	Loss 6.1165e-01 (7.0642e-01)	Acc@1  79.00 ( 75.59)	Acc@5  99.00 ( 98.46)
Test: [ 70/100]	Time  0.027 ( 0.028)	Loss 5.8925e-01 (6.9985e-01)	Acc@1  77.00 ( 75.89)	Acc@5  98.00 ( 98.49)
Test: [ 80/100]	Time  0.024 ( 0.028)	Loss 6.0286e-01 (6.9456e-01)	Acc@1  79.00 ( 76.04)	Acc@5 100.00 ( 98.53)
Test: [ 90/100]	Time  0.024 ( 0.028)	Loss 6.9268e-01 (7.0006e-01)	Acc@1  72.00 ( 75.68)	Acc@5 100.00 ( 98.47)
 * Acc@1 75.870 Acc@5 98.470
### epoch[8] execution time: 28.097319841384888
EPOCH 9
# current learning rate: 0.1
# Switched to train mode...
Epoch: [9][  0/391]	Time  0.257 ( 0.257)	Data  0.194 ( 0.194)	Loss 6.2256e-01 (6.2256e-01)	Acc@1  77.34 ( 77.34)	Acc@5  98.44 ( 98.44)
Epoch: [9][ 10/391]	Time  0.064 ( 0.081)	Data  0.001 ( 0.019)	Loss 5.5939e-01 (6.0812e-01)	Acc@1  78.12 ( 78.34)	Acc@5  99.22 ( 98.86)
Epoch: [9][ 20/391]	Time  0.063 ( 0.073)	Data  0.001 ( 0.010)	Loss 5.4790e-01 (6.0681e-01)	Acc@1  79.69 ( 78.87)	Acc@5  99.22 ( 98.74)
Epoch: [9][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.007)	Loss 6.2423e-01 (6.1014e-01)	Acc@1  78.12 ( 78.55)	Acc@5 100.00 ( 98.82)
Epoch: [9][ 40/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.006)	Loss 6.0693e-01 (6.1786e-01)	Acc@1  78.12 ( 78.41)	Acc@5  99.22 ( 98.84)
Epoch: [9][ 50/391]	Time  0.062 ( 0.068)	Data  0.001 ( 0.005)	Loss 4.9502e-01 (6.2017e-01)	Acc@1  84.38 ( 78.26)	Acc@5  97.66 ( 98.77)
Epoch: [9][ 60/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.004)	Loss 5.8699e-01 (6.1956e-01)	Acc@1  79.69 ( 78.06)	Acc@5 100.00 ( 98.73)
Epoch: [9][ 70/391]	Time  0.061 ( 0.067)	Data  0.001 ( 0.004)	Loss 4.4743e-01 (6.0849e-01)	Acc@1  86.72 ( 78.40)	Acc@5  99.22 ( 98.86)
Epoch: [9][ 80/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 6.2365e-01 (6.0199e-01)	Acc@1  78.91 ( 78.67)	Acc@5 100.00 ( 98.92)
Epoch: [9][ 90/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 5.5343e-01 (5.9485e-01)	Acc@1  81.25 ( 79.09)	Acc@5  99.22 ( 98.89)
Epoch: [9][100/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.003)	Loss 5.7460e-01 (5.9448e-01)	Acc@1  78.91 ( 79.06)	Acc@5 100.00 ( 98.92)
Epoch: [9][110/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 3.6477e-01 (5.9245e-01)	Acc@1  86.72 ( 79.12)	Acc@5  99.22 ( 98.89)
Epoch: [9][120/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 5.8391e-01 (5.8833e-01)	Acc@1  82.81 ( 79.33)	Acc@5  99.22 ( 98.90)
Epoch: [9][130/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 7.0210e-01 (5.9021e-01)	Acc@1  76.56 ( 79.29)	Acc@5  99.22 ( 98.92)
Epoch: [9][140/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.9681e-01 (5.9128e-01)	Acc@1  78.12 ( 79.29)	Acc@5 100.00 ( 98.90)
Epoch: [9][150/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.2789e-01 (5.9150e-01)	Acc@1  82.81 ( 79.26)	Acc@5  98.44 ( 98.90)
Epoch: [9][160/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.2115e-01 (5.9055e-01)	Acc@1  75.00 ( 79.33)	Acc@5  99.22 ( 98.91)
Epoch: [9][170/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.9449e-01 (5.8872e-01)	Acc@1  76.56 ( 79.36)	Acc@5 100.00 ( 98.93)
Epoch: [9][180/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.0654e-01 (5.8749e-01)	Acc@1  83.59 ( 79.36)	Acc@5 100.00 ( 98.95)
Epoch: [9][190/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.0570e-01 (5.8644e-01)	Acc@1  79.69 ( 79.42)	Acc@5  97.66 ( 98.94)
Epoch: [9][200/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.4082e-01 (5.8535e-01)	Acc@1  81.25 ( 79.48)	Acc@5  97.66 ( 98.95)
Epoch: [9][210/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.5946e-01 (5.8528e-01)	Acc@1  80.47 ( 79.49)	Acc@5  99.22 ( 98.96)
Epoch: [9][220/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.1787e-01 (5.8814e-01)	Acc@1  78.91 ( 79.38)	Acc@5  99.22 ( 98.95)
Epoch: [9][230/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.0389e-01 (5.8954e-01)	Acc@1  79.69 ( 79.34)	Acc@5  98.44 ( 98.95)
Epoch: [9][240/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.9869e-01 (5.9048e-01)	Acc@1  76.56 ( 79.27)	Acc@5 100.00 ( 98.94)
Epoch: [9][250/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.6195e-01 (5.8942e-01)	Acc@1  84.38 ( 79.34)	Acc@5  99.22 ( 98.95)
Epoch: [9][260/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.3974e-01 (5.8921e-01)	Acc@1  80.47 ( 79.36)	Acc@5  99.22 ( 98.96)
Epoch: [9][270/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.5020e-01 (5.8836e-01)	Acc@1  81.25 ( 79.43)	Acc@5  96.88 ( 98.93)
Epoch: [9][280/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.0494e-01 (5.8712e-01)	Acc@1  81.25 ( 79.48)	Acc@5  99.22 ( 98.94)
Epoch: [9][290/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.7304e-01 (5.8540e-01)	Acc@1  75.78 ( 79.53)	Acc@5  99.22 ( 98.95)
Epoch: [9][300/391]	Time  0.060 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.0547e-01 (5.8475e-01)	Acc@1  78.12 ( 79.52)	Acc@5  97.66 ( 98.96)
Epoch: [9][310/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.2541e-01 (5.8517e-01)	Acc@1  82.03 ( 79.52)	Acc@5  97.66 ( 98.96)
Epoch: [9][320/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.3325e-01 (5.8537e-01)	Acc@1  80.47 ( 79.51)	Acc@5  98.44 ( 98.96)
Epoch: [9][330/391]	Time  0.058 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.4809e-01 (5.8638e-01)	Acc@1  79.69 ( 79.51)	Acc@5  99.22 ( 98.95)
Epoch: [9][340/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.5793e-01 (5.8543e-01)	Acc@1  82.03 ( 79.55)	Acc@5 100.00 ( 98.96)
Epoch: [9][350/391]	Time  0.066 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.3338e-01 (5.8502e-01)	Acc@1  78.91 ( 79.58)	Acc@5  98.44 ( 98.97)
Epoch: [9][360/391]	Time  0.067 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.4869e-01 (5.8473e-01)	Acc@1  83.59 ( 79.62)	Acc@5  97.66 ( 98.96)
Epoch: [9][370/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.9006e-01 (5.8438e-01)	Acc@1  85.16 ( 79.64)	Acc@5 100.00 ( 98.96)
Epoch: [9][380/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.5185e-01 (5.8425e-01)	Acc@1  78.91 ( 79.64)	Acc@5 100.00 ( 98.96)
Epoch: [9][390/391]	Time  0.048 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.5734e-01 (5.8398e-01)	Acc@1  82.50 ( 79.65)	Acc@5 100.00 ( 98.95)
## e[9] optimizer.zero_grad (sum) time: 0.41024017333984375
## e[9]       loss.backward (sum) time: 6.940341472625732
## e[9]      optimizer.step (sum) time: 3.3534789085388184
## epoch[9] training(only) time: 25.221436023712158
# Switched to evaluate mode...
Test: [  0/100]	Time  0.191 ( 0.191)	Loss 6.4043e-01 (6.4043e-01)	Acc@1  79.00 ( 79.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.024 ( 0.040)	Loss 6.6840e-01 (5.9000e-01)	Acc@1  84.00 ( 80.82)	Acc@5  98.00 ( 98.91)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 5.7012e-01 (5.9527e-01)	Acc@1  79.00 ( 79.57)	Acc@5  99.00 ( 98.57)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 6.0144e-01 (6.1619e-01)	Acc@1  82.00 ( 79.39)	Acc@5  99.00 ( 98.45)
Test: [ 40/100]	Time  0.028 ( 0.029)	Loss 5.9736e-01 (6.1621e-01)	Acc@1  83.00 ( 79.34)	Acc@5  99.00 ( 98.34)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 5.4052e-01 (6.0674e-01)	Acc@1  84.00 ( 79.82)	Acc@5  99.00 ( 98.47)
Test: [ 60/100]	Time  0.028 ( 0.028)	Loss 6.3326e-01 (6.0703e-01)	Acc@1  78.00 ( 79.62)	Acc@5  99.00 ( 98.52)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 6.8182e-01 (6.0585e-01)	Acc@1  75.00 ( 79.48)	Acc@5  97.00 ( 98.55)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 4.8532e-01 (6.0028e-01)	Acc@1  83.00 ( 79.47)	Acc@5 100.00 ( 98.65)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 4.9177e-01 (5.9837e-01)	Acc@1  81.00 ( 79.37)	Acc@5 100.00 ( 98.70)
 * Acc@1 79.390 Acc@5 98.680
### epoch[9] execution time: 28.040098905563354
EPOCH 10
# current learning rate: 0.1
# Switched to train mode...
Epoch: [10][  0/391]	Time  0.257 ( 0.257)	Data  0.190 ( 0.190)	Loss 6.2667e-01 (6.2667e-01)	Acc@1  74.22 ( 74.22)	Acc@5  98.44 ( 98.44)
Epoch: [10][ 10/391]	Time  0.065 ( 0.081)	Data  0.001 ( 0.018)	Loss 6.3735e-01 (5.5085e-01)	Acc@1  77.34 ( 81.53)	Acc@5 100.00 ( 99.15)
Epoch: [10][ 20/391]	Time  0.067 ( 0.073)	Data  0.001 ( 0.010)	Loss 5.6152e-01 (5.4899e-01)	Acc@1  82.03 ( 81.40)	Acc@5  97.66 ( 99.00)
Epoch: [10][ 30/391]	Time  0.066 ( 0.070)	Data  0.001 ( 0.007)	Loss 3.8562e-01 (5.2586e-01)	Acc@1  86.72 ( 81.98)	Acc@5 100.00 ( 99.14)
Epoch: [10][ 40/391]	Time  0.062 ( 0.069)	Data  0.001 ( 0.006)	Loss 5.7911e-01 (5.3183e-01)	Acc@1  76.56 ( 81.42)	Acc@5 100.00 ( 99.16)
Epoch: [10][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.005)	Loss 5.8941e-01 (5.4153e-01)	Acc@1  77.34 ( 80.84)	Acc@5  98.44 ( 99.10)
Epoch: [10][ 60/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.004)	Loss 5.7362e-01 (5.4364e-01)	Acc@1  74.22 ( 80.76)	Acc@5 100.00 ( 99.14)
Epoch: [10][ 70/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.004)	Loss 5.2841e-01 (5.4681e-01)	Acc@1  79.69 ( 80.78)	Acc@5  97.66 ( 99.14)
Epoch: [10][ 80/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 5.5231e-01 (5.5366e-01)	Acc@1  78.91 ( 80.50)	Acc@5  98.44 ( 99.05)
Epoch: [10][ 90/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.9615e-01 (5.5559e-01)	Acc@1  84.38 ( 80.43)	Acc@5 100.00 ( 99.06)
Epoch: [10][100/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.1323e-01 (5.5580e-01)	Acc@1  85.16 ( 80.48)	Acc@5  99.22 ( 99.03)
Epoch: [10][110/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.5980e-01 (5.5479e-01)	Acc@1  88.28 ( 80.55)	Acc@5 100.00 ( 99.03)
Epoch: [10][120/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.003)	Loss 4.1099e-01 (5.5565e-01)	Acc@1  86.72 ( 80.65)	Acc@5 100.00 ( 99.03)
Epoch: [10][130/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.003)	Loss 5.7384e-01 (5.5725e-01)	Acc@1  81.25 ( 80.67)	Acc@5  99.22 ( 99.04)
Epoch: [10][140/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.6696e-01 (5.5787e-01)	Acc@1  78.12 ( 80.58)	Acc@5  96.88 ( 99.05)
Epoch: [10][150/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.6622e-01 (5.6053e-01)	Acc@1  78.12 ( 80.47)	Acc@5  98.44 ( 99.03)
Epoch: [10][160/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.4289e-01 (5.5960e-01)	Acc@1  78.12 ( 80.59)	Acc@5  98.44 ( 99.04)
Epoch: [10][170/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.8098e-01 (5.6039e-01)	Acc@1  84.38 ( 80.57)	Acc@5  97.66 ( 99.02)
Epoch: [10][180/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.3880e-01 (5.6272e-01)	Acc@1  78.91 ( 80.52)	Acc@5  98.44 ( 98.99)
Epoch: [10][190/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.4999e-01 (5.6353e-01)	Acc@1  77.34 ( 80.46)	Acc@5 100.00 ( 98.99)
Epoch: [10][200/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.2882e-01 (5.6150e-01)	Acc@1  82.03 ( 80.56)	Acc@5 100.00 ( 99.00)
Epoch: [10][210/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.1405e-01 (5.5958e-01)	Acc@1  86.72 ( 80.70)	Acc@5 100.00 ( 98.98)
Epoch: [10][220/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.8594e-01 (5.5847e-01)	Acc@1  81.25 ( 80.73)	Acc@5 100.00 ( 98.99)
Epoch: [10][230/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.8485e-01 (5.5712e-01)	Acc@1  84.38 ( 80.79)	Acc@5  99.22 ( 99.00)
Epoch: [10][240/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.5535e-01 (5.5627e-01)	Acc@1  83.59 ( 80.82)	Acc@5 100.00 ( 98.99)
Epoch: [10][250/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.3757e-01 (5.5717e-01)	Acc@1  87.50 ( 80.80)	Acc@5  98.44 ( 98.99)
Epoch: [10][260/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.3970e-01 (5.5976e-01)	Acc@1  75.00 ( 80.69)	Acc@5  96.09 ( 98.99)
Epoch: [10][270/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.7621e-01 (5.6061e-01)	Acc@1  80.47 ( 80.68)	Acc@5  98.44 ( 98.97)
Epoch: [10][280/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.4973e-01 (5.6125e-01)	Acc@1  79.69 ( 80.70)	Acc@5  99.22 ( 98.96)
Epoch: [10][290/391]	Time  0.071 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.5605e-01 (5.6052e-01)	Acc@1  85.16 ( 80.76)	Acc@5  99.22 ( 98.95)
Epoch: [10][300/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.5045e-01 (5.6109e-01)	Acc@1  89.06 ( 80.73)	Acc@5  99.22 ( 98.94)
Epoch: [10][310/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.9245e-01 (5.6136e-01)	Acc@1  82.81 ( 80.74)	Acc@5  98.44 ( 98.94)
Epoch: [10][320/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.1031e-01 (5.6100e-01)	Acc@1  82.81 ( 80.76)	Acc@5 100.00 ( 98.94)
Epoch: [10][330/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.0088e-01 (5.6020e-01)	Acc@1  83.59 ( 80.78)	Acc@5 100.00 ( 98.95)
Epoch: [10][340/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.9669e-01 (5.5886e-01)	Acc@1  82.03 ( 80.83)	Acc@5  98.44 ( 98.94)
Epoch: [10][350/391]	Time  0.057 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.7559e-01 (5.6073e-01)	Acc@1  73.44 ( 80.72)	Acc@5  99.22 ( 98.94)
Epoch: [10][360/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.5664e-01 (5.6279e-01)	Acc@1  72.66 ( 80.64)	Acc@5  98.44 ( 98.92)
Epoch: [10][370/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.0993e-01 (5.6205e-01)	Acc@1  85.16 ( 80.67)	Acc@5  98.44 ( 98.93)
Epoch: [10][380/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.7241e-01 (5.6108e-01)	Acc@1  78.91 ( 80.71)	Acc@5  98.44 ( 98.93)
Epoch: [10][390/391]	Time  0.046 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.9445e-01 (5.6192e-01)	Acc@1  76.25 ( 80.70)	Acc@5 100.00 ( 98.93)
## e[10] optimizer.zero_grad (sum) time: 0.3988513946533203
## e[10]       loss.backward (sum) time: 6.950555086135864
## e[10]      optimizer.step (sum) time: 3.3603248596191406
## epoch[10] training(only) time: 25.2681245803833
# Switched to evaluate mode...
Test: [  0/100]	Time  0.197 ( 0.197)	Loss 7.4738e-01 (7.4738e-01)	Acc@1  74.00 ( 74.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.024 ( 0.042)	Loss 7.2204e-01 (7.2101e-01)	Acc@1  78.00 ( 76.27)	Acc@5  95.00 ( 98.45)
Test: [ 20/100]	Time  0.024 ( 0.034)	Loss 6.9466e-01 (7.3141e-01)	Acc@1  74.00 ( 75.05)	Acc@5  98.00 ( 98.48)
Test: [ 30/100]	Time  0.027 ( 0.031)	Loss 7.0267e-01 (7.3972e-01)	Acc@1  74.00 ( 74.87)	Acc@5  98.00 ( 98.42)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 7.0882e-01 (7.4499e-01)	Acc@1  75.00 ( 75.00)	Acc@5  98.00 ( 98.29)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 6.0140e-01 (7.3601e-01)	Acc@1  81.00 ( 75.53)	Acc@5  98.00 ( 98.14)
Test: [ 60/100]	Time  0.026 ( 0.029)	Loss 7.3758e-01 (7.3785e-01)	Acc@1  66.00 ( 75.44)	Acc@5 100.00 ( 98.10)
Test: [ 70/100]	Time  0.027 ( 0.028)	Loss 8.2299e-01 (7.3137e-01)	Acc@1  71.00 ( 75.55)	Acc@5  98.00 ( 98.24)
Test: [ 80/100]	Time  0.024 ( 0.028)	Loss 5.4755e-01 (7.2487e-01)	Acc@1  82.00 ( 75.64)	Acc@5 100.00 ( 98.30)
Test: [ 90/100]	Time  0.027 ( 0.028)	Loss 7.2364e-01 (7.2896e-01)	Acc@1  76.00 ( 75.67)	Acc@5  99.00 ( 98.26)
 * Acc@1 75.710 Acc@5 98.240
### epoch[10] execution time: 28.191694974899292
EPOCH 11
# current learning rate: 0.1
# Switched to train mode...
Epoch: [11][  0/391]	Time  0.252 ( 0.252)	Data  0.190 ( 0.190)	Loss 4.5012e-01 (4.5012e-01)	Acc@1  83.59 ( 83.59)	Acc@5 100.00 (100.00)
Epoch: [11][ 10/391]	Time  0.060 ( 0.079)	Data  0.001 ( 0.018)	Loss 3.3088e-01 (4.7974e-01)	Acc@1  89.06 ( 83.31)	Acc@5 100.00 ( 99.36)
Epoch: [11][ 20/391]	Time  0.064 ( 0.072)	Data  0.001 ( 0.010)	Loss 6.5737e-01 (5.3299e-01)	Acc@1  77.34 ( 81.66)	Acc@5  98.44 ( 99.22)
Epoch: [11][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.007)	Loss 4.9839e-01 (5.2987e-01)	Acc@1  82.03 ( 81.70)	Acc@5 100.00 ( 99.17)
Epoch: [11][ 40/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.006)	Loss 5.4583e-01 (5.3056e-01)	Acc@1  81.25 ( 81.55)	Acc@5  98.44 ( 99.10)
Epoch: [11][ 50/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.005)	Loss 5.4843e-01 (5.2793e-01)	Acc@1  82.03 ( 81.45)	Acc@5 100.00 ( 99.08)
Epoch: [11][ 60/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.004)	Loss 6.1567e-01 (5.3250e-01)	Acc@1  78.91 ( 81.39)	Acc@5  99.22 ( 99.09)
Epoch: [11][ 70/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.004)	Loss 4.7276e-01 (5.2666e-01)	Acc@1  81.25 ( 81.53)	Acc@5  98.44 ( 99.10)
Epoch: [11][ 80/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.003)	Loss 5.6316e-01 (5.2443e-01)	Acc@1  85.16 ( 81.78)	Acc@5  98.44 ( 99.12)
Epoch: [11][ 90/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 5.4577e-01 (5.2441e-01)	Acc@1  79.69 ( 81.81)	Acc@5  99.22 ( 99.17)
Epoch: [11][100/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 6.6595e-01 (5.2838e-01)	Acc@1  81.25 ( 81.72)	Acc@5  99.22 ( 99.16)
Epoch: [11][110/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 5.7658e-01 (5.2856e-01)	Acc@1  78.91 ( 81.69)	Acc@5  99.22 ( 99.16)
Epoch: [11][120/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 5.2806e-01 (5.2938e-01)	Acc@1  80.47 ( 81.70)	Acc@5 100.00 ( 99.10)
Epoch: [11][130/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 5.3227e-01 (5.2844e-01)	Acc@1  82.03 ( 81.74)	Acc@5  99.22 ( 99.07)
Epoch: [11][140/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.6953e-01 (5.3020e-01)	Acc@1  85.16 ( 81.69)	Acc@5 100.00 ( 99.08)
Epoch: [11][150/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.3076e-01 (5.2887e-01)	Acc@1  81.25 ( 81.76)	Acc@5  98.44 ( 99.08)
Epoch: [11][160/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.5666e-01 (5.3000e-01)	Acc@1  78.91 ( 81.66)	Acc@5  98.44 ( 99.08)
Epoch: [11][170/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.9767e-01 (5.3160e-01)	Acc@1  79.69 ( 81.55)	Acc@5  99.22 ( 99.07)
Epoch: [11][180/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.8313e-01 (5.3237e-01)	Acc@1  83.59 ( 81.48)	Acc@5  99.22 ( 99.05)
Epoch: [11][190/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.9654e-01 (5.3156e-01)	Acc@1  79.69 ( 81.49)	Acc@5  98.44 ( 99.06)
Epoch: [11][200/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.3292e-01 (5.2979e-01)	Acc@1  85.16 ( 81.52)	Acc@5  99.22 ( 99.06)
Epoch: [11][210/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.3236e-01 (5.3310e-01)	Acc@1  76.56 ( 81.46)	Acc@5  99.22 ( 99.03)
Epoch: [11][220/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.0771e-01 (5.3237e-01)	Acc@1  81.25 ( 81.54)	Acc@5 100.00 ( 99.04)
Epoch: [11][230/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.6475e-01 (5.3192e-01)	Acc@1  82.81 ( 81.53)	Acc@5  99.22 ( 99.05)
Epoch: [11][240/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.6133e-01 (5.3316e-01)	Acc@1  82.03 ( 81.51)	Acc@5  98.44 ( 99.05)
Epoch: [11][250/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.0248e-01 (5.3280e-01)	Acc@1  85.16 ( 81.50)	Acc@5 100.00 ( 99.06)
Epoch: [11][260/391]	Time  0.060 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.2947e-01 (5.3338e-01)	Acc@1  80.47 ( 81.47)	Acc@5  99.22 ( 99.06)
Epoch: [11][270/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.2933e-01 (5.3458e-01)	Acc@1  85.94 ( 81.45)	Acc@5  99.22 ( 99.07)
Epoch: [11][280/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.9264e-01 (5.3434e-01)	Acc@1  82.03 ( 81.47)	Acc@5  97.66 ( 99.06)
Epoch: [11][290/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.9372e-01 (5.3561e-01)	Acc@1  79.69 ( 81.42)	Acc@5 100.00 ( 99.07)
Epoch: [11][300/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.8878e-01 (5.3585e-01)	Acc@1  85.94 ( 81.43)	Acc@5 100.00 ( 99.07)
Epoch: [11][310/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.0602e-01 (5.3623e-01)	Acc@1  82.81 ( 81.42)	Acc@5  98.44 ( 99.06)
Epoch: [11][320/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.7885e-01 (5.3667e-01)	Acc@1  75.78 ( 81.43)	Acc@5 100.00 ( 99.06)
Epoch: [11][330/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.2288e-01 (5.3654e-01)	Acc@1  84.38 ( 81.46)	Acc@5 100.00 ( 99.06)
Epoch: [11][340/391]	Time  0.066 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.7024e-01 (5.3721e-01)	Acc@1  83.59 ( 81.43)	Acc@5 100.00 ( 99.05)
Epoch: [11][350/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.6748e-01 (5.3749e-01)	Acc@1  82.81 ( 81.44)	Acc@5  98.44 ( 99.05)
Epoch: [11][360/391]	Time  0.066 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.0785e-01 (5.3760e-01)	Acc@1  82.81 ( 81.47)	Acc@5  97.66 ( 99.05)
Epoch: [11][370/391]	Time  0.060 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.4406e-01 (5.3643e-01)	Acc@1  78.91 ( 81.48)	Acc@5 100.00 ( 99.06)
Epoch: [11][380/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.3552e-01 (5.3769e-01)	Acc@1  78.12 ( 81.47)	Acc@5  99.22 ( 99.05)
Epoch: [11][390/391]	Time  0.047 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.0008e-01 (5.3789e-01)	Acc@1  88.75 ( 81.43)	Acc@5  97.50 ( 99.04)
## e[11] optimizer.zero_grad (sum) time: 0.39635396003723145
## e[11]       loss.backward (sum) time: 6.920300483703613
## e[11]      optimizer.step (sum) time: 3.347245931625366
## epoch[11] training(only) time: 25.16853404045105
# Switched to evaluate mode...
Test: [  0/100]	Time  0.193 ( 0.193)	Loss 6.0631e-01 (6.0631e-01)	Acc@1  78.00 ( 78.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.024 ( 0.041)	Loss 5.1655e-01 (6.1509e-01)	Acc@1  83.00 ( 78.91)	Acc@5  99.00 ( 99.00)
Test: [ 20/100]	Time  0.024 ( 0.034)	Loss 6.9490e-01 (6.3071e-01)	Acc@1  73.00 ( 78.67)	Acc@5  99.00 ( 98.90)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 6.4863e-01 (6.5130e-01)	Acc@1  79.00 ( 78.13)	Acc@5  99.00 ( 98.74)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 5.4682e-01 (6.5388e-01)	Acc@1  85.00 ( 78.27)	Acc@5  98.00 ( 98.66)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 5.1955e-01 (6.5179e-01)	Acc@1  86.00 ( 78.55)	Acc@5  96.00 ( 98.57)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 6.8961e-01 (6.4534e-01)	Acc@1  76.00 ( 78.57)	Acc@5  99.00 ( 98.67)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 5.4156e-01 (6.3846e-01)	Acc@1  81.00 ( 78.76)	Acc@5  98.00 ( 98.77)
Test: [ 80/100]	Time  0.028 ( 0.028)	Loss 4.6703e-01 (6.3556e-01)	Acc@1  82.00 ( 78.84)	Acc@5 100.00 ( 98.78)
Test: [ 90/100]	Time  0.024 ( 0.028)	Loss 5.4106e-01 (6.3491e-01)	Acc@1  84.00 ( 78.89)	Acc@5  99.00 ( 98.78)
 * Acc@1 78.910 Acc@5 98.800
### epoch[11] execution time: 28.0732741355896
EPOCH 12
# current learning rate: 0.1
# Switched to train mode...
Epoch: [12][  0/391]	Time  0.258 ( 0.258)	Data  0.195 ( 0.195)	Loss 4.5332e-01 (4.5332e-01)	Acc@1  85.16 ( 85.16)	Acc@5  98.44 ( 98.44)
Epoch: [12][ 10/391]	Time  0.067 ( 0.083)	Data  0.001 ( 0.019)	Loss 6.0436e-01 (5.1101e-01)	Acc@1  76.56 ( 82.81)	Acc@5  99.22 ( 99.22)
Epoch: [12][ 20/391]	Time  0.063 ( 0.074)	Data  0.001 ( 0.010)	Loss 5.0379e-01 (4.8681e-01)	Acc@1  79.69 ( 83.59)	Acc@5  99.22 ( 99.40)
Epoch: [12][ 30/391]	Time  0.059 ( 0.071)	Data  0.001 ( 0.007)	Loss 5.5644e-01 (4.8747e-01)	Acc@1  80.47 ( 83.52)	Acc@5  99.22 ( 99.34)
Epoch: [12][ 40/391]	Time  0.062 ( 0.070)	Data  0.001 ( 0.006)	Loss 7.0260e-01 (4.9779e-01)	Acc@1  75.78 ( 83.08)	Acc@5  96.88 ( 99.24)
Epoch: [12][ 50/391]	Time  0.060 ( 0.068)	Data  0.001 ( 0.005)	Loss 4.6140e-01 (5.0312e-01)	Acc@1  85.16 ( 82.86)	Acc@5 100.00 ( 99.25)
Epoch: [12][ 60/391]	Time  0.061 ( 0.067)	Data  0.001 ( 0.004)	Loss 4.2381e-01 (5.0539e-01)	Acc@1  83.59 ( 82.70)	Acc@5  99.22 ( 99.30)
Epoch: [12][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 4.4653e-01 (5.1008e-01)	Acc@1  82.03 ( 82.41)	Acc@5 100.00 ( 99.32)
Epoch: [12][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 3.4274e-01 (5.0503e-01)	Acc@1  87.50 ( 82.47)	Acc@5 100.00 ( 99.33)
Epoch: [12][ 90/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.4537e-01 (5.1100e-01)	Acc@1  88.28 ( 82.43)	Acc@5 100.00 ( 99.25)
Epoch: [12][100/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 6.0176e-01 (5.1159e-01)	Acc@1  78.12 ( 82.40)	Acc@5  99.22 ( 99.23)
Epoch: [12][110/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 5.6505e-01 (5.1376e-01)	Acc@1  80.47 ( 82.26)	Acc@5 100.00 ( 99.23)
Epoch: [12][120/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.003)	Loss 5.0095e-01 (5.1444e-01)	Acc@1  82.03 ( 82.25)	Acc@5  98.44 ( 99.23)
Epoch: [12][130/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 5.8552e-01 (5.1856e-01)	Acc@1  75.00 ( 82.11)	Acc@5  99.22 ( 99.21)
Epoch: [12][140/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.003)	Loss 5.0308e-01 (5.1879e-01)	Acc@1  82.03 ( 82.05)	Acc@5  99.22 ( 99.20)
Epoch: [12][150/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.5392e-01 (5.1873e-01)	Acc@1  82.03 ( 82.08)	Acc@5 100.00 ( 99.20)
Epoch: [12][160/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.8559e-01 (5.1802e-01)	Acc@1  84.38 ( 82.14)	Acc@5 100.00 ( 99.20)
Epoch: [12][170/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.4646e-01 (5.1706e-01)	Acc@1  82.81 ( 82.16)	Acc@5  99.22 ( 99.22)
Epoch: [12][180/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.0425e-01 (5.2101e-01)	Acc@1  81.25 ( 82.02)	Acc@5 100.00 ( 99.20)
Epoch: [12][190/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.1528e-01 (5.2317e-01)	Acc@1  81.25 ( 81.95)	Acc@5  99.22 ( 99.17)
Epoch: [12][200/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.2457e-01 (5.2263e-01)	Acc@1  81.25 ( 81.96)	Acc@5 100.00 ( 99.18)
Epoch: [12][210/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.4713e-01 (5.2117e-01)	Acc@1  85.94 ( 82.01)	Acc@5  99.22 ( 99.20)
Epoch: [12][220/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.6936e-01 (5.2070e-01)	Acc@1  80.47 ( 82.06)	Acc@5 100.00 ( 99.19)
Epoch: [12][230/391]	Time  0.074 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.4544e-01 (5.1831e-01)	Acc@1  86.72 ( 82.10)	Acc@5 100.00 ( 99.21)
Epoch: [12][240/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.1714e-01 (5.1773e-01)	Acc@1  83.59 ( 82.10)	Acc@5  99.22 ( 99.21)
Epoch: [12][250/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.0204e-01 (5.1792e-01)	Acc@1  75.78 ( 82.04)	Acc@5 100.00 ( 99.22)
Epoch: [12][260/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.9533e-01 (5.1725e-01)	Acc@1  82.03 ( 82.10)	Acc@5  98.44 ( 99.20)
Epoch: [12][270/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.1637e-01 (5.1639e-01)	Acc@1  75.78 ( 82.11)	Acc@5  98.44 ( 99.21)
Epoch: [12][280/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.6986e-01 (5.1493e-01)	Acc@1  84.38 ( 82.15)	Acc@5 100.00 ( 99.22)
Epoch: [12][290/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.7666e-01 (5.1754e-01)	Acc@1  85.16 ( 82.10)	Acc@5  99.22 ( 99.21)
Epoch: [12][300/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.3752e-01 (5.1930e-01)	Acc@1  79.69 ( 82.02)	Acc@5  99.22 ( 99.19)
Epoch: [12][310/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.0807e-01 (5.2051e-01)	Acc@1  82.81 ( 81.99)	Acc@5  97.66 ( 99.17)
Epoch: [12][320/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.0674e-01 (5.2035e-01)	Acc@1  82.81 ( 81.99)	Acc@5  96.09 ( 99.16)
Epoch: [12][330/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.0944e-01 (5.2125e-01)	Acc@1  86.72 ( 81.98)	Acc@5  97.66 ( 99.15)
Epoch: [12][340/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.7782e-01 (5.1995e-01)	Acc@1  83.59 ( 82.04)	Acc@5 100.00 ( 99.15)
Epoch: [12][350/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.4646e-01 (5.2008e-01)	Acc@1  82.03 ( 82.03)	Acc@5  99.22 ( 99.15)
Epoch: [12][360/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.7852e-01 (5.1953e-01)	Acc@1  81.25 ( 82.03)	Acc@5  98.44 ( 99.15)
Epoch: [12][370/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.8677e-01 (5.1833e-01)	Acc@1  86.72 ( 82.09)	Acc@5 100.00 ( 99.17)
Epoch: [12][380/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.7934e-01 (5.1946e-01)	Acc@1  82.81 ( 82.05)	Acc@5 100.00 ( 99.16)
Epoch: [12][390/391]	Time  0.045 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.9465e-01 (5.2101e-01)	Acc@1  81.25 ( 82.00)	Acc@5 100.00 ( 99.14)
## e[12] optimizer.zero_grad (sum) time: 0.39402031898498535
## e[12]       loss.backward (sum) time: 6.929563999176025
## e[12]      optimizer.step (sum) time: 3.407864570617676
## epoch[12] training(only) time: 25.324851036071777
# Switched to evaluate mode...
Test: [  0/100]	Time  0.207 ( 0.207)	Loss 8.0251e-01 (8.0251e-01)	Acc@1  77.00 ( 77.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.027 ( 0.044)	Loss 7.4406e-01 (7.1135e-01)	Acc@1  77.00 ( 76.91)	Acc@5  95.00 ( 98.27)
Test: [ 20/100]	Time  0.026 ( 0.035)	Loss 6.2162e-01 (7.1231e-01)	Acc@1  79.00 ( 76.95)	Acc@5  99.00 ( 98.14)
Test: [ 30/100]	Time  0.026 ( 0.032)	Loss 6.5776e-01 (7.2106e-01)	Acc@1  80.00 ( 76.39)	Acc@5  98.00 ( 98.10)
Test: [ 40/100]	Time  0.024 ( 0.031)	Loss 7.3488e-01 (7.2326e-01)	Acc@1  79.00 ( 76.49)	Acc@5  98.00 ( 98.00)
Test: [ 50/100]	Time  0.024 ( 0.030)	Loss 6.3880e-01 (7.0618e-01)	Acc@1  77.00 ( 77.14)	Acc@5 100.00 ( 98.16)
Test: [ 60/100]	Time  0.024 ( 0.029)	Loss 6.4090e-01 (7.0628e-01)	Acc@1  77.00 ( 77.03)	Acc@5  99.00 ( 98.16)
Test: [ 70/100]	Time  0.027 ( 0.029)	Loss 7.9433e-01 (7.0161e-01)	Acc@1  78.00 ( 76.96)	Acc@5  97.00 ( 98.21)
Test: [ 80/100]	Time  0.026 ( 0.028)	Loss 6.2471e-01 (6.9571e-01)	Acc@1  79.00 ( 77.05)	Acc@5  98.00 ( 98.23)
Test: [ 90/100]	Time  0.024 ( 0.028)	Loss 5.8065e-01 (6.9768e-01)	Acc@1  83.00 ( 76.84)	Acc@5  99.00 ( 98.29)
 * Acc@1 76.810 Acc@5 98.280
### epoch[12] execution time: 28.245173931121826
EPOCH 13
# current learning rate: 0.1
# Switched to train mode...
Epoch: [13][  0/391]	Time  0.258 ( 0.258)	Data  0.196 ( 0.196)	Loss 5.0604e-01 (5.0604e-01)	Acc@1  82.81 ( 82.81)	Acc@5  98.44 ( 98.44)
Epoch: [13][ 10/391]	Time  0.063 ( 0.082)	Data  0.001 ( 0.019)	Loss 3.7341e-01 (4.7661e-01)	Acc@1  87.50 ( 83.52)	Acc@5  99.22 ( 99.08)
Epoch: [13][ 20/391]	Time  0.064 ( 0.073)	Data  0.001 ( 0.010)	Loss 4.6474e-01 (4.7633e-01)	Acc@1  82.81 ( 83.44)	Acc@5  99.22 ( 99.29)
Epoch: [13][ 30/391]	Time  0.063 ( 0.070)	Data  0.001 ( 0.007)	Loss 6.2469e-01 (4.7924e-01)	Acc@1  77.34 ( 83.11)	Acc@5  98.44 ( 99.27)
Epoch: [13][ 40/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.006)	Loss 4.4992e-01 (4.8612e-01)	Acc@1  82.81 ( 82.81)	Acc@5  99.22 ( 99.18)
Epoch: [13][ 50/391]	Time  0.060 ( 0.067)	Data  0.001 ( 0.005)	Loss 4.8383e-01 (4.9773e-01)	Acc@1  81.25 ( 82.46)	Acc@5  99.22 ( 99.16)
Epoch: [13][ 60/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.004)	Loss 5.7381e-01 (4.9756e-01)	Acc@1  78.91 ( 82.51)	Acc@5  98.44 ( 99.18)
Epoch: [13][ 70/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.004)	Loss 5.5445e-01 (4.9959e-01)	Acc@1  80.47 ( 82.52)	Acc@5  98.44 ( 99.24)
Epoch: [13][ 80/391]	Time  0.059 ( 0.066)	Data  0.001 ( 0.004)	Loss 4.7135e-01 (4.9956e-01)	Acc@1  83.59 ( 82.61)	Acc@5  99.22 ( 99.23)
Epoch: [13][ 90/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.2739e-01 (4.9529e-01)	Acc@1  89.84 ( 82.87)	Acc@5 100.00 ( 99.24)
Epoch: [13][100/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.003)	Loss 5.1580e-01 (4.9944e-01)	Acc@1  80.47 ( 82.65)	Acc@5  99.22 ( 99.20)
Epoch: [13][110/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.003)	Loss 4.1883e-01 (4.9884e-01)	Acc@1  85.94 ( 82.70)	Acc@5 100.00 ( 99.20)
Epoch: [13][120/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 3.4605e-01 (4.9765e-01)	Acc@1  90.62 ( 82.83)	Acc@5  99.22 ( 99.19)
Epoch: [13][130/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.003)	Loss 6.0397e-01 (5.0242e-01)	Acc@1  79.69 ( 82.75)	Acc@5 100.00 ( 99.18)
Epoch: [13][140/391]	Time  0.070 ( 0.065)	Data  0.001 ( 0.003)	Loss 5.6526e-01 (5.0807e-01)	Acc@1  82.81 ( 82.59)	Acc@5  99.22 ( 99.19)
Epoch: [13][150/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.0340e-01 (5.0829e-01)	Acc@1  84.38 ( 82.66)	Acc@5  98.44 ( 99.18)
Epoch: [13][160/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.8023e-01 (5.0770e-01)	Acc@1  86.72 ( 82.65)	Acc@5 100.00 ( 99.18)
Epoch: [13][170/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.7608e-01 (5.0458e-01)	Acc@1  84.38 ( 82.73)	Acc@5 100.00 ( 99.20)
Epoch: [13][180/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.1278e-01 (5.0403e-01)	Acc@1  82.81 ( 82.79)	Acc@5  98.44 ( 99.19)
Epoch: [13][190/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.6527e-01 (5.0162e-01)	Acc@1  84.38 ( 82.91)	Acc@5 100.00 ( 99.22)
Epoch: [13][200/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.4135e-01 (5.0028e-01)	Acc@1  90.62 ( 82.93)	Acc@5 100.00 ( 99.22)
Epoch: [13][210/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.7243e-01 (5.0070e-01)	Acc@1  85.94 ( 82.92)	Acc@5  98.44 ( 99.21)
Epoch: [13][220/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.0670e-01 (4.9852e-01)	Acc@1  89.06 ( 83.01)	Acc@5 100.00 ( 99.23)
Epoch: [13][230/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.4756e-01 (4.9935e-01)	Acc@1  82.81 ( 83.01)	Acc@5  99.22 ( 99.22)
Epoch: [13][240/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.4219e-01 (4.9940e-01)	Acc@1  82.03 ( 82.99)	Acc@5  99.22 ( 99.21)
Epoch: [13][250/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.2420e-01 (4.9902e-01)	Acc@1  87.50 ( 83.01)	Acc@5 100.00 ( 99.21)
Epoch: [13][260/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.8013e-01 (4.9816e-01)	Acc@1  87.50 ( 83.04)	Acc@5  97.66 ( 99.20)
Epoch: [13][270/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.1840e-01 (4.9770e-01)	Acc@1  79.69 ( 83.04)	Acc@5  98.44 ( 99.19)
Epoch: [13][280/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.8986e-01 (4.9757e-01)	Acc@1  81.25 ( 83.04)	Acc@5  98.44 ( 99.19)
Epoch: [13][290/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.0760e-01 (4.9817e-01)	Acc@1  85.94 ( 83.02)	Acc@5  99.22 ( 99.19)
Epoch: [13][300/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.9890e-01 (4.9734e-01)	Acc@1  87.50 ( 83.01)	Acc@5  98.44 ( 99.20)
Epoch: [13][310/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.8572e-01 (4.9721e-01)	Acc@1  79.69 ( 83.01)	Acc@5  99.22 ( 99.19)
Epoch: [13][320/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.5401e-01 (4.9679e-01)	Acc@1  88.28 ( 83.04)	Acc@5  99.22 ( 99.18)
Epoch: [13][330/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.7474e-01 (4.9774e-01)	Acc@1  84.38 ( 83.03)	Acc@5  99.22 ( 99.17)
Epoch: [13][340/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.5791e-01 (4.9909e-01)	Acc@1  83.59 ( 82.99)	Acc@5 100.00 ( 99.16)
Epoch: [13][350/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.9448e-01 (4.9960e-01)	Acc@1  82.03 ( 82.97)	Acc@5  99.22 ( 99.15)
Epoch: [13][360/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.4893e-01 (5.0028e-01)	Acc@1  78.12 ( 82.94)	Acc@5 100.00 ( 99.15)
Epoch: [13][370/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.6100e-01 (5.0043e-01)	Acc@1  81.25 ( 82.92)	Acc@5  98.44 ( 99.14)
Epoch: [13][380/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.9991e-01 (5.0078e-01)	Acc@1  85.16 ( 82.93)	Acc@5  99.22 ( 99.15)
Epoch: [13][390/391]	Time  0.052 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.6236e-01 (5.0159e-01)	Acc@1  83.75 ( 82.90)	Acc@5 100.00 ( 99.15)
## e[13] optimizer.zero_grad (sum) time: 0.3956449031829834
## e[13]       loss.backward (sum) time: 6.959880352020264
## e[13]      optimizer.step (sum) time: 3.343756675720215
## epoch[13] training(only) time: 25.2497079372406
# Switched to evaluate mode...
Test: [  0/100]	Time  0.194 ( 0.194)	Loss 5.8960e-01 (5.8960e-01)	Acc@1  79.00 ( 79.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.024 ( 0.041)	Loss 6.8201e-01 (6.1932e-01)	Acc@1  79.00 ( 78.36)	Acc@5  98.00 ( 99.36)
Test: [ 20/100]	Time  0.024 ( 0.034)	Loss 7.0702e-01 (6.1070e-01)	Acc@1  71.00 ( 78.57)	Acc@5  99.00 ( 98.90)
Test: [ 30/100]	Time  0.027 ( 0.031)	Loss 6.2665e-01 (6.1377e-01)	Acc@1  78.00 ( 78.87)	Acc@5  99.00 ( 98.97)
Test: [ 40/100]	Time  0.024 ( 0.030)	Loss 5.3080e-01 (6.1607e-01)	Acc@1  85.00 ( 79.10)	Acc@5  98.00 ( 98.90)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 5.6495e-01 (6.0855e-01)	Acc@1  81.00 ( 79.20)	Acc@5  98.00 ( 98.88)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 6.9150e-01 (6.1634e-01)	Acc@1  72.00 ( 78.74)	Acc@5  99.00 ( 98.92)
Test: [ 70/100]	Time  0.027 ( 0.028)	Loss 6.4049e-01 (6.1156e-01)	Acc@1  80.00 ( 78.77)	Acc@5  99.00 ( 98.96)
Test: [ 80/100]	Time  0.024 ( 0.028)	Loss 6.4019e-01 (6.1039e-01)	Acc@1  75.00 ( 78.83)	Acc@5  98.00 ( 98.94)
Test: [ 90/100]	Time  0.028 ( 0.028)	Loss 6.0605e-01 (6.1238e-01)	Acc@1  79.00 ( 78.97)	Acc@5  99.00 ( 98.91)
 * Acc@1 79.070 Acc@5 98.920
### epoch[13] execution time: 28.139741897583008
EPOCH 14
# current learning rate: 0.1
# Switched to train mode...
Epoch: [14][  0/391]	Time  0.253 ( 0.253)	Data  0.187 ( 0.187)	Loss 5.2619e-01 (5.2619e-01)	Acc@1  82.81 ( 82.81)	Acc@5  99.22 ( 99.22)
Epoch: [14][ 10/391]	Time  0.063 ( 0.081)	Data  0.001 ( 0.018)	Loss 4.3378e-01 (4.6118e-01)	Acc@1  82.81 ( 84.16)	Acc@5 100.00 ( 99.36)
Epoch: [14][ 20/391]	Time  0.064 ( 0.073)	Data  0.001 ( 0.010)	Loss 3.8589e-01 (4.6395e-01)	Acc@1  87.50 ( 83.97)	Acc@5  99.22 ( 99.40)
Epoch: [14][ 30/391]	Time  0.067 ( 0.070)	Data  0.001 ( 0.007)	Loss 3.9946e-01 (4.7371e-01)	Acc@1  84.38 ( 83.74)	Acc@5  99.22 ( 99.37)
Epoch: [14][ 40/391]	Time  0.062 ( 0.069)	Data  0.001 ( 0.006)	Loss 4.5846e-01 (4.7436e-01)	Acc@1  85.94 ( 83.50)	Acc@5  99.22 ( 99.37)
Epoch: [14][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.005)	Loss 4.6435e-01 (4.6706e-01)	Acc@1  85.16 ( 83.82)	Acc@5  99.22 ( 99.34)
Epoch: [14][ 60/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.004)	Loss 6.8935e-01 (4.6720e-01)	Acc@1  77.34 ( 83.81)	Acc@5  99.22 ( 99.35)
Epoch: [14][ 70/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.004)	Loss 4.3369e-01 (4.6592e-01)	Acc@1  85.94 ( 83.87)	Acc@5  99.22 ( 99.39)
Epoch: [14][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.4787e-01 (4.6424e-01)	Acc@1  83.59 ( 83.83)	Acc@5  98.44 ( 99.37)
Epoch: [14][ 90/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 5.3123e-01 (4.7335e-01)	Acc@1  83.59 ( 83.58)	Acc@5  99.22 ( 99.32)
Epoch: [14][100/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.003)	Loss 5.1933e-01 (4.7932e-01)	Acc@1  82.81 ( 83.44)	Acc@5  99.22 ( 99.28)
Epoch: [14][110/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 7.6036e-01 (4.8600e-01)	Acc@1  73.44 ( 83.23)	Acc@5  97.66 ( 99.23)
Epoch: [14][120/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.7542e-01 (4.8655e-01)	Acc@1  84.38 ( 83.16)	Acc@5 100.00 ( 99.23)
Epoch: [14][130/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 5.2082e-01 (4.8363e-01)	Acc@1  82.03 ( 83.22)	Acc@5  99.22 ( 99.25)
Epoch: [14][140/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.1657e-01 (4.8222e-01)	Acc@1  83.59 ( 83.27)	Acc@5  99.22 ( 99.25)
Epoch: [14][150/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.2732e-01 (4.8155e-01)	Acc@1  84.38 ( 83.30)	Acc@5  99.22 ( 99.25)
Epoch: [14][160/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.1680e-01 (4.8415e-01)	Acc@1  89.06 ( 83.22)	Acc@5  99.22 ( 99.21)
Epoch: [14][170/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.8531e-01 (4.8418e-01)	Acc@1  82.81 ( 83.21)	Acc@5  99.22 ( 99.23)
Epoch: [14][180/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.8897e-01 (4.8323e-01)	Acc@1  82.03 ( 83.23)	Acc@5  98.44 ( 99.24)
Epoch: [14][190/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.5381e-01 (4.8323e-01)	Acc@1  86.72 ( 83.23)	Acc@5  99.22 ( 99.23)
Epoch: [14][200/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.5932e-01 (4.8227e-01)	Acc@1  83.59 ( 83.24)	Acc@5  98.44 ( 99.23)
Epoch: [14][210/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.5511e-01 (4.8255e-01)	Acc@1  84.38 ( 83.26)	Acc@5 100.00 ( 99.23)
Epoch: [14][220/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.0217e-01 (4.8232e-01)	Acc@1  83.59 ( 83.27)	Acc@5 100.00 ( 99.25)
Epoch: [14][230/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.7894e-01 (4.8352e-01)	Acc@1  83.59 ( 83.27)	Acc@5  99.22 ( 99.26)
Epoch: [14][240/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.2002e-01 (4.8505e-01)	Acc@1  81.25 ( 83.21)	Acc@5 100.00 ( 99.25)
Epoch: [14][250/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.0856e-01 (4.8526e-01)	Acc@1  84.38 ( 83.27)	Acc@5  98.44 ( 99.26)
Epoch: [14][260/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.7904e-01 (4.8608e-01)	Acc@1  86.72 ( 83.25)	Acc@5  99.22 ( 99.25)
Epoch: [14][270/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.2548e-01 (4.8696e-01)	Acc@1  84.38 ( 83.23)	Acc@5  99.22 ( 99.25)
Epoch: [14][280/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.4431e-01 (4.8509e-01)	Acc@1  85.16 ( 83.32)	Acc@5 100.00 ( 99.26)
Epoch: [14][290/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.1465e-01 (4.8429e-01)	Acc@1  79.69 ( 83.32)	Acc@5  99.22 ( 99.27)
Epoch: [14][300/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.5311e-01 (4.8429e-01)	Acc@1  84.38 ( 83.31)	Acc@5  99.22 ( 99.26)
Epoch: [14][310/391]	Time  0.059 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.0311e-01 (4.8566e-01)	Acc@1  79.69 ( 83.30)	Acc@5  99.22 ( 99.24)
Epoch: [14][320/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.3194e-01 (4.8646e-01)	Acc@1  78.12 ( 83.25)	Acc@5  98.44 ( 99.23)
Epoch: [14][330/391]	Time  0.059 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.3716e-01 (4.8916e-01)	Acc@1  77.34 ( 83.17)	Acc@5  98.44 ( 99.22)
Epoch: [14][340/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.9149e-01 (4.8878e-01)	Acc@1  86.72 ( 83.19)	Acc@5 100.00 ( 99.22)
Epoch: [14][350/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.1511e-01 (4.8865e-01)	Acc@1  82.81 ( 83.21)	Acc@5 100.00 ( 99.23)
Epoch: [14][360/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.5374e-01 (4.9026e-01)	Acc@1  84.38 ( 83.17)	Acc@5  99.22 ( 99.23)
Epoch: [14][370/391]	Time  0.060 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.4045e-01 (4.8952e-01)	Acc@1  84.38 ( 83.19)	Acc@5 100.00 ( 99.23)
Epoch: [14][380/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.4366e-01 (4.8995e-01)	Acc@1  82.81 ( 83.17)	Acc@5  99.22 ( 99.22)
Epoch: [14][390/391]	Time  0.049 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.7705e-01 (4.8945e-01)	Acc@1  82.50 ( 83.18)	Acc@5 100.00 ( 99.22)
## e[14] optimizer.zero_grad (sum) time: 0.3996577262878418
## e[14]       loss.backward (sum) time: 6.9422407150268555
## e[14]      optimizer.step (sum) time: 3.343097448348999
## epoch[14] training(only) time: 25.210521936416626
# Switched to evaluate mode...
Test: [  0/100]	Time  0.190 ( 0.190)	Loss 6.0027e-01 (6.0027e-01)	Acc@1  82.00 ( 82.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.024 ( 0.041)	Loss 5.2295e-01 (5.1809e-01)	Acc@1  88.00 ( 82.27)	Acc@5  99.00 ( 99.27)
Test: [ 20/100]	Time  0.028 ( 0.034)	Loss 4.6208e-01 (5.3015e-01)	Acc@1  83.00 ( 82.19)	Acc@5  99.00 ( 99.05)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 6.3103e-01 (5.4256e-01)	Acc@1  80.00 ( 81.94)	Acc@5  98.00 ( 98.90)
Test: [ 40/100]	Time  0.024 ( 0.030)	Loss 5.4701e-01 (5.4174e-01)	Acc@1  86.00 ( 82.05)	Acc@5  97.00 ( 98.80)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 4.3644e-01 (5.3912e-01)	Acc@1  82.00 ( 82.14)	Acc@5  99.00 ( 98.82)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 5.9065e-01 (5.4251e-01)	Acc@1  78.00 ( 81.90)	Acc@5 100.00 ( 98.87)
Test: [ 70/100]	Time  0.028 ( 0.028)	Loss 6.8679e-01 (5.4071e-01)	Acc@1  77.00 ( 81.72)	Acc@5 100.00 ( 98.92)
Test: [ 80/100]	Time  0.024 ( 0.028)	Loss 4.6000e-01 (5.3640e-01)	Acc@1  87.00 ( 81.84)	Acc@5  98.00 ( 98.93)
Test: [ 90/100]	Time  0.025 ( 0.027)	Loss 4.4355e-01 (5.3772e-01)	Acc@1  84.00 ( 81.88)	Acc@5 100.00 ( 98.88)
 * Acc@1 82.000 Acc@5 98.930
### epoch[14] execution time: 28.059337377548218
EPOCH 15
# current learning rate: 0.1
# Switched to train mode...
Epoch: [15][  0/391]	Time  0.243 ( 0.243)	Data  0.181 ( 0.181)	Loss 4.0952e-01 (4.0952e-01)	Acc@1  87.50 ( 87.50)	Acc@5 100.00 (100.00)
Epoch: [15][ 10/391]	Time  0.069 ( 0.081)	Data  0.001 ( 0.017)	Loss 4.4821e-01 (4.7549e-01)	Acc@1  85.16 ( 83.17)	Acc@5  99.22 ( 99.43)
Epoch: [15][ 20/391]	Time  0.065 ( 0.073)	Data  0.001 ( 0.010)	Loss 5.0471e-01 (4.7342e-01)	Acc@1  83.59 ( 83.52)	Acc@5  98.44 ( 99.37)
Epoch: [15][ 30/391]	Time  0.062 ( 0.070)	Data  0.001 ( 0.007)	Loss 3.8993e-01 (4.6899e-01)	Acc@1  85.94 ( 83.64)	Acc@5 100.00 ( 99.29)
Epoch: [15][ 40/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.006)	Loss 3.6266e-01 (4.6698e-01)	Acc@1  89.06 ( 83.92)	Acc@5  98.44 ( 99.29)
Epoch: [15][ 50/391]	Time  0.059 ( 0.068)	Data  0.001 ( 0.005)	Loss 4.0265e-01 (4.6768e-01)	Acc@1  83.59 ( 83.67)	Acc@5 100.00 ( 99.34)
Epoch: [15][ 60/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.004)	Loss 5.9045e-01 (4.6455e-01)	Acc@1  80.47 ( 83.88)	Acc@5  98.44 ( 99.30)
Epoch: [15][ 70/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.004)	Loss 4.8119e-01 (4.7064e-01)	Acc@1  85.16 ( 83.73)	Acc@5  99.22 ( 99.24)
Epoch: [15][ 80/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.1113e-01 (4.6986e-01)	Acc@1  82.81 ( 83.83)	Acc@5 100.00 ( 99.23)
Epoch: [15][ 90/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.7732e-01 (4.7229e-01)	Acc@1  85.16 ( 83.72)	Acc@5  99.22 ( 99.20)
Epoch: [15][100/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.4987e-01 (4.7258e-01)	Acc@1  85.16 ( 83.75)	Acc@5  99.22 ( 99.18)
Epoch: [15][110/391]	Time  0.060 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.8507e-01 (4.7282e-01)	Acc@1  87.50 ( 83.80)	Acc@5 100.00 ( 99.24)
Epoch: [15][120/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.0997e-01 (4.7198e-01)	Acc@1  82.03 ( 83.85)	Acc@5 100.00 ( 99.24)
Epoch: [15][130/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.0610e-01 (4.6980e-01)	Acc@1  87.50 ( 83.93)	Acc@5  99.22 ( 99.22)
Epoch: [15][140/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.3281e-01 (4.7283e-01)	Acc@1  89.06 ( 83.87)	Acc@5 100.00 ( 99.21)
Epoch: [15][150/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.7257e-01 (4.7180e-01)	Acc@1  83.59 ( 83.97)	Acc@5  99.22 ( 99.20)
Epoch: [15][160/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.6760e-01 (4.7086e-01)	Acc@1  83.59 ( 84.04)	Acc@5  99.22 ( 99.21)
Epoch: [15][170/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.8684e-01 (4.7208e-01)	Acc@1  85.16 ( 83.91)	Acc@5 100.00 ( 99.21)
Epoch: [15][180/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.0859e-01 (4.7159e-01)	Acc@1  83.59 ( 83.91)	Acc@5 100.00 ( 99.21)
Epoch: [15][190/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.1203e-01 (4.7104e-01)	Acc@1  85.94 ( 83.90)	Acc@5 100.00 ( 99.22)
Epoch: [15][200/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.8227e-01 (4.7207e-01)	Acc@1  83.59 ( 83.86)	Acc@5  99.22 ( 99.22)
Epoch: [15][210/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.4155e-01 (4.7254e-01)	Acc@1  75.78 ( 83.81)	Acc@5  97.66 ( 99.22)
Epoch: [15][220/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.3446e-01 (4.7388e-01)	Acc@1  86.72 ( 83.77)	Acc@5 100.00 ( 99.23)
Epoch: [15][230/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.8269e-01 (4.7367e-01)	Acc@1  82.81 ( 83.76)	Acc@5 100.00 ( 99.23)
Epoch: [15][240/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.0366e-01 (4.7280e-01)	Acc@1  85.16 ( 83.80)	Acc@5 100.00 ( 99.23)
Epoch: [15][250/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.7098e-01 (4.7127e-01)	Acc@1  82.81 ( 83.84)	Acc@5  98.44 ( 99.24)
Epoch: [15][260/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.0078e-01 (4.7064e-01)	Acc@1  88.28 ( 83.86)	Acc@5  99.22 ( 99.25)
Epoch: [15][270/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.6488e-01 (4.7058e-01)	Acc@1  79.69 ( 83.85)	Acc@5  99.22 ( 99.25)
Epoch: [15][280/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.7272e-01 (4.7114e-01)	Acc@1  84.38 ( 83.86)	Acc@5  98.44 ( 99.25)
Epoch: [15][290/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.5923e-01 (4.7160e-01)	Acc@1  82.03 ( 83.86)	Acc@5  99.22 ( 99.25)
Epoch: [15][300/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.4282e-01 (4.7198e-01)	Acc@1  82.81 ( 83.85)	Acc@5  99.22 ( 99.24)
Epoch: [15][310/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.0817e-01 (4.7296e-01)	Acc@1  84.38 ( 83.81)	Acc@5  99.22 ( 99.24)
Epoch: [15][320/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.4898e-01 (4.7373e-01)	Acc@1  78.91 ( 83.80)	Acc@5  96.09 ( 99.24)
Epoch: [15][330/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.8158e-01 (4.7362e-01)	Acc@1  78.91 ( 83.79)	Acc@5 100.00 ( 99.25)
Epoch: [15][340/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.6207e-01 (4.7329e-01)	Acc@1  88.28 ( 83.80)	Acc@5  99.22 ( 99.26)
Epoch: [15][350/391]	Time  0.060 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.0613e-01 (4.7352e-01)	Acc@1  87.50 ( 83.81)	Acc@5  99.22 ( 99.27)
Epoch: [15][360/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.8130e-01 (4.7383e-01)	Acc@1  82.81 ( 83.77)	Acc@5 100.00 ( 99.28)
Epoch: [15][370/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.6620e-01 (4.7386e-01)	Acc@1  85.16 ( 83.75)	Acc@5  99.22 ( 99.28)
Epoch: [15][380/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.7986e-01 (4.7401e-01)	Acc@1  82.81 ( 83.73)	Acc@5  99.22 ( 99.27)
Epoch: [15][390/391]	Time  0.046 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.4029e-01 (4.7488e-01)	Acc@1  76.25 ( 83.67)	Acc@5  98.75 ( 99.26)
## e[15] optimizer.zero_grad (sum) time: 0.4059152603149414
## e[15]       loss.backward (sum) time: 6.9405951499938965
## e[15]      optimizer.step (sum) time: 3.3562183380126953
## epoch[15] training(only) time: 25.246116876602173
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 5.6085e-01 (5.6085e-01)	Acc@1  78.00 ( 78.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.025 ( 0.042)	Loss 6.4780e-01 (6.5577e-01)	Acc@1  80.00 ( 77.73)	Acc@5  99.00 ( 99.45)
Test: [ 20/100]	Time  0.024 ( 0.034)	Loss 5.8533e-01 (6.5608e-01)	Acc@1  76.00 ( 77.38)	Acc@5 100.00 ( 99.00)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 7.8563e-01 (6.7814e-01)	Acc@1  73.00 ( 77.03)	Acc@5  99.00 ( 98.74)
Test: [ 40/100]	Time  0.024 ( 0.030)	Loss 5.6350e-01 (6.7567e-01)	Acc@1  81.00 ( 77.34)	Acc@5  98.00 ( 98.49)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 5.7340e-01 (6.7530e-01)	Acc@1  84.00 ( 77.59)	Acc@5  98.00 ( 98.55)
Test: [ 60/100]	Time  0.030 ( 0.028)	Loss 6.2432e-01 (6.7409e-01)	Acc@1  83.00 ( 77.52)	Acc@5 100.00 ( 98.62)
Test: [ 70/100]	Time  0.026 ( 0.028)	Loss 7.7340e-01 (6.7542e-01)	Acc@1  73.00 ( 77.75)	Acc@5  96.00 ( 98.61)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 4.5150e-01 (6.6621e-01)	Acc@1  81.00 ( 77.88)	Acc@5 100.00 ( 98.73)
Test: [ 90/100]	Time  0.024 ( 0.028)	Loss 4.2245e-01 (6.7120e-01)	Acc@1  85.00 ( 77.67)	Acc@5 100.00 ( 98.73)
 * Acc@1 77.620 Acc@5 98.710
### epoch[15] execution time: 28.146394968032837
EPOCH 16
# current learning rate: 0.1
# Switched to train mode...
Epoch: [16][  0/391]	Time  0.254 ( 0.254)	Data  0.186 ( 0.186)	Loss 5.6377e-01 (5.6377e-01)	Acc@1  81.25 ( 81.25)	Acc@5  99.22 ( 99.22)
Epoch: [16][ 10/391]	Time  0.062 ( 0.081)	Data  0.001 ( 0.018)	Loss 4.4276e-01 (4.9238e-01)	Acc@1  85.94 ( 82.74)	Acc@5 100.00 ( 99.08)
Epoch: [16][ 20/391]	Time  0.064 ( 0.073)	Data  0.001 ( 0.010)	Loss 3.8169e-01 (4.8221e-01)	Acc@1  87.50 ( 82.92)	Acc@5  98.44 ( 99.14)
Epoch: [16][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.007)	Loss 4.7038e-01 (4.7981e-01)	Acc@1  82.03 ( 82.96)	Acc@5  99.22 ( 99.29)
Epoch: [16][ 40/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.006)	Loss 5.6765e-01 (4.6805e-01)	Acc@1  75.78 ( 83.40)	Acc@5 100.00 ( 99.39)
Epoch: [16][ 50/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.005)	Loss 4.0563e-01 (4.6751e-01)	Acc@1  85.16 ( 83.76)	Acc@5  99.22 ( 99.37)
Epoch: [16][ 60/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 3.6131e-01 (4.6696e-01)	Acc@1  85.94 ( 83.75)	Acc@5  99.22 ( 99.35)
Epoch: [16][ 70/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.004)	Loss 4.6462e-01 (4.6491e-01)	Acc@1  81.25 ( 83.76)	Acc@5 100.00 ( 99.37)
Epoch: [16][ 80/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.0705e-01 (4.6001e-01)	Acc@1  85.16 ( 83.87)	Acc@5 100.00 ( 99.39)
Epoch: [16][ 90/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.3486e-01 (4.5662e-01)	Acc@1  87.50 ( 84.07)	Acc@5  99.22 ( 99.42)
Epoch: [16][100/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.2585e-01 (4.5294e-01)	Acc@1  86.72 ( 84.27)	Acc@5  99.22 ( 99.44)
Epoch: [16][110/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.003)	Loss 5.0696e-01 (4.5405e-01)	Acc@1  80.47 ( 84.24)	Acc@5 100.00 ( 99.43)
Epoch: [16][120/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.5735e-01 (4.5928e-01)	Acc@1  85.16 ( 84.07)	Acc@5 100.00 ( 99.40)
Epoch: [16][130/391]	Time  0.060 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.1100e-01 (4.5882e-01)	Acc@1  88.28 ( 84.15)	Acc@5  99.22 ( 99.41)
Epoch: [16][140/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.3668e-01 (4.6027e-01)	Acc@1  86.72 ( 84.16)	Acc@5  99.22 ( 99.40)
Epoch: [16][150/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.1330e-01 (4.6122e-01)	Acc@1  85.16 ( 84.15)	Acc@5  99.22 ( 99.38)
Epoch: [16][160/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.6525e-01 (4.6075e-01)	Acc@1  84.38 ( 84.21)	Acc@5  99.22 ( 99.38)
Epoch: [16][170/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.2582e-01 (4.6079e-01)	Acc@1  83.59 ( 84.21)	Acc@5 100.00 ( 99.38)
Epoch: [16][180/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.7435e-01 (4.6111e-01)	Acc@1  87.50 ( 84.26)	Acc@5 100.00 ( 99.38)
Epoch: [16][190/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.3235e-01 (4.6218e-01)	Acc@1  85.16 ( 84.26)	Acc@5 100.00 ( 99.37)
Epoch: [16][200/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.7969e-01 (4.6113e-01)	Acc@1  89.06 ( 84.31)	Acc@5  98.44 ( 99.36)
Epoch: [16][210/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.4283e-01 (4.6127e-01)	Acc@1  85.94 ( 84.24)	Acc@5  99.22 ( 99.37)
Epoch: [16][220/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.8953e-01 (4.6225e-01)	Acc@1  82.81 ( 84.19)	Acc@5 100.00 ( 99.38)
Epoch: [16][230/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.4698e-01 (4.6121e-01)	Acc@1  86.72 ( 84.22)	Acc@5  99.22 ( 99.37)
Epoch: [16][240/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.0344e-01 (4.6184e-01)	Acc@1  80.47 ( 84.22)	Acc@5 100.00 ( 99.37)
Epoch: [16][250/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.6081e-01 (4.6309e-01)	Acc@1  83.59 ( 84.19)	Acc@5  99.22 ( 99.37)
Epoch: [16][260/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.4612e-01 (4.6234e-01)	Acc@1  91.41 ( 84.18)	Acc@5 100.00 ( 99.38)
Epoch: [16][270/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.4559e-01 (4.6075e-01)	Acc@1  82.81 ( 84.23)	Acc@5  96.88 ( 99.38)
Epoch: [16][280/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.2641e-01 (4.6190e-01)	Acc@1  85.16 ( 84.19)	Acc@5  98.44 ( 99.39)
Epoch: [16][290/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.0680e-01 (4.6249e-01)	Acc@1  85.94 ( 84.18)	Acc@5  99.22 ( 99.39)
Epoch: [16][300/391]	Time  0.059 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.5586e-01 (4.6216e-01)	Acc@1  85.16 ( 84.18)	Acc@5  98.44 ( 99.39)
Epoch: [16][310/391]	Time  0.066 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.3191e-01 (4.6321e-01)	Acc@1  85.94 ( 84.16)	Acc@5  99.22 ( 99.38)
Epoch: [16][320/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.6086e-01 (4.6226e-01)	Acc@1  82.03 ( 84.20)	Acc@5  98.44 ( 99.37)
Epoch: [16][330/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.4418e-01 (4.6239e-01)	Acc@1  86.72 ( 84.21)	Acc@5 100.00 ( 99.37)
Epoch: [16][340/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.8856e-01 (4.6321e-01)	Acc@1  77.34 ( 84.18)	Acc@5  99.22 ( 99.36)
Epoch: [16][350/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.9945e-01 (4.6460e-01)	Acc@1  83.59 ( 84.14)	Acc@5  97.66 ( 99.35)
Epoch: [16][360/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.6383e-01 (4.6491e-01)	Acc@1  79.69 ( 84.12)	Acc@5  98.44 ( 99.34)
Epoch: [16][370/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.0689e-01 (4.6407e-01)	Acc@1  90.62 ( 84.16)	Acc@5  99.22 ( 99.34)
Epoch: [16][380/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.5134e-01 (4.6384e-01)	Acc@1  85.16 ( 84.15)	Acc@5  99.22 ( 99.34)
Epoch: [16][390/391]	Time  0.046 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.2033e-01 (4.6262e-01)	Acc@1  87.50 ( 84.18)	Acc@5  98.75 ( 99.35)
## e[16] optimizer.zero_grad (sum) time: 0.4063410758972168
## e[16]       loss.backward (sum) time: 6.977890491485596
## e[16]      optimizer.step (sum) time: 3.3612418174743652
## epoch[16] training(only) time: 25.257497549057007
# Switched to evaluate mode...
Test: [  0/100]	Time  0.198 ( 0.198)	Loss 6.2644e-01 (6.2644e-01)	Acc@1  80.00 ( 80.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.027 ( 0.042)	Loss 6.7246e-01 (5.8766e-01)	Acc@1  80.00 ( 80.55)	Acc@5  99.00 ( 99.27)
Test: [ 20/100]	Time  0.024 ( 0.034)	Loss 5.2759e-01 (5.6119e-01)	Acc@1  77.00 ( 80.86)	Acc@5  99.00 ( 99.33)
Test: [ 30/100]	Time  0.024 ( 0.032)	Loss 4.5567e-01 (5.6043e-01)	Acc@1  83.00 ( 81.32)	Acc@5  99.00 ( 99.23)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 4.7439e-01 (5.6880e-01)	Acc@1  85.00 ( 81.12)	Acc@5  99.00 ( 99.10)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 4.1352e-01 (5.6684e-01)	Acc@1  83.00 ( 81.24)	Acc@5  98.00 ( 99.10)
Test: [ 60/100]	Time  0.024 ( 0.029)	Loss 5.0486e-01 (5.6365e-01)	Acc@1  80.00 ( 81.18)	Acc@5 100.00 ( 99.20)
Test: [ 70/100]	Time  0.030 ( 0.028)	Loss 6.3053e-01 (5.6465e-01)	Acc@1  83.00 ( 81.27)	Acc@5  99.00 ( 99.24)
Test: [ 80/100]	Time  0.024 ( 0.028)	Loss 4.2180e-01 (5.5977e-01)	Acc@1  83.00 ( 81.40)	Acc@5 100.00 ( 99.30)
Test: [ 90/100]	Time  0.027 ( 0.028)	Loss 5.4028e-01 (5.6519e-01)	Acc@1  78.00 ( 81.23)	Acc@5 100.00 ( 99.29)
 * Acc@1 81.230 Acc@5 99.250
### epoch[16] execution time: 28.124675273895264
EPOCH 17
# current learning rate: 0.1
# Switched to train mode...
Epoch: [17][  0/391]	Time  0.252 ( 0.252)	Data  0.185 ( 0.185)	Loss 3.7193e-01 (3.7193e-01)	Acc@1  89.84 ( 89.84)	Acc@5 100.00 (100.00)
Epoch: [17][ 10/391]	Time  0.059 ( 0.079)	Data  0.001 ( 0.018)	Loss 3.2570e-01 (4.1415e-01)	Acc@1  88.28 ( 86.29)	Acc@5 100.00 ( 99.57)
Epoch: [17][ 20/391]	Time  0.064 ( 0.073)	Data  0.001 ( 0.010)	Loss 4.0059e-01 (4.3109e-01)	Acc@1  84.38 ( 85.68)	Acc@5  99.22 ( 99.48)
Epoch: [17][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.007)	Loss 5.3223e-01 (4.4289e-01)	Acc@1  82.03 ( 85.23)	Acc@5 100.00 ( 99.52)
Epoch: [17][ 40/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.006)	Loss 3.6956e-01 (4.4431e-01)	Acc@1  89.06 ( 85.10)	Acc@5 100.00 ( 99.49)
Epoch: [17][ 50/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.005)	Loss 5.1500e-01 (4.4625e-01)	Acc@1  84.38 ( 85.02)	Acc@5 100.00 ( 99.46)
Epoch: [17][ 60/391]	Time  0.060 ( 0.067)	Data  0.001 ( 0.004)	Loss 3.9755e-01 (4.4608e-01)	Acc@1  84.38 ( 84.96)	Acc@5 100.00 ( 99.49)
Epoch: [17][ 70/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.004)	Loss 3.2445e-01 (4.4360e-01)	Acc@1  85.94 ( 85.04)	Acc@5 100.00 ( 99.50)
Epoch: [17][ 80/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.9276e-01 (4.4275e-01)	Acc@1  85.16 ( 85.07)	Acc@5  99.22 ( 99.52)
Epoch: [17][ 90/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.8142e-01 (4.4606e-01)	Acc@1  82.81 ( 84.91)	Acc@5  99.22 ( 99.48)
Epoch: [17][100/391]	Time  0.073 ( 0.065)	Data  0.001 ( 0.003)	Loss 5.8814e-01 (4.4759e-01)	Acc@1  79.69 ( 84.82)	Acc@5 100.00 ( 99.49)
Epoch: [17][110/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.003)	Loss 3.6171e-01 (4.4496e-01)	Acc@1  88.28 ( 84.88)	Acc@5 100.00 ( 99.51)
Epoch: [17][120/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 4.0505e-01 (4.4520e-01)	Acc@1  86.72 ( 84.92)	Acc@5 100.00 ( 99.50)
Epoch: [17][130/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.003)	Loss 4.1304e-01 (4.4470e-01)	Acc@1  85.16 ( 84.89)	Acc@5 100.00 ( 99.49)
Epoch: [17][140/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.5695e-01 (4.4622e-01)	Acc@1  84.38 ( 84.88)	Acc@5  97.66 ( 99.46)
Epoch: [17][150/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.6673e-01 (4.4735e-01)	Acc@1  82.03 ( 84.80)	Acc@5 100.00 ( 99.45)
Epoch: [17][160/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.5061e-01 (4.4840e-01)	Acc@1  90.62 ( 84.72)	Acc@5  99.22 ( 99.43)
Epoch: [17][170/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.8375e-01 (4.4799e-01)	Acc@1  85.94 ( 84.73)	Acc@5 100.00 ( 99.44)
Epoch: [17][180/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.9656e-01 (4.4949e-01)	Acc@1  89.06 ( 84.69)	Acc@5 100.00 ( 99.43)
Epoch: [17][190/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.8391e-01 (4.4887e-01)	Acc@1  87.50 ( 84.69)	Acc@5  99.22 ( 99.43)
Epoch: [17][200/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.0381e-01 (4.4794e-01)	Acc@1  83.59 ( 84.69)	Acc@5 100.00 ( 99.43)
Epoch: [17][210/391]	Time  0.075 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.8499e-01 (4.4939e-01)	Acc@1  85.94 ( 84.57)	Acc@5 100.00 ( 99.43)
Epoch: [17][220/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.3545e-01 (4.5073e-01)	Acc@1  80.47 ( 84.51)	Acc@5  99.22 ( 99.43)
Epoch: [17][230/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.2942e-01 (4.5114e-01)	Acc@1  78.12 ( 84.51)	Acc@5  99.22 ( 99.43)
Epoch: [17][240/391]	Time  0.074 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.3496e-01 (4.5179e-01)	Acc@1  78.12 ( 84.47)	Acc@5 100.00 ( 99.43)
Epoch: [17][250/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.6464e-01 (4.5272e-01)	Acc@1  76.56 ( 84.44)	Acc@5 100.00 ( 99.42)
Epoch: [17][260/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.1050e-01 (4.5365e-01)	Acc@1  85.16 ( 84.41)	Acc@5 100.00 ( 99.40)
Epoch: [17][270/391]	Time  0.067 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.0918e-01 (4.5392e-01)	Acc@1  79.69 ( 84.36)	Acc@5 100.00 ( 99.41)
Epoch: [17][280/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.3520e-01 (4.5329e-01)	Acc@1  85.94 ( 84.43)	Acc@5 100.00 ( 99.41)
Epoch: [17][290/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.1379e-01 (4.5416e-01)	Acc@1  82.81 ( 84.40)	Acc@5  97.66 ( 99.39)
Epoch: [17][300/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.8498e-01 (4.5423e-01)	Acc@1  79.69 ( 84.36)	Acc@5  98.44 ( 99.39)
Epoch: [17][310/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.5932e-01 (4.5477e-01)	Acc@1  82.03 ( 84.34)	Acc@5 100.00 ( 99.38)
Epoch: [17][320/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.8781e-01 (4.5459e-01)	Acc@1  83.59 ( 84.35)	Acc@5 100.00 ( 99.38)
Epoch: [17][330/391]	Time  0.060 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.9047e-01 (4.5550e-01)	Acc@1  85.16 ( 84.33)	Acc@5 100.00 ( 99.39)
Epoch: [17][340/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.8735e-01 (4.5524e-01)	Acc@1  85.16 ( 84.35)	Acc@5  97.66 ( 99.39)
Epoch: [17][350/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.5980e-01 (4.5733e-01)	Acc@1  83.59 ( 84.29)	Acc@5  99.22 ( 99.38)
Epoch: [17][360/391]	Time  0.059 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.4837e-01 (4.5727e-01)	Acc@1  81.25 ( 84.28)	Acc@5 100.00 ( 99.38)
Epoch: [17][370/391]	Time  0.071 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.0725e-01 (4.5797e-01)	Acc@1  87.50 ( 84.24)	Acc@5 100.00 ( 99.38)
Epoch: [17][380/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.3063e-01 (4.5875e-01)	Acc@1  79.69 ( 84.19)	Acc@5  98.44 ( 99.39)
Epoch: [17][390/391]	Time  0.046 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.0037e-01 (4.5883e-01)	Acc@1  82.50 ( 84.18)	Acc@5  98.75 ( 99.39)
## e[17] optimizer.zero_grad (sum) time: 0.3956475257873535
## e[17]       loss.backward (sum) time: 6.892603158950806
## e[17]      optimizer.step (sum) time: 3.345377206802368
## epoch[17] training(only) time: 25.091392993927002
# Switched to evaluate mode...
Test: [  0/100]	Time  0.194 ( 0.194)	Loss 5.1563e-01 (5.1563e-01)	Acc@1  82.00 ( 82.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.041)	Loss 5.2071e-01 (4.9966e-01)	Acc@1  89.00 ( 83.27)	Acc@5  98.00 ( 99.55)
Test: [ 20/100]	Time  0.024 ( 0.034)	Loss 4.8971e-01 (4.9969e-01)	Acc@1  82.00 ( 83.05)	Acc@5 100.00 ( 99.29)
Test: [ 30/100]	Time  0.024 ( 0.032)	Loss 4.7606e-01 (5.1364e-01)	Acc@1  83.00 ( 82.68)	Acc@5  99.00 ( 99.10)
Test: [ 40/100]	Time  0.024 ( 0.030)	Loss 4.1293e-01 (5.1999e-01)	Acc@1  89.00 ( 82.37)	Acc@5  99.00 ( 99.00)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 3.6896e-01 (5.1273e-01)	Acc@1  90.00 ( 82.73)	Acc@5  99.00 ( 98.96)
Test: [ 60/100]	Time  0.028 ( 0.029)	Loss 5.2500e-01 (5.1589e-01)	Acc@1  81.00 ( 82.54)	Acc@5 100.00 ( 99.02)
Test: [ 70/100]	Time  0.026 ( 0.028)	Loss 6.0588e-01 (5.1393e-01)	Acc@1  81.00 ( 82.63)	Acc@5  99.00 ( 99.01)
Test: [ 80/100]	Time  0.026 ( 0.028)	Loss 4.0698e-01 (5.1003e-01)	Acc@1  84.00 ( 82.67)	Acc@5  99.00 ( 99.09)
Test: [ 90/100]	Time  0.027 ( 0.028)	Loss 5.0409e-01 (5.1151e-01)	Acc@1  79.00 ( 82.65)	Acc@5 100.00 ( 99.11)
 * Acc@1 82.580 Acc@5 99.060
### epoch[17] execution time: 27.997687578201294
EPOCH 18
# current learning rate: 0.1
# Switched to train mode...
Epoch: [18][  0/391]	Time  0.246 ( 0.246)	Data  0.181 ( 0.181)	Loss 2.5933e-01 (2.5933e-01)	Acc@1  91.41 ( 91.41)	Acc@5 100.00 (100.00)
Epoch: [18][ 10/391]	Time  0.065 ( 0.079)	Data  0.001 ( 0.017)	Loss 4.6465e-01 (4.2389e-01)	Acc@1  84.38 ( 85.30)	Acc@5 100.00 ( 99.50)
Epoch: [18][ 20/391]	Time  0.066 ( 0.072)	Data  0.001 ( 0.010)	Loss 4.9843e-01 (4.4297e-01)	Acc@1  80.47 ( 84.38)	Acc@5 100.00 ( 99.48)
Epoch: [18][ 30/391]	Time  0.060 ( 0.069)	Data  0.001 ( 0.007)	Loss 3.6743e-01 (4.4477e-01)	Acc@1  91.41 ( 84.63)	Acc@5 100.00 ( 99.42)
Epoch: [18][ 40/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.005)	Loss 4.0210e-01 (4.4730e-01)	Acc@1  88.28 ( 84.72)	Acc@5  99.22 ( 99.33)
Epoch: [18][ 50/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.005)	Loss 3.5979e-01 (4.4785e-01)	Acc@1  87.50 ( 84.56)	Acc@5 100.00 ( 99.34)
Epoch: [18][ 60/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 3.5350e-01 (4.4741e-01)	Acc@1  85.94 ( 84.66)	Acc@5  99.22 ( 99.35)
Epoch: [18][ 70/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.004)	Loss 3.8625e-01 (4.5095e-01)	Acc@1  84.38 ( 84.51)	Acc@5 100.00 ( 99.33)
Epoch: [18][ 80/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.5646e-01 (4.4881e-01)	Acc@1  85.94 ( 84.53)	Acc@5  97.66 ( 99.31)
Epoch: [18][ 90/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.5496e-01 (4.4470e-01)	Acc@1  89.06 ( 84.58)	Acc@5  99.22 ( 99.32)
Epoch: [18][100/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.9137e-01 (4.4434e-01)	Acc@1  88.28 ( 84.67)	Acc@5 100.00 ( 99.33)
Epoch: [18][110/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 5.4590e-01 (4.4628e-01)	Acc@1  79.69 ( 84.65)	Acc@5  98.44 ( 99.32)
Epoch: [18][120/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.8978e-01 (4.4541e-01)	Acc@1  87.50 ( 84.70)	Acc@5 100.00 ( 99.33)
Epoch: [18][130/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.4032e-01 (4.4535e-01)	Acc@1  89.06 ( 84.70)	Acc@5  99.22 ( 99.34)
Epoch: [18][140/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.4881e-01 (4.4582e-01)	Acc@1  83.59 ( 84.63)	Acc@5  99.22 ( 99.35)
Epoch: [18][150/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.0204e-01 (4.4704e-01)	Acc@1  88.28 ( 84.63)	Acc@5 100.00 ( 99.34)
Epoch: [18][160/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.6419e-01 (4.4780e-01)	Acc@1  86.72 ( 84.59)	Acc@5  99.22 ( 99.36)
Epoch: [18][170/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.0024e-01 (4.5064e-01)	Acc@1  83.59 ( 84.49)	Acc@5  99.22 ( 99.33)
Epoch: [18][180/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.7918e-01 (4.4832e-01)	Acc@1  85.16 ( 84.57)	Acc@5 100.00 ( 99.34)
Epoch: [18][190/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.8561e-01 (4.4836e-01)	Acc@1  87.50 ( 84.55)	Acc@5  99.22 ( 99.33)
Epoch: [18][200/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.1178e-01 (4.4745e-01)	Acc@1  89.06 ( 84.59)	Acc@5  99.22 ( 99.33)
Epoch: [18][210/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.6700e-01 (4.4887e-01)	Acc@1  82.03 ( 84.55)	Acc@5  97.66 ( 99.33)
Epoch: [18][220/391]	Time  0.070 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.8408e-01 (4.4921e-01)	Acc@1  85.94 ( 84.57)	Acc@5 100.00 ( 99.33)
Epoch: [18][230/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.9578e-01 (4.5149e-01)	Acc@1  76.56 ( 84.51)	Acc@5  98.44 ( 99.31)
Epoch: [18][240/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.6363e-01 (4.5307e-01)	Acc@1  85.94 ( 84.46)	Acc@5  99.22 ( 99.30)
Epoch: [18][250/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.3602e-01 (4.5332e-01)	Acc@1  87.50 ( 84.43)	Acc@5 100.00 ( 99.29)
Epoch: [18][260/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.4579e-01 (4.5062e-01)	Acc@1  86.72 ( 84.52)	Acc@5  99.22 ( 99.30)
Epoch: [18][270/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.1590e-01 (4.4982e-01)	Acc@1  89.06 ( 84.52)	Acc@5 100.00 ( 99.32)
Epoch: [18][280/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.0417e-01 (4.5040e-01)	Acc@1  79.69 ( 84.48)	Acc@5 100.00 ( 99.33)
Epoch: [18][290/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.6605e-01 (4.5182e-01)	Acc@1  82.81 ( 84.40)	Acc@5  99.22 ( 99.31)
Epoch: [18][300/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.5794e-01 (4.5289e-01)	Acc@1  71.88 ( 84.33)	Acc@5  98.44 ( 99.31)
Epoch: [18][310/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.6955e-01 (4.5324e-01)	Acc@1  81.25 ( 84.33)	Acc@5 100.00 ( 99.31)
Epoch: [18][320/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.5826e-01 (4.5229e-01)	Acc@1  87.50 ( 84.37)	Acc@5  99.22 ( 99.31)
Epoch: [18][330/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.6312e-01 (4.5295e-01)	Acc@1  83.59 ( 84.35)	Acc@5  99.22 ( 99.31)
Epoch: [18][340/391]	Time  0.059 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.4241e-01 (4.5381e-01)	Acc@1  85.16 ( 84.35)	Acc@5  98.44 ( 99.31)
Epoch: [18][350/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.4039e-01 (4.5299e-01)	Acc@1  84.38 ( 84.39)	Acc@5 100.00 ( 99.31)
Epoch: [18][360/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.3508e-01 (4.5206e-01)	Acc@1  88.28 ( 84.39)	Acc@5 100.00 ( 99.31)
Epoch: [18][370/391]	Time  0.066 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.9354e-01 (4.5252e-01)	Acc@1  85.16 ( 84.35)	Acc@5  99.22 ( 99.32)
Epoch: [18][380/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.1208e-01 (4.5141e-01)	Acc@1  90.62 ( 84.36)	Acc@5 100.00 ( 99.32)
Epoch: [18][390/391]	Time  0.046 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.9026e-01 (4.5098e-01)	Acc@1  88.75 ( 84.37)	Acc@5  98.75 ( 99.32)
## e[18] optimizer.zero_grad (sum) time: 0.4041168689727783
## e[18]       loss.backward (sum) time: 6.912906646728516
## e[18]      optimizer.step (sum) time: 3.366014003753662
## epoch[18] training(only) time: 25.244085550308228
# Switched to evaluate mode...
Test: [  0/100]	Time  0.194 ( 0.194)	Loss 7.2697e-01 (7.2697e-01)	Acc@1  74.00 ( 74.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.024 ( 0.042)	Loss 5.3919e-01 (5.6848e-01)	Acc@1  81.00 ( 80.09)	Acc@5  99.00 ( 99.27)
Test: [ 20/100]	Time  0.024 ( 0.034)	Loss 6.1967e-01 (5.7879e-01)	Acc@1  79.00 ( 79.90)	Acc@5 100.00 ( 99.10)
Test: [ 30/100]	Time  0.026 ( 0.031)	Loss 7.2719e-01 (5.9991e-01)	Acc@1  71.00 ( 79.55)	Acc@5  99.00 ( 99.10)
Test: [ 40/100]	Time  0.024 ( 0.030)	Loss 5.7436e-01 (5.9540e-01)	Acc@1  83.00 ( 79.90)	Acc@5  98.00 ( 99.07)
Test: [ 50/100]	Time  0.026 ( 0.029)	Loss 3.2645e-01 (5.8646e-01)	Acc@1  88.00 ( 80.06)	Acc@5 100.00 ( 99.20)
Test: [ 60/100]	Time  0.024 ( 0.029)	Loss 5.1348e-01 (5.8022e-01)	Acc@1  79.00 ( 80.13)	Acc@5  99.00 ( 99.23)
Test: [ 70/100]	Time  0.026 ( 0.028)	Loss 5.3215e-01 (5.7587e-01)	Acc@1  80.00 ( 80.14)	Acc@5  99.00 ( 99.25)
Test: [ 80/100]	Time  0.028 ( 0.028)	Loss 4.4915e-01 (5.6905e-01)	Acc@1  81.00 ( 80.41)	Acc@5  99.00 ( 99.25)
Test: [ 90/100]	Time  0.024 ( 0.028)	Loss 5.5639e-01 (5.7294e-01)	Acc@1  85.00 ( 80.34)	Acc@5 100.00 ( 99.25)
 * Acc@1 80.400 Acc@5 99.290
### epoch[18] execution time: 28.12831211090088
EPOCH 19
# current learning rate: 0.1
# Switched to train mode...
Epoch: [19][  0/391]	Time  0.253 ( 0.253)	Data  0.187 ( 0.187)	Loss 4.2702e-01 (4.2702e-01)	Acc@1  85.16 ( 85.16)	Acc@5 100.00 (100.00)
Epoch: [19][ 10/391]	Time  0.063 ( 0.081)	Data  0.001 ( 0.018)	Loss 5.0798e-01 (4.4415e-01)	Acc@1  84.38 ( 84.87)	Acc@5  99.22 ( 99.15)
Epoch: [19][ 20/391]	Time  0.064 ( 0.073)	Data  0.001 ( 0.010)	Loss 4.6376e-01 (4.2751e-01)	Acc@1  85.94 ( 85.42)	Acc@5 100.00 ( 99.18)
Epoch: [19][ 30/391]	Time  0.070 ( 0.070)	Data  0.001 ( 0.007)	Loss 3.4145e-01 (4.2308e-01)	Acc@1  89.84 ( 85.84)	Acc@5  98.44 ( 99.17)
Epoch: [19][ 40/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.006)	Loss 4.4168e-01 (4.1874e-01)	Acc@1  88.28 ( 85.98)	Acc@5  99.22 ( 99.20)
Epoch: [19][ 50/391]	Time  0.060 ( 0.068)	Data  0.001 ( 0.005)	Loss 5.3011e-01 (4.2934e-01)	Acc@1  82.81 ( 85.63)	Acc@5  98.44 ( 99.17)
Epoch: [19][ 60/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.004)	Loss 3.4152e-01 (4.3164e-01)	Acc@1  89.84 ( 85.59)	Acc@5 100.00 ( 99.22)
Epoch: [19][ 70/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.004)	Loss 4.3754e-01 (4.3272e-01)	Acc@1  86.72 ( 85.44)	Acc@5  99.22 ( 99.23)
Epoch: [19][ 80/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.3755e-01 (4.3343e-01)	Acc@1  82.81 ( 85.32)	Acc@5 100.00 ( 99.25)
Epoch: [19][ 90/391]	Time  0.060 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.8853e-01 (4.3155e-01)	Acc@1  85.16 ( 85.38)	Acc@5  98.44 ( 99.27)
Epoch: [19][100/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.003)	Loss 6.0790e-01 (4.3544e-01)	Acc@1  77.34 ( 85.16)	Acc@5  99.22 ( 99.27)
Epoch: [19][110/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.003)	Loss 3.1413e-01 (4.3570e-01)	Acc@1  88.28 ( 85.14)	Acc@5 100.00 ( 99.28)
Epoch: [19][120/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.003)	Loss 4.9039e-01 (4.3751e-01)	Acc@1  82.03 ( 85.05)	Acc@5  98.44 ( 99.29)
Epoch: [19][130/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.003)	Loss 3.6761e-01 (4.3682e-01)	Acc@1  84.38 ( 85.03)	Acc@5 100.00 ( 99.30)
Epoch: [19][140/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.7698e-01 (4.4042e-01)	Acc@1  85.94 ( 84.96)	Acc@5  98.44 ( 99.30)
Epoch: [19][150/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.0999e-01 (4.4078e-01)	Acc@1  79.69 ( 84.96)	Acc@5  99.22 ( 99.29)
Epoch: [19][160/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.2991e-01 (4.4037e-01)	Acc@1  87.50 ( 84.97)	Acc@5  99.22 ( 99.31)
Epoch: [19][170/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.0308e-01 (4.3930e-01)	Acc@1  86.72 ( 85.05)	Acc@5  99.22 ( 99.31)
Epoch: [19][180/391]	Time  0.070 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.2904e-01 (4.3746e-01)	Acc@1  87.50 ( 85.09)	Acc@5  99.22 ( 99.32)
Epoch: [19][190/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.1128e-01 (4.3900e-01)	Acc@1  81.25 ( 84.96)	Acc@5  99.22 ( 99.34)
Epoch: [19][200/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.1388e-01 (4.3876e-01)	Acc@1  85.16 ( 84.97)	Acc@5  99.22 ( 99.35)
Epoch: [19][210/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.0050e-01 (4.4188e-01)	Acc@1  82.03 ( 84.87)	Acc@5  99.22 ( 99.34)
Epoch: [19][220/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.4941e-01 (4.4216e-01)	Acc@1  85.16 ( 84.87)	Acc@5 100.00 ( 99.36)
Epoch: [19][230/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.4205e-01 (4.4355e-01)	Acc@1  83.59 ( 84.80)	Acc@5  99.22 ( 99.36)
Epoch: [19][240/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.4560e-01 (4.4242e-01)	Acc@1  85.94 ( 84.87)	Acc@5 100.00 ( 99.36)
Epoch: [19][250/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.4142e-01 (4.4199e-01)	Acc@1  87.50 ( 84.92)	Acc@5  99.22 ( 99.37)
Epoch: [19][260/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.9826e-01 (4.4147e-01)	Acc@1  86.72 ( 84.91)	Acc@5 100.00 ( 99.37)
Epoch: [19][270/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.1653e-01 (4.4113e-01)	Acc@1  88.28 ( 84.90)	Acc@5 100.00 ( 99.38)
Epoch: [19][280/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.1628e-01 (4.4039e-01)	Acc@1  77.34 ( 84.92)	Acc@5  99.22 ( 99.39)
Epoch: [19][290/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.5321e-01 (4.4095e-01)	Acc@1  84.38 ( 84.92)	Acc@5  97.66 ( 99.40)
Epoch: [19][300/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.1772e-01 (4.4062e-01)	Acc@1  85.16 ( 84.93)	Acc@5  98.44 ( 99.38)
Epoch: [19][310/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.1148e-01 (4.3999e-01)	Acc@1  77.34 ( 84.95)	Acc@5  99.22 ( 99.39)
Epoch: [19][320/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.4024e-01 (4.4050e-01)	Acc@1  85.16 ( 84.90)	Acc@5  99.22 ( 99.38)
Epoch: [19][330/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.6324e-01 (4.4184e-01)	Acc@1  82.03 ( 84.84)	Acc@5  99.22 ( 99.38)
Epoch: [19][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.6122e-01 (4.4230e-01)	Acc@1  84.38 ( 84.85)	Acc@5  99.22 ( 99.38)
Epoch: [19][350/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.3013e-01 (4.4222e-01)	Acc@1  82.81 ( 84.85)	Acc@5 100.00 ( 99.38)
Epoch: [19][360/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.7603e-01 (4.4218e-01)	Acc@1  82.81 ( 84.86)	Acc@5 100.00 ( 99.37)
Epoch: [19][370/391]	Time  0.066 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.0669e-01 (4.4161e-01)	Acc@1  91.41 ( 84.88)	Acc@5  99.22 ( 99.37)
Epoch: [19][380/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.1224e-01 (4.4159e-01)	Acc@1  84.38 ( 84.87)	Acc@5 100.00 ( 99.37)
Epoch: [19][390/391]	Time  0.049 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.1584e-01 (4.4279e-01)	Acc@1  77.50 ( 84.82)	Acc@5 100.00 ( 99.36)
## e[19] optimizer.zero_grad (sum) time: 0.40271878242492676
## e[19]       loss.backward (sum) time: 6.919559001922607
## e[19]      optimizer.step (sum) time: 3.359853982925415
## epoch[19] training(only) time: 25.30895447731018
# Switched to evaluate mode...
Test: [  0/100]	Time  0.191 ( 0.191)	Loss 4.9648e-01 (4.9648e-01)	Acc@1  81.00 ( 81.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.027 ( 0.042)	Loss 5.3045e-01 (4.9737e-01)	Acc@1  86.00 ( 82.18)	Acc@5  99.00 ( 99.36)
Test: [ 20/100]	Time  0.027 ( 0.034)	Loss 5.8763e-01 (5.2259e-01)	Acc@1  78.00 ( 81.90)	Acc@5 100.00 ( 99.33)
Test: [ 30/100]	Time  0.027 ( 0.032)	Loss 4.9222e-01 (5.4106e-01)	Acc@1  82.00 ( 81.58)	Acc@5  99.00 ( 99.16)
Test: [ 40/100]	Time  0.024 ( 0.030)	Loss 5.6532e-01 (5.4460e-01)	Acc@1  83.00 ( 81.59)	Acc@5  97.00 ( 99.02)
Test: [ 50/100]	Time  0.026 ( 0.029)	Loss 3.9858e-01 (5.4757e-01)	Acc@1  86.00 ( 81.53)	Acc@5  99.00 ( 98.88)
Test: [ 60/100]	Time  0.027 ( 0.028)	Loss 5.5980e-01 (5.4913e-01)	Acc@1  80.00 ( 81.52)	Acc@5  99.00 ( 98.93)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 5.0616e-01 (5.4608e-01)	Acc@1  83.00 ( 81.82)	Acc@5 100.00 ( 98.96)
Test: [ 80/100]	Time  0.024 ( 0.028)	Loss 3.9544e-01 (5.4101e-01)	Acc@1  86.00 ( 81.86)	Acc@5  99.00 ( 98.96)
Test: [ 90/100]	Time  0.026 ( 0.028)	Loss 5.6306e-01 (5.3925e-01)	Acc@1  81.00 ( 81.91)	Acc@5 100.00 ( 99.01)
 * Acc@1 81.960 Acc@5 99.010
### epoch[19] execution time: 28.189571142196655
EPOCH 20
# current learning rate: 0.1
# Switched to train mode...
Epoch: [20][  0/391]	Time  0.258 ( 0.258)	Data  0.193 ( 0.193)	Loss 3.2306e-01 (3.2306e-01)	Acc@1  89.06 ( 89.06)	Acc@5 100.00 (100.00)
Epoch: [20][ 10/391]	Time  0.058 ( 0.082)	Data  0.001 ( 0.019)	Loss 5.1226e-01 (4.4162e-01)	Acc@1  81.25 ( 84.45)	Acc@5  98.44 ( 99.29)
Epoch: [20][ 20/391]	Time  0.058 ( 0.072)	Data  0.001 ( 0.010)	Loss 4.3980e-01 (4.5176e-01)	Acc@1  82.81 ( 84.15)	Acc@5  99.22 ( 99.14)
Epoch: [20][ 30/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.007)	Loss 3.8395e-01 (4.2575e-01)	Acc@1  85.94 ( 84.85)	Acc@5 100.00 ( 99.34)
Epoch: [20][ 40/391]	Time  0.061 ( 0.068)	Data  0.001 ( 0.006)	Loss 3.9519e-01 (4.2483e-01)	Acc@1  87.50 ( 84.81)	Acc@5 100.00 ( 99.41)
Epoch: [20][ 50/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.005)	Loss 6.1694e-01 (4.3167e-01)	Acc@1  80.47 ( 84.88)	Acc@5  99.22 ( 99.37)
Epoch: [20][ 60/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.004)	Loss 5.0710e-01 (4.3420e-01)	Acc@1  85.94 ( 84.86)	Acc@5  99.22 ( 99.36)
Epoch: [20][ 70/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.004)	Loss 4.3532e-01 (4.3297e-01)	Acc@1  82.03 ( 84.96)	Acc@5  99.22 ( 99.37)
Epoch: [20][ 80/391]	Time  0.060 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.4124e-01 (4.3442e-01)	Acc@1  85.94 ( 84.94)	Acc@5  98.44 ( 99.37)
Epoch: [20][ 90/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.9494e-01 (4.3374e-01)	Acc@1  78.12 ( 84.97)	Acc@5  99.22 ( 99.35)
Epoch: [20][100/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.9228e-01 (4.3800e-01)	Acc@1  81.25 ( 84.83)	Acc@5  99.22 ( 99.33)
Epoch: [20][110/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.003)	Loss 5.0560e-01 (4.3502e-01)	Acc@1  85.16 ( 85.02)	Acc@5  98.44 ( 99.32)
Epoch: [20][120/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.003)	Loss 4.6283e-01 (4.3627e-01)	Acc@1  87.50 ( 85.04)	Acc@5  99.22 ( 99.32)
Epoch: [20][130/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.003)	Loss 6.1312e-01 (4.3960e-01)	Acc@1  78.12 ( 84.92)	Acc@5  99.22 ( 99.34)
Epoch: [20][140/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.2492e-01 (4.3791e-01)	Acc@1  83.59 ( 84.95)	Acc@5  97.66 ( 99.32)
Epoch: [20][150/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.7746e-01 (4.4020e-01)	Acc@1  82.03 ( 84.86)	Acc@5 100.00 ( 99.33)
Epoch: [20][160/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.9468e-01 (4.4104e-01)	Acc@1  89.06 ( 84.83)	Acc@5  99.22 ( 99.34)
Epoch: [20][170/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.0372e-01 (4.4023e-01)	Acc@1  80.47 ( 84.84)	Acc@5  99.22 ( 99.35)
Epoch: [20][180/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.5209e-01 (4.3925e-01)	Acc@1  85.16 ( 84.91)	Acc@5 100.00 ( 99.35)
Epoch: [20][190/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.5330e-01 (4.4071e-01)	Acc@1  78.12 ( 84.86)	Acc@5 100.00 ( 99.36)
Epoch: [20][200/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.5331e-01 (4.4400e-01)	Acc@1  83.59 ( 84.76)	Acc@5  99.22 ( 99.36)
Epoch: [20][210/391]	Time  0.058 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.3486e-01 (4.4440e-01)	Acc@1  79.69 ( 84.70)	Acc@5  98.44 ( 99.36)
Epoch: [20][220/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.4610e-01 (4.4425e-01)	Acc@1  84.38 ( 84.68)	Acc@5  99.22 ( 99.34)
Epoch: [20][230/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.9461e-01 (4.4320e-01)	Acc@1  87.50 ( 84.70)	Acc@5 100.00 ( 99.34)
Epoch: [20][240/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.4102e-01 (4.4209e-01)	Acc@1  84.38 ( 84.73)	Acc@5 100.00 ( 99.35)
Epoch: [20][250/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.3348e-01 (4.4067e-01)	Acc@1  85.16 ( 84.74)	Acc@5  99.22 ( 99.36)
Epoch: [20][260/391]	Time  0.070 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.5012e-01 (4.4037e-01)	Acc@1  93.75 ( 84.78)	Acc@5 100.00 ( 99.35)
Epoch: [20][270/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.3750e-01 (4.3986e-01)	Acc@1  81.25 ( 84.77)	Acc@5 100.00 ( 99.36)
Epoch: [20][280/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.0396e-01 (4.3969e-01)	Acc@1  92.97 ( 84.78)	Acc@5 100.00 ( 99.36)
Epoch: [20][290/391]	Time  0.059 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.7130e-01 (4.3933e-01)	Acc@1  85.16 ( 84.77)	Acc@5 100.00 ( 99.37)
Epoch: [20][300/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.5560e-01 (4.3856e-01)	Acc@1  86.72 ( 84.82)	Acc@5  99.22 ( 99.37)
Epoch: [20][310/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.2211e-01 (4.3864e-01)	Acc@1  85.94 ( 84.84)	Acc@5  98.44 ( 99.38)
Epoch: [20][320/391]	Time  0.069 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.7525e-01 (4.3829e-01)	Acc@1  90.62 ( 84.86)	Acc@5 100.00 ( 99.38)
Epoch: [20][330/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.4764e-01 (4.3829e-01)	Acc@1  84.38 ( 84.86)	Acc@5 100.00 ( 99.39)
Epoch: [20][340/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.1737e-01 (4.3829e-01)	Acc@1  84.38 ( 84.84)	Acc@5 100.00 ( 99.40)
Epoch: [20][350/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.5572e-01 (4.3733e-01)	Acc@1  85.16 ( 84.85)	Acc@5  97.66 ( 99.39)
Epoch: [20][360/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.8209e-01 (4.3782e-01)	Acc@1  86.72 ( 84.85)	Acc@5  98.44 ( 99.39)
Epoch: [20][370/391]	Time  0.059 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.3578e-01 (4.3760e-01)	Acc@1  90.62 ( 84.90)	Acc@5  99.22 ( 99.39)
Epoch: [20][380/391]	Time  0.065 ( 0.064)	Data  0.002 ( 0.002)	Loss 6.0734e-01 (4.3813e-01)	Acc@1  81.25 ( 84.91)	Acc@5  99.22 ( 99.39)
Epoch: [20][390/391]	Time  0.048 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.2890e-01 (4.3753e-01)	Acc@1  90.00 ( 84.93)	Acc@5 100.00 ( 99.40)
## e[20] optimizer.zero_grad (sum) time: 0.4018235206604004
## e[20]       loss.backward (sum) time: 6.904893636703491
## e[20]      optimizer.step (sum) time: 3.3370039463043213
## epoch[20] training(only) time: 25.190343856811523
# Switched to evaluate mode...
Test: [  0/100]	Time  0.192 ( 0.192)	Loss 5.1278e-01 (5.1278e-01)	Acc@1  84.00 ( 84.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.024 ( 0.040)	Loss 4.7206e-01 (4.6918e-01)	Acc@1  87.00 ( 84.45)	Acc@5  98.00 ( 99.36)
Test: [ 20/100]	Time  0.028 ( 0.033)	Loss 5.4022e-01 (4.9692e-01)	Acc@1  79.00 ( 82.86)	Acc@5 100.00 ( 99.19)
Test: [ 30/100]	Time  0.025 ( 0.030)	Loss 5.7738e-01 (5.0956e-01)	Acc@1  80.00 ( 82.74)	Acc@5 100.00 ( 99.00)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 5.1006e-01 (5.1958e-01)	Acc@1  83.00 ( 82.56)	Acc@5  98.00 ( 99.07)
Test: [ 50/100]	Time  0.027 ( 0.028)	Loss 4.5856e-01 (5.2297e-01)	Acc@1  85.00 ( 82.67)	Acc@5  98.00 ( 99.08)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 5.2405e-01 (5.2216e-01)	Acc@1  81.00 ( 82.69)	Acc@5 100.00 ( 99.15)
Test: [ 70/100]	Time  0.028 ( 0.028)	Loss 5.5239e-01 (5.2577e-01)	Acc@1  83.00 ( 82.45)	Acc@5 100.00 ( 99.23)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 4.3356e-01 (5.2186e-01)	Acc@1  87.00 ( 82.54)	Acc@5  99.00 ( 99.26)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 4.7567e-01 (5.2434e-01)	Acc@1  85.00 ( 82.51)	Acc@5 100.00 ( 99.25)
 * Acc@1 82.510 Acc@5 99.280
### epoch[20] execution time: 28.05325198173523
EPOCH 21
# current learning rate: 0.1
# Switched to train mode...
Epoch: [21][  0/391]	Time  0.295 ( 0.295)	Data  0.232 ( 0.232)	Loss 2.9397e-01 (2.9397e-01)	Acc@1  91.41 ( 91.41)	Acc@5  99.22 ( 99.22)
Epoch: [21][ 10/391]	Time  0.062 ( 0.082)	Data  0.001 ( 0.022)	Loss 3.8932e-01 (4.1729e-01)	Acc@1  87.50 ( 86.65)	Acc@5 100.00 ( 99.36)
Epoch: [21][ 20/391]	Time  0.059 ( 0.073)	Data  0.001 ( 0.012)	Loss 3.4979e-01 (4.0869e-01)	Acc@1  89.06 ( 86.46)	Acc@5 100.00 ( 99.55)
Epoch: [21][ 30/391]	Time  0.063 ( 0.070)	Data  0.001 ( 0.008)	Loss 4.3848e-01 (4.1764e-01)	Acc@1  83.59 ( 85.94)	Acc@5 100.00 ( 99.45)
Epoch: [21][ 40/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.007)	Loss 4.1637e-01 (4.2275e-01)	Acc@1  85.94 ( 85.63)	Acc@5 100.00 ( 99.49)
Epoch: [21][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.006)	Loss 4.0672e-01 (4.1866e-01)	Acc@1  85.94 ( 85.65)	Acc@5 100.00 ( 99.51)
Epoch: [21][ 60/391]	Time  0.061 ( 0.067)	Data  0.001 ( 0.005)	Loss 3.1513e-01 (4.1867e-01)	Acc@1  88.28 ( 85.77)	Acc@5 100.00 ( 99.51)
Epoch: [21][ 70/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.004)	Loss 3.2390e-01 (4.1301e-01)	Acc@1  87.50 ( 85.99)	Acc@5 100.00 ( 99.53)
Epoch: [21][ 80/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.004)	Loss 4.8737e-01 (4.0983e-01)	Acc@1  82.03 ( 86.07)	Acc@5 100.00 ( 99.53)
Epoch: [21][ 90/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.004)	Loss 4.2133e-01 (4.1218e-01)	Acc@1  84.38 ( 86.00)	Acc@5 100.00 ( 99.46)
Epoch: [21][100/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.4724e-01 (4.1363e-01)	Acc@1  87.50 ( 85.88)	Acc@5 100.00 ( 99.46)
Epoch: [21][110/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.6905e-01 (4.1311e-01)	Acc@1  85.94 ( 85.82)	Acc@5 100.00 ( 99.47)
Epoch: [21][120/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.0456e-01 (4.1643e-01)	Acc@1  86.72 ( 85.73)	Acc@5  99.22 ( 99.48)
Epoch: [21][130/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 4.6992e-01 (4.2119e-01)	Acc@1  82.81 ( 85.60)	Acc@5 100.00 ( 99.45)
Epoch: [21][140/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.003)	Loss 4.2572e-01 (4.1558e-01)	Acc@1  84.38 ( 85.82)	Acc@5 100.00 ( 99.47)
Epoch: [21][150/391]	Time  0.058 ( 0.065)	Data  0.001 ( 0.003)	Loss 3.7470e-01 (4.1733e-01)	Acc@1  87.50 ( 85.73)	Acc@5  99.22 ( 99.46)
Epoch: [21][160/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 4.3161e-01 (4.1919e-01)	Acc@1  83.59 ( 85.61)	Acc@5 100.00 ( 99.46)
Epoch: [21][170/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.2211e-01 (4.1870e-01)	Acc@1  90.62 ( 85.65)	Acc@5 100.00 ( 99.47)
Epoch: [21][180/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.5192e-01 (4.1704e-01)	Acc@1  87.50 ( 85.67)	Acc@5 100.00 ( 99.47)
Epoch: [21][190/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.1554e-01 (4.1753e-01)	Acc@1  92.19 ( 85.63)	Acc@5  99.22 ( 99.47)
Epoch: [21][200/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.1266e-01 (4.1929e-01)	Acc@1  87.50 ( 85.58)	Acc@5  98.44 ( 99.46)
Epoch: [21][210/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.8924e-01 (4.2357e-01)	Acc@1  81.25 ( 85.46)	Acc@5  97.66 ( 99.44)
Epoch: [21][220/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.9635e-01 (4.2451e-01)	Acc@1  79.69 ( 85.44)	Acc@5  99.22 ( 99.43)
Epoch: [21][230/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.9205e-01 (4.2428e-01)	Acc@1  82.81 ( 85.46)	Acc@5  99.22 ( 99.44)
Epoch: [21][240/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.8309e-01 (4.2245e-01)	Acc@1  83.59 ( 85.55)	Acc@5  98.44 ( 99.44)
Epoch: [21][250/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.8183e-01 (4.2157e-01)	Acc@1  86.72 ( 85.58)	Acc@5  99.22 ( 99.45)
Epoch: [21][260/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.9462e-01 (4.2137e-01)	Acc@1  90.62 ( 85.53)	Acc@5  99.22 ( 99.46)
Epoch: [21][270/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.7431e-01 (4.2306e-01)	Acc@1  79.69 ( 85.46)	Acc@5  99.22 ( 99.46)
Epoch: [21][280/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.8986e-01 (4.2232e-01)	Acc@1  87.50 ( 85.48)	Acc@5  99.22 ( 99.46)
Epoch: [21][290/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.7073e-01 (4.2404e-01)	Acc@1  86.72 ( 85.43)	Acc@5  99.22 ( 99.47)
Epoch: [21][300/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.6042e-01 (4.2503e-01)	Acc@1  83.59 ( 85.37)	Acc@5 100.00 ( 99.46)
Epoch: [21][310/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.4203e-01 (4.2473e-01)	Acc@1  85.94 ( 85.39)	Acc@5 100.00 ( 99.45)
Epoch: [21][320/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.9863e-01 (4.2428e-01)	Acc@1  88.28 ( 85.42)	Acc@5 100.00 ( 99.46)
Epoch: [21][330/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.5830e-01 (4.2374e-01)	Acc@1  87.50 ( 85.47)	Acc@5 100.00 ( 99.46)
Epoch: [21][340/391]	Time  0.059 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.1724e-01 (4.2442e-01)	Acc@1  85.94 ( 85.44)	Acc@5  98.44 ( 99.46)
Epoch: [21][350/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.7331e-01 (4.2565e-01)	Acc@1  87.50 ( 85.39)	Acc@5 100.00 ( 99.46)
Epoch: [21][360/391]	Time  0.059 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.4399e-01 (4.2698e-01)	Acc@1  78.91 ( 85.31)	Acc@5  99.22 ( 99.45)
Epoch: [21][370/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.2270e-01 (4.2817e-01)	Acc@1  84.38 ( 85.27)	Acc@5  99.22 ( 99.45)
Epoch: [21][380/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.2744e-01 (4.2780e-01)	Acc@1  85.16 ( 85.27)	Acc@5 100.00 ( 99.46)
Epoch: [21][390/391]	Time  0.045 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.8441e-01 (4.2780e-01)	Acc@1  82.50 ( 85.26)	Acc@5 100.00 ( 99.46)
## e[21] optimizer.zero_grad (sum) time: 0.39336276054382324
## e[21]       loss.backward (sum) time: 6.9336864948272705
## e[21]      optimizer.step (sum) time: 3.3056440353393555
## epoch[21] training(only) time: 25.126750707626343
# Switched to evaluate mode...
Test: [  0/100]	Time  0.193 ( 0.193)	Loss 5.3352e-01 (5.3352e-01)	Acc@1  79.00 ( 79.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.024 ( 0.041)	Loss 6.4543e-01 (5.2700e-01)	Acc@1  84.00 ( 81.36)	Acc@5  98.00 ( 99.36)
Test: [ 20/100]	Time  0.027 ( 0.034)	Loss 5.8974e-01 (5.2404e-01)	Acc@1  75.00 ( 81.81)	Acc@5 100.00 ( 99.29)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 6.3805e-01 (5.5186e-01)	Acc@1  76.00 ( 81.00)	Acc@5  98.00 ( 99.16)
Test: [ 40/100]	Time  0.024 ( 0.030)	Loss 3.9861e-01 (5.5507e-01)	Acc@1  85.00 ( 81.02)	Acc@5 100.00 ( 99.10)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 4.6698e-01 (5.4304e-01)	Acc@1  83.00 ( 81.69)	Acc@5 100.00 ( 99.18)
Test: [ 60/100]	Time  0.027 ( 0.028)	Loss 5.6714e-01 (5.4387e-01)	Acc@1  82.00 ( 81.62)	Acc@5 100.00 ( 99.23)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 5.2144e-01 (5.3988e-01)	Acc@1  84.00 ( 81.72)	Acc@5 100.00 ( 99.28)
Test: [ 80/100]	Time  0.026 ( 0.028)	Loss 6.2645e-01 (5.4355e-01)	Acc@1  79.00 ( 81.48)	Acc@5 100.00 ( 99.27)
Test: [ 90/100]	Time  0.025 ( 0.027)	Loss 4.4282e-01 (5.3946e-01)	Acc@1  83.00 ( 81.47)	Acc@5 100.00 ( 99.27)
 * Acc@1 81.740 Acc@5 99.260
### epoch[21] execution time: 27.968157052993774
EPOCH 22
# current learning rate: 0.1
# Switched to train mode...
Epoch: [22][  0/391]	Time  0.248 ( 0.248)	Data  0.184 ( 0.184)	Loss 2.9589e-01 (2.9589e-01)	Acc@1  90.62 ( 90.62)	Acc@5 100.00 (100.00)
Epoch: [22][ 10/391]	Time  0.062 ( 0.081)	Data  0.001 ( 0.018)	Loss 4.8699e-01 (3.5960e-01)	Acc@1  84.38 ( 86.72)	Acc@5  99.22 ( 99.72)
Epoch: [22][ 20/391]	Time  0.062 ( 0.073)	Data  0.001 ( 0.010)	Loss 3.0750e-01 (3.9705e-01)	Acc@1  89.84 ( 86.16)	Acc@5 100.00 ( 99.48)
Epoch: [22][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.007)	Loss 5.1330e-01 (4.0006e-01)	Acc@1  83.59 ( 86.49)	Acc@5 100.00 ( 99.57)
Epoch: [22][ 40/391]	Time  0.061 ( 0.069)	Data  0.001 ( 0.006)	Loss 4.2545e-01 (3.9459e-01)	Acc@1  85.94 ( 86.72)	Acc@5  98.44 ( 99.52)
Epoch: [22][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.005)	Loss 4.4169e-01 (3.9272e-01)	Acc@1  82.81 ( 86.50)	Acc@5 100.00 ( 99.60)
Epoch: [22][ 60/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.004)	Loss 5.7863e-01 (3.9694e-01)	Acc@1  81.25 ( 86.41)	Acc@5  99.22 ( 99.59)
Epoch: [22][ 70/391]	Time  0.059 ( 0.067)	Data  0.001 ( 0.004)	Loss 3.4463e-01 (3.9770e-01)	Acc@1  89.06 ( 86.45)	Acc@5  99.22 ( 99.57)
Epoch: [22][ 80/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.2113e-01 (3.9991e-01)	Acc@1  85.94 ( 86.40)	Acc@5 100.00 ( 99.58)
Epoch: [22][ 90/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.8474e-01 (4.0148e-01)	Acc@1  87.50 ( 86.26)	Acc@5  99.22 ( 99.58)
Epoch: [22][100/391]	Time  0.059 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.7122e-01 (4.0550e-01)	Acc@1  88.28 ( 86.22)	Acc@5 100.00 ( 99.56)
Epoch: [22][110/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 6.4619e-01 (4.0676e-01)	Acc@1  77.34 ( 86.19)	Acc@5  98.44 ( 99.56)
Epoch: [22][120/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.003)	Loss 6.0750e-01 (4.0805e-01)	Acc@1  83.59 ( 86.16)	Acc@5  99.22 ( 99.55)
Epoch: [22][130/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.003)	Loss 4.6230e-01 (4.0943e-01)	Acc@1  83.59 ( 86.11)	Acc@5  99.22 ( 99.54)
Epoch: [22][140/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.6977e-01 (4.0717e-01)	Acc@1  87.50 ( 86.13)	Acc@5 100.00 ( 99.53)
Epoch: [22][150/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.1114e-01 (4.0560e-01)	Acc@1  92.97 ( 86.20)	Acc@5 100.00 ( 99.56)
Epoch: [22][160/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.5497e-01 (4.0493e-01)	Acc@1  86.72 ( 86.12)	Acc@5  98.44 ( 99.55)
Epoch: [22][170/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.2235e-01 (4.0543e-01)	Acc@1  84.38 ( 86.09)	Acc@5  98.44 ( 99.52)
Epoch: [22][180/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.6253e-01 (4.0744e-01)	Acc@1  86.72 ( 86.02)	Acc@5  99.22 ( 99.50)
Epoch: [22][190/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.5732e-01 (4.0996e-01)	Acc@1  83.59 ( 85.94)	Acc@5 100.00 ( 99.49)
Epoch: [22][200/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.6284e-01 (4.1006e-01)	Acc@1  85.94 ( 85.96)	Acc@5  99.22 ( 99.49)
Epoch: [22][210/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.5266e-01 (4.1196e-01)	Acc@1  85.94 ( 85.90)	Acc@5 100.00 ( 99.49)
Epoch: [22][220/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.0589e-01 (4.1110e-01)	Acc@1  84.38 ( 85.90)	Acc@5  99.22 ( 99.50)
Epoch: [22][230/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.2108e-01 (4.1059e-01)	Acc@1  85.16 ( 85.91)	Acc@5  99.22 ( 99.50)
Epoch: [22][240/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.2322e-01 (4.1161e-01)	Acc@1  86.72 ( 85.85)	Acc@5 100.00 ( 99.48)
Epoch: [22][250/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.1399e-01 (4.1048e-01)	Acc@1  79.69 ( 85.86)	Acc@5  99.22 ( 99.48)
Epoch: [22][260/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.5416e-01 (4.1092e-01)	Acc@1  87.50 ( 85.86)	Acc@5 100.00 ( 99.48)
Epoch: [22][270/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.0309e-01 (4.1035e-01)	Acc@1  85.16 ( 85.91)	Acc@5 100.00 ( 99.48)
Epoch: [22][280/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.7105e-01 (4.0950e-01)	Acc@1  84.38 ( 85.95)	Acc@5  99.22 ( 99.48)
Epoch: [22][290/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.4229e-01 (4.1169e-01)	Acc@1  82.81 ( 85.85)	Acc@5 100.00 ( 99.47)
Epoch: [22][300/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.4507e-01 (4.1411e-01)	Acc@1  85.16 ( 85.74)	Acc@5 100.00 ( 99.48)
Epoch: [22][310/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.9763e-01 (4.1530e-01)	Acc@1  78.91 ( 85.70)	Acc@5 100.00 ( 99.46)
Epoch: [22][320/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.2343e-01 (4.1421e-01)	Acc@1  89.06 ( 85.75)	Acc@5 100.00 ( 99.46)
Epoch: [22][330/391]	Time  0.066 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.4394e-01 (4.1369e-01)	Acc@1  83.59 ( 85.78)	Acc@5 100.00 ( 99.47)
Epoch: [22][340/391]	Time  0.059 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.5681e-01 (4.1381e-01)	Acc@1  85.94 ( 85.74)	Acc@5  97.66 ( 99.47)
Epoch: [22][350/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.7149e-01 (4.1395e-01)	Acc@1  82.81 ( 85.74)	Acc@5  98.44 ( 99.47)
Epoch: [22][360/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.4251e-01 (4.1370e-01)	Acc@1  84.38 ( 85.75)	Acc@5  98.44 ( 99.47)
Epoch: [22][370/391]	Time  0.059 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.9617e-01 (4.1408e-01)	Acc@1  84.38 ( 85.73)	Acc@5  99.22 ( 99.47)
Epoch: [22][380/391]	Time  0.067 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.4440e-01 (4.1315e-01)	Acc@1  86.72 ( 85.78)	Acc@5  99.22 ( 99.46)
Epoch: [22][390/391]	Time  0.046 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.7067e-01 (4.1416e-01)	Acc@1  86.25 ( 85.75)	Acc@5 100.00 ( 99.46)
## e[22] optimizer.zero_grad (sum) time: 0.40480542182922363
## e[22]       loss.backward (sum) time: 6.958737850189209
## e[22]      optimizer.step (sum) time: 3.3465468883514404
## epoch[22] training(only) time: 25.254071950912476
# Switched to evaluate mode...
Test: [  0/100]	Time  0.197 ( 0.197)	Loss 5.7624e-01 (5.7624e-01)	Acc@1  81.00 ( 81.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.042)	Loss 5.9642e-01 (5.1018e-01)	Acc@1  83.00 ( 83.27)	Acc@5  98.00 ( 99.18)
Test: [ 20/100]	Time  0.028 ( 0.035)	Loss 4.7933e-01 (5.3386e-01)	Acc@1  79.00 ( 81.67)	Acc@5 100.00 ( 99.29)
Test: [ 30/100]	Time  0.024 ( 0.032)	Loss 5.4889e-01 (5.4592e-01)	Acc@1  82.00 ( 81.29)	Acc@5 100.00 ( 99.29)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 5.3391e-01 (5.4293e-01)	Acc@1  78.00 ( 81.71)	Acc@5  99.00 ( 99.27)
Test: [ 50/100]	Time  0.028 ( 0.029)	Loss 4.7804e-01 (5.3914e-01)	Acc@1  85.00 ( 81.94)	Acc@5  97.00 ( 99.12)
Test: [ 60/100]	Time  0.028 ( 0.029)	Loss 5.7898e-01 (5.3994e-01)	Acc@1  80.00 ( 81.82)	Acc@5 100.00 ( 99.20)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 4.8696e-01 (5.3407e-01)	Acc@1  83.00 ( 82.00)	Acc@5 100.00 ( 99.25)
Test: [ 80/100]	Time  0.029 ( 0.028)	Loss 4.3509e-01 (5.2944e-01)	Acc@1  84.00 ( 82.16)	Acc@5 100.00 ( 99.28)
Test: [ 90/100]	Time  0.027 ( 0.028)	Loss 3.5979e-01 (5.2874e-01)	Acc@1  85.00 ( 82.25)	Acc@5 100.00 ( 99.27)
 * Acc@1 82.340 Acc@5 99.270
### epoch[22] execution time: 28.122511386871338
EPOCH 23
# current learning rate: 0.1
# Switched to train mode...
Epoch: [23][  0/391]	Time  0.254 ( 0.254)	Data  0.188 ( 0.188)	Loss 4.0822e-01 (4.0822e-01)	Acc@1  85.16 ( 85.16)	Acc@5  99.22 ( 99.22)
Epoch: [23][ 10/391]	Time  0.062 ( 0.081)	Data  0.001 ( 0.018)	Loss 6.1226e-01 (4.3115e-01)	Acc@1  77.34 ( 85.30)	Acc@5  99.22 ( 99.29)
Epoch: [23][ 20/391]	Time  0.065 ( 0.073)	Data  0.001 ( 0.010)	Loss 3.6481e-01 (4.3352e-01)	Acc@1  87.50 ( 85.04)	Acc@5 100.00 ( 99.44)
Epoch: [23][ 30/391]	Time  0.063 ( 0.070)	Data  0.001 ( 0.007)	Loss 3.8897e-01 (4.3276e-01)	Acc@1  85.94 ( 85.13)	Acc@5  99.22 ( 99.45)
Epoch: [23][ 40/391]	Time  0.065 ( 0.069)	Data  0.002 ( 0.006)	Loss 3.5956e-01 (4.2688e-01)	Acc@1  88.28 ( 85.31)	Acc@5 100.00 ( 99.52)
Epoch: [23][ 50/391]	Time  0.059 ( 0.068)	Data  0.001 ( 0.005)	Loss 4.1589e-01 (4.2145e-01)	Acc@1  87.50 ( 85.34)	Acc@5  99.22 ( 99.54)
Epoch: [23][ 60/391]	Time  0.061 ( 0.067)	Data  0.001 ( 0.004)	Loss 5.2170e-01 (4.1743e-01)	Acc@1  78.91 ( 85.45)	Acc@5  99.22 ( 99.50)
Epoch: [23][ 70/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.004)	Loss 3.1669e-01 (4.1339e-01)	Acc@1  90.62 ( 85.63)	Acc@5 100.00 ( 99.54)
Epoch: [23][ 80/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.5196e-01 (4.1425e-01)	Acc@1  85.16 ( 85.58)	Acc@5  98.44 ( 99.49)
Epoch: [23][ 90/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.003)	Loss 5.5907e-01 (4.1095e-01)	Acc@1  82.81 ( 85.78)	Acc@5  98.44 ( 99.49)
Epoch: [23][100/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.003)	Loss 3.7546e-01 (4.1061e-01)	Acc@1  85.16 ( 85.72)	Acc@5 100.00 ( 99.51)
Epoch: [23][110/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.003)	Loss 3.7826e-01 (4.1389e-01)	Acc@1  87.50 ( 85.58)	Acc@5 100.00 ( 99.51)
Epoch: [23][120/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.003)	Loss 3.8991e-01 (4.1577e-01)	Acc@1  86.72 ( 85.56)	Acc@5 100.00 ( 99.52)
Epoch: [23][130/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 5.1108e-01 (4.1720e-01)	Acc@1  78.12 ( 85.54)	Acc@5 100.00 ( 99.49)
Epoch: [23][140/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.9399e-01 (4.1639e-01)	Acc@1  85.94 ( 85.59)	Acc@5 100.00 ( 99.48)
Epoch: [23][150/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.7278e-01 (4.1700e-01)	Acc@1  86.72 ( 85.61)	Acc@5 100.00 ( 99.49)
Epoch: [23][160/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.0402e-01 (4.2060e-01)	Acc@1  80.47 ( 85.52)	Acc@5  99.22 ( 99.48)
Epoch: [23][170/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.0038e-01 (4.2200e-01)	Acc@1  87.50 ( 85.46)	Acc@5  99.22 ( 99.48)
Epoch: [23][180/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.6942e-01 (4.2110e-01)	Acc@1  84.38 ( 85.48)	Acc@5 100.00 ( 99.48)
Epoch: [23][190/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.2641e-01 (4.2092e-01)	Acc@1  82.81 ( 85.52)	Acc@5  99.22 ( 99.46)
Epoch: [23][200/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.7165e-01 (4.2109e-01)	Acc@1  79.69 ( 85.47)	Acc@5 100.00 ( 99.46)
Epoch: [23][210/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.3096e-01 (4.2091e-01)	Acc@1  85.16 ( 85.47)	Acc@5  98.44 ( 99.45)
Epoch: [23][220/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.2350e-01 (4.1990e-01)	Acc@1  88.28 ( 85.50)	Acc@5  98.44 ( 99.46)
Epoch: [23][230/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.6466e-01 (4.2065e-01)	Acc@1  80.47 ( 85.43)	Acc@5 100.00 ( 99.47)
Epoch: [23][240/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.6053e-01 (4.2030e-01)	Acc@1  85.94 ( 85.44)	Acc@5 100.00 ( 99.48)
Epoch: [23][250/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.7202e-01 (4.1890e-01)	Acc@1  87.50 ( 85.46)	Acc@5  99.22 ( 99.48)
Epoch: [23][260/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.0812e-01 (4.1777e-01)	Acc@1  85.94 ( 85.50)	Acc@5  99.22 ( 99.49)
Epoch: [23][270/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.3745e-01 (4.1775e-01)	Acc@1  83.59 ( 85.50)	Acc@5  99.22 ( 99.49)
Epoch: [23][280/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.9662e-01 (4.1783e-01)	Acc@1  87.50 ( 85.53)	Acc@5 100.00 ( 99.50)
Epoch: [23][290/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.7269e-01 (4.1766e-01)	Acc@1  87.50 ( 85.53)	Acc@5  99.22 ( 99.49)
Epoch: [23][300/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.3464e-01 (4.1931e-01)	Acc@1  80.47 ( 85.51)	Acc@5  98.44 ( 99.48)
Epoch: [23][310/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.4355e-01 (4.2068e-01)	Acc@1  82.03 ( 85.45)	Acc@5  99.22 ( 99.46)
Epoch: [23][320/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.0343e-01 (4.2116e-01)	Acc@1  82.03 ( 85.45)	Acc@5  99.22 ( 99.46)
Epoch: [23][330/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.1981e-01 (4.2150e-01)	Acc@1  89.06 ( 85.46)	Acc@5  99.22 ( 99.47)
Epoch: [23][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.5684e-01 (4.2129e-01)	Acc@1  85.16 ( 85.47)	Acc@5 100.00 ( 99.46)
Epoch: [23][350/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.1445e-01 (4.2290e-01)	Acc@1  84.38 ( 85.41)	Acc@5  99.22 ( 99.45)
Epoch: [23][360/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.8152e-01 (4.2314e-01)	Acc@1  90.62 ( 85.38)	Acc@5  99.22 ( 99.44)
Epoch: [23][370/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.5773e-01 (4.2250e-01)	Acc@1  85.16 ( 85.39)	Acc@5 100.00 ( 99.45)
Epoch: [23][380/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.9718e-01 (4.2148e-01)	Acc@1  82.03 ( 85.43)	Acc@5 100.00 ( 99.46)
Epoch: [23][390/391]	Time  0.046 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.0191e-01 (4.2144e-01)	Acc@1  86.25 ( 85.43)	Acc@5  97.50 ( 99.46)
## e[23] optimizer.zero_grad (sum) time: 0.40333104133605957
## e[23]       loss.backward (sum) time: 7.015202283859253
## e[23]      optimizer.step (sum) time: 3.3711938858032227
## epoch[23] training(only) time: 25.39475655555725
# Switched to evaluate mode...
Test: [  0/100]	Time  0.191 ( 0.191)	Loss 5.1480e-01 (5.1480e-01)	Acc@1  82.00 ( 82.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.027 ( 0.042)	Loss 5.7475e-01 (5.8524e-01)	Acc@1  81.00 ( 79.45)	Acc@5 100.00 ( 99.45)
Test: [ 20/100]	Time  0.029 ( 0.034)	Loss 7.0107e-01 (5.9407e-01)	Acc@1  82.00 ( 80.57)	Acc@5 100.00 ( 99.14)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 6.8739e-01 (6.2033e-01)	Acc@1  69.00 ( 80.06)	Acc@5  99.00 ( 99.19)
Test: [ 40/100]	Time  0.024 ( 0.030)	Loss 4.3169e-01 (6.2456e-01)	Acc@1  87.00 ( 80.00)	Acc@5  98.00 ( 99.05)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 4.5271e-01 (6.1772e-01)	Acc@1  85.00 ( 80.18)	Acc@5  98.00 ( 98.98)
Test: [ 60/100]	Time  0.024 ( 0.029)	Loss 7.8525e-01 (6.1547e-01)	Acc@1  74.00 ( 80.26)	Acc@5 100.00 ( 99.00)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 5.1208e-01 (6.0749e-01)	Acc@1  85.00 ( 80.32)	Acc@5  98.00 ( 99.07)
Test: [ 80/100]	Time  0.024 ( 0.028)	Loss 4.0514e-01 (5.9968e-01)	Acc@1  87.00 ( 80.56)	Acc@5 100.00 ( 99.06)
Test: [ 90/100]	Time  0.027 ( 0.027)	Loss 6.1626e-01 (5.9853e-01)	Acc@1  82.00 ( 80.49)	Acc@5 100.00 ( 99.11)
 * Acc@1 80.640 Acc@5 99.090
### epoch[23] execution time: 28.23990035057068
EPOCH 24
# current learning rate: 0.1
# Switched to train mode...
Epoch: [24][  0/391]	Time  0.254 ( 0.254)	Data  0.189 ( 0.189)	Loss 5.2186e-01 (5.2186e-01)	Acc@1  83.59 ( 83.59)	Acc@5 100.00 (100.00)
Epoch: [24][ 10/391]	Time  0.065 ( 0.081)	Data  0.001 ( 0.018)	Loss 4.3052e-01 (4.1822e-01)	Acc@1  87.50 ( 85.65)	Acc@5 100.00 ( 99.57)
Epoch: [24][ 20/391]	Time  0.061 ( 0.073)	Data  0.001 ( 0.010)	Loss 3.0212e-01 (4.0173e-01)	Acc@1  87.50 ( 85.90)	Acc@5  99.22 ( 99.48)
Epoch: [24][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.007)	Loss 2.9477e-01 (3.9503e-01)	Acc@1  92.19 ( 86.19)	Acc@5 100.00 ( 99.55)
Epoch: [24][ 40/391]	Time  0.062 ( 0.068)	Data  0.001 ( 0.006)	Loss 4.7007e-01 (4.0258e-01)	Acc@1  81.25 ( 85.94)	Acc@5  99.22 ( 99.54)
Epoch: [24][ 50/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.005)	Loss 3.6977e-01 (3.9954e-01)	Acc@1  85.94 ( 85.92)	Acc@5 100.00 ( 99.54)
Epoch: [24][ 60/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.004)	Loss 3.9360e-01 (4.1066e-01)	Acc@1  82.81 ( 85.50)	Acc@5 100.00 ( 99.54)
Epoch: [24][ 70/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.004)	Loss 3.1531e-01 (4.0730e-01)	Acc@1  89.84 ( 85.62)	Acc@5  99.22 ( 99.53)
Epoch: [24][ 80/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.2035e-01 (4.0638e-01)	Acc@1  87.50 ( 85.68)	Acc@5 100.00 ( 99.48)
Epoch: [24][ 90/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.6594e-01 (4.0506e-01)	Acc@1  87.50 ( 85.74)	Acc@5  99.22 ( 99.50)
Epoch: [24][100/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.7917e-01 (4.0109e-01)	Acc@1  90.62 ( 85.89)	Acc@5  99.22 ( 99.52)
Epoch: [24][110/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 4.3932e-01 (4.0169e-01)	Acc@1  86.72 ( 85.85)	Acc@5  98.44 ( 99.53)
Epoch: [24][120/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.003)	Loss 3.9096e-01 (4.0500e-01)	Acc@1  85.94 ( 85.83)	Acc@5 100.00 ( 99.50)
Epoch: [24][130/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.003)	Loss 3.9709e-01 (4.0402e-01)	Acc@1  85.16 ( 85.87)	Acc@5 100.00 ( 99.51)
Epoch: [24][140/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.5143e-01 (4.0674e-01)	Acc@1  85.16 ( 85.80)	Acc@5  98.44 ( 99.46)
Epoch: [24][150/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.7187e-01 (4.0771e-01)	Acc@1  87.50 ( 85.78)	Acc@5  99.22 ( 99.44)
Epoch: [24][160/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.4744e-01 (4.0364e-01)	Acc@1  89.84 ( 85.95)	Acc@5 100.00 ( 99.46)
Epoch: [24][170/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.7920e-01 (4.0375e-01)	Acc@1  89.06 ( 85.96)	Acc@5  99.22 ( 99.47)
Epoch: [24][180/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.1132e-01 (4.0298e-01)	Acc@1  85.94 ( 85.97)	Acc@5  99.22 ( 99.44)
Epoch: [24][190/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.5550e-01 (4.0258e-01)	Acc@1  82.81 ( 85.98)	Acc@5 100.00 ( 99.46)
Epoch: [24][200/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.6952e-01 (4.0603e-01)	Acc@1  85.16 ( 85.89)	Acc@5  98.44 ( 99.45)
Epoch: [24][210/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.9508e-01 (4.0733e-01)	Acc@1  85.16 ( 85.87)	Acc@5  98.44 ( 99.43)
Epoch: [24][220/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.9075e-01 (4.0871e-01)	Acc@1  90.62 ( 85.87)	Acc@5 100.00 ( 99.42)
Epoch: [24][230/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.5164e-01 (4.0787e-01)	Acc@1  83.59 ( 85.91)	Acc@5  99.22 ( 99.43)
Epoch: [24][240/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.0327e-01 (4.0874e-01)	Acc@1  84.38 ( 85.90)	Acc@5  99.22 ( 99.42)
Epoch: [24][250/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.9925e-01 (4.0780e-01)	Acc@1  92.97 ( 85.94)	Acc@5  99.22 ( 99.43)
Epoch: [24][260/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.0387e-01 (4.0713e-01)	Acc@1  90.62 ( 86.02)	Acc@5 100.00 ( 99.42)
Epoch: [24][270/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.0194e-01 (4.0638e-01)	Acc@1  86.72 ( 86.06)	Acc@5 100.00 ( 99.42)
Epoch: [24][280/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.9108e-01 (4.0697e-01)	Acc@1  89.06 ( 86.00)	Acc@5  98.44 ( 99.42)
Epoch: [24][290/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3868e-01 (4.0680e-01)	Acc@1  92.97 ( 86.01)	Acc@5 100.00 ( 99.42)
Epoch: [24][300/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.9928e-01 (4.0738e-01)	Acc@1  82.03 ( 85.99)	Acc@5  99.22 ( 99.42)
Epoch: [24][310/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.4542e-01 (4.0846e-01)	Acc@1  85.94 ( 85.94)	Acc@5  98.44 ( 99.40)
Epoch: [24][320/391]	Time  0.066 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.6202e-01 (4.0802e-01)	Acc@1  89.06 ( 85.95)	Acc@5  99.22 ( 99.41)
Epoch: [24][330/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.4414e-01 (4.0985e-01)	Acc@1  81.25 ( 85.90)	Acc@5  98.44 ( 99.40)
Epoch: [24][340/391]	Time  0.066 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.6256e-01 (4.1038e-01)	Acc@1  88.28 ( 85.87)	Acc@5  99.22 ( 99.41)
Epoch: [24][350/391]	Time  0.059 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.5205e-01 (4.0993e-01)	Acc@1  85.16 ( 85.87)	Acc@5 100.00 ( 99.41)
Epoch: [24][360/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.0042e-01 (4.1027e-01)	Acc@1  91.41 ( 85.87)	Acc@5 100.00 ( 99.41)
Epoch: [24][370/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.8236e-01 (4.1027e-01)	Acc@1  83.59 ( 85.88)	Acc@5 100.00 ( 99.41)
Epoch: [24][380/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.6164e-01 (4.1184e-01)	Acc@1  82.81 ( 85.84)	Acc@5  99.22 ( 99.42)
Epoch: [24][390/391]	Time  0.047 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.3656e-01 (4.1278e-01)	Acc@1  88.75 ( 85.81)	Acc@5 100.00 ( 99.41)
## e[24] optimizer.zero_grad (sum) time: 0.41019630432128906
## e[24]       loss.backward (sum) time: 6.982548475265503
## e[24]      optimizer.step (sum) time: 3.319101095199585
## epoch[24] training(only) time: 25.265571117401123
# Switched to evaluate mode...
Test: [  0/100]	Time  0.202 ( 0.202)	Loss 5.2209e-01 (5.2209e-01)	Acc@1  81.00 ( 81.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.024 ( 0.041)	Loss 5.1893e-01 (5.1790e-01)	Acc@1  83.00 ( 82.00)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.027 ( 0.035)	Loss 5.9376e-01 (5.3627e-01)	Acc@1  81.00 ( 81.33)	Acc@5  99.00 ( 99.38)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 5.6122e-01 (5.6115e-01)	Acc@1  80.00 ( 80.94)	Acc@5  99.00 ( 99.23)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 5.4883e-01 (5.6264e-01)	Acc@1  85.00 ( 81.07)	Acc@5  97.00 ( 99.10)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 4.9756e-01 (5.5615e-01)	Acc@1  84.00 ( 81.45)	Acc@5  99.00 ( 99.10)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 5.4935e-01 (5.5402e-01)	Acc@1  78.00 ( 81.18)	Acc@5  99.00 ( 99.18)
Test: [ 70/100]	Time  0.027 ( 0.028)	Loss 6.4511e-01 (5.4994e-01)	Acc@1  81.00 ( 81.32)	Acc@5 100.00 ( 99.21)
Test: [ 80/100]	Time  0.024 ( 0.028)	Loss 4.8661e-01 (5.4730e-01)	Acc@1  87.00 ( 81.49)	Acc@5 100.00 ( 99.26)
Test: [ 90/100]	Time  0.027 ( 0.028)	Loss 4.2922e-01 (5.4683e-01)	Acc@1  84.00 ( 81.49)	Acc@5 100.00 ( 99.25)
 * Acc@1 81.600 Acc@5 99.260
### epoch[24] execution time: 28.137372255325317
EPOCH 25
# current learning rate: 0.1
# Switched to train mode...
Epoch: [25][  0/391]	Time  0.254 ( 0.254)	Data  0.190 ( 0.190)	Loss 4.5832e-01 (4.5832e-01)	Acc@1  86.72 ( 86.72)	Acc@5 100.00 (100.00)
Epoch: [25][ 10/391]	Time  0.066 ( 0.081)	Data  0.001 ( 0.018)	Loss 2.8994e-01 (3.9302e-01)	Acc@1  89.06 ( 87.14)	Acc@5  99.22 ( 99.50)
Epoch: [25][ 20/391]	Time  0.063 ( 0.073)	Data  0.001 ( 0.010)	Loss 4.4274e-01 (4.0444e-01)	Acc@1  82.81 ( 86.46)	Acc@5 100.00 ( 99.52)
Epoch: [25][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.007)	Loss 3.7866e-01 (3.9337e-01)	Acc@1  85.94 ( 86.47)	Acc@5 100.00 ( 99.60)
Epoch: [25][ 40/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.006)	Loss 4.6300e-01 (3.8666e-01)	Acc@1  83.59 ( 86.79)	Acc@5  99.22 ( 99.60)
Epoch: [25][ 50/391]	Time  0.062 ( 0.068)	Data  0.001 ( 0.005)	Loss 3.9925e-01 (3.8838e-01)	Acc@1  85.16 ( 86.78)	Acc@5 100.00 ( 99.62)
Epoch: [25][ 60/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.004)	Loss 3.6826e-01 (3.8378e-01)	Acc@1  86.72 ( 86.91)	Acc@5 100.00 ( 99.65)
Epoch: [25][ 70/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.004)	Loss 3.3094e-01 (3.8308e-01)	Acc@1  85.16 ( 86.81)	Acc@5 100.00 ( 99.63)
Epoch: [25][ 80/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.4668e-01 (3.8549e-01)	Acc@1  87.50 ( 86.83)	Acc@5 100.00 ( 99.59)
Epoch: [25][ 90/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.6405e-01 (3.8563e-01)	Acc@1  86.72 ( 86.84)	Acc@5 100.00 ( 99.57)
Epoch: [25][100/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 5.0076e-01 (3.8673e-01)	Acc@1  80.47 ( 86.83)	Acc@5 100.00 ( 99.56)
Epoch: [25][110/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.9576e-01 (3.9003e-01)	Acc@1  85.94 ( 86.71)	Acc@5  99.22 ( 99.56)
Epoch: [25][120/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 5.3100e-01 (3.9234e-01)	Acc@1  80.47 ( 86.64)	Acc@5 100.00 ( 99.57)
Epoch: [25][130/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.9179e-01 (3.9329e-01)	Acc@1  87.50 ( 86.60)	Acc@5  99.22 ( 99.56)
Epoch: [25][140/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.2635e-01 (3.9556e-01)	Acc@1  88.28 ( 86.52)	Acc@5 100.00 ( 99.55)
Epoch: [25][150/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.6473e-01 (3.9426e-01)	Acc@1  86.72 ( 86.53)	Acc@5 100.00 ( 99.56)
Epoch: [25][160/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.1303e-01 (3.9748e-01)	Acc@1  88.28 ( 86.45)	Acc@5 100.00 ( 99.55)
Epoch: [25][170/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.1159e-01 (3.9927e-01)	Acc@1  87.50 ( 86.33)	Acc@5  99.22 ( 99.55)
Epoch: [25][180/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.0356e-01 (4.0139e-01)	Acc@1  86.72 ( 86.28)	Acc@5 100.00 ( 99.53)
Epoch: [25][190/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.0314e-01 (4.0085e-01)	Acc@1  84.38 ( 86.30)	Acc@5 100.00 ( 99.54)
Epoch: [25][200/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.3491e-01 (3.9984e-01)	Acc@1  87.50 ( 86.31)	Acc@5 100.00 ( 99.54)
Epoch: [25][210/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.9329e-01 (3.9942e-01)	Acc@1  87.50 ( 86.34)	Acc@5  98.44 ( 99.53)
Epoch: [25][220/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.6575e-01 (3.9840e-01)	Acc@1  83.59 ( 86.35)	Acc@5  99.22 ( 99.52)
Epoch: [25][230/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.9250e-01 (3.9874e-01)	Acc@1  78.91 ( 86.36)	Acc@5  99.22 ( 99.53)
Epoch: [25][240/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.1816e-01 (4.0007e-01)	Acc@1  87.50 ( 86.28)	Acc@5  99.22 ( 99.52)
Epoch: [25][250/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.4150e-01 (4.0076e-01)	Acc@1  82.81 ( 86.25)	Acc@5 100.00 ( 99.52)
Epoch: [25][260/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.9135e-01 (4.0080e-01)	Acc@1  88.28 ( 86.27)	Acc@5  99.22 ( 99.51)
Epoch: [25][270/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.7379e-01 (4.0019e-01)	Acc@1  91.41 ( 86.29)	Acc@5 100.00 ( 99.50)
Epoch: [25][280/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.1970e-01 (3.9967e-01)	Acc@1  88.28 ( 86.32)	Acc@5 100.00 ( 99.50)
Epoch: [25][290/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.6664e-01 (4.0071e-01)	Acc@1  86.72 ( 86.28)	Acc@5  99.22 ( 99.48)
Epoch: [25][300/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.5647e-01 (4.0081e-01)	Acc@1  85.94 ( 86.26)	Acc@5 100.00 ( 99.48)
Epoch: [25][310/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.4480e-01 (4.0074e-01)	Acc@1  82.81 ( 86.27)	Acc@5 100.00 ( 99.48)
Epoch: [25][320/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.6982e-01 (4.0109e-01)	Acc@1  84.38 ( 86.25)	Acc@5 100.00 ( 99.48)
Epoch: [25][330/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.7689e-01 (4.0040e-01)	Acc@1  90.62 ( 86.29)	Acc@5 100.00 ( 99.49)
Epoch: [25][340/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.1045e-01 (4.0089e-01)	Acc@1  83.59 ( 86.28)	Acc@5 100.00 ( 99.48)
Epoch: [25][350/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.8028e-01 (4.0175e-01)	Acc@1  87.50 ( 86.24)	Acc@5  99.22 ( 99.48)
Epoch: [25][360/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.2404e-01 (4.0221e-01)	Acc@1  86.72 ( 86.23)	Acc@5 100.00 ( 99.48)
Epoch: [25][370/391]	Time  0.074 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.6993e-01 (4.0288e-01)	Acc@1  89.84 ( 86.22)	Acc@5  99.22 ( 99.48)
Epoch: [25][380/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.3585e-01 (4.0311e-01)	Acc@1  82.81 ( 86.19)	Acc@5  99.22 ( 99.48)
Epoch: [25][390/391]	Time  0.047 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.2116e-01 (4.0245e-01)	Acc@1  86.25 ( 86.21)	Acc@5  98.75 ( 99.48)
## e[25] optimizer.zero_grad (sum) time: 0.4017961025238037
## e[25]       loss.backward (sum) time: 6.979040861129761
## e[25]      optimizer.step (sum) time: 3.392728567123413
## epoch[25] training(only) time: 25.37429404258728
# Switched to evaluate mode...
Test: [  0/100]	Time  0.194 ( 0.194)	Loss 6.3735e-01 (6.3735e-01)	Acc@1  78.00 ( 78.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.025 ( 0.041)	Loss 4.7356e-01 (4.3128e-01)	Acc@1  85.00 ( 84.27)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.027 ( 0.034)	Loss 4.8363e-01 (4.6772e-01)	Acc@1  80.00 ( 83.38)	Acc@5 100.00 ( 99.19)
Test: [ 30/100]	Time  0.027 ( 0.032)	Loss 5.0461e-01 (4.8490e-01)	Acc@1  83.00 ( 83.13)	Acc@5  98.00 ( 99.10)
Test: [ 40/100]	Time  0.024 ( 0.030)	Loss 4.8777e-01 (4.8227e-01)	Acc@1  85.00 ( 83.37)	Acc@5  98.00 ( 99.10)
Test: [ 50/100]	Time  0.024 ( 0.030)	Loss 4.2861e-01 (4.7957e-01)	Acc@1  85.00 ( 83.43)	Acc@5  98.00 ( 99.02)
Test: [ 60/100]	Time  0.027 ( 0.029)	Loss 5.3394e-01 (4.8457e-01)	Acc@1  82.00 ( 83.18)	Acc@5  99.00 ( 99.05)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 4.6914e-01 (4.8115e-01)	Acc@1  83.00 ( 83.25)	Acc@5 100.00 ( 99.08)
Test: [ 80/100]	Time  0.027 ( 0.028)	Loss 3.8219e-01 (4.7973e-01)	Acc@1  90.00 ( 83.32)	Acc@5 100.00 ( 99.16)
Test: [ 90/100]	Time  0.026 ( 0.028)	Loss 4.6927e-01 (4.7881e-01)	Acc@1  86.00 ( 83.37)	Acc@5 100.00 ( 99.22)
 * Acc@1 83.370 Acc@5 99.190
### epoch[25] execution time: 28.26068687438965
EPOCH 26
# current learning rate: 0.1
# Switched to train mode...
Epoch: [26][  0/391]	Time  0.247 ( 0.247)	Data  0.184 ( 0.184)	Loss 4.9188e-01 (4.9188e-01)	Acc@1  80.47 ( 80.47)	Acc@5 100.00 (100.00)
Epoch: [26][ 10/391]	Time  0.062 ( 0.081)	Data  0.001 ( 0.018)	Loss 2.8235e-01 (3.9180e-01)	Acc@1  89.84 ( 86.72)	Acc@5 100.00 ( 99.72)
Epoch: [26][ 20/391]	Time  0.063 ( 0.073)	Data  0.001 ( 0.010)	Loss 4.0150e-01 (4.0195e-01)	Acc@1  88.28 ( 86.27)	Acc@5 100.00 ( 99.67)
Epoch: [26][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.007)	Loss 4.0245e-01 (3.9855e-01)	Acc@1  85.16 ( 86.39)	Acc@5 100.00 ( 99.60)
Epoch: [26][ 40/391]	Time  0.059 ( 0.068)	Data  0.001 ( 0.006)	Loss 3.0764e-01 (3.8974e-01)	Acc@1  89.84 ( 86.53)	Acc@5 100.00 ( 99.64)
Epoch: [26][ 50/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.005)	Loss 3.0990e-01 (3.9238e-01)	Acc@1  89.84 ( 86.53)	Acc@5 100.00 ( 99.65)
Epoch: [26][ 60/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.004)	Loss 5.5734e-01 (3.9781e-01)	Acc@1  80.47 ( 86.45)	Acc@5 100.00 ( 99.63)
Epoch: [26][ 70/391]	Time  0.059 ( 0.066)	Data  0.001 ( 0.004)	Loss 4.5490e-01 (4.0129e-01)	Acc@1  86.72 ( 86.42)	Acc@5 100.00 ( 99.60)
Epoch: [26][ 80/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.9064e-01 (4.0068e-01)	Acc@1  83.59 ( 86.52)	Acc@5  99.22 ( 99.62)
Epoch: [26][ 90/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.1917e-01 (4.0125e-01)	Acc@1  87.50 ( 86.50)	Acc@5 100.00 ( 99.61)
Epoch: [26][100/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 3.4910e-01 (4.0038e-01)	Acc@1  87.50 ( 86.49)	Acc@5  99.22 ( 99.61)
Epoch: [26][110/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.003)	Loss 3.7612e-01 (3.9961e-01)	Acc@1  87.50 ( 86.54)	Acc@5  98.44 ( 99.60)
Epoch: [26][120/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.003)	Loss 3.0494e-01 (3.9667e-01)	Acc@1  88.28 ( 86.62)	Acc@5 100.00 ( 99.61)
Epoch: [26][130/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 3.0439e-01 (3.9684e-01)	Acc@1  89.84 ( 86.62)	Acc@5 100.00 ( 99.62)
Epoch: [26][140/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.2899e-01 (3.9687e-01)	Acc@1  82.81 ( 86.59)	Acc@5 100.00 ( 99.62)
Epoch: [26][150/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.2639e-01 (4.0135e-01)	Acc@1  87.50 ( 86.37)	Acc@5 100.00 ( 99.60)
Epoch: [26][160/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.7524e-01 (4.0439e-01)	Acc@1  91.41 ( 86.28)	Acc@5 100.00 ( 99.59)
Epoch: [26][170/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.7756e-01 (4.0426e-01)	Acc@1  88.28 ( 86.28)	Acc@5  99.22 ( 99.58)
Epoch: [26][180/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.3952e-01 (4.0485e-01)	Acc@1  86.72 ( 86.28)	Acc@5 100.00 ( 99.56)
Epoch: [26][190/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.4647e-01 (4.0379e-01)	Acc@1  85.94 ( 86.25)	Acc@5 100.00 ( 99.56)
Epoch: [26][200/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.1648e-01 (4.0530e-01)	Acc@1  86.72 ( 86.17)	Acc@5 100.00 ( 99.55)
Epoch: [26][210/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.1944e-01 (4.0538e-01)	Acc@1  85.16 ( 86.18)	Acc@5 100.00 ( 99.56)
Epoch: [26][220/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.9856e-01 (4.0415e-01)	Acc@1  89.84 ( 86.22)	Acc@5 100.00 ( 99.55)
Epoch: [26][230/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.1587e-01 (4.0303e-01)	Acc@1  90.62 ( 86.22)	Acc@5  99.22 ( 99.55)
Epoch: [26][240/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.1888e-01 (4.0175e-01)	Acc@1  89.06 ( 86.25)	Acc@5 100.00 ( 99.54)
Epoch: [26][250/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.2977e-01 (4.0158e-01)	Acc@1  89.06 ( 86.27)	Acc@5 100.00 ( 99.54)
Epoch: [26][260/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.7912e-01 (4.0162e-01)	Acc@1  87.50 ( 86.25)	Acc@5 100.00 ( 99.54)
Epoch: [26][270/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.7354e-01 (4.0235e-01)	Acc@1  89.06 ( 86.25)	Acc@5 100.00 ( 99.54)
Epoch: [26][280/391]	Time  0.059 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.4765e-01 (4.0277e-01)	Acc@1  80.47 ( 86.22)	Acc@5  98.44 ( 99.55)
Epoch: [26][290/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.6874e-01 (4.0238e-01)	Acc@1  83.59 ( 86.19)	Acc@5 100.00 ( 99.55)
Epoch: [26][300/391]	Time  0.066 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.1408e-01 (4.0254e-01)	Acc@1  87.50 ( 86.18)	Acc@5 100.00 ( 99.55)
Epoch: [26][310/391]	Time  0.060 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.4323e-01 (4.0204e-01)	Acc@1  86.72 ( 86.18)	Acc@5  99.22 ( 99.56)
Epoch: [26][320/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.9962e-01 (4.0190e-01)	Acc@1  87.50 ( 86.17)	Acc@5  98.44 ( 99.55)
Epoch: [26][330/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.0161e-01 (4.0172e-01)	Acc@1  84.38 ( 86.15)	Acc@5  99.22 ( 99.54)
Epoch: [26][340/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.2915e-01 (4.0203e-01)	Acc@1  87.50 ( 86.16)	Acc@5  99.22 ( 99.55)
Epoch: [26][350/391]	Time  0.060 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.3621e-01 (4.0181e-01)	Acc@1  85.94 ( 86.18)	Acc@5  99.22 ( 99.54)
Epoch: [26][360/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.6812e-01 (4.0187e-01)	Acc@1  85.16 ( 86.16)	Acc@5 100.00 ( 99.55)
Epoch: [26][370/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.9613e-01 (4.0243e-01)	Acc@1  85.94 ( 86.15)	Acc@5  99.22 ( 99.54)
Epoch: [26][380/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.7688e-01 (4.0224e-01)	Acc@1  85.94 ( 86.15)	Acc@5 100.00 ( 99.54)
Epoch: [26][390/391]	Time  0.046 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.3838e-01 (4.0254e-01)	Acc@1  92.50 ( 86.15)	Acc@5 100.00 ( 99.54)
## e[26] optimizer.zero_grad (sum) time: 0.3992292881011963
## e[26]       loss.backward (sum) time: 6.924927711486816
## e[26]      optimizer.step (sum) time: 3.3353612422943115
## epoch[26] training(only) time: 25.163490772247314
# Switched to evaluate mode...
Test: [  0/100]	Time  0.193 ( 0.193)	Loss 5.1373e-01 (5.1373e-01)	Acc@1  82.00 ( 82.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.025 ( 0.041)	Loss 4.7167e-01 (4.5563e-01)	Acc@1  86.00 ( 84.82)	Acc@5 100.00 ( 99.36)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 4.9641e-01 (4.6843e-01)	Acc@1  79.00 ( 84.24)	Acc@5 100.00 ( 99.33)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 5.0370e-01 (4.7910e-01)	Acc@1  77.00 ( 83.84)	Acc@5 100.00 ( 99.26)
Test: [ 40/100]	Time  0.024 ( 0.030)	Loss 4.3021e-01 (4.7982e-01)	Acc@1  84.00 ( 83.66)	Acc@5  98.00 ( 99.22)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 5.2785e-01 (4.7679e-01)	Acc@1  83.00 ( 83.86)	Acc@5  98.00 ( 99.24)
Test: [ 60/100]	Time  0.027 ( 0.028)	Loss 5.6834e-01 (4.8014e-01)	Acc@1  81.00 ( 83.70)	Acc@5 100.00 ( 99.30)
Test: [ 70/100]	Time  0.027 ( 0.028)	Loss 5.0462e-01 (4.7933e-01)	Acc@1  84.00 ( 83.70)	Acc@5  99.00 ( 99.34)
Test: [ 80/100]	Time  0.029 ( 0.028)	Loss 4.2927e-01 (4.7451e-01)	Acc@1  84.00 ( 83.84)	Acc@5  99.00 ( 99.37)
Test: [ 90/100]	Time  0.027 ( 0.027)	Loss 3.8727e-01 (4.7354e-01)	Acc@1  83.00 ( 83.84)	Acc@5  99.00 ( 99.40)
 * Acc@1 83.810 Acc@5 99.400
### epoch[26] execution time: 28.00248646736145
EPOCH 27
# current learning rate: 0.1
# Switched to train mode...
Epoch: [27][  0/391]	Time  0.249 ( 0.249)	Data  0.186 ( 0.186)	Loss 3.8017e-01 (3.8017e-01)	Acc@1  88.28 ( 88.28)	Acc@5  99.22 ( 99.22)
Epoch: [27][ 10/391]	Time  0.068 ( 0.081)	Data  0.001 ( 0.018)	Loss 4.1389e-01 (3.8798e-01)	Acc@1  82.03 ( 86.15)	Acc@5 100.00 ( 99.57)
Epoch: [27][ 20/391]	Time  0.067 ( 0.073)	Data  0.001 ( 0.010)	Loss 4.4120e-01 (4.0080e-01)	Acc@1  85.16 ( 86.12)	Acc@5  98.44 ( 99.44)
Epoch: [27][ 30/391]	Time  0.060 ( 0.070)	Data  0.001 ( 0.007)	Loss 4.1895e-01 (3.9447e-01)	Acc@1  83.59 ( 86.24)	Acc@5  99.22 ( 99.47)
Epoch: [27][ 40/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.006)	Loss 2.7691e-01 (3.8272e-01)	Acc@1  88.28 ( 86.53)	Acc@5 100.00 ( 99.54)
Epoch: [27][ 50/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.005)	Loss 5.2181e-01 (3.9231e-01)	Acc@1  83.59 ( 86.47)	Acc@5  97.66 ( 99.40)
Epoch: [27][ 60/391]	Time  0.074 ( 0.067)	Data  0.001 ( 0.004)	Loss 3.2955e-01 (3.9151e-01)	Acc@1  88.28 ( 86.53)	Acc@5 100.00 ( 99.47)
Epoch: [27][ 70/391]	Time  0.071 ( 0.067)	Data  0.001 ( 0.004)	Loss 5.5393e-01 (3.8986e-01)	Acc@1  80.47 ( 86.66)	Acc@5 100.00 ( 99.50)
Epoch: [27][ 80/391]	Time  0.058 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.6865e-01 (3.8481e-01)	Acc@1  89.84 ( 86.83)	Acc@5 100.00 ( 99.51)
Epoch: [27][ 90/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.0421e-01 (3.8371e-01)	Acc@1  86.72 ( 86.84)	Acc@5 100.00 ( 99.51)
Epoch: [27][100/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.3492e-01 (3.8097e-01)	Acc@1  90.62 ( 86.94)	Acc@5  98.44 ( 99.51)
Epoch: [27][110/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.4373e-01 (3.8414e-01)	Acc@1  89.06 ( 86.80)	Acc@5 100.00 ( 99.54)
Epoch: [27][120/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.0475e-01 (3.8180e-01)	Acc@1  85.94 ( 86.84)	Acc@5 100.00 ( 99.54)
Epoch: [27][130/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.003)	Loss 3.7498e-01 (3.8348e-01)	Acc@1  86.72 ( 86.80)	Acc@5  99.22 ( 99.53)
Epoch: [27][140/391]	Time  0.064 ( 0.065)	Data  0.002 ( 0.002)	Loss 4.5217e-01 (3.8287e-01)	Acc@1  85.94 ( 86.85)	Acc@5  98.44 ( 99.53)
Epoch: [27][150/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.6574e-01 (3.8392e-01)	Acc@1  84.38 ( 86.79)	Acc@5 100.00 ( 99.54)
Epoch: [27][160/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.2191e-01 (3.8519e-01)	Acc@1  87.50 ( 86.74)	Acc@5  99.22 ( 99.55)
Epoch: [27][170/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.5002e-01 (3.8527e-01)	Acc@1  82.81 ( 86.79)	Acc@5  99.22 ( 99.55)
Epoch: [27][180/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.4279e-01 (3.8732e-01)	Acc@1  86.72 ( 86.73)	Acc@5 100.00 ( 99.54)
Epoch: [27][190/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.7857e-01 (3.8835e-01)	Acc@1  86.72 ( 86.72)	Acc@5 100.00 ( 99.54)
Epoch: [27][200/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.3803e-01 (3.8694e-01)	Acc@1  89.06 ( 86.80)	Acc@5  98.44 ( 99.54)
Epoch: [27][210/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.1491e-01 (3.8644e-01)	Acc@1  83.59 ( 86.79)	Acc@5 100.00 ( 99.55)
Epoch: [27][220/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.7021e-01 (3.8752e-01)	Acc@1  87.50 ( 86.77)	Acc@5 100.00 ( 99.55)
Epoch: [27][230/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.6968e-01 (3.9050e-01)	Acc@1  85.94 ( 86.67)	Acc@5  99.22 ( 99.54)
Epoch: [27][240/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.9430e-01 (3.9196e-01)	Acc@1  81.25 ( 86.60)	Acc@5 100.00 ( 99.54)
Epoch: [27][250/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.0851e-01 (3.9474e-01)	Acc@1  81.25 ( 86.45)	Acc@5  99.22 ( 99.53)
Epoch: [27][260/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.3823e-01 (3.9690e-01)	Acc@1  78.91 ( 86.39)	Acc@5  99.22 ( 99.51)
Epoch: [27][270/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.1158e-01 (3.9682e-01)	Acc@1  85.94 ( 86.41)	Acc@5 100.00 ( 99.52)
Epoch: [27][280/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.6830e-01 (3.9743e-01)	Acc@1  87.50 ( 86.40)	Acc@5  97.66 ( 99.50)
Epoch: [27][290/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.9799e-01 (3.9702e-01)	Acc@1  86.72 ( 86.41)	Acc@5 100.00 ( 99.51)
Epoch: [27][300/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.1400e-01 (3.9715e-01)	Acc@1  90.62 ( 86.43)	Acc@5 100.00 ( 99.50)
Epoch: [27][310/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.5129e-01 (3.9578e-01)	Acc@1  92.19 ( 86.46)	Acc@5  99.22 ( 99.50)
Epoch: [27][320/391]	Time  0.069 ( 0.065)	Data  0.002 ( 0.002)	Loss 3.2191e-01 (3.9544e-01)	Acc@1  89.06 ( 86.44)	Acc@5 100.00 ( 99.51)
Epoch: [27][330/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.5034e-01 (3.9466e-01)	Acc@1  87.50 ( 86.44)	Acc@5 100.00 ( 99.52)
Epoch: [27][340/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.3367e-01 (3.9562e-01)	Acc@1  85.94 ( 86.42)	Acc@5 100.00 ( 99.53)
Epoch: [27][350/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.3106e-01 (3.9577e-01)	Acc@1  84.38 ( 86.40)	Acc@5 100.00 ( 99.53)
Epoch: [27][360/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.3803e-01 (3.9818e-01)	Acc@1  82.03 ( 86.35)	Acc@5 100.00 ( 99.51)
Epoch: [27][370/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.2230e-01 (3.9842e-01)	Acc@1  91.41 ( 86.36)	Acc@5 100.00 ( 99.51)
Epoch: [27][380/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.2137e-01 (3.9891e-01)	Acc@1  84.38 ( 86.32)	Acc@5 100.00 ( 99.51)
Epoch: [27][390/391]	Time  0.045 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.7504e-01 (3.9805e-01)	Acc@1  83.75 ( 86.32)	Acc@5 100.00 ( 99.50)
## e[27] optimizer.zero_grad (sum) time: 0.4109189510345459
## e[27]       loss.backward (sum) time: 7.019257545471191
## e[27]      optimizer.step (sum) time: 3.3701698780059814
## epoch[27] training(only) time: 25.439738750457764
# Switched to evaluate mode...
Test: [  0/100]	Time  0.198 ( 0.198)	Loss 4.4000e-01 (4.4000e-01)	Acc@1  83.00 ( 83.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.027 ( 0.042)	Loss 4.4185e-01 (4.4380e-01)	Acc@1  86.00 ( 85.00)	Acc@5  99.00 ( 99.45)
Test: [ 20/100]	Time  0.024 ( 0.034)	Loss 4.6769e-01 (4.6951e-01)	Acc@1  78.00 ( 83.90)	Acc@5 100.00 ( 99.43)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 4.4196e-01 (4.6593e-01)	Acc@1  85.00 ( 83.97)	Acc@5 100.00 ( 99.48)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 3.7502e-01 (4.6248e-01)	Acc@1  84.00 ( 84.15)	Acc@5  99.00 ( 99.37)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 3.4156e-01 (4.6110e-01)	Acc@1  91.00 ( 84.61)	Acc@5  99.00 ( 99.31)
Test: [ 60/100]	Time  0.028 ( 0.028)	Loss 6.0506e-01 (4.6531e-01)	Acc@1  79.00 ( 84.41)	Acc@5 100.00 ( 99.38)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 3.2409e-01 (4.5546e-01)	Acc@1  87.00 ( 84.72)	Acc@5 100.00 ( 99.44)
Test: [ 80/100]	Time  0.028 ( 0.028)	Loss 3.6590e-01 (4.5365e-01)	Acc@1  87.00 ( 84.64)	Acc@5 100.00 ( 99.47)
Test: [ 90/100]	Time  0.028 ( 0.028)	Loss 3.4750e-01 (4.5354e-01)	Acc@1  93.00 ( 84.60)	Acc@5 100.00 ( 99.48)
 * Acc@1 84.650 Acc@5 99.490
### epoch[27] execution time: 28.336992740631104
EPOCH 28
# current learning rate: 0.1
# Switched to train mode...
Epoch: [28][  0/391]	Time  0.250 ( 0.250)	Data  0.187 ( 0.187)	Loss 3.6399e-01 (3.6399e-01)	Acc@1  88.28 ( 88.28)	Acc@5 100.00 (100.00)
Epoch: [28][ 10/391]	Time  0.064 ( 0.080)	Data  0.001 ( 0.018)	Loss 4.4394e-01 (3.7649e-01)	Acc@1  81.25 ( 85.72)	Acc@5  99.22 ( 99.93)
Epoch: [28][ 20/391]	Time  0.067 ( 0.073)	Data  0.001 ( 0.010)	Loss 4.6164e-01 (3.7704e-01)	Acc@1  85.16 ( 86.79)	Acc@5 100.00 ( 99.67)
Epoch: [28][ 30/391]	Time  0.058 ( 0.070)	Data  0.001 ( 0.007)	Loss 3.7900e-01 (3.8297e-01)	Acc@1  84.38 ( 86.29)	Acc@5  99.22 ( 99.60)
Epoch: [28][ 40/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.006)	Loss 3.6220e-01 (3.8581e-01)	Acc@1  87.50 ( 86.39)	Acc@5  99.22 ( 99.56)
Epoch: [28][ 50/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.005)	Loss 3.5128e-01 (3.8896e-01)	Acc@1  89.84 ( 86.46)	Acc@5  99.22 ( 99.56)
Epoch: [28][ 60/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 3.8482e-01 (3.8912e-01)	Acc@1  87.50 ( 86.67)	Acc@5  99.22 ( 99.56)
Epoch: [28][ 70/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.004)	Loss 3.9739e-01 (3.8658e-01)	Acc@1  84.38 ( 86.76)	Acc@5 100.00 ( 99.58)
Epoch: [28][ 80/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.7602e-01 (3.8131e-01)	Acc@1  89.06 ( 86.94)	Acc@5 100.00 ( 99.56)
Epoch: [28][ 90/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.003)	Loss 3.7883e-01 (3.7936e-01)	Acc@1  86.72 ( 87.00)	Acc@5  99.22 ( 99.56)
Epoch: [28][100/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 3.4257e-01 (3.7938e-01)	Acc@1  90.62 ( 87.02)	Acc@5 100.00 ( 99.57)
Epoch: [28][110/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.003)	Loss 2.6364e-01 (3.7915e-01)	Acc@1  92.19 ( 87.05)	Acc@5 100.00 ( 99.58)
Epoch: [28][120/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.003)	Loss 3.9836e-01 (3.8000e-01)	Acc@1  85.94 ( 86.90)	Acc@5 100.00 ( 99.57)
Epoch: [28][130/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.003)	Loss 4.9792e-01 (3.8138e-01)	Acc@1  80.47 ( 86.81)	Acc@5 100.00 ( 99.57)
Epoch: [28][140/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.4714e-01 (3.8159e-01)	Acc@1  86.72 ( 86.87)	Acc@5  98.44 ( 99.56)
Epoch: [28][150/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3720e-01 (3.8028e-01)	Acc@1  92.97 ( 87.00)	Acc@5 100.00 ( 99.57)
Epoch: [28][160/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.0454e-01 (3.8081e-01)	Acc@1  82.03 ( 87.01)	Acc@5  99.22 ( 99.56)
Epoch: [28][170/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.0275e-01 (3.8266e-01)	Acc@1  85.94 ( 86.99)	Acc@5 100.00 ( 99.56)
Epoch: [28][180/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.4725e-01 (3.8603e-01)	Acc@1  72.66 ( 86.77)	Acc@5  99.22 ( 99.56)
Epoch: [28][190/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.9589e-01 (3.8788e-01)	Acc@1  85.16 ( 86.67)	Acc@5 100.00 ( 99.55)
Epoch: [28][200/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.4708e-01 (3.8606e-01)	Acc@1  85.94 ( 86.78)	Acc@5 100.00 ( 99.55)
Epoch: [28][210/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.3551e-01 (3.8599e-01)	Acc@1  89.84 ( 86.77)	Acc@5 100.00 ( 99.56)
Epoch: [28][220/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.6979e-01 (3.8550e-01)	Acc@1  86.72 ( 86.75)	Acc@5  98.44 ( 99.54)
Epoch: [28][230/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.7717e-01 (3.8595e-01)	Acc@1  85.16 ( 86.75)	Acc@5 100.00 ( 99.52)
Epoch: [28][240/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.5625e-01 (3.8684e-01)	Acc@1  91.41 ( 86.76)	Acc@5 100.00 ( 99.53)
Epoch: [28][250/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.0738e-01 (3.8759e-01)	Acc@1  87.50 ( 86.73)	Acc@5 100.00 ( 99.53)
Epoch: [28][260/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.6832e-01 (3.8803e-01)	Acc@1  86.72 ( 86.70)	Acc@5  99.22 ( 99.53)
Epoch: [28][270/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.5755e-01 (3.8887e-01)	Acc@1  86.72 ( 86.68)	Acc@5 100.00 ( 99.54)
Epoch: [28][280/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.7369e-01 (3.8917e-01)	Acc@1  85.16 ( 86.66)	Acc@5 100.00 ( 99.54)
Epoch: [28][290/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.2725e-01 (3.8991e-01)	Acc@1  82.81 ( 86.61)	Acc@5  99.22 ( 99.54)
Epoch: [28][300/391]	Time  0.069 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.8851e-01 (3.8832e-01)	Acc@1  87.50 ( 86.65)	Acc@5 100.00 ( 99.54)
Epoch: [28][310/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.8035e-01 (3.8930e-01)	Acc@1  82.81 ( 86.64)	Acc@5  98.44 ( 99.53)
Epoch: [28][320/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.4698e-01 (3.8961e-01)	Acc@1  86.72 ( 86.62)	Acc@5  99.22 ( 99.54)
Epoch: [28][330/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.0777e-01 (3.9009e-01)	Acc@1  85.16 ( 86.61)	Acc@5  99.22 ( 99.54)
Epoch: [28][340/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.8924e-01 (3.8825e-01)	Acc@1  89.84 ( 86.68)	Acc@5 100.00 ( 99.54)
Epoch: [28][350/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.0597e-01 (3.8930e-01)	Acc@1  80.47 ( 86.63)	Acc@5 100.00 ( 99.54)
Epoch: [28][360/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.0466e-01 (3.8847e-01)	Acc@1  89.06 ( 86.66)	Acc@5  99.22 ( 99.54)
Epoch: [28][370/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.7596e-01 (3.8962e-01)	Acc@1  79.69 ( 86.61)	Acc@5 100.00 ( 99.54)
Epoch: [28][380/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.5266e-01 (3.8974e-01)	Acc@1  82.03 ( 86.59)	Acc@5  97.66 ( 99.54)
Epoch: [28][390/391]	Time  0.047 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.7450e-01 (3.8904e-01)	Acc@1  87.50 ( 86.61)	Acc@5  97.50 ( 99.54)
## e[28] optimizer.zero_grad (sum) time: 0.39721059799194336
## e[28]       loss.backward (sum) time: 6.927814960479736
## e[28]      optimizer.step (sum) time: 3.388246536254883
## epoch[28] training(only) time: 25.211737871170044
# Switched to evaluate mode...
Test: [  0/100]	Time  0.197 ( 0.197)	Loss 5.2135e-01 (5.2135e-01)	Acc@1  86.00 ( 86.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.026 ( 0.042)	Loss 4.1255e-01 (4.5600e-01)	Acc@1  86.00 ( 84.64)	Acc@5 100.00 ( 99.45)
Test: [ 20/100]	Time  0.024 ( 0.034)	Loss 5.5725e-01 (4.7618e-01)	Acc@1  82.00 ( 84.14)	Acc@5  99.00 ( 99.33)
Test: [ 30/100]	Time  0.024 ( 0.032)	Loss 5.2924e-01 (4.9177e-01)	Acc@1  83.00 ( 83.71)	Acc@5  99.00 ( 99.29)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 4.2377e-01 (4.9008e-01)	Acc@1  84.00 ( 83.59)	Acc@5  99.00 ( 99.24)
Test: [ 50/100]	Time  0.026 ( 0.029)	Loss 4.1300e-01 (4.9017e-01)	Acc@1  85.00 ( 83.63)	Acc@5  99.00 ( 99.31)
Test: [ 60/100]	Time  0.024 ( 0.029)	Loss 3.3206e-01 (4.8769e-01)	Acc@1  91.00 ( 83.67)	Acc@5 100.00 ( 99.36)
Test: [ 70/100]	Time  0.028 ( 0.028)	Loss 5.5875e-01 (4.7827e-01)	Acc@1  82.00 ( 84.00)	Acc@5  99.00 ( 99.39)
Test: [ 80/100]	Time  0.024 ( 0.028)	Loss 4.2167e-01 (4.7404e-01)	Acc@1  84.00 ( 83.98)	Acc@5 100.00 ( 99.41)
Test: [ 90/100]	Time  0.026 ( 0.028)	Loss 3.4837e-01 (4.7393e-01)	Acc@1  85.00 ( 83.86)	Acc@5 100.00 ( 99.42)
 * Acc@1 83.910 Acc@5 99.430
### epoch[28] execution time: 28.121461391448975
EPOCH 29
# current learning rate: 0.1
# Switched to train mode...
Epoch: [29][  0/391]	Time  0.251 ( 0.251)	Data  0.185 ( 0.185)	Loss 5.4568e-01 (5.4568e-01)	Acc@1  80.47 ( 80.47)	Acc@5  99.22 ( 99.22)
Epoch: [29][ 10/391]	Time  0.062 ( 0.078)	Data  0.001 ( 0.018)	Loss 3.9082e-01 (3.9380e-01)	Acc@1  85.94 ( 85.87)	Acc@5  99.22 ( 99.64)
Epoch: [29][ 20/391]	Time  0.065 ( 0.071)	Data  0.001 ( 0.010)	Loss 2.8538e-01 (3.7309e-01)	Acc@1  90.62 ( 86.38)	Acc@5 100.00 ( 99.67)
Epoch: [29][ 30/391]	Time  0.063 ( 0.069)	Data  0.001 ( 0.007)	Loss 3.1639e-01 (3.5990e-01)	Acc@1  90.62 ( 87.02)	Acc@5 100.00 ( 99.65)
Epoch: [29][ 40/391]	Time  0.059 ( 0.068)	Data  0.001 ( 0.006)	Loss 2.1892e-01 (3.5764e-01)	Acc@1  92.19 ( 87.31)	Acc@5 100.00 ( 99.62)
Epoch: [29][ 50/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.005)	Loss 4.1426e-01 (3.5667e-01)	Acc@1  85.16 ( 87.35)	Acc@5 100.00 ( 99.66)
Epoch: [29][ 60/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.004)	Loss 4.0792e-01 (3.5832e-01)	Acc@1  86.72 ( 87.23)	Acc@5  98.44 ( 99.64)
Epoch: [29][ 70/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.004)	Loss 4.2471e-01 (3.6625e-01)	Acc@1  85.16 ( 87.08)	Acc@5  98.44 ( 99.63)
Epoch: [29][ 80/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.5947e-01 (3.6807e-01)	Acc@1  87.50 ( 87.03)	Acc@5  98.44 ( 99.59)
Epoch: [29][ 90/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 4.8932e-01 (3.7108e-01)	Acc@1  85.16 ( 87.02)	Acc@5  98.44 ( 99.60)
Epoch: [29][100/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 3.3730e-01 (3.6856e-01)	Acc@1  86.72 ( 87.07)	Acc@5 100.00 ( 99.62)
Epoch: [29][110/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.003)	Loss 4.7538e-01 (3.7068e-01)	Acc@1  86.72 ( 87.17)	Acc@5 100.00 ( 99.62)
Epoch: [29][120/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.003)	Loss 4.2355e-01 (3.7542e-01)	Acc@1  84.38 ( 87.02)	Acc@5 100.00 ( 99.63)
Epoch: [29][130/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.7949e-01 (3.7460e-01)	Acc@1  84.38 ( 87.00)	Acc@5  98.44 ( 99.63)
Epoch: [29][140/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.6783e-01 (3.7524e-01)	Acc@1  85.94 ( 86.97)	Acc@5  99.22 ( 99.63)
Epoch: [29][150/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.8626e-01 (3.7871e-01)	Acc@1  85.94 ( 86.83)	Acc@5  99.22 ( 99.63)
Epoch: [29][160/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.3251e-01 (3.7883e-01)	Acc@1  88.28 ( 86.84)	Acc@5  98.44 ( 99.61)
Epoch: [29][170/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.5894e-01 (3.7932e-01)	Acc@1  88.28 ( 86.82)	Acc@5 100.00 ( 99.62)
Epoch: [29][180/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.4746e-01 (3.8087e-01)	Acc@1  88.28 ( 86.77)	Acc@5 100.00 ( 99.62)
Epoch: [29][190/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.6022e-01 (3.8021e-01)	Acc@1  82.81 ( 86.82)	Acc@5  99.22 ( 99.63)
Epoch: [29][200/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.6747e-01 (3.8191e-01)	Acc@1  83.59 ( 86.73)	Acc@5 100.00 ( 99.63)
Epoch: [29][210/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.2507e-01 (3.8346e-01)	Acc@1  85.16 ( 86.65)	Acc@5  99.22 ( 99.64)
Epoch: [29][220/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.5407e-01 (3.8313e-01)	Acc@1  87.50 ( 86.67)	Acc@5  98.44 ( 99.63)
Epoch: [29][230/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.7067e-01 (3.8436e-01)	Acc@1  79.69 ( 86.63)	Acc@5 100.00 ( 99.60)
Epoch: [29][240/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.5107e-01 (3.8354e-01)	Acc@1  89.06 ( 86.69)	Acc@5 100.00 ( 99.61)
Epoch: [29][250/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.5615e-01 (3.8329e-01)	Acc@1  85.16 ( 86.73)	Acc@5  99.22 ( 99.61)
Epoch: [29][260/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.6354e-01 (3.8370e-01)	Acc@1  88.28 ( 86.76)	Acc@5  99.22 ( 99.60)
Epoch: [29][270/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.2721e-01 (3.8577e-01)	Acc@1  85.16 ( 86.67)	Acc@5 100.00 ( 99.59)
Epoch: [29][280/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.7043e-01 (3.8651e-01)	Acc@1  87.50 ( 86.64)	Acc@5 100.00 ( 99.59)
Epoch: [29][290/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.2854e-01 (3.8636e-01)	Acc@1  89.06 ( 86.66)	Acc@5 100.00 ( 99.60)
Epoch: [29][300/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.5858e-01 (3.8689e-01)	Acc@1  84.38 ( 86.65)	Acc@5  99.22 ( 99.60)
Epoch: [29][310/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.7204e-01 (3.8789e-01)	Acc@1  84.38 ( 86.63)	Acc@5  99.22 ( 99.60)
Epoch: [29][320/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.9848e-01 (3.8939e-01)	Acc@1  83.59 ( 86.59)	Acc@5  99.22 ( 99.58)
Epoch: [29][330/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.0062e-01 (3.9042e-01)	Acc@1  89.84 ( 86.56)	Acc@5 100.00 ( 99.58)
Epoch: [29][340/391]	Time  0.060 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.4387e-01 (3.9014e-01)	Acc@1  85.94 ( 86.55)	Acc@5 100.00 ( 99.58)
Epoch: [29][350/391]	Time  0.058 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.1464e-01 (3.9067e-01)	Acc@1  89.06 ( 86.54)	Acc@5 100.00 ( 99.56)
Epoch: [29][360/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.0829e-01 (3.9228e-01)	Acc@1  86.72 ( 86.49)	Acc@5 100.00 ( 99.56)
Epoch: [29][370/391]	Time  0.066 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.2180e-01 (3.9304e-01)	Acc@1  84.38 ( 86.48)	Acc@5  99.22 ( 99.55)
Epoch: [29][380/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.2842e-01 (3.9357e-01)	Acc@1  85.16 ( 86.45)	Acc@5  99.22 ( 99.54)
Epoch: [29][390/391]	Time  0.046 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.5201e-01 (3.9359e-01)	Acc@1  86.25 ( 86.46)	Acc@5  98.75 ( 99.54)
## e[29] optimizer.zero_grad (sum) time: 0.39926624298095703
## e[29]       loss.backward (sum) time: 6.935473918914795
## e[29]      optimizer.step (sum) time: 3.3401076793670654
## epoch[29] training(only) time: 25.24456548690796
# Switched to evaluate mode...
Test: [  0/100]	Time  0.190 ( 0.190)	Loss 3.7202e-01 (3.7202e-01)	Acc@1  87.00 ( 87.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.040)	Loss 3.6276e-01 (4.1946e-01)	Acc@1  85.00 ( 85.18)	Acc@5  99.00 ( 99.27)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 4.6716e-01 (4.3721e-01)	Acc@1  84.00 ( 84.95)	Acc@5 100.00 ( 99.33)
Test: [ 30/100]	Time  0.024 ( 0.030)	Loss 4.4041e-01 (4.4930e-01)	Acc@1  85.00 ( 84.87)	Acc@5 100.00 ( 99.13)
Test: [ 40/100]	Time  0.032 ( 0.029)	Loss 3.7218e-01 (4.4881e-01)	Acc@1  89.00 ( 84.98)	Acc@5  97.00 ( 99.10)
Test: [ 50/100]	Time  0.026 ( 0.028)	Loss 4.2657e-01 (4.4697e-01)	Acc@1  82.00 ( 84.98)	Acc@5  98.00 ( 99.10)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 4.9467e-01 (4.5195e-01)	Acc@1  81.00 ( 84.92)	Acc@5 100.00 ( 99.15)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 4.9022e-01 (4.5039e-01)	Acc@1  85.00 ( 84.99)	Acc@5  99.00 ( 99.20)
Test: [ 80/100]	Time  0.027 ( 0.027)	Loss 3.4857e-01 (4.4645e-01)	Acc@1  86.00 ( 85.02)	Acc@5  99.00 ( 99.25)
Test: [ 90/100]	Time  0.029 ( 0.027)	Loss 4.1720e-01 (4.4601e-01)	Acc@1  87.00 ( 85.09)	Acc@5 100.00 ( 99.25)
 * Acc@1 85.120 Acc@5 99.290
### epoch[29] execution time: 28.06136465072632
EPOCH 30
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [30][  0/391]	Time  0.258 ( 0.258)	Data  0.195 ( 0.195)	Loss 5.0210e-01 (5.0210e-01)	Acc@1  85.16 ( 85.16)	Acc@5  99.22 ( 99.22)
Epoch: [30][ 10/391]	Time  0.062 ( 0.082)	Data  0.001 ( 0.019)	Loss 3.0063e-01 (3.6510e-01)	Acc@1  89.06 ( 87.57)	Acc@5 100.00 ( 99.72)
Epoch: [30][ 20/391]	Time  0.063 ( 0.073)	Data  0.001 ( 0.010)	Loss 3.1175e-01 (3.5386e-01)	Acc@1  90.62 ( 87.91)	Acc@5 100.00 ( 99.74)
Epoch: [30][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.007)	Loss 3.2654e-01 (3.3909e-01)	Acc@1  89.06 ( 88.31)	Acc@5  99.22 ( 99.75)
Epoch: [30][ 40/391]	Time  0.062 ( 0.069)	Data  0.001 ( 0.006)	Loss 4.3829e-01 (3.3349e-01)	Acc@1  85.16 ( 88.61)	Acc@5 100.00 ( 99.75)
Epoch: [30][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.005)	Loss 4.5723e-01 (3.3319e-01)	Acc@1  85.16 ( 88.68)	Acc@5  99.22 ( 99.75)
Epoch: [30][ 60/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 2.9024e-01 (3.2964e-01)	Acc@1  92.97 ( 88.79)	Acc@5 100.00 ( 99.77)
Epoch: [30][ 70/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.004)	Loss 3.0421e-01 (3.2687e-01)	Acc@1  86.72 ( 88.80)	Acc@5 100.00 ( 99.76)
Epoch: [30][ 80/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.004)	Loss 2.4398e-01 (3.1943e-01)	Acc@1  92.97 ( 89.11)	Acc@5 100.00 ( 99.78)
Epoch: [30][ 90/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.6464e-01 (3.1741e-01)	Acc@1  96.09 ( 89.12)	Acc@5 100.00 ( 99.78)
Epoch: [30][100/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.1984e-01 (3.1893e-01)	Acc@1  90.62 ( 89.03)	Acc@5  99.22 ( 99.76)
Epoch: [30][110/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.8706e-01 (3.1502e-01)	Acc@1  91.41 ( 89.16)	Acc@5 100.00 ( 99.75)
Epoch: [30][120/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.4886e-01 (3.1264e-01)	Acc@1  89.84 ( 89.31)	Acc@5  99.22 ( 99.75)
Epoch: [30][130/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.003)	Loss 3.2360e-01 (3.0987e-01)	Acc@1  85.94 ( 89.42)	Acc@5 100.00 ( 99.76)
Epoch: [30][140/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 2.4875e-01 (3.0733e-01)	Acc@1  90.62 ( 89.45)	Acc@5 100.00 ( 99.77)
Epoch: [30][150/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0613e-01 (3.0556e-01)	Acc@1  93.75 ( 89.51)	Acc@5 100.00 ( 99.78)
Epoch: [30][160/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.2157e-01 (3.0453e-01)	Acc@1  89.84 ( 89.58)	Acc@5  99.22 ( 99.78)
Epoch: [30][170/391]	Time  0.068 ( 0.065)	Data  0.002 ( 0.002)	Loss 2.3212e-01 (3.0209e-01)	Acc@1  92.97 ( 89.67)	Acc@5 100.00 ( 99.78)
Epoch: [30][180/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.6297e-01 (3.0022e-01)	Acc@1  89.06 ( 89.72)	Acc@5  99.22 ( 99.77)
Epoch: [30][190/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.7860e-01 (2.9832e-01)	Acc@1  91.41 ( 89.77)	Acc@5  99.22 ( 99.75)
Epoch: [30][200/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.6637e-01 (2.9556e-01)	Acc@1  89.84 ( 89.83)	Acc@5  99.22 ( 99.76)
Epoch: [30][210/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.8894e-01 (2.9457e-01)	Acc@1  89.06 ( 89.89)	Acc@5  99.22 ( 99.76)
Epoch: [30][220/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.8819e-01 (2.9417e-01)	Acc@1  92.97 ( 89.90)	Acc@5 100.00 ( 99.76)
Epoch: [30][230/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.2488e-01 (2.9419e-01)	Acc@1  89.84 ( 89.89)	Acc@5  99.22 ( 99.76)
Epoch: [30][240/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.9123e-01 (2.9313e-01)	Acc@1  89.06 ( 89.91)	Acc@5  98.44 ( 99.76)
Epoch: [30][250/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0011e-01 (2.9192e-01)	Acc@1  92.97 ( 89.96)	Acc@5 100.00 ( 99.76)
Epoch: [30][260/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.3923e-01 (2.9084e-01)	Acc@1  89.06 ( 89.99)	Acc@5  99.22 ( 99.76)
Epoch: [30][270/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3674e-01 (2.8977e-01)	Acc@1  92.19 ( 90.05)	Acc@5 100.00 ( 99.76)
Epoch: [30][280/391]	Time  0.071 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.9560e-01 (2.8943e-01)	Acc@1  88.28 ( 90.04)	Acc@5 100.00 ( 99.77)
Epoch: [30][290/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.7043e-01 (2.8900e-01)	Acc@1  88.28 ( 90.07)	Acc@5 100.00 ( 99.77)
Epoch: [30][300/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.9896e-01 (2.8813e-01)	Acc@1  88.28 ( 90.11)	Acc@5  99.22 ( 99.77)
Epoch: [30][310/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3635e-01 (2.8774e-01)	Acc@1  90.62 ( 90.11)	Acc@5 100.00 ( 99.77)
Epoch: [30][320/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.6787e-01 (2.8762e-01)	Acc@1  89.06 ( 90.08)	Acc@5 100.00 ( 99.78)
Epoch: [30][330/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.5876e-01 (2.8652e-01)	Acc@1  90.62 ( 90.10)	Acc@5 100.00 ( 99.78)
Epoch: [30][340/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3431e-01 (2.8608e-01)	Acc@1  95.31 ( 90.11)	Acc@5 100.00 ( 99.79)
Epoch: [30][350/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.1168e-01 (2.8546e-01)	Acc@1  92.19 ( 90.14)	Acc@5  99.22 ( 99.79)
Epoch: [30][360/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6237e-01 (2.8571e-01)	Acc@1  96.88 ( 90.12)	Acc@5 100.00 ( 99.79)
Epoch: [30][370/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2502e-01 (2.8598e-01)	Acc@1  92.19 ( 90.13)	Acc@5 100.00 ( 99.79)
Epoch: [30][380/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2514e-01 (2.8506e-01)	Acc@1  92.97 ( 90.17)	Acc@5  99.22 ( 99.79)
Epoch: [30][390/391]	Time  0.046 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.0055e-01 (2.8399e-01)	Acc@1  90.00 ( 90.20)	Acc@5 100.00 ( 99.79)
## e[30] optimizer.zero_grad (sum) time: 0.4031391143798828
## e[30]       loss.backward (sum) time: 6.934203624725342
## e[30]      optimizer.step (sum) time: 3.4036242961883545
## epoch[30] training(only) time: 25.389548540115356
# Switched to evaluate mode...
Test: [  0/100]	Time  0.193 ( 0.193)	Loss 2.7670e-01 (2.7670e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.041)	Loss 3.4385e-01 (3.0023e-01)	Acc@1  88.00 ( 89.09)	Acc@5  99.00 ( 99.73)
Test: [ 20/100]	Time  0.024 ( 0.034)	Loss 3.7673e-01 (3.1879e-01)	Acc@1  88.00 ( 89.05)	Acc@5  99.00 ( 99.62)
Test: [ 30/100]	Time  0.028 ( 0.031)	Loss 3.1497e-01 (3.2995e-01)	Acc@1  88.00 ( 89.39)	Acc@5 100.00 ( 99.58)
Test: [ 40/100]	Time  0.024 ( 0.030)	Loss 2.5718e-01 (3.2985e-01)	Acc@1  93.00 ( 89.24)	Acc@5  99.00 ( 99.56)
Test: [ 50/100]	Time  0.030 ( 0.029)	Loss 2.3599e-01 (3.2991e-01)	Acc@1  92.00 ( 89.27)	Acc@5  99.00 ( 99.59)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 3.3875e-01 (3.3044e-01)	Acc@1  91.00 ( 89.21)	Acc@5 100.00 ( 99.61)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 4.1444e-01 (3.2713e-01)	Acc@1  85.00 ( 89.24)	Acc@5 100.00 ( 99.65)
Test: [ 80/100]	Time  0.024 ( 0.028)	Loss 2.5698e-01 (3.2628e-01)	Acc@1  92.00 ( 89.28)	Acc@5 100.00 ( 99.65)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 2.3592e-01 (3.2548e-01)	Acc@1  91.00 ( 89.24)	Acc@5 100.00 ( 99.67)
 * Acc@1 89.200 Acc@5 99.680
### epoch[30] execution time: 28.253494262695312
EPOCH 31
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [31][  0/391]	Time  0.295 ( 0.295)	Data  0.232 ( 0.232)	Loss 1.8572e-01 (1.8572e-01)	Acc@1  92.19 ( 92.19)	Acc@5 100.00 (100.00)
Epoch: [31][ 10/391]	Time  0.062 ( 0.083)	Data  0.001 ( 0.022)	Loss 1.8855e-01 (2.3319e-01)	Acc@1  92.19 ( 91.48)	Acc@5 100.00 ( 99.86)
Epoch: [31][ 20/391]	Time  0.066 ( 0.073)	Data  0.001 ( 0.012)	Loss 1.4425e-01 (2.3585e-01)	Acc@1  96.09 ( 91.44)	Acc@5 100.00 ( 99.93)
Epoch: [31][ 30/391]	Time  0.065 ( 0.070)	Data  0.001 ( 0.008)	Loss 1.9665e-01 (2.3357e-01)	Acc@1  92.97 ( 91.31)	Acc@5 100.00 ( 99.87)
Epoch: [31][ 40/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.007)	Loss 2.4398e-01 (2.4486e-01)	Acc@1  90.62 ( 90.97)	Acc@5  99.22 ( 99.85)
Epoch: [31][ 50/391]	Time  0.061 ( 0.068)	Data  0.001 ( 0.006)	Loss 1.9393e-01 (2.3745e-01)	Acc@1  94.53 ( 91.31)	Acc@5 100.00 ( 99.86)
Epoch: [31][ 60/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.005)	Loss 3.2270e-01 (2.4351e-01)	Acc@1  87.50 ( 91.29)	Acc@5 100.00 ( 99.86)
Epoch: [31][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 2.3948e-01 (2.4300e-01)	Acc@1  92.19 ( 91.27)	Acc@5 100.00 ( 99.85)
Epoch: [31][ 80/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.004)	Loss 2.2665e-01 (2.4039e-01)	Acc@1  92.97 ( 91.39)	Acc@5 100.00 ( 99.86)
Epoch: [31][ 90/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.004)	Loss 1.8926e-01 (2.4164e-01)	Acc@1  93.75 ( 91.41)	Acc@5 100.00 ( 99.86)
Epoch: [31][100/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.1269e-01 (2.4237e-01)	Acc@1  92.19 ( 91.46)	Acc@5 100.00 ( 99.85)
Epoch: [31][110/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.7124e-01 (2.4266e-01)	Acc@1  94.53 ( 91.50)	Acc@5 100.00 ( 99.86)
Epoch: [31][120/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.3290e-01 (2.4177e-01)	Acc@1  89.06 ( 91.55)	Acc@5 100.00 ( 99.84)
Epoch: [31][130/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 2.0767e-01 (2.4214e-01)	Acc@1  92.97 ( 91.52)	Acc@5 100.00 ( 99.84)
Epoch: [31][140/391]	Time  0.072 ( 0.065)	Data  0.001 ( 0.003)	Loss 3.0907e-01 (2.4614e-01)	Acc@1  91.41 ( 91.42)	Acc@5  98.44 ( 99.81)
Epoch: [31][150/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.003)	Loss 3.2357e-01 (2.4826e-01)	Acc@1  88.28 ( 91.34)	Acc@5 100.00 ( 99.80)
Epoch: [31][160/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.003)	Loss 2.0155e-01 (2.4661e-01)	Acc@1  92.19 ( 91.42)	Acc@5 100.00 ( 99.81)
Epoch: [31][170/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1994e-01 (2.4539e-01)	Acc@1  91.41 ( 91.42)	Acc@5 100.00 ( 99.81)
Epoch: [31][180/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6651e-01 (2.4420e-01)	Acc@1  94.53 ( 91.46)	Acc@5  99.22 ( 99.80)
Epoch: [31][190/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.4391e-01 (2.4578e-01)	Acc@1  92.19 ( 91.41)	Acc@5 100.00 ( 99.80)
Epoch: [31][200/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2977e-01 (2.4784e-01)	Acc@1  92.97 ( 91.38)	Acc@5 100.00 ( 99.81)
Epoch: [31][210/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1520e-01 (2.4731e-01)	Acc@1  92.19 ( 91.38)	Acc@5 100.00 ( 99.81)
Epoch: [31][220/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2392e-01 (2.4805e-01)	Acc@1  92.19 ( 91.38)	Acc@5 100.00 ( 99.81)
Epoch: [31][230/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.8520e-01 (2.4873e-01)	Acc@1  89.06 ( 91.37)	Acc@5 100.00 ( 99.81)
Epoch: [31][240/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.7481e-01 (2.4779e-01)	Acc@1  89.84 ( 91.40)	Acc@5 100.00 ( 99.81)
Epoch: [31][250/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0647e-01 (2.4688e-01)	Acc@1  92.19 ( 91.42)	Acc@5  99.22 ( 99.81)
Epoch: [31][260/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8732e-01 (2.4728e-01)	Acc@1  93.75 ( 91.42)	Acc@5 100.00 ( 99.81)
Epoch: [31][270/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.4938e-01 (2.4691e-01)	Acc@1  92.97 ( 91.46)	Acc@5 100.00 ( 99.82)
Epoch: [31][280/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9545e-01 (2.4802e-01)	Acc@1  92.97 ( 91.44)	Acc@5 100.00 ( 99.82)
Epoch: [31][290/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.3794e-01 (2.4766e-01)	Acc@1  89.06 ( 91.44)	Acc@5  99.22 ( 99.81)
Epoch: [31][300/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0723e-01 (2.4782e-01)	Acc@1  92.97 ( 91.44)	Acc@5 100.00 ( 99.81)
Epoch: [31][310/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1383e-01 (2.4720e-01)	Acc@1  94.53 ( 91.48)	Acc@5  99.22 ( 99.81)
Epoch: [31][320/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3295e-01 (2.4686e-01)	Acc@1  91.41 ( 91.49)	Acc@5 100.00 ( 99.81)
Epoch: [31][330/391]	Time  0.063 ( 0.065)	Data  0.002 ( 0.002)	Loss 3.0860e-01 (2.4676e-01)	Acc@1  92.19 ( 91.50)	Acc@5  99.22 ( 99.81)
Epoch: [31][340/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2627e-01 (2.4648e-01)	Acc@1  92.19 ( 91.51)	Acc@5 100.00 ( 99.81)
Epoch: [31][350/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1506e-01 (2.4602e-01)	Acc@1  94.53 ( 91.55)	Acc@5 100.00 ( 99.81)
Epoch: [31][360/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2751e-01 (2.4577e-01)	Acc@1  95.31 ( 91.56)	Acc@5  99.22 ( 99.81)
Epoch: [31][370/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.4474e-01 (2.4564e-01)	Acc@1  89.84 ( 91.55)	Acc@5  99.22 ( 99.80)
Epoch: [31][380/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.6253e-01 (2.4615e-01)	Acc@1  84.38 ( 91.53)	Acc@5 100.00 ( 99.81)
Epoch: [31][390/391]	Time  0.046 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.7716e-01 (2.4578e-01)	Acc@1  90.00 ( 91.55)	Acc@5 100.00 ( 99.80)
## e[31] optimizer.zero_grad (sum) time: 0.4000051021575928
## e[31]       loss.backward (sum) time: 6.937852382659912
## e[31]      optimizer.step (sum) time: 3.3597331047058105
## epoch[31] training(only) time: 25.336469888687134
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 2.8154e-01 (2.8154e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.041)	Loss 3.1196e-01 (2.8247e-01)	Acc@1  92.00 ( 89.45)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.028 ( 0.034)	Loss 3.3746e-01 (3.0336e-01)	Acc@1  89.00 ( 89.29)	Acc@5  99.00 ( 99.67)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 2.5886e-01 (3.1557e-01)	Acc@1  88.00 ( 89.23)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 2.6016e-01 (3.1584e-01)	Acc@1  93.00 ( 89.24)	Acc@5  99.00 ( 99.61)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 2.3671e-01 (3.1628e-01)	Acc@1  91.00 ( 89.47)	Acc@5  99.00 ( 99.61)
Test: [ 60/100]	Time  0.027 ( 0.029)	Loss 3.3331e-01 (3.1756e-01)	Acc@1  90.00 ( 89.48)	Acc@5 100.00 ( 99.64)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 4.3423e-01 (3.1340e-01)	Acc@1  85.00 ( 89.56)	Acc@5 100.00 ( 99.68)
Test: [ 80/100]	Time  0.024 ( 0.028)	Loss 2.6693e-01 (3.1285e-01)	Acc@1  92.00 ( 89.64)	Acc@5 100.00 ( 99.67)
Test: [ 90/100]	Time  0.027 ( 0.028)	Loss 2.2986e-01 (3.1113e-01)	Acc@1  92.00 ( 89.62)	Acc@5 100.00 ( 99.70)
 * Acc@1 89.700 Acc@5 99.710
### epoch[31] execution time: 28.263983726501465
EPOCH 32
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [32][  0/391]	Time  0.249 ( 0.249)	Data  0.185 ( 0.185)	Loss 2.7281e-01 (2.7281e-01)	Acc@1  89.06 ( 89.06)	Acc@5 100.00 (100.00)
Epoch: [32][ 10/391]	Time  0.062 ( 0.083)	Data  0.001 ( 0.018)	Loss 2.3718e-01 (2.6330e-01)	Acc@1  90.62 ( 90.13)	Acc@5 100.00 ( 99.93)
Epoch: [32][ 20/391]	Time  0.064 ( 0.073)	Data  0.001 ( 0.010)	Loss 1.9755e-01 (2.4823e-01)	Acc@1  90.62 ( 91.11)	Acc@5 100.00 ( 99.89)
Epoch: [32][ 30/391]	Time  0.065 ( 0.071)	Data  0.001 ( 0.007)	Loss 2.0980e-01 (2.4179e-01)	Acc@1  92.19 ( 91.43)	Acc@5 100.00 ( 99.85)
Epoch: [32][ 40/391]	Time  0.062 ( 0.069)	Data  0.001 ( 0.006)	Loss 2.2195e-01 (2.4119e-01)	Acc@1  92.19 ( 91.31)	Acc@5 100.00 ( 99.89)
Epoch: [32][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.005)	Loss 2.6856e-01 (2.4001e-01)	Acc@1  91.41 ( 91.39)	Acc@5 100.00 ( 99.83)
Epoch: [32][ 60/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.6653e-01 (2.3726e-01)	Acc@1  93.75 ( 91.50)	Acc@5 100.00 ( 99.86)
Epoch: [32][ 70/391]	Time  0.074 ( 0.067)	Data  0.001 ( 0.004)	Loss 2.0633e-01 (2.3614e-01)	Acc@1  92.19 ( 91.62)	Acc@5 100.00 ( 99.80)
Epoch: [32][ 80/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.7064e-01 (2.3341e-01)	Acc@1  95.31 ( 91.79)	Acc@5 100.00 ( 99.80)
Epoch: [32][ 90/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.4452e-01 (2.3285e-01)	Acc@1  91.41 ( 91.78)	Acc@5 100.00 ( 99.82)
Epoch: [32][100/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.7544e-01 (2.3220e-01)	Acc@1  93.75 ( 91.77)	Acc@5 100.00 ( 99.83)
Epoch: [32][110/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.2594e-01 (2.3333e-01)	Acc@1  92.97 ( 91.75)	Acc@5 100.00 ( 99.84)
Epoch: [32][120/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.1245e-01 (2.3254e-01)	Acc@1  92.19 ( 91.80)	Acc@5 100.00 ( 99.85)
Epoch: [32][130/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.0979e-01 (2.3201e-01)	Acc@1  87.50 ( 91.79)	Acc@5 100.00 ( 99.86)
Epoch: [32][140/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.4213e-01 (2.3035e-01)	Acc@1  92.97 ( 91.88)	Acc@5  99.22 ( 99.86)
Epoch: [32][150/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.5078e-01 (2.2950e-01)	Acc@1  90.62 ( 91.91)	Acc@5 100.00 ( 99.87)
Epoch: [32][160/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0591e-01 (2.3043e-01)	Acc@1  92.19 ( 91.92)	Acc@5 100.00 ( 99.85)
Epoch: [32][170/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.5056e-01 (2.3149e-01)	Acc@1  90.62 ( 91.91)	Acc@5  99.22 ( 99.83)
Epoch: [32][180/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3383e-01 (2.3189e-01)	Acc@1  92.97 ( 91.91)	Acc@5 100.00 ( 99.82)
Epoch: [32][190/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1436e-01 (2.3379e-01)	Acc@1  92.19 ( 91.85)	Acc@5  99.22 ( 99.82)
Epoch: [32][200/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6415e-01 (2.3432e-01)	Acc@1  94.53 ( 91.83)	Acc@5  99.22 ( 99.81)
Epoch: [32][210/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.4671e-01 (2.3442e-01)	Acc@1  92.19 ( 91.83)	Acc@5 100.00 ( 99.81)
Epoch: [32][220/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.6303e-01 (2.3336e-01)	Acc@1  89.84 ( 91.87)	Acc@5  99.22 ( 99.82)
Epoch: [32][230/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.4342e-01 (2.3338e-01)	Acc@1  93.75 ( 91.86)	Acc@5 100.00 ( 99.82)
Epoch: [32][240/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5732e-01 (2.3177e-01)	Acc@1  94.53 ( 91.93)	Acc@5 100.00 ( 99.82)
Epoch: [32][250/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2495e-01 (2.3128e-01)	Acc@1  90.62 ( 91.92)	Acc@5 100.00 ( 99.82)
Epoch: [32][260/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.5573e-01 (2.3241e-01)	Acc@1  88.28 ( 91.90)	Acc@5 100.00 ( 99.82)
Epoch: [32][270/391]	Time  0.070 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.8184e-01 (2.3303e-01)	Acc@1  89.84 ( 91.88)	Acc@5  98.44 ( 99.82)
Epoch: [32][280/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.5303e-01 (2.3245e-01)	Acc@1  90.62 ( 91.90)	Acc@5 100.00 ( 99.82)
Epoch: [32][290/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.7942e-01 (2.3192e-01)	Acc@1  89.84 ( 91.91)	Acc@5 100.00 ( 99.82)
Epoch: [32][300/391]	Time  0.067 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.7079e-01 (2.3166e-01)	Acc@1  90.62 ( 91.93)	Acc@5 100.00 ( 99.82)
Epoch: [32][310/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.4431e-01 (2.3194e-01)	Acc@1  95.31 ( 91.92)	Acc@5 100.00 ( 99.82)
Epoch: [32][320/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.7675e-01 (2.3152e-01)	Acc@1  93.75 ( 91.95)	Acc@5  97.66 ( 99.82)
Epoch: [32][330/391]	Time  0.070 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.0730e-01 (2.3170e-01)	Acc@1  92.97 ( 91.94)	Acc@5 100.00 ( 99.81)
Epoch: [32][340/391]	Time  0.066 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.8416e-01 (2.3115e-01)	Acc@1  94.53 ( 91.96)	Acc@5 100.00 ( 99.82)
Epoch: [32][350/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.9676e-01 (2.3077e-01)	Acc@1  93.75 ( 92.00)	Acc@5 100.00 ( 99.82)
Epoch: [32][360/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.3488e-01 (2.3030e-01)	Acc@1  89.84 ( 92.00)	Acc@5 100.00 ( 99.82)
Epoch: [32][370/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.8732e-01 (2.3045e-01)	Acc@1  93.75 ( 91.97)	Acc@5  99.22 ( 99.81)
Epoch: [32][380/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.9276e-01 (2.3011e-01)	Acc@1  92.97 ( 92.00)	Acc@5 100.00 ( 99.82)
Epoch: [32][390/391]	Time  0.048 ( 0.064)	Data  0.002 ( 0.002)	Loss 2.2095e-01 (2.3034e-01)	Acc@1  91.25 ( 91.98)	Acc@5 100.00 ( 99.82)
## e[32] optimizer.zero_grad (sum) time: 0.4059889316558838
## e[32]       loss.backward (sum) time: 6.922392129898071
## e[32]      optimizer.step (sum) time: 3.343681573867798
## epoch[32] training(only) time: 25.203553199768066
# Switched to evaluate mode...
Test: [  0/100]	Time  0.208 ( 0.208)	Loss 2.9404e-01 (2.9404e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.028 ( 0.043)	Loss 3.4006e-01 (2.9056e-01)	Acc@1  87.00 ( 90.00)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.027 ( 0.035)	Loss 3.1608e-01 (3.0663e-01)	Acc@1  89.00 ( 89.76)	Acc@5  99.00 ( 99.71)
Test: [ 30/100]	Time  0.024 ( 0.032)	Loss 2.7145e-01 (3.1521e-01)	Acc@1  89.00 ( 89.74)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.024 ( 0.030)	Loss 2.5472e-01 (3.1428e-01)	Acc@1  93.00 ( 89.63)	Acc@5  99.00 ( 99.68)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 2.2906e-01 (3.1169e-01)	Acc@1  91.00 ( 89.73)	Acc@5  99.00 ( 99.69)
Test: [ 60/100]	Time  0.024 ( 0.029)	Loss 3.3954e-01 (3.1230e-01)	Acc@1  90.00 ( 89.77)	Acc@5 100.00 ( 99.72)
Test: [ 70/100]	Time  0.028 ( 0.028)	Loss 4.2246e-01 (3.0905e-01)	Acc@1  87.00 ( 89.77)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.024 ( 0.028)	Loss 2.6617e-01 (3.0895e-01)	Acc@1  90.00 ( 89.75)	Acc@5 100.00 ( 99.73)
Test: [ 90/100]	Time  0.024 ( 0.028)	Loss 2.4297e-01 (3.0803e-01)	Acc@1  91.00 ( 89.77)	Acc@5 100.00 ( 99.75)
 * Acc@1 89.740 Acc@5 99.760
### epoch[32] execution time: 28.109673738479614
EPOCH 33
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [33][  0/391]	Time  0.245 ( 0.245)	Data  0.180 ( 0.180)	Loss 1.9698e-01 (1.9698e-01)	Acc@1  92.97 ( 92.97)	Acc@5 100.00 (100.00)
Epoch: [33][ 10/391]	Time  0.068 ( 0.080)	Data  0.001 ( 0.017)	Loss 2.1714e-01 (1.9749e-01)	Acc@1  92.19 ( 93.47)	Acc@5 100.00 (100.00)
Epoch: [33][ 20/391]	Time  0.064 ( 0.073)	Data  0.001 ( 0.010)	Loss 2.2832e-01 (2.0547e-01)	Acc@1  93.75 ( 93.19)	Acc@5  99.22 ( 99.89)
Epoch: [33][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.007)	Loss 1.7372e-01 (2.2369e-01)	Acc@1  92.97 ( 92.34)	Acc@5 100.00 ( 99.87)
Epoch: [33][ 40/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.006)	Loss 1.8697e-01 (2.1279e-01)	Acc@1  94.53 ( 92.89)	Acc@5 100.00 ( 99.90)
Epoch: [33][ 50/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.005)	Loss 2.1269e-01 (2.1536e-01)	Acc@1  92.19 ( 92.68)	Acc@5 100.00 ( 99.91)
Epoch: [33][ 60/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.9490e-01 (2.1922e-01)	Acc@1  95.31 ( 92.55)	Acc@5 100.00 ( 99.87)
Epoch: [33][ 70/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.004)	Loss 2.4802e-01 (2.1983e-01)	Acc@1  89.84 ( 92.42)	Acc@5 100.00 ( 99.85)
Epoch: [33][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.7834e-01 (2.1778e-01)	Acc@1  93.75 ( 92.47)	Acc@5 100.00 ( 99.86)
Epoch: [33][ 90/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.7142e-01 (2.1867e-01)	Acc@1  95.31 ( 92.46)	Acc@5 100.00 ( 99.88)
Epoch: [33][100/391]	Time  0.060 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.6338e-01 (2.1896e-01)	Acc@1  87.50 ( 92.51)	Acc@5 100.00 ( 99.89)
Epoch: [33][110/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.5599e-01 (2.1623e-01)	Acc@1  93.75 ( 92.64)	Acc@5 100.00 ( 99.89)
Epoch: [33][120/391]	Time  0.059 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.3185e-01 (2.2065e-01)	Acc@1  92.19 ( 92.50)	Acc@5 100.00 ( 99.89)
Epoch: [33][130/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.4834e-01 (2.2302e-01)	Acc@1  85.16 ( 92.44)	Acc@5 100.00 ( 99.86)
Epoch: [33][140/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9073e-01 (2.2152e-01)	Acc@1  91.41 ( 92.46)	Acc@5 100.00 ( 99.87)
Epoch: [33][150/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9648e-01 (2.2037e-01)	Acc@1  93.75 ( 92.46)	Acc@5  99.22 ( 99.87)
Epoch: [33][160/391]	Time  0.071 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.4752e-01 (2.1989e-01)	Acc@1  90.62 ( 92.46)	Acc@5 100.00 ( 99.86)
Epoch: [33][170/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5322e-01 (2.1872e-01)	Acc@1  94.53 ( 92.49)	Acc@5 100.00 ( 99.86)
Epoch: [33][180/391]	Time  0.070 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.4759e-01 (2.2027e-01)	Acc@1  91.41 ( 92.41)	Acc@5 100.00 ( 99.85)
Epoch: [33][190/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2757e-01 (2.1845e-01)	Acc@1  94.53 ( 92.45)	Acc@5 100.00 ( 99.85)
Epoch: [33][200/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2111e-01 (2.1947e-01)	Acc@1  90.62 ( 92.40)	Acc@5 100.00 ( 99.86)
Epoch: [33][210/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7254e-01 (2.1876e-01)	Acc@1  93.75 ( 92.44)	Acc@5 100.00 ( 99.85)
Epoch: [33][220/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.5297e-01 (2.1847e-01)	Acc@1  93.75 ( 92.46)	Acc@5 100.00 ( 99.86)
Epoch: [33][230/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6517e-01 (2.1760e-01)	Acc@1  94.53 ( 92.48)	Acc@5 100.00 ( 99.86)
Epoch: [33][240/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.2396e-01 (2.1859e-01)	Acc@1  84.38 ( 92.44)	Acc@5 100.00 ( 99.85)
Epoch: [33][250/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9407e-01 (2.1877e-01)	Acc@1  93.75 ( 92.42)	Acc@5 100.00 ( 99.85)
Epoch: [33][260/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2664e-01 (2.1905e-01)	Acc@1  92.97 ( 92.42)	Acc@5  99.22 ( 99.85)
Epoch: [33][270/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7131e-01 (2.1929e-01)	Acc@1  94.53 ( 92.41)	Acc@5 100.00 ( 99.84)
Epoch: [33][280/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3123e-01 (2.1950e-01)	Acc@1  90.62 ( 92.38)	Acc@5 100.00 ( 99.85)
Epoch: [33][290/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.5565e-01 (2.2196e-01)	Acc@1  87.50 ( 92.32)	Acc@5 100.00 ( 99.85)
Epoch: [33][300/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1210e-01 (2.2186e-01)	Acc@1  92.19 ( 92.32)	Acc@5 100.00 ( 99.86)
Epoch: [33][310/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3276e-01 (2.2175e-01)	Acc@1  92.97 ( 92.35)	Acc@5 100.00 ( 99.86)
Epoch: [33][320/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3067e-01 (2.2257e-01)	Acc@1  92.19 ( 92.33)	Acc@5 100.00 ( 99.86)
Epoch: [33][330/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.6947e-01 (2.2350e-01)	Acc@1  92.19 ( 92.29)	Acc@5 100.00 ( 99.85)
Epoch: [33][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6452e-01 (2.2351e-01)	Acc@1  94.53 ( 92.29)	Acc@5 100.00 ( 99.85)
Epoch: [33][350/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.5473e-01 (2.2403e-01)	Acc@1  91.41 ( 92.28)	Acc@5  98.44 ( 99.84)
Epoch: [33][360/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1033e-01 (2.2342e-01)	Acc@1  92.97 ( 92.31)	Acc@5 100.00 ( 99.85)
Epoch: [33][370/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3479e-01 (2.2414e-01)	Acc@1  92.19 ( 92.28)	Acc@5  99.22 ( 99.84)
Epoch: [33][380/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.3221e-01 (2.2337e-01)	Acc@1  89.84 ( 92.31)	Acc@5  98.44 ( 99.84)
Epoch: [33][390/391]	Time  0.047 ( 0.064)	Data  0.002 ( 0.002)	Loss 3.3358e-01 (2.2349e-01)	Acc@1  90.00 ( 92.32)	Acc@5 100.00 ( 99.84)
## e[33] optimizer.zero_grad (sum) time: 0.4016537666320801
## e[33]       loss.backward (sum) time: 6.960450887680054
## e[33]      optimizer.step (sum) time: 3.37111496925354
## epoch[33] training(only) time: 25.294886350631714
# Switched to evaluate mode...
Test: [  0/100]	Time  0.192 ( 0.192)	Loss 2.4655e-01 (2.4655e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.042)	Loss 3.6558e-01 (2.8482e-01)	Acc@1  88.00 ( 89.91)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.027 ( 0.035)	Loss 3.1442e-01 (3.0313e-01)	Acc@1  89.00 ( 90.05)	Acc@5  99.00 ( 99.57)
Test: [ 30/100]	Time  0.024 ( 0.032)	Loss 2.7652e-01 (3.1633e-01)	Acc@1  90.00 ( 89.90)	Acc@5 100.00 ( 99.61)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 2.8028e-01 (3.1560e-01)	Acc@1  93.00 ( 89.90)	Acc@5  99.00 ( 99.61)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 2.0648e-01 (3.1517e-01)	Acc@1  92.00 ( 89.94)	Acc@5  99.00 ( 99.59)
Test: [ 60/100]	Time  0.027 ( 0.029)	Loss 3.3278e-01 (3.1379e-01)	Acc@1  90.00 ( 90.08)	Acc@5 100.00 ( 99.64)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 4.3777e-01 (3.1004e-01)	Acc@1  86.00 ( 90.17)	Acc@5 100.00 ( 99.66)
Test: [ 80/100]	Time  0.024 ( 0.028)	Loss 2.4311e-01 (3.1034e-01)	Acc@1  92.00 ( 90.02)	Acc@5 100.00 ( 99.67)
Test: [ 90/100]	Time  0.029 ( 0.028)	Loss 2.6673e-01 (3.0863e-01)	Acc@1  94.00 ( 90.01)	Acc@5 100.00 ( 99.69)
 * Acc@1 89.960 Acc@5 99.700
### epoch[33] execution time: 28.189330101013184
EPOCH 34
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [34][  0/391]	Time  0.262 ( 0.262)	Data  0.190 ( 0.190)	Loss 1.7483e-01 (1.7483e-01)	Acc@1  92.19 ( 92.19)	Acc@5 100.00 (100.00)
Epoch: [34][ 10/391]	Time  0.062 ( 0.081)	Data  0.001 ( 0.018)	Loss 2.4383e-01 (2.0272e-01)	Acc@1  92.19 ( 93.25)	Acc@5 100.00 (100.00)
Epoch: [34][ 20/391]	Time  0.064 ( 0.072)	Data  0.001 ( 0.010)	Loss 2.3290e-01 (2.1442e-01)	Acc@1  92.19 ( 92.78)	Acc@5  98.44 ( 99.85)
Epoch: [34][ 30/391]	Time  0.059 ( 0.069)	Data  0.001 ( 0.007)	Loss 1.6462e-01 (2.1468e-01)	Acc@1  93.75 ( 92.84)	Acc@5 100.00 ( 99.80)
Epoch: [34][ 40/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.006)	Loss 1.6977e-01 (2.1556e-01)	Acc@1  96.09 ( 92.97)	Acc@5 100.00 ( 99.79)
Epoch: [34][ 50/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.005)	Loss 1.3637e-01 (2.1246e-01)	Acc@1  94.53 ( 92.97)	Acc@5 100.00 ( 99.80)
Epoch: [34][ 60/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.004)	Loss 2.2086e-01 (2.1100e-01)	Acc@1  90.62 ( 92.89)	Acc@5  99.22 ( 99.81)
Epoch: [34][ 70/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.004)	Loss 2.4717e-01 (2.1344e-01)	Acc@1  89.84 ( 92.76)	Acc@5 100.00 ( 99.82)
Epoch: [34][ 80/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.5367e-01 (2.1708e-01)	Acc@1  95.31 ( 92.67)	Acc@5 100.00 ( 99.83)
Epoch: [34][ 90/391]	Time  0.060 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.1839e-01 (2.1618e-01)	Acc@1  92.97 ( 92.75)	Acc@5  98.44 ( 99.81)
Epoch: [34][100/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.5798e-01 (2.1680e-01)	Acc@1  92.97 ( 92.76)	Acc@5  99.22 ( 99.82)
Epoch: [34][110/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.8105e-01 (2.1482e-01)	Acc@1  91.41 ( 92.79)	Acc@5 100.00 ( 99.83)
Epoch: [34][120/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.4453e-01 (2.1571e-01)	Acc@1  95.31 ( 92.76)	Acc@5 100.00 ( 99.83)
Epoch: [34][130/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 2.3307e-01 (2.1462e-01)	Acc@1  93.75 ( 92.80)	Acc@5 100.00 ( 99.84)
Epoch: [34][140/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8991e-01 (2.1487e-01)	Acc@1  93.75 ( 92.79)	Acc@5 100.00 ( 99.84)
Epoch: [34][150/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.9914e-01 (2.1442e-01)	Acc@1  89.84 ( 92.80)	Acc@5 100.00 ( 99.84)
Epoch: [34][160/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.8091e-02 (2.1175e-01)	Acc@1  96.09 ( 92.82)	Acc@5 100.00 ( 99.85)
Epoch: [34][170/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8451e-01 (2.1310e-01)	Acc@1  91.41 ( 92.75)	Acc@5 100.00 ( 99.84)
Epoch: [34][180/391]	Time  0.058 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3935e-01 (2.1382e-01)	Acc@1  92.19 ( 92.74)	Acc@5 100.00 ( 99.84)
Epoch: [34][190/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.2803e-01 (2.1392e-01)	Acc@1  91.41 ( 92.74)	Acc@5 100.00 ( 99.84)
Epoch: [34][200/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.7450e-01 (2.1422e-01)	Acc@1  89.84 ( 92.71)	Acc@5 100.00 ( 99.84)
Epoch: [34][210/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.8725e-01 (2.1393e-01)	Acc@1  91.41 ( 92.68)	Acc@5 100.00 ( 99.85)
Epoch: [34][220/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.5717e-01 (2.1398e-01)	Acc@1  89.06 ( 92.68)	Acc@5 100.00 ( 99.85)
Epoch: [34][230/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2328e-01 (2.1447e-01)	Acc@1  92.97 ( 92.67)	Acc@5 100.00 ( 99.86)
Epoch: [34][240/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3898e-01 (2.1403e-01)	Acc@1  95.31 ( 92.68)	Acc@5 100.00 ( 99.86)
Epoch: [34][250/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9041e-01 (2.1501e-01)	Acc@1  94.53 ( 92.64)	Acc@5  99.22 ( 99.86)
Epoch: [34][260/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0564e-01 (2.1380e-01)	Acc@1  93.75 ( 92.68)	Acc@5 100.00 ( 99.86)
Epoch: [34][270/391]	Time  0.069 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.2497e-01 (2.1365e-01)	Acc@1  90.62 ( 92.66)	Acc@5 100.00 ( 99.86)
Epoch: [34][280/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.7986e-01 (2.1343e-01)	Acc@1  88.28 ( 92.65)	Acc@5 100.00 ( 99.87)
Epoch: [34][290/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.5621e-01 (2.1378e-01)	Acc@1  89.84 ( 92.63)	Acc@5 100.00 ( 99.87)
Epoch: [34][300/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.5756e-01 (2.1311e-01)	Acc@1  94.53 ( 92.64)	Acc@5 100.00 ( 99.87)
Epoch: [34][310/391]	Time  0.066 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.7917e-01 (2.1361e-01)	Acc@1  91.41 ( 92.63)	Acc@5  99.22 ( 99.87)
Epoch: [34][320/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.6078e-01 (2.1283e-01)	Acc@1  94.53 ( 92.66)	Acc@5 100.00 ( 99.87)
Epoch: [34][330/391]	Time  0.066 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.4191e-01 (2.1296e-01)	Acc@1  93.75 ( 92.65)	Acc@5 100.00 ( 99.87)
Epoch: [34][340/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.0435e-01 (2.1241e-01)	Acc@1  94.53 ( 92.68)	Acc@5 100.00 ( 99.87)
Epoch: [34][350/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.9675e-01 (2.1340e-01)	Acc@1  92.19 ( 92.65)	Acc@5 100.00 ( 99.87)
Epoch: [34][360/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.0300e-01 (2.1386e-01)	Acc@1  89.84 ( 92.63)	Acc@5 100.00 ( 99.86)
Epoch: [34][370/391]	Time  0.059 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.1324e-01 (2.1528e-01)	Acc@1  91.41 ( 92.59)	Acc@5 100.00 ( 99.86)
Epoch: [34][380/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.4860e-01 (2.1496e-01)	Acc@1  90.62 ( 92.59)	Acc@5 100.00 ( 99.86)
Epoch: [34][390/391]	Time  0.046 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.9356e-01 (2.1425e-01)	Acc@1  88.75 ( 92.63)	Acc@5 100.00 ( 99.86)
## e[34] optimizer.zero_grad (sum) time: 0.4038121700286865
## e[34]       loss.backward (sum) time: 6.930662393569946
## e[34]      optimizer.step (sum) time: 3.336873769760132
## epoch[34] training(only) time: 25.27544903755188
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 2.7358e-01 (2.7358e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.040)	Loss 3.3014e-01 (2.7441e-01)	Acc@1  90.00 ( 90.64)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 3.2516e-01 (2.9360e-01)	Acc@1  91.00 ( 90.67)	Acc@5  99.00 ( 99.71)
Test: [ 30/100]	Time  0.026 ( 0.031)	Loss 2.4375e-01 (3.0282e-01)	Acc@1  91.00 ( 90.42)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 2.8341e-01 (3.0455e-01)	Acc@1  91.00 ( 90.27)	Acc@5  99.00 ( 99.66)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 2.3013e-01 (3.0411e-01)	Acc@1  91.00 ( 90.35)	Acc@5  99.00 ( 99.69)
Test: [ 60/100]	Time  0.027 ( 0.028)	Loss 3.0684e-01 (3.0299e-01)	Acc@1  90.00 ( 90.31)	Acc@5 100.00 ( 99.70)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 3.9278e-01 (2.9861e-01)	Acc@1  87.00 ( 90.32)	Acc@5 100.00 ( 99.72)
Test: [ 80/100]	Time  0.024 ( 0.028)	Loss 2.5329e-01 (2.9850e-01)	Acc@1  93.00 ( 90.26)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.027 ( 0.028)	Loss 2.6005e-01 (2.9612e-01)	Acc@1  89.00 ( 90.20)	Acc@5 100.00 ( 99.77)
 * Acc@1 90.190 Acc@5 99.770
### epoch[34] execution time: 28.122920036315918
EPOCH 35
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [35][  0/391]	Time  0.244 ( 0.244)	Data  0.179 ( 0.179)	Loss 1.7630e-01 (1.7630e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
Epoch: [35][ 10/391]	Time  0.064 ( 0.080)	Data  0.001 ( 0.017)	Loss 1.7525e-01 (2.0395e-01)	Acc@1  96.88 ( 93.04)	Acc@5 100.00 ( 99.86)
Epoch: [35][ 20/391]	Time  0.062 ( 0.072)	Data  0.001 ( 0.010)	Loss 1.0584e-01 (1.9677e-01)	Acc@1  98.44 ( 93.27)	Acc@5 100.00 ( 99.81)
Epoch: [35][ 30/391]	Time  0.060 ( 0.069)	Data  0.001 ( 0.007)	Loss 1.4096e-01 (1.9932e-01)	Acc@1  96.09 ( 92.89)	Acc@5  99.22 ( 99.85)
Epoch: [35][ 40/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.005)	Loss 2.5982e-01 (1.9931e-01)	Acc@1  89.84 ( 92.91)	Acc@5 100.00 ( 99.87)
Epoch: [35][ 50/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.005)	Loss 2.4564e-01 (1.9833e-01)	Acc@1  92.97 ( 92.95)	Acc@5 100.00 ( 99.88)
Epoch: [35][ 60/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.004)	Loss 2.5121e-01 (2.0055e-01)	Acc@1  90.62 ( 92.96)	Acc@5 100.00 ( 99.88)
Epoch: [35][ 70/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.004)	Loss 1.7224e-01 (1.9943e-01)	Acc@1  92.97 ( 93.05)	Acc@5 100.00 ( 99.89)
Epoch: [35][ 80/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.6280e-01 (1.9825e-01)	Acc@1  92.97 ( 93.10)	Acc@5 100.00 ( 99.88)
Epoch: [35][ 90/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.8195e-01 (2.0271e-01)	Acc@1  92.97 ( 92.97)	Acc@5 100.00 ( 99.85)
Epoch: [35][100/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.6182e-01 (2.0339e-01)	Acc@1  94.53 ( 92.91)	Acc@5 100.00 ( 99.85)
Epoch: [35][110/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 2.7139e-01 (2.0254e-01)	Acc@1  90.62 ( 92.96)	Acc@5 100.00 ( 99.87)
Epoch: [35][120/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 2.2887e-01 (2.0299e-01)	Acc@1  91.41 ( 92.94)	Acc@5 100.00 ( 99.86)
Epoch: [35][130/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.5697e-01 (2.0428e-01)	Acc@1  89.84 ( 92.89)	Acc@5 100.00 ( 99.87)
Epoch: [35][140/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2034e-01 (2.0679e-01)	Acc@1  90.62 ( 92.80)	Acc@5 100.00 ( 99.87)
Epoch: [35][150/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3304e-01 (2.0610e-01)	Acc@1  93.75 ( 92.85)	Acc@5 100.00 ( 99.88)
Epoch: [35][160/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2024e-01 (2.0579e-01)	Acc@1  93.75 ( 92.85)	Acc@5 100.00 ( 99.88)
Epoch: [35][170/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0631e-01 (2.0741e-01)	Acc@1  91.41 ( 92.74)	Acc@5  99.22 ( 99.87)
Epoch: [35][180/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.5701e-01 (2.0928e-01)	Acc@1  90.62 ( 92.68)	Acc@5  99.22 ( 99.86)
Epoch: [35][190/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.6972e-01 (2.0948e-01)	Acc@1  93.75 ( 92.67)	Acc@5 100.00 ( 99.86)
Epoch: [35][200/391]	Time  0.059 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.6090e-01 (2.0954e-01)	Acc@1  92.97 ( 92.66)	Acc@5 100.00 ( 99.86)
Epoch: [35][210/391]	Time  0.066 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.6521e-01 (2.0738e-01)	Acc@1  94.53 ( 92.77)	Acc@5 100.00 ( 99.86)
Epoch: [35][220/391]	Time  0.066 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.4983e-01 (2.0915e-01)	Acc@1  92.19 ( 92.68)	Acc@5 100.00 ( 99.86)
Epoch: [35][230/391]	Time  0.068 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.6135e-01 (2.1016e-01)	Acc@1  94.53 ( 92.65)	Acc@5 100.00 ( 99.86)
Epoch: [35][240/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.7533e-01 (2.0960e-01)	Acc@1  95.31 ( 92.68)	Acc@5  99.22 ( 99.85)
Epoch: [35][250/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 8.6104e-02 (2.0865e-01)	Acc@1  98.44 ( 92.71)	Acc@5 100.00 ( 99.86)
Epoch: [35][260/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.3502e-01 (2.0804e-01)	Acc@1  85.94 ( 92.73)	Acc@5 100.00 ( 99.87)
Epoch: [35][270/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.9814e-01 (2.0722e-01)	Acc@1  91.41 ( 92.77)	Acc@5  99.22 ( 99.87)
Epoch: [35][280/391]	Time  0.066 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.6649e-01 (2.0775e-01)	Acc@1  93.75 ( 92.73)	Acc@5 100.00 ( 99.87)
Epoch: [35][290/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.2494e-01 (2.0765e-01)	Acc@1  92.97 ( 92.72)	Acc@5  99.22 ( 99.87)
Epoch: [35][300/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.4653e-01 (2.0787e-01)	Acc@1  90.62 ( 92.69)	Acc@5  99.22 ( 99.87)
Epoch: [35][310/391]	Time  0.066 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.6805e-01 (2.0771e-01)	Acc@1  93.75 ( 92.70)	Acc@5 100.00 ( 99.87)
Epoch: [35][320/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.3478e-01 (2.0838e-01)	Acc@1  93.75 ( 92.68)	Acc@5 100.00 ( 99.88)
Epoch: [35][330/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.5741e-01 (2.0925e-01)	Acc@1  92.97 ( 92.66)	Acc@5 100.00 ( 99.87)
Epoch: [35][340/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.5747e-01 (2.0916e-01)	Acc@1  93.75 ( 92.67)	Acc@5 100.00 ( 99.87)
Epoch: [35][350/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.1509e-01 (2.0843e-01)	Acc@1  95.31 ( 92.70)	Acc@5 100.00 ( 99.88)
Epoch: [35][360/391]	Time  0.066 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.1003e-01 (2.0878e-01)	Acc@1  93.75 ( 92.70)	Acc@5 100.00 ( 99.87)
Epoch: [35][370/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.4739e-01 (2.0896e-01)	Acc@1  89.06 ( 92.68)	Acc@5 100.00 ( 99.87)
Epoch: [35][380/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.0925e-01 (2.0938e-01)	Acc@1  92.97 ( 92.66)	Acc@5 100.00 ( 99.87)
Epoch: [35][390/391]	Time  0.046 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.6636e-01 (2.0929e-01)	Acc@1  93.75 ( 92.68)	Acc@5 100.00 ( 99.87)
## e[35] optimizer.zero_grad (sum) time: 0.40236639976501465
## e[35]       loss.backward (sum) time: 6.943891525268555
## e[35]      optimizer.step (sum) time: 3.3076608180999756
## epoch[35] training(only) time: 25.180644989013672
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 2.5775e-01 (2.5775e-01)	Acc@1  87.00 ( 87.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.027 ( 0.041)	Loss 3.7998e-01 (2.8341e-01)	Acc@1  89.00 ( 89.82)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.024 ( 0.034)	Loss 3.5088e-01 (2.9762e-01)	Acc@1  86.00 ( 90.00)	Acc@5  99.00 ( 99.57)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 2.6647e-01 (3.0426e-01)	Acc@1  89.00 ( 90.10)	Acc@5 100.00 ( 99.58)
Test: [ 40/100]	Time  0.024 ( 0.030)	Loss 2.5583e-01 (3.0344e-01)	Acc@1  91.00 ( 89.98)	Acc@5  99.00 ( 99.59)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 2.2954e-01 (3.0430e-01)	Acc@1  92.00 ( 90.10)	Acc@5  99.00 ( 99.59)
Test: [ 60/100]	Time  0.027 ( 0.029)	Loss 3.1534e-01 (3.0415e-01)	Acc@1  91.00 ( 90.13)	Acc@5 100.00 ( 99.64)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 4.0821e-01 (3.0087e-01)	Acc@1  88.00 ( 90.23)	Acc@5 100.00 ( 99.68)
Test: [ 80/100]	Time  0.024 ( 0.028)	Loss 2.3757e-01 (3.0041e-01)	Acc@1  93.00 ( 90.26)	Acc@5 100.00 ( 99.69)
Test: [ 90/100]	Time  0.024 ( 0.028)	Loss 2.0044e-01 (2.9744e-01)	Acc@1  95.00 ( 90.33)	Acc@5 100.00 ( 99.70)
 * Acc@1 90.340 Acc@5 99.700
### epoch[35] execution time: 28.04844832420349
EPOCH 36
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [36][  0/391]	Time  0.250 ( 0.250)	Data  0.188 ( 0.188)	Loss 1.7125e-01 (1.7125e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [36][ 10/391]	Time  0.064 ( 0.080)	Data  0.001 ( 0.018)	Loss 1.5858e-01 (1.7669e-01)	Acc@1  94.53 ( 93.96)	Acc@5 100.00 (100.00)
Epoch: [36][ 20/391]	Time  0.065 ( 0.072)	Data  0.001 ( 0.010)	Loss 1.5631e-01 (1.8997e-01)	Acc@1  96.09 ( 93.49)	Acc@5 100.00 ( 99.96)
Epoch: [36][ 30/391]	Time  0.060 ( 0.069)	Data  0.001 ( 0.007)	Loss 2.0934e-01 (1.9424e-01)	Acc@1  93.75 ( 93.22)	Acc@5 100.00 ( 99.95)
Epoch: [36][ 40/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.006)	Loss 2.1012e-01 (1.9384e-01)	Acc@1  91.41 ( 93.22)	Acc@5 100.00 ( 99.94)
Epoch: [36][ 50/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.005)	Loss 1.6605e-01 (1.9257e-01)	Acc@1  92.97 ( 93.32)	Acc@5 100.00 ( 99.95)
Epoch: [36][ 60/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.7947e-01 (1.9505e-01)	Acc@1  94.53 ( 93.26)	Acc@5 100.00 ( 99.94)
Epoch: [36][ 70/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.004)	Loss 1.9712e-01 (1.9370e-01)	Acc@1  92.19 ( 93.32)	Acc@5 100.00 ( 99.94)
Epoch: [36][ 80/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.5930e-01 (1.9541e-01)	Acc@1  90.62 ( 93.35)	Acc@5  99.22 ( 99.93)
Epoch: [36][ 90/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.9894e-01 (1.9517e-01)	Acc@1  94.53 ( 93.40)	Acc@5 100.00 ( 99.91)
Epoch: [36][100/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.8303e-01 (1.9727e-01)	Acc@1  94.53 ( 93.26)	Acc@5 100.00 ( 99.91)
Epoch: [36][110/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.7368e-01 (1.9431e-01)	Acc@1  92.19 ( 93.40)	Acc@5 100.00 ( 99.90)
Epoch: [36][120/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.0881e-01 (1.9630e-01)	Acc@1  91.41 ( 93.30)	Acc@5 100.00 ( 99.90)
Epoch: [36][130/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.6916e-01 (1.9684e-01)	Acc@1  93.75 ( 93.23)	Acc@5 100.00 ( 99.90)
Epoch: [36][140/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2817e-01 (1.9728e-01)	Acc@1  92.97 ( 93.26)	Acc@5 100.00 ( 99.91)
Epoch: [36][150/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.5341e-01 (1.9652e-01)	Acc@1  91.41 ( 93.28)	Acc@5 100.00 ( 99.91)
Epoch: [36][160/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.5064e-01 (1.9598e-01)	Acc@1  90.62 ( 93.27)	Acc@5 100.00 ( 99.91)
Epoch: [36][170/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8144e-01 (1.9469e-01)	Acc@1  93.75 ( 93.33)	Acc@5 100.00 ( 99.92)
Epoch: [36][180/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5215e-01 (1.9534e-01)	Acc@1  95.31 ( 93.29)	Acc@5 100.00 ( 99.91)
Epoch: [36][190/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4055e-01 (1.9615e-01)	Acc@1  96.88 ( 93.26)	Acc@5 100.00 ( 99.91)
Epoch: [36][200/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5843e-01 (1.9709e-01)	Acc@1  95.31 ( 93.22)	Acc@5 100.00 ( 99.91)
Epoch: [36][210/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.0629e-01 (1.9795e-01)	Acc@1  89.06 ( 93.18)	Acc@5 100.00 ( 99.91)
Epoch: [36][220/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2687e-01 (1.9882e-01)	Acc@1  92.97 ( 93.15)	Acc@5 100.00 ( 99.92)
Epoch: [36][230/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8269e-01 (1.9844e-01)	Acc@1  91.41 ( 93.16)	Acc@5 100.00 ( 99.91)
Epoch: [36][240/391]	Time  0.067 ( 0.065)	Data  0.002 ( 0.002)	Loss 2.3699e-01 (1.9946e-01)	Acc@1  91.41 ( 93.14)	Acc@5 100.00 ( 99.90)
Epoch: [36][250/391]	Time  0.072 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.6671e-01 (2.0131e-01)	Acc@1  91.41 ( 93.05)	Acc@5 100.00 ( 99.90)
Epoch: [36][260/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9923e-01 (2.0146e-01)	Acc@1  92.97 ( 93.06)	Acc@5 100.00 ( 99.90)
Epoch: [36][270/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7833e-01 (2.0107e-01)	Acc@1  95.31 ( 93.07)	Acc@5 100.00 ( 99.90)
Epoch: [36][280/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0980e-01 (2.0175e-01)	Acc@1  93.75 ( 93.07)	Acc@5 100.00 ( 99.89)
Epoch: [36][290/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.7305e-01 (2.0231e-01)	Acc@1  90.62 ( 93.09)	Acc@5  99.22 ( 99.88)
Epoch: [36][300/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.6123e-01 (2.0225e-01)	Acc@1  92.19 ( 93.10)	Acc@5 100.00 ( 99.89)
Epoch: [36][310/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4795e-01 (2.0269e-01)	Acc@1  95.31 ( 93.07)	Acc@5 100.00 ( 99.88)
Epoch: [36][320/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5860e-01 (2.0244e-01)	Acc@1  95.31 ( 93.08)	Acc@5 100.00 ( 99.88)
Epoch: [36][330/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3944e-01 (2.0290e-01)	Acc@1  93.75 ( 93.09)	Acc@5  99.22 ( 99.87)
Epoch: [36][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.0534e-01 (2.0317e-01)	Acc@1  92.19 ( 93.07)	Acc@5 100.00 ( 99.87)
Epoch: [36][350/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1356e-01 (2.0307e-01)	Acc@1  95.31 ( 93.08)	Acc@5 100.00 ( 99.87)
Epoch: [36][360/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6953e-01 (2.0201e-01)	Acc@1  93.75 ( 93.11)	Acc@5 100.00 ( 99.87)
Epoch: [36][370/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.7459e-01 (2.0165e-01)	Acc@1  94.53 ( 93.12)	Acc@5 100.00 ( 99.87)
Epoch: [36][380/391]	Time  0.067 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.6593e-01 (2.0174e-01)	Acc@1  95.31 ( 93.11)	Acc@5 100.00 ( 99.87)
Epoch: [36][390/391]	Time  0.046 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.4529e-01 (2.0179e-01)	Acc@1  90.00 ( 93.10)	Acc@5 100.00 ( 99.87)
## e[36] optimizer.zero_grad (sum) time: 0.3969712257385254
## e[36]       loss.backward (sum) time: 6.923516035079956
## e[36]      optimizer.step (sum) time: 3.372188091278076
## epoch[36] training(only) time: 25.25784182548523
# Switched to evaluate mode...
Test: [  0/100]	Time  0.192 ( 0.192)	Loss 2.6014e-01 (2.6014e-01)	Acc@1  86.00 ( 86.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.027 ( 0.041)	Loss 3.5559e-01 (2.7696e-01)	Acc@1  89.00 ( 90.09)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.027 ( 0.034)	Loss 3.5645e-01 (3.0131e-01)	Acc@1  89.00 ( 90.29)	Acc@5  99.00 ( 99.57)
Test: [ 30/100]	Time  0.032 ( 0.032)	Loss 2.5273e-01 (3.0708e-01)	Acc@1  89.00 ( 90.32)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.024 ( 0.030)	Loss 2.5490e-01 (3.0625e-01)	Acc@1  92.00 ( 90.29)	Acc@5  99.00 ( 99.63)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 2.2658e-01 (3.0662e-01)	Acc@1  91.00 ( 90.14)	Acc@5  99.00 ( 99.61)
Test: [ 60/100]	Time  0.027 ( 0.028)	Loss 3.2634e-01 (3.0708e-01)	Acc@1  91.00 ( 90.10)	Acc@5 100.00 ( 99.66)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 4.5660e-01 (3.0274e-01)	Acc@1  86.00 ( 90.17)	Acc@5 100.00 ( 99.70)
Test: [ 80/100]	Time  0.027 ( 0.028)	Loss 2.4031e-01 (3.0284e-01)	Acc@1  94.00 ( 90.16)	Acc@5 100.00 ( 99.70)
Test: [ 90/100]	Time  0.027 ( 0.027)	Loss 2.1777e-01 (2.9965e-01)	Acc@1  93.00 ( 90.14)	Acc@5 100.00 ( 99.73)
 * Acc@1 90.080 Acc@5 99.710
### epoch[36] execution time: 28.143471717834473
EPOCH 37
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [37][  0/391]	Time  0.237 ( 0.237)	Data  0.175 ( 0.175)	Loss 2.8732e-01 (2.8732e-01)	Acc@1  89.84 ( 89.84)	Acc@5 100.00 (100.00)
Epoch: [37][ 10/391]	Time  0.063 ( 0.079)	Data  0.001 ( 0.017)	Loss 2.1460e-01 (1.8944e-01)	Acc@1  92.97 ( 93.04)	Acc@5  99.22 ( 99.86)
Epoch: [37][ 20/391]	Time  0.064 ( 0.072)	Data  0.001 ( 0.009)	Loss 2.3884e-01 (1.8754e-01)	Acc@1  92.19 ( 93.34)	Acc@5 100.00 ( 99.89)
Epoch: [37][ 30/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.007)	Loss 2.3290e-01 (1.8264e-01)	Acc@1  94.53 ( 93.78)	Acc@5  99.22 ( 99.87)
Epoch: [37][ 40/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.005)	Loss 2.0493e-01 (1.8358e-01)	Acc@1  90.62 ( 93.62)	Acc@5 100.00 ( 99.89)
Epoch: [37][ 50/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.005)	Loss 2.0216e-01 (1.8966e-01)	Acc@1  92.97 ( 93.46)	Acc@5 100.00 ( 99.91)
Epoch: [37][ 60/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.004)	Loss 2.5175e-01 (1.8971e-01)	Acc@1  91.41 ( 93.44)	Acc@5 100.00 ( 99.92)
Epoch: [37][ 70/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.7948e-01 (1.8679e-01)	Acc@1  94.53 ( 93.46)	Acc@5 100.00 ( 99.92)
Epoch: [37][ 80/391]	Time  0.073 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.7067e-01 (1.8717e-01)	Acc@1  92.19 ( 93.47)	Acc@5 100.00 ( 99.89)
Epoch: [37][ 90/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.6648e-01 (1.8743e-01)	Acc@1  92.97 ( 93.48)	Acc@5 100.00 ( 99.90)
Epoch: [37][100/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.7531e-01 (1.8693e-01)	Acc@1  94.53 ( 93.49)	Acc@5  99.22 ( 99.89)
Epoch: [37][110/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.4680e-01 (1.8774e-01)	Acc@1  94.53 ( 93.43)	Acc@5  99.22 ( 99.89)
Epoch: [37][120/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.9633e-01 (1.8736e-01)	Acc@1  92.97 ( 93.47)	Acc@5  99.22 ( 99.88)
Epoch: [37][130/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.2204e-01 (1.8810e-01)	Acc@1  89.06 ( 93.45)	Acc@5 100.00 ( 99.89)
Epoch: [37][140/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.6134e-01 (1.8811e-01)	Acc@1  92.97 ( 93.46)	Acc@5 100.00 ( 99.88)
Epoch: [37][150/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2116e-01 (1.8872e-01)	Acc@1  91.41 ( 93.44)	Acc@5 100.00 ( 99.88)
Epoch: [37][160/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1683e-01 (1.8907e-01)	Acc@1  96.09 ( 93.42)	Acc@5 100.00 ( 99.88)
Epoch: [37][170/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3682e-01 (1.8925e-01)	Acc@1  92.19 ( 93.41)	Acc@5 100.00 ( 99.89)
Epoch: [37][180/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4906e-01 (1.8877e-01)	Acc@1  96.88 ( 93.40)	Acc@5 100.00 ( 99.89)
Epoch: [37][190/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9585e-01 (1.8862e-01)	Acc@1  94.53 ( 93.43)	Acc@5 100.00 ( 99.89)
Epoch: [37][200/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1126e-01 (1.8984e-01)	Acc@1  92.97 ( 93.42)	Acc@5 100.00 ( 99.88)
Epoch: [37][210/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1808e-01 (1.8939e-01)	Acc@1  90.62 ( 93.44)	Acc@5 100.00 ( 99.89)
Epoch: [37][220/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6864e-01 (1.9041e-01)	Acc@1  92.97 ( 93.42)	Acc@5 100.00 ( 99.89)
Epoch: [37][230/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8298e-01 (1.9031e-01)	Acc@1  93.75 ( 93.44)	Acc@5 100.00 ( 99.89)
Epoch: [37][240/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6732e-01 (1.9093e-01)	Acc@1  93.75 ( 93.38)	Acc@5 100.00 ( 99.90)
Epoch: [37][250/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0231e-01 (1.9122e-01)	Acc@1  91.41 ( 93.38)	Acc@5 100.00 ( 99.90)
Epoch: [37][260/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4868e-01 (1.9091e-01)	Acc@1  95.31 ( 93.36)	Acc@5 100.00 ( 99.90)
Epoch: [37][270/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7474e-01 (1.9141e-01)	Acc@1  93.75 ( 93.36)	Acc@5 100.00 ( 99.90)
Epoch: [37][280/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.8306e-01 (1.9169e-01)	Acc@1  95.31 ( 93.34)	Acc@5 100.00 ( 99.89)
Epoch: [37][290/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.3530e-01 (1.9287e-01)	Acc@1  92.19 ( 93.31)	Acc@5 100.00 ( 99.90)
Epoch: [37][300/391]	Time  0.059 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.8424e-01 (1.9389e-01)	Acc@1  93.75 ( 93.27)	Acc@5 100.00 ( 99.90)
Epoch: [37][310/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.8125e-01 (1.9362e-01)	Acc@1  93.75 ( 93.28)	Acc@5 100.00 ( 99.90)
Epoch: [37][320/391]	Time  0.060 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.9355e-01 (1.9357e-01)	Acc@1  93.75 ( 93.29)	Acc@5 100.00 ( 99.90)
Epoch: [37][330/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.6563e-01 (1.9382e-01)	Acc@1  92.97 ( 93.27)	Acc@5 100.00 ( 99.90)
Epoch: [37][340/391]	Time  0.069 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.0592e-01 (1.9341e-01)	Acc@1  98.44 ( 93.29)	Acc@5 100.00 ( 99.90)
Epoch: [37][350/391]	Time  0.064 ( 0.064)	Data  0.002 ( 0.002)	Loss 1.2354e-01 (1.9338e-01)	Acc@1  96.09 ( 93.32)	Acc@5 100.00 ( 99.90)
Epoch: [37][360/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.2631e-01 (1.9345e-01)	Acc@1  95.31 ( 93.30)	Acc@5 100.00 ( 99.90)
Epoch: [37][370/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.3793e-01 (1.9405e-01)	Acc@1  95.31 ( 93.28)	Acc@5 100.00 ( 99.90)
Epoch: [37][380/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.0218e-01 (1.9374e-01)	Acc@1  94.53 ( 93.29)	Acc@5 100.00 ( 99.91)
Epoch: [37][390/391]	Time  0.046 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.4253e-01 (1.9383e-01)	Acc@1  91.25 ( 93.31)	Acc@5  98.75 ( 99.90)
## e[37] optimizer.zero_grad (sum) time: 0.40022969245910645
## e[37]       loss.backward (sum) time: 6.915660619735718
## e[37]      optimizer.step (sum) time: 3.3471312522888184
## epoch[37] training(only) time: 25.21165657043457
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 2.6631e-01 (2.6631e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.028 ( 0.042)	Loss 3.4789e-01 (2.7167e-01)	Acc@1  88.00 ( 90.64)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.024 ( 0.034)	Loss 3.2536e-01 (2.9369e-01)	Acc@1  90.00 ( 90.48)	Acc@5  99.00 ( 99.71)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 2.9520e-01 (3.0283e-01)	Acc@1  89.00 ( 90.29)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.024 ( 0.030)	Loss 2.7991e-01 (3.0517e-01)	Acc@1  92.00 ( 90.07)	Acc@5  99.00 ( 99.76)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 2.3053e-01 (3.0356e-01)	Acc@1  91.00 ( 90.10)	Acc@5  99.00 ( 99.73)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 2.9306e-01 (3.0274e-01)	Acc@1  95.00 ( 90.20)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.028 ( 0.028)	Loss 3.8917e-01 (2.9943e-01)	Acc@1  85.00 ( 90.17)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.024 ( 0.028)	Loss 2.4235e-01 (2.9927e-01)	Acc@1  92.00 ( 90.14)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.027 ( 0.028)	Loss 2.1632e-01 (2.9680e-01)	Acc@1  93.00 ( 90.14)	Acc@5 100.00 ( 99.77)
 * Acc@1 90.120 Acc@5 99.760
### epoch[37] execution time: 28.11030673980713
EPOCH 38
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [38][  0/391]	Time  0.251 ( 0.251)	Data  0.183 ( 0.183)	Loss 9.8048e-02 (9.8048e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [38][ 10/391]	Time  0.061 ( 0.080)	Data  0.001 ( 0.018)	Loss 1.4222e-01 (1.6229e-01)	Acc@1  93.75 ( 94.11)	Acc@5 100.00 ( 99.86)
Epoch: [38][ 20/391]	Time  0.061 ( 0.072)	Data  0.001 ( 0.010)	Loss 2.0941e-01 (1.6493e-01)	Acc@1  92.97 ( 94.27)	Acc@5 100.00 ( 99.93)
Epoch: [38][ 30/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.007)	Loss 1.3033e-01 (1.6827e-01)	Acc@1  94.53 ( 94.03)	Acc@5 100.00 ( 99.95)
Epoch: [38][ 40/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.006)	Loss 2.1354e-01 (1.7602e-01)	Acc@1  93.75 ( 93.88)	Acc@5  99.22 ( 99.89)
Epoch: [38][ 50/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.005)	Loss 1.7574e-01 (1.7461e-01)	Acc@1  92.97 ( 93.95)	Acc@5 100.00 ( 99.91)
Epoch: [38][ 60/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.004)	Loss 1.8382e-01 (1.7484e-01)	Acc@1  95.31 ( 93.93)	Acc@5 100.00 ( 99.92)
Epoch: [38][ 70/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.004)	Loss 1.9154e-01 (1.7837e-01)	Acc@1  91.41 ( 93.76)	Acc@5 100.00 ( 99.90)
Epoch: [38][ 80/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.8396e-01 (1.8269e-01)	Acc@1  89.06 ( 93.57)	Acc@5  99.22 ( 99.90)
Epoch: [38][ 90/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.1271e-01 (1.8497e-01)	Acc@1  92.19 ( 93.50)	Acc@5 100.00 ( 99.89)
Epoch: [38][100/391]	Time  0.070 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.9390e-01 (1.8754e-01)	Acc@1  91.41 ( 93.42)	Acc@5 100.00 ( 99.89)
Epoch: [38][110/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.003)	Loss 9.2621e-02 (1.8921e-01)	Acc@1  97.66 ( 93.41)	Acc@5 100.00 ( 99.87)
Epoch: [38][120/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.2729e-01 (1.8848e-01)	Acc@1  96.88 ( 93.46)	Acc@5 100.00 ( 99.87)
Epoch: [38][130/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.3596e-01 (1.8911e-01)	Acc@1  96.09 ( 93.44)	Acc@5 100.00 ( 99.88)
Epoch: [38][140/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7957e-01 (1.8853e-01)	Acc@1  93.75 ( 93.47)	Acc@5 100.00 ( 99.89)
Epoch: [38][150/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6622e-01 (1.8803e-01)	Acc@1  94.53 ( 93.48)	Acc@5 100.00 ( 99.89)
Epoch: [38][160/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7351e-01 (1.8716e-01)	Acc@1  95.31 ( 93.49)	Acc@5 100.00 ( 99.89)
Epoch: [38][170/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8629e-01 (1.8727e-01)	Acc@1  92.97 ( 93.45)	Acc@5 100.00 ( 99.90)
Epoch: [38][180/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9183e-01 (1.8817e-01)	Acc@1  92.97 ( 93.47)	Acc@5 100.00 ( 99.90)
Epoch: [38][190/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0244e-01 (1.8955e-01)	Acc@1  92.97 ( 93.42)	Acc@5  99.22 ( 99.90)
Epoch: [38][200/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.4385e-01 (1.9020e-01)	Acc@1  92.19 ( 93.37)	Acc@5 100.00 ( 99.90)
Epoch: [38][210/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7904e-01 (1.8997e-01)	Acc@1  92.97 ( 93.35)	Acc@5 100.00 ( 99.90)
Epoch: [38][220/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2691e-01 (1.9014e-01)	Acc@1  91.41 ( 93.36)	Acc@5 100.00 ( 99.90)
Epoch: [38][230/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5232e-01 (1.8939e-01)	Acc@1  93.75 ( 93.37)	Acc@5 100.00 ( 99.91)
Epoch: [38][240/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3986e-01 (1.9010e-01)	Acc@1  92.97 ( 93.38)	Acc@5 100.00 ( 99.91)
Epoch: [38][250/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0785e-01 (1.8918e-01)	Acc@1  90.62 ( 93.40)	Acc@5 100.00 ( 99.91)
Epoch: [38][260/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7840e-01 (1.8779e-01)	Acc@1  94.53 ( 93.45)	Acc@5  99.22 ( 99.91)
Epoch: [38][270/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.4590e-02 (1.8792e-01)	Acc@1  96.09 ( 93.46)	Acc@5 100.00 ( 99.91)
Epoch: [38][280/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.3927e-02 (1.8712e-01)	Acc@1  96.09 ( 93.48)	Acc@5 100.00 ( 99.91)
Epoch: [38][290/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6779e-01 (1.8680e-01)	Acc@1  95.31 ( 93.50)	Acc@5 100.00 ( 99.91)
Epoch: [38][300/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1459e-01 (1.8718e-01)	Acc@1  91.41 ( 93.47)	Acc@5 100.00 ( 99.91)
Epoch: [38][310/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1621e-01 (1.8703e-01)	Acc@1  92.97 ( 93.47)	Acc@5 100.00 ( 99.91)
Epoch: [38][320/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2188e-01 (1.8772e-01)	Acc@1  94.53 ( 93.45)	Acc@5  99.22 ( 99.91)
Epoch: [38][330/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7571e-01 (1.8810e-01)	Acc@1  93.75 ( 93.43)	Acc@5  99.22 ( 99.90)
Epoch: [38][340/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6435e-01 (1.8838e-01)	Acc@1  94.53 ( 93.44)	Acc@5 100.00 ( 99.90)
Epoch: [38][350/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6836e-01 (1.8804e-01)	Acc@1  92.19 ( 93.45)	Acc@5 100.00 ( 99.90)
Epoch: [38][360/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4616e-01 (1.8815e-01)	Acc@1  95.31 ( 93.46)	Acc@5 100.00 ( 99.90)
Epoch: [38][370/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0472e-01 (1.8806e-01)	Acc@1  96.09 ( 93.48)	Acc@5 100.00 ( 99.89)
Epoch: [38][380/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3449e-01 (1.8871e-01)	Acc@1  91.41 ( 93.46)	Acc@5 100.00 ( 99.90)
Epoch: [38][390/391]	Time  0.047 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.0508e-01 (1.8961e-01)	Acc@1  90.00 ( 93.45)	Acc@5 100.00 ( 99.89)
## e[38] optimizer.zero_grad (sum) time: 0.39867496490478516
## e[38]       loss.backward (sum) time: 6.9274656772613525
## e[38]      optimizer.step (sum) time: 3.440128803253174
## epoch[38] training(only) time: 25.317628860473633
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 2.7143e-01 (2.7143e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.041)	Loss 3.1965e-01 (2.6568e-01)	Acc@1  91.00 ( 90.64)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.027 ( 0.033)	Loss 3.1564e-01 (2.8598e-01)	Acc@1  91.00 ( 90.81)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 2.7145e-01 (2.9812e-01)	Acc@1  89.00 ( 90.45)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.026 ( 0.030)	Loss 2.6792e-01 (3.0219e-01)	Acc@1  89.00 ( 90.22)	Acc@5  99.00 ( 99.71)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 2.6870e-01 (3.0285e-01)	Acc@1  91.00 ( 90.29)	Acc@5  99.00 ( 99.67)
Test: [ 60/100]	Time  0.027 ( 0.028)	Loss 2.9085e-01 (3.0143e-01)	Acc@1  94.00 ( 90.43)	Acc@5 100.00 ( 99.69)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 4.4479e-01 (2.9769e-01)	Acc@1  86.00 ( 90.52)	Acc@5 100.00 ( 99.72)
Test: [ 80/100]	Time  0.027 ( 0.028)	Loss 2.6805e-01 (2.9854e-01)	Acc@1  91.00 ( 90.46)	Acc@5 100.00 ( 99.73)
Test: [ 90/100]	Time  0.024 ( 0.028)	Loss 1.7997e-01 (2.9541e-01)	Acc@1  95.00 ( 90.41)	Acc@5 100.00 ( 99.75)
 * Acc@1 90.340 Acc@5 99.740
### epoch[38] execution time: 28.165974617004395
EPOCH 39
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [39][  0/391]	Time  0.250 ( 0.250)	Data  0.188 ( 0.188)	Loss 1.5998e-01 (1.5998e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
Epoch: [39][ 10/391]	Time  0.065 ( 0.081)	Data  0.001 ( 0.018)	Loss 1.9789e-01 (1.6705e-01)	Acc@1  93.75 ( 94.39)	Acc@5 100.00 (100.00)
Epoch: [39][ 20/391]	Time  0.059 ( 0.073)	Data  0.001 ( 0.010)	Loss 1.7712e-01 (1.6306e-01)	Acc@1  94.53 ( 94.42)	Acc@5 100.00 (100.00)
Epoch: [39][ 30/391]	Time  0.062 ( 0.069)	Data  0.001 ( 0.007)	Loss 2.3367e-01 (1.6678e-01)	Acc@1  93.75 ( 94.35)	Acc@5 100.00 (100.00)
Epoch: [39][ 40/391]	Time  0.062 ( 0.068)	Data  0.001 ( 0.006)	Loss 1.6646e-01 (1.6966e-01)	Acc@1  93.75 ( 94.02)	Acc@5 100.00 (100.00)
Epoch: [39][ 50/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.005)	Loss 2.1223e-01 (1.7009e-01)	Acc@1  92.19 ( 94.07)	Acc@5 100.00 ( 99.97)
Epoch: [39][ 60/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 2.2601e-01 (1.7197e-01)	Acc@1  92.19 ( 93.99)	Acc@5 100.00 ( 99.96)
Epoch: [39][ 70/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.004)	Loss 2.8109e-01 (1.7221e-01)	Acc@1  92.19 ( 93.89)	Acc@5 100.00 ( 99.97)
Epoch: [39][ 80/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.2878e-01 (1.7134e-01)	Acc@1  95.31 ( 93.93)	Acc@5 100.00 ( 99.95)
Epoch: [39][ 90/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.2869e-01 (1.7744e-01)	Acc@1  91.41 ( 93.78)	Acc@5  99.22 ( 99.92)
Epoch: [39][100/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.4866e-01 (1.7853e-01)	Acc@1  96.09 ( 93.77)	Acc@5 100.00 ( 99.91)
Epoch: [39][110/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 2.0582e-01 (1.7847e-01)	Acc@1  93.75 ( 93.80)	Acc@5 100.00 ( 99.92)
Epoch: [39][120/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.003)	Loss 3.4315e-01 (1.8047e-01)	Acc@1  89.06 ( 93.76)	Acc@5  99.22 ( 99.90)
Epoch: [39][130/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.7964e-01 (1.8053e-01)	Acc@1  92.19 ( 93.67)	Acc@5 100.00 ( 99.90)
Epoch: [39][140/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.6822e-01 (1.7891e-01)	Acc@1  90.62 ( 93.75)	Acc@5 100.00 ( 99.90)
Epoch: [39][150/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.4201e-02 (1.7889e-01)	Acc@1  98.44 ( 93.77)	Acc@5 100.00 ( 99.90)
Epoch: [39][160/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.8472e-01 (1.7863e-01)	Acc@1  88.28 ( 93.76)	Acc@5  99.22 ( 99.90)
Epoch: [39][170/391]	Time  0.075 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2596e-01 (1.7693e-01)	Acc@1  95.31 ( 93.84)	Acc@5 100.00 ( 99.90)
Epoch: [39][180/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8049e-01 (1.7559e-01)	Acc@1  94.53 ( 93.91)	Acc@5 100.00 ( 99.90)
Epoch: [39][190/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2651e-01 (1.7656e-01)	Acc@1  92.97 ( 93.90)	Acc@5 100.00 ( 99.90)
Epoch: [39][200/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0702e-01 (1.7613e-01)	Acc@1  92.19 ( 93.89)	Acc@5 100.00 ( 99.91)
Epoch: [39][210/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8894e-01 (1.7691e-01)	Acc@1  92.97 ( 93.84)	Acc@5 100.00 ( 99.91)
Epoch: [39][220/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.5994e-01 (1.7735e-01)	Acc@1  92.19 ( 93.82)	Acc@5 100.00 ( 99.91)
Epoch: [39][230/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.0950e-01 (1.7892e-01)	Acc@1  89.84 ( 93.76)	Acc@5  99.22 ( 99.91)
Epoch: [39][240/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9053e-01 (1.8051e-01)	Acc@1  93.75 ( 93.70)	Acc@5 100.00 ( 99.89)
Epoch: [39][250/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1217e-01 (1.8204e-01)	Acc@1  92.19 ( 93.66)	Acc@5 100.00 ( 99.89)
Epoch: [39][260/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.4179e-01 (1.8233e-01)	Acc@1  93.75 ( 93.67)	Acc@5 100.00 ( 99.89)
Epoch: [39][270/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1333e-01 (1.8236e-01)	Acc@1  90.62 ( 93.68)	Acc@5 100.00 ( 99.89)
Epoch: [39][280/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.3732e-01 (1.8293e-01)	Acc@1  90.62 ( 93.67)	Acc@5 100.00 ( 99.89)
Epoch: [39][290/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5848e-01 (1.8324e-01)	Acc@1  94.53 ( 93.67)	Acc@5 100.00 ( 99.89)
Epoch: [39][300/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.5974e-01 (1.8345e-01)	Acc@1  92.97 ( 93.67)	Acc@5  99.22 ( 99.89)
Epoch: [39][310/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0446e-01 (1.8273e-01)	Acc@1  96.88 ( 93.68)	Acc@5 100.00 ( 99.89)
Epoch: [39][320/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2898e-01 (1.8293e-01)	Acc@1  95.31 ( 93.69)	Acc@5 100.00 ( 99.88)
Epoch: [39][330/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8812e-01 (1.8214e-01)	Acc@1  92.97 ( 93.72)	Acc@5 100.00 ( 99.88)
Epoch: [39][340/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1505e-01 (1.8171e-01)	Acc@1  96.88 ( 93.73)	Acc@5 100.00 ( 99.89)
Epoch: [39][350/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2434e-01 (1.8125e-01)	Acc@1  92.97 ( 93.74)	Acc@5  99.22 ( 99.88)
Epoch: [39][360/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2965e-01 (1.8090e-01)	Acc@1  92.19 ( 93.75)	Acc@5  99.22 ( 99.89)
Epoch: [39][370/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0535e-01 (1.8065e-01)	Acc@1  91.41 ( 93.76)	Acc@5 100.00 ( 99.88)
Epoch: [39][380/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1260e-01 (1.8168e-01)	Acc@1  96.88 ( 93.74)	Acc@5 100.00 ( 99.88)
Epoch: [39][390/391]	Time  0.045 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.1482e-01 (1.8097e-01)	Acc@1  95.00 ( 93.77)	Acc@5 100.00 ( 99.88)
## e[39] optimizer.zero_grad (sum) time: 0.3995833396911621
## e[39]       loss.backward (sum) time: 6.958357334136963
## e[39]      optimizer.step (sum) time: 3.3719322681427
## epoch[39] training(only) time: 25.315359592437744
# Switched to evaluate mode...
Test: [  0/100]	Time  0.191 ( 0.191)	Loss 2.5989e-01 (2.5989e-01)	Acc@1  87.00 ( 87.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.042)	Loss 3.2760e-01 (2.5873e-01)	Acc@1  91.00 ( 90.36)	Acc@5  99.00 ( 99.73)
Test: [ 20/100]	Time  0.027 ( 0.034)	Loss 3.2726e-01 (2.8127e-01)	Acc@1  88.00 ( 90.48)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.027 ( 0.031)	Loss 2.9669e-01 (2.9755e-01)	Acc@1  88.00 ( 90.13)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.024 ( 0.030)	Loss 2.3501e-01 (3.0371e-01)	Acc@1  93.00 ( 90.00)	Acc@5  99.00 ( 99.73)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 2.6189e-01 (3.0762e-01)	Acc@1  91.00 ( 89.98)	Acc@5  99.00 ( 99.71)
Test: [ 60/100]	Time  0.024 ( 0.029)	Loss 2.9976e-01 (3.0743e-01)	Acc@1  92.00 ( 90.11)	Acc@5 100.00 ( 99.72)
Test: [ 70/100]	Time  0.033 ( 0.028)	Loss 4.4148e-01 (3.0369e-01)	Acc@1  85.00 ( 90.10)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.024 ( 0.028)	Loss 2.2090e-01 (3.0296e-01)	Acc@1  92.00 ( 90.14)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.024 ( 0.028)	Loss 2.0164e-01 (3.0109e-01)	Acc@1  93.00 ( 90.01)	Acc@5 100.00 ( 99.77)
 * Acc@1 90.010 Acc@5 99.760
### epoch[39] execution time: 28.19965171813965
EPOCH 40
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [40][  0/391]	Time  0.258 ( 0.258)	Data  0.190 ( 0.190)	Loss 2.8089e-01 (2.8089e-01)	Acc@1  86.72 ( 86.72)	Acc@5 100.00 (100.00)
Epoch: [40][ 10/391]	Time  0.065 ( 0.081)	Data  0.001 ( 0.018)	Loss 8.9410e-02 (1.8824e-01)	Acc@1  96.88 ( 93.18)	Acc@5 100.00 ( 99.86)
Epoch: [40][ 20/391]	Time  0.063 ( 0.073)	Data  0.001 ( 0.010)	Loss 1.2148e-01 (1.7490e-01)	Acc@1  94.53 ( 93.64)	Acc@5 100.00 ( 99.93)
Epoch: [40][ 30/391]	Time  0.062 ( 0.070)	Data  0.001 ( 0.007)	Loss 2.2232e-01 (1.8250e-01)	Acc@1  91.41 ( 93.35)	Acc@5 100.00 ( 99.92)
Epoch: [40][ 40/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.006)	Loss 2.6098e-01 (1.8394e-01)	Acc@1  89.84 ( 93.41)	Acc@5 100.00 ( 99.92)
Epoch: [40][ 50/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.005)	Loss 1.4677e-01 (1.7802e-01)	Acc@1  96.09 ( 93.72)	Acc@5 100.00 ( 99.94)
Epoch: [40][ 60/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.3501e-01 (1.7687e-01)	Acc@1  96.09 ( 93.75)	Acc@5 100.00 ( 99.95)
Epoch: [40][ 70/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.004)	Loss 1.4128e-01 (1.7933e-01)	Acc@1  96.09 ( 93.72)	Acc@5 100.00 ( 99.96)
Epoch: [40][ 80/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.9171e-01 (1.7943e-01)	Acc@1  92.19 ( 93.61)	Acc@5 100.00 ( 99.96)
Epoch: [40][ 90/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 2.7210e-01 (1.7986e-01)	Acc@1  92.19 ( 93.55)	Acc@5  99.22 ( 99.96)
Epoch: [40][100/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.2686e-01 (1.8066e-01)	Acc@1  94.53 ( 93.49)	Acc@5 100.00 ( 99.96)
Epoch: [40][110/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.003)	Loss 2.3877e-01 (1.8305e-01)	Acc@1  89.84 ( 93.52)	Acc@5 100.00 ( 99.94)
Epoch: [40][120/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.003)	Loss 2.4610e-01 (1.8272e-01)	Acc@1  93.75 ( 93.55)	Acc@5  98.44 ( 99.93)
Epoch: [40][130/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.9619e-01 (1.8145e-01)	Acc@1  93.75 ( 93.61)	Acc@5 100.00 ( 99.93)
Epoch: [40][140/391]	Time  0.067 ( 0.065)	Data  0.002 ( 0.002)	Loss 1.5504e-01 (1.8107e-01)	Acc@1  96.09 ( 93.63)	Acc@5 100.00 ( 99.92)
Epoch: [40][150/391]	Time  0.071 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9032e-01 (1.8148e-01)	Acc@1  93.75 ( 93.58)	Acc@5  99.22 ( 99.92)
Epoch: [40][160/391]	Time  0.058 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4821e-01 (1.8127e-01)	Acc@1  93.75 ( 93.60)	Acc@5 100.00 ( 99.92)
Epoch: [40][170/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1292e-01 (1.7921e-01)	Acc@1  92.97 ( 93.69)	Acc@5 100.00 ( 99.92)
Epoch: [40][180/391]	Time  0.070 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3576e-01 (1.7783e-01)	Acc@1  94.53 ( 93.73)	Acc@5 100.00 ( 99.93)
Epoch: [40][190/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2359e-01 (1.7867e-01)	Acc@1  92.97 ( 93.73)	Acc@5 100.00 ( 99.93)
Epoch: [40][200/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0886e-01 (1.7894e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 ( 99.93)
Epoch: [40][210/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.9545e-01 (1.7967e-01)	Acc@1  91.41 ( 93.76)	Acc@5 100.00 ( 99.93)
Epoch: [40][220/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.6443e-01 (1.7931e-01)	Acc@1  92.97 ( 93.77)	Acc@5 100.00 ( 99.92)
Epoch: [40][230/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.0259e-01 (1.7995e-01)	Acc@1  92.97 ( 93.75)	Acc@5 100.00 ( 99.93)
Epoch: [40][240/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.8156e-01 (1.8024e-01)	Acc@1  92.19 ( 93.73)	Acc@5 100.00 ( 99.92)
Epoch: [40][250/391]	Time  0.069 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.3747e-01 (1.7972e-01)	Acc@1  95.31 ( 93.75)	Acc@5 100.00 ( 99.92)
Epoch: [40][260/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.3912e-01 (1.7927e-01)	Acc@1  95.31 ( 93.79)	Acc@5 100.00 ( 99.92)
Epoch: [40][270/391]	Time  0.067 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.5340e-01 (1.8018e-01)	Acc@1  93.75 ( 93.76)	Acc@5 100.00 ( 99.92)
Epoch: [40][280/391]	Time  0.067 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.6846e-01 (1.7946e-01)	Acc@1  93.75 ( 93.80)	Acc@5 100.00 ( 99.91)
Epoch: [40][290/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.3540e-01 (1.7955e-01)	Acc@1  91.41 ( 93.79)	Acc@5  99.22 ( 99.91)
Epoch: [40][300/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.7497e-01 (1.7937e-01)	Acc@1  93.75 ( 93.81)	Acc@5  99.22 ( 99.91)
Epoch: [40][310/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.6256e-01 (1.7893e-01)	Acc@1  92.97 ( 93.82)	Acc@5 100.00 ( 99.91)
Epoch: [40][320/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.1101e-01 (1.7897e-01)	Acc@1  91.41 ( 93.80)	Acc@5 100.00 ( 99.91)
Epoch: [40][330/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.6496e-01 (1.7867e-01)	Acc@1  93.75 ( 93.82)	Acc@5 100.00 ( 99.90)
Epoch: [40][340/391]	Time  0.059 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.6308e-01 (1.7953e-01)	Acc@1  97.66 ( 93.78)	Acc@5 100.00 ( 99.90)
Epoch: [40][350/391]	Time  0.067 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.5364e-01 (1.7961e-01)	Acc@1  94.53 ( 93.77)	Acc@5 100.00 ( 99.90)
Epoch: [40][360/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.5897e-01 (1.7937e-01)	Acc@1  94.53 ( 93.77)	Acc@5 100.00 ( 99.91)
Epoch: [40][370/391]	Time  0.066 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.3079e-01 (1.7942e-01)	Acc@1  91.41 ( 93.77)	Acc@5 100.00 ( 99.91)
Epoch: [40][380/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.9490e-01 (1.8008e-01)	Acc@1  92.19 ( 93.76)	Acc@5 100.00 ( 99.91)
Epoch: [40][390/391]	Time  0.058 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.7821e-01 (1.8013e-01)	Acc@1  95.00 ( 93.77)	Acc@5  98.75 ( 99.91)
## e[40] optimizer.zero_grad (sum) time: 0.4046502113342285
## e[40]       loss.backward (sum) time: 6.925973415374756
## e[40]      optimizer.step (sum) time: 3.33561110496521
## epoch[40] training(only) time: 25.251546382904053
# Switched to evaluate mode...
Test: [  0/100]	Time  0.196 ( 0.196)	Loss 2.7732e-01 (2.7732e-01)	Acc@1  86.00 ( 86.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.041)	Loss 3.2604e-01 (2.6020e-01)	Acc@1  91.00 ( 90.55)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.027 ( 0.034)	Loss 3.2677e-01 (2.9014e-01)	Acc@1  87.00 ( 90.48)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 2.5380e-01 (2.9771e-01)	Acc@1  88.00 ( 90.61)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.024 ( 0.030)	Loss 2.7197e-01 (3.0009e-01)	Acc@1  92.00 ( 90.59)	Acc@5  99.00 ( 99.76)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 2.7778e-01 (3.0440e-01)	Acc@1  91.00 ( 90.59)	Acc@5  99.00 ( 99.73)
Test: [ 60/100]	Time  0.025 ( 0.028)	Loss 2.9946e-01 (3.0323e-01)	Acc@1  93.00 ( 90.54)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 4.3482e-01 (2.9788e-01)	Acc@1  86.00 ( 90.63)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.024 ( 0.028)	Loss 2.1907e-01 (2.9858e-01)	Acc@1  92.00 ( 90.49)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 1.7773e-01 (2.9631e-01)	Acc@1  95.00 ( 90.38)	Acc@5 100.00 ( 99.78)
 * Acc@1 90.320 Acc@5 99.780
### epoch[40] execution time: 28.097493410110474
EPOCH 41
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [41][  0/391]	Time  0.249 ( 0.249)	Data  0.185 ( 0.185)	Loss 1.7956e-01 (1.7956e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
Epoch: [41][ 10/391]	Time  0.065 ( 0.081)	Data  0.001 ( 0.018)	Loss 1.6087e-01 (1.6816e-01)	Acc@1  92.19 ( 94.18)	Acc@5 100.00 ( 99.93)
Epoch: [41][ 20/391]	Time  0.063 ( 0.073)	Data  0.001 ( 0.010)	Loss 1.5127e-01 (1.6962e-01)	Acc@1  96.09 ( 93.86)	Acc@5 100.00 ( 99.96)
Epoch: [41][ 30/391]	Time  0.062 ( 0.070)	Data  0.001 ( 0.007)	Loss 1.6401e-01 (1.6976e-01)	Acc@1  96.09 ( 93.98)	Acc@5 100.00 ( 99.97)
Epoch: [41][ 40/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.006)	Loss 2.0209e-01 (1.7084e-01)	Acc@1  93.75 ( 94.02)	Acc@5 100.00 ( 99.96)
Epoch: [41][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.005)	Loss 1.4027e-01 (1.6893e-01)	Acc@1  93.75 ( 94.13)	Acc@5 100.00 ( 99.95)
Epoch: [41][ 60/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.004)	Loss 2.7783e-01 (1.6484e-01)	Acc@1  90.62 ( 94.21)	Acc@5  98.44 ( 99.94)
Epoch: [41][ 70/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.2772e-01 (1.6655e-01)	Acc@1  95.31 ( 94.16)	Acc@5 100.00 ( 99.91)
Epoch: [41][ 80/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.5279e-01 (1.7014e-01)	Acc@1  94.53 ( 94.01)	Acc@5 100.00 ( 99.89)
Epoch: [41][ 90/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.1736e-01 (1.7109e-01)	Acc@1  91.41 ( 93.89)	Acc@5 100.00 ( 99.91)
Epoch: [41][100/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.4520e-01 (1.7116e-01)	Acc@1  92.19 ( 93.94)	Acc@5 100.00 ( 99.91)
Epoch: [41][110/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.4867e-01 (1.6877e-01)	Acc@1  94.53 ( 94.03)	Acc@5 100.00 ( 99.92)
Epoch: [41][120/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.5668e-01 (1.6954e-01)	Acc@1  95.31 ( 93.97)	Acc@5  99.22 ( 99.92)
Epoch: [41][130/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.3440e-01 (1.6918e-01)	Acc@1  92.97 ( 93.98)	Acc@5 100.00 ( 99.92)
Epoch: [41][140/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7644e-01 (1.7099e-01)	Acc@1  96.09 ( 93.97)	Acc@5 100.00 ( 99.92)
Epoch: [41][150/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3892e-01 (1.7112e-01)	Acc@1  96.09 ( 93.96)	Acc@5 100.00 ( 99.92)
Epoch: [41][160/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8319e-01 (1.7158e-01)	Acc@1  92.97 ( 93.90)	Acc@5 100.00 ( 99.92)
Epoch: [41][170/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.4269e-01 (1.7220e-01)	Acc@1  92.97 ( 93.87)	Acc@5 100.00 ( 99.92)
Epoch: [41][180/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4668e-01 (1.7164e-01)	Acc@1  93.75 ( 93.88)	Acc@5 100.00 ( 99.92)
Epoch: [41][190/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9299e-01 (1.7219e-01)	Acc@1  93.75 ( 93.91)	Acc@5  99.22 ( 99.91)
Epoch: [41][200/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.6245e-01 (1.7210e-01)	Acc@1  91.41 ( 93.90)	Acc@5 100.00 ( 99.91)
Epoch: [41][210/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2356e-01 (1.7221e-01)	Acc@1  92.19 ( 93.88)	Acc@5 100.00 ( 99.91)
Epoch: [41][220/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8429e-01 (1.7236e-01)	Acc@1  92.97 ( 93.92)	Acc@5 100.00 ( 99.90)
Epoch: [41][230/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5698e-01 (1.7260e-01)	Acc@1  94.53 ( 93.91)	Acc@5 100.00 ( 99.91)
Epoch: [41][240/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6180e-01 (1.7215e-01)	Acc@1  94.53 ( 93.93)	Acc@5 100.00 ( 99.91)
Epoch: [41][250/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2293e-01 (1.7343e-01)	Acc@1  92.97 ( 93.89)	Acc@5 100.00 ( 99.91)
Epoch: [41][260/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.4552e-01 (1.7428e-01)	Acc@1  89.84 ( 93.86)	Acc@5  99.22 ( 99.90)
Epoch: [41][270/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.9244e-01 (1.7561e-01)	Acc@1  90.62 ( 93.83)	Acc@5 100.00 ( 99.89)
Epoch: [41][280/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1776e-01 (1.7607e-01)	Acc@1  92.19 ( 93.81)	Acc@5 100.00 ( 99.90)
Epoch: [41][290/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5192e-01 (1.7583e-01)	Acc@1  93.75 ( 93.82)	Acc@5 100.00 ( 99.90)
Epoch: [41][300/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1387e-01 (1.7585e-01)	Acc@1  93.75 ( 93.82)	Acc@5 100.00 ( 99.90)
Epoch: [41][310/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9114e-01 (1.7561e-01)	Acc@1  96.09 ( 93.84)	Acc@5 100.00 ( 99.90)
Epoch: [41][320/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6489e-01 (1.7441e-01)	Acc@1  95.31 ( 93.90)	Acc@5  99.22 ( 99.90)
Epoch: [41][330/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3716e-01 (1.7458e-01)	Acc@1  93.75 ( 93.89)	Acc@5 100.00 ( 99.91)
Epoch: [41][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2772e-01 (1.7413e-01)	Acc@1  89.84 ( 93.90)	Acc@5 100.00 ( 99.90)
Epoch: [41][350/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6913e-01 (1.7475e-01)	Acc@1  95.31 ( 93.88)	Acc@5 100.00 ( 99.90)
Epoch: [41][360/391]	Time  0.068 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.0297e-01 (1.7571e-01)	Acc@1  92.19 ( 93.85)	Acc@5 100.00 ( 99.90)
Epoch: [41][370/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.5468e-01 (1.7634e-01)	Acc@1  94.53 ( 93.84)	Acc@5 100.00 ( 99.90)
Epoch: [41][380/391]	Time  0.059 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.3066e-01 (1.7632e-01)	Acc@1  94.53 ( 93.83)	Acc@5 100.00 ( 99.90)
Epoch: [41][390/391]	Time  0.046 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.5842e-01 (1.7607e-01)	Acc@1  95.00 ( 93.85)	Acc@5 100.00 ( 99.90)
## e[41] optimizer.zero_grad (sum) time: 0.3978302478790283
## e[41]       loss.backward (sum) time: 6.9025444984436035
## e[41]      optimizer.step (sum) time: 3.365433692932129
## epoch[41] training(only) time: 25.249879598617554
# Switched to evaluate mode...
Test: [  0/100]	Time  0.242 ( 0.242)	Loss 2.5031e-01 (2.5031e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.028 ( 0.046)	Loss 3.3141e-01 (2.7377e-01)	Acc@1  91.00 ( 90.45)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.024 ( 0.036)	Loss 3.4692e-01 (2.9472e-01)	Acc@1  87.00 ( 90.43)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.026 ( 0.032)	Loss 2.6828e-01 (3.0683e-01)	Acc@1  89.00 ( 90.29)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.024 ( 0.031)	Loss 2.5791e-01 (3.0870e-01)	Acc@1  90.00 ( 90.10)	Acc@5  99.00 ( 99.73)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 2.6661e-01 (3.0987e-01)	Acc@1  92.00 ( 90.12)	Acc@5  99.00 ( 99.71)
Test: [ 60/100]	Time  0.024 ( 0.029)	Loss 2.9713e-01 (3.0714e-01)	Acc@1  93.00 ( 90.34)	Acc@5 100.00 ( 99.72)
Test: [ 70/100]	Time  0.024 ( 0.029)	Loss 4.2315e-01 (3.0296e-01)	Acc@1  86.00 ( 90.39)	Acc@5  99.00 ( 99.72)
Test: [ 80/100]	Time  0.027 ( 0.028)	Loss 2.3217e-01 (3.0130e-01)	Acc@1  91.00 ( 90.37)	Acc@5 100.00 ( 99.73)
Test: [ 90/100]	Time  0.024 ( 0.028)	Loss 2.2697e-01 (2.9813e-01)	Acc@1  92.00 ( 90.30)	Acc@5 100.00 ( 99.76)
 * Acc@1 90.310 Acc@5 99.770
### epoch[41] execution time: 28.136103868484497
EPOCH 42
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [42][  0/391]	Time  0.254 ( 0.254)	Data  0.184 ( 0.184)	Loss 1.3851e-01 (1.3851e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [42][ 10/391]	Time  0.063 ( 0.081)	Data  0.001 ( 0.018)	Loss 1.5317e-01 (1.6440e-01)	Acc@1  94.53 ( 94.39)	Acc@5 100.00 ( 99.93)
Epoch: [42][ 20/391]	Time  0.060 ( 0.073)	Data  0.001 ( 0.010)	Loss 2.0236e-01 (1.6068e-01)	Acc@1  91.41 ( 94.23)	Acc@5 100.00 ( 99.96)
Epoch: [42][ 30/391]	Time  0.062 ( 0.070)	Data  0.001 ( 0.007)	Loss 1.7180e-01 (1.5860e-01)	Acc@1  95.31 ( 94.30)	Acc@5 100.00 ( 99.97)
Epoch: [42][ 40/391]	Time  0.061 ( 0.068)	Data  0.001 ( 0.006)	Loss 1.5961e-01 (1.5983e-01)	Acc@1  94.53 ( 94.32)	Acc@5 100.00 ( 99.98)
Epoch: [42][ 50/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.005)	Loss 1.3543e-01 (1.6435e-01)	Acc@1  94.53 ( 94.29)	Acc@5 100.00 ( 99.95)
Epoch: [42][ 60/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.8014e-01 (1.6573e-01)	Acc@1  93.75 ( 94.17)	Acc@5 100.00 ( 99.95)
Epoch: [42][ 70/391]	Time  0.066 ( 0.067)	Data  0.002 ( 0.004)	Loss 9.0943e-02 (1.6285e-01)	Acc@1  96.88 ( 94.32)	Acc@5 100.00 ( 99.96)
Epoch: [42][ 80/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.7610e-01 (1.6251e-01)	Acc@1  92.19 ( 94.27)	Acc@5 100.00 ( 99.96)
Epoch: [42][ 90/391]	Time  0.060 ( 0.066)	Data  0.001 ( 0.003)	Loss 8.3888e-02 (1.6088e-01)	Acc@1  97.66 ( 94.32)	Acc@5 100.00 ( 99.97)
Epoch: [42][100/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.1006e-01 (1.6333e-01)	Acc@1  95.31 ( 94.21)	Acc@5 100.00 ( 99.96)
Epoch: [42][110/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.9477e-01 (1.6623e-01)	Acc@1  93.75 ( 94.14)	Acc@5 100.00 ( 99.96)
Epoch: [42][120/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.6376e-01 (1.6689e-01)	Acc@1  92.97 ( 94.12)	Acc@5 100.00 ( 99.96)
Epoch: [42][130/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.7339e-01 (1.6633e-01)	Acc@1  92.97 ( 94.13)	Acc@5 100.00 ( 99.96)
Epoch: [42][140/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8488e-01 (1.6588e-01)	Acc@1  95.31 ( 94.17)	Acc@5 100.00 ( 99.95)
Epoch: [42][150/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1432e-01 (1.6369e-01)	Acc@1  95.31 ( 94.23)	Acc@5 100.00 ( 99.95)
Epoch: [42][160/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5842e-01 (1.6546e-01)	Acc@1  94.53 ( 94.18)	Acc@5 100.00 ( 99.95)
Epoch: [42][170/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5556e-01 (1.6351e-01)	Acc@1  94.53 ( 94.23)	Acc@5 100.00 ( 99.95)
Epoch: [42][180/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3842e-01 (1.6387e-01)	Acc@1  93.75 ( 94.24)	Acc@5  99.22 ( 99.95)
Epoch: [42][190/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3002e-01 (1.6341e-01)	Acc@1  95.31 ( 94.25)	Acc@5  99.22 ( 99.94)
Epoch: [42][200/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7023e-01 (1.6427e-01)	Acc@1  92.19 ( 94.21)	Acc@5 100.00 ( 99.94)
Epoch: [42][210/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.6506e-01 (1.6507e-01)	Acc@1  84.38 ( 94.19)	Acc@5 100.00 ( 99.93)
Epoch: [42][220/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6297e-01 (1.6498e-01)	Acc@1  92.19 ( 94.19)	Acc@5 100.00 ( 99.94)
Epoch: [42][230/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5374e-01 (1.6582e-01)	Acc@1  93.75 ( 94.16)	Acc@5 100.00 ( 99.93)
Epoch: [42][240/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7542e-01 (1.6519e-01)	Acc@1  92.97 ( 94.19)	Acc@5 100.00 ( 99.94)
Epoch: [42][250/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8920e-01 (1.6619e-01)	Acc@1  92.19 ( 94.17)	Acc@5 100.00 ( 99.94)
Epoch: [42][260/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1858e-01 (1.6617e-01)	Acc@1  96.09 ( 94.15)	Acc@5 100.00 ( 99.94)
Epoch: [42][270/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8901e-01 (1.6748e-01)	Acc@1  92.19 ( 94.13)	Acc@5 100.00 ( 99.94)
Epoch: [42][280/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5934e-01 (1.6872e-01)	Acc@1  92.97 ( 94.07)	Acc@5 100.00 ( 99.94)
Epoch: [42][290/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3642e-01 (1.6837e-01)	Acc@1  94.53 ( 94.09)	Acc@5 100.00 ( 99.94)
Epoch: [42][300/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5659e-01 (1.6752e-01)	Acc@1  92.97 ( 94.10)	Acc@5 100.00 ( 99.94)
Epoch: [42][310/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1396e-01 (1.6897e-01)	Acc@1  92.19 ( 94.07)	Acc@5  99.22 ( 99.93)
Epoch: [42][320/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1272e-01 (1.6952e-01)	Acc@1  92.19 ( 94.07)	Acc@5 100.00 ( 99.93)
Epoch: [42][330/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9440e-01 (1.6974e-01)	Acc@1  92.19 ( 94.05)	Acc@5 100.00 ( 99.94)
Epoch: [42][340/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9224e-01 (1.6993e-01)	Acc@1  94.53 ( 94.06)	Acc@5 100.00 ( 99.93)
Epoch: [42][350/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.5427e-01 (1.7061e-01)	Acc@1  89.84 ( 94.02)	Acc@5 100.00 ( 99.93)
Epoch: [42][360/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8138e-01 (1.7005e-01)	Acc@1  95.31 ( 94.05)	Acc@5 100.00 ( 99.93)
Epoch: [42][370/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3455e-01 (1.7051e-01)	Acc@1  89.84 ( 94.04)	Acc@5 100.00 ( 99.93)
Epoch: [42][380/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9863e-01 (1.7140e-01)	Acc@1  90.62 ( 94.00)	Acc@5 100.00 ( 99.93)
Epoch: [42][390/391]	Time  0.047 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.2143e-02 (1.7241e-01)	Acc@1  98.75 ( 93.97)	Acc@5 100.00 ( 99.92)
## e[42] optimizer.zero_grad (sum) time: 0.40335583686828613
## e[42]       loss.backward (sum) time: 6.975581645965576
## e[42]      optimizer.step (sum) time: 3.3418192863464355
## epoch[42] training(only) time: 25.30666208267212
# Switched to evaluate mode...
Test: [  0/100]	Time  0.194 ( 0.194)	Loss 2.3863e-01 (2.3863e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.041)	Loss 3.1367e-01 (2.7112e-01)	Acc@1  91.00 ( 91.27)	Acc@5  99.00 ( 99.73)
Test: [ 20/100]	Time  0.028 ( 0.034)	Loss 3.1739e-01 (2.9054e-01)	Acc@1  89.00 ( 91.24)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 2.6750e-01 (3.0327e-01)	Acc@1  91.00 ( 91.13)	Acc@5 100.00 ( 99.77)
Test: [ 40/100]	Time  0.028 ( 0.030)	Loss 2.2709e-01 (3.0452e-01)	Acc@1  92.00 ( 90.73)	Acc@5  99.00 ( 99.76)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 2.6650e-01 (3.0979e-01)	Acc@1  90.00 ( 90.61)	Acc@5  99.00 ( 99.73)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 3.0174e-01 (3.0666e-01)	Acc@1  92.00 ( 90.72)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 4.6019e-01 (3.0477e-01)	Acc@1  87.00 ( 90.58)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 2.3327e-01 (3.0427e-01)	Acc@1  92.00 ( 90.62)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.027 ( 0.027)	Loss 1.7190e-01 (2.9948e-01)	Acc@1  94.00 ( 90.65)	Acc@5 100.00 ( 99.76)
 * Acc@1 90.610 Acc@5 99.760
### epoch[42] execution time: 28.151355981826782
EPOCH 43
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [43][  0/391]	Time  0.250 ( 0.250)	Data  0.186 ( 0.186)	Loss 1.8940e-01 (1.8940e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
Epoch: [43][ 10/391]	Time  0.064 ( 0.081)	Data  0.001 ( 0.018)	Loss 1.6499e-01 (1.6310e-01)	Acc@1  96.09 ( 94.53)	Acc@5 100.00 (100.00)
Epoch: [43][ 20/391]	Time  0.065 ( 0.075)	Data  0.001 ( 0.010)	Loss 1.9692e-01 (1.7355e-01)	Acc@1  92.97 ( 93.82)	Acc@5 100.00 ( 99.93)
Epoch: [43][ 30/391]	Time  0.066 ( 0.071)	Data  0.001 ( 0.007)	Loss 1.5561e-01 (1.7104e-01)	Acc@1  96.88 ( 94.25)	Acc@5 100.00 ( 99.95)
Epoch: [43][ 40/391]	Time  0.067 ( 0.070)	Data  0.001 ( 0.006)	Loss 1.3429e-01 (1.6180e-01)	Acc@1  95.31 ( 94.51)	Acc@5 100.00 ( 99.96)
Epoch: [43][ 50/391]	Time  0.061 ( 0.068)	Data  0.001 ( 0.005)	Loss 2.0885e-01 (1.6696e-01)	Acc@1  95.31 ( 94.30)	Acc@5 100.00 ( 99.92)
Epoch: [43][ 60/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.7853e-01 (1.6742e-01)	Acc@1  91.41 ( 94.30)	Acc@5 100.00 ( 99.94)
Epoch: [43][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 2.1077e-01 (1.6831e-01)	Acc@1  92.19 ( 94.39)	Acc@5 100.00 ( 99.93)
Epoch: [43][ 80/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.003)	Loss 8.1288e-02 (1.6679e-01)	Acc@1  96.88 ( 94.47)	Acc@5 100.00 ( 99.93)
Epoch: [43][ 90/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.2664e-01 (1.6464e-01)	Acc@1  93.75 ( 94.50)	Acc@5 100.00 ( 99.93)
Epoch: [43][100/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.1840e-01 (1.6447e-01)	Acc@1  96.09 ( 94.46)	Acc@5 100.00 ( 99.92)
Epoch: [43][110/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.4611e-01 (1.6479e-01)	Acc@1  94.53 ( 94.45)	Acc@5 100.00 ( 99.92)
Epoch: [43][120/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.1396e-01 (1.6606e-01)	Acc@1  96.09 ( 94.46)	Acc@5 100.00 ( 99.91)
Epoch: [43][130/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.7527e-01 (1.6634e-01)	Acc@1  93.75 ( 94.42)	Acc@5 100.00 ( 99.90)
Epoch: [43][140/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3670e-01 (1.6630e-01)	Acc@1  93.75 ( 94.43)	Acc@5 100.00 ( 99.91)
Epoch: [43][150/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4979e-01 (1.6687e-01)	Acc@1  95.31 ( 94.35)	Acc@5 100.00 ( 99.91)
Epoch: [43][160/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5951e-01 (1.6734e-01)	Acc@1  96.09 ( 94.33)	Acc@5 100.00 ( 99.91)
Epoch: [43][170/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6689e-01 (1.6894e-01)	Acc@1  93.75 ( 94.27)	Acc@5 100.00 ( 99.91)
Epoch: [43][180/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.4759e-01 (1.7041e-01)	Acc@1  92.19 ( 94.22)	Acc@5 100.00 ( 99.91)
Epoch: [43][190/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2954e-01 (1.7038e-01)	Acc@1  94.53 ( 94.16)	Acc@5 100.00 ( 99.91)
Epoch: [43][200/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4389e-01 (1.7058e-01)	Acc@1  93.75 ( 94.16)	Acc@5 100.00 ( 99.91)
Epoch: [43][210/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8450e-01 (1.7192e-01)	Acc@1  93.75 ( 94.12)	Acc@5 100.00 ( 99.90)
Epoch: [43][220/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2936e-01 (1.7188e-01)	Acc@1  93.75 ( 94.11)	Acc@5 100.00 ( 99.90)
Epoch: [43][230/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2274e-01 (1.7120e-01)	Acc@1  96.09 ( 94.12)	Acc@5 100.00 ( 99.90)
Epoch: [43][240/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9120e-01 (1.7016e-01)	Acc@1  93.75 ( 94.16)	Acc@5 100.00 ( 99.91)
Epoch: [43][250/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.9454e-02 (1.6985e-01)	Acc@1  96.88 ( 94.18)	Acc@5 100.00 ( 99.91)
Epoch: [43][260/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3330e-01 (1.6962e-01)	Acc@1  93.75 ( 94.20)	Acc@5 100.00 ( 99.91)
Epoch: [43][270/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3273e-01 (1.7043e-01)	Acc@1  96.09 ( 94.17)	Acc@5 100.00 ( 99.91)
Epoch: [43][280/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0210e-01 (1.7067e-01)	Acc@1  92.19 ( 94.13)	Acc@5 100.00 ( 99.91)
Epoch: [43][290/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4497e-01 (1.7044e-01)	Acc@1  94.53 ( 94.13)	Acc@5 100.00 ( 99.91)
Epoch: [43][300/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.0355e-02 (1.6960e-01)	Acc@1  96.88 ( 94.15)	Acc@5 100.00 ( 99.91)
Epoch: [43][310/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.9222e-01 (1.7079e-01)	Acc@1  89.06 ( 94.10)	Acc@5  99.22 ( 99.91)
Epoch: [43][320/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6593e-01 (1.7076e-01)	Acc@1  94.53 ( 94.11)	Acc@5 100.00 ( 99.91)
Epoch: [43][330/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0787e-01 (1.7074e-01)	Acc@1  91.41 ( 94.10)	Acc@5 100.00 ( 99.91)
Epoch: [43][340/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1304e-01 (1.7042e-01)	Acc@1  96.09 ( 94.10)	Acc@5 100.00 ( 99.91)
Epoch: [43][350/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.1827e-02 (1.7019e-01)	Acc@1  96.88 ( 94.11)	Acc@5 100.00 ( 99.92)
Epoch: [43][360/391]	Time  0.070 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.5380e-01 (1.7035e-01)	Acc@1  92.19 ( 94.12)	Acc@5 100.00 ( 99.92)
Epoch: [43][370/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.0747e-01 (1.7109e-01)	Acc@1  92.97 ( 94.10)	Acc@5 100.00 ( 99.92)
Epoch: [43][380/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.4150e-01 (1.7136e-01)	Acc@1  92.97 ( 94.09)	Acc@5 100.00 ( 99.92)
Epoch: [43][390/391]	Time  0.050 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.1508e-01 (1.7119e-01)	Acc@1  96.25 ( 94.09)	Acc@5 100.00 ( 99.92)
## e[43] optimizer.zero_grad (sum) time: 0.4035160541534424
## e[43]       loss.backward (sum) time: 6.986440181732178
## e[43]      optimizer.step (sum) time: 3.328622817993164
## epoch[43] training(only) time: 25.257936716079712
# Switched to evaluate mode...
Test: [  0/100]	Time  0.190 ( 0.190)	Loss 2.6964e-01 (2.6964e-01)	Acc@1  86.00 ( 86.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.025 ( 0.041)	Loss 3.4675e-01 (2.7055e-01)	Acc@1  91.00 ( 90.36)	Acc@5  99.00 ( 99.73)
Test: [ 20/100]	Time  0.024 ( 0.034)	Loss 3.1875e-01 (2.9162e-01)	Acc@1  89.00 ( 90.67)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.027 ( 0.031)	Loss 3.0109e-01 (3.0433e-01)	Acc@1  92.00 ( 90.81)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 2.5375e-01 (3.0713e-01)	Acc@1  92.00 ( 90.54)	Acc@5  99.00 ( 99.73)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 2.4581e-01 (3.0891e-01)	Acc@1  92.00 ( 90.47)	Acc@5  99.00 ( 99.71)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 2.8252e-01 (3.0503e-01)	Acc@1  92.00 ( 90.57)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 4.7137e-01 (3.0040e-01)	Acc@1  87.00 ( 90.59)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.028 ( 0.027)	Loss 2.2444e-01 (3.0139e-01)	Acc@1  93.00 ( 90.53)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 2.2459e-01 (2.9649e-01)	Acc@1  92.00 ( 90.52)	Acc@5 100.00 ( 99.79)
 * Acc@1 90.490 Acc@5 99.790
### epoch[43] execution time: 28.081653356552124
EPOCH 44
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [44][  0/391]	Time  0.254 ( 0.254)	Data  0.184 ( 0.184)	Loss 1.0548e-01 (1.0548e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [44][ 10/391]	Time  0.063 ( 0.080)	Data  0.001 ( 0.018)	Loss 2.1632e-01 (1.6046e-01)	Acc@1  92.19 ( 94.03)	Acc@5 100.00 ( 99.86)
Epoch: [44][ 20/391]	Time  0.063 ( 0.072)	Data  0.001 ( 0.010)	Loss 2.9119e-01 (1.5365e-01)	Acc@1  90.62 ( 94.46)	Acc@5 100.00 ( 99.89)
Epoch: [44][ 30/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.007)	Loss 1.1589e-01 (1.5281e-01)	Acc@1  96.88 ( 94.68)	Acc@5 100.00 ( 99.90)
Epoch: [44][ 40/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.006)	Loss 8.2768e-02 (1.5381e-01)	Acc@1  96.88 ( 94.61)	Acc@5 100.00 ( 99.90)
Epoch: [44][ 50/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.005)	Loss 2.3522e-01 (1.5572e-01)	Acc@1  90.62 ( 94.49)	Acc@5  99.22 ( 99.91)
Epoch: [44][ 60/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.004)	Loss 7.6448e-02 (1.5671e-01)	Acc@1  97.66 ( 94.47)	Acc@5 100.00 ( 99.90)
Epoch: [44][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.4213e-01 (1.5740e-01)	Acc@1  96.09 ( 94.40)	Acc@5 100.00 ( 99.91)
Epoch: [44][ 80/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.2387e-01 (1.5698e-01)	Acc@1  89.84 ( 94.45)	Acc@5 100.00 ( 99.91)
Epoch: [44][ 90/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.3380e-01 (1.5692e-01)	Acc@1  96.09 ( 94.45)	Acc@5 100.00 ( 99.92)
Epoch: [44][100/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.0769e-01 (1.5714e-01)	Acc@1  96.88 ( 94.42)	Acc@5 100.00 ( 99.93)
Epoch: [44][110/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.3930e-01 (1.5958e-01)	Acc@1  94.53 ( 94.28)	Acc@5 100.00 ( 99.92)
Epoch: [44][120/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.4844e-01 (1.5957e-01)	Acc@1  92.19 ( 94.30)	Acc@5 100.00 ( 99.92)
Epoch: [44][130/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.2083e-01 (1.6095e-01)	Acc@1  90.62 ( 94.23)	Acc@5 100.00 ( 99.92)
Epoch: [44][140/391]	Time  0.070 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4431e-01 (1.6312e-01)	Acc@1  97.66 ( 94.18)	Acc@5 100.00 ( 99.92)
Epoch: [44][150/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6611e-01 (1.6369e-01)	Acc@1  94.53 ( 94.17)	Acc@5 100.00 ( 99.92)
Epoch: [44][160/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7305e-01 (1.6355e-01)	Acc@1  93.75 ( 94.16)	Acc@5 100.00 ( 99.92)
Epoch: [44][170/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2133e-01 (1.6474e-01)	Acc@1  93.75 ( 94.12)	Acc@5 100.00 ( 99.92)
Epoch: [44][180/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9224e-01 (1.6389e-01)	Acc@1  91.41 ( 94.13)	Acc@5 100.00 ( 99.92)
Epoch: [44][190/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2846e-01 (1.6482e-01)	Acc@1  96.09 ( 94.11)	Acc@5 100.00 ( 99.92)
Epoch: [44][200/391]	Time  0.070 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3239e-01 (1.6453e-01)	Acc@1  93.75 ( 94.10)	Acc@5 100.00 ( 99.91)
Epoch: [44][210/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7343e-01 (1.6486e-01)	Acc@1  92.19 ( 94.08)	Acc@5 100.00 ( 99.92)
Epoch: [44][220/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8782e-01 (1.6577e-01)	Acc@1  91.41 ( 94.02)	Acc@5 100.00 ( 99.92)
Epoch: [44][230/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2158e-01 (1.6521e-01)	Acc@1  92.19 ( 94.05)	Acc@5 100.00 ( 99.92)
Epoch: [44][240/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.5154e-01 (1.6517e-01)	Acc@1  89.84 ( 94.05)	Acc@5 100.00 ( 99.92)
Epoch: [44][250/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0930e-01 (1.6517e-01)	Acc@1  96.88 ( 94.04)	Acc@5 100.00 ( 99.92)
Epoch: [44][260/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9470e-01 (1.6486e-01)	Acc@1  92.19 ( 94.06)	Acc@5 100.00 ( 99.92)
Epoch: [44][270/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4982e-01 (1.6509e-01)	Acc@1  94.53 ( 94.07)	Acc@5 100.00 ( 99.92)
Epoch: [44][280/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4456e-01 (1.6549e-01)	Acc@1  93.75 ( 94.06)	Acc@5 100.00 ( 99.92)
Epoch: [44][290/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.5141e-01 (1.6547e-01)	Acc@1  88.28 ( 94.06)	Acc@5 100.00 ( 99.92)
Epoch: [44][300/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6279e-01 (1.6555e-01)	Acc@1  92.19 ( 94.05)	Acc@5 100.00 ( 99.92)
Epoch: [44][310/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.5525e-01 (1.6585e-01)	Acc@1  89.84 ( 94.05)	Acc@5 100.00 ( 99.92)
Epoch: [44][320/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2363e-01 (1.6627e-01)	Acc@1  96.09 ( 94.06)	Acc@5 100.00 ( 99.92)
Epoch: [44][330/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1758e-01 (1.6628e-01)	Acc@1  95.31 ( 94.06)	Acc@5 100.00 ( 99.92)
Epoch: [44][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3252e-01 (1.6644e-01)	Acc@1  95.31 ( 94.07)	Acc@5 100.00 ( 99.92)
Epoch: [44][350/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.5175e-01 (1.6702e-01)	Acc@1  89.84 ( 94.05)	Acc@5 100.00 ( 99.92)
Epoch: [44][360/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0659e-01 (1.6671e-01)	Acc@1  91.41 ( 94.06)	Acc@5 100.00 ( 99.92)
Epoch: [44][370/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1563e-01 (1.6748e-01)	Acc@1  91.41 ( 94.02)	Acc@5 100.00 ( 99.92)
Epoch: [44][380/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5419e-01 (1.6761e-01)	Acc@1  93.75 ( 94.02)	Acc@5 100.00 ( 99.92)
Epoch: [44][390/391]	Time  0.047 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.0354e-01 (1.6756e-01)	Acc@1  97.50 ( 94.03)	Acc@5 100.00 ( 99.93)
## e[44] optimizer.zero_grad (sum) time: 0.3980884552001953
## e[44]       loss.backward (sum) time: 6.933231830596924
## e[44]      optimizer.step (sum) time: 3.4046902656555176
## epoch[44] training(only) time: 25.31765341758728
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 2.7035e-01 (2.7035e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.027 ( 0.042)	Loss 3.6184e-01 (2.8115e-01)	Acc@1  90.00 ( 90.64)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.024 ( 0.034)	Loss 3.2752e-01 (2.9553e-01)	Acc@1  90.00 ( 90.67)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.024 ( 0.032)	Loss 2.5360e-01 (3.0573e-01)	Acc@1  91.00 ( 90.61)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.024 ( 0.030)	Loss 2.7279e-01 (3.0588e-01)	Acc@1  91.00 ( 90.49)	Acc@5  99.00 ( 99.68)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 2.3878e-01 (3.0937e-01)	Acc@1  94.00 ( 90.47)	Acc@5  99.00 ( 99.71)
Test: [ 60/100]	Time  0.026 ( 0.029)	Loss 3.0078e-01 (3.0670e-01)	Acc@1  93.00 ( 90.61)	Acc@5 100.00 ( 99.72)
Test: [ 70/100]	Time  0.026 ( 0.028)	Loss 4.6285e-01 (3.0432e-01)	Acc@1  88.00 ( 90.61)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.024 ( 0.028)	Loss 1.8129e-01 (3.0331e-01)	Acc@1  91.00 ( 90.51)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.028 ( 0.028)	Loss 1.9451e-01 (2.9969e-01)	Acc@1  92.00 ( 90.51)	Acc@5 100.00 ( 99.79)
 * Acc@1 90.460 Acc@5 99.780
### epoch[44] execution time: 28.198362112045288
EPOCH 45
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [45][  0/391]	Time  0.250 ( 0.250)	Data  0.188 ( 0.188)	Loss 1.5282e-01 (1.5282e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
Epoch: [45][ 10/391]	Time  0.066 ( 0.079)	Data  0.001 ( 0.018)	Loss 1.5902e-01 (1.5107e-01)	Acc@1  95.31 ( 94.74)	Acc@5 100.00 (100.00)
Epoch: [45][ 20/391]	Time  0.063 ( 0.073)	Data  0.001 ( 0.010)	Loss 1.5002e-01 (1.6674e-01)	Acc@1  96.88 ( 94.20)	Acc@5 100.00 (100.00)
Epoch: [45][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.007)	Loss 1.5388e-01 (1.6521e-01)	Acc@1  94.53 ( 94.20)	Acc@5 100.00 ( 99.97)
Epoch: [45][ 40/391]	Time  0.062 ( 0.068)	Data  0.001 ( 0.006)	Loss 1.5081e-01 (1.5827e-01)	Acc@1  94.53 ( 94.47)	Acc@5 100.00 ( 99.96)
Epoch: [45][ 50/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.005)	Loss 2.0282e-01 (1.6128e-01)	Acc@1  93.75 ( 94.26)	Acc@5 100.00 ( 99.97)
Epoch: [45][ 60/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.0810e-01 (1.5646e-01)	Acc@1  96.09 ( 94.44)	Acc@5 100.00 ( 99.97)
Epoch: [45][ 70/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.004)	Loss 1.5953e-01 (1.5924e-01)	Acc@1  92.97 ( 94.33)	Acc@5 100.00 ( 99.97)
Epoch: [45][ 80/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.8500e-01 (1.5787e-01)	Acc@1  94.53 ( 94.39)	Acc@5 100.00 ( 99.97)
Epoch: [45][ 90/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.6354e-01 (1.6164e-01)	Acc@1  93.75 ( 94.28)	Acc@5 100.00 ( 99.96)
Epoch: [45][100/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.003)	Loss 9.3063e-02 (1.6014e-01)	Acc@1  96.09 ( 94.30)	Acc@5 100.00 ( 99.95)
Epoch: [45][110/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.8053e-01 (1.6233e-01)	Acc@1  92.19 ( 94.23)	Acc@5 100.00 ( 99.95)
Epoch: [45][120/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.3846e-01 (1.6066e-01)	Acc@1  93.75 ( 94.28)	Acc@5 100.00 ( 99.95)
Epoch: [45][130/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.003)	Loss 2.1854e-01 (1.6260e-01)	Acc@1  92.97 ( 94.23)	Acc@5 100.00 ( 99.94)
Epoch: [45][140/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2780e-01 (1.6112e-01)	Acc@1  93.75 ( 94.35)	Acc@5 100.00 ( 99.94)
Epoch: [45][150/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0004e-01 (1.5958e-01)	Acc@1  95.31 ( 94.38)	Acc@5 100.00 ( 99.95)
Epoch: [45][160/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5116e-01 (1.5875e-01)	Acc@1  95.31 ( 94.44)	Acc@5 100.00 ( 99.95)
Epoch: [45][170/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2167e-01 (1.6010e-01)	Acc@1  95.31 ( 94.37)	Acc@5 100.00 ( 99.95)
Epoch: [45][180/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9942e-01 (1.5899e-01)	Acc@1  93.75 ( 94.43)	Acc@5 100.00 ( 99.95)
Epoch: [45][190/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0049e-01 (1.5932e-01)	Acc@1  92.19 ( 94.40)	Acc@5 100.00 ( 99.95)
Epoch: [45][200/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7355e-01 (1.6021e-01)	Acc@1  92.19 ( 94.32)	Acc@5  99.22 ( 99.95)
Epoch: [45][210/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8327e-01 (1.5980e-01)	Acc@1  92.97 ( 94.31)	Acc@5 100.00 ( 99.95)
Epoch: [45][220/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.8104e-02 (1.5969e-01)	Acc@1  96.09 ( 94.32)	Acc@5 100.00 ( 99.95)
Epoch: [45][230/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3058e-01 (1.6005e-01)	Acc@1  91.41 ( 94.31)	Acc@5 100.00 ( 99.95)
Epoch: [45][240/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3765e-01 (1.6026e-01)	Acc@1  93.75 ( 94.30)	Acc@5 100.00 ( 99.94)
Epoch: [45][250/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0663e-01 (1.5983e-01)	Acc@1  96.88 ( 94.31)	Acc@5 100.00 ( 99.95)
Epoch: [45][260/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6886e-01 (1.6004e-01)	Acc@1  93.75 ( 94.30)	Acc@5 100.00 ( 99.95)
Epoch: [45][270/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6807e-01 (1.5945e-01)	Acc@1  94.53 ( 94.32)	Acc@5 100.00 ( 99.95)
Epoch: [45][280/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.8995e-02 (1.5951e-01)	Acc@1  97.66 ( 94.29)	Acc@5 100.00 ( 99.94)
Epoch: [45][290/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.4823e-01 (1.5914e-01)	Acc@1  90.62 ( 94.29)	Acc@5 100.00 ( 99.94)
Epoch: [45][300/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.5696e-01 (1.5976e-01)	Acc@1  96.09 ( 94.27)	Acc@5 100.00 ( 99.95)
Epoch: [45][310/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.1115e-01 (1.5933e-01)	Acc@1  95.31 ( 94.28)	Acc@5 100.00 ( 99.94)
Epoch: [45][320/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.4692e-01 (1.5862e-01)	Acc@1  94.53 ( 94.31)	Acc@5 100.00 ( 99.94)
Epoch: [45][330/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.0585e-01 (1.5844e-01)	Acc@1  96.09 ( 94.31)	Acc@5 100.00 ( 99.94)
Epoch: [45][340/391]	Time  0.067 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.9924e-01 (1.5885e-01)	Acc@1  94.53 ( 94.32)	Acc@5  99.22 ( 99.93)
Epoch: [45][350/391]	Time  0.059 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.6917e-01 (1.5936e-01)	Acc@1  94.53 ( 94.30)	Acc@5 100.00 ( 99.93)
Epoch: [45][360/391]	Time  0.069 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.3251e-02 (1.5964e-01)	Acc@1  97.66 ( 94.31)	Acc@5 100.00 ( 99.93)
Epoch: [45][370/391]	Time  0.075 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.5684e-01 (1.5945e-01)	Acc@1  93.75 ( 94.31)	Acc@5 100.00 ( 99.93)
Epoch: [45][380/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.3605e-01 (1.5939e-01)	Acc@1  96.09 ( 94.33)	Acc@5 100.00 ( 99.93)
Epoch: [45][390/391]	Time  0.049 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.7221e-01 (1.5937e-01)	Acc@1  93.75 ( 94.31)	Acc@5 100.00 ( 99.93)
## e[45] optimizer.zero_grad (sum) time: 0.3993797302246094
## e[45]       loss.backward (sum) time: 6.925800561904907
## e[45]      optimizer.step (sum) time: 3.3493082523345947
## epoch[45] training(only) time: 25.241539001464844
# Switched to evaluate mode...
Test: [  0/100]	Time  0.193 ( 0.193)	Loss 2.7204e-01 (2.7204e-01)	Acc@1  87.00 ( 87.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.041)	Loss 3.6728e-01 (2.8185e-01)	Acc@1  89.00 ( 90.55)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 3.4956e-01 (2.9909e-01)	Acc@1  91.00 ( 90.90)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 2.8179e-01 (3.0842e-01)	Acc@1  92.00 ( 90.94)	Acc@5 100.00 ( 99.77)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 2.5339e-01 (3.1131e-01)	Acc@1  89.00 ( 90.44)	Acc@5  99.00 ( 99.73)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 2.3914e-01 (3.1121e-01)	Acc@1  89.00 ( 90.41)	Acc@5  99.00 ( 99.73)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 2.4352e-01 (3.0645e-01)	Acc@1  94.00 ( 90.48)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.027 ( 0.028)	Loss 4.7653e-01 (3.0472e-01)	Acc@1  87.00 ( 90.51)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.024 ( 0.028)	Loss 2.0967e-01 (3.0502e-01)	Acc@1  91.00 ( 90.38)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 1.7848e-01 (3.0012e-01)	Acc@1  93.00 ( 90.37)	Acc@5 100.00 ( 99.78)
 * Acc@1 90.360 Acc@5 99.770
### epoch[45] execution time: 28.112141847610474
EPOCH 46
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [46][  0/391]	Time  0.242 ( 0.242)	Data  0.179 ( 0.179)	Loss 1.6600e-01 (1.6600e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
Epoch: [46][ 10/391]	Time  0.064 ( 0.081)	Data  0.001 ( 0.017)	Loss 7.5813e-02 (1.5364e-01)	Acc@1  97.66 ( 94.39)	Acc@5 100.00 ( 99.86)
Epoch: [46][ 20/391]	Time  0.066 ( 0.073)	Data  0.001 ( 0.010)	Loss 1.0384e-01 (1.5978e-01)	Acc@1  95.31 ( 94.35)	Acc@5 100.00 ( 99.93)
Epoch: [46][ 30/391]	Time  0.065 ( 0.070)	Data  0.001 ( 0.007)	Loss 1.3597e-01 (1.5278e-01)	Acc@1  96.09 ( 94.61)	Acc@5 100.00 ( 99.92)
Epoch: [46][ 40/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.006)	Loss 1.0739e-01 (1.4737e-01)	Acc@1  96.09 ( 94.82)	Acc@5 100.00 ( 99.92)
Epoch: [46][ 50/391]	Time  0.062 ( 0.068)	Data  0.001 ( 0.005)	Loss 1.0189e-01 (1.4971e-01)	Acc@1  98.44 ( 94.72)	Acc@5 100.00 ( 99.92)
Epoch: [46][ 60/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.5421e-01 (1.5090e-01)	Acc@1  96.09 ( 94.76)	Acc@5 100.00 ( 99.91)
Epoch: [46][ 70/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.2446e-01 (1.5027e-01)	Acc@1  94.53 ( 94.69)	Acc@5 100.00 ( 99.92)
Epoch: [46][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.6589e-01 (1.4994e-01)	Acc@1  92.97 ( 94.72)	Acc@5 100.00 ( 99.93)
Epoch: [46][ 90/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.6585e-01 (1.4949e-01)	Acc@1  92.19 ( 94.70)	Acc@5 100.00 ( 99.91)
Epoch: [46][100/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.6725e-01 (1.5073e-01)	Acc@1  93.75 ( 94.62)	Acc@5 100.00 ( 99.91)
Epoch: [46][110/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.4957e-01 (1.5180e-01)	Acc@1  94.53 ( 94.62)	Acc@5 100.00 ( 99.92)
Epoch: [46][120/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 9.8692e-02 (1.5176e-01)	Acc@1  96.88 ( 94.62)	Acc@5 100.00 ( 99.92)
Epoch: [46][130/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.6087e-01 (1.5197e-01)	Acc@1  95.31 ( 94.58)	Acc@5 100.00 ( 99.92)
Epoch: [46][140/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.1269e-01 (1.5289e-01)	Acc@1  89.84 ( 94.54)	Acc@5 100.00 ( 99.93)
Epoch: [46][150/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.9132e-02 (1.5226e-01)	Acc@1  96.09 ( 94.53)	Acc@5 100.00 ( 99.93)
Epoch: [46][160/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2440e-01 (1.5193e-01)	Acc@1  92.19 ( 94.56)	Acc@5 100.00 ( 99.93)
Epoch: [46][170/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3841e-01 (1.5255e-01)	Acc@1  93.75 ( 94.55)	Acc@5 100.00 ( 99.92)
Epoch: [46][180/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0349e-01 (1.5214e-01)	Acc@1  95.31 ( 94.59)	Acc@5 100.00 ( 99.92)
Epoch: [46][190/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4079e-01 (1.5352e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 ( 99.93)
Epoch: [46][200/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7711e-01 (1.5405e-01)	Acc@1  94.53 ( 94.54)	Acc@5  99.22 ( 99.93)
Epoch: [46][210/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6836e-01 (1.5432e-01)	Acc@1  94.53 ( 94.54)	Acc@5 100.00 ( 99.93)
Epoch: [46][220/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7713e-01 (1.5654e-01)	Acc@1  94.53 ( 94.51)	Acc@5  99.22 ( 99.92)
Epoch: [46][230/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.8988e-02 (1.5588e-01)	Acc@1  96.88 ( 94.52)	Acc@5 100.00 ( 99.92)
Epoch: [46][240/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.6515e-01 (1.5700e-01)	Acc@1  92.97 ( 94.52)	Acc@5 100.00 ( 99.91)
Epoch: [46][250/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1188e-01 (1.5655e-01)	Acc@1  92.97 ( 94.53)	Acc@5 100.00 ( 99.91)
Epoch: [46][260/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.3842e-02 (1.5644e-01)	Acc@1  95.31 ( 94.53)	Acc@5 100.00 ( 99.92)
Epoch: [46][270/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0423e-01 (1.5668e-01)	Acc@1  90.62 ( 94.51)	Acc@5 100.00 ( 99.92)
Epoch: [46][280/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7435e-01 (1.5636e-01)	Acc@1  94.53 ( 94.50)	Acc@5 100.00 ( 99.92)
Epoch: [46][290/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3039e-01 (1.5618e-01)	Acc@1  95.31 ( 94.52)	Acc@5 100.00 ( 99.92)
Epoch: [46][300/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0386e-01 (1.5680e-01)	Acc@1  92.19 ( 94.48)	Acc@5 100.00 ( 99.92)
Epoch: [46][310/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9339e-01 (1.5676e-01)	Acc@1  92.97 ( 94.48)	Acc@5 100.00 ( 99.92)
Epoch: [46][320/391]	Time  0.077 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.6739e-01 (1.5701e-01)	Acc@1  92.97 ( 94.49)	Acc@5 100.00 ( 99.92)
Epoch: [46][330/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4539e-01 (1.5718e-01)	Acc@1  96.09 ( 94.50)	Acc@5 100.00 ( 99.92)
Epoch: [46][340/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3652e-01 (1.5732e-01)	Acc@1  95.31 ( 94.50)	Acc@5 100.00 ( 99.92)
Epoch: [46][350/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0539e-01 (1.5735e-01)	Acc@1  97.66 ( 94.49)	Acc@5 100.00 ( 99.92)
Epoch: [46][360/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2419e-01 (1.5812e-01)	Acc@1  96.88 ( 94.49)	Acc@5 100.00 ( 99.92)
Epoch: [46][370/391]	Time  0.060 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.6609e-01 (1.5874e-01)	Acc@1  93.75 ( 94.46)	Acc@5 100.00 ( 99.92)
Epoch: [46][380/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5570e-01 (1.5926e-01)	Acc@1  93.75 ( 94.45)	Acc@5 100.00 ( 99.91)
Epoch: [46][390/391]	Time  0.048 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.1184e-01 (1.5912e-01)	Acc@1  91.25 ( 94.46)	Acc@5 100.00 ( 99.91)
## e[46] optimizer.zero_grad (sum) time: 0.40486574172973633
## e[46]       loss.backward (sum) time: 6.946797132492065
## e[46]      optimizer.step (sum) time: 3.3607728481292725
## epoch[46] training(only) time: 25.30885601043701
# Switched to evaluate mode...
Test: [  0/100]	Time  0.190 ( 0.190)	Loss 2.5489e-01 (2.5489e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.026 ( 0.041)	Loss 3.7328e-01 (2.7785e-01)	Acc@1  89.00 ( 90.91)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 3.4390e-01 (2.9901e-01)	Acc@1  88.00 ( 90.71)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.028 ( 0.031)	Loss 2.9161e-01 (3.1242e-01)	Acc@1  91.00 ( 90.58)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.024 ( 0.030)	Loss 3.3316e-01 (3.1506e-01)	Acc@1  90.00 ( 90.27)	Acc@5  99.00 ( 99.68)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 2.4872e-01 (3.1909e-01)	Acc@1  89.00 ( 89.96)	Acc@5  99.00 ( 99.69)
Test: [ 60/100]	Time  0.024 ( 0.029)	Loss 2.9470e-01 (3.1593e-01)	Acc@1  94.00 ( 90.02)	Acc@5 100.00 ( 99.70)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 4.3384e-01 (3.1102e-01)	Acc@1  87.00 ( 90.06)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.024 ( 0.028)	Loss 2.4285e-01 (3.1273e-01)	Acc@1  92.00 ( 90.01)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 2.0098e-01 (3.0807e-01)	Acc@1  94.00 ( 90.15)	Acc@5 100.00 ( 99.77)
 * Acc@1 90.110 Acc@5 99.770
### epoch[46] execution time: 28.157491445541382
EPOCH 47
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [47][  0/391]	Time  0.296 ( 0.296)	Data  0.232 ( 0.232)	Loss 1.6168e-01 (1.6168e-01)	Acc@1  92.97 ( 92.97)	Acc@5 100.00 (100.00)
Epoch: [47][ 10/391]	Time  0.064 ( 0.085)	Data  0.001 ( 0.022)	Loss 1.6152e-01 (1.4837e-01)	Acc@1  94.53 ( 93.89)	Acc@5 100.00 (100.00)
Epoch: [47][ 20/391]	Time  0.062 ( 0.075)	Data  0.001 ( 0.012)	Loss 1.1199e-01 (1.4944e-01)	Acc@1  96.88 ( 94.08)	Acc@5 100.00 (100.00)
Epoch: [47][ 30/391]	Time  0.065 ( 0.072)	Data  0.001 ( 0.009)	Loss 2.2139e-01 (1.6612e-01)	Acc@1  92.97 ( 93.83)	Acc@5  98.44 ( 99.92)
Epoch: [47][ 40/391]	Time  0.065 ( 0.070)	Data  0.001 ( 0.007)	Loss 1.2043e-01 (1.7000e-01)	Acc@1  95.31 ( 93.79)	Acc@5 100.00 ( 99.90)
Epoch: [47][ 50/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.006)	Loss 1.6268e-01 (1.6628e-01)	Acc@1  92.97 ( 93.80)	Acc@5  99.22 ( 99.89)
Epoch: [47][ 60/391]	Time  0.078 ( 0.068)	Data  0.001 ( 0.005)	Loss 1.0964e-01 (1.6380e-01)	Acc@1  97.66 ( 93.94)	Acc@5 100.00 ( 99.91)
Epoch: [47][ 70/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 2.0781e-01 (1.6307e-01)	Acc@1  89.84 ( 94.01)	Acc@5 100.00 ( 99.91)
Epoch: [47][ 80/391]	Time  0.060 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.5741e-01 (1.6151e-01)	Acc@1  94.53 ( 94.10)	Acc@5 100.00 ( 99.92)
Epoch: [47][ 90/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.004)	Loss 1.1353e-01 (1.5910e-01)	Acc@1  96.88 ( 94.18)	Acc@5 100.00 ( 99.92)
Epoch: [47][100/391]	Time  0.059 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.7153e-01 (1.5760e-01)	Acc@1  95.31 ( 94.25)	Acc@5 100.00 ( 99.93)
Epoch: [47][110/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.4775e-01 (1.5775e-01)	Acc@1  92.97 ( 94.26)	Acc@5 100.00 ( 99.94)
Epoch: [47][120/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.0972e-01 (1.5696e-01)	Acc@1  98.44 ( 94.29)	Acc@5 100.00 ( 99.94)
Epoch: [47][130/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.7227e-01 (1.5760e-01)	Acc@1  95.31 ( 94.28)	Acc@5 100.00 ( 99.94)
Epoch: [47][140/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.4836e-01 (1.5768e-01)	Acc@1  96.09 ( 94.33)	Acc@5 100.00 ( 99.94)
Epoch: [47][150/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.4359e-01 (1.5858e-01)	Acc@1  95.31 ( 94.33)	Acc@5 100.00 ( 99.95)
Epoch: [47][160/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.4296e-01 (1.5805e-01)	Acc@1  96.88 ( 94.39)	Acc@5 100.00 ( 99.94)
Epoch: [47][170/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.8661e-02 (1.5705e-01)	Acc@1  98.44 ( 94.46)	Acc@5 100.00 ( 99.94)
Epoch: [47][180/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0088e-01 (1.5811e-01)	Acc@1  95.31 ( 94.44)	Acc@5 100.00 ( 99.94)
Epoch: [47][190/391]	Time  0.073 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6594e-01 (1.5813e-01)	Acc@1  93.75 ( 94.43)	Acc@5 100.00 ( 99.95)
Epoch: [47][200/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4599e-01 (1.5659e-01)	Acc@1  93.75 ( 94.49)	Acc@5 100.00 ( 99.95)
Epoch: [47][210/391]	Time  0.070 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5424e-01 (1.5632e-01)	Acc@1  94.53 ( 94.52)	Acc@5 100.00 ( 99.95)
Epoch: [47][220/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1407e-01 (1.5621e-01)	Acc@1  95.31 ( 94.49)	Acc@5 100.00 ( 99.95)
Epoch: [47][230/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.4693e-02 (1.5556e-01)	Acc@1  97.66 ( 94.50)	Acc@5 100.00 ( 99.94)
Epoch: [47][240/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3513e-01 (1.5517e-01)	Acc@1  93.75 ( 94.52)	Acc@5 100.00 ( 99.94)
Epoch: [47][250/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5330e-01 (1.5475e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 ( 99.95)
Epoch: [47][260/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.6707e-01 (1.5521e-01)	Acc@1  89.06 ( 94.50)	Acc@5 100.00 ( 99.95)
Epoch: [47][270/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0315e-01 (1.5457e-01)	Acc@1  92.19 ( 94.50)	Acc@5 100.00 ( 99.95)
Epoch: [47][280/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8343e-01 (1.5389e-01)	Acc@1  96.09 ( 94.53)	Acc@5  99.22 ( 99.95)
Epoch: [47][290/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2719e-01 (1.5366e-01)	Acc@1  89.84 ( 94.54)	Acc@5 100.00 ( 99.95)
Epoch: [47][300/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0577e-01 (1.5389e-01)	Acc@1  91.41 ( 94.54)	Acc@5 100.00 ( 99.95)
Epoch: [47][310/391]	Time  0.075 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1522e-01 (1.5468e-01)	Acc@1  96.88 ( 94.51)	Acc@5 100.00 ( 99.95)
Epoch: [47][320/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6003e-01 (1.5476e-01)	Acc@1  96.09 ( 94.52)	Acc@5 100.00 ( 99.95)
Epoch: [47][330/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9717e-01 (1.5514e-01)	Acc@1  92.19 ( 94.50)	Acc@5 100.00 ( 99.95)
Epoch: [47][340/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3285e-01 (1.5537e-01)	Acc@1  93.75 ( 94.49)	Acc@5 100.00 ( 99.95)
Epoch: [47][350/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5654e-01 (1.5522e-01)	Acc@1  92.97 ( 94.50)	Acc@5 100.00 ( 99.95)
Epoch: [47][360/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.7644e-02 (1.5488e-01)	Acc@1  96.88 ( 94.51)	Acc@5 100.00 ( 99.95)
Epoch: [47][370/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7352e-01 (1.5496e-01)	Acc@1  96.09 ( 94.50)	Acc@5 100.00 ( 99.95)
Epoch: [47][380/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.9159e-02 (1.5490e-01)	Acc@1  96.09 ( 94.51)	Acc@5 100.00 ( 99.94)
Epoch: [47][390/391]	Time  0.048 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.2681e-01 (1.5503e-01)	Acc@1  96.25 ( 94.51)	Acc@5 100.00 ( 99.94)
## e[47] optimizer.zero_grad (sum) time: 0.39752817153930664
## e[47]       loss.backward (sum) time: 6.972651958465576
## e[47]      optimizer.step (sum) time: 3.333836078643799
## epoch[47] training(only) time: 25.32700514793396
# Switched to evaluate mode...
Test: [  0/100]	Time  0.202 ( 0.202)	Loss 2.4948e-01 (2.4948e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.027 ( 0.042)	Loss 3.3904e-01 (2.7761e-01)	Acc@1  89.00 ( 90.55)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.028 ( 0.034)	Loss 3.6963e-01 (2.9992e-01)	Acc@1  90.00 ( 90.48)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.027 ( 0.031)	Loss 2.4176e-01 (3.0895e-01)	Acc@1  92.00 ( 90.42)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.028 ( 0.030)	Loss 2.6970e-01 (3.1268e-01)	Acc@1  90.00 ( 90.20)	Acc@5  99.00 ( 99.68)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 2.6071e-01 (3.1530e-01)	Acc@1  92.00 ( 90.24)	Acc@5  99.00 ( 99.69)
Test: [ 60/100]	Time  0.025 ( 0.028)	Loss 2.6575e-01 (3.1008e-01)	Acc@1  92.00 ( 90.48)	Acc@5 100.00 ( 99.70)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 4.8207e-01 (3.0601e-01)	Acc@1  88.00 ( 90.49)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.028 ( 0.028)	Loss 2.5560e-01 (3.0520e-01)	Acc@1  92.00 ( 90.43)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.028 ( 0.028)	Loss 2.0850e-01 (3.0158e-01)	Acc@1  94.00 ( 90.49)	Acc@5 100.00 ( 99.79)
 * Acc@1 90.460 Acc@5 99.800
### epoch[47] execution time: 28.183844566345215
EPOCH 48
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [48][  0/391]	Time  0.249 ( 0.249)	Data  0.181 ( 0.181)	Loss 1.5628e-01 (1.5628e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
Epoch: [48][ 10/391]	Time  0.066 ( 0.080)	Data  0.001 ( 0.017)	Loss 9.6744e-02 (1.3850e-01)	Acc@1  96.88 ( 95.45)	Acc@5 100.00 (100.00)
Epoch: [48][ 20/391]	Time  0.067 ( 0.073)	Data  0.001 ( 0.010)	Loss 1.2013e-01 (1.4829e-01)	Acc@1  94.53 ( 94.72)	Acc@5 100.00 ( 99.96)
Epoch: [48][ 30/391]	Time  0.060 ( 0.070)	Data  0.001 ( 0.007)	Loss 1.2146e-01 (1.5251e-01)	Acc@1  96.09 ( 94.78)	Acc@5 100.00 ( 99.97)
Epoch: [48][ 40/391]	Time  0.059 ( 0.068)	Data  0.001 ( 0.006)	Loss 1.3207e-01 (1.4891e-01)	Acc@1  93.75 ( 94.84)	Acc@5 100.00 ( 99.98)
Epoch: [48][ 50/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.005)	Loss 9.2386e-02 (1.4737e-01)	Acc@1  96.09 ( 94.88)	Acc@5 100.00 ( 99.98)
Epoch: [48][ 60/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.9632e-01 (1.5195e-01)	Acc@1  92.19 ( 94.71)	Acc@5 100.00 ( 99.97)
Epoch: [48][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.2884e-01 (1.5299e-01)	Acc@1  93.75 ( 94.54)	Acc@5 100.00 ( 99.98)
Epoch: [48][ 80/391]	Time  0.058 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.7793e-01 (1.5336e-01)	Acc@1  90.62 ( 94.51)	Acc@5 100.00 ( 99.97)
Epoch: [48][ 90/391]	Time  0.060 ( 0.066)	Data  0.001 ( 0.003)	Loss 9.7429e-02 (1.5382e-01)	Acc@1  98.44 ( 94.50)	Acc@5 100.00 ( 99.96)
Epoch: [48][100/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.4330e-01 (1.5319e-01)	Acc@1  95.31 ( 94.50)	Acc@5 100.00 ( 99.95)
Epoch: [48][110/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.1051e-01 (1.5249e-01)	Acc@1  92.97 ( 94.54)	Acc@5  99.22 ( 99.94)
Epoch: [48][120/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.6271e-01 (1.5216e-01)	Acc@1  92.97 ( 94.56)	Acc@5 100.00 ( 99.95)
Epoch: [48][130/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.0983e-01 (1.5017e-01)	Acc@1  96.88 ( 94.62)	Acc@5 100.00 ( 99.94)
Epoch: [48][140/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1294e-01 (1.5079e-01)	Acc@1  96.09 ( 94.60)	Acc@5 100.00 ( 99.94)
Epoch: [48][150/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7709e-01 (1.5004e-01)	Acc@1  93.75 ( 94.66)	Acc@5 100.00 ( 99.95)
Epoch: [48][160/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4218e-01 (1.4992e-01)	Acc@1  96.09 ( 94.67)	Acc@5 100.00 ( 99.95)
Epoch: [48][170/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2070e-01 (1.5114e-01)	Acc@1  93.75 ( 94.64)	Acc@5 100.00 ( 99.95)
Epoch: [48][180/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4030e-01 (1.5111e-01)	Acc@1  92.97 ( 94.64)	Acc@5 100.00 ( 99.95)
Epoch: [48][190/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2975e-01 (1.5189e-01)	Acc@1  96.88 ( 94.63)	Acc@5 100.00 ( 99.95)
Epoch: [48][200/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1071e-01 (1.5198e-01)	Acc@1  96.09 ( 94.61)	Acc@5 100.00 ( 99.95)
Epoch: [48][210/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9471e-01 (1.5208e-01)	Acc@1  92.19 ( 94.61)	Acc@5 100.00 ( 99.94)
Epoch: [48][220/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2773e-01 (1.5312e-01)	Acc@1  94.53 ( 94.58)	Acc@5 100.00 ( 99.95)
Epoch: [48][230/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9892e-01 (1.5428e-01)	Acc@1  92.97 ( 94.57)	Acc@5  99.22 ( 99.94)
Epoch: [48][240/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5232e-01 (1.5536e-01)	Acc@1  96.88 ( 94.54)	Acc@5 100.00 ( 99.94)
Epoch: [48][250/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4142e-01 (1.5566e-01)	Acc@1  95.31 ( 94.53)	Acc@5 100.00 ( 99.94)
Epoch: [48][260/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4588e-01 (1.5468e-01)	Acc@1  94.53 ( 94.56)	Acc@5 100.00 ( 99.94)
Epoch: [48][270/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7920e-01 (1.5509e-01)	Acc@1  95.31 ( 94.57)	Acc@5 100.00 ( 99.94)
Epoch: [48][280/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4458e-01 (1.5424e-01)	Acc@1  94.53 ( 94.61)	Acc@5 100.00 ( 99.94)
Epoch: [48][290/391]	Time  0.071 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8611e-01 (1.5390e-01)	Acc@1  95.31 ( 94.61)	Acc@5  99.22 ( 99.94)
Epoch: [48][300/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0987e-01 (1.5344e-01)	Acc@1  96.09 ( 94.65)	Acc@5 100.00 ( 99.95)
Epoch: [48][310/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.7381e-01 (1.5397e-01)	Acc@1  92.97 ( 94.64)	Acc@5 100.00 ( 99.94)
Epoch: [48][320/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9732e-01 (1.5368e-01)	Acc@1  92.97 ( 94.64)	Acc@5  99.22 ( 99.94)
Epoch: [48][330/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8060e-01 (1.5390e-01)	Acc@1  95.31 ( 94.66)	Acc@5 100.00 ( 99.94)
Epoch: [48][340/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.3501e-02 (1.5322e-01)	Acc@1  95.31 ( 94.68)	Acc@5 100.00 ( 99.94)
Epoch: [48][350/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.8629e-02 (1.5262e-01)	Acc@1  98.44 ( 94.72)	Acc@5 100.00 ( 99.94)
Epoch: [48][360/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0207e-01 (1.5184e-01)	Acc@1  96.09 ( 94.75)	Acc@5 100.00 ( 99.94)
Epoch: [48][370/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1945e-01 (1.5190e-01)	Acc@1  96.88 ( 94.74)	Acc@5 100.00 ( 99.94)
Epoch: [48][380/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.0184e-01 (1.5176e-01)	Acc@1  89.84 ( 94.75)	Acc@5  99.22 ( 99.94)
Epoch: [48][390/391]	Time  0.050 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.1369e-01 (1.5217e-01)	Acc@1  96.25 ( 94.72)	Acc@5 100.00 ( 99.94)
## e[48] optimizer.zero_grad (sum) time: 0.4044351577758789
## e[48]       loss.backward (sum) time: 6.982724666595459
## e[48]      optimizer.step (sum) time: 3.3820912837982178
## epoch[48] training(only) time: 25.34125781059265
# Switched to evaluate mode...
Test: [  0/100]	Time  0.197 ( 0.197)	Loss 2.4239e-01 (2.4239e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.028 ( 0.041)	Loss 3.8999e-01 (2.7001e-01)	Acc@1  91.00 ( 90.45)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.027 ( 0.034)	Loss 3.0453e-01 (2.8852e-01)	Acc@1  89.00 ( 90.71)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 2.8064e-01 (3.0249e-01)	Acc@1  91.00 ( 90.68)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 2.7480e-01 (3.0869e-01)	Acc@1  90.00 ( 90.39)	Acc@5  99.00 ( 99.73)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 2.4225e-01 (3.0964e-01)	Acc@1  91.00 ( 90.55)	Acc@5  99.00 ( 99.69)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 2.6553e-01 (3.0694e-01)	Acc@1  95.00 ( 90.61)	Acc@5 100.00 ( 99.70)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 4.8547e-01 (3.0164e-01)	Acc@1  88.00 ( 90.65)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.027 ( 0.028)	Loss 2.4269e-01 (3.0077e-01)	Acc@1  94.00 ( 90.59)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.024 ( 0.028)	Loss 1.9868e-01 (2.9805e-01)	Acc@1  94.00 ( 90.59)	Acc@5 100.00 ( 99.76)
 * Acc@1 90.560 Acc@5 99.750
### epoch[48] execution time: 28.18131923675537
EPOCH 49
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [49][  0/391]	Time  0.247 ( 0.247)	Data  0.181 ( 0.181)	Loss 1.3161e-01 (1.3161e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [49][ 10/391]	Time  0.067 ( 0.081)	Data  0.001 ( 0.017)	Loss 1.2804e-01 (1.3981e-01)	Acc@1  93.75 ( 95.45)	Acc@5 100.00 (100.00)
Epoch: [49][ 20/391]	Time  0.066 ( 0.073)	Data  0.001 ( 0.010)	Loss 1.7659e-01 (1.3376e-01)	Acc@1  94.53 ( 95.35)	Acc@5 100.00 (100.00)
Epoch: [49][ 30/391]	Time  0.066 ( 0.070)	Data  0.001 ( 0.007)	Loss 9.0140e-02 (1.4085e-01)	Acc@1  97.66 ( 95.01)	Acc@5 100.00 ( 99.97)
Epoch: [49][ 40/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.005)	Loss 1.6540e-01 (1.3903e-01)	Acc@1  94.53 ( 95.01)	Acc@5 100.00 ( 99.98)
Epoch: [49][ 50/391]	Time  0.062 ( 0.068)	Data  0.001 ( 0.005)	Loss 1.2795e-01 (1.3816e-01)	Acc@1  95.31 ( 95.08)	Acc@5 100.00 ( 99.98)
Epoch: [49][ 60/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 2.1707e-01 (1.4326e-01)	Acc@1  94.53 ( 94.90)	Acc@5 100.00 ( 99.97)
Epoch: [49][ 70/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.004)	Loss 9.2351e-02 (1.4219e-01)	Acc@1  96.09 ( 94.96)	Acc@5 100.00 ( 99.98)
Epoch: [49][ 80/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.3350e-01 (1.4263e-01)	Acc@1  94.53 ( 94.95)	Acc@5 100.00 ( 99.97)
Epoch: [49][ 90/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.1758e-01 (1.4499e-01)	Acc@1  92.19 ( 94.87)	Acc@5 100.00 ( 99.97)
Epoch: [49][100/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.3921e-01 (1.4688e-01)	Acc@1  94.53 ( 94.83)	Acc@5 100.00 ( 99.95)
Epoch: [49][110/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.6340e-01 (1.4702e-01)	Acc@1  92.97 ( 94.80)	Acc@5 100.00 ( 99.95)
Epoch: [49][120/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.003)	Loss 8.0951e-02 (1.4676e-01)	Acc@1  96.88 ( 94.82)	Acc@5 100.00 ( 99.95)
Epoch: [49][130/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1380e-01 (1.4819e-01)	Acc@1  96.88 ( 94.81)	Acc@5 100.00 ( 99.96)
Epoch: [49][140/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.5136e-01 (1.4766e-01)	Acc@1  92.19 ( 94.86)	Acc@5 100.00 ( 99.96)
Epoch: [49][150/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3401e-01 (1.4848e-01)	Acc@1  94.53 ( 94.83)	Acc@5 100.00 ( 99.95)
Epoch: [49][160/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3862e-01 (1.4826e-01)	Acc@1  95.31 ( 94.86)	Acc@5 100.00 ( 99.95)
Epoch: [49][170/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8986e-01 (1.4776e-01)	Acc@1  94.53 ( 94.89)	Acc@5  99.22 ( 99.94)
Epoch: [49][180/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9540e-01 (1.4937e-01)	Acc@1  92.97 ( 94.82)	Acc@5 100.00 ( 99.94)
Epoch: [49][190/391]	Time  0.064 ( 0.065)	Data  0.002 ( 0.002)	Loss 1.2518e-01 (1.4892e-01)	Acc@1  95.31 ( 94.81)	Acc@5 100.00 ( 99.94)
Epoch: [49][200/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5164e-01 (1.4974e-01)	Acc@1  94.53 ( 94.77)	Acc@5 100.00 ( 99.94)
Epoch: [49][210/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8154e-01 (1.4964e-01)	Acc@1  91.41 ( 94.74)	Acc@5 100.00 ( 99.94)
Epoch: [49][220/391]	Time  0.058 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2678e-01 (1.4950e-01)	Acc@1  97.66 ( 94.75)	Acc@5 100.00 ( 99.94)
Epoch: [49][230/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2935e-01 (1.4879e-01)	Acc@1  93.75 ( 94.78)	Acc@5 100.00 ( 99.95)
Epoch: [49][240/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0323e-01 (1.4913e-01)	Acc@1  95.31 ( 94.76)	Acc@5 100.00 ( 99.94)
Epoch: [49][250/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2487e-01 (1.4932e-01)	Acc@1  95.31 ( 94.75)	Acc@5 100.00 ( 99.95)
Epoch: [49][260/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4160e-01 (1.4875e-01)	Acc@1  94.53 ( 94.76)	Acc@5 100.00 ( 99.95)
Epoch: [49][270/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0177e-01 (1.4960e-01)	Acc@1  93.75 ( 94.74)	Acc@5  99.22 ( 99.95)
Epoch: [49][280/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6236e-01 (1.4892e-01)	Acc@1  94.53 ( 94.76)	Acc@5 100.00 ( 99.95)
Epoch: [49][290/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4567e-01 (1.4884e-01)	Acc@1  94.53 ( 94.77)	Acc@5 100.00 ( 99.95)
Epoch: [49][300/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6668e-01 (1.4960e-01)	Acc@1  94.53 ( 94.74)	Acc@5 100.00 ( 99.95)
Epoch: [49][310/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1785e-01 (1.4940e-01)	Acc@1  95.31 ( 94.73)	Acc@5 100.00 ( 99.95)
Epoch: [49][320/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5045e-01 (1.4967e-01)	Acc@1  96.09 ( 94.73)	Acc@5 100.00 ( 99.95)
Epoch: [49][330/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1116e-01 (1.5027e-01)	Acc@1  95.31 ( 94.74)	Acc@5 100.00 ( 99.95)
Epoch: [49][340/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5267e-01 (1.4989e-01)	Acc@1  95.31 ( 94.75)	Acc@5  99.22 ( 99.95)
Epoch: [49][350/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.8546e-01 (1.5040e-01)	Acc@1  92.97 ( 94.75)	Acc@5 100.00 ( 99.94)
Epoch: [49][360/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8127e-01 (1.5028e-01)	Acc@1  92.97 ( 94.74)	Acc@5 100.00 ( 99.94)
Epoch: [49][370/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9106e-01 (1.5041e-01)	Acc@1  92.97 ( 94.73)	Acc@5 100.00 ( 99.94)
Epoch: [49][380/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3025e-01 (1.5062e-01)	Acc@1  92.97 ( 94.72)	Acc@5  99.22 ( 99.94)
Epoch: [49][390/391]	Time  0.047 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.5800e-02 (1.4979e-01)	Acc@1  98.75 ( 94.76)	Acc@5 100.00 ( 99.94)
## e[49] optimizer.zero_grad (sum) time: 0.3972458839416504
## e[49]       loss.backward (sum) time: 6.974044561386108
## e[49]      optimizer.step (sum) time: 3.4007163047790527
## epoch[49] training(only) time: 25.380762338638306
# Switched to evaluate mode...
Test: [  0/100]	Time  0.197 ( 0.197)	Loss 1.9604e-01 (1.9604e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.041)	Loss 3.8163e-01 (2.5736e-01)	Acc@1  91.00 ( 91.00)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.027 ( 0.034)	Loss 3.1810e-01 (2.8044e-01)	Acc@1  87.00 ( 90.90)	Acc@5  99.00 ( 99.62)
Test: [ 30/100]	Time  0.027 ( 0.031)	Loss 2.8264e-01 (2.9615e-01)	Acc@1  90.00 ( 90.68)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.032 ( 0.030)	Loss 2.3921e-01 (2.9969e-01)	Acc@1  91.00 ( 90.61)	Acc@5  99.00 ( 99.63)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 2.4771e-01 (3.0315e-01)	Acc@1  91.00 ( 90.61)	Acc@5  99.00 ( 99.65)
Test: [ 60/100]	Time  0.028 ( 0.029)	Loss 2.6962e-01 (2.9935e-01)	Acc@1  92.00 ( 90.72)	Acc@5 100.00 ( 99.67)
Test: [ 70/100]	Time  0.028 ( 0.028)	Loss 4.6059e-01 (2.9597e-01)	Acc@1  87.00 ( 90.73)	Acc@5 100.00 ( 99.72)
Test: [ 80/100]	Time  0.028 ( 0.028)	Loss 2.3366e-01 (2.9662e-01)	Acc@1  92.00 ( 90.70)	Acc@5 100.00 ( 99.73)
Test: [ 90/100]	Time  0.024 ( 0.028)	Loss 2.5455e-01 (2.9403e-01)	Acc@1  93.00 ( 90.70)	Acc@5 100.00 ( 99.74)
 * Acc@1 90.680 Acc@5 99.730
### epoch[49] execution time: 28.294551610946655
EPOCH 50
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [50][  0/391]	Time  0.247 ( 0.247)	Data  0.181 ( 0.181)	Loss 1.1017e-01 (1.1017e-01)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [50][ 10/391]	Time  0.067 ( 0.081)	Data  0.001 ( 0.018)	Loss 9.5663e-02 (1.2902e-01)	Acc@1  97.66 ( 95.74)	Acc@5 100.00 ( 99.93)
Epoch: [50][ 20/391]	Time  0.064 ( 0.073)	Data  0.001 ( 0.010)	Loss 2.2900e-01 (1.4116e-01)	Acc@1  91.41 ( 95.20)	Acc@5 100.00 ( 99.93)
Epoch: [50][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.007)	Loss 8.2551e-02 (1.3783e-01)	Acc@1  97.66 ( 95.36)	Acc@5 100.00 ( 99.95)
Epoch: [50][ 40/391]	Time  0.062 ( 0.069)	Data  0.001 ( 0.005)	Loss 1.5020e-01 (1.3701e-01)	Acc@1  92.97 ( 95.29)	Acc@5 100.00 ( 99.92)
Epoch: [50][ 50/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.005)	Loss 1.5515e-01 (1.4445e-01)	Acc@1  94.53 ( 94.99)	Acc@5 100.00 ( 99.94)
Epoch: [50][ 60/391]	Time  0.061 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.4807e-01 (1.4628e-01)	Acc@1  94.53 ( 94.93)	Acc@5 100.00 ( 99.94)
Epoch: [50][ 70/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.0112e-01 (1.4679e-01)	Acc@1  97.66 ( 94.99)	Acc@5 100.00 ( 99.91)
Epoch: [50][ 80/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.9470e-01 (1.4720e-01)	Acc@1  93.75 ( 95.00)	Acc@5 100.00 ( 99.91)
Epoch: [50][ 90/391]	Time  0.058 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.6252e-01 (1.4696e-01)	Acc@1  93.75 ( 94.98)	Acc@5 100.00 ( 99.92)
Epoch: [50][100/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.0298e-01 (1.4597e-01)	Acc@1  93.75 ( 95.02)	Acc@5 100.00 ( 99.93)
Epoch: [50][110/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.3050e-01 (1.4484e-01)	Acc@1  95.31 ( 95.03)	Acc@5 100.00 ( 99.94)
Epoch: [50][120/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.6720e-01 (1.4461e-01)	Acc@1  93.75 ( 95.00)	Acc@5 100.00 ( 99.94)
Epoch: [50][130/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.6332e-02 (1.4274e-01)	Acc@1  99.22 ( 95.09)	Acc@5 100.00 ( 99.93)
Epoch: [50][140/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.8252e-02 (1.4086e-01)	Acc@1  96.88 ( 95.13)	Acc@5 100.00 ( 99.94)
Epoch: [50][150/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6910e-01 (1.4143e-01)	Acc@1  96.09 ( 95.13)	Acc@5 100.00 ( 99.94)
Epoch: [50][160/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6521e-01 (1.4191e-01)	Acc@1  93.75 ( 95.12)	Acc@5 100.00 ( 99.94)
Epoch: [50][170/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8110e-01 (1.4139e-01)	Acc@1  92.97 ( 95.13)	Acc@5 100.00 ( 99.94)
Epoch: [50][180/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5050e-01 (1.4124e-01)	Acc@1  94.53 ( 95.13)	Acc@5 100.00 ( 99.94)
Epoch: [50][190/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2714e-01 (1.4263e-01)	Acc@1  93.75 ( 95.07)	Acc@5  99.22 ( 99.94)
Epoch: [50][200/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.6599e-02 (1.4212e-01)	Acc@1  97.66 ( 95.10)	Acc@5 100.00 ( 99.94)
Epoch: [50][210/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.2184e-02 (1.4112e-01)	Acc@1  97.66 ( 95.14)	Acc@5 100.00 ( 99.94)
Epoch: [50][220/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9745e-01 (1.4169e-01)	Acc@1  92.19 ( 95.07)	Acc@5 100.00 ( 99.94)
Epoch: [50][230/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1627e-01 (1.4202e-01)	Acc@1  96.09 ( 95.08)	Acc@5 100.00 ( 99.94)
Epoch: [50][240/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0288e-01 (1.4255e-01)	Acc@1  96.09 ( 95.04)	Acc@5 100.00 ( 99.94)
Epoch: [50][250/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0539e-01 (1.4186e-01)	Acc@1  97.66 ( 95.05)	Acc@5 100.00 ( 99.94)
Epoch: [50][260/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1616e-01 (1.4208e-01)	Acc@1  95.31 ( 95.04)	Acc@5 100.00 ( 99.95)
Epoch: [50][270/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2422e-01 (1.4308e-01)	Acc@1  92.97 ( 95.00)	Acc@5 100.00 ( 99.94)
Epoch: [50][280/391]	Time  0.058 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3451e-01 (1.4320e-01)	Acc@1  94.53 ( 95.00)	Acc@5 100.00 ( 99.94)
Epoch: [50][290/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2997e-01 (1.4337e-01)	Acc@1  95.31 ( 94.98)	Acc@5 100.00 ( 99.94)
Epoch: [50][300/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4721e-01 (1.4349e-01)	Acc@1  94.53 ( 94.98)	Acc@5 100.00 ( 99.94)
Epoch: [50][310/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3107e-01 (1.4341e-01)	Acc@1  92.97 ( 95.00)	Acc@5 100.00 ( 99.94)
Epoch: [50][320/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3462e-01 (1.4294e-01)	Acc@1  95.31 ( 95.03)	Acc@5 100.00 ( 99.94)
Epoch: [50][330/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.7584e-01 (1.4364e-01)	Acc@1  92.19 ( 95.00)	Acc@5 100.00 ( 99.94)
Epoch: [50][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.6810e-02 (1.4498e-01)	Acc@1  97.66 ( 94.95)	Acc@5 100.00 ( 99.94)
Epoch: [50][350/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.3089e-02 (1.4486e-01)	Acc@1  98.44 ( 94.95)	Acc@5 100.00 ( 99.94)
Epoch: [50][360/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9306e-01 (1.4471e-01)	Acc@1  94.53 ( 94.94)	Acc@5 100.00 ( 99.94)
Epoch: [50][370/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0785e-01 (1.4463e-01)	Acc@1  94.53 ( 94.91)	Acc@5 100.00 ( 99.94)
Epoch: [50][380/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2888e-01 (1.4483e-01)	Acc@1  96.09 ( 94.92)	Acc@5 100.00 ( 99.94)
Epoch: [50][390/391]	Time  0.045 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.4286e-01 (1.4438e-01)	Acc@1  93.75 ( 94.95)	Acc@5 100.00 ( 99.94)
## e[50] optimizer.zero_grad (sum) time: 0.4054238796234131
## e[50]       loss.backward (sum) time: 6.949074745178223
## e[50]      optimizer.step (sum) time: 3.3525390625
## epoch[50] training(only) time: 25.30157732963562
# Switched to evaluate mode...
Test: [  0/100]	Time  0.191 ( 0.191)	Loss 2.3083e-01 (2.3083e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.043)	Loss 3.5240e-01 (2.7994e-01)	Acc@1  91.00 ( 90.55)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.027 ( 0.035)	Loss 3.1820e-01 (2.9809e-01)	Acc@1  87.00 ( 90.57)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.027 ( 0.032)	Loss 3.4456e-01 (3.1041e-01)	Acc@1  88.00 ( 90.68)	Acc@5  99.00 ( 99.61)
Test: [ 40/100]	Time  0.027 ( 0.031)	Loss 2.5062e-01 (3.1655e-01)	Acc@1  92.00 ( 90.49)	Acc@5  99.00 ( 99.63)
Test: [ 50/100]	Time  0.032 ( 0.030)	Loss 2.0914e-01 (3.1555e-01)	Acc@1  92.00 ( 90.49)	Acc@5  99.00 ( 99.65)
Test: [ 60/100]	Time  0.027 ( 0.029)	Loss 3.3099e-01 (3.1090e-01)	Acc@1  91.00 ( 90.56)	Acc@5 100.00 ( 99.69)
Test: [ 70/100]	Time  0.029 ( 0.029)	Loss 4.6581e-01 (3.0794e-01)	Acc@1  87.00 ( 90.63)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.029 ( 0.029)	Loss 1.9884e-01 (3.0593e-01)	Acc@1  92.00 ( 90.63)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.027 ( 0.028)	Loss 2.7178e-01 (3.0150e-01)	Acc@1  92.00 ( 90.69)	Acc@5 100.00 ( 99.76)
 * Acc@1 90.670 Acc@5 99.760
### epoch[50] execution time: 28.24158215522766
EPOCH 51
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [51][  0/391]	Time  0.255 ( 0.255)	Data  0.190 ( 0.190)	Loss 1.4261e-01 (1.4261e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
Epoch: [51][ 10/391]	Time  0.062 ( 0.080)	Data  0.001 ( 0.018)	Loss 1.0464e-01 (1.1697e-01)	Acc@1  96.09 ( 96.02)	Acc@5 100.00 ( 99.86)
Epoch: [51][ 20/391]	Time  0.064 ( 0.071)	Data  0.001 ( 0.010)	Loss 9.7182e-02 (1.2579e-01)	Acc@1  97.66 ( 95.61)	Acc@5 100.00 ( 99.93)
Epoch: [51][ 30/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.007)	Loss 9.8367e-02 (1.3303e-01)	Acc@1  96.09 ( 95.26)	Acc@5 100.00 ( 99.92)
Epoch: [51][ 40/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.006)	Loss 6.3380e-02 (1.3033e-01)	Acc@1  98.44 ( 95.39)	Acc@5 100.00 ( 99.94)
Epoch: [51][ 50/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.005)	Loss 1.6534e-01 (1.2869e-01)	Acc@1  93.75 ( 95.45)	Acc@5 100.00 ( 99.92)
Epoch: [51][ 60/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.004)	Loss 1.0715e-01 (1.2626e-01)	Acc@1  96.09 ( 95.49)	Acc@5 100.00 ( 99.94)
Epoch: [51][ 70/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.004)	Loss 1.7388e-01 (1.2943e-01)	Acc@1  95.31 ( 95.38)	Acc@5 100.00 ( 99.93)
Epoch: [51][ 80/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.4210e-01 (1.3042e-01)	Acc@1  94.53 ( 95.34)	Acc@5 100.00 ( 99.93)
Epoch: [51][ 90/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.9644e-01 (1.3283e-01)	Acc@1  92.19 ( 95.24)	Acc@5 100.00 ( 99.92)
Epoch: [51][100/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.003)	Loss 2.1231e-01 (1.3440e-01)	Acc@1  90.62 ( 95.19)	Acc@5 100.00 ( 99.92)
Epoch: [51][110/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.003)	Loss 2.4545e-01 (1.3590e-01)	Acc@1  92.97 ( 95.16)	Acc@5 100.00 ( 99.93)
Epoch: [51][120/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 2.3901e-01 (1.3642e-01)	Acc@1  91.41 ( 95.14)	Acc@5 100.00 ( 99.94)
Epoch: [51][130/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.3375e-01 (1.3850e-01)	Acc@1  96.09 ( 95.06)	Acc@5 100.00 ( 99.94)
Epoch: [51][140/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5618e-01 (1.3898e-01)	Acc@1  94.53 ( 95.05)	Acc@5  99.22 ( 99.93)
Epoch: [51][150/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3098e-01 (1.3787e-01)	Acc@1  96.09 ( 95.11)	Acc@5 100.00 ( 99.93)
Epoch: [51][160/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6710e-01 (1.3876e-01)	Acc@1  95.31 ( 95.10)	Acc@5 100.00 ( 99.93)
Epoch: [51][170/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2053e-01 (1.3976e-01)	Acc@1  95.31 ( 95.04)	Acc@5 100.00 ( 99.93)
Epoch: [51][180/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4775e-01 (1.4073e-01)	Acc@1  92.97 ( 95.01)	Acc@5 100.00 ( 99.93)
Epoch: [51][190/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.9780e-02 (1.4041e-01)	Acc@1  97.66 ( 95.02)	Acc@5 100.00 ( 99.93)
Epoch: [51][200/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.8421e-02 (1.4070e-01)	Acc@1  96.09 ( 95.02)	Acc@5 100.00 ( 99.93)
Epoch: [51][210/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6565e-01 (1.4304e-01)	Acc@1  96.09 ( 94.98)	Acc@5 100.00 ( 99.94)
Epoch: [51][220/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.3359e-02 (1.4311e-01)	Acc@1  97.66 ( 94.98)	Acc@5 100.00 ( 99.94)
Epoch: [51][230/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7750e-01 (1.4325e-01)	Acc@1  93.75 ( 94.97)	Acc@5  99.22 ( 99.94)
Epoch: [51][240/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1720e-01 (1.4328e-01)	Acc@1  97.66 ( 94.98)	Acc@5 100.00 ( 99.94)
Epoch: [51][250/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1875e-01 (1.4382e-01)	Acc@1  96.09 ( 94.97)	Acc@5 100.00 ( 99.94)
Epoch: [51][260/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8150e-01 (1.4304e-01)	Acc@1  92.19 ( 94.98)	Acc@5 100.00 ( 99.94)
Epoch: [51][270/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4021e-01 (1.4341e-01)	Acc@1  95.31 ( 94.97)	Acc@5 100.00 ( 99.94)
Epoch: [51][280/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.3916e-02 (1.4308e-01)	Acc@1  95.31 ( 94.98)	Acc@5 100.00 ( 99.94)
Epoch: [51][290/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.6636e-02 (1.4230e-01)	Acc@1  96.09 ( 94.99)	Acc@5 100.00 ( 99.95)
Epoch: [51][300/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5218e-01 (1.4302e-01)	Acc@1  93.75 ( 94.98)	Acc@5 100.00 ( 99.94)
Epoch: [51][310/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.5057e-02 (1.4291e-01)	Acc@1  96.09 ( 94.99)	Acc@5 100.00 ( 99.94)
Epoch: [51][320/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8918e-01 (1.4328e-01)	Acc@1  90.62 ( 94.94)	Acc@5 100.00 ( 99.95)
Epoch: [51][330/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2440e-01 (1.4302e-01)	Acc@1  94.53 ( 94.96)	Acc@5  99.22 ( 99.95)
Epoch: [51][340/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4873e-01 (1.4271e-01)	Acc@1  95.31 ( 94.97)	Acc@5 100.00 ( 99.95)
Epoch: [51][350/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2171e-01 (1.4295e-01)	Acc@1  94.53 ( 94.94)	Acc@5 100.00 ( 99.95)
Epoch: [51][360/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.4702e-02 (1.4342e-01)	Acc@1  97.66 ( 94.91)	Acc@5 100.00 ( 99.95)
Epoch: [51][370/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0055e-01 (1.4341e-01)	Acc@1  96.88 ( 94.91)	Acc@5 100.00 ( 99.95)
Epoch: [51][380/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3502e-01 (1.4339e-01)	Acc@1  95.31 ( 94.92)	Acc@5 100.00 ( 99.95)
Epoch: [51][390/391]	Time  0.047 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.6348e-01 (1.4285e-01)	Acc@1  95.00 ( 94.93)	Acc@5 100.00 ( 99.95)
## e[51] optimizer.zero_grad (sum) time: 0.4024331569671631
## e[51]       loss.backward (sum) time: 6.9531028270721436
## e[51]      optimizer.step (sum) time: 3.380779504776001
## epoch[51] training(only) time: 25.359010219573975
# Switched to evaluate mode...
Test: [  0/100]	Time  0.193 ( 0.193)	Loss 2.7158e-01 (2.7158e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.041)	Loss 3.3903e-01 (2.7540e-01)	Acc@1  89.00 ( 91.18)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.028 ( 0.034)	Loss 3.4941e-01 (2.9727e-01)	Acc@1  89.00 ( 90.86)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.024 ( 0.032)	Loss 3.3474e-01 (3.1515e-01)	Acc@1  88.00 ( 90.74)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 2.5462e-01 (3.2170e-01)	Acc@1  90.00 ( 90.54)	Acc@5  99.00 ( 99.68)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 2.9759e-01 (3.2017e-01)	Acc@1  91.00 ( 90.63)	Acc@5  99.00 ( 99.69)
Test: [ 60/100]	Time  0.028 ( 0.029)	Loss 3.1843e-01 (3.1597e-01)	Acc@1  91.00 ( 90.59)	Acc@5 100.00 ( 99.72)
Test: [ 70/100]	Time  0.026 ( 0.028)	Loss 4.6462e-01 (3.1111e-01)	Acc@1  86.00 ( 90.61)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.027 ( 0.028)	Loss 2.4403e-01 (3.0962e-01)	Acc@1  91.00 ( 90.56)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.027 ( 0.028)	Loss 2.3545e-01 (3.0741e-01)	Acc@1  92.00 ( 90.59)	Acc@5 100.00 ( 99.78)
 * Acc@1 90.520 Acc@5 99.770
### epoch[51] execution time: 28.262181282043457
EPOCH 52
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [52][  0/391]	Time  0.253 ( 0.253)	Data  0.190 ( 0.190)	Loss 1.4411e-01 (1.4411e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [52][ 10/391]	Time  0.064 ( 0.082)	Data  0.001 ( 0.018)	Loss 8.2212e-02 (1.4397e-01)	Acc@1  96.88 ( 95.17)	Acc@5 100.00 ( 99.93)
Epoch: [52][ 20/391]	Time  0.064 ( 0.073)	Data  0.001 ( 0.010)	Loss 1.2308e-01 (1.3800e-01)	Acc@1  96.88 ( 95.13)	Acc@5 100.00 ( 99.96)
Epoch: [52][ 30/391]	Time  0.063 ( 0.070)	Data  0.001 ( 0.007)	Loss 1.8654e-01 (1.4007e-01)	Acc@1  92.97 ( 95.24)	Acc@5  99.22 ( 99.95)
Epoch: [52][ 40/391]	Time  0.059 ( 0.069)	Data  0.001 ( 0.006)	Loss 1.4425e-01 (1.3840e-01)	Acc@1  96.09 ( 95.12)	Acc@5 100.00 ( 99.96)
Epoch: [52][ 50/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.005)	Loss 2.1489e-01 (1.3692e-01)	Acc@1  93.75 ( 95.28)	Acc@5 100.00 ( 99.97)
Epoch: [52][ 60/391]	Time  0.060 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.5210e-01 (1.3635e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 ( 99.97)
Epoch: [52][ 70/391]	Time  0.072 ( 0.067)	Data  0.001 ( 0.004)	Loss 9.2042e-02 (1.3568e-01)	Acc@1  96.88 ( 95.27)	Acc@5 100.00 ( 99.97)
Epoch: [52][ 80/391]	Time  0.059 ( 0.066)	Data  0.001 ( 0.003)	Loss 5.3071e-02 (1.3679e-01)	Acc@1  97.66 ( 95.22)	Acc@5 100.00 ( 99.96)
Epoch: [52][ 90/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 9.6697e-02 (1.3825e-01)	Acc@1  95.31 ( 95.14)	Acc@5 100.00 ( 99.96)
Epoch: [52][100/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.3311e-01 (1.3846e-01)	Acc@1  95.31 ( 95.09)	Acc@5 100.00 ( 99.95)
Epoch: [52][110/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.003)	Loss 8.6948e-02 (1.3758e-01)	Acc@1  96.09 ( 95.11)	Acc@5 100.00 ( 99.96)
Epoch: [52][120/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.003)	Loss 9.6843e-02 (1.3867e-01)	Acc@1  96.09 ( 95.02)	Acc@5 100.00 ( 99.96)
Epoch: [52][130/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 8.4545e-02 (1.3762e-01)	Acc@1  96.09 ( 95.06)	Acc@5 100.00 ( 99.96)
Epoch: [52][140/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1151e-01 (1.3814e-01)	Acc@1  92.97 ( 95.05)	Acc@5 100.00 ( 99.96)
Epoch: [52][150/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2608e-01 (1.3752e-01)	Acc@1  95.31 ( 95.08)	Acc@5 100.00 ( 99.96)
Epoch: [52][160/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1012e-01 (1.3766e-01)	Acc@1  94.53 ( 95.08)	Acc@5 100.00 ( 99.96)
Epoch: [52][170/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6982e-01 (1.3880e-01)	Acc@1  92.97 ( 95.02)	Acc@5 100.00 ( 99.96)
Epoch: [52][180/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0587e-01 (1.3793e-01)	Acc@1  96.09 ( 95.02)	Acc@5 100.00 ( 99.97)
Epoch: [52][190/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1357e-01 (1.3846e-01)	Acc@1  94.53 ( 95.04)	Acc@5 100.00 ( 99.96)
Epoch: [52][200/391]	Time  0.070 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0722e-01 (1.3881e-01)	Acc@1  92.19 ( 95.03)	Acc@5 100.00 ( 99.97)
Epoch: [52][210/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2163e-01 (1.3895e-01)	Acc@1  92.97 ( 95.03)	Acc@5  99.22 ( 99.96)
Epoch: [52][220/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3141e-01 (1.3951e-01)	Acc@1  95.31 ( 95.02)	Acc@5 100.00 ( 99.96)
Epoch: [52][230/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.0582e-01 (1.3884e-01)	Acc@1  96.09 ( 95.04)	Acc@5 100.00 ( 99.96)
Epoch: [52][240/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.1132e-01 (1.4015e-01)	Acc@1  96.09 ( 94.99)	Acc@5 100.00 ( 99.96)
Epoch: [52][250/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.4204e-01 (1.4086e-01)	Acc@1  92.19 ( 94.98)	Acc@5 100.00 ( 99.96)
Epoch: [52][260/391]	Time  0.060 ( 0.064)	Data  0.001 ( 0.002)	Loss 8.3563e-02 (1.4059e-01)	Acc@1  96.88 ( 94.98)	Acc@5 100.00 ( 99.96)
Epoch: [52][270/391]	Time  0.067 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.3420e-01 (1.4141e-01)	Acc@1  89.06 ( 94.96)	Acc@5 100.00 ( 99.96)
Epoch: [52][280/391]	Time  0.060 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.9613e-02 (1.4115e-01)	Acc@1  97.66 ( 94.99)	Acc@5 100.00 ( 99.96)
Epoch: [52][290/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.0033e-01 (1.4090e-01)	Acc@1  92.19 ( 94.99)	Acc@5 100.00 ( 99.96)
Epoch: [52][300/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.2274e-01 (1.4054e-01)	Acc@1  96.88 ( 95.00)	Acc@5 100.00 ( 99.96)
Epoch: [52][310/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.6053e-01 (1.4015e-01)	Acc@1  92.19 ( 95.03)	Acc@5 100.00 ( 99.96)
Epoch: [52][320/391]	Time  0.069 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.3690e-01 (1.4025e-01)	Acc@1  96.09 ( 95.03)	Acc@5 100.00 ( 99.96)
Epoch: [52][330/391]	Time  0.075 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.6042e-01 (1.4036e-01)	Acc@1  93.75 ( 95.03)	Acc@5 100.00 ( 99.96)
Epoch: [52][340/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.3131e-01 (1.4015e-01)	Acc@1  94.53 ( 95.04)	Acc@5 100.00 ( 99.96)
Epoch: [52][350/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.2343e-01 (1.4023e-01)	Acc@1  95.31 ( 95.05)	Acc@5 100.00 ( 99.96)
Epoch: [52][360/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 9.2552e-02 (1.4009e-01)	Acc@1  96.09 ( 95.05)	Acc@5 100.00 ( 99.96)
Epoch: [52][370/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.4762e-01 (1.4002e-01)	Acc@1  92.19 ( 95.04)	Acc@5 100.00 ( 99.96)
Epoch: [52][380/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.3051e-01 (1.4066e-01)	Acc@1  90.62 ( 95.02)	Acc@5 100.00 ( 99.96)
Epoch: [52][390/391]	Time  0.048 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.3934e-01 (1.4104e-01)	Acc@1  93.75 ( 95.00)	Acc@5 100.00 ( 99.96)
## e[52] optimizer.zero_grad (sum) time: 0.3940699100494385
## e[52]       loss.backward (sum) time: 6.9478137493133545
## e[52]      optimizer.step (sum) time: 3.31223464012146
## epoch[52] training(only) time: 25.250349760055542
# Switched to evaluate mode...
Test: [  0/100]	Time  0.190 ( 0.190)	Loss 2.9444e-01 (2.9444e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.041)	Loss 3.8742e-01 (2.9189e-01)	Acc@1  89.00 ( 90.00)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.024 ( 0.034)	Loss 3.5654e-01 (3.1074e-01)	Acc@1  90.00 ( 90.33)	Acc@5  99.00 ( 99.71)
Test: [ 30/100]	Time  0.028 ( 0.031)	Loss 3.4470e-01 (3.2452e-01)	Acc@1  90.00 ( 90.45)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.028 ( 0.030)	Loss 2.5883e-01 (3.2845e-01)	Acc@1  91.00 ( 90.22)	Acc@5  99.00 ( 99.71)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 2.8019e-01 (3.3171e-01)	Acc@1  92.00 ( 90.12)	Acc@5  99.00 ( 99.67)
Test: [ 60/100]	Time  0.027 ( 0.029)	Loss 2.4826e-01 (3.2637e-01)	Acc@1  94.00 ( 90.23)	Acc@5 100.00 ( 99.70)
Test: [ 70/100]	Time  0.027 ( 0.028)	Loss 4.8264e-01 (3.2348e-01)	Acc@1  87.00 ( 90.23)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.024 ( 0.028)	Loss 1.7766e-01 (3.2359e-01)	Acc@1  94.00 ( 90.15)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.026 ( 0.028)	Loss 3.2492e-01 (3.1985e-01)	Acc@1  91.00 ( 90.19)	Acc@5 100.00 ( 99.77)
 * Acc@1 90.270 Acc@5 99.760
### epoch[52] execution time: 28.138413667678833
EPOCH 53
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [53][  0/391]	Time  0.253 ( 0.253)	Data  0.184 ( 0.184)	Loss 9.0648e-02 (9.0648e-02)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [53][ 10/391]	Time  0.063 ( 0.079)	Data  0.001 ( 0.018)	Loss 2.3844e-01 (1.4360e-01)	Acc@1  91.41 ( 94.89)	Acc@5 100.00 ( 99.93)
Epoch: [53][ 20/391]	Time  0.064 ( 0.072)	Data  0.001 ( 0.010)	Loss 2.1546e-01 (1.4016e-01)	Acc@1  92.19 ( 95.13)	Acc@5  99.22 ( 99.85)
Epoch: [53][ 30/391]	Time  0.061 ( 0.069)	Data  0.001 ( 0.007)	Loss 1.4239e-01 (1.4335e-01)	Acc@1  95.31 ( 95.11)	Acc@5 100.00 ( 99.85)
Epoch: [53][ 40/391]	Time  0.060 ( 0.068)	Data  0.001 ( 0.006)	Loss 6.6530e-02 (1.4692e-01)	Acc@1  97.66 ( 94.86)	Acc@5 100.00 ( 99.87)
Epoch: [53][ 50/391]	Time  0.060 ( 0.067)	Data  0.001 ( 0.005)	Loss 1.1714e-01 (1.4482e-01)	Acc@1  95.31 ( 94.75)	Acc@5 100.00 ( 99.89)
Epoch: [53][ 60/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.004)	Loss 1.3537e-01 (1.4272e-01)	Acc@1  96.88 ( 94.89)	Acc@5 100.00 ( 99.90)
Epoch: [53][ 70/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.004)	Loss 8.7010e-02 (1.3933e-01)	Acc@1  97.66 ( 95.06)	Acc@5 100.00 ( 99.91)
Epoch: [53][ 80/391]	Time  0.060 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.4783e-01 (1.4135e-01)	Acc@1  95.31 ( 94.99)	Acc@5  99.22 ( 99.90)
Epoch: [53][ 90/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.4198e-01 (1.4049e-01)	Acc@1  93.75 ( 94.99)	Acc@5 100.00 ( 99.91)
Epoch: [53][100/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.6588e-01 (1.4085e-01)	Acc@1  96.88 ( 95.06)	Acc@5 100.00 ( 99.90)
Epoch: [53][110/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.8786e-01 (1.4013e-01)	Acc@1  94.53 ( 95.08)	Acc@5 100.00 ( 99.91)
Epoch: [53][120/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.2722e-01 (1.3806e-01)	Acc@1  94.53 ( 95.16)	Acc@5 100.00 ( 99.92)
Epoch: [53][130/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.6773e-01 (1.3792e-01)	Acc@1  92.97 ( 95.13)	Acc@5 100.00 ( 99.92)
Epoch: [53][140/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2599e-01 (1.3770e-01)	Acc@1  96.88 ( 95.12)	Acc@5 100.00 ( 99.92)
Epoch: [53][150/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5230e-01 (1.3817e-01)	Acc@1  95.31 ( 95.13)	Acc@5 100.00 ( 99.92)
Epoch: [53][160/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5722e-01 (1.3860e-01)	Acc@1  94.53 ( 95.12)	Acc@5  99.22 ( 99.92)
Epoch: [53][170/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4716e-01 (1.3837e-01)	Acc@1  93.75 ( 95.12)	Acc@5 100.00 ( 99.93)
Epoch: [53][180/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3147e-01 (1.3762e-01)	Acc@1  92.97 ( 95.15)	Acc@5 100.00 ( 99.93)
Epoch: [53][190/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6219e-01 (1.3689e-01)	Acc@1  95.31 ( 95.17)	Acc@5 100.00 ( 99.93)
Epoch: [53][200/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3232e-01 (1.3670e-01)	Acc@1  96.88 ( 95.17)	Acc@5  99.22 ( 99.93)
Epoch: [53][210/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.4158e-02 (1.3636e-01)	Acc@1  97.66 ( 95.21)	Acc@5 100.00 ( 99.93)
Epoch: [53][220/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2446e-01 (1.3634e-01)	Acc@1  95.31 ( 95.19)	Acc@5 100.00 ( 99.93)
Epoch: [53][230/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4308e-01 (1.3614e-01)	Acc@1  94.53 ( 95.19)	Acc@5 100.00 ( 99.93)
Epoch: [53][240/391]	Time  0.058 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0316e-01 (1.3545e-01)	Acc@1  97.66 ( 95.21)	Acc@5 100.00 ( 99.94)
Epoch: [53][250/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0128e-01 (1.3572e-01)	Acc@1  96.88 ( 95.20)	Acc@5 100.00 ( 99.93)
Epoch: [53][260/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7794e-01 (1.3611e-01)	Acc@1  95.31 ( 95.18)	Acc@5 100.00 ( 99.93)
Epoch: [53][270/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1739e-01 (1.3620e-01)	Acc@1  94.53 ( 95.18)	Acc@5 100.00 ( 99.93)
Epoch: [53][280/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3720e-01 (1.3564e-01)	Acc@1  95.31 ( 95.19)	Acc@5 100.00 ( 99.94)
Epoch: [53][290/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9938e-01 (1.3641e-01)	Acc@1  93.75 ( 95.17)	Acc@5 100.00 ( 99.94)
Epoch: [53][300/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.1773e-01 (1.3610e-01)	Acc@1  97.66 ( 95.18)	Acc@5 100.00 ( 99.94)
Epoch: [53][310/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.2424e-01 (1.3665e-01)	Acc@1  95.31 ( 95.17)	Acc@5 100.00 ( 99.93)
Epoch: [53][320/391]	Time  0.066 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.2780e-01 (1.3679e-01)	Acc@1  96.88 ( 95.18)	Acc@5 100.00 ( 99.93)
Epoch: [53][330/391]	Time  0.066 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.2592e-01 (1.3643e-01)	Acc@1  92.97 ( 95.20)	Acc@5 100.00 ( 99.94)
Epoch: [53][340/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.1119e-01 (1.3676e-01)	Acc@1  92.19 ( 95.20)	Acc@5 100.00 ( 99.94)
Epoch: [53][350/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.2568e-02 (1.3729e-01)	Acc@1  96.88 ( 95.17)	Acc@5 100.00 ( 99.93)
Epoch: [53][360/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.9134e-02 (1.3758e-01)	Acc@1  97.66 ( 95.16)	Acc@5 100.00 ( 99.93)
Epoch: [53][370/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.3771e-01 (1.3748e-01)	Acc@1  95.31 ( 95.17)	Acc@5 100.00 ( 99.93)
Epoch: [53][380/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.4080e-01 (1.3801e-01)	Acc@1  96.09 ( 95.15)	Acc@5 100.00 ( 99.93)
Epoch: [53][390/391]	Time  0.047 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.3119e-01 (1.3788e-01)	Acc@1  96.25 ( 95.17)	Acc@5 100.00 ( 99.94)
## e[53] optimizer.zero_grad (sum) time: 0.40265750885009766
## e[53]       loss.backward (sum) time: 6.92863392829895
## e[53]      optimizer.step (sum) time: 3.3292386531829834
## epoch[53] training(only) time: 25.225139379501343
# Switched to evaluate mode...
Test: [  0/100]	Time  0.198 ( 0.198)	Loss 2.9645e-01 (2.9645e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.027 ( 0.042)	Loss 4.1695e-01 (2.7125e-01)	Acc@1  86.00 ( 90.73)	Acc@5  99.00 ( 99.45)
Test: [ 20/100]	Time  0.024 ( 0.035)	Loss 3.2358e-01 (2.8861e-01)	Acc@1  88.00 ( 90.95)	Acc@5  99.00 ( 99.57)
Test: [ 30/100]	Time  0.027 ( 0.032)	Loss 3.6215e-01 (3.0760e-01)	Acc@1  89.00 ( 90.61)	Acc@5 100.00 ( 99.61)
Test: [ 40/100]	Time  0.030 ( 0.030)	Loss 2.3865e-01 (3.1210e-01)	Acc@1  90.00 ( 90.61)	Acc@5  99.00 ( 99.61)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 2.6429e-01 (3.1387e-01)	Acc@1  92.00 ( 90.73)	Acc@5  99.00 ( 99.59)
Test: [ 60/100]	Time  0.027 ( 0.028)	Loss 3.4701e-01 (3.1400e-01)	Acc@1  92.00 ( 90.75)	Acc@5 100.00 ( 99.66)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 4.6914e-01 (3.1132e-01)	Acc@1  87.00 ( 90.66)	Acc@5 100.00 ( 99.70)
Test: [ 80/100]	Time  0.027 ( 0.028)	Loss 2.2357e-01 (3.1090e-01)	Acc@1  93.00 ( 90.57)	Acc@5 100.00 ( 99.72)
Test: [ 90/100]	Time  0.024 ( 0.028)	Loss 2.9160e-01 (3.0744e-01)	Acc@1  92.00 ( 90.57)	Acc@5 100.00 ( 99.73)
 * Acc@1 90.610 Acc@5 99.740
### epoch[53] execution time: 28.084373474121094
EPOCH 54
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [54][  0/391]	Time  0.252 ( 0.252)	Data  0.188 ( 0.188)	Loss 7.4221e-02 (7.4221e-02)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [54][ 10/391]	Time  0.061 ( 0.081)	Data  0.001 ( 0.018)	Loss 9.3897e-02 (1.1225e-01)	Acc@1  96.88 ( 96.16)	Acc@5 100.00 (100.00)
Epoch: [54][ 20/391]	Time  0.065 ( 0.072)	Data  0.001 ( 0.010)	Loss 1.7909e-01 (1.3047e-01)	Acc@1  95.31 ( 95.65)	Acc@5  99.22 ( 99.96)
Epoch: [54][ 30/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.007)	Loss 8.2158e-02 (1.2800e-01)	Acc@1  97.66 ( 95.72)	Acc@5 100.00 ( 99.95)
Epoch: [54][ 40/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.006)	Loss 5.9968e-02 (1.2621e-01)	Acc@1  97.66 ( 95.73)	Acc@5 100.00 ( 99.96)
Epoch: [54][ 50/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.005)	Loss 1.1455e-01 (1.2524e-01)	Acc@1  96.09 ( 95.77)	Acc@5 100.00 ( 99.97)
Epoch: [54][ 60/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.2305e-01 (1.2772e-01)	Acc@1  97.66 ( 95.67)	Acc@5  99.22 ( 99.96)
Epoch: [54][ 70/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.6735e-01 (1.2895e-01)	Acc@1  94.53 ( 95.64)	Acc@5 100.00 ( 99.94)
Epoch: [54][ 80/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.1668e-01 (1.2929e-01)	Acc@1  95.31 ( 95.61)	Acc@5 100.00 ( 99.94)
Epoch: [54][ 90/391]	Time  0.060 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.2130e-01 (1.3018e-01)	Acc@1  96.09 ( 95.60)	Acc@5 100.00 ( 99.95)
Epoch: [54][100/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.6843e-01 (1.3065e-01)	Acc@1  92.97 ( 95.51)	Acc@5 100.00 ( 99.95)
Epoch: [54][110/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.3107e-01 (1.3147e-01)	Acc@1  96.09 ( 95.47)	Acc@5 100.00 ( 99.96)
Epoch: [54][120/391]	Time  0.071 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.5315e-01 (1.3220e-01)	Acc@1  96.09 ( 95.47)	Acc@5 100.00 ( 99.95)
Epoch: [54][130/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 7.5005e-02 (1.3057e-01)	Acc@1  98.44 ( 95.50)	Acc@5 100.00 ( 99.96)
Epoch: [54][140/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1817e-01 (1.3048e-01)	Acc@1  94.53 ( 95.47)	Acc@5 100.00 ( 99.96)
Epoch: [54][150/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.1906e-02 (1.2957e-01)	Acc@1  97.66 ( 95.51)	Acc@5 100.00 ( 99.96)
Epoch: [54][160/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.2003e-02 (1.3034e-01)	Acc@1  97.66 ( 95.52)	Acc@5 100.00 ( 99.96)
Epoch: [54][170/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4127e-01 (1.3104e-01)	Acc@1  94.53 ( 95.48)	Acc@5 100.00 ( 99.96)
Epoch: [54][180/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0024e-01 (1.3159e-01)	Acc@1  91.41 ( 95.45)	Acc@5 100.00 ( 99.96)
Epoch: [54][190/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2335e-01 (1.3116e-01)	Acc@1  93.75 ( 95.45)	Acc@5 100.00 ( 99.96)
Epoch: [54][200/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0294e-01 (1.3227e-01)	Acc@1  92.19 ( 95.39)	Acc@5 100.00 ( 99.97)
Epoch: [54][210/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0805e-01 (1.3305e-01)	Acc@1  93.75 ( 95.33)	Acc@5 100.00 ( 99.97)
Epoch: [54][220/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3539e-01 (1.3341e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 ( 99.97)
Epoch: [54][230/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1187e-01 (1.3369e-01)	Acc@1  95.31 ( 95.28)	Acc@5 100.00 ( 99.97)
Epoch: [54][240/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.7676e-01 (1.3440e-01)	Acc@1  92.97 ( 95.25)	Acc@5 100.00 ( 99.96)
Epoch: [54][250/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.6985e-01 (1.3376e-01)	Acc@1  95.31 ( 95.28)	Acc@5 100.00 ( 99.97)
Epoch: [54][260/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 8.3470e-02 (1.3400e-01)	Acc@1  97.66 ( 95.24)	Acc@5 100.00 ( 99.97)
Epoch: [54][270/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.9130e-01 (1.3470e-01)	Acc@1  91.41 ( 95.19)	Acc@5 100.00 ( 99.97)
Epoch: [54][280/391]	Time  0.067 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.3827e-01 (1.3530e-01)	Acc@1  91.41 ( 95.19)	Acc@5 100.00 ( 99.96)
Epoch: [54][290/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.4281e-01 (1.3596e-01)	Acc@1  93.75 ( 95.15)	Acc@5 100.00 ( 99.96)
Epoch: [54][300/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.1650e-01 (1.3629e-01)	Acc@1  96.09 ( 95.14)	Acc@5 100.00 ( 99.96)
Epoch: [54][310/391]	Time  0.059 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.2809e-01 (1.3658e-01)	Acc@1  96.88 ( 95.13)	Acc@5 100.00 ( 99.96)
Epoch: [54][320/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.4285e-01 (1.3650e-01)	Acc@1  93.75 ( 95.13)	Acc@5 100.00 ( 99.97)
Epoch: [54][330/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.5928e-01 (1.3605e-01)	Acc@1  93.75 ( 95.14)	Acc@5  99.22 ( 99.96)
Epoch: [54][340/391]	Time  0.067 ( 0.064)	Data  0.001 ( 0.002)	Loss 8.5908e-02 (1.3544e-01)	Acc@1  96.09 ( 95.18)	Acc@5 100.00 ( 99.97)
Epoch: [54][350/391]	Time  0.067 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.6972e-01 (1.3573e-01)	Acc@1  92.97 ( 95.16)	Acc@5 100.00 ( 99.96)
Epoch: [54][360/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.2962e-01 (1.3583e-01)	Acc@1  92.97 ( 95.15)	Acc@5 100.00 ( 99.96)
Epoch: [54][370/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.7976e-01 (1.3616e-01)	Acc@1  92.97 ( 95.14)	Acc@5 100.00 ( 99.96)
Epoch: [54][380/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.1710e-01 (1.3620e-01)	Acc@1  96.09 ( 95.14)	Acc@5 100.00 ( 99.97)
Epoch: [54][390/391]	Time  0.048 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.0439e-01 (1.3729e-01)	Acc@1  92.50 ( 95.10)	Acc@5 100.00 ( 99.96)
## e[54] optimizer.zero_grad (sum) time: 0.3995485305786133
## e[54]       loss.backward (sum) time: 6.879211664199829
## e[54]      optimizer.step (sum) time: 3.3466031551361084
## epoch[54] training(only) time: 25.163920402526855
# Switched to evaluate mode...
Test: [  0/100]	Time  0.191 ( 0.191)	Loss 2.3706e-01 (2.3706e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.025 ( 0.042)	Loss 3.9633e-01 (2.6386e-01)	Acc@1  89.00 ( 90.55)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.024 ( 0.034)	Loss 3.1444e-01 (2.9292e-01)	Acc@1  88.00 ( 90.48)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 3.2991e-01 (3.0218e-01)	Acc@1  88.00 ( 90.61)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.028 ( 0.030)	Loss 3.0660e-01 (3.1170e-01)	Acc@1  89.00 ( 90.32)	Acc@5  99.00 ( 99.71)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 2.4856e-01 (3.1351e-01)	Acc@1  90.00 ( 90.24)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.028 ( 0.028)	Loss 2.9692e-01 (3.0856e-01)	Acc@1  91.00 ( 90.25)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 4.5750e-01 (3.0674e-01)	Acc@1  86.00 ( 90.24)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.027 ( 0.028)	Loss 2.0316e-01 (3.0622e-01)	Acc@1  92.00 ( 90.32)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.024 ( 0.028)	Loss 2.7636e-01 (3.0324e-01)	Acc@1  93.00 ( 90.41)	Acc@5 100.00 ( 99.80)
 * Acc@1 90.470 Acc@5 99.800
### epoch[54] execution time: 28.01135802268982
EPOCH 55
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [55][  0/391]	Time  0.255 ( 0.255)	Data  0.189 ( 0.189)	Loss 9.3787e-02 (9.3787e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [55][ 10/391]	Time  0.062 ( 0.082)	Data  0.001 ( 0.018)	Loss 1.7060e-01 (1.1951e-01)	Acc@1  92.97 ( 95.74)	Acc@5 100.00 ( 99.93)
Epoch: [55][ 20/391]	Time  0.064 ( 0.074)	Data  0.001 ( 0.010)	Loss 4.5674e-02 (1.2256e-01)	Acc@1  99.22 ( 95.83)	Acc@5 100.00 ( 99.96)
Epoch: [55][ 30/391]	Time  0.063 ( 0.071)	Data  0.001 ( 0.007)	Loss 9.0341e-02 (1.2466e-01)	Acc@1  97.66 ( 95.74)	Acc@5 100.00 ( 99.97)
Epoch: [55][ 40/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.006)	Loss 1.8458e-01 (1.2668e-01)	Acc@1  93.75 ( 95.69)	Acc@5 100.00 ( 99.96)
Epoch: [55][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.005)	Loss 1.7315e-01 (1.2756e-01)	Acc@1  91.41 ( 95.60)	Acc@5 100.00 ( 99.95)
Epoch: [55][ 60/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.2976e-01 (1.2875e-01)	Acc@1  96.88 ( 95.62)	Acc@5 100.00 ( 99.96)
Epoch: [55][ 70/391]	Time  0.071 ( 0.067)	Data  0.001 ( 0.004)	Loss 6.2367e-02 (1.3064e-01)	Acc@1  98.44 ( 95.58)	Acc@5 100.00 ( 99.96)
Epoch: [55][ 80/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.004)	Loss 6.8988e-02 (1.2779e-01)	Acc@1  98.44 ( 95.68)	Acc@5 100.00 ( 99.96)
Epoch: [55][ 90/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.003)	Loss 7.0803e-02 (1.2688e-01)	Acc@1  97.66 ( 95.69)	Acc@5 100.00 ( 99.96)
Epoch: [55][100/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 9.7790e-02 (1.2895e-01)	Acc@1  96.88 ( 95.54)	Acc@5 100.00 ( 99.96)
Epoch: [55][110/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.6256e-01 (1.3148e-01)	Acc@1  92.97 ( 95.50)	Acc@5 100.00 ( 99.95)
Epoch: [55][120/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.2099e-01 (1.3095e-01)	Acc@1  96.09 ( 95.49)	Acc@5 100.00 ( 99.95)
Epoch: [55][130/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.5641e-01 (1.3118e-01)	Acc@1  96.09 ( 95.50)	Acc@5  99.22 ( 99.95)
Epoch: [55][140/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0895e-01 (1.3120e-01)	Acc@1  96.09 ( 95.45)	Acc@5 100.00 ( 99.94)
Epoch: [55][150/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4744e-01 (1.3191e-01)	Acc@1  96.09 ( 95.42)	Acc@5 100.00 ( 99.94)
Epoch: [55][160/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6909e-01 (1.3275e-01)	Acc@1  92.97 ( 95.37)	Acc@5 100.00 ( 99.95)
Epoch: [55][170/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2810e-01 (1.3198e-01)	Acc@1  95.31 ( 95.41)	Acc@5 100.00 ( 99.95)
Epoch: [55][180/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.6368e-02 (1.3191e-01)	Acc@1  96.09 ( 95.39)	Acc@5 100.00 ( 99.95)
Epoch: [55][190/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8130e-01 (1.3284e-01)	Acc@1  95.31 ( 95.37)	Acc@5 100.00 ( 99.95)
Epoch: [55][200/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5227e-01 (1.3360e-01)	Acc@1  94.53 ( 95.35)	Acc@5 100.00 ( 99.95)
Epoch: [55][210/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7103e-01 (1.3401e-01)	Acc@1  92.97 ( 95.30)	Acc@5 100.00 ( 99.95)
Epoch: [55][220/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2924e-01 (1.3442e-01)	Acc@1  94.53 ( 95.28)	Acc@5 100.00 ( 99.95)
Epoch: [55][230/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.5692e-01 (1.3460e-01)	Acc@1  90.62 ( 95.26)	Acc@5 100.00 ( 99.95)
Epoch: [55][240/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0830e-01 (1.3487e-01)	Acc@1  94.53 ( 95.23)	Acc@5 100.00 ( 99.95)
Epoch: [55][250/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2218e-01 (1.3428e-01)	Acc@1  96.09 ( 95.24)	Acc@5  99.22 ( 99.95)
Epoch: [55][260/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1035e-01 (1.3459e-01)	Acc@1  96.09 ( 95.24)	Acc@5 100.00 ( 99.95)
Epoch: [55][270/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6103e-01 (1.3437e-01)	Acc@1  94.53 ( 95.23)	Acc@5 100.00 ( 99.95)
Epoch: [55][280/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.2017e-02 (1.3409e-01)	Acc@1  95.31 ( 95.24)	Acc@5 100.00 ( 99.95)
Epoch: [55][290/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.0354e-02 (1.3380e-01)	Acc@1  98.44 ( 95.27)	Acc@5 100.00 ( 99.95)
Epoch: [55][300/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4693e-01 (1.3450e-01)	Acc@1  93.75 ( 95.24)	Acc@5 100.00 ( 99.95)
Epoch: [55][310/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1396e-01 (1.3471e-01)	Acc@1  96.88 ( 95.24)	Acc@5 100.00 ( 99.95)
Epoch: [55][320/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7186e-01 (1.3478e-01)	Acc@1  93.75 ( 95.24)	Acc@5 100.00 ( 99.95)
Epoch: [55][330/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5926e-01 (1.3448e-01)	Acc@1  92.97 ( 95.26)	Acc@5 100.00 ( 99.95)
Epoch: [55][340/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2861e-01 (1.3523e-01)	Acc@1  94.53 ( 95.23)	Acc@5 100.00 ( 99.95)
Epoch: [55][350/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2629e-01 (1.3563e-01)	Acc@1  96.09 ( 95.22)	Acc@5 100.00 ( 99.94)
Epoch: [55][360/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.9745e-02 (1.3586e-01)	Acc@1  97.66 ( 95.22)	Acc@5 100.00 ( 99.94)
Epoch: [55][370/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1650e-01 (1.3578e-01)	Acc@1  92.19 ( 95.23)	Acc@5 100.00 ( 99.94)
Epoch: [55][380/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5986e-01 (1.3620e-01)	Acc@1  94.53 ( 95.21)	Acc@5 100.00 ( 99.94)
Epoch: [55][390/391]	Time  0.047 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0405e-01 (1.3642e-01)	Acc@1  96.25 ( 95.21)	Acc@5 100.00 ( 99.94)
## e[55] optimizer.zero_grad (sum) time: 0.3985464572906494
## e[55]       loss.backward (sum) time: 6.95613694190979
## e[55]      optimizer.step (sum) time: 3.4048736095428467
## epoch[55] training(only) time: 25.382078886032104
# Switched to evaluate mode...
Test: [  0/100]	Time  0.193 ( 0.193)	Loss 3.0109e-01 (3.0109e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.027 ( 0.042)	Loss 4.1302e-01 (2.7536e-01)	Acc@1  90.00 ( 90.91)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.024 ( 0.035)	Loss 2.9668e-01 (2.9739e-01)	Acc@1  90.00 ( 90.95)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.027 ( 0.032)	Loss 3.6920e-01 (3.1857e-01)	Acc@1  90.00 ( 90.81)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.024 ( 0.030)	Loss 2.9375e-01 (3.2433e-01)	Acc@1  90.00 ( 90.59)	Acc@5  99.00 ( 99.59)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 3.2416e-01 (3.2761e-01)	Acc@1  89.00 ( 90.47)	Acc@5  99.00 ( 99.61)
Test: [ 60/100]	Time  0.027 ( 0.029)	Loss 3.1664e-01 (3.2493e-01)	Acc@1  91.00 ( 90.54)	Acc@5 100.00 ( 99.66)
Test: [ 70/100]	Time  0.026 ( 0.028)	Loss 5.0932e-01 (3.2281e-01)	Acc@1  86.00 ( 90.46)	Acc@5 100.00 ( 99.69)
Test: [ 80/100]	Time  0.027 ( 0.028)	Loss 2.6606e-01 (3.2317e-01)	Acc@1  91.00 ( 90.32)	Acc@5 100.00 ( 99.72)
Test: [ 90/100]	Time  0.024 ( 0.028)	Loss 2.2388e-01 (3.2085e-01)	Acc@1  94.00 ( 90.33)	Acc@5 100.00 ( 99.75)
 * Acc@1 90.350 Acc@5 99.760
### epoch[55] execution time: 28.301018238067627
EPOCH 56
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [56][  0/391]	Time  0.249 ( 0.249)	Data  0.185 ( 0.185)	Loss 1.0137e-01 (1.0137e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [56][ 10/391]	Time  0.059 ( 0.081)	Data  0.001 ( 0.018)	Loss 1.3261e-01 (1.0965e-01)	Acc@1  96.09 ( 95.60)	Acc@5 100.00 (100.00)
Epoch: [56][ 20/391]	Time  0.065 ( 0.073)	Data  0.001 ( 0.010)	Loss 6.8589e-02 (1.2289e-01)	Acc@1  97.66 ( 95.35)	Acc@5 100.00 (100.00)
Epoch: [56][ 30/391]	Time  0.065 ( 0.070)	Data  0.001 ( 0.007)	Loss 1.3050e-01 (1.2948e-01)	Acc@1  96.88 ( 95.26)	Acc@5  99.22 ( 99.95)
Epoch: [56][ 40/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.006)	Loss 1.3946e-01 (1.2782e-01)	Acc@1  96.09 ( 95.41)	Acc@5  99.22 ( 99.94)
Epoch: [56][ 50/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.005)	Loss 1.1955e-01 (1.2457e-01)	Acc@1  94.53 ( 95.53)	Acc@5 100.00 ( 99.95)
Epoch: [56][ 60/391]	Time  0.059 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.5769e-01 (1.2513e-01)	Acc@1  95.31 ( 95.48)	Acc@5  99.22 ( 99.94)
Epoch: [56][ 70/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.004)	Loss 6.3910e-02 (1.2438e-01)	Acc@1  98.44 ( 95.50)	Acc@5 100.00 ( 99.94)
Epoch: [56][ 80/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.1570e-01 (1.2177e-01)	Acc@1  93.75 ( 95.63)	Acc@5 100.00 ( 99.95)
Epoch: [56][ 90/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 9.0740e-02 (1.2161e-01)	Acc@1  97.66 ( 95.60)	Acc@5 100.00 ( 99.95)
Epoch: [56][100/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.2375e-01 (1.2294e-01)	Acc@1  95.31 ( 95.54)	Acc@5 100.00 ( 99.95)
Epoch: [56][110/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.2825e-01 (1.2020e-01)	Acc@1  96.09 ( 95.69)	Acc@5 100.00 ( 99.96)
Epoch: [56][120/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.0923e-01 (1.2127e-01)	Acc@1  95.31 ( 95.69)	Acc@5 100.00 ( 99.96)
Epoch: [56][130/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.4125e-01 (1.2035e-01)	Acc@1  96.09 ( 95.74)	Acc@5 100.00 ( 99.96)
Epoch: [56][140/391]	Time  0.065 ( 0.065)	Data  0.002 ( 0.002)	Loss 2.1486e-01 (1.2256e-01)	Acc@1  92.97 ( 95.65)	Acc@5 100.00 ( 99.96)
Epoch: [56][150/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7652e-01 (1.2338e-01)	Acc@1  92.97 ( 95.62)	Acc@5 100.00 ( 99.96)
Epoch: [56][160/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1756e-01 (1.2348e-01)	Acc@1  94.53 ( 95.59)	Acc@5 100.00 ( 99.97)
Epoch: [56][170/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3762e-01 (1.2531e-01)	Acc@1  95.31 ( 95.50)	Acc@5 100.00 ( 99.96)
Epoch: [56][180/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.0515e-02 (1.2536e-01)	Acc@1  97.66 ( 95.48)	Acc@5 100.00 ( 99.97)
Epoch: [56][190/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1776e-01 (1.2497e-01)	Acc@1  95.31 ( 95.50)	Acc@5 100.00 ( 99.97)
Epoch: [56][200/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3911e-01 (1.2546e-01)	Acc@1  92.19 ( 95.48)	Acc@5 100.00 ( 99.97)
Epoch: [56][210/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.8203e-02 (1.2573e-01)	Acc@1  95.31 ( 95.47)	Acc@5 100.00 ( 99.96)
Epoch: [56][220/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0782e-01 (1.2667e-01)	Acc@1  95.31 ( 95.42)	Acc@5 100.00 ( 99.96)
Epoch: [56][230/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0133e-01 (1.2681e-01)	Acc@1  95.31 ( 95.41)	Acc@5 100.00 ( 99.97)
Epoch: [56][240/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1584e-01 (1.2831e-01)	Acc@1  96.88 ( 95.37)	Acc@5 100.00 ( 99.96)
Epoch: [56][250/391]	Time  0.075 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2469e-01 (1.2822e-01)	Acc@1  95.31 ( 95.36)	Acc@5 100.00 ( 99.96)
Epoch: [56][260/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4126e-01 (1.2917e-01)	Acc@1  94.53 ( 95.32)	Acc@5 100.00 ( 99.96)
Epoch: [56][270/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2055e-01 (1.2894e-01)	Acc@1  94.53 ( 95.34)	Acc@5 100.00 ( 99.96)
Epoch: [56][280/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9934e-01 (1.2925e-01)	Acc@1  94.53 ( 95.34)	Acc@5 100.00 ( 99.96)
Epoch: [56][290/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3055e-01 (1.2953e-01)	Acc@1  96.09 ( 95.33)	Acc@5 100.00 ( 99.96)
Epoch: [56][300/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1327e-01 (1.2936e-01)	Acc@1  96.09 ( 95.34)	Acc@5 100.00 ( 99.96)
Epoch: [56][310/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5193e-01 (1.3021e-01)	Acc@1  96.09 ( 95.29)	Acc@5 100.00 ( 99.96)
Epoch: [56][320/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2104e-01 (1.3066e-01)	Acc@1  95.31 ( 95.28)	Acc@5 100.00 ( 99.96)
Epoch: [56][330/391]	Time  0.058 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0241e-01 (1.3107e-01)	Acc@1  93.75 ( 95.27)	Acc@5 100.00 ( 99.96)
Epoch: [56][340/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8512e-01 (1.3174e-01)	Acc@1  92.19 ( 95.26)	Acc@5 100.00 ( 99.96)
Epoch: [56][350/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.5383e-01 (1.3210e-01)	Acc@1  89.06 ( 95.23)	Acc@5 100.00 ( 99.96)
Epoch: [56][360/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.2708e-01 (1.3272e-01)	Acc@1  92.97 ( 95.21)	Acc@5 100.00 ( 99.96)
Epoch: [56][370/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.8691e-01 (1.3325e-01)	Acc@1  91.41 ( 95.19)	Acc@5 100.00 ( 99.96)
Epoch: [56][380/391]	Time  0.069 ( 0.064)	Data  0.001 ( 0.002)	Loss 9.2650e-02 (1.3312e-01)	Acc@1  98.44 ( 95.19)	Acc@5 100.00 ( 99.96)
Epoch: [56][390/391]	Time  0.049 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.8064e-01 (1.3308e-01)	Acc@1  96.25 ( 95.19)	Acc@5  98.75 ( 99.96)
## e[56] optimizer.zero_grad (sum) time: 0.4063296318054199
## e[56]       loss.backward (sum) time: 6.917473316192627
## e[56]      optimizer.step (sum) time: 3.389187812805176
## epoch[56] training(only) time: 25.25218367576599
# Switched to evaluate mode...
Test: [  0/100]	Time  0.191 ( 0.191)	Loss 3.1573e-01 (3.1573e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.041)	Loss 3.2565e-01 (2.6072e-01)	Acc@1  89.00 ( 91.64)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.029 ( 0.034)	Loss 3.5075e-01 (2.9110e-01)	Acc@1  88.00 ( 90.81)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.035 ( 0.032)	Loss 3.4943e-01 (3.0789e-01)	Acc@1  90.00 ( 90.65)	Acc@5 100.00 ( 99.77)
Test: [ 40/100]	Time  0.024 ( 0.030)	Loss 2.8970e-01 (3.1717e-01)	Acc@1  90.00 ( 90.39)	Acc@5  99.00 ( 99.71)
Test: [ 50/100]	Time  0.028 ( 0.029)	Loss 2.7887e-01 (3.2231e-01)	Acc@1  89.00 ( 90.39)	Acc@5  99.00 ( 99.71)
Test: [ 60/100]	Time  0.027 ( 0.029)	Loss 3.4978e-01 (3.1825e-01)	Acc@1  91.00 ( 90.34)	Acc@5 100.00 ( 99.72)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 4.7054e-01 (3.1301e-01)	Acc@1  87.00 ( 90.39)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.027 ( 0.028)	Loss 2.8920e-01 (3.1225e-01)	Acc@1  92.00 ( 90.42)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.024 ( 0.028)	Loss 2.0693e-01 (3.0903e-01)	Acc@1  92.00 ( 90.44)	Acc@5 100.00 ( 99.80)
 * Acc@1 90.440 Acc@5 99.790
### epoch[56] execution time: 28.099493980407715
EPOCH 57
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [57][  0/391]	Time  0.244 ( 0.244)	Data  0.177 ( 0.177)	Loss 1.2460e-01 (1.2460e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
Epoch: [57][ 10/391]	Time  0.064 ( 0.080)	Data  0.001 ( 0.017)	Loss 9.3668e-02 (1.3195e-01)	Acc@1  97.66 ( 94.89)	Acc@5 100.00 ( 99.93)
Epoch: [57][ 20/391]	Time  0.059 ( 0.072)	Data  0.001 ( 0.010)	Loss 6.5923e-02 (1.1952e-01)	Acc@1  97.66 ( 95.50)	Acc@5 100.00 ( 99.96)
Epoch: [57][ 30/391]	Time  0.062 ( 0.069)	Data  0.001 ( 0.007)	Loss 9.0372e-02 (1.2609e-01)	Acc@1  99.22 ( 95.64)	Acc@5 100.00 ( 99.97)
Epoch: [57][ 40/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.005)	Loss 1.9062e-01 (1.3088e-01)	Acc@1  94.53 ( 95.58)	Acc@5 100.00 ( 99.98)
Epoch: [57][ 50/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.005)	Loss 1.1854e-01 (1.2996e-01)	Acc@1  94.53 ( 95.51)	Acc@5 100.00 ( 99.98)
Epoch: [57][ 60/391]	Time  0.072 ( 0.067)	Data  0.001 ( 0.004)	Loss 9.6438e-02 (1.3011e-01)	Acc@1  96.09 ( 95.39)	Acc@5 100.00 ( 99.97)
Epoch: [57][ 70/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.6850e-01 (1.2976e-01)	Acc@1  95.31 ( 95.49)	Acc@5 100.00 ( 99.97)
Epoch: [57][ 80/391]	Time  0.077 ( 0.067)	Data  0.001 ( 0.003)	Loss 7.1561e-02 (1.2706e-01)	Acc@1  97.66 ( 95.49)	Acc@5 100.00 ( 99.97)
Epoch: [57][ 90/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.2076e-01 (1.2882e-01)	Acc@1  95.31 ( 95.40)	Acc@5 100.00 ( 99.96)
Epoch: [57][100/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.0652e-01 (1.2733e-01)	Acc@1  94.53 ( 95.46)	Acc@5 100.00 ( 99.96)
Epoch: [57][110/391]	Time  0.060 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.4536e-01 (1.2789e-01)	Acc@1  94.53 ( 95.47)	Acc@5 100.00 ( 99.96)
Epoch: [57][120/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.2352e-01 (1.2957e-01)	Acc@1  95.31 ( 95.42)	Acc@5 100.00 ( 99.97)
Epoch: [57][130/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.0162e-02 (1.2848e-01)	Acc@1  99.22 ( 95.42)	Acc@5 100.00 ( 99.96)
Epoch: [57][140/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.9236e-02 (1.2699e-01)	Acc@1  97.66 ( 95.45)	Acc@5 100.00 ( 99.97)
Epoch: [57][150/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2233e-01 (1.2778e-01)	Acc@1  94.53 ( 95.43)	Acc@5 100.00 ( 99.97)
Epoch: [57][160/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0597e-01 (1.2748e-01)	Acc@1  96.09 ( 95.42)	Acc@5 100.00 ( 99.97)
Epoch: [57][170/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8383e-01 (1.2779e-01)	Acc@1  93.75 ( 95.38)	Acc@5 100.00 ( 99.97)
Epoch: [57][180/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.2641e-02 (1.2797e-01)	Acc@1  97.66 ( 95.36)	Acc@5 100.00 ( 99.97)
Epoch: [57][190/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4181e-01 (1.2838e-01)	Acc@1  94.53 ( 95.35)	Acc@5 100.00 ( 99.97)
Epoch: [57][200/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5066e-01 (1.2861e-01)	Acc@1  95.31 ( 95.38)	Acc@5 100.00 ( 99.97)
Epoch: [57][210/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.2327e-02 (1.2912e-01)	Acc@1  96.88 ( 95.36)	Acc@5 100.00 ( 99.97)
Epoch: [57][220/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.5466e-02 (1.2763e-01)	Acc@1  96.88 ( 95.42)	Acc@5 100.00 ( 99.97)
Epoch: [57][230/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4546e-01 (1.2754e-01)	Acc@1  93.75 ( 95.40)	Acc@5 100.00 ( 99.97)
Epoch: [57][240/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3563e-01 (1.2752e-01)	Acc@1  95.31 ( 95.40)	Acc@5 100.00 ( 99.96)
Epoch: [57][250/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3935e-01 (1.2738e-01)	Acc@1  95.31 ( 95.42)	Acc@5 100.00 ( 99.97)
Epoch: [57][260/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1488e-01 (1.2690e-01)	Acc@1  96.09 ( 95.43)	Acc@5 100.00 ( 99.97)
Epoch: [57][270/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1612e-01 (1.2709e-01)	Acc@1  96.88 ( 95.42)	Acc@5 100.00 ( 99.97)
Epoch: [57][280/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.3260e-02 (1.2711e-01)	Acc@1  94.53 ( 95.40)	Acc@5 100.00 ( 99.96)
Epoch: [57][290/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2614e-01 (1.2719e-01)	Acc@1  93.75 ( 95.39)	Acc@5 100.00 ( 99.96)
Epoch: [57][300/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8883e-01 (1.2843e-01)	Acc@1  94.53 ( 95.34)	Acc@5 100.00 ( 99.96)
Epoch: [57][310/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1382e-01 (1.2914e-01)	Acc@1  96.88 ( 95.33)	Acc@5 100.00 ( 99.96)
Epoch: [57][320/391]	Time  0.058 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.7677e-02 (1.2894e-01)	Acc@1  98.44 ( 95.33)	Acc@5 100.00 ( 99.96)
Epoch: [57][330/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3270e-01 (1.2975e-01)	Acc@1  92.97 ( 95.30)	Acc@5 100.00 ( 99.96)
Epoch: [57][340/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1384e-01 (1.3016e-01)	Acc@1  94.53 ( 95.28)	Acc@5 100.00 ( 99.97)
Epoch: [57][350/391]	Time  0.071 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5029e-01 (1.3062e-01)	Acc@1  92.97 ( 95.28)	Acc@5 100.00 ( 99.96)
Epoch: [57][360/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9049e-01 (1.3144e-01)	Acc@1  92.97 ( 95.24)	Acc@5 100.00 ( 99.96)
Epoch: [57][370/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7622e-01 (1.3211e-01)	Acc@1  92.19 ( 95.22)	Acc@5 100.00 ( 99.96)
Epoch: [57][380/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2233e-01 (1.3213e-01)	Acc@1  92.19 ( 95.19)	Acc@5 100.00 ( 99.96)
Epoch: [57][390/391]	Time  0.049 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.3968e-01 (1.3252e-01)	Acc@1  95.00 ( 95.18)	Acc@5 100.00 ( 99.96)
## e[57] optimizer.zero_grad (sum) time: 0.4113757610321045
## e[57]       loss.backward (sum) time: 6.943914175033569
## e[57]      optimizer.step (sum) time: 3.415724039077759
## epoch[57] training(only) time: 25.354610919952393
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 2.7271e-01 (2.7271e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.026 ( 0.041)	Loss 3.8962e-01 (2.8131e-01)	Acc@1  90.00 ( 90.82)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.024 ( 0.034)	Loss 3.3383e-01 (3.0231e-01)	Acc@1  90.00 ( 91.00)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.027 ( 0.031)	Loss 4.5089e-01 (3.2227e-01)	Acc@1  85.00 ( 90.58)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.024 ( 0.030)	Loss 2.8948e-01 (3.2703e-01)	Acc@1  90.00 ( 90.34)	Acc@5  99.00 ( 99.71)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 2.6823e-01 (3.2873e-01)	Acc@1  92.00 ( 90.33)	Acc@5  99.00 ( 99.69)
Test: [ 60/100]	Time  0.027 ( 0.029)	Loss 2.7246e-01 (3.2238e-01)	Acc@1  92.00 ( 90.33)	Acc@5 100.00 ( 99.70)
Test: [ 70/100]	Time  0.030 ( 0.028)	Loss 4.4418e-01 (3.1848e-01)	Acc@1  86.00 ( 90.34)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.026 ( 0.028)	Loss 1.8486e-01 (3.1420e-01)	Acc@1  93.00 ( 90.44)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.026 ( 0.028)	Loss 2.2495e-01 (3.0955e-01)	Acc@1  94.00 ( 90.53)	Acc@5 100.00 ( 99.76)
 * Acc@1 90.510 Acc@5 99.720
### epoch[57] execution time: 28.2482271194458
EPOCH 58
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [58][  0/391]	Time  0.258 ( 0.258)	Data  0.193 ( 0.193)	Loss 1.2005e-01 (1.2005e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [58][ 10/391]	Time  0.064 ( 0.082)	Data  0.001 ( 0.019)	Loss 1.0777e-01 (1.1743e-01)	Acc@1  96.88 ( 96.02)	Acc@5 100.00 (100.00)
Epoch: [58][ 20/391]	Time  0.066 ( 0.073)	Data  0.001 ( 0.010)	Loss 1.2150e-01 (1.1483e-01)	Acc@1  94.53 ( 96.02)	Acc@5 100.00 (100.00)
Epoch: [58][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.007)	Loss 1.6215e-01 (1.1660e-01)	Acc@1  94.53 ( 95.89)	Acc@5 100.00 (100.00)
Epoch: [58][ 40/391]	Time  0.063 ( 0.069)	Data  0.001 ( 0.006)	Loss 1.1402e-01 (1.1449e-01)	Acc@1  95.31 ( 96.02)	Acc@5 100.00 (100.00)
Epoch: [58][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.005)	Loss 1.2064e-01 (1.1525e-01)	Acc@1  96.88 ( 95.99)	Acc@5 100.00 ( 99.97)
Epoch: [58][ 60/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.004)	Loss 7.5097e-02 (1.1441e-01)	Acc@1  96.88 ( 96.02)	Acc@5 100.00 ( 99.95)
Epoch: [58][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.1096e-01 (1.1681e-01)	Acc@1  96.88 ( 95.91)	Acc@5 100.00 ( 99.93)
Epoch: [58][ 80/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.004)	Loss 1.5745e-01 (1.1615e-01)	Acc@1  95.31 ( 95.94)	Acc@5 100.00 ( 99.93)
Epoch: [58][ 90/391]	Time  0.060 ( 0.066)	Data  0.001 ( 0.003)	Loss 8.7991e-02 (1.1632e-01)	Acc@1  97.66 ( 95.87)	Acc@5 100.00 ( 99.94)
Epoch: [58][100/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.1895e-01 (1.1596e-01)	Acc@1  96.09 ( 95.91)	Acc@5 100.00 ( 99.95)
Epoch: [58][110/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.6339e-01 (1.1639e-01)	Acc@1  94.53 ( 95.85)	Acc@5 100.00 ( 99.94)
Epoch: [58][120/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.8895e-01 (1.1796e-01)	Acc@1  92.19 ( 95.76)	Acc@5  99.22 ( 99.94)
Epoch: [58][130/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 8.9482e-02 (1.2016e-01)	Acc@1  96.88 ( 95.74)	Acc@5 100.00 ( 99.94)
Epoch: [58][140/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 8.2817e-02 (1.2008e-01)	Acc@1  97.66 ( 95.78)	Acc@5 100.00 ( 99.94)
Epoch: [58][150/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9809e-01 (1.2190e-01)	Acc@1  91.41 ( 95.73)	Acc@5 100.00 ( 99.94)
Epoch: [58][160/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5726e-01 (1.2247e-01)	Acc@1  95.31 ( 95.70)	Acc@5  99.22 ( 99.94)
Epoch: [58][170/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.8994e-02 (1.2208e-01)	Acc@1  94.53 ( 95.71)	Acc@5 100.00 ( 99.94)
Epoch: [58][180/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.8978e-02 (1.2243e-01)	Acc@1  96.09 ( 95.65)	Acc@5 100.00 ( 99.94)
Epoch: [58][190/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.2389e-02 (1.2351e-01)	Acc@1  96.09 ( 95.61)	Acc@5 100.00 ( 99.94)
Epoch: [58][200/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7071e-01 (1.2515e-01)	Acc@1  92.19 ( 95.55)	Acc@5 100.00 ( 99.94)
Epoch: [58][210/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3293e-01 (1.2539e-01)	Acc@1  94.53 ( 95.56)	Acc@5 100.00 ( 99.94)
Epoch: [58][220/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4508e-01 (1.2486e-01)	Acc@1  95.31 ( 95.58)	Acc@5 100.00 ( 99.95)
Epoch: [58][230/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.0238e-02 (1.2343e-01)	Acc@1  97.66 ( 95.62)	Acc@5 100.00 ( 99.95)
Epoch: [58][240/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.2242e-01 (1.2333e-01)	Acc@1  92.97 ( 95.62)	Acc@5 100.00 ( 99.95)
Epoch: [58][250/391]	Time  0.067 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.4484e-02 (1.2359e-01)	Acc@1  99.22 ( 95.62)	Acc@5 100.00 ( 99.94)
Epoch: [58][260/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5834e-01 (1.2357e-01)	Acc@1  93.75 ( 95.63)	Acc@5 100.00 ( 99.94)
Epoch: [58][270/391]	Time  0.058 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.3964e-01 (1.2355e-01)	Acc@1  93.75 ( 95.62)	Acc@5 100.00 ( 99.95)
Epoch: [58][280/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.2012e-01 (1.2454e-01)	Acc@1  95.31 ( 95.60)	Acc@5 100.00 ( 99.94)
Epoch: [58][290/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.1622e-02 (1.2404e-01)	Acc@1  99.22 ( 95.63)	Acc@5 100.00 ( 99.95)
Epoch: [58][300/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.9130e-02 (1.2463e-01)	Acc@1  96.09 ( 95.63)	Acc@5 100.00 ( 99.95)
Epoch: [58][310/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.8001e-01 (1.2539e-01)	Acc@1  92.97 ( 95.58)	Acc@5 100.00 ( 99.94)
Epoch: [58][320/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.2978e-01 (1.2553e-01)	Acc@1  95.31 ( 95.57)	Acc@5 100.00 ( 99.95)
Epoch: [58][330/391]	Time  0.068 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.5032e-01 (1.2591e-01)	Acc@1  94.53 ( 95.57)	Acc@5 100.00 ( 99.95)
Epoch: [58][340/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.0763e-01 (1.2637e-01)	Acc@1  92.19 ( 95.55)	Acc@5 100.00 ( 99.95)
Epoch: [58][350/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.6888e-01 (1.2651e-01)	Acc@1  93.75 ( 95.55)	Acc@5 100.00 ( 99.95)
Epoch: [58][360/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.9562e-02 (1.2674e-01)	Acc@1  96.88 ( 95.55)	Acc@5 100.00 ( 99.95)
Epoch: [58][370/391]	Time  0.066 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.3197e-01 (1.2665e-01)	Acc@1  93.75 ( 95.55)	Acc@5 100.00 ( 99.95)
Epoch: [58][380/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.1775e-01 (1.2627e-01)	Acc@1  96.09 ( 95.56)	Acc@5 100.00 ( 99.95)
Epoch: [58][390/391]	Time  0.050 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.4562e-01 (1.2660e-01)	Acc@1  92.50 ( 95.54)	Acc@5 100.00 ( 99.95)
## e[58] optimizer.zero_grad (sum) time: 0.3960390090942383
## e[58]       loss.backward (sum) time: 6.939115285873413
## e[58]      optimizer.step (sum) time: 3.350710868835449
## epoch[58] training(only) time: 25.23653268814087
# Switched to evaluate mode...
Test: [  0/100]	Time  0.194 ( 0.194)	Loss 2.9859e-01 (2.9859e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.026 ( 0.042)	Loss 4.0257e-01 (3.0275e-01)	Acc@1  88.00 ( 89.91)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.024 ( 0.034)	Loss 3.4813e-01 (3.1735e-01)	Acc@1  89.00 ( 90.14)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.026 ( 0.031)	Loss 3.8262e-01 (3.3211e-01)	Acc@1  89.00 ( 90.10)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.024 ( 0.030)	Loss 3.2956e-01 (3.3840e-01)	Acc@1  90.00 ( 89.98)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 2.9060e-01 (3.3935e-01)	Acc@1  89.00 ( 90.08)	Acc@5  99.00 ( 99.67)
Test: [ 60/100]	Time  0.024 ( 0.029)	Loss 2.9139e-01 (3.3339e-01)	Acc@1  94.00 ( 90.20)	Acc@5 100.00 ( 99.69)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 4.1867e-01 (3.2719e-01)	Acc@1  88.00 ( 90.24)	Acc@5 100.00 ( 99.70)
Test: [ 80/100]	Time  0.028 ( 0.028)	Loss 2.2488e-01 (3.2603e-01)	Acc@1  92.00 ( 90.16)	Acc@5 100.00 ( 99.73)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 3.1019e-01 (3.2347e-01)	Acc@1  91.00 ( 90.14)	Acc@5 100.00 ( 99.75)
 * Acc@1 90.090 Acc@5 99.750
### epoch[58] execution time: 28.098018646240234
EPOCH 59
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [59][  0/391]	Time  0.300 ( 0.300)	Data  0.235 ( 0.235)	Loss 6.3458e-02 (6.3458e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [59][ 10/391]	Time  0.063 ( 0.084)	Data  0.001 ( 0.022)	Loss 8.5226e-02 (1.3159e-01)	Acc@1  97.66 ( 95.24)	Acc@5 100.00 (100.00)
Epoch: [59][ 20/391]	Time  0.062 ( 0.074)	Data  0.001 ( 0.012)	Loss 6.9461e-02 (1.4904e-01)	Acc@1  97.66 ( 94.64)	Acc@5 100.00 (100.00)
Epoch: [59][ 30/391]	Time  0.061 ( 0.070)	Data  0.001 ( 0.009)	Loss 6.4200e-02 (1.3385e-01)	Acc@1  97.66 ( 95.29)	Acc@5 100.00 ( 99.97)
Epoch: [59][ 40/391]	Time  0.062 ( 0.069)	Data  0.001 ( 0.007)	Loss 1.7653e-01 (1.3228e-01)	Acc@1  95.31 ( 95.45)	Acc@5 100.00 ( 99.98)
Epoch: [59][ 50/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.006)	Loss 1.0129e-01 (1.2934e-01)	Acc@1  96.88 ( 95.63)	Acc@5 100.00 ( 99.98)
Epoch: [59][ 60/391]	Time  0.060 ( 0.067)	Data  0.001 ( 0.005)	Loss 1.7483e-01 (1.2828e-01)	Acc@1  92.97 ( 95.63)	Acc@5 100.00 ( 99.99)
Epoch: [59][ 70/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.004)	Loss 2.3579e-01 (1.2835e-01)	Acc@1  91.41 ( 95.59)	Acc@5 100.00 ( 99.97)
Epoch: [59][ 80/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.004)	Loss 1.8599e-01 (1.2641e-01)	Acc@1  93.75 ( 95.66)	Acc@5 100.00 ( 99.97)
Epoch: [59][ 90/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.004)	Loss 1.5249e-01 (1.2518e-01)	Acc@1  94.53 ( 95.65)	Acc@5 100.00 ( 99.97)
Epoch: [59][100/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.3935e-01 (1.2378e-01)	Acc@1  94.53 ( 95.71)	Acc@5 100.00 ( 99.95)
Epoch: [59][110/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.6052e-01 (1.2268e-01)	Acc@1  96.09 ( 95.78)	Acc@5 100.00 ( 99.95)
Epoch: [59][120/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.0826e-01 (1.2100e-01)	Acc@1  96.88 ( 95.85)	Acc@5 100.00 ( 99.95)
Epoch: [59][130/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.003)	Loss 9.4770e-02 (1.1998e-01)	Acc@1  96.09 ( 95.84)	Acc@5 100.00 ( 99.96)
Epoch: [59][140/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.4968e-01 (1.2096e-01)	Acc@1  95.31 ( 95.82)	Acc@5 100.00 ( 99.96)
Epoch: [59][150/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.5841e-01 (1.2205e-01)	Acc@1  92.97 ( 95.75)	Acc@5 100.00 ( 99.95)
Epoch: [59][160/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.003)	Loss 6.6642e-02 (1.2135e-01)	Acc@1  99.22 ( 95.79)	Acc@5 100.00 ( 99.95)
Epoch: [59][170/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1118e-01 (1.2123e-01)	Acc@1  96.09 ( 95.77)	Acc@5 100.00 ( 99.95)
Epoch: [59][180/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0986e-01 (1.2177e-01)	Acc@1  96.88 ( 95.74)	Acc@5 100.00 ( 99.95)
Epoch: [59][190/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.2335e-02 (1.2284e-01)	Acc@1  96.88 ( 95.70)	Acc@5 100.00 ( 99.96)
Epoch: [59][200/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0614e-01 (1.2289e-01)	Acc@1  96.88 ( 95.69)	Acc@5 100.00 ( 99.96)
Epoch: [59][210/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6669e-01 (1.2350e-01)	Acc@1  95.31 ( 95.66)	Acc@5 100.00 ( 99.96)
Epoch: [59][220/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6926e-01 (1.2307e-01)	Acc@1  93.75 ( 95.66)	Acc@5  98.44 ( 99.95)
Epoch: [59][230/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.1810e-02 (1.2163e-01)	Acc@1  97.66 ( 95.71)	Acc@5 100.00 ( 99.95)
Epoch: [59][240/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.5914e-02 (1.2235e-01)	Acc@1  97.66 ( 95.67)	Acc@5 100.00 ( 99.95)
Epoch: [59][250/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5566e-01 (1.2189e-01)	Acc@1  93.75 ( 95.70)	Acc@5 100.00 ( 99.95)
Epoch: [59][260/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4596e-01 (1.2166e-01)	Acc@1  96.09 ( 95.71)	Acc@5 100.00 ( 99.96)
Epoch: [59][270/391]	Time  0.070 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.4271e-02 (1.2212e-01)	Acc@1  96.09 ( 95.69)	Acc@5 100.00 ( 99.96)
Epoch: [59][280/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.2146e-02 (1.2190e-01)	Acc@1  98.44 ( 95.69)	Acc@5 100.00 ( 99.96)
Epoch: [59][290/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.2489e-02 (1.2179e-01)	Acc@1  95.31 ( 95.67)	Acc@5 100.00 ( 99.96)
Epoch: [59][300/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3991e-01 (1.2225e-01)	Acc@1  96.09 ( 95.66)	Acc@5 100.00 ( 99.96)
Epoch: [59][310/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.3280e-01 (1.2206e-01)	Acc@1  94.53 ( 95.67)	Acc@5 100.00 ( 99.96)
Epoch: [59][320/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.2005e-01 (1.2269e-01)	Acc@1  95.31 ( 95.68)	Acc@5 100.00 ( 99.96)
Epoch: [59][330/391]	Time  0.067 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.5118e-01 (1.2311e-01)	Acc@1  91.41 ( 95.66)	Acc@5  99.22 ( 99.96)
Epoch: [59][340/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.8220e-01 (1.2387e-01)	Acc@1  94.53 ( 95.64)	Acc@5 100.00 ( 99.96)
Epoch: [59][350/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 9.4700e-02 (1.2424e-01)	Acc@1  96.09 ( 95.62)	Acc@5 100.00 ( 99.96)
Epoch: [59][360/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.2063e-01 (1.2448e-01)	Acc@1  98.44 ( 95.62)	Acc@5 100.00 ( 99.96)
Epoch: [59][370/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.1601e-01 (1.2469e-01)	Acc@1  97.66 ( 95.61)	Acc@5 100.00 ( 99.96)
Epoch: [59][380/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.0829e-01 (1.2473e-01)	Acc@1  96.88 ( 95.61)	Acc@5 100.00 ( 99.96)
Epoch: [59][390/391]	Time  0.047 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.1598e-01 (1.2485e-01)	Acc@1  96.25 ( 95.59)	Acc@5 100.00 ( 99.96)
## e[59] optimizer.zero_grad (sum) time: 0.3994145393371582
## e[59]       loss.backward (sum) time: 6.892513990402222
## e[59]      optimizer.step (sum) time: 3.332392692565918
## epoch[59] training(only) time: 25.282095193862915
# Switched to evaluate mode...
Test: [  0/100]	Time  0.191 ( 0.191)	Loss 3.5323e-01 (3.5323e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.026 ( 0.041)	Loss 3.4859e-01 (2.8377e-01)	Acc@1  89.00 ( 90.27)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.028 ( 0.034)	Loss 3.3792e-01 (2.9546e-01)	Acc@1  89.00 ( 90.67)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.028 ( 0.032)	Loss 3.6366e-01 (3.0802e-01)	Acc@1  92.00 ( 90.52)	Acc@5 100.00 ( 99.77)
Test: [ 40/100]	Time  0.024 ( 0.030)	Loss 2.9192e-01 (3.1936e-01)	Acc@1  92.00 ( 90.44)	Acc@5  99.00 ( 99.76)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 3.2581e-01 (3.2439e-01)	Acc@1  92.00 ( 90.45)	Acc@5  99.00 ( 99.75)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 3.2404e-01 (3.2186e-01)	Acc@1  93.00 ( 90.38)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.033 ( 0.028)	Loss 5.2315e-01 (3.1851e-01)	Acc@1  87.00 ( 90.51)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.024 ( 0.028)	Loss 2.4670e-01 (3.1799e-01)	Acc@1  92.00 ( 90.46)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.024 ( 0.028)	Loss 2.2980e-01 (3.1430e-01)	Acc@1  93.00 ( 90.44)	Acc@5 100.00 ( 99.80)
 * Acc@1 90.490 Acc@5 99.800
### epoch[59] execution time: 28.13939356803894
EPOCH 60
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [60][  0/391]	Time  0.243 ( 0.243)	Data  0.176 ( 0.176)	Loss 1.1497e-01 (1.1497e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [60][ 10/391]	Time  0.067 ( 0.080)	Data  0.001 ( 0.017)	Loss 6.4194e-02 (1.2025e-01)	Acc@1  98.44 ( 95.88)	Acc@5 100.00 ( 99.93)
Epoch: [60][ 20/391]	Time  0.065 ( 0.073)	Data  0.001 ( 0.009)	Loss 8.1686e-02 (1.2380e-01)	Acc@1  97.66 ( 95.65)	Acc@5 100.00 ( 99.96)
Epoch: [60][ 30/391]	Time  0.063 ( 0.070)	Data  0.001 ( 0.007)	Loss 1.0625e-01 (1.1983e-01)	Acc@1  96.09 ( 95.79)	Acc@5 100.00 ( 99.97)
Epoch: [60][ 40/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.005)	Loss 1.5846e-01 (1.1993e-01)	Acc@1  96.09 ( 95.83)	Acc@5 100.00 ( 99.98)
Epoch: [60][ 50/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.005)	Loss 6.0286e-02 (1.1909e-01)	Acc@1  99.22 ( 95.74)	Acc@5 100.00 ( 99.98)
Epoch: [60][ 60/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.004)	Loss 5.3425e-02 (1.1791e-01)	Acc@1  98.44 ( 95.77)	Acc@5 100.00 ( 99.97)
Epoch: [60][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 7.1830e-02 (1.1852e-01)	Acc@1  98.44 ( 95.71)	Acc@5 100.00 ( 99.98)
Epoch: [60][ 80/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.4007e-01 (1.1708e-01)	Acc@1  95.31 ( 95.74)	Acc@5 100.00 ( 99.97)
Epoch: [60][ 90/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.2086e-01 (1.1819e-01)	Acc@1  94.53 ( 95.77)	Acc@5 100.00 ( 99.97)
Epoch: [60][100/391]	Time  0.059 ( 0.066)	Data  0.001 ( 0.003)	Loss 8.3565e-02 (1.1768e-01)	Acc@1  96.88 ( 95.79)	Acc@5 100.00 ( 99.98)
Epoch: [60][110/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.5348e-01 (1.1755e-01)	Acc@1  95.31 ( 95.85)	Acc@5 100.00 ( 99.97)
Epoch: [60][120/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.7445e-01 (1.1658e-01)	Acc@1  94.53 ( 95.86)	Acc@5 100.00 ( 99.97)
Epoch: [60][130/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0648e-01 (1.1626e-01)	Acc@1  97.66 ( 95.90)	Acc@5 100.00 ( 99.98)
Epoch: [60][140/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.9869e-02 (1.1592e-01)	Acc@1  97.66 ( 95.88)	Acc@5 100.00 ( 99.98)
Epoch: [60][150/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8923e-01 (1.1721e-01)	Acc@1  93.75 ( 95.83)	Acc@5 100.00 ( 99.97)
Epoch: [60][160/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.7356e-02 (1.1630e-01)	Acc@1  96.88 ( 95.88)	Acc@5 100.00 ( 99.98)
Epoch: [60][170/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.0943e-02 (1.1598e-01)	Acc@1  97.66 ( 95.89)	Acc@5 100.00 ( 99.97)
Epoch: [60][180/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1956e-01 (1.1554e-01)	Acc@1  92.97 ( 95.92)	Acc@5 100.00 ( 99.97)
Epoch: [60][190/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6854e-01 (1.1451e-01)	Acc@1  92.97 ( 95.95)	Acc@5 100.00 ( 99.97)
Epoch: [60][200/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.1438e-02 (1.1437e-01)	Acc@1  99.22 ( 95.96)	Acc@5 100.00 ( 99.97)
Epoch: [60][210/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.8822e-02 (1.1310e-01)	Acc@1  98.44 ( 96.00)	Acc@5 100.00 ( 99.97)
Epoch: [60][220/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.2044e-02 (1.1315e-01)	Acc@1  96.09 ( 95.98)	Acc@5 100.00 ( 99.97)
Epoch: [60][230/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6497e-01 (1.1359e-01)	Acc@1  93.75 ( 95.96)	Acc@5 100.00 ( 99.97)
Epoch: [60][240/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.7687e-01 (1.1411e-01)	Acc@1  93.75 ( 95.93)	Acc@5  99.22 ( 99.96)
Epoch: [60][250/391]	Time  0.067 ( 0.064)	Data  0.001 ( 0.002)	Loss 9.4707e-02 (1.1336e-01)	Acc@1  97.66 ( 95.95)	Acc@5 100.00 ( 99.96)
Epoch: [60][260/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 9.3586e-02 (1.1303e-01)	Acc@1  96.88 ( 95.97)	Acc@5 100.00 ( 99.96)
Epoch: [60][270/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.4130e-01 (1.1303e-01)	Acc@1  95.31 ( 95.99)	Acc@5 100.00 ( 99.97)
Epoch: [60][280/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 9.7770e-02 (1.1298e-01)	Acc@1  97.66 ( 96.00)	Acc@5 100.00 ( 99.96)
Epoch: [60][290/391]	Time  0.068 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.4953e-01 (1.1264e-01)	Acc@1  93.75 ( 96.03)	Acc@5 100.00 ( 99.96)
Epoch: [60][300/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 8.6242e-02 (1.1205e-01)	Acc@1  96.09 ( 96.06)	Acc@5 100.00 ( 99.96)
Epoch: [60][310/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.6347e-01 (1.1302e-01)	Acc@1  91.41 ( 96.03)	Acc@5 100.00 ( 99.96)
Epoch: [60][320/391]	Time  0.066 ( 0.064)	Data  0.001 ( 0.002)	Loss 8.8416e-02 (1.1256e-01)	Acc@1  95.31 ( 96.04)	Acc@5 100.00 ( 99.96)
Epoch: [60][330/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.6105e-02 (1.1203e-01)	Acc@1  97.66 ( 96.06)	Acc@5 100.00 ( 99.96)
Epoch: [60][340/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.2586e-01 (1.1125e-01)	Acc@1  95.31 ( 96.10)	Acc@5 100.00 ( 99.97)
Epoch: [60][350/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.2713e-01 (1.1106e-01)	Acc@1  95.31 ( 96.12)	Acc@5 100.00 ( 99.97)
Epoch: [60][360/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.4858e-01 (1.1145e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 ( 99.97)
Epoch: [60][370/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.2331e-01 (1.1141e-01)	Acc@1  95.31 ( 96.08)	Acc@5 100.00 ( 99.97)
Epoch: [60][380/391]	Time  0.066 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.3779e-01 (1.1135e-01)	Acc@1  95.31 ( 96.10)	Acc@5 100.00 ( 99.97)
Epoch: [60][390/391]	Time  0.047 ( 0.064)	Data  0.001 ( 0.002)	Loss 9.2958e-02 (1.1179e-01)	Acc@1  96.25 ( 96.09)	Acc@5 100.00 ( 99.97)
## e[60] optimizer.zero_grad (sum) time: 0.3999001979827881
## e[60]       loss.backward (sum) time: 6.90550971031189
## e[60]      optimizer.step (sum) time: 3.362445592880249
## epoch[60] training(only) time: 25.22144913673401
# Switched to evaluate mode...
Test: [  0/100]	Time  0.201 ( 0.201)	Loss 3.0711e-01 (3.0711e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.042)	Loss 3.4312e-01 (2.6825e-01)	Acc@1  89.00 ( 90.91)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.026 ( 0.034)	Loss 2.7767e-01 (2.8627e-01)	Acc@1  89.00 ( 90.81)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.026 ( 0.031)	Loss 3.3100e-01 (3.0208e-01)	Acc@1  91.00 ( 90.71)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 2.8687e-01 (3.1217e-01)	Acc@1  90.00 ( 90.44)	Acc@5  99.00 ( 99.68)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 2.5872e-01 (3.1855e-01)	Acc@1  93.00 ( 90.49)	Acc@5  99.00 ( 99.67)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 3.0032e-01 (3.1360e-01)	Acc@1  93.00 ( 90.51)	Acc@5 100.00 ( 99.70)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 4.7922e-01 (3.1053e-01)	Acc@1  87.00 ( 90.63)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.024 ( 0.028)	Loss 1.9892e-01 (3.1009e-01)	Acc@1  92.00 ( 90.53)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.026 ( 0.028)	Loss 2.4753e-01 (3.0737e-01)	Acc@1  94.00 ( 90.56)	Acc@5 100.00 ( 99.76)
 * Acc@1 90.580 Acc@5 99.760
### epoch[60] execution time: 28.121808767318726
EPOCH 61
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [61][  0/391]	Time  0.245 ( 0.245)	Data  0.179 ( 0.179)	Loss 6.4585e-02 (6.4585e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [61][ 10/391]	Time  0.059 ( 0.080)	Data  0.001 ( 0.017)	Loss 1.2524e-01 (9.4971e-02)	Acc@1  95.31 ( 97.02)	Acc@5 100.00 (100.00)
Epoch: [61][ 20/391]	Time  0.063 ( 0.071)	Data  0.001 ( 0.010)	Loss 1.1996e-01 (9.7875e-02)	Acc@1  96.09 ( 96.91)	Acc@5 100.00 (100.00)
Epoch: [61][ 30/391]	Time  0.059 ( 0.069)	Data  0.001 ( 0.007)	Loss 1.4799e-01 (1.0121e-01)	Acc@1  95.31 ( 96.67)	Acc@5 100.00 (100.00)
Epoch: [61][ 40/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.005)	Loss 9.6589e-02 (1.0446e-01)	Acc@1  96.09 ( 96.53)	Acc@5 100.00 (100.00)
Epoch: [61][ 50/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.005)	Loss 5.6881e-02 (1.0220e-01)	Acc@1  98.44 ( 96.58)	Acc@5 100.00 ( 99.98)
Epoch: [61][ 60/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.004)	Loss 1.0026e-01 (1.0473e-01)	Acc@1  97.66 ( 96.48)	Acc@5 100.00 ( 99.99)
Epoch: [61][ 70/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.004)	Loss 8.8702e-02 (1.0478e-01)	Acc@1  96.88 ( 96.47)	Acc@5 100.00 ( 99.99)
Epoch: [61][ 80/391]	Time  0.059 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.5832e-01 (1.0433e-01)	Acc@1  95.31 ( 96.52)	Acc@5 100.00 ( 99.99)
Epoch: [61][ 90/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.2693e-01 (1.0481e-01)	Acc@1  94.53 ( 96.47)	Acc@5 100.00 ( 99.99)
Epoch: [61][100/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 8.3921e-02 (1.0568e-01)	Acc@1  96.88 ( 96.45)	Acc@5 100.00 ( 99.98)
Epoch: [61][110/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 2.3777e-01 (1.0679e-01)	Acc@1  90.62 ( 96.35)	Acc@5 100.00 ( 99.99)
Epoch: [61][120/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.003)	Loss 9.7602e-02 (1.0524e-01)	Acc@1  96.88 ( 96.38)	Acc@5 100.00 ( 99.99)
Epoch: [61][130/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.003)	Loss 7.3697e-02 (1.0587e-01)	Acc@1  97.66 ( 96.37)	Acc@5 100.00 ( 99.99)
Epoch: [61][140/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.4239e-02 (1.0625e-01)	Acc@1  96.88 ( 96.38)	Acc@5 100.00 ( 99.99)
Epoch: [61][150/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.8097e-02 (1.0628e-01)	Acc@1  96.09 ( 96.37)	Acc@5 100.00 ( 99.98)
Epoch: [61][160/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.8616e-02 (1.0480e-01)	Acc@1  98.44 ( 96.44)	Acc@5 100.00 ( 99.99)
Epoch: [61][170/391]	Time  0.070 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.5159e-02 (1.0495e-01)	Acc@1  97.66 ( 96.44)	Acc@5 100.00 ( 99.99)
Epoch: [61][180/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.2894e-02 (1.0493e-01)	Acc@1  97.66 ( 96.44)	Acc@5 100.00 ( 99.98)
Epoch: [61][190/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.5431e-02 (1.0466e-01)	Acc@1  97.66 ( 96.44)	Acc@5 100.00 ( 99.98)
Epoch: [61][200/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6066e-01 (1.0408e-01)	Acc@1  96.88 ( 96.49)	Acc@5 100.00 ( 99.98)
Epoch: [61][210/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.0911e-02 (1.0370e-01)	Acc@1  99.22 ( 96.50)	Acc@5 100.00 ( 99.99)
Epoch: [61][220/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.0060e-02 (1.0444e-01)	Acc@1  98.44 ( 96.46)	Acc@5 100.00 ( 99.98)
Epoch: [61][230/391]	Time  0.059 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.6888e-02 (1.0484e-01)	Acc@1  96.88 ( 96.42)	Acc@5 100.00 ( 99.98)
Epoch: [61][240/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.1506e-01 (1.0511e-01)	Acc@1  95.31 ( 96.39)	Acc@5 100.00 ( 99.98)
Epoch: [61][250/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.1728e-01 (1.0524e-01)	Acc@1  92.19 ( 96.39)	Acc@5  99.22 ( 99.98)
Epoch: [61][260/391]	Time  0.060 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.7397e-01 (1.0589e-01)	Acc@1  92.19 ( 96.34)	Acc@5 100.00 ( 99.98)
Epoch: [61][270/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.7650e-02 (1.0531e-01)	Acc@1  98.44 ( 96.36)	Acc@5 100.00 ( 99.98)
Epoch: [61][280/391]	Time  0.060 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.1173e-01 (1.0523e-01)	Acc@1  96.88 ( 96.37)	Acc@5 100.00 ( 99.98)
Epoch: [61][290/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.2625e-02 (1.0522e-01)	Acc@1  96.88 ( 96.36)	Acc@5 100.00 ( 99.98)
Epoch: [61][300/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.5415e-02 (1.0532e-01)	Acc@1  98.44 ( 96.36)	Acc@5 100.00 ( 99.98)
Epoch: [61][310/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.8575e-02 (1.0513e-01)	Acc@1  96.88 ( 96.37)	Acc@5 100.00 ( 99.98)
Epoch: [61][320/391]	Time  0.067 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.2976e-02 (1.0468e-01)	Acc@1  99.22 ( 96.39)	Acc@5 100.00 ( 99.98)
Epoch: [61][330/391]	Time  0.067 ( 0.064)	Data  0.002 ( 0.002)	Loss 7.9226e-02 (1.0490e-01)	Acc@1  96.88 ( 96.37)	Acc@5 100.00 ( 99.98)
Epoch: [61][340/391]	Time  0.066 ( 0.064)	Data  0.002 ( 0.002)	Loss 5.4840e-02 (1.0491e-01)	Acc@1  98.44 ( 96.38)	Acc@5 100.00 ( 99.98)
Epoch: [61][350/391]	Time  0.064 ( 0.064)	Data  0.002 ( 0.002)	Loss 6.3813e-02 (1.0498e-01)	Acc@1  96.88 ( 96.38)	Acc@5 100.00 ( 99.98)
Epoch: [61][360/391]	Time  0.066 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.7572e-01 (1.0531e-01)	Acc@1  93.75 ( 96.35)	Acc@5 100.00 ( 99.98)
Epoch: [61][370/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.3531e-01 (1.0510e-01)	Acc@1  94.53 ( 96.35)	Acc@5 100.00 ( 99.98)
Epoch: [61][380/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 8.1983e-02 (1.0460e-01)	Acc@1  97.66 ( 96.38)	Acc@5 100.00 ( 99.98)
Epoch: [61][390/391]	Time  0.046 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.8887e-01 (1.0525e-01)	Acc@1  93.75 ( 96.35)	Acc@5 100.00 ( 99.98)
## e[61] optimizer.zero_grad (sum) time: 0.4024641513824463
## e[61]       loss.backward (sum) time: 6.947884798049927
## e[61]      optimizer.step (sum) time: 3.3022406101226807
## epoch[61] training(only) time: 25.211084842681885
# Switched to evaluate mode...
Test: [  0/100]	Time  0.193 ( 0.193)	Loss 3.1828e-01 (3.1828e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.027 ( 0.043)	Loss 3.5337e-01 (2.7334e-01)	Acc@1  88.00 ( 91.00)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.028 ( 0.034)	Loss 3.0515e-01 (2.8844e-01)	Acc@1  90.00 ( 91.24)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.024 ( 0.032)	Loss 3.2668e-01 (3.0100e-01)	Acc@1  91.00 ( 91.13)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.024 ( 0.030)	Loss 2.9035e-01 (3.1151e-01)	Acc@1  92.00 ( 90.78)	Acc@5  99.00 ( 99.68)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 2.6352e-01 (3.1534e-01)	Acc@1  92.00 ( 90.76)	Acc@5  99.00 ( 99.65)
Test: [ 60/100]	Time  0.024 ( 0.029)	Loss 2.9161e-01 (3.1121e-01)	Acc@1  93.00 ( 90.75)	Acc@5 100.00 ( 99.67)
Test: [ 70/100]	Time  0.027 ( 0.028)	Loss 4.5779e-01 (3.0732e-01)	Acc@1  88.00 ( 90.85)	Acc@5 100.00 ( 99.70)
Test: [ 80/100]	Time  0.024 ( 0.028)	Loss 2.0639e-01 (3.0616e-01)	Acc@1  93.00 ( 90.75)	Acc@5 100.00 ( 99.73)
Test: [ 90/100]	Time  0.024 ( 0.028)	Loss 2.2522e-01 (3.0272e-01)	Acc@1  94.00 ( 90.76)	Acc@5 100.00 ( 99.75)
 * Acc@1 90.790 Acc@5 99.750
### epoch[61] execution time: 28.113478183746338
EPOCH 62
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [62][  0/391]	Time  0.253 ( 0.253)	Data  0.188 ( 0.188)	Loss 1.1029e-01 (1.1029e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [62][ 10/391]	Time  0.064 ( 0.081)	Data  0.001 ( 0.018)	Loss 1.0818e-01 (9.6841e-02)	Acc@1  95.31 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [62][ 20/391]	Time  0.062 ( 0.073)	Data  0.001 ( 0.010)	Loss 1.6035e-01 (9.6157e-02)	Acc@1  94.53 ( 96.32)	Acc@5 100.00 (100.00)
Epoch: [62][ 30/391]	Time  0.063 ( 0.070)	Data  0.001 ( 0.007)	Loss 1.1166e-01 (9.4011e-02)	Acc@1  96.88 ( 96.47)	Acc@5 100.00 (100.00)
Epoch: [62][ 40/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.006)	Loss 7.5437e-02 (9.5596e-02)	Acc@1  96.88 ( 96.42)	Acc@5 100.00 ( 99.96)
Epoch: [62][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.005)	Loss 9.5153e-02 (9.5362e-02)	Acc@1  96.88 ( 96.40)	Acc@5 100.00 ( 99.97)
Epoch: [62][ 60/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.004)	Loss 9.9235e-02 (9.5267e-02)	Acc@1  96.88 ( 96.48)	Acc@5 100.00 ( 99.96)
Epoch: [62][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 7.5494e-02 (9.8493e-02)	Acc@1  96.09 ( 96.34)	Acc@5 100.00 ( 99.96)
Epoch: [62][ 80/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 7.5112e-02 (9.6788e-02)	Acc@1  96.88 ( 96.40)	Acc@5 100.00 ( 99.96)
Epoch: [62][ 90/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.3607e-01 (9.5085e-02)	Acc@1  94.53 ( 96.50)	Acc@5 100.00 ( 99.96)
Epoch: [62][100/391]	Time  0.060 ( 0.066)	Data  0.001 ( 0.003)	Loss 6.4761e-02 (9.5392e-02)	Acc@1  97.66 ( 96.47)	Acc@5 100.00 ( 99.96)
Epoch: [62][110/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.003)	Loss 8.3108e-02 (9.5868e-02)	Acc@1  96.88 ( 96.45)	Acc@5 100.00 ( 99.96)
Epoch: [62][120/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.5665e-01 (9.6700e-02)	Acc@1  94.53 ( 96.42)	Acc@5 100.00 ( 99.97)
Epoch: [62][130/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 7.3462e-02 (9.6563e-02)	Acc@1  96.88 ( 96.46)	Acc@5 100.00 ( 99.97)
Epoch: [62][140/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.3482e-02 (9.5561e-02)	Acc@1  99.22 ( 96.55)	Acc@5 100.00 ( 99.97)
Epoch: [62][150/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3536e-01 (9.5172e-02)	Acc@1  96.09 ( 96.59)	Acc@5 100.00 ( 99.97)
Epoch: [62][160/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.5185e-02 (9.6834e-02)	Acc@1  96.88 ( 96.51)	Acc@5 100.00 ( 99.98)
Epoch: [62][170/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0451e-01 (9.7074e-02)	Acc@1  97.66 ( 96.52)	Acc@5 100.00 ( 99.98)
Epoch: [62][180/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0908e-01 (9.6817e-02)	Acc@1  96.88 ( 96.54)	Acc@5 100.00 ( 99.98)
Epoch: [62][190/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4784e-01 (9.8280e-02)	Acc@1  94.53 ( 96.49)	Acc@5  99.22 ( 99.97)
Epoch: [62][200/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.6549e-02 (9.7866e-02)	Acc@1  98.44 ( 96.52)	Acc@5 100.00 ( 99.97)
Epoch: [62][210/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.3585e-02 (9.8770e-02)	Acc@1  97.66 ( 96.50)	Acc@5 100.00 ( 99.97)
Epoch: [62][220/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3932e-01 (9.9287e-02)	Acc@1  93.75 ( 96.47)	Acc@5 100.00 ( 99.98)
Epoch: [62][230/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.6087e-02 (9.9291e-02)	Acc@1  97.66 ( 96.48)	Acc@5 100.00 ( 99.98)
Epoch: [62][240/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.5728e-02 (9.9202e-02)	Acc@1  97.66 ( 96.48)	Acc@5 100.00 ( 99.98)
Epoch: [62][250/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1442e-01 (9.8835e-02)	Acc@1  95.31 ( 96.47)	Acc@5 100.00 ( 99.98)
Epoch: [62][260/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.1675e-02 (9.8024e-02)	Acc@1  96.88 ( 96.51)	Acc@5 100.00 ( 99.98)
Epoch: [62][270/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.3349e-02 (9.8464e-02)	Acc@1  97.66 ( 96.51)	Acc@5 100.00 ( 99.98)
Epoch: [62][280/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2339e-01 (9.8326e-02)	Acc@1  96.09 ( 96.52)	Acc@5 100.00 ( 99.98)
Epoch: [62][290/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.2012e-02 (9.8747e-02)	Acc@1  98.44 ( 96.49)	Acc@5 100.00 ( 99.98)
Epoch: [62][300/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1061e-01 (9.8620e-02)	Acc@1  95.31 ( 96.50)	Acc@5 100.00 ( 99.98)
Epoch: [62][310/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.1798e-02 (9.8743e-02)	Acc@1  96.09 ( 96.50)	Acc@5 100.00 ( 99.98)
Epoch: [62][320/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1822e-01 (9.8840e-02)	Acc@1  96.88 ( 96.51)	Acc@5 100.00 ( 99.98)
Epoch: [62][330/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.1831e-02 (9.9297e-02)	Acc@1  98.44 ( 96.50)	Acc@5 100.00 ( 99.98)
Epoch: [62][340/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7944e-01 (9.9491e-02)	Acc@1  92.19 ( 96.49)	Acc@5 100.00 ( 99.98)
Epoch: [62][350/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.3299e-02 (9.9315e-02)	Acc@1  96.09 ( 96.49)	Acc@5 100.00 ( 99.98)
Epoch: [62][360/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6920e-01 (9.9656e-02)	Acc@1  95.31 ( 96.49)	Acc@5 100.00 ( 99.98)
Epoch: [62][370/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.2928e-02 (9.9946e-02)	Acc@1  98.44 ( 96.47)	Acc@5 100.00 ( 99.98)
Epoch: [62][380/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.4695e-02 (9.9992e-02)	Acc@1  96.88 ( 96.46)	Acc@5 100.00 ( 99.98)
Epoch: [62][390/391]	Time  0.047 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.0406e-01 (1.0015e-01)	Acc@1  95.00 ( 96.46)	Acc@5 100.00 ( 99.98)
## e[62] optimizer.zero_grad (sum) time: 0.40088677406311035
## e[62]       loss.backward (sum) time: 6.957727909088135
## e[62]      optimizer.step (sum) time: 3.381782054901123
## epoch[62] training(only) time: 25.305118083953857
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 3.1906e-01 (3.1906e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.025 ( 0.040)	Loss 3.7553e-01 (2.7151e-01)	Acc@1  88.00 ( 91.09)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 2.9253e-01 (2.8717e-01)	Acc@1  91.00 ( 91.10)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 3.4978e-01 (3.0160e-01)	Acc@1  91.00 ( 91.03)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 3.0900e-01 (3.1163e-01)	Acc@1  91.00 ( 90.83)	Acc@5  99.00 ( 99.71)
Test: [ 50/100]	Time  0.026 ( 0.029)	Loss 2.5998e-01 (3.1654e-01)	Acc@1  93.00 ( 90.71)	Acc@5  99.00 ( 99.71)
Test: [ 60/100]	Time  0.026 ( 0.028)	Loss 2.9292e-01 (3.1260e-01)	Acc@1  95.00 ( 90.74)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.028 ( 0.028)	Loss 4.7638e-01 (3.0897e-01)	Acc@1  87.00 ( 90.86)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.028 ( 0.028)	Loss 1.8758e-01 (3.0789e-01)	Acc@1  93.00 ( 90.75)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.028 ( 0.027)	Loss 2.3603e-01 (3.0515e-01)	Acc@1  93.00 ( 90.77)	Acc@5 100.00 ( 99.78)
 * Acc@1 90.800 Acc@5 99.780
### epoch[62] execution time: 28.194910287857056
EPOCH 63
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [63][  0/391]	Time  0.254 ( 0.254)	Data  0.187 ( 0.187)	Loss 1.2245e-01 (1.2245e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [63][ 10/391]	Time  0.061 ( 0.081)	Data  0.001 ( 0.018)	Loss 9.1391e-02 (8.9250e-02)	Acc@1  96.88 ( 97.16)	Acc@5 100.00 (100.00)
Epoch: [63][ 20/391]	Time  0.062 ( 0.073)	Data  0.001 ( 0.010)	Loss 6.0281e-02 (9.0462e-02)	Acc@1  97.66 ( 97.14)	Acc@5 100.00 (100.00)
Epoch: [63][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.007)	Loss 1.3786e-01 (9.6444e-02)	Acc@1  95.31 ( 96.77)	Acc@5 100.00 ( 99.97)
Epoch: [63][ 40/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.006)	Loss 1.2397e-01 (9.2972e-02)	Acc@1  94.53 ( 96.88)	Acc@5 100.00 ( 99.98)
Epoch: [63][ 50/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.005)	Loss 1.3916e-01 (9.6042e-02)	Acc@1  96.88 ( 96.80)	Acc@5 100.00 ( 99.98)
Epoch: [63][ 60/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.004)	Loss 7.3440e-02 (9.3737e-02)	Acc@1  96.88 ( 96.86)	Acc@5 100.00 ( 99.97)
Epoch: [63][ 70/391]	Time  0.059 ( 0.067)	Data  0.001 ( 0.004)	Loss 4.6559e-02 (9.4063e-02)	Acc@1 100.00 ( 96.84)	Acc@5 100.00 ( 99.98)
Epoch: [63][ 80/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 7.4758e-02 (9.4598e-02)	Acc@1  97.66 ( 96.80)	Acc@5 100.00 ( 99.98)
Epoch: [63][ 90/391]	Time  0.060 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.0610e-01 (9.4738e-02)	Acc@1  95.31 ( 96.78)	Acc@5 100.00 ( 99.97)
Epoch: [63][100/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.0118e-01 (9.3503e-02)	Acc@1  97.66 ( 96.81)	Acc@5 100.00 ( 99.97)
Epoch: [63][110/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 5.6413e-02 (9.3020e-02)	Acc@1  98.44 ( 96.85)	Acc@5 100.00 ( 99.96)
Epoch: [63][120/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.6931e-01 (9.2692e-02)	Acc@1  93.75 ( 96.83)	Acc@5 100.00 ( 99.97)
Epoch: [63][130/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.003)	Loss 7.7336e-02 (9.3162e-02)	Acc@1  96.88 ( 96.83)	Acc@5 100.00 ( 99.96)
Epoch: [63][140/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.1433e-02 (9.2188e-02)	Acc@1  95.31 ( 96.85)	Acc@5 100.00 ( 99.96)
Epoch: [63][150/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3041e-01 (9.3288e-02)	Acc@1  95.31 ( 96.81)	Acc@5 100.00 ( 99.96)
Epoch: [63][160/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1206e-01 (9.3953e-02)	Acc@1  96.09 ( 96.79)	Acc@5 100.00 ( 99.97)
Epoch: [63][170/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6477e-01 (9.4265e-02)	Acc@1  93.75 ( 96.78)	Acc@5 100.00 ( 99.97)
Epoch: [63][180/391]	Time  0.069 ( 0.065)	Data  0.006 ( 0.002)	Loss 7.6634e-02 (9.4242e-02)	Acc@1  97.66 ( 96.78)	Acc@5 100.00 ( 99.97)
Epoch: [63][190/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.7681e-02 (9.4848e-02)	Acc@1  95.31 ( 96.76)	Acc@5 100.00 ( 99.96)
Epoch: [63][200/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.1899e-02 (9.4825e-02)	Acc@1  97.66 ( 96.77)	Acc@5 100.00 ( 99.96)
Epoch: [63][210/391]	Time  0.057 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2732e-01 (9.4986e-02)	Acc@1  94.53 ( 96.76)	Acc@5 100.00 ( 99.96)
Epoch: [63][220/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.7187e-02 (9.5003e-02)	Acc@1  96.09 ( 96.75)	Acc@5 100.00 ( 99.96)
Epoch: [63][230/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.2606e-02 (9.4966e-02)	Acc@1  99.22 ( 96.76)	Acc@5 100.00 ( 99.96)
Epoch: [63][240/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.0802e-02 (9.5100e-02)	Acc@1  97.66 ( 96.75)	Acc@5 100.00 ( 99.96)
Epoch: [63][250/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2574e-01 (9.4863e-02)	Acc@1  94.53 ( 96.76)	Acc@5 100.00 ( 99.97)
Epoch: [63][260/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.8300e-02 (9.5094e-02)	Acc@1  96.88 ( 96.76)	Acc@5 100.00 ( 99.97)
Epoch: [63][270/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.5484e-02 (9.5056e-02)	Acc@1  96.09 ( 96.75)	Acc@5 100.00 ( 99.97)
Epoch: [63][280/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.9524e-02 (9.4979e-02)	Acc@1  97.66 ( 96.74)	Acc@5 100.00 ( 99.97)
Epoch: [63][290/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.6268e-02 (9.4269e-02)	Acc@1  98.44 ( 96.77)	Acc@5 100.00 ( 99.97)
Epoch: [63][300/391]	Time  0.066 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.0450e-02 (9.4349e-02)	Acc@1  99.22 ( 96.76)	Acc@5 100.00 ( 99.97)
Epoch: [63][310/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.1483e-01 (9.4835e-02)	Acc@1  94.53 ( 96.74)	Acc@5 100.00 ( 99.97)
Epoch: [63][320/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 8.7167e-02 (9.5077e-02)	Acc@1  96.09 ( 96.72)	Acc@5 100.00 ( 99.97)
Epoch: [63][330/391]	Time  0.067 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.1934e-01 (9.4838e-02)	Acc@1  96.09 ( 96.72)	Acc@5 100.00 ( 99.97)
Epoch: [63][340/391]	Time  0.059 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.3097e-01 (9.4611e-02)	Acc@1  94.53 ( 96.72)	Acc@5 100.00 ( 99.97)
Epoch: [63][350/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.1280e-01 (9.5344e-02)	Acc@1  94.53 ( 96.71)	Acc@5 100.00 ( 99.97)
Epoch: [63][360/391]	Time  0.066 ( 0.064)	Data  0.001 ( 0.002)	Loss 8.2614e-02 (9.4824e-02)	Acc@1  96.09 ( 96.72)	Acc@5 100.00 ( 99.97)
Epoch: [63][370/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.9472e-02 (9.4331e-02)	Acc@1  98.44 ( 96.74)	Acc@5 100.00 ( 99.97)
Epoch: [63][380/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.2592e-01 (9.4884e-02)	Acc@1  95.31 ( 96.71)	Acc@5 100.00 ( 99.97)
Epoch: [63][390/391]	Time  0.047 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.0601e-02 (9.4985e-02)	Acc@1  98.75 ( 96.71)	Acc@5 100.00 ( 99.97)
## e[63] optimizer.zero_grad (sum) time: 0.4041600227355957
## e[63]       loss.backward (sum) time: 6.952449560165405
## e[63]      optimizer.step (sum) time: 3.2923829555511475
## epoch[63] training(only) time: 25.21557855606079
# Switched to evaluate mode...
Test: [  0/100]	Time  0.201 ( 0.201)	Loss 3.1396e-01 (3.1396e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.043)	Loss 3.6792e-01 (2.7308e-01)	Acc@1  89.00 ( 91.18)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.024 ( 0.035)	Loss 2.9824e-01 (2.8789e-01)	Acc@1  90.00 ( 91.00)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.027 ( 0.032)	Loss 3.3937e-01 (2.9992e-01)	Acc@1  91.00 ( 91.03)	Acc@5 100.00 ( 99.77)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 2.9418e-01 (3.0949e-01)	Acc@1  91.00 ( 90.83)	Acc@5  99.00 ( 99.76)
Test: [ 50/100]	Time  0.027 ( 0.030)	Loss 2.5602e-01 (3.1316e-01)	Acc@1  92.00 ( 90.73)	Acc@5  99.00 ( 99.75)
Test: [ 60/100]	Time  0.024 ( 0.029)	Loss 3.0291e-01 (3.0948e-01)	Acc@1  92.00 ( 90.77)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.024 ( 0.029)	Loss 4.6184e-01 (3.0592e-01)	Acc@1  86.00 ( 90.80)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.024 ( 0.028)	Loss 1.8663e-01 (3.0503e-01)	Acc@1  93.00 ( 90.75)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.027 ( 0.028)	Loss 2.3104e-01 (3.0192e-01)	Acc@1  93.00 ( 90.79)	Acc@5 100.00 ( 99.81)
 * Acc@1 90.850 Acc@5 99.810
### epoch[63] execution time: 28.14075207710266
EPOCH 64
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [64][  0/391]	Time  0.256 ( 0.256)	Data  0.191 ( 0.191)	Loss 1.0858e-01 (1.0858e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [64][ 10/391]	Time  0.065 ( 0.082)	Data  0.001 ( 0.018)	Loss 8.4453e-02 (9.2916e-02)	Acc@1  97.66 ( 96.52)	Acc@5 100.00 (100.00)
Epoch: [64][ 20/391]	Time  0.063 ( 0.073)	Data  0.001 ( 0.010)	Loss 1.5777e-01 (1.0042e-01)	Acc@1  94.53 ( 96.32)	Acc@5 100.00 (100.00)
Epoch: [64][ 30/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.007)	Loss 6.3309e-02 (9.4998e-02)	Acc@1  98.44 ( 96.67)	Acc@5 100.00 (100.00)
Epoch: [64][ 40/391]	Time  0.061 ( 0.068)	Data  0.001 ( 0.006)	Loss 1.1752e-01 (9.1191e-02)	Acc@1  96.09 ( 96.86)	Acc@5  99.22 ( 99.98)
Epoch: [64][ 50/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.005)	Loss 4.6845e-02 (8.7451e-02)	Acc@1  99.22 ( 97.00)	Acc@5 100.00 ( 99.97)
Epoch: [64][ 60/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.004)	Loss 7.8908e-02 (8.8782e-02)	Acc@1  97.66 ( 96.98)	Acc@5 100.00 ( 99.96)
Epoch: [64][ 70/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.004)	Loss 9.8991e-02 (9.0433e-02)	Acc@1  96.09 ( 96.95)	Acc@5 100.00 ( 99.97)
Epoch: [64][ 80/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.2817e-01 (9.0268e-02)	Acc@1  96.88 ( 96.98)	Acc@5 100.00 ( 99.97)
Epoch: [64][ 90/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.0999e-01 (9.0856e-02)	Acc@1  93.75 ( 96.91)	Acc@5 100.00 ( 99.97)
Epoch: [64][100/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.003)	Loss 8.9376e-02 (9.1955e-02)	Acc@1  97.66 ( 96.85)	Acc@5 100.00 ( 99.98)
Epoch: [64][110/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.003)	Loss 5.4947e-02 (9.4733e-02)	Acc@1  98.44 ( 96.69)	Acc@5 100.00 ( 99.97)
Epoch: [64][120/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.003)	Loss 9.0762e-02 (9.5803e-02)	Acc@1  97.66 ( 96.66)	Acc@5 100.00 ( 99.97)
Epoch: [64][130/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.0832e-01 (9.6072e-02)	Acc@1  96.88 ( 96.65)	Acc@5 100.00 ( 99.97)
Epoch: [64][140/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 9.2187e-02 (9.6741e-02)	Acc@1  96.09 ( 96.63)	Acc@5 100.00 ( 99.97)
Epoch: [64][150/391]	Time  0.070 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.1895e-01 (9.5152e-02)	Acc@1  96.09 ( 96.70)	Acc@5 100.00 ( 99.97)
Epoch: [64][160/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4237e-01 (9.5245e-02)	Acc@1  94.53 ( 96.72)	Acc@5 100.00 ( 99.98)
Epoch: [64][170/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5281e-01 (9.5129e-02)	Acc@1  96.09 ( 96.74)	Acc@5 100.00 ( 99.98)
Epoch: [64][180/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2958e-01 (9.5045e-02)	Acc@1  96.09 ( 96.72)	Acc@5 100.00 ( 99.98)
Epoch: [64][190/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4151e-01 (9.5466e-02)	Acc@1  95.31 ( 96.71)	Acc@5 100.00 ( 99.98)
Epoch: [64][200/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6751e-01 (9.7010e-02)	Acc@1  92.19 ( 96.67)	Acc@5 100.00 ( 99.97)
Epoch: [64][210/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0778e-01 (9.7173e-02)	Acc@1  96.88 ( 96.66)	Acc@5 100.00 ( 99.97)
Epoch: [64][220/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.2779e-01 (9.6891e-02)	Acc@1  96.09 ( 96.67)	Acc@5 100.00 ( 99.97)
Epoch: [64][230/391]	Time  0.067 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.4833e-02 (9.7254e-02)	Acc@1  97.66 ( 96.64)	Acc@5 100.00 ( 99.97)
Epoch: [64][240/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.0724e-02 (9.7356e-02)	Acc@1  96.88 ( 96.63)	Acc@5 100.00 ( 99.97)
Epoch: [64][250/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.0078e-01 (9.7811e-02)	Acc@1  96.88 ( 96.59)	Acc@5 100.00 ( 99.98)
Epoch: [64][260/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.9201e-02 (9.7593e-02)	Acc@1  98.44 ( 96.59)	Acc@5 100.00 ( 99.98)
Epoch: [64][270/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.8901e-01 (9.8287e-02)	Acc@1  91.41 ( 96.58)	Acc@5  99.22 ( 99.97)
Epoch: [64][280/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.1194e-01 (9.7709e-02)	Acc@1  96.88 ( 96.61)	Acc@5 100.00 ( 99.97)
Epoch: [64][290/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.1572e-02 (9.7987e-02)	Acc@1  99.22 ( 96.61)	Acc@5 100.00 ( 99.97)
Epoch: [64][300/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 8.3438e-02 (9.7481e-02)	Acc@1  96.88 ( 96.63)	Acc@5 100.00 ( 99.97)
Epoch: [64][310/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 9.4305e-02 (9.7798e-02)	Acc@1  96.88 ( 96.61)	Acc@5 100.00 ( 99.97)
Epoch: [64][320/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.1842e-02 (9.7299e-02)	Acc@1  97.66 ( 96.63)	Acc@5 100.00 ( 99.97)
Epoch: [64][330/391]	Time  0.066 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.5204e-02 (9.7008e-02)	Acc@1  97.66 ( 96.63)	Acc@5 100.00 ( 99.97)
Epoch: [64][340/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.1091e-01 (9.7332e-02)	Acc@1  95.31 ( 96.63)	Acc@5 100.00 ( 99.97)
Epoch: [64][350/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 8.0220e-02 (9.7176e-02)	Acc@1  97.66 ( 96.64)	Acc@5 100.00 ( 99.97)
Epoch: [64][360/391]	Time  0.068 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.2863e-02 (9.7156e-02)	Acc@1  99.22 ( 96.64)	Acc@5 100.00 ( 99.97)
Epoch: [64][370/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.7786e-02 (9.7392e-02)	Acc@1  98.44 ( 96.63)	Acc@5 100.00 ( 99.97)
Epoch: [64][380/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.5929e-01 (9.7788e-02)	Acc@1  96.09 ( 96.63)	Acc@5 100.00 ( 99.97)
Epoch: [64][390/391]	Time  0.057 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.7863e-02 (9.7942e-02)	Acc@1  97.50 ( 96.62)	Acc@5 100.00 ( 99.97)
## e[64] optimizer.zero_grad (sum) time: 0.40061211585998535
## e[64]       loss.backward (sum) time: 6.938982009887695
## e[64]      optimizer.step (sum) time: 3.323439598083496
## epoch[64] training(only) time: 25.24760413169861
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 3.0876e-01 (3.0876e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.025 ( 0.040)	Loss 3.7488e-01 (2.7261e-01)	Acc@1  88.00 ( 91.00)	Acc@5 100.00 ( 99.55)
Test: [ 20/100]	Time  0.028 ( 0.034)	Loss 2.9282e-01 (2.8780e-01)	Acc@1  91.00 ( 90.81)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 3.3978e-01 (3.0175e-01)	Acc@1  91.00 ( 90.81)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.024 ( 0.030)	Loss 3.1293e-01 (3.1182e-01)	Acc@1  91.00 ( 90.61)	Acc@5  99.00 ( 99.68)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 2.6698e-01 (3.1466e-01)	Acc@1  93.00 ( 90.59)	Acc@5  99.00 ( 99.69)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 3.0962e-01 (3.1116e-01)	Acc@1  93.00 ( 90.59)	Acc@5 100.00 ( 99.72)
Test: [ 70/100]	Time  0.027 ( 0.028)	Loss 4.7220e-01 (3.0786e-01)	Acc@1  86.00 ( 90.59)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.027 ( 0.028)	Loss 1.8381e-01 (3.0698e-01)	Acc@1  94.00 ( 90.56)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 2.1421e-01 (3.0379e-01)	Acc@1  94.00 ( 90.66)	Acc@5 100.00 ( 99.77)
 * Acc@1 90.740 Acc@5 99.770
### epoch[64] execution time: 28.066574573516846
EPOCH 65
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [65][  0/391]	Time  0.255 ( 0.255)	Data  0.186 ( 0.186)	Loss 1.0014e-01 (1.0014e-01)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [65][ 10/391]	Time  0.068 ( 0.082)	Data  0.001 ( 0.018)	Loss 5.6801e-02 (8.6604e-02)	Acc@1  97.66 ( 97.30)	Acc@5 100.00 ( 99.86)
Epoch: [65][ 20/391]	Time  0.062 ( 0.073)	Data  0.001 ( 0.010)	Loss 9.9686e-02 (9.2480e-02)	Acc@1  98.44 ( 96.99)	Acc@5 100.00 ( 99.93)
Epoch: [65][ 30/391]	Time  0.062 ( 0.070)	Data  0.001 ( 0.007)	Loss 7.2423e-02 (9.9853e-02)	Acc@1  98.44 ( 96.72)	Acc@5 100.00 ( 99.90)
Epoch: [65][ 40/391]	Time  0.062 ( 0.069)	Data  0.001 ( 0.006)	Loss 9.7795e-02 (1.0009e-01)	Acc@1  97.66 ( 96.72)	Acc@5 100.00 ( 99.92)
Epoch: [65][ 50/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.005)	Loss 1.1766e-01 (9.7027e-02)	Acc@1  96.09 ( 96.78)	Acc@5 100.00 ( 99.94)
Epoch: [65][ 60/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.004)	Loss 9.9772e-02 (9.6848e-02)	Acc@1  95.31 ( 96.81)	Acc@5 100.00 ( 99.95)
Epoch: [65][ 70/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.004)	Loss 7.5175e-02 (9.6277e-02)	Acc@1  95.31 ( 96.78)	Acc@5 100.00 ( 99.96)
Epoch: [65][ 80/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 7.7268e-02 (9.5959e-02)	Acc@1  97.66 ( 96.75)	Acc@5 100.00 ( 99.96)
Epoch: [65][ 90/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 5.6321e-02 (9.4671e-02)	Acc@1  98.44 ( 96.81)	Acc@5 100.00 ( 99.97)
Epoch: [65][100/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.2296e-01 (9.5964e-02)	Acc@1  96.88 ( 96.70)	Acc@5 100.00 ( 99.97)
Epoch: [65][110/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.3071e-01 (9.4594e-02)	Acc@1  94.53 ( 96.71)	Acc@5 100.00 ( 99.97)
Epoch: [65][120/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 5.9243e-02 (9.3792e-02)	Acc@1  99.22 ( 96.78)	Acc@5 100.00 ( 99.97)
Epoch: [65][130/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.0188e-01 (9.3221e-02)	Acc@1  95.31 ( 96.80)	Acc@5 100.00 ( 99.98)
Epoch: [65][140/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.9488e-02 (9.2644e-02)	Acc@1  97.66 ( 96.82)	Acc@5 100.00 ( 99.98)
Epoch: [65][150/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3284e-01 (9.2273e-02)	Acc@1  94.53 ( 96.83)	Acc@5 100.00 ( 99.98)
Epoch: [65][160/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.5600e-02 (9.1715e-02)	Acc@1  96.88 ( 96.87)	Acc@5 100.00 ( 99.98)
Epoch: [65][170/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0181e-01 (9.2144e-02)	Acc@1  96.88 ( 96.85)	Acc@5 100.00 ( 99.98)
Epoch: [65][180/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7416e-01 (9.3008e-02)	Acc@1  92.97 ( 96.84)	Acc@5 100.00 ( 99.98)
Epoch: [65][190/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.1677e-02 (9.3831e-02)	Acc@1  97.66 ( 96.81)	Acc@5 100.00 ( 99.98)
Epoch: [65][200/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.8718e-02 (9.3048e-02)	Acc@1  96.88 ( 96.84)	Acc@5 100.00 ( 99.98)
Epoch: [65][210/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.6318e-02 (9.2977e-02)	Acc@1  98.44 ( 96.85)	Acc@5 100.00 ( 99.98)
Epoch: [65][220/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.1619e-02 (9.3546e-02)	Acc@1  97.66 ( 96.80)	Acc@5 100.00 ( 99.98)
Epoch: [65][230/391]	Time  0.058 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.5968e-02 (9.3690e-02)	Acc@1  97.66 ( 96.79)	Acc@5 100.00 ( 99.98)
Epoch: [65][240/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.4493e-02 (9.2827e-02)	Acc@1  96.88 ( 96.83)	Acc@5 100.00 ( 99.98)
Epoch: [65][250/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 8.0330e-02 (9.2907e-02)	Acc@1  96.88 ( 96.82)	Acc@5 100.00 ( 99.98)
Epoch: [65][260/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.3308e-01 (9.3120e-02)	Acc@1  96.09 ( 96.81)	Acc@5 100.00 ( 99.98)
Epoch: [65][270/391]	Time  0.066 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.3257e-02 (9.3100e-02)	Acc@1  99.22 ( 96.81)	Acc@5 100.00 ( 99.98)
Epoch: [65][280/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 9.0735e-02 (9.3328e-02)	Acc@1  97.66 ( 96.80)	Acc@5  99.22 ( 99.97)
Epoch: [65][290/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.5710e-01 (9.3346e-02)	Acc@1  93.75 ( 96.80)	Acc@5 100.00 ( 99.98)
Epoch: [65][300/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.1298e-01 (9.3552e-02)	Acc@1  96.88 ( 96.79)	Acc@5 100.00 ( 99.98)
Epoch: [65][310/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 9.2893e-02 (9.3986e-02)	Acc@1  96.88 ( 96.79)	Acc@5 100.00 ( 99.98)
Epoch: [65][320/391]	Time  0.067 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.9853e-02 (9.4423e-02)	Acc@1  98.44 ( 96.75)	Acc@5 100.00 ( 99.98)
Epoch: [65][330/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.4946e-02 (9.4183e-02)	Acc@1 100.00 ( 96.77)	Acc@5 100.00 ( 99.98)
Epoch: [65][340/391]	Time  0.060 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.1990e-02 (9.3855e-02)	Acc@1  98.44 ( 96.77)	Acc@5 100.00 ( 99.98)
Epoch: [65][350/391]	Time  0.067 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.5089e-01 (9.4107e-02)	Acc@1  96.09 ( 96.77)	Acc@5 100.00 ( 99.98)
Epoch: [65][360/391]	Time  0.067 ( 0.064)	Data  0.001 ( 0.002)	Loss 8.5622e-02 (9.3993e-02)	Acc@1  96.88 ( 96.78)	Acc@5 100.00 ( 99.98)
Epoch: [65][370/391]	Time  0.077 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.1177e-02 (9.3829e-02)	Acc@1  99.22 ( 96.78)	Acc@5 100.00 ( 99.98)
Epoch: [65][380/391]	Time  0.067 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.8521e-01 (9.3725e-02)	Acc@1  94.53 ( 96.80)	Acc@5 100.00 ( 99.98)
Epoch: [65][390/391]	Time  0.046 ( 0.064)	Data  0.001 ( 0.002)	Loss 9.9996e-02 (9.3514e-02)	Acc@1  96.25 ( 96.81)	Acc@5 100.00 ( 99.98)
## e[65] optimizer.zero_grad (sum) time: 0.40348052978515625
## e[65]       loss.backward (sum) time: 6.965560674667358
## e[65]      optimizer.step (sum) time: 3.321497678756714
## epoch[65] training(only) time: 25.206568241119385
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 3.1179e-01 (3.1179e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.040)	Loss 3.8737e-01 (2.7397e-01)	Acc@1  90.00 ( 91.45)	Acc@5 100.00 ( 99.55)
Test: [ 20/100]	Time  0.026 ( 0.033)	Loss 2.8923e-01 (2.9100e-01)	Acc@1  90.00 ( 91.10)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.027 ( 0.031)	Loss 3.4479e-01 (3.0478e-01)	Acc@1  92.00 ( 91.03)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 3.0712e-01 (3.1412e-01)	Acc@1  92.00 ( 90.90)	Acc@5  99.00 ( 99.68)
Test: [ 50/100]	Time  0.028 ( 0.029)	Loss 2.4927e-01 (3.1772e-01)	Acc@1  93.00 ( 90.86)	Acc@5  99.00 ( 99.69)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 3.1978e-01 (3.1387e-01)	Acc@1  93.00 ( 90.75)	Acc@5 100.00 ( 99.72)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 5.0397e-01 (3.1144e-01)	Acc@1  86.00 ( 90.76)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.024 ( 0.028)	Loss 1.7946e-01 (3.1067e-01)	Acc@1  94.00 ( 90.73)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 2.3418e-01 (3.0730e-01)	Acc@1  93.00 ( 90.79)	Acc@5 100.00 ( 99.78)
 * Acc@1 90.840 Acc@5 99.780
### epoch[65] execution time: 28.07586169242859
EPOCH 66
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [66][  0/391]	Time  0.245 ( 0.245)	Data  0.184 ( 0.184)	Loss 4.5082e-02 (4.5082e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [66][ 10/391]	Time  0.058 ( 0.079)	Data  0.001 ( 0.018)	Loss 1.1561e-01 (9.0368e-02)	Acc@1  96.88 ( 96.95)	Acc@5 100.00 (100.00)
Epoch: [66][ 20/391]	Time  0.063 ( 0.072)	Data  0.001 ( 0.010)	Loss 1.1123e-01 (9.3844e-02)	Acc@1  96.09 ( 96.84)	Acc@5 100.00 (100.00)
Epoch: [66][ 30/391]	Time  0.065 ( 0.070)	Data  0.001 ( 0.007)	Loss 7.9618e-02 (8.9559e-02)	Acc@1  96.88 ( 96.75)	Acc@5 100.00 (100.00)
Epoch: [66][ 40/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.006)	Loss 7.1257e-02 (8.6555e-02)	Acc@1  96.88 ( 97.01)	Acc@5 100.00 ( 99.98)
Epoch: [66][ 50/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.005)	Loss 1.0395e-01 (8.7126e-02)	Acc@1  96.09 ( 96.92)	Acc@5 100.00 ( 99.98)
Epoch: [66][ 60/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.004)	Loss 7.2046e-02 (8.4769e-02)	Acc@1  97.66 ( 96.95)	Acc@5 100.00 ( 99.99)
Epoch: [66][ 70/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.004)	Loss 7.5652e-02 (8.7464e-02)	Acc@1  96.88 ( 96.89)	Acc@5 100.00 ( 99.99)
Epoch: [66][ 80/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 6.9392e-02 (8.6619e-02)	Acc@1  97.66 ( 96.90)	Acc@5 100.00 ( 99.99)
Epoch: [66][ 90/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.2940e-01 (8.7723e-02)	Acc@1  94.53 ( 96.89)	Acc@5  99.22 ( 99.97)
Epoch: [66][100/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 6.2802e-02 (8.7758e-02)	Acc@1  98.44 ( 96.87)	Acc@5 100.00 ( 99.97)
Epoch: [66][110/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 3.0577e-02 (8.7511e-02)	Acc@1  99.22 ( 96.90)	Acc@5 100.00 ( 99.97)
Epoch: [66][120/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.2171e-01 (8.9756e-02)	Acc@1  94.53 ( 96.82)	Acc@5 100.00 ( 99.97)
Epoch: [66][130/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.003)	Loss 7.6538e-02 (9.1200e-02)	Acc@1  98.44 ( 96.77)	Acc@5 100.00 ( 99.98)
Epoch: [66][140/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.6707e-02 (9.1217e-02)	Acc@1  98.44 ( 96.79)	Acc@5 100.00 ( 99.98)
Epoch: [66][150/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.5009e-02 (9.0913e-02)	Acc@1  96.88 ( 96.78)	Acc@5 100.00 ( 99.98)
Epoch: [66][160/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.8182e-02 (9.1854e-02)	Acc@1  97.66 ( 96.71)	Acc@5 100.00 ( 99.98)
Epoch: [66][170/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.1658e-02 (9.1889e-02)	Acc@1  98.44 ( 96.70)	Acc@5 100.00 ( 99.98)
Epoch: [66][180/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3184e-01 (9.2644e-02)	Acc@1  96.88 ( 96.66)	Acc@5  99.22 ( 99.97)
Epoch: [66][190/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.2478e-02 (9.2843e-02)	Acc@1  96.88 ( 96.65)	Acc@5 100.00 ( 99.98)
Epoch: [66][200/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.6924e-02 (9.3502e-02)	Acc@1  98.44 ( 96.64)	Acc@5 100.00 ( 99.97)
Epoch: [66][210/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.7329e-02 (9.3073e-02)	Acc@1  97.66 ( 96.66)	Acc@5 100.00 ( 99.97)
Epoch: [66][220/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.5448e-02 (9.2239e-02)	Acc@1  96.88 ( 96.69)	Acc@5 100.00 ( 99.97)
Epoch: [66][230/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.7404e-02 (9.1637e-02)	Acc@1  98.44 ( 96.73)	Acc@5 100.00 ( 99.97)
Epoch: [66][240/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.6520e-02 (9.1677e-02)	Acc@1  98.44 ( 96.74)	Acc@5 100.00 ( 99.97)
Epoch: [66][250/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.9245e-02 (9.1407e-02)	Acc@1  98.44 ( 96.76)	Acc@5 100.00 ( 99.98)
Epoch: [66][260/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.4094e-02 (9.1565e-02)	Acc@1  98.44 ( 96.75)	Acc@5 100.00 ( 99.98)
Epoch: [66][270/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5700e-01 (9.2025e-02)	Acc@1  95.31 ( 96.75)	Acc@5  99.22 ( 99.97)
Epoch: [66][280/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.6522e-02 (9.1865e-02)	Acc@1  99.22 ( 96.75)	Acc@5 100.00 ( 99.97)
Epoch: [66][290/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.5610e-01 (9.2436e-02)	Acc@1  92.97 ( 96.73)	Acc@5 100.00 ( 99.97)
Epoch: [66][300/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 8.6155e-02 (9.2661e-02)	Acc@1  97.66 ( 96.72)	Acc@5 100.00 ( 99.97)
Epoch: [66][310/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.1989e-02 (9.2606e-02)	Acc@1  96.88 ( 96.73)	Acc@5 100.00 ( 99.97)
Epoch: [66][320/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.4705e-01 (9.3086e-02)	Acc@1  95.31 ( 96.71)	Acc@5 100.00 ( 99.97)
Epoch: [66][330/391]	Time  0.067 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.6652e-02 (9.3457e-02)	Acc@1  97.66 ( 96.69)	Acc@5 100.00 ( 99.97)
Epoch: [66][340/391]	Time  0.067 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.3397e-01 (9.3719e-02)	Acc@1  93.75 ( 96.68)	Acc@5 100.00 ( 99.97)
Epoch: [66][350/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.5304e-02 (9.4124e-02)	Acc@1  97.66 ( 96.66)	Acc@5 100.00 ( 99.97)
Epoch: [66][360/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.0125e-02 (9.4375e-02)	Acc@1  97.66 ( 96.67)	Acc@5 100.00 ( 99.97)
Epoch: [66][370/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 9.8318e-02 (9.4783e-02)	Acc@1  96.88 ( 96.66)	Acc@5 100.00 ( 99.97)
Epoch: [66][380/391]	Time  0.071 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.4291e-01 (9.4941e-02)	Acc@1  93.75 ( 96.66)	Acc@5 100.00 ( 99.98)
Epoch: [66][390/391]	Time  0.047 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.2065e-01 (9.5238e-02)	Acc@1  95.00 ( 96.65)	Acc@5 100.00 ( 99.98)
## e[66] optimizer.zero_grad (sum) time: 0.39689040184020996
## e[66]       loss.backward (sum) time: 6.9464192390441895
## e[66]      optimizer.step (sum) time: 3.353053569793701
## epoch[66] training(only) time: 25.200603008270264
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 3.2797e-01 (3.2797e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.025 ( 0.040)	Loss 3.8027e-01 (2.7389e-01)	Acc@1  89.00 ( 91.36)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 2.9329e-01 (2.9078e-01)	Acc@1  91.00 ( 91.33)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 3.5768e-01 (3.0475e-01)	Acc@1  90.00 ( 91.23)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.025 ( 0.029)	Loss 3.1093e-01 (3.1474e-01)	Acc@1  91.00 ( 90.90)	Acc@5  99.00 ( 99.73)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 2.5787e-01 (3.1906e-01)	Acc@1  93.00 ( 90.71)	Acc@5  99.00 ( 99.73)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 3.2110e-01 (3.1451e-01)	Acc@1  93.00 ( 90.67)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 4.8331e-01 (3.1105e-01)	Acc@1  86.00 ( 90.75)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.027 ( 0.027)	Loss 1.8588e-01 (3.0983e-01)	Acc@1  93.00 ( 90.70)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 2.2848e-01 (3.0665e-01)	Acc@1  93.00 ( 90.78)	Acc@5 100.00 ( 99.78)
 * Acc@1 90.840 Acc@5 99.780
### epoch[66] execution time: 28.05008554458618
EPOCH 67
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [67][  0/391]	Time  0.257 ( 0.257)	Data  0.195 ( 0.195)	Loss 6.4768e-02 (6.4768e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [67][ 10/391]	Time  0.062 ( 0.082)	Data  0.001 ( 0.019)	Loss 6.1348e-02 (9.8599e-02)	Acc@1  97.66 ( 96.16)	Acc@5 100.00 (100.00)
Epoch: [67][ 20/391]	Time  0.067 ( 0.073)	Data  0.001 ( 0.010)	Loss 7.1275e-02 (1.0433e-01)	Acc@1  97.66 ( 95.94)	Acc@5 100.00 (100.00)
Epoch: [67][ 30/391]	Time  0.065 ( 0.071)	Data  0.001 ( 0.007)	Loss 5.9119e-02 (9.6483e-02)	Acc@1  97.66 ( 96.40)	Acc@5 100.00 (100.00)
Epoch: [67][ 40/391]	Time  0.063 ( 0.069)	Data  0.001 ( 0.006)	Loss 5.9227e-02 (9.1861e-02)	Acc@1  98.44 ( 96.65)	Acc@5 100.00 (100.00)
Epoch: [67][ 50/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.005)	Loss 6.7183e-02 (9.0266e-02)	Acc@1  98.44 ( 96.77)	Acc@5 100.00 (100.00)
Epoch: [67][ 60/391]	Time  0.071 ( 0.067)	Data  0.001 ( 0.004)	Loss 7.0897e-02 (8.8788e-02)	Acc@1  96.88 ( 96.81)	Acc@5 100.00 (100.00)
Epoch: [67][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 8.6756e-02 (9.1062e-02)	Acc@1  97.66 ( 96.80)	Acc@5 100.00 (100.00)
Epoch: [67][ 80/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 9.3765e-02 (9.0909e-02)	Acc@1  96.88 ( 96.77)	Acc@5 100.00 ( 99.99)
Epoch: [67][ 90/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.0396e-01 (9.1860e-02)	Acc@1  96.09 ( 96.79)	Acc@5 100.00 ( 99.97)
Epoch: [67][100/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.9794e-01 (9.1416e-02)	Acc@1  94.53 ( 96.85)	Acc@5  99.22 ( 99.97)
Epoch: [67][110/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.003)	Loss 6.1435e-02 (9.0764e-02)	Acc@1  98.44 ( 96.86)	Acc@5 100.00 ( 99.97)
Epoch: [67][120/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.1810e-01 (9.0011e-02)	Acc@1  94.53 ( 96.86)	Acc@5 100.00 ( 99.97)
Epoch: [67][130/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.5584e-01 (9.1487e-02)	Acc@1  96.88 ( 96.84)	Acc@5 100.00 ( 99.98)
Epoch: [67][140/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.8857e-02 (9.1975e-02)	Acc@1  95.31 ( 96.82)	Acc@5 100.00 ( 99.97)
Epoch: [67][150/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.5867e-02 (9.2121e-02)	Acc@1  99.22 ( 96.84)	Acc@5 100.00 ( 99.97)
Epoch: [67][160/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.4536e-02 (9.2508e-02)	Acc@1  96.88 ( 96.82)	Acc@5 100.00 ( 99.98)
Epoch: [67][170/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.6421e-02 (9.2649e-02)	Acc@1  98.44 ( 96.84)	Acc@5 100.00 ( 99.98)
Epoch: [67][180/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.5159e-02 (9.3114e-02)	Acc@1  96.88 ( 96.83)	Acc@5  99.22 ( 99.97)
Epoch: [67][190/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0942e-01 (9.4064e-02)	Acc@1  94.53 ( 96.77)	Acc@5  99.22 ( 99.97)
Epoch: [67][200/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.4768e-02 (9.3831e-02)	Acc@1  97.66 ( 96.78)	Acc@5 100.00 ( 99.97)
Epoch: [67][210/391]	Time  0.058 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.7545e-02 (9.4360e-02)	Acc@1  99.22 ( 96.73)	Acc@5 100.00 ( 99.97)
Epoch: [67][220/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.0788e-02 (9.4233e-02)	Acc@1  96.09 ( 96.74)	Acc@5 100.00 ( 99.97)
Epoch: [67][230/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.4743e-02 (9.4251e-02)	Acc@1  96.88 ( 96.71)	Acc@5 100.00 ( 99.97)
Epoch: [67][240/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.9671e-02 (9.4120e-02)	Acc@1  98.44 ( 96.70)	Acc@5 100.00 ( 99.97)
Epoch: [67][250/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.3642e-02 (9.3152e-02)	Acc@1  98.44 ( 96.74)	Acc@5 100.00 ( 99.97)
Epoch: [67][260/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1718e-01 (9.3166e-02)	Acc@1  95.31 ( 96.75)	Acc@5 100.00 ( 99.97)
Epoch: [67][270/391]	Time  0.066 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.0528e-01 (9.3611e-02)	Acc@1  95.31 ( 96.73)	Acc@5 100.00 ( 99.97)
Epoch: [67][280/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.0844e-02 (9.3776e-02)	Acc@1  97.66 ( 96.74)	Acc@5 100.00 ( 99.96)
Epoch: [67][290/391]	Time  0.059 ( 0.064)	Data  0.001 ( 0.002)	Loss 9.4175e-02 (9.3768e-02)	Acc@1  97.66 ( 96.74)	Acc@5 100.00 ( 99.97)
Epoch: [67][300/391]	Time  0.067 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.4934e-02 (9.3851e-02)	Acc@1  98.44 ( 96.76)	Acc@5 100.00 ( 99.96)
Epoch: [67][310/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 8.1710e-02 (9.3690e-02)	Acc@1  97.66 ( 96.77)	Acc@5 100.00 ( 99.96)
Epoch: [67][320/391]	Time  0.067 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.1222e-01 (9.3328e-02)	Acc@1  97.66 ( 96.79)	Acc@5 100.00 ( 99.97)
Epoch: [67][330/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.0701e-01 (9.3464e-02)	Acc@1  95.31 ( 96.77)	Acc@5 100.00 ( 99.96)
Epoch: [67][340/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.6939e-02 (9.3470e-02)	Acc@1  98.44 ( 96.77)	Acc@5 100.00 ( 99.97)
Epoch: [67][350/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.0801e-01 (9.3491e-02)	Acc@1  96.09 ( 96.77)	Acc@5 100.00 ( 99.96)
Epoch: [67][360/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.2794e-01 (9.3432e-02)	Acc@1  96.09 ( 96.78)	Acc@5 100.00 ( 99.96)
Epoch: [67][370/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.0145e-01 (9.3547e-02)	Acc@1  93.75 ( 96.77)	Acc@5 100.00 ( 99.96)
Epoch: [67][380/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.6922e-02 (9.3449e-02)	Acc@1  97.66 ( 96.77)	Acc@5 100.00 ( 99.96)
Epoch: [67][390/391]	Time  0.049 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.3996e-01 (9.3672e-02)	Acc@1  95.00 ( 96.74)	Acc@5 100.00 ( 99.96)
## e[67] optimizer.zero_grad (sum) time: 0.4030487537384033
## e[67]       loss.backward (sum) time: 6.9673216342926025
## e[67]      optimizer.step (sum) time: 3.34635591506958
## epoch[67] training(only) time: 25.241317749023438
# Switched to evaluate mode...
Test: [  0/100]	Time  0.194 ( 0.194)	Loss 3.2095e-01 (3.2095e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.041)	Loss 3.6400e-01 (2.7636e-01)	Acc@1  91.00 ( 91.55)	Acc@5 100.00 ( 99.55)
Test: [ 20/100]	Time  0.027 ( 0.034)	Loss 3.0634e-01 (2.9332e-01)	Acc@1  89.00 ( 91.24)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.027 ( 0.031)	Loss 3.4861e-01 (3.0725e-01)	Acc@1  91.00 ( 91.26)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.026 ( 0.030)	Loss 2.8951e-01 (3.1606e-01)	Acc@1  91.00 ( 90.88)	Acc@5  99.00 ( 99.68)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 2.5300e-01 (3.1923e-01)	Acc@1  91.00 ( 90.90)	Acc@5  99.00 ( 99.67)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 3.1528e-01 (3.1309e-01)	Acc@1  93.00 ( 90.87)	Acc@5 100.00 ( 99.70)
Test: [ 70/100]	Time  0.027 ( 0.028)	Loss 4.7552e-01 (3.0970e-01)	Acc@1  87.00 ( 90.87)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.027 ( 0.028)	Loss 1.8484e-01 (3.0914e-01)	Acc@1  94.00 ( 90.84)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.024 ( 0.028)	Loss 2.4012e-01 (3.0527e-01)	Acc@1  93.00 ( 90.86)	Acc@5 100.00 ( 99.77)
 * Acc@1 90.920 Acc@5 99.770
### epoch[67] execution time: 28.084279775619507
EPOCH 68
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [68][  0/391]	Time  0.254 ( 0.254)	Data  0.190 ( 0.190)	Loss 1.6339e-01 (1.6339e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [68][ 10/391]	Time  0.063 ( 0.080)	Data  0.001 ( 0.018)	Loss 6.3535e-02 (8.8566e-02)	Acc@1  99.22 ( 97.09)	Acc@5 100.00 (100.00)
Epoch: [68][ 20/391]	Time  0.071 ( 0.072)	Data  0.001 ( 0.010)	Loss 1.2315e-01 (8.8476e-02)	Acc@1  95.31 ( 96.91)	Acc@5 100.00 (100.00)
Epoch: [68][ 30/391]	Time  0.066 ( 0.070)	Data  0.001 ( 0.007)	Loss 1.0911e-01 (9.0479e-02)	Acc@1  96.09 ( 96.85)	Acc@5 100.00 (100.00)
Epoch: [68][ 40/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.006)	Loss 7.8200e-02 (8.9417e-02)	Acc@1  97.66 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [68][ 50/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.005)	Loss 6.8177e-02 (8.9708e-02)	Acc@1  97.66 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [68][ 60/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.2966e-01 (9.1881e-02)	Acc@1  95.31 ( 96.79)	Acc@5 100.00 (100.00)
Epoch: [68][ 70/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.004)	Loss 1.1798e-01 (9.2633e-02)	Acc@1  97.66 ( 96.72)	Acc@5 100.00 (100.00)
Epoch: [68][ 80/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.7655e-01 (9.3346e-02)	Acc@1  94.53 ( 96.70)	Acc@5 100.00 ( 99.99)
Epoch: [68][ 90/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.003)	Loss 8.6142e-02 (9.2038e-02)	Acc@1  97.66 ( 96.72)	Acc@5 100.00 ( 99.99)
Epoch: [68][100/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 7.7612e-02 (8.9770e-02)	Acc@1  98.44 ( 96.83)	Acc@5 100.00 ( 99.99)
Epoch: [68][110/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.2707e-01 (8.9880e-02)	Acc@1  94.53 ( 96.78)	Acc@5 100.00 ( 99.99)
Epoch: [68][120/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.2427e-01 (9.1197e-02)	Acc@1  94.53 ( 96.71)	Acc@5 100.00 ( 99.99)
Epoch: [68][130/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.003)	Loss 8.9178e-02 (9.0675e-02)	Acc@1  97.66 ( 96.75)	Acc@5 100.00 ( 99.99)
Epoch: [68][140/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.0538e-02 (9.1169e-02)	Acc@1  96.88 ( 96.74)	Acc@5 100.00 ( 99.99)
Epoch: [68][150/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.4063e-02 (9.2009e-02)	Acc@1  96.09 ( 96.68)	Acc@5 100.00 ( 99.99)
Epoch: [68][160/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2192e-01 (9.1027e-02)	Acc@1  96.09 ( 96.74)	Acc@5 100.00 ( 99.99)
Epoch: [68][170/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.9654e-02 (9.0464e-02)	Acc@1  96.09 ( 96.77)	Acc@5 100.00 ( 99.99)
Epoch: [68][180/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.1906e-02 (9.1207e-02)	Acc@1  96.88 ( 96.74)	Acc@5 100.00 ( 99.99)
Epoch: [68][190/391]	Time  0.062 ( 0.065)	Data  0.002 ( 0.002)	Loss 1.5308e-01 (9.1662e-02)	Acc@1  95.31 ( 96.73)	Acc@5 100.00 ( 99.99)
Epoch: [68][200/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.9376e-02 (9.1832e-02)	Acc@1  95.31 ( 96.71)	Acc@5 100.00 ( 99.99)
Epoch: [68][210/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.8976e-02 (9.1762e-02)	Acc@1  95.31 ( 96.72)	Acc@5 100.00 ( 99.99)
Epoch: [68][220/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3252e-01 (9.0993e-02)	Acc@1  95.31 ( 96.75)	Acc@5 100.00 ( 99.99)
Epoch: [68][230/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.7614e-02 (8.9930e-02)	Acc@1  99.22 ( 96.81)	Acc@5 100.00 ( 99.99)
Epoch: [68][240/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.0005e-02 (8.8713e-02)	Acc@1  98.44 ( 96.86)	Acc@5 100.00 ( 99.99)
Epoch: [68][250/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.3639e-02 (8.8764e-02)	Acc@1  98.44 ( 96.86)	Acc@5 100.00 ( 99.98)
Epoch: [68][260/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.8109e-02 (8.8986e-02)	Acc@1  97.66 ( 96.85)	Acc@5 100.00 ( 99.99)
Epoch: [68][270/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.8667e-02 (8.9000e-02)	Acc@1  98.44 ( 96.85)	Acc@5 100.00 ( 99.99)
Epoch: [68][280/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.0499e-02 (8.8798e-02)	Acc@1  96.88 ( 96.86)	Acc@5 100.00 ( 99.99)
Epoch: [68][290/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.5190e-02 (8.9446e-02)	Acc@1  98.44 ( 96.84)	Acc@5 100.00 ( 99.99)
Epoch: [68][300/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 9.3172e-02 (8.9097e-02)	Acc@1  96.09 ( 96.85)	Acc@5 100.00 ( 99.99)
Epoch: [68][310/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 9.4771e-02 (8.8946e-02)	Acc@1  97.66 ( 96.86)	Acc@5 100.00 ( 99.99)
Epoch: [68][320/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.0512e-01 (8.9245e-02)	Acc@1  96.09 ( 96.85)	Acc@5 100.00 ( 99.99)
Epoch: [68][330/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.9250e-02 (8.9167e-02)	Acc@1  96.88 ( 96.84)	Acc@5 100.00 ( 99.99)
Epoch: [68][340/391]	Time  0.070 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.3909e-02 (8.9264e-02)	Acc@1  98.44 ( 96.84)	Acc@5 100.00 ( 99.99)
Epoch: [68][350/391]	Time  0.060 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.5405e-01 (8.9791e-02)	Acc@1  93.75 ( 96.84)	Acc@5 100.00 ( 99.99)
Epoch: [68][360/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.6276e-01 (9.0019e-02)	Acc@1  93.75 ( 96.84)	Acc@5 100.00 ( 99.99)
Epoch: [68][370/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.1089e-01 (9.0208e-02)	Acc@1  96.88 ( 96.83)	Acc@5 100.00 ( 99.99)
Epoch: [68][380/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.5358e-02 (9.0413e-02)	Acc@1  99.22 ( 96.82)	Acc@5 100.00 ( 99.99)
Epoch: [68][390/391]	Time  0.046 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.2793e-01 (9.0257e-02)	Acc@1  93.75 ( 96.82)	Acc@5 100.00 ( 99.99)
## e[68] optimizer.zero_grad (sum) time: 0.4013092517852783
## e[68]       loss.backward (sum) time: 6.96035361289978
## e[68]      optimizer.step (sum) time: 3.3201961517333984
## epoch[68] training(only) time: 25.229557037353516
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 3.1375e-01 (3.1375e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.026 ( 0.042)	Loss 3.9315e-01 (2.7657e-01)	Acc@1  90.00 ( 91.82)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.024 ( 0.034)	Loss 2.9961e-01 (2.9483e-01)	Acc@1  89.00 ( 91.24)	Acc@5  99.00 ( 99.71)
Test: [ 30/100]	Time  0.027 ( 0.031)	Loss 3.6222e-01 (3.1104e-01)	Acc@1  91.00 ( 91.10)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.028 ( 0.030)	Loss 2.9019e-01 (3.1831e-01)	Acc@1  91.00 ( 90.88)	Acc@5  99.00 ( 99.73)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 2.6547e-01 (3.2147e-01)	Acc@1  92.00 ( 90.73)	Acc@5  99.00 ( 99.69)
Test: [ 60/100]	Time  0.024 ( 0.029)	Loss 3.2102e-01 (3.1640e-01)	Acc@1  92.00 ( 90.72)	Acc@5 100.00 ( 99.70)
Test: [ 70/100]	Time  0.027 ( 0.028)	Loss 4.7188e-01 (3.1197e-01)	Acc@1  86.00 ( 90.79)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.027 ( 0.028)	Loss 2.0070e-01 (3.1113e-01)	Acc@1  93.00 ( 90.75)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.024 ( 0.028)	Loss 2.2993e-01 (3.0777e-01)	Acc@1  93.00 ( 90.80)	Acc@5 100.00 ( 99.76)
 * Acc@1 90.850 Acc@5 99.760
### epoch[68] execution time: 28.13687038421631
EPOCH 69
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [69][  0/391]	Time  0.250 ( 0.250)	Data  0.182 ( 0.182)	Loss 9.3019e-02 (9.3019e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [69][ 10/391]	Time  0.062 ( 0.080)	Data  0.001 ( 0.018)	Loss 9.3117e-02 (9.1585e-02)	Acc@1  97.66 ( 97.37)	Acc@5 100.00 (100.00)
Epoch: [69][ 20/391]	Time  0.066 ( 0.072)	Data  0.001 ( 0.010)	Loss 9.5421e-02 (9.6605e-02)	Acc@1  96.09 ( 96.91)	Acc@5 100.00 (100.00)
Epoch: [69][ 30/391]	Time  0.066 ( 0.070)	Data  0.001 ( 0.007)	Loss 1.1657e-01 (9.8480e-02)	Acc@1  96.09 ( 96.80)	Acc@5 100.00 ( 99.95)
Epoch: [69][ 40/391]	Time  0.062 ( 0.068)	Data  0.001 ( 0.006)	Loss 1.0254e-01 (9.5513e-02)	Acc@1  97.66 ( 96.78)	Acc@5 100.00 ( 99.96)
Epoch: [69][ 50/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.005)	Loss 1.1048e-01 (9.4902e-02)	Acc@1  95.31 ( 96.75)	Acc@5 100.00 ( 99.97)
Epoch: [69][ 60/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.0841e-01 (9.6499e-02)	Acc@1  96.09 ( 96.61)	Acc@5 100.00 ( 99.96)
Epoch: [69][ 70/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.004)	Loss 3.8465e-02 (9.6623e-02)	Acc@1  99.22 ( 96.67)	Acc@5 100.00 ( 99.96)
Epoch: [69][ 80/391]	Time  0.060 ( 0.066)	Data  0.001 ( 0.003)	Loss 7.5552e-02 (9.3857e-02)	Acc@1  96.88 ( 96.79)	Acc@5 100.00 ( 99.96)
Epoch: [69][ 90/391]	Time  0.064 ( 0.066)	Data  0.002 ( 0.003)	Loss 5.7169e-02 (9.1183e-02)	Acc@1  97.66 ( 96.87)	Acc@5 100.00 ( 99.97)
Epoch: [69][100/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.003)	Loss 5.5805e-02 (8.9965e-02)	Acc@1  97.66 ( 96.91)	Acc@5 100.00 ( 99.97)
Epoch: [69][110/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.003)	Loss 6.4744e-02 (8.8548e-02)	Acc@1  97.66 ( 96.95)	Acc@5 100.00 ( 99.96)
Epoch: [69][120/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.1650e-01 (8.9159e-02)	Acc@1  94.53 ( 96.93)	Acc@5 100.00 ( 99.97)
Epoch: [69][130/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 9.8826e-02 (8.8113e-02)	Acc@1  96.09 ( 96.93)	Acc@5 100.00 ( 99.97)
Epoch: [69][140/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0185e-01 (8.8501e-02)	Acc@1  96.09 ( 96.90)	Acc@5 100.00 ( 99.97)
Epoch: [69][150/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.2065e-02 (8.8321e-02)	Acc@1  96.88 ( 96.92)	Acc@5 100.00 ( 99.97)
Epoch: [69][160/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.3631e-02 (8.7732e-02)	Acc@1  99.22 ( 96.93)	Acc@5 100.00 ( 99.98)
Epoch: [69][170/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.2353e-02 (8.7926e-02)	Acc@1  96.88 ( 96.94)	Acc@5 100.00 ( 99.98)
Epoch: [69][180/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.4305e-02 (8.7731e-02)	Acc@1  97.66 ( 96.93)	Acc@5 100.00 ( 99.98)
Epoch: [69][190/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3451e-01 (8.7457e-02)	Acc@1  93.75 ( 96.93)	Acc@5 100.00 ( 99.98)
Epoch: [69][200/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.5818e-02 (8.7423e-02)	Acc@1  96.88 ( 96.93)	Acc@5 100.00 ( 99.98)
Epoch: [69][210/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.2969e-02 (8.8380e-02)	Acc@1  97.66 ( 96.90)	Acc@5 100.00 ( 99.97)
Epoch: [69][220/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.9976e-02 (8.7889e-02)	Acc@1  96.88 ( 96.90)	Acc@5 100.00 ( 99.98)
Epoch: [69][230/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.3788e-02 (8.7633e-02)	Acc@1  97.66 ( 96.91)	Acc@5 100.00 ( 99.97)
Epoch: [69][240/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.6618e-02 (8.7679e-02)	Acc@1  96.09 ( 96.92)	Acc@5 100.00 ( 99.97)
Epoch: [69][250/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2314e-01 (8.8991e-02)	Acc@1  95.31 ( 96.88)	Acc@5 100.00 ( 99.97)
Epoch: [69][260/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.6449e-02 (8.8594e-02)	Acc@1  99.22 ( 96.91)	Acc@5 100.00 ( 99.97)
Epoch: [69][270/391]	Time  0.068 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.3249e-01 (8.8519e-02)	Acc@1  96.09 ( 96.90)	Acc@5 100.00 ( 99.97)
Epoch: [69][280/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.8173e-02 (8.8888e-02)	Acc@1  97.66 ( 96.88)	Acc@5 100.00 ( 99.97)
Epoch: [69][290/391]	Time  0.066 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.7208e-02 (8.9162e-02)	Acc@1  97.66 ( 96.88)	Acc@5 100.00 ( 99.97)
Epoch: [69][300/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.0344e-01 (8.9588e-02)	Acc@1  96.09 ( 96.87)	Acc@5 100.00 ( 99.97)
Epoch: [69][310/391]	Time  0.072 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1653e-01 (8.9391e-02)	Acc@1  93.75 ( 96.86)	Acc@5 100.00 ( 99.97)
Epoch: [69][320/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.9350e-02 (8.8935e-02)	Acc@1  99.22 ( 96.88)	Acc@5 100.00 ( 99.97)
Epoch: [69][330/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.5770e-02 (8.9250e-02)	Acc@1  97.66 ( 96.86)	Acc@5 100.00 ( 99.97)
Epoch: [69][340/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.2963e-01 (8.9477e-02)	Acc@1  94.53 ( 96.85)	Acc@5 100.00 ( 99.97)
Epoch: [69][350/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.3902e-01 (9.0025e-02)	Acc@1  95.31 ( 96.83)	Acc@5 100.00 ( 99.97)
Epoch: [69][360/391]	Time  0.060 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.6431e-02 (9.0252e-02)	Acc@1  96.88 ( 96.82)	Acc@5 100.00 ( 99.97)
Epoch: [69][370/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 9.5515e-02 (9.0001e-02)	Acc@1  97.66 ( 96.83)	Acc@5 100.00 ( 99.97)
Epoch: [69][380/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.0860e-02 (8.9958e-02)	Acc@1  96.09 ( 96.83)	Acc@5 100.00 ( 99.98)
Epoch: [69][390/391]	Time  0.047 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.8678e-01 (9.0588e-02)	Acc@1  88.75 ( 96.79)	Acc@5 100.00 ( 99.98)
## e[69] optimizer.zero_grad (sum) time: 0.39697837829589844
## e[69]       loss.backward (sum) time: 6.9488365650177
## e[69]      optimizer.step (sum) time: 3.3382580280303955
## epoch[69] training(only) time: 25.2250816822052
# Switched to evaluate mode...
Test: [  0/100]	Time  0.198 ( 0.198)	Loss 3.2022e-01 (3.2022e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.025 ( 0.041)	Loss 3.9918e-01 (2.7518e-01)	Acc@1  89.00 ( 91.45)	Acc@5  99.00 ( 99.55)
Test: [ 20/100]	Time  0.027 ( 0.033)	Loss 2.8149e-01 (2.9135e-01)	Acc@1  90.00 ( 91.05)	Acc@5  99.00 ( 99.62)
Test: [ 30/100]	Time  0.024 ( 0.030)	Loss 3.5657e-01 (3.0867e-01)	Acc@1  89.00 ( 91.03)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.032 ( 0.030)	Loss 3.0486e-01 (3.1718e-01)	Acc@1  91.00 ( 90.80)	Acc@5  99.00 ( 99.66)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 2.7691e-01 (3.2009e-01)	Acc@1  92.00 ( 90.80)	Acc@5  99.00 ( 99.63)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 3.0100e-01 (3.1532e-01)	Acc@1  93.00 ( 90.72)	Acc@5 100.00 ( 99.67)
Test: [ 70/100]	Time  0.026 ( 0.028)	Loss 5.1097e-01 (3.1173e-01)	Acc@1  87.00 ( 90.73)	Acc@5 100.00 ( 99.70)
Test: [ 80/100]	Time  0.024 ( 0.028)	Loss 1.9782e-01 (3.1114e-01)	Acc@1  94.00 ( 90.72)	Acc@5 100.00 ( 99.72)
Test: [ 90/100]	Time  0.027 ( 0.027)	Loss 2.4606e-01 (3.0797e-01)	Acc@1  93.00 ( 90.79)	Acc@5 100.00 ( 99.75)
 * Acc@1 90.840 Acc@5 99.750
### epoch[69] execution time: 28.105616807937622
EPOCH 70
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [70][  0/391]	Time  0.247 ( 0.247)	Data  0.181 ( 0.181)	Loss 1.2078e-01 (1.2078e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [70][ 10/391]	Time  0.062 ( 0.081)	Data  0.001 ( 0.017)	Loss 8.1280e-02 (8.0473e-02)	Acc@1  96.09 ( 97.16)	Acc@5 100.00 (100.00)
Epoch: [70][ 20/391]	Time  0.064 ( 0.073)	Data  0.001 ( 0.010)	Loss 1.5721e-01 (8.6519e-02)	Acc@1  94.53 ( 96.91)	Acc@5 100.00 ( 99.96)
Epoch: [70][ 30/391]	Time  0.066 ( 0.070)	Data  0.001 ( 0.007)	Loss 1.1082e-01 (8.4629e-02)	Acc@1  94.53 ( 96.93)	Acc@5 100.00 ( 99.95)
Epoch: [70][ 40/391]	Time  0.058 ( 0.069)	Data  0.001 ( 0.005)	Loss 9.4884e-02 (8.6478e-02)	Acc@1  96.09 ( 96.91)	Acc@5 100.00 ( 99.92)
Epoch: [70][ 50/391]	Time  0.061 ( 0.067)	Data  0.001 ( 0.005)	Loss 5.8366e-02 (8.7240e-02)	Acc@1  96.88 ( 96.91)	Acc@5 100.00 ( 99.94)
Epoch: [70][ 60/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.004)	Loss 3.1947e-02 (9.0904e-02)	Acc@1  99.22 ( 96.89)	Acc@5 100.00 ( 99.92)
Epoch: [70][ 70/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.004)	Loss 1.0366e-01 (9.0042e-02)	Acc@1  96.88 ( 96.95)	Acc@5 100.00 ( 99.93)
Epoch: [70][ 80/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.3616e-01 (9.0745e-02)	Acc@1  94.53 ( 96.89)	Acc@5 100.00 ( 99.93)
Epoch: [70][ 90/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.003)	Loss 3.1039e-02 (8.9333e-02)	Acc@1 100.00 ( 96.93)	Acc@5 100.00 ( 99.94)
Epoch: [70][100/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 2.0111e-01 (9.1188e-02)	Acc@1  92.97 ( 96.89)	Acc@5 100.00 ( 99.93)
Epoch: [70][110/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.6115e-01 (9.3608e-02)	Acc@1  96.88 ( 96.85)	Acc@5 100.00 ( 99.93)
Epoch: [70][120/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.0475e-01 (9.3537e-02)	Acc@1  95.31 ( 96.85)	Acc@5 100.00 ( 99.94)
Epoch: [70][130/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.003)	Loss 7.8067e-02 (9.3592e-02)	Acc@1  96.09 ( 96.82)	Acc@5 100.00 ( 99.94)
Epoch: [70][140/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.7771e-02 (9.3514e-02)	Acc@1  97.66 ( 96.84)	Acc@5 100.00 ( 99.94)
Epoch: [70][150/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.0850e-02 (9.3244e-02)	Acc@1  98.44 ( 96.83)	Acc@5 100.00 ( 99.95)
Epoch: [70][160/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0905e-01 (9.2043e-02)	Acc@1  96.88 ( 96.87)	Acc@5 100.00 ( 99.95)
Epoch: [70][170/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.1453e-02 (9.2041e-02)	Acc@1  96.88 ( 96.87)	Acc@5 100.00 ( 99.95)
Epoch: [70][180/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.1953e-02 (9.1408e-02)	Acc@1  96.09 ( 96.89)	Acc@5 100.00 ( 99.96)
Epoch: [70][190/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.3020e-02 (9.1721e-02)	Acc@1  97.66 ( 96.87)	Acc@5 100.00 ( 99.96)
Epoch: [70][200/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.3835e-02 (9.1070e-02)	Acc@1  99.22 ( 96.90)	Acc@5 100.00 ( 99.96)
Epoch: [70][210/391]	Time  0.066 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.9209e-02 (9.0699e-02)	Acc@1  98.44 ( 96.94)	Acc@5 100.00 ( 99.96)
Epoch: [70][220/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.4098e-01 (9.1175e-02)	Acc@1  96.09 ( 96.90)	Acc@5 100.00 ( 99.96)
Epoch: [70][230/391]	Time  0.060 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.5681e-01 (9.1244e-02)	Acc@1  93.75 ( 96.90)	Acc@5 100.00 ( 99.97)
Epoch: [70][240/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.6814e-01 (9.1751e-02)	Acc@1  93.75 ( 96.88)	Acc@5 100.00 ( 99.96)
Epoch: [70][250/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.5431e-02 (9.1516e-02)	Acc@1  98.44 ( 96.88)	Acc@5 100.00 ( 99.96)
Epoch: [70][260/391]	Time  0.069 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.0173e-02 (9.1076e-02)	Acc@1  97.66 ( 96.89)	Acc@5 100.00 ( 99.96)
Epoch: [70][270/391]	Time  0.067 ( 0.064)	Data  0.001 ( 0.002)	Loss 8.3014e-02 (9.1082e-02)	Acc@1  97.66 ( 96.88)	Acc@5 100.00 ( 99.97)
Epoch: [70][280/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.9710e-02 (9.1269e-02)	Acc@1  98.44 ( 96.89)	Acc@5 100.00 ( 99.96)
Epoch: [70][290/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.8539e-02 (9.1032e-02)	Acc@1  96.88 ( 96.89)	Acc@5 100.00 ( 99.96)
Epoch: [70][300/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.7914e-02 (9.0568e-02)	Acc@1  99.22 ( 96.91)	Acc@5 100.00 ( 99.96)
Epoch: [70][310/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.1847e-01 (9.0928e-02)	Acc@1  96.88 ( 96.90)	Acc@5  99.22 ( 99.96)
Epoch: [70][320/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.3061e-01 (9.1408e-02)	Acc@1  96.09 ( 96.89)	Acc@5 100.00 ( 99.96)
Epoch: [70][330/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.6730e-02 (9.1439e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 ( 99.96)
Epoch: [70][340/391]	Time  0.060 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.1284e-01 (9.1271e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 ( 99.96)
Epoch: [70][350/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 8.3100e-02 (9.1457e-02)	Acc@1  96.88 ( 96.86)	Acc@5 100.00 ( 99.96)
Epoch: [70][360/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.7508e-01 (9.1880e-02)	Acc@1  92.97 ( 96.83)	Acc@5 100.00 ( 99.96)
Epoch: [70][370/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 8.0648e-02 (9.1805e-02)	Acc@1  96.88 ( 96.84)	Acc@5 100.00 ( 99.96)
Epoch: [70][380/391]	Time  0.060 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.1649e-01 (9.2007e-02)	Acc@1  95.31 ( 96.82)	Acc@5 100.00 ( 99.96)
Epoch: [70][390/391]	Time  0.047 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.2195e-01 (9.2170e-02)	Acc@1  95.00 ( 96.81)	Acc@5 100.00 ( 99.96)
## e[70] optimizer.zero_grad (sum) time: 0.39821743965148926
## e[70]       loss.backward (sum) time: 6.883209705352783
## e[70]      optimizer.step (sum) time: 3.3562023639678955
## epoch[70] training(only) time: 25.16814351081848
# Switched to evaluate mode...
Test: [  0/100]	Time  0.197 ( 0.197)	Loss 3.1951e-01 (3.1951e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.029 ( 0.042)	Loss 4.1212e-01 (2.8178e-01)	Acc@1  90.00 ( 91.18)	Acc@5  99.00 ( 99.55)
Test: [ 20/100]	Time  0.027 ( 0.035)	Loss 3.0232e-01 (2.9614e-01)	Acc@1  89.00 ( 91.05)	Acc@5  99.00 ( 99.57)
Test: [ 30/100]	Time  0.029 ( 0.032)	Loss 3.5455e-01 (3.1047e-01)	Acc@1  89.00 ( 90.97)	Acc@5 100.00 ( 99.61)
Test: [ 40/100]	Time  0.024 ( 0.030)	Loss 3.1603e-01 (3.1948e-01)	Acc@1  91.00 ( 90.76)	Acc@5  99.00 ( 99.66)
Test: [ 50/100]	Time  0.032 ( 0.029)	Loss 2.7730e-01 (3.2183e-01)	Acc@1  93.00 ( 90.71)	Acc@5  99.00 ( 99.67)
Test: [ 60/100]	Time  0.027 ( 0.029)	Loss 3.0230e-01 (3.1684e-01)	Acc@1  93.00 ( 90.70)	Acc@5 100.00 ( 99.70)
Test: [ 70/100]	Time  0.026 ( 0.028)	Loss 5.0506e-01 (3.1242e-01)	Acc@1  87.00 ( 90.77)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.024 ( 0.028)	Loss 2.0320e-01 (3.1111e-01)	Acc@1  93.00 ( 90.70)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.024 ( 0.028)	Loss 2.6353e-01 (3.0828e-01)	Acc@1  93.00 ( 90.76)	Acc@5 100.00 ( 99.75)
 * Acc@1 90.800 Acc@5 99.750
### epoch[70] execution time: 28.077568769454956
EPOCH 71
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [71][  0/391]	Time  0.252 ( 0.252)	Data  0.188 ( 0.188)	Loss 7.3362e-02 (7.3362e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [71][ 10/391]	Time  0.063 ( 0.080)	Data  0.001 ( 0.018)	Loss 1.2703e-01 (8.5237e-02)	Acc@1  92.97 ( 96.59)	Acc@5 100.00 (100.00)
Epoch: [71][ 20/391]	Time  0.068 ( 0.072)	Data  0.001 ( 0.010)	Loss 1.3220e-01 (8.5648e-02)	Acc@1  96.88 ( 96.80)	Acc@5 100.00 ( 99.96)
Epoch: [71][ 30/391]	Time  0.063 ( 0.070)	Data  0.001 ( 0.007)	Loss 6.0610e-02 (8.5382e-02)	Acc@1  98.44 ( 97.05)	Acc@5 100.00 ( 99.97)
Epoch: [71][ 40/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.006)	Loss 1.0113e-01 (8.5009e-02)	Acc@1  95.31 ( 97.01)	Acc@5 100.00 ( 99.98)
Epoch: [71][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.005)	Loss 9.5306e-02 (8.4106e-02)	Acc@1  96.88 ( 97.10)	Acc@5 100.00 ( 99.98)
Epoch: [71][ 60/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.004)	Loss 8.9175e-02 (8.6607e-02)	Acc@1  98.44 ( 96.98)	Acc@5  99.22 ( 99.97)
Epoch: [71][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.1820e-01 (8.6856e-02)	Acc@1  96.09 ( 96.95)	Acc@5 100.00 ( 99.98)
Epoch: [71][ 80/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 9.6786e-02 (8.5753e-02)	Acc@1  97.66 ( 97.01)	Acc@5 100.00 ( 99.98)
Epoch: [71][ 90/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 9.7529e-02 (8.7450e-02)	Acc@1  96.09 ( 97.00)	Acc@5 100.00 ( 99.97)
Epoch: [71][100/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 7.9993e-02 (8.7073e-02)	Acc@1  97.66 ( 97.01)	Acc@5 100.00 ( 99.98)
Epoch: [71][110/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.1410e-02 (8.6948e-02)	Acc@1  99.22 ( 97.01)	Acc@5 100.00 ( 99.97)
Epoch: [71][120/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.6343e-01 (8.7666e-02)	Acc@1  95.31 ( 96.97)	Acc@5 100.00 ( 99.97)
Epoch: [71][130/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.3796e-01 (8.8871e-02)	Acc@1  95.31 ( 96.92)	Acc@5 100.00 ( 99.97)
Epoch: [71][140/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.3090e-02 (8.9688e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 ( 99.97)
Epoch: [71][150/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.2761e-02 (8.9431e-02)	Acc@1  96.88 ( 96.89)	Acc@5 100.00 ( 99.97)
Epoch: [71][160/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.6594e-02 (8.9405e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 ( 99.98)
Epoch: [71][170/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.3241e-02 (8.9406e-02)	Acc@1  97.66 ( 96.85)	Acc@5 100.00 ( 99.97)
Epoch: [71][180/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0421e-01 (8.9485e-02)	Acc@1  97.66 ( 96.84)	Acc@5 100.00 ( 99.97)
Epoch: [71][190/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.9548e-02 (8.9329e-02)	Acc@1  97.66 ( 96.85)	Acc@5 100.00 ( 99.98)
Epoch: [71][200/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.6958e-02 (8.9386e-02)	Acc@1  96.88 ( 96.84)	Acc@5 100.00 ( 99.98)
Epoch: [71][210/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5160e-01 (9.0189e-02)	Acc@1  94.53 ( 96.82)	Acc@5 100.00 ( 99.98)
Epoch: [71][220/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0112e-01 (9.0002e-02)	Acc@1  95.31 ( 96.83)	Acc@5 100.00 ( 99.98)
Epoch: [71][230/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3510e-01 (9.0607e-02)	Acc@1  92.97 ( 96.77)	Acc@5 100.00 ( 99.98)
Epoch: [71][240/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3466e-01 (9.0864e-02)	Acc@1  93.75 ( 96.76)	Acc@5 100.00 ( 99.98)
Epoch: [71][250/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2176e-01 (9.0812e-02)	Acc@1  96.09 ( 96.78)	Acc@5 100.00 ( 99.98)
Epoch: [71][260/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.8961e-02 (9.1095e-02)	Acc@1  97.66 ( 96.76)	Acc@5 100.00 ( 99.98)
Epoch: [71][270/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8366e-01 (9.0753e-02)	Acc@1  92.97 ( 96.77)	Acc@5 100.00 ( 99.98)
Epoch: [71][280/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7687e-01 (9.0465e-02)	Acc@1  94.53 ( 96.78)	Acc@5 100.00 ( 99.98)
Epoch: [71][290/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.1735e-02 (8.9454e-02)	Acc@1  97.66 ( 96.83)	Acc@5 100.00 ( 99.98)
Epoch: [71][300/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.1296e-02 (8.9591e-02)	Acc@1  98.44 ( 96.82)	Acc@5 100.00 ( 99.98)
Epoch: [71][310/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.8742e-02 (8.9854e-02)	Acc@1  96.88 ( 96.82)	Acc@5 100.00 ( 99.98)
Epoch: [71][320/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.9146e-02 (8.9821e-02)	Acc@1  96.09 ( 96.81)	Acc@5 100.00 ( 99.98)
Epoch: [71][330/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.4984e-02 (9.0698e-02)	Acc@1  99.22 ( 96.78)	Acc@5 100.00 ( 99.98)
Epoch: [71][340/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3116e-01 (9.0580e-02)	Acc@1  95.31 ( 96.79)	Acc@5 100.00 ( 99.98)
Epoch: [71][350/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.8009e-02 (9.0250e-02)	Acc@1  96.88 ( 96.80)	Acc@5 100.00 ( 99.98)
Epoch: [71][360/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.4052e-02 (9.0179e-02)	Acc@1  98.44 ( 96.81)	Acc@5 100.00 ( 99.98)
Epoch: [71][370/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0744e-01 (8.9810e-02)	Acc@1  96.88 ( 96.82)	Acc@5 100.00 ( 99.98)
Epoch: [71][380/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0665e-01 (8.9912e-02)	Acc@1  97.66 ( 96.81)	Acc@5  99.22 ( 99.98)
Epoch: [71][390/391]	Time  0.045 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.3096e-01 (9.0041e-02)	Acc@1  95.00 ( 96.80)	Acc@5 100.00 ( 99.98)
## e[71] optimizer.zero_grad (sum) time: 0.4039442539215088
## e[71]       loss.backward (sum) time: 6.961095809936523
## e[71]      optimizer.step (sum) time: 3.351947784423828
## epoch[71] training(only) time: 25.371504068374634
# Switched to evaluate mode...
Test: [  0/100]	Time  0.197 ( 0.197)	Loss 3.1316e-01 (3.1316e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.026 ( 0.041)	Loss 3.9979e-01 (2.8209e-01)	Acc@1  89.00 ( 91.18)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.024 ( 0.034)	Loss 2.9700e-01 (2.9387e-01)	Acc@1  90.00 ( 91.05)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.027 ( 0.031)	Loss 3.7500e-01 (3.1040e-01)	Acc@1  89.00 ( 90.94)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 3.0880e-01 (3.2036e-01)	Acc@1  91.00 ( 90.71)	Acc@5  99.00 ( 99.76)
Test: [ 50/100]	Time  0.026 ( 0.029)	Loss 2.8197e-01 (3.2343e-01)	Acc@1  92.00 ( 90.59)	Acc@5  99.00 ( 99.73)
Test: [ 60/100]	Time  0.026 ( 0.028)	Loss 3.0427e-01 (3.1831e-01)	Acc@1  93.00 ( 90.59)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 4.9693e-01 (3.1378e-01)	Acc@1  88.00 ( 90.69)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.9344e-01 (3.1227e-01)	Acc@1  93.00 ( 90.64)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.025 ( 0.027)	Loss 2.5072e-01 (3.0858e-01)	Acc@1  93.00 ( 90.68)	Acc@5 100.00 ( 99.79)
 * Acc@1 90.730 Acc@5 99.790
### epoch[71] execution time: 28.205233812332153
EPOCH 72
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [72][  0/391]	Time  0.251 ( 0.251)	Data  0.187 ( 0.187)	Loss 6.1186e-02 (6.1186e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [72][ 10/391]	Time  0.064 ( 0.080)	Data  0.001 ( 0.018)	Loss 6.8407e-02 (7.7591e-02)	Acc@1  96.88 ( 97.30)	Acc@5 100.00 ( 99.93)
Epoch: [72][ 20/391]	Time  0.064 ( 0.072)	Data  0.001 ( 0.010)	Loss 9.7717e-02 (8.2917e-02)	Acc@1  97.66 ( 97.17)	Acc@5 100.00 ( 99.96)
Epoch: [72][ 30/391]	Time  0.067 ( 0.070)	Data  0.001 ( 0.007)	Loss 6.1185e-02 (8.3096e-02)	Acc@1  98.44 ( 97.23)	Acc@5 100.00 ( 99.97)
Epoch: [72][ 40/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.006)	Loss 6.6897e-02 (8.4211e-02)	Acc@1  97.66 ( 97.22)	Acc@5 100.00 ( 99.98)
Epoch: [72][ 50/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.005)	Loss 7.0956e-02 (8.2553e-02)	Acc@1  97.66 ( 97.21)	Acc@5 100.00 ( 99.98)
Epoch: [72][ 60/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.004)	Loss 4.5821e-02 (8.2668e-02)	Acc@1  98.44 ( 97.17)	Acc@5 100.00 ( 99.99)
Epoch: [72][ 70/391]	Time  0.073 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.2282e-01 (8.2105e-02)	Acc@1  95.31 ( 97.18)	Acc@5 100.00 ( 99.99)
Epoch: [72][ 80/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.003)	Loss 6.5042e-02 (8.2972e-02)	Acc@1  98.44 ( 97.19)	Acc@5 100.00 ( 99.98)
Epoch: [72][ 90/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.003)	Loss 8.1477e-02 (8.3515e-02)	Acc@1  96.09 ( 97.14)	Acc@5 100.00 ( 99.98)
Epoch: [72][100/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.003)	Loss 8.1951e-02 (8.2893e-02)	Acc@1  97.66 ( 97.17)	Acc@5 100.00 ( 99.98)
Epoch: [72][110/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.003)	Loss 8.5663e-02 (8.3452e-02)	Acc@1  97.66 ( 97.16)	Acc@5 100.00 ( 99.99)
Epoch: [72][120/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.003)	Loss 9.5172e-02 (8.4574e-02)	Acc@1  97.66 ( 97.11)	Acc@5 100.00 ( 99.98)
Epoch: [72][130/391]	Time  0.071 ( 0.065)	Data  0.001 ( 0.003)	Loss 5.7498e-02 (8.3450e-02)	Acc@1  99.22 ( 97.19)	Acc@5 100.00 ( 99.98)
Epoch: [72][140/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3254e-01 (8.4811e-02)	Acc@1  94.53 ( 97.11)	Acc@5 100.00 ( 99.98)
Epoch: [72][150/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1026e-01 (8.5858e-02)	Acc@1  97.66 ( 97.09)	Acc@5 100.00 ( 99.98)
Epoch: [72][160/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3028e-01 (8.7605e-02)	Acc@1  95.31 ( 97.03)	Acc@5  99.22 ( 99.98)
Epoch: [72][170/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2423e-01 (8.7410e-02)	Acc@1  96.09 ( 97.03)	Acc@5 100.00 ( 99.98)
Epoch: [72][180/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.5120e-02 (8.7706e-02)	Acc@1  95.31 ( 96.99)	Acc@5 100.00 ( 99.98)
Epoch: [72][190/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.6932e-02 (8.8432e-02)	Acc@1  96.88 ( 96.97)	Acc@5 100.00 ( 99.98)
Epoch: [72][200/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2565e-01 (8.8578e-02)	Acc@1  96.09 ( 96.96)	Acc@5 100.00 ( 99.98)
Epoch: [72][210/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.3922e-02 (8.7990e-02)	Acc@1  99.22 ( 96.97)	Acc@5 100.00 ( 99.99)
Epoch: [72][220/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.9789e-02 (8.7680e-02)	Acc@1 100.00 ( 97.01)	Acc@5 100.00 ( 99.99)
Epoch: [72][230/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 9.0503e-02 (8.7811e-02)	Acc@1  98.44 ( 97.01)	Acc@5 100.00 ( 99.99)
Epoch: [72][240/391]	Time  0.067 ( 0.064)	Data  0.001 ( 0.002)	Loss 8.1533e-02 (8.7775e-02)	Acc@1  97.66 ( 97.01)	Acc@5 100.00 ( 99.98)
Epoch: [72][250/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.3558e-01 (8.8298e-02)	Acc@1  93.75 ( 96.98)	Acc@5 100.00 ( 99.98)
Epoch: [72][260/391]	Time  0.066 ( 0.064)	Data  0.001 ( 0.002)	Loss 9.4172e-02 (8.8276e-02)	Acc@1  97.66 ( 96.97)	Acc@5 100.00 ( 99.99)
Epoch: [72][270/391]	Time  0.066 ( 0.064)	Data  0.001 ( 0.002)	Loss 8.6357e-02 (8.8341e-02)	Acc@1  96.09 ( 96.95)	Acc@5 100.00 ( 99.99)
Epoch: [72][280/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.0780e-02 (8.8487e-02)	Acc@1  98.44 ( 96.95)	Acc@5 100.00 ( 99.98)
Epoch: [72][290/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.2844e-02 (8.8128e-02)	Acc@1  98.44 ( 96.95)	Acc@5 100.00 ( 99.98)
Epoch: [72][300/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.0349e-01 (8.9020e-02)	Acc@1  95.31 ( 96.92)	Acc@5 100.00 ( 99.98)
Epoch: [72][310/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.4321e-02 (8.9376e-02)	Acc@1  98.44 ( 96.93)	Acc@5 100.00 ( 99.98)
Epoch: [72][320/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.0872e-01 (8.9905e-02)	Acc@1  94.53 ( 96.90)	Acc@5 100.00 ( 99.98)
Epoch: [72][330/391]	Time  0.067 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.7478e-02 (8.9642e-02)	Acc@1  98.44 ( 96.92)	Acc@5 100.00 ( 99.98)
Epoch: [72][340/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.9289e-02 (8.9531e-02)	Acc@1  98.44 ( 96.94)	Acc@5 100.00 ( 99.98)
Epoch: [72][350/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.3818e-01 (8.9757e-02)	Acc@1  94.53 ( 96.94)	Acc@5 100.00 ( 99.98)
Epoch: [72][360/391]	Time  0.069 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.4585e-02 (8.9243e-02)	Acc@1  98.44 ( 96.96)	Acc@5 100.00 ( 99.98)
Epoch: [72][370/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.9137e-02 (9.0164e-02)	Acc@1  96.88 ( 96.92)	Acc@5 100.00 ( 99.98)
Epoch: [72][380/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.0799e-01 (9.0744e-02)	Acc@1  94.53 ( 96.91)	Acc@5 100.00 ( 99.98)
Epoch: [72][390/391]	Time  0.056 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.0061e-01 (9.0780e-02)	Acc@1  96.25 ( 96.91)	Acc@5 100.00 ( 99.98)
## e[72] optimizer.zero_grad (sum) time: 0.39772582054138184
## e[72]       loss.backward (sum) time: 6.924566984176636
## e[72]      optimizer.step (sum) time: 3.3027255535125732
## epoch[72] training(only) time: 25.11275577545166
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 3.1048e-01 (3.1048e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.027 ( 0.042)	Loss 3.9022e-01 (2.8583e-01)	Acc@1  89.00 ( 91.36)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.028 ( 0.034)	Loss 3.0895e-01 (2.9749e-01)	Acc@1  91.00 ( 91.29)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.026 ( 0.031)	Loss 3.5891e-01 (3.1306e-01)	Acc@1  90.00 ( 91.16)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 3.0131e-01 (3.2168e-01)	Acc@1  91.00 ( 90.90)	Acc@5  99.00 ( 99.71)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 2.6854e-01 (3.2275e-01)	Acc@1  91.00 ( 90.82)	Acc@5  99.00 ( 99.71)
Test: [ 60/100]	Time  0.024 ( 0.029)	Loss 3.1846e-01 (3.1857e-01)	Acc@1  92.00 ( 90.80)	Acc@5 100.00 ( 99.72)
Test: [ 70/100]	Time  0.027 ( 0.028)	Loss 4.6867e-01 (3.1339e-01)	Acc@1  88.00 ( 90.83)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.027 ( 0.028)	Loss 2.1127e-01 (3.1246e-01)	Acc@1  92.00 ( 90.75)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.027 ( 0.027)	Loss 2.5016e-01 (3.0797e-01)	Acc@1  93.00 ( 90.84)	Acc@5 100.00 ( 99.77)
 * Acc@1 90.900 Acc@5 99.760
### epoch[72] execution time: 28.00854253768921
EPOCH 73
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [73][  0/391]	Time  0.246 ( 0.246)	Data  0.181 ( 0.181)	Loss 6.1165e-02 (6.1165e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [73][ 10/391]	Time  0.065 ( 0.080)	Data  0.001 ( 0.017)	Loss 3.6512e-02 (6.6775e-02)	Acc@1  99.22 ( 97.80)	Acc@5 100.00 (100.00)
Epoch: [73][ 20/391]	Time  0.064 ( 0.072)	Data  0.001 ( 0.010)	Loss 8.8487e-02 (7.8070e-02)	Acc@1  96.09 ( 97.17)	Acc@5 100.00 (100.00)
Epoch: [73][ 30/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.007)	Loss 7.0467e-02 (8.0705e-02)	Acc@1  97.66 ( 97.13)	Acc@5 100.00 (100.00)
Epoch: [73][ 40/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.005)	Loss 1.7296e-02 (8.0271e-02)	Acc@1 100.00 ( 97.18)	Acc@5 100.00 (100.00)
Epoch: [73][ 50/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.005)	Loss 2.4255e-02 (8.2675e-02)	Acc@1 100.00 ( 97.23)	Acc@5 100.00 (100.00)
Epoch: [73][ 60/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.004)	Loss 6.8954e-02 (8.3579e-02)	Acc@1  97.66 ( 97.18)	Acc@5 100.00 (100.00)
Epoch: [73][ 70/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.004)	Loss 1.0675e-01 (8.3477e-02)	Acc@1  93.75 ( 97.13)	Acc@5 100.00 (100.00)
Epoch: [73][ 80/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 7.3630e-02 (8.4058e-02)	Acc@1  96.09 ( 97.12)	Acc@5 100.00 (100.00)
Epoch: [73][ 90/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 5.4227e-02 (8.2911e-02)	Acc@1  97.66 ( 97.14)	Acc@5 100.00 ( 99.99)
Epoch: [73][100/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.3472e-01 (8.7072e-02)	Acc@1  96.88 ( 97.01)	Acc@5 100.00 ( 99.98)
Epoch: [73][110/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.003)	Loss 5.4612e-02 (8.6146e-02)	Acc@1  98.44 ( 97.02)	Acc@5 100.00 ( 99.99)
Epoch: [73][120/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.003)	Loss 8.6288e-02 (8.7595e-02)	Acc@1  97.66 ( 96.93)	Acc@5 100.00 ( 99.99)
Epoch: [73][130/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1902e-01 (8.7791e-02)	Acc@1  95.31 ( 96.90)	Acc@5 100.00 ( 99.99)
Epoch: [73][140/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.3363e-02 (8.6632e-02)	Acc@1  98.44 ( 96.98)	Acc@5 100.00 ( 99.99)
Epoch: [73][150/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2041e-01 (8.7454e-02)	Acc@1  94.53 ( 96.94)	Acc@5 100.00 ( 99.99)
Epoch: [73][160/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.5067e-02 (8.7505e-02)	Acc@1  98.44 ( 96.96)	Acc@5 100.00 ( 99.99)
Epoch: [73][170/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.4384e-02 (8.7258e-02)	Acc@1  97.66 ( 96.97)	Acc@5 100.00 ( 99.99)
Epoch: [73][180/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2262e-01 (8.8056e-02)	Acc@1  96.09 ( 96.94)	Acc@5 100.00 ( 99.99)
Epoch: [73][190/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.4443e-02 (8.7927e-02)	Acc@1  96.09 ( 96.93)	Acc@5 100.00 ( 99.99)
Epoch: [73][200/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.0542e-02 (8.7843e-02)	Acc@1  95.31 ( 96.95)	Acc@5 100.00 ( 99.98)
Epoch: [73][210/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1685e-01 (8.7491e-02)	Acc@1  95.31 ( 96.95)	Acc@5 100.00 ( 99.99)
Epoch: [73][220/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.9652e-02 (8.7406e-02)	Acc@1  98.44 ( 96.96)	Acc@5 100.00 ( 99.98)
Epoch: [73][230/391]	Time  0.066 ( 0.064)	Data  0.001 ( 0.002)	Loss 9.0638e-02 (8.7505e-02)	Acc@1  96.88 ( 96.94)	Acc@5 100.00 ( 99.98)
Epoch: [73][240/391]	Time  0.059 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.0901e-01 (8.7450e-02)	Acc@1  95.31 ( 96.93)	Acc@5 100.00 ( 99.98)
Epoch: [73][250/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 8.6580e-02 (8.7886e-02)	Acc@1  96.88 ( 96.91)	Acc@5 100.00 ( 99.98)
Epoch: [73][260/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.9119e-02 (8.7860e-02)	Acc@1  97.66 ( 96.91)	Acc@5 100.00 ( 99.99)
Epoch: [73][270/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.2618e-02 (8.7421e-02)	Acc@1  97.66 ( 96.93)	Acc@5 100.00 ( 99.99)
Epoch: [73][280/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 8.5796e-02 (8.7471e-02)	Acc@1  96.88 ( 96.93)	Acc@5 100.00 ( 99.99)
Epoch: [73][290/391]	Time  0.067 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.0720e-01 (8.7929e-02)	Acc@1  96.09 ( 96.90)	Acc@5 100.00 ( 99.99)
Epoch: [73][300/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.8179e-02 (8.8084e-02)	Acc@1  97.66 ( 96.91)	Acc@5 100.00 ( 99.99)
Epoch: [73][310/391]	Time  0.059 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.4089e-02 (8.7718e-02)	Acc@1  97.66 ( 96.92)	Acc@5 100.00 ( 99.99)
Epoch: [73][320/391]	Time  0.060 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.8748e-02 (8.7575e-02)	Acc@1  97.66 ( 96.93)	Acc@5 100.00 ( 99.99)
Epoch: [73][330/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.2767e-01 (8.7931e-02)	Acc@1  94.53 ( 96.91)	Acc@5 100.00 ( 99.99)
Epoch: [73][340/391]	Time  0.066 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.4625e-01 (8.8322e-02)	Acc@1  93.75 ( 96.89)	Acc@5 100.00 ( 99.99)
Epoch: [73][350/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.3591e-01 (8.8760e-02)	Acc@1  96.88 ( 96.89)	Acc@5 100.00 ( 99.99)
Epoch: [73][360/391]	Time  0.066 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.1101e-01 (8.8759e-02)	Acc@1  97.66 ( 96.90)	Acc@5 100.00 ( 99.99)
Epoch: [73][370/391]	Time  0.059 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.2596e-02 (8.8974e-02)	Acc@1  98.44 ( 96.89)	Acc@5 100.00 ( 99.99)
Epoch: [73][380/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.1415e-01 (8.9048e-02)	Acc@1  96.09 ( 96.89)	Acc@5 100.00 ( 99.99)
Epoch: [73][390/391]	Time  0.047 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.2547e-01 (8.9428e-02)	Acc@1  96.25 ( 96.88)	Acc@5  98.75 ( 99.98)
## e[73] optimizer.zero_grad (sum) time: 0.39713430404663086
## e[73]       loss.backward (sum) time: 6.8879876136779785
## e[73]      optimizer.step (sum) time: 3.3161587715148926
## epoch[73] training(only) time: 25.10389733314514
# Switched to evaluate mode...
Test: [  0/100]	Time  0.198 ( 0.198)	Loss 2.9630e-01 (2.9630e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.042)	Loss 3.7491e-01 (2.7924e-01)	Acc@1  90.00 ( 91.45)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.024 ( 0.034)	Loss 2.9060e-01 (2.9446e-01)	Acc@1  91.00 ( 91.24)	Acc@5  99.00 ( 99.71)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 3.1899e-01 (3.0756e-01)	Acc@1  90.00 ( 91.10)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.024 ( 0.030)	Loss 2.8911e-01 (3.1704e-01)	Acc@1  91.00 ( 90.85)	Acc@5  99.00 ( 99.71)
Test: [ 50/100]	Time  0.028 ( 0.029)	Loss 2.4878e-01 (3.1934e-01)	Acc@1  92.00 ( 90.84)	Acc@5  99.00 ( 99.71)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 3.1481e-01 (3.1444e-01)	Acc@1  93.00 ( 90.77)	Acc@5 100.00 ( 99.72)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 4.5013e-01 (3.1094e-01)	Acc@1  87.00 ( 90.73)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.024 ( 0.028)	Loss 2.0944e-01 (3.1033e-01)	Acc@1  92.00 ( 90.63)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.026 ( 0.027)	Loss 2.3458e-01 (3.0704e-01)	Acc@1  93.00 ( 90.70)	Acc@5 100.00 ( 99.77)
 * Acc@1 90.770 Acc@5 99.750
### epoch[73] execution time: 27.926408052444458
EPOCH 74
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [74][  0/391]	Time  0.256 ( 0.256)	Data  0.192 ( 0.192)	Loss 1.2251e-01 (1.2251e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
Epoch: [74][ 10/391]	Time  0.063 ( 0.081)	Data  0.001 ( 0.018)	Loss 1.0932e-01 (9.6473e-02)	Acc@1  96.09 ( 96.45)	Acc@5 100.00 ( 99.93)
Epoch: [74][ 20/391]	Time  0.062 ( 0.072)	Data  0.001 ( 0.010)	Loss 5.1167e-02 (9.2377e-02)	Acc@1  96.88 ( 96.43)	Acc@5 100.00 ( 99.96)
Epoch: [74][ 30/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.007)	Loss 1.0470e-01 (9.4866e-02)	Acc@1  97.66 ( 96.47)	Acc@5 100.00 ( 99.95)
Epoch: [74][ 40/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.006)	Loss 9.6924e-02 (8.8511e-02)	Acc@1  95.31 ( 96.80)	Acc@5 100.00 ( 99.94)
Epoch: [74][ 50/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.005)	Loss 1.0343e-01 (8.7150e-02)	Acc@1  94.53 ( 96.80)	Acc@5 100.00 ( 99.95)
Epoch: [74][ 60/391]	Time  0.061 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.4164e-01 (8.7190e-02)	Acc@1  95.31 ( 96.85)	Acc@5 100.00 ( 99.96)
Epoch: [74][ 70/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.004)	Loss 1.0971e-01 (8.7729e-02)	Acc@1  96.88 ( 96.81)	Acc@5 100.00 ( 99.97)
Epoch: [74][ 80/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 7.9154e-02 (8.8864e-02)	Acc@1  96.88 ( 96.75)	Acc@5 100.00 ( 99.97)
Epoch: [74][ 90/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.9725e-02 (8.7368e-02)	Acc@1 100.00 ( 96.81)	Acc@5 100.00 ( 99.97)
Epoch: [74][100/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.4636e-01 (8.7672e-02)	Acc@1  95.31 ( 96.84)	Acc@5 100.00 ( 99.96)
Epoch: [74][110/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.003)	Loss 6.9502e-02 (8.8633e-02)	Acc@1  97.66 ( 96.82)	Acc@5 100.00 ( 99.96)
Epoch: [74][120/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.003)	Loss 8.9437e-02 (8.8684e-02)	Acc@1  97.66 ( 96.83)	Acc@5 100.00 ( 99.96)
Epoch: [74][130/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.003)	Loss 8.9108e-02 (8.8243e-02)	Acc@1  96.09 ( 96.85)	Acc@5 100.00 ( 99.96)
Epoch: [74][140/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.5936e-02 (8.6914e-02)	Acc@1  96.09 ( 96.91)	Acc@5 100.00 ( 99.97)
Epoch: [74][150/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.3639e-02 (8.7828e-02)	Acc@1  99.22 ( 96.89)	Acc@5 100.00 ( 99.97)
Epoch: [74][160/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.5424e-02 (8.7911e-02)	Acc@1  97.66 ( 96.87)	Acc@5 100.00 ( 99.97)
Epoch: [74][170/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.7160e-02 (8.7743e-02)	Acc@1  97.66 ( 96.88)	Acc@5 100.00 ( 99.97)
Epoch: [74][180/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.5307e-02 (8.7751e-02)	Acc@1  99.22 ( 96.89)	Acc@5 100.00 ( 99.97)
Epoch: [74][190/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.4123e-02 (8.7931e-02)	Acc@1  97.66 ( 96.89)	Acc@5 100.00 ( 99.98)
Epoch: [74][200/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2224e-02 (8.7199e-02)	Acc@1  99.22 ( 96.91)	Acc@5 100.00 ( 99.98)
Epoch: [74][210/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.2795e-02 (8.7357e-02)	Acc@1  96.88 ( 96.89)	Acc@5 100.00 ( 99.98)
Epoch: [74][220/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.4067e-02 (8.7283e-02)	Acc@1  96.09 ( 96.89)	Acc@5 100.00 ( 99.98)
Epoch: [74][230/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.2029e-01 (8.7443e-02)	Acc@1  94.53 ( 96.88)	Acc@5 100.00 ( 99.98)
Epoch: [74][240/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.7426e-02 (8.7168e-02)	Acc@1  96.88 ( 96.90)	Acc@5 100.00 ( 99.98)
Epoch: [74][250/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.5685e-01 (8.7395e-02)	Acc@1  95.31 ( 96.88)	Acc@5 100.00 ( 99.98)
Epoch: [74][260/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.8201e-01 (8.7945e-02)	Acc@1  93.75 ( 96.88)	Acc@5 100.00 ( 99.98)
Epoch: [74][270/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.7000e-01 (8.8327e-02)	Acc@1  94.53 ( 96.87)	Acc@5 100.00 ( 99.98)
Epoch: [74][280/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 9.3202e-02 (8.7688e-02)	Acc@1  96.09 ( 96.90)	Acc@5 100.00 ( 99.98)
Epoch: [74][290/391]	Time  0.067 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.1406e-01 (8.7940e-02)	Acc@1  96.88 ( 96.89)	Acc@5 100.00 ( 99.98)
Epoch: [74][300/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.5120e-02 (8.8583e-02)	Acc@1  98.44 ( 96.86)	Acc@5 100.00 ( 99.98)
Epoch: [74][310/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.2042e-02 (8.8621e-02)	Acc@1  98.44 ( 96.86)	Acc@5 100.00 ( 99.98)
Epoch: [74][320/391]	Time  0.070 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.2796e-01 (8.9040e-02)	Acc@1  93.75 ( 96.84)	Acc@5 100.00 ( 99.98)
Epoch: [74][330/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.2299e-02 (8.8966e-02)	Acc@1  97.66 ( 96.85)	Acc@5 100.00 ( 99.98)
Epoch: [74][340/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.5393e-02 (8.8524e-02)	Acc@1  98.44 ( 96.88)	Acc@5 100.00 ( 99.98)
Epoch: [74][350/391]	Time  0.068 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.7168e-02 (8.8690e-02)	Acc@1  99.22 ( 96.88)	Acc@5 100.00 ( 99.98)
Epoch: [74][360/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 9.7322e-02 (8.8458e-02)	Acc@1  96.09 ( 96.90)	Acc@5 100.00 ( 99.98)
Epoch: [74][370/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 9.6568e-02 (8.8624e-02)	Acc@1  95.31 ( 96.90)	Acc@5 100.00 ( 99.98)
Epoch: [74][380/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 8.8173e-02 (8.8640e-02)	Acc@1  96.88 ( 96.89)	Acc@5 100.00 ( 99.98)
Epoch: [74][390/391]	Time  0.048 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.3080e-02 (8.8749e-02)	Acc@1  97.50 ( 96.88)	Acc@5 100.00 ( 99.98)
## e[74] optimizer.zero_grad (sum) time: 0.39582204818725586
## e[74]       loss.backward (sum) time: 6.927946329116821
## e[74]      optimizer.step (sum) time: 3.3079073429107666
## epoch[74] training(only) time: 25.197124004364014
# Switched to evaluate mode...
Test: [  0/100]	Time  0.198 ( 0.198)	Loss 3.1390e-01 (3.1390e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.042)	Loss 4.2001e-01 (2.8794e-01)	Acc@1  89.00 ( 91.18)	Acc@5  99.00 ( 99.55)
Test: [ 20/100]	Time  0.025 ( 0.034)	Loss 3.0602e-01 (2.9847e-01)	Acc@1  89.00 ( 90.95)	Acc@5  99.00 ( 99.57)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 3.5564e-01 (3.1387e-01)	Acc@1  90.00 ( 90.97)	Acc@5 100.00 ( 99.61)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 2.8730e-01 (3.2191e-01)	Acc@1  91.00 ( 90.80)	Acc@5  99.00 ( 99.63)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 2.6120e-01 (3.2343e-01)	Acc@1  94.00 ( 90.80)	Acc@5  99.00 ( 99.63)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 3.1294e-01 (3.1915e-01)	Acc@1  93.00 ( 90.82)	Acc@5 100.00 ( 99.67)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 4.7588e-01 (3.1441e-01)	Acc@1  87.00 ( 90.85)	Acc@5 100.00 ( 99.70)
Test: [ 80/100]	Time  0.026 ( 0.028)	Loss 2.0519e-01 (3.1368e-01)	Acc@1  93.00 ( 90.83)	Acc@5 100.00 ( 99.73)
Test: [ 90/100]	Time  0.024 ( 0.028)	Loss 2.4372e-01 (3.0934e-01)	Acc@1  93.00 ( 90.90)	Acc@5 100.00 ( 99.75)
 * Acc@1 90.960 Acc@5 99.750
### epoch[74] execution time: 28.053970098495483
EPOCH 75
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [75][  0/391]	Time  0.252 ( 0.252)	Data  0.189 ( 0.189)	Loss 1.3273e-01 (1.3273e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [75][ 10/391]	Time  0.063 ( 0.080)	Data  0.001 ( 0.018)	Loss 1.6806e-01 (8.9420e-02)	Acc@1  94.53 ( 96.95)	Acc@5 100.00 (100.00)
Epoch: [75][ 20/391]	Time  0.061 ( 0.072)	Data  0.001 ( 0.010)	Loss 4.6719e-02 (8.7376e-02)	Acc@1  98.44 ( 97.06)	Acc@5 100.00 ( 99.96)
Epoch: [75][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.007)	Loss 5.5619e-02 (8.8922e-02)	Acc@1  97.66 ( 96.95)	Acc@5 100.00 ( 99.95)
Epoch: [75][ 40/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.006)	Loss 9.3762e-02 (8.9758e-02)	Acc@1  98.44 ( 96.91)	Acc@5 100.00 ( 99.96)
Epoch: [75][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.005)	Loss 1.1030e-01 (9.0748e-02)	Acc@1  96.88 ( 96.91)	Acc@5 100.00 ( 99.94)
Epoch: [75][ 60/391]	Time  0.058 ( 0.067)	Data  0.001 ( 0.004)	Loss 3.6644e-02 (8.5427e-02)	Acc@1  98.44 ( 97.09)	Acc@5 100.00 ( 99.95)
Epoch: [75][ 70/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.004)	Loss 9.3847e-02 (8.6000e-02)	Acc@1  96.88 ( 97.12)	Acc@5 100.00 ( 99.96)
Epoch: [75][ 80/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.003)	Loss 7.6780e-02 (8.6581e-02)	Acc@1  97.66 ( 97.14)	Acc@5 100.00 ( 99.96)
Epoch: [75][ 90/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 8.1020e-02 (8.5874e-02)	Acc@1  97.66 ( 97.21)	Acc@5 100.00 ( 99.96)
Epoch: [75][100/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.003)	Loss 9.3061e-02 (8.4869e-02)	Acc@1  96.88 ( 97.25)	Acc@5 100.00 ( 99.96)
Epoch: [75][110/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.003)	Loss 9.5245e-02 (8.5760e-02)	Acc@1  96.88 ( 97.21)	Acc@5 100.00 ( 99.96)
Epoch: [75][120/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.003)	Loss 7.2264e-02 (8.5930e-02)	Acc@1  97.66 ( 97.21)	Acc@5 100.00 ( 99.95)
Epoch: [75][130/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.003)	Loss 3.4434e-02 (8.7074e-02)	Acc@1 100.00 ( 97.15)	Acc@5 100.00 ( 99.96)
Epoch: [75][140/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0149e-01 (8.7962e-02)	Acc@1  98.44 ( 97.11)	Acc@5  99.22 ( 99.94)
Epoch: [75][150/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.6817e-02 (8.7946e-02)	Acc@1  95.31 ( 97.08)	Acc@5 100.00 ( 99.95)
Epoch: [75][160/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.8489e-02 (8.8863e-02)	Acc@1  96.88 ( 97.06)	Acc@5 100.00 ( 99.95)
Epoch: [75][170/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4301e-01 (8.8871e-02)	Acc@1  92.97 ( 97.03)	Acc@5 100.00 ( 99.95)
Epoch: [75][180/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0278e-01 (8.8596e-02)	Acc@1  96.88 ( 97.05)	Acc@5 100.00 ( 99.95)
Epoch: [75][190/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.1531e-02 (8.8029e-02)	Acc@1  96.88 ( 97.07)	Acc@5 100.00 ( 99.96)
Epoch: [75][200/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.7742e-02 (8.7866e-02)	Acc@1  98.44 ( 97.07)	Acc@5 100.00 ( 99.96)
Epoch: [75][210/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.9156e-02 (8.7399e-02)	Acc@1  97.66 ( 97.09)	Acc@5 100.00 ( 99.96)
Epoch: [75][220/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.1465e-02 (8.6848e-02)	Acc@1  98.44 ( 97.10)	Acc@5 100.00 ( 99.96)
Epoch: [75][230/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4145e-01 (8.6618e-02)	Acc@1  93.75 ( 97.09)	Acc@5 100.00 ( 99.96)
Epoch: [75][240/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 8.9013e-02 (8.6521e-02)	Acc@1  96.09 ( 97.10)	Acc@5 100.00 ( 99.96)
Epoch: [75][250/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 8.8711e-02 (8.6499e-02)	Acc@1  96.09 ( 97.10)	Acc@5 100.00 ( 99.97)
Epoch: [75][260/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.1483e-01 (8.6566e-02)	Acc@1  95.31 ( 97.09)	Acc@5 100.00 ( 99.96)
Epoch: [75][270/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.4177e-01 (8.6769e-02)	Acc@1  94.53 ( 97.08)	Acc@5 100.00 ( 99.97)
Epoch: [75][280/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.3762e-02 (8.6285e-02)	Acc@1  97.66 ( 97.10)	Acc@5 100.00 ( 99.97)
Epoch: [75][290/391]	Time  0.066 ( 0.064)	Data  0.001 ( 0.002)	Loss 8.5732e-02 (8.6173e-02)	Acc@1  96.88 ( 97.10)	Acc@5 100.00 ( 99.97)
Epoch: [75][300/391]	Time  0.068 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.0260e-01 (8.6030e-02)	Acc@1  95.31 ( 97.10)	Acc@5 100.00 ( 99.97)
Epoch: [75][310/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.9962e-02 (8.5752e-02)	Acc@1  98.44 ( 97.12)	Acc@5 100.00 ( 99.97)
Epoch: [75][320/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.5305e-02 (8.5675e-02)	Acc@1  95.31 ( 97.12)	Acc@5 100.00 ( 99.97)
Epoch: [75][330/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.5321e-01 (8.5892e-02)	Acc@1  93.75 ( 97.09)	Acc@5 100.00 ( 99.97)
Epoch: [75][340/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.9240e-02 (8.5726e-02)	Acc@1  97.66 ( 97.10)	Acc@5 100.00 ( 99.97)
Epoch: [75][350/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.9984e-02 (8.5882e-02)	Acc@1  98.44 ( 97.10)	Acc@5 100.00 ( 99.97)
Epoch: [75][360/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.1709e-02 (8.6037e-02)	Acc@1  97.66 ( 97.10)	Acc@5 100.00 ( 99.97)
Epoch: [75][370/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.3090e-01 (8.5930e-02)	Acc@1  94.53 ( 97.09)	Acc@5 100.00 ( 99.97)
Epoch: [75][380/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.3577e-02 (8.5808e-02)	Acc@1  97.66 ( 97.08)	Acc@5 100.00 ( 99.98)
Epoch: [75][390/391]	Time  0.045 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.5141e-02 (8.6341e-02)	Acc@1  98.75 ( 97.05)	Acc@5 100.00 ( 99.98)
## e[75] optimizer.zero_grad (sum) time: 0.39044642448425293
## e[75]       loss.backward (sum) time: 6.904965162277222
## e[75]      optimizer.step (sum) time: 3.361588954925537
## epoch[75] training(only) time: 25.199402809143066
# Switched to evaluate mode...
Test: [  0/100]	Time  0.197 ( 0.197)	Loss 3.1365e-01 (3.1365e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.026 ( 0.042)	Loss 3.8932e-01 (2.8752e-01)	Acc@1  90.00 ( 91.09)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.024 ( 0.035)	Loss 3.1208e-01 (2.9717e-01)	Acc@1  89.00 ( 91.10)	Acc@5  99.00 ( 99.71)
Test: [ 30/100]	Time  0.028 ( 0.032)	Loss 3.3832e-01 (3.1062e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.024 ( 0.030)	Loss 3.0872e-01 (3.2062e-01)	Acc@1  90.00 ( 90.76)	Acc@5  99.00 ( 99.71)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 2.5364e-01 (3.2254e-01)	Acc@1  94.00 ( 90.76)	Acc@5  99.00 ( 99.69)
Test: [ 60/100]	Time  0.028 ( 0.029)	Loss 3.1595e-01 (3.1850e-01)	Acc@1  93.00 ( 90.69)	Acc@5 100.00 ( 99.70)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 4.9270e-01 (3.1469e-01)	Acc@1  88.00 ( 90.75)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.024 ( 0.028)	Loss 2.0218e-01 (3.1351e-01)	Acc@1  94.00 ( 90.70)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.024 ( 0.028)	Loss 2.2538e-01 (3.0947e-01)	Acc@1  93.00 ( 90.76)	Acc@5 100.00 ( 99.77)
 * Acc@1 90.840 Acc@5 99.770
### epoch[75] execution time: 28.09409260749817
EPOCH 76
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [76][  0/391]	Time  0.245 ( 0.245)	Data  0.177 ( 0.177)	Loss 1.5227e-01 (1.5227e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
Epoch: [76][ 10/391]	Time  0.066 ( 0.080)	Data  0.001 ( 0.017)	Loss 4.1115e-02 (8.1009e-02)	Acc@1  99.22 ( 97.30)	Acc@5 100.00 (100.00)
Epoch: [76][ 20/391]	Time  0.064 ( 0.073)	Data  0.001 ( 0.010)	Loss 1.5495e-01 (8.1534e-02)	Acc@1  95.31 ( 97.36)	Acc@5 100.00 (100.00)
Epoch: [76][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.007)	Loss 4.8260e-02 (8.1245e-02)	Acc@1  97.66 ( 97.28)	Acc@5 100.00 (100.00)
Epoch: [76][ 40/391]	Time  0.063 ( 0.069)	Data  0.001 ( 0.005)	Loss 1.3411e-01 (8.5650e-02)	Acc@1  94.53 ( 97.01)	Acc@5 100.00 ( 99.98)
Epoch: [76][ 50/391]	Time  0.060 ( 0.068)	Data  0.001 ( 0.005)	Loss 1.0755e-01 (8.5402e-02)	Acc@1  95.31 ( 97.03)	Acc@5  99.22 ( 99.97)
Epoch: [76][ 60/391]	Time  0.060 ( 0.067)	Data  0.001 ( 0.004)	Loss 7.6124e-02 (8.5908e-02)	Acc@1  99.22 ( 97.04)	Acc@5 100.00 ( 99.95)
Epoch: [76][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 6.1268e-02 (8.4940e-02)	Acc@1  97.66 ( 97.04)	Acc@5 100.00 ( 99.94)
Epoch: [76][ 80/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 9.5217e-02 (8.3818e-02)	Acc@1  96.09 ( 97.11)	Acc@5 100.00 ( 99.95)
Epoch: [76][ 90/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.1468e-01 (8.7315e-02)	Acc@1  94.53 ( 96.99)	Acc@5 100.00 ( 99.96)
Epoch: [76][100/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.003)	Loss 6.4160e-02 (8.6864e-02)	Acc@1  97.66 ( 97.01)	Acc@5 100.00 ( 99.96)
Epoch: [76][110/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.4650e-02 (8.6858e-02)	Acc@1  99.22 ( 97.01)	Acc@5 100.00 ( 99.96)
Epoch: [76][120/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 7.8506e-02 (8.6682e-02)	Acc@1  97.66 ( 97.04)	Acc@5 100.00 ( 99.96)
Epoch: [76][130/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.4858e-02 (8.5689e-02)	Acc@1  98.44 ( 97.07)	Acc@5 100.00 ( 99.96)
Epoch: [76][140/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.4752e-02 (8.5551e-02)	Acc@1  96.09 ( 97.07)	Acc@5 100.00 ( 99.97)
Epoch: [76][150/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3951e-01 (8.5008e-02)	Acc@1  95.31 ( 97.08)	Acc@5 100.00 ( 99.97)
Epoch: [76][160/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.3281e-02 (8.4663e-02)	Acc@1  99.22 ( 97.13)	Acc@5 100.00 ( 99.97)
Epoch: [76][170/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1299e-01 (8.6146e-02)	Acc@1  94.53 ( 97.05)	Acc@5 100.00 ( 99.97)
Epoch: [76][180/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.3771e-02 (8.5852e-02)	Acc@1  98.44 ( 97.06)	Acc@5 100.00 ( 99.97)
Epoch: [76][190/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.0553e-02 (8.6025e-02)	Acc@1  95.31 ( 97.04)	Acc@5 100.00 ( 99.98)
Epoch: [76][200/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.2769e-02 (8.6285e-02)	Acc@1  97.66 ( 97.05)	Acc@5 100.00 ( 99.97)
Epoch: [76][210/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1623e-01 (8.6641e-02)	Acc@1  95.31 ( 97.05)	Acc@5 100.00 ( 99.97)
Epoch: [76][220/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.0073e-02 (8.6113e-02)	Acc@1  98.44 ( 97.06)	Acc@5 100.00 ( 99.98)
Epoch: [76][230/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.9392e-02 (8.6325e-02)	Acc@1  97.66 ( 97.07)	Acc@5 100.00 ( 99.97)
Epoch: [76][240/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.2206e-02 (8.7081e-02)	Acc@1  98.44 ( 97.04)	Acc@5 100.00 ( 99.97)
Epoch: [76][250/391]	Time  0.073 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0749e-01 (8.6762e-02)	Acc@1  96.88 ( 97.04)	Acc@5 100.00 ( 99.97)
Epoch: [76][260/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.7627e-02 (8.6621e-02)	Acc@1  96.09 ( 97.04)	Acc@5 100.00 ( 99.97)
Epoch: [76][270/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.9462e-02 (8.6457e-02)	Acc@1  94.53 ( 97.04)	Acc@5 100.00 ( 99.97)
Epoch: [76][280/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3830e-01 (8.6722e-02)	Acc@1  95.31 ( 97.02)	Acc@5 100.00 ( 99.97)
Epoch: [76][290/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.3684e-02 (8.6721e-02)	Acc@1  98.44 ( 97.02)	Acc@5 100.00 ( 99.97)
Epoch: [76][300/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.5262e-02 (8.6493e-02)	Acc@1  96.88 ( 97.02)	Acc@5 100.00 ( 99.97)
Epoch: [76][310/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0646e-01 (8.6279e-02)	Acc@1  96.09 ( 97.03)	Acc@5 100.00 ( 99.97)
Epoch: [76][320/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.6540e-02 (8.5968e-02)	Acc@1  96.88 ( 97.04)	Acc@5 100.00 ( 99.97)
Epoch: [76][330/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.5877e-02 (8.6214e-02)	Acc@1  96.88 ( 97.05)	Acc@5 100.00 ( 99.97)
Epoch: [76][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0818e-01 (8.6295e-02)	Acc@1  96.88 ( 97.05)	Acc@5 100.00 ( 99.97)
Epoch: [76][350/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.6805e-02 (8.6707e-02)	Acc@1  96.09 ( 97.03)	Acc@5 100.00 ( 99.98)
Epoch: [76][360/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.9039e-02 (8.6945e-02)	Acc@1  97.66 ( 97.04)	Acc@5 100.00 ( 99.98)
Epoch: [76][370/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.7552e-02 (8.6947e-02)	Acc@1  99.22 ( 97.03)	Acc@5 100.00 ( 99.98)
Epoch: [76][380/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.8178e-02 (8.6607e-02)	Acc@1  98.44 ( 97.04)	Acc@5 100.00 ( 99.98)
Epoch: [76][390/391]	Time  0.048 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.0902e-01 (8.6553e-02)	Acc@1  96.25 ( 97.05)	Acc@5 100.00 ( 99.98)
## e[76] optimizer.zero_grad (sum) time: 0.4024498462677002
## e[76]       loss.backward (sum) time: 6.953493118286133
## e[76]      optimizer.step (sum) time: 3.364593505859375
## epoch[76] training(only) time: 25.302754402160645
# Switched to evaluate mode...
Test: [  0/100]	Time  0.201 ( 0.201)	Loss 3.1531e-01 (3.1531e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.041)	Loss 3.9341e-01 (2.8913e-01)	Acc@1  90.00 ( 91.09)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.028 ( 0.033)	Loss 3.0554e-01 (3.0064e-01)	Acc@1  90.00 ( 90.86)	Acc@5  99.00 ( 99.67)
Test: [ 30/100]	Time  0.034 ( 0.031)	Loss 3.4936e-01 (3.1587e-01)	Acc@1  91.00 ( 90.87)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 2.9338e-01 (3.2363e-01)	Acc@1  91.00 ( 90.73)	Acc@5  99.00 ( 99.71)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 2.3736e-01 (3.2454e-01)	Acc@1  93.00 ( 90.73)	Acc@5  99.00 ( 99.69)
Test: [ 60/100]	Time  0.027 ( 0.028)	Loss 3.1785e-01 (3.2032e-01)	Acc@1  92.00 ( 90.69)	Acc@5 100.00 ( 99.70)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 4.5860e-01 (3.1524e-01)	Acc@1  87.00 ( 90.70)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.024 ( 0.028)	Loss 1.9186e-01 (3.1486e-01)	Acc@1  93.00 ( 90.69)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 2.3539e-01 (3.1026e-01)	Acc@1  93.00 ( 90.74)	Acc@5 100.00 ( 99.76)
 * Acc@1 90.780 Acc@5 99.750
### epoch[76] execution time: 28.17807650566101
EPOCH 77
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [77][  0/391]	Time  0.254 ( 0.254)	Data  0.186 ( 0.186)	Loss 1.6019e-01 (1.6019e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
Epoch: [77][ 10/391]	Time  0.064 ( 0.081)	Data  0.001 ( 0.018)	Loss 1.1609e-01 (8.5697e-02)	Acc@1  94.53 ( 96.52)	Acc@5 100.00 (100.00)
Epoch: [77][ 20/391]	Time  0.065 ( 0.073)	Data  0.001 ( 0.010)	Loss 1.3246e-01 (8.4290e-02)	Acc@1  96.09 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [77][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.007)	Loss 1.3925e-01 (8.5857e-02)	Acc@1  95.31 ( 96.95)	Acc@5 100.00 (100.00)
Epoch: [77][ 40/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.006)	Loss 1.1163e-01 (8.5576e-02)	Acc@1  97.66 ( 97.03)	Acc@5  99.22 ( 99.98)
Epoch: [77][ 50/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.005)	Loss 6.6864e-02 (8.6913e-02)	Acc@1  98.44 ( 97.07)	Acc@5 100.00 ( 99.98)
Epoch: [77][ 60/391]	Time  0.061 ( 0.067)	Data  0.001 ( 0.004)	Loss 7.9518e-02 (8.4963e-02)	Acc@1  97.66 ( 97.03)	Acc@5 100.00 ( 99.99)
Epoch: [77][ 70/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.004)	Loss 1.1924e-01 (8.5966e-02)	Acc@1  96.09 ( 97.01)	Acc@5 100.00 ( 99.99)
Epoch: [77][ 80/391]	Time  0.074 ( 0.066)	Data  0.001 ( 0.003)	Loss 8.7155e-02 (8.6183e-02)	Acc@1  96.88 ( 97.02)	Acc@5 100.00 ( 99.99)
Epoch: [77][ 90/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.003)	Loss 7.4959e-02 (8.6570e-02)	Acc@1  97.66 ( 96.98)	Acc@5 100.00 ( 99.99)
Epoch: [77][100/391]	Time  0.059 ( 0.066)	Data  0.001 ( 0.003)	Loss 9.7785e-02 (8.6730e-02)	Acc@1  96.09 ( 96.99)	Acc@5 100.00 ( 99.99)
Epoch: [77][110/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 9.3675e-02 (8.5364e-02)	Acc@1  96.09 ( 97.07)	Acc@5 100.00 ( 99.99)
Epoch: [77][120/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.003)	Loss 8.9189e-02 (8.5355e-02)	Acc@1  96.09 ( 97.08)	Acc@5 100.00 ( 99.99)
Epoch: [77][130/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 7.2753e-02 (8.4914e-02)	Acc@1  98.44 ( 97.09)	Acc@5 100.00 ( 99.99)
Epoch: [77][140/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.9370e-02 (8.5313e-02)	Acc@1  96.88 ( 97.05)	Acc@5 100.00 ( 99.99)
Epoch: [77][150/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.3880e-02 (8.5350e-02)	Acc@1  97.66 ( 97.05)	Acc@5 100.00 ( 99.98)
Epoch: [77][160/391]	Time  0.070 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.2347e-02 (8.5742e-02)	Acc@1  97.66 ( 97.04)	Acc@5 100.00 ( 99.99)
Epoch: [77][170/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1748e-01 (8.6430e-02)	Acc@1  96.09 ( 97.03)	Acc@5 100.00 ( 99.99)
Epoch: [77][180/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.2198e-02 (8.6316e-02)	Acc@1  97.66 ( 97.03)	Acc@5 100.00 ( 99.99)
Epoch: [77][190/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.9158e-02 (8.6288e-02)	Acc@1  97.66 ( 97.02)	Acc@5 100.00 ( 99.99)
Epoch: [77][200/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.2883e-02 (8.5423e-02)	Acc@1  95.31 ( 97.05)	Acc@5 100.00 ( 99.99)
Epoch: [77][210/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.7582e-02 (8.4426e-02)	Acc@1  96.88 ( 97.07)	Acc@5 100.00 ( 99.99)
Epoch: [77][220/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.3271e-02 (8.4638e-02)	Acc@1  96.88 ( 97.03)	Acc@5 100.00 ( 99.99)
Epoch: [77][230/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9503e-01 (8.4921e-02)	Acc@1  92.97 ( 97.02)	Acc@5 100.00 ( 99.99)
Epoch: [77][240/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.4530e-02 (8.4612e-02)	Acc@1  98.44 ( 97.03)	Acc@5 100.00 ( 99.99)
Epoch: [77][250/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.4079e-02 (8.5232e-02)	Acc@1  96.88 ( 97.02)	Acc@5 100.00 ( 99.99)
Epoch: [77][260/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.6431e-02 (8.5428e-02)	Acc@1  97.66 ( 97.02)	Acc@5 100.00 ( 99.99)
Epoch: [77][270/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4244e-01 (8.5752e-02)	Acc@1  94.53 ( 97.00)	Acc@5 100.00 ( 99.99)
Epoch: [77][280/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1041e-01 (8.5806e-02)	Acc@1  96.88 ( 97.01)	Acc@5 100.00 ( 99.99)
Epoch: [77][290/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.9294e-02 (8.6484e-02)	Acc@1  97.66 ( 97.00)	Acc@5 100.00 ( 99.99)
Epoch: [77][300/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.3286e-02 (8.6676e-02)	Acc@1  96.88 ( 96.98)	Acc@5 100.00 ( 99.99)
Epoch: [77][310/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.2114e-02 (8.6406e-02)	Acc@1  96.09 ( 96.99)	Acc@5 100.00 ( 99.99)
Epoch: [77][320/391]	Time  0.066 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.2139e-01 (8.6725e-02)	Acc@1  95.31 ( 96.97)	Acc@5 100.00 ( 99.99)
Epoch: [77][330/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.0848e-01 (8.6713e-02)	Acc@1  95.31 ( 96.97)	Acc@5 100.00 ( 99.99)
Epoch: [77][340/391]	Time  0.071 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.8745e-02 (8.6441e-02)	Acc@1  99.22 ( 96.98)	Acc@5 100.00 ( 99.99)
Epoch: [77][350/391]	Time  0.067 ( 0.064)	Data  0.004 ( 0.002)	Loss 3.5998e-02 (8.6657e-02)	Acc@1  98.44 ( 96.97)	Acc@5 100.00 ( 99.99)
Epoch: [77][360/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.8506e-02 (8.6378e-02)	Acc@1  97.66 ( 96.97)	Acc@5 100.00 ( 99.99)
Epoch: [77][370/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.0806e-01 (8.6327e-02)	Acc@1  92.97 ( 96.97)	Acc@5 100.00 ( 99.99)
Epoch: [77][380/391]	Time  0.060 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.9130e-02 (8.6290e-02)	Acc@1  98.44 ( 96.98)	Acc@5 100.00 ( 99.99)
Epoch: [77][390/391]	Time  0.047 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.0043e-01 (8.6608e-02)	Acc@1  96.25 ( 96.97)	Acc@5 100.00 ( 99.99)
## e[77] optimizer.zero_grad (sum) time: 0.39653611183166504
## e[77]       loss.backward (sum) time: 6.928463459014893
## e[77]      optimizer.step (sum) time: 3.3938205242156982
## epoch[77] training(only) time: 25.290854930877686
# Switched to evaluate mode...
Test: [  0/100]	Time  0.192 ( 0.192)	Loss 3.1749e-01 (3.1749e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.027 ( 0.041)	Loss 4.0314e-01 (2.8903e-01)	Acc@1  89.00 ( 91.27)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 3.2047e-01 (2.9877e-01)	Acc@1  90.00 ( 91.19)	Acc@5  99.00 ( 99.62)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 3.4310e-01 (3.1040e-01)	Acc@1  90.00 ( 91.16)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.024 ( 0.030)	Loss 3.2994e-01 (3.2114e-01)	Acc@1  91.00 ( 90.95)	Acc@5  99.00 ( 99.68)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 2.9080e-01 (3.2330e-01)	Acc@1  90.00 ( 90.84)	Acc@5  99.00 ( 99.69)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 3.1488e-01 (3.2002e-01)	Acc@1  93.00 ( 90.84)	Acc@5 100.00 ( 99.70)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 4.7681e-01 (3.1576e-01)	Acc@1  88.00 ( 90.90)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.024 ( 0.028)	Loss 1.9109e-01 (3.1381e-01)	Acc@1  94.00 ( 90.91)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 2.1674e-01 (3.0976e-01)	Acc@1  93.00 ( 90.99)	Acc@5 100.00 ( 99.76)
 * Acc@1 91.020 Acc@5 99.760
### epoch[77] execution time: 28.125986337661743
EPOCH 78
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [78][  0/391]	Time  0.254 ( 0.254)	Data  0.188 ( 0.188)	Loss 7.4752e-02 (7.4752e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [78][ 10/391]	Time  0.066 ( 0.081)	Data  0.001 ( 0.018)	Loss 9.5193e-02 (7.7828e-02)	Acc@1  96.88 ( 97.30)	Acc@5 100.00 ( 99.93)
Epoch: [78][ 20/391]	Time  0.059 ( 0.072)	Data  0.001 ( 0.010)	Loss 1.1091e-01 (7.5350e-02)	Acc@1  95.31 ( 97.28)	Acc@5 100.00 ( 99.96)
Epoch: [78][ 30/391]	Time  0.059 ( 0.069)	Data  0.001 ( 0.007)	Loss 1.2225e-01 (8.0393e-02)	Acc@1  95.31 ( 97.25)	Acc@5 100.00 ( 99.97)
Epoch: [78][ 40/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.006)	Loss 8.9905e-02 (8.1755e-02)	Acc@1  95.31 ( 97.14)	Acc@5 100.00 ( 99.98)
Epoch: [78][ 50/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.005)	Loss 1.6892e-01 (8.2478e-02)	Acc@1  96.09 ( 97.15)	Acc@5 100.00 ( 99.98)
Epoch: [78][ 60/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.004)	Loss 6.5739e-02 (8.2471e-02)	Acc@1  97.66 ( 97.11)	Acc@5 100.00 ( 99.99)
Epoch: [78][ 70/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.004)	Loss 4.2809e-02 (8.3860e-02)	Acc@1  98.44 ( 97.03)	Acc@5 100.00 ( 99.99)
Epoch: [78][ 80/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.003)	Loss 6.6017e-02 (8.3950e-02)	Acc@1  96.88 ( 96.99)	Acc@5 100.00 ( 99.98)
Epoch: [78][ 90/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.003)	Loss 6.1245e-02 (8.4062e-02)	Acc@1  99.22 ( 97.00)	Acc@5 100.00 ( 99.98)
Epoch: [78][100/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.003)	Loss 5.6117e-02 (8.4626e-02)	Acc@1  97.66 ( 96.98)	Acc@5 100.00 ( 99.98)
Epoch: [78][110/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.0926e-01 (8.5545e-02)	Acc@1  96.09 ( 96.95)	Acc@5 100.00 ( 99.99)
Epoch: [78][120/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 7.2809e-02 (8.5560e-02)	Acc@1  99.22 ( 96.97)	Acc@5 100.00 ( 99.99)
Epoch: [78][130/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 6.4205e-02 (8.4901e-02)	Acc@1  99.22 ( 97.01)	Acc@5 100.00 ( 99.99)
Epoch: [78][140/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1454e-01 (8.5336e-02)	Acc@1  96.88 ( 96.95)	Acc@5 100.00 ( 99.99)
Epoch: [78][150/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.5198e-02 (8.5961e-02)	Acc@1  95.31 ( 96.89)	Acc@5 100.00 ( 99.99)
Epoch: [78][160/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.2199e-02 (8.5341e-02)	Acc@1  99.22 ( 96.91)	Acc@5 100.00 ( 99.99)
Epoch: [78][170/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5935e-01 (8.5201e-02)	Acc@1  93.75 ( 96.92)	Acc@5 100.00 ( 99.99)
Epoch: [78][180/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.1502e-02 (8.5451e-02)	Acc@1  97.66 ( 96.90)	Acc@5 100.00 ( 99.99)
Epoch: [78][190/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.9380e-02 (8.6541e-02)	Acc@1  97.66 ( 96.88)	Acc@5 100.00 ( 99.98)
Epoch: [78][200/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.0481e-02 (8.5820e-02)	Acc@1  97.66 ( 96.93)	Acc@5 100.00 ( 99.98)
Epoch: [78][210/391]	Time  0.075 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.7639e-02 (8.5907e-02)	Acc@1  97.66 ( 96.93)	Acc@5 100.00 ( 99.99)
Epoch: [78][220/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.2683e-02 (8.6281e-02)	Acc@1  95.31 ( 96.90)	Acc@5 100.00 ( 99.99)
Epoch: [78][230/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.4374e-02 (8.6857e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 ( 99.99)
Epoch: [78][240/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.6636e-02 (8.5982e-02)	Acc@1  96.09 ( 96.91)	Acc@5 100.00 ( 99.99)
Epoch: [78][250/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7796e-01 (8.5847e-02)	Acc@1  92.97 ( 96.92)	Acc@5 100.00 ( 99.99)
Epoch: [78][260/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.3747e-02 (8.5639e-02)	Acc@1  99.22 ( 96.91)	Acc@5 100.00 ( 99.99)
Epoch: [78][270/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.4088e-02 (8.5150e-02)	Acc@1  97.66 ( 96.92)	Acc@5 100.00 ( 99.99)
Epoch: [78][280/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3050e-01 (8.5251e-02)	Acc@1  92.97 ( 96.91)	Acc@5 100.00 ( 99.99)
Epoch: [78][290/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.8445e-02 (8.5704e-02)	Acc@1  96.09 ( 96.90)	Acc@5 100.00 ( 99.99)
Epoch: [78][300/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.0666e-02 (8.5800e-02)	Acc@1  96.88 ( 96.89)	Acc@5 100.00 ( 99.99)
Epoch: [78][310/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.7022e-02 (8.6046e-02)	Acc@1  98.44 ( 96.90)	Acc@5 100.00 ( 99.99)
Epoch: [78][320/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.3068e-02 (8.5975e-02)	Acc@1  97.66 ( 96.91)	Acc@5 100.00 ( 99.99)
Epoch: [78][330/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2315e-01 (8.6458e-02)	Acc@1  96.09 ( 96.89)	Acc@5 100.00 ( 99.99)
Epoch: [78][340/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 8.2296e-02 (8.6301e-02)	Acc@1  96.88 ( 96.90)	Acc@5 100.00 ( 99.99)
Epoch: [78][350/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.0484e-01 (8.6599e-02)	Acc@1  96.88 ( 96.89)	Acc@5 100.00 ( 99.99)
Epoch: [78][360/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 8.7043e-02 (8.6231e-02)	Acc@1  96.09 ( 96.90)	Acc@5 100.00 ( 99.99)
Epoch: [78][370/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.7960e-02 (8.6253e-02)	Acc@1  96.88 ( 96.90)	Acc@5 100.00 ( 99.99)
Epoch: [78][380/391]	Time  0.066 ( 0.064)	Data  0.002 ( 0.002)	Loss 8.8774e-02 (8.5898e-02)	Acc@1  96.09 ( 96.92)	Acc@5 100.00 ( 99.99)
Epoch: [78][390/391]	Time  0.047 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.1867e-01 (8.5332e-02)	Acc@1  95.00 ( 96.94)	Acc@5 100.00 ( 99.99)
## e[78] optimizer.zero_grad (sum) time: 0.40804028511047363
## e[78]       loss.backward (sum) time: 6.92748761177063
## e[78]      optimizer.step (sum) time: 3.350825786590576
## epoch[78] training(only) time: 25.298378944396973
# Switched to evaluate mode...
Test: [  0/100]	Time  0.197 ( 0.197)	Loss 2.9946e-01 (2.9946e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.028 ( 0.041)	Loss 3.7338e-01 (2.7750e-01)	Acc@1  91.00 ( 91.45)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.024 ( 0.034)	Loss 2.8589e-01 (2.8993e-01)	Acc@1  90.00 ( 91.19)	Acc@5  99.00 ( 99.71)
Test: [ 30/100]	Time  0.026 ( 0.031)	Loss 3.4797e-01 (3.0498e-01)	Acc@1  91.00 ( 91.26)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.024 ( 0.030)	Loss 3.0910e-01 (3.1548e-01)	Acc@1  92.00 ( 90.98)	Acc@5  99.00 ( 99.73)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 2.6915e-01 (3.1863e-01)	Acc@1  92.00 ( 90.96)	Acc@5  99.00 ( 99.73)
Test: [ 60/100]	Time  0.026 ( 0.029)	Loss 3.1685e-01 (3.1532e-01)	Acc@1  93.00 ( 90.92)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 4.8068e-01 (3.1179e-01)	Acc@1  88.00 ( 90.94)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 2.0839e-01 (3.1135e-01)	Acc@1  93.00 ( 90.91)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.024 ( 0.028)	Loss 2.2200e-01 (3.0717e-01)	Acc@1  93.00 ( 90.96)	Acc@5 100.00 ( 99.79)
 * Acc@1 91.000 Acc@5 99.780
### epoch[78] execution time: 28.16344904899597
EPOCH 79
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [79][  0/391]	Time  0.244 ( 0.244)	Data  0.177 ( 0.177)	Loss 1.1412e-01 (1.1412e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [79][ 10/391]	Time  0.061 ( 0.081)	Data  0.001 ( 0.017)	Loss 1.2311e-01 (7.5613e-02)	Acc@1  95.31 ( 97.66)	Acc@5 100.00 ( 99.93)
Epoch: [79][ 20/391]	Time  0.064 ( 0.073)	Data  0.001 ( 0.009)	Loss 5.8573e-02 (7.8516e-02)	Acc@1  98.44 ( 97.40)	Acc@5 100.00 ( 99.96)
Epoch: [79][ 30/391]	Time  0.062 ( 0.069)	Data  0.001 ( 0.007)	Loss 7.7849e-02 (8.0498e-02)	Acc@1  96.88 ( 97.48)	Acc@5 100.00 ( 99.97)
Epoch: [79][ 40/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.005)	Loss 7.2872e-02 (8.1005e-02)	Acc@1  98.44 ( 97.39)	Acc@5 100.00 ( 99.96)
Epoch: [79][ 50/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.005)	Loss 5.9761e-02 (8.4646e-02)	Acc@1  97.66 ( 97.24)	Acc@5 100.00 ( 99.97)
Epoch: [79][ 60/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.004)	Loss 7.9433e-02 (8.4333e-02)	Acc@1  97.66 ( 97.27)	Acc@5 100.00 ( 99.96)
Epoch: [79][ 70/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.004)	Loss 6.4001e-02 (8.5451e-02)	Acc@1  98.44 ( 97.17)	Acc@5 100.00 ( 99.96)
Epoch: [79][ 80/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.9039e-02 (8.7202e-02)	Acc@1  98.44 ( 97.13)	Acc@5 100.00 ( 99.94)
Epoch: [79][ 90/391]	Time  0.066 ( 0.065)	Data  0.002 ( 0.003)	Loss 3.9678e-02 (8.7138e-02)	Acc@1 100.00 ( 97.16)	Acc@5 100.00 ( 99.95)
Epoch: [79][100/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.0337e-01 (8.7039e-02)	Acc@1  96.09 ( 97.14)	Acc@5 100.00 ( 99.95)
Epoch: [79][110/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.003)	Loss 6.0800e-02 (8.5444e-02)	Acc@1  97.66 ( 97.19)	Acc@5 100.00 ( 99.96)
Epoch: [79][120/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 7.4267e-02 (8.5409e-02)	Acc@1  96.09 ( 97.17)	Acc@5 100.00 ( 99.96)
Epoch: [79][130/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.3804e-02 (8.4932e-02)	Acc@1  98.44 ( 97.19)	Acc@5 100.00 ( 99.96)
Epoch: [79][140/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.7637e-02 (8.5248e-02)	Acc@1  97.66 ( 97.17)	Acc@5 100.00 ( 99.96)
Epoch: [79][150/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.0697e-02 (8.6126e-02)	Acc@1  99.22 ( 97.17)	Acc@5 100.00 ( 99.96)
Epoch: [79][160/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.3284e-02 (8.6066e-02)	Acc@1  97.66 ( 97.17)	Acc@5 100.00 ( 99.96)
Epoch: [79][170/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.8755e-02 (8.5889e-02)	Acc@1  97.66 ( 97.15)	Acc@5 100.00 ( 99.96)
Epoch: [79][180/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1363e-01 (8.6236e-02)	Acc@1  95.31 ( 97.14)	Acc@5 100.00 ( 99.97)
Epoch: [79][190/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0317e-01 (8.5763e-02)	Acc@1  97.66 ( 97.17)	Acc@5 100.00 ( 99.97)
Epoch: [79][200/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1779e-01 (8.5302e-02)	Acc@1  96.09 ( 97.18)	Acc@5 100.00 ( 99.97)
Epoch: [79][210/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.4940e-02 (8.5027e-02)	Acc@1  96.88 ( 97.17)	Acc@5 100.00 ( 99.97)
Epoch: [79][220/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.8258e-02 (8.4538e-02)	Acc@1  96.88 ( 97.16)	Acc@5 100.00 ( 99.97)
Epoch: [79][230/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.6746e-02 (8.4519e-02)	Acc@1  96.09 ( 97.15)	Acc@5 100.00 ( 99.97)
Epoch: [79][240/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5827e-01 (8.5508e-02)	Acc@1  95.31 ( 97.11)	Acc@5 100.00 ( 99.97)
Epoch: [79][250/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.3918e-02 (8.6008e-02)	Acc@1  96.88 ( 97.09)	Acc@5 100.00 ( 99.97)
Epoch: [79][260/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.1079e-02 (8.5731e-02)	Acc@1  99.22 ( 97.08)	Acc@5 100.00 ( 99.97)
Epoch: [79][270/391]	Time  0.064 ( 0.065)	Data  0.002 ( 0.002)	Loss 1.2639e-01 (8.5920e-02)	Acc@1  96.88 ( 97.08)	Acc@5 100.00 ( 99.97)
Epoch: [79][280/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.6140e-02 (8.6102e-02)	Acc@1  98.44 ( 97.09)	Acc@5 100.00 ( 99.97)
Epoch: [79][290/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.1626e-02 (8.5684e-02)	Acc@1  99.22 ( 97.10)	Acc@5 100.00 ( 99.97)
Epoch: [79][300/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.9745e-02 (8.5011e-02)	Acc@1  99.22 ( 97.12)	Acc@5 100.00 ( 99.97)
Epoch: [79][310/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.0698e-01 (8.4377e-02)	Acc@1  96.88 ( 97.16)	Acc@5 100.00 ( 99.97)
Epoch: [79][320/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.5749e-02 (8.4401e-02)	Acc@1  98.44 ( 97.15)	Acc@5 100.00 ( 99.98)
Epoch: [79][330/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 9.8296e-02 (8.4370e-02)	Acc@1  96.09 ( 97.15)	Acc@5 100.00 ( 99.97)
Epoch: [79][340/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.3158e-02 (8.3617e-02)	Acc@1  97.66 ( 97.17)	Acc@5 100.00 ( 99.97)
Epoch: [79][350/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.2610e-01 (8.3990e-02)	Acc@1  96.09 ( 97.16)	Acc@5 100.00 ( 99.98)
Epoch: [79][360/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.7425e-01 (8.3971e-02)	Acc@1  92.97 ( 97.15)	Acc@5 100.00 ( 99.98)
Epoch: [79][370/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.4719e-01 (8.4710e-02)	Acc@1  95.31 ( 97.12)	Acc@5 100.00 ( 99.97)
Epoch: [79][380/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 9.3246e-02 (8.4879e-02)	Acc@1  96.88 ( 97.11)	Acc@5 100.00 ( 99.97)
Epoch: [79][390/391]	Time  0.047 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.2563e-01 (8.4856e-02)	Acc@1  96.25 ( 97.11)	Acc@5 100.00 ( 99.97)
## e[79] optimizer.zero_grad (sum) time: 0.4040682315826416
## e[79]       loss.backward (sum) time: 6.9594810009002686
## e[79]      optimizer.step (sum) time: 3.3184704780578613
## epoch[79] training(only) time: 25.224327564239502
# Switched to evaluate mode...
Test: [  0/100]	Time  0.196 ( 0.196)	Loss 2.9913e-01 (2.9913e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.028 ( 0.043)	Loss 3.9103e-01 (2.7999e-01)	Acc@1  91.00 ( 91.55)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.024 ( 0.034)	Loss 2.9795e-01 (2.9318e-01)	Acc@1  89.00 ( 91.29)	Acc@5  99.00 ( 99.67)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 3.5954e-01 (3.0981e-01)	Acc@1  90.00 ( 91.26)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.024 ( 0.030)	Loss 2.8905e-01 (3.2151e-01)	Acc@1  91.00 ( 91.00)	Acc@5  99.00 ( 99.73)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 2.6159e-01 (3.2427e-01)	Acc@1  93.00 ( 90.92)	Acc@5  99.00 ( 99.71)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 2.8644e-01 (3.1854e-01)	Acc@1  93.00 ( 90.85)	Acc@5 100.00 ( 99.72)
Test: [ 70/100]	Time  0.030 ( 0.028)	Loss 5.1341e-01 (3.1460e-01)	Acc@1  88.00 ( 90.89)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.026 ( 0.028)	Loss 2.0468e-01 (3.1399e-01)	Acc@1  92.00 ( 90.74)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.027 ( 0.027)	Loss 2.3032e-01 (3.0977e-01)	Acc@1  93.00 ( 90.84)	Acc@5 100.00 ( 99.77)
 * Acc@1 90.900 Acc@5 99.760
### epoch[79] execution time: 28.087852001190186
EPOCH 80
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [80][  0/391]	Time  0.263 ( 0.263)	Data  0.197 ( 0.197)	Loss 5.1499e-02 (5.1499e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [80][ 10/391]	Time  0.066 ( 0.081)	Data  0.001 ( 0.019)	Loss 8.4408e-02 (7.8369e-02)	Acc@1  97.66 ( 97.37)	Acc@5 100.00 (100.00)
Epoch: [80][ 20/391]	Time  0.064 ( 0.074)	Data  0.001 ( 0.010)	Loss 6.0531e-02 (9.0497e-02)	Acc@1  97.66 ( 96.84)	Acc@5 100.00 (100.00)
Epoch: [80][ 30/391]	Time  0.064 ( 0.071)	Data  0.001 ( 0.007)	Loss 6.9146e-02 (9.0180e-02)	Acc@1  98.44 ( 96.85)	Acc@5 100.00 (100.00)
Epoch: [80][ 40/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.006)	Loss 1.5089e-01 (9.2220e-02)	Acc@1  96.09 ( 96.93)	Acc@5 100.00 (100.00)
Epoch: [80][ 50/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.005)	Loss 5.2547e-02 (9.0054e-02)	Acc@1  98.44 ( 96.97)	Acc@5 100.00 (100.00)
Epoch: [80][ 60/391]	Time  0.062 ( 0.068)	Data  0.001 ( 0.004)	Loss 5.4363e-02 (8.8237e-02)	Acc@1  97.66 ( 97.08)	Acc@5 100.00 (100.00)
Epoch: [80][ 70/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.004)	Loss 5.3411e-02 (8.6583e-02)	Acc@1  99.22 ( 97.02)	Acc@5 100.00 (100.00)
Epoch: [80][ 80/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.7505e-01 (8.6193e-02)	Acc@1  92.19 ( 97.01)	Acc@5 100.00 (100.00)
Epoch: [80][ 90/391]	Time  0.059 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.2443e-01 (8.5475e-02)	Acc@1  94.53 ( 97.02)	Acc@5 100.00 (100.00)
Epoch: [80][100/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 9.5782e-02 (8.4872e-02)	Acc@1  96.88 ( 97.05)	Acc@5 100.00 ( 99.99)
Epoch: [80][110/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.003)	Loss 8.5490e-02 (8.5025e-02)	Acc@1  96.88 ( 97.06)	Acc@5 100.00 ( 99.99)
Epoch: [80][120/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.003)	Loss 7.2990e-02 (8.4320e-02)	Acc@1  97.66 ( 97.06)	Acc@5 100.00 ( 99.99)
Epoch: [80][130/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.003)	Loss 5.6123e-02 (8.4228e-02)	Acc@1  96.88 ( 97.04)	Acc@5 100.00 ( 99.99)
Epoch: [80][140/391]	Time  0.070 ( 0.065)	Data  0.001 ( 0.003)	Loss 7.6896e-02 (8.4044e-02)	Acc@1  98.44 ( 97.05)	Acc@5 100.00 ( 99.99)
Epoch: [80][150/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.1145e-02 (8.4483e-02)	Acc@1  95.31 ( 97.04)	Acc@5 100.00 ( 99.99)
Epoch: [80][160/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.8109e-02 (8.4783e-02)	Acc@1  96.88 ( 97.02)	Acc@5 100.00 (100.00)
Epoch: [80][170/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.6786e-02 (8.3825e-02)	Acc@1  98.44 ( 97.06)	Acc@5 100.00 (100.00)
Epoch: [80][180/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.5559e-02 (8.3515e-02)	Acc@1  97.66 ( 97.09)	Acc@5 100.00 (100.00)
Epoch: [80][190/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7318e-01 (8.3712e-02)	Acc@1  93.75 ( 97.08)	Acc@5 100.00 (100.00)
Epoch: [80][200/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.3905e-02 (8.3573e-02)	Acc@1  96.09 ( 97.08)	Acc@5  99.22 ( 99.99)
Epoch: [80][210/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.4703e-02 (8.3697e-02)	Acc@1  97.66 ( 97.08)	Acc@5 100.00 ( 99.99)
Epoch: [80][220/391]	Time  0.062 ( 0.065)	Data  0.002 ( 0.002)	Loss 6.5450e-02 (8.3369e-02)	Acc@1  98.44 ( 97.10)	Acc@5 100.00 ( 99.99)
Epoch: [80][230/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.4088e-02 (8.3752e-02)	Acc@1  96.88 ( 97.09)	Acc@5 100.00 ( 99.99)
Epoch: [80][240/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.5226e-02 (8.3734e-02)	Acc@1  97.66 ( 97.09)	Acc@5 100.00 ( 99.99)
Epoch: [80][250/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0273e-01 (8.3367e-02)	Acc@1  97.66 ( 97.12)	Acc@5 100.00 ( 99.99)
Epoch: [80][260/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3220e-01 (8.2735e-02)	Acc@1  96.88 ( 97.15)	Acc@5 100.00 ( 99.99)
Epoch: [80][270/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.8818e-02 (8.2142e-02)	Acc@1  98.44 ( 97.18)	Acc@5 100.00 ( 99.99)
Epoch: [80][280/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.8144e-02 (8.2303e-02)	Acc@1  96.88 ( 97.17)	Acc@5 100.00 ( 99.99)
Epoch: [80][290/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.6160e-02 (8.1981e-02)	Acc@1  99.22 ( 97.19)	Acc@5 100.00 ( 99.99)
Epoch: [80][300/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.6934e-02 (8.1894e-02)	Acc@1  97.66 ( 97.20)	Acc@5 100.00 ( 99.99)
Epoch: [80][310/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3063e-01 (8.2510e-02)	Acc@1  96.88 ( 97.19)	Acc@5 100.00 ( 99.99)
Epoch: [80][320/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.9692e-02 (8.2858e-02)	Acc@1  95.31 ( 97.16)	Acc@5 100.00 ( 99.99)
Epoch: [80][330/391]	Time  0.072 ( 0.065)	Data  0.002 ( 0.002)	Loss 1.3241e-01 (8.3280e-02)	Acc@1  96.88 ( 97.15)	Acc@5 100.00 ( 99.99)
Epoch: [80][340/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.1421e-02 (8.3512e-02)	Acc@1  98.44 ( 97.14)	Acc@5 100.00 ( 99.99)
Epoch: [80][350/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1676e-01 (8.3241e-02)	Acc@1  92.97 ( 97.15)	Acc@5 100.00 ( 99.99)
Epoch: [80][360/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0360e-01 (8.2948e-02)	Acc@1  94.53 ( 97.16)	Acc@5 100.00 ( 99.99)
Epoch: [80][370/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.6302e-02 (8.3331e-02)	Acc@1  96.88 ( 97.15)	Acc@5 100.00 ( 99.99)
Epoch: [80][380/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.2727e-02 (8.3268e-02)	Acc@1  95.31 ( 97.14)	Acc@5 100.00 ( 99.99)
Epoch: [80][390/391]	Time  0.046 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5589e-01 (8.3408e-02)	Acc@1  95.00 ( 97.13)	Acc@5 100.00 ( 99.99)
## e[80] optimizer.zero_grad (sum) time: 0.39916205406188965
## e[80]       loss.backward (sum) time: 6.993139982223511
## e[80]      optimizer.step (sum) time: 3.3695876598358154
## epoch[80] training(only) time: 25.363866090774536
# Switched to evaluate mode...
Test: [  0/100]	Time  0.199 ( 0.199)	Loss 3.2631e-01 (3.2631e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.042)	Loss 4.0263e-01 (2.8178e-01)	Acc@1  90.00 ( 91.73)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.026 ( 0.035)	Loss 3.0353e-01 (2.9548e-01)	Acc@1  88.00 ( 91.24)	Acc@5  99.00 ( 99.71)
Test: [ 30/100]	Time  0.024 ( 0.032)	Loss 3.4412e-01 (3.1234e-01)	Acc@1  91.00 ( 91.23)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.024 ( 0.030)	Loss 3.0019e-01 (3.2230e-01)	Acc@1  91.00 ( 91.00)	Acc@5  99.00 ( 99.73)
Test: [ 50/100]	Time  0.028 ( 0.030)	Loss 2.8052e-01 (3.2669e-01)	Acc@1  91.00 ( 90.88)	Acc@5  99.00 ( 99.71)
Test: [ 60/100]	Time  0.024 ( 0.029)	Loss 3.2806e-01 (3.2326e-01)	Acc@1  93.00 ( 90.77)	Acc@5 100.00 ( 99.72)
Test: [ 70/100]	Time  0.027 ( 0.029)	Loss 5.1245e-01 (3.1909e-01)	Acc@1  87.00 ( 90.86)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.027 ( 0.028)	Loss 1.8269e-01 (3.1761e-01)	Acc@1  94.00 ( 90.81)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.024 ( 0.028)	Loss 2.2182e-01 (3.1423e-01)	Acc@1  92.00 ( 90.81)	Acc@5 100.00 ( 99.76)
 * Acc@1 90.890 Acc@5 99.750
### epoch[80] execution time: 28.24666452407837
EPOCH 81
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [81][  0/391]	Time  0.256 ( 0.256)	Data  0.190 ( 0.190)	Loss 1.0171e-01 (1.0171e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [81][ 10/391]	Time  0.061 ( 0.081)	Data  0.001 ( 0.018)	Loss 2.9404e-02 (9.3915e-02)	Acc@1  99.22 ( 96.73)	Acc@5 100.00 (100.00)
Epoch: [81][ 20/391]	Time  0.064 ( 0.073)	Data  0.001 ( 0.010)	Loss 8.2396e-02 (9.4616e-02)	Acc@1  96.88 ( 96.54)	Acc@5 100.00 (100.00)
Epoch: [81][ 30/391]	Time  0.063 ( 0.070)	Data  0.001 ( 0.007)	Loss 1.0103e-01 (9.0486e-02)	Acc@1  96.88 ( 96.72)	Acc@5 100.00 (100.00)
Epoch: [81][ 40/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.006)	Loss 9.3446e-02 (8.9353e-02)	Acc@1  95.31 ( 96.80)	Acc@5 100.00 (100.00)
Epoch: [81][ 50/391]	Time  0.062 ( 0.068)	Data  0.001 ( 0.005)	Loss 3.3807e-02 (8.8321e-02)	Acc@1  99.22 ( 96.77)	Acc@5 100.00 ( 99.97)
Epoch: [81][ 60/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.004)	Loss 4.9026e-02 (8.8431e-02)	Acc@1  98.44 ( 96.70)	Acc@5 100.00 ( 99.97)
Epoch: [81][ 70/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.004)	Loss 7.2604e-02 (8.7549e-02)	Acc@1  97.66 ( 96.76)	Acc@5 100.00 ( 99.98)
Epoch: [81][ 80/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.003)	Loss 6.6433e-02 (8.6867e-02)	Acc@1  96.88 ( 96.85)	Acc@5 100.00 ( 99.98)
Epoch: [81][ 90/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.003)	Loss 6.3772e-02 (8.8005e-02)	Acc@1  97.66 ( 96.85)	Acc@5 100.00 ( 99.97)
Epoch: [81][100/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.0635e-01 (8.7376e-02)	Acc@1  95.31 ( 96.91)	Acc@5 100.00 ( 99.98)
Epoch: [81][110/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.003)	Loss 9.1522e-02 (8.6198e-02)	Acc@1  96.88 ( 96.97)	Acc@5 100.00 ( 99.98)
Epoch: [81][120/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.003)	Loss 7.8295e-02 (8.6234e-02)	Acc@1  97.66 ( 96.94)	Acc@5 100.00 ( 99.98)
Epoch: [81][130/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.3530e-01 (8.6071e-02)	Acc@1  96.09 ( 96.94)	Acc@5 100.00 ( 99.98)
Epoch: [81][140/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.7743e-02 (8.4611e-02)	Acc@1  97.66 ( 96.99)	Acc@5 100.00 ( 99.98)
Epoch: [81][150/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.4460e-02 (8.3085e-02)	Acc@1  96.88 ( 97.05)	Acc@5 100.00 ( 99.98)
Epoch: [81][160/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.8920e-02 (8.2876e-02)	Acc@1  95.31 ( 97.06)	Acc@5 100.00 ( 99.99)
Epoch: [81][170/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.2576e-02 (8.2801e-02)	Acc@1  97.66 ( 97.06)	Acc@5 100.00 ( 99.99)
Epoch: [81][180/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5701e-01 (8.3389e-02)	Acc@1  93.75 ( 97.03)	Acc@5 100.00 ( 99.99)
Epoch: [81][190/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.3070e-02 (8.4118e-02)	Acc@1  97.66 ( 97.02)	Acc@5 100.00 ( 99.99)
Epoch: [81][200/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.6202e-02 (8.3601e-02)	Acc@1  97.66 ( 97.04)	Acc@5 100.00 ( 99.99)
Epoch: [81][210/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.5192e-02 (8.3556e-02)	Acc@1  97.66 ( 97.05)	Acc@5 100.00 ( 99.99)
Epoch: [81][220/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.5560e-02 (8.3084e-02)	Acc@1  97.66 ( 97.08)	Acc@5 100.00 ( 99.99)
Epoch: [81][230/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.4599e-02 (8.2226e-02)	Acc@1  97.66 ( 97.12)	Acc@5 100.00 ( 99.99)
Epoch: [81][240/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.7626e-02 (8.1704e-02)	Acc@1  99.22 ( 97.14)	Acc@5 100.00 ( 99.99)
Epoch: [81][250/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3329e-01 (8.1618e-02)	Acc@1  96.88 ( 97.15)	Acc@5 100.00 ( 99.99)
Epoch: [81][260/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6736e-01 (8.1578e-02)	Acc@1  93.75 ( 97.13)	Acc@5 100.00 ( 99.99)
Epoch: [81][270/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.5716e-02 (8.1961e-02)	Acc@1  97.66 ( 97.13)	Acc@5 100.00 ( 99.99)
Epoch: [81][280/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.0919e-02 (8.1753e-02)	Acc@1  99.22 ( 97.14)	Acc@5 100.00 ( 99.99)
Epoch: [81][290/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3086e-01 (8.2164e-02)	Acc@1  96.88 ( 97.13)	Acc@5 100.00 ( 99.99)
Epoch: [81][300/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3545e-01 (8.2278e-02)	Acc@1  95.31 ( 97.13)	Acc@5 100.00 ( 99.99)
Epoch: [81][310/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.4964e-02 (8.1968e-02)	Acc@1  99.22 ( 97.14)	Acc@5 100.00 ( 99.98)
Epoch: [81][320/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.9677e-02 (8.1804e-02)	Acc@1  96.88 ( 97.15)	Acc@5 100.00 ( 99.99)
Epoch: [81][330/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.6010e-02 (8.1511e-02)	Acc@1  98.44 ( 97.16)	Acc@5 100.00 ( 99.99)
Epoch: [81][340/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6635e-01 (8.1767e-02)	Acc@1  92.97 ( 97.15)	Acc@5 100.00 ( 99.98)
Epoch: [81][350/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.4843e-02 (8.1652e-02)	Acc@1  97.66 ( 97.15)	Acc@5 100.00 ( 99.98)
Epoch: [81][360/391]	Time  0.067 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.7343e-02 (8.1307e-02)	Acc@1  98.44 ( 97.17)	Acc@5 100.00 ( 99.98)
Epoch: [81][370/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.9357e-02 (8.2178e-02)	Acc@1  97.66 ( 97.13)	Acc@5 100.00 ( 99.99)
Epoch: [81][380/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 8.3813e-02 (8.2202e-02)	Acc@1  98.44 ( 97.14)	Acc@5 100.00 ( 99.99)
Epoch: [81][390/391]	Time  0.045 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.3472e-01 (8.2138e-02)	Acc@1  96.25 ( 97.13)	Acc@5 100.00 ( 99.99)
## e[81] optimizer.zero_grad (sum) time: 0.39616823196411133
## e[81]       loss.backward (sum) time: 6.966531276702881
## e[81]      optimizer.step (sum) time: 3.31182599067688
## epoch[81] training(only) time: 25.227320671081543
# Switched to evaluate mode...
Test: [  0/100]	Time  0.199 ( 0.199)	Loss 3.1551e-01 (3.1551e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.042)	Loss 4.0833e-01 (2.8811e-01)	Acc@1  91.00 ( 91.36)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.027 ( 0.034)	Loss 2.9700e-01 (3.0096e-01)	Acc@1  87.00 ( 90.95)	Acc@5  99.00 ( 99.71)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 3.6752e-01 (3.1735e-01)	Acc@1  90.00 ( 90.77)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.024 ( 0.030)	Loss 2.9855e-01 (3.2784e-01)	Acc@1  91.00 ( 90.61)	Acc@5  99.00 ( 99.68)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 2.7971e-01 (3.3028e-01)	Acc@1  92.00 ( 90.59)	Acc@5  99.00 ( 99.67)
Test: [ 60/100]	Time  0.027 ( 0.028)	Loss 3.0876e-01 (3.2643e-01)	Acc@1  93.00 ( 90.54)	Acc@5 100.00 ( 99.70)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 5.1208e-01 (3.2083e-01)	Acc@1  88.00 ( 90.63)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.024 ( 0.028)	Loss 1.9602e-01 (3.1967e-01)	Acc@1  92.00 ( 90.57)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.024 ( 0.028)	Loss 2.3787e-01 (3.1608e-01)	Acc@1  93.00 ( 90.64)	Acc@5 100.00 ( 99.75)
 * Acc@1 90.690 Acc@5 99.750
### epoch[81] execution time: 28.072787046432495
EPOCH 82
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [82][  0/391]	Time  0.253 ( 0.253)	Data  0.186 ( 0.186)	Loss 9.5489e-02 (9.5489e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [82][ 10/391]	Time  0.065 ( 0.082)	Data  0.001 ( 0.018)	Loss 4.6559e-02 (7.3431e-02)	Acc@1  97.66 ( 97.44)	Acc@5 100.00 (100.00)
Epoch: [82][ 20/391]	Time  0.065 ( 0.074)	Data  0.001 ( 0.010)	Loss 1.0819e-01 (7.8629e-02)	Acc@1  96.09 ( 97.25)	Acc@5 100.00 (100.00)
Epoch: [82][ 30/391]	Time  0.064 ( 0.071)	Data  0.001 ( 0.007)	Loss 3.9495e-02 (8.0569e-02)	Acc@1  97.66 ( 97.05)	Acc@5 100.00 (100.00)
Epoch: [82][ 40/391]	Time  0.063 ( 0.069)	Data  0.001 ( 0.006)	Loss 1.0118e-01 (8.4645e-02)	Acc@1  96.09 ( 96.95)	Acc@5 100.00 ( 99.98)
Epoch: [82][ 50/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.005)	Loss 4.7058e-02 (8.1345e-02)	Acc@1  98.44 ( 97.12)	Acc@5 100.00 ( 99.98)
Epoch: [82][ 60/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.004)	Loss 6.7683e-02 (8.0181e-02)	Acc@1  96.09 ( 97.11)	Acc@5 100.00 ( 99.99)
Epoch: [82][ 70/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.004)	Loss 6.3172e-02 (8.1271e-02)	Acc@1  97.66 ( 97.04)	Acc@5 100.00 ( 99.99)
Epoch: [82][ 80/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.1480e-01 (8.1486e-02)	Acc@1  96.09 ( 97.13)	Acc@5 100.00 ( 99.99)
Epoch: [82][ 90/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 5.7957e-02 (7.9603e-02)	Acc@1  98.44 ( 97.22)	Acc@5 100.00 ( 99.99)
Epoch: [82][100/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 5.7108e-02 (7.9248e-02)	Acc@1  98.44 ( 97.25)	Acc@5 100.00 ( 99.99)
Epoch: [82][110/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.2689e-01 (7.7981e-02)	Acc@1  92.97 ( 97.29)	Acc@5 100.00 ( 99.99)
Epoch: [82][120/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.0822e-01 (7.7561e-02)	Acc@1  96.09 ( 97.29)	Acc@5 100.00 ( 99.99)
Epoch: [82][130/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.0696e-01 (7.7534e-02)	Acc@1  97.66 ( 97.30)	Acc@5 100.00 ( 99.99)
Epoch: [82][140/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1350e-01 (7.8687e-02)	Acc@1  96.88 ( 97.27)	Acc@5 100.00 ( 99.99)
Epoch: [82][150/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.4267e-02 (7.9144e-02)	Acc@1  99.22 ( 97.25)	Acc@5 100.00 ( 99.98)
Epoch: [82][160/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.9245e-02 (7.9294e-02)	Acc@1  98.44 ( 97.23)	Acc@5 100.00 ( 99.98)
Epoch: [82][170/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.2503e-02 (7.9767e-02)	Acc@1  96.88 ( 97.22)	Acc@5 100.00 ( 99.98)
Epoch: [82][180/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.0782e-02 (7.9817e-02)	Acc@1  97.66 ( 97.22)	Acc@5 100.00 ( 99.98)
Epoch: [82][190/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.6337e-02 (7.9881e-02)	Acc@1  98.44 ( 97.22)	Acc@5 100.00 ( 99.98)
Epoch: [82][200/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.1652e-02 (7.9857e-02)	Acc@1  97.66 ( 97.20)	Acc@5 100.00 ( 99.98)
Epoch: [82][210/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.9331e-02 (7.9863e-02)	Acc@1  97.66 ( 97.22)	Acc@5 100.00 ( 99.99)
Epoch: [82][220/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.5664e-02 (8.0943e-02)	Acc@1  99.22 ( 97.18)	Acc@5 100.00 ( 99.99)
Epoch: [82][230/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.4990e-02 (8.0996e-02)	Acc@1  99.22 ( 97.19)	Acc@5 100.00 ( 99.99)
Epoch: [82][240/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.2957e-02 (8.0732e-02)	Acc@1  97.66 ( 97.20)	Acc@5 100.00 ( 99.99)
Epoch: [82][250/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5277e-01 (8.0803e-02)	Acc@1  94.53 ( 97.21)	Acc@5 100.00 ( 99.99)
Epoch: [82][260/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.0358e-02 (8.1243e-02)	Acc@1  97.66 ( 97.19)	Acc@5 100.00 ( 99.99)
Epoch: [82][270/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.8752e-02 (8.1057e-02)	Acc@1  98.44 ( 97.20)	Acc@5 100.00 ( 99.99)
Epoch: [82][280/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2788e-01 (8.1580e-02)	Acc@1  96.88 ( 97.19)	Acc@5 100.00 ( 99.99)
Epoch: [82][290/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.8746e-02 (8.1997e-02)	Acc@1  98.44 ( 97.18)	Acc@5 100.00 ( 99.99)
Epoch: [82][300/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.1625e-02 (8.1704e-02)	Acc@1  97.66 ( 97.20)	Acc@5 100.00 ( 99.99)
Epoch: [82][310/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.0502e-02 (8.2072e-02)	Acc@1  96.88 ( 97.18)	Acc@5 100.00 ( 99.99)
Epoch: [82][320/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.0915e-02 (8.2101e-02)	Acc@1  97.66 ( 97.17)	Acc@5 100.00 ( 99.99)
Epoch: [82][330/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.0160e-02 (8.2007e-02)	Acc@1  98.44 ( 97.17)	Acc@5 100.00 ( 99.99)
Epoch: [82][340/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.7401e-02 (8.1921e-02)	Acc@1  97.66 ( 97.17)	Acc@5 100.00 ( 99.99)
Epoch: [82][350/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.9618e-02 (8.1573e-02)	Acc@1  97.66 ( 97.19)	Acc@5 100.00 ( 99.99)
Epoch: [82][360/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.5627e-02 (8.1271e-02)	Acc@1  99.22 ( 97.20)	Acc@5 100.00 ( 99.99)
Epoch: [82][370/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4352e-01 (8.1135e-02)	Acc@1  95.31 ( 97.21)	Acc@5 100.00 ( 99.99)
Epoch: [82][380/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.7255e-02 (8.1063e-02)	Acc@1  96.09 ( 97.22)	Acc@5 100.00 ( 99.99)
Epoch: [82][390/391]	Time  0.048 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.0978e-02 (8.1373e-02)	Acc@1  98.75 ( 97.20)	Acc@5 100.00 ( 99.99)
## e[82] optimizer.zero_grad (sum) time: 0.405118465423584
## e[82]       loss.backward (sum) time: 6.95702862739563
## e[82]      optimizer.step (sum) time: 3.3942947387695312
## epoch[82] training(only) time: 25.42968988418579
# Switched to evaluate mode...
Test: [  0/100]	Time  0.193 ( 0.193)	Loss 3.2786e-01 (3.2786e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.040)	Loss 3.8107e-01 (2.9154e-01)	Acc@1  90.00 ( 91.09)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.027 ( 0.033)	Loss 3.0177e-01 (3.0331e-01)	Acc@1  89.00 ( 91.19)	Acc@5  99.00 ( 99.67)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 3.5183e-01 (3.1794e-01)	Acc@1  91.00 ( 91.19)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.024 ( 0.030)	Loss 2.7716e-01 (3.2924e-01)	Acc@1  92.00 ( 90.90)	Acc@5  99.00 ( 99.66)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 2.5117e-01 (3.2973e-01)	Acc@1  93.00 ( 90.80)	Acc@5  99.00 ( 99.67)
Test: [ 60/100]	Time  0.027 ( 0.028)	Loss 3.1294e-01 (3.2432e-01)	Acc@1  93.00 ( 90.85)	Acc@5 100.00 ( 99.70)
Test: [ 70/100]	Time  0.027 ( 0.028)	Loss 4.6845e-01 (3.2023e-01)	Acc@1  89.00 ( 90.85)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.8598e-01 (3.1857e-01)	Acc@1  95.00 ( 90.77)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.027 ( 0.027)	Loss 2.4584e-01 (3.1472e-01)	Acc@1  92.00 ( 90.81)	Acc@5 100.00 ( 99.75)
 * Acc@1 90.870 Acc@5 99.740
### epoch[82] execution time: 28.266199111938477
EPOCH 83
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [83][  0/391]	Time  0.253 ( 0.253)	Data  0.189 ( 0.189)	Loss 1.2239e-01 (1.2239e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [83][ 10/391]	Time  0.064 ( 0.079)	Data  0.001 ( 0.018)	Loss 9.2759e-02 (8.4274e-02)	Acc@1  97.66 ( 96.95)	Acc@5 100.00 (100.00)
Epoch: [83][ 20/391]	Time  0.064 ( 0.072)	Data  0.001 ( 0.010)	Loss 1.0693e-01 (8.4348e-02)	Acc@1  96.88 ( 97.14)	Acc@5 100.00 (100.00)
Epoch: [83][ 30/391]	Time  0.062 ( 0.069)	Data  0.001 ( 0.007)	Loss 4.7886e-02 (7.7761e-02)	Acc@1  99.22 ( 97.30)	Acc@5 100.00 (100.00)
Epoch: [83][ 40/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.006)	Loss 6.8911e-02 (7.3833e-02)	Acc@1  96.88 ( 97.41)	Acc@5 100.00 (100.00)
Epoch: [83][ 50/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.005)	Loss 4.7048e-02 (7.1966e-02)	Acc@1  97.66 ( 97.49)	Acc@5 100.00 (100.00)
Epoch: [83][ 60/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.3110e-01 (7.5031e-02)	Acc@1  97.66 ( 97.37)	Acc@5 100.00 (100.00)
Epoch: [83][ 70/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.004)	Loss 1.1627e-01 (7.8644e-02)	Acc@1  96.09 ( 97.25)	Acc@5 100.00 (100.00)
Epoch: [83][ 80/391]	Time  0.070 ( 0.066)	Data  0.001 ( 0.003)	Loss 8.2490e-02 (7.8320e-02)	Acc@1  96.88 ( 97.25)	Acc@5 100.00 (100.00)
Epoch: [83][ 90/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 7.0531e-02 (7.9227e-02)	Acc@1  98.44 ( 97.28)	Acc@5 100.00 (100.00)
Epoch: [83][100/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 6.7421e-02 (7.8692e-02)	Acc@1  99.22 ( 97.32)	Acc@5 100.00 (100.00)
Epoch: [83][110/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.003)	Loss 6.4185e-02 (7.8869e-02)	Acc@1  96.88 ( 97.31)	Acc@5 100.00 (100.00)
Epoch: [83][120/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.003)	Loss 6.1524e-02 (7.8747e-02)	Acc@1  97.66 ( 97.31)	Acc@5 100.00 (100.00)
Epoch: [83][130/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.1713e-01 (7.8998e-02)	Acc@1  94.53 ( 97.27)	Acc@5 100.00 (100.00)
Epoch: [83][140/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0945e-01 (7.8536e-02)	Acc@1  96.09 ( 97.31)	Acc@5 100.00 (100.00)
Epoch: [83][150/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0628e-01 (7.9185e-02)	Acc@1  97.66 ( 97.29)	Acc@5 100.00 (100.00)
Epoch: [83][160/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.0560e-02 (8.0026e-02)	Acc@1  96.09 ( 97.23)	Acc@5 100.00 (100.00)
Epoch: [83][170/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 9.4474e-02 (8.0328e-02)	Acc@1  98.44 ( 97.22)	Acc@5 100.00 (100.00)
Epoch: [83][180/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.2841e-02 (8.0014e-02)	Acc@1  96.88 ( 97.24)	Acc@5 100.00 ( 99.99)
Epoch: [83][190/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.7799e-02 (7.9052e-02)	Acc@1  97.66 ( 97.29)	Acc@5 100.00 ( 99.99)
Epoch: [83][200/391]	Time  0.059 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.5092e-01 (8.0001e-02)	Acc@1  93.75 ( 97.27)	Acc@5 100.00 ( 99.99)
Epoch: [83][210/391]	Time  0.066 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.1185e-01 (8.0087e-02)	Acc@1  96.09 ( 97.27)	Acc@5 100.00 ( 99.99)
Epoch: [83][220/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.4792e-02 (8.0315e-02)	Acc@1  98.44 ( 97.26)	Acc@5 100.00 ( 99.99)
Epoch: [83][230/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.4575e-02 (8.0460e-02)	Acc@1  98.44 ( 97.24)	Acc@5 100.00 ( 99.99)
Epoch: [83][240/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.9421e-02 (8.0260e-02)	Acc@1  98.44 ( 97.24)	Acc@5 100.00 ( 99.99)
Epoch: [83][250/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.5375e-02 (8.0228e-02)	Acc@1  96.88 ( 97.24)	Acc@5 100.00 ( 99.99)
Epoch: [83][260/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.8737e-02 (8.0182e-02)	Acc@1  96.09 ( 97.24)	Acc@5 100.00 ( 99.99)
Epoch: [83][270/391]	Time  0.067 ( 0.064)	Data  0.001 ( 0.002)	Loss 8.2841e-02 (8.0493e-02)	Acc@1  95.31 ( 97.23)	Acc@5 100.00 ( 99.99)
Epoch: [83][280/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.7059e-02 (8.0889e-02)	Acc@1  97.66 ( 97.22)	Acc@5 100.00 ( 99.99)
Epoch: [83][290/391]	Time  0.067 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.5672e-01 (8.1814e-02)	Acc@1  95.31 ( 97.19)	Acc@5 100.00 ( 99.99)
Epoch: [83][300/391]	Time  0.057 ( 0.064)	Data  0.001 ( 0.002)	Loss 8.3098e-02 (8.2875e-02)	Acc@1  96.09 ( 97.15)	Acc@5 100.00 ( 99.99)
Epoch: [83][310/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.9738e-02 (8.2650e-02)	Acc@1  99.22 ( 97.17)	Acc@5 100.00 ( 99.99)
Epoch: [83][320/391]	Time  0.071 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.0609e-02 (8.2271e-02)	Acc@1  99.22 ( 97.18)	Acc@5 100.00 ( 99.99)
Epoch: [83][330/391]	Time  0.066 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.2649e-01 (8.2369e-02)	Acc@1  97.66 ( 97.18)	Acc@5 100.00 ( 99.99)
Epoch: [83][340/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.2677e-02 (8.2373e-02)	Acc@1  98.44 ( 97.17)	Acc@5 100.00 ( 99.98)
Epoch: [83][350/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.5202e-02 (8.2349e-02)	Acc@1  96.09 ( 97.14)	Acc@5 100.00 ( 99.98)
Epoch: [83][360/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 8.6499e-02 (8.2380e-02)	Acc@1  96.88 ( 97.16)	Acc@5 100.00 ( 99.98)
Epoch: [83][370/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.1471e-02 (8.2472e-02)	Acc@1  98.44 ( 97.16)	Acc@5 100.00 ( 99.98)
Epoch: [83][380/391]	Time  0.059 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.1784e-01 (8.2147e-02)	Acc@1  96.09 ( 97.17)	Acc@5 100.00 ( 99.98)
Epoch: [83][390/391]	Time  0.047 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.2486e-02 (8.1993e-02)	Acc@1 100.00 ( 97.17)	Acc@5 100.00 ( 99.98)
## e[83] optimizer.zero_grad (sum) time: 0.3991563320159912
## e[83]       loss.backward (sum) time: 6.947624206542969
## e[83]      optimizer.step (sum) time: 3.292194366455078
## epoch[83] training(only) time: 25.17837691307068
# Switched to evaluate mode...
Test: [  0/100]	Time  0.196 ( 0.196)	Loss 3.3552e-01 (3.3552e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.042)	Loss 3.9188e-01 (2.9074e-01)	Acc@1  91.00 ( 91.64)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.028 ( 0.034)	Loss 2.8801e-01 (3.0529e-01)	Acc@1  89.00 ( 91.24)	Acc@5  99.00 ( 99.67)
Test: [ 30/100]	Time  0.028 ( 0.031)	Loss 3.5714e-01 (3.2023e-01)	Acc@1  91.00 ( 91.19)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.024 ( 0.030)	Loss 3.1901e-01 (3.2992e-01)	Acc@1  90.00 ( 90.93)	Acc@5  99.00 ( 99.63)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 2.5734e-01 (3.3211e-01)	Acc@1  93.00 ( 90.88)	Acc@5  99.00 ( 99.65)
Test: [ 60/100]	Time  0.024 ( 0.029)	Loss 3.2049e-01 (3.2741e-01)	Acc@1  93.00 ( 90.84)	Acc@5 100.00 ( 99.69)
Test: [ 70/100]	Time  0.026 ( 0.028)	Loss 5.0320e-01 (3.2390e-01)	Acc@1  87.00 ( 90.82)	Acc@5 100.00 ( 99.72)
Test: [ 80/100]	Time  0.024 ( 0.028)	Loss 1.8278e-01 (3.2322e-01)	Acc@1  95.00 ( 90.69)	Acc@5 100.00 ( 99.73)
Test: [ 90/100]	Time  0.024 ( 0.028)	Loss 2.3385e-01 (3.1890e-01)	Acc@1  93.00 ( 90.79)	Acc@5 100.00 ( 99.74)
 * Acc@1 90.830 Acc@5 99.730
### epoch[83] execution time: 28.047358989715576
EPOCH 84
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [84][  0/391]	Time  0.252 ( 0.252)	Data  0.186 ( 0.186)	Loss 4.4858e-02 (4.4858e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [84][ 10/391]	Time  0.063 ( 0.081)	Data  0.001 ( 0.018)	Loss 9.7804e-02 (7.6484e-02)	Acc@1  93.75 ( 96.95)	Acc@5 100.00 (100.00)
Epoch: [84][ 20/391]	Time  0.065 ( 0.073)	Data  0.002 ( 0.010)	Loss 6.8858e-02 (7.6395e-02)	Acc@1  96.09 ( 97.02)	Acc@5 100.00 (100.00)
Epoch: [84][ 30/391]	Time  0.060 ( 0.069)	Data  0.001 ( 0.007)	Loss 4.0439e-02 (7.9588e-02)	Acc@1  99.22 ( 97.00)	Acc@5 100.00 (100.00)
Epoch: [84][ 40/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.006)	Loss 7.8166e-02 (8.2920e-02)	Acc@1  96.09 ( 96.91)	Acc@5 100.00 (100.00)
Epoch: [84][ 50/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.005)	Loss 1.2034e-01 (8.4016e-02)	Acc@1  94.53 ( 96.84)	Acc@5 100.00 (100.00)
Epoch: [84][ 60/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.004)	Loss 5.9687e-02 (8.2611e-02)	Acc@1  97.66 ( 96.82)	Acc@5 100.00 (100.00)
Epoch: [84][ 70/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.004)	Loss 9.7615e-02 (8.0790e-02)	Acc@1  96.88 ( 97.00)	Acc@5 100.00 (100.00)
Epoch: [84][ 80/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.003)	Loss 6.6361e-02 (8.0503e-02)	Acc@1  97.66 ( 97.04)	Acc@5 100.00 (100.00)
Epoch: [84][ 90/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.003)	Loss 6.6782e-02 (8.0569e-02)	Acc@1  97.66 ( 97.06)	Acc@5 100.00 (100.00)
Epoch: [84][100/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.003)	Loss 8.7470e-02 (8.1091e-02)	Acc@1  96.09 ( 97.08)	Acc@5 100.00 (100.00)
Epoch: [84][110/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 4.3332e-02 (8.1005e-02)	Acc@1  99.22 ( 97.07)	Acc@5 100.00 (100.00)
Epoch: [84][120/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.4618e-01 (8.0724e-02)	Acc@1  95.31 ( 97.09)	Acc@5 100.00 (100.00)
Epoch: [84][130/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.0217e-02 (7.9343e-02)	Acc@1  98.44 ( 97.17)	Acc@5 100.00 (100.00)
Epoch: [84][140/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.3337e-02 (7.9255e-02)	Acc@1  98.44 ( 97.21)	Acc@5 100.00 (100.00)
Epoch: [84][150/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.9130e-02 (7.8825e-02)	Acc@1  98.44 ( 97.23)	Acc@5 100.00 (100.00)
Epoch: [84][160/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.9426e-02 (7.9423e-02)	Acc@1  96.09 ( 97.23)	Acc@5 100.00 (100.00)
Epoch: [84][170/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.1260e-02 (7.8839e-02)	Acc@1  98.44 ( 97.25)	Acc@5 100.00 (100.00)
Epoch: [84][180/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.2811e-02 (7.8255e-02)	Acc@1  95.31 ( 97.25)	Acc@5 100.00 (100.00)
Epoch: [84][190/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.2450e-02 (7.8172e-02)	Acc@1 100.00 ( 97.26)	Acc@5 100.00 (100.00)
Epoch: [84][200/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2749e-01 (7.8174e-02)	Acc@1  94.53 ( 97.26)	Acc@5 100.00 (100.00)
Epoch: [84][210/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.4277e-01 (7.9338e-02)	Acc@1  91.41 ( 97.21)	Acc@5 100.00 ( 99.99)
Epoch: [84][220/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.3694e-02 (7.9440e-02)	Acc@1  99.22 ( 97.21)	Acc@5 100.00 ( 99.99)
Epoch: [84][230/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.3819e-02 (7.9553e-02)	Acc@1  99.22 ( 97.22)	Acc@5 100.00 ( 99.99)
Epoch: [84][240/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.8493e-02 (7.9980e-02)	Acc@1  97.66 ( 97.22)	Acc@5  99.22 ( 99.99)
Epoch: [84][250/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.9732e-01 (8.0490e-02)	Acc@1  92.97 ( 97.20)	Acc@5 100.00 ( 99.99)
Epoch: [84][260/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.4983e-02 (8.0182e-02)	Acc@1  98.44 ( 97.23)	Acc@5 100.00 ( 99.99)
Epoch: [84][270/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.5576e-02 (7.9882e-02)	Acc@1  99.22 ( 97.25)	Acc@5 100.00 ( 99.99)
Epoch: [84][280/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.5098e-02 (8.0040e-02)	Acc@1  97.66 ( 97.23)	Acc@5 100.00 ( 99.99)
Epoch: [84][290/391]	Time  0.067 ( 0.064)	Data  0.001 ( 0.002)	Loss 9.5542e-02 (8.0316e-02)	Acc@1  96.09 ( 97.22)	Acc@5 100.00 ( 99.99)
Epoch: [84][300/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.3713e-01 (8.0893e-02)	Acc@1  96.09 ( 97.21)	Acc@5 100.00 ( 99.99)
Epoch: [84][310/391]	Time  0.066 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.3975e-02 (8.1164e-02)	Acc@1  98.44 ( 97.20)	Acc@5 100.00 ( 99.99)
Epoch: [84][320/391]	Time  0.059 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.9951e-02 (8.1099e-02)	Acc@1  98.44 ( 97.20)	Acc@5 100.00 ( 99.99)
Epoch: [84][330/391]	Time  0.059 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.1447e-02 (8.1618e-02)	Acc@1  98.44 ( 97.18)	Acc@5 100.00 ( 99.99)
Epoch: [84][340/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.9492e-02 (8.1550e-02)	Acc@1  98.44 ( 97.17)	Acc@5 100.00 ( 99.99)
Epoch: [84][350/391]	Time  0.059 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.3008e-02 (8.1135e-02)	Acc@1  97.66 ( 97.20)	Acc@5 100.00 ( 99.99)
Epoch: [84][360/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 9.0291e-02 (8.1432e-02)	Acc@1  96.88 ( 97.18)	Acc@5  99.22 ( 99.99)
Epoch: [84][370/391]	Time  0.066 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.4384e-02 (8.1684e-02)	Acc@1  98.44 ( 97.18)	Acc@5 100.00 ( 99.99)
Epoch: [84][380/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.2226e-02 (8.1511e-02)	Acc@1  98.44 ( 97.19)	Acc@5 100.00 ( 99.99)
Epoch: [84][390/391]	Time  0.048 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.3901e-02 (8.1513e-02)	Acc@1  98.75 ( 97.19)	Acc@5 100.00 ( 99.99)
## e[84] optimizer.zero_grad (sum) time: 0.39507460594177246
## e[84]       loss.backward (sum) time: 6.922543048858643
## e[84]      optimizer.step (sum) time: 3.315747022628784
## epoch[84] training(only) time: 25.15019178390503
# Switched to evaluate mode...
Test: [  0/100]	Time  0.197 ( 0.197)	Loss 3.2581e-01 (3.2581e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.028 ( 0.042)	Loss 3.8384e-01 (2.8902e-01)	Acc@1  90.00 ( 90.91)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.024 ( 0.034)	Loss 2.9779e-01 (3.0004e-01)	Acc@1  88.00 ( 91.00)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.026 ( 0.032)	Loss 3.5848e-01 (3.1818e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.026 ( 0.030)	Loss 2.9019e-01 (3.2797e-01)	Acc@1  92.00 ( 90.88)	Acc@5  99.00 ( 99.71)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 2.6496e-01 (3.3035e-01)	Acc@1  92.00 ( 90.84)	Acc@5  99.00 ( 99.71)
Test: [ 60/100]	Time  0.024 ( 0.029)	Loss 3.1314e-01 (3.2459e-01)	Acc@1  93.00 ( 90.92)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 4.9134e-01 (3.2124e-01)	Acc@1  87.00 ( 90.87)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.026 ( 0.028)	Loss 2.0254e-01 (3.2017e-01)	Acc@1  93.00 ( 90.80)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.026 ( 0.028)	Loss 2.2811e-01 (3.1513e-01)	Acc@1  92.00 ( 90.89)	Acc@5 100.00 ( 99.79)
 * Acc@1 90.890 Acc@5 99.780
### epoch[84] execution time: 28.07582426071167
EPOCH 85
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [85][  0/391]	Time  0.247 ( 0.247)	Data  0.182 ( 0.182)	Loss 1.1892e-01 (1.1892e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [85][ 10/391]	Time  0.063 ( 0.081)	Data  0.001 ( 0.018)	Loss 1.0657e-01 (8.6968e-02)	Acc@1  96.88 ( 97.02)	Acc@5 100.00 ( 99.93)
Epoch: [85][ 20/391]	Time  0.061 ( 0.072)	Data  0.001 ( 0.010)	Loss 4.8091e-02 (8.0681e-02)	Acc@1  97.66 ( 97.17)	Acc@5 100.00 ( 99.96)
Epoch: [85][ 30/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.007)	Loss 6.6817e-02 (7.5478e-02)	Acc@1  96.88 ( 97.40)	Acc@5 100.00 ( 99.97)
Epoch: [85][ 40/391]	Time  0.062 ( 0.068)	Data  0.001 ( 0.005)	Loss 9.2081e-02 (7.5532e-02)	Acc@1  97.66 ( 97.48)	Acc@5 100.00 ( 99.98)
Epoch: [85][ 50/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.005)	Loss 1.1097e-01 (7.8091e-02)	Acc@1  93.75 ( 97.23)	Acc@5 100.00 ( 99.98)
Epoch: [85][ 60/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.004)	Loss 8.5752e-02 (7.8800e-02)	Acc@1  95.31 ( 97.20)	Acc@5 100.00 ( 99.99)
Epoch: [85][ 70/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.004)	Loss 1.0517e-01 (7.9420e-02)	Acc@1  96.09 ( 97.15)	Acc@5 100.00 ( 99.99)
Epoch: [85][ 80/391]	Time  0.059 ( 0.066)	Data  0.001 ( 0.003)	Loss 6.4668e-02 (7.7783e-02)	Acc@1  97.66 ( 97.19)	Acc@5 100.00 ( 99.99)
Epoch: [85][ 90/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 9.4834e-02 (8.0655e-02)	Acc@1  96.88 ( 97.10)	Acc@5 100.00 ( 99.99)
Epoch: [85][100/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 7.0425e-02 (7.9827e-02)	Acc@1  98.44 ( 97.17)	Acc@5 100.00 ( 99.99)
Epoch: [85][110/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.003)	Loss 7.7410e-02 (8.0279e-02)	Acc@1  96.88 ( 97.16)	Acc@5 100.00 ( 99.99)
Epoch: [85][120/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.003)	Loss 7.8972e-02 (7.9464e-02)	Acc@1  98.44 ( 97.21)	Acc@5 100.00 ( 99.99)
Epoch: [85][130/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1671e-01 (7.9980e-02)	Acc@1  95.31 ( 97.20)	Acc@5 100.00 ( 99.99)
Epoch: [85][140/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0158e-01 (8.0186e-02)	Acc@1  96.09 ( 97.20)	Acc@5 100.00 ( 99.99)
Epoch: [85][150/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.3348e-02 (8.0839e-02)	Acc@1  98.44 ( 97.19)	Acc@5 100.00 ( 99.99)
Epoch: [85][160/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.2036e-02 (8.0098e-02)	Acc@1  99.22 ( 97.20)	Acc@5 100.00 (100.00)
Epoch: [85][170/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2395e-02 (7.9727e-02)	Acc@1 100.00 ( 97.22)	Acc@5 100.00 (100.00)
Epoch: [85][180/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.3016e-02 (7.9942e-02)	Acc@1  98.44 ( 97.21)	Acc@5 100.00 (100.00)
Epoch: [85][190/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.8957e-02 (7.9649e-02)	Acc@1  99.22 ( 97.23)	Acc@5 100.00 (100.00)
Epoch: [85][200/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.5339e-02 (8.0159e-02)	Acc@1  97.66 ( 97.18)	Acc@5 100.00 (100.00)
Epoch: [85][210/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6200e-01 (8.0498e-02)	Acc@1  96.09 ( 97.19)	Acc@5 100.00 ( 99.99)
Epoch: [85][220/391]	Time  0.074 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.5479e-02 (8.0919e-02)	Acc@1  98.44 ( 97.18)	Acc@5 100.00 ( 99.99)
Epoch: [85][230/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0463e-01 (8.1000e-02)	Acc@1  96.88 ( 97.19)	Acc@5 100.00 ( 99.99)
Epoch: [85][240/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.6025e-02 (8.1181e-02)	Acc@1  99.22 ( 97.19)	Acc@5 100.00 ( 99.99)
Epoch: [85][250/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.1528e-02 (8.0844e-02)	Acc@1  97.66 ( 97.20)	Acc@5 100.00 ( 99.99)
Epoch: [85][260/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1680e-01 (8.1023e-02)	Acc@1  96.09 ( 97.20)	Acc@5 100.00 ( 99.99)
Epoch: [85][270/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.8330e-02 (8.0874e-02)	Acc@1  96.88 ( 97.20)	Acc@5 100.00 ( 99.99)
Epoch: [85][280/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.4573e-02 (8.1029e-02)	Acc@1  97.66 ( 97.21)	Acc@5 100.00 ( 99.99)
Epoch: [85][290/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0512e-01 (8.0737e-02)	Acc@1  96.88 ( 97.23)	Acc@5  99.22 ( 99.99)
Epoch: [85][300/391]	Time  0.060 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.3596e-02 (8.0621e-02)	Acc@1  96.88 ( 97.22)	Acc@5 100.00 ( 99.99)
Epoch: [85][310/391]	Time  0.059 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.8897e-02 (8.0905e-02)	Acc@1  99.22 ( 97.22)	Acc@5 100.00 ( 99.99)
Epoch: [85][320/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.2709e-02 (8.1142e-02)	Acc@1  97.66 ( 97.23)	Acc@5 100.00 ( 99.99)
Epoch: [85][330/391]	Time  0.068 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.3213e-01 (8.0734e-02)	Acc@1  93.75 ( 97.23)	Acc@5 100.00 ( 99.99)
Epoch: [85][340/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.7076e-02 (8.0620e-02)	Acc@1  99.22 ( 97.24)	Acc@5 100.00 ( 99.99)
Epoch: [85][350/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.1010e-02 (8.0467e-02)	Acc@1  99.22 ( 97.24)	Acc@5 100.00 ( 99.99)
Epoch: [85][360/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.3948e-02 (8.0171e-02)	Acc@1  98.44 ( 97.25)	Acc@5 100.00 ( 99.99)
Epoch: [85][370/391]	Time  0.074 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.2556e-02 (8.0459e-02)	Acc@1  98.44 ( 97.22)	Acc@5 100.00 ( 99.99)
Epoch: [85][380/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.3971e-01 (8.0938e-02)	Acc@1  96.09 ( 97.20)	Acc@5 100.00 ( 99.99)
Epoch: [85][390/391]	Time  0.046 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.2253e-02 (8.0761e-02)	Acc@1  98.75 ( 97.21)	Acc@5 100.00 ( 99.99)
## e[85] optimizer.zero_grad (sum) time: 0.3908863067626953
## e[85]       loss.backward (sum) time: 6.9696009159088135
## e[85]      optimizer.step (sum) time: 3.351945638656616
## epoch[85] training(only) time: 25.259897708892822
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 3.2616e-01 (3.2616e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.026 ( 0.041)	Loss 3.8788e-01 (2.9432e-01)	Acc@1  91.00 ( 91.55)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 2.6600e-01 (3.0391e-01)	Acc@1  89.00 ( 91.29)	Acc@5  99.00 ( 99.67)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 3.6421e-01 (3.2119e-01)	Acc@1  91.00 ( 91.26)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.024 ( 0.030)	Loss 3.1579e-01 (3.3181e-01)	Acc@1  90.00 ( 90.93)	Acc@5  99.00 ( 99.68)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 2.8067e-01 (3.3388e-01)	Acc@1  93.00 ( 90.92)	Acc@5  99.00 ( 99.67)
Test: [ 60/100]	Time  0.026 ( 0.028)	Loss 3.0683e-01 (3.2810e-01)	Acc@1  92.00 ( 90.82)	Acc@5 100.00 ( 99.70)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 4.8449e-01 (3.2331e-01)	Acc@1  88.00 ( 90.83)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.027 ( 0.028)	Loss 2.1930e-01 (3.2367e-01)	Acc@1  94.00 ( 90.75)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 2.5281e-01 (3.1938e-01)	Acc@1  93.00 ( 90.85)	Acc@5 100.00 ( 99.76)
 * Acc@1 90.890 Acc@5 99.740
### epoch[85] execution time: 28.115562200546265
EPOCH 86
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [86][  0/391]	Time  0.246 ( 0.246)	Data  0.183 ( 0.183)	Loss 5.1245e-02 (5.1245e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [86][ 10/391]	Time  0.064 ( 0.079)	Data  0.001 ( 0.018)	Loss 8.4290e-02 (6.3038e-02)	Acc@1  98.44 ( 98.08)	Acc@5 100.00 (100.00)
Epoch: [86][ 20/391]	Time  0.063 ( 0.071)	Data  0.001 ( 0.010)	Loss 8.5214e-02 (6.9272e-02)	Acc@1  98.44 ( 97.81)	Acc@5 100.00 (100.00)
Epoch: [86][ 30/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.007)	Loss 5.6493e-02 (6.8381e-02)	Acc@1  97.66 ( 97.68)	Acc@5 100.00 (100.00)
Epoch: [86][ 40/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.006)	Loss 1.1799e-01 (7.3868e-02)	Acc@1  94.53 ( 97.43)	Acc@5 100.00 (100.00)
Epoch: [86][ 50/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.005)	Loss 7.6680e-02 (7.3745e-02)	Acc@1  95.31 ( 97.43)	Acc@5 100.00 ( 99.98)
Epoch: [86][ 60/391]	Time  0.061 ( 0.067)	Data  0.001 ( 0.004)	Loss 4.2230e-02 (7.1781e-02)	Acc@1  99.22 ( 97.45)	Acc@5 100.00 ( 99.99)
Epoch: [86][ 70/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.004)	Loss 8.4922e-02 (7.3314e-02)	Acc@1  98.44 ( 97.45)	Acc@5 100.00 ( 99.99)
Epoch: [86][ 80/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 7.9826e-02 (7.4190e-02)	Acc@1  97.66 ( 97.38)	Acc@5 100.00 ( 99.99)
Epoch: [86][ 90/391]	Time  0.059 ( 0.066)	Data  0.001 ( 0.003)	Loss 8.2363e-02 (7.4871e-02)	Acc@1  96.09 ( 97.33)	Acc@5 100.00 ( 99.99)
Epoch: [86][100/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.003)	Loss 6.9326e-02 (7.4834e-02)	Acc@1  96.88 ( 97.33)	Acc@5 100.00 ( 99.99)
Epoch: [86][110/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.003)	Loss 3.3226e-02 (7.5498e-02)	Acc@1  99.22 ( 97.30)	Acc@5 100.00 ( 99.99)
Epoch: [86][120/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 1.4055e-01 (7.7810e-02)	Acc@1  93.75 ( 97.21)	Acc@5 100.00 ( 99.99)
Epoch: [86][130/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.003)	Loss 9.1610e-02 (7.8613e-02)	Acc@1  98.44 ( 97.17)	Acc@5 100.00 ( 99.99)
Epoch: [86][140/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.6994e-02 (7.7709e-02)	Acc@1  96.09 ( 97.19)	Acc@5 100.00 ( 99.99)
Epoch: [86][150/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3710e-01 (7.7816e-02)	Acc@1  94.53 ( 97.19)	Acc@5 100.00 ( 99.99)
Epoch: [86][160/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0614e-01 (7.7916e-02)	Acc@1  96.88 ( 97.23)	Acc@5 100.00 ( 99.99)
Epoch: [86][170/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0716e-01 (7.7586e-02)	Acc@1  96.88 ( 97.27)	Acc@5 100.00 ( 99.99)
Epoch: [86][180/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.4573e-02 (7.6914e-02)	Acc@1  98.44 ( 97.31)	Acc@5 100.00 ( 99.99)
Epoch: [86][190/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.9848e-02 (7.7501e-02)	Acc@1  99.22 ( 97.28)	Acc@5 100.00 ( 99.99)
Epoch: [86][200/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0934e-01 (7.7896e-02)	Acc@1  94.53 ( 97.25)	Acc@5 100.00 ( 99.99)
Epoch: [86][210/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1521e-01 (7.8184e-02)	Acc@1  94.53 ( 97.22)	Acc@5 100.00 ( 99.99)
Epoch: [86][220/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.6429e-02 (7.8471e-02)	Acc@1  96.88 ( 97.20)	Acc@5 100.00 ( 99.99)
Epoch: [86][230/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.6361e-02 (7.8384e-02)	Acc@1  96.88 ( 97.22)	Acc@5 100.00 ( 99.99)
Epoch: [86][240/391]	Time  0.070 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2469e-01 (7.8764e-02)	Acc@1  94.53 ( 97.22)	Acc@5 100.00 ( 99.99)
Epoch: [86][250/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.7041e-02 (7.8679e-02)	Acc@1  96.88 ( 97.22)	Acc@5 100.00 ( 99.99)
Epoch: [86][260/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1704e-01 (7.9187e-02)	Acc@1  93.75 ( 97.21)	Acc@5 100.00 ( 99.99)
Epoch: [86][270/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.2408e-02 (7.8832e-02)	Acc@1  98.44 ( 97.24)	Acc@5 100.00 ( 99.99)
Epoch: [86][280/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.9178e-02 (7.8818e-02)	Acc@1  96.88 ( 97.24)	Acc@5 100.00 ( 99.99)
Epoch: [86][290/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.6143e-02 (7.9622e-02)	Acc@1  97.66 ( 97.19)	Acc@5 100.00 ( 99.99)
Epoch: [86][300/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.4214e-02 (8.0111e-02)	Acc@1  97.66 ( 97.18)	Acc@5 100.00 ( 99.99)
Epoch: [86][310/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.3909e-02 (7.9924e-02)	Acc@1  98.44 ( 97.20)	Acc@5 100.00 ( 99.99)
Epoch: [86][320/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 9.6274e-02 (7.9876e-02)	Acc@1  96.09 ( 97.20)	Acc@5 100.00 ( 99.99)
Epoch: [86][330/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.3196e-01 (8.0230e-02)	Acc@1  96.88 ( 97.19)	Acc@5 100.00 ( 99.99)
Epoch: [86][340/391]	Time  0.066 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.4353e-02 (7.9953e-02)	Acc@1  99.22 ( 97.19)	Acc@5 100.00 ( 99.99)
Epoch: [86][350/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.9552e-02 (7.9956e-02)	Acc@1  99.22 ( 97.18)	Acc@5 100.00 ( 99.99)
Epoch: [86][360/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.8144e-02 (7.9909e-02)	Acc@1 100.00 ( 97.18)	Acc@5 100.00 ( 99.99)
Epoch: [86][370/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.8593e-02 (7.9645e-02)	Acc@1  99.22 ( 97.20)	Acc@5 100.00 ( 99.99)
Epoch: [86][380/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.4018e-02 (7.9300e-02)	Acc@1  96.88 ( 97.21)	Acc@5 100.00 ( 99.99)
Epoch: [86][390/391]	Time  0.046 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.3071e-01 (7.9585e-02)	Acc@1  91.25 ( 97.19)	Acc@5 100.00 ( 99.99)
## e[86] optimizer.zero_grad (sum) time: 0.39704394340515137
## e[86]       loss.backward (sum) time: 6.947786808013916
## e[86]      optimizer.step (sum) time: 3.3460605144500732
## epoch[86] training(only) time: 25.296921968460083
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 3.1869e-01 (3.1869e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.041)	Loss 4.0310e-01 (2.8401e-01)	Acc@1  90.00 ( 90.73)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.027 ( 0.034)	Loss 3.0456e-01 (3.0112e-01)	Acc@1  89.00 ( 90.81)	Acc@5  99.00 ( 99.71)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 3.6914e-01 (3.1999e-01)	Acc@1  91.00 ( 90.87)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.024 ( 0.030)	Loss 3.0283e-01 (3.3025e-01)	Acc@1  92.00 ( 90.68)	Acc@5  99.00 ( 99.68)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 2.7396e-01 (3.3350e-01)	Acc@1  93.00 ( 90.63)	Acc@5  99.00 ( 99.67)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 3.2574e-01 (3.2870e-01)	Acc@1  93.00 ( 90.74)	Acc@5 100.00 ( 99.70)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 5.1645e-01 (3.2646e-01)	Acc@1  86.00 ( 90.70)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 1.9395e-01 (3.2459e-01)	Acc@1  93.00 ( 90.63)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 2.1389e-01 (3.2008e-01)	Acc@1  92.00 ( 90.69)	Acc@5 100.00 ( 99.76)
 * Acc@1 90.720 Acc@5 99.740
### epoch[86] execution time: 28.111101388931274
EPOCH 87
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [87][  0/391]	Time  0.255 ( 0.255)	Data  0.190 ( 0.190)	Loss 9.6259e-02 (9.6259e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [87][ 10/391]	Time  0.066 ( 0.082)	Data  0.001 ( 0.018)	Loss 1.4843e-01 (8.0615e-02)	Acc@1  95.31 ( 97.87)	Acc@5 100.00 (100.00)
Epoch: [87][ 20/391]	Time  0.067 ( 0.073)	Data  0.001 ( 0.010)	Loss 7.3642e-02 (8.3360e-02)	Acc@1  98.44 ( 97.21)	Acc@5 100.00 (100.00)
Epoch: [87][ 30/391]	Time  0.067 ( 0.071)	Data  0.001 ( 0.007)	Loss 6.0557e-02 (8.6478e-02)	Acc@1  98.44 ( 97.00)	Acc@5 100.00 ( 99.97)
Epoch: [87][ 40/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.006)	Loss 6.6757e-02 (8.6789e-02)	Acc@1  97.66 ( 96.95)	Acc@5 100.00 ( 99.98)
Epoch: [87][ 50/391]	Time  0.061 ( 0.068)	Data  0.001 ( 0.005)	Loss 4.1726e-02 (8.4732e-02)	Acc@1  98.44 ( 97.09)	Acc@5 100.00 ( 99.98)
Epoch: [87][ 60/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.004)	Loss 8.8762e-02 (8.3112e-02)	Acc@1  96.88 ( 97.12)	Acc@5 100.00 ( 99.99)
Epoch: [87][ 70/391]	Time  0.059 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.1761e-01 (8.3448e-02)	Acc@1  94.53 ( 97.05)	Acc@5 100.00 ( 99.98)
Epoch: [87][ 80/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 9.7194e-02 (8.2675e-02)	Acc@1  96.09 ( 97.04)	Acc@5 100.00 ( 99.97)
Epoch: [87][ 90/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.9892e-02 (8.1311e-02)	Acc@1 100.00 ( 97.09)	Acc@5 100.00 ( 99.97)
Epoch: [87][100/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.1317e-01 (8.0518e-02)	Acc@1  96.88 ( 97.16)	Acc@5 100.00 ( 99.98)
Epoch: [87][110/391]	Time  0.071 ( 0.066)	Data  0.001 ( 0.003)	Loss 6.7350e-02 (7.8728e-02)	Acc@1  97.66 ( 97.24)	Acc@5 100.00 ( 99.98)
Epoch: [87][120/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.003)	Loss 2.1125e-02 (7.6987e-02)	Acc@1 100.00 ( 97.32)	Acc@5 100.00 ( 99.98)
Epoch: [87][130/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 6.1802e-02 (7.6697e-02)	Acc@1  98.44 ( 97.33)	Acc@5 100.00 ( 99.98)
Epoch: [87][140/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.2804e-02 (7.6658e-02)	Acc@1  97.66 ( 97.33)	Acc@5 100.00 ( 99.98)
Epoch: [87][150/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.7566e-02 (7.6815e-02)	Acc@1 100.00 ( 97.33)	Acc@5 100.00 ( 99.98)
Epoch: [87][160/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.4660e-02 (7.6409e-02)	Acc@1  98.44 ( 97.33)	Acc@5 100.00 ( 99.99)
Epoch: [87][170/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.8992e-02 (7.5836e-02)	Acc@1  97.66 ( 97.34)	Acc@5 100.00 ( 99.99)
Epoch: [87][180/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.4134e-02 (7.6466e-02)	Acc@1  96.09 ( 97.31)	Acc@5 100.00 ( 99.99)
Epoch: [87][190/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.5206e-02 (7.6889e-02)	Acc@1  96.09 ( 97.31)	Acc@5 100.00 ( 99.99)
Epoch: [87][200/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.9860e-02 (7.7016e-02)	Acc@1  97.66 ( 97.28)	Acc@5 100.00 ( 99.99)
Epoch: [87][210/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.7169e-02 (7.7610e-02)	Acc@1  96.88 ( 97.25)	Acc@5 100.00 ( 99.99)
Epoch: [87][220/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.0871e-02 (7.7879e-02)	Acc@1  97.66 ( 97.25)	Acc@5 100.00 ( 99.99)
Epoch: [87][230/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.5694e-02 (7.7580e-02)	Acc@1  96.09 ( 97.28)	Acc@5 100.00 ( 99.99)
Epoch: [87][240/391]	Time  0.060 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.1780e-02 (7.7664e-02)	Acc@1  98.44 ( 97.28)	Acc@5 100.00 ( 99.99)
Epoch: [87][250/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.6811e-02 (7.7307e-02)	Acc@1  96.88 ( 97.29)	Acc@5 100.00 ( 99.99)
Epoch: [87][260/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.0957e-01 (7.8456e-02)	Acc@1  96.09 ( 97.25)	Acc@5 100.00 ( 99.99)
Epoch: [87][270/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.8110e-02 (7.8206e-02)	Acc@1  98.44 ( 97.29)	Acc@5 100.00 ( 99.99)
Epoch: [87][280/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.2648e-02 (7.8191e-02)	Acc@1  99.22 ( 97.29)	Acc@5 100.00 ( 99.99)
Epoch: [87][290/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.6299e-02 (7.8297e-02)	Acc@1  97.66 ( 97.27)	Acc@5 100.00 ( 99.99)
Epoch: [87][300/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.2425e-01 (7.8884e-02)	Acc@1  95.31 ( 97.26)	Acc@5 100.00 ( 99.99)
Epoch: [87][310/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 8.4550e-02 (7.8834e-02)	Acc@1  97.66 ( 97.26)	Acc@5 100.00 ( 99.99)
Epoch: [87][320/391]	Time  0.068 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.9765e-02 (7.8409e-02)	Acc@1  96.88 ( 97.28)	Acc@5 100.00 ( 99.99)
Epoch: [87][330/391]	Time  0.068 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.6086e-02 (7.8446e-02)	Acc@1  98.44 ( 97.28)	Acc@5 100.00 ( 99.99)
Epoch: [87][340/391]	Time  0.066 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.0430e-01 (7.8755e-02)	Acc@1  96.88 ( 97.27)	Acc@5 100.00 ( 99.99)
Epoch: [87][350/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.3801e-02 (7.9224e-02)	Acc@1  96.88 ( 97.24)	Acc@5 100.00 ( 99.99)
Epoch: [87][360/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.2582e-01 (7.9250e-02)	Acc@1  95.31 ( 97.23)	Acc@5 100.00 ( 99.99)
Epoch: [87][370/391]	Time  0.060 ( 0.064)	Data  0.001 ( 0.002)	Loss 9.4488e-02 (7.9275e-02)	Acc@1  97.66 ( 97.23)	Acc@5 100.00 ( 99.99)
Epoch: [87][380/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.4488e-02 (7.9372e-02)	Acc@1  96.88 ( 97.22)	Acc@5 100.00 ( 99.99)
Epoch: [87][390/391]	Time  0.046 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.9450e-02 (7.9713e-02)	Acc@1 100.00 ( 97.21)	Acc@5 100.00 ( 99.99)
## e[87] optimizer.zero_grad (sum) time: 0.3975188732147217
## e[87]       loss.backward (sum) time: 6.93639612197876
## e[87]      optimizer.step (sum) time: 3.3359649181365967
## epoch[87] training(only) time: 25.212339401245117
# Switched to evaluate mode...
Test: [  0/100]	Time  0.194 ( 0.194)	Loss 3.3546e-01 (3.3546e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.028 ( 0.041)	Loss 4.1869e-01 (2.9236e-01)	Acc@1  91.00 ( 91.18)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.024 ( 0.034)	Loss 2.9531e-01 (3.0601e-01)	Acc@1  89.00 ( 91.10)	Acc@5  99.00 ( 99.67)
Test: [ 30/100]	Time  0.028 ( 0.031)	Loss 3.7630e-01 (3.2297e-01)	Acc@1  91.00 ( 91.03)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.024 ( 0.030)	Loss 3.1946e-01 (3.3500e-01)	Acc@1  93.00 ( 90.88)	Acc@5  99.00 ( 99.71)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 2.7383e-01 (3.3641e-01)	Acc@1  93.00 ( 90.82)	Acc@5  99.00 ( 99.71)
Test: [ 60/100]	Time  0.026 ( 0.028)	Loss 3.0198e-01 (3.3205e-01)	Acc@1  93.00 ( 90.75)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.027 ( 0.028)	Loss 4.9397e-01 (3.2755e-01)	Acc@1  86.00 ( 90.75)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.024 ( 0.028)	Loss 2.1268e-01 (3.2746e-01)	Acc@1  94.00 ( 90.73)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.024 ( 0.028)	Loss 2.4331e-01 (3.2279e-01)	Acc@1  93.00 ( 90.79)	Acc@5 100.00 ( 99.77)
 * Acc@1 90.790 Acc@5 99.760
### epoch[87] execution time: 28.065364837646484
EPOCH 88
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [88][  0/391]	Time  0.243 ( 0.243)	Data  0.178 ( 0.178)	Loss 5.5310e-02 (5.5310e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [88][ 10/391]	Time  0.064 ( 0.079)	Data  0.001 ( 0.017)	Loss 3.5734e-02 (8.8993e-02)	Acc@1 100.00 ( 97.09)	Acc@5 100.00 (100.00)
Epoch: [88][ 20/391]	Time  0.068 ( 0.072)	Data  0.001 ( 0.010)	Loss 8.1834e-02 (8.5833e-02)	Acc@1  98.44 ( 96.99)	Acc@5 100.00 (100.00)
Epoch: [88][ 30/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.007)	Loss 1.0329e-01 (8.3984e-02)	Acc@1  96.88 ( 97.15)	Acc@5 100.00 (100.00)
Epoch: [88][ 40/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.005)	Loss 5.4261e-02 (8.2429e-02)	Acc@1  97.66 ( 97.14)	Acc@5 100.00 (100.00)
Epoch: [88][ 50/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.005)	Loss 4.2299e-02 (7.8110e-02)	Acc@1  97.66 ( 97.37)	Acc@5 100.00 (100.00)
Epoch: [88][ 60/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.004)	Loss 1.0758e-01 (7.6963e-02)	Acc@1  95.31 ( 97.34)	Acc@5 100.00 (100.00)
Epoch: [88][ 70/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.004)	Loss 7.5770e-02 (7.5561e-02)	Acc@1  97.66 ( 97.37)	Acc@5 100.00 (100.00)
Epoch: [88][ 80/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 9.3229e-02 (7.5907e-02)	Acc@1  96.09 ( 97.38)	Acc@5 100.00 (100.00)
Epoch: [88][ 90/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.003)	Loss 9.6082e-02 (7.5750e-02)	Acc@1  96.09 ( 97.37)	Acc@5 100.00 ( 99.99)
Epoch: [88][100/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.003)	Loss 6.0959e-02 (7.6205e-02)	Acc@1  97.66 ( 97.34)	Acc@5 100.00 ( 99.99)
Epoch: [88][110/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 7.5633e-02 (7.5413e-02)	Acc@1  97.66 ( 97.36)	Acc@5 100.00 ( 99.99)
Epoch: [88][120/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.003)	Loss 7.0584e-02 (7.5687e-02)	Acc@1  97.66 ( 97.36)	Acc@5 100.00 ( 99.99)
Epoch: [88][130/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3482e-01 (7.5284e-02)	Acc@1  94.53 ( 97.36)	Acc@5 100.00 ( 99.99)
Epoch: [88][140/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1556e-01 (7.6689e-02)	Acc@1  96.88 ( 97.34)	Acc@5 100.00 ( 99.98)
Epoch: [88][150/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0725e-01 (7.7517e-02)	Acc@1  96.09 ( 97.33)	Acc@5 100.00 ( 99.98)
Epoch: [88][160/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.2988e-02 (7.7575e-02)	Acc@1  97.66 ( 97.35)	Acc@5 100.00 ( 99.99)
Epoch: [88][170/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.8609e-02 (7.6754e-02)	Acc@1  98.44 ( 97.37)	Acc@5 100.00 ( 99.99)
Epoch: [88][180/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.3586e-02 (7.6505e-02)	Acc@1  96.88 ( 97.36)	Acc@5 100.00 ( 99.99)
Epoch: [88][190/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.9042e-02 (7.5672e-02)	Acc@1  97.66 ( 97.40)	Acc@5 100.00 ( 99.99)
Epoch: [88][200/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.8741e-02 (7.5062e-02)	Acc@1  97.66 ( 97.43)	Acc@5 100.00 ( 99.99)
Epoch: [88][210/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.1306e-02 (7.5820e-02)	Acc@1  96.09 ( 97.37)	Acc@5 100.00 ( 99.99)
Epoch: [88][220/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.7586e-02 (7.6152e-02)	Acc@1  98.44 ( 97.38)	Acc@5 100.00 ( 99.99)
Epoch: [88][230/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.7839e-02 (7.6118e-02)	Acc@1  98.44 ( 97.39)	Acc@5 100.00 ( 99.99)
Epoch: [88][240/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.9692e-02 (7.6809e-02)	Acc@1  97.66 ( 97.36)	Acc@5 100.00 ( 99.99)
Epoch: [88][250/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.0694e-01 (7.7213e-02)	Acc@1  96.09 ( 97.35)	Acc@5 100.00 ( 99.99)
Epoch: [88][260/391]	Time  0.061 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.8635e-02 (7.7054e-02)	Acc@1  97.66 ( 97.35)	Acc@5 100.00 ( 99.99)
Epoch: [88][270/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.6664e-01 (7.7713e-02)	Acc@1  92.19 ( 97.32)	Acc@5 100.00 ( 99.99)
Epoch: [88][280/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 9.0874e-02 (7.7706e-02)	Acc@1  97.66 ( 97.33)	Acc@5 100.00 ( 99.99)
Epoch: [88][290/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 9.0296e-02 (7.7728e-02)	Acc@1  96.88 ( 97.33)	Acc@5 100.00 ( 99.99)
Epoch: [88][300/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.0952e-01 (7.7847e-02)	Acc@1  96.88 ( 97.33)	Acc@5 100.00 ( 99.99)
Epoch: [88][310/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.8197e-02 (7.7292e-02)	Acc@1  99.22 ( 97.35)	Acc@5 100.00 ( 99.99)
Epoch: [88][320/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.4971e-02 (7.7232e-02)	Acc@1  96.88 ( 97.35)	Acc@5 100.00 ( 99.99)
Epoch: [88][330/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.3034e-01 (7.8091e-02)	Acc@1  96.09 ( 97.31)	Acc@5 100.00 ( 99.99)
Epoch: [88][340/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 8.0157e-02 (7.7672e-02)	Acc@1  96.09 ( 97.33)	Acc@5 100.00 ( 99.99)
Epoch: [88][350/391]	Time  0.066 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.4942e-02 (7.7141e-02)	Acc@1  98.44 ( 97.34)	Acc@5 100.00 ( 99.99)
Epoch: [88][360/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.9991e-02 (7.6874e-02)	Acc@1  97.66 ( 97.34)	Acc@5 100.00 ( 99.99)
Epoch: [88][370/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.0872e-01 (7.7281e-02)	Acc@1  96.09 ( 97.33)	Acc@5 100.00 ( 99.99)
Epoch: [88][380/391]	Time  0.066 ( 0.064)	Data  0.001 ( 0.002)	Loss 2.4833e-01 (7.8279e-02)	Acc@1  92.19 ( 97.29)	Acc@5 100.00 ( 99.99)
Epoch: [88][390/391]	Time  0.044 ( 0.064)	Data  0.001 ( 0.002)	Loss 8.8489e-02 (7.8346e-02)	Acc@1  97.50 ( 97.30)	Acc@5 100.00 ( 99.99)
## e[88] optimizer.zero_grad (sum) time: 0.39267992973327637
## e[88]       loss.backward (sum) time: 6.896684169769287
## e[88]      optimizer.step (sum) time: 3.3622848987579346
## epoch[88] training(only) time: 25.144472122192383
# Switched to evaluate mode...
Test: [  0/100]	Time  0.192 ( 0.192)	Loss 3.1092e-01 (3.1092e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.028 ( 0.042)	Loss 4.1772e-01 (2.9328e-01)	Acc@1  91.00 ( 90.91)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.024 ( 0.034)	Loss 2.9251e-01 (3.0391e-01)	Acc@1  90.00 ( 91.00)	Acc@5  99.00 ( 99.62)
Test: [ 30/100]	Time  0.027 ( 0.031)	Loss 3.9255e-01 (3.2435e-01)	Acc@1  89.00 ( 90.90)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.028 ( 0.030)	Loss 3.1588e-01 (3.3533e-01)	Acc@1  93.00 ( 90.73)	Acc@5  99.00 ( 99.66)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 2.7962e-01 (3.3760e-01)	Acc@1  92.00 ( 90.78)	Acc@5  99.00 ( 99.65)
Test: [ 60/100]	Time  0.028 ( 0.029)	Loss 3.0600e-01 (3.3334e-01)	Acc@1  93.00 ( 90.77)	Acc@5 100.00 ( 99.69)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 5.0313e-01 (3.2769e-01)	Acc@1  88.00 ( 90.76)	Acc@5 100.00 ( 99.72)
Test: [ 80/100]	Time  0.024 ( 0.028)	Loss 1.8801e-01 (3.2655e-01)	Acc@1  94.00 ( 90.65)	Acc@5 100.00 ( 99.73)
Test: [ 90/100]	Time  0.027 ( 0.028)	Loss 2.6124e-01 (3.2197e-01)	Acc@1  92.00 ( 90.75)	Acc@5 100.00 ( 99.75)
 * Acc@1 90.790 Acc@5 99.740
### epoch[88] execution time: 28.042258501052856
EPOCH 89
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [89][  0/391]	Time  0.244 ( 0.244)	Data  0.181 ( 0.181)	Loss 1.5190e-01 (1.5190e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
Epoch: [89][ 10/391]	Time  0.060 ( 0.079)	Data  0.001 ( 0.017)	Loss 4.3988e-02 (9.3828e-02)	Acc@1  98.44 ( 96.45)	Acc@5 100.00 (100.00)
Epoch: [89][ 20/391]	Time  0.064 ( 0.072)	Data  0.001 ( 0.010)	Loss 5.8507e-02 (8.7907e-02)	Acc@1  97.66 ( 96.73)	Acc@5 100.00 (100.00)
Epoch: [89][ 30/391]	Time  0.063 ( 0.069)	Data  0.001 ( 0.007)	Loss 5.4518e-02 (8.0932e-02)	Acc@1  98.44 ( 96.95)	Acc@5 100.00 (100.00)
Epoch: [89][ 40/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.005)	Loss 8.7241e-02 (8.1618e-02)	Acc@1  98.44 ( 96.95)	Acc@5 100.00 ( 99.98)
Epoch: [89][ 50/391]	Time  0.060 ( 0.067)	Data  0.001 ( 0.005)	Loss 9.6930e-02 (8.0574e-02)	Acc@1  97.66 ( 97.09)	Acc@5 100.00 ( 99.98)
Epoch: [89][ 60/391]	Time  0.060 ( 0.066)	Data  0.001 ( 0.004)	Loss 5.7953e-02 (7.9989e-02)	Acc@1  98.44 ( 97.14)	Acc@5 100.00 ( 99.99)
Epoch: [89][ 70/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.004)	Loss 1.0619e-01 (7.9453e-02)	Acc@1  96.09 ( 97.08)	Acc@5 100.00 ( 99.99)
Epoch: [89][ 80/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.003)	Loss 7.2601e-02 (7.8549e-02)	Acc@1  97.66 ( 97.14)	Acc@5 100.00 ( 99.99)
Epoch: [89][ 90/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.003)	Loss 6.6852e-02 (7.6533e-02)	Acc@1  98.44 ( 97.25)	Acc@5 100.00 ( 99.99)
Epoch: [89][100/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 8.8375e-02 (7.6119e-02)	Acc@1  96.09 ( 97.25)	Acc@5 100.00 ( 99.99)
Epoch: [89][110/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.003)	Loss 7.8154e-02 (7.6296e-02)	Acc@1  96.88 ( 97.26)	Acc@5 100.00 ( 99.99)
Epoch: [89][120/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.003)	Loss 5.1960e-02 (7.6409e-02)	Acc@1  98.44 ( 97.23)	Acc@5 100.00 ( 99.99)
Epoch: [89][130/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.7325e-02 (7.7281e-02)	Acc@1  99.22 ( 97.26)	Acc@5 100.00 ( 99.99)
Epoch: [89][140/391]	Time  0.071 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.9494e-02 (7.7678e-02)	Acc@1  96.09 ( 97.22)	Acc@5 100.00 ( 99.99)
Epoch: [89][150/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2396e-01 (7.7026e-02)	Acc@1  93.75 ( 97.23)	Acc@5 100.00 ( 99.99)
Epoch: [89][160/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1289e-01 (7.7717e-02)	Acc@1  97.66 ( 97.24)	Acc@5 100.00 (100.00)
Epoch: [89][170/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.0971e-02 (7.8351e-02)	Acc@1  99.22 ( 97.23)	Acc@5 100.00 (100.00)
Epoch: [89][180/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.8964e-02 (7.8252e-02)	Acc@1  97.66 ( 97.20)	Acc@5 100.00 ( 99.99)
Epoch: [89][190/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5272e-01 (7.7848e-02)	Acc@1  95.31 ( 97.22)	Acc@5 100.00 ( 99.99)
Epoch: [89][200/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.5745e-02 (7.9000e-02)	Acc@1  98.44 ( 97.17)	Acc@5 100.00 ( 99.99)
Epoch: [89][210/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.3516e-02 (7.9066e-02)	Acc@1  97.66 ( 97.16)	Acc@5 100.00 ( 99.99)
Epoch: [89][220/391]	Time  0.059 ( 0.064)	Data  0.001 ( 0.002)	Loss 9.2268e-02 (7.8943e-02)	Acc@1  96.88 ( 97.16)	Acc@5 100.00 ( 99.99)
Epoch: [89][230/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 4.3527e-02 (7.9007e-02)	Acc@1  99.22 ( 97.15)	Acc@5 100.00 ( 99.99)
Epoch: [89][240/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 3.4026e-02 (7.8662e-02)	Acc@1  99.22 ( 97.16)	Acc@5 100.00 ( 99.99)
Epoch: [89][250/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.0281e-02 (7.8615e-02)	Acc@1  99.22 ( 97.18)	Acc@5 100.00 ( 99.99)
Epoch: [89][260/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 9.7206e-02 (7.8251e-02)	Acc@1  96.88 ( 97.19)	Acc@5 100.00 ( 99.99)
Epoch: [89][270/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 8.4189e-02 (7.7987e-02)	Acc@1  96.88 ( 97.20)	Acc@5 100.00 ( 99.99)
Epoch: [89][280/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 8.1308e-02 (7.8007e-02)	Acc@1  96.09 ( 97.19)	Acc@5 100.00 ( 99.99)
Epoch: [89][290/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.7606e-02 (7.7942e-02)	Acc@1  96.88 ( 97.19)	Acc@5 100.00 ( 99.99)
Epoch: [89][300/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.3058e-01 (7.8183e-02)	Acc@1  97.66 ( 97.19)	Acc@5 100.00 ( 99.99)
Epoch: [89][310/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.7953e-02 (7.7886e-02)	Acc@1  96.88 ( 97.20)	Acc@5 100.00 ( 99.99)
Epoch: [89][320/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.1221e-02 (7.8403e-02)	Acc@1  97.66 ( 97.18)	Acc@5 100.00 ( 99.99)
Epoch: [89][330/391]	Time  0.063 ( 0.064)	Data  0.001 ( 0.002)	Loss 6.0367e-02 (7.8188e-02)	Acc@1  96.88 ( 97.20)	Acc@5 100.00 ( 99.99)
Epoch: [89][340/391]	Time  0.065 ( 0.064)	Data  0.001 ( 0.002)	Loss 5.5395e-02 (7.8251e-02)	Acc@1  96.88 ( 97.19)	Acc@5 100.00 ( 99.99)
Epoch: [89][350/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 9.2764e-02 (7.8274e-02)	Acc@1  95.31 ( 97.18)	Acc@5 100.00 ( 99.99)
Epoch: [89][360/391]	Time  0.062 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.4293e-02 (7.8114e-02)	Acc@1  96.88 ( 97.18)	Acc@5 100.00 ( 99.99)
Epoch: [89][370/391]	Time  0.064 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.1317e-01 (7.8318e-02)	Acc@1  96.88 ( 97.17)	Acc@5 100.00 ( 99.99)
Epoch: [89][380/391]	Time  0.066 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.5003e-02 (7.8743e-02)	Acc@1  97.66 ( 97.15)	Acc@5 100.00 ( 99.99)
Epoch: [89][390/391]	Time  0.046 ( 0.064)	Data  0.001 ( 0.002)	Loss 7.7773e-02 (7.8984e-02)	Acc@1  96.25 ( 97.14)	Acc@5 100.00 ( 99.99)
## e[89] optimizer.zero_grad (sum) time: 0.4016432762145996
## e[89]       loss.backward (sum) time: 6.940951347351074
## e[89]      optimizer.step (sum) time: 3.3114235401153564
## epoch[89] training(only) time: 25.203039169311523
# Switched to evaluate mode...
Test: [  0/100]	Time  0.200 ( 0.200)	Loss 3.2547e-01 (3.2547e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.027 ( 0.042)	Loss 4.1541e-01 (2.9938e-01)	Acc@1  91.00 ( 90.64)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.026 ( 0.034)	Loss 3.0733e-01 (3.0929e-01)	Acc@1  89.00 ( 90.76)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.028 ( 0.031)	Loss 3.8422e-01 (3.2341e-01)	Acc@1  89.00 ( 90.84)	Acc@5 100.00 ( 99.77)
Test: [ 40/100]	Time  0.032 ( 0.030)	Loss 3.2886e-01 (3.3483e-01)	Acc@1  91.00 ( 90.61)	Acc@5  99.00 ( 99.71)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 2.8445e-01 (3.3669e-01)	Acc@1  92.00 ( 90.69)	Acc@5  99.00 ( 99.69)
Test: [ 60/100]	Time  0.024 ( 0.029)	Loss 3.1551e-01 (3.3221e-01)	Acc@1  92.00 ( 90.59)	Acc@5 100.00 ( 99.70)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 4.7877e-01 (3.2680e-01)	Acc@1  88.00 ( 90.68)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.026 ( 0.028)	Loss 2.0620e-01 (3.2576e-01)	Acc@1  94.00 ( 90.67)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.028 ( 0.028)	Loss 2.5971e-01 (3.2096e-01)	Acc@1  92.00 ( 90.74)	Acc@5 100.00 ( 99.76)
 * Acc@1 90.820 Acc@5 99.750
### epoch[89] execution time: 28.11899733543396
### Training complete:
#### total training(only) time: 2277.0662631988525
##### Total run time: 2549.9831573963165
