# Model: mobilenet2
# Dataset: cifardecem
# Batch size: 128
# Freezeout: False
# Low precision: True
# Epochs: 90
# Initial learning rate: 0.1
# module_name: models_cifardecem.mobilenet
<function mobilenet2 at 0x7f4c75dfbf28>
# model requested: 'mobilenet2'
# printing out the model
MobileNetV2(
  (pre): Sequential(
    (0): Conv2d(3, 32, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (stage1): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage2): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96)
        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144)
        (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage3): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144)
        (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage4): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192)
        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage5): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576)
        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576)
        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage6): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576)
        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960)
        (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960)
        (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage7): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960)
      (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (conv1): Sequential(
    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (conv2): Conv2d(1280, 10, kernel_size=(1, 1), stride=(1, 1))
)
# model is low precision
# Model: mobilenet2
# Dataset: cifardecem
# Freezeout: False
# Low precision: True
# Initial learning rate: 0.1
# Criterion: CrossEntropyLoss()
# Optimizer: SGD
#	lr 0.1
#	momentum 0.9
#	dampening 0
#	weight_decay 0.0001
#	nesterov False
CIFAR10 loading and normalization
==> Preparing data..
# CIFAR10 / low precision
Files already downloaded and verified
Files already downloaded and verified
#--> Training data: <--#
#-->>
EPOCH 0
# current learning rate: 0.1
# Switched to train mode...
Epoch: [0][  0/391]	Time  3.603 ( 3.603)	Data  0.119 ( 0.119)	Loss 2.2832e+00 (2.2832e+00)	Acc@1  15.62 ( 15.62)	Acc@5  50.78 ( 50.78)
Epoch: [0][ 10/391]	Time  0.066 ( 0.388)	Data  0.001 ( 0.012)	Loss 3.0820e+00 (2.7550e+00)	Acc@1  14.06 ( 13.28)	Acc@5  67.97 ( 56.89)
Epoch: [0][ 20/391]	Time  0.062 ( 0.233)	Data  0.001 ( 0.007)	Loss 2.7188e+00 (2.9108e+00)	Acc@1  16.41 ( 14.47)	Acc@5  63.28 ( 59.23)
Epoch: [0][ 30/391]	Time  0.064 ( 0.179)	Data  0.001 ( 0.005)	Loss 2.3281e+00 (2.7934e+00)	Acc@1  10.94 ( 14.84)	Acc@5  71.09 ( 61.77)
Epoch: [0][ 40/391]	Time  0.065 ( 0.151)	Data  0.001 ( 0.004)	Loss 2.1426e+00 (2.6559e+00)	Acc@1  17.19 ( 16.14)	Acc@5  77.34 ( 64.29)
Epoch: [0][ 50/391]	Time  0.068 ( 0.134)	Data  0.001 ( 0.003)	Loss 1.9834e+00 (2.5479e+00)	Acc@1  21.09 ( 16.70)	Acc@5  78.12 ( 66.84)
Epoch: [0][ 60/391]	Time  0.063 ( 0.123)	Data  0.001 ( 0.003)	Loss 1.9883e+00 (2.4549e+00)	Acc@1  25.78 ( 18.17)	Acc@5  81.25 ( 69.61)
Epoch: [0][ 70/391]	Time  0.061 ( 0.115)	Data  0.001 ( 0.003)	Loss 1.8584e+00 (2.3853e+00)	Acc@1  32.81 ( 19.22)	Acc@5  85.94 ( 71.50)
Epoch: [0][ 80/391]	Time  0.067 ( 0.109)	Data  0.001 ( 0.003)	Loss 2.0859e+00 (2.3289e+00)	Acc@1  18.75 ( 20.14)	Acc@5  81.25 ( 72.79)
Epoch: [0][ 90/391]	Time  0.064 ( 0.104)	Data  0.001 ( 0.002)	Loss 1.8340e+00 (2.2784e+00)	Acc@1  28.12 ( 20.99)	Acc@5  81.25 ( 74.14)
Epoch: [0][100/391]	Time  0.066 ( 0.100)	Data  0.001 ( 0.002)	Loss 1.6953e+00 (2.2315e+00)	Acc@1  37.50 ( 21.90)	Acc@5  91.41 ( 75.36)
Epoch: [0][110/391]	Time  0.064 ( 0.097)	Data  0.001 ( 0.002)	Loss 1.8096e+00 (2.1951e+00)	Acc@1  30.47 ( 22.61)	Acc@5  88.28 ( 76.32)
Epoch: [0][120/391]	Time  0.069 ( 0.094)	Data  0.001 ( 0.002)	Loss 1.9082e+00 (2.1685e+00)	Acc@1  28.12 ( 23.28)	Acc@5  87.50 ( 77.07)
Epoch: [0][130/391]	Time  0.064 ( 0.092)	Data  0.001 ( 0.002)	Loss 1.8477e+00 (2.1424e+00)	Acc@1  27.34 ( 23.71)	Acc@5  86.72 ( 77.68)
Epoch: [0][140/391]	Time  0.063 ( 0.090)	Data  0.001 ( 0.002)	Loss 1.8398e+00 (2.1180e+00)	Acc@1  32.81 ( 24.29)	Acc@5  82.03 ( 78.30)
Epoch: [0][150/391]	Time  0.063 ( 0.089)	Data  0.001 ( 0.002)	Loss 1.8076e+00 (2.0907e+00)	Acc@1  33.59 ( 24.91)	Acc@5  86.72 ( 78.95)
Epoch: [0][160/391]	Time  0.072 ( 0.087)	Data  0.001 ( 0.002)	Loss 1.8799e+00 (2.0683e+00)	Acc@1  32.03 ( 25.45)	Acc@5  83.59 ( 79.50)
Epoch: [0][170/391]	Time  0.065 ( 0.086)	Data  0.001 ( 0.002)	Loss 1.7295e+00 (2.0466e+00)	Acc@1  34.38 ( 25.96)	Acc@5  86.72 ( 80.08)
Epoch: [0][180/391]	Time  0.064 ( 0.085)	Data  0.001 ( 0.002)	Loss 1.8779e+00 (2.0282e+00)	Acc@1  32.03 ( 26.54)	Acc@5  82.03 ( 80.49)
Epoch: [0][190/391]	Time  0.068 ( 0.084)	Data  0.001 ( 0.002)	Loss 1.7900e+00 (2.0141e+00)	Acc@1  32.81 ( 26.87)	Acc@5  83.59 ( 80.79)
Epoch: [0][200/391]	Time  0.066 ( 0.083)	Data  0.001 ( 0.002)	Loss 1.5215e+00 (1.9977e+00)	Acc@1  38.28 ( 27.36)	Acc@5  89.06 ( 81.15)
Epoch: [0][210/391]	Time  0.062 ( 0.082)	Data  0.001 ( 0.002)	Loss 1.5537e+00 (1.9836e+00)	Acc@1  34.38 ( 27.73)	Acc@5  89.84 ( 81.53)
Epoch: [0][220/391]	Time  0.062 ( 0.081)	Data  0.001 ( 0.002)	Loss 1.7324e+00 (1.9700e+00)	Acc@1  34.38 ( 28.21)	Acc@5  89.06 ( 81.86)
Epoch: [0][230/391]	Time  0.059 ( 0.080)	Data  0.001 ( 0.002)	Loss 1.6680e+00 (1.9571e+00)	Acc@1  39.06 ( 28.60)	Acc@5  85.16 ( 82.14)
Epoch: [0][240/391]	Time  0.064 ( 0.080)	Data  0.001 ( 0.002)	Loss 1.6035e+00 (1.9418e+00)	Acc@1  37.50 ( 29.09)	Acc@5  90.62 ( 82.47)
Epoch: [0][250/391]	Time  0.059 ( 0.079)	Data  0.001 ( 0.002)	Loss 1.7178e+00 (1.9310e+00)	Acc@1  36.72 ( 29.35)	Acc@5  85.16 ( 82.77)
Epoch: [0][260/391]	Time  0.067 ( 0.078)	Data  0.001 ( 0.002)	Loss 1.5518e+00 (1.9191e+00)	Acc@1  35.94 ( 29.69)	Acc@5  91.41 ( 83.04)
Epoch: [0][270/391]	Time  0.063 ( 0.078)	Data  0.001 ( 0.002)	Loss 1.5791e+00 (1.9075e+00)	Acc@1  39.06 ( 30.05)	Acc@5  89.84 ( 83.32)
Epoch: [0][280/391]	Time  0.066 ( 0.077)	Data  0.001 ( 0.002)	Loss 1.5957e+00 (1.8970e+00)	Acc@1  40.62 ( 30.32)	Acc@5  89.06 ( 83.57)
Epoch: [0][290/391]	Time  0.064 ( 0.077)	Data  0.001 ( 0.002)	Loss 1.6797e+00 (1.8870e+00)	Acc@1  39.06 ( 30.66)	Acc@5  87.50 ( 83.77)
Epoch: [0][300/391]	Time  0.064 ( 0.077)	Data  0.001 ( 0.001)	Loss 1.4990e+00 (1.8762e+00)	Acc@1  45.31 ( 31.02)	Acc@5  90.62 ( 84.02)
Epoch: [0][310/391]	Time  0.060 ( 0.076)	Data  0.001 ( 0.001)	Loss 1.4648e+00 (1.8645e+00)	Acc@1  44.53 ( 31.43)	Acc@5  92.97 ( 84.25)
Epoch: [0][320/391]	Time  0.065 ( 0.076)	Data  0.001 ( 0.001)	Loss 1.6777e+00 (1.8535e+00)	Acc@1  37.50 ( 31.80)	Acc@5  88.28 ( 84.47)
Epoch: [0][330/391]	Time  0.062 ( 0.076)	Data  0.001 ( 0.001)	Loss 1.5664e+00 (1.8447e+00)	Acc@1  34.38 ( 32.02)	Acc@5  89.06 ( 84.65)
Epoch: [0][340/391]	Time  0.067 ( 0.075)	Data  0.001 ( 0.001)	Loss 1.5547e+00 (1.8374e+00)	Acc@1  45.31 ( 32.24)	Acc@5  91.41 ( 84.82)
Epoch: [0][350/391]	Time  0.063 ( 0.075)	Data  0.001 ( 0.001)	Loss 1.4902e+00 (1.8289e+00)	Acc@1  43.75 ( 32.54)	Acc@5  89.06 ( 84.98)
Epoch: [0][360/391]	Time  0.064 ( 0.075)	Data  0.001 ( 0.001)	Loss 1.5381e+00 (1.8216e+00)	Acc@1  45.31 ( 32.79)	Acc@5  85.94 ( 85.14)
Epoch: [0][370/391]	Time  0.062 ( 0.074)	Data  0.001 ( 0.001)	Loss 1.6914e+00 (1.8124e+00)	Acc@1  39.84 ( 33.12)	Acc@5  92.97 ( 85.35)
Epoch: [0][380/391]	Time  0.064 ( 0.074)	Data  0.001 ( 0.001)	Loss 1.5303e+00 (1.8051e+00)	Acc@1  38.28 ( 33.37)	Acc@5  90.62 ( 85.51)
Epoch: [0][390/391]	Time  0.425 ( 0.075)	Data  0.001 ( 0.001)	Loss 1.6904e+00 (1.7982e+00)	Acc@1  37.50 ( 33.57)	Acc@5  86.25 ( 85.63)
## e[0] optimizer.zero_grad (sum) time: 0.38069963455200195
## e[0]       loss.backward (sum) time: 7.858748912811279
## e[0]      optimizer.step (sum) time: 3.4369664192199707
## epoch[0] training(only) time: 29.244596481323242
# Switched to evaluate mode...
Test: [  0/100]	Time  0.293 ( 0.293)	Loss 1.5244e+00 (1.5244e+00)	Acc@1  47.00 ( 47.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.024 ( 0.050)	Loss 1.6738e+00 (1.5686e+00)	Acc@1  42.00 ( 43.00)	Acc@5  84.00 ( 89.82)
Test: [ 20/100]	Time  0.024 ( 0.038)	Loss 1.3281e+00 (1.5742e+00)	Acc@1  53.00 ( 42.90)	Acc@5  93.00 ( 90.48)
Test: [ 30/100]	Time  0.024 ( 0.034)	Loss 1.3525e+00 (1.5805e+00)	Acc@1  53.00 ( 42.48)	Acc@5  97.00 ( 90.61)
Test: [ 40/100]	Time  0.024 ( 0.032)	Loss 1.5459e+00 (1.5792e+00)	Acc@1  45.00 ( 42.37)	Acc@5  91.00 ( 90.71)
Test: [ 50/100]	Time  0.024 ( 0.030)	Loss 1.5332e+00 (1.5639e+00)	Acc@1  44.00 ( 42.71)	Acc@5  91.00 ( 91.02)
Test: [ 60/100]	Time  0.024 ( 0.029)	Loss 1.5693e+00 (1.5735e+00)	Acc@1  44.00 ( 42.56)	Acc@5  93.00 ( 90.80)
Test: [ 70/100]	Time  0.024 ( 0.029)	Loss 1.4961e+00 (1.5735e+00)	Acc@1  41.00 ( 42.37)	Acc@5  91.00 ( 90.75)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.5303e+00 (1.5705e+00)	Acc@1  44.00 ( 42.40)	Acc@5  91.00 ( 90.89)
Test: [ 90/100]	Time  0.024 ( 0.028)	Loss 1.4893e+00 (1.5788e+00)	Acc@1  43.00 ( 42.24)	Acc@5  93.00 ( 90.77)
 * Acc@1 42.470 Acc@5 90.790
### epoch[0] execution time: 32.12340188026428
EPOCH 1
# current learning rate: 0.1
# Switched to train mode...
Epoch: [1][  0/391]	Time  0.269 ( 0.269)	Data  0.189 ( 0.189)	Loss 1.5215e+00 (1.5215e+00)	Acc@1  42.19 ( 42.19)	Acc@5  91.41 ( 91.41)
Epoch: [1][ 10/391]	Time  0.067 ( 0.083)	Data  0.001 ( 0.018)	Loss 1.4131e+00 (1.5381e+00)	Acc@1  46.88 ( 41.26)	Acc@5  88.28 ( 91.26)
Epoch: [1][ 20/391]	Time  0.063 ( 0.074)	Data  0.001 ( 0.010)	Loss 1.4482e+00 (1.4892e+00)	Acc@1  52.34 ( 44.20)	Acc@5  86.72 ( 91.33)
Epoch: [1][ 30/391]	Time  0.068 ( 0.071)	Data  0.001 ( 0.007)	Loss 1.5078e+00 (1.4710e+00)	Acc@1  43.75 ( 45.36)	Acc@5  90.62 ( 91.56)
Epoch: [1][ 40/391]	Time  0.073 ( 0.069)	Data  0.001 ( 0.006)	Loss 1.4316e+00 (1.4634e+00)	Acc@1  51.56 ( 45.90)	Acc@5  89.06 ( 91.75)
Epoch: [1][ 50/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.005)	Loss 1.5967e+00 (1.4461e+00)	Acc@1  44.53 ( 46.72)	Acc@5  91.41 ( 92.00)
Epoch: [1][ 60/391]	Time  0.060 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.6562e+00 (1.4498e+00)	Acc@1  39.06 ( 46.71)	Acc@5  88.28 ( 91.89)
Epoch: [1][ 70/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.2822e+00 (1.4399e+00)	Acc@1  53.91 ( 47.04)	Acc@5  95.31 ( 92.18)
Epoch: [1][ 80/391]	Time  0.058 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.4346e+00 (1.4377e+00)	Acc@1  40.62 ( 47.02)	Acc@5  92.19 ( 92.21)
Epoch: [1][ 90/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.3037e+00 (1.4313e+00)	Acc@1  56.25 ( 47.38)	Acc@5  92.19 ( 92.21)
Epoch: [1][100/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.4297e+00 (1.4320e+00)	Acc@1  46.09 ( 47.41)	Acc@5  94.53 ( 92.39)
Epoch: [1][110/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.3691e+00 (1.4266e+00)	Acc@1  51.56 ( 47.60)	Acc@5  93.75 ( 92.46)
Epoch: [1][120/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.4336e+00 (1.4233e+00)	Acc@1  46.09 ( 47.70)	Acc@5  94.53 ( 92.54)
Epoch: [1][130/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3115e+00 (1.4210e+00)	Acc@1  53.91 ( 47.81)	Acc@5  91.41 ( 92.57)
Epoch: [1][140/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3389e+00 (1.4166e+00)	Acc@1  55.47 ( 48.02)	Acc@5  93.75 ( 92.69)
Epoch: [1][150/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3184e+00 (1.4118e+00)	Acc@1  49.22 ( 48.27)	Acc@5  91.41 ( 92.73)
Epoch: [1][160/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3457e+00 (1.4124e+00)	Acc@1  52.34 ( 48.24)	Acc@5  92.19 ( 92.75)
Epoch: [1][170/391]	Time  0.071 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2734e+00 (1.4056e+00)	Acc@1  57.03 ( 48.50)	Acc@5  92.97 ( 92.91)
Epoch: [1][180/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2344e+00 (1.4034e+00)	Acc@1  51.56 ( 48.60)	Acc@5  96.09 ( 92.96)
Epoch: [1][190/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3125e+00 (1.3994e+00)	Acc@1  52.34 ( 48.75)	Acc@5  90.62 ( 93.00)
Epoch: [1][200/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1533e+00 (1.3919e+00)	Acc@1  57.81 ( 49.09)	Acc@5  96.88 ( 93.09)
Epoch: [1][210/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2764e+00 (1.3876e+00)	Acc@1  52.34 ( 49.33)	Acc@5  96.09 ( 93.15)
Epoch: [1][220/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3584e+00 (1.3816e+00)	Acc@1  50.00 ( 49.63)	Acc@5  92.97 ( 93.20)
Epoch: [1][230/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1123e+00 (1.3794e+00)	Acc@1  61.72 ( 49.74)	Acc@5  94.53 ( 93.20)
Epoch: [1][240/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4463e+00 (1.3754e+00)	Acc@1  50.00 ( 49.94)	Acc@5  90.62 ( 93.26)
Epoch: [1][250/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2100e+00 (1.3711e+00)	Acc@1  53.12 ( 50.09)	Acc@5  94.53 ( 93.31)
Epoch: [1][260/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1484e+00 (1.3663e+00)	Acc@1  57.81 ( 50.30)	Acc@5  98.44 ( 93.35)
Epoch: [1][270/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4355e+00 (1.3619e+00)	Acc@1  53.12 ( 50.47)	Acc@5  91.41 ( 93.40)
Epoch: [1][280/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3945e+00 (1.3601e+00)	Acc@1  50.78 ( 50.59)	Acc@5  96.88 ( 93.42)
Epoch: [1][290/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2188e+00 (1.3567e+00)	Acc@1  55.47 ( 50.75)	Acc@5  96.09 ( 93.46)
Epoch: [1][300/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2734e+00 (1.3521e+00)	Acc@1  50.00 ( 50.93)	Acc@5  96.09 ( 93.55)
Epoch: [1][310/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2822e+00 (1.3497e+00)	Acc@1  54.69 ( 51.08)	Acc@5  93.75 ( 93.53)
Epoch: [1][320/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0684e+00 (1.3472e+00)	Acc@1  62.50 ( 51.22)	Acc@5  97.66 ( 93.54)
Epoch: [1][330/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2656e+00 (1.3450e+00)	Acc@1  57.03 ( 51.32)	Acc@5  92.97 ( 93.55)
Epoch: [1][340/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3359e+00 (1.3418e+00)	Acc@1  50.78 ( 51.44)	Acc@5  94.53 ( 93.59)
Epoch: [1][350/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2285e+00 (1.3392e+00)	Acc@1  56.25 ( 51.54)	Acc@5  95.31 ( 93.63)
Epoch: [1][360/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2441e+00 (1.3367e+00)	Acc@1  59.38 ( 51.66)	Acc@5  92.97 ( 93.65)
Epoch: [1][370/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.001)	Loss 1.1279e+00 (1.3324e+00)	Acc@1  65.62 ( 51.86)	Acc@5  94.53 ( 93.69)
Epoch: [1][380/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.001)	Loss 1.3037e+00 (1.3290e+00)	Acc@1  53.12 ( 52.01)	Acc@5  93.75 ( 93.73)
Epoch: [1][390/391]	Time  0.053 ( 0.065)	Data  0.001 ( 0.001)	Loss 1.2324e+00 (1.3260e+00)	Acc@1  62.50 ( 52.12)	Acc@5  92.50 ( 93.77)
## e[1] optimizer.zero_grad (sum) time: 0.39174389839172363
## e[1]       loss.backward (sum) time: 7.25484299659729
## e[1]      optimizer.step (sum) time: 3.4844117164611816
## epoch[1] training(only) time: 25.551464557647705
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 1.1924e+00 (1.1924e+00)	Acc@1  55.00 ( 55.00)	Acc@5  96.00 ( 96.00)
Test: [ 10/100]	Time  0.024 ( 0.040)	Loss 1.0654e+00 (1.1422e+00)	Acc@1  58.00 ( 58.64)	Acc@5  96.00 ( 96.27)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 1.0215e+00 (1.1503e+00)	Acc@1  64.00 ( 58.10)	Acc@5  98.00 ( 95.90)
Test: [ 30/100]	Time  0.027 ( 0.030)	Loss 1.1289e+00 (1.1689e+00)	Acc@1  58.00 ( 57.42)	Acc@5  95.00 ( 95.29)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 1.1035e+00 (1.1726e+00)	Acc@1  61.00 ( 57.17)	Acc@5  94.00 ( 95.39)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 1.2334e+00 (1.1688e+00)	Acc@1  61.00 ( 57.49)	Acc@5  94.00 ( 95.49)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 1.1719e+00 (1.1716e+00)	Acc@1  57.00 ( 57.44)	Acc@5  95.00 ( 95.59)
Test: [ 70/100]	Time  0.024 ( 0.027)	Loss 1.0830e+00 (1.1745e+00)	Acc@1  60.00 ( 57.38)	Acc@5  94.00 ( 95.52)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 1.1504e+00 (1.1707e+00)	Acc@1  64.00 ( 57.63)	Acc@5  97.00 ( 95.56)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 1.2490e+00 (1.1764e+00)	Acc@1  48.00 ( 57.42)	Acc@5  95.00 ( 95.36)
 * Acc@1 57.370 Acc@5 95.360
### epoch[1] execution time: 28.348554849624634
EPOCH 2
# current learning rate: 0.1
# Switched to train mode...
Epoch: [2][  0/391]	Time  0.238 ( 0.238)	Data  0.175 ( 0.175)	Loss 1.2861e+00 (1.2861e+00)	Acc@1  51.56 ( 51.56)	Acc@5  89.06 ( 89.06)
Epoch: [2][ 10/391]	Time  0.066 ( 0.079)	Data  0.001 ( 0.017)	Loss 1.1182e+00 (1.2237e+00)	Acc@1  60.16 ( 55.97)	Acc@5  93.75 ( 94.39)
Epoch: [2][ 20/391]	Time  0.063 ( 0.072)	Data  0.001 ( 0.009)	Loss 1.3330e+00 (1.1867e+00)	Acc@1  51.56 ( 57.70)	Acc@5  96.88 ( 95.20)
Epoch: [2][ 30/391]	Time  0.062 ( 0.069)	Data  0.001 ( 0.007)	Loss 1.2031e+00 (1.1657e+00)	Acc@1  58.59 ( 58.54)	Acc@5  94.53 ( 95.26)
Epoch: [2][ 40/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.005)	Loss 1.0186e+00 (1.1593e+00)	Acc@1  57.03 ( 58.67)	Acc@5  96.09 ( 95.10)
Epoch: [2][ 50/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.1094e+00 (1.1456e+00)	Acc@1  64.06 ( 59.42)	Acc@5  94.53 ( 95.14)
Epoch: [2][ 60/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 9.9951e-01 (1.1398e+00)	Acc@1  65.62 ( 59.40)	Acc@5  97.66 ( 95.24)
Epoch: [2][ 70/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.0977e+00 (1.1386e+00)	Acc@1  62.50 ( 59.53)	Acc@5  96.09 ( 95.26)
Epoch: [2][ 80/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.1387e+00 (1.1370e+00)	Acc@1  57.03 ( 59.31)	Acc@5  94.53 ( 95.39)
Epoch: [2][ 90/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.2490e+00 (1.1371e+00)	Acc@1  57.03 ( 59.51)	Acc@5  93.75 ( 95.34)
Epoch: [2][100/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 8.6426e-01 (1.1308e+00)	Acc@1  75.00 ( 59.72)	Acc@5  98.44 ( 95.44)
Epoch: [2][110/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.2979e+00 (1.1300e+00)	Acc@1  55.47 ( 59.83)	Acc@5  95.31 ( 95.35)
Epoch: [2][120/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0791e+00 (1.1318e+00)	Acc@1  61.72 ( 59.73)	Acc@5  97.66 ( 95.33)
Epoch: [2][130/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1973e+00 (1.1338e+00)	Acc@1  60.16 ( 59.64)	Acc@5  95.31 ( 95.39)
Epoch: [2][140/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3086e+00 (1.1377e+00)	Acc@1  52.34 ( 59.50)	Acc@5  93.75 ( 95.33)
Epoch: [2][150/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1338e+00 (1.1423e+00)	Acc@1  55.47 ( 59.29)	Acc@5  97.66 ( 95.33)
Epoch: [2][160/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0840e+00 (1.1413e+00)	Acc@1  60.16 ( 59.37)	Acc@5  98.44 ( 95.36)
Epoch: [2][170/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2471e+00 (1.1419e+00)	Acc@1  57.81 ( 59.29)	Acc@5  91.41 ( 95.38)
Epoch: [2][180/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1562e+00 (1.1432e+00)	Acc@1  59.38 ( 59.28)	Acc@5  96.09 ( 95.33)
Epoch: [2][190/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1582e+00 (1.1391e+00)	Acc@1  62.50 ( 59.50)	Acc@5  93.75 ( 95.38)
Epoch: [2][200/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.7314e-01 (1.1355e+00)	Acc@1  61.72 ( 59.50)	Acc@5  98.44 ( 95.43)
Epoch: [2][210/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0469e+00 (1.1336e+00)	Acc@1  60.94 ( 59.58)	Acc@5  96.09 ( 95.44)
Epoch: [2][220/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0732e+00 (1.1317e+00)	Acc@1  66.41 ( 59.68)	Acc@5  95.31 ( 95.47)
Epoch: [2][230/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0410e+00 (1.1308e+00)	Acc@1  60.94 ( 59.73)	Acc@5  96.88 ( 95.45)
Epoch: [2][240/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.9062e-01 (1.1276e+00)	Acc@1  69.53 ( 59.89)	Acc@5  98.44 ( 95.50)
Epoch: [2][250/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0654e+00 (1.1271e+00)	Acc@1  58.59 ( 59.89)	Acc@5  95.31 ( 95.49)
Epoch: [2][260/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0889e+00 (1.1249e+00)	Acc@1  59.38 ( 59.94)	Acc@5  96.09 ( 95.53)
Epoch: [2][270/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3037e+00 (1.1227e+00)	Acc@1  54.69 ( 60.04)	Acc@5  92.19 ( 95.51)
Epoch: [2][280/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.7412e-01 (1.1209e+00)	Acc@1  65.62 ( 60.12)	Acc@5  97.66 ( 95.51)
Epoch: [2][290/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1787e+00 (1.1188e+00)	Acc@1  61.72 ( 60.21)	Acc@5  97.66 ( 95.55)
Epoch: [2][300/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2393e+00 (1.1175e+00)	Acc@1  63.28 ( 60.27)	Acc@5  92.97 ( 95.54)
Epoch: [2][310/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0850e+00 (1.1142e+00)	Acc@1  61.72 ( 60.39)	Acc@5  95.31 ( 95.59)
Epoch: [2][320/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0977e+00 (1.1121e+00)	Acc@1  59.38 ( 60.48)	Acc@5  97.66 ( 95.62)
Epoch: [2][330/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0039e+00 (1.1086e+00)	Acc@1  62.50 ( 60.56)	Acc@5  96.88 ( 95.67)
Epoch: [2][340/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1699e+00 (1.1068e+00)	Acc@1  55.47 ( 60.63)	Acc@5  94.53 ( 95.69)
Epoch: [2][350/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2041e+00 (1.1059e+00)	Acc@1  57.03 ( 60.64)	Acc@5  95.31 ( 95.70)
Epoch: [2][360/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.001)	Loss 9.9268e-01 (1.1043e+00)	Acc@1  65.62 ( 60.66)	Acc@5  97.66 ( 95.72)
Epoch: [2][370/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.001)	Loss 9.8340e-01 (1.1007e+00)	Acc@1  68.75 ( 60.80)	Acc@5  96.88 ( 95.75)
Epoch: [2][380/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.001)	Loss 9.7852e-01 (1.0980e+00)	Acc@1  62.50 ( 60.89)	Acc@5  97.66 ( 95.79)
Epoch: [2][390/391]	Time  0.045 ( 0.065)	Data  0.001 ( 0.001)	Loss 1.0684e+00 (1.0967e+00)	Acc@1  62.50 ( 60.92)	Acc@5  97.50 ( 95.80)
## e[2] optimizer.zero_grad (sum) time: 0.39064550399780273
## e[2]       loss.backward (sum) time: 7.164564371109009
## e[2]      optimizer.step (sum) time: 3.4825570583343506
## epoch[2] training(only) time: 25.51758885383606
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 1.2051e+00 (1.2051e+00)	Acc@1  63.00 ( 63.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.027 ( 0.040)	Loss 1.2070e+00 (1.1766e+00)	Acc@1  56.00 ( 57.27)	Acc@5  92.00 ( 96.00)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 1.0791e+00 (1.1423e+00)	Acc@1  61.00 ( 58.71)	Acc@5  94.00 ( 96.29)
Test: [ 30/100]	Time  0.024 ( 0.030)	Loss 1.1562e+00 (1.1472e+00)	Acc@1  56.00 ( 58.48)	Acc@5  98.00 ( 96.16)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 1.0566e+00 (1.1433e+00)	Acc@1  62.00 ( 58.61)	Acc@5  96.00 ( 96.15)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 1.2148e+00 (1.1376e+00)	Acc@1  58.00 ( 59.18)	Acc@5  93.00 ( 96.10)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 1.0771e+00 (1.1443e+00)	Acc@1  59.00 ( 58.75)	Acc@5  97.00 ( 96.23)
Test: [ 70/100]	Time  0.024 ( 0.027)	Loss 1.2988e+00 (1.1457e+00)	Acc@1  53.00 ( 58.65)	Acc@5  94.00 ( 96.23)
Test: [ 80/100]	Time  0.027 ( 0.027)	Loss 1.1748e+00 (1.1450e+00)	Acc@1  55.00 ( 58.64)	Acc@5  95.00 ( 96.36)
Test: [ 90/100]	Time  0.027 ( 0.027)	Loss 1.0371e+00 (1.1434e+00)	Acc@1  60.00 ( 58.60)	Acc@5  98.00 ( 96.29)
 * Acc@1 58.650 Acc@5 96.290
### epoch[2] execution time: 28.32611870765686
EPOCH 3
# current learning rate: 0.1
# Switched to train mode...
Epoch: [3][  0/391]	Time  0.268 ( 0.268)	Data  0.202 ( 0.202)	Loss 9.7021e-01 (9.7021e-01)	Acc@1  61.72 ( 61.72)	Acc@5  99.22 ( 99.22)
Epoch: [3][ 10/391]	Time  0.064 ( 0.082)	Data  0.001 ( 0.019)	Loss 9.3115e-01 (1.0344e+00)	Acc@1  66.41 ( 62.22)	Acc@5  97.66 ( 96.66)
Epoch: [3][ 20/391]	Time  0.063 ( 0.074)	Data  0.001 ( 0.011)	Loss 9.4922e-01 (1.0189e+00)	Acc@1  66.41 ( 63.24)	Acc@5  97.66 ( 96.61)
Epoch: [3][ 30/391]	Time  0.066 ( 0.071)	Data  0.001 ( 0.008)	Loss 8.3984e-01 (1.0115e+00)	Acc@1  69.53 ( 63.58)	Acc@5  97.66 ( 96.70)
Epoch: [3][ 40/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.006)	Loss 1.0107e+00 (1.0026e+00)	Acc@1  64.84 ( 63.99)	Acc@5  96.88 ( 96.86)
Epoch: [3][ 50/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.005)	Loss 9.2188e-01 (9.9951e-01)	Acc@1  66.41 ( 64.31)	Acc@5  98.44 ( 96.83)
Epoch: [3][ 60/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.1279e+00 (1.0039e+00)	Acc@1  56.25 ( 64.16)	Acc@5  95.31 ( 96.79)
Epoch: [3][ 70/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.004)	Loss 9.4629e-01 (1.0020e+00)	Acc@1  64.06 ( 64.12)	Acc@5  98.44 ( 96.79)
Epoch: [3][ 80/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.1807e+00 (9.9993e-01)	Acc@1  62.50 ( 64.23)	Acc@5  95.31 ( 96.88)
Epoch: [3][ 90/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.003)	Loss 9.9365e-01 (9.9740e-01)	Acc@1  64.84 ( 64.32)	Acc@5  97.66 ( 96.95)
Epoch: [3][100/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 8.6768e-01 (9.9275e-01)	Acc@1  67.19 ( 64.57)	Acc@5 100.00 ( 96.94)
Epoch: [3][110/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.0244e+00 (9.9264e-01)	Acc@1  67.19 ( 64.74)	Acc@5  92.97 ( 96.80)
Epoch: [3][120/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 9.9463e-01 (9.8936e-01)	Acc@1  66.41 ( 64.73)	Acc@5  94.53 ( 96.79)
Epoch: [3][130/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.0645e+00 (9.9056e-01)	Acc@1  62.50 ( 64.67)	Acc@5  96.09 ( 96.79)
Epoch: [3][140/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0361e+00 (9.9286e-01)	Acc@1  62.50 ( 64.64)	Acc@5  96.88 ( 96.73)
Epoch: [3][150/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.5361e-01 (9.9512e-01)	Acc@1  65.62 ( 64.49)	Acc@5  96.88 ( 96.78)
Epoch: [3][160/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.9805e-01 (9.9452e-01)	Acc@1  62.50 ( 64.48)	Acc@5  95.31 ( 96.79)
Epoch: [3][170/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2373e+00 (9.9278e-01)	Acc@1  58.59 ( 64.59)	Acc@5  92.97 ( 96.80)
Epoch: [3][180/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0400e+00 (9.9040e-01)	Acc@1  60.16 ( 64.72)	Acc@5  96.88 ( 96.80)
Epoch: [3][190/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.6475e-01 (9.8827e-01)	Acc@1  71.09 ( 64.79)	Acc@5  98.44 ( 96.83)
Epoch: [3][200/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0713e+00 (9.8460e-01)	Acc@1  57.03 ( 64.87)	Acc@5  95.31 ( 96.88)
Epoch: [3][210/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.9707e-01 (9.8354e-01)	Acc@1  63.28 ( 64.94)	Acc@5  96.09 ( 96.88)
Epoch: [3][220/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.3311e-01 (9.8265e-01)	Acc@1  61.72 ( 64.96)	Acc@5  98.44 ( 96.86)
Epoch: [3][230/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.4326e-01 (9.7937e-01)	Acc@1  67.97 ( 65.12)	Acc@5  97.66 ( 96.83)
Epoch: [3][240/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.2129e-01 (9.7617e-01)	Acc@1  71.09 ( 65.24)	Acc@5  97.66 ( 96.86)
Epoch: [3][250/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.0918e-01 (9.7334e-01)	Acc@1  65.62 ( 65.35)	Acc@5  96.88 ( 96.89)
Epoch: [3][260/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0391e+00 (9.7226e-01)	Acc@1  68.75 ( 65.38)	Acc@5  96.88 ( 96.91)
Epoch: [3][270/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.5400e-01 (9.6986e-01)	Acc@1  69.53 ( 65.42)	Acc@5  99.22 ( 96.93)
Epoch: [3][280/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.8174e-01 (9.6693e-01)	Acc@1  71.09 ( 65.51)	Acc@5  96.88 ( 96.96)
Epoch: [3][290/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.7988e-01 (9.6450e-01)	Acc@1  71.88 ( 65.62)	Acc@5  97.66 ( 96.99)
Epoch: [3][300/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.9121e-01 (9.6344e-01)	Acc@1  61.72 ( 65.64)	Acc@5  96.88 ( 96.98)
Epoch: [3][310/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.9805e-01 (9.6306e-01)	Acc@1  68.75 ( 65.71)	Acc@5  96.09 ( 96.99)
Epoch: [3][320/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.4277e-01 (9.6159e-01)	Acc@1  68.75 ( 65.70)	Acc@5  98.44 ( 97.01)
Epoch: [3][330/391]	Time  0.070 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.1250e-01 (9.5971e-01)	Acc@1  75.00 ( 65.77)	Acc@5  96.88 ( 97.03)
Epoch: [3][340/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.3359e-01 (9.5860e-01)	Acc@1  66.41 ( 65.78)	Acc@5  96.88 ( 97.04)
Epoch: [3][350/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.2188e-01 (9.5680e-01)	Acc@1  72.66 ( 65.86)	Acc@5  96.88 ( 97.04)
Epoch: [3][360/391]	Time  0.061 ( 0.065)	Data  0.002 ( 0.002)	Loss 9.9463e-01 (9.5539e-01)	Acc@1  62.50 ( 65.89)	Acc@5  96.88 ( 97.06)
Epoch: [3][370/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.6729e-01 (9.5288e-01)	Acc@1  69.53 ( 66.02)	Acc@5  97.66 ( 97.08)
Epoch: [3][380/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.6172e-01 (9.5153e-01)	Acc@1  70.31 ( 66.07)	Acc@5  99.22 ( 97.07)
Epoch: [3][390/391]	Time  0.047 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.1484e-01 (9.4861e-01)	Acc@1  68.75 ( 66.18)	Acc@5  98.75 ( 97.10)
## e[3] optimizer.zero_grad (sum) time: 0.38773655891418457
## e[3]       loss.backward (sum) time: 7.196542501449585
## e[3]      optimizer.step (sum) time: 3.4930362701416016
## epoch[3] training(only) time: 25.558479070663452
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 9.0381e-01 (9.0381e-01)	Acc@1  70.00 ( 70.00)	Acc@5  97.00 ( 97.00)
Test: [ 10/100]	Time  0.024 ( 0.040)	Loss 9.0723e-01 (9.0851e-01)	Acc@1  70.00 ( 68.09)	Acc@5  96.00 ( 97.27)
Test: [ 20/100]	Time  0.026 ( 0.033)	Loss 8.7402e-01 (9.0446e-01)	Acc@1  64.00 ( 67.86)	Acc@5  98.00 ( 97.14)
Test: [ 30/100]	Time  0.024 ( 0.030)	Loss 8.2129e-01 (9.1600e-01)	Acc@1  71.00 ( 68.10)	Acc@5  99.00 ( 97.13)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 8.0566e-01 (9.1693e-01)	Acc@1  76.00 ( 68.15)	Acc@5  96.00 ( 97.24)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 8.4766e-01 (9.0483e-01)	Acc@1  67.00 ( 68.55)	Acc@5  97.00 ( 97.25)
Test: [ 60/100]	Time  0.027 ( 0.028)	Loss 9.6387e-01 (9.1616e-01)	Acc@1  65.00 ( 68.30)	Acc@5  97.00 ( 97.10)
Test: [ 70/100]	Time  0.024 ( 0.027)	Loss 9.4629e-01 (9.1888e-01)	Acc@1  66.00 ( 68.34)	Acc@5  97.00 ( 97.17)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 8.5010e-01 (9.1717e-01)	Acc@1  74.00 ( 68.30)	Acc@5  99.00 ( 97.19)
Test: [ 90/100]	Time  0.026 ( 0.027)	Loss 7.1875e-01 (9.1710e-01)	Acc@1  74.00 ( 68.22)	Acc@5 100.00 ( 97.23)
 * Acc@1 68.300 Acc@5 97.160
### epoch[3] execution time: 28.377737760543823
EPOCH 4
# current learning rate: 0.1
# Switched to train mode...
Epoch: [4][  0/391]	Time  0.264 ( 0.264)	Data  0.201 ( 0.201)	Loss 8.4180e-01 (8.4180e-01)	Acc@1  69.53 ( 69.53)	Acc@5  99.22 ( 99.22)
Epoch: [4][ 10/391]	Time  0.063 ( 0.083)	Data  0.001 ( 0.019)	Loss 9.5215e-01 (8.4126e-01)	Acc@1  67.19 ( 70.24)	Acc@5  97.66 ( 98.01)
Epoch: [4][ 20/391]	Time  0.064 ( 0.074)	Data  0.001 ( 0.011)	Loss 8.0811e-01 (8.6707e-01)	Acc@1  74.22 ( 69.16)	Acc@5  97.66 ( 97.95)
Epoch: [4][ 30/391]	Time  0.064 ( 0.071)	Data  0.001 ( 0.008)	Loss 8.8428e-01 (8.6024e-01)	Acc@1  69.53 ( 69.33)	Acc@5  99.22 ( 97.86)
Epoch: [4][ 40/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.006)	Loss 7.3096e-01 (8.6295e-01)	Acc@1  73.44 ( 69.19)	Acc@5  98.44 ( 97.92)
Epoch: [4][ 50/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.005)	Loss 8.5205e-01 (8.6848e-01)	Acc@1  71.09 ( 68.75)	Acc@5  99.22 ( 97.86)
Epoch: [4][ 60/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 8.3447e-01 (8.6928e-01)	Acc@1  73.44 ( 68.85)	Acc@5  96.88 ( 97.81)
Epoch: [4][ 70/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.0332e+00 (8.6464e-01)	Acc@1  64.84 ( 69.08)	Acc@5  98.44 ( 97.82)
Epoch: [4][ 80/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.0322e+00 (8.6385e-01)	Acc@1  62.50 ( 69.25)	Acc@5  94.53 ( 97.79)
Epoch: [4][ 90/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 7.4023e-01 (8.6246e-01)	Acc@1  71.88 ( 69.27)	Acc@5 100.00 ( 97.80)
Epoch: [4][100/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 8.7793e-01 (8.6078e-01)	Acc@1  67.97 ( 69.24)	Acc@5  97.66 ( 97.84)
Epoch: [4][110/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 9.0967e-01 (8.6423e-01)	Acc@1  71.88 ( 69.11)	Acc@5  95.31 ( 97.78)
Epoch: [4][120/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.0479e+00 (8.6564e-01)	Acc@1  64.06 ( 69.07)	Acc@5  98.44 ( 97.78)
Epoch: [4][130/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.003)	Loss 7.2705e-01 (8.6172e-01)	Acc@1  71.88 ( 69.21)	Acc@5 100.00 ( 97.79)
Epoch: [4][140/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 6.9531e-01 (8.6014e-01)	Acc@1  78.12 ( 69.34)	Acc@5  96.88 ( 97.81)
Epoch: [4][150/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.3389e-01 (8.5930e-01)	Acc@1  74.22 ( 69.35)	Acc@5  99.22 ( 97.83)
Epoch: [4][160/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.5928e-01 (8.5564e-01)	Acc@1  72.66 ( 69.54)	Acc@5  99.22 ( 97.84)
Epoch: [4][170/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.9941e-01 (8.5487e-01)	Acc@1  69.53 ( 69.54)	Acc@5  96.88 ( 97.83)
Epoch: [4][180/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.4561e-01 (8.5292e-01)	Acc@1  75.78 ( 69.65)	Acc@5  99.22 ( 97.85)
Epoch: [4][190/391]	Time  0.071 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.6279e-01 (8.5296e-01)	Acc@1  71.09 ( 69.67)	Acc@5  96.88 ( 97.81)
Epoch: [4][200/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.1797e-01 (8.5572e-01)	Acc@1  71.09 ( 69.64)	Acc@5  96.09 ( 97.79)
Epoch: [4][210/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.3496e-01 (8.5566e-01)	Acc@1  67.97 ( 69.62)	Acc@5  98.44 ( 97.79)
Epoch: [4][220/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.8525e-01 (8.5335e-01)	Acc@1  67.19 ( 69.68)	Acc@5  98.44 ( 97.79)
Epoch: [4][230/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.2080e-01 (8.5356e-01)	Acc@1  71.09 ( 69.67)	Acc@5  96.88 ( 97.79)
Epoch: [4][240/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.7881e-01 (8.5206e-01)	Acc@1  72.66 ( 69.71)	Acc@5 100.00 ( 97.79)
Epoch: [4][250/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.5967e-01 (8.5198e-01)	Acc@1  78.12 ( 69.73)	Acc@5 100.00 ( 97.79)
Epoch: [4][260/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.7090e-01 (8.5105e-01)	Acc@1  77.34 ( 69.80)	Acc@5  99.22 ( 97.79)
Epoch: [4][270/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.8809e-01 (8.4996e-01)	Acc@1  69.53 ( 69.85)	Acc@5  98.44 ( 97.79)
Epoch: [4][280/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.0176e-01 (8.4832e-01)	Acc@1  71.09 ( 69.89)	Acc@5  98.44 ( 97.81)
Epoch: [4][290/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.9941e-01 (8.4850e-01)	Acc@1  70.31 ( 69.89)	Acc@5  96.09 ( 97.80)
Epoch: [4][300/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.6035e-01 (8.4676e-01)	Acc@1  68.75 ( 69.94)	Acc@5 100.00 ( 97.80)
Epoch: [4][310/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.6367e-01 (8.4575e-01)	Acc@1  68.75 ( 70.01)	Acc@5  98.44 ( 97.81)
Epoch: [4][320/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.8555e-01 (8.4290e-01)	Acc@1  73.44 ( 70.13)	Acc@5  97.66 ( 97.82)
Epoch: [4][330/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.6680e-01 (8.4280e-01)	Acc@1  66.41 ( 70.14)	Acc@5  95.31 ( 97.81)
Epoch: [4][340/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.3496e-01 (8.4322e-01)	Acc@1  71.09 ( 70.11)	Acc@5  99.22 ( 97.82)
Epoch: [4][350/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.5928e-01 (8.4306e-01)	Acc@1  75.78 ( 70.13)	Acc@5  96.09 ( 97.82)
Epoch: [4][360/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.8770e-01 (8.4165e-01)	Acc@1  64.84 ( 70.15)	Acc@5  97.66 ( 97.82)
Epoch: [4][370/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.8174e-01 (8.4090e-01)	Acc@1  73.44 ( 70.19)	Acc@5  99.22 ( 97.82)
Epoch: [4][380/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.5879e-01 (8.3985e-01)	Acc@1  70.31 ( 70.24)	Acc@5  96.09 ( 97.81)
Epoch: [4][390/391]	Time  0.046 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.5674e-01 (8.3805e-01)	Acc@1  73.75 ( 70.35)	Acc@5 100.00 ( 97.82)
## e[4] optimizer.zero_grad (sum) time: 0.3955223560333252
## e[4]       loss.backward (sum) time: 7.230632543563843
## e[4]      optimizer.step (sum) time: 3.4161744117736816
## epoch[4] training(only) time: 25.54625129699707
# Switched to evaluate mode...
Test: [  0/100]	Time  0.176 ( 0.176)	Loss 7.2705e-01 (7.2705e-01)	Acc@1  75.00 ( 75.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.024 ( 0.039)	Loss 8.3789e-01 (7.6008e-01)	Acc@1  72.00 ( 73.82)	Acc@5  95.00 ( 98.18)
Test: [ 20/100]	Time  0.025 ( 0.032)	Loss 8.2275e-01 (7.5774e-01)	Acc@1  65.00 ( 73.48)	Acc@5 100.00 ( 98.19)
Test: [ 30/100]	Time  0.024 ( 0.030)	Loss 7.8418e-01 (7.6830e-01)	Acc@1  74.00 ( 73.58)	Acc@5  96.00 ( 98.06)
Test: [ 40/100]	Time  0.027 ( 0.029)	Loss 6.7725e-01 (7.6965e-01)	Acc@1  80.00 ( 73.71)	Acc@5  99.00 ( 98.00)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 6.5869e-01 (7.6844e-01)	Acc@1  79.00 ( 73.88)	Acc@5  99.00 ( 98.06)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 7.0947e-01 (7.6980e-01)	Acc@1  67.00 ( 73.31)	Acc@5  99.00 ( 98.08)
Test: [ 70/100]	Time  0.024 ( 0.027)	Loss 6.3672e-01 (7.7069e-01)	Acc@1  77.00 ( 73.45)	Acc@5  99.00 ( 98.13)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 6.7285e-01 (7.7028e-01)	Acc@1  76.00 ( 73.35)	Acc@5  97.00 ( 98.15)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 7.0264e-01 (7.7612e-01)	Acc@1  70.00 ( 73.09)	Acc@5  99.00 ( 98.08)
 * Acc@1 73.360 Acc@5 98.080
### epoch[4] execution time: 28.310577869415283
EPOCH 5
# current learning rate: 0.1
# Switched to train mode...
Epoch: [5][  0/391]	Time  0.250 ( 0.250)	Data  0.184 ( 0.184)	Loss 7.6318e-01 (7.6318e-01)	Acc@1  72.66 ( 72.66)	Acc@5  98.44 ( 98.44)
Epoch: [5][ 10/391]	Time  0.065 ( 0.081)	Data  0.001 ( 0.018)	Loss 6.9238e-01 (7.5355e-01)	Acc@1  75.00 ( 73.08)	Acc@5  97.66 ( 98.51)
Epoch: [5][ 20/391]	Time  0.067 ( 0.073)	Data  0.001 ( 0.010)	Loss 8.0029e-01 (7.7988e-01)	Acc@1  71.88 ( 72.43)	Acc@5  97.66 ( 98.21)
Epoch: [5][ 30/391]	Time  0.064 ( 0.071)	Data  0.001 ( 0.007)	Loss 7.8906e-01 (7.7728e-01)	Acc@1  71.88 ( 72.66)	Acc@5  98.44 ( 98.34)
Epoch: [5][ 40/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.006)	Loss 8.0273e-01 (7.7406e-01)	Acc@1  74.22 ( 72.94)	Acc@5  97.66 ( 98.32)
Epoch: [5][ 50/391]	Time  0.072 ( 0.068)	Data  0.001 ( 0.005)	Loss 6.0645e-01 (7.6129e-01)	Acc@1  82.03 ( 73.33)	Acc@5 100.00 ( 98.39)
Epoch: [5][ 60/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.004)	Loss 7.4707e-01 (7.6250e-01)	Acc@1  75.00 ( 73.42)	Acc@5  97.66 ( 98.28)
Epoch: [5][ 70/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.004)	Loss 6.8750e-01 (7.5900e-01)	Acc@1  75.78 ( 73.45)	Acc@5  99.22 ( 98.32)
Epoch: [5][ 80/391]	Time  0.060 ( 0.067)	Data  0.001 ( 0.003)	Loss 8.1543e-01 (7.5735e-01)	Acc@1  76.56 ( 73.47)	Acc@5  97.66 ( 98.29)
Epoch: [5][ 90/391]	Time  0.075 ( 0.067)	Data  0.001 ( 0.003)	Loss 6.4160e-01 (7.5930e-01)	Acc@1  75.78 ( 73.42)	Acc@5 100.00 ( 98.29)
Epoch: [5][100/391]	Time  0.061 ( 0.067)	Data  0.001 ( 0.003)	Loss 8.6328e-01 (7.6503e-01)	Acc@1  66.41 ( 73.20)	Acc@5  97.66 ( 98.24)
Epoch: [5][110/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 6.6064e-01 (7.6755e-01)	Acc@1  75.78 ( 73.13)	Acc@5  96.88 ( 98.13)
Epoch: [5][120/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 9.1846e-01 (7.6765e-01)	Acc@1  70.31 ( 73.08)	Acc@5  96.88 ( 98.09)
Epoch: [5][130/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 8.3691e-01 (7.6823e-01)	Acc@1  72.66 ( 73.05)	Acc@5  97.66 ( 98.13)
Epoch: [5][140/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.2461e-01 (7.6946e-01)	Acc@1  74.22 ( 72.98)	Acc@5  99.22 ( 98.11)
Epoch: [5][150/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.5010e-01 (7.6940e-01)	Acc@1  68.75 ( 73.01)	Acc@5  96.88 ( 98.11)
Epoch: [5][160/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.3984e-01 (7.7006e-01)	Acc@1  71.88 ( 72.98)	Acc@5  98.44 ( 98.11)
Epoch: [5][170/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.2080e-01 (7.7172e-01)	Acc@1  75.00 ( 72.98)	Acc@5  96.09 ( 98.07)
Epoch: [5][180/391]	Time  0.060 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.9775e-01 (7.7009e-01)	Acc@1  71.09 ( 73.00)	Acc@5  98.44 ( 98.07)
Epoch: [5][190/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.4922e-01 (7.7199e-01)	Acc@1  70.31 ( 72.89)	Acc@5  96.09 ( 98.08)
Epoch: [5][200/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.9561e-01 (7.7264e-01)	Acc@1  66.41 ( 72.85)	Acc@5  97.66 ( 98.06)
Epoch: [5][210/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.9834e-01 (7.7186e-01)	Acc@1  75.00 ( 72.90)	Acc@5  96.09 ( 98.08)
Epoch: [5][220/391]	Time  0.070 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.3438e-01 (7.6876e-01)	Acc@1  71.09 ( 73.01)	Acc@5  99.22 ( 98.11)
Epoch: [5][230/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.3574e-01 (7.6856e-01)	Acc@1  77.34 ( 73.04)	Acc@5  98.44 ( 98.10)
Epoch: [5][240/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0098e+00 (7.6844e-01)	Acc@1  64.06 ( 73.05)	Acc@5  96.88 ( 98.09)
Epoch: [5][250/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.8262e-01 (7.6670e-01)	Acc@1  75.00 ( 73.12)	Acc@5  98.44 ( 98.12)
Epoch: [5][260/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.4609e-01 (7.6443e-01)	Acc@1  71.88 ( 73.18)	Acc@5  99.22 ( 98.13)
Epoch: [5][270/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.3682e-01 (7.6288e-01)	Acc@1  73.44 ( 73.26)	Acc@5  99.22 ( 98.13)
Epoch: [5][280/391]	Time  0.071 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.5449e-01 (7.6500e-01)	Acc@1  71.88 ( 73.17)	Acc@5  96.88 ( 98.11)
Epoch: [5][290/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.2988e-01 (7.6335e-01)	Acc@1  73.44 ( 73.21)	Acc@5  99.22 ( 98.10)
Epoch: [5][300/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.4648e-01 (7.6248e-01)	Acc@1  78.91 ( 73.21)	Acc@5 100.00 ( 98.11)
Epoch: [5][310/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.0654e-01 (7.6117e-01)	Acc@1  75.78 ( 73.32)	Acc@5  99.22 ( 98.12)
Epoch: [5][320/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.4932e-01 (7.5935e-01)	Acc@1  82.03 ( 73.38)	Acc@5  99.22 ( 98.12)
Epoch: [5][330/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.9570e-01 (7.5657e-01)	Acc@1  78.12 ( 73.41)	Acc@5  99.22 ( 98.14)
Epoch: [5][340/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.0820e-01 (7.5725e-01)	Acc@1  69.53 ( 73.44)	Acc@5  97.66 ( 98.14)
Epoch: [5][350/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.4600e-01 (7.5602e-01)	Acc@1  80.47 ( 73.46)	Acc@5  98.44 ( 98.15)
Epoch: [5][360/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.2021e-01 (7.5619e-01)	Acc@1  76.56 ( 73.45)	Acc@5  99.22 ( 98.15)
Epoch: [5][370/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.6357e-01 (7.5607e-01)	Acc@1  78.12 ( 73.45)	Acc@5  97.66 ( 98.15)
Epoch: [5][380/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.6846e-01 (7.5555e-01)	Acc@1  73.44 ( 73.44)	Acc@5 100.00 ( 98.16)
Epoch: [5][390/391]	Time  0.051 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.0947e-01 (7.5728e-01)	Acc@1  75.00 ( 73.39)	Acc@5 100.00 ( 98.15)
## e[5] optimizer.zero_grad (sum) time: 0.38822031021118164
## e[5]       loss.backward (sum) time: 7.189906358718872
## e[5]      optimizer.step (sum) time: 3.480734348297119
## epoch[5] training(only) time: 25.54212784767151
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 9.7852e-01 (9.7852e-01)	Acc@1  69.00 ( 69.00)	Acc@5  97.00 ( 97.00)
Test: [ 10/100]	Time  0.024 ( 0.040)	Loss 1.1455e+00 (8.9600e-01)	Acc@1  65.00 ( 71.00)	Acc@5  97.00 ( 97.91)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 8.7793e-01 (8.7688e-01)	Acc@1  70.00 ( 70.67)	Acc@5  98.00 ( 97.43)
Test: [ 30/100]	Time  0.024 ( 0.030)	Loss 8.0127e-01 (8.8373e-01)	Acc@1  73.00 ( 70.26)	Acc@5  97.00 ( 97.39)
Test: [ 40/100]	Time  0.025 ( 0.029)	Loss 8.0469e-01 (8.8023e-01)	Acc@1  73.00 ( 70.12)	Acc@5  96.00 ( 97.44)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 7.4023e-01 (8.6898e-01)	Acc@1  73.00 ( 70.51)	Acc@5  97.00 ( 97.45)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 7.7734e-01 (8.7285e-01)	Acc@1  71.00 ( 70.10)	Acc@5  98.00 ( 97.61)
Test: [ 70/100]	Time  0.024 ( 0.027)	Loss 1.0088e+00 (8.7787e-01)	Acc@1  69.00 ( 69.82)	Acc@5  96.00 ( 97.66)
Test: [ 80/100]	Time  0.027 ( 0.027)	Loss 6.5088e-01 (8.6859e-01)	Acc@1  77.00 ( 70.04)	Acc@5 100.00 ( 97.73)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 8.7695e-01 (8.6838e-01)	Acc@1  68.00 ( 70.02)	Acc@5 100.00 ( 97.81)
 * Acc@1 70.100 Acc@5 97.860
### epoch[5] execution time: 28.350675344467163
EPOCH 6
# current learning rate: 0.1
# Switched to train mode...
Epoch: [6][  0/391]	Time  0.237 ( 0.237)	Data  0.176 ( 0.176)	Loss 7.9297e-01 (7.9297e-01)	Acc@1  75.00 ( 75.00)	Acc@5  98.44 ( 98.44)
Epoch: [6][ 10/391]	Time  0.068 ( 0.080)	Data  0.001 ( 0.017)	Loss 7.4561e-01 (7.5284e-01)	Acc@1  74.22 ( 74.36)	Acc@5  96.88 ( 97.66)
Epoch: [6][ 20/391]	Time  0.064 ( 0.072)	Data  0.001 ( 0.009)	Loss 7.2119e-01 (7.3761e-01)	Acc@1  76.56 ( 74.40)	Acc@5  98.44 ( 98.07)
Epoch: [6][ 30/391]	Time  0.065 ( 0.070)	Data  0.001 ( 0.007)	Loss 8.5498e-01 (7.2915e-01)	Acc@1  71.88 ( 74.75)	Acc@5  96.88 ( 98.01)
Epoch: [6][ 40/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.005)	Loss 7.8223e-01 (7.1445e-01)	Acc@1  71.09 ( 75.21)	Acc@5  98.44 ( 98.15)
Epoch: [6][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 6.2598e-01 (7.1725e-01)	Acc@1  76.56 ( 74.72)	Acc@5 100.00 ( 98.27)
Epoch: [6][ 60/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 6.2402e-01 (7.1680e-01)	Acc@1  80.47 ( 74.96)	Acc@5  97.66 ( 98.32)
Epoch: [6][ 70/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.004)	Loss 6.7676e-01 (7.1428e-01)	Acc@1  78.12 ( 75.09)	Acc@5  99.22 ( 98.33)
Epoch: [6][ 80/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 7.0166e-01 (7.1052e-01)	Acc@1  75.78 ( 75.19)	Acc@5  99.22 ( 98.33)
Epoch: [6][ 90/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 7.1533e-01 (7.1016e-01)	Acc@1  77.34 ( 75.37)	Acc@5  98.44 ( 98.33)
Epoch: [6][100/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.003)	Loss 8.8135e-01 (7.1399e-01)	Acc@1  69.53 ( 75.29)	Acc@5  97.66 ( 98.31)
Epoch: [6][110/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 7.1826e-01 (7.1270e-01)	Acc@1  74.22 ( 75.23)	Acc@5  96.88 ( 98.30)
Epoch: [6][120/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.8799e-01 (7.1222e-01)	Acc@1  76.56 ( 75.18)	Acc@5  96.88 ( 98.27)
Epoch: [6][130/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.7627e-01 (7.0902e-01)	Acc@1  75.78 ( 75.29)	Acc@5  99.22 ( 98.31)
Epoch: [6][140/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.3145e-01 (7.0985e-01)	Acc@1  76.56 ( 75.28)	Acc@5  96.88 ( 98.31)
Epoch: [6][150/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.5479e-01 (7.0783e-01)	Acc@1  78.12 ( 75.29)	Acc@5  98.44 ( 98.33)
Epoch: [6][160/391]	Time  0.058 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.7578e-01 (7.0972e-01)	Acc@1  78.12 ( 75.25)	Acc@5  99.22 ( 98.33)
Epoch: [6][170/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.1719e-01 (7.0820e-01)	Acc@1  76.56 ( 75.38)	Acc@5 100.00 ( 98.36)
Epoch: [6][180/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.4707e-01 (7.0947e-01)	Acc@1  75.78 ( 75.36)	Acc@5  96.88 ( 98.36)
Epoch: [6][190/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.0156e-01 (7.0924e-01)	Acc@1  78.12 ( 75.34)	Acc@5  99.22 ( 98.36)
Epoch: [6][200/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.7920e-01 (7.0783e-01)	Acc@1  76.56 ( 75.38)	Acc@5  97.66 ( 98.37)
Epoch: [6][210/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.2988e-01 (7.0634e-01)	Acc@1  80.47 ( 75.41)	Acc@5  99.22 ( 98.39)
Epoch: [6][220/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.7148e-01 (7.0353e-01)	Acc@1  75.00 ( 75.52)	Acc@5  97.66 ( 98.40)
Epoch: [6][230/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.5381e-01 (7.0409e-01)	Acc@1  75.78 ( 75.47)	Acc@5  98.44 ( 98.40)
Epoch: [6][240/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.0205e-01 (7.0431e-01)	Acc@1  78.12 ( 75.47)	Acc@5  99.22 ( 98.40)
Epoch: [6][250/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.1729e-01 (7.0467e-01)	Acc@1  71.88 ( 75.44)	Acc@5  97.66 ( 98.40)
Epoch: [6][260/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.7598e-01 (7.0418e-01)	Acc@1  73.44 ( 75.48)	Acc@5  97.66 ( 98.41)
Epoch: [6][270/391]	Time  0.072 ( 0.065)	Data  0.006 ( 0.002)	Loss 6.1719e-01 (7.0346e-01)	Acc@1  80.47 ( 75.52)	Acc@5  98.44 ( 98.41)
Epoch: [6][280/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.7305e-01 (7.0631e-01)	Acc@1  68.75 ( 75.44)	Acc@5  96.88 ( 98.40)
Epoch: [6][290/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.5479e-01 (7.0505e-01)	Acc@1  79.69 ( 75.50)	Acc@5  97.66 ( 98.41)
Epoch: [6][300/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.0205e-01 (7.0400e-01)	Acc@1  81.25 ( 75.51)	Acc@5  98.44 ( 98.43)
Epoch: [6][310/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.5283e-01 (7.0429e-01)	Acc@1  76.56 ( 75.52)	Acc@5  99.22 ( 98.43)
Epoch: [6][320/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.1201e-01 (7.0499e-01)	Acc@1  70.31 ( 75.55)	Acc@5  98.44 ( 98.42)
Epoch: [6][330/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.9531e-01 (7.0410e-01)	Acc@1  77.34 ( 75.57)	Acc@5  96.88 ( 98.41)
Epoch: [6][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.3086e-01 (7.0365e-01)	Acc@1  78.12 ( 75.59)	Acc@5  96.88 ( 98.41)
Epoch: [6][350/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.0312e-01 (7.0316e-01)	Acc@1  75.78 ( 75.57)	Acc@5  99.22 ( 98.42)
Epoch: [6][360/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.4453e-01 (7.0299e-01)	Acc@1  78.12 ( 75.60)	Acc@5 100.00 ( 98.41)
Epoch: [6][370/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.3633e-01 (7.0185e-01)	Acc@1  71.09 ( 75.61)	Acc@5  98.44 ( 98.41)
Epoch: [6][380/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.1621e-01 (7.0195e-01)	Acc@1  79.69 ( 75.60)	Acc@5  99.22 ( 98.41)
Epoch: [6][390/391]	Time  0.049 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.5361e-01 (7.0274e-01)	Acc@1  70.00 ( 75.60)	Acc@5  95.00 ( 98.41)
## e[6] optimizer.zero_grad (sum) time: 0.3899834156036377
## e[6]       loss.backward (sum) time: 7.190126180648804
## e[6]      optimizer.step (sum) time: 3.4719340801239014
## epoch[6] training(only) time: 25.438713312149048
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 6.3623e-01 (6.3623e-01)	Acc@1  74.00 ( 74.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.024 ( 0.040)	Loss 7.4561e-01 (6.6659e-01)	Acc@1  74.00 ( 75.82)	Acc@5  99.00 ( 98.82)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 6.6504e-01 (6.7348e-01)	Acc@1  75.00 ( 76.05)	Acc@5  99.00 ( 98.48)
Test: [ 30/100]	Time  0.028 ( 0.030)	Loss 8.0566e-01 (6.9026e-01)	Acc@1  73.00 ( 75.52)	Acc@5  98.00 ( 98.55)
Test: [ 40/100]	Time  0.025 ( 0.029)	Loss 6.1768e-01 (6.9405e-01)	Acc@1  80.00 ( 75.46)	Acc@5  97.00 ( 98.46)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 6.8457e-01 (6.8170e-01)	Acc@1  76.00 ( 76.02)	Acc@5  98.00 ( 98.57)
Test: [ 60/100]	Time  0.028 ( 0.028)	Loss 6.2549e-01 (6.8326e-01)	Acc@1  78.00 ( 75.85)	Acc@5  99.00 ( 98.62)
Test: [ 70/100]	Time  0.024 ( 0.027)	Loss 7.7832e-01 (6.8210e-01)	Acc@1  74.00 ( 75.96)	Acc@5  99.00 ( 98.66)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 6.6162e-01 (6.8037e-01)	Acc@1  80.00 ( 76.01)	Acc@5  96.00 ( 98.63)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 5.4736e-01 (6.8017e-01)	Acc@1  81.00 ( 76.05)	Acc@5  99.00 ( 98.59)
 * Acc@1 76.080 Acc@5 98.660
### epoch[6] execution time: 28.238940000534058
EPOCH 7
# current learning rate: 0.1
# Switched to train mode...
Epoch: [7][  0/391]	Time  0.250 ( 0.250)	Data  0.182 ( 0.182)	Loss 5.7617e-01 (5.7617e-01)	Acc@1  78.91 ( 78.91)	Acc@5  99.22 ( 99.22)
Epoch: [7][ 10/391]	Time  0.065 ( 0.081)	Data  0.001 ( 0.018)	Loss 6.0986e-01 (6.5882e-01)	Acc@1  78.12 ( 77.06)	Acc@5  99.22 ( 98.58)
Epoch: [7][ 20/391]	Time  0.063 ( 0.073)	Data  0.001 ( 0.010)	Loss 6.4844e-01 (6.6416e-01)	Acc@1  81.25 ( 76.97)	Acc@5  99.22 ( 98.70)
Epoch: [7][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.007)	Loss 5.0684e-01 (6.4981e-01)	Acc@1  85.16 ( 77.42)	Acc@5  99.22 ( 98.82)
Epoch: [7][ 40/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.005)	Loss 6.4990e-01 (6.5578e-01)	Acc@1  75.00 ( 77.21)	Acc@5  99.22 ( 98.76)
Epoch: [7][ 50/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.005)	Loss 7.9443e-01 (6.6411e-01)	Acc@1  71.88 ( 76.84)	Acc@5 100.00 ( 98.70)
Epoch: [7][ 60/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.004)	Loss 6.7529e-01 (6.7049e-01)	Acc@1  75.00 ( 76.56)	Acc@5 100.00 ( 98.63)
Epoch: [7][ 70/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.004)	Loss 5.3613e-01 (6.6735e-01)	Acc@1  82.03 ( 76.71)	Acc@5  97.66 ( 98.64)
Epoch: [7][ 80/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.003)	Loss 7.1582e-01 (6.7253e-01)	Acc@1  75.78 ( 76.49)	Acc@5  97.66 ( 98.56)
Epoch: [7][ 90/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.003)	Loss 8.1641e-01 (6.7869e-01)	Acc@1  71.88 ( 76.19)	Acc@5  98.44 ( 98.52)
Epoch: [7][100/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 5.5273e-01 (6.7594e-01)	Acc@1  81.25 ( 76.26)	Acc@5  99.22 ( 98.55)
Epoch: [7][110/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 6.2549e-01 (6.7119e-01)	Acc@1  79.69 ( 76.52)	Acc@5  98.44 ( 98.56)
Epoch: [7][120/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.003)	Loss 6.5430e-01 (6.7236e-01)	Acc@1  75.00 ( 76.41)	Acc@5 100.00 ( 98.57)
Epoch: [7][130/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.2705e-01 (6.7162e-01)	Acc@1  75.78 ( 76.41)	Acc@5  99.22 ( 98.54)
Epoch: [7][140/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.9805e-01 (6.6916e-01)	Acc@1  84.38 ( 76.55)	Acc@5  97.66 ( 98.53)
Epoch: [7][150/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.8545e-01 (6.6831e-01)	Acc@1  80.47 ( 76.59)	Acc@5  98.44 ( 98.54)
Epoch: [7][160/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.3867e-01 (6.6531e-01)	Acc@1  77.34 ( 76.70)	Acc@5  98.44 ( 98.54)
Epoch: [7][170/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.3047e-01 (6.6687e-01)	Acc@1  74.22 ( 76.69)	Acc@5  99.22 ( 98.54)
Epoch: [7][180/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.6465e-01 (6.6863e-01)	Acc@1  75.78 ( 76.70)	Acc@5  96.09 ( 98.52)
Epoch: [7][190/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.8105e-01 (6.6857e-01)	Acc@1  80.47 ( 76.68)	Acc@5  98.44 ( 98.53)
Epoch: [7][200/391]	Time  0.075 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.6230e-01 (6.6892e-01)	Acc@1  71.88 ( 76.73)	Acc@5  98.44 ( 98.55)
Epoch: [7][210/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.5986e-01 (6.7170e-01)	Acc@1  73.44 ( 76.67)	Acc@5  96.88 ( 98.52)
Epoch: [7][220/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.1875e-01 (6.7154e-01)	Acc@1  78.91 ( 76.69)	Acc@5  99.22 ( 98.51)
Epoch: [7][230/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.0547e-01 (6.7017e-01)	Acc@1  81.25 ( 76.74)	Acc@5  99.22 ( 98.51)
Epoch: [7][240/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.2158e-01 (6.6917e-01)	Acc@1  77.34 ( 76.78)	Acc@5  99.22 ( 98.54)
Epoch: [7][250/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.7617e-01 (6.6736e-01)	Acc@1  82.03 ( 76.86)	Acc@5  98.44 ( 98.55)
Epoch: [7][260/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.0010e-01 (6.6569e-01)	Acc@1  78.91 ( 76.87)	Acc@5  99.22 ( 98.57)
Epoch: [7][270/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.9326e-01 (6.6447e-01)	Acc@1  78.91 ( 76.91)	Acc@5  99.22 ( 98.60)
Epoch: [7][280/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.1572e-01 (6.6187e-01)	Acc@1  79.69 ( 76.99)	Acc@5  99.22 ( 98.61)
Epoch: [7][290/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.1367e-01 (6.6282e-01)	Acc@1  79.69 ( 76.94)	Acc@5 100.00 ( 98.59)
Epoch: [7][300/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.4668e-01 (6.6315e-01)	Acc@1  71.09 ( 76.93)	Acc@5  97.66 ( 98.59)
Epoch: [7][310/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.9287e-01 (6.6361e-01)	Acc@1  78.12 ( 76.89)	Acc@5  97.66 ( 98.59)
Epoch: [7][320/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.8252e-01 (6.6358e-01)	Acc@1  78.91 ( 76.89)	Acc@5 100.00 ( 98.61)
Epoch: [7][330/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.4990e-01 (6.6240e-01)	Acc@1  76.56 ( 76.91)	Acc@5  97.66 ( 98.60)
Epoch: [7][340/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.2012e-01 (6.6130e-01)	Acc@1  75.00 ( 76.93)	Acc@5  99.22 ( 98.60)
Epoch: [7][350/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.0215e-01 (6.6094e-01)	Acc@1  72.66 ( 76.92)	Acc@5  96.88 ( 98.61)
Epoch: [7][360/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.8750e-01 (6.6065e-01)	Acc@1  77.34 ( 76.92)	Acc@5  96.88 ( 98.60)
Epoch: [7][370/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.0205e-01 (6.5866e-01)	Acc@1  78.12 ( 76.99)	Acc@5  98.44 ( 98.61)
Epoch: [7][380/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.4199e-01 (6.5836e-01)	Acc@1  83.59 ( 77.03)	Acc@5 100.00 ( 98.60)
Epoch: [7][390/391]	Time  0.047 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.4795e-01 (6.5892e-01)	Acc@1  78.75 ( 77.03)	Acc@5 100.00 ( 98.61)
## e[7] optimizer.zero_grad (sum) time: 0.3949408531188965
## e[7]       loss.backward (sum) time: 7.167382001876831
## e[7]      optimizer.step (sum) time: 3.453618049621582
## epoch[7] training(only) time: 25.466506481170654
# Switched to evaluate mode...
Test: [  0/100]	Time  0.190 ( 0.190)	Loss 8.3301e-01 (8.3301e-01)	Acc@1  69.00 ( 69.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.024 ( 0.039)	Loss 1.0957e+00 (9.3493e-01)	Acc@1  69.00 ( 67.82)	Acc@5  95.00 ( 97.55)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 8.3057e-01 (9.2662e-01)	Acc@1  71.00 ( 68.43)	Acc@5  97.00 ( 97.71)
Test: [ 30/100]	Time  0.024 ( 0.030)	Loss 9.7119e-01 (9.4095e-01)	Acc@1  67.00 ( 67.81)	Acc@5  97.00 ( 97.48)
Test: [ 40/100]	Time  0.028 ( 0.029)	Loss 1.0225e+00 (9.5963e-01)	Acc@1  65.00 ( 66.98)	Acc@5  97.00 ( 97.20)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 8.7354e-01 (9.4472e-01)	Acc@1  73.00 ( 67.63)	Acc@5  97.00 ( 97.33)
Test: [ 60/100]	Time  0.026 ( 0.028)	Loss 9.4922e-01 (9.5323e-01)	Acc@1  68.00 ( 67.54)	Acc@5  95.00 ( 97.23)
Test: [ 70/100]	Time  0.024 ( 0.027)	Loss 1.0215e+00 (9.5167e-01)	Acc@1  66.00 ( 67.59)	Acc@5  98.00 ( 97.31)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 8.8330e-01 (9.4734e-01)	Acc@1  71.00 ( 67.68)	Acc@5  99.00 ( 97.36)
Test: [ 90/100]	Time  0.026 ( 0.027)	Loss 9.1260e-01 (9.4886e-01)	Acc@1  73.00 ( 67.56)	Acc@5 100.00 ( 97.31)
 * Acc@1 67.520 Acc@5 97.320
### epoch[7] execution time: 28.222203254699707
EPOCH 8
# current learning rate: 0.1
# Switched to train mode...
Epoch: [8][  0/391]	Time  0.241 ( 0.241)	Data  0.173 ( 0.173)	Loss 5.3271e-01 (5.3271e-01)	Acc@1  82.81 ( 82.81)	Acc@5  99.22 ( 99.22)
Epoch: [8][ 10/391]	Time  0.064 ( 0.080)	Data  0.001 ( 0.017)	Loss 7.2656e-01 (6.8235e-01)	Acc@1  76.56 ( 76.56)	Acc@5  97.66 ( 98.15)
Epoch: [8][ 20/391]	Time  0.064 ( 0.072)	Data  0.001 ( 0.009)	Loss 5.3320e-01 (6.5209e-01)	Acc@1  82.03 ( 77.38)	Acc@5  99.22 ( 98.51)
Epoch: [8][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.007)	Loss 6.5771e-01 (6.3932e-01)	Acc@1  78.91 ( 77.77)	Acc@5  96.88 ( 98.56)
Epoch: [8][ 40/391]	Time  0.062 ( 0.068)	Data  0.001 ( 0.005)	Loss 7.3145e-01 (6.4525e-01)	Acc@1  71.09 ( 77.53)	Acc@5  99.22 ( 98.57)
Epoch: [8][ 50/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.004)	Loss 6.2256e-01 (6.4210e-01)	Acc@1  79.69 ( 77.90)	Acc@5  98.44 ( 98.45)
Epoch: [8][ 60/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.004)	Loss 5.6738e-01 (6.3662e-01)	Acc@1  80.47 ( 78.06)	Acc@5  97.66 ( 98.48)
Epoch: [8][ 70/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 6.8408e-01 (6.3877e-01)	Acc@1  72.66 ( 78.09)	Acc@5  97.66 ( 98.45)
Epoch: [8][ 80/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.003)	Loss 6.6260e-01 (6.4476e-01)	Acc@1  72.66 ( 77.80)	Acc@5  99.22 ( 98.46)
Epoch: [8][ 90/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 6.9580e-01 (6.3790e-01)	Acc@1  75.00 ( 78.03)	Acc@5  99.22 ( 98.54)
Epoch: [8][100/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 6.5869e-01 (6.3165e-01)	Acc@1  77.34 ( 78.22)	Acc@5  98.44 ( 98.58)
Epoch: [8][110/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.003)	Loss 5.9521e-01 (6.2983e-01)	Acc@1  77.34 ( 78.22)	Acc@5  98.44 ( 98.61)
Epoch: [8][120/391]	Time  0.075 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.6367e-01 (6.3146e-01)	Acc@1  74.22 ( 78.15)	Acc@5  98.44 ( 98.62)
Epoch: [8][130/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.6201e-01 (6.3121e-01)	Acc@1  81.25 ( 78.17)	Acc@5 100.00 ( 98.63)
Epoch: [8][140/391]	Time  0.073 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.8984e-01 (6.2942e-01)	Acc@1  81.25 ( 78.19)	Acc@5  97.66 ( 98.64)
Epoch: [8][150/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.3965e-01 (6.2765e-01)	Acc@1  82.81 ( 78.24)	Acc@5 100.00 ( 98.68)
Epoch: [8][160/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.5527e-01 (6.2541e-01)	Acc@1  75.78 ( 78.32)	Acc@5  98.44 ( 98.69)
Epoch: [8][170/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.2163e-01 (6.2284e-01)	Acc@1  83.59 ( 78.44)	Acc@5  99.22 ( 98.71)
Epoch: [8][180/391]	Time  0.071 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.3389e-01 (6.2248e-01)	Acc@1  76.56 ( 78.50)	Acc@5  98.44 ( 98.71)
Epoch: [8][190/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.2598e-01 (6.2368e-01)	Acc@1  75.00 ( 78.40)	Acc@5 100.00 ( 98.72)
Epoch: [8][200/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.1494e-01 (6.2521e-01)	Acc@1  74.22 ( 78.39)	Acc@5  97.66 ( 98.69)
Epoch: [8][210/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.2939e-01 (6.2547e-01)	Acc@1  77.34 ( 78.37)	Acc@5  98.44 ( 98.70)
Epoch: [8][220/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.2266e-01 (6.2675e-01)	Acc@1  75.00 ( 78.28)	Acc@5  99.22 ( 98.70)
Epoch: [8][230/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.1670e-01 (6.2642e-01)	Acc@1  78.91 ( 78.29)	Acc@5  97.66 ( 98.70)
Epoch: [8][240/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.6206e-01 (6.2521e-01)	Acc@1  88.28 ( 78.38)	Acc@5  99.22 ( 98.71)
Epoch: [8][250/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.6738e-01 (6.2590e-01)	Acc@1  81.25 ( 78.37)	Acc@5  99.22 ( 98.71)
Epoch: [8][260/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.2910e-01 (6.2601e-01)	Acc@1  74.22 ( 78.39)	Acc@5  94.53 ( 98.71)
Epoch: [8][270/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.2510e-01 (6.2551e-01)	Acc@1  78.91 ( 78.48)	Acc@5  96.09 ( 98.70)
Epoch: [8][280/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.7910e-01 (6.2485e-01)	Acc@1  79.69 ( 78.46)	Acc@5 100.00 ( 98.70)
Epoch: [8][290/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.4746e-01 (6.2317e-01)	Acc@1  77.34 ( 78.50)	Acc@5  98.44 ( 98.71)
Epoch: [8][300/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.5498e-01 (6.2143e-01)	Acc@1  89.84 ( 78.55)	Acc@5  99.22 ( 98.73)
Epoch: [8][310/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.9688e-01 (6.2114e-01)	Acc@1  73.44 ( 78.55)	Acc@5  99.22 ( 98.73)
Epoch: [8][320/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.7363e-01 (6.1991e-01)	Acc@1  82.03 ( 78.58)	Acc@5 100.00 ( 98.75)
Epoch: [8][330/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.8657e-01 (6.1970e-01)	Acc@1  85.16 ( 78.55)	Acc@5  99.22 ( 98.76)
Epoch: [8][340/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.5859e-01 (6.1907e-01)	Acc@1  79.69 ( 78.59)	Acc@5 100.00 ( 98.76)
Epoch: [8][350/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.8496e-01 (6.1799e-01)	Acc@1  79.69 ( 78.60)	Acc@5 100.00 ( 98.76)
Epoch: [8][360/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.7827e-01 (6.1690e-01)	Acc@1  84.38 ( 78.65)	Acc@5  98.44 ( 98.77)
Epoch: [8][370/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.3770e-01 (6.1762e-01)	Acc@1  75.78 ( 78.64)	Acc@5  99.22 ( 98.78)
Epoch: [8][380/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.1572e-01 (6.1737e-01)	Acc@1  77.34 ( 78.64)	Acc@5  99.22 ( 98.78)
Epoch: [8][390/391]	Time  0.050 ( 0.065)	Data  0.001 ( 0.001)	Loss 5.6201e-01 (6.1784e-01)	Acc@1  81.25 ( 78.63)	Acc@5  98.75 ( 98.78)
## e[8] optimizer.zero_grad (sum) time: 0.3867213726043701
## e[8]       loss.backward (sum) time: 7.243600845336914
## e[8]      optimizer.step (sum) time: 3.4509778022766113
## epoch[8] training(only) time: 25.50966453552246
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 6.5869e-01 (6.5869e-01)	Acc@1  80.00 ( 80.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.027 ( 0.040)	Loss 6.5039e-01 (7.4938e-01)	Acc@1  80.00 ( 75.09)	Acc@5  98.00 ( 98.55)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 7.7344e-01 (7.4584e-01)	Acc@1  74.00 ( 75.33)	Acc@5  98.00 ( 98.48)
Test: [ 30/100]	Time  0.031 ( 0.031)	Loss 8.9111e-01 (7.6545e-01)	Acc@1  64.00 ( 74.68)	Acc@5  98.00 ( 98.39)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 8.3154e-01 (7.7805e-01)	Acc@1  73.00 ( 74.17)	Acc@5  98.00 ( 98.29)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 8.1787e-01 (7.5968e-01)	Acc@1  75.00 ( 74.59)	Acc@5  96.00 ( 98.27)
Test: [ 60/100]	Time  0.028 ( 0.028)	Loss 7.7393e-01 (7.6528e-01)	Acc@1  71.00 ( 74.33)	Acc@5  99.00 ( 98.38)
Test: [ 70/100]	Time  0.024 ( 0.027)	Loss 8.3691e-01 (7.5916e-01)	Acc@1  72.00 ( 74.56)	Acc@5  98.00 ( 98.37)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 7.0020e-01 (7.5723e-01)	Acc@1  75.00 ( 74.53)	Acc@5  99.00 ( 98.41)
Test: [ 90/100]	Time  0.026 ( 0.027)	Loss 8.7793e-01 (7.5714e-01)	Acc@1  70.00 ( 74.43)	Acc@5 100.00 ( 98.37)
 * Acc@1 74.540 Acc@5 98.450
### epoch[8] execution time: 28.294300317764282
EPOCH 9
# current learning rate: 0.1
# Switched to train mode...
Epoch: [9][  0/391]	Time  0.243 ( 0.243)	Data  0.174 ( 0.174)	Loss 5.9814e-01 (5.9814e-01)	Acc@1  78.91 ( 78.91)	Acc@5  98.44 ( 98.44)
Epoch: [9][ 10/391]	Time  0.064 ( 0.082)	Data  0.001 ( 0.017)	Loss 5.7080e-01 (5.9482e-01)	Acc@1  80.47 ( 78.98)	Acc@5  99.22 ( 98.72)
Epoch: [9][ 20/391]	Time  0.064 ( 0.074)	Data  0.001 ( 0.009)	Loss 5.5615e-01 (5.9410e-01)	Acc@1  78.12 ( 79.61)	Acc@5  99.22 ( 98.96)
Epoch: [9][ 30/391]	Time  0.063 ( 0.071)	Data  0.001 ( 0.007)	Loss 5.9521e-01 (5.9295e-01)	Acc@1  75.00 ( 79.56)	Acc@5  99.22 ( 99.02)
Epoch: [9][ 40/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.005)	Loss 4.8633e-01 (6.0440e-01)	Acc@1  85.94 ( 79.29)	Acc@5 100.00 ( 98.95)
Epoch: [9][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 5.6689e-01 (6.0484e-01)	Acc@1  79.69 ( 79.11)	Acc@5  96.88 ( 98.93)
Epoch: [9][ 60/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.004)	Loss 4.1260e-01 (5.9202e-01)	Acc@1  84.38 ( 79.44)	Acc@5  99.22 ( 99.01)
Epoch: [9][ 70/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.004)	Loss 6.8652e-01 (5.9181e-01)	Acc@1  78.12 ( 79.31)	Acc@5  97.66 ( 99.05)
Epoch: [9][ 80/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 6.3965e-01 (5.9376e-01)	Acc@1  75.78 ( 79.20)	Acc@5  99.22 ( 99.04)
Epoch: [9][ 90/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 5.4199e-01 (5.9229e-01)	Acc@1  76.56 ( 79.27)	Acc@5  99.22 ( 99.02)
Epoch: [9][100/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 5.2539e-01 (5.9395e-01)	Acc@1  78.12 ( 79.16)	Acc@5 100.00 ( 99.01)
Epoch: [9][110/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 7.4219e-01 (5.9900e-01)	Acc@1  75.78 ( 79.00)	Acc@5  97.66 ( 98.96)
Epoch: [9][120/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.5615e-01 (5.9979e-01)	Acc@1  78.12 ( 79.03)	Acc@5 100.00 ( 98.97)
Epoch: [9][130/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.9731e-01 (5.9676e-01)	Acc@1  82.03 ( 79.22)	Acc@5 100.00 ( 98.96)
Epoch: [9][140/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.0059e-01 (5.9451e-01)	Acc@1  78.91 ( 79.32)	Acc@5  96.88 ( 98.94)
Epoch: [9][150/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.5967e-01 (5.9451e-01)	Acc@1  75.00 ( 79.31)	Acc@5 100.00 ( 98.93)
Epoch: [9][160/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.3809e-01 (5.9454e-01)	Acc@1  85.16 ( 79.38)	Acc@5  99.22 ( 98.92)
Epoch: [9][170/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.8008e-01 (5.9220e-01)	Acc@1  78.12 ( 79.44)	Acc@5  98.44 ( 98.93)
Epoch: [9][180/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.3125e-01 (5.9016e-01)	Acc@1  81.25 ( 79.50)	Acc@5 100.00 ( 98.95)
Epoch: [9][190/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.3809e-01 (5.8760e-01)	Acc@1  81.25 ( 79.56)	Acc@5  99.22 ( 98.94)
Epoch: [9][200/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.0010e-01 (5.8810e-01)	Acc@1  78.12 ( 79.52)	Acc@5  97.66 ( 98.95)
Epoch: [9][210/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.3242e-01 (5.9037e-01)	Acc@1  75.00 ( 79.45)	Acc@5  98.44 ( 98.94)
Epoch: [9][220/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.8594e-01 (5.9176e-01)	Acc@1  79.69 ( 79.43)	Acc@5  98.44 ( 98.93)
Epoch: [9][230/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.9814e-01 (5.9261e-01)	Acc@1  79.69 ( 79.37)	Acc@5  99.22 ( 98.92)
Epoch: [9][240/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.8594e-01 (5.9358e-01)	Acc@1  77.34 ( 79.36)	Acc@5 100.00 ( 98.93)
Epoch: [9][250/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.0889e-01 (5.9543e-01)	Acc@1  78.91 ( 79.29)	Acc@5  97.66 ( 98.92)
Epoch: [9][260/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.3682e-01 (5.9532e-01)	Acc@1  76.56 ( 79.26)	Acc@5 100.00 ( 98.93)
Epoch: [9][270/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.6738e-01 (5.9472e-01)	Acc@1  81.25 ( 79.28)	Acc@5 100.00 ( 98.93)
Epoch: [9][280/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.2539e-01 (5.9464e-01)	Acc@1  79.69 ( 79.31)	Acc@5  98.44 ( 98.91)
Epoch: [9][290/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.5137e-01 (5.9494e-01)	Acc@1  76.56 ( 79.30)	Acc@5  99.22 ( 98.91)
Epoch: [9][300/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.9326e-01 (5.9507e-01)	Acc@1  78.12 ( 79.30)	Acc@5  99.22 ( 98.93)
Epoch: [9][310/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.0352e-01 (5.9409e-01)	Acc@1  81.25 ( 79.32)	Acc@5  97.66 ( 98.93)
Epoch: [9][320/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.4775e-01 (5.9386e-01)	Acc@1  82.81 ( 79.35)	Acc@5  99.22 ( 98.91)
Epoch: [9][330/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.4785e-01 (5.9339e-01)	Acc@1  82.03 ( 79.38)	Acc@5 100.00 ( 98.92)
Epoch: [9][340/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.9668e-01 (5.9306e-01)	Acc@1  82.03 ( 79.38)	Acc@5  97.66 ( 98.92)
Epoch: [9][350/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.7583e-01 (5.9197e-01)	Acc@1  82.81 ( 79.42)	Acc@5 100.00 ( 98.92)
Epoch: [9][360/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.9375e-01 (5.9241e-01)	Acc@1  77.34 ( 79.38)	Acc@5  96.88 ( 98.92)
Epoch: [9][370/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.1182e-01 (5.9226e-01)	Acc@1  81.25 ( 79.39)	Acc@5  98.44 ( 98.94)
Epoch: [9][380/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.6641e-01 (5.9301e-01)	Acc@1  82.81 ( 79.40)	Acc@5  99.22 ( 98.94)
Epoch: [9][390/391]	Time  0.046 ( 0.065)	Data  0.001 ( 0.001)	Loss 6.6797e-01 (5.9407e-01)	Acc@1  82.50 ( 79.35)	Acc@5  96.25 ( 98.93)
## e[9] optimizer.zero_grad (sum) time: 0.3910045623779297
## e[9]       loss.backward (sum) time: 7.202712297439575
## e[9]      optimizer.step (sum) time: 3.4728682041168213
## epoch[9] training(only) time: 25.550873279571533
# Switched to evaluate mode...
Test: [  0/100]	Time  0.190 ( 0.190)	Loss 6.7334e-01 (6.7334e-01)	Acc@1  77.00 ( 77.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.024 ( 0.041)	Loss 7.2998e-01 (6.5110e-01)	Acc@1  77.00 ( 78.00)	Acc@5  96.00 ( 98.36)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 6.9824e-01 (6.6739e-01)	Acc@1  73.00 ( 77.14)	Acc@5 100.00 ( 98.48)
Test: [ 30/100]	Time  0.027 ( 0.030)	Loss 6.8799e-01 (6.8455e-01)	Acc@1  81.00 ( 77.00)	Acc@5  99.00 ( 98.35)
Test: [ 40/100]	Time  0.027 ( 0.029)	Loss 6.7139e-01 (6.9067e-01)	Acc@1  78.00 ( 76.61)	Acc@5  97.00 ( 98.29)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 6.2354e-01 (6.8321e-01)	Acc@1  79.00 ( 76.92)	Acc@5  99.00 ( 98.31)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 7.0508e-01 (6.7930e-01)	Acc@1  77.00 ( 76.89)	Acc@5  99.00 ( 98.43)
Test: [ 70/100]	Time  0.024 ( 0.027)	Loss 6.6553e-01 (6.7673e-01)	Acc@1  76.00 ( 76.90)	Acc@5 100.00 ( 98.51)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 5.2344e-01 (6.7378e-01)	Acc@1  80.00 ( 76.95)	Acc@5  99.00 ( 98.54)
Test: [ 90/100]	Time  0.027 ( 0.027)	Loss 6.0449e-01 (6.7580e-01)	Acc@1  77.00 ( 76.87)	Acc@5  99.00 ( 98.58)
 * Acc@1 76.900 Acc@5 98.610
### epoch[9] execution time: 28.35369348526001
EPOCH 10
# current learning rate: 0.1
# Switched to train mode...
Epoch: [10][  0/391]	Time  0.245 ( 0.245)	Data  0.182 ( 0.182)	Loss 6.9727e-01 (6.9727e-01)	Acc@1  76.56 ( 76.56)	Acc@5  99.22 ( 99.22)
Epoch: [10][ 10/391]	Time  0.065 ( 0.080)	Data  0.001 ( 0.017)	Loss 5.3955e-01 (5.4801e-01)	Acc@1  82.81 ( 80.68)	Acc@5  99.22 ( 99.15)
Epoch: [10][ 20/391]	Time  0.065 ( 0.073)	Data  0.001 ( 0.010)	Loss 5.7764e-01 (5.4078e-01)	Acc@1  78.12 ( 81.18)	Acc@5  99.22 ( 99.00)
Epoch: [10][ 30/391]	Time  0.065 ( 0.071)	Data  0.001 ( 0.007)	Loss 7.6514e-01 (5.8123e-01)	Acc@1  73.44 ( 80.32)	Acc@5  97.66 ( 98.71)
Epoch: [10][ 40/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.005)	Loss 5.1221e-01 (5.8167e-01)	Acc@1  82.81 ( 80.14)	Acc@5  99.22 ( 98.88)
Epoch: [10][ 50/391]	Time  0.062 ( 0.068)	Data  0.001 ( 0.005)	Loss 6.1719e-01 (5.8217e-01)	Acc@1  74.22 ( 79.90)	Acc@5  98.44 ( 98.96)
Epoch: [10][ 60/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.004)	Loss 5.3125e-01 (5.8182e-01)	Acc@1  83.59 ( 79.97)	Acc@5  97.66 ( 98.98)
Epoch: [10][ 70/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.004)	Loss 5.5518e-01 (5.7702e-01)	Acc@1  80.47 ( 80.13)	Acc@5  97.66 ( 98.97)
Epoch: [10][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 5.9961e-01 (5.7487e-01)	Acc@1  81.25 ( 80.26)	Acc@5  98.44 ( 99.02)
Epoch: [10][ 90/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 5.3711e-01 (5.7477e-01)	Acc@1  79.69 ( 80.25)	Acc@5  99.22 ( 98.98)
Epoch: [10][100/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 5.6152e-01 (5.7436e-01)	Acc@1  78.91 ( 80.25)	Acc@5  99.22 ( 98.99)
Epoch: [10][110/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 7.7148e-01 (5.7556e-01)	Acc@1  76.56 ( 80.27)	Acc@5  96.88 ( 98.98)
Epoch: [10][120/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.003)	Loss 5.4004e-01 (5.7191e-01)	Acc@1  78.91 ( 80.38)	Acc@5  99.22 ( 98.97)
Epoch: [10][130/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.9585e-01 (5.7237e-01)	Acc@1  81.25 ( 80.40)	Acc@5 100.00 ( 98.99)
Epoch: [10][140/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.3574e-01 (5.7251e-01)	Acc@1  76.56 ( 80.38)	Acc@5  99.22 ( 98.98)
Epoch: [10][150/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.5146e-01 (5.7102e-01)	Acc@1  77.34 ( 80.44)	Acc@5  97.66 ( 98.97)
Epoch: [10][160/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.2832e-01 (5.6983e-01)	Acc@1  82.81 ( 80.42)	Acc@5  97.66 ( 98.97)
Epoch: [10][170/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.0547e-01 (5.7074e-01)	Acc@1  78.12 ( 80.41)	Acc@5  99.22 ( 98.98)
Epoch: [10][180/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.3135e-01 (5.7230e-01)	Acc@1  72.66 ( 80.26)	Acc@5  99.22 ( 98.96)
Epoch: [10][190/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.9658e-01 (5.7228e-01)	Acc@1  80.47 ( 80.25)	Acc@5  99.22 ( 98.97)
Epoch: [10][200/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.1855e-01 (5.7364e-01)	Acc@1  82.03 ( 80.20)	Acc@5  99.22 ( 98.98)
Epoch: [10][210/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.3418e-01 (5.7279e-01)	Acc@1  80.47 ( 80.21)	Acc@5 100.00 ( 98.99)
Epoch: [10][220/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.2949e-01 (5.7447e-01)	Acc@1  73.44 ( 80.17)	Acc@5  98.44 ( 98.98)
Epoch: [10][230/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.5728e-01 (5.7568e-01)	Acc@1  85.94 ( 80.13)	Acc@5  98.44 ( 98.96)
Epoch: [10][240/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.8120e-01 (5.7419e-01)	Acc@1  84.38 ( 80.18)	Acc@5  99.22 ( 98.96)
Epoch: [10][250/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.5190e-01 (5.7315e-01)	Acc@1  83.59 ( 80.22)	Acc@5 100.00 ( 98.97)
Epoch: [10][260/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.7432e-01 (5.7185e-01)	Acc@1  80.47 ( 80.27)	Acc@5  96.09 ( 98.97)
Epoch: [10][270/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.6992e-01 (5.7188e-01)	Acc@1  77.34 ( 80.27)	Acc@5  98.44 ( 98.98)
Epoch: [10][280/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.7373e-01 (5.7176e-01)	Acc@1  78.91 ( 80.28)	Acc@5  97.66 ( 98.96)
Epoch: [10][290/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.0684e-01 (5.7154e-01)	Acc@1  83.59 ( 80.28)	Acc@5  97.66 ( 98.95)
Epoch: [10][300/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.1465e-01 (5.7057e-01)	Acc@1  84.38 ( 80.32)	Acc@5  97.66 ( 98.95)
Epoch: [10][310/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.5371e-01 (5.6980e-01)	Acc@1  78.91 ( 80.35)	Acc@5  99.22 ( 98.96)
Epoch: [10][320/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.4150e-01 (5.7008e-01)	Acc@1  80.47 ( 80.27)	Acc@5  99.22 ( 98.97)
Epoch: [10][330/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.3384e-01 (5.6932e-01)	Acc@1  85.16 ( 80.30)	Acc@5 100.00 ( 98.98)
Epoch: [10][340/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.5029e-01 (5.6982e-01)	Acc@1  81.25 ( 80.28)	Acc@5  99.22 ( 98.98)
Epoch: [10][350/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.2100e-01 (5.6932e-01)	Acc@1  78.12 ( 80.28)	Acc@5 100.00 ( 98.99)
Epoch: [10][360/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.5283e-01 (5.7017e-01)	Acc@1  78.12 ( 80.24)	Acc@5 100.00 ( 98.99)
Epoch: [10][370/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.7510e-01 (5.6903e-01)	Acc@1  88.28 ( 80.27)	Acc@5  97.66 ( 98.98)
Epoch: [10][380/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.9473e-01 (5.6818e-01)	Acc@1  81.25 ( 80.31)	Acc@5  99.22 ( 98.99)
Epoch: [10][390/391]	Time  0.055 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.8984e-01 (5.6779e-01)	Acc@1  82.50 ( 80.34)	Acc@5  96.25 ( 98.99)
## e[10] optimizer.zero_grad (sum) time: 0.38097190856933594
## e[10]       loss.backward (sum) time: 7.21617865562439
## e[10]      optimizer.step (sum) time: 3.4278271198272705
## epoch[10] training(only) time: 25.465074062347412
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 5.5078e-01 (5.5078e-01)	Acc@1  82.00 ( 82.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.024 ( 0.040)	Loss 5.7520e-01 (5.8809e-01)	Acc@1  87.00 ( 80.45)	Acc@5  99.00 ( 99.18)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 6.6846e-01 (5.8402e-01)	Acc@1  72.00 ( 80.10)	Acc@5  99.00 ( 98.95)
Test: [ 30/100]	Time  0.024 ( 0.030)	Loss 6.2793e-01 (5.9628e-01)	Acc@1  81.00 ( 80.10)	Acc@5  98.00 ( 98.94)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 5.8740e-01 (5.9346e-01)	Acc@1  87.00 ( 80.46)	Acc@5  98.00 ( 98.83)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 6.6797e-01 (5.9393e-01)	Acc@1  81.00 ( 80.53)	Acc@5  97.00 ( 98.86)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 6.4258e-01 (5.9474e-01)	Acc@1  78.00 ( 80.25)	Acc@5  99.00 ( 98.90)
Test: [ 70/100]	Time  0.024 ( 0.027)	Loss 6.2158e-01 (5.9215e-01)	Acc@1  82.00 ( 80.31)	Acc@5  99.00 ( 98.94)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 5.0049e-01 (5.9175e-01)	Acc@1  81.00 ( 80.05)	Acc@5  99.00 ( 98.89)
Test: [ 90/100]	Time  0.024 ( 0.026)	Loss 3.8232e-01 (5.9201e-01)	Acc@1  86.00 ( 79.97)	Acc@5 100.00 ( 98.97)
 * Acc@1 79.910 Acc@5 99.020
### epoch[10] execution time: 28.249796390533447
EPOCH 11
# current learning rate: 0.1
# Switched to train mode...
Epoch: [11][  0/391]	Time  0.249 ( 0.249)	Data  0.183 ( 0.183)	Loss 6.3721e-01 (6.3721e-01)	Acc@1  81.25 ( 81.25)	Acc@5  97.66 ( 97.66)
Epoch: [11][ 10/391]	Time  0.063 ( 0.080)	Data  0.001 ( 0.018)	Loss 4.2847e-01 (5.3198e-01)	Acc@1  86.72 ( 82.81)	Acc@5 100.00 ( 99.08)
Epoch: [11][ 20/391]	Time  0.058 ( 0.072)	Data  0.001 ( 0.010)	Loss 5.8301e-01 (5.3352e-01)	Acc@1  82.03 ( 82.40)	Acc@5  97.66 ( 98.96)
Epoch: [11][ 30/391]	Time  0.067 ( 0.070)	Data  0.001 ( 0.007)	Loss 6.1328e-01 (5.4108e-01)	Acc@1  82.03 ( 81.78)	Acc@5  98.44 ( 98.97)
Epoch: [11][ 40/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.005)	Loss 5.0244e-01 (5.5650e-01)	Acc@1  81.25 ( 81.04)	Acc@5  98.44 ( 98.93)
Epoch: [11][ 50/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.005)	Loss 4.2993e-01 (5.3888e-01)	Acc@1  84.38 ( 81.54)	Acc@5 100.00 ( 99.03)
Epoch: [11][ 60/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.004)	Loss 5.7520e-01 (5.3942e-01)	Acc@1  82.81 ( 81.48)	Acc@5  97.66 ( 99.03)
Epoch: [11][ 70/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.004)	Loss 6.5332e-01 (5.4578e-01)	Acc@1  76.56 ( 81.32)	Acc@5  99.22 ( 98.95)
Epoch: [11][ 80/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.9146e-01 (5.4198e-01)	Acc@1  78.91 ( 81.31)	Acc@5 100.00 ( 98.96)
Epoch: [11][ 90/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 6.2549e-01 (5.4237e-01)	Acc@1  81.25 ( 81.46)	Acc@5  98.44 ( 98.95)
Epoch: [11][100/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.5483e-01 (5.4388e-01)	Acc@1  85.94 ( 81.44)	Acc@5  99.22 ( 98.98)
Epoch: [11][110/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.003)	Loss 5.1611e-01 (5.4857e-01)	Acc@1  79.69 ( 81.27)	Acc@5  99.22 ( 98.95)
Epoch: [11][120/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 5.5225e-01 (5.4758e-01)	Acc@1  80.47 ( 81.28)	Acc@5  99.22 ( 98.99)
Epoch: [11][130/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.8887e-01 (5.4764e-01)	Acc@1  77.34 ( 81.24)	Acc@5 100.00 ( 99.01)
Epoch: [11][140/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.0791e-01 (5.4800e-01)	Acc@1  77.34 ( 81.14)	Acc@5  99.22 ( 99.04)
Epoch: [11][150/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.4971e-01 (5.4691e-01)	Acc@1  85.94 ( 81.17)	Acc@5  99.22 ( 99.03)
Epoch: [11][160/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.2646e-01 (5.4712e-01)	Acc@1  76.56 ( 81.15)	Acc@5  99.22 ( 99.02)
Epoch: [11][170/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.7534e-01 (5.4749e-01)	Acc@1  82.03 ( 81.16)	Acc@5 100.00 ( 99.03)
Epoch: [11][180/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.4980e-01 (5.4850e-01)	Acc@1  81.25 ( 81.11)	Acc@5  98.44 ( 99.02)
Epoch: [11][190/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.6396e-01 (5.4824e-01)	Acc@1  79.69 ( 81.13)	Acc@5  99.22 ( 99.02)
Epoch: [11][200/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.8281e-01 (5.4758e-01)	Acc@1  85.16 ( 81.16)	Acc@5 100.00 ( 99.02)
Epoch: [11][210/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.9341e-01 (5.4382e-01)	Acc@1  83.59 ( 81.29)	Acc@5 100.00 ( 99.04)
Epoch: [11][220/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.0781e-01 (5.4359e-01)	Acc@1  82.81 ( 81.30)	Acc@5 100.00 ( 99.05)
Epoch: [11][230/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.1963e-01 (5.4453e-01)	Acc@1  79.69 ( 81.26)	Acc@5  99.22 ( 99.05)
Epoch: [11][240/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.7388e-01 (5.4441e-01)	Acc@1  88.28 ( 81.31)	Acc@5  97.66 ( 99.03)
Epoch: [11][250/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.2212e-01 (5.4328e-01)	Acc@1  87.50 ( 81.34)	Acc@5  99.22 ( 99.03)
Epoch: [11][260/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.9375e-01 (5.4466e-01)	Acc@1  77.34 ( 81.25)	Acc@5 100.00 ( 99.03)
Epoch: [11][270/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.0488e-01 (5.4564e-01)	Acc@1  82.81 ( 81.24)	Acc@5  99.22 ( 99.03)
Epoch: [11][280/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.0547e-01 (5.4506e-01)	Acc@1  82.81 ( 81.31)	Acc@5  98.44 ( 99.02)
Epoch: [11][290/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.0078e-01 (5.4597e-01)	Acc@1  72.66 ( 81.27)	Acc@5  99.22 ( 99.02)
Epoch: [11][300/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.7803e-01 (5.4564e-01)	Acc@1  82.81 ( 81.27)	Acc@5 100.00 ( 99.02)
Epoch: [11][310/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.0830e-01 (5.4616e-01)	Acc@1  83.59 ( 81.25)	Acc@5  97.66 ( 99.02)
Epoch: [11][320/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.0293e-01 (5.4553e-01)	Acc@1  83.59 ( 81.29)	Acc@5 100.00 ( 99.02)
Epoch: [11][330/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.8154e-01 (5.4781e-01)	Acc@1  80.47 ( 81.19)	Acc@5  99.22 ( 99.01)
Epoch: [11][340/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.0742e-01 (5.4672e-01)	Acc@1  79.69 ( 81.23)	Acc@5  99.22 ( 99.01)
Epoch: [11][350/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.8154e-01 (5.4806e-01)	Acc@1  75.78 ( 81.17)	Acc@5 100.00 ( 99.02)
Epoch: [11][360/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.8057e-01 (5.4869e-01)	Acc@1  76.56 ( 81.15)	Acc@5  99.22 ( 99.03)
Epoch: [11][370/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.4004e-01 (5.4923e-01)	Acc@1  83.59 ( 81.14)	Acc@5 100.00 ( 99.03)
Epoch: [11][380/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.6982e-01 (5.4875e-01)	Acc@1  79.69 ( 81.10)	Acc@5  96.88 ( 99.03)
Epoch: [11][390/391]	Time  0.045 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.6416e-01 (5.4979e-01)	Acc@1  72.50 ( 81.06)	Acc@5  97.50 ( 99.03)
## e[11] optimizer.zero_grad (sum) time: 0.38718104362487793
## e[11]       loss.backward (sum) time: 7.266253471374512
## e[11]      optimizer.step (sum) time: 3.4196548461914062
## epoch[11] training(only) time: 25.52397108078003
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 5.9863e-01 (5.9863e-01)	Acc@1  81.00 ( 81.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.024 ( 0.040)	Loss 5.4248e-01 (5.8685e-01)	Acc@1  81.00 ( 79.64)	Acc@5  98.00 ( 99.00)
Test: [ 20/100]	Time  0.024 ( 0.032)	Loss 6.6309e-01 (5.9063e-01)	Acc@1  77.00 ( 79.33)	Acc@5  99.00 ( 98.71)
Test: [ 30/100]	Time  0.027 ( 0.030)	Loss 7.1094e-01 (6.1292e-01)	Acc@1  75.00 ( 78.90)	Acc@5  98.00 ( 98.61)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 7.1631e-01 (6.1899e-01)	Acc@1  80.00 ( 78.93)	Acc@5  96.00 ( 98.56)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 6.6748e-01 (6.1276e-01)	Acc@1  83.00 ( 79.51)	Acc@5  97.00 ( 98.47)
Test: [ 60/100]	Time  0.024 ( 0.027)	Loss 6.4014e-01 (6.1230e-01)	Acc@1  81.00 ( 79.57)	Acc@5  99.00 ( 98.64)
Test: [ 70/100]	Time  0.024 ( 0.027)	Loss 7.0654e-01 (6.1044e-01)	Acc@1  75.00 ( 79.55)	Acc@5  98.00 ( 98.69)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 5.1367e-01 (6.0441e-01)	Acc@1  87.00 ( 79.78)	Acc@5  99.00 ( 98.72)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 5.8154e-01 (6.0128e-01)	Acc@1  79.00 ( 79.75)	Acc@5 100.00 ( 98.67)
 * Acc@1 79.780 Acc@5 98.630
### epoch[11] execution time: 28.284098863601685
EPOCH 12
# current learning rate: 0.1
# Switched to train mode...
Epoch: [12][  0/391]	Time  0.245 ( 0.245)	Data  0.181 ( 0.181)	Loss 5.2246e-01 (5.2246e-01)	Acc@1  82.81 ( 82.81)	Acc@5  98.44 ( 98.44)
Epoch: [12][ 10/391]	Time  0.072 ( 0.081)	Data  0.001 ( 0.017)	Loss 5.9912e-01 (5.5344e-01)	Acc@1  76.56 ( 80.47)	Acc@5  99.22 ( 99.08)
Epoch: [12][ 20/391]	Time  0.064 ( 0.074)	Data  0.001 ( 0.010)	Loss 6.5430e-01 (5.3177e-01)	Acc@1  76.56 ( 81.47)	Acc@5  99.22 ( 99.22)
Epoch: [12][ 30/391]	Time  0.062 ( 0.071)	Data  0.001 ( 0.007)	Loss 4.6313e-01 (5.2485e-01)	Acc@1  80.47 ( 81.58)	Acc@5 100.00 ( 99.07)
Epoch: [12][ 40/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.005)	Loss 4.7485e-01 (5.3516e-01)	Acc@1  85.16 ( 81.59)	Acc@5  98.44 ( 99.05)
Epoch: [12][ 50/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.005)	Loss 4.9878e-01 (5.3113e-01)	Acc@1  82.03 ( 81.62)	Acc@5  98.44 ( 99.02)
Epoch: [12][ 60/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.004)	Loss 5.0732e-01 (5.2942e-01)	Acc@1  82.03 ( 81.67)	Acc@5 100.00 ( 99.12)
Epoch: [12][ 70/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.004)	Loss 5.1270e-01 (5.2912e-01)	Acc@1  83.59 ( 81.72)	Acc@5  97.66 ( 99.11)
Epoch: [12][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 5.8691e-01 (5.2952e-01)	Acc@1  81.25 ( 81.66)	Acc@5 100.00 ( 99.15)
Epoch: [12][ 90/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.1309e-01 (5.3093e-01)	Acc@1  85.94 ( 81.64)	Acc@5  99.22 ( 99.16)
Epoch: [12][100/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.003)	Loss 5.3027e-01 (5.3130e-01)	Acc@1  83.59 ( 81.64)	Acc@5  99.22 ( 99.19)
Epoch: [12][110/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.9707e-01 (5.3050e-01)	Acc@1  82.81 ( 81.60)	Acc@5  98.44 ( 99.22)
Epoch: [12][120/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.003)	Loss 5.3809e-01 (5.2919e-01)	Acc@1  78.12 ( 81.61)	Acc@5  98.44 ( 99.21)
Epoch: [12][130/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.0889e-01 (5.2867e-01)	Acc@1  80.47 ( 81.57)	Acc@5  97.66 ( 99.22)
Epoch: [12][140/391]	Time  0.078 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.4395e-01 (5.2942e-01)	Acc@1  81.25 ( 81.52)	Acc@5  99.22 ( 99.24)
Epoch: [12][150/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.3433e-01 (5.2592e-01)	Acc@1  82.81 ( 81.59)	Acc@5  99.22 ( 99.27)
Epoch: [12][160/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.3921e-01 (5.2609e-01)	Acc@1  88.28 ( 81.55)	Acc@5  99.22 ( 99.26)
Epoch: [12][170/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.5957e-01 (5.2553e-01)	Acc@1  82.03 ( 81.56)	Acc@5  98.44 ( 99.26)
Epoch: [12][180/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.9756e-01 (5.2673e-01)	Acc@1  82.03 ( 81.52)	Acc@5  99.22 ( 99.27)
Epoch: [12][190/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.7510e-01 (5.2546e-01)	Acc@1  85.16 ( 81.61)	Acc@5  98.44 ( 99.27)
Epoch: [12][200/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.7744e-01 (5.2392e-01)	Acc@1  83.59 ( 81.65)	Acc@5 100.00 ( 99.26)
Epoch: [12][210/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.0977e-01 (5.2393e-01)	Acc@1  79.69 ( 81.64)	Acc@5 100.00 ( 99.24)
Epoch: [12][220/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.7095e-01 (5.2442e-01)	Acc@1  83.59 ( 81.62)	Acc@5  98.44 ( 99.24)
Epoch: [12][230/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.1768e-01 (5.2447e-01)	Acc@1  78.12 ( 81.62)	Acc@5  98.44 ( 99.22)
Epoch: [12][240/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.4238e-01 (5.2415e-01)	Acc@1  86.72 ( 81.67)	Acc@5  99.22 ( 99.22)
Epoch: [12][250/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.9756e-01 (5.2224e-01)	Acc@1  83.59 ( 81.73)	Acc@5  99.22 ( 99.22)
Epoch: [12][260/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.5566e-01 (5.2270e-01)	Acc@1  79.69 ( 81.67)	Acc@5  98.44 ( 99.21)
Epoch: [12][270/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.4316e-01 (5.2356e-01)	Acc@1  77.34 ( 81.66)	Acc@5  98.44 ( 99.21)
Epoch: [12][280/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.0449e-01 (5.2466e-01)	Acc@1  81.25 ( 81.68)	Acc@5  99.22 ( 99.20)
Epoch: [12][290/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.8604e-01 (5.2388e-01)	Acc@1  77.34 ( 81.71)	Acc@5  98.44 ( 99.21)
Epoch: [12][300/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.5532e-01 (5.2436e-01)	Acc@1  82.03 ( 81.72)	Acc@5 100.00 ( 99.21)
Epoch: [12][310/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.1602e-01 (5.2457e-01)	Acc@1  84.38 ( 81.70)	Acc@5 100.00 ( 99.21)
Epoch: [12][320/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.9863e-01 (5.2443e-01)	Acc@1  82.81 ( 81.72)	Acc@5  99.22 ( 99.21)
Epoch: [12][330/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.9209e-01 (5.2317e-01)	Acc@1  85.16 ( 81.77)	Acc@5  99.22 ( 99.21)
Epoch: [12][340/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.5625e-01 (5.2303e-01)	Acc@1  79.69 ( 81.79)	Acc@5  96.88 ( 99.21)
Epoch: [12][350/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.8169e-01 (5.2243e-01)	Acc@1  81.25 ( 81.84)	Acc@5  98.44 ( 99.21)
Epoch: [12][360/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.1377e-01 (5.2401e-01)	Acc@1  82.03 ( 81.81)	Acc@5 100.00 ( 99.21)
Epoch: [12][370/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.7676e-01 (5.2442e-01)	Acc@1  79.69 ( 81.80)	Acc@5  95.31 ( 99.19)
Epoch: [12][380/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.3809e-01 (5.2428e-01)	Acc@1  82.81 ( 81.82)	Acc@5 100.00 ( 99.18)
Epoch: [12][390/391]	Time  0.047 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.1870e-01 (5.2465e-01)	Acc@1  85.00 ( 81.81)	Acc@5 100.00 ( 99.18)
## e[12] optimizer.zero_grad (sum) time: 0.38781189918518066
## e[12]       loss.backward (sum) time: 7.219914436340332
## e[12]      optimizer.step (sum) time: 3.4533278942108154
## epoch[12] training(only) time: 25.5437114238739
# Switched to evaluate mode...
Test: [  0/100]	Time  0.191 ( 0.191)	Loss 6.5186e-01 (6.5186e-01)	Acc@1  82.00 ( 82.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.024 ( 0.041)	Loss 4.8218e-01 (5.4468e-01)	Acc@1  84.00 ( 81.00)	Acc@5 100.00 ( 99.45)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 5.3027e-01 (5.6135e-01)	Acc@1  79.00 ( 80.67)	Acc@5  99.00 ( 99.10)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 6.2695e-01 (5.7663e-01)	Acc@1  79.00 ( 80.16)	Acc@5 100.00 ( 98.90)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 5.0342e-01 (5.7515e-01)	Acc@1  82.00 ( 80.24)	Acc@5 100.00 ( 98.90)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 5.2051e-01 (5.7213e-01)	Acc@1  82.00 ( 80.27)	Acc@5  99.00 ( 98.80)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 6.2256e-01 (5.7141e-01)	Acc@1  79.00 ( 80.34)	Acc@5 100.00 ( 98.89)
Test: [ 70/100]	Time  0.024 ( 0.027)	Loss 6.6162e-01 (5.7348e-01)	Acc@1  76.00 ( 80.25)	Acc@5  99.00 ( 98.89)
Test: [ 80/100]	Time  0.030 ( 0.027)	Loss 5.2393e-01 (5.7286e-01)	Acc@1  80.00 ( 80.27)	Acc@5 100.00 ( 98.90)
Test: [ 90/100]	Time  0.027 ( 0.027)	Loss 5.1807e-01 (5.6954e-01)	Acc@1  78.00 ( 80.31)	Acc@5  99.00 ( 98.93)
 * Acc@1 80.450 Acc@5 98.910
### epoch[12] execution time: 28.314363479614258
EPOCH 13
# current learning rate: 0.1
# Switched to train mode...
Epoch: [13][  0/391]	Time  0.240 ( 0.240)	Data  0.175 ( 0.175)	Loss 5.1709e-01 (5.1709e-01)	Acc@1  78.91 ( 78.91)	Acc@5  99.22 ( 99.22)
Epoch: [13][ 10/391]	Time  0.066 ( 0.081)	Data  0.001 ( 0.017)	Loss 4.7070e-01 (4.8846e-01)	Acc@1  82.03 ( 83.17)	Acc@5  99.22 ( 99.50)
Epoch: [13][ 20/391]	Time  0.064 ( 0.073)	Data  0.001 ( 0.009)	Loss 4.9780e-01 (4.7264e-01)	Acc@1  82.81 ( 83.59)	Acc@5  98.44 ( 99.44)
Epoch: [13][ 30/391]	Time  0.063 ( 0.070)	Data  0.001 ( 0.007)	Loss 5.4785e-01 (4.9632e-01)	Acc@1  78.91 ( 82.56)	Acc@5 100.00 ( 99.34)
Epoch: [13][ 40/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.005)	Loss 4.2578e-01 (4.9511e-01)	Acc@1  85.16 ( 83.00)	Acc@5 100.00 ( 99.28)
Epoch: [13][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 5.6836e-01 (4.9863e-01)	Acc@1  83.59 ( 82.84)	Acc@5  99.22 ( 99.28)
Epoch: [13][ 60/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.004)	Loss 5.0000e-01 (5.0078e-01)	Acc@1  80.47 ( 82.52)	Acc@5  98.44 ( 99.30)
Epoch: [13][ 70/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.004)	Loss 6.0596e-01 (4.9723e-01)	Acc@1  75.78 ( 82.55)	Acc@5  99.22 ( 99.33)
Epoch: [13][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.2920e-01 (4.9689e-01)	Acc@1  89.84 ( 82.74)	Acc@5 100.00 ( 99.32)
Epoch: [13][ 90/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 5.3760e-01 (5.0064e-01)	Acc@1  79.69 ( 82.62)	Acc@5 100.00 ( 99.28)
Epoch: [13][100/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.8013e-01 (4.9889e-01)	Acc@1  88.28 ( 82.71)	Acc@5  99.22 ( 99.25)
Epoch: [13][110/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.9736e-01 (4.9773e-01)	Acc@1  89.06 ( 82.79)	Acc@5 100.00 ( 99.24)
Epoch: [13][120/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.9023e-01 (4.9595e-01)	Acc@1  83.59 ( 82.92)	Acc@5  99.22 ( 99.20)
Epoch: [13][130/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.9805e-01 (4.9729e-01)	Acc@1  82.81 ( 82.81)	Acc@5  99.22 ( 99.18)
Epoch: [13][140/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.5190e-01 (4.9901e-01)	Acc@1  84.38 ( 82.75)	Acc@5  99.22 ( 99.19)
Epoch: [13][150/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.3271e-01 (5.0051e-01)	Acc@1  80.47 ( 82.71)	Acc@5  99.22 ( 99.17)
Epoch: [13][160/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.4258e-01 (5.0165e-01)	Acc@1  81.25 ( 82.65)	Acc@5  97.66 ( 99.15)
Epoch: [13][170/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.7607e-01 (5.0326e-01)	Acc@1  80.47 ( 82.56)	Acc@5  99.22 ( 99.13)
Epoch: [13][180/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.4312e-01 (5.0506e-01)	Acc@1  83.59 ( 82.45)	Acc@5  99.22 ( 99.11)
Epoch: [13][190/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.7207e-01 (5.0456e-01)	Acc@1  85.94 ( 82.48)	Acc@5  99.22 ( 99.11)
Epoch: [13][200/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.0977e-01 (5.0565e-01)	Acc@1  81.25 ( 82.50)	Acc@5 100.00 ( 99.11)
Epoch: [13][210/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.1274e-01 (5.0363e-01)	Acc@1  87.50 ( 82.56)	Acc@5  99.22 ( 99.12)
Epoch: [13][220/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.6899e-01 (5.0423e-01)	Acc@1  85.94 ( 82.49)	Acc@5  97.66 ( 99.11)
Epoch: [13][230/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.4932e-01 (5.0460e-01)	Acc@1  82.81 ( 82.48)	Acc@5  96.88 ( 99.12)
Epoch: [13][240/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.8789e-01 (5.0428e-01)	Acc@1  78.91 ( 82.51)	Acc@5  99.22 ( 99.13)
Epoch: [13][250/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.4590e-01 (5.0550e-01)	Acc@1  79.69 ( 82.52)	Acc@5 100.00 ( 99.13)
Epoch: [13][260/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.7144e-01 (5.0633e-01)	Acc@1  82.03 ( 82.47)	Acc@5  99.22 ( 99.14)
Epoch: [13][270/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.6606e-01 (5.0538e-01)	Acc@1  85.94 ( 82.52)	Acc@5 100.00 ( 99.16)
Epoch: [13][280/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.1064e-01 (5.0524e-01)	Acc@1  85.16 ( 82.60)	Acc@5  99.22 ( 99.16)
Epoch: [13][290/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.2725e-01 (5.0476e-01)	Acc@1  81.25 ( 82.61)	Acc@5 100.00 ( 99.16)
Epoch: [13][300/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.9585e-01 (5.0562e-01)	Acc@1  82.81 ( 82.61)	Acc@5 100.00 ( 99.17)
Epoch: [13][310/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.1416e-01 (5.0657e-01)	Acc@1  82.03 ( 82.57)	Acc@5  97.66 ( 99.16)
Epoch: [13][320/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.7041e-01 (5.0810e-01)	Acc@1  75.00 ( 82.52)	Acc@5  96.88 ( 99.16)
Epoch: [13][330/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.5283e-01 (5.0861e-01)	Acc@1  81.25 ( 82.56)	Acc@5  96.88 ( 99.14)
Epoch: [13][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.3174e-01 (5.0862e-01)	Acc@1  82.03 ( 82.54)	Acc@5 100.00 ( 99.15)
Epoch: [13][350/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.5957e-01 (5.0753e-01)	Acc@1  81.25 ( 82.60)	Acc@5  99.22 ( 99.15)
Epoch: [13][360/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.5322e-01 (5.0722e-01)	Acc@1  78.91 ( 82.59)	Acc@5  99.22 ( 99.15)
Epoch: [13][370/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.9277e-01 (5.0855e-01)	Acc@1  78.91 ( 82.52)	Acc@5 100.00 ( 99.15)
Epoch: [13][380/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.5435e-01 (5.0733e-01)	Acc@1  81.25 ( 82.55)	Acc@5 100.00 ( 99.16)
Epoch: [13][390/391]	Time  0.053 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.2666e-01 (5.0796e-01)	Acc@1  87.50 ( 82.54)	Acc@5 100.00 ( 99.15)
## e[13] optimizer.zero_grad (sum) time: 0.38481664657592773
## e[13]       loss.backward (sum) time: 7.223834276199341
## e[13]      optimizer.step (sum) time: 3.5073843002319336
## epoch[13] training(only) time: 25.564537286758423
# Switched to evaluate mode...
Test: [  0/100]	Time  0.182 ( 0.182)	Loss 6.0400e-01 (6.0400e-01)	Acc@1  76.00 ( 76.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.024 ( 0.040)	Loss 4.7949e-01 (5.5427e-01)	Acc@1  87.00 ( 81.36)	Acc@5 100.00 ( 99.55)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 6.2256e-01 (5.6423e-01)	Acc@1  78.00 ( 81.00)	Acc@5  99.00 ( 99.24)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 6.0596e-01 (5.6522e-01)	Acc@1  77.00 ( 80.84)	Acc@5 100.00 ( 99.10)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 5.7959e-01 (5.7136e-01)	Acc@1  81.00 ( 80.73)	Acc@5  97.00 ( 98.95)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 6.4258e-01 (5.7078e-01)	Acc@1  78.00 ( 80.63)	Acc@5  99.00 ( 98.92)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 6.3721e-01 (5.8168e-01)	Acc@1  77.00 ( 80.44)	Acc@5 100.00 ( 98.93)
Test: [ 70/100]	Time  0.024 ( 0.027)	Loss 6.4062e-01 (5.8099e-01)	Acc@1  75.00 ( 80.41)	Acc@5 100.00 ( 98.97)
Test: [ 80/100]	Time  0.028 ( 0.027)	Loss 4.9121e-01 (5.7886e-01)	Acc@1  80.00 ( 80.42)	Acc@5  98.00 ( 98.98)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 3.5303e-01 (5.7500e-01)	Acc@1  86.00 ( 80.54)	Acc@5 100.00 ( 99.01)
 * Acc@1 80.650 Acc@5 98.990
### epoch[13] execution time: 28.35869574546814
EPOCH 14
# current learning rate: 0.1
# Switched to train mode...
Epoch: [14][  0/391]	Time  0.252 ( 0.252)	Data  0.188 ( 0.188)	Loss 4.8950e-01 (4.8950e-01)	Acc@1  81.25 ( 81.25)	Acc@5 100.00 (100.00)
Epoch: [14][ 10/391]	Time  0.063 ( 0.081)	Data  0.001 ( 0.018)	Loss 4.5239e-01 (5.2579e-01)	Acc@1  83.59 ( 82.17)	Acc@5 100.00 ( 99.22)
Epoch: [14][ 20/391]	Time  0.064 ( 0.073)	Data  0.001 ( 0.010)	Loss 4.3164e-01 (5.1403e-01)	Acc@1  85.16 ( 82.29)	Acc@5 100.00 ( 99.22)
Epoch: [14][ 30/391]	Time  0.065 ( 0.071)	Data  0.001 ( 0.007)	Loss 3.8379e-01 (4.9003e-01)	Acc@1  86.72 ( 83.37)	Acc@5 100.00 ( 99.24)
Epoch: [14][ 40/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.006)	Loss 4.0234e-01 (4.7838e-01)	Acc@1  85.16 ( 83.37)	Acc@5 100.00 ( 99.39)
Epoch: [14][ 50/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.005)	Loss 3.8477e-01 (4.7768e-01)	Acc@1  88.28 ( 83.50)	Acc@5  99.22 ( 99.36)
Epoch: [14][ 60/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.004)	Loss 6.7676e-01 (4.8433e-01)	Acc@1  77.34 ( 83.22)	Acc@5  98.44 ( 99.36)
Epoch: [14][ 70/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.004)	Loss 5.5859e-01 (4.9098e-01)	Acc@1  77.34 ( 82.80)	Acc@5  99.22 ( 99.35)
Epoch: [14][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 6.2988e-01 (4.9429e-01)	Acc@1  78.12 ( 82.70)	Acc@5  96.88 ( 99.29)
Epoch: [14][ 90/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 5.0830e-01 (4.9981e-01)	Acc@1  82.03 ( 82.60)	Acc@5  99.22 ( 99.21)
Epoch: [14][100/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 5.0830e-01 (4.9781e-01)	Acc@1  85.16 ( 82.76)	Acc@5 100.00 ( 99.20)
Epoch: [14][110/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.3530e-01 (4.9558e-01)	Acc@1  86.72 ( 82.84)	Acc@5  99.22 ( 99.19)
Epoch: [14][120/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.003)	Loss 5.1514e-01 (4.9692e-01)	Acc@1  82.03 ( 82.81)	Acc@5 100.00 ( 99.17)
Epoch: [14][130/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.5137e-01 (4.9902e-01)	Acc@1  76.56 ( 82.72)	Acc@5  99.22 ( 99.17)
Epoch: [14][140/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.5029e-01 (4.9982e-01)	Acc@1  78.91 ( 82.61)	Acc@5 100.00 ( 99.19)
Epoch: [14][150/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.1758e-01 (4.9963e-01)	Acc@1  81.25 ( 82.59)	Acc@5  99.22 ( 99.20)
Epoch: [14][160/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.6191e-01 (5.0139e-01)	Acc@1  85.16 ( 82.54)	Acc@5  99.22 ( 99.16)
Epoch: [14][170/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.6387e-01 (5.0091e-01)	Acc@1  85.16 ( 82.55)	Acc@5 100.00 ( 99.18)
Epoch: [14][180/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.8354e-01 (5.0088e-01)	Acc@1  85.94 ( 82.52)	Acc@5 100.00 ( 99.18)
Epoch: [14][190/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.4116e-01 (5.0208e-01)	Acc@1  85.16 ( 82.49)	Acc@5 100.00 ( 99.20)
Epoch: [14][200/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.6836e-01 (5.0216e-01)	Acc@1  78.12 ( 82.45)	Acc@5 100.00 ( 99.20)
Epoch: [14][210/391]	Time  0.077 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.0195e-01 (5.0227e-01)	Acc@1  85.16 ( 82.49)	Acc@5  98.44 ( 99.20)
Epoch: [14][220/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.4336e-01 (5.0219e-01)	Acc@1  84.38 ( 82.52)	Acc@5  99.22 ( 99.19)
Epoch: [14][230/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.5967e-01 (5.0132e-01)	Acc@1  75.78 ( 82.55)	Acc@5  98.44 ( 99.19)
Epoch: [14][240/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.2549e-01 (5.0147e-01)	Acc@1  79.69 ( 82.60)	Acc@5 100.00 ( 99.21)
Epoch: [14][250/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.9121e-01 (5.0199e-01)	Acc@1  80.47 ( 82.59)	Acc@5  99.22 ( 99.20)
Epoch: [14][260/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.4102e-01 (5.0200e-01)	Acc@1  80.47 ( 82.58)	Acc@5  98.44 ( 99.19)
Epoch: [14][270/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.7754e-01 (5.0135e-01)	Acc@1  82.81 ( 82.63)	Acc@5 100.00 ( 99.19)
Epoch: [14][280/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.6982e-01 (5.0163e-01)	Acc@1  78.12 ( 82.62)	Acc@5  97.66 ( 99.20)
Epoch: [14][290/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.2881e-01 (5.0259e-01)	Acc@1  81.25 ( 82.59)	Acc@5  99.22 ( 99.19)
Epoch: [14][300/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.6055e-01 (5.0254e-01)	Acc@1  80.47 ( 82.57)	Acc@5 100.00 ( 99.21)
Epoch: [14][310/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.1670e-01 (5.0340e-01)	Acc@1  77.34 ( 82.54)	Acc@5 100.00 ( 99.19)
Epoch: [14][320/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.0195e-01 (5.0259e-01)	Acc@1  79.69 ( 82.54)	Acc@5  99.22 ( 99.21)
Epoch: [14][330/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.2920e-01 (5.0358e-01)	Acc@1  84.38 ( 82.51)	Acc@5  99.22 ( 99.19)
Epoch: [14][340/391]	Time  0.070 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.8501e-01 (5.0264e-01)	Acc@1  87.50 ( 82.55)	Acc@5  99.22 ( 99.19)
Epoch: [14][350/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.5435e-01 (5.0219e-01)	Acc@1  82.81 ( 82.56)	Acc@5  99.22 ( 99.19)
Epoch: [14][360/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.2773e-01 (5.0053e-01)	Acc@1  85.94 ( 82.64)	Acc@5 100.00 ( 99.19)
Epoch: [14][370/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.1992e-01 (5.0000e-01)	Acc@1  86.72 ( 82.66)	Acc@5  98.44 ( 99.19)
Epoch: [14][380/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.4678e-01 (4.9918e-01)	Acc@1  85.16 ( 82.67)	Acc@5 100.00 ( 99.20)
Epoch: [14][390/391]	Time  0.047 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.8594e-01 (4.9911e-01)	Acc@1  82.50 ( 82.68)	Acc@5 100.00 ( 99.20)
## e[14] optimizer.zero_grad (sum) time: 0.3975796699523926
## e[14]       loss.backward (sum) time: 7.203952312469482
## e[14]      optimizer.step (sum) time: 3.474217176437378
## epoch[14] training(only) time: 25.58653426170349
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 5.9473e-01 (5.9473e-01)	Acc@1  75.00 ( 75.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.024 ( 0.039)	Loss 5.5176e-01 (6.3177e-01)	Acc@1  82.00 ( 78.73)	Acc@5  98.00 ( 98.73)
Test: [ 20/100]	Time  0.024 ( 0.032)	Loss 6.5918e-01 (6.2444e-01)	Acc@1  77.00 ( 79.24)	Acc@5  99.00 ( 98.71)
Test: [ 30/100]	Time  0.027 ( 0.030)	Loss 8.0762e-01 (6.3080e-01)	Acc@1  72.00 ( 79.23)	Acc@5  98.00 ( 98.65)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 6.8311e-01 (6.4345e-01)	Acc@1  73.00 ( 78.80)	Acc@5  99.00 ( 98.54)
Test: [ 50/100]	Time  0.028 ( 0.028)	Loss 5.8643e-01 (6.3794e-01)	Acc@1  81.00 ( 79.16)	Acc@5  99.00 ( 98.67)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 5.7764e-01 (6.4269e-01)	Acc@1  79.00 ( 78.87)	Acc@5 100.00 ( 98.79)
Test: [ 70/100]	Time  0.027 ( 0.027)	Loss 6.5088e-01 (6.3911e-01)	Acc@1  77.00 ( 78.97)	Acc@5  98.00 ( 98.79)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 5.3320e-01 (6.3327e-01)	Acc@1  82.00 ( 78.95)	Acc@5  99.00 ( 98.83)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 3.0249e-01 (6.2936e-01)	Acc@1  89.00 ( 79.05)	Acc@5  99.00 ( 98.80)
 * Acc@1 79.190 Acc@5 98.760
### epoch[14] execution time: 28.406184911727905
EPOCH 15
# current learning rate: 0.1
# Switched to train mode...
Epoch: [15][  0/391]	Time  0.244 ( 0.244)	Data  0.180 ( 0.180)	Loss 4.9512e-01 (4.9512e-01)	Acc@1  83.59 ( 83.59)	Acc@5 100.00 (100.00)
Epoch: [15][ 10/391]	Time  0.069 ( 0.082)	Data  0.001 ( 0.017)	Loss 5.6982e-01 (4.9035e-01)	Acc@1  78.91 ( 83.24)	Acc@5 100.00 ( 99.64)
Epoch: [15][ 20/391]	Time  0.066 ( 0.073)	Data  0.001 ( 0.010)	Loss 4.8926e-01 (5.0757e-01)	Acc@1  85.16 ( 82.85)	Acc@5  97.66 ( 99.26)
Epoch: [15][ 30/391]	Time  0.064 ( 0.071)	Data  0.001 ( 0.007)	Loss 6.4160e-01 (5.1125e-01)	Acc@1  78.91 ( 82.74)	Acc@5  97.66 ( 99.22)
Epoch: [15][ 40/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.005)	Loss 5.7959e-01 (5.1280e-01)	Acc@1  80.47 ( 82.58)	Acc@5  98.44 ( 99.10)
Epoch: [15][ 50/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.005)	Loss 4.6582e-01 (5.0894e-01)	Acc@1  86.72 ( 82.66)	Acc@5 100.00 ( 99.13)
Epoch: [15][ 60/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 6.0156e-01 (5.0642e-01)	Acc@1  78.12 ( 82.79)	Acc@5  99.22 ( 99.15)
Epoch: [15][ 70/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.004)	Loss 4.2725e-01 (5.0319e-01)	Acc@1  83.59 ( 82.85)	Acc@5 100.00 ( 99.22)
Epoch: [15][ 80/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.003)	Loss 5.1465e-01 (4.9967e-01)	Acc@1  82.03 ( 82.94)	Acc@5 100.00 ( 99.25)
Epoch: [15][ 90/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 6.5967e-01 (4.9719e-01)	Acc@1  75.78 ( 82.98)	Acc@5 100.00 ( 99.27)
Epoch: [15][100/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.003)	Loss 5.5420e-01 (4.9646e-01)	Acc@1  82.03 ( 82.97)	Acc@5 100.00 ( 99.33)
Epoch: [15][110/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.003)	Loss 5.6885e-01 (4.9368e-01)	Acc@1  78.91 ( 83.12)	Acc@5 100.00 ( 99.32)
Epoch: [15][120/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 5.9326e-01 (4.9568e-01)	Acc@1  82.81 ( 83.08)	Acc@5  98.44 ( 99.29)
Epoch: [15][130/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.0479e-01 (4.9534e-01)	Acc@1  87.50 ( 83.14)	Acc@5  99.22 ( 99.27)
Epoch: [15][140/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.0977e-01 (4.9447e-01)	Acc@1  82.03 ( 83.19)	Acc@5 100.00 ( 99.25)
Epoch: [15][150/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.9424e-01 (4.9413e-01)	Acc@1  82.81 ( 83.19)	Acc@5 100.00 ( 99.26)
Epoch: [15][160/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.6543e-01 (4.9269e-01)	Acc@1  78.91 ( 83.22)	Acc@5 100.00 ( 99.27)
Epoch: [15][170/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.0625e-01 (4.9073e-01)	Acc@1  87.50 ( 83.28)	Acc@5 100.00 ( 99.29)
Epoch: [15][180/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.0161e-01 (4.8858e-01)	Acc@1  86.72 ( 83.33)	Acc@5  99.22 ( 99.28)
Epoch: [15][190/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.0400e-01 (4.8962e-01)	Acc@1  77.34 ( 83.32)	Acc@5  98.44 ( 99.28)
Epoch: [15][200/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.9023e-01 (4.9119e-01)	Acc@1  82.81 ( 83.25)	Acc@5  98.44 ( 99.27)
Epoch: [15][210/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.3701e-01 (4.9016e-01)	Acc@1  83.59 ( 83.26)	Acc@5 100.00 ( 99.27)
Epoch: [15][220/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.3750e-01 (4.8891e-01)	Acc@1  82.03 ( 83.25)	Acc@5 100.00 ( 99.28)
Epoch: [15][230/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.1895e-01 (4.8830e-01)	Acc@1  83.59 ( 83.28)	Acc@5  98.44 ( 99.27)
Epoch: [15][240/391]	Time  0.060 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.3872e-01 (4.8887e-01)	Acc@1  81.25 ( 83.25)	Acc@5 100.00 ( 99.28)
Epoch: [15][250/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.3286e-01 (4.8842e-01)	Acc@1  83.59 ( 83.25)	Acc@5 100.00 ( 99.29)
Epoch: [15][260/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.8413e-01 (4.8874e-01)	Acc@1  84.38 ( 83.21)	Acc@5  97.66 ( 99.29)
Epoch: [15][270/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.0674e-01 (4.8855e-01)	Acc@1  85.16 ( 83.22)	Acc@5  99.22 ( 99.29)
Epoch: [15][280/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.9561e-01 (4.8872e-01)	Acc@1  81.25 ( 83.16)	Acc@5  98.44 ( 99.29)
Epoch: [15][290/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.9863e-01 (4.8895e-01)	Acc@1  82.03 ( 83.16)	Acc@5  99.22 ( 99.29)
Epoch: [15][300/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.8364e-01 (4.8803e-01)	Acc@1  79.69 ( 83.18)	Acc@5 100.00 ( 99.30)
Epoch: [15][310/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.9561e-01 (4.8696e-01)	Acc@1  78.91 ( 83.21)	Acc@5 100.00 ( 99.31)
Epoch: [15][320/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.2471e-01 (4.8670e-01)	Acc@1  88.28 ( 83.22)	Acc@5 100.00 ( 99.32)
Epoch: [15][330/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.6240e-01 (4.8675e-01)	Acc@1  85.16 ( 83.20)	Acc@5  99.22 ( 99.32)
Epoch: [15][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.8838e-01 (4.8654e-01)	Acc@1  82.03 ( 83.20)	Acc@5  99.22 ( 99.31)
Epoch: [15][350/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.8145e-01 (4.8565e-01)	Acc@1  82.81 ( 83.24)	Acc@5  98.44 ( 99.32)
Epoch: [15][360/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.4580e-01 (4.8502e-01)	Acc@1  82.03 ( 83.29)	Acc@5  99.22 ( 99.32)
Epoch: [15][370/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.0244e-01 (4.8596e-01)	Acc@1  82.81 ( 83.26)	Acc@5  97.66 ( 99.31)
Epoch: [15][380/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.5044e-01 (4.8606e-01)	Acc@1  84.38 ( 83.25)	Acc@5 100.00 ( 99.31)
Epoch: [15][390/391]	Time  0.048 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.3726e-01 (4.8615e-01)	Acc@1  83.75 ( 83.24)	Acc@5  98.75 ( 99.31)
## e[15] optimizer.zero_grad (sum) time: 0.3889596462249756
## e[15]       loss.backward (sum) time: 7.267880439758301
## e[15]      optimizer.step (sum) time: 3.4885478019714355
## epoch[15] training(only) time: 25.565163135528564
# Switched to evaluate mode...
Test: [  0/100]	Time  0.193 ( 0.193)	Loss 6.9141e-01 (6.9141e-01)	Acc@1  75.00 ( 75.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.024 ( 0.040)	Loss 7.7197e-01 (6.7476e-01)	Acc@1  76.00 ( 78.64)	Acc@5  98.00 ( 98.55)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 7.6758e-01 (6.6525e-01)	Acc@1  74.00 ( 78.62)	Acc@5  99.00 ( 98.57)
Test: [ 30/100]	Time  0.027 ( 0.030)	Loss 7.9443e-01 (6.8216e-01)	Acc@1  75.00 ( 78.06)	Acc@5  99.00 ( 98.55)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 6.7432e-01 (6.7701e-01)	Acc@1  79.00 ( 78.05)	Acc@5  96.00 ( 98.34)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 7.0557e-01 (6.6666e-01)	Acc@1  78.00 ( 78.43)	Acc@5  96.00 ( 98.35)
Test: [ 60/100]	Time  0.027 ( 0.028)	Loss 5.7373e-01 (6.6206e-01)	Acc@1  81.00 ( 78.43)	Acc@5 100.00 ( 98.46)
Test: [ 70/100]	Time  0.024 ( 0.027)	Loss 6.4111e-01 (6.5900e-01)	Acc@1  77.00 ( 78.28)	Acc@5  99.00 ( 98.51)
Test: [ 80/100]	Time  0.027 ( 0.027)	Loss 4.7974e-01 (6.5664e-01)	Acc@1  85.00 ( 78.41)	Acc@5  99.00 ( 98.54)
Test: [ 90/100]	Time  0.027 ( 0.027)	Loss 3.9697e-01 (6.5602e-01)	Acc@1  85.00 ( 78.27)	Acc@5 100.00 ( 98.53)
 * Acc@1 78.420 Acc@5 98.530
### epoch[15] execution time: 28.37212824821472
EPOCH 16
# current learning rate: 0.1
# Switched to train mode...
Epoch: [16][  0/391]	Time  0.250 ( 0.250)	Data  0.189 ( 0.189)	Loss 4.3994e-01 (4.3994e-01)	Acc@1  80.47 ( 80.47)	Acc@5  98.44 ( 98.44)
Epoch: [16][ 10/391]	Time  0.063 ( 0.081)	Data  0.001 ( 0.018)	Loss 6.0791e-01 (4.9576e-01)	Acc@1  77.34 ( 81.53)	Acc@5  99.22 ( 99.43)
Epoch: [16][ 20/391]	Time  0.062 ( 0.073)	Data  0.001 ( 0.010)	Loss 4.5239e-01 (4.8576e-01)	Acc@1  84.38 ( 82.33)	Acc@5 100.00 ( 99.48)
Epoch: [16][ 30/391]	Time  0.069 ( 0.070)	Data  0.001 ( 0.007)	Loss 5.2051e-01 (4.7133e-01)	Acc@1  81.25 ( 83.22)	Acc@5 100.00 ( 99.50)
Epoch: [16][ 40/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.006)	Loss 4.8901e-01 (4.7823e-01)	Acc@1  80.47 ( 83.00)	Acc@5  98.44 ( 99.35)
Epoch: [16][ 50/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.005)	Loss 3.5962e-01 (4.7078e-01)	Acc@1  89.06 ( 83.33)	Acc@5 100.00 ( 99.39)
Epoch: [16][ 60/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.004)	Loss 4.2188e-01 (4.7314e-01)	Acc@1  86.72 ( 83.36)	Acc@5  98.44 ( 99.36)
Epoch: [16][ 70/391]	Time  0.059 ( 0.067)	Data  0.001 ( 0.004)	Loss 5.0635e-01 (4.7688e-01)	Acc@1  83.59 ( 83.29)	Acc@5  97.66 ( 99.33)
Epoch: [16][ 80/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.8120e-01 (4.8081e-01)	Acc@1  81.25 ( 83.17)	Acc@5 100.00 ( 99.33)
Epoch: [16][ 90/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.3530e-01 (4.8086e-01)	Acc@1  88.28 ( 83.23)	Acc@5  98.44 ( 99.29)
Epoch: [16][100/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.5825e-01 (4.8088e-01)	Acc@1  82.81 ( 83.18)	Acc@5 100.00 ( 99.29)
Epoch: [16][110/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.5679e-01 (4.7718e-01)	Acc@1  85.16 ( 83.23)	Acc@5  98.44 ( 99.30)
Epoch: [16][120/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.6387e-01 (4.7108e-01)	Acc@1  79.69 ( 83.48)	Acc@5  99.22 ( 99.32)
Epoch: [16][130/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.1514e-01 (4.7021e-01)	Acc@1  80.47 ( 83.55)	Acc@5  99.22 ( 99.30)
Epoch: [16][140/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.8037e-01 (4.6990e-01)	Acc@1  88.28 ( 83.55)	Acc@5  99.22 ( 99.31)
Epoch: [16][150/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.6826e-01 (4.7023e-01)	Acc@1  84.38 ( 83.52)	Acc@5  99.22 ( 99.32)
Epoch: [16][160/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.7681e-01 (4.7216e-01)	Acc@1  83.59 ( 83.51)	Acc@5 100.00 ( 99.32)
Epoch: [16][170/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.4502e-01 (4.7453e-01)	Acc@1  79.69 ( 83.40)	Acc@5  96.88 ( 99.31)
Epoch: [16][180/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.9185e-01 (4.7341e-01)	Acc@1  88.28 ( 83.46)	Acc@5  99.22 ( 99.28)
Epoch: [16][190/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.6924e-01 (4.7378e-01)	Acc@1  83.59 ( 83.44)	Acc@5  99.22 ( 99.29)
Epoch: [16][200/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.2930e-01 (4.7292e-01)	Acc@1  83.59 ( 83.48)	Acc@5  96.88 ( 99.29)
Epoch: [16][210/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.2812e-01 (4.7176e-01)	Acc@1  91.41 ( 83.62)	Acc@5  99.22 ( 99.28)
Epoch: [16][220/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.6060e-01 (4.7230e-01)	Acc@1  86.72 ( 83.61)	Acc@5 100.00 ( 99.28)
Epoch: [16][230/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.6924e-01 (4.7296e-01)	Acc@1  85.94 ( 83.58)	Acc@5  99.22 ( 99.26)
Epoch: [16][240/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.7617e-01 (4.7406e-01)	Acc@1  82.03 ( 83.57)	Acc@5  99.22 ( 99.26)
Epoch: [16][250/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.8574e-01 (4.7394e-01)	Acc@1  88.28 ( 83.61)	Acc@5  99.22 ( 99.24)
Epoch: [16][260/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.9912e-01 (4.7354e-01)	Acc@1  81.25 ( 83.66)	Acc@5  97.66 ( 99.23)
Epoch: [16][270/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.9365e-01 (4.7578e-01)	Acc@1  81.25 ( 83.59)	Acc@5 100.00 ( 99.22)
Epoch: [16][280/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.4980e-01 (4.7582e-01)	Acc@1  78.91 ( 83.60)	Acc@5  99.22 ( 99.22)
Epoch: [16][290/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.0259e-01 (4.7612e-01)	Acc@1  87.50 ( 83.63)	Acc@5  99.22 ( 99.21)
Epoch: [16][300/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.8511e-01 (4.7557e-01)	Acc@1  85.16 ( 83.62)	Acc@5  96.88 ( 99.21)
Epoch: [16][310/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.2393e-01 (4.7548e-01)	Acc@1  85.94 ( 83.64)	Acc@5  99.22 ( 99.21)
Epoch: [16][320/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.2441e-01 (4.7520e-01)	Acc@1  78.12 ( 83.61)	Acc@5 100.00 ( 99.23)
Epoch: [16][330/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.4849e-01 (4.7513e-01)	Acc@1  85.94 ( 83.63)	Acc@5  98.44 ( 99.21)
Epoch: [16][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.8486e-01 (4.7608e-01)	Acc@1  82.81 ( 83.57)	Acc@5  98.44 ( 99.22)
Epoch: [16][350/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.5811e-01 (4.7518e-01)	Acc@1  84.38 ( 83.60)	Acc@5 100.00 ( 99.22)
Epoch: [16][360/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.6035e-01 (4.7541e-01)	Acc@1  89.06 ( 83.57)	Acc@5 100.00 ( 99.21)
Epoch: [16][370/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.3335e-01 (4.7612e-01)	Acc@1  86.72 ( 83.57)	Acc@5  99.22 ( 99.21)
Epoch: [16][380/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.7471e-01 (4.7657e-01)	Acc@1  78.91 ( 83.57)	Acc@5  99.22 ( 99.21)
Epoch: [16][390/391]	Time  0.047 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.0884e-01 (4.7726e-01)	Acc@1  95.00 ( 83.57)	Acc@5  98.75 ( 99.20)
## e[16] optimizer.zero_grad (sum) time: 0.3945963382720947
## e[16]       loss.backward (sum) time: 7.223407030105591
## e[16]      optimizer.step (sum) time: 3.431898832321167
## epoch[16] training(only) time: 25.525810718536377
# Switched to evaluate mode...
Test: [  0/100]	Time  0.191 ( 0.191)	Loss 5.9082e-01 (5.9082e-01)	Acc@1  79.00 ( 79.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.028 ( 0.041)	Loss 5.3320e-01 (5.7588e-01)	Acc@1  85.00 ( 81.09)	Acc@5  97.00 ( 99.00)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 6.2891e-01 (5.8182e-01)	Acc@1  77.00 ( 81.10)	Acc@5 100.00 ( 99.10)
Test: [ 30/100]	Time  0.028 ( 0.031)	Loss 6.9727e-01 (5.9569e-01)	Acc@1  74.00 ( 80.68)	Acc@5  98.00 ( 99.00)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 4.6899e-01 (5.9672e-01)	Acc@1  86.00 ( 80.56)	Acc@5  98.00 ( 99.00)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 5.5225e-01 (5.8713e-01)	Acc@1  81.00 ( 80.94)	Acc@5  99.00 ( 99.08)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 5.5029e-01 (5.9109e-01)	Acc@1  81.00 ( 80.82)	Acc@5 100.00 ( 99.13)
Test: [ 70/100]	Time  0.027 ( 0.028)	Loss 5.8398e-01 (5.8559e-01)	Acc@1  77.00 ( 81.00)	Acc@5  99.00 ( 99.15)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 4.2310e-01 (5.8222e-01)	Acc@1  85.00 ( 81.02)	Acc@5  99.00 ( 99.11)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 6.4795e-01 (5.8147e-01)	Acc@1  80.00 ( 80.99)	Acc@5 100.00 ( 99.04)
 * Acc@1 80.950 Acc@5 99.070
### epoch[16] execution time: 28.336652040481567
EPOCH 17
# current learning rate: 0.1
# Switched to train mode...
Epoch: [17][  0/391]	Time  0.244 ( 0.244)	Data  0.183 ( 0.183)	Loss 4.9023e-01 (4.9023e-01)	Acc@1  83.59 ( 83.59)	Acc@5  99.22 ( 99.22)
Epoch: [17][ 10/391]	Time  0.063 ( 0.080)	Data  0.001 ( 0.017)	Loss 5.0635e-01 (4.4127e-01)	Acc@1  83.59 ( 85.30)	Acc@5  98.44 ( 99.64)
Epoch: [17][ 20/391]	Time  0.063 ( 0.072)	Data  0.001 ( 0.010)	Loss 5.3564e-01 (4.4355e-01)	Acc@1  79.69 ( 84.90)	Acc@5 100.00 ( 99.67)
Epoch: [17][ 30/391]	Time  0.066 ( 0.070)	Data  0.001 ( 0.007)	Loss 5.0439e-01 (4.3495e-01)	Acc@1  82.03 ( 85.06)	Acc@5 100.00 ( 99.67)
Epoch: [17][ 40/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.005)	Loss 5.2637e-01 (4.4700e-01)	Acc@1  82.81 ( 84.66)	Acc@5  98.44 ( 99.45)
Epoch: [17][ 50/391]	Time  0.062 ( 0.068)	Data  0.001 ( 0.005)	Loss 5.6494e-01 (4.4412e-01)	Acc@1  81.25 ( 84.62)	Acc@5  99.22 ( 99.43)
Epoch: [17][ 60/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.004)	Loss 5.2881e-01 (4.4709e-01)	Acc@1  82.03 ( 84.46)	Acc@5  97.66 ( 99.42)
Epoch: [17][ 70/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.004)	Loss 4.5264e-01 (4.4928e-01)	Acc@1  82.81 ( 84.40)	Acc@5  99.22 ( 99.39)
Epoch: [17][ 80/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 5.5762e-01 (4.4954e-01)	Acc@1  78.91 ( 84.37)	Acc@5  99.22 ( 99.40)
Epoch: [17][ 90/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.5693e-01 (4.4918e-01)	Acc@1  87.50 ( 84.38)	Acc@5 100.00 ( 99.37)
Epoch: [17][100/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.7119e-01 (4.5234e-01)	Acc@1  86.72 ( 84.40)	Acc@5  98.44 ( 99.34)
Epoch: [17][110/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.1138e-01 (4.5458e-01)	Acc@1  85.94 ( 84.37)	Acc@5  99.22 ( 99.35)
Epoch: [17][120/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 5.9619e-01 (4.5837e-01)	Acc@1  78.91 ( 84.17)	Acc@5  98.44 ( 99.32)
Epoch: [17][130/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.9219e-01 (4.5944e-01)	Acc@1  82.81 ( 84.12)	Acc@5  99.22 ( 99.31)
Epoch: [17][140/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.5337e-01 (4.6095e-01)	Acc@1  86.72 ( 84.10)	Acc@5  99.22 ( 99.33)
Epoch: [17][150/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.8608e-01 (4.6265e-01)	Acc@1  82.81 ( 83.98)	Acc@5 100.00 ( 99.34)
Epoch: [17][160/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.6265e-01 (4.6482e-01)	Acc@1  85.16 ( 83.88)	Acc@5 100.00 ( 99.34)
Epoch: [17][170/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.0625e-01 (4.6349e-01)	Acc@1  86.72 ( 83.93)	Acc@5  98.44 ( 99.34)
Epoch: [17][180/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.3809e-01 (4.6308e-01)	Acc@1  83.59 ( 83.95)	Acc@5  99.22 ( 99.33)
Epoch: [17][190/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.0928e-01 (4.6498e-01)	Acc@1  82.03 ( 83.86)	Acc@5 100.00 ( 99.34)
Epoch: [17][200/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.5518e-01 (4.6731e-01)	Acc@1  82.81 ( 83.79)	Acc@5  98.44 ( 99.32)
Epoch: [17][210/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.2783e-01 (4.6917e-01)	Acc@1  79.69 ( 83.69)	Acc@5  99.22 ( 99.32)
Epoch: [17][220/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.3643e-01 (4.6841e-01)	Acc@1  89.84 ( 83.71)	Acc@5 100.00 ( 99.33)
Epoch: [17][230/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.8486e-01 (4.6765e-01)	Acc@1  80.47 ( 83.74)	Acc@5 100.00 ( 99.32)
Epoch: [17][240/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.7256e-01 (4.6937e-01)	Acc@1  86.72 ( 83.70)	Acc@5  99.22 ( 99.32)
Epoch: [17][250/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.1113e-01 (4.6949e-01)	Acc@1  84.38 ( 83.67)	Acc@5  99.22 ( 99.32)
Epoch: [17][260/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.8584e-01 (4.6807e-01)	Acc@1  83.59 ( 83.73)	Acc@5 100.00 ( 99.32)
Epoch: [17][270/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.4302e-01 (4.6824e-01)	Acc@1  87.50 ( 83.73)	Acc@5 100.00 ( 99.32)
Epoch: [17][280/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.3496e-01 (4.6815e-01)	Acc@1  89.84 ( 83.70)	Acc@5 100.00 ( 99.33)
Epoch: [17][290/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.5532e-01 (4.6851e-01)	Acc@1  85.94 ( 83.71)	Acc@5  99.22 ( 99.33)
Epoch: [17][300/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.1187e-01 (4.6827e-01)	Acc@1  85.94 ( 83.72)	Acc@5  99.22 ( 99.32)
Epoch: [17][310/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.8794e-01 (4.6863e-01)	Acc@1  88.28 ( 83.72)	Acc@5 100.00 ( 99.33)
Epoch: [17][320/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.0303e-01 (4.6771e-01)	Acc@1  74.22 ( 83.73)	Acc@5  99.22 ( 99.34)
Epoch: [17][330/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.1953e-01 (4.6743e-01)	Acc@1  80.47 ( 83.74)	Acc@5 100.00 ( 99.34)
Epoch: [17][340/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.9287e-01 (4.6760e-01)	Acc@1  76.56 ( 83.75)	Acc@5 100.00 ( 99.34)
Epoch: [17][350/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.5923e-01 (4.6812e-01)	Acc@1  85.16 ( 83.75)	Acc@5 100.00 ( 99.34)
Epoch: [17][360/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.4775e-01 (4.6823e-01)	Acc@1  84.38 ( 83.78)	Acc@5 100.00 ( 99.34)
Epoch: [17][370/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.8950e-01 (4.6714e-01)	Acc@1  84.38 ( 83.82)	Acc@5  99.22 ( 99.35)
Epoch: [17][380/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.2676e-01 (4.6596e-01)	Acc@1  85.94 ( 83.89)	Acc@5  98.44 ( 99.34)
Epoch: [17][390/391]	Time  0.047 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.3115e-01 (4.6568e-01)	Acc@1  83.75 ( 83.89)	Acc@5 100.00 ( 99.34)
## e[17] optimizer.zero_grad (sum) time: 0.3957407474517822
## e[17]       loss.backward (sum) time: 7.2207560539245605
## e[17]      optimizer.step (sum) time: 3.4169883728027344
## epoch[17] training(only) time: 25.4730064868927
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 5.1172e-01 (5.1172e-01)	Acc@1  85.00 ( 85.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.039)	Loss 6.2842e-01 (5.5633e-01)	Acc@1  85.00 ( 81.55)	Acc@5  96.00 ( 98.91)
Test: [ 20/100]	Time  0.025 ( 0.032)	Loss 6.9678e-01 (5.6655e-01)	Acc@1  79.00 ( 80.90)	Acc@5  99.00 ( 99.10)
Test: [ 30/100]	Time  0.027 ( 0.030)	Loss 6.1963e-01 (5.7352e-01)	Acc@1  78.00 ( 80.68)	Acc@5  98.00 ( 99.13)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 5.4883e-01 (5.7334e-01)	Acc@1  85.00 ( 80.90)	Acc@5  98.00 ( 99.05)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 3.7769e-01 (5.6978e-01)	Acc@1  90.00 ( 80.94)	Acc@5  98.00 ( 98.98)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 5.9570e-01 (5.6873e-01)	Acc@1  85.00 ( 81.11)	Acc@5  99.00 ( 99.02)
Test: [ 70/100]	Time  0.024 ( 0.027)	Loss 6.9531e-01 (5.6538e-01)	Acc@1  80.00 ( 81.15)	Acc@5  99.00 ( 99.06)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 4.4360e-01 (5.6845e-01)	Acc@1  84.00 ( 80.93)	Acc@5  99.00 ( 99.05)
Test: [ 90/100]	Time  0.025 ( 0.027)	Loss 4.8828e-01 (5.6789e-01)	Acc@1  84.00 ( 80.87)	Acc@5 100.00 ( 99.09)
 * Acc@1 80.790 Acc@5 99.100
### epoch[17] execution time: 28.250235080718994
EPOCH 18
# current learning rate: 0.1
# Switched to train mode...
Epoch: [18][  0/391]	Time  0.241 ( 0.241)	Data  0.173 ( 0.173)	Loss 4.7925e-01 (4.7925e-01)	Acc@1  82.03 ( 82.03)	Acc@5  98.44 ( 98.44)
Epoch: [18][ 10/391]	Time  0.065 ( 0.080)	Data  0.001 ( 0.017)	Loss 3.9062e-01 (5.0701e-01)	Acc@1  86.72 ( 82.53)	Acc@5 100.00 ( 99.57)
Epoch: [18][ 20/391]	Time  0.064 ( 0.073)	Data  0.001 ( 0.009)	Loss 3.9014e-01 (4.8984e-01)	Acc@1  84.38 ( 83.22)	Acc@5 100.00 ( 99.52)
Epoch: [18][ 30/391]	Time  0.063 ( 0.070)	Data  0.001 ( 0.007)	Loss 3.4546e-01 (4.6695e-01)	Acc@1  87.50 ( 83.90)	Acc@5 100.00 ( 99.45)
Epoch: [18][ 40/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.005)	Loss 4.8242e-01 (4.7242e-01)	Acc@1  83.59 ( 83.73)	Acc@5  99.22 ( 99.50)
Epoch: [18][ 50/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.004)	Loss 4.2407e-01 (4.6458e-01)	Acc@1  84.38 ( 84.11)	Acc@5  99.22 ( 99.48)
Epoch: [18][ 60/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.004)	Loss 3.9087e-01 (4.7137e-01)	Acc@1  90.62 ( 84.04)	Acc@5  97.66 ( 99.42)
Epoch: [18][ 70/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.004)	Loss 4.6216e-01 (4.7204e-01)	Acc@1  85.94 ( 83.78)	Acc@5  99.22 ( 99.45)
Epoch: [18][ 80/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 5.0977e-01 (4.7246e-01)	Acc@1  86.72 ( 83.81)	Acc@5  97.66 ( 99.44)
Epoch: [18][ 90/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.4165e-01 (4.6740e-01)	Acc@1  85.16 ( 83.98)	Acc@5 100.00 ( 99.48)
Epoch: [18][100/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.8613e-01 (4.6543e-01)	Acc@1  91.41 ( 84.14)	Acc@5  99.22 ( 99.47)
Epoch: [18][110/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 5.3955e-01 (4.6498e-01)	Acc@1  78.91 ( 84.14)	Acc@5 100.00 ( 99.47)
Epoch: [18][120/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.2871e-01 (4.6797e-01)	Acc@1  83.59 ( 84.06)	Acc@5  99.22 ( 99.46)
Epoch: [18][130/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.7378e-01 (4.6957e-01)	Acc@1  89.06 ( 83.95)	Acc@5 100.00 ( 99.45)
Epoch: [18][140/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.8232e-01 (4.6967e-01)	Acc@1  85.94 ( 83.91)	Acc@5 100.00 ( 99.43)
Epoch: [18][150/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.9316e-01 (4.6960e-01)	Acc@1  85.16 ( 83.90)	Acc@5  99.22 ( 99.42)
Epoch: [18][160/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.7192e-01 (4.7060e-01)	Acc@1  82.81 ( 83.85)	Acc@5 100.00 ( 99.43)
Epoch: [18][170/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.5898e-01 (4.7156e-01)	Acc@1  83.59 ( 83.75)	Acc@5  98.44 ( 99.42)
Epoch: [18][180/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.2739e-01 (4.7180e-01)	Acc@1  90.62 ( 83.73)	Acc@5 100.00 ( 99.41)
Epoch: [18][190/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.7534e-01 (4.7197e-01)	Acc@1  82.03 ( 83.76)	Acc@5  99.22 ( 99.39)
Epoch: [18][200/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.7378e-01 (4.7035e-01)	Acc@1  87.50 ( 83.83)	Acc@5  98.44 ( 99.39)
Epoch: [18][210/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.0283e-01 (4.7003e-01)	Acc@1  87.50 ( 83.79)	Acc@5  97.66 ( 99.40)
Epoch: [18][220/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.2129e-01 (4.6923e-01)	Acc@1  89.06 ( 83.81)	Acc@5 100.00 ( 99.38)
Epoch: [18][230/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.2310e-01 (4.6731e-01)	Acc@1  82.81 ( 83.91)	Acc@5 100.00 ( 99.39)
Epoch: [18][240/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.2871e-01 (4.6571e-01)	Acc@1  82.81 ( 83.99)	Acc@5  99.22 ( 99.39)
Epoch: [18][250/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.2197e-01 (4.6513e-01)	Acc@1  82.81 ( 84.02)	Acc@5  98.44 ( 99.39)
Epoch: [18][260/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.5630e-01 (4.6556e-01)	Acc@1  85.16 ( 83.98)	Acc@5  98.44 ( 99.39)
Epoch: [18][270/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.1416e-01 (4.6521e-01)	Acc@1  85.16 ( 84.02)	Acc@5 100.00 ( 99.39)
Epoch: [18][280/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.6523e-01 (4.6523e-01)	Acc@1  84.38 ( 84.03)	Acc@5 100.00 ( 99.37)
Epoch: [18][290/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.4165e-01 (4.6516e-01)	Acc@1  86.72 ( 84.06)	Acc@5 100.00 ( 99.37)
Epoch: [18][300/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.6411e-01 (4.6457e-01)	Acc@1  84.38 ( 84.06)	Acc@5 100.00 ( 99.37)
Epoch: [18][310/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.4141e-01 (4.6270e-01)	Acc@1  89.06 ( 84.15)	Acc@5 100.00 ( 99.38)
Epoch: [18][320/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.5811e-01 (4.6349e-01)	Acc@1  81.25 ( 84.13)	Acc@5 100.00 ( 99.39)
Epoch: [18][330/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.6934e-01 (4.6371e-01)	Acc@1  80.47 ( 84.13)	Acc@5  99.22 ( 99.38)
Epoch: [18][340/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.7729e-01 (4.6528e-01)	Acc@1  81.25 ( 84.06)	Acc@5  99.22 ( 99.37)
Epoch: [18][350/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.9780e-01 (4.6541e-01)	Acc@1  85.16 ( 84.05)	Acc@5  99.22 ( 99.38)
Epoch: [18][360/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.7275e-01 (4.6579e-01)	Acc@1  78.12 ( 84.03)	Acc@5  98.44 ( 99.38)
Epoch: [18][370/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.9316e-01 (4.6622e-01)	Acc@1  87.50 ( 84.04)	Acc@5  98.44 ( 99.38)
Epoch: [18][380/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.4971e-01 (4.6604e-01)	Acc@1  82.03 ( 84.02)	Acc@5  99.22 ( 99.37)
Epoch: [18][390/391]	Time  0.057 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.8804e-01 (4.6560e-01)	Acc@1  80.00 ( 84.02)	Acc@5 100.00 ( 99.37)
## e[18] optimizer.zero_grad (sum) time: 0.39098572731018066
## e[18]       loss.backward (sum) time: 7.262843132019043
## e[18]      optimizer.step (sum) time: 3.440200090408325
## epoch[18] training(only) time: 25.588987588882446
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 5.6201e-01 (5.6201e-01)	Acc@1  79.00 ( 79.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.039)	Loss 7.5439e-01 (5.5706e-01)	Acc@1  80.00 ( 82.82)	Acc@5  98.00 ( 99.36)
Test: [ 20/100]	Time  0.027 ( 0.032)	Loss 6.0254e-01 (5.4728e-01)	Acc@1  77.00 ( 82.19)	Acc@5  99.00 ( 99.33)
Test: [ 30/100]	Time  0.024 ( 0.030)	Loss 5.6543e-01 (5.6225e-01)	Acc@1  80.00 ( 81.65)	Acc@5  99.00 ( 99.23)
Test: [ 40/100]	Time  0.024 ( 0.028)	Loss 5.1172e-01 (5.6271e-01)	Acc@1  83.00 ( 81.61)	Acc@5  99.00 ( 99.17)
Test: [ 50/100]	Time  0.025 ( 0.028)	Loss 4.5312e-01 (5.5473e-01)	Acc@1  84.00 ( 81.92)	Acc@5  98.00 ( 99.12)
Test: [ 60/100]	Time  0.024 ( 0.027)	Loss 4.4946e-01 (5.5052e-01)	Acc@1  85.00 ( 82.07)	Acc@5 100.00 ( 99.13)
Test: [ 70/100]	Time  0.024 ( 0.027)	Loss 6.5771e-01 (5.4837e-01)	Acc@1  79.00 ( 82.14)	Acc@5 100.00 ( 99.20)
Test: [ 80/100]	Time  0.025 ( 0.027)	Loss 4.7681e-01 (5.4754e-01)	Acc@1  84.00 ( 82.07)	Acc@5  99.00 ( 99.16)
Test: [ 90/100]	Time  0.027 ( 0.027)	Loss 4.2480e-01 (5.4858e-01)	Acc@1  85.00 ( 81.97)	Acc@5 100.00 ( 99.18)
 * Acc@1 82.100 Acc@5 99.200
### epoch[18] execution time: 28.349103927612305
EPOCH 19
# current learning rate: 0.1
# Switched to train mode...
Epoch: [19][  0/391]	Time  0.247 ( 0.247)	Data  0.163 ( 0.163)	Loss 3.5449e-01 (3.5449e-01)	Acc@1  87.50 ( 87.50)	Acc@5  99.22 ( 99.22)
Epoch: [19][ 10/391]	Time  0.066 ( 0.081)	Data  0.001 ( 0.016)	Loss 3.7500e-01 (4.1979e-01)	Acc@1  89.06 ( 85.80)	Acc@5  99.22 ( 99.36)
Epoch: [19][ 20/391]	Time  0.064 ( 0.073)	Data  0.001 ( 0.009)	Loss 3.9429e-01 (4.2799e-01)	Acc@1  88.28 ( 85.57)	Acc@5  99.22 ( 99.18)
Epoch: [19][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.006)	Loss 5.5664e-01 (4.3308e-01)	Acc@1  79.69 ( 85.21)	Acc@5 100.00 ( 99.22)
Epoch: [19][ 40/391]	Time  0.058 ( 0.069)	Data  0.001 ( 0.005)	Loss 3.7036e-01 (4.4362e-01)	Acc@1  89.06 ( 84.93)	Acc@5  99.22 ( 99.24)
Epoch: [19][ 50/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.004)	Loss 5.2441e-01 (4.5226e-01)	Acc@1  83.59 ( 84.68)	Acc@5  99.22 ( 99.23)
Epoch: [19][ 60/391]	Time  0.073 ( 0.067)	Data  0.001 ( 0.004)	Loss 4.9927e-01 (4.5538e-01)	Acc@1  83.59 ( 84.57)	Acc@5  99.22 ( 99.22)
Epoch: [19][ 70/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.4546e-01 (4.5690e-01)	Acc@1  89.06 ( 84.61)	Acc@5 100.00 ( 99.21)
Epoch: [19][ 80/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.9258e-01 (4.5720e-01)	Acc@1  89.06 ( 84.58)	Acc@5 100.00 ( 99.24)
Epoch: [19][ 90/391]	Time  0.065 ( 0.067)	Data  0.002 ( 0.003)	Loss 3.7671e-01 (4.5596e-01)	Acc@1  88.28 ( 84.58)	Acc@5 100.00 ( 99.28)
Epoch: [19][100/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.0332e-01 (4.5228e-01)	Acc@1  87.50 ( 84.66)	Acc@5 100.00 ( 99.30)
Epoch: [19][110/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 5.0000e-01 (4.5047e-01)	Acc@1  82.81 ( 84.69)	Acc@5  98.44 ( 99.30)
Epoch: [19][120/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.5679e-01 (4.5043e-01)	Acc@1  82.81 ( 84.57)	Acc@5  99.22 ( 99.33)
Epoch: [19][130/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.0234e-01 (4.5085e-01)	Acc@1  84.38 ( 84.47)	Acc@5  99.22 ( 99.35)
Epoch: [19][140/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.2334e-01 (4.4984e-01)	Acc@1  85.94 ( 84.51)	Acc@5  99.22 ( 99.35)
Epoch: [19][150/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1763e-01 (4.5065e-01)	Acc@1  91.41 ( 84.47)	Acc@5 100.00 ( 99.34)
Epoch: [19][160/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.4507e-01 (4.4956e-01)	Acc@1  82.03 ( 84.53)	Acc@5  99.22 ( 99.32)
Epoch: [19][170/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.8721e-01 (4.4719e-01)	Acc@1  89.06 ( 84.59)	Acc@5  99.22 ( 99.34)
Epoch: [19][180/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.1284e-01 (4.4477e-01)	Acc@1  85.16 ( 84.69)	Acc@5 100.00 ( 99.37)
Epoch: [19][190/391]	Time  0.070 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.4541e-01 (4.4649e-01)	Acc@1  83.59 ( 84.64)	Acc@5  98.44 ( 99.35)
Epoch: [19][200/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.3506e-01 (4.4613e-01)	Acc@1  84.38 ( 84.58)	Acc@5 100.00 ( 99.37)
Epoch: [19][210/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.0889e-01 (4.4908e-01)	Acc@1  82.81 ( 84.47)	Acc@5  99.22 ( 99.37)
Epoch: [19][220/391]	Time  0.059 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.2114e-01 (4.5010e-01)	Acc@1  83.59 ( 84.41)	Acc@5  99.22 ( 99.38)
Epoch: [19][230/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.3799e-01 (4.5071e-01)	Acc@1  83.59 ( 84.43)	Acc@5  99.22 ( 99.37)
Epoch: [19][240/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.6436e-01 (4.5001e-01)	Acc@1  81.25 ( 84.43)	Acc@5 100.00 ( 99.36)
Epoch: [19][250/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5742e-01 (4.5019e-01)	Acc@1  85.16 ( 84.41)	Acc@5 100.00 ( 99.36)
Epoch: [19][260/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.3701e-01 (4.5107e-01)	Acc@1  82.03 ( 84.36)	Acc@5 100.00 ( 99.36)
Epoch: [19][270/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.7324e-01 (4.5034e-01)	Acc@1  75.78 ( 84.33)	Acc@5 100.00 ( 99.37)
Epoch: [19][280/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.6255e-01 (4.4985e-01)	Acc@1  87.50 ( 84.36)	Acc@5 100.00 ( 99.36)
Epoch: [19][290/391]	Time  0.071 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.6509e-01 (4.5016e-01)	Acc@1  82.81 ( 84.36)	Acc@5  99.22 ( 99.36)
Epoch: [19][300/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.9780e-01 (4.5029e-01)	Acc@1  86.72 ( 84.37)	Acc@5  99.22 ( 99.35)
Epoch: [19][310/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.0967e-01 (4.5045e-01)	Acc@1  82.03 ( 84.34)	Acc@5 100.00 ( 99.35)
Epoch: [19][320/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.0928e-01 (4.5091e-01)	Acc@1  82.03 ( 84.30)	Acc@5  99.22 ( 99.34)
Epoch: [19][330/391]	Time  0.072 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.0015e-01 (4.5095e-01)	Acc@1  86.72 ( 84.31)	Acc@5  99.22 ( 99.35)
Epoch: [19][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.5718e-01 (4.4978e-01)	Acc@1  88.28 ( 84.33)	Acc@5 100.00 ( 99.36)
Epoch: [19][350/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.0537e-01 (4.5021e-01)	Acc@1  82.81 ( 84.33)	Acc@5  99.22 ( 99.35)
Epoch: [19][360/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.3848e-01 (4.5017e-01)	Acc@1  85.16 ( 84.33)	Acc@5  97.66 ( 99.35)
Epoch: [19][370/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.9023e-01 (4.5037e-01)	Acc@1  85.94 ( 84.33)	Acc@5  98.44 ( 99.34)
Epoch: [19][380/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.8413e-01 (4.4987e-01)	Acc@1  84.38 ( 84.37)	Acc@5  98.44 ( 99.35)
Epoch: [19][390/391]	Time  0.050 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.5132e-01 (4.5102e-01)	Acc@1  85.00 ( 84.35)	Acc@5 100.00 ( 99.34)
## e[19] optimizer.zero_grad (sum) time: 0.39223217964172363
## e[19]       loss.backward (sum) time: 7.2207653522491455
## e[19]      optimizer.step (sum) time: 3.535355806350708
## epoch[19] training(only) time: 25.60823655128479
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 5.7715e-01 (5.7715e-01)	Acc@1  80.00 ( 80.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.032 ( 0.040)	Loss 5.8643e-01 (5.2890e-01)	Acc@1  81.00 ( 82.09)	Acc@5  99.00 ( 99.18)
Test: [ 20/100]	Time  0.024 ( 0.032)	Loss 7.3389e-01 (5.4865e-01)	Acc@1  72.00 ( 81.43)	Acc@5 100.00 ( 99.10)
Test: [ 30/100]	Time  0.024 ( 0.030)	Loss 6.0889e-01 (5.6132e-01)	Acc@1  77.00 ( 80.94)	Acc@5  98.00 ( 99.03)
Test: [ 40/100]	Time  0.027 ( 0.029)	Loss 4.6362e-01 (5.6339e-01)	Acc@1  83.00 ( 80.83)	Acc@5 100.00 ( 98.98)
Test: [ 50/100]	Time  0.025 ( 0.028)	Loss 5.2246e-01 (5.5516e-01)	Acc@1  82.00 ( 81.25)	Acc@5  99.00 ( 98.96)
Test: [ 60/100]	Time  0.025 ( 0.028)	Loss 4.7974e-01 (5.5533e-01)	Acc@1  84.00 ( 81.28)	Acc@5  99.00 ( 98.97)
Test: [ 70/100]	Time  0.028 ( 0.027)	Loss 5.3223e-01 (5.4510e-01)	Acc@1  81.00 ( 81.41)	Acc@5  99.00 ( 99.03)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 5.4492e-01 (5.4567e-01)	Acc@1  78.00 ( 81.30)	Acc@5  99.00 ( 98.99)
Test: [ 90/100]	Time  0.028 ( 0.027)	Loss 4.4141e-01 (5.4466e-01)	Acc@1  82.00 ( 81.21)	Acc@5 100.00 ( 99.01)
 * Acc@1 81.290 Acc@5 99.070
### epoch[19] execution time: 28.43808388710022
EPOCH 20
# current learning rate: 0.1
# Switched to train mode...
Epoch: [20][  0/391]	Time  0.245 ( 0.245)	Data  0.178 ( 0.178)	Loss 3.8037e-01 (3.8037e-01)	Acc@1  89.06 ( 89.06)	Acc@5  99.22 ( 99.22)
Epoch: [20][ 10/391]	Time  0.063 ( 0.081)	Data  0.001 ( 0.017)	Loss 3.1860e-01 (4.2263e-01)	Acc@1  90.62 ( 85.51)	Acc@5 100.00 ( 99.50)
Epoch: [20][ 20/391]	Time  0.062 ( 0.072)	Data  0.001 ( 0.009)	Loss 4.5239e-01 (4.2589e-01)	Acc@1  82.03 ( 85.49)	Acc@5  98.44 ( 99.14)
Epoch: [20][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.007)	Loss 4.0601e-01 (4.2012e-01)	Acc@1  86.72 ( 85.81)	Acc@5 100.00 ( 99.27)
Epoch: [20][ 40/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.005)	Loss 5.4248e-01 (4.1966e-01)	Acc@1  86.72 ( 85.96)	Acc@5  99.22 ( 99.33)
Epoch: [20][ 50/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.004)	Loss 3.9478e-01 (4.1918e-01)	Acc@1  88.28 ( 86.00)	Acc@5 100.00 ( 99.45)
Epoch: [20][ 60/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.004)	Loss 3.9819e-01 (4.2413e-01)	Acc@1  85.94 ( 85.81)	Acc@5  99.22 ( 99.45)
Epoch: [20][ 70/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.7876e-01 (4.2607e-01)	Acc@1  85.94 ( 85.82)	Acc@5  98.44 ( 99.36)
Epoch: [20][ 80/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.5996e-01 (4.3167e-01)	Acc@1  85.16 ( 85.49)	Acc@5 100.00 ( 99.35)
Epoch: [20][ 90/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 6.1816e-01 (4.3014e-01)	Acc@1  82.81 ( 85.57)	Acc@5  97.66 ( 99.35)
Epoch: [20][100/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.5107e-01 (4.3072e-01)	Acc@1  87.50 ( 85.48)	Acc@5 100.00 ( 99.37)
Epoch: [20][110/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.1616e-01 (4.2734e-01)	Acc@1  89.06 ( 85.58)	Acc@5 100.00 ( 99.37)
Epoch: [20][120/391]	Time  0.058 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.1406e-01 (4.2754e-01)	Acc@1  85.16 ( 85.49)	Acc@5 100.00 ( 99.37)
Epoch: [20][130/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.1846e-01 (4.2898e-01)	Acc@1  85.94 ( 85.45)	Acc@5  99.22 ( 99.37)
Epoch: [20][140/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.9111e-01 (4.2640e-01)	Acc@1  86.72 ( 85.52)	Acc@5  99.22 ( 99.38)
Epoch: [20][150/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.2002e-01 (4.3033e-01)	Acc@1  78.91 ( 85.31)	Acc@5  99.22 ( 99.39)
Epoch: [20][160/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.1611e-01 (4.3284e-01)	Acc@1  86.72 ( 85.24)	Acc@5  98.44 ( 99.40)
Epoch: [20][170/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.9731e-01 (4.3222e-01)	Acc@1  82.03 ( 85.25)	Acc@5 100.00 ( 99.42)
Epoch: [20][180/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.0308e-01 (4.3122e-01)	Acc@1  88.28 ( 85.29)	Acc@5  99.22 ( 99.41)
Epoch: [20][190/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.0186e-01 (4.3147e-01)	Acc@1  85.94 ( 85.27)	Acc@5  99.22 ( 99.40)
Epoch: [20][200/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.5679e-01 (4.3302e-01)	Acc@1  84.38 ( 85.19)	Acc@5 100.00 ( 99.40)
Epoch: [20][210/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.3418e-01 (4.3446e-01)	Acc@1  82.03 ( 85.12)	Acc@5 100.00 ( 99.41)
Epoch: [20][220/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.5859e-01 (4.3613e-01)	Acc@1  80.47 ( 85.08)	Acc@5  98.44 ( 99.41)
Epoch: [20][230/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.9795e-01 (4.3538e-01)	Acc@1  86.72 ( 85.12)	Acc@5 100.00 ( 99.41)
Epoch: [20][240/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.1470e-01 (4.3423e-01)	Acc@1  87.50 ( 85.14)	Acc@5 100.00 ( 99.43)
Epoch: [20][250/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.5962e-01 (4.3583e-01)	Acc@1  82.81 ( 85.04)	Acc@5 100.00 ( 99.43)
Epoch: [20][260/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.9370e-01 (4.3481e-01)	Acc@1  84.38 ( 85.06)	Acc@5 100.00 ( 99.43)
Epoch: [20][270/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.4165e-01 (4.3481e-01)	Acc@1  83.59 ( 85.03)	Acc@5  99.22 ( 99.43)
Epoch: [20][280/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.4160e-01 (4.3690e-01)	Acc@1  78.91 ( 84.95)	Acc@5  98.44 ( 99.42)
Epoch: [20][290/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.8086e-01 (4.3747e-01)	Acc@1  86.72 ( 84.92)	Acc@5 100.00 ( 99.43)
Epoch: [20][300/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.6841e-01 (4.3720e-01)	Acc@1  88.28 ( 84.93)	Acc@5  99.22 ( 99.42)
Epoch: [20][310/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.4150e-01 (4.3835e-01)	Acc@1  85.16 ( 84.90)	Acc@5  97.66 ( 99.41)
Epoch: [20][320/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.3789e-01 (4.3896e-01)	Acc@1  88.28 ( 84.86)	Acc@5 100.00 ( 99.42)
Epoch: [20][330/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.6572e-01 (4.3807e-01)	Acc@1  85.94 ( 84.87)	Acc@5 100.00 ( 99.42)
Epoch: [20][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.6963e-01 (4.3736e-01)	Acc@1  83.59 ( 84.86)	Acc@5 100.00 ( 99.43)
Epoch: [20][350/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.8462e-01 (4.3719e-01)	Acc@1  82.81 ( 84.88)	Acc@5  98.44 ( 99.43)
Epoch: [20][360/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.9961e-01 (4.3881e-01)	Acc@1  78.91 ( 84.81)	Acc@5  99.22 ( 99.42)
Epoch: [20][370/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.8315e-01 (4.3939e-01)	Acc@1  82.03 ( 84.79)	Acc@5  99.22 ( 99.41)
Epoch: [20][380/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.8389e-01 (4.3869e-01)	Acc@1  83.59 ( 84.82)	Acc@5  99.22 ( 99.41)
Epoch: [20][390/391]	Time  0.046 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.6167e-01 (4.3910e-01)	Acc@1  82.50 ( 84.83)	Acc@5  98.75 ( 99.40)
## e[20] optimizer.zero_grad (sum) time: 0.3910067081451416
## e[20]       loss.backward (sum) time: 7.179230451583862
## e[20]      optimizer.step (sum) time: 3.4668939113616943
## epoch[20] training(only) time: 25.463027238845825
# Switched to evaluate mode...
Test: [  0/100]	Time  0.192 ( 0.192)	Loss 4.8560e-01 (4.8560e-01)	Acc@1  83.00 ( 83.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.041)	Loss 7.2217e-01 (5.9832e-01)	Acc@1  75.00 ( 80.55)	Acc@5  99.00 ( 99.36)
Test: [ 20/100]	Time  0.024 ( 0.034)	Loss 7.7539e-01 (5.8609e-01)	Acc@1  76.00 ( 80.38)	Acc@5 100.00 ( 99.33)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 6.6504e-01 (5.9227e-01)	Acc@1  78.00 ( 80.39)	Acc@5 100.00 ( 99.23)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 5.4785e-01 (5.8998e-01)	Acc@1  81.00 ( 80.44)	Acc@5  98.00 ( 99.12)
Test: [ 50/100]	Time  0.028 ( 0.029)	Loss 5.3613e-01 (5.9068e-01)	Acc@1  88.00 ( 80.63)	Acc@5  97.00 ( 99.04)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 6.4697e-01 (5.9013e-01)	Acc@1  80.00 ( 80.51)	Acc@5 100.00 ( 99.11)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 4.5801e-01 (5.8606e-01)	Acc@1  86.00 ( 80.59)	Acc@5  98.00 ( 99.13)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 5.5908e-01 (5.8223e-01)	Acc@1  79.00 ( 80.56)	Acc@5  99.00 ( 99.10)
Test: [ 90/100]	Time  0.025 ( 0.027)	Loss 4.4019e-01 (5.8292e-01)	Acc@1  83.00 ( 80.53)	Acc@5  99.00 ( 99.08)
 * Acc@1 80.580 Acc@5 99.120
### epoch[20] execution time: 28.290790796279907
EPOCH 21
# current learning rate: 0.1
# Switched to train mode...
Epoch: [21][  0/391]	Time  0.235 ( 0.235)	Data  0.172 ( 0.172)	Loss 4.0332e-01 (4.0332e-01)	Acc@1  85.94 ( 85.94)	Acc@5 100.00 (100.00)
Epoch: [21][ 10/391]	Time  0.071 ( 0.081)	Data  0.001 ( 0.017)	Loss 4.5239e-01 (4.1382e-01)	Acc@1  85.94 ( 85.87)	Acc@5  99.22 ( 99.36)
Epoch: [21][ 20/391]	Time  0.063 ( 0.073)	Data  0.001 ( 0.009)	Loss 4.4385e-01 (4.4443e-01)	Acc@1  85.94 ( 84.64)	Acc@5  99.22 ( 99.37)
Epoch: [21][ 30/391]	Time  0.066 ( 0.070)	Data  0.001 ( 0.007)	Loss 3.5229e-01 (4.4312e-01)	Acc@1  89.06 ( 84.75)	Acc@5  99.22 ( 99.45)
Epoch: [21][ 40/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.005)	Loss 3.8159e-01 (4.3729e-01)	Acc@1  88.28 ( 84.66)	Acc@5  99.22 ( 99.50)
Epoch: [21][ 50/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.004)	Loss 3.0981e-01 (4.2843e-01)	Acc@1  88.28 ( 84.68)	Acc@5  99.22 ( 99.53)
Epoch: [21][ 60/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 4.7168e-01 (4.2731e-01)	Acc@1  81.25 ( 84.67)	Acc@5  99.22 ( 99.51)
Epoch: [21][ 70/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.5034e-01 (4.2561e-01)	Acc@1  89.84 ( 84.88)	Acc@5  99.22 ( 99.50)
Epoch: [21][ 80/391]	Time  0.061 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.9756e-01 (4.2897e-01)	Acc@1  83.59 ( 84.81)	Acc@5 100.00 ( 99.50)
Epoch: [21][ 90/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.8345e-01 (4.2342e-01)	Acc@1  91.41 ( 85.10)	Acc@5  98.44 ( 99.48)
Epoch: [21][100/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.6948e-01 (4.2301e-01)	Acc@1  84.38 ( 85.10)	Acc@5  96.09 ( 99.45)
Epoch: [21][110/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 5.1318e-01 (4.2465e-01)	Acc@1  81.25 ( 85.04)	Acc@5 100.00 ( 99.47)
Epoch: [21][120/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3496e-01 (4.2445e-01)	Acc@1  88.28 ( 85.07)	Acc@5 100.00 ( 99.47)
Epoch: [21][130/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.9185e-01 (4.2446e-01)	Acc@1  86.72 ( 85.01)	Acc@5 100.00 ( 99.49)
Epoch: [21][140/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.2163e-01 (4.2326e-01)	Acc@1  82.81 ( 85.03)	Acc@5 100.00 ( 99.50)
Epoch: [21][150/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5742e-01 (4.2512e-01)	Acc@1  88.28 ( 85.02)	Acc@5 100.00 ( 99.48)
Epoch: [21][160/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.9844e-01 (4.2401e-01)	Acc@1  92.19 ( 85.15)	Acc@5  98.44 ( 99.48)
Epoch: [21][170/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.2676e-01 (4.2353e-01)	Acc@1  84.38 ( 85.18)	Acc@5  99.22 ( 99.48)
Epoch: [21][180/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.3223e-01 (4.2285e-01)	Acc@1  82.03 ( 85.23)	Acc@5  98.44 ( 99.48)
Epoch: [21][190/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.1113e-01 (4.2123e-01)	Acc@1  82.81 ( 85.32)	Acc@5 100.00 ( 99.49)
Epoch: [21][200/391]	Time  0.070 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.1738e-01 (4.1967e-01)	Acc@1  87.50 ( 85.40)	Acc@5  99.22 ( 99.50)
Epoch: [21][210/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.6729e-01 (4.1861e-01)	Acc@1  78.91 ( 85.40)	Acc@5  99.22 ( 99.51)
Epoch: [21][220/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.7891e-01 (4.2388e-01)	Acc@1  86.72 ( 85.24)	Acc@5 100.00 ( 99.51)
Epoch: [21][230/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.5400e-01 (4.2579e-01)	Acc@1  85.94 ( 85.18)	Acc@5 100.00 ( 99.51)
Epoch: [21][240/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.2197e-01 (4.2632e-01)	Acc@1  83.59 ( 85.18)	Acc@5  99.22 ( 99.50)
Epoch: [21][250/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.2383e-01 (4.2624e-01)	Acc@1  87.50 ( 85.19)	Acc@5  99.22 ( 99.50)
Epoch: [21][260/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.2993e-01 (4.2571e-01)	Acc@1  85.94 ( 85.22)	Acc@5  99.22 ( 99.50)
Epoch: [21][270/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.7095e-01 (4.2609e-01)	Acc@1  82.03 ( 85.23)	Acc@5  98.44 ( 99.49)
Epoch: [21][280/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.0840e-01 (4.2626e-01)	Acc@1  78.12 ( 85.21)	Acc@5 100.00 ( 99.49)
Epoch: [21][290/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.7241e-01 (4.2525e-01)	Acc@1  82.81 ( 85.25)	Acc@5  99.22 ( 99.50)
Epoch: [21][300/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.5908e-01 (4.2543e-01)	Acc@1  82.03 ( 85.27)	Acc@5  99.22 ( 99.51)
Epoch: [21][310/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.9453e-01 (4.2496e-01)	Acc@1  83.59 ( 85.26)	Acc@5  99.22 ( 99.50)
Epoch: [21][320/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.2505e-01 (4.2596e-01)	Acc@1  85.16 ( 85.22)	Acc@5 100.00 ( 99.51)
Epoch: [21][330/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.0049e-01 (4.2688e-01)	Acc@1  82.81 ( 85.22)	Acc@5  98.44 ( 99.50)
Epoch: [21][340/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.6255e-01 (4.2670e-01)	Acc@1  87.50 ( 85.22)	Acc@5  99.22 ( 99.50)
Epoch: [21][350/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.3384e-01 (4.2674e-01)	Acc@1  85.94 ( 85.23)	Acc@5  98.44 ( 99.50)
Epoch: [21][360/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.0283e-01 (4.2669e-01)	Acc@1  85.94 ( 85.24)	Acc@5  99.22 ( 99.51)
Epoch: [21][370/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.8462e-01 (4.2818e-01)	Acc@1  82.03 ( 85.22)	Acc@5  99.22 ( 99.50)
Epoch: [21][380/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.2490e-01 (4.2863e-01)	Acc@1  83.59 ( 85.20)	Acc@5  99.22 ( 99.50)
Epoch: [21][390/391]	Time  0.047 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.6123e-01 (4.2958e-01)	Acc@1  78.75 ( 85.20)	Acc@5  97.50 ( 99.49)
## e[21] optimizer.zero_grad (sum) time: 0.3846457004547119
## e[21]       loss.backward (sum) time: 7.232359409332275
## e[21]      optimizer.step (sum) time: 3.4261269569396973
## epoch[21] training(only) time: 25.45171880722046
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 3.8330e-01 (3.8330e-01)	Acc@1  85.00 ( 85.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.025 ( 0.040)	Loss 4.5288e-01 (4.5752e-01)	Acc@1  86.00 ( 84.82)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.024 ( 0.034)	Loss 5.8594e-01 (4.7175e-01)	Acc@1  84.00 ( 84.48)	Acc@5  99.00 ( 99.19)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 4.3628e-01 (4.8375e-01)	Acc@1  85.00 ( 84.00)	Acc@5  99.00 ( 99.03)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 4.7583e-01 (4.9000e-01)	Acc@1  84.00 ( 83.83)	Acc@5  99.00 ( 98.95)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 4.3091e-01 (4.9088e-01)	Acc@1  90.00 ( 84.04)	Acc@5  98.00 ( 98.88)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 4.3921e-01 (4.8722e-01)	Acc@1  85.00 ( 84.25)	Acc@5 100.00 ( 98.98)
Test: [ 70/100]	Time  0.026 ( 0.027)	Loss 5.6787e-01 (4.8600e-01)	Acc@1  85.00 ( 84.35)	Acc@5  98.00 ( 99.04)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 3.7817e-01 (4.7954e-01)	Acc@1  87.00 ( 84.35)	Acc@5 100.00 ( 99.12)
Test: [ 90/100]	Time  0.028 ( 0.027)	Loss 3.8550e-01 (4.7911e-01)	Acc@1  87.00 ( 84.27)	Acc@5 100.00 ( 99.13)
 * Acc@1 84.370 Acc@5 99.160
### epoch[21] execution time: 28.240458965301514
EPOCH 22
# current learning rate: 0.1
# Switched to train mode...
Epoch: [22][  0/391]	Time  0.252 ( 0.252)	Data  0.187 ( 0.187)	Loss 3.9404e-01 (3.9404e-01)	Acc@1  85.94 ( 85.94)	Acc@5 100.00 (100.00)
Epoch: [22][ 10/391]	Time  0.062 ( 0.081)	Data  0.001 ( 0.018)	Loss 3.5474e-01 (4.0416e-01)	Acc@1  88.28 ( 86.15)	Acc@5  99.22 ( 99.50)
Epoch: [22][ 20/391]	Time  0.065 ( 0.073)	Data  0.001 ( 0.010)	Loss 5.0488e-01 (4.2034e-01)	Acc@1  82.81 ( 85.45)	Acc@5 100.00 ( 99.40)
Epoch: [22][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.007)	Loss 3.8403e-01 (4.2823e-01)	Acc@1  84.38 ( 85.08)	Acc@5  99.22 ( 99.29)
Epoch: [22][ 40/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.005)	Loss 3.6499e-01 (4.2548e-01)	Acc@1  89.06 ( 85.10)	Acc@5  99.22 ( 99.41)
Epoch: [22][ 50/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.005)	Loss 3.0005e-01 (4.2989e-01)	Acc@1  91.41 ( 84.85)	Acc@5  98.44 ( 99.37)
Epoch: [22][ 60/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.004)	Loss 4.4531e-01 (4.3028e-01)	Acc@1  85.16 ( 84.91)	Acc@5 100.00 ( 99.42)
Epoch: [22][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 2.9248e-01 (4.2195e-01)	Acc@1  90.62 ( 85.17)	Acc@5 100.00 ( 99.48)
Epoch: [22][ 80/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.1836e-01 (4.1892e-01)	Acc@1  90.62 ( 85.33)	Acc@5  99.22 ( 99.47)
Epoch: [22][ 90/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.9780e-01 (4.1644e-01)	Acc@1  82.03 ( 85.48)	Acc@5  99.22 ( 99.47)
Epoch: [22][100/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.2603e-01 (4.1983e-01)	Acc@1  83.59 ( 85.36)	Acc@5 100.00 ( 99.44)
Epoch: [22][110/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.3423e-01 (4.2035e-01)	Acc@1  89.84 ( 85.44)	Acc@5 100.00 ( 99.42)
Epoch: [22][120/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.2935e-01 (4.2059e-01)	Acc@1  87.50 ( 85.45)	Acc@5 100.00 ( 99.43)
Epoch: [22][130/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.2310e-01 (4.1783e-01)	Acc@1  84.38 ( 85.57)	Acc@5  99.22 ( 99.43)
Epoch: [22][140/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.7744e-01 (4.1905e-01)	Acc@1  86.72 ( 85.56)	Acc@5 100.00 ( 99.43)
Epoch: [22][150/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.7705e-01 (4.2089e-01)	Acc@1  81.25 ( 85.45)	Acc@5 100.00 ( 99.43)
Epoch: [22][160/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.4082e-01 (4.2097e-01)	Acc@1  85.94 ( 85.41)	Acc@5 100.00 ( 99.44)
Epoch: [22][170/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5205e-01 (4.1745e-01)	Acc@1  89.06 ( 85.53)	Acc@5 100.00 ( 99.45)
Epoch: [22][180/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.5337e-01 (4.2081e-01)	Acc@1  84.38 ( 85.49)	Acc@5  99.22 ( 99.43)
Epoch: [22][190/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.4473e-01 (4.2311e-01)	Acc@1  86.72 ( 85.40)	Acc@5 100.00 ( 99.43)
Epoch: [22][200/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.3164e-01 (4.2248e-01)	Acc@1  86.72 ( 85.47)	Acc@5  99.22 ( 99.44)
Epoch: [22][210/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.0918e-01 (4.2181e-01)	Acc@1  85.16 ( 85.44)	Acc@5  98.44 ( 99.45)
Epoch: [22][220/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.0977e-01 (4.2137e-01)	Acc@1  82.81 ( 85.40)	Acc@5  98.44 ( 99.45)
Epoch: [22][230/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.0098e-01 (4.2277e-01)	Acc@1  82.81 ( 85.34)	Acc@5  98.44 ( 99.44)
Epoch: [22][240/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.5244e-01 (4.2204e-01)	Acc@1  91.41 ( 85.33)	Acc@5 100.00 ( 99.44)
Epoch: [22][250/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.2642e-01 (4.2177e-01)	Acc@1  89.84 ( 85.38)	Acc@5 100.00 ( 99.44)
Epoch: [22][260/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.6006e-01 (4.2276e-01)	Acc@1  80.47 ( 85.35)	Acc@5  98.44 ( 99.44)
Epoch: [22][270/391]	Time  0.058 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.3237e-01 (4.2416e-01)	Acc@1  85.94 ( 85.34)	Acc@5  99.22 ( 99.44)
Epoch: [22][280/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.5679e-01 (4.2498e-01)	Acc@1  83.59 ( 85.36)	Acc@5  99.22 ( 99.44)
Epoch: [22][290/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.0796e-01 (4.2590e-01)	Acc@1  84.38 ( 85.34)	Acc@5  99.22 ( 99.43)
Epoch: [22][300/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.8613e-01 (4.2482e-01)	Acc@1  89.06 ( 85.35)	Acc@5 100.00 ( 99.44)
Epoch: [22][310/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.2227e-01 (4.2392e-01)	Acc@1  88.28 ( 85.38)	Acc@5 100.00 ( 99.44)
Epoch: [22][320/391]	Time  0.072 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.2539e-01 (4.2485e-01)	Acc@1  79.69 ( 85.34)	Acc@5 100.00 ( 99.44)
Epoch: [22][330/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.2847e-01 (4.2480e-01)	Acc@1  87.50 ( 85.38)	Acc@5  99.22 ( 99.44)
Epoch: [22][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.2114e-01 (4.2479e-01)	Acc@1  85.16 ( 85.40)	Acc@5  99.22 ( 99.44)
Epoch: [22][350/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.0430e-01 (4.2485e-01)	Acc@1  83.59 ( 85.38)	Acc@5 100.00 ( 99.44)
Epoch: [22][360/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.8818e-01 (4.2598e-01)	Acc@1  86.72 ( 85.33)	Acc@5 100.00 ( 99.44)
Epoch: [22][370/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.7021e-01 (4.2681e-01)	Acc@1  86.72 ( 85.31)	Acc@5  99.22 ( 99.44)
Epoch: [22][380/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.2236e-01 (4.2578e-01)	Acc@1  83.59 ( 85.34)	Acc@5 100.00 ( 99.44)
Epoch: [22][390/391]	Time  0.047 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.3774e-01 (4.2584e-01)	Acc@1  85.00 ( 85.33)	Acc@5 100.00 ( 99.44)
## e[22] optimizer.zero_grad (sum) time: 0.3941497802734375
## e[22]       loss.backward (sum) time: 7.255923271179199
## e[22]      optimizer.step (sum) time: 3.400386333465576
## epoch[22] training(only) time: 25.54275345802307
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 4.5581e-01 (4.5581e-01)	Acc@1  82.00 ( 82.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.024 ( 0.040)	Loss 5.0635e-01 (5.4031e-01)	Acc@1  84.00 ( 81.45)	Acc@5 100.00 ( 99.00)
Test: [ 20/100]	Time  0.024 ( 0.032)	Loss 6.4258e-01 (5.4779e-01)	Acc@1  78.00 ( 81.43)	Acc@5 100.00 ( 99.19)
Test: [ 30/100]	Time  0.024 ( 0.030)	Loss 5.4297e-01 (5.4959e-01)	Acc@1  83.00 ( 81.42)	Acc@5 100.00 ( 99.23)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 4.9878e-01 (5.4798e-01)	Acc@1  86.00 ( 81.66)	Acc@5  98.00 ( 99.20)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 4.2310e-01 (5.4679e-01)	Acc@1  87.00 ( 81.76)	Acc@5  98.00 ( 99.12)
Test: [ 60/100]	Time  0.027 ( 0.028)	Loss 5.2344e-01 (5.4398e-01)	Acc@1  81.00 ( 81.80)	Acc@5 100.00 ( 99.11)
Test: [ 70/100]	Time  0.024 ( 0.027)	Loss 4.8682e-01 (5.3506e-01)	Acc@1  84.00 ( 82.11)	Acc@5 100.00 ( 99.20)
Test: [ 80/100]	Time  0.027 ( 0.027)	Loss 4.6338e-01 (5.3149e-01)	Acc@1  83.00 ( 82.11)	Acc@5  98.00 ( 99.21)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 4.7412e-01 (5.3584e-01)	Acc@1  85.00 ( 81.97)	Acc@5 100.00 ( 99.22)
 * Acc@1 81.940 Acc@5 99.250
### epoch[22] execution time: 28.35094666481018
EPOCH 23
# current learning rate: 0.1
# Switched to train mode...
Epoch: [23][  0/391]	Time  0.244 ( 0.244)	Data  0.176 ( 0.176)	Loss 4.9292e-01 (4.9292e-01)	Acc@1  81.25 ( 81.25)	Acc@5  98.44 ( 98.44)
Epoch: [23][ 10/391]	Time  0.065 ( 0.081)	Data  0.001 ( 0.017)	Loss 5.5127e-01 (4.3905e-01)	Acc@1  83.59 ( 84.38)	Acc@5 100.00 ( 99.64)
Epoch: [23][ 20/391]	Time  0.066 ( 0.073)	Data  0.001 ( 0.009)	Loss 4.3726e-01 (4.2257e-01)	Acc@1  83.59 ( 85.12)	Acc@5  99.22 ( 99.63)
Epoch: [23][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.007)	Loss 5.7910e-01 (4.2440e-01)	Acc@1  79.69 ( 84.88)	Acc@5 100.00 ( 99.47)
Epoch: [23][ 40/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.005)	Loss 3.9941e-01 (4.1945e-01)	Acc@1  85.16 ( 85.18)	Acc@5  99.22 ( 99.52)
Epoch: [23][ 50/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.005)	Loss 5.2051e-01 (4.2281e-01)	Acc@1  83.59 ( 85.26)	Acc@5  99.22 ( 99.54)
Epoch: [23][ 60/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.004)	Loss 4.7363e-01 (4.1825e-01)	Acc@1  78.91 ( 85.23)	Acc@5 100.00 ( 99.55)
Epoch: [23][ 70/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.004)	Loss 4.4751e-01 (4.1770e-01)	Acc@1  85.16 ( 85.28)	Acc@5  99.22 ( 99.55)
Epoch: [23][ 80/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.2847e-01 (4.1831e-01)	Acc@1  79.69 ( 85.16)	Acc@5 100.00 ( 99.54)
Epoch: [23][ 90/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.003)	Loss 5.1465e-01 (4.1818e-01)	Acc@1  81.25 ( 85.26)	Acc@5  99.22 ( 99.54)
Epoch: [23][100/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.4985e-01 (4.1685e-01)	Acc@1  85.16 ( 85.23)	Acc@5  99.22 ( 99.57)
Epoch: [23][110/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.6826e-01 (4.1693e-01)	Acc@1  83.59 ( 85.25)	Acc@5  99.22 ( 99.57)
Epoch: [23][120/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5425e-01 (4.1661e-01)	Acc@1  87.50 ( 85.29)	Acc@5 100.00 ( 99.59)
Epoch: [23][130/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.2373e-01 (4.2012e-01)	Acc@1  89.06 ( 85.23)	Acc@5  99.22 ( 99.58)
Epoch: [23][140/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.6719e-01 (4.1943e-01)	Acc@1  87.50 ( 85.27)	Acc@5 100.00 ( 99.56)
Epoch: [23][150/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5425e-01 (4.2108e-01)	Acc@1  87.50 ( 85.19)	Acc@5 100.00 ( 99.56)
Epoch: [23][160/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.8047e-01 (4.2306e-01)	Acc@1  82.03 ( 85.11)	Acc@5  99.22 ( 99.56)
Epoch: [23][170/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.7314e-01 (4.2602e-01)	Acc@1  83.59 ( 84.98)	Acc@5  98.44 ( 99.57)
Epoch: [23][180/391]	Time  0.058 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.0479e-01 (4.2653e-01)	Acc@1  85.94 ( 84.94)	Acc@5 100.00 ( 99.59)
Epoch: [23][190/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.5264e-01 (4.2773e-01)	Acc@1  84.38 ( 84.89)	Acc@5  98.44 ( 99.57)
Epoch: [23][200/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.5669e-01 (4.2750e-01)	Acc@1  82.81 ( 84.91)	Acc@5 100.00 ( 99.56)
Epoch: [23][210/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.9795e-01 (4.2893e-01)	Acc@1  86.72 ( 84.93)	Acc@5  99.22 ( 99.53)
Epoch: [23][220/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.6230e-01 (4.2852e-01)	Acc@1  88.28 ( 84.94)	Acc@5  99.22 ( 99.53)
Epoch: [23][230/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.1914e-01 (4.2942e-01)	Acc@1  77.34 ( 84.91)	Acc@5  97.66 ( 99.51)
Epoch: [23][240/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.5679e-01 (4.2965e-01)	Acc@1  81.25 ( 84.90)	Acc@5 100.00 ( 99.52)
Epoch: [23][250/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.0845e-01 (4.2791e-01)	Acc@1  86.72 ( 84.96)	Acc@5  99.22 ( 99.52)
Epoch: [23][260/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.6094e-01 (4.2673e-01)	Acc@1  85.16 ( 85.01)	Acc@5 100.00 ( 99.53)
Epoch: [23][270/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.0566e-01 (4.2626e-01)	Acc@1  91.41 ( 85.04)	Acc@5 100.00 ( 99.53)
Epoch: [23][280/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.5859e-01 (4.2521e-01)	Acc@1  82.03 ( 85.11)	Acc@5 100.00 ( 99.54)
Epoch: [23][290/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.1812e-01 (4.2619e-01)	Acc@1  89.84 ( 85.13)	Acc@5  99.22 ( 99.52)
Epoch: [23][300/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.6743e-01 (4.2643e-01)	Acc@1  89.06 ( 85.14)	Acc@5 100.00 ( 99.51)
Epoch: [23][310/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.2344e-01 (4.2705e-01)	Acc@1  82.81 ( 85.13)	Acc@5  98.44 ( 99.50)
Epoch: [23][320/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.8301e-01 (4.2576e-01)	Acc@1  78.91 ( 85.20)	Acc@5  98.44 ( 99.50)
Epoch: [23][330/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.1758e-01 (4.2578e-01)	Acc@1  82.81 ( 85.20)	Acc@5  98.44 ( 99.48)
Epoch: [23][340/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.2847e-01 (4.2646e-01)	Acc@1  85.16 ( 85.17)	Acc@5 100.00 ( 99.47)
Epoch: [23][350/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.0000e-01 (4.2695e-01)	Acc@1  79.69 ( 85.15)	Acc@5 100.00 ( 99.46)
Epoch: [23][360/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.6367e-01 (4.2591e-01)	Acc@1  89.84 ( 85.20)	Acc@5 100.00 ( 99.46)
Epoch: [23][370/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.9004e-01 (4.2441e-01)	Acc@1  89.84 ( 85.25)	Acc@5 100.00 ( 99.47)
Epoch: [23][380/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.2554e-01 (4.2458e-01)	Acc@1  84.38 ( 85.27)	Acc@5  98.44 ( 99.46)
Epoch: [23][390/391]	Time  0.046 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.5327e-01 (4.2427e-01)	Acc@1  87.50 ( 85.29)	Acc@5 100.00 ( 99.46)
## e[23] optimizer.zero_grad (sum) time: 0.38696813583374023
## e[23]       loss.backward (sum) time: 7.183328151702881
## e[23]      optimizer.step (sum) time: 3.4150807857513428
## epoch[23] training(only) time: 25.4024920463562
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 5.8594e-01 (5.8594e-01)	Acc@1  80.00 ( 80.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.040)	Loss 6.3623e-01 (4.8666e-01)	Acc@1  81.00 ( 83.18)	Acc@5  99.00 ( 99.55)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 5.1855e-01 (4.9255e-01)	Acc@1  79.00 ( 83.10)	Acc@5  99.00 ( 99.33)
Test: [ 30/100]	Time  0.024 ( 0.030)	Loss 5.1758e-01 (4.9475e-01)	Acc@1  80.00 ( 83.26)	Acc@5 100.00 ( 99.26)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 4.5923e-01 (4.9896e-01)	Acc@1  85.00 ( 83.27)	Acc@5 100.00 ( 99.12)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 4.6802e-01 (5.0089e-01)	Acc@1  85.00 ( 83.31)	Acc@5 100.00 ( 99.16)
Test: [ 60/100]	Time  0.024 ( 0.027)	Loss 4.9658e-01 (4.9926e-01)	Acc@1  79.00 ( 83.25)	Acc@5 100.00 ( 99.15)
Test: [ 70/100]	Time  0.027 ( 0.027)	Loss 4.9902e-01 (4.9305e-01)	Acc@1  84.00 ( 83.34)	Acc@5 100.00 ( 99.18)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 4.7437e-01 (4.9278e-01)	Acc@1  81.00 ( 83.33)	Acc@5 100.00 ( 99.16)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 2.2827e-01 (4.8764e-01)	Acc@1  92.00 ( 83.55)	Acc@5 100.00 ( 99.21)
 * Acc@1 83.730 Acc@5 99.230
### epoch[23] execution time: 28.1544828414917
EPOCH 24
# current learning rate: 0.1
# Switched to train mode...
Epoch: [24][  0/391]	Time  0.256 ( 0.256)	Data  0.188 ( 0.188)	Loss 3.0420e-01 (3.0420e-01)	Acc@1  90.62 ( 90.62)	Acc@5 100.00 (100.00)
Epoch: [24][ 10/391]	Time  0.063 ( 0.081)	Data  0.001 ( 0.018)	Loss 4.7559e-01 (3.8630e-01)	Acc@1  81.25 ( 86.93)	Acc@5 100.00 ( 99.50)
Epoch: [24][ 20/391]	Time  0.064 ( 0.073)	Data  0.001 ( 0.010)	Loss 3.9551e-01 (3.9582e-01)	Acc@1  85.16 ( 86.01)	Acc@5 100.00 ( 99.55)
Epoch: [24][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.007)	Loss 3.0786e-01 (3.9070e-01)	Acc@1  89.06 ( 86.06)	Acc@5  98.44 ( 99.50)
Epoch: [24][ 40/391]	Time  0.061 ( 0.069)	Data  0.001 ( 0.006)	Loss 3.7842e-01 (3.9738e-01)	Acc@1  85.94 ( 85.80)	Acc@5 100.00 ( 99.52)
Epoch: [24][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.005)	Loss 5.4834e-01 (4.0443e-01)	Acc@1  84.38 ( 85.81)	Acc@5  99.22 ( 99.56)
Epoch: [24][ 60/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.004)	Loss 5.9863e-01 (4.1502e-01)	Acc@1  78.12 ( 85.55)	Acc@5 100.00 ( 99.56)
Epoch: [24][ 70/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.004)	Loss 4.7046e-01 (4.2352e-01)	Acc@1  82.81 ( 85.30)	Acc@5 100.00 ( 99.54)
Epoch: [24][ 80/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.8208e-01 (4.2359e-01)	Acc@1  83.59 ( 85.29)	Acc@5  99.22 ( 99.55)
Epoch: [24][ 90/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.3496e-01 (4.2443e-01)	Acc@1  92.19 ( 85.46)	Acc@5 100.00 ( 99.55)
Epoch: [24][100/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.0576e-01 (4.2486e-01)	Acc@1  85.16 ( 85.42)	Acc@5 100.00 ( 99.54)
Epoch: [24][110/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.4375e-01 (4.2098e-01)	Acc@1  87.50 ( 85.54)	Acc@5 100.00 ( 99.56)
Epoch: [24][120/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.3555e-01 (4.2008e-01)	Acc@1  81.25 ( 85.52)	Acc@5  99.22 ( 99.54)
Epoch: [24][130/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.2480e-01 (4.1783e-01)	Acc@1  82.03 ( 85.57)	Acc@5  99.22 ( 99.53)
Epoch: [24][140/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.9746e-01 (4.1622e-01)	Acc@1  88.28 ( 85.62)	Acc@5  99.22 ( 99.55)
Epoch: [24][150/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.3164e-01 (4.1498e-01)	Acc@1  86.72 ( 85.71)	Acc@5  99.22 ( 99.55)
Epoch: [24][160/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.1465e-01 (4.1529e-01)	Acc@1  79.69 ( 85.69)	Acc@5 100.00 ( 99.56)
Epoch: [24][170/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5254e-01 (4.1718e-01)	Acc@1  86.72 ( 85.58)	Acc@5 100.00 ( 99.57)
Epoch: [24][180/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.7036e-01 (4.2101e-01)	Acc@1  87.50 ( 85.43)	Acc@5  98.44 ( 99.53)
Epoch: [24][190/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.9785e-01 (4.2124e-01)	Acc@1  92.19 ( 85.47)	Acc@5 100.00 ( 99.54)
Epoch: [24][200/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.7241e-01 (4.2189e-01)	Acc@1  84.38 ( 85.46)	Acc@5  98.44 ( 99.54)
Epoch: [24][210/391]	Time  0.074 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0981e-01 (4.2218e-01)	Acc@1  88.28 ( 85.43)	Acc@5 100.00 ( 99.54)
Epoch: [24][220/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5864e-01 (4.2053e-01)	Acc@1  83.59 ( 85.49)	Acc@5 100.00 ( 99.53)
Epoch: [24][230/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.2554e-01 (4.1893e-01)	Acc@1  84.38 ( 85.50)	Acc@5  99.22 ( 99.53)
Epoch: [24][240/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.2690e-01 (4.1643e-01)	Acc@1  87.50 ( 85.57)	Acc@5 100.00 ( 99.54)
Epoch: [24][250/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.2407e-01 (4.1624e-01)	Acc@1  86.72 ( 85.62)	Acc@5  99.22 ( 99.53)
Epoch: [24][260/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.9834e-01 (4.1680e-01)	Acc@1  87.50 ( 85.59)	Acc@5 100.00 ( 99.52)
Epoch: [24][270/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.1357e-01 (4.1790e-01)	Acc@1  85.16 ( 85.55)	Acc@5 100.00 ( 99.53)
Epoch: [24][280/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.0493e-01 (4.1667e-01)	Acc@1  91.41 ( 85.61)	Acc@5  99.22 ( 99.52)
Epoch: [24][290/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.7046e-01 (4.1719e-01)	Acc@1  82.03 ( 85.60)	Acc@5  98.44 ( 99.51)
Epoch: [24][300/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.5986e-01 (4.1562e-01)	Acc@1  84.38 ( 85.63)	Acc@5 100.00 ( 99.52)
Epoch: [24][310/391]	Time  0.073 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.3867e-01 (4.1696e-01)	Acc@1  78.12 ( 85.58)	Acc@5  99.22 ( 99.52)
Epoch: [24][320/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.3335e-01 (4.1660e-01)	Acc@1  85.94 ( 85.64)	Acc@5 100.00 ( 99.51)
Epoch: [24][330/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.3994e-01 (4.1802e-01)	Acc@1  84.38 ( 85.61)	Acc@5  99.22 ( 99.51)
Epoch: [24][340/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.5083e-01 (4.1861e-01)	Acc@1  89.06 ( 85.59)	Acc@5 100.00 ( 99.52)
Epoch: [24][350/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.0845e-01 (4.1816e-01)	Acc@1  86.72 ( 85.62)	Acc@5  97.66 ( 99.50)
Epoch: [24][360/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.8291e-01 (4.1777e-01)	Acc@1  82.81 ( 85.61)	Acc@5 100.00 ( 99.50)
Epoch: [24][370/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.4048e-01 (4.1631e-01)	Acc@1  94.53 ( 85.68)	Acc@5  99.22 ( 99.50)
Epoch: [24][380/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.8770e-01 (4.1545e-01)	Acc@1  85.94 ( 85.70)	Acc@5 100.00 ( 99.51)
Epoch: [24][390/391]	Time  0.048 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.6597e-01 (4.1585e-01)	Acc@1  88.75 ( 85.67)	Acc@5 100.00 ( 99.51)
## e[24] optimizer.zero_grad (sum) time: 0.38880181312561035
## e[24]       loss.backward (sum) time: 7.21375036239624
## e[24]      optimizer.step (sum) time: 3.4572765827178955
## epoch[24] training(only) time: 25.552270650863647
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 5.8252e-01 (5.8252e-01)	Acc@1  76.00 ( 76.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.024 ( 0.040)	Loss 6.1865e-01 (5.7417e-01)	Acc@1  77.00 ( 80.36)	Acc@5 100.00 ( 99.09)
Test: [ 20/100]	Time  0.024 ( 0.032)	Loss 6.9531e-01 (5.7168e-01)	Acc@1  74.00 ( 80.29)	Acc@5  99.00 ( 99.14)
Test: [ 30/100]	Time  0.024 ( 0.030)	Loss 5.5615e-01 (5.8786e-01)	Acc@1  77.00 ( 80.10)	Acc@5 100.00 ( 98.94)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 5.4150e-01 (5.8821e-01)	Acc@1  83.00 ( 80.10)	Acc@5  98.00 ( 98.85)
Test: [ 50/100]	Time  0.026 ( 0.028)	Loss 5.6445e-01 (5.8100e-01)	Acc@1  81.00 ( 80.53)	Acc@5 100.00 ( 98.88)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 5.6348e-01 (5.7787e-01)	Acc@1  83.00 ( 80.74)	Acc@5  99.00 ( 98.93)
Test: [ 70/100]	Time  0.024 ( 0.027)	Loss 5.7861e-01 (5.7160e-01)	Acc@1  80.00 ( 81.01)	Acc@5  98.00 ( 98.96)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 5.0879e-01 (5.6637e-01)	Acc@1  85.00 ( 81.14)	Acc@5  98.00 ( 98.99)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 6.1182e-01 (5.6764e-01)	Acc@1  79.00 ( 81.03)	Acc@5  99.00 ( 98.97)
 * Acc@1 81.130 Acc@5 99.000
### epoch[24] execution time: 28.32787847518921
EPOCH 25
# current learning rate: 0.1
# Switched to train mode...
Epoch: [25][  0/391]	Time  0.246 ( 0.246)	Data  0.181 ( 0.181)	Loss 5.1025e-01 (5.1025e-01)	Acc@1  82.03 ( 82.03)	Acc@5  99.22 ( 99.22)
Epoch: [25][ 10/391]	Time  0.069 ( 0.081)	Data  0.001 ( 0.017)	Loss 2.4158e-01 (4.0111e-01)	Acc@1  92.97 ( 85.80)	Acc@5 100.00 ( 99.50)
Epoch: [25][ 20/391]	Time  0.066 ( 0.073)	Data  0.001 ( 0.010)	Loss 3.7085e-01 (3.9756e-01)	Acc@1  89.84 ( 86.46)	Acc@5 100.00 ( 99.48)
Epoch: [25][ 30/391]	Time  0.066 ( 0.071)	Data  0.001 ( 0.007)	Loss 3.1982e-01 (3.9272e-01)	Acc@1  88.28 ( 86.57)	Acc@5 100.00 ( 99.57)
Epoch: [25][ 40/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.005)	Loss 5.1172e-01 (3.9567e-01)	Acc@1  78.91 ( 86.01)	Acc@5 100.00 ( 99.56)
Epoch: [25][ 50/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.005)	Loss 3.7622e-01 (3.9805e-01)	Acc@1  85.16 ( 85.88)	Acc@5 100.00 ( 99.59)
Epoch: [25][ 60/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.004)	Loss 4.1016e-01 (3.9493e-01)	Acc@1  83.59 ( 85.98)	Acc@5  99.22 ( 99.56)
Epoch: [25][ 70/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.004)	Loss 2.9126e-01 (3.9381e-01)	Acc@1  86.72 ( 85.90)	Acc@5 100.00 ( 99.59)
Epoch: [25][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.2617e-01 (3.9169e-01)	Acc@1  84.38 ( 86.01)	Acc@5  99.22 ( 99.59)
Epoch: [25][ 90/391]	Time  0.074 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.9673e-01 (3.9640e-01)	Acc@1  86.72 ( 85.91)	Acc@5 100.00 ( 99.61)
Epoch: [25][100/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.9219e-01 (3.9901e-01)	Acc@1  82.81 ( 85.87)	Acc@5  99.22 ( 99.61)
Epoch: [25][110/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.4780e-01 (3.9821e-01)	Acc@1  90.62 ( 85.90)	Acc@5 100.00 ( 99.60)
Epoch: [25][120/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.0161e-01 (3.9861e-01)	Acc@1  87.50 ( 85.90)	Acc@5 100.00 ( 99.59)
Epoch: [25][130/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.8428e-01 (3.9818e-01)	Acc@1  85.16 ( 85.94)	Acc@5  98.44 ( 99.59)
Epoch: [25][140/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.8853e-01 (3.9666e-01)	Acc@1  84.38 ( 86.06)	Acc@5  99.22 ( 99.60)
Epoch: [25][150/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.7939e-01 (3.9713e-01)	Acc@1  86.72 ( 86.08)	Acc@5 100.00 ( 99.59)
Epoch: [25][160/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.9272e-01 (3.9700e-01)	Acc@1  88.28 ( 86.12)	Acc@5  99.22 ( 99.59)
Epoch: [25][170/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.7588e-01 (3.9482e-01)	Acc@1  91.41 ( 86.22)	Acc@5 100.00 ( 99.59)
Epoch: [25][180/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.7925e-01 (3.9368e-01)	Acc@1  85.16 ( 86.28)	Acc@5  98.44 ( 99.59)
Epoch: [25][190/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.8633e-01 (3.9809e-01)	Acc@1  78.91 ( 86.14)	Acc@5  99.22 ( 99.56)
Epoch: [25][200/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.5566e-01 (4.0216e-01)	Acc@1  80.47 ( 86.00)	Acc@5  98.44 ( 99.55)
Epoch: [25][210/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.8340e-01 (4.0384e-01)	Acc@1  82.03 ( 85.92)	Acc@5  98.44 ( 99.54)
Epoch: [25][220/391]	Time  0.071 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.6362e-01 (4.0440e-01)	Acc@1  86.72 ( 85.94)	Acc@5  99.22 ( 99.52)
Epoch: [25][230/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.4751e-01 (4.0412e-01)	Acc@1  81.25 ( 85.95)	Acc@5 100.00 ( 99.53)
Epoch: [25][240/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.2910e-01 (4.0448e-01)	Acc@1  89.84 ( 85.93)	Acc@5  98.44 ( 99.52)
Epoch: [25][250/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.6816e-01 (4.0465e-01)	Acc@1  85.94 ( 85.91)	Acc@5  99.22 ( 99.53)
Epoch: [25][260/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.3701e-01 (4.0551e-01)	Acc@1  85.16 ( 85.86)	Acc@5 100.00 ( 99.54)
Epoch: [25][270/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.5029e-01 (4.0681e-01)	Acc@1  82.03 ( 85.83)	Acc@5  99.22 ( 99.53)
Epoch: [25][280/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.1299e-01 (4.0603e-01)	Acc@1  88.28 ( 85.84)	Acc@5  99.22 ( 99.53)
Epoch: [25][290/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.0200e-01 (4.0649e-01)	Acc@1  89.06 ( 85.83)	Acc@5 100.00 ( 99.52)
Epoch: [25][300/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.6230e-01 (4.0673e-01)	Acc@1  87.50 ( 85.82)	Acc@5  99.22 ( 99.53)
Epoch: [25][310/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.1089e-01 (4.0743e-01)	Acc@1  87.50 ( 85.81)	Acc@5  99.22 ( 99.52)
Epoch: [25][320/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.0063e-01 (4.0839e-01)	Acc@1  85.16 ( 85.77)	Acc@5 100.00 ( 99.51)
Epoch: [25][330/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.2163e-01 (4.0923e-01)	Acc@1  85.16 ( 85.74)	Acc@5  99.22 ( 99.51)
Epoch: [25][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.2148e-01 (4.0894e-01)	Acc@1  85.94 ( 85.76)	Acc@5  98.44 ( 99.50)
Epoch: [25][350/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.8535e-01 (4.0944e-01)	Acc@1  82.81 ( 85.77)	Acc@5  97.66 ( 99.49)
Epoch: [25][360/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.5522e-01 (4.0922e-01)	Acc@1  86.72 ( 85.79)	Acc@5 100.00 ( 99.50)
Epoch: [25][370/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.0688e-01 (4.0949e-01)	Acc@1  87.50 ( 85.79)	Acc@5  99.22 ( 99.49)
Epoch: [25][380/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.6851e-01 (4.1012e-01)	Acc@1  82.03 ( 85.76)	Acc@5  99.22 ( 99.50)
Epoch: [25][390/391]	Time  0.047 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.5332e-01 (4.1223e-01)	Acc@1  77.50 ( 85.70)	Acc@5  97.50 ( 99.48)
## e[25] optimizer.zero_grad (sum) time: 0.3909778594970703
## e[25]       loss.backward (sum) time: 7.258660554885864
## e[25]      optimizer.step (sum) time: 3.462613582611084
## epoch[25] training(only) time: 25.58134651184082
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 5.0732e-01 (5.0732e-01)	Acc@1  80.00 ( 80.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.024 ( 0.040)	Loss 6.0303e-01 (5.2539e-01)	Acc@1  82.00 ( 82.36)	Acc@5 100.00 ( 99.36)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 5.6934e-01 (5.1131e-01)	Acc@1  81.00 ( 82.62)	Acc@5 100.00 ( 99.14)
Test: [ 30/100]	Time  0.024 ( 0.030)	Loss 5.2490e-01 (5.2599e-01)	Acc@1  79.00 ( 82.45)	Acc@5  99.00 ( 99.10)
Test: [ 40/100]	Time  0.026 ( 0.029)	Loss 4.3384e-01 (5.2509e-01)	Acc@1  86.00 ( 82.54)	Acc@5  98.00 ( 99.05)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 4.7437e-01 (5.2163e-01)	Acc@1  84.00 ( 82.78)	Acc@5  99.00 ( 99.04)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 5.1025e-01 (5.2216e-01)	Acc@1  80.00 ( 82.69)	Acc@5 100.00 ( 99.07)
Test: [ 70/100]	Time  0.024 ( 0.027)	Loss 4.5825e-01 (5.1511e-01)	Acc@1  85.00 ( 82.79)	Acc@5  99.00 ( 99.14)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 4.1748e-01 (5.0934e-01)	Acc@1  87.00 ( 82.95)	Acc@5  99.00 ( 99.17)
Test: [ 90/100]	Time  0.028 ( 0.027)	Loss 3.6890e-01 (5.0835e-01)	Acc@1  90.00 ( 83.01)	Acc@5 100.00 ( 99.16)
 * Acc@1 83.090 Acc@5 99.160
### epoch[25] execution time: 28.365716457366943
EPOCH 26
# current learning rate: 0.1
# Switched to train mode...
Epoch: [26][  0/391]	Time  0.248 ( 0.248)	Data  0.180 ( 0.180)	Loss 4.4165e-01 (4.4165e-01)	Acc@1  86.72 ( 86.72)	Acc@5  98.44 ( 98.44)
Epoch: [26][ 10/391]	Time  0.065 ( 0.081)	Data  0.001 ( 0.017)	Loss 3.4058e-01 (3.8561e-01)	Acc@1  89.06 ( 87.43)	Acc@5  99.22 ( 99.50)
Epoch: [26][ 20/391]	Time  0.065 ( 0.073)	Data  0.001 ( 0.010)	Loss 2.6074e-01 (3.8467e-01)	Acc@1  90.62 ( 87.35)	Acc@5  99.22 ( 99.55)
Epoch: [26][ 30/391]	Time  0.067 ( 0.071)	Data  0.001 ( 0.007)	Loss 3.9551e-01 (3.8605e-01)	Acc@1  85.16 ( 87.17)	Acc@5 100.00 ( 99.60)
Epoch: [26][ 40/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.005)	Loss 4.5166e-01 (3.8434e-01)	Acc@1  82.03 ( 87.10)	Acc@5 100.00 ( 99.62)
Epoch: [26][ 50/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.005)	Loss 3.2153e-01 (3.8730e-01)	Acc@1  86.72 ( 86.89)	Acc@5 100.00 ( 99.60)
Epoch: [26][ 60/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.004)	Loss 3.8110e-01 (3.8619e-01)	Acc@1  90.62 ( 86.91)	Acc@5 100.00 ( 99.64)
Epoch: [26][ 70/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.004)	Loss 4.3457e-01 (3.8250e-01)	Acc@1  85.16 ( 86.83)	Acc@5  99.22 ( 99.63)
Epoch: [26][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.6733e-01 (3.8368e-01)	Acc@1  91.41 ( 86.92)	Acc@5 100.00 ( 99.59)
Epoch: [26][ 90/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.1787e-01 (3.8666e-01)	Acc@1  89.84 ( 86.80)	Acc@5  99.22 ( 99.56)
Epoch: [26][100/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.3057e-01 (3.8842e-01)	Acc@1  89.84 ( 86.78)	Acc@5  99.22 ( 99.57)
Epoch: [26][110/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.3433e-01 (3.9128e-01)	Acc@1  84.38 ( 86.73)	Acc@5  99.22 ( 99.51)
Epoch: [26][120/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.3970e-01 (3.9485e-01)	Acc@1  85.94 ( 86.51)	Acc@5 100.00 ( 99.50)
Epoch: [26][130/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.8413e-01 (3.9263e-01)	Acc@1  85.16 ( 86.61)	Acc@5  98.44 ( 99.49)
Epoch: [26][140/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.6719e-01 (3.9569e-01)	Acc@1  84.38 ( 86.43)	Acc@5 100.00 ( 99.49)
Epoch: [26][150/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.7915e-01 (3.9801e-01)	Acc@1  85.94 ( 86.42)	Acc@5 100.00 ( 99.49)
Epoch: [26][160/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.1475e-01 (4.0292e-01)	Acc@1  75.78 ( 86.24)	Acc@5  99.22 ( 99.50)
Epoch: [26][170/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.7744e-01 (4.0170e-01)	Acc@1  82.81 ( 86.24)	Acc@5 100.00 ( 99.51)
Epoch: [26][180/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.1089e-01 (4.0344e-01)	Acc@1  85.16 ( 86.18)	Acc@5 100.00 ( 99.53)
Epoch: [26][190/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0688e-01 (4.0294e-01)	Acc@1  90.62 ( 86.21)	Acc@5 100.00 ( 99.52)
Epoch: [26][200/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1812e-01 (4.0023e-01)	Acc@1  86.72 ( 86.30)	Acc@5 100.00 ( 99.53)
Epoch: [26][210/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.7666e-01 (3.9963e-01)	Acc@1  84.38 ( 86.34)	Acc@5 100.00 ( 99.53)
Epoch: [26][220/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.2764e-01 (3.9875e-01)	Acc@1  87.50 ( 86.32)	Acc@5 100.00 ( 99.53)
Epoch: [26][230/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.2007e-01 (3.9801e-01)	Acc@1  88.28 ( 86.37)	Acc@5 100.00 ( 99.52)
Epoch: [26][240/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.7646e-01 (3.9895e-01)	Acc@1  89.06 ( 86.32)	Acc@5 100.00 ( 99.53)
Epoch: [26][250/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5571e-01 (3.9942e-01)	Acc@1  88.28 ( 86.28)	Acc@5  99.22 ( 99.54)
Epoch: [26][260/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.2285e-01 (4.0030e-01)	Acc@1  88.28 ( 86.26)	Acc@5  98.44 ( 99.52)
Epoch: [26][270/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.0601e-01 (4.0141e-01)	Acc@1  87.50 ( 86.22)	Acc@5 100.00 ( 99.53)
Epoch: [26][280/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.1865e-01 (4.0289e-01)	Acc@1  76.56 ( 86.16)	Acc@5  99.22 ( 99.52)
Epoch: [26][290/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.7769e-01 (4.0246e-01)	Acc@1  84.38 ( 86.20)	Acc@5  99.22 ( 99.52)
Epoch: [26][300/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.6655e-01 (4.0212e-01)	Acc@1  87.50 ( 86.24)	Acc@5 100.00 ( 99.53)
Epoch: [26][310/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.0527e-01 (4.0107e-01)	Acc@1  87.50 ( 86.27)	Acc@5 100.00 ( 99.53)
Epoch: [26][320/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.2739e-01 (4.0027e-01)	Acc@1  88.28 ( 86.33)	Acc@5 100.00 ( 99.52)
Epoch: [26][330/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.7783e-01 (3.9994e-01)	Acc@1  89.06 ( 86.32)	Acc@5 100.00 ( 99.51)
Epoch: [26][340/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.2246e-01 (4.0053e-01)	Acc@1  83.59 ( 86.29)	Acc@5 100.00 ( 99.52)
Epoch: [26][350/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.8696e-01 (4.0092e-01)	Acc@1  87.50 ( 86.26)	Acc@5  99.22 ( 99.51)
Epoch: [26][360/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.6045e-01 (4.0165e-01)	Acc@1  86.72 ( 86.22)	Acc@5 100.00 ( 99.52)
Epoch: [26][370/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.1309e-01 (4.0180e-01)	Acc@1  85.94 ( 86.21)	Acc@5 100.00 ( 99.52)
Epoch: [26][380/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.3936e-01 (4.0124e-01)	Acc@1  87.50 ( 86.22)	Acc@5  99.22 ( 99.52)
Epoch: [26][390/391]	Time  0.048 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.7324e-01 (4.0272e-01)	Acc@1  86.25 ( 86.17)	Acc@5  98.75 ( 99.51)
## e[26] optimizer.zero_grad (sum) time: 0.3883075714111328
## e[26]       loss.backward (sum) time: 7.239534378051758
## e[26]      optimizer.step (sum) time: 3.457228422164917
## epoch[26] training(only) time: 25.61138391494751
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 4.6680e-01 (4.6680e-01)	Acc@1  85.00 ( 85.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.040)	Loss 6.3232e-01 (6.6442e-01)	Acc@1  80.00 ( 78.27)	Acc@5  97.00 ( 98.09)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 7.0312e-01 (6.7189e-01)	Acc@1  73.00 ( 77.38)	Acc@5  99.00 ( 98.10)
Test: [ 30/100]	Time  0.028 ( 0.030)	Loss 8.6670e-01 (6.9620e-01)	Acc@1  70.00 ( 77.00)	Acc@5  98.00 ( 98.06)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 6.4551e-01 (7.0148e-01)	Acc@1  82.00 ( 77.00)	Acc@5  96.00 ( 97.90)
Test: [ 50/100]	Time  0.027 ( 0.028)	Loss 6.0352e-01 (6.9334e-01)	Acc@1  77.00 ( 77.41)	Acc@5  99.00 ( 97.86)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 8.7939e-01 (7.0065e-01)	Acc@1  69.00 ( 76.95)	Acc@5 100.00 ( 98.00)
Test: [ 70/100]	Time  0.024 ( 0.027)	Loss 7.0508e-01 (6.9361e-01)	Acc@1  79.00 ( 77.28)	Acc@5  98.00 ( 98.03)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 6.1865e-01 (6.8475e-01)	Acc@1  81.00 ( 77.49)	Acc@5  99.00 ( 98.07)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 5.1172e-01 (6.8393e-01)	Acc@1  76.00 ( 77.59)	Acc@5  99.00 ( 98.07)
 * Acc@1 77.780 Acc@5 98.100
### epoch[26] execution time: 28.375874519348145
EPOCH 27
# current learning rate: 0.1
# Switched to train mode...
Epoch: [27][  0/391]	Time  0.244 ( 0.244)	Data  0.181 ( 0.181)	Loss 3.3716e-01 (3.3716e-01)	Acc@1  85.94 ( 85.94)	Acc@5 100.00 (100.00)
Epoch: [27][ 10/391]	Time  0.064 ( 0.082)	Data  0.001 ( 0.017)	Loss 4.4092e-01 (4.0268e-01)	Acc@1  84.38 ( 85.44)	Acc@5 100.00 ( 99.79)
Epoch: [27][ 20/391]	Time  0.066 ( 0.074)	Data  0.001 ( 0.010)	Loss 2.9443e-01 (3.9622e-01)	Acc@1  92.19 ( 86.01)	Acc@5  99.22 ( 99.55)
Epoch: [27][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.007)	Loss 2.9932e-01 (3.8751e-01)	Acc@1  89.06 ( 86.32)	Acc@5  99.22 ( 99.65)
Epoch: [27][ 40/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.005)	Loss 3.9355e-01 (3.8017e-01)	Acc@1  86.72 ( 86.53)	Acc@5 100.00 ( 99.58)
Epoch: [27][ 50/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.005)	Loss 4.8730e-01 (3.8392e-01)	Acc@1  80.47 ( 86.35)	Acc@5 100.00 ( 99.59)
Epoch: [27][ 60/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 5.0732e-01 (3.8650e-01)	Acc@1  79.69 ( 86.23)	Acc@5  99.22 ( 99.58)
Epoch: [27][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 5.3564e-01 (3.9539e-01)	Acc@1  86.72 ( 86.20)	Acc@5  99.22 ( 99.49)
Epoch: [27][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.4531e-01 (4.0069e-01)	Acc@1  88.28 ( 86.03)	Acc@5  99.22 ( 99.48)
Epoch: [27][ 90/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.7314e-01 (4.0360e-01)	Acc@1  84.38 ( 85.94)	Acc@5  99.22 ( 99.48)
Epoch: [27][100/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.1494e-01 (4.0126e-01)	Acc@1  91.41 ( 86.14)	Acc@5  99.22 ( 99.50)
Epoch: [27][110/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.8696e-01 (4.0329e-01)	Acc@1  85.16 ( 85.99)	Acc@5 100.00 ( 99.49)
Epoch: [27][120/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.6206e-01 (4.0395e-01)	Acc@1  89.06 ( 86.00)	Acc@5 100.00 ( 99.52)
Epoch: [27][130/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.8022e-01 (4.0291e-01)	Acc@1  85.16 ( 86.04)	Acc@5 100.00 ( 99.48)
Epoch: [27][140/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.7256e-01 (4.0085e-01)	Acc@1  87.50 ( 86.06)	Acc@5 100.00 ( 99.48)
Epoch: [27][150/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1226e-01 (3.9954e-01)	Acc@1  88.28 ( 86.09)	Acc@5 100.00 ( 99.50)
Epoch: [27][160/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.2969e-01 (3.9904e-01)	Acc@1  82.81 ( 86.07)	Acc@5  99.22 ( 99.51)
Epoch: [27][170/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.6987e-01 (3.9903e-01)	Acc@1  85.94 ( 86.12)	Acc@5  99.22 ( 99.51)
Epoch: [27][180/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.6353e-01 (3.9873e-01)	Acc@1  86.72 ( 86.08)	Acc@5  99.22 ( 99.53)
Epoch: [27][190/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.0635e-01 (4.0138e-01)	Acc@1  78.91 ( 85.98)	Acc@5 100.00 ( 99.51)
Epoch: [27][200/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.7793e-01 (4.0023e-01)	Acc@1  86.72 ( 86.06)	Acc@5 100.00 ( 99.51)
Epoch: [27][210/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3154e-01 (4.0219e-01)	Acc@1  88.28 ( 85.96)	Acc@5  99.22 ( 99.50)
Epoch: [27][220/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.7412e-01 (4.0145e-01)	Acc@1  85.94 ( 86.04)	Acc@5 100.00 ( 99.50)
Epoch: [27][230/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1738e-01 (3.9919e-01)	Acc@1  87.50 ( 86.10)	Acc@5 100.00 ( 99.52)
Epoch: [27][240/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5547e-01 (3.9718e-01)	Acc@1  86.72 ( 86.16)	Acc@5 100.00 ( 99.52)
Epoch: [27][250/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.7720e-01 (3.9790e-01)	Acc@1  87.50 ( 86.12)	Acc@5 100.00 ( 99.52)
Epoch: [27][260/391]	Time  0.059 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.1562e-01 (3.9818e-01)	Acc@1  84.38 ( 86.10)	Acc@5  98.44 ( 99.53)
Epoch: [27][270/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.0791e-01 (3.9879e-01)	Acc@1  78.91 ( 86.07)	Acc@5  99.22 ( 99.52)
Epoch: [27][280/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5693e-01 (3.9964e-01)	Acc@1  89.84 ( 86.03)	Acc@5 100.00 ( 99.52)
Epoch: [27][290/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5840e-01 (4.0080e-01)	Acc@1  87.50 ( 86.04)	Acc@5 100.00 ( 99.50)
Epoch: [27][300/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.7695e-01 (4.0125e-01)	Acc@1  87.50 ( 86.01)	Acc@5  99.22 ( 99.51)
Epoch: [27][310/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.2861e-01 (4.0033e-01)	Acc@1  86.72 ( 86.03)	Acc@5 100.00 ( 99.50)
Epoch: [27][320/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.8232e-01 (4.0027e-01)	Acc@1  84.38 ( 86.03)	Acc@5 100.00 ( 99.50)
Epoch: [27][330/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.1689e-01 (4.0005e-01)	Acc@1  89.06 ( 86.06)	Acc@5 100.00 ( 99.50)
Epoch: [27][340/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.0146e-01 (4.0020e-01)	Acc@1  82.03 ( 86.06)	Acc@5  99.22 ( 99.50)
Epoch: [27][350/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.7710e-01 (4.0047e-01)	Acc@1  90.62 ( 86.06)	Acc@5 100.00 ( 99.50)
Epoch: [27][360/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.3774e-01 (4.0039e-01)	Acc@1  83.59 ( 86.09)	Acc@5 100.00 ( 99.51)
Epoch: [27][370/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.0332e-01 (4.0077e-01)	Acc@1  85.16 ( 86.09)	Acc@5 100.00 ( 99.50)
Epoch: [27][380/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.7866e-01 (3.9968e-01)	Acc@1  85.16 ( 86.12)	Acc@5 100.00 ( 99.50)
Epoch: [27][390/391]	Time  0.048 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8848e-01 (3.9926e-01)	Acc@1  96.25 ( 86.13)	Acc@5 100.00 ( 99.50)
## e[27] optimizer.zero_grad (sum) time: 0.3970487117767334
## e[27]       loss.backward (sum) time: 7.22145676612854
## e[27]      optimizer.step (sum) time: 3.4687445163726807
## epoch[27] training(only) time: 25.586920976638794
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 4.4385e-01 (4.4385e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.040)	Loss 4.8218e-01 (4.3538e-01)	Acc@1  82.00 ( 85.45)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 5.5859e-01 (4.6048e-01)	Acc@1  80.00 ( 84.81)	Acc@5 100.00 ( 99.29)
Test: [ 30/100]	Time  0.024 ( 0.030)	Loss 4.9194e-01 (4.7345e-01)	Acc@1  84.00 ( 84.42)	Acc@5 100.00 ( 99.19)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 3.9209e-01 (4.7426e-01)	Acc@1  82.00 ( 84.24)	Acc@5 100.00 ( 99.29)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 3.8257e-01 (4.7089e-01)	Acc@1  88.00 ( 84.41)	Acc@5  98.00 ( 99.25)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 4.2456e-01 (4.6691e-01)	Acc@1  85.00 ( 84.43)	Acc@5  99.00 ( 99.26)
Test: [ 70/100]	Time  0.028 ( 0.027)	Loss 4.8364e-01 (4.6182e-01)	Acc@1  82.00 ( 84.49)	Acc@5 100.00 ( 99.30)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 3.8281e-01 (4.6611e-01)	Acc@1  86.00 ( 84.40)	Acc@5 100.00 ( 99.28)
Test: [ 90/100]	Time  0.027 ( 0.027)	Loss 2.9785e-01 (4.6632e-01)	Acc@1  90.00 ( 84.32)	Acc@5 100.00 ( 99.26)
 * Acc@1 84.520 Acc@5 99.280
### epoch[27] execution time: 28.359091997146606
EPOCH 28
# current learning rate: 0.1
# Switched to train mode...
Epoch: [28][  0/391]	Time  0.247 ( 0.247)	Data  0.181 ( 0.181)	Loss 3.8770e-01 (3.8770e-01)	Acc@1  85.94 ( 85.94)	Acc@5 100.00 (100.00)
Epoch: [28][ 10/391]	Time  0.064 ( 0.082)	Data  0.001 ( 0.017)	Loss 3.5034e-01 (3.5891e-01)	Acc@1  86.72 ( 87.43)	Acc@5  98.44 ( 99.50)
Epoch: [28][ 20/391]	Time  0.067 ( 0.074)	Data  0.001 ( 0.010)	Loss 5.6396e-01 (3.7808e-01)	Acc@1  85.16 ( 87.09)	Acc@5  96.88 ( 99.44)
Epoch: [28][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.007)	Loss 2.1143e-01 (3.7131e-01)	Acc@1  95.31 ( 87.30)	Acc@5 100.00 ( 99.52)
Epoch: [28][ 40/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.005)	Loss 5.1270e-01 (3.7851e-01)	Acc@1  81.25 ( 86.99)	Acc@5  99.22 ( 99.47)
Epoch: [28][ 50/391]	Time  0.060 ( 0.068)	Data  0.001 ( 0.005)	Loss 4.5630e-01 (3.7354e-01)	Acc@1  84.38 ( 87.19)	Acc@5  98.44 ( 99.48)
Epoch: [28][ 60/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.004)	Loss 3.3325e-01 (3.7288e-01)	Acc@1  89.06 ( 87.18)	Acc@5 100.00 ( 99.49)
Epoch: [28][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 3.7158e-01 (3.7602e-01)	Acc@1  86.72 ( 87.13)	Acc@5 100.00 ( 99.47)
Epoch: [28][ 80/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.8706e-01 (3.7533e-01)	Acc@1  83.59 ( 87.11)	Acc@5  99.22 ( 99.48)
Epoch: [28][ 90/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.1006e-01 (3.7467e-01)	Acc@1  90.62 ( 87.19)	Acc@5  99.22 ( 99.48)
Epoch: [28][100/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.6265e-01 (3.7850e-01)	Acc@1  83.59 ( 87.11)	Acc@5 100.00 ( 99.49)
Epoch: [28][110/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.6729e-01 (3.7688e-01)	Acc@1  89.06 ( 87.10)	Acc@5 100.00 ( 99.52)
Epoch: [28][120/391]	Time  0.074 ( 0.066)	Data  0.001 ( 0.003)	Loss 5.0293e-01 (3.7889e-01)	Acc@1  82.81 ( 87.02)	Acc@5  98.44 ( 99.52)
Epoch: [28][130/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.2686e-01 (3.8291e-01)	Acc@1  82.03 ( 86.84)	Acc@5  99.22 ( 99.51)
Epoch: [28][140/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0322e-01 (3.8372e-01)	Acc@1  89.06 ( 86.78)	Acc@5 100.00 ( 99.50)
Epoch: [28][150/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.8232e-01 (3.8408e-01)	Acc@1  85.94 ( 86.78)	Acc@5 100.00 ( 99.53)
Epoch: [28][160/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.8379e-01 (3.8354e-01)	Acc@1  83.59 ( 86.79)	Acc@5  99.22 ( 99.52)
Epoch: [28][170/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.1865e-01 (3.8334e-01)	Acc@1  78.91 ( 86.80)	Acc@5 100.00 ( 99.54)
Epoch: [28][180/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.4937e-01 (3.8543e-01)	Acc@1  87.50 ( 86.74)	Acc@5 100.00 ( 99.53)
Epoch: [28][190/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.0977e-01 (3.8649e-01)	Acc@1  84.38 ( 86.67)	Acc@5  99.22 ( 99.53)
Epoch: [28][200/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.9429e-01 (3.8688e-01)	Acc@1  83.59 ( 86.64)	Acc@5  99.22 ( 99.53)
Epoch: [28][210/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.6050e-01 (3.8615e-01)	Acc@1  89.84 ( 86.66)	Acc@5  99.22 ( 99.52)
Epoch: [28][220/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.7280e-01 (3.8596e-01)	Acc@1  88.28 ( 86.72)	Acc@5  99.22 ( 99.51)
Epoch: [28][230/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.6221e-01 (3.8442e-01)	Acc@1  90.62 ( 86.75)	Acc@5 100.00 ( 99.51)
Epoch: [28][240/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.4590e-01 (3.8520e-01)	Acc@1  82.03 ( 86.76)	Acc@5 100.00 ( 99.51)
Epoch: [28][250/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5571e-01 (3.8562e-01)	Acc@1  85.94 ( 86.71)	Acc@5 100.00 ( 99.53)
Epoch: [28][260/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.5610e-01 (3.8582e-01)	Acc@1  87.50 ( 86.72)	Acc@5 100.00 ( 99.53)
Epoch: [28][270/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.7070e-01 (3.8596e-01)	Acc@1  83.59 ( 86.70)	Acc@5 100.00 ( 99.53)
Epoch: [28][280/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.3188e-01 (3.8651e-01)	Acc@1  85.16 ( 86.68)	Acc@5 100.00 ( 99.53)
Epoch: [28][290/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.4678e-01 (3.8783e-01)	Acc@1  83.59 ( 86.62)	Acc@5  99.22 ( 99.52)
Epoch: [28][300/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.0454e-01 (3.8805e-01)	Acc@1  85.16 ( 86.63)	Acc@5 100.00 ( 99.52)
Epoch: [28][310/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.6865e-01 (3.8926e-01)	Acc@1  86.72 ( 86.59)	Acc@5  99.22 ( 99.51)
Epoch: [28][320/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3545e-01 (3.8953e-01)	Acc@1  89.06 ( 86.56)	Acc@5  99.22 ( 99.52)
Epoch: [28][330/391]	Time  0.060 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3105e-01 (3.8916e-01)	Acc@1  87.50 ( 86.57)	Acc@5 100.00 ( 99.52)
Epoch: [28][340/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.8345e-01 (3.8986e-01)	Acc@1  88.28 ( 86.54)	Acc@5 100.00 ( 99.53)
Epoch: [28][350/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.6592e-01 (3.9086e-01)	Acc@1  81.25 ( 86.47)	Acc@5 100.00 ( 99.53)
Epoch: [28][360/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.2041e-01 (3.9236e-01)	Acc@1  86.72 ( 86.41)	Acc@5  99.22 ( 99.53)
Epoch: [28][370/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3252e-01 (3.9195e-01)	Acc@1  92.19 ( 86.43)	Acc@5  99.22 ( 99.52)
Epoch: [28][380/391]	Time  0.071 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0713e-01 (3.9187e-01)	Acc@1  89.06 ( 86.44)	Acc@5 100.00 ( 99.52)
Epoch: [28][390/391]	Time  0.050 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.8916e-01 (3.9146e-01)	Acc@1  87.50 ( 86.45)	Acc@5 100.00 ( 99.52)
## e[28] optimizer.zero_grad (sum) time: 0.3971390724182129
## e[28]       loss.backward (sum) time: 7.281579256057739
## e[28]      optimizer.step (sum) time: 3.4631242752075195
## epoch[28] training(only) time: 25.704145431518555
# Switched to evaluate mode...
Test: [  0/100]	Time  0.191 ( 0.191)	Loss 6.9629e-01 (6.9629e-01)	Acc@1  71.00 ( 71.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.041)	Loss 8.6768e-01 (7.4174e-01)	Acc@1  77.00 ( 77.09)	Acc@5  96.00 ( 97.91)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 6.7529e-01 (7.0783e-01)	Acc@1  75.00 ( 77.33)	Acc@5  99.00 ( 98.14)
Test: [ 30/100]	Time  0.032 ( 0.031)	Loss 8.2178e-01 (7.3730e-01)	Acc@1  73.00 ( 77.03)	Acc@5  99.00 ( 98.03)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 8.0811e-01 (7.3351e-01)	Acc@1  76.00 ( 77.24)	Acc@5  98.00 ( 97.85)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 1.0645e+00 (7.3351e-01)	Acc@1  68.00 ( 77.45)	Acc@5  97.00 ( 97.88)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 7.2559e-01 (7.4269e-01)	Acc@1  72.00 ( 77.15)	Acc@5  99.00 ( 98.02)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 9.0723e-01 (7.4306e-01)	Acc@1  71.00 ( 77.08)	Acc@5  99.00 ( 98.14)
Test: [ 80/100]	Time  0.027 ( 0.027)	Loss 5.1221e-01 (7.3510e-01)	Acc@1  85.00 ( 77.16)	Acc@5  99.00 ( 98.15)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 5.2539e-01 (7.3072e-01)	Acc@1  82.00 ( 77.22)	Acc@5 100.00 ( 98.12)
 * Acc@1 77.510 Acc@5 98.180
### epoch[28] execution time: 28.517101764678955
EPOCH 29
# current learning rate: 0.1
# Switched to train mode...
Epoch: [29][  0/391]	Time  0.253 ( 0.253)	Data  0.185 ( 0.185)	Loss 2.9468e-01 (2.9468e-01)	Acc@1  90.62 ( 90.62)	Acc@5  99.22 ( 99.22)
Epoch: [29][ 10/391]	Time  0.063 ( 0.081)	Data  0.001 ( 0.018)	Loss 4.1626e-01 (3.6114e-01)	Acc@1  84.38 ( 86.65)	Acc@5  99.22 ( 99.50)
Epoch: [29][ 20/391]	Time  0.064 ( 0.073)	Data  0.001 ( 0.010)	Loss 3.3936e-01 (3.7944e-01)	Acc@1  86.72 ( 86.46)	Acc@5 100.00 ( 99.44)
Epoch: [29][ 30/391]	Time  0.062 ( 0.070)	Data  0.001 ( 0.007)	Loss 4.6875e-01 (3.7329e-01)	Acc@1  84.38 ( 86.69)	Acc@5  98.44 ( 99.45)
Epoch: [29][ 40/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.006)	Loss 3.1592e-01 (3.6509e-01)	Acc@1  87.50 ( 86.87)	Acc@5 100.00 ( 99.52)
Epoch: [29][ 50/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.005)	Loss 4.8047e-01 (3.6730e-01)	Acc@1  85.16 ( 86.95)	Acc@5  99.22 ( 99.53)
Epoch: [29][ 60/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.004)	Loss 3.2568e-01 (3.6946e-01)	Acc@1  86.72 ( 87.00)	Acc@5 100.00 ( 99.53)
Epoch: [29][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 3.5156e-01 (3.6957e-01)	Acc@1  89.06 ( 87.06)	Acc@5 100.00 ( 99.53)
Epoch: [29][ 80/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.4556e-01 (3.6704e-01)	Acc@1  81.25 ( 87.05)	Acc@5 100.00 ( 99.53)
Epoch: [29][ 90/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.4434e-01 (3.7348e-01)	Acc@1  83.59 ( 86.88)	Acc@5 100.00 ( 99.51)
Epoch: [29][100/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.6196e-01 (3.7207e-01)	Acc@1  92.19 ( 86.93)	Acc@5 100.00 ( 99.53)
Epoch: [29][110/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.0405e-01 (3.7478e-01)	Acc@1  89.06 ( 86.83)	Acc@5  96.88 ( 99.50)
Epoch: [29][120/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.6118e-01 (3.7832e-01)	Acc@1  85.94 ( 86.75)	Acc@5 100.00 ( 99.51)
Epoch: [29][130/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.3823e-01 (3.7714e-01)	Acc@1  85.94 ( 86.85)	Acc@5  99.22 ( 99.52)
Epoch: [29][140/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.4482e-01 (3.7851e-01)	Acc@1  89.84 ( 86.81)	Acc@5  99.22 ( 99.52)
Epoch: [29][150/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.4092e-01 (3.7941e-01)	Acc@1  83.59 ( 86.78)	Acc@5  98.44 ( 99.50)
Epoch: [29][160/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.5605e-01 (3.8006e-01)	Acc@1  85.94 ( 86.78)	Acc@5 100.00 ( 99.52)
Epoch: [29][170/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.1006e-01 (3.7793e-01)	Acc@1  88.28 ( 86.89)	Acc@5 100.00 ( 99.53)
Epoch: [29][180/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.3955e-01 (3.7906e-01)	Acc@1  81.25 ( 86.87)	Acc@5  99.22 ( 99.50)
Epoch: [29][190/391]	Time  0.073 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.8145e-01 (3.8038e-01)	Acc@1  83.59 ( 86.83)	Acc@5 100.00 ( 99.49)
Epoch: [29][200/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.5107e-01 (3.8103e-01)	Acc@1  86.72 ( 86.82)	Acc@5 100.00 ( 99.50)
Epoch: [29][210/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.3301e-01 (3.8141e-01)	Acc@1  89.06 ( 86.84)	Acc@5  99.22 ( 99.49)
Epoch: [29][220/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.9844e-01 (3.8156e-01)	Acc@1  86.72 ( 86.82)	Acc@5 100.00 ( 99.50)
Epoch: [29][230/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.8364e-01 (3.8284e-01)	Acc@1  80.47 ( 86.74)	Acc@5  99.22 ( 99.51)
Epoch: [29][240/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.0835e-01 (3.8250e-01)	Acc@1  87.50 ( 86.74)	Acc@5 100.00 ( 99.52)
Epoch: [29][250/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.2080e-01 (3.8332e-01)	Acc@1  90.62 ( 86.75)	Acc@5 100.00 ( 99.53)
Epoch: [29][260/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.3896e-01 (3.8184e-01)	Acc@1  85.16 ( 86.81)	Acc@5 100.00 ( 99.53)
Epoch: [29][270/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.4277e-01 (3.8276e-01)	Acc@1  87.50 ( 86.79)	Acc@5 100.00 ( 99.52)
Epoch: [29][280/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.4067e-01 (3.8401e-01)	Acc@1  84.38 ( 86.74)	Acc@5  99.22 ( 99.52)
Epoch: [29][290/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.7891e-01 (3.8392e-01)	Acc@1  86.72 ( 86.75)	Acc@5 100.00 ( 99.52)
Epoch: [29][300/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.8389e-01 (3.8499e-01)	Acc@1  84.38 ( 86.70)	Acc@5  99.22 ( 99.51)
Epoch: [29][310/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.5947e-01 (3.8670e-01)	Acc@1  83.59 ( 86.66)	Acc@5  98.44 ( 99.50)
Epoch: [29][320/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.8037e-01 (3.8700e-01)	Acc@1  85.16 ( 86.61)	Acc@5  99.22 ( 99.51)
Epoch: [29][330/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.8340e-01 (3.8790e-01)	Acc@1  83.59 ( 86.57)	Acc@5 100.00 ( 99.51)
Epoch: [29][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.9834e-01 (3.8794e-01)	Acc@1  89.84 ( 86.56)	Acc@5  98.44 ( 99.52)
Epoch: [29][350/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.2935e-01 (3.8701e-01)	Acc@1  89.84 ( 86.59)	Acc@5 100.00 ( 99.52)
Epoch: [29][360/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.9990e-01 (3.8716e-01)	Acc@1  85.94 ( 86.58)	Acc@5 100.00 ( 99.53)
Epoch: [29][370/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.8174e-01 (3.8624e-01)	Acc@1  87.50 ( 86.61)	Acc@5 100.00 ( 99.53)
Epoch: [29][380/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.0591e-01 (3.8692e-01)	Acc@1  88.28 ( 86.58)	Acc@5 100.00 ( 99.53)
Epoch: [29][390/391]	Time  0.048 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.6318e-01 (3.8577e-01)	Acc@1  92.50 ( 86.64)	Acc@5  98.75 ( 99.53)
## e[29] optimizer.zero_grad (sum) time: 0.3860924243927002
## e[29]       loss.backward (sum) time: 7.215970039367676
## e[29]      optimizer.step (sum) time: 3.3990249633789062
## epoch[29] training(only) time: 25.465476036071777
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 4.4287e-01 (4.4287e-01)	Acc@1  84.00 ( 84.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.040)	Loss 5.8594e-01 (4.8197e-01)	Acc@1  83.00 ( 83.27)	Acc@5  99.00 ( 99.45)
Test: [ 20/100]	Time  0.026 ( 0.033)	Loss 4.9487e-01 (4.6944e-01)	Acc@1  82.00 ( 83.52)	Acc@5 100.00 ( 99.48)
Test: [ 30/100]	Time  0.024 ( 0.030)	Loss 4.5752e-01 (4.9063e-01)	Acc@1  83.00 ( 83.45)	Acc@5 100.00 ( 99.45)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 5.5322e-01 (5.0319e-01)	Acc@1  82.00 ( 83.12)	Acc@5  98.00 ( 99.32)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 4.3335e-01 (4.9819e-01)	Acc@1  82.00 ( 83.14)	Acc@5  99.00 ( 99.31)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 4.8926e-01 (4.9568e-01)	Acc@1  84.00 ( 83.10)	Acc@5  99.00 ( 99.34)
Test: [ 70/100]	Time  0.025 ( 0.027)	Loss 5.5469e-01 (4.9247e-01)	Acc@1  81.00 ( 83.20)	Acc@5  99.00 ( 99.35)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 3.9575e-01 (4.9128e-01)	Acc@1  87.00 ( 83.23)	Acc@5  99.00 ( 99.35)
Test: [ 90/100]	Time  0.027 ( 0.027)	Loss 5.5762e-01 (4.9022e-01)	Acc@1  82.00 ( 83.41)	Acc@5  99.00 ( 99.34)
 * Acc@1 83.610 Acc@5 99.370
### epoch[29] execution time: 28.256713390350342
EPOCH 30
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [30][  0/391]	Time  0.257 ( 0.257)	Data  0.173 ( 0.173)	Loss 3.0713e-01 (3.0713e-01)	Acc@1  88.28 ( 88.28)	Acc@5 100.00 (100.00)
Epoch: [30][ 10/391]	Time  0.066 ( 0.081)	Data  0.001 ( 0.017)	Loss 3.4424e-01 (3.3889e-01)	Acc@1  84.38 ( 87.64)	Acc@5 100.00 ( 99.72)
Epoch: [30][ 20/391]	Time  0.064 ( 0.074)	Data  0.001 ( 0.009)	Loss 3.3423e-01 (3.3659e-01)	Acc@1  90.62 ( 88.10)	Acc@5  99.22 ( 99.70)
Epoch: [30][ 30/391]	Time  0.065 ( 0.071)	Data  0.001 ( 0.007)	Loss 2.9639e-01 (3.2432e-01)	Acc@1  89.84 ( 88.63)	Acc@5 100.00 ( 99.72)
Epoch: [30][ 40/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.005)	Loss 1.8860e-01 (3.1862e-01)	Acc@1  92.97 ( 88.83)	Acc@5 100.00 ( 99.71)
Epoch: [30][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 3.0737e-01 (3.2079e-01)	Acc@1  89.06 ( 88.68)	Acc@5 100.00 ( 99.72)
Epoch: [30][ 60/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.004)	Loss 2.4829e-01 (3.1522e-01)	Acc@1  90.62 ( 88.88)	Acc@5 100.00 ( 99.74)
Epoch: [30][ 70/391]	Time  0.062 ( 0.067)	Data  0.002 ( 0.004)	Loss 3.2715e-01 (3.1134e-01)	Acc@1  88.28 ( 89.05)	Acc@5 100.00 ( 99.74)
Epoch: [30][ 80/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.6694e-01 (3.0940e-01)	Acc@1  86.72 ( 89.20)	Acc@5 100.00 ( 99.75)
Epoch: [30][ 90/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.9810e-01 (3.1029e-01)	Acc@1  89.06 ( 89.26)	Acc@5 100.00 ( 99.76)
Epoch: [30][100/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.3081e-01 (3.0757e-01)	Acc@1  92.97 ( 89.40)	Acc@5 100.00 ( 99.78)
Epoch: [30][110/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.1521e-01 (3.0921e-01)	Acc@1  92.19 ( 89.34)	Acc@5 100.00 ( 99.77)
Epoch: [30][120/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.7686e-01 (3.0825e-01)	Acc@1  89.84 ( 89.29)	Acc@5 100.00 ( 99.77)
Epoch: [30][130/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.1436e-01 (3.0561e-01)	Acc@1  92.97 ( 89.38)	Acc@5  99.22 ( 99.77)
Epoch: [30][140/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.6387e-01 (3.0623e-01)	Acc@1  84.38 ( 89.35)	Acc@5  99.22 ( 99.77)
Epoch: [30][150/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.0479e-01 (3.0631e-01)	Acc@1  87.50 ( 89.40)	Acc@5 100.00 ( 99.78)
Epoch: [30][160/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1274e-01 (3.0583e-01)	Acc@1  86.72 ( 89.41)	Acc@5 100.00 ( 99.78)
Epoch: [30][170/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.1699e-01 (3.0532e-01)	Acc@1  85.94 ( 89.42)	Acc@5  99.22 ( 99.77)
Epoch: [30][180/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7310e-01 (3.0264e-01)	Acc@1  92.97 ( 89.55)	Acc@5 100.00 ( 99.76)
Epoch: [30][190/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7383e-01 (3.0003e-01)	Acc@1  92.97 ( 89.62)	Acc@5 100.00 ( 99.77)
Epoch: [30][200/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.4243e-01 (2.9881e-01)	Acc@1  89.84 ( 89.66)	Acc@5 100.00 ( 99.76)
Epoch: [30][210/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.6562e-01 (2.9643e-01)	Acc@1  89.84 ( 89.76)	Acc@5 100.00 ( 99.77)
Epoch: [30][220/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2021e-01 (2.9465e-01)	Acc@1  94.53 ( 89.81)	Acc@5 100.00 ( 99.77)
Epoch: [30][230/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.7344e-01 (2.9394e-01)	Acc@1  86.72 ( 89.85)	Acc@5 100.00 ( 99.78)
Epoch: [30][240/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.6807e-01 (2.9116e-01)	Acc@1  89.06 ( 89.94)	Acc@5 100.00 ( 99.78)
Epoch: [30][250/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2791e-01 (2.9073e-01)	Acc@1  92.97 ( 89.96)	Acc@5  99.22 ( 99.78)
Epoch: [30][260/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.7173e-01 (2.8999e-01)	Acc@1  89.06 ( 90.00)	Acc@5 100.00 ( 99.78)
Epoch: [30][270/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.7100e-01 (2.8901e-01)	Acc@1  91.41 ( 90.05)	Acc@5  99.22 ( 99.78)
Epoch: [30][280/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.6123e-01 (2.8863e-01)	Acc@1  89.84 ( 90.06)	Acc@5 100.00 ( 99.78)
Epoch: [30][290/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2534e-01 (2.8862e-01)	Acc@1  90.62 ( 90.04)	Acc@5 100.00 ( 99.78)
Epoch: [30][300/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0203e-01 (2.8733e-01)	Acc@1  92.19 ( 90.09)	Acc@5 100.00 ( 99.78)
Epoch: [30][310/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3975e-01 (2.8542e-01)	Acc@1  89.06 ( 90.14)	Acc@5 100.00 ( 99.79)
Epoch: [30][320/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.5122e-01 (2.8414e-01)	Acc@1  88.28 ( 90.18)	Acc@5 100.00 ( 99.79)
Epoch: [30][330/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.7305e-01 (2.8397e-01)	Acc@1  88.28 ( 90.18)	Acc@5  99.22 ( 99.79)
Epoch: [30][340/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.4277e-01 (2.8355e-01)	Acc@1  89.06 ( 90.20)	Acc@5  99.22 ( 99.79)
Epoch: [30][350/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.4644e-01 (2.8457e-01)	Acc@1  89.06 ( 90.17)	Acc@5  99.22 ( 99.79)
Epoch: [30][360/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2705e-01 (2.8398e-01)	Acc@1  91.41 ( 90.22)	Acc@5  99.22 ( 99.79)
Epoch: [30][370/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9458e-01 (2.8371e-01)	Acc@1  92.97 ( 90.22)	Acc@5 100.00 ( 99.79)
Epoch: [30][380/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5906e-01 (2.8217e-01)	Acc@1  92.19 ( 90.27)	Acc@5 100.00 ( 99.79)
Epoch: [30][390/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.4548e-01 (2.8218e-01)	Acc@1  91.25 ( 90.29)	Acc@5 100.00 ( 99.79)
## e[30] optimizer.zero_grad (sum) time: 0.3856821060180664
## e[30]       loss.backward (sum) time: 7.209697961807251
## e[30]      optimizer.step (sum) time: 3.4284212589263916
## epoch[30] training(only) time: 25.483588695526123
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 2.4036e-01 (2.4036e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.027 ( 0.040)	Loss 3.5718e-01 (3.2222e-01)	Acc@1  89.00 ( 89.18)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 3.7671e-01 (3.2620e-01)	Acc@1  86.00 ( 89.14)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.024 ( 0.030)	Loss 3.5645e-01 (3.3780e-01)	Acc@1  86.00 ( 88.84)	Acc@5 100.00 ( 99.55)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 3.4546e-01 (3.4137e-01)	Acc@1  85.00 ( 88.61)	Acc@5 100.00 ( 99.51)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 2.3743e-01 (3.3799e-01)	Acc@1  92.00 ( 88.90)	Acc@5  99.00 ( 99.55)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 3.0591e-01 (3.3593e-01)	Acc@1  92.00 ( 88.92)	Acc@5 100.00 ( 99.59)
Test: [ 70/100]	Time  0.026 ( 0.027)	Loss 4.1406e-01 (3.3231e-01)	Acc@1  87.00 ( 89.08)	Acc@5 100.00 ( 99.59)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 2.1753e-01 (3.3180e-01)	Acc@1  92.00 ( 89.07)	Acc@5 100.00 ( 99.58)
Test: [ 90/100]	Time  0.027 ( 0.027)	Loss 2.2522e-01 (3.2993e-01)	Acc@1  91.00 ( 89.02)	Acc@5 100.00 ( 99.59)
 * Acc@1 89.110 Acc@5 99.610
### epoch[30] execution time: 28.306129217147827
EPOCH 31
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [31][  0/391]	Time  0.247 ( 0.247)	Data  0.181 ( 0.181)	Loss 2.4158e-01 (2.4158e-01)	Acc@1  91.41 ( 91.41)	Acc@5 100.00 (100.00)
Epoch: [31][ 10/391]	Time  0.064 ( 0.081)	Data  0.001 ( 0.017)	Loss 3.9185e-01 (2.4286e-01)	Acc@1  85.94 ( 91.69)	Acc@5  99.22 ( 99.86)
Epoch: [31][ 20/391]	Time  0.064 ( 0.073)	Data  0.001 ( 0.010)	Loss 2.4890e-01 (2.5896e-01)	Acc@1  89.84 ( 91.26)	Acc@5  99.22 ( 99.74)
Epoch: [31][ 30/391]	Time  0.063 ( 0.070)	Data  0.001 ( 0.007)	Loss 2.0117e-01 (2.5753e-01)	Acc@1  92.97 ( 91.28)	Acc@5 100.00 ( 99.77)
Epoch: [31][ 40/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.005)	Loss 2.1484e-01 (2.5431e-01)	Acc@1  92.19 ( 91.16)	Acc@5  99.22 ( 99.81)
Epoch: [31][ 50/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.005)	Loss 2.1265e-01 (2.5290e-01)	Acc@1  92.97 ( 91.33)	Acc@5 100.00 ( 99.80)
Epoch: [31][ 60/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.004)	Loss 2.5854e-01 (2.5048e-01)	Acc@1  91.41 ( 91.38)	Acc@5 100.00 ( 99.80)
Epoch: [31][ 70/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.004)	Loss 1.8945e-01 (2.5125e-01)	Acc@1  95.31 ( 91.41)	Acc@5 100.00 ( 99.80)
Epoch: [31][ 80/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.7954e-01 (2.5508e-01)	Acc@1  89.06 ( 91.18)	Acc@5 100.00 ( 99.82)
Epoch: [31][ 90/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.1631e-01 (2.5490e-01)	Acc@1  89.06 ( 91.23)	Acc@5 100.00 ( 99.82)
Epoch: [31][100/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.3669e-01 (2.5285e-01)	Acc@1  89.84 ( 91.24)	Acc@5 100.00 ( 99.82)
Epoch: [31][110/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.6011e-01 (2.5300e-01)	Acc@1  86.72 ( 91.18)	Acc@5 100.00 ( 99.84)
Epoch: [31][120/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.9468e-01 (2.5140e-01)	Acc@1  89.06 ( 91.23)	Acc@5  98.44 ( 99.83)
Epoch: [31][130/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.7246e-01 (2.5230e-01)	Acc@1  91.41 ( 91.19)	Acc@5 100.00 ( 99.83)
Epoch: [31][140/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.8003e-01 (2.5205e-01)	Acc@1  90.62 ( 91.18)	Acc@5 100.00 ( 99.83)
Epoch: [31][150/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.4570e-01 (2.5240e-01)	Acc@1  89.84 ( 91.17)	Acc@5 100.00 ( 99.83)
Epoch: [31][160/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.5708e-01 (2.5336e-01)	Acc@1  89.84 ( 91.17)	Acc@5  99.22 ( 99.83)
Epoch: [31][170/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3438e-01 (2.5151e-01)	Acc@1  90.62 ( 91.25)	Acc@5 100.00 ( 99.82)
Epoch: [31][180/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1191e-01 (2.5037e-01)	Acc@1  89.84 ( 91.27)	Acc@5 100.00 ( 99.82)
Epoch: [31][190/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8921e-01 (2.4932e-01)	Acc@1  93.75 ( 91.30)	Acc@5 100.00 ( 99.82)
Epoch: [31][200/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6699e-01 (2.4917e-01)	Acc@1  93.75 ( 91.32)	Acc@5 100.00 ( 99.82)
Epoch: [31][210/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.4500e-01 (2.4841e-01)	Acc@1  90.62 ( 91.34)	Acc@5 100.00 ( 99.82)
Epoch: [31][220/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8420e-01 (2.4775e-01)	Acc@1  93.75 ( 91.32)	Acc@5 100.00 ( 99.83)
Epoch: [31][230/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5906e-01 (2.4770e-01)	Acc@1  96.09 ( 91.36)	Acc@5 100.00 ( 99.82)
Epoch: [31][240/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2363e-01 (2.4858e-01)	Acc@1  92.97 ( 91.34)	Acc@5 100.00 ( 99.82)
Epoch: [31][250/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.0811e-01 (2.4852e-01)	Acc@1  89.06 ( 91.33)	Acc@5  99.22 ( 99.82)
Epoch: [31][260/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.8833e-01 (2.4880e-01)	Acc@1  92.19 ( 91.34)	Acc@5  98.44 ( 99.82)
Epoch: [31][270/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0984e-01 (2.4834e-01)	Acc@1  90.62 ( 91.32)	Acc@5 100.00 ( 99.82)
Epoch: [31][280/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2083e-01 (2.4872e-01)	Acc@1  91.41 ( 91.30)	Acc@5 100.00 ( 99.82)
Epoch: [31][290/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2070e-01 (2.4788e-01)	Acc@1  92.97 ( 91.34)	Acc@5  99.22 ( 99.82)
Epoch: [31][300/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.9346e-01 (2.4837e-01)	Acc@1  89.84 ( 91.32)	Acc@5 100.00 ( 99.82)
Epoch: [31][310/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2693e-01 (2.4740e-01)	Acc@1  92.19 ( 91.37)	Acc@5 100.00 ( 99.82)
Epoch: [31][320/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8408e-01 (2.4690e-01)	Acc@1  94.53 ( 91.40)	Acc@5 100.00 ( 99.82)
Epoch: [31][330/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0520e-01 (2.4722e-01)	Acc@1  91.41 ( 91.37)	Acc@5 100.00 ( 99.82)
Epoch: [31][340/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0691e-01 (2.4714e-01)	Acc@1  94.53 ( 91.39)	Acc@5 100.00 ( 99.82)
Epoch: [31][350/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.8589e-01 (2.4688e-01)	Acc@1  92.19 ( 91.39)	Acc@5 100.00 ( 99.82)
Epoch: [31][360/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.2104e-01 (2.4681e-01)	Acc@1  89.06 ( 91.39)	Acc@5 100.00 ( 99.83)
Epoch: [31][370/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.0298e-01 (2.4616e-01)	Acc@1  91.41 ( 91.44)	Acc@5 100.00 ( 99.83)
Epoch: [31][380/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3196e-01 (2.4531e-01)	Acc@1  96.09 ( 91.46)	Acc@5 100.00 ( 99.83)
Epoch: [31][390/391]	Time  0.047 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.7539e-01 (2.4475e-01)	Acc@1  90.00 ( 91.48)	Acc@5 100.00 ( 99.83)
## e[31] optimizer.zero_grad (sum) time: 0.3856842517852783
## e[31]       loss.backward (sum) time: 7.237596750259399
## e[31]      optimizer.step (sum) time: 3.4129695892333984
## epoch[31] training(only) time: 25.49992275238037
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 2.4316e-01 (2.4316e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.039)	Loss 3.7036e-01 (3.1168e-01)	Acc@1  88.00 ( 89.36)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 3.6768e-01 (3.1755e-01)	Acc@1  87.00 ( 89.29)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.024 ( 0.030)	Loss 3.2056e-01 (3.2861e-01)	Acc@1  85.00 ( 89.10)	Acc@5 100.00 ( 99.58)
Test: [ 40/100]	Time  0.027 ( 0.029)	Loss 3.5474e-01 (3.3188e-01)	Acc@1  83.00 ( 88.71)	Acc@5 100.00 ( 99.51)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 2.4036e-01 (3.2784e-01)	Acc@1  91.00 ( 89.06)	Acc@5  99.00 ( 99.55)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 2.7856e-01 (3.2632e-01)	Acc@1  92.00 ( 89.15)	Acc@5 100.00 ( 99.61)
Test: [ 70/100]	Time  0.024 ( 0.027)	Loss 4.1357e-01 (3.2360e-01)	Acc@1  88.00 ( 89.25)	Acc@5  99.00 ( 99.62)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 1.8848e-01 (3.2347e-01)	Acc@1  93.00 ( 89.15)	Acc@5 100.00 ( 99.60)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 2.2253e-01 (3.2131e-01)	Acc@1  91.00 ( 89.13)	Acc@5 100.00 ( 99.62)
 * Acc@1 89.190 Acc@5 99.630
### epoch[31] execution time: 28.278332948684692
EPOCH 32
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [32][  0/391]	Time  0.252 ( 0.252)	Data  0.187 ( 0.187)	Loss 1.4734e-01 (1.4734e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [32][ 10/391]	Time  0.072 ( 0.082)	Data  0.001 ( 0.018)	Loss 2.5562e-01 (2.0477e-01)	Acc@1  92.19 ( 93.04)	Acc@5 100.00 (100.00)
Epoch: [32][ 20/391]	Time  0.064 ( 0.074)	Data  0.001 ( 0.010)	Loss 2.0056e-01 (2.0431e-01)	Acc@1  92.19 ( 92.67)	Acc@5 100.00 ( 99.96)
Epoch: [32][ 30/391]	Time  0.064 ( 0.071)	Data  0.001 ( 0.007)	Loss 2.7588e-01 (2.0622e-01)	Acc@1  91.41 ( 92.72)	Acc@5 100.00 ( 99.97)
Epoch: [32][ 40/391]	Time  0.063 ( 0.070)	Data  0.001 ( 0.006)	Loss 3.2739e-01 (2.1581e-01)	Acc@1  86.72 ( 92.15)	Acc@5 100.00 ( 99.94)
Epoch: [32][ 50/391]	Time  0.071 ( 0.069)	Data  0.001 ( 0.005)	Loss 1.9946e-01 (2.1861e-01)	Acc@1  92.97 ( 92.14)	Acc@5 100.00 ( 99.95)
Epoch: [32][ 60/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 2.2034e-01 (2.2118e-01)	Acc@1  92.97 ( 92.14)	Acc@5 100.00 ( 99.96)
Epoch: [32][ 70/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.004)	Loss 3.2202e-01 (2.3031e-01)	Acc@1  91.41 ( 91.85)	Acc@5 100.00 ( 99.96)
Epoch: [32][ 80/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.003)	Loss 2.6880e-01 (2.3221e-01)	Acc@1  88.28 ( 91.75)	Acc@5 100.00 ( 99.94)
Epoch: [32][ 90/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.9824e-01 (2.3232e-01)	Acc@1  91.41 ( 91.76)	Acc@5 100.00 ( 99.93)
Epoch: [32][100/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.6514e-01 (2.3298e-01)	Acc@1  88.28 ( 91.65)	Acc@5 100.00 ( 99.92)
Epoch: [32][110/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.8469e-01 (2.3343e-01)	Acc@1  94.53 ( 91.69)	Acc@5 100.00 ( 99.91)
Epoch: [32][120/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.3132e-01 (2.3424e-01)	Acc@1  92.19 ( 91.65)	Acc@5 100.00 ( 99.91)
Epoch: [32][130/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.9031e-01 (2.3659e-01)	Acc@1  92.19 ( 91.52)	Acc@5 100.00 ( 99.90)
Epoch: [32][140/391]	Time  0.071 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0469e-01 (2.3609e-01)	Acc@1  87.50 ( 91.53)	Acc@5 100.00 ( 99.90)
Epoch: [32][150/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.5220e-01 (2.3490e-01)	Acc@1  91.41 ( 91.56)	Acc@5 100.00 ( 99.90)
Epoch: [32][160/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.7002e-01 (2.3482e-01)	Acc@1  90.62 ( 91.62)	Acc@5 100.00 ( 99.88)
Epoch: [32][170/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1030e-01 (2.3669e-01)	Acc@1  89.06 ( 91.56)	Acc@5 100.00 ( 99.88)
Epoch: [32][180/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9287e-01 (2.3641e-01)	Acc@1  91.41 ( 91.58)	Acc@5 100.00 ( 99.87)
Epoch: [32][190/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.1448e-01 (2.3485e-01)	Acc@1  90.62 ( 91.59)	Acc@5 100.00 ( 99.88)
Epoch: [32][200/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8457e-01 (2.3490e-01)	Acc@1  95.31 ( 91.60)	Acc@5 100.00 ( 99.88)
Epoch: [32][210/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.0813e-01 (2.3535e-01)	Acc@1  92.19 ( 91.62)	Acc@5 100.00 ( 99.87)
Epoch: [32][220/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9434e-01 (2.3465e-01)	Acc@1  94.53 ( 91.64)	Acc@5  99.22 ( 99.87)
Epoch: [32][230/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.6099e-01 (2.3299e-01)	Acc@1  92.97 ( 91.70)	Acc@5 100.00 ( 99.87)
Epoch: [32][240/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.2827e-01 (2.3242e-01)	Acc@1  92.19 ( 91.76)	Acc@5 100.00 ( 99.88)
Epoch: [32][250/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.0312e-01 (2.3187e-01)	Acc@1  93.75 ( 91.76)	Acc@5 100.00 ( 99.87)
Epoch: [32][260/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0396e-01 (2.3199e-01)	Acc@1  89.84 ( 91.77)	Acc@5  99.22 ( 99.87)
Epoch: [32][270/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6626e-01 (2.3316e-01)	Acc@1  93.75 ( 91.76)	Acc@5 100.00 ( 99.86)
Epoch: [32][280/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3855e-01 (2.3225e-01)	Acc@1  96.09 ( 91.81)	Acc@5 100.00 ( 99.85)
Epoch: [32][290/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.0190e-01 (2.3212e-01)	Acc@1  92.97 ( 91.82)	Acc@5 100.00 ( 99.85)
Epoch: [32][300/391]	Time  0.060 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.0117e-01 (2.3139e-01)	Acc@1  91.41 ( 91.83)	Acc@5  99.22 ( 99.85)
Epoch: [32][310/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7114e-01 (2.3099e-01)	Acc@1  95.31 ( 91.85)	Acc@5 100.00 ( 99.85)
Epoch: [32][320/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5693e-01 (2.3052e-01)	Acc@1  86.72 ( 91.87)	Acc@5 100.00 ( 99.86)
Epoch: [32][330/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0933e-01 (2.3152e-01)	Acc@1  89.84 ( 91.84)	Acc@5 100.00 ( 99.85)
Epoch: [32][340/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7017e-01 (2.3104e-01)	Acc@1  93.75 ( 91.85)	Acc@5 100.00 ( 99.85)
Epoch: [32][350/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.1448e-01 (2.3167e-01)	Acc@1  91.41 ( 91.84)	Acc@5 100.00 ( 99.85)
Epoch: [32][360/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.7051e-01 (2.3158e-01)	Acc@1  94.53 ( 91.86)	Acc@5 100.00 ( 99.85)
Epoch: [32][370/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.9883e-01 (2.3230e-01)	Acc@1  87.50 ( 91.85)	Acc@5  99.22 ( 99.84)
Epoch: [32][380/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4929e-01 (2.3187e-01)	Acc@1  95.31 ( 91.87)	Acc@5 100.00 ( 99.85)
Epoch: [32][390/391]	Time  0.049 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.1006e-01 (2.3203e-01)	Acc@1  92.50 ( 91.87)	Acc@5 100.00 ( 99.85)
## e[32] optimizer.zero_grad (sum) time: 0.39048218727111816
## e[32]       loss.backward (sum) time: 7.304015159606934
## e[32]      optimizer.step (sum) time: 3.489061117172241
## epoch[32] training(only) time: 25.723212957382202
# Switched to evaluate mode...
Test: [  0/100]	Time  0.192 ( 0.192)	Loss 2.3389e-01 (2.3389e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.027 ( 0.040)	Loss 3.9819e-01 (3.0958e-01)	Acc@1  90.00 ( 90.09)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.024 ( 0.034)	Loss 3.7500e-01 (3.1370e-01)	Acc@1  85.00 ( 89.52)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 3.0811e-01 (3.2292e-01)	Acc@1  87.00 ( 89.35)	Acc@5 100.00 ( 99.52)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 3.5278e-01 (3.2452e-01)	Acc@1  86.00 ( 89.32)	Acc@5 100.00 ( 99.49)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 2.4939e-01 (3.2279e-01)	Acc@1  87.00 ( 89.25)	Acc@5  99.00 ( 99.51)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 2.3389e-01 (3.2083e-01)	Acc@1  94.00 ( 89.34)	Acc@5 100.00 ( 99.57)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 4.1089e-01 (3.1872e-01)	Acc@1  87.00 ( 89.48)	Acc@5 100.00 ( 99.62)
Test: [ 80/100]	Time  0.030 ( 0.027)	Loss 2.0654e-01 (3.1953e-01)	Acc@1  92.00 ( 89.36)	Acc@5 100.00 ( 99.62)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 2.3425e-01 (3.1860e-01)	Acc@1  92.00 ( 89.36)	Acc@5 100.00 ( 99.64)
 * Acc@1 89.420 Acc@5 99.650
### epoch[32] execution time: 28.529369831085205
EPOCH 33
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [33][  0/391]	Time  0.245 ( 0.245)	Data  0.182 ( 0.182)	Loss 2.0178e-01 (2.0178e-01)	Acc@1  95.31 ( 95.31)	Acc@5  98.44 ( 98.44)
Epoch: [33][ 10/391]	Time  0.065 ( 0.082)	Data  0.001 ( 0.017)	Loss 2.9224e-01 (2.0145e-01)	Acc@1  89.84 ( 93.18)	Acc@5 100.00 ( 99.79)
Epoch: [33][ 20/391]	Time  0.064 ( 0.074)	Data  0.001 ( 0.010)	Loss 2.1985e-01 (2.2412e-01)	Acc@1  91.41 ( 92.60)	Acc@5 100.00 ( 99.70)
Epoch: [33][ 30/391]	Time  0.064 ( 0.071)	Data  0.001 ( 0.007)	Loss 2.4609e-01 (2.2841e-01)	Acc@1  91.41 ( 92.16)	Acc@5  98.44 ( 99.72)
Epoch: [33][ 40/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.006)	Loss 1.9495e-01 (2.1839e-01)	Acc@1  92.19 ( 92.45)	Acc@5 100.00 ( 99.77)
Epoch: [33][ 50/391]	Time  0.061 ( 0.068)	Data  0.001 ( 0.005)	Loss 1.4050e-01 (2.2017e-01)	Acc@1  94.53 ( 92.20)	Acc@5 100.00 ( 99.82)
Epoch: [33][ 60/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.004)	Loss 2.3450e-01 (2.2378e-01)	Acc@1  88.28 ( 91.93)	Acc@5 100.00 ( 99.80)
Epoch: [33][ 70/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.004)	Loss 2.0422e-01 (2.2421e-01)	Acc@1  90.62 ( 91.92)	Acc@5 100.00 ( 99.81)
Epoch: [33][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.4023e-01 (2.2697e-01)	Acc@1  92.19 ( 91.85)	Acc@5 100.00 ( 99.82)
Epoch: [33][ 90/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.9189e-01 (2.2505e-01)	Acc@1  96.09 ( 91.87)	Acc@5 100.00 ( 99.83)
Epoch: [33][100/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.2961e-01 (2.2470e-01)	Acc@1  91.41 ( 91.92)	Acc@5 100.00 ( 99.82)
Epoch: [33][110/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.9446e-01 (2.2666e-01)	Acc@1  93.75 ( 91.93)	Acc@5 100.00 ( 99.80)
Epoch: [33][120/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.6123e-01 (2.2372e-01)	Acc@1  90.62 ( 92.04)	Acc@5 100.00 ( 99.80)
Epoch: [33][130/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0396e-01 (2.2294e-01)	Acc@1  87.50 ( 92.13)	Acc@5 100.00 ( 99.80)
Epoch: [33][140/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.6465e-01 (2.2298e-01)	Acc@1  89.84 ( 92.17)	Acc@5  99.22 ( 99.80)
Epoch: [33][150/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.2241e-01 (2.2209e-01)	Acc@1  92.19 ( 92.24)	Acc@5 100.00 ( 99.80)
Epoch: [33][160/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7041e-01 (2.2152e-01)	Acc@1  95.31 ( 92.28)	Acc@5 100.00 ( 99.81)
Epoch: [33][170/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9678e-01 (2.2194e-01)	Acc@1  93.75 ( 92.28)	Acc@5 100.00 ( 99.81)
Epoch: [33][180/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.6099e-01 (2.2184e-01)	Acc@1  88.28 ( 92.28)	Acc@5 100.00 ( 99.81)
Epoch: [33][190/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.5928e-01 (2.2155e-01)	Acc@1  92.19 ( 92.31)	Acc@5  99.22 ( 99.81)
Epoch: [33][200/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.7686e-01 (2.2286e-01)	Acc@1  89.06 ( 92.26)	Acc@5 100.00 ( 99.81)
Epoch: [33][210/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9421e-01 (2.2329e-01)	Acc@1  93.75 ( 92.21)	Acc@5 100.00 ( 99.81)
Epoch: [33][220/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.1899e-01 (2.2238e-01)	Acc@1  92.97 ( 92.25)	Acc@5 100.00 ( 99.81)
Epoch: [33][230/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9885e-01 (2.2239e-01)	Acc@1  89.84 ( 92.23)	Acc@5 100.00 ( 99.82)
Epoch: [33][240/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.1191e-01 (2.2224e-01)	Acc@1  93.75 ( 92.28)	Acc@5  99.22 ( 99.82)
Epoch: [33][250/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.7515e-01 (2.2184e-01)	Acc@1  91.41 ( 92.31)	Acc@5 100.00 ( 99.82)
Epoch: [33][260/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.3962e-01 (2.2204e-01)	Acc@1  92.19 ( 92.30)	Acc@5 100.00 ( 99.83)
Epoch: [33][270/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7310e-01 (2.2090e-01)	Acc@1  93.75 ( 92.33)	Acc@5 100.00 ( 99.83)
Epoch: [33][280/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.2300e-01 (2.2021e-01)	Acc@1  91.41 ( 92.36)	Acc@5 100.00 ( 99.83)
Epoch: [33][290/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8787e-01 (2.2000e-01)	Acc@1  92.19 ( 92.38)	Acc@5 100.00 ( 99.84)
Epoch: [33][300/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9836e-01 (2.1987e-01)	Acc@1  92.97 ( 92.38)	Acc@5 100.00 ( 99.84)
Epoch: [33][310/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9238e-01 (2.1961e-01)	Acc@1  93.75 ( 92.40)	Acc@5 100.00 ( 99.84)
Epoch: [33][320/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2424e-01 (2.2081e-01)	Acc@1  94.53 ( 92.38)	Acc@5 100.00 ( 99.83)
Epoch: [33][330/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3315e-01 (2.1997e-01)	Acc@1  92.97 ( 92.39)	Acc@5 100.00 ( 99.84)
Epoch: [33][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9519e-01 (2.1970e-01)	Acc@1  93.75 ( 92.42)	Acc@5 100.00 ( 99.84)
Epoch: [33][350/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.4243e-01 (2.2004e-01)	Acc@1  90.62 ( 92.40)	Acc@5 100.00 ( 99.85)
Epoch: [33][360/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.7720e-01 (2.1998e-01)	Acc@1  85.16 ( 92.38)	Acc@5 100.00 ( 99.85)
Epoch: [33][370/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.4487e-01 (2.2006e-01)	Acc@1  92.97 ( 92.37)	Acc@5  99.22 ( 99.85)
Epoch: [33][380/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7273e-01 (2.2023e-01)	Acc@1  93.75 ( 92.35)	Acc@5 100.00 ( 99.85)
Epoch: [33][390/391]	Time  0.052 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.8589e-01 (2.1957e-01)	Acc@1  90.00 ( 92.37)	Acc@5 100.00 ( 99.85)
## e[33] optimizer.zero_grad (sum) time: 0.38634157180786133
## e[33]       loss.backward (sum) time: 7.261350870132446
## e[33]      optimizer.step (sum) time: 3.468534469604492
## epoch[33] training(only) time: 25.582541704177856
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 2.6831e-01 (2.6831e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.039)	Loss 3.7183e-01 (3.0164e-01)	Acc@1  89.00 ( 89.73)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.024 ( 0.032)	Loss 3.3716e-01 (3.0640e-01)	Acc@1  85.00 ( 89.67)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.027 ( 0.030)	Loss 2.9541e-01 (3.1482e-01)	Acc@1  85.00 ( 89.61)	Acc@5 100.00 ( 99.52)
Test: [ 40/100]	Time  0.027 ( 0.029)	Loss 3.3032e-01 (3.1775e-01)	Acc@1  87.00 ( 89.56)	Acc@5 100.00 ( 99.49)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 2.1716e-01 (3.1391e-01)	Acc@1  91.00 ( 89.67)	Acc@5  99.00 ( 99.49)
Test: [ 60/100]	Time  0.024 ( 0.027)	Loss 2.3523e-01 (3.1289e-01)	Acc@1  91.00 ( 89.64)	Acc@5 100.00 ( 99.54)
Test: [ 70/100]	Time  0.024 ( 0.027)	Loss 4.1089e-01 (3.1085e-01)	Acc@1  87.00 ( 89.79)	Acc@5 100.00 ( 99.54)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 2.0911e-01 (3.1154e-01)	Acc@1  92.00 ( 89.67)	Acc@5 100.00 ( 99.53)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 1.9604e-01 (3.0891e-01)	Acc@1  92.00 ( 89.65)	Acc@5 100.00 ( 99.54)
 * Acc@1 89.780 Acc@5 99.550
### epoch[33] execution time: 28.37453603744507
EPOCH 34
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [34][  0/391]	Time  0.241 ( 0.241)	Data  0.175 ( 0.175)	Loss 2.3071e-01 (2.3071e-01)	Acc@1  91.41 ( 91.41)	Acc@5 100.00 (100.00)
Epoch: [34][ 10/391]	Time  0.065 ( 0.079)	Data  0.001 ( 0.017)	Loss 2.3535e-01 (1.9795e-01)	Acc@1  92.19 ( 93.11)	Acc@5 100.00 ( 99.93)
Epoch: [34][ 20/391]	Time  0.064 ( 0.072)	Data  0.001 ( 0.009)	Loss 2.3682e-01 (2.0423e-01)	Acc@1  90.62 ( 92.82)	Acc@5 100.00 ( 99.89)
Epoch: [34][ 30/391]	Time  0.063 ( 0.070)	Data  0.001 ( 0.007)	Loss 2.2583e-01 (2.0614e-01)	Acc@1  92.97 ( 92.84)	Acc@5  99.22 ( 99.85)
Epoch: [34][ 40/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.005)	Loss 1.6846e-01 (2.0782e-01)	Acc@1  93.75 ( 92.64)	Acc@5 100.00 ( 99.83)
Epoch: [34][ 50/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.005)	Loss 2.4719e-01 (2.1299e-01)	Acc@1  91.41 ( 92.48)	Acc@5 100.00 ( 99.85)
Epoch: [34][ 60/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 2.9541e-01 (2.0999e-01)	Acc@1  89.84 ( 92.61)	Acc@5 100.00 ( 99.85)
Epoch: [34][ 70/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.5247e-01 (2.1045e-01)	Acc@1  93.75 ( 92.65)	Acc@5 100.00 ( 99.85)
Epoch: [34][ 80/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.4182e-01 (2.0979e-01)	Acc@1  90.62 ( 92.71)	Acc@5 100.00 ( 99.85)
Epoch: [34][ 90/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.3242e-01 (2.1268e-01)	Acc@1  95.31 ( 92.61)	Acc@5 100.00 ( 99.84)
Epoch: [34][100/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.3391e-01 (2.1320e-01)	Acc@1  95.31 ( 92.67)	Acc@5 100.00 ( 99.85)
Epoch: [34][110/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.1226e-01 (2.1432e-01)	Acc@1  90.62 ( 92.69)	Acc@5  99.22 ( 99.85)
Epoch: [34][120/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.7222e-01 (2.1353e-01)	Acc@1  92.19 ( 92.71)	Acc@5  99.22 ( 99.85)
Epoch: [34][130/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9336e-01 (2.1202e-01)	Acc@1  90.62 ( 92.77)	Acc@5 100.00 ( 99.85)
Epoch: [34][140/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.7710e-01 (2.1073e-01)	Acc@1  89.06 ( 92.77)	Acc@5 100.00 ( 99.86)
Epoch: [34][150/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1470e-01 (2.1103e-01)	Acc@1  89.06 ( 92.72)	Acc@5 100.00 ( 99.86)
Epoch: [34][160/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8005e-01 (2.1359e-01)	Acc@1  94.53 ( 92.61)	Acc@5 100.00 ( 99.84)
Epoch: [34][170/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.7476e-01 (2.1415e-01)	Acc@1  86.72 ( 92.60)	Acc@5 100.00 ( 99.84)
Epoch: [34][180/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.0544e-01 (2.1652e-01)	Acc@1  94.53 ( 92.56)	Acc@5 100.00 ( 99.84)
Epoch: [34][190/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.0020e-01 (2.1754e-01)	Acc@1  92.19 ( 92.51)	Acc@5 100.00 ( 99.84)
Epoch: [34][200/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.1155e-01 (2.1560e-01)	Acc@1  92.19 ( 92.58)	Acc@5 100.00 ( 99.85)
Epoch: [34][210/391]	Time  0.074 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7432e-01 (2.1589e-01)	Acc@1  94.53 ( 92.60)	Acc@5 100.00 ( 99.86)
Epoch: [34][220/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.4524e-01 (2.1801e-01)	Acc@1  89.84 ( 92.53)	Acc@5 100.00 ( 99.86)
Epoch: [34][230/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.5049e-01 (2.1844e-01)	Acc@1  91.41 ( 92.51)	Acc@5 100.00 ( 99.86)
Epoch: [34][240/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3086e-01 (2.1823e-01)	Acc@1  96.09 ( 92.51)	Acc@5 100.00 ( 99.85)
Epoch: [34][250/391]	Time  0.072 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2693e-01 (2.1826e-01)	Acc@1  92.19 ( 92.48)	Acc@5 100.00 ( 99.86)
Epoch: [34][260/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7786e-01 (2.1701e-01)	Acc@1  91.41 ( 92.50)	Acc@5 100.00 ( 99.87)
Epoch: [34][270/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2791e-01 (2.1738e-01)	Acc@1  93.75 ( 92.51)	Acc@5 100.00 ( 99.87)
Epoch: [34][280/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.1113e-01 (2.1794e-01)	Acc@1  87.50 ( 92.49)	Acc@5  99.22 ( 99.87)
Epoch: [34][290/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7798e-01 (2.1667e-01)	Acc@1  93.75 ( 92.51)	Acc@5 100.00 ( 99.87)
Epoch: [34][300/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.7417e-01 (2.1606e-01)	Acc@1  89.84 ( 92.51)	Acc@5 100.00 ( 99.87)
Epoch: [34][310/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3047e-01 (2.1596e-01)	Acc@1  91.41 ( 92.51)	Acc@5 100.00 ( 99.87)
Epoch: [34][320/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2192e-01 (2.1553e-01)	Acc@1  93.75 ( 92.52)	Acc@5 100.00 ( 99.87)
Epoch: [34][330/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1899e-01 (2.1438e-01)	Acc@1  92.97 ( 92.58)	Acc@5 100.00 ( 99.87)
Epoch: [34][340/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.2227e-01 (2.1413e-01)	Acc@1  91.41 ( 92.59)	Acc@5 100.00 ( 99.86)
Epoch: [34][350/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6785e-01 (2.1359e-01)	Acc@1  93.75 ( 92.61)	Acc@5 100.00 ( 99.87)
Epoch: [34][360/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0544e-01 (2.1440e-01)	Acc@1  91.41 ( 92.59)	Acc@5 100.00 ( 99.86)
Epoch: [34][370/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2021e-01 (2.1443e-01)	Acc@1  93.75 ( 92.61)	Acc@5  99.22 ( 99.86)
Epoch: [34][380/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1875e-01 (2.1384e-01)	Acc@1  93.75 ( 92.62)	Acc@5 100.00 ( 99.86)
Epoch: [34][390/391]	Time  0.048 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.4219e-01 (2.1388e-01)	Acc@1  91.25 ( 92.63)	Acc@5 100.00 ( 99.86)
## e[34] optimizer.zero_grad (sum) time: 0.38507509231567383
## e[34]       loss.backward (sum) time: 7.2305474281311035
## e[34]      optimizer.step (sum) time: 3.4646434783935547
## epoch[34] training(only) time: 25.54000234603882
# Switched to evaluate mode...
Test: [  0/100]	Time  0.181 ( 0.181)	Loss 2.4597e-01 (2.4597e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.040)	Loss 3.7549e-01 (2.9534e-01)	Acc@1  90.00 ( 90.18)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 3.4888e-01 (3.0179e-01)	Acc@1  86.00 ( 89.95)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.024 ( 0.030)	Loss 3.0127e-01 (3.1350e-01)	Acc@1  86.00 ( 89.90)	Acc@5 100.00 ( 99.52)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 3.5815e-01 (3.1563e-01)	Acc@1  88.00 ( 89.73)	Acc@5 100.00 ( 99.51)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 2.4487e-01 (3.1488e-01)	Acc@1  89.00 ( 89.80)	Acc@5  99.00 ( 99.53)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 2.0789e-01 (3.1149e-01)	Acc@1  95.00 ( 89.92)	Acc@5 100.00 ( 99.59)
Test: [ 70/100]	Time  0.024 ( 0.027)	Loss 4.2139e-01 (3.0857e-01)	Acc@1  87.00 ( 89.97)	Acc@5 100.00 ( 99.62)
Test: [ 80/100]	Time  0.025 ( 0.027)	Loss 2.2485e-01 (3.0967e-01)	Acc@1  91.00 ( 89.89)	Acc@5 100.00 ( 99.62)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 2.0398e-01 (3.0846e-01)	Acc@1  91.00 ( 89.85)	Acc@5 100.00 ( 99.62)
 * Acc@1 89.870 Acc@5 99.620
### epoch[34] execution time: 28.372039079666138
EPOCH 35
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [35][  0/391]	Time  0.244 ( 0.244)	Data  0.180 ( 0.180)	Loss 1.4746e-01 (1.4746e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
Epoch: [35][ 10/391]	Time  0.066 ( 0.080)	Data  0.001 ( 0.017)	Loss 1.6833e-01 (2.0135e-01)	Acc@1  92.19 ( 93.18)	Acc@5 100.00 ( 99.93)
Epoch: [35][ 20/391]	Time  0.063 ( 0.073)	Data  0.001 ( 0.010)	Loss 1.6003e-01 (1.9936e-01)	Acc@1  92.97 ( 93.08)	Acc@5 100.00 ( 99.93)
Epoch: [35][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.007)	Loss 2.2925e-01 (2.0364e-01)	Acc@1  94.53 ( 92.99)	Acc@5 100.00 ( 99.92)
Epoch: [35][ 40/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.005)	Loss 2.2070e-01 (1.9779e-01)	Acc@1  91.41 ( 93.16)	Acc@5 100.00 ( 99.90)
Epoch: [35][ 50/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.005)	Loss 2.0093e-01 (2.0101e-01)	Acc@1  91.41 ( 92.95)	Acc@5 100.00 ( 99.91)
Epoch: [35][ 60/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.4368e-01 (2.0302e-01)	Acc@1  94.53 ( 92.92)	Acc@5  99.22 ( 99.88)
Epoch: [35][ 70/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.6138e-01 (2.0223e-01)	Acc@1  93.75 ( 92.91)	Acc@5 100.00 ( 99.88)
Epoch: [35][ 80/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.3892e-01 (2.0131e-01)	Acc@1  94.53 ( 92.94)	Acc@5 100.00 ( 99.88)
Epoch: [35][ 90/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.8677e-01 (2.0458e-01)	Acc@1  94.53 ( 92.93)	Acc@5 100.00 ( 99.88)
Epoch: [35][100/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.3572e-01 (2.0619e-01)	Acc@1  88.28 ( 92.81)	Acc@5 100.00 ( 99.89)
Epoch: [35][110/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.1826e-01 (2.0523e-01)	Acc@1  90.62 ( 92.85)	Acc@5 100.00 ( 99.89)
Epoch: [35][120/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.4880e-01 (2.0521e-01)	Acc@1  96.09 ( 92.92)	Acc@5 100.00 ( 99.88)
Epoch: [35][130/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.0190e-01 (2.0533e-01)	Acc@1  92.97 ( 92.87)	Acc@5 100.00 ( 99.89)
Epoch: [35][140/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5845e-01 (2.0610e-01)	Acc@1  95.31 ( 92.89)	Acc@5 100.00 ( 99.88)
Epoch: [35][150/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9507e-01 (2.0657e-01)	Acc@1  92.97 ( 92.91)	Acc@5 100.00 ( 99.88)
Epoch: [35][160/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.1790e-01 (2.0754e-01)	Acc@1  92.97 ( 92.87)	Acc@5 100.00 ( 99.89)
Epoch: [35][170/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6479e-01 (2.0659e-01)	Acc@1  92.97 ( 92.92)	Acc@5 100.00 ( 99.89)
Epoch: [35][180/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3904e-01 (2.0547e-01)	Acc@1  95.31 ( 92.94)	Acc@5 100.00 ( 99.89)
Epoch: [35][190/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.6025e-01 (2.0509e-01)	Acc@1  92.19 ( 92.96)	Acc@5 100.00 ( 99.89)
Epoch: [35][200/391]	Time  0.060 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5735e-01 (2.0585e-01)	Acc@1  93.75 ( 92.92)	Acc@5 100.00 ( 99.90)
Epoch: [35][210/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.1924e-01 (2.0498e-01)	Acc@1  91.41 ( 92.94)	Acc@5 100.00 ( 99.90)
Epoch: [35][220/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.1106e-01 (2.0456e-01)	Acc@1  92.19 ( 92.96)	Acc@5 100.00 ( 99.89)
Epoch: [35][230/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9580e-01 (2.0472e-01)	Acc@1  92.97 ( 92.93)	Acc@5 100.00 ( 99.90)
Epoch: [35][240/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.5098e-01 (2.0580e-01)	Acc@1  92.19 ( 92.91)	Acc@5 100.00 ( 99.90)
Epoch: [35][250/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6357e-01 (2.0536e-01)	Acc@1  95.31 ( 92.93)	Acc@5 100.00 ( 99.89)
Epoch: [35][260/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.1765e-01 (2.0670e-01)	Acc@1  91.41 ( 92.85)	Acc@5 100.00 ( 99.89)
Epoch: [35][270/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3472e-01 (2.0826e-01)	Acc@1  87.50 ( 92.82)	Acc@5  98.44 ( 99.88)
Epoch: [35][280/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8018e-01 (2.0907e-01)	Acc@1  93.75 ( 92.82)	Acc@5 100.00 ( 99.87)
Epoch: [35][290/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0630e-01 (2.0953e-01)	Acc@1  92.19 ( 92.81)	Acc@5 100.00 ( 99.87)
Epoch: [35][300/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.0566e-01 (2.0938e-01)	Acc@1  85.94 ( 92.81)	Acc@5 100.00 ( 99.87)
Epoch: [35][310/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.5439e-01 (2.1095e-01)	Acc@1  91.41 ( 92.77)	Acc@5 100.00 ( 99.87)
Epoch: [35][320/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0776e-01 (2.1064e-01)	Acc@1  92.19 ( 92.80)	Acc@5 100.00 ( 99.87)
Epoch: [35][330/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0850e-01 (2.1093e-01)	Acc@1  92.97 ( 92.78)	Acc@5 100.00 ( 99.87)
Epoch: [35][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7029e-01 (2.1082e-01)	Acc@1  92.19 ( 92.77)	Acc@5 100.00 ( 99.87)
Epoch: [35][350/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9604e-01 (2.1022e-01)	Acc@1  94.53 ( 92.78)	Acc@5 100.00 ( 99.87)
Epoch: [35][360/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.7295e-01 (2.0994e-01)	Acc@1  90.62 ( 92.80)	Acc@5 100.00 ( 99.87)
Epoch: [35][370/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3938e-01 (2.0976e-01)	Acc@1  89.06 ( 92.79)	Acc@5 100.00 ( 99.87)
Epoch: [35][380/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5173e-01 (2.0966e-01)	Acc@1  96.88 ( 92.79)	Acc@5 100.00 ( 99.87)
Epoch: [35][390/391]	Time  0.047 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0081e-01 (2.0921e-01)	Acc@1  95.00 ( 92.82)	Acc@5 100.00 ( 99.87)
## e[35] optimizer.zero_grad (sum) time: 0.3867647647857666
## e[35]       loss.backward (sum) time: 7.25611686706543
## e[35]      optimizer.step (sum) time: 3.4877126216888428
## epoch[35] training(only) time: 25.562036752700806
# Switched to evaluate mode...
Test: [  0/100]	Time  0.192 ( 0.192)	Loss 2.7100e-01 (2.7100e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.041)	Loss 3.5693e-01 (3.0801e-01)	Acc@1  91.00 ( 90.18)	Acc@5 100.00 ( 99.55)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 3.5547e-01 (3.1177e-01)	Acc@1  85.00 ( 89.71)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.027 ( 0.031)	Loss 3.0762e-01 (3.1735e-01)	Acc@1  86.00 ( 89.55)	Acc@5 100.00 ( 99.55)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 3.1641e-01 (3.2022e-01)	Acc@1  88.00 ( 89.41)	Acc@5 100.00 ( 99.51)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 2.6123e-01 (3.1676e-01)	Acc@1  86.00 ( 89.31)	Acc@5  99.00 ( 99.55)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 2.0898e-01 (3.1061e-01)	Acc@1  94.00 ( 89.64)	Acc@5 100.00 ( 99.59)
Test: [ 70/100]	Time  0.024 ( 0.027)	Loss 3.6694e-01 (3.0687e-01)	Acc@1  87.00 ( 89.76)	Acc@5 100.00 ( 99.63)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 1.9873e-01 (3.0812e-01)	Acc@1  93.00 ( 89.74)	Acc@5 100.00 ( 99.64)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 2.3108e-01 (3.0734e-01)	Acc@1  90.00 ( 89.75)	Acc@5 100.00 ( 99.64)
 * Acc@1 89.850 Acc@5 99.660
### epoch[35] execution time: 28.357259035110474
EPOCH 36
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [36][  0/391]	Time  0.241 ( 0.241)	Data  0.171 ( 0.171)	Loss 3.0273e-01 (3.0273e-01)	Acc@1  90.62 ( 90.62)	Acc@5  99.22 ( 99.22)
Epoch: [36][ 10/391]	Time  0.064 ( 0.080)	Data  0.001 ( 0.016)	Loss 1.8457e-01 (2.0243e-01)	Acc@1  92.97 ( 93.75)	Acc@5 100.00 ( 99.86)
Epoch: [36][ 20/391]	Time  0.063 ( 0.073)	Data  0.001 ( 0.009)	Loss 1.8628e-01 (1.9225e-01)	Acc@1  93.75 ( 94.16)	Acc@5  99.22 ( 99.81)
Epoch: [36][ 30/391]	Time  0.070 ( 0.070)	Data  0.001 ( 0.006)	Loss 1.5198e-01 (1.9240e-01)	Acc@1  96.09 ( 93.98)	Acc@5 100.00 ( 99.80)
Epoch: [36][ 40/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.005)	Loss 1.9751e-01 (2.0199e-01)	Acc@1  93.75 ( 93.33)	Acc@5 100.00 ( 99.81)
Epoch: [36][ 50/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.9385e-01 (2.0551e-01)	Acc@1  95.31 ( 93.03)	Acc@5 100.00 ( 99.85)
Epoch: [36][ 60/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.7896e-01 (2.0631e-01)	Acc@1  92.97 ( 93.01)	Acc@5 100.00 ( 99.85)
Epoch: [36][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.8042e-01 (2.0282e-01)	Acc@1  93.75 ( 93.22)	Acc@5 100.00 ( 99.86)
Epoch: [36][ 80/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.5332e-01 (2.0137e-01)	Acc@1  95.31 ( 93.27)	Acc@5 100.00 ( 99.86)
Epoch: [36][ 90/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.2964e-01 (2.0104e-01)	Acc@1  94.53 ( 93.31)	Acc@5 100.00 ( 99.86)
Epoch: [36][100/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.4136e-01 (1.9816e-01)	Acc@1  96.09 ( 93.39)	Acc@5 100.00 ( 99.88)
Epoch: [36][110/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.3428e-01 (1.9716e-01)	Acc@1  92.97 ( 93.40)	Acc@5 100.00 ( 99.88)
Epoch: [36][120/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4062e-01 (1.9474e-01)	Acc@1  93.75 ( 93.43)	Acc@5 100.00 ( 99.89)
Epoch: [36][130/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9580e-01 (1.9547e-01)	Acc@1  94.53 ( 93.40)	Acc@5  99.22 ( 99.88)
Epoch: [36][140/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8091e-01 (1.9451e-01)	Acc@1  93.75 ( 93.43)	Acc@5 100.00 ( 99.88)
Epoch: [36][150/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.2119e-01 (1.9504e-01)	Acc@1  89.06 ( 93.34)	Acc@5 100.00 ( 99.89)
Epoch: [36][160/391]	Time  0.070 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.9077e-01 (1.9516e-01)	Acc@1  89.06 ( 93.28)	Acc@5  99.22 ( 99.89)
Epoch: [36][170/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4746e-01 (1.9413e-01)	Acc@1  95.31 ( 93.32)	Acc@5 100.00 ( 99.89)
Epoch: [36][180/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7773e-01 (1.9387e-01)	Acc@1  92.19 ( 93.33)	Acc@5 100.00 ( 99.89)
Epoch: [36][190/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0789e-01 (1.9351e-01)	Acc@1  92.19 ( 93.30)	Acc@5 100.00 ( 99.89)
Epoch: [36][200/391]	Time  0.070 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.7368e-01 (1.9470e-01)	Acc@1  90.62 ( 93.27)	Acc@5 100.00 ( 99.90)
Epoch: [36][210/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0337e-01 (1.9626e-01)	Acc@1  94.53 ( 93.20)	Acc@5  99.22 ( 99.89)
Epoch: [36][220/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0773e-01 (1.9521e-01)	Acc@1  96.88 ( 93.27)	Acc@5 100.00 ( 99.89)
Epoch: [36][230/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0898e-01 (1.9512e-01)	Acc@1  92.97 ( 93.29)	Acc@5 100.00 ( 99.90)
Epoch: [36][240/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2095e-01 (1.9563e-01)	Acc@1  93.75 ( 93.26)	Acc@5 100.00 ( 99.90)
Epoch: [36][250/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.2056e-01 (1.9589e-01)	Acc@1  88.28 ( 93.24)	Acc@5 100.00 ( 99.90)
Epoch: [36][260/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.8467e-01 (1.9652e-01)	Acc@1  88.28 ( 93.18)	Acc@5  99.22 ( 99.90)
Epoch: [36][270/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.5464e-01 (1.9605e-01)	Acc@1  92.97 ( 93.20)	Acc@5  99.22 ( 99.89)
Epoch: [36][280/391]	Time  0.065 ( 0.065)	Data  0.002 ( 0.002)	Loss 1.6187e-01 (1.9674e-01)	Acc@1  94.53 ( 93.16)	Acc@5  99.22 ( 99.89)
Epoch: [36][290/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2058e-01 (1.9701e-01)	Acc@1  92.97 ( 93.13)	Acc@5 100.00 ( 99.90)
Epoch: [36][300/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6418e-01 (1.9743e-01)	Acc@1  92.97 ( 93.11)	Acc@5 100.00 ( 99.89)
Epoch: [36][310/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8896e-01 (1.9741e-01)	Acc@1  93.75 ( 93.12)	Acc@5 100.00 ( 99.89)
Epoch: [36][320/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0947e-01 (1.9855e-01)	Acc@1  92.19 ( 93.08)	Acc@5 100.00 ( 99.89)
Epoch: [36][330/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4319e-01 (1.9745e-01)	Acc@1  94.53 ( 93.11)	Acc@5 100.00 ( 99.88)
Epoch: [36][340/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2754e-01 (1.9772e-01)	Acc@1  92.19 ( 93.09)	Acc@5 100.00 ( 99.89)
Epoch: [36][350/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3108e-01 (1.9782e-01)	Acc@1  92.97 ( 93.08)	Acc@5 100.00 ( 99.89)
Epoch: [36][360/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.6392e-01 (1.9828e-01)	Acc@1  92.19 ( 93.09)	Acc@5 100.00 ( 99.89)
Epoch: [36][370/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0081e-01 (1.9895e-01)	Acc@1  90.62 ( 93.06)	Acc@5 100.00 ( 99.89)
Epoch: [36][380/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0789e-01 (1.9968e-01)	Acc@1  93.75 ( 93.03)	Acc@5 100.00 ( 99.89)
Epoch: [36][390/391]	Time  0.048 ( 0.065)	Data  0.001 ( 0.001)	Loss 1.5662e-01 (2.0033e-01)	Acc@1  95.00 ( 92.98)	Acc@5 100.00 ( 99.88)
## e[36] optimizer.zero_grad (sum) time: 0.3887176513671875
## e[36]       loss.backward (sum) time: 7.200481653213501
## e[36]      optimizer.step (sum) time: 3.4402050971984863
## epoch[36] training(only) time: 25.50152015686035
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 2.7905e-01 (2.7905e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.040)	Loss 4.1943e-01 (3.1325e-01)	Acc@1  89.00 ( 89.64)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.026 ( 0.033)	Loss 3.2837e-01 (3.1080e-01)	Acc@1  84.00 ( 89.48)	Acc@5 100.00 ( 99.52)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 3.3496e-01 (3.1673e-01)	Acc@1  86.00 ( 89.55)	Acc@5 100.00 ( 99.55)
Test: [ 40/100]	Time  0.027 ( 0.029)	Loss 3.3936e-01 (3.2063e-01)	Acc@1  86.00 ( 89.27)	Acc@5 100.00 ( 99.51)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 2.4768e-01 (3.1988e-01)	Acc@1  90.00 ( 89.31)	Acc@5  99.00 ( 99.51)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 2.1521e-01 (3.1540e-01)	Acc@1  91.00 ( 89.39)	Acc@5 100.00 ( 99.57)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 4.2773e-01 (3.1414e-01)	Acc@1  86.00 ( 89.56)	Acc@5 100.00 ( 99.59)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 2.0203e-01 (3.1442e-01)	Acc@1  93.00 ( 89.58)	Acc@5 100.00 ( 99.58)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 2.0410e-01 (3.1285e-01)	Acc@1  92.00 ( 89.60)	Acc@5 100.00 ( 99.60)
 * Acc@1 89.680 Acc@5 99.620
### epoch[36] execution time: 28.307491302490234
EPOCH 37
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [37][  0/391]	Time  0.242 ( 0.242)	Data  0.178 ( 0.178)	Loss 9.0942e-02 (9.0942e-02)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [37][ 10/391]	Time  0.067 ( 0.081)	Data  0.001 ( 0.017)	Loss 1.6223e-01 (1.7645e-01)	Acc@1  93.75 ( 92.90)	Acc@5 100.00 (100.00)
Epoch: [37][ 20/391]	Time  0.063 ( 0.073)	Data  0.001 ( 0.010)	Loss 1.9531e-01 (1.9377e-01)	Acc@1  92.19 ( 92.63)	Acc@5 100.00 ( 99.96)
Epoch: [37][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.007)	Loss 1.1536e-01 (1.8908e-01)	Acc@1  96.88 ( 92.97)	Acc@5 100.00 ( 99.90)
Epoch: [37][ 40/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.005)	Loss 1.6663e-01 (1.8881e-01)	Acc@1  95.31 ( 93.03)	Acc@5 100.00 ( 99.90)
Epoch: [37][ 50/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.005)	Loss 2.6294e-01 (1.9212e-01)	Acc@1  89.06 ( 93.03)	Acc@5 100.00 ( 99.91)
Epoch: [37][ 60/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.2329e-01 (1.9260e-01)	Acc@1  95.31 ( 93.03)	Acc@5 100.00 ( 99.92)
Epoch: [37][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 2.6343e-01 (1.9221e-01)	Acc@1  92.97 ( 93.06)	Acc@5 100.00 ( 99.92)
Epoch: [37][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.1509e-01 (1.9172e-01)	Acc@1  93.75 ( 93.09)	Acc@5 100.00 ( 99.92)
Epoch: [37][ 90/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.3816e-01 (1.9738e-01)	Acc@1  90.62 ( 92.91)	Acc@5 100.00 ( 99.91)
Epoch: [37][100/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.1973e-01 (1.9715e-01)	Acc@1  92.97 ( 92.94)	Acc@5  99.22 ( 99.90)
Epoch: [37][110/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.9873e-01 (1.9641e-01)	Acc@1  92.19 ( 92.94)	Acc@5 100.00 ( 99.90)
Epoch: [37][120/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.3281e-01 (1.9546e-01)	Acc@1  96.09 ( 92.99)	Acc@5 100.00 ( 99.91)
Epoch: [37][130/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7297e-01 (1.9582e-01)	Acc@1  95.31 ( 93.03)	Acc@5 100.00 ( 99.91)
Epoch: [37][140/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5259e-01 (1.9546e-01)	Acc@1  94.53 ( 92.97)	Acc@5 100.00 ( 99.91)
Epoch: [37][150/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8005e-01 (1.9466e-01)	Acc@1  92.97 ( 93.00)	Acc@5 100.00 ( 99.92)
Epoch: [37][160/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.3962e-01 (1.9600e-01)	Acc@1  88.28 ( 92.93)	Acc@5 100.00 ( 99.91)
Epoch: [37][170/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.1155e-01 (1.9545e-01)	Acc@1  91.41 ( 92.98)	Acc@5 100.00 ( 99.91)
Epoch: [37][180/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.4939e-01 (1.9623e-01)	Acc@1  91.41 ( 92.99)	Acc@5 100.00 ( 99.90)
Epoch: [37][190/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7773e-01 (1.9527e-01)	Acc@1  94.53 ( 93.05)	Acc@5  99.22 ( 99.90)
Epoch: [37][200/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.6252e-02 (1.9480e-01)	Acc@1  97.66 ( 93.04)	Acc@5 100.00 ( 99.90)
Epoch: [37][210/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.5830e-01 (1.9595e-01)	Acc@1  91.41 ( 93.00)	Acc@5  98.44 ( 99.90)
Epoch: [37][220/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5857e-01 (1.9532e-01)	Acc@1  95.31 ( 93.05)	Acc@5 100.00 ( 99.89)
Epoch: [37][230/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2034e-01 (1.9485e-01)	Acc@1  93.75 ( 93.07)	Acc@5 100.00 ( 99.89)
Epoch: [37][240/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3828e-01 (1.9521e-01)	Acc@1  88.28 ( 93.04)	Acc@5 100.00 ( 99.90)
Epoch: [37][250/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9812e-01 (1.9521e-01)	Acc@1  93.75 ( 93.07)	Acc@5 100.00 ( 99.89)
Epoch: [37][260/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9836e-01 (1.9641e-01)	Acc@1  91.41 ( 93.02)	Acc@5 100.00 ( 99.89)
Epoch: [37][270/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7273e-01 (1.9622e-01)	Acc@1  94.53 ( 93.05)	Acc@5  99.22 ( 99.88)
Epoch: [37][280/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5198e-01 (1.9525e-01)	Acc@1  95.31 ( 93.09)	Acc@5 100.00 ( 99.89)
Epoch: [37][290/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0435e-01 (1.9513e-01)	Acc@1  92.97 ( 93.09)	Acc@5 100.00 ( 99.89)
Epoch: [37][300/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.2495e-01 (1.9640e-01)	Acc@1  85.94 ( 93.06)	Acc@5 100.00 ( 99.89)
Epoch: [37][310/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3901e-01 (1.9747e-01)	Acc@1  92.97 ( 93.01)	Acc@5 100.00 ( 99.89)
Epoch: [37][320/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3523e-01 (1.9765e-01)	Acc@1  90.62 ( 93.01)	Acc@5 100.00 ( 99.89)
Epoch: [37][330/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.8516e-01 (1.9790e-01)	Acc@1  89.84 ( 93.01)	Acc@5  99.22 ( 99.89)
Epoch: [37][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1633e-01 (1.9811e-01)	Acc@1  94.53 ( 93.01)	Acc@5 100.00 ( 99.89)
Epoch: [37][350/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2976e-01 (1.9719e-01)	Acc@1  96.09 ( 93.05)	Acc@5 100.00 ( 99.89)
Epoch: [37][360/391]	Time  0.073 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3608e-01 (1.9736e-01)	Acc@1  92.97 ( 93.05)	Acc@5  99.22 ( 99.89)
Epoch: [37][370/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2183e-01 (1.9718e-01)	Acc@1  94.53 ( 93.06)	Acc@5 100.00 ( 99.88)
Epoch: [37][380/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9653e-01 (1.9691e-01)	Acc@1  94.53 ( 93.07)	Acc@5 100.00 ( 99.89)
Epoch: [37][390/391]	Time  0.050 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6284e-01 (1.9673e-01)	Acc@1  93.75 ( 93.08)	Acc@5 100.00 ( 99.88)
## e[37] optimizer.zero_grad (sum) time: 0.3886394500732422
## e[37]       loss.backward (sum) time: 7.2555835247039795
## e[37]      optimizer.step (sum) time: 3.4586551189422607
## epoch[37] training(only) time: 25.58689546585083
# Switched to evaluate mode...
Test: [  0/100]	Time  0.190 ( 0.190)	Loss 2.4548e-01 (2.4548e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.040)	Loss 3.8916e-01 (3.1241e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 3.4351e-01 (3.1138e-01)	Acc@1  86.00 ( 90.05)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.024 ( 0.030)	Loss 3.0200e-01 (3.1930e-01)	Acc@1  87.00 ( 89.84)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 3.5767e-01 (3.2303e-01)	Acc@1  88.00 ( 89.59)	Acc@5 100.00 ( 99.61)
Test: [ 50/100]	Time  0.028 ( 0.028)	Loss 2.6465e-01 (3.2087e-01)	Acc@1  89.00 ( 89.61)	Acc@5  99.00 ( 99.63)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 2.1838e-01 (3.1671e-01)	Acc@1  91.00 ( 89.77)	Acc@5 100.00 ( 99.67)
Test: [ 70/100]	Time  0.027 ( 0.028)	Loss 4.1553e-01 (3.1569e-01)	Acc@1  89.00 ( 89.87)	Acc@5 100.00 ( 99.69)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 2.1143e-01 (3.1530e-01)	Acc@1  90.00 ( 89.86)	Acc@5 100.00 ( 99.68)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 2.0642e-01 (3.1466e-01)	Acc@1  91.00 ( 89.87)	Acc@5 100.00 ( 99.68)
 * Acc@1 89.910 Acc@5 99.700
### epoch[37] execution time: 28.417073965072632
EPOCH 38
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [38][  0/391]	Time  0.244 ( 0.244)	Data  0.169 ( 0.169)	Loss 2.6196e-01 (2.6196e-01)	Acc@1  92.19 ( 92.19)	Acc@5  99.22 ( 99.22)
Epoch: [38][ 10/391]	Time  0.064 ( 0.081)	Data  0.001 ( 0.016)	Loss 1.7969e-01 (1.8343e-01)	Acc@1  92.19 ( 94.25)	Acc@5 100.00 ( 99.64)
Epoch: [38][ 20/391]	Time  0.064 ( 0.073)	Data  0.001 ( 0.009)	Loss 1.6699e-01 (1.8388e-01)	Acc@1  95.31 ( 93.75)	Acc@5 100.00 ( 99.78)
Epoch: [38][ 30/391]	Time  0.066 ( 0.070)	Data  0.001 ( 0.006)	Loss 2.0593e-01 (1.8172e-01)	Acc@1  92.97 ( 93.75)	Acc@5 100.00 ( 99.80)
Epoch: [38][ 40/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.005)	Loss 1.9727e-01 (1.8281e-01)	Acc@1  91.41 ( 93.64)	Acc@5 100.00 ( 99.85)
Epoch: [38][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 2.1216e-01 (1.8111e-01)	Acc@1  93.75 ( 93.86)	Acc@5 100.00 ( 99.85)
Epoch: [38][ 60/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.7639e-01 (1.8419e-01)	Acc@1  94.53 ( 93.63)	Acc@5 100.00 ( 99.87)
Epoch: [38][ 70/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.3608e-01 (1.8787e-01)	Acc@1  90.62 ( 93.51)	Acc@5 100.00 ( 99.88)
Epoch: [38][ 80/391]	Time  0.059 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.2634e-01 (1.8766e-01)	Acc@1  96.09 ( 93.45)	Acc@5 100.00 ( 99.89)
Epoch: [38][ 90/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.7212e-01 (1.8616e-01)	Acc@1  92.97 ( 93.53)	Acc@5  99.22 ( 99.90)
Epoch: [38][100/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.0150e-01 (1.8481e-01)	Acc@1  95.31 ( 93.59)	Acc@5 100.00 ( 99.88)
Epoch: [38][110/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.6638e-01 (1.8587e-01)	Acc@1  92.19 ( 93.55)	Acc@5 100.00 ( 99.89)
Epoch: [38][120/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.6172e-01 (1.8604e-01)	Acc@1  87.50 ( 93.52)	Acc@5 100.00 ( 99.90)
Epoch: [38][130/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9116e-01 (1.8545e-01)	Acc@1  95.31 ( 93.56)	Acc@5 100.00 ( 99.90)
Epoch: [38][140/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5076e-01 (1.8547e-01)	Acc@1  94.53 ( 93.54)	Acc@5 100.00 ( 99.88)
Epoch: [38][150/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7297e-01 (1.8697e-01)	Acc@1  92.19 ( 93.44)	Acc@5 100.00 ( 99.89)
Epoch: [38][160/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.1790e-01 (1.8636e-01)	Acc@1  92.97 ( 93.51)	Acc@5 100.00 ( 99.89)
Epoch: [38][170/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4783e-01 (1.8742e-01)	Acc@1  96.88 ( 93.47)	Acc@5 100.00 ( 99.89)
Epoch: [38][180/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6882e-01 (1.8741e-01)	Acc@1  93.75 ( 93.48)	Acc@5 100.00 ( 99.90)
Epoch: [38][190/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9324e-01 (1.8811e-01)	Acc@1  94.53 ( 93.48)	Acc@5 100.00 ( 99.90)
Epoch: [38][200/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8689e-01 (1.8798e-01)	Acc@1  91.41 ( 93.49)	Acc@5 100.00 ( 99.90)
Epoch: [38][210/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2390e-01 (1.8782e-01)	Acc@1  96.09 ( 93.54)	Acc@5 100.00 ( 99.90)
Epoch: [38][220/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6907e-01 (1.8777e-01)	Acc@1  93.75 ( 93.53)	Acc@5 100.00 ( 99.90)
Epoch: [38][230/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1240e-01 (1.8769e-01)	Acc@1  94.53 ( 93.56)	Acc@5  99.22 ( 99.89)
Epoch: [38][240/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3047e-01 (1.8709e-01)	Acc@1  93.75 ( 93.58)	Acc@5  99.22 ( 99.89)
Epoch: [38][250/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9495e-01 (1.8790e-01)	Acc@1  94.53 ( 93.57)	Acc@5 100.00 ( 99.89)
Epoch: [38][260/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1145e-01 (1.8754e-01)	Acc@1  96.88 ( 93.57)	Acc@5 100.00 ( 99.89)
Epoch: [38][270/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.2983e-01 (1.8855e-01)	Acc@1  88.28 ( 93.53)	Acc@5 100.00 ( 99.89)
Epoch: [38][280/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3513e-01 (1.8830e-01)	Acc@1  94.53 ( 93.54)	Acc@5 100.00 ( 99.89)
Epoch: [38][290/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0455e-01 (1.8699e-01)	Acc@1  96.88 ( 93.58)	Acc@5 100.00 ( 99.89)
Epoch: [38][300/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5393e-01 (1.8721e-01)	Acc@1  95.31 ( 93.57)	Acc@5 100.00 ( 99.89)
Epoch: [38][310/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.5303e-01 (1.8816e-01)	Acc@1  91.41 ( 93.54)	Acc@5  99.22 ( 99.89)
Epoch: [38][320/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1130e-01 (1.8847e-01)	Acc@1  90.62 ( 93.52)	Acc@5 100.00 ( 99.89)
Epoch: [38][330/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3208e-01 (1.8769e-01)	Acc@1  94.53 ( 93.54)	Acc@5 100.00 ( 99.89)
Epoch: [38][340/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2998e-01 (1.8847e-01)	Acc@1  91.41 ( 93.51)	Acc@5 100.00 ( 99.89)
Epoch: [38][350/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4526e-01 (1.8934e-01)	Acc@1  94.53 ( 93.47)	Acc@5 100.00 ( 99.88)
Epoch: [38][360/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7993e-01 (1.8900e-01)	Acc@1  92.97 ( 93.48)	Acc@5 100.00 ( 99.88)
Epoch: [38][370/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2217e-01 (1.8964e-01)	Acc@1  92.19 ( 93.47)	Acc@5 100.00 ( 99.88)
Epoch: [38][380/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.001)	Loss 1.7822e-01 (1.8913e-01)	Acc@1  94.53 ( 93.49)	Acc@5 100.00 ( 99.89)
Epoch: [38][390/391]	Time  0.057 ( 0.065)	Data  0.001 ( 0.001)	Loss 1.6125e-01 (1.8933e-01)	Acc@1  93.75 ( 93.48)	Acc@5 100.00 ( 99.89)
## e[38] optimizer.zero_grad (sum) time: 0.38898515701293945
## e[38]       loss.backward (sum) time: 7.2324230670928955
## e[38]      optimizer.step (sum) time: 3.4398908615112305
## epoch[38] training(only) time: 25.52525305747986
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 2.6147e-01 (2.6147e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.028 ( 0.039)	Loss 4.3994e-01 (3.0769e-01)	Acc@1  88.00 ( 90.18)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.028 ( 0.033)	Loss 3.4961e-01 (3.0791e-01)	Acc@1  87.00 ( 89.90)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.026 ( 0.030)	Loss 3.3789e-01 (3.1402e-01)	Acc@1  87.00 ( 89.90)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 3.5156e-01 (3.1725e-01)	Acc@1  90.00 ( 89.80)	Acc@5 100.00 ( 99.63)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 2.5977e-01 (3.1649e-01)	Acc@1  92.00 ( 89.75)	Acc@5  99.00 ( 99.63)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 2.3352e-01 (3.1230e-01)	Acc@1  91.00 ( 89.84)	Acc@5 100.00 ( 99.66)
Test: [ 70/100]	Time  0.024 ( 0.027)	Loss 4.5947e-01 (3.1178e-01)	Acc@1  86.00 ( 89.86)	Acc@5 100.00 ( 99.65)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 2.3584e-01 (3.1340e-01)	Acc@1  92.00 ( 89.83)	Acc@5 100.00 ( 99.64)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 1.8616e-01 (3.1171e-01)	Acc@1  91.00 ( 89.81)	Acc@5 100.00 ( 99.65)
 * Acc@1 89.890 Acc@5 99.660
### epoch[38] execution time: 28.317654848098755
EPOCH 39
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [39][  0/391]	Time  0.252 ( 0.252)	Data  0.185 ( 0.185)	Loss 1.8445e-01 (1.8445e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
Epoch: [39][ 10/391]	Time  0.065 ( 0.082)	Data  0.001 ( 0.018)	Loss 2.3779e-01 (1.8112e-01)	Acc@1  92.97 ( 93.47)	Acc@5  99.22 ( 99.79)
Epoch: [39][ 20/391]	Time  0.064 ( 0.074)	Data  0.001 ( 0.010)	Loss 1.3220e-01 (1.8547e-01)	Acc@1  96.09 ( 93.64)	Acc@5 100.00 ( 99.85)
Epoch: [39][ 30/391]	Time  0.067 ( 0.071)	Data  0.001 ( 0.007)	Loss 2.0349e-01 (1.8596e-01)	Acc@1  92.19 ( 93.50)	Acc@5 100.00 ( 99.85)
Epoch: [39][ 40/391]	Time  0.063 ( 0.069)	Data  0.001 ( 0.006)	Loss 1.1127e-01 (1.8007e-01)	Acc@1  97.66 ( 93.73)	Acc@5 100.00 ( 99.87)
Epoch: [39][ 50/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.005)	Loss 9.4238e-02 (1.8404e-01)	Acc@1  98.44 ( 93.75)	Acc@5 100.00 ( 99.88)
Epoch: [39][ 60/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.2524e-01 (1.7767e-01)	Acc@1  96.09 ( 93.90)	Acc@5 100.00 ( 99.90)
Epoch: [39][ 70/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.0950e-01 (1.7604e-01)	Acc@1  96.09 ( 93.98)	Acc@5 100.00 ( 99.90)
Epoch: [39][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.2451e-01 (1.7682e-01)	Acc@1  95.31 ( 94.00)	Acc@5 100.00 ( 99.88)
Epoch: [39][ 90/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.3169e-01 (1.7653e-01)	Acc@1  92.19 ( 93.99)	Acc@5  99.22 ( 99.89)
Epoch: [39][100/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.8909e-01 (1.7894e-01)	Acc@1  92.97 ( 93.89)	Acc@5 100.00 ( 99.88)
Epoch: [39][110/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.4880e-01 (1.7810e-01)	Acc@1  94.53 ( 93.91)	Acc@5 100.00 ( 99.89)
Epoch: [39][120/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.1487e-01 (1.7805e-01)	Acc@1  95.31 ( 93.84)	Acc@5 100.00 ( 99.89)
Epoch: [39][130/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.5635e-01 (1.7900e-01)	Acc@1  89.84 ( 93.79)	Acc@5 100.00 ( 99.89)
Epoch: [39][140/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4978e-01 (1.7915e-01)	Acc@1  94.53 ( 93.75)	Acc@5 100.00 ( 99.89)
Epoch: [39][150/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.1948e-01 (1.7741e-01)	Acc@1  93.75 ( 93.79)	Acc@5 100.00 ( 99.90)
Epoch: [39][160/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.6660e-02 (1.7548e-01)	Acc@1  98.44 ( 93.84)	Acc@5 100.00 ( 99.89)
Epoch: [39][170/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3452e-01 (1.7740e-01)	Acc@1  96.09 ( 93.81)	Acc@5 100.00 ( 99.89)
Epoch: [39][180/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4819e-01 (1.7694e-01)	Acc@1  96.09 ( 93.81)	Acc@5 100.00 ( 99.89)
Epoch: [39][190/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.7979e-01 (1.7812e-01)	Acc@1  90.62 ( 93.77)	Acc@5 100.00 ( 99.89)
Epoch: [39][200/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7175e-01 (1.7885e-01)	Acc@1  96.09 ( 93.75)	Acc@5 100.00 ( 99.89)
Epoch: [39][210/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6296e-01 (1.7931e-01)	Acc@1  94.53 ( 93.73)	Acc@5 100.00 ( 99.89)
Epoch: [39][220/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5466e-01 (1.7879e-01)	Acc@1  92.97 ( 93.71)	Acc@5 100.00 ( 99.90)
Epoch: [39][230/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8835e-01 (1.7901e-01)	Acc@1  92.97 ( 93.71)	Acc@5 100.00 ( 99.90)
Epoch: [39][240/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4392e-01 (1.7928e-01)	Acc@1  96.09 ( 93.72)	Acc@5  99.22 ( 99.89)
Epoch: [39][250/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0144e-01 (1.7890e-01)	Acc@1  96.88 ( 93.74)	Acc@5 100.00 ( 99.89)
Epoch: [39][260/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5320e-01 (1.7871e-01)	Acc@1  93.75 ( 93.72)	Acc@5 100.00 ( 99.90)
Epoch: [39][270/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2961e-01 (1.8000e-01)	Acc@1  90.62 ( 93.69)	Acc@5 100.00 ( 99.89)
Epoch: [39][280/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0325e-01 (1.8003e-01)	Acc@1  92.97 ( 93.68)	Acc@5 100.00 ( 99.89)
Epoch: [39][290/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2311e-01 (1.8037e-01)	Acc@1  95.31 ( 93.69)	Acc@5 100.00 ( 99.90)
Epoch: [39][300/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0483e-01 (1.8061e-01)	Acc@1  90.62 ( 93.65)	Acc@5 100.00 ( 99.90)
Epoch: [39][310/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8958e-01 (1.8026e-01)	Acc@1  95.31 ( 93.68)	Acc@5 100.00 ( 99.90)
Epoch: [39][320/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8677e-01 (1.8121e-01)	Acc@1  92.97 ( 93.67)	Acc@5 100.00 ( 99.90)
Epoch: [39][330/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.8125e-01 (1.8160e-01)	Acc@1  91.41 ( 93.67)	Acc@5  99.22 ( 99.90)
Epoch: [39][340/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9226e-01 (1.8186e-01)	Acc@1  90.62 ( 93.65)	Acc@5 100.00 ( 99.90)
Epoch: [39][350/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0862e-01 (1.8199e-01)	Acc@1  92.19 ( 93.65)	Acc@5  99.22 ( 99.90)
Epoch: [39][360/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7139e-01 (1.8213e-01)	Acc@1  92.97 ( 93.62)	Acc@5  99.22 ( 99.90)
Epoch: [39][370/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2559e-01 (1.8240e-01)	Acc@1  94.53 ( 93.62)	Acc@5 100.00 ( 99.90)
Epoch: [39][380/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.3154e-01 (1.8294e-01)	Acc@1  89.06 ( 93.59)	Acc@5 100.00 ( 99.90)
Epoch: [39][390/391]	Time  0.048 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2095e-01 (1.8282e-01)	Acc@1  90.00 ( 93.60)	Acc@5 100.00 ( 99.90)
## e[39] optimizer.zero_grad (sum) time: 0.3877546787261963
## e[39]       loss.backward (sum) time: 7.255281209945679
## e[39]      optimizer.step (sum) time: 3.4786181449890137
## epoch[39] training(only) time: 25.52341318130493
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 2.7148e-01 (2.7148e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.032 ( 0.040)	Loss 4.2065e-01 (3.1475e-01)	Acc@1  89.00 ( 90.00)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.026 ( 0.033)	Loss 3.3813e-01 (3.1924e-01)	Acc@1  88.00 ( 89.86)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.027 ( 0.031)	Loss 3.3081e-01 (3.2653e-01)	Acc@1  86.00 ( 89.90)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 3.7549e-01 (3.2920e-01)	Acc@1  90.00 ( 89.73)	Acc@5 100.00 ( 99.61)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 2.5146e-01 (3.2835e-01)	Acc@1  92.00 ( 89.82)	Acc@5  99.00 ( 99.59)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 2.1082e-01 (3.2303e-01)	Acc@1  91.00 ( 89.77)	Acc@5 100.00 ( 99.62)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 3.7036e-01 (3.1749e-01)	Acc@1  88.00 ( 90.04)	Acc@5 100.00 ( 99.62)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 2.2681e-01 (3.1706e-01)	Acc@1  91.00 ( 90.09)	Acc@5 100.00 ( 99.62)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 2.1594e-01 (3.1301e-01)	Acc@1  90.00 ( 90.05)	Acc@5 100.00 ( 99.63)
 * Acc@1 90.120 Acc@5 99.630
### epoch[39] execution time: 28.356956005096436
EPOCH 40
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [40][  0/391]	Time  0.242 ( 0.242)	Data  0.177 ( 0.177)	Loss 1.2354e-01 (1.2354e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [40][ 10/391]	Time  0.063 ( 0.081)	Data  0.001 ( 0.017)	Loss 3.5718e-01 (2.2064e-01)	Acc@1  87.50 ( 92.47)	Acc@5  98.44 ( 99.79)
Epoch: [40][ 20/391]	Time  0.063 ( 0.073)	Data  0.001 ( 0.009)	Loss 1.9421e-01 (1.8917e-01)	Acc@1  94.53 ( 93.60)	Acc@5 100.00 ( 99.85)
Epoch: [40][ 30/391]	Time  0.068 ( 0.071)	Data  0.001 ( 0.007)	Loss 2.4219e-01 (1.9652e-01)	Acc@1  92.97 ( 93.42)	Acc@5 100.00 ( 99.87)
Epoch: [40][ 40/391]	Time  0.063 ( 0.069)	Data  0.001 ( 0.005)	Loss 1.6736e-01 (1.9094e-01)	Acc@1  93.75 ( 93.46)	Acc@5 100.00 ( 99.90)
Epoch: [40][ 50/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.005)	Loss 1.5234e-01 (1.8987e-01)	Acc@1  94.53 ( 93.55)	Acc@5 100.00 ( 99.91)
Epoch: [40][ 60/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.0956e-01 (1.8662e-01)	Acc@1  96.88 ( 93.70)	Acc@5 100.00 ( 99.90)
Epoch: [40][ 70/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.004)	Loss 2.3474e-01 (1.8765e-01)	Acc@1  92.97 ( 93.62)	Acc@5 100.00 ( 99.88)
Epoch: [40][ 80/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.5125e-01 (1.8854e-01)	Acc@1  93.75 ( 93.60)	Acc@5 100.00 ( 99.89)
Epoch: [40][ 90/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 7.1289e-02 (1.8533e-01)	Acc@1  99.22 ( 93.70)	Acc@5 100.00 ( 99.91)
Epoch: [40][100/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.1969e-01 (1.8536e-01)	Acc@1  96.88 ( 93.71)	Acc@5 100.00 ( 99.90)
Epoch: [40][110/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.3340e-01 (1.8469e-01)	Acc@1  93.75 ( 93.73)	Acc@5 100.00 ( 99.90)
Epoch: [40][120/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4539e-01 (1.8277e-01)	Acc@1  96.09 ( 93.78)	Acc@5 100.00 ( 99.89)
Epoch: [40][130/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9788e-01 (1.8221e-01)	Acc@1  94.53 ( 93.80)	Acc@5 100.00 ( 99.90)
Epoch: [40][140/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0010e-01 (1.8025e-01)	Acc@1  96.09 ( 93.88)	Acc@5 100.00 ( 99.91)
Epoch: [40][150/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3684e-01 (1.8110e-01)	Acc@1  94.53 ( 93.84)	Acc@5 100.00 ( 99.91)
Epoch: [40][160/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2659e-01 (1.8169e-01)	Acc@1  95.31 ( 93.80)	Acc@5 100.00 ( 99.91)
Epoch: [40][170/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.0117e-01 (1.8402e-01)	Acc@1  91.41 ( 93.69)	Acc@5 100.00 ( 99.90)
Epoch: [40][180/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0447e-01 (1.8453e-01)	Acc@1  92.19 ( 93.66)	Acc@5 100.00 ( 99.91)
Epoch: [40][190/391]	Time  0.072 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3267e-01 (1.8508e-01)	Acc@1  90.62 ( 93.64)	Acc@5 100.00 ( 99.91)
Epoch: [40][200/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.6245e-01 (1.8453e-01)	Acc@1  92.97 ( 93.67)	Acc@5 100.00 ( 99.91)
Epoch: [40][210/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4246e-01 (1.8403e-01)	Acc@1  96.09 ( 93.68)	Acc@5 100.00 ( 99.90)
Epoch: [40][220/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1902e-01 (1.8298e-01)	Acc@1  95.31 ( 93.70)	Acc@5 100.00 ( 99.90)
Epoch: [40][230/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6992e-01 (1.8347e-01)	Acc@1  93.75 ( 93.67)	Acc@5 100.00 ( 99.91)
Epoch: [40][240/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6089e-01 (1.8194e-01)	Acc@1  92.19 ( 93.72)	Acc@5 100.00 ( 99.91)
Epoch: [40][250/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9421e-01 (1.8251e-01)	Acc@1  91.41 ( 93.67)	Acc@5  99.22 ( 99.90)
Epoch: [40][260/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3755e-01 (1.8186e-01)	Acc@1  92.19 ( 93.70)	Acc@5 100.00 ( 99.90)
Epoch: [40][270/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7676e-01 (1.8101e-01)	Acc@1  93.75 ( 93.73)	Acc@5 100.00 ( 99.90)
Epoch: [40][280/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8384e-01 (1.8080e-01)	Acc@1  93.75 ( 93.76)	Acc@5 100.00 ( 99.90)
Epoch: [40][290/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4819e-01 (1.8080e-01)	Acc@1  92.97 ( 93.73)	Acc@5 100.00 ( 99.90)
Epoch: [40][300/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3916e-01 (1.8060e-01)	Acc@1  94.53 ( 93.72)	Acc@5 100.00 ( 99.90)
Epoch: [40][310/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7920e-01 (1.8031e-01)	Acc@1  92.97 ( 93.71)	Acc@5 100.00 ( 99.90)
Epoch: [40][320/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.5488e-01 (1.8077e-01)	Acc@1  92.97 ( 93.70)	Acc@5  99.22 ( 99.90)
Epoch: [40][330/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.3154e-01 (1.8079e-01)	Acc@1  89.06 ( 93.70)	Acc@5  99.22 ( 99.90)
Epoch: [40][340/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2119e-01 (1.8085e-01)	Acc@1  91.41 ( 93.71)	Acc@5 100.00 ( 99.90)
Epoch: [40][350/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2192e-01 (1.8230e-01)	Acc@1  91.41 ( 93.66)	Acc@5 100.00 ( 99.89)
Epoch: [40][360/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7529e-01 (1.8251e-01)	Acc@1  95.31 ( 93.64)	Acc@5 100.00 ( 99.89)
Epoch: [40][370/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1749e-01 (1.8272e-01)	Acc@1  96.88 ( 93.64)	Acc@5 100.00 ( 99.89)
Epoch: [40][380/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0093e-01 (1.8218e-01)	Acc@1  92.97 ( 93.65)	Acc@5 100.00 ( 99.90)
Epoch: [40][390/391]	Time  0.046 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1582e-01 (1.8254e-01)	Acc@1  91.25 ( 93.63)	Acc@5 100.00 ( 99.90)
## e[40] optimizer.zero_grad (sum) time: 0.3897411823272705
## e[40]       loss.backward (sum) time: 7.253023862838745
## e[40]      optimizer.step (sum) time: 3.429814577102661
## epoch[40] training(only) time: 25.552669763565063
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 2.7148e-01 (2.7148e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.040)	Loss 4.1357e-01 (3.1181e-01)	Acc@1  91.00 ( 90.64)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.028 ( 0.033)	Loss 3.4131e-01 (3.1391e-01)	Acc@1  87.00 ( 90.05)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.027 ( 0.030)	Loss 2.9712e-01 (3.1941e-01)	Acc@1  88.00 ( 89.94)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.026 ( 0.029)	Loss 3.7695e-01 (3.2319e-01)	Acc@1  89.00 ( 89.76)	Acc@5 100.00 ( 99.63)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 2.8613e-01 (3.2025e-01)	Acc@1  90.00 ( 89.88)	Acc@5  99.00 ( 99.63)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 1.9666e-01 (3.1369e-01)	Acc@1  93.00 ( 89.97)	Acc@5 100.00 ( 99.67)
Test: [ 70/100]	Time  0.024 ( 0.027)	Loss 3.5889e-01 (3.1152e-01)	Acc@1  89.00 ( 90.03)	Acc@5 100.00 ( 99.66)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 1.7224e-01 (3.1140e-01)	Acc@1  93.00 ( 90.09)	Acc@5 100.00 ( 99.65)
Test: [ 90/100]	Time  0.027 ( 0.027)	Loss 1.7920e-01 (3.0913e-01)	Acc@1  90.00 ( 90.09)	Acc@5 100.00 ( 99.66)
 * Acc@1 90.140 Acc@5 99.660
### epoch[40] execution time: 28.356974124908447
EPOCH 41
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [41][  0/391]	Time  0.248 ( 0.248)	Data  0.179 ( 0.179)	Loss 1.7273e-01 (1.7273e-01)	Acc@1  92.97 ( 92.97)	Acc@5 100.00 (100.00)
Epoch: [41][ 10/391]	Time  0.063 ( 0.082)	Data  0.001 ( 0.017)	Loss 1.1353e-01 (1.5667e-01)	Acc@1  95.31 ( 94.03)	Acc@5 100.00 ( 99.93)
Epoch: [41][ 20/391]	Time  0.066 ( 0.074)	Data  0.001 ( 0.009)	Loss 2.0874e-01 (1.5337e-01)	Acc@1  92.19 ( 94.57)	Acc@5  99.22 ( 99.85)
Epoch: [41][ 30/391]	Time  0.063 ( 0.071)	Data  0.001 ( 0.007)	Loss 1.1560e-01 (1.6631e-01)	Acc@1  97.66 ( 94.23)	Acc@5 100.00 ( 99.85)
Epoch: [41][ 40/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.005)	Loss 1.7712e-01 (1.6519e-01)	Acc@1  93.75 ( 94.26)	Acc@5 100.00 ( 99.89)
Epoch: [41][ 50/391]	Time  0.070 ( 0.069)	Data  0.001 ( 0.005)	Loss 1.4551e-01 (1.6291e-01)	Acc@1  95.31 ( 94.36)	Acc@5 100.00 ( 99.91)
Epoch: [41][ 60/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.4270e-01 (1.6155e-01)	Acc@1  95.31 ( 94.44)	Acc@5 100.00 ( 99.92)
Epoch: [41][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.3953e-01 (1.6196e-01)	Acc@1  96.09 ( 94.42)	Acc@5 100.00 ( 99.92)
Epoch: [41][ 80/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.0093e-01 (1.6462e-01)	Acc@1  92.19 ( 94.39)	Acc@5 100.00 ( 99.92)
Epoch: [41][ 90/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.7029e-01 (1.6486e-01)	Acc@1  94.53 ( 94.35)	Acc@5 100.00 ( 99.93)
Epoch: [41][100/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.1582e-01 (1.6676e-01)	Acc@1  92.19 ( 94.30)	Acc@5  99.22 ( 99.91)
Epoch: [41][110/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.1603e-01 (1.6752e-01)	Acc@1  95.31 ( 94.21)	Acc@5 100.00 ( 99.91)
Epoch: [41][120/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.1558e-01 (1.7019e-01)	Acc@1  90.62 ( 94.08)	Acc@5 100.00 ( 99.91)
Epoch: [41][130/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3245e-01 (1.6965e-01)	Acc@1  96.09 ( 94.11)	Acc@5 100.00 ( 99.91)
Epoch: [41][140/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2231e-01 (1.7036e-01)	Acc@1  95.31 ( 94.07)	Acc@5 100.00 ( 99.91)
Epoch: [41][150/391]	Time  0.071 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.6074e-01 (1.7105e-01)	Acc@1  89.84 ( 94.00)	Acc@5 100.00 ( 99.90)
Epoch: [41][160/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.2070e-01 (1.7274e-01)	Acc@1  94.53 ( 93.97)	Acc@5 100.00 ( 99.90)
Epoch: [41][170/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.8125e-01 (1.7371e-01)	Acc@1  92.19 ( 93.96)	Acc@5 100.00 ( 99.90)
Epoch: [41][180/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.5205e-02 (1.7286e-01)	Acc@1  97.66 ( 93.98)	Acc@5 100.00 ( 99.91)
Epoch: [41][190/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2537e-01 (1.7435e-01)	Acc@1  96.09 ( 93.90)	Acc@5 100.00 ( 99.91)
Epoch: [41][200/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.9407e-02 (1.7444e-01)	Acc@1  96.88 ( 93.86)	Acc@5 100.00 ( 99.91)
Epoch: [41][210/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5295e-01 (1.7503e-01)	Acc@1  95.31 ( 93.85)	Acc@5 100.00 ( 99.92)
Epoch: [41][220/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8787e-01 (1.7575e-01)	Acc@1  92.97 ( 93.81)	Acc@5 100.00 ( 99.92)
Epoch: [41][230/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.9487e-02 (1.7578e-01)	Acc@1  96.09 ( 93.80)	Acc@5 100.00 ( 99.92)
Epoch: [41][240/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.0581e-01 (1.7649e-01)	Acc@1  91.41 ( 93.80)	Acc@5 100.00 ( 99.91)
Epoch: [41][250/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7273e-01 (1.7660e-01)	Acc@1  96.88 ( 93.83)	Acc@5 100.00 ( 99.91)
Epoch: [41][260/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.2131e-01 (1.7647e-01)	Acc@1  93.75 ( 93.85)	Acc@5 100.00 ( 99.91)
Epoch: [41][270/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8835e-01 (1.7701e-01)	Acc@1  94.53 ( 93.84)	Acc@5  99.22 ( 99.91)
Epoch: [41][280/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.7466e-01 (1.7729e-01)	Acc@1  89.06 ( 93.82)	Acc@5 100.00 ( 99.91)
Epoch: [41][290/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8372e-01 (1.7687e-01)	Acc@1  93.75 ( 93.85)	Acc@5 100.00 ( 99.91)
Epoch: [41][300/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.6440e-01 (1.7802e-01)	Acc@1  90.62 ( 93.83)	Acc@5  99.22 ( 99.90)
Epoch: [41][310/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1436e-01 (1.7855e-01)	Acc@1  91.41 ( 93.78)	Acc@5 100.00 ( 99.91)
Epoch: [41][320/391]	Time  0.061 ( 0.065)	Data  0.002 ( 0.002)	Loss 1.7346e-01 (1.7878e-01)	Acc@1  93.75 ( 93.79)	Acc@5 100.00 ( 99.91)
Epoch: [41][330/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4868e-01 (1.7843e-01)	Acc@1  93.75 ( 93.79)	Acc@5 100.00 ( 99.90)
Epoch: [41][340/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7407e-01 (1.7892e-01)	Acc@1  93.75 ( 93.78)	Acc@5 100.00 ( 99.90)
Epoch: [41][350/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9104e-01 (1.7900e-01)	Acc@1  91.41 ( 93.76)	Acc@5 100.00 ( 99.91)
Epoch: [41][360/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4526e-01 (1.7922e-01)	Acc@1  93.75 ( 93.74)	Acc@5 100.00 ( 99.91)
Epoch: [41][370/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.7783e-01 (1.8068e-01)	Acc@1  90.62 ( 93.70)	Acc@5 100.00 ( 99.91)
Epoch: [41][380/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.4902e-01 (1.8103e-01)	Acc@1  90.62 ( 93.68)	Acc@5 100.00 ( 99.91)
Epoch: [41][390/391]	Time  0.047 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.5388e-02 (1.8111e-01)	Acc@1  96.25 ( 93.67)	Acc@5 100.00 ( 99.91)
## e[41] optimizer.zero_grad (sum) time: 0.3932192325592041
## e[41]       loss.backward (sum) time: 7.263276815414429
## e[41]      optimizer.step (sum) time: 3.4137139320373535
## epoch[41] training(only) time: 25.591713666915894
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 2.9663e-01 (2.9663e-01)	Acc@1  87.00 ( 87.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.039)	Loss 4.3359e-01 (3.0730e-01)	Acc@1  89.00 ( 90.82)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 3.4497e-01 (3.0733e-01)	Acc@1  88.00 ( 90.19)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.024 ( 0.030)	Loss 3.2104e-01 (3.1000e-01)	Acc@1  87.00 ( 90.19)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 4.1016e-01 (3.1532e-01)	Acc@1  88.00 ( 89.85)	Acc@5 100.00 ( 99.63)
Test: [ 50/100]	Time  0.027 ( 0.028)	Loss 2.4280e-01 (3.1372e-01)	Acc@1  92.00 ( 89.92)	Acc@5  99.00 ( 99.65)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 2.0862e-01 (3.0859e-01)	Acc@1  92.00 ( 90.08)	Acc@5 100.00 ( 99.69)
Test: [ 70/100]	Time  0.027 ( 0.027)	Loss 4.1650e-01 (3.0871e-01)	Acc@1  88.00 ( 90.15)	Acc@5 100.00 ( 99.68)
Test: [ 80/100]	Time  0.025 ( 0.027)	Loss 1.8237e-01 (3.1005e-01)	Acc@1  92.00 ( 90.07)	Acc@5 100.00 ( 99.67)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 1.8665e-01 (3.0806e-01)	Acc@1  92.00 ( 90.16)	Acc@5 100.00 ( 99.66)
 * Acc@1 90.160 Acc@5 99.660
### epoch[41] execution time: 28.355881214141846
EPOCH 42
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [42][  0/391]	Time  0.240 ( 0.240)	Data  0.172 ( 0.172)	Loss 2.4805e-01 (2.4805e-01)	Acc@1  91.41 ( 91.41)	Acc@5 100.00 (100.00)
Epoch: [42][ 10/391]	Time  0.065 ( 0.082)	Data  0.001 ( 0.017)	Loss 1.1920e-01 (1.8350e-01)	Acc@1  96.88 ( 93.82)	Acc@5 100.00 ( 99.93)
Epoch: [42][ 20/391]	Time  0.065 ( 0.074)	Data  0.001 ( 0.009)	Loss 1.9019e-01 (1.8963e-01)	Acc@1  93.75 ( 93.34)	Acc@5  99.22 ( 99.89)
Epoch: [42][ 30/391]	Time  0.067 ( 0.071)	Data  0.001 ( 0.007)	Loss 1.1090e-01 (1.8611e-01)	Acc@1  96.09 ( 93.55)	Acc@5 100.00 ( 99.90)
Epoch: [42][ 40/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.005)	Loss 1.3098e-01 (1.8252e-01)	Acc@1  96.88 ( 93.50)	Acc@5 100.00 ( 99.90)
Epoch: [42][ 50/391]	Time  0.062 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.4307e-01 (1.7637e-01)	Acc@1  95.31 ( 93.80)	Acc@5 100.00 ( 99.91)
Epoch: [42][ 60/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.2769e-01 (1.7377e-01)	Acc@1  94.53 ( 93.78)	Acc@5 100.00 ( 99.90)
Epoch: [42][ 70/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.5771e-01 (1.7216e-01)	Acc@1  92.97 ( 93.86)	Acc@5 100.00 ( 99.90)
Epoch: [42][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.4844e-01 (1.7313e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 ( 99.90)
Epoch: [42][ 90/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.5808e-01 (1.7242e-01)	Acc@1  95.31 ( 93.82)	Acc@5 100.00 ( 99.90)
Epoch: [42][100/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.7126e-01 (1.7393e-01)	Acc@1  92.97 ( 93.76)	Acc@5 100.00 ( 99.90)
Epoch: [42][110/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.6992e-01 (1.7539e-01)	Acc@1  96.88 ( 93.74)	Acc@5 100.00 ( 99.90)
Epoch: [42][120/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.8101e-01 (1.7574e-01)	Acc@1  92.19 ( 93.73)	Acc@5 100.00 ( 99.91)
Epoch: [42][130/391]	Time  0.071 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9507e-01 (1.7581e-01)	Acc@1  91.41 ( 93.74)	Acc@5 100.00 ( 99.92)
Epoch: [42][140/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6418e-01 (1.7622e-01)	Acc@1  92.97 ( 93.75)	Acc@5 100.00 ( 99.91)
Epoch: [42][150/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.0776e-01 (1.7631e-01)	Acc@1  92.19 ( 93.75)	Acc@5  99.22 ( 99.91)
Epoch: [42][160/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8323e-01 (1.7458e-01)	Acc@1  92.97 ( 93.80)	Acc@5 100.00 ( 99.91)
Epoch: [42][170/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.5293e-01 (1.7583e-01)	Acc@1  89.06 ( 93.74)	Acc@5 100.00 ( 99.91)
Epoch: [42][180/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.1655e-01 (1.7417e-01)	Acc@1  92.19 ( 93.81)	Acc@5 100.00 ( 99.91)
Epoch: [42][190/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.5093e-02 (1.7422e-01)	Acc@1  96.09 ( 93.82)	Acc@5 100.00 ( 99.91)
Epoch: [42][200/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2732e-01 (1.7408e-01)	Acc@1  94.53 ( 93.84)	Acc@5 100.00 ( 99.91)
Epoch: [42][210/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.8296e-01 (1.7478e-01)	Acc@1  89.84 ( 93.77)	Acc@5 100.00 ( 99.91)
Epoch: [42][220/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4978e-01 (1.7469e-01)	Acc@1  92.97 ( 93.77)	Acc@5 100.00 ( 99.91)
Epoch: [42][230/391]	Time  0.071 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4697e-01 (1.7409e-01)	Acc@1  94.53 ( 93.78)	Acc@5 100.00 ( 99.92)
Epoch: [42][240/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5454e-01 (1.7360e-01)	Acc@1  93.75 ( 93.82)	Acc@5 100.00 ( 99.92)
Epoch: [42][250/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7297e-01 (1.7304e-01)	Acc@1  95.31 ( 93.85)	Acc@5 100.00 ( 99.91)
Epoch: [42][260/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1719e-01 (1.7355e-01)	Acc@1  96.09 ( 93.85)	Acc@5 100.00 ( 99.91)
Epoch: [42][270/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.6489e-01 (1.7371e-01)	Acc@1  90.62 ( 93.85)	Acc@5  99.22 ( 99.91)
Epoch: [42][280/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1536e-01 (1.7393e-01)	Acc@1  98.44 ( 93.88)	Acc@5 100.00 ( 99.91)
Epoch: [42][290/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0205e-01 (1.7306e-01)	Acc@1  97.66 ( 93.92)	Acc@5 100.00 ( 99.91)
Epoch: [42][300/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1631e-01 (1.7239e-01)	Acc@1  92.97 ( 93.93)	Acc@5 100.00 ( 99.91)
Epoch: [42][310/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.4666e-02 (1.7261e-01)	Acc@1  96.09 ( 93.92)	Acc@5 100.00 ( 99.92)
Epoch: [42][320/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5027e-01 (1.7330e-01)	Acc@1  96.09 ( 93.92)	Acc@5 100.00 ( 99.91)
Epoch: [42][330/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1436e-01 (1.7311e-01)	Acc@1  92.97 ( 93.93)	Acc@5 100.00 ( 99.92)
Epoch: [42][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1985e-01 (1.7343e-01)	Acc@1  91.41 ( 93.93)	Acc@5 100.00 ( 99.92)
Epoch: [42][350/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6418e-01 (1.7338e-01)	Acc@1  93.75 ( 93.92)	Acc@5 100.00 ( 99.92)
Epoch: [42][360/391]	Time  0.070 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1194e-01 (1.7360e-01)	Acc@1  96.88 ( 93.91)	Acc@5 100.00 ( 99.92)
Epoch: [42][370/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.5391e-01 (1.7367e-01)	Acc@1  92.19 ( 93.91)	Acc@5 100.00 ( 99.92)
Epoch: [42][380/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0071e-01 (1.7433e-01)	Acc@1  97.66 ( 93.89)	Acc@5 100.00 ( 99.92)
Epoch: [42][390/391]	Time  0.048 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6797e-01 (1.7515e-01)	Acc@1  93.75 ( 93.85)	Acc@5 100.00 ( 99.92)
## e[42] optimizer.zero_grad (sum) time: 0.3940274715423584
## e[42]       loss.backward (sum) time: 7.2284440994262695
## e[42]      optimizer.step (sum) time: 3.4572415351867676
## epoch[42] training(only) time: 25.58253288269043
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 3.4424e-01 (3.4424e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.040)	Loss 4.7510e-01 (3.1924e-01)	Acc@1  88.00 ( 90.09)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 3.5498e-01 (3.1322e-01)	Acc@1  86.00 ( 90.10)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.027 ( 0.031)	Loss 3.4277e-01 (3.1986e-01)	Acc@1  87.00 ( 90.10)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.028 ( 0.030)	Loss 3.7671e-01 (3.2296e-01)	Acc@1  88.00 ( 90.05)	Acc@5 100.00 ( 99.59)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 2.6099e-01 (3.2160e-01)	Acc@1  90.00 ( 90.06)	Acc@5  99.00 ( 99.63)
Test: [ 60/100]	Time  0.027 ( 0.028)	Loss 2.2253e-01 (3.1593e-01)	Acc@1  92.00 ( 90.18)	Acc@5 100.00 ( 99.67)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 4.0063e-01 (3.1171e-01)	Acc@1  88.00 ( 90.31)	Acc@5 100.00 ( 99.68)
Test: [ 80/100]	Time  0.027 ( 0.027)	Loss 1.7981e-01 (3.1343e-01)	Acc@1  94.00 ( 90.27)	Acc@5 100.00 ( 99.67)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 2.0813e-01 (3.1244e-01)	Acc@1  90.00 ( 90.18)	Acc@5 100.00 ( 99.67)
 * Acc@1 90.210 Acc@5 99.680
### epoch[42] execution time: 28.39962863922119
EPOCH 43
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [43][  0/391]	Time  0.247 ( 0.247)	Data  0.180 ( 0.180)	Loss 1.4697e-01 (1.4697e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [43][ 10/391]	Time  0.066 ( 0.082)	Data  0.001 ( 0.017)	Loss 1.0461e-01 (1.9536e-01)	Acc@1  97.66 ( 93.11)	Acc@5 100.00 ( 99.93)
Epoch: [43][ 20/391]	Time  0.063 ( 0.073)	Data  0.001 ( 0.010)	Loss 1.8665e-01 (1.8272e-01)	Acc@1  93.75 ( 93.49)	Acc@5  99.22 ( 99.93)
Epoch: [43][ 30/391]	Time  0.065 ( 0.071)	Data  0.001 ( 0.007)	Loss 1.0681e-01 (1.7407e-01)	Acc@1  95.31 ( 93.93)	Acc@5 100.00 ( 99.95)
Epoch: [43][ 40/391]	Time  0.060 ( 0.069)	Data  0.001 ( 0.005)	Loss 1.3806e-01 (1.7444e-01)	Acc@1  94.53 ( 93.92)	Acc@5  99.22 ( 99.92)
Epoch: [43][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.005)	Loss 1.1188e-01 (1.7229e-01)	Acc@1  96.88 ( 93.84)	Acc@5 100.00 ( 99.89)
Epoch: [43][ 60/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.5601e-01 (1.7207e-01)	Acc@1  92.19 ( 93.87)	Acc@5 100.00 ( 99.90)
Epoch: [43][ 70/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.004)	Loss 2.0825e-01 (1.6912e-01)	Acc@1  92.97 ( 94.05)	Acc@5 100.00 ( 99.91)
Epoch: [43][ 80/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.6614e-01 (1.6759e-01)	Acc@1  94.53 ( 94.05)	Acc@5 100.00 ( 99.90)
Epoch: [43][ 90/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.0227e-01 (1.6463e-01)	Acc@1  93.75 ( 94.21)	Acc@5 100.00 ( 99.91)
Epoch: [43][100/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.1176e-01 (1.6437e-01)	Acc@1  94.53 ( 94.23)	Acc@5 100.00 ( 99.91)
Epoch: [43][110/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.2756e-01 (1.6397e-01)	Acc@1  94.53 ( 94.17)	Acc@5 100.00 ( 99.92)
Epoch: [43][120/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.0093e-01 (1.6665e-01)	Acc@1  93.75 ( 94.05)	Acc@5 100.00 ( 99.92)
Epoch: [43][130/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.1338e-01 (1.6696e-01)	Acc@1  94.53 ( 94.08)	Acc@5 100.00 ( 99.93)
Epoch: [43][140/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6455e-01 (1.6906e-01)	Acc@1  93.75 ( 93.98)	Acc@5 100.00 ( 99.93)
Epoch: [43][150/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0907e-01 (1.6898e-01)	Acc@1  97.66 ( 94.02)	Acc@5 100.00 ( 99.93)
Epoch: [43][160/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.3716e-01 (1.6883e-01)	Acc@1  89.06 ( 94.04)	Acc@5 100.00 ( 99.93)
Epoch: [43][170/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0801e-01 (1.6901e-01)	Acc@1  91.41 ( 94.04)	Acc@5 100.00 ( 99.93)
Epoch: [43][180/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8262e-01 (1.6838e-01)	Acc@1  92.97 ( 94.06)	Acc@5 100.00 ( 99.93)
Epoch: [43][190/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.5073e-01 (1.7117e-01)	Acc@1  91.41 ( 93.96)	Acc@5 100.00 ( 99.92)
Epoch: [43][200/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0632e-01 (1.6964e-01)	Acc@1  96.88 ( 94.01)	Acc@5 100.00 ( 99.92)
Epoch: [43][210/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5979e-01 (1.6989e-01)	Acc@1  94.53 ( 93.98)	Acc@5 100.00 ( 99.93)
Epoch: [43][220/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2585e-01 (1.7003e-01)	Acc@1  93.75 ( 93.97)	Acc@5 100.00 ( 99.92)
Epoch: [43][230/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1436e-01 (1.6983e-01)	Acc@1  93.75 ( 94.00)	Acc@5 100.00 ( 99.92)
Epoch: [43][240/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7432e-01 (1.6836e-01)	Acc@1  94.53 ( 94.07)	Acc@5 100.00 ( 99.93)
Epoch: [43][250/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6992e-01 (1.6864e-01)	Acc@1  94.53 ( 94.05)	Acc@5 100.00 ( 99.93)
Epoch: [43][260/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3721e-01 (1.6971e-01)	Acc@1  95.31 ( 94.00)	Acc@5 100.00 ( 99.93)
Epoch: [43][270/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3318e-01 (1.6885e-01)	Acc@1  96.88 ( 94.05)	Acc@5 100.00 ( 99.93)
Epoch: [43][280/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4221e-01 (1.6931e-01)	Acc@1  94.53 ( 94.06)	Acc@5 100.00 ( 99.93)
Epoch: [43][290/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.6392e-01 (1.7059e-01)	Acc@1  89.06 ( 94.02)	Acc@5 100.00 ( 99.93)
Epoch: [43][300/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1371e-01 (1.6988e-01)	Acc@1  94.53 ( 94.03)	Acc@5 100.00 ( 99.92)
Epoch: [43][310/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2329e-01 (1.7004e-01)	Acc@1  95.31 ( 94.03)	Acc@5 100.00 ( 99.93)
Epoch: [43][320/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0413e-01 (1.7089e-01)	Acc@1  96.09 ( 93.97)	Acc@5 100.00 ( 99.93)
Epoch: [43][330/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2671e-01 (1.7023e-01)	Acc@1  94.53 ( 94.01)	Acc@5 100.00 ( 99.93)
Epoch: [43][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1975e-01 (1.7110e-01)	Acc@1  97.66 ( 93.97)	Acc@5 100.00 ( 99.93)
Epoch: [43][350/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5564e-01 (1.7129e-01)	Acc@1  94.53 ( 93.98)	Acc@5 100.00 ( 99.92)
Epoch: [43][360/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4453e-01 (1.7099e-01)	Acc@1  96.09 ( 94.00)	Acc@5 100.00 ( 99.93)
Epoch: [43][370/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2561e-01 (1.7074e-01)	Acc@1  96.09 ( 94.00)	Acc@5 100.00 ( 99.92)
Epoch: [43][380/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4294e-01 (1.7068e-01)	Acc@1  95.31 ( 94.02)	Acc@5 100.00 ( 99.93)
Epoch: [43][390/391]	Time  0.050 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3953e-01 (1.7073e-01)	Acc@1  95.00 ( 94.02)	Acc@5 100.00 ( 99.93)
## e[43] optimizer.zero_grad (sum) time: 0.3856017589569092
## e[43]       loss.backward (sum) time: 7.20134162902832
## e[43]      optimizer.step (sum) time: 3.406741142272949
## epoch[43] training(only) time: 25.418301105499268
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 3.3911e-01 (3.3911e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.040)	Loss 4.3042e-01 (3.2435e-01)	Acc@1  90.00 ( 90.55)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 3.2764e-01 (3.1552e-01)	Acc@1  85.00 ( 90.29)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 3.0542e-01 (3.2223e-01)	Acc@1  88.00 ( 90.19)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.025 ( 0.029)	Loss 3.6230e-01 (3.2506e-01)	Acc@1  91.00 ( 90.00)	Acc@5 100.00 ( 99.63)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 2.2913e-01 (3.2191e-01)	Acc@1  92.00 ( 90.10)	Acc@5 100.00 ( 99.63)
Test: [ 60/100]	Time  0.025 ( 0.028)	Loss 2.1716e-01 (3.1677e-01)	Acc@1  94.00 ( 90.10)	Acc@5 100.00 ( 99.67)
Test: [ 70/100]	Time  0.028 ( 0.027)	Loss 4.1748e-01 (3.1434e-01)	Acc@1  89.00 ( 90.20)	Acc@5 100.00 ( 99.69)
Test: [ 80/100]	Time  0.027 ( 0.027)	Loss 1.8066e-01 (3.1488e-01)	Acc@1  95.00 ( 90.16)	Acc@5 100.00 ( 99.68)
Test: [ 90/100]	Time  0.027 ( 0.027)	Loss 2.0520e-01 (3.1274e-01)	Acc@1  93.00 ( 90.16)	Acc@5 100.00 ( 99.68)
 * Acc@1 90.250 Acc@5 99.680
### epoch[43] execution time: 28.226827144622803
EPOCH 44
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [44][  0/391]	Time  0.244 ( 0.244)	Data  0.179 ( 0.179)	Loss 2.0093e-01 (2.0093e-01)	Acc@1  89.84 ( 89.84)	Acc@5 100.00 (100.00)
Epoch: [44][ 10/391]	Time  0.064 ( 0.081)	Data  0.001 ( 0.017)	Loss 2.2791e-01 (1.7581e-01)	Acc@1  91.41 ( 93.39)	Acc@5 100.00 (100.00)
Epoch: [44][ 20/391]	Time  0.064 ( 0.073)	Data  0.001 ( 0.010)	Loss 1.5808e-01 (1.5901e-01)	Acc@1  94.53 ( 94.23)	Acc@5 100.00 (100.00)
Epoch: [44][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.007)	Loss 1.5833e-01 (1.5152e-01)	Acc@1  93.75 ( 94.41)	Acc@5  99.22 ( 99.97)
Epoch: [44][ 40/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.005)	Loss 1.3416e-01 (1.5800e-01)	Acc@1  97.66 ( 94.38)	Acc@5 100.00 ( 99.96)
Epoch: [44][ 50/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.005)	Loss 1.8909e-01 (1.6151e-01)	Acc@1  93.75 ( 94.35)	Acc@5  99.22 ( 99.94)
Epoch: [44][ 60/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.4490e-01 (1.6285e-01)	Acc@1  93.75 ( 94.29)	Acc@5 100.00 ( 99.94)
Epoch: [44][ 70/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.4661e-01 (1.6263e-01)	Acc@1  92.97 ( 94.34)	Acc@5 100.00 ( 99.93)
Epoch: [44][ 80/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.8762e-01 (1.6111e-01)	Acc@1  92.97 ( 94.43)	Acc@5 100.00 ( 99.94)
Epoch: [44][ 90/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.2256e-01 (1.6195e-01)	Acc@1  93.75 ( 94.35)	Acc@5 100.00 ( 99.94)
Epoch: [44][100/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.7686e-01 (1.6281e-01)	Acc@1  89.84 ( 94.28)	Acc@5 100.00 ( 99.95)
Epoch: [44][110/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.2299e-01 (1.6163e-01)	Acc@1  93.75 ( 94.28)	Acc@5  99.22 ( 99.94)
Epoch: [44][120/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.3879e-01 (1.6399e-01)	Acc@1  96.09 ( 94.23)	Acc@5 100.00 ( 99.93)
Epoch: [44][130/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8896e-01 (1.6537e-01)	Acc@1  91.41 ( 94.16)	Acc@5 100.00 ( 99.92)
Epoch: [44][140/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.4158e-01 (1.6338e-01)	Acc@1  92.97 ( 94.21)	Acc@5 100.00 ( 99.93)
Epoch: [44][150/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0352e-01 (1.6390e-01)	Acc@1  96.09 ( 94.23)	Acc@5 100.00 ( 99.93)
Epoch: [44][160/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4001e-01 (1.6517e-01)	Acc@1  94.53 ( 94.22)	Acc@5 100.00 ( 99.93)
Epoch: [44][170/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6797e-01 (1.6575e-01)	Acc@1  95.31 ( 94.19)	Acc@5 100.00 ( 99.94)
Epoch: [44][180/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7078e-01 (1.6464e-01)	Acc@1  93.75 ( 94.22)	Acc@5 100.00 ( 99.94)
Epoch: [44][190/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6760e-01 (1.6627e-01)	Acc@1  93.75 ( 94.14)	Acc@5 100.00 ( 99.94)
Epoch: [44][200/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.5391e-01 (1.6672e-01)	Acc@1  92.97 ( 94.12)	Acc@5  99.22 ( 99.94)
Epoch: [44][210/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1619e-01 (1.6732e-01)	Acc@1  91.41 ( 94.08)	Acc@5 100.00 ( 99.94)
Epoch: [44][220/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2009e-01 (1.6720e-01)	Acc@1  92.97 ( 94.09)	Acc@5 100.00 ( 99.94)
Epoch: [44][230/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9812e-01 (1.6713e-01)	Acc@1  96.09 ( 94.08)	Acc@5 100.00 ( 99.94)
Epoch: [44][240/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8408e-01 (1.6762e-01)	Acc@1  94.53 ( 94.06)	Acc@5 100.00 ( 99.94)
Epoch: [44][250/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3196e-01 (1.6811e-01)	Acc@1  95.31 ( 94.03)	Acc@5 100.00 ( 99.95)
Epoch: [44][260/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.6147e-01 (1.6793e-01)	Acc@1  93.75 ( 94.06)	Acc@5 100.00 ( 99.95)
Epoch: [44][270/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7407e-01 (1.6746e-01)	Acc@1  94.53 ( 94.11)	Acc@5 100.00 ( 99.95)
Epoch: [44][280/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7505e-01 (1.6780e-01)	Acc@1  92.19 ( 94.10)	Acc@5 100.00 ( 99.94)
Epoch: [44][290/391]	Time  0.071 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2299e-01 (1.6794e-01)	Acc@1  96.88 ( 94.09)	Acc@5 100.00 ( 99.94)
Epoch: [44][300/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6833e-01 (1.6835e-01)	Acc@1  92.97 ( 94.08)	Acc@5 100.00 ( 99.94)
Epoch: [44][310/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0508e-01 (1.6818e-01)	Acc@1  92.19 ( 94.10)	Acc@5 100.00 ( 99.94)
Epoch: [44][320/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1741e-01 (1.6823e-01)	Acc@1  92.19 ( 94.09)	Acc@5 100.00 ( 99.94)
Epoch: [44][330/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7102e-01 (1.6866e-01)	Acc@1  92.19 ( 94.06)	Acc@5 100.00 ( 99.94)
Epoch: [44][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2061e-01 (1.6885e-01)	Acc@1  95.31 ( 94.04)	Acc@5 100.00 ( 99.94)
Epoch: [44][350/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2976e-01 (1.6932e-01)	Acc@1  95.31 ( 94.02)	Acc@5 100.00 ( 99.94)
Epoch: [44][360/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.4915e-01 (1.6920e-01)	Acc@1  90.62 ( 94.01)	Acc@5  99.22 ( 99.94)
Epoch: [44][370/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7139e-01 (1.6939e-01)	Acc@1  95.31 ( 94.01)	Acc@5 100.00 ( 99.93)
Epoch: [44][380/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9568e-01 (1.6946e-01)	Acc@1  93.75 ( 94.02)	Acc@5 100.00 ( 99.93)
Epoch: [44][390/391]	Time  0.048 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5259e-01 (1.6923e-01)	Acc@1  95.00 ( 94.03)	Acc@5 100.00 ( 99.94)
## e[44] optimizer.zero_grad (sum) time: 0.38458681106567383
## e[44]       loss.backward (sum) time: 7.217370271682739
## e[44]      optimizer.step (sum) time: 3.4519927501678467
## epoch[44] training(only) time: 25.53174376487732
# Switched to evaluate mode...
Test: [  0/100]	Time  0.193 ( 0.193)	Loss 3.2324e-01 (3.2324e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.040)	Loss 4.1968e-01 (3.2188e-01)	Acc@1  90.00 ( 89.91)	Acc@5 100.00 ( 99.55)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 3.2446e-01 (3.1296e-01)	Acc@1  87.00 ( 89.95)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.024 ( 0.030)	Loss 3.3374e-01 (3.2476e-01)	Acc@1  85.00 ( 89.71)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 3.3350e-01 (3.2456e-01)	Acc@1  92.00 ( 89.78)	Acc@5  99.00 ( 99.63)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 1.7725e-01 (3.2331e-01)	Acc@1  94.00 ( 89.86)	Acc@5  99.00 ( 99.63)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 2.3230e-01 (3.1842e-01)	Acc@1  90.00 ( 89.92)	Acc@5 100.00 ( 99.66)
Test: [ 70/100]	Time  0.026 ( 0.027)	Loss 3.7598e-01 (3.1494e-01)	Acc@1  87.00 ( 90.03)	Acc@5 100.00 ( 99.66)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 1.8445e-01 (3.1670e-01)	Acc@1  94.00 ( 90.00)	Acc@5 100.00 ( 99.64)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 2.4121e-01 (3.1434e-01)	Acc@1  90.00 ( 89.97)	Acc@5 100.00 ( 99.66)
 * Acc@1 90.020 Acc@5 99.650
### epoch[44] execution time: 28.28862953186035
EPOCH 45
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [45][  0/391]	Time  0.251 ( 0.251)	Data  0.185 ( 0.185)	Loss 2.5659e-01 (2.5659e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
Epoch: [45][ 10/391]	Time  0.065 ( 0.083)	Data  0.001 ( 0.018)	Loss 1.1212e-01 (1.8264e-01)	Acc@1  95.31 ( 93.25)	Acc@5 100.00 ( 99.93)
Epoch: [45][ 20/391]	Time  0.066 ( 0.075)	Data  0.001 ( 0.010)	Loss 1.3757e-01 (1.6859e-01)	Acc@1  95.31 ( 93.60)	Acc@5  99.22 ( 99.81)
Epoch: [45][ 30/391]	Time  0.064 ( 0.072)	Data  0.001 ( 0.007)	Loss 1.2524e-01 (1.6381e-01)	Acc@1  94.53 ( 93.95)	Acc@5 100.00 ( 99.85)
Epoch: [45][ 40/391]	Time  0.065 ( 0.070)	Data  0.001 ( 0.006)	Loss 1.7542e-01 (1.6458e-01)	Acc@1  96.09 ( 94.13)	Acc@5 100.00 ( 99.89)
Epoch: [45][ 50/391]	Time  0.063 ( 0.069)	Data  0.001 ( 0.005)	Loss 1.0400e-01 (1.6222e-01)	Acc@1  95.31 ( 94.18)	Acc@5 100.00 ( 99.88)
Epoch: [45][ 60/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 2.1191e-01 (1.6137e-01)	Acc@1  91.41 ( 94.22)	Acc@5 100.00 ( 99.90)
Epoch: [45][ 70/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 6.4026e-02 (1.5925e-01)	Acc@1  98.44 ( 94.33)	Acc@5 100.00 ( 99.91)
Epoch: [45][ 80/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.2463e-01 (1.5980e-01)	Acc@1  94.53 ( 94.36)	Acc@5 100.00 ( 99.91)
Epoch: [45][ 90/391]	Time  0.077 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.7102e-01 (1.5926e-01)	Acc@1  92.19 ( 94.32)	Acc@5 100.00 ( 99.92)
Epoch: [45][100/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.3562e-01 (1.5796e-01)	Acc@1  93.75 ( 94.35)	Acc@5 100.00 ( 99.93)
Epoch: [45][110/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.0095e-01 (1.5969e-01)	Acc@1  96.09 ( 94.29)	Acc@5 100.00 ( 99.92)
Epoch: [45][120/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.8003e-01 (1.6255e-01)	Acc@1  92.19 ( 94.21)	Acc@5 100.00 ( 99.92)
Epoch: [45][130/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5918e-01 (1.6487e-01)	Acc@1  93.75 ( 94.15)	Acc@5 100.00 ( 99.92)
Epoch: [45][140/391]	Time  0.070 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2036e-01 (1.6514e-01)	Acc@1  95.31 ( 94.18)	Acc@5 100.00 ( 99.91)
Epoch: [45][150/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.6304e-02 (1.6466e-01)	Acc@1  96.88 ( 94.22)	Acc@5 100.00 ( 99.92)
Epoch: [45][160/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1444e-01 (1.6455e-01)	Acc@1  96.88 ( 94.23)	Acc@5 100.00 ( 99.92)
Epoch: [45][170/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1298e-01 (1.6335e-01)	Acc@1  96.09 ( 94.26)	Acc@5 100.00 ( 99.93)
Epoch: [45][180/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1078e-01 (1.6233e-01)	Acc@1  96.09 ( 94.29)	Acc@5  99.22 ( 99.93)
Epoch: [45][190/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5002e-01 (1.6096e-01)	Acc@1  92.97 ( 94.35)	Acc@5 100.00 ( 99.93)
Epoch: [45][200/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8518e-01 (1.6318e-01)	Acc@1  92.19 ( 94.28)	Acc@5 100.00 ( 99.93)
Epoch: [45][210/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.1863e-01 (1.6418e-01)	Acc@1  95.31 ( 94.26)	Acc@5 100.00 ( 99.93)
Epoch: [45][220/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.0117e-01 (1.6319e-01)	Acc@1  92.19 ( 94.29)	Acc@5 100.00 ( 99.93)
Epoch: [45][230/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8994e-01 (1.6340e-01)	Acc@1  92.97 ( 94.29)	Acc@5 100.00 ( 99.93)
Epoch: [45][240/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7957e-01 (1.6365e-01)	Acc@1  91.41 ( 94.28)	Acc@5 100.00 ( 99.94)
Epoch: [45][250/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5808e-01 (1.6370e-01)	Acc@1  95.31 ( 94.27)	Acc@5 100.00 ( 99.93)
Epoch: [45][260/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3037e-01 (1.6357e-01)	Acc@1  92.19 ( 94.26)	Acc@5 100.00 ( 99.93)
Epoch: [45][270/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8530e-01 (1.6431e-01)	Acc@1  92.97 ( 94.24)	Acc@5 100.00 ( 99.93)
Epoch: [45][280/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0541e-01 (1.6466e-01)	Acc@1  96.09 ( 94.25)	Acc@5 100.00 ( 99.93)
Epoch: [45][290/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9531e-01 (1.6602e-01)	Acc@1  92.97 ( 94.18)	Acc@5 100.00 ( 99.93)
Epoch: [45][300/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4734e-01 (1.6556e-01)	Acc@1  92.97 ( 94.20)	Acc@5 100.00 ( 99.93)
Epoch: [45][310/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.4548e-01 (1.6563e-01)	Acc@1  89.84 ( 94.20)	Acc@5 100.00 ( 99.93)
Epoch: [45][320/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8945e-01 (1.6623e-01)	Acc@1  91.41 ( 94.18)	Acc@5 100.00 ( 99.93)
Epoch: [45][330/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1692e-01 (1.6656e-01)	Acc@1  92.19 ( 94.17)	Acc@5  99.22 ( 99.93)
Epoch: [45][340/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7896e-01 (1.6641e-01)	Acc@1  90.62 ( 94.17)	Acc@5 100.00 ( 99.93)
Epoch: [45][350/391]	Time  0.071 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6809e-01 (1.6640e-01)	Acc@1  94.53 ( 94.16)	Acc@5  99.22 ( 99.93)
Epoch: [45][360/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3376e-01 (1.6637e-01)	Acc@1  91.41 ( 94.16)	Acc@5 100.00 ( 99.92)
Epoch: [45][370/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8213e-01 (1.6616e-01)	Acc@1  92.97 ( 94.17)	Acc@5 100.00 ( 99.93)
Epoch: [45][380/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8176e-01 (1.6616e-01)	Acc@1  93.75 ( 94.17)	Acc@5  99.22 ( 99.92)
Epoch: [45][390/391]	Time  0.050 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8518e-01 (1.6574e-01)	Acc@1  91.25 ( 94.18)	Acc@5 100.00 ( 99.92)
## e[45] optimizer.zero_grad (sum) time: 0.38730311393737793
## e[45]       loss.backward (sum) time: 7.268683671951294
## e[45]      optimizer.step (sum) time: 3.4301462173461914
## epoch[45] training(only) time: 25.574663400650024
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 3.6157e-01 (3.6157e-01)	Acc@1  86.00 ( 86.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.041)	Loss 4.2163e-01 (3.2085e-01)	Acc@1  89.00 ( 89.91)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 3.8257e-01 (3.1647e-01)	Acc@1  85.00 ( 90.05)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.024 ( 0.030)	Loss 3.2593e-01 (3.2550e-01)	Acc@1  88.00 ( 89.77)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 4.1333e-01 (3.2635e-01)	Acc@1  88.00 ( 89.71)	Acc@5 100.00 ( 99.66)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 2.2668e-01 (3.2368e-01)	Acc@1  93.00 ( 89.84)	Acc@5  99.00 ( 99.69)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 2.0911e-01 (3.1896e-01)	Acc@1  94.00 ( 89.98)	Acc@5 100.00 ( 99.72)
Test: [ 70/100]	Time  0.024 ( 0.027)	Loss 4.6362e-01 (3.1629e-01)	Acc@1  88.00 ( 90.10)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 2.1619e-01 (3.1754e-01)	Acc@1  94.00 ( 90.06)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 2.2131e-01 (3.1470e-01)	Acc@1  89.00 ( 90.08)	Acc@5 100.00 ( 99.75)
 * Acc@1 90.180 Acc@5 99.750
### epoch[45] execution time: 28.383209228515625
EPOCH 46
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [46][  0/391]	Time  0.246 ( 0.246)	Data  0.176 ( 0.176)	Loss 1.1884e-01 (1.1884e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [46][ 10/391]	Time  0.064 ( 0.081)	Data  0.001 ( 0.017)	Loss 1.3953e-01 (1.5110e-01)	Acc@1  96.09 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [46][ 20/391]	Time  0.064 ( 0.073)	Data  0.001 ( 0.009)	Loss 2.2144e-01 (1.5672e-01)	Acc@1  93.75 ( 94.98)	Acc@5  99.22 ( 99.93)
Epoch: [46][ 30/391]	Time  0.062 ( 0.070)	Data  0.001 ( 0.007)	Loss 1.2201e-01 (1.4859e-01)	Acc@1  97.66 ( 95.04)	Acc@5 100.00 ( 99.95)
Epoch: [46][ 40/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.005)	Loss 1.1365e-01 (1.5086e-01)	Acc@1  96.09 ( 94.89)	Acc@5 100.00 ( 99.96)
Epoch: [46][ 50/391]	Time  0.060 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.1768e-01 (1.5051e-01)	Acc@1  95.31 ( 94.76)	Acc@5 100.00 ( 99.94)
Epoch: [46][ 60/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.004)	Loss 7.4829e-02 (1.5308e-01)	Acc@1  96.88 ( 94.68)	Acc@5 100.00 ( 99.92)
Epoch: [46][ 70/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.2793e-01 (1.5713e-01)	Acc@1  95.31 ( 94.48)	Acc@5 100.00 ( 99.91)
Epoch: [46][ 80/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.003)	Loss 9.1858e-02 (1.5693e-01)	Acc@1  96.09 ( 94.47)	Acc@5 100.00 ( 99.89)
Epoch: [46][ 90/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.6772e-01 (1.5761e-01)	Acc@1  93.75 ( 94.42)	Acc@5 100.00 ( 99.90)
Epoch: [46][100/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.5771e-01 (1.5651e-01)	Acc@1  92.97 ( 94.44)	Acc@5 100.00 ( 99.90)
Epoch: [46][110/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.1863e-01 (1.5771e-01)	Acc@1  91.41 ( 94.48)	Acc@5 100.00 ( 99.89)
Epoch: [46][120/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 9.4604e-02 (1.5826e-01)	Acc@1  96.88 ( 94.47)	Acc@5 100.00 ( 99.90)
Epoch: [46][130/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3477e-01 (1.5732e-01)	Acc@1  92.97 ( 94.49)	Acc@5 100.00 ( 99.89)
Epoch: [46][140/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.1411e-01 (1.5805e-01)	Acc@1  92.97 ( 94.43)	Acc@5 100.00 ( 99.90)
Epoch: [46][150/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.0520e-01 (1.5891e-01)	Acc@1  92.97 ( 94.34)	Acc@5 100.00 ( 99.91)
Epoch: [46][160/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.5815e-02 (1.5797e-01)	Acc@1  97.66 ( 94.41)	Acc@5 100.00 ( 99.91)
Epoch: [46][170/391]	Time  0.060 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2854e-01 (1.5883e-01)	Acc@1  96.09 ( 94.39)	Acc@5 100.00 ( 99.92)
Epoch: [46][180/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8604e-01 (1.5873e-01)	Acc@1  92.97 ( 94.38)	Acc@5 100.00 ( 99.92)
Epoch: [46][190/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3220e-01 (1.5839e-01)	Acc@1  94.53 ( 94.39)	Acc@5 100.00 ( 99.93)
Epoch: [46][200/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0645e-01 (1.5877e-01)	Acc@1  96.09 ( 94.40)	Acc@5 100.00 ( 99.93)
Epoch: [46][210/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4978e-01 (1.5827e-01)	Acc@1  92.97 ( 94.42)	Acc@5 100.00 ( 99.93)
Epoch: [46][220/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1069e-01 (1.5994e-01)	Acc@1  93.75 ( 94.36)	Acc@5 100.00 ( 99.93)
Epoch: [46][230/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1792e-01 (1.6191e-01)	Acc@1  93.75 ( 94.27)	Acc@5 100.00 ( 99.92)
Epoch: [46][240/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3635e-01 (1.6176e-01)	Acc@1  94.53 ( 94.27)	Acc@5 100.00 ( 99.93)
Epoch: [46][250/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5833e-01 (1.6146e-01)	Acc@1  92.97 ( 94.26)	Acc@5 100.00 ( 99.93)
Epoch: [46][260/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9861e-01 (1.6143e-01)	Acc@1  92.97 ( 94.27)	Acc@5  99.22 ( 99.93)
Epoch: [46][270/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9470e-01 (1.6134e-01)	Acc@1  91.41 ( 94.26)	Acc@5 100.00 ( 99.93)
Epoch: [46][280/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.1064e-02 (1.6143e-01)	Acc@1  97.66 ( 94.25)	Acc@5 100.00 ( 99.93)
Epoch: [46][290/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4734e-01 (1.6146e-01)	Acc@1  95.31 ( 94.25)	Acc@5 100.00 ( 99.93)
Epoch: [46][300/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2903e-01 (1.6136e-01)	Acc@1  95.31 ( 94.24)	Acc@5  99.22 ( 99.93)
Epoch: [46][310/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0801e-01 (1.6144e-01)	Acc@1  92.97 ( 94.22)	Acc@5 100.00 ( 99.93)
Epoch: [46][320/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2610e-01 (1.6154e-01)	Acc@1  92.97 ( 94.19)	Acc@5 100.00 ( 99.93)
Epoch: [46][330/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6650e-01 (1.6158e-01)	Acc@1  93.75 ( 94.19)	Acc@5 100.00 ( 99.93)
Epoch: [46][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3340e-01 (1.6261e-01)	Acc@1  87.50 ( 94.17)	Acc@5 100.00 ( 99.92)
Epoch: [46][350/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6577e-01 (1.6216e-01)	Acc@1  92.97 ( 94.18)	Acc@5 100.00 ( 99.92)
Epoch: [46][360/391]	Time  0.073 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2327e-01 (1.6221e-01)	Acc@1  92.97 ( 94.19)	Acc@5 100.00 ( 99.92)
Epoch: [46][370/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4941e-01 (1.6174e-01)	Acc@1  93.75 ( 94.22)	Acc@5 100.00 ( 99.93)
Epoch: [46][380/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8335e-01 (1.6163e-01)	Acc@1  93.75 ( 94.24)	Acc@5  99.22 ( 99.92)
Epoch: [46][390/391]	Time  0.047 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9946e-01 (1.6154e-01)	Acc@1  93.75 ( 94.24)	Acc@5 100.00 ( 99.93)
## e[46] optimizer.zero_grad (sum) time: 0.38904714584350586
## e[46]       loss.backward (sum) time: 7.19680643081665
## e[46]      optimizer.step (sum) time: 3.491596221923828
## epoch[46] training(only) time: 25.581371307373047
# Switched to evaluate mode...
Test: [  0/100]	Time  0.196 ( 0.196)	Loss 2.8564e-01 (2.8564e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.041)	Loss 4.0381e-01 (3.1962e-01)	Acc@1  91.00 ( 90.45)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.028 ( 0.034)	Loss 3.2593e-01 (3.1644e-01)	Acc@1  88.00 ( 90.14)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 3.3057e-01 (3.1867e-01)	Acc@1  88.00 ( 90.19)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.024 ( 0.030)	Loss 3.8867e-01 (3.2242e-01)	Acc@1  90.00 ( 89.95)	Acc@5  99.00 ( 99.66)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 2.1631e-01 (3.2129e-01)	Acc@1  90.00 ( 89.88)	Acc@5  99.00 ( 99.67)
Test: [ 60/100]	Time  0.025 ( 0.028)	Loss 1.9067e-01 (3.1524e-01)	Acc@1  93.00 ( 90.00)	Acc@5 100.00 ( 99.70)
Test: [ 70/100]	Time  0.028 ( 0.028)	Loss 4.2798e-01 (3.1502e-01)	Acc@1  89.00 ( 90.13)	Acc@5 100.00 ( 99.72)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 2.2192e-01 (3.1452e-01)	Acc@1  92.00 ( 90.17)	Acc@5 100.00 ( 99.70)
Test: [ 90/100]	Time  0.026 ( 0.027)	Loss 2.2693e-01 (3.1162e-01)	Acc@1  91.00 ( 90.25)	Acc@5 100.00 ( 99.71)
 * Acc@1 90.380 Acc@5 99.730
### epoch[46] execution time: 28.430817127227783
EPOCH 47
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [47][  0/391]	Time  0.252 ( 0.252)	Data  0.189 ( 0.189)	Loss 2.0959e-01 (2.0959e-01)	Acc@1  90.62 ( 90.62)	Acc@5 100.00 (100.00)
Epoch: [47][ 10/391]	Time  0.065 ( 0.082)	Data  0.001 ( 0.018)	Loss 2.3193e-01 (1.5188e-01)	Acc@1  92.19 ( 94.46)	Acc@5 100.00 ( 99.86)
Epoch: [47][ 20/391]	Time  0.064 ( 0.074)	Data  0.001 ( 0.010)	Loss 1.1041e-01 (1.4924e-01)	Acc@1  96.88 ( 94.57)	Acc@5 100.00 ( 99.93)
Epoch: [47][ 30/391]	Time  0.062 ( 0.071)	Data  0.001 ( 0.007)	Loss 1.0223e-01 (1.5269e-01)	Acc@1  96.09 ( 94.56)	Acc@5 100.00 ( 99.95)
Epoch: [47][ 40/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.006)	Loss 1.6492e-01 (1.5448e-01)	Acc@1  92.97 ( 94.55)	Acc@5 100.00 ( 99.96)
Epoch: [47][ 50/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.005)	Loss 9.8572e-02 (1.5429e-01)	Acc@1  96.09 ( 94.59)	Acc@5 100.00 ( 99.97)
Epoch: [47][ 60/391]	Time  0.062 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.3086e-01 (1.5666e-01)	Acc@1  95.31 ( 94.56)	Acc@5 100.00 ( 99.96)
Epoch: [47][ 70/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.1041e-01 (1.5316e-01)	Acc@1  96.88 ( 94.74)	Acc@5 100.00 ( 99.97)
Epoch: [47][ 80/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.2830e-01 (1.5592e-01)	Acc@1  96.88 ( 94.67)	Acc@5 100.00 ( 99.96)
Epoch: [47][ 90/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.3318e-01 (1.5366e-01)	Acc@1  95.31 ( 94.75)	Acc@5 100.00 ( 99.96)
Epoch: [47][100/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.2607e-01 (1.5634e-01)	Acc@1  92.97 ( 94.62)	Acc@5  99.22 ( 99.94)
Epoch: [47][110/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.1810e-01 (1.5491e-01)	Acc@1  96.09 ( 94.66)	Acc@5 100.00 ( 99.94)
Epoch: [47][120/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.4209e-01 (1.5568e-01)	Acc@1  96.09 ( 94.65)	Acc@5 100.00 ( 99.94)
Epoch: [47][130/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.1299e-02 (1.5549e-01)	Acc@1  97.66 ( 94.64)	Acc@5 100.00 ( 99.94)
Epoch: [47][140/391]	Time  0.073 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.2595e-01 (1.5703e-01)	Acc@1  92.97 ( 94.60)	Acc@5  99.22 ( 99.93)
Epoch: [47][150/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.1252e-01 (1.5885e-01)	Acc@1  92.19 ( 94.49)	Acc@5 100.00 ( 99.93)
Epoch: [47][160/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.1492e-02 (1.5706e-01)	Acc@1  95.31 ( 94.55)	Acc@5 100.00 ( 99.93)
Epoch: [47][170/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6663e-01 (1.5820e-01)	Acc@1  92.97 ( 94.54)	Acc@5 100.00 ( 99.93)
Epoch: [47][180/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.5754e-02 (1.5709e-01)	Acc@1  98.44 ( 94.57)	Acc@5 100.00 ( 99.92)
Epoch: [47][190/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.2205e-02 (1.5651e-01)	Acc@1  98.44 ( 94.56)	Acc@5 100.00 ( 99.93)
Epoch: [47][200/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4062e-01 (1.5664e-01)	Acc@1  94.53 ( 94.55)	Acc@5 100.00 ( 99.93)
Epoch: [47][210/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.9844e-02 (1.5542e-01)	Acc@1  96.88 ( 94.61)	Acc@5 100.00 ( 99.93)
Epoch: [47][220/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7932e-01 (1.5496e-01)	Acc@1  91.41 ( 94.57)	Acc@5 100.00 ( 99.93)
Epoch: [47][230/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6895e-01 (1.5520e-01)	Acc@1  92.19 ( 94.53)	Acc@5 100.00 ( 99.93)
Epoch: [47][240/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0907e-01 (1.5644e-01)	Acc@1  96.09 ( 94.49)	Acc@5 100.00 ( 99.93)
Epoch: [47][250/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.0789e-01 (1.5640e-01)	Acc@1  91.41 ( 94.46)	Acc@5 100.00 ( 99.93)
Epoch: [47][260/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6418e-01 (1.5620e-01)	Acc@1  96.09 ( 94.48)	Acc@5 100.00 ( 99.93)
Epoch: [47][270/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8884e-01 (1.5696e-01)	Acc@1  92.97 ( 94.48)	Acc@5 100.00 ( 99.93)
Epoch: [47][280/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3525e-01 (1.5733e-01)	Acc@1  92.97 ( 94.47)	Acc@5 100.00 ( 99.92)
Epoch: [47][290/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.5293e-01 (1.5797e-01)	Acc@1  89.84 ( 94.43)	Acc@5 100.00 ( 99.92)
Epoch: [47][300/391]	Time  0.070 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6956e-01 (1.5778e-01)	Acc@1  94.53 ( 94.45)	Acc@5  99.22 ( 99.92)
Epoch: [47][310/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4539e-01 (1.5807e-01)	Acc@1  94.53 ( 94.43)	Acc@5 100.00 ( 99.92)
Epoch: [47][320/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8787e-01 (1.5795e-01)	Acc@1  92.97 ( 94.46)	Acc@5 100.00 ( 99.92)
Epoch: [47][330/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.8511e-02 (1.5875e-01)	Acc@1  94.53 ( 94.41)	Acc@5 100.00 ( 99.92)
Epoch: [47][340/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3208e-01 (1.5832e-01)	Acc@1  96.09 ( 94.44)	Acc@5 100.00 ( 99.92)
Epoch: [47][350/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1716e-01 (1.5910e-01)	Acc@1  92.19 ( 94.41)	Acc@5  99.22 ( 99.91)
Epoch: [47][360/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6174e-01 (1.5893e-01)	Acc@1  92.97 ( 94.41)	Acc@5 100.00 ( 99.92)
Epoch: [47][370/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8701e-01 (1.5849e-01)	Acc@1  92.19 ( 94.43)	Acc@5 100.00 ( 99.92)
Epoch: [47][380/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2006e-01 (1.5812e-01)	Acc@1  96.88 ( 94.46)	Acc@5  99.22 ( 99.91)
Epoch: [47][390/391]	Time  0.047 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2585e-01 (1.5856e-01)	Acc@1  97.50 ( 94.45)	Acc@5 100.00 ( 99.91)
## e[47] optimizer.zero_grad (sum) time: 0.39713573455810547
## e[47]       loss.backward (sum) time: 7.242620468139648
## e[47]      optimizer.step (sum) time: 3.4408481121063232
## epoch[47] training(only) time: 25.547754287719727
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 3.0176e-01 (3.0176e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.039)	Loss 3.8599e-01 (3.1155e-01)	Acc@1  91.00 ( 90.27)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 3.8867e-01 (3.1071e-01)	Acc@1  84.00 ( 90.19)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 3.1079e-01 (3.1168e-01)	Acc@1  86.00 ( 90.10)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.024 ( 0.030)	Loss 3.9941e-01 (3.2238e-01)	Acc@1  88.00 ( 89.76)	Acc@5  99.00 ( 99.63)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 2.2021e-01 (3.2080e-01)	Acc@1  92.00 ( 89.92)	Acc@5  99.00 ( 99.63)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 1.9031e-01 (3.1649e-01)	Acc@1  91.00 ( 89.97)	Acc@5 100.00 ( 99.66)
Test: [ 70/100]	Time  0.027 ( 0.028)	Loss 3.7964e-01 (3.1572e-01)	Acc@1  90.00 ( 90.10)	Acc@5 100.00 ( 99.69)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 1.6101e-01 (3.1496e-01)	Acc@1  94.00 ( 90.11)	Acc@5 100.00 ( 99.68)
Test: [ 90/100]	Time  0.027 ( 0.027)	Loss 2.0801e-01 (3.1270e-01)	Acc@1  90.00 ( 90.10)	Acc@5 100.00 ( 99.68)
 * Acc@1 90.200 Acc@5 99.690
### epoch[47] execution time: 28.354787588119507
EPOCH 48
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [48][  0/391]	Time  0.251 ( 0.251)	Data  0.182 ( 0.182)	Loss 1.6602e-01 (1.6602e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [48][ 10/391]	Time  0.064 ( 0.081)	Data  0.001 ( 0.017)	Loss 2.0020e-01 (1.5252e-01)	Acc@1  91.41 ( 94.25)	Acc@5 100.00 ( 99.93)
Epoch: [48][ 20/391]	Time  0.066 ( 0.073)	Data  0.001 ( 0.010)	Loss 1.1182e-01 (1.4652e-01)	Acc@1  94.53 ( 94.38)	Acc@5 100.00 ( 99.96)
Epoch: [48][ 30/391]	Time  0.066 ( 0.070)	Data  0.001 ( 0.007)	Loss 1.3660e-01 (1.4515e-01)	Acc@1  95.31 ( 94.81)	Acc@5 100.00 ( 99.92)
Epoch: [48][ 40/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.005)	Loss 1.3586e-01 (1.5016e-01)	Acc@1  93.75 ( 94.66)	Acc@5 100.00 ( 99.94)
Epoch: [48][ 50/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.005)	Loss 1.5625e-01 (1.5434e-01)	Acc@1  93.75 ( 94.47)	Acc@5 100.00 ( 99.94)
Epoch: [48][ 60/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.1969e-01 (1.5608e-01)	Acc@1  96.09 ( 94.44)	Acc@5 100.00 ( 99.92)
Epoch: [48][ 70/391]	Time  0.073 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.5576e-01 (1.5469e-01)	Acc@1  95.31 ( 94.51)	Acc@5 100.00 ( 99.93)
Epoch: [48][ 80/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.3159e-01 (1.5555e-01)	Acc@1  95.31 ( 94.45)	Acc@5 100.00 ( 99.92)
Epoch: [48][ 90/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.6504e-01 (1.5783e-01)	Acc@1  93.75 ( 94.34)	Acc@5 100.00 ( 99.93)
Epoch: [48][100/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.3730e-01 (1.5775e-01)	Acc@1  91.41 ( 94.36)	Acc@5 100.00 ( 99.94)
Epoch: [48][110/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.9885e-01 (1.5833e-01)	Acc@1  94.53 ( 94.36)	Acc@5 100.00 ( 99.94)
Epoch: [48][120/391]	Time  0.076 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.6565e-01 (1.5819e-01)	Acc@1  91.41 ( 94.35)	Acc@5 100.00 ( 99.95)
Epoch: [48][130/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6858e-01 (1.6036e-01)	Acc@1  93.75 ( 94.27)	Acc@5 100.00 ( 99.95)
Epoch: [48][140/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1664e-01 (1.6031e-01)	Acc@1  94.53 ( 94.28)	Acc@5 100.00 ( 99.94)
Epoch: [48][150/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.0776e-01 (1.6118e-01)	Acc@1  94.53 ( 94.27)	Acc@5 100.00 ( 99.94)
Epoch: [48][160/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.0532e-01 (1.6086e-01)	Acc@1  93.75 ( 94.27)	Acc@5 100.00 ( 99.95)
Epoch: [48][170/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.2278e-01 (1.6139e-01)	Acc@1  92.19 ( 94.28)	Acc@5 100.00 ( 99.95)
Epoch: [48][180/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1145e-01 (1.6230e-01)	Acc@1  95.31 ( 94.26)	Acc@5 100.00 ( 99.94)
Epoch: [48][190/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.0691e-01 (1.6149e-01)	Acc@1  91.41 ( 94.29)	Acc@5 100.00 ( 99.95)
Epoch: [48][200/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.0801e-01 (1.6209e-01)	Acc@1  92.97 ( 94.29)	Acc@5 100.00 ( 99.95)
Epoch: [48][210/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5039e-01 (1.6023e-01)	Acc@1  93.75 ( 94.35)	Acc@5  99.22 ( 99.95)
Epoch: [48][220/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3110e-01 (1.6002e-01)	Acc@1  96.09 ( 94.35)	Acc@5 100.00 ( 99.95)
Epoch: [48][230/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.5488e-01 (1.5954e-01)	Acc@1  91.41 ( 94.36)	Acc@5  99.22 ( 99.94)
Epoch: [48][240/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6431e-01 (1.5928e-01)	Acc@1  94.53 ( 94.38)	Acc@5 100.00 ( 99.94)
Epoch: [48][250/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4905e-01 (1.5845e-01)	Acc@1  94.53 ( 94.43)	Acc@5 100.00 ( 99.94)
Epoch: [48][260/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9031e-01 (1.5855e-01)	Acc@1  94.53 ( 94.41)	Acc@5  99.22 ( 99.94)
Epoch: [48][270/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7139e-01 (1.5823e-01)	Acc@1  92.97 ( 94.38)	Acc@5 100.00 ( 99.94)
Epoch: [48][280/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2314e-01 (1.5862e-01)	Acc@1  89.84 ( 94.36)	Acc@5 100.00 ( 99.94)
Epoch: [48][290/391]	Time  0.070 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6882e-01 (1.5841e-01)	Acc@1  92.97 ( 94.36)	Acc@5 100.00 ( 99.94)
Epoch: [48][300/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.0210e-02 (1.5870e-01)	Acc@1  96.88 ( 94.33)	Acc@5 100.00 ( 99.94)
Epoch: [48][310/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8286e-01 (1.5797e-01)	Acc@1  95.31 ( 94.36)	Acc@5 100.00 ( 99.94)
Epoch: [48][320/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.5610e-01 (1.5743e-01)	Acc@1  92.19 ( 94.39)	Acc@5  99.22 ( 99.94)
Epoch: [48][330/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2913e-01 (1.5762e-01)	Acc@1  91.41 ( 94.41)	Acc@5 100.00 ( 99.94)
Epoch: [48][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3452e-01 (1.5690e-01)	Acc@1  93.75 ( 94.42)	Acc@5 100.00 ( 99.94)
Epoch: [48][350/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3440e-01 (1.5709e-01)	Acc@1  94.53 ( 94.41)	Acc@5 100.00 ( 99.94)
Epoch: [48][360/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9507e-01 (1.5786e-01)	Acc@1  92.97 ( 94.38)	Acc@5  99.22 ( 99.93)
Epoch: [48][370/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2042e-01 (1.5783e-01)	Acc@1  94.53 ( 94.38)	Acc@5 100.00 ( 99.93)
Epoch: [48][380/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5796e-01 (1.5759e-01)	Acc@1  95.31 ( 94.39)	Acc@5  99.22 ( 99.93)
Epoch: [48][390/391]	Time  0.048 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8286e-01 (1.5823e-01)	Acc@1  93.75 ( 94.38)	Acc@5 100.00 ( 99.93)
## e[48] optimizer.zero_grad (sum) time: 0.3903076648712158
## e[48]       loss.backward (sum) time: 7.240722179412842
## e[48]      optimizer.step (sum) time: 3.426799774169922
## epoch[48] training(only) time: 25.50370764732361
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 3.1982e-01 (3.1982e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.040)	Loss 4.2090e-01 (3.1139e-01)	Acc@1  88.00 ( 90.09)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.027 ( 0.032)	Loss 3.4521e-01 (3.1423e-01)	Acc@1  88.00 ( 90.05)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.034 ( 0.030)	Loss 3.2959e-01 (3.2229e-01)	Acc@1  86.00 ( 89.90)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 3.7769e-01 (3.2251e-01)	Acc@1  87.00 ( 89.80)	Acc@5  99.00 ( 99.68)
Test: [ 50/100]	Time  0.027 ( 0.028)	Loss 2.2583e-01 (3.2228e-01)	Acc@1  93.00 ( 89.88)	Acc@5 100.00 ( 99.69)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 2.2949e-01 (3.1614e-01)	Acc@1  91.00 ( 90.03)	Acc@5 100.00 ( 99.70)
Test: [ 70/100]	Time  0.024 ( 0.027)	Loss 4.2773e-01 (3.1520e-01)	Acc@1  89.00 ( 90.15)	Acc@5 100.00 ( 99.72)
Test: [ 80/100]	Time  0.026 ( 0.027)	Loss 1.6565e-01 (3.1519e-01)	Acc@1  95.00 ( 90.20)	Acc@5 100.00 ( 99.72)
Test: [ 90/100]	Time  0.027 ( 0.027)	Loss 2.0667e-01 (3.1335e-01)	Acc@1  90.00 ( 90.22)	Acc@5 100.00 ( 99.71)
 * Acc@1 90.280 Acc@5 99.710
### epoch[48] execution time: 28.26324963569641
EPOCH 49
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [49][  0/391]	Time  0.253 ( 0.253)	Data  0.183 ( 0.183)	Loss 7.3120e-02 (7.3120e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [49][ 10/391]	Time  0.062 ( 0.081)	Data  0.001 ( 0.017)	Loss 1.0785e-01 (1.3598e-01)	Acc@1  96.09 ( 94.89)	Acc@5 100.00 ( 99.86)
Epoch: [49][ 20/391]	Time  0.064 ( 0.074)	Data  0.001 ( 0.010)	Loss 1.3684e-01 (1.3350e-01)	Acc@1  94.53 ( 95.01)	Acc@5 100.00 ( 99.93)
Epoch: [49][ 30/391]	Time  0.064 ( 0.071)	Data  0.001 ( 0.007)	Loss 1.5051e-01 (1.4113e-01)	Acc@1  95.31 ( 94.61)	Acc@5 100.00 ( 99.95)
Epoch: [49][ 40/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.006)	Loss 9.8816e-02 (1.4570e-01)	Acc@1  96.88 ( 94.57)	Acc@5 100.00 ( 99.94)
Epoch: [49][ 50/391]	Time  0.063 ( 0.069)	Data  0.001 ( 0.005)	Loss 1.2683e-01 (1.4944e-01)	Acc@1  96.09 ( 94.50)	Acc@5 100.00 ( 99.95)
Epoch: [49][ 60/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.7102e-01 (1.4943e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 ( 99.96)
Epoch: [49][ 70/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.004)	Loss 7.2083e-02 (1.4576e-01)	Acc@1  98.44 ( 94.72)	Acc@5 100.00 ( 99.97)
Epoch: [49][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.8616e-01 (1.4996e-01)	Acc@1  92.19 ( 94.56)	Acc@5 100.00 ( 99.95)
Epoch: [49][ 90/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.3108e-01 (1.5362e-01)	Acc@1  92.97 ( 94.51)	Acc@5 100.00 ( 99.94)
Epoch: [49][100/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.4392e-01 (1.5232e-01)	Acc@1  95.31 ( 94.55)	Acc@5 100.00 ( 99.92)
Epoch: [49][110/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.5881e-01 (1.5399e-01)	Acc@1  93.75 ( 94.45)	Acc@5 100.00 ( 99.92)
Epoch: [49][120/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.5967e-01 (1.5312e-01)	Acc@1  96.09 ( 94.55)	Acc@5 100.00 ( 99.92)
Epoch: [49][130/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7627e-01 (1.5428e-01)	Acc@1  94.53 ( 94.52)	Acc@5 100.00 ( 99.92)
Epoch: [49][140/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.0642e-01 (1.5506e-01)	Acc@1  92.19 ( 94.53)	Acc@5 100.00 ( 99.91)
Epoch: [49][150/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2964e-01 (1.5512e-01)	Acc@1  96.09 ( 94.56)	Acc@5  99.22 ( 99.91)
Epoch: [49][160/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5088e-01 (1.5482e-01)	Acc@1  94.53 ( 94.54)	Acc@5 100.00 ( 99.91)
Epoch: [49][170/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.8467e-01 (1.5559e-01)	Acc@1  86.72 ( 94.50)	Acc@5 100.00 ( 99.92)
Epoch: [49][180/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.3171e-02 (1.5694e-01)	Acc@1  97.66 ( 94.46)	Acc@5 100.00 ( 99.92)
Epoch: [49][190/391]	Time  0.070 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1523e-01 (1.5680e-01)	Acc@1  96.88 ( 94.45)	Acc@5 100.00 ( 99.93)
Epoch: [49][200/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6125e-01 (1.5670e-01)	Acc@1  96.09 ( 94.47)	Acc@5 100.00 ( 99.93)
Epoch: [49][210/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.8379e-02 (1.5574e-01)	Acc@1  97.66 ( 94.51)	Acc@5 100.00 ( 99.93)
Epoch: [49][220/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5259e-01 (1.5506e-01)	Acc@1  93.75 ( 94.53)	Acc@5 100.00 ( 99.94)
Epoch: [49][230/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4062e-01 (1.5541e-01)	Acc@1  95.31 ( 94.53)	Acc@5 100.00 ( 99.94)
Epoch: [49][240/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4771e-01 (1.5540e-01)	Acc@1  94.53 ( 94.52)	Acc@5 100.00 ( 99.93)
Epoch: [49][250/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9897e-01 (1.5618e-01)	Acc@1  90.62 ( 94.50)	Acc@5 100.00 ( 99.93)
Epoch: [49][260/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7456e-01 (1.5584e-01)	Acc@1  93.75 ( 94.49)	Acc@5 100.00 ( 99.93)
Epoch: [49][270/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0278e-01 (1.5568e-01)	Acc@1  96.88 ( 94.50)	Acc@5 100.00 ( 99.93)
Epoch: [49][280/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6370e-01 (1.5595e-01)	Acc@1  93.75 ( 94.51)	Acc@5 100.00 ( 99.94)
Epoch: [49][290/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7688e-01 (1.5573e-01)	Acc@1  93.75 ( 94.49)	Acc@5  99.22 ( 99.94)
Epoch: [49][300/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2122e-01 (1.5507e-01)	Acc@1  95.31 ( 94.52)	Acc@5 100.00 ( 99.94)
Epoch: [49][310/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.2397e-02 (1.5553e-01)	Acc@1  97.66 ( 94.50)	Acc@5 100.00 ( 99.93)
Epoch: [49][320/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7542e-01 (1.5510e-01)	Acc@1  93.75 ( 94.50)	Acc@5  99.22 ( 99.93)
Epoch: [49][330/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1450e-01 (1.5602e-01)	Acc@1  96.88 ( 94.48)	Acc@5 100.00 ( 99.93)
Epoch: [49][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4221e-01 (1.5641e-01)	Acc@1  95.31 ( 94.48)	Acc@5 100.00 ( 99.94)
Epoch: [49][350/391]	Time  0.058 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4734e-01 (1.5679e-01)	Acc@1  92.97 ( 94.46)	Acc@5 100.00 ( 99.94)
Epoch: [49][360/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6052e-01 (1.5706e-01)	Acc@1  94.53 ( 94.44)	Acc@5 100.00 ( 99.94)
Epoch: [49][370/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5649e-01 (1.5706e-01)	Acc@1  92.97 ( 94.42)	Acc@5 100.00 ( 99.93)
Epoch: [49][380/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0730e-01 (1.5687e-01)	Acc@1  96.09 ( 94.43)	Acc@5 100.00 ( 99.93)
Epoch: [49][390/391]	Time  0.056 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.9834e-02 (1.5688e-01)	Acc@1  98.75 ( 94.42)	Acc@5 100.00 ( 99.93)
## e[49] optimizer.zero_grad (sum) time: 0.3891334533691406
## e[49]       loss.backward (sum) time: 7.231419563293457
## e[49]      optimizer.step (sum) time: 3.437854528427124
## epoch[49] training(only) time: 25.58669924736023
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 2.9834e-01 (2.9834e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.039)	Loss 3.9990e-01 (3.0763e-01)	Acc@1  89.00 ( 90.18)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.024 ( 0.032)	Loss 3.6499e-01 (3.0345e-01)	Acc@1  87.00 ( 90.33)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.024 ( 0.030)	Loss 2.9736e-01 (3.1293e-01)	Acc@1  88.00 ( 90.29)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 3.6841e-01 (3.1476e-01)	Acc@1  88.00 ( 90.12)	Acc@5  99.00 ( 99.68)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 2.0740e-01 (3.1415e-01)	Acc@1  94.00 ( 90.08)	Acc@5  99.00 ( 99.69)
Test: [ 60/100]	Time  0.024 ( 0.027)	Loss 1.8945e-01 (3.0809e-01)	Acc@1  92.00 ( 90.15)	Acc@5 100.00 ( 99.72)
Test: [ 70/100]	Time  0.024 ( 0.027)	Loss 4.5361e-01 (3.0900e-01)	Acc@1  89.00 ( 90.17)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 1.6138e-01 (3.1047e-01)	Acc@1  92.00 ( 90.14)	Acc@5 100.00 ( 99.73)
Test: [ 90/100]	Time  0.027 ( 0.027)	Loss 2.8296e-01 (3.0950e-01)	Acc@1  89.00 ( 90.18)	Acc@5 100.00 ( 99.74)
 * Acc@1 90.210 Acc@5 99.740
### epoch[49] execution time: 28.389636278152466
EPOCH 50
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [50][  0/391]	Time  0.251 ( 0.251)	Data  0.185 ( 0.185)	Loss 8.1909e-02 (8.1909e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [50][ 10/391]	Time  0.063 ( 0.081)	Data  0.001 ( 0.018)	Loss 1.4490e-01 (1.3273e-01)	Acc@1  94.53 ( 95.88)	Acc@5 100.00 (100.00)
Epoch: [50][ 20/391]	Time  0.065 ( 0.074)	Data  0.001 ( 0.010)	Loss 9.6802e-02 (1.3883e-01)	Acc@1  97.66 ( 95.54)	Acc@5 100.00 ( 99.96)
Epoch: [50][ 30/391]	Time  0.063 ( 0.071)	Data  0.001 ( 0.007)	Loss 1.4062e-01 (1.4109e-01)	Acc@1  94.53 ( 95.26)	Acc@5 100.00 ( 99.92)
Epoch: [50][ 40/391]	Time  0.065 ( 0.070)	Data  0.001 ( 0.006)	Loss 1.4758e-01 (1.4275e-01)	Acc@1  96.09 ( 95.16)	Acc@5 100.00 ( 99.94)
Epoch: [50][ 50/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.005)	Loss 1.1548e-01 (1.4113e-01)	Acc@1  95.31 ( 95.22)	Acc@5 100.00 ( 99.95)
Epoch: [50][ 60/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.8127e-01 (1.4157e-01)	Acc@1  93.75 ( 95.30)	Acc@5 100.00 ( 99.96)
Epoch: [50][ 70/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.1206e-01 (1.3993e-01)	Acc@1  96.09 ( 95.28)	Acc@5 100.00 ( 99.97)
Epoch: [50][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.6614e-01 (1.4306e-01)	Acc@1  94.53 ( 95.09)	Acc@5 100.00 ( 99.97)
Epoch: [50][ 90/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.2046e-01 (1.4359e-01)	Acc@1  91.41 ( 95.09)	Acc@5 100.00 ( 99.97)
Epoch: [50][100/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.4453e-01 (1.4568e-01)	Acc@1  93.75 ( 94.96)	Acc@5 100.00 ( 99.96)
Epoch: [50][110/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.2134e-01 (1.4619e-01)	Acc@1  95.31 ( 94.93)	Acc@5 100.00 ( 99.96)
Epoch: [50][120/391]	Time  0.059 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.5513e-01 (1.4833e-01)	Acc@1  92.97 ( 94.85)	Acc@5 100.00 ( 99.97)
Epoch: [50][130/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6260e-01 (1.4854e-01)	Acc@1  94.53 ( 94.83)	Acc@5  99.22 ( 99.96)
Epoch: [50][140/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6345e-01 (1.4905e-01)	Acc@1  94.53 ( 94.80)	Acc@5 100.00 ( 99.96)
Epoch: [50][150/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.7168e-02 (1.5010e-01)	Acc@1  96.88 ( 94.72)	Acc@5 100.00 ( 99.96)
Epoch: [50][160/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.4817e-01 (1.5057e-01)	Acc@1  90.62 ( 94.72)	Acc@5 100.00 ( 99.96)
Epoch: [50][170/391]	Time  0.060 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3367e-01 (1.5248e-01)	Acc@1  96.09 ( 94.65)	Acc@5 100.00 ( 99.95)
Epoch: [50][180/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6455e-01 (1.5252e-01)	Acc@1  92.97 ( 94.63)	Acc@5 100.00 ( 99.95)
Epoch: [50][190/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4368e-01 (1.5239e-01)	Acc@1  93.75 ( 94.66)	Acc@5 100.00 ( 99.95)
Epoch: [50][200/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8091e-01 (1.5379e-01)	Acc@1  93.75 ( 94.68)	Acc@5 100.00 ( 99.95)
Epoch: [50][210/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5833e-01 (1.5378e-01)	Acc@1  93.75 ( 94.63)	Acc@5 100.00 ( 99.95)
Epoch: [50][220/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1401e-01 (1.5418e-01)	Acc@1  95.31 ( 94.62)	Acc@5 100.00 ( 99.95)
Epoch: [50][230/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3586e-01 (1.5392e-01)	Acc@1  93.75 ( 94.61)	Acc@5 100.00 ( 99.95)
Epoch: [50][240/391]	Time  0.071 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4404e-01 (1.5399e-01)	Acc@1  95.31 ( 94.60)	Acc@5 100.00 ( 99.94)
Epoch: [50][250/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.5520e-02 (1.5382e-01)	Acc@1  97.66 ( 94.58)	Acc@5 100.00 ( 99.94)
Epoch: [50][260/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3440e-01 (1.5393e-01)	Acc@1  96.09 ( 94.56)	Acc@5  99.22 ( 99.94)
Epoch: [50][270/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6992e-01 (1.5369e-01)	Acc@1  92.19 ( 94.55)	Acc@5  99.22 ( 99.94)
Epoch: [50][280/391]	Time  0.071 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6638e-01 (1.5296e-01)	Acc@1  91.41 ( 94.57)	Acc@5 100.00 ( 99.94)
Epoch: [50][290/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.9956e-02 (1.5282e-01)	Acc@1  96.88 ( 94.59)	Acc@5 100.00 ( 99.94)
Epoch: [50][300/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4331e-01 (1.5276e-01)	Acc@1  94.53 ( 94.59)	Acc@5 100.00 ( 99.94)
Epoch: [50][310/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.1228e-01 (1.5331e-01)	Acc@1  96.09 ( 94.59)	Acc@5  99.22 ( 99.94)
Epoch: [50][320/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5991e-01 (1.5368e-01)	Acc@1  92.19 ( 94.56)	Acc@5 100.00 ( 99.94)
Epoch: [50][330/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8677e-01 (1.5458e-01)	Acc@1  93.75 ( 94.53)	Acc@5 100.00 ( 99.94)
Epoch: [50][340/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.4473e-02 (1.5503e-01)	Acc@1  96.88 ( 94.51)	Acc@5 100.00 ( 99.94)
Epoch: [50][350/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.4778e-02 (1.5450e-01)	Acc@1  98.44 ( 94.54)	Acc@5 100.00 ( 99.94)
Epoch: [50][360/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3330e-01 (1.5468e-01)	Acc@1  96.09 ( 94.54)	Acc@5 100.00 ( 99.94)
Epoch: [50][370/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4783e-01 (1.5485e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 ( 99.94)
Epoch: [50][380/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2268e-01 (1.5494e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 ( 99.94)
Epoch: [50][390/391]	Time  0.056 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8433e-01 (1.5439e-01)	Acc@1  92.50 ( 94.55)	Acc@5 100.00 ( 99.94)
## e[50] optimizer.zero_grad (sum) time: 0.3904397487640381
## e[50]       loss.backward (sum) time: 7.25870680809021
## e[50]      optimizer.step (sum) time: 3.5061161518096924
## epoch[50] training(only) time: 25.645814180374146
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 3.0615e-01 (3.0615e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.040)	Loss 4.5581e-01 (3.2602e-01)	Acc@1  89.00 ( 89.55)	Acc@5  99.00 ( 99.73)
Test: [ 20/100]	Time  0.027 ( 0.033)	Loss 3.9355e-01 (3.1564e-01)	Acc@1  87.00 ( 89.71)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 3.4253e-01 (3.2006e-01)	Acc@1  87.00 ( 89.87)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.025 ( 0.029)	Loss 3.1836e-01 (3.2148e-01)	Acc@1  90.00 ( 89.85)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 2.1692e-01 (3.2086e-01)	Acc@1  92.00 ( 89.71)	Acc@5  99.00 ( 99.67)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 1.9885e-01 (3.1694e-01)	Acc@1  93.00 ( 89.84)	Acc@5 100.00 ( 99.70)
Test: [ 70/100]	Time  0.024 ( 0.027)	Loss 4.5972e-01 (3.1672e-01)	Acc@1  88.00 ( 89.82)	Acc@5 100.00 ( 99.72)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 1.4990e-01 (3.1802e-01)	Acc@1  94.00 ( 89.80)	Acc@5 100.00 ( 99.72)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 2.2400e-01 (3.1737e-01)	Acc@1  90.00 ( 89.84)	Acc@5 100.00 ( 99.73)
 * Acc@1 89.950 Acc@5 99.730
### epoch[50] execution time: 28.42383909225464
EPOCH 51
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [51][  0/391]	Time  0.248 ( 0.248)	Data  0.175 ( 0.175)	Loss 1.8384e-01 (1.8384e-01)	Acc@1  92.97 ( 92.97)	Acc@5 100.00 (100.00)
Epoch: [51][ 10/391]	Time  0.063 ( 0.081)	Data  0.001 ( 0.017)	Loss 1.1761e-01 (1.3437e-01)	Acc@1  93.75 ( 94.74)	Acc@5 100.00 (100.00)
Epoch: [51][ 20/391]	Time  0.062 ( 0.073)	Data  0.001 ( 0.009)	Loss 1.6736e-01 (1.4292e-01)	Acc@1  93.75 ( 94.49)	Acc@5 100.00 (100.00)
Epoch: [51][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.007)	Loss 1.8152e-01 (1.4921e-01)	Acc@1  94.53 ( 94.41)	Acc@5 100.00 ( 99.97)
Epoch: [51][ 40/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.005)	Loss 1.9873e-01 (1.4731e-01)	Acc@1  89.84 ( 94.55)	Acc@5 100.00 ( 99.96)
Epoch: [51][ 50/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.5234e-01 (1.4737e-01)	Acc@1  93.75 ( 94.68)	Acc@5 100.00 ( 99.95)
Epoch: [51][ 60/391]	Time  0.062 ( 0.068)	Data  0.001 ( 0.004)	Loss 2.8882e-01 (1.4857e-01)	Acc@1  92.97 ( 94.68)	Acc@5 100.00 ( 99.96)
Epoch: [51][ 70/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.9568e-01 (1.4933e-01)	Acc@1  92.97 ( 94.63)	Acc@5  99.22 ( 99.94)
Epoch: [51][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.0931e-01 (1.5151e-01)	Acc@1  95.31 ( 94.52)	Acc@5 100.00 ( 99.95)
Epoch: [51][ 90/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.2152e-01 (1.5254e-01)	Acc@1  95.31 ( 94.47)	Acc@5 100.00 ( 99.96)
Epoch: [51][100/391]	Time  0.072 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.7566e-01 (1.5150e-01)	Acc@1  93.75 ( 94.54)	Acc@5 100.00 ( 99.96)
Epoch: [51][110/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.0630e-01 (1.5177e-01)	Acc@1  93.75 ( 94.52)	Acc@5 100.00 ( 99.96)
Epoch: [51][120/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3562e-01 (1.5005e-01)	Acc@1  94.53 ( 94.57)	Acc@5 100.00 ( 99.97)
Epoch: [51][130/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3794e-01 (1.5092e-01)	Acc@1  94.53 ( 94.51)	Acc@5 100.00 ( 99.97)
Epoch: [51][140/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5759e-01 (1.5079e-01)	Acc@1  93.75 ( 94.51)	Acc@5 100.00 ( 99.97)
Epoch: [51][150/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.5098e-01 (1.4969e-01)	Acc@1  92.19 ( 94.59)	Acc@5 100.00 ( 99.97)
Epoch: [51][160/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4526e-01 (1.5041e-01)	Acc@1  94.53 ( 94.54)	Acc@5 100.00 ( 99.98)
Epoch: [51][170/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.5430e-02 (1.5057e-01)	Acc@1  98.44 ( 94.53)	Acc@5 100.00 ( 99.98)
Epoch: [51][180/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1035e-01 (1.5047e-01)	Acc@1  96.09 ( 94.53)	Acc@5 100.00 ( 99.98)
Epoch: [51][190/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5076e-01 (1.5048e-01)	Acc@1  96.09 ( 94.55)	Acc@5 100.00 ( 99.98)
Epoch: [51][200/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.7534e-02 (1.4971e-01)	Acc@1  96.88 ( 94.61)	Acc@5 100.00 ( 99.97)
Epoch: [51][210/391]	Time  0.078 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2152e-01 (1.5079e-01)	Acc@1  95.31 ( 94.55)	Acc@5 100.00 ( 99.97)
Epoch: [51][220/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.9609e-02 (1.4978e-01)	Acc@1  96.88 ( 94.56)	Acc@5 100.00 ( 99.97)
Epoch: [51][230/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2671e-01 (1.4934e-01)	Acc@1  94.53 ( 94.56)	Acc@5 100.00 ( 99.97)
Epoch: [51][240/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5173e-01 (1.4932e-01)	Acc@1  93.75 ( 94.57)	Acc@5 100.00 ( 99.97)
Epoch: [51][250/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0706e-01 (1.4867e-01)	Acc@1  96.88 ( 94.60)	Acc@5 100.00 ( 99.97)
Epoch: [51][260/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.8440e-02 (1.4836e-01)	Acc@1  97.66 ( 94.61)	Acc@5 100.00 ( 99.97)
Epoch: [51][270/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3184e-01 (1.4879e-01)	Acc@1  93.75 ( 94.58)	Acc@5 100.00 ( 99.97)
Epoch: [51][280/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1688e-01 (1.4872e-01)	Acc@1  96.09 ( 94.58)	Acc@5 100.00 ( 99.97)
Epoch: [51][290/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4648e-01 (1.4945e-01)	Acc@1  94.53 ( 94.58)	Acc@5 100.00 ( 99.97)
Epoch: [51][300/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3306e-01 (1.4949e-01)	Acc@1  95.31 ( 94.58)	Acc@5 100.00 ( 99.97)
Epoch: [51][310/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7310e-01 (1.4939e-01)	Acc@1  95.31 ( 94.58)	Acc@5 100.00 ( 99.97)
Epoch: [51][320/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5845e-01 (1.4971e-01)	Acc@1  92.97 ( 94.57)	Acc@5 100.00 ( 99.97)
Epoch: [51][330/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3074e-01 (1.5010e-01)	Acc@1  94.53 ( 94.54)	Acc@5 100.00 ( 99.97)
Epoch: [51][340/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.3740e-01 (1.5036e-01)	Acc@1  90.62 ( 94.55)	Acc@5 100.00 ( 99.97)
Epoch: [51][350/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8335e-01 (1.5058e-01)	Acc@1  93.75 ( 94.54)	Acc@5 100.00 ( 99.97)
Epoch: [51][360/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2537e-01 (1.5017e-01)	Acc@1  96.09 ( 94.56)	Acc@5 100.00 ( 99.97)
Epoch: [51][370/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2937e-01 (1.5071e-01)	Acc@1  91.41 ( 94.56)	Acc@5 100.00 ( 99.97)
Epoch: [51][380/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.001)	Loss 1.3635e-01 (1.5027e-01)	Acc@1  96.88 ( 94.59)	Acc@5 100.00 ( 99.97)
Epoch: [51][390/391]	Time  0.048 ( 0.065)	Data  0.001 ( 0.001)	Loss 1.8372e-01 (1.5022e-01)	Acc@1  95.00 ( 94.60)	Acc@5 100.00 ( 99.96)
## e[51] optimizer.zero_grad (sum) time: 0.38904786109924316
## e[51]       loss.backward (sum) time: 7.262789487838745
## e[51]      optimizer.step (sum) time: 3.4223687648773193
## epoch[51] training(only) time: 25.579111337661743
# Switched to evaluate mode...
Test: [  0/100]	Time  0.193 ( 0.193)	Loss 2.5635e-01 (2.5635e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.040)	Loss 4.3042e-01 (3.3243e-01)	Acc@1  87.00 ( 89.18)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 3.7915e-01 (3.2060e-01)	Acc@1  85.00 ( 89.43)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 3.2446e-01 (3.2018e-01)	Acc@1  89.00 ( 89.94)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.026 ( 0.029)	Loss 3.4839e-01 (3.2660e-01)	Acc@1  89.00 ( 89.76)	Acc@5 100.00 ( 99.71)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 2.3718e-01 (3.2278e-01)	Acc@1  91.00 ( 89.78)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 1.8225e-01 (3.1675e-01)	Acc@1  93.00 ( 89.95)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.024 ( 0.027)	Loss 4.3579e-01 (3.1576e-01)	Acc@1  89.00 ( 90.10)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 1.5918e-01 (3.1575e-01)	Acc@1  93.00 ( 90.16)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 2.3535e-01 (3.1578e-01)	Acc@1  89.00 ( 90.08)	Acc@5 100.00 ( 99.77)
 * Acc@1 90.130 Acc@5 99.780
### epoch[51] execution time: 28.402649879455566
EPOCH 52
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [52][  0/391]	Time  0.240 ( 0.240)	Data  0.173 ( 0.173)	Loss 2.4243e-01 (2.4243e-01)	Acc@1  92.97 ( 92.97)	Acc@5 100.00 (100.00)
Epoch: [52][ 10/391]	Time  0.067 ( 0.081)	Data  0.001 ( 0.017)	Loss 1.5344e-01 (1.4934e-01)	Acc@1  92.19 ( 95.03)	Acc@5 100.00 ( 99.86)
Epoch: [52][ 20/391]	Time  0.065 ( 0.073)	Data  0.001 ( 0.009)	Loss 1.5271e-01 (1.4913e-01)	Acc@1  92.19 ( 94.83)	Acc@5 100.00 ( 99.89)
Epoch: [52][ 30/391]	Time  0.062 ( 0.071)	Data  0.001 ( 0.007)	Loss 9.8206e-02 (1.4882e-01)	Acc@1  96.88 ( 94.83)	Acc@5 100.00 ( 99.90)
Epoch: [52][ 40/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.005)	Loss 1.2512e-01 (1.5191e-01)	Acc@1  95.31 ( 94.65)	Acc@5 100.00 ( 99.89)
Epoch: [52][ 50/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.3318e-01 (1.5526e-01)	Acc@1  95.31 ( 94.45)	Acc@5 100.00 ( 99.89)
Epoch: [52][ 60/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.3501e-01 (1.5106e-01)	Acc@1  96.09 ( 94.62)	Acc@5 100.00 ( 99.91)
Epoch: [52][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.3892e-01 (1.5279e-01)	Acc@1  95.31 ( 94.62)	Acc@5 100.00 ( 99.91)
Epoch: [52][ 80/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.003)	Loss 8.9355e-02 (1.5009e-01)	Acc@1  95.31 ( 94.68)	Acc@5 100.00 ( 99.92)
Epoch: [52][ 90/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 9.1614e-02 (1.4961e-01)	Acc@1  96.88 ( 94.74)	Acc@5 100.00 ( 99.93)
Epoch: [52][100/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.0992e-01 (1.4870e-01)	Acc@1  96.09 ( 94.82)	Acc@5 100.00 ( 99.91)
Epoch: [52][110/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.3135e-01 (1.4638e-01)	Acc@1  95.31 ( 94.93)	Acc@5 100.00 ( 99.92)
Epoch: [52][120/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5588e-01 (1.4561e-01)	Acc@1  94.53 ( 94.88)	Acc@5 100.00 ( 99.92)
Epoch: [52][130/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3354e-01 (1.4395e-01)	Acc@1  95.31 ( 94.93)	Acc@5 100.00 ( 99.93)
Epoch: [52][140/391]	Time  0.072 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0333e-01 (1.4367e-01)	Acc@1  96.09 ( 94.92)	Acc@5 100.00 ( 99.93)
Epoch: [52][150/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.4666e-02 (1.4193e-01)	Acc@1  97.66 ( 94.95)	Acc@5 100.00 ( 99.94)
Epoch: [52][160/391]	Time  0.060 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5137e-01 (1.4228e-01)	Acc@1  94.53 ( 94.95)	Acc@5 100.00 ( 99.94)
Epoch: [52][170/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.9905e-02 (1.4165e-01)	Acc@1  96.88 ( 94.99)	Acc@5 100.00 ( 99.94)
Epoch: [52][180/391]	Time  0.059 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0272e-01 (1.4097e-01)	Acc@1  96.88 ( 95.04)	Acc@5 100.00 ( 99.94)
Epoch: [52][190/391]	Time  0.059 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5991e-01 (1.4170e-01)	Acc@1  94.53 ( 95.01)	Acc@5 100.00 ( 99.94)
Epoch: [52][200/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2012e-01 (1.4161e-01)	Acc@1  96.09 ( 95.02)	Acc@5 100.00 ( 99.95)
Epoch: [52][210/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8591e-01 (1.4090e-01)	Acc@1  94.53 ( 95.07)	Acc@5 100.00 ( 99.95)
Epoch: [52][220/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2036e-01 (1.4069e-01)	Acc@1  96.09 ( 95.04)	Acc@5 100.00 ( 99.95)
Epoch: [52][230/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2408e-01 (1.4112e-01)	Acc@1  96.09 ( 95.01)	Acc@5 100.00 ( 99.95)
Epoch: [52][240/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2817e-01 (1.4165e-01)	Acc@1  94.53 ( 94.98)	Acc@5 100.00 ( 99.95)
Epoch: [52][250/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5259e-01 (1.4172e-01)	Acc@1  91.41 ( 94.95)	Acc@5 100.00 ( 99.95)
Epoch: [52][260/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0016e-01 (1.4251e-01)	Acc@1  94.53 ( 94.91)	Acc@5 100.00 ( 99.95)
Epoch: [52][270/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9397e-01 (1.4219e-01)	Acc@1  95.31 ( 94.93)	Acc@5 100.00 ( 99.95)
Epoch: [52][280/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5527e-01 (1.4188e-01)	Acc@1  96.88 ( 94.95)	Acc@5 100.00 ( 99.95)
Epoch: [52][290/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2244e-01 (1.4170e-01)	Acc@1  95.31 ( 94.96)	Acc@5 100.00 ( 99.95)
Epoch: [52][300/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6345e-01 (1.4233e-01)	Acc@1  95.31 ( 94.93)	Acc@5 100.00 ( 99.95)
Epoch: [52][310/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.0261e-02 (1.4265e-01)	Acc@1  96.88 ( 94.92)	Acc@5 100.00 ( 99.94)
Epoch: [52][320/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6431e-01 (1.4271e-01)	Acc@1  94.53 ( 94.93)	Acc@5 100.00 ( 99.95)
Epoch: [52][330/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0181e-01 (1.4282e-01)	Acc@1  96.88 ( 94.94)	Acc@5 100.00 ( 99.95)
Epoch: [52][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1884e-01 (1.4187e-01)	Acc@1  96.09 ( 94.99)	Acc@5 100.00 ( 99.95)
Epoch: [52][350/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5796e-01 (1.4217e-01)	Acc@1  95.31 ( 94.97)	Acc@5 100.00 ( 99.95)
Epoch: [52][360/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2817e-01 (1.4270e-01)	Acc@1  95.31 ( 94.94)	Acc@5 100.00 ( 99.95)
Epoch: [52][370/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3684e-01 (1.4276e-01)	Acc@1  96.09 ( 94.94)	Acc@5 100.00 ( 99.95)
Epoch: [52][380/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3977e-01 (1.4320e-01)	Acc@1  95.31 ( 94.91)	Acc@5 100.00 ( 99.95)
Epoch: [52][390/391]	Time  0.049 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0825e-01 (1.4375e-01)	Acc@1  95.00 ( 94.91)	Acc@5 100.00 ( 99.94)
## e[52] optimizer.zero_grad (sum) time: 0.39140868186950684
## e[52]       loss.backward (sum) time: 7.229323863983154
## e[52]      optimizer.step (sum) time: 3.423537254333496
## epoch[52] training(only) time: 25.541152715682983
# Switched to evaluate mode...
Test: [  0/100]	Time  0.191 ( 0.191)	Loss 2.9712e-01 (2.9712e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.040)	Loss 4.0918e-01 (3.1831e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 3.3936e-01 (3.1442e-01)	Acc@1  88.00 ( 89.90)	Acc@5 100.00 ( 99.48)
Test: [ 30/100]	Time  0.024 ( 0.030)	Loss 2.8589e-01 (3.2094e-01)	Acc@1  90.00 ( 89.90)	Acc@5 100.00 ( 99.52)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 3.6743e-01 (3.2526e-01)	Acc@1  88.00 ( 89.76)	Acc@5  99.00 ( 99.49)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 2.1326e-01 (3.2374e-01)	Acc@1  93.00 ( 89.80)	Acc@5 100.00 ( 99.53)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 1.5576e-01 (3.1765e-01)	Acc@1  93.00 ( 90.07)	Acc@5 100.00 ( 99.59)
Test: [ 70/100]	Time  0.027 ( 0.027)	Loss 4.0845e-01 (3.1797e-01)	Acc@1  89.00 ( 90.11)	Acc@5 100.00 ( 99.63)
Test: [ 80/100]	Time  0.030 ( 0.027)	Loss 1.9238e-01 (3.1704e-01)	Acc@1  94.00 ( 90.16)	Acc@5 100.00 ( 99.63)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 3.1396e-01 (3.1626e-01)	Acc@1  87.00 ( 90.16)	Acc@5 100.00 ( 99.65)
 * Acc@1 90.250 Acc@5 99.680
### epoch[52] execution time: 28.3507559299469
EPOCH 53
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [53][  0/391]	Time  0.246 ( 0.246)	Data  0.179 ( 0.179)	Loss 1.6260e-01 (1.6260e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [53][ 10/391]	Time  0.066 ( 0.081)	Data  0.001 ( 0.017)	Loss 1.0785e-01 (1.4017e-01)	Acc@1  96.09 ( 95.38)	Acc@5 100.00 ( 99.93)
Epoch: [53][ 20/391]	Time  0.065 ( 0.074)	Data  0.001 ( 0.010)	Loss 2.2900e-01 (1.3000e-01)	Acc@1  94.53 ( 95.80)	Acc@5 100.00 ( 99.96)
Epoch: [53][ 30/391]	Time  0.065 ( 0.071)	Data  0.001 ( 0.007)	Loss 9.4299e-02 (1.2904e-01)	Acc@1  96.09 ( 95.84)	Acc@5 100.00 ( 99.95)
Epoch: [53][ 40/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.005)	Loss 1.1237e-01 (1.2913e-01)	Acc@1  93.75 ( 95.69)	Acc@5 100.00 ( 99.94)
Epoch: [53][ 50/391]	Time  0.061 ( 0.068)	Data  0.001 ( 0.005)	Loss 1.6260e-01 (1.2826e-01)	Acc@1  94.53 ( 95.71)	Acc@5 100.00 ( 99.95)
Epoch: [53][ 60/391]	Time  0.060 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.2122e-01 (1.3151e-01)	Acc@1  96.88 ( 95.52)	Acc@5 100.00 ( 99.95)
Epoch: [53][ 70/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.2457e-01 (1.2984e-01)	Acc@1  96.09 ( 95.57)	Acc@5 100.00 ( 99.96)
Epoch: [53][ 80/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.0321e-01 (1.2738e-01)	Acc@1  95.31 ( 95.61)	Acc@5 100.00 ( 99.96)
Epoch: [53][ 90/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.2347e-01 (1.2836e-01)	Acc@1  96.88 ( 95.67)	Acc@5 100.00 ( 99.97)
Epoch: [53][100/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.5295e-01 (1.2955e-01)	Acc@1  95.31 ( 95.58)	Acc@5 100.00 ( 99.97)
Epoch: [53][110/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.0529e-01 (1.2900e-01)	Acc@1  96.09 ( 95.57)	Acc@5 100.00 ( 99.96)
Epoch: [53][120/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.3047e-01 (1.3065e-01)	Acc@1  89.84 ( 95.45)	Acc@5 100.00 ( 99.96)
Epoch: [53][130/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4697e-01 (1.3110e-01)	Acc@1  96.09 ( 95.40)	Acc@5 100.00 ( 99.96)
Epoch: [53][140/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2952e-01 (1.3144e-01)	Acc@1  95.31 ( 95.38)	Acc@5 100.00 ( 99.96)
Epoch: [53][150/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0406e-01 (1.3176e-01)	Acc@1  95.31 ( 95.33)	Acc@5 100.00 ( 99.96)
Epoch: [53][160/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.6252e-02 (1.3238e-01)	Acc@1  95.31 ( 95.32)	Acc@5 100.00 ( 99.96)
Epoch: [53][170/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.3506e-02 (1.3287e-01)	Acc@1  96.09 ( 95.27)	Acc@5 100.00 ( 99.96)
Epoch: [53][180/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4465e-01 (1.3436e-01)	Acc@1  92.97 ( 95.23)	Acc@5 100.00 ( 99.96)
Epoch: [53][190/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2274e-01 (1.3392e-01)	Acc@1  96.88 ( 95.24)	Acc@5 100.00 ( 99.96)
Epoch: [53][200/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9788e-01 (1.3579e-01)	Acc@1  94.53 ( 95.20)	Acc@5 100.00 ( 99.96)
Epoch: [53][210/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.0242e-02 (1.3547e-01)	Acc@1  97.66 ( 95.20)	Acc@5 100.00 ( 99.96)
Epoch: [53][220/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3025e-01 (1.3583e-01)	Acc@1  96.09 ( 95.20)	Acc@5 100.00 ( 99.95)
Epoch: [53][230/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2878e-01 (1.3614e-01)	Acc@1  96.88 ( 95.16)	Acc@5 100.00 ( 99.95)
Epoch: [53][240/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6089e-01 (1.3697e-01)	Acc@1  94.53 ( 95.12)	Acc@5  99.22 ( 99.95)
Epoch: [53][250/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5967e-01 (1.3771e-01)	Acc@1  93.75 ( 95.08)	Acc@5 100.00 ( 99.95)
Epoch: [53][260/391]	Time  0.070 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4880e-01 (1.3710e-01)	Acc@1  94.53 ( 95.10)	Acc@5 100.00 ( 99.95)
Epoch: [53][270/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1130e-01 (1.3819e-01)	Acc@1  92.19 ( 95.07)	Acc@5 100.00 ( 99.95)
Epoch: [53][280/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2292e-01 (1.3740e-01)	Acc@1  94.53 ( 95.10)	Acc@5 100.00 ( 99.95)
Epoch: [53][290/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.4829e-02 (1.3743e-01)	Acc@1  97.66 ( 95.10)	Acc@5 100.00 ( 99.95)
Epoch: [53][300/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.4971e-02 (1.3806e-01)	Acc@1  96.09 ( 95.08)	Acc@5 100.00 ( 99.95)
Epoch: [53][310/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1094e-01 (1.3967e-01)	Acc@1  89.84 ( 95.02)	Acc@5 100.00 ( 99.95)
Epoch: [53][320/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3232e-01 (1.4069e-01)	Acc@1  95.31 ( 94.99)	Acc@5  99.22 ( 99.95)
Epoch: [53][330/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1224e-01 (1.4095e-01)	Acc@1  96.09 ( 94.99)	Acc@5 100.00 ( 99.95)
Epoch: [53][340/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8604e-01 (1.4148e-01)	Acc@1  94.53 ( 94.98)	Acc@5  99.22 ( 99.95)
Epoch: [53][350/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0944e-01 (1.4191e-01)	Acc@1  96.88 ( 94.96)	Acc@5 100.00 ( 99.94)
Epoch: [53][360/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3684e-01 (1.4197e-01)	Acc@1  95.31 ( 94.96)	Acc@5 100.00 ( 99.95)
Epoch: [53][370/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.7351e-02 (1.4144e-01)	Acc@1  98.44 ( 94.99)	Acc@5 100.00 ( 99.95)
Epoch: [53][380/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2048e-01 (1.4125e-01)	Acc@1  95.31 ( 95.01)	Acc@5 100.00 ( 99.95)
Epoch: [53][390/391]	Time  0.047 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9128e-01 (1.4196e-01)	Acc@1  92.50 ( 94.99)	Acc@5 100.00 ( 99.95)
## e[53] optimizer.zero_grad (sum) time: 0.3922398090362549
## e[53]       loss.backward (sum) time: 7.222912549972534
## e[53]      optimizer.step (sum) time: 3.441831588745117
## epoch[53] training(only) time: 25.55639624595642
# Switched to evaluate mode...
Test: [  0/100]	Time  0.190 ( 0.190)	Loss 2.8369e-01 (2.8369e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.040)	Loss 4.3530e-01 (3.2460e-01)	Acc@1  91.00 ( 89.91)	Acc@5 100.00 ( 99.55)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 3.5815e-01 (3.1880e-01)	Acc@1  87.00 ( 90.19)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 2.9688e-01 (3.2196e-01)	Acc@1  87.00 ( 90.23)	Acc@5 100.00 ( 99.58)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 3.2495e-01 (3.2187e-01)	Acc@1  90.00 ( 90.10)	Acc@5 100.00 ( 99.61)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 2.3804e-01 (3.2306e-01)	Acc@1  91.00 ( 89.98)	Acc@5  99.00 ( 99.65)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 2.0337e-01 (3.1687e-01)	Acc@1  91.00 ( 90.15)	Acc@5 100.00 ( 99.69)
Test: [ 70/100]	Time  0.024 ( 0.027)	Loss 4.3457e-01 (3.1700e-01)	Acc@1  88.00 ( 90.20)	Acc@5 100.00 ( 99.69)
Test: [ 80/100]	Time  0.027 ( 0.027)	Loss 1.5259e-01 (3.1855e-01)	Acc@1  93.00 ( 90.20)	Acc@5 100.00 ( 99.69)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 2.5317e-01 (3.1877e-01)	Acc@1  89.00 ( 90.15)	Acc@5 100.00 ( 99.71)
 * Acc@1 90.200 Acc@5 99.720
### epoch[53] execution time: 28.38159465789795
EPOCH 54
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [54][  0/391]	Time  0.248 ( 0.248)	Data  0.183 ( 0.183)	Loss 1.1304e-01 (1.1304e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
Epoch: [54][ 10/391]	Time  0.066 ( 0.081)	Data  0.001 ( 0.018)	Loss 1.1505e-01 (1.2958e-01)	Acc@1  96.88 ( 95.24)	Acc@5 100.00 (100.00)
Epoch: [54][ 20/391]	Time  0.064 ( 0.072)	Data  0.001 ( 0.010)	Loss 1.8201e-01 (1.3622e-01)	Acc@1  93.75 ( 94.61)	Acc@5 100.00 (100.00)
Epoch: [54][ 30/391]	Time  0.062 ( 0.070)	Data  0.001 ( 0.007)	Loss 9.0759e-02 (1.3251e-01)	Acc@1  96.88 ( 94.71)	Acc@5 100.00 (100.00)
Epoch: [54][ 40/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.005)	Loss 1.3928e-01 (1.3437e-01)	Acc@1  94.53 ( 94.86)	Acc@5 100.00 ( 99.98)
Epoch: [54][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.005)	Loss 1.0803e-01 (1.3293e-01)	Acc@1  96.09 ( 95.02)	Acc@5 100.00 ( 99.98)
Epoch: [54][ 60/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.5930e-01 (1.3889e-01)	Acc@1  92.97 ( 94.77)	Acc@5 100.00 ( 99.97)
Epoch: [54][ 70/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.3367e-01 (1.3750e-01)	Acc@1  96.88 ( 94.86)	Acc@5 100.00 ( 99.94)
Epoch: [54][ 80/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.0315e-01 (1.3668e-01)	Acc@1  96.88 ( 94.96)	Acc@5 100.00 ( 99.94)
Epoch: [54][ 90/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.3257e-01 (1.3797e-01)	Acc@1  94.53 ( 94.99)	Acc@5 100.00 ( 99.94)
Epoch: [54][100/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.3306e-01 (1.3734e-01)	Acc@1  95.31 ( 95.01)	Acc@5 100.00 ( 99.95)
Epoch: [54][110/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.3291e-01 (1.3856e-01)	Acc@1  92.19 ( 95.04)	Acc@5 100.00 ( 99.94)
Epoch: [54][120/391]	Time  0.073 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.5015e-01 (1.3870e-01)	Acc@1  91.41 ( 95.03)	Acc@5 100.00 ( 99.95)
Epoch: [54][130/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.4849e-02 (1.3964e-01)	Acc@1  96.88 ( 95.04)	Acc@5 100.00 ( 99.95)
Epoch: [54][140/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.1472e-01 (1.3970e-01)	Acc@1  94.53 ( 95.03)	Acc@5 100.00 ( 99.95)
Epoch: [54][150/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.0444e-02 (1.3894e-01)	Acc@1  97.66 ( 95.07)	Acc@5 100.00 ( 99.95)
Epoch: [54][160/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2817e-01 (1.3941e-01)	Acc@1  95.31 ( 95.10)	Acc@5 100.00 ( 99.96)
Epoch: [54][170/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.7810e-02 (1.3955e-01)	Acc@1  96.88 ( 95.07)	Acc@5 100.00 ( 99.96)
Epoch: [54][180/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6223e-01 (1.3998e-01)	Acc@1  94.53 ( 95.02)	Acc@5  99.22 ( 99.96)
Epoch: [54][190/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4490e-01 (1.4043e-01)	Acc@1  93.75 ( 95.01)	Acc@5 100.00 ( 99.96)
Epoch: [54][200/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4905e-01 (1.4006e-01)	Acc@1  95.31 ( 95.00)	Acc@5 100.00 ( 99.96)
Epoch: [54][210/391]	Time  0.074 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.8787e-02 (1.3987e-01)	Acc@1  97.66 ( 95.02)	Acc@5 100.00 ( 99.96)
Epoch: [54][220/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0120e-01 (1.3970e-01)	Acc@1  96.88 ( 95.00)	Acc@5 100.00 ( 99.96)
Epoch: [54][230/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1761e-01 (1.4002e-01)	Acc@1  95.31 ( 94.98)	Acc@5 100.00 ( 99.96)
Epoch: [54][240/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1240e-01 (1.3980e-01)	Acc@1  92.19 ( 95.03)	Acc@5 100.00 ( 99.96)
Epoch: [54][250/391]	Time  0.070 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.5510e-02 (1.4045e-01)	Acc@1  96.09 ( 95.01)	Acc@5 100.00 ( 99.96)
Epoch: [54][260/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4587e-01 (1.4077e-01)	Acc@1  95.31 ( 94.99)	Acc@5 100.00 ( 99.96)
Epoch: [54][270/391]	Time  0.071 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5625e-01 (1.3994e-01)	Acc@1  96.09 ( 95.02)	Acc@5 100.00 ( 99.96)
Epoch: [54][280/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8176e-01 (1.3940e-01)	Acc@1  94.53 ( 95.05)	Acc@5  99.22 ( 99.96)
Epoch: [54][290/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2622e-01 (1.3928e-01)	Acc@1  96.88 ( 95.07)	Acc@5 100.00 ( 99.96)
Epoch: [54][300/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4465e-01 (1.3896e-01)	Acc@1  95.31 ( 95.09)	Acc@5 100.00 ( 99.96)
Epoch: [54][310/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9531e-01 (1.3901e-01)	Acc@1  92.19 ( 95.08)	Acc@5 100.00 ( 99.96)
Epoch: [54][320/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1737e-01 (1.3914e-01)	Acc@1  96.09 ( 95.07)	Acc@5 100.00 ( 99.96)
Epoch: [54][330/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7468e-01 (1.3892e-01)	Acc@1  94.53 ( 95.08)	Acc@5 100.00 ( 99.96)
Epoch: [54][340/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1252e-01 (1.3891e-01)	Acc@1  92.97 ( 95.09)	Acc@5 100.00 ( 99.96)
Epoch: [54][350/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5308e-01 (1.3956e-01)	Acc@1  94.53 ( 95.06)	Acc@5 100.00 ( 99.96)
Epoch: [54][360/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3721e-01 (1.3937e-01)	Acc@1  95.31 ( 95.07)	Acc@5 100.00 ( 99.96)
Epoch: [54][370/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1350e-01 (1.3903e-01)	Acc@1  92.19 ( 95.09)	Acc@5 100.00 ( 99.96)
Epoch: [54][380/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8628e-01 (1.3908e-01)	Acc@1  92.19 ( 95.09)	Acc@5 100.00 ( 99.96)
Epoch: [54][390/391]	Time  0.047 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0239e-01 (1.3916e-01)	Acc@1  91.25 ( 95.08)	Acc@5 100.00 ( 99.96)
## e[54] optimizer.zero_grad (sum) time: 0.39324212074279785
## e[54]       loss.backward (sum) time: 7.243587970733643
## e[54]      optimizer.step (sum) time: 3.468143939971924
## epoch[54] training(only) time: 25.617525577545166
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 2.9761e-01 (2.9761e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.040)	Loss 4.2212e-01 (3.5421e-01)	Acc@1  87.00 ( 89.64)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.026 ( 0.033)	Loss 4.1333e-01 (3.4341e-01)	Acc@1  87.00 ( 89.62)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.024 ( 0.030)	Loss 2.9175e-01 (3.3812e-01)	Acc@1  90.00 ( 90.19)	Acc@5  99.00 ( 99.61)
Test: [ 40/100]	Time  0.028 ( 0.029)	Loss 3.7598e-01 (3.4238e-01)	Acc@1  85.00 ( 90.05)	Acc@5 100.00 ( 99.63)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 2.2656e-01 (3.3919e-01)	Acc@1  92.00 ( 89.90)	Acc@5  99.00 ( 99.63)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 1.8713e-01 (3.3381e-01)	Acc@1  94.00 ( 90.10)	Acc@5 100.00 ( 99.66)
Test: [ 70/100]	Time  0.024 ( 0.027)	Loss 4.4800e-01 (3.3786e-01)	Acc@1  88.00 ( 90.07)	Acc@5 100.00 ( 99.66)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 1.3354e-01 (3.4096e-01)	Acc@1  96.00 ( 89.99)	Acc@5 100.00 ( 99.67)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 2.8369e-01 (3.4172e-01)	Acc@1  91.00 ( 89.96)	Acc@5 100.00 ( 99.66)
 * Acc@1 90.050 Acc@5 99.670
### epoch[54] execution time: 28.374030828475952
EPOCH 55
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [55][  0/391]	Time  0.244 ( 0.244)	Data  0.179 ( 0.179)	Loss 1.6541e-01 (1.6541e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
Epoch: [55][ 10/391]	Time  0.061 ( 0.081)	Data  0.001 ( 0.017)	Loss 9.4910e-02 (1.1863e-01)	Acc@1  96.88 ( 95.67)	Acc@5 100.00 (100.00)
Epoch: [55][ 20/391]	Time  0.063 ( 0.073)	Data  0.001 ( 0.010)	Loss 1.7065e-01 (1.2716e-01)	Acc@1  92.97 ( 95.46)	Acc@5 100.00 (100.00)
Epoch: [55][ 30/391]	Time  0.060 ( 0.070)	Data  0.001 ( 0.007)	Loss 2.0862e-01 (1.3235e-01)	Acc@1  92.97 ( 95.24)	Acc@5 100.00 (100.00)
Epoch: [55][ 40/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.005)	Loss 1.1389e-01 (1.3089e-01)	Acc@1  96.88 ( 95.24)	Acc@5 100.00 ( 99.98)
Epoch: [55][ 50/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.005)	Loss 1.5991e-01 (1.3104e-01)	Acc@1  94.53 ( 95.22)	Acc@5 100.00 ( 99.98)
Epoch: [55][ 60/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.3904e-01 (1.2803e-01)	Acc@1  93.75 ( 95.30)	Acc@5 100.00 ( 99.97)
Epoch: [55][ 70/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.004)	Loss 8.6426e-02 (1.2722e-01)	Acc@1  98.44 ( 95.42)	Acc@5 100.00 ( 99.98)
Epoch: [55][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.0535e-01 (1.2563e-01)	Acc@1  95.31 ( 95.41)	Acc@5 100.00 ( 99.98)
Epoch: [55][ 90/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.2888e-01 (1.2616e-01)	Acc@1  94.53 ( 95.49)	Acc@5 100.00 ( 99.97)
Epoch: [55][100/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.2396e-01 (1.2704e-01)	Acc@1  94.53 ( 95.53)	Acc@5 100.00 ( 99.97)
Epoch: [55][110/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.1765e-01 (1.2605e-01)	Acc@1  92.19 ( 95.55)	Acc@5 100.00 ( 99.97)
Epoch: [55][120/391]	Time  0.073 ( 0.066)	Data  0.001 ( 0.003)	Loss 9.3445e-02 (1.2663e-01)	Acc@1  96.88 ( 95.49)	Acc@5 100.00 ( 99.97)
Epoch: [55][130/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.9304e-02 (1.2687e-01)	Acc@1  96.09 ( 95.46)	Acc@5 100.00 ( 99.97)
Epoch: [55][140/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4136e-01 (1.2688e-01)	Acc@1  92.19 ( 95.43)	Acc@5 100.00 ( 99.97)
Epoch: [55][150/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5442e-01 (1.2771e-01)	Acc@1  93.75 ( 95.42)	Acc@5 100.00 ( 99.97)
Epoch: [55][160/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.1980e-02 (1.2741e-01)	Acc@1  96.88 ( 95.44)	Acc@5 100.00 ( 99.96)
Epoch: [55][170/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3477e-01 (1.2807e-01)	Acc@1  93.75 ( 95.39)	Acc@5 100.00 ( 99.96)
Epoch: [55][180/391]	Time  0.060 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.2681e-01 (1.2900e-01)	Acc@1  92.19 ( 95.39)	Acc@5 100.00 ( 99.97)
Epoch: [55][190/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.1675e-02 (1.2908e-01)	Acc@1  97.66 ( 95.39)	Acc@5 100.00 ( 99.97)
Epoch: [55][200/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2054e-01 (1.3007e-01)	Acc@1  96.09 ( 95.36)	Acc@5 100.00 ( 99.97)
Epoch: [55][210/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.8909e-02 (1.2963e-01)	Acc@1  96.88 ( 95.37)	Acc@5 100.00 ( 99.97)
Epoch: [55][220/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.1460e-01 (1.2991e-01)	Acc@1  94.53 ( 95.36)	Acc@5 100.00 ( 99.97)
Epoch: [55][230/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6357e-01 (1.3039e-01)	Acc@1  93.75 ( 95.36)	Acc@5 100.00 ( 99.97)
Epoch: [55][240/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.3872e-02 (1.3030e-01)	Acc@1  96.09 ( 95.35)	Acc@5 100.00 ( 99.97)
Epoch: [55][250/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2622e-01 (1.3127e-01)	Acc@1  96.88 ( 95.32)	Acc@5 100.00 ( 99.97)
Epoch: [55][260/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8921e-01 (1.3112e-01)	Acc@1  92.97 ( 95.35)	Acc@5 100.00 ( 99.97)
Epoch: [55][270/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.0178e-01 (1.3112e-01)	Acc@1  93.75 ( 95.36)	Acc@5  99.22 ( 99.97)
Epoch: [55][280/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.8014e-02 (1.3130e-01)	Acc@1  99.22 ( 95.35)	Acc@5 100.00 ( 99.97)
Epoch: [55][290/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1475e-01 (1.3161e-01)	Acc@1  96.88 ( 95.34)	Acc@5 100.00 ( 99.97)
Epoch: [55][300/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8591e-01 (1.3189e-01)	Acc@1  96.09 ( 95.35)	Acc@5 100.00 ( 99.97)
Epoch: [55][310/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1301e-01 (1.3256e-01)	Acc@1  92.97 ( 95.33)	Acc@5 100.00 ( 99.97)
Epoch: [55][320/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2360e-01 (1.3381e-01)	Acc@1  95.31 ( 95.27)	Acc@5 100.00 ( 99.97)
Epoch: [55][330/391]	Time  0.071 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1926e-01 (1.3381e-01)	Acc@1  96.09 ( 95.25)	Acc@5 100.00 ( 99.97)
Epoch: [55][340/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4526e-01 (1.3445e-01)	Acc@1  94.53 ( 95.23)	Acc@5 100.00 ( 99.97)
Epoch: [55][350/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.6191e-02 (1.3471e-01)	Acc@1  96.88 ( 95.20)	Acc@5 100.00 ( 99.97)
Epoch: [55][360/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8494e-01 (1.3495e-01)	Acc@1  93.75 ( 95.20)	Acc@5 100.00 ( 99.97)
Epoch: [55][370/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7395e-01 (1.3573e-01)	Acc@1  95.31 ( 95.19)	Acc@5 100.00 ( 99.97)
Epoch: [55][380/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.4658e-01 (1.3595e-01)	Acc@1  91.41 ( 95.20)	Acc@5 100.00 ( 99.97)
Epoch: [55][390/391]	Time  0.047 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2207e-01 (1.3594e-01)	Acc@1  96.25 ( 95.21)	Acc@5 100.00 ( 99.97)
## e[55] optimizer.zero_grad (sum) time: 0.39721226692199707
## e[55]       loss.backward (sum) time: 7.3008880615234375
## e[55]      optimizer.step (sum) time: 3.431684970855713
## epoch[55] training(only) time: 25.641175270080566
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 3.0176e-01 (3.0176e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.040)	Loss 4.7192e-01 (3.2543e-01)	Acc@1  88.00 ( 89.91)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 3.3594e-01 (3.2236e-01)	Acc@1  90.00 ( 90.24)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.024 ( 0.030)	Loss 2.7734e-01 (3.2814e-01)	Acc@1  88.00 ( 90.29)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 3.8477e-01 (3.2756e-01)	Acc@1  89.00 ( 90.20)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 1.9995e-01 (3.2542e-01)	Acc@1  90.00 ( 89.96)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 1.9434e-01 (3.1996e-01)	Acc@1  96.00 ( 90.26)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.024 ( 0.027)	Loss 4.6362e-01 (3.2349e-01)	Acc@1  88.00 ( 90.15)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 1.2292e-01 (3.2273e-01)	Acc@1  94.00 ( 90.17)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.027 ( 0.027)	Loss 2.7881e-01 (3.2267e-01)	Acc@1  88.00 ( 90.12)	Acc@5 100.00 ( 99.74)
 * Acc@1 90.270 Acc@5 99.760
### epoch[55] execution time: 28.459430932998657
EPOCH 56
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [56][  0/391]	Time  0.239 ( 0.239)	Data  0.174 ( 0.174)	Loss 1.9238e-01 (1.9238e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
Epoch: [56][ 10/391]	Time  0.064 ( 0.080)	Data  0.001 ( 0.017)	Loss 1.0974e-01 (1.4323e-01)	Acc@1  97.66 ( 95.17)	Acc@5 100.00 ( 99.86)
Epoch: [56][ 20/391]	Time  0.064 ( 0.073)	Data  0.001 ( 0.009)	Loss 1.2195e-01 (1.4394e-01)	Acc@1  94.53 ( 94.79)	Acc@5 100.00 ( 99.89)
Epoch: [56][ 30/391]	Time  0.070 ( 0.070)	Data  0.001 ( 0.007)	Loss 1.2439e-01 (1.3808e-01)	Acc@1  96.09 ( 94.96)	Acc@5 100.00 ( 99.92)
Epoch: [56][ 40/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.005)	Loss 1.9849e-01 (1.4283e-01)	Acc@1  92.97 ( 94.89)	Acc@5 100.00 ( 99.90)
Epoch: [56][ 50/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.4648e-01 (1.3852e-01)	Acc@1  96.09 ( 95.10)	Acc@5  99.22 ( 99.91)
Epoch: [56][ 60/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.5784e-01 (1.4052e-01)	Acc@1  96.09 ( 95.07)	Acc@5 100.00 ( 99.92)
Epoch: [56][ 70/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.1548e-01 (1.3726e-01)	Acc@1  95.31 ( 95.17)	Acc@5 100.00 ( 99.93)
Epoch: [56][ 80/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 9.6924e-02 (1.3737e-01)	Acc@1  94.53 ( 95.16)	Acc@5 100.00 ( 99.94)
Epoch: [56][ 90/391]	Time  0.061 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.5295e-01 (1.3797e-01)	Acc@1  95.31 ( 95.15)	Acc@5 100.00 ( 99.95)
Epoch: [56][100/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.4722e-01 (1.3709e-01)	Acc@1  94.53 ( 95.23)	Acc@5 100.00 ( 99.94)
Epoch: [56][110/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.4856e-01 (1.3647e-01)	Acc@1  92.97 ( 95.23)	Acc@5 100.00 ( 99.94)
Epoch: [56][120/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1951e-01 (1.3606e-01)	Acc@1  96.09 ( 95.25)	Acc@5 100.00 ( 99.94)
Epoch: [56][130/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2177e-01 (1.3652e-01)	Acc@1  95.31 ( 95.22)	Acc@5 100.00 ( 99.95)
Epoch: [56][140/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1591e-01 (1.3642e-01)	Acc@1  94.53 ( 95.22)	Acc@5 100.00 ( 99.95)
Epoch: [56][150/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4648e-01 (1.3504e-01)	Acc@1  95.31 ( 95.28)	Acc@5 100.00 ( 99.95)
Epoch: [56][160/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5698e-01 (1.3470e-01)	Acc@1  94.53 ( 95.29)	Acc@5 100.00 ( 99.95)
Epoch: [56][170/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.4304e-01 (1.3549e-01)	Acc@1  90.62 ( 95.25)	Acc@5 100.00 ( 99.95)
Epoch: [56][180/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2793e-01 (1.3509e-01)	Acc@1  95.31 ( 95.26)	Acc@5 100.00 ( 99.95)
Epoch: [56][190/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.7078e-02 (1.3514e-01)	Acc@1  97.66 ( 95.26)	Acc@5 100.00 ( 99.95)
Epoch: [56][200/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9373e-01 (1.3507e-01)	Acc@1  93.75 ( 95.25)	Acc@5  99.22 ( 99.95)
Epoch: [56][210/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7212e-01 (1.3574e-01)	Acc@1  94.53 ( 95.25)	Acc@5 100.00 ( 99.94)
Epoch: [56][220/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.4473e-02 (1.3485e-01)	Acc@1  96.88 ( 95.28)	Acc@5 100.00 ( 99.94)
Epoch: [56][230/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0968e-01 (1.3522e-01)	Acc@1  96.09 ( 95.26)	Acc@5 100.00 ( 99.94)
Epoch: [56][240/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2805e-01 (1.3566e-01)	Acc@1  96.09 ( 95.22)	Acc@5 100.00 ( 99.94)
Epoch: [56][250/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9177e-01 (1.3620e-01)	Acc@1  92.97 ( 95.19)	Acc@5 100.00 ( 99.94)
Epoch: [56][260/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8188e-01 (1.3638e-01)	Acc@1  92.97 ( 95.17)	Acc@5 100.00 ( 99.94)
Epoch: [56][270/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.3506e-02 (1.3608e-01)	Acc@1  96.09 ( 95.17)	Acc@5 100.00 ( 99.94)
Epoch: [56][280/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5344e-01 (1.3678e-01)	Acc@1  94.53 ( 95.15)	Acc@5 100.00 ( 99.94)
Epoch: [56][290/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5735e-01 (1.3654e-01)	Acc@1  92.19 ( 95.16)	Acc@5 100.00 ( 99.94)
Epoch: [56][300/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5063e-01 (1.3712e-01)	Acc@1  95.31 ( 95.14)	Acc@5 100.00 ( 99.94)
Epoch: [56][310/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3721e-01 (1.3767e-01)	Acc@1  92.97 ( 95.11)	Acc@5 100.00 ( 99.94)
Epoch: [56][320/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9934e-01 (1.3760e-01)	Acc@1  90.62 ( 95.10)	Acc@5 100.00 ( 99.94)
Epoch: [56][330/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2253e-01 (1.3813e-01)	Acc@1  91.41 ( 95.09)	Acc@5 100.00 ( 99.95)
Epoch: [56][340/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.7524e-02 (1.3794e-01)	Acc@1  96.88 ( 95.09)	Acc@5 100.00 ( 99.95)
Epoch: [56][350/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5186e-01 (1.3797e-01)	Acc@1  92.97 ( 95.10)	Acc@5 100.00 ( 99.95)
Epoch: [56][360/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3428e-01 (1.3786e-01)	Acc@1  94.53 ( 95.11)	Acc@5 100.00 ( 99.95)
Epoch: [56][370/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6589e-01 (1.3808e-01)	Acc@1  93.75 ( 95.10)	Acc@5 100.00 ( 99.95)
Epoch: [56][380/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2756e-01 (1.3827e-01)	Acc@1  92.97 ( 95.08)	Acc@5 100.00 ( 99.95)
Epoch: [56][390/391]	Time  0.048 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9141e-01 (1.3847e-01)	Acc@1  95.00 ( 95.08)	Acc@5 100.00 ( 99.95)
## e[56] optimizer.zero_grad (sum) time: 0.38666725158691406
## e[56]       loss.backward (sum) time: 7.232864618301392
## e[56]      optimizer.step (sum) time: 3.4779598712921143
## epoch[56] training(only) time: 25.49976682662964
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 2.8345e-01 (2.8345e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.040)	Loss 4.3213e-01 (3.3117e-01)	Acc@1  90.00 ( 90.91)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 4.7656e-01 (3.2607e-01)	Acc@1  84.00 ( 90.43)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.024 ( 0.030)	Loss 2.6465e-01 (3.2622e-01)	Acc@1  87.00 ( 90.39)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 3.5352e-01 (3.3429e-01)	Acc@1  89.00 ( 90.12)	Acc@5  99.00 ( 99.54)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 2.0911e-01 (3.3122e-01)	Acc@1  94.00 ( 90.10)	Acc@5 100.00 ( 99.61)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 2.0154e-01 (3.2394e-01)	Acc@1  92.00 ( 90.33)	Acc@5 100.00 ( 99.66)
Test: [ 70/100]	Time  0.024 ( 0.027)	Loss 4.5068e-01 (3.2315e-01)	Acc@1  89.00 ( 90.38)	Acc@5 100.00 ( 99.66)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 1.6516e-01 (3.2148e-01)	Acc@1  94.00 ( 90.40)	Acc@5 100.00 ( 99.67)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 2.4707e-01 (3.1993e-01)	Acc@1  91.00 ( 90.37)	Acc@5 100.00 ( 99.69)
 * Acc@1 90.460 Acc@5 99.700
### epoch[56] execution time: 28.297795057296753
EPOCH 57
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [57][  0/391]	Time  0.244 ( 0.244)	Data  0.177 ( 0.177)	Loss 1.7419e-01 (1.7419e-01)	Acc@1  92.19 ( 92.19)	Acc@5 100.00 (100.00)
Epoch: [57][ 10/391]	Time  0.065 ( 0.081)	Data  0.001 ( 0.017)	Loss 1.2195e-01 (1.3214e-01)	Acc@1  95.31 ( 94.82)	Acc@5 100.00 ( 99.93)
Epoch: [57][ 20/391]	Time  0.064 ( 0.073)	Data  0.001 ( 0.009)	Loss 1.3599e-01 (1.3435e-01)	Acc@1  94.53 ( 95.13)	Acc@5 100.00 ( 99.89)
Epoch: [57][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.007)	Loss 1.4600e-01 (1.3157e-01)	Acc@1  94.53 ( 95.29)	Acc@5 100.00 ( 99.92)
Epoch: [57][ 40/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.005)	Loss 5.3558e-02 (1.2692e-01)	Acc@1  98.44 ( 95.48)	Acc@5 100.00 ( 99.94)
Epoch: [57][ 50/391]	Time  0.062 ( 0.068)	Data  0.001 ( 0.005)	Loss 9.1919e-02 (1.2819e-01)	Acc@1  96.09 ( 95.47)	Acc@5 100.00 ( 99.95)
Epoch: [57][ 60/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.6296e-01 (1.2833e-01)	Acc@1  94.53 ( 95.45)	Acc@5 100.00 ( 99.96)
Epoch: [57][ 70/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.9751e-01 (1.2910e-01)	Acc@1  95.31 ( 95.50)	Acc@5 100.00 ( 99.97)
Epoch: [57][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.5723e-01 (1.2875e-01)	Acc@1  95.31 ( 95.50)	Acc@5 100.00 ( 99.97)
Epoch: [57][ 90/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.8665e-01 (1.2883e-01)	Acc@1  92.97 ( 95.51)	Acc@5 100.00 ( 99.97)
Epoch: [57][100/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.0748e-01 (1.3042e-01)	Acc@1  95.31 ( 95.49)	Acc@5 100.00 ( 99.96)
Epoch: [57][110/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.9385e-01 (1.3350e-01)	Acc@1  94.53 ( 95.43)	Acc@5 100.00 ( 99.96)
Epoch: [57][120/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.4478e-01 (1.3319e-01)	Acc@1  95.31 ( 95.46)	Acc@5  99.22 ( 99.96)
Epoch: [57][130/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0297e-01 (1.3265e-01)	Acc@1  96.88 ( 95.49)	Acc@5 100.00 ( 99.96)
Epoch: [57][140/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4343e-01 (1.3357e-01)	Acc@1  96.09 ( 95.47)	Acc@5 100.00 ( 99.96)
Epoch: [57][150/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7322e-01 (1.3548e-01)	Acc@1  93.75 ( 95.38)	Acc@5 100.00 ( 99.96)
Epoch: [57][160/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.2285e-02 (1.3548e-01)	Acc@1  96.09 ( 95.38)	Acc@5 100.00 ( 99.97)
Epoch: [57][170/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5088e-01 (1.3579e-01)	Acc@1  94.53 ( 95.34)	Acc@5 100.00 ( 99.97)
Epoch: [57][180/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0730e-01 (1.3513e-01)	Acc@1  95.31 ( 95.33)	Acc@5 100.00 ( 99.97)
Epoch: [57][190/391]	Time  0.071 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8335e-01 (1.3661e-01)	Acc@1  93.75 ( 95.30)	Acc@5 100.00 ( 99.97)
Epoch: [57][200/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.6792e-02 (1.3691e-01)	Acc@1  97.66 ( 95.26)	Acc@5 100.00 ( 99.97)
Epoch: [57][210/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6748e-01 (1.3715e-01)	Acc@1  96.09 ( 95.22)	Acc@5 100.00 ( 99.97)
Epoch: [57][220/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0181e-01 (1.3694e-01)	Acc@1  97.66 ( 95.22)	Acc@5 100.00 ( 99.97)
Epoch: [57][230/391]	Time  0.070 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4722e-01 (1.3690e-01)	Acc@1  95.31 ( 95.22)	Acc@5 100.00 ( 99.97)
Epoch: [57][240/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.5449e-02 (1.3689e-01)	Acc@1  96.09 ( 95.20)	Acc@5 100.00 ( 99.96)
Epoch: [57][250/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3208e-01 (1.3679e-01)	Acc@1  94.53 ( 95.22)	Acc@5 100.00 ( 99.96)
Epoch: [57][260/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2036e-01 (1.3600e-01)	Acc@1  96.88 ( 95.26)	Acc@5 100.00 ( 99.96)
Epoch: [57][270/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.5939e-02 (1.3468e-01)	Acc@1  98.44 ( 95.31)	Acc@5 100.00 ( 99.97)
Epoch: [57][280/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1692e-01 (1.3502e-01)	Acc@1  92.19 ( 95.31)	Acc@5  99.22 ( 99.96)
Epoch: [57][290/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3110e-01 (1.3492e-01)	Acc@1  93.75 ( 95.30)	Acc@5 100.00 ( 99.96)
Epoch: [57][300/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8372e-01 (1.3466e-01)	Acc@1  94.53 ( 95.29)	Acc@5 100.00 ( 99.96)
Epoch: [57][310/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1456e-01 (1.3492e-01)	Acc@1  96.09 ( 95.28)	Acc@5 100.00 ( 99.96)
Epoch: [57][320/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7590e-01 (1.3457e-01)	Acc@1  94.53 ( 95.30)	Acc@5 100.00 ( 99.96)
Epoch: [57][330/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1920e-01 (1.3434e-01)	Acc@1  93.75 ( 95.29)	Acc@5 100.00 ( 99.96)
Epoch: [57][340/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2146e-01 (1.3447e-01)	Acc@1  95.31 ( 95.29)	Acc@5 100.00 ( 99.96)
Epoch: [57][350/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.2571e-02 (1.3438e-01)	Acc@1  96.88 ( 95.28)	Acc@5 100.00 ( 99.96)
Epoch: [57][360/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4331e-01 (1.3439e-01)	Acc@1  96.09 ( 95.28)	Acc@5 100.00 ( 99.96)
Epoch: [57][370/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1346e-01 (1.3541e-01)	Acc@1  94.53 ( 95.23)	Acc@5 100.00 ( 99.96)
Epoch: [57][380/391]	Time  0.073 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9080e-01 (1.3591e-01)	Acc@1  92.19 ( 95.22)	Acc@5 100.00 ( 99.96)
Epoch: [57][390/391]	Time  0.045 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.9336e-02 (1.3564e-01)	Acc@1  97.50 ( 95.21)	Acc@5 100.00 ( 99.96)
## e[57] optimizer.zero_grad (sum) time: 0.3843722343444824
## e[57]       loss.backward (sum) time: 7.266634941101074
## e[57]      optimizer.step (sum) time: 3.438896894454956
## epoch[57] training(only) time: 25.495365858078003
# Switched to evaluate mode...
Test: [  0/100]	Time  0.179 ( 0.179)	Loss 3.5034e-01 (3.5034e-01)	Acc@1  87.00 ( 87.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.039)	Loss 4.4165e-01 (3.6292e-01)	Acc@1  91.00 ( 89.09)	Acc@5  99.00 ( 99.55)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 3.5986e-01 (3.4433e-01)	Acc@1  86.00 ( 89.52)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.024 ( 0.030)	Loss 2.7759e-01 (3.4484e-01)	Acc@1  90.00 ( 89.84)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 3.7036e-01 (3.4735e-01)	Acc@1  89.00 ( 89.66)	Acc@5  99.00 ( 99.61)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 2.7075e-01 (3.4368e-01)	Acc@1  90.00 ( 89.65)	Acc@5  99.00 ( 99.65)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 1.9202e-01 (3.3762e-01)	Acc@1  94.00 ( 89.84)	Acc@5 100.00 ( 99.66)
Test: [ 70/100]	Time  0.026 ( 0.027)	Loss 4.3066e-01 (3.3278e-01)	Acc@1  88.00 ( 90.03)	Acc@5 100.00 ( 99.68)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 1.8250e-01 (3.3265e-01)	Acc@1  92.00 ( 89.95)	Acc@5 100.00 ( 99.67)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 4.0552e-01 (3.3403e-01)	Acc@1  87.00 ( 89.95)	Acc@5 100.00 ( 99.65)
 * Acc@1 90.000 Acc@5 99.640
### epoch[57] execution time: 28.306990385055542
EPOCH 58
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [58][  0/391]	Time  0.242 ( 0.242)	Data  0.179 ( 0.179)	Loss 1.0901e-01 (1.0901e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [58][ 10/391]	Time  0.069 ( 0.081)	Data  0.001 ( 0.017)	Loss 7.2266e-02 (1.2911e-01)	Acc@1  96.88 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [58][ 20/391]	Time  0.064 ( 0.074)	Data  0.001 ( 0.010)	Loss 8.3679e-02 (1.2508e-01)	Acc@1  96.09 ( 95.68)	Acc@5 100.00 ( 99.96)
Epoch: [58][ 30/391]	Time  0.065 ( 0.071)	Data  0.001 ( 0.007)	Loss 1.5417e-01 (1.3198e-01)	Acc@1  94.53 ( 95.49)	Acc@5 100.00 ( 99.92)
Epoch: [58][ 40/391]	Time  0.063 ( 0.069)	Data  0.001 ( 0.005)	Loss 2.3584e-01 (1.3269e-01)	Acc@1  94.53 ( 95.48)	Acc@5 100.00 ( 99.94)
Epoch: [58][ 50/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.005)	Loss 1.5051e-01 (1.3073e-01)	Acc@1  95.31 ( 95.59)	Acc@5  99.22 ( 99.92)
Epoch: [58][ 60/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.1304e-01 (1.2718e-01)	Acc@1  96.09 ( 95.80)	Acc@5 100.00 ( 99.94)
Epoch: [58][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.0376e-01 (1.2432e-01)	Acc@1  96.88 ( 95.83)	Acc@5 100.00 ( 99.94)
Epoch: [58][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.6711e-01 (1.2581e-01)	Acc@1  94.53 ( 95.73)	Acc@5 100.00 ( 99.94)
Epoch: [58][ 90/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.3806e-01 (1.2562e-01)	Acc@1  95.31 ( 95.78)	Acc@5 100.00 ( 99.93)
Epoch: [58][100/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.3499e-01 (1.2814e-01)	Acc@1  92.97 ( 95.67)	Acc@5 100.00 ( 99.94)
Epoch: [58][110/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.003)	Loss 8.4045e-02 (1.2784e-01)	Acc@1  96.88 ( 95.64)	Acc@5 100.00 ( 99.94)
Epoch: [58][120/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.5869e-01 (1.3052e-01)	Acc@1  94.53 ( 95.54)	Acc@5 100.00 ( 99.93)
Epoch: [58][130/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.7139e-02 (1.2985e-01)	Acc@1  98.44 ( 95.50)	Acc@5 100.00 ( 99.93)
Epoch: [58][140/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.8511e-02 (1.2909e-01)	Acc@1  94.53 ( 95.50)	Acc@5 100.00 ( 99.94)
Epoch: [58][150/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.0557e-02 (1.2858e-01)	Acc@1  97.66 ( 95.51)	Acc@5 100.00 ( 99.94)
Epoch: [58][160/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2769e-01 (1.2746e-01)	Acc@1  94.53 ( 95.51)	Acc@5 100.00 ( 99.94)
Epoch: [58][170/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.2205e-02 (1.2774e-01)	Acc@1  97.66 ( 95.49)	Acc@5 100.00 ( 99.95)
Epoch: [58][180/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1145e-01 (1.2803e-01)	Acc@1  95.31 ( 95.51)	Acc@5 100.00 ( 99.95)
Epoch: [58][190/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1035e-01 (1.2768e-01)	Acc@1  96.88 ( 95.50)	Acc@5 100.00 ( 99.95)
Epoch: [58][200/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5674e-01 (1.2799e-01)	Acc@1  95.31 ( 95.50)	Acc@5  99.22 ( 99.95)
Epoch: [58][210/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2152e-01 (1.2843e-01)	Acc@1  94.53 ( 95.47)	Acc@5 100.00 ( 99.95)
Epoch: [58][220/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.4352e-02 (1.2803e-01)	Acc@1  98.44 ( 95.51)	Acc@5 100.00 ( 99.95)
Epoch: [58][230/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4636e-01 (1.2815e-01)	Acc@1  95.31 ( 95.50)	Acc@5 100.00 ( 99.95)
Epoch: [58][240/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1420e-01 (1.2736e-01)	Acc@1  95.31 ( 95.53)	Acc@5 100.00 ( 99.95)
Epoch: [58][250/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4685e-01 (1.2753e-01)	Acc@1  96.09 ( 95.53)	Acc@5 100.00 ( 99.95)
Epoch: [58][260/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4673e-01 (1.2821e-01)	Acc@1  93.75 ( 95.48)	Acc@5 100.00 ( 99.95)
Epoch: [58][270/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2671e-01 (1.2816e-01)	Acc@1  94.53 ( 95.46)	Acc@5 100.00 ( 99.95)
Epoch: [58][280/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4465e-01 (1.2886e-01)	Acc@1  96.88 ( 95.43)	Acc@5  99.22 ( 99.95)
Epoch: [58][290/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3550e-01 (1.2898e-01)	Acc@1  94.53 ( 95.41)	Acc@5 100.00 ( 99.95)
Epoch: [58][300/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.0068e-01 (1.2858e-01)	Acc@1  93.75 ( 95.44)	Acc@5 100.00 ( 99.95)
Epoch: [58][310/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.0505e-02 (1.2826e-01)	Acc@1  97.66 ( 95.46)	Acc@5 100.00 ( 99.95)
Epoch: [58][320/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.9175e-01 (1.2939e-01)	Acc@1  90.62 ( 95.42)	Acc@5 100.00 ( 99.95)
Epoch: [58][330/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2805e-01 (1.2946e-01)	Acc@1  94.53 ( 95.42)	Acc@5 100.00 ( 99.96)
Epoch: [58][340/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2878e-01 (1.2941e-01)	Acc@1  94.53 ( 95.41)	Acc@5 100.00 ( 99.95)
Epoch: [58][350/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4221e-01 (1.2957e-01)	Acc@1  96.88 ( 95.40)	Acc@5 100.00 ( 99.95)
Epoch: [58][360/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1536e-01 (1.2909e-01)	Acc@1  96.09 ( 95.41)	Acc@5 100.00 ( 99.95)
Epoch: [58][370/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5955e-01 (1.2922e-01)	Acc@1  95.31 ( 95.40)	Acc@5 100.00 ( 99.96)
Epoch: [58][380/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2988e-01 (1.2937e-01)	Acc@1  94.53 ( 95.40)	Acc@5 100.00 ( 99.96)
Epoch: [58][390/391]	Time  0.056 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6907e-01 (1.2990e-01)	Acc@1  93.75 ( 95.37)	Acc@5  98.75 ( 99.96)
## e[58] optimizer.zero_grad (sum) time: 0.39138269424438477
## e[58]       loss.backward (sum) time: 7.311842679977417
## e[58]      optimizer.step (sum) time: 3.486294746398926
## epoch[58] training(only) time: 25.730013608932495
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 2.8662e-01 (2.8662e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.039)	Loss 4.6802e-01 (3.3607e-01)	Acc@1  88.00 ( 90.18)	Acc@5  99.00 ( 99.82)
Test: [ 20/100]	Time  0.027 ( 0.033)	Loss 3.4619e-01 (3.2762e-01)	Acc@1  87.00 ( 90.57)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.024 ( 0.030)	Loss 2.9565e-01 (3.3482e-01)	Acc@1  87.00 ( 90.58)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 3.6475e-01 (3.3580e-01)	Acc@1  89.00 ( 90.37)	Acc@5 100.00 ( 99.56)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 2.0105e-01 (3.3451e-01)	Acc@1  91.00 ( 90.27)	Acc@5  99.00 ( 99.61)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 1.5869e-01 (3.3008e-01)	Acc@1  94.00 ( 90.25)	Acc@5 100.00 ( 99.62)
Test: [ 70/100]	Time  0.026 ( 0.027)	Loss 3.8745e-01 (3.3176e-01)	Acc@1  88.00 ( 90.20)	Acc@5 100.00 ( 99.65)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 1.5320e-01 (3.3071e-01)	Acc@1  95.00 ( 90.22)	Acc@5 100.00 ( 99.64)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 2.8174e-01 (3.2956e-01)	Acc@1  89.00 ( 90.21)	Acc@5 100.00 ( 99.63)
 * Acc@1 90.230 Acc@5 99.620
### epoch[58] execution time: 28.48735737800598
EPOCH 59
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [59][  0/391]	Time  0.241 ( 0.241)	Data  0.171 ( 0.171)	Loss 9.7778e-02 (9.7778e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [59][ 10/391]	Time  0.064 ( 0.081)	Data  0.001 ( 0.016)	Loss 9.3872e-02 (1.2710e-01)	Acc@1  96.88 ( 95.53)	Acc@5 100.00 ( 99.86)
Epoch: [59][ 20/391]	Time  0.063 ( 0.073)	Data  0.001 ( 0.009)	Loss 1.9507e-01 (1.3686e-01)	Acc@1  92.19 ( 95.09)	Acc@5 100.00 ( 99.89)
Epoch: [59][ 30/391]	Time  0.066 ( 0.071)	Data  0.001 ( 0.006)	Loss 2.2180e-01 (1.3745e-01)	Acc@1  92.19 ( 94.93)	Acc@5 100.00 ( 99.90)
Epoch: [59][ 40/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.005)	Loss 6.1676e-02 (1.3757e-01)	Acc@1  97.66 ( 95.03)	Acc@5 100.00 ( 99.92)
Epoch: [59][ 50/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.004)	Loss 1.2695e-01 (1.3770e-01)	Acc@1  94.53 ( 94.91)	Acc@5 100.00 ( 99.94)
Epoch: [59][ 60/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.4856e-01 (1.3680e-01)	Acc@1  92.97 ( 95.04)	Acc@5 100.00 ( 99.95)
Epoch: [59][ 70/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 5.8899e-02 (1.3573e-01)	Acc@1  99.22 ( 95.11)	Acc@5 100.00 ( 99.96)
Epoch: [59][ 80/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.0583e-01 (1.3939e-01)	Acc@1  96.09 ( 95.05)	Acc@5 100.00 ( 99.91)
Epoch: [59][ 90/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.2683e-01 (1.3810e-01)	Acc@1  96.88 ( 95.13)	Acc@5 100.00 ( 99.92)
Epoch: [59][100/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.5503e-01 (1.3819e-01)	Acc@1  95.31 ( 95.13)	Acc@5  99.22 ( 99.92)
Epoch: [59][110/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 7.5562e-02 (1.3666e-01)	Acc@1  99.22 ( 95.20)	Acc@5 100.00 ( 99.93)
Epoch: [59][120/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2646e-01 (1.3389e-01)	Acc@1  96.09 ( 95.33)	Acc@5 100.00 ( 99.93)
Epoch: [59][130/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4282e-01 (1.3394e-01)	Acc@1  95.31 ( 95.36)	Acc@5 100.00 ( 99.93)
Epoch: [59][140/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.0750e-02 (1.3326e-01)	Acc@1  96.09 ( 95.36)	Acc@5 100.00 ( 99.93)
Epoch: [59][150/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9873e-01 (1.3262e-01)	Acc@1  93.75 ( 95.40)	Acc@5 100.00 ( 99.93)
Epoch: [59][160/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1615e-01 (1.3198e-01)	Acc@1  95.31 ( 95.40)	Acc@5 100.00 ( 99.94)
Epoch: [59][170/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.5632e-02 (1.3133e-01)	Acc@1  96.09 ( 95.44)	Acc@5 100.00 ( 99.94)
Epoch: [59][180/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5454e-01 (1.3158e-01)	Acc@1  96.09 ( 95.45)	Acc@5 100.00 ( 99.94)
Epoch: [59][190/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.7087e-02 (1.3092e-01)	Acc@1  96.09 ( 95.44)	Acc@5 100.00 ( 99.95)
Epoch: [59][200/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1700e-01 (1.2991e-01)	Acc@1  96.09 ( 95.48)	Acc@5 100.00 ( 99.95)
Epoch: [59][210/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8274e-01 (1.3101e-01)	Acc@1  92.19 ( 95.48)	Acc@5 100.00 ( 99.94)
Epoch: [59][220/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4282e-01 (1.3032e-01)	Acc@1  95.31 ( 95.47)	Acc@5 100.00 ( 99.94)
Epoch: [59][230/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.3914e-02 (1.2979e-01)	Acc@1  98.44 ( 95.52)	Acc@5 100.00 ( 99.94)
Epoch: [59][240/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.2876e-02 (1.2869e-01)	Acc@1  98.44 ( 95.55)	Acc@5 100.00 ( 99.94)
Epoch: [59][250/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4502e-01 (1.2948e-01)	Acc@1  96.09 ( 95.51)	Acc@5 100.00 ( 99.94)
Epoch: [59][260/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8408e-01 (1.2945e-01)	Acc@1  93.75 ( 95.53)	Acc@5 100.00 ( 99.94)
Epoch: [59][270/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.1106e-01 (1.2998e-01)	Acc@1  92.19 ( 95.48)	Acc@5 100.00 ( 99.95)
Epoch: [59][280/391]	Time  0.070 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2927e-01 (1.3039e-01)	Acc@1  96.09 ( 95.48)	Acc@5 100.00 ( 99.95)
Epoch: [59][290/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7285e-01 (1.3070e-01)	Acc@1  93.75 ( 95.45)	Acc@5 100.00 ( 99.95)
Epoch: [59][300/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1530e-01 (1.3127e-01)	Acc@1  97.66 ( 95.42)	Acc@5 100.00 ( 99.95)
Epoch: [59][310/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0870e-01 (1.3162e-01)	Acc@1  96.09 ( 95.41)	Acc@5 100.00 ( 99.95)
Epoch: [59][320/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.9702e-02 (1.3165e-01)	Acc@1  97.66 ( 95.40)	Acc@5 100.00 ( 99.95)
Epoch: [59][330/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8188e-01 (1.3264e-01)	Acc@1  96.09 ( 95.37)	Acc@5 100.00 ( 99.95)
Epoch: [59][340/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.4229e-02 (1.3282e-01)	Acc@1  96.88 ( 95.38)	Acc@5 100.00 ( 99.95)
Epoch: [59][350/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5283e-01 (1.3301e-01)	Acc@1  94.53 ( 95.36)	Acc@5 100.00 ( 99.95)
Epoch: [59][360/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8713e-01 (1.3279e-01)	Acc@1  92.97 ( 95.37)	Acc@5 100.00 ( 99.95)
Epoch: [59][370/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0669e-01 (1.3321e-01)	Acc@1  97.66 ( 95.36)	Acc@5 100.00 ( 99.95)
Epoch: [59][380/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5283e-01 (1.3353e-01)	Acc@1  95.31 ( 95.36)	Acc@5 100.00 ( 99.95)
Epoch: [59][390/391]	Time  0.046 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4465e-01 (1.3292e-01)	Acc@1  93.75 ( 95.36)	Acc@5 100.00 ( 99.95)
## e[59] optimizer.zero_grad (sum) time: 0.3966090679168701
## e[59]       loss.backward (sum) time: 7.263001203536987
## e[59]      optimizer.step (sum) time: 3.4801337718963623
## epoch[59] training(only) time: 25.700440168380737
# Switched to evaluate mode...
Test: [  0/100]	Time  0.190 ( 0.190)	Loss 2.7515e-01 (2.7515e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.032 ( 0.040)	Loss 3.9551e-01 (3.3444e-01)	Acc@1  89.00 ( 90.00)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 3.6719e-01 (3.3381e-01)	Acc@1  87.00 ( 90.00)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 3.4277e-01 (3.3021e-01)	Acc@1  87.00 ( 90.19)	Acc@5 100.00 ( 99.58)
Test: [ 40/100]	Time  0.027 ( 0.029)	Loss 4.2383e-01 (3.3642e-01)	Acc@1  89.00 ( 89.98)	Acc@5  98.00 ( 99.54)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 2.1411e-01 (3.3288e-01)	Acc@1  92.00 ( 89.96)	Acc@5 100.00 ( 99.63)
Test: [ 60/100]	Time  0.027 ( 0.028)	Loss 1.9971e-01 (3.2537e-01)	Acc@1  92.00 ( 90.13)	Acc@5 100.00 ( 99.64)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 4.1284e-01 (3.2498e-01)	Acc@1  87.00 ( 90.14)	Acc@5 100.00 ( 99.68)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 1.8591e-01 (3.2466e-01)	Acc@1  91.00 ( 90.16)	Acc@5 100.00 ( 99.68)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 2.4915e-01 (3.2399e-01)	Acc@1  87.00 ( 90.14)	Acc@5 100.00 ( 99.68)
 * Acc@1 90.210 Acc@5 99.690
### epoch[59] execution time: 28.50404953956604
EPOCH 60
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [60][  0/391]	Time  0.248 ( 0.248)	Data  0.181 ( 0.181)	Loss 1.0437e-01 (1.0437e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [60][ 10/391]	Time  0.069 ( 0.082)	Data  0.001 ( 0.017)	Loss 9.1370e-02 (1.0977e-01)	Acc@1  96.88 ( 96.80)	Acc@5 100.00 (100.00)
Epoch: [60][ 20/391]	Time  0.066 ( 0.074)	Data  0.001 ( 0.010)	Loss 8.0261e-02 (1.1793e-01)	Acc@1  97.66 ( 96.39)	Acc@5 100.00 ( 99.96)
Epoch: [60][ 30/391]	Time  0.071 ( 0.071)	Data  0.001 ( 0.007)	Loss 6.3782e-02 (1.2018e-01)	Acc@1  98.44 ( 95.94)	Acc@5 100.00 ( 99.97)
Epoch: [60][ 40/391]	Time  0.065 ( 0.070)	Data  0.001 ( 0.006)	Loss 8.5266e-02 (1.2244e-01)	Acc@1  97.66 ( 95.79)	Acc@5 100.00 ( 99.98)
Epoch: [60][ 50/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.005)	Loss 1.5417e-01 (1.2433e-01)	Acc@1  95.31 ( 95.79)	Acc@5 100.00 ( 99.98)
Epoch: [60][ 60/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 9.6863e-02 (1.2055e-01)	Acc@1  96.88 ( 95.90)	Acc@5 100.00 ( 99.99)
Epoch: [60][ 70/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.1627e-01 (1.1957e-01)	Acc@1  96.88 ( 95.91)	Acc@5 100.00 ( 99.99)
Epoch: [60][ 80/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.0321e-01 (1.1664e-01)	Acc@1  97.66 ( 95.98)	Acc@5 100.00 ( 99.99)
Epoch: [60][ 90/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.1102e-01 (1.1599e-01)	Acc@1  96.88 ( 95.98)	Acc@5 100.00 ( 99.99)
Epoch: [60][100/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 9.8938e-02 (1.1551e-01)	Acc@1  96.88 ( 95.99)	Acc@5 100.00 ( 99.99)
Epoch: [60][110/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.1615e-01 (1.1502e-01)	Acc@1  95.31 ( 96.02)	Acc@5 100.00 ( 99.99)
Epoch: [60][120/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.3794e-01 (1.1565e-01)	Acc@1  97.66 ( 96.02)	Acc@5  99.22 ( 99.98)
Epoch: [60][130/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.0629e-02 (1.1478e-01)	Acc@1  98.44 ( 96.05)	Acc@5 100.00 ( 99.98)
Epoch: [60][140/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0162e-01 (1.1533e-01)	Acc@1  96.88 ( 96.05)	Acc@5 100.00 ( 99.98)
Epoch: [60][150/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.1299e-02 (1.1475e-01)	Acc@1  97.66 ( 96.04)	Acc@5 100.00 ( 99.97)
Epoch: [60][160/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8066e-01 (1.1500e-01)	Acc@1  94.53 ( 96.01)	Acc@5 100.00 ( 99.97)
Epoch: [60][170/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4270e-01 (1.1521e-01)	Acc@1  96.09 ( 96.02)	Acc@5 100.00 ( 99.97)
Epoch: [60][180/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.9896e-02 (1.1529e-01)	Acc@1  98.44 ( 95.99)	Acc@5 100.00 ( 99.97)
Epoch: [60][190/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.0994e-02 (1.1608e-01)	Acc@1  96.88 ( 95.96)	Acc@5 100.00 ( 99.98)
Epoch: [60][200/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3074e-01 (1.1630e-01)	Acc@1  94.53 ( 95.92)	Acc@5 100.00 ( 99.98)
Epoch: [60][210/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.3364e-02 (1.1593e-01)	Acc@1  97.66 ( 95.94)	Acc@5 100.00 ( 99.97)
Epoch: [60][220/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6370e-01 (1.1617e-01)	Acc@1  95.31 ( 95.92)	Acc@5 100.00 ( 99.97)
Epoch: [60][230/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.7455e-02 (1.1574e-01)	Acc@1  99.22 ( 95.94)	Acc@5 100.00 ( 99.97)
Epoch: [60][240/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1359e-01 (1.1646e-01)	Acc@1  94.53 ( 95.95)	Acc@5 100.00 ( 99.97)
Epoch: [60][250/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.7515e-02 (1.1572e-01)	Acc@1  96.88 ( 95.96)	Acc@5 100.00 ( 99.97)
Epoch: [60][260/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0736e-01 (1.1619e-01)	Acc@1  96.09 ( 95.93)	Acc@5 100.00 ( 99.97)
Epoch: [60][270/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3977e-01 (1.1669e-01)	Acc@1  96.09 ( 95.90)	Acc@5 100.00 ( 99.97)
Epoch: [60][280/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9080e-01 (1.1705e-01)	Acc@1  93.75 ( 95.90)	Acc@5 100.00 ( 99.97)
Epoch: [60][290/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.8196e-02 (1.1681e-01)	Acc@1  96.88 ( 95.89)	Acc@5 100.00 ( 99.97)
Epoch: [60][300/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5125e-01 (1.1721e-01)	Acc@1  93.75 ( 95.88)	Acc@5 100.00 ( 99.96)
Epoch: [60][310/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.6975e-02 (1.1725e-01)	Acc@1  96.88 ( 95.88)	Acc@5 100.00 ( 99.96)
Epoch: [60][320/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5845e-01 (1.1722e-01)	Acc@1  94.53 ( 95.87)	Acc@5 100.00 ( 99.96)
Epoch: [60][330/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.7769e-02 (1.1707e-01)	Acc@1  95.31 ( 95.88)	Acc@5 100.00 ( 99.96)
Epoch: [60][340/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.0618e-02 (1.1679e-01)	Acc@1  98.44 ( 95.88)	Acc@5 100.00 ( 99.97)
Epoch: [60][350/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5845e-01 (1.1670e-01)	Acc@1  94.53 ( 95.86)	Acc@5 100.00 ( 99.97)
Epoch: [60][360/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1261e-01 (1.1658e-01)	Acc@1  94.53 ( 95.85)	Acc@5 100.00 ( 99.97)
Epoch: [60][370/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.9641e-02 (1.1676e-01)	Acc@1  98.44 ( 95.85)	Acc@5 100.00 ( 99.97)
Epoch: [60][380/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.9426e-02 (1.1668e-01)	Acc@1  96.09 ( 95.85)	Acc@5 100.00 ( 99.97)
Epoch: [60][390/391]	Time  0.047 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4465e-01 (1.1659e-01)	Acc@1  93.75 ( 95.86)	Acc@5 100.00 ( 99.97)
## e[60] optimizer.zero_grad (sum) time: 0.3876528739929199
## e[60]       loss.backward (sum) time: 7.250046730041504
## e[60]      optimizer.step (sum) time: 3.4776182174682617
## epoch[60] training(only) time: 25.633185863494873
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 2.7954e-01 (2.7954e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.040)	Loss 4.0869e-01 (3.3474e-01)	Acc@1  88.00 ( 89.36)	Acc@5  99.00 ( 99.82)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 3.8013e-01 (3.2917e-01)	Acc@1  87.00 ( 89.62)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 3.0957e-01 (3.2288e-01)	Acc@1  88.00 ( 89.94)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.027 ( 0.029)	Loss 4.0479e-01 (3.2849e-01)	Acc@1  89.00 ( 89.85)	Acc@5  99.00 ( 99.56)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 2.2888e-01 (3.2546e-01)	Acc@1  92.00 ( 89.88)	Acc@5 100.00 ( 99.63)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 1.8896e-01 (3.1809e-01)	Acc@1  94.00 ( 90.11)	Acc@5 100.00 ( 99.64)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 4.1333e-01 (3.1928e-01)	Acc@1  89.00 ( 90.08)	Acc@5 100.00 ( 99.68)
Test: [ 80/100]	Time  0.027 ( 0.028)	Loss 1.5283e-01 (3.1899e-01)	Acc@1  92.00 ( 90.14)	Acc@5 100.00 ( 99.68)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 2.1741e-01 (3.1748e-01)	Acc@1  90.00 ( 90.18)	Acc@5 100.00 ( 99.69)
 * Acc@1 90.270 Acc@5 99.690
### epoch[60] execution time: 28.452815055847168
EPOCH 61
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [61][  0/391]	Time  0.249 ( 0.249)	Data  0.184 ( 0.184)	Loss 8.9844e-02 (8.9844e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [61][ 10/391]	Time  0.064 ( 0.082)	Data  0.001 ( 0.018)	Loss 1.4551e-01 (1.2371e-01)	Acc@1  92.97 ( 95.45)	Acc@5 100.00 (100.00)
Epoch: [61][ 20/391]	Time  0.068 ( 0.074)	Data  0.001 ( 0.010)	Loss 2.1240e-01 (1.2439e-01)	Acc@1  91.41 ( 95.54)	Acc@5 100.00 (100.00)
Epoch: [61][ 30/391]	Time  0.063 ( 0.071)	Data  0.001 ( 0.007)	Loss 1.3293e-01 (1.1945e-01)	Acc@1  94.53 ( 95.69)	Acc@5 100.00 (100.00)
Epoch: [61][ 40/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.006)	Loss 9.4910e-02 (1.2094e-01)	Acc@1  96.88 ( 95.58)	Acc@5 100.00 (100.00)
Epoch: [61][ 50/391]	Time  0.060 ( 0.068)	Data  0.001 ( 0.005)	Loss 9.6313e-02 (1.1810e-01)	Acc@1  96.09 ( 95.62)	Acc@5 100.00 (100.00)
Epoch: [61][ 60/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.004)	Loss 7.9895e-02 (1.1625e-01)	Acc@1  97.66 ( 95.76)	Acc@5 100.00 ( 99.97)
Epoch: [61][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.4673e-01 (1.1905e-01)	Acc@1  93.75 ( 95.71)	Acc@5 100.00 ( 99.98)
Epoch: [61][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.9348e-01 (1.2046e-01)	Acc@1  93.75 ( 95.62)	Acc@5 100.00 ( 99.98)
Epoch: [61][ 90/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.1078e-01 (1.2093e-01)	Acc@1  96.88 ( 95.61)	Acc@5  99.22 ( 99.97)
Epoch: [61][100/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.2073e-01 (1.2031e-01)	Acc@1  95.31 ( 95.66)	Acc@5 100.00 ( 99.97)
Epoch: [61][110/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.6223e-01 (1.2095e-01)	Acc@1  96.09 ( 95.62)	Acc@5 100.00 ( 99.97)
Epoch: [61][120/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.0864e-01 (1.1998e-01)	Acc@1  95.31 ( 95.65)	Acc@5 100.00 ( 99.97)
Epoch: [61][130/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.9102e-02 (1.2054e-01)	Acc@1  99.22 ( 95.65)	Acc@5 100.00 ( 99.98)
Epoch: [61][140/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.3059e-02 (1.2101e-01)	Acc@1  96.09 ( 95.66)	Acc@5 100.00 ( 99.97)
Epoch: [61][150/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1469e-01 (1.2134e-01)	Acc@1  96.09 ( 95.69)	Acc@5 100.00 ( 99.96)
Epoch: [61][160/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.7129e-02 (1.1952e-01)	Acc@1  98.44 ( 95.78)	Acc@5 100.00 ( 99.97)
Epoch: [61][170/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.4482e-02 (1.1885e-01)	Acc@1  96.88 ( 95.81)	Acc@5 100.00 ( 99.97)
Epoch: [61][180/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1761e-01 (1.1857e-01)	Acc@1  94.53 ( 95.83)	Acc@5 100.00 ( 99.97)
Epoch: [61][190/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.7892e-02 (1.1684e-01)	Acc@1  99.22 ( 95.89)	Acc@5 100.00 ( 99.97)
Epoch: [61][200/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1517e-01 (1.1579e-01)	Acc@1  96.88 ( 95.94)	Acc@5 100.00 ( 99.97)
Epoch: [61][210/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0187e-01 (1.1546e-01)	Acc@1  96.09 ( 95.94)	Acc@5 100.00 ( 99.97)
Epoch: [61][220/391]	Time  0.073 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.4116e-02 (1.1501e-01)	Acc@1  96.88 ( 95.98)	Acc@5 100.00 ( 99.97)
Epoch: [61][230/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.8258e-02 (1.1504e-01)	Acc@1  96.88 ( 95.98)	Acc@5 100.00 ( 99.97)
Epoch: [61][240/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8457e-01 (1.1510e-01)	Acc@1  89.84 ( 95.97)	Acc@5 100.00 ( 99.97)
Epoch: [61][250/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1499e-01 (1.1421e-01)	Acc@1  95.31 ( 96.01)	Acc@5 100.00 ( 99.98)
Epoch: [61][260/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2720e-01 (1.1450e-01)	Acc@1  96.09 ( 96.01)	Acc@5  99.22 ( 99.97)
Epoch: [61][270/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.2632e-02 (1.1451e-01)	Acc@1  96.88 ( 96.00)	Acc@5 100.00 ( 99.97)
Epoch: [61][280/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.9225e-02 (1.1383e-01)	Acc@1 100.00 ( 96.02)	Acc@5 100.00 ( 99.97)
Epoch: [61][290/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4111e-01 (1.1396e-01)	Acc@1  92.97 ( 96.00)	Acc@5 100.00 ( 99.97)
Epoch: [61][300/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3208e-01 (1.1404e-01)	Acc@1  95.31 ( 95.99)	Acc@5 100.00 ( 99.97)
Epoch: [61][310/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3379e-01 (1.1465e-01)	Acc@1  96.09 ( 95.97)	Acc@5 100.00 ( 99.97)
Epoch: [61][320/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2091e-01 (1.1471e-01)	Acc@1  95.31 ( 95.96)	Acc@5 100.00 ( 99.97)
Epoch: [61][330/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5527e-01 (1.1481e-01)	Acc@1  94.53 ( 95.95)	Acc@5 100.00 ( 99.97)
Epoch: [61][340/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0175e-01 (1.1500e-01)	Acc@1  96.88 ( 95.93)	Acc@5 100.00 ( 99.97)
Epoch: [61][350/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3733e-01 (1.1494e-01)	Acc@1  94.53 ( 95.94)	Acc@5 100.00 ( 99.97)
Epoch: [61][360/391]	Time  0.078 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3806e-01 (1.1505e-01)	Acc@1  96.09 ( 95.93)	Acc@5  99.22 ( 99.97)
Epoch: [61][370/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.2957e-02 (1.1473e-01)	Acc@1  96.88 ( 95.94)	Acc@5 100.00 ( 99.97)
Epoch: [61][380/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0272e-01 (1.1484e-01)	Acc@1  96.88 ( 95.95)	Acc@5 100.00 ( 99.97)
Epoch: [61][390/391]	Time  0.048 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.7769e-02 (1.1506e-01)	Acc@1  98.75 ( 95.95)	Acc@5 100.00 ( 99.97)
## e[61] optimizer.zero_grad (sum) time: 0.3890547752380371
## e[61]       loss.backward (sum) time: 7.25071120262146
## e[61]      optimizer.step (sum) time: 3.4611120223999023
## epoch[61] training(only) time: 25.556628942489624
# Switched to evaluate mode...
Test: [  0/100]	Time  0.190 ( 0.190)	Loss 2.7808e-01 (2.7808e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.040)	Loss 3.6353e-01 (3.3059e-01)	Acc@1  89.00 ( 89.73)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.024 ( 0.034)	Loss 3.5400e-01 (3.2406e-01)	Acc@1  86.00 ( 89.71)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 2.9224e-01 (3.1968e-01)	Acc@1  88.00 ( 90.10)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.027 ( 0.029)	Loss 4.0479e-01 (3.2578e-01)	Acc@1  89.00 ( 89.90)	Acc@5  99.00 ( 99.56)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 2.1753e-01 (3.2236e-01)	Acc@1  93.00 ( 89.98)	Acc@5 100.00 ( 99.63)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 1.7627e-01 (3.1458e-01)	Acc@1  95.00 ( 90.30)	Acc@5 100.00 ( 99.64)
Test: [ 70/100]	Time  0.027 ( 0.028)	Loss 3.9575e-01 (3.1506e-01)	Acc@1  89.00 ( 90.23)	Acc@5 100.00 ( 99.68)
Test: [ 80/100]	Time  0.027 ( 0.027)	Loss 1.4612e-01 (3.1539e-01)	Acc@1  94.00 ( 90.30)	Acc@5 100.00 ( 99.68)
Test: [ 90/100]	Time  0.027 ( 0.027)	Loss 2.2522e-01 (3.1436e-01)	Acc@1  89.00 ( 90.30)	Acc@5 100.00 ( 99.68)
 * Acc@1 90.360 Acc@5 99.700
### epoch[61] execution time: 28.345738410949707
EPOCH 62
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [62][  0/391]	Time  0.239 ( 0.239)	Data  0.173 ( 0.173)	Loss 8.3923e-02 (8.3923e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [62][ 10/391]	Time  0.062 ( 0.080)	Data  0.001 ( 0.017)	Loss 1.4160e-01 (1.0700e-01)	Acc@1  94.53 ( 96.31)	Acc@5 100.00 (100.00)
Epoch: [62][ 20/391]	Time  0.067 ( 0.073)	Data  0.001 ( 0.009)	Loss 1.1621e-01 (1.1058e-01)	Acc@1  96.88 ( 96.32)	Acc@5  99.22 ( 99.96)
Epoch: [62][ 30/391]	Time  0.066 ( 0.070)	Data  0.001 ( 0.007)	Loss 1.3794e-01 (1.0831e-01)	Acc@1  94.53 ( 96.24)	Acc@5 100.00 ( 99.97)
Epoch: [62][ 40/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.005)	Loss 1.2073e-01 (1.0810e-01)	Acc@1  94.53 ( 96.21)	Acc@5 100.00 ( 99.98)
Epoch: [62][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.0101e-01 (1.0705e-01)	Acc@1  95.31 ( 96.19)	Acc@5 100.00 ( 99.98)
Epoch: [62][ 60/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.9971e-01 (1.0783e-01)	Acc@1  92.19 ( 96.09)	Acc@5  99.22 ( 99.97)
Epoch: [62][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.0852e-01 (1.0675e-01)	Acc@1  96.09 ( 96.17)	Acc@5 100.00 ( 99.98)
Epoch: [62][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.8079e-01 (1.0642e-01)	Acc@1  93.75 ( 96.23)	Acc@5 100.00 ( 99.98)
Epoch: [62][ 90/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 7.4402e-02 (1.0692e-01)	Acc@1  97.66 ( 96.20)	Acc@5 100.00 ( 99.97)
Epoch: [62][100/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.003)	Loss 9.9731e-02 (1.0778e-01)	Acc@1  94.53 ( 96.12)	Acc@5 100.00 ( 99.98)
Epoch: [62][110/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.0651e-01 (1.0668e-01)	Acc@1  97.66 ( 96.19)	Acc@5 100.00 ( 99.98)
Epoch: [62][120/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 6.9580e-02 (1.0690e-01)	Acc@1  97.66 ( 96.17)	Acc@5 100.00 ( 99.98)
Epoch: [62][130/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.9103e-02 (1.0511e-01)	Acc@1  99.22 ( 96.28)	Acc@5 100.00 ( 99.98)
Epoch: [62][140/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.4260e-02 (1.0387e-01)	Acc@1  98.44 ( 96.37)	Acc@5 100.00 ( 99.98)
Epoch: [62][150/391]	Time  0.060 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.0383e-02 (1.0543e-01)	Acc@1  97.66 ( 96.27)	Acc@5 100.00 ( 99.98)
Epoch: [62][160/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.7017e-02 (1.0385e-01)	Acc@1  97.66 ( 96.33)	Acc@5 100.00 ( 99.98)
Epoch: [62][170/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2195e-01 (1.0491e-01)	Acc@1  96.09 ( 96.34)	Acc@5 100.00 ( 99.98)
Epoch: [62][180/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.8135e-02 (1.0469e-01)	Acc@1  97.66 ( 96.36)	Acc@5 100.00 ( 99.98)
Epoch: [62][190/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1249e-01 (1.0637e-01)	Acc@1  95.31 ( 96.28)	Acc@5 100.00 ( 99.98)
Epoch: [62][200/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7749e-01 (1.0705e-01)	Acc@1  92.97 ( 96.26)	Acc@5 100.00 ( 99.98)
Epoch: [62][210/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0114e-01 (1.0725e-01)	Acc@1  96.88 ( 96.26)	Acc@5 100.00 ( 99.98)
Epoch: [62][220/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.8745e-02 (1.0726e-01)	Acc@1  97.66 ( 96.26)	Acc@5 100.00 ( 99.98)
Epoch: [62][230/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2415e-01 (1.0754e-01)	Acc@1  94.53 ( 96.23)	Acc@5 100.00 ( 99.97)
Epoch: [62][240/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.1707e-02 (1.0659e-01)	Acc@1  97.66 ( 96.28)	Acc@5 100.00 ( 99.97)
Epoch: [62][250/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1328e-01 (1.0632e-01)	Acc@1  96.88 ( 96.29)	Acc@5 100.00 ( 99.98)
Epoch: [62][260/391]	Time  0.070 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5833e-01 (1.0619e-01)	Acc@1  93.75 ( 96.28)	Acc@5 100.00 ( 99.98)
Epoch: [62][270/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1304e-01 (1.0646e-01)	Acc@1  97.66 ( 96.27)	Acc@5 100.00 ( 99.98)
Epoch: [62][280/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0181e-01 (1.0733e-01)	Acc@1  96.09 ( 96.24)	Acc@5 100.00 ( 99.97)
Epoch: [62][290/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4990e-01 (1.0766e-01)	Acc@1  94.53 ( 96.21)	Acc@5 100.00 ( 99.98)
Epoch: [62][300/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4929e-01 (1.0777e-01)	Acc@1  93.75 ( 96.22)	Acc@5 100.00 ( 99.97)
Epoch: [62][310/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0858e-01 (1.0802e-01)	Acc@1  93.75 ( 96.20)	Acc@5 100.00 ( 99.97)
Epoch: [62][320/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.0515e-02 (1.0870e-01)	Acc@1  95.31 ( 96.18)	Acc@5 100.00 ( 99.97)
Epoch: [62][330/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6138e-01 (1.0845e-01)	Acc@1  96.09 ( 96.19)	Acc@5 100.00 ( 99.97)
Epoch: [62][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.4901e-02 (1.0847e-01)	Acc@1  98.44 ( 96.18)	Acc@5 100.00 ( 99.97)
Epoch: [62][350/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.1543e-02 (1.0827e-01)	Acc@1  96.88 ( 96.18)	Acc@5 100.00 ( 99.98)
Epoch: [62][360/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2268e-01 (1.0881e-01)	Acc@1  93.75 ( 96.17)	Acc@5 100.00 ( 99.98)
Epoch: [62][370/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6150e-01 (1.0921e-01)	Acc@1  92.97 ( 96.15)	Acc@5 100.00 ( 99.97)
Epoch: [62][380/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.6252e-02 (1.0975e-01)	Acc@1  96.88 ( 96.13)	Acc@5 100.00 ( 99.97)
Epoch: [62][390/391]	Time  0.047 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1997e-01 (1.0987e-01)	Acc@1  93.75 ( 96.14)	Acc@5 100.00 ( 99.97)
## e[62] optimizer.zero_grad (sum) time: 0.3922007083892822
## e[62]       loss.backward (sum) time: 7.2144434452056885
## e[62]      optimizer.step (sum) time: 3.4669291973114014
## epoch[62] training(only) time: 25.6014666557312
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 2.8345e-01 (2.8345e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.026 ( 0.040)	Loss 4.0820e-01 (3.3456e-01)	Acc@1  90.00 ( 90.18)	Acc@5  99.00 ( 99.82)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 3.5596e-01 (3.2454e-01)	Acc@1  86.00 ( 90.10)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.028 ( 0.031)	Loss 2.7856e-01 (3.1973e-01)	Acc@1  88.00 ( 90.23)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.027 ( 0.029)	Loss 3.8672e-01 (3.2288e-01)	Acc@1  89.00 ( 90.10)	Acc@5  99.00 ( 99.61)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 2.1692e-01 (3.2073e-01)	Acc@1  93.00 ( 90.14)	Acc@5 100.00 ( 99.65)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 1.9067e-01 (3.1360e-01)	Acc@1  95.00 ( 90.41)	Acc@5 100.00 ( 99.67)
Test: [ 70/100]	Time  0.024 ( 0.027)	Loss 4.4653e-01 (3.1516e-01)	Acc@1  90.00 ( 90.38)	Acc@5 100.00 ( 99.70)
Test: [ 80/100]	Time  0.027 ( 0.027)	Loss 1.3660e-01 (3.1516e-01)	Acc@1  94.00 ( 90.36)	Acc@5 100.00 ( 99.72)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 2.3633e-01 (3.1376e-01)	Acc@1  89.00 ( 90.41)	Acc@5 100.00 ( 99.71)
 * Acc@1 90.470 Acc@5 99.710
### epoch[62] execution time: 28.373931169509888
EPOCH 63
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [63][  0/391]	Time  0.252 ( 0.252)	Data  0.188 ( 0.188)	Loss 1.3428e-01 (1.3428e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
Epoch: [63][ 10/391]	Time  0.067 ( 0.082)	Data  0.001 ( 0.018)	Loss 1.4087e-01 (1.1365e-01)	Acc@1  94.53 ( 95.74)	Acc@5 100.00 (100.00)
Epoch: [63][ 20/391]	Time  0.065 ( 0.073)	Data  0.001 ( 0.010)	Loss 1.1292e-01 (1.1011e-01)	Acc@1  95.31 ( 95.76)	Acc@5 100.00 (100.00)
Epoch: [63][ 30/391]	Time  0.064 ( 0.071)	Data  0.001 ( 0.007)	Loss 1.6711e-01 (1.1144e-01)	Acc@1  94.53 ( 95.77)	Acc@5 100.00 (100.00)
Epoch: [63][ 40/391]	Time  0.063 ( 0.069)	Data  0.001 ( 0.006)	Loss 1.4893e-01 (1.1507e-01)	Acc@1  95.31 ( 95.83)	Acc@5 100.00 (100.00)
Epoch: [63][ 50/391]	Time  0.061 ( 0.068)	Data  0.001 ( 0.005)	Loss 1.1676e-01 (1.1354e-01)	Acc@1  96.88 ( 95.93)	Acc@5 100.00 ( 99.98)
Epoch: [63][ 60/391]	Time  0.062 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.3135e-01 (1.1385e-01)	Acc@1  93.75 ( 95.81)	Acc@5 100.00 ( 99.99)
Epoch: [63][ 70/391]	Time  0.061 ( 0.067)	Data  0.001 ( 0.004)	Loss 7.8308e-02 (1.1165e-01)	Acc@1  98.44 ( 95.96)	Acc@5 100.00 ( 99.98)
Epoch: [63][ 80/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 6.9336e-02 (1.0998e-01)	Acc@1  97.66 ( 96.05)	Acc@5 100.00 ( 99.98)
Epoch: [63][ 90/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.003)	Loss 6.6895e-02 (1.1153e-01)	Acc@1  98.44 ( 95.97)	Acc@5 100.00 ( 99.98)
Epoch: [63][100/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 9.7717e-02 (1.1080e-01)	Acc@1  94.53 ( 96.00)	Acc@5 100.00 ( 99.98)
Epoch: [63][110/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.003)	Loss 6.1737e-02 (1.1216e-01)	Acc@1  97.66 ( 95.97)	Acc@5 100.00 ( 99.98)
Epoch: [63][120/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.0608e-01 (1.1238e-01)	Acc@1  96.88 ( 95.95)	Acc@5 100.00 ( 99.98)
Epoch: [63][130/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5039e-01 (1.1362e-01)	Acc@1  95.31 ( 95.93)	Acc@5 100.00 ( 99.98)
Epoch: [63][140/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.3801e-02 (1.1509e-01)	Acc@1  96.09 ( 95.92)	Acc@5 100.00 ( 99.97)
Epoch: [63][150/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.0699e-02 (1.1417e-01)	Acc@1  97.66 ( 95.99)	Acc@5 100.00 ( 99.97)
Epoch: [63][160/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.0659e-02 (1.1373e-01)	Acc@1  98.44 ( 96.01)	Acc@5 100.00 ( 99.97)
Epoch: [63][170/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.8501e-02 (1.1235e-01)	Acc@1  96.09 ( 96.06)	Acc@5 100.00 ( 99.97)
Epoch: [63][180/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0620e-01 (1.1163e-01)	Acc@1  96.88 ( 96.06)	Acc@5 100.00 ( 99.97)
Epoch: [63][190/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0132e-01 (1.1349e-01)	Acc@1  96.88 ( 96.01)	Acc@5 100.00 ( 99.98)
Epoch: [63][200/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3208e-01 (1.1257e-01)	Acc@1  95.31 ( 96.02)	Acc@5 100.00 ( 99.98)
Epoch: [63][210/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.5378e-02 (1.1270e-01)	Acc@1  96.88 ( 96.01)	Acc@5 100.00 ( 99.97)
Epoch: [63][220/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.7961e-02 (1.1201e-01)	Acc@1  97.66 ( 96.05)	Acc@5 100.00 ( 99.97)
Epoch: [63][230/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3391e-01 (1.1151e-01)	Acc@1  96.88 ( 96.10)	Acc@5 100.00 ( 99.97)
Epoch: [63][240/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.2163e-02 (1.1112e-01)	Acc@1  96.88 ( 96.10)	Acc@5 100.00 ( 99.97)
Epoch: [63][250/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.7036e-02 (1.1141e-01)	Acc@1  95.31 ( 96.09)	Acc@5 100.00 ( 99.97)
Epoch: [63][260/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.4087e-02 (1.1119e-01)	Acc@1  96.88 ( 96.10)	Acc@5 100.00 ( 99.97)
Epoch: [63][270/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0132e-01 (1.1144e-01)	Acc@1  96.88 ( 96.10)	Acc@5 100.00 ( 99.97)
Epoch: [63][280/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0400e-01 (1.1150e-01)	Acc@1  97.66 ( 96.08)	Acc@5 100.00 ( 99.96)
Epoch: [63][290/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.0862e-02 (1.1177e-01)	Acc@1  97.66 ( 96.06)	Acc@5 100.00 ( 99.97)
Epoch: [63][300/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.3740e-02 (1.1147e-01)	Acc@1  97.66 ( 96.09)	Acc@5 100.00 ( 99.97)
Epoch: [63][310/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.5022e-02 (1.1138e-01)	Acc@1  97.66 ( 96.10)	Acc@5 100.00 ( 99.97)
Epoch: [63][320/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.8633e-02 (1.1087e-01)	Acc@1  96.88 ( 96.11)	Acc@5 100.00 ( 99.97)
Epoch: [63][330/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3013e-01 (1.1129e-01)	Acc@1  94.53 ( 96.08)	Acc@5  99.22 ( 99.96)
Epoch: [63][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.6843e-02 (1.1159e-01)	Acc@1  97.66 ( 96.08)	Acc@5 100.00 ( 99.96)
Epoch: [63][350/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.8206e-02 (1.1181e-01)	Acc@1  95.31 ( 96.07)	Acc@5 100.00 ( 99.96)
Epoch: [63][360/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1871e-01 (1.1144e-01)	Acc@1  95.31 ( 96.08)	Acc@5  99.22 ( 99.96)
Epoch: [63][370/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.3872e-02 (1.1178e-01)	Acc@1  96.88 ( 96.07)	Acc@5 100.00 ( 99.96)
Epoch: [63][380/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.8806e-02 (1.1102e-01)	Acc@1  96.09 ( 96.10)	Acc@5 100.00 ( 99.96)
Epoch: [63][390/391]	Time  0.046 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5625e-01 (1.1093e-01)	Acc@1  95.00 ( 96.11)	Acc@5 100.00 ( 99.96)
## e[63] optimizer.zero_grad (sum) time: 0.39302492141723633
## e[63]       loss.backward (sum) time: 7.236270427703857
## e[63]      optimizer.step (sum) time: 3.4343461990356445
## epoch[63] training(only) time: 25.575429439544678
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 2.6831e-01 (2.6831e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.039)	Loss 3.9697e-01 (3.3677e-01)	Acc@1  89.00 ( 89.82)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 3.6084e-01 (3.2568e-01)	Acc@1  85.00 ( 89.81)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.027 ( 0.030)	Loss 2.7930e-01 (3.2188e-01)	Acc@1  88.00 ( 90.19)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.027 ( 0.029)	Loss 3.9502e-01 (3.2551e-01)	Acc@1  89.00 ( 90.22)	Acc@5  99.00 ( 99.59)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 2.2327e-01 (3.2451e-01)	Acc@1  92.00 ( 90.20)	Acc@5 100.00 ( 99.65)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 1.9226e-01 (3.1715e-01)	Acc@1  94.00 ( 90.46)	Acc@5 100.00 ( 99.66)
Test: [ 70/100]	Time  0.027 ( 0.027)	Loss 4.3799e-01 (3.1776e-01)	Acc@1  89.00 ( 90.42)	Acc@5 100.00 ( 99.69)
Test: [ 80/100]	Time  0.028 ( 0.027)	Loss 1.3794e-01 (3.1793e-01)	Acc@1  94.00 ( 90.38)	Acc@5 100.00 ( 99.68)
Test: [ 90/100]	Time  0.027 ( 0.027)	Loss 2.4365e-01 (3.1683e-01)	Acc@1  89.00 ( 90.46)	Acc@5 100.00 ( 99.68)
 * Acc@1 90.490 Acc@5 99.680
### epoch[63] execution time: 28.396429777145386
EPOCH 64
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [64][  0/391]	Time  0.244 ( 0.244)	Data  0.182 ( 0.182)	Loss 1.3501e-01 (1.3501e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
Epoch: [64][ 10/391]	Time  0.066 ( 0.081)	Data  0.001 ( 0.017)	Loss 1.1914e-01 (1.1348e-01)	Acc@1  98.44 ( 96.66)	Acc@5 100.00 (100.00)
Epoch: [64][ 20/391]	Time  0.064 ( 0.073)	Data  0.001 ( 0.010)	Loss 9.2468e-02 (1.0757e-01)	Acc@1  96.09 ( 96.35)	Acc@5 100.00 (100.00)
Epoch: [64][ 30/391]	Time  0.065 ( 0.070)	Data  0.001 ( 0.007)	Loss 5.0507e-02 (1.1067e-01)	Acc@1  98.44 ( 96.19)	Acc@5 100.00 ( 99.97)
Epoch: [64][ 40/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.005)	Loss 1.2830e-01 (1.0970e-01)	Acc@1  95.31 ( 96.19)	Acc@5  99.22 ( 99.96)
Epoch: [64][ 50/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.005)	Loss 1.2201e-01 (1.1294e-01)	Acc@1  96.09 ( 96.00)	Acc@5 100.00 ( 99.95)
Epoch: [64][ 60/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.004)	Loss 2.5928e-01 (1.1754e-01)	Acc@1  89.84 ( 95.75)	Acc@5 100.00 ( 99.96)
Epoch: [64][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.4441e-01 (1.1641e-01)	Acc@1  93.75 ( 95.83)	Acc@5 100.00 ( 99.96)
Epoch: [64][ 80/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.4319e-01 (1.1445e-01)	Acc@1  93.75 ( 95.92)	Acc@5 100.00 ( 99.95)
Epoch: [64][ 90/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 7.8857e-02 (1.1421e-01)	Acc@1  97.66 ( 95.92)	Acc@5 100.00 ( 99.96)
Epoch: [64][100/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.3428e-01 (1.1380e-01)	Acc@1  95.31 ( 95.95)	Acc@5 100.00 ( 99.96)
Epoch: [64][110/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.2421e-01 (1.1352e-01)	Acc@1  96.09 ( 95.95)	Acc@5 100.00 ( 99.96)
Epoch: [64][120/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.4891e-02 (1.1454e-01)	Acc@1  99.22 ( 95.95)	Acc@5 100.00 ( 99.96)
Epoch: [64][130/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.0251e-02 (1.1331e-01)	Acc@1  98.44 ( 95.99)	Acc@5 100.00 ( 99.96)
Epoch: [64][140/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4636e-01 (1.1381e-01)	Acc@1  94.53 ( 95.98)	Acc@5 100.00 ( 99.96)
Epoch: [64][150/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.8694e-02 (1.1412e-01)	Acc@1  96.88 ( 96.00)	Acc@5 100.00 ( 99.96)
Epoch: [64][160/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0065e-01 (1.1425e-01)	Acc@1  95.31 ( 95.95)	Acc@5 100.00 ( 99.96)
Epoch: [64][170/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.6008e-02 (1.1342e-01)	Acc@1  96.88 ( 95.97)	Acc@5 100.00 ( 99.96)
Epoch: [64][180/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5149e-01 (1.1343e-01)	Acc@1  95.31 ( 95.96)	Acc@5 100.00 ( 99.97)
Epoch: [64][190/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6882e-01 (1.1287e-01)	Acc@1  92.19 ( 95.98)	Acc@5 100.00 ( 99.97)
Epoch: [64][200/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.0942e-02 (1.1202e-01)	Acc@1  96.88 ( 96.02)	Acc@5 100.00 ( 99.97)
Epoch: [64][210/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5283e-01 (1.1218e-01)	Acc@1  94.53 ( 96.00)	Acc@5 100.00 ( 99.97)
Epoch: [64][220/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4136e-01 (1.1313e-01)	Acc@1  93.75 ( 95.98)	Acc@5 100.00 ( 99.96)
Epoch: [64][230/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1938e-01 (1.1274e-01)	Acc@1  94.53 ( 95.97)	Acc@5 100.00 ( 99.96)
Epoch: [64][240/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3940e-01 (1.1272e-01)	Acc@1  96.09 ( 95.99)	Acc@5 100.00 ( 99.96)
Epoch: [64][250/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.4402e-02 (1.1280e-01)	Acc@1  97.66 ( 95.99)	Acc@5 100.00 ( 99.97)
Epoch: [64][260/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2878e-01 (1.1248e-01)	Acc@1  93.75 ( 96.02)	Acc@5 100.00 ( 99.97)
Epoch: [64][270/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1627e-01 (1.1300e-01)	Acc@1  93.75 ( 95.98)	Acc@5 100.00 ( 99.97)
Epoch: [64][280/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.4819e-02 (1.1186e-01)	Acc@1  97.66 ( 96.04)	Acc@5 100.00 ( 99.97)
Epoch: [64][290/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1243e-01 (1.1119e-01)	Acc@1  96.88 ( 96.06)	Acc@5 100.00 ( 99.97)
Epoch: [64][300/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.8450e-02 (1.1082e-01)	Acc@1  97.66 ( 96.08)	Acc@5 100.00 ( 99.97)
Epoch: [64][310/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4343e-01 (1.1107e-01)	Acc@1  94.53 ( 96.07)	Acc@5 100.00 ( 99.97)
Epoch: [64][320/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.3049e-02 (1.1015e-01)	Acc@1  97.66 ( 96.11)	Acc@5 100.00 ( 99.97)
Epoch: [64][330/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1072e-01 (1.1021e-01)	Acc@1  96.09 ( 96.12)	Acc@5 100.00 ( 99.97)
Epoch: [64][340/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3367e-01 (1.0972e-01)	Acc@1  95.31 ( 96.12)	Acc@5 100.00 ( 99.97)
Epoch: [64][350/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0541e-01 (1.0983e-01)	Acc@1  94.53 ( 96.10)	Acc@5 100.00 ( 99.97)
Epoch: [64][360/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.9692e-02 (1.0929e-01)	Acc@1  98.44 ( 96.12)	Acc@5 100.00 ( 99.97)
Epoch: [64][370/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0406e-01 (1.0913e-01)	Acc@1  94.53 ( 96.14)	Acc@5 100.00 ( 99.97)
Epoch: [64][380/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.9885e-02 (1.0947e-01)	Acc@1  97.66 ( 96.13)	Acc@5 100.00 ( 99.98)
Epoch: [64][390/391]	Time  0.048 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.7075e-01 (1.0959e-01)	Acc@1  86.25 ( 96.11)	Acc@5 100.00 ( 99.98)
## e[64] optimizer.zero_grad (sum) time: 0.3823239803314209
## e[64]       loss.backward (sum) time: 7.229184150695801
## e[64]      optimizer.step (sum) time: 3.453275442123413
## epoch[64] training(only) time: 25.539004802703857
# Switched to evaluate mode...
Test: [  0/100]	Time  0.192 ( 0.192)	Loss 2.9565e-01 (2.9565e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.041)	Loss 3.8428e-01 (3.2972e-01)	Acc@1  89.00 ( 90.00)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 3.4937e-01 (3.2303e-01)	Acc@1  86.00 ( 89.90)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.027 ( 0.031)	Loss 2.7490e-01 (3.1956e-01)	Acc@1  87.00 ( 90.16)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 3.9844e-01 (3.2357e-01)	Acc@1  89.00 ( 90.20)	Acc@5  99.00 ( 99.61)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 2.1472e-01 (3.2123e-01)	Acc@1  91.00 ( 90.16)	Acc@5 100.00 ( 99.65)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 1.8774e-01 (3.1346e-01)	Acc@1  94.00 ( 90.43)	Acc@5 100.00 ( 99.66)
Test: [ 70/100]	Time  0.024 ( 0.027)	Loss 4.1992e-01 (3.1443e-01)	Acc@1  87.00 ( 90.30)	Acc@5 100.00 ( 99.69)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 1.4355e-01 (3.1515e-01)	Acc@1  95.00 ( 90.32)	Acc@5 100.00 ( 99.69)
Test: [ 90/100]	Time  0.025 ( 0.027)	Loss 2.4365e-01 (3.1383e-01)	Acc@1  89.00 ( 90.41)	Acc@5 100.00 ( 99.69)
 * Acc@1 90.470 Acc@5 99.700
### epoch[64] execution time: 28.312711000442505
EPOCH 65
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [65][  0/391]	Time  0.244 ( 0.244)	Data  0.181 ( 0.181)	Loss 1.7224e-01 (1.7224e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
Epoch: [65][ 10/391]	Time  0.063 ( 0.081)	Data  0.001 ( 0.017)	Loss 1.4917e-01 (1.1645e-01)	Acc@1  95.31 ( 95.88)	Acc@5 100.00 (100.00)
Epoch: [65][ 20/391]	Time  0.065 ( 0.073)	Data  0.001 ( 0.010)	Loss 1.4075e-01 (1.1257e-01)	Acc@1  95.31 ( 95.98)	Acc@5 100.00 ( 99.89)
Epoch: [65][ 30/391]	Time  0.065 ( 0.070)	Data  0.001 ( 0.007)	Loss 9.4849e-02 (1.1260e-01)	Acc@1  95.31 ( 95.82)	Acc@5 100.00 ( 99.92)
Epoch: [65][ 40/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.005)	Loss 1.2494e-01 (1.1238e-01)	Acc@1  96.09 ( 95.94)	Acc@5 100.00 ( 99.94)
Epoch: [65][ 50/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.005)	Loss 1.5039e-01 (1.1221e-01)	Acc@1  92.97 ( 95.96)	Acc@5 100.00 ( 99.95)
Epoch: [65][ 60/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.6321e-01 (1.1276e-01)	Acc@1  95.31 ( 95.95)	Acc@5 100.00 ( 99.96)
Epoch: [65][ 70/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.1755e-01 (1.1207e-01)	Acc@1  94.53 ( 95.93)	Acc@5 100.00 ( 99.97)
Epoch: [65][ 80/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.6040e-01 (1.1159e-01)	Acc@1  93.75 ( 95.94)	Acc@5 100.00 ( 99.97)
Epoch: [65][ 90/391]	Time  0.073 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.3721e-01 (1.1265e-01)	Acc@1  95.31 ( 95.91)	Acc@5 100.00 ( 99.97)
Epoch: [65][100/391]	Time  0.071 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.2549e-01 (1.1183e-01)	Acc@1  96.09 ( 95.97)	Acc@5 100.00 ( 99.98)
Epoch: [65][110/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.7163e-01 (1.1172e-01)	Acc@1  94.53 ( 96.00)	Acc@5 100.00 ( 99.98)
Epoch: [65][120/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 9.2712e-02 (1.1236e-01)	Acc@1  96.88 ( 96.00)	Acc@5 100.00 ( 99.97)
Epoch: [65][130/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1542e-01 (1.1159e-01)	Acc@1  96.09 ( 96.03)	Acc@5 100.00 ( 99.98)
Epoch: [65][140/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4941e-01 (1.1120e-01)	Acc@1  92.19 ( 96.05)	Acc@5 100.00 ( 99.98)
Epoch: [65][150/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0211e-01 (1.1110e-01)	Acc@1  95.31 ( 96.05)	Acc@5 100.00 ( 99.97)
Epoch: [65][160/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.1909e-02 (1.1054e-01)	Acc@1  98.44 ( 96.07)	Acc@5 100.00 ( 99.98)
Epoch: [65][170/391]	Time  0.070 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6809e-01 (1.1239e-01)	Acc@1  95.31 ( 96.03)	Acc@5 100.00 ( 99.98)
Epoch: [65][180/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.5999e-02 (1.1126e-01)	Acc@1  97.66 ( 96.09)	Acc@5 100.00 ( 99.98)
Epoch: [65][190/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.9121e-02 (1.1101e-01)	Acc@1  98.44 ( 96.12)	Acc@5 100.00 ( 99.98)
Epoch: [65][200/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.9854e-02 (1.1058e-01)	Acc@1  96.88 ( 96.14)	Acc@5 100.00 ( 99.98)
Epoch: [65][210/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0925e-01 (1.1175e-01)	Acc@1  95.31 ( 96.08)	Acc@5 100.00 ( 99.98)
Epoch: [65][220/391]	Time  0.076 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.7627e-02 (1.1103e-01)	Acc@1  96.88 ( 96.10)	Acc@5 100.00 ( 99.98)
Epoch: [65][230/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.3323e-02 (1.1114e-01)	Acc@1  96.88 ( 96.09)	Acc@5 100.00 ( 99.98)
Epoch: [65][240/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1041e-01 (1.1114e-01)	Acc@1  96.88 ( 96.09)	Acc@5 100.00 ( 99.98)
Epoch: [65][250/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6736e-01 (1.1180e-01)	Acc@1  95.31 ( 96.07)	Acc@5 100.00 ( 99.98)
Epoch: [65][260/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.6670e-02 (1.1178e-01)	Acc@1  97.66 ( 96.05)	Acc@5 100.00 ( 99.99)
Epoch: [65][270/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.4177e-02 (1.1169e-01)	Acc@1  97.66 ( 96.04)	Acc@5 100.00 ( 99.99)
Epoch: [65][280/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0510e-01 (1.1231e-01)	Acc@1  96.09 ( 96.02)	Acc@5 100.00 ( 99.98)
Epoch: [65][290/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.7760e-02 (1.1202e-01)	Acc@1  98.44 ( 96.04)	Acc@5 100.00 ( 99.98)
Epoch: [65][300/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.7168e-02 (1.1191e-01)	Acc@1  97.66 ( 96.04)	Acc@5 100.00 ( 99.98)
Epoch: [65][310/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2744e-01 (1.1189e-01)	Acc@1  95.31 ( 96.05)	Acc@5 100.00 ( 99.98)
Epoch: [65][320/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.0759e-02 (1.1196e-01)	Acc@1  96.88 ( 96.04)	Acc@5 100.00 ( 99.98)
Epoch: [65][330/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.1167e-02 (1.1150e-01)	Acc@1  97.66 ( 96.06)	Acc@5 100.00 ( 99.98)
Epoch: [65][340/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0779e-01 (1.1100e-01)	Acc@1  94.53 ( 96.08)	Acc@5 100.00 ( 99.98)
Epoch: [65][350/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.4758e-02 (1.1065e-01)	Acc@1  98.44 ( 96.08)	Acc@5 100.00 ( 99.98)
Epoch: [65][360/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7627e-01 (1.1126e-01)	Acc@1  91.41 ( 96.05)	Acc@5 100.00 ( 99.98)
Epoch: [65][370/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.8796e-02 (1.1100e-01)	Acc@1  97.66 ( 96.06)	Acc@5 100.00 ( 99.98)
Epoch: [65][380/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.7312e-02 (1.1001e-01)	Acc@1  99.22 ( 96.10)	Acc@5 100.00 ( 99.98)
Epoch: [65][390/391]	Time  0.047 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1584e-01 (1.0988e-01)	Acc@1  96.25 ( 96.10)	Acc@5 100.00 ( 99.98)
## e[65] optimizer.zero_grad (sum) time: 0.3896145820617676
## e[65]       loss.backward (sum) time: 7.246360778808594
## e[65]      optimizer.step (sum) time: 3.487515926361084
## epoch[65] training(only) time: 25.610371589660645
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 2.9053e-01 (2.9053e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.027 ( 0.041)	Loss 3.9722e-01 (3.3108e-01)	Acc@1  90.00 ( 89.91)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 3.6841e-01 (3.2287e-01)	Acc@1  87.00 ( 90.10)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 2.7686e-01 (3.2019e-01)	Acc@1  88.00 ( 90.23)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.027 ( 0.029)	Loss 3.8916e-01 (3.2350e-01)	Acc@1  88.00 ( 90.02)	Acc@5  99.00 ( 99.56)
Test: [ 50/100]	Time  0.028 ( 0.029)	Loss 2.2583e-01 (3.2117e-01)	Acc@1  91.00 ( 90.10)	Acc@5 100.00 ( 99.63)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 2.0837e-01 (3.1440e-01)	Acc@1  93.00 ( 90.31)	Acc@5 100.00 ( 99.64)
Test: [ 70/100]	Time  0.024 ( 0.027)	Loss 4.2383e-01 (3.1399e-01)	Acc@1  89.00 ( 90.34)	Acc@5 100.00 ( 99.68)
Test: [ 80/100]	Time  0.027 ( 0.027)	Loss 1.5198e-01 (3.1420e-01)	Acc@1  93.00 ( 90.33)	Acc@5 100.00 ( 99.68)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 2.7051e-01 (3.1301e-01)	Acc@1  89.00 ( 90.45)	Acc@5 100.00 ( 99.68)
 * Acc@1 90.470 Acc@5 99.680
### epoch[65] execution time: 28.386667251586914
EPOCH 66
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [66][  0/391]	Time  0.245 ( 0.245)	Data  0.178 ( 0.178)	Loss 9.8633e-02 (9.8633e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [66][ 10/391]	Time  0.064 ( 0.080)	Data  0.001 ( 0.017)	Loss 1.3306e-01 (1.0813e-01)	Acc@1  93.75 ( 95.95)	Acc@5 100.00 ( 99.93)
Epoch: [66][ 20/391]	Time  0.063 ( 0.072)	Data  0.001 ( 0.009)	Loss 1.4319e-01 (1.0832e-01)	Acc@1  96.09 ( 96.06)	Acc@5 100.00 ( 99.96)
Epoch: [66][ 30/391]	Time  0.065 ( 0.070)	Data  0.001 ( 0.007)	Loss 7.1777e-02 (1.0776e-01)	Acc@1  96.09 ( 96.07)	Acc@5 100.00 ( 99.97)
Epoch: [66][ 40/391]	Time  0.071 ( 0.069)	Data  0.001 ( 0.005)	Loss 7.9346e-02 (1.0536e-01)	Acc@1  98.44 ( 96.15)	Acc@5 100.00 ( 99.98)
Epoch: [66][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.005)	Loss 9.0149e-02 (1.0830e-01)	Acc@1  98.44 ( 96.06)	Acc@5 100.00 ( 99.98)
Epoch: [66][ 60/391]	Time  0.061 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.5344e-01 (1.1079e-01)	Acc@1  92.97 ( 95.86)	Acc@5 100.00 ( 99.99)
Epoch: [66][ 70/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.0944e-01 (1.1006e-01)	Acc@1  96.88 ( 95.99)	Acc@5 100.00 ( 99.98)
Epoch: [66][ 80/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.5356e-01 (1.1029e-01)	Acc@1  92.97 ( 95.96)	Acc@5 100.00 ( 99.97)
Epoch: [66][ 90/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.0223e-01 (1.1015e-01)	Acc@1  95.31 ( 96.01)	Acc@5 100.00 ( 99.97)
Epoch: [66][100/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.0044e-01 (1.1180e-01)	Acc@1  93.75 ( 95.96)	Acc@5 100.00 ( 99.97)
Epoch: [66][110/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.1261e-01 (1.1225e-01)	Acc@1  97.66 ( 95.99)	Acc@5 100.00 ( 99.96)
Epoch: [66][120/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.8018e-01 (1.1284e-01)	Acc@1  94.53 ( 95.98)	Acc@5 100.00 ( 99.96)
Epoch: [66][130/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.1361e-02 (1.1083e-01)	Acc@1  98.44 ( 96.06)	Acc@5 100.00 ( 99.96)
Epoch: [66][140/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.4087e-02 (1.0958e-01)	Acc@1  97.66 ( 96.10)	Acc@5 100.00 ( 99.97)
Epoch: [66][150/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1676e-01 (1.0956e-01)	Acc@1  96.09 ( 96.11)	Acc@5 100.00 ( 99.96)
Epoch: [66][160/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1938e-01 (1.0940e-01)	Acc@1  95.31 ( 96.13)	Acc@5 100.00 ( 99.97)
Epoch: [66][170/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6589e-01 (1.1061e-01)	Acc@1  94.53 ( 96.10)	Acc@5 100.00 ( 99.96)
Epoch: [66][180/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4343e-01 (1.1015e-01)	Acc@1  94.53 ( 96.11)	Acc@5 100.00 ( 99.97)
Epoch: [66][190/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0785e-01 (1.0913e-01)	Acc@1  95.31 ( 96.12)	Acc@5 100.00 ( 99.97)
Epoch: [66][200/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.3740e-02 (1.1010e-01)	Acc@1  95.31 ( 96.08)	Acc@5 100.00 ( 99.96)
Epoch: [66][210/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3538e-01 (1.0986e-01)	Acc@1  92.97 ( 96.08)	Acc@5 100.00 ( 99.96)
Epoch: [66][220/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4722e-01 (1.1122e-01)	Acc@1  95.31 ( 96.04)	Acc@5 100.00 ( 99.96)
Epoch: [66][230/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3708e-01 (1.1138e-01)	Acc@1  95.31 ( 96.04)	Acc@5 100.00 ( 99.96)
Epoch: [66][240/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.5847e-02 (1.1092e-01)	Acc@1  98.44 ( 96.05)	Acc@5 100.00 ( 99.96)
Epoch: [66][250/391]	Time  0.064 ( 0.065)	Data  0.002 ( 0.002)	Loss 1.0870e-01 (1.1090e-01)	Acc@1  96.09 ( 96.02)	Acc@5 100.00 ( 99.97)
Epoch: [66][260/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6846e-01 (1.1082e-01)	Acc@1  93.75 ( 96.05)	Acc@5 100.00 ( 99.97)
Epoch: [66][270/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4331e-01 (1.1052e-01)	Acc@1  94.53 ( 96.06)	Acc@5 100.00 ( 99.97)
Epoch: [66][280/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0779e-01 (1.1025e-01)	Acc@1  96.09 ( 96.07)	Acc@5 100.00 ( 99.96)
Epoch: [66][290/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7725e-01 (1.1006e-01)	Acc@1  92.97 ( 96.06)	Acc@5 100.00 ( 99.97)
Epoch: [66][300/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5271e-01 (1.1011e-01)	Acc@1  95.31 ( 96.06)	Acc@5 100.00 ( 99.96)
Epoch: [66][310/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6833e-01 (1.1005e-01)	Acc@1  95.31 ( 96.07)	Acc@5 100.00 ( 99.96)
Epoch: [66][320/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.4951e-02 (1.0966e-01)	Acc@1  97.66 ( 96.10)	Acc@5 100.00 ( 99.97)
Epoch: [66][330/391]	Time  0.071 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.0088e-02 (1.0978e-01)	Acc@1  96.09 ( 96.08)	Acc@5 100.00 ( 99.97)
Epoch: [66][340/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1987e-01 (1.0997e-01)	Acc@1  97.66 ( 96.08)	Acc@5  99.22 ( 99.96)
Epoch: [66][350/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5491e-01 (1.1058e-01)	Acc@1  94.53 ( 96.05)	Acc@5 100.00 ( 99.96)
Epoch: [66][360/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3232e-01 (1.1062e-01)	Acc@1  96.09 ( 96.07)	Acc@5 100.00 ( 99.96)
Epoch: [66][370/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.3181e-02 (1.1073e-01)	Acc@1  97.66 ( 96.06)	Acc@5 100.00 ( 99.96)
Epoch: [66][380/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2128e-01 (1.1075e-01)	Acc@1  95.31 ( 96.07)	Acc@5 100.00 ( 99.96)
Epoch: [66][390/391]	Time  0.047 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.6416e-01 (1.1133e-01)	Acc@1  92.50 ( 96.04)	Acc@5 100.00 ( 99.96)
## e[66] optimizer.zero_grad (sum) time: 0.3896021842956543
## e[66]       loss.backward (sum) time: 7.267220497131348
## e[66]      optimizer.step (sum) time: 3.453087091445923
## epoch[66] training(only) time: 25.630284547805786
# Switched to evaluate mode...
Test: [  0/100]	Time  0.192 ( 0.192)	Loss 2.8784e-01 (2.8784e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.040)	Loss 3.9697e-01 (3.3022e-01)	Acc@1  89.00 ( 90.09)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 3.4985e-01 (3.2245e-01)	Acc@1  86.00 ( 89.90)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.024 ( 0.030)	Loss 2.5684e-01 (3.1797e-01)	Acc@1  89.00 ( 90.23)	Acc@5 100.00 ( 99.61)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 3.8574e-01 (3.2074e-01)	Acc@1  89.00 ( 90.24)	Acc@5  99.00 ( 99.56)
Test: [ 50/100]	Time  0.025 ( 0.028)	Loss 2.1667e-01 (3.1890e-01)	Acc@1  92.00 ( 90.22)	Acc@5 100.00 ( 99.65)
Test: [ 60/100]	Time  0.027 ( 0.028)	Loss 1.7273e-01 (3.1140e-01)	Acc@1  94.00 ( 90.46)	Acc@5 100.00 ( 99.66)
Test: [ 70/100]	Time  0.027 ( 0.027)	Loss 4.4751e-01 (3.1303e-01)	Acc@1  88.00 ( 90.45)	Acc@5 100.00 ( 99.69)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 1.3928e-01 (3.1375e-01)	Acc@1  95.00 ( 90.48)	Acc@5 100.00 ( 99.69)
Test: [ 90/100]	Time  0.025 ( 0.027)	Loss 2.2900e-01 (3.1280e-01)	Acc@1  89.00 ( 90.53)	Acc@5 100.00 ( 99.69)
 * Acc@1 90.570 Acc@5 99.700
### epoch[66] execution time: 28.408804178237915
EPOCH 67
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [67][  0/391]	Time  0.247 ( 0.247)	Data  0.183 ( 0.183)	Loss 1.3269e-01 (1.3269e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [67][ 10/391]	Time  0.065 ( 0.082)	Data  0.001 ( 0.018)	Loss 8.3740e-02 (9.8641e-02)	Acc@1  96.88 ( 96.45)	Acc@5 100.00 (100.00)
Epoch: [67][ 20/391]	Time  0.065 ( 0.074)	Data  0.001 ( 0.010)	Loss 9.1919e-02 (1.0764e-01)	Acc@1  96.88 ( 96.13)	Acc@5 100.00 (100.00)
Epoch: [67][ 30/391]	Time  0.063 ( 0.071)	Data  0.001 ( 0.007)	Loss 1.3574e-01 (1.0829e-01)	Acc@1  95.31 ( 96.14)	Acc@5 100.00 (100.00)
Epoch: [67][ 40/391]	Time  0.062 ( 0.069)	Data  0.001 ( 0.006)	Loss 8.2031e-02 (1.0469e-01)	Acc@1  97.66 ( 96.32)	Acc@5 100.00 (100.00)
Epoch: [67][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.005)	Loss 1.4478e-01 (1.0770e-01)	Acc@1  95.31 ( 96.22)	Acc@5 100.00 ( 99.98)
Epoch: [67][ 60/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.4148e-01 (1.0813e-01)	Acc@1  93.75 ( 96.18)	Acc@5 100.00 ( 99.99)
Epoch: [67][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.7529e-01 (1.0944e-01)	Acc@1  95.31 ( 96.08)	Acc@5 100.00 ( 99.99)
Epoch: [67][ 80/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.0596e-01 (1.0932e-01)	Acc@1  95.31 ( 96.03)	Acc@5 100.00 ( 99.99)
Epoch: [67][ 90/391]	Time  0.059 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.3367e-01 (1.0863e-01)	Acc@1  95.31 ( 96.04)	Acc@5 100.00 ( 99.99)
Epoch: [67][100/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.003)	Loss 6.9763e-02 (1.0760e-01)	Acc@1  96.88 ( 96.10)	Acc@5 100.00 ( 99.98)
Epoch: [67][110/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.6284e-01 (1.0708e-01)	Acc@1  94.53 ( 96.11)	Acc@5 100.00 ( 99.99)
Epoch: [67][120/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.0675e-01 (1.0749e-01)	Acc@1  96.09 ( 96.08)	Acc@5 100.00 ( 99.99)
Epoch: [67][130/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1237e-01 (1.0940e-01)	Acc@1  96.09 ( 96.00)	Acc@5 100.00 ( 99.98)
Epoch: [67][140/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1761e-01 (1.0932e-01)	Acc@1  96.88 ( 96.02)	Acc@5 100.00 ( 99.98)
Epoch: [67][150/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.3242e-02 (1.0811e-01)	Acc@1  96.88 ( 96.08)	Acc@5 100.00 ( 99.98)
Epoch: [67][160/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5247e-01 (1.0956e-01)	Acc@1  93.75 ( 96.01)	Acc@5 100.00 ( 99.98)
Epoch: [67][170/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3660e-01 (1.1130e-01)	Acc@1  93.75 ( 95.97)	Acc@5 100.00 ( 99.97)
Epoch: [67][180/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.8075e-02 (1.1004e-01)	Acc@1  97.66 ( 96.04)	Acc@5 100.00 ( 99.97)
Epoch: [67][190/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.6477e-02 (1.0933e-01)	Acc@1  96.88 ( 96.04)	Acc@5 100.00 ( 99.98)
Epoch: [67][200/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3147e-01 (1.0963e-01)	Acc@1  94.53 ( 96.05)	Acc@5 100.00 ( 99.98)
Epoch: [67][210/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.2216e-02 (1.0893e-01)	Acc@1  97.66 ( 96.06)	Acc@5 100.00 ( 99.98)
Epoch: [67][220/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.6122e-02 (1.0900e-01)	Acc@1  98.44 ( 96.09)	Acc@5 100.00 ( 99.97)
Epoch: [67][230/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0620e-01 (1.0860e-01)	Acc@1  95.31 ( 96.08)	Acc@5  99.22 ( 99.97)
Epoch: [67][240/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.9426e-02 (1.0881e-01)	Acc@1  96.09 ( 96.05)	Acc@5 100.00 ( 99.97)
Epoch: [67][250/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.7290e-02 (1.0921e-01)	Acc@1  96.09 ( 96.06)	Acc@5 100.00 ( 99.97)
Epoch: [67][260/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.9265e-02 (1.0909e-01)	Acc@1  99.22 ( 96.06)	Acc@5  99.22 ( 99.97)
Epoch: [67][270/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0901e-01 (1.0945e-01)	Acc@1  96.88 ( 96.05)	Acc@5 100.00 ( 99.97)
Epoch: [67][280/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.5154e-02 (1.1004e-01)	Acc@1  96.88 ( 96.04)	Acc@5 100.00 ( 99.97)
Epoch: [67][290/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5991e-01 (1.0978e-01)	Acc@1  96.09 ( 96.05)	Acc@5 100.00 ( 99.97)
Epoch: [67][300/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.7444e-02 (1.1007e-01)	Acc@1  98.44 ( 96.06)	Acc@5 100.00 ( 99.97)
Epoch: [67][310/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.4106e-02 (1.0980e-01)	Acc@1  97.66 ( 96.06)	Acc@5 100.00 ( 99.96)
Epoch: [67][320/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1365e-01 (1.0948e-01)	Acc@1  96.09 ( 96.07)	Acc@5 100.00 ( 99.97)
Epoch: [67][330/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.3740e-02 (1.0921e-01)	Acc@1  96.88 ( 96.09)	Acc@5 100.00 ( 99.97)
Epoch: [67][340/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.2805e-02 (1.0896e-01)	Acc@1  97.66 ( 96.10)	Acc@5 100.00 ( 99.97)
Epoch: [67][350/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.5380e-02 (1.0904e-01)	Acc@1  99.22 ( 96.10)	Acc@5 100.00 ( 99.97)
Epoch: [67][360/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1530e-01 (1.0907e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 ( 99.97)
Epoch: [67][370/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.3140e-02 (1.0887e-01)	Acc@1  95.31 ( 96.09)	Acc@5 100.00 ( 99.97)
Epoch: [67][380/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.0688e-02 (1.0881e-01)	Acc@1  97.66 ( 96.10)	Acc@5 100.00 ( 99.97)
Epoch: [67][390/391]	Time  0.052 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5552e-01 (1.0847e-01)	Acc@1  95.00 ( 96.12)	Acc@5 100.00 ( 99.97)
## e[67] optimizer.zero_grad (sum) time: 0.3859424591064453
## e[67]       loss.backward (sum) time: 7.183936357498169
## e[67]      optimizer.step (sum) time: 3.4137003421783447
## epoch[67] training(only) time: 25.45092511177063
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 2.7026e-01 (2.7026e-01)	Acc@1  87.00 ( 87.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.040)	Loss 3.8574e-01 (3.2874e-01)	Acc@1  90.00 ( 89.91)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.024 ( 0.034)	Loss 3.5425e-01 (3.1961e-01)	Acc@1  85.00 ( 90.00)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.028 ( 0.031)	Loss 2.6733e-01 (3.1692e-01)	Acc@1  87.00 ( 90.19)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.027 ( 0.029)	Loss 3.8721e-01 (3.2095e-01)	Acc@1  89.00 ( 90.12)	Acc@5  99.00 ( 99.56)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 2.2192e-01 (3.1863e-01)	Acc@1  92.00 ( 90.10)	Acc@5 100.00 ( 99.61)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 1.7822e-01 (3.1164e-01)	Acc@1  94.00 ( 90.33)	Acc@5 100.00 ( 99.62)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 4.2944e-01 (3.1181e-01)	Acc@1  89.00 ( 90.34)	Acc@5 100.00 ( 99.66)
Test: [ 80/100]	Time  0.027 ( 0.027)	Loss 1.4978e-01 (3.1173e-01)	Acc@1  93.00 ( 90.35)	Acc@5 100.00 ( 99.68)
Test: [ 90/100]	Time  0.025 ( 0.027)	Loss 2.3535e-01 (3.1035e-01)	Acc@1  89.00 ( 90.40)	Acc@5 100.00 ( 99.68)
 * Acc@1 90.450 Acc@5 99.700
### epoch[67] execution time: 28.272006273269653
EPOCH 68
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [68][  0/391]	Time  0.238 ( 0.238)	Data  0.172 ( 0.172)	Loss 1.1578e-01 (1.1578e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [68][ 10/391]	Time  0.062 ( 0.080)	Data  0.001 ( 0.017)	Loss 7.7576e-02 (1.0270e-01)	Acc@1  98.44 ( 95.95)	Acc@5 100.00 (100.00)
Epoch: [68][ 20/391]	Time  0.066 ( 0.073)	Data  0.001 ( 0.009)	Loss 1.1658e-01 (1.0616e-01)	Acc@1  94.53 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [68][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.007)	Loss 4.5563e-02 (1.0535e-01)	Acc@1  99.22 ( 96.42)	Acc@5 100.00 (100.00)
Epoch: [68][ 40/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.005)	Loss 1.0236e-01 (1.0770e-01)	Acc@1  96.88 ( 96.27)	Acc@5 100.00 ( 99.98)
Epoch: [68][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.5271e-01 (1.0923e-01)	Acc@1  92.97 ( 96.20)	Acc@5  99.22 ( 99.97)
Epoch: [68][ 60/391]	Time  0.059 ( 0.067)	Data  0.001 ( 0.004)	Loss 8.5266e-02 (1.0676e-01)	Acc@1  96.88 ( 96.32)	Acc@5 100.00 ( 99.97)
Epoch: [68][ 70/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.0968e-01 (1.0472e-01)	Acc@1  96.09 ( 96.46)	Acc@5 100.00 ( 99.98)
Epoch: [68][ 80/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.8591e-01 (1.0377e-01)	Acc@1  93.75 ( 96.50)	Acc@5 100.00 ( 99.97)
Epoch: [68][ 90/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.2323e-01 (1.0245e-01)	Acc@1  96.88 ( 96.51)	Acc@5 100.00 ( 99.97)
Epoch: [68][100/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 9.4604e-02 (1.0091e-01)	Acc@1  96.88 ( 96.57)	Acc@5 100.00 ( 99.98)
Epoch: [68][110/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 8.6182e-02 (1.0162e-01)	Acc@1  97.66 ( 96.52)	Acc@5 100.00 ( 99.98)
Epoch: [68][120/391]	Time  0.076 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.0968e-01 (1.0419e-01)	Acc@1  96.88 ( 96.44)	Acc@5 100.00 ( 99.96)
Epoch: [68][130/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6895e-01 (1.0452e-01)	Acc@1  92.97 ( 96.40)	Acc@5 100.00 ( 99.96)
Epoch: [68][140/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.5571e-02 (1.0312e-01)	Acc@1  96.88 ( 96.45)	Acc@5 100.00 ( 99.96)
Epoch: [68][150/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1823e-01 (1.0471e-01)	Acc@1  96.09 ( 96.39)	Acc@5 100.00 ( 99.96)
Epoch: [68][160/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2805e-01 (1.0566e-01)	Acc@1  96.09 ( 96.36)	Acc@5 100.00 ( 99.96)
Epoch: [68][170/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.6426e-02 (1.0469e-01)	Acc@1  96.88 ( 96.36)	Acc@5 100.00 ( 99.96)
Epoch: [68][180/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0999e-01 (1.0344e-01)	Acc@1  96.88 ( 96.40)	Acc@5 100.00 ( 99.96)
Epoch: [68][190/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4343e-01 (1.0452e-01)	Acc@1  95.31 ( 96.34)	Acc@5 100.00 ( 99.96)
Epoch: [68][200/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.9233e-02 (1.0500e-01)	Acc@1  96.09 ( 96.32)	Acc@5 100.00 ( 99.96)
Epoch: [68][210/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8872e-01 (1.0610e-01)	Acc@1  92.19 ( 96.31)	Acc@5 100.00 ( 99.96)
Epoch: [68][220/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.7759e-02 (1.0608e-01)	Acc@1  97.66 ( 96.31)	Acc@5 100.00 ( 99.96)
Epoch: [68][230/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5503e-01 (1.0685e-01)	Acc@1  94.53 ( 96.26)	Acc@5 100.00 ( 99.96)
Epoch: [68][240/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.4158e-02 (1.0724e-01)	Acc@1  96.88 ( 96.23)	Acc@5 100.00 ( 99.96)
Epoch: [68][250/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.2468e-02 (1.0773e-01)	Acc@1  96.88 ( 96.24)	Acc@5 100.00 ( 99.96)
Epoch: [68][260/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.7046e-02 (1.0787e-01)	Acc@1  97.66 ( 96.25)	Acc@5 100.00 ( 99.96)
Epoch: [68][270/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.5369e-02 (1.0765e-01)	Acc@1  96.88 ( 96.25)	Acc@5 100.00 ( 99.96)
Epoch: [68][280/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6736e-01 (1.0729e-01)	Acc@1  94.53 ( 96.28)	Acc@5 100.00 ( 99.96)
Epoch: [68][290/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.2469e-02 (1.0822e-01)	Acc@1  99.22 ( 96.24)	Acc@5 100.00 ( 99.97)
Epoch: [68][300/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6760e-01 (1.0773e-01)	Acc@1  96.09 ( 96.30)	Acc@5 100.00 ( 99.97)
Epoch: [68][310/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1951e-01 (1.0740e-01)	Acc@1  97.66 ( 96.32)	Acc@5 100.00 ( 99.97)
Epoch: [68][320/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.2632e-02 (1.0744e-01)	Acc@1  97.66 ( 96.31)	Acc@5 100.00 ( 99.97)
Epoch: [68][330/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.7107e-02 (1.0673e-01)	Acc@1  96.09 ( 96.34)	Acc@5 100.00 ( 99.97)
Epoch: [68][340/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.9712e-02 (1.0661e-01)	Acc@1  97.66 ( 96.35)	Acc@5 100.00 ( 99.97)
Epoch: [68][350/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8311e-01 (1.0704e-01)	Acc@1  91.41 ( 96.31)	Acc@5 100.00 ( 99.97)
Epoch: [68][360/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.4055e-02 (1.0710e-01)	Acc@1  96.88 ( 96.32)	Acc@5 100.00 ( 99.97)
Epoch: [68][370/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0303e-01 (1.0730e-01)	Acc@1  95.31 ( 96.30)	Acc@5 100.00 ( 99.97)
Epoch: [68][380/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2091e-01 (1.0718e-01)	Acc@1  95.31 ( 96.30)	Acc@5 100.00 ( 99.97)
Epoch: [68][390/391]	Time  0.046 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0923e-01 (1.0750e-01)	Acc@1  93.75 ( 96.30)	Acc@5  98.75 ( 99.96)
## e[68] optimizer.zero_grad (sum) time: 0.3865065574645996
## e[68]       loss.backward (sum) time: 7.235201597213745
## e[68]      optimizer.step (sum) time: 3.4443626403808594
## epoch[68] training(only) time: 25.494932889938354
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 3.0444e-01 (3.0444e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.040)	Loss 3.9478e-01 (3.2889e-01)	Acc@1  89.00 ( 90.18)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.024 ( 0.032)	Loss 3.5889e-01 (3.2123e-01)	Acc@1  88.00 ( 90.24)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.032 ( 0.030)	Loss 2.6880e-01 (3.2019e-01)	Acc@1  88.00 ( 90.42)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 3.6890e-01 (3.2309e-01)	Acc@1  88.00 ( 90.29)	Acc@5  99.00 ( 99.61)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 2.1716e-01 (3.2094e-01)	Acc@1  93.00 ( 90.27)	Acc@5 100.00 ( 99.69)
Test: [ 60/100]	Time  0.028 ( 0.027)	Loss 1.9543e-01 (3.1361e-01)	Acc@1  95.00 ( 90.52)	Acc@5 100.00 ( 99.69)
Test: [ 70/100]	Time  0.024 ( 0.027)	Loss 4.1919e-01 (3.1400e-01)	Acc@1  89.00 ( 90.49)	Acc@5 100.00 ( 99.72)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 1.4661e-01 (3.1478e-01)	Acc@1  95.00 ( 90.51)	Acc@5 100.00 ( 99.70)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 2.5635e-01 (3.1287e-01)	Acc@1  89.00 ( 90.60)	Acc@5 100.00 ( 99.70)
 * Acc@1 90.640 Acc@5 99.710
### epoch[68] execution time: 28.239351987838745
EPOCH 69
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [69][  0/391]	Time  0.249 ( 0.249)	Data  0.181 ( 0.181)	Loss 1.3806e-01 (1.3806e-01)	Acc@1  92.97 ( 92.97)	Acc@5 100.00 (100.00)
Epoch: [69][ 10/391]	Time  0.066 ( 0.082)	Data  0.001 ( 0.017)	Loss 1.0834e-01 (1.0431e-01)	Acc@1  95.31 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [69][ 20/391]	Time  0.064 ( 0.074)	Data  0.001 ( 0.010)	Loss 5.4138e-02 (9.8944e-02)	Acc@1  99.22 ( 96.47)	Acc@5 100.00 (100.00)
Epoch: [69][ 30/391]	Time  0.066 ( 0.071)	Data  0.001 ( 0.007)	Loss 5.4596e-02 (9.8710e-02)	Acc@1  98.44 ( 96.65)	Acc@5 100.00 ( 99.97)
Epoch: [69][ 40/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.006)	Loss 1.3025e-01 (9.8712e-02)	Acc@1  96.09 ( 96.80)	Acc@5 100.00 ( 99.98)
Epoch: [69][ 50/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.005)	Loss 9.8877e-02 (9.7387e-02)	Acc@1  96.09 ( 96.91)	Acc@5 100.00 ( 99.98)
Epoch: [69][ 60/391]	Time  0.062 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.0309e-01 (9.9633e-02)	Acc@1  96.09 ( 96.79)	Acc@5 100.00 ( 99.99)
Epoch: [69][ 70/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.004)	Loss 8.8196e-02 (1.0017e-01)	Acc@1  97.66 ( 96.74)	Acc@5 100.00 ( 99.99)
Epoch: [69][ 80/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.5308e-01 (1.0300e-01)	Acc@1  92.19 ( 96.60)	Acc@5 100.00 ( 99.98)
Epoch: [69][ 90/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 9.0027e-02 (1.0308e-01)	Acc@1  96.09 ( 96.57)	Acc@5 100.00 ( 99.97)
Epoch: [69][100/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.3049e-01 (1.0459e-01)	Acc@1  95.31 ( 96.49)	Acc@5 100.00 ( 99.98)
Epoch: [69][110/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.1005e-01 (1.0723e-01)	Acc@1  96.09 ( 96.39)	Acc@5 100.00 ( 99.98)
Epoch: [69][120/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.0883e-01 (1.0732e-01)	Acc@1  96.88 ( 96.38)	Acc@5 100.00 ( 99.98)
Epoch: [69][130/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5771e-01 (1.0635e-01)	Acc@1  93.75 ( 96.40)	Acc@5 100.00 ( 99.98)
Epoch: [69][140/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.4290e-02 (1.0787e-01)	Acc@1  96.88 ( 96.36)	Acc@5 100.00 ( 99.98)
Epoch: [69][150/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.6548e-02 (1.0844e-01)	Acc@1  97.66 ( 96.36)	Acc@5 100.00 ( 99.98)
Epoch: [69][160/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0175e-01 (1.0930e-01)	Acc@1  96.09 ( 96.33)	Acc@5 100.00 ( 99.99)
Epoch: [69][170/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3269e-01 (1.0930e-01)	Acc@1  95.31 ( 96.34)	Acc@5 100.00 ( 99.99)
Epoch: [69][180/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.1431e-02 (1.0879e-01)	Acc@1  97.66 ( 96.34)	Acc@5 100.00 ( 99.99)
Epoch: [69][190/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.1188e-02 (1.0795e-01)	Acc@1  98.44 ( 96.39)	Acc@5 100.00 ( 99.99)
Epoch: [69][200/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5332e-01 (1.0818e-01)	Acc@1  94.53 ( 96.37)	Acc@5 100.00 ( 99.99)
Epoch: [69][210/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4709e-01 (1.0796e-01)	Acc@1  95.31 ( 96.40)	Acc@5 100.00 ( 99.99)
Epoch: [69][220/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8762e-01 (1.0811e-01)	Acc@1  93.75 ( 96.39)	Acc@5 100.00 ( 99.99)
Epoch: [69][230/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1835e-01 (1.0710e-01)	Acc@1  96.88 ( 96.44)	Acc@5 100.00 ( 99.99)
Epoch: [69][240/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.3730e-02 (1.0786e-01)	Acc@1  96.09 ( 96.42)	Acc@5 100.00 ( 99.99)
Epoch: [69][250/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1615e-01 (1.0782e-01)	Acc@1  97.66 ( 96.42)	Acc@5 100.00 ( 99.98)
Epoch: [69][260/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7371e-01 (1.0780e-01)	Acc@1  94.53 ( 96.41)	Acc@5 100.00 ( 99.98)
Epoch: [69][270/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.3547e-02 (1.0812e-01)	Acc@1  97.66 ( 96.37)	Acc@5 100.00 ( 99.98)
Epoch: [69][280/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1707e-01 (1.0752e-01)	Acc@1  93.75 ( 96.38)	Acc@5 100.00 ( 99.98)
Epoch: [69][290/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.6191e-02 (1.0727e-01)	Acc@1  96.88 ( 96.39)	Acc@5 100.00 ( 99.98)
Epoch: [69][300/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1499e-01 (1.0709e-01)	Acc@1  96.09 ( 96.38)	Acc@5 100.00 ( 99.98)
Epoch: [69][310/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5918e-01 (1.0743e-01)	Acc@1  93.75 ( 96.36)	Acc@5 100.00 ( 99.98)
Epoch: [69][320/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0303e-01 (1.0716e-01)	Acc@1  95.31 ( 96.37)	Acc@5 100.00 ( 99.98)
Epoch: [69][330/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.0496e-02 (1.0672e-01)	Acc@1  96.88 ( 96.36)	Acc@5 100.00 ( 99.98)
Epoch: [69][340/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.4382e-02 (1.0687e-01)	Acc@1  99.22 ( 96.35)	Acc@5 100.00 ( 99.98)
Epoch: [69][350/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.2468e-02 (1.0663e-01)	Acc@1  96.09 ( 96.36)	Acc@5 100.00 ( 99.98)
Epoch: [69][360/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.7961e-02 (1.0672e-01)	Acc@1  96.09 ( 96.35)	Acc@5 100.00 ( 99.98)
Epoch: [69][370/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5015e-01 (1.0681e-01)	Acc@1  95.31 ( 96.34)	Acc@5 100.00 ( 99.98)
Epoch: [69][380/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.5553e-02 (1.0654e-01)	Acc@1  99.22 ( 96.35)	Acc@5 100.00 ( 99.98)
Epoch: [69][390/391]	Time  0.047 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.0515e-02 (1.0637e-01)	Acc@1  96.25 ( 96.34)	Acc@5 100.00 ( 99.98)
## e[69] optimizer.zero_grad (sum) time: 0.38872766494750977
## e[69]       loss.backward (sum) time: 7.252645254135132
## e[69]      optimizer.step (sum) time: 3.475999116897583
## epoch[69] training(only) time: 25.667394161224365
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 2.9346e-01 (2.9346e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.027 ( 0.040)	Loss 4.0112e-01 (3.3272e-01)	Acc@1  89.00 ( 89.82)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 3.5645e-01 (3.2272e-01)	Acc@1  86.00 ( 89.81)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.024 ( 0.030)	Loss 2.5342e-01 (3.1869e-01)	Acc@1  90.00 ( 90.32)	Acc@5 100.00 ( 99.61)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 3.7207e-01 (3.2175e-01)	Acc@1  89.00 ( 90.37)	Acc@5  99.00 ( 99.56)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 2.2168e-01 (3.2071e-01)	Acc@1  92.00 ( 90.31)	Acc@5 100.00 ( 99.65)
Test: [ 60/100]	Time  0.028 ( 0.028)	Loss 1.8469e-01 (3.1330e-01)	Acc@1  94.00 ( 90.57)	Acc@5 100.00 ( 99.66)
Test: [ 70/100]	Time  0.024 ( 0.027)	Loss 4.2358e-01 (3.1399e-01)	Acc@1  88.00 ( 90.51)	Acc@5 100.00 ( 99.69)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 1.3196e-01 (3.1487e-01)	Acc@1  95.00 ( 90.51)	Acc@5 100.00 ( 99.68)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 2.3926e-01 (3.1322e-01)	Acc@1  89.00 ( 90.58)	Acc@5 100.00 ( 99.68)
 * Acc@1 90.620 Acc@5 99.690
### epoch[69] execution time: 28.487224817276
EPOCH 70
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [70][  0/391]	Time  0.239 ( 0.239)	Data  0.173 ( 0.173)	Loss 5.7617e-02 (5.7617e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [70][ 10/391]	Time  0.063 ( 0.080)	Data  0.001 ( 0.017)	Loss 9.4116e-02 (1.0520e-01)	Acc@1  96.88 ( 96.24)	Acc@5 100.00 (100.00)
Epoch: [70][ 20/391]	Time  0.064 ( 0.074)	Data  0.001 ( 0.009)	Loss 1.1523e-01 (1.0648e-01)	Acc@1  97.66 ( 96.24)	Acc@5 100.00 (100.00)
Epoch: [70][ 30/391]	Time  0.063 ( 0.071)	Data  0.001 ( 0.007)	Loss 8.0444e-02 (1.0393e-01)	Acc@1  96.88 ( 96.42)	Acc@5 100.00 (100.00)
Epoch: [70][ 40/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.005)	Loss 1.1322e-01 (1.1223e-01)	Acc@1  97.66 ( 96.06)	Acc@5 100.00 (100.00)
Epoch: [70][ 50/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.004)	Loss 7.2937e-02 (1.0991e-01)	Acc@1  97.66 ( 96.17)	Acc@5 100.00 (100.00)
Epoch: [70][ 60/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.2793e-01 (1.0739e-01)	Acc@1  95.31 ( 96.29)	Acc@5 100.00 (100.00)
Epoch: [70][ 70/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.3269e-01 (1.0565e-01)	Acc@1  96.09 ( 96.40)	Acc@5 100.00 ( 99.99)
Epoch: [70][ 80/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 9.4360e-02 (1.0357e-01)	Acc@1  96.88 ( 96.49)	Acc@5 100.00 ( 99.99)
Epoch: [70][ 90/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 9.2102e-02 (1.0400e-01)	Acc@1  96.88 ( 96.53)	Acc@5 100.00 ( 99.98)
Epoch: [70][100/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.003)	Loss 8.9600e-02 (1.0279e-01)	Acc@1  96.88 ( 96.59)	Acc@5 100.00 ( 99.98)
Epoch: [70][110/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 8.1299e-02 (1.0089e-01)	Acc@1  96.88 ( 96.63)	Acc@5 100.00 ( 99.99)
Epoch: [70][120/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.6660e-02 (1.0206e-01)	Acc@1  96.88 ( 96.59)	Acc@5 100.00 ( 99.98)
Epoch: [70][130/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.8929e-02 (1.0282e-01)	Acc@1  98.44 ( 96.58)	Acc@5 100.00 ( 99.98)
Epoch: [70][140/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5674e-01 (1.0387e-01)	Acc@1  96.09 ( 96.53)	Acc@5 100.00 ( 99.98)
Epoch: [70][150/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.7148e-02 (1.0367e-01)	Acc@1  97.66 ( 96.54)	Acc@5 100.00 ( 99.97)
Epoch: [70][160/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.1594e-02 (1.0438e-01)	Acc@1  96.09 ( 96.49)	Acc@5 100.00 ( 99.98)
Epoch: [70][170/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.9966e-02 (1.0514e-01)	Acc@1  95.31 ( 96.45)	Acc@5 100.00 ( 99.97)
Epoch: [70][180/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4124e-01 (1.0607e-01)	Acc@1  92.97 ( 96.40)	Acc@5 100.00 ( 99.97)
Epoch: [70][190/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.7341e-02 (1.0692e-01)	Acc@1  96.09 ( 96.36)	Acc@5 100.00 ( 99.98)
Epoch: [70][200/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.0720e-02 (1.0645e-01)	Acc@1  97.66 ( 96.39)	Acc@5 100.00 ( 99.98)
Epoch: [70][210/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3318e-01 (1.0612e-01)	Acc@1  95.31 ( 96.41)	Acc@5 100.00 ( 99.98)
Epoch: [70][220/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1078e-01 (1.0582e-01)	Acc@1  97.66 ( 96.43)	Acc@5 100.00 ( 99.98)
Epoch: [70][230/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.7749e-02 (1.0645e-01)	Acc@1  98.44 ( 96.41)	Acc@5 100.00 ( 99.98)
Epoch: [70][240/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.3599e-02 (1.0569e-01)	Acc@1  96.88 ( 96.42)	Acc@5 100.00 ( 99.98)
Epoch: [70][250/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.1919e-02 (1.0566e-01)	Acc@1  96.09 ( 96.43)	Acc@5 100.00 ( 99.98)
Epoch: [70][260/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.7454e-02 (1.0519e-01)	Acc@1  97.66 ( 96.45)	Acc@5 100.00 ( 99.98)
Epoch: [70][270/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0919e-01 (1.0571e-01)	Acc@1  96.09 ( 96.43)	Acc@5 100.00 ( 99.98)
Epoch: [70][280/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0449e-01 (1.0593e-01)	Acc@1  96.09 ( 96.41)	Acc@5 100.00 ( 99.98)
Epoch: [70][290/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.2612e-02 (1.0580e-01)	Acc@1  99.22 ( 96.42)	Acc@5 100.00 ( 99.97)
Epoch: [70][300/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.6843e-02 (1.0589e-01)	Acc@1  97.66 ( 96.41)	Acc@5 100.00 ( 99.97)
Epoch: [70][310/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.9397e-02 (1.0542e-01)	Acc@1  98.44 ( 96.42)	Acc@5 100.00 ( 99.97)
Epoch: [70][320/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.2651e-02 (1.0541e-01)	Acc@1  96.88 ( 96.42)	Acc@5 100.00 ( 99.98)
Epoch: [70][330/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1597e-01 (1.0541e-01)	Acc@1  96.09 ( 96.44)	Acc@5 100.00 ( 99.98)
Epoch: [70][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2585e-01 (1.0501e-01)	Acc@1  94.53 ( 96.45)	Acc@5 100.00 ( 99.97)
Epoch: [70][350/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9543e-01 (1.0552e-01)	Acc@1  90.62 ( 96.43)	Acc@5 100.00 ( 99.98)
Epoch: [70][360/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0272e-01 (1.0653e-01)	Acc@1  96.88 ( 96.37)	Acc@5 100.00 ( 99.98)
Epoch: [70][370/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1517e-01 (1.0605e-01)	Acc@1  96.09 ( 96.38)	Acc@5 100.00 ( 99.97)
Epoch: [70][380/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2457e-01 (1.0630e-01)	Acc@1  95.31 ( 96.36)	Acc@5 100.00 ( 99.98)
Epoch: [70][390/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.4951e-02 (1.0637e-01)	Acc@1  97.50 ( 96.34)	Acc@5 100.00 ( 99.98)
## e[70] optimizer.zero_grad (sum) time: 0.395632266998291
## e[70]       loss.backward (sum) time: 7.239680051803589
## e[70]      optimizer.step (sum) time: 3.4258460998535156
## epoch[70] training(only) time: 25.63267731666565
# Switched to evaluate mode...
Test: [  0/100]	Time  0.198 ( 0.198)	Loss 2.9614e-01 (2.9614e-01)	Acc@1  87.00 ( 87.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.025 ( 0.041)	Loss 3.9697e-01 (3.3143e-01)	Acc@1  89.00 ( 89.73)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 3.5474e-01 (3.2053e-01)	Acc@1  86.00 ( 89.95)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 2.5854e-01 (3.1770e-01)	Acc@1  89.00 ( 90.32)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 3.7598e-01 (3.2086e-01)	Acc@1  89.00 ( 90.41)	Acc@5  99.00 ( 99.61)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 2.2827e-01 (3.2018e-01)	Acc@1  92.00 ( 90.37)	Acc@5 100.00 ( 99.67)
Test: [ 60/100]	Time  0.027 ( 0.028)	Loss 1.8042e-01 (3.1298e-01)	Acc@1  95.00 ( 90.59)	Acc@5 100.00 ( 99.69)
Test: [ 70/100]	Time  0.024 ( 0.027)	Loss 4.2749e-01 (3.1393e-01)	Acc@1  87.00 ( 90.51)	Acc@5 100.00 ( 99.72)
Test: [ 80/100]	Time  0.027 ( 0.027)	Loss 1.3428e-01 (3.1471e-01)	Acc@1  95.00 ( 90.52)	Acc@5 100.00 ( 99.72)
Test: [ 90/100]	Time  0.027 ( 0.027)	Loss 2.3401e-01 (3.1319e-01)	Acc@1  89.00 ( 90.59)	Acc@5 100.00 ( 99.71)
 * Acc@1 90.640 Acc@5 99.720
### epoch[70] execution time: 28.477352619171143
EPOCH 71
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [71][  0/391]	Time  0.251 ( 0.251)	Data  0.186 ( 0.186)	Loss 7.8613e-02 (7.8613e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [71][ 10/391]	Time  0.063 ( 0.082)	Data  0.001 ( 0.018)	Loss 1.1780e-01 (1.0449e-01)	Acc@1  95.31 ( 96.52)	Acc@5 100.00 ( 99.93)
Epoch: [71][ 20/391]	Time  0.065 ( 0.074)	Data  0.001 ( 0.010)	Loss 1.2109e-01 (1.0616e-01)	Acc@1  96.09 ( 96.35)	Acc@5 100.00 ( 99.96)
Epoch: [71][ 30/391]	Time  0.063 ( 0.071)	Data  0.001 ( 0.007)	Loss 1.1188e-01 (1.0910e-01)	Acc@1  96.09 ( 96.14)	Acc@5 100.00 ( 99.97)
Epoch: [71][ 40/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.006)	Loss 1.3733e-01 (1.0612e-01)	Acc@1  96.09 ( 96.25)	Acc@5 100.00 ( 99.98)
Epoch: [71][ 50/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.005)	Loss 8.0261e-02 (1.0646e-01)	Acc@1  96.09 ( 96.26)	Acc@5 100.00 ( 99.98)
Epoch: [71][ 60/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.1273e-01 (1.0931e-01)	Acc@1  96.09 ( 96.14)	Acc@5 100.00 ( 99.97)
Epoch: [71][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.0571e-01 (1.0853e-01)	Acc@1  96.88 ( 96.16)	Acc@5 100.00 ( 99.98)
Epoch: [71][ 80/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.9612e-02 (1.0775e-01)	Acc@1  98.44 ( 96.20)	Acc@5 100.00 ( 99.98)
Epoch: [71][ 90/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.0596e-01 (1.0711e-01)	Acc@1  96.09 ( 96.21)	Acc@5 100.00 ( 99.97)
Epoch: [71][100/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.2610e-01 (1.0742e-01)	Acc@1  94.53 ( 96.16)	Acc@5 100.00 ( 99.97)
Epoch: [71][110/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.2070e-01 (1.0686e-01)	Acc@1  90.62 ( 96.17)	Acc@5  99.22 ( 99.96)
Epoch: [71][120/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 8.8684e-02 (1.0594e-01)	Acc@1  96.09 ( 96.19)	Acc@5 100.00 ( 99.96)
Epoch: [71][130/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.7343e-02 (1.0625e-01)	Acc@1  97.66 ( 96.18)	Acc@5 100.00 ( 99.96)
Epoch: [71][140/391]	Time  0.072 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.7961e-02 (1.0645e-01)	Acc@1  97.66 ( 96.21)	Acc@5 100.00 ( 99.96)
Epoch: [71][150/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1053e-01 (1.0492e-01)	Acc@1  96.09 ( 96.25)	Acc@5 100.00 ( 99.96)
Epoch: [71][160/391]	Time  0.059 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.9285e-02 (1.0645e-01)	Acc@1  96.09 ( 96.21)	Acc@5 100.00 ( 99.96)
Epoch: [71][170/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0529e-01 (1.0594e-01)	Acc@1  96.88 ( 96.27)	Acc@5  99.22 ( 99.96)
Epoch: [71][180/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.8074e-02 (1.0592e-01)	Acc@1  96.88 ( 96.24)	Acc@5 100.00 ( 99.96)
Epoch: [71][190/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.4106e-02 (1.0609e-01)	Acc@1  97.66 ( 96.25)	Acc@5 100.00 ( 99.95)
Epoch: [71][200/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3818e-01 (1.0637e-01)	Acc@1  93.75 ( 96.22)	Acc@5 100.00 ( 99.95)
Epoch: [71][210/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.9233e-02 (1.0578e-01)	Acc@1  95.31 ( 96.24)	Acc@5 100.00 ( 99.95)
Epoch: [71][220/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.5012e-02 (1.0545e-01)	Acc@1  97.66 ( 96.25)	Acc@5 100.00 ( 99.95)
Epoch: [71][230/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1279e-01 (1.0471e-01)	Acc@1  97.66 ( 96.28)	Acc@5  98.44 ( 99.95)
Epoch: [71][240/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.3274e-02 (1.0513e-01)	Acc@1  98.44 ( 96.25)	Acc@5 100.00 ( 99.95)
Epoch: [71][250/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.7098e-02 (1.0528e-01)	Acc@1  97.66 ( 96.22)	Acc@5 100.00 ( 99.95)
Epoch: [71][260/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.9712e-02 (1.0556e-01)	Acc@1  96.88 ( 96.20)	Acc@5 100.00 ( 99.95)
Epoch: [71][270/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7529e-01 (1.0602e-01)	Acc@1  92.97 ( 96.17)	Acc@5 100.00 ( 99.95)
Epoch: [71][280/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.9722e-02 (1.0644e-01)	Acc@1  97.66 ( 96.15)	Acc@5 100.00 ( 99.95)
Epoch: [71][290/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2378e-01 (1.0667e-01)	Acc@1  92.97 ( 96.13)	Acc@5 100.00 ( 99.95)
Epoch: [71][300/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0120e-01 (1.0658e-01)	Acc@1  96.09 ( 96.15)	Acc@5 100.00 ( 99.95)
Epoch: [71][310/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.4321e-02 (1.0645e-01)	Acc@1  99.22 ( 96.16)	Acc@5 100.00 ( 99.95)
Epoch: [71][320/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.9783e-02 (1.0637e-01)	Acc@1  97.66 ( 96.19)	Acc@5 100.00 ( 99.95)
Epoch: [71][330/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.1310e-02 (1.0606e-01)	Acc@1  98.44 ( 96.20)	Acc@5 100.00 ( 99.96)
Epoch: [71][340/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1414e-01 (1.0622e-01)	Acc@1  95.31 ( 96.20)	Acc@5 100.00 ( 99.95)
Epoch: [71][350/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1957e-01 (1.0619e-01)	Acc@1  95.31 ( 96.21)	Acc@5 100.00 ( 99.95)
Epoch: [71][360/391]	Time  0.073 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.6721e-02 (1.0617e-01)	Acc@1  99.22 ( 96.22)	Acc@5 100.00 ( 99.95)
Epoch: [71][370/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0455e-01 (1.0663e-01)	Acc@1  94.53 ( 96.21)	Acc@5 100.00 ( 99.96)
Epoch: [71][380/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8323e-01 (1.0676e-01)	Acc@1  92.97 ( 96.19)	Acc@5 100.00 ( 99.96)
Epoch: [71][390/391]	Time  0.048 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5479e-01 (1.0704e-01)	Acc@1  92.50 ( 96.18)	Acc@5 100.00 ( 99.96)
## e[71] optimizer.zero_grad (sum) time: 0.3871772289276123
## e[71]       loss.backward (sum) time: 7.2661473751068115
## e[71]      optimizer.step (sum) time: 3.448335647583008
## epoch[71] training(only) time: 25.599460124969482
# Switched to evaluate mode...
Test: [  0/100]	Time  0.191 ( 0.191)	Loss 3.0078e-01 (3.0078e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.040)	Loss 3.8184e-01 (3.2929e-01)	Acc@1  89.00 ( 89.73)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.027 ( 0.033)	Loss 3.5522e-01 (3.2068e-01)	Acc@1  88.00 ( 89.95)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.024 ( 0.030)	Loss 2.5635e-01 (3.1793e-01)	Acc@1  88.00 ( 90.13)	Acc@5 100.00 ( 99.55)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 3.8208e-01 (3.2238e-01)	Acc@1  89.00 ( 90.17)	Acc@5  99.00 ( 99.49)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 2.2314e-01 (3.2063e-01)	Acc@1  92.00 ( 90.20)	Acc@5 100.00 ( 99.57)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 1.8726e-01 (3.1243e-01)	Acc@1  93.00 ( 90.48)	Acc@5 100.00 ( 99.59)
Test: [ 70/100]	Time  0.024 ( 0.027)	Loss 4.1919e-01 (3.1340e-01)	Acc@1  87.00 ( 90.39)	Acc@5 100.00 ( 99.63)
Test: [ 80/100]	Time  0.027 ( 0.027)	Loss 1.3257e-01 (3.1463e-01)	Acc@1  95.00 ( 90.43)	Acc@5 100.00 ( 99.63)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 2.3718e-01 (3.1324e-01)	Acc@1  89.00 ( 90.52)	Acc@5 100.00 ( 99.64)
 * Acc@1 90.570 Acc@5 99.650
### epoch[71] execution time: 28.424882888793945
EPOCH 72
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [72][  0/391]	Time  0.254 ( 0.254)	Data  0.186 ( 0.186)	Loss 1.1023e-01 (1.1023e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
Epoch: [72][ 10/391]	Time  0.068 ( 0.082)	Data  0.001 ( 0.018)	Loss 9.6924e-02 (1.1507e-01)	Acc@1  98.44 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [72][ 20/391]	Time  0.065 ( 0.074)	Data  0.001 ( 0.010)	Loss 6.7505e-02 (1.0425e-01)	Acc@1  96.88 ( 96.35)	Acc@5 100.00 (100.00)
Epoch: [72][ 30/391]	Time  0.063 ( 0.071)	Data  0.001 ( 0.007)	Loss 1.3391e-01 (1.0256e-01)	Acc@1  97.66 ( 96.35)	Acc@5 100.00 (100.00)
Epoch: [72][ 40/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.006)	Loss 1.1963e-01 (1.0502e-01)	Acc@1  96.09 ( 96.25)	Acc@5 100.00 (100.00)
Epoch: [72][ 50/391]	Time  0.063 ( 0.069)	Data  0.001 ( 0.005)	Loss 1.0107e-01 (1.0410e-01)	Acc@1  96.09 ( 96.35)	Acc@5 100.00 (100.00)
Epoch: [72][ 60/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.004)	Loss 2.2278e-01 (1.0420e-01)	Acc@1  92.19 ( 96.31)	Acc@5 100.00 (100.00)
Epoch: [72][ 70/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.8542e-01 (1.0604e-01)	Acc@1  94.53 ( 96.30)	Acc@5 100.00 ( 99.99)
Epoch: [72][ 80/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 8.7219e-02 (1.0316e-01)	Acc@1  96.09 ( 96.42)	Acc@5 100.00 ( 99.99)
Epoch: [72][ 90/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.3671e-02 (1.0225e-01)	Acc@1 100.00 ( 96.49)	Acc@5 100.00 ( 99.99)
Epoch: [72][100/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 9.1553e-02 (1.0189e-01)	Acc@1  97.66 ( 96.52)	Acc@5 100.00 ( 99.99)
Epoch: [72][110/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.5320e-01 (1.0232e-01)	Acc@1  95.31 ( 96.56)	Acc@5 100.00 ( 99.99)
Epoch: [72][120/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.0974e-01 (1.0072e-01)	Acc@1  96.88 ( 96.64)	Acc@5 100.00 ( 99.99)
Epoch: [72][130/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.3435e-02 (1.0134e-01)	Acc@1  96.88 ( 96.59)	Acc@5 100.00 ( 99.99)
Epoch: [72][140/391]	Time  0.073 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.9060e-02 (1.0071e-01)	Acc@1  96.09 ( 96.59)	Acc@5 100.00 ( 99.99)
Epoch: [72][150/391]	Time  0.059 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.2703e-02 (1.0005e-01)	Acc@1  96.88 ( 96.63)	Acc@5 100.00 ( 99.99)
Epoch: [72][160/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5466e-01 (1.0102e-01)	Acc@1  95.31 ( 96.57)	Acc@5 100.00 (100.00)
Epoch: [72][170/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.8796e-02 (1.0137e-01)	Acc@1  96.88 ( 96.53)	Acc@5 100.00 ( 99.99)
Epoch: [72][180/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0455e-01 (1.0177e-01)	Acc@1  95.31 ( 96.47)	Acc@5 100.00 ( 99.99)
Epoch: [72][190/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.0852e-02 (1.0076e-01)	Acc@1  98.44 ( 96.50)	Acc@5 100.00 ( 99.99)
Epoch: [72][200/391]	Time  0.070 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3123e-01 (1.0075e-01)	Acc@1  95.31 ( 96.49)	Acc@5 100.00 ( 99.99)
Epoch: [72][210/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1877e-01 (1.0119e-01)	Acc@1  96.88 ( 96.47)	Acc@5 100.00 ( 99.99)
Epoch: [72][220/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.3425e-01 (1.0198e-01)	Acc@1  91.41 ( 96.41)	Acc@5 100.00 ( 99.99)
Epoch: [72][230/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.5410e-02 (1.0106e-01)	Acc@1  99.22 ( 96.44)	Acc@5 100.00 ( 99.99)
Epoch: [72][240/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7004e-01 (1.0201e-01)	Acc@1  94.53 ( 96.42)	Acc@5 100.00 ( 99.99)
Epoch: [72][250/391]	Time  0.070 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1591e-01 (1.0188e-01)	Acc@1  93.75 ( 96.42)	Acc@5 100.00 ( 99.99)
Epoch: [72][260/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.9529e-02 (1.0118e-01)	Acc@1  96.09 ( 96.45)	Acc@5 100.00 ( 99.99)
Epoch: [72][270/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.0811e-02 (1.0083e-01)	Acc@1  96.09 ( 96.46)	Acc@5 100.00 ( 99.99)
Epoch: [72][280/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.2021e-02 (1.0070e-01)	Acc@1  97.66 ( 96.46)	Acc@5 100.00 ( 99.99)
Epoch: [72][290/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.3018e-02 (1.0102e-01)	Acc@1  96.88 ( 96.46)	Acc@5 100.00 ( 99.98)
Epoch: [72][300/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1206e-01 (1.0146e-01)	Acc@1  96.09 ( 96.42)	Acc@5 100.00 ( 99.98)
Epoch: [72][310/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.6589e-02 (1.0120e-01)	Acc@1  97.66 ( 96.44)	Acc@5 100.00 ( 99.98)
Epoch: [72][320/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.0730e-02 (1.0137e-01)	Acc@1  97.66 ( 96.43)	Acc@5 100.00 ( 99.99)
Epoch: [72][330/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.9540e-02 (1.0169e-01)	Acc@1  97.66 ( 96.41)	Acc@5 100.00 ( 99.98)
Epoch: [72][340/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2732e-01 (1.0145e-01)	Acc@1  96.88 ( 96.44)	Acc@5 100.00 ( 99.98)
Epoch: [72][350/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1469e-01 (1.0186e-01)	Acc@1  95.31 ( 96.41)	Acc@5 100.00 ( 99.98)
Epoch: [72][360/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.9490e-02 (1.0141e-01)	Acc@1  99.22 ( 96.44)	Acc@5 100.00 ( 99.98)
Epoch: [72][370/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2817e-01 (1.0098e-01)	Acc@1  94.53 ( 96.46)	Acc@5 100.00 ( 99.98)
Epoch: [72][380/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8323e-01 (1.0105e-01)	Acc@1  94.53 ( 96.44)	Acc@5 100.00 ( 99.98)
Epoch: [72][390/391]	Time  0.047 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2115e-01 (1.0151e-01)	Acc@1  97.50 ( 96.44)	Acc@5 100.00 ( 99.98)
## e[72] optimizer.zero_grad (sum) time: 0.39536166191101074
## e[72]       loss.backward (sum) time: 7.276015043258667
## e[72]      optimizer.step (sum) time: 3.4474151134490967
## epoch[72] training(only) time: 25.693870306015015
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 3.0347e-01 (3.0347e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.039)	Loss 4.1504e-01 (3.3137e-01)	Acc@1  89.00 ( 89.82)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 3.5938e-01 (3.2367e-01)	Acc@1  86.00 ( 89.95)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.024 ( 0.030)	Loss 2.8052e-01 (3.2126e-01)	Acc@1  88.00 ( 90.16)	Acc@5 100.00 ( 99.58)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 3.9575e-01 (3.2393e-01)	Acc@1  88.00 ( 90.24)	Acc@5  99.00 ( 99.54)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 2.2107e-01 (3.2228e-01)	Acc@1  92.00 ( 90.25)	Acc@5 100.00 ( 99.61)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 2.0386e-01 (3.1546e-01)	Acc@1  95.00 ( 90.52)	Acc@5 100.00 ( 99.66)
Test: [ 70/100]	Time  0.027 ( 0.027)	Loss 4.3237e-01 (3.1616e-01)	Acc@1  87.00 ( 90.48)	Acc@5 100.00 ( 99.69)
Test: [ 80/100]	Time  0.030 ( 0.027)	Loss 1.3501e-01 (3.1709e-01)	Acc@1  95.00 ( 90.46)	Acc@5 100.00 ( 99.69)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 2.4695e-01 (3.1521e-01)	Acc@1  90.00 ( 90.59)	Acc@5 100.00 ( 99.69)
 * Acc@1 90.630 Acc@5 99.700
### epoch[72] execution time: 28.46167755126953
EPOCH 73
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [73][  0/391]	Time  0.238 ( 0.238)	Data  0.173 ( 0.173)	Loss 1.2280e-01 (1.2280e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [73][ 10/391]	Time  0.064 ( 0.079)	Data  0.001 ( 0.017)	Loss 1.0669e-01 (9.2074e-02)	Acc@1  96.88 ( 96.66)	Acc@5 100.00 (100.00)
Epoch: [73][ 20/391]	Time  0.064 ( 0.072)	Data  0.001 ( 0.009)	Loss 1.6895e-01 (1.0342e-01)	Acc@1  92.19 ( 96.32)	Acc@5 100.00 (100.00)
Epoch: [73][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.007)	Loss 8.3130e-02 (1.0371e-01)	Acc@1  96.88 ( 96.30)	Acc@5 100.00 (100.00)
Epoch: [73][ 40/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.005)	Loss 6.2134e-02 (9.9579e-02)	Acc@1  98.44 ( 96.49)	Acc@5 100.00 (100.00)
Epoch: [73][ 50/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.3074e-01 (1.0245e-01)	Acc@1  95.31 ( 96.46)	Acc@5 100.00 ( 99.98)
Epoch: [73][ 60/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.7432e-01 (1.0170e-01)	Acc@1  95.31 ( 96.55)	Acc@5 100.00 ( 99.99)
Epoch: [73][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.0992e-01 (1.0378e-01)	Acc@1  96.09 ( 96.43)	Acc@5 100.00 ( 99.98)
Epoch: [73][ 80/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 9.4971e-02 (1.0281e-01)	Acc@1  98.44 ( 96.49)	Acc@5 100.00 ( 99.98)
Epoch: [73][ 90/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.3245e-01 (1.0243e-01)	Acc@1  96.09 ( 96.51)	Acc@5 100.00 ( 99.98)
Epoch: [73][100/391]	Time  0.071 ( 0.067)	Data  0.001 ( 0.003)	Loss 8.7952e-02 (1.0175e-01)	Acc@1  96.88 ( 96.53)	Acc@5 100.00 ( 99.98)
Epoch: [73][110/391]	Time  0.060 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.0364e-01 (1.0335e-01)	Acc@1  96.09 ( 96.40)	Acc@5 100.00 ( 99.99)
Epoch: [73][120/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.003)	Loss 8.5938e-02 (1.0421e-01)	Acc@1  97.66 ( 96.41)	Acc@5 100.00 ( 99.98)
Epoch: [73][130/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1853e-01 (1.0454e-01)	Acc@1  96.88 ( 96.38)	Acc@5 100.00 ( 99.98)
Epoch: [73][140/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0205e-01 (1.0486e-01)	Acc@1  96.88 ( 96.39)	Acc@5 100.00 ( 99.98)
Epoch: [73][150/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.9417e-02 (1.0381e-01)	Acc@1  97.66 ( 96.44)	Acc@5 100.00 ( 99.98)
Epoch: [73][160/391]	Time  0.070 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5576e-01 (1.0414e-01)	Acc@1  94.53 ( 96.42)	Acc@5 100.00 ( 99.99)
Epoch: [73][170/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.3669e-02 (1.0479e-01)	Acc@1  96.88 ( 96.39)	Acc@5 100.00 ( 99.99)
Epoch: [73][180/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.0486e-02 (1.0292e-01)	Acc@1  99.22 ( 96.46)	Acc@5 100.00 ( 99.99)
Epoch: [73][190/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8469e-01 (1.0336e-01)	Acc@1  92.97 ( 96.43)	Acc@5 100.00 ( 99.99)
Epoch: [73][200/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4734e-01 (1.0438e-01)	Acc@1  95.31 ( 96.39)	Acc@5 100.00 ( 99.99)
Epoch: [73][210/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1102e-01 (1.0453e-01)	Acc@1  95.31 ( 96.38)	Acc@5 100.00 ( 99.99)
Epoch: [73][220/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.0872e-02 (1.0489e-01)	Acc@1  98.44 ( 96.37)	Acc@5 100.00 ( 99.99)
Epoch: [73][230/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.8196e-02 (1.0507e-01)	Acc@1  96.88 ( 96.36)	Acc@5 100.00 ( 99.99)
Epoch: [73][240/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0980e-01 (1.0539e-01)	Acc@1  96.09 ( 96.34)	Acc@5 100.00 ( 99.99)
Epoch: [73][250/391]	Time  0.070 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.5928e-02 (1.0547e-01)	Acc@1  98.44 ( 96.33)	Acc@5 100.00 ( 99.99)
Epoch: [73][260/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4197e-01 (1.0507e-01)	Acc@1  92.97 ( 96.32)	Acc@5 100.00 ( 99.99)
Epoch: [73][270/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.3750e-02 (1.0549e-01)	Acc@1  96.88 ( 96.30)	Acc@5 100.00 ( 99.99)
Epoch: [73][280/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.6539e-02 (1.0528e-01)	Acc@1  99.22 ( 96.30)	Acc@5 100.00 ( 99.99)
Epoch: [73][290/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.5979e-02 (1.0524e-01)	Acc@1  98.44 ( 96.31)	Acc@5 100.00 ( 99.99)
Epoch: [73][300/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.2754e-02 (1.0469e-01)	Acc@1  97.66 ( 96.31)	Acc@5 100.00 ( 99.99)
Epoch: [73][310/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2659e-01 (1.0495e-01)	Acc@1  95.31 ( 96.32)	Acc@5 100.00 ( 99.98)
Epoch: [73][320/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1041e-01 (1.0495e-01)	Acc@1  95.31 ( 96.32)	Acc@5 100.00 ( 99.99)
Epoch: [73][330/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2537e-01 (1.0529e-01)	Acc@1  93.75 ( 96.29)	Acc@5 100.00 ( 99.98)
Epoch: [73][340/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.6548e-02 (1.0545e-01)	Acc@1  96.88 ( 96.29)	Acc@5 100.00 ( 99.98)
Epoch: [73][350/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2250e-01 (1.0555e-01)	Acc@1  97.66 ( 96.27)	Acc@5 100.00 ( 99.98)
Epoch: [73][360/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.7871e-02 (1.0503e-01)	Acc@1  97.66 ( 96.29)	Acc@5 100.00 ( 99.98)
Epoch: [73][370/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.6853e-02 (1.0478e-01)	Acc@1  97.66 ( 96.30)	Acc@5 100.00 ( 99.99)
Epoch: [73][380/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.7087e-02 (1.0481e-01)	Acc@1  96.88 ( 96.30)	Acc@5 100.00 ( 99.99)
Epoch: [73][390/391]	Time  0.048 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0736e-01 (1.0431e-01)	Acc@1  97.50 ( 96.33)	Acc@5 100.00 ( 99.99)
## e[73] optimizer.zero_grad (sum) time: 0.3915073871612549
## e[73]       loss.backward (sum) time: 7.217334985733032
## e[73]      optimizer.step (sum) time: 3.5167288780212402
## epoch[73] training(only) time: 25.582849979400635
# Switched to evaluate mode...
Test: [  0/100]	Time  0.190 ( 0.190)	Loss 2.9956e-01 (2.9956e-01)	Acc@1  86.00 ( 86.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.040)	Loss 3.7500e-01 (3.2429e-01)	Acc@1  89.00 ( 90.00)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 3.5718e-01 (3.1836e-01)	Acc@1  86.00 ( 90.05)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.026 ( 0.031)	Loss 2.7026e-01 (3.1866e-01)	Acc@1  89.00 ( 90.23)	Acc@5 100.00 ( 99.61)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 3.7329e-01 (3.2155e-01)	Acc@1  89.00 ( 90.22)	Acc@5  99.00 ( 99.54)
Test: [ 50/100]	Time  0.025 ( 0.028)	Loss 2.1350e-01 (3.1914e-01)	Acc@1  93.00 ( 90.24)	Acc@5 100.00 ( 99.61)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 1.9312e-01 (3.1200e-01)	Acc@1  94.00 ( 90.48)	Acc@5 100.00 ( 99.62)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 4.1479e-01 (3.1222e-01)	Acc@1  88.00 ( 90.41)	Acc@5 100.00 ( 99.65)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 1.5002e-01 (3.1321e-01)	Acc@1  94.00 ( 90.41)	Acc@5 100.00 ( 99.65)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 2.4927e-01 (3.1100e-01)	Acc@1  89.00 ( 90.52)	Acc@5 100.00 ( 99.66)
 * Acc@1 90.550 Acc@5 99.670
### epoch[73] execution time: 28.37881898880005
EPOCH 74
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [74][  0/391]	Time  0.251 ( 0.251)	Data  0.183 ( 0.183)	Loss 1.5552e-01 (1.5552e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
Epoch: [74][ 10/391]	Time  0.067 ( 0.082)	Data  0.001 ( 0.018)	Loss 1.2769e-01 (1.1146e-01)	Acc@1  95.31 ( 95.60)	Acc@5 100.00 ( 99.93)
Epoch: [74][ 20/391]	Time  0.065 ( 0.074)	Data  0.001 ( 0.010)	Loss 7.2388e-02 (1.0333e-01)	Acc@1  97.66 ( 96.28)	Acc@5 100.00 ( 99.96)
Epoch: [74][ 30/391]	Time  0.067 ( 0.071)	Data  0.001 ( 0.007)	Loss 6.6406e-02 (1.0010e-01)	Acc@1  97.66 ( 96.47)	Acc@5 100.00 ( 99.95)
Epoch: [74][ 40/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.005)	Loss 1.0614e-01 (9.6931e-02)	Acc@1  97.66 ( 96.68)	Acc@5 100.00 ( 99.96)
Epoch: [74][ 50/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.005)	Loss 9.9548e-02 (1.0214e-01)	Acc@1  96.88 ( 96.57)	Acc@5 100.00 ( 99.97)
Epoch: [74][ 60/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.004)	Loss 3.8971e-02 (1.0156e-01)	Acc@1 100.00 ( 96.57)	Acc@5 100.00 ( 99.97)
Epoch: [74][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.8762e-01 (1.0275e-01)	Acc@1  92.97 ( 96.53)	Acc@5  99.22 ( 99.97)
Epoch: [74][ 80/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.0022e-01 (1.0374e-01)	Acc@1  97.66 ( 96.51)	Acc@5 100.00 ( 99.96)
Epoch: [74][ 90/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.2988e-01 (1.0191e-01)	Acc@1  96.09 ( 96.57)	Acc@5 100.00 ( 99.97)
Epoch: [74][100/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 8.7280e-02 (1.0207e-01)	Acc@1  97.66 ( 96.59)	Acc@5 100.00 ( 99.97)
Epoch: [74][110/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 9.2224e-02 (9.9762e-02)	Acc@1  97.66 ( 96.70)	Acc@5 100.00 ( 99.97)
Epoch: [74][120/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.4832e-01 (1.0158e-01)	Acc@1  92.19 ( 96.57)	Acc@5 100.00 ( 99.97)
Epoch: [74][130/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.3323e-02 (1.0079e-01)	Acc@1  97.66 ( 96.59)	Acc@5 100.00 ( 99.98)
Epoch: [74][140/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3220e-01 (1.0140e-01)	Acc@1  96.09 ( 96.55)	Acc@5 100.00 ( 99.97)
Epoch: [74][150/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.3130e-02 (1.0088e-01)	Acc@1  97.66 ( 96.53)	Acc@5 100.00 ( 99.97)
Epoch: [74][160/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4307e-01 (1.0012e-01)	Acc@1  95.31 ( 96.58)	Acc@5 100.00 ( 99.98)
Epoch: [74][170/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4490e-01 (1.0013e-01)	Acc@1  94.53 ( 96.58)	Acc@5 100.00 ( 99.98)
Epoch: [74][180/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1334e-01 (1.0023e-01)	Acc@1  96.09 ( 96.56)	Acc@5 100.00 ( 99.97)
Epoch: [74][190/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2482e-01 (1.0053e-01)	Acc@1  96.09 ( 96.56)	Acc@5 100.00 ( 99.98)
Epoch: [74][200/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.2773e-02 (1.0080e-01)	Acc@1  96.88 ( 96.52)	Acc@5 100.00 ( 99.98)
Epoch: [74][210/391]	Time  0.077 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1969e-01 (1.0147e-01)	Acc@1  93.75 ( 96.51)	Acc@5 100.00 ( 99.97)
Epoch: [74][220/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.2764e-02 (1.0214e-01)	Acc@1  96.88 ( 96.47)	Acc@5 100.00 ( 99.98)
Epoch: [74][230/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.3762e-02 (1.0136e-01)	Acc@1  99.22 ( 96.52)	Acc@5 100.00 ( 99.98)
Epoch: [74][240/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1151e-01 (1.0165e-01)	Acc@1  96.09 ( 96.50)	Acc@5 100.00 ( 99.98)
Epoch: [74][250/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0040e-01 (1.0129e-01)	Acc@1  97.66 ( 96.51)	Acc@5 100.00 ( 99.98)
Epoch: [74][260/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.5439e-02 (1.0163e-01)	Acc@1  96.88 ( 96.47)	Acc@5 100.00 ( 99.98)
Epoch: [74][270/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.8745e-02 (1.0166e-01)	Acc@1  96.88 ( 96.46)	Acc@5 100.00 ( 99.97)
Epoch: [74][280/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.9224e-02 (1.0187e-01)	Acc@1  96.88 ( 96.46)	Acc@5 100.00 ( 99.97)
Epoch: [74][290/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4966e-01 (1.0263e-01)	Acc@1  96.09 ( 96.43)	Acc@5 100.00 ( 99.97)
Epoch: [74][300/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.8889e-02 (1.0233e-01)	Acc@1  99.22 ( 96.44)	Acc@5 100.00 ( 99.97)
Epoch: [74][310/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.2346e-02 (1.0233e-01)	Acc@1  96.09 ( 96.43)	Acc@5 100.00 ( 99.97)
Epoch: [74][320/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6553e-01 (1.0233e-01)	Acc@1  93.75 ( 96.43)	Acc@5 100.00 ( 99.97)
Epoch: [74][330/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1340e-01 (1.0280e-01)	Acc@1  96.88 ( 96.42)	Acc@5 100.00 ( 99.97)
Epoch: [74][340/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0846e-01 (1.0275e-01)	Acc@1  96.09 ( 96.41)	Acc@5 100.00 ( 99.97)
Epoch: [74][350/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.3782e-02 (1.0245e-01)	Acc@1  97.66 ( 96.42)	Acc@5 100.00 ( 99.97)
Epoch: [74][360/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.6172e-02 (1.0177e-01)	Acc@1  96.88 ( 96.45)	Acc@5 100.00 ( 99.97)
Epoch: [74][370/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.5134e-02 (1.0159e-01)	Acc@1  96.88 ( 96.46)	Acc@5 100.00 ( 99.97)
Epoch: [74][380/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.7036e-02 (1.0197e-01)	Acc@1  97.66 ( 96.45)	Acc@5 100.00 ( 99.97)
Epoch: [74][390/391]	Time  0.046 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2500e-01 (1.0203e-01)	Acc@1  97.50 ( 96.45)	Acc@5 100.00 ( 99.97)
## e[74] optimizer.zero_grad (sum) time: 0.3890085220336914
## e[74]       loss.backward (sum) time: 7.237890958786011
## e[74]      optimizer.step (sum) time: 3.4706082344055176
## epoch[74] training(only) time: 25.611202001571655
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 2.8564e-01 (2.8564e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.040)	Loss 4.0112e-01 (3.3059e-01)	Acc@1  90.00 ( 89.73)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.024 ( 0.032)	Loss 3.5254e-01 (3.2229e-01)	Acc@1  87.00 ( 89.90)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.024 ( 0.030)	Loss 2.5854e-01 (3.1914e-01)	Acc@1  89.00 ( 90.16)	Acc@5 100.00 ( 99.61)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 3.7769e-01 (3.2184e-01)	Acc@1  89.00 ( 90.20)	Acc@5  99.00 ( 99.56)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 2.2168e-01 (3.2107e-01)	Acc@1  92.00 ( 90.22)	Acc@5  99.00 ( 99.63)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 1.9385e-01 (3.1383e-01)	Acc@1  94.00 ( 90.43)	Acc@5 100.00 ( 99.64)
Test: [ 70/100]	Time  0.024 ( 0.027)	Loss 4.4434e-01 (3.1426e-01)	Acc@1  87.00 ( 90.37)	Acc@5 100.00 ( 99.68)
Test: [ 80/100]	Time  0.025 ( 0.027)	Loss 1.2915e-01 (3.1493e-01)	Acc@1  95.00 ( 90.40)	Acc@5 100.00 ( 99.68)
Test: [ 90/100]	Time  0.027 ( 0.027)	Loss 2.5122e-01 (3.1327e-01)	Acc@1  89.00 ( 90.48)	Acc@5 100.00 ( 99.68)
 * Acc@1 90.560 Acc@5 99.690
### epoch[74] execution time: 28.412769317626953
EPOCH 75
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [75][  0/391]	Time  0.241 ( 0.241)	Data  0.176 ( 0.176)	Loss 9.8083e-02 (9.8083e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [75][ 10/391]	Time  0.060 ( 0.080)	Data  0.001 ( 0.017)	Loss 1.4307e-01 (9.9229e-02)	Acc@1  96.09 ( 96.95)	Acc@5 100.00 ( 99.93)
Epoch: [75][ 20/391]	Time  0.063 ( 0.073)	Data  0.001 ( 0.009)	Loss 1.0333e-01 (1.0621e-01)	Acc@1  96.09 ( 96.47)	Acc@5 100.00 ( 99.96)
Epoch: [75][ 30/391]	Time  0.064 ( 0.071)	Data  0.001 ( 0.007)	Loss 1.7871e-01 (1.0625e-01)	Acc@1  94.53 ( 96.45)	Acc@5 100.00 ( 99.97)
Epoch: [75][ 40/391]	Time  0.062 ( 0.069)	Data  0.001 ( 0.005)	Loss 9.3811e-02 (1.0394e-01)	Acc@1  96.09 ( 96.44)	Acc@5 100.00 ( 99.98)
Epoch: [75][ 50/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.2201e-01 (1.0463e-01)	Acc@1  95.31 ( 96.51)	Acc@5 100.00 ( 99.98)
Epoch: [75][ 60/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.004)	Loss 7.0984e-02 (1.0385e-01)	Acc@1  97.66 ( 96.50)	Acc@5 100.00 ( 99.99)
Epoch: [75][ 70/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.004)	Loss 6.0516e-02 (1.0400e-01)	Acc@1  99.22 ( 96.47)	Acc@5 100.00 ( 99.98)
Epoch: [75][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 9.1980e-02 (1.0507e-01)	Acc@1  96.88 ( 96.41)	Acc@5 100.00 ( 99.98)
Epoch: [75][ 90/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 7.3364e-02 (1.0402e-01)	Acc@1  97.66 ( 96.45)	Acc@5 100.00 ( 99.98)
Epoch: [75][100/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.3708e-01 (1.0297e-01)	Acc@1  95.31 ( 96.49)	Acc@5 100.00 ( 99.98)
Epoch: [75][110/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 7.6660e-02 (1.0110e-01)	Acc@1  96.88 ( 96.55)	Acc@5 100.00 ( 99.99)
Epoch: [75][120/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 6.3110e-02 (9.9339e-02)	Acc@1  97.66 ( 96.59)	Acc@5 100.00 ( 99.99)
Epoch: [75][130/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2939e-01 (1.0052e-01)	Acc@1  95.31 ( 96.54)	Acc@5 100.00 ( 99.99)
Epoch: [75][140/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.3313e-02 (1.0063e-01)	Acc@1  96.88 ( 96.51)	Acc@5 100.00 ( 99.98)
Epoch: [75][150/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1597e-01 (1.0116e-01)	Acc@1  96.09 ( 96.51)	Acc@5 100.00 ( 99.98)
Epoch: [75][160/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3940e-01 (1.0174e-01)	Acc@1  92.97 ( 96.45)	Acc@5 100.00 ( 99.98)
Epoch: [75][170/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.8427e-02 (1.0080e-01)	Acc@1 100.00 ( 96.51)	Acc@5 100.00 ( 99.98)
Epoch: [75][180/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6101e-01 (1.0101e-01)	Acc@1  94.53 ( 96.50)	Acc@5 100.00 ( 99.98)
Epoch: [75][190/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3342e-01 (1.0134e-01)	Acc@1  93.75 ( 96.46)	Acc@5 100.00 ( 99.98)
Epoch: [75][200/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.9529e-02 (1.0121e-01)	Acc@1  97.66 ( 96.47)	Acc@5 100.00 ( 99.98)
Epoch: [75][210/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.5613e-02 (1.0147e-01)	Acc@1  98.44 ( 96.46)	Acc@5 100.00 ( 99.98)
Epoch: [75][220/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4160e-01 (1.0189e-01)	Acc@1  96.09 ( 96.46)	Acc@5 100.00 ( 99.98)
Epoch: [75][230/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1481e-01 (1.0248e-01)	Acc@1  95.31 ( 96.44)	Acc@5 100.00 ( 99.98)
Epoch: [75][240/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7712e-01 (1.0340e-01)	Acc@1  95.31 ( 96.42)	Acc@5  99.22 ( 99.97)
Epoch: [75][250/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.0496e-02 (1.0339e-01)	Acc@1  97.66 ( 96.42)	Acc@5 100.00 ( 99.98)
Epoch: [75][260/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3965e-01 (1.0295e-01)	Acc@1  95.31 ( 96.45)	Acc@5 100.00 ( 99.98)
Epoch: [75][270/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.4821e-02 (1.0251e-01)	Acc@1  98.44 ( 96.45)	Acc@5 100.00 ( 99.98)
Epoch: [75][280/391]	Time  0.066 ( 0.065)	Data  0.002 ( 0.002)	Loss 1.2195e-01 (1.0264e-01)	Acc@1  96.88 ( 96.45)	Acc@5 100.00 ( 99.98)
Epoch: [75][290/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.7097e-02 (1.0261e-01)	Acc@1  96.09 ( 96.44)	Acc@5 100.00 ( 99.98)
Epoch: [75][300/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1389e-01 (1.0285e-01)	Acc@1  96.88 ( 96.44)	Acc@5 100.00 ( 99.98)
Epoch: [75][310/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.8735e-02 (1.0294e-01)	Acc@1  95.31 ( 96.42)	Acc@5 100.00 ( 99.98)
Epoch: [75][320/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.8450e-02 (1.0310e-01)	Acc@1  96.88 ( 96.42)	Acc@5 100.00 ( 99.98)
Epoch: [75][330/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.9163e-02 (1.0377e-01)	Acc@1  99.22 ( 96.40)	Acc@5 100.00 ( 99.98)
Epoch: [75][340/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1835e-01 (1.0368e-01)	Acc@1  97.66 ( 96.41)	Acc@5 100.00 ( 99.98)
Epoch: [75][350/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3794e-01 (1.0429e-01)	Acc@1  96.09 ( 96.39)	Acc@5  99.22 ( 99.97)
Epoch: [75][360/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1426e-01 (1.0439e-01)	Acc@1  95.31 ( 96.39)	Acc@5 100.00 ( 99.97)
Epoch: [75][370/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.9346e-02 (1.0410e-01)	Acc@1  96.09 ( 96.41)	Acc@5 100.00 ( 99.97)
Epoch: [75][380/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2213e-01 (1.0434e-01)	Acc@1  94.53 ( 96.39)	Acc@5 100.00 ( 99.97)
Epoch: [75][390/391]	Time  0.048 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3889e-01 (1.0502e-01)	Acc@1  91.25 ( 96.36)	Acc@5 100.00 ( 99.97)
## e[75] optimizer.zero_grad (sum) time: 0.3898324966430664
## e[75]       loss.backward (sum) time: 7.303062438964844
## e[75]      optimizer.step (sum) time: 3.426292896270752
## epoch[75] training(only) time: 25.60237765312195
# Switched to evaluate mode...
Test: [  0/100]	Time  0.191 ( 0.191)	Loss 3.0420e-01 (3.0420e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.040)	Loss 3.8354e-01 (3.3066e-01)	Acc@1  89.00 ( 90.00)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 3.5742e-01 (3.2192e-01)	Acc@1  86.00 ( 89.95)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.028 ( 0.031)	Loss 2.7197e-01 (3.2047e-01)	Acc@1  89.00 ( 90.16)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.024 ( 0.030)	Loss 3.9185e-01 (3.2414e-01)	Acc@1  88.00 ( 90.20)	Acc@5  99.00 ( 99.56)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 2.2266e-01 (3.2223e-01)	Acc@1  92.00 ( 90.10)	Acc@5 100.00 ( 99.63)
Test: [ 60/100]	Time  0.027 ( 0.028)	Loss 1.9006e-01 (3.1410e-01)	Acc@1  94.00 ( 90.38)	Acc@5 100.00 ( 99.66)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 4.2651e-01 (3.1419e-01)	Acc@1  88.00 ( 90.34)	Acc@5 100.00 ( 99.68)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.3721e-01 (3.1533e-01)	Acc@1  95.00 ( 90.32)	Acc@5 100.00 ( 99.67)
Test: [ 90/100]	Time  0.025 ( 0.027)	Loss 2.5928e-01 (3.1385e-01)	Acc@1  89.00 ( 90.45)	Acc@5 100.00 ( 99.67)
 * Acc@1 90.510 Acc@5 99.680
### epoch[75] execution time: 28.424490451812744
EPOCH 76
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [76][  0/391]	Time  0.237 ( 0.237)	Data  0.172 ( 0.172)	Loss 5.7251e-02 (5.7251e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [76][ 10/391]	Time  0.063 ( 0.080)	Data  0.001 ( 0.017)	Loss 9.7229e-02 (9.2773e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 ( 99.93)
Epoch: [76][ 20/391]	Time  0.064 ( 0.072)	Data  0.001 ( 0.009)	Loss 1.0046e-01 (9.1700e-02)	Acc@1  96.09 ( 96.73)	Acc@5 100.00 ( 99.96)
Epoch: [76][ 30/391]	Time  0.068 ( 0.070)	Data  0.001 ( 0.007)	Loss 1.0309e-01 (9.3277e-02)	Acc@1  96.09 ( 96.72)	Acc@5 100.00 ( 99.97)
Epoch: [76][ 40/391]	Time  0.062 ( 0.069)	Data  0.001 ( 0.005)	Loss 1.0150e-01 (9.5294e-02)	Acc@1  96.88 ( 96.72)	Acc@5 100.00 ( 99.98)
Epoch: [76][ 50/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.004)	Loss 3.9886e-02 (9.6966e-02)	Acc@1  98.44 ( 96.69)	Acc@5 100.00 ( 99.97)
Epoch: [76][ 60/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.004)	Loss 7.9651e-02 (9.7651e-02)	Acc@1  96.09 ( 96.54)	Acc@5 100.00 ( 99.97)
Epoch: [76][ 70/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.2177e-01 (9.8442e-02)	Acc@1  96.09 ( 96.57)	Acc@5 100.00 ( 99.97)
Epoch: [76][ 80/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.3416e-01 (9.8505e-02)	Acc@1  93.75 ( 96.50)	Acc@5 100.00 ( 99.97)
Epoch: [76][ 90/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.7302e-02 (9.8647e-02)	Acc@1  98.44 ( 96.45)	Acc@5 100.00 ( 99.97)
Epoch: [76][100/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.003)	Loss 6.5308e-02 (9.8987e-02)	Acc@1  97.66 ( 96.48)	Acc@5 100.00 ( 99.98)
Epoch: [76][110/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.003)	Loss 6.8481e-02 (9.8125e-02)	Acc@1  98.44 ( 96.49)	Acc@5  99.22 ( 99.97)
Epoch: [76][120/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1334e-01 (9.9045e-02)	Acc@1  96.88 ( 96.49)	Acc@5 100.00 ( 99.97)
Epoch: [76][130/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5637e-01 (9.9546e-02)	Acc@1  96.09 ( 96.48)	Acc@5 100.00 ( 99.98)
Epoch: [76][140/391]	Time  0.070 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.3262e-02 (1.0020e-01)	Acc@1  97.66 ( 96.45)	Acc@5  99.22 ( 99.97)
Epoch: [76][150/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0205e-01 (1.0110e-01)	Acc@1  96.88 ( 96.44)	Acc@5 100.00 ( 99.97)
Epoch: [76][160/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.1370e-02 (1.0177e-01)	Acc@1  98.44 ( 96.40)	Acc@5 100.00 ( 99.98)
Epoch: [76][170/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3245e-01 (1.0280e-01)	Acc@1  94.53 ( 96.33)	Acc@5 100.00 ( 99.98)
Epoch: [76][180/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4270e-01 (1.0326e-01)	Acc@1  96.09 ( 96.33)	Acc@5  99.22 ( 99.97)
Epoch: [76][190/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.0200e-02 (1.0238e-01)	Acc@1  96.88 ( 96.36)	Acc@5 100.00 ( 99.98)
Epoch: [76][200/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1444e-01 (1.0383e-01)	Acc@1  95.31 ( 96.28)	Acc@5 100.00 ( 99.97)
Epoch: [76][210/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.7769e-02 (1.0480e-01)	Acc@1  96.88 ( 96.26)	Acc@5 100.00 ( 99.97)
Epoch: [76][220/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0730e-01 (1.0389e-01)	Acc@1  96.88 ( 96.30)	Acc@5 100.00 ( 99.97)
Epoch: [76][230/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1322e-01 (1.0354e-01)	Acc@1  93.75 ( 96.30)	Acc@5 100.00 ( 99.97)
Epoch: [76][240/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.3120e-02 (1.0461e-01)	Acc@1  96.88 ( 96.24)	Acc@5 100.00 ( 99.97)
Epoch: [76][250/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1389e-01 (1.0458e-01)	Acc@1  95.31 ( 96.22)	Acc@5 100.00 ( 99.97)
Epoch: [76][260/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.8369e-02 (1.0455e-01)	Acc@1  96.88 ( 96.21)	Acc@5 100.00 ( 99.97)
Epoch: [76][270/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6138e-01 (1.0519e-01)	Acc@1  96.88 ( 96.21)	Acc@5 100.00 ( 99.97)
Epoch: [76][280/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2769e-01 (1.0537e-01)	Acc@1  94.53 ( 96.20)	Acc@5 100.00 ( 99.97)
Epoch: [76][290/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3818e-01 (1.0569e-01)	Acc@1  93.75 ( 96.19)	Acc@5 100.00 ( 99.98)
Epoch: [76][300/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.3018e-02 (1.0509e-01)	Acc@1  96.09 ( 96.22)	Acc@5 100.00 ( 99.98)
Epoch: [76][310/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2683e-01 (1.0531e-01)	Acc@1  95.31 ( 96.22)	Acc@5 100.00 ( 99.98)
Epoch: [76][320/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.0872e-02 (1.0541e-01)	Acc@1  96.09 ( 96.21)	Acc@5 100.00 ( 99.98)
Epoch: [76][330/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0339e-01 (1.0512e-01)	Acc@1  96.88 ( 96.24)	Acc@5 100.00 ( 99.98)
Epoch: [76][340/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.5084e-02 (1.0505e-01)	Acc@1  97.66 ( 96.23)	Acc@5 100.00 ( 99.98)
Epoch: [76][350/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.5195e-02 (1.0537e-01)	Acc@1  96.09 ( 96.22)	Acc@5 100.00 ( 99.98)
Epoch: [76][360/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.3984e-02 (1.0548e-01)	Acc@1  95.31 ( 96.21)	Acc@5 100.00 ( 99.98)
Epoch: [76][370/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.2673e-02 (1.0554e-01)	Acc@1  97.66 ( 96.22)	Acc@5 100.00 ( 99.97)
Epoch: [76][380/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.4463e-02 (1.0505e-01)	Acc@1  97.66 ( 96.25)	Acc@5 100.00 ( 99.97)
Epoch: [76][390/391]	Time  0.048 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.5603e-02 (1.0510e-01)	Acc@1  97.50 ( 96.25)	Acc@5 100.00 ( 99.97)
## e[76] optimizer.zero_grad (sum) time: 0.38745856285095215
## e[76]       loss.backward (sum) time: 7.240772008895874
## e[76]      optimizer.step (sum) time: 3.4603281021118164
## epoch[76] training(only) time: 25.63633131980896
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 2.7734e-01 (2.7734e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.040)	Loss 3.8525e-01 (3.3212e-01)	Acc@1  89.00 ( 90.18)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 3.6304e-01 (3.2266e-01)	Acc@1  85.00 ( 90.19)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.024 ( 0.030)	Loss 2.6025e-01 (3.1986e-01)	Acc@1  88.00 ( 90.45)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.027 ( 0.029)	Loss 3.7915e-01 (3.2326e-01)	Acc@1  89.00 ( 90.37)	Acc@5  99.00 ( 99.61)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 2.3108e-01 (3.2317e-01)	Acc@1  92.00 ( 90.31)	Acc@5  99.00 ( 99.65)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 1.9934e-01 (3.1525e-01)	Acc@1  94.00 ( 90.51)	Acc@5 100.00 ( 99.66)
Test: [ 70/100]	Time  0.027 ( 0.027)	Loss 4.3628e-01 (3.1560e-01)	Acc@1  88.00 ( 90.51)	Acc@5 100.00 ( 99.69)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 1.3330e-01 (3.1625e-01)	Acc@1  94.00 ( 90.51)	Acc@5 100.00 ( 99.68)
Test: [ 90/100]	Time  0.027 ( 0.027)	Loss 2.3914e-01 (3.1458e-01)	Acc@1  89.00 ( 90.56)	Acc@5 100.00 ( 99.69)
 * Acc@1 90.690 Acc@5 99.690
### epoch[76] execution time: 28.43856453895569
EPOCH 77
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [77][  0/391]	Time  0.261 ( 0.261)	Data  0.191 ( 0.191)	Loss 1.2103e-01 (1.2103e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [77][ 10/391]	Time  0.064 ( 0.081)	Data  0.001 ( 0.018)	Loss 1.1920e-01 (1.0282e-01)	Acc@1  97.66 ( 96.80)	Acc@5 100.00 ( 99.93)
Epoch: [77][ 20/391]	Time  0.063 ( 0.073)	Data  0.001 ( 0.010)	Loss 6.8176e-02 (9.8531e-02)	Acc@1  98.44 ( 96.95)	Acc@5 100.00 ( 99.96)
Epoch: [77][ 30/391]	Time  0.062 ( 0.071)	Data  0.001 ( 0.007)	Loss 9.0027e-02 (9.8418e-02)	Acc@1  96.88 ( 96.70)	Acc@5 100.00 ( 99.97)
Epoch: [77][ 40/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.006)	Loss 7.1533e-02 (9.5364e-02)	Acc@1  97.66 ( 96.65)	Acc@5 100.00 ( 99.96)
Epoch: [77][ 50/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.005)	Loss 6.4575e-02 (1.0058e-01)	Acc@1  98.44 ( 96.40)	Acc@5 100.00 ( 99.94)
Epoch: [77][ 60/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.0199e-01 (1.0046e-01)	Acc@1  96.88 ( 96.31)	Acc@5 100.00 ( 99.95)
Epoch: [77][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 8.7036e-02 (1.0070e-01)	Acc@1  95.31 ( 96.36)	Acc@5 100.00 ( 99.94)
Epoch: [77][ 80/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.2732e-01 (9.9604e-02)	Acc@1  94.53 ( 96.45)	Acc@5 100.00 ( 99.95)
Epoch: [77][ 90/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.4062e-01 (9.9639e-02)	Acc@1  94.53 ( 96.48)	Acc@5 100.00 ( 99.96)
Epoch: [77][100/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.003)	Loss 9.1003e-02 (1.0102e-01)	Acc@1  98.44 ( 96.47)	Acc@5 100.00 ( 99.96)
Epoch: [77][110/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 7.7637e-02 (1.0265e-01)	Acc@1  96.88 ( 96.40)	Acc@5 100.00 ( 99.96)
Epoch: [77][120/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.5845e-01 (1.0333e-01)	Acc@1  96.88 ( 96.39)	Acc@5 100.00 ( 99.96)
Epoch: [77][130/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.0803e-01 (1.0224e-01)	Acc@1  96.09 ( 96.43)	Acc@5 100.00 ( 99.96)
Epoch: [77][140/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.7280e-02 (1.0225e-01)	Acc@1  96.09 ( 96.42)	Acc@5 100.00 ( 99.96)
Epoch: [77][150/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.7290e-02 (1.0353e-01)	Acc@1  97.66 ( 96.40)	Acc@5 100.00 ( 99.96)
Epoch: [77][160/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2268e-01 (1.0397e-01)	Acc@1  96.88 ( 96.39)	Acc@5 100.00 ( 99.97)
Epoch: [77][170/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5210e-01 (1.0485e-01)	Acc@1  92.19 ( 96.33)	Acc@5 100.00 ( 99.96)
Epoch: [77][180/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.5276e-02 (1.0519e-01)	Acc@1  95.31 ( 96.27)	Acc@5 100.00 ( 99.97)
Epoch: [77][190/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.0325e-01 (1.0543e-01)	Acc@1  91.41 ( 96.25)	Acc@5 100.00 ( 99.96)
Epoch: [77][200/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0004e-01 (1.0543e-01)	Acc@1  95.31 ( 96.25)	Acc@5 100.00 ( 99.97)
Epoch: [77][210/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.1584e-02 (1.0647e-01)	Acc@1  97.66 ( 96.20)	Acc@5 100.00 ( 99.97)
Epoch: [77][220/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.7168e-02 (1.0673e-01)	Acc@1  96.88 ( 96.19)	Acc@5 100.00 ( 99.97)
Epoch: [77][230/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.1421e-02 (1.0605e-01)	Acc@1  96.09 ( 96.20)	Acc@5 100.00 ( 99.97)
Epoch: [77][240/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.2205e-01 (1.0746e-01)	Acc@1  90.62 ( 96.14)	Acc@5 100.00 ( 99.97)
Epoch: [77][250/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5601e-01 (1.0718e-01)	Acc@1  95.31 ( 96.16)	Acc@5 100.00 ( 99.97)
Epoch: [77][260/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4026e-01 (1.0767e-01)	Acc@1  94.53 ( 96.13)	Acc@5 100.00 ( 99.97)
Epoch: [77][270/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5662e-01 (1.0719e-01)	Acc@1  94.53 ( 96.15)	Acc@5 100.00 ( 99.97)
Epoch: [77][280/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3098e-01 (1.0692e-01)	Acc@1  93.75 ( 96.17)	Acc@5 100.00 ( 99.97)
Epoch: [77][290/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.0698e-02 (1.0665e-01)	Acc@1  96.88 ( 96.18)	Acc@5 100.00 ( 99.97)
Epoch: [77][300/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.1462e-02 (1.0671e-01)	Acc@1  97.66 ( 96.19)	Acc@5 100.00 ( 99.97)
Epoch: [77][310/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.8379e-02 (1.0627e-01)	Acc@1  98.44 ( 96.20)	Acc@5 100.00 ( 99.97)
Epoch: [77][320/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1487e-01 (1.0614e-01)	Acc@1  97.66 ( 96.22)	Acc@5 100.00 ( 99.97)
Epoch: [77][330/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1328e-01 (1.0619e-01)	Acc@1  94.53 ( 96.20)	Acc@5 100.00 ( 99.97)
Epoch: [77][340/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.7140e-02 (1.0556e-01)	Acc@1  99.22 ( 96.22)	Acc@5 100.00 ( 99.97)
Epoch: [77][350/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.5308e-02 (1.0572e-01)	Acc@1  97.66 ( 96.23)	Acc@5 100.00 ( 99.97)
Epoch: [77][360/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3025e-01 (1.0615e-01)	Acc@1  94.53 ( 96.21)	Acc@5 100.00 ( 99.97)
Epoch: [77][370/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.0374e-02 (1.0621e-01)	Acc@1  96.88 ( 96.21)	Acc@5 100.00 ( 99.97)
Epoch: [77][380/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.7454e-02 (1.0645e-01)	Acc@1  98.44 ( 96.22)	Acc@5 100.00 ( 99.97)
Epoch: [77][390/391]	Time  0.047 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.1514e-02 (1.0625e-01)	Acc@1  98.75 ( 96.24)	Acc@5 100.00 ( 99.97)
## e[77] optimizer.zero_grad (sum) time: 0.39377832412719727
## e[77]       loss.backward (sum) time: 7.273858070373535
## e[77]      optimizer.step (sum) time: 3.4407477378845215
## epoch[77] training(only) time: 25.646095275878906
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 2.9297e-01 (2.9297e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.040)	Loss 3.9844e-01 (3.3519e-01)	Acc@1  89.00 ( 90.09)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 3.6353e-01 (3.2595e-01)	Acc@1  87.00 ( 90.10)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.024 ( 0.030)	Loss 2.6489e-01 (3.2288e-01)	Acc@1  89.00 ( 90.26)	Acc@5 100.00 ( 99.58)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 3.8135e-01 (3.2629e-01)	Acc@1  89.00 ( 90.29)	Acc@5 100.00 ( 99.54)
Test: [ 50/100]	Time  0.028 ( 0.028)	Loss 2.2473e-01 (3.2489e-01)	Acc@1  93.00 ( 90.24)	Acc@5 100.00 ( 99.63)
Test: [ 60/100]	Time  0.026 ( 0.028)	Loss 1.9373e-01 (3.1784e-01)	Acc@1  94.00 ( 90.41)	Acc@5 100.00 ( 99.66)
Test: [ 70/100]	Time  0.024 ( 0.028)	Loss 4.2017e-01 (3.1737e-01)	Acc@1  88.00 ( 90.37)	Acc@5 100.00 ( 99.69)
Test: [ 80/100]	Time  0.026 ( 0.027)	Loss 1.3184e-01 (3.1762e-01)	Acc@1  95.00 ( 90.40)	Acc@5 100.00 ( 99.67)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 2.5098e-01 (3.1547e-01)	Acc@1  89.00 ( 90.52)	Acc@5 100.00 ( 99.67)
 * Acc@1 90.610 Acc@5 99.680
### epoch[77] execution time: 28.480961322784424
EPOCH 78
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [78][  0/391]	Time  0.247 ( 0.247)	Data  0.183 ( 0.183)	Loss 1.0254e-01 (1.0254e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [78][ 10/391]	Time  0.064 ( 0.081)	Data  0.001 ( 0.018)	Loss 1.0809e-01 (1.1293e-01)	Acc@1  96.09 ( 95.74)	Acc@5 100.00 (100.00)
Epoch: [78][ 20/391]	Time  0.064 ( 0.073)	Data  0.001 ( 0.010)	Loss 1.2201e-01 (1.0769e-01)	Acc@1  96.09 ( 96.17)	Acc@5 100.00 ( 99.96)
Epoch: [78][ 30/391]	Time  0.060 ( 0.070)	Data  0.001 ( 0.007)	Loss 7.7698e-02 (1.0201e-01)	Acc@1  96.88 ( 96.40)	Acc@5 100.00 ( 99.97)
Epoch: [78][ 40/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.006)	Loss 7.1899e-02 (1.0507e-01)	Acc@1  97.66 ( 96.30)	Acc@5 100.00 ( 99.96)
Epoch: [78][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.005)	Loss 5.6091e-02 (1.0109e-01)	Acc@1  97.66 ( 96.38)	Acc@5 100.00 ( 99.97)
Epoch: [78][ 60/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.004)	Loss 7.8186e-02 (1.0147e-01)	Acc@1  96.88 ( 96.34)	Acc@5 100.00 ( 99.97)
Epoch: [78][ 70/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.004)	Loss 7.7209e-02 (9.8861e-02)	Acc@1  98.44 ( 96.49)	Acc@5 100.00 ( 99.98)
Epoch: [78][ 80/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.6162e-01 (9.9432e-02)	Acc@1  95.31 ( 96.53)	Acc@5 100.00 ( 99.97)
Epoch: [78][ 90/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.1694e-01 (1.0056e-01)	Acc@1  96.09 ( 96.47)	Acc@5 100.00 ( 99.97)
Epoch: [78][100/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 8.8196e-02 (1.0163e-01)	Acc@1  96.88 ( 96.46)	Acc@5 100.00 ( 99.97)
Epoch: [78][110/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.1566e-01 (1.0128e-01)	Acc@1  95.31 ( 96.45)	Acc@5 100.00 ( 99.97)
Epoch: [78][120/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.1084e-01 (1.0094e-01)	Acc@1  96.88 ( 96.47)	Acc@5 100.00 ( 99.97)
Epoch: [78][130/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0767e-01 (1.0141e-01)	Acc@1  96.09 ( 96.48)	Acc@5 100.00 ( 99.98)
Epoch: [78][140/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3440e-01 (1.0075e-01)	Acc@1  95.31 ( 96.51)	Acc@5 100.00 ( 99.98)
Epoch: [78][150/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.8003e-02 (9.9317e-02)	Acc@1  97.66 ( 96.57)	Acc@5 100.00 ( 99.98)
Epoch: [78][160/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1975e-01 (9.9708e-02)	Acc@1  96.09 ( 96.56)	Acc@5 100.00 ( 99.98)
Epoch: [78][170/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.8635e-02 (9.9506e-02)	Acc@1 100.00 ( 96.55)	Acc@5 100.00 ( 99.97)
Epoch: [78][180/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5552e-01 (9.9505e-02)	Acc@1  96.09 ( 96.53)	Acc@5 100.00 ( 99.97)
Epoch: [78][190/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.0516e-02 (9.9996e-02)	Acc@1  97.66 ( 96.48)	Acc@5 100.00 ( 99.98)
Epoch: [78][200/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.1472e-02 (9.9974e-02)	Acc@1  98.44 ( 96.51)	Acc@5 100.00 ( 99.98)
Epoch: [78][210/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.0608e-02 (1.0022e-01)	Acc@1  99.22 ( 96.47)	Acc@5 100.00 ( 99.98)
Epoch: [78][220/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.4463e-02 (9.9308e-02)	Acc@1  98.44 ( 96.51)	Acc@5 100.00 ( 99.98)
Epoch: [78][230/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0413e-01 (9.9114e-02)	Acc@1  95.31 ( 96.51)	Acc@5 100.00 ( 99.98)
Epoch: [78][240/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.7891e-02 (9.8803e-02)	Acc@1  94.53 ( 96.51)	Acc@5 100.00 ( 99.98)
Epoch: [78][250/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4612e-01 (9.9423e-02)	Acc@1  93.75 ( 96.50)	Acc@5 100.00 ( 99.98)
Epoch: [78][260/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.3191e-02 (9.9458e-02)	Acc@1  97.66 ( 96.48)	Acc@5 100.00 ( 99.98)
Epoch: [78][270/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5283e-01 (1.0073e-01)	Acc@1  96.88 ( 96.43)	Acc@5 100.00 ( 99.98)
Epoch: [78][280/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.2275e-02 (1.0035e-01)	Acc@1  98.44 ( 96.44)	Acc@5 100.00 ( 99.98)
Epoch: [78][290/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0114e-01 (1.0079e-01)	Acc@1  96.88 ( 96.44)	Acc@5 100.00 ( 99.98)
Epoch: [78][300/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.1919e-02 (1.0105e-01)	Acc@1  97.66 ( 96.44)	Acc@5 100.00 ( 99.98)
Epoch: [78][310/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2622e-01 (1.0087e-01)	Acc@1  95.31 ( 96.45)	Acc@5 100.00 ( 99.97)
Epoch: [78][320/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4417e-01 (1.0055e-01)	Acc@1  95.31 ( 96.47)	Acc@5 100.00 ( 99.98)
Epoch: [78][330/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3037e-01 (1.0092e-01)	Acc@1  93.75 ( 96.46)	Acc@5 100.00 ( 99.98)
Epoch: [78][340/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.2754e-02 (1.0096e-01)	Acc@1  97.66 ( 96.48)	Acc@5 100.00 ( 99.98)
Epoch: [78][350/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.3079e-02 (1.0098e-01)	Acc@1  96.88 ( 96.49)	Acc@5 100.00 ( 99.98)
Epoch: [78][360/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4575e-01 (1.0089e-01)	Acc@1  96.09 ( 96.48)	Acc@5 100.00 ( 99.98)
Epoch: [78][370/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.2510e-02 (1.0148e-01)	Acc@1  98.44 ( 96.46)	Acc@5 100.00 ( 99.98)
Epoch: [78][380/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0883e-01 (1.0179e-01)	Acc@1  96.09 ( 96.45)	Acc@5 100.00 ( 99.98)
Epoch: [78][390/391]	Time  0.055 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.0515e-02 (1.0149e-01)	Acc@1  93.75 ( 96.44)	Acc@5 100.00 ( 99.98)
## e[78] optimizer.zero_grad (sum) time: 0.3929474353790283
## e[78]       loss.backward (sum) time: 7.297710180282593
## e[78]      optimizer.step (sum) time: 3.465932607650757
## epoch[78] training(only) time: 25.692606687545776
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 2.8955e-01 (2.8955e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.027 ( 0.040)	Loss 3.8257e-01 (3.3057e-01)	Acc@1  89.00 ( 90.18)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 3.6792e-01 (3.2221e-01)	Acc@1  86.00 ( 90.19)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.024 ( 0.030)	Loss 2.6685e-01 (3.2051e-01)	Acc@1  89.00 ( 90.42)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 3.8818e-01 (3.2467e-01)	Acc@1  89.00 ( 90.34)	Acc@5 100.00 ( 99.59)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 2.2888e-01 (3.2337e-01)	Acc@1  93.00 ( 90.35)	Acc@5 100.00 ( 99.67)
Test: [ 60/100]	Time  0.026 ( 0.028)	Loss 2.0996e-01 (3.1599e-01)	Acc@1  94.00 ( 90.57)	Acc@5 100.00 ( 99.67)
Test: [ 70/100]	Time  0.027 ( 0.027)	Loss 4.2212e-01 (3.1607e-01)	Acc@1  89.00 ( 90.52)	Acc@5 100.00 ( 99.70)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 1.3293e-01 (3.1700e-01)	Acc@1  95.00 ( 90.53)	Acc@5 100.00 ( 99.69)
Test: [ 90/100]	Time  0.025 ( 0.027)	Loss 2.5806e-01 (3.1496e-01)	Acc@1  89.00 ( 90.66)	Acc@5 100.00 ( 99.69)
 * Acc@1 90.720 Acc@5 99.700
### epoch[78] execution time: 28.50642466545105
EPOCH 79
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [79][  0/391]	Time  0.244 ( 0.244)	Data  0.176 ( 0.176)	Loss 4.8126e-02 (4.8126e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [79][ 10/391]	Time  0.067 ( 0.081)	Data  0.001 ( 0.017)	Loss 6.1340e-02 (7.5589e-02)	Acc@1  97.66 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [79][ 20/391]	Time  0.067 ( 0.074)	Data  0.001 ( 0.009)	Loss 1.0309e-01 (8.5182e-02)	Acc@1  97.66 ( 96.73)	Acc@5 100.00 (100.00)
Epoch: [79][ 30/391]	Time  0.067 ( 0.071)	Data  0.001 ( 0.007)	Loss 4.8157e-02 (9.1943e-02)	Acc@1  97.66 ( 96.55)	Acc@5 100.00 (100.00)
Epoch: [79][ 40/391]	Time  0.065 ( 0.070)	Data  0.001 ( 0.005)	Loss 1.1566e-01 (9.6249e-02)	Acc@1  96.09 ( 96.49)	Acc@5 100.00 (100.00)
Epoch: [79][ 50/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.004)	Loss 1.0553e-01 (9.8057e-02)	Acc@1  96.09 ( 96.40)	Acc@5 100.00 (100.00)
Epoch: [79][ 60/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.5784e-01 (9.8151e-02)	Acc@1  95.31 ( 96.47)	Acc@5 100.00 (100.00)
Epoch: [79][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 4.5441e-02 (9.8592e-02)	Acc@1  99.22 ( 96.41)	Acc@5 100.00 (100.00)
Epoch: [79][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.3354e-01 (9.9654e-02)	Acc@1  92.97 ( 96.34)	Acc@5 100.00 (100.00)
Epoch: [79][ 90/391]	Time  0.061 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.4697e-01 (1.0135e-01)	Acc@1  95.31 ( 96.28)	Acc@5 100.00 ( 99.99)
Epoch: [79][100/391]	Time  0.071 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.2115e-01 (1.0257e-01)	Acc@1  95.31 ( 96.21)	Acc@5 100.00 ( 99.99)
Epoch: [79][110/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.1469e-01 (1.0277e-01)	Acc@1  96.09 ( 96.20)	Acc@5 100.00 ( 99.99)
Epoch: [79][120/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.0107e-01 (1.0369e-01)	Acc@1  96.09 ( 96.18)	Acc@5 100.00 ( 99.99)
Epoch: [79][130/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3635e-01 (1.0525e-01)	Acc@1  95.31 ( 96.10)	Acc@5 100.00 ( 99.99)
Epoch: [79][140/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.5032e-02 (1.0532e-01)	Acc@1  97.66 ( 96.09)	Acc@5 100.00 ( 99.99)
Epoch: [79][150/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.9182e-02 (1.0536e-01)	Acc@1  96.88 ( 96.12)	Acc@5 100.00 ( 99.99)
Epoch: [79][160/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1987e-01 (1.0428e-01)	Acc@1  94.53 ( 96.14)	Acc@5 100.00 ( 99.99)
Epoch: [79][170/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1047e-01 (1.0393e-01)	Acc@1  93.75 ( 96.15)	Acc@5 100.00 ( 99.99)
Epoch: [79][180/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.9783e-02 (1.0292e-01)	Acc@1  96.88 ( 96.21)	Acc@5 100.00 ( 99.99)
Epoch: [79][190/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.6284e-02 (1.0294e-01)	Acc@1  99.22 ( 96.23)	Acc@5 100.00 ( 99.99)
Epoch: [79][200/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.3843e-02 (1.0347e-01)	Acc@1  98.44 ( 96.20)	Acc@5 100.00 ( 99.99)
Epoch: [79][210/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.5022e-02 (1.0297e-01)	Acc@1  97.66 ( 96.24)	Acc@5 100.00 ( 99.99)
Epoch: [79][220/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.9661e-02 (1.0268e-01)	Acc@1  96.09 ( 96.24)	Acc@5 100.00 ( 99.99)
Epoch: [79][230/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.8135e-02 (1.0257e-01)	Acc@1  96.09 ( 96.25)	Acc@5 100.00 ( 99.99)
Epoch: [79][240/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.5815e-02 (1.0212e-01)	Acc@1  96.88 ( 96.28)	Acc@5 100.00 ( 99.99)
Epoch: [79][250/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.4158e-02 (1.0218e-01)	Acc@1  98.44 ( 96.29)	Acc@5 100.00 ( 99.99)
Epoch: [79][260/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.8684e-02 (1.0187e-01)	Acc@1  97.66 ( 96.32)	Acc@5 100.00 ( 99.99)
Epoch: [79][270/391]	Time  0.060 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.0627e-02 (1.0244e-01)	Acc@1  96.88 ( 96.29)	Acc@5 100.00 ( 99.98)
Epoch: [79][280/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6211e-01 (1.0327e-01)	Acc@1  91.41 ( 96.25)	Acc@5  99.22 ( 99.98)
Epoch: [79][290/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.3628e-02 (1.0283e-01)	Acc@1  96.09 ( 96.26)	Acc@5 100.00 ( 99.98)
Epoch: [79][300/391]	Time  0.060 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.5520e-02 (1.0237e-01)	Acc@1  98.44 ( 96.28)	Acc@5 100.00 ( 99.98)
Epoch: [79][310/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.8298e-02 (1.0193e-01)	Acc@1  96.88 ( 96.31)	Acc@5 100.00 ( 99.98)
Epoch: [79][320/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0382e-01 (1.0195e-01)	Acc@1  95.31 ( 96.29)	Acc@5 100.00 ( 99.98)
Epoch: [79][330/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.3669e-02 (1.0172e-01)	Acc@1  97.66 ( 96.30)	Acc@5 100.00 ( 99.98)
Epoch: [79][340/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.9731e-02 (1.0201e-01)	Acc@1  96.09 ( 96.29)	Acc@5 100.00 ( 99.98)
Epoch: [79][350/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5942e-01 (1.0258e-01)	Acc@1  92.19 ( 96.27)	Acc@5 100.00 ( 99.98)
Epoch: [79][360/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.8918e-02 (1.0312e-01)	Acc@1  96.88 ( 96.26)	Acc@5 100.00 ( 99.98)
Epoch: [79][370/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9031e-01 (1.0309e-01)	Acc@1  93.75 ( 96.28)	Acc@5 100.00 ( 99.98)
Epoch: [79][380/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.4576e-02 (1.0280e-01)	Acc@1  99.22 ( 96.29)	Acc@5 100.00 ( 99.98)
Epoch: [79][390/391]	Time  0.049 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.2876e-02 (1.0265e-01)	Acc@1  97.50 ( 96.30)	Acc@5 100.00 ( 99.98)
## e[79] optimizer.zero_grad (sum) time: 0.3889758586883545
## e[79]       loss.backward (sum) time: 7.238043785095215
## e[79]      optimizer.step (sum) time: 3.483269453048706
## epoch[79] training(only) time: 25.68074369430542
# Switched to evaluate mode...
Test: [  0/100]	Time  0.196 ( 0.196)	Loss 2.9028e-01 (2.9028e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.033 ( 0.041)	Loss 3.8232e-01 (3.3113e-01)	Acc@1  89.00 ( 89.82)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.028 ( 0.033)	Loss 3.6743e-01 (3.2166e-01)	Acc@1  87.00 ( 89.86)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 2.6001e-01 (3.1910e-01)	Acc@1  89.00 ( 90.13)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 3.8330e-01 (3.2439e-01)	Acc@1  88.00 ( 90.15)	Acc@5 100.00 ( 99.59)
Test: [ 50/100]	Time  0.026 ( 0.028)	Loss 2.4121e-01 (3.2318e-01)	Acc@1  92.00 ( 90.18)	Acc@5  99.00 ( 99.63)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 1.9580e-01 (3.1527e-01)	Acc@1  94.00 ( 90.49)	Acc@5 100.00 ( 99.67)
Test: [ 70/100]	Time  0.028 ( 0.028)	Loss 4.2065e-01 (3.1656e-01)	Acc@1  87.00 ( 90.45)	Acc@5 100.00 ( 99.70)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 1.2744e-01 (3.1719e-01)	Acc@1  95.00 ( 90.46)	Acc@5 100.00 ( 99.69)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 2.3743e-01 (3.1549e-01)	Acc@1  89.00 ( 90.54)	Acc@5 100.00 ( 99.70)
 * Acc@1 90.620 Acc@5 99.720
### epoch[79] execution time: 28.48074960708618
EPOCH 80
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [80][  0/391]	Time  0.240 ( 0.240)	Data  0.175 ( 0.175)	Loss 1.1542e-01 (1.1542e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [80][ 10/391]	Time  0.063 ( 0.080)	Data  0.001 ( 0.017)	Loss 5.7739e-02 (7.8641e-02)	Acc@1  98.44 ( 97.02)	Acc@5 100.00 (100.00)
Epoch: [80][ 20/391]	Time  0.063 ( 0.073)	Data  0.001 ( 0.009)	Loss 9.9854e-02 (8.4651e-02)	Acc@1  96.88 ( 97.10)	Acc@5 100.00 (100.00)
Epoch: [80][ 30/391]	Time  0.065 ( 0.070)	Data  0.001 ( 0.007)	Loss 1.7163e-01 (8.7965e-02)	Acc@1  95.31 ( 96.98)	Acc@5 100.00 (100.00)
Epoch: [80][ 40/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.005)	Loss 8.3801e-02 (8.8925e-02)	Acc@1  98.44 ( 96.97)	Acc@5 100.00 (100.00)
Epoch: [80][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.005)	Loss 1.4978e-01 (9.1787e-02)	Acc@1  94.53 ( 96.69)	Acc@5 100.00 (100.00)
Epoch: [80][ 60/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.004)	Loss 6.4087e-02 (9.2223e-02)	Acc@1  96.88 ( 96.62)	Acc@5 100.00 (100.00)
Epoch: [80][ 70/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.004)	Loss 5.8807e-02 (9.1148e-02)	Acc@1  99.22 ( 96.62)	Acc@5 100.00 (100.00)
Epoch: [80][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 8.4839e-02 (9.3774e-02)	Acc@1  98.44 ( 96.56)	Acc@5 100.00 (100.00)
Epoch: [80][ 90/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 9.5276e-02 (9.4579e-02)	Acc@1  97.66 ( 96.55)	Acc@5 100.00 (100.00)
Epoch: [80][100/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.3416e-01 (9.5579e-02)	Acc@1  95.31 ( 96.53)	Acc@5  99.22 ( 99.99)
Epoch: [80][110/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.3098e-01 (9.5676e-02)	Acc@1  95.31 ( 96.55)	Acc@5 100.00 ( 99.99)
Epoch: [80][120/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 6.3171e-02 (9.6134e-02)	Acc@1  97.66 ( 96.54)	Acc@5 100.00 ( 99.99)
Epoch: [80][130/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.0964e-02 (9.6412e-02)	Acc@1  98.44 ( 96.57)	Acc@5 100.00 ( 99.98)
Epoch: [80][140/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.7699e-02 (9.5675e-02)	Acc@1  98.44 ( 96.61)	Acc@5 100.00 ( 99.98)
Epoch: [80][150/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7285e-01 (9.5682e-02)	Acc@1  95.31 ( 96.60)	Acc@5 100.00 ( 99.98)
Epoch: [80][160/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0077e-01 (9.5722e-02)	Acc@1  96.09 ( 96.60)	Acc@5 100.00 ( 99.99)
Epoch: [80][170/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8469e-01 (9.6949e-02)	Acc@1  95.31 ( 96.60)	Acc@5 100.00 ( 99.99)
Epoch: [80][180/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.5876e-02 (9.6316e-02)	Acc@1  94.53 ( 96.62)	Acc@5 100.00 ( 99.99)
Epoch: [80][190/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1194e-01 (9.6704e-02)	Acc@1  96.09 ( 96.61)	Acc@5 100.00 ( 99.99)
Epoch: [80][200/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3691e-02 (9.7285e-02)	Acc@1 100.00 ( 96.58)	Acc@5 100.00 ( 99.99)
Epoch: [80][210/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.9111e-02 (9.7263e-02)	Acc@1  97.66 ( 96.59)	Acc@5 100.00 ( 99.99)
Epoch: [80][220/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.3618e-02 (9.7846e-02)	Acc@1  96.09 ( 96.55)	Acc@5 100.00 ( 99.99)
Epoch: [80][230/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.4575e-02 (9.7714e-02)	Acc@1  98.44 ( 96.54)	Acc@5 100.00 ( 99.99)
Epoch: [80][240/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.7708e-02 (9.8239e-02)	Acc@1  97.66 ( 96.51)	Acc@5 100.00 ( 99.99)
Epoch: [80][250/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.7791e-02 (9.8141e-02)	Acc@1  98.44 ( 96.49)	Acc@5 100.00 ( 99.99)
Epoch: [80][260/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.4055e-02 (9.8579e-02)	Acc@1  95.31 ( 96.47)	Acc@5 100.00 ( 99.99)
Epoch: [80][270/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1823e-01 (9.8989e-02)	Acc@1  96.09 ( 96.45)	Acc@5 100.00 ( 99.99)
Epoch: [80][280/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5820e-01 (9.9839e-02)	Acc@1  92.19 ( 96.43)	Acc@5 100.00 ( 99.99)
Epoch: [80][290/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1176e-01 (1.0006e-01)	Acc@1  95.31 ( 96.41)	Acc@5 100.00 ( 99.99)
Epoch: [80][300/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.8989e-02 (1.0030e-01)	Acc@1  97.66 ( 96.40)	Acc@5 100.00 ( 99.99)
Epoch: [80][310/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.6782e-02 (1.0075e-01)	Acc@1  98.44 ( 96.38)	Acc@5 100.00 ( 99.99)
Epoch: [80][320/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.6914e-02 (1.0081e-01)	Acc@1  98.44 ( 96.39)	Acc@5 100.00 ( 99.99)
Epoch: [80][330/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.6050e-02 (1.0074e-01)	Acc@1  98.44 ( 96.40)	Acc@5 100.00 ( 99.99)
Epoch: [80][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.8909e-02 (1.0098e-01)	Acc@1  96.88 ( 96.38)	Acc@5 100.00 ( 99.99)
Epoch: [80][350/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0492e-01 (1.0156e-01)	Acc@1  96.88 ( 96.37)	Acc@5 100.00 ( 99.99)
Epoch: [80][360/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3110e-01 (1.0226e-01)	Acc@1  96.88 ( 96.35)	Acc@5 100.00 ( 99.99)
Epoch: [80][370/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.5703e-02 (1.0198e-01)	Acc@1  96.09 ( 96.36)	Acc@5 100.00 ( 99.99)
Epoch: [80][380/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.1533e-02 (1.0168e-01)	Acc@1  96.88 ( 96.37)	Acc@5 100.00 ( 99.99)
Epoch: [80][390/391]	Time  0.047 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.4238e-02 (1.0220e-01)	Acc@1  96.25 ( 96.36)	Acc@5 100.00 ( 99.99)
## e[80] optimizer.zero_grad (sum) time: 0.3872804641723633
## e[80]       loss.backward (sum) time: 7.234198331832886
## e[80]      optimizer.step (sum) time: 3.478059768676758
## epoch[80] training(only) time: 25.60252285003662
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 2.8223e-01 (2.8223e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.041)	Loss 4.1382e-01 (3.3397e-01)	Acc@1  89.00 ( 89.82)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.027 ( 0.034)	Loss 3.7109e-01 (3.2217e-01)	Acc@1  85.00 ( 89.90)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 2.6611e-01 (3.1992e-01)	Acc@1  88.00 ( 90.29)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 3.7402e-01 (3.2454e-01)	Acc@1  89.00 ( 90.22)	Acc@5  99.00 ( 99.63)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 2.5073e-01 (3.2340e-01)	Acc@1  91.00 ( 90.18)	Acc@5  99.00 ( 99.65)
Test: [ 60/100]	Time  0.026 ( 0.028)	Loss 1.8970e-01 (3.1665e-01)	Acc@1  94.00 ( 90.38)	Acc@5 100.00 ( 99.69)
Test: [ 70/100]	Time  0.024 ( 0.027)	Loss 4.2920e-01 (3.1670e-01)	Acc@1  88.00 ( 90.41)	Acc@5 100.00 ( 99.72)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 1.2378e-01 (3.1685e-01)	Acc@1  94.00 ( 90.46)	Acc@5 100.00 ( 99.70)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 2.4109e-01 (3.1467e-01)	Acc@1  89.00 ( 90.57)	Acc@5 100.00 ( 99.70)
 * Acc@1 90.680 Acc@5 99.710
### epoch[80] execution time: 28.38846254348755
EPOCH 81
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [81][  0/391]	Time  0.244 ( 0.244)	Data  0.180 ( 0.180)	Loss 5.2216e-02 (5.2216e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [81][ 10/391]	Time  0.073 ( 0.082)	Data  0.001 ( 0.017)	Loss 1.4087e-01 (9.8580e-02)	Acc@1  96.09 ( 96.59)	Acc@5 100.00 (100.00)
Epoch: [81][ 20/391]	Time  0.062 ( 0.074)	Data  0.001 ( 0.010)	Loss 1.3721e-01 (1.0471e-01)	Acc@1  95.31 ( 96.35)	Acc@5 100.00 (100.00)
Epoch: [81][ 30/391]	Time  0.065 ( 0.071)	Data  0.001 ( 0.007)	Loss 1.4697e-01 (1.0217e-01)	Acc@1  94.53 ( 96.22)	Acc@5 100.00 (100.00)
Epoch: [81][ 40/391]	Time  0.071 ( 0.069)	Data  0.001 ( 0.005)	Loss 7.7759e-02 (9.7108e-02)	Acc@1  97.66 ( 96.47)	Acc@5 100.00 (100.00)
Epoch: [81][ 50/391]	Time  0.059 ( 0.069)	Data  0.001 ( 0.005)	Loss 1.2500e-01 (9.7004e-02)	Acc@1  93.75 ( 96.48)	Acc@5 100.00 ( 99.98)
Epoch: [81][ 60/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.1499e-01 (1.0021e-01)	Acc@1  95.31 ( 96.35)	Acc@5 100.00 ( 99.99)
Epoch: [81][ 70/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.2878e-01 (1.0254e-01)	Acc@1  93.75 ( 96.25)	Acc@5 100.00 ( 99.99)
Epoch: [81][ 80/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.3306e-01 (1.0439e-01)	Acc@1  92.19 ( 96.19)	Acc@5 100.00 ( 99.99)
Epoch: [81][ 90/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 8.1299e-02 (1.0510e-01)	Acc@1  97.66 ( 96.09)	Acc@5 100.00 ( 99.99)
Epoch: [81][100/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.2067e-01 (1.0542e-01)	Acc@1  96.88 ( 96.13)	Acc@5 100.00 ( 99.98)
Epoch: [81][110/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.1719e-01 (1.0397e-01)	Acc@1  96.88 ( 96.23)	Acc@5 100.00 ( 99.98)
Epoch: [81][120/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 9.0698e-02 (1.0378e-01)	Acc@1  96.09 ( 96.22)	Acc@5 100.00 ( 99.97)
Epoch: [81][130/391]	Time  0.060 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.2339e-02 (1.0276e-01)	Acc@1 100.00 ( 96.31)	Acc@5 100.00 ( 99.98)
Epoch: [81][140/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.5573e-02 (1.0176e-01)	Acc@1  98.44 ( 96.34)	Acc@5 100.00 ( 99.98)
Epoch: [81][150/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.0059e-02 (1.0172e-01)	Acc@1  97.66 ( 96.35)	Acc@5 100.00 ( 99.98)
Epoch: [81][160/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.6985e-02 (1.0183e-01)	Acc@1  96.09 ( 96.34)	Acc@5 100.00 ( 99.98)
Epoch: [81][170/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1261e-01 (1.0175e-01)	Acc@1  94.53 ( 96.32)	Acc@5 100.00 ( 99.98)
Epoch: [81][180/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.7708e-02 (1.0180e-01)	Acc@1  96.09 ( 96.34)	Acc@5 100.00 ( 99.98)
Epoch: [81][190/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.2938e-02 (1.0247e-01)	Acc@1  99.22 ( 96.34)	Acc@5 100.00 ( 99.98)
Epoch: [81][200/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.5256e-02 (1.0194e-01)	Acc@1  97.66 ( 96.35)	Acc@5 100.00 ( 99.97)
Epoch: [81][210/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3159e-01 (1.0191e-01)	Acc@1  95.31 ( 96.37)	Acc@5 100.00 ( 99.97)
Epoch: [81][220/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.6111e-02 (1.0130e-01)	Acc@1  97.66 ( 96.40)	Acc@5 100.00 ( 99.97)
Epoch: [81][230/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.8563e-02 (1.0160e-01)	Acc@1  99.22 ( 96.39)	Acc@5 100.00 ( 99.97)
Epoch: [81][240/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.7505e-02 (1.0099e-01)	Acc@1  98.44 ( 96.42)	Acc@5 100.00 ( 99.97)
Epoch: [81][250/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.6252e-02 (1.0013e-01)	Acc@1  96.88 ( 96.45)	Acc@5 100.00 ( 99.97)
Epoch: [81][260/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.7993e-02 (9.9415e-02)	Acc@1  96.88 ( 96.48)	Acc@5 100.00 ( 99.97)
Epoch: [81][270/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3623e-01 (1.0003e-01)	Acc@1  94.53 ( 96.45)	Acc@5 100.00 ( 99.97)
Epoch: [81][280/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.6987e-02 (1.0010e-01)	Acc@1  99.22 ( 96.45)	Acc@5 100.00 ( 99.97)
Epoch: [81][290/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.5022e-02 (1.0023e-01)	Acc@1  97.66 ( 96.46)	Acc@5 100.00 ( 99.97)
Epoch: [81][300/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2500e-01 (1.0054e-01)	Acc@1  93.75 ( 96.43)	Acc@5 100.00 ( 99.97)
Epoch: [81][310/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.6619e-02 (1.0045e-01)	Acc@1  95.31 ( 96.44)	Acc@5 100.00 ( 99.97)
Epoch: [81][320/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.8186e-02 (9.9718e-02)	Acc@1  97.66 ( 96.47)	Acc@5 100.00 ( 99.97)
Epoch: [81][330/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3696e-01 (9.9859e-02)	Acc@1  95.31 ( 96.48)	Acc@5  99.22 ( 99.97)
Epoch: [81][340/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6870e-01 (1.0080e-01)	Acc@1  94.53 ( 96.44)	Acc@5 100.00 ( 99.97)
Epoch: [81][350/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.6609e-02 (1.0087e-01)	Acc@1  96.88 ( 96.45)	Acc@5 100.00 ( 99.97)
Epoch: [81][360/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0358e-01 (1.0107e-01)	Acc@1  96.88 ( 96.44)	Acc@5 100.00 ( 99.97)
Epoch: [81][370/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.3374e-02 (1.0128e-01)	Acc@1  96.88 ( 96.45)	Acc@5 100.00 ( 99.97)
Epoch: [81][380/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.7026e-02 (1.0084e-01)	Acc@1  96.09 ( 96.46)	Acc@5 100.00 ( 99.97)
Epoch: [81][390/391]	Time  0.051 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.6426e-02 (1.0149e-01)	Acc@1  97.50 ( 96.45)	Acc@5 100.00 ( 99.97)
## e[81] optimizer.zero_grad (sum) time: 0.38717007637023926
## e[81]       loss.backward (sum) time: 7.198592901229858
## e[81]      optimizer.step (sum) time: 3.5576789379119873
## epoch[81] training(only) time: 25.606197834014893
# Switched to evaluate mode...
Test: [  0/100]	Time  0.196 ( 0.196)	Loss 2.9736e-01 (2.9736e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.041)	Loss 3.8818e-01 (3.3331e-01)	Acc@1  89.00 ( 89.82)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.026 ( 0.034)	Loss 3.6914e-01 (3.2302e-01)	Acc@1  85.00 ( 89.81)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 2.7124e-01 (3.2029e-01)	Acc@1  89.00 ( 90.26)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 3.9429e-01 (3.2419e-01)	Acc@1  89.00 ( 90.27)	Acc@5  99.00 ( 99.56)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 2.3804e-01 (3.2307e-01)	Acc@1  92.00 ( 90.29)	Acc@5  99.00 ( 99.59)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 1.8628e-01 (3.1563e-01)	Acc@1  95.00 ( 90.56)	Acc@5 100.00 ( 99.64)
Test: [ 70/100]	Time  0.024 ( 0.027)	Loss 4.3774e-01 (3.1664e-01)	Acc@1  87.00 ( 90.46)	Acc@5 100.00 ( 99.68)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 1.2549e-01 (3.1719e-01)	Acc@1  95.00 ( 90.44)	Acc@5 100.00 ( 99.67)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 2.4329e-01 (3.1529e-01)	Acc@1  89.00 ( 90.55)	Acc@5 100.00 ( 99.67)
 * Acc@1 90.650 Acc@5 99.680
### epoch[81] execution time: 28.37941551208496
EPOCH 82
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [82][  0/391]	Time  0.239 ( 0.239)	Data  0.173 ( 0.173)	Loss 7.1533e-02 (7.1533e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [82][ 10/391]	Time  0.066 ( 0.080)	Data  0.001 ( 0.017)	Loss 1.0938e-01 (1.0195e-01)	Acc@1  96.09 ( 96.31)	Acc@5  99.22 ( 99.93)
Epoch: [82][ 20/391]	Time  0.062 ( 0.073)	Data  0.001 ( 0.009)	Loss 1.5698e-01 (9.7162e-02)	Acc@1  94.53 ( 96.54)	Acc@5 100.00 ( 99.96)
Epoch: [82][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.007)	Loss 3.2501e-02 (1.0501e-01)	Acc@1 100.00 ( 96.45)	Acc@5 100.00 ( 99.97)
Epoch: [82][ 40/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.005)	Loss 7.4219e-02 (1.0496e-01)	Acc@1  98.44 ( 96.47)	Acc@5 100.00 ( 99.98)
Epoch: [82][ 50/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.5588e-01 (1.0805e-01)	Acc@1  95.31 ( 96.35)	Acc@5 100.00 ( 99.98)
Epoch: [82][ 60/391]	Time  0.061 ( 0.067)	Data  0.001 ( 0.004)	Loss 7.8552e-02 (1.0522e-01)	Acc@1  96.09 ( 96.44)	Acc@5 100.00 ( 99.99)
Epoch: [82][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 6.2469e-02 (1.0371e-01)	Acc@1  96.88 ( 96.56)	Acc@5 100.00 ( 99.98)
Epoch: [82][ 80/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.6387e-02 (1.0444e-01)	Acc@1  99.22 ( 96.53)	Acc@5 100.00 ( 99.98)
Epoch: [82][ 90/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.4526e-01 (1.0501e-01)	Acc@1  94.53 ( 96.51)	Acc@5 100.00 ( 99.98)
Epoch: [82][100/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 6.1127e-02 (1.0466e-01)	Acc@1  98.44 ( 96.49)	Acc@5 100.00 ( 99.98)
Epoch: [82][110/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.0394e-01 (1.0309e-01)	Acc@1  96.09 ( 96.53)	Acc@5 100.00 ( 99.98)
Epoch: [82][120/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4880e-01 (1.0259e-01)	Acc@1  95.31 ( 96.53)	Acc@5 100.00 ( 99.97)
Epoch: [82][130/391]	Time  0.070 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3806e-01 (1.0348e-01)	Acc@1  96.09 ( 96.47)	Acc@5 100.00 ( 99.97)
Epoch: [82][140/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7322e-01 (1.0539e-01)	Acc@1  93.75 ( 96.38)	Acc@5 100.00 ( 99.97)
Epoch: [82][150/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2512e-01 (1.0574e-01)	Acc@1  96.09 ( 96.40)	Acc@5 100.00 ( 99.97)
Epoch: [82][160/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3892e-01 (1.0585e-01)	Acc@1  96.88 ( 96.44)	Acc@5 100.00 ( 99.97)
Epoch: [82][170/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.8420e-02 (1.0476e-01)	Acc@1  96.88 ( 96.47)	Acc@5 100.00 ( 99.97)
Epoch: [82][180/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4844e-01 (1.0363e-01)	Acc@1  96.88 ( 96.52)	Acc@5 100.00 ( 99.97)
Epoch: [82][190/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0864e-01 (1.0297e-01)	Acc@1  96.88 ( 96.54)	Acc@5 100.00 ( 99.98)
Epoch: [82][200/391]	Time  0.073 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.8125e-02 (1.0301e-01)	Acc@1  97.66 ( 96.54)	Acc@5 100.00 ( 99.98)
Epoch: [82][210/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3623e-01 (1.0356e-01)	Acc@1  92.97 ( 96.49)	Acc@5 100.00 ( 99.98)
Epoch: [82][220/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.4097e-02 (1.0299e-01)	Acc@1  96.88 ( 96.52)	Acc@5 100.00 ( 99.98)
Epoch: [82][230/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.1899e-02 (1.0231e-01)	Acc@1  97.66 ( 96.54)	Acc@5 100.00 ( 99.98)
Epoch: [82][240/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3477e-01 (1.0183e-01)	Acc@1  95.31 ( 96.57)	Acc@5 100.00 ( 99.98)
Epoch: [82][250/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1877e-01 (1.0202e-01)	Acc@1  95.31 ( 96.53)	Acc@5 100.00 ( 99.98)
Epoch: [82][260/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1053e-01 (1.0196e-01)	Acc@1  96.09 ( 96.54)	Acc@5 100.00 ( 99.98)
Epoch: [82][270/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.7932e-02 (1.0145e-01)	Acc@1  98.44 ( 96.55)	Acc@5 100.00 ( 99.98)
Epoch: [82][280/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1890e-01 (1.0132e-01)	Acc@1  96.88 ( 96.54)	Acc@5 100.00 ( 99.98)
Epoch: [82][290/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.0698e-02 (1.0138e-01)	Acc@1  98.44 ( 96.54)	Acc@5 100.00 ( 99.98)
Epoch: [82][300/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0754e-01 (1.0143e-01)	Acc@1  94.53 ( 96.51)	Acc@5 100.00 ( 99.98)
Epoch: [82][310/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.8481e-02 (1.0108e-01)	Acc@1  98.44 ( 96.51)	Acc@5 100.00 ( 99.98)
Epoch: [82][320/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0669e-01 (1.0082e-01)	Acc@1  96.09 ( 96.52)	Acc@5 100.00 ( 99.98)
Epoch: [82][330/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0162e-01 (1.0067e-01)	Acc@1  95.31 ( 96.53)	Acc@5 100.00 ( 99.98)
Epoch: [82][340/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.2410e-02 (1.0072e-01)	Acc@1  99.22 ( 96.52)	Acc@5 100.00 ( 99.98)
Epoch: [82][350/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.8145e-02 (1.0068e-01)	Acc@1  93.75 ( 96.51)	Acc@5 100.00 ( 99.98)
Epoch: [82][360/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.8115e-02 (1.0096e-01)	Acc@1  98.44 ( 96.51)	Acc@5 100.00 ( 99.98)
Epoch: [82][370/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.6814e-02 (1.0104e-01)	Acc@1  98.44 ( 96.51)	Acc@5 100.00 ( 99.98)
Epoch: [82][380/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4124e-01 (1.0116e-01)	Acc@1  96.09 ( 96.50)	Acc@5 100.00 ( 99.98)
Epoch: [82][390/391]	Time  0.050 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9080e-01 (1.0139e-01)	Acc@1  91.25 ( 96.49)	Acc@5 100.00 ( 99.98)
## e[82] optimizer.zero_grad (sum) time: 0.37946629524230957
## e[82]       loss.backward (sum) time: 7.1889824867248535
## e[82]      optimizer.step (sum) time: 3.4379680156707764
## epoch[82] training(only) time: 25.486452102661133
# Switched to evaluate mode...
Test: [  0/100]	Time  0.180 ( 0.180)	Loss 2.9712e-01 (2.9712e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.039)	Loss 4.0430e-01 (3.3127e-01)	Acc@1  89.00 ( 90.09)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 3.6182e-01 (3.2204e-01)	Acc@1  86.00 ( 90.10)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.024 ( 0.030)	Loss 2.7271e-01 (3.1967e-01)	Acc@1  88.00 ( 90.32)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 3.8013e-01 (3.2371e-01)	Acc@1  89.00 ( 90.17)	Acc@5  99.00 ( 99.63)
Test: [ 50/100]	Time  0.027 ( 0.028)	Loss 2.3596e-01 (3.2212e-01)	Acc@1  93.00 ( 90.18)	Acc@5 100.00 ( 99.67)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 1.9360e-01 (3.1512e-01)	Acc@1  94.00 ( 90.39)	Acc@5 100.00 ( 99.67)
Test: [ 70/100]	Time  0.024 ( 0.027)	Loss 4.3750e-01 (3.1534e-01)	Acc@1  88.00 ( 90.37)	Acc@5 100.00 ( 99.70)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 1.3733e-01 (3.1558e-01)	Acc@1  94.00 ( 90.35)	Acc@5 100.00 ( 99.70)
Test: [ 90/100]	Time  0.028 ( 0.027)	Loss 2.4146e-01 (3.1316e-01)	Acc@1  89.00 ( 90.47)	Acc@5 100.00 ( 99.70)
 * Acc@1 90.540 Acc@5 99.710
### epoch[82] execution time: 28.28342318534851
EPOCH 83
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [83][  0/391]	Time  0.247 ( 0.247)	Data  0.175 ( 0.175)	Loss 6.4453e-02 (6.4453e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [83][ 10/391]	Time  0.065 ( 0.081)	Data  0.001 ( 0.017)	Loss 1.6980e-01 (1.0330e-01)	Acc@1  92.97 ( 95.67)	Acc@5 100.00 (100.00)
Epoch: [83][ 20/391]	Time  0.065 ( 0.073)	Data  0.001 ( 0.009)	Loss 1.1548e-01 (9.9919e-02)	Acc@1  96.09 ( 96.17)	Acc@5 100.00 (100.00)
Epoch: [83][ 30/391]	Time  0.061 ( 0.070)	Data  0.001 ( 0.007)	Loss 3.4210e-02 (9.7753e-02)	Acc@1 100.00 ( 96.35)	Acc@5 100.00 (100.00)
Epoch: [83][ 40/391]	Time  0.063 ( 0.069)	Data  0.001 ( 0.005)	Loss 7.7942e-02 (9.9891e-02)	Acc@1  96.88 ( 96.25)	Acc@5 100.00 (100.00)
Epoch: [83][ 50/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.2512e-01 (1.0055e-01)	Acc@1  96.09 ( 96.31)	Acc@5 100.00 (100.00)
Epoch: [83][ 60/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.004)	Loss 7.3669e-02 (9.8095e-02)	Acc@1  97.66 ( 96.43)	Acc@5 100.00 (100.00)
Epoch: [83][ 70/391]	Time  0.061 ( 0.067)	Data  0.001 ( 0.003)	Loss 5.0323e-02 (9.9399e-02)	Acc@1  99.22 ( 96.43)	Acc@5 100.00 (100.00)
Epoch: [83][ 80/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 5.6549e-02 (9.7717e-02)	Acc@1  97.66 ( 96.55)	Acc@5 100.00 (100.00)
Epoch: [83][ 90/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.0361e-01 (9.9086e-02)	Acc@1  93.75 ( 96.51)	Acc@5  99.22 ( 99.98)
Epoch: [83][100/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.1670e-01 (9.9454e-02)	Acc@1  96.88 ( 96.48)	Acc@5 100.00 ( 99.98)
Epoch: [83][110/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 7.6416e-02 (9.9873e-02)	Acc@1  96.09 ( 96.47)	Acc@5 100.00 ( 99.99)
Epoch: [83][120/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.4177e-02 (9.9054e-02)	Acc@1  96.88 ( 96.50)	Acc@5 100.00 ( 99.99)
Epoch: [83][130/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.2266e-02 (1.0011e-01)	Acc@1  96.09 ( 96.43)	Acc@5 100.00 ( 99.99)
Epoch: [83][140/391]	Time  0.073 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.0312e-02 (9.9153e-02)	Acc@1  96.88 ( 96.48)	Acc@5 100.00 ( 99.99)
Epoch: [83][150/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.9854e-02 (9.8757e-02)	Acc@1  96.88 ( 96.52)	Acc@5 100.00 ( 99.98)
Epoch: [83][160/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3965e-01 (9.8999e-02)	Acc@1  97.66 ( 96.52)	Acc@5 100.00 ( 99.99)
Epoch: [83][170/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.0093e-01 (9.8774e-02)	Acc@1  92.97 ( 96.51)	Acc@5  99.22 ( 99.98)
Epoch: [83][180/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1841e-01 (9.9318e-02)	Acc@1  96.88 ( 96.49)	Acc@5 100.00 ( 99.98)
Epoch: [83][190/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.5162e-02 (9.9672e-02)	Acc@1 100.00 ( 96.49)	Acc@5 100.00 ( 99.98)
Epoch: [83][200/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.0813e-01 (1.0052e-01)	Acc@1  95.31 ( 96.47)	Acc@5 100.00 ( 99.98)
Epoch: [83][210/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7065e-01 (1.0131e-01)	Acc@1  93.75 ( 96.43)	Acc@5 100.00 ( 99.99)
Epoch: [83][220/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.4543e-02 (1.0065e-01)	Acc@1  96.88 ( 96.45)	Acc@5 100.00 ( 99.99)
Epoch: [83][230/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.0425e-02 (1.0014e-01)	Acc@1  96.88 ( 96.47)	Acc@5 100.00 ( 99.99)
Epoch: [83][240/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.8615e-02 (9.9870e-02)	Acc@1  98.44 ( 96.49)	Acc@5 100.00 ( 99.99)
Epoch: [83][250/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0760e-01 (1.0001e-01)	Acc@1  96.09 ( 96.48)	Acc@5 100.00 ( 99.99)
Epoch: [83][260/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3635e-01 (1.0082e-01)	Acc@1  96.09 ( 96.46)	Acc@5 100.00 ( 99.99)
Epoch: [83][270/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.0374e-02 (1.0116e-01)	Acc@1  98.44 ( 96.46)	Acc@5 100.00 ( 99.99)
Epoch: [83][280/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.5703e-02 (1.0154e-01)	Acc@1  96.88 ( 96.44)	Acc@5 100.00 ( 99.98)
Epoch: [83][290/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.0862e-02 (1.0121e-01)	Acc@1  97.66 ( 96.47)	Acc@5 100.00 ( 99.98)
Epoch: [83][300/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.3608e-02 (1.0147e-01)	Acc@1  96.88 ( 96.45)	Acc@5 100.00 ( 99.98)
Epoch: [83][310/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.4453e-02 (1.0178e-01)	Acc@1  97.66 ( 96.46)	Acc@5 100.00 ( 99.98)
Epoch: [83][320/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.0566e-02 (1.0155e-01)	Acc@1  96.88 ( 96.46)	Acc@5 100.00 ( 99.98)
Epoch: [83][330/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.8979e-02 (1.0120e-01)	Acc@1  95.31 ( 96.49)	Acc@5 100.00 ( 99.98)
Epoch: [83][340/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.6111e-02 (1.0070e-01)	Acc@1  97.66 ( 96.52)	Acc@5 100.00 ( 99.98)
Epoch: [83][350/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1890e-01 (1.0027e-01)	Acc@1  95.31 ( 96.53)	Acc@5 100.00 ( 99.98)
Epoch: [83][360/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2164e-01 (1.0023e-01)	Acc@1  95.31 ( 96.53)	Acc@5 100.00 ( 99.98)
Epoch: [83][370/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.2224e-02 (9.9931e-02)	Acc@1  95.31 ( 96.53)	Acc@5 100.00 ( 99.99)
Epoch: [83][380/391]	Time  0.058 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0028e-01 (1.0032e-01)	Acc@1  96.09 ( 96.50)	Acc@5 100.00 ( 99.98)
Epoch: [83][390/391]	Time  0.050 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1456e-01 (1.0045e-01)	Acc@1  93.75 ( 96.49)	Acc@5 100.00 ( 99.98)
## e[83] optimizer.zero_grad (sum) time: 0.3844318389892578
## e[83]       loss.backward (sum) time: 7.248141527175903
## e[83]      optimizer.step (sum) time: 3.475088357925415
## epoch[83] training(only) time: 25.581133604049683
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 2.9907e-01 (2.9907e-01)	Acc@1  87.00 ( 87.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.039)	Loss 4.1113e-01 (3.3368e-01)	Acc@1  89.00 ( 89.73)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 3.7866e-01 (3.2510e-01)	Acc@1  84.00 ( 89.67)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.024 ( 0.030)	Loss 2.6440e-01 (3.2160e-01)	Acc@1  88.00 ( 90.10)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 3.8403e-01 (3.2494e-01)	Acc@1  89.00 ( 90.12)	Acc@5  99.00 ( 99.68)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 2.4365e-01 (3.2433e-01)	Acc@1  93.00 ( 90.08)	Acc@5  99.00 ( 99.67)
Test: [ 60/100]	Time  0.027 ( 0.027)	Loss 1.9910e-01 (3.1713e-01)	Acc@1  94.00 ( 90.31)	Acc@5 100.00 ( 99.70)
Test: [ 70/100]	Time  0.024 ( 0.027)	Loss 4.4556e-01 (3.1823e-01)	Acc@1  87.00 ( 90.31)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 1.2830e-01 (3.1861e-01)	Acc@1  95.00 ( 90.30)	Acc@5 100.00 ( 99.73)
Test: [ 90/100]	Time  0.025 ( 0.027)	Loss 2.3853e-01 (3.1680e-01)	Acc@1  89.00 ( 90.42)	Acc@5 100.00 ( 99.73)
 * Acc@1 90.490 Acc@5 99.720
### epoch[83] execution time: 28.373851776123047
EPOCH 84
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [84][  0/391]	Time  0.243 ( 0.243)	Data  0.179 ( 0.179)	Loss 8.4106e-02 (8.4106e-02)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [84][ 10/391]	Time  0.064 ( 0.080)	Data  0.001 ( 0.017)	Loss 1.0175e-01 (1.0118e-01)	Acc@1  96.09 ( 96.02)	Acc@5 100.00 (100.00)
Epoch: [84][ 20/391]	Time  0.062 ( 0.073)	Data  0.001 ( 0.009)	Loss 1.0712e-01 (1.0359e-01)	Acc@1  97.66 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [84][ 30/391]	Time  0.068 ( 0.070)	Data  0.001 ( 0.007)	Loss 1.2158e-01 (1.0901e-01)	Acc@1  94.53 ( 95.92)	Acc@5 100.00 (100.00)
Epoch: [84][ 40/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.005)	Loss 7.5806e-02 (1.0765e-01)	Acc@1  99.22 ( 96.13)	Acc@5 100.00 (100.00)
Epoch: [84][ 50/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.005)	Loss 1.0480e-01 (1.0643e-01)	Acc@1  96.09 ( 96.22)	Acc@5 100.00 (100.00)
Epoch: [84][ 60/391]	Time  0.058 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.0754e-01 (1.0620e-01)	Acc@1  95.31 ( 96.27)	Acc@5 100.00 ( 99.99)
Epoch: [84][ 70/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.004)	Loss 9.6008e-02 (1.0521e-01)	Acc@1  96.09 ( 96.26)	Acc@5 100.00 ( 99.99)
Epoch: [84][ 80/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.2018e-01 (1.0398e-01)	Acc@1  94.53 ( 96.32)	Acc@5 100.00 ( 99.99)
Epoch: [84][ 90/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.4441e-01 (1.0290e-01)	Acc@1  96.09 ( 96.40)	Acc@5 100.00 ( 99.99)
Epoch: [84][100/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.2347e-01 (1.0533e-01)	Acc@1  94.53 ( 96.26)	Acc@5 100.00 ( 99.99)
Epoch: [84][110/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.1255e-01 (1.0656e-01)	Acc@1  94.53 ( 96.27)	Acc@5 100.00 ( 99.99)
Epoch: [84][120/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.5320e-01 (1.0590e-01)	Acc@1  95.31 ( 96.29)	Acc@5 100.00 ( 99.99)
Epoch: [84][130/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.2346e-02 (1.0621e-01)	Acc@1  94.53 ( 96.28)	Acc@5 100.00 ( 99.99)
Epoch: [84][140/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2805e-01 (1.0570e-01)	Acc@1  95.31 ( 96.31)	Acc@5 100.00 ( 99.99)
Epoch: [84][150/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8298e-01 (1.0669e-01)	Acc@1  93.75 ( 96.30)	Acc@5 100.00 ( 99.99)
Epoch: [84][160/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.7454e-02 (1.0595e-01)	Acc@1  96.88 ( 96.31)	Acc@5 100.00 ( 99.99)
Epoch: [84][170/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.3181e-02 (1.0378e-01)	Acc@1  97.66 ( 96.41)	Acc@5 100.00 ( 99.99)
Epoch: [84][180/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.9734e-02 (1.0418e-01)	Acc@1 100.00 ( 96.40)	Acc@5 100.00 ( 99.99)
Epoch: [84][190/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.5186e-02 (1.0353e-01)	Acc@1  98.44 ( 96.44)	Acc@5 100.00 ( 99.99)
Epoch: [84][200/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.5308e-02 (1.0286e-01)	Acc@1  97.66 ( 96.47)	Acc@5 100.00 ( 99.99)
Epoch: [84][210/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3977e-01 (1.0261e-01)	Acc@1  94.53 ( 96.46)	Acc@5  99.22 ( 99.99)
Epoch: [84][220/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.0444e-02 (1.0210e-01)	Acc@1  96.88 ( 96.46)	Acc@5 100.00 ( 99.99)
Epoch: [84][230/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1884e-01 (1.0280e-01)	Acc@1  95.31 ( 96.45)	Acc@5 100.00 ( 99.99)
Epoch: [84][240/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0059e-01 (1.0264e-01)	Acc@1  96.88 ( 96.46)	Acc@5 100.00 ( 99.99)
Epoch: [84][250/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.1443e-02 (1.0227e-01)	Acc@1  98.44 ( 96.47)	Acc@5 100.00 ( 99.99)
Epoch: [84][260/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.7810e-02 (1.0178e-01)	Acc@1  98.44 ( 96.48)	Acc@5 100.00 ( 99.99)
Epoch: [84][270/391]	Time  0.074 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6467e-01 (1.0217e-01)	Acc@1  92.97 ( 96.44)	Acc@5 100.00 ( 99.99)
Epoch: [84][280/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.7749e-02 (1.0199e-01)	Acc@1  98.44 ( 96.45)	Acc@5 100.00 ( 99.98)
Epoch: [84][290/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.6223e-02 (1.0205e-01)	Acc@1  99.22 ( 96.45)	Acc@5 100.00 ( 99.98)
Epoch: [84][300/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.2581e-02 (1.0198e-01)	Acc@1  96.88 ( 96.44)	Acc@5 100.00 ( 99.98)
Epoch: [84][310/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8591e-01 (1.0216e-01)	Acc@1  92.97 ( 96.43)	Acc@5 100.00 ( 99.98)
Epoch: [84][320/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6602e-01 (1.0231e-01)	Acc@1  95.31 ( 96.44)	Acc@5 100.00 ( 99.99)
Epoch: [84][330/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.7942e-02 (1.0226e-01)	Acc@1  97.66 ( 96.46)	Acc@5 100.00 ( 99.99)
Epoch: [84][340/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.2092e-02 (1.0191e-01)	Acc@1  96.88 ( 96.46)	Acc@5 100.00 ( 99.99)
Epoch: [84][350/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.3110e-02 (1.0137e-01)	Acc@1  98.44 ( 96.47)	Acc@5 100.00 ( 99.98)
Epoch: [84][360/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.2327e-02 (1.0111e-01)	Acc@1  97.66 ( 96.47)	Acc@5 100.00 ( 99.98)
Epoch: [84][370/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.9702e-02 (1.0092e-01)	Acc@1  96.88 ( 96.49)	Acc@5 100.00 ( 99.98)
Epoch: [84][380/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1560e-01 (1.0034e-01)	Acc@1  95.31 ( 96.51)	Acc@5 100.00 ( 99.98)
Epoch: [84][390/391]	Time  0.050 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0132e-01 (1.0057e-01)	Acc@1  96.25 ( 96.49)	Acc@5 100.00 ( 99.98)
## e[84] optimizer.zero_grad (sum) time: 0.3875410556793213
## e[84]       loss.backward (sum) time: 7.21031379699707
## e[84]      optimizer.step (sum) time: 3.485680103302002
## epoch[84] training(only) time: 25.515793085098267
# Switched to evaluate mode...
Test: [  0/100]	Time  0.190 ( 0.190)	Loss 3.0054e-01 (3.0054e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.041)	Loss 4.0259e-01 (3.3245e-01)	Acc@1  89.00 ( 90.27)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 3.7256e-01 (3.2425e-01)	Acc@1  86.00 ( 90.19)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 2.7466e-01 (3.2312e-01)	Acc@1  88.00 ( 90.42)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 3.7402e-01 (3.2516e-01)	Acc@1  89.00 ( 90.39)	Acc@5  99.00 ( 99.59)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 2.3865e-01 (3.2492e-01)	Acc@1  93.00 ( 90.35)	Acc@5  99.00 ( 99.61)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 2.0728e-01 (3.1808e-01)	Acc@1  94.00 ( 90.54)	Acc@5 100.00 ( 99.62)
Test: [ 70/100]	Time  0.027 ( 0.027)	Loss 4.4214e-01 (3.1797e-01)	Acc@1  88.00 ( 90.48)	Acc@5 100.00 ( 99.65)
Test: [ 80/100]	Time  0.027 ( 0.027)	Loss 1.3062e-01 (3.1846e-01)	Acc@1  94.00 ( 90.47)	Acc@5 100.00 ( 99.64)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 2.6221e-01 (3.1662e-01)	Acc@1  89.00 ( 90.58)	Acc@5 100.00 ( 99.65)
 * Acc@1 90.680 Acc@5 99.650
### epoch[84] execution time: 28.345633268356323
EPOCH 85
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [85][  0/391]	Time  0.239 ( 0.239)	Data  0.172 ( 0.172)	Loss 1.0388e-01 (1.0388e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [85][ 10/391]	Time  0.063 ( 0.081)	Data  0.001 ( 0.017)	Loss 6.8359e-02 (9.8142e-02)	Acc@1  97.66 ( 96.66)	Acc@5 100.00 ( 99.93)
Epoch: [85][ 20/391]	Time  0.064 ( 0.073)	Data  0.001 ( 0.009)	Loss 1.6882e-01 (9.5939e-02)	Acc@1  94.53 ( 96.84)	Acc@5 100.00 ( 99.96)
Epoch: [85][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.007)	Loss 5.3162e-02 (9.1970e-02)	Acc@1  98.44 ( 96.98)	Acc@5 100.00 ( 99.97)
Epoch: [85][ 40/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.005)	Loss 1.0638e-01 (9.4175e-02)	Acc@1  96.09 ( 96.86)	Acc@5 100.00 ( 99.98)
Epoch: [85][ 50/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.004)	Loss 8.6060e-02 (9.3765e-02)	Acc@1  98.44 ( 96.84)	Acc@5 100.00 ( 99.97)
Epoch: [85][ 60/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.9885e-01 (9.4003e-02)	Acc@1  93.75 ( 96.86)	Acc@5 100.00 ( 99.97)
Epoch: [85][ 70/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 6.0547e-02 (9.5180e-02)	Acc@1  98.44 ( 96.81)	Acc@5 100.00 ( 99.98)
Epoch: [85][ 80/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 7.4524e-02 (9.3580e-02)	Acc@1  97.66 ( 96.85)	Acc@5 100.00 ( 99.98)
Epoch: [85][ 90/391]	Time  0.076 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.5430e-01 (9.6131e-02)	Acc@1  93.75 ( 96.77)	Acc@5 100.00 ( 99.98)
Epoch: [85][100/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.9031e-01 (9.6751e-02)	Acc@1  92.97 ( 96.72)	Acc@5 100.00 ( 99.98)
Epoch: [85][110/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.5894e-01 (9.7262e-02)	Acc@1  96.88 ( 96.73)	Acc@5 100.00 ( 99.99)
Epoch: [85][120/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.9050e-02 (9.8958e-02)	Acc@1  99.22 ( 96.69)	Acc@5 100.00 ( 99.98)
Epoch: [85][130/391]	Time  0.059 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1365e-01 (9.8722e-02)	Acc@1  96.09 ( 96.69)	Acc@5 100.00 ( 99.98)
Epoch: [85][140/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.2998e-02 (9.8031e-02)	Acc@1  96.88 ( 96.69)	Acc@5 100.00 ( 99.98)
Epoch: [85][150/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1658e-01 (9.8281e-02)	Acc@1  96.09 ( 96.68)	Acc@5 100.00 ( 99.98)
Epoch: [85][160/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6675e-01 (9.8469e-02)	Acc@1  93.75 ( 96.68)	Acc@5 100.00 ( 99.98)
Epoch: [85][170/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4746e-01 (9.8889e-02)	Acc@1  95.31 ( 96.67)	Acc@5 100.00 ( 99.97)
Epoch: [85][180/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.6243e-02 (9.9238e-02)	Acc@1  96.88 ( 96.62)	Acc@5 100.00 ( 99.97)
Epoch: [85][190/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6797e-01 (9.9186e-02)	Acc@1  94.53 ( 96.63)	Acc@5 100.00 ( 99.98)
Epoch: [85][200/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2659e-01 (9.9230e-02)	Acc@1  96.09 ( 96.60)	Acc@5 100.00 ( 99.97)
Epoch: [85][210/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.5552e-02 (9.9625e-02)	Acc@1  97.66 ( 96.60)	Acc@5 100.00 ( 99.97)
Epoch: [85][220/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1694e-01 (9.9618e-02)	Acc@1  96.88 ( 96.62)	Acc@5 100.00 ( 99.98)
Epoch: [85][230/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4514e-01 (9.9930e-02)	Acc@1  94.53 ( 96.62)	Acc@5 100.00 ( 99.98)
Epoch: [85][240/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3733e-01 (9.9912e-02)	Acc@1  95.31 ( 96.64)	Acc@5 100.00 ( 99.98)
Epoch: [85][250/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0028e-01 (9.9752e-02)	Acc@1  96.09 ( 96.63)	Acc@5 100.00 ( 99.98)
Epoch: [85][260/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2805e-01 (1.0003e-01)	Acc@1  94.53 ( 96.61)	Acc@5 100.00 ( 99.97)
Epoch: [85][270/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4941e-01 (1.0031e-01)	Acc@1  92.19 ( 96.60)	Acc@5 100.00 ( 99.97)
Epoch: [85][280/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0767e-01 (1.0055e-01)	Acc@1  98.44 ( 96.59)	Acc@5 100.00 ( 99.97)
Epoch: [85][290/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.9478e-02 (1.0017e-01)	Acc@1  96.09 ( 96.59)	Acc@5 100.00 ( 99.98)
Epoch: [85][300/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0022e-01 (9.9660e-02)	Acc@1  96.88 ( 96.60)	Acc@5 100.00 ( 99.98)
Epoch: [85][310/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.5675e-02 (1.0004e-01)	Acc@1  98.44 ( 96.57)	Acc@5 100.00 ( 99.97)
Epoch: [85][320/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.8552e-02 (9.9566e-02)	Acc@1  96.09 ( 96.58)	Acc@5 100.00 ( 99.98)
Epoch: [85][330/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3733e-01 (1.0003e-01)	Acc@1  95.31 ( 96.56)	Acc@5 100.00 ( 99.98)
Epoch: [85][340/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1755e-01 (9.9990e-02)	Acc@1  97.66 ( 96.57)	Acc@5 100.00 ( 99.98)
Epoch: [85][350/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.5876e-02 (1.0045e-01)	Acc@1  96.88 ( 96.55)	Acc@5 100.00 ( 99.98)
Epoch: [85][360/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4111e-01 (1.0059e-01)	Acc@1  96.09 ( 96.54)	Acc@5 100.00 ( 99.98)
Epoch: [85][370/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.9763e-02 (1.0077e-01)	Acc@1  96.88 ( 96.52)	Acc@5 100.00 ( 99.97)
Epoch: [85][380/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6846e-01 (1.0065e-01)	Acc@1  94.53 ( 96.53)	Acc@5 100.00 ( 99.98)
Epoch: [85][390/391]	Time  0.047 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4893e-01 (1.0090e-01)	Acc@1  96.25 ( 96.52)	Acc@5 100.00 ( 99.98)
## e[85] optimizer.zero_grad (sum) time: 0.37686753273010254
## e[85]       loss.backward (sum) time: 7.204124450683594
## e[85]      optimizer.step (sum) time: 3.494123935699463
## epoch[85] training(only) time: 25.539556741714478
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 3.1299e-01 (3.1299e-01)	Acc@1  87.00 ( 87.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.027 ( 0.040)	Loss 3.9819e-01 (3.3111e-01)	Acc@1  89.00 ( 89.91)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.026 ( 0.033)	Loss 3.7378e-01 (3.2235e-01)	Acc@1  85.00 ( 90.00)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 2.7075e-01 (3.2126e-01)	Acc@1  89.00 ( 90.32)	Acc@5 100.00 ( 99.61)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 3.7500e-01 (3.2345e-01)	Acc@1  89.00 ( 90.32)	Acc@5  99.00 ( 99.56)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 2.1753e-01 (3.2206e-01)	Acc@1  93.00 ( 90.27)	Acc@5 100.00 ( 99.59)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 1.9849e-01 (3.1552e-01)	Acc@1  94.00 ( 90.51)	Acc@5 100.00 ( 99.62)
Test: [ 70/100]	Time  0.024 ( 0.027)	Loss 4.4653e-01 (3.1566e-01)	Acc@1  88.00 ( 90.52)	Acc@5 100.00 ( 99.65)
Test: [ 80/100]	Time  0.027 ( 0.027)	Loss 1.3293e-01 (3.1620e-01)	Acc@1  94.00 ( 90.51)	Acc@5 100.00 ( 99.67)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 2.6172e-01 (3.1407e-01)	Acc@1  89.00 ( 90.58)	Acc@5 100.00 ( 99.67)
 * Acc@1 90.670 Acc@5 99.680
### epoch[85] execution time: 28.33885622024536
EPOCH 86
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [86][  0/391]	Time  0.268 ( 0.268)	Data  0.199 ( 0.199)	Loss 1.2854e-01 (1.2854e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
Epoch: [86][ 10/391]	Time  0.064 ( 0.083)	Data  0.001 ( 0.019)	Loss 1.3110e-01 (1.0558e-01)	Acc@1  95.31 ( 96.52)	Acc@5 100.00 (100.00)
Epoch: [86][ 20/391]	Time  0.063 ( 0.073)	Data  0.001 ( 0.011)	Loss 1.0809e-01 (1.0133e-01)	Acc@1  95.31 ( 96.47)	Acc@5 100.00 (100.00)
Epoch: [86][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.008)	Loss 9.5337e-02 (1.0087e-01)	Acc@1  97.66 ( 96.57)	Acc@5 100.00 (100.00)
Epoch: [86][ 40/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.006)	Loss 8.8257e-02 (9.8004e-02)	Acc@1  96.09 ( 96.59)	Acc@5 100.00 (100.00)
Epoch: [86][ 50/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.005)	Loss 7.5012e-02 (9.9516e-02)	Acc@1  97.66 ( 96.55)	Acc@5 100.00 ( 99.98)
Epoch: [86][ 60/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.5674e-01 (1.0266e-01)	Acc@1  93.75 ( 96.47)	Acc@5 100.00 ( 99.99)
Epoch: [86][ 70/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.004)	Loss 6.8909e-02 (1.0136e-01)	Acc@1  98.44 ( 96.47)	Acc@5 100.00 ( 99.99)
Epoch: [86][ 80/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.004)	Loss 9.5337e-02 (1.0145e-01)	Acc@1  96.88 ( 96.50)	Acc@5  99.22 ( 99.98)
Epoch: [86][ 90/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.0181e-01 (1.0275e-01)	Acc@1  96.88 ( 96.47)	Acc@5 100.00 ( 99.98)
Epoch: [86][100/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 6.7383e-02 (1.0359e-01)	Acc@1  98.44 ( 96.48)	Acc@5 100.00 ( 99.98)
Epoch: [86][110/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.3257e-01 (1.0233e-01)	Acc@1  96.09 ( 96.49)	Acc@5 100.00 ( 99.98)
Epoch: [86][120/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.2537e-01 (1.0465e-01)	Acc@1  96.09 ( 96.36)	Acc@5 100.00 ( 99.98)
Epoch: [86][130/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.003)	Loss 6.4087e-02 (1.0391e-01)	Acc@1  96.09 ( 96.36)	Acc@5 100.00 ( 99.98)
Epoch: [86][140/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.6294e-02 (1.0244e-01)	Acc@1  96.88 ( 96.41)	Acc@5 100.00 ( 99.98)
Epoch: [86][150/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.0486e-02 (1.0187e-01)	Acc@1  98.44 ( 96.41)	Acc@5 100.00 ( 99.98)
Epoch: [86][160/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.3547e-02 (1.0109e-01)	Acc@1  99.22 ( 96.42)	Acc@5 100.00 ( 99.98)
Epoch: [86][170/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.9233e-02 (9.9620e-02)	Acc@1  97.66 ( 96.49)	Acc@5 100.00 ( 99.98)
Epoch: [86][180/391]	Time  0.060 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.6162e-02 (9.9538e-02)	Acc@1  97.66 ( 96.48)	Acc@5 100.00 ( 99.98)
Epoch: [86][190/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.6915e-02 (9.9067e-02)	Acc@1  97.66 ( 96.48)	Acc@5 100.00 ( 99.98)
Epoch: [86][200/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.4098e-02 (9.8593e-02)	Acc@1  97.66 ( 96.49)	Acc@5 100.00 ( 99.98)
Epoch: [86][210/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.7791e-02 (9.8808e-02)	Acc@1 100.00 ( 96.47)	Acc@5 100.00 ( 99.99)
Epoch: [86][220/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.7219e-02 (9.8468e-02)	Acc@1  97.66 ( 96.49)	Acc@5 100.00 ( 99.99)
Epoch: [86][230/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.1482e-02 (9.8320e-02)	Acc@1  96.88 ( 96.51)	Acc@5 100.00 ( 99.98)
Epoch: [86][240/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.6365e-02 (9.9041e-02)	Acc@1  96.88 ( 96.48)	Acc@5 100.00 ( 99.98)
Epoch: [86][250/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.9346e-02 (9.8515e-02)	Acc@1  96.88 ( 96.51)	Acc@5 100.00 ( 99.98)
Epoch: [86][260/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2952e-01 (9.9054e-02)	Acc@1  96.09 ( 96.49)	Acc@5 100.00 ( 99.98)
Epoch: [86][270/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.0669e-02 (9.9031e-02)	Acc@1  98.44 ( 96.49)	Acc@5 100.00 ( 99.98)
Epoch: [86][280/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.9722e-02 (9.8549e-02)	Acc@1  96.09 ( 96.51)	Acc@5 100.00 ( 99.98)
Epoch: [86][290/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1395e-01 (9.8495e-02)	Acc@1  96.09 ( 96.52)	Acc@5 100.00 ( 99.98)
Epoch: [86][300/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0632e-01 (9.8245e-02)	Acc@1  96.09 ( 96.54)	Acc@5 100.00 ( 99.98)
Epoch: [86][310/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3684e-01 (9.8584e-02)	Acc@1  95.31 ( 96.52)	Acc@5  99.22 ( 99.98)
Epoch: [86][320/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1755e-01 (9.8404e-02)	Acc@1  94.53 ( 96.51)	Acc@5 100.00 ( 99.98)
Epoch: [86][330/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.1797e-02 (9.8516e-02)	Acc@1  96.88 ( 96.52)	Acc@5 100.00 ( 99.98)
Epoch: [86][340/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1871e-01 (9.8322e-02)	Acc@1  96.88 ( 96.53)	Acc@5 100.00 ( 99.98)
Epoch: [86][350/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.9844e-02 (9.7974e-02)	Acc@1  99.22 ( 96.56)	Acc@5 100.00 ( 99.98)
Epoch: [86][360/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.8623e-02 (9.7869e-02)	Acc@1  95.31 ( 96.56)	Acc@5 100.00 ( 99.98)
Epoch: [86][370/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.7566e-02 (9.8535e-02)	Acc@1  97.66 ( 96.54)	Acc@5 100.00 ( 99.98)
Epoch: [86][380/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0925e-01 (9.8966e-02)	Acc@1  96.88 ( 96.52)	Acc@5 100.00 ( 99.98)
Epoch: [86][390/391]	Time  0.049 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5515e-01 (9.9034e-02)	Acc@1  93.75 ( 96.51)	Acc@5 100.00 ( 99.98)
## e[86] optimizer.zero_grad (sum) time: 0.38181447982788086
## e[86]       loss.backward (sum) time: 7.2128376960754395
## e[86]      optimizer.step (sum) time: 3.4867942333221436
## epoch[86] training(only) time: 25.573182106018066
# Switched to evaluate mode...
Test: [  0/100]	Time  0.203 ( 0.203)	Loss 2.9565e-01 (2.9565e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.042)	Loss 4.2969e-01 (3.3974e-01)	Acc@1  89.00 ( 90.09)	Acc@5  99.00 ( 99.82)
Test: [ 20/100]	Time  0.024 ( 0.034)	Loss 3.8257e-01 (3.2930e-01)	Acc@1  86.00 ( 90.05)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 2.6001e-01 (3.2406e-01)	Acc@1  89.00 ( 90.48)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 3.8477e-01 (3.2587e-01)	Acc@1  90.00 ( 90.39)	Acc@5  99.00 ( 99.61)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 2.3718e-01 (3.2592e-01)	Acc@1  93.00 ( 90.35)	Acc@5  99.00 ( 99.65)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 1.7651e-01 (3.1840e-01)	Acc@1  93.00 ( 90.57)	Acc@5 100.00 ( 99.69)
Test: [ 70/100]	Time  0.024 ( 0.027)	Loss 4.6777e-01 (3.2013e-01)	Acc@1  87.00 ( 90.58)	Acc@5 100.00 ( 99.70)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 1.2421e-01 (3.2054e-01)	Acc@1  96.00 ( 90.56)	Acc@5 100.00 ( 99.72)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 2.3047e-01 (3.1914e-01)	Acc@1  89.00 ( 90.59)	Acc@5 100.00 ( 99.73)
 * Acc@1 90.690 Acc@5 99.720
### epoch[86] execution time: 28.338154077529907
EPOCH 87
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [87][  0/391]	Time  0.249 ( 0.249)	Data  0.183 ( 0.183)	Loss 1.4441e-01 (1.4441e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [87][ 10/391]	Time  0.064 ( 0.081)	Data  0.001 ( 0.018)	Loss 7.3975e-02 (1.0108e-01)	Acc@1  97.66 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [87][ 20/391]	Time  0.063 ( 0.072)	Data  0.001 ( 0.010)	Loss 5.9998e-02 (1.0305e-01)	Acc@1  97.66 ( 96.73)	Acc@5 100.00 (100.00)
Epoch: [87][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.007)	Loss 5.3497e-02 (9.5916e-02)	Acc@1  99.22 ( 96.88)	Acc@5 100.00 ( 99.97)
Epoch: [87][ 40/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.006)	Loss 9.6924e-02 (9.3542e-02)	Acc@1  95.31 ( 96.82)	Acc@5 100.00 ( 99.98)
Epoch: [87][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.005)	Loss 8.8623e-02 (9.4274e-02)	Acc@1  97.66 ( 96.77)	Acc@5 100.00 ( 99.98)
Epoch: [87][ 60/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.004)	Loss 5.0995e-02 (9.4025e-02)	Acc@1  98.44 ( 96.81)	Acc@5 100.00 ( 99.97)
Epoch: [87][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 8.0322e-02 (9.5513e-02)	Acc@1  96.09 ( 96.72)	Acc@5 100.00 ( 99.97)
Epoch: [87][ 80/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 8.6365e-02 (9.5406e-02)	Acc@1  97.66 ( 96.75)	Acc@5 100.00 ( 99.97)
Epoch: [87][ 90/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.0864e-01 (9.8859e-02)	Acc@1  95.31 ( 96.63)	Acc@5 100.00 ( 99.97)
Epoch: [87][100/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 9.3872e-02 (1.0027e-01)	Acc@1  96.09 ( 96.56)	Acc@5 100.00 ( 99.98)
Epoch: [87][110/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.0352e-01 (1.0033e-01)	Acc@1  96.09 ( 96.54)	Acc@5 100.00 ( 99.98)
Epoch: [87][120/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.1755e-01 (1.0097e-01)	Acc@1  94.53 ( 96.48)	Acc@5 100.00 ( 99.97)
Epoch: [87][130/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.2083e-02 (1.0062e-01)	Acc@1  96.88 ( 96.51)	Acc@5 100.00 ( 99.97)
Epoch: [87][140/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.5063e-02 (1.0134e-01)	Acc@1  98.44 ( 96.49)	Acc@5 100.00 ( 99.97)
Epoch: [87][150/391]	Time  0.060 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.8726e-02 (1.0057e-01)	Acc@1  98.44 ( 96.51)	Acc@5 100.00 ( 99.96)
Epoch: [87][160/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.3120e-02 (9.9398e-02)	Acc@1  97.66 ( 96.55)	Acc@5 100.00 ( 99.96)
Epoch: [87][170/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.9609e-02 (9.8606e-02)	Acc@1  96.88 ( 96.58)	Acc@5 100.00 ( 99.96)
Epoch: [87][180/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1053e-01 (9.7761e-02)	Acc@1  96.09 ( 96.60)	Acc@5 100.00 ( 99.96)
Epoch: [87][190/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2445e-01 (9.8160e-02)	Acc@1  95.31 ( 96.58)	Acc@5 100.00 ( 99.96)
Epoch: [87][200/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.2937e-02 (9.7916e-02)	Acc@1  97.66 ( 96.61)	Acc@5 100.00 ( 99.97)
Epoch: [87][210/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.7993e-02 (9.7312e-02)	Acc@1  99.22 ( 96.62)	Acc@5 100.00 ( 99.97)
Epoch: [87][220/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.5886e-02 (9.6766e-02)	Acc@1  96.09 ( 96.67)	Acc@5 100.00 ( 99.97)
Epoch: [87][230/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3877e-01 (9.8010e-02)	Acc@1  92.97 ( 96.63)	Acc@5 100.00 ( 99.97)
Epoch: [87][240/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1292e-01 (9.8155e-02)	Acc@1  96.09 ( 96.63)	Acc@5 100.00 ( 99.97)
Epoch: [87][250/391]	Time  0.070 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5906e-01 (9.8032e-02)	Acc@1  94.53 ( 96.63)	Acc@5 100.00 ( 99.97)
Epoch: [87][260/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3110e-01 (9.8767e-02)	Acc@1  96.09 ( 96.63)	Acc@5 100.00 ( 99.97)
Epoch: [87][270/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3318e-01 (9.8724e-02)	Acc@1  94.53 ( 96.61)	Acc@5 100.00 ( 99.97)
Epoch: [87][280/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.0261e-02 (9.9405e-02)	Acc@1  98.44 ( 96.59)	Acc@5 100.00 ( 99.97)
Epoch: [87][290/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7493e-01 (9.9686e-02)	Acc@1  94.53 ( 96.57)	Acc@5 100.00 ( 99.97)
Epoch: [87][300/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0059e-01 (9.9806e-02)	Acc@1  97.66 ( 96.53)	Acc@5 100.00 ( 99.97)
Epoch: [87][310/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3269e-01 (9.9867e-02)	Acc@1  94.53 ( 96.52)	Acc@5 100.00 ( 99.97)
Epoch: [87][320/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3281e-01 (9.9834e-02)	Acc@1  96.09 ( 96.53)	Acc@5 100.00 ( 99.97)
Epoch: [87][330/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.8918e-02 (9.9618e-02)	Acc@1  97.66 ( 96.55)	Acc@5 100.00 ( 99.97)
Epoch: [87][340/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.0984e-02 (9.9396e-02)	Acc@1  99.22 ( 96.56)	Acc@5 100.00 ( 99.97)
Epoch: [87][350/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.5125e-02 (9.8926e-02)	Acc@1  99.22 ( 96.58)	Acc@5 100.00 ( 99.97)
Epoch: [87][360/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.6487e-02 (9.9337e-02)	Acc@1  96.88 ( 96.56)	Acc@5 100.00 ( 99.97)
Epoch: [87][370/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.9407e-02 (9.9411e-02)	Acc@1  96.88 ( 96.55)	Acc@5 100.00 ( 99.97)
Epoch: [87][380/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0095e-01 (9.9154e-02)	Acc@1  96.88 ( 96.55)	Acc@5 100.00 ( 99.97)
Epoch: [87][390/391]	Time  0.047 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7004e-01 (9.9277e-02)	Acc@1  93.75 ( 96.55)	Acc@5 100.00 ( 99.97)
## e[87] optimizer.zero_grad (sum) time: 0.38648366928100586
## e[87]       loss.backward (sum) time: 7.2561352252960205
## e[87]      optimizer.step (sum) time: 3.4346656799316406
## epoch[87] training(only) time: 25.545910358428955
# Switched to evaluate mode...
Test: [  0/100]	Time  0.201 ( 0.201)	Loss 2.8491e-01 (2.8491e-01)	Acc@1  89.00 ( 89.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.041)	Loss 3.9941e-01 (3.3201e-01)	Acc@1  89.00 ( 89.82)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 3.7256e-01 (3.2434e-01)	Acc@1  86.00 ( 89.86)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.024 ( 0.031)	Loss 2.7393e-01 (3.2206e-01)	Acc@1  89.00 ( 90.23)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 3.7646e-01 (3.2519e-01)	Acc@1  89.00 ( 90.27)	Acc@5 100.00 ( 99.63)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 2.3059e-01 (3.2414e-01)	Acc@1  93.00 ( 90.25)	Acc@5 100.00 ( 99.67)
Test: [ 60/100]	Time  0.025 ( 0.028)	Loss 1.8152e-01 (3.1726e-01)	Acc@1  95.00 ( 90.48)	Acc@5 100.00 ( 99.69)
Test: [ 70/100]	Time  0.027 ( 0.027)	Loss 4.4019e-01 (3.1767e-01)	Acc@1  87.00 ( 90.39)	Acc@5 100.00 ( 99.72)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 1.3477e-01 (3.1842e-01)	Acc@1  94.00 ( 90.37)	Acc@5 100.00 ( 99.72)
Test: [ 90/100]	Time  0.027 ( 0.027)	Loss 2.4109e-01 (3.1636e-01)	Acc@1  90.00 ( 90.49)	Acc@5 100.00 ( 99.71)
 * Acc@1 90.580 Acc@5 99.720
### epoch[87] execution time: 28.36456060409546
EPOCH 88
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [88][  0/391]	Time  0.252 ( 0.252)	Data  0.187 ( 0.187)	Loss 5.3131e-02 (5.3131e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [88][ 10/391]	Time  0.063 ( 0.082)	Data  0.001 ( 0.018)	Loss 5.0293e-02 (8.2478e-02)	Acc@1  98.44 ( 97.30)	Acc@5 100.00 ( 99.93)
Epoch: [88][ 20/391]	Time  0.065 ( 0.074)	Data  0.001 ( 0.010)	Loss 5.8014e-02 (8.8747e-02)	Acc@1  98.44 ( 97.06)	Acc@5 100.00 ( 99.93)
Epoch: [88][ 30/391]	Time  0.067 ( 0.071)	Data  0.001 ( 0.007)	Loss 8.7952e-02 (9.1397e-02)	Acc@1  97.66 ( 96.85)	Acc@5 100.00 ( 99.95)
Epoch: [88][ 40/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.006)	Loss 9.9792e-02 (9.2142e-02)	Acc@1  95.31 ( 96.70)	Acc@5 100.00 ( 99.96)
Epoch: [88][ 50/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.005)	Loss 4.7943e-02 (9.2365e-02)	Acc@1  99.22 ( 96.77)	Acc@5 100.00 ( 99.97)
Epoch: [88][ 60/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.3696e-01 (9.1891e-02)	Acc@1  94.53 ( 96.81)	Acc@5 100.00 ( 99.96)
Epoch: [88][ 70/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.004)	Loss 8.5754e-02 (9.1641e-02)	Acc@1  98.44 ( 96.83)	Acc@5 100.00 ( 99.97)
Epoch: [88][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 9.8877e-02 (9.1688e-02)	Acc@1  96.09 ( 96.87)	Acc@5 100.00 ( 99.97)
Epoch: [88][ 90/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.003)	Loss 6.3721e-02 (9.3681e-02)	Acc@1  98.44 ( 96.76)	Acc@5 100.00 ( 99.97)
Epoch: [88][100/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.1023e-01 (9.5137e-02)	Acc@1  95.31 ( 96.70)	Acc@5 100.00 ( 99.98)
Epoch: [88][110/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.003)	Loss 8.8379e-02 (9.4997e-02)	Acc@1  97.66 ( 96.71)	Acc@5 100.00 ( 99.98)
Epoch: [88][120/391]	Time  0.074 ( 0.067)	Data  0.001 ( 0.003)	Loss 7.4890e-02 (9.5394e-02)	Acc@1  96.88 ( 96.65)	Acc@5 100.00 ( 99.98)
Epoch: [88][130/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.0149e-02 (9.4833e-02)	Acc@1  97.66 ( 96.68)	Acc@5 100.00 ( 99.98)
Epoch: [88][140/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0938e-01 (9.5792e-02)	Acc@1  97.66 ( 96.64)	Acc@5 100.00 ( 99.98)
Epoch: [88][150/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.3110e-02 (9.4865e-02)	Acc@1  98.44 ( 96.68)	Acc@5 100.00 ( 99.97)
Epoch: [88][160/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.7383e-02 (9.5099e-02)	Acc@1  98.44 ( 96.68)	Acc@5 100.00 ( 99.97)
Epoch: [88][170/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3208e-01 (9.3989e-02)	Acc@1  97.66 ( 96.74)	Acc@5 100.00 ( 99.97)
Epoch: [88][180/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.5195e-02 (9.3940e-02)	Acc@1  97.66 ( 96.73)	Acc@5 100.00 ( 99.97)
Epoch: [88][190/391]	Time  0.059 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.0017e-02 (9.5668e-02)	Acc@1  98.44 ( 96.67)	Acc@5 100.00 ( 99.97)
Epoch: [88][200/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.4565e-02 (9.6364e-02)	Acc@1  97.66 ( 96.63)	Acc@5 100.00 ( 99.97)
Epoch: [88][210/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.5938e-02 (9.6181e-02)	Acc@1  96.09 ( 96.62)	Acc@5 100.00 ( 99.97)
Epoch: [88][220/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0156e-01 (9.6651e-02)	Acc@1  96.09 ( 96.58)	Acc@5 100.00 ( 99.98)
Epoch: [88][230/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4319e-01 (9.6420e-02)	Acc@1  96.09 ( 96.61)	Acc@5  99.22 ( 99.97)
Epoch: [88][240/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.2632e-02 (9.6695e-02)	Acc@1  97.66 ( 96.60)	Acc@5 100.00 ( 99.97)
Epoch: [88][250/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3635e-01 (9.7446e-02)	Acc@1  95.31 ( 96.58)	Acc@5 100.00 ( 99.98)
Epoch: [88][260/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.1829e-02 (9.7604e-02)	Acc@1  97.66 ( 96.58)	Acc@5 100.00 ( 99.98)
Epoch: [88][270/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7480e-01 (9.8139e-02)	Acc@1  95.31 ( 96.58)	Acc@5 100.00 ( 99.98)
Epoch: [88][280/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.6936e-02 (9.7935e-02)	Acc@1  98.44 ( 96.57)	Acc@5 100.00 ( 99.98)
Epoch: [88][290/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1072e-01 (9.8172e-02)	Acc@1  96.09 ( 96.56)	Acc@5  99.22 ( 99.97)
Epoch: [88][300/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0150e-01 (9.7587e-02)	Acc@1  95.31 ( 96.58)	Acc@5 100.00 ( 99.97)
Epoch: [88][310/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.8572e-02 (9.7176e-02)	Acc@1  95.31 ( 96.58)	Acc@5 100.00 ( 99.97)
Epoch: [88][320/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0944e-01 (9.7855e-02)	Acc@1  96.88 ( 96.56)	Acc@5 100.00 ( 99.97)
Epoch: [88][330/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.5806e-02 (9.8007e-02)	Acc@1  96.88 ( 96.55)	Acc@5 100.00 ( 99.97)
Epoch: [88][340/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2512e-01 (9.8035e-02)	Acc@1  96.09 ( 96.54)	Acc@5 100.00 ( 99.97)
Epoch: [88][350/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.2590e-02 (9.8136e-02)	Acc@1  97.66 ( 96.53)	Acc@5 100.00 ( 99.98)
Epoch: [88][360/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1755e-01 (9.8173e-02)	Acc@1  96.09 ( 96.55)	Acc@5 100.00 ( 99.98)
Epoch: [88][370/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6748e-01 (9.8149e-02)	Acc@1  93.75 ( 96.55)	Acc@5 100.00 ( 99.98)
Epoch: [88][380/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1987e-01 (9.8463e-02)	Acc@1  96.09 ( 96.53)	Acc@5 100.00 ( 99.98)
Epoch: [88][390/391]	Time  0.048 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.6538e-02 (9.8735e-02)	Acc@1  97.50 ( 96.52)	Acc@5 100.00 ( 99.98)
## e[88] optimizer.zero_grad (sum) time: 0.38492488861083984
## e[88]       loss.backward (sum) time: 7.2438108921051025
## e[88]      optimizer.step (sum) time: 3.4200730323791504
## epoch[88] training(only) time: 25.605812072753906
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 2.8955e-01 (2.8955e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.027 ( 0.040)	Loss 3.9795e-01 (3.3302e-01)	Acc@1  89.00 ( 90.00)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.028 ( 0.033)	Loss 3.8525e-01 (3.2500e-01)	Acc@1  86.00 ( 90.19)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.032 ( 0.031)	Loss 2.7271e-01 (3.2158e-01)	Acc@1  88.00 ( 90.45)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.024 ( 0.029)	Loss 3.8501e-01 (3.2553e-01)	Acc@1  88.00 ( 90.39)	Acc@5  99.00 ( 99.56)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 2.3718e-01 (3.2438e-01)	Acc@1  93.00 ( 90.39)	Acc@5 100.00 ( 99.63)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 1.9678e-01 (3.1657e-01)	Acc@1  94.00 ( 90.66)	Acc@5 100.00 ( 99.66)
Test: [ 70/100]	Time  0.024 ( 0.027)	Loss 4.3579e-01 (3.1714e-01)	Acc@1  87.00 ( 90.61)	Acc@5 100.00 ( 99.69)
Test: [ 80/100]	Time  0.026 ( 0.027)	Loss 1.2830e-01 (3.1722e-01)	Acc@1  95.00 ( 90.60)	Acc@5 100.00 ( 99.69)
Test: [ 90/100]	Time  0.025 ( 0.027)	Loss 2.5269e-01 (3.1565e-01)	Acc@1  89.00 ( 90.68)	Acc@5 100.00 ( 99.69)
 * Acc@1 90.750 Acc@5 99.700
### epoch[88] execution time: 28.422595739364624
EPOCH 89
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [89][  0/391]	Time  0.252 ( 0.252)	Data  0.185 ( 0.185)	Loss 9.5581e-02 (9.5581e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [89][ 10/391]	Time  0.063 ( 0.082)	Data  0.001 ( 0.018)	Loss 1.4392e-01 (9.9251e-02)	Acc@1  96.88 ( 97.09)	Acc@5 100.00 (100.00)
Epoch: [89][ 20/391]	Time  0.065 ( 0.074)	Data  0.001 ( 0.010)	Loss 9.1797e-02 (9.8428e-02)	Acc@1  96.88 ( 96.73)	Acc@5 100.00 (100.00)
Epoch: [89][ 30/391]	Time  0.062 ( 0.071)	Data  0.001 ( 0.007)	Loss 6.9092e-02 (1.0025e-01)	Acc@1  96.88 ( 96.50)	Acc@5 100.00 (100.00)
Epoch: [89][ 40/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.006)	Loss 1.0547e-01 (1.0129e-01)	Acc@1  96.09 ( 96.51)	Acc@5 100.00 ( 99.98)
Epoch: [89][ 50/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.005)	Loss 8.6914e-02 (9.8322e-02)	Acc@1  98.44 ( 96.58)	Acc@5 100.00 ( 99.98)
Epoch: [89][ 60/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.0291e-01 (9.7240e-02)	Acc@1  96.88 ( 96.68)	Acc@5 100.00 ( 99.97)
Epoch: [89][ 70/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.004)	Loss 6.9580e-02 (9.4921e-02)	Acc@1  97.66 ( 96.75)	Acc@5 100.00 ( 99.98)
Epoch: [89][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 8.8013e-02 (9.3528e-02)	Acc@1  96.09 ( 96.84)	Acc@5 100.00 ( 99.97)
Epoch: [89][ 90/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.1473e-02 (9.2162e-02)	Acc@1 100.00 ( 96.88)	Acc@5 100.00 ( 99.97)
Epoch: [89][100/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 8.9233e-02 (9.1370e-02)	Acc@1  95.31 ( 96.88)	Acc@5 100.00 ( 99.98)
Epoch: [89][110/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 5.7098e-02 (9.0144e-02)	Acc@1  99.22 ( 96.93)	Acc@5 100.00 ( 99.98)
Epoch: [89][120/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.3281e-01 (9.2287e-02)	Acc@1  92.97 ( 96.82)	Acc@5 100.00 ( 99.98)
Epoch: [89][130/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.7351e-02 (9.2961e-02)	Acc@1  96.09 ( 96.80)	Acc@5 100.00 ( 99.98)
Epoch: [89][140/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.0262e-02 (9.3597e-02)	Acc@1  97.66 ( 96.75)	Acc@5 100.00 ( 99.98)
Epoch: [89][150/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.7058e-02 (9.4374e-02)	Acc@1  98.44 ( 96.72)	Acc@5 100.00 ( 99.98)
Epoch: [89][160/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.3741e-02 (9.4684e-02)	Acc@1  97.66 ( 96.69)	Acc@5 100.00 ( 99.99)
Epoch: [89][170/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.8928e-02 (9.5261e-02)	Acc@1  96.09 ( 96.69)	Acc@5 100.00 ( 99.99)
Epoch: [89][180/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.8633e-02 (9.6464e-02)	Acc@1  95.31 ( 96.65)	Acc@5 100.00 ( 99.98)
Epoch: [89][190/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.2998e-02 (9.5674e-02)	Acc@1  96.88 ( 96.66)	Acc@5 100.00 ( 99.98)
Epoch: [89][200/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1121e-01 (9.7000e-02)	Acc@1  96.88 ( 96.62)	Acc@5 100.00 ( 99.98)
Epoch: [89][210/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.2205e-02 (9.7027e-02)	Acc@1  97.66 ( 96.59)	Acc@5 100.00 ( 99.99)
Epoch: [89][220/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.5215e-02 (9.6908e-02)	Acc@1  98.44 ( 96.62)	Acc@5 100.00 ( 99.99)
Epoch: [89][230/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2036e-01 (9.7161e-02)	Acc@1  96.88 ( 96.62)	Acc@5 100.00 ( 99.99)
Epoch: [89][240/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.0200e-02 (9.7028e-02)	Acc@1  96.88 ( 96.63)	Acc@5 100.00 ( 99.99)
Epoch: [89][250/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.9426e-02 (9.7024e-02)	Acc@1  95.31 ( 96.63)	Acc@5 100.00 ( 99.98)
Epoch: [89][260/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4355e-01 (9.7726e-02)	Acc@1  94.53 ( 96.60)	Acc@5 100.00 ( 99.98)
Epoch: [89][270/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.2529e-02 (9.7395e-02)	Acc@1  96.88 ( 96.61)	Acc@5 100.00 ( 99.98)
Epoch: [89][280/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2927e-01 (9.7732e-02)	Acc@1  95.31 ( 96.61)	Acc@5 100.00 ( 99.97)
Epoch: [89][290/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4856e-01 (9.7739e-02)	Acc@1  96.88 ( 96.61)	Acc@5 100.00 ( 99.98)
Epoch: [89][300/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2158e-01 (9.7799e-02)	Acc@1  95.31 ( 96.60)	Acc@5 100.00 ( 99.98)
Epoch: [89][310/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.3925e-02 (9.7453e-02)	Acc@1  98.44 ( 96.61)	Acc@5 100.00 ( 99.98)
Epoch: [89][320/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.4749e-02 (9.7452e-02)	Acc@1  98.44 ( 96.60)	Acc@5 100.00 ( 99.98)
Epoch: [89][330/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.1614e-02 (9.7525e-02)	Acc@1  97.66 ( 96.60)	Acc@5 100.00 ( 99.98)
Epoch: [89][340/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.4148e-02 (9.7132e-02)	Acc@1  98.44 ( 96.63)	Acc@5 100.00 ( 99.98)
Epoch: [89][350/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.1350e-02 (9.7017e-02)	Acc@1  97.66 ( 96.63)	Acc@5 100.00 ( 99.98)
Epoch: [89][360/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.1492e-02 (9.7401e-02)	Acc@1  97.66 ( 96.62)	Acc@5 100.00 ( 99.98)
Epoch: [89][370/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0303e-01 (9.7738e-02)	Acc@1  96.09 ( 96.61)	Acc@5 100.00 ( 99.97)
Epoch: [89][380/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5125e-01 (9.8216e-02)	Acc@1  93.75 ( 96.59)	Acc@5 100.00 ( 99.98)
Epoch: [89][390/391]	Time  0.046 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4355e-01 (9.8199e-02)	Acc@1  95.00 ( 96.60)	Acc@5 100.00 ( 99.98)
## e[89] optimizer.zero_grad (sum) time: 0.3840968608856201
## e[89]       loss.backward (sum) time: 7.234437465667725
## e[89]      optimizer.step (sum) time: 3.414581298828125
## epoch[89] training(only) time: 25.576725006103516
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 2.9492e-01 (2.9492e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.041)	Loss 4.1479e-01 (3.3279e-01)	Acc@1  89.00 ( 89.82)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 3.8599e-01 (3.2595e-01)	Acc@1  85.00 ( 89.71)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.028 ( 0.031)	Loss 2.6929e-01 (3.2227e-01)	Acc@1  89.00 ( 90.23)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.027 ( 0.029)	Loss 3.8208e-01 (3.2537e-01)	Acc@1  87.00 ( 90.20)	Acc@5  99.00 ( 99.59)
Test: [ 50/100]	Time  0.024 ( 0.028)	Loss 2.4829e-01 (3.2536e-01)	Acc@1  93.00 ( 90.22)	Acc@5  99.00 ( 99.61)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 2.0605e-01 (3.1804e-01)	Acc@1  94.00 ( 90.48)	Acc@5 100.00 ( 99.66)
Test: [ 70/100]	Time  0.026 ( 0.027)	Loss 4.4482e-01 (3.1986e-01)	Acc@1  87.00 ( 90.41)	Acc@5 100.00 ( 99.69)
Test: [ 80/100]	Time  0.024 ( 0.027)	Loss 1.2537e-01 (3.2094e-01)	Acc@1  95.00 ( 90.40)	Acc@5 100.00 ( 99.69)
Test: [ 90/100]	Time  0.024 ( 0.027)	Loss 2.3816e-01 (3.1937e-01)	Acc@1  89.00 ( 90.45)	Acc@5 100.00 ( 99.69)
 * Acc@1 90.560 Acc@5 99.700
### epoch[89] execution time: 28.360965251922607
### Training complete:
#### total training(only) time: 2304.510263442993
##### Total run time: 2560.2052071094513
