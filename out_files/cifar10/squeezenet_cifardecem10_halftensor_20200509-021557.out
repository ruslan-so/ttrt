# Model: squeezenet
# Dataset: cifardecem
# Batch size: 128
# Freezeout: False
# Low precision: True
# Epochs: 90
# Initial learning rate: 0.1
# module_name: models_cifardecem.squeezenet
<function squeezenet at 0x7fc704771f28>
# model requested: 'squeezenet'
# printing out the model
SqueezeNet(
  (stem): Sequential(
    (0): Conv2d(3, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fire2): Fire(
    (squeeze): Sequential(
      (0): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (expand_1x1): Sequential(
      (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (expand_3x3): Sequential(
      (0): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
  )
  (fire3): Fire(
    (squeeze): Sequential(
      (0): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (expand_1x1): Sequential(
      (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (expand_3x3): Sequential(
      (0): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
  )
  (fire4): Fire(
    (squeeze): Sequential(
      (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (expand_1x1): Sequential(
      (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (expand_3x3): Sequential(
      (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
  )
  (fire5): Fire(
    (squeeze): Sequential(
      (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (expand_1x1): Sequential(
      (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (expand_3x3): Sequential(
      (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
  )
  (fire6): Fire(
    (squeeze): Sequential(
      (0): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (expand_1x1): Sequential(
      (0): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (expand_3x3): Sequential(
      (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
  )
  (fire7): Fire(
    (squeeze): Sequential(
      (0): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (expand_1x1): Sequential(
      (0): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (expand_3x3): Sequential(
      (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
  )
  (fire8): Fire(
    (squeeze): Sequential(
      (0): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (expand_1x1): Sequential(
      (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (expand_3x3): Sequential(
      (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
  )
  (fire9): Fire(
    (squeeze): Sequential(
      (0): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (expand_1x1): Sequential(
      (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (expand_3x3): Sequential(
      (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
  )
  (conv10): Conv2d(512, 10, kernel_size=(1, 1), stride=(1, 1))
  (avg): AdaptiveAvgPool2d(output_size=1)
  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
# model is low precision
# Model: squeezenet
# Dataset: cifardecem
# Freezeout: False
# Low precision: True
# Initial learning rate: 0.1
# Criterion: CrossEntropyLoss()
# Optimizer: SGD
#	lr 0.1
#	momentum 0.9
#	dampening 0
#	weight_decay 0.0001
#	nesterov False
CIFAR10 loading and normalization
==> Preparing data..
# CIFAR10 / low precision
Files already downloaded and verified
Files already downloaded and verified
#--> Training data: <--#
#-->>
EPOCH 0
# current learning rate: 0.1
# Switched to train mode...
Epoch: [0][  0/391]	Time  3.409 ( 3.409)	Data  0.107 ( 0.107)	Loss 2.4023e+00 (2.4023e+00)	Acc@1  10.94 ( 10.94)	Acc@5  44.53 ( 44.53)
Epoch: [0][ 10/391]	Time  0.037 ( 0.347)	Data  0.001 ( 0.011)	Loss 2.3105e+00 (2.2768e+00)	Acc@1  22.66 ( 15.55)	Acc@5  67.97 ( 63.71)
Epoch: [0][ 20/391]	Time  0.038 ( 0.201)	Data  0.001 ( 0.006)	Loss 1.9756e+00 (2.1934e+00)	Acc@1  24.22 ( 18.75)	Acc@5  77.34 ( 69.49)
Epoch: [0][ 30/391]	Time  0.040 ( 0.149)	Data  0.001 ( 0.005)	Loss 2.1406e+00 (2.1413e+00)	Acc@1  25.78 ( 20.49)	Acc@5  77.34 ( 73.03)
Epoch: [0][ 40/391]	Time  0.038 ( 0.122)	Data  0.001 ( 0.004)	Loss 2.0059e+00 (2.0917e+00)	Acc@1  20.31 ( 22.52)	Acc@5  81.25 ( 75.32)
Epoch: [0][ 50/391]	Time  0.038 ( 0.106)	Data  0.001 ( 0.003)	Loss 1.8311e+00 (2.0496e+00)	Acc@1  30.47 ( 23.88)	Acc@5  85.94 ( 77.02)
Epoch: [0][ 60/391]	Time  0.041 ( 0.095)	Data  0.001 ( 0.003)	Loss 1.8418e+00 (2.0241e+00)	Acc@1  25.78 ( 24.74)	Acc@5  85.16 ( 78.28)
Epoch: [0][ 70/391]	Time  0.042 ( 0.088)	Data  0.001 ( 0.003)	Loss 1.7939e+00 (1.9976e+00)	Acc@1  34.38 ( 25.57)	Acc@5  82.81 ( 79.18)
Epoch: [0][ 80/391]	Time  0.040 ( 0.082)	Data  0.001 ( 0.002)	Loss 1.8467e+00 (1.9766e+00)	Acc@1  26.56 ( 26.34)	Acc@5  85.16 ( 79.87)
Epoch: [0][ 90/391]	Time  0.052 ( 0.077)	Data  0.001 ( 0.002)	Loss 1.8047e+00 (1.9524e+00)	Acc@1  25.78 ( 27.09)	Acc@5  88.28 ( 80.63)
Epoch: [0][100/391]	Time  0.042 ( 0.074)	Data  0.001 ( 0.002)	Loss 1.8184e+00 (1.9353e+00)	Acc@1  30.47 ( 27.79)	Acc@5  82.03 ( 81.13)
Epoch: [0][110/391]	Time  0.035 ( 0.071)	Data  0.001 ( 0.002)	Loss 1.7354e+00 (1.9132e+00)	Acc@1  32.81 ( 28.48)	Acc@5  82.03 ( 81.69)
Epoch: [0][120/391]	Time  0.039 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.7295e+00 (1.8908e+00)	Acc@1  35.94 ( 29.34)	Acc@5  89.84 ( 82.22)
Epoch: [0][130/391]	Time  0.037 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5527e+00 (1.8749e+00)	Acc@1  37.50 ( 29.95)	Acc@5  92.19 ( 82.78)
Epoch: [0][140/391]	Time  0.038 ( 0.064)	Data  0.001 ( 0.002)	Loss 1.7393e+00 (1.8653e+00)	Acc@1  32.03 ( 30.30)	Acc@5  88.28 ( 83.05)
Epoch: [0][150/391]	Time  0.038 ( 0.062)	Data  0.001 ( 0.002)	Loss 1.5967e+00 (1.8485e+00)	Acc@1  40.62 ( 30.87)	Acc@5  94.53 ( 83.49)
Epoch: [0][160/391]	Time  0.038 ( 0.061)	Data  0.001 ( 0.002)	Loss 1.6816e+00 (1.8345e+00)	Acc@1  31.25 ( 31.48)	Acc@5  88.28 ( 83.88)
Epoch: [0][170/391]	Time  0.038 ( 0.060)	Data  0.001 ( 0.002)	Loss 1.5713e+00 (1.8189e+00)	Acc@1  43.75 ( 32.08)	Acc@5  89.84 ( 84.24)
Epoch: [0][180/391]	Time  0.038 ( 0.058)	Data  0.001 ( 0.002)	Loss 1.6172e+00 (1.8042e+00)	Acc@1  38.28 ( 32.67)	Acc@5  89.84 ( 84.60)
Epoch: [0][190/391]	Time  0.038 ( 0.057)	Data  0.001 ( 0.002)	Loss 1.5078e+00 (1.7908e+00)	Acc@1  45.31 ( 33.14)	Acc@5  89.06 ( 84.92)
Epoch: [0][200/391]	Time  0.040 ( 0.057)	Data  0.001 ( 0.002)	Loss 1.4619e+00 (1.7776e+00)	Acc@1  42.19 ( 33.64)	Acc@5  92.19 ( 85.21)
Epoch: [0][210/391]	Time  0.040 ( 0.056)	Data  0.001 ( 0.002)	Loss 1.5156e+00 (1.7653e+00)	Acc@1  41.41 ( 34.14)	Acc@5  91.41 ( 85.53)
Epoch: [0][220/391]	Time  0.040 ( 0.055)	Data  0.001 ( 0.002)	Loss 1.5439e+00 (1.7519e+00)	Acc@1  50.78 ( 34.63)	Acc@5  91.41 ( 85.85)
Epoch: [0][230/391]	Time  0.042 ( 0.054)	Data  0.001 ( 0.002)	Loss 1.5498e+00 (1.7421e+00)	Acc@1  40.62 ( 34.98)	Acc@5  89.84 ( 86.06)
Epoch: [0][240/391]	Time  0.038 ( 0.054)	Data  0.001 ( 0.001)	Loss 1.3760e+00 (1.7302e+00)	Acc@1  55.47 ( 35.41)	Acc@5  91.41 ( 86.34)
Epoch: [0][250/391]	Time  0.038 ( 0.053)	Data  0.001 ( 0.001)	Loss 1.5459e+00 (1.7201e+00)	Acc@1  41.41 ( 35.80)	Acc@5  92.97 ( 86.55)
Epoch: [0][260/391]	Time  0.039 ( 0.053)	Data  0.001 ( 0.001)	Loss 1.4502e+00 (1.7110e+00)	Acc@1  46.09 ( 36.13)	Acc@5  93.75 ( 86.77)
Epoch: [0][270/391]	Time  0.039 ( 0.052)	Data  0.001 ( 0.001)	Loss 1.4824e+00 (1.7024e+00)	Acc@1  50.00 ( 36.51)	Acc@5  92.97 ( 86.93)
Epoch: [0][280/391]	Time  0.039 ( 0.052)	Data  0.001 ( 0.001)	Loss 1.3818e+00 (1.6914e+00)	Acc@1  51.56 ( 36.92)	Acc@5  91.41 ( 87.13)
Epoch: [0][290/391]	Time  0.042 ( 0.051)	Data  0.001 ( 0.001)	Loss 1.3701e+00 (1.6810e+00)	Acc@1  44.53 ( 37.28)	Acc@5  95.31 ( 87.35)
Epoch: [0][300/391]	Time  0.041 ( 0.051)	Data  0.001 ( 0.001)	Loss 1.5381e+00 (1.6726e+00)	Acc@1  45.31 ( 37.62)	Acc@5  92.97 ( 87.54)
Epoch: [0][310/391]	Time  0.038 ( 0.051)	Data  0.001 ( 0.001)	Loss 1.3271e+00 (1.6625e+00)	Acc@1  49.22 ( 37.99)	Acc@5  94.53 ( 87.75)
Epoch: [0][320/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.001)	Loss 1.4648e+00 (1.6537e+00)	Acc@1  46.09 ( 38.33)	Acc@5  91.41 ( 87.92)
Epoch: [0][330/391]	Time  0.041 ( 0.050)	Data  0.001 ( 0.001)	Loss 1.3242e+00 (1.6444e+00)	Acc@1  47.66 ( 38.69)	Acc@5  95.31 ( 88.10)
Epoch: [0][340/391]	Time  0.035 ( 0.050)	Data  0.001 ( 0.001)	Loss 1.2119e+00 (1.6343e+00)	Acc@1  50.78 ( 39.05)	Acc@5  96.09 ( 88.28)
Epoch: [0][350/391]	Time  0.038 ( 0.049)	Data  0.001 ( 0.001)	Loss 1.5029e+00 (1.6269e+00)	Acc@1  46.88 ( 39.31)	Acc@5  88.28 ( 88.41)
Epoch: [0][360/391]	Time  0.038 ( 0.049)	Data  0.001 ( 0.001)	Loss 1.2598e+00 (1.6186e+00)	Acc@1  53.12 ( 39.67)	Acc@5  92.97 ( 88.54)
Epoch: [0][370/391]	Time  0.037 ( 0.049)	Data  0.001 ( 0.001)	Loss 1.3740e+00 (1.6109e+00)	Acc@1  51.56 ( 39.97)	Acc@5  90.62 ( 88.68)
Epoch: [0][380/391]	Time  0.039 ( 0.049)	Data  0.001 ( 0.001)	Loss 1.2520e+00 (1.6029e+00)	Acc@1  56.25 ( 40.33)	Acc@5  96.09 ( 88.81)
Epoch: [0][390/391]	Time  0.287 ( 0.049)	Data  0.001 ( 0.001)	Loss 1.2432e+00 (1.5964e+00)	Acc@1  55.00 ( 40.62)	Acc@5  92.50 ( 88.93)
## e[0] optimizer.zero_grad (sum) time: 0.2524385452270508
## e[0]       loss.backward (sum) time: 4.493370771408081
## e[0]      optimizer.step (sum) time: 1.7878782749176025
## epoch[0] training(only) time: 19.188926696777344
# Switched to evaluate mode...
Test: [  0/100]	Time  0.249 ( 0.249)	Loss 1.1836e+00 (1.1836e+00)	Acc@1  55.00 ( 55.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.024 ( 0.041)	Loss 1.1797e+00 (1.2647e+00)	Acc@1  60.00 ( 54.27)	Acc@5  95.00 ( 93.36)
Test: [ 20/100]	Time  0.017 ( 0.030)	Loss 1.2051e+00 (1.2761e+00)	Acc@1  55.00 ( 53.57)	Acc@5  97.00 ( 93.62)
Test: [ 30/100]	Time  0.025 ( 0.027)	Loss 1.2305e+00 (1.3017e+00)	Acc@1  59.00 ( 52.81)	Acc@5  96.00 ( 94.10)
Test: [ 40/100]	Time  0.021 ( 0.025)	Loss 1.2461e+00 (1.3016e+00)	Acc@1  55.00 ( 52.78)	Acc@5  92.00 ( 93.78)
Test: [ 50/100]	Time  0.016 ( 0.024)	Loss 1.2637e+00 (1.2974e+00)	Acc@1  51.00 ( 52.88)	Acc@5  93.00 ( 93.96)
Test: [ 60/100]	Time  0.021 ( 0.023)	Loss 1.3818e+00 (1.2962e+00)	Acc@1  47.00 ( 53.05)	Acc@5  96.00 ( 94.07)
Test: [ 70/100]	Time  0.023 ( 0.023)	Loss 1.3184e+00 (1.3024e+00)	Acc@1  53.00 ( 52.68)	Acc@5  96.00 ( 94.04)
Test: [ 80/100]	Time  0.022 ( 0.023)	Loss 1.1084e+00 (1.2972e+00)	Acc@1  60.00 ( 52.78)	Acc@5  95.00 ( 94.12)
Test: [ 90/100]	Time  0.021 ( 0.022)	Loss 1.2373e+00 (1.2962e+00)	Acc@1  54.00 ( 52.98)	Acc@5  98.00 ( 94.14)
 * Acc@1 53.090 Acc@5 94.140
### epoch[0] execution time: 21.521842002868652
EPOCH 1
# current learning rate: 0.1
# Switched to train mode...
Epoch: [1][  0/391]	Time  0.228 ( 0.228)	Data  0.182 ( 0.182)	Loss 1.3389e+00 (1.3389e+00)	Acc@1  50.00 ( 50.00)	Acc@5  92.19 ( 92.19)
Epoch: [1][ 10/391]	Time  0.039 ( 0.057)	Data  0.001 ( 0.017)	Loss 1.2041e+00 (1.2962e+00)	Acc@1  55.47 ( 52.56)	Acc@5  95.31 ( 94.11)
Epoch: [1][ 20/391]	Time  0.039 ( 0.049)	Data  0.001 ( 0.010)	Loss 1.3008e+00 (1.3023e+00)	Acc@1  55.47 ( 52.83)	Acc@5  92.97 ( 94.05)
Epoch: [1][ 30/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.007)	Loss 1.3320e+00 (1.2920e+00)	Acc@1  51.56 ( 52.92)	Acc@5  92.97 ( 94.25)
Epoch: [1][ 40/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.005)	Loss 1.4893e+00 (1.2823e+00)	Acc@1  42.97 ( 53.11)	Acc@5  90.62 ( 94.49)
Epoch: [1][ 50/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.0967e+00 (1.2846e+00)	Acc@1  63.28 ( 53.12)	Acc@5  96.09 ( 94.45)
Epoch: [1][ 60/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.3027e+00 (1.2786e+00)	Acc@1  51.56 ( 53.53)	Acc@5  94.53 ( 94.36)
Epoch: [1][ 70/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.1836e+00 (1.2713e+00)	Acc@1  58.59 ( 53.73)	Acc@5  96.88 ( 94.55)
Epoch: [1][ 80/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.1816e+00 (1.2638e+00)	Acc@1  60.94 ( 54.01)	Acc@5  94.53 ( 94.64)
Epoch: [1][ 90/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.1562e+00 (1.2576e+00)	Acc@1  60.16 ( 54.34)	Acc@5  92.97 ( 94.64)
Epoch: [1][100/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.2188e+00 (1.2553e+00)	Acc@1  53.91 ( 54.59)	Acc@5  96.09 ( 94.62)
Epoch: [1][110/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.1973e+00 (1.2471e+00)	Acc@1  56.25 ( 54.97)	Acc@5  94.53 ( 94.74)
Epoch: [1][120/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0996e+00 (1.2448e+00)	Acc@1  57.81 ( 54.97)	Acc@5  97.66 ( 94.83)
Epoch: [1][130/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1865e+00 (1.2442e+00)	Acc@1  58.59 ( 54.99)	Acc@5  95.31 ( 94.76)
Epoch: [1][140/391]	Time  0.052 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1816e+00 (1.2379e+00)	Acc@1  57.81 ( 55.19)	Acc@5  94.53 ( 94.84)
Epoch: [1][150/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1611e+00 (1.2334e+00)	Acc@1  57.03 ( 55.29)	Acc@5  94.53 ( 94.86)
Epoch: [1][160/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2539e+00 (1.2261e+00)	Acc@1  53.12 ( 55.58)	Acc@5  95.31 ( 94.92)
Epoch: [1][170/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1133e+00 (1.2195e+00)	Acc@1  60.94 ( 55.82)	Acc@5  96.88 ( 94.99)
Epoch: [1][180/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3799e+00 (1.2193e+00)	Acc@1  46.09 ( 55.82)	Acc@5  94.53 ( 94.92)
Epoch: [1][190/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0469e+00 (1.2166e+00)	Acc@1  61.72 ( 55.96)	Acc@5  94.53 ( 94.99)
Epoch: [1][200/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1621e+00 (1.2103e+00)	Acc@1  54.69 ( 56.17)	Acc@5  99.22 ( 95.06)
Epoch: [1][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0830e+00 (1.2084e+00)	Acc@1  56.25 ( 56.29)	Acc@5  95.31 ( 95.03)
Epoch: [1][220/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1836e+00 (1.2077e+00)	Acc@1  54.69 ( 56.28)	Acc@5  94.53 ( 95.06)
Epoch: [1][230/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1914e+00 (1.2057e+00)	Acc@1  56.25 ( 56.37)	Acc@5  97.66 ( 95.06)
Epoch: [1][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0566e+00 (1.2023e+00)	Acc@1  61.72 ( 56.48)	Acc@5  99.22 ( 95.11)
Epoch: [1][250/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0684e+00 (1.1997e+00)	Acc@1  62.50 ( 56.61)	Acc@5  97.66 ( 95.12)
Epoch: [1][260/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.3213e-01 (1.1960e+00)	Acc@1  67.19 ( 56.72)	Acc@5  96.88 ( 95.16)
Epoch: [1][270/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2109e+00 (1.1934e+00)	Acc@1  53.91 ( 56.84)	Acc@5  93.75 ( 95.20)
Epoch: [1][280/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1533e+00 (1.1885e+00)	Acc@1  57.81 ( 57.06)	Acc@5  92.97 ( 95.21)
Epoch: [1][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1279e+00 (1.1858e+00)	Acc@1  64.06 ( 57.22)	Acc@5  95.31 ( 95.21)
Epoch: [1][300/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.2061e+00 (1.1836e+00)	Acc@1  57.03 ( 57.26)	Acc@5  93.75 ( 95.23)
Epoch: [1][310/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.2891e+00 (1.1811e+00)	Acc@1  57.03 ( 57.39)	Acc@5  94.53 ( 95.25)
Epoch: [1][320/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.1934e+00 (1.1785e+00)	Acc@1  51.56 ( 57.52)	Acc@5  96.09 ( 95.28)
Epoch: [1][330/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.1689e+00 (1.1763e+00)	Acc@1  63.28 ( 57.60)	Acc@5  92.97 ( 95.29)
Epoch: [1][340/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.0986e+00 (1.1747e+00)	Acc@1  60.16 ( 57.67)	Acc@5  98.44 ( 95.31)
Epoch: [1][350/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.1201e+00 (1.1741e+00)	Acc@1  59.38 ( 57.66)	Acc@5  96.88 ( 95.32)
Epoch: [1][360/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.1484e+00 (1.1727e+00)	Acc@1  60.16 ( 57.72)	Acc@5  94.53 ( 95.34)
Epoch: [1][370/391]	Time  0.039 ( 0.041)	Data  0.002 ( 0.001)	Loss 8.7842e-01 (1.1695e+00)	Acc@1  66.41 ( 57.83)	Acc@5 100.00 ( 95.40)
Epoch: [1][380/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 9.6094e-01 (1.1674e+00)	Acc@1  61.72 ( 57.93)	Acc@5  96.88 ( 95.40)
Epoch: [1][390/391]	Time  0.030 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.1475e+00 (1.1637e+00)	Acc@1  56.25 ( 58.09)	Acc@5  96.25 ( 95.41)
## e[1] optimizer.zero_grad (sum) time: 0.2506296634674072
## e[1]       loss.backward (sum) time: 4.168181657791138
## e[1]      optimizer.step (sum) time: 1.7841343879699707
## epoch[1] training(only) time: 15.952919244766235
# Switched to evaluate mode...
Test: [  0/100]	Time  0.165 ( 0.165)	Loss 1.1465e+00 (1.1465e+00)	Acc@1  59.00 ( 59.00)	Acc@5  96.00 ( 96.00)
Test: [ 10/100]	Time  0.017 ( 0.033)	Loss 1.0674e+00 (1.2842e+00)	Acc@1  65.00 ( 57.64)	Acc@5  96.00 ( 94.82)
Test: [ 20/100]	Time  0.020 ( 0.026)	Loss 1.3828e+00 (1.3236e+00)	Acc@1  52.00 ( 56.24)	Acc@5  95.00 ( 94.57)
Test: [ 30/100]	Time  0.019 ( 0.024)	Loss 1.1885e+00 (1.3329e+00)	Acc@1  60.00 ( 55.71)	Acc@5  94.00 ( 94.52)
Test: [ 40/100]	Time  0.017 ( 0.023)	Loss 1.3398e+00 (1.3432e+00)	Acc@1  54.00 ( 55.63)	Acc@5  98.00 ( 94.76)
Test: [ 50/100]	Time  0.017 ( 0.022)	Loss 1.2959e+00 (1.3398e+00)	Acc@1  58.00 ( 55.96)	Acc@5  94.00 ( 94.90)
Test: [ 60/100]	Time  0.016 ( 0.022)	Loss 1.3779e+00 (1.3491e+00)	Acc@1  53.00 ( 55.66)	Acc@5  98.00 ( 94.97)
Test: [ 70/100]	Time  0.017 ( 0.021)	Loss 1.1836e+00 (1.3513e+00)	Acc@1  58.00 ( 55.42)	Acc@5  97.00 ( 95.08)
Test: [ 80/100]	Time  0.017 ( 0.021)	Loss 1.3096e+00 (1.3496e+00)	Acc@1  55.00 ( 55.54)	Acc@5  94.00 ( 95.19)
Test: [ 90/100]	Time  0.020 ( 0.021)	Loss 1.4346e+00 (1.3535e+00)	Acc@1  53.00 ( 55.45)	Acc@5 100.00 ( 95.15)
 * Acc@1 55.440 Acc@5 95.120
### epoch[1] execution time: 18.147687673568726
EPOCH 2
# current learning rate: 0.1
# Switched to train mode...
Epoch: [2][  0/391]	Time  0.212 ( 0.212)	Data  0.172 ( 0.172)	Loss 1.1055e+00 (1.1055e+00)	Acc@1  60.16 ( 60.16)	Acc@5  97.66 ( 97.66)
Epoch: [2][ 10/391]	Time  0.040 ( 0.057)	Data  0.001 ( 0.017)	Loss 1.1074e+00 (1.0530e+00)	Acc@1  60.94 ( 63.92)	Acc@5  96.09 ( 96.59)
Epoch: [2][ 20/391]	Time  0.038 ( 0.049)	Data  0.001 ( 0.009)	Loss 9.7363e-01 (1.0672e+00)	Acc@1  66.41 ( 62.17)	Acc@5  96.09 ( 96.50)
Epoch: [2][ 30/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.006)	Loss 8.9844e-01 (1.0465e+00)	Acc@1  65.62 ( 62.60)	Acc@5  99.22 ( 96.77)
Epoch: [2][ 40/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.005)	Loss 1.2061e+00 (1.0450e+00)	Acc@1  57.81 ( 62.46)	Acc@5  92.97 ( 96.55)
Epoch: [2][ 50/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.0488e+00 (1.0349e+00)	Acc@1  64.06 ( 62.79)	Acc@5  96.09 ( 96.68)
Epoch: [2][ 60/391]	Time  0.045 ( 0.043)	Data  0.001 ( 0.004)	Loss 9.1504e-01 (1.0315e+00)	Acc@1  70.31 ( 62.96)	Acc@5  97.66 ( 96.71)
Epoch: [2][ 70/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 9.4043e-01 (1.0279e+00)	Acc@1  65.62 ( 63.12)	Acc@5  96.09 ( 96.67)
Epoch: [2][ 80/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.0967e+00 (1.0280e+00)	Acc@1  62.50 ( 63.09)	Acc@5  97.66 ( 96.69)
Epoch: [2][ 90/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 9.7607e-01 (1.0283e+00)	Acc@1  67.19 ( 63.18)	Acc@5  96.09 ( 96.60)
Epoch: [2][100/391]	Time  0.054 ( 0.042)	Data  0.001 ( 0.003)	Loss 8.2812e-01 (1.0241e+00)	Acc@1  69.53 ( 63.25)	Acc@5  99.22 ( 96.63)
Epoch: [2][110/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 8.9258e-01 (1.0189e+00)	Acc@1  65.62 ( 63.44)	Acc@5  97.66 ( 96.68)
Epoch: [2][120/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.7754e-01 (1.0197e+00)	Acc@1  59.38 ( 63.40)	Acc@5  97.66 ( 96.61)
Epoch: [2][130/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.9160e-01 (1.0169e+00)	Acc@1  69.53 ( 63.56)	Acc@5  98.44 ( 96.70)
Epoch: [2][140/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0811e+00 (1.0137e+00)	Acc@1  62.50 ( 63.64)	Acc@5  96.09 ( 96.73)
Epoch: [2][150/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.7344e-01 (1.0055e+00)	Acc@1  74.22 ( 64.01)	Acc@5  99.22 ( 96.79)
Epoch: [2][160/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1221e+00 (1.0056e+00)	Acc@1  56.25 ( 64.07)	Acc@5  98.44 ( 96.80)
Epoch: [2][170/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0752e+00 (1.0073e+00)	Acc@1  64.06 ( 64.06)	Acc@5  94.53 ( 96.74)
Epoch: [2][180/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.0430e+00 (1.0066e+00)	Acc@1  61.72 ( 64.13)	Acc@5  99.22 ( 96.70)
Epoch: [2][190/391]	Time  0.044 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.1387e+00 (1.0062e+00)	Acc@1  57.81 ( 64.16)	Acc@5  96.88 ( 96.67)
Epoch: [2][200/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.002)	Loss 9.0869e-01 (1.0043e+00)	Acc@1  67.97 ( 64.20)	Acc@5  98.44 ( 96.67)
Epoch: [2][210/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 9.9512e-01 (1.0028e+00)	Acc@1  61.72 ( 64.24)	Acc@5  96.09 ( 96.66)
Epoch: [2][220/391]	Time  0.044 ( 0.040)	Data  0.001 ( 0.002)	Loss 8.7354e-01 (1.0011e+00)	Acc@1  69.53 ( 64.28)	Acc@5  96.09 ( 96.66)
Epoch: [2][230/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 9.2920e-01 (9.9943e-01)	Acc@1  67.19 ( 64.41)	Acc@5  95.31 ( 96.66)
Epoch: [2][240/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.002)	Loss 9.6289e-01 (9.9624e-01)	Acc@1  61.72 ( 64.55)	Acc@5  98.44 ( 96.70)
Epoch: [2][250/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 9.7119e-01 (9.9247e-01)	Acc@1  67.19 ( 64.68)	Acc@5  96.88 ( 96.74)
Epoch: [2][260/391]	Time  0.043 ( 0.040)	Data  0.001 ( 0.002)	Loss 9.9561e-01 (9.9110e-01)	Acc@1  62.50 ( 64.72)	Acc@5  97.66 ( 96.75)
Epoch: [2][270/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 8.1641e-01 (9.8829e-01)	Acc@1  67.97 ( 64.81)	Acc@5  98.44 ( 96.76)
Epoch: [2][280/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 8.2812e-01 (9.8528e-01)	Acc@1  64.84 ( 64.86)	Acc@5  98.44 ( 96.80)
Epoch: [2][290/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 7.5293e-01 (9.8074e-01)	Acc@1  69.53 ( 65.01)	Acc@5  99.22 ( 96.83)
Epoch: [2][300/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 7.2021e-01 (9.7842e-01)	Acc@1  77.34 ( 65.13)	Acc@5  98.44 ( 96.86)
Epoch: [2][310/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 8.4766e-01 (9.7712e-01)	Acc@1  66.41 ( 65.16)	Acc@5  96.88 ( 96.87)
Epoch: [2][320/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.0811e+00 (9.7585e-01)	Acc@1  59.38 ( 65.27)	Acc@5  95.31 ( 96.87)
Epoch: [2][330/391]	Time  0.045 ( 0.040)	Data  0.001 ( 0.001)	Loss 9.1602e-01 (9.7443e-01)	Acc@1  67.97 ( 65.31)	Acc@5  96.88 ( 96.90)
Epoch: [2][340/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 9.6338e-01 (9.7285e-01)	Acc@1  63.28 ( 65.38)	Acc@5  97.66 ( 96.91)
Epoch: [2][350/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.8955e-01 (9.7064e-01)	Acc@1  71.09 ( 65.46)	Acc@5  99.22 ( 96.93)
Epoch: [2][360/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 8.5156e-01 (9.6978e-01)	Acc@1  73.44 ( 65.53)	Acc@5  97.66 ( 96.93)
Epoch: [2][370/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.001)	Loss 8.5254e-01 (9.6676e-01)	Acc@1  69.53 ( 65.66)	Acc@5  99.22 ( 96.95)
Epoch: [2][380/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.001)	Loss 9.6143e-01 (9.6504e-01)	Acc@1  64.84 ( 65.73)	Acc@5  96.09 ( 96.98)
Epoch: [2][390/391]	Time  0.027 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.0996e-01 (9.6330e-01)	Acc@1  76.25 ( 65.79)	Acc@5  98.75 ( 96.99)
## e[2] optimizer.zero_grad (sum) time: 0.2522757053375244
## e[2]       loss.backward (sum) time: 4.04346227645874
## e[2]      optimizer.step (sum) time: 1.8203749656677246
## epoch[2] training(only) time: 15.721766471862793
# Switched to evaluate mode...
Test: [  0/100]	Time  0.175 ( 0.175)	Loss 8.6084e-01 (8.6084e-01)	Acc@1  67.00 ( 67.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.020 ( 0.034)	Loss 9.1553e-01 (9.8287e-01)	Acc@1  67.00 ( 65.82)	Acc@5  97.00 ( 97.55)
Test: [ 20/100]	Time  0.016 ( 0.027)	Loss 9.6045e-01 (9.7391e-01)	Acc@1  64.00 ( 65.81)	Acc@5  98.00 ( 97.43)
Test: [ 30/100]	Time  0.022 ( 0.024)	Loss 9.0039e-01 (9.8997e-01)	Acc@1  71.00 ( 65.42)	Acc@5  96.00 ( 97.10)
Test: [ 40/100]	Time  0.019 ( 0.023)	Loss 9.3359e-01 (9.8832e-01)	Acc@1  73.00 ( 65.29)	Acc@5  98.00 ( 97.02)
Test: [ 50/100]	Time  0.020 ( 0.022)	Loss 7.9150e-01 (9.7446e-01)	Acc@1  74.00 ( 66.02)	Acc@5  97.00 ( 96.96)
Test: [ 60/100]	Time  0.025 ( 0.022)	Loss 1.0361e+00 (9.8237e-01)	Acc@1  62.00 ( 65.75)	Acc@5  97.00 ( 96.93)
Test: [ 70/100]	Time  0.020 ( 0.022)	Loss 9.3311e-01 (9.8963e-01)	Acc@1  65.00 ( 65.65)	Acc@5  97.00 ( 96.93)
Test: [ 80/100]	Time  0.020 ( 0.022)	Loss 7.7637e-01 (9.8626e-01)	Acc@1  67.00 ( 65.74)	Acc@5  99.00 ( 97.02)
Test: [ 90/100]	Time  0.020 ( 0.022)	Loss 8.6963e-01 (9.8720e-01)	Acc@1  61.00 ( 65.62)	Acc@5  99.00 ( 97.03)
 * Acc@1 65.700 Acc@5 97.060
### epoch[2] execution time: 17.98774743080139
EPOCH 3
# current learning rate: 0.1
# Switched to train mode...
Epoch: [3][  0/391]	Time  0.217 ( 0.217)	Data  0.178 ( 0.178)	Loss 8.6572e-01 (8.6572e-01)	Acc@1  68.75 ( 68.75)	Acc@5  96.09 ( 96.09)
Epoch: [3][ 10/391]	Time  0.041 ( 0.056)	Data  0.001 ( 0.017)	Loss 8.5205e-01 (9.0345e-01)	Acc@1  72.66 ( 67.90)	Acc@5  96.09 ( 97.37)
Epoch: [3][ 20/391]	Time  0.040 ( 0.048)	Data  0.001 ( 0.009)	Loss 8.9893e-01 (8.9209e-01)	Acc@1  70.31 ( 69.38)	Acc@5  96.88 ( 97.17)
Epoch: [3][ 30/391]	Time  0.040 ( 0.046)	Data  0.001 ( 0.007)	Loss 8.6768e-01 (8.8500e-01)	Acc@1  70.31 ( 69.48)	Acc@5  96.88 ( 97.20)
Epoch: [3][ 40/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.005)	Loss 7.5098e-01 (8.7804e-01)	Acc@1  75.00 ( 69.70)	Acc@5 100.00 ( 97.41)
Epoch: [3][ 50/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.004)	Loss 9.5801e-01 (8.6693e-01)	Acc@1  67.97 ( 69.84)	Acc@5  98.44 ( 97.64)
Epoch: [3][ 60/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.1270e+00 (8.6428e-01)	Acc@1  57.03 ( 69.63)	Acc@5  96.09 ( 97.75)
Epoch: [3][ 70/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 7.9004e-01 (8.6533e-01)	Acc@1  71.88 ( 69.55)	Acc@5  96.09 ( 97.73)
Epoch: [3][ 80/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.8223e-01 (8.6520e-01)	Acc@1  70.31 ( 69.41)	Acc@5  99.22 ( 97.79)
Epoch: [3][ 90/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.9736e-01 (8.6160e-01)	Acc@1  69.53 ( 69.51)	Acc@5  99.22 ( 97.88)
Epoch: [3][100/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 8.8086e-01 (8.6058e-01)	Acc@1  68.75 ( 69.51)	Acc@5  95.31 ( 97.87)
Epoch: [3][110/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 9.5410e-01 (8.6428e-01)	Acc@1  63.28 ( 69.28)	Acc@5  96.88 ( 97.85)
Epoch: [3][120/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.0605e-01 (8.6091e-01)	Acc@1  78.12 ( 69.33)	Acc@5  98.44 ( 97.88)
Epoch: [3][130/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0557e+00 (8.6054e-01)	Acc@1  62.50 ( 69.42)	Acc@5  96.88 ( 97.85)
Epoch: [3][140/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.7412e-01 (8.6267e-01)	Acc@1  63.28 ( 69.33)	Acc@5  98.44 ( 97.84)
Epoch: [3][150/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.7607e-01 (8.6714e-01)	Acc@1  67.97 ( 69.22)	Acc@5  96.09 ( 97.85)
Epoch: [3][160/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.9062e-01 (8.6645e-01)	Acc@1  70.31 ( 69.20)	Acc@5  97.66 ( 97.88)
Epoch: [3][170/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.0859e-01 (8.6228e-01)	Acc@1  72.66 ( 69.36)	Acc@5  98.44 ( 97.93)
Epoch: [3][180/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.0215e-01 (8.6029e-01)	Acc@1  77.34 ( 69.44)	Acc@5  96.88 ( 97.93)
Epoch: [3][190/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.0957e-01 (8.6141e-01)	Acc@1  71.88 ( 69.50)	Acc@5  96.09 ( 97.89)
Epoch: [3][200/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.3398e-01 (8.6202e-01)	Acc@1  70.31 ( 69.48)	Acc@5  94.53 ( 97.86)
Epoch: [3][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.6709e-01 (8.6144e-01)	Acc@1  71.09 ( 69.52)	Acc@5  97.66 ( 97.83)
Epoch: [3][220/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.4043e-01 (8.6121e-01)	Acc@1  68.75 ( 69.55)	Acc@5  94.53 ( 97.80)
Epoch: [3][230/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.5264e-01 (8.6023e-01)	Acc@1  69.53 ( 69.66)	Acc@5  97.66 ( 97.80)
Epoch: [3][240/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.4082e-01 (8.5932e-01)	Acc@1  68.75 ( 69.70)	Acc@5  98.44 ( 97.82)
Epoch: [3][250/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.3594e-01 (8.5986e-01)	Acc@1  70.31 ( 69.68)	Acc@5  96.09 ( 97.80)
Epoch: [3][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.6084e-01 (8.5788e-01)	Acc@1  69.53 ( 69.72)	Acc@5  99.22 ( 97.80)
Epoch: [3][270/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.0371e-01 (8.5638e-01)	Acc@1  73.44 ( 69.77)	Acc@5  96.88 ( 97.79)
Epoch: [3][280/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 8.3301e-01 (8.5384e-01)	Acc@1  71.09 ( 69.89)	Acc@5  98.44 ( 97.81)
Epoch: [3][290/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 8.1543e-01 (8.5388e-01)	Acc@1  72.66 ( 69.92)	Acc@5  96.88 ( 97.80)
Epoch: [3][300/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 8.3203e-01 (8.5209e-01)	Acc@1  71.88 ( 69.96)	Acc@5  98.44 ( 97.82)
Epoch: [3][310/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.8857e-01 (8.5168e-01)	Acc@1  71.88 ( 69.99)	Acc@5  99.22 ( 97.81)
Epoch: [3][320/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.6162e-01 (8.5094e-01)	Acc@1  75.78 ( 70.01)	Acc@5  98.44 ( 97.81)
Epoch: [3][330/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.5098e-01 (8.4817e-01)	Acc@1  73.44 ( 70.12)	Acc@5  98.44 ( 97.83)
Epoch: [3][340/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.2656e-01 (8.4658e-01)	Acc@1  74.22 ( 70.18)	Acc@5  99.22 ( 97.85)
Epoch: [3][350/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 8.0371e-01 (8.4515e-01)	Acc@1  70.31 ( 70.25)	Acc@5  97.66 ( 97.86)
Epoch: [3][360/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.3193e-01 (8.4310e-01)	Acc@1  71.88 ( 70.30)	Acc@5  98.44 ( 97.87)
Epoch: [3][370/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.3535e-01 (8.4131e-01)	Acc@1  74.22 ( 70.34)	Acc@5  99.22 ( 97.88)
Epoch: [3][380/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.6172e-01 (8.4029e-01)	Acc@1  75.00 ( 70.39)	Acc@5  98.44 ( 97.89)
Epoch: [3][390/391]	Time  0.031 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.8906e-01 (8.3866e-01)	Acc@1  70.00 ( 70.46)	Acc@5  97.50 ( 97.90)
## e[3] optimizer.zero_grad (sum) time: 0.2520885467529297
## e[3]       loss.backward (sum) time: 4.142747163772583
## e[3]      optimizer.step (sum) time: 1.8124442100524902
## epoch[3] training(only) time: 15.7883882522583
# Switched to evaluate mode...
Test: [  0/100]	Time  0.162 ( 0.162)	Loss 9.0869e-01 (9.0869e-01)	Acc@1  69.00 ( 69.00)	Acc@5  97.00 ( 97.00)
Test: [ 10/100]	Time  0.018 ( 0.032)	Loss 9.2725e-01 (8.9666e-01)	Acc@1  71.00 ( 68.73)	Acc@5  99.00 ( 97.91)
Test: [ 20/100]	Time  0.023 ( 0.026)	Loss 9.0039e-01 (8.6575e-01)	Acc@1  68.00 ( 69.57)	Acc@5  99.00 ( 98.05)
Test: [ 30/100]	Time  0.017 ( 0.024)	Loss 9.2627e-01 (8.8147e-01)	Acc@1  69.00 ( 69.26)	Acc@5  96.00 ( 97.81)
Test: [ 40/100]	Time  0.017 ( 0.023)	Loss 9.3506e-01 (8.8226e-01)	Acc@1  70.00 ( 69.20)	Acc@5  98.00 ( 97.73)
Test: [ 50/100]	Time  0.017 ( 0.022)	Loss 7.8320e-01 (8.6911e-01)	Acc@1  75.00 ( 69.86)	Acc@5  96.00 ( 97.67)
Test: [ 60/100]	Time  0.016 ( 0.022)	Loss 9.6240e-01 (8.7927e-01)	Acc@1  67.00 ( 69.62)	Acc@5  98.00 ( 97.77)
Test: [ 70/100]	Time  0.018 ( 0.021)	Loss 9.5654e-01 (8.7850e-01)	Acc@1  71.00 ( 69.62)	Acc@5  97.00 ( 97.86)
Test: [ 80/100]	Time  0.017 ( 0.021)	Loss 7.1191e-01 (8.8165e-01)	Acc@1  75.00 ( 69.60)	Acc@5  99.00 ( 97.84)
Test: [ 90/100]	Time  0.020 ( 0.021)	Loss 7.7344e-01 (8.8376e-01)	Acc@1  75.00 ( 69.64)	Acc@5  99.00 ( 97.85)
 * Acc@1 69.650 Acc@5 97.830
### epoch[3] execution time: 17.9562668800354
EPOCH 4
# current learning rate: 0.1
# Switched to train mode...
Epoch: [4][  0/391]	Time  0.205 ( 0.205)	Data  0.164 ( 0.164)	Loss 8.5010e-01 (8.5010e-01)	Acc@1  71.88 ( 71.88)	Acc@5  98.44 ( 98.44)
Epoch: [4][ 10/391]	Time  0.041 ( 0.056)	Data  0.001 ( 0.016)	Loss 6.3330e-01 (7.5857e-01)	Acc@1  83.59 ( 74.15)	Acc@5  98.44 ( 98.30)
Epoch: [4][ 20/391]	Time  0.039 ( 0.049)	Data  0.001 ( 0.009)	Loss 6.5576e-01 (7.3542e-01)	Acc@1  75.78 ( 73.85)	Acc@5  99.22 ( 98.51)
Epoch: [4][ 30/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.006)	Loss 7.2900e-01 (7.5187e-01)	Acc@1  75.78 ( 73.54)	Acc@5  97.66 ( 98.16)
Epoch: [4][ 40/391]	Time  0.044 ( 0.044)	Data  0.001 ( 0.005)	Loss 9.0576e-01 (7.6110e-01)	Acc@1  68.75 ( 73.27)	Acc@5  98.44 ( 98.17)
Epoch: [4][ 50/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.004)	Loss 8.1738e-01 (7.6293e-01)	Acc@1  70.31 ( 72.98)	Acc@5  98.44 ( 98.15)
Epoch: [4][ 60/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.004)	Loss 6.8604e-01 (7.6405e-01)	Acc@1  75.00 ( 72.72)	Acc@5  98.44 ( 98.18)
Epoch: [4][ 70/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.003)	Loss 8.0811e-01 (7.6491e-01)	Acc@1  71.88 ( 72.88)	Acc@5  97.66 ( 98.21)
Epoch: [4][ 80/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.9336e-01 (7.6188e-01)	Acc@1  78.91 ( 73.07)	Acc@5  97.66 ( 98.23)
Epoch: [4][ 90/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.5869e-01 (7.6239e-01)	Acc@1  76.56 ( 73.16)	Acc@5  98.44 ( 98.19)
Epoch: [4][100/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.1191e-01 (7.5811e-01)	Acc@1  76.56 ( 73.42)	Acc@5  97.66 ( 98.24)
Epoch: [4][110/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 7.3486e-01 (7.5345e-01)	Acc@1  75.00 ( 73.59)	Acc@5  99.22 ( 98.25)
Epoch: [4][120/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.9141e-01 (7.5517e-01)	Acc@1  77.34 ( 73.55)	Acc@5  96.88 ( 98.21)
Epoch: [4][130/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.6963e-01 (7.6022e-01)	Acc@1  69.53 ( 73.33)	Acc@5  96.88 ( 98.16)
Epoch: [4][140/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.1973e-01 (7.6008e-01)	Acc@1  73.44 ( 73.35)	Acc@5  97.66 ( 98.18)
Epoch: [4][150/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.0410e-01 (7.5815e-01)	Acc@1  79.69 ( 73.38)	Acc@5  97.66 ( 98.15)
Epoch: [4][160/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.7051e-01 (7.5513e-01)	Acc@1  70.31 ( 73.48)	Acc@5  99.22 ( 98.19)
Epoch: [4][170/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.3330e-01 (7.5356e-01)	Acc@1  75.78 ( 73.52)	Acc@5 100.00 ( 98.22)
Epoch: [4][180/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.1328e-01 (7.5303e-01)	Acc@1  80.47 ( 73.51)	Acc@5  97.66 ( 98.23)
Epoch: [4][190/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.6738e-01 (7.5207e-01)	Acc@1  79.69 ( 73.62)	Acc@5  99.22 ( 98.21)
Epoch: [4][200/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.9238e-01 (7.5054e-01)	Acc@1  75.78 ( 73.72)	Acc@5  97.66 ( 98.21)
Epoch: [4][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.6611e-01 (7.5003e-01)	Acc@1  77.34 ( 73.76)	Acc@5  96.09 ( 98.20)
Epoch: [4][220/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.8506e-01 (7.4927e-01)	Acc@1  74.22 ( 73.80)	Acc@5 100.00 ( 98.24)
Epoch: [4][230/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.7480e-01 (7.4957e-01)	Acc@1  73.44 ( 73.75)	Acc@5  99.22 ( 98.24)
Epoch: [4][240/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.5928e-01 (7.5016e-01)	Acc@1  75.78 ( 73.76)	Acc@5  97.66 ( 98.25)
Epoch: [4][250/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.9844e-01 (7.5050e-01)	Acc@1  71.09 ( 73.72)	Acc@5  98.44 ( 98.28)
Epoch: [4][260/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.7139e-01 (7.4881e-01)	Acc@1  78.12 ( 73.77)	Acc@5  98.44 ( 98.30)
Epoch: [4][270/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.1641e-01 (7.4859e-01)	Acc@1  69.53 ( 73.75)	Acc@5  98.44 ( 98.31)
Epoch: [4][280/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.5342e-01 (7.4731e-01)	Acc@1  75.00 ( 73.82)	Acc@5  96.88 ( 98.34)
Epoch: [4][290/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.8320e-01 (7.4694e-01)	Acc@1  73.44 ( 73.82)	Acc@5 100.00 ( 98.34)
Epoch: [4][300/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 9.8291e-01 (7.4893e-01)	Acc@1  67.19 ( 73.81)	Acc@5  96.88 ( 98.33)
Epoch: [4][310/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.1045e-01 (7.4773e-01)	Acc@1  74.22 ( 73.89)	Acc@5  97.66 ( 98.32)
Epoch: [4][320/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.9053e-01 (7.4872e-01)	Acc@1  76.56 ( 73.89)	Acc@5  94.53 ( 98.29)
Epoch: [4][330/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.3682e-01 (7.4809e-01)	Acc@1  75.78 ( 73.92)	Acc@5  96.09 ( 98.27)
Epoch: [4][340/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 9.0430e-01 (7.4832e-01)	Acc@1  70.31 ( 73.94)	Acc@5  96.88 ( 98.26)
Epoch: [4][350/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 9.5410e-01 (7.4769e-01)	Acc@1  63.28 ( 73.90)	Acc@5  98.44 ( 98.27)
Epoch: [4][360/391]	Time  0.048 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.8125e-01 (7.4723e-01)	Acc@1  72.66 ( 73.92)	Acc@5  96.09 ( 98.27)
Epoch: [4][370/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.7578e-01 (7.4690e-01)	Acc@1  78.12 ( 73.94)	Acc@5  98.44 ( 98.26)
Epoch: [4][380/391]	Time  0.043 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.3477e-01 (7.4530e-01)	Acc@1  77.34 ( 74.01)	Acc@5 100.00 ( 98.28)
Epoch: [4][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.0879e-01 (7.4371e-01)	Acc@1  85.00 ( 74.06)	Acc@5 100.00 ( 98.29)
## e[4] optimizer.zero_grad (sum) time: 0.25067615509033203
## e[4]       loss.backward (sum) time: 4.214133024215698
## e[4]      optimizer.step (sum) time: 1.778137445449829
## epoch[4] training(only) time: 15.924003839492798
# Switched to evaluate mode...
Test: [  0/100]	Time  0.178 ( 0.178)	Loss 7.6465e-01 (7.6465e-01)	Acc@1  72.00 ( 72.00)	Acc@5  97.00 ( 97.00)
Test: [ 10/100]	Time  0.023 ( 0.034)	Loss 8.0957e-01 (7.8991e-01)	Acc@1  75.00 ( 73.00)	Acc@5  97.00 ( 98.18)
Test: [ 20/100]	Time  0.017 ( 0.028)	Loss 1.0947e+00 (8.2043e-01)	Acc@1  63.00 ( 72.00)	Acc@5  98.00 ( 98.14)
Test: [ 30/100]	Time  0.018 ( 0.025)	Loss 9.3408e-01 (8.2304e-01)	Acc@1  69.00 ( 72.52)	Acc@5  93.00 ( 97.94)
Test: [ 40/100]	Time  0.018 ( 0.024)	Loss 6.9385e-01 (8.1576e-01)	Acc@1  78.00 ( 72.32)	Acc@5  97.00 ( 98.02)
Test: [ 50/100]	Time  0.017 ( 0.023)	Loss 7.0557e-01 (8.0756e-01)	Acc@1  71.00 ( 72.59)	Acc@5  97.00 ( 97.96)
Test: [ 60/100]	Time  0.023 ( 0.022)	Loss 8.6719e-01 (8.1175e-01)	Acc@1  70.00 ( 72.26)	Acc@5  98.00 ( 98.05)
Test: [ 70/100]	Time  0.021 ( 0.022)	Loss 8.0615e-01 (8.1973e-01)	Acc@1  76.00 ( 72.04)	Acc@5 100.00 ( 98.07)
Test: [ 80/100]	Time  0.019 ( 0.022)	Loss 6.1865e-01 (8.2261e-01)	Acc@1  79.00 ( 72.20)	Acc@5  98.00 ( 98.11)
Test: [ 90/100]	Time  0.020 ( 0.022)	Loss 7.3438e-01 (8.2808e-01)	Acc@1  74.00 ( 71.96)	Acc@5  98.00 ( 98.08)
 * Acc@1 71.980 Acc@5 98.090
### epoch[4] execution time: 18.18304681777954
EPOCH 5
# current learning rate: 0.1
# Switched to train mode...
Epoch: [5][  0/391]	Time  0.207 ( 0.207)	Data  0.165 ( 0.165)	Loss 7.6855e-01 (7.6855e-01)	Acc@1  75.00 ( 75.00)	Acc@5  96.88 ( 96.88)
Epoch: [5][ 10/391]	Time  0.038 ( 0.054)	Data  0.001 ( 0.016)	Loss 9.3750e-01 (6.9403e-01)	Acc@1  67.97 ( 76.07)	Acc@5  98.44 ( 98.01)
Epoch: [5][ 20/391]	Time  0.041 ( 0.048)	Data  0.001 ( 0.009)	Loss 7.2852e-01 (6.8580e-01)	Acc@1  72.66 ( 76.53)	Acc@5  97.66 ( 98.25)
Epoch: [5][ 30/391]	Time  0.040 ( 0.045)	Data  0.001 ( 0.006)	Loss 7.3047e-01 (6.8093e-01)	Acc@1  74.22 ( 76.74)	Acc@5  96.88 ( 98.31)
Epoch: [5][ 40/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.005)	Loss 8.2275e-01 (6.7808e-01)	Acc@1  71.09 ( 76.64)	Acc@5  96.09 ( 98.30)
Epoch: [5][ 50/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.004)	Loss 8.1494e-01 (6.9333e-01)	Acc@1  72.66 ( 76.32)	Acc@5  98.44 ( 98.19)
Epoch: [5][ 60/391]	Time  0.043 ( 0.043)	Data  0.001 ( 0.004)	Loss 6.3135e-01 (6.9268e-01)	Acc@1  75.00 ( 76.29)	Acc@5 100.00 ( 98.26)
Epoch: [5][ 70/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.9385e-01 (6.8778e-01)	Acc@1  74.22 ( 76.46)	Acc@5  97.66 ( 98.36)
Epoch: [5][ 80/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.5928e-01 (6.8270e-01)	Acc@1  74.22 ( 76.72)	Acc@5  96.88 ( 98.41)
Epoch: [5][ 90/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.0215e-01 (6.7938e-01)	Acc@1  76.56 ( 76.75)	Acc@5  99.22 ( 98.43)
Epoch: [5][100/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.9238e-01 (6.8117e-01)	Acc@1  73.44 ( 76.61)	Acc@5  98.44 ( 98.45)
Epoch: [5][110/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.3965e-01 (6.8214e-01)	Acc@1  78.91 ( 76.49)	Acc@5  99.22 ( 98.51)
Epoch: [5][120/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.0840e-01 (6.8445e-01)	Acc@1  79.69 ( 76.45)	Acc@5  99.22 ( 98.47)
Epoch: [5][130/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1562e-01 (6.8025e-01)	Acc@1  81.25 ( 76.57)	Acc@5 100.00 ( 98.47)
Epoch: [5][140/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.2412e-01 (6.7647e-01)	Acc@1  76.56 ( 76.71)	Acc@5  96.88 ( 98.48)
Epoch: [5][150/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8877e-01 (6.7361e-01)	Acc@1  82.03 ( 76.75)	Acc@5  99.22 ( 98.50)
Epoch: [5][160/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3091e-01 (6.7329e-01)	Acc@1  84.38 ( 76.74)	Acc@5  99.22 ( 98.51)
Epoch: [5][170/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.4355e-01 (6.7346e-01)	Acc@1  80.47 ( 76.75)	Acc@5  98.44 ( 98.56)
Epoch: [5][180/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.0801e-01 (6.7609e-01)	Acc@1  75.00 ( 76.62)	Acc@5  97.66 ( 98.56)
Epoch: [5][190/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.1240e-01 (6.7514e-01)	Acc@1  77.34 ( 76.62)	Acc@5  97.66 ( 98.55)
Epoch: [5][200/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.0410e-01 (6.7443e-01)	Acc@1  76.56 ( 76.63)	Acc@5  99.22 ( 98.55)
Epoch: [5][210/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.3584e-01 (6.7472e-01)	Acc@1  75.78 ( 76.63)	Acc@5  97.66 ( 98.56)
Epoch: [5][220/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2051e-01 (6.7473e-01)	Acc@1  83.59 ( 76.69)	Acc@5  98.44 ( 98.55)
Epoch: [5][230/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.2939e-01 (6.7407e-01)	Acc@1  78.12 ( 76.69)	Acc@5  98.44 ( 98.56)
Epoch: [5][240/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.3232e-01 (6.7198e-01)	Acc@1  75.00 ( 76.76)	Acc@5  99.22 ( 98.55)
Epoch: [5][250/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.6895e-01 (6.7155e-01)	Acc@1  76.56 ( 76.77)	Acc@5  98.44 ( 98.56)
Epoch: [5][260/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 7.1973e-01 (6.7061e-01)	Acc@1  78.12 ( 76.83)	Acc@5  97.66 ( 98.55)
Epoch: [5][270/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 7.5684e-01 (6.7209e-01)	Acc@1  69.53 ( 76.81)	Acc@5  98.44 ( 98.54)
Epoch: [5][280/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 5.9473e-01 (6.6990e-01)	Acc@1  75.00 ( 76.87)	Acc@5 100.00 ( 98.55)
Epoch: [5][290/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.7422e-01 (6.6888e-01)	Acc@1  81.25 ( 76.95)	Acc@5  99.22 ( 98.56)
Epoch: [5][300/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.2246e-01 (6.6859e-01)	Acc@1  79.69 ( 76.96)	Acc@5  99.22 ( 98.56)
Epoch: [5][310/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.6455e-01 (6.6767e-01)	Acc@1  76.56 ( 76.97)	Acc@5  99.22 ( 98.58)
Epoch: [5][320/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.4316e-01 (6.6739e-01)	Acc@1  75.78 ( 77.01)	Acc@5  96.88 ( 98.58)
Epoch: [5][330/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.1133e-01 (6.6657e-01)	Acc@1  78.12 ( 77.05)	Acc@5  99.22 ( 98.58)
Epoch: [5][340/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.9912e-01 (6.6506e-01)	Acc@1  78.91 ( 77.06)	Acc@5  99.22 ( 98.60)
Epoch: [5][350/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.2363e-01 (6.6553e-01)	Acc@1  71.88 ( 77.00)	Acc@5  98.44 ( 98.60)
Epoch: [5][360/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.0400e-01 (6.6549e-01)	Acc@1  79.69 ( 77.00)	Acc@5  97.66 ( 98.60)
Epoch: [5][370/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.0850e-01 (6.6452e-01)	Acc@1  76.56 ( 77.03)	Acc@5  97.66 ( 98.61)
Epoch: [5][380/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.4404e-01 (6.6306e-01)	Acc@1  77.34 ( 77.06)	Acc@5  98.44 ( 98.61)
Epoch: [5][390/391]	Time  0.029 ( 0.040)	Data  0.001 ( 0.001)	Loss 9.2334e-01 (6.6318e-01)	Acc@1  63.75 ( 77.05)	Acc@5 100.00 ( 98.60)
## e[5] optimizer.zero_grad (sum) time: 0.25333666801452637
## e[5]       loss.backward (sum) time: 4.133172273635864
## e[5]      optimizer.step (sum) time: 1.792524814605713
## epoch[5] training(only) time: 15.802698373794556
# Switched to evaluate mode...
Test: [  0/100]	Time  0.168 ( 0.168)	Loss 7.8271e-01 (7.8271e-01)	Acc@1  75.00 ( 75.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.034)	Loss 8.5303e-01 (7.8358e-01)	Acc@1  74.00 ( 74.27)	Acc@5  98.00 ( 98.91)
Test: [ 20/100]	Time  0.017 ( 0.027)	Loss 9.5264e-01 (7.8584e-01)	Acc@1  69.00 ( 73.67)	Acc@5  99.00 ( 98.67)
Test: [ 30/100]	Time  0.023 ( 0.025)	Loss 8.1250e-01 (7.8116e-01)	Acc@1  75.00 ( 73.97)	Acc@5  97.00 ( 98.52)
Test: [ 40/100]	Time  0.021 ( 0.024)	Loss 7.1582e-01 (7.7719e-01)	Acc@1  76.00 ( 74.10)	Acc@5  99.00 ( 98.44)
Test: [ 50/100]	Time  0.020 ( 0.023)	Loss 7.1387e-01 (7.7341e-01)	Acc@1  76.00 ( 74.49)	Acc@5  98.00 ( 98.33)
Test: [ 60/100]	Time  0.017 ( 0.023)	Loss 7.4365e-01 (7.7179e-01)	Acc@1  73.00 ( 74.52)	Acc@5 100.00 ( 98.34)
Test: [ 70/100]	Time  0.020 ( 0.022)	Loss 7.2656e-01 (7.6962e-01)	Acc@1  76.00 ( 74.65)	Acc@5  99.00 ( 98.39)
Test: [ 80/100]	Time  0.023 ( 0.022)	Loss 5.7129e-01 (7.6978e-01)	Acc@1  80.00 ( 74.58)	Acc@5 100.00 ( 98.40)
Test: [ 90/100]	Time  0.021 ( 0.022)	Loss 6.6260e-01 (7.7129e-01)	Acc@1  77.00 ( 74.64)	Acc@5 100.00 ( 98.40)
 * Acc@1 74.670 Acc@5 98.420
### epoch[5] execution time: 18.06844425201416
EPOCH 6
# current learning rate: 0.1
# Switched to train mode...
Epoch: [6][  0/391]	Time  0.220 ( 0.220)	Data  0.170 ( 0.170)	Loss 6.0205e-01 (6.0205e-01)	Acc@1  82.81 ( 82.81)	Acc@5  96.88 ( 96.88)
Epoch: [6][ 10/391]	Time  0.042 ( 0.056)	Data  0.001 ( 0.016)	Loss 6.3379e-01 (6.7068e-01)	Acc@1  77.34 ( 78.05)	Acc@5  99.22 ( 98.44)
Epoch: [6][ 20/391]	Time  0.038 ( 0.049)	Data  0.001 ( 0.009)	Loss 5.9277e-01 (6.3177e-01)	Acc@1  78.12 ( 77.64)	Acc@5 100.00 ( 98.77)
Epoch: [6][ 30/391]	Time  0.038 ( 0.046)	Data  0.001 ( 0.006)	Loss 7.4561e-01 (6.3862e-01)	Acc@1  75.00 ( 77.55)	Acc@5  98.44 ( 98.61)
Epoch: [6][ 40/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.005)	Loss 4.5142e-01 (6.2839e-01)	Acc@1  85.94 ( 78.20)	Acc@5  98.44 ( 98.59)
Epoch: [6][ 50/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.004)	Loss 5.9717e-01 (6.3258e-01)	Acc@1  79.69 ( 78.20)	Acc@5  98.44 ( 98.58)
Epoch: [6][ 60/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.004)	Loss 6.3281e-01 (6.1881e-01)	Acc@1  79.69 ( 78.52)	Acc@5  97.66 ( 98.64)
Epoch: [6][ 70/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.5752e-01 (6.1564e-01)	Acc@1  84.38 ( 78.52)	Acc@5  99.22 ( 98.66)
Epoch: [6][ 80/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.9131e-01 (6.1477e-01)	Acc@1  76.56 ( 78.50)	Acc@5  99.22 ( 98.66)
Epoch: [6][ 90/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.2295e-01 (6.1707e-01)	Acc@1  82.81 ( 78.43)	Acc@5 100.00 ( 98.69)
Epoch: [6][100/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.2197e-01 (6.1580e-01)	Acc@1  84.38 ( 78.53)	Acc@5  99.22 ( 98.69)
Epoch: [6][110/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.9287e-01 (6.1938e-01)	Acc@1  77.34 ( 78.40)	Acc@5  97.66 ( 98.73)
Epoch: [6][120/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.5127e-01 (6.1922e-01)	Acc@1  79.69 ( 78.30)	Acc@5  99.22 ( 98.71)
Epoch: [6][130/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.4688e-01 (6.1836e-01)	Acc@1  79.69 ( 78.34)	Acc@5 100.00 ( 98.74)
Epoch: [6][140/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2881e-01 (6.1648e-01)	Acc@1  79.69 ( 78.41)	Acc@5 100.00 ( 98.74)
Epoch: [6][150/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.0205e-01 (6.1496e-01)	Acc@1  77.34 ( 78.43)	Acc@5 100.00 ( 98.75)
Epoch: [6][160/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.4160e-01 (6.1558e-01)	Acc@1  81.25 ( 78.43)	Acc@5  99.22 ( 98.78)
Epoch: [6][170/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.1729e-01 (6.1704e-01)	Acc@1  73.44 ( 78.38)	Acc@5  99.22 ( 98.78)
Epoch: [6][180/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0977e-01 (6.1865e-01)	Acc@1  80.47 ( 78.29)	Acc@5  99.22 ( 98.77)
Epoch: [6][190/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.9375e-01 (6.1945e-01)	Acc@1  80.47 ( 78.30)	Acc@5  98.44 ( 98.76)
Epoch: [6][200/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.6309e-01 (6.1812e-01)	Acc@1  77.34 ( 78.42)	Acc@5  98.44 ( 98.76)
Epoch: [6][210/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.7617e-01 (6.1472e-01)	Acc@1  82.03 ( 78.51)	Acc@5  97.66 ( 98.77)
Epoch: [6][220/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7119e-01 (6.1337e-01)	Acc@1  85.94 ( 78.52)	Acc@5 100.00 ( 98.78)
Epoch: [6][230/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2002e-01 (6.1259e-01)	Acc@1  82.03 ( 78.58)	Acc@5 100.00 ( 98.79)
Epoch: [6][240/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.3232e-01 (6.1440e-01)	Acc@1  74.22 ( 78.52)	Acc@5  99.22 ( 98.79)
Epoch: [6][250/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.6948e-01 (6.1335e-01)	Acc@1  86.72 ( 78.56)	Acc@5  99.22 ( 98.81)
Epoch: [6][260/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.8301e-01 (6.1116e-01)	Acc@1  78.91 ( 78.63)	Acc@5 100.00 ( 98.82)
Epoch: [6][270/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.4746e-01 (6.1069e-01)	Acc@1  75.00 ( 78.64)	Acc@5  99.22 ( 98.85)
Epoch: [6][280/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.1240e-01 (6.1088e-01)	Acc@1  78.12 ( 78.66)	Acc@5  98.44 ( 98.86)
Epoch: [6][290/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.9619e-01 (6.1000e-01)	Acc@1  78.91 ( 78.70)	Acc@5  99.22 ( 98.85)
Epoch: [6][300/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.2842e-01 (6.0885e-01)	Acc@1  75.78 ( 78.74)	Acc@5  99.22 ( 98.84)
Epoch: [6][310/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.0166e-01 (6.0977e-01)	Acc@1  76.56 ( 78.72)	Acc@5  99.22 ( 98.83)
Epoch: [6][320/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.4121e-01 (6.0983e-01)	Acc@1  71.88 ( 78.72)	Acc@5  96.88 ( 98.82)
Epoch: [6][330/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.2002e-01 (6.0958e-01)	Acc@1  83.59 ( 78.72)	Acc@5  99.22 ( 98.83)
Epoch: [6][340/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.8643e-01 (6.0895e-01)	Acc@1  76.56 ( 78.68)	Acc@5  98.44 ( 98.82)
Epoch: [6][350/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.8594e-01 (6.0920e-01)	Acc@1  77.34 ( 78.65)	Acc@5 100.00 ( 98.83)
Epoch: [6][360/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.9102e-01 (6.0972e-01)	Acc@1  73.44 ( 78.67)	Acc@5  96.09 ( 98.82)
Epoch: [6][370/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.9395e-01 (6.0907e-01)	Acc@1  78.12 ( 78.77)	Acc@5  98.44 ( 98.83)
Epoch: [6][380/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.2393e-01 (6.0867e-01)	Acc@1  78.91 ( 78.78)	Acc@5  99.22 ( 98.83)
Epoch: [6][390/391]	Time  0.028 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.2207e-01 (6.1014e-01)	Acc@1  82.50 ( 78.74)	Acc@5 100.00 ( 98.83)
## e[6] optimizer.zero_grad (sum) time: 0.2504692077636719
## e[6]       loss.backward (sum) time: 4.229239463806152
## e[6]      optimizer.step (sum) time: 1.749495267868042
## epoch[6] training(only) time: 15.988923072814941
# Switched to evaluate mode...
Test: [  0/100]	Time  0.193 ( 0.193)	Loss 8.4229e-01 (8.4229e-01)	Acc@1  75.00 ( 75.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.021 ( 0.035)	Loss 8.9014e-01 (8.7709e-01)	Acc@1  68.00 ( 71.64)	Acc@5  99.00 ( 98.55)
Test: [ 20/100]	Time  0.017 ( 0.028)	Loss 9.5508e-01 (8.4333e-01)	Acc@1  68.00 ( 72.10)	Acc@5  97.00 ( 98.43)
Test: [ 30/100]	Time  0.024 ( 0.025)	Loss 7.7002e-01 (8.5441e-01)	Acc@1  72.00 ( 72.39)	Acc@5  98.00 ( 98.23)
Test: [ 40/100]	Time  0.017 ( 0.024)	Loss 8.0859e-01 (8.6142e-01)	Acc@1  77.00 ( 71.93)	Acc@5  98.00 ( 98.22)
Test: [ 50/100]	Time  0.019 ( 0.023)	Loss 8.2373e-01 (8.5514e-01)	Acc@1  71.00 ( 72.16)	Acc@5  95.00 ( 98.14)
Test: [ 60/100]	Time  0.018 ( 0.023)	Loss 8.4766e-01 (8.5317e-01)	Acc@1  71.00 ( 71.98)	Acc@5  99.00 ( 98.21)
Test: [ 70/100]	Time  0.020 ( 0.022)	Loss 8.4814e-01 (8.5005e-01)	Acc@1  75.00 ( 72.01)	Acc@5 100.00 ( 98.27)
Test: [ 80/100]	Time  0.020 ( 0.022)	Loss 7.4463e-01 (8.5169e-01)	Acc@1  69.00 ( 72.02)	Acc@5  99.00 ( 98.30)
Test: [ 90/100]	Time  0.017 ( 0.021)	Loss 8.3545e-01 (8.5416e-01)	Acc@1  71.00 ( 71.98)	Acc@5 100.00 ( 98.30)
 * Acc@1 72.020 Acc@5 98.310
### epoch[6] execution time: 18.208956718444824
EPOCH 7
# current learning rate: 0.1
# Switched to train mode...
Epoch: [7][  0/391]	Time  0.221 ( 0.221)	Data  0.175 ( 0.175)	Loss 8.8965e-01 (8.8965e-01)	Acc@1  68.75 ( 68.75)	Acc@5  94.53 ( 94.53)
Epoch: [7][ 10/391]	Time  0.038 ( 0.056)	Data  0.001 ( 0.017)	Loss 5.3906e-01 (6.4238e-01)	Acc@1  82.03 ( 77.63)	Acc@5  99.22 ( 98.22)
Epoch: [7][ 20/391]	Time  0.039 ( 0.048)	Data  0.001 ( 0.009)	Loss 6.2354e-01 (6.1339e-01)	Acc@1  75.78 ( 78.42)	Acc@5  98.44 ( 98.51)
Epoch: [7][ 30/391]	Time  0.038 ( 0.046)	Data  0.001 ( 0.007)	Loss 5.0391e-01 (6.0693e-01)	Acc@1  78.12 ( 78.15)	Acc@5 100.00 ( 98.71)
Epoch: [7][ 40/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.005)	Loss 5.2686e-01 (6.0126e-01)	Acc@1  82.81 ( 78.54)	Acc@5  99.22 ( 98.84)
Epoch: [7][ 50/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 4.2651e-01 (5.9705e-01)	Acc@1  85.94 ( 78.80)	Acc@5 100.00 ( 98.96)
Epoch: [7][ 60/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 4.8291e-01 (5.9248e-01)	Acc@1  84.38 ( 79.00)	Acc@5  98.44 ( 98.92)
Epoch: [7][ 70/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.003)	Loss 5.0342e-01 (5.8843e-01)	Acc@1  80.47 ( 79.26)	Acc@5  98.44 ( 98.86)
Epoch: [7][ 80/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.2305e-01 (5.8863e-01)	Acc@1  81.25 ( 79.40)	Acc@5  98.44 ( 98.85)
Epoch: [7][ 90/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.0498e-01 (5.8464e-01)	Acc@1  80.47 ( 79.56)	Acc@5  98.44 ( 98.86)
Epoch: [7][100/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.5508e-01 (5.8016e-01)	Acc@1  82.03 ( 79.76)	Acc@5  99.22 ( 98.87)
Epoch: [7][110/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.3140e-01 (5.7518e-01)	Acc@1  85.94 ( 79.95)	Acc@5  99.22 ( 98.91)
Epoch: [7][120/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0259e-01 (5.7688e-01)	Acc@1  86.72 ( 79.93)	Acc@5 100.00 ( 98.89)
Epoch: [7][130/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.7090e-01 (5.7670e-01)	Acc@1  78.12 ( 79.96)	Acc@5  98.44 ( 98.90)
Epoch: [7][140/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7412e-01 (5.7482e-01)	Acc@1  84.38 ( 80.05)	Acc@5 100.00 ( 98.91)
Epoch: [7][150/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.1768e-01 (5.7747e-01)	Acc@1  81.25 ( 79.92)	Acc@5  99.22 ( 98.91)
Epoch: [7][160/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.9961e-01 (5.7579e-01)	Acc@1  79.69 ( 79.98)	Acc@5  98.44 ( 98.93)
Epoch: [7][170/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2930e-01 (5.7412e-01)	Acc@1  82.81 ( 80.07)	Acc@5  99.22 ( 98.93)
Epoch: [7][180/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.7520e-01 (5.7278e-01)	Acc@1  80.47 ( 80.14)	Acc@5  97.66 ( 98.92)
Epoch: [7][190/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.3467e-01 (5.7267e-01)	Acc@1  82.81 ( 80.10)	Acc@5  99.22 ( 98.91)
Epoch: [7][200/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2148e-01 (5.7336e-01)	Acc@1  82.81 ( 80.05)	Acc@5 100.00 ( 98.93)
Epoch: [7][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2881e-01 (5.7279e-01)	Acc@1  81.25 ( 80.09)	Acc@5  99.22 ( 98.93)
Epoch: [7][220/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.6689e-01 (5.7460e-01)	Acc@1  77.34 ( 80.02)	Acc@5  99.22 ( 98.91)
Epoch: [7][230/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1611e-01 (5.7439e-01)	Acc@1  82.03 ( 79.98)	Acc@5  99.22 ( 98.90)
Epoch: [7][240/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.4248e-01 (5.7324e-01)	Acc@1  81.25 ( 80.02)	Acc@5  99.22 ( 98.91)
Epoch: [7][250/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.0996e-01 (5.7300e-01)	Acc@1  76.56 ( 80.00)	Acc@5 100.00 ( 98.93)
Epoch: [7][260/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0527e-01 (5.7325e-01)	Acc@1  83.59 ( 80.01)	Acc@5 100.00 ( 98.90)
Epoch: [7][270/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.002)	Loss 7.2168e-01 (5.7479e-01)	Acc@1  74.22 ( 79.98)	Acc@5  99.22 ( 98.90)
Epoch: [7][280/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 3.6475e-01 (5.7464e-01)	Acc@1  86.72 ( 79.96)	Acc@5 100.00 ( 98.90)
Epoch: [7][290/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 4.9316e-01 (5.7201e-01)	Acc@1  86.72 ( 80.06)	Acc@5  99.22 ( 98.91)
Epoch: [7][300/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.2651e-01 (5.6985e-01)	Acc@1  85.16 ( 80.14)	Acc@5 100.00 ( 98.92)
Epoch: [7][310/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.4307e-01 (5.6962e-01)	Acc@1  78.91 ( 80.17)	Acc@5  99.22 ( 98.91)
Epoch: [7][320/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.1523e-01 (5.6854e-01)	Acc@1  78.12 ( 80.22)	Acc@5  98.44 ( 98.91)
Epoch: [7][330/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.3262e-01 (5.6823e-01)	Acc@1  85.16 ( 80.26)	Acc@5  98.44 ( 98.90)
Epoch: [7][340/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.1572e-01 (5.6789e-01)	Acc@1  77.34 ( 80.27)	Acc@5 100.00 ( 98.91)
Epoch: [7][350/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.4043e-01 (5.6878e-01)	Acc@1  85.94 ( 80.23)	Acc@5  99.22 ( 98.91)
Epoch: [7][360/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.5225e-01 (5.6942e-01)	Acc@1  81.25 ( 80.23)	Acc@5  99.22 ( 98.92)
Epoch: [7][370/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.9194e-01 (5.6987e-01)	Acc@1  82.03 ( 80.19)	Acc@5 100.00 ( 98.93)
Epoch: [7][380/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.9634e-01 (5.6869e-01)	Acc@1  81.25 ( 80.22)	Acc@5 100.00 ( 98.93)
Epoch: [7][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.8799e-01 (5.6916e-01)	Acc@1  78.75 ( 80.22)	Acc@5  97.50 ( 98.92)
## e[7] optimizer.zero_grad (sum) time: 0.24909734725952148
## e[7]       loss.backward (sum) time: 4.156692266464233
## e[7]      optimizer.step (sum) time: 1.7763285636901855
## epoch[7] training(only) time: 15.88259482383728
# Switched to evaluate mode...
Test: [  0/100]	Time  0.175 ( 0.175)	Loss 8.4961e-01 (8.4961e-01)	Acc@1  74.00 ( 74.00)	Acc@5  97.00 ( 97.00)
Test: [ 10/100]	Time  0.018 ( 0.034)	Loss 6.5479e-01 (7.6913e-01)	Acc@1  79.00 ( 74.55)	Acc@5  97.00 ( 98.73)
Test: [ 20/100]	Time  0.017 ( 0.028)	Loss 9.6387e-01 (7.6597e-01)	Acc@1  64.00 ( 74.19)	Acc@5  98.00 ( 98.43)
Test: [ 30/100]	Time  0.017 ( 0.025)	Loss 9.6094e-01 (7.8525e-01)	Acc@1  67.00 ( 73.97)	Acc@5  97.00 ( 98.39)
Test: [ 40/100]	Time  0.023 ( 0.023)	Loss 5.3125e-01 (7.7577e-01)	Acc@1  81.00 ( 74.17)	Acc@5 100.00 ( 98.29)
Test: [ 50/100]	Time  0.023 ( 0.023)	Loss 7.3828e-01 (7.6177e-01)	Acc@1  73.00 ( 74.57)	Acc@5  96.00 ( 98.31)
Test: [ 60/100]	Time  0.021 ( 0.022)	Loss 9.1650e-01 (7.6755e-01)	Acc@1  72.00 ( 74.33)	Acc@5 100.00 ( 98.31)
Test: [ 70/100]	Time  0.019 ( 0.022)	Loss 6.7822e-01 (7.6172e-01)	Acc@1  75.00 ( 74.46)	Acc@5  99.00 ( 98.37)
Test: [ 80/100]	Time  0.022 ( 0.022)	Loss 7.1191e-01 (7.6203e-01)	Acc@1  76.00 ( 74.63)	Acc@5  99.00 ( 98.38)
Test: [ 90/100]	Time  0.022 ( 0.022)	Loss 6.0449e-01 (7.6158e-01)	Acc@1  81.00 ( 74.76)	Acc@5 100.00 ( 98.36)
 * Acc@1 74.870 Acc@5 98.320
### epoch[7] execution time: 18.166630744934082
EPOCH 8
# current learning rate: 0.1
# Switched to train mode...
Epoch: [8][  0/391]	Time  0.206 ( 0.206)	Data  0.166 ( 0.166)	Loss 4.8047e-01 (4.8047e-01)	Acc@1  82.03 ( 82.03)	Acc@5  99.22 ( 99.22)
Epoch: [8][ 10/391]	Time  0.039 ( 0.055)	Data  0.001 ( 0.016)	Loss 5.0098e-01 (4.8178e-01)	Acc@1  81.25 ( 82.32)	Acc@5  99.22 ( 99.29)
Epoch: [8][ 20/391]	Time  0.041 ( 0.048)	Data  0.001 ( 0.009)	Loss 4.3628e-01 (5.1773e-01)	Acc@1  85.16 ( 81.85)	Acc@5  99.22 ( 99.00)
Epoch: [8][ 30/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.006)	Loss 4.8340e-01 (5.3321e-01)	Acc@1  82.81 ( 81.22)	Acc@5  98.44 ( 98.94)
Epoch: [8][ 40/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.005)	Loss 4.7974e-01 (5.3956e-01)	Acc@1  85.16 ( 81.31)	Acc@5 100.00 ( 99.03)
Epoch: [8][ 50/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.004)	Loss 5.2539e-01 (5.5073e-01)	Acc@1  79.69 ( 80.84)	Acc@5  99.22 ( 99.00)
Epoch: [8][ 60/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.004)	Loss 5.7715e-01 (5.5526e-01)	Acc@1  76.56 ( 80.84)	Acc@5 100.00 ( 98.96)
Epoch: [8][ 70/391]	Time  0.034 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.0352e-01 (5.5169e-01)	Acc@1  79.69 ( 80.89)	Acc@5  99.22 ( 99.01)
Epoch: [8][ 80/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.7959e-01 (5.5478e-01)	Acc@1  81.25 ( 80.71)	Acc@5  99.22 ( 99.03)
Epoch: [8][ 90/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.003)	Loss 6.6455e-01 (5.4858e-01)	Acc@1  78.91 ( 81.07)	Acc@5  99.22 ( 99.00)
Epoch: [8][100/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.003)	Loss 4.2554e-01 (5.4558e-01)	Acc@1  85.94 ( 81.13)	Acc@5  99.22 ( 99.01)
Epoch: [8][110/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.4404e-01 (5.4960e-01)	Acc@1  77.34 ( 80.92)	Acc@5 100.00 ( 99.05)
Epoch: [8][120/391]	Time  0.034 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.4844e-01 (5.5066e-01)	Acc@1  78.12 ( 80.90)	Acc@5  96.09 ( 99.04)
Epoch: [8][130/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.5078e-01 (5.5334e-01)	Acc@1  82.81 ( 80.85)	Acc@5  98.44 ( 99.06)
Epoch: [8][140/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7339e-01 (5.5393e-01)	Acc@1  85.16 ( 80.85)	Acc@5  99.22 ( 99.02)
Epoch: [8][150/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.3125e-01 (5.5419e-01)	Acc@1  81.25 ( 80.79)	Acc@5  98.44 ( 99.03)
Epoch: [8][160/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7437e-01 (5.5280e-01)	Acc@1  83.59 ( 80.89)	Acc@5 100.00 ( 99.05)
Epoch: [8][170/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.6602e-01 (5.5570e-01)	Acc@1  78.12 ( 80.76)	Acc@5  99.22 ( 99.03)
Epoch: [8][180/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9390e-01 (5.5496e-01)	Acc@1  77.34 ( 80.74)	Acc@5  99.22 ( 99.05)
Epoch: [8][190/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.5039e-01 (5.5716e-01)	Acc@1  78.12 ( 80.68)	Acc@5  99.22 ( 99.03)
Epoch: [8][200/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.3672e-01 (5.5378e-01)	Acc@1  78.91 ( 80.80)	Acc@5 100.00 ( 99.04)
Epoch: [8][210/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0723e-01 (5.5516e-01)	Acc@1  89.84 ( 80.76)	Acc@5 100.00 ( 99.04)
Epoch: [8][220/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3945e-01 (5.5465e-01)	Acc@1  86.72 ( 80.74)	Acc@5  98.44 ( 99.03)
Epoch: [8][230/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 5.0342e-01 (5.5469e-01)	Acc@1  82.03 ( 80.77)	Acc@5  99.22 ( 99.04)
Epoch: [8][240/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 5.8838e-01 (5.5312e-01)	Acc@1  76.56 ( 80.84)	Acc@5  99.22 ( 99.02)
Epoch: [8][250/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 5.7715e-01 (5.5264e-01)	Acc@1  80.47 ( 80.85)	Acc@5  97.66 ( 99.01)
Epoch: [8][260/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 5.4980e-01 (5.5083e-01)	Acc@1  82.03 ( 80.93)	Acc@5  99.22 ( 99.03)
Epoch: [8][270/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 5.4639e-01 (5.5091e-01)	Acc@1  83.59 ( 80.99)	Acc@5  99.22 ( 99.04)
Epoch: [8][280/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.002)	Loss 6.7383e-01 (5.5147e-01)	Acc@1  76.56 ( 80.95)	Acc@5  97.66 ( 99.02)
Epoch: [8][290/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.7339e-01 (5.5178e-01)	Acc@1  85.16 ( 80.94)	Acc@5  99.22 ( 99.02)
Epoch: [8][300/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.4238e-01 (5.4981e-01)	Acc@1  82.03 ( 80.99)	Acc@5 100.00 ( 99.03)
Epoch: [8][310/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.6406e-01 (5.4933e-01)	Acc@1  75.78 ( 80.98)	Acc@5 100.00 ( 99.05)
Epoch: [8][320/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.7729e-01 (5.4993e-01)	Acc@1  81.25 ( 80.95)	Acc@5 100.00 ( 99.04)
Epoch: [8][330/391]	Time  0.044 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.2051e-01 (5.5018e-01)	Acc@1  83.59 ( 80.96)	Acc@5  97.66 ( 99.03)
Epoch: [8][340/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.8105e-01 (5.5079e-01)	Acc@1  85.16 ( 80.97)	Acc@5  99.22 ( 99.03)
Epoch: [8][350/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.3335e-01 (5.5116e-01)	Acc@1  85.94 ( 80.95)	Acc@5 100.00 ( 99.03)
Epoch: [8][360/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.7129e-01 (5.5065e-01)	Acc@1  79.69 ( 80.96)	Acc@5 100.00 ( 99.03)
Epoch: [8][370/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.8350e-01 (5.4995e-01)	Acc@1  78.91 ( 80.98)	Acc@5  98.44 ( 99.03)
Epoch: [8][380/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.9521e-01 (5.4883e-01)	Acc@1  81.25 ( 81.01)	Acc@5  99.22 ( 99.04)
Epoch: [8][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.2393e-01 (5.4860e-01)	Acc@1  78.75 ( 81.04)	Acc@5 100.00 ( 99.04)
## e[8] optimizer.zero_grad (sum) time: 0.2514045238494873
## e[8]       loss.backward (sum) time: 4.129547119140625
## e[8]      optimizer.step (sum) time: 1.8012075424194336
## epoch[8] training(only) time: 15.844970226287842
# Switched to evaluate mode...
Test: [  0/100]	Time  0.171 ( 0.171)	Loss 6.2451e-01 (6.2451e-01)	Acc@1  82.00 ( 82.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.020 ( 0.034)	Loss 4.9365e-01 (6.4881e-01)	Acc@1  82.00 ( 78.00)	Acc@5  99.00 ( 99.27)
Test: [ 20/100]	Time  0.017 ( 0.028)	Loss 8.9062e-01 (6.5298e-01)	Acc@1  64.00 ( 78.05)	Acc@5  99.00 ( 99.10)
Test: [ 30/100]	Time  0.024 ( 0.026)	Loss 6.8701e-01 (6.5203e-01)	Acc@1  82.00 ( 78.39)	Acc@5  97.00 ( 98.94)
Test: [ 40/100]	Time  0.020 ( 0.024)	Loss 6.1816e-01 (6.4744e-01)	Acc@1  83.00 ( 78.85)	Acc@5  98.00 ( 98.98)
Test: [ 50/100]	Time  0.019 ( 0.023)	Loss 6.0303e-01 (6.3887e-01)	Acc@1  79.00 ( 79.35)	Acc@5  97.00 ( 98.92)
Test: [ 60/100]	Time  0.019 ( 0.023)	Loss 6.7627e-01 (6.4972e-01)	Acc@1  75.00 ( 78.57)	Acc@5 100.00 ( 98.87)
Test: [ 70/100]	Time  0.020 ( 0.022)	Loss 5.8398e-01 (6.4724e-01)	Acc@1  81.00 ( 78.66)	Acc@5  98.00 ( 98.87)
Test: [ 80/100]	Time  0.017 ( 0.022)	Loss 5.0781e-01 (6.4096e-01)	Acc@1  77.00 ( 78.83)	Acc@5 100.00 ( 98.90)
Test: [ 90/100]	Time  0.021 ( 0.022)	Loss 4.5557e-01 (6.4249e-01)	Acc@1  84.00 ( 78.77)	Acc@5 100.00 ( 98.93)
 * Acc@1 78.840 Acc@5 98.900
### epoch[8] execution time: 18.14626979827881
EPOCH 9
# current learning rate: 0.1
# Switched to train mode...
Epoch: [9][  0/391]	Time  0.221 ( 0.221)	Data  0.172 ( 0.172)	Loss 7.1631e-01 (7.1631e-01)	Acc@1  76.56 ( 76.56)	Acc@5  98.44 ( 98.44)
Epoch: [9][ 10/391]	Time  0.038 ( 0.056)	Data  0.001 ( 0.016)	Loss 5.0439e-01 (5.3420e-01)	Acc@1  80.47 ( 81.25)	Acc@5  99.22 ( 99.15)
Epoch: [9][ 20/391]	Time  0.042 ( 0.048)	Data  0.001 ( 0.009)	Loss 4.2603e-01 (5.2361e-01)	Acc@1  85.94 ( 81.77)	Acc@5  98.44 ( 99.11)
Epoch: [9][ 30/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.006)	Loss 5.2588e-01 (5.3964e-01)	Acc@1  81.25 ( 81.40)	Acc@5 100.00 ( 99.02)
Epoch: [9][ 40/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.005)	Loss 4.9878e-01 (5.3407e-01)	Acc@1  81.25 ( 81.50)	Acc@5  99.22 ( 99.05)
Epoch: [9][ 50/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.004)	Loss 6.1963e-01 (5.2222e-01)	Acc@1  77.34 ( 81.88)	Acc@5  99.22 ( 99.11)
Epoch: [9][ 60/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.004)	Loss 2.7417e-01 (5.1824e-01)	Acc@1  91.41 ( 81.98)	Acc@5 100.00 ( 99.18)
Epoch: [9][ 70/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.003)	Loss 5.6348e-01 (5.2016e-01)	Acc@1  78.91 ( 81.81)	Acc@5  99.22 ( 99.14)
Epoch: [9][ 80/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.7031e-01 (5.2303e-01)	Acc@1  79.69 ( 81.72)	Acc@5  99.22 ( 99.14)
Epoch: [9][ 90/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.9243e-01 (5.3126e-01)	Acc@1  84.38 ( 81.44)	Acc@5  99.22 ( 99.14)
Epoch: [9][100/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.3418e-01 (5.2743e-01)	Acc@1  78.91 ( 81.58)	Acc@5  98.44 ( 99.14)
Epoch: [9][110/391]	Time  0.044 ( 0.042)	Data  0.003 ( 0.002)	Loss 3.9624e-01 (5.2776e-01)	Acc@1  87.50 ( 81.64)	Acc@5 100.00 ( 99.14)
Epoch: [9][120/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.4102e-01 (5.2600e-01)	Acc@1  82.03 ( 81.64)	Acc@5  98.44 ( 99.15)
Epoch: [9][130/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.4102e-01 (5.2530e-01)	Acc@1  78.12 ( 81.66)	Acc@5 100.00 ( 99.12)
Epoch: [9][140/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.8691e-01 (5.2591e-01)	Acc@1  82.03 ( 81.65)	Acc@5  98.44 ( 99.11)
Epoch: [9][150/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.3809e-01 (5.2536e-01)	Acc@1  85.16 ( 81.74)	Acc@5  97.66 ( 99.11)
Epoch: [9][160/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.6069e-01 (5.2252e-01)	Acc@1  85.94 ( 81.85)	Acc@5 100.00 ( 99.12)
Epoch: [9][170/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4922e-01 (5.2036e-01)	Acc@1  82.03 ( 81.90)	Acc@5 100.00 ( 99.13)
Epoch: [9][180/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.3076e-01 (5.1914e-01)	Acc@1  81.25 ( 82.00)	Acc@5  99.22 ( 99.10)
Epoch: [9][190/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1318e-01 (5.1868e-01)	Acc@1  81.25 ( 82.04)	Acc@5  99.22 ( 99.08)
Epoch: [9][200/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4141e-01 (5.1979e-01)	Acc@1  83.59 ( 82.02)	Acc@5  99.22 ( 99.07)
Epoch: [9][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0732e-01 (5.1722e-01)	Acc@1  80.47 ( 82.09)	Acc@5  97.66 ( 99.08)
Epoch: [9][220/391]	Time  0.050 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.5820e-01 (5.1755e-01)	Acc@1  75.78 ( 82.08)	Acc@5  98.44 ( 99.08)
Epoch: [9][230/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0649e-01 (5.1851e-01)	Acc@1  83.59 ( 81.97)	Acc@5  99.22 ( 99.09)
Epoch: [9][240/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.5020e-01 (5.1614e-01)	Acc@1  81.25 ( 82.03)	Acc@5 100.00 ( 99.10)
Epoch: [9][250/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.6396e-01 (5.1736e-01)	Acc@1  76.56 ( 81.97)	Acc@5  99.22 ( 99.09)
Epoch: [9][260/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2100e-01 (5.1609e-01)	Acc@1  78.91 ( 81.98)	Acc@5 100.00 ( 99.10)
Epoch: [9][270/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.4600e-01 (5.1655e-01)	Acc@1  74.22 ( 81.96)	Acc@5  98.44 ( 99.10)
Epoch: [9][280/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.9976e-01 (5.1525e-01)	Acc@1  81.25 ( 82.04)	Acc@5  99.22 ( 99.11)
Epoch: [9][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.1318e-01 (5.1465e-01)	Acc@1  82.81 ( 82.08)	Acc@5  97.66 ( 99.11)
Epoch: [9][300/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.7437e-01 (5.1568e-01)	Acc@1  84.38 ( 82.08)	Acc@5  99.22 ( 99.11)
Epoch: [9][310/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.0537e-01 (5.1473e-01)	Acc@1  82.81 ( 82.12)	Acc@5  99.22 ( 99.12)
Epoch: [9][320/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.2734e-01 (5.1429e-01)	Acc@1  80.47 ( 82.13)	Acc@5 100.00 ( 99.14)
Epoch: [9][330/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.4248e-01 (5.1400e-01)	Acc@1  80.47 ( 82.15)	Acc@5  99.22 ( 99.13)
Epoch: [9][340/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.0645e-01 (5.1364e-01)	Acc@1  75.78 ( 82.14)	Acc@5  98.44 ( 99.13)
Epoch: [9][350/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.6748e-01 (5.1279e-01)	Acc@1  73.44 ( 82.16)	Acc@5 100.00 ( 99.14)
Epoch: [9][360/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.8740e-01 (5.1344e-01)	Acc@1  78.12 ( 82.13)	Acc@5  97.66 ( 99.13)
Epoch: [9][370/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.8936e-01 (5.1518e-01)	Acc@1  77.34 ( 82.09)	Acc@5  98.44 ( 99.12)
Epoch: [9][380/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.6016e-01 (5.1635e-01)	Acc@1  79.69 ( 82.07)	Acc@5  96.88 ( 99.11)
Epoch: [9][390/391]	Time  0.029 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.6641e-01 (5.1581e-01)	Acc@1  82.50 ( 82.10)	Acc@5  97.50 ( 99.11)
## e[9] optimizer.zero_grad (sum) time: 0.25314927101135254
## e[9]       loss.backward (sum) time: 4.198664903640747
## e[9]      optimizer.step (sum) time: 1.7592670917510986
## epoch[9] training(only) time: 15.946017026901245
# Switched to evaluate mode...
Test: [  0/100]	Time  0.180 ( 0.180)	Loss 6.0498e-01 (6.0498e-01)	Acc@1  77.00 ( 77.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.017 ( 0.034)	Loss 5.7959e-01 (5.6135e-01)	Acc@1  84.00 ( 81.00)	Acc@5  98.00 ( 99.18)
Test: [ 20/100]	Time  0.022 ( 0.028)	Loss 7.6855e-01 (5.7217e-01)	Acc@1  71.00 ( 80.00)	Acc@5 100.00 ( 98.90)
Test: [ 30/100]	Time  0.019 ( 0.025)	Loss 6.1426e-01 (5.7362e-01)	Acc@1  81.00 ( 80.13)	Acc@5  96.00 ( 98.84)
Test: [ 40/100]	Time  0.019 ( 0.023)	Loss 4.7778e-01 (5.6920e-01)	Acc@1  82.00 ( 80.27)	Acc@5 100.00 ( 98.90)
Test: [ 50/100]	Time  0.017 ( 0.022)	Loss 4.7412e-01 (5.6518e-01)	Acc@1  84.00 ( 80.53)	Acc@5  97.00 ( 98.84)
Test: [ 60/100]	Time  0.022 ( 0.022)	Loss 5.3369e-01 (5.6432e-01)	Acc@1  80.00 ( 80.51)	Acc@5 100.00 ( 98.95)
Test: [ 70/100]	Time  0.023 ( 0.022)	Loss 5.3369e-01 (5.6289e-01)	Acc@1  82.00 ( 80.54)	Acc@5  99.00 ( 98.99)
Test: [ 80/100]	Time  0.022 ( 0.022)	Loss 4.2993e-01 (5.6106e-01)	Acc@1  83.00 ( 80.44)	Acc@5 100.00 ( 99.04)
Test: [ 90/100]	Time  0.026 ( 0.021)	Loss 4.1284e-01 (5.6152e-01)	Acc@1  86.00 ( 80.38)	Acc@5 100.00 ( 99.01)
 * Acc@1 80.370 Acc@5 99.060
### epoch[9] execution time: 18.19771146774292
EPOCH 10
# current learning rate: 0.1
# Switched to train mode...
Epoch: [10][  0/391]	Time  0.212 ( 0.212)	Data  0.170 ( 0.170)	Loss 5.0879e-01 (5.0879e-01)	Acc@1  83.59 ( 83.59)	Acc@5  98.44 ( 98.44)
Epoch: [10][ 10/391]	Time  0.041 ( 0.056)	Data  0.001 ( 0.016)	Loss 4.9976e-01 (5.0100e-01)	Acc@1  84.38 ( 82.95)	Acc@5  97.66 ( 99.01)
Epoch: [10][ 20/391]	Time  0.039 ( 0.049)	Data  0.001 ( 0.009)	Loss 4.4629e-01 (4.8489e-01)	Acc@1  85.94 ( 83.59)	Acc@5  98.44 ( 99.14)
Epoch: [10][ 30/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.007)	Loss 3.7891e-01 (4.7783e-01)	Acc@1  86.72 ( 84.00)	Acc@5  99.22 ( 99.17)
Epoch: [10][ 40/391]	Time  0.040 ( 0.045)	Data  0.001 ( 0.005)	Loss 5.7031e-01 (4.8193e-01)	Acc@1  76.56 ( 83.65)	Acc@5  98.44 ( 99.18)
Epoch: [10][ 50/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.004)	Loss 3.7427e-01 (4.6830e-01)	Acc@1  89.84 ( 84.16)	Acc@5  99.22 ( 99.22)
Epoch: [10][ 60/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.004)	Loss 4.3091e-01 (4.7654e-01)	Acc@1  86.72 ( 83.86)	Acc@5  98.44 ( 99.21)
Epoch: [10][ 70/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.1821e-01 (4.7426e-01)	Acc@1  85.94 ( 83.75)	Acc@5  99.22 ( 99.23)
Epoch: [10][ 80/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.7085e-01 (4.7383e-01)	Acc@1  89.06 ( 83.76)	Acc@5 100.00 ( 99.24)
Epoch: [10][ 90/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.0303e-01 (4.8824e-01)	Acc@1  78.91 ( 83.27)	Acc@5 100.00 ( 99.21)
Epoch: [10][100/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.0498e-01 (4.8770e-01)	Acc@1  78.91 ( 83.24)	Acc@5  97.66 ( 99.20)
Epoch: [10][110/391]	Time  0.049 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.4639e-01 (4.8732e-01)	Acc@1  85.94 ( 83.28)	Acc@5  98.44 ( 99.18)
Epoch: [10][120/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.5615e-01 (4.8492e-01)	Acc@1  82.03 ( 83.32)	Acc@5  99.22 ( 99.22)
Epoch: [10][130/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.9531e-01 (4.8953e-01)	Acc@1  75.78 ( 83.12)	Acc@5  98.44 ( 99.15)
Epoch: [10][140/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.4150e-01 (4.9097e-01)	Acc@1  80.47 ( 83.09)	Acc@5  98.44 ( 99.11)
Epoch: [10][150/391]	Time  0.042 ( 0.041)	Data  0.002 ( 0.002)	Loss 4.6973e-01 (4.9465e-01)	Acc@1  82.81 ( 82.92)	Acc@5  99.22 ( 99.10)
Epoch: [10][160/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.9961e-01 (4.9746e-01)	Acc@1  82.81 ( 82.83)	Acc@5  99.22 ( 99.10)
Epoch: [10][170/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2407e-01 (4.9836e-01)	Acc@1  86.72 ( 82.71)	Acc@5  98.44 ( 99.10)
Epoch: [10][180/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8574e-01 (4.9755e-01)	Acc@1  86.72 ( 82.74)	Acc@5 100.00 ( 99.11)
Epoch: [10][190/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3335e-01 (4.9661e-01)	Acc@1  82.81 ( 82.76)	Acc@5  99.22 ( 99.12)
Epoch: [10][200/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.9375e-01 (4.9853e-01)	Acc@1  83.59 ( 82.67)	Acc@5  98.44 ( 99.12)
Epoch: [10][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.6411e-01 (4.9965e-01)	Acc@1  85.16 ( 82.66)	Acc@5  99.22 ( 99.13)
Epoch: [10][220/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9746e-01 (4.9864e-01)	Acc@1  86.72 ( 82.76)	Acc@5 100.00 ( 99.12)
Epoch: [10][230/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 4.7681e-01 (4.9997e-01)	Acc@1  80.47 ( 82.76)	Acc@5  99.22 ( 99.11)
Epoch: [10][240/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 5.0146e-01 (4.9826e-01)	Acc@1  81.25 ( 82.82)	Acc@5  99.22 ( 99.13)
Epoch: [10][250/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.002)	Loss 5.0391e-01 (4.9841e-01)	Acc@1  83.59 ( 82.85)	Acc@5  99.22 ( 99.13)
Epoch: [10][260/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 5.3564e-01 (4.9760e-01)	Acc@1  82.03 ( 82.87)	Acc@5  99.22 ( 99.12)
Epoch: [10][270/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 4.3945e-01 (4.9719e-01)	Acc@1  81.25 ( 82.83)	Acc@5 100.00 ( 99.13)
Epoch: [10][280/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 4.6899e-01 (4.9788e-01)	Acc@1  81.25 ( 82.77)	Acc@5  99.22 ( 99.13)
Epoch: [10][290/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.002)	Loss 5.5811e-01 (4.9847e-01)	Acc@1  78.91 ( 82.72)	Acc@5  98.44 ( 99.12)
Epoch: [10][300/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 5.7910e-01 (4.9799e-01)	Acc@1  78.91 ( 82.75)	Acc@5  99.22 ( 99.11)
Epoch: [10][310/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 5.4980e-01 (4.9705e-01)	Acc@1  78.91 ( 82.76)	Acc@5  99.22 ( 99.13)
Epoch: [10][320/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 4.9341e-01 (4.9543e-01)	Acc@1  82.03 ( 82.84)	Acc@5  99.22 ( 99.13)
Epoch: [10][330/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.7373e-01 (4.9612e-01)	Acc@1  81.25 ( 82.82)	Acc@5  99.22 ( 99.15)
Epoch: [10][340/391]	Time  0.054 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.9863e-01 (4.9648e-01)	Acc@1  78.12 ( 82.82)	Acc@5  97.66 ( 99.14)
Epoch: [10][350/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.9854e-01 (4.9852e-01)	Acc@1  84.38 ( 82.76)	Acc@5  99.22 ( 99.13)
Epoch: [10][360/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.9038e-01 (4.9794e-01)	Acc@1  88.28 ( 82.77)	Acc@5  99.22 ( 99.13)
Epoch: [10][370/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.9331e-01 (4.9676e-01)	Acc@1  86.72 ( 82.80)	Acc@5 100.00 ( 99.13)
Epoch: [10][380/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.0830e-01 (4.9636e-01)	Acc@1  81.25 ( 82.82)	Acc@5  99.22 ( 99.13)
Epoch: [10][390/391]	Time  0.029 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.7451e-01 (4.9495e-01)	Acc@1  87.50 ( 82.85)	Acc@5 100.00 ( 99.13)
## e[10] optimizer.zero_grad (sum) time: 0.2510392665863037
## e[10]       loss.backward (sum) time: 4.152078628540039
## e[10]      optimizer.step (sum) time: 1.7718055248260498
## epoch[10] training(only) time: 15.821666717529297
# Switched to evaluate mode...
Test: [  0/100]	Time  0.164 ( 0.164)	Loss 6.2939e-01 (6.2939e-01)	Acc@1  78.00 ( 78.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.025 ( 0.033)	Loss 6.0645e-01 (6.8570e-01)	Acc@1  79.00 ( 77.64)	Acc@5  97.00 ( 99.18)
Test: [ 20/100]	Time  0.020 ( 0.026)	Loss 7.8418e-01 (7.0235e-01)	Acc@1  74.00 ( 77.43)	Acc@5 100.00 ( 98.86)
Test: [ 30/100]	Time  0.022 ( 0.024)	Loss 8.5986e-01 (7.0165e-01)	Acc@1  78.00 ( 77.77)	Acc@5  98.00 ( 98.58)
Test: [ 40/100]	Time  0.020 ( 0.023)	Loss 6.0156e-01 (6.9811e-01)	Acc@1  80.00 ( 77.90)	Acc@5  99.00 ( 98.51)
Test: [ 50/100]	Time  0.016 ( 0.023)	Loss 6.8750e-01 (6.9801e-01)	Acc@1  78.00 ( 77.82)	Acc@5  98.00 ( 98.49)
Test: [ 60/100]	Time  0.021 ( 0.022)	Loss 7.6611e-01 (6.9707e-01)	Acc@1  75.00 ( 77.67)	Acc@5 100.00 ( 98.57)
Test: [ 70/100]	Time  0.017 ( 0.021)	Loss 6.2158e-01 (6.9545e-01)	Acc@1  78.00 ( 77.59)	Acc@5  99.00 ( 98.65)
Test: [ 80/100]	Time  0.023 ( 0.021)	Loss 6.3330e-01 (6.9355e-01)	Acc@1  81.00 ( 77.49)	Acc@5  99.00 ( 98.69)
Test: [ 90/100]	Time  0.017 ( 0.021)	Loss 7.2021e-01 (6.9694e-01)	Acc@1  73.00 ( 77.32)	Acc@5  99.00 ( 98.66)
 * Acc@1 77.300 Acc@5 98.710
### epoch[10] execution time: 18.048556566238403
EPOCH 11
# current learning rate: 0.1
# Switched to train mode...
Epoch: [11][  0/391]	Time  0.223 ( 0.223)	Data  0.183 ( 0.183)	Loss 4.1406e-01 (4.1406e-01)	Acc@1  89.06 ( 89.06)	Acc@5 100.00 (100.00)
Epoch: [11][ 10/391]	Time  0.038 ( 0.056)	Data  0.001 ( 0.017)	Loss 5.7568e-01 (4.8890e-01)	Acc@1  78.91 ( 84.02)	Acc@5 100.00 ( 99.50)
Epoch: [11][ 20/391]	Time  0.039 ( 0.048)	Data  0.001 ( 0.010)	Loss 4.1357e-01 (4.7468e-01)	Acc@1  86.72 ( 83.93)	Acc@5  99.22 ( 99.55)
Epoch: [11][ 30/391]	Time  0.038 ( 0.046)	Data  0.001 ( 0.007)	Loss 4.1431e-01 (4.7690e-01)	Acc@1  84.38 ( 83.39)	Acc@5  99.22 ( 99.57)
Epoch: [11][ 40/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.005)	Loss 4.7681e-01 (4.7506e-01)	Acc@1  82.81 ( 83.35)	Acc@5 100.00 ( 99.58)
Epoch: [11][ 50/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.004)	Loss 5.1416e-01 (4.7976e-01)	Acc@1  81.25 ( 83.24)	Acc@5  99.22 ( 99.49)
Epoch: [11][ 60/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.004)	Loss 4.7900e-01 (4.7745e-01)	Acc@1  84.38 ( 83.52)	Acc@5 100.00 ( 99.54)
Epoch: [11][ 70/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.3271e-01 (4.7102e-01)	Acc@1  85.16 ( 83.84)	Acc@5  97.66 ( 99.50)
Epoch: [11][ 80/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.9619e-01 (4.6766e-01)	Acc@1  79.69 ( 83.90)	Acc@5  99.22 ( 99.54)
Epoch: [11][ 90/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.0928e-01 (4.7134e-01)	Acc@1  78.91 ( 83.76)	Acc@5 100.00 ( 99.50)
Epoch: [11][100/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.4116e-01 (4.7336e-01)	Acc@1  82.81 ( 83.77)	Acc@5 100.00 ( 99.47)
Epoch: [11][110/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.9014e-01 (4.6805e-01)	Acc@1  89.84 ( 83.98)	Acc@5  99.22 ( 99.47)
Epoch: [11][120/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3628e-01 (4.7095e-01)	Acc@1  84.38 ( 83.91)	Acc@5  98.44 ( 99.44)
Epoch: [11][130/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2881e-01 (4.7158e-01)	Acc@1  80.47 ( 83.84)	Acc@5  99.22 ( 99.43)
Epoch: [11][140/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0049e-01 (4.7222e-01)	Acc@1  82.81 ( 83.84)	Acc@5 100.00 ( 99.42)
Epoch: [11][150/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1699e-01 (4.7327e-01)	Acc@1  86.72 ( 83.79)	Acc@5  99.22 ( 99.38)
Epoch: [11][160/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0488e-01 (4.7188e-01)	Acc@1  82.03 ( 83.80)	Acc@5  99.22 ( 99.39)
Epoch: [11][170/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1577e-01 (4.6884e-01)	Acc@1  84.38 ( 83.90)	Acc@5 100.00 ( 99.39)
Epoch: [11][180/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.5483e-01 (4.6818e-01)	Acc@1  85.16 ( 83.96)	Acc@5  98.44 ( 99.37)
Epoch: [11][190/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.6104e-01 (4.7038e-01)	Acc@1  78.91 ( 83.84)	Acc@5  99.22 ( 99.36)
Epoch: [11][200/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0635e-01 (4.7238e-01)	Acc@1  82.81 ( 83.78)	Acc@5 100.00 ( 99.36)
Epoch: [11][210/391]	Time  0.045 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3398e-01 (4.7103e-01)	Acc@1  89.06 ( 83.83)	Acc@5 100.00 ( 99.38)
Epoch: [11][220/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.6753e-01 (4.6976e-01)	Acc@1  82.81 ( 83.84)	Acc@5  97.66 ( 99.38)
Epoch: [11][230/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.6216e-01 (4.6787e-01)	Acc@1  82.03 ( 83.89)	Acc@5  99.22 ( 99.37)
Epoch: [11][240/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.8350e-01 (4.7021e-01)	Acc@1  81.25 ( 83.82)	Acc@5  99.22 ( 99.35)
Epoch: [11][250/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2275e-01 (4.6931e-01)	Acc@1  87.50 ( 83.85)	Acc@5 100.00 ( 99.36)
Epoch: [11][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3203e-01 (4.6816e-01)	Acc@1  90.62 ( 83.88)	Acc@5 100.00 ( 99.35)
Epoch: [11][270/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2588e-01 (4.6683e-01)	Acc@1  80.47 ( 83.95)	Acc@5  99.22 ( 99.36)
Epoch: [11][280/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0098e-01 (4.6631e-01)	Acc@1  81.25 ( 84.00)	Acc@5  99.22 ( 99.36)
Epoch: [11][290/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.6914e-01 (4.6625e-01)	Acc@1  87.50 ( 83.97)	Acc@5 100.00 ( 99.36)
Epoch: [11][300/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.8696e-01 (4.6770e-01)	Acc@1  87.50 ( 83.90)	Acc@5  98.44 ( 99.35)
Epoch: [11][310/391]	Time  0.045 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.7217e-01 (4.6677e-01)	Acc@1  82.03 ( 83.94)	Acc@5 100.00 ( 99.35)
Epoch: [11][320/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.4531e-01 (4.6855e-01)	Acc@1  85.16 ( 83.90)	Acc@5  99.22 ( 99.34)
Epoch: [11][330/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.7915e-01 (4.6858e-01)	Acc@1  85.16 ( 83.88)	Acc@5  99.22 ( 99.34)
Epoch: [11][340/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.2783e-01 (4.6941e-01)	Acc@1  82.03 ( 83.85)	Acc@5  97.66 ( 99.33)
Epoch: [11][350/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.4336e-01 (4.6938e-01)	Acc@1  84.38 ( 83.85)	Acc@5 100.00 ( 99.34)
Epoch: [11][360/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.6113e-01 (4.7082e-01)	Acc@1  79.69 ( 83.80)	Acc@5  99.22 ( 99.34)
Epoch: [11][370/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.2212e-01 (4.7169e-01)	Acc@1  82.81 ( 83.74)	Acc@5 100.00 ( 99.34)
Epoch: [11][380/391]	Time  0.043 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.8486e-01 (4.7176e-01)	Acc@1  82.81 ( 83.73)	Acc@5 100.00 ( 99.35)
Epoch: [11][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.5537e-01 (4.7190e-01)	Acc@1  77.50 ( 83.76)	Acc@5  97.50 ( 99.34)
## e[11] optimizer.zero_grad (sum) time: 0.24967455863952637
## e[11]       loss.backward (sum) time: 4.1759045124053955
## e[11]      optimizer.step (sum) time: 1.7673578262329102
## epoch[11] training(only) time: 15.943673372268677
# Switched to evaluate mode...
Test: [  0/100]	Time  0.168 ( 0.168)	Loss 6.3574e-01 (6.3574e-01)	Acc@1  77.00 ( 77.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.017 ( 0.033)	Loss 6.2549e-01 (6.0913e-01)	Acc@1  81.00 ( 78.82)	Acc@5  99.00 ( 99.00)
Test: [ 20/100]	Time  0.019 ( 0.026)	Loss 5.6885e-01 (6.1040e-01)	Acc@1  80.00 ( 79.52)	Acc@5 100.00 ( 99.00)
Test: [ 30/100]	Time  0.020 ( 0.024)	Loss 6.0791e-01 (6.0421e-01)	Acc@1  78.00 ( 79.52)	Acc@5  98.00 ( 99.00)
Test: [ 40/100]	Time  0.023 ( 0.023)	Loss 5.6982e-01 (6.0593e-01)	Acc@1  83.00 ( 79.46)	Acc@5  99.00 ( 98.98)
Test: [ 50/100]	Time  0.021 ( 0.022)	Loss 4.9121e-01 (6.0342e-01)	Acc@1  81.00 ( 79.57)	Acc@5  99.00 ( 98.92)
Test: [ 60/100]	Time  0.019 ( 0.022)	Loss 4.7852e-01 (6.0207e-01)	Acc@1  84.00 ( 79.69)	Acc@5  99.00 ( 98.93)
Test: [ 70/100]	Time  0.019 ( 0.022)	Loss 7.2510e-01 (5.9571e-01)	Acc@1  80.00 ( 79.89)	Acc@5  98.00 ( 98.97)
Test: [ 80/100]	Time  0.021 ( 0.021)	Loss 4.6509e-01 (5.9441e-01)	Acc@1  82.00 ( 79.85)	Acc@5 100.00 ( 99.04)
Test: [ 90/100]	Time  0.018 ( 0.021)	Loss 4.6411e-01 (5.9508e-01)	Acc@1  86.00 ( 79.84)	Acc@5 100.00 ( 99.07)
 * Acc@1 79.870 Acc@5 99.070
### epoch[11] execution time: 18.124716997146606
EPOCH 12
# current learning rate: 0.1
# Switched to train mode...
Epoch: [12][  0/391]	Time  0.225 ( 0.225)	Data  0.184 ( 0.184)	Loss 5.0879e-01 (5.0879e-01)	Acc@1  80.47 ( 80.47)	Acc@5  98.44 ( 98.44)
Epoch: [12][ 10/391]	Time  0.041 ( 0.057)	Data  0.001 ( 0.018)	Loss 4.9414e-01 (4.5763e-01)	Acc@1  83.59 ( 84.52)	Acc@5  98.44 ( 98.86)
Epoch: [12][ 20/391]	Time  0.038 ( 0.050)	Data  0.001 ( 0.010)	Loss 4.9805e-01 (4.4460e-01)	Acc@1  83.59 ( 85.16)	Acc@5  99.22 ( 99.11)
Epoch: [12][ 30/391]	Time  0.042 ( 0.046)	Data  0.001 ( 0.007)	Loss 4.3213e-01 (4.5220e-01)	Acc@1  84.38 ( 84.48)	Acc@5  99.22 ( 99.14)
Epoch: [12][ 40/391]	Time  0.038 ( 0.045)	Data  0.001 ( 0.005)	Loss 4.8682e-01 (4.4613e-01)	Acc@1  81.25 ( 84.78)	Acc@5 100.00 ( 99.18)
Epoch: [12][ 50/391]	Time  0.037 ( 0.044)	Data  0.001 ( 0.005)	Loss 4.1724e-01 (4.4055e-01)	Acc@1  84.38 ( 84.97)	Acc@5  99.22 ( 99.16)
Epoch: [12][ 60/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 4.9707e-01 (4.3593e-01)	Acc@1  78.91 ( 85.04)	Acc@5 100.00 ( 99.21)
Epoch: [12][ 70/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.004)	Loss 3.7524e-01 (4.3571e-01)	Acc@1  91.41 ( 85.11)	Acc@5  98.44 ( 99.17)
Epoch: [12][ 80/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.7754e-01 (4.3360e-01)	Acc@1  82.81 ( 85.13)	Acc@5 100.00 ( 99.24)
Epoch: [12][ 90/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.8218e-01 (4.3267e-01)	Acc@1  83.59 ( 85.10)	Acc@5 100.00 ( 99.27)
Epoch: [12][100/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.0405e-01 (4.3743e-01)	Acc@1  85.94 ( 85.02)	Acc@5 100.00 ( 99.27)
Epoch: [12][110/391]	Time  0.042 ( 0.041)	Data  0.003 ( 0.003)	Loss 3.5913e-01 (4.4054e-01)	Acc@1  85.16 ( 84.80)	Acc@5 100.00 ( 99.28)
Epoch: [12][120/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.003)	Loss 4.0015e-01 (4.4222e-01)	Acc@1  83.59 ( 84.76)	Acc@5  99.22 ( 99.24)
Epoch: [12][130/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5449e-01 (4.4266e-01)	Acc@1  86.72 ( 84.70)	Acc@5  99.22 ( 99.24)
Epoch: [12][140/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.3418e-01 (4.4706e-01)	Acc@1  78.91 ( 84.52)	Acc@5  97.66 ( 99.22)
Epoch: [12][150/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9707e-01 (4.4888e-01)	Acc@1  83.59 ( 84.51)	Acc@5  97.66 ( 99.22)
Epoch: [12][160/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9341e-01 (4.5244e-01)	Acc@1  82.03 ( 84.38)	Acc@5 100.00 ( 99.24)
Epoch: [12][170/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7192e-01 (4.5223e-01)	Acc@1  79.69 ( 84.36)	Acc@5 100.00 ( 99.26)
Epoch: [12][180/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.8936e-01 (4.5187e-01)	Acc@1  77.34 ( 84.35)	Acc@5  99.22 ( 99.27)
Epoch: [12][190/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.7861e-01 (4.5315e-01)	Acc@1  80.47 ( 84.30)	Acc@5  97.66 ( 99.26)
Epoch: [12][200/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1611e-01 (4.5355e-01)	Acc@1  85.16 ( 84.31)	Acc@5 100.00 ( 99.27)
Epoch: [12][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7534e-01 (4.5434e-01)	Acc@1  84.38 ( 84.26)	Acc@5  99.22 ( 99.28)
Epoch: [12][220/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2432e-01 (4.5301e-01)	Acc@1  82.03 ( 84.30)	Acc@5  99.22 ( 99.29)
Epoch: [12][230/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 3.5791e-01 (4.5268e-01)	Acc@1  88.28 ( 84.32)	Acc@5 100.00 ( 99.29)
Epoch: [12][240/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 4.5093e-01 (4.5359e-01)	Acc@1  82.81 ( 84.31)	Acc@5  98.44 ( 99.30)
Epoch: [12][250/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.002)	Loss 4.5825e-01 (4.5239e-01)	Acc@1  82.81 ( 84.33)	Acc@5  99.22 ( 99.30)
Epoch: [12][260/391]	Time  0.043 ( 0.040)	Data  0.001 ( 0.002)	Loss 4.1553e-01 (4.5114e-01)	Acc@1  86.72 ( 84.36)	Acc@5  99.22 ( 99.31)
Epoch: [12][270/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.002)	Loss 5.2783e-01 (4.5131e-01)	Acc@1  81.25 ( 84.38)	Acc@5  98.44 ( 99.31)
Epoch: [12][280/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.002)	Loss 5.1904e-01 (4.4902e-01)	Acc@1  84.38 ( 84.47)	Acc@5 100.00 ( 99.32)
Epoch: [12][290/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.002)	Loss 4.3042e-01 (4.4857e-01)	Acc@1  85.94 ( 84.50)	Acc@5  98.44 ( 99.32)
Epoch: [12][300/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.002)	Loss 5.1855e-01 (4.4986e-01)	Acc@1  80.47 ( 84.47)	Acc@5 100.00 ( 99.32)
Epoch: [12][310/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 3.0908e-01 (4.4912e-01)	Acc@1  89.84 ( 84.50)	Acc@5 100.00 ( 99.33)
Epoch: [12][320/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 4.7852e-01 (4.5113e-01)	Acc@1  83.59 ( 84.43)	Acc@5  98.44 ( 99.32)
Epoch: [12][330/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 3.5596e-01 (4.5047e-01)	Acc@1  88.28 ( 84.46)	Acc@5  99.22 ( 99.33)
Epoch: [12][340/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.3677e-01 (4.5007e-01)	Acc@1  82.81 ( 84.45)	Acc@5  98.44 ( 99.33)
Epoch: [12][350/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.5303e-01 (4.5019e-01)	Acc@1  88.28 ( 84.46)	Acc@5 100.00 ( 99.33)
Epoch: [12][360/391]	Time  0.044 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.1772e-01 (4.4972e-01)	Acc@1  84.38 ( 84.49)	Acc@5 100.00 ( 99.33)
Epoch: [12][370/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.6094e-01 (4.4920e-01)	Acc@1  82.81 ( 84.50)	Acc@5 100.00 ( 99.33)
Epoch: [12][380/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.5181e-01 (4.4842e-01)	Acc@1  88.28 ( 84.52)	Acc@5 100.00 ( 99.33)
Epoch: [12][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.6543e-01 (4.4987e-01)	Acc@1  77.50 ( 84.47)	Acc@5  98.75 ( 99.33)
## e[12] optimizer.zero_grad (sum) time: 0.2509455680847168
## e[12]       loss.backward (sum) time: 4.064301013946533
## e[12]      optimizer.step (sum) time: 1.8018405437469482
## epoch[12] training(only) time: 15.699250936508179
# Switched to evaluate mode...
Test: [  0/100]	Time  0.176 ( 0.176)	Loss 4.8901e-01 (4.8901e-01)	Acc@1  81.00 ( 81.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.020 ( 0.034)	Loss 4.9243e-01 (5.2368e-01)	Acc@1  81.00 ( 81.82)	Acc@5 100.00 ( 99.36)
Test: [ 20/100]	Time  0.024 ( 0.028)	Loss 5.8789e-01 (5.3163e-01)	Acc@1  75.00 ( 81.81)	Acc@5 100.00 ( 99.33)
Test: [ 30/100]	Time  0.023 ( 0.026)	Loss 6.1182e-01 (5.4286e-01)	Acc@1  80.00 ( 81.90)	Acc@5  98.00 ( 99.19)
Test: [ 40/100]	Time  0.017 ( 0.024)	Loss 5.2051e-01 (5.4490e-01)	Acc@1  83.00 ( 81.78)	Acc@5  99.00 ( 99.20)
Test: [ 50/100]	Time  0.021 ( 0.023)	Loss 6.8066e-01 (5.4265e-01)	Acc@1  79.00 ( 82.08)	Acc@5  98.00 ( 99.18)
Test: [ 60/100]	Time  0.018 ( 0.022)	Loss 4.8901e-01 (5.4601e-01)	Acc@1  82.00 ( 81.79)	Acc@5 100.00 ( 99.20)
Test: [ 70/100]	Time  0.019 ( 0.022)	Loss 4.8975e-01 (5.4260e-01)	Acc@1  83.00 ( 81.93)	Acc@5  99.00 ( 99.20)
Test: [ 80/100]	Time  0.021 ( 0.022)	Loss 4.0820e-01 (5.3682e-01)	Acc@1  84.00 ( 82.09)	Acc@5  99.00 ( 99.23)
Test: [ 90/100]	Time  0.022 ( 0.022)	Loss 5.3467e-01 (5.3930e-01)	Acc@1  81.00 ( 81.93)	Acc@5 100.00 ( 99.25)
 * Acc@1 81.880 Acc@5 99.290
### epoch[12] execution time: 17.983299493789673
EPOCH 13
# current learning rate: 0.1
# Switched to train mode...
Epoch: [13][  0/391]	Time  0.218 ( 0.218)	Data  0.177 ( 0.177)	Loss 4.3530e-01 (4.3530e-01)	Acc@1  82.81 ( 82.81)	Acc@5  99.22 ( 99.22)
Epoch: [13][ 10/391]	Time  0.041 ( 0.056)	Data  0.001 ( 0.017)	Loss 5.8984e-01 (4.9452e-01)	Acc@1  81.25 ( 82.95)	Acc@5  98.44 ( 99.29)
Epoch: [13][ 20/391]	Time  0.041 ( 0.049)	Data  0.001 ( 0.009)	Loss 4.2090e-01 (4.7511e-01)	Acc@1  85.16 ( 83.97)	Acc@5 100.00 ( 99.03)
Epoch: [13][ 30/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.007)	Loss 4.5483e-01 (4.5718e-01)	Acc@1  85.16 ( 84.83)	Acc@5  98.44 ( 99.04)
Epoch: [13][ 40/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.005)	Loss 3.1177e-01 (4.4857e-01)	Acc@1  91.41 ( 85.06)	Acc@5  99.22 ( 99.16)
Epoch: [13][ 50/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.004)	Loss 5.4492e-01 (4.5106e-01)	Acc@1  84.38 ( 85.00)	Acc@5  99.22 ( 99.17)
Epoch: [13][ 60/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 4.9390e-01 (4.4343e-01)	Acc@1  76.56 ( 85.00)	Acc@5 100.00 ( 99.18)
Epoch: [13][ 70/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.4790e-01 (4.3862e-01)	Acc@1  87.50 ( 85.13)	Acc@5 100.00 ( 99.26)
Epoch: [13][ 80/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.6914e-01 (4.3528e-01)	Acc@1  86.72 ( 85.16)	Acc@5 100.00 ( 99.30)
Epoch: [13][ 90/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.4448e-01 (4.3613e-01)	Acc@1  89.84 ( 85.06)	Acc@5 100.00 ( 99.32)
Epoch: [13][100/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.2017e-01 (4.3742e-01)	Acc@1  83.59 ( 84.95)	Acc@5 100.00 ( 99.35)
Epoch: [13][110/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.0796e-01 (4.3843e-01)	Acc@1  85.16 ( 84.82)	Acc@5  99.22 ( 99.35)
Epoch: [13][120/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.5630e-01 (4.3695e-01)	Acc@1  84.38 ( 84.91)	Acc@5 100.00 ( 99.35)
Epoch: [13][130/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7134e-01 (4.3704e-01)	Acc@1  87.50 ( 84.95)	Acc@5 100.00 ( 99.36)
Epoch: [13][140/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.7080e-01 (4.3702e-01)	Acc@1  81.25 ( 85.01)	Acc@5  98.44 ( 99.37)
Epoch: [13][150/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8599e-01 (4.3746e-01)	Acc@1  85.16 ( 84.96)	Acc@5 100.00 ( 99.39)
Epoch: [13][160/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9365e-01 (4.3681e-01)	Acc@1  82.03 ( 85.01)	Acc@5 100.00 ( 99.41)
Epoch: [13][170/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5547e-01 (4.3597e-01)	Acc@1  87.50 ( 84.99)	Acc@5 100.00 ( 99.41)
Epoch: [13][180/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7622e-01 (4.3559e-01)	Acc@1  85.94 ( 85.04)	Acc@5 100.00 ( 99.40)
Epoch: [13][190/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1724e-01 (4.3553e-01)	Acc@1  84.38 ( 85.06)	Acc@5 100.00 ( 99.42)
Epoch: [13][200/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2397e-01 (4.3540e-01)	Acc@1  89.84 ( 85.04)	Acc@5 100.00 ( 99.43)
Epoch: [13][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9014e-01 (4.3479e-01)	Acc@1  88.28 ( 85.07)	Acc@5  99.22 ( 99.44)
Epoch: [13][220/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0146e-01 (4.3376e-01)	Acc@1  82.03 ( 85.05)	Acc@5  99.22 ( 99.46)
Epoch: [13][230/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0342e-01 (4.3422e-01)	Acc@1  84.38 ( 85.02)	Acc@5  99.22 ( 99.46)
Epoch: [13][240/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5205e-01 (4.3353e-01)	Acc@1  87.50 ( 85.06)	Acc@5 100.00 ( 99.47)
Epoch: [13][250/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.2510e-01 (4.3316e-01)	Acc@1  95.31 ( 85.13)	Acc@5 100.00 ( 99.46)
Epoch: [13][260/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8550e-01 (4.3446e-01)	Acc@1  85.94 ( 85.07)	Acc@5  99.22 ( 99.46)
Epoch: [13][270/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8867e-01 (4.3572e-01)	Acc@1  85.16 ( 85.01)	Acc@5  98.44 ( 99.46)
Epoch: [13][280/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6938e-01 (4.3521e-01)	Acc@1  85.16 ( 84.99)	Acc@5 100.00 ( 99.46)
Epoch: [13][290/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.7959e-01 (4.3578e-01)	Acc@1  78.12 ( 84.99)	Acc@5  99.22 ( 99.46)
Epoch: [13][300/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.8955e-01 (4.3545e-01)	Acc@1  89.06 ( 84.98)	Acc@5 100.00 ( 99.45)
Epoch: [13][310/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.4360e-01 (4.3680e-01)	Acc@1  85.94 ( 84.95)	Acc@5  98.44 ( 99.44)
Epoch: [13][320/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.0176e-01 (4.3677e-01)	Acc@1  87.50 ( 84.92)	Acc@5 100.00 ( 99.43)
Epoch: [13][330/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.0293e-01 (4.3741e-01)	Acc@1  82.81 ( 84.88)	Acc@5 100.00 ( 99.44)
Epoch: [13][340/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.1763e-01 (4.3775e-01)	Acc@1  90.62 ( 84.90)	Acc@5  99.22 ( 99.43)
Epoch: [13][350/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.2163e-01 (4.3706e-01)	Acc@1  85.94 ( 84.92)	Acc@5 100.00 ( 99.43)
Epoch: [13][360/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.3701e-01 (4.3763e-01)	Acc@1  83.59 ( 84.89)	Acc@5 100.00 ( 99.42)
Epoch: [13][370/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.1968e-01 (4.3680e-01)	Acc@1  82.81 ( 84.92)	Acc@5  99.22 ( 99.42)
Epoch: [13][380/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.6450e-01 (4.3638e-01)	Acc@1  85.16 ( 84.93)	Acc@5  99.22 ( 99.42)
Epoch: [13][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.7632e-01 (4.3599e-01)	Acc@1  82.50 ( 84.95)	Acc@5 100.00 ( 99.42)
## e[13] optimizer.zero_grad (sum) time: 0.2501180171966553
## e[13]       loss.backward (sum) time: 4.191583633422852
## e[13]      optimizer.step (sum) time: 1.751807451248169
## epoch[13] training(only) time: 15.95549488067627
# Switched to evaluate mode...
Test: [  0/100]	Time  0.166 ( 0.166)	Loss 5.5664e-01 (5.5664e-01)	Acc@1  80.00 ( 80.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.020 ( 0.034)	Loss 5.4980e-01 (5.6137e-01)	Acc@1  78.00 ( 80.64)	Acc@5  99.00 ( 99.27)
Test: [ 20/100]	Time  0.017 ( 0.027)	Loss 7.2461e-01 (5.6346e-01)	Acc@1  77.00 ( 81.62)	Acc@5  99.00 ( 99.00)
Test: [ 30/100]	Time  0.017 ( 0.025)	Loss 5.5957e-01 (5.5089e-01)	Acc@1  83.00 ( 82.39)	Acc@5  98.00 ( 99.13)
Test: [ 40/100]	Time  0.020 ( 0.023)	Loss 5.1904e-01 (5.4827e-01)	Acc@1  80.00 ( 82.27)	Acc@5 100.00 ( 99.17)
Test: [ 50/100]	Time  0.023 ( 0.023)	Loss 5.0977e-01 (5.4472e-01)	Acc@1  82.00 ( 82.29)	Acc@5  98.00 ( 99.12)
Test: [ 60/100]	Time  0.020 ( 0.023)	Loss 5.4492e-01 (5.4765e-01)	Acc@1  81.00 ( 82.13)	Acc@5 100.00 ( 99.13)
Test: [ 70/100]	Time  0.022 ( 0.022)	Loss 5.1855e-01 (5.4974e-01)	Acc@1  82.00 ( 81.93)	Acc@5 100.00 ( 99.18)
Test: [ 80/100]	Time  0.017 ( 0.022)	Loss 4.4849e-01 (5.5129e-01)	Acc@1  83.00 ( 81.81)	Acc@5  99.00 ( 99.15)
Test: [ 90/100]	Time  0.020 ( 0.022)	Loss 3.4448e-01 (5.5281e-01)	Acc@1  86.00 ( 81.78)	Acc@5 100.00 ( 99.09)
 * Acc@1 81.670 Acc@5 99.070
### epoch[13] execution time: 18.24793004989624
EPOCH 14
# current learning rate: 0.1
# Switched to train mode...
Epoch: [14][  0/391]	Time  0.214 ( 0.214)	Data  0.174 ( 0.174)	Loss 4.4580e-01 (4.4580e-01)	Acc@1  84.38 ( 84.38)	Acc@5  98.44 ( 98.44)
Epoch: [14][ 10/391]	Time  0.042 ( 0.056)	Data  0.001 ( 0.017)	Loss 3.0029e-01 (4.0843e-01)	Acc@1  89.84 ( 86.22)	Acc@5 100.00 ( 99.43)
Epoch: [14][ 20/391]	Time  0.042 ( 0.049)	Data  0.001 ( 0.009)	Loss 3.0957e-01 (3.9967e-01)	Acc@1  89.06 ( 86.57)	Acc@5 100.00 ( 99.55)
Epoch: [14][ 30/391]	Time  0.040 ( 0.046)	Data  0.001 ( 0.007)	Loss 5.9326e-01 (4.1502e-01)	Acc@1  79.69 ( 86.16)	Acc@5  99.22 ( 99.42)
Epoch: [14][ 40/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.005)	Loss 4.4873e-01 (4.1510e-01)	Acc@1  84.38 ( 85.92)	Acc@5 100.00 ( 99.50)
Epoch: [14][ 50/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.004)	Loss 2.7734e-01 (4.1488e-01)	Acc@1  89.06 ( 85.81)	Acc@5 100.00 ( 99.53)
Epoch: [14][ 60/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.004)	Loss 4.8657e-01 (4.1911e-01)	Acc@1  85.94 ( 85.57)	Acc@5  99.22 ( 99.49)
Epoch: [14][ 70/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 6.6992e-01 (4.1963e-01)	Acc@1  75.78 ( 85.55)	Acc@5  97.66 ( 99.43)
Epoch: [14][ 80/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.0664e-01 (4.2013e-01)	Acc@1  88.28 ( 85.50)	Acc@5 100.00 ( 99.42)
Epoch: [14][ 90/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.5581e-01 (4.2200e-01)	Acc@1  83.59 ( 85.50)	Acc@5  98.44 ( 99.39)
Epoch: [14][100/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.9355e-01 (4.2171e-01)	Acc@1  85.94 ( 85.46)	Acc@5 100.00 ( 99.43)
Epoch: [14][110/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.7832e-01 (4.1994e-01)	Acc@1  91.41 ( 85.61)	Acc@5 100.00 ( 99.44)
Epoch: [14][120/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.1309e-01 (4.2111e-01)	Acc@1  85.94 ( 85.60)	Acc@5 100.00 ( 99.42)
Epoch: [14][130/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.2896e-01 (4.2193e-01)	Acc@1  88.28 ( 85.62)	Acc@5 100.00 ( 99.42)
Epoch: [14][140/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9561e-01 (4.2565e-01)	Acc@1  82.81 ( 85.49)	Acc@5  99.22 ( 99.41)
Epoch: [14][150/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1138e-01 (4.2607e-01)	Acc@1  83.59 ( 85.48)	Acc@5  99.22 ( 99.39)
Epoch: [14][160/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.6802e-01 (4.2481e-01)	Acc@1  83.59 ( 85.46)	Acc@5  97.66 ( 99.40)
Epoch: [14][170/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2783e-01 (4.2502e-01)	Acc@1  82.81 ( 85.45)	Acc@5  99.22 ( 99.40)
Epoch: [14][180/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4790e-01 (4.2531e-01)	Acc@1  85.94 ( 85.43)	Acc@5 100.00 ( 99.40)
Epoch: [14][190/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7207e-01 (4.2484e-01)	Acc@1  85.16 ( 85.44)	Acc@5  99.22 ( 99.39)
Epoch: [14][200/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1016e-01 (4.2556e-01)	Acc@1  85.16 ( 85.37)	Acc@5 100.00 ( 99.41)
Epoch: [14][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.4541e-01 (4.2386e-01)	Acc@1  77.34 ( 85.43)	Acc@5 100.00 ( 99.41)
Epoch: [14][220/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9097e-01 (4.2310e-01)	Acc@1  82.81 ( 85.41)	Acc@5  99.22 ( 99.42)
Epoch: [14][230/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.5435e-01 (4.2366e-01)	Acc@1  84.38 ( 85.38)	Acc@5  99.22 ( 99.42)
Epoch: [14][240/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4116e-01 (4.2501e-01)	Acc@1  86.72 ( 85.37)	Acc@5  98.44 ( 99.41)
Epoch: [14][250/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.6826e-01 (4.2607e-01)	Acc@1  82.81 ( 85.36)	Acc@5 100.00 ( 99.41)
Epoch: [14][260/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7681e-01 (4.2523e-01)	Acc@1  83.59 ( 85.39)	Acc@5  98.44 ( 99.40)
Epoch: [14][270/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2886e-01 (4.2532e-01)	Acc@1  86.72 ( 85.39)	Acc@5 100.00 ( 99.41)
Epoch: [14][280/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.6592e-01 (4.2532e-01)	Acc@1  81.25 ( 85.41)	Acc@5  99.22 ( 99.41)
Epoch: [14][290/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 3.6572e-01 (4.2584e-01)	Acc@1  86.72 ( 85.41)	Acc@5 100.00 ( 99.41)
Epoch: [14][300/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.1309e-01 (4.2601e-01)	Acc@1  85.16 ( 85.42)	Acc@5  98.44 ( 99.41)
Epoch: [14][310/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.4189e-01 (4.2750e-01)	Acc@1  88.28 ( 85.37)	Acc@5  99.22 ( 99.41)
Epoch: [14][320/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.4580e-01 (4.2728e-01)	Acc@1  81.25 ( 85.38)	Acc@5  99.22 ( 99.42)
Epoch: [14][330/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.4849e-01 (4.2743e-01)	Acc@1  85.94 ( 85.38)	Acc@5 100.00 ( 99.43)
Epoch: [14][340/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.8633e-01 (4.2737e-01)	Acc@1  83.59 ( 85.37)	Acc@5  99.22 ( 99.43)
Epoch: [14][350/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.9341e-01 (4.2785e-01)	Acc@1  80.47 ( 85.35)	Acc@5  98.44 ( 99.44)
Epoch: [14][360/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.9795e-01 (4.2727e-01)	Acc@1  86.72 ( 85.34)	Acc@5  98.44 ( 99.44)
Epoch: [14][370/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.0000e-01 (4.2733e-01)	Acc@1  88.28 ( 85.37)	Acc@5  99.22 ( 99.43)
Epoch: [14][380/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.2139e-01 (4.2730e-01)	Acc@1  82.81 ( 85.35)	Acc@5 100.00 ( 99.43)
Epoch: [14][390/391]	Time  0.026 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.1953e-01 (4.2742e-01)	Acc@1  82.50 ( 85.34)	Acc@5  98.75 ( 99.43)
## e[14] optimizer.zero_grad (sum) time: 0.2517893314361572
## e[14]       loss.backward (sum) time: 4.186630487442017
## e[14]      optimizer.step (sum) time: 1.7427778244018555
## epoch[14] training(only) time: 15.885684728622437
# Switched to evaluate mode...
Test: [  0/100]	Time  0.168 ( 0.168)	Loss 6.6797e-01 (6.6797e-01)	Acc@1  78.00 ( 78.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.017 ( 0.033)	Loss 4.6655e-01 (4.8762e-01)	Acc@1  83.00 ( 82.45)	Acc@5  99.00 ( 99.27)
Test: [ 20/100]	Time  0.020 ( 0.027)	Loss 5.2100e-01 (4.9483e-01)	Acc@1  80.00 ( 82.90)	Acc@5 100.00 ( 99.38)
Test: [ 30/100]	Time  0.021 ( 0.024)	Loss 5.7275e-01 (4.9950e-01)	Acc@1  83.00 ( 83.13)	Acc@5  98.00 ( 99.32)
Test: [ 40/100]	Time  0.020 ( 0.023)	Loss 4.5947e-01 (4.8960e-01)	Acc@1  83.00 ( 83.46)	Acc@5  99.00 ( 99.22)
Test: [ 50/100]	Time  0.022 ( 0.023)	Loss 5.1709e-01 (4.9145e-01)	Acc@1  86.00 ( 83.71)	Acc@5  97.00 ( 99.18)
Test: [ 60/100]	Time  0.021 ( 0.022)	Loss 5.4492e-01 (4.9296e-01)	Acc@1  84.00 ( 83.67)	Acc@5 100.00 ( 99.20)
Test: [ 70/100]	Time  0.024 ( 0.022)	Loss 4.8755e-01 (4.8923e-01)	Acc@1  84.00 ( 83.79)	Acc@5 100.00 ( 99.23)
Test: [ 80/100]	Time  0.022 ( 0.022)	Loss 3.4375e-01 (4.8964e-01)	Acc@1  88.00 ( 83.73)	Acc@5 100.00 ( 99.27)
Test: [ 90/100]	Time  0.020 ( 0.022)	Loss 3.1689e-01 (4.9044e-01)	Acc@1  89.00 ( 83.67)	Acc@5 100.00 ( 99.32)
 * Acc@1 83.710 Acc@5 99.320
### epoch[14] execution time: 18.16798758506775
EPOCH 15
# current learning rate: 0.1
# Switched to train mode...
Epoch: [15][  0/391]	Time  0.219 ( 0.219)	Data  0.177 ( 0.177)	Loss 4.0527e-01 (4.0527e-01)	Acc@1  87.50 ( 87.50)	Acc@5  99.22 ( 99.22)
Epoch: [15][ 10/391]	Time  0.038 ( 0.056)	Data  0.001 ( 0.017)	Loss 3.7866e-01 (4.2991e-01)	Acc@1  88.28 ( 86.08)	Acc@5  98.44 ( 99.22)
Epoch: [15][ 20/391]	Time  0.038 ( 0.048)	Data  0.001 ( 0.009)	Loss 3.6426e-01 (4.1509e-01)	Acc@1  88.28 ( 86.57)	Acc@5 100.00 ( 99.44)
Epoch: [15][ 30/391]	Time  0.038 ( 0.045)	Data  0.001 ( 0.007)	Loss 4.8950e-01 (4.0641e-01)	Acc@1  79.69 ( 86.57)	Acc@5 100.00 ( 99.55)
Epoch: [15][ 40/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.005)	Loss 5.7373e-01 (4.1567e-01)	Acc@1  82.03 ( 86.13)	Acc@5  99.22 ( 99.52)
Epoch: [15][ 50/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.004)	Loss 4.7656e-01 (4.1852e-01)	Acc@1  82.03 ( 85.97)	Acc@5  99.22 ( 99.51)
Epoch: [15][ 60/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.004)	Loss 4.2749e-01 (4.1834e-01)	Acc@1  85.16 ( 85.78)	Acc@5  98.44 ( 99.49)
Epoch: [15][ 70/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.8169e-01 (4.1696e-01)	Acc@1  85.94 ( 85.78)	Acc@5 100.00 ( 99.50)
Epoch: [15][ 80/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.4937e-01 (4.1491e-01)	Acc@1  88.28 ( 85.72)	Acc@5 100.00 ( 99.49)
Epoch: [15][ 90/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.0161e-01 (4.1999e-01)	Acc@1  87.50 ( 85.42)	Acc@5  99.22 ( 99.45)
Epoch: [15][100/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.7095e-01 (4.1954e-01)	Acc@1  84.38 ( 85.55)	Acc@5 100.00 ( 99.44)
Epoch: [15][110/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.1040e-01 (4.1900e-01)	Acc@1  84.38 ( 85.55)	Acc@5 100.00 ( 99.44)
Epoch: [15][120/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1416e-01 (4.1931e-01)	Acc@1  78.91 ( 85.41)	Acc@5 100.00 ( 99.44)
Epoch: [15][130/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8013e-01 (4.1912e-01)	Acc@1  88.28 ( 85.48)	Acc@5  99.22 ( 99.43)
Epoch: [15][140/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7412e-01 (4.2197e-01)	Acc@1  82.03 ( 85.34)	Acc@5 100.00 ( 99.43)
Epoch: [15][150/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5205e-01 (4.1978e-01)	Acc@1  85.94 ( 85.40)	Acc@5 100.00 ( 99.44)
Epoch: [15][160/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0210e-01 (4.1733e-01)	Acc@1  88.28 ( 85.52)	Acc@5  99.22 ( 99.44)
Epoch: [15][170/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1611e-01 (4.1756e-01)	Acc@1  83.59 ( 85.46)	Acc@5  99.22 ( 99.43)
Epoch: [15][180/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4326e-01 (4.1682e-01)	Acc@1  85.94 ( 85.53)	Acc@5 100.00 ( 99.41)
Epoch: [15][190/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0112e-01 (4.1458e-01)	Acc@1  84.38 ( 85.59)	Acc@5  99.22 ( 99.41)
Epoch: [15][200/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.8076e-01 (4.1280e-01)	Acc@1  89.06 ( 85.63)	Acc@5 100.00 ( 99.42)
Epoch: [15][210/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7656e-01 (4.1250e-01)	Acc@1  79.69 ( 85.57)	Acc@5 100.00 ( 99.43)
Epoch: [15][220/391]	Time  0.048 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.5322e-01 (4.1203e-01)	Acc@1  84.38 ( 85.60)	Acc@5  98.44 ( 99.43)
Epoch: [15][230/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4824e-01 (4.1338e-01)	Acc@1  85.16 ( 85.54)	Acc@5  99.22 ( 99.43)
Epoch: [15][240/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8647e-01 (4.1289e-01)	Acc@1  87.50 ( 85.57)	Acc@5 100.00 ( 99.43)
Epoch: [15][250/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.3369e-01 (4.1339e-01)	Acc@1  83.59 ( 85.54)	Acc@5  95.31 ( 99.42)
Epoch: [15][260/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3350e-01 (4.1453e-01)	Acc@1  85.94 ( 85.49)	Acc@5 100.00 ( 99.42)
Epoch: [15][270/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.4377e-01 (4.1434e-01)	Acc@1  92.19 ( 85.51)	Acc@5 100.00 ( 99.42)
Epoch: [15][280/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9600e-01 (4.1426e-01)	Acc@1  89.06 ( 85.53)	Acc@5  98.44 ( 99.41)
Epoch: [15][290/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.6670e-01 (4.1382e-01)	Acc@1  85.16 ( 85.53)	Acc@5  99.22 ( 99.41)
Epoch: [15][300/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.4473e-01 (4.1273e-01)	Acc@1  87.50 ( 85.54)	Acc@5  98.44 ( 99.42)
Epoch: [15][310/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.1426e-01 (4.1335e-01)	Acc@1  80.47 ( 85.51)	Acc@5  98.44 ( 99.43)
Epoch: [15][320/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.0342e-01 (4.1352e-01)	Acc@1  84.38 ( 85.52)	Acc@5  99.22 ( 99.44)
Epoch: [15][330/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.0635e-01 (4.1546e-01)	Acc@1  82.81 ( 85.49)	Acc@5 100.00 ( 99.41)
Epoch: [15][340/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.3862e-01 (4.1499e-01)	Acc@1  88.28 ( 85.54)	Acc@5 100.00 ( 99.41)
Epoch: [15][350/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.3960e-01 (4.1524e-01)	Acc@1  90.62 ( 85.53)	Acc@5  97.66 ( 99.41)
Epoch: [15][360/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.1064e-01 (4.1477e-01)	Acc@1  87.50 ( 85.57)	Acc@5 100.00 ( 99.41)
Epoch: [15][370/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.0635e-01 (4.1466e-01)	Acc@1  78.91 ( 85.55)	Acc@5 100.00 ( 99.41)
Epoch: [15][380/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.4424e-01 (4.1558e-01)	Acc@1  85.94 ( 85.54)	Acc@5 100.00 ( 99.41)
Epoch: [15][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.2529e-01 (4.1700e-01)	Acc@1  85.00 ( 85.49)	Acc@5 100.00 ( 99.40)
## e[15] optimizer.zero_grad (sum) time: 0.25074005126953125
## e[15]       loss.backward (sum) time: 4.263311147689819
## e[15]      optimizer.step (sum) time: 1.732217788696289
## epoch[15] training(only) time: 15.972334384918213
# Switched to evaluate mode...
Test: [  0/100]	Time  0.180 ( 0.180)	Loss 5.0928e-01 (5.0928e-01)	Acc@1  79.00 ( 79.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.019 ( 0.035)	Loss 5.4248e-01 (4.7452e-01)	Acc@1  85.00 ( 83.64)	Acc@5  99.00 ( 99.55)
Test: [ 20/100]	Time  0.024 ( 0.028)	Loss 5.9033e-01 (4.9899e-01)	Acc@1  80.00 ( 82.86)	Acc@5  99.00 ( 99.33)
Test: [ 30/100]	Time  0.019 ( 0.026)	Loss 5.9424e-01 (5.1700e-01)	Acc@1  80.00 ( 82.48)	Acc@5  99.00 ( 99.29)
Test: [ 40/100]	Time  0.023 ( 0.025)	Loss 4.6509e-01 (5.1797e-01)	Acc@1  79.00 ( 82.39)	Acc@5  99.00 ( 99.24)
Test: [ 50/100]	Time  0.022 ( 0.024)	Loss 5.3760e-01 (5.1191e-01)	Acc@1  82.00 ( 82.67)	Acc@5  98.00 ( 99.18)
Test: [ 60/100]	Time  0.017 ( 0.023)	Loss 5.2051e-01 (5.1740e-01)	Acc@1  81.00 ( 82.52)	Acc@5 100.00 ( 99.20)
Test: [ 70/100]	Time  0.020 ( 0.023)	Loss 4.6240e-01 (5.1457e-01)	Acc@1  83.00 ( 82.63)	Acc@5  99.00 ( 99.20)
Test: [ 80/100]	Time  0.023 ( 0.023)	Loss 4.6558e-01 (5.1093e-01)	Acc@1  83.00 ( 82.72)	Acc@5  99.00 ( 99.19)
Test: [ 90/100]	Time  0.024 ( 0.022)	Loss 4.3042e-01 (5.1034e-01)	Acc@1  85.00 ( 82.74)	Acc@5 100.00 ( 99.22)
 * Acc@1 82.750 Acc@5 99.240
### epoch[15] execution time: 18.291676998138428
EPOCH 16
# current learning rate: 0.1
# Switched to train mode...
Epoch: [16][  0/391]	Time  0.214 ( 0.214)	Data  0.174 ( 0.174)	Loss 3.7891e-01 (3.7891e-01)	Acc@1  85.16 ( 85.16)	Acc@5  99.22 ( 99.22)
Epoch: [16][ 10/391]	Time  0.040 ( 0.056)	Data  0.001 ( 0.017)	Loss 4.0308e-01 (3.8601e-01)	Acc@1  84.38 ( 87.14)	Acc@5 100.00 ( 99.64)
Epoch: [16][ 20/391]	Time  0.041 ( 0.049)	Data  0.001 ( 0.009)	Loss 4.5801e-01 (4.0168e-01)	Acc@1  86.72 ( 86.31)	Acc@5 100.00 ( 99.70)
Epoch: [16][ 30/391]	Time  0.042 ( 0.046)	Data  0.001 ( 0.007)	Loss 4.2017e-01 (4.1224e-01)	Acc@1  86.72 ( 85.94)	Acc@5 100.00 ( 99.55)
Epoch: [16][ 40/391]	Time  0.038 ( 0.045)	Data  0.001 ( 0.005)	Loss 3.7451e-01 (3.9757e-01)	Acc@1  84.38 ( 86.59)	Acc@5 100.00 ( 99.58)
Epoch: [16][ 50/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.004)	Loss 3.5132e-01 (3.9430e-01)	Acc@1  89.06 ( 86.75)	Acc@5 100.00 ( 99.63)
Epoch: [16][ 60/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.004)	Loss 4.3018e-01 (3.9870e-01)	Acc@1  84.38 ( 86.44)	Acc@5  98.44 ( 99.63)
Epoch: [16][ 70/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.003)	Loss 2.6099e-01 (3.9771e-01)	Acc@1  93.75 ( 86.44)	Acc@5 100.00 ( 99.60)
Epoch: [16][ 80/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.4717e-01 (3.9799e-01)	Acc@1  89.06 ( 86.50)	Acc@5  99.22 ( 99.61)
Epoch: [16][ 90/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.6094e-01 (3.9664e-01)	Acc@1  80.47 ( 86.41)	Acc@5 100.00 ( 99.60)
Epoch: [16][100/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.1562e-01 (4.0188e-01)	Acc@1  79.69 ( 86.15)	Acc@5  97.66 ( 99.56)
Epoch: [16][110/391]	Time  0.045 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.3569e-01 (4.0392e-01)	Acc@1  89.06 ( 86.06)	Acc@5  99.22 ( 99.53)
Epoch: [16][120/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0283e-01 (4.0272e-01)	Acc@1  89.06 ( 86.08)	Acc@5  99.22 ( 99.50)
Epoch: [16][130/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6890e-01 (4.0207e-01)	Acc@1  84.38 ( 86.13)	Acc@5 100.00 ( 99.49)
Epoch: [16][140/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5742e-01 (4.0354e-01)	Acc@1  88.28 ( 86.10)	Acc@5 100.00 ( 99.51)
Epoch: [16][150/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.9297e-01 (4.0452e-01)	Acc@1  92.19 ( 86.11)	Acc@5  99.22 ( 99.49)
Epoch: [16][160/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8525e-01 (4.0373e-01)	Acc@1  85.94 ( 86.09)	Acc@5  98.44 ( 99.51)
Epoch: [16][170/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.8882e-01 (4.0032e-01)	Acc@1  91.41 ( 86.27)	Acc@5 100.00 ( 99.52)
Epoch: [16][180/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.6562e-01 (3.9759e-01)	Acc@1  90.62 ( 86.33)	Acc@5 100.00 ( 99.53)
Epoch: [16][190/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4790e-01 (3.9839e-01)	Acc@1  88.28 ( 86.30)	Acc@5 100.00 ( 99.52)
Epoch: [16][200/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2773e-01 (3.9942e-01)	Acc@1  85.94 ( 86.31)	Acc@5 100.00 ( 99.54)
Epoch: [16][210/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8232e-01 (4.0193e-01)	Acc@1  85.94 ( 86.21)	Acc@5  99.22 ( 99.52)
Epoch: [16][220/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1494e-01 (4.0368e-01)	Acc@1  88.28 ( 86.18)	Acc@5  99.22 ( 99.50)
Epoch: [16][230/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8330e-01 (4.0291e-01)	Acc@1  86.72 ( 86.17)	Acc@5 100.00 ( 99.50)
Epoch: [16][240/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.5176e-01 (4.0368e-01)	Acc@1  83.59 ( 86.11)	Acc@5  98.44 ( 99.51)
Epoch: [16][250/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2344e-01 (4.0501e-01)	Acc@1  82.03 ( 86.07)	Acc@5  97.66 ( 99.51)
Epoch: [16][260/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2544e-01 (4.0453e-01)	Acc@1  88.28 ( 86.07)	Acc@5 100.00 ( 99.51)
Epoch: [16][270/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2041e-01 (4.0483e-01)	Acc@1  85.94 ( 86.04)	Acc@5  98.44 ( 99.50)
Epoch: [16][280/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.3418e-01 (4.0534e-01)	Acc@1  80.47 ( 86.01)	Acc@5  98.44 ( 99.49)
Epoch: [16][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1284e-01 (4.0448e-01)	Acc@1  87.50 ( 86.04)	Acc@5  99.22 ( 99.49)
Epoch: [16][300/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7012e-01 (4.0378e-01)	Acc@1  89.84 ( 86.08)	Acc@5 100.00 ( 99.49)
Epoch: [16][310/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.9614e-01 (4.0345e-01)	Acc@1  86.72 ( 86.10)	Acc@5 100.00 ( 99.49)
Epoch: [16][320/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.8672e-01 (4.0225e-01)	Acc@1  88.28 ( 86.18)	Acc@5 100.00 ( 99.49)
Epoch: [16][330/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.5420e-01 (4.0297e-01)	Acc@1  79.69 ( 86.13)	Acc@5  99.22 ( 99.48)
Epoch: [16][340/391]	Time  0.046 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.8501e-01 (4.0236e-01)	Acc@1  86.72 ( 86.18)	Acc@5  97.66 ( 99.47)
Epoch: [16][350/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.4629e-01 (4.0309e-01)	Acc@1  85.16 ( 86.18)	Acc@5  99.22 ( 99.47)
Epoch: [16][360/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.2056e-01 (4.0374e-01)	Acc@1  87.50 ( 86.18)	Acc@5 100.00 ( 99.48)
Epoch: [16][370/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.4800e-01 (4.0453e-01)	Acc@1  83.59 ( 86.14)	Acc@5  99.22 ( 99.48)
Epoch: [16][380/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.9380e-01 (4.0531e-01)	Acc@1  85.94 ( 86.11)	Acc@5  98.44 ( 99.47)
Epoch: [16][390/391]	Time  0.026 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.9707e-01 (4.0660e-01)	Acc@1  86.25 ( 86.06)	Acc@5  98.75 ( 99.46)
## e[16] optimizer.zero_grad (sum) time: 0.252338171005249
## e[16]       loss.backward (sum) time: 4.191229343414307
## e[16]      optimizer.step (sum) time: 1.743783950805664
## epoch[16] training(only) time: 15.918222665786743
# Switched to evaluate mode...
Test: [  0/100]	Time  0.162 ( 0.162)	Loss 5.5615e-01 (5.5615e-01)	Acc@1  81.00 ( 81.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.023 ( 0.033)	Loss 5.2441e-01 (6.0050e-01)	Acc@1  82.00 ( 81.09)	Acc@5  99.00 ( 99.00)
Test: [ 20/100]	Time  0.017 ( 0.026)	Loss 6.4502e-01 (6.0704e-01)	Acc@1  72.00 ( 80.52)	Acc@5 100.00 ( 98.90)
Test: [ 30/100]	Time  0.018 ( 0.024)	Loss 5.8301e-01 (6.0098e-01)	Acc@1  80.00 ( 80.68)	Acc@5  98.00 ( 98.81)
Test: [ 40/100]	Time  0.021 ( 0.023)	Loss 6.5576e-01 (6.0708e-01)	Acc@1  79.00 ( 80.63)	Acc@5  99.00 ( 98.88)
Test: [ 50/100]	Time  0.020 ( 0.022)	Loss 6.7285e-01 (6.0724e-01)	Acc@1  79.00 ( 80.69)	Acc@5  97.00 ( 98.82)
Test: [ 60/100]	Time  0.020 ( 0.022)	Loss 6.2793e-01 (6.0865e-01)	Acc@1  78.00 ( 80.54)	Acc@5  99.00 ( 98.82)
Test: [ 70/100]	Time  0.017 ( 0.021)	Loss 5.6689e-01 (6.0459e-01)	Acc@1  88.00 ( 80.68)	Acc@5  98.00 ( 98.90)
Test: [ 80/100]	Time  0.020 ( 0.021)	Loss 5.7666e-01 (6.0013e-01)	Acc@1  83.00 ( 80.72)	Acc@5 100.00 ( 98.94)
Test: [ 90/100]	Time  0.023 ( 0.021)	Loss 5.8936e-01 (6.0709e-01)	Acc@1  81.00 ( 80.53)	Acc@5 100.00 ( 98.93)
 * Acc@1 80.410 Acc@5 98.910
### epoch[16] execution time: 18.15387511253357
EPOCH 17
# current learning rate: 0.1
# Switched to train mode...
Epoch: [17][  0/391]	Time  0.218 ( 0.218)	Data  0.176 ( 0.176)	Loss 3.5352e-01 (3.5352e-01)	Acc@1  89.84 ( 89.84)	Acc@5 100.00 (100.00)
Epoch: [17][ 10/391]	Time  0.038 ( 0.056)	Data  0.001 ( 0.017)	Loss 5.3955e-01 (4.0543e-01)	Acc@1  81.25 ( 85.65)	Acc@5 100.00 ( 99.72)
Epoch: [17][ 20/391]	Time  0.041 ( 0.049)	Data  0.001 ( 0.009)	Loss 3.3984e-01 (3.9914e-01)	Acc@1  87.50 ( 86.35)	Acc@5 100.00 ( 99.59)
Epoch: [17][ 30/391]	Time  0.040 ( 0.046)	Data  0.001 ( 0.007)	Loss 4.0356e-01 (3.8746e-01)	Acc@1  89.06 ( 86.92)	Acc@5  99.22 ( 99.65)
Epoch: [17][ 40/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.005)	Loss 4.1772e-01 (3.8620e-01)	Acc@1  88.28 ( 86.95)	Acc@5 100.00 ( 99.66)
Epoch: [17][ 50/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 2.9248e-01 (3.7873e-01)	Acc@1  87.50 ( 86.99)	Acc@5 100.00 ( 99.63)
Epoch: [17][ 60/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.004)	Loss 4.4727e-01 (3.8168e-01)	Acc@1  82.81 ( 86.77)	Acc@5  99.22 ( 99.63)
Epoch: [17][ 70/391]	Time  0.037 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.3433e-01 (3.8387e-01)	Acc@1  83.59 ( 86.64)	Acc@5 100.00 ( 99.60)
Epoch: [17][ 80/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.6157e-01 (3.7798e-01)	Acc@1  83.59 ( 86.82)	Acc@5 100.00 ( 99.62)
Epoch: [17][ 90/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.1763e-01 (3.7878e-01)	Acc@1  86.72 ( 86.77)	Acc@5  99.22 ( 99.61)
Epoch: [17][100/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.0664e-01 (3.7141e-01)	Acc@1  89.06 ( 87.08)	Acc@5  99.22 ( 99.62)
Epoch: [17][110/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3276e-01 (3.6900e-01)	Acc@1  89.06 ( 87.22)	Acc@5  99.22 ( 99.61)
Epoch: [17][120/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2529e-01 (3.7073e-01)	Acc@1  83.59 ( 87.18)	Acc@5  99.22 ( 99.62)
Epoch: [17][130/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8037e-01 (3.7225e-01)	Acc@1  85.94 ( 87.13)	Acc@5  98.44 ( 99.59)
Epoch: [17][140/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9429e-01 (3.7156e-01)	Acc@1  85.16 ( 87.13)	Acc@5 100.00 ( 99.61)
Epoch: [17][150/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0234e-01 (3.7333e-01)	Acc@1  87.50 ( 87.08)	Acc@5  99.22 ( 99.60)
Epoch: [17][160/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8208e-01 (3.7396e-01)	Acc@1  89.06 ( 87.05)	Acc@5 100.00 ( 99.60)
Epoch: [17][170/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.5078e-01 (3.7507e-01)	Acc@1  78.91 ( 87.02)	Acc@5  99.22 ( 99.59)
Epoch: [17][180/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3130e-01 (3.7677e-01)	Acc@1  89.06 ( 86.96)	Acc@5 100.00 ( 99.58)
Epoch: [17][190/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7412e-01 (3.8079e-01)	Acc@1  82.81 ( 86.86)	Acc@5 100.00 ( 99.57)
Epoch: [17][200/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8535e-01 (3.8028e-01)	Acc@1  83.59 ( 86.87)	Acc@5 100.00 ( 99.58)
Epoch: [17][210/391]	Time  0.040 ( 0.041)	Data  0.002 ( 0.002)	Loss 3.3936e-01 (3.7929e-01)	Acc@1  89.84 ( 86.89)	Acc@5  99.22 ( 99.59)
Epoch: [17][220/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7573e-01 (3.8062e-01)	Acc@1  86.72 ( 86.87)	Acc@5  99.22 ( 99.58)
Epoch: [17][230/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.9614e-01 (3.8260e-01)	Acc@1  91.41 ( 86.79)	Acc@5  99.22 ( 99.58)
Epoch: [17][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.6606e-01 (3.8305e-01)	Acc@1  84.38 ( 86.77)	Acc@5 100.00 ( 99.59)
Epoch: [17][250/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.9688e-01 (3.8196e-01)	Acc@1  89.84 ( 86.81)	Acc@5  99.22 ( 99.59)
Epoch: [17][260/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1138e-01 (3.8218e-01)	Acc@1  86.72 ( 86.79)	Acc@5  99.22 ( 99.58)
Epoch: [17][270/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7231e-01 (3.8385e-01)	Acc@1  87.50 ( 86.79)	Acc@5 100.00 ( 99.58)
Epoch: [17][280/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8062e-01 (3.8483e-01)	Acc@1  87.50 ( 86.74)	Acc@5 100.00 ( 99.57)
Epoch: [17][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2920e-01 (3.8493e-01)	Acc@1  85.94 ( 86.73)	Acc@5  99.22 ( 99.55)
Epoch: [17][300/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3677e-01 (3.8447e-01)	Acc@1  85.16 ( 86.77)	Acc@5  99.22 ( 99.55)
Epoch: [17][310/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.2358e-01 (3.8668e-01)	Acc@1  84.38 ( 86.68)	Acc@5 100.00 ( 99.54)
Epoch: [17][320/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.1235e-01 (3.8799e-01)	Acc@1  85.94 ( 86.62)	Acc@5 100.00 ( 99.53)
Epoch: [17][330/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.1138e-01 (3.8790e-01)	Acc@1  85.94 ( 86.61)	Acc@5 100.00 ( 99.54)
Epoch: [17][340/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.9229e-01 (3.8898e-01)	Acc@1  77.34 ( 86.56)	Acc@5 100.00 ( 99.54)
Epoch: [17][350/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.3813e-01 (3.8875e-01)	Acc@1  88.28 ( 86.57)	Acc@5 100.00 ( 99.53)
Epoch: [17][360/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.2422e-01 (3.8875e-01)	Acc@1  87.50 ( 86.56)	Acc@5 100.00 ( 99.54)
Epoch: [17][370/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.8540e-01 (3.8880e-01)	Acc@1  89.84 ( 86.55)	Acc@5 100.00 ( 99.54)
Epoch: [17][380/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.5093e-01 (3.9037e-01)	Acc@1  85.16 ( 86.50)	Acc@5 100.00 ( 99.52)
Epoch: [17][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.9688e-01 (3.9071e-01)	Acc@1  91.25 ( 86.47)	Acc@5 100.00 ( 99.53)
## e[17] optimizer.zero_grad (sum) time: 0.25318145751953125
## e[17]       loss.backward (sum) time: 4.191782474517822
## e[17]      optimizer.step (sum) time: 1.7528715133666992
## epoch[17] training(only) time: 15.917550802230835
# Switched to evaluate mode...
Test: [  0/100]	Time  0.173 ( 0.173)	Loss 6.6504e-01 (6.6504e-01)	Acc@1  77.00 ( 77.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.019 ( 0.034)	Loss 7.8613e-01 (6.5574e-01)	Acc@1  77.00 ( 79.73)	Acc@5 100.00 ( 99.18)
Test: [ 20/100]	Time  0.017 ( 0.027)	Loss 8.2227e-01 (6.4617e-01)	Acc@1  77.00 ( 79.52)	Acc@5  99.00 ( 98.90)
Test: [ 30/100]	Time  0.020 ( 0.025)	Loss 6.5674e-01 (6.4428e-01)	Acc@1  78.00 ( 79.71)	Acc@5  99.00 ( 98.77)
Test: [ 40/100]	Time  0.023 ( 0.024)	Loss 6.6650e-01 (6.3745e-01)	Acc@1  78.00 ( 79.83)	Acc@5 100.00 ( 98.88)
Test: [ 50/100]	Time  0.017 ( 0.023)	Loss 5.0342e-01 (6.3372e-01)	Acc@1  82.00 ( 80.00)	Acc@5 100.00 ( 98.84)
Test: [ 60/100]	Time  0.019 ( 0.022)	Loss 7.4414e-01 (6.3389e-01)	Acc@1  79.00 ( 80.00)	Acc@5  99.00 ( 98.90)
Test: [ 70/100]	Time  0.020 ( 0.022)	Loss 6.2256e-01 (6.3404e-01)	Acc@1  79.00 ( 79.75)	Acc@5  99.00 ( 98.96)
Test: [ 80/100]	Time  0.021 ( 0.022)	Loss 4.5459e-01 (6.2856e-01)	Acc@1  84.00 ( 79.75)	Acc@5 100.00 ( 98.96)
Test: [ 90/100]	Time  0.020 ( 0.022)	Loss 4.9854e-01 (6.3218e-01)	Acc@1  81.00 ( 79.56)	Acc@5 100.00 ( 99.00)
 * Acc@1 79.470 Acc@5 99.020
### epoch[17] execution time: 18.204699754714966
EPOCH 18
# current learning rate: 0.1
# Switched to train mode...
Epoch: [18][  0/391]	Time  0.207 ( 0.207)	Data  0.169 ( 0.169)	Loss 4.2041e-01 (4.2041e-01)	Acc@1  83.59 ( 83.59)	Acc@5  99.22 ( 99.22)
Epoch: [18][ 10/391]	Time  0.039 ( 0.055)	Data  0.001 ( 0.016)	Loss 3.5474e-01 (3.8104e-01)	Acc@1  89.06 ( 87.14)	Acc@5 100.00 ( 99.57)
Epoch: [18][ 20/391]	Time  0.043 ( 0.049)	Data  0.001 ( 0.009)	Loss 3.2471e-01 (3.6132e-01)	Acc@1  88.28 ( 87.54)	Acc@5 100.00 ( 99.59)
Epoch: [18][ 30/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.006)	Loss 4.9536e-01 (3.6628e-01)	Acc@1  80.47 ( 87.17)	Acc@5  99.22 ( 99.52)
Epoch: [18][ 40/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.005)	Loss 2.5977e-01 (3.6112e-01)	Acc@1  92.97 ( 87.42)	Acc@5 100.00 ( 99.54)
Epoch: [18][ 50/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.004)	Loss 4.0479e-01 (3.6943e-01)	Acc@1  85.94 ( 87.01)	Acc@5  99.22 ( 99.53)
Epoch: [18][ 60/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.004)	Loss 3.4521e-01 (3.6628e-01)	Acc@1  88.28 ( 87.12)	Acc@5 100.00 ( 99.51)
Epoch: [18][ 70/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.4326e-01 (3.6925e-01)	Acc@1  89.84 ( 87.17)	Acc@5  99.22 ( 99.46)
Epoch: [18][ 80/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.7695e-01 (3.7137e-01)	Acc@1  87.50 ( 87.10)	Acc@5 100.00 ( 99.43)
Epoch: [18][ 90/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.7183e-01 (3.7162e-01)	Acc@1  84.38 ( 86.98)	Acc@5 100.00 ( 99.47)
Epoch: [18][100/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 2.6465e-01 (3.7355e-01)	Acc@1  89.84 ( 86.91)	Acc@5  99.22 ( 99.45)
Epoch: [18][110/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4448e-01 (3.7565e-01)	Acc@1  87.50 ( 86.81)	Acc@5 100.00 ( 99.47)
Epoch: [18][120/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3140e-01 (3.7511e-01)	Acc@1  85.16 ( 86.89)	Acc@5  99.22 ( 99.46)
Epoch: [18][130/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1982e-01 (3.7523e-01)	Acc@1  85.16 ( 86.84)	Acc@5 100.00 ( 99.48)
Epoch: [18][140/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4692e-01 (3.7668e-01)	Acc@1  88.28 ( 86.85)	Acc@5 100.00 ( 99.48)
Epoch: [18][150/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3408e-01 (3.7800e-01)	Acc@1  85.94 ( 86.84)	Acc@5  99.22 ( 99.49)
Epoch: [18][160/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.9126e-01 (3.7683e-01)	Acc@1  88.28 ( 86.82)	Acc@5 100.00 ( 99.50)
Epoch: [18][170/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8770e-01 (3.7768e-01)	Acc@1  87.50 ( 86.80)	Acc@5 100.00 ( 99.49)
Epoch: [18][180/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.8979e-01 (3.7738e-01)	Acc@1  91.41 ( 86.83)	Acc@5  99.22 ( 99.50)
Epoch: [18][190/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.9468e-01 (3.7513e-01)	Acc@1  90.62 ( 86.89)	Acc@5  99.22 ( 99.51)
Epoch: [18][200/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8013e-01 (3.7704e-01)	Acc@1  86.72 ( 86.85)	Acc@5 100.00 ( 99.52)
Epoch: [18][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9746e-01 (3.8021e-01)	Acc@1  84.38 ( 86.76)	Acc@5  99.22 ( 99.51)
Epoch: [18][220/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7720e-01 (3.8110e-01)	Acc@1  85.94 ( 86.79)	Acc@5 100.00 ( 99.50)
Epoch: [18][230/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9575e-01 (3.8267e-01)	Acc@1  85.94 ( 86.75)	Acc@5 100.00 ( 99.49)
Epoch: [18][240/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7573e-01 (3.8299e-01)	Acc@1  88.28 ( 86.72)	Acc@5  98.44 ( 99.47)
Epoch: [18][250/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9624e-01 (3.8167e-01)	Acc@1  82.81 ( 86.72)	Acc@5 100.00 ( 99.48)
Epoch: [18][260/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.9419e-01 (3.8291e-01)	Acc@1  91.41 ( 86.70)	Acc@5  99.22 ( 99.47)
Epoch: [18][270/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8804e-01 (3.8272e-01)	Acc@1  82.03 ( 86.71)	Acc@5 100.00 ( 99.48)
Epoch: [18][280/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.8135e-01 (3.8197e-01)	Acc@1  84.38 ( 86.70)	Acc@5  99.22 ( 99.48)
Epoch: [18][290/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.8379e-01 (3.8163e-01)	Acc@1  86.72 ( 86.73)	Acc@5  99.22 ( 99.49)
Epoch: [18][300/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.4482e-01 (3.8296e-01)	Acc@1  82.81 ( 86.69)	Acc@5  98.44 ( 99.48)
Epoch: [18][310/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.2725e-01 (3.8552e-01)	Acc@1  86.72 ( 86.63)	Acc@5  98.44 ( 99.47)
Epoch: [18][320/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.0542e-01 (3.8548e-01)	Acc@1  88.28 ( 86.61)	Acc@5  99.22 ( 99.48)
Epoch: [18][330/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.3921e-01 (3.8576e-01)	Acc@1  85.94 ( 86.61)	Acc@5 100.00 ( 99.48)
Epoch: [18][340/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.2275e-01 (3.8584e-01)	Acc@1  88.28 ( 86.59)	Acc@5  99.22 ( 99.49)
Epoch: [18][350/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.4849e-01 (3.8544e-01)	Acc@1  82.03 ( 86.59)	Acc@5 100.00 ( 99.49)
Epoch: [18][360/391]	Time  0.043 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.3521e-01 (3.8520e-01)	Acc@1  88.28 ( 86.58)	Acc@5  99.22 ( 99.50)
Epoch: [18][370/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.8721e-01 (3.8628e-01)	Acc@1  85.94 ( 86.55)	Acc@5 100.00 ( 99.49)
Epoch: [18][380/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.9575e-01 (3.8644e-01)	Acc@1  89.84 ( 86.55)	Acc@5 100.00 ( 99.50)
Epoch: [18][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.8882e-01 (3.8574e-01)	Acc@1  91.25 ( 86.58)	Acc@5 100.00 ( 99.50)
## e[18] optimizer.zero_grad (sum) time: 0.2501990795135498
## e[18]       loss.backward (sum) time: 4.170049428939819
## e[18]      optimizer.step (sum) time: 1.733628273010254
## epoch[18] training(only) time: 15.83893871307373
# Switched to evaluate mode...
Test: [  0/100]	Time  0.173 ( 0.173)	Loss 4.0747e-01 (4.0747e-01)	Acc@1  86.00 ( 86.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.017 ( 0.034)	Loss 5.1416e-01 (4.5891e-01)	Acc@1  84.00 ( 84.18)	Acc@5 100.00 ( 99.55)
Test: [ 20/100]	Time  0.023 ( 0.028)	Loss 5.2832e-01 (4.6184e-01)	Acc@1  81.00 ( 84.05)	Acc@5  99.00 ( 99.24)
Test: [ 30/100]	Time  0.017 ( 0.025)	Loss 5.0781e-01 (4.6794e-01)	Acc@1  82.00 ( 84.03)	Acc@5  97.00 ( 99.13)
Test: [ 40/100]	Time  0.022 ( 0.023)	Loss 4.6289e-01 (4.7755e-01)	Acc@1  88.00 ( 84.07)	Acc@5  99.00 ( 99.00)
Test: [ 50/100]	Time  0.017 ( 0.022)	Loss 5.0977e-01 (4.8160e-01)	Acc@1  84.00 ( 84.14)	Acc@5  99.00 ( 99.02)
Test: [ 60/100]	Time  0.021 ( 0.022)	Loss 4.6362e-01 (4.8473e-01)	Acc@1  82.00 ( 84.16)	Acc@5 100.00 ( 99.00)
Test: [ 70/100]	Time  0.021 ( 0.022)	Loss 5.1123e-01 (4.8016e-01)	Acc@1  82.00 ( 84.14)	Acc@5 100.00 ( 99.10)
Test: [ 80/100]	Time  0.020 ( 0.021)	Loss 3.2275e-01 (4.7925e-01)	Acc@1  87.00 ( 84.16)	Acc@5 100.00 ( 99.12)
Test: [ 90/100]	Time  0.021 ( 0.021)	Loss 3.6328e-01 (4.7992e-01)	Acc@1  86.00 ( 84.08)	Acc@5 100.00 ( 99.12)
 * Acc@1 84.170 Acc@5 99.160
### epoch[18] execution time: 18.092934370040894
EPOCH 19
# current learning rate: 0.1
# Switched to train mode...
Epoch: [19][  0/391]	Time  0.219 ( 0.219)	Data  0.178 ( 0.178)	Loss 3.2397e-01 (3.2397e-01)	Acc@1  88.28 ( 88.28)	Acc@5  99.22 ( 99.22)
Epoch: [19][ 10/391]	Time  0.043 ( 0.057)	Data  0.001 ( 0.017)	Loss 3.1934e-01 (3.5467e-01)	Acc@1  89.06 ( 88.14)	Acc@5 100.00 ( 99.57)
Epoch: [19][ 20/391]	Time  0.038 ( 0.049)	Data  0.001 ( 0.009)	Loss 3.4961e-01 (3.3396e-01)	Acc@1  88.28 ( 88.76)	Acc@5  99.22 ( 99.70)
Epoch: [19][ 30/391]	Time  0.042 ( 0.046)	Data  0.001 ( 0.007)	Loss 3.6426e-01 (3.4091e-01)	Acc@1  86.72 ( 88.46)	Acc@5  99.22 ( 99.65)
Epoch: [19][ 40/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.005)	Loss 2.8760e-01 (3.4435e-01)	Acc@1  89.84 ( 88.28)	Acc@5 100.00 ( 99.73)
Epoch: [19][ 50/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.004)	Loss 3.6157e-01 (3.4868e-01)	Acc@1  87.50 ( 88.04)	Acc@5  99.22 ( 99.71)
Epoch: [19][ 60/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.004)	Loss 4.4287e-01 (3.5747e-01)	Acc@1  85.94 ( 87.79)	Acc@5 100.00 ( 99.67)
Epoch: [19][ 70/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.1846e-01 (3.5558e-01)	Acc@1  82.03 ( 87.79)	Acc@5  99.22 ( 99.64)
Epoch: [19][ 80/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.2432e-01 (3.5774e-01)	Acc@1  86.72 ( 87.71)	Acc@5  98.44 ( 99.60)
Epoch: [19][ 90/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.1055e-01 (3.5725e-01)	Acc@1  87.50 ( 87.62)	Acc@5 100.00 ( 99.59)
Epoch: [19][100/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.7510e-01 (3.5589e-01)	Acc@1  82.81 ( 87.57)	Acc@5 100.00 ( 99.61)
Epoch: [19][110/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.0347e-01 (3.5658e-01)	Acc@1  90.62 ( 87.61)	Acc@5 100.00 ( 99.61)
Epoch: [19][120/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.0396e-01 (3.5524e-01)	Acc@1  88.28 ( 87.61)	Acc@5  98.44 ( 99.60)
Epoch: [19][130/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.8110e-01 (3.5440e-01)	Acc@1  85.94 ( 87.61)	Acc@5 100.00 ( 99.61)
Epoch: [19][140/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.0884e-01 (3.5433e-01)	Acc@1  89.84 ( 87.61)	Acc@5  99.22 ( 99.61)
Epoch: [19][150/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4873e-01 (3.5668e-01)	Acc@1  85.16 ( 87.55)	Acc@5 100.00 ( 99.62)
Epoch: [19][160/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0049e-01 (3.5970e-01)	Acc@1  81.25 ( 87.45)	Acc@5 100.00 ( 99.60)
Epoch: [19][170/391]	Time  0.040 ( 0.041)	Data  0.002 ( 0.002)	Loss 3.6328e-01 (3.6264e-01)	Acc@1  87.50 ( 87.28)	Acc@5  99.22 ( 99.60)
Epoch: [19][180/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0918e-01 (3.6371e-01)	Acc@1  86.72 ( 87.29)	Acc@5 100.00 ( 99.59)
Epoch: [19][190/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6987e-01 (3.6503e-01)	Acc@1  86.72 ( 87.16)	Acc@5  99.22 ( 99.58)
Epoch: [19][200/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4961e-01 (3.6533e-01)	Acc@1  85.94 ( 87.12)	Acc@5  99.22 ( 99.59)
Epoch: [19][210/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.7100e-01 (3.6329e-01)	Acc@1  90.62 ( 87.24)	Acc@5  99.22 ( 99.59)
Epoch: [19][220/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4287e-01 (3.6633e-01)	Acc@1  82.03 ( 87.15)	Acc@5 100.00 ( 99.58)
Epoch: [19][230/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.3516e-01 (3.6942e-01)	Acc@1  80.47 ( 87.03)	Acc@5 100.00 ( 99.58)
Epoch: [19][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.8418e-01 (3.6919e-01)	Acc@1  88.28 ( 87.06)	Acc@5  99.22 ( 99.57)
Epoch: [19][250/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6743e-01 (3.6823e-01)	Acc@1  82.81 ( 87.08)	Acc@5 100.00 ( 99.58)
Epoch: [19][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1479e-01 (3.6876e-01)	Acc@1  88.28 ( 87.11)	Acc@5  99.22 ( 99.57)
Epoch: [19][270/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3652e-01 (3.6726e-01)	Acc@1  87.50 ( 87.17)	Acc@5  99.22 ( 99.58)
Epoch: [19][280/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.5557e-01 (3.6708e-01)	Acc@1  81.25 ( 87.16)	Acc@5  99.22 ( 99.57)
Epoch: [19][290/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3164e-01 (3.6691e-01)	Acc@1  85.94 ( 87.20)	Acc@5  98.44 ( 99.57)
Epoch: [19][300/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.5181e-01 (3.6815e-01)	Acc@1  88.28 ( 87.16)	Acc@5  98.44 ( 99.57)
Epoch: [19][310/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.9468e-01 (3.6793e-01)	Acc@1  91.41 ( 87.17)	Acc@5  99.22 ( 99.57)
Epoch: [19][320/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.6973e-01 (3.7038e-01)	Acc@1  82.03 ( 87.08)	Acc@5  99.22 ( 99.57)
Epoch: [19][330/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.0161e-01 (3.7096e-01)	Acc@1  87.50 ( 87.07)	Acc@5  99.22 ( 99.56)
Epoch: [19][340/391]	Time  0.035 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.2007e-01 (3.7147e-01)	Acc@1  89.84 ( 87.03)	Acc@5 100.00 ( 99.56)
Epoch: [19][350/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.2188e-01 (3.7154e-01)	Acc@1  85.94 ( 87.05)	Acc@5 100.00 ( 99.56)
Epoch: [19][360/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.4790e-01 (3.7157e-01)	Acc@1  88.28 ( 87.04)	Acc@5  99.22 ( 99.55)
Epoch: [19][370/391]	Time  0.036 ( 0.041)	Data  0.002 ( 0.001)	Loss 2.5806e-01 (3.7170e-01)	Acc@1  92.97 ( 87.03)	Acc@5 100.00 ( 99.55)
Epoch: [19][380/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.9883e-01 (3.7211e-01)	Acc@1  89.06 ( 87.02)	Acc@5  99.22 ( 99.55)
Epoch: [19][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.1064e-01 (3.7232e-01)	Acc@1  86.25 ( 87.03)	Acc@5  98.75 ( 99.55)
## e[19] optimizer.zero_grad (sum) time: 0.2537715435028076
## e[19]       loss.backward (sum) time: 4.174264669418335
## e[19]      optimizer.step (sum) time: 1.7677865028381348
## epoch[19] training(only) time: 15.958072423934937
# Switched to evaluate mode...
Test: [  0/100]	Time  0.174 ( 0.174)	Loss 4.4580e-01 (4.4580e-01)	Acc@1  87.00 ( 87.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.020 ( 0.034)	Loss 7.1436e-01 (5.0684e-01)	Acc@1  79.00 ( 82.55)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.017 ( 0.028)	Loss 4.9219e-01 (4.9486e-01)	Acc@1  82.00 ( 82.43)	Acc@5  99.00 ( 99.43)
Test: [ 30/100]	Time  0.020 ( 0.025)	Loss 6.1963e-01 (5.0450e-01)	Acc@1  79.00 ( 82.61)	Acc@5  98.00 ( 99.35)
Test: [ 40/100]	Time  0.017 ( 0.024)	Loss 5.3955e-01 (5.0678e-01)	Acc@1  83.00 ( 82.39)	Acc@5  99.00 ( 99.32)
Test: [ 50/100]	Time  0.025 ( 0.023)	Loss 4.7437e-01 (5.0328e-01)	Acc@1  82.00 ( 82.76)	Acc@5 100.00 ( 99.27)
Test: [ 60/100]	Time  0.021 ( 0.023)	Loss 6.0010e-01 (5.1029e-01)	Acc@1  84.00 ( 82.54)	Acc@5  98.00 ( 99.26)
Test: [ 70/100]	Time  0.022 ( 0.022)	Loss 6.8066e-01 (5.0695e-01)	Acc@1  78.00 ( 82.62)	Acc@5  99.00 ( 99.34)
Test: [ 80/100]	Time  0.017 ( 0.022)	Loss 5.3467e-01 (5.0399e-01)	Acc@1  82.00 ( 82.62)	Acc@5 100.00 ( 99.37)
Test: [ 90/100]	Time  0.023 ( 0.021)	Loss 4.4849e-01 (5.0157e-01)	Acc@1  85.00 ( 82.76)	Acc@5 100.00 ( 99.40)
 * Acc@1 82.860 Acc@5 99.410
### epoch[19] execution time: 18.214610815048218
EPOCH 20
# current learning rate: 0.1
# Switched to train mode...
Epoch: [20][  0/391]	Time  0.210 ( 0.210)	Data  0.171 ( 0.171)	Loss 2.7319e-01 (2.7319e-01)	Acc@1  91.41 ( 91.41)	Acc@5 100.00 (100.00)
Epoch: [20][ 10/391]	Time  0.038 ( 0.056)	Data  0.001 ( 0.016)	Loss 2.6904e-01 (3.2204e-01)	Acc@1  89.84 ( 88.07)	Acc@5 100.00 ( 99.43)
Epoch: [20][ 20/391]	Time  0.036 ( 0.048)	Data  0.001 ( 0.009)	Loss 3.1934e-01 (3.2117e-01)	Acc@1  88.28 ( 88.62)	Acc@5  99.22 ( 99.59)
Epoch: [20][ 30/391]	Time  0.040 ( 0.045)	Data  0.001 ( 0.006)	Loss 1.6870e-01 (3.3841e-01)	Acc@1  94.53 ( 88.28)	Acc@5 100.00 ( 99.67)
Epoch: [20][ 40/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.005)	Loss 3.8013e-01 (3.3362e-01)	Acc@1  88.28 ( 88.41)	Acc@5 100.00 ( 99.70)
Epoch: [20][ 50/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 4.0527e-01 (3.3889e-01)	Acc@1  83.59 ( 88.16)	Acc@5 100.00 ( 99.69)
Epoch: [20][ 60/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.004)	Loss 2.6587e-01 (3.3934e-01)	Acc@1  92.19 ( 88.24)	Acc@5  99.22 ( 99.71)
Epoch: [20][ 70/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.2617e-01 (3.3572e-01)	Acc@1  89.06 ( 88.35)	Acc@5  99.22 ( 99.68)
Epoch: [20][ 80/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.0664e-01 (3.3316e-01)	Acc@1  89.84 ( 88.45)	Acc@5 100.00 ( 99.70)
Epoch: [20][ 90/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.3359e-01 (3.3882e-01)	Acc@1  85.16 ( 88.31)	Acc@5 100.00 ( 99.67)
Epoch: [20][100/391]	Time  0.035 ( 0.041)	Data  0.001 ( 0.003)	Loss 4.4775e-01 (3.4495e-01)	Acc@1  85.94 ( 88.09)	Acc@5 100.00 ( 99.66)
Epoch: [20][110/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 3.7744e-01 (3.4861e-01)	Acc@1  86.72 ( 87.89)	Acc@5 100.00 ( 99.63)
Epoch: [20][120/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1616e-01 (3.4900e-01)	Acc@1  89.06 ( 87.82)	Acc@5  99.22 ( 99.63)
Epoch: [20][130/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8086e-01 (3.5433e-01)	Acc@1  87.50 ( 87.70)	Acc@5  99.22 ( 99.62)
Epoch: [20][140/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3579e-01 (3.5513e-01)	Acc@1  85.16 ( 87.67)	Acc@5  99.22 ( 99.61)
Epoch: [20][150/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.5371e-01 (3.5550e-01)	Acc@1  80.47 ( 87.64)	Acc@5  99.22 ( 99.62)
Epoch: [20][160/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.0908e-01 (3.5630e-01)	Acc@1  88.28 ( 87.62)	Acc@5 100.00 ( 99.61)
Epoch: [20][170/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7900e-01 (3.5845e-01)	Acc@1  82.03 ( 87.50)	Acc@5  99.22 ( 99.60)
Epoch: [20][180/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.6782e-01 (3.5981e-01)	Acc@1  88.28 ( 87.46)	Acc@5 100.00 ( 99.60)
Epoch: [20][190/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.8784e-01 (3.5800e-01)	Acc@1  93.75 ( 87.58)	Acc@5 100.00 ( 99.61)
Epoch: [20][200/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7817e-01 (3.5950e-01)	Acc@1  89.06 ( 87.53)	Acc@5  97.66 ( 99.60)
Epoch: [20][210/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7720e-01 (3.5985e-01)	Acc@1  88.28 ( 87.53)	Acc@5  98.44 ( 99.60)
Epoch: [20][220/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4814e-01 (3.6028e-01)	Acc@1  88.28 ( 87.51)	Acc@5 100.00 ( 99.60)
Epoch: [20][230/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6694e-01 (3.5966e-01)	Acc@1  86.72 ( 87.51)	Acc@5 100.00 ( 99.61)
Epoch: [20][240/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3911e-01 (3.5845e-01)	Acc@1  91.41 ( 87.56)	Acc@5 100.00 ( 99.61)
Epoch: [20][250/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3325e-01 (3.5893e-01)	Acc@1  86.72 ( 87.54)	Acc@5 100.00 ( 99.60)
Epoch: [20][260/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0674e-01 (3.5968e-01)	Acc@1  86.72 ( 87.55)	Acc@5  99.22 ( 99.59)
Epoch: [20][270/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6401e-01 (3.5888e-01)	Acc@1  85.94 ( 87.55)	Acc@5 100.00 ( 99.59)
Epoch: [20][280/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1543e-01 (3.5859e-01)	Acc@1  90.62 ( 87.56)	Acc@5  99.22 ( 99.59)
Epoch: [20][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0479e-01 (3.5963e-01)	Acc@1  89.06 ( 87.53)	Acc@5  99.22 ( 99.57)
Epoch: [20][300/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.2017e-01 (3.6016e-01)	Acc@1  84.38 ( 87.52)	Acc@5  99.22 ( 99.57)
Epoch: [20][310/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.6499e-01 (3.6220e-01)	Acc@1  85.94 ( 87.46)	Acc@5 100.00 ( 99.57)
Epoch: [20][320/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.3018e-01 (3.6365e-01)	Acc@1  85.94 ( 87.41)	Acc@5  98.44 ( 99.56)
Epoch: [20][330/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.3921e-01 (3.6532e-01)	Acc@1  86.72 ( 87.37)	Acc@5 100.00 ( 99.56)
Epoch: [20][340/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.6597e-01 (3.6632e-01)	Acc@1  88.28 ( 87.35)	Acc@5  98.44 ( 99.55)
Epoch: [20][350/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.2080e-01 (3.6614e-01)	Acc@1  89.06 ( 87.36)	Acc@5 100.00 ( 99.55)
Epoch: [20][360/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.3276e-01 (3.6654e-01)	Acc@1  89.06 ( 87.36)	Acc@5  98.44 ( 99.55)
Epoch: [20][370/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.0161e-01 (3.6757e-01)	Acc@1  83.59 ( 87.32)	Acc@5  99.22 ( 99.55)
Epoch: [20][380/391]	Time  0.047 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.8306e-01 (3.6784e-01)	Acc@1  88.28 ( 87.31)	Acc@5 100.00 ( 99.55)
Epoch: [20][390/391]	Time  0.029 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.4336e-01 (3.6751e-01)	Acc@1  83.75 ( 87.32)	Acc@5 100.00 ( 99.55)
## e[20] optimizer.zero_grad (sum) time: 0.25009632110595703
## e[20]       loss.backward (sum) time: 4.146662473678589
## e[20]      optimizer.step (sum) time: 1.7620563507080078
## epoch[20] training(only) time: 15.853094100952148
# Switched to evaluate mode...
Test: [  0/100]	Time  0.173 ( 0.173)	Loss 4.8926e-01 (4.8926e-01)	Acc@1  87.00 ( 87.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.018 ( 0.035)	Loss 6.2256e-01 (5.2737e-01)	Acc@1  82.00 ( 82.82)	Acc@5 100.00 ( 99.55)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 7.4170e-01 (5.3849e-01)	Acc@1  72.00 ( 82.33)	Acc@5 100.00 ( 99.33)
Test: [ 30/100]	Time  0.023 ( 0.025)	Loss 7.3926e-01 (5.4762e-01)	Acc@1  78.00 ( 82.42)	Acc@5 100.00 ( 99.35)
Test: [ 40/100]	Time  0.017 ( 0.024)	Loss 6.6895e-01 (5.6333e-01)	Acc@1  81.00 ( 82.27)	Acc@5  98.00 ( 99.24)
Test: [ 50/100]	Time  0.022 ( 0.023)	Loss 5.3174e-01 (5.6179e-01)	Acc@1  81.00 ( 82.06)	Acc@5  97.00 ( 99.18)
Test: [ 60/100]	Time  0.022 ( 0.022)	Loss 6.0400e-01 (5.6402e-01)	Acc@1  81.00 ( 82.00)	Acc@5 100.00 ( 99.18)
Test: [ 70/100]	Time  0.023 ( 0.022)	Loss 5.2930e-01 (5.6632e-01)	Acc@1  83.00 ( 81.86)	Acc@5 100.00 ( 99.20)
Test: [ 80/100]	Time  0.017 ( 0.021)	Loss 4.4653e-01 (5.6325e-01)	Acc@1  83.00 ( 81.81)	Acc@5 100.00 ( 99.22)
Test: [ 90/100]	Time  0.022 ( 0.021)	Loss 4.5728e-01 (5.6404e-01)	Acc@1  84.00 ( 81.65)	Acc@5 100.00 ( 99.24)
 * Acc@1 81.720 Acc@5 99.260
### epoch[20] execution time: 18.082639455795288
EPOCH 21
# current learning rate: 0.1
# Switched to train mode...
Epoch: [21][  0/391]	Time  0.213 ( 0.213)	Data  0.173 ( 0.173)	Loss 4.2163e-01 (4.2163e-01)	Acc@1  85.94 ( 85.94)	Acc@5  98.44 ( 98.44)
Epoch: [21][ 10/391]	Time  0.038 ( 0.055)	Data  0.001 ( 0.017)	Loss 4.0063e-01 (3.3958e-01)	Acc@1  84.38 ( 88.57)	Acc@5 100.00 ( 99.72)
Epoch: [21][ 20/391]	Time  0.040 ( 0.048)	Data  0.001 ( 0.009)	Loss 4.1431e-01 (3.4219e-01)	Acc@1  88.28 ( 88.47)	Acc@5  98.44 ( 99.67)
Epoch: [21][ 30/391]	Time  0.042 ( 0.045)	Data  0.001 ( 0.006)	Loss 1.7126e-01 (3.5088e-01)	Acc@1  95.31 ( 88.23)	Acc@5 100.00 ( 99.45)
Epoch: [21][ 40/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.005)	Loss 4.3188e-01 (3.5339e-01)	Acc@1  83.59 ( 88.00)	Acc@5  99.22 ( 99.39)
Epoch: [21][ 50/391]	Time  0.043 ( 0.043)	Data  0.001 ( 0.004)	Loss 3.5303e-01 (3.5441e-01)	Acc@1  87.50 ( 87.91)	Acc@5  99.22 ( 99.42)
Epoch: [21][ 60/391]	Time  0.040 ( 0.043)	Data  0.002 ( 0.004)	Loss 3.8916e-01 (3.5318e-01)	Acc@1  89.06 ( 88.00)	Acc@5  99.22 ( 99.40)
Epoch: [21][ 70/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.5342e-01 (3.5062e-01)	Acc@1  91.41 ( 88.13)	Acc@5  99.22 ( 99.45)
Epoch: [21][ 80/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.1689e-01 (3.4967e-01)	Acc@1  89.06 ( 88.10)	Acc@5 100.00 ( 99.47)
Epoch: [21][ 90/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.7271e-01 (3.4805e-01)	Acc@1  89.84 ( 88.20)	Acc@5 100.00 ( 99.47)
Epoch: [21][100/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.0908e-01 (3.4769e-01)	Acc@1  92.97 ( 88.20)	Acc@5  99.22 ( 99.47)
Epoch: [21][110/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3447e-01 (3.4587e-01)	Acc@1  88.28 ( 88.24)	Acc@5 100.00 ( 99.48)
Epoch: [21][120/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1201e-01 (3.4673e-01)	Acc@1  91.41 ( 88.29)	Acc@5  99.22 ( 99.49)
Epoch: [21][130/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9062e-01 (3.4651e-01)	Acc@1  85.94 ( 88.26)	Acc@5  98.44 ( 99.50)
Epoch: [21][140/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.7417e-01 (3.4563e-01)	Acc@1  91.41 ( 88.24)	Acc@5 100.00 ( 99.51)
Epoch: [21][150/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.9150e-01 (3.4597e-01)	Acc@1  89.06 ( 88.23)	Acc@5 100.00 ( 99.51)
Epoch: [21][160/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.9346e-01 (3.4592e-01)	Acc@1  89.06 ( 88.26)	Acc@5 100.00 ( 99.50)
Epoch: [21][170/391]	Time  0.045 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3765e-01 (3.4633e-01)	Acc@1  88.28 ( 88.21)	Acc@5 100.00 ( 99.52)
Epoch: [21][180/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9624e-01 (3.4624e-01)	Acc@1  87.50 ( 88.24)	Acc@5 100.00 ( 99.53)
Epoch: [21][190/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8926e-01 (3.4789e-01)	Acc@1  82.81 ( 88.17)	Acc@5  99.22 ( 99.53)
Epoch: [21][200/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6328e-01 (3.4808e-01)	Acc@1  87.50 ( 88.15)	Acc@5  99.22 ( 99.54)
Epoch: [21][210/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2236e-01 (3.4981e-01)	Acc@1  88.28 ( 88.10)	Acc@5  99.22 ( 99.55)
Epoch: [21][220/391]	Time  0.052 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4189e-01 (3.5176e-01)	Acc@1  84.38 ( 88.02)	Acc@5  97.66 ( 99.54)
Epoch: [21][230/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7085e-01 (3.5207e-01)	Acc@1  89.84 ( 88.01)	Acc@5 100.00 ( 99.54)
Epoch: [21][240/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0747e-01 (3.5247e-01)	Acc@1  87.50 ( 88.00)	Acc@5  99.22 ( 99.53)
Epoch: [21][250/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0649e-01 (3.5280e-01)	Acc@1  88.28 ( 88.01)	Acc@5  99.22 ( 99.54)
Epoch: [21][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1968e-01 (3.5487e-01)	Acc@1  88.28 ( 87.92)	Acc@5  99.22 ( 99.54)
Epoch: [21][270/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6060e-01 (3.5466e-01)	Acc@1  88.28 ( 87.94)	Acc@5  98.44 ( 99.54)
Epoch: [21][280/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.8882e-01 (3.5392e-01)	Acc@1  89.84 ( 87.96)	Acc@5 100.00 ( 99.55)
Epoch: [21][290/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.5190e-01 (3.5474e-01)	Acc@1  85.16 ( 87.93)	Acc@5  99.22 ( 99.56)
Epoch: [21][300/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8706e-01 (3.5659e-01)	Acc@1  84.38 ( 87.86)	Acc@5 100.00 ( 99.56)
Epoch: [21][310/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.4131e-01 (3.5684e-01)	Acc@1  86.72 ( 87.84)	Acc@5 100.00 ( 99.56)
Epoch: [21][320/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.9160e-01 (3.5652e-01)	Acc@1  85.94 ( 87.86)	Acc@5  98.44 ( 99.56)
Epoch: [21][330/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.7856e-01 (3.5599e-01)	Acc@1  91.41 ( 87.87)	Acc@5 100.00 ( 99.56)
Epoch: [21][340/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.3838e-01 (3.5613e-01)	Acc@1  89.06 ( 87.84)	Acc@5  99.22 ( 99.56)
Epoch: [21][350/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.4790e-01 (3.5614e-01)	Acc@1  87.50 ( 87.80)	Acc@5 100.00 ( 99.56)
Epoch: [21][360/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.3960e-01 (3.5683e-01)	Acc@1  89.84 ( 87.79)	Acc@5 100.00 ( 99.55)
Epoch: [21][370/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.7744e-01 (3.5744e-01)	Acc@1  86.72 ( 87.78)	Acc@5  99.22 ( 99.56)
Epoch: [21][380/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.6680e-01 (3.5858e-01)	Acc@1  78.91 ( 87.73)	Acc@5 100.00 ( 99.57)
Epoch: [21][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.2979e-01 (3.5884e-01)	Acc@1  81.25 ( 87.71)	Acc@5 100.00 ( 99.57)
## e[21] optimizer.zero_grad (sum) time: 0.2506370544433594
## e[21]       loss.backward (sum) time: 4.237305402755737
## e[21]      optimizer.step (sum) time: 1.718052864074707
## epoch[21] training(only) time: 15.916148662567139
# Switched to evaluate mode...
Test: [  0/100]	Time  0.182 ( 0.182)	Loss 3.9258e-01 (3.9258e-01)	Acc@1  86.00 ( 86.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.017 ( 0.035)	Loss 4.0552e-01 (4.8633e-01)	Acc@1  83.00 ( 83.18)	Acc@5 100.00 ( 99.45)
Test: [ 20/100]	Time  0.023 ( 0.027)	Loss 6.4502e-01 (5.0989e-01)	Acc@1  79.00 ( 83.24)	Acc@5  99.00 ( 99.14)
Test: [ 30/100]	Time  0.020 ( 0.025)	Loss 6.8945e-01 (5.1662e-01)	Acc@1  80.00 ( 83.03)	Acc@5  99.00 ( 99.29)
Test: [ 40/100]	Time  0.017 ( 0.024)	Loss 5.1367e-01 (5.2309e-01)	Acc@1  84.00 ( 82.88)	Acc@5  99.00 ( 99.20)
Test: [ 50/100]	Time  0.017 ( 0.023)	Loss 5.6494e-01 (5.1548e-01)	Acc@1  84.00 ( 83.12)	Acc@5  98.00 ( 99.12)
Test: [ 60/100]	Time  0.021 ( 0.022)	Loss 4.3140e-01 (5.1510e-01)	Acc@1  86.00 ( 83.03)	Acc@5  99.00 ( 99.18)
Test: [ 70/100]	Time  0.017 ( 0.022)	Loss 3.5815e-01 (5.1318e-01)	Acc@1  86.00 ( 82.96)	Acc@5 100.00 ( 99.23)
Test: [ 80/100]	Time  0.025 ( 0.022)	Loss 4.9072e-01 (5.1432e-01)	Acc@1  83.00 ( 83.02)	Acc@5 100.00 ( 99.19)
Test: [ 90/100]	Time  0.021 ( 0.022)	Loss 4.3579e-01 (5.0927e-01)	Acc@1  88.00 ( 83.30)	Acc@5  99.00 ( 99.18)
 * Acc@1 83.330 Acc@5 99.180
### epoch[21] execution time: 18.196319580078125
EPOCH 22
# current learning rate: 0.1
# Switched to train mode...
Epoch: [22][  0/391]	Time  0.216 ( 0.216)	Data  0.176 ( 0.176)	Loss 4.8389e-01 (4.8389e-01)	Acc@1  87.50 ( 87.50)	Acc@5 100.00 (100.00)
Epoch: [22][ 10/391]	Time  0.040 ( 0.056)	Data  0.001 ( 0.017)	Loss 3.1299e-01 (3.2847e-01)	Acc@1  89.84 ( 88.64)	Acc@5 100.00 ( 99.86)
Epoch: [22][ 20/391]	Time  0.040 ( 0.049)	Data  0.001 ( 0.009)	Loss 2.8467e-01 (3.5335e-01)	Acc@1  89.84 ( 87.69)	Acc@5 100.00 ( 99.67)
Epoch: [22][ 30/391]	Time  0.038 ( 0.046)	Data  0.001 ( 0.007)	Loss 2.1948e-01 (3.4112e-01)	Acc@1  92.19 ( 87.75)	Acc@5 100.00 ( 99.75)
Epoch: [22][ 40/391]	Time  0.040 ( 0.045)	Data  0.001 ( 0.005)	Loss 3.8477e-01 (3.3823e-01)	Acc@1  86.72 ( 88.01)	Acc@5  99.22 ( 99.70)
Epoch: [22][ 50/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.004)	Loss 3.8037e-01 (3.4000e-01)	Acc@1  88.28 ( 87.94)	Acc@5  99.22 ( 99.68)
Epoch: [22][ 60/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.004)	Loss 4.4800e-01 (3.4915e-01)	Acc@1  88.28 ( 87.79)	Acc@5 100.00 ( 99.63)
Epoch: [22][ 70/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.1128e-01 (3.4755e-01)	Acc@1  88.28 ( 87.75)	Acc@5 100.00 ( 99.66)
Epoch: [22][ 80/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.4500e-01 (3.4337e-01)	Acc@1  89.84 ( 88.06)	Acc@5 100.00 ( 99.67)
Epoch: [22][ 90/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.3359e-01 (3.4153e-01)	Acc@1  85.16 ( 88.14)	Acc@5  99.22 ( 99.66)
Epoch: [22][100/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.6816e-01 (3.4634e-01)	Acc@1  87.50 ( 87.96)	Acc@5 100.00 ( 99.64)
Epoch: [22][110/391]	Time  0.051 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.0298e-01 (3.4548e-01)	Acc@1  89.06 ( 87.98)	Acc@5  99.22 ( 99.63)
Epoch: [22][120/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.8125e-01 (3.4685e-01)	Acc@1  92.19 ( 87.89)	Acc@5  98.44 ( 99.63)
Epoch: [22][130/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.9404e-01 (3.4990e-01)	Acc@1  86.72 ( 87.79)	Acc@5 100.00 ( 99.62)
Epoch: [22][140/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.8062e-01 (3.5086e-01)	Acc@1  86.72 ( 87.72)	Acc@5 100.00 ( 99.63)
Epoch: [22][150/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1602e-01 (3.5208e-01)	Acc@1  85.16 ( 87.74)	Acc@5  99.22 ( 99.63)
Epoch: [22][160/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0283e-01 (3.5335e-01)	Acc@1  85.94 ( 87.70)	Acc@5  99.22 ( 99.63)
Epoch: [22][170/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1396e-01 (3.5392e-01)	Acc@1  90.62 ( 87.72)	Acc@5 100.00 ( 99.63)
Epoch: [22][180/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3203e-01 (3.5313e-01)	Acc@1  87.50 ( 87.74)	Acc@5  99.22 ( 99.63)
Epoch: [22][190/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5205e-01 (3.5330e-01)	Acc@1  85.94 ( 87.73)	Acc@5 100.00 ( 99.64)
Epoch: [22][200/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1455e-01 (3.5526e-01)	Acc@1  84.38 ( 87.64)	Acc@5  99.22 ( 99.63)
Epoch: [22][210/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0234e-01 (3.5482e-01)	Acc@1  85.16 ( 87.67)	Acc@5  98.44 ( 99.64)
Epoch: [22][220/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7803e-01 (3.5495e-01)	Acc@1  86.72 ( 87.67)	Acc@5  99.22 ( 99.64)
Epoch: [22][230/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3423e-01 (3.5338e-01)	Acc@1  93.75 ( 87.73)	Acc@5  98.44 ( 99.65)
Epoch: [22][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8037e-01 (3.5432e-01)	Acc@1  84.38 ( 87.64)	Acc@5 100.00 ( 99.65)
Epoch: [22][250/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8818e-01 (3.5380e-01)	Acc@1  85.94 ( 87.65)	Acc@5 100.00 ( 99.65)
Epoch: [22][260/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.0444e-01 (3.5253e-01)	Acc@1  89.84 ( 87.69)	Acc@5 100.00 ( 99.65)
Epoch: [22][270/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4521e-01 (3.5381e-01)	Acc@1  88.28 ( 87.63)	Acc@5 100.00 ( 99.64)
Epoch: [22][280/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6279e-01 (3.5395e-01)	Acc@1  85.94 ( 87.67)	Acc@5 100.00 ( 99.64)
Epoch: [22][290/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.5898e-01 (3.5534e-01)	Acc@1  85.94 ( 87.64)	Acc@5  98.44 ( 99.64)
Epoch: [22][300/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9878e-01 (3.5499e-01)	Acc@1  82.03 ( 87.65)	Acc@5 100.00 ( 99.64)
Epoch: [22][310/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.7939e-01 (3.5500e-01)	Acc@1  85.94 ( 87.67)	Acc@5 100.00 ( 99.64)
Epoch: [22][320/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.6597e-01 (3.5434e-01)	Acc@1  85.94 ( 87.70)	Acc@5  99.22 ( 99.64)
Epoch: [22][330/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.5317e-01 (3.5484e-01)	Acc@1  94.53 ( 87.69)	Acc@5 100.00 ( 99.64)
Epoch: [22][340/391]	Time  0.054 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.3547e-01 (3.5570e-01)	Acc@1  90.62 ( 87.66)	Acc@5 100.00 ( 99.64)
Epoch: [22][350/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.0811e-01 (3.5618e-01)	Acc@1  89.84 ( 87.65)	Acc@5  99.22 ( 99.64)
Epoch: [22][360/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.6328e-01 (3.5650e-01)	Acc@1  88.28 ( 87.63)	Acc@5 100.00 ( 99.64)
Epoch: [22][370/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.3604e-01 (3.5646e-01)	Acc@1  87.50 ( 87.64)	Acc@5 100.00 ( 99.65)
Epoch: [22][380/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.0112e-01 (3.5576e-01)	Acc@1  85.16 ( 87.67)	Acc@5 100.00 ( 99.65)
Epoch: [22][390/391]	Time  0.027 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.2744e-01 (3.5587e-01)	Acc@1  80.00 ( 87.66)	Acc@5 100.00 ( 99.65)
## e[22] optimizer.zero_grad (sum) time: 0.24935054779052734
## e[22]       loss.backward (sum) time: 4.221111536026001
## e[22]      optimizer.step (sum) time: 1.7451002597808838
## epoch[22] training(only) time: 15.985851049423218
# Switched to evaluate mode...
Test: [  0/100]	Time  0.172 ( 0.172)	Loss 4.7217e-01 (4.7217e-01)	Acc@1  83.00 ( 83.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.035)	Loss 6.2549e-01 (4.5628e-01)	Acc@1  80.00 ( 83.91)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.021 ( 0.028)	Loss 5.6396e-01 (4.6617e-01)	Acc@1  76.00 ( 83.76)	Acc@5  99.00 ( 99.24)
Test: [ 30/100]	Time  0.020 ( 0.025)	Loss 6.1572e-01 (4.8050e-01)	Acc@1  82.00 ( 83.71)	Acc@5  99.00 ( 99.19)
Test: [ 40/100]	Time  0.022 ( 0.024)	Loss 6.7334e-01 (4.9362e-01)	Acc@1  81.00 ( 83.34)	Acc@5  98.00 ( 99.12)
Test: [ 50/100]	Time  0.021 ( 0.023)	Loss 3.9697e-01 (4.8946e-01)	Acc@1  88.00 ( 83.82)	Acc@5 100.00 ( 99.08)
Test: [ 60/100]	Time  0.022 ( 0.023)	Loss 4.5093e-01 (4.9628e-01)	Acc@1  86.00 ( 83.59)	Acc@5  99.00 ( 99.15)
Test: [ 70/100]	Time  0.021 ( 0.022)	Loss 6.3477e-01 (4.9601e-01)	Acc@1  79.00 ( 83.61)	Acc@5  99.00 ( 99.17)
Test: [ 80/100]	Time  0.021 ( 0.022)	Loss 3.7622e-01 (4.9518e-01)	Acc@1  86.00 ( 83.57)	Acc@5 100.00 ( 99.20)
Test: [ 90/100]	Time  0.018 ( 0.022)	Loss 4.4727e-01 (4.9703e-01)	Acc@1  83.00 ( 83.59)	Acc@5 100.00 ( 99.21)
 * Acc@1 83.560 Acc@5 99.220
### epoch[22] execution time: 18.27253770828247
EPOCH 23
# current learning rate: 0.1
# Switched to train mode...
Epoch: [23][  0/391]	Time  0.212 ( 0.212)	Data  0.163 ( 0.163)	Loss 4.2871e-01 (4.2871e-01)	Acc@1  85.16 ( 85.16)	Acc@5 100.00 (100.00)
Epoch: [23][ 10/391]	Time  0.039 ( 0.056)	Data  0.001 ( 0.016)	Loss 4.0234e-01 (3.6233e-01)	Acc@1  88.28 ( 88.14)	Acc@5  98.44 ( 99.57)
Epoch: [23][ 20/391]	Time  0.040 ( 0.048)	Data  0.001 ( 0.009)	Loss 4.1479e-01 (3.3579e-01)	Acc@1  84.38 ( 88.54)	Acc@5  99.22 ( 99.67)
Epoch: [23][ 30/391]	Time  0.038 ( 0.045)	Data  0.001 ( 0.006)	Loss 2.8174e-01 (3.3635e-01)	Acc@1  89.84 ( 88.48)	Acc@5 100.00 ( 99.72)
Epoch: [23][ 40/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.005)	Loss 3.6035e-01 (3.4680e-01)	Acc@1  88.28 ( 88.22)	Acc@5 100.00 ( 99.66)
Epoch: [23][ 50/391]	Time  0.036 ( 0.043)	Data  0.001 ( 0.004)	Loss 4.1406e-01 (3.4637e-01)	Acc@1  85.94 ( 88.19)	Acc@5  99.22 ( 99.59)
Epoch: [23][ 60/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.004)	Loss 2.4316e-01 (3.4064e-01)	Acc@1  91.41 ( 88.20)	Acc@5 100.00 ( 99.62)
Epoch: [23][ 70/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.5142e-01 (3.4507e-01)	Acc@1  85.94 ( 88.01)	Acc@5  99.22 ( 99.58)
Epoch: [23][ 80/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.5840e-01 (3.4119e-01)	Acc@1  86.72 ( 88.13)	Acc@5 100.00 ( 99.59)
Epoch: [23][ 90/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.1543e-01 (3.3905e-01)	Acc@1  90.62 ( 88.17)	Acc@5  99.22 ( 99.62)
Epoch: [23][100/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.0796e-01 (3.4171e-01)	Acc@1  85.16 ( 88.08)	Acc@5  99.22 ( 99.61)
Epoch: [23][110/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.6841e-01 (3.4195e-01)	Acc@1  89.84 ( 88.06)	Acc@5 100.00 ( 99.60)
Epoch: [23][120/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7891e-01 (3.4319e-01)	Acc@1  85.94 ( 88.07)	Acc@5 100.00 ( 99.60)
Epoch: [23][130/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3594e-01 (3.4659e-01)	Acc@1  88.28 ( 87.98)	Acc@5 100.00 ( 99.59)
Epoch: [23][140/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3887e-01 (3.4674e-01)	Acc@1  89.06 ( 87.92)	Acc@5 100.00 ( 99.61)
Epoch: [23][150/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.0996e-01 (3.4545e-01)	Acc@1  91.41 ( 87.96)	Acc@5 100.00 ( 99.60)
Epoch: [23][160/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7598e-01 (3.4440e-01)	Acc@1  85.94 ( 87.93)	Acc@5 100.00 ( 99.62)
Epoch: [23][170/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1592e-01 (3.4534e-01)	Acc@1  91.41 ( 87.96)	Acc@5  99.22 ( 99.60)
Epoch: [23][180/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6938e-01 (3.4593e-01)	Acc@1  87.50 ( 87.96)	Acc@5  99.22 ( 99.60)
Epoch: [23][190/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6572e-01 (3.4517e-01)	Acc@1  86.72 ( 88.00)	Acc@5 100.00 ( 99.61)
Epoch: [23][200/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5962e-01 (3.4425e-01)	Acc@1  83.59 ( 88.01)	Acc@5 100.00 ( 99.61)
Epoch: [23][210/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7500e-01 (3.4443e-01)	Acc@1  83.59 ( 87.99)	Acc@5 100.00 ( 99.62)
Epoch: [23][220/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7402e-01 (3.4415e-01)	Acc@1  84.38 ( 88.02)	Acc@5 100.00 ( 99.63)
Epoch: [23][230/391]	Time  0.046 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4189e-01 (3.4398e-01)	Acc@1  86.72 ( 88.00)	Acc@5 100.00 ( 99.64)
Epoch: [23][240/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.5586e-01 (3.4222e-01)	Acc@1  91.41 ( 88.09)	Acc@5  99.22 ( 99.64)
Epoch: [23][250/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.1741e-01 (3.4100e-01)	Acc@1  90.62 ( 88.12)	Acc@5 100.00 ( 99.64)
Epoch: [23][260/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.7524e-01 (3.4100e-01)	Acc@1  88.28 ( 88.14)	Acc@5 100.00 ( 99.63)
Epoch: [23][270/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.3433e-01 (3.4087e-01)	Acc@1  86.72 ( 88.14)	Acc@5 100.00 ( 99.64)
Epoch: [23][280/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.3179e-01 (3.4118e-01)	Acc@1  88.28 ( 88.16)	Acc@5 100.00 ( 99.64)
Epoch: [23][290/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.0356e-01 (3.4156e-01)	Acc@1  83.59 ( 88.13)	Acc@5 100.00 ( 99.64)
Epoch: [23][300/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.8379e-01 (3.4152e-01)	Acc@1  87.50 ( 88.10)	Acc@5 100.00 ( 99.65)
Epoch: [23][310/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.5610e-01 (3.4100e-01)	Acc@1  91.41 ( 88.10)	Acc@5 100.00 ( 99.65)
Epoch: [23][320/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.4500e-01 (3.4193e-01)	Acc@1  94.53 ( 88.12)	Acc@5 100.00 ( 99.65)
Epoch: [23][330/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.7915e-01 (3.4306e-01)	Acc@1  83.59 ( 88.08)	Acc@5 100.00 ( 99.65)
Epoch: [23][340/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.2520e-01 (3.4363e-01)	Acc@1  87.50 ( 88.05)	Acc@5  99.22 ( 99.65)
Epoch: [23][350/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.0063e-01 (3.4368e-01)	Acc@1  84.38 ( 88.05)	Acc@5  98.44 ( 99.65)
Epoch: [23][360/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.1958e-01 (3.4335e-01)	Acc@1  89.06 ( 88.07)	Acc@5 100.00 ( 99.65)
Epoch: [23][370/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.6265e-01 (3.4470e-01)	Acc@1  85.16 ( 88.04)	Acc@5  99.22 ( 99.64)
Epoch: [23][380/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.7339e-01 (3.4581e-01)	Acc@1  87.50 ( 88.01)	Acc@5 100.00 ( 99.65)
Epoch: [23][390/391]	Time  0.026 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.4619e-01 (3.4579e-01)	Acc@1  88.75 ( 87.98)	Acc@5 100.00 ( 99.65)
## e[23] optimizer.zero_grad (sum) time: 0.2496497631072998
## e[23]       loss.backward (sum) time: 4.20942759513855
## e[23]      optimizer.step (sum) time: 1.7524189949035645
## epoch[23] training(only) time: 15.93984603881836
# Switched to evaluate mode...
Test: [  0/100]	Time  0.164 ( 0.164)	Loss 4.4507e-01 (4.4507e-01)	Acc@1  90.00 ( 90.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.023 ( 0.033)	Loss 4.4702e-01 (4.1443e-01)	Acc@1  84.00 ( 87.18)	Acc@5  99.00 ( 99.45)
Test: [ 20/100]	Time  0.021 ( 0.027)	Loss 4.8438e-01 (4.2512e-01)	Acc@1  78.00 ( 86.48)	Acc@5  99.00 ( 99.14)
Test: [ 30/100]	Time  0.022 ( 0.025)	Loss 6.1816e-01 (4.3999e-01)	Acc@1  82.00 ( 86.06)	Acc@5  98.00 ( 99.19)
Test: [ 40/100]	Time  0.021 ( 0.024)	Loss 5.5762e-01 (4.4071e-01)	Acc@1  83.00 ( 86.00)	Acc@5  98.00 ( 99.17)
Test: [ 50/100]	Time  0.021 ( 0.023)	Loss 3.7598e-01 (4.3098e-01)	Acc@1  85.00 ( 86.29)	Acc@5  98.00 ( 99.16)
Test: [ 60/100]	Time  0.023 ( 0.022)	Loss 4.1455e-01 (4.2837e-01)	Acc@1  86.00 ( 86.16)	Acc@5 100.00 ( 99.26)
Test: [ 70/100]	Time  0.023 ( 0.022)	Loss 5.2100e-01 (4.2746e-01)	Acc@1  80.00 ( 85.99)	Acc@5  99.00 ( 99.28)
Test: [ 80/100]	Time  0.019 ( 0.022)	Loss 3.1348e-01 (4.2797e-01)	Acc@1  88.00 ( 85.86)	Acc@5 100.00 ( 99.28)
Test: [ 90/100]	Time  0.017 ( 0.022)	Loss 3.5718e-01 (4.2979e-01)	Acc@1  86.00 ( 85.77)	Acc@5  98.00 ( 99.26)
 * Acc@1 85.650 Acc@5 99.300
### epoch[23] execution time: 18.20075511932373
EPOCH 24
# current learning rate: 0.1
# Switched to train mode...
Epoch: [24][  0/391]	Time  0.212 ( 0.212)	Data  0.170 ( 0.170)	Loss 2.5610e-01 (2.5610e-01)	Acc@1  89.06 ( 89.06)	Acc@5 100.00 (100.00)
Epoch: [24][ 10/391]	Time  0.038 ( 0.056)	Data  0.001 ( 0.016)	Loss 4.6289e-01 (3.5153e-01)	Acc@1  89.06 ( 88.00)	Acc@5  97.66 ( 99.36)
Epoch: [24][ 20/391]	Time  0.041 ( 0.049)	Data  0.001 ( 0.009)	Loss 4.7339e-01 (3.5102e-01)	Acc@1  81.25 ( 87.65)	Acc@5  99.22 ( 99.48)
Epoch: [24][ 30/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.006)	Loss 3.2617e-01 (3.4575e-01)	Acc@1  89.84 ( 87.70)	Acc@5 100.00 ( 99.52)
Epoch: [24][ 40/391]	Time  0.037 ( 0.044)	Data  0.001 ( 0.005)	Loss 2.1863e-01 (3.3260e-01)	Acc@1  93.75 ( 88.11)	Acc@5  99.22 ( 99.60)
Epoch: [24][ 50/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.004)	Loss 3.2837e-01 (3.3219e-01)	Acc@1  89.84 ( 88.16)	Acc@5 100.00 ( 99.63)
Epoch: [24][ 60/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.004)	Loss 3.4375e-01 (3.3838e-01)	Acc@1  89.06 ( 88.11)	Acc@5 100.00 ( 99.67)
Epoch: [24][ 70/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.5376e-01 (3.4605e-01)	Acc@1  89.06 ( 87.92)	Acc@5  98.44 ( 99.65)
Epoch: [24][ 80/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.7368e-01 (3.4267e-01)	Acc@1  89.84 ( 88.09)	Acc@5  99.22 ( 99.64)
Epoch: [24][ 90/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.0176e-01 (3.4289e-01)	Acc@1  87.50 ( 88.14)	Acc@5 100.00 ( 99.65)
Epoch: [24][100/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.003)	Loss 2.7661e-01 (3.3649e-01)	Acc@1  90.62 ( 88.38)	Acc@5 100.00 ( 99.65)
Epoch: [24][110/391]	Time  0.045 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5327e-01 (3.3328e-01)	Acc@1  89.06 ( 88.46)	Acc@5  99.22 ( 99.65)
Epoch: [24][120/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0137e-01 (3.3262e-01)	Acc@1  83.59 ( 88.47)	Acc@5  99.22 ( 99.64)
Epoch: [24][130/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3154e-01 (3.3245e-01)	Acc@1  86.72 ( 88.56)	Acc@5 100.00 ( 99.61)
Epoch: [24][140/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.2534e-01 (3.3271e-01)	Acc@1  92.97 ( 88.60)	Acc@5 100.00 ( 99.63)
Epoch: [24][150/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.9590e-01 (3.3221e-01)	Acc@1  89.06 ( 88.58)	Acc@5 100.00 ( 99.65)
Epoch: [24][160/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2554e-01 (3.3584e-01)	Acc@1  84.38 ( 88.44)	Acc@5  99.22 ( 99.64)
Epoch: [24][170/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.9468e-01 (3.3644e-01)	Acc@1  91.41 ( 88.39)	Acc@5 100.00 ( 99.65)
Epoch: [24][180/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.5977e-01 (3.3801e-01)	Acc@1  91.41 ( 88.35)	Acc@5 100.00 ( 99.64)
Epoch: [24][190/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.3975e-01 (3.3844e-01)	Acc@1  91.41 ( 88.35)	Acc@5 100.00 ( 99.62)
Epoch: [24][200/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.9517e-01 (3.3861e-01)	Acc@1  89.06 ( 88.35)	Acc@5 100.00 ( 99.62)
Epoch: [24][210/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.7612e-01 (3.3875e-01)	Acc@1  88.28 ( 88.30)	Acc@5 100.00 ( 99.63)
Epoch: [24][220/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6060e-01 (3.3694e-01)	Acc@1  86.72 ( 88.36)	Acc@5 100.00 ( 99.63)
Epoch: [24][230/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.2974e-01 (3.3810e-01)	Acc@1  90.62 ( 88.33)	Acc@5 100.00 ( 99.63)
Epoch: [24][240/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5693e-01 (3.3835e-01)	Acc@1  85.16 ( 88.30)	Acc@5 100.00 ( 99.63)
Epoch: [24][250/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4751e-01 (3.3949e-01)	Acc@1  83.59 ( 88.25)	Acc@5 100.00 ( 99.63)
Epoch: [24][260/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.6221e-01 (3.3824e-01)	Acc@1  90.62 ( 88.31)	Acc@5 100.00 ( 99.63)
Epoch: [24][270/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7671e-01 (3.3897e-01)	Acc@1  85.16 ( 88.26)	Acc@5 100.00 ( 99.62)
Epoch: [24][280/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.9126e-01 (3.3853e-01)	Acc@1  88.28 ( 88.25)	Acc@5 100.00 ( 99.63)
Epoch: [24][290/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.7563e-01 (3.3768e-01)	Acc@1  91.41 ( 88.28)	Acc@5 100.00 ( 99.63)
Epoch: [24][300/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.3433e-01 (3.3892e-01)	Acc@1  85.94 ( 88.24)	Acc@5  99.22 ( 99.63)
Epoch: [24][310/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.2876e-01 (3.3706e-01)	Acc@1  91.41 ( 88.27)	Acc@5 100.00 ( 99.64)
Epoch: [24][320/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.4912e-01 (3.3738e-01)	Acc@1  87.50 ( 88.27)	Acc@5  99.22 ( 99.63)
Epoch: [24][330/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.0547e-01 (3.3968e-01)	Acc@1  82.81 ( 88.22)	Acc@5  98.44 ( 99.62)
Epoch: [24][340/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.4863e-01 (3.4183e-01)	Acc@1  86.72 ( 88.15)	Acc@5  99.22 ( 99.62)
Epoch: [24][350/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.0005e-01 (3.4173e-01)	Acc@1  89.06 ( 88.15)	Acc@5 100.00 ( 99.61)
Epoch: [24][360/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.6963e-01 (3.4225e-01)	Acc@1  87.50 ( 88.15)	Acc@5  99.22 ( 99.61)
Epoch: [24][370/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.0576e-01 (3.4164e-01)	Acc@1  83.59 ( 88.19)	Acc@5 100.00 ( 99.61)
Epoch: [24][380/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.4814e-01 (3.4184e-01)	Acc@1  84.38 ( 88.18)	Acc@5 100.00 ( 99.62)
Epoch: [24][390/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.1626e-01 (3.4173e-01)	Acc@1  85.00 ( 88.18)	Acc@5  97.50 ( 99.62)
## e[24] optimizer.zero_grad (sum) time: 0.25151801109313965
## e[24]       loss.backward (sum) time: 4.180248022079468
## e[24]      optimizer.step (sum) time: 1.7716751098632812
## epoch[24] training(only) time: 15.91917634010315
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 4.9438e-01 (4.9438e-01)	Acc@1  85.00 ( 85.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.017 ( 0.032)	Loss 5.2295e-01 (5.1709e-01)	Acc@1  87.00 ( 83.45)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.023 ( 0.026)	Loss 6.1182e-01 (5.0951e-01)	Acc@1  80.00 ( 83.43)	Acc@5  99.00 ( 99.38)
Test: [ 30/100]	Time  0.020 ( 0.024)	Loss 5.5859e-01 (5.1185e-01)	Acc@1  82.00 ( 83.94)	Acc@5  99.00 ( 99.32)
Test: [ 40/100]	Time  0.021 ( 0.023)	Loss 4.4824e-01 (5.2424e-01)	Acc@1  84.00 ( 83.63)	Acc@5  98.00 ( 99.24)
Test: [ 50/100]	Time  0.017 ( 0.022)	Loss 4.0454e-01 (5.1447e-01)	Acc@1  87.00 ( 83.98)	Acc@5  97.00 ( 99.25)
Test: [ 60/100]	Time  0.019 ( 0.022)	Loss 4.1016e-01 (5.1997e-01)	Acc@1  85.00 ( 83.69)	Acc@5 100.00 ( 99.28)
Test: [ 70/100]	Time  0.017 ( 0.022)	Loss 5.6348e-01 (5.1696e-01)	Acc@1  86.00 ( 83.79)	Acc@5  99.00 ( 99.28)
Test: [ 80/100]	Time  0.017 ( 0.021)	Loss 5.1221e-01 (5.1571e-01)	Acc@1  82.00 ( 83.68)	Acc@5 100.00 ( 99.32)
Test: [ 90/100]	Time  0.022 ( 0.021)	Loss 4.4702e-01 (5.1125e-01)	Acc@1  84.00 ( 83.64)	Acc@5 100.00 ( 99.32)
 * Acc@1 83.560 Acc@5 99.320
### epoch[24] execution time: 18.15934133529663
EPOCH 25
# current learning rate: 0.1
# Switched to train mode...
Epoch: [25][  0/391]	Time  0.208 ( 0.208)	Data  0.167 ( 0.167)	Loss 2.4463e-01 (2.4463e-01)	Acc@1  92.19 ( 92.19)	Acc@5 100.00 (100.00)
Epoch: [25][ 10/391]	Time  0.041 ( 0.055)	Data  0.001 ( 0.016)	Loss 4.0894e-01 (3.0807e-01)	Acc@1  87.50 ( 89.13)	Acc@5 100.00 ( 99.79)
Epoch: [25][ 20/391]	Time  0.040 ( 0.048)	Data  0.001 ( 0.009)	Loss 3.7744e-01 (3.0677e-01)	Acc@1  81.25 ( 89.06)	Acc@5 100.00 ( 99.85)
Epoch: [25][ 30/391]	Time  0.040 ( 0.045)	Data  0.001 ( 0.006)	Loss 2.6050e-01 (3.1007e-01)	Acc@1  91.41 ( 89.19)	Acc@5  99.22 ( 99.77)
Epoch: [25][ 40/391]	Time  0.044 ( 0.044)	Data  0.001 ( 0.005)	Loss 3.1470e-01 (3.1320e-01)	Acc@1  89.06 ( 89.12)	Acc@5  99.22 ( 99.75)
Epoch: [25][ 50/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.004)	Loss 3.7964e-01 (3.1845e-01)	Acc@1  88.28 ( 89.06)	Acc@5 100.00 ( 99.72)
Epoch: [25][ 60/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.004)	Loss 3.7402e-01 (3.2148e-01)	Acc@1  85.16 ( 88.97)	Acc@5 100.00 ( 99.72)
Epoch: [25][ 70/391]	Time  0.043 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.0625e-01 (3.2283e-01)	Acc@1  82.03 ( 88.90)	Acc@5  99.22 ( 99.67)
Epoch: [25][ 80/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.4961e-01 (3.2737e-01)	Acc@1  88.28 ( 88.71)	Acc@5 100.00 ( 99.68)
Epoch: [25][ 90/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.0166e-01 (3.2653e-01)	Acc@1  92.19 ( 88.68)	Acc@5 100.00 ( 99.68)
Epoch: [25][100/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.2593e-01 (3.2835e-01)	Acc@1  87.50 ( 88.64)	Acc@5 100.00 ( 99.67)
Epoch: [25][110/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.0146e-01 (3.3155e-01)	Acc@1  82.81 ( 88.50)	Acc@5 100.00 ( 99.68)
Epoch: [25][120/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9355e-01 (3.2949e-01)	Acc@1  88.28 ( 88.63)	Acc@5  98.44 ( 99.67)
Epoch: [25][130/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.8833e-01 (3.2830e-01)	Acc@1  89.84 ( 88.70)	Acc@5 100.00 ( 99.69)
Epoch: [25][140/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3032e-01 (3.2649e-01)	Acc@1  92.97 ( 88.77)	Acc@5  99.22 ( 99.70)
Epoch: [25][150/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5303e-01 (3.2907e-01)	Acc@1  86.72 ( 88.63)	Acc@5 100.00 ( 99.69)
Epoch: [25][160/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2397e-01 (3.3209e-01)	Acc@1  86.72 ( 88.49)	Acc@5 100.00 ( 99.69)
Epoch: [25][170/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6597e-01 (3.3307e-01)	Acc@1  85.16 ( 88.37)	Acc@5 100.00 ( 99.69)
Epoch: [25][180/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.1814e-01 (3.3328e-01)	Acc@1  95.31 ( 88.41)	Acc@5 100.00 ( 99.68)
Epoch: [25][190/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.0103e-01 (3.3216e-01)	Acc@1  89.84 ( 88.45)	Acc@5 100.00 ( 99.69)
Epoch: [25][200/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3203e-01 (3.3249e-01)	Acc@1  87.50 ( 88.47)	Acc@5 100.00 ( 99.69)
Epoch: [25][210/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1641e-01 (3.3241e-01)	Acc@1  87.50 ( 88.45)	Acc@5  99.22 ( 99.69)
Epoch: [25][220/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0283e-01 (3.3220e-01)	Acc@1  85.16 ( 88.45)	Acc@5  99.22 ( 99.70)
Epoch: [25][230/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3936e-01 (3.3283e-01)	Acc@1  87.50 ( 88.40)	Acc@5 100.00 ( 99.70)
Epoch: [25][240/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.002)	Loss 3.7207e-01 (3.3383e-01)	Acc@1  85.94 ( 88.40)	Acc@5 100.00 ( 99.69)
Epoch: [25][250/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.002)	Loss 3.8452e-01 (3.3364e-01)	Acc@1  85.16 ( 88.40)	Acc@5 100.00 ( 99.69)
Epoch: [25][260/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 4.0771e-01 (3.3518e-01)	Acc@1  86.72 ( 88.35)	Acc@5  99.22 ( 99.69)
Epoch: [25][270/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 3.3643e-01 (3.3563e-01)	Acc@1  85.94 ( 88.32)	Acc@5 100.00 ( 99.69)
Epoch: [25][280/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 2.3816e-01 (3.3487e-01)	Acc@1  89.06 ( 88.33)	Acc@5 100.00 ( 99.68)
Epoch: [25][290/391]	Time  0.043 ( 0.040)	Data  0.001 ( 0.002)	Loss 4.0942e-01 (3.3518e-01)	Acc@1  84.38 ( 88.31)	Acc@5  99.22 ( 99.68)
Epoch: [25][300/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.8208e-01 (3.3568e-01)	Acc@1  86.72 ( 88.30)	Acc@5  99.22 ( 99.68)
Epoch: [25][310/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.5376e-01 (3.3678e-01)	Acc@1  86.72 ( 88.26)	Acc@5 100.00 ( 99.67)
Epoch: [25][320/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.0635e-01 (3.3735e-01)	Acc@1  85.16 ( 88.22)	Acc@5  99.22 ( 99.67)
Epoch: [25][330/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.9697e-01 (3.3814e-01)	Acc@1  88.28 ( 88.19)	Acc@5  99.22 ( 99.67)
Epoch: [25][340/391]	Time  0.044 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.3789e-01 (3.3801e-01)	Acc@1  90.62 ( 88.23)	Acc@5  99.22 ( 99.66)
Epoch: [25][350/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.7461e-01 (3.3918e-01)	Acc@1  87.50 ( 88.18)	Acc@5  99.22 ( 99.66)
Epoch: [25][360/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.2700e-01 (3.3949e-01)	Acc@1  85.94 ( 88.19)	Acc@5 100.00 ( 99.66)
Epoch: [25][370/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.1309e-01 (3.4049e-01)	Acc@1  88.28 ( 88.12)	Acc@5  98.44 ( 99.66)
Epoch: [25][380/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.1104e-01 (3.4079e-01)	Acc@1  89.06 ( 88.12)	Acc@5 100.00 ( 99.65)
Epoch: [25][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.1958e-01 (3.4120e-01)	Acc@1  91.25 ( 88.11)	Acc@5 100.00 ( 99.65)
## e[25] optimizer.zero_grad (sum) time: 0.2534143924713135
## e[25]       loss.backward (sum) time: 4.153951406478882
## e[25]      optimizer.step (sum) time: 1.7659263610839844
## epoch[25] training(only) time: 15.816335916519165
# Switched to evaluate mode...
Test: [  0/100]	Time  0.170 ( 0.170)	Loss 5.8545e-01 (5.8545e-01)	Acc@1  82.00 ( 82.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.021 ( 0.034)	Loss 5.4834e-01 (4.7681e-01)	Acc@1  80.00 ( 85.18)	Acc@5  99.00 ( 99.18)
Test: [ 20/100]	Time  0.018 ( 0.027)	Loss 6.5527e-01 (4.7289e-01)	Acc@1  79.00 ( 85.52)	Acc@5 100.00 ( 99.05)
Test: [ 30/100]	Time  0.020 ( 0.024)	Loss 5.3906e-01 (4.8924e-01)	Acc@1  84.00 ( 85.03)	Acc@5  99.00 ( 99.10)
Test: [ 40/100]	Time  0.019 ( 0.023)	Loss 5.5127e-01 (4.8772e-01)	Acc@1  81.00 ( 84.56)	Acc@5  99.00 ( 99.12)
Test: [ 50/100]	Time  0.017 ( 0.023)	Loss 3.3960e-01 (4.7803e-01)	Acc@1  87.00 ( 84.80)	Acc@5  99.00 ( 99.08)
Test: [ 60/100]	Time  0.020 ( 0.022)	Loss 5.2100e-01 (4.8486e-01)	Acc@1  88.00 ( 84.66)	Acc@5  99.00 ( 99.15)
Test: [ 70/100]	Time  0.017 ( 0.022)	Loss 5.6689e-01 (4.8143e-01)	Acc@1  83.00 ( 84.59)	Acc@5 100.00 ( 99.18)
Test: [ 80/100]	Time  0.017 ( 0.021)	Loss 3.7646e-01 (4.7707e-01)	Acc@1  87.00 ( 84.69)	Acc@5  99.00 ( 99.19)
Test: [ 90/100]	Time  0.019 ( 0.021)	Loss 3.3911e-01 (4.7882e-01)	Acc@1  89.00 ( 84.56)	Acc@5 100.00 ( 99.19)
 * Acc@1 84.520 Acc@5 99.190
### epoch[25] execution time: 18.01496648788452
EPOCH 26
# current learning rate: 0.1
# Switched to train mode...
Epoch: [26][  0/391]	Time  0.218 ( 0.218)	Data  0.175 ( 0.175)	Loss 2.5049e-01 (2.5049e-01)	Acc@1  90.62 ( 90.62)	Acc@5 100.00 (100.00)
Epoch: [26][ 10/391]	Time  0.039 ( 0.056)	Data  0.001 ( 0.017)	Loss 4.0820e-01 (2.9530e-01)	Acc@1  86.72 ( 88.78)	Acc@5 100.00 ( 99.86)
Epoch: [26][ 20/391]	Time  0.042 ( 0.049)	Data  0.001 ( 0.009)	Loss 1.6394e-01 (2.8792e-01)	Acc@1  93.75 ( 89.47)	Acc@5 100.00 ( 99.85)
Epoch: [26][ 30/391]	Time  0.038 ( 0.046)	Data  0.001 ( 0.007)	Loss 2.6660e-01 (2.9251e-01)	Acc@1  92.19 ( 89.64)	Acc@5  99.22 ( 99.82)
Epoch: [26][ 40/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.005)	Loss 3.1641e-01 (2.9441e-01)	Acc@1  89.84 ( 89.65)	Acc@5 100.00 ( 99.77)
Epoch: [26][ 50/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.004)	Loss 4.3262e-01 (2.9925e-01)	Acc@1  85.94 ( 89.68)	Acc@5 100.00 ( 99.80)
Epoch: [26][ 60/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.004)	Loss 2.3816e-01 (3.0386e-01)	Acc@1  91.41 ( 89.42)	Acc@5 100.00 ( 99.80)
Epoch: [26][ 70/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.003)	Loss 5.0146e-01 (3.0884e-01)	Acc@1  85.16 ( 89.32)	Acc@5  99.22 ( 99.79)
Epoch: [26][ 80/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.3423e-01 (3.1242e-01)	Acc@1  89.06 ( 89.23)	Acc@5 100.00 ( 99.80)
Epoch: [26][ 90/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.8452e-01 (3.1482e-01)	Acc@1  87.50 ( 89.18)	Acc@5  98.44 ( 99.76)
Epoch: [26][100/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.5693e-01 (3.1751e-01)	Acc@1  87.50 ( 89.08)	Acc@5 100.00 ( 99.74)
Epoch: [26][110/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.3547e-01 (3.1676e-01)	Acc@1  92.19 ( 89.03)	Acc@5 100.00 ( 99.74)
Epoch: [26][120/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.1494e-01 (3.1976e-01)	Acc@1  89.84 ( 88.94)	Acc@5  99.22 ( 99.74)
Epoch: [26][130/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.8931e-01 (3.1733e-01)	Acc@1  88.28 ( 89.03)	Acc@5 100.00 ( 99.75)
Epoch: [26][140/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6353e-01 (3.1694e-01)	Acc@1  85.16 ( 89.00)	Acc@5 100.00 ( 99.76)
Epoch: [26][150/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8306e-01 (3.1835e-01)	Acc@1  88.28 ( 88.90)	Acc@5 100.00 ( 99.76)
Epoch: [26][160/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5376e-01 (3.2090e-01)	Acc@1  87.50 ( 88.84)	Acc@5 100.00 ( 99.76)
Epoch: [26][170/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4751e-01 (3.2299e-01)	Acc@1  82.03 ( 88.77)	Acc@5  99.22 ( 99.74)
Epoch: [26][180/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1650e-01 (3.2650e-01)	Acc@1  83.59 ( 88.61)	Acc@5  99.22 ( 99.72)
Epoch: [26][190/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2983e-01 (3.2632e-01)	Acc@1  88.28 ( 88.60)	Acc@5 100.00 ( 99.72)
Epoch: [26][200/391]	Time  0.045 ( 0.041)	Data  0.002 ( 0.002)	Loss 3.9404e-01 (3.2818e-01)	Acc@1  83.59 ( 88.57)	Acc@5 100.00 ( 99.73)
Epoch: [26][210/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.8784e-01 (3.2867e-01)	Acc@1  89.84 ( 88.54)	Acc@5 100.00 ( 99.73)
Epoch: [26][220/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.3071e-01 (3.3084e-01)	Acc@1  92.19 ( 88.44)	Acc@5 100.00 ( 99.72)
Epoch: [26][230/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9707e-01 (3.3213e-01)	Acc@1  86.72 ( 88.41)	Acc@5  99.22 ( 99.71)
Epoch: [26][240/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6230e-01 (3.3122e-01)	Acc@1  85.94 ( 88.42)	Acc@5  99.22 ( 99.70)
Epoch: [26][250/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.4609e-01 (3.3010e-01)	Acc@1  91.41 ( 88.46)	Acc@5  99.22 ( 99.69)
Epoch: [26][260/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.8589e-01 (3.2975e-01)	Acc@1  90.62 ( 88.51)	Acc@5  99.22 ( 99.69)
Epoch: [26][270/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.9126e-01 (3.2969e-01)	Acc@1  89.06 ( 88.52)	Acc@5 100.00 ( 99.69)
Epoch: [26][280/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2178e-01 (3.2976e-01)	Acc@1  89.84 ( 88.52)	Acc@5 100.00 ( 99.69)
Epoch: [26][290/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1494e-01 (3.2999e-01)	Acc@1  87.50 ( 88.52)	Acc@5 100.00 ( 99.68)
Epoch: [26][300/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.0396e-01 (3.3116e-01)	Acc@1  89.06 ( 88.48)	Acc@5 100.00 ( 99.69)
Epoch: [26][310/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2529e-01 (3.3223e-01)	Acc@1  85.94 ( 88.43)	Acc@5 100.00 ( 99.68)
Epoch: [26][320/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3799e-01 (3.3359e-01)	Acc@1  83.59 ( 88.38)	Acc@5  99.22 ( 99.67)
Epoch: [26][330/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.9126e-01 (3.3227e-01)	Acc@1  92.19 ( 88.41)	Acc@5  99.22 ( 99.68)
Epoch: [26][340/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.7939e-01 (3.3259e-01)	Acc@1  87.50 ( 88.40)	Acc@5 100.00 ( 99.68)
Epoch: [26][350/391]	Time  0.045 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.6230e-01 (3.3218e-01)	Acc@1  88.28 ( 88.44)	Acc@5 100.00 ( 99.68)
Epoch: [26][360/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.8110e-01 (3.3260e-01)	Acc@1  86.72 ( 88.42)	Acc@5  99.22 ( 99.68)
Epoch: [26][370/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.1479e-01 (3.3302e-01)	Acc@1  85.94 ( 88.41)	Acc@5 100.00 ( 99.68)
Epoch: [26][380/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.9199e-01 (3.3298e-01)	Acc@1  90.62 ( 88.42)	Acc@5 100.00 ( 99.68)
Epoch: [26][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.9761e-01 (3.3436e-01)	Acc@1  88.75 ( 88.37)	Acc@5 100.00 ( 99.69)
## e[26] optimizer.zero_grad (sum) time: 0.24928879737854004
## e[26]       loss.backward (sum) time: 4.190961122512817
## e[26]      optimizer.step (sum) time: 1.7662267684936523
## epoch[26] training(only) time: 15.958192348480225
# Switched to evaluate mode...
Test: [  0/100]	Time  0.167 ( 0.167)	Loss 4.8584e-01 (4.8584e-01)	Acc@1  83.00 ( 83.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.016 ( 0.033)	Loss 4.8486e-01 (4.8795e-01)	Acc@1  83.00 ( 83.64)	Acc@5 100.00 ( 99.27)
Test: [ 20/100]	Time  0.020 ( 0.027)	Loss 6.0742e-01 (5.0349e-01)	Acc@1  84.00 ( 83.62)	Acc@5 100.00 ( 98.90)
Test: [ 30/100]	Time  0.020 ( 0.025)	Loss 6.0059e-01 (5.1738e-01)	Acc@1  84.00 ( 83.52)	Acc@5  96.00 ( 98.68)
Test: [ 40/100]	Time  0.017 ( 0.023)	Loss 6.0547e-01 (5.1473e-01)	Acc@1  86.00 ( 83.56)	Acc@5  99.00 ( 98.66)
Test: [ 50/100]	Time  0.021 ( 0.023)	Loss 3.2178e-01 (5.0519e-01)	Acc@1  89.00 ( 83.98)	Acc@5  99.00 ( 98.69)
Test: [ 60/100]	Time  0.020 ( 0.022)	Loss 7.0557e-01 (5.1101e-01)	Acc@1  78.00 ( 83.64)	Acc@5  98.00 ( 98.74)
Test: [ 70/100]	Time  0.020 ( 0.022)	Loss 4.8877e-01 (5.0310e-01)	Acc@1  85.00 ( 83.68)	Acc@5 100.00 ( 98.83)
Test: [ 80/100]	Time  0.022 ( 0.022)	Loss 3.8257e-01 (5.0363e-01)	Acc@1  89.00 ( 83.73)	Acc@5  99.00 ( 98.80)
Test: [ 90/100]	Time  0.020 ( 0.022)	Loss 4.0796e-01 (5.0279e-01)	Acc@1  83.00 ( 83.75)	Acc@5 100.00 ( 98.84)
 * Acc@1 83.700 Acc@5 98.780
### epoch[26] execution time: 18.249629497528076
EPOCH 27
# current learning rate: 0.1
# Switched to train mode...
Epoch: [27][  0/391]	Time  0.209 ( 0.209)	Data  0.168 ( 0.168)	Loss 2.3438e-01 (2.3438e-01)	Acc@1  92.97 ( 92.97)	Acc@5  99.22 ( 99.22)
Epoch: [27][ 10/391]	Time  0.041 ( 0.056)	Data  0.001 ( 0.016)	Loss 2.6782e-01 (3.1607e-01)	Acc@1  89.84 ( 89.35)	Acc@5 100.00 ( 99.57)
Epoch: [27][ 20/391]	Time  0.039 ( 0.049)	Data  0.001 ( 0.009)	Loss 2.5342e-01 (2.9936e-01)	Acc@1  91.41 ( 89.55)	Acc@5 100.00 ( 99.55)
Epoch: [27][ 30/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.006)	Loss 2.4011e-01 (3.0008e-01)	Acc@1  92.19 ( 89.49)	Acc@5 100.00 ( 99.67)
Epoch: [27][ 40/391]	Time  0.038 ( 0.045)	Data  0.001 ( 0.005)	Loss 2.6367e-01 (3.0292e-01)	Acc@1  92.19 ( 89.50)	Acc@5 100.00 ( 99.68)
Epoch: [27][ 50/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.004)	Loss 2.1582e-01 (3.0167e-01)	Acc@1  91.41 ( 89.40)	Acc@5 100.00 ( 99.71)
Epoch: [27][ 60/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.004)	Loss 2.5073e-01 (3.0548e-01)	Acc@1  91.41 ( 89.19)	Acc@5 100.00 ( 99.72)
Epoch: [27][ 70/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.0127e-01 (3.1041e-01)	Acc@1  87.50 ( 88.91)	Acc@5 100.00 ( 99.72)
Epoch: [27][ 80/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.003)	Loss 2.8857e-01 (3.1015e-01)	Acc@1  89.06 ( 88.91)	Acc@5 100.00 ( 99.75)
Epoch: [27][ 90/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.7905e-01 (3.1337e-01)	Acc@1  88.28 ( 88.85)	Acc@5 100.00 ( 99.75)
Epoch: [27][100/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.0273e-01 (3.1496e-01)	Acc@1  90.62 ( 88.92)	Acc@5  99.22 ( 99.74)
Epoch: [27][110/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.6050e-01 (3.1676e-01)	Acc@1  92.97 ( 88.87)	Acc@5 100.00 ( 99.72)
Epoch: [27][120/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.0715e-01 (3.1443e-01)	Acc@1  91.41 ( 88.94)	Acc@5 100.00 ( 99.72)
Epoch: [27][130/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1226e-01 (3.1412e-01)	Acc@1  88.28 ( 88.95)	Acc@5  99.22 ( 99.69)
Epoch: [27][140/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.5366e-01 (3.1207e-01)	Acc@1  92.19 ( 89.05)	Acc@5 100.00 ( 99.71)
Epoch: [27][150/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7866e-01 (3.1013e-01)	Acc@1  88.28 ( 89.13)	Acc@5  99.22 ( 99.70)
Epoch: [27][160/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3179e-01 (3.0869e-01)	Acc@1  89.84 ( 89.22)	Acc@5  99.22 ( 99.69)
Epoch: [27][170/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.0835e-01 (3.0921e-01)	Acc@1  89.06 ( 89.19)	Acc@5 100.00 ( 99.68)
Epoch: [27][180/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2568e-01 (3.1125e-01)	Acc@1  88.28 ( 89.11)	Acc@5 100.00 ( 99.68)
Epoch: [27][190/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2568e-01 (3.1254e-01)	Acc@1  86.72 ( 89.03)	Acc@5  99.22 ( 99.66)
Epoch: [27][200/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2324e-01 (3.1396e-01)	Acc@1  85.16 ( 88.92)	Acc@5 100.00 ( 99.67)
Epoch: [27][210/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6304e-01 (3.1606e-01)	Acc@1  89.06 ( 88.84)	Acc@5  99.22 ( 99.66)
Epoch: [27][220/391]	Time  0.052 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.2986e-01 (3.1603e-01)	Acc@1  92.19 ( 88.84)	Acc@5 100.00 ( 99.66)
Epoch: [27][230/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.3645e-01 (3.1616e-01)	Acc@1  92.19 ( 88.79)	Acc@5 100.00 ( 99.67)
Epoch: [27][240/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2373e-01 (3.1767e-01)	Acc@1  89.06 ( 88.74)	Acc@5  99.22 ( 99.66)
Epoch: [27][250/391]	Time  0.035 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0869e-01 (3.1982e-01)	Acc@1  89.84 ( 88.70)	Acc@5  98.44 ( 99.66)
Epoch: [27][260/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.9175e-01 (3.1949e-01)	Acc@1  91.41 ( 88.72)	Acc@5 100.00 ( 99.67)
Epoch: [27][270/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2544e-01 (3.2087e-01)	Acc@1  86.72 ( 88.64)	Acc@5 100.00 ( 99.67)
Epoch: [27][280/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3130e-01 (3.2211e-01)	Acc@1  85.94 ( 88.56)	Acc@5 100.00 ( 99.68)
Epoch: [27][290/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.0430e-01 (3.2288e-01)	Acc@1  83.59 ( 88.54)	Acc@5  99.22 ( 99.67)
Epoch: [27][300/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.9966e-01 (3.2426e-01)	Acc@1  85.16 ( 88.51)	Acc@5 100.00 ( 99.68)
Epoch: [27][310/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.3340e-01 (3.2505e-01)	Acc@1  91.41 ( 88.48)	Acc@5 100.00 ( 99.67)
Epoch: [27][320/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.6475e-01 (3.2568e-01)	Acc@1  86.72 ( 88.45)	Acc@5  99.22 ( 99.67)
Epoch: [27][330/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.0029e-01 (3.2559e-01)	Acc@1  88.28 ( 88.46)	Acc@5  99.22 ( 99.67)
Epoch: [27][340/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.7280e-01 (3.2505e-01)	Acc@1  81.25 ( 88.47)	Acc@5  99.22 ( 99.67)
Epoch: [27][350/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.5889e-01 (3.2534e-01)	Acc@1  88.28 ( 88.49)	Acc@5 100.00 ( 99.67)
Epoch: [27][360/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.3545e-01 (3.2627e-01)	Acc@1  86.72 ( 88.47)	Acc@5 100.00 ( 99.68)
Epoch: [27][370/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.6133e-01 (3.2668e-01)	Acc@1  85.94 ( 88.46)	Acc@5 100.00 ( 99.67)
Epoch: [27][380/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.0005e-01 (3.2686e-01)	Acc@1  88.28 ( 88.46)	Acc@5  99.22 ( 99.68)
Epoch: [27][390/391]	Time  0.027 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.2788e-01 (3.2696e-01)	Acc@1  87.50 ( 88.45)	Acc@5 100.00 ( 99.68)
## e[27] optimizer.zero_grad (sum) time: 0.24976229667663574
## e[27]       loss.backward (sum) time: 4.148865461349487
## e[27]      optimizer.step (sum) time: 1.7475345134735107
## epoch[27] training(only) time: 15.873524188995361
# Switched to evaluate mode...
Test: [  0/100]	Time  0.165 ( 0.165)	Loss 4.9341e-01 (4.9341e-01)	Acc@1  86.00 ( 86.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.017 ( 0.032)	Loss 4.9243e-01 (4.4658e-01)	Acc@1  85.00 ( 84.73)	Acc@5  98.00 ( 99.18)
Test: [ 20/100]	Time  0.021 ( 0.027)	Loss 5.9570e-01 (4.6302e-01)	Acc@1  78.00 ( 84.48)	Acc@5  99.00 ( 99.14)
Test: [ 30/100]	Time  0.016 ( 0.024)	Loss 5.5664e-01 (4.6606e-01)	Acc@1  82.00 ( 84.65)	Acc@5  99.00 ( 99.10)
Test: [ 40/100]	Time  0.018 ( 0.023)	Loss 4.3652e-01 (4.6358e-01)	Acc@1  86.00 ( 84.61)	Acc@5  99.00 ( 99.12)
Test: [ 50/100]	Time  0.023 ( 0.023)	Loss 3.6499e-01 (4.5928e-01)	Acc@1  86.00 ( 84.86)	Acc@5  99.00 ( 99.08)
Test: [ 60/100]	Time  0.020 ( 0.022)	Loss 5.2246e-01 (4.6227e-01)	Acc@1  85.00 ( 84.69)	Acc@5  98.00 ( 99.10)
Test: [ 70/100]	Time  0.021 ( 0.022)	Loss 5.4590e-01 (4.6488e-01)	Acc@1  82.00 ( 84.65)	Acc@5 100.00 ( 99.10)
Test: [ 80/100]	Time  0.017 ( 0.022)	Loss 3.1494e-01 (4.7050e-01)	Acc@1  89.00 ( 84.70)	Acc@5 100.00 ( 99.09)
Test: [ 90/100]	Time  0.016 ( 0.021)	Loss 3.6060e-01 (4.6977e-01)	Acc@1  88.00 ( 84.73)	Acc@5 100.00 ( 99.10)
 * Acc@1 84.710 Acc@5 99.110
### epoch[27] execution time: 18.101549863815308
EPOCH 28
# current learning rate: 0.1
# Switched to train mode...
Epoch: [28][  0/391]	Time  0.212 ( 0.212)	Data  0.171 ( 0.171)	Loss 2.4841e-01 (2.4841e-01)	Acc@1  90.62 ( 90.62)	Acc@5 100.00 (100.00)
Epoch: [28][ 10/391]	Time  0.038 ( 0.056)	Data  0.001 ( 0.016)	Loss 2.9639e-01 (2.8855e-01)	Acc@1  89.84 ( 89.99)	Acc@5 100.00 ( 99.93)
Epoch: [28][ 20/391]	Time  0.040 ( 0.049)	Data  0.001 ( 0.009)	Loss 3.5327e-01 (3.0771e-01)	Acc@1  84.38 ( 89.21)	Acc@5  99.22 ( 99.81)
Epoch: [28][ 30/391]	Time  0.039 ( 0.045)	Data  0.001 ( 0.006)	Loss 2.3975e-01 (2.9263e-01)	Acc@1  92.97 ( 89.89)	Acc@5  99.22 ( 99.82)
Epoch: [28][ 40/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.005)	Loss 2.8882e-01 (2.9473e-01)	Acc@1  87.50 ( 89.63)	Acc@5 100.00 ( 99.79)
Epoch: [28][ 50/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.004)	Loss 2.5879e-01 (2.9148e-01)	Acc@1  90.62 ( 89.75)	Acc@5  99.22 ( 99.77)
Epoch: [28][ 60/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.004)	Loss 2.9077e-01 (2.9095e-01)	Acc@1  90.62 ( 89.91)	Acc@5 100.00 ( 99.74)
Epoch: [28][ 70/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.9297e-01 (2.9316e-01)	Acc@1  93.75 ( 89.84)	Acc@5 100.00 ( 99.77)
Epoch: [28][ 80/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.1846e-01 (2.9322e-01)	Acc@1  85.94 ( 89.82)	Acc@5  99.22 ( 99.76)
Epoch: [28][ 90/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.7173e-01 (2.9152e-01)	Acc@1  88.28 ( 89.79)	Acc@5  99.22 ( 99.75)
Epoch: [28][100/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.9663e-01 (2.9281e-01)	Acc@1  89.06 ( 89.71)	Acc@5 100.00 ( 99.74)
Epoch: [28][110/391]	Time  0.053 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.0347e-01 (2.9483e-01)	Acc@1  89.06 ( 89.60)	Acc@5 100.00 ( 99.76)
Epoch: [28][120/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0234e-01 (2.9925e-01)	Acc@1  83.59 ( 89.37)	Acc@5 100.00 ( 99.74)
Epoch: [28][130/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.2546e-01 (3.0005e-01)	Acc@1  93.75 ( 89.37)	Acc@5 100.00 ( 99.74)
Epoch: [28][140/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.0762e-01 (3.0264e-01)	Acc@1  88.28 ( 89.28)	Acc@5 100.00 ( 99.73)
Epoch: [28][150/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5596e-01 (3.0511e-01)	Acc@1  87.50 ( 89.24)	Acc@5 100.00 ( 99.73)
Epoch: [28][160/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9038e-01 (3.0802e-01)	Acc@1  83.59 ( 89.11)	Acc@5 100.00 ( 99.72)
Epoch: [28][170/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2627e-01 (3.0870e-01)	Acc@1  85.16 ( 89.08)	Acc@5  97.66 ( 99.72)
Epoch: [28][180/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9893e-01 (3.0925e-01)	Acc@1  89.06 ( 89.12)	Acc@5  99.22 ( 99.72)
Epoch: [28][190/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6548e-01 (3.1068e-01)	Acc@1  88.28 ( 89.07)	Acc@5  99.22 ( 99.72)
Epoch: [28][200/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8804e-01 (3.1204e-01)	Acc@1  81.25 ( 89.02)	Acc@5  99.22 ( 99.72)
Epoch: [28][210/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.2888e-01 (3.1138e-01)	Acc@1  92.97 ( 89.00)	Acc@5  99.22 ( 99.71)
Epoch: [28][220/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2104e-01 (3.1168e-01)	Acc@1  89.84 ( 89.03)	Acc@5 100.00 ( 99.71)
Epoch: [28][230/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.3779e-01 (3.1214e-01)	Acc@1  92.19 ( 89.00)	Acc@5  99.22 ( 99.69)
Epoch: [28][240/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3105e-01 (3.1335e-01)	Acc@1  86.72 ( 88.96)	Acc@5 100.00 ( 99.70)
Epoch: [28][250/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.0469e-01 (3.1344e-01)	Acc@1  89.06 ( 88.98)	Acc@5 100.00 ( 99.70)
Epoch: [28][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.2847e-01 (3.1329e-01)	Acc@1  84.38 ( 89.00)	Acc@5  99.22 ( 99.70)
Epoch: [28][270/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.1431e-01 (3.1356e-01)	Acc@1  87.50 ( 88.99)	Acc@5 100.00 ( 99.71)
Epoch: [28][280/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.0879e-01 (3.1459e-01)	Acc@1  85.16 ( 88.95)	Acc@5  98.44 ( 99.71)
Epoch: [28][290/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.8467e-01 (3.1502e-01)	Acc@1  87.50 ( 88.93)	Acc@5 100.00 ( 99.71)
Epoch: [28][300/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.5156e-01 (3.1527e-01)	Acc@1  85.94 ( 88.92)	Acc@5 100.00 ( 99.70)
Epoch: [28][310/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.2104e-01 (3.1502e-01)	Acc@1  90.62 ( 88.93)	Acc@5 100.00 ( 99.71)
Epoch: [28][320/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.4277e-01 (3.1538e-01)	Acc@1  85.16 ( 88.91)	Acc@5 100.00 ( 99.71)
Epoch: [28][330/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.6050e-01 (3.1637e-01)	Acc@1  89.06 ( 88.88)	Acc@5 100.00 ( 99.71)
Epoch: [28][340/391]	Time  0.050 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.5376e-01 (3.1634e-01)	Acc@1  89.06 ( 88.89)	Acc@5 100.00 ( 99.71)
Epoch: [28][350/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.4524e-01 (3.1683e-01)	Acc@1  89.84 ( 88.88)	Acc@5  99.22 ( 99.71)
Epoch: [28][360/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.6196e-01 (3.1655e-01)	Acc@1  91.41 ( 88.90)	Acc@5 100.00 ( 99.71)
Epoch: [28][370/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.7051e-01 (3.1696e-01)	Acc@1  90.62 ( 88.89)	Acc@5 100.00 ( 99.71)
Epoch: [28][380/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.9746e-01 (3.1757e-01)	Acc@1  85.94 ( 88.87)	Acc@5  99.22 ( 99.71)
Epoch: [28][390/391]	Time  0.027 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.1577e-01 (3.1792e-01)	Acc@1  86.25 ( 88.88)	Acc@5 100.00 ( 99.71)
## e[28] optimizer.zero_grad (sum) time: 0.24918699264526367
## e[28]       loss.backward (sum) time: 4.192508220672607
## e[28]      optimizer.step (sum) time: 1.7502901554107666
## epoch[28] training(only) time: 15.901843547821045
# Switched to evaluate mode...
Test: [  0/100]	Time  0.175 ( 0.175)	Loss 4.7949e-01 (4.7949e-01)	Acc@1  85.00 ( 85.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.021 ( 0.035)	Loss 5.6299e-01 (5.2124e-01)	Acc@1  83.00 ( 83.73)	Acc@5 100.00 ( 99.18)
Test: [ 20/100]	Time  0.018 ( 0.028)	Loss 7.2754e-01 (5.1192e-01)	Acc@1  76.00 ( 83.86)	Acc@5  97.00 ( 98.95)
Test: [ 30/100]	Time  0.016 ( 0.025)	Loss 5.9033e-01 (5.2352e-01)	Acc@1  78.00 ( 83.65)	Acc@5 100.00 ( 99.03)
Test: [ 40/100]	Time  0.022 ( 0.024)	Loss 4.8486e-01 (5.2995e-01)	Acc@1  86.00 ( 83.63)	Acc@5  99.00 ( 98.93)
Test: [ 50/100]	Time  0.021 ( 0.023)	Loss 4.5703e-01 (5.2428e-01)	Acc@1  87.00 ( 83.75)	Acc@5  96.00 ( 98.94)
Test: [ 60/100]	Time  0.022 ( 0.023)	Loss 5.6982e-01 (5.2649e-01)	Acc@1  81.00 ( 83.66)	Acc@5  99.00 ( 99.00)
Test: [ 70/100]	Time  0.022 ( 0.023)	Loss 4.7778e-01 (5.2002e-01)	Acc@1  83.00 ( 83.75)	Acc@5 100.00 ( 99.06)
Test: [ 80/100]	Time  0.021 ( 0.022)	Loss 3.6914e-01 (5.1513e-01)	Acc@1  89.00 ( 83.96)	Acc@5  99.00 ( 99.06)
Test: [ 90/100]	Time  0.017 ( 0.022)	Loss 3.8525e-01 (5.1143e-01)	Acc@1  89.00 ( 83.92)	Acc@5 100.00 ( 99.01)
 * Acc@1 83.850 Acc@5 99.070
### epoch[28] execution time: 18.191776752471924
EPOCH 29
# current learning rate: 0.1
# Switched to train mode...
Epoch: [29][  0/391]	Time  0.217 ( 0.217)	Data  0.176 ( 0.176)	Loss 2.7417e-01 (2.7417e-01)	Acc@1  89.84 ( 89.84)	Acc@5 100.00 (100.00)
Epoch: [29][ 10/391]	Time  0.040 ( 0.056)	Data  0.001 ( 0.017)	Loss 3.1323e-01 (2.9633e-01)	Acc@1  86.72 ( 89.56)	Acc@5 100.00 ( 99.93)
Epoch: [29][ 20/391]	Time  0.041 ( 0.048)	Data  0.001 ( 0.009)	Loss 3.7720e-01 (3.0926e-01)	Acc@1  87.50 ( 89.47)	Acc@5 100.00 ( 99.78)
Epoch: [29][ 30/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.007)	Loss 3.6157e-01 (3.1084e-01)	Acc@1  85.94 ( 89.11)	Acc@5 100.00 ( 99.72)
Epoch: [29][ 40/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.005)	Loss 1.8250e-01 (3.0056e-01)	Acc@1  95.31 ( 89.56)	Acc@5 100.00 ( 99.75)
Epoch: [29][ 50/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.004)	Loss 4.8193e-01 (3.0359e-01)	Acc@1  86.72 ( 89.63)	Acc@5  98.44 ( 99.72)
Epoch: [29][ 60/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.004)	Loss 3.7891e-01 (3.0518e-01)	Acc@1  85.94 ( 89.43)	Acc@5  99.22 ( 99.69)
Epoch: [29][ 70/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.6685e-01 (3.0442e-01)	Acc@1  92.19 ( 89.46)	Acc@5  99.22 ( 99.68)
Epoch: [29][ 80/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.9272e-01 (3.0423e-01)	Acc@1  88.28 ( 89.55)	Acc@5 100.00 ( 99.66)
Epoch: [29][ 90/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.5806e-01 (3.0168e-01)	Acc@1  91.41 ( 89.65)	Acc@5 100.00 ( 99.67)
Epoch: [29][100/391]	Time  0.035 ( 0.041)	Data  0.001 ( 0.003)	Loss 3.2666e-01 (3.0649e-01)	Acc@1  88.28 ( 89.52)	Acc@5 100.00 ( 99.66)
Epoch: [29][110/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.9663e-01 (3.0597e-01)	Acc@1  89.84 ( 89.49)	Acc@5 100.00 ( 99.68)
Epoch: [29][120/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9771e-01 (3.0934e-01)	Acc@1  86.72 ( 89.35)	Acc@5  99.22 ( 99.68)
Epoch: [29][130/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.7295e-01 (3.1019e-01)	Acc@1  92.19 ( 89.31)	Acc@5  99.22 ( 99.67)
Epoch: [29][140/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1885e-01 (3.0912e-01)	Acc@1  87.50 ( 89.28)	Acc@5 100.00 ( 99.68)
Epoch: [29][150/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.0166e-01 (3.0911e-01)	Acc@1  92.19 ( 89.23)	Acc@5 100.00 ( 99.68)
Epoch: [29][160/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3262e-01 (3.0939e-01)	Acc@1  85.16 ( 89.18)	Acc@5 100.00 ( 99.70)
Epoch: [29][170/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.2253e-01 (3.0921e-01)	Acc@1  90.62 ( 89.23)	Acc@5 100.00 ( 99.69)
Epoch: [29][180/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7896e-01 (3.0974e-01)	Acc@1  93.75 ( 89.23)	Acc@5 100.00 ( 99.68)
Epoch: [29][190/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.6460e-01 (3.1313e-01)	Acc@1  84.38 ( 89.14)	Acc@5  99.22 ( 99.67)
Epoch: [29][200/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2959e-01 (3.1462e-01)	Acc@1  89.06 ( 89.11)	Acc@5  99.22 ( 99.66)
Epoch: [29][210/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2446e-01 (3.1518e-01)	Acc@1  88.28 ( 89.10)	Acc@5 100.00 ( 99.66)
Epoch: [29][220/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2886e-01 (3.1630e-01)	Acc@1  87.50 ( 89.08)	Acc@5  99.22 ( 99.66)
Epoch: [29][230/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.9883e-01 (3.1662e-01)	Acc@1  90.62 ( 89.03)	Acc@5  99.22 ( 99.67)
Epoch: [29][240/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6646e-01 (3.1605e-01)	Acc@1  87.50 ( 89.05)	Acc@5  97.66 ( 99.66)
Epoch: [29][250/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.002)	Loss 2.0276e-01 (3.1575e-01)	Acc@1  95.31 ( 89.07)	Acc@5 100.00 ( 99.67)
Epoch: [29][260/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 2.4353e-01 (3.1575e-01)	Acc@1  89.06 ( 89.06)	Acc@5 100.00 ( 99.66)
Epoch: [29][270/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 3.7256e-01 (3.1731e-01)	Acc@1  85.94 ( 89.02)	Acc@5 100.00 ( 99.66)
Epoch: [29][280/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 4.3896e-01 (3.1795e-01)	Acc@1  87.50 ( 88.99)	Acc@5 100.00 ( 99.66)
Epoch: [29][290/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.002)	Loss 3.3936e-01 (3.1780e-01)	Acc@1  90.62 ( 88.99)	Acc@5 100.00 ( 99.66)
Epoch: [29][300/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.6157e-01 (3.1843e-01)	Acc@1  84.38 ( 88.94)	Acc@5  98.44 ( 99.66)
Epoch: [29][310/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.5815e-01 (3.1951e-01)	Acc@1  84.38 ( 88.89)	Acc@5 100.00 ( 99.67)
Epoch: [29][320/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.3374e-01 (3.1969e-01)	Acc@1  89.84 ( 88.90)	Acc@5 100.00 ( 99.66)
Epoch: [29][330/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.8418e-01 (3.2014e-01)	Acc@1  88.28 ( 88.87)	Acc@5 100.00 ( 99.66)
Epoch: [29][340/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.3862e-01 (3.2042e-01)	Acc@1  86.72 ( 88.84)	Acc@5 100.00 ( 99.66)
Epoch: [29][350/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.9346e-01 (3.1986e-01)	Acc@1  88.28 ( 88.87)	Acc@5  99.22 ( 99.66)
Epoch: [29][360/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.8542e-01 (3.1942e-01)	Acc@1  92.19 ( 88.90)	Acc@5 100.00 ( 99.66)
Epoch: [29][370/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.6353e-01 (3.1932e-01)	Acc@1  89.06 ( 88.89)	Acc@5  99.22 ( 99.66)
Epoch: [29][380/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.5293e-01 (3.1918e-01)	Acc@1  90.62 ( 88.87)	Acc@5 100.00 ( 99.67)
Epoch: [29][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.2104e-01 (3.1984e-01)	Acc@1  87.50 ( 88.82)	Acc@5 100.00 ( 99.66)
## e[29] optimizer.zero_grad (sum) time: 0.25073671340942383
## e[29]       loss.backward (sum) time: 4.174325704574585
## e[29]      optimizer.step (sum) time: 1.746985912322998
## epoch[29] training(only) time: 15.86596393585205
# Switched to evaluate mode...
Test: [  0/100]	Time  0.173 ( 0.173)	Loss 7.1338e-01 (7.1338e-01)	Acc@1  81.00 ( 81.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.025 ( 0.033)	Loss 6.2207e-01 (6.2247e-01)	Acc@1  81.00 ( 80.45)	Acc@5  99.00 ( 99.00)
Test: [ 20/100]	Time  0.020 ( 0.028)	Loss 8.1543e-01 (6.1749e-01)	Acc@1  72.00 ( 81.14)	Acc@5 100.00 ( 98.76)
Test: [ 30/100]	Time  0.018 ( 0.025)	Loss 9.3799e-01 (6.4329e-01)	Acc@1  72.00 ( 80.48)	Acc@5  99.00 ( 98.90)
Test: [ 40/100]	Time  0.020 ( 0.024)	Loss 7.3975e-01 (6.5170e-01)	Acc@1  77.00 ( 80.15)	Acc@5  97.00 ( 98.85)
Test: [ 50/100]	Time  0.019 ( 0.023)	Loss 6.8408e-01 (6.4753e-01)	Acc@1  83.00 ( 80.41)	Acc@5  96.00 ( 98.80)
Test: [ 60/100]	Time  0.017 ( 0.023)	Loss 6.9775e-01 (6.4417e-01)	Acc@1  80.00 ( 80.38)	Acc@5 100.00 ( 98.90)
Test: [ 70/100]	Time  0.017 ( 0.022)	Loss 6.1963e-01 (6.3441e-01)	Acc@1  80.00 ( 80.41)	Acc@5 100.00 ( 99.00)
Test: [ 80/100]	Time  0.019 ( 0.022)	Loss 6.0010e-01 (6.4181e-01)	Acc@1  80.00 ( 80.44)	Acc@5  99.00 ( 98.99)
Test: [ 90/100]	Time  0.023 ( 0.022)	Loss 4.8608e-01 (6.4423e-01)	Acc@1  81.00 ( 80.22)	Acc@5 100.00 ( 99.00)
 * Acc@1 80.020 Acc@5 99.020
### epoch[29] execution time: 18.151561498641968
EPOCH 30
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [30][  0/391]	Time  0.216 ( 0.216)	Data  0.175 ( 0.175)	Loss 2.4451e-01 (2.4451e-01)	Acc@1  92.19 ( 92.19)	Acc@5 100.00 (100.00)
Epoch: [30][ 10/391]	Time  0.042 ( 0.056)	Data  0.001 ( 0.017)	Loss 2.6172e-01 (2.7459e-01)	Acc@1  87.50 ( 89.99)	Acc@5 100.00 ( 99.86)
Epoch: [30][ 20/391]	Time  0.041 ( 0.049)	Data  0.002 ( 0.009)	Loss 2.1484e-01 (2.7495e-01)	Acc@1  92.19 ( 90.14)	Acc@5  99.22 ( 99.74)
Epoch: [30][ 30/391]	Time  0.042 ( 0.046)	Data  0.001 ( 0.007)	Loss 2.4963e-01 (2.6423e-01)	Acc@1  90.62 ( 90.62)	Acc@5  99.22 ( 99.72)
Epoch: [30][ 40/391]	Time  0.039 ( 0.045)	Data  0.001 ( 0.005)	Loss 2.3828e-01 (2.6136e-01)	Acc@1  91.41 ( 90.66)	Acc@5 100.00 ( 99.75)
Epoch: [30][ 50/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.004)	Loss 2.1338e-01 (2.5028e-01)	Acc@1  91.41 ( 91.18)	Acc@5 100.00 ( 99.80)
Epoch: [30][ 60/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.004)	Loss 3.0664e-01 (2.5503e-01)	Acc@1  89.84 ( 91.01)	Acc@5  99.22 ( 99.80)
Epoch: [30][ 70/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.8762e-01 (2.4885e-01)	Acc@1  94.53 ( 91.38)	Acc@5 100.00 ( 99.81)
Epoch: [30][ 80/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.3718e-01 (2.4637e-01)	Acc@1  92.19 ( 91.46)	Acc@5 100.00 ( 99.83)
Epoch: [30][ 90/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.003)	Loss 1.5710e-01 (2.4507e-01)	Acc@1  93.75 ( 91.51)	Acc@5 100.00 ( 99.82)
Epoch: [30][100/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.003)	Loss 2.5293e-01 (2.4353e-01)	Acc@1  91.41 ( 91.62)	Acc@5 100.00 ( 99.81)
Epoch: [30][110/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.003)	Loss 2.3792e-01 (2.3950e-01)	Acc@1  94.53 ( 91.80)	Acc@5  99.22 ( 99.82)
Epoch: [30][120/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4856e-01 (2.3820e-01)	Acc@1  96.09 ( 91.90)	Acc@5 100.00 ( 99.80)
Epoch: [30][130/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7102e-01 (2.3688e-01)	Acc@1  94.53 ( 91.96)	Acc@5 100.00 ( 99.80)
Epoch: [30][140/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.8335e-01 (2.3465e-01)	Acc@1  93.75 ( 92.04)	Acc@5 100.00 ( 99.81)
Epoch: [30][150/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3684e-01 (2.3228e-01)	Acc@1  96.88 ( 92.18)	Acc@5 100.00 ( 99.81)
Epoch: [30][160/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4539e-01 (2.3013e-01)	Acc@1  94.53 ( 92.22)	Acc@5 100.00 ( 99.82)
Epoch: [30][170/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.002)	Loss 2.0508e-01 (2.3144e-01)	Acc@1  93.75 ( 92.17)	Acc@5 100.00 ( 99.82)
Epoch: [30][180/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 2.7222e-01 (2.3264e-01)	Acc@1  89.84 ( 92.10)	Acc@5 100.00 ( 99.82)
Epoch: [30][190/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.8408e-01 (2.3191e-01)	Acc@1  93.75 ( 92.13)	Acc@5 100.00 ( 99.82)
Epoch: [30][200/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.002)	Loss 2.1033e-01 (2.3096e-01)	Acc@1  92.97 ( 92.16)	Acc@5 100.00 ( 99.83)
Epoch: [30][210/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 3.3228e-01 (2.3025e-01)	Acc@1  89.84 ( 92.17)	Acc@5 100.00 ( 99.83)
Epoch: [30][220/391]	Time  0.044 ( 0.040)	Data  0.001 ( 0.002)	Loss 2.5879e-01 (2.2902e-01)	Acc@1  92.97 ( 92.21)	Acc@5 100.00 ( 99.83)
Epoch: [30][230/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 3.1836e-01 (2.2878e-01)	Acc@1  89.84 ( 92.20)	Acc@5  98.44 ( 99.82)
Epoch: [30][240/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.0107e-01 (2.2770e-01)	Acc@1  98.44 ( 92.27)	Acc@5 100.00 ( 99.83)
Epoch: [30][250/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 2.3730e-01 (2.2649e-01)	Acc@1  91.41 ( 92.33)	Acc@5 100.00 ( 99.83)
Epoch: [30][260/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.002)	Loss 2.5220e-01 (2.2666e-01)	Acc@1  89.06 ( 92.32)	Acc@5 100.00 ( 99.83)
Epoch: [30][270/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 2.2656e-01 (2.2644e-01)	Acc@1  92.97 ( 92.32)	Acc@5 100.00 ( 99.83)
Epoch: [30][280/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.6345e-01 (2.2505e-01)	Acc@1  97.66 ( 92.41)	Acc@5 100.00 ( 99.83)
Epoch: [30][290/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.5881e-01 (2.2338e-01)	Acc@1  94.53 ( 92.45)	Acc@5 100.00 ( 99.83)
Epoch: [30][300/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.8320e-01 (2.2300e-01)	Acc@1  92.97 ( 92.45)	Acc@5  99.22 ( 99.83)
Epoch: [30][310/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.8054e-01 (2.2238e-01)	Acc@1  95.31 ( 92.48)	Acc@5 100.00 ( 99.84)
Epoch: [30][320/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.5308e-01 (2.2078e-01)	Acc@1  96.09 ( 92.56)	Acc@5 100.00 ( 99.84)
Epoch: [30][330/391]	Time  0.044 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.5369e-01 (2.2038e-01)	Acc@1  94.53 ( 92.55)	Acc@5 100.00 ( 99.84)
Epoch: [30][340/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.5454e-01 (2.1871e-01)	Acc@1  94.53 ( 92.62)	Acc@5 100.00 ( 99.84)
Epoch: [30][350/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.1790e-01 (2.1767e-01)	Acc@1  90.62 ( 92.66)	Acc@5 100.00 ( 99.84)
Epoch: [30][360/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.0544e-01 (2.1714e-01)	Acc@1  92.97 ( 92.65)	Acc@5 100.00 ( 99.84)
Epoch: [30][370/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.9153e-01 (2.1695e-01)	Acc@1  92.97 ( 92.65)	Acc@5 100.00 ( 99.84)
Epoch: [30][380/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.4526e-01 (2.1649e-01)	Acc@1  96.09 ( 92.64)	Acc@5 100.00 ( 99.84)
Epoch: [30][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.3645e-01 (2.1620e-01)	Acc@1  92.50 ( 92.64)	Acc@5 100.00 ( 99.85)
## e[30] optimizer.zero_grad (sum) time: 0.24844121932983398
## e[30]       loss.backward (sum) time: 4.084440469741821
## e[30]      optimizer.step (sum) time: 1.7827317714691162
## epoch[30] training(only) time: 15.831498146057129
# Switched to evaluate mode...
Test: [  0/100]	Time  0.175 ( 0.175)	Loss 3.1421e-01 (3.1421e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.021 ( 0.035)	Loss 3.0005e-01 (2.6802e-01)	Acc@1  89.00 ( 91.09)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.023 ( 0.028)	Loss 3.3789e-01 (2.8250e-01)	Acc@1  84.00 ( 90.33)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.017 ( 0.025)	Loss 3.7793e-01 (2.9426e-01)	Acc@1  87.00 ( 90.16)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.017 ( 0.023)	Loss 3.4766e-01 (2.9887e-01)	Acc@1  87.00 ( 90.10)	Acc@5  99.00 ( 99.68)
Test: [ 50/100]	Time  0.023 ( 0.023)	Loss 2.1069e-01 (2.9767e-01)	Acc@1  93.00 ( 90.33)	Acc@5 100.00 ( 99.63)
Test: [ 60/100]	Time  0.016 ( 0.022)	Loss 3.0420e-01 (2.9788e-01)	Acc@1  93.00 ( 90.18)	Acc@5 100.00 ( 99.67)
Test: [ 70/100]	Time  0.020 ( 0.022)	Loss 2.9932e-01 (2.9118e-01)	Acc@1  89.00 ( 90.34)	Acc@5 100.00 ( 99.69)
Test: [ 80/100]	Time  0.017 ( 0.021)	Loss 1.8018e-01 (2.9343e-01)	Acc@1  92.00 ( 90.27)	Acc@5 100.00 ( 99.69)
Test: [ 90/100]	Time  0.018 ( 0.021)	Loss 2.1802e-01 (2.9265e-01)	Acc@1  95.00 ( 90.30)	Acc@5 100.00 ( 99.69)
 * Acc@1 90.280 Acc@5 99.700
### epoch[30] execution time: 18.075498342514038
EPOCH 31
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [31][  0/391]	Time  0.206 ( 0.206)	Data  0.165 ( 0.165)	Loss 1.3330e-01 (1.3330e-01)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [31][ 10/391]	Time  0.041 ( 0.056)	Data  0.001 ( 0.016)	Loss 1.9958e-01 (1.9662e-01)	Acc@1  92.19 ( 93.25)	Acc@5 100.00 (100.00)
Epoch: [31][ 20/391]	Time  0.041 ( 0.048)	Data  0.001 ( 0.009)	Loss 1.6150e-01 (1.8605e-01)	Acc@1  94.53 ( 93.60)	Acc@5 100.00 ( 99.89)
Epoch: [31][ 30/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.006)	Loss 1.8665e-01 (1.8680e-01)	Acc@1  92.19 ( 93.65)	Acc@5 100.00 ( 99.85)
Epoch: [31][ 40/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.005)	Loss 1.7346e-01 (1.8958e-01)	Acc@1  93.75 ( 93.58)	Acc@5 100.00 ( 99.85)
Epoch: [31][ 50/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.004)	Loss 2.3218e-01 (1.8919e-01)	Acc@1  91.41 ( 93.52)	Acc@5 100.00 ( 99.86)
Epoch: [31][ 60/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.9678e-01 (1.9151e-01)	Acc@1  94.53 ( 93.53)	Acc@5 100.00 ( 99.86)
Epoch: [31][ 70/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.9629e-01 (1.9034e-01)	Acc@1  92.97 ( 93.61)	Acc@5  99.22 ( 99.86)
Epoch: [31][ 80/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.7249e-01 (1.9012e-01)	Acc@1  94.53 ( 93.59)	Acc@5 100.00 ( 99.87)
Epoch: [31][ 90/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.5051e-01 (1.8657e-01)	Acc@1  95.31 ( 93.72)	Acc@5 100.00 ( 99.89)
Epoch: [31][100/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.2571e-01 (1.8878e-01)	Acc@1  92.19 ( 93.56)	Acc@5 100.00 ( 99.88)
Epoch: [31][110/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.5723e-01 (1.8780e-01)	Acc@1  96.09 ( 93.60)	Acc@5 100.00 ( 99.86)
Epoch: [31][120/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.3926e-01 (1.8826e-01)	Acc@1  93.75 ( 93.62)	Acc@5 100.00 ( 99.87)
Epoch: [31][130/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.3572e-01 (1.8807e-01)	Acc@1  92.19 ( 93.62)	Acc@5 100.00 ( 99.87)
Epoch: [31][140/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7346e-01 (1.8921e-01)	Acc@1  94.53 ( 93.53)	Acc@5 100.00 ( 99.88)
Epoch: [31][150/391]	Time  0.041 ( 0.041)	Data  0.002 ( 0.002)	Loss 1.2695e-01 (1.8886e-01)	Acc@1  96.88 ( 93.52)	Acc@5 100.00 ( 99.89)
Epoch: [31][160/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.0898e-01 (1.8903e-01)	Acc@1  93.75 ( 93.52)	Acc@5  99.22 ( 99.88)
Epoch: [31][170/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3037e-01 (1.8892e-01)	Acc@1  96.09 ( 93.54)	Acc@5 100.00 ( 99.88)
Epoch: [31][180/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.9592e-01 (1.8914e-01)	Acc@1  91.41 ( 93.51)	Acc@5 100.00 ( 99.88)
Epoch: [31][190/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6528e-01 (1.8989e-01)	Acc@1  93.75 ( 93.46)	Acc@5 100.00 ( 99.88)
Epoch: [31][200/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.3816e-01 (1.8985e-01)	Acc@1  89.06 ( 93.45)	Acc@5 100.00 ( 99.88)
Epoch: [31][210/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7249e-01 (1.9072e-01)	Acc@1  92.97 ( 93.38)	Acc@5 100.00 ( 99.88)
Epoch: [31][220/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7773e-01 (1.8954e-01)	Acc@1  93.75 ( 93.43)	Acc@5 100.00 ( 99.88)
Epoch: [31][230/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7932e-01 (1.8942e-01)	Acc@1  93.75 ( 93.44)	Acc@5 100.00 ( 99.88)
Epoch: [31][240/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4673e-01 (1.8999e-01)	Acc@1  95.31 ( 93.45)	Acc@5 100.00 ( 99.88)
Epoch: [31][250/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.1899e-01 (1.8984e-01)	Acc@1  92.19 ( 93.44)	Acc@5  99.22 ( 99.87)
Epoch: [31][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.7124e-01 (1.9003e-01)	Acc@1  87.50 ( 93.44)	Acc@5 100.00 ( 99.87)
Epoch: [31][270/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.4062e-01 (1.9011e-01)	Acc@1  94.53 ( 93.42)	Acc@5 100.00 ( 99.88)
Epoch: [31][280/391]	Time  0.045 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.3428e-01 (1.8864e-01)	Acc@1  97.66 ( 93.50)	Acc@5 100.00 ( 99.88)
Epoch: [31][290/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.0251e-01 (1.8804e-01)	Acc@1  91.41 ( 93.54)	Acc@5 100.00 ( 99.88)
Epoch: [31][300/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.3477e-01 (1.8738e-01)	Acc@1  94.53 ( 93.57)	Acc@5 100.00 ( 99.88)
Epoch: [31][310/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.0923e-01 (1.8759e-01)	Acc@1  93.75 ( 93.56)	Acc@5 100.00 ( 99.89)
Epoch: [31][320/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.8689e-01 (1.8761e-01)	Acc@1  94.53 ( 93.55)	Acc@5 100.00 ( 99.88)
Epoch: [31][330/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.1204e-01 (1.8753e-01)	Acc@1  90.62 ( 93.55)	Acc@5 100.00 ( 99.88)
Epoch: [31][340/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.2864e-01 (1.8684e-01)	Acc@1  92.97 ( 93.59)	Acc@5  99.22 ( 99.88)
Epoch: [31][350/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.7588e-01 (1.8741e-01)	Acc@1  89.06 ( 93.57)	Acc@5  99.22 ( 99.88)
Epoch: [31][360/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.5857e-01 (1.8667e-01)	Acc@1  95.31 ( 93.61)	Acc@5 100.00 ( 99.89)
Epoch: [31][370/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.0520e-01 (1.8641e-01)	Acc@1  94.53 ( 93.62)	Acc@5 100.00 ( 99.88)
Epoch: [31][380/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.3584e-01 (1.8605e-01)	Acc@1  90.62 ( 93.61)	Acc@5 100.00 ( 99.89)
Epoch: [31][390/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.8784e-01 (1.8648e-01)	Acc@1  92.50 ( 93.59)	Acc@5 100.00 ( 99.89)
## e[31] optimizer.zero_grad (sum) time: 0.25051116943359375
## e[31]       loss.backward (sum) time: 4.171280145645142
## e[31]      optimizer.step (sum) time: 1.7405247688293457
## epoch[31] training(only) time: 15.846548795700073
# Switched to evaluate mode...
Test: [  0/100]	Time  0.170 ( 0.170)	Loss 3.0225e-01 (3.0225e-01)	Acc@1  93.00 ( 93.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.020 ( 0.032)	Loss 3.2764e-01 (2.6225e-01)	Acc@1  88.00 ( 90.82)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.017 ( 0.027)	Loss 3.0688e-01 (2.7744e-01)	Acc@1  87.00 ( 90.43)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.021 ( 0.025)	Loss 3.5498e-01 (2.8888e-01)	Acc@1  88.00 ( 90.35)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.022 ( 0.023)	Loss 3.6182e-01 (2.9422e-01)	Acc@1  87.00 ( 90.20)	Acc@5  99.00 ( 99.66)
Test: [ 50/100]	Time  0.018 ( 0.022)	Loss 1.9458e-01 (2.9213e-01)	Acc@1  92.00 ( 90.47)	Acc@5 100.00 ( 99.63)
Test: [ 60/100]	Time  0.017 ( 0.022)	Loss 3.1641e-01 (2.9284e-01)	Acc@1  93.00 ( 90.36)	Acc@5 100.00 ( 99.69)
Test: [ 70/100]	Time  0.017 ( 0.021)	Loss 3.1445e-01 (2.8720e-01)	Acc@1  89.00 ( 90.44)	Acc@5 100.00 ( 99.70)
Test: [ 80/100]	Time  0.025 ( 0.021)	Loss 1.7468e-01 (2.8948e-01)	Acc@1  93.00 ( 90.37)	Acc@5 100.00 ( 99.72)
Test: [ 90/100]	Time  0.019 ( 0.021)	Loss 1.9958e-01 (2.8800e-01)	Acc@1  94.00 ( 90.38)	Acc@5 100.00 ( 99.73)
 * Acc@1 90.360 Acc@5 99.730
### epoch[31] execution time: 18.06959557533264
EPOCH 32
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [32][  0/391]	Time  0.219 ( 0.219)	Data  0.176 ( 0.176)	Loss 2.2791e-01 (2.2791e-01)	Acc@1  92.19 ( 92.19)	Acc@5  99.22 ( 99.22)
Epoch: [32][ 10/391]	Time  0.042 ( 0.057)	Data  0.001 ( 0.017)	Loss 1.4136e-01 (1.5871e-01)	Acc@1  95.31 ( 95.17)	Acc@5 100.00 ( 99.86)
Epoch: [32][ 20/391]	Time  0.038 ( 0.050)	Data  0.001 ( 0.009)	Loss 1.1224e-01 (1.6702e-01)	Acc@1  96.88 ( 94.68)	Acc@5 100.00 ( 99.89)
Epoch: [32][ 30/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.007)	Loss 1.6833e-01 (1.6743e-01)	Acc@1  93.75 ( 94.48)	Acc@5 100.00 ( 99.90)
Epoch: [32][ 40/391]	Time  0.038 ( 0.045)	Data  0.001 ( 0.005)	Loss 2.0557e-01 (1.7259e-01)	Acc@1  94.53 ( 94.38)	Acc@5  99.22 ( 99.85)
Epoch: [32][ 50/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.3062e-01 (1.7160e-01)	Acc@1  96.09 ( 94.24)	Acc@5 100.00 ( 99.88)
Epoch: [32][ 60/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.004)	Loss 2.1753e-01 (1.7218e-01)	Acc@1  91.41 ( 94.28)	Acc@5 100.00 ( 99.90)
Epoch: [32][ 70/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.8591e-01 (1.7417e-01)	Acc@1  93.75 ( 94.21)	Acc@5 100.00 ( 99.90)
Epoch: [32][ 80/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.3926e-01 (1.7576e-01)	Acc@1  91.41 ( 94.14)	Acc@5 100.00 ( 99.89)
Epoch: [32][ 90/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.9812e-01 (1.7419e-01)	Acc@1  94.53 ( 94.23)	Acc@5 100.00 ( 99.91)
Epoch: [32][100/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.5784e-01 (1.7410e-01)	Acc@1  93.75 ( 94.25)	Acc@5 100.00 ( 99.90)
Epoch: [32][110/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.7224e-01 (1.7454e-01)	Acc@1  94.53 ( 94.19)	Acc@5 100.00 ( 99.91)
Epoch: [32][120/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.9836e-01 (1.7396e-01)	Acc@1  92.19 ( 94.25)	Acc@5 100.00 ( 99.92)
Epoch: [32][130/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3049e-01 (1.7104e-01)	Acc@1  97.66 ( 94.41)	Acc@5 100.00 ( 99.92)
Epoch: [32][140/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.4158e-01 (1.7194e-01)	Acc@1  92.19 ( 94.32)	Acc@5 100.00 ( 99.92)
Epoch: [32][150/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5051e-01 (1.7243e-01)	Acc@1  95.31 ( 94.28)	Acc@5 100.00 ( 99.91)
Epoch: [32][160/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.0142e-01 (1.7202e-01)	Acc@1  92.97 ( 94.31)	Acc@5 100.00 ( 99.91)
Epoch: [32][170/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5796e-01 (1.7162e-01)	Acc@1  96.09 ( 94.33)	Acc@5 100.00 ( 99.92)
Epoch: [32][180/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.1948e-01 (1.7409e-01)	Acc@1  92.97 ( 94.24)	Acc@5 100.00 ( 99.92)
Epoch: [32][190/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.9243e-02 (1.7454e-01)	Acc@1  98.44 ( 94.20)	Acc@5 100.00 ( 99.91)
Epoch: [32][200/391]	Time  0.035 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.8591e-01 (1.7504e-01)	Acc@1  92.97 ( 94.13)	Acc@5 100.00 ( 99.91)
Epoch: [32][210/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1115e-01 (1.7487e-01)	Acc@1  95.31 ( 94.12)	Acc@5 100.00 ( 99.89)
Epoch: [32][220/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.3938e-01 (1.7491e-01)	Acc@1  91.41 ( 94.13)	Acc@5 100.00 ( 99.90)
Epoch: [32][230/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.4207e-01 (1.7502e-01)	Acc@1  91.41 ( 94.11)	Acc@5  99.22 ( 99.90)
Epoch: [32][240/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.1646e-01 (1.7488e-01)	Acc@1  96.09 ( 94.08)	Acc@5 100.00 ( 99.90)
Epoch: [32][250/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.4282e-01 (1.7490e-01)	Acc@1  95.31 ( 94.10)	Acc@5 100.00 ( 99.90)
Epoch: [32][260/391]	Time  0.045 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.3232e-01 (1.7501e-01)	Acc@1  92.97 ( 94.08)	Acc@5 100.00 ( 99.90)
Epoch: [32][270/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.6772e-01 (1.7473e-01)	Acc@1  92.97 ( 94.11)	Acc@5 100.00 ( 99.90)
Epoch: [32][280/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.4868e-01 (1.7487e-01)	Acc@1  95.31 ( 94.09)	Acc@5 100.00 ( 99.90)
Epoch: [32][290/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.9336e-01 (1.7461e-01)	Acc@1  96.09 ( 94.09)	Acc@5 100.00 ( 99.91)
Epoch: [32][300/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.2720e-01 (1.7435e-01)	Acc@1  96.09 ( 94.10)	Acc@5 100.00 ( 99.91)
Epoch: [32][310/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.6541e-01 (1.7397e-01)	Acc@1  94.53 ( 94.12)	Acc@5 100.00 ( 99.91)
Epoch: [32][320/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.4404e-01 (1.7395e-01)	Acc@1  96.09 ( 94.11)	Acc@5 100.00 ( 99.91)
Epoch: [32][330/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.5930e-01 (1.7473e-01)	Acc@1  95.31 ( 94.08)	Acc@5 100.00 ( 99.91)
Epoch: [32][340/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.5918e-01 (1.7446e-01)	Acc@1  95.31 ( 94.07)	Acc@5 100.00 ( 99.91)
Epoch: [32][350/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.8652e-01 (1.7492e-01)	Acc@1  92.97 ( 94.06)	Acc@5 100.00 ( 99.90)
Epoch: [32][360/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.1838e-01 (1.7443e-01)	Acc@1  92.19 ( 94.10)	Acc@5 100.00 ( 99.91)
Epoch: [32][370/391]	Time  0.045 ( 0.040)	Data  0.001 ( 0.001)	Loss 9.6069e-02 (1.7353e-01)	Acc@1  97.66 ( 94.13)	Acc@5 100.00 ( 99.91)
Epoch: [32][380/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.2083e-01 (1.7338e-01)	Acc@1  91.41 ( 94.13)	Acc@5 100.00 ( 99.91)
Epoch: [32][390/391]	Time  0.030 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.4402e-01 (1.7388e-01)	Acc@1  93.75 ( 94.12)	Acc@5 100.00 ( 99.91)
## e[32] optimizer.zero_grad (sum) time: 0.25032711029052734
## e[32]       loss.backward (sum) time: 4.144102573394775
## e[32]      optimizer.step (sum) time: 1.7670645713806152
## epoch[32] training(only) time: 15.867016792297363
# Switched to evaluate mode...
Test: [  0/100]	Time  0.172 ( 0.172)	Loss 2.9517e-01 (2.9517e-01)	Acc@1  93.00 ( 93.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.019 ( 0.033)	Loss 3.0981e-01 (2.6109e-01)	Acc@1  88.00 ( 91.27)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.022 ( 0.028)	Loss 2.9736e-01 (2.8140e-01)	Acc@1  86.00 ( 90.81)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.022 ( 0.025)	Loss 3.8794e-01 (2.9224e-01)	Acc@1  86.00 ( 90.61)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.023 ( 0.024)	Loss 3.5449e-01 (2.9523e-01)	Acc@1  87.00 ( 90.41)	Acc@5  99.00 ( 99.66)
Test: [ 50/100]	Time  0.017 ( 0.023)	Loss 1.8835e-01 (2.9222e-01)	Acc@1  93.00 ( 90.76)	Acc@5 100.00 ( 99.63)
Test: [ 60/100]	Time  0.020 ( 0.023)	Loss 3.1641e-01 (2.9295e-01)	Acc@1  91.00 ( 90.59)	Acc@5 100.00 ( 99.67)
Test: [ 70/100]	Time  0.021 ( 0.022)	Loss 3.6060e-01 (2.8703e-01)	Acc@1  87.00 ( 90.63)	Acc@5 100.00 ( 99.70)
Test: [ 80/100]	Time  0.020 ( 0.022)	Loss 1.6309e-01 (2.8944e-01)	Acc@1  93.00 ( 90.57)	Acc@5 100.00 ( 99.70)
Test: [ 90/100]	Time  0.017 ( 0.022)	Loss 2.1057e-01 (2.8843e-01)	Acc@1  93.00 ( 90.53)	Acc@5 100.00 ( 99.70)
 * Acc@1 90.520 Acc@5 99.710
### epoch[32] execution time: 18.129859924316406
EPOCH 33
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [33][  0/391]	Time  0.222 ( 0.222)	Data  0.173 ( 0.173)	Loss 1.4600e-01 (1.4600e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
Epoch: [33][ 10/391]	Time  0.043 ( 0.057)	Data  0.001 ( 0.017)	Loss 2.2241e-01 (1.4374e-01)	Acc@1  92.19 ( 94.89)	Acc@5 100.00 (100.00)
Epoch: [33][ 20/391]	Time  0.038 ( 0.049)	Data  0.001 ( 0.009)	Loss 1.3721e-01 (1.5847e-01)	Acc@1  95.31 ( 94.46)	Acc@5 100.00 ( 99.93)
Epoch: [33][ 30/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.006)	Loss 1.3806e-01 (1.5739e-01)	Acc@1  96.09 ( 94.68)	Acc@5 100.00 ( 99.90)
Epoch: [33][ 40/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.005)	Loss 1.3916e-01 (1.6006e-01)	Acc@1  96.09 ( 94.66)	Acc@5 100.00 ( 99.92)
Epoch: [33][ 50/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.004)	Loss 2.2363e-01 (1.6645e-01)	Acc@1  93.75 ( 94.49)	Acc@5 100.00 ( 99.94)
Epoch: [33][ 60/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.2537e-01 (1.6399e-01)	Acc@1  94.53 ( 94.61)	Acc@5 100.00 ( 99.94)
Epoch: [33][ 70/391]	Time  0.036 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.3696e-01 (1.6248e-01)	Acc@1  96.88 ( 94.66)	Acc@5 100.00 ( 99.93)
Epoch: [33][ 80/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.2290e-01 (1.6120e-01)	Acc@1  93.75 ( 94.74)	Acc@5  99.22 ( 99.93)
Epoch: [33][ 90/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.3965e-01 (1.6224e-01)	Acc@1  96.09 ( 94.75)	Acc@5 100.00 ( 99.94)
Epoch: [33][100/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.3782e-01 (1.6161e-01)	Acc@1  96.88 ( 94.78)	Acc@5 100.00 ( 99.95)
Epoch: [33][110/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.1511e-01 (1.6225e-01)	Acc@1  98.44 ( 94.76)	Acc@5 100.00 ( 99.94)
Epoch: [33][120/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3013e-01 (1.6442e-01)	Acc@1  96.09 ( 94.66)	Acc@5 100.00 ( 99.94)
Epoch: [33][130/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.3401e-01 (1.6521e-01)	Acc@1  92.97 ( 94.63)	Acc@5 100.00 ( 99.94)
Epoch: [33][140/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3977e-01 (1.6577e-01)	Acc@1  95.31 ( 94.57)	Acc@5 100.00 ( 99.94)
Epoch: [33][150/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1383e-01 (1.6474e-01)	Acc@1  94.53 ( 94.58)	Acc@5 100.00 ( 99.94)
Epoch: [33][160/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.8796e-02 (1.6386e-01)	Acc@1  98.44 ( 94.59)	Acc@5 100.00 ( 99.93)
Epoch: [33][170/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.9861e-01 (1.6542e-01)	Acc@1  93.75 ( 94.53)	Acc@5 100.00 ( 99.92)
Epoch: [33][180/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.6025e-01 (1.6647e-01)	Acc@1  91.41 ( 94.48)	Acc@5 100.00 ( 99.92)
Epoch: [33][190/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.1277e-01 (1.6631e-01)	Acc@1  89.84 ( 94.51)	Acc@5 100.00 ( 99.93)
Epoch: [33][200/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6919e-01 (1.6616e-01)	Acc@1  94.53 ( 94.52)	Acc@5 100.00 ( 99.93)
Epoch: [33][210/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7749e-01 (1.6645e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 ( 99.93)
Epoch: [33][220/391]	Time  0.045 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.5415e-01 (1.6712e-01)	Acc@1  91.41 ( 94.46)	Acc@5 100.00 ( 99.93)
Epoch: [33][230/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6956e-01 (1.6632e-01)	Acc@1  95.31 ( 94.51)	Acc@5 100.00 ( 99.93)
Epoch: [33][240/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.4585e-02 (1.6540e-01)	Acc@1  98.44 ( 94.53)	Acc@5 100.00 ( 99.93)
Epoch: [33][250/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.8994e-01 (1.6601e-01)	Acc@1  90.62 ( 94.48)	Acc@5 100.00 ( 99.93)
Epoch: [33][260/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3269e-01 (1.6644e-01)	Acc@1  94.53 ( 94.46)	Acc@5 100.00 ( 99.93)
Epoch: [33][270/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.002)	Loss 2.0056e-01 (1.6711e-01)	Acc@1  93.75 ( 94.44)	Acc@5 100.00 ( 99.92)
Epoch: [33][280/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.9519e-01 (1.6746e-01)	Acc@1  93.75 ( 94.41)	Acc@5 100.00 ( 99.92)
Epoch: [33][290/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 2.5098e-01 (1.6779e-01)	Acc@1  89.84 ( 94.39)	Acc@5  99.22 ( 99.92)
Epoch: [33][300/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.7017e-01 (1.6742e-01)	Acc@1  94.53 ( 94.40)	Acc@5 100.00 ( 99.92)
Epoch: [33][310/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 2.5684e-01 (1.6752e-01)	Acc@1  92.19 ( 94.39)	Acc@5  99.22 ( 99.92)
Epoch: [33][320/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.3416e-01 (1.6817e-01)	Acc@1  96.09 ( 94.35)	Acc@5 100.00 ( 99.92)
Epoch: [33][330/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.8762e-01 (1.6771e-01)	Acc@1  91.41 ( 94.35)	Acc@5 100.00 ( 99.92)
Epoch: [33][340/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.4758e-01 (1.6730e-01)	Acc@1  95.31 ( 94.35)	Acc@5 100.00 ( 99.91)
Epoch: [33][350/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.7603e-01 (1.6709e-01)	Acc@1  93.75 ( 94.35)	Acc@5 100.00 ( 99.91)
Epoch: [33][360/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.0626e-01 (1.6687e-01)	Acc@1  95.31 ( 94.36)	Acc@5 100.00 ( 99.92)
Epoch: [33][370/391]	Time  0.048 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.3416e-01 (1.6631e-01)	Acc@1  94.53 ( 94.40)	Acc@5 100.00 ( 99.92)
Epoch: [33][380/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.0773e-01 (1.6588e-01)	Acc@1  96.88 ( 94.41)	Acc@5 100.00 ( 99.92)
Epoch: [33][390/391]	Time  0.029 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.2769e-01 (1.6611e-01)	Acc@1  93.75 ( 94.39)	Acc@5 100.00 ( 99.92)
## e[33] optimizer.zero_grad (sum) time: 0.24843430519104004
## e[33]       loss.backward (sum) time: 4.072928428649902
## e[33]      optimizer.step (sum) time: 1.7697081565856934
## epoch[33] training(only) time: 15.829751253128052
# Switched to evaluate mode...
Test: [  0/100]	Time  0.172 ( 0.172)	Loss 2.6367e-01 (2.6367e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.017 ( 0.034)	Loss 3.2959e-01 (2.5876e-01)	Acc@1  89.00 ( 91.45)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.023 ( 0.027)	Loss 3.3838e-01 (2.7870e-01)	Acc@1  88.00 ( 90.95)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.023 ( 0.025)	Loss 3.5352e-01 (2.8835e-01)	Acc@1  87.00 ( 90.71)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.018 ( 0.024)	Loss 3.7061e-01 (2.9301e-01)	Acc@1  87.00 ( 90.61)	Acc@5  99.00 ( 99.71)
Test: [ 50/100]	Time  0.021 ( 0.023)	Loss 1.9263e-01 (2.9136e-01)	Acc@1  93.00 ( 90.90)	Acc@5 100.00 ( 99.69)
Test: [ 60/100]	Time  0.022 ( 0.023)	Loss 3.0762e-01 (2.9148e-01)	Acc@1  93.00 ( 90.74)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.022 ( 0.022)	Loss 3.5303e-01 (2.8467e-01)	Acc@1  88.00 ( 90.63)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.022 ( 0.022)	Loss 1.5881e-01 (2.8808e-01)	Acc@1  93.00 ( 90.54)	Acc@5 100.00 ( 99.73)
Test: [ 90/100]	Time  0.019 ( 0.022)	Loss 2.0789e-01 (2.8768e-01)	Acc@1  93.00 ( 90.49)	Acc@5 100.00 ( 99.73)
 * Acc@1 90.480 Acc@5 99.720
### epoch[33] execution time: 18.143704891204834
EPOCH 34
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [34][  0/391]	Time  0.209 ( 0.209)	Data  0.167 ( 0.167)	Loss 1.4844e-01 (1.4844e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
Epoch: [34][ 10/391]	Time  0.045 ( 0.056)	Data  0.001 ( 0.016)	Loss 1.1023e-01 (1.7999e-01)	Acc@1  96.88 ( 94.11)	Acc@5 100.00 ( 99.79)
Epoch: [34][ 20/391]	Time  0.041 ( 0.049)	Data  0.001 ( 0.009)	Loss 1.6772e-01 (1.7041e-01)	Acc@1  92.97 ( 94.46)	Acc@5 100.00 ( 99.81)
Epoch: [34][ 30/391]	Time  0.043 ( 0.046)	Data  0.001 ( 0.006)	Loss 1.1072e-01 (1.6286e-01)	Acc@1  96.09 ( 94.71)	Acc@5 100.00 ( 99.85)
Epoch: [34][ 40/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.005)	Loss 1.1841e-01 (1.6264e-01)	Acc@1  95.31 ( 94.49)	Acc@5 100.00 ( 99.89)
Epoch: [34][ 50/391]	Time  0.034 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.7542e-01 (1.6588e-01)	Acc@1  93.75 ( 94.38)	Acc@5 100.00 ( 99.89)
Epoch: [34][ 60/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.004)	Loss 8.8013e-02 (1.5967e-01)	Acc@1  96.88 ( 94.51)	Acc@5 100.00 ( 99.91)
Epoch: [34][ 70/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.6248e-01 (1.5967e-01)	Acc@1  93.75 ( 94.54)	Acc@5 100.00 ( 99.91)
Epoch: [34][ 80/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.0925e-01 (1.5746e-01)	Acc@1  97.66 ( 94.64)	Acc@5 100.00 ( 99.92)
Epoch: [34][ 90/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.4160e-01 (1.5640e-01)	Acc@1  96.09 ( 94.64)	Acc@5 100.00 ( 99.93)
Epoch: [34][100/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.003)	Loss 2.2925e-01 (1.5579e-01)	Acc@1  92.19 ( 94.66)	Acc@5 100.00 ( 99.94)
Epoch: [34][110/391]	Time  0.050 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6687e-01 (1.5783e-01)	Acc@1  93.75 ( 94.58)	Acc@5 100.00 ( 99.94)
Epoch: [34][120/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.5977e-01 (1.5923e-01)	Acc@1  92.19 ( 94.56)	Acc@5 100.00 ( 99.94)
Epoch: [34][130/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4941e-01 (1.5787e-01)	Acc@1  96.09 ( 94.69)	Acc@5 100.00 ( 99.94)
Epoch: [34][140/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3611e-01 (1.5960e-01)	Acc@1  95.31 ( 94.63)	Acc@5 100.00 ( 99.94)
Epoch: [34][150/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.4294e-01 (1.6133e-01)	Acc@1  95.31 ( 94.55)	Acc@5 100.00 ( 99.95)
Epoch: [34][160/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.6040e-01 (1.6128e-01)	Acc@1  96.09 ( 94.55)	Acc@5 100.00 ( 99.94)
Epoch: [34][170/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.0046e-01 (1.6028e-01)	Acc@1  96.88 ( 94.59)	Acc@5 100.00 ( 99.94)
Epoch: [34][180/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 2.2900e-01 (1.6009e-01)	Acc@1  89.84 ( 94.57)	Acc@5 100.00 ( 99.94)
Epoch: [34][190/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.8469e-01 (1.6010e-01)	Acc@1  93.75 ( 94.57)	Acc@5 100.00 ( 99.94)
Epoch: [34][200/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.4917e-01 (1.6068e-01)	Acc@1  93.75 ( 94.52)	Acc@5 100.00 ( 99.94)
Epoch: [34][210/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.0504e-01 (1.5977e-01)	Acc@1  96.09 ( 94.56)	Acc@5 100.00 ( 99.94)
Epoch: [34][220/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.4612e-01 (1.5915e-01)	Acc@1  93.75 ( 94.56)	Acc@5 100.00 ( 99.94)
Epoch: [34][230/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.4880e-01 (1.5976e-01)	Acc@1  93.75 ( 94.53)	Acc@5 100.00 ( 99.94)
Epoch: [34][240/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.6248e-01 (1.6002e-01)	Acc@1  93.75 ( 94.52)	Acc@5 100.00 ( 99.94)
Epoch: [34][250/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.8176e-01 (1.5966e-01)	Acc@1  93.75 ( 94.52)	Acc@5 100.00 ( 99.94)
Epoch: [34][260/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.8286e-01 (1.6010e-01)	Acc@1  94.53 ( 94.50)	Acc@5  99.22 ( 99.93)
Epoch: [34][270/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 8.5632e-02 (1.6025e-01)	Acc@1  97.66 ( 94.51)	Acc@5 100.00 ( 99.93)
Epoch: [34][280/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 2.3877e-01 (1.6133e-01)	Acc@1  92.19 ( 94.46)	Acc@5 100.00 ( 99.93)
Epoch: [34][290/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.5637e-01 (1.6131e-01)	Acc@1  94.53 ( 94.46)	Acc@5 100.00 ( 99.93)
Epoch: [34][300/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.4392e-01 (1.6154e-01)	Acc@1  96.88 ( 94.46)	Acc@5 100.00 ( 99.93)
Epoch: [34][310/391]	Time  0.043 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.0496e-01 (1.6179e-01)	Acc@1  93.75 ( 94.46)	Acc@5 100.00 ( 99.93)
Epoch: [34][320/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.1713e-01 (1.6095e-01)	Acc@1  97.66 ( 94.48)	Acc@5 100.00 ( 99.93)
Epoch: [34][330/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.8774e-01 (1.6081e-01)	Acc@1  95.31 ( 94.48)	Acc@5 100.00 ( 99.93)
Epoch: [34][340/391]	Time  0.048 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.1743e-01 (1.6106e-01)	Acc@1  96.88 ( 94.48)	Acc@5 100.00 ( 99.93)
Epoch: [34][350/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.6797e-01 (1.6099e-01)	Acc@1  92.19 ( 94.47)	Acc@5 100.00 ( 99.93)
Epoch: [34][360/391]	Time  0.043 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.9006e-01 (1.6215e-01)	Acc@1  92.97 ( 94.42)	Acc@5 100.00 ( 99.93)
Epoch: [34][370/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.8005e-01 (1.6215e-01)	Acc@1  92.19 ( 94.43)	Acc@5 100.00 ( 99.92)
Epoch: [34][380/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.2207e-01 (1.6194e-01)	Acc@1  95.31 ( 94.44)	Acc@5 100.00 ( 99.92)
Epoch: [34][390/391]	Time  0.029 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.9666e-01 (1.6179e-01)	Acc@1  93.75 ( 94.43)	Acc@5 100.00 ( 99.92)
## e[34] optimizer.zero_grad (sum) time: 0.25040268898010254
## e[34]       loss.backward (sum) time: 4.108824968338013
## e[34]      optimizer.step (sum) time: 1.772092342376709
## epoch[34] training(only) time: 15.814221143722534
# Switched to evaluate mode...
Test: [  0/100]	Time  0.177 ( 0.177)	Loss 2.6733e-01 (2.6733e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.017 ( 0.035)	Loss 3.0493e-01 (2.6075e-01)	Acc@1  91.00 ( 91.36)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.017 ( 0.027)	Loss 3.2715e-01 (2.7927e-01)	Acc@1  88.00 ( 91.14)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.021 ( 0.025)	Loss 3.6914e-01 (2.9065e-01)	Acc@1  87.00 ( 90.71)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.023 ( 0.024)	Loss 3.7378e-01 (2.9285e-01)	Acc@1  88.00 ( 90.63)	Acc@5  99.00 ( 99.71)
Test: [ 50/100]	Time  0.023 ( 0.023)	Loss 1.9312e-01 (2.8995e-01)	Acc@1  94.00 ( 90.94)	Acc@5 100.00 ( 99.67)
Test: [ 60/100]	Time  0.017 ( 0.023)	Loss 3.4009e-01 (2.8907e-01)	Acc@1  92.00 ( 90.79)	Acc@5 100.00 ( 99.72)
Test: [ 70/100]	Time  0.020 ( 0.022)	Loss 3.1934e-01 (2.8267e-01)	Acc@1  88.00 ( 90.79)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.017 ( 0.022)	Loss 1.5942e-01 (2.8552e-01)	Acc@1  92.00 ( 90.68)	Acc@5 100.00 ( 99.72)
Test: [ 90/100]	Time  0.020 ( 0.022)	Loss 2.0740e-01 (2.8432e-01)	Acc@1  93.00 ( 90.66)	Acc@5 100.00 ( 99.71)
 * Acc@1 90.700 Acc@5 99.720
### epoch[34] execution time: 18.059054613113403
EPOCH 35
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [35][  0/391]	Time  0.208 ( 0.208)	Data  0.164 ( 0.164)	Loss 2.2620e-01 (2.2620e-01)	Acc@1  92.97 ( 92.97)	Acc@5  99.22 ( 99.22)
Epoch: [35][ 10/391]	Time  0.040 ( 0.054)	Data  0.001 ( 0.016)	Loss 1.1603e-01 (1.4426e-01)	Acc@1  97.66 ( 95.67)	Acc@5 100.00 ( 99.93)
Epoch: [35][ 20/391]	Time  0.038 ( 0.047)	Data  0.001 ( 0.009)	Loss 1.4453e-01 (1.5673e-01)	Acc@1  94.53 ( 94.90)	Acc@5 100.00 ( 99.89)
Epoch: [35][ 30/391]	Time  0.038 ( 0.045)	Data  0.001 ( 0.006)	Loss 2.1619e-01 (1.5821e-01)	Acc@1  94.53 ( 94.68)	Acc@5 100.00 ( 99.90)
Epoch: [35][ 40/391]	Time  0.037 ( 0.044)	Data  0.001 ( 0.005)	Loss 1.7432e-01 (1.5597e-01)	Acc@1  95.31 ( 94.66)	Acc@5 100.00 ( 99.90)
Epoch: [35][ 50/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.7969e-01 (1.5248e-01)	Acc@1  94.53 ( 94.93)	Acc@5  99.22 ( 99.89)
Epoch: [35][ 60/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.3123e-01 (1.4980e-01)	Acc@1  94.53 ( 95.08)	Acc@5 100.00 ( 99.90)
Epoch: [35][ 70/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.5320e-01 (1.4987e-01)	Acc@1  93.75 ( 95.05)	Acc@5 100.00 ( 99.91)
Epoch: [35][ 80/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.1310e-01 (1.4713e-01)	Acc@1  95.31 ( 95.14)	Acc@5 100.00 ( 99.92)
Epoch: [35][ 90/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 9.8938e-02 (1.4880e-01)	Acc@1  96.88 ( 95.02)	Acc@5 100.00 ( 99.93)
Epoch: [35][100/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.003)	Loss 1.0968e-01 (1.4808e-01)	Acc@1  96.88 ( 95.03)	Acc@5 100.00 ( 99.94)
Epoch: [35][110/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.8359e-01 (1.4823e-01)	Acc@1  94.53 ( 95.07)	Acc@5  99.22 ( 99.92)
Epoch: [35][120/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.8518e-01 (1.5007e-01)	Acc@1  92.19 ( 94.96)	Acc@5 100.00 ( 99.93)
Epoch: [35][130/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1157e-01 (1.4972e-01)	Acc@1  96.88 ( 94.92)	Acc@5 100.00 ( 99.93)
Epoch: [35][140/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.9775e-01 (1.4960e-01)	Acc@1  93.75 ( 94.92)	Acc@5  99.22 ( 99.93)
Epoch: [35][150/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.6670e-02 (1.5043e-01)	Acc@1  98.44 ( 94.92)	Acc@5 100.00 ( 99.93)
Epoch: [35][160/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7480e-01 (1.4997e-01)	Acc@1  92.97 ( 94.91)	Acc@5 100.00 ( 99.93)
Epoch: [35][170/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7773e-01 (1.5113e-01)	Acc@1  94.53 ( 94.89)	Acc@5 100.00 ( 99.93)
Epoch: [35][180/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3452e-01 (1.5114e-01)	Acc@1  95.31 ( 94.88)	Acc@5 100.00 ( 99.94)
Epoch: [35][190/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.9600e-02 (1.5145e-01)	Acc@1  97.66 ( 94.87)	Acc@5 100.00 ( 99.94)
Epoch: [35][200/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4807e-01 (1.5144e-01)	Acc@1  96.09 ( 94.86)	Acc@5 100.00 ( 99.94)
Epoch: [35][210/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4075e-01 (1.5023e-01)	Acc@1  96.09 ( 94.92)	Acc@5 100.00 ( 99.94)
Epoch: [35][220/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4246e-01 (1.4962e-01)	Acc@1  95.31 ( 94.94)	Acc@5 100.00 ( 99.94)
Epoch: [35][230/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6052e-01 (1.5059e-01)	Acc@1  96.09 ( 94.90)	Acc@5 100.00 ( 99.94)
Epoch: [35][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4612e-01 (1.5179e-01)	Acc@1  94.53 ( 94.87)	Acc@5 100.00 ( 99.94)
Epoch: [35][250/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4990e-01 (1.5229e-01)	Acc@1  94.53 ( 94.85)	Acc@5 100.00 ( 99.94)
Epoch: [35][260/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.8250e-01 (1.5217e-01)	Acc@1  92.97 ( 94.86)	Acc@5 100.00 ( 99.94)
Epoch: [35][270/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2878e-01 (1.5388e-01)	Acc@1  96.88 ( 94.79)	Acc@5 100.00 ( 99.93)
Epoch: [35][280/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.5513e-01 (1.5423e-01)	Acc@1  92.19 ( 94.78)	Acc@5 100.00 ( 99.93)
Epoch: [35][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.5049e-01 (1.5469e-01)	Acc@1  91.41 ( 94.75)	Acc@5 100.00 ( 99.94)
Epoch: [35][300/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.3416e-01 (1.5394e-01)	Acc@1  95.31 ( 94.78)	Acc@5 100.00 ( 99.94)
Epoch: [35][310/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.9519e-01 (1.5590e-01)	Acc@1  93.75 ( 94.71)	Acc@5 100.00 ( 99.93)
Epoch: [35][320/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.4282e-01 (1.5505e-01)	Acc@1  96.88 ( 94.75)	Acc@5 100.00 ( 99.93)
Epoch: [35][330/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.3062e-01 (1.5540e-01)	Acc@1  96.09 ( 94.73)	Acc@5 100.00 ( 99.93)
Epoch: [35][340/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.6309e-01 (1.5612e-01)	Acc@1  95.31 ( 94.69)	Acc@5  99.22 ( 99.93)
Epoch: [35][350/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.6162e-01 (1.5589e-01)	Acc@1  94.53 ( 94.70)	Acc@5 100.00 ( 99.93)
Epoch: [35][360/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 9.6436e-02 (1.5525e-01)	Acc@1  96.88 ( 94.71)	Acc@5 100.00 ( 99.94)
Epoch: [35][370/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.1143e-01 (1.5552e-01)	Acc@1  93.75 ( 94.70)	Acc@5 100.00 ( 99.94)
Epoch: [35][380/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.4355e-01 (1.5546e-01)	Acc@1  94.53 ( 94.69)	Acc@5 100.00 ( 99.94)
Epoch: [35][390/391]	Time  0.025 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.2461e-01 (1.5581e-01)	Acc@1  88.75 ( 94.67)	Acc@5 100.00 ( 99.94)
## e[35] optimizer.zero_grad (sum) time: 0.24914908409118652
## e[35]       loss.backward (sum) time: 4.2032904624938965
## e[35]      optimizer.step (sum) time: 1.699061632156372
## epoch[35] training(only) time: 15.912533521652222
# Switched to evaluate mode...
Test: [  0/100]	Time  0.165 ( 0.165)	Loss 2.5562e-01 (2.5562e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.017 ( 0.034)	Loss 2.8882e-01 (2.6783e-01)	Acc@1  88.00 ( 91.18)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.019 ( 0.027)	Loss 3.4277e-01 (2.9020e-01)	Acc@1  86.00 ( 90.67)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.023 ( 0.025)	Loss 3.8550e-01 (2.9690e-01)	Acc@1  86.00 ( 90.52)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.018 ( 0.024)	Loss 4.0674e-01 (3.0030e-01)	Acc@1  85.00 ( 90.32)	Acc@5  99.00 ( 99.71)
Test: [ 50/100]	Time  0.021 ( 0.023)	Loss 2.0105e-01 (2.9721e-01)	Acc@1  92.00 ( 90.69)	Acc@5 100.00 ( 99.65)
Test: [ 60/100]	Time  0.020 ( 0.023)	Loss 3.4424e-01 (2.9513e-01)	Acc@1  92.00 ( 90.61)	Acc@5 100.00 ( 99.70)
Test: [ 70/100]	Time  0.022 ( 0.023)	Loss 3.4106e-01 (2.8948e-01)	Acc@1  90.00 ( 90.61)	Acc@5 100.00 ( 99.72)
Test: [ 80/100]	Time  0.018 ( 0.022)	Loss 1.5125e-01 (2.9339e-01)	Acc@1  94.00 ( 90.47)	Acc@5 100.00 ( 99.70)
Test: [ 90/100]	Time  0.019 ( 0.022)	Loss 1.8481e-01 (2.9271e-01)	Acc@1  93.00 ( 90.47)	Acc@5 100.00 ( 99.70)
 * Acc@1 90.530 Acc@5 99.710
### epoch[35] execution time: 18.220272302627563
EPOCH 36
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [36][  0/391]	Time  0.238 ( 0.238)	Data  0.191 ( 0.191)	Loss 1.5405e-01 (1.5405e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
Epoch: [36][ 10/391]	Time  0.038 ( 0.058)	Data  0.001 ( 0.018)	Loss 1.4453e-01 (1.4394e-01)	Acc@1  96.88 ( 95.24)	Acc@5 100.00 (100.00)
Epoch: [36][ 20/391]	Time  0.040 ( 0.050)	Data  0.001 ( 0.010)	Loss 1.3660e-01 (1.4929e-01)	Acc@1  94.53 ( 94.87)	Acc@5 100.00 ( 99.96)
Epoch: [36][ 30/391]	Time  0.041 ( 0.047)	Data  0.001 ( 0.007)	Loss 8.5876e-02 (1.4578e-01)	Acc@1  97.66 ( 95.26)	Acc@5 100.00 ( 99.95)
Epoch: [36][ 40/391]	Time  0.038 ( 0.045)	Data  0.001 ( 0.006)	Loss 1.0461e-01 (1.3887e-01)	Acc@1  96.09 ( 95.48)	Acc@5 100.00 ( 99.94)
Epoch: [36][ 50/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.005)	Loss 1.2262e-01 (1.3948e-01)	Acc@1  96.88 ( 95.45)	Acc@5 100.00 ( 99.95)
Epoch: [36][ 60/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.6101e-01 (1.4508e-01)	Acc@1  93.75 ( 95.17)	Acc@5 100.00 ( 99.95)
Epoch: [36][ 70/391]	Time  0.036 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.4124e-01 (1.4387e-01)	Acc@1  94.53 ( 95.14)	Acc@5 100.00 ( 99.94)
Epoch: [36][ 80/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.0449e-01 (1.4421e-01)	Acc@1  96.09 ( 95.15)	Acc@5 100.00 ( 99.94)
Epoch: [36][ 90/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.8616e-01 (1.4280e-01)	Acc@1  92.97 ( 95.22)	Acc@5 100.00 ( 99.95)
Epoch: [36][100/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.6233e-02 (1.4228e-01)	Acc@1  97.66 ( 95.27)	Acc@5 100.00 ( 99.94)
Epoch: [36][110/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.8005e-01 (1.4393e-01)	Acc@1  94.53 ( 95.18)	Acc@5 100.00 ( 99.94)
Epoch: [36][120/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.3269e-01 (1.4433e-01)	Acc@1  93.75 ( 95.15)	Acc@5 100.00 ( 99.94)
Epoch: [36][130/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.1167e-01 (1.4443e-01)	Acc@1  91.41 ( 95.11)	Acc@5 100.00 ( 99.94)
Epoch: [36][140/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.8030e-01 (1.4559e-01)	Acc@1  93.75 ( 95.05)	Acc@5 100.00 ( 99.94)
Epoch: [36][150/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.8296e-01 (1.4707e-01)	Acc@1  92.19 ( 94.97)	Acc@5 100.00 ( 99.94)
Epoch: [36][160/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5247e-01 (1.4677e-01)	Acc@1  95.31 ( 94.99)	Acc@5 100.00 ( 99.94)
Epoch: [36][170/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.8689e-01 (1.4917e-01)	Acc@1  93.75 ( 94.91)	Acc@5 100.00 ( 99.95)
Epoch: [36][180/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0309e-01 (1.4883e-01)	Acc@1  95.31 ( 94.88)	Acc@5 100.00 ( 99.95)
Epoch: [36][190/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.9243e-02 (1.4766e-01)	Acc@1  94.53 ( 94.92)	Acc@5 100.00 ( 99.95)
Epoch: [36][200/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7261e-01 (1.4711e-01)	Acc@1  94.53 ( 94.94)	Acc@5 100.00 ( 99.95)
Epoch: [36][210/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1041e-01 (1.4612e-01)	Acc@1  97.66 ( 94.97)	Acc@5 100.00 ( 99.96)
Epoch: [36][220/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1145e-01 (1.4695e-01)	Acc@1  94.53 ( 94.93)	Acc@5 100.00 ( 99.95)
Epoch: [36][230/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4355e-01 (1.4706e-01)	Acc@1  94.53 ( 94.93)	Acc@5 100.00 ( 99.95)
Epoch: [36][240/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0242e-01 (1.4768e-01)	Acc@1  99.22 ( 94.94)	Acc@5 100.00 ( 99.95)
Epoch: [36][250/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1389e-01 (1.4701e-01)	Acc@1  94.53 ( 94.96)	Acc@5 100.00 ( 99.95)
Epoch: [36][260/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5649e-01 (1.4656e-01)	Acc@1  94.53 ( 94.98)	Acc@5 100.00 ( 99.95)
Epoch: [36][270/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4233e-01 (1.4689e-01)	Acc@1  94.53 ( 94.98)	Acc@5 100.00 ( 99.95)
Epoch: [36][280/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3525e-01 (1.4719e-01)	Acc@1  96.09 ( 94.95)	Acc@5 100.00 ( 99.95)
Epoch: [36][290/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0852e-01 (1.4731e-01)	Acc@1  98.44 ( 94.93)	Acc@5 100.00 ( 99.95)
Epoch: [36][300/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0822e-01 (1.4739e-01)	Acc@1  96.09 ( 94.92)	Acc@5 100.00 ( 99.95)
Epoch: [36][310/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.9385e-01 (1.4887e-01)	Acc@1  92.97 ( 94.82)	Acc@5 100.00 ( 99.94)
Epoch: [36][320/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.2805e-01 (1.4925e-01)	Acc@1  95.31 ( 94.80)	Acc@5 100.00 ( 99.94)
Epoch: [36][330/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.3611e-01 (1.4882e-01)	Acc@1  96.88 ( 94.82)	Acc@5 100.00 ( 99.94)
Epoch: [36][340/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.7554e-01 (1.4871e-01)	Acc@1  92.19 ( 94.81)	Acc@5 100.00 ( 99.95)
Epoch: [36][350/391]	Time  0.038 ( 0.040)	Data  0.002 ( 0.001)	Loss 1.7749e-01 (1.4895e-01)	Acc@1  94.53 ( 94.83)	Acc@5 100.00 ( 99.95)
Epoch: [36][360/391]	Time  0.039 ( 0.040)	Data  0.000 ( 0.001)	Loss 1.1566e-01 (1.4886e-01)	Acc@1  96.09 ( 94.83)	Acc@5 100.00 ( 99.95)
Epoch: [36][370/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.3523e-01 (1.4913e-01)	Acc@1  90.62 ( 94.82)	Acc@5 100.00 ( 99.95)
Epoch: [36][380/391]	Time  0.044 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.4817e-01 (1.4906e-01)	Acc@1  91.41 ( 94.83)	Acc@5 100.00 ( 99.95)
Epoch: [36][390/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.7749e-01 (1.4942e-01)	Acc@1  96.25 ( 94.83)	Acc@5 100.00 ( 99.95)
## e[36] optimizer.zero_grad (sum) time: 0.24997425079345703
## e[36]       loss.backward (sum) time: 4.181022644042969
## e[36]      optimizer.step (sum) time: 1.7598230838775635
## epoch[36] training(only) time: 15.889315605163574
# Switched to evaluate mode...
Test: [  0/100]	Time  0.170 ( 0.170)	Loss 2.7417e-01 (2.7417e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.021 ( 0.034)	Loss 3.1543e-01 (2.6344e-01)	Acc@1  90.00 ( 91.55)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.017 ( 0.028)	Loss 3.3740e-01 (2.8670e-01)	Acc@1  86.00 ( 90.81)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.019 ( 0.025)	Loss 3.9429e-01 (2.9556e-01)	Acc@1  84.00 ( 90.61)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.021 ( 0.024)	Loss 3.3203e-01 (2.9663e-01)	Acc@1  90.00 ( 90.46)	Acc@5  99.00 ( 99.73)
Test: [ 50/100]	Time  0.019 ( 0.023)	Loss 1.7334e-01 (2.9365e-01)	Acc@1  94.00 ( 90.82)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.017 ( 0.023)	Loss 3.5352e-01 (2.9399e-01)	Acc@1  90.00 ( 90.72)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.020 ( 0.023)	Loss 3.6304e-01 (2.8775e-01)	Acc@1  86.00 ( 90.72)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.020 ( 0.022)	Loss 1.5637e-01 (2.9215e-01)	Acc@1  94.00 ( 90.58)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.021 ( 0.022)	Loss 2.1375e-01 (2.9079e-01)	Acc@1  95.00 ( 90.65)	Acc@5 100.00 ( 99.75)
 * Acc@1 90.750 Acc@5 99.760
### epoch[36] execution time: 18.222697734832764
EPOCH 37
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [37][  0/391]	Time  0.222 ( 0.222)	Data  0.180 ( 0.180)	Loss 1.9165e-01 (1.9165e-01)	Acc@1  92.97 ( 92.97)	Acc@5 100.00 (100.00)
Epoch: [37][ 10/391]	Time  0.041 ( 0.057)	Data  0.001 ( 0.017)	Loss 1.4258e-01 (1.4205e-01)	Acc@1  96.09 ( 95.17)	Acc@5 100.00 ( 99.93)
Epoch: [37][ 20/391]	Time  0.042 ( 0.048)	Data  0.001 ( 0.009)	Loss 8.4656e-02 (1.3374e-01)	Acc@1  97.66 ( 95.54)	Acc@5 100.00 ( 99.96)
Epoch: [37][ 30/391]	Time  0.042 ( 0.045)	Data  0.001 ( 0.007)	Loss 1.1957e-01 (1.4218e-01)	Acc@1  95.31 ( 95.36)	Acc@5 100.00 ( 99.95)
Epoch: [37][ 40/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.005)	Loss 2.0020e-01 (1.3939e-01)	Acc@1  90.62 ( 95.27)	Acc@5 100.00 ( 99.94)
Epoch: [37][ 50/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 5.4993e-02 (1.3912e-01)	Acc@1  97.66 ( 95.34)	Acc@5 100.00 ( 99.95)
Epoch: [37][ 60/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.7517e-01 (1.4351e-01)	Acc@1  95.31 ( 95.16)	Acc@5  99.22 ( 99.94)
Epoch: [37][ 70/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.7664e-01 (1.4539e-01)	Acc@1  96.09 ( 95.14)	Acc@5 100.00 ( 99.94)
Epoch: [37][ 80/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.2720e-01 (1.4692e-01)	Acc@1  94.53 ( 94.95)	Acc@5 100.00 ( 99.94)
Epoch: [37][ 90/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.1615e-01 (1.4626e-01)	Acc@1  95.31 ( 94.88)	Acc@5 100.00 ( 99.95)
Epoch: [37][100/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.003)	Loss 1.4514e-01 (1.4463e-01)	Acc@1  95.31 ( 94.93)	Acc@5 100.00 ( 99.95)
Epoch: [37][110/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.1665e-02 (1.4504e-01)	Acc@1  97.66 ( 94.90)	Acc@5 100.00 ( 99.95)
Epoch: [37][120/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.8074e-02 (1.4379e-01)	Acc@1  96.88 ( 94.98)	Acc@5 100.00 ( 99.95)
Epoch: [37][130/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1328e-01 (1.4208e-01)	Acc@1  97.66 ( 95.04)	Acc@5 100.00 ( 99.95)
Epoch: [37][140/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5637e-01 (1.4243e-01)	Acc@1  95.31 ( 95.02)	Acc@5 100.00 ( 99.96)
Epoch: [37][150/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6443e-01 (1.4263e-01)	Acc@1  94.53 ( 95.02)	Acc@5 100.00 ( 99.96)
Epoch: [37][160/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0645e-01 (1.4363e-01)	Acc@1  96.09 ( 95.00)	Acc@5 100.00 ( 99.95)
Epoch: [37][170/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.5073e-01 (1.4384e-01)	Acc@1  90.62 ( 94.99)	Acc@5 100.00 ( 99.95)
Epoch: [37][180/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6724e-01 (1.4331e-01)	Acc@1  97.66 ( 95.03)	Acc@5 100.00 ( 99.95)
Epoch: [37][190/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4722e-01 (1.4361e-01)	Acc@1  95.31 ( 95.02)	Acc@5 100.00 ( 99.95)
Epoch: [37][200/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1884e-01 (1.4288e-01)	Acc@1  97.66 ( 95.07)	Acc@5  99.22 ( 99.95)
Epoch: [37][210/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5002e-01 (1.4262e-01)	Acc@1  92.97 ( 95.07)	Acc@5 100.00 ( 99.95)
Epoch: [37][220/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6199e-01 (1.4286e-01)	Acc@1  95.31 ( 95.07)	Acc@5 100.00 ( 99.95)
Epoch: [37][230/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5137e-01 (1.4269e-01)	Acc@1  96.09 ( 95.10)	Acc@5 100.00 ( 99.95)
Epoch: [37][240/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7896e-01 (1.4296e-01)	Acc@1  95.31 ( 95.11)	Acc@5  99.22 ( 99.95)
Epoch: [37][250/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.8018e-01 (1.4314e-01)	Acc@1  92.97 ( 95.10)	Acc@5 100.00 ( 99.95)
Epoch: [37][260/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6174e-01 (1.4282e-01)	Acc@1  95.31 ( 95.11)	Acc@5 100.00 ( 99.95)
Epoch: [37][270/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0809e-01 (1.4266e-01)	Acc@1  97.66 ( 95.10)	Acc@5 100.00 ( 99.95)
Epoch: [37][280/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5625e-01 (1.4239e-01)	Acc@1  94.53 ( 95.13)	Acc@5 100.00 ( 99.95)
Epoch: [37][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.4807e-01 (1.4294e-01)	Acc@1  96.09 ( 95.09)	Acc@5 100.00 ( 99.95)
Epoch: [37][300/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.5259e-01 (1.4289e-01)	Acc@1  94.53 ( 95.10)	Acc@5 100.00 ( 99.95)
Epoch: [37][310/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.1664e-01 (1.4324e-01)	Acc@1  95.31 ( 95.09)	Acc@5 100.00 ( 99.95)
Epoch: [37][320/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.3013e-01 (1.4304e-01)	Acc@1  94.53 ( 95.09)	Acc@5 100.00 ( 99.95)
Epoch: [37][330/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.4526e-01 (1.4338e-01)	Acc@1  96.09 ( 95.06)	Acc@5 100.00 ( 99.95)
Epoch: [37][340/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.3096e-01 (1.4425e-01)	Acc@1  92.19 ( 95.02)	Acc@5 100.00 ( 99.95)
Epoch: [37][350/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.8921e-01 (1.4490e-01)	Acc@1  94.53 ( 95.01)	Acc@5  99.22 ( 99.95)
Epoch: [37][360/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.5662e-01 (1.4459e-01)	Acc@1  93.75 ( 95.01)	Acc@5 100.00 ( 99.95)
Epoch: [37][370/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.3037e-01 (1.4515e-01)	Acc@1  95.31 ( 94.97)	Acc@5 100.00 ( 99.95)
Epoch: [37][380/391]	Time  0.043 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.7725e-01 (1.4621e-01)	Acc@1  93.75 ( 94.92)	Acc@5 100.00 ( 99.95)
Epoch: [37][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.4941e-01 (1.4622e-01)	Acc@1  96.25 ( 94.93)	Acc@5 100.00 ( 99.95)
## e[37] optimizer.zero_grad (sum) time: 0.2498188018798828
## e[37]       loss.backward (sum) time: 4.1819007396698
## e[37]      optimizer.step (sum) time: 1.7176153659820557
## epoch[37] training(only) time: 15.854871273040771
# Switched to evaluate mode...
Test: [  0/100]	Time  0.167 ( 0.167)	Loss 2.5220e-01 (2.5220e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.020 ( 0.034)	Loss 3.1958e-01 (2.7005e-01)	Acc@1  86.00 ( 91.36)	Acc@5  99.00 ( 99.82)
Test: [ 20/100]	Time  0.019 ( 0.027)	Loss 3.3911e-01 (2.9054e-01)	Acc@1  87.00 ( 90.76)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.022 ( 0.025)	Loss 3.8867e-01 (3.0081e-01)	Acc@1  88.00 ( 90.71)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.020 ( 0.024)	Loss 3.6304e-01 (3.0321e-01)	Acc@1  88.00 ( 90.39)	Acc@5  99.00 ( 99.71)
Test: [ 50/100]	Time  0.023 ( 0.023)	Loss 2.0813e-01 (2.9829e-01)	Acc@1  91.00 ( 90.69)	Acc@5 100.00 ( 99.67)
Test: [ 60/100]	Time  0.019 ( 0.023)	Loss 3.3252e-01 (2.9704e-01)	Acc@1  92.00 ( 90.67)	Acc@5 100.00 ( 99.72)
Test: [ 70/100]	Time  0.017 ( 0.022)	Loss 3.5498e-01 (2.9203e-01)	Acc@1  89.00 ( 90.68)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.022 ( 0.022)	Loss 1.3318e-01 (2.9542e-01)	Acc@1  96.00 ( 90.63)	Acc@5 100.00 ( 99.73)
Test: [ 90/100]	Time  0.019 ( 0.022)	Loss 1.9519e-01 (2.9375e-01)	Acc@1  94.00 ( 90.66)	Acc@5 100.00 ( 99.73)
 * Acc@1 90.710 Acc@5 99.740
### epoch[37] execution time: 18.102123022079468
EPOCH 38
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [38][  0/391]	Time  0.212 ( 0.212)	Data  0.171 ( 0.171)	Loss 1.4331e-01 (1.4331e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [38][ 10/391]	Time  0.041 ( 0.056)	Data  0.001 ( 0.016)	Loss 2.0642e-01 (1.3934e-01)	Acc@1  92.19 ( 94.67)	Acc@5  99.22 ( 99.93)
Epoch: [38][ 20/391]	Time  0.039 ( 0.049)	Data  0.001 ( 0.009)	Loss 1.6113e-01 (1.3602e-01)	Acc@1  95.31 ( 94.90)	Acc@5 100.00 ( 99.96)
Epoch: [38][ 30/391]	Time  0.038 ( 0.045)	Data  0.001 ( 0.006)	Loss 1.4343e-01 (1.4024e-01)	Acc@1  95.31 ( 94.81)	Acc@5 100.00 ( 99.95)
Epoch: [38][ 40/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.005)	Loss 1.4526e-01 (1.3903e-01)	Acc@1  93.75 ( 95.01)	Acc@5 100.00 ( 99.96)
Epoch: [38][ 50/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.2158e-01 (1.3889e-01)	Acc@1  96.09 ( 94.99)	Acc@5 100.00 ( 99.94)
Epoch: [38][ 60/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.2585e-01 (1.4075e-01)	Acc@1  94.53 ( 94.94)	Acc@5 100.00 ( 99.95)
Epoch: [38][ 70/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.5210e-01 (1.3867e-01)	Acc@1  93.75 ( 94.97)	Acc@5 100.00 ( 99.96)
Epoch: [38][ 80/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.4880e-01 (1.3870e-01)	Acc@1  95.31 ( 95.02)	Acc@5 100.00 ( 99.96)
Epoch: [38][ 90/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.1517e-01 (1.3869e-01)	Acc@1  96.09 ( 95.03)	Acc@5 100.00 ( 99.96)
Epoch: [38][100/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.3257e-01 (1.3805e-01)	Acc@1  93.75 ( 95.05)	Acc@5 100.00 ( 99.95)
Epoch: [38][110/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.1948e-01 (1.3895e-01)	Acc@1  92.19 ( 95.03)	Acc@5  99.22 ( 99.94)
Epoch: [38][120/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.6113e-01 (1.3993e-01)	Acc@1  92.19 ( 94.99)	Acc@5 100.00 ( 99.95)
Epoch: [38][130/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4526e-01 (1.3862e-01)	Acc@1  94.53 ( 95.06)	Acc@5 100.00 ( 99.95)
Epoch: [38][140/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.3608e-02 (1.3976e-01)	Acc@1  98.44 ( 95.07)	Acc@5 100.00 ( 99.94)
Epoch: [38][150/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6028e-01 (1.3983e-01)	Acc@1  93.75 ( 95.07)	Acc@5 100.00 ( 99.94)
Epoch: [38][160/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4661e-01 (1.4084e-01)	Acc@1  94.53 ( 95.03)	Acc@5  99.22 ( 99.94)
Epoch: [38][170/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.3140e-02 (1.4048e-01)	Acc@1  96.09 ( 95.05)	Acc@5 100.00 ( 99.94)
Epoch: [38][180/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0376e-01 (1.4041e-01)	Acc@1  98.44 ( 95.05)	Acc@5 100.00 ( 99.94)
Epoch: [38][190/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4722e-01 (1.3939e-01)	Acc@1  94.53 ( 95.09)	Acc@5 100.00 ( 99.94)
Epoch: [38][200/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2634e-01 (1.3881e-01)	Acc@1  96.88 ( 95.11)	Acc@5 100.00 ( 99.94)
Epoch: [38][210/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4978e-01 (1.3857e-01)	Acc@1  95.31 ( 95.13)	Acc@5 100.00 ( 99.94)
Epoch: [38][220/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4661e-01 (1.3854e-01)	Acc@1  95.31 ( 95.11)	Acc@5 100.00 ( 99.95)
Epoch: [38][230/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1212e-01 (1.3836e-01)	Acc@1  94.53 ( 95.11)	Acc@5 100.00 ( 99.95)
Epoch: [38][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.4158e-02 (1.3809e-01)	Acc@1  96.09 ( 95.12)	Acc@5 100.00 ( 99.95)
Epoch: [38][250/391]	Time  0.035 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.0813e-01 (1.3812e-01)	Acc@1  93.75 ( 95.11)	Acc@5 100.00 ( 99.95)
Epoch: [38][260/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5625e-01 (1.3752e-01)	Acc@1  93.75 ( 95.15)	Acc@5 100.00 ( 99.95)
Epoch: [38][270/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3403e-01 (1.3836e-01)	Acc@1  96.09 ( 95.14)	Acc@5 100.00 ( 99.95)
Epoch: [38][280/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2830e-01 (1.3805e-01)	Acc@1  96.09 ( 95.15)	Acc@5 100.00 ( 99.95)
Epoch: [38][290/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4929e-01 (1.3874e-01)	Acc@1  95.31 ( 95.14)	Acc@5 100.00 ( 99.95)
Epoch: [38][300/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.4697e-01 (1.3875e-01)	Acc@1  92.97 ( 95.14)	Acc@5 100.00 ( 99.94)
Epoch: [38][310/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.6956e-01 (1.3902e-01)	Acc@1  95.31 ( 95.14)	Acc@5  99.22 ( 99.94)
Epoch: [38][320/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.0822e-01 (1.3897e-01)	Acc@1  96.88 ( 95.15)	Acc@5 100.00 ( 99.93)
Epoch: [38][330/391]	Time  0.045 ( 0.040)	Data  0.001 ( 0.001)	Loss 9.9609e-02 (1.3858e-01)	Acc@1  96.88 ( 95.15)	Acc@5 100.00 ( 99.94)
Epoch: [38][340/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.1163e-01 (1.3809e-01)	Acc@1  96.88 ( 95.18)	Acc@5 100.00 ( 99.94)
Epoch: [38][350/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.7773e-01 (1.3820e-01)	Acc@1  95.31 ( 95.17)	Acc@5 100.00 ( 99.94)
Epoch: [38][360/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.5588e-01 (1.3854e-01)	Acc@1  92.97 ( 95.15)	Acc@5 100.00 ( 99.94)
Epoch: [38][370/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.2610e-01 (1.3957e-01)	Acc@1  97.66 ( 95.12)	Acc@5 100.00 ( 99.93)
Epoch: [38][380/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.4612e-01 (1.3949e-01)	Acc@1  92.97 ( 95.13)	Acc@5 100.00 ( 99.93)
Epoch: [38][390/391]	Time  0.031 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.5269e-01 (1.3924e-01)	Acc@1  93.75 ( 95.14)	Acc@5 100.00 ( 99.93)
## e[38] optimizer.zero_grad (sum) time: 0.2511308193206787
## e[38]       loss.backward (sum) time: 4.164083242416382
## e[38]      optimizer.step (sum) time: 1.7608568668365479
## epoch[38] training(only) time: 15.924202919006348
# Switched to evaluate mode...
Test: [  0/100]	Time  0.169 ( 0.169)	Loss 2.3535e-01 (2.3535e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.019 ( 0.033)	Loss 3.5840e-01 (2.6495e-01)	Acc@1  88.00 ( 91.18)	Acc@5  99.00 ( 99.82)
Test: [ 20/100]	Time  0.016 ( 0.026)	Loss 3.5449e-01 (2.8380e-01)	Acc@1  87.00 ( 90.57)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.023 ( 0.024)	Loss 3.9917e-01 (2.9295e-01)	Acc@1  85.00 ( 90.29)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.017 ( 0.023)	Loss 3.5205e-01 (2.9431e-01)	Acc@1  87.00 ( 90.20)	Acc@5  99.00 ( 99.73)
Test: [ 50/100]	Time  0.022 ( 0.023)	Loss 1.7688e-01 (2.9120e-01)	Acc@1  94.00 ( 90.39)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.019 ( 0.022)	Loss 3.3057e-01 (2.8934e-01)	Acc@1  91.00 ( 90.51)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.018 ( 0.022)	Loss 3.9307e-01 (2.8530e-01)	Acc@1  86.00 ( 90.51)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.015 ( 0.021)	Loss 1.4514e-01 (2.8887e-01)	Acc@1  95.00 ( 90.47)	Acc@5 100.00 ( 99.73)
Test: [ 90/100]	Time  0.017 ( 0.021)	Loss 2.1973e-01 (2.8705e-01)	Acc@1  93.00 ( 90.51)	Acc@5 100.00 ( 99.73)
 * Acc@1 90.570 Acc@5 99.730
### epoch[38] execution time: 18.110409021377563
EPOCH 39
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [39][  0/391]	Time  0.212 ( 0.212)	Data  0.170 ( 0.170)	Loss 2.3572e-01 (2.3572e-01)	Acc@1  91.41 ( 91.41)	Acc@5 100.00 (100.00)
Epoch: [39][ 10/391]	Time  0.042 ( 0.056)	Data  0.001 ( 0.016)	Loss 1.5100e-01 (1.4792e-01)	Acc@1  92.97 ( 94.74)	Acc@5 100.00 (100.00)
Epoch: [39][ 20/391]	Time  0.042 ( 0.048)	Data  0.001 ( 0.009)	Loss 1.7310e-01 (1.5136e-01)	Acc@1  94.53 ( 94.64)	Acc@5 100.00 (100.00)
Epoch: [39][ 30/391]	Time  0.039 ( 0.045)	Data  0.001 ( 0.006)	Loss 1.1163e-01 (1.4563e-01)	Acc@1  96.09 ( 94.73)	Acc@5 100.00 (100.00)
Epoch: [39][ 40/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.005)	Loss 1.7651e-01 (1.4315e-01)	Acc@1  92.19 ( 94.84)	Acc@5 100.00 (100.00)
Epoch: [39][ 50/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 9.6252e-02 (1.3960e-01)	Acc@1  96.88 ( 95.11)	Acc@5 100.00 ( 99.98)
Epoch: [39][ 60/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.3892e-01 (1.3854e-01)	Acc@1  96.09 ( 95.13)	Acc@5 100.00 ( 99.97)
Epoch: [39][ 70/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.3843e-01 (1.3787e-01)	Acc@1  95.31 ( 95.17)	Acc@5 100.00 ( 99.98)
Epoch: [39][ 80/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.2402e-01 (1.3619e-01)	Acc@1  96.09 ( 95.24)	Acc@5  99.22 ( 99.97)
Epoch: [39][ 90/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 9.3628e-02 (1.3601e-01)	Acc@1  96.88 ( 95.32)	Acc@5 100.00 ( 99.97)
Epoch: [39][100/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.003)	Loss 1.7834e-01 (1.3585e-01)	Acc@1  92.97 ( 95.30)	Acc@5 100.00 ( 99.97)
Epoch: [39][110/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0730e-01 (1.3384e-01)	Acc@1  96.88 ( 95.37)	Acc@5  99.22 ( 99.96)
Epoch: [39][120/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.1130e-01 (1.3567e-01)	Acc@1  92.19 ( 95.29)	Acc@5 100.00 ( 99.96)
Epoch: [39][130/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3733e-01 (1.3462e-01)	Acc@1  95.31 ( 95.29)	Acc@5 100.00 ( 99.96)
Epoch: [39][140/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3208e-01 (1.3429e-01)	Acc@1  96.09 ( 95.32)	Acc@5 100.00 ( 99.96)
Epoch: [39][150/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.4658e-01 (1.3548e-01)	Acc@1  91.41 ( 95.30)	Acc@5  99.22 ( 99.94)
Epoch: [39][160/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.9897e-01 (1.3636e-01)	Acc@1  94.53 ( 95.26)	Acc@5 100.00 ( 99.94)
Epoch: [39][170/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0712e-01 (1.3603e-01)	Acc@1  95.31 ( 95.24)	Acc@5 100.00 ( 99.95)
Epoch: [39][180/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4038e-01 (1.3689e-01)	Acc@1  94.53 ( 95.23)	Acc@5 100.00 ( 99.95)
Epoch: [39][190/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4038e-01 (1.3576e-01)	Acc@1  94.53 ( 95.28)	Acc@5 100.00 ( 99.95)
Epoch: [39][200/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7737e-01 (1.3560e-01)	Acc@1  92.97 ( 95.27)	Acc@5 100.00 ( 99.95)
Epoch: [39][210/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1615e-01 (1.3563e-01)	Acc@1  93.75 ( 95.26)	Acc@5 100.00 ( 99.94)
Epoch: [39][220/391]	Time  0.046 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5955e-01 (1.3690e-01)	Acc@1  95.31 ( 95.23)	Acc@5  99.22 ( 99.94)
Epoch: [39][230/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.0020e-01 (1.3700e-01)	Acc@1  92.19 ( 95.24)	Acc@5  99.22 ( 99.94)
Epoch: [39][240/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3696e-01 (1.3777e-01)	Acc@1  96.09 ( 95.21)	Acc@5 100.00 ( 99.94)
Epoch: [39][250/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3538e-01 (1.3738e-01)	Acc@1  94.53 ( 95.21)	Acc@5 100.00 ( 99.93)
Epoch: [39][260/391]	Time  0.045 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7346e-01 (1.3825e-01)	Acc@1  96.09 ( 95.20)	Acc@5 100.00 ( 99.93)
Epoch: [39][270/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1176e-01 (1.3882e-01)	Acc@1  96.09 ( 95.18)	Acc@5 100.00 ( 99.92)
Epoch: [39][280/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.1609e-01 (1.3891e-01)	Acc@1  96.09 ( 95.17)	Acc@5 100.00 ( 99.92)
Epoch: [39][290/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.1265e-01 (1.3909e-01)	Acc@1  92.97 ( 95.16)	Acc@5 100.00 ( 99.92)
Epoch: [39][300/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.3867e-01 (1.3922e-01)	Acc@1  96.88 ( 95.16)	Acc@5 100.00 ( 99.92)
Epoch: [39][310/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.6663e-01 (1.3905e-01)	Acc@1  93.75 ( 95.17)	Acc@5 100.00 ( 99.92)
Epoch: [39][320/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.5979e-01 (1.3866e-01)	Acc@1  95.31 ( 95.19)	Acc@5  99.22 ( 99.92)
Epoch: [39][330/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.8628e-01 (1.3862e-01)	Acc@1  93.75 ( 95.18)	Acc@5 100.00 ( 99.92)
Epoch: [39][340/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.2012e-01 (1.3897e-01)	Acc@1  96.09 ( 95.15)	Acc@5 100.00 ( 99.92)
Epoch: [39][350/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.0032e-01 (1.3946e-01)	Acc@1  92.97 ( 95.14)	Acc@5 100.00 ( 99.93)
Epoch: [39][360/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.5002e-01 (1.3981e-01)	Acc@1  96.09 ( 95.13)	Acc@5 100.00 ( 99.93)
Epoch: [39][370/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.7126e-01 (1.3976e-01)	Acc@1  96.88 ( 95.15)	Acc@5 100.00 ( 99.93)
Epoch: [39][380/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 9.5764e-02 (1.3947e-01)	Acc@1  96.88 ( 95.16)	Acc@5 100.00 ( 99.93)
Epoch: [39][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 9.7595e-02 (1.3923e-01)	Acc@1  96.25 ( 95.19)	Acc@5 100.00 ( 99.93)
## e[39] optimizer.zero_grad (sum) time: 0.25082898139953613
## e[39]       loss.backward (sum) time: 4.203812837600708
## e[39]      optimizer.step (sum) time: 1.749009609222412
## epoch[39] training(only) time: 15.919508934020996
# Switched to evaluate mode...
Test: [  0/100]	Time  0.180 ( 0.180)	Loss 2.6001e-01 (2.6001e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.023 ( 0.036)	Loss 3.0225e-01 (2.6480e-01)	Acc@1  87.00 ( 91.00)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 3.5034e-01 (2.8430e-01)	Acc@1  85.00 ( 90.62)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.017 ( 0.026)	Loss 4.0063e-01 (2.9198e-01)	Acc@1  87.00 ( 90.52)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.020 ( 0.025)	Loss 3.5840e-01 (2.9566e-01)	Acc@1  85.00 ( 90.29)	Acc@5  99.00 ( 99.66)
Test: [ 50/100]	Time  0.018 ( 0.024)	Loss 1.6736e-01 (2.9126e-01)	Acc@1  94.00 ( 90.73)	Acc@5 100.00 ( 99.61)
Test: [ 60/100]	Time  0.017 ( 0.023)	Loss 3.7280e-01 (2.9143e-01)	Acc@1  91.00 ( 90.72)	Acc@5 100.00 ( 99.66)
Test: [ 70/100]	Time  0.017 ( 0.023)	Loss 3.4790e-01 (2.8851e-01)	Acc@1  89.00 ( 90.72)	Acc@5 100.00 ( 99.68)
Test: [ 80/100]	Time  0.021 ( 0.022)	Loss 1.6943e-01 (2.9304e-01)	Acc@1  94.00 ( 90.56)	Acc@5 100.00 ( 99.68)
Test: [ 90/100]	Time  0.020 ( 0.022)	Loss 2.3438e-01 (2.9174e-01)	Acc@1  94.00 ( 90.57)	Acc@5 100.00 ( 99.69)
 * Acc@1 90.610 Acc@5 99.710
### epoch[39] execution time: 18.20306658744812
EPOCH 40
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [40][  0/391]	Time  0.214 ( 0.214)	Data  0.174 ( 0.174)	Loss 1.0114e-01 (1.0114e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [40][ 10/391]	Time  0.038 ( 0.055)	Data  0.001 ( 0.017)	Loss 2.5122e-01 (1.4790e-01)	Acc@1  93.75 ( 94.74)	Acc@5  99.22 ( 99.79)
Epoch: [40][ 20/391]	Time  0.040 ( 0.049)	Data  0.001 ( 0.009)	Loss 8.3130e-02 (1.4015e-01)	Acc@1  97.66 ( 95.20)	Acc@5 100.00 ( 99.85)
Epoch: [40][ 30/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.007)	Loss 1.1072e-01 (1.2995e-01)	Acc@1  95.31 ( 95.67)	Acc@5 100.00 ( 99.90)
Epoch: [40][ 40/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.005)	Loss 1.3818e-01 (1.3102e-01)	Acc@1  93.75 ( 95.58)	Acc@5 100.00 ( 99.92)
Epoch: [40][ 50/391]	Time  0.035 ( 0.043)	Data  0.001 ( 0.004)	Loss 9.4177e-02 (1.2752e-01)	Acc@1  96.88 ( 95.62)	Acc@5 100.00 ( 99.92)
Epoch: [40][ 60/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.0864e-01 (1.2475e-01)	Acc@1  96.09 ( 95.74)	Acc@5 100.00 ( 99.94)
Epoch: [40][ 70/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.4868e-01 (1.2565e-01)	Acc@1  93.75 ( 95.68)	Acc@5 100.00 ( 99.93)
Epoch: [40][ 80/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.8457e-01 (1.2702e-01)	Acc@1  96.09 ( 95.68)	Acc@5  98.44 ( 99.91)
Epoch: [40][ 90/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.0779e-01 (1.2727e-01)	Acc@1  96.09 ( 95.66)	Acc@5 100.00 ( 99.92)
Epoch: [40][100/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.1713e-01 (1.2767e-01)	Acc@1  95.31 ( 95.61)	Acc@5 100.00 ( 99.93)
Epoch: [40][110/391]	Time  0.049 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.0052e-01 (1.2821e-01)	Acc@1  97.66 ( 95.59)	Acc@5 100.00 ( 99.93)
Epoch: [40][120/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.7280e-02 (1.2688e-01)	Acc@1  97.66 ( 95.61)	Acc@5 100.00 ( 99.94)
Epoch: [40][130/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0968e-01 (1.2858e-01)	Acc@1  95.31 ( 95.56)	Acc@5 100.00 ( 99.94)
Epoch: [40][140/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3696e-01 (1.2848e-01)	Acc@1  95.31 ( 95.56)	Acc@5 100.00 ( 99.94)
Epoch: [40][150/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.5388e-02 (1.2703e-01)	Acc@1  97.66 ( 95.63)	Acc@5 100.00 ( 99.94)
Epoch: [40][160/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3989e-01 (1.2692e-01)	Acc@1  96.09 ( 95.65)	Acc@5 100.00 ( 99.94)
Epoch: [40][170/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.1371e-02 (1.2644e-01)	Acc@1  99.22 ( 95.69)	Acc@5 100.00 ( 99.95)
Epoch: [40][180/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6772e-01 (1.2683e-01)	Acc@1  93.75 ( 95.70)	Acc@5 100.00 ( 99.94)
Epoch: [40][190/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.8877e-02 (1.2724e-01)	Acc@1  96.09 ( 95.67)	Acc@5 100.00 ( 99.95)
Epoch: [40][200/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.8083e-02 (1.2753e-01)	Acc@1  97.66 ( 95.67)	Acc@5 100.00 ( 99.95)
Epoch: [40][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.1787e-02 (1.2681e-01)	Acc@1  99.22 ( 95.70)	Acc@5  99.22 ( 99.94)
Epoch: [40][220/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5869e-01 (1.2720e-01)	Acc@1  93.75 ( 95.67)	Acc@5 100.00 ( 99.95)
Epoch: [40][230/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5442e-01 (1.2850e-01)	Acc@1  96.09 ( 95.63)	Acc@5  99.22 ( 99.94)
Epoch: [40][240/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0364e-01 (1.2856e-01)	Acc@1  96.09 ( 95.61)	Acc@5 100.00 ( 99.94)
Epoch: [40][250/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1395e-01 (1.2800e-01)	Acc@1  96.88 ( 95.62)	Acc@5 100.00 ( 99.94)
Epoch: [40][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2488e-01 (1.2793e-01)	Acc@1  93.75 ( 95.61)	Acc@5 100.00 ( 99.94)
Epoch: [40][270/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.7078e-01 (1.2768e-01)	Acc@1  95.31 ( 95.64)	Acc@5 100.00 ( 99.94)
Epoch: [40][280/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.001)	Loss 8.3557e-02 (1.2805e-01)	Acc@1  96.88 ( 95.61)	Acc@5 100.00 ( 99.94)
Epoch: [40][290/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.6809e-01 (1.2862e-01)	Acc@1  92.97 ( 95.59)	Acc@5 100.00 ( 99.94)
Epoch: [40][300/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.2866e-01 (1.2932e-01)	Acc@1  96.09 ( 95.56)	Acc@5 100.00 ( 99.94)
Epoch: [40][310/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.9104e-01 (1.2970e-01)	Acc@1  93.75 ( 95.55)	Acc@5 100.00 ( 99.94)
Epoch: [40][320/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.5771e-01 (1.2975e-01)	Acc@1  96.09 ( 95.55)	Acc@5 100.00 ( 99.94)
Epoch: [40][330/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.2354e-01 (1.2956e-01)	Acc@1  96.09 ( 95.57)	Acc@5 100.00 ( 99.95)
Epoch: [40][340/391]	Time  0.051 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.2402e-01 (1.2965e-01)	Acc@1  93.75 ( 95.57)	Acc@5 100.00 ( 99.95)
Epoch: [40][350/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.0315e-01 (1.3027e-01)	Acc@1  95.31 ( 95.55)	Acc@5 100.00 ( 99.94)
Epoch: [40][360/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.7029e-01 (1.3082e-01)	Acc@1  91.41 ( 95.54)	Acc@5 100.00 ( 99.94)
Epoch: [40][370/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.1066e-01 (1.3127e-01)	Acc@1  96.88 ( 95.51)	Acc@5 100.00 ( 99.94)
Epoch: [40][380/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.1584e-01 (1.3136e-01)	Acc@1  95.31 ( 95.51)	Acc@5 100.00 ( 99.94)
Epoch: [40][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.6538e-01 (1.3148e-01)	Acc@1  92.50 ( 95.52)	Acc@5 100.00 ( 99.94)
## e[40] optimizer.zero_grad (sum) time: 0.24953985214233398
## e[40]       loss.backward (sum) time: 4.137642860412598
## e[40]      optimizer.step (sum) time: 1.7354764938354492
## epoch[40] training(only) time: 15.864288806915283
# Switched to evaluate mode...
Test: [  0/100]	Time  0.181 ( 0.181)	Loss 3.2715e-01 (3.2715e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.019 ( 0.035)	Loss 2.8662e-01 (2.8921e-01)	Acc@1  87.00 ( 90.91)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.020 ( 0.028)	Loss 3.3936e-01 (2.9910e-01)	Acc@1  87.00 ( 90.76)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.019 ( 0.025)	Loss 4.1455e-01 (3.0896e-01)	Acc@1  87.00 ( 90.61)	Acc@5  99.00 ( 99.58)
Test: [ 40/100]	Time  0.020 ( 0.024)	Loss 3.6597e-01 (3.0674e-01)	Acc@1  88.00 ( 90.44)	Acc@5  99.00 ( 99.59)
Test: [ 50/100]	Time  0.025 ( 0.023)	Loss 2.0239e-01 (3.0347e-01)	Acc@1  94.00 ( 90.65)	Acc@5 100.00 ( 99.55)
Test: [ 60/100]	Time  0.023 ( 0.023)	Loss 3.6255e-01 (3.0212e-01)	Acc@1  91.00 ( 90.64)	Acc@5  99.00 ( 99.61)
Test: [ 70/100]	Time  0.017 ( 0.022)	Loss 3.7329e-01 (2.9723e-01)	Acc@1  89.00 ( 90.69)	Acc@5 100.00 ( 99.62)
Test: [ 80/100]	Time  0.021 ( 0.022)	Loss 1.5381e-01 (3.0114e-01)	Acc@1  93.00 ( 90.49)	Acc@5 100.00 ( 99.60)
Test: [ 90/100]	Time  0.020 ( 0.022)	Loss 2.5391e-01 (2.9789e-01)	Acc@1  92.00 ( 90.57)	Acc@5 100.00 ( 99.60)
 * Acc@1 90.610 Acc@5 99.630
### epoch[40] execution time: 18.164544343948364
EPOCH 41
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [41][  0/391]	Time  0.217 ( 0.217)	Data  0.176 ( 0.176)	Loss 1.4062e-01 (1.4062e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
Epoch: [41][ 10/391]	Time  0.041 ( 0.057)	Data  0.001 ( 0.017)	Loss 1.2219e-01 (1.2428e-01)	Acc@1  94.53 ( 95.31)	Acc@5 100.00 ( 99.93)
Epoch: [41][ 20/391]	Time  0.042 ( 0.049)	Data  0.001 ( 0.009)	Loss 1.3867e-01 (1.2566e-01)	Acc@1  92.97 ( 95.42)	Acc@5 100.00 ( 99.96)
Epoch: [41][ 30/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.006)	Loss 1.1951e-01 (1.2906e-01)	Acc@1  96.09 ( 95.56)	Acc@5 100.00 ( 99.97)
Epoch: [41][ 40/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.005)	Loss 2.1204e-01 (1.2795e-01)	Acc@1  91.41 ( 95.60)	Acc@5 100.00 ( 99.96)
Epoch: [41][ 50/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.3391e-01 (1.2848e-01)	Acc@1  95.31 ( 95.54)	Acc@5 100.00 ( 99.94)
Epoch: [41][ 60/391]	Time  0.043 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.6992e-01 (1.2769e-01)	Acc@1  92.19 ( 95.54)	Acc@5 100.00 ( 99.95)
Epoch: [41][ 70/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.4514e-01 (1.2611e-01)	Acc@1  94.53 ( 95.62)	Acc@5 100.00 ( 99.96)
Epoch: [41][ 80/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.5991e-01 (1.2570e-01)	Acc@1  92.97 ( 95.65)	Acc@5  99.22 ( 99.94)
Epoch: [41][ 90/391]	Time  0.038 ( 0.042)	Data  0.002 ( 0.003)	Loss 9.1980e-02 (1.2695e-01)	Acc@1  97.66 ( 95.61)	Acc@5 100.00 ( 99.94)
Epoch: [41][100/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.0913e-01 (1.2453e-01)	Acc@1  96.09 ( 95.71)	Acc@5 100.00 ( 99.94)
Epoch: [41][110/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.1285e-01 (1.2576e-01)	Acc@1  96.09 ( 95.68)	Acc@5 100.00 ( 99.94)
Epoch: [41][120/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.0691e-01 (1.2767e-01)	Acc@1  91.41 ( 95.58)	Acc@5  99.22 ( 99.94)
Epoch: [41][130/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0022e-01 (1.2693e-01)	Acc@1  96.88 ( 95.59)	Acc@5 100.00 ( 99.93)
Epoch: [41][140/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5723e-01 (1.2740e-01)	Acc@1  93.75 ( 95.59)	Acc@5 100.00 ( 99.94)
Epoch: [41][150/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.3840e-01 (1.2927e-01)	Acc@1  92.97 ( 95.54)	Acc@5 100.00 ( 99.94)
Epoch: [41][160/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7810e-01 (1.3061e-01)	Acc@1  94.53 ( 95.50)	Acc@5 100.00 ( 99.94)
Epoch: [41][170/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.3792e-02 (1.2972e-01)	Acc@1  98.44 ( 95.55)	Acc@5 100.00 ( 99.94)
Epoch: [41][180/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1401e-01 (1.2986e-01)	Acc@1  96.09 ( 95.55)	Acc@5 100.00 ( 99.94)
Epoch: [41][190/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6687e-01 (1.2922e-01)	Acc@1  93.75 ( 95.57)	Acc@5 100.00 ( 99.95)
Epoch: [41][200/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3867e-01 (1.2987e-01)	Acc@1  95.31 ( 95.51)	Acc@5 100.00 ( 99.94)
Epoch: [41][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6956e-01 (1.2986e-01)	Acc@1  93.75 ( 95.53)	Acc@5 100.00 ( 99.94)
Epoch: [41][220/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1414e-01 (1.3056e-01)	Acc@1  96.88 ( 95.50)	Acc@5 100.00 ( 99.93)
Epoch: [41][230/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.4045e-02 (1.3033e-01)	Acc@1  96.88 ( 95.48)	Acc@5 100.00 ( 99.93)
Epoch: [41][240/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1731e-01 (1.2980e-01)	Acc@1  97.66 ( 95.53)	Acc@5 100.00 ( 99.94)
Epoch: [41][250/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4556e-02 (1.2942e-01)	Acc@1  99.22 ( 95.55)	Acc@5 100.00 ( 99.94)
Epoch: [41][260/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6748e-01 (1.2955e-01)	Acc@1  92.97 ( 95.56)	Acc@5 100.00 ( 99.94)
Epoch: [41][270/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5222e-01 (1.2974e-01)	Acc@1  95.31 ( 95.54)	Acc@5 100.00 ( 99.94)
Epoch: [41][280/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.6272e-01 (1.2969e-01)	Acc@1  92.19 ( 95.53)	Acc@5 100.00 ( 99.94)
Epoch: [41][290/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.5198e-01 (1.2950e-01)	Acc@1  92.19 ( 95.53)	Acc@5 100.00 ( 99.94)
Epoch: [41][300/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.0364e-01 (1.2966e-01)	Acc@1  96.88 ( 95.53)	Acc@5 100.00 ( 99.95)
Epoch: [41][310/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.5698e-01 (1.3019e-01)	Acc@1  96.09 ( 95.51)	Acc@5 100.00 ( 99.95)
Epoch: [41][320/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.0693e-01 (1.3023e-01)	Acc@1  95.31 ( 95.53)	Acc@5 100.00 ( 99.94)
Epoch: [41][330/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.0754e-01 (1.3072e-01)	Acc@1  98.44 ( 95.53)	Acc@5 100.00 ( 99.94)
Epoch: [41][340/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.3867e-01 (1.3112e-01)	Acc@1  95.31 ( 95.52)	Acc@5 100.00 ( 99.94)
Epoch: [41][350/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.001)	Loss 8.8501e-02 (1.3138e-01)	Acc@1  97.66 ( 95.50)	Acc@5 100.00 ( 99.94)
Epoch: [41][360/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 9.3750e-02 (1.3162e-01)	Acc@1  96.09 ( 95.51)	Acc@5 100.00 ( 99.94)
Epoch: [41][370/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 9.0393e-02 (1.3131e-01)	Acc@1  98.44 ( 95.52)	Acc@5 100.00 ( 99.95)
Epoch: [41][380/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.2817e-01 (1.3107e-01)	Acc@1  94.53 ( 95.54)	Acc@5 100.00 ( 99.95)
Epoch: [41][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.8176e-01 (1.3086e-01)	Acc@1  91.25 ( 95.54)	Acc@5 100.00 ( 99.95)
## e[41] optimizer.zero_grad (sum) time: 0.25466394424438477
## e[41]       loss.backward (sum) time: 4.190965414047241
## e[41]      optimizer.step (sum) time: 1.7304425239562988
## epoch[41] training(only) time: 15.929116249084473
# Switched to evaluate mode...
Test: [  0/100]	Time  0.171 ( 0.171)	Loss 2.5342e-01 (2.5342e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.017 ( 0.034)	Loss 3.5620e-01 (2.7728e-01)	Acc@1  86.00 ( 91.00)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.017 ( 0.027)	Loss 3.4790e-01 (2.9349e-01)	Acc@1  89.00 ( 90.90)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.020 ( 0.025)	Loss 3.8574e-01 (3.0099e-01)	Acc@1  88.00 ( 90.77)	Acc@5 100.00 ( 99.61)
Test: [ 40/100]	Time  0.020 ( 0.024)	Loss 3.7964e-01 (3.0326e-01)	Acc@1  87.00 ( 90.49)	Acc@5  99.00 ( 99.61)
Test: [ 50/100]	Time  0.021 ( 0.023)	Loss 1.7053e-01 (2.9871e-01)	Acc@1  94.00 ( 90.76)	Acc@5 100.00 ( 99.59)
Test: [ 60/100]	Time  0.021 ( 0.022)	Loss 3.6060e-01 (2.9822e-01)	Acc@1  91.00 ( 90.77)	Acc@5 100.00 ( 99.66)
Test: [ 70/100]	Time  0.021 ( 0.022)	Loss 4.2529e-01 (2.9529e-01)	Acc@1  88.00 ( 90.65)	Acc@5 100.00 ( 99.68)
Test: [ 80/100]	Time  0.019 ( 0.022)	Loss 1.1646e-01 (2.9799e-01)	Acc@1  95.00 ( 90.56)	Acc@5 100.00 ( 99.68)
Test: [ 90/100]	Time  0.021 ( 0.022)	Loss 2.0984e-01 (2.9570e-01)	Acc@1  92.00 ( 90.65)	Acc@5 100.00 ( 99.70)
 * Acc@1 90.760 Acc@5 99.720
### epoch[41] execution time: 18.217806577682495
EPOCH 42
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [42][  0/391]	Time  0.219 ( 0.219)	Data  0.176 ( 0.176)	Loss 7.2449e-02 (7.2449e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [42][ 10/391]	Time  0.038 ( 0.056)	Data  0.001 ( 0.017)	Loss 1.2323e-01 (1.2020e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 ( 99.93)
Epoch: [42][ 20/391]	Time  0.037 ( 0.049)	Data  0.001 ( 0.009)	Loss 1.1346e-01 (1.2601e-01)	Acc@1  96.09 ( 95.72)	Acc@5 100.00 ( 99.93)
Epoch: [42][ 30/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.007)	Loss 1.4868e-01 (1.2688e-01)	Acc@1  96.09 ( 95.67)	Acc@5 100.00 ( 99.95)
Epoch: [42][ 40/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.005)	Loss 1.6638e-01 (1.2656e-01)	Acc@1  96.09 ( 95.66)	Acc@5 100.00 ( 99.96)
Epoch: [42][ 50/391]	Time  0.044 ( 0.044)	Data  0.001 ( 0.004)	Loss 6.6772e-02 (1.2480e-01)	Acc@1  97.66 ( 95.65)	Acc@5 100.00 ( 99.97)
Epoch: [42][ 60/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.1835e-01 (1.2361e-01)	Acc@1  95.31 ( 95.72)	Acc@5 100.00 ( 99.97)
Epoch: [42][ 70/391]	Time  0.037 ( 0.043)	Data  0.001 ( 0.003)	Loss 7.9651e-02 (1.2075e-01)	Acc@1  98.44 ( 95.83)	Acc@5 100.00 ( 99.97)
Epoch: [42][ 80/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.4832e-01 (1.1984e-01)	Acc@1  93.75 ( 95.87)	Acc@5 100.00 ( 99.97)
Epoch: [42][ 90/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.5076e-01 (1.1951e-01)	Acc@1  92.97 ( 95.90)	Acc@5 100.00 ( 99.97)
Epoch: [42][100/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.0040e-01 (1.2191e-01)	Acc@1  96.88 ( 95.80)	Acc@5 100.00 ( 99.97)
Epoch: [42][110/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4343e-01 (1.2411e-01)	Acc@1  95.31 ( 95.67)	Acc@5 100.00 ( 99.97)
Epoch: [42][120/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0498e-01 (1.2403e-01)	Acc@1  97.66 ( 95.71)	Acc@5 100.00 ( 99.97)
Epoch: [42][130/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4197e-01 (1.2498e-01)	Acc@1  93.75 ( 95.69)	Acc@5 100.00 ( 99.96)
Epoch: [42][140/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4624e-01 (1.2573e-01)	Acc@1  96.88 ( 95.70)	Acc@5 100.00 ( 99.97)
Epoch: [42][150/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3708e-01 (1.2606e-01)	Acc@1  95.31 ( 95.67)	Acc@5 100.00 ( 99.97)
Epoch: [42][160/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3550e-01 (1.2690e-01)	Acc@1  96.09 ( 95.63)	Acc@5 100.00 ( 99.97)
Epoch: [42][170/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.1858e-02 (1.2669e-01)	Acc@1  96.09 ( 95.62)	Acc@5 100.00 ( 99.97)
Epoch: [42][180/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4001e-01 (1.2688e-01)	Acc@1  94.53 ( 95.61)	Acc@5 100.00 ( 99.97)
Epoch: [42][190/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0736e-01 (1.2698e-01)	Acc@1  96.09 ( 95.61)	Acc@5 100.00 ( 99.97)
Epoch: [42][200/391]	Time  0.040 ( 0.041)	Data  0.002 ( 0.002)	Loss 1.5601e-01 (1.2666e-01)	Acc@1  92.19 ( 95.64)	Acc@5 100.00 ( 99.97)
Epoch: [42][210/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7200e-01 (1.2714e-01)	Acc@1  95.31 ( 95.61)	Acc@5 100.00 ( 99.97)
Epoch: [42][220/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.1187e-02 (1.2624e-01)	Acc@1  97.66 ( 95.65)	Acc@5 100.00 ( 99.97)
Epoch: [42][230/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7676e-01 (1.2709e-01)	Acc@1  95.31 ( 95.63)	Acc@5 100.00 ( 99.97)
Epoch: [42][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5149e-01 (1.2743e-01)	Acc@1  92.19 ( 95.60)	Acc@5 100.00 ( 99.97)
Epoch: [42][250/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5283e-01 (1.2779e-01)	Acc@1  94.53 ( 95.59)	Acc@5 100.00 ( 99.97)
Epoch: [42][260/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.0437e-01 (1.2805e-01)	Acc@1  96.88 ( 95.59)	Acc@5 100.00 ( 99.97)
Epoch: [42][270/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.5149e-01 (1.2791e-01)	Acc@1  93.75 ( 95.56)	Acc@5 100.00 ( 99.97)
Epoch: [42][280/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.9128e-01 (1.2790e-01)	Acc@1  92.97 ( 95.56)	Acc@5 100.00 ( 99.97)
Epoch: [42][290/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.7249e-01 (1.2793e-01)	Acc@1  93.75 ( 95.55)	Acc@5 100.00 ( 99.97)
Epoch: [42][300/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.3660e-01 (1.2798e-01)	Acc@1  97.66 ( 95.56)	Acc@5 100.00 ( 99.97)
Epoch: [42][310/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.5112e-01 (1.2742e-01)	Acc@1  95.31 ( 95.59)	Acc@5 100.00 ( 99.96)
Epoch: [42][320/391]	Time  0.045 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.4551e-01 (1.2833e-01)	Acc@1  94.53 ( 95.55)	Acc@5 100.00 ( 99.96)
Epoch: [42][330/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.1737e-01 (1.2800e-01)	Acc@1  96.88 ( 95.56)	Acc@5  99.22 ( 99.96)
Epoch: [42][340/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.3013e-01 (1.2813e-01)	Acc@1  95.31 ( 95.55)	Acc@5  99.22 ( 99.96)
Epoch: [42][350/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 9.3384e-02 (1.2803e-01)	Acc@1  97.66 ( 95.55)	Acc@5 100.00 ( 99.96)
Epoch: [42][360/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.7932e-01 (1.2818e-01)	Acc@1  92.19 ( 95.55)	Acc@5 100.00 ( 99.96)
Epoch: [42][370/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.0522e-01 (1.2810e-01)	Acc@1  96.88 ( 95.57)	Acc@5 100.00 ( 99.96)
Epoch: [42][380/391]	Time  0.043 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.2756e-01 (1.2807e-01)	Acc@1  95.31 ( 95.57)	Acc@5 100.00 ( 99.96)
Epoch: [42][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.0032e-01 (1.2788e-01)	Acc@1  92.50 ( 95.58)	Acc@5  98.75 ( 99.96)
## e[42] optimizer.zero_grad (sum) time: 0.24970674514770508
## e[42]       loss.backward (sum) time: 4.143203258514404
## e[42]      optimizer.step (sum) time: 1.7685267925262451
## epoch[42] training(only) time: 15.85586929321289
# Switched to evaluate mode...
Test: [  0/100]	Time  0.162 ( 0.162)	Loss 2.7905e-01 (2.7905e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.021 ( 0.033)	Loss 3.1543e-01 (2.9578e-01)	Acc@1  86.00 ( 90.36)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.017 ( 0.027)	Loss 3.4741e-01 (3.0809e-01)	Acc@1  87.00 ( 90.24)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.020 ( 0.025)	Loss 3.9966e-01 (3.1185e-01)	Acc@1  85.00 ( 90.23)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.023 ( 0.024)	Loss 3.6938e-01 (3.1174e-01)	Acc@1  89.00 ( 90.12)	Acc@5  99.00 ( 99.73)
Test: [ 50/100]	Time  0.017 ( 0.023)	Loss 2.0972e-01 (3.0851e-01)	Acc@1  93.00 ( 90.39)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.017 ( 0.022)	Loss 3.5840e-01 (3.0667e-01)	Acc@1  92.00 ( 90.36)	Acc@5  99.00 ( 99.77)
Test: [ 70/100]	Time  0.021 ( 0.022)	Loss 4.2456e-01 (3.0365e-01)	Acc@1  86.00 ( 90.27)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.019 ( 0.022)	Loss 1.1847e-01 (3.0660e-01)	Acc@1  94.00 ( 90.19)	Acc@5 100.00 ( 99.73)
Test: [ 90/100]	Time  0.017 ( 0.021)	Loss 2.0728e-01 (3.0457e-01)	Acc@1  93.00 ( 90.26)	Acc@5 100.00 ( 99.74)
 * Acc@1 90.310 Acc@5 99.750
### epoch[42] execution time: 18.075923919677734
EPOCH 43
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [43][  0/391]	Time  0.216 ( 0.216)	Data  0.170 ( 0.170)	Loss 1.2225e-01 (1.2225e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [43][ 10/391]	Time  0.039 ( 0.056)	Data  0.001 ( 0.016)	Loss 7.3486e-02 (1.3598e-01)	Acc@1  97.66 ( 95.10)	Acc@5 100.00 (100.00)
Epoch: [43][ 20/391]	Time  0.041 ( 0.048)	Data  0.001 ( 0.009)	Loss 9.7412e-02 (1.3473e-01)	Acc@1  96.88 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [43][ 30/391]	Time  0.038 ( 0.045)	Data  0.001 ( 0.006)	Loss 1.0614e-01 (1.2914e-01)	Acc@1  96.09 ( 95.26)	Acc@5 100.00 ( 99.97)
Epoch: [43][ 40/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.005)	Loss 1.5540e-01 (1.2776e-01)	Acc@1  92.19 ( 95.27)	Acc@5 100.00 ( 99.98)
Epoch: [43][ 50/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.3330e-01 (1.2832e-01)	Acc@1  96.09 ( 95.27)	Acc@5 100.00 ( 99.98)
Epoch: [43][ 60/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.8774e-01 (1.2670e-01)	Acc@1  94.53 ( 95.39)	Acc@5 100.00 ( 99.99)
Epoch: [43][ 70/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.1261e-01 (1.2534e-01)	Acc@1  96.09 ( 95.47)	Acc@5 100.00 ( 99.98)
Epoch: [43][ 80/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.1200e-01 (1.2386e-01)	Acc@1  97.66 ( 95.56)	Acc@5 100.00 ( 99.98)
Epoch: [43][ 90/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 9.0454e-02 (1.2511e-01)	Acc@1  97.66 ( 95.54)	Acc@5 100.00 ( 99.97)
Epoch: [43][100/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.6028e-01 (1.2540e-01)	Acc@1  94.53 ( 95.54)	Acc@5  99.22 ( 99.95)
Epoch: [43][110/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0364e-01 (1.2498e-01)	Acc@1  95.31 ( 95.55)	Acc@5 100.00 ( 99.96)
Epoch: [43][120/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.7717e-02 (1.2474e-01)	Acc@1  97.66 ( 95.61)	Acc@5 100.00 ( 99.96)
Epoch: [43][130/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2323e-01 (1.2589e-01)	Acc@1  97.66 ( 95.58)	Acc@5 100.00 ( 99.96)
Epoch: [43][140/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.0200e-02 (1.2522e-01)	Acc@1  96.88 ( 95.64)	Acc@5 100.00 ( 99.96)
Epoch: [43][150/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1664e-01 (1.2520e-01)	Acc@1  95.31 ( 95.64)	Acc@5 100.00 ( 99.96)
Epoch: [43][160/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4050e-01 (1.2493e-01)	Acc@1  95.31 ( 95.71)	Acc@5 100.00 ( 99.97)
Epoch: [43][170/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.6721e-02 (1.2413e-01)	Acc@1  97.66 ( 95.78)	Acc@5 100.00 ( 99.97)
Epoch: [43][180/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.5359e-02 (1.2308e-01)	Acc@1 100.00 ( 95.81)	Acc@5 100.00 ( 99.97)
Epoch: [43][190/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0767e-01 (1.2294e-01)	Acc@1  96.09 ( 95.83)	Acc@5 100.00 ( 99.97)
Epoch: [43][200/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3208e-01 (1.2324e-01)	Acc@1  96.88 ( 95.84)	Acc@5 100.00 ( 99.97)
Epoch: [43][210/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.0864e-01 (1.2324e-01)	Acc@1  96.88 ( 95.82)	Acc@5 100.00 ( 99.97)
Epoch: [43][220/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.0419e-01 (1.2232e-01)	Acc@1  97.66 ( 95.86)	Acc@5 100.00 ( 99.97)
Epoch: [43][230/391]	Time  0.051 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.4351e-02 (1.2287e-01)	Acc@1  96.88 ( 95.83)	Acc@5 100.00 ( 99.97)
Epoch: [43][240/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 9.8511e-02 (1.2233e-01)	Acc@1  96.88 ( 95.87)	Acc@5 100.00 ( 99.97)
Epoch: [43][250/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.3501e-01 (1.2322e-01)	Acc@1  95.31 ( 95.83)	Acc@5 100.00 ( 99.97)
Epoch: [43][260/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 9.0149e-02 (1.2319e-01)	Acc@1  97.66 ( 95.84)	Acc@5 100.00 ( 99.96)
Epoch: [43][270/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.6162e-01 (1.2285e-01)	Acc@1  93.75 ( 95.86)	Acc@5 100.00 ( 99.96)
Epoch: [43][280/391]	Time  0.043 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.0278e-01 (1.2280e-01)	Acc@1  97.66 ( 95.85)	Acc@5 100.00 ( 99.96)
Epoch: [43][290/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 9.4971e-02 (1.2194e-01)	Acc@1  97.66 ( 95.89)	Acc@5 100.00 ( 99.96)
Epoch: [43][300/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.3086e-01 (1.2213e-01)	Acc@1  92.97 ( 95.88)	Acc@5 100.00 ( 99.95)
Epoch: [43][310/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 8.7585e-02 (1.2198e-01)	Acc@1  97.66 ( 95.87)	Acc@5 100.00 ( 99.95)
Epoch: [43][320/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.6748e-01 (1.2208e-01)	Acc@1  95.31 ( 95.87)	Acc@5 100.00 ( 99.96)
Epoch: [43][330/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 9.6619e-02 (1.2203e-01)	Acc@1  94.53 ( 95.87)	Acc@5 100.00 ( 99.96)
Epoch: [43][340/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.6016e-01 (1.2205e-01)	Acc@1  89.84 ( 95.88)	Acc@5 100.00 ( 99.96)
Epoch: [43][350/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.1436e-01 (1.2230e-01)	Acc@1  92.19 ( 95.86)	Acc@5 100.00 ( 99.96)
Epoch: [43][360/391]	Time  0.043 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.6416e-02 (1.2233e-01)	Acc@1  98.44 ( 95.86)	Acc@5 100.00 ( 99.96)
Epoch: [43][370/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 8.9783e-02 (1.2223e-01)	Acc@1  96.88 ( 95.85)	Acc@5 100.00 ( 99.96)
Epoch: [43][380/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 9.0637e-02 (1.2216e-01)	Acc@1  97.66 ( 95.85)	Acc@5 100.00 ( 99.96)
Epoch: [43][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.3154e-01 (1.2329e-01)	Acc@1  86.25 ( 95.78)	Acc@5 100.00 ( 99.96)
## e[43] optimizer.zero_grad (sum) time: 0.2499685287475586
## e[43]       loss.backward (sum) time: 4.174290418624878
## e[43]      optimizer.step (sum) time: 1.734241008758545
## epoch[43] training(only) time: 15.846966981887817
# Switched to evaluate mode...
Test: [  0/100]	Time  0.165 ( 0.165)	Loss 2.8760e-01 (2.8760e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.017 ( 0.032)	Loss 3.8013e-01 (3.0391e-01)	Acc@1  89.00 ( 91.09)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.017 ( 0.026)	Loss 3.5107e-01 (3.0965e-01)	Acc@1  87.00 ( 90.95)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.019 ( 0.024)	Loss 4.3774e-01 (3.1676e-01)	Acc@1  87.00 ( 90.42)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.020 ( 0.023)	Loss 3.7402e-01 (3.1645e-01)	Acc@1  86.00 ( 90.32)	Acc@5  99.00 ( 99.66)
Test: [ 50/100]	Time  0.019 ( 0.023)	Loss 2.4719e-01 (3.1152e-01)	Acc@1  93.00 ( 90.43)	Acc@5 100.00 ( 99.65)
Test: [ 60/100]	Time  0.022 ( 0.023)	Loss 3.2886e-01 (3.1101e-01)	Acc@1  92.00 ( 90.39)	Acc@5 100.00 ( 99.69)
Test: [ 70/100]	Time  0.018 ( 0.022)	Loss 4.0332e-01 (3.0897e-01)	Acc@1  89.00 ( 90.30)	Acc@5 100.00 ( 99.69)
Test: [ 80/100]	Time  0.017 ( 0.022)	Loss 1.2341e-01 (3.1176e-01)	Acc@1  96.00 ( 90.15)	Acc@5 100.00 ( 99.70)
Test: [ 90/100]	Time  0.020 ( 0.021)	Loss 2.1790e-01 (3.0968e-01)	Acc@1  94.00 ( 90.29)	Acc@5 100.00 ( 99.73)
 * Acc@1 90.370 Acc@5 99.730
### epoch[43] execution time: 18.050057649612427
EPOCH 44
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [44][  0/391]	Time  0.213 ( 0.213)	Data  0.168 ( 0.168)	Loss 6.6650e-02 (6.6650e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [44][ 10/391]	Time  0.045 ( 0.056)	Data  0.001 ( 0.016)	Loss 6.5063e-02 (1.2360e-01)	Acc@1  98.44 ( 95.45)	Acc@5 100.00 ( 99.93)
Epoch: [44][ 20/391]	Time  0.038 ( 0.049)	Data  0.001 ( 0.009)	Loss 1.6162e-01 (1.2655e-01)	Acc@1  92.97 ( 95.28)	Acc@5 100.00 ( 99.96)
Epoch: [44][ 30/391]	Time  0.038 ( 0.046)	Data  0.001 ( 0.006)	Loss 7.9529e-02 (1.2874e-01)	Acc@1  96.88 ( 95.26)	Acc@5 100.00 ( 99.92)
Epoch: [44][ 40/391]	Time  0.039 ( 0.045)	Data  0.001 ( 0.005)	Loss 1.4062e-01 (1.2571e-01)	Acc@1  94.53 ( 95.27)	Acc@5 100.00 ( 99.94)
Epoch: [44][ 50/391]	Time  0.043 ( 0.044)	Data  0.001 ( 0.004)	Loss 9.5215e-02 (1.2398e-01)	Acc@1  99.22 ( 95.47)	Acc@5 100.00 ( 99.95)
Epoch: [44][ 60/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.4050e-01 (1.2389e-01)	Acc@1  96.88 ( 95.41)	Acc@5 100.00 ( 99.96)
Epoch: [44][ 70/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.1377e-01 (1.2208e-01)	Acc@1  94.53 ( 95.50)	Acc@5 100.00 ( 99.97)
Epoch: [44][ 80/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.2286e-01 (1.1956e-01)	Acc@1  94.53 ( 95.63)	Acc@5 100.00 ( 99.97)
Epoch: [44][ 90/391]	Time  0.045 ( 0.042)	Data  0.001 ( 0.003)	Loss 9.9609e-02 (1.2027e-01)	Acc@1  96.88 ( 95.59)	Acc@5 100.00 ( 99.97)
Epoch: [44][100/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.2598e-01 (1.2236e-01)	Acc@1  96.88 ( 95.56)	Acc@5 100.00 ( 99.97)
Epoch: [44][110/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 9.8083e-02 (1.2149e-01)	Acc@1  96.88 ( 95.62)	Acc@5 100.00 ( 99.97)
Epoch: [44][120/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.5662e-01 (1.2038e-01)	Acc@1  96.88 ( 95.74)	Acc@5 100.00 ( 99.97)
Epoch: [44][130/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.4075e-01 (1.2174e-01)	Acc@1  93.75 ( 95.68)	Acc@5 100.00 ( 99.97)
Epoch: [44][140/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0327e-01 (1.2123e-01)	Acc@1  96.09 ( 95.70)	Acc@5 100.00 ( 99.97)
Epoch: [44][150/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.9976e-02 (1.2020e-01)	Acc@1  96.09 ( 95.74)	Acc@5 100.00 ( 99.97)
Epoch: [44][160/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5698e-01 (1.2085e-01)	Acc@1  93.75 ( 95.72)	Acc@5  99.22 ( 99.97)
Epoch: [44][170/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1292e-01 (1.2009e-01)	Acc@1  96.88 ( 95.77)	Acc@5 100.00 ( 99.97)
Epoch: [44][180/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.2529e-02 (1.2130e-01)	Acc@1  96.09 ( 95.74)	Acc@5 100.00 ( 99.97)
Epoch: [44][190/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6174e-01 (1.2203e-01)	Acc@1  93.75 ( 95.69)	Acc@5 100.00 ( 99.97)
Epoch: [44][200/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5930e-01 (1.2179e-01)	Acc@1  92.97 ( 95.70)	Acc@5 100.00 ( 99.97)
Epoch: [44][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2115e-01 (1.2205e-01)	Acc@1  96.88 ( 95.70)	Acc@5 100.00 ( 99.97)
Epoch: [44][220/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2878e-01 (1.2233e-01)	Acc@1  96.09 ( 95.68)	Acc@5 100.00 ( 99.96)
Epoch: [44][230/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.9824e-02 (1.2264e-01)	Acc@1  98.44 ( 95.68)	Acc@5 100.00 ( 99.97)
Epoch: [44][240/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5576e-01 (1.2270e-01)	Acc@1  92.19 ( 95.66)	Acc@5 100.00 ( 99.97)
Epoch: [44][250/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.7881e-02 (1.2200e-01)	Acc@1  97.66 ( 95.70)	Acc@5 100.00 ( 99.97)
Epoch: [44][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4514e-01 (1.2212e-01)	Acc@1  95.31 ( 95.73)	Acc@5 100.00 ( 99.97)
Epoch: [44][270/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0425e-01 (1.2215e-01)	Acc@1  96.88 ( 95.73)	Acc@5 100.00 ( 99.96)
Epoch: [44][280/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.2339e-01 (1.2209e-01)	Acc@1  93.75 ( 95.75)	Acc@5 100.00 ( 99.96)
Epoch: [44][290/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 8.4900e-02 (1.2203e-01)	Acc@1  96.88 ( 95.73)	Acc@5 100.00 ( 99.97)
Epoch: [44][300/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.2888e-01 (1.2261e-01)	Acc@1  93.75 ( 95.71)	Acc@5 100.00 ( 99.97)
Epoch: [44][310/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.6702e-02 (1.2219e-01)	Acc@1  97.66 ( 95.72)	Acc@5 100.00 ( 99.97)
Epoch: [44][320/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.7590e-01 (1.2251e-01)	Acc@1  94.53 ( 95.71)	Acc@5 100.00 ( 99.97)
Epoch: [44][330/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.6345e-01 (1.2325e-01)	Acc@1  95.31 ( 95.69)	Acc@5 100.00 ( 99.96)
Epoch: [44][340/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 9.9731e-02 (1.2306e-01)	Acc@1  96.09 ( 95.69)	Acc@5 100.00 ( 99.97)
Epoch: [44][350/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.1627e-01 (1.2319e-01)	Acc@1  96.09 ( 95.68)	Acc@5 100.00 ( 99.96)
Epoch: [44][360/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.5308e-01 (1.2347e-01)	Acc@1  95.31 ( 95.67)	Acc@5 100.00 ( 99.96)
Epoch: [44][370/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.4038e-01 (1.2377e-01)	Acc@1  92.97 ( 95.65)	Acc@5 100.00 ( 99.96)
Epoch: [44][380/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.0553e-01 (1.2321e-01)	Acc@1  95.31 ( 95.68)	Acc@5 100.00 ( 99.97)
Epoch: [44][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.3293e-02 (1.2259e-01)	Acc@1  97.50 ( 95.71)	Acc@5 100.00 ( 99.97)
## e[44] optimizer.zero_grad (sum) time: 0.25078344345092773
## e[44]       loss.backward (sum) time: 4.189353704452515
## e[44]      optimizer.step (sum) time: 1.7328057289123535
## epoch[44] training(only) time: 15.945053339004517
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 2.5464e-01 (2.5464e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.036)	Loss 3.4351e-01 (2.7747e-01)	Acc@1  87.00 ( 90.73)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.017 ( 0.028)	Loss 4.0088e-01 (3.0241e-01)	Acc@1  86.00 ( 90.38)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.019 ( 0.026)	Loss 4.1431e-01 (3.0644e-01)	Acc@1  86.00 ( 90.29)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.017 ( 0.024)	Loss 3.3130e-01 (3.0796e-01)	Acc@1  88.00 ( 90.27)	Acc@5  99.00 ( 99.63)
Test: [ 50/100]	Time  0.019 ( 0.024)	Loss 1.7371e-01 (3.0504e-01)	Acc@1  92.00 ( 90.43)	Acc@5 100.00 ( 99.67)
Test: [ 60/100]	Time  0.023 ( 0.023)	Loss 3.6060e-01 (3.0518e-01)	Acc@1  90.00 ( 90.44)	Acc@5 100.00 ( 99.70)
Test: [ 70/100]	Time  0.020 ( 0.023)	Loss 3.6353e-01 (3.0149e-01)	Acc@1  90.00 ( 90.46)	Acc@5 100.00 ( 99.72)
Test: [ 80/100]	Time  0.023 ( 0.023)	Loss 1.4661e-01 (3.0531e-01)	Acc@1  93.00 ( 90.36)	Acc@5 100.00 ( 99.70)
Test: [ 90/100]	Time  0.021 ( 0.023)	Loss 2.1851e-01 (3.0430e-01)	Acc@1  95.00 ( 90.41)	Acc@5 100.00 ( 99.71)
 * Acc@1 90.510 Acc@5 99.730
### epoch[44] execution time: 18.274404048919678
EPOCH 45
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [45][  0/391]	Time  0.218 ( 0.218)	Data  0.177 ( 0.177)	Loss 1.8518e-01 (1.8518e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [45][ 10/391]	Time  0.044 ( 0.057)	Data  0.001 ( 0.017)	Loss 6.7383e-02 (1.1500e-01)	Acc@1  98.44 ( 96.16)	Acc@5 100.00 ( 99.93)
Epoch: [45][ 20/391]	Time  0.039 ( 0.049)	Data  0.001 ( 0.009)	Loss 1.1182e-01 (1.2042e-01)	Acc@1  95.31 ( 96.02)	Acc@5 100.00 ( 99.93)
Epoch: [45][ 30/391]	Time  0.043 ( 0.046)	Data  0.001 ( 0.007)	Loss 1.4209e-01 (1.2837e-01)	Acc@1  96.09 ( 95.64)	Acc@5 100.00 ( 99.90)
Epoch: [45][ 40/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.005)	Loss 7.1045e-02 (1.2242e-01)	Acc@1  97.66 ( 95.87)	Acc@5 100.00 ( 99.90)
Epoch: [45][ 50/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.9482e-01 (1.2059e-01)	Acc@1  93.75 ( 95.85)	Acc@5 100.00 ( 99.92)
Epoch: [45][ 60/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.004)	Loss 6.2042e-02 (1.1614e-01)	Acc@1  98.44 ( 96.04)	Acc@5 100.00 ( 99.94)
Epoch: [45][ 70/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.3708e-01 (1.1295e-01)	Acc@1  94.53 ( 96.18)	Acc@5 100.00 ( 99.94)
Epoch: [45][ 80/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.1743e-01 (1.1226e-01)	Acc@1  96.09 ( 96.28)	Acc@5 100.00 ( 99.94)
Epoch: [45][ 90/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.3538e-01 (1.1345e-01)	Acc@1  95.31 ( 96.26)	Acc@5 100.00 ( 99.93)
Epoch: [45][100/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.3193e-01 (1.1549e-01)	Acc@1  95.31 ( 96.14)	Acc@5  99.22 ( 99.93)
Epoch: [45][110/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.4441e-01 (1.1522e-01)	Acc@1  95.31 ( 96.14)	Acc@5 100.00 ( 99.94)
Epoch: [45][120/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.3525e-01 (1.1494e-01)	Acc@1  93.75 ( 96.14)	Acc@5 100.00 ( 99.94)
Epoch: [45][130/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.3049e-02 (1.1493e-01)	Acc@1  98.44 ( 96.12)	Acc@5 100.00 ( 99.93)
Epoch: [45][140/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.3669e-01 (1.1512e-01)	Acc@1  91.41 ( 96.09)	Acc@5 100.00 ( 99.94)
Epoch: [45][150/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5271e-01 (1.1597e-01)	Acc@1  94.53 ( 96.04)	Acc@5 100.00 ( 99.94)
Epoch: [45][160/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5796e-01 (1.1585e-01)	Acc@1  94.53 ( 96.02)	Acc@5 100.00 ( 99.95)
Epoch: [45][170/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.9250e-01 (1.1612e-01)	Acc@1  92.19 ( 96.03)	Acc@5  99.22 ( 99.95)
Epoch: [45][180/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4929e-01 (1.1668e-01)	Acc@1  94.53 ( 96.01)	Acc@5 100.00 ( 99.94)
Epoch: [45][190/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4014e-01 (1.1637e-01)	Acc@1  95.31 ( 96.03)	Acc@5 100.00 ( 99.95)
Epoch: [45][200/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1243e-01 (1.1660e-01)	Acc@1  95.31 ( 95.98)	Acc@5 100.00 ( 99.95)
Epoch: [45][210/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2042e-01 (1.1818e-01)	Acc@1  97.66 ( 95.94)	Acc@5 100.00 ( 99.95)
Epoch: [45][220/391]	Time  0.054 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.3506e-02 (1.1796e-01)	Acc@1  96.88 ( 95.95)	Acc@5 100.00 ( 99.95)
Epoch: [45][230/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.7515e-02 (1.1712e-01)	Acc@1  96.88 ( 95.98)	Acc@5 100.00 ( 99.95)
Epoch: [45][240/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5649e-01 (1.1750e-01)	Acc@1  94.53 ( 95.94)	Acc@5 100.00 ( 99.95)
Epoch: [45][250/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7407e-01 (1.1787e-01)	Acc@1  93.75 ( 95.97)	Acc@5 100.00 ( 99.95)
Epoch: [45][260/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1572e-01 (1.1829e-01)	Acc@1  96.09 ( 95.95)	Acc@5 100.00 ( 99.95)
Epoch: [45][270/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5698e-01 (1.1806e-01)	Acc@1  95.31 ( 95.96)	Acc@5 100.00 ( 99.95)
Epoch: [45][280/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2433e-01 (1.1811e-01)	Acc@1  95.31 ( 95.96)	Acc@5 100.00 ( 99.95)
Epoch: [45][290/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1951e-01 (1.1813e-01)	Acc@1  95.31 ( 95.97)	Acc@5 100.00 ( 99.95)
Epoch: [45][300/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4648e-01 (1.1910e-01)	Acc@1  96.09 ( 95.93)	Acc@5 100.00 ( 99.95)
Epoch: [45][310/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5454e-01 (1.1891e-01)	Acc@1  94.53 ( 95.93)	Acc@5 100.00 ( 99.95)
Epoch: [45][320/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.4563e-01 (1.1892e-01)	Acc@1  95.31 ( 95.93)	Acc@5 100.00 ( 99.95)
Epoch: [45][330/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.5894e-01 (1.1910e-01)	Acc@1  93.75 ( 95.92)	Acc@5 100.00 ( 99.94)
Epoch: [45][340/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.4856e-01 (1.1976e-01)	Acc@1  96.09 ( 95.89)	Acc@5 100.00 ( 99.95)
Epoch: [45][350/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.001)	Loss 8.1177e-02 (1.1986e-01)	Acc@1  97.66 ( 95.88)	Acc@5 100.00 ( 99.94)
Epoch: [45][360/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.5967e-01 (1.1967e-01)	Acc@1  92.97 ( 95.88)	Acc@5 100.00 ( 99.95)
Epoch: [45][370/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.2598e-01 (1.1982e-01)	Acc@1  94.53 ( 95.86)	Acc@5 100.00 ( 99.95)
Epoch: [45][380/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.2537e-01 (1.2006e-01)	Acc@1  98.44 ( 95.87)	Acc@5 100.00 ( 99.94)
Epoch: [45][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.4612e-01 (1.1983e-01)	Acc@1  93.75 ( 95.87)	Acc@5 100.00 ( 99.94)
## e[45] optimizer.zero_grad (sum) time: 0.24927639961242676
## e[45]       loss.backward (sum) time: 4.180461645126343
## e[45]      optimizer.step (sum) time: 1.7680041790008545
## epoch[45] training(only) time: 15.900550842285156
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 2.6074e-01 (2.6074e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.021 ( 0.036)	Loss 3.0225e-01 (2.9128e-01)	Acc@1  88.00 ( 90.91)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.019 ( 0.029)	Loss 3.6060e-01 (3.0006e-01)	Acc@1  86.00 ( 90.62)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.017 ( 0.026)	Loss 4.1772e-01 (3.0600e-01)	Acc@1  86.00 ( 90.52)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.020 ( 0.024)	Loss 3.3228e-01 (3.0545e-01)	Acc@1  93.00 ( 90.59)	Acc@5  99.00 ( 99.66)
Test: [ 50/100]	Time  0.018 ( 0.023)	Loss 1.6382e-01 (2.9982e-01)	Acc@1  93.00 ( 90.92)	Acc@5 100.00 ( 99.65)
Test: [ 60/100]	Time  0.021 ( 0.022)	Loss 3.6597e-01 (3.0004e-01)	Acc@1  90.00 ( 90.77)	Acc@5 100.00 ( 99.70)
Test: [ 70/100]	Time  0.017 ( 0.022)	Loss 3.7842e-01 (2.9692e-01)	Acc@1  89.00 ( 90.76)	Acc@5 100.00 ( 99.70)
Test: [ 80/100]	Time  0.021 ( 0.022)	Loss 1.4929e-01 (3.0121e-01)	Acc@1  94.00 ( 90.62)	Acc@5 100.00 ( 99.72)
Test: [ 90/100]	Time  0.023 ( 0.021)	Loss 2.5659e-01 (3.0026e-01)	Acc@1  93.00 ( 90.67)	Acc@5 100.00 ( 99.74)
 * Acc@1 90.760 Acc@5 99.750
### epoch[45] execution time: 18.133581161499023
EPOCH 46
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [46][  0/391]	Time  0.211 ( 0.211)	Data  0.165 ( 0.165)	Loss 1.2781e-01 (1.2781e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [46][ 10/391]	Time  0.041 ( 0.056)	Data  0.001 ( 0.016)	Loss 9.1431e-02 (1.1814e-01)	Acc@1  97.66 ( 96.38)	Acc@5 100.00 ( 99.93)
Epoch: [46][ 20/391]	Time  0.038 ( 0.049)	Data  0.001 ( 0.009)	Loss 6.6833e-02 (1.1174e-01)	Acc@1  97.66 ( 96.28)	Acc@5 100.00 ( 99.96)
Epoch: [46][ 30/391]	Time  0.042 ( 0.046)	Data  0.001 ( 0.006)	Loss 9.7595e-02 (1.1091e-01)	Acc@1  96.88 ( 96.24)	Acc@5 100.00 ( 99.97)
Epoch: [46][ 40/391]	Time  0.040 ( 0.045)	Data  0.001 ( 0.005)	Loss 8.0017e-02 (1.1108e-01)	Acc@1  97.66 ( 96.28)	Acc@5 100.00 ( 99.98)
Epoch: [46][ 50/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.1023e-01 (1.1435e-01)	Acc@1  96.88 ( 96.16)	Acc@5 100.00 ( 99.95)
Epoch: [46][ 60/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.004)	Loss 8.7341e-02 (1.1070e-01)	Acc@1  96.88 ( 96.23)	Acc@5 100.00 ( 99.96)
Epoch: [46][ 70/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.3318e-01 (1.0975e-01)	Acc@1  96.88 ( 96.32)	Acc@5 100.00 ( 99.97)
Epoch: [46][ 80/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.4233e-01 (1.0965e-01)	Acc@1  96.88 ( 96.32)	Acc@5 100.00 ( 99.97)
Epoch: [46][ 90/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.8347e-01 (1.1252e-01)	Acc@1  94.53 ( 96.23)	Acc@5 100.00 ( 99.97)
Epoch: [46][100/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 9.6558e-02 (1.1197e-01)	Acc@1  96.88 ( 96.23)	Acc@5 100.00 ( 99.97)
Epoch: [46][110/391]	Time  0.050 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3611e-01 (1.1374e-01)	Acc@1  94.53 ( 96.11)	Acc@5 100.00 ( 99.97)
Epoch: [46][120/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5027e-01 (1.1493e-01)	Acc@1  92.19 ( 96.08)	Acc@5 100.00 ( 99.97)
Epoch: [46][130/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3696e-01 (1.1551e-01)	Acc@1  94.53 ( 96.06)	Acc@5 100.00 ( 99.97)
Epoch: [46][140/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8645e-02 (1.1405e-01)	Acc@1  98.44 ( 96.10)	Acc@5 100.00 ( 99.97)
Epoch: [46][150/391]	Time  0.038 ( 0.041)	Data  0.002 ( 0.002)	Loss 9.6863e-02 (1.1461e-01)	Acc@1  97.66 ( 96.10)	Acc@5 100.00 ( 99.97)
Epoch: [46][160/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.1482e-02 (1.1545e-01)	Acc@1  97.66 ( 96.07)	Acc@5 100.00 ( 99.97)
Epoch: [46][170/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.4829e-02 (1.1633e-01)	Acc@1  99.22 ( 96.05)	Acc@5 100.00 ( 99.97)
Epoch: [46][180/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.1594e-02 (1.1664e-01)	Acc@1  99.22 ( 96.04)	Acc@5 100.00 ( 99.97)
Epoch: [46][190/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2643e-02 (1.1516e-01)	Acc@1  98.44 ( 96.11)	Acc@5 100.00 ( 99.97)
Epoch: [46][200/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.2170e-01 (1.1626e-01)	Acc@1  95.31 ( 96.03)	Acc@5 100.00 ( 99.97)
Epoch: [46][210/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.5625e-01 (1.1676e-01)	Acc@1  92.97 ( 95.98)	Acc@5 100.00 ( 99.97)
Epoch: [46][220/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.1829e-01 (1.1680e-01)	Acc@1  95.31 ( 96.01)	Acc@5 100.00 ( 99.96)
Epoch: [46][230/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.5454e-01 (1.1718e-01)	Acc@1  95.31 ( 96.00)	Acc@5  99.22 ( 99.96)
Epoch: [46][240/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.002)	Loss 9.3994e-02 (1.1730e-01)	Acc@1  97.66 ( 95.99)	Acc@5 100.00 ( 99.95)
Epoch: [46][250/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.2164e-01 (1.1681e-01)	Acc@1  93.75 ( 96.00)	Acc@5 100.00 ( 99.95)
Epoch: [46][260/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 8.4656e-02 (1.1635e-01)	Acc@1  96.88 ( 96.04)	Acc@5 100.00 ( 99.95)
Epoch: [46][270/391]	Time  0.045 ( 0.040)	Data  0.001 ( 0.002)	Loss 7.6843e-02 (1.1632e-01)	Acc@1  98.44 ( 96.02)	Acc@5 100.00 ( 99.95)
Epoch: [46][280/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.0333e-02 (1.1690e-01)	Acc@1  99.22 ( 96.00)	Acc@5 100.00 ( 99.95)
Epoch: [46][290/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.0742e-01 (1.1663e-01)	Acc@1  96.09 ( 96.01)	Acc@5 100.00 ( 99.95)
Epoch: [46][300/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.3379e-01 (1.1633e-01)	Acc@1  93.75 ( 96.00)	Acc@5 100.00 ( 99.95)
Epoch: [46][310/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.5366e-01 (1.1686e-01)	Acc@1  92.19 ( 95.99)	Acc@5 100.00 ( 99.95)
Epoch: [46][320/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 8.6609e-02 (1.1669e-01)	Acc@1  98.44 ( 95.99)	Acc@5 100.00 ( 99.95)
Epoch: [46][330/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 8.2153e-02 (1.1624e-01)	Acc@1  96.09 ( 96.01)	Acc@5 100.00 ( 99.96)
Epoch: [46][340/391]	Time  0.047 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.7542e-01 (1.1663e-01)	Acc@1  93.75 ( 95.99)	Acc@5 100.00 ( 99.96)
Epoch: [46][350/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.3611e-01 (1.1702e-01)	Acc@1  95.31 ( 95.97)	Acc@5 100.00 ( 99.96)
Epoch: [46][360/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 8.1299e-02 (1.1764e-01)	Acc@1  96.88 ( 95.94)	Acc@5 100.00 ( 99.96)
Epoch: [46][370/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.6438e-02 (1.1734e-01)	Acc@1 100.00 ( 95.96)	Acc@5 100.00 ( 99.96)
Epoch: [46][380/391]	Time  0.043 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.2964e-01 (1.1749e-01)	Acc@1  95.31 ( 95.95)	Acc@5 100.00 ( 99.96)
Epoch: [46][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.1475e-01 (1.1696e-01)	Acc@1  97.50 ( 95.97)	Acc@5 100.00 ( 99.96)
## e[46] optimizer.zero_grad (sum) time: 0.24993467330932617
## e[46]       loss.backward (sum) time: 4.096691131591797
## e[46]      optimizer.step (sum) time: 1.778714656829834
## epoch[46] training(only) time: 15.81499695777893
# Switched to evaluate mode...
Test: [  0/100]	Time  0.166 ( 0.166)	Loss 2.6685e-01 (2.6685e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.020 ( 0.034)	Loss 3.2251e-01 (2.7762e-01)	Acc@1  91.00 ( 91.36)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.022 ( 0.027)	Loss 3.7305e-01 (2.9812e-01)	Acc@1  88.00 ( 90.95)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.021 ( 0.025)	Loss 4.4971e-01 (3.0595e-01)	Acc@1  85.00 ( 90.42)	Acc@5 100.00 ( 99.77)
Test: [ 40/100]	Time  0.020 ( 0.023)	Loss 3.5669e-01 (3.0764e-01)	Acc@1  89.00 ( 90.41)	Acc@5  99.00 ( 99.76)
Test: [ 50/100]	Time  0.017 ( 0.023)	Loss 1.6125e-01 (3.0128e-01)	Acc@1  94.00 ( 90.51)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.017 ( 0.022)	Loss 3.7280e-01 (2.9970e-01)	Acc@1  90.00 ( 90.46)	Acc@5  99.00 ( 99.75)
Test: [ 70/100]	Time  0.018 ( 0.022)	Loss 3.8208e-01 (2.9678e-01)	Acc@1  90.00 ( 90.49)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.017 ( 0.022)	Loss 1.6150e-01 (3.0190e-01)	Acc@1  94.00 ( 90.40)	Acc@5 100.00 ( 99.73)
Test: [ 90/100]	Time  0.019 ( 0.021)	Loss 2.1667e-01 (2.9960e-01)	Acc@1  93.00 ( 90.47)	Acc@5 100.00 ( 99.75)
 * Acc@1 90.540 Acc@5 99.760
### epoch[46] execution time: 18.014337301254272
EPOCH 47
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [47][  0/391]	Time  0.211 ( 0.211)	Data  0.165 ( 0.165)	Loss 7.7087e-02 (7.7087e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [47][ 10/391]	Time  0.043 ( 0.056)	Data  0.001 ( 0.016)	Loss 1.2854e-01 (9.4091e-02)	Acc@1  96.88 ( 97.16)	Acc@5  99.22 ( 99.93)
Epoch: [47][ 20/391]	Time  0.039 ( 0.048)	Data  0.001 ( 0.009)	Loss 1.9434e-01 (1.1200e-01)	Acc@1  92.97 ( 96.13)	Acc@5 100.00 ( 99.96)
Epoch: [47][ 30/391]	Time  0.040 ( 0.045)	Data  0.001 ( 0.006)	Loss 5.1788e-02 (1.0412e-01)	Acc@1  99.22 ( 96.50)	Acc@5 100.00 ( 99.97)
Epoch: [47][ 40/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.005)	Loss 5.4596e-02 (1.0526e-01)	Acc@1 100.00 ( 96.46)	Acc@5 100.00 ( 99.98)
Epoch: [47][ 50/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.5784e-01 (1.0997e-01)	Acc@1  95.31 ( 96.29)	Acc@5 100.00 ( 99.98)
Epoch: [47][ 60/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.0327e-01 (1.0560e-01)	Acc@1  97.66 ( 96.49)	Acc@5 100.00 ( 99.99)
Epoch: [47][ 70/391]	Time  0.035 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.6580e-02 (1.0425e-01)	Acc@1  97.66 ( 96.58)	Acc@5 100.00 ( 99.99)
Epoch: [47][ 80/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.6895e-01 (1.0489e-01)	Acc@1  95.31 ( 96.54)	Acc@5 100.00 ( 99.98)
Epoch: [47][ 90/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.2549e-01 (1.0438e-01)	Acc@1  94.53 ( 96.51)	Acc@5 100.00 ( 99.98)
Epoch: [47][100/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 1.4783e-01 (1.0611e-01)	Acc@1  96.09 ( 96.43)	Acc@5 100.00 ( 99.98)
Epoch: [47][110/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4758e-01 (1.0656e-01)	Acc@1  93.75 ( 96.42)	Acc@5 100.00 ( 99.99)
Epoch: [47][120/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2704e-02 (1.0698e-01)	Acc@1  97.66 ( 96.38)	Acc@5 100.00 ( 99.99)
Epoch: [47][130/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0309e-01 (1.0765e-01)	Acc@1  96.88 ( 96.32)	Acc@5 100.00 ( 99.98)
Epoch: [47][140/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.5205e-02 (1.0764e-01)	Acc@1  97.66 ( 96.35)	Acc@5 100.00 ( 99.98)
Epoch: [47][150/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1157e-01 (1.0638e-01)	Acc@1  96.09 ( 96.40)	Acc@5 100.00 ( 99.98)
Epoch: [47][160/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2122e-01 (1.0666e-01)	Acc@1  94.53 ( 96.39)	Acc@5 100.00 ( 99.99)
Epoch: [47][170/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.9365e-02 (1.0566e-01)	Acc@1  96.09 ( 96.42)	Acc@5 100.00 ( 99.99)
Epoch: [47][180/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.6130e-02 (1.0633e-01)	Acc@1  96.09 ( 96.37)	Acc@5 100.00 ( 99.99)
Epoch: [47][190/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1646e-01 (1.0744e-01)	Acc@1  96.09 ( 96.32)	Acc@5 100.00 ( 99.98)
Epoch: [47][200/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2183e-01 (1.0873e-01)	Acc@1  95.31 ( 96.28)	Acc@5 100.00 ( 99.98)
Epoch: [47][210/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.8420e-02 (1.0893e-01)	Acc@1  97.66 ( 96.29)	Acc@5 100.00 ( 99.98)
Epoch: [47][220/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6370e-01 (1.0947e-01)	Acc@1  95.31 ( 96.25)	Acc@5  99.22 ( 99.98)
Epoch: [47][230/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2305e-01 (1.0924e-01)	Acc@1  97.66 ( 96.26)	Acc@5 100.00 ( 99.98)
Epoch: [47][240/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2042e-01 (1.0940e-01)	Acc@1  96.88 ( 96.26)	Acc@5 100.00 ( 99.98)
Epoch: [47][250/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0870e-01 (1.0935e-01)	Acc@1  95.31 ( 96.25)	Acc@5 100.00 ( 99.98)
Epoch: [47][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.8989e-02 (1.0938e-01)	Acc@1  97.66 ( 96.24)	Acc@5 100.00 ( 99.98)
Epoch: [47][270/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4441e-01 (1.0956e-01)	Acc@1  93.75 ( 96.23)	Acc@5 100.00 ( 99.98)
Epoch: [47][280/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 9.7107e-02 (1.0953e-01)	Acc@1  97.66 ( 96.26)	Acc@5 100.00 ( 99.97)
Epoch: [47][290/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.7515e-02 (1.0931e-01)	Acc@1  96.88 ( 96.26)	Acc@5 100.00 ( 99.98)
Epoch: [47][300/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.4646e-02 (1.0948e-01)	Acc@1  97.66 ( 96.24)	Acc@5 100.00 ( 99.97)
Epoch: [47][310/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.6492e-01 (1.0949e-01)	Acc@1  94.53 ( 96.25)	Acc@5 100.00 ( 99.97)
Epoch: [47][320/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.0923e-02 (1.0992e-01)	Acc@1  98.44 ( 96.23)	Acc@5 100.00 ( 99.97)
Epoch: [47][330/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.0150e-02 (1.1021e-01)	Acc@1  99.22 ( 96.21)	Acc@5 100.00 ( 99.97)
Epoch: [47][340/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.0895e-01 (1.1067e-01)	Acc@1  96.09 ( 96.19)	Acc@5 100.00 ( 99.97)
Epoch: [47][350/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 9.3994e-02 (1.1129e-01)	Acc@1  97.66 ( 96.18)	Acc@5 100.00 ( 99.97)
Epoch: [47][360/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 8.9600e-02 (1.1147e-01)	Acc@1  97.66 ( 96.17)	Acc@5 100.00 ( 99.97)
Epoch: [47][370/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.6660e-02 (1.1122e-01)	Acc@1  96.88 ( 96.18)	Acc@5 100.00 ( 99.97)
Epoch: [47][380/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.0166e-01 (1.1179e-01)	Acc@1  94.53 ( 96.16)	Acc@5 100.00 ( 99.97)
Epoch: [47][390/391]	Time  0.027 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.5356e-01 (1.1232e-01)	Acc@1  96.25 ( 96.13)	Acc@5 100.00 ( 99.97)
## e[47] optimizer.zero_grad (sum) time: 0.250424861907959
## e[47]       loss.backward (sum) time: 4.207319736480713
## e[47]      optimizer.step (sum) time: 1.7183666229248047
## epoch[47] training(only) time: 15.919323682785034
# Switched to evaluate mode...
Test: [  0/100]	Time  0.180 ( 0.180)	Loss 2.5195e-01 (2.5195e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.017 ( 0.033)	Loss 3.5352e-01 (2.8809e-01)	Acc@1  88.00 ( 90.45)	Acc@5  99.00 ( 99.82)
Test: [ 20/100]	Time  0.019 ( 0.027)	Loss 4.7290e-01 (3.1420e-01)	Acc@1  82.00 ( 90.33)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.020 ( 0.025)	Loss 4.1699e-01 (3.2913e-01)	Acc@1  85.00 ( 89.84)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.017 ( 0.024)	Loss 3.6035e-01 (3.2715e-01)	Acc@1  89.00 ( 89.95)	Acc@5  99.00 ( 99.68)
Test: [ 50/100]	Time  0.017 ( 0.023)	Loss 1.7737e-01 (3.1931e-01)	Acc@1  93.00 ( 90.22)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.017 ( 0.023)	Loss 3.4131e-01 (3.1603e-01)	Acc@1  93.00 ( 90.25)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.019 ( 0.022)	Loss 4.0918e-01 (3.1241e-01)	Acc@1  88.00 ( 90.30)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.021 ( 0.022)	Loss 1.1865e-01 (3.1687e-01)	Acc@1  96.00 ( 90.26)	Acc@5 100.00 ( 99.72)
Test: [ 90/100]	Time  0.021 ( 0.022)	Loss 2.5586e-01 (3.1431e-01)	Acc@1  92.00 ( 90.34)	Acc@5 100.00 ( 99.73)
 * Acc@1 90.450 Acc@5 99.740
### epoch[47] execution time: 18.1633517742157
EPOCH 48
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [48][  0/391]	Time  0.213 ( 0.213)	Data  0.171 ( 0.171)	Loss 1.3721e-01 (1.3721e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [48][ 10/391]	Time  0.038 ( 0.055)	Data  0.001 ( 0.016)	Loss 7.4158e-02 (1.0822e-01)	Acc@1  97.66 ( 96.66)	Acc@5 100.00 (100.00)
Epoch: [48][ 20/391]	Time  0.040 ( 0.049)	Data  0.001 ( 0.009)	Loss 1.0522e-01 (1.0857e-01)	Acc@1  96.09 ( 96.50)	Acc@5 100.00 (100.00)
Epoch: [48][ 30/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.006)	Loss 7.6782e-02 (1.1646e-01)	Acc@1  97.66 ( 96.19)	Acc@5 100.00 (100.00)
Epoch: [48][ 40/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.005)	Loss 9.8755e-02 (1.1373e-01)	Acc@1  96.88 ( 96.21)	Acc@5 100.00 (100.00)
Epoch: [48][ 50/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.5308e-01 (1.1137e-01)	Acc@1  95.31 ( 96.26)	Acc@5 100.00 (100.00)
Epoch: [48][ 60/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.0431e-01 (1.0617e-01)	Acc@1  96.88 ( 96.44)	Acc@5 100.00 (100.00)
Epoch: [48][ 70/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 6.0059e-02 (1.0751e-01)	Acc@1  96.88 ( 96.39)	Acc@5 100.00 (100.00)
Epoch: [48][ 80/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.4905e-01 (1.1012e-01)	Acc@1  96.88 ( 96.20)	Acc@5 100.00 (100.00)
Epoch: [48][ 90/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.3538e-01 (1.1074e-01)	Acc@1  96.09 ( 96.25)	Acc@5 100.00 (100.00)
Epoch: [48][100/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.5637e-01 (1.1005e-01)	Acc@1  93.75 ( 96.29)	Acc@5 100.00 (100.00)
Epoch: [48][110/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.1383e-01 (1.1148e-01)	Acc@1  96.09 ( 96.26)	Acc@5 100.00 ( 99.99)
Epoch: [48][120/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.1914e-01 (1.1148e-01)	Acc@1  96.09 ( 96.25)	Acc@5 100.00 ( 99.99)
Epoch: [48][130/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2238e-01 (1.1098e-01)	Acc@1  96.88 ( 96.23)	Acc@5 100.00 ( 99.99)
Epoch: [48][140/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3171e-01 (1.1072e-01)	Acc@1  95.31 ( 96.27)	Acc@5 100.00 ( 99.99)
Epoch: [48][150/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1023e-01 (1.1035e-01)	Acc@1  95.31 ( 96.25)	Acc@5 100.00 ( 99.99)
Epoch: [48][160/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.3445e-02 (1.1066e-01)	Acc@1  95.31 ( 96.22)	Acc@5 100.00 ( 99.99)
Epoch: [48][170/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.4657e-02 (1.1096e-01)	Acc@1  97.66 ( 96.22)	Acc@5 100.00 ( 99.99)
Epoch: [48][180/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.8929e-02 (1.1067e-01)	Acc@1  98.44 ( 96.21)	Acc@5 100.00 ( 99.99)
Epoch: [48][190/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5015e-01 (1.1086e-01)	Acc@1  94.53 ( 96.21)	Acc@5 100.00 ( 99.98)
Epoch: [48][200/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2335e-01 (1.1023e-01)	Acc@1  95.31 ( 96.25)	Acc@5 100.00 ( 99.98)
Epoch: [48][210/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.2388e-02 (1.1032e-01)	Acc@1  98.44 ( 96.25)	Acc@5 100.00 ( 99.98)
Epoch: [48][220/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1230e-01 (1.1064e-01)	Acc@1  96.88 ( 96.24)	Acc@5 100.00 ( 99.98)
Epoch: [48][230/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.8298e-02 (1.1011e-01)	Acc@1  96.88 ( 96.23)	Acc@5 100.00 ( 99.98)
Epoch: [48][240/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.8867e-02 (1.0991e-01)	Acc@1  96.88 ( 96.24)	Acc@5 100.00 ( 99.98)
Epoch: [48][250/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1713e-01 (1.1016e-01)	Acc@1  94.53 ( 96.24)	Acc@5 100.00 ( 99.98)
Epoch: [48][260/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7760e-02 (1.0989e-01)	Acc@1  98.44 ( 96.25)	Acc@5 100.00 ( 99.98)
Epoch: [48][270/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4636e-01 (1.1035e-01)	Acc@1  92.97 ( 96.24)	Acc@5 100.00 ( 99.97)
Epoch: [48][280/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.3696e-01 (1.1084e-01)	Acc@1  96.09 ( 96.24)	Acc@5  99.22 ( 99.97)
Epoch: [48][290/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.8420e-02 (1.1056e-01)	Acc@1  98.44 ( 96.24)	Acc@5 100.00 ( 99.97)
Epoch: [48][300/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.1560e-01 (1.1046e-01)	Acc@1  96.09 ( 96.23)	Acc@5 100.00 ( 99.97)
Epoch: [48][310/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.8774e-01 (1.1007e-01)	Acc@1  93.75 ( 96.25)	Acc@5 100.00 ( 99.97)
Epoch: [48][320/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.5857e-02 (1.1024e-01)	Acc@1  98.44 ( 96.25)	Acc@5 100.00 ( 99.97)
Epoch: [48][330/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.4734e-01 (1.1063e-01)	Acc@1  95.31 ( 96.25)	Acc@5 100.00 ( 99.97)
Epoch: [48][340/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.6602e-01 (1.1089e-01)	Acc@1  95.31 ( 96.22)	Acc@5 100.00 ( 99.97)
Epoch: [48][350/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.6650e-01 (1.1154e-01)	Acc@1  95.31 ( 96.22)	Acc@5 100.00 ( 99.97)
Epoch: [48][360/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.0779e-01 (1.1183e-01)	Acc@1  94.53 ( 96.20)	Acc@5 100.00 ( 99.97)
Epoch: [48][370/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.3342e-01 (1.1166e-01)	Acc@1  92.97 ( 96.20)	Acc@5 100.00 ( 99.97)
Epoch: [48][380/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.4819e-01 (1.1195e-01)	Acc@1  94.53 ( 96.19)	Acc@5 100.00 ( 99.97)
Epoch: [48][390/391]	Time  0.029 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.4673e-01 (1.1211e-01)	Acc@1  93.75 ( 96.16)	Acc@5 100.00 ( 99.96)
## e[48] optimizer.zero_grad (sum) time: 0.2495718002319336
## e[48]       loss.backward (sum) time: 4.209828853607178
## e[48]      optimizer.step (sum) time: 1.7174584865570068
## epoch[48] training(only) time: 15.906165838241577
# Switched to evaluate mode...
Test: [  0/100]	Time  0.174 ( 0.174)	Loss 2.8027e-01 (2.8027e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.018 ( 0.032)	Loss 3.2373e-01 (3.0297e-01)	Acc@1  90.00 ( 90.36)	Acc@5  99.00 ( 99.73)
Test: [ 20/100]	Time  0.021 ( 0.027)	Loss 3.9185e-01 (3.1842e-01)	Acc@1  85.00 ( 90.43)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.019 ( 0.025)	Loss 4.7778e-01 (3.2546e-01)	Acc@1  87.00 ( 90.19)	Acc@5 100.00 ( 99.61)
Test: [ 40/100]	Time  0.017 ( 0.023)	Loss 2.9956e-01 (3.2285e-01)	Acc@1  93.00 ( 90.20)	Acc@5  99.00 ( 99.61)
Test: [ 50/100]	Time  0.019 ( 0.023)	Loss 1.7261e-01 (3.1717e-01)	Acc@1  95.00 ( 90.41)	Acc@5 100.00 ( 99.67)
Test: [ 60/100]	Time  0.017 ( 0.022)	Loss 3.6450e-01 (3.1211e-01)	Acc@1  91.00 ( 90.49)	Acc@5 100.00 ( 99.72)
Test: [ 70/100]	Time  0.021 ( 0.022)	Loss 3.7549e-01 (3.0870e-01)	Acc@1  90.00 ( 90.54)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.016 ( 0.021)	Loss 1.4575e-01 (3.1293e-01)	Acc@1  93.00 ( 90.42)	Acc@5 100.00 ( 99.70)
Test: [ 90/100]	Time  0.017 ( 0.021)	Loss 2.3328e-01 (3.1169e-01)	Acc@1  95.00 ( 90.47)	Acc@5 100.00 ( 99.71)
 * Acc@1 90.460 Acc@5 99.730
### epoch[48] execution time: 18.112006425857544
EPOCH 49
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [49][  0/391]	Time  0.219 ( 0.219)	Data  0.178 ( 0.178)	Loss 1.0016e-01 (1.0016e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
Epoch: [49][ 10/391]	Time  0.040 ( 0.056)	Data  0.001 ( 0.017)	Loss 7.9407e-02 (1.1634e-01)	Acc@1  98.44 ( 95.74)	Acc@5  99.22 ( 99.93)
Epoch: [49][ 20/391]	Time  0.037 ( 0.048)	Data  0.001 ( 0.009)	Loss 6.5857e-02 (1.1144e-01)	Acc@1  97.66 ( 95.94)	Acc@5 100.00 ( 99.96)
Epoch: [49][ 30/391]	Time  0.040 ( 0.045)	Data  0.001 ( 0.007)	Loss 6.2988e-02 (1.0855e-01)	Acc@1  99.22 ( 96.07)	Acc@5 100.00 ( 99.95)
Epoch: [49][ 40/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.005)	Loss 9.3689e-02 (1.1023e-01)	Acc@1  95.31 ( 96.09)	Acc@5 100.00 ( 99.96)
Epoch: [49][ 50/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.004)	Loss 8.5205e-02 (1.1124e-01)	Acc@1  98.44 ( 96.09)	Acc@5 100.00 ( 99.97)
Epoch: [49][ 60/391]	Time  0.044 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.3232e-01 (1.0773e-01)	Acc@1  95.31 ( 96.17)	Acc@5 100.00 ( 99.97)
Epoch: [49][ 70/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.5319e-02 (1.0534e-01)	Acc@1  99.22 ( 96.28)	Acc@5 100.00 ( 99.98)
Epoch: [49][ 80/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.0071e-01 (1.0671e-01)	Acc@1  94.53 ( 96.21)	Acc@5 100.00 ( 99.96)
Epoch: [49][ 90/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.0602e-01 (1.0819e-01)	Acc@1  96.09 ( 96.18)	Acc@5 100.00 ( 99.97)
Epoch: [49][100/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.0007e-02 (1.0779e-01)	Acc@1  96.09 ( 96.14)	Acc@5 100.00 ( 99.97)
Epoch: [49][110/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 9.2957e-02 (1.0637e-01)	Acc@1  97.66 ( 96.21)	Acc@5 100.00 ( 99.97)
Epoch: [49][120/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.4502e-01 (1.0578e-01)	Acc@1  96.88 ( 96.27)	Acc@5 100.00 ( 99.97)
Epoch: [49][130/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.2947e-02 (1.0525e-01)	Acc@1  96.88 ( 96.28)	Acc@5 100.00 ( 99.98)
Epoch: [49][140/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1627e-01 (1.0654e-01)	Acc@1  97.66 ( 96.27)	Acc@5 100.00 ( 99.98)
Epoch: [49][150/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4258e-01 (1.0694e-01)	Acc@1  95.31 ( 96.22)	Acc@5 100.00 ( 99.98)
Epoch: [49][160/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.7200e-02 (1.0658e-01)	Acc@1  96.88 ( 96.23)	Acc@5 100.00 ( 99.98)
Epoch: [49][170/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3135e-01 (1.0830e-01)	Acc@1  96.88 ( 96.16)	Acc@5 100.00 ( 99.98)
Epoch: [49][180/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.5815e-02 (1.0821e-01)	Acc@1  96.88 ( 96.16)	Acc@5 100.00 ( 99.98)
Epoch: [49][190/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0370e-01 (1.0867e-01)	Acc@1  96.88 ( 96.14)	Acc@5 100.00 ( 99.98)
Epoch: [49][200/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.3779e-01 (1.0907e-01)	Acc@1  89.84 ( 96.12)	Acc@5 100.00 ( 99.98)
Epoch: [49][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.1411e-02 (1.0960e-01)	Acc@1  98.44 ( 96.10)	Acc@5 100.00 ( 99.97)
Epoch: [49][220/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0925e-01 (1.1016e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 ( 99.97)
Epoch: [49][230/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5259e-01 (1.1010e-01)	Acc@1  92.97 ( 96.09)	Acc@5 100.00 ( 99.97)
Epoch: [49][240/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1438e-01 (1.1005e-01)	Acc@1  94.53 ( 96.07)	Acc@5 100.00 ( 99.97)
Epoch: [49][250/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.9956e-02 (1.0990e-01)	Acc@1  96.88 ( 96.08)	Acc@5 100.00 ( 99.98)
Epoch: [49][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.3140e-02 (1.0930e-01)	Acc@1  96.88 ( 96.11)	Acc@5 100.00 ( 99.97)
Epoch: [49][270/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4514e-01 (1.0955e-01)	Acc@1  92.97 ( 96.10)	Acc@5 100.00 ( 99.97)
Epoch: [49][280/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1554e-01 (1.0949e-01)	Acc@1  96.88 ( 96.10)	Acc@5 100.00 ( 99.97)
Epoch: [49][290/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.1438e-01 (1.0911e-01)	Acc@1  94.53 ( 96.12)	Acc@5 100.00 ( 99.97)
Epoch: [49][300/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.1914e-01 (1.0944e-01)	Acc@1  97.66 ( 96.12)	Acc@5 100.00 ( 99.97)
Epoch: [49][310/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.0046e-01 (1.0901e-01)	Acc@1  96.88 ( 96.14)	Acc@5 100.00 ( 99.97)
Epoch: [49][320/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 9.5154e-02 (1.0925e-01)	Acc@1  95.31 ( 96.12)	Acc@5 100.00 ( 99.98)
Epoch: [49][330/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 9.4055e-02 (1.0882e-01)	Acc@1  96.88 ( 96.14)	Acc@5 100.00 ( 99.98)
Epoch: [49][340/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.0962e-01 (1.0912e-01)	Acc@1  96.09 ( 96.13)	Acc@5 100.00 ( 99.97)
Epoch: [49][350/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 8.2092e-02 (1.0944e-01)	Acc@1  96.88 ( 96.11)	Acc@5 100.00 ( 99.97)
Epoch: [49][360/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.5723e-01 (1.0955e-01)	Acc@1  92.97 ( 96.10)	Acc@5 100.00 ( 99.97)
Epoch: [49][370/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.4441e-01 (1.0994e-01)	Acc@1  94.53 ( 96.10)	Acc@5 100.00 ( 99.97)
Epoch: [49][380/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.0535e-01 (1.0974e-01)	Acc@1  96.09 ( 96.13)	Acc@5 100.00 ( 99.97)
Epoch: [49][390/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.4292e-01 (1.0942e-01)	Acc@1  92.50 ( 96.15)	Acc@5 100.00 ( 99.97)
## e[49] optimizer.zero_grad (sum) time: 0.2510411739349365
## e[49]       loss.backward (sum) time: 4.1753246784210205
## e[49]      optimizer.step (sum) time: 1.7311477661132812
## epoch[49] training(only) time: 15.905986070632935
# Switched to evaluate mode...
Test: [  0/100]	Time  0.170 ( 0.170)	Loss 2.8394e-01 (2.8394e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.020 ( 0.034)	Loss 3.5645e-01 (3.1058e-01)	Acc@1  90.00 ( 91.18)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.018 ( 0.028)	Loss 4.6851e-01 (3.2502e-01)	Acc@1  85.00 ( 90.95)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.017 ( 0.025)	Loss 4.5190e-01 (3.3149e-01)	Acc@1  83.00 ( 90.55)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.023 ( 0.023)	Loss 3.3887e-01 (3.2966e-01)	Acc@1  90.00 ( 90.49)	Acc@5  99.00 ( 99.61)
Test: [ 50/100]	Time  0.017 ( 0.023)	Loss 1.5515e-01 (3.1980e-01)	Acc@1  93.00 ( 90.61)	Acc@5 100.00 ( 99.67)
Test: [ 60/100]	Time  0.024 ( 0.022)	Loss 3.5156e-01 (3.1780e-01)	Acc@1  91.00 ( 90.57)	Acc@5 100.00 ( 99.72)
Test: [ 70/100]	Time  0.019 ( 0.022)	Loss 3.6133e-01 (3.1262e-01)	Acc@1  88.00 ( 90.61)	Acc@5 100.00 ( 99.72)
Test: [ 80/100]	Time  0.023 ( 0.022)	Loss 1.2756e-01 (3.1658e-01)	Acc@1  96.00 ( 90.52)	Acc@5 100.00 ( 99.73)
Test: [ 90/100]	Time  0.017 ( 0.021)	Loss 3.0225e-01 (3.1577e-01)	Acc@1  92.00 ( 90.58)	Acc@5 100.00 ( 99.74)
 * Acc@1 90.630 Acc@5 99.760
### epoch[49] execution time: 18.152023315429688
EPOCH 50
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [50][  0/391]	Time  0.214 ( 0.214)	Data  0.172 ( 0.172)	Loss 1.2769e-01 (1.2769e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [50][ 10/391]	Time  0.039 ( 0.056)	Data  0.001 ( 0.016)	Loss 1.6650e-01 (1.0859e-01)	Acc@1  93.75 ( 96.45)	Acc@5 100.00 ( 99.86)
Epoch: [50][ 20/391]	Time  0.042 ( 0.049)	Data  0.001 ( 0.009)	Loss 1.5540e-01 (1.0999e-01)	Acc@1  96.09 ( 96.39)	Acc@5 100.00 ( 99.93)
Epoch: [50][ 30/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.006)	Loss 1.7139e-01 (1.1024e-01)	Acc@1  93.75 ( 96.30)	Acc@5  99.22 ( 99.92)
Epoch: [50][ 40/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.005)	Loss 1.0291e-01 (1.1112e-01)	Acc@1  96.09 ( 96.34)	Acc@5 100.00 ( 99.94)
Epoch: [50][ 50/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.004)	Loss 9.3872e-02 (1.1002e-01)	Acc@1  96.88 ( 96.40)	Acc@5 100.00 ( 99.95)
Epoch: [50][ 60/391]	Time  0.036 ( 0.043)	Data  0.001 ( 0.004)	Loss 7.7820e-02 (1.0718e-01)	Acc@1  95.31 ( 96.41)	Acc@5 100.00 ( 99.96)
Epoch: [50][ 70/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.0129e-02 (1.0686e-01)	Acc@1  97.66 ( 96.35)	Acc@5 100.00 ( 99.97)
Epoch: [50][ 80/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.5088e-01 (1.0926e-01)	Acc@1  95.31 ( 96.23)	Acc@5 100.00 ( 99.97)
Epoch: [50][ 90/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 8.6914e-02 (1.0834e-01)	Acc@1  96.88 ( 96.29)	Acc@5 100.00 ( 99.97)
Epoch: [50][100/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.003)	Loss 8.1116e-02 (1.0768e-01)	Acc@1  96.09 ( 96.31)	Acc@5 100.00 ( 99.96)
Epoch: [50][110/391]	Time  0.045 ( 0.041)	Data  0.001 ( 0.003)	Loss 8.5266e-02 (1.0639e-01)	Acc@1  97.66 ( 96.37)	Acc@5 100.00 ( 99.96)
Epoch: [50][120/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4429e-01 (1.0672e-01)	Acc@1  96.88 ( 96.36)	Acc@5 100.00 ( 99.97)
Epoch: [50][130/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.1370e-02 (1.0678e-01)	Acc@1  95.31 ( 96.34)	Acc@5 100.00 ( 99.97)
Epoch: [50][140/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1224e-01 (1.0645e-01)	Acc@1  95.31 ( 96.32)	Acc@5 100.00 ( 99.97)
Epoch: [50][150/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.4971e-02 (1.0602e-01)	Acc@1  97.66 ( 96.33)	Acc@5 100.00 ( 99.97)
Epoch: [50][160/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.7017e-02 (1.0699e-01)	Acc@1  96.88 ( 96.28)	Acc@5 100.00 ( 99.97)
Epoch: [50][170/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4233e-01 (1.0676e-01)	Acc@1  96.09 ( 96.29)	Acc@5 100.00 ( 99.97)
Epoch: [50][180/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0083e-01 (1.0673e-01)	Acc@1  96.09 ( 96.31)	Acc@5 100.00 ( 99.97)
Epoch: [50][190/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.6558e-02 (1.0683e-01)	Acc@1  96.09 ( 96.30)	Acc@5 100.00 ( 99.97)
Epoch: [50][200/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0004e-01 (1.0753e-01)	Acc@1  96.88 ( 96.26)	Acc@5 100.00 ( 99.97)
Epoch: [50][210/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8523e-02 (1.0734e-01)	Acc@1  98.44 ( 96.27)	Acc@5 100.00 ( 99.97)
Epoch: [50][220/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5161e-01 (1.0770e-01)	Acc@1  92.97 ( 96.25)	Acc@5 100.00 ( 99.97)
Epoch: [50][230/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.002)	Loss 9.1248e-02 (1.0797e-01)	Acc@1  97.66 ( 96.27)	Acc@5 100.00 ( 99.97)
Epoch: [50][240/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.002)	Loss 6.0638e-02 (1.0685e-01)	Acc@1  97.66 ( 96.31)	Acc@5 100.00 ( 99.97)
Epoch: [50][250/391]	Time  0.043 ( 0.040)	Data  0.001 ( 0.002)	Loss 9.7351e-02 (1.0736e-01)	Acc@1  97.66 ( 96.29)	Acc@5 100.00 ( 99.98)
Epoch: [50][260/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.0065e-01 (1.0690e-01)	Acc@1  96.09 ( 96.30)	Acc@5 100.00 ( 99.97)
Epoch: [50][270/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.1493e-01 (1.0654e-01)	Acc@1  96.09 ( 96.32)	Acc@5 100.00 ( 99.97)
Epoch: [50][280/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.7480e-01 (1.0730e-01)	Acc@1  95.31 ( 96.30)	Acc@5 100.00 ( 99.97)
Epoch: [50][290/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 7.4036e-02 (1.0815e-01)	Acc@1  97.66 ( 96.25)	Acc@5 100.00 ( 99.97)
Epoch: [50][300/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.0999e-01 (1.0788e-01)	Acc@1  96.88 ( 96.26)	Acc@5 100.00 ( 99.97)
Epoch: [50][310/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.2683e-02 (1.0748e-01)	Acc@1  98.44 ( 96.28)	Acc@5 100.00 ( 99.97)
Epoch: [50][320/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.0822e-01 (1.0768e-01)	Acc@1  95.31 ( 96.26)	Acc@5 100.00 ( 99.96)
Epoch: [50][330/391]	Time  0.046 ( 0.040)	Data  0.001 ( 0.001)	Loss 9.1248e-02 (1.0785e-01)	Acc@1  96.88 ( 96.26)	Acc@5 100.00 ( 99.96)
Epoch: [50][340/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.1090e-01 (1.0818e-01)	Acc@1  95.31 ( 96.24)	Acc@5 100.00 ( 99.97)
Epoch: [50][350/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 9.7595e-02 (1.0859e-01)	Acc@1  96.09 ( 96.23)	Acc@5 100.00 ( 99.96)
Epoch: [50][360/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.5576e-01 (1.0862e-01)	Acc@1  96.88 ( 96.23)	Acc@5 100.00 ( 99.97)
Epoch: [50][370/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.9509e-02 (1.0880e-01)	Acc@1  97.66 ( 96.22)	Acc@5 100.00 ( 99.97)
Epoch: [50][380/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.1993e-01 (1.0851e-01)	Acc@1  96.88 ( 96.23)	Acc@5 100.00 ( 99.97)
Epoch: [50][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.2473e-01 (1.0902e-01)	Acc@1  93.75 ( 96.21)	Acc@5 100.00 ( 99.97)
## e[50] optimizer.zero_grad (sum) time: 0.2496340274810791
## e[50]       loss.backward (sum) time: 4.139420986175537
## e[50]      optimizer.step (sum) time: 1.7886552810668945
## epoch[50] training(only) time: 15.84956693649292
# Switched to evaluate mode...
Test: [  0/100]	Time  0.174 ( 0.174)	Loss 2.6758e-01 (2.6758e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.020 ( 0.035)	Loss 3.3350e-01 (3.0081e-01)	Acc@1  88.00 ( 90.73)	Acc@5  99.00 ( 99.73)
Test: [ 20/100]	Time  0.024 ( 0.028)	Loss 4.1699e-01 (3.1814e-01)	Acc@1  87.00 ( 90.52)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.017 ( 0.025)	Loss 4.4287e-01 (3.2445e-01)	Acc@1  87.00 ( 90.29)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.021 ( 0.024)	Loss 3.1274e-01 (3.2345e-01)	Acc@1  90.00 ( 90.24)	Acc@5  99.00 ( 99.66)
Test: [ 50/100]	Time  0.020 ( 0.023)	Loss 1.6272e-01 (3.1776e-01)	Acc@1  94.00 ( 90.53)	Acc@5 100.00 ( 99.69)
Test: [ 60/100]	Time  0.020 ( 0.023)	Loss 3.6377e-01 (3.1649e-01)	Acc@1  91.00 ( 90.44)	Acc@5 100.00 ( 99.72)
Test: [ 70/100]	Time  0.023 ( 0.022)	Loss 3.7598e-01 (3.1247e-01)	Acc@1  88.00 ( 90.39)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.022 ( 0.022)	Loss 1.4417e-01 (3.1567e-01)	Acc@1  94.00 ( 90.19)	Acc@5 100.00 ( 99.70)
Test: [ 90/100]	Time  0.020 ( 0.022)	Loss 2.2485e-01 (3.1392e-01)	Acc@1  95.00 ( 90.37)	Acc@5 100.00 ( 99.73)
 * Acc@1 90.410 Acc@5 99.740
### epoch[50] execution time: 18.13038206100464
EPOCH 51
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [51][  0/391]	Time  0.218 ( 0.218)	Data  0.177 ( 0.177)	Loss 9.6436e-02 (9.6436e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [51][ 10/391]	Time  0.038 ( 0.056)	Data  0.001 ( 0.017)	Loss 8.9966e-02 (1.0905e-01)	Acc@1  96.88 ( 96.16)	Acc@5 100.00 (100.00)
Epoch: [51][ 20/391]	Time  0.038 ( 0.049)	Data  0.001 ( 0.009)	Loss 7.9712e-02 (1.1502e-01)	Acc@1  96.88 ( 95.94)	Acc@5 100.00 ( 99.96)
Epoch: [51][ 30/391]	Time  0.038 ( 0.046)	Data  0.001 ( 0.007)	Loss 1.4795e-01 (1.1017e-01)	Acc@1  92.97 ( 96.04)	Acc@5 100.00 ( 99.95)
Epoch: [51][ 40/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.005)	Loss 1.7517e-01 (1.1095e-01)	Acc@1  92.97 ( 96.09)	Acc@5 100.00 ( 99.96)
Epoch: [51][ 50/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.1340e-01 (1.1100e-01)	Acc@1  94.53 ( 96.17)	Acc@5 100.00 ( 99.97)
Epoch: [51][ 60/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.004)	Loss 8.0078e-02 (1.0957e-01)	Acc@1  96.88 ( 96.18)	Acc@5 100.00 ( 99.97)
Epoch: [51][ 70/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.1017e-01 (1.1137e-01)	Acc@1  96.09 ( 96.17)	Acc@5 100.00 ( 99.98)
Epoch: [51][ 80/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 8.2153e-02 (1.1094e-01)	Acc@1  97.66 ( 96.15)	Acc@5 100.00 ( 99.98)
Epoch: [51][ 90/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.0968e-01 (1.1186e-01)	Acc@1  95.31 ( 96.10)	Acc@5 100.00 ( 99.98)
Epoch: [51][100/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.4038e-01 (1.1037e-01)	Acc@1  95.31 ( 96.16)	Acc@5 100.00 ( 99.98)
Epoch: [51][110/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 8.7463e-02 (1.1072e-01)	Acc@1  96.88 ( 96.18)	Acc@5 100.00 ( 99.98)
Epoch: [51][120/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.4575e-02 (1.0989e-01)	Acc@1  97.66 ( 96.21)	Acc@5 100.00 ( 99.98)
Epoch: [51][130/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.9357e-02 (1.0945e-01)	Acc@1  97.66 ( 96.22)	Acc@5 100.00 ( 99.98)
Epoch: [51][140/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.6975e-02 (1.0826e-01)	Acc@1  94.53 ( 96.22)	Acc@5 100.00 ( 99.98)
Epoch: [51][150/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1493e-01 (1.0786e-01)	Acc@1  97.66 ( 96.22)	Acc@5 100.00 ( 99.98)
Epoch: [51][160/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.2561e-02 (1.0661e-01)	Acc@1  98.44 ( 96.28)	Acc@5 100.00 ( 99.99)
Epoch: [51][170/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3379e-01 (1.0775e-01)	Acc@1  95.31 ( 96.24)	Acc@5 100.00 ( 99.98)
Epoch: [51][180/391]	Time  0.049 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.4697e-02 (1.0784e-01)	Acc@1  97.66 ( 96.26)	Acc@5 100.00 ( 99.97)
Epoch: [51][190/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5625e-01 (1.0811e-01)	Acc@1  95.31 ( 96.25)	Acc@5 100.00 ( 99.98)
Epoch: [51][200/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0443e-01 (1.0847e-01)	Acc@1  94.53 ( 96.26)	Acc@5 100.00 ( 99.98)
Epoch: [51][210/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.9854e-02 (1.0881e-01)	Acc@1  96.88 ( 96.25)	Acc@5 100.00 ( 99.98)
Epoch: [51][220/391]	Time  0.051 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2524e-01 (1.0776e-01)	Acc@1  93.75 ( 96.26)	Acc@5 100.00 ( 99.98)
Epoch: [51][230/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1921e-02 (1.0731e-01)	Acc@1  99.22 ( 96.28)	Acc@5 100.00 ( 99.98)
Epoch: [51][240/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0193e-01 (1.0734e-01)	Acc@1  96.88 ( 96.28)	Acc@5 100.00 ( 99.98)
Epoch: [51][250/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.8257e-02 (1.0737e-01)	Acc@1  96.88 ( 96.31)	Acc@5 100.00 ( 99.97)
Epoch: [51][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.9214e-02 (1.0688e-01)	Acc@1  98.44 ( 96.33)	Acc@5 100.00 ( 99.97)
Epoch: [51][270/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.0942e-02 (1.0655e-01)	Acc@1  96.88 ( 96.35)	Acc@5 100.00 ( 99.97)
Epoch: [51][280/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1163e-01 (1.0671e-01)	Acc@1  97.66 ( 96.34)	Acc@5 100.00 ( 99.97)
Epoch: [51][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0486e-01 (1.0681e-01)	Acc@1  97.66 ( 96.35)	Acc@5 100.00 ( 99.97)
Epoch: [51][300/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.7412e-02 (1.0602e-01)	Acc@1  95.31 ( 96.38)	Acc@5 100.00 ( 99.97)
Epoch: [51][310/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.1743e-01 (1.0596e-01)	Acc@1  96.09 ( 96.38)	Acc@5 100.00 ( 99.97)
Epoch: [51][320/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.001)	Loss 8.0872e-02 (1.0542e-01)	Acc@1  98.44 ( 96.40)	Acc@5 100.00 ( 99.97)
Epoch: [51][330/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.001)	Loss 8.0200e-02 (1.0577e-01)	Acc@1  96.88 ( 96.38)	Acc@5 100.00 ( 99.97)
Epoch: [51][340/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.4404e-01 (1.0566e-01)	Acc@1  95.31 ( 96.40)	Acc@5  99.22 ( 99.97)
Epoch: [51][350/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.1249e-01 (1.0535e-01)	Acc@1  96.09 ( 96.41)	Acc@5 100.00 ( 99.97)
Epoch: [51][360/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 9.6313e-02 (1.0547e-01)	Acc@1  96.88 ( 96.40)	Acc@5 100.00 ( 99.97)
Epoch: [51][370/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.9775e-01 (1.0587e-01)	Acc@1  93.75 ( 96.38)	Acc@5 100.00 ( 99.97)
Epoch: [51][380/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.0571e-01 (1.0614e-01)	Acc@1  96.09 ( 96.38)	Acc@5 100.00 ( 99.97)
Epoch: [51][390/391]	Time  0.027 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.2274e-01 (1.0662e-01)	Acc@1  97.50 ( 96.36)	Acc@5 100.00 ( 99.97)
## e[51] optimizer.zero_grad (sum) time: 0.2506558895111084
## e[51]       loss.backward (sum) time: 4.152081251144409
## e[51]      optimizer.step (sum) time: 1.742462396621704
## epoch[51] training(only) time: 15.873625993728638
# Switched to evaluate mode...
Test: [  0/100]	Time  0.168 ( 0.168)	Loss 2.4475e-01 (2.4475e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.017 ( 0.033)	Loss 3.6206e-01 (2.9896e-01)	Acc@1  87.00 ( 90.82)	Acc@5  99.00 ( 99.82)
Test: [ 20/100]	Time  0.019 ( 0.026)	Loss 4.4678e-01 (3.2463e-01)	Acc@1  85.00 ( 90.62)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.020 ( 0.024)	Loss 4.7339e-01 (3.3177e-01)	Acc@1  86.00 ( 90.52)	Acc@5 100.00 ( 99.77)
Test: [ 40/100]	Time  0.022 ( 0.023)	Loss 3.0688e-01 (3.2692e-01)	Acc@1  94.00 ( 90.63)	Acc@5  99.00 ( 99.73)
Test: [ 50/100]	Time  0.017 ( 0.023)	Loss 1.5283e-01 (3.1920e-01)	Acc@1  94.00 ( 90.78)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.016 ( 0.022)	Loss 4.1406e-01 (3.1784e-01)	Acc@1  91.00 ( 90.77)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.016 ( 0.021)	Loss 4.1382e-01 (3.1520e-01)	Acc@1  87.00 ( 90.65)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.022 ( 0.021)	Loss 1.4331e-01 (3.1806e-01)	Acc@1  95.00 ( 90.57)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.023 ( 0.021)	Loss 2.8052e-01 (3.1643e-01)	Acc@1  93.00 ( 90.59)	Acc@5 100.00 ( 99.75)
 * Acc@1 90.650 Acc@5 99.760
### epoch[51] execution time: 18.11590003967285
EPOCH 52
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [52][  0/391]	Time  0.215 ( 0.215)	Data  0.168 ( 0.168)	Loss 1.0828e-01 (1.0828e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [52][ 10/391]	Time  0.041 ( 0.056)	Data  0.001 ( 0.016)	Loss 7.1899e-02 (8.6318e-02)	Acc@1  95.31 ( 96.59)	Acc@5 100.00 (100.00)
Epoch: [52][ 20/391]	Time  0.039 ( 0.049)	Data  0.001 ( 0.009)	Loss 1.2085e-01 (9.9556e-02)	Acc@1  93.75 ( 96.17)	Acc@5 100.00 (100.00)
Epoch: [52][ 30/391]	Time  0.038 ( 0.046)	Data  0.001 ( 0.006)	Loss 7.1106e-02 (9.5798e-02)	Acc@1  97.66 ( 96.45)	Acc@5 100.00 (100.00)
Epoch: [52][ 40/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.005)	Loss 8.4473e-02 (9.9675e-02)	Acc@1  96.88 ( 96.32)	Acc@5 100.00 ( 99.98)
Epoch: [52][ 50/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 8.5510e-02 (9.4697e-02)	Acc@1  97.66 ( 96.49)	Acc@5 100.00 ( 99.98)
Epoch: [52][ 60/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.9421e-01 (9.5894e-02)	Acc@1  94.53 ( 96.53)	Acc@5 100.00 ( 99.99)
Epoch: [52][ 70/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.003)	Loss 8.1055e-02 (9.5806e-02)	Acc@1  97.66 ( 96.54)	Acc@5 100.00 ( 99.97)
Epoch: [52][ 80/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.6355e-02 (9.8397e-02)	Acc@1  99.22 ( 96.48)	Acc@5 100.00 ( 99.97)
Epoch: [52][ 90/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.6355e-02 (9.6127e-02)	Acc@1  97.66 ( 96.58)	Acc@5 100.00 ( 99.97)
Epoch: [52][100/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.3477e-01 (9.7577e-02)	Acc@1  94.53 ( 96.53)	Acc@5 100.00 ( 99.98)
Epoch: [52][110/391]	Time  0.045 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.2937e-02 (9.7932e-02)	Acc@1  97.66 ( 96.54)	Acc@5 100.00 ( 99.98)
Epoch: [52][120/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.8928e-02 (9.7927e-02)	Acc@1  97.66 ( 96.59)	Acc@5 100.00 ( 99.97)
Epoch: [52][130/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.2703e-02 (9.8867e-02)	Acc@1  96.09 ( 96.54)	Acc@5 100.00 ( 99.97)
Epoch: [52][140/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.1421e-02 (9.8691e-02)	Acc@1  97.66 ( 96.60)	Acc@5 100.00 ( 99.97)
Epoch: [52][150/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2622e-01 (9.9196e-02)	Acc@1  95.31 ( 96.59)	Acc@5 100.00 ( 99.97)
Epoch: [52][160/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.2764e-02 (9.9747e-02)	Acc@1  96.88 ( 96.55)	Acc@5 100.00 ( 99.98)
Epoch: [52][170/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5894e-01 (1.0104e-01)	Acc@1  92.97 ( 96.50)	Acc@5 100.00 ( 99.98)
Epoch: [52][180/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0466e-02 (1.0068e-01)	Acc@1  99.22 ( 96.53)	Acc@5 100.00 ( 99.98)
Epoch: [52][190/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7729e-02 (1.0089e-01)	Acc@1  99.22 ( 96.52)	Acc@5 100.00 ( 99.98)
Epoch: [52][200/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.9609e-02 (1.0108e-01)	Acc@1  95.31 ( 96.47)	Acc@5 100.00 ( 99.98)
Epoch: [52][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.3628e-02 (1.0048e-01)	Acc@1  96.88 ( 96.49)	Acc@5 100.00 ( 99.98)
Epoch: [52][220/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3147e-01 (1.0087e-01)	Acc@1  98.44 ( 96.49)	Acc@5 100.00 ( 99.98)
Epoch: [52][230/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.4900e-02 (1.0060e-01)	Acc@1  97.66 ( 96.50)	Acc@5 100.00 ( 99.98)
Epoch: [52][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2378e-01 (1.0163e-01)	Acc@1  93.75 ( 96.46)	Acc@5  99.22 ( 99.98)
Epoch: [52][250/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.8319e-02 (1.0171e-01)	Acc@1  98.44 ( 96.45)	Acc@5 100.00 ( 99.98)
Epoch: [52][260/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6833e-01 (1.0206e-01)	Acc@1  92.97 ( 96.45)	Acc@5 100.00 ( 99.98)
Epoch: [52][270/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4746e-01 (1.0157e-01)	Acc@1  93.75 ( 96.46)	Acc@5  99.22 ( 99.98)
Epoch: [52][280/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.0114e-01 (1.0180e-01)	Acc@1  96.09 ( 96.46)	Acc@5 100.00 ( 99.98)
Epoch: [52][290/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.7566e-02 (1.0230e-01)	Acc@1  97.66 ( 96.43)	Acc@5 100.00 ( 99.98)
Epoch: [52][300/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.3062e-01 (1.0314e-01)	Acc@1  93.75 ( 96.40)	Acc@5 100.00 ( 99.98)
Epoch: [52][310/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 9.9548e-02 (1.0350e-01)	Acc@1  96.88 ( 96.40)	Acc@5 100.00 ( 99.98)
Epoch: [52][320/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.4478e-01 (1.0381e-01)	Acc@1  94.53 ( 96.39)	Acc@5 100.00 ( 99.98)
Epoch: [52][330/391]	Time  0.047 ( 0.040)	Data  0.001 ( 0.001)	Loss 9.5886e-02 (1.0349e-01)	Acc@1  96.09 ( 96.41)	Acc@5 100.00 ( 99.98)
Epoch: [52][340/391]	Time  0.045 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.2122e-01 (1.0358e-01)	Acc@1  94.53 ( 96.40)	Acc@5 100.00 ( 99.98)
Epoch: [52][350/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 9.3994e-02 (1.0378e-01)	Acc@1  96.09 ( 96.39)	Acc@5 100.00 ( 99.97)
Epoch: [52][360/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 9.1797e-02 (1.0362e-01)	Acc@1  96.88 ( 96.41)	Acc@5 100.00 ( 99.97)
Epoch: [52][370/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.0779e-01 (1.0378e-01)	Acc@1  96.09 ( 96.39)	Acc@5 100.00 ( 99.97)
Epoch: [52][380/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.3269e-01 (1.0391e-01)	Acc@1  94.53 ( 96.40)	Acc@5 100.00 ( 99.97)
Epoch: [52][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 8.9478e-02 (1.0385e-01)	Acc@1  96.25 ( 96.39)	Acc@5 100.00 ( 99.97)
## e[52] optimizer.zero_grad (sum) time: 0.24932289123535156
## e[52]       loss.backward (sum) time: 4.17684268951416
## e[52]      optimizer.step (sum) time: 1.72810959815979
## epoch[52] training(only) time: 15.881408214569092
# Switched to evaluate mode...
Test: [  0/100]	Time  0.167 ( 0.167)	Loss 2.6196e-01 (2.6196e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 3.5352e-01 (3.2241e-01)	Acc@1  87.00 ( 90.64)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.018 ( 0.028)	Loss 4.1431e-01 (3.3554e-01)	Acc@1  85.00 ( 90.52)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.021 ( 0.026)	Loss 4.3140e-01 (3.3224e-01)	Acc@1  86.00 ( 90.26)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.017 ( 0.024)	Loss 3.5522e-01 (3.3196e-01)	Acc@1  92.00 ( 90.17)	Acc@5  99.00 ( 99.63)
Test: [ 50/100]	Time  0.017 ( 0.023)	Loss 1.4624e-01 (3.2417e-01)	Acc@1  95.00 ( 90.29)	Acc@5 100.00 ( 99.69)
Test: [ 60/100]	Time  0.017 ( 0.023)	Loss 3.4155e-01 (3.2123e-01)	Acc@1  92.00 ( 90.31)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.021 ( 0.022)	Loss 4.1284e-01 (3.1733e-01)	Acc@1  89.00 ( 90.32)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.018 ( 0.022)	Loss 1.1694e-01 (3.2107e-01)	Acc@1  96.00 ( 90.26)	Acc@5 100.00 ( 99.70)
Test: [ 90/100]	Time  0.018 ( 0.022)	Loss 1.8713e-01 (3.1830e-01)	Acc@1  95.00 ( 90.40)	Acc@5 100.00 ( 99.71)
 * Acc@1 90.500 Acc@5 99.740
### epoch[52] execution time: 18.134666919708252
EPOCH 53
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [53][  0/391]	Time  0.218 ( 0.218)	Data  0.177 ( 0.177)	Loss 6.0883e-02 (6.0883e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [53][ 10/391]	Time  0.040 ( 0.056)	Data  0.001 ( 0.017)	Loss 1.2408e-01 (9.6699e-02)	Acc@1  96.09 ( 96.31)	Acc@5 100.00 (100.00)
Epoch: [53][ 20/391]	Time  0.038 ( 0.048)	Data  0.001 ( 0.009)	Loss 1.0358e-01 (9.4620e-02)	Acc@1  96.88 ( 96.69)	Acc@5 100.00 (100.00)
Epoch: [53][ 30/391]	Time  0.043 ( 0.045)	Data  0.001 ( 0.007)	Loss 7.9529e-02 (9.5166e-02)	Acc@1  97.66 ( 96.80)	Acc@5 100.00 (100.00)
Epoch: [53][ 40/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.005)	Loss 9.9976e-02 (9.6634e-02)	Acc@1  96.88 ( 96.67)	Acc@5 100.00 (100.00)
Epoch: [53][ 50/391]	Time  0.044 ( 0.043)	Data  0.001 ( 0.004)	Loss 2.5589e-02 (9.5091e-02)	Acc@1 100.00 ( 96.68)	Acc@5 100.00 (100.00)
Epoch: [53][ 60/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.6028e-01 (9.6450e-02)	Acc@1  94.53 ( 96.70)	Acc@5 100.00 (100.00)
Epoch: [53][ 70/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.1761e-01 (9.5102e-02)	Acc@1  96.09 ( 96.76)	Acc@5  99.22 ( 99.99)
Epoch: [53][ 80/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.0663e-01 (9.4473e-02)	Acc@1  98.44 ( 96.81)	Acc@5 100.00 ( 99.99)
Epoch: [53][ 90/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.2488e-01 (9.6222e-02)	Acc@1  94.53 ( 96.73)	Acc@5 100.00 ( 99.99)
Epoch: [53][100/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.1322e-01 (9.8252e-02)	Acc@1  94.53 ( 96.55)	Acc@5 100.00 ( 99.98)
Epoch: [53][110/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.3894e-02 (9.8046e-02)	Acc@1  97.66 ( 96.56)	Acc@5 100.00 ( 99.98)
Epoch: [53][120/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.6406e-02 (9.7859e-02)	Acc@1  98.44 ( 96.58)	Acc@5 100.00 ( 99.98)
Epoch: [53][130/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.7595e-02 (9.8368e-02)	Acc@1  97.66 ( 96.59)	Acc@5 100.00 ( 99.98)
Epoch: [53][140/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7852e-02 (9.8228e-02)	Acc@1  98.44 ( 96.55)	Acc@5 100.00 ( 99.98)
Epoch: [53][150/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.0088e-02 (9.9723e-02)	Acc@1  96.88 ( 96.48)	Acc@5 100.00 ( 99.98)
Epoch: [53][160/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3318e-01 (9.9332e-02)	Acc@1  96.88 ( 96.49)	Acc@5 100.00 ( 99.98)
Epoch: [53][170/391]	Time  0.040 ( 0.041)	Data  0.002 ( 0.002)	Loss 7.8735e-02 (9.9551e-02)	Acc@1  97.66 ( 96.47)	Acc@5 100.00 ( 99.98)
Epoch: [53][180/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.6375e-02 (9.8523e-02)	Acc@1  96.09 ( 96.52)	Acc@5 100.00 ( 99.98)
Epoch: [53][190/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.4280e-02 (9.8044e-02)	Acc@1  96.88 ( 96.54)	Acc@5 100.00 ( 99.98)
Epoch: [53][200/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 9.0027e-02 (9.8632e-02)	Acc@1  96.88 ( 96.52)	Acc@5 100.00 ( 99.98)
Epoch: [53][210/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.2732e-01 (9.8436e-02)	Acc@1  94.53 ( 96.52)	Acc@5 100.00 ( 99.99)
Epoch: [53][220/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.1353e-01 (9.8512e-02)	Acc@1  95.31 ( 96.52)	Acc@5 100.00 ( 99.99)
Epoch: [53][230/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 6.8970e-02 (9.8875e-02)	Acc@1  96.88 ( 96.49)	Acc@5 100.00 ( 99.99)
Epoch: [53][240/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.2793e-01 (9.8386e-02)	Acc@1  93.75 ( 96.52)	Acc@5 100.00 ( 99.99)
Epoch: [53][250/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 8.0261e-02 (9.7777e-02)	Acc@1  96.09 ( 96.54)	Acc@5 100.00 ( 99.99)
Epoch: [53][260/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 9.0393e-02 (9.7354e-02)	Acc@1  96.88 ( 96.58)	Acc@5 100.00 ( 99.99)
Epoch: [53][270/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 6.7566e-02 (9.7334e-02)	Acc@1  96.09 ( 96.56)	Acc@5 100.00 ( 99.99)
Epoch: [53][280/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.0382e-01 (9.6985e-02)	Acc@1  96.88 ( 96.58)	Acc@5 100.00 ( 99.98)
Epoch: [53][290/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.0730e-01 (9.7220e-02)	Acc@1  95.31 ( 96.58)	Acc@5 100.00 ( 99.98)
Epoch: [53][300/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.2256e-02 (9.7276e-02)	Acc@1  97.66 ( 96.59)	Acc@5 100.00 ( 99.98)
Epoch: [53][310/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.2634e-01 (9.7442e-02)	Acc@1  96.09 ( 96.60)	Acc@5 100.00 ( 99.98)
Epoch: [53][320/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 8.1665e-02 (9.8234e-02)	Acc@1  96.09 ( 96.56)	Acc@5 100.00 ( 99.99)
Epoch: [53][330/391]	Time  0.045 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.0101e-01 (9.8101e-02)	Acc@1  96.09 ( 96.59)	Acc@5 100.00 ( 99.98)
Epoch: [53][340/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.1859e-01 (9.8687e-02)	Acc@1  96.09 ( 96.56)	Acc@5 100.00 ( 99.98)
Epoch: [53][350/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.1121e-01 (9.8521e-02)	Acc@1  96.09 ( 96.57)	Acc@5 100.00 ( 99.98)
Epoch: [53][360/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.7861e-02 (9.8385e-02)	Acc@1  98.44 ( 96.59)	Acc@5 100.00 ( 99.98)
Epoch: [53][370/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 9.3018e-02 (9.8199e-02)	Acc@1  98.44 ( 96.61)	Acc@5 100.00 ( 99.98)
Epoch: [53][380/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.9896e-02 (9.7867e-02)	Acc@1  98.44 ( 96.61)	Acc@5 100.00 ( 99.98)
Epoch: [53][390/391]	Time  0.030 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.5039e-01 (9.8277e-02)	Acc@1  93.75 ( 96.60)	Acc@5 100.00 ( 99.98)
## e[53] optimizer.zero_grad (sum) time: 0.2498314380645752
## e[53]       loss.backward (sum) time: 4.129127502441406
## e[53]      optimizer.step (sum) time: 1.7798542976379395
## epoch[53] training(only) time: 15.81724238395691
# Switched to evaluate mode...
Test: [  0/100]	Time  0.168 ( 0.168)	Loss 2.9199e-01 (2.9199e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.019 ( 0.035)	Loss 3.4326e-01 (3.1572e-01)	Acc@1  91.00 ( 90.73)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.021 ( 0.029)	Loss 3.8306e-01 (3.3334e-01)	Acc@1  89.00 ( 90.57)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.021 ( 0.026)	Loss 4.9316e-01 (3.4164e-01)	Acc@1  82.00 ( 90.23)	Acc@5 100.00 ( 99.61)
Test: [ 40/100]	Time  0.022 ( 0.025)	Loss 3.9966e-01 (3.3711e-01)	Acc@1  89.00 ( 90.20)	Acc@5  99.00 ( 99.61)
Test: [ 50/100]	Time  0.019 ( 0.024)	Loss 1.4026e-01 (3.2549e-01)	Acc@1  97.00 ( 90.33)	Acc@5 100.00 ( 99.65)
Test: [ 60/100]	Time  0.020 ( 0.023)	Loss 3.2837e-01 (3.2088e-01)	Acc@1  93.00 ( 90.44)	Acc@5 100.00 ( 99.70)
Test: [ 70/100]	Time  0.020 ( 0.022)	Loss 4.9243e-01 (3.1677e-01)	Acc@1  89.00 ( 90.41)	Acc@5 100.00 ( 99.70)
Test: [ 80/100]	Time  0.021 ( 0.022)	Loss 1.2842e-01 (3.2100e-01)	Acc@1  95.00 ( 90.33)	Acc@5 100.00 ( 99.69)
Test: [ 90/100]	Time  0.022 ( 0.022)	Loss 2.4243e-01 (3.2034e-01)	Acc@1  92.00 ( 90.36)	Acc@5 100.00 ( 99.70)
 * Acc@1 90.520 Acc@5 99.710
### epoch[53] execution time: 18.089752197265625
EPOCH 54
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [54][  0/391]	Time  0.215 ( 0.215)	Data  0.172 ( 0.172)	Loss 4.4250e-02 (4.4250e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [54][ 10/391]	Time  0.039 ( 0.055)	Data  0.001 ( 0.016)	Loss 5.7312e-02 (8.8315e-02)	Acc@1  97.66 ( 96.95)	Acc@5 100.00 (100.00)
Epoch: [54][ 20/391]	Time  0.040 ( 0.049)	Data  0.001 ( 0.009)	Loss 1.0699e-01 (9.9240e-02)	Acc@1  95.31 ( 96.69)	Acc@5 100.00 (100.00)
Epoch: [54][ 30/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.006)	Loss 4.4128e-02 (9.2162e-02)	Acc@1  98.44 ( 96.75)	Acc@5 100.00 ( 99.97)
Epoch: [54][ 40/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.005)	Loss 8.8745e-02 (8.6797e-02)	Acc@1  96.09 ( 97.08)	Acc@5 100.00 ( 99.98)
Epoch: [54][ 50/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.004)	Loss 6.9763e-02 (8.4540e-02)	Acc@1  96.88 ( 97.18)	Acc@5 100.00 ( 99.97)
Epoch: [54][ 60/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.4697e-01 (8.5133e-02)	Acc@1  93.75 ( 97.05)	Acc@5 100.00 ( 99.97)
Epoch: [54][ 70/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 9.6497e-02 (8.8594e-02)	Acc@1  96.88 ( 96.94)	Acc@5 100.00 ( 99.97)
Epoch: [54][ 80/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.1719e-01 (8.9620e-02)	Acc@1  95.31 ( 96.88)	Acc@5 100.00 ( 99.97)
Epoch: [54][ 90/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.0670e-02 (8.9996e-02)	Acc@1 100.00 ( 96.88)	Acc@5 100.00 ( 99.97)
Epoch: [54][100/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.2622e-01 (8.9161e-02)	Acc@1  95.31 ( 96.88)	Acc@5 100.00 ( 99.97)
Epoch: [54][110/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.9031e-02 (9.2460e-02)	Acc@1  98.44 ( 96.76)	Acc@5 100.00 ( 99.97)
Epoch: [54][120/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.2041e-02 (9.3757e-02)	Acc@1  97.66 ( 96.73)	Acc@5 100.00 ( 99.97)
Epoch: [54][130/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.3923e-02 (9.4378e-02)	Acc@1  97.66 ( 96.70)	Acc@5 100.00 ( 99.97)
Epoch: [54][140/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.4819e-02 (9.3729e-02)	Acc@1  97.66 ( 96.70)	Acc@5 100.00 ( 99.97)
Epoch: [54][150/391]	Time  0.045 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.1675e-02 (9.4534e-02)	Acc@1  97.66 ( 96.68)	Acc@5 100.00 ( 99.97)
Epoch: [54][160/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.0190e-01 (9.6382e-02)	Acc@1  92.19 ( 96.59)	Acc@5 100.00 ( 99.97)
Epoch: [54][170/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.5674e-02 (9.6312e-02)	Acc@1  98.44 ( 96.61)	Acc@5 100.00 ( 99.97)
Epoch: [54][180/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.1665e-02 (9.6065e-02)	Acc@1  96.09 ( 96.60)	Acc@5 100.00 ( 99.97)
Epoch: [54][190/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4136e-01 (9.6419e-02)	Acc@1  96.09 ( 96.62)	Acc@5  99.22 ( 99.97)
Epoch: [54][200/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0004e-01 (9.6500e-02)	Acc@1  96.88 ( 96.64)	Acc@5 100.00 ( 99.97)
Epoch: [54][210/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.1615e-01 (9.6860e-02)	Acc@1  96.88 ( 96.66)	Acc@5 100.00 ( 99.97)
Epoch: [54][220/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.002)	Loss 7.0740e-02 (9.7422e-02)	Acc@1  98.44 ( 96.62)	Acc@5 100.00 ( 99.98)
Epoch: [54][230/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.002)	Loss 6.5979e-02 (9.7703e-02)	Acc@1  97.66 ( 96.61)	Acc@5 100.00 ( 99.98)
Epoch: [54][240/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 7.4280e-02 (9.7542e-02)	Acc@1  97.66 ( 96.63)	Acc@5 100.00 ( 99.98)
Epoch: [54][250/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 4.3915e-02 (9.7368e-02)	Acc@1  99.22 ( 96.62)	Acc@5 100.00 ( 99.98)
Epoch: [54][260/391]	Time  0.047 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.3977e-01 (9.7537e-02)	Acc@1  94.53 ( 96.61)	Acc@5 100.00 ( 99.98)
Epoch: [54][270/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 9.7534e-02 (9.7959e-02)	Acc@1  96.88 ( 96.58)	Acc@5 100.00 ( 99.98)
Epoch: [54][280/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 7.8857e-02 (9.7971e-02)	Acc@1  98.44 ( 96.59)	Acc@5 100.00 ( 99.98)
Epoch: [54][290/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 9.6313e-02 (9.8079e-02)	Acc@1  97.66 ( 96.59)	Acc@5 100.00 ( 99.98)
Epoch: [54][300/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.1298e-01 (9.8182e-02)	Acc@1  95.31 ( 96.60)	Acc@5 100.00 ( 99.98)
Epoch: [54][310/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.1322e-01 (9.8182e-02)	Acc@1  96.09 ( 96.59)	Acc@5 100.00 ( 99.98)
Epoch: [54][320/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.0193e-01 (9.7994e-02)	Acc@1  96.88 ( 96.60)	Acc@5 100.00 ( 99.98)
Epoch: [54][330/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.001)	Loss 9.9182e-02 (9.8058e-02)	Acc@1  96.88 ( 96.60)	Acc@5 100.00 ( 99.98)
Epoch: [54][340/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.7969e-01 (9.8786e-02)	Acc@1  94.53 ( 96.57)	Acc@5 100.00 ( 99.98)
Epoch: [54][350/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.5967e-01 (9.9388e-02)	Acc@1  96.09 ( 96.56)	Acc@5 100.00 ( 99.98)
Epoch: [54][360/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.2998e-02 (9.9819e-02)	Acc@1  96.88 ( 96.55)	Acc@5 100.00 ( 99.98)
Epoch: [54][370/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.0205e-01 (9.9878e-02)	Acc@1  96.09 ( 96.54)	Acc@5 100.00 ( 99.98)
Epoch: [54][380/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.2500e-02 (1.0002e-01)	Acc@1  98.44 ( 96.54)	Acc@5 100.00 ( 99.98)
Epoch: [54][390/391]	Time  0.032 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.3672e-01 (1.0030e-01)	Acc@1  95.00 ( 96.52)	Acc@5 100.00 ( 99.98)
## e[54] optimizer.zero_grad (sum) time: 0.2501027584075928
## e[54]       loss.backward (sum) time: 4.114470481872559
## e[54]      optimizer.step (sum) time: 1.7660911083221436
## epoch[54] training(only) time: 15.76556658744812
# Switched to evaluate mode...
Test: [  0/100]	Time  0.172 ( 0.172)	Loss 2.7197e-01 (2.7197e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.017 ( 0.032)	Loss 3.3350e-01 (2.9496e-01)	Acc@1  89.00 ( 91.36)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.023 ( 0.027)	Loss 3.7183e-01 (3.1425e-01)	Acc@1  86.00 ( 90.90)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.019 ( 0.025)	Loss 4.3188e-01 (3.2489e-01)	Acc@1  86.00 ( 90.58)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.023 ( 0.024)	Loss 3.0884e-01 (3.2525e-01)	Acc@1  87.00 ( 90.39)	Acc@5  99.00 ( 99.63)
Test: [ 50/100]	Time  0.017 ( 0.023)	Loss 2.2729e-01 (3.1795e-01)	Acc@1  94.00 ( 90.61)	Acc@5 100.00 ( 99.69)
Test: [ 60/100]	Time  0.025 ( 0.022)	Loss 3.7573e-01 (3.1801e-01)	Acc@1  91.00 ( 90.62)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.017 ( 0.022)	Loss 4.1089e-01 (3.1275e-01)	Acc@1  88.00 ( 90.63)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.017 ( 0.021)	Loss 1.5369e-01 (3.1886e-01)	Acc@1  93.00 ( 90.53)	Acc@5 100.00 ( 99.69)
Test: [ 90/100]	Time  0.016 ( 0.021)	Loss 2.2583e-01 (3.1678e-01)	Acc@1  92.00 ( 90.59)	Acc@5 100.00 ( 99.70)
 * Acc@1 90.740 Acc@5 99.720
### epoch[54] execution time: 17.968360900878906
EPOCH 55
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [55][  0/391]	Time  0.205 ( 0.205)	Data  0.166 ( 0.166)	Loss 1.0913e-01 (1.0913e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [55][ 10/391]	Time  0.038 ( 0.055)	Data  0.001 ( 0.016)	Loss 1.3074e-01 (1.0988e-01)	Acc@1  96.09 ( 96.38)	Acc@5 100.00 (100.00)
Epoch: [55][ 20/391]	Time  0.037 ( 0.048)	Data  0.001 ( 0.009)	Loss 4.3854e-02 (1.0514e-01)	Acc@1  98.44 ( 96.47)	Acc@5 100.00 (100.00)
Epoch: [55][ 30/391]	Time  0.042 ( 0.045)	Data  0.001 ( 0.006)	Loss 5.9387e-02 (9.3035e-02)	Acc@1  96.88 ( 96.90)	Acc@5 100.00 (100.00)
Epoch: [55][ 40/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.005)	Loss 9.2468e-02 (9.2887e-02)	Acc@1  97.66 ( 96.93)	Acc@5 100.00 (100.00)
Epoch: [55][ 50/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.0852e-01 (9.1370e-02)	Acc@1  94.53 ( 96.95)	Acc@5 100.00 (100.00)
Epoch: [55][ 60/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.1029e-01 (9.6571e-02)	Acc@1  97.66 ( 96.89)	Acc@5 100.00 ( 99.99)
Epoch: [55][ 70/391]	Time  0.043 ( 0.042)	Data  0.004 ( 0.003)	Loss 5.9814e-02 (9.4013e-02)	Acc@1  97.66 ( 96.99)	Acc@5 100.00 ( 99.99)
Epoch: [55][ 80/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.3425e-02 (9.3186e-02)	Acc@1  96.09 ( 97.01)	Acc@5 100.00 ( 99.99)
Epoch: [55][ 90/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.2488e-01 (9.5145e-02)	Acc@1  94.53 ( 96.91)	Acc@5 100.00 ( 99.99)
Epoch: [55][100/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.1664e-01 (9.5936e-02)	Acc@1  96.09 ( 96.84)	Acc@5 100.00 ( 99.98)
Epoch: [55][110/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.9712e-02 (9.6711e-02)	Acc@1  97.66 ( 96.78)	Acc@5 100.00 ( 99.98)
Epoch: [55][120/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1700e-01 (9.6791e-02)	Acc@1  96.88 ( 96.74)	Acc@5 100.00 ( 99.98)
Epoch: [55][130/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.5989e-02 (9.6153e-02)	Acc@1  96.88 ( 96.75)	Acc@5 100.00 ( 99.98)
Epoch: [55][140/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.1798e-02 (9.5974e-02)	Acc@1  98.44 ( 96.76)	Acc@5 100.00 ( 99.98)
Epoch: [55][150/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3269e-01 (9.6715e-02)	Acc@1  93.75 ( 96.72)	Acc@5 100.00 ( 99.98)
Epoch: [55][160/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.6914e-02 (9.6733e-02)	Acc@1  96.88 ( 96.71)	Acc@5 100.00 ( 99.98)
Epoch: [55][170/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.2764e-02 (9.5781e-02)	Acc@1  95.31 ( 96.75)	Acc@5 100.00 ( 99.98)
Epoch: [55][180/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0449e-01 (9.4882e-02)	Acc@1  94.53 ( 96.77)	Acc@5 100.00 ( 99.98)
Epoch: [55][190/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.4280e-02 (9.4308e-02)	Acc@1  96.88 ( 96.79)	Acc@5 100.00 ( 99.98)
Epoch: [55][200/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1670e-01 (9.4354e-02)	Acc@1  96.09 ( 96.77)	Acc@5 100.00 ( 99.97)
Epoch: [55][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.3406e-02 (9.4304e-02)	Acc@1  98.44 ( 96.78)	Acc@5 100.00 ( 99.97)
Epoch: [55][220/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5063e-01 (9.4390e-02)	Acc@1  94.53 ( 96.76)	Acc@5 100.00 ( 99.98)
Epoch: [55][230/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.3486e-02 (9.4416e-02)	Acc@1  97.66 ( 96.76)	Acc@5 100.00 ( 99.97)
Epoch: [55][240/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0262e-02 (9.4067e-02)	Acc@1  98.44 ( 96.78)	Acc@5 100.00 ( 99.97)
Epoch: [55][250/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0242e-01 (9.3838e-02)	Acc@1  96.88 ( 96.81)	Acc@5 100.00 ( 99.98)
Epoch: [55][260/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.1401e-02 (9.3372e-02)	Acc@1  98.44 ( 96.85)	Acc@5 100.00 ( 99.97)
Epoch: [55][270/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1517e-01 (9.4060e-02)	Acc@1  96.09 ( 96.81)	Acc@5 100.00 ( 99.97)
Epoch: [55][280/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.0557e-02 (9.4444e-02)	Acc@1  99.22 ( 96.80)	Acc@5 100.00 ( 99.97)
Epoch: [55][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3721e-01 (9.4549e-02)	Acc@1  96.09 ( 96.79)	Acc@5 100.00 ( 99.97)
Epoch: [55][300/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.4463e-02 (9.4848e-02)	Acc@1  96.88 ( 96.77)	Acc@5 100.00 ( 99.97)
Epoch: [55][310/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 9.1370e-02 (9.4866e-02)	Acc@1  96.09 ( 96.76)	Acc@5 100.00 ( 99.97)
Epoch: [55][320/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.0992e-01 (9.4875e-02)	Acc@1  95.31 ( 96.77)	Acc@5 100.00 ( 99.97)
Epoch: [55][330/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 8.4473e-02 (9.5100e-02)	Acc@1  97.66 ( 96.75)	Acc@5 100.00 ( 99.97)
Epoch: [55][340/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.3123e-01 (9.5239e-02)	Acc@1  94.53 ( 96.74)	Acc@5 100.00 ( 99.97)
Epoch: [55][350/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.1975e-01 (9.5266e-02)	Acc@1  94.53 ( 96.73)	Acc@5 100.00 ( 99.97)
Epoch: [55][360/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 8.2642e-02 (9.5023e-02)	Acc@1  97.66 ( 96.75)	Acc@5 100.00 ( 99.97)
Epoch: [55][370/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.4343e-01 (9.5523e-02)	Acc@1  95.31 ( 96.73)	Acc@5 100.00 ( 99.97)
Epoch: [55][380/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.0394e-01 (9.5956e-02)	Acc@1  96.09 ( 96.70)	Acc@5 100.00 ( 99.98)
Epoch: [55][390/391]	Time  0.026 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.7209e-02 (9.6044e-02)	Acc@1  97.50 ( 96.70)	Acc@5 100.00 ( 99.98)
## e[55] optimizer.zero_grad (sum) time: 0.24993443489074707
## e[55]       loss.backward (sum) time: 4.135299921035767
## e[55]      optimizer.step (sum) time: 1.7552382946014404
## epoch[55] training(only) time: 15.941097736358643
# Switched to evaluate mode...
Test: [  0/100]	Time  0.177 ( 0.177)	Loss 3.1348e-01 (3.1348e-01)	Acc@1  93.00 ( 93.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 3.6523e-01 (3.0649e-01)	Acc@1  86.00 ( 91.18)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.021 ( 0.028)	Loss 3.7939e-01 (3.1811e-01)	Acc@1  88.00 ( 91.33)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.018 ( 0.026)	Loss 4.9756e-01 (3.3359e-01)	Acc@1  85.00 ( 90.68)	Acc@5 100.00 ( 99.61)
Test: [ 40/100]	Time  0.023 ( 0.024)	Loss 3.4546e-01 (3.2997e-01)	Acc@1  89.00 ( 90.63)	Acc@5  99.00 ( 99.63)
Test: [ 50/100]	Time  0.020 ( 0.024)	Loss 2.1265e-01 (3.2108e-01)	Acc@1  94.00 ( 90.69)	Acc@5 100.00 ( 99.69)
Test: [ 60/100]	Time  0.019 ( 0.023)	Loss 3.9404e-01 (3.1982e-01)	Acc@1  92.00 ( 90.59)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.019 ( 0.023)	Loss 4.0088e-01 (3.1484e-01)	Acc@1  87.00 ( 90.55)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.023 ( 0.022)	Loss 1.4502e-01 (3.1882e-01)	Acc@1  95.00 ( 90.51)	Acc@5 100.00 ( 99.70)
Test: [ 90/100]	Time  0.017 ( 0.022)	Loss 2.3279e-01 (3.1599e-01)	Acc@1  93.00 ( 90.60)	Acc@5 100.00 ( 99.70)
 * Acc@1 90.670 Acc@5 99.720
### epoch[55] execution time: 18.26704978942871
EPOCH 56
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [56][  0/391]	Time  0.209 ( 0.209)	Data  0.167 ( 0.167)	Loss 9.9121e-02 (9.9121e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [56][ 10/391]	Time  0.038 ( 0.055)	Data  0.001 ( 0.016)	Loss 5.8929e-02 (9.0235e-02)	Acc@1  98.44 ( 96.73)	Acc@5 100.00 (100.00)
Epoch: [56][ 20/391]	Time  0.040 ( 0.048)	Data  0.001 ( 0.009)	Loss 6.7932e-02 (9.6707e-02)	Acc@1  98.44 ( 96.21)	Acc@5 100.00 (100.00)
Epoch: [56][ 30/391]	Time  0.039 ( 0.045)	Data  0.001 ( 0.006)	Loss 1.4099e-01 (9.9931e-02)	Acc@1  96.88 ( 96.24)	Acc@5  99.22 ( 99.97)
Epoch: [56][ 40/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.005)	Loss 1.1572e-01 (1.0058e-01)	Acc@1  96.88 ( 96.36)	Acc@5 100.00 ( 99.98)
Epoch: [56][ 50/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.0901e-01 (1.0028e-01)	Acc@1  95.31 ( 96.43)	Acc@5 100.00 ( 99.98)
Epoch: [56][ 60/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.004)	Loss 8.6243e-02 (1.0048e-01)	Acc@1  97.66 ( 96.44)	Acc@5 100.00 ( 99.99)
Epoch: [56][ 70/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 9.0759e-02 (9.9067e-02)	Acc@1  96.88 ( 96.53)	Acc@5 100.00 ( 99.99)
Epoch: [56][ 80/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.3428e-01 (9.8116e-02)	Acc@1  93.75 ( 96.58)	Acc@5 100.00 ( 99.99)
Epoch: [56][ 90/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.1951e-01 (9.6165e-02)	Acc@1  96.09 ( 96.67)	Acc@5 100.00 ( 99.99)
Epoch: [56][100/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.003)	Loss 3.8727e-02 (9.6662e-02)	Acc@1 100.00 ( 96.64)	Acc@5 100.00 ( 99.99)
Epoch: [56][110/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.2021e-02 (9.6048e-02)	Acc@1  98.44 ( 96.67)	Acc@5 100.00 ( 99.99)
Epoch: [56][120/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2979e-02 (9.5580e-02)	Acc@1  98.44 ( 96.72)	Acc@5 100.00 ( 99.99)
Epoch: [56][130/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4014e-01 (9.5760e-02)	Acc@1  95.31 ( 96.70)	Acc@5  99.22 ( 99.98)
Epoch: [56][140/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.5510e-02 (9.5716e-02)	Acc@1  96.88 ( 96.66)	Acc@5 100.00 ( 99.98)
Epoch: [56][150/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2079e-01 (9.5654e-02)	Acc@1  96.88 ( 96.69)	Acc@5 100.00 ( 99.98)
Epoch: [56][160/391]	Time  0.046 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.5449e-02 (9.5952e-02)	Acc@1  96.88 ( 96.66)	Acc@5 100.00 ( 99.99)
Epoch: [56][170/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3762e-02 (9.5679e-02)	Acc@1  98.44 ( 96.69)	Acc@5 100.00 ( 99.99)
Epoch: [56][180/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.6008e-02 (9.5298e-02)	Acc@1  97.66 ( 96.72)	Acc@5 100.00 ( 99.99)
Epoch: [56][190/391]	Time  0.034 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4250e-02 (9.5137e-02)	Acc@1  97.66 ( 96.71)	Acc@5 100.00 ( 99.99)
Epoch: [56][200/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.2031e-02 (9.4970e-02)	Acc@1  96.88 ( 96.71)	Acc@5 100.00 ( 99.99)
Epoch: [56][210/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1456e-01 (9.4459e-02)	Acc@1  96.09 ( 96.75)	Acc@5 100.00 ( 99.99)
Epoch: [56][220/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.5144e-02 (9.3461e-02)	Acc@1  96.88 ( 96.78)	Acc@5 100.00 ( 99.99)
Epoch: [56][230/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2341e-01 (9.3712e-02)	Acc@1  96.88 ( 96.77)	Acc@5 100.00 ( 99.99)
Epoch: [56][240/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.7412e-02 (9.3803e-02)	Acc@1  94.53 ( 96.76)	Acc@5 100.00 ( 99.98)
Epoch: [56][250/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.5747e-01 (9.3898e-02)	Acc@1  95.31 ( 96.75)	Acc@5 100.00 ( 99.98)
Epoch: [56][260/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 9.0759e-02 (9.4041e-02)	Acc@1  97.66 ( 96.74)	Acc@5 100.00 ( 99.98)
Epoch: [56][270/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 6.7688e-02 (9.3234e-02)	Acc@1  97.66 ( 96.78)	Acc@5 100.00 ( 99.98)
Epoch: [56][280/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.002)	Loss 8.4229e-02 (9.3122e-02)	Acc@1  97.66 ( 96.78)	Acc@5 100.00 ( 99.98)
Epoch: [56][290/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.0974e-01 (9.3374e-02)	Acc@1  96.88 ( 96.76)	Acc@5 100.00 ( 99.98)
Epoch: [56][300/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.3599e-01 (9.4041e-02)	Acc@1  94.53 ( 96.73)	Acc@5 100.00 ( 99.98)
Epoch: [56][310/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.6785e-01 (9.4489e-02)	Acc@1  96.09 ( 96.74)	Acc@5  99.22 ( 99.98)
Epoch: [56][320/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.3293e-01 (9.4801e-02)	Acc@1  94.53 ( 96.72)	Acc@5 100.00 ( 99.98)
Epoch: [56][330/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 8.1909e-02 (9.5124e-02)	Acc@1  96.09 ( 96.71)	Acc@5 100.00 ( 99.98)
Epoch: [56][340/391]	Time  0.043 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.0449e-01 (9.5105e-02)	Acc@1  96.88 ( 96.70)	Acc@5 100.00 ( 99.98)
Epoch: [56][350/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 8.1116e-02 (9.5327e-02)	Acc@1  97.66 ( 96.69)	Acc@5 100.00 ( 99.98)
Epoch: [56][360/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.9133e-02 (9.5525e-02)	Acc@1  99.22 ( 96.68)	Acc@5 100.00 ( 99.98)
Epoch: [56][370/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 8.4961e-02 (9.5414e-02)	Acc@1  96.09 ( 96.68)	Acc@5 100.00 ( 99.98)
Epoch: [56][380/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.3833e-02 (9.5312e-02)	Acc@1  98.44 ( 96.68)	Acc@5 100.00 ( 99.98)
Epoch: [56][390/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.8359e-01 (9.5465e-02)	Acc@1  93.75 ( 96.68)	Acc@5 100.00 ( 99.98)
## e[56] optimizer.zero_grad (sum) time: 0.25072813034057617
## e[56]       loss.backward (sum) time: 4.139263868331909
## e[56]      optimizer.step (sum) time: 1.7824397087097168
## epoch[56] training(only) time: 15.796299695968628
# Switched to evaluate mode...
Test: [  0/100]	Time  0.160 ( 0.160)	Loss 3.7549e-01 (3.7549e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.017 ( 0.033)	Loss 4.5483e-01 (3.2279e-01)	Acc@1  89.00 ( 91.09)	Acc@5  99.00 ( 99.82)
Test: [ 20/100]	Time  0.023 ( 0.027)	Loss 3.5205e-01 (3.3677e-01)	Acc@1  87.00 ( 90.43)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.025 ( 0.025)	Loss 4.2847e-01 (3.5348e-01)	Acc@1  87.00 ( 90.06)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.021 ( 0.023)	Loss 3.6865e-01 (3.4906e-01)	Acc@1  89.00 ( 90.12)	Acc@5  99.00 ( 99.66)
Test: [ 50/100]	Time  0.020 ( 0.022)	Loss 2.3950e-01 (3.3876e-01)	Acc@1  93.00 ( 90.12)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.019 ( 0.022)	Loss 3.9478e-01 (3.3675e-01)	Acc@1  92.00 ( 90.23)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.019 ( 0.021)	Loss 4.7656e-01 (3.2863e-01)	Acc@1  87.00 ( 90.28)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.019 ( 0.021)	Loss 1.5479e-01 (3.3499e-01)	Acc@1  95.00 ( 90.20)	Acc@5  99.00 ( 99.69)
Test: [ 90/100]	Time  0.020 ( 0.021)	Loss 2.8442e-01 (3.3190e-01)	Acc@1  91.00 ( 90.29)	Acc@5 100.00 ( 99.71)
 * Acc@1 90.440 Acc@5 99.710
### epoch[56] execution time: 17.98866868019104
EPOCH 57
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [57][  0/391]	Time  0.228 ( 0.228)	Data  0.188 ( 0.188)	Loss 7.1716e-02 (7.1716e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [57][ 10/391]	Time  0.040 ( 0.057)	Data  0.001 ( 0.018)	Loss 1.3354e-01 (9.4507e-02)	Acc@1  96.09 ( 96.80)	Acc@5  99.22 ( 99.93)
Epoch: [57][ 20/391]	Time  0.042 ( 0.049)	Data  0.001 ( 0.010)	Loss 9.0210e-02 (9.0937e-02)	Acc@1  96.09 ( 96.95)	Acc@5 100.00 ( 99.96)
Epoch: [57][ 30/391]	Time  0.038 ( 0.046)	Data  0.001 ( 0.007)	Loss 8.0811e-02 (8.9017e-02)	Acc@1  96.09 ( 96.80)	Acc@5 100.00 ( 99.97)
Epoch: [57][ 40/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.005)	Loss 1.0315e-01 (9.1606e-02)	Acc@1  95.31 ( 96.82)	Acc@5 100.00 ( 99.98)
Epoch: [57][ 50/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.005)	Loss 9.1553e-02 (9.2334e-02)	Acc@1  96.09 ( 96.84)	Acc@5 100.00 ( 99.98)
Epoch: [57][ 60/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.004)	Loss 5.8716e-02 (9.1163e-02)	Acc@1  98.44 ( 96.89)	Acc@5 100.00 ( 99.96)
Epoch: [57][ 70/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 6.3538e-02 (8.9976e-02)	Acc@1  98.44 ( 96.99)	Acc@5 100.00 ( 99.97)
Epoch: [57][ 80/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 9.8145e-02 (9.0603e-02)	Acc@1  95.31 ( 96.90)	Acc@5 100.00 ( 99.97)
Epoch: [57][ 90/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 8.3008e-02 (9.0406e-02)	Acc@1  96.88 ( 96.89)	Acc@5 100.00 ( 99.97)
Epoch: [57][100/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.8918e-02 (9.1861e-02)	Acc@1  97.66 ( 96.86)	Acc@5 100.00 ( 99.97)
Epoch: [57][110/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.4585e-02 (9.2704e-02)	Acc@1  99.22 ( 96.81)	Acc@5 100.00 ( 99.97)
Epoch: [57][120/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.8247e-02 (9.1811e-02)	Acc@1  98.44 ( 96.82)	Acc@5 100.00 ( 99.97)
Epoch: [57][130/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.2947e-02 (9.1884e-02)	Acc@1  96.09 ( 96.80)	Acc@5 100.00 ( 99.98)
Epoch: [57][140/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.3008e-02 (9.1806e-02)	Acc@1  96.88 ( 96.80)	Acc@5 100.00 ( 99.98)
Epoch: [57][150/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.5510e-02 (9.1256e-02)	Acc@1  98.44 ( 96.86)	Acc@5 100.00 ( 99.98)
Epoch: [57][160/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.7473e-02 (9.1398e-02)	Acc@1  97.66 ( 96.87)	Acc@5 100.00 ( 99.98)
Epoch: [57][170/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.0129e-02 (9.1464e-02)	Acc@1  98.44 ( 96.89)	Acc@5 100.00 ( 99.98)
Epoch: [57][180/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8859e-02 (9.0417e-02)	Acc@1  99.22 ( 96.96)	Acc@5 100.00 ( 99.98)
Epoch: [57][190/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.2163e-02 (9.0513e-02)	Acc@1  95.31 ( 96.94)	Acc@5 100.00 ( 99.98)
Epoch: [57][200/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.5410e-02 (8.9972e-02)	Acc@1  99.22 ( 96.95)	Acc@5 100.00 ( 99.98)
Epoch: [57][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.7646e-02 (8.9515e-02)	Acc@1  96.88 ( 96.97)	Acc@5 100.00 ( 99.98)
Epoch: [57][220/391]	Time  0.051 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1737e-01 (8.9915e-02)	Acc@1  96.09 ( 96.97)	Acc@5 100.00 ( 99.98)
Epoch: [57][230/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5674e-01 (9.0153e-02)	Acc@1  93.75 ( 96.95)	Acc@5 100.00 ( 99.98)
Epoch: [57][240/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.6427e-02 (8.9658e-02)	Acc@1  97.66 ( 96.95)	Acc@5 100.00 ( 99.98)
Epoch: [57][250/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.3445e-02 (8.9296e-02)	Acc@1  96.09 ( 96.95)	Acc@5 100.00 ( 99.98)
Epoch: [57][260/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0602e-01 (8.9984e-02)	Acc@1  96.09 ( 96.90)	Acc@5 100.00 ( 99.98)
Epoch: [57][270/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.3538e-02 (8.9842e-02)	Acc@1  98.44 ( 96.91)	Acc@5 100.00 ( 99.98)
Epoch: [57][280/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2476e-01 (9.0155e-02)	Acc@1  95.31 ( 96.89)	Acc@5 100.00 ( 99.98)
Epoch: [57][290/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0598e-02 (8.9907e-02)	Acc@1  98.44 ( 96.90)	Acc@5 100.00 ( 99.98)
Epoch: [57][300/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.6914e-02 (9.0233e-02)	Acc@1  96.09 ( 96.88)	Acc@5 100.00 ( 99.98)
Epoch: [57][310/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.6488e-02 (9.0222e-02)	Acc@1  98.44 ( 96.88)	Acc@5 100.00 ( 99.98)
Epoch: [57][320/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.1914e-01 (9.0496e-02)	Acc@1  96.09 ( 96.86)	Acc@5 100.00 ( 99.99)
Epoch: [57][330/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.0974e-02 (9.0650e-02)	Acc@1  96.88 ( 96.86)	Acc@5 100.00 ( 99.99)
Epoch: [57][340/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.0323e-02 (9.0727e-02)	Acc@1  98.44 ( 96.87)	Acc@5 100.00 ( 99.98)
Epoch: [57][350/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.2524e-01 (9.1199e-02)	Acc@1  96.09 ( 96.84)	Acc@5 100.00 ( 99.98)
Epoch: [57][360/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.2134e-02 (9.2042e-02)	Acc@1  96.88 ( 96.80)	Acc@5 100.00 ( 99.98)
Epoch: [57][370/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.7332e-02 (9.2389e-02)	Acc@1  97.66 ( 96.79)	Acc@5 100.00 ( 99.98)
Epoch: [57][380/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.5939e-02 (9.1800e-02)	Acc@1  97.66 ( 96.81)	Acc@5 100.00 ( 99.98)
Epoch: [57][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.5500e-02 (9.2053e-02)	Acc@1  98.75 ( 96.80)	Acc@5 100.00 ( 99.98)
## e[57] optimizer.zero_grad (sum) time: 0.25087928771972656
## e[57]       loss.backward (sum) time: 4.187626361846924
## e[57]      optimizer.step (sum) time: 1.7153174877166748
## epoch[57] training(only) time: 15.909616708755493
# Switched to evaluate mode...
Test: [  0/100]	Time  0.179 ( 0.179)	Loss 2.9492e-01 (2.9492e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.020 ( 0.032)	Loss 3.7134e-01 (3.1125e-01)	Acc@1  88.00 ( 90.36)	Acc@5  99.00 ( 99.82)
Test: [ 20/100]	Time  0.022 ( 0.027)	Loss 3.8965e-01 (3.3010e-01)	Acc@1  88.00 ( 90.29)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.018 ( 0.025)	Loss 4.2773e-01 (3.3690e-01)	Acc@1  86.00 ( 90.35)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.020 ( 0.023)	Loss 3.5010e-01 (3.3590e-01)	Acc@1  88.00 ( 90.29)	Acc@5 100.00 ( 99.68)
Test: [ 50/100]	Time  0.017 ( 0.022)	Loss 2.2717e-01 (3.2733e-01)	Acc@1  91.00 ( 90.29)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.023 ( 0.022)	Loss 3.8843e-01 (3.2554e-01)	Acc@1  91.00 ( 90.30)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.017 ( 0.021)	Loss 4.3677e-01 (3.2016e-01)	Acc@1  87.00 ( 90.37)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.023 ( 0.021)	Loss 1.3440e-01 (3.2547e-01)	Acc@1  95.00 ( 90.37)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.017 ( 0.021)	Loss 2.6636e-01 (3.2586e-01)	Acc@1  94.00 ( 90.35)	Acc@5 100.00 ( 99.76)
 * Acc@1 90.510 Acc@5 99.750
### epoch[57] execution time: 18.106956005096436
EPOCH 58
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [58][  0/391]	Time  0.215 ( 0.215)	Data  0.177 ( 0.177)	Loss 1.1786e-01 (1.1786e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [58][ 10/391]	Time  0.038 ( 0.056)	Data  0.001 ( 0.017)	Loss 1.1896e-01 (8.5566e-02)	Acc@1  95.31 ( 97.09)	Acc@5 100.00 (100.00)
Epoch: [58][ 20/391]	Time  0.039 ( 0.049)	Data  0.001 ( 0.009)	Loss 1.3098e-01 (8.9815e-02)	Acc@1  96.09 ( 96.99)	Acc@5 100.00 ( 99.96)
Epoch: [58][ 30/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.007)	Loss 1.0565e-01 (9.1980e-02)	Acc@1  93.75 ( 96.77)	Acc@5 100.00 ( 99.97)
Epoch: [58][ 40/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.005)	Loss 1.4246e-01 (9.0434e-02)	Acc@1  96.09 ( 96.95)	Acc@5 100.00 ( 99.98)
Epoch: [58][ 50/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.004)	Loss 6.3660e-02 (8.9613e-02)	Acc@1  98.44 ( 97.03)	Acc@5 100.00 ( 99.98)
Epoch: [58][ 60/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.004)	Loss 9.7900e-02 (9.1565e-02)	Acc@1  96.09 ( 96.93)	Acc@5 100.00 ( 99.99)
Epoch: [58][ 70/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 9.8999e-02 (8.9456e-02)	Acc@1  96.88 ( 97.07)	Acc@5 100.00 ( 99.99)
Epoch: [58][ 80/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 8.9417e-02 (8.7781e-02)	Acc@1  96.09 ( 97.07)	Acc@5 100.00 ( 99.99)
Epoch: [58][ 90/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.3782e-02 (8.6406e-02)	Acc@1  97.66 ( 97.07)	Acc@5 100.00 ( 99.99)
Epoch: [58][100/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.5511e-02 (8.6128e-02)	Acc@1  97.66 ( 97.07)	Acc@5 100.00 ( 99.99)
Epoch: [58][110/391]	Time  0.045 ( 0.041)	Data  0.001 ( 0.003)	Loss 4.0161e-02 (8.6282e-02)	Acc@1 100.00 ( 97.07)	Acc@5 100.00 ( 99.99)
Epoch: [58][120/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.5693e-02 (8.5803e-02)	Acc@1  97.66 ( 97.11)	Acc@5 100.00 ( 99.98)
Epoch: [58][130/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.2805e-02 (8.5883e-02)	Acc@1  97.66 ( 97.10)	Acc@5 100.00 ( 99.98)
Epoch: [58][140/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.1665e-02 (8.7737e-02)	Acc@1  97.66 ( 97.05)	Acc@5 100.00 ( 99.97)
Epoch: [58][150/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1361e-02 (8.7495e-02)	Acc@1  97.66 ( 97.05)	Acc@5 100.00 ( 99.97)
Epoch: [58][160/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.0251e-02 (8.7726e-02)	Acc@1  97.66 ( 97.01)	Acc@5 100.00 ( 99.98)
Epoch: [58][170/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6833e-01 (8.8817e-02)	Acc@1  93.75 ( 96.93)	Acc@5 100.00 ( 99.98)
Epoch: [58][180/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.6294e-02 (8.8754e-02)	Acc@1  96.09 ( 96.91)	Acc@5 100.00 ( 99.98)
Epoch: [58][190/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1249e-01 (8.9117e-02)	Acc@1  95.31 ( 96.92)	Acc@5 100.00 ( 99.98)
Epoch: [58][200/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2246e-02 (8.9364e-02)	Acc@1  99.22 ( 96.91)	Acc@5 100.00 ( 99.98)
Epoch: [58][210/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1304e-01 (9.0102e-02)	Acc@1  96.09 ( 96.89)	Acc@5 100.00 ( 99.98)
Epoch: [58][220/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.8115e-02 (9.0665e-02)	Acc@1  96.88 ( 96.85)	Acc@5 100.00 ( 99.98)
Epoch: [58][230/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5906e-01 (9.0976e-02)	Acc@1  95.31 ( 96.86)	Acc@5 100.00 ( 99.98)
Epoch: [58][240/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9561e-02 (9.1006e-02)	Acc@1  98.44 ( 96.83)	Acc@5 100.00 ( 99.98)
Epoch: [58][250/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.3403e-01 (9.0871e-02)	Acc@1  96.88 ( 96.86)	Acc@5 100.00 ( 99.98)
Epoch: [58][260/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 4.0253e-02 (9.0780e-02)	Acc@1  98.44 ( 96.85)	Acc@5 100.00 ( 99.98)
Epoch: [58][270/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.0400e-01 (9.0780e-02)	Acc@1  96.88 ( 96.86)	Acc@5 100.00 ( 99.98)
Epoch: [58][280/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 6.5552e-02 (9.0392e-02)	Acc@1  98.44 ( 96.86)	Acc@5 100.00 ( 99.98)
Epoch: [58][290/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.0803e-01 (9.0220e-02)	Acc@1  94.53 ( 96.87)	Acc@5 100.00 ( 99.98)
Epoch: [58][300/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.4087e-02 (9.0607e-02)	Acc@1  98.44 ( 96.86)	Acc@5 100.00 ( 99.98)
Epoch: [58][310/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.3416e-01 (9.1012e-02)	Acc@1  95.31 ( 96.83)	Acc@5 100.00 ( 99.98)
Epoch: [58][320/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.4280e-02 (9.0841e-02)	Acc@1  99.22 ( 96.84)	Acc@5 100.00 ( 99.98)
Epoch: [58][330/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.5320e-01 (9.0770e-02)	Acc@1  95.31 ( 96.86)	Acc@5 100.00 ( 99.98)
Epoch: [58][340/391]	Time  0.051 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.2384e-01 (9.0810e-02)	Acc@1  93.75 ( 96.85)	Acc@5 100.00 ( 99.98)
Epoch: [58][350/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 8.0505e-02 (9.1073e-02)	Acc@1  96.88 ( 96.85)	Acc@5 100.00 ( 99.98)
Epoch: [58][360/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.3730e-02 (9.1150e-02)	Acc@1  96.88 ( 96.84)	Acc@5 100.00 ( 99.98)
Epoch: [58][370/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.1676e-02 (9.1130e-02)	Acc@1  97.66 ( 96.82)	Acc@5 100.00 ( 99.98)
Epoch: [58][380/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.6631e-02 (9.0888e-02)	Acc@1  98.44 ( 96.83)	Acc@5 100.00 ( 99.98)
Epoch: [58][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.4155e-02 (9.1430e-02)	Acc@1 100.00 ( 96.81)	Acc@5 100.00 ( 99.98)
## e[58] optimizer.zero_grad (sum) time: 0.25118494033813477
## e[58]       loss.backward (sum) time: 4.188560962677002
## e[58]      optimizer.step (sum) time: 1.7299814224243164
## epoch[58] training(only) time: 15.854339361190796
# Switched to evaluate mode...
Test: [  0/100]	Time  0.173 ( 0.173)	Loss 2.8394e-01 (2.8394e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.021 ( 0.035)	Loss 4.0137e-01 (2.9601e-01)	Acc@1  89.00 ( 91.64)	Acc@5  99.00 ( 99.82)
Test: [ 20/100]	Time  0.017 ( 0.028)	Loss 4.1064e-01 (3.1333e-01)	Acc@1  88.00 ( 91.38)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.017 ( 0.025)	Loss 4.2065e-01 (3.2256e-01)	Acc@1  87.00 ( 91.03)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.017 ( 0.023)	Loss 3.8916e-01 (3.2184e-01)	Acc@1  89.00 ( 90.98)	Acc@5  99.00 ( 99.73)
Test: [ 50/100]	Time  0.016 ( 0.023)	Loss 2.2998e-01 (3.1707e-01)	Acc@1  92.00 ( 91.10)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.024 ( 0.022)	Loss 3.8599e-01 (3.1833e-01)	Acc@1  89.00 ( 90.84)	Acc@5  99.00 ( 99.75)
Test: [ 70/100]	Time  0.017 ( 0.022)	Loss 4.2383e-01 (3.1521e-01)	Acc@1  88.00 ( 90.83)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.021 ( 0.022)	Loss 1.4868e-01 (3.1911e-01)	Acc@1  93.00 ( 90.73)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.017 ( 0.021)	Loss 2.5342e-01 (3.1715e-01)	Acc@1  95.00 ( 90.81)	Acc@5 100.00 ( 99.76)
 * Acc@1 90.820 Acc@5 99.770
### epoch[58] execution time: 18.070926189422607
EPOCH 59
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [59][  0/391]	Time  0.212 ( 0.212)	Data  0.171 ( 0.171)	Loss 6.7261e-02 (6.7261e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [59][ 10/391]	Time  0.039 ( 0.056)	Data  0.001 ( 0.016)	Loss 7.7209e-02 (9.6422e-02)	Acc@1  97.66 ( 97.02)	Acc@5 100.00 (100.00)
Epoch: [59][ 20/391]	Time  0.039 ( 0.048)	Data  0.001 ( 0.009)	Loss 9.5215e-02 (9.2513e-02)	Acc@1  95.31 ( 96.99)	Acc@5 100.00 (100.00)
Epoch: [59][ 30/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.006)	Loss 7.8003e-02 (9.5147e-02)	Acc@1  97.66 ( 96.85)	Acc@5 100.00 (100.00)
Epoch: [59][ 40/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.005)	Loss 5.3986e-02 (9.2742e-02)	Acc@1  99.22 ( 96.95)	Acc@5 100.00 (100.00)
Epoch: [59][ 50/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.004)	Loss 5.5145e-02 (9.3053e-02)	Acc@1  99.22 ( 96.91)	Acc@5 100.00 (100.00)
Epoch: [59][ 60/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.004)	Loss 9.4238e-02 (9.3574e-02)	Acc@1  96.88 ( 96.86)	Acc@5 100.00 (100.00)
Epoch: [59][ 70/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.5588e-01 (9.6005e-02)	Acc@1  95.31 ( 96.86)	Acc@5 100.00 (100.00)
Epoch: [59][ 80/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 8.0627e-02 (9.5332e-02)	Acc@1  96.88 ( 96.85)	Acc@5 100.00 (100.00)
Epoch: [59][ 90/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.6956e-02 (9.3857e-02)	Acc@1  97.66 ( 96.91)	Acc@5 100.00 (100.00)
Epoch: [59][100/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.1664e-01 (9.3289e-02)	Acc@1  95.31 ( 96.94)	Acc@5 100.00 ( 99.99)
Epoch: [59][110/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.0699e-02 (9.3379e-02)	Acc@1  96.88 ( 96.95)	Acc@5 100.00 ( 99.99)
Epoch: [59][120/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.3293e-02 (9.3381e-02)	Acc@1  98.44 ( 96.94)	Acc@5 100.00 ( 99.99)
Epoch: [59][130/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 7.5684e-02 (9.2838e-02)	Acc@1  97.66 ( 96.98)	Acc@5 100.00 ( 99.99)
Epoch: [59][140/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.5022e-02 (9.2550e-02)	Acc@1  97.66 ( 96.99)	Acc@5 100.00 ( 99.99)
Epoch: [59][150/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.1188e-02 (9.3047e-02)	Acc@1  98.44 ( 96.95)	Acc@5 100.00 ( 99.98)
Epoch: [59][160/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0934e-02 (9.2579e-02)	Acc@1  97.66 ( 96.91)	Acc@5 100.00 ( 99.99)
Epoch: [59][170/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.1787e-02 (9.2440e-02)	Acc@1  96.09 ( 96.89)	Acc@5 100.00 ( 99.99)
Epoch: [59][180/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.3313e-02 (9.2744e-02)	Acc@1  96.88 ( 96.85)	Acc@5 100.00 ( 99.99)
Epoch: [59][190/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.5929e-02 (9.1322e-02)	Acc@1  97.66 ( 96.90)	Acc@5 100.00 ( 99.99)
Epoch: [59][200/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.5735e-02 (9.0391e-02)	Acc@1  97.66 ( 96.93)	Acc@5 100.00 ( 99.99)
Epoch: [59][210/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6565e-01 (9.1434e-02)	Acc@1  93.75 ( 96.89)	Acc@5 100.00 ( 99.99)
Epoch: [59][220/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.0007e-02 (9.0879e-02)	Acc@1  96.88 ( 96.91)	Acc@5 100.00 ( 99.99)
Epoch: [59][230/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.9172e-02 (9.0935e-02)	Acc@1  99.22 ( 96.93)	Acc@5 100.00 ( 99.99)
Epoch: [59][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0657e-01 (9.0827e-02)	Acc@1  95.31 ( 96.91)	Acc@5 100.00 ( 99.99)
Epoch: [59][250/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1560e-01 (9.0739e-02)	Acc@1  96.09 ( 96.91)	Acc@5 100.00 ( 99.99)
Epoch: [59][260/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.5317e-02 (9.0733e-02)	Acc@1  96.88 ( 96.91)	Acc@5 100.00 ( 99.99)
Epoch: [59][270/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.8115e-02 (9.0632e-02)	Acc@1  99.22 ( 96.93)	Acc@5 100.00 ( 99.99)
Epoch: [59][280/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.7322e-02 (9.0604e-02)	Acc@1  98.44 ( 96.91)	Acc@5 100.00 ( 99.99)
Epoch: [59][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.8237e-02 (9.0959e-02)	Acc@1  96.88 ( 96.90)	Acc@5 100.00 ( 99.99)
Epoch: [59][300/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 8.8135e-02 (9.1118e-02)	Acc@1  97.66 ( 96.89)	Acc@5 100.00 ( 99.99)
Epoch: [59][310/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.3525e-01 (9.1858e-02)	Acc@1  95.31 ( 96.88)	Acc@5 100.00 ( 99.98)
Epoch: [59][320/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 8.1055e-02 (9.1283e-02)	Acc@1  97.66 ( 96.91)	Acc@5 100.00 ( 99.98)
Epoch: [59][330/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.3975e-02 (9.1049e-02)	Acc@1  97.66 ( 96.92)	Acc@5 100.00 ( 99.98)
Epoch: [59][340/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.8420e-02 (9.1282e-02)	Acc@1  97.66 ( 96.92)	Acc@5 100.00 ( 99.98)
Epoch: [59][350/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.0803e-01 (9.1474e-02)	Acc@1  96.09 ( 96.91)	Acc@5 100.00 ( 99.98)
Epoch: [59][360/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.0699e-02 (9.1770e-02)	Acc@1  96.88 ( 96.89)	Acc@5 100.00 ( 99.98)
Epoch: [59][370/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 9.0820e-02 (9.1722e-02)	Acc@1  96.88 ( 96.90)	Acc@5 100.00 ( 99.99)
Epoch: [59][380/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 9.3140e-02 (9.1981e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 ( 99.99)
Epoch: [59][390/391]	Time  0.027 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.4209e-01 (9.2334e-02)	Acc@1  96.25 ( 96.87)	Acc@5 100.00 ( 99.98)
## e[59] optimizer.zero_grad (sum) time: 0.2506873607635498
## e[59]       loss.backward (sum) time: 4.182395935058594
## e[59]      optimizer.step (sum) time: 1.727977991104126
## epoch[59] training(only) time: 15.90652871131897
# Switched to evaluate mode...
Test: [  0/100]	Time  0.178 ( 0.178)	Loss 2.6318e-01 (2.6318e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.017 ( 0.034)	Loss 3.6108e-01 (2.8678e-01)	Acc@1  87.00 ( 90.64)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.021 ( 0.027)	Loss 4.2114e-01 (3.2082e-01)	Acc@1  87.00 ( 90.57)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.019 ( 0.025)	Loss 4.9219e-01 (3.3458e-01)	Acc@1  86.00 ( 90.23)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.020 ( 0.024)	Loss 4.3164e-01 (3.3489e-01)	Acc@1  88.00 ( 90.39)	Acc@5  99.00 ( 99.63)
Test: [ 50/100]	Time  0.016 ( 0.023)	Loss 2.2412e-01 (3.2756e-01)	Acc@1  94.00 ( 90.69)	Acc@5 100.00 ( 99.61)
Test: [ 60/100]	Time  0.020 ( 0.023)	Loss 3.9233e-01 (3.2817e-01)	Acc@1  92.00 ( 90.62)	Acc@5 100.00 ( 99.66)
Test: [ 70/100]	Time  0.016 ( 0.022)	Loss 5.0635e-01 (3.2462e-01)	Acc@1  86.00 ( 90.56)	Acc@5 100.00 ( 99.68)
Test: [ 80/100]	Time  0.021 ( 0.022)	Loss 1.3452e-01 (3.2878e-01)	Acc@1  95.00 ( 90.42)	Acc@5 100.00 ( 99.67)
Test: [ 90/100]	Time  0.018 ( 0.022)	Loss 2.6147e-01 (3.2744e-01)	Acc@1  93.00 ( 90.52)	Acc@5 100.00 ( 99.67)
 * Acc@1 90.580 Acc@5 99.680
### epoch[59] execution time: 18.15782904624939
EPOCH 60
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [60][  0/391]	Time  0.224 ( 0.224)	Data  0.184 ( 0.184)	Loss 1.3037e-01 (1.3037e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
Epoch: [60][ 10/391]	Time  0.041 ( 0.057)	Data  0.001 ( 0.018)	Loss 9.8083e-02 (8.4869e-02)	Acc@1  97.66 ( 96.66)	Acc@5 100.00 (100.00)
Epoch: [60][ 20/391]	Time  0.042 ( 0.050)	Data  0.001 ( 0.010)	Loss 6.2500e-02 (8.2614e-02)	Acc@1  96.09 ( 96.91)	Acc@5 100.00 ( 99.96)
Epoch: [60][ 30/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.007)	Loss 4.9713e-02 (8.8960e-02)	Acc@1  98.44 ( 96.72)	Acc@5 100.00 ( 99.97)
Epoch: [60][ 40/391]	Time  0.038 ( 0.045)	Data  0.001 ( 0.005)	Loss 4.4037e-02 (9.1432e-02)	Acc@1  98.44 ( 96.76)	Acc@5 100.00 ( 99.98)
Epoch: [60][ 50/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.004)	Loss 6.7749e-02 (8.7300e-02)	Acc@1  96.88 ( 96.89)	Acc@5 100.00 ( 99.98)
Epoch: [60][ 60/391]	Time  0.037 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.1053e-01 (8.9082e-02)	Acc@1  95.31 ( 96.77)	Acc@5 100.00 ( 99.99)
Epoch: [60][ 70/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.003)	Loss 6.9092e-02 (8.7558e-02)	Acc@1  97.66 ( 96.85)	Acc@5 100.00 ( 99.99)
Epoch: [60][ 80/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 9.1736e-02 (8.6068e-02)	Acc@1  96.88 ( 96.97)	Acc@5 100.00 ( 99.99)
Epoch: [60][ 90/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.3293e-01 (8.6049e-02)	Acc@1  92.19 ( 96.99)	Acc@5  99.22 ( 99.98)
Epoch: [60][100/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 9.1614e-02 (8.5944e-02)	Acc@1  96.09 ( 96.98)	Acc@5 100.00 ( 99.98)
Epoch: [60][110/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.5552e-02 (8.5455e-02)	Acc@1  96.88 ( 96.99)	Acc@5 100.00 ( 99.98)
Epoch: [60][120/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.6956e-02 (8.5881e-02)	Acc@1  98.44 ( 96.97)	Acc@5 100.00 ( 99.98)
Epoch: [60][130/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.3506e-02 (8.6923e-02)	Acc@1  96.88 ( 96.89)	Acc@5 100.00 ( 99.98)
Epoch: [60][140/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.7341e-02 (8.7493e-02)	Acc@1  96.09 ( 96.87)	Acc@5 100.00 ( 99.98)
Epoch: [60][150/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.8938e-02 (8.7309e-02)	Acc@1  97.66 ( 96.90)	Acc@5 100.00 ( 99.98)
Epoch: [60][160/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.2632e-02 (8.7122e-02)	Acc@1  97.66 ( 96.92)	Acc@5 100.00 ( 99.98)
Epoch: [60][170/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.1787e-02 (8.6480e-02)	Acc@1  97.66 ( 96.96)	Acc@5 100.00 ( 99.98)
Epoch: [60][180/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.1787e-02 (8.6641e-02)	Acc@1  96.09 ( 96.97)	Acc@5 100.00 ( 99.98)
Epoch: [60][190/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.2286e-02 (8.6089e-02)	Acc@1  98.44 ( 97.01)	Acc@5 100.00 ( 99.98)
Epoch: [60][200/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.3721e-02 (8.6002e-02)	Acc@1  97.66 ( 97.01)	Acc@5 100.00 ( 99.98)
Epoch: [60][210/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0741e-02 (8.4753e-02)	Acc@1  99.22 ( 97.05)	Acc@5 100.00 ( 99.99)
Epoch: [60][220/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.6111e-02 (8.4390e-02)	Acc@1  96.88 ( 97.07)	Acc@5 100.00 ( 99.99)
Epoch: [60][230/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.5898e-02 (8.4073e-02)	Acc@1  99.22 ( 97.07)	Acc@5 100.00 ( 99.99)
Epoch: [60][240/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.3660e-02 (8.4208e-02)	Acc@1  97.66 ( 97.08)	Acc@5 100.00 ( 99.99)
Epoch: [60][250/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.6772e-02 (8.3520e-02)	Acc@1  96.09 ( 97.10)	Acc@5 100.00 ( 99.99)
Epoch: [60][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.9172e-02 (8.4074e-02)	Acc@1  97.66 ( 97.09)	Acc@5 100.00 ( 99.99)
Epoch: [60][270/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1591e-01 (8.3985e-02)	Acc@1  95.31 ( 97.11)	Acc@5 100.00 ( 99.99)
Epoch: [60][280/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.1553e-02 (8.4633e-02)	Acc@1  96.09 ( 97.07)	Acc@5 100.00 ( 99.99)
Epoch: [60][290/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.7871e-02 (8.4621e-02)	Acc@1  97.66 ( 97.06)	Acc@5 100.00 ( 99.99)
Epoch: [60][300/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0474e-01 (8.4803e-02)	Acc@1  96.09 ( 97.07)	Acc@5 100.00 ( 99.99)
Epoch: [60][310/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.9915e-02 (8.4910e-02)	Acc@1  97.66 ( 97.05)	Acc@5 100.00 ( 99.99)
Epoch: [60][320/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2744e-01 (8.4609e-02)	Acc@1  96.88 ( 97.07)	Acc@5 100.00 ( 99.99)
Epoch: [60][330/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.0632e-01 (8.4237e-02)	Acc@1  96.88 ( 97.09)	Acc@5 100.00 ( 99.99)
Epoch: [60][340/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.2341e-01 (8.4156e-02)	Acc@1  93.75 ( 97.09)	Acc@5 100.00 ( 99.99)
Epoch: [60][350/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.001)	Loss 9.1248e-02 (8.4047e-02)	Acc@1  96.88 ( 97.09)	Acc@5 100.00 ( 99.99)
Epoch: [60][360/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.8523e-02 (8.3899e-02)	Acc@1  99.22 ( 97.08)	Acc@5 100.00 ( 99.99)
Epoch: [60][370/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 8.1848e-02 (8.4088e-02)	Acc@1  97.66 ( 97.08)	Acc@5 100.00 ( 99.99)
Epoch: [60][380/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.2327e-02 (8.3534e-02)	Acc@1  97.66 ( 97.11)	Acc@5 100.00 ( 99.99)
Epoch: [60][390/391]	Time  0.027 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.1626e-02 (8.3740e-02)	Acc@1 100.00 ( 97.11)	Acc@5 100.00 ( 99.99)
## e[60] optimizer.zero_grad (sum) time: 0.24834203720092773
## e[60]       loss.backward (sum) time: 4.232807874679565
## e[60]      optimizer.step (sum) time: 1.717780351638794
## epoch[60] training(only) time: 15.910844802856445
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 2.6636e-01 (2.6636e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.019 ( 0.034)	Loss 3.3813e-01 (2.8203e-01)	Acc@1  89.00 ( 91.09)	Acc@5  99.00 ( 99.55)
Test: [ 20/100]	Time  0.024 ( 0.028)	Loss 4.0112e-01 (3.1179e-01)	Acc@1  90.00 ( 91.10)	Acc@5 100.00 ( 99.52)
Test: [ 30/100]	Time  0.018 ( 0.025)	Loss 4.6240e-01 (3.2319e-01)	Acc@1  85.00 ( 90.74)	Acc@5 100.00 ( 99.61)
Test: [ 40/100]	Time  0.017 ( 0.023)	Loss 4.1064e-01 (3.2362e-01)	Acc@1  87.00 ( 90.80)	Acc@5  99.00 ( 99.61)
Test: [ 50/100]	Time  0.016 ( 0.022)	Loss 2.2729e-01 (3.1621e-01)	Acc@1  93.00 ( 90.88)	Acc@5 100.00 ( 99.63)
Test: [ 60/100]	Time  0.019 ( 0.022)	Loss 3.6475e-01 (3.1540e-01)	Acc@1  91.00 ( 90.84)	Acc@5 100.00 ( 99.67)
Test: [ 70/100]	Time  0.022 ( 0.022)	Loss 4.5605e-01 (3.1196e-01)	Acc@1  87.00 ( 90.82)	Acc@5 100.00 ( 99.68)
Test: [ 80/100]	Time  0.022 ( 0.022)	Loss 1.2561e-01 (3.1691e-01)	Acc@1  94.00 ( 90.64)	Acc@5 100.00 ( 99.67)
Test: [ 90/100]	Time  0.023 ( 0.021)	Loss 2.6929e-01 (3.1604e-01)	Acc@1  92.00 ( 90.69)	Acc@5 100.00 ( 99.68)
 * Acc@1 90.780 Acc@5 99.700
### epoch[60] execution time: 18.17229986190796
EPOCH 61
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [61][  0/391]	Time  0.207 ( 0.207)	Data  0.166 ( 0.166)	Loss 7.1655e-02 (7.1655e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [61][ 10/391]	Time  0.040 ( 0.055)	Data  0.001 ( 0.016)	Loss 7.3425e-02 (8.9866e-02)	Acc@1  98.44 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [61][ 20/391]	Time  0.038 ( 0.048)	Data  0.001 ( 0.009)	Loss 1.3733e-01 (8.7020e-02)	Acc@1  95.31 ( 97.14)	Acc@5 100.00 (100.00)
Epoch: [61][ 30/391]	Time  0.038 ( 0.045)	Data  0.001 ( 0.006)	Loss 1.7065e-01 (8.8869e-02)	Acc@1  94.53 ( 97.23)	Acc@5 100.00 (100.00)
Epoch: [61][ 40/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.005)	Loss 6.1249e-02 (8.6077e-02)	Acc@1  96.88 ( 97.20)	Acc@5 100.00 (100.00)
Epoch: [61][ 50/391]	Time  0.035 ( 0.043)	Data  0.001 ( 0.004)	Loss 9.7900e-02 (8.2874e-02)	Acc@1  96.88 ( 97.29)	Acc@5 100.00 (100.00)
Epoch: [61][ 60/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.004)	Loss 5.0201e-02 (8.2497e-02)	Acc@1  97.66 ( 97.26)	Acc@5 100.00 ( 99.99)
Epoch: [61][ 70/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.3488e-02 (8.4684e-02)	Acc@1  99.22 ( 97.13)	Acc@5 100.00 ( 99.98)
Epoch: [61][ 80/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 8.2886e-02 (8.6544e-02)	Acc@1  97.66 ( 97.09)	Acc@5 100.00 ( 99.97)
Epoch: [61][ 90/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.003)	Loss 4.4403e-02 (8.6997e-02)	Acc@1  99.22 ( 97.12)	Acc@5 100.00 ( 99.97)
Epoch: [61][100/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.003)	Loss 9.2224e-02 (8.7369e-02)	Acc@1  98.44 ( 97.11)	Acc@5 100.00 ( 99.98)
Epoch: [61][110/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.0139e-02 (8.7487e-02)	Acc@1  96.88 ( 97.08)	Acc@5 100.00 ( 99.98)
Epoch: [61][120/391]	Time  0.034 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.8806e-02 (8.5191e-02)	Acc@1  97.66 ( 97.18)	Acc@5 100.00 ( 99.98)
Epoch: [61][130/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.7261e-02 (8.5316e-02)	Acc@1  98.44 ( 97.17)	Acc@5 100.00 ( 99.98)
Epoch: [61][140/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.0129e-02 (8.4442e-02)	Acc@1  98.44 ( 97.19)	Acc@5 100.00 ( 99.98)
Epoch: [61][150/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2549e-01 (8.4337e-02)	Acc@1  96.88 ( 97.19)	Acc@5 100.00 ( 99.98)
Epoch: [61][160/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.7656e-02 (8.4169e-02)	Acc@1  96.09 ( 97.21)	Acc@5 100.00 ( 99.99)
Epoch: [61][170/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.0750e-02 (8.4779e-02)	Acc@1  97.66 ( 97.21)	Acc@5 100.00 ( 99.99)
Epoch: [61][180/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2512e-01 (8.5081e-02)	Acc@1  95.31 ( 97.20)	Acc@5 100.00 ( 99.99)
Epoch: [61][190/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 6.3354e-02 (8.4900e-02)	Acc@1  98.44 ( 97.19)	Acc@5 100.00 ( 99.99)
Epoch: [61][200/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 3.3997e-02 (8.5036e-02)	Acc@1 100.00 ( 97.18)	Acc@5 100.00 ( 99.99)
Epoch: [61][210/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 4.3854e-02 (8.5406e-02)	Acc@1  99.22 ( 97.18)	Acc@5 100.00 ( 99.99)
Epoch: [61][220/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 7.5012e-02 (8.4790e-02)	Acc@1  99.22 ( 97.21)	Acc@5 100.00 ( 99.99)
Epoch: [61][230/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 7.9163e-02 (8.4692e-02)	Acc@1  97.66 ( 97.23)	Acc@5 100.00 ( 99.99)
Epoch: [61][240/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 5.5023e-02 (8.4136e-02)	Acc@1  98.44 ( 97.23)	Acc@5 100.00 ( 99.99)
Epoch: [61][250/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 5.7922e-02 (8.3727e-02)	Acc@1  98.44 ( 97.25)	Acc@5 100.00 ( 99.99)
Epoch: [61][260/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.0449e-01 (8.4028e-02)	Acc@1  96.09 ( 97.23)	Acc@5 100.00 ( 99.99)
Epoch: [61][270/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 3.9948e-02 (8.3831e-02)	Acc@1  99.22 ( 97.25)	Acc@5 100.00 ( 99.99)
Epoch: [61][280/391]	Time  0.043 ( 0.040)	Data  0.001 ( 0.002)	Loss 6.9031e-02 (8.3424e-02)	Acc@1  98.44 ( 97.26)	Acc@5 100.00 ( 99.99)
Epoch: [61][290/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.2266e-02 (8.2810e-02)	Acc@1  97.66 ( 97.28)	Acc@5 100.00 ( 99.99)
Epoch: [61][300/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.5674e-02 (8.2559e-02)	Acc@1  97.66 ( 97.27)	Acc@5 100.00 ( 99.99)
Epoch: [61][310/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.9468e-02 (8.2963e-02)	Acc@1  96.09 ( 97.24)	Acc@5 100.00 ( 99.99)
Epoch: [61][320/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.1151e-01 (8.3044e-02)	Acc@1  95.31 ( 97.22)	Acc@5 100.00 ( 99.99)
Epoch: [61][330/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.8441e-02 (8.3091e-02)	Acc@1  98.44 ( 97.22)	Acc@5 100.00 ( 99.99)
Epoch: [61][340/391]	Time  0.045 ( 0.040)	Data  0.001 ( 0.001)	Loss 9.7595e-02 (8.3075e-02)	Acc@1  96.09 ( 97.23)	Acc@5 100.00 ( 99.99)
Epoch: [61][350/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.5002e-02 (8.2770e-02)	Acc@1  98.44 ( 97.25)	Acc@5 100.00 ( 99.99)
Epoch: [61][360/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.1208e-02 (8.2731e-02)	Acc@1  98.44 ( 97.25)	Acc@5 100.00 ( 99.99)
Epoch: [61][370/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.7820e-02 (8.2269e-02)	Acc@1  96.09 ( 97.27)	Acc@5 100.00 ( 99.99)
Epoch: [61][380/391]	Time  0.043 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.6050e-02 (8.2367e-02)	Acc@1  97.66 ( 97.25)	Acc@5 100.00 ( 99.99)
Epoch: [61][390/391]	Time  0.027 ( 0.040)	Data  0.001 ( 0.001)	Loss 8.4167e-02 (8.1773e-02)	Acc@1  98.75 ( 97.28)	Acc@5 100.00 ( 99.99)
## e[61] optimizer.zero_grad (sum) time: 0.24902582168579102
## e[61]       loss.backward (sum) time: 4.148980379104614
## e[61]      optimizer.step (sum) time: 1.742922067642212
## epoch[61] training(only) time: 15.879656076431274
# Switched to evaluate mode...
Test: [  0/100]	Time  0.166 ( 0.166)	Loss 2.6196e-01 (2.6196e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.020 ( 0.032)	Loss 3.4717e-01 (2.8307e-01)	Acc@1  89.00 ( 90.91)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.024 ( 0.027)	Loss 4.0137e-01 (3.1224e-01)	Acc@1  89.00 ( 90.76)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.017 ( 0.024)	Loss 4.3506e-01 (3.2098e-01)	Acc@1  86.00 ( 90.61)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.020 ( 0.023)	Loss 4.2236e-01 (3.2229e-01)	Acc@1  87.00 ( 90.59)	Acc@5  99.00 ( 99.63)
Test: [ 50/100]	Time  0.020 ( 0.023)	Loss 2.0532e-01 (3.1419e-01)	Acc@1  93.00 ( 90.71)	Acc@5 100.00 ( 99.63)
Test: [ 60/100]	Time  0.023 ( 0.022)	Loss 3.6646e-01 (3.1410e-01)	Acc@1  91.00 ( 90.64)	Acc@5 100.00 ( 99.67)
Test: [ 70/100]	Time  0.020 ( 0.022)	Loss 4.4678e-01 (3.1074e-01)	Acc@1  86.00 ( 90.63)	Acc@5 100.00 ( 99.68)
Test: [ 80/100]	Time  0.018 ( 0.021)	Loss 1.0590e-01 (3.1490e-01)	Acc@1  94.00 ( 90.51)	Acc@5 100.00 ( 99.67)
Test: [ 90/100]	Time  0.016 ( 0.021)	Loss 2.5830e-01 (3.1465e-01)	Acc@1  93.00 ( 90.56)	Acc@5 100.00 ( 99.68)
 * Acc@1 90.700 Acc@5 99.680
### epoch[61] execution time: 18.06507658958435
EPOCH 62
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [62][  0/391]	Time  0.214 ( 0.214)	Data  0.173 ( 0.173)	Loss 4.2389e-02 (4.2389e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [62][ 10/391]	Time  0.038 ( 0.056)	Data  0.001 ( 0.016)	Loss 5.0018e-02 (7.3070e-02)	Acc@1  99.22 ( 97.87)	Acc@5 100.00 (100.00)
Epoch: [62][ 20/391]	Time  0.038 ( 0.048)	Data  0.001 ( 0.009)	Loss 5.0171e-02 (7.3517e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [62][ 30/391]	Time  0.040 ( 0.045)	Data  0.001 ( 0.007)	Loss 5.4718e-02 (7.7600e-02)	Acc@1  97.66 ( 97.56)	Acc@5 100.00 ( 99.97)
Epoch: [62][ 40/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.005)	Loss 8.5449e-02 (7.8473e-02)	Acc@1  97.66 ( 97.47)	Acc@5 100.00 ( 99.98)
Epoch: [62][ 50/391]	Time  0.039 ( 0.043)	Data  0.000 ( 0.004)	Loss 6.4270e-02 (7.9040e-02)	Acc@1  97.66 ( 97.44)	Acc@5 100.00 ( 99.98)
Epoch: [62][ 60/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.004)	Loss 8.6670e-02 (7.8941e-02)	Acc@1  95.31 ( 97.41)	Acc@5 100.00 ( 99.99)
Epoch: [62][ 70/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.1289e-02 (7.7906e-02)	Acc@1  97.66 ( 97.41)	Acc@5 100.00 ( 99.99)
Epoch: [62][ 80/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.6772e-02 (8.2123e-02)	Acc@1  97.66 ( 97.26)	Acc@5 100.00 ( 99.99)
Epoch: [62][ 90/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.003)	Loss 5.2795e-02 (8.0501e-02)	Acc@1  98.44 ( 97.32)	Acc@5 100.00 ( 99.99)
Epoch: [62][100/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.003)	Loss 1.3391e-01 (8.1779e-02)	Acc@1  93.75 ( 97.28)	Acc@5 100.00 ( 99.99)
Epoch: [62][110/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0730e-01 (8.3070e-02)	Acc@1  96.88 ( 97.19)	Acc@5 100.00 ( 99.99)
Epoch: [62][120/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.3914e-02 (8.2468e-02)	Acc@1  96.88 ( 97.22)	Acc@5 100.00 ( 99.99)
Epoch: [62][130/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3579e-02 (8.1343e-02)	Acc@1  98.44 ( 97.25)	Acc@5 100.00 ( 99.99)
Epoch: [62][140/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2979e-02 (8.1208e-02)	Acc@1  97.66 ( 97.24)	Acc@5 100.00 ( 99.99)
Epoch: [62][150/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.8501e-02 (8.1066e-02)	Acc@1  98.44 ( 97.28)	Acc@5 100.00 ( 99.99)
Epoch: [62][160/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 6.8115e-02 (8.0698e-02)	Acc@1  98.44 ( 97.28)	Acc@5 100.00 ( 99.99)
Epoch: [62][170/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.1676e-01 (8.0829e-02)	Acc@1  95.31 ( 97.25)	Acc@5 100.00 ( 99.99)
Epoch: [62][180/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.3623e-01 (8.0046e-02)	Acc@1  93.75 ( 97.29)	Acc@5 100.00 ( 99.99)
Epoch: [62][190/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 6.4941e-02 (7.9669e-02)	Acc@1  99.22 ( 97.32)	Acc@5 100.00 ( 99.99)
Epoch: [62][200/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 6.3721e-02 (7.9085e-02)	Acc@1  98.44 ( 97.35)	Acc@5 100.00 ( 99.99)
Epoch: [62][210/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 4.1656e-02 (7.9224e-02)	Acc@1  99.22 ( 97.37)	Acc@5 100.00 ( 99.99)
Epoch: [62][220/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 8.5754e-02 (7.9534e-02)	Acc@1  96.88 ( 97.35)	Acc@5 100.00 ( 99.99)
Epoch: [62][230/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 8.1360e-02 (7.9986e-02)	Acc@1  96.88 ( 97.34)	Acc@5 100.00 ( 99.99)
Epoch: [62][240/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.002)	Loss 5.0140e-02 (7.9937e-02)	Acc@1  99.22 ( 97.35)	Acc@5 100.00 ( 99.99)
Epoch: [62][250/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.1926e-01 (8.0008e-02)	Acc@1  95.31 ( 97.35)	Acc@5 100.00 ( 99.99)
Epoch: [62][260/391]	Time  0.044 ( 0.040)	Data  0.001 ( 0.002)	Loss 6.9885e-02 (7.9529e-02)	Acc@1  97.66 ( 97.37)	Acc@5 100.00 ( 99.99)
Epoch: [62][270/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.002)	Loss 7.9651e-02 (7.9397e-02)	Acc@1  97.66 ( 97.39)	Acc@5 100.00 ( 99.99)
Epoch: [62][280/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 8.7769e-02 (7.9741e-02)	Acc@1  97.66 ( 97.38)	Acc@5 100.00 ( 99.99)
Epoch: [62][290/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.3147e-01 (7.9782e-02)	Acc@1  94.53 ( 97.36)	Acc@5 100.00 ( 99.99)
Epoch: [62][300/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 4.3549e-02 (7.9841e-02)	Acc@1  98.44 ( 97.38)	Acc@5 100.00 ( 99.99)
Epoch: [62][310/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 5.3894e-02 (7.9804e-02)	Acc@1  98.44 ( 97.39)	Acc@5 100.00 ( 99.99)
Epoch: [62][320/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.1542e-01 (7.9850e-02)	Acc@1  96.09 ( 97.39)	Acc@5 100.00 ( 99.99)
Epoch: [62][330/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.6885e-02 (7.9564e-02)	Acc@1  98.44 ( 97.40)	Acc@5 100.00 ( 99.99)
Epoch: [62][340/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.0873e-02 (7.9872e-02)	Acc@1  98.44 ( 97.38)	Acc@5 100.00 ( 99.99)
Epoch: [62][350/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.5115e-02 (8.0176e-02)	Acc@1  98.44 ( 97.37)	Acc@5 100.00 ( 99.99)
Epoch: [62][360/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.2074e-02 (7.9743e-02)	Acc@1 100.00 ( 97.39)	Acc@5 100.00 ( 99.99)
Epoch: [62][370/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.2177e-01 (8.0362e-02)	Acc@1  94.53 ( 97.36)	Acc@5 100.00 ( 99.99)
Epoch: [62][380/391]	Time  0.043 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.0984e-02 (8.0537e-02)	Acc@1  97.66 ( 97.34)	Acc@5 100.00 ( 99.99)
Epoch: [62][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.7688e-01 (8.0634e-02)	Acc@1  93.75 ( 97.35)	Acc@5 100.00 ( 99.99)
## e[62] optimizer.zero_grad (sum) time: 0.2506749629974365
## e[62]       loss.backward (sum) time: 4.150521278381348
## e[62]      optimizer.step (sum) time: 1.7318565845489502
## epoch[62] training(only) time: 15.8080894947052
# Switched to evaluate mode...
Test: [  0/100]	Time  0.167 ( 0.167)	Loss 2.5806e-01 (2.5806e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.021 ( 0.034)	Loss 3.4033e-01 (2.8637e-01)	Acc@1  89.00 ( 91.45)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.018 ( 0.028)	Loss 3.9893e-01 (3.1600e-01)	Acc@1  89.00 ( 91.05)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.020 ( 0.025)	Loss 4.8193e-01 (3.2685e-01)	Acc@1  86.00 ( 90.84)	Acc@5 100.00 ( 99.61)
Test: [ 40/100]	Time  0.017 ( 0.024)	Loss 4.1504e-01 (3.2870e-01)	Acc@1  89.00 ( 90.90)	Acc@5  99.00 ( 99.61)
Test: [ 50/100]	Time  0.016 ( 0.023)	Loss 2.2717e-01 (3.2014e-01)	Acc@1  93.00 ( 90.84)	Acc@5 100.00 ( 99.61)
Test: [ 60/100]	Time  0.020 ( 0.022)	Loss 3.7378e-01 (3.2003e-01)	Acc@1  91.00 ( 90.77)	Acc@5 100.00 ( 99.67)
Test: [ 70/100]	Time  0.020 ( 0.022)	Loss 4.2529e-01 (3.1507e-01)	Acc@1  88.00 ( 90.76)	Acc@5 100.00 ( 99.68)
Test: [ 80/100]	Time  0.020 ( 0.022)	Loss 1.1133e-01 (3.1946e-01)	Acc@1  94.00 ( 90.62)	Acc@5 100.00 ( 99.67)
Test: [ 90/100]	Time  0.024 ( 0.022)	Loss 2.6416e-01 (3.1916e-01)	Acc@1  93.00 ( 90.70)	Acc@5 100.00 ( 99.69)
 * Acc@1 90.800 Acc@5 99.710
### epoch[62] execution time: 18.09679388999939
EPOCH 63
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [63][  0/391]	Time  0.216 ( 0.216)	Data  0.166 ( 0.166)	Loss 5.0842e-02 (5.0842e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [63][ 10/391]	Time  0.038 ( 0.056)	Data  0.001 ( 0.016)	Loss 4.9103e-02 (6.6600e-02)	Acc@1  99.22 ( 97.73)	Acc@5 100.00 (100.00)
Epoch: [63][ 20/391]	Time  0.041 ( 0.048)	Data  0.001 ( 0.009)	Loss 8.7585e-02 (8.1588e-02)	Acc@1  96.09 ( 96.95)	Acc@5 100.00 (100.00)
Epoch: [63][ 30/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.006)	Loss 8.5693e-02 (8.1601e-02)	Acc@1  96.09 ( 97.13)	Acc@5 100.00 (100.00)
Epoch: [63][ 40/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.005)	Loss 8.7769e-02 (8.0225e-02)	Acc@1  97.66 ( 97.10)	Acc@5 100.00 (100.00)
Epoch: [63][ 50/391]	Time  0.043 ( 0.044)	Data  0.001 ( 0.004)	Loss 3.6621e-02 (7.5979e-02)	Acc@1  99.22 ( 97.35)	Acc@5 100.00 (100.00)
Epoch: [63][ 60/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.1023e-01 (7.6621e-02)	Acc@1  95.31 ( 97.36)	Acc@5 100.00 (100.00)
Epoch: [63][ 70/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 9.9365e-02 (7.7657e-02)	Acc@1  96.09 ( 97.28)	Acc@5 100.00 (100.00)
Epoch: [63][ 80/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.9092e-02 (7.8477e-02)	Acc@1  96.88 ( 97.30)	Acc@5 100.00 (100.00)
Epoch: [63][ 90/391]	Time  0.035 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.5939e-02 (7.8154e-02)	Acc@1  97.66 ( 97.34)	Acc@5 100.00 (100.00)
Epoch: [63][100/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.7952e-02 (7.8480e-02)	Acc@1  97.66 ( 97.35)	Acc@5 100.00 (100.00)
Epoch: [63][110/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.1919e-02 (7.8748e-02)	Acc@1  97.66 ( 97.38)	Acc@5 100.00 (100.00)
Epoch: [63][120/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2765e-02 (7.8615e-02)	Acc@1  98.44 ( 97.42)	Acc@5 100.00 (100.00)
Epoch: [63][130/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.8848e-01 (8.0244e-02)	Acc@1  92.19 ( 97.32)	Acc@5 100.00 (100.00)
Epoch: [63][140/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.5693e-02 (8.0692e-02)	Acc@1  97.66 ( 97.29)	Acc@5 100.00 ( 99.99)
Epoch: [63][150/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2158e-01 (8.0580e-02)	Acc@1  95.31 ( 97.28)	Acc@5 100.00 ( 99.99)
Epoch: [63][160/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.5593e-02 (8.0317e-02)	Acc@1 100.00 ( 97.33)	Acc@5 100.00 (100.00)
Epoch: [63][170/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1121e-01 (8.0943e-02)	Acc@1  96.88 ( 97.27)	Acc@5 100.00 (100.00)
Epoch: [63][180/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0232e-02 (8.0438e-02)	Acc@1  98.44 ( 97.30)	Acc@5 100.00 (100.00)
Epoch: [63][190/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0894e-02 (8.0313e-02)	Acc@1  98.44 ( 97.32)	Acc@5 100.00 (100.00)
Epoch: [63][200/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.6487e-02 (7.9770e-02)	Acc@1  96.88 ( 97.34)	Acc@5 100.00 (100.00)
Epoch: [63][210/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.5650e-02 (7.9702e-02)	Acc@1 100.00 ( 97.35)	Acc@5 100.00 (100.00)
Epoch: [63][220/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3098e-01 (7.9849e-02)	Acc@1  94.53 ( 97.36)	Acc@5 100.00 (100.00)
Epoch: [63][230/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9957e-02 (7.9707e-02)	Acc@1  99.22 ( 97.38)	Acc@5 100.00 (100.00)
Epoch: [63][240/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 4.4159e-02 (7.9828e-02)	Acc@1  99.22 ( 97.36)	Acc@5 100.00 (100.00)
Epoch: [63][250/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 7.1838e-02 (7.9608e-02)	Acc@1  97.66 ( 97.38)	Acc@5 100.00 (100.00)
Epoch: [63][260/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 6.4880e-02 (8.0142e-02)	Acc@1  96.88 ( 97.35)	Acc@5 100.00 ( 99.99)
Epoch: [63][270/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 4.1840e-02 (7.9823e-02)	Acc@1 100.00 ( 97.35)	Acc@5 100.00 ( 99.99)
Epoch: [63][280/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.0649e-02 (7.9645e-02)	Acc@1  98.44 ( 97.36)	Acc@5 100.00 ( 99.99)
Epoch: [63][290/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.1249e-02 (7.9754e-02)	Acc@1  97.66 ( 97.36)	Acc@5 100.00 ( 99.99)
Epoch: [63][300/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 9.8145e-02 (7.9403e-02)	Acc@1  96.88 ( 97.38)	Acc@5 100.00 ( 99.99)
Epoch: [63][310/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.9265e-02 (7.9485e-02)	Acc@1  99.22 ( 97.37)	Acc@5 100.00 ( 99.99)
Epoch: [63][320/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.2561e-02 (7.9324e-02)	Acc@1  98.44 ( 97.37)	Acc@5 100.00 ( 99.99)
Epoch: [63][330/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.2646e-01 (7.9272e-02)	Acc@1  95.31 ( 97.38)	Acc@5 100.00 ( 99.99)
Epoch: [63][340/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.5166e-02 (7.9182e-02)	Acc@1  98.44 ( 97.38)	Acc@5 100.00 ( 99.99)
Epoch: [63][350/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.5878e-02 (7.8670e-02)	Acc@1  98.44 ( 97.40)	Acc@5 100.00 ( 99.99)
Epoch: [63][360/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.8391e-02 (7.8136e-02)	Acc@1  99.22 ( 97.42)	Acc@5 100.00 ( 99.99)
Epoch: [63][370/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 9.9426e-02 (7.8286e-02)	Acc@1  97.66 ( 97.41)	Acc@5 100.00 ( 99.99)
Epoch: [63][380/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.4219e-02 (7.8202e-02)	Acc@1  97.66 ( 97.41)	Acc@5 100.00 ( 99.99)
Epoch: [63][390/391]	Time  0.030 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.6064e-01 (7.8149e-02)	Acc@1  93.75 ( 97.41)	Acc@5  98.75 ( 99.99)
## e[63] optimizer.zero_grad (sum) time: 0.24950885772705078
## e[63]       loss.backward (sum) time: 4.167611598968506
## e[63]      optimizer.step (sum) time: 1.7503652572631836
## epoch[63] training(only) time: 15.863084554672241
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 2.5146e-01 (2.5146e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.019 ( 0.033)	Loss 3.4180e-01 (2.8020e-01)	Acc@1  89.00 ( 91.36)	Acc@5  99.00 ( 99.55)
Test: [ 20/100]	Time  0.021 ( 0.027)	Loss 3.9404e-01 (3.0949e-01)	Acc@1  87.00 ( 91.05)	Acc@5 100.00 ( 99.52)
Test: [ 30/100]	Time  0.023 ( 0.024)	Loss 4.4824e-01 (3.1965e-01)	Acc@1  86.00 ( 90.90)	Acc@5 100.00 ( 99.61)
Test: [ 40/100]	Time  0.019 ( 0.023)	Loss 4.0137e-01 (3.2113e-01)	Acc@1  89.00 ( 90.80)	Acc@5  99.00 ( 99.61)
Test: [ 50/100]	Time  0.021 ( 0.022)	Loss 2.2876e-01 (3.1394e-01)	Acc@1  93.00 ( 90.88)	Acc@5 100.00 ( 99.61)
Test: [ 60/100]	Time  0.020 ( 0.021)	Loss 3.7524e-01 (3.1426e-01)	Acc@1  91.00 ( 90.75)	Acc@5 100.00 ( 99.66)
Test: [ 70/100]	Time  0.021 ( 0.021)	Loss 4.1187e-01 (3.0993e-01)	Acc@1  87.00 ( 90.73)	Acc@5 100.00 ( 99.66)
Test: [ 80/100]	Time  0.022 ( 0.021)	Loss 1.2000e-01 (3.1441e-01)	Acc@1  94.00 ( 90.57)	Acc@5 100.00 ( 99.65)
Test: [ 90/100]	Time  0.022 ( 0.021)	Loss 2.7148e-01 (3.1395e-01)	Acc@1  93.00 ( 90.64)	Acc@5 100.00 ( 99.67)
 * Acc@1 90.760 Acc@5 99.680
### epoch[63] execution time: 18.046791315078735
EPOCH 64
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [64][  0/391]	Time  0.210 ( 0.210)	Data  0.170 ( 0.170)	Loss 8.4656e-02 (8.4656e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [64][ 10/391]	Time  0.038 ( 0.056)	Data  0.001 ( 0.016)	Loss 1.0120e-01 (1.0251e-01)	Acc@1  94.53 ( 96.24)	Acc@5 100.00 ( 99.93)
Epoch: [64][ 20/391]	Time  0.040 ( 0.049)	Data  0.001 ( 0.009)	Loss 1.2744e-01 (8.5661e-02)	Acc@1  96.09 ( 96.91)	Acc@5 100.00 ( 99.96)
Epoch: [64][ 30/391]	Time  0.040 ( 0.046)	Data  0.001 ( 0.006)	Loss 1.2115e-01 (8.3918e-02)	Acc@1  95.31 ( 96.95)	Acc@5  99.22 ( 99.95)
Epoch: [64][ 40/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.005)	Loss 9.5642e-02 (8.3382e-02)	Acc@1  96.88 ( 97.08)	Acc@5 100.00 ( 99.96)
Epoch: [64][ 50/391]	Time  0.037 ( 0.043)	Data  0.001 ( 0.004)	Loss 4.0894e-02 (8.2588e-02)	Acc@1 100.00 ( 97.18)	Acc@5 100.00 ( 99.97)
Epoch: [64][ 60/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 4.9683e-02 (8.1112e-02)	Acc@1  99.22 ( 97.23)	Acc@5 100.00 ( 99.97)
Epoch: [64][ 70/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.9185e-02 (8.0311e-02)	Acc@1  99.22 ( 97.30)	Acc@5 100.00 ( 99.97)
Epoch: [64][ 80/391]	Time  0.043 ( 0.042)	Data  0.006 ( 0.003)	Loss 1.3354e-01 (7.9931e-02)	Acc@1  94.53 ( 97.28)	Acc@5 100.00 ( 99.97)
Epoch: [64][ 90/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.003)	Loss 8.7036e-02 (8.0997e-02)	Acc@1  96.88 ( 97.26)	Acc@5 100.00 ( 99.97)
Epoch: [64][100/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.1005e-01 (8.1944e-02)	Acc@1  96.09 ( 97.20)	Acc@5 100.00 ( 99.98)
Epoch: [64][110/391]	Time  0.045 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.8696e-02 (8.0933e-02)	Acc@1  99.22 ( 97.23)	Acc@5 100.00 ( 99.98)
Epoch: [64][120/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.9205e-02 (7.9660e-02)	Acc@1 100.00 ( 97.29)	Acc@5 100.00 ( 99.98)
Epoch: [64][130/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.6497e-02 (8.0047e-02)	Acc@1  97.66 ( 97.26)	Acc@5 100.00 ( 99.98)
Epoch: [64][140/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.8623e-02 (8.0033e-02)	Acc@1  97.66 ( 97.25)	Acc@5 100.00 ( 99.98)
Epoch: [64][150/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.0688e-02 (7.9379e-02)	Acc@1  97.66 ( 97.29)	Acc@5 100.00 ( 99.98)
Epoch: [64][160/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2976e-01 (7.9127e-02)	Acc@1  95.31 ( 97.32)	Acc@5 100.00 ( 99.99)
Epoch: [64][170/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4758e-01 (7.9202e-02)	Acc@1  96.09 ( 97.32)	Acc@5 100.00 ( 99.99)
Epoch: [64][180/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0809e-01 (8.0221e-02)	Acc@1  96.09 ( 97.29)	Acc@5 100.00 ( 99.98)
Epoch: [64][190/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.4779e-02 (8.0163e-02)	Acc@1  97.66 ( 97.29)	Acc@5 100.00 ( 99.98)
Epoch: [64][200/391]	Time  0.034 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0873e-02 (8.0032e-02)	Acc@1  99.22 ( 97.29)	Acc@5 100.00 ( 99.98)
Epoch: [64][210/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.3933e-02 (8.0139e-02)	Acc@1  96.88 ( 97.28)	Acc@5 100.00 ( 99.99)
Epoch: [64][220/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.5642e-02 (8.0072e-02)	Acc@1  97.66 ( 97.30)	Acc@5 100.00 ( 99.98)
Epoch: [64][230/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2585e-01 (8.0367e-02)	Acc@1  95.31 ( 97.29)	Acc@5 100.00 ( 99.98)
Epoch: [64][240/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0468e-01 (8.0438e-02)	Acc@1  95.31 ( 97.27)	Acc@5 100.00 ( 99.98)
Epoch: [64][250/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.7402e-02 (8.0051e-02)	Acc@1  96.88 ( 97.28)	Acc@5 100.00 ( 99.98)
Epoch: [64][260/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.8848e-02 (7.9669e-02)	Acc@1  98.44 ( 97.30)	Acc@5 100.00 ( 99.99)
Epoch: [64][270/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 6.2500e-02 (7.9610e-02)	Acc@1  97.66 ( 97.30)	Acc@5 100.00 ( 99.99)
Epoch: [64][280/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.0187e-01 (7.9679e-02)	Acc@1  96.88 ( 97.30)	Acc@5 100.00 ( 99.98)
Epoch: [64][290/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 9.1919e-02 (7.9842e-02)	Acc@1  96.09 ( 97.29)	Acc@5 100.00 ( 99.98)
Epoch: [64][300/391]	Time  0.045 ( 0.040)	Data  0.001 ( 0.002)	Loss 8.6548e-02 (7.9687e-02)	Acc@1  96.09 ( 97.29)	Acc@5 100.00 ( 99.98)
Epoch: [64][310/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 9.9487e-02 (7.9798e-02)	Acc@1  96.09 ( 97.29)	Acc@5 100.00 ( 99.98)
Epoch: [64][320/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.6967e-02 (7.9468e-02)	Acc@1  98.44 ( 97.31)	Acc@5 100.00 ( 99.99)
Epoch: [64][330/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.001)	Loss 8.2214e-02 (7.8922e-02)	Acc@1  95.31 ( 97.32)	Acc@5 100.00 ( 99.99)
Epoch: [64][340/391]	Time  0.048 ( 0.040)	Data  0.002 ( 0.001)	Loss 6.2927e-02 (7.8786e-02)	Acc@1  96.88 ( 97.33)	Acc@5 100.00 ( 99.98)
Epoch: [64][350/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.7231e-02 (7.9021e-02)	Acc@1  99.22 ( 97.32)	Acc@5 100.00 ( 99.98)
Epoch: [64][360/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 8.5388e-02 (7.8702e-02)	Acc@1  97.66 ( 97.34)	Acc@5 100.00 ( 99.98)
Epoch: [64][370/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.8910e-02 (7.8864e-02)	Acc@1  99.22 ( 97.33)	Acc@5 100.00 ( 99.99)
Epoch: [64][380/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.7688e-02 (7.8591e-02)	Acc@1  98.44 ( 97.35)	Acc@5 100.00 ( 99.99)
Epoch: [64][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.9835e-02 (7.8705e-02)	Acc@1  97.50 ( 97.35)	Acc@5 100.00 ( 99.99)
## e[64] optimizer.zero_grad (sum) time: 0.2515828609466553
## e[64]       loss.backward (sum) time: 4.131759166717529
## e[64]      optimizer.step (sum) time: 1.7700350284576416
## epoch[64] training(only) time: 15.84276270866394
# Switched to evaluate mode...
Test: [  0/100]	Time  0.175 ( 0.175)	Loss 2.6611e-01 (2.6611e-01)	Acc@1  93.00 ( 93.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.018 ( 0.033)	Loss 3.3740e-01 (2.8044e-01)	Acc@1  89.00 ( 91.45)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.017 ( 0.027)	Loss 3.7866e-01 (3.0727e-01)	Acc@1  89.00 ( 91.24)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.016 ( 0.025)	Loss 4.5166e-01 (3.1710e-01)	Acc@1  87.00 ( 91.00)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.020 ( 0.023)	Loss 4.1089e-01 (3.1895e-01)	Acc@1  88.00 ( 91.02)	Acc@5  99.00 ( 99.63)
Test: [ 50/100]	Time  0.024 ( 0.023)	Loss 2.0105e-01 (3.1124e-01)	Acc@1  93.00 ( 91.08)	Acc@5 100.00 ( 99.65)
Test: [ 60/100]	Time  0.021 ( 0.022)	Loss 3.7231e-01 (3.1139e-01)	Acc@1  91.00 ( 90.92)	Acc@5 100.00 ( 99.69)
Test: [ 70/100]	Time  0.017 ( 0.022)	Loss 4.4385e-01 (3.0769e-01)	Acc@1  87.00 ( 90.86)	Acc@5 100.00 ( 99.69)
Test: [ 80/100]	Time  0.023 ( 0.022)	Loss 1.0345e-01 (3.1194e-01)	Acc@1  94.00 ( 90.69)	Acc@5 100.00 ( 99.69)
Test: [ 90/100]	Time  0.017 ( 0.021)	Loss 2.6343e-01 (3.1117e-01)	Acc@1  94.00 ( 90.78)	Acc@5 100.00 ( 99.69)
 * Acc@1 90.870 Acc@5 99.700
### epoch[64] execution time: 18.087205410003662
EPOCH 65
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [65][  0/391]	Time  0.230 ( 0.230)	Data  0.181 ( 0.181)	Loss 3.3752e-02 (3.3752e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [65][ 10/391]	Time  0.040 ( 0.058)	Data  0.001 ( 0.017)	Loss 9.1187e-02 (7.2959e-02)	Acc@1  95.31 ( 97.73)	Acc@5 100.00 ( 99.86)
Epoch: [65][ 20/391]	Time  0.038 ( 0.049)	Data  0.001 ( 0.010)	Loss 9.5703e-02 (7.7301e-02)	Acc@1  98.44 ( 97.66)	Acc@5 100.00 ( 99.93)
Epoch: [65][ 30/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.007)	Loss 1.4099e-01 (7.9943e-02)	Acc@1  94.53 ( 97.53)	Acc@5 100.00 ( 99.92)
Epoch: [65][ 40/391]	Time  0.038 ( 0.045)	Data  0.001 ( 0.005)	Loss 5.4565e-02 (7.9626e-02)	Acc@1  98.44 ( 97.56)	Acc@5 100.00 ( 99.94)
Epoch: [65][ 50/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.1261e-01 (7.7151e-02)	Acc@1  94.53 ( 97.59)	Acc@5 100.00 ( 99.94)
Epoch: [65][ 60/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.004)	Loss 7.3669e-02 (7.4628e-02)	Acc@1  96.88 ( 97.64)	Acc@5 100.00 ( 99.95)
Epoch: [65][ 70/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.8513e-02 (7.6560e-02)	Acc@1 100.00 ( 97.62)	Acc@5 100.00 ( 99.96)
Epoch: [65][ 80/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.7820e-02 (7.6683e-02)	Acc@1  98.44 ( 97.61)	Acc@5 100.00 ( 99.96)
Epoch: [65][ 90/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.9702e-02 (7.6271e-02)	Acc@1  96.88 ( 97.60)	Acc@5 100.00 ( 99.97)
Epoch: [65][100/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.3513e-01 (7.7030e-02)	Acc@1  94.53 ( 97.52)	Acc@5 100.00 ( 99.97)
Epoch: [65][110/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 7.0251e-02 (7.6466e-02)	Acc@1  98.44 ( 97.55)	Acc@5 100.00 ( 99.97)
Epoch: [65][120/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1145e-01 (7.5992e-02)	Acc@1  96.88 ( 97.59)	Acc@5 100.00 ( 99.97)
Epoch: [65][130/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.7036e-02 (7.6162e-02)	Acc@1  96.88 ( 97.56)	Acc@5 100.00 ( 99.98)
Epoch: [65][140/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1426e-01 (7.5733e-02)	Acc@1  96.88 ( 97.61)	Acc@5 100.00 ( 99.98)
Epoch: [65][150/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.1981e-02 (7.5621e-02)	Acc@1  96.88 ( 97.60)	Acc@5 100.00 ( 99.97)
Epoch: [65][160/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1931e-02 (7.5710e-02)	Acc@1  99.22 ( 97.58)	Acc@5 100.00 ( 99.98)
Epoch: [65][170/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.2764e-02 (7.5952e-02)	Acc@1  98.44 ( 97.58)	Acc@5 100.00 ( 99.98)
Epoch: [65][180/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.7534e-02 (7.6018e-02)	Acc@1  96.09 ( 97.54)	Acc@5 100.00 ( 99.98)
Epoch: [65][190/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.1350e-02 (7.6742e-02)	Acc@1  96.09 ( 97.49)	Acc@5 100.00 ( 99.98)
Epoch: [65][200/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.9946e-02 (7.6675e-02)	Acc@1  97.66 ( 97.50)	Acc@5 100.00 ( 99.98)
Epoch: [65][210/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9133e-02 (7.6499e-02)	Acc@1  99.22 ( 97.53)	Acc@5 100.00 ( 99.98)
Epoch: [65][220/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.2266e-02 (7.6492e-02)	Acc@1  98.44 ( 97.53)	Acc@5 100.00 ( 99.98)
Epoch: [65][230/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1206e-01 (7.6592e-02)	Acc@1  96.88 ( 97.53)	Acc@5 100.00 ( 99.98)
Epoch: [65][240/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2067e-01 (7.7284e-02)	Acc@1  95.31 ( 97.50)	Acc@5 100.00 ( 99.98)
Epoch: [65][250/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2898e-02 (7.7280e-02)	Acc@1 100.00 ( 97.50)	Acc@5 100.00 ( 99.98)
Epoch: [65][260/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4368e-01 (7.7621e-02)	Acc@1  95.31 ( 97.49)	Acc@5 100.00 ( 99.98)
Epoch: [65][270/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.6101e-02 (7.7313e-02)	Acc@1  98.44 ( 97.50)	Acc@5 100.00 ( 99.98)
Epoch: [65][280/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 8.5022e-02 (7.7296e-02)	Acc@1  96.88 ( 97.49)	Acc@5 100.00 ( 99.98)
Epoch: [65][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 9.7473e-02 (7.7470e-02)	Acc@1  97.66 ( 97.48)	Acc@5 100.00 ( 99.98)
Epoch: [65][300/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 8.4778e-02 (7.7684e-02)	Acc@1  97.66 ( 97.47)	Acc@5 100.00 ( 99.98)
Epoch: [65][310/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.6213e-02 (7.7866e-02)	Acc@1  98.44 ( 97.47)	Acc@5 100.00 ( 99.98)
Epoch: [65][320/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.8552e-02 (7.7928e-02)	Acc@1  97.66 ( 97.46)	Acc@5 100.00 ( 99.98)
Epoch: [65][330/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 8.4351e-02 (7.8556e-02)	Acc@1  98.44 ( 97.45)	Acc@5 100.00 ( 99.98)
Epoch: [65][340/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.3792e-02 (7.8336e-02)	Acc@1  95.31 ( 97.45)	Acc@5 100.00 ( 99.98)
Epoch: [65][350/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.2231e-01 (7.8439e-02)	Acc@1  96.09 ( 97.44)	Acc@5 100.00 ( 99.98)
Epoch: [65][360/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.8918e-02 (7.8790e-02)	Acc@1  96.09 ( 97.41)	Acc@5 100.00 ( 99.98)
Epoch: [65][370/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.1871e-01 (7.8545e-02)	Acc@1  95.31 ( 97.42)	Acc@5 100.00 ( 99.99)
Epoch: [65][380/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.3303e-02 (7.8298e-02)	Acc@1  99.22 ( 97.42)	Acc@5 100.00 ( 99.99)
Epoch: [65][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.0016e-01 (7.8221e-02)	Acc@1  93.75 ( 97.43)	Acc@5 100.00 ( 99.99)
## e[65] optimizer.zero_grad (sum) time: 0.2516293525695801
## e[65]       loss.backward (sum) time: 4.2001049518585205
## e[65]      optimizer.step (sum) time: 1.7353618144989014
## epoch[65] training(only) time: 15.887391090393066
# Switched to evaluate mode...
Test: [  0/100]	Time  0.177 ( 0.177)	Loss 2.5635e-01 (2.5635e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.018 ( 0.032)	Loss 3.6694e-01 (2.8254e-01)	Acc@1  88.00 ( 91.45)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.024 ( 0.026)	Loss 3.9771e-01 (3.0822e-01)	Acc@1  87.00 ( 91.29)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.021 ( 0.025)	Loss 4.5483e-01 (3.1761e-01)	Acc@1  88.00 ( 91.10)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.022 ( 0.024)	Loss 4.1821e-01 (3.2038e-01)	Acc@1  89.00 ( 91.07)	Acc@5  99.00 ( 99.68)
Test: [ 50/100]	Time  0.020 ( 0.023)	Loss 2.1094e-01 (3.1338e-01)	Acc@1  94.00 ( 91.20)	Acc@5 100.00 ( 99.67)
Test: [ 60/100]	Time  0.020 ( 0.022)	Loss 3.7061e-01 (3.1373e-01)	Acc@1  90.00 ( 91.00)	Acc@5 100.00 ( 99.70)
Test: [ 70/100]	Time  0.019 ( 0.022)	Loss 4.3262e-01 (3.1012e-01)	Acc@1  88.00 ( 90.96)	Acc@5 100.00 ( 99.70)
Test: [ 80/100]	Time  0.017 ( 0.022)	Loss 1.0583e-01 (3.1445e-01)	Acc@1  95.00 ( 90.74)	Acc@5 100.00 ( 99.69)
Test: [ 90/100]	Time  0.017 ( 0.021)	Loss 2.5513e-01 (3.1349e-01)	Acc@1  94.00 ( 90.80)	Acc@5 100.00 ( 99.71)
 * Acc@1 90.890 Acc@5 99.730
### epoch[65] execution time: 18.08214783668518
EPOCH 66
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [66][  0/391]	Time  0.215 ( 0.215)	Data  0.174 ( 0.174)	Loss 3.7659e-02 (3.7659e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [66][ 10/391]	Time  0.040 ( 0.056)	Data  0.001 ( 0.017)	Loss 5.8746e-02 (6.4973e-02)	Acc@1  98.44 ( 97.80)	Acc@5 100.00 (100.00)
Epoch: [66][ 20/391]	Time  0.042 ( 0.049)	Data  0.001 ( 0.009)	Loss 4.5105e-02 (6.7471e-02)	Acc@1  98.44 ( 97.84)	Acc@5 100.00 (100.00)
Epoch: [66][ 30/391]	Time  0.040 ( 0.046)	Data  0.001 ( 0.007)	Loss 9.6069e-02 (7.7535e-02)	Acc@1  97.66 ( 97.56)	Acc@5 100.00 ( 99.97)
Epoch: [66][ 40/391]	Time  0.038 ( 0.045)	Data  0.001 ( 0.005)	Loss 5.9387e-02 (7.5763e-02)	Acc@1  97.66 ( 97.58)	Acc@5 100.00 ( 99.98)
Epoch: [66][ 50/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.1755e-01 (7.8511e-02)	Acc@1  96.09 ( 97.44)	Acc@5 100.00 ( 99.98)
Epoch: [66][ 60/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 5.4901e-02 (7.7442e-02)	Acc@1  99.22 ( 97.46)	Acc@5 100.00 ( 99.99)
Epoch: [66][ 70/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.003)	Loss 5.7312e-02 (7.7886e-02)	Acc@1  99.22 ( 97.46)	Acc@5 100.00 ( 99.99)
Epoch: [66][ 80/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.3945e-02 (7.8008e-02)	Acc@1  98.44 ( 97.45)	Acc@5 100.00 ( 99.99)
Epoch: [66][ 90/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.6355e-02 (7.8376e-02)	Acc@1  97.66 ( 97.42)	Acc@5 100.00 ( 99.99)
Epoch: [66][100/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.0309e-01 (7.6681e-02)	Acc@1  96.88 ( 97.49)	Acc@5 100.00 ( 99.99)
Epoch: [66][110/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0468e-01 (7.6435e-02)	Acc@1  96.09 ( 97.49)	Acc@5 100.00 ( 99.99)
Epoch: [66][120/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.5745e-02 (7.6678e-02)	Acc@1  97.66 ( 97.48)	Acc@5 100.00 ( 99.99)
Epoch: [66][130/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.2021e-02 (7.7063e-02)	Acc@1  97.66 ( 97.44)	Acc@5 100.00 ( 99.99)
Epoch: [66][140/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.4595e-02 (7.7243e-02)	Acc@1  97.66 ( 97.41)	Acc@5 100.00 ( 99.99)
Epoch: [66][150/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.6008e-02 (7.7058e-02)	Acc@1  96.88 ( 97.43)	Acc@5 100.00 ( 99.99)
Epoch: [66][160/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.4331e-02 (7.7383e-02)	Acc@1  98.44 ( 97.41)	Acc@5 100.00 ( 99.99)
Epoch: [66][170/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.3069e-02 (7.7096e-02)	Acc@1  96.88 ( 97.42)	Acc@5 100.00 ( 99.99)
Epoch: [66][180/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.6965e-02 (7.6928e-02)	Acc@1  97.66 ( 97.44)	Acc@5 100.00 ( 99.99)
Epoch: [66][190/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.8359e-02 (7.7348e-02)	Acc@1  98.44 ( 97.42)	Acc@5 100.00 ( 99.99)
Epoch: [66][200/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0461e-01 (7.7499e-02)	Acc@1  95.31 ( 97.40)	Acc@5 100.00 ( 99.99)
Epoch: [66][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.7097e-02 (7.7803e-02)	Acc@1  97.66 ( 97.37)	Acc@5 100.00 ( 99.99)
Epoch: [66][220/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0649e-02 (7.7602e-02)	Acc@1  99.22 ( 97.37)	Acc@5 100.00 ( 99.99)
Epoch: [66][230/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6431e-01 (7.7477e-02)	Acc@1  95.31 ( 97.41)	Acc@5 100.00 ( 99.99)
Epoch: [66][240/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.3914e-02 (7.7121e-02)	Acc@1  99.22 ( 97.42)	Acc@5 100.00 ( 99.99)
Epoch: [66][250/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.5825e-02 (7.6839e-02)	Acc@1  96.09 ( 97.44)	Acc@5 100.00 ( 99.99)
Epoch: [66][260/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.8411e-02 (7.6546e-02)	Acc@1  96.88 ( 97.45)	Acc@5 100.00 ( 99.99)
Epoch: [66][270/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7689e-02 (7.6549e-02)	Acc@1 100.00 ( 97.44)	Acc@5 100.00 ( 99.99)
Epoch: [66][280/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.3506e-02 (7.6802e-02)	Acc@1  96.88 ( 97.45)	Acc@5 100.00 ( 99.99)
Epoch: [66][290/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.0706e-01 (7.6770e-02)	Acc@1  95.31 ( 97.45)	Acc@5 100.00 ( 99.99)
Epoch: [66][300/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.7637e-02 (7.7008e-02)	Acc@1  96.88 ( 97.44)	Acc@5 100.00 ( 99.99)
Epoch: [66][310/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.9896e-02 (7.6459e-02)	Acc@1  97.66 ( 97.45)	Acc@5 100.00 ( 99.99)
Epoch: [66][320/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.4951e-02 (7.6806e-02)	Acc@1  96.88 ( 97.44)	Acc@5 100.00 (100.00)
Epoch: [66][330/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 8.6670e-02 (7.6714e-02)	Acc@1  96.88 ( 97.45)	Acc@5 100.00 (100.00)
Epoch: [66][340/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 9.2285e-02 (7.6521e-02)	Acc@1  96.88 ( 97.45)	Acc@5 100.00 (100.00)
Epoch: [66][350/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.5074e-02 (7.6348e-02)	Acc@1  98.44 ( 97.46)	Acc@5 100.00 (100.00)
Epoch: [66][360/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 9.4666e-02 (7.6435e-02)	Acc@1  96.88 ( 97.46)	Acc@5 100.00 (100.00)
Epoch: [66][370/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.2389e-02 (7.6368e-02)	Acc@1  99.22 ( 97.46)	Acc@5 100.00 ( 99.99)
Epoch: [66][380/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.2781e-01 (7.5972e-02)	Acc@1  94.53 ( 97.47)	Acc@5 100.00 ( 99.99)
Epoch: [66][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.1554e-01 (7.6000e-02)	Acc@1  96.25 ( 97.47)	Acc@5 100.00 ( 99.99)
## e[66] optimizer.zero_grad (sum) time: 0.2509450912475586
## e[66]       loss.backward (sum) time: 4.144891977310181
## e[66]      optimizer.step (sum) time: 1.7640657424926758
## epoch[66] training(only) time: 15.863166570663452
# Switched to evaluate mode...
Test: [  0/100]	Time  0.166 ( 0.166)	Loss 2.5586e-01 (2.5586e-01)	Acc@1  93.00 ( 93.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.020 ( 0.034)	Loss 3.3960e-01 (2.7969e-01)	Acc@1  88.00 ( 91.27)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.021 ( 0.027)	Loss 3.8672e-01 (3.0676e-01)	Acc@1  86.00 ( 91.14)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.020 ( 0.025)	Loss 4.4043e-01 (3.1452e-01)	Acc@1  87.00 ( 90.94)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.021 ( 0.024)	Loss 4.0796e-01 (3.1610e-01)	Acc@1  90.00 ( 90.93)	Acc@5  99.00 ( 99.66)
Test: [ 50/100]	Time  0.017 ( 0.023)	Loss 1.9666e-01 (3.0940e-01)	Acc@1  94.00 ( 91.06)	Acc@5 100.00 ( 99.69)
Test: [ 60/100]	Time  0.021 ( 0.022)	Loss 3.7598e-01 (3.0962e-01)	Acc@1  91.00 ( 90.97)	Acc@5 100.00 ( 99.72)
Test: [ 70/100]	Time  0.016 ( 0.022)	Loss 4.3262e-01 (3.0694e-01)	Acc@1  87.00 ( 90.92)	Acc@5 100.00 ( 99.72)
Test: [ 80/100]	Time  0.017 ( 0.021)	Loss 1.0236e-01 (3.1134e-01)	Acc@1  94.00 ( 90.74)	Acc@5 100.00 ( 99.72)
Test: [ 90/100]	Time  0.017 ( 0.021)	Loss 2.5244e-01 (3.1034e-01)	Acc@1  94.00 ( 90.84)	Acc@5 100.00 ( 99.73)
 * Acc@1 90.930 Acc@5 99.740
### epoch[66] execution time: 18.03784728050232
EPOCH 67
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [67][  0/391]	Time  0.226 ( 0.226)	Data  0.183 ( 0.183)	Loss 5.1849e-02 (5.1849e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [67][ 10/391]	Time  0.038 ( 0.056)	Data  0.001 ( 0.017)	Loss 6.6833e-02 (6.6602e-02)	Acc@1  97.66 ( 97.87)	Acc@5 100.00 (100.00)
Epoch: [67][ 20/391]	Time  0.043 ( 0.049)	Data  0.001 ( 0.010)	Loss 7.3547e-02 (7.0573e-02)	Acc@1  97.66 ( 97.81)	Acc@5 100.00 (100.00)
Epoch: [67][ 30/391]	Time  0.042 ( 0.046)	Data  0.001 ( 0.007)	Loss 8.3862e-02 (7.1408e-02)	Acc@1  96.88 ( 97.86)	Acc@5 100.00 (100.00)
Epoch: [67][ 40/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.005)	Loss 6.3965e-02 (7.1373e-02)	Acc@1  96.09 ( 97.68)	Acc@5 100.00 (100.00)
Epoch: [67][ 50/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.1005e-01 (7.2581e-02)	Acc@1  94.53 ( 97.50)	Acc@5 100.00 (100.00)
Epoch: [67][ 60/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.3647e-01 (7.5643e-02)	Acc@1  94.53 ( 97.36)	Acc@5 100.00 ( 99.99)
Epoch: [67][ 70/391]	Time  0.043 ( 0.043)	Data  0.001 ( 0.003)	Loss 7.8003e-02 (7.4465e-02)	Acc@1  98.44 ( 97.45)	Acc@5 100.00 ( 99.99)
Epoch: [67][ 80/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.6793e-02 (7.3649e-02)	Acc@1  99.22 ( 97.53)	Acc@5 100.00 ( 99.99)
Epoch: [67][ 90/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.3162e-02 (7.3295e-02)	Acc@1  98.44 ( 97.57)	Acc@5 100.00 ( 99.99)
Epoch: [67][100/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.003)	Loss 9.3201e-02 (7.2597e-02)	Acc@1  96.88 ( 97.62)	Acc@5 100.00 ( 99.99)
Epoch: [67][110/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 3.6896e-02 (7.2484e-02)	Acc@1  99.22 ( 97.61)	Acc@5 100.00 ( 99.99)
Epoch: [67][120/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.6528e-02 (7.4052e-02)	Acc@1  98.44 ( 97.60)	Acc@5 100.00 ( 99.99)
Epoch: [67][130/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8767e-02 (7.3649e-02)	Acc@1  99.22 ( 97.62)	Acc@5 100.00 ( 99.99)
Epoch: [67][140/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.6895e-02 (7.3001e-02)	Acc@1  97.66 ( 97.65)	Acc@5 100.00 ( 99.99)
Epoch: [67][150/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.9102e-02 (7.3375e-02)	Acc@1  96.09 ( 97.61)	Acc@5 100.00 ( 99.99)
Epoch: [67][160/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2114e-02 (7.4147e-02)	Acc@1  98.44 ( 97.57)	Acc@5 100.00 (100.00)
Epoch: [67][170/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1780e-01 (7.3558e-02)	Acc@1  98.44 ( 97.62)	Acc@5 100.00 (100.00)
Epoch: [67][180/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.5471e-02 (7.3557e-02)	Acc@1  99.22 ( 97.64)	Acc@5 100.00 (100.00)
Epoch: [67][190/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.9895e-02 (7.3725e-02)	Acc@1  97.66 ( 97.63)	Acc@5 100.00 (100.00)
Epoch: [67][200/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1169e-01 (7.3819e-02)	Acc@1  97.66 ( 97.64)	Acc@5 100.00 (100.00)
Epoch: [67][210/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9093e-02 (7.4259e-02)	Acc@1  99.22 ( 97.62)	Acc@5 100.00 (100.00)
Epoch: [67][220/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.3191e-02 (7.4962e-02)	Acc@1  98.44 ( 97.59)	Acc@5 100.00 (100.00)
Epoch: [67][230/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.6660e-02 (7.5002e-02)	Acc@1  96.88 ( 97.59)	Acc@5 100.00 (100.00)
Epoch: [67][240/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.9407e-02 (7.5102e-02)	Acc@1  96.88 ( 97.58)	Acc@5 100.00 (100.00)
Epoch: [67][250/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2603e-02 (7.5063e-02)	Acc@1 100.00 ( 97.58)	Acc@5 100.00 (100.00)
Epoch: [67][260/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.8074e-02 (7.5146e-02)	Acc@1  96.88 ( 97.59)	Acc@5 100.00 (100.00)
Epoch: [67][270/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.6660e-02 (7.4904e-02)	Acc@1  98.44 ( 97.58)	Acc@5 100.00 (100.00)
Epoch: [67][280/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2024e-01 (7.4659e-02)	Acc@1  97.66 ( 97.60)	Acc@5 100.00 (100.00)
Epoch: [67][290/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.8054e-02 (7.4904e-02)	Acc@1  97.66 ( 97.60)	Acc@5 100.00 (100.00)
Epoch: [67][300/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.4077e-02 (7.4496e-02)	Acc@1  98.44 ( 97.61)	Acc@5 100.00 (100.00)
Epoch: [67][310/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 8.9539e-02 (7.4838e-02)	Acc@1  96.88 ( 97.60)	Acc@5 100.00 ( 99.99)
Epoch: [67][320/391]	Time  0.047 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.0435e-02 (7.4872e-02)	Acc@1  97.66 ( 97.59)	Acc@5 100.00 (100.00)
Epoch: [67][330/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.4636e-02 (7.5534e-02)	Acc@1  97.66 ( 97.58)	Acc@5 100.00 ( 99.99)
Epoch: [67][340/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.0781e-02 (7.5597e-02)	Acc@1  99.22 ( 97.59)	Acc@5 100.00 ( 99.99)
Epoch: [67][350/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.1499e-01 (7.5802e-02)	Acc@1  95.31 ( 97.56)	Acc@5 100.00 ( 99.99)
Epoch: [67][360/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.2632e-02 (7.6078e-02)	Acc@1  97.66 ( 97.54)	Acc@5 100.00 ( 99.99)
Epoch: [67][370/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.6956e-02 (7.5993e-02)	Acc@1  97.66 ( 97.54)	Acc@5 100.00 ( 99.99)
Epoch: [67][380/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.4209e-02 (7.5773e-02)	Acc@1  99.22 ( 97.55)	Acc@5 100.00 ( 99.99)
Epoch: [67][390/391]	Time  0.030 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.5481e-02 (7.5595e-02)	Acc@1  97.50 ( 97.55)	Acc@5 100.00 ( 99.99)
## e[67] optimizer.zero_grad (sum) time: 0.24960994720458984
## e[67]       loss.backward (sum) time: 4.215548515319824
## e[67]      optimizer.step (sum) time: 1.7398393154144287
## epoch[67] training(only) time: 15.979076385498047
# Switched to evaluate mode...
Test: [  0/100]	Time  0.160 ( 0.160)	Loss 2.5317e-01 (2.5317e-01)	Acc@1  93.00 ( 93.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.018 ( 0.032)	Loss 3.4839e-01 (2.7959e-01)	Acc@1  88.00 ( 91.45)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.021 ( 0.026)	Loss 3.7891e-01 (3.0720e-01)	Acc@1  89.00 ( 91.38)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.022 ( 0.024)	Loss 4.6411e-01 (3.1808e-01)	Acc@1  88.00 ( 91.16)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.019 ( 0.023)	Loss 3.9111e-01 (3.1893e-01)	Acc@1  90.00 ( 91.07)	Acc@5  99.00 ( 99.66)
Test: [ 50/100]	Time  0.023 ( 0.023)	Loss 2.0959e-01 (3.1293e-01)	Acc@1  93.00 ( 91.10)	Acc@5 100.00 ( 99.69)
Test: [ 60/100]	Time  0.023 ( 0.022)	Loss 3.8428e-01 (3.1351e-01)	Acc@1  90.00 ( 91.02)	Acc@5 100.00 ( 99.72)
Test: [ 70/100]	Time  0.018 ( 0.022)	Loss 4.3408e-01 (3.0920e-01)	Acc@1  87.00 ( 90.94)	Acc@5 100.00 ( 99.72)
Test: [ 80/100]	Time  0.024 ( 0.022)	Loss 1.1847e-01 (3.1371e-01)	Acc@1  94.00 ( 90.75)	Acc@5 100.00 ( 99.72)
Test: [ 90/100]	Time  0.022 ( 0.022)	Loss 2.7075e-01 (3.1239e-01)	Acc@1  94.00 ( 90.86)	Acc@5 100.00 ( 99.73)
 * Acc@1 90.970 Acc@5 99.740
### epoch[67] execution time: 18.280019283294678
EPOCH 68
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [68][  0/391]	Time  0.207 ( 0.207)	Data  0.166 ( 0.166)	Loss 4.9561e-02 (4.9561e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [68][ 10/391]	Time  0.040 ( 0.055)	Data  0.001 ( 0.016)	Loss 1.1926e-01 (7.9949e-02)	Acc@1  96.88 ( 97.23)	Acc@5 100.00 (100.00)
Epoch: [68][ 20/391]	Time  0.042 ( 0.048)	Data  0.001 ( 0.009)	Loss 1.5723e-01 (7.9473e-02)	Acc@1  95.31 ( 97.17)	Acc@5 100.00 (100.00)
Epoch: [68][ 30/391]	Time  0.038 ( 0.046)	Data  0.001 ( 0.006)	Loss 7.8979e-02 (7.8257e-02)	Acc@1  97.66 ( 97.13)	Acc@5 100.00 (100.00)
Epoch: [68][ 40/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.005)	Loss 7.4524e-02 (7.6636e-02)	Acc@1  98.44 ( 97.22)	Acc@5 100.00 (100.00)
Epoch: [68][ 50/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.004)	Loss 2.3834e-02 (7.4311e-02)	Acc@1 100.00 ( 97.35)	Acc@5 100.00 (100.00)
Epoch: [68][ 60/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.004)	Loss 9.1248e-02 (7.6252e-02)	Acc@1  96.09 ( 97.18)	Acc@5 100.00 ( 99.99)
Epoch: [68][ 70/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.5439e-02 (7.5335e-02)	Acc@1  96.88 ( 97.29)	Acc@5 100.00 ( 99.99)
Epoch: [68][ 80/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.7678e-02 (7.4059e-02)	Acc@1  98.44 ( 97.38)	Acc@5 100.00 ( 99.99)
Epoch: [68][ 90/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.8624e-02 (7.2545e-02)	Acc@1  97.66 ( 97.47)	Acc@5 100.00 ( 99.99)
Epoch: [68][100/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.6294e-02 (7.3695e-02)	Acc@1  97.66 ( 97.42)	Acc@5 100.00 ( 99.99)
Epoch: [68][110/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.4138e-02 (7.3572e-02)	Acc@1  98.44 ( 97.48)	Acc@5 100.00 ( 99.99)
Epoch: [68][120/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.9296e-02 (7.2580e-02)	Acc@1  99.22 ( 97.55)	Acc@5 100.00 ( 99.99)
Epoch: [68][130/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.2163e-02 (7.3100e-02)	Acc@1  96.88 ( 97.54)	Acc@5 100.00 ( 99.99)
Epoch: [68][140/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5461e-02 (7.2274e-02)	Acc@1  99.22 ( 97.57)	Acc@5 100.00 ( 99.99)
Epoch: [68][150/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.2205e-02 (7.2919e-02)	Acc@1  99.22 ( 97.55)	Acc@5 100.00 ( 99.99)
Epoch: [68][160/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.6294e-02 (7.2649e-02)	Acc@1  96.88 ( 97.56)	Acc@5 100.00 (100.00)
Epoch: [68][170/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.9548e-02 (7.3149e-02)	Acc@1  96.88 ( 97.55)	Acc@5 100.00 (100.00)
Epoch: [68][180/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.6375e-02 (7.3505e-02)	Acc@1  96.88 ( 97.52)	Acc@5 100.00 (100.00)
Epoch: [68][190/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.7037e-02 (7.3913e-02)	Acc@1  98.44 ( 97.51)	Acc@5 100.00 (100.00)
Epoch: [68][200/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7394e-02 (7.4059e-02)	Acc@1  99.22 ( 97.51)	Acc@5 100.00 (100.00)
Epoch: [68][210/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.0271e-02 (7.3837e-02)	Acc@1  96.88 ( 97.51)	Acc@5 100.00 (100.00)
Epoch: [68][220/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.5083e-02 (7.3416e-02)	Acc@1  97.66 ( 97.52)	Acc@5 100.00 (100.00)
Epoch: [68][230/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0476e-02 (7.3496e-02)	Acc@1  99.22 ( 97.53)	Acc@5 100.00 (100.00)
Epoch: [68][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1107e-02 (7.3926e-02)	Acc@1  98.44 ( 97.51)	Acc@5 100.00 (100.00)
Epoch: [68][250/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.6792e-02 (7.4510e-02)	Acc@1  96.09 ( 97.49)	Acc@5 100.00 (100.00)
Epoch: [68][260/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.6670e-02 (7.4911e-02)	Acc@1  96.09 ( 97.46)	Acc@5 100.00 (100.00)
Epoch: [68][270/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.1788e-02 (7.4563e-02)	Acc@1  98.44 ( 97.49)	Acc@5 100.00 (100.00)
Epoch: [68][280/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.2744e-01 (7.4934e-02)	Acc@1  95.31 ( 97.48)	Acc@5 100.00 (100.00)
Epoch: [68][290/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.2510e-02 (7.5562e-02)	Acc@1  98.44 ( 97.44)	Acc@5 100.00 (100.00)
Epoch: [68][300/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.7434e-02 (7.5746e-02)	Acc@1  97.66 ( 97.43)	Acc@5 100.00 (100.00)
Epoch: [68][310/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 8.4961e-02 (7.5466e-02)	Acc@1  96.88 ( 97.44)	Acc@5 100.00 (100.00)
Epoch: [68][320/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.0312e-02 (7.4908e-02)	Acc@1  97.66 ( 97.47)	Acc@5 100.00 (100.00)
Epoch: [68][330/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.8380e-02 (7.5483e-02)	Acc@1  97.66 ( 97.47)	Acc@5 100.00 ( 99.99)
Epoch: [68][340/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.001)	Loss 8.7463e-02 (7.5248e-02)	Acc@1  96.88 ( 97.47)	Acc@5 100.00 ( 99.99)
Epoch: [68][350/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.8542e-02 (7.5424e-02)	Acc@1  97.66 ( 97.48)	Acc@5 100.00 ( 99.99)
Epoch: [68][360/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 8.8928e-02 (7.5357e-02)	Acc@1  96.88 ( 97.47)	Acc@5 100.00 ( 99.99)
Epoch: [68][370/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.1090e-01 (7.5087e-02)	Acc@1  96.09 ( 97.48)	Acc@5 100.00 ( 99.99)
Epoch: [68][380/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 8.9600e-02 (7.5711e-02)	Acc@1  96.88 ( 97.45)	Acc@5 100.00 ( 99.99)
Epoch: [68][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.7648e-02 (7.5732e-02)	Acc@1  98.75 ( 97.44)	Acc@5 100.00 ( 99.99)
## e[68] optimizer.zero_grad (sum) time: 0.2477118968963623
## e[68]       loss.backward (sum) time: 4.139721870422363
## e[68]      optimizer.step (sum) time: 1.7409729957580566
## epoch[68] training(only) time: 15.876598358154297
# Switched to evaluate mode...
Test: [  0/100]	Time  0.174 ( 0.174)	Loss 2.6807e-01 (2.6807e-01)	Acc@1  93.00 ( 93.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.020 ( 0.035)	Loss 3.7109e-01 (2.8159e-01)	Acc@1  87.00 ( 91.64)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.017 ( 0.027)	Loss 3.8672e-01 (3.0737e-01)	Acc@1  86.00 ( 91.10)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.023 ( 0.025)	Loss 4.3579e-01 (3.1539e-01)	Acc@1  87.00 ( 90.84)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.017 ( 0.024)	Loss 4.1162e-01 (3.1803e-01)	Acc@1  88.00 ( 90.76)	Acc@5  99.00 ( 99.68)
Test: [ 50/100]	Time  0.026 ( 0.023)	Loss 2.0679e-01 (3.1072e-01)	Acc@1  93.00 ( 90.90)	Acc@5 100.00 ( 99.69)
Test: [ 60/100]	Time  0.022 ( 0.022)	Loss 3.7378e-01 (3.1130e-01)	Acc@1  91.00 ( 90.79)	Acc@5 100.00 ( 99.72)
Test: [ 70/100]	Time  0.024 ( 0.022)	Loss 4.2139e-01 (3.0733e-01)	Acc@1  86.00 ( 90.75)	Acc@5 100.00 ( 99.72)
Test: [ 80/100]	Time  0.020 ( 0.022)	Loss 1.0651e-01 (3.1164e-01)	Acc@1  94.00 ( 90.63)	Acc@5 100.00 ( 99.72)
Test: [ 90/100]	Time  0.019 ( 0.022)	Loss 2.4622e-01 (3.1042e-01)	Acc@1  94.00 ( 90.71)	Acc@5 100.00 ( 99.73)
 * Acc@1 90.890 Acc@5 99.730
### epoch[68] execution time: 18.11317467689514
EPOCH 69
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [69][  0/391]	Time  0.210 ( 0.210)	Data  0.167 ( 0.167)	Loss 4.9316e-02 (4.9316e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [69][ 10/391]	Time  0.039 ( 0.056)	Data  0.001 ( 0.016)	Loss 4.5074e-02 (6.3109e-02)	Acc@1  98.44 ( 97.73)	Acc@5 100.00 (100.00)
Epoch: [69][ 20/391]	Time  0.042 ( 0.048)	Data  0.001 ( 0.009)	Loss 9.5276e-02 (6.9331e-02)	Acc@1  96.09 ( 97.47)	Acc@5 100.00 (100.00)
Epoch: [69][ 30/391]	Time  0.042 ( 0.045)	Data  0.001 ( 0.006)	Loss 6.3354e-02 (7.2661e-02)	Acc@1  98.44 ( 97.43)	Acc@5 100.00 (100.00)
Epoch: [69][ 40/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.005)	Loss 4.9896e-02 (7.2166e-02)	Acc@1  98.44 ( 97.50)	Acc@5 100.00 (100.00)
Epoch: [69][ 50/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.1194e-01 (7.1555e-02)	Acc@1  95.31 ( 97.53)	Acc@5 100.00 (100.00)
Epoch: [69][ 60/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 5.6274e-02 (7.1363e-02)	Acc@1  96.88 ( 97.59)	Acc@5 100.00 (100.00)
Epoch: [69][ 70/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.1749e-01 (7.1187e-02)	Acc@1  95.31 ( 97.56)	Acc@5 100.00 (100.00)
Epoch: [69][ 80/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.1899e-02 (7.3691e-02)	Acc@1  98.44 ( 97.48)	Acc@5 100.00 (100.00)
Epoch: [69][ 90/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.3232e-02 (7.4114e-02)	Acc@1  99.22 ( 97.47)	Acc@5 100.00 ( 99.99)
Epoch: [69][100/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.003)	Loss 4.9072e-02 (7.3433e-02)	Acc@1  98.44 ( 97.50)	Acc@5 100.00 ( 99.99)
Epoch: [69][110/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3159e-01 (7.3607e-02)	Acc@1  96.09 ( 97.52)	Acc@5 100.00 ( 99.99)
Epoch: [69][120/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1194e-01 (7.4198e-02)	Acc@1  96.09 ( 97.51)	Acc@5 100.00 ( 99.99)
Epoch: [69][130/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.1523e-02 (7.4282e-02)	Acc@1  98.44 ( 97.50)	Acc@5 100.00 ( 99.99)
Epoch: [69][140/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.1676e-02 (7.4675e-02)	Acc@1  98.44 ( 97.48)	Acc@5 100.00 ( 99.99)
Epoch: [69][150/391]	Time  0.041 ( 0.041)	Data  0.003 ( 0.002)	Loss 6.4819e-02 (7.4551e-02)	Acc@1  98.44 ( 97.48)	Acc@5 100.00 ( 99.99)
Epoch: [69][160/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3243e-02 (7.4122e-02)	Acc@1  98.44 ( 97.49)	Acc@5 100.00 ( 99.99)
Epoch: [69][170/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.6243e-02 (7.4132e-02)	Acc@1  96.88 ( 97.48)	Acc@5 100.00 ( 99.99)
Epoch: [69][180/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.0449e-01 (7.4763e-02)	Acc@1  96.88 ( 97.45)	Acc@5 100.00 ( 99.99)
Epoch: [69][190/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.002)	Loss 8.3862e-02 (7.4931e-02)	Acc@1  96.88 ( 97.45)	Acc@5 100.00 ( 99.99)
Epoch: [69][200/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.1792e-01 (7.4584e-02)	Acc@1  96.88 ( 97.47)	Acc@5 100.00 ( 99.99)
Epoch: [69][210/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.0260e-01 (7.5122e-02)	Acc@1  96.88 ( 97.46)	Acc@5 100.00 ( 99.99)
Epoch: [69][220/391]	Time  0.045 ( 0.040)	Data  0.001 ( 0.002)	Loss 8.7952e-02 (7.5042e-02)	Acc@1  96.88 ( 97.45)	Acc@5 100.00 ( 99.99)
Epoch: [69][230/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.0895e-01 (7.5314e-02)	Acc@1  94.53 ( 97.44)	Acc@5 100.00 ( 99.99)
Epoch: [69][240/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 6.3293e-02 (7.5451e-02)	Acc@1  97.66 ( 97.44)	Acc@5 100.00 ( 99.99)
Epoch: [69][250/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 5.4779e-02 (7.5157e-02)	Acc@1  97.66 ( 97.45)	Acc@5 100.00 ( 99.99)
Epoch: [69][260/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.2225e-01 (7.4976e-02)	Acc@1  96.09 ( 97.43)	Acc@5 100.00 ( 99.99)
Epoch: [69][270/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.002)	Loss 3.1677e-02 (7.4747e-02)	Acc@1  99.22 ( 97.46)	Acc@5 100.00 ( 99.99)
Epoch: [69][280/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.002)	Loss 4.9103e-02 (7.4311e-02)	Acc@1  98.44 ( 97.48)	Acc@5 100.00 ( 99.99)
Epoch: [69][290/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 6.9031e-02 (7.4377e-02)	Acc@1  98.44 ( 97.49)	Acc@5 100.00 ( 99.99)
Epoch: [69][300/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 4.3518e-02 (7.4239e-02)	Acc@1  98.44 ( 97.49)	Acc@5 100.00 ( 99.99)
Epoch: [69][310/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 9.2041e-02 (7.4100e-02)	Acc@1  96.09 ( 97.50)	Acc@5 100.00 ( 99.99)
Epoch: [69][320/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.1838e-02 (7.4014e-02)	Acc@1  97.66 ( 97.50)	Acc@5 100.00 ( 99.99)
Epoch: [69][330/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.0171e-02 (7.4231e-02)	Acc@1  98.44 ( 97.49)	Acc@5 100.00 ( 99.99)
Epoch: [69][340/391]	Time  0.043 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.7526e-02 (7.4418e-02)	Acc@1  98.44 ( 97.49)	Acc@5 100.00 ( 99.99)
Epoch: [69][350/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.2866e-02 (7.4441e-02)	Acc@1  98.44 ( 97.49)	Acc@5 100.00 ( 99.99)
Epoch: [69][360/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.3997e-02 (7.4782e-02)	Acc@1 100.00 ( 97.47)	Acc@5 100.00 ( 99.99)
Epoch: [69][370/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.9824e-02 (7.4855e-02)	Acc@1  96.88 ( 97.46)	Acc@5 100.00 ( 99.99)
Epoch: [69][380/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.8030e-02 (7.4873e-02)	Acc@1 100.00 ( 97.47)	Acc@5 100.00 ( 99.99)
Epoch: [69][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.0020e-01 (7.4973e-02)	Acc@1  92.50 ( 97.48)	Acc@5 100.00 ( 99.99)
## e[69] optimizer.zero_grad (sum) time: 0.2500331401824951
## e[69]       loss.backward (sum) time: 4.025739669799805
## e[69]      optimizer.step (sum) time: 1.7900373935699463
## epoch[69] training(only) time: 15.68661379814148
# Switched to evaluate mode...
Test: [  0/100]	Time  0.166 ( 0.166)	Loss 2.5659e-01 (2.5659e-01)	Acc@1  93.00 ( 93.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.021 ( 0.033)	Loss 3.4985e-01 (2.8033e-01)	Acc@1  89.00 ( 91.45)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.023 ( 0.027)	Loss 3.7915e-01 (3.0574e-01)	Acc@1  87.00 ( 91.24)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.020 ( 0.025)	Loss 4.3750e-01 (3.1497e-01)	Acc@1  87.00 ( 90.94)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.023 ( 0.024)	Loss 4.0186e-01 (3.1640e-01)	Acc@1  91.00 ( 91.00)	Acc@5  99.00 ( 99.68)
Test: [ 50/100]	Time  0.018 ( 0.023)	Loss 1.8811e-01 (3.0963e-01)	Acc@1  94.00 ( 91.04)	Acc@5 100.00 ( 99.69)
Test: [ 60/100]	Time  0.020 ( 0.023)	Loss 3.7744e-01 (3.0966e-01)	Acc@1  91.00 ( 90.92)	Acc@5 100.00 ( 99.72)
Test: [ 70/100]	Time  0.021 ( 0.022)	Loss 4.3530e-01 (3.0608e-01)	Acc@1  88.00 ( 90.89)	Acc@5 100.00 ( 99.72)
Test: [ 80/100]	Time  0.022 ( 0.022)	Loss 1.0876e-01 (3.1083e-01)	Acc@1  94.00 ( 90.72)	Acc@5 100.00 ( 99.69)
Test: [ 90/100]	Time  0.023 ( 0.022)	Loss 2.6147e-01 (3.0985e-01)	Acc@1  94.00 ( 90.82)	Acc@5 100.00 ( 99.70)
 * Acc@1 90.920 Acc@5 99.710
### epoch[69] execution time: 17.977824211120605
EPOCH 70
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [70][  0/391]	Time  0.218 ( 0.218)	Data  0.174 ( 0.174)	Loss 6.7078e-02 (6.7078e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [70][ 10/391]	Time  0.040 ( 0.057)	Data  0.001 ( 0.017)	Loss 5.6000e-02 (6.8459e-02)	Acc@1  97.66 ( 97.23)	Acc@5 100.00 (100.00)
Epoch: [70][ 20/391]	Time  0.040 ( 0.050)	Data  0.001 ( 0.009)	Loss 5.4413e-02 (7.1596e-02)	Acc@1  98.44 ( 97.43)	Acc@5 100.00 ( 99.96)
Epoch: [70][ 30/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.006)	Loss 8.6914e-02 (7.5301e-02)	Acc@1  98.44 ( 97.38)	Acc@5 100.00 ( 99.97)
Epoch: [70][ 40/391]	Time  0.039 ( 0.045)	Data  0.001 ( 0.005)	Loss 6.9336e-02 (7.4840e-02)	Acc@1  96.88 ( 97.37)	Acc@5 100.00 ( 99.98)
Epoch: [70][ 50/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.5723e-01 (7.8321e-02)	Acc@1  95.31 ( 97.32)	Acc@5 100.00 ( 99.98)
Epoch: [70][ 60/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.0498e-01 (7.4971e-02)	Acc@1  96.88 ( 97.41)	Acc@5 100.00 ( 99.99)
Epoch: [70][ 70/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.003)	Loss 7.0740e-02 (7.4654e-02)	Acc@1  96.88 ( 97.41)	Acc@5 100.00 ( 99.99)
Epoch: [70][ 80/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.9163e-02 (7.3828e-02)	Acc@1  97.66 ( 97.53)	Acc@5 100.00 ( 99.99)
Epoch: [70][ 90/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 8.5510e-02 (7.2408e-02)	Acc@1  96.09 ( 97.57)	Acc@5 100.00 ( 99.99)
Epoch: [70][100/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.9824e-02 (7.3340e-02)	Acc@1  98.44 ( 97.56)	Acc@5 100.00 ( 99.99)
Epoch: [70][110/391]	Time  0.048 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.9856e-02 (7.3245e-02)	Acc@1  99.22 ( 97.58)	Acc@5 100.00 ( 99.99)
Epoch: [70][120/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 8.0444e-02 (7.3092e-02)	Acc@1  97.66 ( 97.58)	Acc@5 100.00 ( 99.99)
Epoch: [70][130/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.8054e-02 (7.3895e-02)	Acc@1  96.88 ( 97.54)	Acc@5 100.00 ( 99.99)
Epoch: [70][140/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.3313e-02 (7.4605e-02)	Acc@1  97.66 ( 97.53)	Acc@5 100.00 ( 99.99)
Epoch: [70][150/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.6904e-02 (7.4723e-02)	Acc@1  97.66 ( 97.53)	Acc@5 100.00 ( 99.99)
Epoch: [70][160/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.1421e-02 (7.5738e-02)	Acc@1  96.88 ( 97.49)	Acc@5 100.00 (100.00)
Epoch: [70][170/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.6680e-02 (7.6523e-02)	Acc@1  96.88 ( 97.48)	Acc@5 100.00 (100.00)
Epoch: [70][180/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.9336e-02 (7.6557e-02)	Acc@1  96.09 ( 97.48)	Acc@5 100.00 ( 99.99)
Epoch: [70][190/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.8685e-02 (7.6167e-02)	Acc@1  98.44 ( 97.50)	Acc@5 100.00 ( 99.99)
Epoch: [70][200/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0894e-02 (7.5410e-02)	Acc@1  99.22 ( 97.54)	Acc@5 100.00 ( 99.99)
Epoch: [70][210/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.5817e-02 (7.5685e-02)	Acc@1  97.66 ( 97.53)	Acc@5 100.00 ( 99.99)
Epoch: [70][220/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.2510e-02 (7.5636e-02)	Acc@1  97.66 ( 97.54)	Acc@5 100.00 ( 99.99)
Epoch: [70][230/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2646e-01 (7.6130e-02)	Acc@1  95.31 ( 97.53)	Acc@5 100.00 ( 99.99)
Epoch: [70][240/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0907e-01 (7.6196e-02)	Acc@1  94.53 ( 97.52)	Acc@5 100.00 ( 99.99)
Epoch: [70][250/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.7952e-02 (7.5805e-02)	Acc@1  97.66 ( 97.52)	Acc@5 100.00 ( 99.99)
Epoch: [70][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.8787e-02 (7.5457e-02)	Acc@1  99.22 ( 97.55)	Acc@5 100.00 ( 99.99)
Epoch: [70][270/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2389e-02 (7.5311e-02)	Acc@1  99.22 ( 97.56)	Acc@5 100.00 ( 99.99)
Epoch: [70][280/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0425e-01 (7.5482e-02)	Acc@1  96.88 ( 97.56)	Acc@5 100.00 ( 99.99)
Epoch: [70][290/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.2122e-01 (7.5454e-02)	Acc@1  96.88 ( 97.56)	Acc@5 100.00 ( 99.99)
Epoch: [70][300/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.8366e-02 (7.5198e-02)	Acc@1 100.00 ( 97.58)	Acc@5 100.00 ( 99.99)
Epoch: [70][310/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.4169e-02 (7.5251e-02)	Acc@1  98.44 ( 97.56)	Acc@5 100.00 ( 99.99)
Epoch: [70][320/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.6294e-02 (7.5373e-02)	Acc@1  97.66 ( 97.55)	Acc@5 100.00 ( 99.99)
Epoch: [70][330/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.5400e-02 (7.5047e-02)	Acc@1  99.22 ( 97.56)	Acc@5 100.00 ( 99.99)
Epoch: [70][340/391]	Time  0.054 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.8696e-02 (7.4610e-02)	Acc@1  98.44 ( 97.57)	Acc@5 100.00 ( 99.99)
Epoch: [70][350/391]	Time  0.039 ( 0.041)	Data  0.002 ( 0.001)	Loss 1.2756e-01 (7.4932e-02)	Acc@1  96.09 ( 97.55)	Acc@5 100.00 ( 99.99)
Epoch: [70][360/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.2549e-01 (7.4891e-02)	Acc@1  95.31 ( 97.56)	Acc@5 100.00 ( 99.99)
Epoch: [70][370/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.9011e-02 (7.4807e-02)	Acc@1  98.44 ( 97.56)	Acc@5 100.00 ( 99.99)
Epoch: [70][380/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 9.0210e-02 (7.5260e-02)	Acc@1  97.66 ( 97.54)	Acc@5 100.00 ( 99.99)
Epoch: [70][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.0068e-02 (7.5442e-02)	Acc@1  98.75 ( 97.54)	Acc@5 100.00 ( 99.99)
## e[70] optimizer.zero_grad (sum) time: 0.25116586685180664
## e[70]       loss.backward (sum) time: 4.218330144882202
## e[70]      optimizer.step (sum) time: 1.7029216289520264
## epoch[70] training(only) time: 15.947835683822632
# Switched to evaluate mode...
Test: [  0/100]	Time  0.175 ( 0.175)	Loss 2.6904e-01 (2.6904e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.021 ( 0.035)	Loss 3.5913e-01 (2.8357e-01)	Acc@1  88.00 ( 91.18)	Acc@5  99.00 ( 99.73)
Test: [ 20/100]	Time  0.017 ( 0.027)	Loss 3.7256e-01 (3.0679e-01)	Acc@1  86.00 ( 90.90)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.020 ( 0.025)	Loss 4.4702e-01 (3.1545e-01)	Acc@1  87.00 ( 90.68)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.022 ( 0.023)	Loss 4.0649e-01 (3.1691e-01)	Acc@1  88.00 ( 90.76)	Acc@5  99.00 ( 99.68)
Test: [ 50/100]	Time  0.020 ( 0.023)	Loss 2.0007e-01 (3.1097e-01)	Acc@1  94.00 ( 90.88)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.023 ( 0.022)	Loss 3.7402e-01 (3.1113e-01)	Acc@1  91.00 ( 90.77)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.025 ( 0.022)	Loss 4.4482e-01 (3.0770e-01)	Acc@1  86.00 ( 90.69)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.023 ( 0.022)	Loss 1.1298e-01 (3.1234e-01)	Acc@1  93.00 ( 90.52)	Acc@5 100.00 ( 99.72)
Test: [ 90/100]	Time  0.021 ( 0.022)	Loss 2.6270e-01 (3.1135e-01)	Acc@1  94.00 ( 90.62)	Acc@5 100.00 ( 99.74)
 * Acc@1 90.770 Acc@5 99.740
### epoch[70] execution time: 18.215100049972534
EPOCH 71
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [71][  0/391]	Time  0.218 ( 0.218)	Data  0.174 ( 0.174)	Loss 7.5806e-02 (7.5806e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [71][ 10/391]	Time  0.037 ( 0.056)	Data  0.001 ( 0.017)	Loss 1.0284e-01 (6.8873e-02)	Acc@1  95.31 ( 97.73)	Acc@5 100.00 (100.00)
Epoch: [71][ 20/391]	Time  0.038 ( 0.047)	Data  0.001 ( 0.009)	Loss 6.2622e-02 (7.0118e-02)	Acc@1  98.44 ( 97.62)	Acc@5 100.00 (100.00)
Epoch: [71][ 30/391]	Time  0.040 ( 0.045)	Data  0.001 ( 0.007)	Loss 6.6650e-02 (7.1336e-02)	Acc@1  96.88 ( 97.53)	Acc@5 100.00 (100.00)
Epoch: [71][ 40/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.005)	Loss 8.4961e-02 (6.7662e-02)	Acc@1  96.88 ( 97.75)	Acc@5 100.00 (100.00)
Epoch: [71][ 50/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.004)	Loss 7.3547e-02 (6.9377e-02)	Acc@1  97.66 ( 97.70)	Acc@5 100.00 ( 99.98)
Epoch: [71][ 60/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.004)	Loss 7.4036e-02 (7.2161e-02)	Acc@1  98.44 ( 97.68)	Acc@5 100.00 ( 99.99)
Epoch: [71][ 70/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 8.1909e-02 (7.2610e-02)	Acc@1  96.88 ( 97.63)	Acc@5 100.00 ( 99.99)
Epoch: [71][ 80/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.0461e-01 (7.0684e-02)	Acc@1  96.09 ( 97.72)	Acc@5 100.00 ( 99.99)
Epoch: [71][ 90/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.7993e-02 (7.2185e-02)	Acc@1  97.66 ( 97.62)	Acc@5 100.00 ( 99.99)
Epoch: [71][100/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.5857e-02 (7.2461e-02)	Acc@1  96.88 ( 97.63)	Acc@5 100.00 ( 99.99)
Epoch: [71][110/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.003)	Loss 8.7891e-02 (7.2376e-02)	Acc@1  96.09 ( 97.62)	Acc@5 100.00 ( 99.99)
Epoch: [71][120/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.8735e-02 (7.3469e-02)	Acc@1  96.88 ( 97.52)	Acc@5 100.00 ( 99.99)
Epoch: [71][130/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.6772e-02 (7.3805e-02)	Acc@1  98.44 ( 97.50)	Acc@5 100.00 ( 99.99)
Epoch: [71][140/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1438e-01 (7.4318e-02)	Acc@1  96.09 ( 97.46)	Acc@5  99.22 ( 99.98)
Epoch: [71][150/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.9153e-02 (7.4072e-02)	Acc@1  96.88 ( 97.45)	Acc@5 100.00 ( 99.98)
Epoch: [71][160/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7292e-02 (7.4066e-02)	Acc@1 100.00 ( 97.46)	Acc@5 100.00 ( 99.99)
Epoch: [71][170/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.0576e-02 (7.3732e-02)	Acc@1  96.09 ( 97.47)	Acc@5 100.00 ( 99.99)
Epoch: [71][180/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9154e-02 (7.4030e-02)	Acc@1  99.22 ( 97.46)	Acc@5 100.00 ( 99.99)
Epoch: [71][190/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1555e-02 (7.4149e-02)	Acc@1 100.00 ( 97.46)	Acc@5 100.00 ( 99.99)
Epoch: [71][200/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.4565e-02 (7.4554e-02)	Acc@1  98.44 ( 97.45)	Acc@5 100.00 ( 99.98)
Epoch: [71][210/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2286e-01 (7.4909e-02)	Acc@1  94.53 ( 97.42)	Acc@5 100.00 ( 99.98)
Epoch: [71][220/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.8674e-02 (7.5796e-02)	Acc@1  98.44 ( 97.39)	Acc@5 100.00 ( 99.98)
Epoch: [71][230/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1788e-02 (7.5730e-02)	Acc@1  99.22 ( 97.39)	Acc@5 100.00 ( 99.98)
Epoch: [71][240/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3386e-02 (7.5600e-02)	Acc@1 100.00 ( 97.41)	Acc@5 100.00 ( 99.98)
Epoch: [71][250/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2368e-02 (7.5706e-02)	Acc@1  99.22 ( 97.41)	Acc@5 100.00 ( 99.98)
Epoch: [71][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.5510e-02 (7.5672e-02)	Acc@1  96.88 ( 97.42)	Acc@5 100.00 ( 99.98)
Epoch: [71][270/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.3201e-02 (7.5343e-02)	Acc@1  96.09 ( 97.43)	Acc@5 100.00 ( 99.98)
Epoch: [71][280/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0934e-02 (7.5146e-02)	Acc@1 100.00 ( 97.44)	Acc@5 100.00 ( 99.98)
Epoch: [71][290/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.5939e-02 (7.5294e-02)	Acc@1  98.44 ( 97.41)	Acc@5 100.00 ( 99.98)
Epoch: [71][300/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.4890e-02 (7.5394e-02)	Acc@1  95.31 ( 97.41)	Acc@5 100.00 ( 99.98)
Epoch: [71][310/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6591e-02 (7.5081e-02)	Acc@1  99.22 ( 97.42)	Acc@5 100.00 ( 99.98)
Epoch: [71][320/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.6943e-01 (7.5477e-02)	Acc@1  94.53 ( 97.42)	Acc@5 100.00 ( 99.98)
Epoch: [71][330/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.3794e-01 (7.5311e-02)	Acc@1  96.09 ( 97.43)	Acc@5 100.00 ( 99.98)
Epoch: [71][340/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.0370e-01 (7.5160e-02)	Acc@1  96.88 ( 97.44)	Acc@5 100.00 ( 99.98)
Epoch: [71][350/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.0297e-01 (7.5319e-02)	Acc@1  96.09 ( 97.42)	Acc@5 100.00 ( 99.98)
Epoch: [71][360/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.2582e-02 (7.5438e-02)	Acc@1  97.66 ( 97.41)	Acc@5 100.00 ( 99.98)
Epoch: [71][370/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.1444e-01 (7.5351e-02)	Acc@1  95.31 ( 97.42)	Acc@5 100.00 ( 99.99)
Epoch: [71][380/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.9957e-02 (7.5819e-02)	Acc@1  97.66 ( 97.39)	Acc@5 100.00 ( 99.99)
Epoch: [71][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.5410e-02 (7.5478e-02)	Acc@1  98.75 ( 97.41)	Acc@5 100.00 ( 99.98)
## e[71] optimizer.zero_grad (sum) time: 0.2509438991546631
## e[71]       loss.backward (sum) time: 4.1413538455963135
## e[71]      optimizer.step (sum) time: 1.7584939002990723
## epoch[71] training(only) time: 15.927724599838257
# Switched to evaluate mode...
Test: [  0/100]	Time  0.171 ( 0.171)	Loss 2.6929e-01 (2.6929e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.017 ( 0.033)	Loss 3.8892e-01 (2.8173e-01)	Acc@1  87.00 ( 91.45)	Acc@5  99.00 ( 99.73)
Test: [ 20/100]	Time  0.017 ( 0.026)	Loss 3.6987e-01 (3.0502e-01)	Acc@1  86.00 ( 91.00)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.020 ( 0.024)	Loss 4.2749e-01 (3.1254e-01)	Acc@1  87.00 ( 90.81)	Acc@5 100.00 ( 99.77)
Test: [ 40/100]	Time  0.018 ( 0.022)	Loss 4.2773e-01 (3.1541e-01)	Acc@1  88.00 ( 90.71)	Acc@5  99.00 ( 99.73)
Test: [ 50/100]	Time  0.020 ( 0.022)	Loss 1.9275e-01 (3.0845e-01)	Acc@1  93.00 ( 90.88)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.018 ( 0.022)	Loss 3.7183e-01 (3.0869e-01)	Acc@1  92.00 ( 90.70)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.021 ( 0.021)	Loss 4.6045e-01 (3.0609e-01)	Acc@1  87.00 ( 90.69)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.020 ( 0.021)	Loss 1.0284e-01 (3.1049e-01)	Acc@1  95.00 ( 90.52)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.017 ( 0.021)	Loss 2.3865e-01 (3.0921e-01)	Acc@1  94.00 ( 90.60)	Acc@5 100.00 ( 99.76)
 * Acc@1 90.720 Acc@5 99.760
### epoch[71] execution time: 18.131311655044556
EPOCH 72
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [72][  0/391]	Time  0.218 ( 0.218)	Data  0.177 ( 0.177)	Loss 6.9214e-02 (6.9214e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [72][ 10/391]	Time  0.037 ( 0.056)	Data  0.001 ( 0.017)	Loss 8.4351e-02 (8.7705e-02)	Acc@1  96.09 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [72][ 20/391]	Time  0.040 ( 0.049)	Data  0.001 ( 0.009)	Loss 4.6387e-02 (7.8292e-02)	Acc@1  99.22 ( 97.28)	Acc@5 100.00 (100.00)
Epoch: [72][ 30/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.007)	Loss 9.9731e-02 (8.2341e-02)	Acc@1  95.31 ( 97.23)	Acc@5 100.00 (100.00)
Epoch: [72][ 40/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.005)	Loss 6.9336e-02 (8.1941e-02)	Acc@1  99.22 ( 97.33)	Acc@5 100.00 (100.00)
Epoch: [72][ 50/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.004)	Loss 5.3162e-02 (7.9065e-02)	Acc@1  98.44 ( 97.41)	Acc@5 100.00 (100.00)
Epoch: [72][ 60/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.0358e-01 (7.8584e-02)	Acc@1  96.09 ( 97.41)	Acc@5 100.00 (100.00)
Epoch: [72][ 70/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.003)	Loss 5.1392e-02 (7.8398e-02)	Acc@1  97.66 ( 97.37)	Acc@5 100.00 (100.00)
Epoch: [72][ 80/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.1536e-01 (7.8801e-02)	Acc@1  94.53 ( 97.36)	Acc@5 100.00 (100.00)
Epoch: [72][ 90/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.0138e-01 (7.9521e-02)	Acc@1  94.53 ( 97.30)	Acc@5 100.00 (100.00)
Epoch: [72][100/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.8420e-02 (8.0406e-02)	Acc@1  96.09 ( 97.22)	Acc@5 100.00 (100.00)
Epoch: [72][110/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.0822e-02 (7.9562e-02)	Acc@1  98.44 ( 97.27)	Acc@5 100.00 (100.00)
Epoch: [72][120/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7781e-02 (7.8731e-02)	Acc@1  99.22 ( 97.26)	Acc@5 100.00 (100.00)
Epoch: [72][130/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0771e-02 (7.7835e-02)	Acc@1  98.44 ( 97.33)	Acc@5 100.00 (100.00)
Epoch: [72][140/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.0496e-02 (7.6618e-02)	Acc@1  96.88 ( 97.40)	Acc@5 100.00 (100.00)
Epoch: [72][150/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.4087e-02 (7.6602e-02)	Acc@1  97.66 ( 97.40)	Acc@5 100.00 (100.00)
Epoch: [72][160/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4607e-02 (7.5715e-02)	Acc@1  99.22 ( 97.46)	Acc@5 100.00 (100.00)
Epoch: [72][170/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.1829e-02 (7.5689e-02)	Acc@1  97.66 ( 97.44)	Acc@5 100.00 (100.00)
Epoch: [72][180/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.9102e-02 (7.5461e-02)	Acc@1  96.88 ( 97.44)	Acc@5 100.00 (100.00)
Epoch: [72][190/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1194e-01 (7.5468e-02)	Acc@1  95.31 ( 97.47)	Acc@5 100.00 (100.00)
Epoch: [72][200/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1957e-01 (7.5700e-02)	Acc@1  96.09 ( 97.43)	Acc@5 100.00 (100.00)
Epoch: [72][210/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0443e-01 (7.5988e-02)	Acc@1  97.66 ( 97.44)	Acc@5 100.00 (100.00)
Epoch: [72][220/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0162e-01 (7.6377e-02)	Acc@1  97.66 ( 97.41)	Acc@5 100.00 (100.00)
Epoch: [72][230/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.6304e-02 (7.6674e-02)	Acc@1  96.88 ( 97.41)	Acc@5 100.00 (100.00)
Epoch: [72][240/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.5073e-02 (7.7142e-02)	Acc@1  97.66 ( 97.39)	Acc@5 100.00 ( 99.99)
Epoch: [72][250/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.3864e-02 (7.6982e-02)	Acc@1  98.44 ( 97.39)	Acc@5 100.00 ( 99.99)
Epoch: [72][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.2041e-02 (7.7126e-02)	Acc@1  97.66 ( 97.38)	Acc@5 100.00 ( 99.99)
Epoch: [72][270/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.4900e-02 (7.7400e-02)	Acc@1  97.66 ( 97.36)	Acc@5 100.00 ( 99.99)
Epoch: [72][280/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.5552e-02 (7.6971e-02)	Acc@1  97.66 ( 97.37)	Acc@5 100.00 ( 99.99)
Epoch: [72][290/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.4036e-02 (7.6790e-02)	Acc@1  97.66 ( 97.38)	Acc@5 100.00 ( 99.99)
Epoch: [72][300/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.9102e-02 (7.7144e-02)	Acc@1  96.88 ( 97.37)	Acc@5 100.00 ( 99.99)
Epoch: [72][310/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.8918e-02 (7.7370e-02)	Acc@1  97.66 ( 97.36)	Acc@5 100.00 ( 99.99)
Epoch: [72][320/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.8674e-02 (7.7089e-02)	Acc@1  96.88 ( 97.36)	Acc@5 100.00 ( 99.99)
Epoch: [72][330/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.0984e-02 (7.6728e-02)	Acc@1  96.09 ( 97.38)	Acc@5 100.00 ( 99.99)
Epoch: [72][340/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.8757e-02 (7.6392e-02)	Acc@1  99.22 ( 97.40)	Acc@5 100.00 ( 99.99)
Epoch: [72][350/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.7861e-02 (7.5652e-02)	Acc@1  98.44 ( 97.43)	Acc@5 100.00 ( 99.99)
Epoch: [72][360/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.8604e-02 (7.5526e-02)	Acc@1  96.88 ( 97.44)	Acc@5 100.00 ( 99.99)
Epoch: [72][370/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 8.6670e-02 (7.5671e-02)	Acc@1  96.09 ( 97.43)	Acc@5 100.00 ( 99.99)
Epoch: [72][380/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.1361e-02 (7.5989e-02)	Acc@1  97.66 ( 97.41)	Acc@5 100.00 ( 99.99)
Epoch: [72][390/391]	Time  0.026 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.3318e-01 (7.5882e-02)	Acc@1  95.00 ( 97.41)	Acc@5 100.00 ( 99.99)
## e[72] optimizer.zero_grad (sum) time: 0.24997639656066895
## e[72]       loss.backward (sum) time: 4.214510679244995
## e[72]      optimizer.step (sum) time: 1.734844446182251
## epoch[72] training(only) time: 15.93760895729065
# Switched to evaluate mode...
Test: [  0/100]	Time  0.173 ( 0.173)	Loss 2.7612e-01 (2.7612e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.020 ( 0.034)	Loss 3.7305e-01 (2.8740e-01)	Acc@1  89.00 ( 91.55)	Acc@5  99.00 ( 99.73)
Test: [ 20/100]	Time  0.022 ( 0.027)	Loss 3.8208e-01 (3.1027e-01)	Acc@1  88.00 ( 91.14)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.021 ( 0.025)	Loss 4.4873e-01 (3.1991e-01)	Acc@1  86.00 ( 90.77)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.019 ( 0.024)	Loss 3.9551e-01 (3.2111e-01)	Acc@1  89.00 ( 90.76)	Acc@5  99.00 ( 99.68)
Test: [ 50/100]	Time  0.017 ( 0.023)	Loss 2.0447e-01 (3.1334e-01)	Acc@1  93.00 ( 90.86)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.021 ( 0.022)	Loss 3.7573e-01 (3.1303e-01)	Acc@1  91.00 ( 90.75)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.020 ( 0.022)	Loss 4.2993e-01 (3.0881e-01)	Acc@1  87.00 ( 90.75)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.017 ( 0.022)	Loss 1.1298e-01 (3.1385e-01)	Acc@1  95.00 ( 90.60)	Acc@5 100.00 ( 99.70)
Test: [ 90/100]	Time  0.017 ( 0.022)	Loss 2.8027e-01 (3.1274e-01)	Acc@1  94.00 ( 90.67)	Acc@5 100.00 ( 99.73)
 * Acc@1 90.800 Acc@5 99.730
### epoch[72] execution time: 18.18868613243103
EPOCH 73
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [73][  0/391]	Time  0.216 ( 0.216)	Data  0.173 ( 0.173)	Loss 8.8623e-02 (8.8623e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [73][ 10/391]	Time  0.038 ( 0.055)	Data  0.001 ( 0.017)	Loss 7.3730e-02 (7.8219e-02)	Acc@1  97.66 ( 97.37)	Acc@5 100.00 (100.00)
Epoch: [73][ 20/391]	Time  0.042 ( 0.048)	Data  0.001 ( 0.009)	Loss 8.8867e-02 (7.5611e-02)	Acc@1  96.09 ( 97.40)	Acc@5 100.00 (100.00)
Epoch: [73][ 30/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.006)	Loss 3.5614e-02 (6.8710e-02)	Acc@1 100.00 ( 97.68)	Acc@5 100.00 (100.00)
Epoch: [73][ 40/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.005)	Loss 4.8950e-02 (6.9017e-02)	Acc@1  97.66 ( 97.62)	Acc@5 100.00 ( 99.98)
Epoch: [73][ 50/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.004)	Loss 8.3801e-02 (7.3018e-02)	Acc@1  96.09 ( 97.41)	Acc@5 100.00 ( 99.98)
Epoch: [73][ 60/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.004)	Loss 5.7434e-02 (7.2239e-02)	Acc@1  98.44 ( 97.55)	Acc@5 100.00 ( 99.99)
Epoch: [73][ 70/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.1145e-01 (7.1145e-02)	Acc@1  96.88 ( 97.63)	Acc@5  99.22 ( 99.98)
Epoch: [73][ 80/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.5378e-02 (7.4435e-02)	Acc@1  96.88 ( 97.54)	Acc@5 100.00 ( 99.98)
Epoch: [73][ 90/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.2917e-02 (7.4411e-02)	Acc@1  98.44 ( 97.54)	Acc@5 100.00 ( 99.98)
Epoch: [73][100/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 8.1360e-02 (7.3758e-02)	Acc@1  96.88 ( 97.56)	Acc@5 100.00 ( 99.98)
Epoch: [73][110/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 7.3059e-02 (7.4615e-02)	Acc@1  98.44 ( 97.50)	Acc@5 100.00 ( 99.99)
Epoch: [73][120/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.4475e-02 (7.5118e-02)	Acc@1  99.22 ( 97.49)	Acc@5 100.00 ( 99.98)
Epoch: [73][130/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.8848e-02 (7.5465e-02)	Acc@1  96.88 ( 97.49)	Acc@5 100.00 ( 99.98)
Epoch: [73][140/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2384e-01 (7.6028e-02)	Acc@1  96.88 ( 97.49)	Acc@5 100.00 ( 99.98)
Epoch: [73][150/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.4229e-02 (7.6320e-02)	Acc@1  96.88 ( 97.48)	Acc@5 100.00 ( 99.98)
Epoch: [73][160/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.7587e-02 (7.6497e-02)	Acc@1  98.44 ( 97.43)	Acc@5 100.00 ( 99.98)
Epoch: [73][170/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.0811e-02 (7.6139e-02)	Acc@1  98.44 ( 97.45)	Acc@5 100.00 ( 99.98)
Epoch: [73][180/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.5520e-02 (7.7131e-02)	Acc@1  96.09 ( 97.38)	Acc@5 100.00 ( 99.98)
Epoch: [73][190/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.4412e-02 (7.7431e-02)	Acc@1  97.66 ( 97.37)	Acc@5 100.00 ( 99.98)
Epoch: [73][200/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.1665e-02 (7.7255e-02)	Acc@1  97.66 ( 97.40)	Acc@5 100.00 ( 99.98)
Epoch: [73][210/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.6558e-02 (7.7171e-02)	Acc@1  96.09 ( 97.40)	Acc@5 100.00 ( 99.99)
Epoch: [73][220/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.5632e-02 (7.7168e-02)	Acc@1  95.31 ( 97.40)	Acc@5 100.00 ( 99.99)
Epoch: [73][230/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.1370e-02 (7.7629e-02)	Acc@1  96.88 ( 97.40)	Acc@5 100.00 ( 99.99)
Epoch: [73][240/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3843e-01 (7.7555e-02)	Acc@1  95.31 ( 97.40)	Acc@5 100.00 ( 99.99)
Epoch: [73][250/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.5206e-02 (7.7570e-02)	Acc@1  99.22 ( 97.41)	Acc@5 100.00 ( 99.99)
Epoch: [73][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.6650e-02 (7.7190e-02)	Acc@1  98.44 ( 97.42)	Acc@5 100.00 ( 99.99)
Epoch: [73][270/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.0679e-02 (7.6871e-02)	Acc@1  96.88 ( 97.45)	Acc@5 100.00 ( 99.99)
Epoch: [73][280/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 9.7534e-02 (7.7189e-02)	Acc@1  96.88 ( 97.45)	Acc@5 100.00 ( 99.98)
Epoch: [73][290/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 9.3750e-02 (7.6809e-02)	Acc@1  96.88 ( 97.48)	Acc@5 100.00 ( 99.98)
Epoch: [73][300/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.9052e-02 (7.7264e-02)	Acc@1  96.88 ( 97.45)	Acc@5 100.00 ( 99.98)
Epoch: [73][310/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.2067e-01 (7.7228e-02)	Acc@1  95.31 ( 97.45)	Acc@5 100.00 ( 99.98)
Epoch: [73][320/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.5378e-02 (7.6283e-02)	Acc@1  97.66 ( 97.49)	Acc@5 100.00 ( 99.98)
Epoch: [73][330/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.8420e-02 (7.6375e-02)	Acc@1  97.66 ( 97.48)	Acc@5 100.00 ( 99.98)
Epoch: [73][340/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 8.8623e-02 (7.6525e-02)	Acc@1  96.09 ( 97.48)	Acc@5 100.00 ( 99.98)
Epoch: [73][350/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 9.1553e-02 (7.6284e-02)	Acc@1  96.09 ( 97.48)	Acc@5 100.00 ( 99.98)
Epoch: [73][360/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.9153e-02 (7.6716e-02)	Acc@1  97.66 ( 97.47)	Acc@5 100.00 ( 99.98)
Epoch: [73][370/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.5012e-02 (7.6832e-02)	Acc@1  96.88 ( 97.45)	Acc@5 100.00 ( 99.98)
Epoch: [73][380/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.5186e-02 (7.6562e-02)	Acc@1  97.66 ( 97.46)	Acc@5 100.00 ( 99.98)
Epoch: [73][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.1316e-01 (7.6733e-02)	Acc@1  96.25 ( 97.46)	Acc@5 100.00 ( 99.98)
## e[73] optimizer.zero_grad (sum) time: 0.2498013973236084
## e[73]       loss.backward (sum) time: 4.175480604171753
## e[73]      optimizer.step (sum) time: 1.740562915802002
## epoch[73] training(only) time: 15.912273406982422
# Switched to evaluate mode...
Test: [  0/100]	Time  0.170 ( 0.170)	Loss 2.7490e-01 (2.7490e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 3.4277e-01 (2.8784e-01)	Acc@1  89.00 ( 91.18)	Acc@5  99.00 ( 99.73)
Test: [ 20/100]	Time  0.022 ( 0.028)	Loss 3.7085e-01 (3.1193e-01)	Acc@1  89.00 ( 91.24)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.022 ( 0.026)	Loss 4.4775e-01 (3.2117e-01)	Acc@1  87.00 ( 90.90)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.022 ( 0.025)	Loss 3.9819e-01 (3.2112e-01)	Acc@1  90.00 ( 90.93)	Acc@5  99.00 ( 99.68)
Test: [ 50/100]	Time  0.021 ( 0.024)	Loss 1.8445e-01 (3.1374e-01)	Acc@1  94.00 ( 91.02)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.023 ( 0.023)	Loss 3.6890e-01 (3.1262e-01)	Acc@1  91.00 ( 90.90)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.023 ( 0.023)	Loss 4.6899e-01 (3.0898e-01)	Acc@1  87.00 ( 90.86)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.018 ( 0.023)	Loss 1.0468e-01 (3.1421e-01)	Acc@1  95.00 ( 90.68)	Acc@5 100.00 ( 99.73)
Test: [ 90/100]	Time  0.023 ( 0.022)	Loss 2.7661e-01 (3.1333e-01)	Acc@1  93.00 ( 90.78)	Acc@5 100.00 ( 99.74)
 * Acc@1 90.920 Acc@5 99.740
### epoch[73] execution time: 18.23153519630432
EPOCH 74
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [74][  0/391]	Time  0.217 ( 0.217)	Data  0.175 ( 0.175)	Loss 4.1718e-02 (4.1718e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [74][ 10/391]	Time  0.041 ( 0.056)	Data  0.001 ( 0.017)	Loss 8.7646e-02 (7.3945e-02)	Acc@1  96.88 ( 97.51)	Acc@5 100.00 (100.00)
Epoch: [74][ 20/391]	Time  0.041 ( 0.049)	Data  0.001 ( 0.009)	Loss 9.7839e-02 (7.4021e-02)	Acc@1  96.88 ( 97.51)	Acc@5 100.00 ( 99.96)
Epoch: [74][ 30/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.006)	Loss 1.1664e-01 (8.0658e-02)	Acc@1  96.88 ( 97.15)	Acc@5 100.00 ( 99.97)
Epoch: [74][ 40/391]	Time  0.043 ( 0.045)	Data  0.001 ( 0.005)	Loss 6.6223e-02 (7.8674e-02)	Acc@1  97.66 ( 97.28)	Acc@5 100.00 ( 99.94)
Epoch: [74][ 50/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.2183e-01 (7.6730e-02)	Acc@1  94.53 ( 97.27)	Acc@5 100.00 ( 99.95)
Epoch: [74][ 60/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 8.8440e-02 (7.6506e-02)	Acc@1  97.66 ( 97.31)	Acc@5 100.00 ( 99.96)
Epoch: [74][ 70/391]	Time  0.034 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.4779e-02 (7.4906e-02)	Acc@1  98.44 ( 97.38)	Acc@5 100.00 ( 99.97)
Epoch: [74][ 80/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.6610e-02 (7.4083e-02)	Acc@1  97.66 ( 97.38)	Acc@5 100.00 ( 99.97)
Epoch: [74][ 90/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.8401e-02 (7.3199e-02)	Acc@1  98.44 ( 97.42)	Acc@5 100.00 ( 99.97)
Epoch: [74][100/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.003)	Loss 5.9235e-02 (7.2120e-02)	Acc@1  97.66 ( 97.47)	Acc@5 100.00 ( 99.98)
Epoch: [74][110/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.8694e-02 (7.1252e-02)	Acc@1  97.66 ( 97.55)	Acc@5 100.00 ( 99.98)
Epoch: [74][120/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.3984e-02 (7.2188e-02)	Acc@1  96.88 ( 97.49)	Acc@5 100.00 ( 99.98)
Epoch: [74][130/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2067e-01 (7.2993e-02)	Acc@1  96.09 ( 97.48)	Acc@5 100.00 ( 99.98)
Epoch: [74][140/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1450e-01 (7.3267e-02)	Acc@1  95.31 ( 97.46)	Acc@5 100.00 ( 99.98)
Epoch: [74][150/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0437e-01 (7.3588e-02)	Acc@1  96.09 ( 97.48)	Acc@5 100.00 ( 99.98)
Epoch: [74][160/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.5267e-02 (7.3423e-02)	Acc@1  97.66 ( 97.48)	Acc@5 100.00 ( 99.98)
Epoch: [74][170/391]	Time  0.041 ( 0.041)	Data  0.002 ( 0.002)	Loss 5.1086e-02 (7.2998e-02)	Acc@1  97.66 ( 97.49)	Acc@5 100.00 ( 99.98)
Epoch: [74][180/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.4087e-02 (7.2630e-02)	Acc@1  99.22 ( 97.53)	Acc@5 100.00 ( 99.98)
Epoch: [74][190/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.8064e-02 (7.3167e-02)	Acc@1  96.88 ( 97.54)	Acc@5 100.00 ( 99.98)
Epoch: [74][200/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.5083e-02 (7.3804e-02)	Acc@1  97.66 ( 97.51)	Acc@5 100.00 ( 99.98)
Epoch: [74][210/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.5125e-02 (7.3975e-02)	Acc@1  99.22 ( 97.52)	Acc@5 100.00 ( 99.99)
Epoch: [74][220/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8798e-02 (7.3821e-02)	Acc@1  99.22 ( 97.53)	Acc@5 100.00 ( 99.99)
Epoch: [74][230/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.2520e-02 (7.3327e-02)	Acc@1  97.66 ( 97.54)	Acc@5 100.00 ( 99.99)
Epoch: [74][240/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.2998e-02 (7.3622e-02)	Acc@1  97.66 ( 97.52)	Acc@5 100.00 ( 99.99)
Epoch: [74][250/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8879e-02 (7.3686e-02)	Acc@1  99.22 ( 97.53)	Acc@5 100.00 ( 99.99)
Epoch: [74][260/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3196e-01 (7.3974e-02)	Acc@1  94.53 ( 97.52)	Acc@5 100.00 ( 99.99)
Epoch: [74][270/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1666e-02 (7.3738e-02)	Acc@1  99.22 ( 97.51)	Acc@5 100.00 ( 99.99)
Epoch: [74][280/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4576e-02 (7.3850e-02)	Acc@1  99.22 ( 97.51)	Acc@5 100.00 ( 99.99)
Epoch: [74][290/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.6609e-02 (7.4231e-02)	Acc@1  96.09 ( 97.50)	Acc@5 100.00 ( 99.99)
Epoch: [74][300/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 9.9854e-02 (7.3829e-02)	Acc@1  96.88 ( 97.52)	Acc@5 100.00 ( 99.99)
Epoch: [74][310/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.8430e-02 (7.3768e-02)	Acc@1  97.66 ( 97.52)	Acc@5 100.00 ( 99.99)
Epoch: [74][320/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.3669e-02 (7.3515e-02)	Acc@1  96.88 ( 97.53)	Acc@5 100.00 ( 99.99)
Epoch: [74][330/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.5308e-01 (7.3969e-02)	Acc@1  94.53 ( 97.52)	Acc@5 100.00 ( 99.99)
Epoch: [74][340/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.2964e-02 (7.3505e-02)	Acc@1 100.00 ( 97.55)	Acc@5 100.00 ( 99.99)
Epoch: [74][350/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.8624e-02 (7.3135e-02)	Acc@1  98.44 ( 97.56)	Acc@5 100.00 ( 99.99)
Epoch: [74][360/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 9.1125e-02 (7.3001e-02)	Acc@1  97.66 ( 97.56)	Acc@5 100.00 ( 99.99)
Epoch: [74][370/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.1731e-01 (7.3044e-02)	Acc@1  96.09 ( 97.56)	Acc@5 100.00 ( 99.99)
Epoch: [74][380/391]	Time  0.043 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.7773e-01 (7.3409e-02)	Acc@1  94.53 ( 97.55)	Acc@5 100.00 ( 99.99)
Epoch: [74][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.1279e-01 (7.3427e-02)	Acc@1  96.25 ( 97.55)	Acc@5 100.00 ( 99.99)
## e[74] optimizer.zero_grad (sum) time: 0.24947214126586914
## e[74]       loss.backward (sum) time: 4.189168453216553
## e[74]      optimizer.step (sum) time: 1.6939995288848877
## epoch[74] training(only) time: 15.925747632980347
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 2.7612e-01 (2.7612e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.017 ( 0.031)	Loss 3.6426e-01 (2.8607e-01)	Acc@1  88.00 ( 91.36)	Acc@5  99.00 ( 99.73)
Test: [ 20/100]	Time  0.023 ( 0.025)	Loss 3.6841e-01 (3.0924e-01)	Acc@1  88.00 ( 91.24)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.022 ( 0.024)	Loss 4.4360e-01 (3.1667e-01)	Acc@1  86.00 ( 90.97)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.022 ( 0.023)	Loss 4.0210e-01 (3.1775e-01)	Acc@1  90.00 ( 91.05)	Acc@5  99.00 ( 99.68)
Test: [ 50/100]	Time  0.020 ( 0.022)	Loss 1.9836e-01 (3.1013e-01)	Acc@1  93.00 ( 91.12)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.022 ( 0.022)	Loss 3.8403e-01 (3.0986e-01)	Acc@1  91.00 ( 90.93)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.023 ( 0.022)	Loss 4.3921e-01 (3.0645e-01)	Acc@1  87.00 ( 90.87)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.017 ( 0.021)	Loss 9.9609e-02 (3.1146e-01)	Acc@1  93.00 ( 90.67)	Acc@5 100.00 ( 99.70)
Test: [ 90/100]	Time  0.021 ( 0.021)	Loss 2.5684e-01 (3.1050e-01)	Acc@1  94.00 ( 90.77)	Acc@5 100.00 ( 99.71)
 * Acc@1 90.900 Acc@5 99.720
### epoch[74] execution time: 18.12679934501648
EPOCH 75
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [75][  0/391]	Time  0.218 ( 0.218)	Data  0.177 ( 0.177)	Loss 9.5886e-02 (9.5886e-02)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [75][ 10/391]	Time  0.042 ( 0.056)	Data  0.001 ( 0.017)	Loss 7.2815e-02 (8.1521e-02)	Acc@1  96.88 ( 97.37)	Acc@5 100.00 (100.00)
Epoch: [75][ 20/391]	Time  0.042 ( 0.049)	Data  0.001 ( 0.009)	Loss 3.3356e-02 (7.6683e-02)	Acc@1 100.00 ( 97.54)	Acc@5 100.00 (100.00)
Epoch: [75][ 30/391]	Time  0.042 ( 0.046)	Data  0.001 ( 0.007)	Loss 3.9886e-02 (7.7836e-02)	Acc@1  98.44 ( 97.53)	Acc@5 100.00 (100.00)
Epoch: [75][ 40/391]	Time  0.038 ( 0.045)	Data  0.001 ( 0.005)	Loss 4.8950e-02 (7.4370e-02)	Acc@1  99.22 ( 97.62)	Acc@5 100.00 (100.00)
Epoch: [75][ 50/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.004)	Loss 7.9834e-02 (7.2933e-02)	Acc@1  96.88 ( 97.63)	Acc@5 100.00 (100.00)
Epoch: [75][ 60/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 5.5176e-02 (7.3512e-02)	Acc@1  98.44 ( 97.58)	Acc@5 100.00 ( 99.97)
Epoch: [75][ 70/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.4302e-02 (7.3864e-02)	Acc@1  99.22 ( 97.52)	Acc@5 100.00 ( 99.98)
Epoch: [75][ 80/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.3671e-02 (7.4058e-02)	Acc@1  99.22 ( 97.50)	Acc@5 100.00 ( 99.98)
Epoch: [75][ 90/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.9734e-02 (7.2269e-02)	Acc@1  99.22 ( 97.60)	Acc@5 100.00 ( 99.98)
Epoch: [75][100/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.2494e-01 (7.3290e-02)	Acc@1  95.31 ( 97.56)	Acc@5 100.00 ( 99.98)
Epoch: [75][110/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.003)	Loss 1.1096e-01 (7.4460e-02)	Acc@1  96.88 ( 97.52)	Acc@5 100.00 ( 99.99)
Epoch: [75][120/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4128e-02 (7.3410e-02)	Acc@1  99.22 ( 97.55)	Acc@5 100.00 ( 99.99)
Epoch: [75][130/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1229e-02 (7.1675e-02)	Acc@1  98.44 ( 97.60)	Acc@5 100.00 ( 99.99)
Epoch: [75][140/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2358e-02 (7.1673e-02)	Acc@1  98.44 ( 97.57)	Acc@5 100.00 ( 99.99)
Epoch: [75][150/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.8305e-02 (7.1990e-02)	Acc@1  99.22 ( 97.57)	Acc@5 100.00 ( 99.99)
Epoch: [75][160/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8370e-02 (7.2674e-02)	Acc@1  98.44 ( 97.54)	Acc@5 100.00 ( 99.99)
Epoch: [75][170/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.7261e-02 (7.2765e-02)	Acc@1  96.88 ( 97.53)	Acc@5 100.00 ( 99.99)
Epoch: [75][180/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.9285e-02 (7.3134e-02)	Acc@1  97.66 ( 97.54)	Acc@5 100.00 ( 99.99)
Epoch: [75][190/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.2510e-02 (7.2898e-02)	Acc@1  97.66 ( 97.55)	Acc@5 100.00 ( 99.99)
Epoch: [75][200/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.4707e-02 (7.3292e-02)	Acc@1  98.44 ( 97.56)	Acc@5 100.00 ( 99.98)
Epoch: [75][210/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1487e-01 (7.2764e-02)	Acc@1  96.88 ( 97.58)	Acc@5 100.00 ( 99.99)
Epoch: [75][220/391]	Time  0.045 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.6875e-02 (7.2648e-02)	Acc@1  99.22 ( 97.59)	Acc@5 100.00 ( 99.98)
Epoch: [75][230/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6469e-02 (7.3331e-02)	Acc@1  99.22 ( 97.56)	Acc@5 100.00 ( 99.98)
Epoch: [75][240/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.2622e-02 (7.3241e-02)	Acc@1  96.88 ( 97.58)	Acc@5 100.00 ( 99.98)
Epoch: [75][250/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.4890e-02 (7.3447e-02)	Acc@1  96.09 ( 97.55)	Acc@5 100.00 ( 99.98)
Epoch: [75][260/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.8328e-02 (7.3844e-02)	Acc@1  96.09 ( 97.55)	Acc@5 100.00 ( 99.98)
Epoch: [75][270/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.5562e-02 (7.3858e-02)	Acc@1  97.66 ( 97.54)	Acc@5 100.00 ( 99.98)
Epoch: [75][280/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.3497e-02 (7.3319e-02)	Acc@1  98.44 ( 97.57)	Acc@5 100.00 ( 99.98)
Epoch: [75][290/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.9426e-02 (7.3990e-02)	Acc@1  97.66 ( 97.54)	Acc@5 100.00 ( 99.98)
Epoch: [75][300/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.1431e-02 (7.4109e-02)	Acc@1  96.88 ( 97.54)	Acc@5 100.00 ( 99.98)
Epoch: [75][310/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0193e-01 (7.4370e-02)	Acc@1  96.88 ( 97.52)	Acc@5 100.00 ( 99.98)
Epoch: [75][320/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 9.2651e-02 (7.4445e-02)	Acc@1  96.09 ( 97.52)	Acc@5 100.00 ( 99.99)
Epoch: [75][330/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 8.7891e-02 (7.4526e-02)	Acc@1  95.31 ( 97.51)	Acc@5 100.00 ( 99.99)
Epoch: [75][340/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.001)	Loss 8.1421e-02 (7.4461e-02)	Acc@1  96.88 ( 97.52)	Acc@5 100.00 ( 99.98)
Epoch: [75][350/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.1310e-02 (7.4757e-02)	Acc@1  97.66 ( 97.50)	Acc@5 100.00 ( 99.98)
Epoch: [75][360/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.9734e-02 (7.4580e-02)	Acc@1  99.22 ( 97.51)	Acc@5 100.00 ( 99.98)
Epoch: [75][370/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.1533e-02 (7.4535e-02)	Acc@1  97.66 ( 97.50)	Acc@5 100.00 ( 99.99)
Epoch: [75][380/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.7943e-02 (7.4468e-02)	Acc@1  98.44 ( 97.51)	Acc@5 100.00 ( 99.98)
Epoch: [75][390/391]	Time  0.027 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.0193e-01 (7.4288e-02)	Acc@1  96.25 ( 97.51)	Acc@5 100.00 ( 99.98)
## e[75] optimizer.zero_grad (sum) time: 0.24953413009643555
## e[75]       loss.backward (sum) time: 4.200900077819824
## e[75]      optimizer.step (sum) time: 1.7555279731750488
## epoch[75] training(only) time: 15.902446269989014
# Switched to evaluate mode...
Test: [  0/100]	Time  0.172 ( 0.172)	Loss 2.8809e-01 (2.8809e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.018 ( 0.034)	Loss 3.9331e-01 (2.8834e-01)	Acc@1  87.00 ( 91.45)	Acc@5  99.00 ( 99.73)
Test: [ 20/100]	Time  0.022 ( 0.028)	Loss 3.6865e-01 (3.0751e-01)	Acc@1  87.00 ( 91.10)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.017 ( 0.025)	Loss 4.5068e-01 (3.1589e-01)	Acc@1  86.00 ( 90.74)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.021 ( 0.024)	Loss 4.1846e-01 (3.1897e-01)	Acc@1  87.00 ( 90.66)	Acc@5  99.00 ( 99.66)
Test: [ 50/100]	Time  0.021 ( 0.023)	Loss 1.9861e-01 (3.1120e-01)	Acc@1  94.00 ( 90.82)	Acc@5 100.00 ( 99.69)
Test: [ 60/100]	Time  0.022 ( 0.023)	Loss 3.6646e-01 (3.1137e-01)	Acc@1  92.00 ( 90.67)	Acc@5 100.00 ( 99.72)
Test: [ 70/100]	Time  0.017 ( 0.022)	Loss 4.3921e-01 (3.0779e-01)	Acc@1  88.00 ( 90.69)	Acc@5 100.00 ( 99.72)
Test: [ 80/100]	Time  0.023 ( 0.022)	Loss 1.0553e-01 (3.1225e-01)	Acc@1  94.00 ( 90.57)	Acc@5 100.00 ( 99.72)
Test: [ 90/100]	Time  0.024 ( 0.022)	Loss 2.5220e-01 (3.1053e-01)	Acc@1  94.00 ( 90.71)	Acc@5 100.00 ( 99.74)
 * Acc@1 90.830 Acc@5 99.750
### epoch[75] execution time: 18.204346179962158
EPOCH 76
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [76][  0/391]	Time  0.221 ( 0.221)	Data  0.181 ( 0.181)	Loss 5.7648e-02 (5.7648e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [76][ 10/391]	Time  0.040 ( 0.056)	Data  0.001 ( 0.017)	Loss 1.2219e-01 (8.8101e-02)	Acc@1  96.88 ( 97.02)	Acc@5 100.00 (100.00)
Epoch: [76][ 20/391]	Time  0.038 ( 0.049)	Data  0.001 ( 0.009)	Loss 7.2144e-02 (7.6577e-02)	Acc@1  97.66 ( 97.51)	Acc@5 100.00 (100.00)
Epoch: [76][ 30/391]	Time  0.040 ( 0.046)	Data  0.001 ( 0.007)	Loss 7.0129e-02 (7.5311e-02)	Acc@1  96.88 ( 97.45)	Acc@5 100.00 (100.00)
Epoch: [76][ 40/391]	Time  0.038 ( 0.045)	Data  0.001 ( 0.005)	Loss 7.9468e-02 (7.1665e-02)	Acc@1  97.66 ( 97.56)	Acc@5 100.00 (100.00)
Epoch: [76][ 50/391]	Time  0.043 ( 0.044)	Data  0.001 ( 0.004)	Loss 5.8502e-02 (7.3084e-02)	Acc@1  97.66 ( 97.53)	Acc@5 100.00 (100.00)
Epoch: [76][ 60/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.004)	Loss 3.9429e-02 (7.4533e-02)	Acc@1  98.44 ( 97.45)	Acc@5 100.00 (100.00)
Epoch: [76][ 70/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.7434e-02 (7.5963e-02)	Acc@1  99.22 ( 97.47)	Acc@5 100.00 (100.00)
Epoch: [76][ 80/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.3721e-02 (7.5339e-02)	Acc@1  97.66 ( 97.53)	Acc@5 100.00 (100.00)
Epoch: [76][ 90/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.0455e-02 (7.4797e-02)	Acc@1  96.88 ( 97.53)	Acc@5 100.00 (100.00)
Epoch: [76][100/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.6223e-02 (7.4069e-02)	Acc@1  99.22 ( 97.54)	Acc@5 100.00 (100.00)
Epoch: [76][110/391]	Time  0.050 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.2388e-02 (7.3084e-02)	Acc@1  98.44 ( 97.58)	Acc@5 100.00 (100.00)
Epoch: [76][120/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1688e-01 (7.2530e-02)	Acc@1  96.09 ( 97.57)	Acc@5 100.00 (100.00)
Epoch: [76][130/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.4036e-02 (7.2422e-02)	Acc@1  97.66 ( 97.57)	Acc@5 100.00 (100.00)
Epoch: [76][140/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2898e-02 (7.2201e-02)	Acc@1 100.00 ( 97.56)	Acc@5 100.00 (100.00)
Epoch: [76][150/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.3772e-02 (7.2641e-02)	Acc@1  98.44 ( 97.57)	Acc@5 100.00 ( 99.99)
Epoch: [76][160/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.8481e-02 (7.2730e-02)	Acc@1  98.44 ( 97.58)	Acc@5 100.00 ( 99.99)
Epoch: [76][170/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.1421e-02 (7.3063e-02)	Acc@1  96.88 ( 97.57)	Acc@5 100.00 ( 99.99)
Epoch: [76][180/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.9143e-02 (7.2621e-02)	Acc@1  98.44 ( 97.60)	Acc@5 100.00 ( 99.99)
Epoch: [76][190/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.4412e-02 (7.2529e-02)	Acc@1  98.44 ( 97.62)	Acc@5 100.00 ( 99.99)
Epoch: [76][200/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.0089e-02 (7.1797e-02)	Acc@1  97.66 ( 97.65)	Acc@5 100.00 ( 99.99)
Epoch: [76][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.7585e-02 (7.1922e-02)	Acc@1  96.09 ( 97.62)	Acc@5 100.00 ( 99.99)
Epoch: [76][220/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.7759e-02 (7.2139e-02)	Acc@1  96.88 ( 97.61)	Acc@5 100.00 ( 99.99)
Epoch: [76][230/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.0496e-02 (7.1966e-02)	Acc@1  96.88 ( 97.62)	Acc@5 100.00 ( 99.99)
Epoch: [76][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.6365e-02 (7.2160e-02)	Acc@1  96.88 ( 97.60)	Acc@5 100.00 ( 99.99)
Epoch: [76][250/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.9541e-02 (7.2962e-02)	Acc@1  99.22 ( 97.54)	Acc@5 100.00 ( 99.99)
Epoch: [76][260/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.6670e-02 (7.3166e-02)	Acc@1  97.66 ( 97.56)	Acc@5 100.00 ( 99.99)
Epoch: [76][270/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2268e-01 (7.3425e-02)	Acc@1  96.09 ( 97.55)	Acc@5 100.00 ( 99.99)
Epoch: [76][280/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.4412e-02 (7.3407e-02)	Acc@1  96.88 ( 97.55)	Acc@5 100.00 ( 99.99)
Epoch: [76][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.2155e-02 (7.3784e-02)	Acc@1  98.44 ( 97.55)	Acc@5 100.00 ( 99.99)
Epoch: [76][300/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.7983e-02 (7.3412e-02)	Acc@1  98.44 ( 97.56)	Acc@5 100.00 ( 99.99)
Epoch: [76][310/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.3242e-02 (7.3419e-02)	Acc@1  98.44 ( 97.56)	Acc@5 100.00 ( 99.99)
Epoch: [76][320/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 9.4666e-02 (7.3474e-02)	Acc@1  96.09 ( 97.56)	Acc@5  99.22 ( 99.99)
Epoch: [76][330/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 8.1299e-02 (7.3421e-02)	Acc@1  96.09 ( 97.56)	Acc@5 100.00 ( 99.99)
Epoch: [76][340/391]	Time  0.052 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.3030e-02 (7.2941e-02)	Acc@1  99.22 ( 97.59)	Acc@5 100.00 ( 99.99)
Epoch: [76][350/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 9.9121e-02 (7.2980e-02)	Acc@1  96.88 ( 97.59)	Acc@5 100.00 ( 99.99)
Epoch: [76][360/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.5227e-02 (7.2876e-02)	Acc@1  97.66 ( 97.59)	Acc@5 100.00 ( 99.99)
Epoch: [76][370/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.9214e-02 (7.2911e-02)	Acc@1  97.66 ( 97.59)	Acc@5 100.00 ( 99.99)
Epoch: [76][380/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.6265e-02 (7.3041e-02)	Acc@1  99.22 ( 97.58)	Acc@5 100.00 ( 99.99)
Epoch: [76][390/391]	Time  0.030 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.3773e-02 (7.2947e-02)	Acc@1  98.75 ( 97.59)	Acc@5 100.00 ( 99.99)
## e[76] optimizer.zero_grad (sum) time: 0.24942517280578613
## e[76]       loss.backward (sum) time: 4.183794260025024
## e[76]      optimizer.step (sum) time: 1.7205817699432373
## epoch[76] training(only) time: 15.887789726257324
# Switched to evaluate mode...
Test: [  0/100]	Time  0.172 ( 0.172)	Loss 2.7588e-01 (2.7588e-01)	Acc@1  93.00 ( 93.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.024 ( 0.034)	Loss 3.8403e-01 (2.8911e-01)	Acc@1  88.00 ( 91.55)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.020 ( 0.028)	Loss 3.8257e-01 (3.1081e-01)	Acc@1  88.00 ( 91.10)	Acc@5 100.00 ( 99.57)
Test: [ 30/100]	Time  0.017 ( 0.026)	Loss 4.5581e-01 (3.2013e-01)	Acc@1  88.00 ( 90.90)	Acc@5 100.00 ( 99.65)
Test: [ 40/100]	Time  0.017 ( 0.024)	Loss 3.8818e-01 (3.2165e-01)	Acc@1  89.00 ( 90.83)	Acc@5  99.00 ( 99.63)
Test: [ 50/100]	Time  0.016 ( 0.023)	Loss 1.9971e-01 (3.1431e-01)	Acc@1  93.00 ( 90.92)	Acc@5 100.00 ( 99.67)
Test: [ 60/100]	Time  0.016 ( 0.022)	Loss 3.8916e-01 (3.1390e-01)	Acc@1  91.00 ( 90.82)	Acc@5 100.00 ( 99.70)
Test: [ 70/100]	Time  0.019 ( 0.022)	Loss 4.1309e-01 (3.0943e-01)	Acc@1  87.00 ( 90.73)	Acc@5 100.00 ( 99.70)
Test: [ 80/100]	Time  0.017 ( 0.021)	Loss 1.0577e-01 (3.1412e-01)	Acc@1  95.00 ( 90.62)	Acc@5 100.00 ( 99.68)
Test: [ 90/100]	Time  0.017 ( 0.021)	Loss 2.8223e-01 (3.1273e-01)	Acc@1  94.00 ( 90.73)	Acc@5 100.00 ( 99.70)
 * Acc@1 90.870 Acc@5 99.710
### epoch[76] execution time: 18.09142255783081
EPOCH 77
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [77][  0/391]	Time  0.215 ( 0.215)	Data  0.172 ( 0.172)	Loss 1.0217e-01 (1.0217e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [77][ 10/391]	Time  0.040 ( 0.056)	Data  0.001 ( 0.016)	Loss 5.9082e-02 (6.9466e-02)	Acc@1  99.22 ( 98.01)	Acc@5 100.00 (100.00)
Epoch: [77][ 20/391]	Time  0.038 ( 0.048)	Data  0.001 ( 0.009)	Loss 9.3079e-02 (7.2035e-02)	Acc@1  97.66 ( 97.73)	Acc@5 100.00 (100.00)
Epoch: [77][ 30/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.006)	Loss 6.8481e-02 (7.3592e-02)	Acc@1  98.44 ( 97.68)	Acc@5 100.00 (100.00)
Epoch: [77][ 40/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.005)	Loss 8.9844e-02 (7.4810e-02)	Acc@1  98.44 ( 97.79)	Acc@5 100.00 (100.00)
Epoch: [77][ 50/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.1017e-01 (7.3933e-02)	Acc@1  96.09 ( 97.79)	Acc@5 100.00 (100.00)
Epoch: [77][ 60/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 5.2582e-02 (7.4546e-02)	Acc@1  98.44 ( 97.72)	Acc@5 100.00 ( 99.99)
Epoch: [77][ 70/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.6477e-02 (7.2700e-02)	Acc@1  97.66 ( 97.74)	Acc@5 100.00 ( 99.99)
Epoch: [77][ 80/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.2816e-02 (7.3545e-02)	Acc@1 100.00 ( 97.68)	Acc@5 100.00 ( 99.98)
Epoch: [77][ 90/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.9143e-02 (7.4286e-02)	Acc@1  98.44 ( 97.66)	Acc@5  99.22 ( 99.97)
Epoch: [77][100/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.8970e-02 (7.4654e-02)	Acc@1  96.88 ( 97.60)	Acc@5 100.00 ( 99.98)
Epoch: [77][110/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.9824e-02 (7.5017e-02)	Acc@1  96.88 ( 97.57)	Acc@5 100.00 ( 99.97)
Epoch: [77][120/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.7985e-02 (7.4075e-02)	Acc@1 100.00 ( 97.59)	Acc@5 100.00 ( 99.97)
Epoch: [77][130/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.4177e-02 (7.2726e-02)	Acc@1  96.88 ( 97.66)	Acc@5 100.00 ( 99.98)
Epoch: [77][140/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.3529e-02 (7.2284e-02)	Acc@1 100.00 ( 97.67)	Acc@5 100.00 ( 99.97)
Epoch: [77][150/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.4138e-02 (7.2019e-02)	Acc@1  97.66 ( 97.65)	Acc@5 100.00 ( 99.97)
Epoch: [77][160/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.1533e-02 (7.1816e-02)	Acc@1  98.44 ( 97.67)	Acc@5 100.00 ( 99.98)
Epoch: [77][170/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3660e-01 (7.1591e-02)	Acc@1  95.31 ( 97.68)	Acc@5 100.00 ( 99.98)
Epoch: [77][180/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4220e-02 (7.1700e-02)	Acc@1  98.44 ( 97.66)	Acc@5 100.00 ( 99.98)
Epoch: [77][190/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4856e-01 (7.2251e-02)	Acc@1  92.19 ( 97.63)	Acc@5 100.00 ( 99.98)
Epoch: [77][200/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0309e-01 (7.2726e-02)	Acc@1  96.09 ( 97.57)	Acc@5 100.00 ( 99.98)
Epoch: [77][210/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.9662e-02 (7.2783e-02)	Acc@1  96.88 ( 97.59)	Acc@5 100.00 ( 99.98)
Epoch: [77][220/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.5573e-02 (7.2259e-02)	Acc@1  97.66 ( 97.60)	Acc@5 100.00 ( 99.98)
Epoch: [77][230/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4478e-01 (7.2915e-02)	Acc@1  96.09 ( 97.57)	Acc@5 100.00 ( 99.98)
Epoch: [77][240/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.2468e-02 (7.3048e-02)	Acc@1  96.88 ( 97.58)	Acc@5 100.00 ( 99.98)
Epoch: [77][250/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9561e-02 (7.3368e-02)	Acc@1  99.22 ( 97.55)	Acc@5 100.00 ( 99.98)
Epoch: [77][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.0688e-02 (7.3523e-02)	Acc@1  97.66 ( 97.55)	Acc@5 100.00 ( 99.98)
Epoch: [77][270/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.2581e-02 (7.4047e-02)	Acc@1  97.66 ( 97.52)	Acc@5 100.00 ( 99.98)
Epoch: [77][280/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.5876e-02 (7.3760e-02)	Acc@1  97.66 ( 97.53)	Acc@5 100.00 ( 99.98)
Epoch: [77][290/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 9.2407e-02 (7.4071e-02)	Acc@1  96.09 ( 97.52)	Acc@5 100.00 ( 99.98)
Epoch: [77][300/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.6854e-02 (7.4065e-02)	Acc@1  97.66 ( 97.52)	Acc@5 100.00 ( 99.98)
Epoch: [77][310/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.2915e-01 (7.3955e-02)	Acc@1  95.31 ( 97.51)	Acc@5 100.00 ( 99.98)
Epoch: [77][320/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.6224e-02 (7.4300e-02)	Acc@1  99.22 ( 97.51)	Acc@5 100.00 ( 99.98)
Epoch: [77][330/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.8737e-02 (7.3759e-02)	Acc@1  99.22 ( 97.54)	Acc@5 100.00 ( 99.98)
Epoch: [77][340/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.0374e-02 (7.3441e-02)	Acc@1  98.44 ( 97.55)	Acc@5 100.00 ( 99.98)
Epoch: [77][350/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.8177e-02 (7.3371e-02)	Acc@1 100.00 ( 97.56)	Acc@5 100.00 ( 99.98)
Epoch: [77][360/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.2461e-02 (7.3227e-02)	Acc@1 100.00 ( 97.58)	Acc@5 100.00 ( 99.98)
Epoch: [77][370/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 8.3679e-02 (7.3032e-02)	Acc@1  97.66 ( 97.59)	Acc@5 100.00 ( 99.98)
Epoch: [77][380/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.0883e-01 (7.3804e-02)	Acc@1  96.09 ( 97.57)	Acc@5 100.00 ( 99.98)
Epoch: [77][390/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.1536e-01 (7.3780e-02)	Acc@1  97.50 ( 97.56)	Acc@5 100.00 ( 99.98)
## e[77] optimizer.zero_grad (sum) time: 0.2500143051147461
## e[77]       loss.backward (sum) time: 4.2178308963775635
## e[77]      optimizer.step (sum) time: 1.7303175926208496
## epoch[77] training(only) time: 15.949724674224854
# Switched to evaluate mode...
Test: [  0/100]	Time  0.165 ( 0.165)	Loss 2.6074e-01 (2.6074e-01)	Acc@1  93.00 ( 93.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.026 ( 0.033)	Loss 3.4326e-01 (2.8852e-01)	Acc@1  88.00 ( 91.09)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.021 ( 0.027)	Loss 3.7939e-01 (3.1310e-01)	Acc@1  87.00 ( 91.00)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.020 ( 0.025)	Loss 4.4897e-01 (3.1849e-01)	Acc@1  86.00 ( 90.90)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.022 ( 0.024)	Loss 3.9990e-01 (3.1951e-01)	Acc@1  90.00 ( 90.90)	Acc@5  99.00 ( 99.66)
Test: [ 50/100]	Time  0.020 ( 0.023)	Loss 1.8994e-01 (3.1293e-01)	Acc@1  93.00 ( 90.92)	Acc@5 100.00 ( 99.69)
Test: [ 60/100]	Time  0.020 ( 0.023)	Loss 3.8550e-01 (3.1179e-01)	Acc@1  91.00 ( 90.87)	Acc@5 100.00 ( 99.72)
Test: [ 70/100]	Time  0.020 ( 0.023)	Loss 4.4238e-01 (3.0861e-01)	Acc@1  86.00 ( 90.79)	Acc@5 100.00 ( 99.72)
Test: [ 80/100]	Time  0.021 ( 0.022)	Loss 9.4360e-02 (3.1363e-01)	Acc@1  95.00 ( 90.62)	Acc@5 100.00 ( 99.69)
Test: [ 90/100]	Time  0.019 ( 0.022)	Loss 2.5317e-01 (3.1275e-01)	Acc@1  93.00 ( 90.68)	Acc@5 100.00 ( 99.70)
 * Acc@1 90.810 Acc@5 99.720
### epoch[77] execution time: 18.292927980422974
EPOCH 78
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [78][  0/391]	Time  0.214 ( 0.214)	Data  0.173 ( 0.173)	Loss 6.4148e-02 (6.4148e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [78][ 10/391]	Time  0.038 ( 0.056)	Data  0.001 ( 0.017)	Loss 3.7018e-02 (7.1741e-02)	Acc@1 100.00 ( 97.80)	Acc@5 100.00 (100.00)
Epoch: [78][ 20/391]	Time  0.044 ( 0.049)	Data  0.001 ( 0.009)	Loss 8.3313e-02 (7.4614e-02)	Acc@1  96.09 ( 97.62)	Acc@5 100.00 (100.00)
Epoch: [78][ 30/391]	Time  0.038 ( 0.046)	Data  0.001 ( 0.006)	Loss 4.1168e-02 (7.3850e-02)	Acc@1  99.22 ( 97.76)	Acc@5 100.00 (100.00)
Epoch: [78][ 40/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.005)	Loss 6.3660e-02 (7.0243e-02)	Acc@1  96.88 ( 97.75)	Acc@5 100.00 (100.00)
Epoch: [78][ 50/391]	Time  0.037 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.4270e-01 (7.0460e-02)	Acc@1  93.75 ( 97.75)	Acc@5 100.00 (100.00)
Epoch: [78][ 60/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.004)	Loss 5.5328e-02 (7.1640e-02)	Acc@1  98.44 ( 97.68)	Acc@5 100.00 ( 99.99)
Epoch: [78][ 70/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.2094e-02 (7.2676e-02)	Acc@1 100.00 ( 97.62)	Acc@5 100.00 ( 99.99)
Epoch: [78][ 80/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.3599e-02 (7.2390e-02)	Acc@1  96.09 ( 97.59)	Acc@5 100.00 ( 99.99)
Epoch: [78][ 90/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 4.1718e-02 (7.2501e-02)	Acc@1  99.22 ( 97.59)	Acc@5 100.00 ( 99.98)
Epoch: [78][100/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.003)	Loss 5.1239e-02 (7.2030e-02)	Acc@1  99.22 ( 97.62)	Acc@5 100.00 ( 99.98)
Epoch: [78][110/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.003)	Loss 3.3112e-02 (7.0863e-02)	Acc@1  99.22 ( 97.66)	Acc@5 100.00 ( 99.99)
Epoch: [78][120/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3989e-01 (7.1382e-02)	Acc@1  95.31 ( 97.62)	Acc@5 100.00 ( 99.99)
Epoch: [78][130/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0266e-01 (7.1660e-02)	Acc@1  97.66 ( 97.63)	Acc@5 100.00 ( 99.98)
Epoch: [78][140/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.3496e-02 (7.1721e-02)	Acc@1  98.44 ( 97.61)	Acc@5 100.00 ( 99.98)
Epoch: [78][150/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.7769e-02 (7.1147e-02)	Acc@1  96.09 ( 97.61)	Acc@5 100.00 ( 99.98)
Epoch: [78][160/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.2754e-02 (7.0908e-02)	Acc@1  97.66 ( 97.65)	Acc@5 100.00 ( 99.99)
Epoch: [78][170/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1077e-02 (7.0794e-02)	Acc@1  99.22 ( 97.67)	Acc@5 100.00 ( 99.98)
Epoch: [78][180/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.1309e-02 (7.0594e-02)	Acc@1  96.88 ( 97.68)	Acc@5 100.00 ( 99.98)
Epoch: [78][190/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2856e-02 (7.0727e-02)	Acc@1  98.44 ( 97.68)	Acc@5 100.00 ( 99.98)
Epoch: [78][200/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 5.8105e-02 (7.0963e-02)	Acc@1  98.44 ( 97.70)	Acc@5 100.00 ( 99.98)
Epoch: [78][210/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 8.6182e-02 (7.1334e-02)	Acc@1  96.09 ( 97.69)	Acc@5 100.00 ( 99.98)
Epoch: [78][220/391]	Time  0.044 ( 0.040)	Data  0.001 ( 0.002)	Loss 9.2834e-02 (7.1410e-02)	Acc@1  96.88 ( 97.68)	Acc@5 100.00 ( 99.98)
Epoch: [78][230/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 4.5258e-02 (7.0631e-02)	Acc@1  99.22 ( 97.71)	Acc@5 100.00 ( 99.98)
Epoch: [78][240/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 7.5684e-02 (7.0442e-02)	Acc@1  97.66 ( 97.73)	Acc@5 100.00 ( 99.98)
Epoch: [78][250/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 7.6904e-02 (7.0659e-02)	Acc@1  96.09 ( 97.72)	Acc@5 100.00 ( 99.98)
Epoch: [78][260/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 5.9143e-02 (7.0871e-02)	Acc@1  98.44 ( 97.71)	Acc@5 100.00 ( 99.98)
Epoch: [78][270/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 9.1309e-02 (7.1437e-02)	Acc@1  96.88 ( 97.69)	Acc@5 100.00 ( 99.98)
Epoch: [78][280/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 6.7688e-02 (7.1154e-02)	Acc@1  97.66 ( 97.70)	Acc@5 100.00 ( 99.98)
Epoch: [78][290/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 4.2450e-02 (7.1091e-02)	Acc@1  99.22 ( 97.72)	Acc@5 100.00 ( 99.98)
Epoch: [78][300/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.002)	Loss 7.8186e-02 (7.1070e-02)	Acc@1  96.88 ( 97.72)	Acc@5 100.00 ( 99.98)
Epoch: [78][310/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.0858e-01 (7.1512e-02)	Acc@1  95.31 ( 97.69)	Acc@5 100.00 ( 99.98)
Epoch: [78][320/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.1078e-01 (7.1848e-02)	Acc@1  96.09 ( 97.68)	Acc@5 100.00 ( 99.98)
Epoch: [78][330/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.4951e-02 (7.1861e-02)	Acc@1  97.66 ( 97.68)	Acc@5 100.00 ( 99.98)
Epoch: [78][340/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.7434e-02 (7.1908e-02)	Acc@1  99.22 ( 97.67)	Acc@5 100.00 ( 99.98)
Epoch: [78][350/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.9407e-02 (7.2083e-02)	Acc@1  97.66 ( 97.67)	Acc@5 100.00 ( 99.98)
Epoch: [78][360/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 8.2947e-02 (7.2080e-02)	Acc@1  98.44 ( 97.67)	Acc@5 100.00 ( 99.98)
Epoch: [78][370/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.7749e-02 (7.2176e-02)	Acc@1  97.66 ( 97.67)	Acc@5 100.00 ( 99.98)
Epoch: [78][380/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.0864e-01 (7.2459e-02)	Acc@1  94.53 ( 97.66)	Acc@5 100.00 ( 99.98)
Epoch: [78][390/391]	Time  0.029 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.2659e-01 (7.2531e-02)	Acc@1  95.00 ( 97.65)	Acc@5 100.00 ( 99.98)
## e[78] optimizer.zero_grad (sum) time: 0.25124096870422363
## e[78]       loss.backward (sum) time: 4.139019727706909
## e[78]      optimizer.step (sum) time: 1.7523417472839355
## epoch[78] training(only) time: 15.814969301223755
# Switched to evaluate mode...
Test: [  0/100]	Time  0.171 ( 0.171)	Loss 2.8589e-01 (2.8589e-01)	Acc@1  93.00 ( 93.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.018 ( 0.032)	Loss 3.8354e-01 (2.8832e-01)	Acc@1  87.00 ( 91.09)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.022 ( 0.027)	Loss 3.6499e-01 (3.1009e-01)	Acc@1  89.00 ( 91.05)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.017 ( 0.024)	Loss 4.3774e-01 (3.1619e-01)	Acc@1  87.00 ( 90.94)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.021 ( 0.023)	Loss 4.0112e-01 (3.1720e-01)	Acc@1  88.00 ( 90.85)	Acc@5  99.00 ( 99.66)
Test: [ 50/100]	Time  0.021 ( 0.023)	Loss 1.8262e-01 (3.0995e-01)	Acc@1  94.00 ( 91.00)	Acc@5 100.00 ( 99.69)
Test: [ 60/100]	Time  0.018 ( 0.022)	Loss 3.8281e-01 (3.0950e-01)	Acc@1  91.00 ( 90.89)	Acc@5 100.00 ( 99.72)
Test: [ 70/100]	Time  0.020 ( 0.022)	Loss 4.6289e-01 (3.0633e-01)	Acc@1  86.00 ( 90.82)	Acc@5 100.00 ( 99.72)
Test: [ 80/100]	Time  0.023 ( 0.022)	Loss 9.8206e-02 (3.1115e-01)	Acc@1  94.00 ( 90.62)	Acc@5 100.00 ( 99.69)
Test: [ 90/100]	Time  0.020 ( 0.022)	Loss 2.5488e-01 (3.1000e-01)	Acc@1  94.00 ( 90.75)	Acc@5 100.00 ( 99.69)
 * Acc@1 90.830 Acc@5 99.690
### epoch[78] execution time: 18.06249737739563
EPOCH 79
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [79][  0/391]	Time  0.210 ( 0.210)	Data  0.168 ( 0.168)	Loss 1.9547e-02 (1.9547e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [79][ 10/391]	Time  0.038 ( 0.055)	Data  0.001 ( 0.016)	Loss 8.6975e-02 (7.4689e-02)	Acc@1  97.66 ( 97.51)	Acc@5 100.00 (100.00)
Epoch: [79][ 20/391]	Time  0.041 ( 0.048)	Data  0.001 ( 0.009)	Loss 1.0461e-01 (7.4613e-02)	Acc@1  96.09 ( 97.40)	Acc@5 100.00 (100.00)
Epoch: [79][ 30/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.006)	Loss 2.9587e-02 (7.3569e-02)	Acc@1  98.44 ( 97.40)	Acc@5 100.00 (100.00)
Epoch: [79][ 40/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.005)	Loss 4.7455e-02 (7.4006e-02)	Acc@1  98.44 ( 97.41)	Acc@5 100.00 (100.00)
Epoch: [79][ 50/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.1780e-01 (7.5917e-02)	Acc@1  95.31 ( 97.30)	Acc@5 100.00 (100.00)
Epoch: [79][ 60/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 5.8838e-02 (7.4926e-02)	Acc@1  97.66 ( 97.31)	Acc@5 100.00 (100.00)
Epoch: [79][ 70/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.6448e-02 (7.6412e-02)	Acc@1  99.22 ( 97.25)	Acc@5 100.00 ( 99.99)
Epoch: [79][ 80/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.0863e-02 (7.6168e-02)	Acc@1  99.22 ( 97.28)	Acc@5 100.00 ( 99.99)
Epoch: [79][ 90/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.5186e-02 (7.4945e-02)	Acc@1  97.66 ( 97.36)	Acc@5 100.00 ( 99.99)
Epoch: [79][100/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 8.3191e-02 (7.4279e-02)	Acc@1  98.44 ( 97.42)	Acc@5 100.00 ( 99.99)
Epoch: [79][110/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0748e-01 (7.4606e-02)	Acc@1  96.88 ( 97.40)	Acc@5 100.00 ( 99.99)
Epoch: [79][120/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.4656e-02 (7.3795e-02)	Acc@1  95.31 ( 97.44)	Acc@5 100.00 ( 99.99)
Epoch: [79][130/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0974e-01 (7.4097e-02)	Acc@1  96.09 ( 97.41)	Acc@5 100.00 ( 99.99)
Epoch: [79][140/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8340e-02 (7.3172e-02)	Acc@1  99.22 ( 97.46)	Acc@5 100.00 ( 99.99)
Epoch: [79][150/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.3436e-02 (7.3452e-02)	Acc@1  98.44 ( 97.45)	Acc@5 100.00 ( 99.99)
Epoch: [79][160/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0077e-01 (7.3734e-02)	Acc@1  97.66 ( 97.43)	Acc@5 100.00 (100.00)
Epoch: [79][170/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.4778e-02 (7.3568e-02)	Acc@1  95.31 ( 97.43)	Acc@5 100.00 (100.00)
Epoch: [79][180/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.9854e-02 (7.3374e-02)	Acc@1  96.88 ( 97.45)	Acc@5 100.00 (100.00)
Epoch: [79][190/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.8013e-02 (7.2972e-02)	Acc@1  96.88 ( 97.47)	Acc@5 100.00 (100.00)
Epoch: [79][200/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.1279e-02 (7.2709e-02)	Acc@1  99.22 ( 97.49)	Acc@5 100.00 (100.00)
Epoch: [79][210/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.6965e-02 (7.2859e-02)	Acc@1  96.88 ( 97.48)	Acc@5 100.00 (100.00)
Epoch: [79][220/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.4229e-02 (7.3033e-02)	Acc@1  95.31 ( 97.48)	Acc@5 100.00 ( 99.99)
Epoch: [79][230/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.5500e-02 (7.3446e-02)	Acc@1  96.88 ( 97.47)	Acc@5 100.00 ( 99.99)
Epoch: [79][240/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.8197e-02 (7.3203e-02)	Acc@1  98.44 ( 97.50)	Acc@5 100.00 ( 99.99)
Epoch: [79][250/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.5634e-02 (7.2776e-02)	Acc@1  99.22 ( 97.53)	Acc@5 100.00 ( 99.99)
Epoch: [79][260/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.1848e-02 (7.2734e-02)	Acc@1  96.88 ( 97.54)	Acc@5 100.00 ( 99.99)
Epoch: [79][270/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.3252e-02 (7.2258e-02)	Acc@1  96.88 ( 97.56)	Acc@5 100.00 ( 99.99)
Epoch: [79][280/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.7037e-02 (7.2024e-02)	Acc@1  98.44 ( 97.58)	Acc@5 100.00 ( 99.99)
Epoch: [79][290/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0436e-02 (7.1863e-02)	Acc@1 100.00 ( 97.60)	Acc@5 100.00 ( 99.99)
Epoch: [79][300/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.3496e-02 (7.2241e-02)	Acc@1  97.66 ( 97.60)	Acc@5 100.00 ( 99.99)
Epoch: [79][310/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.4495e-02 (7.2385e-02)	Acc@1  99.22 ( 97.59)	Acc@5 100.00 ( 99.99)
Epoch: [79][320/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.0718e-01 (7.2769e-02)	Acc@1  97.66 ( 97.58)	Acc@5 100.00 ( 99.99)
Epoch: [79][330/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.9357e-02 (7.3061e-02)	Acc@1  98.44 ( 97.56)	Acc@5 100.00 ( 99.99)
Epoch: [79][340/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.7881e-02 (7.3041e-02)	Acc@1  97.66 ( 97.58)	Acc@5 100.00 ( 99.99)
Epoch: [79][350/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.6610e-02 (7.3001e-02)	Acc@1  98.44 ( 97.58)	Acc@5 100.00 ( 99.99)
Epoch: [79][360/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.1218e-02 (7.2829e-02)	Acc@1  99.22 ( 97.59)	Acc@5 100.00 ( 99.99)
Epoch: [79][370/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.001)	Loss 8.3130e-02 (7.2903e-02)	Acc@1  97.66 ( 97.58)	Acc@5 100.00 ( 99.99)
Epoch: [79][380/391]	Time  0.043 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.2572e-02 (7.2722e-02)	Acc@1  98.44 ( 97.60)	Acc@5 100.00 ( 99.99)
Epoch: [79][390/391]	Time  0.027 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.2521e-02 (7.2775e-02)	Acc@1 100.00 ( 97.60)	Acc@5 100.00 ( 99.99)
## e[79] optimizer.zero_grad (sum) time: 0.24933671951293945
## e[79]       loss.backward (sum) time: 4.1814351081848145
## e[79]      optimizer.step (sum) time: 1.7185871601104736
## epoch[79] training(only) time: 15.878312587738037
# Switched to evaluate mode...
Test: [  0/100]	Time  0.176 ( 0.176)	Loss 2.8784e-01 (2.8784e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.023 ( 0.034)	Loss 3.6963e-01 (2.9064e-01)	Acc@1  88.00 ( 91.27)	Acc@5  99.00 ( 99.73)
Test: [ 20/100]	Time  0.020 ( 0.028)	Loss 3.6816e-01 (3.1180e-01)	Acc@1  89.00 ( 91.29)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.020 ( 0.026)	Loss 4.3604e-01 (3.2097e-01)	Acc@1  88.00 ( 91.13)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.021 ( 0.024)	Loss 4.0259e-01 (3.2094e-01)	Acc@1  88.00 ( 91.00)	Acc@5  99.00 ( 99.68)
Test: [ 50/100]	Time  0.019 ( 0.023)	Loss 1.8262e-01 (3.1348e-01)	Acc@1  94.00 ( 91.06)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.022 ( 0.023)	Loss 3.8135e-01 (3.1255e-01)	Acc@1  91.00 ( 90.97)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.017 ( 0.022)	Loss 4.6387e-01 (3.0886e-01)	Acc@1  86.00 ( 90.87)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.023 ( 0.022)	Loss 1.0626e-01 (3.1421e-01)	Acc@1  95.00 ( 90.70)	Acc@5 100.00 ( 99.70)
Test: [ 90/100]	Time  0.021 ( 0.022)	Loss 2.8198e-01 (3.1286e-01)	Acc@1  94.00 ( 90.82)	Acc@5 100.00 ( 99.70)
 * Acc@1 90.950 Acc@5 99.710
### epoch[79] execution time: 18.195474863052368
EPOCH 80
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [80][  0/391]	Time  0.213 ( 0.213)	Data  0.171 ( 0.171)	Loss 7.8491e-02 (7.8491e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [80][ 10/391]	Time  0.038 ( 0.055)	Data  0.001 ( 0.016)	Loss 7.6965e-02 (7.6127e-02)	Acc@1  97.66 ( 97.23)	Acc@5 100.00 (100.00)
Epoch: [80][ 20/391]	Time  0.039 ( 0.049)	Data  0.001 ( 0.009)	Loss 4.5013e-02 (7.7170e-02)	Acc@1  99.22 ( 97.54)	Acc@5 100.00 (100.00)
Epoch: [80][ 30/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.006)	Loss 8.6853e-02 (7.5870e-02)	Acc@1  98.44 ( 97.58)	Acc@5 100.00 (100.00)
Epoch: [80][ 40/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.005)	Loss 1.1804e-01 (7.5839e-02)	Acc@1  96.88 ( 97.47)	Acc@5 100.00 (100.00)
Epoch: [80][ 50/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.004)	Loss 7.1533e-02 (7.4620e-02)	Acc@1  97.66 ( 97.40)	Acc@5 100.00 (100.00)
Epoch: [80][ 60/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 9.6069e-02 (7.3466e-02)	Acc@1  95.31 ( 97.43)	Acc@5 100.00 (100.00)
Epoch: [80][ 70/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.8046e-02 (7.3670e-02)	Acc@1 100.00 ( 97.45)	Acc@5 100.00 ( 99.99)
Epoch: [80][ 80/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 8.9172e-02 (7.2513e-02)	Acc@1  96.88 ( 97.52)	Acc@5 100.00 ( 99.99)
Epoch: [80][ 90/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.1208e-02 (7.1780e-02)	Acc@1  97.66 ( 97.54)	Acc@5 100.00 ( 99.99)
Epoch: [80][100/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.6477e-02 (7.2004e-02)	Acc@1  97.66 ( 97.53)	Acc@5 100.00 ( 99.99)
Epoch: [80][110/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4087e-01 (7.2377e-02)	Acc@1  95.31 ( 97.50)	Acc@5  99.22 ( 99.99)
Epoch: [80][120/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2177e-01 (7.3746e-02)	Acc@1  96.88 ( 97.44)	Acc@5 100.00 ( 99.99)
Epoch: [80][130/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.4666e-02 (7.4148e-02)	Acc@1  96.09 ( 97.42)	Acc@5 100.00 ( 99.99)
Epoch: [80][140/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0455e-01 (7.3401e-02)	Acc@1  96.09 ( 97.44)	Acc@5 100.00 ( 99.99)
Epoch: [80][150/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.6610e-02 (7.3401e-02)	Acc@1  97.66 ( 97.46)	Acc@5 100.00 ( 99.99)
Epoch: [80][160/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.7373e-02 (7.3000e-02)	Acc@1  98.44 ( 97.48)	Acc@5 100.00 ( 99.99)
Epoch: [80][170/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.5764e-02 (7.3225e-02)	Acc@1  96.88 ( 97.47)	Acc@5 100.00 ( 99.99)
Epoch: [80][180/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.1249e-02 (7.2902e-02)	Acc@1  97.66 ( 97.49)	Acc@5 100.00 ( 99.99)
Epoch: [80][190/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.7251e-02 (7.1895e-02)	Acc@1  98.44 ( 97.55)	Acc@5 100.00 ( 99.99)
Epoch: [80][200/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.6477e-02 (7.1355e-02)	Acc@1  99.22 ( 97.61)	Acc@5 100.00 ( 99.99)
Epoch: [80][210/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.7139e-02 (7.1597e-02)	Acc@1  97.66 ( 97.60)	Acc@5 100.00 ( 99.99)
Epoch: [80][220/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.002)	Loss 3.9825e-02 (7.1664e-02)	Acc@1  99.22 ( 97.62)	Acc@5 100.00 ( 99.99)
Epoch: [80][230/391]	Time  0.037 ( 0.040)	Data  0.002 ( 0.002)	Loss 4.2755e-02 (7.1675e-02)	Acc@1 100.00 ( 97.62)	Acc@5 100.00 ( 99.99)
Epoch: [80][240/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 4.4281e-02 (7.2298e-02)	Acc@1  99.22 ( 97.58)	Acc@5 100.00 ( 99.99)
Epoch: [80][250/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 6.2805e-02 (7.2484e-02)	Acc@1  96.88 ( 97.57)	Acc@5 100.00 ( 99.99)
Epoch: [80][260/391]	Time  0.046 ( 0.040)	Data  0.001 ( 0.002)	Loss 6.5552e-02 (7.2955e-02)	Acc@1  97.66 ( 97.54)	Acc@5 100.00 ( 99.99)
Epoch: [80][270/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 6.1798e-02 (7.2931e-02)	Acc@1  98.44 ( 97.54)	Acc@5 100.00 ( 99.99)
Epoch: [80][280/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 8.6914e-02 (7.2606e-02)	Acc@1  96.88 ( 97.57)	Acc@5 100.00 ( 99.99)
Epoch: [80][290/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 4.2847e-02 (7.2648e-02)	Acc@1  99.22 ( 97.58)	Acc@5 100.00 ( 99.99)
Epoch: [80][300/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.1566e-01 (7.2394e-02)	Acc@1  96.09 ( 97.60)	Acc@5 100.00 ( 99.99)
Epoch: [80][310/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.6357e-01 (7.2845e-02)	Acc@1  95.31 ( 97.58)	Acc@5 100.00 ( 99.99)
Epoch: [80][320/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 7.6660e-02 (7.2896e-02)	Acc@1  96.09 ( 97.57)	Acc@5 100.00 ( 99.99)
Epoch: [80][330/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.9490e-02 (7.2913e-02)	Acc@1  99.22 ( 97.57)	Acc@5 100.00 ( 99.99)
Epoch: [80][340/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.8430e-02 (7.3228e-02)	Acc@1  98.44 ( 97.56)	Acc@5 100.00 ( 99.99)
Epoch: [80][350/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 8.1787e-02 (7.3545e-02)	Acc@1  96.88 ( 97.54)	Acc@5 100.00 ( 99.99)
Epoch: [80][360/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.8237e-02 (7.3850e-02)	Acc@1  97.66 ( 97.53)	Acc@5 100.00 ( 99.99)
Epoch: [80][370/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.1982e-02 (7.3875e-02)	Acc@1  99.22 ( 97.53)	Acc@5 100.00 ( 99.99)
Epoch: [80][380/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.7852e-02 (7.4767e-02)	Acc@1  99.22 ( 97.50)	Acc@5 100.00 ( 99.99)
Epoch: [80][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.5979e-02 (7.4681e-02)	Acc@1  97.50 ( 97.50)	Acc@5 100.00 ( 99.99)
## e[80] optimizer.zero_grad (sum) time: 0.2514383792877197
## e[80]       loss.backward (sum) time: 4.106319904327393
## e[80]      optimizer.step (sum) time: 1.7457287311553955
## epoch[80] training(only) time: 15.825580835342407
# Switched to evaluate mode...
Test: [  0/100]	Time  0.180 ( 0.180)	Loss 2.7295e-01 (2.7295e-01)	Acc@1  92.00 ( 92.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.021 ( 0.034)	Loss 3.7451e-01 (2.9035e-01)	Acc@1  88.00 ( 91.09)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.021 ( 0.027)	Loss 3.7646e-01 (3.1381e-01)	Acc@1  88.00 ( 91.14)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.024 ( 0.025)	Loss 4.4214e-01 (3.2076e-01)	Acc@1  87.00 ( 90.90)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.020 ( 0.024)	Loss 4.0186e-01 (3.2179e-01)	Acc@1  89.00 ( 90.95)	Acc@5  99.00 ( 99.66)
Test: [ 50/100]	Time  0.020 ( 0.023)	Loss 1.8567e-01 (3.1503e-01)	Acc@1  94.00 ( 91.08)	Acc@5 100.00 ( 99.69)
Test: [ 60/100]	Time  0.019 ( 0.022)	Loss 3.8745e-01 (3.1436e-01)	Acc@1  91.00 ( 90.95)	Acc@5 100.00 ( 99.72)
Test: [ 70/100]	Time  0.018 ( 0.022)	Loss 4.4507e-01 (3.1110e-01)	Acc@1  87.00 ( 90.87)	Acc@5 100.00 ( 99.72)
Test: [ 80/100]	Time  0.018 ( 0.022)	Loss 9.9976e-02 (3.1587e-01)	Acc@1  96.00 ( 90.74)	Acc@5 100.00 ( 99.70)
Test: [ 90/100]	Time  0.020 ( 0.022)	Loss 2.6807e-01 (3.1465e-01)	Acc@1  94.00 ( 90.82)	Acc@5 100.00 ( 99.71)
 * Acc@1 90.960 Acc@5 99.730
### epoch[80] execution time: 18.09761929512024
EPOCH 81
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [81][  0/391]	Time  0.206 ( 0.206)	Data  0.165 ( 0.165)	Loss 5.4932e-02 (5.4932e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [81][ 10/391]	Time  0.042 ( 0.056)	Data  0.001 ( 0.016)	Loss 8.0994e-02 (7.9052e-02)	Acc@1  97.66 ( 97.73)	Acc@5 100.00 (100.00)
Epoch: [81][ 20/391]	Time  0.043 ( 0.048)	Data  0.001 ( 0.009)	Loss 8.7280e-02 (7.8727e-02)	Acc@1  96.88 ( 97.62)	Acc@5 100.00 (100.00)
Epoch: [81][ 30/391]	Time  0.038 ( 0.045)	Data  0.001 ( 0.006)	Loss 6.5186e-02 (7.3808e-02)	Acc@1  96.88 ( 97.78)	Acc@5 100.00 (100.00)
Epoch: [81][ 40/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.005)	Loss 3.5461e-02 (7.4768e-02)	Acc@1  99.22 ( 97.73)	Acc@5 100.00 (100.00)
Epoch: [81][ 50/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.3354e-01 (7.5136e-02)	Acc@1  94.53 ( 97.75)	Acc@5 100.00 (100.00)
Epoch: [81][ 60/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.004)	Loss 4.6936e-02 (7.2989e-02)	Acc@1  99.22 ( 97.77)	Acc@5 100.00 (100.00)
Epoch: [81][ 70/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.003)	Loss 6.4758e-02 (7.3546e-02)	Acc@1  98.44 ( 97.74)	Acc@5 100.00 (100.00)
Epoch: [81][ 80/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.5349e-02 (7.1613e-02)	Acc@1  99.22 ( 97.81)	Acc@5 100.00 ( 99.99)
Epoch: [81][ 90/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.1066e-02 (7.0350e-02)	Acc@1  96.88 ( 97.86)	Acc@5 100.00 ( 99.99)
Epoch: [81][100/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.0129e-02 (7.1720e-02)	Acc@1  97.66 ( 97.77)	Acc@5 100.00 ( 99.99)
Epoch: [81][110/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.2927e-01 (7.2877e-02)	Acc@1  94.53 ( 97.74)	Acc@5 100.00 ( 99.99)
Epoch: [81][120/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.2521e-02 (7.2382e-02)	Acc@1  98.44 ( 97.75)	Acc@5 100.00 ( 99.99)
Epoch: [81][130/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.4106e-02 (7.1860e-02)	Acc@1  96.88 ( 97.82)	Acc@5 100.00 ( 99.99)
Epoch: [81][140/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.0759e-02 (7.1785e-02)	Acc@1  96.09 ( 97.78)	Acc@5 100.00 ( 99.99)
Epoch: [81][150/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.9355e-02 (7.1654e-02)	Acc@1  96.88 ( 97.78)	Acc@5 100.00 ( 99.99)
Epoch: [81][160/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2826e-02 (7.0762e-02)	Acc@1  98.44 ( 97.78)	Acc@5 100.00 ( 99.99)
Epoch: [81][170/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.6863e-02 (7.1154e-02)	Acc@1  96.09 ( 97.78)	Acc@5 100.00 ( 99.99)
Epoch: [81][180/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1639e-01 (7.0720e-02)	Acc@1  96.09 ( 97.79)	Acc@5 100.00 ( 99.99)
Epoch: [81][190/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2054e-01 (7.1492e-02)	Acc@1  96.09 ( 97.75)	Acc@5 100.00 ( 99.99)
Epoch: [81][200/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.3730e-02 (7.1396e-02)	Acc@1  97.66 ( 97.76)	Acc@5 100.00 ( 99.99)
Epoch: [81][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7668e-02 (7.1871e-02)	Acc@1  99.22 ( 97.72)	Acc@5 100.00 ( 99.99)
Epoch: [81][220/391]	Time  0.052 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.3506e-02 (7.2027e-02)	Acc@1  97.66 ( 97.70)	Acc@5 100.00 ( 99.99)
Epoch: [81][230/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0046e-01 (7.2091e-02)	Acc@1  96.09 ( 97.71)	Acc@5 100.00 ( 99.99)
Epoch: [81][240/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4124e-01 (7.1737e-02)	Acc@1  96.09 ( 97.73)	Acc@5 100.00 ( 99.99)
Epoch: [81][250/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.9814e-02 (7.1652e-02)	Acc@1  99.22 ( 97.73)	Acc@5 100.00 ( 99.99)
Epoch: [81][260/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.0576e-02 (7.1907e-02)	Acc@1  95.31 ( 97.72)	Acc@5 100.00 ( 99.99)
Epoch: [81][270/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9561e-02 (7.2495e-02)	Acc@1  98.44 ( 97.69)	Acc@5 100.00 ( 99.99)
Epoch: [81][280/391]	Time  0.043 ( 0.040)	Data  0.001 ( 0.002)	Loss 7.3120e-02 (7.2464e-02)	Acc@1  96.09 ( 97.68)	Acc@5 100.00 ( 99.99)
Epoch: [81][290/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 4.9103e-02 (7.2209e-02)	Acc@1  98.44 ( 97.68)	Acc@5 100.00 ( 99.99)
Epoch: [81][300/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 7.4646e-02 (7.2185e-02)	Acc@1  96.88 ( 97.68)	Acc@5 100.00 ( 99.99)
Epoch: [81][310/391]	Time  0.040 ( 0.040)	Data  0.002 ( 0.002)	Loss 8.0872e-02 (7.2256e-02)	Acc@1  97.66 ( 97.68)	Acc@5 100.00 ( 99.99)
Epoch: [81][320/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 8.1665e-02 (7.1934e-02)	Acc@1  98.44 ( 97.70)	Acc@5 100.00 ( 99.99)
Epoch: [81][330/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.8738e-01 (7.2619e-02)	Acc@1  92.97 ( 97.67)	Acc@5 100.00 ( 99.99)
Epoch: [81][340/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.4260e-02 (7.2594e-02)	Acc@1  98.44 ( 97.67)	Acc@5 100.00 ( 99.99)
Epoch: [81][350/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.5479e-01 (7.2930e-02)	Acc@1  94.53 ( 97.66)	Acc@5 100.00 ( 99.99)
Epoch: [81][360/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.001)	Loss 8.4106e-02 (7.3227e-02)	Acc@1  97.66 ( 97.64)	Acc@5 100.00 ( 99.99)
Epoch: [81][370/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 9.8877e-02 (7.3437e-02)	Acc@1  96.09 ( 97.64)	Acc@5 100.00 ( 99.99)
Epoch: [81][380/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.8115e-02 (7.2971e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 ( 99.99)
Epoch: [81][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.5928e-02 (7.3387e-02)	Acc@1  96.25 ( 97.64)	Acc@5 100.00 ( 99.99)
## e[81] optimizer.zero_grad (sum) time: 0.2494649887084961
## e[81]       loss.backward (sum) time: 4.089258432388306
## e[81]      optimizer.step (sum) time: 1.7821943759918213
## epoch[81] training(only) time: 15.83638310432434
# Switched to evaluate mode...
Test: [  0/100]	Time  0.167 ( 0.167)	Loss 2.6074e-01 (2.6074e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.020 ( 0.034)	Loss 3.6035e-01 (2.8441e-01)	Acc@1  89.00 ( 91.73)	Acc@5  99.00 ( 99.73)
Test: [ 20/100]	Time  0.017 ( 0.027)	Loss 3.6475e-01 (3.0696e-01)	Acc@1  87.00 ( 91.19)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.016 ( 0.025)	Loss 4.5654e-01 (3.1629e-01)	Acc@1  87.00 ( 90.97)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.020 ( 0.024)	Loss 4.1455e-01 (3.1840e-01)	Acc@1  90.00 ( 90.98)	Acc@5  99.00 ( 99.68)
Test: [ 50/100]	Time  0.019 ( 0.023)	Loss 1.8958e-01 (3.1059e-01)	Acc@1  94.00 ( 91.08)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.020 ( 0.022)	Loss 3.7598e-01 (3.1023e-01)	Acc@1  91.00 ( 90.89)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.017 ( 0.022)	Loss 4.4556e-01 (3.0593e-01)	Acc@1  86.00 ( 90.82)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.019 ( 0.022)	Loss 1.0931e-01 (3.1082e-01)	Acc@1  94.00 ( 90.70)	Acc@5 100.00 ( 99.72)
Test: [ 90/100]	Time  0.020 ( 0.021)	Loss 2.6392e-01 (3.0952e-01)	Acc@1  94.00 ( 90.81)	Acc@5 100.00 ( 99.74)
 * Acc@1 90.920 Acc@5 99.750
### epoch[81] execution time: 18.066017389297485
EPOCH 82
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [82][  0/391]	Time  0.206 ( 0.206)	Data  0.158 ( 0.158)	Loss 5.6274e-02 (5.6274e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [82][ 10/391]	Time  0.039 ( 0.056)	Data  0.001 ( 0.015)	Loss 8.8379e-02 (7.0604e-02)	Acc@1  96.88 ( 97.59)	Acc@5 100.00 (100.00)
Epoch: [82][ 20/391]	Time  0.043 ( 0.049)	Data  0.001 ( 0.008)	Loss 1.6565e-01 (8.2172e-02)	Acc@1  92.97 ( 97.10)	Acc@5 100.00 (100.00)
Epoch: [82][ 30/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.006)	Loss 7.4158e-02 (7.9169e-02)	Acc@1  98.44 ( 97.28)	Acc@5 100.00 (100.00)
Epoch: [82][ 40/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.005)	Loss 5.0995e-02 (8.2311e-02)	Acc@1  99.22 ( 97.29)	Acc@5 100.00 (100.00)
Epoch: [82][ 50/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.004)	Loss 9.3384e-02 (7.7339e-02)	Acc@1  97.66 ( 97.52)	Acc@5 100.00 (100.00)
Epoch: [82][ 60/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.004)	Loss 2.6810e-02 (7.7670e-02)	Acc@1 100.00 ( 97.52)	Acc@5 100.00 (100.00)
Epoch: [82][ 70/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.003)	Loss 8.5083e-02 (7.6719e-02)	Acc@1  97.66 ( 97.56)	Acc@5 100.00 (100.00)
Epoch: [82][ 80/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.3427e-02 (7.4510e-02)	Acc@1  99.22 ( 97.65)	Acc@5 100.00 (100.00)
Epoch: [82][ 90/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.6660e-02 (7.3330e-02)	Acc@1  97.66 ( 97.70)	Acc@5 100.00 (100.00)
Epoch: [82][100/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.1963e-01 (7.3246e-02)	Acc@1  96.09 ( 97.68)	Acc@5 100.00 (100.00)
Epoch: [82][110/391]	Time  0.046 ( 0.042)	Data  0.001 ( 0.002)	Loss 8.1421e-02 (7.3420e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [82][120/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.6233e-02 (7.4555e-02)	Acc@1  96.88 ( 97.60)	Acc@5 100.00 (100.00)
Epoch: [82][130/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.3669e-02 (7.4095e-02)	Acc@1  96.88 ( 97.62)	Acc@5 100.00 (100.00)
Epoch: [82][140/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0596e-01 (7.3989e-02)	Acc@1  95.31 ( 97.61)	Acc@5 100.00 (100.00)
Epoch: [82][150/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0558e-02 (7.4432e-02)	Acc@1  99.22 ( 97.59)	Acc@5 100.00 (100.00)
Epoch: [82][160/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.7322e-02 (7.4994e-02)	Acc@1  97.66 ( 97.56)	Acc@5 100.00 (100.00)
Epoch: [82][170/391]	Time  0.045 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.3354e-02 (7.5422e-02)	Acc@1  99.22 ( 97.55)	Acc@5 100.00 (100.00)
Epoch: [82][180/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.3313e-02 (7.6158e-02)	Acc@1  96.09 ( 97.51)	Acc@5 100.00 (100.00)
Epoch: [82][190/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1945e-01 (7.6866e-02)	Acc@1  96.88 ( 97.50)	Acc@5 100.00 ( 99.99)
Epoch: [82][200/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.7993e-02 (7.6052e-02)	Acc@1  98.44 ( 97.53)	Acc@5 100.00 ( 99.99)
Epoch: [82][210/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.8389e-02 (7.5675e-02)	Acc@1  97.66 ( 97.55)	Acc@5 100.00 ( 99.99)
Epoch: [82][220/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1006e-02 (7.5467e-02)	Acc@1  98.44 ( 97.54)	Acc@5 100.00 ( 99.99)
Epoch: [82][230/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.1066e-02 (7.5163e-02)	Acc@1  99.22 ( 97.54)	Acc@5 100.00 ( 99.99)
Epoch: [82][240/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8279e-02 (7.5000e-02)	Acc@1  98.44 ( 97.57)	Acc@5 100.00 ( 99.99)
Epoch: [82][250/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1877e-01 (7.5048e-02)	Acc@1  95.31 ( 97.56)	Acc@5 100.00 ( 99.99)
Epoch: [82][260/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0309e-01 (7.4897e-02)	Acc@1  96.09 ( 97.57)	Acc@5 100.00 ( 99.99)
Epoch: [82][270/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 8.0383e-02 (7.5329e-02)	Acc@1  97.66 ( 97.56)	Acc@5 100.00 ( 99.99)
Epoch: [82][280/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.2323e-01 (7.5375e-02)	Acc@1  95.31 ( 97.56)	Acc@5 100.00 ( 99.99)
Epoch: [82][290/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 8.8989e-02 (7.5070e-02)	Acc@1  96.88 ( 97.57)	Acc@5 100.00 ( 99.99)
Epoch: [82][300/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.6530e-02 (7.4818e-02)	Acc@1  98.44 ( 97.58)	Acc@5 100.00 ( 99.99)
Epoch: [82][310/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.4453e-02 (7.5051e-02)	Acc@1  98.44 ( 97.57)	Acc@5 100.00 ( 99.99)
Epoch: [82][320/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.8218e-02 (7.4664e-02)	Acc@1  98.44 ( 97.59)	Acc@5 100.00 ( 99.99)
Epoch: [82][330/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.2511e-02 (7.4573e-02)	Acc@1  99.22 ( 97.60)	Acc@5 100.00 ( 99.99)
Epoch: [82][340/391]	Time  0.052 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.2659e-01 (7.4514e-02)	Acc@1  94.53 ( 97.60)	Acc@5 100.00 ( 99.99)
Epoch: [82][350/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.2185e-02 (7.4625e-02)	Acc@1  99.22 ( 97.61)	Acc@5 100.00 ( 99.99)
Epoch: [82][360/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.001)	Loss 9.1614e-02 (7.4698e-02)	Acc@1  96.88 ( 97.61)	Acc@5 100.00 ( 99.99)
Epoch: [82][370/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.5664e-02 (7.4437e-02)	Acc@1  98.44 ( 97.62)	Acc@5 100.00 ( 99.99)
Epoch: [82][380/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.5044e-02 (7.4648e-02)	Acc@1  99.22 ( 97.60)	Acc@5 100.00 ( 99.99)
Epoch: [82][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 2.1500e-02 (7.4930e-02)	Acc@1 100.00 ( 97.59)	Acc@5 100.00 ( 99.99)
## e[82] optimizer.zero_grad (sum) time: 0.24880504608154297
## e[82]       loss.backward (sum) time: 4.166005849838257
## e[82]      optimizer.step (sum) time: 1.75661301612854
## epoch[82] training(only) time: 15.874382495880127
# Switched to evaluate mode...
Test: [  0/100]	Time  0.173 ( 0.173)	Loss 2.7026e-01 (2.7026e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.018 ( 0.033)	Loss 3.7793e-01 (2.9029e-01)	Acc@1  88.00 ( 91.18)	Acc@5  99.00 ( 99.73)
Test: [ 20/100]	Time  0.020 ( 0.026)	Loss 3.6963e-01 (3.1074e-01)	Acc@1  88.00 ( 91.10)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.018 ( 0.024)	Loss 4.3579e-01 (3.1807e-01)	Acc@1  87.00 ( 90.97)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.021 ( 0.023)	Loss 4.0479e-01 (3.1854e-01)	Acc@1  89.00 ( 90.95)	Acc@5  99.00 ( 99.68)
Test: [ 50/100]	Time  0.023 ( 0.023)	Loss 1.7261e-01 (3.1078e-01)	Acc@1  94.00 ( 91.08)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.021 ( 0.022)	Loss 3.7817e-01 (3.1025e-01)	Acc@1  91.00 ( 90.93)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.020 ( 0.022)	Loss 4.7632e-01 (3.0722e-01)	Acc@1  87.00 ( 90.87)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.023 ( 0.022)	Loss 1.0278e-01 (3.1214e-01)	Acc@1  96.00 ( 90.72)	Acc@5 100.00 ( 99.73)
Test: [ 90/100]	Time  0.017 ( 0.022)	Loss 2.7539e-01 (3.1064e-01)	Acc@1  94.00 ( 90.84)	Acc@5 100.00 ( 99.74)
 * Acc@1 90.940 Acc@5 99.740
### epoch[82] execution time: 18.136489629745483
EPOCH 83
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [83][  0/391]	Time  0.210 ( 0.210)	Data  0.168 ( 0.168)	Loss 7.3486e-02 (7.3486e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [83][ 10/391]	Time  0.040 ( 0.055)	Data  0.001 ( 0.016)	Loss 5.3436e-02 (7.0992e-02)	Acc@1  98.44 ( 97.73)	Acc@5 100.00 (100.00)
Epoch: [83][ 20/391]	Time  0.042 ( 0.048)	Data  0.001 ( 0.009)	Loss 5.7861e-02 (7.8278e-02)	Acc@1  98.44 ( 97.51)	Acc@5 100.00 (100.00)
Epoch: [83][ 30/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.006)	Loss 3.8177e-02 (7.3820e-02)	Acc@1  99.22 ( 97.58)	Acc@5 100.00 (100.00)
Epoch: [83][ 40/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.005)	Loss 1.0474e-01 (7.1357e-02)	Acc@1  96.88 ( 97.81)	Acc@5 100.00 ( 99.98)
Epoch: [83][ 50/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 7.0435e-02 (7.1115e-02)	Acc@1  97.66 ( 97.81)	Acc@5 100.00 ( 99.98)
Epoch: [83][ 60/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.004)	Loss 8.2458e-02 (7.2249e-02)	Acc@1  96.88 ( 97.71)	Acc@5 100.00 ( 99.99)
Epoch: [83][ 70/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.5380e-02 (7.2781e-02)	Acc@1 100.00 ( 97.63)	Acc@5 100.00 ( 99.99)
Epoch: [83][ 80/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.2429e-02 (7.1486e-02)	Acc@1  98.44 ( 97.67)	Acc@5 100.00 ( 99.99)
Epoch: [83][ 90/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.5603e-02 (7.2092e-02)	Acc@1  99.22 ( 97.65)	Acc@5 100.00 ( 99.99)
Epoch: [83][100/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.9744e-02 (7.2124e-02)	Acc@1  98.44 ( 97.63)	Acc@5 100.00 ( 99.99)
Epoch: [83][110/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.9275e-02 (7.1446e-02)	Acc@1  97.66 ( 97.65)	Acc@5 100.00 ( 99.99)
Epoch: [83][120/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0565e-01 (7.2643e-02)	Acc@1  96.09 ( 97.62)	Acc@5 100.00 ( 99.99)
Epoch: [83][130/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.7566e-02 (7.1974e-02)	Acc@1  96.09 ( 97.64)	Acc@5 100.00 ( 99.99)
Epoch: [83][140/391]	Time  0.035 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.0210e-02 (7.1748e-02)	Acc@1  96.09 ( 97.64)	Acc@5 100.00 ( 99.99)
Epoch: [83][150/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.5197e-02 (7.2711e-02)	Acc@1 100.00 ( 97.61)	Acc@5 100.00 ( 99.99)
Epoch: [83][160/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.6101e-02 (7.2279e-02)	Acc@1  98.44 ( 97.66)	Acc@5 100.00 ( 99.99)
Epoch: [83][170/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0232e-02 (7.1504e-02)	Acc@1  99.22 ( 97.69)	Acc@5 100.00 ( 99.99)
Epoch: [83][180/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2964e-01 (7.1768e-02)	Acc@1  96.09 ( 97.66)	Acc@5 100.00 ( 99.99)
Epoch: [83][190/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.9031e-02 (7.1736e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 ( 99.99)
Epoch: [83][200/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.3853e-02 (7.1651e-02)	Acc@1  96.09 ( 97.66)	Acc@5 100.00 ( 99.99)
Epoch: [83][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2755e-02 (7.1197e-02)	Acc@1  98.44 ( 97.67)	Acc@5 100.00 ( 99.99)
Epoch: [83][220/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.2346e-02 (7.1366e-02)	Acc@1  96.88 ( 97.65)	Acc@5 100.00 ( 99.99)
Epoch: [83][230/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.5542e-02 (7.1433e-02)	Acc@1  99.22 ( 97.65)	Acc@5 100.00 ( 99.99)
Epoch: [83][240/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 6.5125e-02 (7.1162e-02)	Acc@1  98.44 ( 97.67)	Acc@5 100.00 ( 99.99)
Epoch: [83][250/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 4.3793e-02 (7.1351e-02)	Acc@1  99.22 ( 97.67)	Acc@5 100.00 ( 99.99)
Epoch: [83][260/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.002)	Loss 6.1920e-02 (7.0988e-02)	Acc@1  97.66 ( 97.70)	Acc@5 100.00 ( 99.99)
Epoch: [83][270/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 8.8196e-02 (7.1113e-02)	Acc@1  96.88 ( 97.68)	Acc@5 100.00 ( 99.99)
Epoch: [83][280/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.0248e-01 (7.1001e-02)	Acc@1  97.66 ( 97.70)	Acc@5 100.00 ( 99.99)
Epoch: [83][290/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.7526e-02 (7.1076e-02)	Acc@1  97.66 ( 97.70)	Acc@5 100.00 ( 99.99)
Epoch: [83][300/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.8533e-02 (7.1529e-02)	Acc@1  96.88 ( 97.70)	Acc@5 100.00 ( 99.99)
Epoch: [83][310/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.3452e-01 (7.1722e-02)	Acc@1  95.31 ( 97.69)	Acc@5 100.00 ( 99.99)
Epoch: [83][320/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 8.2092e-02 (7.1643e-02)	Acc@1  96.09 ( 97.69)	Acc@5 100.00 ( 99.99)
Epoch: [83][330/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.4270e-02 (7.1658e-02)	Acc@1  98.44 ( 97.69)	Acc@5 100.00 ( 99.99)
Epoch: [83][340/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.0516e-02 (7.1896e-02)	Acc@1  97.66 ( 97.68)	Acc@5 100.00 ( 99.99)
Epoch: [83][350/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.1228e-02 (7.2208e-02)	Acc@1  96.88 ( 97.66)	Acc@5 100.00 ( 99.99)
Epoch: [83][360/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.5012e-02 (7.2227e-02)	Acc@1  97.66 ( 97.65)	Acc@5 100.00 ( 99.99)
Epoch: [83][370/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.0742e-01 (7.2173e-02)	Acc@1  96.09 ( 97.66)	Acc@5 100.00 ( 99.99)
Epoch: [83][380/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.8645e-02 (7.2349e-02)	Acc@1  99.22 ( 97.66)	Acc@5 100.00 ( 99.99)
Epoch: [83][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.8308e-02 (7.2290e-02)	Acc@1  97.50 ( 97.66)	Acc@5 100.00 ( 99.99)
## e[83] optimizer.zero_grad (sum) time: 0.2522766590118408
## e[83]       loss.backward (sum) time: 4.1820068359375
## e[83]      optimizer.step (sum) time: 1.7421290874481201
## epoch[83] training(only) time: 15.829005718231201
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 2.8394e-01 (2.8394e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.023 ( 0.033)	Loss 3.8110e-01 (2.9142e-01)	Acc@1  88.00 ( 91.55)	Acc@5  99.00 ( 99.73)
Test: [ 20/100]	Time  0.021 ( 0.026)	Loss 3.6719e-01 (3.1168e-01)	Acc@1  89.00 ( 91.29)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.022 ( 0.024)	Loss 4.3970e-01 (3.2253e-01)	Acc@1  87.00 ( 90.94)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.018 ( 0.023)	Loss 3.9722e-01 (3.2376e-01)	Acc@1  89.00 ( 90.85)	Acc@5  99.00 ( 99.68)
Test: [ 50/100]	Time  0.020 ( 0.022)	Loss 1.9653e-01 (3.1508e-01)	Acc@1  93.00 ( 90.90)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.019 ( 0.022)	Loss 3.8037e-01 (3.1425e-01)	Acc@1  91.00 ( 90.85)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.019 ( 0.022)	Loss 4.4312e-01 (3.0925e-01)	Acc@1  86.00 ( 90.85)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.024 ( 0.022)	Loss 1.1169e-01 (3.1478e-01)	Acc@1  95.00 ( 90.72)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.016 ( 0.021)	Loss 2.9321e-01 (3.1308e-01)	Acc@1  93.00 ( 90.82)	Acc@5 100.00 ( 99.76)
 * Acc@1 91.000 Acc@5 99.760
### epoch[83] execution time: 18.05450129508972
EPOCH 84
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [84][  0/391]	Time  0.215 ( 0.215)	Data  0.171 ( 0.171)	Loss 3.5339e-02 (3.5339e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [84][ 10/391]	Time  0.041 ( 0.056)	Data  0.001 ( 0.016)	Loss 4.8859e-02 (4.8766e-02)	Acc@1  99.22 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [84][ 20/391]	Time  0.040 ( 0.049)	Data  0.001 ( 0.009)	Loss 9.4238e-02 (6.5871e-02)	Acc@1  97.66 ( 98.18)	Acc@5 100.00 (100.00)
Epoch: [84][ 30/391]	Time  0.038 ( 0.046)	Data  0.001 ( 0.006)	Loss 9.1553e-02 (7.2009e-02)	Acc@1  96.88 ( 97.76)	Acc@5 100.00 (100.00)
Epoch: [84][ 40/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.005)	Loss 4.4556e-02 (7.1177e-02)	Acc@1  97.66 ( 97.73)	Acc@5 100.00 (100.00)
Epoch: [84][ 50/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.004)	Loss 8.4961e-02 (7.2628e-02)	Acc@1  97.66 ( 97.63)	Acc@5 100.00 (100.00)
Epoch: [84][ 60/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.1194e-01 (7.4520e-02)	Acc@1  95.31 ( 97.52)	Acc@5 100.00 (100.00)
Epoch: [84][ 70/391]	Time  0.041 ( 0.042)	Data  0.002 ( 0.003)	Loss 8.4900e-02 (7.4194e-02)	Acc@1  96.09 ( 97.54)	Acc@5 100.00 (100.00)
Epoch: [84][ 80/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.7587e-02 (7.3375e-02)	Acc@1  99.22 ( 97.57)	Acc@5 100.00 (100.00)
Epoch: [84][ 90/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.0211e-01 (7.4413e-02)	Acc@1  96.88 ( 97.55)	Acc@5 100.00 (100.00)
Epoch: [84][100/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.4829e-02 (7.5007e-02)	Acc@1  96.88 ( 97.50)	Acc@5 100.00 ( 99.99)
Epoch: [84][110/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.2642e-02 (7.5724e-02)	Acc@1  97.66 ( 97.48)	Acc@5 100.00 ( 99.99)
Epoch: [84][120/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.6082e-02 (7.4246e-02)	Acc@1  97.66 ( 97.53)	Acc@5 100.00 ( 99.99)
Epoch: [84][130/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7302e-02 (7.4466e-02)	Acc@1 100.00 ( 97.55)	Acc@5 100.00 ( 99.99)
Epoch: [84][140/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.4961e-02 (7.3356e-02)	Acc@1  96.88 ( 97.62)	Acc@5 100.00 ( 99.99)
Epoch: [84][150/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.5603e-02 (7.3649e-02)	Acc@1  98.44 ( 97.61)	Acc@5 100.00 ( 99.99)
Epoch: [84][160/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0303e-01 (7.3553e-02)	Acc@1  96.09 ( 97.62)	Acc@5 100.00 ( 99.99)
Epoch: [84][170/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.6854e-02 (7.3484e-02)	Acc@1  97.66 ( 97.61)	Acc@5 100.00 ( 99.99)
Epoch: [84][180/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.1848e-02 (7.3851e-02)	Acc@1  96.88 ( 97.60)	Acc@5 100.00 ( 99.99)
Epoch: [84][190/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8177e-02 (7.3555e-02)	Acc@1  99.22 ( 97.61)	Acc@5 100.00 ( 99.99)
Epoch: [84][200/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4250e-02 (7.3154e-02)	Acc@1  99.22 ( 97.64)	Acc@5 100.00 ( 99.99)
Epoch: [84][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.9875e-02 (7.3233e-02)	Acc@1  98.44 ( 97.65)	Acc@5 100.00 ( 99.99)
Epoch: [84][220/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.0200e-02 (7.3386e-02)	Acc@1  97.66 ( 97.65)	Acc@5 100.00 ( 99.99)
Epoch: [84][230/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.2224e-02 (7.3331e-02)	Acc@1  96.88 ( 97.66)	Acc@5 100.00 ( 99.99)
Epoch: [84][240/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.6416e-02 (7.3898e-02)	Acc@1  96.09 ( 97.64)	Acc@5 100.00 ( 99.99)
Epoch: [84][250/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.4871e-02 (7.3720e-02)	Acc@1  98.44 ( 97.65)	Acc@5 100.00 ( 99.99)
Epoch: [84][260/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7048e-02 (7.3481e-02)	Acc@1  99.22 ( 97.64)	Acc@5 100.00 ( 99.99)
Epoch: [84][270/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.5012e-02 (7.3089e-02)	Acc@1  96.88 ( 97.66)	Acc@5 100.00 ( 99.99)
Epoch: [84][280/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.3862e-02 (7.3190e-02)	Acc@1  98.44 ( 97.66)	Acc@5 100.00 ( 99.99)
Epoch: [84][290/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.6548e-02 (7.3364e-02)	Acc@1  97.66 ( 97.67)	Acc@5 100.00 ( 99.98)
Epoch: [84][300/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.6355e-02 (7.3051e-02)	Acc@1  97.66 ( 97.70)	Acc@5 100.00 ( 99.98)
Epoch: [84][310/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.5695e-02 (7.3270e-02)	Acc@1  99.22 ( 97.68)	Acc@5 100.00 ( 99.98)
Epoch: [84][320/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.4036e-02 (7.3268e-02)	Acc@1  97.66 ( 97.69)	Acc@5 100.00 ( 99.98)
Epoch: [84][330/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 9.1797e-02 (7.3952e-02)	Acc@1  96.09 ( 97.65)	Acc@5 100.00 ( 99.98)
Epoch: [84][340/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.2399e-02 (7.3831e-02)	Acc@1  97.66 ( 97.64)	Acc@5 100.00 ( 99.98)
Epoch: [84][350/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.5969e-02 (7.3856e-02)	Acc@1  97.66 ( 97.64)	Acc@5 100.00 ( 99.98)
Epoch: [84][360/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.7953e-02 (7.4180e-02)	Acc@1  98.44 ( 97.62)	Acc@5 100.00 ( 99.98)
Epoch: [84][370/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.4880e-02 (7.4300e-02)	Acc@1  98.44 ( 97.61)	Acc@5 100.00 ( 99.98)
Epoch: [84][380/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.7200e-02 (7.4107e-02)	Acc@1  96.88 ( 97.61)	Acc@5 100.00 ( 99.98)
Epoch: [84][390/391]	Time  0.029 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.4678e-02 (7.4161e-02)	Acc@1  98.75 ( 97.61)	Acc@5 100.00 ( 99.98)
## e[84] optimizer.zero_grad (sum) time: 0.25006937980651855
## e[84]       loss.backward (sum) time: 4.138538122177124
## e[84]      optimizer.step (sum) time: 1.7679839134216309
## epoch[84] training(only) time: 15.852864742279053
# Switched to evaluate mode...
Test: [  0/100]	Time  0.175 ( 0.175)	Loss 2.8418e-01 (2.8418e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.016 ( 0.033)	Loss 3.7744e-01 (2.9411e-01)	Acc@1  88.00 ( 91.27)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.020 ( 0.027)	Loss 3.7061e-01 (3.1287e-01)	Acc@1  89.00 ( 91.14)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.019 ( 0.025)	Loss 4.5361e-01 (3.2260e-01)	Acc@1  86.00 ( 90.81)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.017 ( 0.023)	Loss 3.9648e-01 (3.2225e-01)	Acc@1  88.00 ( 90.80)	Acc@5  99.00 ( 99.66)
Test: [ 50/100]	Time  0.018 ( 0.023)	Loss 1.8066e-01 (3.1408e-01)	Acc@1  94.00 ( 90.88)	Acc@5 100.00 ( 99.69)
Test: [ 60/100]	Time  0.023 ( 0.022)	Loss 3.7622e-01 (3.1324e-01)	Acc@1  91.00 ( 90.77)	Acc@5 100.00 ( 99.72)
Test: [ 70/100]	Time  0.022 ( 0.022)	Loss 4.7803e-01 (3.0915e-01)	Acc@1  86.00 ( 90.73)	Acc@5 100.00 ( 99.72)
Test: [ 80/100]	Time  0.017 ( 0.022)	Loss 1.1743e-01 (3.1477e-01)	Acc@1  95.00 ( 90.62)	Acc@5 100.00 ( 99.69)
Test: [ 90/100]	Time  0.021 ( 0.021)	Loss 2.8345e-01 (3.1305e-01)	Acc@1  93.00 ( 90.74)	Acc@5 100.00 ( 99.71)
 * Acc@1 90.860 Acc@5 99.720
### epoch[84] execution time: 18.096871614456177
EPOCH 85
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [85][  0/391]	Time  0.215 ( 0.215)	Data  0.174 ( 0.174)	Loss 1.3037e-01 (1.3037e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [85][ 10/391]	Time  0.038 ( 0.055)	Data  0.001 ( 0.017)	Loss 1.1456e-01 (9.1951e-02)	Acc@1  95.31 ( 97.30)	Acc@5 100.00 (100.00)
Epoch: [85][ 20/391]	Time  0.038 ( 0.048)	Data  0.001 ( 0.009)	Loss 5.6213e-02 (8.0858e-02)	Acc@1  98.44 ( 97.58)	Acc@5 100.00 (100.00)
Epoch: [85][ 30/391]	Time  0.038 ( 0.045)	Data  0.001 ( 0.006)	Loss 4.3579e-02 (7.8246e-02)	Acc@1  98.44 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [85][ 40/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.005)	Loss 9.3384e-02 (7.5260e-02)	Acc@1  95.31 ( 97.68)	Acc@5 100.00 (100.00)
Epoch: [85][ 50/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.004)	Loss 4.8706e-02 (7.3551e-02)	Acc@1 100.00 ( 97.73)	Acc@5 100.00 (100.00)
Epoch: [85][ 60/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 4.9255e-02 (7.2781e-02)	Acc@1  98.44 ( 97.72)	Acc@5 100.00 (100.00)
Epoch: [85][ 70/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.2571e-02 (7.1735e-02)	Acc@1  97.66 ( 97.76)	Acc@5 100.00 (100.00)
Epoch: [85][ 80/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.0577e-01 (7.2152e-02)	Acc@1  96.88 ( 97.74)	Acc@5 100.00 (100.00)
Epoch: [85][ 90/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.003)	Loss 1.0492e-01 (7.3683e-02)	Acc@1  96.09 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [85][100/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 6.9092e-02 (7.3002e-02)	Acc@1  97.66 ( 97.65)	Acc@5 100.00 (100.00)
Epoch: [85][110/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.6853e-02 (7.2097e-02)	Acc@1  96.09 ( 97.68)	Acc@5 100.00 (100.00)
Epoch: [85][120/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.3303e-02 (7.2141e-02)	Acc@1  97.66 ( 97.64)	Acc@5 100.00 (100.00)
Epoch: [85][130/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1361e-02 (7.1820e-02)	Acc@1 100.00 ( 97.68)	Acc@5 100.00 (100.00)
Epoch: [85][140/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.2957e-02 (7.1798e-02)	Acc@1  96.88 ( 97.67)	Acc@5 100.00 (100.00)
Epoch: [85][150/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.6904e-02 (7.2206e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [85][160/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.5022e-02 (7.2493e-02)	Acc@1  96.88 ( 97.62)	Acc@5 100.00 (100.00)
Epoch: [85][170/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1473e-02 (7.2687e-02)	Acc@1  98.44 ( 97.62)	Acc@5 100.00 (100.00)
Epoch: [85][180/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.0312e-02 (7.3003e-02)	Acc@1  97.66 ( 97.63)	Acc@5 100.00 (100.00)
Epoch: [85][190/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5466e-01 (7.3253e-02)	Acc@1  96.88 ( 97.62)	Acc@5 100.00 (100.00)
Epoch: [85][200/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.7281e-02 (7.3322e-02)	Acc@1  98.44 ( 97.63)	Acc@5 100.00 (100.00)
Epoch: [85][210/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.3660e-02 (7.3484e-02)	Acc@1  98.44 ( 97.62)	Acc@5 100.00 (100.00)
Epoch: [85][220/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.4331e-02 (7.3275e-02)	Acc@1  97.66 ( 97.63)	Acc@5 100.00 (100.00)
Epoch: [85][230/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.3689e-02 (7.3595e-02)	Acc@1  96.88 ( 97.61)	Acc@5 100.00 (100.00)
Epoch: [85][240/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.9050e-02 (7.3578e-02)	Acc@1  96.88 ( 97.59)	Acc@5 100.00 (100.00)
Epoch: [85][250/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 9.9243e-02 (7.3726e-02)	Acc@1  97.66 ( 97.58)	Acc@5 100.00 (100.00)
Epoch: [85][260/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 6.3904e-02 (7.3621e-02)	Acc@1  98.44 ( 97.60)	Acc@5 100.00 (100.00)
Epoch: [85][270/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 3.9062e-02 (7.3112e-02)	Acc@1  98.44 ( 97.61)	Acc@5 100.00 (100.00)
Epoch: [85][280/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.002)	Loss 8.9722e-02 (7.3320e-02)	Acc@1  96.09 ( 97.59)	Acc@5 100.00 (100.00)
Epoch: [85][290/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.002)	Loss 7.1655e-02 (7.3348e-02)	Acc@1  97.66 ( 97.60)	Acc@5 100.00 ( 99.99)
Epoch: [85][300/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.1365e-01 (7.3523e-02)	Acc@1  94.53 ( 97.58)	Acc@5 100.00 ( 99.99)
Epoch: [85][310/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.6956e-02 (7.3476e-02)	Acc@1  97.66 ( 97.57)	Acc@5 100.00 ( 99.99)
Epoch: [85][320/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.2948e-02 (7.3188e-02)	Acc@1  99.22 ( 97.58)	Acc@5 100.00 (100.00)
Epoch: [85][330/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 8.2092e-02 (7.3176e-02)	Acc@1  96.88 ( 97.59)	Acc@5 100.00 ( 99.99)
Epoch: [85][340/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.5511e-02 (7.3192e-02)	Acc@1  97.66 ( 97.59)	Acc@5 100.00 ( 99.99)
Epoch: [85][350/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.5256e-02 (7.2803e-02)	Acc@1  96.88 ( 97.59)	Acc@5 100.00 ( 99.99)
Epoch: [85][360/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.5562e-02 (7.2849e-02)	Acc@1  97.66 ( 97.59)	Acc@5 100.00 ( 99.99)
Epoch: [85][370/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.5115e-02 (7.2597e-02)	Acc@1  98.44 ( 97.60)	Acc@5 100.00 ( 99.99)
Epoch: [85][380/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.3171e-02 (7.2763e-02)	Acc@1  96.88 ( 97.58)	Acc@5 100.00 ( 99.99)
Epoch: [85][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.5847e-02 (7.2796e-02)	Acc@1 100.00 ( 97.58)	Acc@5 100.00 ( 99.99)
## e[85] optimizer.zero_grad (sum) time: 0.24920153617858887
## e[85]       loss.backward (sum) time: 4.121463298797607
## e[85]      optimizer.step (sum) time: 1.745744228363037
## epoch[85] training(only) time: 15.782433986663818
# Switched to evaluate mode...
Test: [  0/100]	Time  0.170 ( 0.170)	Loss 2.5684e-01 (2.5684e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.018 ( 0.033)	Loss 3.8843e-01 (2.8331e-01)	Acc@1  87.00 ( 91.18)	Acc@5  99.00 ( 99.73)
Test: [ 20/100]	Time  0.021 ( 0.026)	Loss 3.6182e-01 (3.0602e-01)	Acc@1  87.00 ( 91.00)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.022 ( 0.025)	Loss 4.4824e-01 (3.1329e-01)	Acc@1  88.00 ( 90.90)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.020 ( 0.024)	Loss 3.9502e-01 (3.1519e-01)	Acc@1  89.00 ( 90.85)	Acc@5  99.00 ( 99.68)
Test: [ 50/100]	Time  0.020 ( 0.023)	Loss 1.8579e-01 (3.0915e-01)	Acc@1  94.00 ( 90.96)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.023 ( 0.023)	Loss 3.9038e-01 (3.0974e-01)	Acc@1  91.00 ( 90.80)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.021 ( 0.022)	Loss 4.4995e-01 (3.0685e-01)	Acc@1  87.00 ( 90.75)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.020 ( 0.022)	Loss 1.0651e-01 (3.1136e-01)	Acc@1  93.00 ( 90.57)	Acc@5 100.00 ( 99.72)
Test: [ 90/100]	Time  0.022 ( 0.022)	Loss 2.5928e-01 (3.0972e-01)	Acc@1  94.00 ( 90.73)	Acc@5 100.00 ( 99.74)
 * Acc@1 90.840 Acc@5 99.740
### epoch[85] execution time: 18.088003396987915
EPOCH 86
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [86][  0/391]	Time  0.222 ( 0.222)	Data  0.176 ( 0.176)	Loss 7.3853e-02 (7.3853e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [86][ 10/391]	Time  0.041 ( 0.057)	Data  0.001 ( 0.017)	Loss 6.5125e-02 (7.3933e-02)	Acc@1  98.44 ( 97.87)	Acc@5 100.00 ( 99.93)
Epoch: [86][ 20/391]	Time  0.039 ( 0.049)	Data  0.001 ( 0.009)	Loss 1.0870e-01 (7.7727e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 ( 99.93)
Epoch: [86][ 30/391]	Time  0.040 ( 0.046)	Data  0.001 ( 0.007)	Loss 3.2867e-02 (7.8175e-02)	Acc@1 100.00 ( 97.58)	Acc@5 100.00 ( 99.95)
Epoch: [86][ 40/391]	Time  0.040 ( 0.045)	Data  0.001 ( 0.005)	Loss 5.7251e-02 (7.7888e-02)	Acc@1  98.44 ( 97.54)	Acc@5 100.00 ( 99.96)
Epoch: [86][ 50/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.004)	Loss 4.8065e-02 (7.5113e-02)	Acc@1  98.44 ( 97.61)	Acc@5 100.00 ( 99.97)
Epoch: [86][ 60/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.004)	Loss 4.9774e-02 (7.4518e-02)	Acc@1  97.66 ( 97.57)	Acc@5 100.00 ( 99.96)
Epoch: [86][ 70/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.1595e-02 (7.2734e-02)	Acc@1  99.22 ( 97.59)	Acc@5 100.00 ( 99.97)
Epoch: [86][ 80/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.9591e-02 (7.3448e-02)	Acc@1  98.44 ( 97.58)	Acc@5 100.00 ( 99.97)
Epoch: [86][ 90/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.7781e-02 (7.3125e-02)	Acc@1  99.22 ( 97.62)	Acc@5 100.00 ( 99.97)
Epoch: [86][100/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 9.3384e-02 (7.4576e-02)	Acc@1  96.88 ( 97.51)	Acc@5 100.00 ( 99.98)
Epoch: [86][110/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.7161e-02 (7.2505e-02)	Acc@1  99.22 ( 97.59)	Acc@5 100.00 ( 99.98)
Epoch: [86][120/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.8064e-02 (7.3391e-02)	Acc@1  97.66 ( 97.53)	Acc@5 100.00 ( 99.98)
Epoch: [86][130/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.9479e-02 (7.2705e-02)	Acc@1  98.44 ( 97.53)	Acc@5 100.00 ( 99.98)
Epoch: [86][140/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8096e-02 (7.2784e-02)	Acc@1  97.66 ( 97.54)	Acc@5 100.00 ( 99.98)
Epoch: [86][150/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.5989e-02 (7.1945e-02)	Acc@1  98.44 ( 97.60)	Acc@5 100.00 ( 99.98)
Epoch: [86][160/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.6284e-02 (7.2394e-02)	Acc@1  99.22 ( 97.61)	Acc@5 100.00 ( 99.99)
Epoch: [86][170/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3762e-02 (7.2621e-02)	Acc@1 100.00 ( 97.61)	Acc@5 100.00 ( 99.99)
Epoch: [86][180/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3708e-01 (7.2484e-02)	Acc@1  96.88 ( 97.61)	Acc@5 100.00 ( 99.99)
Epoch: [86][190/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.7200e-02 (7.2707e-02)	Acc@1  97.66 ( 97.59)	Acc@5 100.00 ( 99.98)
Epoch: [86][200/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.0210e-02 (7.2783e-02)	Acc@1  97.66 ( 97.61)	Acc@5 100.00 ( 99.98)
Epoch: [86][210/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.4880e-02 (7.2130e-02)	Acc@1  97.66 ( 97.65)	Acc@5 100.00 ( 99.99)
Epoch: [86][220/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1383e-01 (7.2575e-02)	Acc@1  96.09 ( 97.62)	Acc@5 100.00 ( 99.99)
Epoch: [86][230/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.1035e-02 (7.3137e-02)	Acc@1  98.44 ( 97.60)	Acc@5 100.00 ( 99.99)
Epoch: [86][240/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.3130e-02 (7.2890e-02)	Acc@1  97.66 ( 97.59)	Acc@5 100.00 ( 99.99)
Epoch: [86][250/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.3120e-02 (7.2663e-02)	Acc@1  97.66 ( 97.60)	Acc@5 100.00 ( 99.99)
Epoch: [86][260/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2366e-01 (7.2683e-02)	Acc@1  93.75 ( 97.58)	Acc@5 100.00 ( 99.99)
Epoch: [86][270/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.4351e-02 (7.2603e-02)	Acc@1  97.66 ( 97.58)	Acc@5 100.00 ( 99.99)
Epoch: [86][280/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.3013e-01 (7.2986e-02)	Acc@1  96.88 ( 97.58)	Acc@5 100.00 ( 99.99)
Epoch: [86][290/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.4768e-02 (7.2598e-02)	Acc@1  97.66 ( 97.59)	Acc@5 100.00 ( 99.99)
Epoch: [86][300/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.001)	Loss 8.8196e-02 (7.2791e-02)	Acc@1  96.09 ( 97.57)	Acc@5 100.00 ( 99.99)
Epoch: [86][310/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.8218e-02 (7.2638e-02)	Acc@1  98.44 ( 97.58)	Acc@5 100.00 ( 99.99)
Epoch: [86][320/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.3171e-02 (7.2733e-02)	Acc@1  99.22 ( 97.57)	Acc@5 100.00 ( 99.99)
Epoch: [86][330/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 8.3313e-02 (7.2480e-02)	Acc@1  96.09 ( 97.58)	Acc@5 100.00 ( 99.99)
Epoch: [86][340/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.9885e-02 (7.2196e-02)	Acc@1  96.88 ( 97.60)	Acc@5 100.00 ( 99.99)
Epoch: [86][350/391]	Time  0.043 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.0675e-01 (7.2676e-02)	Acc@1  97.66 ( 97.59)	Acc@5 100.00 ( 99.99)
Epoch: [86][360/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.3335e-02 (7.2731e-02)	Acc@1  99.22 ( 97.58)	Acc@5 100.00 ( 99.99)
Epoch: [86][370/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.0608e-01 (7.2900e-02)	Acc@1  94.53 ( 97.57)	Acc@5 100.00 ( 99.99)
Epoch: [86][380/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.3242e-02 (7.3180e-02)	Acc@1  97.66 ( 97.55)	Acc@5 100.00 ( 99.99)
Epoch: [86][390/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.001)	Loss 9.5581e-02 (7.3778e-02)	Acc@1  95.00 ( 97.53)	Acc@5 100.00 ( 99.99)
## e[86] optimizer.zero_grad (sum) time: 0.25092482566833496
## e[86]       loss.backward (sum) time: 4.170524597167969
## e[86]      optimizer.step (sum) time: 1.7676990032196045
## epoch[86] training(only) time: 15.828967332839966
# Switched to evaluate mode...
Test: [  0/100]	Time  0.173 ( 0.173)	Loss 2.6904e-01 (2.6904e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.024 ( 0.034)	Loss 3.5962e-01 (2.8986e-01)	Acc@1  87.00 ( 91.27)	Acc@5  99.00 ( 99.73)
Test: [ 20/100]	Time  0.017 ( 0.026)	Loss 3.5547e-01 (3.0976e-01)	Acc@1  90.00 ( 91.10)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.020 ( 0.024)	Loss 4.4849e-01 (3.1658e-01)	Acc@1  86.00 ( 90.87)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.016 ( 0.023)	Loss 3.9111e-01 (3.1715e-01)	Acc@1  88.00 ( 90.83)	Acc@5  99.00 ( 99.68)
Test: [ 50/100]	Time  0.017 ( 0.022)	Loss 1.9409e-01 (3.0951e-01)	Acc@1  93.00 ( 90.94)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.019 ( 0.022)	Loss 3.8623e-01 (3.0871e-01)	Acc@1  91.00 ( 90.80)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.023 ( 0.022)	Loss 4.2627e-01 (3.0505e-01)	Acc@1  88.00 ( 90.80)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.017 ( 0.021)	Loss 1.1377e-01 (3.0970e-01)	Acc@1  94.00 ( 90.62)	Acc@5 100.00 ( 99.70)
Test: [ 90/100]	Time  0.017 ( 0.021)	Loss 2.7197e-01 (3.0826e-01)	Acc@1  94.00 ( 90.76)	Acc@5 100.00 ( 99.70)
 * Acc@1 90.890 Acc@5 99.720
### epoch[86] execution time: 18.05854821205139
EPOCH 87
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [87][  0/391]	Time  0.211 ( 0.211)	Data  0.166 ( 0.166)	Loss 7.4707e-02 (7.4707e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [87][ 10/391]	Time  0.038 ( 0.055)	Data  0.001 ( 0.016)	Loss 4.7974e-02 (8.1379e-02)	Acc@1 100.00 ( 97.23)	Acc@5 100.00 (100.00)
Epoch: [87][ 20/391]	Time  0.042 ( 0.048)	Data  0.001 ( 0.009)	Loss 6.6650e-02 (8.4996e-02)	Acc@1  96.88 ( 97.10)	Acc@5 100.00 (100.00)
Epoch: [87][ 30/391]	Time  0.039 ( 0.045)	Data  0.001 ( 0.006)	Loss 7.0251e-02 (7.9246e-02)	Acc@1  98.44 ( 97.35)	Acc@5 100.00 ( 99.97)
Epoch: [87][ 40/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.005)	Loss 2.9556e-02 (7.7314e-02)	Acc@1 100.00 ( 97.54)	Acc@5 100.00 ( 99.98)
Epoch: [87][ 50/391]	Time  0.036 ( 0.043)	Data  0.001 ( 0.004)	Loss 3.7811e-02 (7.6523e-02)	Acc@1  99.22 ( 97.46)	Acc@5 100.00 ( 99.98)
Epoch: [87][ 60/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.004)	Loss 5.6641e-02 (7.8261e-02)	Acc@1  99.22 ( 97.40)	Acc@5 100.00 ( 99.99)
Epoch: [87][ 70/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 9.1492e-02 (7.7290e-02)	Acc@1  95.31 ( 97.40)	Acc@5 100.00 ( 99.99)
Epoch: [87][ 80/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.0449e-01 (7.6959e-02)	Acc@1  96.88 ( 97.43)	Acc@5 100.00 ( 99.99)
Epoch: [87][ 90/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 1.0242e-01 (7.6972e-02)	Acc@1  95.31 ( 97.42)	Acc@5 100.00 ( 99.99)
Epoch: [87][100/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 2.7847e-02 (7.6119e-02)	Acc@1 100.00 ( 97.47)	Acc@5 100.00 ( 99.99)
Epoch: [87][110/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.9336e-02 (7.3939e-02)	Acc@1  96.88 ( 97.56)	Acc@5 100.00 ( 99.99)
Epoch: [87][120/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.8868e-02 (7.2218e-02)	Acc@1  99.22 ( 97.64)	Acc@5 100.00 ( 99.99)
Epoch: [87][130/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.1279e-02 (7.1661e-02)	Acc@1  96.88 ( 97.64)	Acc@5 100.00 ( 99.99)
Epoch: [87][140/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.5754e-02 (7.1357e-02)	Acc@1  96.09 ( 97.66)	Acc@5 100.00 ( 99.99)
Epoch: [87][150/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1514e-02 (7.1002e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 ( 99.99)
Epoch: [87][160/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.6365e-02 (7.1378e-02)	Acc@1  96.88 ( 97.67)	Acc@5 100.00 ( 99.99)
Epoch: [87][170/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2969e-02 (7.0794e-02)	Acc@1  98.44 ( 97.71)	Acc@5 100.00 ( 99.99)
Epoch: [87][180/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 8.0017e-02 (7.1069e-02)	Acc@1  95.31 ( 97.68)	Acc@5 100.00 ( 99.98)
Epoch: [87][190/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.5576e-01 (7.2007e-02)	Acc@1  96.09 ( 97.64)	Acc@5 100.00 ( 99.98)
Epoch: [87][200/391]	Time  0.046 ( 0.040)	Data  0.001 ( 0.002)	Loss 7.5623e-02 (7.2191e-02)	Acc@1  96.88 ( 97.63)	Acc@5 100.00 ( 99.98)
Epoch: [87][210/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 9.7046e-02 (7.2589e-02)	Acc@1  98.44 ( 97.64)	Acc@5 100.00 ( 99.99)
Epoch: [87][220/391]	Time  0.053 ( 0.040)	Data  0.001 ( 0.002)	Loss 8.1482e-02 (7.1938e-02)	Acc@1  96.88 ( 97.66)	Acc@5 100.00 ( 99.99)
Epoch: [87][230/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 4.7913e-02 (7.1109e-02)	Acc@1  99.22 ( 97.69)	Acc@5 100.00 ( 99.99)
Epoch: [87][240/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 8.2153e-02 (7.1050e-02)	Acc@1  96.09 ( 97.69)	Acc@5 100.00 ( 99.99)
Epoch: [87][250/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 6.1523e-02 (7.0871e-02)	Acc@1  97.66 ( 97.69)	Acc@5 100.00 ( 99.99)
Epoch: [87][260/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.2695e-01 (7.1420e-02)	Acc@1  96.88 ( 97.68)	Acc@5 100.00 ( 99.99)
Epoch: [87][270/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 6.8909e-02 (7.1218e-02)	Acc@1  97.66 ( 97.68)	Acc@5 100.00 ( 99.99)
Epoch: [87][280/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.002)	Loss 4.3304e-02 (7.1160e-02)	Acc@1 100.00 ( 97.68)	Acc@5 100.00 ( 99.99)
Epoch: [87][290/391]	Time  0.042 ( 0.040)	Data  0.001 ( 0.002)	Loss 9.3750e-02 (7.2029e-02)	Acc@1  96.88 ( 97.65)	Acc@5 100.00 ( 99.99)
Epoch: [87][300/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.002)	Loss 8.9661e-02 (7.2168e-02)	Acc@1  96.88 ( 97.63)	Acc@5 100.00 ( 99.99)
Epoch: [87][310/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.6040e-02 (7.2023e-02)	Acc@1  98.44 ( 97.64)	Acc@5 100.00 ( 99.99)
Epoch: [87][320/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.001)	Loss 6.5613e-02 (7.1952e-02)	Acc@1  97.66 ( 97.64)	Acc@5 100.00 ( 99.99)
Epoch: [87][330/391]	Time  0.046 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.8308e-02 (7.1947e-02)	Acc@1  96.88 ( 97.64)	Acc@5 100.00 ( 99.99)
Epoch: [87][340/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 8.9478e-02 (7.2090e-02)	Acc@1  96.88 ( 97.64)	Acc@5 100.00 ( 99.99)
Epoch: [87][350/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 4.3060e-02 (7.2253e-02)	Acc@1 100.00 ( 97.63)	Acc@5 100.00 ( 99.99)
Epoch: [87][360/391]	Time  0.043 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.5562e-02 (7.2236e-02)	Acc@1  97.66 ( 97.63)	Acc@5 100.00 ( 99.99)
Epoch: [87][370/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.4463e-02 (7.2060e-02)	Acc@1  98.44 ( 97.64)	Acc@5 100.00 ( 99.99)
Epoch: [87][380/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.001)	Loss 7.1960e-02 (7.2378e-02)	Acc@1  96.09 ( 97.62)	Acc@5 100.00 ( 99.99)
Epoch: [87][390/391]	Time  0.031 ( 0.040)	Data  0.001 ( 0.001)	Loss 9.4971e-02 (7.2143e-02)	Acc@1  96.25 ( 97.63)	Acc@5 100.00 ( 99.99)
## e[87] optimizer.zero_grad (sum) time: 0.2510354518890381
## e[87]       loss.backward (sum) time: 4.074407339096069
## e[87]      optimizer.step (sum) time: 1.7790000438690186
## epoch[87] training(only) time: 15.718382120132446
# Switched to evaluate mode...
Test: [  0/100]	Time  0.175 ( 0.175)	Loss 2.5732e-01 (2.5732e-01)	Acc@1  93.00 ( 93.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.021 ( 0.035)	Loss 3.7549e-01 (2.8726e-01)	Acc@1  88.00 ( 91.64)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.022 ( 0.028)	Loss 3.7231e-01 (3.1056e-01)	Acc@1  86.00 ( 91.14)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.022 ( 0.026)	Loss 4.5776e-01 (3.1613e-01)	Acc@1  87.00 ( 90.94)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.024 ( 0.025)	Loss 4.1040e-01 (3.1869e-01)	Acc@1  90.00 ( 90.80)	Acc@5  99.00 ( 99.66)
Test: [ 50/100]	Time  0.019 ( 0.024)	Loss 1.9202e-01 (3.1176e-01)	Acc@1  93.00 ( 90.92)	Acc@5 100.00 ( 99.69)
Test: [ 60/100]	Time  0.020 ( 0.023)	Loss 3.9062e-01 (3.1240e-01)	Acc@1  91.00 ( 90.77)	Acc@5 100.00 ( 99.72)
Test: [ 70/100]	Time  0.021 ( 0.022)	Loss 4.3115e-01 (3.0870e-01)	Acc@1  87.00 ( 90.76)	Acc@5 100.00 ( 99.72)
Test: [ 80/100]	Time  0.019 ( 0.022)	Loss 9.8816e-02 (3.1265e-01)	Acc@1  93.00 ( 90.63)	Acc@5 100.00 ( 99.69)
Test: [ 90/100]	Time  0.021 ( 0.022)	Loss 2.5220e-01 (3.1137e-01)	Acc@1  94.00 ( 90.76)	Acc@5 100.00 ( 99.71)
 * Acc@1 90.930 Acc@5 99.730
### epoch[87] execution time: 17.9973406791687
EPOCH 88
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [88][  0/391]	Time  0.214 ( 0.214)	Data  0.175 ( 0.175)	Loss 7.1289e-02 (7.1289e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [88][ 10/391]	Time  0.038 ( 0.055)	Data  0.001 ( 0.017)	Loss 9.1125e-02 (7.3649e-02)	Acc@1  97.66 ( 98.08)	Acc@5 100.00 (100.00)
Epoch: [88][ 20/391]	Time  0.041 ( 0.048)	Data  0.001 ( 0.009)	Loss 4.7882e-02 (7.3685e-02)	Acc@1  99.22 ( 98.07)	Acc@5 100.00 (100.00)
Epoch: [88][ 30/391]	Time  0.038 ( 0.046)	Data  0.001 ( 0.006)	Loss 6.0303e-02 (7.4482e-02)	Acc@1  96.09 ( 97.78)	Acc@5 100.00 (100.00)
Epoch: [88][ 40/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.005)	Loss 1.6272e-01 (7.4737e-02)	Acc@1  94.53 ( 97.75)	Acc@5 100.00 ( 99.98)
Epoch: [88][ 50/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.004)	Loss 2.8809e-02 (7.3230e-02)	Acc@1 100.00 ( 97.81)	Acc@5 100.00 ( 99.98)
Epoch: [88][ 60/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.0693e-01 (7.2609e-02)	Acc@1  96.09 ( 97.80)	Acc@5 100.00 ( 99.99)
Epoch: [88][ 70/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.2725e-02 (7.5342e-02)	Acc@1  98.44 ( 97.65)	Acc@5 100.00 ( 99.98)
Epoch: [88][ 80/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.5684e-02 (7.5045e-02)	Acc@1  96.88 ( 97.62)	Acc@5 100.00 ( 99.98)
Epoch: [88][ 90/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.1591e-01 (7.5719e-02)	Acc@1  94.53 ( 97.56)	Acc@5 100.00 ( 99.98)
Epoch: [88][100/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.003)	Loss 1.1469e-01 (7.5924e-02)	Acc@1  96.09 ( 97.54)	Acc@5 100.00 ( 99.98)
Epoch: [88][110/391]	Time  0.048 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3457e-02 (7.5471e-02)	Acc@1  98.44 ( 97.54)	Acc@5 100.00 ( 99.98)
Epoch: [88][120/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.7098e-02 (7.5479e-02)	Acc@1  98.44 ( 97.53)	Acc@5 100.00 ( 99.98)
Epoch: [88][130/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9377e-02 (7.4685e-02)	Acc@1  97.66 ( 97.56)	Acc@5 100.00 ( 99.98)
Epoch: [88][140/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.3201e-02 (7.4495e-02)	Acc@1  97.66 ( 97.58)	Acc@5 100.00 ( 99.98)
Epoch: [88][150/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0598e-02 (7.4169e-02)	Acc@1  99.22 ( 97.58)	Acc@5 100.00 ( 99.98)
Epoch: [88][160/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2976e-01 (7.4245e-02)	Acc@1  96.88 ( 97.59)	Acc@5 100.00 ( 99.99)
Epoch: [88][170/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.7404e-02 (7.3662e-02)	Acc@1  98.44 ( 97.61)	Acc@5 100.00 ( 99.99)
Epoch: [88][180/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.9143e-02 (7.3631e-02)	Acc@1  98.44 ( 97.61)	Acc@5 100.00 ( 99.99)
Epoch: [88][190/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.2988e-02 (7.2522e-02)	Acc@1  98.44 ( 97.67)	Acc@5 100.00 ( 99.99)
Epoch: [88][200/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.9661e-02 (7.2953e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 ( 99.98)
Epoch: [88][210/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.4829e-02 (7.3183e-02)	Acc@1  96.09 ( 97.65)	Acc@5 100.00 ( 99.99)
Epoch: [88][220/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.1768e-02 (7.3186e-02)	Acc@1  98.44 ( 97.65)	Acc@5 100.00 ( 99.99)
Epoch: [88][230/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.0374e-02 (7.2995e-02)	Acc@1  97.66 ( 97.64)	Acc@5 100.00 ( 99.99)
Epoch: [88][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.6711e-02 (7.2638e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 ( 99.99)
Epoch: [88][250/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.2927e-02 (7.2279e-02)	Acc@1  97.66 ( 97.65)	Acc@5 100.00 ( 99.98)
Epoch: [88][260/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3762e-02 (7.2408e-02)	Acc@1  99.22 ( 97.66)	Acc@5 100.00 ( 99.99)
Epoch: [88][270/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.7017e-02 (7.2402e-02)	Acc@1  96.88 ( 97.65)	Acc@5 100.00 ( 99.99)
Epoch: [88][280/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1237e-01 (7.2461e-02)	Acc@1  97.66 ( 97.65)	Acc@5 100.00 ( 99.99)
Epoch: [88][290/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 9.5398e-02 (7.2329e-02)	Acc@1  96.09 ( 97.66)	Acc@5 100.00 ( 99.99)
Epoch: [88][300/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 9.1431e-02 (7.2276e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 ( 99.99)
Epoch: [88][310/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.1047e-01 (7.2897e-02)	Acc@1  94.53 ( 97.61)	Acc@5 100.00 ( 99.98)
Epoch: [88][320/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.1411e-02 (7.2945e-02)	Acc@1  97.66 ( 97.60)	Acc@5 100.00 ( 99.99)
Epoch: [88][330/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.3184e-01 (7.3070e-02)	Acc@1  96.88 ( 97.59)	Acc@5 100.00 ( 99.99)
Epoch: [88][340/391]	Time  0.049 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.3467e-02 (7.2642e-02)	Acc@1  99.22 ( 97.62)	Acc@5 100.00 ( 99.99)
Epoch: [88][350/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.0376e-01 (7.2932e-02)	Acc@1  95.31 ( 97.61)	Acc@5 100.00 ( 99.99)
Epoch: [88][360/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.4819e-02 (7.2958e-02)	Acc@1  97.66 ( 97.61)	Acc@5 100.00 ( 99.99)
Epoch: [88][370/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.1830e-02 (7.2635e-02)	Acc@1 100.00 ( 97.63)	Acc@5 100.00 ( 99.99)
Epoch: [88][380/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.001)	Loss 5.4840e-02 (7.2571e-02)	Acc@1  99.22 ( 97.64)	Acc@5 100.00 ( 99.99)
Epoch: [88][390/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.001)	Loss 3.4668e-02 (7.2722e-02)	Acc@1 100.00 ( 97.63)	Acc@5 100.00 ( 99.99)
## e[88] optimizer.zero_grad (sum) time: 0.2501060962677002
## e[88]       loss.backward (sum) time: 4.161121368408203
## e[88]      optimizer.step (sum) time: 1.7443263530731201
## epoch[88] training(only) time: 15.923601150512695
# Switched to evaluate mode...
Test: [  0/100]	Time  0.178 ( 0.178)	Loss 2.7222e-01 (2.7222e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.017 ( 0.034)	Loss 3.6865e-01 (2.8922e-01)	Acc@1  88.00 ( 91.00)	Acc@5  99.00 ( 99.73)
Test: [ 20/100]	Time  0.024 ( 0.027)	Loss 3.5645e-01 (3.0923e-01)	Acc@1  88.00 ( 90.90)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.021 ( 0.024)	Loss 4.5972e-01 (3.1529e-01)	Acc@1  86.00 ( 90.81)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.024 ( 0.023)	Loss 4.0503e-01 (3.1705e-01)	Acc@1  89.00 ( 90.78)	Acc@5  99.00 ( 99.68)
Test: [ 50/100]	Time  0.019 ( 0.022)	Loss 1.9360e-01 (3.0981e-01)	Acc@1  94.00 ( 90.88)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.023 ( 0.022)	Loss 3.7158e-01 (3.0906e-01)	Acc@1  91.00 ( 90.82)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.022 ( 0.022)	Loss 4.6387e-01 (3.0642e-01)	Acc@1  87.00 ( 90.80)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.019 ( 0.021)	Loss 9.6313e-02 (3.1123e-01)	Acc@1  94.00 ( 90.60)	Acc@5 100.00 ( 99.72)
Test: [ 90/100]	Time  0.021 ( 0.021)	Loss 2.4744e-01 (3.1017e-01)	Acc@1  94.00 ( 90.76)	Acc@5 100.00 ( 99.73)
 * Acc@1 90.930 Acc@5 99.740
### epoch[88] execution time: 18.165237188339233
EPOCH 89
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [89][  0/391]	Time  0.219 ( 0.219)	Data  0.172 ( 0.172)	Loss 5.9357e-02 (5.9357e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [89][ 10/391]	Time  0.040 ( 0.056)	Data  0.001 ( 0.016)	Loss 6.9824e-02 (8.1404e-02)	Acc@1  97.66 ( 97.09)	Acc@5 100.00 (100.00)
Epoch: [89][ 20/391]	Time  0.042 ( 0.049)	Data  0.001 ( 0.009)	Loss 4.7211e-02 (6.7297e-02)	Acc@1  98.44 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [89][ 30/391]	Time  0.038 ( 0.046)	Data  0.001 ( 0.006)	Loss 3.6133e-02 (6.6612e-02)	Acc@1  98.44 ( 97.76)	Acc@5 100.00 (100.00)
Epoch: [89][ 40/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.005)	Loss 3.7476e-02 (6.7020e-02)	Acc@1  99.22 ( 97.73)	Acc@5 100.00 (100.00)
Epoch: [89][ 50/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.2061e-01 (6.7342e-02)	Acc@1  93.75 ( 97.70)	Acc@5 100.00 (100.00)
Epoch: [89][ 60/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 7.7087e-02 (6.8083e-02)	Acc@1  96.88 ( 97.69)	Acc@5 100.00 (100.00)
Epoch: [89][ 70/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.7394e-02 (6.9033e-02)	Acc@1  98.44 ( 97.65)	Acc@5 100.00 (100.00)
Epoch: [89][ 80/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.9956e-02 (7.1226e-02)	Acc@1  97.66 ( 97.55)	Acc@5 100.00 (100.00)
Epoch: [89][ 90/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.9419e-02 (7.2040e-02)	Acc@1  99.22 ( 97.54)	Acc@5 100.00 (100.00)
Epoch: [89][100/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.9153e-02 (7.1735e-02)	Acc@1  97.66 ( 97.58)	Acc@5 100.00 (100.00)
Epoch: [89][110/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.6469e-02 (7.0796e-02)	Acc@1 100.00 ( 97.64)	Acc@5 100.00 (100.00)
Epoch: [89][120/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9103e-02 (7.1132e-02)	Acc@1  98.44 ( 97.61)	Acc@5 100.00 (100.00)
Epoch: [89][130/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1595e-02 (7.0822e-02)	Acc@1  98.44 ( 97.64)	Acc@5 100.00 (100.00)
Epoch: [89][140/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.6783e-02 (7.0206e-02)	Acc@1  99.22 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [89][150/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.8684e-02 (7.0582e-02)	Acc@1  96.88 ( 97.64)	Acc@5 100.00 (100.00)
Epoch: [89][160/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.0608e-02 (7.0677e-02)	Acc@1  97.66 ( 97.62)	Acc@5 100.00 (100.00)
Epoch: [89][170/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.1228e-02 (7.0324e-02)	Acc@1  97.66 ( 97.63)	Acc@5 100.00 (100.00)
Epoch: [89][180/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0083e-01 (7.0748e-02)	Acc@1  96.88 ( 97.64)	Acc@5 100.00 ( 99.99)
Epoch: [89][190/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9957e-02 (7.1157e-02)	Acc@1  98.44 ( 97.63)	Acc@5 100.00 ( 99.99)
Epoch: [89][200/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8971e-02 (7.0819e-02)	Acc@1  99.22 ( 97.63)	Acc@5 100.00 ( 99.99)
Epoch: [89][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3365e-02 (7.1031e-02)	Acc@1  99.22 ( 97.62)	Acc@5 100.00 ( 99.99)
Epoch: [89][220/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9520e-02 (7.1330e-02)	Acc@1 100.00 ( 97.62)	Acc@5 100.00 ( 99.99)
Epoch: [89][230/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.0730e-02 (7.0817e-02)	Acc@1  98.44 ( 97.63)	Acc@5 100.00 ( 99.99)
Epoch: [89][240/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.1919e-02 (7.0830e-02)	Acc@1  97.66 ( 97.63)	Acc@5 100.00 ( 99.99)
Epoch: [89][250/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.3435e-02 (7.1119e-02)	Acc@1  95.31 ( 97.63)	Acc@5 100.00 ( 99.99)
Epoch: [89][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0894e-02 (7.0900e-02)	Acc@1  98.44 ( 97.66)	Acc@5 100.00 ( 99.99)
Epoch: [89][270/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2643e-02 (7.0256e-02)	Acc@1  98.44 ( 97.69)	Acc@5 100.00 ( 99.99)
Epoch: [89][280/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.9214e-02 (7.0939e-02)	Acc@1  98.44 ( 97.67)	Acc@5 100.00 ( 99.99)
Epoch: [89][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.9885e-02 (7.0605e-02)	Acc@1  96.88 ( 97.67)	Acc@5 100.00 ( 99.99)
Epoch: [89][300/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.0687e-01 (7.0481e-02)	Acc@1  96.09 ( 97.67)	Acc@5 100.00 ( 99.99)
Epoch: [89][310/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.1361e-02 (7.0694e-02)	Acc@1  99.22 ( 97.67)	Acc@5 100.00 ( 99.99)
Epoch: [89][320/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.6804e-02 (7.0224e-02)	Acc@1  99.22 ( 97.69)	Acc@5 100.00 ( 99.99)
Epoch: [89][330/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.3020e-02 (7.0662e-02)	Acc@1  99.22 ( 97.67)	Acc@5 100.00 ( 99.99)
Epoch: [89][340/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.4718e-02 (7.0866e-02)	Acc@1  98.44 ( 97.66)	Acc@5 100.00 ( 99.99)
Epoch: [89][350/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.5847e-02 (7.0946e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 ( 99.99)
Epoch: [89][360/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 8.5205e-02 (7.1082e-02)	Acc@1  96.09 ( 97.65)	Acc@5 100.00 ( 99.99)
Epoch: [89][370/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.8552e-02 (7.0988e-02)	Acc@1  99.22 ( 97.67)	Acc@5 100.00 ( 99.99)
Epoch: [89][380/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.1228e-02 (7.1159e-02)	Acc@1  99.22 ( 97.66)	Acc@5 100.00 ( 99.99)
Epoch: [89][390/391]	Time  0.029 ( 0.040)	Data  0.001 ( 0.001)	Loss 1.2744e-01 (7.1541e-02)	Acc@1  96.25 ( 97.64)	Acc@5 100.00 ( 99.99)
## e[89] optimizer.zero_grad (sum) time: 0.24942684173583984
## e[89]       loss.backward (sum) time: 4.204882621765137
## e[89]      optimizer.step (sum) time: 1.7433209419250488
## epoch[89] training(only) time: 15.918869972229004
# Switched to evaluate mode...
Test: [  0/100]	Time  0.174 ( 0.174)	Loss 2.6733e-01 (2.6733e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 3.6548e-01 (2.9294e-01)	Acc@1  88.00 ( 91.18)	Acc@5  99.00 ( 99.64)
Test: [ 20/100]	Time  0.017 ( 0.027)	Loss 3.5986e-01 (3.1144e-01)	Acc@1  89.00 ( 91.19)	Acc@5 100.00 ( 99.62)
Test: [ 30/100]	Time  0.017 ( 0.025)	Loss 4.5776e-01 (3.1766e-01)	Acc@1  86.00 ( 90.94)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.017 ( 0.023)	Loss 4.0601e-01 (3.1795e-01)	Acc@1  89.00 ( 90.95)	Acc@5  99.00 ( 99.66)
Test: [ 50/100]	Time  0.020 ( 0.022)	Loss 1.8103e-01 (3.1081e-01)	Acc@1  94.00 ( 91.02)	Acc@5 100.00 ( 99.69)
Test: [ 60/100]	Time  0.017 ( 0.022)	Loss 3.7329e-01 (3.0960e-01)	Acc@1  91.00 ( 90.90)	Acc@5 100.00 ( 99.72)
Test: [ 70/100]	Time  0.023 ( 0.022)	Loss 4.7754e-01 (3.0694e-01)	Acc@1  86.00 ( 90.85)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.017 ( 0.021)	Loss 9.7961e-02 (3.1175e-01)	Acc@1  95.00 ( 90.67)	Acc@5 100.00 ( 99.70)
Test: [ 90/100]	Time  0.019 ( 0.021)	Loss 2.5635e-01 (3.1077e-01)	Acc@1  94.00 ( 90.76)	Acc@5 100.00 ( 99.71)
 * Acc@1 90.900 Acc@5 99.730
### epoch[89] execution time: 18.12758207321167
### Training complete:
#### total training(only) time: 1432.2273406982422
##### Total run time: 1638.729050397873
