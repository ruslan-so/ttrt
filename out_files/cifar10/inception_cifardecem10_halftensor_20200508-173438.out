# Model: inception
# Dataset: cifardecem
# Batch size: 128
# Freezeout: False
# Low precision: True
# Epochs: 90
# Initial learning rate: 0.1
# module_name: models_cifardecem.inception
<function inception at 0x7fa249251f28>
# model requested: 'inception'
# printing out the model
InceptionV3(
  (Conv2d_1a_3x3): BasicConv2d(
    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (Conv2d_2a_3x3): BasicConv2d(
    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (Conv2d_2b_3x3): BasicConv2d(
    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (Conv2d_3b_1x1): BasicConv2d(
    (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (Conv2d_4a_3x3): BasicConv2d(
    (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)
    (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (Mixed_5b): InceptionA(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch5x5): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branch3x3): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branchpool): Sequential(
      (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
      (1): BasicConv2d(
        (conv): Conv2d(192, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
  )
  (Mixed_5c): InceptionA(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch5x5): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branch3x3): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branchpool): Sequential(
      (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
      (1): BasicConv2d(
        (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
  )
  (Mixed_5d): InceptionA(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch5x5): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branch3x3): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branchpool): Sequential(
      (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
      (1): BasicConv2d(
        (conv): Conv2d(288, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
  )
  (Mixed_6a): InceptionB(
    (branch3x3): BasicConv2d(
      (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3stack): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branchpool): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (Mixed_6b): InceptionC(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch7x7): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branch7x7stack): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): BasicConv2d(
        (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): BasicConv2d(
        (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branch_pool): Sequential(
      (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
      (1): BasicConv2d(
        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
  )
  (Mixed_6c): InceptionC(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch7x7): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branch7x7stack): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): BasicConv2d(
        (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): BasicConv2d(
        (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branch_pool): Sequential(
      (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
      (1): BasicConv2d(
        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
  )
  (Mixed_6d): InceptionC(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch7x7): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branch7x7stack): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): BasicConv2d(
        (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): BasicConv2d(
        (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branch_pool): Sequential(
      (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
      (1): BasicConv2d(
        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
  )
  (Mixed_6e): InceptionC(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch7x7): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branch7x7stack): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): BasicConv2d(
        (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): BasicConv2d(
        (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branch_pool): Sequential(
      (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
      (1): BasicConv2d(
        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
  )
  (Mixed_7a): InceptionD(
    (branch3x3): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)
        (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branch7x7): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): BasicConv2d(
        (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branchpool): AvgPool2d(kernel_size=3, stride=2, padding=0)
  )
  (Mixed_7b): InceptionE(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3_1): BasicConv2d(
      (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3_2a): BasicConv2d(
      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3_2b): BasicConv2d(
      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3stack_1): BasicConv2d(
      (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3stack_2): BasicConv2d(
      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3stack_3a): BasicConv2d(
      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3stack_3b): BasicConv2d(
      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch_pool): Sequential(
      (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
      (1): BasicConv2d(
        (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
  )
  (Mixed_7c): InceptionE(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3_1): BasicConv2d(
      (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3_2a): BasicConv2d(
      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3_2b): BasicConv2d(
      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3stack_1): BasicConv2d(
      (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3stack_2): BasicConv2d(
      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3stack_3a): BasicConv2d(
      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3stack_3b): BasicConv2d(
      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch_pool): Sequential(
      (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
      (1): BasicConv2d(
        (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout2d(p=0.5, inplace=False)
  (linear): Linear(in_features=2048, out_features=10, bias=True)
)
# model is low precision
# Model: inception
# Dataset: cifardecem
# Freezeout: False
# Low precision: True
# Initial learning rate: 0.1
# Criterion: CrossEntropyLoss()
# Optimizer: SGD
#	lr 0.1
#	momentum 0.9
#	dampening 0
#	weight_decay 0.0001
#	nesterov False
CIFAR10 loading and normalization
==> Preparing data..
# CIFAR10 / low precision
Files already downloaded and verified
Files already downloaded and verified
#--> Training data: <--#
#-->>
EPOCH 0
# current learning rate: 0.1
# Switched to train mode...
Epoch: [0][  0/391]	Time  5.125 ( 5.125)	Data  0.105 ( 0.105)	Loss 2.3750e+00 (2.3750e+00)	Acc@1  10.94 ( 10.94)	Acc@5  51.56 ( 51.56)
Epoch: [0][ 10/391]	Time  0.159 ( 0.612)	Data  0.001 ( 0.010)	Loss 4.0039e+00 (3.3858e+00)	Acc@1  11.72 ( 10.80)	Acc@5  50.78 ( 51.63)
Epoch: [0][ 20/391]	Time  0.158 ( 0.397)	Data  0.001 ( 0.006)	Loss 2.5938e+00 (3.1391e+00)	Acc@1  14.06 ( 12.09)	Acc@5  50.00 ( 53.72)
Epoch: [0][ 30/391]	Time  0.159 ( 0.320)	Data  0.001 ( 0.004)	Loss 2.5156e+00 (2.9230e+00)	Acc@1  16.41 ( 13.53)	Acc@5  60.16 ( 55.80)
Epoch: [0][ 40/391]	Time  0.159 ( 0.281)	Data  0.001 ( 0.003)	Loss 2.5762e+00 (2.7884e+00)	Acc@1  11.72 ( 14.56)	Acc@5  69.53 ( 59.15)
Epoch: [0][ 50/391]	Time  0.159 ( 0.257)	Data  0.001 ( 0.003)	Loss 2.5000e+00 (2.7175e+00)	Acc@1  15.62 ( 15.03)	Acc@5  64.84 ( 61.47)
Epoch: [0][ 60/391]	Time  0.159 ( 0.241)	Data  0.001 ( 0.003)	Loss 2.5801e+00 (2.6534e+00)	Acc@1  17.97 ( 15.33)	Acc@5  70.31 ( 63.13)
Epoch: [0][ 70/391]	Time  0.159 ( 0.230)	Data  0.001 ( 0.002)	Loss 2.2227e+00 (2.5951e+00)	Acc@1  20.31 ( 15.83)	Acc@5  68.75 ( 64.71)
Epoch: [0][ 80/391]	Time  0.160 ( 0.221)	Data  0.001 ( 0.002)	Loss 2.2461e+00 (2.5409e+00)	Acc@1  22.66 ( 16.49)	Acc@5  77.34 ( 66.20)
Epoch: [0][ 90/391]	Time  0.158 ( 0.215)	Data  0.001 ( 0.002)	Loss 2.2383e+00 (2.5006e+00)	Acc@1  18.75 ( 16.98)	Acc@5  72.66 ( 67.33)
Epoch: [0][100/391]	Time  0.158 ( 0.209)	Data  0.001 ( 0.002)	Loss 1.9355e+00 (2.4626e+00)	Acc@1  28.12 ( 17.44)	Acc@5  82.03 ( 68.56)
Epoch: [0][110/391]	Time  0.159 ( 0.205)	Data  0.001 ( 0.002)	Loss 2.2734e+00 (2.4320e+00)	Acc@1  28.91 ( 17.88)	Acc@5  84.38 ( 69.58)
Epoch: [0][120/391]	Time  0.160 ( 0.201)	Data  0.001 ( 0.002)	Loss 2.0000e+00 (2.4060e+00)	Acc@1  20.31 ( 18.07)	Acc@5  85.16 ( 70.44)
Epoch: [0][130/391]	Time  0.159 ( 0.198)	Data  0.001 ( 0.002)	Loss 2.0039e+00 (2.3829e+00)	Acc@1  21.09 ( 18.37)	Acc@5  80.47 ( 71.11)
Epoch: [0][140/391]	Time  0.160 ( 0.195)	Data  0.001 ( 0.002)	Loss 2.1641e+00 (2.3632e+00)	Acc@1  22.66 ( 18.76)	Acc@5  81.25 ( 71.83)
Epoch: [0][150/391]	Time  0.158 ( 0.193)	Data  0.001 ( 0.002)	Loss 2.1348e+00 (2.3487e+00)	Acc@1  17.19 ( 18.97)	Acc@5  82.03 ( 72.38)
Epoch: [0][160/391]	Time  0.165 ( 0.191)	Data  0.001 ( 0.002)	Loss 2.2109e+00 (2.3314e+00)	Acc@1  25.78 ( 19.26)	Acc@5  78.12 ( 72.89)
Epoch: [0][170/391]	Time  0.159 ( 0.189)	Data  0.001 ( 0.002)	Loss 1.9756e+00 (2.3207e+00)	Acc@1  23.44 ( 19.35)	Acc@5  82.03 ( 73.21)
Epoch: [0][180/391]	Time  0.161 ( 0.188)	Data  0.001 ( 0.002)	Loss 2.1270e+00 (2.3082e+00)	Acc@1  19.53 ( 19.50)	Acc@5  78.12 ( 73.57)
Epoch: [0][190/391]	Time  0.159 ( 0.186)	Data  0.001 ( 0.002)	Loss 1.9814e+00 (2.2959e+00)	Acc@1  15.62 ( 19.54)	Acc@5  78.12 ( 73.92)
Epoch: [0][200/391]	Time  0.160 ( 0.185)	Data  0.001 ( 0.002)	Loss 2.0820e+00 (2.2837e+00)	Acc@1  21.09 ( 19.69)	Acc@5  74.22 ( 74.15)
Epoch: [0][210/391]	Time  0.159 ( 0.184)	Data  0.001 ( 0.001)	Loss 2.0312e+00 (2.2698e+00)	Acc@1  26.56 ( 20.01)	Acc@5  80.47 ( 74.51)
Epoch: [0][220/391]	Time  0.159 ( 0.183)	Data  0.001 ( 0.001)	Loss 2.0801e+00 (2.2582e+00)	Acc@1  21.09 ( 20.20)	Acc@5  78.91 ( 74.87)
Epoch: [0][230/391]	Time  0.158 ( 0.182)	Data  0.001 ( 0.001)	Loss 1.9512e+00 (2.2462e+00)	Acc@1  30.47 ( 20.44)	Acc@5  84.38 ( 75.20)
Epoch: [0][240/391]	Time  0.160 ( 0.181)	Data  0.001 ( 0.001)	Loss 1.8838e+00 (2.2364e+00)	Acc@1  32.03 ( 20.62)	Acc@5  82.03 ( 75.41)
Epoch: [0][250/391]	Time  0.160 ( 0.180)	Data  0.001 ( 0.001)	Loss 1.9316e+00 (2.2270e+00)	Acc@1  28.12 ( 20.75)	Acc@5  82.81 ( 75.62)
Epoch: [0][260/391]	Time  0.163 ( 0.179)	Data  0.001 ( 0.001)	Loss 2.1582e+00 (2.2199e+00)	Acc@1  15.62 ( 20.87)	Acc@5  69.53 ( 75.78)
Epoch: [0][270/391]	Time  0.158 ( 0.179)	Data  0.001 ( 0.001)	Loss 2.0840e+00 (2.2118e+00)	Acc@1  20.31 ( 20.99)	Acc@5  80.47 ( 75.94)
Epoch: [0][280/391]	Time  0.159 ( 0.178)	Data  0.001 ( 0.001)	Loss 2.0938e+00 (2.2051e+00)	Acc@1  17.19 ( 21.10)	Acc@5  83.59 ( 76.16)
Epoch: [0][290/391]	Time  0.159 ( 0.177)	Data  0.001 ( 0.001)	Loss 1.9551e+00 (2.1973e+00)	Acc@1  21.09 ( 21.30)	Acc@5  84.38 ( 76.44)
Epoch: [0][300/391]	Time  0.159 ( 0.177)	Data  0.001 ( 0.001)	Loss 1.8867e+00 (2.1921e+00)	Acc@1  26.56 ( 21.36)	Acc@5  85.16 ( 76.59)
Epoch: [0][310/391]	Time  0.159 ( 0.176)	Data  0.001 ( 0.001)	Loss 1.9736e+00 (2.1872e+00)	Acc@1  28.91 ( 21.46)	Acc@5  80.47 ( 76.79)
Epoch: [0][320/391]	Time  0.161 ( 0.176)	Data  0.001 ( 0.001)	Loss 1.9756e+00 (2.1813e+00)	Acc@1  28.91 ( 21.64)	Acc@5  79.69 ( 76.94)
Epoch: [0][330/391]	Time  0.163 ( 0.175)	Data  0.001 ( 0.001)	Loss 1.8818e+00 (2.1760e+00)	Acc@1  27.34 ( 21.73)	Acc@5  89.06 ( 77.11)
Epoch: [0][340/391]	Time  0.159 ( 0.175)	Data  0.001 ( 0.001)	Loss 2.1191e+00 (2.1707e+00)	Acc@1  22.66 ( 21.85)	Acc@5  73.44 ( 77.25)
Epoch: [0][350/391]	Time  0.159 ( 0.174)	Data  0.001 ( 0.001)	Loss 2.1680e+00 (2.1644e+00)	Acc@1  25.00 ( 21.97)	Acc@5  75.78 ( 77.40)
Epoch: [0][360/391]	Time  0.169 ( 0.174)	Data  0.001 ( 0.001)	Loss 1.9727e+00 (2.1610e+00)	Acc@1  27.34 ( 22.03)	Acc@5  80.47 ( 77.53)
Epoch: [0][370/391]	Time  0.159 ( 0.174)	Data  0.001 ( 0.001)	Loss 2.0195e+00 (2.1563e+00)	Acc@1  27.34 ( 22.13)	Acc@5  85.94 ( 77.69)
Epoch: [0][380/391]	Time  0.159 ( 0.173)	Data  0.001 ( 0.001)	Loss 1.8506e+00 (2.1504e+00)	Acc@1  28.12 ( 22.22)	Acc@5  84.38 ( 77.82)
Epoch: [0][390/391]	Time  1.866 ( 0.177)	Data  0.001 ( 0.001)	Loss 2.0625e+00 (2.1465e+00)	Acc@1  27.50 ( 22.28)	Acc@5  86.25 ( 77.91)
## e[0] optimizer.zero_grad (sum) time: 0.5509040355682373
## e[0]       loss.backward (sum) time: 14.847565174102783
## e[0]      optimizer.step (sum) time: 25.549957752227783
## epoch[0] training(only) time: 69.39448380470276
# Switched to evaluate mode...
Test: [  0/100]	Time  0.740 ( 0.740)	Loss 2.4453e+00 (2.4453e+00)	Acc@1  20.00 ( 20.00)	Acc@5  73.00 ( 73.00)
Test: [ 10/100]	Time  0.048 ( 0.116)	Loss 2.3125e+00 (2.2549e+00)	Acc@1  18.00 ( 20.55)	Acc@5  67.00 ( 76.82)
Test: [ 20/100]	Time  0.046 ( 0.083)	Loss 2.1367e+00 (2.4407e+00)	Acc@1  26.00 ( 21.52)	Acc@5  81.00 ( 75.90)
Test: [ 30/100]	Time  0.056 ( 0.072)	Loss 2.2363e+00 (2.3762e+00)	Acc@1  30.00 ( 21.19)	Acc@5  77.00 ( 75.06)
Test: [ 40/100]	Time  0.046 ( 0.066)	Loss 2.2422e+00 (2.3416e+00)	Acc@1  23.00 ( 20.83)	Acc@5  72.00 ( 74.71)
Test: [ 50/100]	Time  0.046 ( 0.062)	Loss 2.3438e+00 (2.3227e+00)	Acc@1  14.00 ( 21.24)	Acc@5  72.00 ( 74.94)
Test: [ 60/100]	Time  0.046 ( 0.059)	Loss 2.2090e+00 (2.3146e+00)	Acc@1  21.00 ( 20.97)	Acc@5  74.00 ( 74.36)
Test: [ 70/100]	Time  0.046 ( 0.057)	Loss 2.6230e+00 (2.3105e+00)	Acc@1  19.00 ( 20.92)	Acc@5  67.00 ( 74.06)
Test: [ 80/100]	Time  0.046 ( 0.056)	Loss 2.1016e+00 (2.3128e+00)	Acc@1  21.00 ( 21.12)	Acc@5  81.00 ( 74.21)
Test: [ 90/100]	Time  0.046 ( 0.055)	Loss 2.1543e+00 (2.2994e+00)	Acc@1  20.00 ( 21.03)	Acc@5  74.00 ( 74.02)
 * Acc@1 21.030 Acc@5 74.040
### epoch[0] execution time: 74.93523931503296
EPOCH 1
# current learning rate: 0.1
# Switched to train mode...
Epoch: [1][  0/391]	Time  0.348 ( 0.348)	Data  0.149 ( 0.149)	Loss 1.9570e+00 (1.9570e+00)	Acc@1  25.00 ( 25.00)	Acc@5  86.72 ( 86.72)
Epoch: [1][ 10/391]	Time  0.160 ( 0.177)	Data  0.001 ( 0.014)	Loss 1.9473e+00 (1.9347e+00)	Acc@1  23.44 ( 25.00)	Acc@5  82.81 ( 84.66)
Epoch: [1][ 20/391]	Time  0.159 ( 0.169)	Data  0.001 ( 0.008)	Loss 1.9209e+00 (1.9300e+00)	Acc@1  21.88 ( 26.56)	Acc@5  86.72 ( 84.04)
Epoch: [1][ 30/391]	Time  0.159 ( 0.166)	Data  0.001 ( 0.006)	Loss 1.8516e+00 (1.9252e+00)	Acc@1  30.47 ( 27.17)	Acc@5  89.06 ( 83.62)
Epoch: [1][ 40/391]	Time  0.159 ( 0.165)	Data  0.001 ( 0.005)	Loss 1.9766e+00 (1.9334e+00)	Acc@1  21.09 ( 26.79)	Acc@5  80.47 ( 83.38)
Epoch: [1][ 50/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.9229e+00 (1.9324e+00)	Acc@1  28.12 ( 26.85)	Acc@5  84.38 ( 83.41)
Epoch: [1][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.9824e+00 (1.9273e+00)	Acc@1  28.12 ( 26.87)	Acc@5  82.03 ( 83.57)
Epoch: [1][ 70/391]	Time  0.159 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.8232e+00 (1.9173e+00)	Acc@1  34.38 ( 27.44)	Acc@5  84.38 ( 83.70)
Epoch: [1][ 80/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.1641e+00 (1.9262e+00)	Acc@1  20.31 ( 27.30)	Acc@5  78.91 ( 83.48)
Epoch: [1][ 90/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.0684e+00 (1.9296e+00)	Acc@1  24.22 ( 27.15)	Acc@5  82.03 ( 83.71)
Epoch: [1][100/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.0625e+00 (1.9295e+00)	Acc@1  29.69 ( 27.24)	Acc@5  83.59 ( 83.73)
Epoch: [1][110/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.8984e+00 (1.9278e+00)	Acc@1  30.47 ( 27.25)	Acc@5  88.28 ( 83.90)
Epoch: [1][120/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.9277e+00 (1.9249e+00)	Acc@1  32.81 ( 27.49)	Acc@5  80.47 ( 83.97)
Epoch: [1][130/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.8525e+00 (1.9203e+00)	Acc@1  31.25 ( 27.76)	Acc@5  82.81 ( 84.00)
Epoch: [1][140/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.8184e+00 (1.9181e+00)	Acc@1  31.25 ( 27.85)	Acc@5  89.84 ( 84.15)
Epoch: [1][150/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.8467e+00 (1.9151e+00)	Acc@1  33.59 ( 27.96)	Acc@5  83.59 ( 84.20)
Epoch: [1][160/391]	Time  0.171 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7168e+00 (1.9094e+00)	Acc@1  30.47 ( 28.12)	Acc@5  90.62 ( 84.38)
Epoch: [1][170/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.9658e+00 (1.9070e+00)	Acc@1  32.81 ( 28.35)	Acc@5  80.47 ( 84.36)
Epoch: [1][180/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8008e+00 (1.9023e+00)	Acc@1  35.16 ( 28.48)	Acc@5  84.38 ( 84.45)
Epoch: [1][190/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7686e+00 (1.8980e+00)	Acc@1  38.28 ( 28.63)	Acc@5  83.59 ( 84.60)
Epoch: [1][200/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7754e+00 (1.8934e+00)	Acc@1  31.25 ( 28.74)	Acc@5  87.50 ( 84.72)
Epoch: [1][210/391]	Time  0.172 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8730e+00 (1.8918e+00)	Acc@1  35.94 ( 28.88)	Acc@5  82.03 ( 84.75)
Epoch: [1][220/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8613e+00 (1.8891e+00)	Acc@1  32.03 ( 28.98)	Acc@5  85.94 ( 84.77)
Epoch: [1][230/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8936e+00 (1.8870e+00)	Acc@1  28.91 ( 29.16)	Acc@5  87.50 ( 84.86)
Epoch: [1][240/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7881e+00 (1.8835e+00)	Acc@1  29.69 ( 29.34)	Acc@5  89.06 ( 84.96)
Epoch: [1][250/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.9482e+00 (1.8810e+00)	Acc@1  28.91 ( 29.42)	Acc@5  85.94 ( 85.03)
Epoch: [1][260/391]	Time  0.168 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7363e+00 (1.8775e+00)	Acc@1  32.03 ( 29.55)	Acc@5  89.84 ( 85.15)
Epoch: [1][270/391]	Time  0.166 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7715e+00 (1.8749e+00)	Acc@1  28.12 ( 29.68)	Acc@5  88.28 ( 85.25)
Epoch: [1][280/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8115e+00 (1.8720e+00)	Acc@1  35.16 ( 29.83)	Acc@5  85.16 ( 85.30)
Epoch: [1][290/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8896e+00 (1.8718e+00)	Acc@1  33.59 ( 29.93)	Acc@5  86.72 ( 85.29)
Epoch: [1][300/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6836e+00 (1.8703e+00)	Acc@1  32.03 ( 30.03)	Acc@5  92.97 ( 85.35)
Epoch: [1][310/391]	Time  0.168 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.9434e+00 (1.8695e+00)	Acc@1  31.25 ( 30.10)	Acc@5  82.03 ( 85.33)
Epoch: [1][320/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8887e+00 (1.8663e+00)	Acc@1  29.69 ( 30.24)	Acc@5  88.28 ( 85.40)
Epoch: [1][330/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8213e+00 (1.8654e+00)	Acc@1  34.38 ( 30.42)	Acc@5  87.50 ( 85.41)
Epoch: [1][340/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6738e+00 (1.8633e+00)	Acc@1  34.38 ( 30.46)	Acc@5  92.19 ( 85.49)
Epoch: [1][350/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6943e+00 (1.8595e+00)	Acc@1  35.16 ( 30.62)	Acc@5  90.62 ( 85.56)
Epoch: [1][360/391]	Time  0.168 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.6777e+00 (1.8558e+00)	Acc@1  39.84 ( 30.76)	Acc@5  86.72 ( 85.64)
Epoch: [1][370/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.7227e+00 (1.8538e+00)	Acc@1  33.59 ( 30.85)	Acc@5  86.72 ( 85.66)
Epoch: [1][380/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.6973e+00 (1.8516e+00)	Acc@1  41.41 ( 30.96)	Acc@5  89.84 ( 85.70)
Epoch: [1][390/391]	Time  0.115 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.7236e+00 (1.8497e+00)	Acc@1  38.75 ( 31.06)	Acc@5  87.50 ( 85.77)
## e[1] optimizer.zero_grad (sum) time: 0.5433602333068848
## e[1]       loss.backward (sum) time: 12.109315872192383
## e[1]      optimizer.step (sum) time: 25.815388917922974
## epoch[1] training(only) time: 63.30058789253235
# Switched to evaluate mode...
Test: [  0/100]	Time  0.178 ( 0.178)	Loss 1.6113e+00 (1.6113e+00)	Acc@1  45.00 ( 45.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 1.6055e+00 (1.6762e+00)	Acc@1  39.00 ( 39.00)	Acc@5  89.00 ( 90.45)
Test: [ 20/100]	Time  0.048 ( 0.054)	Loss 1.6348e+00 (1.7464e+00)	Acc@1  30.00 ( 38.29)	Acc@5  89.00 ( 89.86)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.6455e+00 (1.7335e+00)	Acc@1  43.00 ( 38.55)	Acc@5  92.00 ( 89.45)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.6826e+00 (1.7250e+00)	Acc@1  38.00 ( 38.68)	Acc@5  88.00 ( 89.17)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.5762e+00 (1.7008e+00)	Acc@1  35.00 ( 39.33)	Acc@5  91.00 ( 89.76)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.7783e+00 (1.7010e+00)	Acc@1  37.00 ( 39.15)	Acc@5  90.00 ( 89.66)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.6943e+00 (1.7126e+00)	Acc@1  35.00 ( 38.30)	Acc@5  86.00 ( 89.56)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 2.1328e+00 (1.7179e+00)	Acc@1  38.00 ( 38.43)	Acc@5  83.00 ( 89.69)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.6201e+00 (1.7150e+00)	Acc@1  38.00 ( 38.33)	Acc@5  96.00 ( 89.68)
 * Acc@1 38.460 Acc@5 89.620
### epoch[1] execution time: 68.25966024398804
EPOCH 2
# current learning rate: 0.1
# Switched to train mode...
Epoch: [2][  0/391]	Time  0.317 ( 0.317)	Data  0.147 ( 0.147)	Loss 1.7246e+00 (1.7246e+00)	Acc@1  39.06 ( 39.06)	Acc@5  88.28 ( 88.28)
Epoch: [2][ 10/391]	Time  0.160 ( 0.176)	Data  0.001 ( 0.014)	Loss 1.7178e+00 (1.7177e+00)	Acc@1  30.47 ( 35.94)	Acc@5  88.28 ( 88.85)
Epoch: [2][ 20/391]	Time  0.161 ( 0.168)	Data  0.001 ( 0.008)	Loss 1.7783e+00 (1.7451e+00)	Acc@1  31.25 ( 34.19)	Acc@5  87.50 ( 88.06)
Epoch: [2][ 30/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.006)	Loss 1.7568e+00 (1.7331e+00)	Acc@1  32.03 ( 34.83)	Acc@5  87.50 ( 88.23)
Epoch: [2][ 40/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.005)	Loss 1.7979e+00 (1.7468e+00)	Acc@1  38.28 ( 34.74)	Acc@5  86.72 ( 87.98)
Epoch: [2][ 50/391]	Time  0.159 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.6348e+00 (1.7347e+00)	Acc@1  45.31 ( 35.28)	Acc@5  89.06 ( 88.04)
Epoch: [2][ 60/391]	Time  0.159 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.7393e+00 (1.7358e+00)	Acc@1  37.50 ( 35.23)	Acc@5  90.62 ( 88.08)
Epoch: [2][ 70/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.7578e+00 (1.7326e+00)	Acc@1  34.38 ( 35.28)	Acc@5  89.06 ( 88.19)
Epoch: [2][ 80/391]	Time  0.172 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.8184e+00 (1.7369e+00)	Acc@1  30.47 ( 35.19)	Acc@5  85.16 ( 88.09)
Epoch: [2][ 90/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.6660e+00 (1.7309e+00)	Acc@1  39.84 ( 35.54)	Acc@5  85.94 ( 88.15)
Epoch: [2][100/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.6543e+00 (1.7291e+00)	Acc@1  35.16 ( 35.64)	Acc@5  90.62 ( 88.25)
Epoch: [2][110/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.002)	Loss 1.7197e+00 (1.7298e+00)	Acc@1  34.38 ( 35.61)	Acc@5  86.72 ( 88.26)
Epoch: [2][120/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7588e+00 (1.7318e+00)	Acc@1  33.59 ( 35.69)	Acc@5  88.28 ( 88.15)
Epoch: [2][130/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7324e+00 (1.7311e+00)	Acc@1  37.50 ( 35.61)	Acc@5  89.06 ( 88.30)
Epoch: [2][140/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7139e+00 (1.7284e+00)	Acc@1  39.84 ( 35.85)	Acc@5  85.16 ( 88.36)
Epoch: [2][150/391]	Time  0.173 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6562e+00 (1.7259e+00)	Acc@1  37.50 ( 35.93)	Acc@5  87.50 ( 88.34)
Epoch: [2][160/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5488e+00 (1.7206e+00)	Acc@1  39.84 ( 36.21)	Acc@5  92.97 ( 88.40)
Epoch: [2][170/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7285e+00 (1.7174e+00)	Acc@1  31.25 ( 36.23)	Acc@5  90.62 ( 88.51)
Epoch: [2][180/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7471e+00 (1.7141e+00)	Acc@1  36.72 ( 36.25)	Acc@5  84.38 ( 88.59)
Epoch: [2][190/391]	Time  0.175 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6816e+00 (1.7150e+00)	Acc@1  32.03 ( 36.10)	Acc@5  93.75 ( 88.68)
Epoch: [2][200/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.8076e+00 (1.7137e+00)	Acc@1  39.06 ( 36.28)	Acc@5  89.84 ( 88.72)
Epoch: [2][210/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6982e+00 (1.7132e+00)	Acc@1  36.72 ( 36.24)	Acc@5  89.06 ( 88.76)
Epoch: [2][220/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.8584e+00 (1.7114e+00)	Acc@1  31.25 ( 36.35)	Acc@5  89.06 ( 88.83)
Epoch: [2][230/391]	Time  0.175 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7998e+00 (1.7117e+00)	Acc@1  28.12 ( 36.34)	Acc@5  85.16 ( 88.80)
Epoch: [2][240/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5996e+00 (1.7098e+00)	Acc@1  43.75 ( 36.38)	Acc@5  92.19 ( 88.88)
Epoch: [2][250/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5703e+00 (1.7084e+00)	Acc@1  40.62 ( 36.49)	Acc@5  93.75 ( 88.92)
Epoch: [2][260/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7412e+00 (1.7083e+00)	Acc@1  35.16 ( 36.53)	Acc@5  88.28 ( 88.94)
Epoch: [2][270/391]	Time  0.165 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7822e+00 (1.7074e+00)	Acc@1  33.59 ( 36.56)	Acc@5  85.16 ( 88.94)
Epoch: [2][280/391]	Time  0.167 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5010e+00 (1.7042e+00)	Acc@1  40.62 ( 36.67)	Acc@5  91.41 ( 88.99)
Epoch: [2][290/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7168e+00 (1.7014e+00)	Acc@1  39.84 ( 36.80)	Acc@5  89.84 ( 89.03)
Epoch: [2][300/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7344e+00 (1.6998e+00)	Acc@1  32.81 ( 36.85)	Acc@5  87.50 ( 89.03)
Epoch: [2][310/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7217e+00 (1.6982e+00)	Acc@1  38.28 ( 36.96)	Acc@5  92.97 ( 89.02)
Epoch: [2][320/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8135e+00 (1.6953e+00)	Acc@1  39.84 ( 37.06)	Acc@5  83.59 ( 89.06)
Epoch: [2][330/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7852e+00 (1.6941e+00)	Acc@1  35.94 ( 37.13)	Acc@5  86.72 ( 89.08)
Epoch: [2][340/391]	Time  0.173 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6064e+00 (1.6903e+00)	Acc@1  39.06 ( 37.24)	Acc@5  91.41 ( 89.13)
Epoch: [2][350/391]	Time  0.176 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4209e+00 (1.6874e+00)	Acc@1  46.88 ( 37.34)	Acc@5  91.41 ( 89.18)
Epoch: [2][360/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.6650e+00 (1.6864e+00)	Acc@1  35.16 ( 37.34)	Acc@5  90.62 ( 89.20)
Epoch: [2][370/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.4941e+00 (1.6849e+00)	Acc@1  46.09 ( 37.39)	Acc@5  95.31 ( 89.19)
Epoch: [2][380/391]	Time  0.165 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.4766e+00 (1.6821e+00)	Acc@1  44.53 ( 37.49)	Acc@5  92.97 ( 89.21)
Epoch: [2][390/391]	Time  0.115 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.5381e+00 (1.6809e+00)	Acc@1  35.00 ( 37.54)	Acc@5  95.00 ( 89.25)
## e[2] optimizer.zero_grad (sum) time: 0.5317423343658447
## e[2]       loss.backward (sum) time: 12.161601066589355
## e[2]      optimizer.step (sum) time: 25.884793043136597
## epoch[2] training(only) time: 63.483821392059326
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 3.4824e+00 (3.4824e+00)	Acc@1  46.00 ( 46.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.046 ( 0.059)	Loss 2.4219e+00 (2.6997e+00)	Acc@1  49.00 ( 42.45)	Acc@5  86.00 ( 88.82)
Test: [ 20/100]	Time  0.047 ( 0.054)	Loss 2.3926e+00 (2.4066e+00)	Acc@1  43.00 ( 42.48)	Acc@5  93.00 ( 89.05)
Test: [ 30/100]	Time  0.047 ( 0.051)	Loss 1.8779e+00 (2.5026e+00)	Acc@1  38.00 ( 41.65)	Acc@5  92.00 ( 89.16)
Test: [ 40/100]	Time  0.047 ( 0.050)	Loss 2.2441e+00 (2.4879e+00)	Acc@1  32.00 ( 40.83)	Acc@5  83.00 ( 89.00)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 2.0625e+00 (2.4746e+00)	Acc@1  40.00 ( 40.80)	Acc@5  91.00 ( 89.20)
Test: [ 60/100]	Time  0.055 ( 0.049)	Loss 2.1289e+00 (2.4152e+00)	Acc@1  33.00 ( 40.72)	Acc@5  88.00 ( 89.38)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 4.5859e+00 (2.4551e+00)	Acc@1  38.00 ( 40.15)	Acc@5  90.00 ( 89.44)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 2.7988e+00 (2.4677e+00)	Acc@1  47.00 ( 40.40)	Acc@5  86.00 ( 89.52)
Test: [ 90/100]	Time  0.046 ( 0.048)	Loss 2.0020e+00 (2.4576e+00)	Acc@1  38.00 ( 40.41)	Acc@5  94.00 ( 89.49)
 * Acc@1 40.500 Acc@5 89.540
### epoch[2] execution time: 68.42114067077637
EPOCH 3
# current learning rate: 0.1
# Switched to train mode...
Epoch: [3][  0/391]	Time  0.315 ( 0.315)	Data  0.145 ( 0.145)	Loss 1.6279e+00 (1.6279e+00)	Acc@1  39.84 ( 39.84)	Acc@5  91.41 ( 91.41)
Epoch: [3][ 10/391]	Time  0.160 ( 0.175)	Data  0.001 ( 0.014)	Loss 1.5977e+00 (1.6376e+00)	Acc@1  37.50 ( 41.83)	Acc@5  93.75 ( 89.35)
Epoch: [3][ 20/391]	Time  0.161 ( 0.168)	Data  0.001 ( 0.008)	Loss 1.5586e+00 (1.6482e+00)	Acc@1  39.84 ( 41.15)	Acc@5  90.62 ( 89.03)
Epoch: [3][ 30/391]	Time  0.159 ( 0.166)	Data  0.001 ( 0.006)	Loss 1.6035e+00 (1.6516e+00)	Acc@1  41.41 ( 41.03)	Acc@5  89.84 ( 88.81)
Epoch: [3][ 40/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.005)	Loss 1.5234e+00 (1.6372e+00)	Acc@1  43.75 ( 40.95)	Acc@5  92.19 ( 89.16)
Epoch: [3][ 50/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.5674e+00 (1.6355e+00)	Acc@1  42.97 ( 40.73)	Acc@5  91.41 ( 89.38)
Epoch: [3][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.5215e+00 (1.6280e+00)	Acc@1  46.09 ( 40.75)	Acc@5  92.19 ( 89.72)
Epoch: [3][ 70/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.6035e+00 (1.6215e+00)	Acc@1  36.72 ( 40.81)	Acc@5  89.06 ( 89.89)
Epoch: [3][ 80/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.4795e+00 (1.6138e+00)	Acc@1  44.53 ( 41.19)	Acc@5  92.19 ( 90.07)
Epoch: [3][ 90/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.7617e+00 (1.6173e+00)	Acc@1  33.59 ( 40.93)	Acc@5  87.50 ( 90.10)
Epoch: [3][100/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.4990e+00 (1.6095e+00)	Acc@1  44.53 ( 41.16)	Acc@5  91.41 ( 90.25)
Epoch: [3][110/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5205e+00 (1.6072e+00)	Acc@1  46.09 ( 41.37)	Acc@5  92.97 ( 90.27)
Epoch: [3][120/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6064e+00 (1.6031e+00)	Acc@1  46.09 ( 41.51)	Acc@5  89.84 ( 90.26)
Epoch: [3][130/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5361e+00 (1.5988e+00)	Acc@1  50.00 ( 41.67)	Acc@5  93.75 ( 90.39)
Epoch: [3][140/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4863e+00 (1.5975e+00)	Acc@1  45.31 ( 41.58)	Acc@5  90.62 ( 90.48)
Epoch: [3][150/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5957e+00 (1.5974e+00)	Acc@1  43.75 ( 41.62)	Acc@5  90.62 ( 90.46)
Epoch: [3][160/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7461e+00 (1.5939e+00)	Acc@1  28.91 ( 41.77)	Acc@5  89.06 ( 90.53)
Epoch: [3][170/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6699e+00 (1.5910e+00)	Acc@1  36.72 ( 41.84)	Acc@5  89.84 ( 90.57)
Epoch: [3][180/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6201e+00 (1.5870e+00)	Acc@1  35.94 ( 41.96)	Acc@5  90.62 ( 90.65)
Epoch: [3][190/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5527e+00 (1.5818e+00)	Acc@1  42.97 ( 42.02)	Acc@5  88.28 ( 90.70)
Epoch: [3][200/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4551e+00 (1.5769e+00)	Acc@1  45.31 ( 42.20)	Acc@5  89.84 ( 90.72)
Epoch: [3][210/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5391e+00 (1.5735e+00)	Acc@1  43.75 ( 42.40)	Acc@5  92.19 ( 90.81)
Epoch: [3][220/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5537e+00 (1.5739e+00)	Acc@1  46.09 ( 42.26)	Acc@5  92.19 ( 90.84)
Epoch: [3][230/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4756e+00 (1.5702e+00)	Acc@1  43.75 ( 42.38)	Acc@5  93.75 ( 90.87)
Epoch: [3][240/391]	Time  0.167 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5596e+00 (1.5718e+00)	Acc@1  42.97 ( 42.34)	Acc@5  92.97 ( 90.85)
Epoch: [3][250/391]	Time  0.170 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6191e+00 (1.5697e+00)	Acc@1  43.75 ( 42.46)	Acc@5  88.28 ( 90.86)
Epoch: [3][260/391]	Time  0.177 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7471e+00 (1.5700e+00)	Acc@1  33.59 ( 42.44)	Acc@5  89.06 ( 90.83)
Epoch: [3][270/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6094e+00 (1.5698e+00)	Acc@1  37.50 ( 42.43)	Acc@5  90.62 ( 90.80)
Epoch: [3][280/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6338e+00 (1.5683e+00)	Acc@1  40.62 ( 42.48)	Acc@5  90.62 ( 90.82)
Epoch: [3][290/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4385e+00 (1.5674e+00)	Acc@1  45.31 ( 42.53)	Acc@5  92.19 ( 90.80)
Epoch: [3][300/391]	Time  0.171 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5273e+00 (1.5652e+00)	Acc@1  45.31 ( 42.59)	Acc@5  92.19 ( 90.83)
Epoch: [3][310/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3086e+00 (1.5615e+00)	Acc@1  48.44 ( 42.69)	Acc@5  95.31 ( 90.85)
Epoch: [3][320/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4199e+00 (1.5593e+00)	Acc@1  50.00 ( 42.82)	Acc@5  89.06 ( 90.88)
Epoch: [3][330/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4707e+00 (1.5556e+00)	Acc@1  43.75 ( 42.96)	Acc@5  95.31 ( 90.92)
Epoch: [3][340/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3623e+00 (1.5522e+00)	Acc@1  52.34 ( 43.09)	Acc@5  92.97 ( 90.98)
Epoch: [3][350/391]	Time  0.171 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3760e+00 (1.5491e+00)	Acc@1  49.22 ( 43.18)	Acc@5  92.19 ( 91.02)
Epoch: [3][360/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3730e+00 (1.5455e+00)	Acc@1  43.75 ( 43.32)	Acc@5  92.97 ( 91.05)
Epoch: [3][370/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4980e+00 (1.5421e+00)	Acc@1  44.53 ( 43.44)	Acc@5  92.19 ( 91.09)
Epoch: [3][380/391]	Time  0.177 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.4893e+00 (1.5378e+00)	Acc@1  43.75 ( 43.61)	Acc@5  93.75 ( 91.14)
Epoch: [3][390/391]	Time  0.114 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.2539e+00 (1.5335e+00)	Acc@1  55.00 ( 43.83)	Acc@5  95.00 ( 91.20)
## e[3] optimizer.zero_grad (sum) time: 0.5254130363464355
## e[3]       loss.backward (sum) time: 12.181137084960938
## e[3]      optimizer.step (sum) time: 25.895430088043213
## epoch[3] training(only) time: 63.4195921421051
# Switched to evaluate mode...
Test: [  0/100]	Time  0.179 ( 0.179)	Loss 1.3086e+00 (1.3086e+00)	Acc@1  62.00 ( 62.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.047 ( 0.058)	Loss 1.2080e+00 (1.4173e+00)	Acc@1  57.00 ( 53.09)	Acc@5  92.00 ( 92.73)
Test: [ 20/100]	Time  0.046 ( 0.053)	Loss 1.3164e+00 (1.4235e+00)	Acc@1  55.00 ( 52.76)	Acc@5  97.00 ( 93.67)
Test: [ 30/100]	Time  0.046 ( 0.051)	Loss 1.5176e+00 (1.4335e+00)	Acc@1  54.00 ( 51.42)	Acc@5  94.00 ( 93.55)
Test: [ 40/100]	Time  0.047 ( 0.050)	Loss 1.4336e+00 (1.4468e+00)	Acc@1  52.00 ( 51.05)	Acc@5  89.00 ( 93.34)
Test: [ 50/100]	Time  0.047 ( 0.049)	Loss 1.2744e+00 (1.4237e+00)	Acc@1  51.00 ( 51.39)	Acc@5  98.00 ( 93.76)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.5439e+00 (1.4192e+00)	Acc@1  45.00 ( 51.69)	Acc@5  92.00 ( 93.72)
Test: [ 70/100]	Time  0.048 ( 0.049)	Loss 1.3203e+00 (1.4188e+00)	Acc@1  50.00 ( 51.13)	Acc@5  93.00 ( 93.75)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.2637e+00 (1.4110e+00)	Acc@1  51.00 ( 51.15)	Acc@5  93.00 ( 93.91)
Test: [ 90/100]	Time  0.048 ( 0.048)	Loss 1.1670e+00 (1.4018e+00)	Acc@1  54.00 ( 51.25)	Acc@5  99.00 ( 93.98)
 * Acc@1 51.260 Acc@5 93.920
### epoch[3] execution time: 68.3487982749939
EPOCH 4
# current learning rate: 0.1
# Switched to train mode...
Epoch: [4][  0/391]	Time  0.316 ( 0.316)	Data  0.151 ( 0.151)	Loss 1.4678e+00 (1.4678e+00)	Acc@1  47.66 ( 47.66)	Acc@5  91.41 ( 91.41)
Epoch: [4][ 10/391]	Time  0.163 ( 0.175)	Data  0.001 ( 0.015)	Loss 1.2959e+00 (1.4004e+00)	Acc@1  56.25 ( 49.79)	Acc@5  92.19 ( 93.25)
Epoch: [4][ 20/391]	Time  0.164 ( 0.169)	Data  0.001 ( 0.008)	Loss 1.3320e+00 (1.4174e+00)	Acc@1  53.12 ( 48.62)	Acc@5  95.31 ( 92.82)
Epoch: [4][ 30/391]	Time  0.171 ( 0.167)	Data  0.001 ( 0.006)	Loss 1.4160e+00 (1.4175e+00)	Acc@1  46.09 ( 48.64)	Acc@5  92.19 ( 92.64)
Epoch: [4][ 40/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.005)	Loss 1.4062e+00 (1.4092e+00)	Acc@1  50.00 ( 49.12)	Acc@5  94.53 ( 92.78)
Epoch: [4][ 50/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.3271e+00 (1.3967e+00)	Acc@1  55.47 ( 49.36)	Acc@5  93.75 ( 92.94)
Epoch: [4][ 60/391]	Time  0.159 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.4316e+00 (1.4018e+00)	Acc@1  49.22 ( 49.35)	Acc@5  96.88 ( 92.84)
Epoch: [4][ 70/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.3271e+00 (1.3998e+00)	Acc@1  51.56 ( 49.39)	Acc@5  92.97 ( 92.85)
Epoch: [4][ 80/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.4277e+00 (1.3972e+00)	Acc@1  46.88 ( 49.52)	Acc@5  89.06 ( 92.92)
Epoch: [4][ 90/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.2891e+00 (1.3947e+00)	Acc@1  50.00 ( 49.37)	Acc@5  96.88 ( 93.04)
Epoch: [4][100/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.3281e+00 (1.3889e+00)	Acc@1  48.44 ( 49.53)	Acc@5  95.31 ( 93.14)
Epoch: [4][110/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3564e+00 (1.3899e+00)	Acc@1  53.91 ( 49.55)	Acc@5  90.62 ( 93.11)
Epoch: [4][120/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3223e+00 (1.3857e+00)	Acc@1  52.34 ( 49.77)	Acc@5  91.41 ( 93.06)
Epoch: [4][130/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3447e+00 (1.3866e+00)	Acc@1  48.44 ( 49.73)	Acc@5  92.97 ( 93.02)
Epoch: [4][140/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1387e+00 (1.3855e+00)	Acc@1  58.59 ( 49.64)	Acc@5  96.88 ( 93.03)
Epoch: [4][150/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4043e+00 (1.3860e+00)	Acc@1  48.44 ( 49.64)	Acc@5  91.41 ( 93.04)
Epoch: [4][160/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2900e+00 (1.3803e+00)	Acc@1  50.78 ( 49.84)	Acc@5  92.19 ( 93.09)
Epoch: [4][170/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2949e+00 (1.3787e+00)	Acc@1  55.47 ( 49.89)	Acc@5  97.66 ( 93.07)
Epoch: [4][180/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4609e+00 (1.3759e+00)	Acc@1  43.75 ( 50.03)	Acc@5  92.97 ( 93.09)
Epoch: [4][190/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1221e+00 (1.3706e+00)	Acc@1  61.72 ( 50.17)	Acc@5  92.97 ( 93.20)
Epoch: [4][200/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2910e+00 (1.3674e+00)	Acc@1  50.00 ( 50.28)	Acc@5  94.53 ( 93.26)
Epoch: [4][210/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4297e+00 (1.3658e+00)	Acc@1  48.44 ( 50.40)	Acc@5  94.53 ( 93.29)
Epoch: [4][220/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3887e+00 (1.3644e+00)	Acc@1  52.34 ( 50.51)	Acc@5  95.31 ( 93.28)
Epoch: [4][230/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3672e+00 (1.3635e+00)	Acc@1  50.78 ( 50.57)	Acc@5  93.75 ( 93.24)
Epoch: [4][240/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3848e+00 (1.3581e+00)	Acc@1  51.56 ( 50.72)	Acc@5  90.62 ( 93.32)
Epoch: [4][250/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3516e+00 (1.3531e+00)	Acc@1  51.56 ( 50.92)	Acc@5  92.97 ( 93.39)
Epoch: [4][260/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3525e+00 (1.3518e+00)	Acc@1  53.12 ( 50.99)	Acc@5  94.53 ( 93.42)
Epoch: [4][270/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3311e+00 (1.3498e+00)	Acc@1  50.78 ( 51.10)	Acc@5  96.88 ( 93.49)
Epoch: [4][280/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2656e+00 (1.3464e+00)	Acc@1  53.91 ( 51.24)	Acc@5  92.97 ( 93.53)
Epoch: [4][290/391]	Time  0.168 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3643e+00 (1.3443e+00)	Acc@1  55.47 ( 51.32)	Acc@5  92.19 ( 93.56)
Epoch: [4][300/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0811e+00 (1.3404e+00)	Acc@1  60.94 ( 51.48)	Acc@5  95.31 ( 93.57)
Epoch: [4][310/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3330e+00 (1.3377e+00)	Acc@1  55.47 ( 51.61)	Acc@5  92.97 ( 93.61)
Epoch: [4][320/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3457e+00 (1.3342e+00)	Acc@1  53.12 ( 51.76)	Acc@5  93.75 ( 93.65)
Epoch: [4][330/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3223e+00 (1.3321e+00)	Acc@1  50.78 ( 51.77)	Acc@5  92.19 ( 93.70)
Epoch: [4][340/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2676e+00 (1.3301e+00)	Acc@1  56.25 ( 51.90)	Acc@5  92.19 ( 93.69)
Epoch: [4][350/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2969e+00 (1.3279e+00)	Acc@1  49.22 ( 52.01)	Acc@5  92.19 ( 93.69)
Epoch: [4][360/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3525e+00 (1.3265e+00)	Acc@1  50.00 ( 52.12)	Acc@5  92.97 ( 93.70)
Epoch: [4][370/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3018e+00 (1.3256e+00)	Acc@1  50.78 ( 52.12)	Acc@5  95.31 ( 93.71)
Epoch: [4][380/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0820e+00 (1.3224e+00)	Acc@1  57.03 ( 52.22)	Acc@5  98.44 ( 93.77)
Epoch: [4][390/391]	Time  0.115 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0693e+00 (1.3198e+00)	Acc@1  60.00 ( 52.34)	Acc@5  97.50 ( 93.80)
## e[4] optimizer.zero_grad (sum) time: 0.5203266143798828
## e[4]       loss.backward (sum) time: 12.246272325515747
## e[4]      optimizer.step (sum) time: 25.859110593795776
## epoch[4] training(only) time: 63.54501700401306
# Switched to evaluate mode...
Test: [  0/100]	Time  0.180 ( 0.180)	Loss 1.5361e+00 (1.5361e+00)	Acc@1  54.00 ( 54.00)	Acc@5  96.00 ( 96.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 1.1621e+00 (1.4252e+00)	Acc@1  56.00 ( 54.82)	Acc@5  93.00 ( 93.73)
Test: [ 20/100]	Time  0.046 ( 0.053)	Loss 1.2695e+00 (1.5042e+00)	Acc@1  59.00 ( 54.76)	Acc@5  95.00 ( 93.48)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.3203e+00 (1.5008e+00)	Acc@1  62.00 ( 54.87)	Acc@5  96.00 ( 93.23)
Test: [ 40/100]	Time  0.047 ( 0.050)	Loss 1.3652e+00 (1.4953e+00)	Acc@1  61.00 ( 54.56)	Acc@5  95.00 ( 93.22)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.3623e+00 (1.4574e+00)	Acc@1  57.00 ( 54.96)	Acc@5  92.00 ( 93.47)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.3574e+00 (1.4546e+00)	Acc@1  57.00 ( 54.59)	Acc@5  94.00 ( 93.56)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 1.2773e+00 (1.4587e+00)	Acc@1  55.00 ( 54.39)	Acc@5  93.00 ( 93.59)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 1.5625e+00 (1.4568e+00)	Acc@1  56.00 ( 54.41)	Acc@5  92.00 ( 93.59)
Test: [ 90/100]	Time  0.056 ( 0.048)	Loss 1.0586e+00 (1.4550e+00)	Acc@1  60.00 ( 54.19)	Acc@5  98.00 ( 93.58)
 * Acc@1 54.000 Acc@5 93.590
### epoch[4] execution time: 68.4918372631073
EPOCH 5
# current learning rate: 0.1
# Switched to train mode...
Epoch: [5][  0/391]	Time  0.323 ( 0.323)	Data  0.142 ( 0.142)	Loss 1.2305e+00 (1.2305e+00)	Acc@1  53.91 ( 53.91)	Acc@5  97.66 ( 97.66)
Epoch: [5][ 10/391]	Time  0.160 ( 0.177)	Data  0.001 ( 0.014)	Loss 1.3057e+00 (1.2222e+00)	Acc@1  55.47 ( 56.53)	Acc@5  96.09 ( 94.74)
Epoch: [5][ 20/391]	Time  0.161 ( 0.169)	Data  0.001 ( 0.008)	Loss 1.1826e+00 (1.2046e+00)	Acc@1  57.03 ( 56.66)	Acc@5  96.88 ( 94.94)
Epoch: [5][ 30/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.006)	Loss 1.1025e+00 (1.1844e+00)	Acc@1  58.59 ( 57.08)	Acc@5  97.66 ( 95.21)
Epoch: [5][ 40/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.005)	Loss 1.1963e+00 (1.2023e+00)	Acc@1  59.38 ( 56.63)	Acc@5  90.62 ( 94.86)
Epoch: [5][ 50/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.1484e+00 (1.2060e+00)	Acc@1  60.94 ( 56.40)	Acc@5  93.75 ( 94.87)
Epoch: [5][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.2930e+00 (1.1932e+00)	Acc@1  57.03 ( 56.89)	Acc@5  93.75 ( 94.84)
Epoch: [5][ 70/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.1680e+00 (1.1987e+00)	Acc@1  59.38 ( 56.57)	Acc@5  96.09 ( 94.80)
Epoch: [5][ 80/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.4561e+00 (1.1925e+00)	Acc@1  51.56 ( 56.84)	Acc@5  92.19 ( 94.84)
Epoch: [5][ 90/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.2656e+00 (1.1970e+00)	Acc@1  53.12 ( 56.54)	Acc@5  92.97 ( 94.90)
Epoch: [5][100/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2734e+00 (1.1924e+00)	Acc@1  55.47 ( 56.73)	Acc@5  95.31 ( 94.90)
Epoch: [5][110/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0996e+00 (1.1902e+00)	Acc@1  60.16 ( 56.79)	Acc@5  96.88 ( 94.95)
Epoch: [5][120/391]	Time  0.165 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0312e+00 (1.1857e+00)	Acc@1  64.06 ( 56.88)	Acc@5  96.09 ( 95.03)
Epoch: [5][130/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.8584e-01 (1.1869e+00)	Acc@1  63.28 ( 56.83)	Acc@5  96.88 ( 95.06)
Epoch: [5][140/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0928e+00 (1.1823e+00)	Acc@1  62.50 ( 57.00)	Acc@5  96.88 ( 95.09)
Epoch: [5][150/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1836e+00 (1.1816e+00)	Acc@1  57.81 ( 57.08)	Acc@5  92.97 ( 95.07)
Epoch: [5][160/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0332e+00 (1.1790e+00)	Acc@1  62.50 ( 57.14)	Acc@5  96.09 ( 95.18)
Epoch: [5][170/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0156e+00 (1.1754e+00)	Acc@1  66.41 ( 57.40)	Acc@5  96.88 ( 95.22)
Epoch: [5][180/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1885e+00 (1.1725e+00)	Acc@1  53.91 ( 57.45)	Acc@5  94.53 ( 95.23)
Epoch: [5][190/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2646e+00 (1.1737e+00)	Acc@1  55.47 ( 57.44)	Acc@5  95.31 ( 95.22)
Epoch: [5][200/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0430e+00 (1.1697e+00)	Acc@1  64.06 ( 57.61)	Acc@5  96.09 ( 95.25)
Epoch: [5][210/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2461e+00 (1.1694e+00)	Acc@1  57.03 ( 57.67)	Acc@5  93.75 ( 95.23)
Epoch: [5][220/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2988e+00 (1.1698e+00)	Acc@1  58.59 ( 57.67)	Acc@5  92.19 ( 95.24)
Epoch: [5][230/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3535e+00 (1.1721e+00)	Acc@1  47.66 ( 57.60)	Acc@5  96.88 ( 95.27)
Epoch: [5][240/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2256e+00 (1.1700e+00)	Acc@1  59.38 ( 57.71)	Acc@5  96.88 ( 95.27)
Epoch: [5][250/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1777e+00 (1.1690e+00)	Acc@1  60.16 ( 57.78)	Acc@5  95.31 ( 95.30)
Epoch: [5][260/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1094e+00 (1.1672e+00)	Acc@1  57.03 ( 57.83)	Acc@5  94.53 ( 95.32)
Epoch: [5][270/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2891e+00 (1.1656e+00)	Acc@1  58.59 ( 57.86)	Acc@5  95.31 ( 95.37)
Epoch: [5][280/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1777e+00 (1.1640e+00)	Acc@1  56.25 ( 57.91)	Acc@5  93.75 ( 95.37)
Epoch: [5][290/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0967e+00 (1.1627e+00)	Acc@1  61.72 ( 57.89)	Acc@5  92.97 ( 95.40)
Epoch: [5][300/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0703e+00 (1.1626e+00)	Acc@1  64.06 ( 57.89)	Acc@5  96.09 ( 95.40)
Epoch: [5][310/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2100e+00 (1.1598e+00)	Acc@1  55.47 ( 58.01)	Acc@5  97.66 ( 95.44)
Epoch: [5][320/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.7607e-01 (1.1562e+00)	Acc@1  65.62 ( 58.12)	Acc@5  98.44 ( 95.47)
Epoch: [5][330/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1885e+00 (1.1545e+00)	Acc@1  53.12 ( 58.22)	Acc@5  96.09 ( 95.48)
Epoch: [5][340/391]	Time  0.172 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.7217e-01 (1.1533e+00)	Acc@1  63.28 ( 58.27)	Acc@5  96.09 ( 95.48)
Epoch: [5][350/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0596e+00 (1.1514e+00)	Acc@1  65.62 ( 58.37)	Acc@5  95.31 ( 95.48)
Epoch: [5][360/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.9268e-01 (1.1502e+00)	Acc@1  65.62 ( 58.44)	Acc@5  98.44 ( 95.48)
Epoch: [5][370/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0576e+00 (1.1486e+00)	Acc@1  56.25 ( 58.49)	Acc@5  96.88 ( 95.48)
Epoch: [5][380/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.3428e+00 (1.1482e+00)	Acc@1  52.34 ( 58.54)	Acc@5  89.06 ( 95.47)
Epoch: [5][390/391]	Time  0.126 ( 0.162)	Data  0.001 ( 0.001)	Loss 9.6680e-01 (1.1455e+00)	Acc@1  70.00 ( 58.63)	Acc@5  93.75 ( 95.50)
## e[5] optimizer.zero_grad (sum) time: 0.5212857723236084
## e[5]       loss.backward (sum) time: 12.241545915603638
## e[5]      optimizer.step (sum) time: 25.868861198425293
## epoch[5] training(only) time: 63.36958646774292
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 1.1436e+00 (1.1436e+00)	Acc@1  58.00 ( 58.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.048 ( 0.061)	Loss 9.4434e-01 (1.0649e+00)	Acc@1  65.00 ( 61.73)	Acc@5  98.00 ( 96.09)
Test: [ 20/100]	Time  0.064 ( 0.055)	Loss 1.2354e+00 (1.0554e+00)	Acc@1  58.00 ( 61.90)	Acc@5  93.00 ( 95.90)
Test: [ 30/100]	Time  0.047 ( 0.053)	Loss 9.3750e-01 (1.0584e+00)	Acc@1  68.00 ( 62.19)	Acc@5  97.00 ( 96.03)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.0205e+00 (1.0577e+00)	Acc@1  69.00 ( 62.12)	Acc@5  95.00 ( 95.88)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.1035e+00 (1.0509e+00)	Acc@1  60.00 ( 62.29)	Acc@5  96.00 ( 96.10)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.1348e+00 (1.0586e+00)	Acc@1  60.00 ( 61.85)	Acc@5  95.00 ( 96.16)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 9.7656e-01 (1.0639e+00)	Acc@1  59.00 ( 61.46)	Acc@5 100.00 ( 96.17)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.0498e+00 (1.0634e+00)	Acc@1  62.00 ( 61.58)	Acc@5  98.00 ( 96.23)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.0215e+00 (1.0721e+00)	Acc@1  65.00 ( 61.51)	Acc@5  95.00 ( 96.09)
 * Acc@1 61.660 Acc@5 96.050
### epoch[5] execution time: 68.34443640708923
EPOCH 6
# current learning rate: 0.1
# Switched to train mode...
Epoch: [6][  0/391]	Time  0.329 ( 0.329)	Data  0.153 ( 0.153)	Loss 1.0615e+00 (1.0615e+00)	Acc@1  64.06 ( 64.06)	Acc@5  92.97 ( 92.97)
Epoch: [6][ 10/391]	Time  0.160 ( 0.176)	Data  0.001 ( 0.015)	Loss 8.8379e-01 (9.8557e-01)	Acc@1  66.41 ( 64.63)	Acc@5  98.44 ( 96.38)
Epoch: [6][ 20/391]	Time  0.170 ( 0.169)	Data  0.001 ( 0.008)	Loss 9.8682e-01 (1.0140e+00)	Acc@1  67.19 ( 64.14)	Acc@5  96.09 ( 96.54)
Epoch: [6][ 30/391]	Time  0.161 ( 0.166)	Data  0.001 ( 0.006)	Loss 1.1289e+00 (1.0330e+00)	Acc@1  55.47 ( 62.80)	Acc@5  96.09 ( 96.57)
Epoch: [6][ 40/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.005)	Loss 1.0801e+00 (1.0400e+00)	Acc@1  57.03 ( 62.42)	Acc@5  95.31 ( 96.49)
Epoch: [6][ 50/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.2500e+00 (1.0485e+00)	Acc@1  54.69 ( 62.06)	Acc@5  93.75 ( 96.43)
Epoch: [6][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.1270e+00 (1.0516e+00)	Acc@1  57.81 ( 62.14)	Acc@5  97.66 ( 96.43)
Epoch: [6][ 70/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 9.1846e-01 (1.0478e+00)	Acc@1  67.97 ( 62.28)	Acc@5  96.88 ( 96.46)
Epoch: [6][ 80/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 9.1113e-01 (1.0411e+00)	Acc@1  68.75 ( 62.60)	Acc@5  96.88 ( 96.49)
Epoch: [6][ 90/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.1006e+00 (1.0414e+00)	Acc@1  58.59 ( 62.63)	Acc@5  97.66 ( 96.46)
Epoch: [6][100/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.003)	Loss 1.0654e+00 (1.0411e+00)	Acc@1  67.19 ( 62.76)	Acc@5  94.53 ( 96.50)
Epoch: [6][110/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.5703e-01 (1.0446e+00)	Acc@1  69.53 ( 62.60)	Acc@5  96.09 ( 96.52)
Epoch: [6][120/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.8486e-01 (1.0472e+00)	Acc@1  65.62 ( 62.62)	Acc@5  96.09 ( 96.38)
Epoch: [6][130/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1367e+00 (1.0461e+00)	Acc@1  56.25 ( 62.63)	Acc@5  96.88 ( 96.45)
Epoch: [6][140/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0127e+00 (1.0477e+00)	Acc@1  71.09 ( 62.64)	Acc@5  96.09 ( 96.45)
Epoch: [6][150/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.9121e-01 (1.0417e+00)	Acc@1  57.81 ( 62.84)	Acc@5  99.22 ( 96.53)
Epoch: [6][160/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1943e+00 (1.0413e+00)	Acc@1  59.38 ( 62.88)	Acc@5  95.31 ( 96.56)
Epoch: [6][170/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2744e+00 (1.0403e+00)	Acc@1  55.47 ( 62.99)	Acc@5  96.09 ( 96.54)
Epoch: [6][180/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0127e+00 (1.0359e+00)	Acc@1  64.84 ( 63.13)	Acc@5  98.44 ( 96.58)
Epoch: [6][190/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1885e+00 (1.0370e+00)	Acc@1  63.28 ( 63.14)	Acc@5  94.53 ( 96.56)
Epoch: [6][200/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0713e+00 (1.0368e+00)	Acc@1  57.81 ( 63.15)	Acc@5  96.88 ( 96.56)
Epoch: [6][210/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.1055e-01 (1.0327e+00)	Acc@1  69.53 ( 63.32)	Acc@5  98.44 ( 96.58)
Epoch: [6][220/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0693e+00 (1.0304e+00)	Acc@1  54.69 ( 63.35)	Acc@5  97.66 ( 96.63)
Epoch: [6][230/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.8682e-01 (1.0298e+00)	Acc@1  63.28 ( 63.43)	Acc@5  98.44 ( 96.65)
Epoch: [6][240/391]	Time  0.167 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.1113e-01 (1.0283e+00)	Acc@1  64.84 ( 63.46)	Acc@5  96.88 ( 96.66)
Epoch: [6][250/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.9902e-01 (1.0279e+00)	Acc@1  66.41 ( 63.49)	Acc@5  97.66 ( 96.65)
Epoch: [6][260/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.4629e-01 (1.0271e+00)	Acc@1  67.19 ( 63.62)	Acc@5  96.88 ( 96.65)
Epoch: [6][270/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.4678e-01 (1.0237e+00)	Acc@1  66.41 ( 63.72)	Acc@5  94.53 ( 96.66)
Epoch: [6][280/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1445e+00 (1.0236e+00)	Acc@1  59.38 ( 63.71)	Acc@5  96.09 ( 96.64)
Epoch: [6][290/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.5020e-01 (1.0238e+00)	Acc@1  67.97 ( 63.70)	Acc@5  98.44 ( 96.65)
Epoch: [6][300/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.3213e-01 (1.0213e+00)	Acc@1  64.84 ( 63.78)	Acc@5  96.88 ( 96.66)
Epoch: [6][310/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.9902e-01 (1.0192e+00)	Acc@1  64.06 ( 63.86)	Acc@5  96.88 ( 96.69)
Epoch: [6][320/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1143e+00 (1.0186e+00)	Acc@1  60.16 ( 63.90)	Acc@5  96.88 ( 96.70)
Epoch: [6][330/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0293e+00 (1.0180e+00)	Acc@1  70.31 ( 63.93)	Acc@5  96.88 ( 96.68)
Epoch: [6][340/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.4766e-01 (1.0161e+00)	Acc@1  67.97 ( 64.02)	Acc@5  97.66 ( 96.69)
Epoch: [6][350/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0039e+00 (1.0141e+00)	Acc@1  64.06 ( 64.09)	Acc@5  96.88 ( 96.70)
Epoch: [6][360/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.3848e-01 (1.0118e+00)	Acc@1  67.97 ( 64.21)	Acc@5  93.75 ( 96.69)
Epoch: [6][370/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0215e+00 (1.0100e+00)	Acc@1  60.94 ( 64.25)	Acc@5  97.66 ( 96.71)
Epoch: [6][380/391]	Time  0.167 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.4141e-01 (1.0079e+00)	Acc@1  68.75 ( 64.32)	Acc@5  97.66 ( 96.74)
Epoch: [6][390/391]	Time  0.113 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.9160e-01 (1.0067e+00)	Acc@1  70.00 ( 64.38)	Acc@5  96.25 ( 96.72)
## e[6] optimizer.zero_grad (sum) time: 0.5145790576934814
## e[6]       loss.backward (sum) time: 12.298566102981567
## e[6]      optimizer.step (sum) time: 25.82528567314148
## epoch[6] training(only) time: 63.35304546356201
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 7.4902e-01 (7.4902e-01)	Acc@1  73.00 ( 73.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.048 ( 0.061)	Loss 6.9141e-01 (8.2271e-01)	Acc@1  75.00 ( 70.45)	Acc@5 100.00 ( 97.91)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 9.3018e-01 (8.5856e-01)	Acc@1  65.00 ( 68.81)	Acc@5  98.00 ( 97.62)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 8.8330e-01 (8.6167e-01)	Acc@1  66.00 ( 68.65)	Acc@5  99.00 ( 97.58)
Test: [ 40/100]	Time  0.047 ( 0.052)	Loss 9.1650e-01 (8.6263e-01)	Acc@1  68.00 ( 68.68)	Acc@5  96.00 ( 97.37)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 8.9111e-01 (8.5735e-01)	Acc@1  69.00 ( 69.29)	Acc@5  97.00 ( 97.37)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 8.2275e-01 (8.6732e-01)	Acc@1  69.00 ( 68.95)	Acc@5  97.00 ( 97.39)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 8.8818e-01 (8.7162e-01)	Acc@1  72.00 ( 68.83)	Acc@5  95.00 ( 97.46)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 8.0762e-01 (8.6934e-01)	Acc@1  75.00 ( 69.01)	Acc@5  97.00 ( 97.52)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 7.5977e-01 (8.7347e-01)	Acc@1  71.00 ( 68.86)	Acc@5  98.00 ( 97.51)
 * Acc@1 68.860 Acc@5 97.520
### epoch[6] execution time: 68.34224104881287
EPOCH 7
# current learning rate: 0.1
# Switched to train mode...
Epoch: [7][  0/391]	Time  0.318 ( 0.318)	Data  0.145 ( 0.145)	Loss 9.1846e-01 (9.1846e-01)	Acc@1  69.53 ( 69.53)	Acc@5 100.00 (100.00)
Epoch: [7][ 10/391]	Time  0.159 ( 0.175)	Data  0.001 ( 0.014)	Loss 1.1650e+00 (9.1988e-01)	Acc@1  57.81 ( 67.26)	Acc@5  93.75 ( 96.73)
Epoch: [7][ 20/391]	Time  0.161 ( 0.169)	Data  0.001 ( 0.008)	Loss 1.2090e+00 (9.3534e-01)	Acc@1  61.72 ( 67.26)	Acc@5  93.75 ( 96.73)
Epoch: [7][ 30/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.006)	Loss 8.5742e-01 (9.1746e-01)	Acc@1  67.97 ( 67.41)	Acc@5  96.88 ( 96.90)
Epoch: [7][ 40/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.005)	Loss 7.7441e-01 (9.0091e-01)	Acc@1  71.09 ( 68.10)	Acc@5  97.66 ( 96.86)
Epoch: [7][ 50/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.004)	Loss 8.9648e-01 (9.1171e-01)	Acc@1  69.53 ( 67.49)	Acc@5  96.09 ( 96.89)
Epoch: [7][ 60/391]	Time  0.164 ( 0.164)	Data  0.001 ( 0.003)	Loss 8.5596e-01 (9.0809e-01)	Acc@1  67.19 ( 67.78)	Acc@5  98.44 ( 96.98)
Epoch: [7][ 70/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.2441e+00 (9.1930e-01)	Acc@1  57.03 ( 67.37)	Acc@5  94.53 ( 96.99)
Epoch: [7][ 80/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 9.6484e-01 (9.2440e-01)	Acc@1  67.19 ( 67.10)	Acc@5  96.09 ( 97.04)
Epoch: [7][ 90/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 8.5596e-01 (9.2178e-01)	Acc@1  71.88 ( 67.31)	Acc@5  96.88 ( 97.04)
Epoch: [7][100/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 8.9404e-01 (9.2009e-01)	Acc@1  64.84 ( 67.19)	Acc@5  97.66 ( 97.11)
Epoch: [7][110/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.7656e-01 (9.1407e-01)	Acc@1  67.97 ( 67.39)	Acc@5  97.66 ( 97.21)
Epoch: [7][120/391]	Time  0.169 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.5508e-01 (9.1259e-01)	Acc@1  67.97 ( 67.50)	Acc@5  94.53 ( 97.24)
Epoch: [7][130/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.7002e-01 (9.0897e-01)	Acc@1  67.97 ( 67.61)	Acc@5  99.22 ( 97.30)
Epoch: [7][140/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.9854e-01 (9.0650e-01)	Acc@1  67.19 ( 67.72)	Acc@5  96.88 ( 97.32)
Epoch: [7][150/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.0723e-01 (9.0373e-01)	Acc@1  67.19 ( 67.89)	Acc@5  97.66 ( 97.37)
Epoch: [7][160/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.9746e-01 (9.0269e-01)	Acc@1  67.97 ( 67.93)	Acc@5  98.44 ( 97.37)
Epoch: [7][170/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.2139e-01 (9.0086e-01)	Acc@1  74.22 ( 68.01)	Acc@5  96.88 ( 97.35)
Epoch: [7][180/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.7734e-01 (8.9901e-01)	Acc@1  68.75 ( 68.05)	Acc@5  99.22 ( 97.36)
Epoch: [7][190/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.9453e-01 (8.9601e-01)	Acc@1  70.31 ( 68.17)	Acc@5  96.88 ( 97.39)
Epoch: [7][200/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.6914e-01 (8.9596e-01)	Acc@1  69.53 ( 68.26)	Acc@5  96.09 ( 97.38)
Epoch: [7][210/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.5459e-01 (8.9592e-01)	Acc@1  70.31 ( 68.32)	Acc@5  96.88 ( 97.39)
Epoch: [7][220/391]	Time  0.165 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.3545e-01 (8.9426e-01)	Acc@1  68.75 ( 68.35)	Acc@5  98.44 ( 97.46)
Epoch: [7][230/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.4502e-01 (8.9357e-01)	Acc@1  78.12 ( 68.38)	Acc@5  96.88 ( 97.45)
Epoch: [7][240/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.2627e-01 (8.9373e-01)	Acc@1  69.53 ( 68.40)	Acc@5  96.88 ( 97.46)
Epoch: [7][250/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.4727e-01 (8.9485e-01)	Acc@1  64.84 ( 68.37)	Acc@5  96.88 ( 97.44)
Epoch: [7][260/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.5840e-01 (8.9653e-01)	Acc@1  67.97 ( 68.30)	Acc@5  95.31 ( 97.42)
Epoch: [7][270/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.5195e-01 (8.9615e-01)	Acc@1  72.66 ( 68.31)	Acc@5  96.09 ( 97.41)
Epoch: [7][280/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.4424e-01 (8.9534e-01)	Acc@1  74.22 ( 68.38)	Acc@5  97.66 ( 97.45)
Epoch: [7][290/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.3691e-01 (8.9376e-01)	Acc@1  67.97 ( 68.41)	Acc@5  97.66 ( 97.44)
Epoch: [7][300/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.5840e-01 (8.9266e-01)	Acc@1  69.53 ( 68.47)	Acc@5  96.88 ( 97.44)
Epoch: [7][310/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.1006e-01 (8.9105e-01)	Acc@1  73.44 ( 68.52)	Acc@5  99.22 ( 97.46)
Epoch: [7][320/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.6289e-01 (8.8891e-01)	Acc@1  69.53 ( 68.61)	Acc@5  95.31 ( 97.46)
Epoch: [7][330/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0205e+00 (8.8748e-01)	Acc@1  64.06 ( 68.66)	Acc@5  94.53 ( 97.46)
Epoch: [7][340/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.6963e-01 (8.8616e-01)	Acc@1  71.88 ( 68.74)	Acc@5  96.09 ( 97.45)
Epoch: [7][350/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.1113e-01 (8.8690e-01)	Acc@1  67.97 ( 68.73)	Acc@5  97.66 ( 97.43)
Epoch: [7][360/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.5215e-01 (8.8640e-01)	Acc@1  67.19 ( 68.77)	Acc@5  98.44 ( 97.44)
Epoch: [7][370/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.2324e-01 (8.8506e-01)	Acc@1  72.66 ( 68.82)	Acc@5  97.66 ( 97.45)
Epoch: [7][380/391]	Time  0.179 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.1777e-01 (8.8313e-01)	Acc@1  76.56 ( 68.88)	Acc@5  96.88 ( 97.46)
Epoch: [7][390/391]	Time  0.115 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.9736e-01 (8.8155e-01)	Acc@1  72.50 ( 68.95)	Acc@5  97.50 ( 97.46)
## e[7] optimizer.zero_grad (sum) time: 0.5242950916290283
## e[7]       loss.backward (sum) time: 12.304092407226562
## e[7]      optimizer.step (sum) time: 25.862804889678955
## epoch[7] training(only) time: 63.38201379776001
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 9.7119e-01 (9.7119e-01)	Acc@1  70.00 ( 70.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.048 ( 0.062)	Loss 6.1572e-01 (9.0354e-01)	Acc@1  76.00 ( 68.00)	Acc@5  99.00 ( 97.45)
Test: [ 20/100]	Time  0.048 ( 0.055)	Loss 1.2764e+00 (9.2613e-01)	Acc@1  61.00 ( 67.43)	Acc@5  98.00 ( 97.76)
Test: [ 30/100]	Time  0.060 ( 0.053)	Loss 9.9023e-01 (9.6116e-01)	Acc@1  65.00 ( 67.03)	Acc@5  98.00 ( 97.32)
Test: [ 40/100]	Time  0.048 ( 0.052)	Loss 1.0117e+00 (9.5585e-01)	Acc@1  67.00 ( 67.51)	Acc@5  94.00 ( 97.20)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 9.6191e-01 (9.5341e-01)	Acc@1  71.00 ( 67.75)	Acc@5  96.00 ( 97.22)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 1.1123e+00 (9.5967e-01)	Acc@1  64.00 ( 67.23)	Acc@5  97.00 ( 97.21)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 8.5645e-01 (9.6464e-01)	Acc@1  70.00 ( 67.44)	Acc@5  97.00 ( 97.25)
Test: [ 80/100]	Time  0.047 ( 0.050)	Loss 8.4619e-01 (9.6354e-01)	Acc@1  72.00 ( 67.43)	Acc@5  97.00 ( 97.32)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 8.1885e-01 (9.6652e-01)	Acc@1  70.00 ( 67.20)	Acc@5  99.00 ( 97.35)
 * Acc@1 67.410 Acc@5 97.350
### epoch[7] execution time: 68.39264369010925
EPOCH 8
# current learning rate: 0.1
# Switched to train mode...
Epoch: [8][  0/391]	Time  0.314 ( 0.314)	Data  0.139 ( 0.139)	Loss 7.8467e-01 (7.8467e-01)	Acc@1  75.78 ( 75.78)	Acc@5  99.22 ( 99.22)
Epoch: [8][ 10/391]	Time  0.160 ( 0.174)	Data  0.001 ( 0.014)	Loss 7.8662e-01 (7.7219e-01)	Acc@1  73.44 ( 73.93)	Acc@5  98.44 ( 98.30)
Epoch: [8][ 20/391]	Time  0.161 ( 0.168)	Data  0.001 ( 0.008)	Loss 7.5977e-01 (8.0392e-01)	Acc@1  76.56 ( 72.54)	Acc@5  99.22 ( 97.95)
Epoch: [8][ 30/391]	Time  0.161 ( 0.166)	Data  0.001 ( 0.006)	Loss 6.8555e-01 (8.1360e-01)	Acc@1  78.91 ( 72.10)	Acc@5  98.44 ( 97.83)
Epoch: [8][ 40/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 6.7236e-01 (8.1718e-01)	Acc@1  77.34 ( 71.80)	Acc@5  99.22 ( 97.96)
Epoch: [8][ 50/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.004)	Loss 7.0996e-01 (8.0879e-01)	Acc@1  75.00 ( 72.00)	Acc@5  99.22 ( 97.98)
Epoch: [8][ 60/391]	Time  0.171 ( 0.164)	Data  0.001 ( 0.003)	Loss 9.1748e-01 (8.0843e-01)	Acc@1  67.97 ( 71.55)	Acc@5  96.88 ( 97.93)
Epoch: [8][ 70/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 6.8164e-01 (8.0642e-01)	Acc@1  71.09 ( 71.54)	Acc@5 100.00 ( 98.09)
Epoch: [8][ 80/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 7.4561e-01 (8.0012e-01)	Acc@1  72.66 ( 71.84)	Acc@5 100.00 ( 98.12)
Epoch: [8][ 90/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 6.5723e-01 (7.9158e-01)	Acc@1  75.00 ( 72.18)	Acc@5  97.66 ( 98.15)
Epoch: [8][100/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 8.4375e-01 (7.9403e-01)	Acc@1  71.88 ( 72.09)	Acc@5  98.44 ( 98.11)
Epoch: [8][110/391]	Time  0.172 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.2607e-01 (7.9354e-01)	Acc@1  77.34 ( 72.25)	Acc@5  97.66 ( 98.08)
Epoch: [8][120/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.0166e-01 (7.9997e-01)	Acc@1  75.78 ( 72.08)	Acc@5 100.00 ( 98.06)
Epoch: [8][130/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.6562e-01 (7.9743e-01)	Acc@1  73.44 ( 72.12)	Acc@5  97.66 ( 98.06)
Epoch: [8][140/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.7002e-01 (7.9720e-01)	Acc@1  69.53 ( 72.06)	Acc@5 100.00 ( 98.03)
Epoch: [8][150/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.1924e-01 (7.9779e-01)	Acc@1  74.22 ( 72.03)	Acc@5  98.44 ( 98.02)
Epoch: [8][160/391]	Time  0.169 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.3740e-01 (7.9747e-01)	Acc@1  68.75 ( 72.00)	Acc@5  99.22 ( 98.04)
Epoch: [8][170/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.9150e-01 (7.9610e-01)	Acc@1  71.09 ( 72.07)	Acc@5  99.22 ( 98.06)
Epoch: [8][180/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.6904e-01 (7.9545e-01)	Acc@1  72.66 ( 72.03)	Acc@5  95.31 ( 98.09)
Epoch: [8][190/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.8311e-01 (7.9201e-01)	Acc@1  75.78 ( 72.17)	Acc@5  97.66 ( 98.08)
Epoch: [8][200/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.1201e-01 (7.9108e-01)	Acc@1  75.78 ( 72.22)	Acc@5  95.31 ( 98.06)
Epoch: [8][210/391]	Time  0.172 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.9258e-01 (7.9138e-01)	Acc@1  65.62 ( 72.18)	Acc@5  99.22 ( 98.05)
Epoch: [8][220/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.4121e-01 (7.9096e-01)	Acc@1  76.56 ( 72.17)	Acc@5  97.66 ( 98.07)
Epoch: [8][230/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.8311e-01 (7.8908e-01)	Acc@1  76.56 ( 72.27)	Acc@5  98.44 ( 98.06)
Epoch: [8][240/391]	Time  0.176 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.8770e-01 (7.8815e-01)	Acc@1  67.97 ( 72.33)	Acc@5  96.09 ( 98.05)
Epoch: [8][250/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.4355e-01 (7.8925e-01)	Acc@1  75.00 ( 72.29)	Acc@5  98.44 ( 98.04)
Epoch: [8][260/391]	Time  0.172 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.6797e-01 (7.8734e-01)	Acc@1  78.12 ( 72.35)	Acc@5  97.66 ( 98.05)
Epoch: [8][270/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.4268e-01 (7.8746e-01)	Acc@1  74.22 ( 72.40)	Acc@5  98.44 ( 98.06)
Epoch: [8][280/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.2559e-01 (7.8678e-01)	Acc@1  71.88 ( 72.45)	Acc@5  98.44 ( 98.07)
Epoch: [8][290/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.1787e-01 (7.8636e-01)	Acc@1  71.88 ( 72.47)	Acc@5  98.44 ( 98.07)
Epoch: [8][300/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.6406e-01 (7.8623e-01)	Acc@1  76.56 ( 72.48)	Acc@5 100.00 ( 98.09)
Epoch: [8][310/391]	Time  0.170 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.9688e-01 (7.8736e-01)	Acc@1  73.44 ( 72.49)	Acc@5  96.09 ( 98.05)
Epoch: [8][320/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.3828e-01 (7.8632e-01)	Acc@1  72.66 ( 72.54)	Acc@5  99.22 ( 98.04)
Epoch: [8][330/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.3984e-01 (7.8561e-01)	Acc@1  71.09 ( 72.58)	Acc@5  97.66 ( 98.06)
Epoch: [8][340/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.7930e-01 (7.8508e-01)	Acc@1  69.53 ( 72.59)	Acc@5  98.44 ( 98.07)
Epoch: [8][350/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.0107e-01 (7.8407e-01)	Acc@1  80.47 ( 72.67)	Acc@5  96.88 ( 98.06)
Epoch: [8][360/391]	Time  0.172 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.9492e-01 (7.8270e-01)	Acc@1  67.19 ( 72.72)	Acc@5  99.22 ( 98.08)
Epoch: [8][370/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 7.3389e-01 (7.8162e-01)	Acc@1  74.22 ( 72.80)	Acc@5  97.66 ( 98.07)
Epoch: [8][380/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 6.3135e-01 (7.8062e-01)	Acc@1  78.91 ( 72.81)	Acc@5 100.00 ( 98.08)
Epoch: [8][390/391]	Time  0.118 ( 0.162)	Data  0.001 ( 0.001)	Loss 7.5000e-01 (7.8161e-01)	Acc@1  73.75 ( 72.78)	Acc@5  98.75 ( 98.08)
## e[8] optimizer.zero_grad (sum) time: 0.5271306037902832
## e[8]       loss.backward (sum) time: 12.333653688430786
## e[8]      optimizer.step (sum) time: 25.81037712097168
## epoch[8] training(only) time: 63.42624616622925
# Switched to evaluate mode...
Test: [  0/100]	Time  0.178 ( 0.178)	Loss 5.9814e-01 (5.9814e-01)	Acc@1  79.00 ( 79.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.048 ( 0.061)	Loss 6.5967e-01 (7.0881e-01)	Acc@1  80.00 ( 74.45)	Acc@5  97.00 ( 98.82)
Test: [ 20/100]	Time  0.048 ( 0.055)	Loss 9.4727e-01 (7.2893e-01)	Acc@1  70.00 ( 73.71)	Acc@5  98.00 ( 98.24)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 7.3047e-01 (7.4772e-01)	Acc@1  77.00 ( 74.03)	Acc@5  99.00 ( 98.10)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 6.6455e-01 (7.4177e-01)	Acc@1  80.00 ( 73.93)	Acc@5  96.00 ( 98.07)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 5.9131e-01 (7.3696e-01)	Acc@1  81.00 ( 74.33)	Acc@5  99.00 ( 98.16)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 7.3389e-01 (7.4103e-01)	Acc@1  69.00 ( 74.07)	Acc@5 100.00 ( 98.21)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 7.5879e-01 (7.4609e-01)	Acc@1  75.00 ( 74.13)	Acc@5  98.00 ( 98.24)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 4.8438e-01 (7.4043e-01)	Acc@1  86.00 ( 74.44)	Acc@5  98.00 ( 98.26)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 6.1035e-01 (7.3990e-01)	Acc@1  74.00 ( 74.36)	Acc@5 100.00 ( 98.23)
 * Acc@1 74.400 Acc@5 98.190
### epoch[8] execution time: 68.39479422569275
EPOCH 9
# current learning rate: 0.1
# Switched to train mode...
Epoch: [9][  0/391]	Time  0.319 ( 0.319)	Data  0.143 ( 0.143)	Loss 7.0703e-01 (7.0703e-01)	Acc@1  74.22 ( 74.22)	Acc@5  98.44 ( 98.44)
Epoch: [9][ 10/391]	Time  0.160 ( 0.176)	Data  0.001 ( 0.014)	Loss 6.0547e-01 (7.4867e-01)	Acc@1  79.69 ( 72.87)	Acc@5  97.66 ( 98.51)
Epoch: [9][ 20/391]	Time  0.160 ( 0.169)	Data  0.001 ( 0.008)	Loss 7.0557e-01 (7.5274e-01)	Acc@1  72.66 ( 73.66)	Acc@5  99.22 ( 98.18)
Epoch: [9][ 30/391]	Time  0.161 ( 0.166)	Data  0.001 ( 0.006)	Loss 8.0029e-01 (7.4561e-01)	Acc@1  68.75 ( 73.82)	Acc@5 100.00 ( 98.24)
Epoch: [9][ 40/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.005)	Loss 7.7393e-01 (7.3599e-01)	Acc@1  71.09 ( 74.49)	Acc@5  99.22 ( 98.30)
Epoch: [9][ 50/391]	Time  0.159 ( 0.164)	Data  0.001 ( 0.004)	Loss 6.4453e-01 (7.2426e-01)	Acc@1  74.22 ( 74.91)	Acc@5 100.00 ( 98.38)
Epoch: [9][ 60/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 7.3535e-01 (7.3092e-01)	Acc@1  75.00 ( 74.60)	Acc@5  99.22 ( 98.37)
Epoch: [9][ 70/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 8.3984e-01 (7.3160e-01)	Acc@1  75.78 ( 74.71)	Acc@5  96.88 ( 98.32)
Epoch: [9][ 80/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 6.9727e-01 (7.2223e-01)	Acc@1  73.44 ( 75.05)	Acc@5  98.44 ( 98.43)
Epoch: [9][ 90/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 7.4854e-01 (7.2436e-01)	Acc@1  75.00 ( 74.97)	Acc@5  96.88 ( 98.38)
Epoch: [9][100/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 6.6602e-01 (7.2810e-01)	Acc@1  75.78 ( 74.78)	Acc@5  98.44 ( 98.38)
Epoch: [9][110/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.4014e-01 (7.2300e-01)	Acc@1  79.69 ( 74.93)	Acc@5  98.44 ( 98.40)
Epoch: [9][120/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.5732e-01 (7.1928e-01)	Acc@1  72.66 ( 74.90)	Acc@5  98.44 ( 98.44)
Epoch: [9][130/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.7324e-01 (7.1763e-01)	Acc@1  83.59 ( 75.00)	Acc@5  99.22 ( 98.48)
Epoch: [9][140/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.3525e-01 (7.1511e-01)	Acc@1  77.34 ( 75.16)	Acc@5  99.22 ( 98.48)
Epoch: [9][150/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.8447e-01 (7.1707e-01)	Acc@1  79.69 ( 75.11)	Acc@5  99.22 ( 98.45)
Epoch: [9][160/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.6357e-01 (7.1611e-01)	Acc@1  78.12 ( 75.17)	Acc@5  97.66 ( 98.47)
Epoch: [9][170/391]	Time  0.167 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.8184e-01 (7.1936e-01)	Acc@1  69.53 ( 75.04)	Acc@5  98.44 ( 98.46)
Epoch: [9][180/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.6318e-01 (7.1975e-01)	Acc@1  74.22 ( 75.01)	Acc@5  96.88 ( 98.45)
Epoch: [9][190/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.6211e-01 (7.2018e-01)	Acc@1  75.00 ( 74.97)	Acc@5  98.44 ( 98.47)
Epoch: [9][200/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.6455e-01 (7.2141e-01)	Acc@1  73.44 ( 74.90)	Acc@5 100.00 ( 98.47)
Epoch: [9][210/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.7041e-01 (7.1863e-01)	Acc@1  78.12 ( 74.97)	Acc@5  96.88 ( 98.47)
Epoch: [9][220/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.2119e-01 (7.2015e-01)	Acc@1  74.22 ( 74.88)	Acc@5  98.44 ( 98.47)
Epoch: [9][230/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.8945e-01 (7.1812e-01)	Acc@1  79.69 ( 74.93)	Acc@5  97.66 ( 98.44)
Epoch: [9][240/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.2744e-01 (7.1614e-01)	Acc@1  81.25 ( 75.10)	Acc@5  98.44 ( 98.44)
Epoch: [9][250/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.8848e-01 (7.1707e-01)	Acc@1  75.78 ( 75.07)	Acc@5  97.66 ( 98.43)
Epoch: [9][260/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.1729e-01 (7.1524e-01)	Acc@1  71.09 ( 75.09)	Acc@5  98.44 ( 98.45)
Epoch: [9][270/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.4717e-01 (7.1442e-01)	Acc@1  71.09 ( 75.14)	Acc@5  96.09 ( 98.46)
Epoch: [9][280/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.8691e-01 (7.1205e-01)	Acc@1  79.69 ( 75.23)	Acc@5  97.66 ( 98.46)
Epoch: [9][290/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.2607e-01 (7.1248e-01)	Acc@1  71.88 ( 75.17)	Acc@5  97.66 ( 98.44)
Epoch: [9][300/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.0908e-01 (7.1328e-01)	Acc@1  73.44 ( 75.13)	Acc@5  96.09 ( 98.43)
Epoch: [9][310/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.4365e-01 (7.1417e-01)	Acc@1  71.09 ( 75.10)	Acc@5  99.22 ( 98.42)
Epoch: [9][320/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.8555e-01 (7.1356e-01)	Acc@1  78.12 ( 75.12)	Acc@5  97.66 ( 98.41)
Epoch: [9][330/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.3340e-01 (7.1464e-01)	Acc@1  76.56 ( 75.08)	Acc@5  98.44 ( 98.41)
Epoch: [9][340/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.5635e-01 (7.1591e-01)	Acc@1  69.53 ( 75.01)	Acc@5  98.44 ( 98.39)
Epoch: [9][350/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.1543e-01 (7.1557e-01)	Acc@1  71.09 ( 75.01)	Acc@5  98.44 ( 98.39)
Epoch: [9][360/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.7812e-01 (7.1341e-01)	Acc@1  78.12 ( 75.10)	Acc@5  99.22 ( 98.40)
Epoch: [9][370/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.6943e-01 (7.1287e-01)	Acc@1  75.00 ( 75.11)	Acc@5  98.44 ( 98.40)
Epoch: [9][380/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.4238e-01 (7.1262e-01)	Acc@1  69.53 ( 75.11)	Acc@5  96.09 ( 98.40)
Epoch: [9][390/391]	Time  0.118 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.7344e-01 (7.1486e-01)	Acc@1  75.00 ( 75.05)	Acc@5  98.75 ( 98.39)
## e[9] optimizer.zero_grad (sum) time: 0.5292270183563232
## e[9]       loss.backward (sum) time: 12.32592511177063
## e[9]      optimizer.step (sum) time: 25.82200598716736
## epoch[9] training(only) time: 63.493648290634155
# Switched to evaluate mode...
Test: [  0/100]	Time  0.182 ( 0.182)	Loss 5.9375e-01 (5.9375e-01)	Acc@1  82.00 ( 82.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.048 ( 0.060)	Loss 6.0303e-01 (6.7512e-01)	Acc@1  82.00 ( 77.45)	Acc@5  99.00 ( 98.91)
Test: [ 20/100]	Time  0.048 ( 0.055)	Loss 6.6162e-01 (6.7625e-01)	Acc@1  77.00 ( 76.57)	Acc@5 100.00 ( 98.76)
Test: [ 30/100]	Time  0.047 ( 0.053)	Loss 6.6846e-01 (6.9462e-01)	Acc@1  75.00 ( 76.06)	Acc@5  96.00 ( 98.48)
Test: [ 40/100]	Time  0.049 ( 0.052)	Loss 7.6367e-01 (6.8823e-01)	Acc@1  77.00 ( 76.15)	Acc@5  96.00 ( 98.27)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 6.0010e-01 (6.8066e-01)	Acc@1  79.00 ( 76.35)	Acc@5  99.00 ( 98.35)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 6.5918e-01 (6.8133e-01)	Acc@1  78.00 ( 76.30)	Acc@5  99.00 ( 98.43)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 6.8115e-01 (6.8350e-01)	Acc@1  80.00 ( 76.35)	Acc@5  98.00 ( 98.38)
Test: [ 80/100]	Time  0.058 ( 0.049)	Loss 5.6152e-01 (6.8054e-01)	Acc@1  82.00 ( 76.46)	Acc@5  99.00 ( 98.41)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 6.8604e-01 (6.8129e-01)	Acc@1  73.00 ( 76.41)	Acc@5 100.00 ( 98.44)
 * Acc@1 76.370 Acc@5 98.460
### epoch[9] execution time: 68.47583389282227
EPOCH 10
# current learning rate: 0.1
# Switched to train mode...
Epoch: [10][  0/391]	Time  0.326 ( 0.326)	Data  0.156 ( 0.156)	Loss 7.2119e-01 (7.2119e-01)	Acc@1  74.22 ( 74.22)	Acc@5 100.00 (100.00)
Epoch: [10][ 10/391]	Time  0.160 ( 0.177)	Data  0.001 ( 0.015)	Loss 5.5957e-01 (6.3980e-01)	Acc@1  78.91 ( 77.13)	Acc@5 100.00 ( 99.22)
Epoch: [10][ 20/391]	Time  0.161 ( 0.169)	Data  0.001 ( 0.008)	Loss 5.1172e-01 (6.3245e-01)	Acc@1  84.38 ( 77.90)	Acc@5 100.00 ( 99.26)
Epoch: [10][ 30/391]	Time  0.160 ( 0.167)	Data  0.001 ( 0.006)	Loss 5.3418e-01 (6.3498e-01)	Acc@1  82.03 ( 77.77)	Acc@5  97.66 ( 99.17)
Epoch: [10][ 40/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.005)	Loss 6.4795e-01 (6.4740e-01)	Acc@1  75.00 ( 77.36)	Acc@5  99.22 ( 99.01)
Epoch: [10][ 50/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.004)	Loss 5.8252e-01 (6.4604e-01)	Acc@1  78.12 ( 77.36)	Acc@5 100.00 ( 98.91)
Epoch: [10][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 6.0352e-01 (6.4572e-01)	Acc@1  81.25 ( 77.41)	Acc@5  98.44 ( 98.89)
Epoch: [10][ 70/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 6.1572e-01 (6.4312e-01)	Acc@1  78.91 ( 77.56)	Acc@5  99.22 ( 98.84)
Epoch: [10][ 80/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 6.5283e-01 (6.4511e-01)	Acc@1  80.47 ( 77.44)	Acc@5  97.66 ( 98.82)
Epoch: [10][ 90/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 6.4453e-01 (6.3950e-01)	Acc@1  78.12 ( 77.70)	Acc@5 100.00 ( 98.82)
Epoch: [10][100/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.003)	Loss 7.4023e-01 (6.4092e-01)	Acc@1  71.88 ( 77.76)	Acc@5  96.88 ( 98.82)
Epoch: [10][110/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 5.7178e-01 (6.4176e-01)	Acc@1  81.25 ( 77.68)	Acc@5  97.66 ( 98.84)
Epoch: [10][120/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.0703e-01 (6.4562e-01)	Acc@1  74.22 ( 77.54)	Acc@5  99.22 ( 98.81)
Epoch: [10][130/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.6729e-01 (6.5255e-01)	Acc@1  70.31 ( 77.37)	Acc@5  95.31 ( 98.75)
Epoch: [10][140/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.3926e-01 (6.5405e-01)	Acc@1  76.56 ( 77.39)	Acc@5  99.22 ( 98.77)
Epoch: [10][150/391]	Time  0.172 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.2988e-01 (6.5732e-01)	Acc@1  77.34 ( 77.21)	Acc@5  98.44 ( 98.76)
Epoch: [10][160/391]	Time  0.172 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.1562e-01 (6.5678e-01)	Acc@1  79.69 ( 77.21)	Acc@5 100.00 ( 98.75)
Epoch: [10][170/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.8203e-01 (6.5455e-01)	Acc@1  80.47 ( 77.30)	Acc@5  99.22 ( 98.74)
Epoch: [10][180/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.2100e-01 (6.5683e-01)	Acc@1  82.81 ( 77.23)	Acc@5  98.44 ( 98.71)
Epoch: [10][190/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.8701e-01 (6.5871e-01)	Acc@1  77.34 ( 77.16)	Acc@5  97.66 ( 98.71)
Epoch: [10][200/391]	Time  0.172 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.0791e-01 (6.5796e-01)	Acc@1  78.12 ( 77.18)	Acc@5 100.00 ( 98.73)
Epoch: [10][210/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.3760e-01 (6.5508e-01)	Acc@1  82.81 ( 77.31)	Acc@5  99.22 ( 98.73)
Epoch: [10][220/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.9141e-01 (6.5712e-01)	Acc@1  73.44 ( 77.28)	Acc@5  97.66 ( 98.70)
Epoch: [10][230/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.3906e-01 (6.5703e-01)	Acc@1  86.72 ( 77.32)	Acc@5  98.44 ( 98.68)
Epoch: [10][240/391]	Time  0.165 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.0791e-01 (6.5665e-01)	Acc@1  77.34 ( 77.37)	Acc@5  97.66 ( 98.66)
Epoch: [10][250/391]	Time  0.173 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.9668e-01 (6.5569e-01)	Acc@1  78.91 ( 77.36)	Acc@5  99.22 ( 98.67)
Epoch: [10][260/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.1719e-01 (6.5647e-01)	Acc@1  77.34 ( 77.33)	Acc@5  99.22 ( 98.67)
Epoch: [10][270/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.2119e-01 (6.5565e-01)	Acc@1  75.78 ( 77.32)	Acc@5  97.66 ( 98.66)
Epoch: [10][280/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.7617e-01 (6.5418e-01)	Acc@1  78.91 ( 77.41)	Acc@5  98.44 ( 98.65)
Epoch: [10][290/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.3838e-01 (6.5508e-01)	Acc@1  71.09 ( 77.38)	Acc@5  99.22 ( 98.65)
Epoch: [10][300/391]	Time  0.173 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.5869e-01 (6.5511e-01)	Acc@1  80.47 ( 77.35)	Acc@5  99.22 ( 98.65)
Epoch: [10][310/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.5386e-01 (6.5315e-01)	Acc@1  85.16 ( 77.41)	Acc@5  99.22 ( 98.66)
Epoch: [10][320/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.1729e-01 (6.5340e-01)	Acc@1  74.22 ( 77.36)	Acc@5  97.66 ( 98.68)
Epoch: [10][330/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.0264e-01 (6.5283e-01)	Acc@1  75.78 ( 77.41)	Acc@5  97.66 ( 98.67)
Epoch: [10][340/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.1367e-01 (6.5128e-01)	Acc@1  83.59 ( 77.49)	Acc@5 100.00 ( 98.68)
Epoch: [10][350/391]	Time  0.172 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.2686e-01 (6.5170e-01)	Acc@1  80.47 ( 77.46)	Acc@5  99.22 ( 98.67)
Epoch: [10][360/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.7715e-01 (6.5098e-01)	Acc@1  77.34 ( 77.49)	Acc@5  99.22 ( 98.68)
Epoch: [10][370/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.5127e-01 (6.5037e-01)	Acc@1  81.25 ( 77.55)	Acc@5  98.44 ( 98.69)
Epoch: [10][380/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.4844e-01 (6.4910e-01)	Acc@1  75.78 ( 77.59)	Acc@5  99.22 ( 98.70)
Epoch: [10][390/391]	Time  0.116 ( 0.162)	Data  0.001 ( 0.001)	Loss 7.6611e-01 (6.4874e-01)	Acc@1  72.50 ( 77.57)	Acc@5 100.00 ( 98.70)
## e[10] optimizer.zero_grad (sum) time: 0.5166244506835938
## e[10]       loss.backward (sum) time: 12.319819450378418
## e[10]      optimizer.step (sum) time: 25.81771206855774
## epoch[10] training(only) time: 63.53225922584534
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 6.9971e-01 (6.9971e-01)	Acc@1  79.00 ( 79.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.048 ( 0.060)	Loss 5.6104e-01 (6.6504e-01)	Acc@1  81.00 ( 77.73)	Acc@5  99.00 ( 98.82)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 9.0869e-01 (6.6792e-01)	Acc@1  66.00 ( 77.24)	Acc@5  99.00 ( 98.71)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 7.3633e-01 (6.8763e-01)	Acc@1  71.00 ( 76.23)	Acc@5 100.00 ( 98.52)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 6.6260e-01 (6.8236e-01)	Acc@1  77.00 ( 76.68)	Acc@5  99.00 ( 98.59)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 6.8262e-01 (6.8092e-01)	Acc@1  78.00 ( 76.71)	Acc@5  98.00 ( 98.63)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 8.9209e-01 (6.8605e-01)	Acc@1  65.00 ( 76.43)	Acc@5  98.00 ( 98.64)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 6.8506e-01 (6.9625e-01)	Acc@1  78.00 ( 76.25)	Acc@5 100.00 ( 98.62)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 5.7129e-01 (6.9585e-01)	Acc@1  78.00 ( 76.05)	Acc@5  99.00 ( 98.64)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 6.2646e-01 (7.0294e-01)	Acc@1  75.00 ( 75.70)	Acc@5  99.00 ( 98.60)
 * Acc@1 75.930 Acc@5 98.520
### epoch[10] execution time: 68.50908660888672
EPOCH 11
# current learning rate: 0.1
# Switched to train mode...
Epoch: [11][  0/391]	Time  0.320 ( 0.320)	Data  0.141 ( 0.141)	Loss 5.4346e-01 (5.4346e-01)	Acc@1  79.69 ( 79.69)	Acc@5  98.44 ( 98.44)
Epoch: [11][ 10/391]	Time  0.160 ( 0.175)	Data  0.001 ( 0.014)	Loss 4.6680e-01 (5.8758e-01)	Acc@1  83.59 ( 79.05)	Acc@5  99.22 ( 98.86)
Epoch: [11][ 20/391]	Time  0.159 ( 0.169)	Data  0.001 ( 0.008)	Loss 8.0176e-01 (6.0677e-01)	Acc@1  70.31 ( 78.68)	Acc@5  99.22 ( 98.88)
Epoch: [11][ 30/391]	Time  0.176 ( 0.167)	Data  0.001 ( 0.006)	Loss 4.6094e-01 (5.9487e-01)	Acc@1  85.94 ( 79.08)	Acc@5  98.44 ( 98.71)
Epoch: [11][ 40/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.005)	Loss 5.4980e-01 (5.9293e-01)	Acc@1  78.91 ( 79.15)	Acc@5  98.44 ( 98.76)
Epoch: [11][ 50/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 5.1416e-01 (5.8565e-01)	Acc@1  80.47 ( 79.38)	Acc@5 100.00 ( 98.82)
Epoch: [11][ 60/391]	Time  0.175 ( 0.164)	Data  0.001 ( 0.003)	Loss 6.2061e-01 (5.7871e-01)	Acc@1  79.69 ( 79.69)	Acc@5  97.66 ( 98.80)
Epoch: [11][ 70/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 6.6064e-01 (5.8243e-01)	Acc@1  75.00 ( 79.57)	Acc@5  98.44 ( 98.79)
Epoch: [11][ 80/391]	Time  0.161 ( 0.164)	Data  0.002 ( 0.003)	Loss 7.5342e-01 (5.7896e-01)	Acc@1  73.44 ( 79.80)	Acc@5  97.66 ( 98.82)
Epoch: [11][ 90/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 7.2461e-01 (5.8170e-01)	Acc@1  74.22 ( 79.65)	Acc@5  98.44 ( 98.84)
Epoch: [11][100/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 6.5918e-01 (5.8788e-01)	Acc@1  77.34 ( 79.39)	Acc@5 100.00 ( 98.85)
Epoch: [11][110/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.002)	Loss 7.2168e-01 (5.8890e-01)	Acc@1  77.34 ( 79.34)	Acc@5  99.22 ( 98.85)
Epoch: [11][120/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.6826e-01 (5.9041e-01)	Acc@1  82.03 ( 79.30)	Acc@5  98.44 ( 98.83)
Epoch: [11][130/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.2832e-01 (5.8848e-01)	Acc@1  78.12 ( 79.32)	Acc@5  99.22 ( 98.84)
Epoch: [11][140/391]	Time  0.165 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.5439e-01 (5.9135e-01)	Acc@1  68.75 ( 79.25)	Acc@5  98.44 ( 98.85)
Epoch: [11][150/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.8887e-01 (5.9171e-01)	Acc@1  79.69 ( 79.27)	Acc@5  97.66 ( 98.84)
Epoch: [11][160/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.4365e-01 (5.9310e-01)	Acc@1  76.56 ( 79.28)	Acc@5  96.88 ( 98.81)
Epoch: [11][170/391]	Time  0.167 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.6152e-01 (5.9390e-01)	Acc@1  78.12 ( 79.23)	Acc@5  99.22 ( 98.83)
Epoch: [11][180/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.0586e-01 (5.9597e-01)	Acc@1  80.47 ( 79.11)	Acc@5 100.00 ( 98.84)
Epoch: [11][190/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.6641e-01 (5.9781e-01)	Acc@1  78.91 ( 79.08)	Acc@5 100.00 ( 98.85)
Epoch: [11][200/391]	Time  0.173 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.6445e-01 (5.9816e-01)	Acc@1  80.47 ( 79.09)	Acc@5  99.22 ( 98.85)
Epoch: [11][210/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.3066e-01 (5.9650e-01)	Acc@1  85.94 ( 79.14)	Acc@5 100.00 ( 98.86)
Epoch: [11][220/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.9004e-01 (5.9585e-01)	Acc@1  75.78 ( 79.15)	Acc@5  97.66 ( 98.85)
Epoch: [11][230/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.3535e-01 (5.9477e-01)	Acc@1  75.78 ( 79.18)	Acc@5  99.22 ( 98.87)
Epoch: [11][240/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.8311e-01 (5.9592e-01)	Acc@1  71.88 ( 79.10)	Acc@5  98.44 ( 98.88)
Epoch: [11][250/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.0820e-01 (5.9626e-01)	Acc@1  85.16 ( 79.03)	Acc@5  99.22 ( 98.88)
Epoch: [11][260/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.9170e-01 (5.9497e-01)	Acc@1  82.81 ( 79.07)	Acc@5  99.22 ( 98.87)
Epoch: [11][270/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.6348e-01 (5.9395e-01)	Acc@1  77.34 ( 79.11)	Acc@5  99.22 ( 98.88)
Epoch: [11][280/391]	Time  0.166 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.5664e-01 (5.9395e-01)	Acc@1  81.25 ( 79.11)	Acc@5  99.22 ( 98.89)
Epoch: [11][290/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.6162e-01 (5.9477e-01)	Acc@1  78.12 ( 79.12)	Acc@5  97.66 ( 98.89)
Epoch: [11][300/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.1533e-01 (5.9498e-01)	Acc@1  80.47 ( 79.16)	Acc@5  98.44 ( 98.88)
Epoch: [11][310/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.6543e-01 (5.9645e-01)	Acc@1  80.47 ( 79.12)	Acc@5 100.00 ( 98.88)
Epoch: [11][320/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.7666e-01 (5.9718e-01)	Acc@1  80.47 ( 79.11)	Acc@5  98.44 ( 98.88)
Epoch: [11][330/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.1577e-01 (5.9762e-01)	Acc@1  83.59 ( 79.08)	Acc@5 100.00 ( 98.88)
Epoch: [11][340/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.6357e-01 (5.9774e-01)	Acc@1  80.47 ( 79.08)	Acc@5  99.22 ( 98.89)
Epoch: [11][350/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.7344e-01 (5.9836e-01)	Acc@1  75.78 ( 79.05)	Acc@5 100.00 ( 98.89)
Epoch: [11][360/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.7290e-01 (5.9868e-01)	Acc@1  86.72 ( 79.06)	Acc@5  99.22 ( 98.89)
Epoch: [11][370/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.0654e-01 (5.9847e-01)	Acc@1  77.34 ( 79.05)	Acc@5  99.22 ( 98.90)
Epoch: [11][380/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.9961e-01 (5.9848e-01)	Acc@1  80.47 ( 79.07)	Acc@5  98.44 ( 98.90)
Epoch: [11][390/391]	Time  0.117 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.6299e-01 (5.9898e-01)	Acc@1  80.00 ( 79.07)	Acc@5 100.00 ( 98.89)
## e[11] optimizer.zero_grad (sum) time: 0.530339241027832
## e[11]       loss.backward (sum) time: 12.314843893051147
## e[11]      optimizer.step (sum) time: 25.834882020950317
## epoch[11] training(only) time: 63.65002918243408
# Switched to evaluate mode...
Test: [  0/100]	Time  0.179 ( 0.179)	Loss 6.0840e-01 (6.0840e-01)	Acc@1  81.00 ( 81.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.048 ( 0.062)	Loss 6.5771e-01 (6.4524e-01)	Acc@1  77.00 ( 78.09)	Acc@5  99.00 ( 98.18)
Test: [ 20/100]	Time  0.048 ( 0.055)	Loss 7.3926e-01 (6.4448e-01)	Acc@1  75.00 ( 79.00)	Acc@5 100.00 ( 98.38)
Test: [ 30/100]	Time  0.047 ( 0.053)	Loss 6.3574e-01 (6.4831e-01)	Acc@1  79.00 ( 78.61)	Acc@5 100.00 ( 98.45)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 6.7139e-01 (6.4603e-01)	Acc@1  79.00 ( 78.56)	Acc@5  98.00 ( 98.41)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 5.6006e-01 (6.3569e-01)	Acc@1  81.00 ( 78.71)	Acc@5  97.00 ( 98.49)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 6.4014e-01 (6.3769e-01)	Acc@1  78.00 ( 78.67)	Acc@5  97.00 ( 98.48)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 5.8545e-01 (6.4140e-01)	Acc@1  79.00 ( 78.44)	Acc@5 100.00 ( 98.46)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 6.6602e-01 (6.4279e-01)	Acc@1  80.00 ( 78.51)	Acc@5  97.00 ( 98.46)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 5.0000e-01 (6.4249e-01)	Acc@1  83.00 ( 78.54)	Acc@5  99.00 ( 98.43)
 * Acc@1 78.530 Acc@5 98.460
### epoch[11] execution time: 68.64336705207825
EPOCH 12
# current learning rate: 0.1
# Switched to train mode...
Epoch: [12][  0/391]	Time  0.315 ( 0.315)	Data  0.137 ( 0.137)	Loss 4.5312e-01 (4.5312e-01)	Acc@1  85.94 ( 85.94)	Acc@5  98.44 ( 98.44)
Epoch: [12][ 10/391]	Time  0.160 ( 0.176)	Data  0.001 ( 0.013)	Loss 3.9746e-01 (5.6414e-01)	Acc@1  86.72 ( 80.75)	Acc@5 100.00 ( 99.08)
Epoch: [12][ 20/391]	Time  0.161 ( 0.168)	Data  0.001 ( 0.008)	Loss 6.6650e-01 (5.8623e-01)	Acc@1  79.69 ( 79.99)	Acc@5  97.66 ( 98.96)
Epoch: [12][ 30/391]	Time  0.161 ( 0.166)	Data  0.001 ( 0.006)	Loss 5.7617e-01 (5.7514e-01)	Acc@1  82.03 ( 80.39)	Acc@5  99.22 ( 99.02)
Epoch: [12][ 40/391]	Time  0.162 ( 0.165)	Data  0.001 ( 0.004)	Loss 6.4355e-01 (5.6781e-01)	Acc@1  75.00 ( 80.47)	Acc@5 100.00 ( 99.10)
Epoch: [12][ 50/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 6.7627e-01 (5.6495e-01)	Acc@1  78.91 ( 80.64)	Acc@5  97.66 ( 99.08)
Epoch: [12][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 4.7485e-01 (5.6446e-01)	Acc@1  85.94 ( 80.48)	Acc@5 100.00 ( 99.12)
Epoch: [12][ 70/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 6.4258e-01 (5.6709e-01)	Acc@1  81.25 ( 80.37)	Acc@5  98.44 ( 99.09)
Epoch: [12][ 80/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 7.8320e-01 (5.6148e-01)	Acc@1  73.44 ( 80.48)	Acc@5  97.66 ( 99.10)
Epoch: [12][ 90/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 4.6436e-01 (5.5795e-01)	Acc@1  81.25 ( 80.58)	Acc@5  99.22 ( 99.13)
Epoch: [12][100/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.2549e-01 (5.5418e-01)	Acc@1  79.69 ( 80.69)	Acc@5  99.22 ( 99.13)
Epoch: [12][110/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.2451e-01 (5.5357e-01)	Acc@1  76.56 ( 80.72)	Acc@5  99.22 ( 99.11)
Epoch: [12][120/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.5713e-01 (5.5751e-01)	Acc@1  79.69 ( 80.57)	Acc@5  99.22 ( 99.09)
Epoch: [12][130/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.9668e-01 (5.5998e-01)	Acc@1  78.12 ( 80.50)	Acc@5  99.22 ( 99.06)
Epoch: [12][140/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.9658e-01 (5.6239e-01)	Acc@1  83.59 ( 80.52)	Acc@5  99.22 ( 99.02)
Epoch: [12][150/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.2090e-01 (5.6224e-01)	Acc@1  85.16 ( 80.50)	Acc@5 100.00 ( 99.02)
Epoch: [12][160/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.7764e-01 (5.6407e-01)	Acc@1  82.81 ( 80.43)	Acc@5  99.22 ( 99.00)
Epoch: [12][170/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.8525e-01 (5.6223e-01)	Acc@1  88.28 ( 80.43)	Acc@5 100.00 ( 98.99)
Epoch: [12][180/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.0781e-01 (5.6265e-01)	Acc@1  82.03 ( 80.43)	Acc@5 100.00 ( 98.98)
Epoch: [12][190/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.7773e-01 (5.6198e-01)	Acc@1  76.56 ( 80.44)	Acc@5  97.66 ( 98.97)
Epoch: [12][200/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.4395e-01 (5.6070e-01)	Acc@1  80.47 ( 80.48)	Acc@5  98.44 ( 98.97)
Epoch: [12][210/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.0166e-01 (5.6274e-01)	Acc@1  75.78 ( 80.48)	Acc@5  98.44 ( 98.97)
Epoch: [12][220/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.5811e-01 (5.6240e-01)	Acc@1  81.25 ( 80.42)	Acc@5  99.22 ( 98.96)
Epoch: [12][230/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.8496e-01 (5.6206e-01)	Acc@1  75.78 ( 80.43)	Acc@5  99.22 ( 98.96)
Epoch: [12][240/391]	Time  0.172 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.1904e-01 (5.5961e-01)	Acc@1  81.25 ( 80.53)	Acc@5  98.44 ( 98.97)
Epoch: [12][250/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.4590e-01 (5.5728e-01)	Acc@1  84.38 ( 80.63)	Acc@5  98.44 ( 98.99)
Epoch: [12][260/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.3506e-01 (5.5580e-01)	Acc@1  84.38 ( 80.71)	Acc@5  98.44 ( 98.99)
Epoch: [12][270/391]	Time  0.167 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.3574e-01 (5.5481e-01)	Acc@1  76.56 ( 80.76)	Acc@5  99.22 ( 99.00)
Epoch: [12][280/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.8218e-01 (5.5434e-01)	Acc@1  81.25 ( 80.75)	Acc@5  98.44 ( 99.01)
Epoch: [12][290/391]	Time  0.170 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.5146e-01 (5.5456e-01)	Acc@1  71.88 ( 80.75)	Acc@5  98.44 ( 99.00)
Epoch: [12][300/391]	Time  0.177 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.0400e-01 (5.5415e-01)	Acc@1  80.47 ( 80.76)	Acc@5  99.22 ( 99.02)
Epoch: [12][310/391]	Time  0.166 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.8169e-01 (5.5346e-01)	Acc@1  80.47 ( 80.78)	Acc@5 100.00 ( 99.03)
Epoch: [12][320/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.0918e-01 (5.5218e-01)	Acc@1  85.94 ( 80.86)	Acc@5 100.00 ( 99.03)
Epoch: [12][330/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.9580e-01 (5.5167e-01)	Acc@1  76.56 ( 80.86)	Acc@5  97.66 ( 99.04)
Epoch: [12][340/391]	Time  0.170 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.5811e-01 (5.5154e-01)	Acc@1  79.69 ( 80.86)	Acc@5  99.22 ( 99.05)
Epoch: [12][350/391]	Time  0.167 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.6787e-01 (5.5225e-01)	Acc@1  75.78 ( 80.84)	Acc@5 100.00 ( 99.05)
Epoch: [12][360/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.5020e-01 (5.5107e-01)	Acc@1  85.16 ( 80.90)	Acc@5  99.22 ( 99.06)
Epoch: [12][370/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.9033e-01 (5.5119e-01)	Acc@1  79.69 ( 80.89)	Acc@5  98.44 ( 99.07)
Epoch: [12][380/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.8506e-01 (5.5100e-01)	Acc@1  76.56 ( 80.87)	Acc@5  98.44 ( 99.07)
Epoch: [12][390/391]	Time  0.126 ( 0.163)	Data  0.001 ( 0.001)	Loss 3.2104e-01 (5.5027e-01)	Acc@1  90.00 ( 80.90)	Acc@5 100.00 ( 99.07)
## e[12] optimizer.zero_grad (sum) time: 0.534653902053833
## e[12]       loss.backward (sum) time: 12.334263563156128
## e[12]      optimizer.step (sum) time: 25.825819969177246
## epoch[12] training(only) time: 63.66393685340881
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 4.4312e-01 (4.4312e-01)	Acc@1  85.00 ( 85.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.059)	Loss 5.1855e-01 (5.7022e-01)	Acc@1  82.00 ( 80.73)	Acc@5  99.00 ( 99.27)
Test: [ 20/100]	Time  0.059 ( 0.054)	Loss 6.9580e-01 (5.8518e-01)	Acc@1  73.00 ( 80.29)	Acc@5 100.00 ( 99.14)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 6.7676e-01 (6.0124e-01)	Acc@1  76.00 ( 79.77)	Acc@5 100.00 ( 99.03)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 7.4219e-01 (6.0646e-01)	Acc@1  76.00 ( 79.61)	Acc@5  97.00 ( 98.93)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 6.6162e-01 (6.0913e-01)	Acc@1  78.00 ( 79.37)	Acc@5  99.00 ( 98.90)
Test: [ 60/100]	Time  0.047 ( 0.049)	Loss 5.4834e-01 (6.1139e-01)	Acc@1  83.00 ( 79.33)	Acc@5 100.00 ( 98.98)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 5.8203e-01 (6.1381e-01)	Acc@1  86.00 ( 79.44)	Acc@5 100.00 ( 98.99)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 4.5117e-01 (6.0823e-01)	Acc@1  85.00 ( 79.58)	Acc@5  99.00 ( 99.01)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 4.5166e-01 (6.1566e-01)	Acc@1  82.00 ( 79.44)	Acc@5 100.00 ( 99.00)
 * Acc@1 79.420 Acc@5 99.030
### epoch[12] execution time: 68.60450530052185
EPOCH 13
# current learning rate: 0.1
# Switched to train mode...
Epoch: [13][  0/391]	Time  0.307 ( 0.307)	Data  0.137 ( 0.137)	Loss 5.5615e-01 (5.5615e-01)	Acc@1  80.47 ( 80.47)	Acc@5  98.44 ( 98.44)
Epoch: [13][ 10/391]	Time  0.160 ( 0.175)	Data  0.001 ( 0.013)	Loss 6.0645e-01 (5.0994e-01)	Acc@1  76.56 ( 81.96)	Acc@5  99.22 ( 99.22)
Epoch: [13][ 20/391]	Time  0.160 ( 0.169)	Data  0.001 ( 0.008)	Loss 3.8110e-01 (5.1561e-01)	Acc@1  87.50 ( 81.85)	Acc@5  98.44 ( 99.00)
Epoch: [13][ 30/391]	Time  0.162 ( 0.166)	Data  0.001 ( 0.005)	Loss 5.4004e-01 (4.9963e-01)	Acc@1  79.69 ( 82.61)	Acc@5 100.00 ( 99.17)
Epoch: [13][ 40/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 6.1377e-01 (5.1505e-01)	Acc@1  81.25 ( 82.20)	Acc@5  96.88 ( 99.07)
Epoch: [13][ 50/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 4.1187e-01 (5.1064e-01)	Acc@1  85.16 ( 82.48)	Acc@5 100.00 ( 99.14)
Epoch: [13][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 4.2554e-01 (5.1104e-01)	Acc@1  84.38 ( 82.42)	Acc@5  99.22 ( 99.10)
Epoch: [13][ 70/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 6.4111e-01 (5.0941e-01)	Acc@1  78.12 ( 82.42)	Acc@5  98.44 ( 99.12)
Epoch: [13][ 80/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 6.7188e-01 (5.1595e-01)	Acc@1  79.69 ( 82.17)	Acc@5  99.22 ( 99.13)
Epoch: [13][ 90/391]	Time  0.168 ( 0.163)	Data  0.001 ( 0.003)	Loss 6.3477e-01 (5.1649e-01)	Acc@1  77.34 ( 82.23)	Acc@5  99.22 ( 99.13)
Epoch: [13][100/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.4395e-01 (5.1736e-01)	Acc@1  78.12 ( 82.14)	Acc@5  97.66 ( 99.09)
Epoch: [13][110/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.5664e-01 (5.1689e-01)	Acc@1  81.25 ( 82.19)	Acc@5 100.00 ( 99.11)
Epoch: [13][120/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.1230e-01 (5.1766e-01)	Acc@1  84.38 ( 82.21)	Acc@5  98.44 ( 99.12)
Epoch: [13][130/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.5322e-01 (5.1470e-01)	Acc@1  81.25 ( 82.32)	Acc@5  99.22 ( 99.14)
Epoch: [13][140/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.8887e-01 (5.1504e-01)	Acc@1  82.03 ( 82.36)	Acc@5  97.66 ( 99.12)
Epoch: [13][150/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.9229e-01 (5.1433e-01)	Acc@1  79.69 ( 82.36)	Acc@5 100.00 ( 99.11)
Epoch: [13][160/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.6543e-01 (5.1580e-01)	Acc@1  76.56 ( 82.29)	Acc@5 100.00 ( 99.08)
Epoch: [13][170/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.2969e-01 (5.1734e-01)	Acc@1  84.38 ( 82.29)	Acc@5  99.22 ( 99.07)
Epoch: [13][180/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.4395e-01 (5.1824e-01)	Acc@1  77.34 ( 82.18)	Acc@5  99.22 ( 99.09)
Epoch: [13][190/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.3418e-01 (5.2013e-01)	Acc@1  82.81 ( 82.11)	Acc@5  99.22 ( 99.09)
Epoch: [13][200/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.7822e-01 (5.1847e-01)	Acc@1  75.00 ( 82.17)	Acc@5 100.00 ( 99.12)
Epoch: [13][210/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.9487e-01 (5.2014e-01)	Acc@1  82.81 ( 82.13)	Acc@5  98.44 ( 99.11)
Epoch: [13][220/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.4736e-01 (5.2056e-01)	Acc@1  80.47 ( 82.13)	Acc@5 100.00 ( 99.12)
Epoch: [13][230/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.2539e-01 (5.2125e-01)	Acc@1  82.03 ( 82.07)	Acc@5  98.44 ( 99.12)
Epoch: [13][240/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.4648e-01 (5.2305e-01)	Acc@1  78.91 ( 82.03)	Acc@5  99.22 ( 99.11)
Epoch: [13][250/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.1797e-01 (5.2248e-01)	Acc@1  89.84 ( 82.05)	Acc@5  99.22 ( 99.10)
Epoch: [13][260/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.4600e-01 (5.2191e-01)	Acc@1  77.34 ( 82.07)	Acc@5  99.22 ( 99.11)
Epoch: [13][270/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.5176e-01 (5.2139e-01)	Acc@1  82.03 ( 82.09)	Acc@5  98.44 ( 99.11)
Epoch: [13][280/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.0928e-01 (5.2010e-01)	Acc@1  79.69 ( 82.13)	Acc@5  99.22 ( 99.13)
Epoch: [13][290/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.0879e-01 (5.1950e-01)	Acc@1  78.91 ( 82.13)	Acc@5 100.00 ( 99.14)
Epoch: [13][300/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.0010e-01 (5.1869e-01)	Acc@1  81.25 ( 82.18)	Acc@5  98.44 ( 99.15)
Epoch: [13][310/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.4653e-01 (5.1829e-01)	Acc@1  85.94 ( 82.21)	Acc@5 100.00 ( 99.15)
Epoch: [13][320/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.5176e-01 (5.1886e-01)	Acc@1  80.47 ( 82.17)	Acc@5  98.44 ( 99.14)
Epoch: [13][330/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.2646e-01 (5.1880e-01)	Acc@1  78.91 ( 82.19)	Acc@5  97.66 ( 99.14)
Epoch: [13][340/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.3311e-01 (5.2013e-01)	Acc@1  83.59 ( 82.14)	Acc@5 100.00 ( 99.14)
Epoch: [13][350/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.5996e-01 (5.2049e-01)	Acc@1  85.16 ( 82.13)	Acc@5 100.00 ( 99.14)
Epoch: [13][360/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.9668e-01 (5.2007e-01)	Acc@1  80.47 ( 82.15)	Acc@5  99.22 ( 99.15)
Epoch: [13][370/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.2197e-01 (5.1999e-01)	Acc@1  83.59 ( 82.17)	Acc@5  98.44 ( 99.13)
Epoch: [13][380/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 4.7485e-01 (5.2004e-01)	Acc@1  84.38 ( 82.15)	Acc@5  99.22 ( 99.14)
Epoch: [13][390/391]	Time  0.119 ( 0.162)	Data  0.001 ( 0.001)	Loss 4.4287e-01 (5.1921e-01)	Acc@1  87.50 ( 82.17)	Acc@5 100.00 ( 99.14)
## e[13] optimizer.zero_grad (sum) time: 0.5264661312103271
## e[13]       loss.backward (sum) time: 12.387765645980835
## e[13]      optimizer.step (sum) time: 25.7761447429657
## epoch[13] training(only) time: 63.46664309501648
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 6.3721e-01 (6.3721e-01)	Acc@1  83.00 ( 83.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.048 ( 0.061)	Loss 6.5771e-01 (6.6464e-01)	Acc@1  81.00 ( 78.18)	Acc@5  99.00 ( 99.18)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 6.3574e-01 (6.6055e-01)	Acc@1  78.00 ( 78.43)	Acc@5 100.00 ( 99.24)
Test: [ 30/100]	Time  0.048 ( 0.053)	Loss 6.7969e-01 (6.5593e-01)	Acc@1  76.00 ( 78.42)	Acc@5 100.00 ( 99.19)
Test: [ 40/100]	Time  0.048 ( 0.052)	Loss 7.4463e-01 (6.5748e-01)	Acc@1  78.00 ( 78.66)	Acc@5  98.00 ( 99.12)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 6.8359e-01 (6.4301e-01)	Acc@1  75.00 ( 78.86)	Acc@5  99.00 ( 99.08)
Test: [ 60/100]	Time  0.048 ( 0.051)	Loss 6.5088e-01 (6.4361e-01)	Acc@1  71.00 ( 78.64)	Acc@5 100.00 ( 99.13)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 7.3633e-01 (6.3952e-01)	Acc@1  77.00 ( 78.75)	Acc@5 100.00 ( 99.18)
Test: [ 80/100]	Time  0.047 ( 0.050)	Loss 5.9131e-01 (6.3924e-01)	Acc@1  79.00 ( 78.84)	Acc@5  98.00 ( 99.16)
Test: [ 90/100]	Time  0.047 ( 0.050)	Loss 5.0732e-01 (6.3823e-01)	Acc@1  86.00 ( 78.88)	Acc@5 100.00 ( 99.16)
 * Acc@1 79.030 Acc@5 99.200
### epoch[13] execution time: 68.51983404159546
EPOCH 14
# current learning rate: 0.1
# Switched to train mode...
Epoch: [14][  0/391]	Time  0.321 ( 0.321)	Data  0.145 ( 0.145)	Loss 5.1123e-01 (5.1123e-01)	Acc@1  79.69 ( 79.69)	Acc@5 100.00 (100.00)
Epoch: [14][ 10/391]	Time  0.160 ( 0.175)	Data  0.001 ( 0.014)	Loss 5.6934e-01 (5.1258e-01)	Acc@1  78.91 ( 81.46)	Acc@5  99.22 ( 99.50)
Epoch: [14][ 20/391]	Time  0.159 ( 0.169)	Data  0.001 ( 0.008)	Loss 4.3823e-01 (5.0675e-01)	Acc@1  85.16 ( 82.25)	Acc@5  97.66 ( 99.22)
Epoch: [14][ 30/391]	Time  0.161 ( 0.167)	Data  0.001 ( 0.006)	Loss 4.6826e-01 (5.0532e-01)	Acc@1  81.25 ( 82.26)	Acc@5 100.00 ( 99.19)
Epoch: [14][ 40/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.005)	Loss 5.5957e-01 (5.0133e-01)	Acc@1  82.81 ( 82.22)	Acc@5  96.88 ( 99.26)
Epoch: [14][ 50/391]	Time  0.177 ( 0.165)	Data  0.001 ( 0.004)	Loss 4.7607e-01 (5.0374e-01)	Acc@1  84.38 ( 82.18)	Acc@5 100.00 ( 99.23)
Epoch: [14][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 4.9585e-01 (5.0168e-01)	Acc@1  84.38 ( 82.39)	Acc@5  98.44 ( 99.15)
Epoch: [14][ 70/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 5.2490e-01 (5.0271e-01)	Acc@1  76.56 ( 82.39)	Acc@5  98.44 ( 99.15)
Epoch: [14][ 80/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 4.5312e-01 (5.0220e-01)	Acc@1  85.94 ( 82.30)	Acc@5  99.22 ( 99.14)
Epoch: [14][ 90/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 4.0601e-01 (5.0004e-01)	Acc@1  88.28 ( 82.37)	Acc@5  99.22 ( 99.18)
Epoch: [14][100/391]	Time  0.165 ( 0.164)	Data  0.001 ( 0.003)	Loss 3.6108e-01 (5.0279e-01)	Acc@1  90.62 ( 82.37)	Acc@5  99.22 ( 99.16)
Epoch: [14][110/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.002)	Loss 4.9219e-01 (4.9991e-01)	Acc@1  82.03 ( 82.48)	Acc@5 100.00 ( 99.18)
Epoch: [14][120/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.002)	Loss 6.0596e-01 (4.9773e-01)	Acc@1  82.81 ( 82.57)	Acc@5  96.09 ( 99.17)
Epoch: [14][130/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.5532e-01 (4.9712e-01)	Acc@1  85.16 ( 82.66)	Acc@5  98.44 ( 99.17)
Epoch: [14][140/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.3994e-01 (4.9405e-01)	Acc@1  83.59 ( 82.77)	Acc@5 100.00 ( 99.20)
Epoch: [14][150/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.7432e-01 (4.9197e-01)	Acc@1  77.34 ( 82.84)	Acc@5  99.22 ( 99.21)
Epoch: [14][160/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.5288e-01 (4.9588e-01)	Acc@1  85.94 ( 82.82)	Acc@5  98.44 ( 99.18)
Epoch: [14][170/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.8037e-01 (4.9482e-01)	Acc@1  85.94 ( 82.84)	Acc@5 100.00 ( 99.17)
Epoch: [14][180/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.8843e-01 (4.9381e-01)	Acc@1  85.16 ( 82.95)	Acc@5 100.00 ( 99.20)
Epoch: [14][190/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.3813e-01 (4.9228e-01)	Acc@1  86.72 ( 83.02)	Acc@5 100.00 ( 99.21)
Epoch: [14][200/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.5605e-01 (4.9157e-01)	Acc@1  85.16 ( 83.04)	Acc@5  97.66 ( 99.20)
Epoch: [14][210/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.1675e-01 (4.9236e-01)	Acc@1  84.38 ( 82.98)	Acc@5  99.22 ( 99.19)
Epoch: [14][220/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.3262e-01 (4.9128e-01)	Acc@1  83.59 ( 83.02)	Acc@5  99.22 ( 99.17)
Epoch: [14][230/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.0449e-01 (4.8944e-01)	Acc@1  78.91 ( 83.12)	Acc@5  97.66 ( 99.17)
Epoch: [14][240/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.4116e-01 (4.8857e-01)	Acc@1  81.25 ( 83.10)	Acc@5 100.00 ( 99.18)
Epoch: [14][250/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.5615e-01 (4.8827e-01)	Acc@1  84.38 ( 83.14)	Acc@5  98.44 ( 99.18)
Epoch: [14][260/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.6211e-01 (4.8986e-01)	Acc@1  75.00 ( 83.05)	Acc@5 100.00 ( 99.19)
Epoch: [14][270/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.9536e-01 (4.8966e-01)	Acc@1  83.59 ( 83.07)	Acc@5  99.22 ( 99.19)
Epoch: [14][280/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.2441e-01 (4.9005e-01)	Acc@1  81.25 ( 83.03)	Acc@5 100.00 ( 99.19)
Epoch: [14][290/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.3408e-01 (4.8968e-01)	Acc@1  83.59 ( 83.05)	Acc@5  99.22 ( 99.19)
Epoch: [14][300/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.7217e-01 (4.9062e-01)	Acc@1  84.38 ( 82.96)	Acc@5  98.44 ( 99.19)
Epoch: [14][310/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.3076e-01 (4.9165e-01)	Acc@1  83.59 ( 82.97)	Acc@5  98.44 ( 99.19)
Epoch: [14][320/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.0649e-01 (4.9155e-01)	Acc@1  85.94 ( 82.98)	Acc@5  99.22 ( 99.20)
Epoch: [14][330/391]	Time  0.165 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.7266e-01 (4.9197e-01)	Acc@1  83.59 ( 82.93)	Acc@5  99.22 ( 99.20)
Epoch: [14][340/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.5322e-01 (4.9197e-01)	Acc@1  76.56 ( 82.87)	Acc@5  97.66 ( 99.19)
Epoch: [14][350/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 6.3721e-01 (4.9212e-01)	Acc@1  83.59 ( 82.89)	Acc@5  98.44 ( 99.19)
Epoch: [14][360/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 7.7148e-01 (4.9215e-01)	Acc@1  77.34 ( 82.89)	Acc@5  98.44 ( 99.19)
Epoch: [14][370/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 6.1963e-01 (4.9194e-01)	Acc@1  78.91 ( 82.91)	Acc@5  99.22 ( 99.20)
Epoch: [14][380/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.001)	Loss 6.2402e-01 (4.9224e-01)	Acc@1  78.12 ( 82.90)	Acc@5  98.44 ( 99.20)
Epoch: [14][390/391]	Time  0.117 ( 0.162)	Data  0.001 ( 0.001)	Loss 5.3174e-01 (4.9240e-01)	Acc@1  82.50 ( 82.92)	Acc@5  95.00 ( 99.19)
## e[14] optimizer.zero_grad (sum) time: 0.5239121913909912
## e[14]       loss.backward (sum) time: 12.345140218734741
## e[14]      optimizer.step (sum) time: 25.8078875541687
## epoch[14] training(only) time: 63.508787393569946
# Switched to evaluate mode...
Test: [  0/100]	Time  0.179 ( 0.179)	Loss 4.5703e-01 (4.5703e-01)	Acc@1  83.00 ( 83.00)	Acc@5  98.00 ( 98.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 4.6118e-01 (5.3857e-01)	Acc@1  84.00 ( 81.82)	Acc@5 100.00 ( 99.36)
Test: [ 20/100]	Time  0.047 ( 0.053)	Loss 6.0986e-01 (5.5924e-01)	Acc@1  77.00 ( 81.14)	Acc@5 100.00 ( 99.00)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 5.4053e-01 (5.6530e-01)	Acc@1  81.00 ( 81.32)	Acc@5 100.00 ( 99.03)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 5.9277e-01 (5.6882e-01)	Acc@1  82.00 ( 81.15)	Acc@5  98.00 ( 98.95)
Test: [ 50/100]	Time  0.055 ( 0.050)	Loss 5.2979e-01 (5.6084e-01)	Acc@1  85.00 ( 81.53)	Acc@5 100.00 ( 98.92)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 5.5615e-01 (5.6329e-01)	Acc@1  83.00 ( 81.41)	Acc@5 100.00 ( 98.98)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 7.0361e-01 (5.6698e-01)	Acc@1  80.00 ( 81.30)	Acc@5  99.00 ( 99.00)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 4.2822e-01 (5.6352e-01)	Acc@1  86.00 ( 81.35)	Acc@5 100.00 ( 99.07)
Test: [ 90/100]	Time  0.049 ( 0.048)	Loss 3.8770e-01 (5.6902e-01)	Acc@1  85.00 ( 81.05)	Acc@5 100.00 ( 99.11)
 * Acc@1 80.980 Acc@5 99.100
### epoch[14] execution time: 68.44213604927063
EPOCH 15
# current learning rate: 0.1
# Switched to train mode...
Epoch: [15][  0/391]	Time  0.317 ( 0.317)	Data  0.150 ( 0.150)	Loss 4.3262e-01 (4.3262e-01)	Acc@1  81.25 ( 81.25)	Acc@5  99.22 ( 99.22)
Epoch: [15][ 10/391]	Time  0.171 ( 0.177)	Data  0.001 ( 0.015)	Loss 3.9429e-01 (4.1366e-01)	Acc@1  85.94 ( 85.58)	Acc@5  98.44 ( 99.57)
Epoch: [15][ 20/391]	Time  0.160 ( 0.169)	Data  0.001 ( 0.008)	Loss 6.3379e-01 (4.2554e-01)	Acc@1  82.81 ( 85.01)	Acc@5  96.88 ( 99.37)
Epoch: [15][ 30/391]	Time  0.162 ( 0.167)	Data  0.001 ( 0.006)	Loss 4.4116e-01 (4.2499e-01)	Acc@1  84.38 ( 84.90)	Acc@5  99.22 ( 99.40)
Epoch: [15][ 40/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.005)	Loss 6.3770e-01 (4.4494e-01)	Acc@1  79.69 ( 84.09)	Acc@5 100.00 ( 99.39)
Epoch: [15][ 50/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 5.6494e-01 (4.5119e-01)	Acc@1  79.69 ( 83.88)	Acc@5  98.44 ( 99.37)
Epoch: [15][ 60/391]	Time  0.175 ( 0.165)	Data  0.001 ( 0.004)	Loss 3.8501e-01 (4.5232e-01)	Acc@1  85.16 ( 83.84)	Acc@5  98.44 ( 99.37)
Epoch: [15][ 70/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 3.6646e-01 (4.4432e-01)	Acc@1  91.41 ( 84.15)	Acc@5  99.22 ( 99.39)
Epoch: [15][ 80/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 4.8730e-01 (4.4956e-01)	Acc@1  82.81 ( 83.98)	Acc@5  98.44 ( 99.40)
Epoch: [15][ 90/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 5.3467e-01 (4.5146e-01)	Acc@1  80.47 ( 83.97)	Acc@5 100.00 ( 99.43)
Epoch: [15][100/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 5.5664e-01 (4.5605e-01)	Acc@1  83.59 ( 83.88)	Acc@5  99.22 ( 99.41)
Epoch: [15][110/391]	Time  0.171 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.4995e-01 (4.5957e-01)	Acc@1  86.72 ( 83.87)	Acc@5 100.00 ( 99.41)
Epoch: [15][120/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.9326e-01 (4.6062e-01)	Acc@1  85.16 ( 83.92)	Acc@5  97.66 ( 99.37)
Epoch: [15][130/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.1768e-01 (4.6294e-01)	Acc@1  76.56 ( 83.83)	Acc@5  98.44 ( 99.36)
Epoch: [15][140/391]	Time  0.177 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.6436e-01 (4.6179e-01)	Acc@1  82.81 ( 83.81)	Acc@5  99.22 ( 99.36)
Epoch: [15][150/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.8438e-01 (4.6268e-01)	Acc@1  81.25 ( 83.72)	Acc@5  99.22 ( 99.36)
Epoch: [15][160/391]	Time  0.169 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.7476e-01 (4.6301e-01)	Acc@1  85.94 ( 83.71)	Acc@5 100.00 ( 99.36)
Epoch: [15][170/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.3857e-01 (4.6422e-01)	Acc@1  78.12 ( 83.61)	Acc@5 100.00 ( 99.35)
Epoch: [15][180/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.0112e-01 (4.6403e-01)	Acc@1  89.06 ( 83.71)	Acc@5  99.22 ( 99.33)
Epoch: [15][190/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.4727e-01 (4.6491e-01)	Acc@1  84.38 ( 83.75)	Acc@5 100.00 ( 99.31)
Epoch: [15][200/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.0049e-01 (4.6212e-01)	Acc@1  79.69 ( 83.84)	Acc@5  99.22 ( 99.33)
Epoch: [15][210/391]	Time  0.173 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.5435e-01 (4.6109e-01)	Acc@1  83.59 ( 83.87)	Acc@5 100.00 ( 99.34)
Epoch: [15][220/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.2881e-01 (4.6203e-01)	Acc@1  83.59 ( 83.91)	Acc@5 100.00 ( 99.35)
Epoch: [15][230/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.3125e-01 (4.6251e-01)	Acc@1  78.91 ( 83.83)	Acc@5  99.22 ( 99.35)
Epoch: [15][240/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.9756e-01 (4.6246e-01)	Acc@1  83.59 ( 83.82)	Acc@5 100.00 ( 99.35)
Epoch: [15][250/391]	Time  0.176 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.0186e-01 (4.6279e-01)	Acc@1  84.38 ( 83.85)	Acc@5 100.00 ( 99.35)
Epoch: [15][260/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.5068e-01 (4.6455e-01)	Acc@1  85.16 ( 83.81)	Acc@5 100.00 ( 99.36)
Epoch: [15][270/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.4214e-01 (4.6419e-01)	Acc@1  86.72 ( 83.83)	Acc@5 100.00 ( 99.35)
Epoch: [15][280/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.2246e-01 (4.6553e-01)	Acc@1  77.34 ( 83.78)	Acc@5 100.00 ( 99.36)
Epoch: [15][290/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.6460e-01 (4.6664e-01)	Acc@1  85.94 ( 83.75)	Acc@5 100.00 ( 99.36)
Epoch: [15][300/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.7720e-01 (4.6679e-01)	Acc@1  86.72 ( 83.75)	Acc@5  99.22 ( 99.36)
Epoch: [15][310/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.6455e-01 (4.6740e-01)	Acc@1  79.69 ( 83.75)	Acc@5  98.44 ( 99.35)
Epoch: [15][320/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.5811e-01 (4.6765e-01)	Acc@1  80.47 ( 83.76)	Acc@5  97.66 ( 99.32)
Epoch: [15][330/391]	Time  0.169 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.8447e-01 (4.6689e-01)	Acc@1  78.12 ( 83.77)	Acc@5 100.00 ( 99.33)
Epoch: [15][340/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.6313e-01 (4.6634e-01)	Acc@1  82.81 ( 83.79)	Acc@5 100.00 ( 99.33)
Epoch: [15][350/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.7803e-01 (4.6660e-01)	Acc@1  80.47 ( 83.80)	Acc@5 100.00 ( 99.34)
Epoch: [15][360/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.6411e-01 (4.6619e-01)	Acc@1  82.03 ( 83.81)	Acc@5 100.00 ( 99.35)
Epoch: [15][370/391]	Time  0.168 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.8755e-01 (4.6616e-01)	Acc@1  85.16 ( 83.81)	Acc@5  98.44 ( 99.34)
Epoch: [15][380/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.0967e-01 (4.6629e-01)	Acc@1  83.59 ( 83.79)	Acc@5  99.22 ( 99.33)
Epoch: [15][390/391]	Time  0.118 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.2656e-01 (4.6528e-01)	Acc@1  76.25 ( 83.85)	Acc@5 100.00 ( 99.33)
## e[15] optimizer.zero_grad (sum) time: 0.5225517749786377
## e[15]       loss.backward (sum) time: 12.39928913116455
## e[15]      optimizer.step (sum) time: 25.743048906326294
## epoch[15] training(only) time: 63.555368423461914
# Switched to evaluate mode...
Test: [  0/100]	Time  0.181 ( 0.181)	Loss 4.5483e-01 (4.5483e-01)	Acc@1  83.00 ( 83.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.047 ( 0.060)	Loss 6.0352e-01 (4.9117e-01)	Acc@1  82.00 ( 84.27)	Acc@5 100.00 ( 99.09)
Test: [ 20/100]	Time  0.048 ( 0.053)	Loss 4.8145e-01 (4.8987e-01)	Acc@1  83.00 ( 83.76)	Acc@5 100.00 ( 99.00)
Test: [ 30/100]	Time  0.046 ( 0.051)	Loss 5.8398e-01 (4.9542e-01)	Acc@1  82.00 ( 83.81)	Acc@5  97.00 ( 98.97)
Test: [ 40/100]	Time  0.047 ( 0.050)	Loss 5.2832e-01 (4.9767e-01)	Acc@1  82.00 ( 83.49)	Acc@5  98.00 ( 98.90)
Test: [ 50/100]	Time  0.048 ( 0.050)	Loss 4.4556e-01 (4.9826e-01)	Acc@1  87.00 ( 83.61)	Acc@5 100.00 ( 98.90)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 5.5225e-01 (4.9691e-01)	Acc@1  84.00 ( 83.51)	Acc@5 100.00 ( 98.97)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 5.1270e-01 (4.9690e-01)	Acc@1  81.00 ( 83.55)	Acc@5  99.00 ( 99.00)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 4.5239e-01 (4.9463e-01)	Acc@1  82.00 ( 83.41)	Acc@5 100.00 ( 99.09)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 4.2456e-01 (5.0130e-01)	Acc@1  85.00 ( 83.23)	Acc@5  99.00 ( 99.08)
 * Acc@1 83.270 Acc@5 99.130
### epoch[15] execution time: 68.51603317260742
EPOCH 16
# current learning rate: 0.1
# Switched to train mode...
Epoch: [16][  0/391]	Time  0.319 ( 0.319)	Data  0.148 ( 0.148)	Loss 5.1123e-01 (5.1123e-01)	Acc@1  81.25 ( 81.25)	Acc@5 100.00 (100.00)
Epoch: [16][ 10/391]	Time  0.160 ( 0.176)	Data  0.001 ( 0.014)	Loss 5.2539e-01 (4.3277e-01)	Acc@1  79.69 ( 85.01)	Acc@5  99.22 ( 99.50)
Epoch: [16][ 20/391]	Time  0.160 ( 0.169)	Data  0.001 ( 0.008)	Loss 4.1650e-01 (4.3329e-01)	Acc@1  82.81 ( 84.38)	Acc@5 100.00 ( 99.63)
Epoch: [16][ 30/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.006)	Loss 3.3081e-01 (4.2998e-01)	Acc@1  86.72 ( 84.95)	Acc@5  99.22 ( 99.65)
Epoch: [16][ 40/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.005)	Loss 5.2441e-01 (4.2705e-01)	Acc@1  82.81 ( 85.18)	Acc@5  99.22 ( 99.62)
Epoch: [16][ 50/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.004)	Loss 4.3286e-01 (4.3704e-01)	Acc@1  82.81 ( 84.70)	Acc@5  97.66 ( 99.51)
Epoch: [16][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 3.1787e-01 (4.3313e-01)	Acc@1  89.84 ( 84.77)	Acc@5 100.00 ( 99.54)
Epoch: [16][ 70/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.2202e-01 (4.3401e-01)	Acc@1  89.06 ( 84.87)	Acc@5 100.00 ( 99.54)
Epoch: [16][ 80/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 4.4946e-01 (4.3466e-01)	Acc@1  84.38 ( 84.79)	Acc@5 100.00 ( 99.58)
Epoch: [16][ 90/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.9624e-01 (4.3824e-01)	Acc@1  87.50 ( 84.77)	Acc@5  99.22 ( 99.54)
Epoch: [16][100/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.4644e-01 (4.3711e-01)	Acc@1  89.06 ( 84.68)	Acc@5 100.00 ( 99.51)
Epoch: [16][110/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.5728e-01 (4.3661e-01)	Acc@1  84.38 ( 84.71)	Acc@5 100.00 ( 99.53)
Epoch: [16][120/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.6621e-01 (4.3607e-01)	Acc@1  87.50 ( 84.76)	Acc@5  99.22 ( 99.52)
Epoch: [16][130/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.9570e-01 (4.4044e-01)	Acc@1  78.12 ( 84.65)	Acc@5 100.00 ( 99.49)
Epoch: [16][140/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.4922e-01 (4.4092e-01)	Acc@1  82.03 ( 84.66)	Acc@5  99.22 ( 99.47)
Epoch: [16][150/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.5532e-01 (4.3940e-01)	Acc@1  84.38 ( 84.77)	Acc@5  99.22 ( 99.45)
Epoch: [16][160/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.9131e-01 (4.3925e-01)	Acc@1  81.25 ( 84.82)	Acc@5  98.44 ( 99.45)
Epoch: [16][170/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.3188e-01 (4.3910e-01)	Acc@1  84.38 ( 84.80)	Acc@5  99.22 ( 99.44)
Epoch: [16][180/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.9780e-01 (4.3969e-01)	Acc@1  84.38 ( 84.72)	Acc@5  99.22 ( 99.43)
Epoch: [16][190/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.5557e-01 (4.4165e-01)	Acc@1  83.59 ( 84.67)	Acc@5  98.44 ( 99.44)
Epoch: [16][200/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.3530e-01 (4.4137e-01)	Acc@1  87.50 ( 84.68)	Acc@5  99.22 ( 99.44)
Epoch: [16][210/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.3867e-01 (4.4042e-01)	Acc@1  78.91 ( 84.75)	Acc@5  96.88 ( 99.43)
Epoch: [16][220/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.3262e-01 (4.4250e-01)	Acc@1  82.81 ( 84.69)	Acc@5  99.22 ( 99.42)
Epoch: [16][230/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.2246e-01 (4.4211e-01)	Acc@1  82.03 ( 84.72)	Acc@5 100.00 ( 99.42)
Epoch: [16][240/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.2871e-01 (4.4142e-01)	Acc@1  85.16 ( 84.74)	Acc@5  99.22 ( 99.43)
Epoch: [16][250/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.3652e-01 (4.4041e-01)	Acc@1  83.59 ( 84.74)	Acc@5  99.22 ( 99.43)
Epoch: [16][260/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.5400e-01 (4.3993e-01)	Acc@1  85.94 ( 84.77)	Acc@5 100.00 ( 99.41)
Epoch: [16][270/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.5176e-01 (4.4044e-01)	Acc@1  81.25 ( 84.76)	Acc@5 100.00 ( 99.41)
Epoch: [16][280/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.7876e-01 (4.4070e-01)	Acc@1  82.03 ( 84.77)	Acc@5 100.00 ( 99.40)
Epoch: [16][290/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.8257e-01 (4.4138e-01)	Acc@1  86.72 ( 84.74)	Acc@5  98.44 ( 99.40)
Epoch: [16][300/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.2236e-01 (4.4072e-01)	Acc@1  85.16 ( 84.74)	Acc@5  99.22 ( 99.41)
Epoch: [16][310/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.7451e-01 (4.4126e-01)	Acc@1  85.94 ( 84.72)	Acc@5 100.00 ( 99.41)
Epoch: [16][320/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.8687e-01 (4.4112e-01)	Acc@1  90.62 ( 84.73)	Acc@5 100.00 ( 99.40)
Epoch: [16][330/391]	Time  0.177 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.1528e-01 (4.4054e-01)	Acc@1  87.50 ( 84.76)	Acc@5 100.00 ( 99.41)
Epoch: [16][340/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.3960e-01 (4.3953e-01)	Acc@1  90.62 ( 84.80)	Acc@5 100.00 ( 99.42)
Epoch: [16][350/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.1416e-01 (4.4021e-01)	Acc@1  83.59 ( 84.77)	Acc@5  99.22 ( 99.42)
Epoch: [16][360/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.9121e-01 (4.3896e-01)	Acc@1  83.59 ( 84.81)	Acc@5 100.00 ( 99.42)
Epoch: [16][370/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.3604e-01 (4.3858e-01)	Acc@1  84.38 ( 84.79)	Acc@5  98.44 ( 99.42)
Epoch: [16][380/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.2236e-01 (4.3890e-01)	Acc@1  86.72 ( 84.79)	Acc@5 100.00 ( 99.42)
Epoch: [16][390/391]	Time  0.116 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.8486e-01 (4.3852e-01)	Acc@1  86.25 ( 84.82)	Acc@5  97.50 ( 99.41)
## e[16] optimizer.zero_grad (sum) time: 0.5267229080200195
## e[16]       loss.backward (sum) time: 12.390830993652344
## e[16]      optimizer.step (sum) time: 25.74826741218567
## epoch[16] training(only) time: 63.36301589012146
# Switched to evaluate mode...
Test: [  0/100]	Time  0.181 ( 0.181)	Loss 4.1797e-01 (4.1797e-01)	Acc@1  85.00 ( 85.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.048 ( 0.060)	Loss 5.4883e-01 (5.5424e-01)	Acc@1  81.00 ( 81.18)	Acc@5 100.00 ( 99.27)
Test: [ 20/100]	Time  0.048 ( 0.054)	Loss 5.1465e-01 (5.4840e-01)	Acc@1  83.00 ( 81.52)	Acc@5  99.00 ( 99.24)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 5.0195e-01 (5.4931e-01)	Acc@1  82.00 ( 81.55)	Acc@5  99.00 ( 99.32)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 5.3174e-01 (5.4845e-01)	Acc@1  81.00 ( 81.49)	Acc@5 100.00 ( 99.32)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 5.1465e-01 (5.3752e-01)	Acc@1  79.00 ( 81.76)	Acc@5  98.00 ( 99.29)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 7.3730e-01 (5.5093e-01)	Acc@1  76.00 ( 81.44)	Acc@5 100.00 ( 99.34)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 5.0635e-01 (5.5006e-01)	Acc@1  82.00 ( 81.52)	Acc@5 100.00 ( 99.38)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 5.0391e-01 (5.4897e-01)	Acc@1  83.00 ( 81.52)	Acc@5 100.00 ( 99.41)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 4.5679e-01 (5.4970e-01)	Acc@1  83.00 ( 81.44)	Acc@5  99.00 ( 99.43)
 * Acc@1 81.420 Acc@5 99.440
### epoch[16] execution time: 68.33485865592957
EPOCH 17
# current learning rate: 0.1
# Switched to train mode...
Epoch: [17][  0/391]	Time  0.329 ( 0.329)	Data  0.157 ( 0.157)	Loss 4.2871e-01 (4.2871e-01)	Acc@1  84.38 ( 84.38)	Acc@5 100.00 (100.00)
Epoch: [17][ 10/391]	Time  0.161 ( 0.177)	Data  0.001 ( 0.015)	Loss 3.7012e-01 (4.1064e-01)	Acc@1  85.16 ( 85.09)	Acc@5 100.00 ( 99.86)
Epoch: [17][ 20/391]	Time  0.173 ( 0.170)	Data  0.001 ( 0.009)	Loss 3.4082e-01 (4.2294e-01)	Acc@1  85.94 ( 85.12)	Acc@5  99.22 ( 99.67)
Epoch: [17][ 30/391]	Time  0.160 ( 0.167)	Data  0.001 ( 0.006)	Loss 6.0791e-01 (4.0211e-01)	Acc@1  82.03 ( 86.11)	Acc@5 100.00 ( 99.62)
Epoch: [17][ 40/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.005)	Loss 4.6167e-01 (4.0125e-01)	Acc@1  85.94 ( 86.17)	Acc@5  97.66 ( 99.56)
Epoch: [17][ 50/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 3.1201e-01 (4.0138e-01)	Acc@1  89.84 ( 86.17)	Acc@5 100.00 ( 99.51)
Epoch: [17][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 3.2520e-01 (3.9828e-01)	Acc@1  89.06 ( 86.32)	Acc@5  99.22 ( 99.54)
Epoch: [17][ 70/391]	Time  0.177 ( 0.164)	Data  0.001 ( 0.003)	Loss 3.6572e-01 (4.0158e-01)	Acc@1  87.50 ( 86.14)	Acc@5  99.22 ( 99.48)
Epoch: [17][ 80/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 4.3359e-01 (4.0292e-01)	Acc@1  85.16 ( 86.10)	Acc@5 100.00 ( 99.54)
Epoch: [17][ 90/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 4.8975e-01 (4.0120e-01)	Acc@1  82.03 ( 86.13)	Acc@5 100.00 ( 99.54)
Epoch: [17][100/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.9761e-01 (4.0055e-01)	Acc@1  91.41 ( 86.08)	Acc@5  99.22 ( 99.56)
Epoch: [17][110/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.003)	Loss 4.1821e-01 (4.0237e-01)	Acc@1  84.38 ( 86.02)	Acc@5  99.22 ( 99.56)
Epoch: [17][120/391]	Time  0.169 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.7607e-01 (4.0595e-01)	Acc@1  86.72 ( 85.96)	Acc@5  96.88 ( 99.51)
Epoch: [17][130/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.7783e-01 (4.0583e-01)	Acc@1  89.06 ( 86.01)	Acc@5  99.22 ( 99.48)
Epoch: [17][140/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.2041e-01 (4.0624e-01)	Acc@1  85.16 ( 86.10)	Acc@5  99.22 ( 99.48)
Epoch: [17][150/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.3359e-01 (4.0696e-01)	Acc@1  86.72 ( 86.07)	Acc@5  99.22 ( 99.48)
Epoch: [17][160/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.6655e-01 (4.0659e-01)	Acc@1  81.25 ( 86.08)	Acc@5  99.22 ( 99.47)
Epoch: [17][170/391]	Time  0.168 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.3115e-01 (4.1007e-01)	Acc@1  87.50 ( 85.99)	Acc@5  98.44 ( 99.45)
Epoch: [17][180/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.1909e-01 (4.1252e-01)	Acc@1  88.28 ( 85.89)	Acc@5 100.00 ( 99.43)
Epoch: [17][190/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.5703e-01 (4.1446e-01)	Acc@1  83.59 ( 85.79)	Acc@5 100.00 ( 99.42)
Epoch: [17][200/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.9248e-01 (4.1456e-01)	Acc@1  92.97 ( 85.79)	Acc@5  99.22 ( 99.42)
Epoch: [17][210/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.9355e-01 (4.1575e-01)	Acc@1  85.94 ( 85.76)	Acc@5 100.00 ( 99.41)
Epoch: [17][220/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.1235e-01 (4.1674e-01)	Acc@1  85.16 ( 85.68)	Acc@5  99.22 ( 99.42)
Epoch: [17][230/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.2007e-01 (4.1783e-01)	Acc@1  91.41 ( 85.66)	Acc@5  99.22 ( 99.41)
Epoch: [17][240/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.2773e-01 (4.1688e-01)	Acc@1  85.16 ( 85.77)	Acc@5  97.66 ( 99.39)
Epoch: [17][250/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.6265e-01 (4.1719e-01)	Acc@1  81.25 ( 85.74)	Acc@5 100.00 ( 99.41)
Epoch: [17][260/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.1738e-01 (4.1703e-01)	Acc@1  89.06 ( 85.70)	Acc@5  99.22 ( 99.40)
Epoch: [17][270/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.6753e-01 (4.1658e-01)	Acc@1  84.38 ( 85.73)	Acc@5 100.00 ( 99.40)
Epoch: [17][280/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.8574e-01 (4.1814e-01)	Acc@1  92.97 ( 85.67)	Acc@5 100.00 ( 99.40)
Epoch: [17][290/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.5703e-01 (4.1854e-01)	Acc@1  84.38 ( 85.66)	Acc@5  99.22 ( 99.40)
Epoch: [17][300/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.6499e-01 (4.1962e-01)	Acc@1  84.38 ( 85.63)	Acc@5 100.00 ( 99.40)
Epoch: [17][310/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.2822e-01 (4.1921e-01)	Acc@1  86.72 ( 85.65)	Acc@5 100.00 ( 99.40)
Epoch: [17][320/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.0059e-01 (4.2013e-01)	Acc@1  78.91 ( 85.59)	Acc@5  99.22 ( 99.41)
Epoch: [17][330/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.7964e-01 (4.1979e-01)	Acc@1  88.28 ( 85.61)	Acc@5 100.00 ( 99.40)
Epoch: [17][340/391]	Time  0.165 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.3711e-01 (4.2000e-01)	Acc@1  80.47 ( 85.61)	Acc@5 100.00 ( 99.41)
Epoch: [17][350/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.3032e-01 (4.1892e-01)	Acc@1  88.28 ( 85.66)	Acc@5  99.22 ( 99.41)
Epoch: [17][360/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 3.5327e-01 (4.1806e-01)	Acc@1  89.84 ( 85.69)	Acc@5  99.22 ( 99.40)
Epoch: [17][370/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 3.4473e-01 (4.1755e-01)	Acc@1  87.50 ( 85.72)	Acc@5 100.00 ( 99.40)
Epoch: [17][380/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 3.0322e-01 (4.1738e-01)	Acc@1  87.50 ( 85.72)	Acc@5 100.00 ( 99.40)
Epoch: [17][390/391]	Time  0.116 ( 0.162)	Data  0.001 ( 0.001)	Loss 4.7388e-01 (4.1769e-01)	Acc@1  81.25 ( 85.70)	Acc@5  97.50 ( 99.40)
## e[17] optimizer.zero_grad (sum) time: 0.5238893032073975
## e[17]       loss.backward (sum) time: 12.437732934951782
## e[17]      optimizer.step (sum) time: 25.724162101745605
## epoch[17] training(only) time: 63.4129056930542
# Switched to evaluate mode...
Test: [  0/100]	Time  0.179 ( 0.179)	Loss 4.6655e-01 (4.6655e-01)	Acc@1  79.00 ( 79.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.048 ( 0.060)	Loss 4.8413e-01 (5.4284e-01)	Acc@1  85.00 ( 81.36)	Acc@5 100.00 ( 99.45)
Test: [ 20/100]	Time  0.047 ( 0.054)	Loss 5.9766e-01 (5.4206e-01)	Acc@1  80.00 ( 81.57)	Acc@5  98.00 ( 99.00)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 4.6680e-01 (5.5588e-01)	Acc@1  86.00 ( 81.74)	Acc@5 100.00 ( 98.97)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 6.3232e-01 (5.6038e-01)	Acc@1  84.00 ( 81.80)	Acc@5  98.00 ( 98.90)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 4.3066e-01 (5.5606e-01)	Acc@1  89.00 ( 81.88)	Acc@5  99.00 ( 98.88)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 6.8115e-01 (5.5596e-01)	Acc@1  80.00 ( 81.89)	Acc@5  99.00 ( 98.90)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 5.2148e-01 (5.5523e-01)	Acc@1  82.00 ( 81.96)	Acc@5  99.00 ( 98.89)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 4.2407e-01 (5.5239e-01)	Acc@1  87.00 ( 82.05)	Acc@5 100.00 ( 98.95)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 5.5176e-01 (5.5684e-01)	Acc@1  76.00 ( 81.96)	Acc@5 100.00 ( 98.99)
 * Acc@1 81.980 Acc@5 98.990
### epoch[17] execution time: 68.36216878890991
EPOCH 18
# current learning rate: 0.1
# Switched to train mode...
Epoch: [18][  0/391]	Time  0.313 ( 0.313)	Data  0.145 ( 0.145)	Loss 4.8022e-01 (4.8022e-01)	Acc@1  82.81 ( 82.81)	Acc@5  99.22 ( 99.22)
Epoch: [18][ 10/391]	Time  0.162 ( 0.175)	Data  0.001 ( 0.014)	Loss 4.1113e-01 (4.0443e-01)	Acc@1  87.50 ( 86.15)	Acc@5 100.00 ( 99.64)
Epoch: [18][ 20/391]	Time  0.160 ( 0.168)	Data  0.001 ( 0.008)	Loss 4.9146e-01 (3.9895e-01)	Acc@1  82.81 ( 86.31)	Acc@5 100.00 ( 99.63)
Epoch: [18][ 30/391]	Time  0.161 ( 0.166)	Data  0.001 ( 0.006)	Loss 3.9990e-01 (3.9088e-01)	Acc@1  86.72 ( 86.79)	Acc@5  99.22 ( 99.65)
Epoch: [18][ 40/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.005)	Loss 2.2510e-01 (3.8862e-01)	Acc@1  92.19 ( 86.93)	Acc@5 100.00 ( 99.62)
Epoch: [18][ 50/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 3.6011e-01 (3.8868e-01)	Acc@1  87.50 ( 86.81)	Acc@5 100.00 ( 99.60)
Epoch: [18][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 2.5537e-01 (3.8728e-01)	Acc@1  90.62 ( 86.82)	Acc@5 100.00 ( 99.62)
Epoch: [18][ 70/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.8818e-01 (3.8604e-01)	Acc@1  88.28 ( 86.86)	Acc@5  99.22 ( 99.58)
Epoch: [18][ 80/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.6279e-01 (3.8669e-01)	Acc@1  87.50 ( 86.84)	Acc@5  99.22 ( 99.58)
Epoch: [18][ 90/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 4.8926e-01 (3.8877e-01)	Acc@1  85.94 ( 86.74)	Acc@5  98.44 ( 99.57)
Epoch: [18][100/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 5.0098e-01 (3.9083e-01)	Acc@1  82.03 ( 86.64)	Acc@5  99.22 ( 99.55)
Epoch: [18][110/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.2715e-01 (3.9395e-01)	Acc@1  89.84 ( 86.56)	Acc@5 100.00 ( 99.54)
Epoch: [18][120/391]	Time  0.169 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.0781e-01 (3.9453e-01)	Acc@1  80.47 ( 86.46)	Acc@5 100.00 ( 99.56)
Epoch: [18][130/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.1333e-01 (3.9787e-01)	Acc@1  84.38 ( 86.37)	Acc@5  99.22 ( 99.53)
Epoch: [18][140/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.0293e-01 (4.0144e-01)	Acc@1  82.81 ( 86.26)	Acc@5  99.22 ( 99.50)
Epoch: [18][150/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.8301e-01 (4.0147e-01)	Acc@1  79.69 ( 86.23)	Acc@5 100.00 ( 99.50)
Epoch: [18][160/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.5093e-01 (3.9887e-01)	Acc@1  89.06 ( 86.35)	Acc@5  96.88 ( 99.48)
Epoch: [18][170/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.7427e-01 (3.9967e-01)	Acc@1  85.94 ( 86.31)	Acc@5  99.22 ( 99.47)
Epoch: [18][180/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.2061e-01 (4.0026e-01)	Acc@1  82.81 ( 86.29)	Acc@5 100.00 ( 99.46)
Epoch: [18][190/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.7412e-01 (4.0084e-01)	Acc@1  83.59 ( 86.25)	Acc@5  99.22 ( 99.48)
Epoch: [18][200/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.8145e-01 (3.9976e-01)	Acc@1  80.47 ( 86.26)	Acc@5 100.00 ( 99.49)
Epoch: [18][210/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.2104e-01 (3.9808e-01)	Acc@1  85.94 ( 86.30)	Acc@5 100.00 ( 99.49)
Epoch: [18][220/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.2749e-01 (3.9782e-01)	Acc@1  82.81 ( 86.29)	Acc@5 100.00 ( 99.50)
Epoch: [18][230/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.4556e-01 (3.9885e-01)	Acc@1  84.38 ( 86.22)	Acc@5  99.22 ( 99.50)
Epoch: [18][240/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.2373e-01 (3.9852e-01)	Acc@1  88.28 ( 86.16)	Acc@5 100.00 ( 99.50)
Epoch: [18][250/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.3057e-01 (3.9829e-01)	Acc@1  86.72 ( 86.17)	Acc@5 100.00 ( 99.49)
Epoch: [18][260/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.3359e-01 (3.9820e-01)	Acc@1  87.50 ( 86.19)	Acc@5  98.44 ( 99.48)
Epoch: [18][270/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.5728e-01 (3.9817e-01)	Acc@1  84.38 ( 86.17)	Acc@5 100.00 ( 99.48)
Epoch: [18][280/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.0649e-01 (3.9812e-01)	Acc@1  83.59 ( 86.18)	Acc@5 100.00 ( 99.48)
Epoch: [18][290/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.6680e-01 (3.9890e-01)	Acc@1  85.94 ( 86.18)	Acc@5  99.22 ( 99.47)
Epoch: [18][300/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.2969e-01 (3.9833e-01)	Acc@1  84.38 ( 86.21)	Acc@5 100.00 ( 99.47)
Epoch: [18][310/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.2163e-01 (3.9918e-01)	Acc@1  85.94 ( 86.19)	Acc@5  99.22 ( 99.47)
Epoch: [18][320/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.1689e-01 (3.9941e-01)	Acc@1  90.62 ( 86.17)	Acc@5 100.00 ( 99.47)
Epoch: [18][330/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.7378e-01 (3.9829e-01)	Acc@1  87.50 ( 86.19)	Acc@5  97.66 ( 99.47)
Epoch: [18][340/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 4.9536e-01 (3.9805e-01)	Acc@1  84.38 ( 86.22)	Acc@5  99.22 ( 99.46)
Epoch: [18][350/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 4.3652e-01 (3.9931e-01)	Acc@1  85.16 ( 86.19)	Acc@5  99.22 ( 99.45)
Epoch: [18][360/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 4.7583e-01 (3.9969e-01)	Acc@1  85.16 ( 86.17)	Acc@5 100.00 ( 99.46)
Epoch: [18][370/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.001)	Loss 4.0820e-01 (3.9964e-01)	Acc@1  84.38 ( 86.16)	Acc@5  99.22 ( 99.46)
Epoch: [18][380/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 5.5566e-01 (4.0009e-01)	Acc@1  79.69 ( 86.14)	Acc@5  97.66 ( 99.47)
Epoch: [18][390/391]	Time  0.117 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.7026e-01 (4.0030e-01)	Acc@1  92.50 ( 86.13)	Acc@5 100.00 ( 99.47)
## e[18] optimizer.zero_grad (sum) time: 0.5265240669250488
## e[18]       loss.backward (sum) time: 12.412782669067383
## e[18]      optimizer.step (sum) time: 25.761199235916138
## epoch[18] training(only) time: 63.418912410736084
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 3.3716e-01 (3.3716e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.048 ( 0.062)	Loss 4.9707e-01 (4.5257e-01)	Acc@1  84.00 ( 85.09)	Acc@5  99.00 ( 99.00)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 5.2832e-01 (4.5814e-01)	Acc@1  78.00 ( 84.67)	Acc@5 100.00 ( 99.24)
Test: [ 30/100]	Time  0.057 ( 0.053)	Loss 4.8950e-01 (4.6365e-01)	Acc@1  87.00 ( 84.61)	Acc@5 100.00 ( 99.39)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 4.5483e-01 (4.6542e-01)	Acc@1  85.00 ( 84.85)	Acc@5  97.00 ( 99.20)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 3.7183e-01 (4.5785e-01)	Acc@1  89.00 ( 85.27)	Acc@5  99.00 ( 99.18)
Test: [ 60/100]	Time  0.048 ( 0.050)	Loss 3.6987e-01 (4.5504e-01)	Acc@1  88.00 ( 85.20)	Acc@5 100.00 ( 99.26)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 3.9258e-01 (4.5183e-01)	Acc@1  87.00 ( 85.23)	Acc@5 100.00 ( 99.31)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 2.8027e-01 (4.4540e-01)	Acc@1  92.00 ( 85.42)	Acc@5  99.00 ( 99.33)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 3.7231e-01 (4.5034e-01)	Acc@1  83.00 ( 85.12)	Acc@5 100.00 ( 99.38)
 * Acc@1 85.120 Acc@5 99.370
### epoch[18] execution time: 68.42317366600037
EPOCH 19
# current learning rate: 0.1
# Switched to train mode...
Epoch: [19][  0/391]	Time  0.330 ( 0.330)	Data  0.143 ( 0.143)	Loss 3.4595e-01 (3.4595e-01)	Acc@1  86.72 ( 86.72)	Acc@5 100.00 (100.00)
Epoch: [19][ 10/391]	Time  0.161 ( 0.176)	Data  0.001 ( 0.014)	Loss 3.2910e-01 (3.6290e-01)	Acc@1  89.06 ( 88.14)	Acc@5 100.00 ( 99.72)
Epoch: [19][ 20/391]	Time  0.161 ( 0.169)	Data  0.001 ( 0.008)	Loss 3.7134e-01 (3.6717e-01)	Acc@1  86.72 ( 87.50)	Acc@5  99.22 ( 99.74)
Epoch: [19][ 30/391]	Time  0.160 ( 0.167)	Data  0.001 ( 0.006)	Loss 4.3579e-01 (3.6872e-01)	Acc@1  82.81 ( 87.27)	Acc@5  99.22 ( 99.57)
Epoch: [19][ 40/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.005)	Loss 3.1079e-01 (3.7060e-01)	Acc@1  90.62 ( 87.12)	Acc@5 100.00 ( 99.52)
Epoch: [19][ 50/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.004)	Loss 3.2788e-01 (3.7114e-01)	Acc@1  86.72 ( 87.01)	Acc@5 100.00 ( 99.57)
Epoch: [19][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 3.5156e-01 (3.7382e-01)	Acc@1  87.50 ( 86.81)	Acc@5 100.00 ( 99.55)
Epoch: [19][ 70/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 4.6558e-01 (3.7353e-01)	Acc@1  83.59 ( 86.86)	Acc@5  99.22 ( 99.52)
Epoch: [19][ 80/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.6108e-01 (3.6896e-01)	Acc@1  85.16 ( 86.97)	Acc@5 100.00 ( 99.54)
Epoch: [19][ 90/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.8794e-01 (3.6775e-01)	Acc@1  85.16 ( 86.98)	Acc@5 100.00 ( 99.53)
Epoch: [19][100/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 4.4287e-01 (3.7778e-01)	Acc@1  82.81 ( 86.64)	Acc@5  99.22 ( 99.50)
Epoch: [19][110/391]	Time  0.165 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.8770e-01 (3.7960e-01)	Acc@1  89.06 ( 86.65)	Acc@5  99.22 ( 99.48)
Epoch: [19][120/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.1235e-01 (3.8305e-01)	Acc@1  85.16 ( 86.53)	Acc@5 100.00 ( 99.48)
Epoch: [19][130/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.5068e-01 (3.8259e-01)	Acc@1  84.38 ( 86.62)	Acc@5 100.00 ( 99.48)
Epoch: [19][140/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.7974e-01 (3.8125e-01)	Acc@1  85.94 ( 86.68)	Acc@5  98.44 ( 99.47)
Epoch: [19][150/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.7295e-01 (3.8129e-01)	Acc@1  89.06 ( 86.68)	Acc@5 100.00 ( 99.48)
Epoch: [19][160/391]	Time  0.172 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.0210e-01 (3.8174e-01)	Acc@1  84.38 ( 86.63)	Acc@5  99.22 ( 99.49)
Epoch: [19][170/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.0991e-01 (3.8226e-01)	Acc@1  85.16 ( 86.60)	Acc@5  99.22 ( 99.50)
Epoch: [19][180/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.0830e-01 (3.8516e-01)	Acc@1  85.16 ( 86.55)	Acc@5  99.22 ( 99.49)
Epoch: [19][190/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.3604e-01 (3.8659e-01)	Acc@1  84.38 ( 86.53)	Acc@5  98.44 ( 99.48)
Epoch: [19][200/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.8467e-01 (3.8634e-01)	Acc@1  90.62 ( 86.52)	Acc@5  99.22 ( 99.49)
Epoch: [19][210/391]	Time  0.168 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.8818e-01 (3.8840e-01)	Acc@1  87.50 ( 86.43)	Acc@5  98.44 ( 99.50)
Epoch: [19][220/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.8394e-01 (3.8969e-01)	Acc@1  89.84 ( 86.42)	Acc@5 100.00 ( 99.50)
Epoch: [19][230/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.0771e-01 (3.8989e-01)	Acc@1  85.94 ( 86.42)	Acc@5 100.00 ( 99.51)
Epoch: [19][240/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.1074e-01 (3.8993e-01)	Acc@1  81.25 ( 86.43)	Acc@5  97.66 ( 99.50)
Epoch: [19][250/391]	Time  0.165 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.6704e-01 (3.9014e-01)	Acc@1  84.38 ( 86.43)	Acc@5 100.00 ( 99.50)
Epoch: [19][260/391]	Time  0.173 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.6533e-01 (3.9216e-01)	Acc@1  84.38 ( 86.37)	Acc@5 100.00 ( 99.50)
Epoch: [19][270/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.4932e-01 (3.9389e-01)	Acc@1  78.91 ( 86.31)	Acc@5 100.00 ( 99.50)
Epoch: [19][280/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.1763e-01 (3.9470e-01)	Acc@1  88.28 ( 86.30)	Acc@5  99.22 ( 99.48)
Epoch: [19][290/391]	Time  0.169 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.3276e-01 (3.9603e-01)	Acc@1  89.06 ( 86.29)	Acc@5  99.22 ( 99.48)
Epoch: [19][300/391]	Time  0.176 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.3350e-01 (3.9710e-01)	Acc@1  85.94 ( 86.24)	Acc@5  99.22 ( 99.48)
Epoch: [19][310/391]	Time  0.175 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.8647e-01 (3.9648e-01)	Acc@1  87.50 ( 86.28)	Acc@5  99.22 ( 99.47)
Epoch: [19][320/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.8242e-01 (3.9683e-01)	Acc@1  83.59 ( 86.29)	Acc@5  99.22 ( 99.47)
Epoch: [19][330/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.0449e-01 (3.9636e-01)	Acc@1  77.34 ( 86.31)	Acc@5  99.22 ( 99.48)
Epoch: [19][340/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.2920e-01 (3.9586e-01)	Acc@1  84.38 ( 86.32)	Acc@5 100.00 ( 99.49)
Epoch: [19][350/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.3237e-01 (3.9646e-01)	Acc@1  84.38 ( 86.26)	Acc@5  99.22 ( 99.50)
Epoch: [19][360/391]	Time  0.181 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.6738e-01 (3.9645e-01)	Acc@1  82.03 ( 86.26)	Acc@5  99.22 ( 99.50)
Epoch: [19][370/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.1797e-01 (3.9659e-01)	Acc@1  86.72 ( 86.27)	Acc@5 100.00 ( 99.50)
Epoch: [19][380/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.5488e-01 (3.9673e-01)	Acc@1  92.97 ( 86.28)	Acc@5 100.00 ( 99.50)
Epoch: [19][390/391]	Time  0.115 ( 0.162)	Data  0.001 ( 0.001)	Loss 3.9307e-01 (3.9659e-01)	Acc@1  85.00 ( 86.27)	Acc@5 100.00 ( 99.50)
## e[19] optimizer.zero_grad (sum) time: 0.5209438800811768
## e[19]       loss.backward (sum) time: 12.429892778396606
## e[19]      optimizer.step (sum) time: 25.711691856384277
## epoch[19] training(only) time: 63.503671646118164
# Switched to evaluate mode...
Test: [  0/100]	Time  0.181 ( 0.181)	Loss 3.6255e-01 (3.6255e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 5.2783e-01 (4.1096e-01)	Acc@1  87.00 ( 86.55)	Acc@5 100.00 ( 99.55)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 4.9512e-01 (4.0548e-01)	Acc@1  80.00 ( 86.33)	Acc@5 100.00 ( 99.29)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 4.1040e-01 (4.3583e-01)	Acc@1  89.00 ( 85.77)	Acc@5 100.00 ( 99.39)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 4.8096e-01 (4.3457e-01)	Acc@1  86.00 ( 85.88)	Acc@5  96.00 ( 99.27)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 3.9795e-01 (4.3202e-01)	Acc@1  87.00 ( 85.92)	Acc@5 100.00 ( 99.25)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 4.6289e-01 (4.3202e-01)	Acc@1  84.00 ( 85.85)	Acc@5 100.00 ( 99.31)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 3.9697e-01 (4.2732e-01)	Acc@1  87.00 ( 85.89)	Acc@5 100.00 ( 99.35)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 4.8584e-01 (4.2807e-01)	Acc@1  84.00 ( 85.86)	Acc@5 100.00 ( 99.40)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 3.7427e-01 (4.3129e-01)	Acc@1  85.00 ( 85.62)	Acc@5 100.00 ( 99.42)
 * Acc@1 85.600 Acc@5 99.430
### epoch[19] execution time: 68.4752836227417
EPOCH 20
# current learning rate: 0.1
# Switched to train mode...
Epoch: [20][  0/391]	Time  0.319 ( 0.319)	Data  0.143 ( 0.143)	Loss 2.7832e-01 (2.7832e-01)	Acc@1  90.62 ( 90.62)	Acc@5  99.22 ( 99.22)
Epoch: [20][ 10/391]	Time  0.160 ( 0.175)	Data  0.001 ( 0.014)	Loss 3.6011e-01 (3.4142e-01)	Acc@1  87.50 ( 88.21)	Acc@5  99.22 ( 99.72)
Epoch: [20][ 20/391]	Time  0.160 ( 0.168)	Data  0.001 ( 0.008)	Loss 3.4814e-01 (3.6594e-01)	Acc@1  90.62 ( 87.46)	Acc@5  99.22 ( 99.74)
Epoch: [20][ 30/391]	Time  0.160 ( 0.167)	Data  0.001 ( 0.006)	Loss 3.8989e-01 (3.6592e-01)	Acc@1  88.28 ( 87.10)	Acc@5 100.00 ( 99.82)
Epoch: [20][ 40/391]	Time  0.159 ( 0.166)	Data  0.001 ( 0.005)	Loss 3.9917e-01 (3.6439e-01)	Acc@1  89.84 ( 87.18)	Acc@5  98.44 ( 99.73)
Epoch: [20][ 50/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 4.2749e-01 (3.6474e-01)	Acc@1  85.16 ( 87.27)	Acc@5 100.00 ( 99.74)
Epoch: [20][ 60/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.003)	Loss 4.9927e-01 (3.6458e-01)	Acc@1  83.59 ( 87.36)	Acc@5  98.44 ( 99.73)
Epoch: [20][ 70/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 4.1675e-01 (3.6079e-01)	Acc@1  84.38 ( 87.51)	Acc@5  99.22 ( 99.70)
Epoch: [20][ 80/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 3.9331e-01 (3.6226e-01)	Acc@1  86.72 ( 87.48)	Acc@5  99.22 ( 99.69)
Epoch: [20][ 90/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 3.0005e-01 (3.6235e-01)	Acc@1  90.62 ( 87.48)	Acc@5 100.00 ( 99.67)
Epoch: [20][100/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.4229e-01 (3.6600e-01)	Acc@1  88.28 ( 87.42)	Acc@5  99.22 ( 99.63)
Epoch: [20][110/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.5581e-01 (3.7045e-01)	Acc@1  85.16 ( 87.37)	Acc@5 100.00 ( 99.60)
Epoch: [20][120/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.6099e-01 (3.6527e-01)	Acc@1  87.50 ( 87.51)	Acc@5 100.00 ( 99.61)
Epoch: [20][130/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.4646e-01 (3.6409e-01)	Acc@1  91.41 ( 87.56)	Acc@5 100.00 ( 99.61)
Epoch: [20][140/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.1919e-01 (3.6535e-01)	Acc@1  84.38 ( 87.54)	Acc@5  98.44 ( 99.62)
Epoch: [20][150/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.5142e-01 (3.6314e-01)	Acc@1  85.16 ( 87.61)	Acc@5 100.00 ( 99.63)
Epoch: [20][160/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.9111e-01 (3.6561e-01)	Acc@1  85.94 ( 87.54)	Acc@5  98.44 ( 99.60)
Epoch: [20][170/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.0884e-01 (3.6525e-01)	Acc@1  89.06 ( 87.54)	Acc@5  99.22 ( 99.57)
Epoch: [20][180/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.0146e-01 (3.6805e-01)	Acc@1  84.38 ( 87.41)	Acc@5  98.44 ( 99.56)
Epoch: [20][190/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.8638e-01 (3.6962e-01)	Acc@1  92.19 ( 87.36)	Acc@5  99.22 ( 99.56)
Epoch: [20][200/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.9810e-01 (3.7033e-01)	Acc@1  92.19 ( 87.31)	Acc@5 100.00 ( 99.55)
Epoch: [20][210/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.0698e-01 (3.7244e-01)	Acc@1  88.28 ( 87.24)	Acc@5  98.44 ( 99.54)
Epoch: [20][220/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.8804e-01 (3.7486e-01)	Acc@1  81.25 ( 87.17)	Acc@5  99.22 ( 99.54)
Epoch: [20][230/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.8062e-01 (3.7444e-01)	Acc@1  87.50 ( 87.20)	Acc@5 100.00 ( 99.54)
Epoch: [20][240/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.6025e-01 (3.7556e-01)	Acc@1  90.62 ( 87.17)	Acc@5 100.00 ( 99.53)
Epoch: [20][250/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.0835e-01 (3.7606e-01)	Acc@1  89.84 ( 87.16)	Acc@5 100.00 ( 99.53)
Epoch: [20][260/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.6035e-01 (3.7497e-01)	Acc@1  89.84 ( 87.20)	Acc@5  99.22 ( 99.54)
Epoch: [20][270/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.1318e-01 (3.7547e-01)	Acc@1  82.03 ( 87.16)	Acc@5  99.22 ( 99.55)
Epoch: [20][280/391]	Time  0.171 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.1895e-01 (3.7490e-01)	Acc@1  85.94 ( 87.17)	Acc@5 100.00 ( 99.55)
Epoch: [20][290/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.1812e-01 (3.7435e-01)	Acc@1  87.50 ( 87.18)	Acc@5 100.00 ( 99.55)
Epoch: [20][300/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.7378e-01 (3.7351e-01)	Acc@1  83.59 ( 87.19)	Acc@5 100.00 ( 99.55)
Epoch: [20][310/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.9810e-01 (3.7379e-01)	Acc@1  89.06 ( 87.17)	Acc@5  99.22 ( 99.55)
Epoch: [20][320/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.0176e-01 (3.7338e-01)	Acc@1  89.06 ( 87.20)	Acc@5 100.00 ( 99.56)
Epoch: [20][330/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.6255e-01 (3.7192e-01)	Acc@1  86.72 ( 87.25)	Acc@5  99.22 ( 99.55)
Epoch: [20][340/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.6401e-01 (3.7246e-01)	Acc@1  91.41 ( 87.22)	Acc@5  99.22 ( 99.56)
Epoch: [20][350/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.0471e-01 (3.7167e-01)	Acc@1  93.75 ( 87.24)	Acc@5 100.00 ( 99.57)
Epoch: [20][360/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.8711e-01 (3.7164e-01)	Acc@1  90.62 ( 87.23)	Acc@5 100.00 ( 99.56)
Epoch: [20][370/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.7368e-01 (3.7149e-01)	Acc@1  88.28 ( 87.23)	Acc@5 100.00 ( 99.56)
Epoch: [20][380/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.0879e-01 (3.7199e-01)	Acc@1  82.03 ( 87.21)	Acc@5  99.22 ( 99.56)
Epoch: [20][390/391]	Time  0.118 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.0825e-01 (3.7162e-01)	Acc@1  93.75 ( 87.23)	Acc@5 100.00 ( 99.56)
## e[20] optimizer.zero_grad (sum) time: 0.5271668434143066
## e[20]       loss.backward (sum) time: 12.453136444091797
## e[20]      optimizer.step (sum) time: 25.70223569869995
## epoch[20] training(only) time: 63.47688269615173
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 4.3164e-01 (4.3164e-01)	Acc@1  84.00 ( 84.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.048 ( 0.061)	Loss 4.2822e-01 (4.5634e-01)	Acc@1  87.00 ( 85.27)	Acc@5 100.00 ( 99.55)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 5.8301e-01 (4.6623e-01)	Acc@1  80.00 ( 84.90)	Acc@5 100.00 ( 99.38)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 5.7959e-01 (4.7201e-01)	Acc@1  82.00 ( 85.00)	Acc@5 100.00 ( 99.39)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 4.4727e-01 (4.7139e-01)	Acc@1  87.00 ( 84.80)	Acc@5  99.00 ( 99.41)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 4.0869e-01 (4.6258e-01)	Acc@1  88.00 ( 85.16)	Acc@5  99.00 ( 99.43)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 3.9258e-01 (4.5816e-01)	Acc@1  84.00 ( 85.18)	Acc@5  99.00 ( 99.44)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 5.1318e-01 (4.5836e-01)	Acc@1  83.00 ( 85.11)	Acc@5  99.00 ( 99.44)
Test: [ 80/100]	Time  0.057 ( 0.049)	Loss 4.4067e-01 (4.6172e-01)	Acc@1  86.00 ( 85.04)	Acc@5 100.00 ( 99.48)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 4.1797e-01 (4.6573e-01)	Acc@1  86.00 ( 84.90)	Acc@5 100.00 ( 99.48)
 * Acc@1 84.830 Acc@5 99.500
### epoch[20] execution time: 68.43679904937744
EPOCH 21
# current learning rate: 0.1
# Switched to train mode...
Epoch: [21][  0/391]	Time  0.317 ( 0.317)	Data  0.143 ( 0.143)	Loss 2.5415e-01 (2.5415e-01)	Acc@1  90.62 ( 90.62)	Acc@5 100.00 (100.00)
Epoch: [21][ 10/391]	Time  0.162 ( 0.177)	Data  0.001 ( 0.014)	Loss 2.2961e-01 (3.5084e-01)	Acc@1  90.62 ( 87.93)	Acc@5 100.00 ( 99.50)
Epoch: [21][ 20/391]	Time  0.161 ( 0.169)	Data  0.001 ( 0.008)	Loss 5.4150e-01 (3.6302e-01)	Acc@1  83.59 ( 87.39)	Acc@5  99.22 ( 99.67)
Epoch: [21][ 30/391]	Time  0.162 ( 0.166)	Data  0.001 ( 0.006)	Loss 4.4116e-01 (3.6280e-01)	Acc@1  82.81 ( 87.27)	Acc@5 100.00 ( 99.72)
Epoch: [21][ 40/391]	Time  0.173 ( 0.166)	Data  0.001 ( 0.005)	Loss 2.5293e-01 (3.5288e-01)	Acc@1  91.41 ( 87.67)	Acc@5 100.00 ( 99.70)
Epoch: [21][ 50/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.8738e-01 (3.4627e-01)	Acc@1  94.53 ( 87.85)	Acc@5 100.00 ( 99.71)
Epoch: [21][ 60/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.1692e-01 (3.4469e-01)	Acc@1  94.53 ( 88.06)	Acc@5 100.00 ( 99.71)
Epoch: [21][ 70/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 3.2959e-01 (3.4424e-01)	Acc@1  87.50 ( 88.15)	Acc@5 100.00 ( 99.71)
Epoch: [21][ 80/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.2351e-01 (3.4117e-01)	Acc@1  92.19 ( 88.18)	Acc@5 100.00 ( 99.73)
Epoch: [21][ 90/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.2300e-01 (3.4210e-01)	Acc@1  89.84 ( 88.20)	Acc@5 100.00 ( 99.74)
Epoch: [21][100/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.8198e-01 (3.4571e-01)	Acc@1  85.94 ( 88.11)	Acc@5 100.00 ( 99.74)
Epoch: [21][110/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.3066e-01 (3.4559e-01)	Acc@1  85.16 ( 88.01)	Acc@5  99.22 ( 99.74)
Epoch: [21][120/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.2522e-01 (3.4562e-01)	Acc@1  93.75 ( 88.03)	Acc@5 100.00 ( 99.75)
Epoch: [21][130/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.6025e-01 (3.4560e-01)	Acc@1  90.62 ( 88.06)	Acc@5 100.00 ( 99.73)
Epoch: [21][140/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.0005e-01 (3.4545e-01)	Acc@1  85.16 ( 87.99)	Acc@5 100.00 ( 99.73)
Epoch: [21][150/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.5425e-01 (3.4441e-01)	Acc@1  87.50 ( 88.06)	Acc@5 100.00 ( 99.72)
Epoch: [21][160/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.9736e-01 (3.4502e-01)	Acc@1  89.84 ( 88.03)	Acc@5 100.00 ( 99.71)
Epoch: [21][170/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.5171e-01 (3.4468e-01)	Acc@1  93.75 ( 88.06)	Acc@5 100.00 ( 99.70)
Epoch: [21][180/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.8638e-01 (3.4451e-01)	Acc@1  88.28 ( 88.07)	Acc@5 100.00 ( 99.71)
Epoch: [21][190/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.9614e-01 (3.4606e-01)	Acc@1  87.50 ( 87.97)	Acc@5 100.00 ( 99.71)
Epoch: [21][200/391]	Time  0.171 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.8857e-01 (3.4533e-01)	Acc@1  89.84 ( 87.97)	Acc@5 100.00 ( 99.69)
Epoch: [21][210/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.4385e-01 (3.4516e-01)	Acc@1  84.38 ( 88.02)	Acc@5  99.22 ( 99.68)
Epoch: [21][220/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.5269e-01 (3.4595e-01)	Acc@1  89.84 ( 88.02)	Acc@5 100.00 ( 99.68)
Epoch: [21][230/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.1724e-01 (3.4823e-01)	Acc@1  85.16 ( 87.96)	Acc@5  99.22 ( 99.66)
Epoch: [21][240/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.1006e-01 (3.4873e-01)	Acc@1  89.06 ( 87.93)	Acc@5 100.00 ( 99.66)
Epoch: [21][250/391]	Time  0.179 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.2764e-01 (3.4872e-01)	Acc@1  88.28 ( 87.90)	Acc@5 100.00 ( 99.66)
Epoch: [21][260/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.4878e-01 (3.4867e-01)	Acc@1  92.97 ( 87.91)	Acc@5 100.00 ( 99.66)
Epoch: [21][270/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.6597e-01 (3.4892e-01)	Acc@1  86.72 ( 87.94)	Acc@5  99.22 ( 99.66)
Epoch: [21][280/391]	Time  0.170 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.2212e-01 (3.5035e-01)	Acc@1  83.59 ( 87.91)	Acc@5 100.00 ( 99.65)
Epoch: [21][290/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.3999e-01 (3.5201e-01)	Acc@1  91.41 ( 87.84)	Acc@5 100.00 ( 99.64)
Epoch: [21][300/391]	Time  0.171 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.7524e-01 (3.5276e-01)	Acc@1  87.50 ( 87.82)	Acc@5  99.22 ( 99.64)
Epoch: [21][310/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.4800e-01 (3.5423e-01)	Acc@1  83.59 ( 87.80)	Acc@5  98.44 ( 99.63)
Epoch: [21][320/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.1152e-01 (3.5388e-01)	Acc@1  91.41 ( 87.80)	Acc@5  99.22 ( 99.62)
Epoch: [21][330/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.001)	Loss 3.6157e-01 (3.5274e-01)	Acc@1  84.38 ( 87.84)	Acc@5 100.00 ( 99.63)
Epoch: [21][340/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.3926e-01 (3.5313e-01)	Acc@1  91.41 ( 87.83)	Acc@5 100.00 ( 99.63)
Epoch: [21][350/391]	Time  0.172 ( 0.162)	Data  0.001 ( 0.001)	Loss 4.2896e-01 (3.5393e-01)	Acc@1  81.25 ( 87.80)	Acc@5 100.00 ( 99.63)
Epoch: [21][360/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 4.2798e-01 (3.5386e-01)	Acc@1  87.50 ( 87.79)	Acc@5  99.22 ( 99.63)
Epoch: [21][370/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.8223e-01 (3.5272e-01)	Acc@1  89.84 ( 87.82)	Acc@5 100.00 ( 99.64)
Epoch: [21][380/391]	Time  0.179 ( 0.162)	Data  0.002 ( 0.001)	Loss 3.9429e-01 (3.5335e-01)	Acc@1  84.38 ( 87.77)	Acc@5  99.22 ( 99.64)
Epoch: [21][390/391]	Time  0.116 ( 0.162)	Data  0.001 ( 0.001)	Loss 3.6499e-01 (3.5336e-01)	Acc@1  86.25 ( 87.80)	Acc@5 100.00 ( 99.64)
## e[21] optimizer.zero_grad (sum) time: 0.5246434211730957
## e[21]       loss.backward (sum) time: 12.395877599716187
## e[21]      optimizer.step (sum) time: 25.76655602455139
## epoch[21] training(only) time: 63.5217125415802
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 3.6719e-01 (3.6719e-01)	Acc@1  86.00 ( 86.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.048 ( 0.061)	Loss 4.4336e-01 (4.6222e-01)	Acc@1  85.00 ( 84.09)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.048 ( 0.055)	Loss 5.3125e-01 (4.7981e-01)	Acc@1  85.00 ( 84.24)	Acc@5 100.00 ( 99.52)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 4.9658e-01 (4.9668e-01)	Acc@1  81.00 ( 83.74)	Acc@5 100.00 ( 99.45)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 4.5508e-01 (4.9110e-01)	Acc@1  86.00 ( 83.80)	Acc@5  98.00 ( 99.37)
Test: [ 50/100]	Time  0.048 ( 0.051)	Loss 1.7102e-01 (4.7813e-01)	Acc@1  94.00 ( 84.37)	Acc@5 100.00 ( 99.35)
Test: [ 60/100]	Time  0.048 ( 0.050)	Loss 4.3921e-01 (4.7812e-01)	Acc@1  86.00 ( 84.30)	Acc@5 100.00 ( 99.38)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 5.9277e-01 (4.8000e-01)	Acc@1  82.00 ( 84.45)	Acc@5 100.00 ( 99.38)
Test: [ 80/100]	Time  0.047 ( 0.050)	Loss 4.8096e-01 (4.7955e-01)	Acc@1  82.00 ( 84.44)	Acc@5  99.00 ( 99.41)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 5.4248e-01 (4.8280e-01)	Acc@1  81.00 ( 84.30)	Acc@5 100.00 ( 99.42)
 * Acc@1 84.400 Acc@5 99.460
### epoch[21] execution time: 68.52505326271057
EPOCH 22
# current learning rate: 0.1
# Switched to train mode...
Epoch: [22][  0/391]	Time  0.328 ( 0.328)	Data  0.156 ( 0.156)	Loss 2.6465e-01 (2.6465e-01)	Acc@1  89.84 ( 89.84)	Acc@5 100.00 (100.00)
Epoch: [22][ 10/391]	Time  0.160 ( 0.176)	Data  0.001 ( 0.015)	Loss 3.1885e-01 (3.2758e-01)	Acc@1  92.19 ( 88.99)	Acc@5 100.00 ( 99.79)
Epoch: [22][ 20/391]	Time  0.160 ( 0.169)	Data  0.001 ( 0.008)	Loss 3.5474e-01 (3.1749e-01)	Acc@1  85.94 ( 88.91)	Acc@5 100.00 ( 99.81)
Epoch: [22][ 30/391]	Time  0.162 ( 0.167)	Data  0.001 ( 0.006)	Loss 3.1592e-01 (3.2654e-01)	Acc@1  88.28 ( 88.33)	Acc@5  99.22 ( 99.77)
Epoch: [22][ 40/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.005)	Loss 2.4829e-01 (3.2910e-01)	Acc@1  92.19 ( 88.21)	Acc@5 100.00 ( 99.75)
Epoch: [22][ 50/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.004)	Loss 3.1665e-01 (3.2727e-01)	Acc@1  89.06 ( 88.36)	Acc@5 100.00 ( 99.75)
Epoch: [22][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 4.0894e-01 (3.3193e-01)	Acc@1  87.50 ( 88.27)	Acc@5 100.00 ( 99.65)
Epoch: [22][ 70/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.8271e-01 (3.2960e-01)	Acc@1  92.19 ( 88.44)	Acc@5  99.22 ( 99.66)
Epoch: [22][ 80/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.6270e-01 (3.3167e-01)	Acc@1  89.84 ( 88.43)	Acc@5 100.00 ( 99.67)
Epoch: [22][ 90/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.9126e-01 (3.3585e-01)	Acc@1  89.06 ( 88.29)	Acc@5  99.22 ( 99.63)
Epoch: [22][100/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 3.9087e-01 (3.3873e-01)	Acc@1  85.16 ( 88.17)	Acc@5  99.22 ( 99.62)
Epoch: [22][110/391]	Time  0.163 ( 0.164)	Data  0.001 ( 0.002)	Loss 2.9639e-01 (3.3659e-01)	Acc@1  85.16 ( 88.23)	Acc@5  99.22 ( 99.62)
Epoch: [22][120/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.5620e-01 (3.3726e-01)	Acc@1  87.50 ( 88.19)	Acc@5 100.00 ( 99.63)
Epoch: [22][130/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.4507e-01 (3.3568e-01)	Acc@1  82.81 ( 88.25)	Acc@5 100.00 ( 99.62)
Epoch: [22][140/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.8599e-01 (3.3630e-01)	Acc@1  88.28 ( 88.19)	Acc@5 100.00 ( 99.62)
Epoch: [22][150/391]	Time  0.177 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.0835e-01 (3.3771e-01)	Acc@1  89.84 ( 88.19)	Acc@5  99.22 ( 99.64)
Epoch: [22][160/391]	Time  0.176 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.9673e-01 (3.3678e-01)	Acc@1  89.06 ( 88.16)	Acc@5  99.22 ( 99.66)
Epoch: [22][170/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.1123e-01 (3.3942e-01)	Acc@1  84.38 ( 88.11)	Acc@5 100.00 ( 99.67)
Epoch: [22][180/391]	Time  0.167 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.5840e-01 (3.4114e-01)	Acc@1  89.06 ( 88.07)	Acc@5  99.22 ( 99.66)
Epoch: [22][190/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.0967e-01 (3.4328e-01)	Acc@1  85.16 ( 87.98)	Acc@5 100.00 ( 99.65)
Epoch: [22][200/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.0635e-01 (3.4519e-01)	Acc@1  85.16 ( 87.94)	Acc@5  99.22 ( 99.63)
Epoch: [22][210/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.7222e-01 (3.4528e-01)	Acc@1  90.62 ( 87.92)	Acc@5  99.22 ( 99.63)
Epoch: [22][220/391]	Time  0.167 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.4717e-01 (3.4486e-01)	Acc@1  89.06 ( 87.92)	Acc@5 100.00 ( 99.63)
Epoch: [22][230/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.4194e-01 (3.4363e-01)	Acc@1  91.41 ( 88.00)	Acc@5 100.00 ( 99.63)
Epoch: [22][240/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.5083e-01 (3.4221e-01)	Acc@1  86.72 ( 88.05)	Acc@5 100.00 ( 99.63)
Epoch: [22][250/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.0898e-01 (3.4100e-01)	Acc@1  95.31 ( 88.12)	Acc@5  99.22 ( 99.61)
Epoch: [22][260/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.3179e-01 (3.4057e-01)	Acc@1  88.28 ( 88.11)	Acc@5 100.00 ( 99.61)
Epoch: [22][270/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.6758e-01 (3.4058e-01)	Acc@1  91.41 ( 88.12)	Acc@5 100.00 ( 99.61)
Epoch: [22][280/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.3401e-01 (3.4015e-01)	Acc@1  92.97 ( 88.12)	Acc@5 100.00 ( 99.62)
Epoch: [22][290/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.1030e-01 (3.3925e-01)	Acc@1  92.97 ( 88.15)	Acc@5  99.22 ( 99.63)
Epoch: [22][300/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.7661e-01 (3.3837e-01)	Acc@1  89.84 ( 88.18)	Acc@5 100.00 ( 99.63)
Epoch: [22][310/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.1885e-01 (3.3843e-01)	Acc@1  88.28 ( 88.17)	Acc@5 100.00 ( 99.64)
Epoch: [22][320/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.0981e-01 (3.3703e-01)	Acc@1  89.84 ( 88.20)	Acc@5 100.00 ( 99.64)
Epoch: [22][330/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.3228e-01 (3.3644e-01)	Acc@1  90.62 ( 88.23)	Acc@5  99.22 ( 99.65)
Epoch: [22][340/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.5635e-01 (3.3609e-01)	Acc@1  91.41 ( 88.26)	Acc@5  98.44 ( 99.64)
Epoch: [22][350/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.9722e-01 (3.3659e-01)	Acc@1  88.28 ( 88.27)	Acc@5  99.22 ( 99.64)
Epoch: [22][360/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.5107e-01 (3.3734e-01)	Acc@1  88.28 ( 88.22)	Acc@5 100.00 ( 99.64)
Epoch: [22][370/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.9932e-01 (3.3696e-01)	Acc@1  90.62 ( 88.23)	Acc@5 100.00 ( 99.64)
Epoch: [22][380/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.0933e-01 (3.3717e-01)	Acc@1  90.62 ( 88.20)	Acc@5  99.22 ( 99.64)
Epoch: [22][390/391]	Time  0.116 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.7231e-01 (3.3800e-01)	Acc@1  90.00 ( 88.20)	Acc@5 100.00 ( 99.64)
## e[22] optimizer.zero_grad (sum) time: 0.5198233127593994
## e[22]       loss.backward (sum) time: 12.415399551391602
## e[22]      optimizer.step (sum) time: 25.76516819000244
## epoch[22] training(only) time: 63.57869291305542
# Switched to evaluate mode...
Test: [  0/100]	Time  0.181 ( 0.181)	Loss 3.5718e-01 (3.5718e-01)	Acc@1  88.00 ( 88.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.048 ( 0.061)	Loss 3.9697e-01 (4.4624e-01)	Acc@1  87.00 ( 84.82)	Acc@5 100.00 ( 99.55)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 5.1074e-01 (4.4212e-01)	Acc@1  83.00 ( 85.14)	Acc@5 100.00 ( 99.38)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 4.1089e-01 (4.5469e-01)	Acc@1  86.00 ( 84.94)	Acc@5 100.00 ( 99.39)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 5.3125e-01 (4.4633e-01)	Acc@1  83.00 ( 85.34)	Acc@5  99.00 ( 99.41)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 3.8696e-01 (4.4405e-01)	Acc@1  86.00 ( 85.41)	Acc@5  99.00 ( 99.35)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 3.8379e-01 (4.3869e-01)	Acc@1  86.00 ( 85.66)	Acc@5 100.00 ( 99.36)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 4.1113e-01 (4.3841e-01)	Acc@1  88.00 ( 85.73)	Acc@5 100.00 ( 99.41)
Test: [ 80/100]	Time  0.048 ( 0.049)	Loss 3.6035e-01 (4.3725e-01)	Acc@1  87.00 ( 85.84)	Acc@5 100.00 ( 99.41)
Test: [ 90/100]	Time  0.056 ( 0.049)	Loss 4.9463e-01 (4.3739e-01)	Acc@1  82.00 ( 85.71)	Acc@5 100.00 ( 99.45)
 * Acc@1 85.690 Acc@5 99.470
### epoch[22] execution time: 68.58259105682373
EPOCH 23
# current learning rate: 0.1
# Switched to train mode...
Epoch: [23][  0/391]	Time  0.333 ( 0.333)	Data  0.154 ( 0.154)	Loss 2.5244e-01 (2.5244e-01)	Acc@1  92.19 ( 92.19)	Acc@5 100.00 (100.00)
Epoch: [23][ 10/391]	Time  0.161 ( 0.178)	Data  0.001 ( 0.015)	Loss 2.9297e-01 (3.3221e-01)	Acc@1  86.72 ( 88.00)	Acc@5 100.00 ( 99.79)
Epoch: [23][ 20/391]	Time  0.159 ( 0.170)	Data  0.001 ( 0.008)	Loss 3.5962e-01 (3.3488e-01)	Acc@1  89.06 ( 88.54)	Acc@5  99.22 ( 99.78)
Epoch: [23][ 30/391]	Time  0.161 ( 0.167)	Data  0.001 ( 0.006)	Loss 3.3960e-01 (3.3153e-01)	Acc@1  86.72 ( 88.28)	Acc@5 100.00 ( 99.80)
Epoch: [23][ 40/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.005)	Loss 3.4399e-01 (3.3261e-01)	Acc@1  89.84 ( 88.45)	Acc@5  99.22 ( 99.73)
Epoch: [23][ 50/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 3.0151e-01 (3.3540e-01)	Acc@1  89.06 ( 88.34)	Acc@5  98.44 ( 99.71)
Epoch: [23][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 4.2896e-01 (3.3477e-01)	Acc@1  85.94 ( 88.54)	Acc@5 100.00 ( 99.68)
Epoch: [23][ 70/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 4.9365e-01 (3.3230e-01)	Acc@1  83.59 ( 88.48)	Acc@5  99.22 ( 99.67)
Epoch: [23][ 80/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.8687e-01 (3.3097e-01)	Acc@1  92.19 ( 88.52)	Acc@5 100.00 ( 99.64)
Epoch: [23][ 90/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.6646e-01 (3.3109e-01)	Acc@1  86.72 ( 88.58)	Acc@5  98.44 ( 99.64)
Epoch: [23][100/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.6709e-01 (3.3043e-01)	Acc@1  90.62 ( 88.56)	Acc@5 100.00 ( 99.64)
Epoch: [23][110/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.5317e-01 (3.2732e-01)	Acc@1  91.41 ( 88.58)	Acc@5 100.00 ( 99.66)
Epoch: [23][120/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.3010e-01 (3.2360e-01)	Acc@1  89.84 ( 88.71)	Acc@5 100.00 ( 99.66)
Epoch: [23][130/391]	Time  0.165 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.0293e-01 (3.2159e-01)	Acc@1  82.03 ( 88.73)	Acc@5 100.00 ( 99.67)
Epoch: [23][140/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.7158e-01 (3.2215e-01)	Acc@1  85.94 ( 88.70)	Acc@5 100.00 ( 99.66)
Epoch: [23][150/391]	Time  0.167 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.8159e-01 (3.2075e-01)	Acc@1  86.72 ( 88.71)	Acc@5  99.22 ( 99.67)
Epoch: [23][160/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.2007e-01 (3.2061e-01)	Acc@1  85.94 ( 88.72)	Acc@5 100.00 ( 99.67)
Epoch: [23][170/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.0591e-01 (3.1904e-01)	Acc@1  88.28 ( 88.76)	Acc@5 100.00 ( 99.69)
Epoch: [23][180/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.6440e-01 (3.2027e-01)	Acc@1  88.28 ( 88.74)	Acc@5 100.00 ( 99.68)
Epoch: [23][190/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.8804e-01 (3.2234e-01)	Acc@1  86.72 ( 88.71)	Acc@5 100.00 ( 99.66)
Epoch: [23][200/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.3203e-01 (3.2253e-01)	Acc@1  90.62 ( 88.69)	Acc@5 100.00 ( 99.65)
Epoch: [23][210/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5710e-01 (3.2367e-01)	Acc@1  94.53 ( 88.64)	Acc@5 100.00 ( 99.66)
Epoch: [23][220/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.8208e-01 (3.2599e-01)	Acc@1  87.50 ( 88.55)	Acc@5 100.00 ( 99.65)
Epoch: [23][230/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.4253e-01 (3.2829e-01)	Acc@1  87.50 ( 88.50)	Acc@5  99.22 ( 99.64)
Epoch: [23][240/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.1367e-01 (3.3038e-01)	Acc@1  85.16 ( 88.44)	Acc@5 100.00 ( 99.64)
Epoch: [23][250/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.4702e-01 (3.3089e-01)	Acc@1  80.47 ( 88.44)	Acc@5  99.22 ( 99.64)
Epoch: [23][260/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.5605e-01 (3.3022e-01)	Acc@1  84.38 ( 88.45)	Acc@5  99.22 ( 99.64)
Epoch: [23][270/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.6011e-01 (3.2949e-01)	Acc@1  86.72 ( 88.48)	Acc@5 100.00 ( 99.65)
Epoch: [23][280/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.0444e-01 (3.2921e-01)	Acc@1  89.84 ( 88.48)	Acc@5 100.00 ( 99.66)
Epoch: [23][290/391]	Time  0.167 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.7964e-01 (3.3152e-01)	Acc@1  84.38 ( 88.39)	Acc@5 100.00 ( 99.65)
Epoch: [23][300/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.6167e-01 (3.3115e-01)	Acc@1  83.59 ( 88.39)	Acc@5  99.22 ( 99.65)
Epoch: [23][310/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.6807e-01 (3.3202e-01)	Acc@1  92.19 ( 88.36)	Acc@5 100.00 ( 99.65)
Epoch: [23][320/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.3799e-01 (3.3217e-01)	Acc@1  84.38 ( 88.36)	Acc@5  99.22 ( 99.64)
Epoch: [23][330/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.0322e-01 (3.3298e-01)	Acc@1  88.28 ( 88.33)	Acc@5 100.00 ( 99.65)
Epoch: [23][340/391]	Time  0.168 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.0029e-01 (3.3196e-01)	Acc@1  90.62 ( 88.39)	Acc@5  99.22 ( 99.65)
Epoch: [23][350/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.3447e-01 (3.3165e-01)	Acc@1  90.62 ( 88.41)	Acc@5  97.66 ( 99.65)
Epoch: [23][360/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.6548e-01 (3.3090e-01)	Acc@1  90.62 ( 88.44)	Acc@5  99.22 ( 99.65)
Epoch: [23][370/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.6230e-01 (3.3084e-01)	Acc@1  88.28 ( 88.44)	Acc@5 100.00 ( 99.65)
Epoch: [23][380/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.6001e-01 (3.3053e-01)	Acc@1  90.62 ( 88.45)	Acc@5 100.00 ( 99.65)
Epoch: [23][390/391]	Time  0.130 ( 0.162)	Data  0.001 ( 0.001)	Loss 4.7070e-01 (3.3067e-01)	Acc@1  85.00 ( 88.45)	Acc@5 100.00 ( 99.65)
## e[23] optimizer.zero_grad (sum) time: 0.5221896171569824
## e[23]       loss.backward (sum) time: 12.462562561035156
## e[23]      optimizer.step (sum) time: 25.714293479919434
## epoch[23] training(only) time: 63.486772537231445
# Switched to evaluate mode...
Test: [  0/100]	Time  0.178 ( 0.178)	Loss 3.3496e-01 (3.3496e-01)	Acc@1  89.00 ( 89.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.048 ( 0.060)	Loss 4.1846e-01 (4.5297e-01)	Acc@1  88.00 ( 85.73)	Acc@5 100.00 ( 99.18)
Test: [ 20/100]	Time  0.061 ( 0.055)	Loss 4.2188e-01 (4.4160e-01)	Acc@1  89.00 ( 85.90)	Acc@5  99.00 ( 99.29)
Test: [ 30/100]	Time  0.048 ( 0.053)	Loss 3.8257e-01 (4.5016e-01)	Acc@1  89.00 ( 85.97)	Acc@5 100.00 ( 99.35)
Test: [ 40/100]	Time  0.048 ( 0.051)	Loss 3.3643e-01 (4.4300e-01)	Acc@1  89.00 ( 86.05)	Acc@5 100.00 ( 99.37)
Test: [ 50/100]	Time  0.048 ( 0.051)	Loss 4.2334e-01 (4.3708e-01)	Acc@1  85.00 ( 86.24)	Acc@5 100.00 ( 99.31)
Test: [ 60/100]	Time  0.048 ( 0.051)	Loss 4.4141e-01 (4.3275e-01)	Acc@1  81.00 ( 86.15)	Acc@5 100.00 ( 99.34)
Test: [ 70/100]	Time  0.048 ( 0.050)	Loss 3.6865e-01 (4.3128e-01)	Acc@1  90.00 ( 86.31)	Acc@5 100.00 ( 99.35)
Test: [ 80/100]	Time  0.048 ( 0.050)	Loss 1.9434e-01 (4.2774e-01)	Acc@1  91.00 ( 86.30)	Acc@5  99.00 ( 99.38)
Test: [ 90/100]	Time  0.048 ( 0.050)	Loss 4.7827e-01 (4.2806e-01)	Acc@1  86.00 ( 86.23)	Acc@5 100.00 ( 99.41)
 * Acc@1 86.350 Acc@5 99.440
### epoch[23] execution time: 68.57199597358704
EPOCH 24
# current learning rate: 0.1
# Switched to train mode...
Epoch: [24][  0/391]	Time  0.322 ( 0.322)	Data  0.148 ( 0.148)	Loss 2.9004e-01 (2.9004e-01)	Acc@1  91.41 ( 91.41)	Acc@5  99.22 ( 99.22)
Epoch: [24][ 10/391]	Time  0.160 ( 0.176)	Data  0.001 ( 0.015)	Loss 2.8125e-01 (2.8781e-01)	Acc@1  89.84 ( 89.99)	Acc@5 100.00 ( 99.43)
Epoch: [24][ 20/391]	Time  0.168 ( 0.169)	Data  0.001 ( 0.008)	Loss 2.6978e-01 (2.9947e-01)	Acc@1  90.62 ( 89.21)	Acc@5 100.00 ( 99.55)
Epoch: [24][ 30/391]	Time  0.161 ( 0.166)	Data  0.001 ( 0.006)	Loss 2.6587e-01 (2.9891e-01)	Acc@1  90.62 ( 89.09)	Acc@5 100.00 ( 99.70)
Epoch: [24][ 40/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.005)	Loss 3.3130e-01 (3.1295e-01)	Acc@1  85.94 ( 88.66)	Acc@5 100.00 ( 99.60)
Epoch: [24][ 50/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 3.9648e-01 (3.1525e-01)	Acc@1  83.59 ( 88.50)	Acc@5  99.22 ( 99.63)
Epoch: [24][ 60/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.004)	Loss 2.8345e-01 (3.1559e-01)	Acc@1  89.84 ( 88.52)	Acc@5  99.22 ( 99.67)
Epoch: [24][ 70/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.3279e-01 (3.0766e-01)	Acc@1  90.62 ( 88.85)	Acc@5 100.00 ( 99.68)
Epoch: [24][ 80/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.8818e-01 (3.0837e-01)	Acc@1  85.94 ( 88.93)	Acc@5  99.22 ( 99.67)
Epoch: [24][ 90/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.1519e-01 (3.1239e-01)	Acc@1  85.94 ( 88.80)	Acc@5 100.00 ( 99.67)
Epoch: [24][100/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.1494e-01 (3.1163e-01)	Acc@1  89.06 ( 88.91)	Acc@5 100.00 ( 99.65)
Epoch: [24][110/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.0178e-01 (3.1242e-01)	Acc@1  90.62 ( 88.82)	Acc@5 100.00 ( 99.65)
Epoch: [24][120/391]	Time  0.163 ( 0.163)	Data  0.002 ( 0.002)	Loss 4.2969e-01 (3.1436e-01)	Acc@1  85.94 ( 88.70)	Acc@5  99.22 ( 99.64)
Epoch: [24][130/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.2021e-01 (3.0954e-01)	Acc@1  92.97 ( 88.90)	Acc@5 100.00 ( 99.66)
Epoch: [24][140/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.3374e-01 (3.1048e-01)	Acc@1  91.41 ( 88.90)	Acc@5 100.00 ( 99.67)
Epoch: [24][150/391]	Time  0.171 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.5586e-01 (3.0996e-01)	Acc@1  92.19 ( 88.95)	Acc@5 100.00 ( 99.67)
Epoch: [24][160/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.8257e-01 (3.1008e-01)	Acc@1  84.38 ( 88.98)	Acc@5  99.22 ( 99.67)
Epoch: [24][170/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.8955e-01 (3.0860e-01)	Acc@1  89.84 ( 89.06)	Acc@5 100.00 ( 99.68)
Epoch: [24][180/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.1772e-01 (3.0751e-01)	Acc@1  85.94 ( 89.16)	Acc@5 100.00 ( 99.68)
Epoch: [24][190/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.8613e-01 (3.0878e-01)	Acc@1  89.06 ( 89.14)	Acc@5  99.22 ( 99.67)
Epoch: [24][200/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.6450e-01 (3.1033e-01)	Acc@1  86.72 ( 89.09)	Acc@5  98.44 ( 99.66)
Epoch: [24][210/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.8613e-01 (3.1210e-01)	Acc@1  91.41 ( 89.03)	Acc@5 100.00 ( 99.67)
Epoch: [24][220/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.4326e-01 (3.1269e-01)	Acc@1  84.38 ( 89.00)	Acc@5  99.22 ( 99.67)
Epoch: [24][230/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.9248e-01 (3.1242e-01)	Acc@1  89.06 ( 89.03)	Acc@5 100.00 ( 99.67)
Epoch: [24][240/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.8735e-01 (3.1375e-01)	Acc@1  90.62 ( 89.00)	Acc@5  99.22 ( 99.67)
Epoch: [24][250/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.1870e-01 (3.1406e-01)	Acc@1  82.03 ( 88.98)	Acc@5 100.00 ( 99.66)
Epoch: [24][260/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.6196e-01 (3.1478e-01)	Acc@1  92.19 ( 88.98)	Acc@5 100.00 ( 99.66)
Epoch: [24][270/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.8003e-01 (3.1458e-01)	Acc@1  88.28 ( 89.01)	Acc@5 100.00 ( 99.65)
Epoch: [24][280/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.3779e-01 (3.1441e-01)	Acc@1  92.97 ( 89.03)	Acc@5 100.00 ( 99.66)
Epoch: [24][290/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.2285e-01 (3.1590e-01)	Acc@1  84.38 ( 88.98)	Acc@5 100.00 ( 99.66)
Epoch: [24][300/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.5513e-01 (3.1601e-01)	Acc@1  90.62 ( 88.99)	Acc@5 100.00 ( 99.66)
Epoch: [24][310/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.9355e-01 (3.1623e-01)	Acc@1  85.94 ( 88.98)	Acc@5 100.00 ( 99.66)
Epoch: [24][320/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.4326e-01 (3.1629e-01)	Acc@1  86.72 ( 88.98)	Acc@5 100.00 ( 99.66)
Epoch: [24][330/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.4438e-01 (3.1648e-01)	Acc@1  92.19 ( 88.97)	Acc@5 100.00 ( 99.67)
Epoch: [24][340/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.6660e-01 (3.1658e-01)	Acc@1  92.19 ( 88.97)	Acc@5 100.00 ( 99.66)
Epoch: [24][350/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7432e-01 (3.1590e-01)	Acc@1  95.31 ( 89.02)	Acc@5 100.00 ( 99.67)
Epoch: [24][360/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.4995e-01 (3.1540e-01)	Acc@1  83.59 ( 89.03)	Acc@5 100.00 ( 99.67)
Epoch: [24][370/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.7524e-01 (3.1558e-01)	Acc@1  85.94 ( 89.05)	Acc@5 100.00 ( 99.68)
Epoch: [24][380/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.9268e-01 (3.1523e-01)	Acc@1  82.81 ( 89.06)	Acc@5 100.00 ( 99.68)
Epoch: [24][390/391]	Time  0.119 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.5776e-01 (3.1546e-01)	Acc@1  82.50 ( 89.05)	Acc@5  98.75 ( 99.68)
## e[24] optimizer.zero_grad (sum) time: 0.5246615409851074
## e[24]       loss.backward (sum) time: 12.431896924972534
## e[24]      optimizer.step (sum) time: 25.745766401290894
## epoch[24] training(only) time: 63.484790086746216
# Switched to evaluate mode...
Test: [  0/100]	Time  0.192 ( 0.192)	Loss 3.9258e-01 (3.9258e-01)	Acc@1  88.00 ( 88.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 5.6250e-01 (4.8184e-01)	Acc@1  88.00 ( 85.82)	Acc@5  99.00 ( 99.18)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 4.3481e-01 (4.6011e-01)	Acc@1  85.00 ( 85.90)	Acc@5 100.00 ( 99.33)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 4.3994e-01 (4.6147e-01)	Acc@1  85.00 ( 86.00)	Acc@5  99.00 ( 99.39)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 6.0449e-01 (4.6542e-01)	Acc@1  84.00 ( 85.93)	Acc@5  99.00 ( 99.32)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 5.1270e-01 (4.6316e-01)	Acc@1  84.00 ( 85.90)	Acc@5 100.00 ( 99.35)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 3.4253e-01 (4.5662e-01)	Acc@1  92.00 ( 85.72)	Acc@5 100.00 ( 99.39)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 5.7129e-01 (4.5007e-01)	Acc@1  83.00 ( 85.96)	Acc@5  99.00 ( 99.41)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 3.8892e-01 (4.4266e-01)	Acc@1  88.00 ( 86.06)	Acc@5  99.00 ( 99.43)
Test: [ 90/100]	Time  0.048 ( 0.049)	Loss 3.6865e-01 (4.4701e-01)	Acc@1  86.00 ( 85.92)	Acc@5 100.00 ( 99.43)
 * Acc@1 85.880 Acc@5 99.460
### epoch[24] execution time: 68.47276401519775
EPOCH 25
# current learning rate: 0.1
# Switched to train mode...
Epoch: [25][  0/391]	Time  0.331 ( 0.331)	Data  0.153 ( 0.153)	Loss 4.9219e-01 (4.9219e-01)	Acc@1  81.25 ( 81.25)	Acc@5  98.44 ( 98.44)
Epoch: [25][ 10/391]	Time  0.160 ( 0.176)	Data  0.001 ( 0.015)	Loss 2.2314e-01 (3.1315e-01)	Acc@1  92.97 ( 88.99)	Acc@5 100.00 ( 99.57)
Epoch: [25][ 20/391]	Time  0.160 ( 0.169)	Data  0.001 ( 0.008)	Loss 3.0225e-01 (2.9889e-01)	Acc@1  89.06 ( 89.17)	Acc@5 100.00 ( 99.78)
Epoch: [25][ 30/391]	Time  0.162 ( 0.167)	Data  0.001 ( 0.006)	Loss 3.5693e-01 (3.0003e-01)	Acc@1  85.94 ( 89.36)	Acc@5 100.00 ( 99.70)
Epoch: [25][ 40/391]	Time  0.161 ( 0.166)	Data  0.001 ( 0.005)	Loss 2.4927e-01 (2.9566e-01)	Acc@1  92.97 ( 89.52)	Acc@5 100.00 ( 99.73)
Epoch: [25][ 50/391]	Time  0.161 ( 0.166)	Data  0.001 ( 0.004)	Loss 3.0737e-01 (2.9929e-01)	Acc@1  89.84 ( 89.23)	Acc@5  99.22 ( 99.72)
Epoch: [25][ 60/391]	Time  0.162 ( 0.165)	Data  0.001 ( 0.004)	Loss 2.2705e-01 (2.9733e-01)	Acc@1  93.75 ( 89.25)	Acc@5 100.00 ( 99.76)
Epoch: [25][ 70/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.8564e-01 (2.9457e-01)	Acc@1  88.28 ( 89.35)	Acc@5  99.22 ( 99.74)
Epoch: [25][ 80/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 3.3545e-01 (2.9280e-01)	Acc@1  88.28 ( 89.46)	Acc@5 100.00 ( 99.76)
Epoch: [25][ 90/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.0349e-01 (2.9130e-01)	Acc@1  92.97 ( 89.47)	Acc@5 100.00 ( 99.77)
Epoch: [25][100/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.4229e-01 (2.9142e-01)	Acc@1  91.41 ( 89.70)	Acc@5 100.00 ( 99.74)
Epoch: [25][110/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.3230e-01 (2.8834e-01)	Acc@1  93.75 ( 89.77)	Acc@5 100.00 ( 99.77)
Epoch: [25][120/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.5562e-01 (2.8728e-01)	Acc@1  94.53 ( 89.87)	Acc@5  99.22 ( 99.77)
Epoch: [25][130/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.6685e-01 (2.9012e-01)	Acc@1  92.97 ( 89.83)	Acc@5 100.00 ( 99.74)
Epoch: [25][140/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.1470e-01 (2.9336e-01)	Acc@1  91.41 ( 89.78)	Acc@5  99.22 ( 99.74)
Epoch: [25][150/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.4817e-01 (2.9364e-01)	Acc@1  91.41 ( 89.78)	Acc@5 100.00 ( 99.75)
Epoch: [25][160/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.9272e-01 (2.9351e-01)	Acc@1  85.16 ( 89.80)	Acc@5 100.00 ( 99.74)
Epoch: [25][170/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.2400e-01 (2.9157e-01)	Acc@1  92.19 ( 89.85)	Acc@5 100.00 ( 99.75)
Epoch: [25][180/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.4573e-01 (2.9266e-01)	Acc@1  92.19 ( 89.77)	Acc@5 100.00 ( 99.75)
Epoch: [25][190/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.2205e-01 (2.9237e-01)	Acc@1  92.19 ( 89.79)	Acc@5  99.22 ( 99.75)
Epoch: [25][200/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.7295e-01 (2.9317e-01)	Acc@1  90.62 ( 89.72)	Acc@5 100.00 ( 99.76)
Epoch: [25][210/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.0381e-01 (2.9517e-01)	Acc@1  88.28 ( 89.67)	Acc@5  99.22 ( 99.75)
Epoch: [25][220/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.7954e-01 (2.9625e-01)	Acc@1  89.84 ( 89.65)	Acc@5  99.22 ( 99.73)
Epoch: [25][230/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.0088e-01 (2.9614e-01)	Acc@1  85.94 ( 89.65)	Acc@5 100.00 ( 99.74)
Epoch: [25][240/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.9785e-01 (2.9571e-01)	Acc@1  88.28 ( 89.67)	Acc@5 100.00 ( 99.74)
Epoch: [25][250/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.9395e-01 (2.9653e-01)	Acc@1  89.84 ( 89.64)	Acc@5 100.00 ( 99.74)
Epoch: [25][260/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.1616e-01 (2.9708e-01)	Acc@1  88.28 ( 89.61)	Acc@5 100.00 ( 99.74)
Epoch: [25][270/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.1738e-01 (2.9658e-01)	Acc@1  87.50 ( 89.61)	Acc@5 100.00 ( 99.74)
Epoch: [25][280/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.3213e-01 (2.9859e-01)	Acc@1  85.16 ( 89.56)	Acc@5  99.22 ( 99.74)
Epoch: [25][290/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.8076e-01 (2.9958e-01)	Acc@1  89.84 ( 89.52)	Acc@5  99.22 ( 99.73)
Epoch: [25][300/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.0078e-01 (2.9999e-01)	Acc@1  88.28 ( 89.50)	Acc@5 100.00 ( 99.74)
Epoch: [25][310/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.9087e-01 (3.0074e-01)	Acc@1  88.28 ( 89.48)	Acc@5  98.44 ( 99.73)
Epoch: [25][320/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.8540e-01 (3.0037e-01)	Acc@1  89.84 ( 89.51)	Acc@5 100.00 ( 99.74)
Epoch: [25][330/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.2324e-01 (3.0166e-01)	Acc@1  88.28 ( 89.44)	Acc@5 100.00 ( 99.74)
Epoch: [25][340/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.0959e-01 (3.0226e-01)	Acc@1  90.62 ( 89.43)	Acc@5 100.00 ( 99.73)
Epoch: [25][350/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.3560e-01 (3.0263e-01)	Acc@1  93.75 ( 89.42)	Acc@5 100.00 ( 99.72)
Epoch: [25][360/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.5122e-01 (3.0307e-01)	Acc@1  90.62 ( 89.40)	Acc@5 100.00 ( 99.72)
Epoch: [25][370/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.2217e-01 (3.0406e-01)	Acc@1  91.41 ( 89.35)	Acc@5 100.00 ( 99.71)
Epoch: [25][380/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.6367e-01 (3.0478e-01)	Acc@1  89.06 ( 89.31)	Acc@5 100.00 ( 99.71)
Epoch: [25][390/391]	Time  0.116 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.1221e-01 (3.0437e-01)	Acc@1  88.75 ( 89.35)	Acc@5 100.00 ( 99.71)
## e[25] optimizer.zero_grad (sum) time: 0.5259444713592529
## e[25]       loss.backward (sum) time: 12.491591215133667
## e[25]      optimizer.step (sum) time: 25.675116539001465
## epoch[25] training(only) time: 63.51092076301575
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 4.5703e-01 (4.5703e-01)	Acc@1  88.00 ( 88.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.048 ( 0.061)	Loss 4.5312e-01 (4.5296e-01)	Acc@1  90.00 ( 86.55)	Acc@5 100.00 ( 99.36)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 5.2832e-01 (4.4729e-01)	Acc@1  84.00 ( 85.90)	Acc@5 100.00 ( 99.52)
Test: [ 30/100]	Time  0.056 ( 0.052)	Loss 4.5386e-01 (4.4873e-01)	Acc@1  84.00 ( 85.94)	Acc@5  99.00 ( 99.58)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 4.8877e-01 (4.5366e-01)	Acc@1  84.00 ( 85.51)	Acc@5  98.00 ( 99.46)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 3.7402e-01 (4.4340e-01)	Acc@1  88.00 ( 85.84)	Acc@5  99.00 ( 99.43)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 3.2056e-01 (4.3639e-01)	Acc@1  89.00 ( 85.89)	Acc@5 100.00 ( 99.49)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 5.9131e-01 (4.3555e-01)	Acc@1  86.00 ( 85.87)	Acc@5 100.00 ( 99.52)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 2.9468e-01 (4.3596e-01)	Acc@1  89.00 ( 85.84)	Acc@5 100.00 ( 99.53)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 4.2090e-01 (4.3805e-01)	Acc@1  83.00 ( 85.80)	Acc@5 100.00 ( 99.53)
 * Acc@1 85.990 Acc@5 99.570
### epoch[25] execution time: 68.47198414802551
EPOCH 26
# current learning rate: 0.1
# Switched to train mode...
Epoch: [26][  0/391]	Time  0.327 ( 0.327)	Data  0.154 ( 0.154)	Loss 2.3682e-01 (2.3682e-01)	Acc@1  92.19 ( 92.19)	Acc@5 100.00 (100.00)
Epoch: [26][ 10/391]	Time  0.161 ( 0.176)	Data  0.001 ( 0.015)	Loss 2.7319e-01 (3.0174e-01)	Acc@1  90.62 ( 89.91)	Acc@5 100.00 ( 99.64)
Epoch: [26][ 20/391]	Time  0.159 ( 0.168)	Data  0.001 ( 0.008)	Loss 2.5269e-01 (2.7709e-01)	Acc@1  92.19 ( 90.66)	Acc@5 100.00 ( 99.67)
Epoch: [26][ 30/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.006)	Loss 3.9185e-01 (2.8132e-01)	Acc@1  87.50 ( 90.90)	Acc@5 100.00 ( 99.65)
Epoch: [26][ 40/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.005)	Loss 2.6245e-01 (2.8409e-01)	Acc@1  91.41 ( 90.53)	Acc@5 100.00 ( 99.68)
Epoch: [26][ 50/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.004)	Loss 4.1870e-01 (2.8396e-01)	Acc@1  85.94 ( 90.52)	Acc@5  99.22 ( 99.69)
Epoch: [26][ 60/391]	Time  0.169 ( 0.164)	Data  0.001 ( 0.004)	Loss 2.6465e-01 (2.8654e-01)	Acc@1  92.97 ( 90.38)	Acc@5 100.00 ( 99.69)
Epoch: [26][ 70/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 3.1006e-01 (2.8865e-01)	Acc@1  89.84 ( 90.28)	Acc@5  99.22 ( 99.69)
Epoch: [26][ 80/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.6440e-01 (2.9429e-01)	Acc@1  89.84 ( 90.09)	Acc@5 100.00 ( 99.70)
Epoch: [26][ 90/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.8823e-01 (2.8926e-01)	Acc@1  92.97 ( 90.22)	Acc@5 100.00 ( 99.73)
Epoch: [26][100/391]	Time  0.169 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.4229e-01 (2.8899e-01)	Acc@1  85.94 ( 90.22)	Acc@5 100.00 ( 99.73)
Epoch: [26][110/391]	Time  0.173 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.0361e-01 (2.8713e-01)	Acc@1  91.41 ( 90.19)	Acc@5 100.00 ( 99.75)
Epoch: [26][120/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.6904e-01 (2.8932e-01)	Acc@1  92.97 ( 90.21)	Acc@5 100.00 ( 99.74)
Epoch: [26][130/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7090e-01 (2.8953e-01)	Acc@1  94.53 ( 90.26)	Acc@5 100.00 ( 99.75)
Epoch: [26][140/391]	Time  0.168 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.4692e-01 (2.8971e-01)	Acc@1  85.16 ( 90.24)	Acc@5  99.22 ( 99.76)
Epoch: [26][150/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.5801e-01 (2.9141e-01)	Acc@1  83.59 ( 90.22)	Acc@5  99.22 ( 99.76)
Epoch: [26][160/391]	Time  0.173 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.4817e-01 (2.9247e-01)	Acc@1  90.62 ( 90.17)	Acc@5  99.22 ( 99.74)
Epoch: [26][170/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.9275e-01 (2.9045e-01)	Acc@1  92.97 ( 90.22)	Acc@5 100.00 ( 99.75)
Epoch: [26][180/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.5010e-01 (2.8966e-01)	Acc@1  89.06 ( 90.19)	Acc@5 100.00 ( 99.76)
Epoch: [26][190/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.9907e-01 (2.8972e-01)	Acc@1  88.28 ( 90.19)	Acc@5 100.00 ( 99.77)
Epoch: [26][200/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.4985e-01 (2.9048e-01)	Acc@1  87.50 ( 90.14)	Acc@5  99.22 ( 99.77)
Epoch: [26][210/391]	Time  0.171 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.7075e-01 (2.8876e-01)	Acc@1  89.84 ( 90.19)	Acc@5  99.22 ( 99.77)
Epoch: [26][220/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.5474e-01 (2.8828e-01)	Acc@1  87.50 ( 90.17)	Acc@5 100.00 ( 99.78)
Epoch: [26][230/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.2986e-01 (2.8754e-01)	Acc@1  92.97 ( 90.19)	Acc@5 100.00 ( 99.78)
Epoch: [26][240/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.0820e-01 (2.8789e-01)	Acc@1  84.38 ( 90.15)	Acc@5 100.00 ( 99.78)
Epoch: [26][250/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.0532e-01 (2.8775e-01)	Acc@1  92.97 ( 90.12)	Acc@5 100.00 ( 99.78)
Epoch: [26][260/391]	Time  0.171 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.3691e-01 (2.8919e-01)	Acc@1  88.28 ( 90.08)	Acc@5 100.00 ( 99.78)
Epoch: [26][270/391]	Time  0.168 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.2852e-01 (2.9016e-01)	Acc@1  92.97 ( 90.04)	Acc@5 100.00 ( 99.77)
Epoch: [26][280/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.7954e-01 (2.8901e-01)	Acc@1  87.50 ( 90.07)	Acc@5  99.22 ( 99.77)
Epoch: [26][290/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.0479e-01 (2.9001e-01)	Acc@1  85.16 ( 90.06)	Acc@5  99.22 ( 99.77)
Epoch: [26][300/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.5562e-01 (2.8865e-01)	Acc@1  89.84 ( 90.07)	Acc@5 100.00 ( 99.78)
Epoch: [26][310/391]	Time  0.176 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5906e-01 (2.8942e-01)	Acc@1  94.53 ( 90.07)	Acc@5 100.00 ( 99.76)
Epoch: [26][320/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.7393e-01 (2.8930e-01)	Acc@1  89.84 ( 90.05)	Acc@5 100.00 ( 99.77)
Epoch: [26][330/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.7271e-01 (2.8839e-01)	Acc@1  87.50 ( 90.07)	Acc@5 100.00 ( 99.77)
Epoch: [26][340/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.3350e-01 (2.8873e-01)	Acc@1  91.41 ( 90.08)	Acc@5 100.00 ( 99.77)
Epoch: [26][350/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.4775e-01 (2.8999e-01)	Acc@1  87.50 ( 90.04)	Acc@5 100.00 ( 99.78)
Epoch: [26][360/391]	Time  0.168 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.3447e-01 (2.9034e-01)	Acc@1  89.06 ( 90.02)	Acc@5 100.00 ( 99.77)
Epoch: [26][370/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.5122e-01 (2.9034e-01)	Acc@1  89.84 ( 90.02)	Acc@5  99.22 ( 99.77)
Epoch: [26][380/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.3450e-01 (2.8925e-01)	Acc@1  91.41 ( 90.04)	Acc@5  99.22 ( 99.77)
Epoch: [26][390/391]	Time  0.118 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.9341e-01 (2.8930e-01)	Acc@1  86.25 ( 90.03)	Acc@5  98.75 ( 99.76)
## e[26] optimizer.zero_grad (sum) time: 0.5187830924987793
## e[26]       loss.backward (sum) time: 12.4673752784729
## e[26]      optimizer.step (sum) time: 25.702387809753418
## epoch[26] training(only) time: 63.56432104110718
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 2.1252e-01 (2.1252e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.060)	Loss 3.8135e-01 (4.2080e-01)	Acc@1  85.00 ( 86.27)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.047 ( 0.054)	Loss 4.1943e-01 (4.0930e-01)	Acc@1  87.00 ( 87.29)	Acc@5 100.00 ( 99.43)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 4.2554e-01 (4.0995e-01)	Acc@1  87.00 ( 87.26)	Acc@5 100.00 ( 99.42)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 4.5459e-01 (4.1111e-01)	Acc@1  87.00 ( 87.22)	Acc@5  98.00 ( 99.39)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 3.0713e-01 (4.1509e-01)	Acc@1  90.00 ( 87.00)	Acc@5 100.00 ( 99.35)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 5.2539e-01 (4.1488e-01)	Acc@1  86.00 ( 87.08)	Acc@5 100.00 ( 99.41)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 3.7012e-01 (4.1861e-01)	Acc@1  88.00 ( 86.97)	Acc@5 100.00 ( 99.44)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 3.5352e-01 (4.2001e-01)	Acc@1  88.00 ( 86.84)	Acc@5 100.00 ( 99.47)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 4.3921e-01 (4.2381e-01)	Acc@1  88.00 ( 86.81)	Acc@5 100.00 ( 99.51)
 * Acc@1 86.820 Acc@5 99.510
### epoch[26] execution time: 68.51898169517517
EPOCH 27
# current learning rate: 0.1
# Switched to train mode...
Epoch: [27][  0/391]	Time  0.325 ( 0.325)	Data  0.155 ( 0.155)	Loss 2.8564e-01 (2.8564e-01)	Acc@1  89.84 ( 89.84)	Acc@5 100.00 (100.00)
Epoch: [27][ 10/391]	Time  0.160 ( 0.177)	Data  0.001 ( 0.015)	Loss 2.9785e-01 (2.5382e-01)	Acc@1  89.06 ( 90.77)	Acc@5 100.00 ( 99.72)
Epoch: [27][ 20/391]	Time  0.161 ( 0.169)	Data  0.001 ( 0.008)	Loss 1.8689e-01 (2.5134e-01)	Acc@1  93.75 ( 90.96)	Acc@5 100.00 ( 99.74)
Epoch: [27][ 30/391]	Time  0.161 ( 0.167)	Data  0.001 ( 0.006)	Loss 3.3423e-01 (2.6230e-01)	Acc@1  89.06 ( 90.73)	Acc@5  99.22 ( 99.72)
Epoch: [27][ 40/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.005)	Loss 1.9775e-01 (2.5356e-01)	Acc@1  92.97 ( 91.16)	Acc@5 100.00 ( 99.75)
Epoch: [27][ 50/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 3.2422e-01 (2.6415e-01)	Acc@1  85.16 ( 90.73)	Acc@5  99.22 ( 99.68)
Epoch: [27][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 3.6523e-01 (2.6653e-01)	Acc@1  87.50 ( 90.71)	Acc@5  99.22 ( 99.71)
Epoch: [27][ 70/391]	Time  0.166 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.8115e-01 (2.6600e-01)	Acc@1  94.53 ( 90.71)	Acc@5 100.00 ( 99.69)
Epoch: [27][ 80/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 3.4082e-01 (2.7352e-01)	Acc@1  88.28 ( 90.44)	Acc@5  99.22 ( 99.70)
Epoch: [27][ 90/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.9736e-01 (2.7442e-01)	Acc@1  87.50 ( 90.35)	Acc@5 100.00 ( 99.68)
Epoch: [27][100/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.8296e-01 (2.7397e-01)	Acc@1  90.62 ( 90.36)	Acc@5 100.00 ( 99.69)
Epoch: [27][110/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.2812e-01 (2.7705e-01)	Acc@1  87.50 ( 90.25)	Acc@5 100.00 ( 99.72)
Epoch: [27][120/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.6245e-01 (2.7817e-01)	Acc@1  91.41 ( 90.17)	Acc@5 100.00 ( 99.73)
Epoch: [27][130/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.8320e-01 (2.7770e-01)	Acc@1  92.19 ( 90.20)	Acc@5 100.00 ( 99.73)
Epoch: [27][140/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.9355e-01 (2.8072e-01)	Acc@1  85.16 ( 90.08)	Acc@5  99.22 ( 99.72)
Epoch: [27][150/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5674e-01 (2.7875e-01)	Acc@1  93.75 ( 90.18)	Acc@5 100.00 ( 99.73)
Epoch: [27][160/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.5767e-01 (2.7972e-01)	Acc@1  88.28 ( 90.14)	Acc@5 100.00 ( 99.75)
Epoch: [27][170/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.3633e-01 (2.8007e-01)	Acc@1  90.62 ( 90.11)	Acc@5 100.00 ( 99.75)
Epoch: [27][180/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.0591e-01 (2.8100e-01)	Acc@1  89.84 ( 90.09)	Acc@5  99.22 ( 99.75)
Epoch: [27][190/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.0005e-01 (2.8016e-01)	Acc@1  90.62 ( 90.13)	Acc@5 100.00 ( 99.75)
Epoch: [27][200/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.2583e-01 (2.7964e-01)	Acc@1  92.97 ( 90.21)	Acc@5 100.00 ( 99.75)
Epoch: [27][210/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.5830e-01 (2.7992e-01)	Acc@1  90.62 ( 90.23)	Acc@5 100.00 ( 99.76)
Epoch: [27][220/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.5767e-01 (2.7945e-01)	Acc@1  86.72 ( 90.23)	Acc@5 100.00 ( 99.75)
Epoch: [27][230/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.5073e-01 (2.7887e-01)	Acc@1  92.19 ( 90.28)	Acc@5 100.00 ( 99.75)
Epoch: [27][240/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.4277e-01 (2.7874e-01)	Acc@1  87.50 ( 90.29)	Acc@5  99.22 ( 99.75)
Epoch: [27][250/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.4741e-01 (2.7888e-01)	Acc@1  91.41 ( 90.30)	Acc@5  99.22 ( 99.75)
Epoch: [27][260/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.5830e-01 (2.7960e-01)	Acc@1  90.62 ( 90.28)	Acc@5 100.00 ( 99.76)
Epoch: [27][270/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.9248e-01 (2.7958e-01)	Acc@1  89.06 ( 90.30)	Acc@5  99.22 ( 99.75)
Epoch: [27][280/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.6792e-01 (2.8035e-01)	Acc@1  86.72 ( 90.26)	Acc@5 100.00 ( 99.75)
Epoch: [27][290/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.2935e-01 (2.8145e-01)	Acc@1  89.84 ( 90.21)	Acc@5 100.00 ( 99.76)
Epoch: [27][300/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.8940e-01 (2.8363e-01)	Acc@1  87.50 ( 90.11)	Acc@5 100.00 ( 99.75)
Epoch: [27][310/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.5596e-01 (2.8420e-01)	Acc@1  84.38 ( 90.10)	Acc@5 100.00 ( 99.74)
Epoch: [27][320/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.6514e-01 (2.8444e-01)	Acc@1  92.19 ( 90.10)	Acc@5  99.22 ( 99.74)
Epoch: [27][330/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.6841e-01 (2.8473e-01)	Acc@1  87.50 ( 90.08)	Acc@5 100.00 ( 99.74)
Epoch: [27][340/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.5229e-01 (2.8588e-01)	Acc@1  89.06 ( 90.05)	Acc@5 100.00 ( 99.74)
Epoch: [27][350/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.7036e-01 (2.8628e-01)	Acc@1  85.94 ( 90.04)	Acc@5 100.00 ( 99.74)
Epoch: [27][360/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.5215e-01 (2.8739e-01)	Acc@1  86.72 ( 90.00)	Acc@5 100.00 ( 99.73)
Epoch: [27][370/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.1887e-01 (2.8721e-01)	Acc@1  89.84 ( 90.02)	Acc@5 100.00 ( 99.73)
Epoch: [27][380/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.2656e-01 (2.8743e-01)	Acc@1  91.41 ( 90.02)	Acc@5  99.22 ( 99.73)
Epoch: [27][390/391]	Time  0.117 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.5000e-01 (2.8761e-01)	Acc@1  92.50 ( 90.01)	Acc@5 100.00 ( 99.72)
## e[27] optimizer.zero_grad (sum) time: 0.5251789093017578
## e[27]       loss.backward (sum) time: 12.438363552093506
## e[27]      optimizer.step (sum) time: 25.67998766899109
## epoch[27] training(only) time: 63.54798460006714
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 2.0923e-01 (2.0923e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.048 ( 0.061)	Loss 3.8647e-01 (4.0806e-01)	Acc@1  87.00 ( 85.82)	Acc@5 100.00 ( 99.27)
Test: [ 20/100]	Time  0.048 ( 0.055)	Loss 3.4741e-01 (3.9608e-01)	Acc@1  84.00 ( 86.62)	Acc@5 100.00 ( 99.29)
Test: [ 30/100]	Time  0.048 ( 0.053)	Loss 3.7183e-01 (4.0505e-01)	Acc@1  87.00 ( 86.55)	Acc@5  99.00 ( 99.39)
Test: [ 40/100]	Time  0.046 ( 0.052)	Loss 4.7949e-01 (4.0558e-01)	Acc@1  87.00 ( 86.85)	Acc@5  99.00 ( 99.37)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 3.1958e-01 (3.9612e-01)	Acc@1  89.00 ( 86.92)	Acc@5 100.00 ( 99.43)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 2.4475e-01 (3.9069e-01)	Acc@1  92.00 ( 86.98)	Acc@5 100.00 ( 99.48)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 4.6973e-01 (3.9072e-01)	Acc@1  91.00 ( 87.03)	Acc@5 100.00 ( 99.52)
Test: [ 80/100]	Time  0.057 ( 0.049)	Loss 3.1641e-01 (3.9434e-01)	Acc@1  86.00 ( 86.95)	Acc@5 100.00 ( 99.56)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 3.5107e-01 (3.9590e-01)	Acc@1  90.00 ( 86.88)	Acc@5 100.00 ( 99.58)
 * Acc@1 86.880 Acc@5 99.590
### epoch[27] execution time: 68.52918124198914
EPOCH 28
# current learning rate: 0.1
# Switched to train mode...
Epoch: [28][  0/391]	Time  0.320 ( 0.320)	Data  0.139 ( 0.139)	Loss 2.1948e-01 (2.1948e-01)	Acc@1  92.19 ( 92.19)	Acc@5 100.00 (100.00)
Epoch: [28][ 10/391]	Time  0.160 ( 0.176)	Data  0.001 ( 0.014)	Loss 3.0908e-01 (2.4612e-01)	Acc@1  89.84 ( 90.77)	Acc@5 100.00 ( 99.86)
Epoch: [28][ 20/391]	Time  0.161 ( 0.169)	Data  0.001 ( 0.008)	Loss 1.3281e-01 (2.2829e-01)	Acc@1  96.09 ( 91.26)	Acc@5 100.00 ( 99.93)
Epoch: [28][ 30/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.006)	Loss 1.4783e-01 (2.3175e-01)	Acc@1  95.31 ( 91.41)	Acc@5 100.00 ( 99.92)
Epoch: [28][ 40/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.005)	Loss 2.0557e-01 (2.3275e-01)	Acc@1  92.97 ( 91.50)	Acc@5 100.00 ( 99.90)
Epoch: [28][ 50/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 2.0032e-01 (2.3781e-01)	Acc@1  92.97 ( 91.39)	Acc@5 100.00 ( 99.91)
Epoch: [28][ 60/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.5918e-01 (2.3731e-01)	Acc@1  93.75 ( 91.43)	Acc@5 100.00 ( 99.92)
Epoch: [28][ 70/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 3.4131e-01 (2.4231e-01)	Acc@1  89.06 ( 91.29)	Acc@5 100.00 ( 99.90)
Epoch: [28][ 80/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.7500e-01 (2.5135e-01)	Acc@1  85.94 ( 91.05)	Acc@5 100.00 ( 99.88)
Epoch: [28][ 90/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.3108e-01 (2.5556e-01)	Acc@1  93.75 ( 91.00)	Acc@5 100.00 ( 99.88)
Epoch: [28][100/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.2446e-01 (2.5569e-01)	Acc@1  89.84 ( 90.98)	Acc@5 100.00 ( 99.88)
Epoch: [28][110/391]	Time  0.166 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.5562e-01 (2.5550e-01)	Acc@1  91.41 ( 90.96)	Acc@5 100.00 ( 99.88)
Epoch: [28][120/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.2974e-01 (2.5594e-01)	Acc@1  91.41 ( 90.93)	Acc@5 100.00 ( 99.87)
Epoch: [28][130/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.0967e-01 (2.5871e-01)	Acc@1  85.16 ( 90.89)	Acc@5 100.00 ( 99.86)
Epoch: [28][140/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.6313e-01 (2.6255e-01)	Acc@1  85.16 ( 90.79)	Acc@5 100.00 ( 99.86)
Epoch: [28][150/391]	Time  0.172 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.4438e-01 (2.6306e-01)	Acc@1  91.41 ( 90.84)	Acc@5 100.00 ( 99.84)
Epoch: [28][160/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.9370e-01 (2.6576e-01)	Acc@1  89.06 ( 90.69)	Acc@5 100.00 ( 99.83)
Epoch: [28][170/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.1274e-01 (2.6723e-01)	Acc@1  87.50 ( 90.63)	Acc@5 100.00 ( 99.83)
Epoch: [28][180/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7004e-01 (2.6855e-01)	Acc@1  94.53 ( 90.58)	Acc@5 100.00 ( 99.81)
Epoch: [28][190/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.0591e-01 (2.6726e-01)	Acc@1  91.41 ( 90.65)	Acc@5 100.00 ( 99.82)
Epoch: [28][200/391]	Time  0.171 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.6294e-01 (2.6716e-01)	Acc@1  94.53 ( 90.69)	Acc@5 100.00 ( 99.82)
Epoch: [28][210/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.0288e-01 (2.6639e-01)	Acc@1  93.75 ( 90.73)	Acc@5 100.00 ( 99.82)
Epoch: [28][220/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.9958e-01 (2.6759e-01)	Acc@1  95.31 ( 90.70)	Acc@5  99.22 ( 99.81)
Epoch: [28][230/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.6978e-01 (2.6735e-01)	Acc@1  89.84 ( 90.67)	Acc@5 100.00 ( 99.80)
Epoch: [28][240/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.6572e-01 (2.6895e-01)	Acc@1  87.50 ( 90.59)	Acc@5 100.00 ( 99.80)
Epoch: [28][250/391]	Time  0.169 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.6538e-01 (2.6904e-01)	Acc@1  89.06 ( 90.60)	Acc@5 100.00 ( 99.80)
Epoch: [28][260/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.9736e-01 (2.6879e-01)	Acc@1  90.62 ( 90.65)	Acc@5  99.22 ( 99.78)
Epoch: [28][270/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.4302e-01 (2.6874e-01)	Acc@1  89.06 ( 90.64)	Acc@5 100.00 ( 99.79)
Epoch: [28][280/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.1274e-01 (2.7067e-01)	Acc@1  91.41 ( 90.56)	Acc@5 100.00 ( 99.79)
Epoch: [28][290/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.5391e-01 (2.7318e-01)	Acc@1  92.97 ( 90.48)	Acc@5  99.22 ( 99.77)
Epoch: [28][300/391]	Time  0.168 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.6343e-01 (2.7414e-01)	Acc@1  92.97 ( 90.46)	Acc@5 100.00 ( 99.77)
Epoch: [28][310/391]	Time  0.171 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.9043e-01 (2.7474e-01)	Acc@1  94.53 ( 90.43)	Acc@5 100.00 ( 99.77)
Epoch: [28][320/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.8149e-01 (2.7386e-01)	Acc@1  90.62 ( 90.47)	Acc@5 100.00 ( 99.77)
Epoch: [28][330/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8213e-01 (2.7407e-01)	Acc@1  96.88 ( 90.48)	Acc@5  99.22 ( 99.76)
Epoch: [28][340/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.5610e-01 (2.7385e-01)	Acc@1  92.97 ( 90.51)	Acc@5 100.00 ( 99.76)
Epoch: [28][350/391]	Time  0.168 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.6084e-01 (2.7447e-01)	Acc@1  89.84 ( 90.49)	Acc@5 100.00 ( 99.77)
Epoch: [28][360/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.9419e-01 (2.7490e-01)	Acc@1  87.50 ( 90.46)	Acc@5 100.00 ( 99.77)
Epoch: [28][370/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 4.0869e-01 (2.7548e-01)	Acc@1  85.94 ( 90.43)	Acc@5 100.00 ( 99.77)
Epoch: [28][380/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.001)	Loss 3.7866e-01 (2.7604e-01)	Acc@1  87.50 ( 90.40)	Acc@5 100.00 ( 99.77)
Epoch: [28][390/391]	Time  0.116 ( 0.162)	Data  0.001 ( 0.001)	Loss 3.2910e-01 (2.7682e-01)	Acc@1  88.75 ( 90.37)	Acc@5 100.00 ( 99.77)
## e[28] optimizer.zero_grad (sum) time: 0.5214507579803467
## e[28]       loss.backward (sum) time: 12.428349494934082
## e[28]      optimizer.step (sum) time: 25.719374656677246
## epoch[28] training(only) time: 63.388368368148804
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 3.1616e-01 (3.1616e-01)	Acc@1  90.00 ( 90.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.060)	Loss 4.2114e-01 (3.8625e-01)	Acc@1  86.00 ( 86.91)	Acc@5  99.00 ( 99.45)
Test: [ 20/100]	Time  0.047 ( 0.054)	Loss 4.9902e-01 (3.9961e-01)	Acc@1  80.00 ( 86.48)	Acc@5 100.00 ( 99.48)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 4.4678e-01 (4.1329e-01)	Acc@1  82.00 ( 86.52)	Acc@5  99.00 ( 99.48)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 3.7891e-01 (4.0380e-01)	Acc@1  89.00 ( 86.95)	Acc@5  99.00 ( 99.37)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 2.9395e-01 (4.0418e-01)	Acc@1  90.00 ( 86.88)	Acc@5 100.00 ( 99.35)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 3.7524e-01 (4.0473e-01)	Acc@1  85.00 ( 86.57)	Acc@5 100.00 ( 99.39)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 3.0005e-01 (4.0486e-01)	Acc@1  91.00 ( 86.66)	Acc@5 100.00 ( 99.45)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 3.3691e-01 (4.0337e-01)	Acc@1  89.00 ( 86.84)	Acc@5 100.00 ( 99.44)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 3.9258e-01 (4.0428e-01)	Acc@1  87.00 ( 86.88)	Acc@5 100.00 ( 99.45)
 * Acc@1 86.920 Acc@5 99.480
### epoch[28] execution time: 68.35167813301086
EPOCH 29
# current learning rate: 0.1
# Switched to train mode...
Epoch: [29][  0/391]	Time  0.330 ( 0.330)	Data  0.162 ( 0.162)	Loss 1.8579e-01 (1.8579e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
Epoch: [29][ 10/391]	Time  0.160 ( 0.176)	Data  0.001 ( 0.016)	Loss 2.1301e-01 (2.7070e-01)	Acc@1  92.19 ( 91.26)	Acc@5 100.00 ( 99.79)
Epoch: [29][ 20/391]	Time  0.159 ( 0.169)	Data  0.001 ( 0.009)	Loss 2.8345e-01 (2.7544e-01)	Acc@1  91.41 ( 90.74)	Acc@5  99.22 ( 99.81)
Epoch: [29][ 30/391]	Time  0.160 ( 0.167)	Data  0.001 ( 0.006)	Loss 2.2632e-01 (2.6401e-01)	Acc@1  91.41 ( 91.18)	Acc@5 100.00 ( 99.85)
Epoch: [29][ 40/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.005)	Loss 3.0078e-01 (2.6318e-01)	Acc@1  90.62 ( 91.22)	Acc@5 100.00 ( 99.83)
Epoch: [29][ 50/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 2.1924e-01 (2.5117e-01)	Acc@1  91.41 ( 91.42)	Acc@5 100.00 ( 99.83)
Epoch: [29][ 60/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.004)	Loss 2.1472e-01 (2.4488e-01)	Acc@1  92.19 ( 91.74)	Acc@5 100.00 ( 99.82)
Epoch: [29][ 70/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 4.4922e-01 (2.5369e-01)	Acc@1  85.94 ( 91.52)	Acc@5  99.22 ( 99.78)
Epoch: [29][ 80/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.4036e-01 (2.5904e-01)	Acc@1  92.97 ( 91.29)	Acc@5 100.00 ( 99.80)
Epoch: [29][ 90/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 4.0918e-01 (2.6274e-01)	Acc@1  84.38 ( 91.11)	Acc@5 100.00 ( 99.81)
Epoch: [29][100/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.4961e-01 (2.6037e-01)	Acc@1  89.06 ( 91.21)	Acc@5  99.22 ( 99.81)
Epoch: [29][110/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.7856e-01 (2.5956e-01)	Acc@1  89.84 ( 91.23)	Acc@5  99.22 ( 99.80)
Epoch: [29][120/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.9624e-01 (2.6271e-01)	Acc@1  87.50 ( 91.08)	Acc@5  98.44 ( 99.79)
Epoch: [29][130/391]	Time  0.166 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.4622e-01 (2.6134e-01)	Acc@1  91.41 ( 91.11)	Acc@5 100.00 ( 99.80)
Epoch: [29][140/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.0420e-01 (2.6228e-01)	Acc@1  87.50 ( 91.00)	Acc@5 100.00 ( 99.81)
Epoch: [29][150/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.2568e-01 (2.6231e-01)	Acc@1  89.06 ( 91.01)	Acc@5 100.00 ( 99.82)
Epoch: [29][160/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4844e-01 (2.6248e-01)	Acc@1  93.75 ( 90.98)	Acc@5 100.00 ( 99.82)
Epoch: [29][170/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.7686e-01 (2.6251e-01)	Acc@1  91.41 ( 91.00)	Acc@5  99.22 ( 99.81)
Epoch: [29][180/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.1814e-01 (2.6331e-01)	Acc@1  92.97 ( 90.97)	Acc@5 100.00 ( 99.81)
Epoch: [29][190/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.9761e-01 (2.6247e-01)	Acc@1  89.84 ( 90.98)	Acc@5 100.00 ( 99.80)
Epoch: [29][200/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.6978e-01 (2.6384e-01)	Acc@1  91.41 ( 90.92)	Acc@5 100.00 ( 99.81)
Epoch: [29][210/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7798e-01 (2.6325e-01)	Acc@1  92.97 ( 90.94)	Acc@5 100.00 ( 99.80)
Epoch: [29][220/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.1960e-01 (2.6264e-01)	Acc@1  91.41 ( 90.94)	Acc@5 100.00 ( 99.79)
Epoch: [29][230/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.1821e-01 (2.6450e-01)	Acc@1  87.50 ( 90.90)	Acc@5 100.00 ( 99.78)
Epoch: [29][240/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.2559e-01 (2.6385e-01)	Acc@1  92.19 ( 90.94)	Acc@5 100.00 ( 99.78)
Epoch: [29][250/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.8882e-01 (2.6404e-01)	Acc@1  91.41 ( 90.92)	Acc@5 100.00 ( 99.79)
Epoch: [29][260/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.9932e-01 (2.6532e-01)	Acc@1  92.19 ( 90.90)	Acc@5  99.22 ( 99.78)
Epoch: [29][270/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.4927e-01 (2.6554e-01)	Acc@1  92.19 ( 90.88)	Acc@5 100.00 ( 99.78)
Epoch: [29][280/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.4814e-01 (2.6701e-01)	Acc@1  86.72 ( 90.81)	Acc@5  97.66 ( 99.77)
Epoch: [29][290/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.8662e-01 (2.6788e-01)	Acc@1  92.19 ( 90.78)	Acc@5  99.22 ( 99.77)
Epoch: [29][300/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6541e-01 (2.6781e-01)	Acc@1  95.31 ( 90.77)	Acc@5 100.00 ( 99.78)
Epoch: [29][310/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.1445e-01 (2.6847e-01)	Acc@1  87.50 ( 90.73)	Acc@5 100.00 ( 99.78)
Epoch: [29][320/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.6025e-01 (2.6994e-01)	Acc@1  90.62 ( 90.68)	Acc@5 100.00 ( 99.77)
Epoch: [29][330/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.7954e-01 (2.6971e-01)	Acc@1  89.84 ( 90.68)	Acc@5 100.00 ( 99.75)
Epoch: [29][340/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.0112e-01 (2.6985e-01)	Acc@1  85.94 ( 90.67)	Acc@5  99.22 ( 99.75)
Epoch: [29][350/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.3193e-01 (2.6937e-01)	Acc@1  90.62 ( 90.68)	Acc@5 100.00 ( 99.76)
Epoch: [29][360/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.4927e-01 (2.6936e-01)	Acc@1  90.62 ( 90.68)	Acc@5 100.00 ( 99.76)
Epoch: [29][370/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.2544e-01 (2.6899e-01)	Acc@1  89.84 ( 90.70)	Acc@5 100.00 ( 99.76)
Epoch: [29][380/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.9468e-01 (2.6919e-01)	Acc@1  88.28 ( 90.67)	Acc@5 100.00 ( 99.76)
Epoch: [29][390/391]	Time  0.117 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.6411e-01 (2.7084e-01)	Acc@1  82.50 ( 90.63)	Acc@5 100.00 ( 99.77)
## e[29] optimizer.zero_grad (sum) time: 0.5166120529174805
## e[29]       loss.backward (sum) time: 12.486394882202148
## e[29]      optimizer.step (sum) time: 25.666400909423828
## epoch[29] training(only) time: 63.375444173812866
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 3.5303e-01 (3.5303e-01)	Acc@1  88.00 ( 88.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.047 ( 0.062)	Loss 5.0781e-01 (4.1941e-01)	Acc@1  81.00 ( 87.00)	Acc@5 100.00 ( 99.45)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 4.1919e-01 (4.4022e-01)	Acc@1  88.00 ( 85.90)	Acc@5 100.00 ( 99.52)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 4.4824e-01 (4.5029e-01)	Acc@1  85.00 ( 85.74)	Acc@5 100.00 ( 99.55)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 5.9082e-01 (4.6375e-01)	Acc@1  83.00 ( 85.56)	Acc@5  97.00 ( 99.41)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 4.6191e-01 (4.5740e-01)	Acc@1  88.00 ( 85.75)	Acc@5  99.00 ( 99.49)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 3.9404e-01 (4.5403e-01)	Acc@1  88.00 ( 85.85)	Acc@5 100.00 ( 99.52)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 5.1807e-01 (4.5940e-01)	Acc@1  85.00 ( 85.69)	Acc@5 100.00 ( 99.56)
Test: [ 80/100]	Time  0.048 ( 0.049)	Loss 4.9683e-01 (4.6091e-01)	Acc@1  85.00 ( 85.65)	Acc@5 100.00 ( 99.58)
Test: [ 90/100]	Time  0.048 ( 0.049)	Loss 5.2832e-01 (4.6021e-01)	Acc@1  85.00 ( 85.55)	Acc@5 100.00 ( 99.59)
 * Acc@1 85.460 Acc@5 99.600
### epoch[29] execution time: 68.36861228942871
EPOCH 30
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [30][  0/391]	Time  0.329 ( 0.329)	Data  0.161 ( 0.161)	Loss 1.7053e-01 (1.7053e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [30][ 10/391]	Time  0.160 ( 0.176)	Data  0.001 ( 0.016)	Loss 2.0032e-01 (2.2986e-01)	Acc@1  92.97 ( 91.83)	Acc@5 100.00 ( 99.93)
Epoch: [30][ 20/391]	Time  0.160 ( 0.169)	Data  0.001 ( 0.009)	Loss 2.8662e-01 (2.2892e-01)	Acc@1  89.06 ( 91.93)	Acc@5  99.22 ( 99.81)
Epoch: [30][ 30/391]	Time  0.177 ( 0.167)	Data  0.001 ( 0.006)	Loss 2.4792e-01 (2.2787e-01)	Acc@1  89.84 ( 92.09)	Acc@5 100.00 ( 99.85)
Epoch: [30][ 40/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.005)	Loss 2.4182e-01 (2.1810e-01)	Acc@1  92.97 ( 92.36)	Acc@5  99.22 ( 99.85)
Epoch: [30][ 50/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.7554e-01 (2.1358e-01)	Acc@1  93.75 ( 92.52)	Acc@5 100.00 ( 99.88)
Epoch: [30][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.3574e-01 (2.1007e-01)	Acc@1  96.09 ( 92.61)	Acc@5 100.00 ( 99.87)
Epoch: [30][ 70/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.1863e-01 (2.0739e-01)	Acc@1  91.41 ( 92.84)	Acc@5 100.00 ( 99.88)
Epoch: [30][ 80/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.6577e-01 (2.0133e-01)	Acc@1  94.53 ( 93.01)	Acc@5  99.22 ( 99.88)
Epoch: [30][ 90/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.6035e-01 (2.0026e-01)	Acc@1  87.50 ( 93.07)	Acc@5 100.00 ( 99.90)
Epoch: [30][100/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.8003e-01 (1.9863e-01)	Acc@1  89.06 ( 93.15)	Acc@5 100.00 ( 99.91)
Epoch: [30][110/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.3425e-01 (1.9786e-01)	Acc@1  90.62 ( 93.14)	Acc@5 100.00 ( 99.91)
Epoch: [30][120/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.5195e-01 (1.9655e-01)	Acc@1  92.19 ( 93.20)	Acc@5 100.00 ( 99.90)
Epoch: [30][130/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.4780e-01 (1.9517e-01)	Acc@1  90.62 ( 93.24)	Acc@5  99.22 ( 99.90)
Epoch: [30][140/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2067e-01 (1.9295e-01)	Acc@1  96.09 ( 93.35)	Acc@5 100.00 ( 99.90)
Epoch: [30][150/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6370e-01 (1.9013e-01)	Acc@1  96.09 ( 93.42)	Acc@5 100.00 ( 99.90)
Epoch: [30][160/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5442e-01 (1.8789e-01)	Acc@1  93.75 ( 93.49)	Acc@5 100.00 ( 99.90)
Epoch: [30][170/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5942e-01 (1.8545e-01)	Acc@1  96.09 ( 93.59)	Acc@5 100.00 ( 99.90)
Epoch: [30][180/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.6680e-02 (1.8303e-01)	Acc@1  95.31 ( 93.68)	Acc@5 100.00 ( 99.91)
Epoch: [30][190/391]	Time  0.172 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5137e-01 (1.8042e-01)	Acc@1  95.31 ( 93.75)	Acc@5  99.22 ( 99.91)
Epoch: [30][200/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0583e-01 (1.7971e-01)	Acc@1  97.66 ( 93.80)	Acc@5 100.00 ( 99.91)
Epoch: [30][210/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.3994e-02 (1.7821e-01)	Acc@1  96.88 ( 93.84)	Acc@5 100.00 ( 99.91)
Epoch: [30][220/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.0142e-01 (1.7616e-01)	Acc@1  91.41 ( 93.90)	Acc@5 100.00 ( 99.91)
Epoch: [30][230/391]	Time  0.166 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.2957e-02 (1.7498e-01)	Acc@1  96.88 ( 93.92)	Acc@5 100.00 ( 99.91)
Epoch: [30][240/391]	Time  0.172 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.4890e-02 (1.7368e-01)	Acc@1  99.22 ( 93.95)	Acc@5 100.00 ( 99.91)
Epoch: [30][250/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3477e-01 (1.7400e-01)	Acc@1  94.53 ( 93.96)	Acc@5 100.00 ( 99.91)
Epoch: [30][260/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6907e-01 (1.7343e-01)	Acc@1  92.97 ( 93.97)	Acc@5 100.00 ( 99.91)
Epoch: [30][270/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3013e-01 (1.7246e-01)	Acc@1  95.31 ( 93.99)	Acc@5 100.00 ( 99.92)
Epoch: [30][280/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1890e-01 (1.7134e-01)	Acc@1  95.31 ( 94.03)	Acc@5 100.00 ( 99.91)
Epoch: [30][290/391]	Time  0.169 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.1863e-01 (1.7026e-01)	Acc@1  92.19 ( 94.07)	Acc@5  98.44 ( 99.91)
Epoch: [30][300/391]	Time  0.177 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.1675e-02 (1.6945e-01)	Acc@1  97.66 ( 94.09)	Acc@5 100.00 ( 99.91)
Epoch: [30][310/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.9006e-01 (1.6874e-01)	Acc@1  92.19 ( 94.11)	Acc@5 100.00 ( 99.91)
Epoch: [30][320/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2415e-01 (1.6798e-01)	Acc@1  96.09 ( 94.14)	Acc@5 100.00 ( 99.91)
Epoch: [30][330/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7847e-01 (1.6845e-01)	Acc@1  94.53 ( 94.14)	Acc@5 100.00 ( 99.91)
Epoch: [30][340/391]	Time  0.171 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.1960e-01 (1.6867e-01)	Acc@1  92.97 ( 94.13)	Acc@5 100.00 ( 99.91)
Epoch: [30][350/391]	Time  0.176 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.2935e-01 (1.6832e-01)	Acc@1  89.06 ( 94.15)	Acc@5 100.00 ( 99.91)
Epoch: [30][360/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1511e-01 (1.6715e-01)	Acc@1  94.53 ( 94.20)	Acc@5 100.00 ( 99.91)
Epoch: [30][370/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2103e-01 (1.6667e-01)	Acc@1  96.88 ( 94.21)	Acc@5 100.00 ( 99.91)
Epoch: [30][380/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.2205e-01 (1.6615e-01)	Acc@1  92.97 ( 94.23)	Acc@5 100.00 ( 99.91)
Epoch: [30][390/391]	Time  0.124 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.1167e-01 (1.6494e-01)	Acc@1  95.00 ( 94.27)	Acc@5 100.00 ( 99.91)
## e[30] optimizer.zero_grad (sum) time: 0.5249137878417969
## e[30]       loss.backward (sum) time: 12.509062051773071
## e[30]      optimizer.step (sum) time: 25.71231985092163
## epoch[30] training(only) time: 63.52948188781738
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 1.5210e-01 (1.5210e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.059)	Loss 2.9907e-01 (2.6599e-01)	Acc@1  91.00 ( 90.64)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.058 ( 0.054)	Loss 2.6465e-01 (2.7956e-01)	Acc@1  90.00 ( 90.57)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.047 ( 0.051)	Loss 2.8906e-01 (2.8810e-01)	Acc@1  90.00 ( 90.77)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.050 ( 0.050)	Loss 4.5386e-01 (2.9464e-01)	Acc@1  89.00 ( 90.80)	Acc@5  98.00 ( 99.66)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 2.1826e-01 (2.9359e-01)	Acc@1  95.00 ( 90.88)	Acc@5 100.00 ( 99.69)
Test: [ 60/100]	Time  0.047 ( 0.049)	Loss 2.7588e-01 (2.9204e-01)	Acc@1  94.00 ( 90.85)	Acc@5 100.00 ( 99.72)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 2.7197e-01 (2.8989e-01)	Acc@1  93.00 ( 90.89)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.048 ( 0.049)	Loss 2.2144e-01 (2.8642e-01)	Acc@1  93.00 ( 91.01)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 2.4414e-01 (2.8497e-01)	Acc@1  89.00 ( 91.04)	Acc@5 100.00 ( 99.78)
 * Acc@1 90.940 Acc@5 99.800
### epoch[30] execution time: 68.49151492118835
EPOCH 31
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [31][  0/391]	Time  0.315 ( 0.315)	Data  0.137 ( 0.137)	Loss 1.4844e-01 (1.4844e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [31][ 10/391]	Time  0.161 ( 0.176)	Data  0.001 ( 0.013)	Loss 1.5051e-01 (1.4490e-01)	Acc@1  94.53 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [31][ 20/391]	Time  0.161 ( 0.169)	Data  0.001 ( 0.008)	Loss 1.0120e-01 (1.3710e-01)	Acc@1  96.09 ( 95.42)	Acc@5 100.00 (100.00)
Epoch: [31][ 30/391]	Time  0.161 ( 0.167)	Data  0.001 ( 0.006)	Loss 9.6680e-02 (1.2935e-01)	Acc@1  97.66 ( 95.61)	Acc@5 100.00 (100.00)
Epoch: [31][ 40/391]	Time  0.162 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.8213e-01 (1.2889e-01)	Acc@1  90.62 ( 95.43)	Acc@5 100.00 (100.00)
Epoch: [31][ 50/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.5771e-01 (1.2757e-01)	Acc@1  94.53 ( 95.45)	Acc@5 100.00 (100.00)
Epoch: [31][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.2488e-01 (1.2587e-01)	Acc@1  96.09 ( 95.56)	Acc@5 100.00 ( 99.99)
Epoch: [31][ 70/391]	Time  0.159 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.3291e-01 (1.2801e-01)	Acc@1  92.19 ( 95.51)	Acc@5 100.00 ( 99.97)
Epoch: [31][ 80/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.2964e-01 (1.2992e-01)	Acc@1  94.53 ( 95.44)	Acc@5 100.00 ( 99.97)
Epoch: [31][ 90/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.003)	Loss 6.2134e-02 (1.2964e-01)	Acc@1  98.44 ( 95.44)	Acc@5 100.00 ( 99.97)
Epoch: [31][100/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 8.1238e-02 (1.2787e-01)	Acc@1  98.44 ( 95.54)	Acc@5 100.00 ( 99.98)
Epoch: [31][110/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7041e-01 (1.2989e-01)	Acc@1  93.75 ( 95.45)	Acc@5 100.00 ( 99.98)
Epoch: [31][120/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2311e-01 (1.3072e-01)	Acc@1  95.31 ( 95.42)	Acc@5 100.00 ( 99.98)
Epoch: [31][130/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.8030e-01 (1.3149e-01)	Acc@1  92.19 ( 95.40)	Acc@5 100.00 ( 99.98)
Epoch: [31][140/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7603e-01 (1.2980e-01)	Acc@1  96.09 ( 95.48)	Acc@5 100.00 ( 99.98)
Epoch: [31][150/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.4036e-01 (1.3030e-01)	Acc@1  90.62 ( 95.48)	Acc@5 100.00 ( 99.98)
Epoch: [31][160/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1548e-01 (1.3035e-01)	Acc@1  96.09 ( 95.49)	Acc@5 100.00 ( 99.97)
Epoch: [31][170/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5796e-01 (1.3086e-01)	Acc@1  93.75 ( 95.47)	Acc@5 100.00 ( 99.97)
Epoch: [31][180/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0016e-01 (1.3092e-01)	Acc@1  96.09 ( 95.47)	Acc@5 100.00 ( 99.97)
Epoch: [31][190/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.0271e-02 (1.2970e-01)	Acc@1  96.09 ( 95.53)	Acc@5 100.00 ( 99.97)
Epoch: [31][200/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7603e-01 (1.2936e-01)	Acc@1  93.75 ( 95.53)	Acc@5 100.00 ( 99.97)
Epoch: [31][210/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5808e-01 (1.2863e-01)	Acc@1  96.09 ( 95.59)	Acc@5 100.00 ( 99.97)
Epoch: [31][220/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0791e-01 (1.2744e-01)	Acc@1  96.09 ( 95.63)	Acc@5 100.00 ( 99.97)
Epoch: [31][230/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.1106e-01 (1.2862e-01)	Acc@1  93.75 ( 95.58)	Acc@5 100.00 ( 99.97)
Epoch: [31][240/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1176e-01 (1.2804e-01)	Acc@1  95.31 ( 95.61)	Acc@5 100.00 ( 99.97)
Epoch: [31][250/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2549e-01 (1.2791e-01)	Acc@1  93.75 ( 95.59)	Acc@5 100.00 ( 99.97)
Epoch: [31][260/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.0435e-02 (1.2788e-01)	Acc@1  98.44 ( 95.58)	Acc@5 100.00 ( 99.97)
Epoch: [31][270/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3037e-01 (1.2760e-01)	Acc@1  94.53 ( 95.60)	Acc@5 100.00 ( 99.97)
Epoch: [31][280/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2439e-01 (1.2734e-01)	Acc@1  94.53 ( 95.61)	Acc@5 100.00 ( 99.97)
Epoch: [31][290/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.8003e-02 (1.2687e-01)	Acc@1  96.88 ( 95.61)	Acc@5 100.00 ( 99.97)
Epoch: [31][300/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.6713e-02 (1.2722e-01)	Acc@1  99.22 ( 95.61)	Acc@5 100.00 ( 99.96)
Epoch: [31][310/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1676e-01 (1.2751e-01)	Acc@1  96.09 ( 95.59)	Acc@5 100.00 ( 99.96)
Epoch: [31][320/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4575e-01 (1.2862e-01)	Acc@1  95.31 ( 95.57)	Acc@5 100.00 ( 99.96)
Epoch: [31][330/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3306e-01 (1.2808e-01)	Acc@1  94.53 ( 95.59)	Acc@5 100.00 ( 99.96)
Epoch: [31][340/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1475e-01 (1.2836e-01)	Acc@1  95.31 ( 95.58)	Acc@5 100.00 ( 99.96)
Epoch: [31][350/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2524e-01 (1.2854e-01)	Acc@1  96.09 ( 95.57)	Acc@5 100.00 ( 99.96)
Epoch: [31][360/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0474e-01 (1.2868e-01)	Acc@1  96.88 ( 95.57)	Acc@5 100.00 ( 99.96)
Epoch: [31][370/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2164e-01 (1.2821e-01)	Acc@1  96.09 ( 95.58)	Acc@5 100.00 ( 99.96)
Epoch: [31][380/391]	Time  0.165 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.6284e-02 (1.2775e-01)	Acc@1  98.44 ( 95.61)	Acc@5 100.00 ( 99.96)
Epoch: [31][390/391]	Time  0.117 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.9031e-01 (1.2760e-01)	Acc@1  90.00 ( 95.61)	Acc@5 100.00 ( 99.96)
## e[31] optimizer.zero_grad (sum) time: 0.5248680114746094
## e[31]       loss.backward (sum) time: 12.464205980300903
## e[31]      optimizer.step (sum) time: 25.738433361053467
## epoch[31] training(only) time: 63.55638074874878
# Switched to evaluate mode...
Test: [  0/100]	Time  0.182 ( 0.182)	Loss 1.4648e-01 (1.4648e-01)	Acc@1  96.00 ( 96.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.048 ( 0.060)	Loss 2.8442e-01 (2.6545e-01)	Acc@1  91.00 ( 91.55)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.048 ( 0.055)	Loss 2.8247e-01 (2.8313e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 2.8882e-01 (2.9229e-01)	Acc@1  91.00 ( 91.19)	Acc@5 100.00 ( 99.77)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 4.4849e-01 (2.9673e-01)	Acc@1  89.00 ( 90.93)	Acc@5  99.00 ( 99.73)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.8860e-01 (2.9466e-01)	Acc@1  96.00 ( 90.92)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 3.0420e-01 (2.9376e-01)	Acc@1  92.00 ( 90.87)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.048 ( 0.049)	Loss 2.7856e-01 (2.9169e-01)	Acc@1  92.00 ( 90.92)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 2.2913e-01 (2.8857e-01)	Acc@1  92.00 ( 91.00)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 2.6562e-01 (2.8746e-01)	Acc@1  89.00 ( 90.95)	Acc@5 100.00 ( 99.81)
 * Acc@1 90.890 Acc@5 99.830
### epoch[31] execution time: 68.557608127594
EPOCH 32
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [32][  0/391]	Time  0.319 ( 0.319)	Data  0.126 ( 0.126)	Loss 7.8613e-02 (7.8613e-02)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [32][ 10/391]	Time  0.160 ( 0.175)	Data  0.001 ( 0.013)	Loss 1.0364e-01 (1.1346e-01)	Acc@1  96.88 ( 95.88)	Acc@5 100.00 (100.00)
Epoch: [32][ 20/391]	Time  0.161 ( 0.168)	Data  0.001 ( 0.007)	Loss 1.6882e-01 (1.2172e-01)	Acc@1  94.53 ( 95.57)	Acc@5 100.00 (100.00)
Epoch: [32][ 30/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.005)	Loss 8.1360e-02 (1.1983e-01)	Acc@1  97.66 ( 95.64)	Acc@5 100.00 (100.00)
Epoch: [32][ 40/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.9031e-01 (1.2329e-01)	Acc@1  94.53 ( 95.67)	Acc@5 100.00 ( 99.94)
Epoch: [32][ 50/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 5.5908e-02 (1.1841e-01)	Acc@1  97.66 ( 95.93)	Acc@5 100.00 ( 99.94)
Epoch: [32][ 60/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.6089e-01 (1.1875e-01)	Acc@1  96.88 ( 95.94)	Acc@5 100.00 ( 99.92)
Epoch: [32][ 70/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.0876e-01 (1.1764e-01)	Acc@1  94.53 ( 95.97)	Acc@5 100.00 ( 99.92)
Epoch: [32][ 80/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 7.8979e-02 (1.1603e-01)	Acc@1  96.88 ( 95.98)	Acc@5 100.00 ( 99.93)
Epoch: [32][ 90/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 5.7892e-02 (1.1459e-01)	Acc@1  98.44 ( 96.03)	Acc@5 100.00 ( 99.93)
Epoch: [32][100/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5771e-01 (1.1474e-01)	Acc@1  93.75 ( 96.02)	Acc@5 100.00 ( 99.94)
Epoch: [32][110/391]	Time  0.173 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.8816e-02 (1.1640e-01)	Acc@1  96.88 ( 95.90)	Acc@5 100.00 ( 99.94)
Epoch: [32][120/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.5939e-02 (1.1760e-01)	Acc@1  97.66 ( 95.87)	Acc@5 100.00 ( 99.95)
Epoch: [32][130/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0626e-01 (1.1702e-01)	Acc@1  96.88 ( 95.90)	Acc@5 100.00 ( 99.95)
Epoch: [32][140/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.8035e-02 (1.1637e-01)	Acc@1  99.22 ( 95.95)	Acc@5 100.00 ( 99.95)
Epoch: [32][150/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.7341e-02 (1.1634e-01)	Acc@1  96.09 ( 95.91)	Acc@5 100.00 ( 99.95)
Epoch: [32][160/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.2825e-02 (1.1554e-01)	Acc@1  96.09 ( 95.95)	Acc@5 100.00 ( 99.95)
Epoch: [32][170/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.1951e-02 (1.1486e-01)	Acc@1  97.66 ( 95.98)	Acc@5 100.00 ( 99.95)
Epoch: [32][180/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5723e-01 (1.1514e-01)	Acc@1  91.41 ( 95.96)	Acc@5 100.00 ( 99.95)
Epoch: [32][190/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7456e-01 (1.1520e-01)	Acc@1  91.41 ( 95.95)	Acc@5 100.00 ( 99.96)
Epoch: [32][200/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0138e-01 (1.1534e-01)	Acc@1  93.75 ( 95.95)	Acc@5 100.00 ( 99.95)
Epoch: [32][210/391]	Time  0.165 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.7089e-02 (1.1492e-01)	Acc@1 100.00 ( 95.95)	Acc@5 100.00 ( 99.94)
Epoch: [32][220/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0553e-01 (1.1427e-01)	Acc@1  95.31 ( 95.98)	Acc@5 100.00 ( 99.94)
Epoch: [32][230/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5979e-01 (1.1392e-01)	Acc@1  92.97 ( 95.97)	Acc@5 100.00 ( 99.94)
Epoch: [32][240/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.3669e-02 (1.1414e-01)	Acc@1  96.88 ( 95.94)	Acc@5 100.00 ( 99.94)
Epoch: [32][250/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4429e-01 (1.1440e-01)	Acc@1  97.66 ( 95.94)	Acc@5 100.00 ( 99.95)
Epoch: [32][260/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.0933e-02 (1.1388e-01)	Acc@1  96.88 ( 95.98)	Acc@5 100.00 ( 99.95)
Epoch: [32][270/391]	Time  0.177 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.0820e-02 (1.1381e-01)	Acc@1  97.66 ( 96.00)	Acc@5 100.00 ( 99.95)
Epoch: [32][280/391]	Time  0.170 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.2927e-02 (1.1305e-01)	Acc@1  97.66 ( 96.00)	Acc@5 100.00 ( 99.95)
Epoch: [32][290/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.7332e-02 (1.1308e-01)	Acc@1  98.44 ( 96.01)	Acc@5 100.00 ( 99.95)
Epoch: [32][300/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.1707e-02 (1.1310e-01)	Acc@1  96.88 ( 96.01)	Acc@5 100.00 ( 99.96)
Epoch: [32][310/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.3181e-02 (1.1344e-01)	Acc@1  96.09 ( 96.00)	Acc@5 100.00 ( 99.96)
Epoch: [32][320/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0718e-01 (1.1355e-01)	Acc@1  94.53 ( 96.00)	Acc@5 100.00 ( 99.96)
Epoch: [32][330/391]	Time  0.173 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.6782e-02 (1.1340e-01)	Acc@1  96.09 ( 96.02)	Acc@5 100.00 ( 99.96)
Epoch: [32][340/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.8884e-01 (1.1351e-01)	Acc@1  91.41 ( 96.03)	Acc@5 100.00 ( 99.96)
Epoch: [32][350/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.9751e-01 (1.1432e-01)	Acc@1  94.53 ( 96.00)	Acc@5  99.22 ( 99.96)
Epoch: [32][360/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.001)	Loss 9.7107e-02 (1.1420e-01)	Acc@1  96.88 ( 96.00)	Acc@5 100.00 ( 99.96)
Epoch: [32][370/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.001)	Loss 2.3596e-01 (1.1402e-01)	Acc@1  92.19 ( 96.00)	Acc@5 100.00 ( 99.96)
Epoch: [32][380/391]	Time  0.168 ( 0.163)	Data  0.001 ( 0.001)	Loss 1.2732e-01 (1.1401e-01)	Acc@1  95.31 ( 96.00)	Acc@5 100.00 ( 99.96)
Epoch: [32][390/391]	Time  0.118 ( 0.162)	Data  0.001 ( 0.001)	Loss 8.7891e-02 (1.1348e-01)	Acc@1  96.25 ( 96.01)	Acc@5 100.00 ( 99.96)
## e[32] optimizer.zero_grad (sum) time: 0.528223991394043
## e[32]       loss.backward (sum) time: 12.469282865524292
## e[32]      optimizer.step (sum) time: 25.723639488220215
## epoch[32] training(only) time: 63.60715889930725
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 1.6492e-01 (1.6492e-01)	Acc@1  94.00 ( 94.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.047 ( 0.060)	Loss 2.6221e-01 (2.6565e-01)	Acc@1  91.00 ( 91.45)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.048 ( 0.054)	Loss 2.7368e-01 (2.7799e-01)	Acc@1  90.00 ( 91.29)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 2.9907e-01 (2.8728e-01)	Acc@1  91.00 ( 91.48)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 4.6558e-01 (2.9106e-01)	Acc@1  87.00 ( 91.46)	Acc@5  98.00 ( 99.68)
Test: [ 50/100]	Time  0.055 ( 0.050)	Loss 2.1045e-01 (2.8891e-01)	Acc@1  95.00 ( 91.55)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.047 ( 0.049)	Loss 2.5586e-01 (2.8744e-01)	Acc@1  94.00 ( 91.44)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 2.9224e-01 (2.8625e-01)	Acc@1  94.00 ( 91.52)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.048 ( 0.049)	Loss 2.2522e-01 (2.8293e-01)	Acc@1  91.00 ( 91.68)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 2.0410e-01 (2.8257e-01)	Acc@1  90.00 ( 91.56)	Acc@5 100.00 ( 99.79)
 * Acc@1 91.450 Acc@5 99.800
### epoch[32] execution time: 68.56710815429688
EPOCH 33
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [33][  0/391]	Time  0.310 ( 0.310)	Data  0.139 ( 0.139)	Loss 7.5562e-02 (7.5562e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [33][ 10/391]	Time  0.170 ( 0.175)	Data  0.001 ( 0.014)	Loss 1.1853e-01 (1.0315e-01)	Acc@1  96.09 ( 96.52)	Acc@5 100.00 ( 99.93)
Epoch: [33][ 20/391]	Time  0.162 ( 0.168)	Data  0.001 ( 0.008)	Loss 1.4856e-01 (9.9801e-02)	Acc@1  95.31 ( 96.61)	Acc@5 100.00 ( 99.96)
Epoch: [33][ 30/391]	Time  0.161 ( 0.166)	Data  0.001 ( 0.006)	Loss 8.7036e-02 (9.6996e-02)	Acc@1  96.88 ( 96.82)	Acc@5 100.00 ( 99.97)
Epoch: [33][ 40/391]	Time  0.165 ( 0.166)	Data  0.001 ( 0.005)	Loss 7.0679e-02 (9.4083e-02)	Acc@1  97.66 ( 96.84)	Acc@5 100.00 ( 99.98)
Epoch: [33][ 50/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.0950e-01 (9.8307e-02)	Acc@1  95.31 ( 96.55)	Acc@5 100.00 ( 99.97)
Epoch: [33][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.0687e-01 (9.7336e-02)	Acc@1  96.09 ( 96.66)	Acc@5 100.00 ( 99.96)
Epoch: [33][ 70/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 6.3354e-02 (9.6341e-02)	Acc@1  98.44 ( 96.70)	Acc@5 100.00 ( 99.97)
Epoch: [33][ 80/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.2402e-01 (9.6589e-02)	Acc@1  96.09 ( 96.63)	Acc@5 100.00 ( 99.97)
Epoch: [33][ 90/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 6.4453e-02 (9.7156e-02)	Acc@1  97.66 ( 96.63)	Acc@5 100.00 ( 99.97)
Epoch: [33][100/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 6.7749e-02 (9.7730e-02)	Acc@1  98.44 ( 96.57)	Acc@5 100.00 ( 99.97)
Epoch: [33][110/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7041e-01 (9.8820e-02)	Acc@1  92.97 ( 96.48)	Acc@5 100.00 ( 99.97)
Epoch: [33][120/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.2529e-02 (1.0048e-01)	Acc@1  98.44 ( 96.46)	Acc@5 100.00 ( 99.97)
Epoch: [33][130/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0120e-01 (1.0066e-01)	Acc@1  97.66 ( 96.45)	Acc@5 100.00 ( 99.98)
Epoch: [33][140/391]	Time  0.176 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.8176e-02 (1.0073e-01)	Acc@1  98.44 ( 96.50)	Acc@5 100.00 ( 99.97)
Epoch: [33][150/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6467e-01 (1.0113e-01)	Acc@1  94.53 ( 96.48)	Acc@5 100.00 ( 99.97)
Epoch: [33][160/391]	Time  0.165 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7883e-01 (1.0273e-01)	Acc@1  94.53 ( 96.44)	Acc@5 100.00 ( 99.97)
Epoch: [33][170/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.1238e-02 (1.0390e-01)	Acc@1  97.66 ( 96.38)	Acc@5 100.00 ( 99.97)
Epoch: [33][180/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.5500e-02 (1.0364e-01)	Acc@1  98.44 ( 96.38)	Acc@5 100.00 ( 99.97)
Epoch: [33][190/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.3262e-02 (1.0289e-01)	Acc@1  96.09 ( 96.42)	Acc@5 100.00 ( 99.97)
Epoch: [33][200/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1456e-01 (1.0371e-01)	Acc@1  95.31 ( 96.40)	Acc@5 100.00 ( 99.97)
Epoch: [33][210/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2708e-01 (1.0386e-01)	Acc@1  93.75 ( 96.40)	Acc@5 100.00 ( 99.97)
Epoch: [33][220/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.6741e-02 (1.0356e-01)	Acc@1  96.88 ( 96.41)	Acc@5 100.00 ( 99.97)
Epoch: [33][230/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6455e-01 (1.0414e-01)	Acc@1  93.75 ( 96.37)	Acc@5 100.00 ( 99.97)
Epoch: [33][240/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.0669e-02 (1.0358e-01)	Acc@1  97.66 ( 96.40)	Acc@5 100.00 ( 99.97)
Epoch: [33][250/391]	Time  0.177 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3782e-01 (1.0340e-01)	Acc@1  96.09 ( 96.41)	Acc@5 100.00 ( 99.98)
Epoch: [33][260/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.4769e-02 (1.0381e-01)	Acc@1  99.22 ( 96.36)	Acc@5 100.00 ( 99.98)
Epoch: [33][270/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.4901e-02 (1.0403e-01)	Acc@1  97.66 ( 96.34)	Acc@5 100.00 ( 99.98)
Epoch: [33][280/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.1960e-02 (1.0391e-01)	Acc@1  97.66 ( 96.36)	Acc@5 100.00 ( 99.98)
Epoch: [33][290/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4673e-01 (1.0439e-01)	Acc@1  95.31 ( 96.34)	Acc@5 100.00 ( 99.98)
Epoch: [33][300/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2402e-01 (1.0462e-01)	Acc@1  93.75 ( 96.33)	Acc@5 100.00 ( 99.98)
Epoch: [33][310/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.0881e-02 (1.0439e-01)	Acc@1  97.66 ( 96.34)	Acc@5 100.00 ( 99.98)
Epoch: [33][320/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3220e-01 (1.0478e-01)	Acc@1  96.09 ( 96.31)	Acc@5 100.00 ( 99.98)
Epoch: [33][330/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.0383e-02 (1.0506e-01)	Acc@1  96.88 ( 96.31)	Acc@5 100.00 ( 99.98)
Epoch: [33][340/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.9121e-02 (1.0473e-01)	Acc@1  96.88 ( 96.32)	Acc@5 100.00 ( 99.98)
Epoch: [33][350/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8481e-01 (1.0496e-01)	Acc@1  92.19 ( 96.30)	Acc@5 100.00 ( 99.98)
Epoch: [33][360/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.0444e-02 (1.0445e-01)	Acc@1  96.09 ( 96.32)	Acc@5 100.00 ( 99.98)
Epoch: [33][370/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7273e-01 (1.0408e-01)	Acc@1  94.53 ( 96.34)	Acc@5 100.00 ( 99.98)
Epoch: [33][380/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.7761e-01 (1.0437e-01)	Acc@1  95.31 ( 96.34)	Acc@5  99.22 ( 99.98)
Epoch: [33][390/391]	Time  0.117 ( 0.162)	Data  0.001 ( 0.001)	Loss 3.8239e-02 (1.0367e-01)	Acc@1 100.00 ( 96.36)	Acc@5 100.00 ( 99.98)
## e[33] optimizer.zero_grad (sum) time: 0.5253949165344238
## e[33]       loss.backward (sum) time: 12.424836874008179
## e[33]      optimizer.step (sum) time: 25.77182650566101
## epoch[33] training(only) time: 63.55146646499634
# Switched to evaluate mode...
Test: [  0/100]	Time  0.182 ( 0.182)	Loss 1.6406e-01 (1.6406e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 2.8882e-01 (2.6201e-01)	Acc@1  92.00 ( 91.36)	Acc@5 100.00 ( 99.91)
Test: [ 20/100]	Time  0.049 ( 0.054)	Loss 3.5522e-01 (2.8364e-01)	Acc@1  90.00 ( 91.24)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 2.8320e-01 (2.9147e-01)	Acc@1  94.00 ( 91.39)	Acc@5 100.00 ( 99.77)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 4.6118e-01 (2.9252e-01)	Acc@1  89.00 ( 91.44)	Acc@5  98.00 ( 99.71)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 2.0044e-01 (2.8897e-01)	Acc@1  95.00 ( 91.57)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.047 ( 0.049)	Loss 2.5415e-01 (2.8770e-01)	Acc@1  95.00 ( 91.61)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 2.7734e-01 (2.8620e-01)	Acc@1  93.00 ( 91.65)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 2.0837e-01 (2.8298e-01)	Acc@1  92.00 ( 91.78)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 2.5684e-01 (2.8341e-01)	Acc@1  89.00 ( 91.68)	Acc@5 100.00 ( 99.80)
 * Acc@1 91.570 Acc@5 99.810
### epoch[33] execution time: 68.49855542182922
EPOCH 34
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [34][  0/391]	Time  0.330 ( 0.330)	Data  0.153 ( 0.153)	Loss 7.8613e-02 (7.8613e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [34][ 10/391]	Time  0.161 ( 0.177)	Data  0.001 ( 0.015)	Loss 4.0039e-02 (8.0561e-02)	Acc@1 100.00 ( 97.23)	Acc@5 100.00 (100.00)
Epoch: [34][ 20/391]	Time  0.160 ( 0.169)	Data  0.001 ( 0.008)	Loss 5.3406e-02 (8.5879e-02)	Acc@1  98.44 ( 96.95)	Acc@5 100.00 (100.00)
Epoch: [34][ 30/391]	Time  0.160 ( 0.167)	Data  0.001 ( 0.006)	Loss 1.0400e-01 (8.3161e-02)	Acc@1  96.09 ( 97.20)	Acc@5 100.00 (100.00)
Epoch: [34][ 40/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.005)	Loss 8.0200e-02 (8.4447e-02)	Acc@1  97.66 ( 97.16)	Acc@5 100.00 ( 99.98)
Epoch: [34][ 50/391]	Time  0.174 ( 0.166)	Data  0.001 ( 0.004)	Loss 1.3538e-01 (8.3325e-02)	Acc@1  95.31 ( 97.24)	Acc@5 100.00 ( 99.98)
Epoch: [34][ 60/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 9.9731e-02 (8.5035e-02)	Acc@1  96.09 ( 97.14)	Acc@5 100.00 ( 99.97)
Epoch: [34][ 70/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.003)	Loss 7.4890e-02 (8.7643e-02)	Acc@1  96.88 ( 96.97)	Acc@5 100.00 ( 99.98)
Epoch: [34][ 80/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.8323e-01 (9.0373e-02)	Acc@1  95.31 ( 96.96)	Acc@5 100.00 ( 99.97)
Epoch: [34][ 90/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 4.3915e-02 (8.9776e-02)	Acc@1  97.66 ( 96.95)	Acc@5 100.00 ( 99.97)
Epoch: [34][100/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 6.0730e-02 (8.8321e-02)	Acc@1  97.66 ( 97.02)	Acc@5 100.00 ( 99.97)
Epoch: [34][110/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 4.3091e-02 (8.9155e-02)	Acc@1  98.44 ( 96.97)	Acc@5 100.00 ( 99.97)
Epoch: [34][120/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.002)	Loss 7.2754e-02 (8.9922e-02)	Acc@1  98.44 ( 96.97)	Acc@5 100.00 ( 99.97)
Epoch: [34][130/391]	Time  0.173 ( 0.164)	Data  0.001 ( 0.002)	Loss 5.5420e-02 (9.0744e-02)	Acc@1  98.44 ( 96.96)	Acc@5 100.00 ( 99.98)
Epoch: [34][140/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.002)	Loss 1.2445e-01 (9.1602e-02)	Acc@1  94.53 ( 96.92)	Acc@5 100.00 ( 99.98)
Epoch: [34][150/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1249e-01 (9.1529e-02)	Acc@1  96.88 ( 96.93)	Acc@5 100.00 ( 99.96)
Epoch: [34][160/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.2073e-02 (9.1475e-02)	Acc@1  96.88 ( 96.91)	Acc@5 100.00 ( 99.97)
Epoch: [34][170/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.0251e-02 (9.0904e-02)	Acc@1  97.66 ( 96.92)	Acc@5 100.00 ( 99.97)
Epoch: [34][180/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.6111e-02 (9.0203e-02)	Acc@1  97.66 ( 96.94)	Acc@5 100.00 ( 99.97)
Epoch: [34][190/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.2603e-02 (9.0458e-02)	Acc@1  99.22 ( 96.92)	Acc@5 100.00 ( 99.97)
Epoch: [34][200/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.3051e-02 (9.0033e-02)	Acc@1  99.22 ( 96.94)	Acc@5 100.00 ( 99.97)
Epoch: [34][210/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6431e-01 (9.0486e-02)	Acc@1  92.97 ( 96.95)	Acc@5 100.00 ( 99.97)
Epoch: [34][220/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7969e-01 (9.1056e-02)	Acc@1  93.75 ( 96.92)	Acc@5 100.00 ( 99.97)
Epoch: [34][230/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0394e-01 (9.0379e-02)	Acc@1  96.88 ( 96.95)	Acc@5 100.00 ( 99.97)
Epoch: [34][240/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.9172e-02 (9.1500e-02)	Acc@1  96.09 ( 96.89)	Acc@5 100.00 ( 99.96)
Epoch: [34][250/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.8501e-02 (9.1688e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 ( 99.96)
Epoch: [34][260/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3379e-01 (9.2369e-02)	Acc@1  95.31 ( 96.86)	Acc@5 100.00 ( 99.96)
Epoch: [34][270/391]	Time  0.172 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.5215e-02 (9.2875e-02)	Acc@1  95.31 ( 96.84)	Acc@5 100.00 ( 99.96)
Epoch: [34][280/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.9783e-02 (9.2515e-02)	Acc@1  96.88 ( 96.85)	Acc@5 100.00 ( 99.96)
Epoch: [34][290/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.0699e-02 (9.2994e-02)	Acc@1  98.44 ( 96.83)	Acc@5 100.00 ( 99.96)
Epoch: [34][300/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.8469e-01 (9.2688e-02)	Acc@1  96.09 ( 96.84)	Acc@5  99.22 ( 99.96)
Epoch: [34][310/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.9041e-02 (9.2798e-02)	Acc@1  96.09 ( 96.83)	Acc@5 100.00 ( 99.96)
Epoch: [34][320/391]	Time  0.173 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0022e-01 (9.2911e-02)	Acc@1  96.09 ( 96.83)	Acc@5 100.00 ( 99.96)
Epoch: [34][330/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.4697e-02 (9.3339e-02)	Acc@1  96.88 ( 96.79)	Acc@5 100.00 ( 99.96)
Epoch: [34][340/391]	Time  0.170 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.2581e-02 (9.4262e-02)	Acc@1  96.88 ( 96.76)	Acc@5 100.00 ( 99.96)
Epoch: [34][350/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3196e-01 (9.4431e-02)	Acc@1  93.75 ( 96.76)	Acc@5 100.00 ( 99.96)
Epoch: [34][360/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2671e-01 (9.4487e-02)	Acc@1  96.09 ( 96.75)	Acc@5 100.00 ( 99.96)
Epoch: [34][370/391]	Time  0.172 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.5215e-02 (9.4320e-02)	Acc@1  96.88 ( 96.74)	Acc@5 100.00 ( 99.96)
Epoch: [34][380/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.3279e-01 (9.4584e-02)	Acc@1  92.19 ( 96.73)	Acc@5 100.00 ( 99.96)
Epoch: [34][390/391]	Time  0.116 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5356e-01 (9.4646e-02)	Acc@1  91.25 ( 96.72)	Acc@5 100.00 ( 99.96)
## e[34] optimizer.zero_grad (sum) time: 0.5256600379943848
## e[34]       loss.backward (sum) time: 12.412750005722046
## e[34]      optimizer.step (sum) time: 25.755358934402466
## epoch[34] training(only) time: 63.54400587081909
# Switched to evaluate mode...
Test: [  0/100]	Time  0.180 ( 0.180)	Loss 1.5442e-01 (1.5442e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.059)	Loss 2.8857e-01 (2.6669e-01)	Acc@1  90.00 ( 91.73)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.048 ( 0.054)	Loss 2.8125e-01 (2.8586e-01)	Acc@1  89.00 ( 91.33)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.048 ( 0.052)	Loss 3.1616e-01 (2.9799e-01)	Acc@1  93.00 ( 91.45)	Acc@5 100.00 ( 99.68)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 4.3213e-01 (3.0130e-01)	Acc@1  89.00 ( 91.37)	Acc@5  99.00 ( 99.63)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 1.8494e-01 (2.9885e-01)	Acc@1  96.00 ( 91.45)	Acc@5 100.00 ( 99.67)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 2.4768e-01 (2.9529e-01)	Acc@1  92.00 ( 91.49)	Acc@5 100.00 ( 99.69)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 2.9688e-01 (2.9249e-01)	Acc@1  93.00 ( 91.56)	Acc@5 100.00 ( 99.70)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 2.2168e-01 (2.9070e-01)	Acc@1  93.00 ( 91.68)	Acc@5 100.00 ( 99.72)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 2.4207e-01 (2.9067e-01)	Acc@1  91.00 ( 91.58)	Acc@5 100.00 ( 99.74)
 * Acc@1 91.440 Acc@5 99.750
### epoch[34] execution time: 68.496826171875
EPOCH 35
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [35][  0/391]	Time  0.325 ( 0.325)	Data  0.142 ( 0.142)	Loss 5.5786e-02 (5.5786e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [35][ 10/391]	Time  0.161 ( 0.176)	Data  0.001 ( 0.014)	Loss 2.4872e-02 (8.3255e-02)	Acc@1 100.00 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [35][ 20/391]	Time  0.161 ( 0.169)	Data  0.001 ( 0.008)	Loss 8.7158e-02 (8.0160e-02)	Acc@1  97.66 ( 97.14)	Acc@5 100.00 (100.00)
Epoch: [35][ 30/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.006)	Loss 1.2927e-01 (7.7111e-02)	Acc@1  96.88 ( 97.28)	Acc@5 100.00 (100.00)
Epoch: [35][ 40/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.004)	Loss 8.2520e-02 (8.1936e-02)	Acc@1  97.66 ( 97.18)	Acc@5 100.00 ( 99.98)
Epoch: [35][ 50/391]	Time  0.169 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.2988e-01 (8.7206e-02)	Acc@1  96.09 ( 97.00)	Acc@5 100.00 ( 99.98)
Epoch: [35][ 60/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.4294e-01 (8.7849e-02)	Acc@1  96.88 ( 96.98)	Acc@5 100.00 ( 99.99)
Epoch: [35][ 70/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.4197e-01 (8.6582e-02)	Acc@1  94.53 ( 97.04)	Acc@5 100.00 ( 99.99)
Epoch: [35][ 80/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 8.6304e-02 (8.7736e-02)	Acc@1  97.66 ( 97.04)	Acc@5 100.00 ( 99.99)
Epoch: [35][ 90/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 6.5552e-02 (8.8830e-02)	Acc@1  97.66 ( 96.97)	Acc@5 100.00 ( 99.98)
Epoch: [35][100/391]	Time  0.169 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.6040e-02 (8.7918e-02)	Acc@1  97.66 ( 97.02)	Acc@5 100.00 ( 99.98)
Epoch: [35][110/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.1736e-02 (8.7388e-02)	Acc@1  96.88 ( 97.07)	Acc@5 100.00 ( 99.98)
Epoch: [35][120/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.8562e-02 (8.6528e-02)	Acc@1  98.44 ( 97.11)	Acc@5 100.00 ( 99.97)
Epoch: [35][130/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1310e-01 (8.7876e-02)	Acc@1  96.88 ( 97.05)	Acc@5 100.00 ( 99.98)
Epoch: [35][140/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.3384e-02 (8.8081e-02)	Acc@1  97.66 ( 97.04)	Acc@5 100.00 ( 99.98)
Epoch: [35][150/391]	Time  0.170 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.1960e-02 (8.7460e-02)	Acc@1  97.66 ( 97.06)	Acc@5 100.00 ( 99.98)
Epoch: [35][160/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.9418e-02 (8.7508e-02)	Acc@1  97.66 ( 97.03)	Acc@5 100.00 ( 99.98)
Epoch: [35][170/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.6152e-02 (8.8258e-02)	Acc@1  99.22 ( 96.99)	Acc@5 100.00 ( 99.97)
Epoch: [35][180/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.1289e-02 (8.8286e-02)	Acc@1  98.44 ( 96.99)	Acc@5 100.00 ( 99.97)
Epoch: [35][190/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1353e-01 (8.8279e-02)	Acc@1  96.88 ( 96.99)	Acc@5 100.00 ( 99.97)
Epoch: [35][200/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.8726e-02 (8.8189e-02)	Acc@1  97.66 ( 97.00)	Acc@5 100.00 ( 99.97)
Epoch: [35][210/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.2327e-02 (8.8241e-02)	Acc@1  96.88 ( 96.97)	Acc@5 100.00 ( 99.97)
Epoch: [35][220/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.7107e-02 (8.8377e-02)	Acc@1  96.09 ( 96.97)	Acc@5  99.22 ( 99.97)
Epoch: [35][230/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4453e-01 (8.7770e-02)	Acc@1  96.09 ( 97.01)	Acc@5 100.00 ( 99.97)
Epoch: [35][240/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.1260e-02 (8.7855e-02)	Acc@1  99.22 ( 97.00)	Acc@5 100.00 ( 99.97)
Epoch: [35][250/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.2683e-02 (8.8777e-02)	Acc@1  97.66 ( 96.96)	Acc@5 100.00 ( 99.97)
Epoch: [35][260/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1621e-01 (8.9603e-02)	Acc@1  95.31 ( 96.93)	Acc@5 100.00 ( 99.97)
Epoch: [35][270/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.7524e-02 (8.9316e-02)	Acc@1  97.66 ( 96.93)	Acc@5 100.00 ( 99.97)
Epoch: [35][280/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3623e-01 (8.9410e-02)	Acc@1  95.31 ( 96.94)	Acc@5 100.00 ( 99.97)
Epoch: [35][290/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4160e-01 (8.9381e-02)	Acc@1  96.88 ( 96.94)	Acc@5  99.22 ( 99.97)
Epoch: [35][300/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.7341e-02 (8.9211e-02)	Acc@1  97.66 ( 96.94)	Acc@5 100.00 ( 99.97)
Epoch: [35][310/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.3933e-02 (8.8991e-02)	Acc@1  96.88 ( 96.95)	Acc@5 100.00 ( 99.97)
Epoch: [35][320/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.0710e-02 (8.8524e-02)	Acc@1  99.22 ( 96.97)	Acc@5 100.00 ( 99.97)
Epoch: [35][330/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.6782e-02 (8.8499e-02)	Acc@1  97.66 ( 96.96)	Acc@5 100.00 ( 99.97)
Epoch: [35][340/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.2476e-01 (8.8322e-02)	Acc@1  95.31 ( 96.96)	Acc@5 100.00 ( 99.97)
Epoch: [35][350/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.3855e-01 (8.8660e-02)	Acc@1  96.09 ( 96.94)	Acc@5 100.00 ( 99.97)
Epoch: [35][360/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.2964e-01 (8.8930e-02)	Acc@1  95.31 ( 96.94)	Acc@5 100.00 ( 99.97)
Epoch: [35][370/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 8.2642e-02 (8.9448e-02)	Acc@1  96.88 ( 96.91)	Acc@5 100.00 ( 99.97)
Epoch: [35][380/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 8.7524e-02 (8.9826e-02)	Acc@1  96.09 ( 96.89)	Acc@5 100.00 ( 99.97)
Epoch: [35][390/391]	Time  0.115 ( 0.162)	Data  0.001 ( 0.001)	Loss 6.0394e-02 (8.9664e-02)	Acc@1  98.75 ( 96.90)	Acc@5 100.00 ( 99.97)
## e[35] optimizer.zero_grad (sum) time: 0.5185177326202393
## e[35]       loss.backward (sum) time: 12.410831928253174
## e[35]      optimizer.step (sum) time: 25.76046848297119
## epoch[35] training(only) time: 63.372769594192505
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 1.6064e-01 (1.6064e-01)	Acc@1  94.00 ( 94.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.064 ( 0.062)	Loss 2.8101e-01 (2.6656e-01)	Acc@1  92.00 ( 91.36)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.048 ( 0.056)	Loss 3.5278e-01 (2.9161e-01)	Acc@1  89.00 ( 91.19)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.048 ( 0.053)	Loss 2.8491e-01 (2.9969e-01)	Acc@1  93.00 ( 91.68)	Acc@5 100.00 ( 99.77)
Test: [ 40/100]	Time  0.048 ( 0.052)	Loss 4.1797e-01 (2.9936e-01)	Acc@1  90.00 ( 91.54)	Acc@5  99.00 ( 99.73)
Test: [ 50/100]	Time  0.048 ( 0.051)	Loss 1.5161e-01 (2.9396e-01)	Acc@1  97.00 ( 91.69)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.046 ( 0.051)	Loss 2.6685e-01 (2.9047e-01)	Acc@1  94.00 ( 91.75)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 2.6538e-01 (2.8772e-01)	Acc@1  94.00 ( 91.80)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.047 ( 0.050)	Loss 2.3132e-01 (2.8507e-01)	Acc@1  91.00 ( 91.91)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 2.7344e-01 (2.8515e-01)	Acc@1  91.00 ( 91.92)	Acc@5 100.00 ( 99.78)
 * Acc@1 91.860 Acc@5 99.790
### epoch[35] execution time: 68.40089344978333
EPOCH 36
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [36][  0/391]	Time  0.327 ( 0.327)	Data  0.154 ( 0.154)	Loss 9.2468e-02 (9.2468e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [36][ 10/391]	Time  0.160 ( 0.176)	Data  0.001 ( 0.015)	Loss 5.8441e-02 (8.5308e-02)	Acc@1  97.66 ( 97.09)	Acc@5 100.00 (100.00)
Epoch: [36][ 20/391]	Time  0.160 ( 0.170)	Data  0.001 ( 0.008)	Loss 8.8379e-02 (8.1058e-02)	Acc@1  96.88 ( 97.40)	Acc@5 100.00 ( 99.96)
Epoch: [36][ 30/391]	Time  0.160 ( 0.167)	Data  0.001 ( 0.006)	Loss 5.7617e-02 (8.1320e-02)	Acc@1  98.44 ( 97.33)	Acc@5 100.00 ( 99.97)
Epoch: [36][ 40/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.005)	Loss 1.5320e-01 (8.2556e-02)	Acc@1  94.53 ( 97.24)	Acc@5 100.00 ( 99.98)
Epoch: [36][ 50/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.004)	Loss 3.7018e-02 (8.2508e-02)	Acc@1  99.22 ( 97.27)	Acc@5 100.00 ( 99.98)
Epoch: [36][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 9.0332e-02 (8.3530e-02)	Acc@1  96.88 ( 97.16)	Acc@5 100.00 ( 99.99)
Epoch: [36][ 70/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.0431e-01 (8.3331e-02)	Acc@1  96.09 ( 97.18)	Acc@5 100.00 ( 99.98)
Epoch: [36][ 80/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 9.9854e-02 (8.4432e-02)	Acc@1  92.97 ( 97.09)	Acc@5 100.00 ( 99.98)
Epoch: [36][ 90/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 5.4199e-02 (8.3358e-02)	Acc@1  98.44 ( 97.12)	Acc@5 100.00 ( 99.98)
Epoch: [36][100/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 6.1584e-02 (8.2174e-02)	Acc@1  97.66 ( 97.16)	Acc@5 100.00 ( 99.98)
Epoch: [36][110/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 9.1431e-02 (8.1646e-02)	Acc@1  96.09 ( 97.20)	Acc@5 100.00 ( 99.98)
Epoch: [36][120/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.1431e-02 (8.1538e-02)	Acc@1  96.88 ( 97.20)	Acc@5 100.00 ( 99.97)
Epoch: [36][130/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.3722e-02 (8.1309e-02)	Acc@1  99.22 ( 97.19)	Acc@5 100.00 ( 99.98)
Epoch: [36][140/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2238e-01 (8.1276e-02)	Acc@1  95.31 ( 97.16)	Acc@5 100.00 ( 99.98)
Epoch: [36][150/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5564e-01 (8.0908e-02)	Acc@1  94.53 ( 97.19)	Acc@5 100.00 ( 99.98)
Epoch: [36][160/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.5867e-02 (8.2328e-02)	Acc@1  98.44 ( 97.15)	Acc@5 100.00 ( 99.98)
Epoch: [36][170/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0999e-01 (8.2040e-02)	Acc@1  96.09 ( 97.16)	Acc@5 100.00 ( 99.98)
Epoch: [36][180/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.1809e-02 (8.2797e-02)	Acc@1  98.44 ( 97.13)	Acc@5 100.00 ( 99.98)
Epoch: [36][190/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4673e-01 (8.2512e-02)	Acc@1  92.97 ( 97.13)	Acc@5 100.00 ( 99.98)
Epoch: [36][200/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.2490e-02 (8.2279e-02)	Acc@1  97.66 ( 97.15)	Acc@5 100.00 ( 99.98)
Epoch: [36][210/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3403e-01 (8.3418e-02)	Acc@1  95.31 ( 97.12)	Acc@5 100.00 ( 99.99)
Epoch: [36][220/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.4727e-02 (8.3036e-02)	Acc@1  96.88 ( 97.14)	Acc@5 100.00 ( 99.99)
Epoch: [36][230/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0217e-01 (8.2950e-02)	Acc@1  96.09 ( 97.13)	Acc@5 100.00 ( 99.99)
Epoch: [36][240/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.3079e-02 (8.2970e-02)	Acc@1  95.31 ( 97.14)	Acc@5 100.00 ( 99.98)
Epoch: [36][250/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.2816e-02 (8.2952e-02)	Acc@1  98.44 ( 97.15)	Acc@5 100.00 ( 99.98)
Epoch: [36][260/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.1779e-02 (8.2554e-02)	Acc@1  98.44 ( 97.16)	Acc@5 100.00 ( 99.98)
Epoch: [36][270/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.7087e-02 (8.2809e-02)	Acc@1  96.88 ( 97.15)	Acc@5 100.00 ( 99.98)
Epoch: [36][280/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.6265e-02 (8.2279e-02)	Acc@1  99.22 ( 97.17)	Acc@5 100.00 ( 99.98)
Epoch: [36][290/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.9326e-02 (8.2090e-02)	Acc@1  98.44 ( 97.16)	Acc@5 100.00 ( 99.98)
Epoch: [36][300/391]	Time  0.166 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.0923e-02 (8.2715e-02)	Acc@1  95.31 ( 97.13)	Acc@5 100.00 ( 99.98)
Epoch: [36][310/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.0964e-02 (8.2623e-02)	Acc@1  99.22 ( 97.14)	Acc@5 100.00 ( 99.98)
Epoch: [36][320/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.4900e-02 (8.2703e-02)	Acc@1  98.44 ( 97.14)	Acc@5 100.00 ( 99.98)
Epoch: [36][330/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2646e-01 (8.2502e-02)	Acc@1  96.09 ( 97.16)	Acc@5 100.00 ( 99.98)
Epoch: [36][340/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3440e-01 (8.2795e-02)	Acc@1  95.31 ( 97.15)	Acc@5 100.00 ( 99.98)
Epoch: [36][350/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.1289e-02 (8.2490e-02)	Acc@1  96.88 ( 97.17)	Acc@5 100.00 ( 99.98)
Epoch: [36][360/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.9357e-02 (8.2265e-02)	Acc@1  96.88 ( 97.17)	Acc@5 100.00 ( 99.98)
Epoch: [36][370/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0083e-01 (8.2389e-02)	Acc@1  95.31 ( 97.18)	Acc@5 100.00 ( 99.98)
Epoch: [36][380/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.1218e-02 (8.2058e-02)	Acc@1  97.66 ( 97.19)	Acc@5 100.00 ( 99.98)
Epoch: [36][390/391]	Time  0.116 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.9235e-02 (8.2310e-02)	Acc@1  97.50 ( 97.19)	Acc@5 100.00 ( 99.98)
## e[36] optimizer.zero_grad (sum) time: 0.5239846706390381
## e[36]       loss.backward (sum) time: 12.465860366821289
## e[36]      optimizer.step (sum) time: 25.72987127304077
## epoch[36] training(only) time: 63.475165367126465
# Switched to evaluate mode...
Test: [  0/100]	Time  0.178 ( 0.178)	Loss 1.8713e-01 (1.8713e-01)	Acc@1  95.00 ( 95.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.048 ( 0.061)	Loss 3.3057e-01 (2.7650e-01)	Acc@1  90.00 ( 91.27)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.048 ( 0.055)	Loss 3.2861e-01 (2.8730e-01)	Acc@1  92.00 ( 91.33)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.048 ( 0.053)	Loss 3.1250e-01 (2.9729e-01)	Acc@1  91.00 ( 91.58)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.048 ( 0.052)	Loss 4.1650e-01 (3.0039e-01)	Acc@1  90.00 ( 91.61)	Acc@5  99.00 ( 99.68)
Test: [ 50/100]	Time  0.048 ( 0.051)	Loss 1.8298e-01 (2.9830e-01)	Acc@1  96.00 ( 91.73)	Acc@5 100.00 ( 99.69)
Test: [ 60/100]	Time  0.052 ( 0.051)	Loss 2.9175e-01 (2.9904e-01)	Acc@1  94.00 ( 91.75)	Acc@5 100.00 ( 99.72)
Test: [ 70/100]	Time  0.046 ( 0.051)	Loss 2.7808e-01 (2.9534e-01)	Acc@1  94.00 ( 91.76)	Acc@5 100.00 ( 99.73)
Test: [ 80/100]	Time  0.047 ( 0.050)	Loss 1.9702e-01 (2.9108e-01)	Acc@1  91.00 ( 91.85)	Acc@5 100.00 ( 99.74)
Test: [ 90/100]	Time  0.047 ( 0.050)	Loss 2.8442e-01 (2.9220e-01)	Acc@1  92.00 ( 91.84)	Acc@5 100.00 ( 99.75)
 * Acc@1 91.730 Acc@5 99.770
### epoch[36] execution time: 68.52901244163513
EPOCH 37
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [37][  0/391]	Time  0.327 ( 0.327)	Data  0.145 ( 0.145)	Loss 9.9976e-02 (9.9976e-02)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [37][ 10/391]	Time  0.161 ( 0.177)	Data  0.001 ( 0.014)	Loss 3.0762e-02 (7.9745e-02)	Acc@1  99.22 ( 97.16)	Acc@5 100.00 (100.00)
Epoch: [37][ 20/391]	Time  0.160 ( 0.169)	Data  0.001 ( 0.008)	Loss 7.4890e-02 (7.7681e-02)	Acc@1  96.88 ( 97.21)	Acc@5 100.00 (100.00)
Epoch: [37][ 30/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.006)	Loss 3.5583e-02 (7.0471e-02)	Acc@1  98.44 ( 97.43)	Acc@5 100.00 (100.00)
Epoch: [37][ 40/391]	Time  0.177 ( 0.165)	Data  0.001 ( 0.005)	Loss 4.8340e-02 (7.1558e-02)	Acc@1  98.44 ( 97.37)	Acc@5 100.00 (100.00)
Epoch: [37][ 50/391]	Time  0.162 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.0022e-01 (7.2344e-02)	Acc@1  95.31 ( 97.30)	Acc@5 100.00 (100.00)
Epoch: [37][ 60/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.004)	Loss 5.6671e-02 (7.2714e-02)	Acc@1  99.22 ( 97.31)	Acc@5 100.00 ( 99.99)
Epoch: [37][ 70/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.2500e-01 (7.2310e-02)	Acc@1  96.88 ( 97.37)	Acc@5 100.00 ( 99.99)
Epoch: [37][ 80/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.003)	Loss 5.1941e-02 (7.3407e-02)	Acc@1  98.44 ( 97.38)	Acc@5 100.00 ( 99.97)
Epoch: [37][ 90/391]	Time  0.173 ( 0.164)	Data  0.001 ( 0.003)	Loss 6.7566e-02 (7.2680e-02)	Acc@1  97.66 ( 97.39)	Acc@5 100.00 ( 99.97)
Epoch: [37][100/391]	Time  0.170 ( 0.164)	Data  0.001 ( 0.003)	Loss 3.3844e-02 (7.2796e-02)	Acc@1  99.22 ( 97.41)	Acc@5 100.00 ( 99.98)
Epoch: [37][110/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.002)	Loss 9.7961e-02 (7.3049e-02)	Acc@1  96.09 ( 97.40)	Acc@5 100.00 ( 99.98)
Epoch: [37][120/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.2786e-02 (7.2554e-02)	Acc@1  99.22 ( 97.44)	Acc@5 100.00 ( 99.98)
Epoch: [37][130/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.5806e-02 (7.2948e-02)	Acc@1  97.66 ( 97.41)	Acc@5 100.00 ( 99.98)
Epoch: [37][140/391]	Time  0.172 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5344e-01 (7.3389e-02)	Acc@1  96.09 ( 97.42)	Acc@5 100.00 ( 99.98)
Epoch: [37][150/391]	Time  0.165 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.3059e-02 (7.3939e-02)	Acc@1  96.88 ( 97.41)	Acc@5 100.00 ( 99.98)
Epoch: [37][160/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.8054e-02 (7.3940e-02)	Acc@1  98.44 ( 97.44)	Acc@5 100.00 ( 99.99)
Epoch: [37][170/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.1401e-02 (7.4479e-02)	Acc@1  96.88 ( 97.41)	Acc@5 100.00 ( 99.98)
Epoch: [37][180/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.8279e-02 (7.4146e-02)	Acc@1  99.22 ( 97.41)	Acc@5 100.00 ( 99.98)
Epoch: [37][190/391]	Time  0.172 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.2397e-02 (7.4420e-02)	Acc@1  96.09 ( 97.38)	Acc@5 100.00 ( 99.98)
Epoch: [37][200/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.1909e-02 (7.4283e-02)	Acc@1  97.66 ( 97.38)	Acc@5 100.00 ( 99.98)
Epoch: [37][210/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.6406e-02 (7.4158e-02)	Acc@1  98.44 ( 97.40)	Acc@5 100.00 ( 99.99)
Epoch: [37][220/391]	Time  0.165 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.3018e-02 (7.4877e-02)	Acc@1  96.09 ( 97.37)	Acc@5 100.00 ( 99.99)
Epoch: [37][230/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0706e-01 (7.4224e-02)	Acc@1  96.88 ( 97.41)	Acc@5 100.00 ( 99.99)
Epoch: [37][240/391]	Time  0.169 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.3478e-02 (7.4087e-02)	Acc@1  99.22 ( 97.41)	Acc@5 100.00 ( 99.99)
Epoch: [37][250/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.2510e-02 (7.3505e-02)	Acc@1  95.31 ( 97.42)	Acc@5 100.00 ( 99.99)
Epoch: [37][260/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3586e-01 (7.4409e-02)	Acc@1  95.31 ( 97.37)	Acc@5 100.00 ( 99.99)
Epoch: [37][270/391]	Time  0.172 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2207e-01 (7.4504e-02)	Acc@1  96.09 ( 97.37)	Acc@5 100.00 ( 99.99)
Epoch: [37][280/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.2887e-02 (7.5196e-02)	Acc@1  98.44 ( 97.33)	Acc@5 100.00 ( 99.99)
Epoch: [37][290/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0840e-01 (7.5898e-02)	Acc@1  96.88 ( 97.32)	Acc@5 100.00 ( 99.98)
Epoch: [37][300/391]	Time  0.166 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.5449e-02 (7.5871e-02)	Acc@1  95.31 ( 97.31)	Acc@5 100.00 ( 99.98)
Epoch: [37][310/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.8157e-02 (7.6268e-02)	Acc@1  97.66 ( 97.29)	Acc@5 100.00 ( 99.98)
Epoch: [37][320/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.001)	Loss 7.3425e-02 (7.6373e-02)	Acc@1  96.88 ( 97.28)	Acc@5 100.00 ( 99.98)
Epoch: [37][330/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.001)	Loss 1.1249e-01 (7.6901e-02)	Acc@1  96.09 ( 97.26)	Acc@5 100.00 ( 99.98)
Epoch: [37][340/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.001)	Loss 7.5012e-02 (7.7000e-02)	Acc@1  97.66 ( 97.26)	Acc@5 100.00 ( 99.98)
Epoch: [37][350/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.001)	Loss 5.3986e-02 (7.7151e-02)	Acc@1  97.66 ( 97.26)	Acc@5 100.00 ( 99.98)
Epoch: [37][360/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.001)	Loss 9.2407e-02 (7.7408e-02)	Acc@1  96.09 ( 97.26)	Acc@5 100.00 ( 99.98)
Epoch: [37][370/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.001)	Loss 1.0413e-01 (7.7379e-02)	Acc@1  97.66 ( 97.26)	Acc@5 100.00 ( 99.98)
Epoch: [37][380/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 4.3304e-02 (7.7271e-02)	Acc@1  98.44 ( 97.28)	Acc@5 100.00 ( 99.98)
Epoch: [37][390/391]	Time  0.117 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.0773e-01 (7.7540e-02)	Acc@1  95.00 ( 97.27)	Acc@5 100.00 ( 99.98)
## e[37] optimizer.zero_grad (sum) time: 0.5284328460693359
## e[37]       loss.backward (sum) time: 12.471458911895752
## e[37]      optimizer.step (sum) time: 25.71466302871704
## epoch[37] training(only) time: 63.58606219291687
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 1.7615e-01 (1.7615e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.048 ( 0.061)	Loss 2.6172e-01 (2.7579e-01)	Acc@1  93.00 ( 91.64)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.048 ( 0.055)	Loss 3.7378e-01 (2.9110e-01)	Acc@1  88.00 ( 91.48)	Acc@5 100.00 ( 99.81)
Test: [ 30/100]	Time  0.048 ( 0.053)	Loss 2.8174e-01 (2.9536e-01)	Acc@1  94.00 ( 91.81)	Acc@5 100.00 ( 99.77)
Test: [ 40/100]	Time  0.060 ( 0.052)	Loss 4.8120e-01 (2.9819e-01)	Acc@1  90.00 ( 91.80)	Acc@5  98.00 ( 99.71)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.8298e-01 (2.9665e-01)	Acc@1  96.00 ( 91.84)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 2.6367e-01 (2.9299e-01)	Acc@1  93.00 ( 91.92)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 3.0859e-01 (2.9244e-01)	Acc@1  92.00 ( 91.93)	Acc@5 100.00 ( 99.80)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.8115e-01 (2.8958e-01)	Acc@1  95.00 ( 92.02)	Acc@5 100.00 ( 99.81)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 2.6465e-01 (2.8988e-01)	Acc@1  91.00 ( 92.03)	Acc@5 100.00 ( 99.81)
 * Acc@1 91.840 Acc@5 99.820
### epoch[37] execution time: 68.60205626487732
EPOCH 38
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [38][  0/391]	Time  0.334 ( 0.334)	Data  0.158 ( 0.158)	Loss 3.8971e-02 (3.8971e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [38][ 10/391]	Time  0.161 ( 0.177)	Data  0.001 ( 0.015)	Loss 1.1823e-01 (8.7253e-02)	Acc@1  96.09 ( 97.09)	Acc@5 100.00 (100.00)
Epoch: [38][ 20/391]	Time  0.161 ( 0.170)	Data  0.001 ( 0.009)	Loss 5.0964e-02 (7.1926e-02)	Acc@1  99.22 ( 97.51)	Acc@5 100.00 (100.00)
Epoch: [38][ 30/391]	Time  0.160 ( 0.167)	Data  0.001 ( 0.006)	Loss 4.9408e-02 (7.1942e-02)	Acc@1  99.22 ( 97.61)	Acc@5 100.00 (100.00)
Epoch: [38][ 40/391]	Time  0.162 ( 0.166)	Data  0.001 ( 0.005)	Loss 4.5380e-02 (6.8647e-02)	Acc@1  99.22 ( 97.69)	Acc@5 100.00 (100.00)
Epoch: [38][ 50/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.004)	Loss 9.5154e-02 (6.8314e-02)	Acc@1  96.09 ( 97.61)	Acc@5 100.00 (100.00)
Epoch: [38][ 60/391]	Time  0.174 ( 0.166)	Data  0.001 ( 0.004)	Loss 1.3489e-01 (6.9817e-02)	Acc@1  94.53 ( 97.57)	Acc@5 100.00 (100.00)
Epoch: [38][ 70/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.003)	Loss 6.8787e-02 (7.1414e-02)	Acc@1  98.44 ( 97.52)	Acc@5 100.00 (100.00)
Epoch: [38][ 80/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.003)	Loss 7.6904e-02 (7.0948e-02)	Acc@1  96.88 ( 97.52)	Acc@5 100.00 (100.00)
Epoch: [38][ 90/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 5.7678e-02 (7.1320e-02)	Acc@1  97.66 ( 97.50)	Acc@5 100.00 (100.00)
Epoch: [38][100/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 5.7770e-02 (7.2790e-02)	Acc@1  99.22 ( 97.47)	Acc@5 100.00 (100.00)
Epoch: [38][110/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 4.7729e-02 (7.2209e-02)	Acc@1  99.22 ( 97.54)	Acc@5 100.00 (100.00)
Epoch: [38][120/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.002)	Loss 6.7932e-02 (7.2973e-02)	Acc@1  96.88 ( 97.53)	Acc@5 100.00 (100.00)
Epoch: [38][130/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.002)	Loss 3.7170e-02 (7.4048e-02)	Acc@1  99.22 ( 97.53)	Acc@5 100.00 (100.00)
Epoch: [38][140/391]	Time  0.163 ( 0.164)	Data  0.001 ( 0.002)	Loss 4.5654e-02 (7.4684e-02)	Acc@1  98.44 ( 97.51)	Acc@5 100.00 (100.00)
Epoch: [38][150/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1151e-01 (7.5068e-02)	Acc@1  97.66 ( 97.49)	Acc@5 100.00 (100.00)
Epoch: [38][160/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.1726e-02 (7.5663e-02)	Acc@1  97.66 ( 97.45)	Acc@5 100.00 (100.00)
Epoch: [38][170/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.3406e-02 (7.5312e-02)	Acc@1  96.88 ( 97.46)	Acc@5 100.00 (100.00)
Epoch: [38][180/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0425e-01 (7.5470e-02)	Acc@1  96.88 ( 97.45)	Acc@5 100.00 (100.00)
Epoch: [38][190/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.1819e-02 (7.5670e-02)	Acc@1  98.44 ( 97.44)	Acc@5 100.00 (100.00)
Epoch: [38][200/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.8013e-02 (7.5934e-02)	Acc@1  97.66 ( 97.43)	Acc@5 100.00 (100.00)
Epoch: [38][210/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.6488e-02 (7.5413e-02)	Acc@1  97.66 ( 97.42)	Acc@5 100.00 (100.00)
Epoch: [38][220/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.9602e-02 (7.5284e-02)	Acc@1  99.22 ( 97.44)	Acc@5 100.00 (100.00)
Epoch: [38][230/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.4148e-02 (7.4933e-02)	Acc@1  96.88 ( 97.45)	Acc@5 100.00 (100.00)
Epoch: [38][240/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.3486e-02 (7.4674e-02)	Acc@1  96.09 ( 97.46)	Acc@5 100.00 (100.00)
Epoch: [38][250/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.8359e-02 (7.4416e-02)	Acc@1  96.88 ( 97.47)	Acc@5 100.00 (100.00)
Epoch: [38][260/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.9712e-02 (7.4621e-02)	Acc@1  96.09 ( 97.46)	Acc@5 100.00 (100.00)
Epoch: [38][270/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.0075e-02 (7.3749e-02)	Acc@1  99.22 ( 97.50)	Acc@5 100.00 (100.00)
Epoch: [38][280/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.1697e-02 (7.3738e-02)	Acc@1  98.44 ( 97.51)	Acc@5 100.00 (100.00)
Epoch: [38][290/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.2407e-02 (7.4581e-02)	Acc@1  95.31 ( 97.48)	Acc@5 100.00 (100.00)
Epoch: [38][300/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.1962e-02 (7.4951e-02)	Acc@1  98.44 ( 97.46)	Acc@5 100.00 (100.00)
Epoch: [38][310/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.6223e-02 (7.4501e-02)	Acc@1  97.66 ( 97.46)	Acc@5 100.00 (100.00)
Epoch: [38][320/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.2134e-02 (7.4926e-02)	Acc@1  96.88 ( 97.45)	Acc@5 100.00 (100.00)
Epoch: [38][330/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.5693e-02 (7.5037e-02)	Acc@1  97.66 ( 97.45)	Acc@5 100.00 (100.00)
Epoch: [38][340/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.9814e-02 (7.4750e-02)	Acc@1  98.44 ( 97.46)	Acc@5 100.00 (100.00)
Epoch: [38][350/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.6101e-02 (7.4617e-02)	Acc@1  96.88 ( 97.46)	Acc@5 100.00 (100.00)
Epoch: [38][360/391]	Time  0.176 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.0945e-02 (7.4466e-02)	Acc@1 100.00 ( 97.47)	Acc@5 100.00 (100.00)
Epoch: [38][370/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.6355e-02 (7.4192e-02)	Acc@1  96.88 ( 97.48)	Acc@5 100.00 (100.00)
Epoch: [38][380/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1200e-01 (7.4032e-02)	Acc@1  98.44 ( 97.49)	Acc@5 100.00 (100.00)
Epoch: [38][390/391]	Time  0.113 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2482e-01 (7.4180e-02)	Acc@1  96.25 ( 97.49)	Acc@5  98.75 ( 99.99)
## e[38] optimizer.zero_grad (sum) time: 0.532280683517456
## e[38]       loss.backward (sum) time: 12.446054697036743
## e[38]      optimizer.step (sum) time: 25.734694242477417
## epoch[38] training(only) time: 63.618985176086426
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 1.7395e-01 (1.7395e-01)	Acc@1  94.00 ( 94.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.046 ( 0.059)	Loss 2.7759e-01 (2.8214e-01)	Acc@1  93.00 ( 92.00)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.047 ( 0.054)	Loss 3.9673e-01 (2.9593e-01)	Acc@1  90.00 ( 91.90)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.047 ( 0.051)	Loss 2.6953e-01 (3.0188e-01)	Acc@1  93.00 ( 91.90)	Acc@5 100.00 ( 99.71)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 4.3726e-01 (3.0349e-01)	Acc@1  91.00 ( 91.78)	Acc@5  99.00 ( 99.68)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 1.8774e-01 (3.0189e-01)	Acc@1  96.00 ( 91.78)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.047 ( 0.049)	Loss 2.6465e-01 (2.9833e-01)	Acc@1  93.00 ( 91.77)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 3.1567e-01 (2.9631e-01)	Acc@1  93.00 ( 91.76)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 2.3938e-01 (2.9324e-01)	Acc@1  91.00 ( 91.86)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 3.1030e-01 (2.9385e-01)	Acc@1  89.00 ( 91.81)	Acc@5 100.00 ( 99.78)
 * Acc@1 91.740 Acc@5 99.800
### epoch[38] execution time: 68.56738471984863
EPOCH 39
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [39][  0/391]	Time  0.327 ( 0.327)	Data  0.151 ( 0.151)	Loss 6.0150e-02 (6.0150e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [39][ 10/391]	Time  0.161 ( 0.176)	Data  0.001 ( 0.015)	Loss 1.0687e-01 (6.7462e-02)	Acc@1  95.31 ( 97.80)	Acc@5 100.00 (100.00)
Epoch: [39][ 20/391]	Time  0.161 ( 0.169)	Data  0.001 ( 0.008)	Loss 7.2937e-02 (6.4301e-02)	Acc@1  96.88 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [39][ 30/391]	Time  0.160 ( 0.167)	Data  0.001 ( 0.006)	Loss 3.3661e-02 (6.3640e-02)	Acc@1 100.00 ( 97.78)	Acc@5 100.00 ( 99.97)
Epoch: [39][ 40/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.005)	Loss 4.4220e-02 (6.2822e-02)	Acc@1  98.44 ( 97.75)	Acc@5 100.00 ( 99.98)
Epoch: [39][ 50/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.3184e-01 (6.7075e-02)	Acc@1  96.09 ( 97.70)	Acc@5 100.00 ( 99.98)
Epoch: [39][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.0297e-01 (6.6231e-02)	Acc@1  97.66 ( 97.76)	Acc@5 100.00 ( 99.99)
Epoch: [39][ 70/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 6.6528e-02 (6.7059e-02)	Acc@1  98.44 ( 97.69)	Acc@5 100.00 ( 99.99)
Epoch: [39][ 80/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.003)	Loss 6.1371e-02 (6.6923e-02)	Acc@1  97.66 ( 97.75)	Acc@5 100.00 ( 99.99)
Epoch: [39][ 90/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 6.9946e-02 (6.7205e-02)	Acc@1  98.44 ( 97.79)	Acc@5 100.00 ( 99.99)
Epoch: [39][100/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.4607e-02 (6.8282e-02)	Acc@1  99.22 ( 97.76)	Acc@5 100.00 ( 99.99)
Epoch: [39][110/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.003)	Loss 7.9102e-02 (6.7821e-02)	Acc@1  97.66 ( 97.78)	Acc@5 100.00 ( 99.99)
Epoch: [39][120/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.1433e-02 (6.9053e-02)	Acc@1  99.22 ( 97.75)	Acc@5 100.00 ( 99.99)
Epoch: [39][130/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.7209e-02 (6.9872e-02)	Acc@1  98.44 ( 97.69)	Acc@5 100.00 ( 99.99)
Epoch: [39][140/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.0515e-02 (7.0633e-02)	Acc@1  96.88 ( 97.65)	Acc@5 100.00 ( 99.98)
Epoch: [39][150/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.6854e-02 (7.0434e-02)	Acc@1  97.66 ( 97.63)	Acc@5 100.00 ( 99.98)
Epoch: [39][160/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.1565e-02 (6.9316e-02)	Acc@1  99.22 ( 97.68)	Acc@5 100.00 ( 99.99)
Epoch: [39][170/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.1909e-02 (6.9928e-02)	Acc@1  96.88 ( 97.66)	Acc@5 100.00 ( 99.98)
Epoch: [39][180/391]	Time  0.172 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.0374e-02 (7.0006e-02)	Acc@1  96.09 ( 97.63)	Acc@5 100.00 ( 99.98)
Epoch: [39][190/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.1055e-02 (6.9347e-02)	Acc@1  96.88 ( 97.66)	Acc@5 100.00 ( 99.98)
Epoch: [39][200/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.5675e-02 (6.8452e-02)	Acc@1  99.22 ( 97.68)	Acc@5 100.00 ( 99.98)
Epoch: [39][210/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4905e-01 (6.8790e-02)	Acc@1  93.75 ( 97.66)	Acc@5 100.00 ( 99.98)
Epoch: [39][220/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.5410e-02 (6.9137e-02)	Acc@1  99.22 ( 97.65)	Acc@5 100.00 ( 99.98)
Epoch: [39][230/391]	Time  0.172 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.4177e-02 (6.8977e-02)	Acc@1  96.09 ( 97.64)	Acc@5 100.00 ( 99.98)
Epoch: [39][240/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.5022e-02 (6.8989e-02)	Acc@1  95.31 ( 97.63)	Acc@5 100.00 ( 99.98)
Epoch: [39][250/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.4097e-02 (6.8454e-02)	Acc@1  96.88 ( 97.65)	Acc@5 100.00 ( 99.98)
Epoch: [39][260/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.5420e-02 (6.8438e-02)	Acc@1  97.66 ( 97.64)	Acc@5 100.00 ( 99.99)
Epoch: [39][270/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.2275e-02 (6.8019e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 ( 99.99)
Epoch: [39][280/391]	Time  0.173 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.5044e-02 (6.8653e-02)	Acc@1  98.44 ( 97.65)	Acc@5 100.00 ( 99.98)
Epoch: [39][290/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.1321e-02 (6.8756e-02)	Acc@1  98.44 ( 97.66)	Acc@5 100.00 ( 99.98)
Epoch: [39][300/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.5237e-02 (6.8685e-02)	Acc@1  99.22 ( 97.66)	Acc@5 100.00 ( 99.98)
Epoch: [39][310/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.8422e-02 (6.8872e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 ( 99.98)
Epoch: [39][320/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.4382e-02 (6.8697e-02)	Acc@1  97.66 ( 97.67)	Acc@5 100.00 ( 99.98)
Epoch: [39][330/391]	Time  0.172 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0040e-01 (6.9258e-02)	Acc@1  94.53 ( 97.64)	Acc@5 100.00 ( 99.98)
Epoch: [39][340/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.0618e-02 (6.9425e-02)	Acc@1  98.44 ( 97.62)	Acc@5 100.00 ( 99.98)
Epoch: [39][350/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.4006e-02 (6.9598e-02)	Acc@1  98.44 ( 97.62)	Acc@5 100.00 ( 99.98)
Epoch: [39][360/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0315e-01 (7.0028e-02)	Acc@1  96.09 ( 97.60)	Acc@5 100.00 ( 99.98)
Epoch: [39][370/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.1472e-02 (6.9939e-02)	Acc@1  97.66 ( 97.61)	Acc@5 100.00 ( 99.99)
Epoch: [39][380/391]	Time  0.177 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.1289e-02 (7.0315e-02)	Acc@1  96.09 ( 97.60)	Acc@5 100.00 ( 99.99)
Epoch: [39][390/391]	Time  0.115 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1749e-01 (7.0344e-02)	Acc@1  97.50 ( 97.61)	Acc@5 100.00 ( 99.98)
## e[39] optimizer.zero_grad (sum) time: 0.525444507598877
## e[39]       loss.backward (sum) time: 12.445432901382446
## e[39]      optimizer.step (sum) time: 25.74148201942444
## epoch[39] training(only) time: 63.50914692878723
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 1.5552e-01 (1.5552e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.048 ( 0.062)	Loss 2.8809e-01 (2.7333e-01)	Acc@1  94.00 ( 92.18)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 4.0259e-01 (2.9950e-01)	Acc@1  90.00 ( 91.86)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 2.8223e-01 (3.0201e-01)	Acc@1  92.00 ( 91.84)	Acc@5 100.00 ( 99.74)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 5.1221e-01 (3.0881e-01)	Acc@1  89.00 ( 91.68)	Acc@5  99.00 ( 99.73)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 2.3254e-01 (3.0850e-01)	Acc@1  95.00 ( 91.55)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 2.5732e-01 (3.0261e-01)	Acc@1  94.00 ( 91.70)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.057 ( 0.049)	Loss 3.2422e-01 (3.0113e-01)	Acc@1  93.00 ( 91.75)	Acc@5 100.00 ( 99.80)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 2.4146e-01 (2.9672e-01)	Acc@1  89.00 ( 91.85)	Acc@5 100.00 ( 99.81)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 2.5269e-01 (2.9524e-01)	Acc@1  93.00 ( 91.81)	Acc@5 100.00 ( 99.82)
 * Acc@1 91.750 Acc@5 99.830
### epoch[39] execution time: 68.47302436828613
EPOCH 40
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [40][  0/391]	Time  0.316 ( 0.316)	Data  0.146 ( 0.146)	Loss 6.2988e-02 (6.2988e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [40][ 10/391]	Time  0.160 ( 0.176)	Data  0.001 ( 0.014)	Loss 3.6621e-02 (6.4780e-02)	Acc@1  98.44 ( 97.73)	Acc@5 100.00 (100.00)
Epoch: [40][ 20/391]	Time  0.160 ( 0.169)	Data  0.001 ( 0.008)	Loss 4.3152e-02 (6.8146e-02)	Acc@1  98.44 ( 97.58)	Acc@5 100.00 (100.00)
Epoch: [40][ 30/391]	Time  0.162 ( 0.167)	Data  0.001 ( 0.006)	Loss 1.1346e-01 (6.9235e-02)	Acc@1  95.31 ( 97.63)	Acc@5 100.00 ( 99.97)
Epoch: [40][ 40/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.005)	Loss 7.0557e-02 (6.8080e-02)	Acc@1  97.66 ( 97.77)	Acc@5 100.00 ( 99.98)
Epoch: [40][ 50/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.004)	Loss 5.8746e-02 (6.4587e-02)	Acc@1  97.66 ( 97.86)	Acc@5 100.00 ( 99.98)
Epoch: [40][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 5.1788e-02 (6.3358e-02)	Acc@1  98.44 ( 97.98)	Acc@5 100.00 ( 99.97)
Epoch: [40][ 70/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.9424e-02 (6.2161e-02)	Acc@1 100.00 ( 98.04)	Acc@5 100.00 ( 99.98)
Epoch: [40][ 80/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.6443e-01 (6.1759e-02)	Acc@1  94.53 ( 98.02)	Acc@5 100.00 ( 99.98)
Epoch: [40][ 90/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 8.1787e-02 (6.1033e-02)	Acc@1  96.09 ( 98.03)	Acc@5 100.00 ( 99.98)
Epoch: [40][100/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 5.4596e-02 (6.0556e-02)	Acc@1  97.66 ( 98.03)	Acc@5 100.00 ( 99.98)
Epoch: [40][110/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3306e-01 (6.1995e-02)	Acc@1  96.88 ( 97.99)	Acc@5 100.00 ( 99.99)
Epoch: [40][120/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.6631e-02 (6.1105e-02)	Acc@1 100.00 ( 98.02)	Acc@5 100.00 ( 99.99)
Epoch: [40][130/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.6295e-02 (6.1415e-02)	Acc@1  98.44 ( 98.01)	Acc@5 100.00 ( 99.99)
Epoch: [40][140/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.0200e-02 (6.1696e-02)	Acc@1  96.88 ( 97.99)	Acc@5 100.00 ( 99.99)
Epoch: [40][150/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.6957e-02 (6.3368e-02)	Acc@1  97.66 ( 97.90)	Acc@5 100.00 ( 99.99)
Epoch: [40][160/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.7200e-02 (6.3711e-02)	Acc@1  97.66 ( 97.84)	Acc@5 100.00 ( 99.99)
Epoch: [40][170/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.4128e-02 (6.3680e-02)	Acc@1  98.44 ( 97.85)	Acc@5 100.00 ( 99.99)
Epoch: [40][180/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1993e-01 (6.3340e-02)	Acc@1  96.88 ( 97.87)	Acc@5 100.00 ( 99.99)
Epoch: [40][190/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.2755e-02 (6.3405e-02)	Acc@1  98.44 ( 97.86)	Acc@5 100.00 ( 99.99)
Epoch: [40][200/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.3740e-02 (6.3245e-02)	Acc@1  96.09 ( 97.85)	Acc@5 100.00 ( 99.99)
Epoch: [40][210/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.7556e-02 (6.3681e-02)	Acc@1  97.66 ( 97.81)	Acc@5 100.00 ( 99.99)
Epoch: [40][220/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.6487e-02 (6.3941e-02)	Acc@1  96.88 ( 97.80)	Acc@5 100.00 ( 99.99)
Epoch: [40][230/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.0669e-02 (6.4354e-02)	Acc@1  97.66 ( 97.79)	Acc@5 100.00 ( 99.99)
Epoch: [40][240/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.0446e-02 (6.4004e-02)	Acc@1  98.44 ( 97.81)	Acc@5 100.00 ( 99.99)
Epoch: [40][250/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.7791e-02 (6.3909e-02)	Acc@1  99.22 ( 97.81)	Acc@5 100.00 ( 99.99)
Epoch: [40][260/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.5634e-02 (6.3528e-02)	Acc@1  96.88 ( 97.82)	Acc@5 100.00 ( 99.99)
Epoch: [40][270/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.0881e-02 (6.4372e-02)	Acc@1  97.66 ( 97.78)	Acc@5 100.00 ( 99.99)
Epoch: [40][280/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.9580e-02 (6.4447e-02)	Acc@1  97.66 ( 97.77)	Acc@5 100.00 ( 99.99)
Epoch: [40][290/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0461e-01 (6.4923e-02)	Acc@1  96.88 ( 97.76)	Acc@5 100.00 ( 99.99)
Epoch: [40][300/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.0863e-02 (6.5243e-02)	Acc@1  99.22 ( 97.74)	Acc@5 100.00 ( 99.99)
Epoch: [40][310/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.4535e-02 (6.5199e-02)	Acc@1  96.88 ( 97.73)	Acc@5 100.00 ( 99.99)
Epoch: [40][320/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.2927e-02 (6.5385e-02)	Acc@1  98.44 ( 97.70)	Acc@5 100.00 ( 99.99)
Epoch: [40][330/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.8308e-02 (6.5722e-02)	Acc@1  98.44 ( 97.69)	Acc@5 100.00 ( 99.99)
Epoch: [40][340/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.4778e-02 (6.5646e-02)	Acc@1  96.88 ( 97.69)	Acc@5 100.00 ( 99.99)
Epoch: [40][350/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.2439e-02 (6.5789e-02)	Acc@1  97.66 ( 97.69)	Acc@5 100.00 ( 99.99)
Epoch: [40][360/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.0044e-01 (6.6564e-02)	Acc@1  92.19 ( 97.66)	Acc@5 100.00 ( 99.99)
Epoch: [40][370/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.7526e-02 (6.6089e-02)	Acc@1  97.66 ( 97.68)	Acc@5 100.00 ( 99.99)
Epoch: [40][380/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1823e-01 (6.6267e-02)	Acc@1  96.88 ( 97.66)	Acc@5 100.00 ( 99.99)
Epoch: [40][390/391]	Time  0.116 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.4473e-02 (6.6582e-02)	Acc@1  95.00 ( 97.65)	Acc@5 100.00 ( 99.98)
## e[40] optimizer.zero_grad (sum) time: 0.526606559753418
## e[40]       loss.backward (sum) time: 12.452383279800415
## e[40]      optimizer.step (sum) time: 25.763908624649048
## epoch[40] training(only) time: 63.50276303291321
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 1.5088e-01 (1.5088e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 2.9053e-01 (2.8136e-01)	Acc@1  91.00 ( 91.00)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 3.6768e-01 (2.9514e-01)	Acc@1  90.00 ( 91.33)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 3.3594e-01 (3.0548e-01)	Acc@1  91.00 ( 91.52)	Acc@5 100.00 ( 99.77)
Test: [ 40/100]	Time  0.048 ( 0.051)	Loss 4.6631e-01 (3.0761e-01)	Acc@1  91.00 ( 91.68)	Acc@5  99.00 ( 99.73)
Test: [ 50/100]	Time  0.048 ( 0.050)	Loss 1.7847e-01 (3.0768e-01)	Acc@1  95.00 ( 91.65)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.048 ( 0.050)	Loss 3.1348e-01 (3.0470e-01)	Acc@1  91.00 ( 91.57)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.048 ( 0.050)	Loss 2.8784e-01 (3.0360e-01)	Acc@1  94.00 ( 91.61)	Acc@5 100.00 ( 99.82)
Test: [ 80/100]	Time  0.047 ( 0.050)	Loss 2.5073e-01 (3.0030e-01)	Acc@1  92.00 ( 91.70)	Acc@5 100.00 ( 99.83)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 2.7954e-01 (3.0110e-01)	Acc@1  92.00 ( 91.70)	Acc@5 100.00 ( 99.82)
 * Acc@1 91.650 Acc@5 99.820
### epoch[40] execution time: 68.5058798789978
EPOCH 41
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [41][  0/391]	Time  0.310 ( 0.310)	Data  0.141 ( 0.141)	Loss 7.5562e-02 (7.5562e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [41][ 10/391]	Time  0.161 ( 0.175)	Data  0.001 ( 0.014)	Loss 7.8796e-02 (7.1614e-02)	Acc@1  96.09 ( 97.37)	Acc@5 100.00 (100.00)
Epoch: [41][ 20/391]	Time  0.160 ( 0.168)	Data  0.001 ( 0.008)	Loss 9.0149e-02 (6.2022e-02)	Acc@1  96.88 ( 97.84)	Acc@5 100.00 (100.00)
Epoch: [41][ 30/391]	Time  0.162 ( 0.166)	Data  0.001 ( 0.006)	Loss 1.6968e-02 (5.8073e-02)	Acc@1 100.00 ( 98.06)	Acc@5 100.00 (100.00)
Epoch: [41][ 40/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.005)	Loss 4.2786e-02 (5.6990e-02)	Acc@1  99.22 ( 98.04)	Acc@5 100.00 (100.00)
Epoch: [41][ 50/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.004)	Loss 3.3386e-02 (5.6678e-02)	Acc@1  99.22 ( 98.04)	Acc@5 100.00 (100.00)
Epoch: [41][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 7.6355e-02 (5.8067e-02)	Acc@1  95.31 ( 97.98)	Acc@5 100.00 (100.00)
Epoch: [41][ 70/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 5.0018e-02 (5.7955e-02)	Acc@1  98.44 ( 98.01)	Acc@5 100.00 (100.00)
Epoch: [41][ 80/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 9.6130e-02 (5.9842e-02)	Acc@1  98.44 ( 97.96)	Acc@5 100.00 (100.00)
Epoch: [41][ 90/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.8900e-02 (5.8389e-02)	Acc@1  99.22 ( 98.06)	Acc@5 100.00 (100.00)
Epoch: [41][100/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 5.0720e-02 (5.9814e-02)	Acc@1  98.44 ( 98.02)	Acc@5 100.00 (100.00)
Epoch: [41][110/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.4840e-02 (6.0423e-02)	Acc@1  98.44 ( 97.99)	Acc@5 100.00 (100.00)
Epoch: [41][120/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.8940e-02 (6.0431e-02)	Acc@1  96.88 ( 97.97)	Acc@5 100.00 (100.00)
Epoch: [41][130/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.5023e-02 (6.0004e-02)	Acc@1  97.66 ( 97.96)	Acc@5 100.00 (100.00)
Epoch: [41][140/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.8950e-02 (5.9819e-02)	Acc@1  96.88 ( 97.93)	Acc@5 100.00 (100.00)
Epoch: [41][150/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.6091e-02 (6.0000e-02)	Acc@1  98.44 ( 97.91)	Acc@5 100.00 (100.00)
Epoch: [41][160/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.2598e-02 (6.0244e-02)	Acc@1 100.00 ( 97.93)	Acc@5 100.00 (100.00)
Epoch: [41][170/391]	Time  0.169 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.3314e-02 (5.9821e-02)	Acc@1  98.44 ( 97.93)	Acc@5 100.00 (100.00)
Epoch: [41][180/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.3650e-02 (5.9856e-02)	Acc@1  96.88 ( 97.92)	Acc@5 100.00 (100.00)
Epoch: [41][190/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.7271e-02 (5.9695e-02)	Acc@1  96.88 ( 97.92)	Acc@5 100.00 (100.00)
Epoch: [41][200/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0791e-01 (5.9782e-02)	Acc@1  95.31 ( 97.91)	Acc@5 100.00 (100.00)
Epoch: [41][210/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.5939e-02 (5.9972e-02)	Acc@1  98.44 ( 97.91)	Acc@5 100.00 (100.00)
Epoch: [41][220/391]	Time  0.173 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.2042e-02 (5.9383e-02)	Acc@1  97.66 ( 97.94)	Acc@5 100.00 (100.00)
Epoch: [41][230/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.0129e-02 (5.9729e-02)	Acc@1  96.88 ( 97.93)	Acc@5 100.00 (100.00)
Epoch: [41][240/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.4565e-02 (5.9221e-02)	Acc@1  98.44 ( 97.94)	Acc@5 100.00 (100.00)
Epoch: [41][250/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.2327e-02 (5.9793e-02)	Acc@1  98.44 ( 97.91)	Acc@5 100.00 (100.00)
Epoch: [41][260/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.4880e-02 (5.9850e-02)	Acc@1  97.66 ( 97.90)	Acc@5 100.00 (100.00)
Epoch: [41][270/391]	Time  0.174 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.4219e-02 (5.9713e-02)	Acc@1  98.44 ( 97.91)	Acc@5 100.00 (100.00)
Epoch: [41][280/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.8584e-02 (5.9690e-02)	Acc@1  98.44 ( 97.91)	Acc@5 100.00 (100.00)
Epoch: [41][290/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.8950e-02 (5.9263e-02)	Acc@1  99.22 ( 97.94)	Acc@5 100.00 (100.00)
Epoch: [41][300/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.0557e-02 (5.9792e-02)	Acc@1  98.44 ( 97.91)	Acc@5 100.00 (100.00)
Epoch: [41][310/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.8237e-02 (5.9690e-02)	Acc@1  96.88 ( 97.92)	Acc@5 100.00 (100.00)
Epoch: [41][320/391]	Time  0.170 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.5847e-02 (5.9478e-02)	Acc@1  98.44 ( 97.93)	Acc@5 100.00 (100.00)
Epoch: [41][330/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.3314e-02 (5.9873e-02)	Acc@1  98.44 ( 97.92)	Acc@5 100.00 (100.00)
Epoch: [41][340/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5271e-01 (6.0164e-02)	Acc@1  95.31 ( 97.92)	Acc@5 100.00 (100.00)
Epoch: [41][350/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.4270e-02 (6.0454e-02)	Acc@1  97.66 ( 97.91)	Acc@5 100.00 (100.00)
Epoch: [41][360/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.9001e-02 (6.0421e-02)	Acc@1  98.44 ( 97.91)	Acc@5 100.00 (100.00)
Epoch: [41][370/391]	Time  0.172 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.3354e-02 (6.0580e-02)	Acc@1  98.44 ( 97.90)	Acc@5 100.00 (100.00)
Epoch: [41][380/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.0190e-02 (6.0595e-02)	Acc@1  97.66 ( 97.90)	Acc@5 100.00 (100.00)
Epoch: [41][390/391]	Time  0.114 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.4943e-02 (6.0712e-02)	Acc@1  98.75 ( 97.90)	Acc@5 100.00 (100.00)
## e[41] optimizer.zero_grad (sum) time: 0.5209908485412598
## e[41]       loss.backward (sum) time: 12.450039863586426
## e[41]      optimizer.step (sum) time: 25.71346402168274
## epoch[41] training(only) time: 63.48106575012207
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 1.5845e-01 (1.5845e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.048 ( 0.061)	Loss 3.2471e-01 (2.8829e-01)	Acc@1  92.00 ( 92.09)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.048 ( 0.055)	Loss 3.7988e-01 (3.0262e-01)	Acc@1  90.00 ( 92.10)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.048 ( 0.053)	Loss 3.3179e-01 (3.1201e-01)	Acc@1  92.00 ( 91.81)	Acc@5  99.00 ( 99.68)
Test: [ 40/100]	Time  0.048 ( 0.052)	Loss 4.7656e-01 (3.1779e-01)	Acc@1  90.00 ( 91.76)	Acc@5  99.00 ( 99.66)
Test: [ 50/100]	Time  0.048 ( 0.051)	Loss 1.9385e-01 (3.1627e-01)	Acc@1  95.00 ( 91.69)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.048 ( 0.051)	Loss 2.7930e-01 (3.1350e-01)	Acc@1  93.00 ( 91.72)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.048 ( 0.051)	Loss 2.9028e-01 (3.0996e-01)	Acc@1  93.00 ( 91.80)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 2.9175e-01 (3.0941e-01)	Acc@1  89.00 ( 91.84)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.047 ( 0.050)	Loss 2.6514e-01 (3.1019e-01)	Acc@1  91.00 ( 91.78)	Acc@5 100.00 ( 99.80)
 * Acc@1 91.700 Acc@5 99.800
### epoch[41] execution time: 68.53930997848511
EPOCH 42
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [42][  0/391]	Time  0.335 ( 0.335)	Data  0.154 ( 0.154)	Loss 4.5319e-02 (4.5319e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [42][ 10/391]	Time  0.161 ( 0.176)	Data  0.001 ( 0.015)	Loss 5.0781e-02 (7.0215e-02)	Acc@1  97.66 ( 97.09)	Acc@5 100.00 (100.00)
Epoch: [42][ 20/391]	Time  0.161 ( 0.169)	Data  0.001 ( 0.008)	Loss 3.1128e-02 (6.2820e-02)	Acc@1  99.22 ( 97.62)	Acc@5 100.00 (100.00)
Epoch: [42][ 30/391]	Time  0.160 ( 0.167)	Data  0.001 ( 0.006)	Loss 6.5369e-02 (6.1184e-02)	Acc@1  96.88 ( 97.73)	Acc@5 100.00 (100.00)
Epoch: [42][ 40/391]	Time  0.161 ( 0.166)	Data  0.001 ( 0.005)	Loss 5.0323e-02 (5.8950e-02)	Acc@1  98.44 ( 97.79)	Acc@5  99.22 ( 99.98)
Epoch: [42][ 50/391]	Time  0.169 ( 0.165)	Data  0.001 ( 0.004)	Loss 3.2349e-02 (5.6714e-02)	Acc@1  98.44 ( 97.89)	Acc@5 100.00 ( 99.98)
Epoch: [42][ 60/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.004)	Loss 9.4788e-02 (6.0910e-02)	Acc@1  96.88 ( 97.80)	Acc@5 100.00 ( 99.97)
Epoch: [42][ 70/391]	Time  0.176 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.9348e-02 (5.9979e-02)	Acc@1 100.00 ( 97.83)	Acc@5 100.00 ( 99.98)
Epoch: [42][ 80/391]	Time  0.173 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.7700e-02 (5.9642e-02)	Acc@1 100.00 ( 97.89)	Acc@5 100.00 ( 99.98)
Epoch: [42][ 90/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 8.9294e-02 (5.9686e-02)	Acc@1  96.88 ( 97.90)	Acc@5 100.00 ( 99.98)
Epoch: [42][100/391]	Time  0.173 ( 0.164)	Data  0.001 ( 0.003)	Loss 9.8145e-02 (6.0230e-02)	Acc@1  95.31 ( 97.86)	Acc@5 100.00 ( 99.98)
Epoch: [42][110/391]	Time  0.163 ( 0.164)	Data  0.001 ( 0.003)	Loss 5.3406e-02 (6.0232e-02)	Acc@1  98.44 ( 97.85)	Acc@5 100.00 ( 99.99)
Epoch: [42][120/391]	Time  0.165 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.5909e-02 (5.9064e-02)	Acc@1  99.22 ( 97.86)	Acc@5 100.00 ( 99.99)
Epoch: [42][130/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.0669e-02 (5.8351e-02)	Acc@1  97.66 ( 97.88)	Acc@5 100.00 ( 99.99)
Epoch: [42][140/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.7830e-02 (5.7627e-02)	Acc@1  96.88 ( 97.92)	Acc@5 100.00 ( 99.99)
Epoch: [42][150/391]	Time  0.168 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.5806e-02 (5.7443e-02)	Acc@1  97.66 ( 97.91)	Acc@5 100.00 ( 99.99)
Epoch: [42][160/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.2734e-02 (5.8294e-02)	Acc@1  98.44 ( 97.89)	Acc@5 100.00 ( 99.99)
Epoch: [42][170/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.3425e-02 (5.8492e-02)	Acc@1  97.66 ( 97.89)	Acc@5 100.00 ( 99.99)
Epoch: [42][180/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.6276e-02 (5.9366e-02)	Acc@1  98.44 ( 97.90)	Acc@5 100.00 ( 99.99)
Epoch: [42][190/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.3589e-02 (5.9513e-02)	Acc@1  98.44 ( 97.89)	Acc@5 100.00 ( 99.99)
Epoch: [42][200/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.8633e-02 (5.9222e-02)	Acc@1  97.66 ( 97.91)	Acc@5 100.00 ( 99.99)
Epoch: [42][210/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.8257e-02 (5.9361e-02)	Acc@1  98.44 ( 97.91)	Acc@5 100.00 ( 99.99)
Epoch: [42][220/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.0935e-02 (5.9139e-02)	Acc@1  99.22 ( 97.90)	Acc@5 100.00 ( 99.99)
Epoch: [42][230/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.2349e-02 (5.9506e-02)	Acc@1  98.44 ( 97.89)	Acc@5 100.00 ( 99.99)
Epoch: [42][240/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.8370e-02 (5.9818e-02)	Acc@1  98.44 ( 97.89)	Acc@5 100.00 ( 99.99)
Epoch: [42][250/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.8064e-02 (5.9689e-02)	Acc@1  96.09 ( 97.89)	Acc@5 100.00 ( 99.99)
Epoch: [42][260/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.6234e-02 (5.9382e-02)	Acc@1  98.44 ( 97.90)	Acc@5 100.00 ( 99.99)
Epoch: [42][270/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.2632e-02 (5.9536e-02)	Acc@1  97.66 ( 97.90)	Acc@5 100.00 ( 99.99)
Epoch: [42][280/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.5928e-02 (5.9489e-02)	Acc@1  98.44 ( 97.90)	Acc@5 100.00 ( 99.99)
Epoch: [42][290/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.7760e-02 (5.9786e-02)	Acc@1  99.22 ( 97.88)	Acc@5 100.00 ( 99.99)
Epoch: [42][300/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.7454e-02 (5.9904e-02)	Acc@1  96.88 ( 97.89)	Acc@5 100.00 ( 99.99)
Epoch: [42][310/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.0964e-02 (6.0068e-02)	Acc@1  98.44 ( 97.87)	Acc@5 100.00 ( 99.99)
Epoch: [42][320/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.3425e-02 (5.9905e-02)	Acc@1  96.88 ( 97.88)	Acc@5 100.00 (100.00)
Epoch: [42][330/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.0364e-02 (6.0291e-02)	Acc@1  98.44 ( 97.86)	Acc@5 100.00 (100.00)
Epoch: [42][340/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.2642e-02 (6.0286e-02)	Acc@1  97.66 ( 97.88)	Acc@5 100.00 ( 99.99)
Epoch: [42][350/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.4551e-02 (6.0227e-02)	Acc@1  99.22 ( 97.89)	Acc@5 100.00 ( 99.99)
Epoch: [42][360/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.3750e-02 (6.0353e-02)	Acc@1  96.09 ( 97.89)	Acc@5 100.00 ( 99.99)
Epoch: [42][370/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.7598e-02 (6.0530e-02)	Acc@1  99.22 ( 97.88)	Acc@5 100.00 ( 99.99)
Epoch: [42][380/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.5674e-02 (6.0587e-02)	Acc@1  97.66 ( 97.89)	Acc@5 100.00 ( 99.99)
Epoch: [42][390/391]	Time  0.118 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5686e-01 (6.0400e-02)	Acc@1  96.25 ( 97.90)	Acc@5 100.00 ( 99.99)
## e[42] optimizer.zero_grad (sum) time: 0.5290830135345459
## e[42]       loss.backward (sum) time: 12.411503076553345
## e[42]      optimizer.step (sum) time: 25.795987129211426
## epoch[42] training(only) time: 63.57406759262085
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 1.5625e-01 (1.5625e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.064 ( 0.062)	Loss 3.0835e-01 (2.8334e-01)	Acc@1  91.00 ( 91.91)	Acc@5 100.00 ( 99.64)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 4.0479e-01 (2.9985e-01)	Acc@1  90.00 ( 91.81)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 3.3447e-01 (3.0676e-01)	Acc@1  92.00 ( 91.94)	Acc@5  99.00 ( 99.71)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 4.6826e-01 (3.1475e-01)	Acc@1  90.00 ( 91.78)	Acc@5  99.00 ( 99.68)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 1.8909e-01 (3.1619e-01)	Acc@1  96.00 ( 91.80)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 2.2974e-01 (3.1244e-01)	Acc@1  95.00 ( 91.84)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 3.3887e-01 (3.1007e-01)	Acc@1  93.00 ( 91.89)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 2.1008e-01 (3.0768e-01)	Acc@1  93.00 ( 91.90)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 2.8442e-01 (3.0844e-01)	Acc@1  92.00 ( 91.79)	Acc@5 100.00 ( 99.78)
 * Acc@1 91.690 Acc@5 99.790
### epoch[42] execution time: 68.55591082572937
EPOCH 43
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [43][  0/391]	Time  0.314 ( 0.314)	Data  0.138 ( 0.138)	Loss 7.5195e-02 (7.5195e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [43][ 10/391]	Time  0.160 ( 0.175)	Data  0.001 ( 0.013)	Loss 5.4749e-02 (4.7637e-02)	Acc@1  96.88 ( 98.15)	Acc@5 100.00 (100.00)
Epoch: [43][ 20/391]	Time  0.162 ( 0.169)	Data  0.001 ( 0.008)	Loss 5.2124e-02 (5.7518e-02)	Acc@1  98.44 ( 97.81)	Acc@5 100.00 (100.00)
Epoch: [43][ 30/391]	Time  0.161 ( 0.166)	Data  0.001 ( 0.005)	Loss 6.0425e-02 (5.5264e-02)	Acc@1  97.66 ( 98.06)	Acc@5 100.00 (100.00)
Epoch: [43][ 40/391]	Time  0.162 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.2256e-01 (5.7116e-02)	Acc@1  94.53 ( 98.02)	Acc@5  99.22 ( 99.98)
Epoch: [43][ 50/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.004)	Loss 4.5410e-02 (5.7393e-02)	Acc@1  98.44 ( 98.05)	Acc@5 100.00 ( 99.97)
Epoch: [43][ 60/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 4.4861e-02 (5.6898e-02)	Acc@1  98.44 ( 98.13)	Acc@5 100.00 ( 99.97)
Epoch: [43][ 70/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 4.9316e-02 (5.5685e-02)	Acc@1  98.44 ( 98.21)	Acc@5 100.00 ( 99.98)
Epoch: [43][ 80/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.0748e-01 (5.5815e-02)	Acc@1  96.88 ( 98.22)	Acc@5 100.00 ( 99.98)
Epoch: [43][ 90/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.003)	Loss 7.1777e-02 (5.5712e-02)	Acc@1  97.66 ( 98.20)	Acc@5 100.00 ( 99.98)
Epoch: [43][100/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.6987e-02 (5.4976e-02)	Acc@1  98.44 ( 98.22)	Acc@5 100.00 ( 99.98)
Epoch: [43][110/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.2866e-02 (5.4760e-02)	Acc@1  97.66 ( 98.23)	Acc@5 100.00 ( 99.98)
Epoch: [43][120/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.7689e-02 (5.4512e-02)	Acc@1  98.44 ( 98.23)	Acc@5 100.00 ( 99.98)
Epoch: [43][130/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.3956e-02 (5.3437e-02)	Acc@1  99.22 ( 98.25)	Acc@5 100.00 ( 99.98)
Epoch: [43][140/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.7627e-02 (5.3074e-02)	Acc@1  96.88 ( 98.26)	Acc@5 100.00 ( 99.98)
Epoch: [43][150/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.8755e-02 (5.3455e-02)	Acc@1  96.88 ( 98.23)	Acc@5 100.00 ( 99.98)
Epoch: [43][160/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.2734e-02 (5.3051e-02)	Acc@1  97.66 ( 98.21)	Acc@5 100.00 ( 99.99)
Epoch: [43][170/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.9133e-02 (5.3272e-02)	Acc@1  98.44 ( 98.19)	Acc@5 100.00 ( 99.99)
Epoch: [43][180/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.3232e-02 (5.3370e-02)	Acc@1  97.66 ( 98.18)	Acc@5 100.00 ( 99.99)
Epoch: [43][190/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.5195e-02 (5.3294e-02)	Acc@1  97.66 ( 98.19)	Acc@5 100.00 ( 99.99)
Epoch: [43][200/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.8319e-02 (5.4090e-02)	Acc@1  96.88 ( 98.15)	Acc@5 100.00 ( 99.99)
Epoch: [43][210/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.3163e-02 (5.3984e-02)	Acc@1 100.00 ( 98.14)	Acc@5 100.00 ( 99.99)
Epoch: [43][220/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.2572e-02 (5.3875e-02)	Acc@1  97.66 ( 98.14)	Acc@5 100.00 ( 99.99)
Epoch: [43][230/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.3132e-02 (5.3171e-02)	Acc@1 100.00 ( 98.16)	Acc@5 100.00 ( 99.99)
Epoch: [43][240/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.7354e-02 (5.3302e-02)	Acc@1  99.22 ( 98.15)	Acc@5 100.00 ( 99.99)
Epoch: [43][250/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.0964e-02 (5.3477e-02)	Acc@1  98.44 ( 98.16)	Acc@5 100.00 ( 99.99)
Epoch: [43][260/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.8105e-02 (5.3766e-02)	Acc@1  98.44 ( 98.16)	Acc@5 100.00 ( 99.99)
Epoch: [43][270/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.1860e-02 (5.3507e-02)	Acc@1  99.22 ( 98.16)	Acc@5 100.00 ( 99.99)
Epoch: [43][280/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.9601e-02 (5.3310e-02)	Acc@1  98.44 ( 98.16)	Acc@5 100.00 ( 99.99)
Epoch: [43][290/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.4270e-02 (5.3480e-02)	Acc@1  96.88 ( 98.15)	Acc@5 100.00 ( 99.99)
Epoch: [43][300/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.7069e-02 (5.3559e-02)	Acc@1  99.22 ( 98.15)	Acc@5 100.00 ( 99.99)
Epoch: [43][310/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.5084e-02 (5.3292e-02)	Acc@1  97.66 ( 98.16)	Acc@5 100.00 ( 99.99)
Epoch: [43][320/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.4861e-02 (5.3597e-02)	Acc@1  99.22 ( 98.13)	Acc@5 100.00 ( 99.99)
Epoch: [43][330/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5125e-01 (5.3985e-02)	Acc@1  94.53 ( 98.10)	Acc@5 100.00 ( 99.99)
Epoch: [43][340/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.5796e-02 (5.3948e-02)	Acc@1  97.66 ( 98.10)	Acc@5 100.00 ( 99.99)
Epoch: [43][350/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.4055e-02 (5.4033e-02)	Acc@1  97.66 ( 98.10)	Acc@5 100.00 ( 99.99)
Epoch: [43][360/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.5491e-02 (5.3751e-02)	Acc@1  96.88 ( 98.12)	Acc@5 100.00 ( 99.99)
Epoch: [43][370/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.7922e-02 (5.3968e-02)	Acc@1  96.88 ( 98.12)	Acc@5 100.00 ( 99.99)
Epoch: [43][380/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.4647e-02 (5.3659e-02)	Acc@1  98.44 ( 98.13)	Acc@5 100.00 ( 99.99)
Epoch: [43][390/391]	Time  0.115 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6388e-02 (5.4020e-02)	Acc@1 100.00 ( 98.12)	Acc@5 100.00 ( 99.99)
## e[43] optimizer.zero_grad (sum) time: 0.5230135917663574
## e[43]       loss.backward (sum) time: 12.418331861495972
## e[43]      optimizer.step (sum) time: 25.78400945663452
## epoch[43] training(only) time: 63.51617980003357
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 1.6150e-01 (1.6150e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.060)	Loss 2.8906e-01 (2.7389e-01)	Acc@1  93.00 ( 91.73)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 4.0454e-01 (2.9161e-01)	Acc@1  91.00 ( 92.00)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 3.0835e-01 (3.0098e-01)	Acc@1  92.00 ( 92.10)	Acc@5  99.00 ( 99.77)
Test: [ 40/100]	Time  0.047 ( 0.050)	Loss 4.8877e-01 (3.0774e-01)	Acc@1  90.00 ( 92.00)	Acc@5  98.00 ( 99.73)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.7957e-01 (3.0991e-01)	Acc@1  96.00 ( 91.90)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 2.4573e-01 (3.0693e-01)	Acc@1  93.00 ( 91.90)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 2.9224e-01 (3.0414e-01)	Acc@1  94.00 ( 91.82)	Acc@5 100.00 ( 99.82)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 2.5269e-01 (3.0233e-01)	Acc@1  91.00 ( 91.88)	Acc@5 100.00 ( 99.81)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 3.1641e-01 (3.0221e-01)	Acc@1  92.00 ( 91.80)	Acc@5 100.00 ( 99.81)
 * Acc@1 91.680 Acc@5 99.820
### epoch[43] execution time: 68.4848563671112
EPOCH 44
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [44][  0/391]	Time  0.324 ( 0.324)	Data  0.155 ( 0.155)	Loss 2.7390e-02 (2.7390e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [44][ 10/391]	Time  0.160 ( 0.177)	Data  0.001 ( 0.015)	Loss 1.0229e-01 (5.2263e-02)	Acc@1  96.09 ( 98.08)	Acc@5 100.00 (100.00)
Epoch: [44][ 20/391]	Time  0.167 ( 0.169)	Data  0.001 ( 0.009)	Loss 6.1554e-02 (5.5739e-02)	Acc@1  99.22 ( 98.18)	Acc@5 100.00 ( 99.96)
Epoch: [44][ 30/391]	Time  0.170 ( 0.167)	Data  0.001 ( 0.006)	Loss 2.9984e-02 (5.6621e-02)	Acc@1  98.44 ( 98.06)	Acc@5 100.00 ( 99.97)
Epoch: [44][ 40/391]	Time  0.173 ( 0.166)	Data  0.001 ( 0.005)	Loss 5.3833e-02 (5.1374e-02)	Acc@1  97.66 ( 98.27)	Acc@5 100.00 ( 99.98)
Epoch: [44][ 50/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.004)	Loss 5.8929e-02 (5.0960e-02)	Acc@1  97.66 ( 98.35)	Acc@5 100.00 ( 99.98)
Epoch: [44][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 9.9487e-02 (5.0863e-02)	Acc@1  96.09 ( 98.35)	Acc@5 100.00 ( 99.97)
Epoch: [44][ 70/391]	Time  0.163 ( 0.164)	Data  0.001 ( 0.003)	Loss 7.8552e-02 (5.1912e-02)	Acc@1  97.66 ( 98.32)	Acc@5 100.00 ( 99.97)
Epoch: [44][ 80/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 4.9866e-02 (5.1963e-02)	Acc@1  98.44 ( 98.29)	Acc@5 100.00 ( 99.97)
Epoch: [44][ 90/391]	Time  0.169 ( 0.164)	Data  0.001 ( 0.003)	Loss 3.4241e-02 (5.1782e-02)	Acc@1  99.22 ( 98.26)	Acc@5 100.00 ( 99.97)
Epoch: [44][100/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 4.6173e-02 (5.1696e-02)	Acc@1  97.66 ( 98.24)	Acc@5 100.00 ( 99.98)
Epoch: [44][110/391]	Time  0.163 ( 0.164)	Data  0.001 ( 0.003)	Loss 3.4302e-02 (5.0851e-02)	Acc@1  99.22 ( 98.29)	Acc@5 100.00 ( 99.98)
Epoch: [44][120/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.0599e-02 (5.0811e-02)	Acc@1 100.00 ( 98.28)	Acc@5 100.00 ( 99.98)
Epoch: [44][130/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.4617e-02 (5.0462e-02)	Acc@1  98.44 ( 98.30)	Acc@5 100.00 ( 99.98)
Epoch: [44][140/391]	Time  0.170 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.2074e-02 (4.9784e-02)	Acc@1 100.00 ( 98.34)	Acc@5 100.00 ( 99.98)
Epoch: [44][150/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.7567e-02 (4.9104e-02)	Acc@1  98.44 ( 98.35)	Acc@5 100.00 ( 99.98)
Epoch: [44][160/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.5513e-02 (4.9065e-02)	Acc@1  99.22 ( 98.36)	Acc@5 100.00 ( 99.99)
Epoch: [44][170/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.3732e-02 (4.8852e-02)	Acc@1  96.88 ( 98.36)	Acc@5 100.00 ( 99.99)
Epoch: [44][180/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0681e-01 (4.9966e-02)	Acc@1  96.88 ( 98.33)	Acc@5 100.00 ( 99.99)
Epoch: [44][190/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.0007e-02 (4.9946e-02)	Acc@1  97.66 ( 98.34)	Acc@5 100.00 ( 99.99)
Epoch: [44][200/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.1464e-02 (4.9532e-02)	Acc@1  98.44 ( 98.34)	Acc@5 100.00 ( 99.99)
Epoch: [44][210/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.7109e-02 (4.9462e-02)	Acc@1  98.44 ( 98.33)	Acc@5 100.00 ( 99.99)
Epoch: [44][220/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.1870e-02 (5.0061e-02)	Acc@1  96.88 ( 98.30)	Acc@5 100.00 ( 99.99)
Epoch: [44][230/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.0028e-02 (4.9946e-02)	Acc@1  97.66 ( 98.31)	Acc@5 100.00 ( 99.99)
Epoch: [44][240/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5526e-02 (5.0220e-02)	Acc@1 100.00 ( 98.30)	Acc@5 100.00 ( 99.99)
Epoch: [44][250/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.7402e-02 (5.0151e-02)	Acc@1  97.66 ( 98.31)	Acc@5 100.00 ( 99.99)
Epoch: [44][260/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.4790e-02 (5.0287e-02)	Acc@1  98.44 ( 98.29)	Acc@5 100.00 ( 99.99)
Epoch: [44][270/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.3386e-02 (5.0064e-02)	Acc@1  99.22 ( 98.30)	Acc@5 100.00 ( 99.99)
Epoch: [44][280/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.5256e-02 (4.9816e-02)	Acc@1  97.66 ( 98.32)	Acc@5 100.00 ( 99.99)
Epoch: [44][290/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.3060e-02 (5.0233e-02)	Acc@1  98.44 ( 98.30)	Acc@5 100.00 ( 99.99)
Epoch: [44][300/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7075e-02 (4.9896e-02)	Acc@1 100.00 ( 98.32)	Acc@5 100.00 ( 99.99)
Epoch: [44][310/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.4299e-02 (4.9916e-02)	Acc@1  96.88 ( 98.32)	Acc@5 100.00 ( 99.99)
Epoch: [44][320/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.5562e-02 (5.0403e-02)	Acc@1  98.44 ( 98.29)	Acc@5 100.00 ( 99.99)
Epoch: [44][330/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.8422e-02 (5.0336e-02)	Acc@1  97.66 ( 98.30)	Acc@5 100.00 ( 99.99)
Epoch: [44][340/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.5614e-02 (5.0120e-02)	Acc@1  98.44 ( 98.31)	Acc@5 100.00 ( 99.99)
Epoch: [44][350/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.5369e-02 (5.0240e-02)	Acc@1  97.66 ( 98.30)	Acc@5 100.00 ( 99.99)
Epoch: [44][360/391]	Time  0.178 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.9617e-02 (5.0117e-02)	Acc@1  99.22 ( 98.30)	Acc@5 100.00 ( 99.99)
Epoch: [44][370/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.6051e-02 (4.9984e-02)	Acc@1  98.44 ( 98.30)	Acc@5 100.00 ( 99.99)
Epoch: [44][380/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.9276e-02 (4.9941e-02)	Acc@1  97.66 ( 98.30)	Acc@5 100.00 ( 99.99)
Epoch: [44][390/391]	Time  0.114 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.0020e-02 (4.9870e-02)	Acc@1 100.00 ( 98.30)	Acc@5 100.00 ( 99.99)
## e[44] optimizer.zero_grad (sum) time: 0.5257067680358887
## e[44]       loss.backward (sum) time: 12.376322507858276
## e[44]      optimizer.step (sum) time: 25.828439950942993
## epoch[44] training(only) time: 63.605141401290894
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 2.0459e-01 (2.0459e-01)	Acc@1  93.00 ( 93.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.059)	Loss 2.9541e-01 (2.8503e-01)	Acc@1  92.00 ( 91.55)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.047 ( 0.053)	Loss 3.5522e-01 (2.9931e-01)	Acc@1  91.00 ( 91.71)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.046 ( 0.051)	Loss 3.4888e-01 (3.0727e-01)	Acc@1  93.00 ( 91.81)	Acc@5  99.00 ( 99.71)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 3.8281e-01 (3.0830e-01)	Acc@1  90.00 ( 91.78)	Acc@5  99.00 ( 99.71)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 1.9055e-01 (3.1115e-01)	Acc@1  95.00 ( 91.78)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 2.7393e-01 (3.0835e-01)	Acc@1  94.00 ( 91.80)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 3.8501e-01 (3.0652e-01)	Acc@1  93.00 ( 91.93)	Acc@5 100.00 ( 99.80)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 2.4133e-01 (3.0435e-01)	Acc@1  92.00 ( 92.05)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 3.1592e-01 (3.0616e-01)	Acc@1  90.00 ( 91.95)	Acc@5 100.00 ( 99.79)
 * Acc@1 91.840 Acc@5 99.790
### epoch[44] execution time: 68.543710231781
EPOCH 45
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [45][  0/391]	Time  0.316 ( 0.316)	Data  0.139 ( 0.139)	Loss 3.3722e-02 (3.3722e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [45][ 10/391]	Time  0.161 ( 0.176)	Data  0.001 ( 0.014)	Loss 4.2694e-02 (4.8021e-02)	Acc@1  99.22 ( 98.51)	Acc@5 100.00 (100.00)
Epoch: [45][ 20/391]	Time  0.173 ( 0.169)	Data  0.001 ( 0.008)	Loss 8.5449e-02 (4.5421e-02)	Acc@1  98.44 ( 98.70)	Acc@5 100.00 (100.00)
Epoch: [45][ 30/391]	Time  0.159 ( 0.166)	Data  0.001 ( 0.006)	Loss 5.2002e-02 (4.6659e-02)	Acc@1  97.66 ( 98.61)	Acc@5 100.00 (100.00)
Epoch: [45][ 40/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.005)	Loss 6.1432e-02 (4.5777e-02)	Acc@1  97.66 ( 98.61)	Acc@5 100.00 (100.00)
Epoch: [45][ 50/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 5.4199e-02 (4.4584e-02)	Acc@1  97.66 ( 98.59)	Acc@5 100.00 (100.00)
Epoch: [45][ 60/391]	Time  0.162 ( 0.165)	Data  0.001 ( 0.003)	Loss 1.0811e-02 (4.2998e-02)	Acc@1 100.00 ( 98.60)	Acc@5 100.00 (100.00)
Epoch: [45][ 70/391]	Time  0.168 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.1286e-02 (4.5069e-02)	Acc@1 100.00 ( 98.56)	Acc@5 100.00 (100.00)
Epoch: [45][ 80/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 3.0334e-02 (4.4341e-02)	Acc@1 100.00 ( 98.60)	Acc@5 100.00 (100.00)
Epoch: [45][ 90/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.003)	Loss 4.4006e-02 (4.5149e-02)	Acc@1  97.66 ( 98.51)	Acc@5 100.00 (100.00)
Epoch: [45][100/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 3.8635e-02 (4.5011e-02)	Acc@1  99.22 ( 98.49)	Acc@5 100.00 (100.00)
Epoch: [45][110/391]	Time  0.164 ( 0.164)	Data  0.001 ( 0.002)	Loss 3.3966e-02 (4.4974e-02)	Acc@1  98.44 ( 98.49)	Acc@5 100.00 ( 99.99)
Epoch: [45][120/391]	Time  0.172 ( 0.164)	Data  0.001 ( 0.002)	Loss 7.5012e-02 (4.5817e-02)	Acc@1  96.09 ( 98.45)	Acc@5 100.00 ( 99.99)
Epoch: [45][130/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.8879e-02 (4.5717e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 ( 99.99)
Epoch: [45][140/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.2995e-02 (4.5862e-02)	Acc@1  99.22 ( 98.43)	Acc@5 100.00 ( 99.99)
Epoch: [45][150/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.4490e-02 (4.5952e-02)	Acc@1  99.22 ( 98.42)	Acc@5 100.00 ( 99.99)
Epoch: [45][160/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.4246e-02 (4.6749e-02)	Acc@1  99.22 ( 98.38)	Acc@5 100.00 (100.00)
Epoch: [45][170/391]	Time  0.168 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.4504e-02 (4.6802e-02)	Acc@1  98.44 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [45][180/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.8186e-02 (4.7570e-02)	Acc@1  97.66 ( 98.34)	Acc@5 100.00 (100.00)
Epoch: [45][190/391]	Time  0.165 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.5634e-02 (4.7349e-02)	Acc@1  97.66 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [45][200/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.6967e-02 (4.7700e-02)	Acc@1  98.44 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [45][210/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.0720e-02 (4.7470e-02)	Acc@1  97.66 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [45][220/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.6549e-02 (4.7105e-02)	Acc@1  97.66 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [45][230/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.8030e-02 (4.7248e-02)	Acc@1  99.22 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [45][240/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5152e-02 (4.6801e-02)	Acc@1 100.00 ( 98.40)	Acc@5 100.00 (100.00)
Epoch: [45][250/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.2561e-02 (4.7131e-02)	Acc@1  98.44 ( 98.39)	Acc@5 100.00 (100.00)
Epoch: [45][260/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.6722e-02 (4.7276e-02)	Acc@1  98.44 ( 98.38)	Acc@5 100.00 (100.00)
Epoch: [45][270/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.1439e-02 (4.7078e-02)	Acc@1 100.00 ( 98.39)	Acc@5 100.00 (100.00)
Epoch: [45][280/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.0121e-02 (4.7066e-02)	Acc@1  99.22 ( 98.38)	Acc@5 100.00 (100.00)
Epoch: [45][290/391]	Time  0.165 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.2354e-02 (4.7415e-02)	Acc@1 100.00 ( 98.39)	Acc@5 100.00 (100.00)
Epoch: [45][300/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.8055e-02 (4.7264e-02)	Acc@1  99.22 ( 98.39)	Acc@5 100.00 (100.00)
Epoch: [45][310/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.9723e-02 (4.7355e-02)	Acc@1  96.88 ( 98.39)	Acc@5 100.00 (100.00)
Epoch: [45][320/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.7344e-02 (4.7756e-02)	Acc@1  99.22 ( 98.38)	Acc@5 100.00 (100.00)
Epoch: [45][330/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.1229e-02 (4.8130e-02)	Acc@1  98.44 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [45][340/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.001)	Loss 3.0914e-02 (4.8198e-02)	Acc@1  99.22 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [45][350/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.001)	Loss 5.1605e-02 (4.8188e-02)	Acc@1  96.88 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [45][360/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.001)	Loss 6.1615e-02 (4.8303e-02)	Acc@1  98.44 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [45][370/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.001)	Loss 2.8961e-02 (4.8204e-02)	Acc@1  98.44 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [45][380/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.001)	Loss 4.4189e-02 (4.8490e-02)	Acc@1  98.44 ( 98.35)	Acc@5 100.00 (100.00)
Epoch: [45][390/391]	Time  0.118 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.3586e-01 (4.9143e-02)	Acc@1  96.25 ( 98.33)	Acc@5 100.00 ( 99.99)
## e[45] optimizer.zero_grad (sum) time: 0.5273487567901611
## e[45]       loss.backward (sum) time: 12.342242956161499
## e[45]      optimizer.step (sum) time: 25.832071781158447
## epoch[45] training(only) time: 63.62197303771973
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 2.0190e-01 (2.0190e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 3.1299e-01 (2.9622e-01)	Acc@1  93.00 ( 91.73)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 4.2505e-01 (3.1262e-01)	Acc@1  89.00 ( 91.90)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 3.8477e-01 (3.1587e-01)	Acc@1  91.00 ( 91.94)	Acc@5  99.00 ( 99.71)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 5.2490e-01 (3.2365e-01)	Acc@1  90.00 ( 91.93)	Acc@5  98.00 ( 99.71)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 2.1863e-01 (3.2700e-01)	Acc@1  94.00 ( 91.71)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 3.1763e-01 (3.2459e-01)	Acc@1  89.00 ( 91.62)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 2.7759e-01 (3.1939e-01)	Acc@1  94.00 ( 91.58)	Acc@5 100.00 ( 99.80)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 3.3667e-01 (3.1920e-01)	Acc@1  88.00 ( 91.63)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 3.1787e-01 (3.1703e-01)	Acc@1  89.00 ( 91.63)	Acc@5 100.00 ( 99.80)
 * Acc@1 91.540 Acc@5 99.800
### epoch[45] execution time: 68.58250331878662
EPOCH 46
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [46][  0/391]	Time  0.313 ( 0.313)	Data  0.137 ( 0.137)	Loss 1.9089e-02 (1.9089e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [46][ 10/391]	Time  0.161 ( 0.175)	Data  0.001 ( 0.014)	Loss 2.7466e-02 (5.1913e-02)	Acc@1 100.00 ( 98.22)	Acc@5 100.00 (100.00)
Epoch: [46][ 20/391]	Time  0.160 ( 0.168)	Data  0.001 ( 0.008)	Loss 3.2074e-02 (4.7180e-02)	Acc@1  98.44 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [46][ 30/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.005)	Loss 7.8674e-02 (5.1529e-02)	Acc@1  96.88 ( 98.11)	Acc@5 100.00 (100.00)
Epoch: [46][ 40/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 5.3650e-02 (5.2226e-02)	Acc@1  96.88 ( 98.13)	Acc@5 100.00 ( 99.98)
Epoch: [46][ 50/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 3.8788e-02 (5.1682e-02)	Acc@1  98.44 ( 98.18)	Acc@5 100.00 ( 99.98)
Epoch: [46][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.4765e-02 (5.1273e-02)	Acc@1  99.22 ( 98.23)	Acc@5 100.00 ( 99.99)
Epoch: [46][ 70/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 5.5267e-02 (5.0201e-02)	Acc@1  98.44 ( 98.27)	Acc@5 100.00 ( 99.99)
Epoch: [46][ 80/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 5.5084e-02 (4.9473e-02)	Acc@1  98.44 ( 98.28)	Acc@5 100.00 ( 99.99)
Epoch: [46][ 90/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 5.8746e-02 (4.9665e-02)	Acc@1  98.44 ( 98.27)	Acc@5 100.00 ( 99.99)
Epoch: [46][100/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.002)	Loss 3.5675e-02 (4.9235e-02)	Acc@1  98.44 ( 98.30)	Acc@5 100.00 ( 99.99)
Epoch: [46][110/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.002)	Loss 3.0136e-02 (4.9754e-02)	Acc@1  99.22 ( 98.29)	Acc@5 100.00 ( 99.99)
Epoch: [46][120/391]	Time  0.165 ( 0.164)	Data  0.001 ( 0.002)	Loss 5.0232e-02 (5.0019e-02)	Acc@1  98.44 ( 98.31)	Acc@5 100.00 ( 99.99)
Epoch: [46][130/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.6101e-02 (4.9997e-02)	Acc@1  97.66 ( 98.30)	Acc@5 100.00 ( 99.99)
Epoch: [46][140/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.2440e-02 (4.9728e-02)	Acc@1  98.44 ( 98.30)	Acc@5 100.00 ( 99.99)
Epoch: [46][150/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.6233e-02 (4.9709e-02)	Acc@1  96.09 ( 98.28)	Acc@5 100.00 ( 99.99)
Epoch: [46][160/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6418e-02 (4.9980e-02)	Acc@1 100.00 ( 98.29)	Acc@5 100.00 (100.00)
Epoch: [46][170/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.7445e-02 (4.9792e-02)	Acc@1  99.22 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [46][180/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.1819e-02 (4.9783e-02)	Acc@1  97.66 ( 98.29)	Acc@5 100.00 (100.00)
Epoch: [46][190/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.8258e-02 (4.9149e-02)	Acc@1  98.44 ( 98.32)	Acc@5 100.00 (100.00)
Epoch: [46][200/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.5806e-02 (4.9480e-02)	Acc@1  96.88 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [46][210/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.9500e-02 (4.9071e-02)	Acc@1  99.22 ( 98.32)	Acc@5 100.00 (100.00)
Epoch: [46][220/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.5950e-02 (4.9287e-02)	Acc@1  98.44 ( 98.31)	Acc@5 100.00 (100.00)
Epoch: [46][230/391]	Time  0.165 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.3875e-02 (4.9673e-02)	Acc@1  98.44 ( 98.29)	Acc@5 100.00 (100.00)
Epoch: [46][240/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.9590e-02 (4.9340e-02)	Acc@1  96.88 ( 98.32)	Acc@5 100.00 (100.00)
Epoch: [46][250/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1490e-02 (4.9396e-02)	Acc@1 100.00 ( 98.32)	Acc@5 100.00 (100.00)
Epoch: [46][260/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.9377e-02 (4.9506e-02)	Acc@1  97.66 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [46][270/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.4373e-02 (4.9131e-02)	Acc@1  98.44 ( 98.32)	Acc@5 100.00 (100.00)
Epoch: [46][280/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.8624e-02 (4.8637e-02)	Acc@1  98.44 ( 98.35)	Acc@5 100.00 (100.00)
Epoch: [46][290/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.5084e-02 (4.9405e-02)	Acc@1  97.66 ( 98.31)	Acc@5 100.00 (100.00)
Epoch: [46][300/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.6660e-02 (4.9867e-02)	Acc@1  97.66 ( 98.31)	Acc@5 100.00 (100.00)
Epoch: [46][310/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.7485e-02 (4.9993e-02)	Acc@1  97.66 ( 98.29)	Acc@5 100.00 (100.00)
Epoch: [46][320/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.0679e-02 (5.0246e-02)	Acc@1  96.88 ( 98.28)	Acc@5 100.00 (100.00)
Epoch: [46][330/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.7953e-02 (5.0257e-02)	Acc@1  97.66 ( 98.29)	Acc@5 100.00 (100.00)
Epoch: [46][340/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.2816e-02 (5.0298e-02)	Acc@1  98.44 ( 98.28)	Acc@5 100.00 (100.00)
Epoch: [46][350/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.3549e-02 (5.0020e-02)	Acc@1  97.66 ( 98.29)	Acc@5 100.00 (100.00)
Epoch: [46][360/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.0813e-02 (5.0130e-02)	Acc@1 100.00 ( 98.29)	Acc@5 100.00 (100.00)
Epoch: [46][370/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.2812e-02 (5.0070e-02)	Acc@1  99.22 ( 98.29)	Acc@5 100.00 (100.00)
Epoch: [46][380/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.8381e-02 (4.9998e-02)	Acc@1 100.00 ( 98.29)	Acc@5 100.00 (100.00)
Epoch: [46][390/391]	Time  0.118 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.9001e-02 (4.9638e-02)	Acc@1  98.75 ( 98.30)	Acc@5 100.00 (100.00)
## e[46] optimizer.zero_grad (sum) time: 0.5329697132110596
## e[46]       loss.backward (sum) time: 12.367038488388062
## e[46]      optimizer.step (sum) time: 25.826480388641357
## epoch[46] training(only) time: 63.673325538635254
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 1.5613e-01 (1.5613e-01)	Acc@1  97.00 ( 97.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 3.6304e-01 (2.8754e-01)	Acc@1  91.00 ( 92.45)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.047 ( 0.054)	Loss 4.1016e-01 (3.0867e-01)	Acc@1  91.00 ( 92.14)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.057 ( 0.052)	Loss 3.4302e-01 (3.0631e-01)	Acc@1  92.00 ( 92.03)	Acc@5  98.00 ( 99.68)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 4.6655e-01 (3.1170e-01)	Acc@1  88.00 ( 91.93)	Acc@5  99.00 ( 99.66)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.7761e-01 (3.1746e-01)	Acc@1  96.00 ( 91.90)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.047 ( 0.049)	Loss 2.6880e-01 (3.1579e-01)	Acc@1  94.00 ( 91.92)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 3.9819e-01 (3.1619e-01)	Acc@1  93.00 ( 91.96)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 2.2803e-01 (3.1596e-01)	Acc@1  93.00 ( 92.04)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 3.0103e-01 (3.1934e-01)	Acc@1  91.00 ( 91.95)	Acc@5 100.00 ( 99.76)
 * Acc@1 91.780 Acc@5 99.760
### epoch[46] execution time: 68.63185214996338
EPOCH 47
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [47][  0/391]	Time  0.332 ( 0.332)	Data  0.159 ( 0.159)	Loss 3.5156e-02 (3.5156e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [47][ 10/391]	Time  0.160 ( 0.176)	Data  0.001 ( 0.015)	Loss 6.3232e-02 (4.5520e-02)	Acc@1  98.44 ( 98.51)	Acc@5 100.00 (100.00)
Epoch: [47][ 20/391]	Time  0.160 ( 0.169)	Data  0.001 ( 0.009)	Loss 9.3628e-02 (4.7380e-02)	Acc@1  96.88 ( 98.59)	Acc@5 100.00 (100.00)
Epoch: [47][ 30/391]	Time  0.160 ( 0.167)	Data  0.001 ( 0.006)	Loss 5.3253e-02 (4.6168e-02)	Acc@1  97.66 ( 98.59)	Acc@5 100.00 (100.00)
Epoch: [47][ 40/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.005)	Loss 6.5369e-02 (4.4439e-02)	Acc@1  96.88 ( 98.55)	Acc@5 100.00 (100.00)
Epoch: [47][ 50/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.0124e-02 (4.3265e-02)	Acc@1 100.00 ( 98.62)	Acc@5 100.00 (100.00)
Epoch: [47][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 5.2399e-02 (4.3658e-02)	Acc@1  96.88 ( 98.62)	Acc@5 100.00 (100.00)
Epoch: [47][ 70/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 3.1525e-02 (4.4232e-02)	Acc@1 100.00 ( 98.58)	Acc@5 100.00 (100.00)
Epoch: [47][ 80/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.9587e-02 (4.3277e-02)	Acc@1  99.22 ( 98.61)	Acc@5 100.00 (100.00)
Epoch: [47][ 90/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.8168e-02 (4.3357e-02)	Acc@1  99.22 ( 98.58)	Acc@5 100.00 (100.00)
Epoch: [47][100/391]	Time  0.167 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.2964e-02 (4.3389e-02)	Acc@1 100.00 ( 98.60)	Acc@5 100.00 (100.00)
Epoch: [47][110/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.6632e-02 (4.3024e-02)	Acc@1  99.22 ( 98.62)	Acc@5 100.00 (100.00)
Epoch: [47][120/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.9978e-02 (4.3944e-02)	Acc@1  98.44 ( 98.61)	Acc@5 100.00 (100.00)
Epoch: [47][130/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.9526e-02 (4.3647e-02)	Acc@1  99.22 ( 98.62)	Acc@5 100.00 (100.00)
Epoch: [47][140/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.9479e-02 (4.3923e-02)	Acc@1  97.66 ( 98.59)	Acc@5 100.00 (100.00)
Epoch: [47][150/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.2042e-02 (4.3515e-02)	Acc@1  96.88 ( 98.59)	Acc@5 100.00 (100.00)
Epoch: [47][160/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.1942e-02 (4.2747e-02)	Acc@1 100.00 ( 98.63)	Acc@5 100.00 (100.00)
Epoch: [47][170/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.8920e-02 (4.2956e-02)	Acc@1  97.66 ( 98.59)	Acc@5 100.00 (100.00)
Epoch: [47][180/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.7664e-02 (4.2889e-02)	Acc@1  99.22 ( 98.58)	Acc@5 100.00 (100.00)
Epoch: [47][190/391]	Time  0.165 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.4210e-02 (4.2955e-02)	Acc@1 100.00 ( 98.61)	Acc@5 100.00 (100.00)
Epoch: [47][200/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.8431e-02 (4.2510e-02)	Acc@1  96.88 ( 98.61)	Acc@5 100.00 (100.00)
Epoch: [47][210/391]	Time  0.173 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5396e-02 (4.2649e-02)	Acc@1 100.00 ( 98.61)	Acc@5 100.00 (100.00)
Epoch: [47][220/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.2908e-02 (4.2935e-02)	Acc@1  97.66 ( 98.59)	Acc@5 100.00 (100.00)
Epoch: [47][230/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.1851e-02 (4.3142e-02)	Acc@1 100.00 ( 98.59)	Acc@5 100.00 (100.00)
Epoch: [47][240/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.9927e-02 (4.3108e-02)	Acc@1  98.44 ( 98.59)	Acc@5 100.00 (100.00)
Epoch: [47][250/391]	Time  0.176 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.4952e-02 (4.2949e-02)	Acc@1  97.66 ( 98.59)	Acc@5 100.00 (100.00)
Epoch: [47][260/391]	Time  0.172 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.1840e-02 (4.3217e-02)	Acc@1  98.44 ( 98.57)	Acc@5 100.00 (100.00)
Epoch: [47][270/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.3009e-02 (4.3350e-02)	Acc@1  97.66 ( 98.56)	Acc@5 100.00 (100.00)
Epoch: [47][280/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2413e-02 (4.3448e-02)	Acc@1 100.00 ( 98.55)	Acc@5 100.00 ( 99.99)
Epoch: [47][290/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.8584e-02 (4.3432e-02)	Acc@1  98.44 ( 98.55)	Acc@5 100.00 ( 99.99)
Epoch: [47][300/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.7420e-02 (4.3336e-02)	Acc@1  98.44 ( 98.54)	Acc@5 100.00 ( 99.99)
Epoch: [47][310/391]	Time  0.171 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.8992e-02 (4.3273e-02)	Acc@1  99.22 ( 98.54)	Acc@5 100.00 ( 99.99)
Epoch: [47][320/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.8167e-02 (4.3526e-02)	Acc@1  96.88 ( 98.54)	Acc@5 100.00 (100.00)
Epoch: [47][330/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7166e-02 (4.3788e-02)	Acc@1  98.44 ( 98.52)	Acc@5 100.00 (100.00)
Epoch: [47][340/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3512e-02 (4.4071e-02)	Acc@1 100.00 ( 98.51)	Acc@5 100.00 (100.00)
Epoch: [47][350/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.0955e-02 (4.4073e-02)	Acc@1  97.66 ( 98.50)	Acc@5 100.00 (100.00)
Epoch: [47][360/391]	Time  0.173 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.4524e-02 (4.4337e-02)	Acc@1  96.09 ( 98.49)	Acc@5 100.00 (100.00)
Epoch: [47][370/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.7354e-02 (4.4426e-02)	Acc@1  99.22 ( 98.49)	Acc@5 100.00 (100.00)
Epoch: [47][380/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.0161e-02 (4.4426e-02)	Acc@1  98.44 ( 98.49)	Acc@5 100.00 (100.00)
Epoch: [47][390/391]	Time  0.118 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.5206e-02 (4.4733e-02)	Acc@1  96.25 ( 98.47)	Acc@5 100.00 (100.00)
## e[47] optimizer.zero_grad (sum) time: 0.526597261428833
## e[47]       loss.backward (sum) time: 12.394082307815552
## e[47]      optimizer.step (sum) time: 25.805830717086792
## epoch[47] training(only) time: 63.655962228775024
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 1.6626e-01 (1.6626e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 3.1226e-01 (2.9559e-01)	Acc@1  94.00 ( 92.00)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 4.8462e-01 (3.1756e-01)	Acc@1  90.00 ( 92.29)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 3.6548e-01 (3.1890e-01)	Acc@1  93.00 ( 92.23)	Acc@5  99.00 ( 99.68)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 5.0635e-01 (3.2598e-01)	Acc@1  90.00 ( 92.20)	Acc@5  99.00 ( 99.66)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 2.1851e-01 (3.2955e-01)	Acc@1  96.00 ( 92.00)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.047 ( 0.049)	Loss 2.6514e-01 (3.2729e-01)	Acc@1  91.00 ( 91.84)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 3.4155e-01 (3.2637e-01)	Acc@1  94.00 ( 91.90)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 2.8760e-01 (3.2431e-01)	Acc@1  91.00 ( 92.01)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 2.9199e-01 (3.2460e-01)	Acc@1  92.00 ( 91.97)	Acc@5 100.00 ( 99.81)
 * Acc@1 91.860 Acc@5 99.810
### epoch[47] execution time: 68.59913349151611
EPOCH 48
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [48][  0/391]	Time  0.324 ( 0.324)	Data  0.150 ( 0.150)	Loss 3.6072e-02 (3.6072e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [48][ 10/391]	Time  0.161 ( 0.177)	Data  0.001 ( 0.015)	Loss 5.2246e-02 (4.6533e-02)	Acc@1  98.44 ( 98.15)	Acc@5 100.00 (100.00)
Epoch: [48][ 20/391]	Time  0.160 ( 0.169)	Data  0.001 ( 0.008)	Loss 1.6479e-02 (4.1222e-02)	Acc@1 100.00 ( 98.29)	Acc@5 100.00 (100.00)
Epoch: [48][ 30/391]	Time  0.161 ( 0.167)	Data  0.001 ( 0.006)	Loss 5.3467e-02 (3.9484e-02)	Acc@1  97.66 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [48][ 40/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.005)	Loss 1.3390e-02 (4.0117e-02)	Acc@1 100.00 ( 98.40)	Acc@5 100.00 (100.00)
Epoch: [48][ 50/391]	Time  0.169 ( 0.165)	Data  0.001 ( 0.004)	Loss 2.4231e-02 (4.0097e-02)	Acc@1  99.22 ( 98.47)	Acc@5 100.00 (100.00)
Epoch: [48][ 60/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.004)	Loss 3.6804e-02 (4.0566e-02)	Acc@1  98.44 ( 98.49)	Acc@5 100.00 (100.00)
Epoch: [48][ 70/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 5.3986e-02 (4.1569e-02)	Acc@1  98.44 ( 98.50)	Acc@5 100.00 (100.00)
Epoch: [48][ 80/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.5259e-02 (4.0567e-02)	Acc@1 100.00 ( 98.60)	Acc@5 100.00 ( 99.99)
Epoch: [48][ 90/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 4.2633e-02 (4.0294e-02)	Acc@1  98.44 ( 98.62)	Acc@5 100.00 ( 99.99)
Epoch: [48][100/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 6.3049e-02 (4.0795e-02)	Acc@1  97.66 ( 98.56)	Acc@5 100.00 ( 99.99)
Epoch: [48][110/391]	Time  0.165 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.6022e-02 (4.0317e-02)	Acc@1 100.00 ( 98.61)	Acc@5 100.00 ( 99.99)
Epoch: [48][120/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.7546e-02 (4.0419e-02)	Acc@1  98.44 ( 98.64)	Acc@5 100.00 ( 99.99)
Epoch: [48][130/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.7969e-02 (4.0655e-02)	Acc@1 100.00 ( 98.62)	Acc@5 100.00 ( 99.99)
Epoch: [48][140/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.3904e-02 (4.0560e-02)	Acc@1  96.88 ( 98.61)	Acc@5 100.00 ( 99.99)
Epoch: [48][150/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.9661e-02 (4.0309e-02)	Acc@1  96.09 ( 98.63)	Acc@5 100.00 ( 99.99)
Epoch: [48][160/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3779e-02 (3.9958e-02)	Acc@1 100.00 ( 98.66)	Acc@5 100.00 (100.00)
Epoch: [48][170/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.3975e-02 (4.1018e-02)	Acc@1  97.66 ( 98.60)	Acc@5 100.00 (100.00)
Epoch: [48][180/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.1464e-02 (4.0557e-02)	Acc@1  98.44 ( 98.63)	Acc@5 100.00 (100.00)
Epoch: [48][190/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.2318e-02 (4.0971e-02)	Acc@1  99.22 ( 98.62)	Acc@5 100.00 (100.00)
Epoch: [48][200/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.6824e-02 (4.1654e-02)	Acc@1  97.66 ( 98.60)	Acc@5 100.00 (100.00)
Epoch: [48][210/391]	Time  0.165 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.7588e-02 (4.1446e-02)	Acc@1  98.44 ( 98.61)	Acc@5 100.00 (100.00)
Epoch: [48][220/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.3365e-02 (4.1541e-02)	Acc@1  97.66 ( 98.61)	Acc@5 100.00 (100.00)
Epoch: [48][230/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.4607e-02 (4.1290e-02)	Acc@1  99.22 ( 98.63)	Acc@5 100.00 ( 99.99)
Epoch: [48][240/391]	Time  0.165 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.3539e-02 (4.1041e-02)	Acc@1  99.22 ( 98.64)	Acc@5 100.00 ( 99.99)
Epoch: [48][250/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.2124e-02 (4.0933e-02)	Acc@1  98.44 ( 98.65)	Acc@5 100.00 ( 99.99)
Epoch: [48][260/391]	Time  0.163 ( 0.163)	Data  0.002 ( 0.002)	Loss 5.3741e-02 (4.1031e-02)	Acc@1  97.66 ( 98.63)	Acc@5 100.00 ( 99.99)
Epoch: [48][270/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5503e-02 (4.1044e-02)	Acc@1 100.00 ( 98.63)	Acc@5 100.00 ( 99.99)
Epoch: [48][280/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.1820e-02 (4.1028e-02)	Acc@1 100.00 ( 98.64)	Acc@5 100.00 ( 99.99)
Epoch: [48][290/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.0355e-02 (4.1074e-02)	Acc@1  99.22 ( 98.64)	Acc@5 100.00 ( 99.99)
Epoch: [48][300/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.5940e-02 (4.0991e-02)	Acc@1 100.00 ( 98.64)	Acc@5 100.00 ( 99.99)
Epoch: [48][310/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.6824e-02 (4.1261e-02)	Acc@1  98.44 ( 98.62)	Acc@5 100.00 ( 99.99)
Epoch: [48][320/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.0029e-02 (4.1262e-02)	Acc@1  98.44 ( 98.61)	Acc@5 100.00 ( 99.99)
Epoch: [48][330/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.5711e-02 (4.1416e-02)	Acc@1  99.22 ( 98.61)	Acc@5 100.00 ( 99.99)
Epoch: [48][340/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.7323e-02 (4.1320e-02)	Acc@1  98.44 ( 98.61)	Acc@5 100.00 ( 99.99)
Epoch: [48][350/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.8066e-02 (4.0989e-02)	Acc@1 100.00 ( 98.62)	Acc@5 100.00 ( 99.99)
Epoch: [48][360/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.2480e-02 (4.0970e-02)	Acc@1  98.44 ( 98.63)	Acc@5 100.00 ( 99.99)
Epoch: [48][370/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.0638e-02 (4.1088e-02)	Acc@1  97.66 ( 98.62)	Acc@5 100.00 ( 99.99)
Epoch: [48][380/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3412e-02 (4.1500e-02)	Acc@1 100.00 ( 98.60)	Acc@5 100.00 ( 99.99)
Epoch: [48][390/391]	Time  0.115 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5854e-02 (4.1147e-02)	Acc@1 100.00 ( 98.62)	Acc@5 100.00 ( 99.99)
## e[48] optimizer.zero_grad (sum) time: 0.5263514518737793
## e[48]       loss.backward (sum) time: 12.398078441619873
## e[48]      optimizer.step (sum) time: 25.831271171569824
## epoch[48] training(only) time: 63.605870485305786
# Switched to evaluate mode...
Test: [  0/100]	Time  0.190 ( 0.190)	Loss 1.4111e-01 (1.4111e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.048 ( 0.061)	Loss 3.4351e-01 (2.9358e-01)	Acc@1  92.00 ( 92.18)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.060 ( 0.056)	Loss 3.9453e-01 (3.1721e-01)	Acc@1  91.00 ( 92.10)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.048 ( 0.053)	Loss 3.1616e-01 (3.1761e-01)	Acc@1  93.00 ( 92.29)	Acc@5  99.00 ( 99.71)
Test: [ 40/100]	Time  0.047 ( 0.052)	Loss 4.4678e-01 (3.2349e-01)	Acc@1  90.00 ( 92.12)	Acc@5  99.00 ( 99.71)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 2.1985e-01 (3.2493e-01)	Acc@1  96.00 ( 92.16)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 2.0569e-01 (3.2026e-01)	Acc@1  92.00 ( 92.08)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 3.8135e-01 (3.1744e-01)	Acc@1  92.00 ( 92.01)	Acc@5 100.00 ( 99.80)
Test: [ 80/100]	Time  0.048 ( 0.049)	Loss 2.4841e-01 (3.1581e-01)	Acc@1  91.00 ( 92.02)	Acc@5 100.00 ( 99.81)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 3.3911e-01 (3.1637e-01)	Acc@1  91.00 ( 92.01)	Acc@5 100.00 ( 99.81)
 * Acc@1 91.930 Acc@5 99.810
### epoch[48] execution time: 68.59681701660156
EPOCH 49
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [49][  0/391]	Time  0.307 ( 0.307)	Data  0.139 ( 0.139)	Loss 5.2826e-02 (5.2826e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [49][ 10/391]	Time  0.161 ( 0.175)	Data  0.001 ( 0.014)	Loss 6.9336e-02 (4.9015e-02)	Acc@1  98.44 ( 98.51)	Acc@5 100.00 (100.00)
Epoch: [49][ 20/391]	Time  0.161 ( 0.169)	Data  0.001 ( 0.008)	Loss 4.8096e-02 (4.4346e-02)	Acc@1  98.44 ( 98.66)	Acc@5 100.00 (100.00)
Epoch: [49][ 30/391]	Time  0.165 ( 0.166)	Data  0.001 ( 0.006)	Loss 3.5583e-02 (4.3870e-02)	Acc@1  98.44 ( 98.64)	Acc@5 100.00 (100.00)
Epoch: [49][ 40/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 3.2227e-02 (4.2512e-02)	Acc@1  98.44 ( 98.65)	Acc@5 100.00 (100.00)
Epoch: [49][ 50/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 7.3120e-02 (4.6274e-02)	Acc@1  97.66 ( 98.50)	Acc@5 100.00 (100.00)
Epoch: [49][ 60/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.4664e-02 (4.4768e-02)	Acc@1 100.00 ( 98.57)	Acc@5 100.00 (100.00)
Epoch: [49][ 70/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.3562e-01 (4.5688e-02)	Acc@1  94.53 ( 98.49)	Acc@5 100.00 (100.00)
Epoch: [49][ 80/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 5.0262e-02 (4.5167e-02)	Acc@1  97.66 ( 98.48)	Acc@5 100.00 (100.00)
Epoch: [49][ 90/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.9394e-02 (4.4316e-02)	Acc@1 100.00 ( 98.52)	Acc@5 100.00 (100.00)
Epoch: [49][100/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.2468e-02 (4.5360e-02)	Acc@1  96.88 ( 98.50)	Acc@5 100.00 (100.00)
Epoch: [49][110/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.1525e-02 (4.5026e-02)	Acc@1  99.22 ( 98.53)	Acc@5 100.00 (100.00)
Epoch: [49][120/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.4424e-02 (4.4051e-02)	Acc@1  98.44 ( 98.57)	Acc@5 100.00 (100.00)
Epoch: [49][130/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.9816e-02 (4.3548e-02)	Acc@1  99.22 ( 98.60)	Acc@5 100.00 (100.00)
Epoch: [49][140/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5594e-02 (4.4454e-02)	Acc@1 100.00 ( 98.57)	Acc@5 100.00 (100.00)
Epoch: [49][150/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.3223e-02 (4.4833e-02)	Acc@1  98.44 ( 98.53)	Acc@5 100.00 (100.00)
Epoch: [49][160/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.5970e-02 (4.4056e-02)	Acc@1  99.22 ( 98.54)	Acc@5 100.00 (100.00)
Epoch: [49][170/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.2684e-02 (4.3964e-02)	Acc@1  99.22 ( 98.56)	Acc@5 100.00 (100.00)
Epoch: [49][180/391]	Time  0.165 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2848e-02 (4.4154e-02)	Acc@1 100.00 ( 98.56)	Acc@5 100.00 (100.00)
Epoch: [49][190/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5160e-02 (4.3864e-02)	Acc@1 100.00 ( 98.56)	Acc@5 100.00 (100.00)
Epoch: [49][200/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.1769e-02 (4.3205e-02)	Acc@1  98.44 ( 98.59)	Acc@5 100.00 (100.00)
Epoch: [49][210/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.7603e-02 (4.3183e-02)	Acc@1  98.44 ( 98.59)	Acc@5 100.00 (100.00)
Epoch: [49][220/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.4729e-02 (4.3006e-02)	Acc@1  99.22 ( 98.59)	Acc@5 100.00 (100.00)
Epoch: [49][230/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.7811e-02 (4.2999e-02)	Acc@1  98.44 ( 98.59)	Acc@5 100.00 (100.00)
Epoch: [49][240/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.2995e-02 (4.2433e-02)	Acc@1  99.22 ( 98.61)	Acc@5 100.00 (100.00)
Epoch: [49][250/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.2511e-02 (4.2480e-02)	Acc@1  98.44 ( 98.61)	Acc@5 100.00 (100.00)
Epoch: [49][260/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.3486e-02 (4.2477e-02)	Acc@1  97.66 ( 98.61)	Acc@5 100.00 (100.00)
Epoch: [49][270/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.0950e-02 (4.2093e-02)	Acc@1 100.00 ( 98.64)	Acc@5 100.00 (100.00)
Epoch: [49][280/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2207e-02 (4.2336e-02)	Acc@1 100.00 ( 98.62)	Acc@5 100.00 (100.00)
Epoch: [49][290/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.0995e-02 (4.2215e-02)	Acc@1  98.44 ( 98.63)	Acc@5 100.00 (100.00)
Epoch: [49][300/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.0035e-02 (4.1890e-02)	Acc@1  99.22 ( 98.64)	Acc@5 100.00 (100.00)
Epoch: [49][310/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.1229e-02 (4.1900e-02)	Acc@1  97.66 ( 98.63)	Acc@5 100.00 (100.00)
Epoch: [49][320/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.8523e-02 (4.1891e-02)	Acc@1  98.44 ( 98.63)	Acc@5 100.00 (100.00)
Epoch: [49][330/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.3965e-02 (4.2234e-02)	Acc@1  96.88 ( 98.60)	Acc@5 100.00 (100.00)
Epoch: [49][340/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.9988e-02 (4.2366e-02)	Acc@1  97.66 ( 98.59)	Acc@5 100.00 (100.00)
Epoch: [49][350/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.2349e-02 (4.2375e-02)	Acc@1 100.00 ( 98.59)	Acc@5 100.00 (100.00)
Epoch: [49][360/391]	Time  0.178 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.0394e-02 (4.2182e-02)	Acc@1  96.88 ( 98.60)	Acc@5 100.00 (100.00)
Epoch: [49][370/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.5878e-02 (4.2432e-02)	Acc@1  98.44 ( 98.58)	Acc@5 100.00 (100.00)
Epoch: [49][380/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.8458e-02 (4.2460e-02)	Acc@1 100.00 ( 98.58)	Acc@5 100.00 (100.00)
Epoch: [49][390/391]	Time  0.116 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.8574e-02 (4.2370e-02)	Acc@1  98.75 ( 98.58)	Acc@5 100.00 (100.00)
## e[49] optimizer.zero_grad (sum) time: 0.5192680358886719
## e[49]       loss.backward (sum) time: 12.410557985305786
## e[49]      optimizer.step (sum) time: 25.797646045684814
## epoch[49] training(only) time: 63.44876408576965
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 1.4270e-01 (1.4270e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.048 ( 0.061)	Loss 2.9517e-01 (3.0422e-01)	Acc@1  91.00 ( 91.36)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 4.7949e-01 (3.1710e-01)	Acc@1  89.00 ( 91.52)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 3.4619e-01 (3.1871e-01)	Acc@1  92.00 ( 92.10)	Acc@5  99.00 ( 99.68)
Test: [ 40/100]	Time  0.048 ( 0.051)	Loss 5.2344e-01 (3.2913e-01)	Acc@1  89.00 ( 91.93)	Acc@5  99.00 ( 99.66)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 2.3999e-01 (3.3404e-01)	Acc@1  96.00 ( 91.82)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 2.2131e-01 (3.2983e-01)	Acc@1  93.00 ( 91.85)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 3.3984e-01 (3.2936e-01)	Acc@1  93.00 ( 91.90)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 2.3840e-01 (3.2542e-01)	Acc@1  93.00 ( 91.99)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 2.8711e-01 (3.2699e-01)	Acc@1  92.00 ( 91.91)	Acc@5 100.00 ( 99.79)
 * Acc@1 91.870 Acc@5 99.790
### epoch[49] execution time: 68.42303228378296
EPOCH 50
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [50][  0/391]	Time  0.336 ( 0.336)	Data  0.161 ( 0.161)	Loss 1.1215e-02 (1.1215e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [50][ 10/391]	Time  0.160 ( 0.177)	Data  0.001 ( 0.016)	Loss 1.1353e-02 (2.1336e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [50][ 20/391]	Time  0.160 ( 0.170)	Data  0.001 ( 0.009)	Loss 2.0767e-02 (2.8174e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [50][ 30/391]	Time  0.161 ( 0.167)	Data  0.001 ( 0.006)	Loss 3.0792e-02 (2.9745e-02)	Acc@1  98.44 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [50][ 40/391]	Time  0.162 ( 0.166)	Data  0.001 ( 0.005)	Loss 3.0457e-02 (3.2143e-02)	Acc@1  98.44 ( 98.89)	Acc@5 100.00 (100.00)
Epoch: [50][ 50/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 5.0507e-02 (3.2536e-02)	Acc@1  97.66 ( 98.87)	Acc@5 100.00 (100.00)
Epoch: [50][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 5.6580e-02 (3.2812e-02)	Acc@1  97.66 ( 98.91)	Acc@5 100.00 (100.00)
Epoch: [50][ 70/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 3.2684e-02 (3.2943e-02)	Acc@1 100.00 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [50][ 80/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 4.5135e-02 (3.3355e-02)	Acc@1  99.22 ( 98.94)	Acc@5 100.00 (100.00)
Epoch: [50][ 90/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.7878e-02 (3.3726e-02)	Acc@1  99.22 ( 98.94)	Acc@5 100.00 (100.00)
Epoch: [50][100/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.8818e-02 (3.5145e-02)	Acc@1  98.44 ( 98.86)	Acc@5 100.00 (100.00)
Epoch: [50][110/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.6047e-02 (3.4707e-02)	Acc@1 100.00 ( 98.87)	Acc@5 100.00 (100.00)
Epoch: [50][120/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.6335e-02 (3.5023e-02)	Acc@1  97.66 ( 98.86)	Acc@5 100.00 (100.00)
Epoch: [50][130/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2527e-02 (3.5122e-02)	Acc@1 100.00 ( 98.84)	Acc@5 100.00 (100.00)
Epoch: [50][140/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.5380e-02 (3.5409e-02)	Acc@1  99.22 ( 98.85)	Acc@5 100.00 (100.00)
Epoch: [50][150/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.2781e-02 (3.5988e-02)	Acc@1  99.22 ( 98.82)	Acc@5 100.00 (100.00)
Epoch: [50][160/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.7078e-02 (3.5815e-02)	Acc@1  97.66 ( 98.83)	Acc@5 100.00 (100.00)
Epoch: [50][170/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.2317e-02 (3.6796e-02)	Acc@1  98.44 ( 98.78)	Acc@5 100.00 (100.00)
Epoch: [50][180/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.7659e-02 (3.6266e-02)	Acc@1  99.22 ( 98.80)	Acc@5 100.00 (100.00)
Epoch: [50][190/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.7159e-02 (3.6900e-02)	Acc@1  98.44 ( 98.76)	Acc@5 100.00 (100.00)
Epoch: [50][200/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.1716e-02 (3.7851e-02)	Acc@1  96.88 ( 98.71)	Acc@5 100.00 (100.00)
Epoch: [50][210/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.6413e-02 (3.7821e-02)	Acc@1  99.22 ( 98.71)	Acc@5 100.00 (100.00)
Epoch: [50][220/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.1758e-02 (3.8091e-02)	Acc@1  97.66 ( 98.69)	Acc@5  99.22 (100.00)
Epoch: [50][230/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.8824e-02 (3.8190e-02)	Acc@1  99.22 ( 98.70)	Acc@5 100.00 (100.00)
Epoch: [50][240/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.6213e-02 (3.7885e-02)	Acc@1  98.44 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [50][250/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.4271e-02 (3.7873e-02)	Acc@1  98.44 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [50][260/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.8910e-02 (3.7989e-02)	Acc@1  99.22 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [50][270/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.0935e-02 (3.8006e-02)	Acc@1  99.22 ( 98.71)	Acc@5 100.00 (100.00)
Epoch: [50][280/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3969e-02 (3.8035e-02)	Acc@1 100.00 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [50][290/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.9326e-02 (3.8082e-02)	Acc@1  98.44 ( 98.73)	Acc@5 100.00 (100.00)
Epoch: [50][300/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.8330e-02 (3.8251e-02)	Acc@1  97.66 ( 98.71)	Acc@5 100.00 (100.00)
Epoch: [50][310/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.7872e-02 (3.8214e-02)	Acc@1  98.44 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [50][320/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.4800e-02 (3.8544e-02)	Acc@1  98.44 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [50][330/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.5502e-02 (3.8737e-02)	Acc@1  98.44 ( 98.71)	Acc@5 100.00 (100.00)
Epoch: [50][340/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.6205e-02 (3.8585e-02)	Acc@1 100.00 ( 98.71)	Acc@5 100.00 (100.00)
Epoch: [50][350/391]	Time  0.177 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.9928e-02 (3.9043e-02)	Acc@1  99.22 ( 98.70)	Acc@5 100.00 (100.00)
Epoch: [50][360/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 5.0934e-02 (3.9074e-02)	Acc@1  97.66 ( 98.71)	Acc@5 100.00 (100.00)
Epoch: [50][370/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 5.1636e-02 (3.9043e-02)	Acc@1  97.66 ( 98.70)	Acc@5 100.00 (100.00)
Epoch: [50][380/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 4.3976e-02 (3.8943e-02)	Acc@1  97.66 ( 98.71)	Acc@5 100.00 (100.00)
Epoch: [50][390/391]	Time  0.117 ( 0.162)	Data  0.001 ( 0.001)	Loss 6.9702e-02 (3.9026e-02)	Acc@1  97.50 ( 98.70)	Acc@5 100.00 (100.00)
## e[50] optimizer.zero_grad (sum) time: 0.5212006568908691
## e[50]       loss.backward (sum) time: 12.390732526779175
## e[50]      optimizer.step (sum) time: 25.80691957473755
## epoch[50] training(only) time: 63.500152587890625
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 1.6699e-01 (1.6699e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 3.1689e-01 (2.9599e-01)	Acc@1  92.00 ( 92.27)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 4.7144e-01 (3.1757e-01)	Acc@1  90.00 ( 92.05)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 3.7231e-01 (3.2160e-01)	Acc@1  89.00 ( 92.16)	Acc@5  99.00 ( 99.65)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 4.9951e-01 (3.2822e-01)	Acc@1  89.00 ( 92.12)	Acc@5 100.00 ( 99.63)
Test: [ 50/100]	Time  0.055 ( 0.050)	Loss 1.5662e-01 (3.2812e-01)	Acc@1  95.00 ( 91.96)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.047 ( 0.049)	Loss 2.2412e-01 (3.2338e-01)	Acc@1  91.00 ( 91.89)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 3.2178e-01 (3.2235e-01)	Acc@1  92.00 ( 91.90)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 2.7197e-01 (3.2020e-01)	Acc@1  92.00 ( 91.95)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 3.0713e-01 (3.2282e-01)	Acc@1  91.00 ( 91.89)	Acc@5 100.00 ( 99.79)
 * Acc@1 91.870 Acc@5 99.800
### epoch[50] execution time: 68.44666385650635
EPOCH 51
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [51][  0/391]	Time  0.323 ( 0.323)	Data  0.146 ( 0.146)	Loss 1.2123e-02 (1.2123e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [51][ 10/391]	Time  0.172 ( 0.177)	Data  0.001 ( 0.014)	Loss 4.6570e-02 (3.4957e-02)	Acc@1  98.44 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [51][ 20/391]	Time  0.162 ( 0.169)	Data  0.001 ( 0.008)	Loss 1.9577e-02 (3.1488e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [51][ 30/391]	Time  0.161 ( 0.166)	Data  0.001 ( 0.006)	Loss 4.1992e-02 (3.3268e-02)	Acc@1  97.66 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [51][ 40/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.005)	Loss 5.1758e-02 (3.3213e-02)	Acc@1  96.88 ( 98.95)	Acc@5 100.00 ( 99.98)
Epoch: [51][ 50/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.7029e-02 (3.4258e-02)	Acc@1  99.22 ( 98.93)	Acc@5 100.00 ( 99.98)
Epoch: [51][ 60/391]	Time  0.168 ( 0.164)	Data  0.001 ( 0.004)	Loss 2.2858e-02 (3.3314e-02)	Acc@1 100.00 ( 98.99)	Acc@5 100.00 ( 99.99)
Epoch: [51][ 70/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.6672e-02 (3.3368e-02)	Acc@1  99.22 ( 98.99)	Acc@5 100.00 ( 99.99)
Epoch: [51][ 80/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.2837e-02 (3.3698e-02)	Acc@1  99.22 ( 98.95)	Acc@5 100.00 ( 99.99)
Epoch: [51][ 90/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 7.5073e-02 (3.3536e-02)	Acc@1  96.88 ( 98.94)	Acc@5 100.00 ( 99.99)
Epoch: [51][100/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.5339e-02 (3.2853e-02)	Acc@1  99.22 ( 98.98)	Acc@5 100.00 ( 99.99)
Epoch: [51][110/391]	Time  0.171 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.6906e-02 (3.3018e-02)	Acc@1  98.44 ( 98.99)	Acc@5 100.00 ( 99.99)
Epoch: [51][120/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.2755e-02 (3.3585e-02)	Acc@1  98.44 ( 98.96)	Acc@5 100.00 ( 99.99)
Epoch: [51][130/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.0111e-02 (3.3080e-02)	Acc@1 100.00 ( 98.97)	Acc@5 100.00 ( 99.99)
Epoch: [51][140/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.8723e-02 (3.3139e-02)	Acc@1 100.00 ( 98.96)	Acc@5 100.00 ( 99.99)
Epoch: [51][150/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.2898e-02 (3.2608e-02)	Acc@1  98.44 ( 98.97)	Acc@5 100.00 ( 99.99)
Epoch: [51][160/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.6143e-02 (3.3005e-02)	Acc@1  98.44 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [51][170/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.8796e-02 (3.4077e-02)	Acc@1  96.88 ( 98.90)	Acc@5 100.00 (100.00)
Epoch: [51][180/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.1027e-02 (3.4503e-02)	Acc@1  99.22 ( 98.89)	Acc@5 100.00 (100.00)
Epoch: [51][190/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.0801e-02 (3.4455e-02)	Acc@1  97.66 ( 98.90)	Acc@5 100.00 (100.00)
Epoch: [51][200/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.2388e-02 (3.4746e-02)	Acc@1  97.66 ( 98.88)	Acc@5 100.00 (100.00)
Epoch: [51][210/391]	Time  0.165 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.4607e-02 (3.4405e-02)	Acc@1  98.44 ( 98.89)	Acc@5 100.00 (100.00)
Epoch: [51][220/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.1716e-02 (3.4784e-02)	Acc@1  97.66 ( 98.87)	Acc@5 100.00 (100.00)
Epoch: [51][230/391]	Time  0.167 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.8283e-03 (3.4534e-02)	Acc@1 100.00 ( 98.87)	Acc@5 100.00 (100.00)
Epoch: [51][240/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.6108e-02 (3.4486e-02)	Acc@1 100.00 ( 98.88)	Acc@5 100.00 (100.00)
Epoch: [51][250/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.7689e-02 (3.4849e-02)	Acc@1  98.44 ( 98.88)	Acc@5 100.00 (100.00)
Epoch: [51][260/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.6474e-02 (3.4974e-02)	Acc@1 100.00 ( 98.87)	Acc@5 100.00 (100.00)
Epoch: [51][270/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.6539e-02 (3.5177e-02)	Acc@1  98.44 ( 98.85)	Acc@5 100.00 (100.00)
Epoch: [51][280/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.0994e-02 (3.5269e-02)	Acc@1  98.44 ( 98.85)	Acc@5 100.00 (100.00)
Epoch: [51][290/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5518e-02 (3.5042e-02)	Acc@1 100.00 ( 98.87)	Acc@5 100.00 (100.00)
Epoch: [51][300/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.5715e-02 (3.4908e-02)	Acc@1  97.66 ( 98.87)	Acc@5 100.00 (100.00)
Epoch: [51][310/391]	Time  0.166 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0956e-02 (3.5359e-02)	Acc@1 100.00 ( 98.84)	Acc@5 100.00 ( 99.99)
Epoch: [51][320/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.1239e-02 (3.5237e-02)	Acc@1  98.44 ( 98.85)	Acc@5 100.00 (100.00)
Epoch: [51][330/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6449e-02 (3.5331e-02)	Acc@1 100.00 ( 98.84)	Acc@5 100.00 (100.00)
Epoch: [51][340/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.2786e-02 (3.5202e-02)	Acc@1  98.44 ( 98.84)	Acc@5 100.00 (100.00)
Epoch: [51][350/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.8116e-02 (3.5153e-02)	Acc@1  98.44 ( 98.84)	Acc@5 100.00 (100.00)
Epoch: [51][360/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.6346e-02 (3.5288e-02)	Acc@1  99.22 ( 98.84)	Acc@5 100.00 (100.00)
Epoch: [51][370/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.001)	Loss 2.3926e-02 (3.5269e-02)	Acc@1 100.00 ( 98.84)	Acc@5 100.00 (100.00)
Epoch: [51][380/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.001)	Loss 4.0741e-02 (3.5411e-02)	Acc@1  98.44 ( 98.84)	Acc@5 100.00 (100.00)
Epoch: [51][390/391]	Time  0.116 ( 0.162)	Data  0.001 ( 0.001)	Loss 7.4585e-02 (3.5661e-02)	Acc@1  97.50 ( 98.82)	Acc@5 100.00 (100.00)
## e[51] optimizer.zero_grad (sum) time: 0.5318577289581299
## e[51]       loss.backward (sum) time: 12.411720275878906
## e[51]      optimizer.step (sum) time: 25.822826862335205
## epoch[51] training(only) time: 63.609275341033936
# Switched to evaluate mode...
Test: [  0/100]	Time  0.182 ( 0.182)	Loss 1.7554e-01 (1.7554e-01)	Acc@1  94.00 ( 94.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 3.1885e-01 (2.9451e-01)	Acc@1  92.00 ( 91.91)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 4.4629e-01 (3.2409e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 3.1592e-01 (3.2439e-01)	Acc@1  92.00 ( 92.13)	Acc@5  98.00 ( 99.65)
Test: [ 40/100]	Time  0.048 ( 0.051)	Loss 5.0635e-01 (3.3245e-01)	Acc@1  88.00 ( 92.07)	Acc@5  99.00 ( 99.63)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 2.0605e-01 (3.3405e-01)	Acc@1  96.00 ( 92.14)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.047 ( 0.049)	Loss 2.2607e-01 (3.2620e-01)	Acc@1  94.00 ( 92.11)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 4.0527e-01 (3.2741e-01)	Acc@1  92.00 ( 92.17)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 2.2351e-01 (3.2778e-01)	Acc@1  93.00 ( 92.19)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 3.8110e-01 (3.2988e-01)	Acc@1  92.00 ( 92.12)	Acc@5 100.00 ( 99.79)
 * Acc@1 92.090 Acc@5 99.800
### epoch[51] execution time: 68.56876182556152
EPOCH 52
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [52][  0/391]	Time  0.320 ( 0.320)	Data  0.148 ( 0.148)	Loss 2.3819e-02 (2.3819e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [52][ 10/391]	Time  0.160 ( 0.176)	Data  0.001 ( 0.015)	Loss 4.6295e-02 (3.0282e-02)	Acc@1  97.66 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [52][ 20/391]	Time  0.160 ( 0.169)	Data  0.001 ( 0.008)	Loss 1.5945e-02 (3.2180e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [52][ 30/391]	Time  0.166 ( 0.167)	Data  0.001 ( 0.006)	Loss 4.4769e-02 (3.2831e-02)	Acc@1  97.66 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [52][ 40/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.005)	Loss 2.6718e-02 (3.0421e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [52][ 50/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.3802e-02 (3.3384e-02)	Acc@1 100.00 ( 98.94)	Acc@5 100.00 (100.00)
Epoch: [52][ 60/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.8631e-02 (3.1872e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [52][ 70/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.003)	Loss 4.7150e-02 (3.2239e-02)	Acc@1  99.22 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [52][ 80/391]	Time  0.169 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.6174e-02 (3.2587e-02)	Acc@1  99.22 ( 98.97)	Acc@5 100.00 ( 99.99)
Epoch: [52][ 90/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.0277e-02 (3.4297e-02)	Acc@1 100.00 ( 98.93)	Acc@5 100.00 ( 99.99)
Epoch: [52][100/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.6602e-02 (3.3362e-02)	Acc@1 100.00 ( 98.97)	Acc@5 100.00 ( 99.99)
Epoch: [52][110/391]	Time  0.163 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.7715e-02 (3.2573e-02)	Acc@1 100.00 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [52][120/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.002)	Loss 4.9622e-02 (3.1779e-02)	Acc@1  98.44 ( 99.02)	Acc@5 100.00 ( 99.99)
Epoch: [52][130/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.002)	Loss 4.3579e-02 (3.2020e-02)	Acc@1  98.44 ( 98.97)	Acc@5 100.00 ( 99.99)
Epoch: [52][140/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.3691e-02 (3.1981e-02)	Acc@1  98.44 ( 98.96)	Acc@5 100.00 ( 99.99)
Epoch: [52][150/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.1147e-02 (3.2000e-02)	Acc@1  98.44 ( 98.95)	Acc@5 100.00 ( 99.99)
Epoch: [52][160/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.0762e-02 (3.2012e-02)	Acc@1 100.00 ( 98.96)	Acc@5 100.00 ( 99.99)
Epoch: [52][170/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.3091e-02 (3.1726e-02)	Acc@1  98.44 ( 98.99)	Acc@5 100.00 ( 99.99)
Epoch: [52][180/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.9480e-02 (3.2464e-02)	Acc@1  99.22 ( 98.98)	Acc@5 100.00 ( 99.99)
Epoch: [52][190/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.4033e-02 (3.2634e-02)	Acc@1  99.22 ( 98.97)	Acc@5 100.00 ( 99.99)
Epoch: [52][200/391]	Time  0.175 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.8849e-02 (3.2589e-02)	Acc@1  98.44 ( 98.97)	Acc@5 100.00 ( 99.99)
Epoch: [52][210/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0674e-02 (3.2225e-02)	Acc@1 100.00 ( 98.99)	Acc@5 100.00 ( 99.99)
Epoch: [52][220/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.1072e-02 (3.2098e-02)	Acc@1 100.00 ( 99.00)	Acc@5 100.00 ( 99.99)
Epoch: [52][230/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.8574e-02 (3.2314e-02)	Acc@1  98.44 ( 98.99)	Acc@5 100.00 ( 99.99)
Epoch: [52][240/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.6528e-02 (3.2483e-02)	Acc@1  99.22 ( 98.98)	Acc@5 100.00 ( 99.99)
Epoch: [52][250/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.2664e-02 (3.2374e-02)	Acc@1  97.66 ( 98.97)	Acc@5 100.00 ( 99.99)
Epoch: [52][260/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.8899e-02 (3.2747e-02)	Acc@1  96.88 ( 98.95)	Acc@5 100.00 ( 99.99)
Epoch: [52][270/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.9287e-02 (3.2984e-02)	Acc@1  99.22 ( 98.95)	Acc@5 100.00 ( 99.99)
Epoch: [52][280/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.6172e-02 (3.3166e-02)	Acc@1  98.44 ( 98.95)	Acc@5 100.00 ( 99.99)
Epoch: [52][290/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.0262e-02 (3.3571e-02)	Acc@1  98.44 ( 98.93)	Acc@5 100.00 ( 99.99)
Epoch: [52][300/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.9774e-02 (3.3519e-02)	Acc@1  98.44 ( 98.94)	Acc@5 100.00 ( 99.99)
Epoch: [52][310/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.6946e-02 (3.3915e-02)	Acc@1  99.22 ( 98.92)	Acc@5 100.00 ( 99.99)
Epoch: [52][320/391]	Time  0.170 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.7505e-02 (3.4104e-02)	Acc@1  97.66 ( 98.91)	Acc@5 100.00 ( 99.99)
Epoch: [52][330/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.1117e-02 (3.4275e-02)	Acc@1  97.66 ( 98.90)	Acc@5 100.00 ( 99.99)
Epoch: [52][340/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.5054e-02 (3.4756e-02)	Acc@1  98.44 ( 98.87)	Acc@5 100.00 ( 99.99)
Epoch: [52][350/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.1301e-02 (3.4633e-02)	Acc@1 100.00 ( 98.87)	Acc@5 100.00 ( 99.99)
Epoch: [52][360/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.4454e-02 (3.4820e-02)	Acc@1  98.44 ( 98.87)	Acc@5 100.00 ( 99.99)
Epoch: [52][370/391]	Time  0.172 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7975e-02 (3.4618e-02)	Acc@1  99.22 ( 98.88)	Acc@5 100.00 ( 99.99)
Epoch: [52][380/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.7069e-02 (3.4429e-02)	Acc@1  99.22 ( 98.89)	Acc@5 100.00 ( 99.99)
Epoch: [52][390/391]	Time  0.118 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.0822e-02 (3.4443e-02)	Acc@1  97.50 ( 98.88)	Acc@5 100.00 ( 99.99)
## e[52] optimizer.zero_grad (sum) time: 0.5331695079803467
## e[52]       loss.backward (sum) time: 12.385149717330933
## e[52]      optimizer.step (sum) time: 25.814377784729004
## epoch[52] training(only) time: 63.64538359642029
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 1.5845e-01 (1.5845e-01)	Acc@1  97.00 ( 97.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 3.0542e-01 (3.0000e-01)	Acc@1  93.00 ( 92.18)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.047 ( 0.054)	Loss 3.7769e-01 (3.1839e-01)	Acc@1  92.00 ( 92.05)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 3.4131e-01 (3.1896e-01)	Acc@1  92.00 ( 92.29)	Acc@5  99.00 ( 99.71)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 5.5469e-01 (3.3203e-01)	Acc@1  89.00 ( 92.15)	Acc@5  99.00 ( 99.68)
Test: [ 50/100]	Time  0.050 ( 0.050)	Loss 1.9043e-01 (3.3093e-01)	Acc@1  95.00 ( 92.10)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.048 ( 0.050)	Loss 2.1851e-01 (3.2692e-01)	Acc@1  95.00 ( 91.97)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 3.2886e-01 (3.2604e-01)	Acc@1  94.00 ( 91.93)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 2.3779e-01 (3.2761e-01)	Acc@1  93.00 ( 91.89)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 3.5693e-01 (3.2808e-01)	Acc@1  91.00 ( 91.89)	Acc@5 100.00 ( 99.79)
 * Acc@1 91.860 Acc@5 99.800
### epoch[52] execution time: 68.63257813453674
EPOCH 53
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [53][  0/391]	Time  0.337 ( 0.337)	Data  0.155 ( 0.155)	Loss 4.6448e-02 (4.6448e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [53][ 10/391]	Time  0.162 ( 0.177)	Data  0.001 ( 0.015)	Loss 4.5807e-02 (3.3191e-02)	Acc@1  98.44 ( 98.58)	Acc@5 100.00 (100.00)
Epoch: [53][ 20/391]	Time  0.160 ( 0.170)	Data  0.001 ( 0.008)	Loss 2.4506e-02 (3.4740e-02)	Acc@1  99.22 ( 98.62)	Acc@5 100.00 (100.00)
Epoch: [53][ 30/391]	Time  0.160 ( 0.167)	Data  0.001 ( 0.006)	Loss 3.5980e-02 (3.4197e-02)	Acc@1  99.22 ( 98.71)	Acc@5 100.00 (100.00)
Epoch: [53][ 40/391]	Time  0.162 ( 0.165)	Data  0.001 ( 0.005)	Loss 2.5238e-02 (3.5297e-02)	Acc@1  99.22 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [53][ 50/391]	Time  0.175 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.0681e-02 (3.3763e-02)	Acc@1 100.00 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [53][ 60/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.004)	Loss 2.9007e-02 (3.3518e-02)	Acc@1  99.22 ( 98.82)	Acc@5 100.00 (100.00)
Epoch: [53][ 70/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.7618e-02 (3.3193e-02)	Acc@1  99.22 ( 98.84)	Acc@5 100.00 (100.00)
Epoch: [53][ 80/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.1253e-02 (3.2845e-02)	Acc@1 100.00 ( 98.84)	Acc@5 100.00 (100.00)
Epoch: [53][ 90/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 4.9011e-02 (3.3406e-02)	Acc@1  97.66 ( 98.81)	Acc@5 100.00 (100.00)
Epoch: [53][100/391]	Time  0.172 ( 0.164)	Data  0.001 ( 0.003)	Loss 3.2440e-02 (3.2800e-02)	Acc@1  99.22 ( 98.86)	Acc@5 100.00 (100.00)
Epoch: [53][110/391]	Time  0.167 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.5217e-02 (3.3112e-02)	Acc@1  99.22 ( 98.84)	Acc@5 100.00 (100.00)
Epoch: [53][120/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.5711e-02 (3.3453e-02)	Acc@1 100.00 ( 98.85)	Acc@5 100.00 (100.00)
Epoch: [53][130/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.8961e-02 (3.2777e-02)	Acc@1  99.22 ( 98.90)	Acc@5 100.00 (100.00)
Epoch: [53][140/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1497e-02 (3.2853e-02)	Acc@1  99.22 ( 98.89)	Acc@5 100.00 (100.00)
Epoch: [53][150/391]	Time  0.168 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.0624e-02 (3.3410e-02)	Acc@1  99.22 ( 98.88)	Acc@5 100.00 (100.00)
Epoch: [53][160/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.9312e-02 (3.2974e-02)	Acc@1  99.22 ( 98.91)	Acc@5 100.00 (100.00)
Epoch: [53][170/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.0791e-02 (3.2781e-02)	Acc@1  96.09 ( 98.90)	Acc@5 100.00 (100.00)
Epoch: [53][180/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.6345e-02 (3.2971e-02)	Acc@1  98.44 ( 98.90)	Acc@5 100.00 (100.00)
Epoch: [53][190/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.4338e-02 (3.2736e-02)	Acc@1  99.22 ( 98.91)	Acc@5 100.00 (100.00)
Epoch: [53][200/391]	Time  0.172 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.3833e-02 (3.3017e-02)	Acc@1  97.66 ( 98.89)	Acc@5 100.00 (100.00)
Epoch: [53][210/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.7231e-02 (3.2812e-02)	Acc@1  98.44 ( 98.90)	Acc@5 100.00 (100.00)
Epoch: [53][220/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.3477e-02 (3.2904e-02)	Acc@1  99.22 ( 98.90)	Acc@5 100.00 (100.00)
Epoch: [53][230/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7883e-02 (3.3066e-02)	Acc@1 100.00 ( 98.89)	Acc@5 100.00 (100.00)
Epoch: [53][240/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.7079e-02 (3.2849e-02)	Acc@1  98.44 ( 98.90)	Acc@5 100.00 (100.00)
Epoch: [53][250/391]	Time  0.169 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.0314e-02 (3.2485e-02)	Acc@1  98.44 ( 98.92)	Acc@5 100.00 (100.00)
Epoch: [53][260/391]	Time  0.178 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.2877e-02 (3.2859e-02)	Acc@1  98.44 ( 98.91)	Acc@5 100.00 (100.00)
Epoch: [53][270/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.2949e-02 (3.2507e-02)	Acc@1  99.22 ( 98.91)	Acc@5 100.00 (100.00)
Epoch: [53][280/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.7292e-02 (3.2792e-02)	Acc@1  99.22 ( 98.91)	Acc@5 100.00 (100.00)
Epoch: [53][290/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.6352e-02 (3.2664e-02)	Acc@1  99.22 ( 98.92)	Acc@5 100.00 (100.00)
Epoch: [53][300/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.1656e-02 (3.2874e-02)	Acc@1  96.88 ( 98.90)	Acc@5 100.00 (100.00)
Epoch: [53][310/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.3547e-02 (3.2816e-02)	Acc@1  98.44 ( 98.91)	Acc@5 100.00 (100.00)
Epoch: [53][320/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.7023e-02 (3.2490e-02)	Acc@1  99.22 ( 98.92)	Acc@5 100.00 (100.00)
Epoch: [53][330/391]	Time  0.174 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7929e-02 (3.2440e-02)	Acc@1  99.22 ( 98.92)	Acc@5 100.00 (100.00)
Epoch: [53][340/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.3814e-03 (3.2364e-02)	Acc@1 100.00 ( 98.94)	Acc@5 100.00 (100.00)
Epoch: [53][350/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.3575e-02 (3.2412e-02)	Acc@1  99.22 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [53][360/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.3792e-02 (3.2678e-02)	Acc@1  96.88 ( 98.92)	Acc@5 100.00 (100.00)
Epoch: [53][370/391]	Time  0.165 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1055e-02 (3.2469e-02)	Acc@1 100.00 ( 98.92)	Acc@5 100.00 (100.00)
Epoch: [53][380/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.7756e-02 (3.2464e-02)	Acc@1  98.44 ( 98.92)	Acc@5 100.00 (100.00)
Epoch: [53][390/391]	Time  0.118 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.8580e-02 (3.2401e-02)	Acc@1 100.00 ( 98.93)	Acc@5 100.00 (100.00)
## e[53] optimizer.zero_grad (sum) time: 0.5312533378601074
## e[53]       loss.backward (sum) time: 12.426021337509155
## e[53]      optimizer.step (sum) time: 25.777158975601196
## epoch[53] training(only) time: 63.621389865875244
# Switched to evaluate mode...
Test: [  0/100]	Time  0.181 ( 0.181)	Loss 1.4294e-01 (1.4294e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.058)	Loss 2.8174e-01 (3.0896e-01)	Acc@1  92.00 ( 91.64)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.047 ( 0.053)	Loss 4.7363e-01 (3.3490e-01)	Acc@1  92.00 ( 91.67)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.047 ( 0.051)	Loss 3.6963e-01 (3.3867e-01)	Acc@1  93.00 ( 91.81)	Acc@5  99.00 ( 99.68)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 4.7119e-01 (3.4315e-01)	Acc@1  91.00 ( 91.98)	Acc@5  99.00 ( 99.71)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 2.4023e-01 (3.4811e-01)	Acc@1  95.00 ( 91.92)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.047 ( 0.049)	Loss 2.2644e-01 (3.4286e-01)	Acc@1  93.00 ( 91.89)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 3.7061e-01 (3.4593e-01)	Acc@1  91.00 ( 91.90)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 2.1045e-01 (3.4609e-01)	Acc@1  94.00 ( 91.99)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.047 ( 0.048)	Loss 3.3594e-01 (3.4661e-01)	Acc@1  89.00 ( 91.89)	Acc@5 100.00 ( 99.79)
 * Acc@1 91.820 Acc@5 99.790
### epoch[53] execution time: 68.55542922019958
EPOCH 54
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [54][  0/391]	Time  0.336 ( 0.336)	Data  0.156 ( 0.156)	Loss 1.8097e-02 (1.8097e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [54][ 10/391]	Time  0.161 ( 0.177)	Data  0.001 ( 0.015)	Loss 9.0332e-03 (3.2331e-02)	Acc@1 100.00 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [54][ 20/391]	Time  0.161 ( 0.170)	Data  0.001 ( 0.009)	Loss 6.3843e-02 (3.0313e-02)	Acc@1  96.88 ( 98.92)	Acc@5 100.00 (100.00)
Epoch: [54][ 30/391]	Time  0.161 ( 0.167)	Data  0.001 ( 0.006)	Loss 3.7079e-02 (2.9316e-02)	Acc@1  98.44 ( 98.94)	Acc@5 100.00 (100.00)
Epoch: [54][ 40/391]	Time  0.162 ( 0.167)	Data  0.001 ( 0.005)	Loss 2.1759e-02 (2.8439e-02)	Acc@1 100.00 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [54][ 50/391]	Time  0.161 ( 0.166)	Data  0.001 ( 0.004)	Loss 2.8534e-02 (2.8838e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [54][ 60/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.9012e-02 (2.8202e-02)	Acc@1 100.00 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [54][ 70/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.003)	Loss 2.3575e-02 (2.8617e-02)	Acc@1 100.00 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [54][ 80/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.8234e-02 (2.8950e-02)	Acc@1  99.22 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [54][ 90/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.0721e-02 (2.8964e-02)	Acc@1 100.00 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [54][100/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 3.4241e-02 (2.9111e-02)	Acc@1  99.22 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [54][110/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.003)	Loss 4.1443e-02 (2.9568e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [54][120/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.002)	Loss 5.0781e-02 (3.0441e-02)	Acc@1  96.88 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [54][130/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.002)	Loss 2.4002e-02 (3.0540e-02)	Acc@1 100.00 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [54][140/391]	Time  0.165 ( 0.164)	Data  0.001 ( 0.002)	Loss 5.3894e-02 (3.0467e-02)	Acc@1  98.44 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [54][150/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.002)	Loss 2.9297e-02 (3.0214e-02)	Acc@1  99.22 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [54][160/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4641e-02 (2.9910e-02)	Acc@1 100.00 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [54][170/391]	Time  0.165 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.4261e-02 (3.0042e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [54][180/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.7445e-02 (2.9876e-02)	Acc@1 100.00 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [54][190/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6190e-02 (2.9743e-02)	Acc@1 100.00 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [54][200/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.5736e-02 (2.9940e-02)	Acc@1  97.66 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [54][210/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3824e-02 (3.0356e-02)	Acc@1 100.00 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [54][220/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.5156e-02 (3.0436e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [54][230/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.1264e-03 (2.9970e-02)	Acc@1 100.00 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [54][240/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.7079e-02 (3.0022e-02)	Acc@1  97.66 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [54][250/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.6316e-02 (3.0011e-02)	Acc@1  98.44 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [54][260/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0803e-02 (3.0099e-02)	Acc@1 100.00 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [54][270/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.8176e-02 (3.0134e-02)	Acc@1  96.88 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [54][280/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.7298e-02 (3.0251e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [54][290/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5388e-02 (3.0651e-02)	Acc@1 100.00 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [54][300/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.4708e-02 (3.0614e-02)	Acc@1  99.22 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [54][310/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.6591e-02 (3.0491e-02)	Acc@1  98.44 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [54][320/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.6321e-02 (3.0555e-02)	Acc@1  98.44 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [54][330/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7471e-02 (3.0357e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [54][340/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2260e-02 (3.0468e-02)	Acc@1 100.00 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [54][350/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.9398e-02 (3.0795e-02)	Acc@1  98.44 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [54][360/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.5084e-02 (3.0925e-02)	Acc@1  97.66 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [54][370/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0864e-02 (3.1133e-02)	Acc@1 100.00 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [54][380/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.5695e-02 (3.1210e-02)	Acc@1  97.66 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [54][390/391]	Time  0.116 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3702e-02 (3.1324e-02)	Acc@1 100.00 ( 98.97)	Acc@5 100.00 (100.00)
## e[54] optimizer.zero_grad (sum) time: 0.5341100692749023
## e[54]       loss.backward (sum) time: 12.4222412109375
## e[54]      optimizer.step (sum) time: 25.803011655807495
## epoch[54] training(only) time: 63.67264723777771
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 1.4648e-01 (1.4648e-01)	Acc@1  97.00 ( 97.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 3.1323e-01 (3.1446e-01)	Acc@1  92.00 ( 91.91)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.050 ( 0.055)	Loss 5.2148e-01 (3.3796e-01)	Acc@1  91.00 ( 91.67)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.048 ( 0.053)	Loss 3.1738e-01 (3.3409e-01)	Acc@1  93.00 ( 92.06)	Acc@5  99.00 ( 99.71)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 5.2100e-01 (3.3962e-01)	Acc@1  89.00 ( 92.20)	Acc@5  99.00 ( 99.73)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 1.9238e-01 (3.4152e-01)	Acc@1  94.00 ( 92.16)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 2.3755e-01 (3.3624e-01)	Acc@1  93.00 ( 92.11)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 3.3521e-01 (3.3444e-01)	Acc@1  93.00 ( 92.10)	Acc@5 100.00 ( 99.82)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 1.8921e-01 (3.3434e-01)	Acc@1  93.00 ( 92.16)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 3.9722e-01 (3.3796e-01)	Acc@1  90.00 ( 92.04)	Acc@5 100.00 ( 99.81)
 * Acc@1 91.980 Acc@5 99.810
### epoch[54] execution time: 68.65718340873718
EPOCH 55
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [55][  0/391]	Time  0.315 ( 0.315)	Data  0.141 ( 0.141)	Loss 4.0344e-02 (4.0344e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [55][ 10/391]	Time  0.162 ( 0.176)	Data  0.001 ( 0.014)	Loss 4.1046e-02 (4.3015e-02)	Acc@1  99.22 ( 98.58)	Acc@5 100.00 (100.00)
Epoch: [55][ 20/391]	Time  0.161 ( 0.169)	Data  0.001 ( 0.008)	Loss 2.0615e-02 (3.9409e-02)	Acc@1  98.44 ( 98.51)	Acc@5 100.00 (100.00)
Epoch: [55][ 30/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.006)	Loss 2.0477e-02 (3.3770e-02)	Acc@1  99.22 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [55][ 40/391]	Time  0.172 ( 0.165)	Data  0.001 ( 0.005)	Loss 3.9001e-02 (3.2660e-02)	Acc@1  98.44 ( 98.84)	Acc@5 100.00 (100.00)
Epoch: [55][ 50/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.004)	Loss 3.6407e-02 (3.1584e-02)	Acc@1  99.22 ( 98.90)	Acc@5 100.00 (100.00)
Epoch: [55][ 60/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.6159e-02 (3.0985e-02)	Acc@1 100.00 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [55][ 70/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.3819e-02 (3.0410e-02)	Acc@1  99.22 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [55][ 80/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.6434e-02 (2.9871e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [55][ 90/391]	Time  0.170 ( 0.163)	Data  0.001 ( 0.003)	Loss 8.7967e-03 (2.8662e-02)	Acc@1 100.00 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [55][100/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.9368e-02 (2.8652e-02)	Acc@1  97.66 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [55][110/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.2087e-03 (2.9159e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [55][120/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.0229e-03 (2.8450e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [55][130/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.2297e-02 (2.8378e-02)	Acc@1  99.22 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [55][140/391]	Time  0.168 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.9041e-02 (2.9421e-02)	Acc@1  96.88 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [55][150/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7731e-02 (2.8821e-02)	Acc@1  99.22 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [55][160/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.2806e-02 (2.8612e-02)	Acc@1  99.22 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [55][170/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.0807e-02 (2.8488e-02)	Acc@1  97.66 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [55][180/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.1433e-02 (2.8603e-02)	Acc@1  98.44 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [55][190/391]	Time  0.169 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.3793e-02 (2.8479e-02)	Acc@1  98.44 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [55][200/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.9287e-02 (2.8413e-02)	Acc@1 100.00 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [55][210/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.6133e-02 (2.8945e-02)	Acc@1  98.44 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [55][220/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.0283e-02 (2.9013e-02)	Acc@1  98.44 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [55][230/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.2867e-02 (2.8777e-02)	Acc@1  99.22 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [55][240/391]	Time  0.171 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5434e-02 (2.8846e-02)	Acc@1 100.00 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [55][250/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.0490e-03 (2.8877e-02)	Acc@1 100.00 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [55][260/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.6840e-02 (2.8719e-02)	Acc@1  99.22 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [55][270/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.6641e-02 (2.8793e-02)	Acc@1  96.09 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [55][280/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.8513e-02 (2.9137e-02)	Acc@1  98.44 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [55][290/391]	Time  0.171 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.2781e-02 (2.8991e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [55][300/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.6031e-02 (2.9097e-02)	Acc@1  98.44 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [55][310/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.6570e-02 (2.9076e-02)	Acc@1  97.66 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [55][320/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.7903e-02 (2.9205e-02)	Acc@1  98.44 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [55][330/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.1362e-02 (2.9182e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [55][340/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0651e-02 (2.9178e-02)	Acc@1 100.00 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [55][350/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.7983e-02 (2.9392e-02)	Acc@1  96.88 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [55][360/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.9103e-02 (2.9137e-02)	Acc@1  98.44 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [55][370/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.6152e-02 (2.9319e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [55][380/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.2318e-02 (2.9142e-02)	Acc@1  98.44 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [55][390/391]	Time  0.117 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.4180e-02 (2.8923e-02)	Acc@1  98.75 ( 99.06)	Acc@5 100.00 (100.00)
## e[55] optimizer.zero_grad (sum) time: 0.5267908573150635
## e[55]       loss.backward (sum) time: 12.383941888809204
## e[55]      optimizer.step (sum) time: 25.828892707824707
## epoch[55] training(only) time: 63.50222849845886
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 1.9336e-01 (1.9336e-01)	Acc@1  97.00 ( 97.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.048 ( 0.061)	Loss 3.2666e-01 (2.9073e-01)	Acc@1  91.00 ( 92.27)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 4.8096e-01 (3.3074e-01)	Acc@1  90.00 ( 92.00)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 3.5596e-01 (3.2907e-01)	Acc@1  92.00 ( 92.26)	Acc@5  99.00 ( 99.65)
Test: [ 40/100]	Time  0.055 ( 0.051)	Loss 4.7705e-01 (3.3788e-01)	Acc@1  89.00 ( 91.98)	Acc@5  99.00 ( 99.66)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 1.3623e-01 (3.3846e-01)	Acc@1  94.00 ( 91.86)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 2.6050e-01 (3.3472e-01)	Acc@1  93.00 ( 91.89)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 3.1543e-01 (3.3422e-01)	Acc@1  92.00 ( 92.01)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 2.5635e-01 (3.3509e-01)	Acc@1  92.00 ( 92.00)	Acc@5 100.00 ( 99.75)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 3.1958e-01 (3.3904e-01)	Acc@1  91.00 ( 91.91)	Acc@5 100.00 ( 99.78)
 * Acc@1 91.820 Acc@5 99.780
### epoch[55] execution time: 68.47103476524353
EPOCH 56
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [56][  0/391]	Time  0.322 ( 0.322)	Data  0.147 ( 0.147)	Loss 1.4648e-02 (1.4648e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [56][ 10/391]	Time  0.160 ( 0.177)	Data  0.001 ( 0.014)	Loss 2.1805e-02 (2.7484e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [56][ 20/391]	Time  0.161 ( 0.170)	Data  0.001 ( 0.008)	Loss 2.3605e-02 (2.6675e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [56][ 30/391]	Time  0.160 ( 0.167)	Data  0.001 ( 0.006)	Loss 6.2866e-02 (2.8403e-02)	Acc@1  96.88 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [56][ 40/391]	Time  0.167 ( 0.166)	Data  0.001 ( 0.005)	Loss 3.5919e-02 (2.9109e-02)	Acc@1  98.44 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [56][ 50/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.6190e-02 (2.8930e-02)	Acc@1 100.00 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [56][ 60/391]	Time  0.162 ( 0.165)	Data  0.001 ( 0.004)	Loss 6.6719e-03 (2.7173e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [56][ 70/391]	Time  0.162 ( 0.165)	Data  0.001 ( 0.003)	Loss 1.1765e-02 (2.7011e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [56][ 80/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.3664e-02 (2.8025e-02)	Acc@1  99.22 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [56][ 90/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 4.8798e-02 (2.7433e-02)	Acc@1  97.66 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [56][100/391]	Time  0.164 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.9012e-02 (2.7448e-02)	Acc@1 100.00 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [56][110/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.5430e-02 (2.7432e-02)	Acc@1  98.44 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [56][120/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5030e-02 (2.7143e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [56][130/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6266e-02 (2.7951e-02)	Acc@1  99.22 ( 99.09)	Acc@5 100.00 ( 99.99)
Epoch: [56][140/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.1179e-02 (2.7822e-02)	Acc@1  99.22 ( 99.07)	Acc@5 100.00 ( 99.99)
Epoch: [56][150/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.7664e-02 (2.7629e-02)	Acc@1  98.44 ( 99.08)	Acc@5 100.00 ( 99.99)
Epoch: [56][160/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.8494e-02 (2.7633e-02)	Acc@1  99.22 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [56][170/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.8625e-02 (2.7369e-02)	Acc@1  99.22 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [56][180/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.6713e-02 (2.7457e-02)	Acc@1  98.44 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [56][190/391]	Time  0.166 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.9073e-02 (2.7716e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [56][200/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1337e-02 (2.7626e-02)	Acc@1 100.00 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [56][210/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.8458e-02 (2.7863e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [56][220/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.5635e-02 (2.8070e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [56][230/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.9337e-02 (2.8030e-02)	Acc@1  96.88 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [56][240/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3695e-02 (2.8161e-02)	Acc@1 100.00 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [56][250/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5762e-02 (2.8131e-02)	Acc@1 100.00 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [56][260/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.6611e-02 (2.8342e-02)	Acc@1  98.44 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [56][270/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.9806e-02 (2.8354e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [56][280/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4969e-02 (2.8182e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [56][290/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.9730e-02 (2.8222e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [56][300/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1627e-02 (2.8074e-02)	Acc@1 100.00 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [56][310/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.2474e-03 (2.8256e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [56][320/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.3844e-02 (2.8453e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [56][330/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.0447e-02 (2.8510e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [56][340/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.8556e-03 (2.8248e-02)	Acc@1 100.00 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [56][350/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0078e-02 (2.8347e-02)	Acc@1 100.00 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [56][360/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6785e-02 (2.8265e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [56][370/391]	Time  0.177 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.7817e-02 (2.8637e-02)	Acc@1  98.44 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [56][380/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5121e-02 (2.8419e-02)	Acc@1 100.00 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [56][390/391]	Time  0.119 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1490e-02 (2.8349e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 (100.00)
## e[56] optimizer.zero_grad (sum) time: 0.5368502140045166
## e[56]       loss.backward (sum) time: 12.416005373001099
## e[56]      optimizer.step (sum) time: 25.815258026123047
## epoch[56] training(only) time: 63.70167803764343
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 1.8286e-01 (1.8286e-01)	Acc@1  96.00 ( 96.00)	Acc@5  99.00 ( 99.00)
Test: [ 10/100]	Time  0.047 ( 0.060)	Loss 3.2544e-01 (2.8741e-01)	Acc@1  91.00 ( 91.82)	Acc@5 100.00 ( 99.73)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 5.0293e-01 (3.2169e-01)	Acc@1  92.00 ( 91.86)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 3.7842e-01 (3.2853e-01)	Acc@1  93.00 ( 92.16)	Acc@5  99.00 ( 99.58)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 5.2295e-01 (3.4030e-01)	Acc@1  89.00 ( 91.98)	Acc@5  99.00 ( 99.63)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.8530e-01 (3.4925e-01)	Acc@1  95.00 ( 91.92)	Acc@5 100.00 ( 99.65)
Test: [ 60/100]	Time  0.048 ( 0.050)	Loss 2.2803e-01 (3.4575e-01)	Acc@1  93.00 ( 91.82)	Acc@5 100.00 ( 99.70)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 3.1152e-01 (3.4511e-01)	Acc@1  92.00 ( 91.89)	Acc@5 100.00 ( 99.72)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 1.8750e-01 (3.4164e-01)	Acc@1  94.00 ( 91.98)	Acc@5 100.00 ( 99.73)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 2.7466e-01 (3.4296e-01)	Acc@1  94.00 ( 91.90)	Acc@5 100.00 ( 99.74)
 * Acc@1 91.810 Acc@5 99.740
### epoch[56] execution time: 68.68830800056458
EPOCH 57
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [57][  0/391]	Time  0.317 ( 0.317)	Data  0.145 ( 0.145)	Loss 1.2123e-02 (1.2123e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [57][ 10/391]	Time  0.160 ( 0.175)	Data  0.001 ( 0.014)	Loss 5.5573e-02 (2.8683e-02)	Acc@1  98.44 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [57][ 20/391]	Time  0.160 ( 0.169)	Data  0.001 ( 0.008)	Loss 3.3112e-02 (2.9381e-02)	Acc@1  99.22 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [57][ 30/391]	Time  0.162 ( 0.167)	Data  0.001 ( 0.006)	Loss 5.0720e-02 (3.2050e-02)	Acc@1  98.44 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [57][ 40/391]	Time  0.162 ( 0.166)	Data  0.001 ( 0.005)	Loss 3.8818e-02 (3.0615e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [57][ 50/391]	Time  0.162 ( 0.165)	Data  0.001 ( 0.004)	Loss 3.4729e-02 (3.0100e-02)	Acc@1  97.66 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [57][ 60/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.3306e-02 (2.9395e-02)	Acc@1 100.00 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [57][ 70/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.4536e-02 (2.8585e-02)	Acc@1  98.44 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [57][ 80/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 7.5378e-02 (2.9585e-02)	Acc@1  98.44 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [57][ 90/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.0971e-02 (2.9129e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [57][100/391]	Time  0.163 ( 0.164)	Data  0.001 ( 0.003)	Loss 9.3002e-03 (2.9602e-02)	Acc@1 100.00 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [57][110/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.8463e-02 (2.9008e-02)	Acc@1  99.22 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [57][120/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.2166e-02 (2.9477e-02)	Acc@1  98.44 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [57][130/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.4536e-02 (2.9566e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [57][140/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.3335e-02 (2.9732e-02)	Acc@1  98.44 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [57][150/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.0566e-03 (3.0530e-02)	Acc@1 100.00 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [57][160/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.9397e-02 (3.0855e-02)	Acc@1  97.66 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [57][170/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0323e-02 (3.0278e-02)	Acc@1 100.00 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [57][180/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.4626e-02 (3.0593e-02)	Acc@1  97.66 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [57][190/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.1708e-02 (3.0656e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [57][200/391]	Time  0.165 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.8091e-02 (3.1060e-02)	Acc@1  99.22 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [57][210/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5015e-02 (3.0983e-02)	Acc@1 100.00 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [57][220/391]	Time  0.167 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.5339e-02 (3.0866e-02)	Acc@1  98.44 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [57][230/391]	Time  0.166 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4465e-02 (3.0894e-02)	Acc@1  99.22 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [57][240/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.6452e-03 (3.0747e-02)	Acc@1 100.00 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [57][250/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.8483e-02 (3.0557e-02)	Acc@1  98.44 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [57][260/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.1525e-02 (3.0447e-02)	Acc@1  97.66 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [57][270/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.9124e-02 (3.0357e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [57][280/391]	Time  0.173 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.9388e-02 (3.0205e-02)	Acc@1  98.44 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [57][290/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.4048e-02 (2.9973e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [57][300/391]	Time  0.168 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.7451e-02 (2.9708e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [57][310/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.4490e-02 (2.9760e-02)	Acc@1  99.22 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [57][320/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.1982e-02 (2.9679e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [57][330/391]	Time  0.172 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6968e-02 (2.9531e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [57][340/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4664e-02 (2.9471e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [57][350/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.1103e-02 (2.9441e-02)	Acc@1 100.00 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [57][360/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.2980e-02 (2.9216e-02)	Acc@1 100.00 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [57][370/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6464e-02 (2.9161e-02)	Acc@1 100.00 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [57][380/391]	Time  0.172 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.3782e-02 (2.9197e-02)	Acc@1  96.88 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [57][390/391]	Time  0.119 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6052e-02 (2.8998e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 (100.00)
## e[57] optimizer.zero_grad (sum) time: 0.5357041358947754
## e[57]       loss.backward (sum) time: 12.48685598373413
## e[57]      optimizer.step (sum) time: 25.770796537399292
## epoch[57] training(only) time: 63.63434386253357
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 1.9019e-01 (1.9019e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 2.8418e-01 (3.1570e-01)	Acc@1  92.00 ( 91.64)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.047 ( 0.054)	Loss 4.7827e-01 (3.4340e-01)	Acc@1  90.00 ( 91.76)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 3.4937e-01 (3.4308e-01)	Acc@1  91.00 ( 91.90)	Acc@5  99.00 ( 99.71)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 5.2100e-01 (3.4781e-01)	Acc@1  91.00 ( 92.05)	Acc@5  99.00 ( 99.71)
Test: [ 50/100]	Time  0.056 ( 0.050)	Loss 1.6431e-01 (3.5058e-01)	Acc@1  96.00 ( 92.04)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.047 ( 0.049)	Loss 3.1763e-01 (3.4428e-01)	Acc@1  93.00 ( 92.05)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 3.3960e-01 (3.4319e-01)	Acc@1  93.00 ( 92.13)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 1.8652e-01 (3.4314e-01)	Acc@1  93.00 ( 92.10)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 3.7378e-01 (3.4563e-01)	Acc@1  92.00 ( 92.07)	Acc@5 100.00 ( 99.80)
 * Acc@1 92.020 Acc@5 99.800
### epoch[57] execution time: 68.59935402870178
EPOCH 58
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [58][  0/391]	Time  0.325 ( 0.325)	Data  0.147 ( 0.147)	Loss 1.1940e-02 (1.1940e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [58][ 10/391]	Time  0.181 ( 0.177)	Data  0.001 ( 0.014)	Loss 1.8784e-02 (2.6105e-02)	Acc@1 100.00 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [58][ 20/391]	Time  0.162 ( 0.170)	Data  0.001 ( 0.008)	Loss 1.4946e-02 (2.1162e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [58][ 30/391]	Time  0.161 ( 0.167)	Data  0.001 ( 0.006)	Loss 3.1677e-02 (2.1696e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [58][ 40/391]	Time  0.161 ( 0.166)	Data  0.001 ( 0.005)	Loss 1.2100e-02 (2.1523e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [58][ 50/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.004)	Loss 2.6657e-02 (2.2552e-02)	Acc@1  98.44 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [58][ 60/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.004)	Loss 5.2216e-02 (2.2548e-02)	Acc@1  96.09 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [58][ 70/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.0468e-02 (2.3344e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [58][ 80/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.4139e-02 (2.3919e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [58][ 90/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.0432e-02 (2.3109e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [58][100/391]	Time  0.163 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.4002e-02 (2.2917e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [58][110/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0078e-02 (2.3003e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [58][120/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6296e-02 (2.3139e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [58][130/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.4612e-02 (2.3257e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [58][140/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.8049e-03 (2.3046e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [58][150/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.9988e-02 (2.3415e-02)	Acc@1  97.66 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [58][160/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.4452e-03 (2.2849e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [58][170/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6769e-02 (2.2743e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [58][180/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4732e-02 (2.2713e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [58][190/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.2471e-02 (2.3261e-02)	Acc@1  98.44 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [58][200/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.8411e-02 (2.4248e-02)	Acc@1  97.66 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [58][210/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.8244e-02 (2.4152e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [58][220/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.9836e-02 (2.4085e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [58][230/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.8799e-02 (2.3626e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [58][240/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.0081e-03 (2.3946e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [58][250/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3138e-02 (2.3865e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [58][260/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5228e-02 (2.3662e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [58][270/391]	Time  0.173 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2207e-02 (2.3892e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [58][280/391]	Time  0.166 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5121e-02 (2.3881e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [58][290/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.4475e-02 (2.4120e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [58][300/391]	Time  0.167 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.6895e-02 (2.4245e-02)	Acc@1  98.44 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [58][310/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.9951e-03 (2.4285e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [58][320/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.8348e-03 (2.4245e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [58][330/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.4826e-02 (2.4284e-02)	Acc@1  99.22 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [58][340/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4153e-02 (2.4031e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [58][350/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.7964e-02 (2.4193e-02)	Acc@1  98.44 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [58][360/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.2827e-02 (2.3971e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [58][370/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.5439e-02 (2.4315e-02)	Acc@1  98.44 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [58][380/391]	Time  0.167 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.9831e-02 (2.4396e-02)	Acc@1  99.22 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [58][390/391]	Time  0.118 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7471e-02 (2.4493e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 (100.00)
## e[58] optimizer.zero_grad (sum) time: 0.541968822479248
## e[58]       loss.backward (sum) time: 12.467654705047607
## e[58]      optimizer.step (sum) time: 25.79312515258789
## epoch[58] training(only) time: 63.67225456237793
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 1.6956e-01 (1.6956e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 2.8711e-01 (3.1327e-01)	Acc@1  94.00 ( 91.91)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.047 ( 0.054)	Loss 4.5703e-01 (3.2656e-01)	Acc@1  92.00 ( 91.71)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 3.5571e-01 (3.2804e-01)	Acc@1  91.00 ( 91.94)	Acc@5  99.00 ( 99.71)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 5.3760e-01 (3.3693e-01)	Acc@1  90.00 ( 92.10)	Acc@5  99.00 ( 99.71)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 2.2168e-01 (3.4351e-01)	Acc@1  96.00 ( 92.14)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 2.6318e-01 (3.3803e-01)	Acc@1  93.00 ( 92.18)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 3.5693e-01 (3.3882e-01)	Acc@1  92.00 ( 92.20)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 2.6904e-01 (3.3608e-01)	Acc@1  92.00 ( 92.16)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 2.8662e-01 (3.3781e-01)	Acc@1  92.00 ( 92.10)	Acc@5 100.00 ( 99.80)
 * Acc@1 92.070 Acc@5 99.810
### epoch[58] execution time: 68.62928867340088
EPOCH 59
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [59][  0/391]	Time  0.317 ( 0.317)	Data  0.142 ( 0.142)	Loss 9.3689e-03 (9.3689e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [59][ 10/391]	Time  0.161 ( 0.176)	Data  0.001 ( 0.014)	Loss 2.5253e-02 (1.7830e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [59][ 20/391]	Time  0.161 ( 0.169)	Data  0.001 ( 0.008)	Loss 2.4673e-02 (1.8588e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [59][ 30/391]	Time  0.161 ( 0.167)	Data  0.001 ( 0.006)	Loss 2.4643e-02 (1.7726e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [59][ 40/391]	Time  0.161 ( 0.166)	Data  0.001 ( 0.005)	Loss 2.7527e-02 (2.1579e-02)	Acc@1  98.44 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [59][ 50/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.0437e-02 (2.1121e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [59][ 60/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.003)	Loss 1.4465e-02 (2.0992e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [59][ 70/391]	Time  0.177 ( 0.165)	Data  0.001 ( 0.003)	Loss 1.3641e-02 (2.1176e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [59][ 80/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 4.1382e-02 (2.1677e-02)	Acc@1  98.44 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [59][ 90/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.8549e-02 (2.1674e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [59][100/391]	Time  0.165 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.6688e-02 (2.1120e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [59][110/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.002)	Loss 8.3191e-02 (2.2179e-02)	Acc@1  97.66 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [59][120/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.002)	Loss 2.0050e-02 (2.2311e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [59][130/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.002)	Loss 1.8402e-02 (2.2750e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [59][140/391]	Time  0.164 ( 0.164)	Data  0.001 ( 0.002)	Loss 1.6968e-02 (2.3434e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [59][150/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.0842e-02 (2.3644e-02)	Acc@1  98.44 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [59][160/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.9302e-02 (2.4002e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [59][170/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.5950e-02 (2.4361e-02)	Acc@1  98.44 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [59][180/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.8120e-03 (2.4477e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [59][190/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.2349e-02 (2.4994e-02)	Acc@1  98.44 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [59][200/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.0598e-02 (2.5193e-02)	Acc@1  98.44 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [59][210/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.7037e-02 (2.5179e-02)	Acc@1  98.44 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [59][220/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0216e-02 (2.5204e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [59][230/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.4108e-02 (2.5552e-02)	Acc@1  97.66 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [59][240/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3458e-02 (2.5385e-02)	Acc@1 100.00 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [59][250/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.2185e-03 (2.5434e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [59][260/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.3712e-02 (2.5535e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [59][270/391]	Time  0.173 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.9999e-02 (2.5873e-02)	Acc@1  98.44 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [59][280/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.1133e-02 (2.5843e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [59][290/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.4888e-03 (2.6041e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [59][300/391]	Time  0.171 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.1530e-02 (2.6294e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [59][310/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.9193e-03 (2.6248e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [59][320/391]	Time  0.172 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.3662e-03 (2.6411e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [59][330/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4900e-02 (2.6234e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [59][340/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.5925e-02 (2.6031e-02)	Acc@1  98.44 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [59][350/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4542e-02 (2.6181e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [59][360/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.2232e-02 (2.6332e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [59][370/391]	Time  0.171 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0452e-02 (2.6462e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [59][380/391]	Time  0.165 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.1286e-02 (2.6515e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [59][390/391]	Time  0.116 ( 0.163)	Data  0.001 ( 0.001)	Loss 1.0126e-01 (2.6819e-02)	Acc@1  97.50 ( 99.16)	Acc@5 100.00 (100.00)
## e[59] optimizer.zero_grad (sum) time: 0.5343911647796631
## e[59]       loss.backward (sum) time: 12.45468521118164
## e[59]      optimizer.step (sum) time: 25.794397115707397
## epoch[59] training(only) time: 63.674299240112305
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 1.6907e-01 (1.6907e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.059)	Loss 3.4644e-01 (3.2655e-01)	Acc@1  91.00 ( 92.09)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.047 ( 0.054)	Loss 5.2197e-01 (3.5221e-01)	Acc@1  90.00 ( 91.81)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 3.4912e-01 (3.4626e-01)	Acc@1  93.00 ( 92.06)	Acc@5  99.00 ( 99.74)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 5.0977e-01 (3.5462e-01)	Acc@1  88.00 ( 92.02)	Acc@5  98.00 ( 99.73)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 2.2620e-01 (3.6073e-01)	Acc@1  96.00 ( 92.00)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.047 ( 0.049)	Loss 2.6953e-01 (3.5139e-01)	Acc@1  92.00 ( 92.05)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 3.7695e-01 (3.5487e-01)	Acc@1  94.00 ( 92.07)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 2.9517e-01 (3.5043e-01)	Acc@1  93.00 ( 92.20)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 2.7832e-01 (3.5233e-01)	Acc@1  93.00 ( 92.15)	Acc@5 100.00 ( 99.81)
 * Acc@1 92.100 Acc@5 99.820
### epoch[59] execution time: 68.62782382965088
EPOCH 60
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [60][  0/391]	Time  0.337 ( 0.337)	Data  0.150 ( 0.150)	Loss 5.3787e-03 (5.3787e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [60][ 10/391]	Time  0.162 ( 0.177)	Data  0.001 ( 0.015)	Loss 2.0126e-02 (2.2010e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [60][ 20/391]	Time  0.160 ( 0.170)	Data  0.001 ( 0.008)	Loss 1.3443e-02 (2.1242e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [60][ 30/391]	Time  0.164 ( 0.168)	Data  0.001 ( 0.006)	Loss 1.0040e-02 (2.1826e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [60][ 40/391]	Time  0.161 ( 0.167)	Data  0.001 ( 0.005)	Loss 7.2327e-03 (2.1211e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [60][ 50/391]	Time  0.172 ( 0.166)	Data  0.001 ( 0.004)	Loss 1.4832e-02 (2.1707e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [60][ 60/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.3947e-02 (2.1042e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [60][ 70/391]	Time  0.162 ( 0.165)	Data  0.001 ( 0.003)	Loss 4.1168e-02 (2.1248e-02)	Acc@1  98.44 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [60][ 80/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 3.2471e-02 (2.1131e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [60][ 90/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 6.0921e-03 (2.0805e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [60][100/391]	Time  0.169 ( 0.164)	Data  0.001 ( 0.003)	Loss 3.7933e-02 (2.1197e-02)	Acc@1  98.44 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [60][110/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.9861e-02 (2.1095e-02)	Acc@1  98.44 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [60][120/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.002)	Loss 2.0294e-02 (2.0950e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [60][130/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.002)	Loss 2.0096e-02 (2.0788e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [60][140/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0094e-02 (2.1298e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [60][150/391]	Time  0.171 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.4729e-02 (2.1252e-02)	Acc@1  98.44 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [60][160/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2733e-02 (2.0838e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [60][170/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4702e-02 (2.0952e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [60][180/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.9424e-02 (2.0794e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [60][190/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.7786e-02 (2.0631e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [60][200/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.6011e-02 (2.1388e-02)	Acc@1  98.44 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [60][210/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.8218e-02 (2.1589e-02)	Acc@1  98.44 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [60][220/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.2644e-02 (2.1526e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [60][230/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0185e-02 (2.1295e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [60][240/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.9246e-02 (2.1495e-02)	Acc@1  97.66 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [60][250/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.1586e-02 (2.1476e-02)	Acc@1  98.44 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [60][260/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.2877e-02 (2.1677e-02)	Acc@1  97.66 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [60][270/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0963e-02 (2.1595e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [60][280/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.9520e-02 (2.1698e-02)	Acc@1  98.44 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [60][290/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6968e-02 (2.1483e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [60][300/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0773e-02 (2.1471e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [60][310/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.5586e-03 (2.1598e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [60][320/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4801e-02 (2.1554e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [60][330/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.0501e-03 (2.1383e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [60][340/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.6448e-02 (2.1484e-02)	Acc@1  96.88 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [60][350/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3268e-02 (2.1544e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [60][360/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.3640e-02 (2.1520e-02)	Acc@1  98.44 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [60][370/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1200e-02 (2.1481e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [60][380/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.8600e-02 (2.1631e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [60][390/391]	Time  0.117 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8921e-02 (2.1630e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
## e[60] optimizer.zero_grad (sum) time: 0.5362789630889893
## e[60]       loss.backward (sum) time: 12.472631692886353
## e[60]      optimizer.step (sum) time: 25.740283966064453
## epoch[60] training(only) time: 63.62845778465271
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 2.0740e-01 (2.0740e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.060 ( 0.063)	Loss 3.4009e-01 (3.2549e-01)	Acc@1  91.00 ( 92.18)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.048 ( 0.056)	Loss 5.0781e-01 (3.4648e-01)	Acc@1  90.00 ( 91.90)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.047 ( 0.053)	Loss 3.5864e-01 (3.4119e-01)	Acc@1  92.00 ( 92.10)	Acc@5  99.00 ( 99.77)
Test: [ 40/100]	Time  0.047 ( 0.052)	Loss 5.0098e-01 (3.4736e-01)	Acc@1  89.00 ( 92.17)	Acc@5  98.00 ( 99.76)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 2.3376e-01 (3.5108e-01)	Acc@1  94.00 ( 92.18)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 2.4341e-01 (3.4294e-01)	Acc@1  91.00 ( 92.10)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 3.8184e-01 (3.4552e-01)	Acc@1  94.00 ( 92.07)	Acc@5 100.00 ( 99.80)
Test: [ 80/100]	Time  0.048 ( 0.050)	Loss 2.8418e-01 (3.4288e-01)	Acc@1  93.00 ( 92.19)	Acc@5 100.00 ( 99.81)
Test: [ 90/100]	Time  0.047 ( 0.050)	Loss 3.0200e-01 (3.4331e-01)	Acc@1  93.00 ( 92.19)	Acc@5 100.00 ( 99.82)
 * Acc@1 92.100 Acc@5 99.830
### epoch[60] execution time: 68.6695511341095
EPOCH 61
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [61][  0/391]	Time  0.328 ( 0.328)	Data  0.141 ( 0.141)	Loss 5.2216e-02 (5.2216e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [61][ 10/391]	Time  0.161 ( 0.176)	Data  0.001 ( 0.014)	Loss 2.1988e-02 (3.0068e-02)	Acc@1  99.22 ( 98.79)	Acc@5 100.00 (100.00)
Epoch: [61][ 20/391]	Time  0.161 ( 0.170)	Data  0.001 ( 0.008)	Loss 2.4872e-02 (2.4799e-02)	Acc@1  98.44 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [61][ 30/391]	Time  0.161 ( 0.167)	Data  0.001 ( 0.006)	Loss 6.0364e-02 (2.6774e-02)	Acc@1  96.88 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [61][ 40/391]	Time  0.161 ( 0.166)	Data  0.001 ( 0.005)	Loss 1.8280e-02 (2.6927e-02)	Acc@1  99.22 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [61][ 50/391]	Time  0.162 ( 0.165)	Data  0.001 ( 0.004)	Loss 5.2704e-02 (2.7316e-02)	Acc@1  98.44 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [61][ 60/391]	Time  0.162 ( 0.165)	Data  0.001 ( 0.003)	Loss 1.5991e-02 (2.6407e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [61][ 70/391]	Time  0.162 ( 0.165)	Data  0.001 ( 0.003)	Loss 1.5762e-02 (2.5165e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [61][ 80/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 7.3013e-03 (2.4892e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [61][ 90/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.7441e-02 (2.4184e-02)	Acc@1  99.22 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [61][100/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.003)	Loss 9.2163e-03 (2.3366e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [61][110/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.002)	Loss 9.8419e-03 (2.2712e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [61][120/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.002)	Loss 1.3649e-02 (2.3134e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [61][130/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.002)	Loss 1.7868e-02 (2.2765e-02)	Acc@1  98.44 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [61][140/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.2779e-03 (2.2074e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [61][150/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1620e-02 (2.1648e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [61][160/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.0643e-03 (2.1510e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [61][170/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.6596e-02 (2.1159e-02)	Acc@1  98.44 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [61][180/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.5864e-02 (2.1083e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [61][190/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7548e-02 (2.1137e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [61][200/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.2491e-02 (2.0875e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [61][210/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.8967e-02 (2.1308e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [61][220/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1131e-02 (2.1442e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [61][230/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.8946e-02 (2.1247e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [61][240/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4572e-02 (2.1050e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [61][250/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.1171e-03 (2.1054e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [61][260/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.4413e-02 (2.1001e-02)	Acc@1  97.66 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [61][270/391]	Time  0.165 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.4550e-03 (2.1173e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [61][280/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.2369e-02 (2.1444e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [61][290/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.3468e-02 (2.1202e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [61][300/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4931e-02 (2.1155e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [61][310/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.3629e-03 (2.0969e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [61][320/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4030e-02 (2.0880e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [61][330/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.2746e-03 (2.0737e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [61][340/391]	Time  0.177 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.5696e-02 (2.0558e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [61][350/391]	Time  0.176 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6663e-02 (2.0425e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [61][360/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.6550e-02 (2.0520e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [61][370/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.3520e-03 (2.0398e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [61][380/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.4299e-03 (2.0556e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [61][390/391]	Time  0.118 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.4790e-02 (2.0630e-02)	Acc@1  98.75 ( 99.42)	Acc@5 100.00 (100.00)
## e[61] optimizer.zero_grad (sum) time: 0.5367624759674072
## e[61]       loss.backward (sum) time: 12.470093727111816
## e[61]      optimizer.step (sum) time: 25.757407903671265
## epoch[61] training(only) time: 63.66230511665344
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 1.7798e-01 (1.7798e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.063)	Loss 3.3130e-01 (3.2530e-01)	Acc@1  92.00 ( 92.09)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 5.2637e-01 (3.4683e-01)	Acc@1  90.00 ( 92.00)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.047 ( 0.053)	Loss 3.3398e-01 (3.4062e-01)	Acc@1  92.00 ( 92.26)	Acc@5  99.00 ( 99.77)
Test: [ 40/100]	Time  0.048 ( 0.051)	Loss 5.1416e-01 (3.4807e-01)	Acc@1  88.00 ( 92.22)	Acc@5  98.00 ( 99.76)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 2.2437e-01 (3.5189e-01)	Acc@1  95.00 ( 92.22)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 2.4231e-01 (3.4320e-01)	Acc@1  92.00 ( 92.18)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 3.7573e-01 (3.4579e-01)	Acc@1  94.00 ( 92.18)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 2.7515e-01 (3.4263e-01)	Acc@1  95.00 ( 92.33)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 2.8247e-01 (3.4315e-01)	Acc@1  93.00 ( 92.29)	Acc@5 100.00 ( 99.82)
 * Acc@1 92.220 Acc@5 99.830
### epoch[61] execution time: 68.66491889953613
EPOCH 62
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [62][  0/391]	Time  0.327 ( 0.327)	Data  0.155 ( 0.155)	Loss 1.6144e-02 (1.6144e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [62][ 10/391]	Time  0.160 ( 0.177)	Data  0.001 ( 0.015)	Loss 9.7275e-03 (2.3350e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [62][ 20/391]	Time  0.161 ( 0.170)	Data  0.001 ( 0.009)	Loss 1.1925e-02 (1.9791e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [62][ 30/391]	Time  0.161 ( 0.167)	Data  0.001 ( 0.006)	Loss 6.2752e-03 (1.8398e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [62][ 40/391]	Time  0.173 ( 0.166)	Data  0.001 ( 0.005)	Loss 9.2926e-03 (1.9679e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [62][ 50/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 2.5406e-02 (1.8788e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [62][ 60/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.8494e-02 (1.9291e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [62][ 70/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.9196e-02 (1.9918e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [62][ 80/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 3.1311e-02 (2.1048e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [62][ 90/391]	Time  0.171 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.0660e-02 (2.1455e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [62][100/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.7761e-02 (2.1047e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [62][110/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.8091e-02 (2.0494e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [62][120/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4900e-02 (2.0866e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [62][130/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.0627e-03 (2.0671e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [62][140/391]	Time  0.172 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1963e-02 (2.0249e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [62][150/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.2202e-02 (2.0626e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [62][160/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.5918e-03 (2.0193e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [62][170/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2695e-02 (2.0564e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [62][180/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.1924e-03 (2.0140e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [62][190/391]	Time  0.172 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.8959e-03 (2.0252e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [62][200/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.9221e-02 (2.0073e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [62][210/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3924e-02 (2.0631e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [62][220/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.7286e-03 (2.0199e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [62][230/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.7793e-03 (2.0097e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [62][240/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.8959e-03 (2.0054e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [62][250/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.1711e-03 (2.0375e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [62][260/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.9907e-02 (2.0285e-02)	Acc@1  98.44 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [62][270/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.1487e-03 (2.0574e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [62][280/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.0325e-02 (2.0567e-02)	Acc@1  98.44 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [62][290/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.2703e-03 (2.0592e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [62][300/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.2430e-02 (2.0781e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [62][310/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.4454e-02 (2.0880e-02)	Acc@1  98.44 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [62][320/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5137e-02 (2.0901e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [62][330/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.9111e-03 (2.1007e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [62][340/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.7449e-03 (2.1080e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [62][350/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7090e-02 (2.1095e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [62][360/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3992e-02 (2.1000e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [62][370/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.1079e-03 (2.0865e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [62][380/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.7798e-03 (2.0756e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [62][390/391]	Time  0.117 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6693e-02 (2.0730e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
## e[62] optimizer.zero_grad (sum) time: 0.5352499485015869
## e[62]       loss.backward (sum) time: 12.469711780548096
## e[62]      optimizer.step (sum) time: 25.77037763595581
## epoch[62] training(only) time: 63.59343647956848
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 1.8530e-01 (1.8530e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 3.5938e-01 (3.2487e-01)	Acc@1  90.00 ( 92.00)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 5.0830e-01 (3.4789e-01)	Acc@1  90.00 ( 91.90)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 3.3643e-01 (3.4282e-01)	Acc@1  93.00 ( 92.16)	Acc@5  99.00 ( 99.77)
Test: [ 40/100]	Time  0.056 ( 0.051)	Loss 5.1221e-01 (3.4838e-01)	Acc@1  89.00 ( 92.20)	Acc@5  98.00 ( 99.73)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 2.0435e-01 (3.5228e-01)	Acc@1  96.00 ( 92.22)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 2.3962e-01 (3.4366e-01)	Acc@1  92.00 ( 92.18)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 4.1675e-01 (3.4667e-01)	Acc@1  93.00 ( 92.11)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 2.9053e-01 (3.4439e-01)	Acc@1  93.00 ( 92.19)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 2.7856e-01 (3.4559e-01)	Acc@1  93.00 ( 92.16)	Acc@5 100.00 ( 99.82)
 * Acc@1 92.120 Acc@5 99.830
### epoch[62] execution time: 68.55355310440063
EPOCH 63
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [63][  0/391]	Time  0.335 ( 0.335)	Data  0.163 ( 0.163)	Loss 4.1992e-02 (4.1992e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [63][ 10/391]	Time  0.160 ( 0.177)	Data  0.001 ( 0.016)	Loss 7.5226e-03 (2.2206e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [63][ 20/391]	Time  0.160 ( 0.170)	Data  0.001 ( 0.009)	Loss 5.1613e-03 (2.0917e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [63][ 30/391]	Time  0.161 ( 0.167)	Data  0.001 ( 0.006)	Loss 2.8076e-02 (2.1642e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [63][ 40/391]	Time  0.161 ( 0.166)	Data  0.001 ( 0.005)	Loss 1.2085e-02 (2.1156e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [63][ 50/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.004)	Loss 8.0750e-02 (2.1467e-02)	Acc@1  97.66 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [63][ 60/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.004)	Loss 5.5542e-03 (2.2383e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [63][ 70/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.3544e-02 (2.2514e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [63][ 80/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.5604e-02 (2.2489e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [63][ 90/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.0111e-02 (2.1980e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [63][100/391]	Time  0.164 ( 0.164)	Data  0.001 ( 0.003)	Loss 6.1615e-02 (2.2429e-02)	Acc@1  97.66 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [63][110/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 8.5983e-03 (2.1616e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [63][120/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 8.6899e-03 (2.1243e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [63][130/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.8398e-03 (2.1453e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [63][140/391]	Time  0.165 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.3361e-02 (2.1449e-02)	Acc@1  98.44 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [63][150/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.9694e-02 (2.1481e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [63][160/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0483e-02 (2.1435e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [63][170/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.3978e-03 (2.1172e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [63][180/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0033e-02 (2.1163e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [63][190/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7014e-02 (2.0777e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [63][200/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.8305e-03 (2.0635e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [63][210/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.5449e-03 (2.0583e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [63][220/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.8427e-02 (2.0673e-02)	Acc@1  98.44 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [63][230/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.0432e-02 (2.0672e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [63][240/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2154e-02 (2.0350e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [63][250/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4656e-02 (2.0080e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [63][260/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5976e-02 (2.0083e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [63][270/391]	Time  0.171 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.8909e-02 (2.0490e-02)	Acc@1  97.66 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [63][280/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.8626e-03 (2.0518e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [63][290/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0765e-02 (2.0842e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [63][300/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.5193e-03 (2.0783e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [63][310/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.2468e-03 (2.0866e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [63][320/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.9242e-03 (2.0796e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [63][330/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.8204e-02 (2.0748e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [63][340/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.1179e-02 (2.0736e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [63][350/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.0569e-02 (2.0844e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [63][360/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.3765e-03 (2.0789e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [63][370/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.5162e-02 (2.0719e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [63][380/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.9424e-02 (2.0603e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [63][390/391]	Time  0.118 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6708e-02 (2.0677e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
## e[63] optimizer.zero_grad (sum) time: 0.5444173812866211
## e[63]       loss.backward (sum) time: 12.496052026748657
## e[63]      optimizer.step (sum) time: 25.76852035522461
## epoch[63] training(only) time: 63.72027778625488
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 1.8921e-01 (1.8921e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.048 ( 0.061)	Loss 3.5205e-01 (3.2561e-01)	Acc@1  90.00 ( 92.00)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 5.1514e-01 (3.4797e-01)	Acc@1  90.00 ( 91.81)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 3.3667e-01 (3.4124e-01)	Acc@1  93.00 ( 92.10)	Acc@5  99.00 ( 99.77)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 4.9902e-01 (3.4710e-01)	Acc@1  89.00 ( 92.07)	Acc@5  98.00 ( 99.76)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 2.0679e-01 (3.4912e-01)	Acc@1  95.00 ( 92.00)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 2.4878e-01 (3.4086e-01)	Acc@1  91.00 ( 92.02)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 3.6841e-01 (3.4212e-01)	Acc@1  93.00 ( 91.99)	Acc@5 100.00 ( 99.80)
Test: [ 80/100]	Time  0.048 ( 0.049)	Loss 3.1006e-01 (3.3954e-01)	Acc@1  92.00 ( 92.06)	Acc@5 100.00 ( 99.81)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 3.1665e-01 (3.4030e-01)	Acc@1  92.00 ( 92.01)	Acc@5 100.00 ( 99.84)
 * Acc@1 91.970 Acc@5 99.840
### epoch[63] execution time: 68.72042107582092
EPOCH 64
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [64][  0/391]	Time  0.312 ( 0.312)	Data  0.141 ( 0.141)	Loss 5.8250e-03 (5.8250e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [64][ 10/391]	Time  0.160 ( 0.175)	Data  0.001 ( 0.014)	Loss 3.0136e-02 (1.5469e-02)	Acc@1  98.44 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [64][ 20/391]	Time  0.161 ( 0.169)	Data  0.001 ( 0.008)	Loss 1.4915e-02 (1.9908e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [64][ 30/391]	Time  0.161 ( 0.166)	Data  0.001 ( 0.006)	Loss 2.5162e-02 (1.8685e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [64][ 40/391]	Time  0.162 ( 0.165)	Data  0.001 ( 0.005)	Loss 5.6229e-03 (1.7728e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [64][ 50/391]	Time  0.166 ( 0.165)	Data  0.001 ( 0.004)	Loss 3.1647e-02 (1.7499e-02)	Acc@1  98.44 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [64][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 3.9902e-03 (1.7300e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [64][ 70/391]	Time  0.163 ( 0.164)	Data  0.001 ( 0.003)	Loss 8.6060e-03 (1.7969e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [64][ 80/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.2903e-02 (1.8151e-02)	Acc@1  98.44 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [64][ 90/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.2939e-02 (1.8741e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [64][100/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.4114e-02 (1.8452e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [64][110/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.3666e-02 (1.8445e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [64][120/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6388e-02 (1.8040e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [64][130/391]	Time  0.170 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2764e-02 (1.8102e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [64][140/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.5815e-03 (1.7997e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [64][150/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6113e-02 (1.8082e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [64][160/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.8015e-02 (1.8386e-02)	Acc@1  98.44 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [64][170/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.2064e-02 (1.8686e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [64][180/391]	Time  0.180 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.9106e-03 (1.8762e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [64][190/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.8234e-02 (1.8616e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [64][200/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.7863e-02 (1.8481e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [64][210/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.0100e-02 (1.8838e-02)	Acc@1  97.66 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [64][220/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.0121e-02 (1.8872e-02)	Acc@1  98.44 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [64][230/391]	Time  0.172 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5350e-02 (1.9127e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [64][240/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.0020e-02 (1.9034e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [64][250/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4946e-02 (1.9049e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [64][260/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5556e-02 (1.9025e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [64][270/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.3875e-02 (1.9080e-02)	Acc@1  98.44 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [64][280/391]	Time  0.179 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.8330e-02 (1.9276e-02)	Acc@1  98.44 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [64][290/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0971e-02 (1.9371e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [64][300/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.6047e-02 (1.9418e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [64][310/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.9669e-02 (1.9357e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [64][320/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5732e-02 (1.9510e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [64][330/391]	Time  0.172 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1780e-02 (1.9668e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [64][340/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.0016e-03 (1.9521e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [64][350/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.5583e-02 (1.9556e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [64][360/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.6764e-02 (1.9431e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [64][370/391]	Time  0.177 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7838e-02 (1.9399e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [64][380/391]	Time  0.169 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.5896e-03 (1.9357e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [64][390/391]	Time  0.118 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.3902e-03 (1.9309e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
## e[64] optimizer.zero_grad (sum) time: 0.5347576141357422
## e[64]       loss.backward (sum) time: 12.44960069656372
## e[64]      optimizer.step (sum) time: 25.771220684051514
## epoch[64] training(only) time: 63.67645001411438
# Switched to evaluate mode...
Test: [  0/100]	Time  0.190 ( 0.190)	Loss 1.8237e-01 (1.8237e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.062)	Loss 3.3740e-01 (3.2180e-01)	Acc@1  91.00 ( 92.27)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 5.1611e-01 (3.4107e-01)	Acc@1  90.00 ( 92.19)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.048 ( 0.053)	Loss 3.3008e-01 (3.3378e-01)	Acc@1  94.00 ( 92.42)	Acc@5  99.00 ( 99.77)
Test: [ 40/100]	Time  0.048 ( 0.052)	Loss 4.9634e-01 (3.4028e-01)	Acc@1  90.00 ( 92.41)	Acc@5  98.00 ( 99.73)
Test: [ 50/100]	Time  0.048 ( 0.052)	Loss 2.2144e-01 (3.4449e-01)	Acc@1  95.00 ( 92.37)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.048 ( 0.051)	Loss 2.4658e-01 (3.3645e-01)	Acc@1  93.00 ( 92.34)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.057 ( 0.051)	Loss 3.6646e-01 (3.3897e-01)	Acc@1  94.00 ( 92.30)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.048 ( 0.050)	Loss 2.8735e-01 (3.3648e-01)	Acc@1  94.00 ( 92.38)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.047 ( 0.050)	Loss 2.7808e-01 (3.3747e-01)	Acc@1  93.00 ( 92.33)	Acc@5 100.00 ( 99.82)
 * Acc@1 92.240 Acc@5 99.830
### epoch[64] execution time: 68.7451400756836
EPOCH 65
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [65][  0/391]	Time  0.321 ( 0.321)	Data  0.152 ( 0.152)	Loss 6.8359e-03 (6.8359e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [65][ 10/391]	Time  0.160 ( 0.177)	Data  0.001 ( 0.015)	Loss 3.1830e-02 (1.5673e-02)	Acc@1  98.44 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [65][ 20/391]	Time  0.161 ( 0.169)	Data  0.001 ( 0.008)	Loss 2.5543e-02 (1.6513e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [65][ 30/391]	Time  0.161 ( 0.167)	Data  0.001 ( 0.006)	Loss 1.5526e-02 (2.0454e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [65][ 40/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.005)	Loss 5.7487e-03 (1.8846e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [65][ 50/391]	Time  0.162 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.1314e-02 (1.9507e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [65][ 60/391]	Time  0.162 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.1299e-02 (1.8888e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [65][ 70/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.4384e-02 (1.9170e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [65][ 80/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.1429e-02 (1.9172e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [65][ 90/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.5205e-02 (1.9694e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [65][100/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 3.0151e-02 (1.9678e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [65][110/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.2461e-02 (1.9655e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [65][120/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.0752e-02 (1.9783e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [65][130/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.2621e-03 (1.9669e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [65][140/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.6163e-02 (1.9176e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [65][150/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.2297e-02 (1.9196e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [65][160/391]	Time  0.165 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2680e-02 (1.9325e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [65][170/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4549e-02 (1.9340e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [65][180/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.4810e-02 (1.9322e-02)	Acc@1  96.88 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [65][190/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2047e-02 (1.9045e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [65][200/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.4686e-03 (1.9028e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [65][210/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.1073e-03 (1.8839e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [65][220/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0803e-02 (1.9158e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [65][230/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.4943e-02 (1.9197e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [65][240/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5808e-02 (1.9204e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [65][250/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4053e-02 (1.9190e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [65][260/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6144e-02 (1.8969e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [65][270/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.7241e-02 (1.9141e-02)	Acc@1  98.44 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [65][280/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7349e-02 (1.9015e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [65][290/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.9678e-02 (1.8986e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [65][300/391]	Time  0.169 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.0823e-02 (1.9044e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [65][310/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.1678e-03 (1.8805e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [65][320/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.3923e-03 (1.8828e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [65][330/391]	Time  0.166 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.3274e-02 (1.9160e-02)	Acc@1  98.44 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [65][340/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.3802e-02 (1.9238e-02)	Acc@1  98.44 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [65][350/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.3002e-03 (1.8989e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [65][360/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5625e-02 (1.9093e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [65][370/391]	Time  0.166 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.7069e-02 (1.9098e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [65][380/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.0956e-03 (1.8987e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [65][390/391]	Time  0.118 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0490e-02 (1.9122e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
## e[65] optimizer.zero_grad (sum) time: 0.539818525314331
## e[65]       loss.backward (sum) time: 12.496074914932251
## e[65]      optimizer.step (sum) time: 25.715522050857544
## epoch[65] training(only) time: 63.71396255493164
# Switched to evaluate mode...
Test: [  0/100]	Time  0.202 ( 0.202)	Loss 1.5771e-01 (1.5771e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 3.2861e-01 (3.1929e-01)	Acc@1  93.00 ( 91.82)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 5.0195e-01 (3.4301e-01)	Acc@1  90.00 ( 91.67)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 3.3374e-01 (3.3720e-01)	Acc@1  93.00 ( 92.06)	Acc@5  99.00 ( 99.77)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 5.0049e-01 (3.4129e-01)	Acc@1  90.00 ( 92.17)	Acc@5  98.00 ( 99.76)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 2.1716e-01 (3.4588e-01)	Acc@1  95.00 ( 92.20)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 2.4548e-01 (3.3830e-01)	Acc@1  95.00 ( 92.23)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 3.8916e-01 (3.4182e-01)	Acc@1  93.00 ( 92.21)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 2.6367e-01 (3.3956e-01)	Acc@1  95.00 ( 92.35)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 2.8882e-01 (3.4036e-01)	Acc@1  93.00 ( 92.32)	Acc@5 100.00 ( 99.81)
 * Acc@1 92.270 Acc@5 99.820
### epoch[65] execution time: 68.68664288520813
EPOCH 66
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [66][  0/391]	Time  0.325 ( 0.325)	Data  0.143 ( 0.143)	Loss 1.9592e-02 (1.9592e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [66][ 10/391]	Time  0.162 ( 0.178)	Data  0.001 ( 0.014)	Loss 2.5421e-02 (2.3843e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [66][ 20/391]	Time  0.165 ( 0.170)	Data  0.001 ( 0.008)	Loss 1.2741e-02 (2.0787e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [66][ 30/391]	Time  0.161 ( 0.167)	Data  0.001 ( 0.006)	Loss 2.9587e-02 (2.1509e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [66][ 40/391]	Time  0.161 ( 0.166)	Data  0.001 ( 0.005)	Loss 1.3443e-02 (2.0753e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [66][ 50/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.6037e-02 (2.1082e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [66][ 60/391]	Time  0.162 ( 0.165)	Data  0.001 ( 0.003)	Loss 3.1158e-02 (2.1540e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [66][ 70/391]	Time  0.170 ( 0.164)	Data  0.001 ( 0.003)	Loss 5.0087e-03 (2.0707e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [66][ 80/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.4282e-02 (2.0038e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [66][ 90/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.0157e-02 (1.9901e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [66][100/391]	Time  0.163 ( 0.164)	Data  0.001 ( 0.003)	Loss 7.7896e-03 (1.9705e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [66][110/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.002)	Loss 4.1656e-03 (1.9672e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [66][120/391]	Time  0.173 ( 0.164)	Data  0.001 ( 0.002)	Loss 7.9193e-03 (1.9383e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [66][130/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.3629e-03 (1.9298e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [66][140/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5480e-02 (1.9709e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [66][150/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.7403e-03 (1.9695e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [66][160/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.4445e-02 (1.9516e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [66][170/391]	Time  0.171 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.9683e-02 (1.9891e-02)	Acc@1  98.44 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [66][180/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0628e-02 (1.9831e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [66][190/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.1311e-02 (1.9835e-02)	Acc@1  98.44 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [66][200/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.7738e-03 (2.0083e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [66][210/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.2806e-02 (2.0041e-02)	Acc@1  97.66 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [66][220/391]	Time  0.171 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3748e-02 (2.0114e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [66][230/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2512e-02 (2.0123e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [66][240/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.3694e-03 (1.9812e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [66][250/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1200e-02 (1.9858e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [66][260/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5747e-02 (1.9781e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [66][270/391]	Time  0.171 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.1035e-02 (1.9823e-02)	Acc@1  97.66 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [66][280/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.5894e-02 (1.9935e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [66][290/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.8245e-03 (2.0155e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [66][300/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.7781e-02 (2.0266e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [66][310/391]	Time  0.170 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6342e-02 (1.9988e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [66][320/391]	Time  0.172 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.9858e-03 (1.9829e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [66][330/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0811e-02 (1.9852e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [66][340/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1024e-02 (1.9729e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [66][350/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.7054e-02 (1.9571e-02)	Acc@1  98.44 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [66][360/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2367e-02 (1.9451e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [66][370/391]	Time  0.170 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.5509e-03 (1.9645e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [66][380/391]	Time  0.165 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.6047e-02 (1.9532e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [66][390/391]	Time  0.115 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5640e-02 (1.9528e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
## e[66] optimizer.zero_grad (sum) time: 0.5382513999938965
## e[66]       loss.backward (sum) time: 12.498410701751709
## e[66]      optimizer.step (sum) time: 25.724571228027344
## epoch[66] training(only) time: 63.74392294883728
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 1.7078e-01 (1.7078e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.060)	Loss 3.3594e-01 (3.1545e-01)	Acc@1  94.00 ( 92.64)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.047 ( 0.054)	Loss 5.1367e-01 (3.4178e-01)	Acc@1  90.00 ( 92.10)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 3.3618e-01 (3.3397e-01)	Acc@1  95.00 ( 92.42)	Acc@5  99.00 ( 99.77)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 4.9243e-01 (3.3982e-01)	Acc@1  90.00 ( 92.39)	Acc@5  98.00 ( 99.76)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 2.2949e-01 (3.4382e-01)	Acc@1  95.00 ( 92.35)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 2.5317e-01 (3.3619e-01)	Acc@1  92.00 ( 92.31)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 3.7061e-01 (3.3925e-01)	Acc@1  94.00 ( 92.25)	Acc@5 100.00 ( 99.80)
Test: [ 80/100]	Time  0.048 ( 0.049)	Loss 2.7637e-01 (3.3670e-01)	Acc@1  92.00 ( 92.33)	Acc@5 100.00 ( 99.81)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 2.7783e-01 (3.3712e-01)	Acc@1  93.00 ( 92.27)	Acc@5 100.00 ( 99.84)
 * Acc@1 92.200 Acc@5 99.840
### epoch[66] execution time: 68.72392988204956
EPOCH 67
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [67][  0/391]	Time  0.325 ( 0.325)	Data  0.142 ( 0.142)	Loss 9.9258e-03 (9.9258e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [67][ 10/391]	Time  0.161 ( 0.176)	Data  0.001 ( 0.014)	Loss 1.5793e-02 (1.2306e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [67][ 20/391]	Time  0.162 ( 0.169)	Data  0.001 ( 0.008)	Loss 4.0245e-03 (1.3180e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [67][ 30/391]	Time  0.160 ( 0.167)	Data  0.001 ( 0.006)	Loss 2.3087e-02 (1.3879e-02)	Acc@1  98.44 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [67][ 40/391]	Time  0.161 ( 0.166)	Data  0.001 ( 0.005)	Loss 1.2169e-02 (1.5115e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [67][ 50/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.3451e-02 (1.6165e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [67][ 60/391]	Time  0.177 ( 0.165)	Data  0.001 ( 0.003)	Loss 1.6068e-02 (1.6474e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [67][ 70/391]	Time  0.163 ( 0.165)	Data  0.001 ( 0.003)	Loss 1.1986e-02 (1.7660e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [67][ 80/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.0874e-02 (1.7737e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [67][ 90/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 7.1411e-03 (1.8015e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [67][100/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.9974e-02 (1.8393e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [67][110/391]	Time  0.164 ( 0.164)	Data  0.001 ( 0.002)	Loss 1.7090e-02 (1.8333e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [67][120/391]	Time  0.165 ( 0.164)	Data  0.001 ( 0.002)	Loss 1.3840e-02 (1.8882e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [67][130/391]	Time  0.164 ( 0.164)	Data  0.001 ( 0.002)	Loss 2.7740e-02 (1.8999e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [67][140/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.002)	Loss 1.6052e-02 (1.9079e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [67][150/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.002)	Loss 7.9117e-03 (1.8705e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [67][160/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5945e-02 (1.8803e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [67][170/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6083e-02 (1.8666e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [67][180/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5823e-02 (1.8857e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [67][190/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.2403e-03 (1.9420e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [67][200/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.3139e-03 (1.9400e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [67][210/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7319e-02 (1.9516e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [67][220/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.7444e-03 (1.9304e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [67][230/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5778e-02 (1.9370e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [67][240/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1513e-02 (1.9341e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [67][250/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5388e-02 (1.9195e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [67][260/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.2781e-02 (1.9298e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [67][270/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4580e-02 (1.9310e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [67][280/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.0152e-03 (1.9111e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [67][290/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.2594e-03 (1.9154e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [67][300/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.5563e-02 (1.9224e-02)	Acc@1  97.66 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [67][310/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6357e-02 (1.9264e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [67][320/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.0655e-02 (1.9254e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [67][330/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2070e-02 (1.9223e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [67][340/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.5909e-02 (1.9250e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [67][350/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4778e-02 (1.9293e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [67][360/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5022e-02 (1.9188e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [67][370/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.1869e-03 (1.9062e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [67][380/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.2795e-03 (1.9132e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [67][390/391]	Time  0.118 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1520e-02 (1.8995e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
## e[67] optimizer.zero_grad (sum) time: 0.5418026447296143
## e[67]       loss.backward (sum) time: 12.418739795684814
## e[67]      optimizer.step (sum) time: 25.79210615158081
## epoch[67] training(only) time: 63.735639810562134
# Switched to evaluate mode...
Test: [  0/100]	Time  0.191 ( 0.191)	Loss 1.6980e-01 (1.6980e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.060 ( 0.062)	Loss 3.2324e-01 (3.2075e-01)	Acc@1  93.00 ( 92.18)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 5.0049e-01 (3.4173e-01)	Acc@1  90.00 ( 92.00)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.048 ( 0.053)	Loss 3.2715e-01 (3.3546e-01)	Acc@1  94.00 ( 92.32)	Acc@5  99.00 ( 99.77)
Test: [ 40/100]	Time  0.048 ( 0.052)	Loss 5.0000e-01 (3.4048e-01)	Acc@1  89.00 ( 92.34)	Acc@5  98.00 ( 99.73)
Test: [ 50/100]	Time  0.048 ( 0.051)	Loss 2.0752e-01 (3.4353e-01)	Acc@1  95.00 ( 92.33)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.048 ( 0.051)	Loss 2.4365e-01 (3.3507e-01)	Acc@1  93.00 ( 92.36)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.048 ( 0.050)	Loss 3.7402e-01 (3.3720e-01)	Acc@1  94.00 ( 92.32)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.047 ( 0.050)	Loss 2.7686e-01 (3.3466e-01)	Acc@1  94.00 ( 92.41)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.046 ( 0.050)	Loss 3.0078e-01 (3.3558e-01)	Acc@1  93.00 ( 92.36)	Acc@5 100.00 ( 99.80)
 * Acc@1 92.300 Acc@5 99.810
### epoch[67] execution time: 68.80840730667114
EPOCH 68
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [68][  0/391]	Time  0.324 ( 0.324)	Data  0.146 ( 0.146)	Loss 1.5518e-02 (1.5518e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [68][ 10/391]	Time  0.160 ( 0.176)	Data  0.001 ( 0.014)	Loss 2.9800e-02 (1.9993e-02)	Acc@1  98.44 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [68][ 20/391]	Time  0.161 ( 0.169)	Data  0.001 ( 0.008)	Loss 1.9012e-02 (2.2591e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [68][ 30/391]	Time  0.161 ( 0.167)	Data  0.001 ( 0.006)	Loss 1.7960e-02 (2.2354e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [68][ 40/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.005)	Loss 6.3744e-03 (1.9944e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [68][ 50/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.004)	Loss 2.3834e-02 (2.1666e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [68][ 60/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.004)	Loss 7.9346e-03 (2.2606e-02)	Acc@1 100.00 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [68][ 70/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.6800e-02 (2.2450e-02)	Acc@1  99.22 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [68][ 80/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.5808e-02 (2.1194e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [68][ 90/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 6.1874e-03 (2.1005e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [68][100/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.2400e-02 (2.0599e-02)	Acc@1  98.44 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [68][110/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.4488e-02 (2.0018e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [68][120/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1185e-02 (1.9815e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [68][130/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.9716e-03 (1.9663e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [68][140/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.0930e-02 (1.9375e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [68][150/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.5146e-02 (1.9121e-02)	Acc@1  98.44 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [68][160/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.3662e-03 (1.9057e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [68][170/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2886e-02 (1.8960e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [68][180/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.7039e-02 (1.9017e-02)	Acc@1  98.44 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [68][190/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.6011e-02 (1.8792e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [68][200/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.1423e-02 (1.8519e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [68][210/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.7967e-03 (1.8524e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [68][220/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.9258e-03 (1.8566e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [68][230/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.8442e-02 (1.8414e-02)	Acc@1  98.44 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [68][240/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.6194e-02 (1.8763e-02)	Acc@1  98.44 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [68][250/391]	Time  0.166 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.0807e-02 (1.8711e-02)	Acc@1  98.44 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [68][260/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6479e-02 (1.8673e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [68][270/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7868e-02 (1.8746e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [68][280/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5884e-02 (1.8642e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [68][290/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3206e-02 (1.8692e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [68][300/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0048e-02 (1.8684e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [68][310/391]	Time  0.173 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.6962e-02 (1.8699e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [68][320/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3687e-02 (1.8620e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [68][330/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.6957e-02 (1.8483e-02)	Acc@1  98.44 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [68][340/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0704e-02 (1.8458e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [68][350/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.1727e-03 (1.8465e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [68][360/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0445e-02 (1.8675e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [68][370/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.5711e-02 (1.8718e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [68][380/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.2171e-02 (1.9010e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [68][390/391]	Time  0.115 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.2812e-02 (1.9001e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
## e[68] optimizer.zero_grad (sum) time: 0.5348305702209473
## e[68]       loss.backward (sum) time: 12.48160696029663
## e[68]      optimizer.step (sum) time: 25.712182998657227
## epoch[68] training(only) time: 63.57538151741028
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 1.7395e-01 (1.7395e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 3.4253e-01 (3.1971e-01)	Acc@1  93.00 ( 91.82)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.047 ( 0.054)	Loss 5.0439e-01 (3.4359e-01)	Acc@1  91.00 ( 91.86)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 3.2422e-01 (3.3839e-01)	Acc@1  93.00 ( 92.06)	Acc@5  99.00 ( 99.77)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 5.1123e-01 (3.4271e-01)	Acc@1  89.00 ( 92.15)	Acc@5  98.00 ( 99.76)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 1.9800e-01 (3.4562e-01)	Acc@1  95.00 ( 92.24)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 2.4316e-01 (3.3765e-01)	Acc@1  94.00 ( 92.23)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 4.0918e-01 (3.4002e-01)	Acc@1  92.00 ( 92.20)	Acc@5 100.00 ( 99.82)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 2.6807e-01 (3.3846e-01)	Acc@1  94.00 ( 92.31)	Acc@5 100.00 ( 99.83)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 2.8369e-01 (3.3893e-01)	Acc@1  92.00 ( 92.29)	Acc@5 100.00 ( 99.84)
 * Acc@1 92.250 Acc@5 99.840
### epoch[68] execution time: 68.53990459442139
EPOCH 69
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [69][  0/391]	Time  0.327 ( 0.327)	Data  0.141 ( 0.141)	Loss 2.7512e-02 (2.7512e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [69][ 10/391]	Time  0.161 ( 0.177)	Data  0.001 ( 0.014)	Loss 7.5073e-03 (1.9161e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [69][ 20/391]	Time  0.161 ( 0.170)	Data  0.001 ( 0.008)	Loss 3.0563e-02 (1.9779e-02)	Acc@1  97.66 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [69][ 30/391]	Time  0.161 ( 0.167)	Data  0.001 ( 0.006)	Loss 1.7990e-02 (1.8389e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [69][ 40/391]	Time  0.161 ( 0.166)	Data  0.001 ( 0.005)	Loss 1.8631e-02 (1.7909e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [69][ 50/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.004)	Loss 3.2227e-02 (1.9310e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [69][ 60/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.3560e-02 (1.9264e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [69][ 70/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.5579e-02 (1.9522e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [69][ 80/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 5.8975e-03 (1.9049e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [69][ 90/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.7563e-02 (1.9096e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [69][100/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 9.3918e-03 (1.8512e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [69][110/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.8763e-02 (1.8613e-02)	Acc@1  98.44 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [69][120/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.9694e-02 (1.8709e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [69][130/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.1449e-03 (1.8913e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [69][140/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0307e-02 (2.0362e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [69][150/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.3468e-02 (1.9817e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [69][160/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.1500e-02 (1.9682e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [69][170/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4832e-02 (1.9668e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [69][180/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.1921e-02 (1.9624e-02)	Acc@1  98.44 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [69][190/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2497e-02 (1.9452e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [69][200/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.7863e-02 (1.9160e-02)	Acc@1  98.44 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [69][210/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.6310e-03 (1.8832e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [69][220/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.2858e-02 (1.8726e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [69][230/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.9958e-02 (1.8612e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [69][240/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6190e-02 (1.8462e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [69][250/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.0233e-02 (1.8419e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [69][260/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.6840e-02 (1.8560e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [69][270/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2939e-02 (1.8391e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [69][280/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4122e-02 (1.8267e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [69][290/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.5177e-02 (1.8430e-02)	Acc@1  98.44 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [69][300/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.8025e-02 (1.8453e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [69][310/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.2751e-02 (1.8531e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [69][320/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.7298e-02 (1.8586e-02)	Acc@1  98.44 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [69][330/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3191e-02 (1.8476e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [69][340/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.0981e-02 (1.8592e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [69][350/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.8198e-02 (1.8625e-02)	Acc@1  98.44 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [69][360/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.001)	Loss 2.2079e-02 (1.8606e-02)	Acc@1  98.44 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [69][370/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.001)	Loss 1.9165e-02 (1.8678e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [69][380/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.001)	Loss 5.7182e-03 (1.8544e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [69][390/391]	Time  0.118 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.4870e-02 (1.8491e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
## e[69] optimizer.zero_grad (sum) time: 0.5293052196502686
## e[69]       loss.backward (sum) time: 12.475963354110718
## e[69]      optimizer.step (sum) time: 25.74509072303772
## epoch[69] training(only) time: 63.61656618118286
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 1.6858e-01 (1.6858e-01)	Acc@1  95.00 ( 95.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.048 ( 0.061)	Loss 3.3667e-01 (3.2101e-01)	Acc@1  92.00 ( 91.82)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.048 ( 0.056)	Loss 5.1221e-01 (3.4267e-01)	Acc@1  90.00 ( 91.67)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 3.3618e-01 (3.3639e-01)	Acc@1  92.00 ( 91.97)	Acc@5  99.00 ( 99.77)
Test: [ 40/100]	Time  0.047 ( 0.052)	Loss 5.0537e-01 (3.4101e-01)	Acc@1  89.00 ( 92.00)	Acc@5  98.00 ( 99.73)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 2.1167e-01 (3.4352e-01)	Acc@1  95.00 ( 92.06)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.057 ( 0.050)	Loss 2.4915e-01 (3.3660e-01)	Acc@1  93.00 ( 92.05)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 3.6890e-01 (3.3970e-01)	Acc@1  93.00 ( 92.01)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 2.8003e-01 (3.3684e-01)	Acc@1  94.00 ( 92.12)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 2.8857e-01 (3.3739e-01)	Acc@1  93.00 ( 92.10)	Acc@5 100.00 ( 99.82)
 * Acc@1 92.050 Acc@5 99.830
### epoch[69] execution time: 68.60424733161926
EPOCH 70
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [70][  0/391]	Time  0.322 ( 0.322)	Data  0.137 ( 0.137)	Loss 1.2634e-02 (1.2634e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [70][ 10/391]	Time  0.161 ( 0.176)	Data  0.001 ( 0.013)	Loss 4.0466e-02 (2.1152e-02)	Acc@1  98.44 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [70][ 20/391]	Time  0.162 ( 0.169)	Data  0.002 ( 0.008)	Loss 1.4801e-02 (1.9382e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [70][ 30/391]	Time  0.161 ( 0.167)	Data  0.001 ( 0.006)	Loss 2.3087e-02 (1.7925e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [70][ 40/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.004)	Loss 3.1281e-02 (1.8686e-02)	Acc@1  98.44 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [70][ 50/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.004)	Loss 6.2294e-03 (1.8721e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [70][ 60/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.003)	Loss 4.8065e-03 (1.9351e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [70][ 70/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.4145e-02 (1.9083e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [70][ 80/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.7929e-02 (1.8893e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [70][ 90/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.1574e-02 (1.8808e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [70][100/391]	Time  0.164 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.7563e-02 (1.8778e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [70][110/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.002)	Loss 6.9237e-03 (1.9195e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [70][120/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7532e-02 (1.9032e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [70][130/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.0616e-03 (1.8647e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [70][140/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6937e-02 (1.8447e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [70][150/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.8387e-02 (1.8333e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [70][160/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.5330e-02 (1.8555e-02)	Acc@1  98.44 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [70][170/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.8066e-02 (1.8686e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [70][180/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2451e-02 (1.8853e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [70][190/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.2837e-02 (1.8912e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [70][200/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.8631e-02 (1.8766e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [70][210/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.0945e-02 (1.8836e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [70][220/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.7557e-02 (1.8730e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [70][230/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3725e-02 (1.8791e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [70][240/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.2959e-02 (1.9083e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [70][250/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5617e-02 (1.8833e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [70][260/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.6907e-03 (1.8806e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [70][270/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.6703e-02 (1.8828e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [70][280/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.8267e-03 (1.9032e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [70][290/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5991e-02 (1.8897e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [70][300/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.1776e-03 (1.8543e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [70][310/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.3160e-03 (1.8772e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [70][320/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.2064e-02 (1.8805e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [70][330/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.9597e-03 (1.8759e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [70][340/391]	Time  0.165 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0719e-02 (1.8822e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [70][350/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.9897e-02 (1.8725e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [70][360/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.3084e-03 (1.8748e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [70][370/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.2779e-03 (1.8685e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [70][380/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6083e-02 (1.8496e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [70][390/391]	Time  0.115 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.6901e-02 (1.8491e-02)	Acc@1  98.75 ( 99.46)	Acc@5 100.00 (100.00)
## e[70] optimizer.zero_grad (sum) time: 0.5339457988739014
## e[70]       loss.backward (sum) time: 12.468310832977295
## e[70]      optimizer.step (sum) time: 25.729620933532715
## epoch[70] training(only) time: 63.63123059272766
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 1.6638e-01 (1.6638e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.048 ( 0.062)	Loss 3.4155e-01 (3.2373e-01)	Acc@1  92.00 ( 91.82)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.048 ( 0.056)	Loss 5.1514e-01 (3.4935e-01)	Acc@1  91.00 ( 91.71)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.047 ( 0.053)	Loss 3.3643e-01 (3.4279e-01)	Acc@1  93.00 ( 92.00)	Acc@5  99.00 ( 99.77)
Test: [ 40/100]	Time  0.047 ( 0.052)	Loss 5.1318e-01 (3.4865e-01)	Acc@1  89.00 ( 92.05)	Acc@5  98.00 ( 99.76)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 2.0276e-01 (3.5053e-01)	Acc@1  95.00 ( 92.10)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.050 ( 0.050)	Loss 2.3193e-01 (3.4181e-01)	Acc@1  93.00 ( 92.13)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.048 ( 0.050)	Loss 3.7695e-01 (3.4289e-01)	Acc@1  93.00 ( 92.15)	Acc@5 100.00 ( 99.80)
Test: [ 80/100]	Time  0.047 ( 0.050)	Loss 2.5684e-01 (3.4001e-01)	Acc@1  95.00 ( 92.22)	Acc@5 100.00 ( 99.81)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 2.8955e-01 (3.4039e-01)	Acc@1  94.00 ( 92.22)	Acc@5 100.00 ( 99.82)
 * Acc@1 92.200 Acc@5 99.830
### epoch[70] execution time: 68.64849877357483
EPOCH 71
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [71][  0/391]	Time  0.328 ( 0.328)	Data  0.139 ( 0.139)	Loss 1.6022e-02 (1.6022e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [71][ 10/391]	Time  0.162 ( 0.176)	Data  0.001 ( 0.014)	Loss 2.1820e-02 (1.7810e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [71][ 20/391]	Time  0.160 ( 0.169)	Data  0.001 ( 0.008)	Loss 4.9255e-02 (1.8638e-02)	Acc@1  98.44 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [71][ 30/391]	Time  0.175 ( 0.167)	Data  0.001 ( 0.006)	Loss 6.2675e-03 (1.6008e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [71][ 40/391]	Time  0.168 ( 0.166)	Data  0.001 ( 0.005)	Loss 2.5299e-02 (1.6840e-02)	Acc@1  98.44 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [71][ 50/391]	Time  0.162 ( 0.165)	Data  0.001 ( 0.004)	Loss 9.8419e-03 (1.6456e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [71][ 60/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.003)	Loss 1.5030e-02 (1.6979e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [71][ 70/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.0002e-02 (1.7036e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [71][ 80/391]	Time  0.171 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.2512e-02 (1.6978e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [71][ 90/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.4353e-02 (1.6909e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [71][100/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.4551e-02 (1.6795e-02)	Acc@1  98.44 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [71][110/391]	Time  0.163 ( 0.164)	Data  0.001 ( 0.002)	Loss 1.0567e-02 (1.7011e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [71][120/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.002)	Loss 2.9312e-02 (1.7413e-02)	Acc@1  98.44 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [71][130/391]	Time  0.172 ( 0.164)	Data  0.001 ( 0.002)	Loss 1.9211e-02 (1.7881e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [71][140/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.002)	Loss 1.8555e-02 (1.8213e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [71][150/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.002)	Loss 1.3458e-02 (1.8438e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [71][160/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.002)	Loss 8.4152e-03 (1.8348e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [71][170/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.7781e-02 (1.8622e-02)	Acc@1  98.44 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [71][180/391]	Time  0.172 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.4986e-03 (1.8545e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [71][190/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.8845e-02 (1.8373e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [71][200/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.8992e-02 (1.8278e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [71][210/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7792e-02 (1.8521e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [71][220/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0818e-02 (1.9026e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [71][230/391]	Time  0.171 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7761e-02 (1.9017e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [71][240/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.2537e-02 (1.8975e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [71][250/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2299e-02 (1.8744e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [71][260/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3710e-02 (1.8836e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [71][270/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.2795e-03 (1.8490e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [71][280/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.1618e-03 (1.8313e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [71][290/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.8723e-02 (1.8433e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [71][300/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.001)	Loss 2.4826e-02 (1.8569e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [71][310/391]	Time  0.169 ( 0.163)	Data  0.001 ( 0.001)	Loss 1.5610e-02 (1.8410e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [71][320/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.001)	Loss 1.5450e-02 (1.8333e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [71][330/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.001)	Loss 4.3488e-03 (1.8436e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [71][340/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.001)	Loss 1.3069e-02 (1.8488e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [71][350/391]	Time  0.177 ( 0.163)	Data  0.001 ( 0.001)	Loss 2.9205e-02 (1.8451e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [71][360/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.001)	Loss 6.6109e-03 (1.8504e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [71][370/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.001)	Loss 7.0343e-03 (1.8375e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [71][380/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.001)	Loss 1.0246e-02 (1.8247e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [71][390/391]	Time  0.117 ( 0.163)	Data  0.001 ( 0.001)	Loss 4.1504e-02 (1.8414e-02)	Acc@1  97.50 ( 99.43)	Acc@5 100.00 (100.00)
## e[71] optimizer.zero_grad (sum) time: 0.5363013744354248
## e[71]       loss.backward (sum) time: 12.469228506088257
## e[71]      optimizer.step (sum) time: 25.761638164520264
## epoch[71] training(only) time: 63.79825305938721
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 1.5942e-01 (1.5942e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 3.2617e-01 (3.1818e-01)	Acc@1  93.00 ( 92.09)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.047 ( 0.054)	Loss 4.8584e-01 (3.3911e-01)	Acc@1  90.00 ( 91.95)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 3.2666e-01 (3.3780e-01)	Acc@1  93.00 ( 92.23)	Acc@5  99.00 ( 99.74)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 4.8999e-01 (3.4162e-01)	Acc@1  89.00 ( 92.22)	Acc@5  99.00 ( 99.73)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 1.9153e-01 (3.4544e-01)	Acc@1  95.00 ( 92.25)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 2.3242e-01 (3.3697e-01)	Acc@1  93.00 ( 92.26)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 4.0771e-01 (3.3979e-01)	Acc@1  93.00 ( 92.20)	Acc@5 100.00 ( 99.80)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 2.5781e-01 (3.3823e-01)	Acc@1  95.00 ( 92.28)	Acc@5 100.00 ( 99.81)
Test: [ 90/100]	Time  0.058 ( 0.049)	Loss 3.0029e-01 (3.3924e-01)	Acc@1  92.00 ( 92.29)	Acc@5 100.00 ( 99.82)
 * Acc@1 92.270 Acc@5 99.830
### epoch[71] execution time: 68.78945207595825
EPOCH 72
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [72][  0/391]	Time  0.340 ( 0.340)	Data  0.161 ( 0.161)	Loss 2.6855e-02 (2.6855e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [72][ 10/391]	Time  0.161 ( 0.179)	Data  0.001 ( 0.016)	Loss 5.5656e-03 (2.0985e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [72][ 20/391]	Time  0.161 ( 0.170)	Data  0.001 ( 0.009)	Loss 1.9165e-02 (2.1515e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [72][ 30/391]	Time  0.162 ( 0.167)	Data  0.001 ( 0.006)	Loss 3.8391e-02 (2.2553e-02)	Acc@1  97.66 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [72][ 40/391]	Time  0.162 ( 0.166)	Data  0.001 ( 0.005)	Loss 9.1019e-03 (2.0743e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [72][ 50/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.8646e-02 (1.9354e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [72][ 60/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.0406e-02 (2.0767e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [72][ 70/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.3916e-02 (2.1217e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [72][ 80/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.6083e-02 (2.0360e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [72][ 90/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.2720e-02 (2.0238e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [72][100/391]	Time  0.164 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.0788e-02 (1.9954e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [72][110/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 9.5749e-03 (1.9592e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [72][120/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.1276e-02 (1.9570e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [72][130/391]	Time  0.167 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.4986e-03 (1.9281e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [72][140/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.4223e-03 (1.9053e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [72][150/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.6577e-03 (1.8909e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [72][160/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5839e-02 (1.9156e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [72][170/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.8829e-02 (1.8898e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [72][180/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.7607e-03 (1.9036e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [72][190/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.9340e-03 (1.8711e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [72][200/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5190e-02 (1.8876e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [72][210/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.5400e-02 (1.9027e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [72][220/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4687e-02 (1.9235e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [72][230/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4359e-02 (1.9302e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [72][240/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.3303e-02 (1.9343e-02)	Acc@1  96.88 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [72][250/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.3610e-02 (1.9360e-02)	Acc@1  97.66 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [72][260/391]	Time  0.178 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.2441e-03 (1.9238e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [72][270/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.1819e-02 (1.9479e-02)	Acc@1  98.44 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [72][280/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.9653e-02 (1.9420e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [72][290/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.4528e-03 (1.9366e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [72][300/391]	Time  0.171 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.9215e-02 (1.9186e-02)	Acc@1  97.66 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [72][310/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.0344e-02 (1.9284e-02)	Acc@1  98.44 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [72][320/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1032e-02 (1.9180e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [72][330/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.1934e-03 (1.9206e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [72][340/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2512e-02 (1.9196e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [72][350/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3657e-02 (1.9103e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [72][360/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.3193e-02 (1.9009e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [72][370/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.5978e-03 (1.8962e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [72][380/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0086e-02 (1.9097e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [72][390/391]	Time  0.120 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.5073e-02 (1.9119e-02)	Acc@1  98.75 ( 99.47)	Acc@5 100.00 (100.00)
## e[72] optimizer.zero_grad (sum) time: 0.5365860462188721
## e[72]       loss.backward (sum) time: 12.497592687606812
## e[72]      optimizer.step (sum) time: 25.687421083450317
## epoch[72] training(only) time: 63.65664339065552
# Switched to evaluate mode...
Test: [  0/100]	Time  0.207 ( 0.207)	Loss 1.6101e-01 (1.6101e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.062)	Loss 3.4595e-01 (3.2333e-01)	Acc@1  91.00 ( 92.18)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.048 ( 0.055)	Loss 5.1172e-01 (3.4293e-01)	Acc@1  90.00 ( 92.10)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.048 ( 0.053)	Loss 3.3521e-01 (3.3715e-01)	Acc@1  93.00 ( 92.42)	Acc@5  99.00 ( 99.71)
Test: [ 40/100]	Time  0.048 ( 0.052)	Loss 5.3223e-01 (3.4374e-01)	Acc@1  90.00 ( 92.44)	Acc@5  98.00 ( 99.68)
Test: [ 50/100]	Time  0.052 ( 0.051)	Loss 2.1118e-01 (3.4698e-01)	Acc@1  96.00 ( 92.41)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.047 ( 0.051)	Loss 2.3352e-01 (3.3981e-01)	Acc@1  94.00 ( 92.33)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 3.9258e-01 (3.4341e-01)	Acc@1  94.00 ( 92.30)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 2.6416e-01 (3.4057e-01)	Acc@1  93.00 ( 92.38)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.048 ( 0.050)	Loss 2.5635e-01 (3.4139e-01)	Acc@1  94.00 ( 92.36)	Acc@5 100.00 ( 99.80)
 * Acc@1 92.310 Acc@5 99.810
### epoch[72] execution time: 68.68380498886108
EPOCH 73
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [73][  0/391]	Time  0.326 ( 0.326)	Data  0.155 ( 0.155)	Loss 1.4519e-02 (1.4519e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [73][ 10/391]	Time  0.161 ( 0.177)	Data  0.001 ( 0.015)	Loss 1.5381e-02 (1.9333e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [73][ 20/391]	Time  0.172 ( 0.170)	Data  0.001 ( 0.009)	Loss 8.1177e-03 (1.7985e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [73][ 30/391]	Time  0.162 ( 0.168)	Data  0.001 ( 0.006)	Loss 2.6642e-02 (1.7389e-02)	Acc@1  98.44 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [73][ 40/391]	Time  0.161 ( 0.167)	Data  0.001 ( 0.005)	Loss 5.4779e-03 (1.7126e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [73][ 50/391]	Time  0.162 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.3985e-02 (1.7080e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [73][ 60/391]	Time  0.162 ( 0.165)	Data  0.001 ( 0.004)	Loss 2.3315e-02 (1.7860e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [73][ 70/391]	Time  0.171 ( 0.164)	Data  0.001 ( 0.003)	Loss 3.3600e-02 (1.7829e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [73][ 80/391]	Time  0.165 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.4366e-02 (1.7987e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [73][ 90/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.1948e-02 (1.8192e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [73][100/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.6632e-02 (1.8499e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [73][110/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 6.3324e-03 (1.8198e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [73][120/391]	Time  0.168 ( 0.164)	Data  0.001 ( 0.002)	Loss 1.7822e-02 (1.7988e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [73][130/391]	Time  0.179 ( 0.164)	Data  0.001 ( 0.002)	Loss 2.6016e-02 (1.7714e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [73][140/391]	Time  0.166 ( 0.164)	Data  0.001 ( 0.002)	Loss 1.9592e-02 (1.7202e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [73][150/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2711e-02 (1.7102e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [73][160/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.9520e-03 (1.7016e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [73][170/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1360e-02 (1.7179e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [73][180/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.4323e-02 (1.7113e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [73][190/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.5992e-03 (1.6718e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [73][200/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.3307e-03 (1.6563e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [73][210/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0796e-02 (1.6320e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [73][220/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.0877e-03 (1.6185e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [73][230/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.7598e-02 (1.6224e-02)	Acc@1  97.66 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [73][240/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.5166e-02 (1.6442e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [73][250/391]	Time  0.166 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.9302e-02 (1.6399e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [73][260/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.4826e-02 (1.6365e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [73][270/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.6741e-03 (1.6614e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [73][280/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.9180e-02 (1.6481e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [73][290/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.4506e-02 (1.6645e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [73][300/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0857e-02 (1.6857e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [73][310/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.3682e-02 (1.6812e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [73][320/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.4452e-03 (1.6815e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [73][330/391]	Time  0.179 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.1464e-02 (1.7042e-02)	Acc@1  98.44 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [73][340/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.4749e-02 (1.7076e-02)	Acc@1  98.44 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [73][350/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.0131e-03 (1.7059e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [73][360/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.3422e-02 (1.7125e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [73][370/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.9974e-02 (1.7137e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [73][380/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.2964e-02 (1.7048e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [73][390/391]	Time  0.118 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.1483e-02 (1.7276e-02)	Acc@1  97.50 ( 99.55)	Acc@5 100.00 (100.00)
## e[73] optimizer.zero_grad (sum) time: 0.5359578132629395
## e[73]       loss.backward (sum) time: 12.446873664855957
## e[73]      optimizer.step (sum) time: 25.73514461517334
## epoch[73] training(only) time: 63.74338150024414
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 1.6785e-01 (1.6785e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 3.3594e-01 (3.2609e-01)	Acc@1  93.00 ( 92.27)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.047 ( 0.054)	Loss 5.2832e-01 (3.5261e-01)	Acc@1  90.00 ( 91.90)	Acc@5 100.00 ( 99.67)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 3.4106e-01 (3.4241e-01)	Acc@1  95.00 ( 92.35)	Acc@5  99.00 ( 99.71)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 5.1562e-01 (3.4748e-01)	Acc@1  89.00 ( 92.27)	Acc@5  98.00 ( 99.68)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 2.2510e-01 (3.5041e-01)	Acc@1  95.00 ( 92.20)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 2.5903e-01 (3.4242e-01)	Acc@1  93.00 ( 92.21)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 3.6475e-01 (3.4498e-01)	Acc@1  93.00 ( 92.23)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 2.7271e-01 (3.4235e-01)	Acc@1  92.00 ( 92.31)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 2.9443e-01 (3.4266e-01)	Acc@1  93.00 ( 92.21)	Acc@5 100.00 ( 99.77)
 * Acc@1 92.140 Acc@5 99.780
### epoch[73] execution time: 68.71709895133972
EPOCH 74
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [74][  0/391]	Time  0.320 ( 0.320)	Data  0.150 ( 0.150)	Loss 1.1566e-02 (1.1566e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [74][ 10/391]	Time  0.161 ( 0.175)	Data  0.001 ( 0.015)	Loss 4.8561e-03 (1.1040e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [74][ 20/391]	Time  0.161 ( 0.169)	Data  0.001 ( 0.008)	Loss 3.3054e-03 (1.2545e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [74][ 30/391]	Time  0.162 ( 0.167)	Data  0.001 ( 0.006)	Loss 1.4046e-02 (1.5376e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [74][ 40/391]	Time  0.162 ( 0.166)	Data  0.001 ( 0.005)	Loss 8.4457e-03 (1.6242e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [74][ 50/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.004)	Loss 3.8483e-02 (1.8117e-02)	Acc@1  98.44 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [74][ 60/391]	Time  0.162 ( 0.165)	Data  0.001 ( 0.004)	Loss 4.4983e-02 (1.8896e-02)	Acc@1  97.66 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [74][ 70/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.003)	Loss 1.0025e-02 (1.8145e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [74][ 80/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 4.6326e-02 (1.7960e-02)	Acc@1  98.44 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [74][ 90/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.1927e-02 (1.7782e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [74][100/391]	Time  0.164 ( 0.164)	Data  0.002 ( 0.003)	Loss 8.2474e-03 (1.7412e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [74][110/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.8341e-02 (1.7183e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [74][120/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.002)	Loss 6.3438e-03 (1.6964e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [74][130/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5236e-02 (1.7127e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [74][140/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.3002e-03 (1.7005e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [74][150/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5030e-02 (1.6841e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [74][160/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.1793e-03 (1.6860e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [74][170/391]	Time  0.167 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2108e-02 (1.6851e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [74][180/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.8997e-02 (1.6482e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [74][190/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.5206e-02 (1.6751e-02)	Acc@1  98.44 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [74][200/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1559e-02 (1.6762e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [74][210/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.0120e-03 (1.6600e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [74][220/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.0721e-02 (1.6665e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [74][230/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0742e-02 (1.6660e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [74][240/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.7934e-03 (1.6955e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [74][250/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.4485e-03 (1.6696e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [74][260/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.4617e-02 (1.6892e-02)	Acc@1  98.44 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [74][270/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.8372e-02 (1.6778e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [74][280/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0941e-02 (1.6627e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [74][290/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.2949e-02 (1.6619e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [74][300/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1559e-02 (1.6626e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [74][310/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.5193e-03 (1.6605e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [74][320/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.3689e-03 (1.6553e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [74][330/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6968e-02 (1.6553e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [74][340/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.7810e-02 (1.6794e-02)	Acc@1  98.44 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [74][350/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.0004e-02 (1.6980e-02)	Acc@1  98.44 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [74][360/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1955e-02 (1.6992e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [74][370/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.3722e-03 (1.6947e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [74][380/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0292e-02 (1.6994e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [74][390/391]	Time  0.115 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.9084e-03 (1.6900e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
## e[74] optimizer.zero_grad (sum) time: 0.5331621170043945
## e[74]       loss.backward (sum) time: 12.490337371826172
## e[74]      optimizer.step (sum) time: 25.74723219871521
## epoch[74] training(only) time: 63.58764982223511
# Switched to evaluate mode...
Test: [  0/100]	Time  0.191 ( 0.191)	Loss 1.6382e-01 (1.6382e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 3.3936e-01 (3.1675e-01)	Acc@1  93.00 ( 92.27)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 5.2295e-01 (3.4308e-01)	Acc@1  90.00 ( 91.95)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.055 ( 0.052)	Loss 3.2959e-01 (3.3716e-01)	Acc@1  94.00 ( 92.29)	Acc@5  99.00 ( 99.77)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 5.0586e-01 (3.4352e-01)	Acc@1  89.00 ( 92.24)	Acc@5  98.00 ( 99.73)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 2.0056e-01 (3.4588e-01)	Acc@1  95.00 ( 92.25)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.047 ( 0.049)	Loss 2.4780e-01 (3.3821e-01)	Acc@1  92.00 ( 92.26)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 3.7256e-01 (3.3997e-01)	Acc@1  93.00 ( 92.24)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 2.6245e-01 (3.3782e-01)	Acc@1  94.00 ( 92.28)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 2.9150e-01 (3.3841e-01)	Acc@1  92.00 ( 92.23)	Acc@5 100.00 ( 99.82)
 * Acc@1 92.210 Acc@5 99.830
### epoch[74] execution time: 68.53774619102478
EPOCH 75
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [75][  0/391]	Time  0.323 ( 0.323)	Data  0.145 ( 0.145)	Loss 2.4551e-02 (2.4551e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [75][ 10/391]	Time  0.161 ( 0.176)	Data  0.001 ( 0.014)	Loss 1.2688e-02 (1.6511e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [75][ 20/391]	Time  0.161 ( 0.169)	Data  0.001 ( 0.008)	Loss 1.7578e-02 (1.7634e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [75][ 30/391]	Time  0.161 ( 0.167)	Data  0.001 ( 0.006)	Loss 2.9877e-02 (1.6658e-02)	Acc@1  98.44 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [75][ 40/391]	Time  0.161 ( 0.166)	Data  0.001 ( 0.005)	Loss 1.5732e-02 (1.7379e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [75][ 50/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.7532e-02 (1.7293e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [75][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 6.7749e-03 (1.6878e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [75][ 70/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.0109e-02 (1.7791e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [75][ 80/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.7120e-02 (1.7656e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [75][ 90/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.2602e-03 (1.7667e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [75][100/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4862e-02 (1.7456e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [75][110/391]	Time  0.178 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.8259e-02 (1.7139e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [75][120/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.5569e-03 (1.7358e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [75][130/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1436e-02 (1.6983e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [75][140/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2039e-02 (1.7303e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [75][150/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0872e-02 (1.7701e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [75][160/391]	Time  0.169 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.9073e-02 (1.7423e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [75][170/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.9150e-02 (1.7458e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [75][180/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1559e-02 (1.7399e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [75][190/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0986e-02 (1.7418e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [75][200/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.2305e-03 (1.7223e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [75][210/391]	Time  0.180 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3138e-02 (1.7330e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [75][220/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.3243e-02 (1.7800e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [75][230/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.7787e-03 (1.7994e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [75][240/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.6154e-02 (1.8047e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [75][250/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.9190e-02 (1.8115e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [75][260/391]	Time  0.171 ( 0.163)	Data  0.001 ( 0.001)	Loss 7.5150e-03 (1.8079e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [75][270/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.001)	Loss 1.4954e-02 (1.7916e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [75][280/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.001)	Loss 2.0966e-02 (1.7930e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [75][290/391]	Time  0.167 ( 0.163)	Data  0.001 ( 0.001)	Loss 7.8125e-03 (1.7891e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [75][300/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.001)	Loss 1.7349e-02 (1.7934e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [75][310/391]	Time  0.172 ( 0.163)	Data  0.001 ( 0.001)	Loss 1.4015e-02 (1.7959e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [75][320/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.001)	Loss 8.6288e-03 (1.7975e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [75][330/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.3580e-02 (1.8054e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [75][340/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.6724e-02 (1.8029e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [75][350/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.3466e-02 (1.7849e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [75][360/391]	Time  0.171 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.3153e-02 (1.7752e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [75][370/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.001)	Loss 4.9553e-03 (1.7688e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [75][380/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 9.1476e-03 (1.7663e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [75][390/391]	Time  0.114 ( 0.162)	Data  0.001 ( 0.001)	Loss 9.8801e-03 (1.7845e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
## e[75] optimizer.zero_grad (sum) time: 0.5258841514587402
## e[75]       loss.backward (sum) time: 12.48134708404541
## e[75]      optimizer.step (sum) time: 25.72044610977173
## epoch[75] training(only) time: 63.53855586051941
# Switched to evaluate mode...
Test: [  0/100]	Time  0.191 ( 0.191)	Loss 1.6907e-01 (1.6907e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.048 ( 0.063)	Loss 3.4009e-01 (3.1906e-01)	Acc@1  90.00 ( 91.91)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.047 ( 0.056)	Loss 4.8901e-01 (3.4231e-01)	Acc@1  90.00 ( 91.81)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.048 ( 0.053)	Loss 3.2861e-01 (3.3698e-01)	Acc@1  94.00 ( 92.19)	Acc@5  99.00 ( 99.77)
Test: [ 40/100]	Time  0.047 ( 0.052)	Loss 5.1025e-01 (3.4094e-01)	Acc@1  89.00 ( 92.15)	Acc@5  98.00 ( 99.73)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 1.9312e-01 (3.4336e-01)	Acc@1  95.00 ( 92.16)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 2.4622e-01 (3.3576e-01)	Acc@1  93.00 ( 92.18)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 3.9868e-01 (3.3774e-01)	Acc@1  94.00 ( 92.21)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.047 ( 0.050)	Loss 2.9199e-01 (3.3597e-01)	Acc@1  92.00 ( 92.28)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 3.0200e-01 (3.3689e-01)	Acc@1  93.00 ( 92.24)	Acc@5 100.00 ( 99.81)
 * Acc@1 92.200 Acc@5 99.820
### epoch[75] execution time: 68.54278206825256
EPOCH 76
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [76][  0/391]	Time  0.319 ( 0.319)	Data  0.146 ( 0.146)	Loss 2.1118e-02 (2.1118e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [76][ 10/391]	Time  0.161 ( 0.176)	Data  0.001 ( 0.014)	Loss 6.6261e-03 (1.8584e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [76][ 20/391]	Time  0.161 ( 0.169)	Data  0.001 ( 0.008)	Loss 2.4124e-02 (1.9801e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [76][ 30/391]	Time  0.161 ( 0.167)	Data  0.001 ( 0.006)	Loss 8.9951e-03 (1.9150e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [76][ 40/391]	Time  0.161 ( 0.166)	Data  0.001 ( 0.005)	Loss 1.3206e-02 (1.8921e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [76][ 50/391]	Time  0.162 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.4023e-02 (1.8035e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [76][ 60/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.6525e-02 (1.8968e-02)	Acc@1  98.44 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [76][ 70/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 3.2684e-02 (1.9576e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [76][ 80/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.3870e-02 (1.9486e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [76][ 90/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.2955e-02 (1.8674e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [76][100/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.8341e-02 (1.7996e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [76][110/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3062e-02 (1.7599e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [76][120/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7685e-02 (1.7776e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [76][130/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5717e-02 (1.7501e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [76][140/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.8910e-02 (1.7695e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [76][150/391]	Time  0.173 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.8588e-03 (1.7606e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [76][160/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.1774e-02 (1.7854e-02)	Acc@1  98.44 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [76][170/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.8671e-02 (1.7806e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [76][180/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.2400e-02 (1.7598e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [76][190/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.4637e-02 (1.7934e-02)	Acc@1  97.66 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [76][200/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.0354e-02 (1.8100e-02)	Acc@1  97.66 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [76][210/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.8280e-02 (1.7930e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [76][220/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.7972e-03 (1.7978e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [76][230/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2924e-02 (1.7777e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [76][240/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2558e-02 (1.7733e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [76][250/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5671e-02 (1.7582e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [76][260/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.1760e-03 (1.7808e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [76][270/391]	Time  0.165 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.9831e-02 (1.8118e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [76][280/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.5070e-02 (1.8072e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [76][290/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.4078e-02 (1.7992e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [76][300/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6663e-02 (1.8022e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [76][310/391]	Time  0.170 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.2206e-02 (1.7974e-02)	Acc@1  98.44 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [76][320/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.1270e-02 (1.8014e-02)	Acc@1  98.44 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [76][330/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4145e-02 (1.7986e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [76][340/391]	Time  0.168 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.8812e-03 (1.8011e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [76][350/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.2842e-02 (1.8152e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [76][360/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.4922e-02 (1.8123e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [76][370/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.0142e-02 (1.8161e-02)	Acc@1  98.44 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [76][380/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1635e-02 (1.8014e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [76][390/391]	Time  0.118 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.9501e-02 (1.7954e-02)	Acc@1  98.75 ( 99.50)	Acc@5 100.00 (100.00)
## e[76] optimizer.zero_grad (sum) time: 0.5358304977416992
## e[76]       loss.backward (sum) time: 12.531457424163818
## e[76]      optimizer.step (sum) time: 25.70449948310852
## epoch[76] training(only) time: 63.72400140762329
# Switched to evaluate mode...
Test: [  0/100]	Time  0.190 ( 0.190)	Loss 1.5930e-01 (1.5930e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.052 ( 0.061)	Loss 3.3521e-01 (3.1810e-01)	Acc@1  93.00 ( 92.09)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.048 ( 0.056)	Loss 4.9854e-01 (3.4306e-01)	Acc@1  90.00 ( 91.86)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.048 ( 0.053)	Loss 3.3081e-01 (3.3665e-01)	Acc@1  94.00 ( 92.29)	Acc@5  99.00 ( 99.77)
Test: [ 40/100]	Time  0.048 ( 0.052)	Loss 4.9292e-01 (3.4084e-01)	Acc@1  90.00 ( 92.29)	Acc@5  99.00 ( 99.78)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 2.0874e-01 (3.4428e-01)	Acc@1  95.00 ( 92.29)	Acc@5 100.00 ( 99.80)
Test: [ 60/100]	Time  0.047 ( 0.051)	Loss 2.3230e-01 (3.3572e-01)	Acc@1  93.00 ( 92.31)	Acc@5 100.00 ( 99.82)
Test: [ 70/100]	Time  0.048 ( 0.050)	Loss 3.8843e-01 (3.3765e-01)	Acc@1  92.00 ( 92.31)	Acc@5 100.00 ( 99.82)
Test: [ 80/100]	Time  0.057 ( 0.050)	Loss 2.6904e-01 (3.3629e-01)	Acc@1  93.00 ( 92.38)	Acc@5 100.00 ( 99.83)
Test: [ 90/100]	Time  0.046 ( 0.050)	Loss 2.9028e-01 (3.3720e-01)	Acc@1  93.00 ( 92.35)	Acc@5 100.00 ( 99.84)
 * Acc@1 92.290 Acc@5 99.840
### epoch[76] execution time: 68.75552701950073
EPOCH 77
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [77][  0/391]	Time  0.326 ( 0.326)	Data  0.155 ( 0.155)	Loss 1.9669e-02 (1.9669e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [77][ 10/391]	Time  0.161 ( 0.177)	Data  0.001 ( 0.015)	Loss 1.6922e-02 (1.6743e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [77][ 20/391]	Time  0.164 ( 0.170)	Data  0.001 ( 0.008)	Loss 1.1078e-02 (1.4255e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [77][ 30/391]	Time  0.161 ( 0.167)	Data  0.001 ( 0.006)	Loss 4.0100e-02 (1.4427e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [77][ 40/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.005)	Loss 5.8975e-03 (1.3192e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [77][ 50/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.004)	Loss 8.3466e-03 (1.2882e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [77][ 60/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.004)	Loss 3.8788e-02 (1.4205e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [77][ 70/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.003)	Loss 5.7793e-03 (1.4075e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [77][ 80/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.1322e-02 (1.4424e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [77][ 90/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.5284e-02 (1.4782e-02)	Acc@1  98.44 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [77][100/391]	Time  0.171 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.5778e-02 (1.5109e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [77][110/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 9.7504e-03 (1.4706e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [77][120/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.002)	Loss 3.6011e-03 (1.4399e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [77][130/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.002)	Loss 2.9816e-02 (1.4678e-02)	Acc@1  98.44 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [77][140/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.7863e-02 (1.5332e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [77][150/391]	Time  0.171 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.0035e-02 (1.5509e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [77][160/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1627e-02 (1.5675e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [77][170/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.9466e-03 (1.5576e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [77][180/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.5684e-03 (1.5364e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [77][190/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0696e-02 (1.5305e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [77][200/391]	Time  0.171 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.9874e-03 (1.5374e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [77][210/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.3956e-03 (1.5716e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [77][220/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1009e-02 (1.5659e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [77][230/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.4430e-03 (1.5662e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [77][240/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5854e-02 (1.5797e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [77][250/391]	Time  0.173 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.1635e-03 (1.5774e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [77][260/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7044e-02 (1.5710e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [77][270/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.0043e-03 (1.5776e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [77][280/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.7569e-03 (1.5817e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [77][290/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5915e-02 (1.5871e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [77][300/391]	Time  0.172 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.7427e-03 (1.5827e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [77][310/391]	Time  0.178 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5686e-02 (1.5773e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [77][320/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.7598e-02 (1.5796e-02)	Acc@1  98.44 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [77][330/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.4048e-02 (1.5789e-02)	Acc@1  98.44 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [77][340/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.8951e-02 (1.5845e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [77][350/391]	Time  0.171 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.4698e-02 (1.6060e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [77][360/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.001)	Loss 1.6006e-02 (1.5957e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [77][370/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.001)	Loss 1.4267e-02 (1.5912e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [77][380/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.001)	Loss 1.8051e-02 (1.6055e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [77][390/391]	Time  0.113 ( 0.163)	Data  0.001 ( 0.001)	Loss 3.4485e-02 (1.6059e-02)	Acc@1  98.75 ( 99.58)	Acc@5 100.00 (100.00)
## e[77] optimizer.zero_grad (sum) time: 0.5342471599578857
## e[77]       loss.backward (sum) time: 12.47405481338501
## e[77]      optimizer.step (sum) time: 25.744452953338623
## epoch[77] training(only) time: 63.64779329299927
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 1.5161e-01 (1.5161e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.048 ( 0.060)	Loss 3.1616e-01 (3.1328e-01)	Acc@1  93.00 ( 91.82)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.048 ( 0.055)	Loss 5.3125e-01 (3.3993e-01)	Acc@1  90.00 ( 91.81)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 3.2446e-01 (3.3355e-01)	Acc@1  94.00 ( 92.16)	Acc@5  99.00 ( 99.74)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 5.2051e-01 (3.3934e-01)	Acc@1  89.00 ( 92.07)	Acc@5  98.00 ( 99.71)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 2.1863e-01 (3.4226e-01)	Acc@1  95.00 ( 92.18)	Acc@5 100.00 ( 99.73)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 2.5635e-01 (3.3456e-01)	Acc@1  93.00 ( 92.23)	Acc@5 100.00 ( 99.75)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 3.6597e-01 (3.3803e-01)	Acc@1  93.00 ( 92.18)	Acc@5 100.00 ( 99.76)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 2.6416e-01 (3.3567e-01)	Acc@1  94.00 ( 92.31)	Acc@5 100.00 ( 99.78)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 2.8760e-01 (3.3628e-01)	Acc@1  93.00 ( 92.25)	Acc@5 100.00 ( 99.80)
 * Acc@1 92.190 Acc@5 99.810
### epoch[77] execution time: 68.62466192245483
EPOCH 78
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [78][  0/391]	Time  0.326 ( 0.326)	Data  0.126 ( 0.126)	Loss 3.0731e-02 (3.0731e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [78][ 10/391]	Time  0.161 ( 0.176)	Data  0.001 ( 0.012)	Loss 1.5015e-02 (1.9639e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [78][ 20/391]	Time  0.161 ( 0.170)	Data  0.001 ( 0.007)	Loss 2.1194e-02 (1.8369e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [78][ 30/391]	Time  0.161 ( 0.168)	Data  0.001 ( 0.005)	Loss 1.4908e-02 (1.7007e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [78][ 40/391]	Time  0.161 ( 0.166)	Data  0.001 ( 0.004)	Loss 1.2199e-02 (1.6236e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [78][ 50/391]	Time  0.161 ( 0.166)	Data  0.001 ( 0.004)	Loss 2.0981e-02 (1.6889e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [78][ 60/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.003)	Loss 1.9043e-02 (1.7727e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [78][ 70/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.003)	Loss 4.2458e-03 (1.7367e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [78][ 80/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 3.8116e-02 (1.7226e-02)	Acc@1  98.44 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [78][ 90/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 7.9346e-03 (1.6733e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [78][100/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.002)	Loss 1.6708e-02 (1.6989e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [78][110/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.002)	Loss 1.4229e-02 (1.6909e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [78][120/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.002)	Loss 1.0010e-02 (1.6753e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [78][130/391]	Time  0.165 ( 0.164)	Data  0.001 ( 0.002)	Loss 4.3640e-02 (1.8003e-02)	Acc@1  98.44 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [78][140/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6785e-02 (1.8328e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [78][150/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.1164e-02 (1.8329e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [78][160/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7822e-02 (1.8217e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [78][170/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6937e-02 (1.8134e-02)	Acc@1  98.44 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [78][180/391]	Time  0.175 ( 0.164)	Data  0.001 ( 0.002)	Loss 4.3526e-03 (1.7893e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [78][190/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.2000e-03 (1.7811e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [78][200/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3649e-02 (1.7640e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [78][210/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.5536e-03 (1.7402e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [78][220/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0544e-02 (1.7137e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [78][230/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.3531e-03 (1.6995e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [78][240/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.3834e-02 (1.7161e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [78][250/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2482e-02 (1.7045e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [78][260/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.1820e-02 (1.7010e-02)	Acc@1  98.44 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [78][270/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.8289e-03 (1.6994e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [78][280/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.6359e-03 (1.7168e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [78][290/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.0109e-03 (1.7201e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [78][300/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2497e-02 (1.7265e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [78][310/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1528e-02 (1.7341e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [78][320/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.2397e-03 (1.7120e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [78][330/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.6992e-03 (1.7217e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [78][340/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5671e-02 (1.7285e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [78][350/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.4692e-03 (1.7267e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [78][360/391]	Time  0.165 ( 0.163)	Data  0.001 ( 0.001)	Loss 8.9188e-03 (1.7319e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [78][370/391]	Time  0.180 ( 0.163)	Data  0.001 ( 0.001)	Loss 3.5645e-02 (1.7653e-02)	Acc@1  98.44 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [78][380/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.001)	Loss 2.5040e-02 (1.7557e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [78][390/391]	Time  0.115 ( 0.163)	Data  0.001 ( 0.001)	Loss 1.8250e-02 (1.7701e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
## e[78] optimizer.zero_grad (sum) time: 0.53519606590271
## e[78]       loss.backward (sum) time: 12.464005708694458
## e[78]      optimizer.step (sum) time: 25.75257396697998
## epoch[78] training(only) time: 63.773457765579224
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 1.4709e-01 (1.4709e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 3.0688e-01 (3.1392e-01)	Acc@1  94.00 ( 91.73)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.047 ( 0.054)	Loss 5.1562e-01 (3.3769e-01)	Acc@1  90.00 ( 91.71)	Acc@5 100.00 ( 99.71)
Test: [ 30/100]	Time  0.048 ( 0.052)	Loss 3.2031e-01 (3.3340e-01)	Acc@1  94.00 ( 92.16)	Acc@5  99.00 ( 99.71)
Test: [ 40/100]	Time  0.048 ( 0.051)	Loss 5.1123e-01 (3.3896e-01)	Acc@1  89.00 ( 92.20)	Acc@5  98.00 ( 99.68)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 2.0959e-01 (3.4290e-01)	Acc@1  95.00 ( 92.24)	Acc@5 100.00 ( 99.71)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 2.4231e-01 (3.3425e-01)	Acc@1  93.00 ( 92.26)	Acc@5 100.00 ( 99.74)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 3.7622e-01 (3.3756e-01)	Acc@1  94.00 ( 92.25)	Acc@5 100.00 ( 99.75)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 2.4768e-01 (3.3541e-01)	Acc@1  94.00 ( 92.33)	Acc@5 100.00 ( 99.77)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 2.8882e-01 (3.3604e-01)	Acc@1  92.00 ( 92.27)	Acc@5 100.00 ( 99.78)
 * Acc@1 92.210 Acc@5 99.790
### epoch[78] execution time: 68.75932455062866
EPOCH 79
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [79][  0/391]	Time  0.317 ( 0.317)	Data  0.146 ( 0.146)	Loss 3.4210e-02 (3.4210e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [79][ 10/391]	Time  0.161 ( 0.177)	Data  0.001 ( 0.014)	Loss 2.3010e-02 (1.8831e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [79][ 20/391]	Time  0.161 ( 0.169)	Data  0.001 ( 0.008)	Loss 1.9989e-02 (1.9509e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [79][ 30/391]	Time  0.160 ( 0.167)	Data  0.001 ( 0.006)	Loss 1.1215e-02 (2.1475e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [79][ 40/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.005)	Loss 6.9885e-02 (2.3031e-02)	Acc@1  97.66 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [79][ 50/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.004)	Loss 2.5269e-02 (2.1781e-02)	Acc@1  98.44 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [79][ 60/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.4153e-02 (2.0237e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [79][ 70/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 7.5035e-03 (2.0113e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [79][ 80/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.1383e-02 (2.0134e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [79][ 90/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.6896e-02 (2.0244e-02)	Acc@1  98.44 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [79][100/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 9.0637e-03 (1.9621e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [79][110/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0933e-02 (1.9803e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [79][120/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2177e-02 (1.9303e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [79][130/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6861e-02 (1.9021e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [79][140/391]	Time  0.169 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.1210e-02 (1.9476e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [79][150/391]	Time  0.166 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.7664e-02 (1.9488e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [79][160/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5114e-02 (1.9430e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [79][170/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1032e-02 (1.9609e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [79][180/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.1024e-03 (1.9401e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [79][190/391]	Time  0.168 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3557e-02 (1.9109e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [79][200/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.1378e-02 (1.8745e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [79][210/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.9363e-02 (1.8560e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [79][220/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.5978e-03 (1.8357e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [79][230/391]	Time  0.165 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.3386e-02 (1.8303e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [79][240/391]	Time  0.179 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4633e-02 (1.7949e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [79][250/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.7466e-02 (1.7913e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [79][260/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.8951e-02 (1.7988e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [79][270/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7303e-02 (1.7807e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [79][280/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.6337e-02 (1.7856e-02)	Acc@1  98.44 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [79][290/391]	Time  0.172 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.5923e-03 (1.7877e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [79][300/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.2720e-02 (1.8064e-02)	Acc@1  98.44 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [79][310/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1940e-02 (1.8043e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [79][320/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3702e-02 (1.8040e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [79][330/391]	Time  0.165 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3626e-02 (1.8028e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [79][340/391]	Time  0.169 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6327e-02 (1.8079e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [79][350/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3145e-02 (1.7873e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [79][360/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.0920e-02 (1.7786e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [79][370/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7868e-02 (1.7783e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [79][380/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7075e-02 (1.7716e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [79][390/391]	Time  0.127 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1398e-02 (1.7729e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
## e[79] optimizer.zero_grad (sum) time: 0.5351130962371826
## e[79]       loss.backward (sum) time: 12.469961166381836
## e[79]      optimizer.step (sum) time: 25.722713470458984
## epoch[79] training(only) time: 63.55298113822937
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 1.4563e-01 (1.4563e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.059)	Loss 3.1396e-01 (3.1553e-01)	Acc@1  94.00 ( 92.09)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.058 ( 0.054)	Loss 5.1953e-01 (3.4218e-01)	Acc@1  90.00 ( 91.67)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.047 ( 0.051)	Loss 3.2422e-01 (3.3657e-01)	Acc@1  92.00 ( 91.97)	Acc@5  99.00 ( 99.77)
Test: [ 40/100]	Time  0.047 ( 0.050)	Loss 5.1172e-01 (3.4226e-01)	Acc@1  89.00 ( 92.02)	Acc@5  98.00 ( 99.73)
Test: [ 50/100]	Time  0.048 ( 0.050)	Loss 2.1643e-01 (3.4531e-01)	Acc@1  95.00 ( 92.18)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.047 ( 0.049)	Loss 2.3682e-01 (3.3718e-01)	Acc@1  94.00 ( 92.25)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 3.7183e-01 (3.3995e-01)	Acc@1  93.00 ( 92.23)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 2.5146e-01 (3.3761e-01)	Acc@1  94.00 ( 92.33)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 2.9004e-01 (3.3767e-01)	Acc@1  92.00 ( 92.31)	Acc@5 100.00 ( 99.82)
 * Acc@1 92.280 Acc@5 99.830
### epoch[79] execution time: 68.50262117385864
EPOCH 80
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [80][  0/391]	Time  0.336 ( 0.336)	Data  0.127 ( 0.127)	Loss 2.4368e-02 (2.4368e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [80][ 10/391]	Time  0.162 ( 0.177)	Data  0.001 ( 0.013)	Loss 1.0117e-02 (1.5744e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [80][ 20/391]	Time  0.160 ( 0.170)	Data  0.001 ( 0.007)	Loss 4.0710e-02 (1.3973e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [80][ 30/391]	Time  0.160 ( 0.167)	Data  0.001 ( 0.005)	Loss 9.3079e-03 (1.4851e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [80][ 40/391]	Time  0.161 ( 0.166)	Data  0.001 ( 0.004)	Loss 2.8488e-02 (1.5457e-02)	Acc@1  98.44 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [80][ 50/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 9.7122e-03 (1.5922e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [80][ 60/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.003)	Loss 1.3268e-02 (1.5606e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [80][ 70/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.4811e-02 (1.5667e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [80][ 80/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.3245e-02 (1.6149e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [80][ 90/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 4.6356e-02 (1.6230e-02)	Acc@1  97.66 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [80][100/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.002)	Loss 1.1169e-02 (1.6332e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [80][110/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.002)	Loss 5.6732e-02 (1.6366e-02)	Acc@1  97.66 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [80][120/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.6150e-03 (1.6469e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [80][130/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.9928e-02 (1.6975e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [80][140/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.0043e-03 (1.7292e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [80][150/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.0719e-03 (1.7139e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [80][160/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.8027e-03 (1.7131e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [80][170/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.0314e-02 (1.7215e-02)	Acc@1  98.44 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [80][180/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.1487e-03 (1.7270e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [80][190/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1635e-02 (1.7001e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [80][200/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.8572e-03 (1.7019e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [80][210/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.1097e-02 (1.6892e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [80][220/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.3531e-03 (1.6985e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [80][230/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1292e-02 (1.7059e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [80][240/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.3558e-03 (1.6990e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [80][250/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.5291e-03 (1.6923e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [80][260/391]	Time  0.165 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.4577e-03 (1.6876e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [80][270/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.4240e-03 (1.6636e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [80][280/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2436e-02 (1.6573e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [80][290/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.3447e-02 (1.6511e-02)	Acc@1  98.44 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [80][300/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.8904e-03 (1.6589e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [80][310/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5961e-02 (1.6575e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [80][320/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0757e-02 (1.6700e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [80][330/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1673e-02 (1.6760e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [80][340/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.8666e-02 (1.6864e-02)	Acc@1  97.66 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [80][350/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.5156e-02 (1.6923e-02)	Acc@1  97.66 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [80][360/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.001)	Loss 2.5421e-02 (1.6903e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [80][370/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.001)	Loss 8.5983e-03 (1.7048e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [80][380/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.001)	Loss 1.7548e-02 (1.7053e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [80][390/391]	Time  0.120 ( 0.162)	Data  0.001 ( 0.001)	Loss 5.2567e-03 (1.7007e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
## e[80] optimizer.zero_grad (sum) time: 0.5252363681793213
## e[80]       loss.backward (sum) time: 12.465604543685913
## e[80]      optimizer.step (sum) time: 25.741560697555542
## epoch[80] training(only) time: 63.586636781692505
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 1.7395e-01 (1.7395e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.059)	Loss 3.2202e-01 (3.1687e-01)	Acc@1  92.00 ( 91.91)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 5.1904e-01 (3.4251e-01)	Acc@1  90.00 ( 91.95)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.047 ( 0.051)	Loss 3.3911e-01 (3.3508e-01)	Acc@1  94.00 ( 92.32)	Acc@5  99.00 ( 99.77)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 5.0684e-01 (3.4135e-01)	Acc@1  90.00 ( 92.29)	Acc@5  98.00 ( 99.73)
Test: [ 50/100]	Time  0.048 ( 0.050)	Loss 2.2888e-01 (3.4323e-01)	Acc@1  95.00 ( 92.29)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 2.6367e-01 (3.3636e-01)	Acc@1  92.00 ( 92.28)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 3.6206e-01 (3.3838e-01)	Acc@1  93.00 ( 92.28)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 2.6660e-01 (3.3686e-01)	Acc@1  94.00 ( 92.33)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 3.1567e-01 (3.3675e-01)	Acc@1  92.00 ( 92.34)	Acc@5 100.00 ( 99.81)
 * Acc@1 92.240 Acc@5 99.820
### epoch[80] execution time: 68.54557728767395
EPOCH 81
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [81][  0/391]	Time  0.329 ( 0.329)	Data  0.150 ( 0.150)	Loss 1.0536e-02 (1.0536e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [81][ 10/391]	Time  0.162 ( 0.177)	Data  0.001 ( 0.015)	Loss 2.0615e-02 (2.0545e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [81][ 20/391]	Time  0.160 ( 0.169)	Data  0.001 ( 0.008)	Loss 9.2010e-03 (2.0331e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [81][ 30/391]	Time  0.162 ( 0.167)	Data  0.001 ( 0.006)	Loss 2.2522e-02 (2.0195e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [81][ 40/391]	Time  0.161 ( 0.166)	Data  0.001 ( 0.005)	Loss 1.9104e-02 (1.9674e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [81][ 50/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.0735e-02 (1.9239e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [81][ 60/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.3649e-02 (1.8700e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [81][ 70/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.003)	Loss 5.9509e-03 (1.8771e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [81][ 80/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 4.4922e-02 (1.8327e-02)	Acc@1  98.44 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [81][ 90/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 4.6082e-03 (1.8059e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [81][100/391]	Time  0.164 ( 0.164)	Data  0.001 ( 0.003)	Loss 8.4991e-03 (1.8088e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [81][110/391]	Time  0.163 ( 0.164)	Data  0.001 ( 0.002)	Loss 1.3351e-02 (1.8828e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [81][120/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3763e-02 (1.8796e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [81][130/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.4986e-03 (1.8186e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [81][140/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.4943e-02 (1.8249e-02)	Acc@1  98.44 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [81][150/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6022e-02 (1.8378e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [81][160/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4282e-02 (1.8746e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [81][170/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.9165e-02 (1.8452e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [81][180/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0361e-02 (1.8192e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [81][190/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.9836e-02 (1.8090e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [81][200/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.1805e-02 (1.7922e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [81][210/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.4883e-03 (1.8109e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [81][220/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.6489e-02 (1.8159e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [81][230/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.5912e-03 (1.7953e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [81][240/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.0106e-02 (1.7827e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [81][250/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.9684e-02 (1.7876e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [81][260/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.1107e-02 (1.7994e-02)	Acc@1  98.44 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [81][270/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2367e-02 (1.8151e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [81][280/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.8327e-03 (1.7910e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [81][290/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.4490e-02 (1.7853e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [81][300/391]	Time  0.174 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.1024e-03 (1.7699e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [81][310/391]	Time  0.167 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.5711e-02 (1.7743e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [81][320/391]	Time  0.165 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.8648e-03 (1.7813e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [81][330/391]	Time  0.175 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.6910e-03 (1.7979e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [81][340/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7563e-02 (1.8252e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [81][350/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.5726e-02 (1.8281e-02)	Acc@1  98.44 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [81][360/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5656e-02 (1.8111e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [81][370/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0780e-02 (1.8143e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [81][380/391]	Time  0.180 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6663e-02 (1.8041e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [81][390/391]	Time  0.115 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.6642e-02 (1.8059e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
## e[81] optimizer.zero_grad (sum) time: 0.529038667678833
## e[81]       loss.backward (sum) time: 12.497757911682129
## e[81]      optimizer.step (sum) time: 25.68238139152527
## epoch[81] training(only) time: 63.629589319229126
# Switched to evaluate mode...
Test: [  0/100]	Time  0.175 ( 0.175)	Loss 1.5149e-01 (1.5149e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.059)	Loss 3.1885e-01 (3.1454e-01)	Acc@1  94.00 ( 92.36)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.047 ( 0.053)	Loss 4.8999e-01 (3.3820e-01)	Acc@1  90.00 ( 92.14)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 3.2642e-01 (3.3515e-01)	Acc@1  93.00 ( 92.26)	Acc@5  99.00 ( 99.77)
Test: [ 40/100]	Time  0.047 ( 0.050)	Loss 4.9341e-01 (3.3917e-01)	Acc@1  89.00 ( 92.20)	Acc@5  98.00 ( 99.73)
Test: [ 50/100]	Time  0.055 ( 0.050)	Loss 1.9629e-01 (3.4207e-01)	Acc@1  95.00 ( 92.24)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.047 ( 0.049)	Loss 2.2302e-01 (3.3398e-01)	Acc@1  93.00 ( 92.21)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 3.7207e-01 (3.3585e-01)	Acc@1  94.00 ( 92.23)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 2.6318e-01 (3.3424e-01)	Acc@1  94.00 ( 92.30)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 3.1396e-01 (3.3496e-01)	Acc@1  92.00 ( 92.26)	Acc@5 100.00 ( 99.82)
 * Acc@1 92.190 Acc@5 99.830
### epoch[81] execution time: 68.5725793838501
EPOCH 82
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [82][  0/391]	Time  0.309 ( 0.309)	Data  0.138 ( 0.138)	Loss 1.1925e-02 (1.1925e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [82][ 10/391]	Time  0.173 ( 0.176)	Data  0.001 ( 0.014)	Loss 1.5320e-02 (1.8275e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [82][ 20/391]	Time  0.160 ( 0.169)	Data  0.001 ( 0.008)	Loss 2.8000e-02 (1.8362e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [82][ 30/391]	Time  0.161 ( 0.167)	Data  0.001 ( 0.006)	Loss 1.7807e-02 (2.0441e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [82][ 40/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.005)	Loss 1.4725e-02 (2.0824e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [82][ 50/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.0223e-02 (2.0380e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [82][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.1011e-02 (1.9549e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [82][ 70/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.7563e-02 (1.8999e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [82][ 80/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.9001e-02 (1.8914e-02)	Acc@1  98.44 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [82][ 90/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.2161e-02 (1.8210e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [82][100/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 4.1290e-02 (1.8158e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [82][110/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.8661e-02 (1.8877e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [82][120/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.3678e-03 (1.8669e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [82][130/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.6207e-03 (1.8605e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [82][140/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2466e-02 (1.8540e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [82][150/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.6338e-03 (1.8145e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [82][160/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.7981e-03 (1.7708e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [82][170/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.3825e-03 (1.7525e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [82][180/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.0643e-03 (1.7479e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [82][190/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.0140e-02 (1.7622e-02)	Acc@1  98.44 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [82][200/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.0889e-02 (1.7609e-02)	Acc@1  98.44 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [82][210/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3458e-02 (1.7914e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [82][220/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.0948e-03 (1.7632e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [82][230/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.2479e-03 (1.7421e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [82][240/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.5618e-03 (1.7636e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [82][250/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.4621e-03 (1.7574e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [82][260/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.7115e-02 (1.7580e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [82][270/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3573e-02 (1.7456e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [82][280/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.0948e-03 (1.7525e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [82][290/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.4342e-02 (1.7468e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [82][300/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.6836e-03 (1.7348e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [82][310/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.1383e-02 (1.7342e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [82][320/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 3.0579e-02 (1.7676e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [82][330/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.6083e-02 (1.7865e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [82][340/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.8112e-02 (1.7895e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [82][350/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.1810e-02 (1.7681e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [82][360/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.001)	Loss 5.4840e-02 (1.7867e-02)	Acc@1  97.66 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [82][370/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 8.8806e-03 (1.7790e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [82][380/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.5320e-02 (1.7766e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [82][390/391]	Time  0.116 ( 0.162)	Data  0.001 ( 0.001)	Loss 4.7607e-02 (1.7729e-02)	Acc@1  97.50 ( 99.48)	Acc@5 100.00 (100.00)
## e[82] optimizer.zero_grad (sum) time: 0.5238211154937744
## e[82]       loss.backward (sum) time: 12.462645769119263
## e[82]      optimizer.step (sum) time: 25.72019910812378
## epoch[82] training(only) time: 63.48318529129028
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 1.5894e-01 (1.5894e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 3.2520e-01 (3.1534e-01)	Acc@1  93.00 ( 91.64)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.047 ( 0.054)	Loss 5.3174e-01 (3.4109e-01)	Acc@1  90.00 ( 91.71)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.048 ( 0.052)	Loss 3.2373e-01 (3.3572e-01)	Acc@1  93.00 ( 92.03)	Acc@5  99.00 ( 99.77)
Test: [ 40/100]	Time  0.048 ( 0.051)	Loss 5.0098e-01 (3.4082e-01)	Acc@1  89.00 ( 92.02)	Acc@5  98.00 ( 99.73)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 2.0642e-01 (3.4384e-01)	Acc@1  95.00 ( 92.12)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 2.5635e-01 (3.3527e-01)	Acc@1  93.00 ( 92.23)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 3.7500e-01 (3.3761e-01)	Acc@1  93.00 ( 92.20)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 2.5317e-01 (3.3582e-01)	Acc@1  94.00 ( 92.30)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.049 ( 0.049)	Loss 3.0005e-01 (3.3672e-01)	Acc@1  92.00 ( 92.25)	Acc@5 100.00 ( 99.82)
 * Acc@1 92.220 Acc@5 99.830
### epoch[82] execution time: 68.4782543182373
EPOCH 83
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [83][  0/391]	Time  0.323 ( 0.323)	Data  0.148 ( 0.148)	Loss 9.7122e-03 (9.7122e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [83][ 10/391]	Time  0.162 ( 0.177)	Data  0.001 ( 0.014)	Loss 3.1311e-02 (1.5870e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [83][ 20/391]	Time  0.161 ( 0.169)	Data  0.001 ( 0.008)	Loss 2.9510e-02 (1.6535e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [83][ 30/391]	Time  0.160 ( 0.167)	Data  0.001 ( 0.006)	Loss 1.2924e-02 (1.6086e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [83][ 40/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.005)	Loss 8.1406e-03 (1.6307e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [83][ 50/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 3.0613e-03 (1.6568e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [83][ 60/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.8524e-02 (1.7158e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [83][ 70/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.7288e-02 (1.6956e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [83][ 80/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.003)	Loss 9.8877e-03 (1.7901e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [83][ 90/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.3290e-02 (1.7402e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [83][100/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.3453e-02 (1.6887e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [83][110/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.1765e-03 (1.6667e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [83][120/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.9379e-02 (1.6677e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [83][130/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.0627e-03 (1.6975e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [83][140/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0910e-02 (1.6806e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [83][150/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.9280e-03 (1.6494e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [83][160/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.0103e-03 (1.6521e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [83][170/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1726e-02 (1.6549e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [83][180/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.9331e-03 (1.6575e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [83][190/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4923e-02 (1.6564e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [83][200/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.5223e-02 (1.6595e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [83][210/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.8676e-02 (1.6822e-02)	Acc@1  98.44 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [83][220/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6846e-02 (1.6847e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [83][230/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6205e-02 (1.6722e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [83][240/391]	Time  0.166 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.3875e-02 (1.6683e-02)	Acc@1  98.44 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [83][250/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.6180e-03 (1.6817e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [83][260/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3969e-02 (1.6761e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [83][270/391]	Time  0.172 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.3280e-03 (1.6482e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [83][280/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.9880e-03 (1.6729e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [83][290/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.8038e-03 (1.6804e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [83][300/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.0261e-03 (1.6709e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [83][310/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.7974e-02 (1.6723e-02)	Acc@1  98.44 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [83][320/391]	Time  0.173 ( 0.163)	Data  0.002 ( 0.002)	Loss 1.8539e-02 (1.6701e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [83][330/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.4912e-02 (1.6736e-02)	Acc@1  98.44 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [83][340/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.8937e-03 (1.6901e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [83][350/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4359e-02 (1.7106e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [83][360/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.8735e-03 (1.7057e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [83][370/391]	Time  0.180 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.3544e-02 (1.6968e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [83][380/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.6817e-03 (1.6806e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [83][390/391]	Time  0.115 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.5928e-02 (1.6840e-02)	Acc@1  97.50 ( 99.52)	Acc@5 100.00 (100.00)
## e[83] optimizer.zero_grad (sum) time: 0.5320172309875488
## e[83]       loss.backward (sum) time: 12.421689510345459
## e[83]      optimizer.step (sum) time: 25.742106199264526
## epoch[83] training(only) time: 63.61097240447998
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 1.6052e-01 (1.6052e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.060)	Loss 3.3398e-01 (3.0545e-01)	Acc@1  92.00 ( 92.36)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.047 ( 0.054)	Loss 4.8657e-01 (3.3347e-01)	Acc@1  90.00 ( 91.86)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.048 ( 0.052)	Loss 3.1885e-01 (3.2954e-01)	Acc@1  94.00 ( 92.26)	Acc@5  99.00 ( 99.77)
Test: [ 40/100]	Time  0.048 ( 0.052)	Loss 4.9805e-01 (3.3488e-01)	Acc@1  90.00 ( 92.29)	Acc@5  99.00 ( 99.76)
Test: [ 50/100]	Time  0.048 ( 0.051)	Loss 1.9385e-01 (3.3775e-01)	Acc@1  95.00 ( 92.33)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.048 ( 0.051)	Loss 2.3621e-01 (3.3080e-01)	Acc@1  93.00 ( 92.33)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.048 ( 0.050)	Loss 3.9795e-01 (3.3391e-01)	Acc@1  93.00 ( 92.32)	Acc@5 100.00 ( 99.80)
Test: [ 80/100]	Time  0.056 ( 0.050)	Loss 2.7344e-01 (3.3263e-01)	Acc@1  93.00 ( 92.42)	Acc@5 100.00 ( 99.81)
Test: [ 90/100]	Time  0.047 ( 0.050)	Loss 2.9248e-01 (3.3360e-01)	Acc@1  93.00 ( 92.40)	Acc@5 100.00 ( 99.84)
 * Acc@1 92.310 Acc@5 99.840
### epoch[83] execution time: 68.64546179771423
EPOCH 84
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [84][  0/391]	Time  0.340 ( 0.340)	Data  0.153 ( 0.153)	Loss 1.4687e-02 (1.4687e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [84][ 10/391]	Time  0.161 ( 0.177)	Data  0.001 ( 0.015)	Loss 1.9333e-02 (1.6934e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [84][ 20/391]	Time  0.161 ( 0.170)	Data  0.001 ( 0.008)	Loss 3.4981e-03 (1.8813e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [84][ 30/391]	Time  0.160 ( 0.168)	Data  0.001 ( 0.006)	Loss 2.7649e-02 (1.9561e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [84][ 40/391]	Time  0.161 ( 0.166)	Data  0.001 ( 0.005)	Loss 1.6342e-02 (1.9153e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [84][ 50/391]	Time  0.171 ( 0.166)	Data  0.001 ( 0.004)	Loss 3.0609e-02 (1.8935e-02)	Acc@1  98.44 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [84][ 60/391]	Time  0.166 ( 0.165)	Data  0.001 ( 0.004)	Loss 3.9917e-02 (1.8898e-02)	Acc@1  98.44 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [84][ 70/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.003)	Loss 1.1398e-02 (1.8207e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [84][ 80/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.003)	Loss 8.0948e-03 (1.8018e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [84][ 90/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 5.9052e-03 (1.7523e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [84][100/391]	Time  0.174 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.7191e-02 (1.7330e-02)	Acc@1  98.44 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [84][110/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 3.3966e-02 (1.7328e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [84][120/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.002)	Loss 1.2901e-02 (1.7085e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [84][130/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.002)	Loss 1.2650e-02 (1.6640e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [84][140/391]	Time  0.164 ( 0.164)	Data  0.001 ( 0.002)	Loss 1.2779e-02 (1.6508e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [84][150/391]	Time  0.171 ( 0.164)	Data  0.001 ( 0.002)	Loss 1.1139e-02 (1.6562e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [84][160/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.002)	Loss 1.0582e-02 (1.6798e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [84][170/391]	Time  0.165 ( 0.164)	Data  0.001 ( 0.002)	Loss 1.0231e-02 (1.6999e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [84][180/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.002)	Loss 1.8524e-02 (1.6971e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [84][190/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6449e-02 (1.6992e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [84][200/391]	Time  0.171 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.5578e-03 (1.7078e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [84][210/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.8959e-03 (1.6871e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [84][220/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.8997e-02 (1.6948e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [84][230/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.4983e-02 (1.7072e-02)	Acc@1  97.66 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [84][240/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.0807e-02 (1.7119e-02)	Acc@1  98.44 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [84][250/391]	Time  0.170 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.3142e-02 (1.7252e-02)	Acc@1  98.44 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [84][260/391]	Time  0.176 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.5264e-03 (1.7132e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [84][270/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6159e-02 (1.7188e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [84][280/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4961e-02 (1.7192e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [84][290/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4824e-02 (1.7389e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [84][300/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6891e-02 (1.7448e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [84][310/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.4952e-02 (1.7769e-02)	Acc@1  98.44 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [84][320/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6724e-02 (1.7675e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [84][330/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.3967e-03 (1.7518e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [84][340/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.7084e-02 (1.7654e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [84][350/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.9816e-02 (1.7727e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [84][360/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.4302e-02 (1.7695e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [84][370/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.9856e-02 (1.7639e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [84][380/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.7024e-03 (1.7462e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [84][390/391]	Time  0.117 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1978e-02 (1.7369e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
## e[84] optimizer.zero_grad (sum) time: 0.5362918376922607
## e[84]       loss.backward (sum) time: 12.529686212539673
## e[84]      optimizer.step (sum) time: 25.70604920387268
## epoch[84] training(only) time: 63.68750023841858
# Switched to evaluate mode...
Test: [  0/100]	Time  0.190 ( 0.190)	Loss 1.5027e-01 (1.5027e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.059)	Loss 3.1567e-01 (3.1743e-01)	Acc@1  93.00 ( 92.18)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.048 ( 0.054)	Loss 5.2246e-01 (3.4134e-01)	Acc@1  90.00 ( 91.90)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 3.2104e-01 (3.3284e-01)	Acc@1  93.00 ( 92.16)	Acc@5  99.00 ( 99.77)
Test: [ 40/100]	Time  0.048 ( 0.051)	Loss 5.1904e-01 (3.3973e-01)	Acc@1  89.00 ( 92.10)	Acc@5  99.00 ( 99.76)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 2.1814e-01 (3.4240e-01)	Acc@1  95.00 ( 92.08)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 2.5830e-01 (3.3455e-01)	Acc@1  92.00 ( 92.16)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 3.4082e-01 (3.3678e-01)	Acc@1  94.00 ( 92.17)	Acc@5 100.00 ( 99.80)
Test: [ 80/100]	Time  0.049 ( 0.049)	Loss 2.6562e-01 (3.3401e-01)	Acc@1  94.00 ( 92.31)	Acc@5 100.00 ( 99.81)
Test: [ 90/100]	Time  0.048 ( 0.049)	Loss 3.0127e-01 (3.3478e-01)	Acc@1  93.00 ( 92.22)	Acc@5 100.00 ( 99.84)
 * Acc@1 92.170 Acc@5 99.840
### epoch[84] execution time: 68.67280173301697
EPOCH 85
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [85][  0/391]	Time  0.316 ( 0.316)	Data  0.140 ( 0.140)	Loss 1.9653e-02 (1.9653e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [85][ 10/391]	Time  0.161 ( 0.175)	Data  0.001 ( 0.014)	Loss 5.3329e-03 (1.5355e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [85][ 20/391]	Time  0.162 ( 0.169)	Data  0.001 ( 0.008)	Loss 1.6586e-02 (1.6130e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [85][ 30/391]	Time  0.161 ( 0.167)	Data  0.001 ( 0.006)	Loss 9.8953e-03 (1.9463e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [85][ 40/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.005)	Loss 7.0679e-02 (1.9655e-02)	Acc@1  98.44 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [85][ 50/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.4404e-02 (1.9193e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [85][ 60/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.003)	Loss 6.5308e-02 (2.0573e-02)	Acc@1  96.88 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [85][ 70/391]	Time  0.162 ( 0.165)	Data  0.001 ( 0.003)	Loss 3.6907e-03 (1.9798e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [85][ 80/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.003)	Loss 1.0254e-02 (1.9599e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [85][ 90/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.003)	Loss 1.3458e-02 (1.9227e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [85][100/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 4.6082e-03 (1.9395e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [85][110/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.002)	Loss 3.1403e-02 (1.9412e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [85][120/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.002)	Loss 1.3489e-02 (1.9119e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [85][130/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.002)	Loss 2.8656e-02 (1.8790e-02)	Acc@1  98.44 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [85][140/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.002)	Loss 5.4588e-03 (1.8579e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [85][150/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.002)	Loss 3.3142e-02 (1.8427e-02)	Acc@1  97.66 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [85][160/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.002)	Loss 2.0691e-02 (1.8212e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [85][170/391]	Time  0.164 ( 0.164)	Data  0.001 ( 0.002)	Loss 6.6719e-03 (1.8034e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [85][180/391]	Time  0.163 ( 0.164)	Data  0.001 ( 0.002)	Loss 1.2802e-02 (1.8348e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [85][190/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7151e-02 (1.8282e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [85][200/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4977e-02 (1.8053e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [85][210/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.6512e-03 (1.8226e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [85][220/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2360e-02 (1.8237e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [85][230/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.7068e-02 (1.8304e-02)	Acc@1  97.66 ( 99.38)	Acc@5  99.22 (100.00)
Epoch: [85][240/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.3923e-03 (1.8175e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [85][250/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1391e-02 (1.8082e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [85][260/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.0905e-02 (1.8367e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [85][270/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1261e-02 (1.8410e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [85][280/391]	Time  0.166 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.9455e-02 (1.8473e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 ( 99.99)
Epoch: [85][290/391]	Time  0.165 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.8512e-03 (1.8282e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 ( 99.99)
Epoch: [85][300/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.9073e-02 (1.8238e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 ( 99.99)
Epoch: [85][310/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5541e-02 (1.8036e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 ( 99.99)
Epoch: [85][320/391]	Time  0.169 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.0267e-03 (1.7903e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [85][330/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3741e-02 (1.7903e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [85][340/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.9651e-03 (1.7714e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [85][350/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.3575e-02 (1.7937e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [85][360/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2054e-02 (1.7882e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [85][370/391]	Time  0.171 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.5357e-03 (1.7838e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [85][380/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.4452e-03 (1.7721e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [85][390/391]	Time  0.118 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.9928e-02 (1.7682e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
## e[85] optimizer.zero_grad (sum) time: 0.5370354652404785
## e[85]       loss.backward (sum) time: 12.420605897903442
## e[85]      optimizer.step (sum) time: 25.765382289886475
## epoch[85] training(only) time: 63.72748565673828
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 1.4819e-01 (1.4819e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.048 ( 0.062)	Loss 3.1958e-01 (3.1123e-01)	Acc@1  94.00 ( 92.27)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.048 ( 0.055)	Loss 5.0635e-01 (3.3439e-01)	Acc@1  90.00 ( 91.90)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.048 ( 0.054)	Loss 3.2324e-01 (3.2986e-01)	Acc@1  94.00 ( 92.19)	Acc@5  99.00 ( 99.71)
Test: [ 40/100]	Time  0.046 ( 0.052)	Loss 4.8706e-01 (3.3498e-01)	Acc@1  90.00 ( 92.24)	Acc@5  99.00 ( 99.71)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 2.2363e-01 (3.3869e-01)	Acc@1  95.00 ( 92.31)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 2.3694e-01 (3.3086e-01)	Acc@1  94.00 ( 92.30)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 3.7061e-01 (3.3439e-01)	Acc@1  94.00 ( 92.27)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 2.4500e-01 (3.3258e-01)	Acc@1  94.00 ( 92.38)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 2.9126e-01 (3.3353e-01)	Acc@1  93.00 ( 92.36)	Acc@5 100.00 ( 99.80)
 * Acc@1 92.300 Acc@5 99.810
### epoch[85] execution time: 68.71537780761719
EPOCH 86
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [86][  0/391]	Time  0.324 ( 0.324)	Data  0.144 ( 0.144)	Loss 7.6447e-03 (7.6447e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [86][ 10/391]	Time  0.160 ( 0.177)	Data  0.001 ( 0.014)	Loss 2.3041e-02 (2.3851e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [86][ 20/391]	Time  0.161 ( 0.169)	Data  0.001 ( 0.008)	Loss 3.2272e-03 (1.9727e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [86][ 30/391]	Time  0.160 ( 0.167)	Data  0.001 ( 0.006)	Loss 1.8402e-02 (1.7669e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [86][ 40/391]	Time  0.172 ( 0.166)	Data  0.001 ( 0.004)	Loss 1.5419e-02 (1.7100e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [86][ 50/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.004)	Loss 2.3804e-02 (1.7821e-02)	Acc@1  98.44 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [86][ 60/391]	Time  0.162 ( 0.165)	Data  0.001 ( 0.003)	Loss 7.1411e-03 (1.6669e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [86][ 70/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 9.2926e-03 (1.6812e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [86][ 80/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.003)	Loss 9.0790e-03 (1.6986e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 ( 99.99)
Epoch: [86][ 90/391]	Time  0.173 ( 0.164)	Data  0.001 ( 0.003)	Loss 6.1111e-03 (1.6558e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 ( 99.99)
Epoch: [86][100/391]	Time  0.163 ( 0.164)	Data  0.001 ( 0.002)	Loss 2.0905e-02 (1.6334e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 ( 99.99)
Epoch: [86][110/391]	Time  0.163 ( 0.164)	Data  0.001 ( 0.002)	Loss 2.0615e-02 (1.6222e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 ( 99.99)
Epoch: [86][120/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.0651e-03 (1.5927e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 ( 99.99)
Epoch: [86][130/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4252e-02 (1.6511e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 ( 99.99)
Epoch: [86][140/391]	Time  0.173 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.9324e-03 (1.6664e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 ( 99.99)
Epoch: [86][150/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2115e-02 (1.6616e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 ( 99.99)
Epoch: [86][160/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.0436e-03 (1.6714e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [86][170/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.4321e-03 (1.6701e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [86][180/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3298e-02 (1.6650e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [86][190/391]	Time  0.177 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.3259e-03 (1.6838e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [86][200/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2100e-02 (1.6673e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [86][210/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.9479e-02 (1.6940e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [86][220/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.9669e-02 (1.6992e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [86][230/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.6810e-02 (1.6879e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [86][240/391]	Time  0.171 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.1553e-03 (1.6936e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [86][250/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2650e-02 (1.6912e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [86][260/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5266e-02 (1.6949e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [86][270/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.6616e-03 (1.6644e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [86][280/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.8177e-02 (1.6484e-02)	Acc@1  98.44 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [86][290/391]	Time  0.171 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.9667e-03 (1.6356e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [86][300/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6251e-02 (1.6430e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [86][310/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.0920e-02 (1.6434e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [86][320/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3809e-03 (1.6367e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [86][330/391]	Time  0.166 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2230e-02 (1.6380e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [86][340/391]	Time  0.168 ( 0.163)	Data  0.001 ( 0.001)	Loss 1.0384e-02 (1.6274e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [86][350/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.001)	Loss 2.6794e-02 (1.6242e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [86][360/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.001)	Loss 1.4099e-02 (1.6207e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [86][370/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.001)	Loss 6.8932e-03 (1.6028e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [86][380/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.001)	Loss 1.7410e-02 (1.6175e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [86][390/391]	Time  0.118 ( 0.163)	Data  0.001 ( 0.001)	Loss 1.6953e-02 (1.6132e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
## e[86] optimizer.zero_grad (sum) time: 0.5379848480224609
## e[86]       loss.backward (sum) time: 12.478767156600952
## e[86]      optimizer.step (sum) time: 25.730387687683105
## epoch[86] training(only) time: 63.64488172531128
# Switched to evaluate mode...
Test: [  0/100]	Time  0.192 ( 0.192)	Loss 1.4771e-01 (1.4771e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.048 ( 0.061)	Loss 3.1323e-01 (3.1027e-01)	Acc@1  93.00 ( 91.91)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 4.9585e-01 (3.3500e-01)	Acc@1  90.00 ( 91.86)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 3.0664e-01 (3.3025e-01)	Acc@1  94.00 ( 92.26)	Acc@5  99.00 ( 99.74)
Test: [ 40/100]	Time  0.057 ( 0.051)	Loss 5.0586e-01 (3.3512e-01)	Acc@1  89.00 ( 92.24)	Acc@5  99.00 ( 99.73)
Test: [ 50/100]	Time  0.048 ( 0.051)	Loss 1.9104e-01 (3.3849e-01)	Acc@1  95.00 ( 92.27)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 2.4170e-01 (3.3062e-01)	Acc@1  93.00 ( 92.31)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 4.0015e-01 (3.3374e-01)	Acc@1  93.00 ( 92.32)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 2.7246e-01 (3.3253e-01)	Acc@1  93.00 ( 92.43)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 3.1274e-01 (3.3375e-01)	Acc@1  92.00 ( 92.36)	Acc@5 100.00 ( 99.81)
 * Acc@1 92.290 Acc@5 99.820
### epoch[86] execution time: 68.65471601486206
EPOCH 87
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [87][  0/391]	Time  0.322 ( 0.322)	Data  0.136 ( 0.136)	Loss 6.3629e-03 (6.3629e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [87][ 10/391]	Time  0.161 ( 0.178)	Data  0.001 ( 0.013)	Loss 1.7151e-02 (1.3613e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [87][ 20/391]	Time  0.161 ( 0.170)	Data  0.001 ( 0.008)	Loss 2.7237e-02 (1.5622e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [87][ 30/391]	Time  0.161 ( 0.167)	Data  0.001 ( 0.006)	Loss 2.9388e-02 (1.6122e-02)	Acc@1  98.44 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [87][ 40/391]	Time  0.165 ( 0.166)	Data  0.001 ( 0.004)	Loss 5.4703e-03 (1.6517e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [87][ 50/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 7.4806e-03 (1.6666e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [87][ 60/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.003)	Loss 2.1835e-02 (1.8445e-02)	Acc@1  98.44 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [87][ 70/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 4.5441e-02 (1.9581e-02)	Acc@1  96.88 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [87][ 80/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.8402e-02 (1.8572e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [87][ 90/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 8.2626e-03 (1.8132e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [87][100/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.0773e-02 (1.8205e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [87][110/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4282e-02 (1.8473e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [87][120/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6220e-02 (1.8733e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [87][130/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0620e-02 (1.8399e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [87][140/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.3869e-03 (1.8152e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [87][150/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4801e-02 (1.8171e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [87][160/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.9553e-03 (1.7951e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [87][170/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7807e-02 (1.8197e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [87][180/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6022e-02 (1.7814e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [87][190/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.1586e-02 (1.7667e-02)	Acc@1  98.44 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [87][200/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.2686e-03 (1.7295e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [87][210/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.0228e-02 (1.7484e-02)	Acc@1  98.44 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [87][220/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.1024e-03 (1.7835e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [87][230/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7441e-02 (1.7789e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [87][240/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.2415e-02 (1.7774e-02)	Acc@1  98.44 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [87][250/391]	Time  0.166 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.2217e-02 (1.7592e-02)	Acc@1  98.44 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [87][260/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.0081e-03 (1.7516e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [87][270/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.4882e-02 (1.7572e-02)	Acc@1  98.44 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [87][280/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.4668e-02 (1.7605e-02)	Acc@1  98.44 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [87][290/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.3773e-02 (1.7640e-02)	Acc@1  98.44 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [87][300/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.2888e-02 (1.7725e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [87][310/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.001)	Loss 1.5198e-02 (1.7734e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [87][320/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.001)	Loss 6.9771e-03 (1.7840e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [87][330/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.001)	Loss 1.2627e-02 (1.7711e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [87][340/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.001)	Loss 3.6564e-03 (1.7744e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [87][350/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.001)	Loss 1.3977e-02 (1.7740e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [87][360/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.6556e-02 (1.7671e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [87][370/391]	Time  0.178 ( 0.163)	Data  0.001 ( 0.001)	Loss 1.1971e-02 (1.7662e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [87][380/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.001)	Loss 1.2466e-02 (1.7743e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [87][390/391]	Time  0.115 ( 0.162)	Data  0.001 ( 0.001)	Loss 3.2928e-02 (1.7729e-02)	Acc@1  98.75 ( 99.48)	Acc@5 100.00 (100.00)
## e[87] optimizer.zero_grad (sum) time: 0.5250318050384521
## e[87]       loss.backward (sum) time: 12.459116220474243
## e[87]      optimizer.step (sum) time: 25.727174282073975
## epoch[87] training(only) time: 63.579673528671265
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 1.5625e-01 (1.5625e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.048 ( 0.061)	Loss 3.1567e-01 (3.2338e-01)	Acc@1  94.00 ( 92.27)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 5.3564e-01 (3.4490e-01)	Acc@1  90.00 ( 91.95)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 3.3301e-01 (3.3593e-01)	Acc@1  94.00 ( 92.26)	Acc@5  99.00 ( 99.74)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 5.2783e-01 (3.4413e-01)	Acc@1  90.00 ( 92.22)	Acc@5  98.00 ( 99.71)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 2.3193e-01 (3.4668e-01)	Acc@1  95.00 ( 92.27)	Acc@5 100.00 ( 99.75)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 2.3987e-01 (3.3901e-01)	Acc@1  93.00 ( 92.26)	Acc@5 100.00 ( 99.77)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 3.5620e-01 (3.4176e-01)	Acc@1  93.00 ( 92.24)	Acc@5 100.00 ( 99.77)
Test: [ 80/100]	Time  0.048 ( 0.049)	Loss 2.5049e-01 (3.3887e-01)	Acc@1  94.00 ( 92.36)	Acc@5 100.00 ( 99.79)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 3.0493e-01 (3.3917e-01)	Acc@1  94.00 ( 92.32)	Acc@5 100.00 ( 99.81)
 * Acc@1 92.260 Acc@5 99.820
### epoch[87] execution time: 68.55581760406494
EPOCH 88
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [88][  0/391]	Time  0.330 ( 0.330)	Data  0.161 ( 0.161)	Loss 1.5221e-02 (1.5221e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [88][ 10/391]	Time  0.160 ( 0.176)	Data  0.001 ( 0.016)	Loss 1.4900e-02 (1.1534e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [88][ 20/391]	Time  0.160 ( 0.169)	Data  0.001 ( 0.009)	Loss 1.7090e-02 (1.6546e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [88][ 30/391]	Time  0.161 ( 0.167)	Data  0.001 ( 0.006)	Loss 2.3026e-02 (1.6647e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [88][ 40/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.005)	Loss 8.3923e-03 (1.5829e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [88][ 50/391]	Time  0.159 ( 0.165)	Data  0.001 ( 0.004)	Loss 6.8779e-03 (1.6032e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [88][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.1177e-02 (1.6875e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [88][ 70/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 3.4760e-02 (1.6344e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [88][ 80/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.4572e-02 (1.6619e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [88][ 90/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.5797e-02 (1.6320e-02)	Acc@1  98.44 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [88][100/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 8.7128e-03 (1.6790e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [88][110/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.2123e-02 (1.6871e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [88][120/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7929e-02 (1.6961e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [88][130/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.0386e-02 (1.7090e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [88][140/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2283e-02 (1.6686e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [88][150/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.9028e-02 (1.6672e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [88][160/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4626e-02 (1.7067e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [88][170/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.1626e-02 (1.7237e-02)	Acc@1  98.44 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [88][180/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.8893e-03 (1.7201e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [88][190/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.0548e-02 (1.7148e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [88][200/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7487e-02 (1.7014e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [88][210/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3199e-02 (1.6783e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [88][220/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.0914e-02 (1.6963e-02)	Acc@1  98.44 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [88][230/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.8626e-03 (1.7158e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [88][240/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.3697e-02 (1.7204e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [88][250/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.1332e-02 (1.7173e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [88][260/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.5696e-02 (1.7197e-02)	Acc@1  98.44 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [88][270/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.0182e-02 (1.7129e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [88][280/391]	Time  0.173 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.2043e-02 (1.7092e-02)	Acc@1  98.44 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [88][290/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.1678e-03 (1.6953e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [88][300/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0315e-02 (1.6942e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [88][310/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4206e-02 (1.6954e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [88][320/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.9180e-02 (1.6844e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [88][330/391]	Time  0.178 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.5765e-03 (1.6760e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [88][340/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.7400e-03 (1.6733e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [88][350/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.2561e-03 (1.6645e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [88][360/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.6774e-03 (1.6660e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [88][370/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7914e-02 (1.6577e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [88][380/391]	Time  0.178 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.1798e-03 (1.6586e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [88][390/391]	Time  0.115 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3802e-02 (1.6540e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
## e[88] optimizer.zero_grad (sum) time: 0.5261402130126953
## e[88]       loss.backward (sum) time: 12.433669805526733
## e[88]      optimizer.step (sum) time: 25.734461307525635
## epoch[88] training(only) time: 63.470374584198
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 1.7664e-01 (1.7664e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 3.3130e-01 (3.1458e-01)	Acc@1  93.00 ( 92.27)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.049 ( 0.054)	Loss 4.8779e-01 (3.3865e-01)	Acc@1  90.00 ( 91.95)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 3.3813e-01 (3.3228e-01)	Acc@1  94.00 ( 92.29)	Acc@5  99.00 ( 99.77)
Test: [ 40/100]	Time  0.048 ( 0.051)	Loss 4.9902e-01 (3.3814e-01)	Acc@1  90.00 ( 92.29)	Acc@5  99.00 ( 99.76)
Test: [ 50/100]	Time  0.055 ( 0.051)	Loss 2.1130e-01 (3.3982e-01)	Acc@1  95.00 ( 92.27)	Acc@5 100.00 ( 99.78)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 2.3938e-01 (3.3272e-01)	Acc@1  93.00 ( 92.26)	Acc@5 100.00 ( 99.80)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 3.8525e-01 (3.3427e-01)	Acc@1  93.00 ( 92.28)	Acc@5 100.00 ( 99.80)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 2.7954e-01 (3.3295e-01)	Acc@1  92.00 ( 92.35)	Acc@5 100.00 ( 99.81)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 3.2080e-01 (3.3360e-01)	Acc@1  93.00 ( 92.30)	Acc@5 100.00 ( 99.84)
 * Acc@1 92.210 Acc@5 99.840
### epoch[88] execution time: 68.47964358329773
EPOCH 89
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [89][  0/391]	Time  0.315 ( 0.315)	Data  0.144 ( 0.144)	Loss 1.1917e-02 (1.1917e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [89][ 10/391]	Time  0.172 ( 0.176)	Data  0.001 ( 0.014)	Loss 1.5137e-02 (1.1756e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [89][ 20/391]	Time  0.161 ( 0.169)	Data  0.001 ( 0.008)	Loss 9.4223e-03 (1.4690e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [89][ 30/391]	Time  0.161 ( 0.166)	Data  0.001 ( 0.006)	Loss 1.3397e-02 (1.6076e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [89][ 40/391]	Time  0.162 ( 0.165)	Data  0.001 ( 0.005)	Loss 2.5375e-02 (1.6794e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [89][ 50/391]	Time  0.162 ( 0.165)	Data  0.001 ( 0.004)	Loss 9.8648e-03 (1.7205e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [89][ 60/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.004)	Loss 2.7817e-02 (1.8000e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [89][ 70/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 5.3139e-03 (1.8318e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [89][ 80/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.3081e-02 (1.7443e-02)	Acc@1  98.44 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [89][ 90/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.0620e-02 (1.6940e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [89][100/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.003)	Loss 8.7738e-03 (1.6900e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [89][110/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0208e-02 (1.6823e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [89][120/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.3051e-02 (1.6776e-02)	Acc@1  98.44 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [89][130/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.9667e-03 (1.6199e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [89][140/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.4299e-03 (1.5926e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [89][150/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.7733e-03 (1.6147e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [89][160/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.5749e-03 (1.6091e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [89][170/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.8588e-03 (1.6103e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [89][180/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.4647e-02 (1.6533e-02)	Acc@1  98.44 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [89][190/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3573e-02 (1.6362e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [89][200/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.2614e-02 (1.6351e-02)	Acc@1  98.44 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [89][210/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.1677e-02 (1.6485e-02)	Acc@1  98.44 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [89][220/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.9346e-03 (1.6785e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [89][230/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.1308e-03 (1.6941e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [89][240/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.0691e-02 (1.7012e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [89][250/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.6986e-03 (1.6898e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [89][260/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.0248e-02 (1.6908e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [89][270/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0826e-02 (1.6909e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [89][280/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.7280e-03 (1.6868e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [89][290/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.4399e-02 (1.6649e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [89][300/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.9882e-02 (1.6829e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [89][310/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.2684e-02 (1.7168e-02)	Acc@1  98.44 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [89][320/391]	Time  0.170 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.2125e-02 (1.7141e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [89][330/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.5101e-02 (1.7070e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [89][340/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1459e-02 (1.6891e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [89][350/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.1346e-03 (1.6968e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [89][360/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.8648e-03 (1.6870e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [89][370/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.5825e-03 (1.6915e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [89][380/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4709e-02 (1.6965e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [89][390/391]	Time  0.116 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.8687e-02 (1.6957e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
## e[89] optimizer.zero_grad (sum) time: 0.5294795036315918
## e[89]       loss.backward (sum) time: 12.45770001411438
## e[89]      optimizer.step (sum) time: 25.740511417388916
## epoch[89] training(only) time: 63.492955923080444
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 1.4917e-01 (1.4917e-01)	Acc@1  96.00 ( 96.00)	Acc@5 100.00 (100.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 3.2031e-01 (3.1808e-01)	Acc@1  93.00 ( 91.91)	Acc@5 100.00 ( 99.82)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 5.3271e-01 (3.4419e-01)	Acc@1  90.00 ( 91.76)	Acc@5 100.00 ( 99.76)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 3.1909e-01 (3.3789e-01)	Acc@1  94.00 ( 92.10)	Acc@5  99.00 ( 99.74)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 5.0537e-01 (3.4263e-01)	Acc@1  89.00 ( 92.10)	Acc@5  99.00 ( 99.73)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 2.0825e-01 (3.4568e-01)	Acc@1  95.00 ( 92.18)	Acc@5 100.00 ( 99.76)
Test: [ 60/100]	Time  0.047 ( 0.049)	Loss 2.4304e-01 (3.3671e-01)	Acc@1  93.00 ( 92.26)	Acc@5 100.00 ( 99.79)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 3.6548e-01 (3.3889e-01)	Acc@1  93.00 ( 92.25)	Acc@5 100.00 ( 99.79)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 2.4841e-01 (3.3660e-01)	Acc@1  94.00 ( 92.38)	Acc@5 100.00 ( 99.80)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 3.0933e-01 (3.3718e-01)	Acc@1  92.00 ( 92.30)	Acc@5 100.00 ( 99.81)
 * Acc@1 92.250 Acc@5 99.820
### epoch[89] execution time: 68.43022465705872
### Training complete:
#### total training(only) time: 5726.6824543476105
##### Total run time: 6179.686965942383
