# Model: inception
# Dataset: cifarcentum
# Batch size: 128
# Freezeout: False
# Low precision: False
# Epochs: 90
# Initial learning rate: 0.1
# module_name: models_cifarcentum.inception
<function inception at 0x7fcb4a74bf28>
# model requested: 'inception'
# printing out the model
InceptionV3(
  (Conv2d_1a_3x3): BasicConv2d(
    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (Conv2d_2a_3x3): BasicConv2d(
    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (Conv2d_2b_3x3): BasicConv2d(
    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (Conv2d_3b_1x1): BasicConv2d(
    (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (Conv2d_4a_3x3): BasicConv2d(
    (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)
    (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (Mixed_5b): InceptionA(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch5x5): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branch3x3): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branchpool): Sequential(
      (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
      (1): BasicConv2d(
        (conv): Conv2d(192, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
  )
  (Mixed_5c): InceptionA(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch5x5): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branch3x3): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branchpool): Sequential(
      (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
      (1): BasicConv2d(
        (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
  )
  (Mixed_5d): InceptionA(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch5x5): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branch3x3): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branchpool): Sequential(
      (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
      (1): BasicConv2d(
        (conv): Conv2d(288, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
  )
  (Mixed_6a): InceptionB(
    (branch3x3): BasicConv2d(
      (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3stack): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branchpool): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (Mixed_6b): InceptionC(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch7x7): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branch7x7stack): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): BasicConv2d(
        (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): BasicConv2d(
        (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branch_pool): Sequential(
      (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
      (1): BasicConv2d(
        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
  )
  (Mixed_6c): InceptionC(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch7x7): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branch7x7stack): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): BasicConv2d(
        (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): BasicConv2d(
        (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branch_pool): Sequential(
      (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
      (1): BasicConv2d(
        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
  )
  (Mixed_6d): InceptionC(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch7x7): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branch7x7stack): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): BasicConv2d(
        (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): BasicConv2d(
        (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branch_pool): Sequential(
      (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
      (1): BasicConv2d(
        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
  )
  (Mixed_6e): InceptionC(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch7x7): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branch7x7stack): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): BasicConv2d(
        (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): BasicConv2d(
        (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branch_pool): Sequential(
      (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
      (1): BasicConv2d(
        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
  )
  (Mixed_7a): InceptionD(
    (branch3x3): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)
        (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branch7x7): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): BasicConv2d(
        (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branchpool): AvgPool2d(kernel_size=3, stride=2, padding=0)
  )
  (Mixed_7b): InceptionE(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3_1): BasicConv2d(
      (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3_2a): BasicConv2d(
      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3_2b): BasicConv2d(
      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3stack_1): BasicConv2d(
      (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3stack_2): BasicConv2d(
      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3stack_3a): BasicConv2d(
      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3stack_3b): BasicConv2d(
      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch_pool): Sequential(
      (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
      (1): BasicConv2d(
        (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
  )
  (Mixed_7c): InceptionE(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3_1): BasicConv2d(
      (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3_2a): BasicConv2d(
      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3_2b): BasicConv2d(
      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3stack_1): BasicConv2d(
      (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3stack_2): BasicConv2d(
      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3stack_3a): BasicConv2d(
      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3stack_3b): BasicConv2d(
      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch_pool): Sequential(
      (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
      (1): BasicConv2d(
        (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout2d(p=0.5, inplace=False)
  (linear): Linear(in_features=2048, out_features=100, bias=True)
)
# model is full precision
# Model: inception
# Dataset: cifarcentum
# Freezeout: False
# Low precision: False
# Initial learning rate: 0.1
# Criterion: CrossEntropyLoss()
# Optimizer: SGD
#	lr 0.1
#	momentum 0.9
#	dampening 0
#	weight_decay 0.0001
#	nesterov False
CIFAR100 loading and normalization
==> Preparing data..
# CIFAR100 / full precision
Files already downloaded and verified
Files already downloaded and verified
#--> Training data: <--#
#-->>
EPOCH 0
# current learning rate: 0.1
# Switched to train mode...
Epoch: [0][  0/391]	Time  4.925 ( 4.925)	Data  0.111 ( 0.111)	Loss 4.6701e+00 (4.6701e+00)	Acc@1   2.34 (  2.34)	Acc@5   7.03 (  7.03)
Epoch: [0][ 10/391]	Time  0.238 ( 0.665)	Data  0.001 ( 0.011)	Loss 4.7850e+00 (5.3444e+00)	Acc@1   2.34 (  1.21)	Acc@5  10.94 (  5.97)
Epoch: [0][ 20/391]	Time  0.237 ( 0.462)	Data  0.001 ( 0.006)	Loss 5.1720e+00 (5.2804e+00)	Acc@1   1.56 (  1.12)	Acc@5   7.81 (  5.84)
Epoch: [0][ 30/391]	Time  0.237 ( 0.390)	Data  0.001 ( 0.005)	Loss 4.7014e+00 (5.1214e+00)	Acc@1   0.78 (  1.01)	Acc@5   3.91 (  5.24)
Epoch: [0][ 40/391]	Time  0.238 ( 0.353)	Data  0.001 ( 0.004)	Loss 4.7141e+00 (5.0117e+00)	Acc@1   0.78 (  1.09)	Acc@5  10.16 (  5.28)
Epoch: [0][ 50/391]	Time  0.239 ( 0.331)	Data  0.001 ( 0.003)	Loss 4.6554e+00 (4.9354e+00)	Acc@1   1.56 (  1.06)	Acc@5   7.81 (  5.59)
Epoch: [0][ 60/391]	Time  0.240 ( 0.315)	Data  0.001 ( 0.003)	Loss 4.4925e+00 (4.8804e+00)	Acc@1   2.34 (  1.17)	Acc@5   7.81 (  5.81)
Epoch: [0][ 70/391]	Time  0.240 ( 0.305)	Data  0.001 ( 0.003)	Loss 4.5065e+00 (4.8369e+00)	Acc@1   0.78 (  1.33)	Acc@5   7.03 (  6.11)
Epoch: [0][ 80/391]	Time  0.238 ( 0.297)	Data  0.001 ( 0.002)	Loss 4.5265e+00 (4.7950e+00)	Acc@1   1.56 (  1.48)	Acc@5   7.81 (  6.81)
Epoch: [0][ 90/391]	Time  0.238 ( 0.290)	Data  0.001 ( 0.002)	Loss 4.4820e+00 (4.7637e+00)	Acc@1   2.34 (  1.53)	Acc@5  11.72 (  7.08)
Epoch: [0][100/391]	Time  0.238 ( 0.285)	Data  0.001 ( 0.002)	Loss 4.4088e+00 (4.7305e+00)	Acc@1   0.78 (  1.55)	Acc@5   8.59 (  7.59)
Epoch: [0][110/391]	Time  0.239 ( 0.281)	Data  0.001 ( 0.002)	Loss 4.4305e+00 (4.7013e+00)	Acc@1   1.56 (  1.64)	Acc@5  12.50 (  8.19)
Epoch: [0][120/391]	Time  0.239 ( 0.278)	Data  0.001 ( 0.002)	Loss 4.4763e+00 (4.6771e+00)	Acc@1   2.34 (  1.78)	Acc@5  10.94 (  8.70)
Epoch: [0][130/391]	Time  0.239 ( 0.275)	Data  0.001 ( 0.002)	Loss 4.3968e+00 (4.6546e+00)	Acc@1   2.34 (  1.91)	Acc@5  10.94 (  9.14)
Epoch: [0][140/391]	Time  0.239 ( 0.272)	Data  0.001 ( 0.002)	Loss 4.4916e+00 (4.6350e+00)	Acc@1   2.34 (  1.97)	Acc@5  12.50 (  9.53)
Epoch: [0][150/391]	Time  0.239 ( 0.270)	Data  0.001 ( 0.002)	Loss 4.3522e+00 (4.6164e+00)	Acc@1   3.91 (  2.09)	Acc@5  14.06 (  9.80)
Epoch: [0][160/391]	Time  0.240 ( 0.268)	Data  0.001 ( 0.002)	Loss 4.3523e+00 (4.5994e+00)	Acc@1   1.56 (  2.20)	Acc@5  12.50 ( 10.21)
Epoch: [0][170/391]	Time  0.241 ( 0.266)	Data  0.001 ( 0.002)	Loss 4.3415e+00 (4.5836e+00)	Acc@1   3.91 (  2.26)	Acc@5  12.50 ( 10.49)
Epoch: [0][180/391]	Time  0.239 ( 0.265)	Data  0.001 ( 0.002)	Loss 4.3230e+00 (4.5675e+00)	Acc@1   4.69 (  2.40)	Acc@5  21.09 ( 10.83)
Epoch: [0][190/391]	Time  0.239 ( 0.264)	Data  0.001 ( 0.002)	Loss 4.3076e+00 (4.5538e+00)	Acc@1   4.69 (  2.46)	Acc@5  16.41 ( 11.15)
Epoch: [0][200/391]	Time  0.240 ( 0.263)	Data  0.001 ( 0.002)	Loss 4.2673e+00 (4.5408e+00)	Acc@1   4.69 (  2.55)	Acc@5  14.06 ( 11.46)
Epoch: [0][210/391]	Time  0.240 ( 0.261)	Data  0.001 ( 0.002)	Loss 4.0445e+00 (4.5274e+00)	Acc@1   8.59 (  2.64)	Acc@5  22.66 ( 11.69)
Epoch: [0][220/391]	Time  0.241 ( 0.261)	Data  0.001 ( 0.002)	Loss 4.2189e+00 (4.5159e+00)	Acc@1   4.69 (  2.73)	Acc@5  21.09 ( 11.91)
Epoch: [0][230/391]	Time  0.239 ( 0.260)	Data  0.001 ( 0.002)	Loss 4.2871e+00 (4.5046e+00)	Acc@1   3.12 (  2.80)	Acc@5  16.41 ( 12.12)
Epoch: [0][240/391]	Time  0.240 ( 0.259)	Data  0.001 ( 0.002)	Loss 4.2632e+00 (4.4952e+00)	Acc@1   4.69 (  2.84)	Acc@5  14.06 ( 12.33)
Epoch: [0][250/391]	Time  0.241 ( 0.258)	Data  0.001 ( 0.002)	Loss 4.3026e+00 (4.4853e+00)	Acc@1   4.69 (  2.90)	Acc@5  18.75 ( 12.56)
Epoch: [0][260/391]	Time  0.240 ( 0.257)	Data  0.001 ( 0.001)	Loss 4.2862e+00 (4.4748e+00)	Acc@1   3.12 (  3.01)	Acc@5  12.50 ( 12.82)
Epoch: [0][270/391]	Time  0.241 ( 0.257)	Data  0.001 ( 0.001)	Loss 4.2409e+00 (4.4652e+00)	Acc@1   3.91 (  3.08)	Acc@5  20.31 ( 13.01)
Epoch: [0][280/391]	Time  0.240 ( 0.256)	Data  0.001 ( 0.001)	Loss 4.1024e+00 (4.4556e+00)	Acc@1   5.47 (  3.15)	Acc@5  26.56 ( 13.28)
Epoch: [0][290/391]	Time  0.241 ( 0.256)	Data  0.001 ( 0.001)	Loss 4.2462e+00 (4.4473e+00)	Acc@1   4.69 (  3.21)	Acc@5  16.41 ( 13.46)
Epoch: [0][300/391]	Time  0.239 ( 0.255)	Data  0.001 ( 0.001)	Loss 4.1640e+00 (4.4383e+00)	Acc@1   2.34 (  3.24)	Acc@5  14.84 ( 13.64)
Epoch: [0][310/391]	Time  0.240 ( 0.255)	Data  0.001 ( 0.001)	Loss 4.1720e+00 (4.4290e+00)	Acc@1   3.91 (  3.32)	Acc@5  17.19 ( 13.90)
Epoch: [0][320/391]	Time  0.241 ( 0.254)	Data  0.001 ( 0.001)	Loss 4.2518e+00 (4.4222e+00)	Acc@1   3.12 (  3.39)	Acc@5  14.84 ( 14.06)
Epoch: [0][330/391]	Time  0.240 ( 0.254)	Data  0.001 ( 0.001)	Loss 4.2657e+00 (4.4148e+00)	Acc@1   7.03 (  3.42)	Acc@5  21.09 ( 14.22)
Epoch: [0][340/391]	Time  0.239 ( 0.253)	Data  0.001 ( 0.001)	Loss 4.3029e+00 (4.4079e+00)	Acc@1   4.69 (  3.47)	Acc@5  13.28 ( 14.37)
Epoch: [0][350/391]	Time  0.240 ( 0.253)	Data  0.001 ( 0.001)	Loss 4.2034e+00 (4.3998e+00)	Acc@1   6.25 (  3.52)	Acc@5  15.62 ( 14.53)
Epoch: [0][360/391]	Time  0.240 ( 0.253)	Data  0.001 ( 0.001)	Loss 4.0530e+00 (4.3926e+00)	Acc@1   3.91 (  3.60)	Acc@5  18.75 ( 14.69)
Epoch: [0][370/391]	Time  0.239 ( 0.252)	Data  0.001 ( 0.001)	Loss 4.1160e+00 (4.3857e+00)	Acc@1   4.69 (  3.67)	Acc@5  21.88 ( 14.88)
Epoch: [0][380/391]	Time  0.240 ( 0.252)	Data  0.001 ( 0.001)	Loss 4.0496e+00 (4.3793e+00)	Acc@1   5.47 (  3.72)	Acc@5  21.09 ( 15.05)
Epoch: [0][390/391]	Time  1.884 ( 0.256)	Data  0.001 ( 0.001)	Loss 4.0352e+00 (4.3734e+00)	Acc@1  10.00 (  3.76)	Acc@5  26.25 ( 15.19)
## e[0] optimizer.zero_grad (sum) time: 0.48830485343933105
## e[0]       loss.backward (sum) time: 13.434413433074951
## e[0]      optimizer.step (sum) time: 47.16766619682312
## epoch[0] training(only) time: 100.14591932296753
# Switched to evaluate mode...
Test: [  0/100]	Time  0.786 ( 0.786)	Loss 4.1818e+00 (4.1818e+00)	Acc@1   6.00 (  6.00)	Acc@5  16.00 ( 16.00)
Test: [ 10/100]	Time  0.077 ( 0.148)	Loss 4.1465e+00 (4.2154e+00)	Acc@1   6.00 (  6.55)	Acc@5  27.00 ( 22.36)
Test: [ 20/100]	Time  0.081 ( 0.115)	Loss 4.0187e+00 (4.1703e+00)	Acc@1  10.00 (  6.52)	Acc@5  25.00 ( 22.57)
Test: [ 30/100]	Time  0.078 ( 0.103)	Loss 4.2971e+00 (4.1699e+00)	Acc@1   8.00 (  6.32)	Acc@5  24.00 ( 22.77)
Test: [ 40/100]	Time  0.078 ( 0.097)	Loss 4.3049e+00 (4.1900e+00)	Acc@1   7.00 (  6.20)	Acc@5  21.00 ( 22.59)
Test: [ 50/100]	Time  0.078 ( 0.093)	Loss 4.1522e+00 (4.1901e+00)	Acc@1   6.00 (  6.12)	Acc@5  19.00 ( 22.35)
Test: [ 60/100]	Time  0.078 ( 0.090)	Loss 4.2740e+00 (4.1953e+00)	Acc@1   3.00 (  5.93)	Acc@5  28.00 ( 22.10)
Test: [ 70/100]	Time  0.079 ( 0.089)	Loss 4.2978e+00 (4.2113e+00)	Acc@1   5.00 (  5.85)	Acc@5  24.00 ( 22.04)
Test: [ 80/100]	Time  0.078 ( 0.087)	Loss 4.1671e+00 (4.2063e+00)	Acc@1   7.00 (  5.72)	Acc@5  22.00 ( 21.81)
Test: [ 90/100]	Time  0.077 ( 0.086)	Loss 4.0783e+00 (4.2018e+00)	Acc@1   8.00 (  5.80)	Acc@5  27.00 ( 22.13)
 * Acc@1 5.860 Acc@5 22.100
### epoch[0] execution time: 108.783775806427
EPOCH 1
# current learning rate: 0.1
# Switched to train mode...
Epoch: [1][  0/391]	Time  0.438 ( 0.438)	Data  0.179 ( 0.179)	Loss 4.2568e+00 (4.2568e+00)	Acc@1   3.12 (  3.12)	Acc@5  16.41 ( 16.41)
Epoch: [1][ 10/391]	Time  0.239 ( 0.259)	Data  0.001 ( 0.017)	Loss 4.1133e+00 (4.1084e+00)	Acc@1  10.16 (  4.83)	Acc@5  25.00 ( 20.67)
Epoch: [1][ 20/391]	Time  0.241 ( 0.250)	Data  0.001 ( 0.010)	Loss 4.0637e+00 (4.1112e+00)	Acc@1   6.25 (  4.65)	Acc@5  23.44 ( 21.28)
Epoch: [1][ 30/391]	Time  0.241 ( 0.247)	Data  0.001 ( 0.007)	Loss 4.1260e+00 (4.1104e+00)	Acc@1   4.69 (  4.56)	Acc@5  20.31 ( 21.27)
Epoch: [1][ 40/391]	Time  0.249 ( 0.246)	Data  0.001 ( 0.005)	Loss 4.1124e+00 (4.0986e+00)	Acc@1   7.03 (  5.03)	Acc@5  20.31 ( 22.03)
Epoch: [1][ 50/391]	Time  0.242 ( 0.245)	Data  0.001 ( 0.005)	Loss 3.9533e+00 (4.0909e+00)	Acc@1   3.12 (  5.22)	Acc@5  23.44 ( 22.32)
Epoch: [1][ 60/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 4.2083e+00 (4.0923e+00)	Acc@1   6.25 (  5.25)	Acc@5  19.53 ( 22.27)
Epoch: [1][ 70/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 4.1878e+00 (4.0940e+00)	Acc@1   5.47 (  5.51)	Acc@5  18.75 ( 22.26)
Epoch: [1][ 80/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.003)	Loss 4.0680e+00 (4.0940e+00)	Acc@1   8.59 (  5.45)	Acc@5  25.78 ( 22.33)
Epoch: [1][ 90/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 4.0753e+00 (4.0907e+00)	Acc@1   4.69 (  5.53)	Acc@5  21.09 ( 22.51)
Epoch: [1][100/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.9574e+00 (4.0840e+00)	Acc@1   3.91 (  5.57)	Acc@5  26.56 ( 22.74)
Epoch: [1][110/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.9610e+00 (4.0816e+00)	Acc@1   8.59 (  5.69)	Acc@5  28.91 ( 23.02)
Epoch: [1][120/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.9898e+00 (4.0792e+00)	Acc@1   8.59 (  5.75)	Acc@5  30.47 ( 23.15)
Epoch: [1][130/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 4.1614e+00 (4.0789e+00)	Acc@1   6.25 (  5.81)	Acc@5  17.97 ( 23.12)
Epoch: [1][140/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.0309e+00 (4.0745e+00)	Acc@1   3.12 (  5.81)	Acc@5  21.88 ( 23.15)
Epoch: [1][150/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.9283e+00 (4.0683e+00)	Acc@1  11.72 (  5.87)	Acc@5  27.34 ( 23.29)
Epoch: [1][160/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.1126e+00 (4.0651e+00)	Acc@1   7.03 (  6.01)	Acc@5  20.31 ( 23.44)
Epoch: [1][170/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.9812e+00 (4.0611e+00)	Acc@1   9.38 (  6.11)	Acc@5  30.47 ( 23.61)
Epoch: [1][180/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.0968e+00 (4.0592e+00)	Acc@1   6.25 (  6.15)	Acc@5  25.78 ( 23.71)
Epoch: [1][190/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.9053e+00 (4.0584e+00)	Acc@1   7.81 (  6.15)	Acc@5  26.56 ( 23.68)
Epoch: [1][200/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.0134e+00 (4.0581e+00)	Acc@1   3.12 (  6.17)	Acc@5  25.00 ( 23.69)
Epoch: [1][210/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.0202e+00 (4.0550e+00)	Acc@1   7.03 (  6.22)	Acc@5  25.78 ( 23.79)
Epoch: [1][220/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.0349e+00 (4.0502e+00)	Acc@1   7.03 (  6.25)	Acc@5  22.66 ( 23.87)
Epoch: [1][230/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.9851e+00 (4.0471e+00)	Acc@1   5.47 (  6.32)	Acc@5  22.66 ( 23.96)
Epoch: [1][240/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.8537e+00 (4.0433e+00)	Acc@1  10.94 (  6.40)	Acc@5  28.12 ( 24.12)
Epoch: [1][250/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.9992e+00 (4.0395e+00)	Acc@1  10.16 (  6.48)	Acc@5  26.56 ( 24.28)
Epoch: [1][260/391]	Time  0.249 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.9685e+00 (4.0343e+00)	Acc@1   3.91 (  6.55)	Acc@5  21.88 ( 24.47)
Epoch: [1][270/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.0276e+00 (4.0333e+00)	Acc@1  11.72 (  6.58)	Acc@5  25.78 ( 24.50)
Epoch: [1][280/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.9387e+00 (4.0308e+00)	Acc@1   6.25 (  6.62)	Acc@5  29.69 ( 24.57)
Epoch: [1][290/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.9984e+00 (4.0271e+00)	Acc@1   7.81 (  6.67)	Acc@5  27.34 ( 24.72)
Epoch: [1][300/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.8804e+00 (4.0233e+00)	Acc@1  10.94 (  6.69)	Acc@5  28.91 ( 24.78)
Epoch: [1][310/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.0510e+00 (4.0212e+00)	Acc@1   5.47 (  6.70)	Acc@5  21.09 ( 24.90)
Epoch: [1][320/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.8890e+00 (4.0184e+00)	Acc@1   9.38 (  6.75)	Acc@5  35.16 ( 25.03)
Epoch: [1][330/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.0153e+00 (4.0138e+00)	Acc@1   5.47 (  6.81)	Acc@5  26.56 ( 25.20)
Epoch: [1][340/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.0099e+00 (4.0102e+00)	Acc@1   5.47 (  6.86)	Acc@5  25.00 ( 25.30)
Epoch: [1][350/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.7919e+00 (4.0062e+00)	Acc@1   8.59 (  6.92)	Acc@5  32.81 ( 25.46)
Epoch: [1][360/391]	Time  0.260 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.9560e+00 (4.0041e+00)	Acc@1   4.69 (  6.94)	Acc@5  31.25 ( 25.52)
Epoch: [1][370/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.9460e+00 (4.0012e+00)	Acc@1   9.38 (  6.98)	Acc@5  23.44 ( 25.61)
Epoch: [1][380/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.8955e+00 (3.9972e+00)	Acc@1   9.38 (  7.06)	Acc@5  26.56 ( 25.74)
Epoch: [1][390/391]	Time  0.177 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.8529e+00 (3.9960e+00)	Acc@1  11.25 (  7.09)	Acc@5  32.50 ( 25.81)
## e[1] optimizer.zero_grad (sum) time: 0.4907112121582031
## e[1]       loss.backward (sum) time: 11.006463289260864
## e[1]      optimizer.step (sum) time: 47.992980003356934
## epoch[1] training(only) time: 94.89262557029724
# Switched to evaluate mode...
Test: [  0/100]	Time  0.212 ( 0.212)	Loss 3.8553e+00 (3.8553e+00)	Acc@1  10.00 ( 10.00)	Acc@5  30.00 ( 30.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 3.8831e+00 (3.7905e+00)	Acc@1  11.00 ( 10.91)	Acc@5  27.00 ( 32.00)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 3.7848e+00 (3.7977e+00)	Acc@1  11.00 ( 10.14)	Acc@5  29.00 ( 32.57)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 3.9564e+00 (3.7980e+00)	Acc@1   8.00 ( 10.13)	Acc@5  23.00 ( 32.00)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 3.8526e+00 (3.7968e+00)	Acc@1  10.00 ( 10.17)	Acc@5  34.00 ( 32.29)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 3.7419e+00 (3.7931e+00)	Acc@1  15.00 ( 10.33)	Acc@5  38.00 ( 32.59)
Test: [ 60/100]	Time  0.077 ( 0.080)	Loss 3.6409e+00 (3.7859e+00)	Acc@1   5.00 ( 10.41)	Acc@5  39.00 ( 32.64)
Test: [ 70/100]	Time  0.079 ( 0.080)	Loss 3.8945e+00 (3.7881e+00)	Acc@1  12.00 ( 10.45)	Acc@5  31.00 ( 32.72)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 3.9771e+00 (3.7950e+00)	Acc@1   5.00 ( 10.40)	Acc@5  31.00 ( 32.53)
Test: [ 90/100]	Time  0.079 ( 0.079)	Loss 3.6287e+00 (3.7898e+00)	Acc@1  13.00 ( 10.41)	Acc@5  38.00 ( 32.73)
 * Acc@1 10.220 Acc@5 32.580
### epoch[1] execution time: 102.90898561477661
EPOCH 2
# current learning rate: 0.1
# Switched to train mode...
Epoch: [2][  0/391]	Time  0.404 ( 0.404)	Data  0.153 ( 0.153)	Loss 3.8066e+00 (3.8066e+00)	Acc@1   3.91 (  3.91)	Acc@5  28.91 ( 28.91)
Epoch: [2][ 10/391]	Time  0.241 ( 0.256)	Data  0.001 ( 0.015)	Loss 3.7945e+00 (3.8027e+00)	Acc@1   7.81 (  8.59)	Acc@5  28.12 ( 30.04)
Epoch: [2][ 20/391]	Time  0.244 ( 0.249)	Data  0.001 ( 0.008)	Loss 3.8675e+00 (3.8245e+00)	Acc@1   7.81 (  8.89)	Acc@5  28.91 ( 30.25)
Epoch: [2][ 30/391]	Time  0.241 ( 0.247)	Data  0.001 ( 0.006)	Loss 3.7227e+00 (3.8186e+00)	Acc@1  12.50 (  9.20)	Acc@5  34.38 ( 30.82)
Epoch: [2][ 40/391]	Time  0.243 ( 0.246)	Data  0.001 ( 0.005)	Loss 3.6590e+00 (3.8233e+00)	Acc@1   9.38 (  9.07)	Acc@5  33.59 ( 30.64)
Epoch: [2][ 50/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.004)	Loss 3.8183e+00 (3.8395e+00)	Acc@1   8.59 (  9.02)	Acc@5  32.03 ( 30.30)
Epoch: [2][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 3.9542e+00 (3.8306e+00)	Acc@1   5.47 (  9.30)	Acc@5  25.78 ( 30.74)
Epoch: [2][ 70/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 3.9687e+00 (3.8267e+00)	Acc@1   7.81 (  9.55)	Acc@5  30.47 ( 30.82)
Epoch: [2][ 80/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.003)	Loss 3.7293e+00 (3.8237e+00)	Acc@1  12.50 (  9.68)	Acc@5  34.38 ( 31.10)
Epoch: [2][ 90/391]	Time  0.244 ( 0.244)	Data  0.001 ( 0.003)	Loss 3.8747e+00 (3.8246e+00)	Acc@1   7.03 (  9.71)	Acc@5  28.91 ( 30.98)
Epoch: [2][100/391]	Time  0.244 ( 0.244)	Data  0.001 ( 0.003)	Loss 3.6386e+00 (3.8237e+00)	Acc@1  16.41 (  9.89)	Acc@5  37.50 ( 31.07)
Epoch: [2][110/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.003)	Loss 3.7366e+00 (3.8187e+00)	Acc@1  10.94 (  9.99)	Acc@5  35.16 ( 31.27)
Epoch: [2][120/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.5863e+00 (3.8136e+00)	Acc@1  10.94 (  9.94)	Acc@5  38.28 ( 31.31)
Epoch: [2][130/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.8177e+00 (3.8120e+00)	Acc@1  12.50 ( 10.11)	Acc@5  35.94 ( 31.45)
Epoch: [2][140/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.6970e+00 (3.8100e+00)	Acc@1  10.16 ( 10.10)	Acc@5  32.81 ( 31.43)
Epoch: [2][150/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.6406e+00 (3.8061e+00)	Acc@1  14.06 ( 10.20)	Acc@5  38.28 ( 31.60)
Epoch: [2][160/391]	Time  0.250 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.6320e+00 (3.8025e+00)	Acc@1  13.28 ( 10.27)	Acc@5  39.84 ( 31.77)
Epoch: [2][170/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.6832e+00 (3.8005e+00)	Acc@1  10.16 ( 10.36)	Acc@5  39.06 ( 31.84)
Epoch: [2][180/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.6184e+00 (3.7981e+00)	Acc@1   7.81 ( 10.33)	Acc@5  36.72 ( 31.96)
Epoch: [2][190/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.8518e+00 (3.7930e+00)	Acc@1  10.94 ( 10.41)	Acc@5  31.25 ( 32.15)
Epoch: [2][200/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.8374e+00 (3.7887e+00)	Acc@1  10.16 ( 10.50)	Acc@5  29.69 ( 32.33)
Epoch: [2][210/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.8231e+00 (3.7865e+00)	Acc@1  10.94 ( 10.48)	Acc@5  35.16 ( 32.43)
Epoch: [2][220/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.8774e+00 (3.7858e+00)	Acc@1  13.28 ( 10.51)	Acc@5  31.25 ( 32.48)
Epoch: [2][230/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.7209e+00 (3.7866e+00)	Acc@1  10.16 ( 10.50)	Acc@5  34.38 ( 32.41)
Epoch: [2][240/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.6837e+00 (3.7806e+00)	Acc@1  14.06 ( 10.70)	Acc@5  34.38 ( 32.62)
Epoch: [2][250/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.6403e+00 (3.7771e+00)	Acc@1  13.28 ( 10.72)	Acc@5  40.62 ( 32.75)
Epoch: [2][260/391]	Time  0.244 ( 0.243)	Data  0.002 ( 0.002)	Loss 3.8540e+00 (3.7742e+00)	Acc@1   9.38 ( 10.80)	Acc@5  33.59 ( 32.85)
Epoch: [2][270/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.5286e+00 (3.7734e+00)	Acc@1  17.19 ( 10.85)	Acc@5  39.06 ( 32.95)
Epoch: [2][280/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.8602e+00 (3.7721e+00)	Acc@1   7.81 ( 10.85)	Acc@5  30.47 ( 32.93)
Epoch: [2][290/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.5217e+00 (3.7671e+00)	Acc@1  12.50 ( 10.95)	Acc@5  38.28 ( 33.12)
Epoch: [2][300/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.4910e+00 (3.7596e+00)	Acc@1  12.50 ( 11.03)	Acc@5  37.50 ( 33.33)
Epoch: [2][310/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.7132e+00 (3.7559e+00)	Acc@1  10.94 ( 11.10)	Acc@5  35.94 ( 33.51)
Epoch: [2][320/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.4907e+00 (3.7528e+00)	Acc@1  15.62 ( 11.16)	Acc@5  42.97 ( 33.62)
Epoch: [2][330/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.5648e+00 (3.7482e+00)	Acc@1  13.28 ( 11.20)	Acc@5  38.28 ( 33.75)
Epoch: [2][340/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.7271e+00 (3.7468e+00)	Acc@1  11.72 ( 11.24)	Acc@5  35.16 ( 33.77)
Epoch: [2][350/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.6812e+00 (3.7451e+00)	Acc@1  13.28 ( 11.28)	Acc@5  36.72 ( 33.84)
Epoch: [2][360/391]	Time  0.249 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.6701e+00 (3.7414e+00)	Acc@1  15.62 ( 11.36)	Acc@5  33.59 ( 33.96)
Epoch: [2][370/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.6078e+00 (3.7364e+00)	Acc@1  17.19 ( 11.44)	Acc@5  37.50 ( 34.11)
Epoch: [2][380/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.6875e+00 (3.7343e+00)	Acc@1  16.41 ( 11.47)	Acc@5  35.16 ( 34.22)
Epoch: [2][390/391]	Time  0.178 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.8234e+00 (3.7329e+00)	Acc@1  16.25 ( 11.54)	Acc@5  31.25 ( 34.27)
## e[2] optimizer.zero_grad (sum) time: 0.491823673248291
## e[2]       loss.backward (sum) time: 11.028390645980835
## e[2]      optimizer.step (sum) time: 48.109010219573975
## epoch[2] training(only) time: 95.00451803207397
# Switched to evaluate mode...
Test: [  0/100]	Time  0.201 ( 0.201)	Loss 3.5784e+00 (3.5784e+00)	Acc@1  17.00 ( 17.00)	Acc@5  45.00 ( 45.00)
Test: [ 10/100]	Time  0.078 ( 0.089)	Loss 3.6326e+00 (3.5882e+00)	Acc@1   9.00 ( 13.55)	Acc@5  39.00 ( 41.36)
Test: [ 20/100]	Time  0.081 ( 0.084)	Loss 3.2663e+00 (3.5490e+00)	Acc@1  17.00 ( 14.33)	Acc@5  50.00 ( 42.57)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 3.7155e+00 (3.5411e+00)	Acc@1  17.00 ( 14.87)	Acc@5  35.00 ( 42.03)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 3.5828e+00 (3.5405e+00)	Acc@1  14.00 ( 14.88)	Acc@5  31.00 ( 42.02)
Test: [ 50/100]	Time  0.078 ( 0.080)	Loss 3.4791e+00 (3.5376e+00)	Acc@1  17.00 ( 15.35)	Acc@5  41.00 ( 41.78)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 3.4469e+00 (3.5353e+00)	Acc@1  21.00 ( 15.56)	Acc@5  44.00 ( 41.64)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 3.6531e+00 (3.5369e+00)	Acc@1  11.00 ( 15.15)	Acc@5  39.00 ( 41.56)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 3.7118e+00 (3.5466e+00)	Acc@1  11.00 ( 14.98)	Acc@5  33.00 ( 41.30)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 3.5186e+00 (3.5414e+00)	Acc@1  13.00 ( 15.01)	Acc@5  42.00 ( 41.46)
 * Acc@1 15.010 Acc@5 41.420
### epoch[2] execution time: 103.01577091217041
EPOCH 3
# current learning rate: 0.1
# Switched to train mode...
Epoch: [3][  0/391]	Time  0.405 ( 0.405)	Data  0.157 ( 0.157)	Loss 3.7508e+00 (3.7508e+00)	Acc@1   7.81 (  7.81)	Acc@5  32.81 ( 32.81)
Epoch: [3][ 10/391]	Time  0.240 ( 0.256)	Data  0.001 ( 0.015)	Loss 3.6557e+00 (3.5933e+00)	Acc@1  14.06 ( 14.06)	Acc@5  35.94 ( 40.62)
Epoch: [3][ 20/391]	Time  0.241 ( 0.249)	Data  0.001 ( 0.008)	Loss 3.6209e+00 (3.6012e+00)	Acc@1  16.41 ( 13.21)	Acc@5  37.50 ( 39.36)
Epoch: [3][ 30/391]	Time  0.241 ( 0.247)	Data  0.001 ( 0.006)	Loss 3.6450e+00 (3.6051e+00)	Acc@1  11.72 ( 13.58)	Acc@5  36.72 ( 39.31)
Epoch: [3][ 40/391]	Time  0.244 ( 0.245)	Data  0.001 ( 0.005)	Loss 3.5124e+00 (3.6069e+00)	Acc@1  15.62 ( 13.51)	Acc@5  42.97 ( 39.06)
Epoch: [3][ 50/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.004)	Loss 3.6460e+00 (3.5939e+00)	Acc@1   7.81 ( 13.79)	Acc@5  32.81 ( 39.29)
Epoch: [3][ 60/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 3.4482e+00 (3.5807e+00)	Acc@1  17.97 ( 14.31)	Acc@5  44.53 ( 39.66)
Epoch: [3][ 70/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 3.6314e+00 (3.5822e+00)	Acc@1  16.41 ( 14.28)	Acc@5  39.84 ( 39.45)
Epoch: [3][ 80/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 3.6603e+00 (3.5827e+00)	Acc@1  10.94 ( 14.20)	Acc@5  40.62 ( 39.37)
Epoch: [3][ 90/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.003)	Loss 3.6091e+00 (3.5730e+00)	Acc@1  13.28 ( 14.31)	Acc@5  39.84 ( 39.73)
Epoch: [3][100/391]	Time  0.243 ( 0.244)	Data  0.001 ( 0.003)	Loss 3.4547e+00 (3.5706e+00)	Acc@1  17.97 ( 14.40)	Acc@5  46.88 ( 39.87)
Epoch: [3][110/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.5616e+00 (3.5656e+00)	Acc@1  17.19 ( 14.46)	Acc@5  38.28 ( 39.95)
Epoch: [3][120/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.2783e+00 (3.5627e+00)	Acc@1  20.31 ( 14.51)	Acc@5  45.31 ( 40.13)
Epoch: [3][130/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.4461e+00 (3.5586e+00)	Acc@1  16.41 ( 14.61)	Acc@5  44.53 ( 40.29)
Epoch: [3][140/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.7125e+00 (3.5570e+00)	Acc@1  10.16 ( 14.66)	Acc@5  34.38 ( 40.40)
Epoch: [3][150/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.5232e+00 (3.5534e+00)	Acc@1  16.41 ( 14.80)	Acc@5  41.41 ( 40.46)
Epoch: [3][160/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.3531e+00 (3.5486e+00)	Acc@1  19.53 ( 14.89)	Acc@5  45.31 ( 40.60)
Epoch: [3][170/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.4895e+00 (3.5472e+00)	Acc@1  13.28 ( 14.85)	Acc@5  43.75 ( 40.65)
Epoch: [3][180/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.5680e+00 (3.5451e+00)	Acc@1  14.06 ( 14.85)	Acc@5  41.41 ( 40.73)
Epoch: [3][190/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.5428e+00 (3.5376e+00)	Acc@1  17.97 ( 14.97)	Acc@5  39.84 ( 40.92)
Epoch: [3][200/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.1177e+00 (3.5304e+00)	Acc@1  27.34 ( 15.09)	Acc@5  53.91 ( 41.11)
Epoch: [3][210/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.4795e+00 (3.5287e+00)	Acc@1  17.97 ( 15.18)	Acc@5  47.66 ( 41.21)
Epoch: [3][220/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.4201e+00 (3.5242e+00)	Acc@1  17.19 ( 15.24)	Acc@5  44.53 ( 41.31)
Epoch: [3][230/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.4883e+00 (3.5235e+00)	Acc@1  13.28 ( 15.25)	Acc@5  46.88 ( 41.35)
Epoch: [3][240/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.2744e+00 (3.5171e+00)	Acc@1  19.53 ( 15.39)	Acc@5  50.00 ( 41.51)
Epoch: [3][250/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.4052e+00 (3.5132e+00)	Acc@1  18.75 ( 15.43)	Acc@5  41.41 ( 41.60)
Epoch: [3][260/391]	Time  0.246 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.4235e+00 (3.5107e+00)	Acc@1  20.31 ( 15.46)	Acc@5  42.97 ( 41.62)
Epoch: [3][270/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.4466e+00 (3.5058e+00)	Acc@1  16.41 ( 15.48)	Acc@5  45.31 ( 41.76)
Epoch: [3][280/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.5010e+00 (3.5014e+00)	Acc@1  16.41 ( 15.54)	Acc@5  39.06 ( 41.87)
Epoch: [3][290/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.5436e+00 (3.4981e+00)	Acc@1  15.62 ( 15.61)	Acc@5  41.41 ( 41.95)
Epoch: [3][300/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.2858e+00 (3.4951e+00)	Acc@1  15.62 ( 15.69)	Acc@5  48.44 ( 42.03)
Epoch: [3][310/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.5182e+00 (3.4922e+00)	Acc@1  16.41 ( 15.73)	Acc@5  39.84 ( 42.07)
Epoch: [3][320/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.4345e+00 (3.4880e+00)	Acc@1  17.19 ( 15.81)	Acc@5  41.41 ( 42.20)
Epoch: [3][330/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.2731e+00 (3.4838e+00)	Acc@1  21.09 ( 15.90)	Acc@5  50.00 ( 42.36)
Epoch: [3][340/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.4147e+00 (3.4800e+00)	Acc@1  16.41 ( 15.94)	Acc@5  48.44 ( 42.50)
Epoch: [3][350/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.2666e+00 (3.4751e+00)	Acc@1  19.53 ( 16.01)	Acc@5  50.78 ( 42.65)
Epoch: [3][360/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.3831e+00 (3.4702e+00)	Acc@1  17.97 ( 16.10)	Acc@5  45.31 ( 42.84)
Epoch: [3][370/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.3931e+00 (3.4673e+00)	Acc@1  13.28 ( 16.14)	Acc@5  42.19 ( 42.94)
Epoch: [3][380/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.4053e+00 (3.4642e+00)	Acc@1  16.41 ( 16.16)	Acc@5  47.66 ( 43.06)
Epoch: [3][390/391]	Time  0.179 ( 0.243)	Data  0.001 ( 0.001)	Loss 3.2246e+00 (3.4604e+00)	Acc@1  22.50 ( 16.25)	Acc@5  52.50 ( 43.17)
## e[3] optimizer.zero_grad (sum) time: 0.49048757553100586
## e[3]       loss.backward (sum) time: 11.042630672454834
## e[3]      optimizer.step (sum) time: 48.033884048461914
## epoch[3] training(only) time: 95.03893828392029
# Switched to evaluate mode...
Test: [  0/100]	Time  0.209 ( 0.209)	Loss 3.1598e+00 (3.1598e+00)	Acc@1  23.00 ( 23.00)	Acc@5  50.00 ( 50.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 3.3179e+00 (3.2532e+00)	Acc@1  12.00 ( 20.64)	Acc@5  46.00 ( 48.09)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 3.0432e+00 (3.2295e+00)	Acc@1  24.00 ( 21.29)	Acc@5  58.00 ( 49.19)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 3.3256e+00 (3.2293e+00)	Acc@1  22.00 ( 21.16)	Acc@5  43.00 ( 48.94)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 3.2448e+00 (3.2220e+00)	Acc@1  18.00 ( 21.02)	Acc@5  50.00 ( 48.98)
Test: [ 50/100]	Time  0.079 ( 0.081)	Loss 3.1400e+00 (3.2242e+00)	Acc@1  26.00 ( 21.10)	Acc@5  54.00 ( 49.35)
Test: [ 60/100]	Time  0.077 ( 0.080)	Loss 3.0267e+00 (3.2256e+00)	Acc@1  26.00 ( 21.18)	Acc@5  57.00 ( 49.18)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 3.3593e+00 (3.2243e+00)	Acc@1  16.00 ( 20.97)	Acc@5  49.00 ( 49.27)
Test: [ 80/100]	Time  0.077 ( 0.080)	Loss 3.4043e+00 (3.2330e+00)	Acc@1  21.00 ( 20.78)	Acc@5  47.00 ( 49.20)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 3.2239e+00 (3.2232e+00)	Acc@1  23.00 ( 20.85)	Acc@5  51.00 ( 49.54)
 * Acc@1 20.800 Acc@5 49.540
### epoch[3] execution time: 103.05608916282654
EPOCH 4
# current learning rate: 0.1
# Switched to train mode...
Epoch: [4][  0/391]	Time  0.387 ( 0.387)	Data  0.146 ( 0.146)	Loss 3.2950e+00 (3.2950e+00)	Acc@1  17.19 ( 17.19)	Acc@5  47.66 ( 47.66)
Epoch: [4][ 10/391]	Time  0.240 ( 0.254)	Data  0.001 ( 0.014)	Loss 3.1314e+00 (3.2559e+00)	Acc@1  22.66 ( 19.32)	Acc@5  52.34 ( 48.58)
Epoch: [4][ 20/391]	Time  0.240 ( 0.248)	Data  0.001 ( 0.008)	Loss 3.1341e+00 (3.2633e+00)	Acc@1  22.66 ( 19.61)	Acc@5  50.00 ( 49.26)
Epoch: [4][ 30/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.006)	Loss 3.1112e+00 (3.2649e+00)	Acc@1  21.88 ( 19.56)	Acc@5  52.34 ( 49.42)
Epoch: [4][ 40/391]	Time  0.239 ( 0.244)	Data  0.001 ( 0.005)	Loss 3.1184e+00 (3.2730e+00)	Acc@1  16.41 ( 19.26)	Acc@5  53.91 ( 49.35)
Epoch: [4][ 50/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 3.2063e+00 (3.2854e+00)	Acc@1  18.75 ( 18.90)	Acc@5  51.56 ( 48.99)
Epoch: [4][ 60/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.004)	Loss 3.4426e+00 (3.2881e+00)	Acc@1  14.06 ( 18.79)	Acc@5  45.31 ( 48.81)
Epoch: [4][ 70/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.1427e+00 (3.2781e+00)	Acc@1  28.91 ( 19.19)	Acc@5  51.56 ( 48.93)
Epoch: [4][ 80/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.9175e+00 (3.2657e+00)	Acc@1  32.03 ( 19.34)	Acc@5  53.91 ( 49.04)
Epoch: [4][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.3353e+00 (3.2658e+00)	Acc@1  22.66 ( 19.42)	Acc@5  47.66 ( 49.05)
Epoch: [4][100/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.4099e+00 (3.2664e+00)	Acc@1  21.88 ( 19.53)	Acc@5  51.56 ( 49.07)
Epoch: [4][110/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.3913e+00 (3.2614e+00)	Acc@1  20.31 ( 19.76)	Acc@5  49.22 ( 49.17)
Epoch: [4][120/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.0908e+00 (3.2571e+00)	Acc@1  28.12 ( 19.89)	Acc@5  51.56 ( 49.16)
Epoch: [4][130/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.3442e+00 (3.2619e+00)	Acc@1  21.09 ( 19.86)	Acc@5  49.22 ( 48.94)
Epoch: [4][140/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.1738e+00 (3.2523e+00)	Acc@1  20.31 ( 19.99)	Acc@5  57.03 ( 49.21)
Epoch: [4][150/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.1725e+00 (3.2507e+00)	Acc@1  27.34 ( 20.05)	Acc@5  50.78 ( 49.15)
Epoch: [4][160/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.9047e+00 (3.2444e+00)	Acc@1  29.69 ( 20.26)	Acc@5  54.69 ( 49.22)
Epoch: [4][170/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.1361e+00 (3.2375e+00)	Acc@1  24.22 ( 20.43)	Acc@5  50.78 ( 49.40)
Epoch: [4][180/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.1414e+00 (3.2327e+00)	Acc@1  25.00 ( 20.47)	Acc@5  53.12 ( 49.56)
Epoch: [4][190/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.2119e+00 (3.2325e+00)	Acc@1  25.78 ( 20.54)	Acc@5  48.44 ( 49.47)
Epoch: [4][200/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.3494e+00 (3.2286e+00)	Acc@1  20.31 ( 20.61)	Acc@5  46.88 ( 49.59)
Epoch: [4][210/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.3102e+00 (3.2246e+00)	Acc@1  20.31 ( 20.72)	Acc@5  44.53 ( 49.59)
Epoch: [4][220/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.0222e+00 (3.2190e+00)	Acc@1  28.12 ( 20.89)	Acc@5  50.78 ( 49.69)
Epoch: [4][230/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.1899e+00 (3.2156e+00)	Acc@1  21.88 ( 20.99)	Acc@5  53.91 ( 49.76)
Epoch: [4][240/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.9069e+00 (3.2099e+00)	Acc@1  33.59 ( 21.11)	Acc@5  57.03 ( 49.92)
Epoch: [4][250/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.1321e+00 (3.2057e+00)	Acc@1  17.19 ( 21.18)	Acc@5  49.22 ( 50.02)
Epoch: [4][260/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.1891e+00 (3.2034e+00)	Acc@1  19.53 ( 21.23)	Acc@5  52.34 ( 50.08)
Epoch: [4][270/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.9625e+00 (3.1981e+00)	Acc@1  22.66 ( 21.31)	Acc@5  54.69 ( 50.26)
Epoch: [4][280/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.0805e+00 (3.1949e+00)	Acc@1  19.53 ( 21.36)	Acc@5  54.69 ( 50.33)
Epoch: [4][290/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.2002e+00 (3.1954e+00)	Acc@1  17.19 ( 21.32)	Acc@5  45.31 ( 50.30)
Epoch: [4][300/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8954e+00 (3.1936e+00)	Acc@1  26.56 ( 21.38)	Acc@5  55.47 ( 50.31)
Epoch: [4][310/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.9479e+00 (3.1877e+00)	Acc@1  25.00 ( 21.50)	Acc@5  53.12 ( 50.49)
Epoch: [4][320/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.9109e+00 (3.1831e+00)	Acc@1  27.34 ( 21.57)	Acc@5  53.91 ( 50.58)
Epoch: [4][330/391]	Time  0.248 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.2491e+00 (3.1807e+00)	Acc@1  21.09 ( 21.60)	Acc@5  51.56 ( 50.64)
Epoch: [4][340/391]	Time  0.254 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.9009e+00 (3.1774e+00)	Acc@1  33.59 ( 21.71)	Acc@5  56.25 ( 50.73)
Epoch: [4][350/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.9226e+00 (3.1746e+00)	Acc@1  25.78 ( 21.75)	Acc@5  56.25 ( 50.81)
Epoch: [4][360/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8548e+00 (3.1722e+00)	Acc@1  28.91 ( 21.76)	Acc@5  59.38 ( 50.90)
Epoch: [4][370/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.0038e+00 (3.1698e+00)	Acc@1  24.22 ( 21.77)	Acc@5  52.34 ( 50.96)
Epoch: [4][380/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.1542e+00 (3.1689e+00)	Acc@1  26.56 ( 21.80)	Acc@5  49.22 ( 50.98)
Epoch: [4][390/391]	Time  0.178 ( 0.242)	Data  0.001 ( 0.001)	Loss 2.9934e+00 (3.1669e+00)	Acc@1  21.25 ( 21.84)	Acc@5  51.25 ( 51.04)
## e[4] optimizer.zero_grad (sum) time: 0.4929640293121338
## e[4]       loss.backward (sum) time: 11.067845821380615
## e[4]      optimizer.step (sum) time: 47.92225098609924
## epoch[4] training(only) time: 94.73690152168274
# Switched to evaluate mode...
Test: [  0/100]	Time  0.212 ( 0.212)	Loss 2.9756e+00 (2.9756e+00)	Acc@1  26.00 ( 26.00)	Acc@5  57.00 ( 57.00)
Test: [ 10/100]	Time  0.077 ( 0.090)	Loss 3.1162e+00 (2.9394e+00)	Acc@1  23.00 ( 27.27)	Acc@5  53.00 ( 58.36)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 2.7177e+00 (2.9058e+00)	Acc@1  31.00 ( 27.62)	Acc@5  59.00 ( 58.90)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 3.0173e+00 (2.9024e+00)	Acc@1  29.00 ( 27.90)	Acc@5  54.00 ( 59.03)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 2.8631e+00 (2.9087e+00)	Acc@1  30.00 ( 28.05)	Acc@5  57.00 ( 58.78)
Test: [ 50/100]	Time  0.077 ( 0.080)	Loss 2.8603e+00 (2.9149e+00)	Acc@1  28.00 ( 27.94)	Acc@5  56.00 ( 58.67)
Test: [ 60/100]	Time  0.079 ( 0.080)	Loss 3.0258e+00 (2.9233e+00)	Acc@1  29.00 ( 27.89)	Acc@5  64.00 ( 58.30)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 3.1844e+00 (2.9258e+00)	Acc@1  23.00 ( 27.79)	Acc@5  53.00 ( 58.28)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 3.0523e+00 (2.9323e+00)	Acc@1  25.00 ( 27.67)	Acc@5  52.00 ( 58.07)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 2.9256e+00 (2.9309e+00)	Acc@1  33.00 ( 27.91)	Acc@5  60.00 ( 58.00)
 * Acc@1 27.790 Acc@5 58.070
### epoch[4] execution time: 102.74920773506165
EPOCH 5
# current learning rate: 0.1
# Switched to train mode...
Epoch: [5][  0/391]	Time  0.386 ( 0.386)	Data  0.144 ( 0.144)	Loss 2.9396e+00 (2.9396e+00)	Acc@1  24.22 ( 24.22)	Acc@5  57.81 ( 57.81)
Epoch: [5][ 10/391]	Time  0.247 ( 0.254)	Data  0.001 ( 0.014)	Loss 3.0725e+00 (2.9358e+00)	Acc@1  21.09 ( 26.78)	Acc@5  56.25 ( 58.24)
Epoch: [5][ 20/391]	Time  0.240 ( 0.248)	Data  0.001 ( 0.008)	Loss 3.1323e+00 (2.9404e+00)	Acc@1  24.22 ( 26.82)	Acc@5  53.12 ( 57.03)
Epoch: [5][ 30/391]	Time  0.242 ( 0.246)	Data  0.001 ( 0.006)	Loss 3.1825e+00 (2.9674e+00)	Acc@1  24.22 ( 26.34)	Acc@5  50.78 ( 56.22)
Epoch: [5][ 40/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.005)	Loss 2.8321e+00 (2.9722e+00)	Acc@1  29.69 ( 26.37)	Acc@5  59.38 ( 56.00)
Epoch: [5][ 50/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 3.0998e+00 (2.9753e+00)	Acc@1  21.09 ( 26.04)	Acc@5  53.12 ( 55.88)
Epoch: [5][ 60/391]	Time  0.244 ( 0.244)	Data  0.001 ( 0.004)	Loss 2.6166e+00 (2.9599e+00)	Acc@1  32.03 ( 26.19)	Acc@5  64.84 ( 56.88)
Epoch: [5][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.1591e+00 (2.9558e+00)	Acc@1  21.88 ( 26.08)	Acc@5  49.22 ( 56.83)
Epoch: [5][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.0051e+00 (2.9553e+00)	Acc@1  25.78 ( 25.98)	Acc@5  49.22 ( 56.77)
Epoch: [5][ 90/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.9792e+00 (2.9577e+00)	Acc@1  32.81 ( 25.94)	Acc@5  56.25 ( 56.64)
Epoch: [5][100/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.9121e+00 (2.9560e+00)	Acc@1  21.88 ( 25.91)	Acc@5  60.16 ( 56.55)
Epoch: [5][110/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.0046e+00 (2.9499e+00)	Acc@1  25.78 ( 26.08)	Acc@5  50.78 ( 56.64)
Epoch: [5][120/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.8129e+00 (2.9350e+00)	Acc@1  32.03 ( 26.41)	Acc@5  59.38 ( 56.98)
Epoch: [5][130/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.7873e+00 (2.9321e+00)	Acc@1  25.00 ( 26.35)	Acc@5  56.25 ( 56.97)
Epoch: [5][140/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.0235e+00 (2.9293e+00)	Acc@1  25.78 ( 26.48)	Acc@5  56.25 ( 57.05)
Epoch: [5][150/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.1626e+00 (2.9356e+00)	Acc@1  27.34 ( 26.37)	Acc@5  50.00 ( 56.90)
Epoch: [5][160/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.9748e+00 (2.9326e+00)	Acc@1  22.66 ( 26.40)	Acc@5  58.59 ( 57.02)
Epoch: [5][170/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.9082e+00 (2.9327e+00)	Acc@1  24.22 ( 26.38)	Acc@5  56.25 ( 56.99)
Epoch: [5][180/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.9442e+00 (2.9317e+00)	Acc@1  21.88 ( 26.43)	Acc@5  61.72 ( 57.05)
Epoch: [5][190/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.7096e+00 (2.9317e+00)	Acc@1  32.81 ( 26.42)	Acc@5  62.50 ( 57.03)
Epoch: [5][200/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.6566e+00 (2.9245e+00)	Acc@1  32.03 ( 26.55)	Acc@5  62.50 ( 57.25)
Epoch: [5][210/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8830e+00 (2.9203e+00)	Acc@1  25.00 ( 26.63)	Acc@5  53.91 ( 57.34)
Epoch: [5][220/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6074e+00 (2.9170e+00)	Acc@1  27.34 ( 26.67)	Acc@5  63.28 ( 57.40)
Epoch: [5][230/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.9753e+00 (2.9141e+00)	Acc@1  26.56 ( 26.70)	Acc@5  58.59 ( 57.50)
Epoch: [5][240/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.9354e+00 (2.9124e+00)	Acc@1  21.09 ( 26.71)	Acc@5  57.03 ( 57.54)
Epoch: [5][250/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.5912e+00 (2.9067e+00)	Acc@1  30.47 ( 26.76)	Acc@5  71.09 ( 57.71)
Epoch: [5][260/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.7094e+00 (2.9011e+00)	Acc@1  33.59 ( 26.86)	Acc@5  60.16 ( 57.82)
Epoch: [5][270/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.0852e+00 (2.8976e+00)	Acc@1  26.56 ( 26.95)	Acc@5  55.47 ( 57.92)
Epoch: [5][280/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8756e+00 (2.8956e+00)	Acc@1  21.09 ( 26.95)	Acc@5  64.06 ( 57.95)
Epoch: [5][290/391]	Time  0.246 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6858e+00 (2.8932e+00)	Acc@1  34.38 ( 27.05)	Acc@5  62.50 ( 57.98)
Epoch: [5][300/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6445e+00 (2.8890e+00)	Acc@1  33.59 ( 27.16)	Acc@5  66.41 ( 58.07)
Epoch: [5][310/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6185e+00 (2.8866e+00)	Acc@1  29.69 ( 27.19)	Acc@5  61.72 ( 58.15)
Epoch: [5][320/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8147e+00 (2.8858e+00)	Acc@1  25.00 ( 27.24)	Acc@5  64.06 ( 58.24)
Epoch: [5][330/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.9324e+00 (2.8844e+00)	Acc@1  24.22 ( 27.28)	Acc@5  60.94 ( 58.31)
Epoch: [5][340/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8205e+00 (2.8819e+00)	Acc@1  25.00 ( 27.30)	Acc@5  59.38 ( 58.38)
Epoch: [5][350/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4737e+00 (2.8782e+00)	Acc@1  35.94 ( 27.36)	Acc@5  69.53 ( 58.52)
Epoch: [5][360/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.9179e+00 (2.8768e+00)	Acc@1  21.88 ( 27.40)	Acc@5  53.91 ( 58.53)
Epoch: [5][370/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.7019e+00 (2.8732e+00)	Acc@1  31.25 ( 27.45)	Acc@5  59.38 ( 58.60)
Epoch: [5][380/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.7624e+00 (2.8691e+00)	Acc@1  32.81 ( 27.53)	Acc@5  63.28 ( 58.68)
Epoch: [5][390/391]	Time  0.178 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.7295e+00 (2.8653e+00)	Acc@1  28.75 ( 27.57)	Acc@5  62.50 ( 58.74)
## e[5] optimizer.zero_grad (sum) time: 0.4940006732940674
## e[5]       loss.backward (sum) time: 11.068202495574951
## e[5]      optimizer.step (sum) time: 47.893068075180054
## epoch[5] training(only) time: 94.8130259513855
# Switched to evaluate mode...
Test: [  0/100]	Time  0.206 ( 0.206)	Loss 2.5372e+00 (2.5372e+00)	Acc@1  30.00 ( 30.00)	Acc@5  70.00 ( 70.00)
Test: [ 10/100]	Time  0.078 ( 0.089)	Loss 2.8560e+00 (2.7447e+00)	Acc@1  30.00 ( 31.64)	Acc@5  66.00 ( 62.18)
Test: [ 20/100]	Time  0.077 ( 0.084)	Loss 2.4458e+00 (2.7492e+00)	Acc@1  37.00 ( 31.52)	Acc@5  70.00 ( 62.86)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 2.8670e+00 (2.7396e+00)	Acc@1  31.00 ( 31.55)	Acc@5  60.00 ( 63.00)
Test: [ 40/100]	Time  0.077 ( 0.081)	Loss 2.8318e+00 (2.7407e+00)	Acc@1  27.00 ( 31.56)	Acc@5  61.00 ( 62.80)
Test: [ 50/100]	Time  0.077 ( 0.080)	Loss 2.5503e+00 (2.7401e+00)	Acc@1  35.00 ( 31.49)	Acc@5  67.00 ( 62.71)
Test: [ 60/100]	Time  0.077 ( 0.080)	Loss 2.5757e+00 (2.7367e+00)	Acc@1  32.00 ( 31.43)	Acc@5  67.00 ( 62.51)
Test: [ 70/100]	Time  0.077 ( 0.080)	Loss 2.9117e+00 (2.7422e+00)	Acc@1  22.00 ( 31.21)	Acc@5  64.00 ( 62.39)
Test: [ 80/100]	Time  0.079 ( 0.079)	Loss 2.9419e+00 (2.7557e+00)	Acc@1  21.00 ( 30.78)	Acc@5  56.00 ( 62.15)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 2.6150e+00 (2.7476e+00)	Acc@1  36.00 ( 30.87)	Acc@5  70.00 ( 62.44)
 * Acc@1 30.870 Acc@5 62.310
### epoch[5] execution time: 102.81166553497314
EPOCH 6
# current learning rate: 0.1
# Switched to train mode...
Epoch: [6][  0/391]	Time  0.402 ( 0.402)	Data  0.150 ( 0.150)	Loss 2.6341e+00 (2.6341e+00)	Acc@1  27.34 ( 27.34)	Acc@5  67.97 ( 67.97)
Epoch: [6][ 10/391]	Time  0.241 ( 0.256)	Data  0.001 ( 0.015)	Loss 2.5202e+00 (2.6489e+00)	Acc@1  33.59 ( 31.32)	Acc@5  65.62 ( 64.42)
Epoch: [6][ 20/391]	Time  0.242 ( 0.249)	Data  0.001 ( 0.008)	Loss 2.5183e+00 (2.6388e+00)	Acc@1  34.38 ( 31.40)	Acc@5  60.94 ( 64.55)
Epoch: [6][ 30/391]	Time  0.242 ( 0.247)	Data  0.001 ( 0.006)	Loss 2.8492e+00 (2.6454e+00)	Acc@1  25.78 ( 31.10)	Acc@5  59.38 ( 64.74)
Epoch: [6][ 40/391]	Time  0.242 ( 0.245)	Data  0.001 ( 0.005)	Loss 2.4685e+00 (2.6562e+00)	Acc@1  40.62 ( 30.91)	Acc@5  63.28 ( 64.16)
Epoch: [6][ 50/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 2.6109e+00 (2.6487e+00)	Acc@1  27.34 ( 31.05)	Acc@5  60.94 ( 64.08)
Epoch: [6][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 2.2781e+00 (2.6391e+00)	Acc@1  41.41 ( 31.33)	Acc@5  72.66 ( 64.32)
Epoch: [6][ 70/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 2.6937e+00 (2.6417e+00)	Acc@1  31.25 ( 31.51)	Acc@5  68.75 ( 64.35)
Epoch: [6][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.9842e+00 (2.6477e+00)	Acc@1  28.12 ( 31.58)	Acc@5  54.69 ( 64.05)
Epoch: [6][ 90/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.7147e+00 (2.6558e+00)	Acc@1  29.69 ( 31.41)	Acc@5  62.50 ( 64.00)
Epoch: [6][100/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.5539e+00 (2.6604e+00)	Acc@1  37.50 ( 31.44)	Acc@5  65.62 ( 63.72)
Epoch: [6][110/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.8602e+00 (2.6603e+00)	Acc@1  26.56 ( 31.25)	Acc@5  59.38 ( 63.71)
Epoch: [6][120/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.6505e+00 (2.6560e+00)	Acc@1  37.50 ( 31.37)	Acc@5  61.72 ( 63.72)
Epoch: [6][130/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.5414e+00 (2.6499e+00)	Acc@1  34.38 ( 31.56)	Acc@5  66.41 ( 63.79)
Epoch: [6][140/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.5580e+00 (2.6461e+00)	Acc@1  32.81 ( 31.66)	Acc@5  66.41 ( 63.87)
Epoch: [6][150/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4066e+00 (2.6433e+00)	Acc@1  38.28 ( 31.68)	Acc@5  69.53 ( 63.95)
Epoch: [6][160/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.5386e+00 (2.6442e+00)	Acc@1  34.38 ( 31.63)	Acc@5  60.94 ( 64.00)
Epoch: [6][170/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6932e+00 (2.6409e+00)	Acc@1  33.59 ( 31.73)	Acc@5  61.72 ( 64.05)
Epoch: [6][180/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2668e+00 (2.6375e+00)	Acc@1  42.19 ( 31.85)	Acc@5  71.88 ( 64.11)
Epoch: [6][190/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8587e+00 (2.6353e+00)	Acc@1  27.34 ( 31.90)	Acc@5  57.81 ( 64.16)
Epoch: [6][200/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.7157e+00 (2.6346e+00)	Acc@1  32.81 ( 31.93)	Acc@5  59.38 ( 64.13)
Epoch: [6][210/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6370e+00 (2.6311e+00)	Acc@1  35.16 ( 32.04)	Acc@5  70.31 ( 64.27)
Epoch: [6][220/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6111e+00 (2.6320e+00)	Acc@1  29.69 ( 32.01)	Acc@5  61.72 ( 64.25)
Epoch: [6][230/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.5350e+00 (2.6291e+00)	Acc@1  28.91 ( 32.09)	Acc@5  68.75 ( 64.34)
Epoch: [6][240/391]	Time  0.247 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8308e+00 (2.6280e+00)	Acc@1  34.38 ( 32.21)	Acc@5  60.16 ( 64.36)
Epoch: [6][250/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.5366e+00 (2.6223e+00)	Acc@1  42.97 ( 32.34)	Acc@5  70.31 ( 64.51)
Epoch: [6][260/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4760e+00 (2.6190e+00)	Acc@1  35.16 ( 32.43)	Acc@5  66.41 ( 64.67)
Epoch: [6][270/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3521e+00 (2.6155e+00)	Acc@1  36.72 ( 32.53)	Acc@5  74.22 ( 64.77)
Epoch: [6][280/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6089e+00 (2.6111e+00)	Acc@1  30.47 ( 32.65)	Acc@5  62.50 ( 64.85)
Epoch: [6][290/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.5393e+00 (2.6088e+00)	Acc@1  39.06 ( 32.73)	Acc@5  64.06 ( 64.88)
Epoch: [6][300/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4001e+00 (2.6067e+00)	Acc@1  35.16 ( 32.81)	Acc@5  71.09 ( 64.95)
Epoch: [6][310/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6100e+00 (2.6059e+00)	Acc@1  34.38 ( 32.86)	Acc@5  68.75 ( 64.93)
Epoch: [6][320/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.5519e+00 (2.6035e+00)	Acc@1  35.16 ( 32.92)	Acc@5  67.97 ( 64.96)
Epoch: [6][330/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.5245e+00 (2.5992e+00)	Acc@1  32.03 ( 33.00)	Acc@5  64.06 ( 65.09)
Epoch: [6][340/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.5493e+00 (2.5961e+00)	Acc@1  30.47 ( 33.04)	Acc@5  65.62 ( 65.19)
Epoch: [6][350/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4360e+00 (2.5937e+00)	Acc@1  37.50 ( 33.10)	Acc@5  69.53 ( 65.24)
Epoch: [6][360/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4425e+00 (2.5924e+00)	Acc@1  35.94 ( 33.11)	Acc@5  66.41 ( 65.27)
Epoch: [6][370/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6377e+00 (2.5891e+00)	Acc@1  32.81 ( 33.19)	Acc@5  61.72 ( 65.34)
Epoch: [6][380/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4785e+00 (2.5863e+00)	Acc@1  35.94 ( 33.27)	Acc@5  67.97 ( 65.40)
Epoch: [6][390/391]	Time  0.181 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0889e+00 (2.5835e+00)	Acc@1  38.75 ( 33.30)	Acc@5  81.25 ( 65.46)
## e[6] optimizer.zero_grad (sum) time: 0.49204468727111816
## e[6]       loss.backward (sum) time: 11.075896501541138
## e[6]      optimizer.step (sum) time: 47.88150358200073
## epoch[6] training(only) time: 94.73133397102356
# Switched to evaluate mode...
Test: [  0/100]	Time  0.208 ( 0.208)	Loss 2.3487e+00 (2.3487e+00)	Acc@1  35.00 ( 35.00)	Acc@5  71.00 ( 71.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 2.7874e+00 (2.4837e+00)	Acc@1  30.00 ( 37.45)	Acc@5  66.00 ( 69.09)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 2.0743e+00 (2.4213e+00)	Acc@1  44.00 ( 38.67)	Acc@5  76.00 ( 69.62)
Test: [ 30/100]	Time  0.079 ( 0.082)	Loss 2.5246e+00 (2.4242e+00)	Acc@1  37.00 ( 38.65)	Acc@5  64.00 ( 69.58)
Test: [ 40/100]	Time  0.077 ( 0.081)	Loss 2.6658e+00 (2.4366e+00)	Acc@1  33.00 ( 38.12)	Acc@5  65.00 ( 69.78)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 2.3324e+00 (2.4567e+00)	Acc@1  41.00 ( 38.35)	Acc@5  72.00 ( 69.67)
Test: [ 60/100]	Time  0.077 ( 0.080)	Loss 2.3981e+00 (2.4588e+00)	Acc@1  42.00 ( 38.05)	Acc@5  69.00 ( 69.41)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 2.6634e+00 (2.4638e+00)	Acc@1  35.00 ( 38.11)	Acc@5  68.00 ( 69.24)
Test: [ 80/100]	Time  0.077 ( 0.080)	Loss 2.3676e+00 (2.4648e+00)	Acc@1  33.00 ( 37.95)	Acc@5  69.00 ( 69.21)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 2.3824e+00 (2.4521e+00)	Acc@1  38.00 ( 38.23)	Acc@5  73.00 ( 69.56)
 * Acc@1 38.240 Acc@5 69.510
### epoch[6] execution time: 102.76859951019287
EPOCH 7
# current learning rate: 0.1
# Switched to train mode...
Epoch: [7][  0/391]	Time  0.391 ( 0.391)	Data  0.149 ( 0.149)	Loss 2.1568e+00 (2.1568e+00)	Acc@1  43.75 ( 43.75)	Acc@5  75.78 ( 75.78)
Epoch: [7][ 10/391]	Time  0.240 ( 0.255)	Data  0.001 ( 0.015)	Loss 2.3671e+00 (2.4486e+00)	Acc@1  36.72 ( 37.36)	Acc@5  67.97 ( 68.18)
Epoch: [7][ 20/391]	Time  0.242 ( 0.249)	Data  0.001 ( 0.008)	Loss 2.1336e+00 (2.3805e+00)	Acc@1  44.53 ( 38.13)	Acc@5  75.00 ( 69.79)
Epoch: [7][ 30/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.006)	Loss 2.3561e+00 (2.3892e+00)	Acc@1  35.16 ( 37.78)	Acc@5  66.41 ( 69.23)
Epoch: [7][ 40/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.005)	Loss 2.2677e+00 (2.3978e+00)	Acc@1  36.72 ( 37.56)	Acc@5  70.31 ( 69.30)
Epoch: [7][ 50/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.004)	Loss 2.5007e+00 (2.4175e+00)	Acc@1  36.72 ( 36.98)	Acc@5  65.62 ( 68.77)
Epoch: [7][ 60/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.003)	Loss 2.3014e+00 (2.4202e+00)	Acc@1  34.38 ( 36.94)	Acc@5  71.88 ( 68.83)
Epoch: [7][ 70/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 2.4376e+00 (2.4267e+00)	Acc@1  35.16 ( 36.75)	Acc@5  72.66 ( 68.76)
Epoch: [7][ 80/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.3850e+00 (2.4174e+00)	Acc@1  39.84 ( 36.94)	Acc@5  70.31 ( 69.04)
Epoch: [7][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.3008e+00 (2.4192e+00)	Acc@1  39.84 ( 36.89)	Acc@5  66.41 ( 68.79)
Epoch: [7][100/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.6494e+00 (2.4249e+00)	Acc@1  33.59 ( 36.73)	Acc@5  62.50 ( 68.65)
Epoch: [7][110/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.3936e+00 (2.4312e+00)	Acc@1  38.28 ( 36.62)	Acc@5  71.88 ( 68.63)
Epoch: [7][120/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.1410e+00 (2.4226e+00)	Acc@1  42.19 ( 36.52)	Acc@5  70.31 ( 68.86)
Epoch: [7][130/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0991e+00 (2.4166e+00)	Acc@1  49.22 ( 36.67)	Acc@5  75.00 ( 69.10)
Epoch: [7][140/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3051e+00 (2.4141e+00)	Acc@1  39.84 ( 36.64)	Acc@5  70.31 ( 69.28)
Epoch: [7][150/391]	Time  0.254 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0557e+00 (2.4159e+00)	Acc@1  47.66 ( 36.70)	Acc@5  72.66 ( 69.10)
Epoch: [7][160/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2028e+00 (2.4112e+00)	Acc@1  45.31 ( 36.81)	Acc@5  75.78 ( 69.21)
Epoch: [7][170/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4216e+00 (2.4105e+00)	Acc@1  33.59 ( 36.81)	Acc@5  65.62 ( 69.32)
Epoch: [7][180/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1858e+00 (2.4066e+00)	Acc@1  46.88 ( 36.89)	Acc@5  74.22 ( 69.46)
Epoch: [7][190/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2382e+00 (2.4007e+00)	Acc@1  37.50 ( 36.92)	Acc@5  75.00 ( 69.57)
Epoch: [7][200/391]	Time  0.250 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3262e+00 (2.3965e+00)	Acc@1  36.72 ( 37.06)	Acc@5  68.75 ( 69.67)
Epoch: [7][210/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3126e+00 (2.3962e+00)	Acc@1  39.84 ( 37.07)	Acc@5  73.44 ( 69.67)
Epoch: [7][220/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4313e+00 (2.3940e+00)	Acc@1  39.84 ( 37.17)	Acc@5  68.75 ( 69.67)
Epoch: [7][230/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2480e+00 (2.3931e+00)	Acc@1  33.59 ( 37.15)	Acc@5  73.44 ( 69.70)
Epoch: [7][240/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4161e+00 (2.3929e+00)	Acc@1  37.50 ( 37.23)	Acc@5  65.62 ( 69.69)
Epoch: [7][250/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4117e+00 (2.3923e+00)	Acc@1  41.41 ( 37.27)	Acc@5  71.88 ( 69.69)
Epoch: [7][260/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0550e+00 (2.3871e+00)	Acc@1  45.31 ( 37.39)	Acc@5  78.91 ( 69.81)
Epoch: [7][270/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2440e+00 (2.3849e+00)	Acc@1  38.28 ( 37.43)	Acc@5  70.31 ( 69.83)
Epoch: [7][280/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4558e+00 (2.3827e+00)	Acc@1  33.59 ( 37.54)	Acc@5  67.97 ( 69.83)
Epoch: [7][290/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.5327e+00 (2.3836e+00)	Acc@1  32.03 ( 37.54)	Acc@5  71.09 ( 69.83)
Epoch: [7][300/391]	Time  0.248 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3775e+00 (2.3820e+00)	Acc@1  40.62 ( 37.55)	Acc@5  70.31 ( 69.90)
Epoch: [7][310/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4804e+00 (2.3808e+00)	Acc@1  35.16 ( 37.56)	Acc@5  65.62 ( 69.96)
Epoch: [7][320/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4127e+00 (2.3791e+00)	Acc@1  35.94 ( 37.62)	Acc@5  66.41 ( 69.95)
Epoch: [7][330/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2777e+00 (2.3772e+00)	Acc@1  42.19 ( 37.65)	Acc@5  74.22 ( 70.00)
Epoch: [7][340/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2865e+00 (2.3759e+00)	Acc@1  40.62 ( 37.70)	Acc@5  70.31 ( 70.04)
Epoch: [7][350/391]	Time  0.246 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3967e+00 (2.3746e+00)	Acc@1  36.72 ( 37.76)	Acc@5  67.19 ( 70.07)
Epoch: [7][360/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1214e+00 (2.3722e+00)	Acc@1  39.06 ( 37.78)	Acc@5  78.12 ( 70.13)
Epoch: [7][370/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2164e+00 (2.3697e+00)	Acc@1  41.41 ( 37.84)	Acc@5  71.09 ( 70.16)
Epoch: [7][380/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3141e+00 (2.3706e+00)	Acc@1  42.97 ( 37.86)	Acc@5  71.88 ( 70.12)
Epoch: [7][390/391]	Time  0.179 ( 0.242)	Data  0.001 ( 0.001)	Loss 2.4148e+00 (2.3684e+00)	Acc@1  41.25 ( 37.94)	Acc@5  66.25 ( 70.14)
## e[7] optimizer.zero_grad (sum) time: 0.49394989013671875
## e[7]       loss.backward (sum) time: 11.078200817108154
## e[7]      optimizer.step (sum) time: 47.87004995346069
## epoch[7] training(only) time: 94.74537181854248
# Switched to evaluate mode...
Test: [  0/100]	Time  0.208 ( 0.208)	Loss 2.0336e+00 (2.0336e+00)	Acc@1  49.00 ( 49.00)	Acc@5  78.00 ( 78.00)
Test: [ 10/100]	Time  0.077 ( 0.090)	Loss 2.3152e+00 (2.2025e+00)	Acc@1  36.00 ( 43.00)	Acc@5  78.00 ( 75.00)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 1.9021e+00 (2.1640e+00)	Acc@1  44.00 ( 43.76)	Acc@5  82.00 ( 75.19)
Test: [ 30/100]	Time  0.077 ( 0.082)	Loss 2.3681e+00 (2.1924e+00)	Acc@1  37.00 ( 43.23)	Acc@5  69.00 ( 74.10)
Test: [ 40/100]	Time  0.079 ( 0.081)	Loss 2.2091e+00 (2.1974e+00)	Acc@1  38.00 ( 42.95)	Acc@5  76.00 ( 74.22)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 2.1823e+00 (2.2146e+00)	Acc@1  46.00 ( 42.49)	Acc@5  72.00 ( 73.88)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 2.1606e+00 (2.2196e+00)	Acc@1  45.00 ( 42.28)	Acc@5  77.00 ( 73.69)
Test: [ 70/100]	Time  0.079 ( 0.080)	Loss 2.4228e+00 (2.2279e+00)	Acc@1  42.00 ( 42.20)	Acc@5  72.00 ( 73.49)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 2.2334e+00 (2.2310e+00)	Acc@1  41.00 ( 41.99)	Acc@5  70.00 ( 73.31)
Test: [ 90/100]	Time  0.077 ( 0.079)	Loss 1.9233e+00 (2.2263e+00)	Acc@1  44.00 ( 41.91)	Acc@5  79.00 ( 73.49)
 * Acc@1 41.980 Acc@5 73.560
### epoch[7] execution time: 102.75487041473389
EPOCH 8
# current learning rate: 0.1
# Switched to train mode...
Epoch: [8][  0/391]	Time  0.401 ( 0.401)	Data  0.159 ( 0.159)	Loss 2.0607e+00 (2.0607e+00)	Acc@1  45.31 ( 45.31)	Acc@5  78.12 ( 78.12)
Epoch: [8][ 10/391]	Time  0.240 ( 0.256)	Data  0.001 ( 0.015)	Loss 2.2338e+00 (2.1556e+00)	Acc@1  39.06 ( 41.26)	Acc@5  72.66 ( 74.29)
Epoch: [8][ 20/391]	Time  0.241 ( 0.249)	Data  0.001 ( 0.009)	Loss 2.0390e+00 (2.1565e+00)	Acc@1  46.09 ( 41.93)	Acc@5  76.56 ( 73.88)
Epoch: [8][ 30/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.006)	Loss 2.1324e+00 (2.1615e+00)	Acc@1  35.16 ( 42.14)	Acc@5  78.12 ( 73.69)
Epoch: [8][ 40/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.005)	Loss 2.2503e+00 (2.1832e+00)	Acc@1  37.50 ( 41.90)	Acc@5  70.31 ( 73.11)
Epoch: [8][ 50/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 2.0468e+00 (2.1813e+00)	Acc@1  44.53 ( 41.91)	Acc@5  78.91 ( 73.58)
Epoch: [8][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 2.1912e+00 (2.1909e+00)	Acc@1  42.97 ( 41.75)	Acc@5  70.31 ( 73.46)
Epoch: [8][ 70/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.003)	Loss 2.1450e+00 (2.2039e+00)	Acc@1  39.84 ( 41.25)	Acc@5  76.56 ( 73.37)
Epoch: [8][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.3143e+00 (2.2068e+00)	Acc@1  32.81 ( 41.00)	Acc@5  69.53 ( 73.19)
Epoch: [8][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.3854e+00 (2.2067e+00)	Acc@1  33.59 ( 41.11)	Acc@5  66.41 ( 73.25)
Epoch: [8][100/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.2125e+00 (2.2079e+00)	Acc@1  38.28 ( 41.18)	Acc@5  75.00 ( 73.28)
Epoch: [8][110/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.0952e+00 (2.2077e+00)	Acc@1  39.06 ( 41.10)	Acc@5  76.56 ( 73.28)
Epoch: [8][120/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.2437e+00 (2.2112e+00)	Acc@1  43.75 ( 41.04)	Acc@5  76.56 ( 73.28)
Epoch: [8][130/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.3002e+00 (2.2165e+00)	Acc@1  37.50 ( 40.99)	Acc@5  69.53 ( 73.14)
Epoch: [8][140/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.1639e+00 (2.2138e+00)	Acc@1  41.41 ( 41.08)	Acc@5  73.44 ( 73.19)
Epoch: [8][150/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.0456e+00 (2.2114e+00)	Acc@1  42.97 ( 41.13)	Acc@5  74.22 ( 73.21)
Epoch: [8][160/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.7065e+00 (2.2117e+00)	Acc@1  32.03 ( 41.12)	Acc@5  62.50 ( 73.21)
Epoch: [8][170/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9606e+00 (2.2135e+00)	Acc@1  47.66 ( 41.02)	Acc@5  76.56 ( 73.17)
Epoch: [8][180/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2162e+00 (2.2095e+00)	Acc@1  41.41 ( 41.16)	Acc@5  71.09 ( 73.25)
Epoch: [8][190/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0873e+00 (2.2111e+00)	Acc@1  43.75 ( 41.16)	Acc@5  73.44 ( 73.20)
Epoch: [8][200/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9775e+00 (2.2093e+00)	Acc@1  46.88 ( 41.19)	Acc@5  78.91 ( 73.24)
Epoch: [8][210/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1693e+00 (2.2113e+00)	Acc@1  39.06 ( 41.14)	Acc@5  71.09 ( 73.16)
Epoch: [8][220/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1504e+00 (2.2117e+00)	Acc@1  36.72 ( 41.06)	Acc@5  71.09 ( 73.18)
Epoch: [8][230/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1578e+00 (2.2096e+00)	Acc@1  42.97 ( 41.03)	Acc@5  76.56 ( 73.27)
Epoch: [8][240/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2061e+00 (2.2089e+00)	Acc@1  35.94 ( 41.04)	Acc@5  75.00 ( 73.28)
Epoch: [8][250/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9282e+00 (2.2057e+00)	Acc@1  50.78 ( 41.12)	Acc@5  75.00 ( 73.34)
Epoch: [8][260/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1417e+00 (2.2062e+00)	Acc@1  44.53 ( 41.07)	Acc@5  71.88 ( 73.34)
Epoch: [8][270/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2821e+00 (2.2079e+00)	Acc@1  38.28 ( 41.01)	Acc@5  68.75 ( 73.32)
Epoch: [8][280/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3129e+00 (2.2055e+00)	Acc@1  37.50 ( 41.09)	Acc@5  73.44 ( 73.38)
Epoch: [8][290/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1991e+00 (2.2052e+00)	Acc@1  38.28 ( 41.04)	Acc@5  75.78 ( 73.40)
Epoch: [8][300/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2949e+00 (2.2045e+00)	Acc@1  35.94 ( 41.06)	Acc@5  67.97 ( 73.40)
Epoch: [8][310/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2510e+00 (2.2011e+00)	Acc@1  42.97 ( 41.14)	Acc@5  71.09 ( 73.45)
Epoch: [8][320/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0843e+00 (2.2005e+00)	Acc@1  42.97 ( 41.16)	Acc@5  75.78 ( 73.47)
Epoch: [8][330/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1048e+00 (2.2008e+00)	Acc@1  39.84 ( 41.14)	Acc@5  75.78 ( 73.49)
Epoch: [8][340/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8422e+00 (2.1983e+00)	Acc@1  46.88 ( 41.16)	Acc@5  79.69 ( 73.56)
Epoch: [8][350/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3250e+00 (2.1986e+00)	Acc@1  34.38 ( 41.15)	Acc@5  74.22 ( 73.56)
Epoch: [8][360/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3652e+00 (2.1977e+00)	Acc@1  39.06 ( 41.20)	Acc@5  73.44 ( 73.55)
Epoch: [8][370/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0474e+00 (2.1941e+00)	Acc@1  45.31 ( 41.31)	Acc@5  78.12 ( 73.64)
Epoch: [8][380/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3589e+00 (2.1939e+00)	Acc@1  35.94 ( 41.32)	Acc@5  71.09 ( 73.65)
Epoch: [8][390/391]	Time  0.180 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1927e+00 (2.1950e+00)	Acc@1  37.50 ( 41.33)	Acc@5  78.75 ( 73.65)
## e[8] optimizer.zero_grad (sum) time: 0.4932870864868164
## e[8]       loss.backward (sum) time: 11.102005004882812
## e[8]      optimizer.step (sum) time: 47.853862047195435
## epoch[8] training(only) time: 94.73535060882568
# Switched to evaluate mode...
Test: [  0/100]	Time  0.242 ( 0.242)	Loss 2.0961e+00 (2.0961e+00)	Acc@1  47.00 ( 47.00)	Acc@5  75.00 ( 75.00)
Test: [ 10/100]	Time  0.078 ( 0.093)	Loss 2.1970e+00 (2.2142e+00)	Acc@1  36.00 ( 42.73)	Acc@5  75.00 ( 73.00)
Test: [ 20/100]	Time  0.078 ( 0.086)	Loss 2.0127e+00 (2.1992e+00)	Acc@1  52.00 ( 43.10)	Acc@5  74.00 ( 73.57)
Test: [ 30/100]	Time  0.078 ( 0.083)	Loss 2.2550e+00 (2.2094e+00)	Acc@1  43.00 ( 42.87)	Acc@5  72.00 ( 73.39)
Test: [ 40/100]	Time  0.078 ( 0.082)	Loss 2.2423e+00 (2.1909e+00)	Acc@1  46.00 ( 43.39)	Acc@5  73.00 ( 73.93)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 2.0911e+00 (2.2084e+00)	Acc@1  45.00 ( 43.06)	Acc@5  76.00 ( 73.67)
Test: [ 60/100]	Time  0.077 ( 0.081)	Loss 2.1834e+00 (2.2107e+00)	Acc@1  41.00 ( 42.46)	Acc@5  75.00 ( 73.62)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 2.2804e+00 (2.2138e+00)	Acc@1  46.00 ( 42.37)	Acc@5  73.00 ( 73.76)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 2.3607e+00 (2.2203e+00)	Acc@1  38.00 ( 42.16)	Acc@5  65.00 ( 73.49)
Test: [ 90/100]	Time  0.077 ( 0.080)	Loss 2.1903e+00 (2.2091e+00)	Acc@1  46.00 ( 42.36)	Acc@5  73.00 ( 73.68)
 * Acc@1 42.420 Acc@5 73.670
### epoch[8] execution time: 102.77685236930847
EPOCH 9
# current learning rate: 0.1
# Switched to train mode...
Epoch: [9][  0/391]	Time  0.429 ( 0.429)	Data  0.187 ( 0.187)	Loss 2.0540e+00 (2.0540e+00)	Acc@1  48.44 ( 48.44)	Acc@5  73.44 ( 73.44)
Epoch: [9][ 10/391]	Time  0.241 ( 0.258)	Data  0.001 ( 0.018)	Loss 2.0328e+00 (2.1437e+00)	Acc@1  44.53 ( 42.26)	Acc@5  75.00 ( 74.64)
Epoch: [9][ 20/391]	Time  0.240 ( 0.250)	Data  0.001 ( 0.010)	Loss 1.7463e+00 (2.0617e+00)	Acc@1  50.78 ( 43.45)	Acc@5  79.69 ( 76.49)
Epoch: [9][ 30/391]	Time  0.241 ( 0.247)	Data  0.001 ( 0.007)	Loss 2.2144e+00 (2.0871e+00)	Acc@1  41.41 ( 43.09)	Acc@5  71.09 ( 76.46)
Epoch: [9][ 40/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.006)	Loss 2.0529e+00 (2.0777e+00)	Acc@1  46.88 ( 43.39)	Acc@5  73.44 ( 76.81)
Epoch: [9][ 50/391]	Time  0.242 ( 0.246)	Data  0.001 ( 0.005)	Loss 2.1634e+00 (2.0850e+00)	Acc@1  42.97 ( 43.50)	Acc@5  75.78 ( 76.44)
Epoch: [9][ 60/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.004)	Loss 2.2097e+00 (2.0907e+00)	Acc@1  39.84 ( 43.58)	Acc@5  76.56 ( 76.31)
Epoch: [9][ 70/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 2.0652e+00 (2.0975e+00)	Acc@1  46.88 ( 43.49)	Acc@5  71.88 ( 76.05)
Epoch: [9][ 80/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 2.0642e+00 (2.1077e+00)	Acc@1  45.31 ( 43.26)	Acc@5  78.91 ( 75.88)
Epoch: [9][ 90/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 2.0816e+00 (2.1029e+00)	Acc@1  39.84 ( 43.36)	Acc@5  75.78 ( 75.90)
Epoch: [9][100/391]	Time  0.243 ( 0.244)	Data  0.001 ( 0.003)	Loss 1.9933e+00 (2.1037e+00)	Acc@1  42.97 ( 43.41)	Acc@5  75.78 ( 75.75)
Epoch: [9][110/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 1.9961e+00 (2.1013e+00)	Acc@1  44.53 ( 43.52)	Acc@5  80.47 ( 75.79)
Epoch: [9][120/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.9958e+00 (2.0997e+00)	Acc@1  46.88 ( 43.58)	Acc@5  73.44 ( 75.79)
Epoch: [9][130/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.0898e+00 (2.0970e+00)	Acc@1  44.53 ( 43.53)	Acc@5  79.69 ( 75.89)
Epoch: [9][140/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.9771e+00 (2.0952e+00)	Acc@1  47.66 ( 43.57)	Acc@5  77.34 ( 76.00)
Epoch: [9][150/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.3137e+00 (2.0992e+00)	Acc@1  39.84 ( 43.54)	Acc@5  69.53 ( 75.89)
Epoch: [9][160/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.1357e+00 (2.0987e+00)	Acc@1  42.97 ( 43.58)	Acc@5  74.22 ( 75.88)
Epoch: [9][170/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.1750e+00 (2.0947e+00)	Acc@1  39.06 ( 43.67)	Acc@5  73.44 ( 75.98)
Epoch: [9][180/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.0256e+00 (2.0897e+00)	Acc@1  44.53 ( 43.85)	Acc@5  73.44 ( 76.04)
Epoch: [9][190/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.1158e+00 (2.0883e+00)	Acc@1  44.53 ( 43.84)	Acc@5  74.22 ( 76.03)
Epoch: [9][200/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.9246e+00 (2.0858e+00)	Acc@1  47.66 ( 43.84)	Acc@5  81.25 ( 76.13)
Epoch: [9][210/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.8221e+00 (2.0842e+00)	Acc@1  49.22 ( 43.91)	Acc@5  80.47 ( 76.13)
Epoch: [9][220/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.9473e+00 (2.0808e+00)	Acc@1  47.66 ( 43.97)	Acc@5  78.91 ( 76.24)
Epoch: [9][230/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.9711e+00 (2.0819e+00)	Acc@1  44.53 ( 43.96)	Acc@5  79.69 ( 76.19)
Epoch: [9][240/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.9841e+00 (2.0810e+00)	Acc@1  50.78 ( 43.96)	Acc@5  76.56 ( 76.18)
Epoch: [9][250/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.2543e+00 (2.0795e+00)	Acc@1  39.06 ( 43.95)	Acc@5  71.88 ( 76.25)
Epoch: [9][260/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.9275e+00 (2.0792e+00)	Acc@1  50.00 ( 44.00)	Acc@5  77.34 ( 76.28)
Epoch: [9][270/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.1069e+00 (2.0779e+00)	Acc@1  46.88 ( 44.10)	Acc@5  75.78 ( 76.33)
Epoch: [9][280/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.9053e+00 (2.0782e+00)	Acc@1  47.66 ( 44.09)	Acc@5  78.91 ( 76.33)
Epoch: [9][290/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.0277e+00 (2.0754e+00)	Acc@1  47.66 ( 44.17)	Acc@5  78.91 ( 76.36)
Epoch: [9][300/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.8909e+00 (2.0727e+00)	Acc@1  46.88 ( 44.28)	Acc@5  77.34 ( 76.43)
Epoch: [9][310/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.7374e+00 (2.0700e+00)	Acc@1  51.56 ( 44.38)	Acc@5  83.59 ( 76.50)
Epoch: [9][320/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7643e+00 (2.0706e+00)	Acc@1  49.22 ( 44.39)	Acc@5  82.03 ( 76.46)
Epoch: [9][330/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9277e+00 (2.0658e+00)	Acc@1  51.56 ( 44.53)	Acc@5  77.34 ( 76.51)
Epoch: [9][340/391]	Time  0.246 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9449e+00 (2.0671e+00)	Acc@1  43.75 ( 44.46)	Acc@5  78.12 ( 76.49)
Epoch: [9][350/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8287e+00 (2.0656e+00)	Acc@1  50.78 ( 44.51)	Acc@5  85.16 ( 76.51)
Epoch: [9][360/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9754e+00 (2.0626e+00)	Acc@1  53.91 ( 44.62)	Acc@5  78.12 ( 76.53)
Epoch: [9][370/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9013e+00 (2.0597e+00)	Acc@1  48.44 ( 44.70)	Acc@5  75.78 ( 76.58)
Epoch: [9][380/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2279e+00 (2.0599e+00)	Acc@1  40.62 ( 44.66)	Acc@5  71.09 ( 76.60)
Epoch: [9][390/391]	Time  0.177 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1532e+00 (2.0599e+00)	Acc@1  46.25 ( 44.66)	Acc@5  73.75 ( 76.58)
## e[9] optimizer.zero_grad (sum) time: 0.49330973625183105
## e[9]       loss.backward (sum) time: 11.063624143600464
## e[9]      optimizer.step (sum) time: 47.8999400138855
## epoch[9] training(only) time: 94.8389139175415
# Switched to evaluate mode...
Test: [  0/100]	Time  0.207 ( 0.207)	Loss 1.9901e+00 (1.9901e+00)	Acc@1  58.00 ( 58.00)	Acc@5  76.00 ( 76.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 2.1686e+00 (2.0109e+00)	Acc@1  44.00 ( 49.00)	Acc@5  80.00 ( 78.82)
Test: [ 20/100]	Time  0.080 ( 0.084)	Loss 1.9262e+00 (2.0142e+00)	Acc@1  44.00 ( 47.86)	Acc@5  80.00 ( 78.05)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 2.1032e+00 (2.0476e+00)	Acc@1  42.00 ( 46.81)	Acc@5  82.00 ( 77.68)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.8799e+00 (2.0420e+00)	Acc@1  51.00 ( 46.63)	Acc@5  80.00 ( 77.56)
Test: [ 50/100]	Time  0.077 ( 0.080)	Loss 1.9134e+00 (2.0345e+00)	Acc@1  52.00 ( 46.88)	Acc@5  76.00 ( 77.33)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 2.0530e+00 (2.0163e+00)	Acc@1  43.00 ( 46.82)	Acc@5  77.00 ( 77.72)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 1.9904e+00 (2.0146e+00)	Acc@1  48.00 ( 46.86)	Acc@5  77.00 ( 77.58)
Test: [ 80/100]	Time  0.079 ( 0.080)	Loss 2.0263e+00 (2.0143e+00)	Acc@1  45.00 ( 46.78)	Acc@5  76.00 ( 77.49)
Test: [ 90/100]	Time  0.077 ( 0.079)	Loss 1.9376e+00 (2.0021e+00)	Acc@1  44.00 ( 46.99)	Acc@5  79.00 ( 77.84)
 * Acc@1 47.190 Acc@5 77.810
### epoch[9] execution time: 102.85161113739014
EPOCH 10
# current learning rate: 0.1
# Switched to train mode...
Epoch: [10][  0/391]	Time  0.400 ( 0.400)	Data  0.146 ( 0.146)	Loss 1.7883e+00 (1.7883e+00)	Acc@1  54.69 ( 54.69)	Acc@5  84.38 ( 84.38)
Epoch: [10][ 10/391]	Time  0.241 ( 0.256)	Data  0.001 ( 0.014)	Loss 2.0570e+00 (1.8561e+00)	Acc@1  44.53 ( 49.72)	Acc@5  74.22 ( 80.82)
Epoch: [10][ 20/391]	Time  0.244 ( 0.249)	Data  0.001 ( 0.008)	Loss 1.5664e+00 (1.8601e+00)	Acc@1  56.25 ( 48.96)	Acc@5  84.38 ( 80.58)
Epoch: [10][ 30/391]	Time  0.241 ( 0.247)	Data  0.001 ( 0.006)	Loss 1.8629e+00 (1.8527e+00)	Acc@1  46.09 ( 49.22)	Acc@5  81.25 ( 80.52)
Epoch: [10][ 40/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.005)	Loss 2.0288e+00 (1.8639e+00)	Acc@1  49.22 ( 49.28)	Acc@5  82.03 ( 80.22)
Epoch: [10][ 50/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.8320e+00 (1.8762e+00)	Acc@1  50.00 ( 49.07)	Acc@5  78.12 ( 80.10)
Epoch: [10][ 60/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.003)	Loss 1.7541e+00 (1.8879e+00)	Acc@1  50.00 ( 48.45)	Acc@5  82.03 ( 79.82)
Epoch: [10][ 70/391]	Time  0.245 ( 0.244)	Data  0.001 ( 0.003)	Loss 2.1265e+00 (1.8988e+00)	Acc@1  41.41 ( 48.12)	Acc@5  79.69 ( 79.63)
Epoch: [10][ 80/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.2073e+00 (1.9102e+00)	Acc@1  42.19 ( 47.85)	Acc@5  72.66 ( 79.32)
Epoch: [10][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.1539e+00 (1.9198e+00)	Acc@1  39.06 ( 47.64)	Acc@5  73.44 ( 79.09)
Epoch: [10][100/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.0808e+00 (1.9243e+00)	Acc@1  45.31 ( 47.65)	Acc@5  77.34 ( 79.12)
Epoch: [10][110/391]	Time  0.239 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.8831e+00 (1.9221e+00)	Acc@1  47.66 ( 47.79)	Acc@5  84.38 ( 79.22)
Epoch: [10][120/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.7520e+00 (1.9195e+00)	Acc@1  49.22 ( 47.72)	Acc@5  82.03 ( 79.21)
Epoch: [10][130/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.9362e+00 (1.9161e+00)	Acc@1  44.53 ( 47.72)	Acc@5  77.34 ( 79.20)
Epoch: [10][140/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.0043e+00 (1.9203e+00)	Acc@1  47.66 ( 47.61)	Acc@5  73.44 ( 79.12)
Epoch: [10][150/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8491e+00 (1.9239e+00)	Acc@1  48.44 ( 47.56)	Acc@5  82.81 ( 79.07)
Epoch: [10][160/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9806e+00 (1.9291e+00)	Acc@1  50.00 ( 47.52)	Acc@5  74.22 ( 78.90)
Epoch: [10][170/391]	Time  0.247 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1415e+00 (1.9316e+00)	Acc@1  41.41 ( 47.40)	Acc@5  78.91 ( 78.91)
Epoch: [10][180/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9658e+00 (1.9296e+00)	Acc@1  47.66 ( 47.44)	Acc@5  79.69 ( 78.91)
Epoch: [10][190/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0403e+00 (1.9288e+00)	Acc@1  45.31 ( 47.43)	Acc@5  78.91 ( 78.92)
Epoch: [10][200/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6435e+00 (1.9243e+00)	Acc@1  55.47 ( 47.64)	Acc@5  82.81 ( 79.00)
Epoch: [10][210/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9999e+00 (1.9244e+00)	Acc@1  51.56 ( 47.59)	Acc@5  77.34 ( 79.02)
Epoch: [10][220/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6587e+00 (1.9246e+00)	Acc@1  53.91 ( 47.64)	Acc@5  84.38 ( 78.98)
Epoch: [10][230/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0424e+00 (1.9277e+00)	Acc@1  45.31 ( 47.58)	Acc@5  81.25 ( 78.90)
Epoch: [10][240/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0184e+00 (1.9297e+00)	Acc@1  46.09 ( 47.59)	Acc@5  77.34 ( 78.88)
Epoch: [10][250/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0037e+00 (1.9313e+00)	Acc@1  44.53 ( 47.51)	Acc@5  77.34 ( 78.87)
Epoch: [10][260/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1402e+00 (1.9335e+00)	Acc@1  42.19 ( 47.46)	Acc@5  71.88 ( 78.80)
Epoch: [10][270/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2053e+00 (1.9353e+00)	Acc@1  42.97 ( 47.40)	Acc@5  72.66 ( 78.74)
Epoch: [10][280/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7568e+00 (1.9315e+00)	Acc@1  50.78 ( 47.49)	Acc@5  83.59 ( 78.81)
Epoch: [10][290/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0158e+00 (1.9288e+00)	Acc@1  47.66 ( 47.56)	Acc@5  82.81 ( 78.89)
Epoch: [10][300/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8037e+00 (1.9296e+00)	Acc@1  50.78 ( 47.55)	Acc@5  80.47 ( 78.90)
Epoch: [10][310/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7611e+00 (1.9285e+00)	Acc@1  49.22 ( 47.61)	Acc@5  83.59 ( 78.93)
Epoch: [10][320/391]	Time  0.253 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9043e+00 (1.9267e+00)	Acc@1  50.00 ( 47.62)	Acc@5  80.47 ( 79.02)
Epoch: [10][330/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2283e+00 (1.9271e+00)	Acc@1  38.28 ( 47.60)	Acc@5  68.75 ( 78.98)
Epoch: [10][340/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9718e+00 (1.9285e+00)	Acc@1  42.97 ( 47.55)	Acc@5  74.22 ( 78.97)
Epoch: [10][350/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8934e+00 (1.9282e+00)	Acc@1  48.44 ( 47.57)	Acc@5  79.69 ( 78.96)
Epoch: [10][360/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1249e+00 (1.9267e+00)	Acc@1  41.41 ( 47.62)	Acc@5  75.78 ( 78.98)
Epoch: [10][370/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0440e+00 (1.9271e+00)	Acc@1  48.44 ( 47.64)	Acc@5  77.34 ( 78.97)
Epoch: [10][380/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9245e+00 (1.9252e+00)	Acc@1  49.22 ( 47.67)	Acc@5  78.12 ( 78.97)
Epoch: [10][390/391]	Time  0.179 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2029e+00 (1.9228e+00)	Acc@1  45.00 ( 47.71)	Acc@5  72.50 ( 79.02)
## e[10] optimizer.zero_grad (sum) time: 0.4926176071166992
## e[10]       loss.backward (sum) time: 11.07999062538147
## e[10]      optimizer.step (sum) time: 47.89618110656738
## epoch[10] training(only) time: 94.68186473846436
# Switched to evaluate mode...
Test: [  0/100]	Time  0.207 ( 0.207)	Loss 2.0028e+00 (2.0028e+00)	Acc@1  52.00 ( 52.00)	Acc@5  72.00 ( 72.00)
Test: [ 10/100]	Time  0.077 ( 0.089)	Loss 2.0472e+00 (1.9471e+00)	Acc@1  45.00 ( 49.82)	Acc@5  81.00 ( 78.64)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 1.6988e+00 (1.9156e+00)	Acc@1  50.00 ( 49.95)	Acc@5  86.00 ( 78.67)
Test: [ 30/100]	Time  0.077 ( 0.082)	Loss 1.9550e+00 (1.9539e+00)	Acc@1  49.00 ( 49.84)	Acc@5  79.00 ( 78.55)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.8662e+00 (1.9513e+00)	Acc@1  52.00 ( 49.22)	Acc@5  80.00 ( 78.80)
Test: [ 50/100]	Time  0.078 ( 0.080)	Loss 1.8998e+00 (1.9679e+00)	Acc@1  51.00 ( 48.92)	Acc@5  76.00 ( 78.69)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 2.0292e+00 (1.9647e+00)	Acc@1  45.00 ( 48.95)	Acc@5  78.00 ( 78.41)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 1.8341e+00 (1.9678e+00)	Acc@1  46.00 ( 48.75)	Acc@5  79.00 ( 78.25)
Test: [ 80/100]	Time  0.078 ( 0.079)	Loss 2.0740e+00 (1.9740e+00)	Acc@1  44.00 ( 48.54)	Acc@5  75.00 ( 78.16)
Test: [ 90/100]	Time  0.079 ( 0.079)	Loss 2.0239e+00 (1.9674e+00)	Acc@1  49.00 ( 48.74)	Acc@5  82.00 ( 78.36)
 * Acc@1 48.810 Acc@5 78.420
### epoch[10] execution time: 102.68392944335938
EPOCH 11
# current learning rate: 0.1
# Switched to train mode...
Epoch: [11][  0/391]	Time  0.408 ( 0.408)	Data  0.166 ( 0.166)	Loss 1.9358e+00 (1.9358e+00)	Acc@1  45.31 ( 45.31)	Acc@5  75.78 ( 75.78)
Epoch: [11][ 10/391]	Time  0.241 ( 0.256)	Data  0.001 ( 0.016)	Loss 1.5721e+00 (1.8028e+00)	Acc@1  60.16 ( 50.36)	Acc@5  83.59 ( 80.04)
Epoch: [11][ 20/391]	Time  0.240 ( 0.249)	Data  0.001 ( 0.009)	Loss 1.9033e+00 (1.8348e+00)	Acc@1  46.09 ( 49.81)	Acc@5  78.12 ( 79.69)
Epoch: [11][ 30/391]	Time  0.242 ( 0.247)	Data  0.001 ( 0.006)	Loss 1.9623e+00 (1.8186e+00)	Acc@1  44.53 ( 49.87)	Acc@5  79.69 ( 80.39)
Epoch: [11][ 40/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.005)	Loss 1.5882e+00 (1.8180e+00)	Acc@1  56.25 ( 50.34)	Acc@5  85.94 ( 80.32)
Epoch: [11][ 50/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.004)	Loss 1.6278e+00 (1.8071e+00)	Acc@1  56.25 ( 50.49)	Acc@5  82.81 ( 80.58)
Epoch: [11][ 60/391]	Time  0.242 ( 0.245)	Data  0.001 ( 0.004)	Loss 1.5729e+00 (1.8005e+00)	Acc@1  57.03 ( 50.65)	Acc@5  85.94 ( 80.74)
Epoch: [11][ 70/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.003)	Loss 1.7785e+00 (1.8111e+00)	Acc@1  53.91 ( 50.28)	Acc@5  82.03 ( 80.67)
Epoch: [11][ 80/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 1.7405e+00 (1.8139e+00)	Acc@1  49.22 ( 50.19)	Acc@5  82.03 ( 80.73)
Epoch: [11][ 90/391]	Time  0.239 ( 0.244)	Data  0.001 ( 0.003)	Loss 1.7501e+00 (1.8091e+00)	Acc@1  51.56 ( 50.17)	Acc@5  83.59 ( 80.79)
Epoch: [11][100/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 1.8291e+00 (1.8106e+00)	Acc@1  49.22 ( 50.16)	Acc@5  82.81 ( 80.86)
Epoch: [11][110/391]	Time  0.247 ( 0.244)	Data  0.001 ( 0.003)	Loss 1.9467e+00 (1.8114e+00)	Acc@1  52.34 ( 50.29)	Acc@5  78.12 ( 80.87)
Epoch: [11][120/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.5599e+00 (1.8100e+00)	Acc@1  56.25 ( 50.39)	Acc@5  86.72 ( 80.91)
Epoch: [11][130/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.7976e+00 (1.8120e+00)	Acc@1  50.00 ( 50.35)	Acc@5  82.81 ( 80.87)
Epoch: [11][140/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.7465e+00 (1.8105e+00)	Acc@1  48.44 ( 50.28)	Acc@5  82.03 ( 80.91)
Epoch: [11][150/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.8513e+00 (1.8155e+00)	Acc@1  46.09 ( 50.18)	Acc@5  78.12 ( 80.81)
Epoch: [11][160/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.9282e+00 (1.8139e+00)	Acc@1  44.53 ( 50.16)	Acc@5  78.91 ( 80.90)
Epoch: [11][170/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.1315e+00 (1.8161e+00)	Acc@1  45.31 ( 50.16)	Acc@5  74.22 ( 80.83)
Epoch: [11][180/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.8218e+00 (1.8160e+00)	Acc@1  50.00 ( 50.17)	Acc@5  82.03 ( 80.81)
Epoch: [11][190/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.6262e+00 (1.8150e+00)	Acc@1  53.91 ( 50.19)	Acc@5  85.94 ( 80.84)
Epoch: [11][200/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.8721e+00 (1.8163e+00)	Acc@1  40.62 ( 50.07)	Acc@5  78.12 ( 80.80)
Epoch: [11][210/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.7142e+00 (1.8170e+00)	Acc@1  46.09 ( 49.98)	Acc@5  82.81 ( 80.73)
Epoch: [11][220/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.9753e+00 (1.8187e+00)	Acc@1  46.88 ( 49.98)	Acc@5  82.03 ( 80.68)
Epoch: [11][230/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.9470e+00 (1.8180e+00)	Acc@1  49.22 ( 49.97)	Acc@5  78.91 ( 80.75)
Epoch: [11][240/391]	Time  0.252 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.6627e+00 (1.8155e+00)	Acc@1  53.12 ( 50.07)	Acc@5  82.03 ( 80.81)
Epoch: [11][250/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.9537e+00 (1.8156e+00)	Acc@1  45.31 ( 50.16)	Acc@5  75.78 ( 80.75)
Epoch: [11][260/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.8177e+00 (1.8161e+00)	Acc@1  50.00 ( 50.20)	Acc@5  78.91 ( 80.75)
Epoch: [11][270/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.8519e+00 (1.8162e+00)	Acc@1  52.34 ( 50.20)	Acc@5  77.34 ( 80.75)
Epoch: [11][280/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.8748e+00 (1.8121e+00)	Acc@1  44.53 ( 50.26)	Acc@5  78.91 ( 80.81)
Epoch: [11][290/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.8890e+00 (1.8104e+00)	Acc@1  49.22 ( 50.32)	Acc@5  76.56 ( 80.83)
Epoch: [11][300/391]	Time  0.246 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.6778e+00 (1.8071e+00)	Acc@1  50.78 ( 50.40)	Acc@5  82.81 ( 80.88)
Epoch: [11][310/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.5008e+00 (1.8067e+00)	Acc@1  57.81 ( 50.41)	Acc@5  89.84 ( 80.90)
Epoch: [11][320/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.8455e+00 (1.8051e+00)	Acc@1  51.56 ( 50.42)	Acc@5  83.59 ( 80.95)
Epoch: [11][330/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.6056e+00 (1.8047e+00)	Acc@1  53.91 ( 50.43)	Acc@5  82.03 ( 80.95)
Epoch: [11][340/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8737e+00 (1.8070e+00)	Acc@1  50.78 ( 50.35)	Acc@5  81.25 ( 80.90)
Epoch: [11][350/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5754e+00 (1.8073e+00)	Acc@1  60.94 ( 50.33)	Acc@5  84.38 ( 80.92)
Epoch: [11][360/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.9059e+00 (1.8095e+00)	Acc@1  45.31 ( 50.29)	Acc@5  81.25 ( 80.91)
Epoch: [11][370/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.8277e+00 (1.8101e+00)	Acc@1  53.91 ( 50.28)	Acc@5  78.91 ( 80.91)
Epoch: [11][380/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7633e+00 (1.8092e+00)	Acc@1  50.78 ( 50.30)	Acc@5  85.94 ( 80.92)
Epoch: [11][390/391]	Time  0.179 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7136e+00 (1.8085e+00)	Acc@1  52.50 ( 50.29)	Acc@5  83.75 ( 80.96)
## e[11] optimizer.zero_grad (sum) time: 0.490720272064209
## e[11]       loss.backward (sum) time: 11.099127769470215
## e[11]      optimizer.step (sum) time: 47.876357555389404
## epoch[11] training(only) time: 94.84009456634521
# Switched to evaluate mode...
Test: [  0/100]	Time  0.210 ( 0.210)	Loss 1.8357e+00 (1.8357e+00)	Acc@1  51.00 ( 51.00)	Acc@5  77.00 ( 77.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 2.0400e+00 (1.8739e+00)	Acc@1  45.00 ( 50.45)	Acc@5  81.00 ( 79.91)
Test: [ 20/100]	Time  0.077 ( 0.084)	Loss 1.7069e+00 (1.8044e+00)	Acc@1  54.00 ( 51.81)	Acc@5  80.00 ( 80.86)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 1.7339e+00 (1.8202e+00)	Acc@1  50.00 ( 51.45)	Acc@5  88.00 ( 80.52)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.7079e+00 (1.8037e+00)	Acc@1  54.00 ( 51.78)	Acc@5  79.00 ( 80.88)
Test: [ 50/100]	Time  0.079 ( 0.080)	Loss 1.7270e+00 (1.8233e+00)	Acc@1  57.00 ( 51.55)	Acc@5  77.00 ( 80.59)
Test: [ 60/100]	Time  0.077 ( 0.080)	Loss 1.8659e+00 (1.8293e+00)	Acc@1  47.00 ( 51.23)	Acc@5  83.00 ( 80.56)
Test: [ 70/100]	Time  0.079 ( 0.080)	Loss 1.8872e+00 (1.8341e+00)	Acc@1  50.00 ( 51.10)	Acc@5  81.00 ( 80.69)
Test: [ 80/100]	Time  0.077 ( 0.079)	Loss 1.9689e+00 (1.8356e+00)	Acc@1  50.00 ( 50.80)	Acc@5  76.00 ( 80.68)
Test: [ 90/100]	Time  0.079 ( 0.079)	Loss 1.9053e+00 (1.8267e+00)	Acc@1  53.00 ( 51.16)	Acc@5  77.00 ( 80.88)
 * Acc@1 51.180 Acc@5 80.840
### epoch[11] execution time: 102.84877634048462
EPOCH 12
# current learning rate: 0.1
# Switched to train mode...
Epoch: [12][  0/391]	Time  0.397 ( 0.397)	Data  0.155 ( 0.155)	Loss 1.8120e+00 (1.8120e+00)	Acc@1  56.25 ( 56.25)	Acc@5  78.91 ( 78.91)
Epoch: [12][ 10/391]	Time  0.241 ( 0.255)	Data  0.001 ( 0.015)	Loss 1.5780e+00 (1.6846e+00)	Acc@1  59.38 ( 53.12)	Acc@5  84.38 ( 82.60)
Epoch: [12][ 20/391]	Time  0.241 ( 0.249)	Data  0.001 ( 0.008)	Loss 1.6607e+00 (1.6505e+00)	Acc@1  55.47 ( 54.02)	Acc@5  80.47 ( 83.63)
Epoch: [12][ 30/391]	Time  0.251 ( 0.247)	Data  0.001 ( 0.006)	Loss 1.8508e+00 (1.6593e+00)	Acc@1  55.47 ( 53.86)	Acc@5  80.47 ( 83.74)
Epoch: [12][ 40/391]	Time  0.242 ( 0.246)	Data  0.001 ( 0.005)	Loss 1.6146e+00 (1.6592e+00)	Acc@1  58.59 ( 53.87)	Acc@5  82.81 ( 83.56)
Epoch: [12][ 50/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.004)	Loss 1.8666e+00 (1.6732e+00)	Acc@1  48.44 ( 53.68)	Acc@5  83.59 ( 83.41)
Epoch: [12][ 60/391]	Time  0.246 ( 0.245)	Data  0.001 ( 0.004)	Loss 1.7590e+00 (1.6554e+00)	Acc@1  53.12 ( 53.98)	Acc@5  83.59 ( 83.66)
Epoch: [12][ 70/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 1.6007e+00 (1.6585e+00)	Acc@1  57.03 ( 53.85)	Acc@5  84.38 ( 83.78)
Epoch: [12][ 80/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 1.7035e+00 (1.6650e+00)	Acc@1  51.56 ( 53.55)	Acc@5  81.25 ( 83.53)
Epoch: [12][ 90/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.003)	Loss 1.6797e+00 (1.6669e+00)	Acc@1  53.12 ( 53.55)	Acc@5  81.25 ( 83.54)
Epoch: [12][100/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.003)	Loss 1.7753e+00 (1.6723e+00)	Acc@1  50.78 ( 53.47)	Acc@5  78.12 ( 83.25)
Epoch: [12][110/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.4505e+00 (1.6760e+00)	Acc@1  60.94 ( 53.42)	Acc@5  89.06 ( 83.35)
Epoch: [12][120/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.6416e+00 (1.6748e+00)	Acc@1  55.47 ( 53.55)	Acc@5  85.94 ( 83.30)
Epoch: [12][130/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.6672e+00 (1.6758e+00)	Acc@1  53.12 ( 53.59)	Acc@5  80.47 ( 83.21)
Epoch: [12][140/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.7395e+00 (1.6765e+00)	Acc@1  48.44 ( 53.54)	Acc@5  78.91 ( 83.20)
Epoch: [12][150/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.5981e+00 (1.6769e+00)	Acc@1  57.81 ( 53.50)	Acc@5  83.59 ( 83.26)
Epoch: [12][160/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.5848e+00 (1.6782e+00)	Acc@1  57.81 ( 53.45)	Acc@5  85.94 ( 83.22)
Epoch: [12][170/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.7316e+00 (1.6771e+00)	Acc@1  49.22 ( 53.42)	Acc@5  85.94 ( 83.25)
Epoch: [12][180/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.7566e+00 (1.6838e+00)	Acc@1  52.34 ( 53.30)	Acc@5  83.59 ( 83.15)
Epoch: [12][190/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.6789e+00 (1.6859e+00)	Acc@1  50.78 ( 53.23)	Acc@5  83.59 ( 83.12)
Epoch: [12][200/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.6301e+00 (1.6865e+00)	Acc@1  57.03 ( 53.25)	Acc@5  84.38 ( 83.05)
Epoch: [12][210/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.4504e+00 (1.6876e+00)	Acc@1  59.38 ( 53.22)	Acc@5  84.38 ( 82.98)
Epoch: [12][220/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.8021e+00 (1.6897e+00)	Acc@1  48.44 ( 53.15)	Acc@5  79.69 ( 82.93)
Epoch: [12][230/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.4904e+00 (1.6918e+00)	Acc@1  56.25 ( 53.10)	Acc@5  88.28 ( 82.90)
Epoch: [12][240/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.9138e+00 (1.6964e+00)	Acc@1  51.56 ( 53.05)	Acc@5  79.69 ( 82.82)
Epoch: [12][250/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.8928e+00 (1.7015e+00)	Acc@1  48.44 ( 52.95)	Acc@5  79.69 ( 82.73)
Epoch: [12][260/391]	Time  0.246 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.8114e+00 (1.7044e+00)	Acc@1  50.78 ( 52.93)	Acc@5  78.91 ( 82.70)
Epoch: [12][270/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.6383e+00 (1.7035e+00)	Acc@1  54.69 ( 52.92)	Acc@5  84.38 ( 82.72)
Epoch: [12][280/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.0207e+00 (1.7057e+00)	Acc@1  50.00 ( 52.87)	Acc@5  80.47 ( 82.68)
Epoch: [12][290/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.6322e+00 (1.7059e+00)	Acc@1  50.78 ( 52.84)	Acc@5  82.03 ( 82.71)
Epoch: [12][300/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.6234e+00 (1.7054e+00)	Acc@1  55.47 ( 52.85)	Acc@5  85.16 ( 82.75)
Epoch: [12][310/391]	Time  0.248 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.8155e+00 (1.7050e+00)	Acc@1  48.44 ( 52.86)	Acc@5  81.25 ( 82.77)
Epoch: [12][320/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.7882e+00 (1.7058e+00)	Acc@1  51.56 ( 52.83)	Acc@5  80.47 ( 82.73)
Epoch: [12][330/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.8620e+00 (1.7069e+00)	Acc@1  46.88 ( 52.82)	Acc@5  81.25 ( 82.71)
Epoch: [12][340/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.5812e+00 (1.7071e+00)	Acc@1  54.69 ( 52.80)	Acc@5  89.06 ( 82.70)
Epoch: [12][350/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.8276e+00 (1.7106e+00)	Acc@1  49.22 ( 52.72)	Acc@5  79.69 ( 82.66)
Epoch: [12][360/391]	Time  0.248 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.8504e+00 (1.7100e+00)	Acc@1  47.66 ( 52.75)	Acc@5  82.81 ( 82.70)
Epoch: [12][370/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.8601e+00 (1.7097e+00)	Acc@1  50.78 ( 52.79)	Acc@5  79.69 ( 82.68)
Epoch: [12][380/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.6304e+00 (1.7107e+00)	Acc@1  53.12 ( 52.74)	Acc@5  84.38 ( 82.66)
Epoch: [12][390/391]	Time  0.178 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6761e+00 (1.7087e+00)	Acc@1  57.50 ( 52.77)	Acc@5  82.50 ( 82.70)
## e[12] optimizer.zero_grad (sum) time: 0.49555540084838867
## e[12]       loss.backward (sum) time: 11.122957706451416
## e[12]      optimizer.step (sum) time: 47.84533762931824
## epoch[12] training(only) time: 94.88393425941467
# Switched to evaluate mode...
Test: [  0/100]	Time  0.208 ( 0.208)	Loss 1.7215e+00 (1.7215e+00)	Acc@1  53.00 ( 53.00)	Acc@5  80.00 ( 80.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.8096e+00 (1.6931e+00)	Acc@1  50.00 ( 54.73)	Acc@5  84.00 ( 82.00)
Test: [ 20/100]	Time  0.079 ( 0.084)	Loss 1.6628e+00 (1.7029e+00)	Acc@1  56.00 ( 53.76)	Acc@5  77.00 ( 81.86)
Test: [ 30/100]	Time  0.077 ( 0.082)	Loss 1.6175e+00 (1.7080e+00)	Acc@1  52.00 ( 53.48)	Acc@5  84.00 ( 82.03)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.8126e+00 (1.7087e+00)	Acc@1  55.00 ( 53.22)	Acc@5  85.00 ( 82.22)
Test: [ 50/100]	Time  0.077 ( 0.081)	Loss 1.5533e+00 (1.7209e+00)	Acc@1  58.00 ( 52.84)	Acc@5  86.00 ( 82.02)
Test: [ 60/100]	Time  0.080 ( 0.080)	Loss 1.7593e+00 (1.7170e+00)	Acc@1  52.00 ( 52.62)	Acc@5  82.00 ( 82.07)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 1.7673e+00 (1.7220e+00)	Acc@1  50.00 ( 52.75)	Acc@5  86.00 ( 82.11)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 1.6515e+00 (1.7240e+00)	Acc@1  56.00 ( 52.86)	Acc@5  82.00 ( 82.00)
Test: [ 90/100]	Time  0.078 ( 0.080)	Loss 1.7075e+00 (1.7101e+00)	Acc@1  50.00 ( 53.22)	Acc@5  81.00 ( 82.15)
 * Acc@1 53.310 Acc@5 82.270
### epoch[12] execution time: 102.91869449615479
EPOCH 13
# current learning rate: 0.1
# Switched to train mode...
Epoch: [13][  0/391]	Time  0.428 ( 0.428)	Data  0.186 ( 0.186)	Loss 1.4632e+00 (1.4632e+00)	Acc@1  58.59 ( 58.59)	Acc@5  85.94 ( 85.94)
Epoch: [13][ 10/391]	Time  0.241 ( 0.259)	Data  0.001 ( 0.018)	Loss 1.2802e+00 (1.5448e+00)	Acc@1  58.59 ( 56.75)	Acc@5  91.41 ( 85.94)
Epoch: [13][ 20/391]	Time  0.241 ( 0.250)	Data  0.001 ( 0.010)	Loss 1.7424e+00 (1.5932e+00)	Acc@1  50.78 ( 54.84)	Acc@5  84.38 ( 85.31)
Epoch: [13][ 30/391]	Time  0.241 ( 0.247)	Data  0.001 ( 0.007)	Loss 1.7222e+00 (1.5766e+00)	Acc@1  53.12 ( 55.17)	Acc@5  82.03 ( 85.31)
Epoch: [13][ 40/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.006)	Loss 1.6004e+00 (1.6003e+00)	Acc@1  60.94 ( 54.88)	Acc@5  85.16 ( 84.93)
Epoch: [13][ 50/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.005)	Loss 1.5496e+00 (1.5915e+00)	Acc@1  53.12 ( 55.15)	Acc@5  85.94 ( 85.03)
Epoch: [13][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.6358e+00 (1.6040e+00)	Acc@1  52.34 ( 54.88)	Acc@5  82.03 ( 84.91)
Epoch: [13][ 70/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.5516e+00 (1.5856e+00)	Acc@1  57.03 ( 55.33)	Acc@5  85.16 ( 85.15)
Epoch: [13][ 80/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.003)	Loss 1.5823e+00 (1.5891e+00)	Acc@1  56.25 ( 55.04)	Acc@5  85.94 ( 85.01)
Epoch: [13][ 90/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.8431e+00 (1.5910e+00)	Acc@1  47.66 ( 55.07)	Acc@5  79.69 ( 84.81)
Epoch: [13][100/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.1993e+00 (1.5950e+00)	Acc@1  67.19 ( 55.11)	Acc@5  91.41 ( 84.62)
Epoch: [13][110/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.4821e+00 (1.5927e+00)	Acc@1  51.56 ( 55.14)	Acc@5  85.94 ( 84.62)
Epoch: [13][120/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.7184e+00 (1.5991e+00)	Acc@1  50.00 ( 54.97)	Acc@5  85.94 ( 84.64)
Epoch: [13][130/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.3247e+00 (1.5951e+00)	Acc@1  63.28 ( 55.14)	Acc@5  91.41 ( 84.68)
Epoch: [13][140/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.0059e+00 (1.5974e+00)	Acc@1  48.44 ( 55.13)	Acc@5  77.34 ( 84.61)
Epoch: [13][150/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.7723e+00 (1.5977e+00)	Acc@1  51.56 ( 55.13)	Acc@5  82.03 ( 84.64)
Epoch: [13][160/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.4836e+00 (1.5976e+00)	Acc@1  59.38 ( 55.09)	Acc@5  85.16 ( 84.66)
Epoch: [13][170/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.5487e+00 (1.5978e+00)	Acc@1  58.59 ( 55.04)	Acc@5  85.94 ( 84.63)
Epoch: [13][180/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.6645e+00 (1.5998e+00)	Acc@1  56.25 ( 54.97)	Acc@5  88.28 ( 84.60)
Epoch: [13][190/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.5381e+00 (1.6030e+00)	Acc@1  57.03 ( 54.99)	Acc@5  85.94 ( 84.51)
Epoch: [13][200/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5565e+00 (1.6004e+00)	Acc@1  60.94 ( 55.06)	Acc@5  82.81 ( 84.53)
Epoch: [13][210/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7262e+00 (1.6018e+00)	Acc@1  56.25 ( 55.05)	Acc@5  85.16 ( 84.51)
Epoch: [13][220/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7378e+00 (1.6006e+00)	Acc@1  56.25 ( 55.10)	Acc@5  79.69 ( 84.52)
Epoch: [13][230/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2834e+00 (1.6024e+00)	Acc@1  57.81 ( 55.04)	Acc@5  91.41 ( 84.45)
Epoch: [13][240/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5398e+00 (1.6058e+00)	Acc@1  57.03 ( 54.94)	Acc@5  84.38 ( 84.40)
Epoch: [13][250/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.4634e+00 (1.6069e+00)	Acc@1  58.59 ( 54.92)	Acc@5  88.28 ( 84.40)
Epoch: [13][260/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5880e+00 (1.6101e+00)	Acc@1  55.47 ( 54.86)	Acc@5  87.50 ( 84.31)
Epoch: [13][270/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.4718e+00 (1.6124e+00)	Acc@1  57.03 ( 54.78)	Acc@5  86.72 ( 84.26)
Epoch: [13][280/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8847e+00 (1.6153e+00)	Acc@1  50.78 ( 54.75)	Acc@5  74.22 ( 84.18)
Epoch: [13][290/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8115e+00 (1.6158e+00)	Acc@1  53.91 ( 54.71)	Acc@5  84.38 ( 84.22)
Epoch: [13][300/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6107e+00 (1.6181e+00)	Acc@1  56.25 ( 54.65)	Acc@5  85.94 ( 84.20)
Epoch: [13][310/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7209e+00 (1.6195e+00)	Acc@1  53.12 ( 54.56)	Acc@5  79.69 ( 84.16)
Epoch: [13][320/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5299e+00 (1.6201e+00)	Acc@1  57.81 ( 54.54)	Acc@5  83.59 ( 84.16)
Epoch: [13][330/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.4374e+00 (1.6180e+00)	Acc@1  57.81 ( 54.58)	Acc@5  89.06 ( 84.18)
Epoch: [13][340/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7449e+00 (1.6165e+00)	Acc@1  49.22 ( 54.62)	Acc@5  80.47 ( 84.23)
Epoch: [13][350/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6648e+00 (1.6154e+00)	Acc@1  52.34 ( 54.62)	Acc@5  84.38 ( 84.30)
Epoch: [13][360/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6616e+00 (1.6140e+00)	Acc@1  55.47 ( 54.66)	Acc@5  82.81 ( 84.32)
Epoch: [13][370/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.4955e+00 (1.6138e+00)	Acc@1  51.56 ( 54.65)	Acc@5  84.38 ( 84.30)
Epoch: [13][380/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5984e+00 (1.6140e+00)	Acc@1  54.69 ( 54.65)	Acc@5  85.94 ( 84.34)
Epoch: [13][390/391]	Time  0.178 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.4700e+00 (1.6132e+00)	Acc@1  60.00 ( 54.67)	Acc@5  88.75 ( 84.36)
## e[13] optimizer.zero_grad (sum) time: 0.494276762008667
## e[13]       loss.backward (sum) time: 11.093676328659058
## e[13]      optimizer.step (sum) time: 47.88457441329956
## epoch[13] training(only) time: 94.75651288032532
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 1.5303e+00 (1.5303e+00)	Acc@1  58.00 ( 58.00)	Acc@5  84.00 ( 84.00)
Test: [ 10/100]	Time  0.078 ( 0.088)	Loss 1.8566e+00 (1.6509e+00)	Acc@1  45.00 ( 56.45)	Acc@5  83.00 ( 83.27)
Test: [ 20/100]	Time  0.078 ( 0.083)	Loss 1.3658e+00 (1.6253e+00)	Acc@1  61.00 ( 56.24)	Acc@5  87.00 ( 84.19)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 1.6204e+00 (1.6464e+00)	Acc@1  52.00 ( 55.42)	Acc@5  85.00 ( 83.68)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.7924e+00 (1.6501e+00)	Acc@1  54.00 ( 54.93)	Acc@5  80.00 ( 83.78)
Test: [ 50/100]	Time  0.078 ( 0.080)	Loss 1.5748e+00 (1.6568e+00)	Acc@1  56.00 ( 54.71)	Acc@5  81.00 ( 83.43)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.8414e+00 (1.6580e+00)	Acc@1  51.00 ( 54.41)	Acc@5  80.00 ( 83.34)
Test: [ 70/100]	Time  0.077 ( 0.080)	Loss 1.8635e+00 (1.6571e+00)	Acc@1  46.00 ( 54.55)	Acc@5  75.00 ( 83.34)
Test: [ 80/100]	Time  0.079 ( 0.079)	Loss 1.6520e+00 (1.6582e+00)	Acc@1  54.00 ( 54.54)	Acc@5  82.00 ( 83.15)
Test: [ 90/100]	Time  0.077 ( 0.079)	Loss 1.6624e+00 (1.6518e+00)	Acc@1  59.00 ( 54.68)	Acc@5  84.00 ( 83.26)
 * Acc@1 54.800 Acc@5 83.360
### epoch[13] execution time: 102.76534986495972
EPOCH 14
# current learning rate: 0.1
# Switched to train mode...
Epoch: [14][  0/391]	Time  0.399 ( 0.399)	Data  0.151 ( 0.151)	Loss 1.4746e+00 (1.4746e+00)	Acc@1  60.16 ( 60.16)	Acc@5  84.38 ( 84.38)
Epoch: [14][ 10/391]	Time  0.241 ( 0.256)	Data  0.001 ( 0.015)	Loss 1.3358e+00 (1.4918e+00)	Acc@1  59.38 ( 58.24)	Acc@5  92.19 ( 86.15)
Epoch: [14][ 20/391]	Time  0.242 ( 0.249)	Data  0.001 ( 0.008)	Loss 1.5387e+00 (1.4961e+00)	Acc@1  57.81 ( 58.22)	Acc@5  83.59 ( 85.75)
Epoch: [14][ 30/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.006)	Loss 1.8211e+00 (1.5035e+00)	Acc@1  46.88 ( 58.04)	Acc@5  83.59 ( 85.69)
Epoch: [14][ 40/391]	Time  0.247 ( 0.245)	Data  0.001 ( 0.005)	Loss 1.5418e+00 (1.4903e+00)	Acc@1  57.03 ( 58.82)	Acc@5  86.72 ( 85.86)
Epoch: [14][ 50/391]	Time  0.242 ( 0.245)	Data  0.001 ( 0.004)	Loss 1.4954e+00 (1.4912e+00)	Acc@1  60.16 ( 58.56)	Acc@5  82.03 ( 86.00)
Epoch: [14][ 60/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.7225e+00 (1.4966e+00)	Acc@1  53.91 ( 58.39)	Acc@5  84.38 ( 85.96)
Epoch: [14][ 70/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 1.4360e+00 (1.4977e+00)	Acc@1  55.47 ( 58.12)	Acc@5  88.28 ( 85.97)
Epoch: [14][ 80/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.003)	Loss 1.6838e+00 (1.4966e+00)	Acc@1  59.38 ( 58.22)	Acc@5  82.03 ( 85.98)
Epoch: [14][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.4366e+00 (1.4903e+00)	Acc@1  57.81 ( 58.16)	Acc@5  90.62 ( 86.16)
Epoch: [14][100/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.3397e+00 (1.4938e+00)	Acc@1  62.50 ( 58.11)	Acc@5  89.84 ( 86.10)
Epoch: [14][110/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.6087e+00 (1.5009e+00)	Acc@1  57.03 ( 57.89)	Acc@5  84.38 ( 86.03)
Epoch: [14][120/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.5417e+00 (1.5030e+00)	Acc@1  53.12 ( 57.84)	Acc@5  84.38 ( 85.93)
Epoch: [14][130/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.6701e+00 (1.5080e+00)	Acc@1  53.91 ( 57.69)	Acc@5  85.16 ( 85.86)
Epoch: [14][140/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.6256e+00 (1.5121e+00)	Acc@1  51.56 ( 57.59)	Acc@5  84.38 ( 85.74)
Epoch: [14][150/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.6060e+00 (1.5097e+00)	Acc@1  55.47 ( 57.54)	Acc@5  82.81 ( 85.82)
Epoch: [14][160/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.4069e+00 (1.5108e+00)	Acc@1  58.59 ( 57.43)	Acc@5  89.06 ( 85.88)
Epoch: [14][170/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.7836e+00 (1.5098e+00)	Acc@1  52.34 ( 57.42)	Acc@5  82.81 ( 85.96)
Epoch: [14][180/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.5278e+00 (1.5102e+00)	Acc@1  54.69 ( 57.39)	Acc@5  85.16 ( 86.00)
Epoch: [14][190/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.3717e+00 (1.5100e+00)	Acc@1  64.06 ( 57.43)	Acc@5  89.84 ( 86.02)
Epoch: [14][200/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.5539e+00 (1.5098e+00)	Acc@1  57.03 ( 57.51)	Acc@5  87.50 ( 86.01)
Epoch: [14][210/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.4483e+00 (1.5061e+00)	Acc@1  57.81 ( 57.68)	Acc@5  86.72 ( 86.07)
Epoch: [14][220/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7173e+00 (1.5092e+00)	Acc@1  56.25 ( 57.63)	Acc@5  82.81 ( 86.03)
Epoch: [14][230/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5064e+00 (1.5109e+00)	Acc@1  56.25 ( 57.60)	Acc@5  86.72 ( 85.97)
Epoch: [14][240/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6389e+00 (1.5072e+00)	Acc@1  57.03 ( 57.69)	Acc@5  85.94 ( 86.04)
Epoch: [14][250/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6327e+00 (1.5111e+00)	Acc@1  54.69 ( 57.58)	Acc@5  85.16 ( 85.99)
Epoch: [14][260/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7043e+00 (1.5154e+00)	Acc@1  50.00 ( 57.41)	Acc@5  84.38 ( 85.93)
Epoch: [14][270/391]	Time  0.250 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5378e+00 (1.5156e+00)	Acc@1  54.69 ( 57.40)	Acc@5  84.38 ( 85.91)
Epoch: [14][280/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5965e+00 (1.5178e+00)	Acc@1  55.47 ( 57.36)	Acc@5  85.16 ( 85.83)
Epoch: [14][290/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.4979e+00 (1.5164e+00)	Acc@1  58.59 ( 57.40)	Acc@5  87.50 ( 85.85)
Epoch: [14][300/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5934e+00 (1.5161e+00)	Acc@1  57.03 ( 57.41)	Acc@5  81.25 ( 85.87)
Epoch: [14][310/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.4014e+00 (1.5181e+00)	Acc@1  57.81 ( 57.35)	Acc@5  88.28 ( 85.87)
Epoch: [14][320/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6815e+00 (1.5190e+00)	Acc@1  49.22 ( 57.33)	Acc@5  88.28 ( 85.86)
Epoch: [14][330/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6947e+00 (1.5215e+00)	Acc@1  50.00 ( 57.24)	Acc@5  83.59 ( 85.85)
Epoch: [14][340/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6038e+00 (1.5198e+00)	Acc@1  53.91 ( 57.28)	Acc@5  85.16 ( 85.88)
Epoch: [14][350/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.4693e+00 (1.5190e+00)	Acc@1  57.81 ( 57.32)	Acc@5  89.84 ( 85.89)
Epoch: [14][360/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.4541e+00 (1.5202e+00)	Acc@1  61.72 ( 57.30)	Acc@5  88.28 ( 85.90)
Epoch: [14][370/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8513e+00 (1.5193e+00)	Acc@1  49.22 ( 57.33)	Acc@5  82.03 ( 85.92)
Epoch: [14][380/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6240e+00 (1.5204e+00)	Acc@1  59.38 ( 57.31)	Acc@5  88.28 ( 85.93)
Epoch: [14][390/391]	Time  0.177 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5660e+00 (1.5208e+00)	Acc@1  57.50 ( 57.28)	Acc@5  90.00 ( 85.96)
## e[14] optimizer.zero_grad (sum) time: 0.4923117160797119
## e[14]       loss.backward (sum) time: 11.114269495010376
## e[14]      optimizer.step (sum) time: 47.87549042701721
## epoch[14] training(only) time: 94.80577898025513
# Switched to evaluate mode...
Test: [  0/100]	Time  0.235 ( 0.235)	Loss 1.6014e+00 (1.6014e+00)	Acc@1  55.00 ( 55.00)	Acc@5  85.00 ( 85.00)
Test: [ 10/100]	Time  0.078 ( 0.092)	Loss 2.0029e+00 (1.8055e+00)	Acc@1  45.00 ( 52.73)	Acc@5  80.00 ( 81.64)
Test: [ 20/100]	Time  0.077 ( 0.085)	Loss 1.5456e+00 (1.7622e+00)	Acc@1  58.00 ( 53.24)	Acc@5  89.00 ( 82.52)
Test: [ 30/100]	Time  0.078 ( 0.083)	Loss 1.9495e+00 (1.7880e+00)	Acc@1  47.00 ( 52.45)	Acc@5  83.00 ( 82.23)
Test: [ 40/100]	Time  0.077 ( 0.082)	Loss 1.7808e+00 (1.7927e+00)	Acc@1  55.00 ( 52.29)	Acc@5  84.00 ( 82.37)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 1.6907e+00 (1.8043e+00)	Acc@1  60.00 ( 52.45)	Acc@5  84.00 ( 82.06)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.7667e+00 (1.7951e+00)	Acc@1  45.00 ( 52.30)	Acc@5  83.00 ( 82.15)
Test: [ 70/100]	Time  0.079 ( 0.080)	Loss 1.9343e+00 (1.7987e+00)	Acc@1  45.00 ( 52.24)	Acc@5  78.00 ( 82.10)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 1.8108e+00 (1.8036e+00)	Acc@1  51.00 ( 52.10)	Acc@5  80.00 ( 82.01)
Test: [ 90/100]	Time  0.079 ( 0.080)	Loss 1.7163e+00 (1.7885e+00)	Acc@1  53.00 ( 52.31)	Acc@5  82.00 ( 82.22)
 * Acc@1 52.630 Acc@5 82.200
### epoch[14] execution time: 102.83989238739014
EPOCH 15
# current learning rate: 0.1
# Switched to train mode...
Epoch: [15][  0/391]	Time  0.390 ( 0.390)	Data  0.148 ( 0.148)	Loss 1.3006e+00 (1.3006e+00)	Acc@1  63.28 ( 63.28)	Acc@5  89.06 ( 89.06)
Epoch: [15][ 10/391]	Time  0.241 ( 0.254)	Data  0.001 ( 0.014)	Loss 1.3022e+00 (1.4304e+00)	Acc@1  60.94 ( 59.23)	Acc@5  89.84 ( 87.86)
Epoch: [15][ 20/391]	Time  0.240 ( 0.248)	Data  0.001 ( 0.008)	Loss 1.3145e+00 (1.4208e+00)	Acc@1  64.84 ( 59.71)	Acc@5  87.50 ( 87.65)
Epoch: [15][ 30/391]	Time  0.247 ( 0.246)	Data  0.001 ( 0.006)	Loss 1.2106e+00 (1.4045e+00)	Acc@1  64.06 ( 60.18)	Acc@5  89.06 ( 87.78)
Epoch: [15][ 40/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.005)	Loss 1.5193e+00 (1.4080e+00)	Acc@1  58.59 ( 60.06)	Acc@5  85.94 ( 87.52)
Epoch: [15][ 50/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.3967e+00 (1.4241e+00)	Acc@1  64.84 ( 59.97)	Acc@5  89.84 ( 87.16)
Epoch: [15][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.2981e+00 (1.4178e+00)	Acc@1  63.28 ( 60.04)	Acc@5  92.19 ( 87.38)
Epoch: [15][ 70/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.4047e+00 (1.4198e+00)	Acc@1  61.72 ( 59.83)	Acc@5  85.94 ( 87.30)
Epoch: [15][ 80/391]	Time  0.247 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.6287e+00 (1.4228e+00)	Acc@1  58.59 ( 59.75)	Acc@5  85.94 ( 87.23)
Epoch: [15][ 90/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.5303e+00 (1.4285e+00)	Acc@1  55.47 ( 59.56)	Acc@5  85.16 ( 87.18)
Epoch: [15][100/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.4570e+00 (1.4311e+00)	Acc@1  65.62 ( 59.54)	Acc@5  84.38 ( 87.13)
Epoch: [15][110/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.4340e+00 (1.4318e+00)	Acc@1  53.91 ( 59.40)	Acc@5  85.94 ( 87.29)
Epoch: [15][120/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.3771e+00 (1.4309e+00)	Acc@1  54.69 ( 59.48)	Acc@5  89.06 ( 87.36)
Epoch: [15][130/391]	Time  0.246 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.3565e+00 (1.4306e+00)	Acc@1  57.81 ( 59.50)	Acc@5  90.62 ( 87.42)
Epoch: [15][140/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5067e+00 (1.4325e+00)	Acc@1  59.38 ( 59.51)	Acc@5  88.28 ( 87.38)
Epoch: [15][150/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2292e+00 (1.4336e+00)	Acc@1  63.28 ( 59.39)	Acc@5  92.19 ( 87.42)
Epoch: [15][160/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3853e+00 (1.4309e+00)	Acc@1  59.38 ( 59.41)	Acc@5  86.72 ( 87.46)
Epoch: [15][170/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.4971e+00 (1.4312e+00)	Acc@1  57.03 ( 59.34)	Acc@5  87.50 ( 87.50)
Epoch: [15][180/391]	Time  0.247 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3607e+00 (1.4360e+00)	Acc@1  62.50 ( 59.24)	Acc@5  88.28 ( 87.45)
Epoch: [15][190/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6180e+00 (1.4364e+00)	Acc@1  46.88 ( 59.23)	Acc@5  87.50 ( 87.46)
Epoch: [15][200/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5060e+00 (1.4384e+00)	Acc@1  57.03 ( 59.20)	Acc@5  85.94 ( 87.41)
Epoch: [15][210/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3951e+00 (1.4356e+00)	Acc@1  61.72 ( 59.25)	Acc@5  85.94 ( 87.38)
Epoch: [15][220/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5410e+00 (1.4338e+00)	Acc@1  55.47 ( 59.29)	Acc@5  85.94 ( 87.43)
Epoch: [15][230/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.4790e+00 (1.4339e+00)	Acc@1  51.56 ( 59.27)	Acc@5  88.28 ( 87.44)
Epoch: [15][240/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5913e+00 (1.4365e+00)	Acc@1  53.91 ( 59.25)	Acc@5  85.16 ( 87.34)
Epoch: [15][250/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5039e+00 (1.4362e+00)	Acc@1  59.38 ( 59.23)	Acc@5  83.59 ( 87.28)
Epoch: [15][260/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3734e+00 (1.4388e+00)	Acc@1  54.69 ( 59.14)	Acc@5  92.19 ( 87.26)
Epoch: [15][270/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3325e+00 (1.4387e+00)	Acc@1  66.41 ( 59.20)	Acc@5  87.50 ( 87.24)
Epoch: [15][280/391]	Time  0.247 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5043e+00 (1.4384e+00)	Acc@1  55.47 ( 59.23)	Acc@5  84.38 ( 87.24)
Epoch: [15][290/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5789e+00 (1.4370e+00)	Acc@1  59.38 ( 59.33)	Acc@5  81.25 ( 87.24)
Epoch: [15][300/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3626e+00 (1.4368e+00)	Acc@1  60.94 ( 59.35)	Acc@5  89.84 ( 87.26)
Epoch: [15][310/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3792e+00 (1.4379e+00)	Acc@1  64.06 ( 59.34)	Acc@5  85.94 ( 87.27)
Epoch: [15][320/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7760e+00 (1.4406e+00)	Acc@1  50.78 ( 59.24)	Acc@5  83.59 ( 87.24)
Epoch: [15][330/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.4830e+00 (1.4427e+00)	Acc@1  57.81 ( 59.20)	Acc@5  88.28 ( 87.20)
Epoch: [15][340/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6615e+00 (1.4446e+00)	Acc@1  58.59 ( 59.15)	Acc@5  85.16 ( 87.17)
Epoch: [15][350/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5725e+00 (1.4456e+00)	Acc@1  52.34 ( 59.09)	Acc@5  85.16 ( 87.18)
Epoch: [15][360/391]	Time  0.254 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6122e+00 (1.4470e+00)	Acc@1  57.03 ( 59.09)	Acc@5  85.16 ( 87.15)
Epoch: [15][370/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.4689e+00 (1.4477e+00)	Acc@1  57.81 ( 59.10)	Acc@5  86.72 ( 87.12)
Epoch: [15][380/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.4076e+00 (1.4470e+00)	Acc@1  60.16 ( 59.12)	Acc@5  85.94 ( 87.11)
Epoch: [15][390/391]	Time  0.178 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5097e+00 (1.4476e+00)	Acc@1  61.25 ( 59.11)	Acc@5  81.25 ( 87.11)
## e[15] optimizer.zero_grad (sum) time: 0.49231958389282227
## e[15]       loss.backward (sum) time: 11.088548421859741
## e[15]      optimizer.step (sum) time: 47.89067316055298
## epoch[15] training(only) time: 94.74275827407837
# Switched to evaluate mode...
Test: [  0/100]	Time  0.210 ( 0.210)	Loss 1.4866e+00 (1.4866e+00)	Acc@1  66.00 ( 66.00)	Acc@5  85.00 ( 85.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.5128e+00 (1.6090e+00)	Acc@1  58.00 ( 58.09)	Acc@5  87.00 ( 84.18)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 1.3582e+00 (1.5878e+00)	Acc@1  68.00 ( 57.90)	Acc@5  88.00 ( 84.52)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 1.6169e+00 (1.6167e+00)	Acc@1  51.00 ( 56.23)	Acc@5  87.00 ( 84.32)
Test: [ 40/100]	Time  0.079 ( 0.081)	Loss 1.5731e+00 (1.6127e+00)	Acc@1  56.00 ( 55.78)	Acc@5  82.00 ( 84.17)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 1.5508e+00 (1.6184e+00)	Acc@1  54.00 ( 55.73)	Acc@5  83.00 ( 83.94)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.6301e+00 (1.6246e+00)	Acc@1  52.00 ( 55.44)	Acc@5  87.00 ( 83.70)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 1.5462e+00 (1.6342e+00)	Acc@1  54.00 ( 55.28)	Acc@5  85.00 ( 83.49)
Test: [ 80/100]	Time  0.079 ( 0.080)	Loss 1.7130e+00 (1.6380e+00)	Acc@1  54.00 ( 55.25)	Acc@5  81.00 ( 83.48)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.7120e+00 (1.6214e+00)	Acc@1  54.00 ( 55.69)	Acc@5  82.00 ( 83.91)
 * Acc@1 55.820 Acc@5 83.910
### epoch[15] execution time: 102.76592087745667
EPOCH 16
# current learning rate: 0.1
# Switched to train mode...
Epoch: [16][  0/391]	Time  0.398 ( 0.398)	Data  0.156 ( 0.156)	Loss 1.2859e+00 (1.2859e+00)	Acc@1  63.28 ( 63.28)	Acc@5  87.50 ( 87.50)
Epoch: [16][ 10/391]	Time  0.241 ( 0.256)	Data  0.001 ( 0.015)	Loss 1.4685e+00 (1.3745e+00)	Acc@1  57.03 ( 60.44)	Acc@5  91.41 ( 88.49)
Epoch: [16][ 20/391]	Time  0.241 ( 0.249)	Data  0.001 ( 0.009)	Loss 1.3133e+00 (1.3302e+00)	Acc@1  63.28 ( 62.39)	Acc@5  86.72 ( 88.65)
Epoch: [16][ 30/391]	Time  0.242 ( 0.247)	Data  0.001 ( 0.006)	Loss 1.4869e+00 (1.3339e+00)	Acc@1  59.38 ( 62.32)	Acc@5  83.59 ( 88.79)
Epoch: [16][ 40/391]	Time  0.245 ( 0.246)	Data  0.001 ( 0.005)	Loss 1.4843e+00 (1.3477e+00)	Acc@1  56.25 ( 62.10)	Acc@5  90.62 ( 88.93)
Epoch: [16][ 50/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.004)	Loss 1.1839e+00 (1.3440e+00)	Acc@1  62.50 ( 62.03)	Acc@5  91.41 ( 89.11)
Epoch: [16][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.3265e+00 (1.3433e+00)	Acc@1  56.25 ( 61.74)	Acc@5  89.84 ( 89.06)
Epoch: [16][ 70/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 1.4880e+00 (1.3408e+00)	Acc@1  57.81 ( 61.87)	Acc@5  86.72 ( 88.93)
Epoch: [16][ 80/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.003)	Loss 1.2496e+00 (1.3409e+00)	Acc@1  64.06 ( 61.80)	Acc@5  86.72 ( 88.92)
Epoch: [16][ 90/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.3763e+00 (1.3435e+00)	Acc@1  58.59 ( 61.63)	Acc@5  88.28 ( 88.82)
Epoch: [16][100/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.3823e+00 (1.3424e+00)	Acc@1  58.59 ( 61.59)	Acc@5  87.50 ( 88.86)
Epoch: [16][110/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.2232e+00 (1.3464e+00)	Acc@1  67.19 ( 61.41)	Acc@5  88.28 ( 89.00)
Epoch: [16][120/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.5831e+00 (1.3548e+00)	Acc@1  56.25 ( 61.31)	Acc@5  86.72 ( 88.91)
Epoch: [16][130/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.5940e+00 (1.3548e+00)	Acc@1  56.25 ( 61.15)	Acc@5  85.16 ( 88.95)
Epoch: [16][140/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.4021e+00 (1.3572e+00)	Acc@1  57.81 ( 61.10)	Acc@5  88.28 ( 88.87)
Epoch: [16][150/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.4912e+00 (1.3570e+00)	Acc@1  55.47 ( 61.14)	Acc@5  89.06 ( 88.91)
Epoch: [16][160/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.3473e+00 (1.3592e+00)	Acc@1  60.16 ( 61.10)	Acc@5  89.84 ( 88.89)
Epoch: [16][170/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.7401e+00 (1.3600e+00)	Acc@1  56.25 ( 61.12)	Acc@5  82.03 ( 88.88)
Epoch: [16][180/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.3205e+00 (1.3619e+00)	Acc@1  66.41 ( 61.09)	Acc@5  87.50 ( 88.79)
Epoch: [16][190/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2147e+00 (1.3643e+00)	Acc@1  67.19 ( 61.05)	Acc@5  88.28 ( 88.70)
Epoch: [16][200/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3371e+00 (1.3638e+00)	Acc@1  60.94 ( 61.03)	Acc@5  92.97 ( 88.70)
Epoch: [16][210/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.4421e+00 (1.3642e+00)	Acc@1  57.03 ( 60.97)	Acc@5  89.06 ( 88.71)
Epoch: [16][220/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6981e+00 (1.3661e+00)	Acc@1  56.25 ( 60.96)	Acc@5  80.47 ( 88.65)
Epoch: [16][230/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.4791e+00 (1.3688e+00)	Acc@1  63.28 ( 60.88)	Acc@5  87.50 ( 88.61)
Epoch: [16][240/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1878e+00 (1.3715e+00)	Acc@1  63.28 ( 60.80)	Acc@5  92.19 ( 88.58)
Epoch: [16][250/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.4601e+00 (1.3743e+00)	Acc@1  57.03 ( 60.77)	Acc@5  85.94 ( 88.54)
Epoch: [16][260/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.4711e+00 (1.3764e+00)	Acc@1  59.38 ( 60.70)	Acc@5  85.16 ( 88.47)
Epoch: [16][270/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3805e+00 (1.3767e+00)	Acc@1  61.72 ( 60.68)	Acc@5  89.06 ( 88.46)
Epoch: [16][280/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3244e+00 (1.3763e+00)	Acc@1  66.41 ( 60.67)	Acc@5  89.06 ( 88.51)
Epoch: [16][290/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2907e+00 (1.3769e+00)	Acc@1  62.50 ( 60.65)	Acc@5  88.28 ( 88.50)
Epoch: [16][300/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1669e+00 (1.3780e+00)	Acc@1  63.28 ( 60.60)	Acc@5  92.19 ( 88.46)
Epoch: [16][310/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3952e+00 (1.3789e+00)	Acc@1  70.31 ( 60.64)	Acc@5  85.94 ( 88.45)
Epoch: [16][320/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1971e+00 (1.3792e+00)	Acc@1  64.06 ( 60.69)	Acc@5  90.62 ( 88.44)
Epoch: [16][330/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0996e+00 (1.3775e+00)	Acc@1  67.97 ( 60.76)	Acc@5  91.41 ( 88.46)
Epoch: [16][340/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8196e+00 (1.3792e+00)	Acc@1  51.56 ( 60.75)	Acc@5  77.34 ( 88.43)
Epoch: [16][350/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3756e+00 (1.3789e+00)	Acc@1  58.59 ( 60.73)	Acc@5  89.06 ( 88.44)
Epoch: [16][360/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.4780e+00 (1.3813e+00)	Acc@1  57.03 ( 60.65)	Acc@5  85.16 ( 88.42)
Epoch: [16][370/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5028e+00 (1.3844e+00)	Acc@1  57.81 ( 60.61)	Acc@5  85.16 ( 88.38)
Epoch: [16][380/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2858e+00 (1.3850e+00)	Acc@1  62.50 ( 60.61)	Acc@5  90.62 ( 88.37)
Epoch: [16][390/391]	Time  0.179 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3015e+00 (1.3840e+00)	Acc@1  65.00 ( 60.63)	Acc@5  91.25 ( 88.38)
## e[16] optimizer.zero_grad (sum) time: 0.4933054447174072
## e[16]       loss.backward (sum) time: 11.110371589660645
## e[16]      optimizer.step (sum) time: 47.84814929962158
## epoch[16] training(only) time: 94.7989149093628
# Switched to evaluate mode...
Test: [  0/100]	Time  0.226 ( 0.226)	Loss 1.4676e+00 (1.4676e+00)	Acc@1  52.00 ( 52.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.078 ( 0.091)	Loss 1.6220e+00 (1.5932e+00)	Acc@1  52.00 ( 57.73)	Acc@5  90.00 ( 83.91)
Test: [ 20/100]	Time  0.078 ( 0.085)	Loss 1.2965e+00 (1.5492e+00)	Acc@1  60.00 ( 57.95)	Acc@5  91.00 ( 84.76)
Test: [ 30/100]	Time  0.079 ( 0.083)	Loss 1.6466e+00 (1.5832e+00)	Acc@1  49.00 ( 57.58)	Acc@5  89.00 ( 84.10)
Test: [ 40/100]	Time  0.078 ( 0.082)	Loss 1.6098e+00 (1.5889e+00)	Acc@1  57.00 ( 57.22)	Acc@5  83.00 ( 84.12)
Test: [ 50/100]	Time  0.079 ( 0.081)	Loss 1.5581e+00 (1.5998e+00)	Acc@1  59.00 ( 57.31)	Acc@5  87.00 ( 83.98)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.7778e+00 (1.5922e+00)	Acc@1  52.00 ( 56.98)	Acc@5  82.00 ( 84.23)
Test: [ 70/100]	Time  0.079 ( 0.080)	Loss 1.7994e+00 (1.5957e+00)	Acc@1  54.00 ( 56.93)	Acc@5  81.00 ( 84.31)
Test: [ 80/100]	Time  0.079 ( 0.080)	Loss 1.5456e+00 (1.6010e+00)	Acc@1  56.00 ( 56.89)	Acc@5  85.00 ( 84.07)
Test: [ 90/100]	Time  0.079 ( 0.080)	Loss 1.6474e+00 (1.5949e+00)	Acc@1  54.00 ( 57.04)	Acc@5  86.00 ( 84.25)
 * Acc@1 57.200 Acc@5 84.300
### epoch[16] execution time: 102.84247612953186
EPOCH 17
# current learning rate: 0.1
# Switched to train mode...
Epoch: [17][  0/391]	Time  0.400 ( 0.400)	Data  0.158 ( 0.158)	Loss 1.3001e+00 (1.3001e+00)	Acc@1  63.28 ( 63.28)	Acc@5  87.50 ( 87.50)
Epoch: [17][ 10/391]	Time  0.241 ( 0.256)	Data  0.001 ( 0.015)	Loss 1.4556e+00 (1.2695e+00)	Acc@1  58.59 ( 62.64)	Acc@5  87.50 ( 90.62)
Epoch: [17][ 20/391]	Time  0.249 ( 0.249)	Data  0.001 ( 0.009)	Loss 1.2737e+00 (1.2546e+00)	Acc@1  62.50 ( 63.84)	Acc@5  91.41 ( 90.51)
Epoch: [17][ 30/391]	Time  0.241 ( 0.247)	Data  0.001 ( 0.006)	Loss 1.2550e+00 (1.2475e+00)	Acc@1  62.50 ( 64.67)	Acc@5  91.41 ( 90.68)
Epoch: [17][ 40/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.005)	Loss 1.3903e+00 (1.2557e+00)	Acc@1  61.72 ( 64.58)	Acc@5  90.62 ( 90.36)
Epoch: [17][ 50/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.4601e+00 (1.2637e+00)	Acc@1  59.38 ( 63.92)	Acc@5  85.94 ( 90.24)
Epoch: [17][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.0954e+00 (1.2518e+00)	Acc@1  67.97 ( 64.16)	Acc@5  95.31 ( 90.45)
Epoch: [17][ 70/391]	Time  0.246 ( 0.244)	Data  0.001 ( 0.003)	Loss 1.3921e+00 (1.2525e+00)	Acc@1  57.03 ( 64.04)	Acc@5  85.16 ( 90.39)
Epoch: [17][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.1630e+00 (1.2464e+00)	Acc@1  67.97 ( 64.25)	Acc@5  94.53 ( 90.34)
Epoch: [17][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.4565e+00 (1.2501e+00)	Acc@1  59.38 ( 64.06)	Acc@5  88.28 ( 90.26)
Epoch: [17][100/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.2741e+00 (1.2480e+00)	Acc@1  64.06 ( 64.26)	Acc@5  84.38 ( 90.14)
Epoch: [17][110/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.3992e+00 (1.2488e+00)	Acc@1  62.50 ( 64.23)	Acc@5  85.16 ( 90.10)
Epoch: [17][120/391]	Time  0.247 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.2012e+00 (1.2490e+00)	Acc@1  64.84 ( 64.23)	Acc@5  92.19 ( 90.10)
Epoch: [17][130/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.0306e+00 (1.2500e+00)	Acc@1  70.31 ( 64.30)	Acc@5  91.41 ( 90.01)
Epoch: [17][140/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.2387e+00 (1.2513e+00)	Acc@1  66.41 ( 64.20)	Acc@5  92.97 ( 90.02)
Epoch: [17][150/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3395e+00 (1.2482e+00)	Acc@1  60.16 ( 64.34)	Acc@5  90.62 ( 90.05)
Epoch: [17][160/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6530e+00 (1.2541e+00)	Acc@1  57.03 ( 64.22)	Acc@5  83.59 ( 89.93)
Epoch: [17][170/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2219e+00 (1.2549e+00)	Acc@1  60.94 ( 64.10)	Acc@5  93.75 ( 89.91)
Epoch: [17][180/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2309e+00 (1.2549e+00)	Acc@1  62.50 ( 64.00)	Acc@5  90.62 ( 89.90)
Epoch: [17][190/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2952e+00 (1.2553e+00)	Acc@1  61.72 ( 63.98)	Acc@5  92.19 ( 89.88)
Epoch: [17][200/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3352e+00 (1.2572e+00)	Acc@1  64.06 ( 63.93)	Acc@5  89.84 ( 89.86)
Epoch: [17][210/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5025e+00 (1.2635e+00)	Acc@1  57.03 ( 63.83)	Acc@5  84.38 ( 89.76)
Epoch: [17][220/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5251e+00 (1.2667e+00)	Acc@1  60.94 ( 63.73)	Acc@5  83.59 ( 89.68)
Epoch: [17][230/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2465e+00 (1.2673e+00)	Acc@1  66.41 ( 63.71)	Acc@5  88.28 ( 89.66)
Epoch: [17][240/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5622e+00 (1.2700e+00)	Acc@1  57.03 ( 63.61)	Acc@5  84.38 ( 89.64)
Epoch: [17][250/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3368e+00 (1.2754e+00)	Acc@1  59.38 ( 63.46)	Acc@5  89.06 ( 89.57)
Epoch: [17][260/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2254e+00 (1.2782e+00)	Acc@1  65.62 ( 63.39)	Acc@5  90.62 ( 89.50)
Epoch: [17][270/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0746e+00 (1.2785e+00)	Acc@1  65.62 ( 63.42)	Acc@5  89.06 ( 89.47)
Epoch: [17][280/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3242e+00 (1.2792e+00)	Acc@1  63.28 ( 63.41)	Acc@5  88.28 ( 89.47)
Epoch: [17][290/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3025e+00 (1.2805e+00)	Acc@1  63.28 ( 63.38)	Acc@5  88.28 ( 89.47)
Epoch: [17][300/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3091e+00 (1.2831e+00)	Acc@1  59.38 ( 63.28)	Acc@5  90.62 ( 89.43)
Epoch: [17][310/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3517e+00 (1.2829e+00)	Acc@1  63.28 ( 63.32)	Acc@5  88.28 ( 89.41)
Epoch: [17][320/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1641e+00 (1.2810e+00)	Acc@1  68.75 ( 63.36)	Acc@5  91.41 ( 89.43)
Epoch: [17][330/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2180e+00 (1.2847e+00)	Acc@1  60.94 ( 63.23)	Acc@5  90.62 ( 89.41)
Epoch: [17][340/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5848e+00 (1.2863e+00)	Acc@1  60.16 ( 63.20)	Acc@5  80.47 ( 89.33)
Epoch: [17][350/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2307e+00 (1.2879e+00)	Acc@1  63.28 ( 63.16)	Acc@5  89.84 ( 89.34)
Epoch: [17][360/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2602e+00 (1.2887e+00)	Acc@1  66.41 ( 63.12)	Acc@5  93.75 ( 89.33)
Epoch: [17][370/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2995e+00 (1.2894e+00)	Acc@1  62.50 ( 63.10)	Acc@5  93.75 ( 89.33)
Epoch: [17][380/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3567e+00 (1.2887e+00)	Acc@1  67.97 ( 63.14)	Acc@5  82.81 ( 89.30)
Epoch: [17][390/391]	Time  0.178 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5348e+00 (1.2922e+00)	Acc@1  62.50 ( 63.06)	Acc@5  83.75 ( 89.27)
## e[17] optimizer.zero_grad (sum) time: 0.4942359924316406
## e[17]       loss.backward (sum) time: 11.097002506256104
## e[17]      optimizer.step (sum) time: 47.87928891181946
## epoch[17] training(only) time: 94.72443723678589
# Switched to evaluate mode...
Test: [  0/100]	Time  0.235 ( 0.235)	Loss 1.5553e+00 (1.5553e+00)	Acc@1  60.00 ( 60.00)	Acc@5  84.00 ( 84.00)
Test: [ 10/100]	Time  0.078 ( 0.092)	Loss 1.4904e+00 (1.5935e+00)	Acc@1  62.00 ( 59.18)	Acc@5  87.00 ( 84.64)
Test: [ 20/100]	Time  0.078 ( 0.086)	Loss 1.2755e+00 (1.5723e+00)	Acc@1  63.00 ( 57.86)	Acc@5  89.00 ( 84.67)
Test: [ 30/100]	Time  0.078 ( 0.083)	Loss 1.5278e+00 (1.5889e+00)	Acc@1  54.00 ( 57.26)	Acc@5  89.00 ( 84.29)
Test: [ 40/100]	Time  0.078 ( 0.082)	Loss 1.3772e+00 (1.5891e+00)	Acc@1  63.00 ( 57.07)	Acc@5  85.00 ( 84.73)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 1.5828e+00 (1.6007e+00)	Acc@1  58.00 ( 56.80)	Acc@5  86.00 ( 84.65)
Test: [ 60/100]	Time  0.079 ( 0.081)	Loss 1.6667e+00 (1.5884e+00)	Acc@1  53.00 ( 56.89)	Acc@5  84.00 ( 84.90)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 1.5357e+00 (1.5895e+00)	Acc@1  52.00 ( 56.87)	Acc@5  88.00 ( 84.89)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 1.5372e+00 (1.5918e+00)	Acc@1  58.00 ( 56.86)	Acc@5  87.00 ( 84.77)
Test: [ 90/100]	Time  0.078 ( 0.080)	Loss 1.7505e+00 (1.5817e+00)	Acc@1  52.00 ( 57.07)	Acc@5  84.00 ( 84.96)
 * Acc@1 57.170 Acc@5 84.990
### epoch[17] execution time: 102.78812217712402
EPOCH 18
# current learning rate: 0.1
# Switched to train mode...
Epoch: [18][  0/391]	Time  0.386 ( 0.386)	Data  0.144 ( 0.144)	Loss 1.1571e+00 (1.1571e+00)	Acc@1  66.41 ( 66.41)	Acc@5  92.19 ( 92.19)
Epoch: [18][ 10/391]	Time  0.241 ( 0.254)	Data  0.001 ( 0.014)	Loss 1.0037e+00 (1.1683e+00)	Acc@1  72.66 ( 66.41)	Acc@5  94.53 ( 91.55)
Epoch: [18][ 20/391]	Time  0.241 ( 0.248)	Data  0.001 ( 0.008)	Loss 1.2667e+00 (1.1486e+00)	Acc@1  60.94 ( 66.59)	Acc@5  89.84 ( 91.89)
Epoch: [18][ 30/391]	Time  0.242 ( 0.246)	Data  0.001 ( 0.006)	Loss 1.0364e+00 (1.1516e+00)	Acc@1  67.97 ( 66.78)	Acc@5  91.41 ( 91.56)
Epoch: [18][ 40/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.005)	Loss 1.1426e+00 (1.1570e+00)	Acc@1  65.62 ( 66.33)	Acc@5  94.53 ( 91.63)
Epoch: [18][ 50/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.0916e+00 (1.1677e+00)	Acc@1  64.84 ( 65.98)	Acc@5  96.09 ( 91.61)
Epoch: [18][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.1726e+00 (1.1615e+00)	Acc@1  67.19 ( 66.19)	Acc@5  90.62 ( 91.59)
Epoch: [18][ 70/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 1.0808e+00 (1.1584e+00)	Acc@1  67.97 ( 66.30)	Acc@5  95.31 ( 91.78)
Epoch: [18][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.3574e+00 (1.1735e+00)	Acc@1  59.38 ( 65.79)	Acc@5  89.06 ( 91.44)
Epoch: [18][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.1891e+00 (1.1832e+00)	Acc@1  67.97 ( 65.70)	Acc@5  92.19 ( 91.21)
Epoch: [18][100/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.2615e+00 (1.1896e+00)	Acc@1  61.72 ( 65.55)	Acc@5  92.97 ( 91.07)
Epoch: [18][110/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.5119e+00 (1.1959e+00)	Acc@1  57.81 ( 65.34)	Acc@5  85.94 ( 90.93)
Epoch: [18][120/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 9.5227e-01 (1.1944e+00)	Acc@1  72.66 ( 65.32)	Acc@5  93.75 ( 90.97)
Epoch: [18][130/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1641e+00 (1.1935e+00)	Acc@1  64.84 ( 65.38)	Acc@5  92.19 ( 91.02)
Epoch: [18][140/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1977e+00 (1.1956e+00)	Acc@1  68.75 ( 65.28)	Acc@5  88.28 ( 90.99)
Epoch: [18][150/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.2071e+00 (1.1970e+00)	Acc@1  66.41 ( 65.27)	Acc@5  89.06 ( 90.97)
Epoch: [18][160/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.4072e+00 (1.1989e+00)	Acc@1  60.16 ( 65.22)	Acc@5  87.50 ( 90.91)
Epoch: [18][170/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.5893e+00 (1.2035e+00)	Acc@1  55.47 ( 65.04)	Acc@5  82.03 ( 90.81)
Epoch: [18][180/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.0120e+00 (1.2062e+00)	Acc@1  67.19 ( 64.98)	Acc@5  92.97 ( 90.80)
Epoch: [18][190/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1417e+00 (1.2082e+00)	Acc@1  67.97 ( 64.98)	Acc@5  89.06 ( 90.74)
Epoch: [18][200/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1506e+00 (1.2074e+00)	Acc@1  64.84 ( 65.00)	Acc@5  90.62 ( 90.72)
Epoch: [18][210/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.2810e+00 (1.2114e+00)	Acc@1  62.50 ( 64.87)	Acc@5  88.28 ( 90.67)
Epoch: [18][220/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.2885e+00 (1.2130e+00)	Acc@1  60.94 ( 64.82)	Acc@5  90.62 ( 90.67)
Epoch: [18][230/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.5452e+00 (1.2149e+00)	Acc@1  57.81 ( 64.73)	Acc@5  85.94 ( 90.64)
Epoch: [18][240/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.3077e+00 (1.2183e+00)	Acc@1  60.94 ( 64.61)	Acc@5  91.41 ( 90.59)
Epoch: [18][250/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.2944e+00 (1.2197e+00)	Acc@1  57.81 ( 64.58)	Acc@5  92.19 ( 90.53)
Epoch: [18][260/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.4729e+00 (1.2212e+00)	Acc@1  61.72 ( 64.53)	Acc@5  89.84 ( 90.52)
Epoch: [18][270/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1627e+00 (1.2209e+00)	Acc@1  64.06 ( 64.57)	Acc@5  92.19 ( 90.52)
Epoch: [18][280/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.2864e+00 (1.2191e+00)	Acc@1  63.28 ( 64.64)	Acc@5  89.84 ( 90.51)
Epoch: [18][290/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.3864e+00 (1.2211e+00)	Acc@1  64.84 ( 64.63)	Acc@5  82.81 ( 90.42)
Epoch: [18][300/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.5544e+00 (1.2256e+00)	Acc@1  56.25 ( 64.52)	Acc@5  85.16 ( 90.35)
Epoch: [18][310/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.4517e+00 (1.2273e+00)	Acc@1  53.91 ( 64.43)	Acc@5  87.50 ( 90.35)
Epoch: [18][320/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.0322e+00 (1.2280e+00)	Acc@1  70.31 ( 64.41)	Acc@5  91.41 ( 90.32)
Epoch: [18][330/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.4416e+00 (1.2302e+00)	Acc@1  60.16 ( 64.35)	Acc@5  85.94 ( 90.29)
Epoch: [18][340/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.3237e+00 (1.2295e+00)	Acc@1  61.72 ( 64.38)	Acc@5  88.28 ( 90.30)
Epoch: [18][350/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.2851e+00 (1.2292e+00)	Acc@1  63.28 ( 64.41)	Acc@5  91.41 ( 90.32)
Epoch: [18][360/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.0643e+00 (1.2301e+00)	Acc@1  71.88 ( 64.39)	Acc@5  91.41 ( 90.29)
Epoch: [18][370/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.3200e+00 (1.2325e+00)	Acc@1  63.28 ( 64.37)	Acc@5  88.28 ( 90.27)
Epoch: [18][380/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.5645e+00 (1.2336e+00)	Acc@1  52.34 ( 64.32)	Acc@5  87.50 ( 90.28)
Epoch: [18][390/391]	Time  0.177 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5833e+00 (1.2373e+00)	Acc@1  60.00 ( 64.22)	Acc@5  82.50 ( 90.23)
## e[18] optimizer.zero_grad (sum) time: 0.49634838104248047
## e[18]       loss.backward (sum) time: 11.078643321990967
## e[18]      optimizer.step (sum) time: 47.93959975242615
## epoch[18] training(only) time: 94.8752269744873
# Switched to evaluate mode...
Test: [  0/100]	Time  0.212 ( 0.212)	Loss 1.5576e+00 (1.5576e+00)	Acc@1  57.00 ( 57.00)	Acc@5  82.00 ( 82.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.5509e+00 (1.5132e+00)	Acc@1  58.00 ( 58.64)	Acc@5  90.00 ( 86.55)
Test: [ 20/100]	Time  0.077 ( 0.084)	Loss 1.3234e+00 (1.4524e+00)	Acc@1  62.00 ( 60.43)	Acc@5  90.00 ( 86.81)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 1.5893e+00 (1.4753e+00)	Acc@1  50.00 ( 59.65)	Acc@5  88.00 ( 86.16)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.3851e+00 (1.4928e+00)	Acc@1  63.00 ( 58.73)	Acc@5  85.00 ( 85.85)
Test: [ 50/100]	Time  0.079 ( 0.081)	Loss 1.4839e+00 (1.5030e+00)	Acc@1  58.00 ( 58.45)	Acc@5  85.00 ( 85.51)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.5488e+00 (1.4970e+00)	Acc@1  54.00 ( 58.44)	Acc@5  88.00 ( 85.77)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 1.5505e+00 (1.4954e+00)	Acc@1  55.00 ( 58.55)	Acc@5  85.00 ( 85.83)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 1.6108e+00 (1.4984e+00)	Acc@1  56.00 ( 58.62)	Acc@5  83.00 ( 85.85)
Test: [ 90/100]	Time  0.079 ( 0.079)	Loss 1.4906e+00 (1.4851e+00)	Acc@1  63.00 ( 59.00)	Acc@5  85.00 ( 86.01)
 * Acc@1 59.170 Acc@5 86.020
### epoch[18] execution time: 102.89514398574829
EPOCH 19
# current learning rate: 0.1
# Switched to train mode...
Epoch: [19][  0/391]	Time  0.397 ( 0.397)	Data  0.154 ( 0.154)	Loss 8.8808e-01 (8.8808e-01)	Acc@1  75.78 ( 75.78)	Acc@5  94.53 ( 94.53)
Epoch: [19][ 10/391]	Time  0.242 ( 0.255)	Data  0.001 ( 0.015)	Loss 1.1848e+00 (1.1437e+00)	Acc@1  64.84 ( 67.12)	Acc@5  92.19 ( 91.41)
Epoch: [19][ 20/391]	Time  0.241 ( 0.249)	Data  0.001 ( 0.008)	Loss 1.0143e+00 (1.1303e+00)	Acc@1  67.97 ( 66.70)	Acc@5  96.88 ( 91.85)
Epoch: [19][ 30/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.006)	Loss 1.0285e+00 (1.1000e+00)	Acc@1  67.19 ( 67.34)	Acc@5  91.41 ( 92.04)
Epoch: [19][ 40/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.005)	Loss 1.3294e+00 (1.1090e+00)	Acc@1  59.38 ( 67.00)	Acc@5  87.50 ( 91.81)
Epoch: [19][ 50/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.2287e+00 (1.1105e+00)	Acc@1  65.62 ( 67.08)	Acc@5  88.28 ( 91.73)
Epoch: [19][ 60/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.2493e+00 (1.1159e+00)	Acc@1  60.94 ( 67.09)	Acc@5  91.41 ( 91.82)
Epoch: [19][ 70/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.003)	Loss 9.9715e-01 (1.1128e+00)	Acc@1  69.53 ( 67.14)	Acc@5  94.53 ( 91.88)
Epoch: [19][ 80/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.0143e+00 (1.1165e+00)	Acc@1  71.09 ( 67.17)	Acc@5  89.84 ( 91.73)
Epoch: [19][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.2584e+00 (1.1293e+00)	Acc@1  63.28 ( 67.02)	Acc@5  92.19 ( 91.52)
Epoch: [19][100/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.2322e+00 (1.1331e+00)	Acc@1  64.84 ( 66.88)	Acc@5  89.84 ( 91.36)
Epoch: [19][110/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 8.7538e-01 (1.1266e+00)	Acc@1  75.00 ( 67.12)	Acc@5  96.09 ( 91.46)
Epoch: [19][120/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1284e+00 (1.1272e+00)	Acc@1  68.75 ( 67.09)	Acc@5  92.97 ( 91.52)
Epoch: [19][130/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.4845e+00 (1.1337e+00)	Acc@1  57.81 ( 66.91)	Acc@5  85.94 ( 91.44)
Epoch: [19][140/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1659e+00 (1.1378e+00)	Acc@1  65.62 ( 66.71)	Acc@5  91.41 ( 91.49)
Epoch: [19][150/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1253e+00 (1.1399e+00)	Acc@1  69.53 ( 66.72)	Acc@5  92.97 ( 91.48)
Epoch: [19][160/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1305e+00 (1.1405e+00)	Acc@1  63.28 ( 66.60)	Acc@5  92.97 ( 91.57)
Epoch: [19][170/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2434e+00 (1.1438e+00)	Acc@1  68.75 ( 66.52)	Acc@5  88.28 ( 91.55)
Epoch: [19][180/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2182e+00 (1.1464e+00)	Acc@1  65.62 ( 66.41)	Acc@5  92.97 ( 91.52)
Epoch: [19][190/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0354e+00 (1.1465e+00)	Acc@1  68.75 ( 66.35)	Acc@5  90.62 ( 91.54)
Epoch: [19][200/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0864e+00 (1.1473e+00)	Acc@1  68.75 ( 66.35)	Acc@5  92.19 ( 91.51)
Epoch: [19][210/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3142e+00 (1.1500e+00)	Acc@1  53.91 ( 66.25)	Acc@5  90.62 ( 91.47)
Epoch: [19][220/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1287e+00 (1.1497e+00)	Acc@1  64.84 ( 66.24)	Acc@5  89.06 ( 91.47)
Epoch: [19][230/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1105e+00 (1.1509e+00)	Acc@1  67.97 ( 66.32)	Acc@5  92.19 ( 91.47)
Epoch: [19][240/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0930e+00 (1.1540e+00)	Acc@1  67.19 ( 66.24)	Acc@5  95.31 ( 91.43)
Epoch: [19][250/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3792e+00 (1.1551e+00)	Acc@1  61.72 ( 66.30)	Acc@5  88.28 ( 91.39)
Epoch: [19][260/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0892e+00 (1.1574e+00)	Acc@1  68.75 ( 66.25)	Acc@5  92.19 ( 91.32)
Epoch: [19][270/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3449e+00 (1.1572e+00)	Acc@1  66.41 ( 66.27)	Acc@5  89.06 ( 91.32)
Epoch: [19][280/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0904e+00 (1.1601e+00)	Acc@1  71.09 ( 66.19)	Acc@5  89.84 ( 91.26)
Epoch: [19][290/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1859e+00 (1.1605e+00)	Acc@1  65.62 ( 66.18)	Acc@5  92.97 ( 91.26)
Epoch: [19][300/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2291e+00 (1.1635e+00)	Acc@1  67.97 ( 66.12)	Acc@5  90.62 ( 91.21)
Epoch: [19][310/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2238e+00 (1.1652e+00)	Acc@1  66.41 ( 66.09)	Acc@5  91.41 ( 91.17)
Epoch: [19][320/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2433e+00 (1.1680e+00)	Acc@1  64.06 ( 66.06)	Acc@5  91.41 ( 91.12)
Epoch: [19][330/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0518e+00 (1.1701e+00)	Acc@1  73.44 ( 66.08)	Acc@5  93.75 ( 91.08)
Epoch: [19][340/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0330e+00 (1.1707e+00)	Acc@1  72.66 ( 66.05)	Acc@5  91.41 ( 91.09)
Epoch: [19][350/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3762e+00 (1.1713e+00)	Acc@1  62.50 ( 66.05)	Acc@5  92.97 ( 91.09)
Epoch: [19][360/391]	Time  0.246 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1819e+00 (1.1698e+00)	Acc@1  64.84 ( 66.10)	Acc@5  90.62 ( 91.12)
Epoch: [19][370/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1056e+00 (1.1700e+00)	Acc@1  65.62 ( 66.11)	Acc@5  92.97 ( 91.12)
Epoch: [19][380/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0734e+00 (1.1693e+00)	Acc@1  72.66 ( 66.15)	Acc@5  91.41 ( 91.12)
Epoch: [19][390/391]	Time  0.178 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3583e+00 (1.1702e+00)	Acc@1  66.25 ( 66.11)	Acc@5  86.25 ( 91.12)
## e[19] optimizer.zero_grad (sum) time: 0.49270176887512207
## e[19]       loss.backward (sum) time: 11.05512285232544
## e[19]      optimizer.step (sum) time: 47.8768048286438
## epoch[19] training(only) time: 94.67607712745667
# Switched to evaluate mode...
Test: [  0/100]	Time  0.237 ( 0.237)	Loss 1.4736e+00 (1.4736e+00)	Acc@1  62.00 ( 62.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.079 ( 0.092)	Loss 1.4549e+00 (1.5708e+00)	Acc@1  64.00 ( 59.55)	Acc@5  89.00 ( 85.45)
Test: [ 20/100]	Time  0.078 ( 0.085)	Loss 1.1649e+00 (1.5241e+00)	Acc@1  69.00 ( 60.19)	Acc@5  86.00 ( 85.81)
Test: [ 30/100]	Time  0.079 ( 0.083)	Loss 1.8434e+00 (1.5513e+00)	Acc@1  54.00 ( 59.97)	Acc@5  86.00 ( 85.42)
Test: [ 40/100]	Time  0.077 ( 0.082)	Loss 1.5530e+00 (1.5533e+00)	Acc@1  65.00 ( 59.90)	Acc@5  86.00 ( 85.59)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 1.4199e+00 (1.5688e+00)	Acc@1  65.00 ( 59.59)	Acc@5  89.00 ( 85.33)
Test: [ 60/100]	Time  0.077 ( 0.081)	Loss 1.6403e+00 (1.5632e+00)	Acc@1  56.00 ( 59.41)	Acc@5  83.00 ( 85.51)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 1.6741e+00 (1.5604e+00)	Acc@1  57.00 ( 59.23)	Acc@5  84.00 ( 85.75)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 1.6783e+00 (1.5671e+00)	Acc@1  60.00 ( 59.19)	Acc@5  83.00 ( 85.47)
Test: [ 90/100]	Time  0.078 ( 0.080)	Loss 1.5804e+00 (1.5574e+00)	Acc@1  59.00 ( 59.44)	Acc@5  82.00 ( 85.48)
 * Acc@1 59.550 Acc@5 85.510
### epoch[19] execution time: 102.73132658004761
EPOCH 20
# current learning rate: 0.1
# Switched to train mode...
Epoch: [20][  0/391]	Time  0.422 ( 0.422)	Data  0.180 ( 0.180)	Loss 1.0697e+00 (1.0697e+00)	Acc@1  71.88 ( 71.88)	Acc@5  89.06 ( 89.06)
Epoch: [20][ 10/391]	Time  0.241 ( 0.258)	Data  0.001 ( 0.017)	Loss 1.1483e+00 (9.5739e-01)	Acc@1  66.41 ( 71.95)	Acc@5  93.75 ( 94.18)
Epoch: [20][ 20/391]	Time  0.241 ( 0.250)	Data  0.001 ( 0.010)	Loss 9.3103e-01 (9.9643e-01)	Acc@1  74.22 ( 70.65)	Acc@5  94.53 ( 93.71)
Epoch: [20][ 30/391]	Time  0.241 ( 0.247)	Data  0.001 ( 0.007)	Loss 1.2053e+00 (1.0087e+00)	Acc@1  66.41 ( 70.46)	Acc@5  89.84 ( 93.83)
Epoch: [20][ 40/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.006)	Loss 9.8455e-01 (1.0312e+00)	Acc@1  73.44 ( 69.80)	Acc@5  92.97 ( 93.43)
Epoch: [20][ 50/391]	Time  0.242 ( 0.245)	Data  0.001 ( 0.005)	Loss 8.9554e-01 (1.0309e+00)	Acc@1  73.44 ( 69.55)	Acc@5  96.09 ( 93.44)
Epoch: [20][ 60/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.004)	Loss 1.0309e+00 (1.0415e+00)	Acc@1  67.19 ( 69.07)	Acc@5  93.75 ( 93.22)
Epoch: [20][ 70/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.004)	Loss 1.1376e+00 (1.0455e+00)	Acc@1  63.28 ( 69.16)	Acc@5  92.97 ( 93.20)
Epoch: [20][ 80/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.003)	Loss 1.0076e+00 (1.0505e+00)	Acc@1  65.62 ( 69.01)	Acc@5  95.31 ( 93.09)
Epoch: [20][ 90/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 9.8735e-01 (1.0558e+00)	Acc@1  70.31 ( 68.93)	Acc@5  93.75 ( 92.90)
Epoch: [20][100/391]	Time  0.243 ( 0.244)	Data  0.001 ( 0.003)	Loss 1.1949e+00 (1.0579e+00)	Acc@1  64.84 ( 68.90)	Acc@5  91.41 ( 92.82)
Epoch: [20][110/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.2514e+00 (1.0634e+00)	Acc@1  60.94 ( 68.69)	Acc@5  91.41 ( 92.76)
Epoch: [20][120/391]	Time  0.239 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.1420e+00 (1.0674e+00)	Acc@1  71.09 ( 68.52)	Acc@5  91.41 ( 92.69)
Epoch: [20][130/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.2234e+00 (1.0751e+00)	Acc@1  62.50 ( 68.31)	Acc@5  92.19 ( 92.60)
Epoch: [20][140/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1711e+00 (1.0780e+00)	Acc@1  67.19 ( 68.23)	Acc@5  92.19 ( 92.54)
Epoch: [20][150/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1784e+00 (1.0792e+00)	Acc@1  66.41 ( 68.19)	Acc@5  92.19 ( 92.49)
Epoch: [20][160/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.0331e+00 (1.0796e+00)	Acc@1  65.62 ( 68.11)	Acc@5  93.75 ( 92.47)
Epoch: [20][170/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1602e+00 (1.0813e+00)	Acc@1  68.75 ( 68.14)	Acc@5  92.19 ( 92.42)
Epoch: [20][180/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.0398e+00 (1.0843e+00)	Acc@1  66.41 ( 68.09)	Acc@5  90.62 ( 92.36)
Epoch: [20][190/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.3103e+00 (1.0875e+00)	Acc@1  60.94 ( 68.01)	Acc@5  85.16 ( 92.29)
Epoch: [20][200/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.0268e+00 (1.0920e+00)	Acc@1  70.31 ( 67.92)	Acc@5  89.06 ( 92.22)
Epoch: [20][210/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1371e+00 (1.0940e+00)	Acc@1  66.41 ( 67.88)	Acc@5  92.97 ( 92.19)
Epoch: [20][220/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.0710e+00 (1.0937e+00)	Acc@1  71.09 ( 67.85)	Acc@5  92.19 ( 92.18)
Epoch: [20][230/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1326e+00 (1.0972e+00)	Acc@1  60.94 ( 67.74)	Acc@5  92.97 ( 92.14)
Epoch: [20][240/391]	Time  0.251 ( 0.243)	Data  0.001 ( 0.002)	Loss 9.7650e-01 (1.0986e+00)	Acc@1  70.31 ( 67.67)	Acc@5  91.41 ( 92.12)
Epoch: [20][250/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.2157e+00 (1.0984e+00)	Acc@1  64.84 ( 67.70)	Acc@5  92.19 ( 92.12)
Epoch: [20][260/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.0086e+00 (1.1004e+00)	Acc@1  67.97 ( 67.64)	Acc@5  92.19 ( 92.10)
Epoch: [20][270/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.3374e+00 (1.1018e+00)	Acc@1  63.28 ( 67.63)	Acc@5  89.06 ( 92.07)
Epoch: [20][280/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.4192e+00 (1.1038e+00)	Acc@1  53.12 ( 67.51)	Acc@5  92.19 ( 92.10)
Epoch: [20][290/391]	Time  0.246 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1871e+00 (1.1055e+00)	Acc@1  68.75 ( 67.48)	Acc@5  89.06 ( 92.05)
Epoch: [20][300/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.2127e+00 (1.1049e+00)	Acc@1  64.06 ( 67.51)	Acc@5  88.28 ( 92.04)
Epoch: [20][310/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.3743e+00 (1.1068e+00)	Acc@1  60.16 ( 67.47)	Acc@5  87.50 ( 92.00)
Epoch: [20][320/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1230e+00 (1.1072e+00)	Acc@1  71.09 ( 67.46)	Acc@5  92.97 ( 92.00)
Epoch: [20][330/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 9.2775e-01 (1.1095e+00)	Acc@1  73.44 ( 67.45)	Acc@5  93.75 ( 91.95)
Epoch: [20][340/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1862e+00 (1.1091e+00)	Acc@1  62.50 ( 67.47)	Acc@5  92.97 ( 91.95)
Epoch: [20][350/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.2114e+00 (1.1087e+00)	Acc@1  66.41 ( 67.49)	Acc@5  91.41 ( 91.95)
Epoch: [20][360/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1156e+00 (1.1101e+00)	Acc@1  64.06 ( 67.43)	Acc@5  89.84 ( 91.93)
Epoch: [20][370/391]	Time  0.255 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.2200e+00 (1.1103e+00)	Acc@1  70.31 ( 67.43)	Acc@5  90.62 ( 91.92)
Epoch: [20][380/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.2868e+00 (1.1117e+00)	Acc@1  64.06 ( 67.37)	Acc@5  88.28 ( 91.91)
Epoch: [20][390/391]	Time  0.179 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2065e+00 (1.1126e+00)	Acc@1  63.75 ( 67.35)	Acc@5  91.25 ( 91.91)
## e[20] optimizer.zero_grad (sum) time: 0.49286985397338867
## e[20]       loss.backward (sum) time: 11.065600872039795
## e[20]      optimizer.step (sum) time: 47.88775277137756
## epoch[20] training(only) time: 94.85153698921204
# Switched to evaluate mode...
Test: [  0/100]	Time  0.207 ( 0.207)	Loss 1.4260e+00 (1.4260e+00)	Acc@1  60.00 ( 60.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.4403e+00 (1.4974e+00)	Acc@1  58.00 ( 60.82)	Acc@5  87.00 ( 85.73)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 1.1470e+00 (1.4551e+00)	Acc@1  69.00 ( 60.48)	Acc@5  94.00 ( 86.90)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 1.6079e+00 (1.4966e+00)	Acc@1  58.00 ( 60.00)	Acc@5  87.00 ( 86.42)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.3303e+00 (1.4919e+00)	Acc@1  64.00 ( 59.93)	Acc@5  90.00 ( 86.90)
Test: [ 50/100]	Time  0.079 ( 0.081)	Loss 1.4703e+00 (1.4980e+00)	Acc@1  63.00 ( 60.04)	Acc@5  82.00 ( 86.43)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.5941e+00 (1.4971e+00)	Acc@1  61.00 ( 59.69)	Acc@5  84.00 ( 86.43)
Test: [ 70/100]	Time  0.079 ( 0.080)	Loss 1.6923e+00 (1.5162e+00)	Acc@1  58.00 ( 59.38)	Acc@5  83.00 ( 86.20)
Test: [ 80/100]	Time  0.077 ( 0.080)	Loss 1.5522e+00 (1.5231e+00)	Acc@1  61.00 ( 59.19)	Acc@5  81.00 ( 86.10)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.6316e+00 (1.5068e+00)	Acc@1  56.00 ( 59.55)	Acc@5  87.00 ( 86.33)
 * Acc@1 59.670 Acc@5 86.410
### epoch[20] execution time: 102.87545323371887
EPOCH 21
# current learning rate: 0.1
# Switched to train mode...
Epoch: [21][  0/391]	Time  0.397 ( 0.397)	Data  0.155 ( 0.155)	Loss 9.3287e-01 (9.3287e-01)	Acc@1  74.22 ( 74.22)	Acc@5  94.53 ( 94.53)
Epoch: [21][ 10/391]	Time  0.241 ( 0.255)	Data  0.001 ( 0.015)	Loss 1.0100e+00 (1.0096e+00)	Acc@1  68.75 ( 69.96)	Acc@5  94.53 ( 93.61)
Epoch: [21][ 20/391]	Time  0.241 ( 0.249)	Data  0.001 ( 0.008)	Loss 9.1089e-01 (1.0070e+00)	Acc@1  74.22 ( 70.50)	Acc@5  96.88 ( 93.60)
Epoch: [21][ 30/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.006)	Loss 9.9906e-01 (1.0060e+00)	Acc@1  73.44 ( 70.59)	Acc@5  92.19 ( 93.32)
Epoch: [21][ 40/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.005)	Loss 8.6204e-01 (9.9493e-01)	Acc@1  77.34 ( 70.79)	Acc@5  94.53 ( 93.39)
Epoch: [21][ 50/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.0802e+00 (9.9099e-01)	Acc@1  67.19 ( 70.71)	Acc@5  92.97 ( 93.43)
Epoch: [21][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 9.1706e-01 (9.9845e-01)	Acc@1  71.09 ( 70.56)	Acc@5  92.19 ( 93.29)
Epoch: [21][ 70/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.1152e+00 (1.0010e+00)	Acc@1  65.62 ( 70.50)	Acc@5  92.19 ( 93.36)
Epoch: [21][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 9.7016e-01 (9.9371e-01)	Acc@1  71.88 ( 70.73)	Acc@5  95.31 ( 93.45)
Epoch: [21][ 90/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.2914e+00 (9.9787e-01)	Acc@1  64.84 ( 70.54)	Acc@5  89.84 ( 93.37)
Epoch: [21][100/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.1619e+00 (1.0027e+00)	Acc@1  60.16 ( 70.33)	Acc@5  93.75 ( 93.32)
Epoch: [21][110/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 7.6981e-01 (1.0023e+00)	Acc@1  75.78 ( 70.40)	Acc@5  96.88 ( 93.36)
Epoch: [21][120/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.4488e+00 (1.0072e+00)	Acc@1  57.81 ( 70.19)	Acc@5  89.06 ( 93.27)
Epoch: [21][130/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0823e+00 (1.0050e+00)	Acc@1  67.19 ( 70.26)	Acc@5  92.97 ( 93.30)
Epoch: [21][140/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1111e+00 (1.0029e+00)	Acc@1  67.97 ( 70.32)	Acc@5  90.62 ( 93.29)
Epoch: [21][150/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0366e+00 (1.0047e+00)	Acc@1  68.75 ( 70.28)	Acc@5  94.53 ( 93.24)
Epoch: [21][160/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.3859e-01 (1.0046e+00)	Acc@1  68.75 ( 70.23)	Acc@5  96.09 ( 93.28)
Epoch: [21][170/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.1095e-01 (1.0043e+00)	Acc@1  72.66 ( 70.23)	Acc@5  92.19 ( 93.26)
Epoch: [21][180/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0213e+00 (1.0071e+00)	Acc@1  69.53 ( 70.18)	Acc@5  92.19 ( 93.18)
Epoch: [21][190/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1889e+00 (1.0121e+00)	Acc@1  63.28 ( 70.09)	Acc@5  91.41 ( 93.16)
Epoch: [21][200/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.8847e-01 (1.0149e+00)	Acc@1  69.53 ( 69.99)	Acc@5  92.97 ( 93.09)
Epoch: [21][210/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.5427e-01 (1.0176e+00)	Acc@1  81.25 ( 70.03)	Acc@5  94.53 ( 93.01)
Epoch: [21][220/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0897e+00 (1.0201e+00)	Acc@1  71.09 ( 69.97)	Acc@5  91.41 ( 92.99)
Epoch: [21][230/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.5716e-01 (1.0216e+00)	Acc@1  70.31 ( 69.96)	Acc@5  94.53 ( 93.02)
Epoch: [21][240/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.2402e-01 (1.0213e+00)	Acc@1  74.22 ( 69.98)	Acc@5  93.75 ( 93.02)
Epoch: [21][250/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.8415e-01 (1.0226e+00)	Acc@1  67.97 ( 69.95)	Acc@5  94.53 ( 92.98)
Epoch: [21][260/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0055e+00 (1.0243e+00)	Acc@1  75.00 ( 69.93)	Acc@5  91.41 ( 92.97)
Epoch: [21][270/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3217e+00 (1.0295e+00)	Acc@1  64.06 ( 69.80)	Acc@5  89.06 ( 92.91)
Epoch: [21][280/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1282e+00 (1.0342e+00)	Acc@1  67.19 ( 69.67)	Acc@5  89.84 ( 92.84)
Epoch: [21][290/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.4302e-01 (1.0380e+00)	Acc@1  69.53 ( 69.60)	Acc@5  94.53 ( 92.82)
Epoch: [21][300/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0746e+00 (1.0409e+00)	Acc@1  67.97 ( 69.47)	Acc@5  90.62 ( 92.78)
Epoch: [21][310/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0477e+00 (1.0444e+00)	Acc@1  64.84 ( 69.34)	Acc@5  95.31 ( 92.69)
Epoch: [21][320/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2369e+00 (1.0476e+00)	Acc@1  62.50 ( 69.22)	Acc@5  92.97 ( 92.68)
Epoch: [21][330/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2328e+00 (1.0511e+00)	Acc@1  64.06 ( 69.17)	Acc@5  90.62 ( 92.61)
Epoch: [21][340/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1296e+00 (1.0518e+00)	Acc@1  67.19 ( 69.16)	Acc@5  92.19 ( 92.61)
Epoch: [21][350/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1781e+00 (1.0526e+00)	Acc@1  65.62 ( 69.14)	Acc@5  93.75 ( 92.59)
Epoch: [21][360/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0817e+00 (1.0532e+00)	Acc@1  70.31 ( 69.13)	Acc@5  89.06 ( 92.56)
Epoch: [21][370/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.5215e-01 (1.0543e+00)	Acc@1  71.88 ( 69.12)	Acc@5  95.31 ( 92.54)
Epoch: [21][380/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0833e+00 (1.0550e+00)	Acc@1  68.75 ( 69.09)	Acc@5  92.19 ( 92.53)
Epoch: [21][390/391]	Time  0.178 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1952e+00 (1.0563e+00)	Acc@1  68.75 ( 69.09)	Acc@5  91.25 ( 92.51)
## e[21] optimizer.zero_grad (sum) time: 0.4898998737335205
## e[21]       loss.backward (sum) time: 11.12168025970459
## e[21]      optimizer.step (sum) time: 47.831737995147705
## epoch[21] training(only) time: 94.71281504631042
# Switched to evaluate mode...
Test: [  0/100]	Time  0.231 ( 0.231)	Loss 1.4390e+00 (1.4390e+00)	Acc@1  62.00 ( 62.00)	Acc@5  85.00 ( 85.00)
Test: [ 10/100]	Time  0.078 ( 0.092)	Loss 1.6686e+00 (1.5505e+00)	Acc@1  56.00 ( 59.82)	Acc@5  86.00 ( 85.64)
Test: [ 20/100]	Time  0.078 ( 0.085)	Loss 1.4364e+00 (1.4977e+00)	Acc@1  64.00 ( 60.71)	Acc@5  90.00 ( 86.29)
Test: [ 30/100]	Time  0.078 ( 0.083)	Loss 1.5789e+00 (1.5228e+00)	Acc@1  58.00 ( 60.26)	Acc@5  87.00 ( 86.00)
Test: [ 40/100]	Time  0.079 ( 0.082)	Loss 1.6086e+00 (1.5207e+00)	Acc@1  52.00 ( 60.20)	Acc@5  88.00 ( 86.10)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 1.4737e+00 (1.5370e+00)	Acc@1  60.00 ( 59.75)	Acc@5  91.00 ( 85.90)
Test: [ 60/100]	Time  0.079 ( 0.080)	Loss 1.5729e+00 (1.5247e+00)	Acc@1  64.00 ( 59.70)	Acc@5  84.00 ( 86.26)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 1.3714e+00 (1.5282e+00)	Acc@1  60.00 ( 59.56)	Acc@5  90.00 ( 86.27)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 1.4105e+00 (1.5243e+00)	Acc@1  62.00 ( 59.85)	Acc@5  88.00 ( 86.20)
Test: [ 90/100]	Time  0.078 ( 0.080)	Loss 1.8183e+00 (1.5143e+00)	Acc@1  52.00 ( 60.22)	Acc@5  82.00 ( 86.40)
 * Acc@1 60.540 Acc@5 86.480
### epoch[21] execution time: 102.77431130409241
EPOCH 22
# current learning rate: 0.1
# Switched to train mode...
Epoch: [22][  0/391]	Time  0.385 ( 0.385)	Data  0.143 ( 0.143)	Loss 9.1563e-01 (9.1563e-01)	Acc@1  73.44 ( 73.44)	Acc@5  96.09 ( 96.09)
Epoch: [22][ 10/391]	Time  0.241 ( 0.254)	Data  0.001 ( 0.014)	Loss 8.5865e-01 (9.1409e-01)	Acc@1  75.78 ( 73.37)	Acc@5  94.53 ( 93.68)
Epoch: [22][ 20/391]	Time  0.241 ( 0.248)	Data  0.001 ( 0.008)	Loss 9.5426e-01 (9.2633e-01)	Acc@1  71.88 ( 72.17)	Acc@5  95.31 ( 93.68)
Epoch: [22][ 30/391]	Time  0.246 ( 0.246)	Data  0.001 ( 0.006)	Loss 1.0983e+00 (9.1788e-01)	Acc@1  68.75 ( 72.83)	Acc@5  92.19 ( 93.98)
Epoch: [22][ 40/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.005)	Loss 1.0653e+00 (9.1549e-01)	Acc@1  67.97 ( 72.85)	Acc@5  89.84 ( 94.09)
Epoch: [22][ 50/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 8.5308e-01 (9.1256e-01)	Acc@1  75.00 ( 72.69)	Acc@5  92.19 ( 94.16)
Epoch: [22][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 9.8972e-01 (9.1391e-01)	Acc@1  71.88 ( 72.75)	Acc@5  95.31 ( 94.19)
Epoch: [22][ 70/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.0216e+00 (9.1131e-01)	Acc@1  70.31 ( 72.82)	Acc@5  92.19 ( 94.25)
Epoch: [22][ 80/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.0875e+00 (9.1230e-01)	Acc@1  69.53 ( 72.89)	Acc@5  91.41 ( 94.18)
Epoch: [22][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.0041e+00 (9.0590e-01)	Acc@1  71.88 ( 73.13)	Acc@5  92.19 ( 94.29)
Epoch: [22][100/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 7.9352e-01 (9.0863e-01)	Acc@1  75.00 ( 72.88)	Acc@5  96.88 ( 94.30)
Epoch: [22][110/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.0723e+00 (9.1683e-01)	Acc@1  70.31 ( 72.72)	Acc@5  92.97 ( 94.24)
Epoch: [22][120/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 9.9353e-01 (9.2214e-01)	Acc@1  69.53 ( 72.52)	Acc@5  96.88 ( 94.25)
Epoch: [22][130/391]	Time  0.250 ( 0.243)	Data  0.001 ( 0.002)	Loss 9.1037e-01 (9.2929e-01)	Acc@1  75.78 ( 72.41)	Acc@5  95.31 ( 94.13)
Epoch: [22][140/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 9.5136e-01 (9.4081e-01)	Acc@1  69.53 ( 72.03)	Acc@5  95.31 ( 94.00)
Epoch: [22][150/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.0504e+00 (9.4641e-01)	Acc@1  69.53 ( 71.85)	Acc@5  91.41 ( 93.90)
Epoch: [22][160/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0098e+00 (9.5365e-01)	Acc@1  71.88 ( 71.64)	Acc@5  93.75 ( 93.79)
Epoch: [22][170/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1106e+00 (9.5963e-01)	Acc@1  68.75 ( 71.53)	Acc@5  92.97 ( 93.74)
Epoch: [22][180/391]	Time  0.248 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.9377e-01 (9.6262e-01)	Acc@1  67.97 ( 71.44)	Acc@5  96.88 ( 93.75)
Epoch: [22][190/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1590e+00 (9.6694e-01)	Acc@1  68.75 ( 71.27)	Acc@5  92.19 ( 93.68)
Epoch: [22][200/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.5842e-01 (9.7316e-01)	Acc@1  81.25 ( 71.11)	Acc@5  92.19 ( 93.58)
Epoch: [22][210/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1232e+00 (9.8020e-01)	Acc@1  64.06 ( 70.91)	Acc@5  91.41 ( 93.48)
Epoch: [22][220/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1117e+00 (9.8364e-01)	Acc@1  60.94 ( 70.75)	Acc@5  92.19 ( 93.45)
Epoch: [22][230/391]	Time  0.248 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0167e+00 (9.8712e-01)	Acc@1  68.75 ( 70.68)	Acc@5  96.09 ( 93.41)
Epoch: [22][240/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.4539e-01 (9.8930e-01)	Acc@1  72.66 ( 70.64)	Acc@5  92.19 ( 93.37)
Epoch: [22][250/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0480e+00 (9.8928e-01)	Acc@1  71.88 ( 70.65)	Acc@5  92.19 ( 93.37)
Epoch: [22][260/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0032e+00 (9.9111e-01)	Acc@1  68.75 ( 70.59)	Acc@5  92.97 ( 93.37)
Epoch: [22][270/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0928e+00 (9.9283e-01)	Acc@1  71.09 ( 70.62)	Acc@5  90.62 ( 93.31)
Epoch: [22][280/391]	Time  0.246 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2186e+00 (9.9599e-01)	Acc@1  62.50 ( 70.55)	Acc@5  89.06 ( 93.23)
Epoch: [22][290/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2051e+00 (9.9764e-01)	Acc@1  69.53 ( 70.50)	Acc@5  89.06 ( 93.25)
Epoch: [22][300/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.7697e-01 (9.9897e-01)	Acc@1  71.09 ( 70.45)	Acc@5  93.75 ( 93.25)
Epoch: [22][310/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.8122e-01 (1.0004e+00)	Acc@1  71.09 ( 70.42)	Acc@5  94.53 ( 93.22)
Epoch: [22][320/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1411e+00 (1.0024e+00)	Acc@1  67.19 ( 70.34)	Acc@5  90.62 ( 93.23)
Epoch: [22][330/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.9520e-01 (1.0016e+00)	Acc@1  75.78 ( 70.39)	Acc@5  93.75 ( 93.24)
Epoch: [22][340/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.7653e-01 (1.0010e+00)	Acc@1  70.31 ( 70.41)	Acc@5  94.53 ( 93.24)
Epoch: [22][350/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1643e+00 (1.0034e+00)	Acc@1  63.28 ( 70.33)	Acc@5  92.19 ( 93.23)
Epoch: [22][360/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.9169e-01 (1.0057e+00)	Acc@1  74.22 ( 70.25)	Acc@5  94.53 ( 93.21)
Epoch: [22][370/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.5035e-01 (1.0076e+00)	Acc@1  72.66 ( 70.20)	Acc@5  96.88 ( 93.20)
Epoch: [22][380/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0518e+00 (1.0082e+00)	Acc@1  70.31 ( 70.18)	Acc@5  92.19 ( 93.17)
Epoch: [22][390/391]	Time  0.180 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.6128e-01 (1.0085e+00)	Acc@1  75.00 ( 70.17)	Acc@5  97.50 ( 93.18)
## e[22] optimizer.zero_grad (sum) time: 0.49587464332580566
## e[22]       loss.backward (sum) time: 11.078601837158203
## e[22]      optimizer.step (sum) time: 47.890209913253784
## epoch[22] training(only) time: 94.74553227424622
# Switched to evaluate mode...
Test: [  0/100]	Time  0.209 ( 0.209)	Loss 1.5253e+00 (1.5253e+00)	Acc@1  55.00 ( 55.00)	Acc@5  84.00 ( 84.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.5741e+00 (1.5158e+00)	Acc@1  54.00 ( 60.18)	Acc@5  91.00 ( 86.45)
Test: [ 20/100]	Time  0.077 ( 0.084)	Loss 1.5161e+00 (1.4985e+00)	Acc@1  62.00 ( 61.14)	Acc@5  87.00 ( 86.33)
Test: [ 30/100]	Time  0.077 ( 0.082)	Loss 1.5778e+00 (1.5515e+00)	Acc@1  57.00 ( 59.61)	Acc@5  89.00 ( 85.74)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.3936e+00 (1.5482e+00)	Acc@1  61.00 ( 59.44)	Acc@5  87.00 ( 85.93)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 1.5092e+00 (1.5721e+00)	Acc@1  62.00 ( 59.43)	Acc@5  84.00 ( 85.61)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.6652e+00 (1.5611e+00)	Acc@1  59.00 ( 59.46)	Acc@5  85.00 ( 85.67)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 1.6146e+00 (1.5607e+00)	Acc@1  58.00 ( 59.37)	Acc@5  86.00 ( 85.73)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 1.7816e+00 (1.5567e+00)	Acc@1  58.00 ( 59.60)	Acc@5  86.00 ( 85.83)
Test: [ 90/100]	Time  0.079 ( 0.080)	Loss 1.5583e+00 (1.5385e+00)	Acc@1  59.00 ( 59.93)	Acc@5  87.00 ( 86.09)
 * Acc@1 60.070 Acc@5 86.150
### epoch[22] execution time: 102.77310419082642
EPOCH 23
# current learning rate: 0.1
# Switched to train mode...
Epoch: [23][  0/391]	Time  0.394 ( 0.394)	Data  0.152 ( 0.152)	Loss 8.9324e-01 (8.9324e-01)	Acc@1  74.22 ( 74.22)	Acc@5  96.09 ( 96.09)
Epoch: [23][ 10/391]	Time  0.241 ( 0.255)	Data  0.001 ( 0.015)	Loss 7.9563e-01 (9.2306e-01)	Acc@1  78.12 ( 71.16)	Acc@5  95.31 ( 95.10)
Epoch: [23][ 20/391]	Time  0.242 ( 0.249)	Data  0.001 ( 0.008)	Loss 9.0480e-01 (9.4051e-01)	Acc@1  70.31 ( 70.83)	Acc@5  93.75 ( 94.68)
Epoch: [23][ 30/391]	Time  0.241 ( 0.247)	Data  0.001 ( 0.006)	Loss 8.5434e-01 (9.3226e-01)	Acc@1  77.34 ( 71.50)	Acc@5  92.97 ( 94.33)
Epoch: [23][ 40/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.005)	Loss 9.2352e-01 (9.2059e-01)	Acc@1  70.31 ( 71.97)	Acc@5  94.53 ( 94.44)
Epoch: [23][ 50/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.004)	Loss 6.3470e-01 (9.0845e-01)	Acc@1  81.25 ( 72.67)	Acc@5  97.66 ( 94.62)
Epoch: [23][ 60/391]	Time  0.242 ( 0.245)	Data  0.001 ( 0.004)	Loss 1.0891e+00 (9.0915e-01)	Acc@1  71.88 ( 72.82)	Acc@5  91.41 ( 94.52)
Epoch: [23][ 70/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 8.4183e-01 (9.1107e-01)	Acc@1  76.56 ( 72.81)	Acc@5  95.31 ( 94.34)
Epoch: [23][ 80/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 9.0064e-01 (9.0092e-01)	Acc@1  75.00 ( 73.17)	Acc@5  96.09 ( 94.44)
Epoch: [23][ 90/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 1.0817e+00 (9.0538e-01)	Acc@1  67.97 ( 73.08)	Acc@5  88.28 ( 94.37)
Epoch: [23][100/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 7.5727e-01 (9.0412e-01)	Acc@1  73.44 ( 73.13)	Acc@5  95.31 ( 94.34)
Epoch: [23][110/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 8.2773e-01 (9.0217e-01)	Acc@1  74.22 ( 73.11)	Acc@5  96.88 ( 94.43)
Epoch: [23][120/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.0570e+00 (9.0015e-01)	Acc@1  69.53 ( 73.17)	Acc@5  91.41 ( 94.44)
Epoch: [23][130/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.0974e+00 (9.0468e-01)	Acc@1  66.41 ( 73.00)	Acc@5  91.41 ( 94.41)
Epoch: [23][140/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 9.1020e-01 (9.0857e-01)	Acc@1  71.88 ( 72.92)	Acc@5  94.53 ( 94.38)
Epoch: [23][150/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 8.1588e-01 (9.1101e-01)	Acc@1  71.09 ( 72.76)	Acc@5  96.88 ( 94.38)
Epoch: [23][160/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 8.3318e-01 (9.1246e-01)	Acc@1  79.69 ( 72.81)	Acc@5  96.88 ( 94.33)
Epoch: [23][170/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.0663e+00 (9.1800e-01)	Acc@1  67.97 ( 72.69)	Acc@5  91.41 ( 94.24)
Epoch: [23][180/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 8.8535e-01 (9.1910e-01)	Acc@1  75.00 ( 72.62)	Acc@5  93.75 ( 94.22)
Epoch: [23][190/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.002)	Loss 8.7868e-01 (9.2111e-01)	Acc@1  78.12 ( 72.61)	Acc@5  90.62 ( 94.15)
Epoch: [23][200/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 9.9141e-01 (9.2266e-01)	Acc@1  68.75 ( 72.57)	Acc@5  92.97 ( 94.13)
Epoch: [23][210/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 8.7246e-01 (9.2418e-01)	Acc@1  76.56 ( 72.54)	Acc@5  92.97 ( 94.12)
Epoch: [23][220/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1012e+00 (9.2715e-01)	Acc@1  65.62 ( 72.43)	Acc@5  90.62 ( 94.09)
Epoch: [23][230/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.0458e+00 (9.3005e-01)	Acc@1  65.62 ( 72.34)	Acc@5  95.31 ( 94.06)
Epoch: [23][240/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 8.7535e-01 (9.3267e-01)	Acc@1  72.66 ( 72.26)	Acc@5  93.75 ( 94.03)
Epoch: [23][250/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.0849e+00 (9.3588e-01)	Acc@1  70.31 ( 72.20)	Acc@5  92.97 ( 94.00)
Epoch: [23][260/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 7.8150e-01 (9.3854e-01)	Acc@1  76.56 ( 72.11)	Acc@5  93.75 ( 93.97)
Epoch: [23][270/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 9.3318e-01 (9.4081e-01)	Acc@1  70.31 ( 72.09)	Acc@5 100.00 ( 93.97)
Epoch: [23][280/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.6716e-01 (9.4210e-01)	Acc@1  70.31 ( 72.03)	Acc@5  92.97 ( 93.95)
Epoch: [23][290/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.3907e-01 (9.4205e-01)	Acc@1  74.22 ( 72.05)	Acc@5  95.31 ( 93.94)
Epoch: [23][300/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.2408e-01 (9.4295e-01)	Acc@1  71.88 ( 72.02)	Acc@5  93.75 ( 93.94)
Epoch: [23][310/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.2673e-01 (9.4426e-01)	Acc@1  75.00 ( 72.00)	Acc@5  96.09 ( 93.94)
Epoch: [23][320/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2108e+00 (9.4627e-01)	Acc@1  60.94 ( 71.95)	Acc@5  89.06 ( 93.90)
Epoch: [23][330/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1100e+00 (9.4803e-01)	Acc@1  71.09 ( 71.87)	Acc@5  91.41 ( 93.88)
Epoch: [23][340/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.2074e-01 (9.5044e-01)	Acc@1  74.22 ( 71.82)	Acc@5  91.41 ( 93.84)
Epoch: [23][350/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2486e+00 (9.5154e-01)	Acc@1  64.84 ( 71.79)	Acc@5  90.62 ( 93.80)
Epoch: [23][360/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1168e+00 (9.5287e-01)	Acc@1  66.41 ( 71.73)	Acc@5  91.41 ( 93.82)
Epoch: [23][370/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.2645e-01 (9.5676e-01)	Acc@1  72.66 ( 71.62)	Acc@5  93.75 ( 93.77)
Epoch: [23][380/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.4129e-01 (9.5882e-01)	Acc@1  71.88 ( 71.60)	Acc@5  92.97 ( 93.74)
Epoch: [23][390/391]	Time  0.180 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0799e+00 (9.6058e-01)	Acc@1  66.25 ( 71.54)	Acc@5  95.00 ( 93.73)
## e[23] optimizer.zero_grad (sum) time: 0.4940934181213379
## e[23]       loss.backward (sum) time: 11.092129945755005
## e[23]      optimizer.step (sum) time: 47.88355302810669
## epoch[23] training(only) time: 94.82920360565186
# Switched to evaluate mode...
Test: [  0/100]	Time  0.242 ( 0.242)	Loss 1.3373e+00 (1.3373e+00)	Acc@1  64.00 ( 64.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.079 ( 0.093)	Loss 1.4341e+00 (1.4079e+00)	Acc@1  64.00 ( 63.18)	Acc@5  88.00 ( 87.45)
Test: [ 20/100]	Time  0.078 ( 0.086)	Loss 1.2173e+00 (1.3891e+00)	Acc@1  69.00 ( 63.29)	Acc@5  89.00 ( 87.76)
Test: [ 30/100]	Time  0.079 ( 0.083)	Loss 1.7076e+00 (1.4236e+00)	Acc@1  52.00 ( 62.32)	Acc@5  88.00 ( 87.32)
Test: [ 40/100]	Time  0.077 ( 0.082)	Loss 1.2255e+00 (1.4228e+00)	Acc@1  65.00 ( 62.24)	Acc@5  90.00 ( 87.68)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 1.6188e+00 (1.4454e+00)	Acc@1  63.00 ( 62.04)	Acc@5  87.00 ( 87.25)
Test: [ 60/100]	Time  0.078 ( 0.081)	Loss 1.3879e+00 (1.4292e+00)	Acc@1  63.00 ( 62.31)	Acc@5  86.00 ( 87.43)
Test: [ 70/100]	Time  0.077 ( 0.080)	Loss 1.6395e+00 (1.4220e+00)	Acc@1  61.00 ( 62.46)	Acc@5  84.00 ( 87.61)
Test: [ 80/100]	Time  0.077 ( 0.080)	Loss 1.4030e+00 (1.4193e+00)	Acc@1  61.00 ( 62.54)	Acc@5  86.00 ( 87.56)
Test: [ 90/100]	Time  0.078 ( 0.080)	Loss 1.6394e+00 (1.4113e+00)	Acc@1  56.00 ( 62.58)	Acc@5  82.00 ( 87.60)
 * Acc@1 62.700 Acc@5 87.610
### epoch[23] execution time: 102.88330936431885
EPOCH 24
# current learning rate: 0.1
# Switched to train mode...
Epoch: [24][  0/391]	Time  0.403 ( 0.403)	Data  0.159 ( 0.159)	Loss 1.1381e+00 (1.1381e+00)	Acc@1  66.41 ( 66.41)	Acc@5  89.06 ( 89.06)
Epoch: [24][ 10/391]	Time  0.242 ( 0.257)	Data  0.001 ( 0.015)	Loss 8.1534e-01 (9.0722e-01)	Acc@1  75.00 ( 73.37)	Acc@5  96.09 ( 95.24)
Epoch: [24][ 20/391]	Time  0.248 ( 0.250)	Data  0.001 ( 0.009)	Loss 9.7059e-01 (8.9660e-01)	Acc@1  71.88 ( 72.73)	Acc@5  93.75 ( 94.64)
Epoch: [24][ 30/391]	Time  0.241 ( 0.247)	Data  0.001 ( 0.006)	Loss 6.4882e-01 (8.8546e-01)	Acc@1  80.47 ( 73.34)	Acc@5  95.31 ( 94.76)
Epoch: [24][ 40/391]	Time  0.240 ( 0.246)	Data  0.001 ( 0.005)	Loss 9.1243e-01 (8.6219e-01)	Acc@1  75.00 ( 73.99)	Acc@5  92.97 ( 95.05)
Epoch: [24][ 50/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.004)	Loss 9.5875e-01 (8.5649e-01)	Acc@1  71.09 ( 74.31)	Acc@5  93.75 ( 94.94)
Epoch: [24][ 60/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.004)	Loss 8.4479e-01 (8.6334e-01)	Acc@1  75.78 ( 74.09)	Acc@5  96.09 ( 94.90)
Epoch: [24][ 70/391]	Time  0.244 ( 0.244)	Data  0.001 ( 0.003)	Loss 9.5480e-01 (8.6188e-01)	Acc@1  72.66 ( 74.09)	Acc@5  93.75 ( 94.96)
Epoch: [24][ 80/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 9.4768e-01 (8.6211e-01)	Acc@1  72.66 ( 74.12)	Acc@5  93.75 ( 94.93)
Epoch: [24][ 90/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.003)	Loss 6.6374e-01 (8.5385e-01)	Acc@1  83.59 ( 74.36)	Acc@5  94.53 ( 94.98)
Epoch: [24][100/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.0474e+00 (8.6177e-01)	Acc@1  67.19 ( 74.04)	Acc@5  96.09 ( 94.97)
Epoch: [24][110/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.0112e+00 (8.6285e-01)	Acc@1  73.44 ( 74.04)	Acc@5  95.31 ( 95.03)
Epoch: [24][120/391]	Time  0.246 ( 0.243)	Data  0.001 ( 0.002)	Loss 8.9654e-01 (8.6350e-01)	Acc@1  71.88 ( 74.01)	Acc@5  94.53 ( 94.99)
Epoch: [24][130/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 7.9490e-01 (8.6109e-01)	Acc@1  71.09 ( 74.08)	Acc@5  95.31 ( 95.03)
Epoch: [24][140/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.0373e+00 (8.5914e-01)	Acc@1  68.75 ( 74.19)	Acc@5  92.97 ( 94.99)
Epoch: [24][150/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1482e+00 (8.6504e-01)	Acc@1  67.19 ( 74.03)	Acc@5  89.84 ( 94.92)
Epoch: [24][160/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 7.5906e-01 (8.6749e-01)	Acc@1  78.12 ( 73.97)	Acc@5  95.31 ( 94.89)
Epoch: [24][170/391]	Time  0.248 ( 0.243)	Data  0.001 ( 0.002)	Loss 9.6483e-01 (8.6963e-01)	Acc@1  73.44 ( 73.98)	Acc@5  96.88 ( 94.86)
Epoch: [24][180/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 9.5921e-01 (8.7194e-01)	Acc@1  70.31 ( 73.89)	Acc@5  94.53 ( 94.84)
Epoch: [24][190/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 7.9027e-01 (8.7185e-01)	Acc@1  75.00 ( 73.88)	Acc@5  96.88 ( 94.86)
Epoch: [24][200/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 8.7536e-01 (8.7437e-01)	Acc@1  72.66 ( 73.76)	Acc@5 100.00 ( 94.88)
Epoch: [24][210/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 8.4319e-01 (8.7814e-01)	Acc@1  70.31 ( 73.63)	Acc@5  96.88 ( 94.85)
Epoch: [24][220/391]	Time  0.247 ( 0.243)	Data  0.001 ( 0.002)	Loss 7.1321e-01 (8.8047e-01)	Acc@1  76.56 ( 73.54)	Acc@5  98.44 ( 94.82)
Epoch: [24][230/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.0140e+00 (8.8710e-01)	Acc@1  68.75 ( 73.36)	Acc@5  93.75 ( 94.73)
Epoch: [24][240/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1945e+00 (8.9068e-01)	Acc@1  67.19 ( 73.26)	Acc@5  85.16 ( 94.64)
Epoch: [24][250/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 7.6539e-01 (8.9252e-01)	Acc@1  76.56 ( 73.21)	Acc@5  96.88 ( 94.61)
Epoch: [24][260/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 9.7341e-01 (8.9472e-01)	Acc@1  69.53 ( 73.16)	Acc@5  92.19 ( 94.58)
Epoch: [24][270/391]	Time  0.249 ( 0.243)	Data  0.001 ( 0.002)	Loss 7.1753e-01 (8.9610e-01)	Acc@1  77.34 ( 73.13)	Acc@5  96.09 ( 94.56)
Epoch: [24][280/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 9.9763e-01 (9.0148e-01)	Acc@1  64.06 ( 72.96)	Acc@5  95.31 ( 94.51)
Epoch: [24][290/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 9.2795e-01 (9.0253e-01)	Acc@1  67.19 ( 72.95)	Acc@5  94.53 ( 94.49)
Epoch: [24][300/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 8.8878e-01 (9.0467e-01)	Acc@1  78.12 ( 72.95)	Acc@5  91.41 ( 94.44)
Epoch: [24][310/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 8.7240e-01 (9.0472e-01)	Acc@1  73.44 ( 72.96)	Acc@5  95.31 ( 94.44)
Epoch: [24][320/391]	Time  0.246 ( 0.243)	Data  0.001 ( 0.002)	Loss 9.1246e-01 (9.0564e-01)	Acc@1  70.31 ( 72.89)	Acc@5  96.09 ( 94.43)
Epoch: [24][330/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 9.4961e-01 (9.0563e-01)	Acc@1  71.09 ( 72.90)	Acc@5  91.41 ( 94.43)
Epoch: [24][340/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 9.4170e-01 (9.0554e-01)	Acc@1  70.31 ( 72.88)	Acc@5  94.53 ( 94.42)
Epoch: [24][350/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 8.8102e-01 (9.0663e-01)	Acc@1  72.66 ( 72.85)	Acc@5  93.75 ( 94.41)
Epoch: [24][360/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 7.5153e-01 (9.0942e-01)	Acc@1  79.69 ( 72.79)	Acc@5  96.09 ( 94.38)
Epoch: [24][370/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1401e+00 (9.1034e-01)	Acc@1  67.19 ( 72.81)	Acc@5  93.75 ( 94.40)
Epoch: [24][380/391]	Time  0.251 ( 0.243)	Data  0.001 ( 0.002)	Loss 6.6850e-01 (9.1077e-01)	Acc@1  81.25 ( 72.81)	Acc@5  97.66 ( 94.40)
Epoch: [24][390/391]	Time  0.180 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.7574e-01 (9.1239e-01)	Acc@1  75.00 ( 72.77)	Acc@5  96.25 ( 94.37)
## e[24] optimizer.zero_grad (sum) time: 0.49426698684692383
## e[24]       loss.backward (sum) time: 11.121860027313232
## e[24]      optimizer.step (sum) time: 47.85486626625061
## epoch[24] training(only) time: 94.87600469589233
# Switched to evaluate mode...
Test: [  0/100]	Time  0.205 ( 0.205)	Loss 1.2911e+00 (1.2911e+00)	Acc@1  59.00 ( 59.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.079 ( 0.090)	Loss 1.4976e+00 (1.5011e+00)	Acc@1  58.00 ( 62.09)	Acc@5  88.00 ( 87.55)
Test: [ 20/100]	Time  0.077 ( 0.084)	Loss 1.3219e+00 (1.4395e+00)	Acc@1  65.00 ( 63.00)	Acc@5  90.00 ( 87.76)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 1.5643e+00 (1.4572e+00)	Acc@1  58.00 ( 62.58)	Acc@5  89.00 ( 87.19)
Test: [ 40/100]	Time  0.077 ( 0.081)	Loss 1.4393e+00 (1.4605e+00)	Acc@1  62.00 ( 62.49)	Acc@5  88.00 ( 87.12)
Test: [ 50/100]	Time  0.079 ( 0.081)	Loss 1.7226e+00 (1.4840e+00)	Acc@1  65.00 ( 62.22)	Acc@5  85.00 ( 86.88)
Test: [ 60/100]	Time  0.077 ( 0.080)	Loss 1.6835e+00 (1.4671e+00)	Acc@1  61.00 ( 62.28)	Acc@5  85.00 ( 87.20)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 1.3133e+00 (1.4783e+00)	Acc@1  66.00 ( 62.13)	Acc@5  89.00 ( 87.17)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 1.5293e+00 (1.4708e+00)	Acc@1  56.00 ( 61.98)	Acc@5  84.00 ( 87.22)
Test: [ 90/100]	Time  0.080 ( 0.079)	Loss 1.6932e+00 (1.4593e+00)	Acc@1  62.00 ( 62.26)	Acc@5  88.00 ( 87.42)
 * Acc@1 62.500 Acc@5 87.530
### epoch[24] execution time: 102.89227390289307
EPOCH 25
# current learning rate: 0.1
# Switched to train mode...
Epoch: [25][  0/391]	Time  0.387 ( 0.387)	Data  0.117 ( 0.117)	Loss 7.7892e-01 (7.7892e-01)	Acc@1  78.12 ( 78.12)	Acc@5  94.53 ( 94.53)
Epoch: [25][ 10/391]	Time  0.241 ( 0.254)	Data  0.001 ( 0.012)	Loss 8.6123e-01 (8.1206e-01)	Acc@1  72.66 ( 75.92)	Acc@5  96.09 ( 94.74)
Epoch: [25][ 20/391]	Time  0.242 ( 0.248)	Data  0.001 ( 0.007)	Loss 7.2055e-01 (8.0429e-01)	Acc@1  78.12 ( 76.26)	Acc@5  95.31 ( 95.28)
Epoch: [25][ 30/391]	Time  0.242 ( 0.246)	Data  0.001 ( 0.005)	Loss 5.8158e-01 (8.0083e-01)	Acc@1  82.03 ( 76.34)	Acc@5  96.88 ( 95.29)
Epoch: [25][ 40/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.004)	Loss 7.3385e-01 (7.8384e-01)	Acc@1  78.12 ( 76.81)	Acc@5  96.88 ( 95.39)
Epoch: [25][ 50/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 6.3716e-01 (7.6781e-01)	Acc@1  78.12 ( 76.96)	Acc@5  98.44 ( 95.77)
Epoch: [25][ 60/391]	Time  0.248 ( 0.244)	Data  0.001 ( 0.003)	Loss 8.6428e-01 (7.6340e-01)	Acc@1  74.22 ( 77.01)	Acc@5  96.09 ( 95.81)
Epoch: [25][ 70/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 7.2326e-01 (7.7329e-01)	Acc@1  76.56 ( 76.64)	Acc@5  96.88 ( 95.74)
Epoch: [25][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 8.3847e-01 (7.7270e-01)	Acc@1  71.09 ( 76.56)	Acc@5  94.53 ( 95.78)
Epoch: [25][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 7.1768e-01 (7.7018e-01)	Acc@1  75.78 ( 76.64)	Acc@5  99.22 ( 95.87)
Epoch: [25][100/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 7.7000e-01 (7.7167e-01)	Acc@1  81.25 ( 76.48)	Acc@5  95.31 ( 95.88)
Epoch: [25][110/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.0130e+00 (7.7850e-01)	Acc@1  71.88 ( 76.32)	Acc@5  90.62 ( 95.85)
Epoch: [25][120/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 8.4945e-01 (7.8304e-01)	Acc@1  72.66 ( 76.24)	Acc@5  96.88 ( 95.79)
Epoch: [25][130/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 9.6252e-01 (7.8812e-01)	Acc@1  70.31 ( 76.17)	Acc@5  92.97 ( 95.76)
Epoch: [25][140/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 8.6177e-01 (7.9659e-01)	Acc@1  71.88 ( 75.78)	Acc@5  94.53 ( 95.74)
Epoch: [25][150/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 8.0276e-01 (7.9757e-01)	Acc@1  73.44 ( 75.76)	Acc@5  95.31 ( 95.72)
Epoch: [25][160/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0375e+00 (8.0305e-01)	Acc@1  69.53 ( 75.56)	Acc@5  92.97 ( 95.71)
Epoch: [25][170/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.0014e-01 (8.0515e-01)	Acc@1  81.25 ( 75.50)	Acc@5  92.97 ( 95.65)
Epoch: [25][180/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.1457e-01 (8.0614e-01)	Acc@1  75.78 ( 75.47)	Acc@5  96.09 ( 95.64)
Epoch: [25][190/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.1561e-01 (8.0708e-01)	Acc@1  77.34 ( 75.42)	Acc@5  96.88 ( 95.62)
Epoch: [25][200/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.4760e-01 (8.0822e-01)	Acc@1  72.66 ( 75.44)	Acc@5  93.75 ( 95.58)
Epoch: [25][210/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2266e+00 (8.1205e-01)	Acc@1  60.16 ( 75.31)	Acc@5  93.75 ( 95.55)
Epoch: [25][220/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.4287e-01 (8.1764e-01)	Acc@1  71.09 ( 75.16)	Acc@5  91.41 ( 95.46)
Epoch: [25][230/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0329e+00 (8.2130e-01)	Acc@1  71.88 ( 75.10)	Acc@5  91.41 ( 95.38)
Epoch: [25][240/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.9135e-01 (8.2395e-01)	Acc@1  69.53 ( 75.05)	Acc@5  94.53 ( 95.33)
Epoch: [25][250/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.3152e-01 (8.2655e-01)	Acc@1  73.44 ( 75.00)	Acc@5  92.97 ( 95.30)
Epoch: [25][260/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.3833e-01 (8.2845e-01)	Acc@1  76.56 ( 74.98)	Acc@5  92.97 ( 95.28)
Epoch: [25][270/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.0078e-01 (8.3326e-01)	Acc@1  73.44 ( 74.85)	Acc@5  92.19 ( 95.23)
Epoch: [25][280/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0220e+00 (8.3691e-01)	Acc@1  70.31 ( 74.76)	Acc@5  91.41 ( 95.20)
Epoch: [25][290/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.2846e-01 (8.3900e-01)	Acc@1  74.22 ( 74.70)	Acc@5  91.41 ( 95.19)
Epoch: [25][300/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1952e+00 (8.4012e-01)	Acc@1  60.94 ( 74.65)	Acc@5  92.19 ( 95.20)
Epoch: [25][310/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0586e+00 (8.4296e-01)	Acc@1  65.62 ( 74.56)	Acc@5  92.97 ( 95.17)
Epoch: [25][320/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1250e+00 (8.4686e-01)	Acc@1  74.22 ( 74.49)	Acc@5  92.97 ( 95.14)
Epoch: [25][330/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.1269e-01 (8.4790e-01)	Acc@1  71.88 ( 74.46)	Acc@5  96.88 ( 95.14)
Epoch: [25][340/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.1202e-01 (8.5086e-01)	Acc@1  70.31 ( 74.38)	Acc@5  94.53 ( 95.11)
Epoch: [25][350/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1197e+00 (8.5455e-01)	Acc@1  73.44 ( 74.28)	Acc@5  91.41 ( 95.06)
Epoch: [25][360/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1963e+00 (8.5876e-01)	Acc@1  69.53 ( 74.17)	Acc@5  88.28 ( 95.01)
Epoch: [25][370/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.001)	Loss 8.2901e-01 (8.6080e-01)	Acc@1  75.78 ( 74.10)	Acc@5  94.53 ( 94.98)
Epoch: [25][380/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.001)	Loss 9.4940e-01 (8.6225e-01)	Acc@1  71.09 ( 74.00)	Acc@5  96.09 ( 94.98)
Epoch: [25][390/391]	Time  0.178 ( 0.242)	Data  0.001 ( 0.001)	Loss 7.9241e-01 (8.6352e-01)	Acc@1  72.50 ( 73.99)	Acc@5  97.50 ( 94.97)
## e[25] optimizer.zero_grad (sum) time: 0.4931912422180176
## e[25]       loss.backward (sum) time: 11.082072257995605
## e[25]      optimizer.step (sum) time: 47.87240433692932
## epoch[25] training(only) time: 94.75280475616455
# Switched to evaluate mode...
Test: [  0/100]	Time  0.205 ( 0.205)	Loss 1.4285e+00 (1.4285e+00)	Acc@1  61.00 ( 61.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.079 ( 0.090)	Loss 1.4722e+00 (1.3906e+00)	Acc@1  59.00 ( 65.00)	Acc@5  86.00 ( 87.55)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 1.1815e+00 (1.3410e+00)	Acc@1  66.00 ( 65.05)	Acc@5  89.00 ( 88.86)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 1.5504e+00 (1.3901e+00)	Acc@1  58.00 ( 64.00)	Acc@5  87.00 ( 88.00)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.2850e+00 (1.3967e+00)	Acc@1  60.00 ( 63.05)	Acc@5  89.00 ( 88.10)
Test: [ 50/100]	Time  0.078 ( 0.080)	Loss 1.2961e+00 (1.4040e+00)	Acc@1  68.00 ( 63.25)	Acc@5  89.00 ( 87.75)
Test: [ 60/100]	Time  0.077 ( 0.080)	Loss 1.5293e+00 (1.3946e+00)	Acc@1  59.00 ( 63.07)	Acc@5  86.00 ( 87.97)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 1.4360e+00 (1.4017e+00)	Acc@1  62.00 ( 62.89)	Acc@5  88.00 ( 87.93)
Test: [ 80/100]	Time  0.077 ( 0.080)	Loss 1.5830e+00 (1.4006e+00)	Acc@1  58.00 ( 62.79)	Acc@5  80.00 ( 87.81)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.6080e+00 (1.3896e+00)	Acc@1  56.00 ( 62.93)	Acc@5  87.00 ( 88.07)
 * Acc@1 63.240 Acc@5 88.180
### epoch[25] execution time: 102.75928521156311
EPOCH 26
# current learning rate: 0.1
# Switched to train mode...
Epoch: [26][  0/391]	Time  0.392 ( 0.392)	Data  0.150 ( 0.150)	Loss 6.5336e-01 (6.5336e-01)	Acc@1  82.81 ( 82.81)	Acc@5  96.88 ( 96.88)
Epoch: [26][ 10/391]	Time  0.240 ( 0.255)	Data  0.001 ( 0.015)	Loss 6.8390e-01 (7.1727e-01)	Acc@1  75.78 ( 78.76)	Acc@5  97.66 ( 96.09)
Epoch: [26][ 20/391]	Time  0.242 ( 0.249)	Data  0.001 ( 0.008)	Loss 7.1693e-01 (7.3092e-01)	Acc@1  77.34 ( 77.86)	Acc@5  98.44 ( 95.91)
Epoch: [26][ 30/391]	Time  0.242 ( 0.247)	Data  0.001 ( 0.006)	Loss 6.8869e-01 (7.2130e-01)	Acc@1  78.12 ( 78.30)	Acc@5  93.75 ( 95.87)
Epoch: [26][ 40/391]	Time  0.243 ( 0.245)	Data  0.001 ( 0.005)	Loss 6.7427e-01 (7.2355e-01)	Acc@1  80.47 ( 78.18)	Acc@5  96.09 ( 95.92)
Epoch: [26][ 50/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.004)	Loss 6.4301e-01 (7.2024e-01)	Acc@1  78.91 ( 78.37)	Acc@5  97.66 ( 96.05)
Epoch: [26][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 8.3181e-01 (7.2478e-01)	Acc@1  71.88 ( 78.12)	Acc@5  93.75 ( 96.06)
Epoch: [26][ 70/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 7.6806e-01 (7.1877e-01)	Acc@1  75.00 ( 78.16)	Acc@5  96.88 ( 96.26)
Epoch: [26][ 80/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 8.6720e-01 (7.3073e-01)	Acc@1  72.66 ( 77.75)	Acc@5  94.53 ( 96.28)
Epoch: [26][ 90/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.003)	Loss 6.0723e-01 (7.3923e-01)	Acc@1  80.47 ( 77.49)	Acc@5  98.44 ( 96.19)
Epoch: [26][100/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 7.9512e-01 (7.3750e-01)	Acc@1  77.34 ( 77.55)	Acc@5  94.53 ( 96.16)
Epoch: [26][110/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 7.4581e-01 (7.4511e-01)	Acc@1  77.34 ( 77.28)	Acc@5  97.66 ( 96.06)
Epoch: [26][120/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 8.0292e-01 (7.5280e-01)	Acc@1  75.00 ( 77.00)	Acc@5  96.09 ( 95.92)
Epoch: [26][130/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 7.1403e-01 (7.6134e-01)	Acc@1  82.03 ( 76.74)	Acc@5  96.88 ( 95.83)
Epoch: [26][140/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 6.1555e-01 (7.6209e-01)	Acc@1  80.47 ( 76.71)	Acc@5  98.44 ( 95.78)
Epoch: [26][150/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 7.9500e-01 (7.6455e-01)	Acc@1  75.00 ( 76.73)	Acc@5  98.44 ( 95.79)
Epoch: [26][160/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 7.5029e-01 (7.6794e-01)	Acc@1  76.56 ( 76.65)	Acc@5  96.88 ( 95.79)
Epoch: [26][170/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 6.7026e-01 (7.7089e-01)	Acc@1  78.12 ( 76.58)	Acc@5  97.66 ( 95.76)
Epoch: [26][180/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 7.3302e-01 (7.7335e-01)	Acc@1  77.34 ( 76.53)	Acc@5  96.88 ( 95.79)
Epoch: [26][190/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 9.0392e-01 (7.7803e-01)	Acc@1  75.00 ( 76.35)	Acc@5  94.53 ( 95.80)
Epoch: [26][200/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.0602e+00 (7.7883e-01)	Acc@1  70.31 ( 76.33)	Acc@5  90.62 ( 95.81)
Epoch: [26][210/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 8.7810e-01 (7.7827e-01)	Acc@1  71.09 ( 76.40)	Acc@5  94.53 ( 95.81)
Epoch: [26][220/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.6212e-01 (7.8093e-01)	Acc@1  68.75 ( 76.22)	Acc@5  96.09 ( 95.80)
Epoch: [26][230/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.1787e-01 (7.8125e-01)	Acc@1  73.44 ( 76.23)	Acc@5  92.19 ( 95.80)
Epoch: [26][240/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.1632e-01 (7.8232e-01)	Acc@1  69.53 ( 76.24)	Acc@5  94.53 ( 95.79)
Epoch: [26][250/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.2035e-01 (7.8502e-01)	Acc@1  73.44 ( 76.10)	Acc@5  97.66 ( 95.75)
Epoch: [26][260/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.2375e-01 (7.8562e-01)	Acc@1  78.12 ( 76.08)	Acc@5  95.31 ( 95.76)
Epoch: [26][270/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.1838e-01 (7.9050e-01)	Acc@1  78.12 ( 75.93)	Acc@5  97.66 ( 95.71)
Epoch: [26][280/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.5408e-01 (7.9624e-01)	Acc@1  75.00 ( 75.81)	Acc@5  94.53 ( 95.64)
Epoch: [26][290/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.0763e-01 (7.9919e-01)	Acc@1  77.34 ( 75.71)	Acc@5  94.53 ( 95.61)
Epoch: [26][300/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.5214e-01 (8.0034e-01)	Acc@1  85.16 ( 75.68)	Acc@5  98.44 ( 95.61)
Epoch: [26][310/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.6410e-01 (8.0278e-01)	Acc@1  69.53 ( 75.60)	Acc@5  95.31 ( 95.58)
Epoch: [26][320/391]	Time  0.249 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.2304e-01 (8.0303e-01)	Acc@1  79.69 ( 75.60)	Acc@5  96.09 ( 95.56)
Epoch: [26][330/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.2182e-01 (8.0406e-01)	Acc@1  75.00 ( 75.57)	Acc@5  94.53 ( 95.58)
Epoch: [26][340/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.9960e-01 (8.0722e-01)	Acc@1  74.22 ( 75.48)	Acc@5  94.53 ( 95.56)
Epoch: [26][350/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.2830e-01 (8.0903e-01)	Acc@1  68.75 ( 75.44)	Acc@5  94.53 ( 95.55)
Epoch: [26][360/391]	Time  0.252 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.2816e-01 (8.1204e-01)	Acc@1  68.75 ( 75.35)	Acc@5  96.09 ( 95.51)
Epoch: [26][370/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.8302e-01 (8.1270e-01)	Acc@1  77.34 ( 75.31)	Acc@5  95.31 ( 95.49)
Epoch: [26][380/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.9743e-01 (8.1468e-01)	Acc@1  75.00 ( 75.25)	Acc@5  95.31 ( 95.49)
Epoch: [26][390/391]	Time  0.179 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1229e+00 (8.1732e-01)	Acc@1  68.75 ( 75.18)	Acc@5  93.75 ( 95.45)
## e[26] optimizer.zero_grad (sum) time: 0.49346303939819336
## e[26]       loss.backward (sum) time: 11.081396102905273
## e[26]      optimizer.step (sum) time: 47.85316562652588
## epoch[26] training(only) time: 94.832275390625
# Switched to evaluate mode...
Test: [  0/100]	Time  0.205 ( 0.205)	Loss 1.4355e+00 (1.4355e+00)	Acc@1  61.00 ( 61.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.079 ( 0.090)	Loss 1.6601e+00 (1.4930e+00)	Acc@1  59.00 ( 62.45)	Acc@5  91.00 ( 87.55)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 1.3084e+00 (1.4350e+00)	Acc@1  69.00 ( 62.38)	Acc@5  89.00 ( 88.81)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 1.5942e+00 (1.4789e+00)	Acc@1  52.00 ( 61.26)	Acc@5  85.00 ( 87.81)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.5317e+00 (1.4639e+00)	Acc@1  65.00 ( 61.68)	Acc@5  87.00 ( 88.27)
Test: [ 50/100]	Time  0.078 ( 0.080)	Loss 1.3627e+00 (1.4832e+00)	Acc@1  63.00 ( 61.63)	Acc@5  89.00 ( 88.25)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.5297e+00 (1.4709e+00)	Acc@1  60.00 ( 61.64)	Acc@5  87.00 ( 88.43)
Test: [ 70/100]	Time  0.077 ( 0.080)	Loss 1.6491e+00 (1.4745e+00)	Acc@1  57.00 ( 61.83)	Acc@5  91.00 ( 88.31)
Test: [ 80/100]	Time  0.079 ( 0.080)	Loss 1.5730e+00 (1.4755e+00)	Acc@1  64.00 ( 62.02)	Acc@5  86.00 ( 88.19)
Test: [ 90/100]	Time  0.077 ( 0.079)	Loss 1.4751e+00 (1.4632e+00)	Acc@1  58.00 ( 62.37)	Acc@5  87.00 ( 88.31)
 * Acc@1 62.430 Acc@5 88.370
### epoch[26] execution time: 102.84479451179504
EPOCH 27
# current learning rate: 0.1
# Switched to train mode...
Epoch: [27][  0/391]	Time  0.389 ( 0.389)	Data  0.132 ( 0.132)	Loss 9.7716e-01 (9.7716e-01)	Acc@1  69.53 ( 69.53)	Acc@5  96.09 ( 96.09)
Epoch: [27][ 10/391]	Time  0.241 ( 0.255)	Data  0.001 ( 0.013)	Loss 6.0411e-01 (7.5390e-01)	Acc@1  84.38 ( 78.27)	Acc@5  96.88 ( 96.59)
Epoch: [27][ 20/391]	Time  0.241 ( 0.248)	Data  0.001 ( 0.007)	Loss 8.4061e-01 (7.5650e-01)	Acc@1  75.78 ( 77.94)	Acc@5  94.53 ( 96.39)
Epoch: [27][ 30/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.005)	Loss 6.4965e-01 (7.4333e-01)	Acc@1  82.03 ( 77.92)	Acc@5  96.09 ( 96.37)
Epoch: [27][ 40/391]	Time  0.245 ( 0.245)	Data  0.001 ( 0.004)	Loss 6.5525e-01 (7.3486e-01)	Acc@1  82.03 ( 77.93)	Acc@5  96.09 ( 96.47)
Epoch: [27][ 50/391]	Time  0.249 ( 0.244)	Data  0.001 ( 0.004)	Loss 6.3773e-01 (7.3496e-01)	Acc@1  81.25 ( 77.85)	Acc@5  97.66 ( 96.40)
Epoch: [27][ 60/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.003)	Loss 6.4506e-01 (7.3216e-01)	Acc@1  79.69 ( 77.97)	Acc@5  97.66 ( 96.53)
Epoch: [27][ 70/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 5.3255e-01 (7.2570e-01)	Acc@1  84.38 ( 78.09)	Acc@5  97.66 ( 96.61)
Epoch: [27][ 80/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 8.0139e-01 (7.2399e-01)	Acc@1  74.22 ( 78.13)	Acc@5  97.66 ( 96.61)
Epoch: [27][ 90/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.003)	Loss 6.0132e-01 (7.2854e-01)	Acc@1  81.25 ( 78.06)	Acc@5  97.66 ( 96.57)
Epoch: [27][100/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 7.2802e-01 (7.2967e-01)	Acc@1  75.00 ( 77.89)	Acc@5  99.22 ( 96.55)
Epoch: [27][110/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 7.8601e-01 (7.3173e-01)	Acc@1  77.34 ( 77.80)	Acc@5  97.66 ( 96.52)
Epoch: [27][120/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 6.6887e-01 (7.3473e-01)	Acc@1  75.78 ( 77.65)	Acc@5  96.88 ( 96.46)
Epoch: [27][130/391]	Time  0.239 ( 0.243)	Data  0.001 ( 0.002)	Loss 8.5892e-01 (7.3939e-01)	Acc@1  75.00 ( 77.62)	Acc@5  93.75 ( 96.34)
Epoch: [27][140/391]	Time  0.246 ( 0.243)	Data  0.001 ( 0.002)	Loss 6.6077e-01 (7.3861e-01)	Acc@1  82.81 ( 77.66)	Acc@5  96.88 ( 96.37)
Epoch: [27][150/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.0866e-01 (7.4552e-01)	Acc@1  75.00 ( 77.39)	Acc@5  93.75 ( 96.32)
Epoch: [27][160/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.2765e-01 (7.4694e-01)	Acc@1  82.03 ( 77.25)	Acc@5  96.09 ( 96.33)
Epoch: [27][170/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.8295e-01 (7.4982e-01)	Acc@1  75.00 ( 77.12)	Acc@5  96.88 ( 96.33)
Epoch: [27][180/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.7695e-01 (7.5602e-01)	Acc@1  75.78 ( 76.98)	Acc@5  93.75 ( 96.28)
Epoch: [27][190/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.9347e-01 (7.5926e-01)	Acc@1  75.78 ( 76.85)	Acc@5  92.97 ( 96.27)
Epoch: [27][200/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.5686e-01 (7.6231e-01)	Acc@1  78.91 ( 76.78)	Acc@5  97.66 ( 96.24)
Epoch: [27][210/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.4212e-01 (7.6530e-01)	Acc@1  75.78 ( 76.70)	Acc@5  96.88 ( 96.19)
Epoch: [27][220/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.8745e-01 (7.7169e-01)	Acc@1  71.88 ( 76.55)	Acc@5  93.75 ( 96.08)
Epoch: [27][230/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.2404e-01 (7.7579e-01)	Acc@1  82.03 ( 76.47)	Acc@5  98.44 ( 96.06)
Epoch: [27][240/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.4036e-01 (7.7683e-01)	Acc@1  72.66 ( 76.41)	Acc@5  95.31 ( 96.06)
Epoch: [27][250/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.4406e-01 (7.7678e-01)	Acc@1  78.12 ( 76.33)	Acc@5  96.09 ( 96.08)
Epoch: [27][260/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.0042e-01 (7.7940e-01)	Acc@1  76.56 ( 76.20)	Acc@5  96.88 ( 96.05)
Epoch: [27][270/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.7759e-01 (7.8054e-01)	Acc@1  78.91 ( 76.18)	Acc@5  96.88 ( 96.04)
Epoch: [27][280/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.0161e-01 (7.8114e-01)	Acc@1  73.44 ( 76.17)	Acc@5  96.09 ( 96.02)
Epoch: [27][290/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.1468e-01 (7.8226e-01)	Acc@1  73.44 ( 76.12)	Acc@5  96.88 ( 96.01)
Epoch: [27][300/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.3616e-01 (7.8267e-01)	Acc@1  75.00 ( 76.11)	Acc@5  96.09 ( 96.02)
Epoch: [27][310/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.0537e-01 (7.8345e-01)	Acc@1  71.88 ( 76.08)	Acc@5  96.09 ( 95.99)
Epoch: [27][320/391]	Time  0.248 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.0100e-01 (7.8387e-01)	Acc@1  76.56 ( 76.09)	Acc@5  92.97 ( 96.01)
Epoch: [27][330/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.3223e-01 (7.8497e-01)	Acc@1  72.66 ( 76.05)	Acc@5  96.09 ( 95.97)
Epoch: [27][340/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.1689e-01 (7.8688e-01)	Acc@1  76.56 ( 76.00)	Acc@5  97.66 ( 95.95)
Epoch: [27][350/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.8941e-01 (7.8848e-01)	Acc@1  82.03 ( 75.99)	Acc@5  97.66 ( 95.92)
Epoch: [27][360/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.4997e-01 (7.8776e-01)	Acc@1  75.78 ( 76.04)	Acc@5  97.66 ( 95.93)
Epoch: [27][370/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.4436e-01 (7.8953e-01)	Acc@1  77.34 ( 76.01)	Acc@5  95.31 ( 95.90)
Epoch: [27][380/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.8656e-01 (7.8950e-01)	Acc@1  72.66 ( 75.98)	Acc@5  96.88 ( 95.91)
Epoch: [27][390/391]	Time  0.179 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.2775e-01 (7.8979e-01)	Acc@1  78.75 ( 75.98)	Acc@5  97.50 ( 95.90)
## e[27] optimizer.zero_grad (sum) time: 0.4920015335083008
## e[27]       loss.backward (sum) time: 11.121997594833374
## e[27]      optimizer.step (sum) time: 47.841545820236206
## epoch[27] training(only) time: 94.73824906349182
# Switched to evaluate mode...
Test: [  0/100]	Time  0.214 ( 0.214)	Loss 1.5052e+00 (1.5052e+00)	Acc@1  61.00 ( 61.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.077 ( 0.090)	Loss 1.5930e+00 (1.5432e+00)	Acc@1  59.00 ( 63.55)	Acc@5  91.00 ( 87.91)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 1.2613e+00 (1.5159e+00)	Acc@1  63.00 ( 63.19)	Acc@5  92.00 ( 88.67)
Test: [ 30/100]	Time  0.077 ( 0.082)	Loss 1.6943e+00 (1.5769e+00)	Acc@1  52.00 ( 62.23)	Acc@5  87.00 ( 87.55)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.5331e+00 (1.5799e+00)	Acc@1  61.00 ( 62.07)	Acc@5  88.00 ( 87.71)
Test: [ 50/100]	Time  0.076 ( 0.081)	Loss 1.6419e+00 (1.5977e+00)	Acc@1  58.00 ( 61.90)	Acc@5  87.00 ( 87.31)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.8842e+00 (1.5916e+00)	Acc@1  59.00 ( 61.80)	Acc@5  87.00 ( 87.49)
Test: [ 70/100]	Time  0.077 ( 0.080)	Loss 1.5562e+00 (1.5913e+00)	Acc@1  56.00 ( 61.52)	Acc@5  88.00 ( 87.39)
Test: [ 80/100]	Time  0.079 ( 0.080)	Loss 2.2660e+00 (1.5857e+00)	Acc@1  59.00 ( 61.73)	Acc@5  85.00 ( 87.37)
Test: [ 90/100]	Time  0.079 ( 0.079)	Loss 1.8387e+00 (1.5777e+00)	Acc@1  59.00 ( 61.88)	Acc@5  84.00 ( 87.36)
 * Acc@1 62.130 Acc@5 87.390
### epoch[27] execution time: 102.74708557128906
EPOCH 28
# current learning rate: 0.1
# Switched to train mode...
Epoch: [28][  0/391]	Time  0.388 ( 0.388)	Data  0.140 ( 0.140)	Loss 7.5760e-01 (7.5760e-01)	Acc@1  77.34 ( 77.34)	Acc@5  97.66 ( 97.66)
Epoch: [28][ 10/391]	Time  0.240 ( 0.255)	Data  0.001 ( 0.014)	Loss 8.0640e-01 (7.5572e-01)	Acc@1  75.78 ( 77.41)	Acc@5  93.75 ( 96.09)
Epoch: [28][ 20/391]	Time  0.246 ( 0.248)	Data  0.001 ( 0.008)	Loss 5.4858e-01 (7.0994e-01)	Acc@1  82.81 ( 78.05)	Acc@5  99.22 ( 96.99)
Epoch: [28][ 30/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.006)	Loss 6.6568e-01 (6.8664e-01)	Acc@1  78.91 ( 78.55)	Acc@5  97.66 ( 97.15)
Epoch: [28][ 40/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.004)	Loss 6.3204e-01 (6.9044e-01)	Acc@1  78.91 ( 78.39)	Acc@5  96.09 ( 97.14)
Epoch: [28][ 50/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 6.3352e-01 (6.8085e-01)	Acc@1  80.47 ( 78.54)	Acc@5 100.00 ( 97.23)
Epoch: [28][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 7.7030e-01 (6.8129e-01)	Acc@1  72.66 ( 78.60)	Acc@5  96.88 ( 97.25)
Epoch: [28][ 70/391]	Time  0.246 ( 0.244)	Data  0.001 ( 0.003)	Loss 5.4999e-01 (6.8100e-01)	Acc@1  82.81 ( 78.59)	Acc@5  97.66 ( 97.21)
Epoch: [28][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 6.1076e-01 (6.8285e-01)	Acc@1  81.25 ( 78.56)	Acc@5  98.44 ( 97.15)
Epoch: [28][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 7.8499e-01 (6.8252e-01)	Acc@1  76.56 ( 78.71)	Acc@5  95.31 ( 97.07)
Epoch: [28][100/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 7.3912e-01 (6.8676e-01)	Acc@1  78.12 ( 78.68)	Acc@5  96.09 ( 96.95)
Epoch: [28][110/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 7.2870e-01 (6.9027e-01)	Acc@1  79.69 ( 78.68)	Acc@5  95.31 ( 96.81)
Epoch: [28][120/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 6.9845e-01 (6.8983e-01)	Acc@1  76.56 ( 78.66)	Acc@5  99.22 ( 96.79)
Epoch: [28][130/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 7.1418e-01 (6.9051e-01)	Acc@1  78.91 ( 78.73)	Acc@5  96.09 ( 96.86)
Epoch: [28][140/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 5.2252e-01 (6.9408e-01)	Acc@1  80.47 ( 78.67)	Acc@5  99.22 ( 96.81)
Epoch: [28][150/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 6.9381e-01 (6.9473e-01)	Acc@1  78.12 ( 78.69)	Acc@5  97.66 ( 96.82)
Epoch: [28][160/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 8.8301e-01 (6.9806e-01)	Acc@1  73.44 ( 78.47)	Acc@5  93.75 ( 96.81)
Epoch: [28][170/391]	Time  0.246 ( 0.243)	Data  0.001 ( 0.002)	Loss 7.3401e-01 (6.9942e-01)	Acc@1  74.22 ( 78.44)	Acc@5  99.22 ( 96.78)
Epoch: [28][180/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 6.9505e-01 (7.0103e-01)	Acc@1  78.91 ( 78.41)	Acc@5  96.88 ( 96.73)
Epoch: [28][190/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 5.1593e-01 (7.0597e-01)	Acc@1  83.59 ( 78.31)	Acc@5  99.22 ( 96.68)
Epoch: [28][200/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.002)	Loss 7.7091e-01 (7.0900e-01)	Acc@1  75.00 ( 78.28)	Acc@5  96.09 ( 96.65)
Epoch: [28][210/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.2243e-01 (7.0850e-01)	Acc@1  84.38 ( 78.28)	Acc@5  96.88 ( 96.66)
Epoch: [28][220/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.1774e-01 (7.1118e-01)	Acc@1  76.56 ( 78.24)	Acc@5  96.88 ( 96.61)
Epoch: [28][230/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.3778e-01 (7.1194e-01)	Acc@1  75.00 ( 78.26)	Acc@5  94.53 ( 96.60)
Epoch: [28][240/391]	Time  0.250 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.2544e-01 (7.1499e-01)	Acc@1  78.91 ( 78.16)	Acc@5  96.88 ( 96.57)
Epoch: [28][250/391]	Time  0.247 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.8897e-01 (7.1614e-01)	Acc@1  71.09 ( 78.10)	Acc@5  95.31 ( 96.55)
Epoch: [28][260/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.8277e-01 (7.2138e-01)	Acc@1  69.53 ( 77.93)	Acc@5  96.88 ( 96.51)
Epoch: [28][270/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.4301e-01 (7.2105e-01)	Acc@1  78.12 ( 77.93)	Acc@5  96.09 ( 96.53)
Epoch: [28][280/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.3828e-01 (7.2465e-01)	Acc@1  77.34 ( 77.86)	Acc@5  95.31 ( 96.47)
Epoch: [28][290/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.6663e-01 (7.2672e-01)	Acc@1  82.03 ( 77.82)	Acc@5  96.88 ( 96.43)
Epoch: [28][300/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.0040e-01 (7.2715e-01)	Acc@1  79.69 ( 77.78)	Acc@5  97.66 ( 96.43)
Epoch: [28][310/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.8840e-01 (7.2815e-01)	Acc@1  77.34 ( 77.78)	Acc@5  94.53 ( 96.42)
Epoch: [28][320/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1828e+00 (7.3079e-01)	Acc@1  70.31 ( 77.75)	Acc@5  92.19 ( 96.39)
Epoch: [28][330/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.0190e-01 (7.3370e-01)	Acc@1  75.78 ( 77.66)	Acc@5  93.75 ( 96.36)
Epoch: [28][340/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.5059e-01 (7.3476e-01)	Acc@1  72.66 ( 77.65)	Acc@5  92.97 ( 96.36)
Epoch: [28][350/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.9651e-01 (7.3914e-01)	Acc@1  72.66 ( 77.56)	Acc@5  96.09 ( 96.29)
Epoch: [28][360/391]	Time  0.250 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.0450e-01 (7.4035e-01)	Acc@1  81.25 ( 77.50)	Acc@5  96.88 ( 96.29)
Epoch: [28][370/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.8422e-01 (7.4210e-01)	Acc@1  75.78 ( 77.45)	Acc@5  96.09 ( 96.27)
Epoch: [28][380/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.5533e-01 (7.4336e-01)	Acc@1  74.22 ( 77.42)	Acc@5  97.66 ( 96.26)
Epoch: [28][390/391]	Time  0.178 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.1373e-01 (7.4428e-01)	Acc@1  78.75 ( 77.37)	Acc@5 100.00 ( 96.27)
## e[28] optimizer.zero_grad (sum) time: 0.4931659698486328
## e[28]       loss.backward (sum) time: 11.103499174118042
## e[28]      optimizer.step (sum) time: 47.83941435813904
## epoch[28] training(only) time: 94.81371903419495
# Switched to evaluate mode...
Test: [  0/100]	Time  0.206 ( 0.206)	Loss 1.4329e+00 (1.4329e+00)	Acc@1  64.00 ( 64.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.079 ( 0.089)	Loss 1.6561e+00 (1.4726e+00)	Acc@1  58.00 ( 64.18)	Acc@5  89.00 ( 87.55)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 1.1114e+00 (1.4130e+00)	Acc@1  69.00 ( 64.76)	Acc@5  91.00 ( 88.48)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 1.7189e+00 (1.4349e+00)	Acc@1  47.00 ( 63.81)	Acc@5  91.00 ( 88.23)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.2343e+00 (1.4396e+00)	Acc@1  68.00 ( 63.95)	Acc@5  88.00 ( 88.37)
Test: [ 50/100]	Time  0.079 ( 0.080)	Loss 1.4797e+00 (1.4451e+00)	Acc@1  65.00 ( 63.80)	Acc@5  86.00 ( 88.10)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.3830e+00 (1.4264e+00)	Acc@1  65.00 ( 63.92)	Acc@5  90.00 ( 88.46)
Test: [ 70/100]	Time  0.079 ( 0.080)	Loss 1.5418e+00 (1.4392e+00)	Acc@1  60.00 ( 63.72)	Acc@5  86.00 ( 88.37)
Test: [ 80/100]	Time  0.077 ( 0.080)	Loss 1.4910e+00 (1.4450e+00)	Acc@1  63.00 ( 63.54)	Acc@5  88.00 ( 88.28)
Test: [ 90/100]	Time  0.079 ( 0.079)	Loss 1.6488e+00 (1.4378e+00)	Acc@1  54.00 ( 63.59)	Acc@5  88.00 ( 88.41)
 * Acc@1 63.400 Acc@5 88.450
### epoch[28] execution time: 102.81853318214417
EPOCH 29
# current learning rate: 0.1
# Switched to train mode...
Epoch: [29][  0/391]	Time  0.391 ( 0.391)	Data  0.147 ( 0.147)	Loss 4.9062e-01 (4.9062e-01)	Acc@1  82.03 ( 82.03)	Acc@5 100.00 (100.00)
Epoch: [29][ 10/391]	Time  0.241 ( 0.254)	Data  0.001 ( 0.014)	Loss 5.7489e-01 (5.8869e-01)	Acc@1  82.03 ( 81.04)	Acc@5  99.22 ( 98.22)
Epoch: [29][ 20/391]	Time  0.241 ( 0.248)	Data  0.001 ( 0.008)	Loss 8.4314e-01 (6.1148e-01)	Acc@1  73.44 ( 79.95)	Acc@5  95.31 ( 97.81)
Epoch: [29][ 30/391]	Time  0.240 ( 0.246)	Data  0.001 ( 0.006)	Loss 6.6242e-01 (6.2714e-01)	Acc@1  79.69 ( 79.44)	Acc@5  96.88 ( 97.73)
Epoch: [29][ 40/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.005)	Loss 7.3570e-01 (6.4449e-01)	Acc@1  79.69 ( 79.34)	Acc@5  95.31 ( 97.35)
Epoch: [29][ 50/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 6.7886e-01 (6.5359e-01)	Acc@1  79.69 ( 79.29)	Acc@5  94.53 ( 97.29)
Epoch: [29][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 6.9574e-01 (6.6013e-01)	Acc@1  80.47 ( 79.23)	Acc@5  98.44 ( 97.21)
Epoch: [29][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 5.3242e-01 (6.5813e-01)	Acc@1  82.03 ( 79.36)	Acc@5  97.66 ( 97.24)
Epoch: [29][ 80/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 6.6075e-01 (6.5574e-01)	Acc@1  78.91 ( 79.52)	Acc@5  97.66 ( 97.24)
Epoch: [29][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 5.4658e-01 (6.5512e-01)	Acc@1  82.03 ( 79.61)	Acc@5  98.44 ( 97.22)
Epoch: [29][100/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 7.1538e-01 (6.5779e-01)	Acc@1  77.34 ( 79.57)	Acc@5  96.09 ( 97.19)
Epoch: [29][110/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 7.0963e-01 (6.6163e-01)	Acc@1  75.78 ( 79.46)	Acc@5  95.31 ( 97.09)
Epoch: [29][120/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 8.4323e-01 (6.6780e-01)	Acc@1  74.22 ( 79.36)	Acc@5  97.66 ( 96.96)
Epoch: [29][130/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 7.5305e-01 (6.7436e-01)	Acc@1  78.91 ( 79.17)	Acc@5  98.44 ( 96.88)
Epoch: [29][140/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 6.8660e-01 (6.7761e-01)	Acc@1  82.81 ( 79.19)	Acc@5  93.75 ( 96.84)
Epoch: [29][150/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 5.1925e-01 (6.7991e-01)	Acc@1  85.94 ( 79.15)	Acc@5  97.66 ( 96.81)
Epoch: [29][160/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 5.1415e-01 (6.7969e-01)	Acc@1  83.59 ( 79.20)	Acc@5  98.44 ( 96.78)
Epoch: [29][170/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 7.1095e-01 (6.8131e-01)	Acc@1  79.69 ( 79.11)	Acc@5  96.88 ( 96.76)
Epoch: [29][180/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 8.6172e-01 (6.8283e-01)	Acc@1  73.44 ( 79.02)	Acc@5  96.09 ( 96.75)
Epoch: [29][190/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 8.5512e-01 (6.8842e-01)	Acc@1  79.69 ( 78.85)	Acc@5  95.31 ( 96.69)
Epoch: [29][200/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 5.2728e-01 (6.8878e-01)	Acc@1  82.81 ( 78.87)	Acc@5  97.66 ( 96.69)
Epoch: [29][210/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.4118e-01 (6.8907e-01)	Acc@1  85.16 ( 78.84)	Acc@5  99.22 ( 96.71)
Epoch: [29][220/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 5.0047e-01 (6.9034e-01)	Acc@1  82.81 ( 78.78)	Acc@5  99.22 ( 96.69)
Epoch: [29][230/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.6199e-01 (6.9196e-01)	Acc@1  82.81 ( 78.74)	Acc@5  97.66 ( 96.66)
Epoch: [29][240/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.5834e-01 (6.9305e-01)	Acc@1  78.12 ( 78.70)	Acc@5  98.44 ( 96.63)
Epoch: [29][250/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.6653e-01 (6.9549e-01)	Acc@1  73.44 ( 78.62)	Acc@5  93.75 ( 96.61)
Epoch: [29][260/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.3241e-01 (6.9783e-01)	Acc@1  79.69 ( 78.60)	Acc@5  95.31 ( 96.59)
Epoch: [29][270/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.3912e-01 (6.9891e-01)	Acc@1  79.69 ( 78.60)	Acc@5  96.88 ( 96.61)
Epoch: [29][280/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.9206e-01 (6.9910e-01)	Acc@1  74.22 ( 78.60)	Acc@5  96.88 ( 96.62)
Epoch: [29][290/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.2972e-01 (7.0021e-01)	Acc@1  70.31 ( 78.60)	Acc@5  93.75 ( 96.59)
Epoch: [29][300/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.7424e-01 (7.0070e-01)	Acc@1  78.12 ( 78.60)	Acc@5  96.09 ( 96.56)
Epoch: [29][310/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.5085e-01 (7.0312e-01)	Acc@1  75.78 ( 78.47)	Acc@5  95.31 ( 96.57)
Epoch: [29][320/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.2290e-01 (7.0304e-01)	Acc@1  81.25 ( 78.49)	Acc@5  96.09 ( 96.58)
Epoch: [29][330/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.9834e-01 (7.0532e-01)	Acc@1  73.44 ( 78.39)	Acc@5  99.22 ( 96.55)
Epoch: [29][340/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.5301e-01 (7.0568e-01)	Acc@1  80.47 ( 78.37)	Acc@5  95.31 ( 96.54)
Epoch: [29][350/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.9955e-01 (7.0825e-01)	Acc@1  75.00 ( 78.32)	Acc@5  95.31 ( 96.51)
Epoch: [29][360/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.8910e-01 (7.0948e-01)	Acc@1  71.88 ( 78.25)	Acc@5  96.88 ( 96.53)
Epoch: [29][370/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.5916e-01 (7.1128e-01)	Acc@1  75.00 ( 78.24)	Acc@5  97.66 ( 96.50)
Epoch: [29][380/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.001)	Loss 9.4165e-01 (7.1473e-01)	Acc@1  71.88 ( 78.13)	Acc@5  93.75 ( 96.47)
Epoch: [29][390/391]	Time  0.178 ( 0.242)	Data  0.001 ( 0.001)	Loss 8.5495e-01 (7.1748e-01)	Acc@1  70.00 ( 78.05)	Acc@5  93.75 ( 96.44)
## e[29] optimizer.zero_grad (sum) time: 0.4913792610168457
## e[29]       loss.backward (sum) time: 11.125900268554688
## e[29]      optimizer.step (sum) time: 47.854697704315186
## epoch[29] training(only) time: 94.81983995437622
# Switched to evaluate mode...
Test: [  0/100]	Time  0.217 ( 0.217)	Loss 1.3476e+00 (1.3476e+00)	Acc@1  62.00 ( 62.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.077 ( 0.091)	Loss 1.5521e+00 (1.5643e+00)	Acc@1  60.00 ( 63.18)	Acc@5  90.00 ( 87.09)
Test: [ 20/100]	Time  0.078 ( 0.085)	Loss 1.6826e+00 (1.5309e+00)	Acc@1  59.00 ( 62.90)	Acc@5  87.00 ( 87.76)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 1.8949e+00 (1.5596e+00)	Acc@1  58.00 ( 62.52)	Acc@5  83.00 ( 87.48)
Test: [ 40/100]	Time  0.079 ( 0.081)	Loss 1.4828e+00 (1.5538e+00)	Acc@1  64.00 ( 62.46)	Acc@5  89.00 ( 87.68)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 2.0056e+00 (1.5765e+00)	Acc@1  60.00 ( 62.47)	Acc@5  82.00 ( 87.04)
Test: [ 60/100]	Time  0.077 ( 0.080)	Loss 1.5559e+00 (1.5573e+00)	Acc@1  63.00 ( 62.59)	Acc@5  88.00 ( 87.59)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 1.5980e+00 (1.5568e+00)	Acc@1  63.00 ( 62.51)	Acc@5  89.00 ( 87.63)
Test: [ 80/100]	Time  0.079 ( 0.080)	Loss 1.6871e+00 (1.5416e+00)	Acc@1  60.00 ( 62.70)	Acc@5  85.00 ( 87.67)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.6430e+00 (1.5253e+00)	Acc@1  61.00 ( 63.09)	Acc@5  84.00 ( 87.82)
 * Acc@1 63.250 Acc@5 88.010
### epoch[29] execution time: 102.86549234390259
EPOCH 30
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [30][  0/391]	Time  0.391 ( 0.391)	Data  0.133 ( 0.133)	Loss 7.0271e-01 (7.0271e-01)	Acc@1  77.34 ( 77.34)	Acc@5  97.66 ( 97.66)
Epoch: [30][ 10/391]	Time  0.242 ( 0.255)	Data  0.001 ( 0.013)	Loss 4.9886e-01 (6.2440e-01)	Acc@1  84.38 ( 80.82)	Acc@5  99.22 ( 97.44)
Epoch: [30][ 20/391]	Time  0.242 ( 0.249)	Data  0.001 ( 0.007)	Loss 6.6337e-01 (6.0908e-01)	Acc@1  79.69 ( 81.06)	Acc@5  95.31 ( 97.25)
Epoch: [30][ 30/391]	Time  0.242 ( 0.246)	Data  0.001 ( 0.005)	Loss 4.9925e-01 (5.8064e-01)	Acc@1  85.94 ( 82.01)	Acc@5  97.66 ( 97.40)
Epoch: [30][ 40/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.004)	Loss 4.4204e-01 (5.6045e-01)	Acc@1  85.94 ( 82.93)	Acc@5  99.22 ( 97.54)
Epoch: [30][ 50/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.004)	Loss 4.7874e-01 (5.5305e-01)	Acc@1  87.50 ( 83.27)	Acc@5  98.44 ( 97.63)
Epoch: [30][ 60/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.003)	Loss 3.6133e-01 (5.4259e-01)	Acc@1  90.62 ( 83.62)	Acc@5  99.22 ( 97.77)
Epoch: [30][ 70/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.7593e-01 (5.2898e-01)	Acc@1  92.19 ( 84.12)	Acc@5  98.44 ( 97.92)
Epoch: [30][ 80/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 4.4427e-01 (5.2026e-01)	Acc@1  85.16 ( 84.46)	Acc@5  98.44 ( 98.02)
Epoch: [30][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 4.5218e-01 (5.1214e-01)	Acc@1  88.28 ( 84.74)	Acc@5  96.09 ( 98.05)
Epoch: [30][100/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 5.7553e-01 (5.0717e-01)	Acc@1  84.38 ( 84.81)	Acc@5  98.44 ( 98.08)
Epoch: [30][110/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.5846e-01 (4.9769e-01)	Acc@1  88.28 ( 85.16)	Acc@5  98.44 ( 98.18)
Epoch: [30][120/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.7624e-01 (4.9137e-01)	Acc@1  85.16 ( 85.38)	Acc@5  98.44 ( 98.24)
Epoch: [30][130/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.4807e-01 (4.8594e-01)	Acc@1  84.38 ( 85.56)	Acc@5  98.44 ( 98.29)
Epoch: [30][140/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.6634e-01 (4.7904e-01)	Acc@1  86.72 ( 85.77)	Acc@5  98.44 ( 98.33)
Epoch: [30][150/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.7903e-01 (4.7565e-01)	Acc@1  87.50 ( 85.85)	Acc@5  99.22 ( 98.37)
Epoch: [30][160/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.7572e-01 (4.7153e-01)	Acc@1  83.59 ( 85.98)	Acc@5  98.44 ( 98.37)
Epoch: [30][170/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.4294e-01 (4.6627e-01)	Acc@1  88.28 ( 86.11)	Acc@5  99.22 ( 98.41)
Epoch: [30][180/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4510e-01 (4.6179e-01)	Acc@1  92.19 ( 86.23)	Acc@5 100.00 ( 98.45)
Epoch: [30][190/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.2727e-01 (4.5913e-01)	Acc@1  85.94 ( 86.34)	Acc@5  97.66 ( 98.44)
Epoch: [30][200/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.4938e-01 (4.5502e-01)	Acc@1  89.06 ( 86.42)	Acc@5  99.22 ( 98.46)
Epoch: [30][210/391]	Time  0.246 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3712e-01 (4.5014e-01)	Acc@1  92.97 ( 86.61)	Acc@5  98.44 ( 98.48)
Epoch: [30][220/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.4289e-01 (4.4738e-01)	Acc@1  92.97 ( 86.67)	Acc@5  98.44 ( 98.51)
Epoch: [30][230/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.7722e-01 (4.4512e-01)	Acc@1  89.06 ( 86.76)	Acc@5  99.22 ( 98.50)
Epoch: [30][240/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3235e-01 (4.4172e-01)	Acc@1  92.97 ( 86.89)	Acc@5 100.00 ( 98.53)
Epoch: [30][250/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.7989e-01 (4.3863e-01)	Acc@1  92.19 ( 87.01)	Acc@5  99.22 ( 98.53)
Epoch: [30][260/391]	Time  0.246 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.6692e-01 (4.3616e-01)	Acc@1  87.50 ( 87.08)	Acc@5  99.22 ( 98.55)
Epoch: [30][270/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.8909e-01 (4.3170e-01)	Acc@1  89.84 ( 87.24)	Acc@5  99.22 ( 98.57)
Epoch: [30][280/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.4116e-01 (4.2909e-01)	Acc@1  88.28 ( 87.32)	Acc@5  98.44 ( 98.60)
Epoch: [30][290/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.001)	Loss 3.5233e-01 (4.2675e-01)	Acc@1  91.41 ( 87.38)	Acc@5  99.22 ( 98.62)
Epoch: [30][300/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.001)	Loss 3.0119e-01 (4.2427e-01)	Acc@1  91.41 ( 87.44)	Acc@5  99.22 ( 98.63)
Epoch: [30][310/391]	Time  0.247 ( 0.242)	Data  0.001 ( 0.001)	Loss 3.6988e-01 (4.2210e-01)	Acc@1  89.06 ( 87.49)	Acc@5  98.44 ( 98.65)
Epoch: [30][320/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.001)	Loss 3.3626e-01 (4.1972e-01)	Acc@1  89.84 ( 87.54)	Acc@5 100.00 ( 98.66)
Epoch: [30][330/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.001)	Loss 4.7535e-01 (4.1779e-01)	Acc@1  84.38 ( 87.60)	Acc@5  98.44 ( 98.68)
Epoch: [30][340/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.001)	Loss 2.0379e-01 (4.1583e-01)	Acc@1  94.53 ( 87.66)	Acc@5 100.00 ( 98.69)
Epoch: [30][350/391]	Time  0.251 ( 0.242)	Data  0.001 ( 0.001)	Loss 3.3879e-01 (4.1451e-01)	Acc@1  85.94 ( 87.70)	Acc@5 100.00 ( 98.70)
Epoch: [30][360/391]	Time  0.247 ( 0.242)	Data  0.001 ( 0.001)	Loss 3.2350e-01 (4.1330e-01)	Acc@1  89.84 ( 87.74)	Acc@5  99.22 ( 98.71)
Epoch: [30][370/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.001)	Loss 4.0383e-01 (4.1192e-01)	Acc@1  85.16 ( 87.76)	Acc@5  98.44 ( 98.72)
Epoch: [30][380/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.001)	Loss 3.8198e-01 (4.1021e-01)	Acc@1  89.84 ( 87.83)	Acc@5  98.44 ( 98.71)
Epoch: [30][390/391]	Time  0.179 ( 0.242)	Data  0.001 ( 0.001)	Loss 2.0095e-01 (4.0881e-01)	Acc@1  95.00 ( 87.87)	Acc@5 100.00 ( 98.73)
## e[30] optimizer.zero_grad (sum) time: 0.49538302421569824
## e[30]       loss.backward (sum) time: 11.095997095108032
## e[30]      optimizer.step (sum) time: 47.85962772369385
## epoch[30] training(only) time: 94.72857928276062
# Switched to evaluate mode...
Test: [  0/100]	Time  0.209 ( 0.209)	Loss 9.5318e-01 (9.5318e-01)	Acc@1  78.00 ( 78.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.079 ( 0.090)	Loss 1.0944e+00 (1.0756e+00)	Acc@1  72.00 ( 72.27)	Acc@5  94.00 ( 91.45)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 9.9741e-01 (1.0505e+00)	Acc@1  75.00 ( 72.86)	Acc@5  94.00 ( 92.19)
Test: [ 30/100]	Time  0.077 ( 0.082)	Loss 1.3758e+00 (1.0882e+00)	Acc@1  64.00 ( 72.03)	Acc@5  93.00 ( 91.87)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.0976e+00 (1.0991e+00)	Acc@1  76.00 ( 71.78)	Acc@5  93.00 ( 92.00)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 1.4453e+00 (1.1193e+00)	Acc@1  66.00 ( 71.35)	Acc@5  91.00 ( 91.69)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.1633e+00 (1.0981e+00)	Acc@1  72.00 ( 71.69)	Acc@5  89.00 ( 92.11)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 1.1024e+00 (1.0988e+00)	Acc@1  70.00 ( 71.59)	Acc@5  89.00 ( 92.07)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 1.2259e+00 (1.0914e+00)	Acc@1  70.00 ( 71.70)	Acc@5  93.00 ( 92.12)
Test: [ 90/100]	Time  0.079 ( 0.079)	Loss 1.2596e+00 (1.0796e+00)	Acc@1  64.00 ( 71.91)	Acc@5  89.00 ( 92.20)
 * Acc@1 71.920 Acc@5 92.240
### epoch[30] execution time: 102.76082038879395
EPOCH 31
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [31][  0/391]	Time  0.399 ( 0.399)	Data  0.157 ( 0.157)	Loss 3.7952e-01 (3.7952e-01)	Acc@1  87.50 ( 87.50)	Acc@5 100.00 (100.00)
Epoch: [31][ 10/391]	Time  0.240 ( 0.255)	Data  0.001 ( 0.015)	Loss 3.9371e-01 (3.3085e-01)	Acc@1  86.72 ( 89.35)	Acc@5  99.22 ( 99.64)
Epoch: [31][ 20/391]	Time  0.241 ( 0.248)	Data  0.001 ( 0.008)	Loss 2.4546e-01 (3.0602e-01)	Acc@1  94.53 ( 90.40)	Acc@5  99.22 ( 99.63)
Epoch: [31][ 30/391]	Time  0.241 ( 0.247)	Data  0.001 ( 0.006)	Loss 2.9334e-01 (3.0360e-01)	Acc@1  90.62 ( 90.62)	Acc@5  98.44 ( 99.52)
Epoch: [31][ 40/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.005)	Loss 1.9673e-01 (3.0699e-01)	Acc@1  96.09 ( 90.66)	Acc@5 100.00 ( 99.54)
Epoch: [31][ 50/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 3.3486e-01 (3.1117e-01)	Acc@1  90.62 ( 90.70)	Acc@5  99.22 ( 99.48)
Epoch: [31][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 2.7397e-01 (3.0846e-01)	Acc@1  91.41 ( 90.78)	Acc@5 100.00 ( 99.50)
Epoch: [31][ 70/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 2.9355e-01 (3.0913e-01)	Acc@1  91.41 ( 90.80)	Acc@5  99.22 ( 99.46)
Epoch: [31][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.6554e-01 (3.0762e-01)	Acc@1  88.28 ( 90.91)	Acc@5  99.22 ( 99.44)
Epoch: [31][ 90/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.8894e-01 (3.0935e-01)	Acc@1  92.97 ( 90.94)	Acc@5 100.00 ( 99.42)
Epoch: [31][100/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.0390e-01 (3.0851e-01)	Acc@1  92.19 ( 90.97)	Acc@5 100.00 ( 99.42)
Epoch: [31][110/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.5863e-01 (3.1009e-01)	Acc@1  87.50 ( 90.90)	Acc@5  98.44 ( 99.38)
Epoch: [31][120/391]	Time  0.246 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.8103e-01 (3.0905e-01)	Acc@1  91.41 ( 90.97)	Acc@5 100.00 ( 99.40)
Epoch: [31][130/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.9232e-01 (3.0808e-01)	Acc@1  90.62 ( 90.98)	Acc@5  98.44 ( 99.40)
Epoch: [31][140/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.3933e-01 (3.0642e-01)	Acc@1  88.28 ( 91.00)	Acc@5  99.22 ( 99.37)
Epoch: [31][150/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.7652e-01 (3.0403e-01)	Acc@1  91.41 ( 91.05)	Acc@5 100.00 ( 99.39)
Epoch: [31][160/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.7760e-01 (3.0242e-01)	Acc@1  94.53 ( 91.14)	Acc@5  99.22 ( 99.40)
Epoch: [31][170/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.2427e-01 (3.0224e-01)	Acc@1  89.06 ( 91.16)	Acc@5  99.22 ( 99.37)
Epoch: [31][180/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.4803e-01 (3.0151e-01)	Acc@1  90.62 ( 91.22)	Acc@5 100.00 ( 99.39)
Epoch: [31][190/391]	Time  0.246 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.7874e-01 (3.0056e-01)	Acc@1  92.97 ( 91.28)	Acc@5 100.00 ( 99.39)
Epoch: [31][200/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8971e-01 (3.0007e-01)	Acc@1  91.41 ( 91.29)	Acc@5  98.44 ( 99.39)
Epoch: [31][210/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.7198e-01 (3.0032e-01)	Acc@1  92.19 ( 91.22)	Acc@5  99.22 ( 99.38)
Epoch: [31][220/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.5809e-01 (2.9990e-01)	Acc@1  92.19 ( 91.22)	Acc@5 100.00 ( 99.38)
Epoch: [31][230/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9282e-01 (2.9913e-01)	Acc@1  96.09 ( 91.23)	Acc@5 100.00 ( 99.38)
Epoch: [31][240/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.5765e-01 (2.9949e-01)	Acc@1  92.19 ( 91.21)	Acc@5 100.00 ( 99.37)
Epoch: [31][250/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2598e-01 (2.9919e-01)	Acc@1  95.31 ( 91.20)	Acc@5  99.22 ( 99.37)
Epoch: [31][260/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1209e-01 (2.9869e-01)	Acc@1  94.53 ( 91.23)	Acc@5  99.22 ( 99.38)
Epoch: [31][270/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2261e-01 (2.9772e-01)	Acc@1  92.97 ( 91.26)	Acc@5 100.00 ( 99.38)
Epoch: [31][280/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.5703e-01 (2.9695e-01)	Acc@1  87.50 ( 91.30)	Acc@5 100.00 ( 99.39)
Epoch: [31][290/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.3091e-01 (2.9658e-01)	Acc@1  90.62 ( 91.32)	Acc@5 100.00 ( 99.39)
Epoch: [31][300/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0662e-01 (2.9600e-01)	Acc@1  92.97 ( 91.33)	Acc@5 100.00 ( 99.40)
Epoch: [31][310/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.6874e-01 (2.9562e-01)	Acc@1  90.62 ( 91.38)	Acc@5  99.22 ( 99.39)
Epoch: [31][320/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3372e-01 (2.9521e-01)	Acc@1  92.19 ( 91.38)	Acc@5 100.00 ( 99.39)
Epoch: [31][330/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1287e-01 (2.9553e-01)	Acc@1  93.75 ( 91.39)	Acc@5 100.00 ( 99.38)
Epoch: [31][340/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2598e-01 (2.9487e-01)	Acc@1  91.41 ( 91.40)	Acc@5  99.22 ( 99.38)
Epoch: [31][350/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3104e-01 (2.9381e-01)	Acc@1  94.53 ( 91.46)	Acc@5 100.00 ( 99.39)
Epoch: [31][360/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.7783e-01 (2.9315e-01)	Acc@1  92.19 ( 91.48)	Acc@5 100.00 ( 99.40)
Epoch: [31][370/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3015e-01 (2.9277e-01)	Acc@1  91.41 ( 91.48)	Acc@5 100.00 ( 99.40)
Epoch: [31][380/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4177e-01 (2.9142e-01)	Acc@1  92.19 ( 91.52)	Acc@5 100.00 ( 99.41)
Epoch: [31][390/391]	Time  0.177 ( 0.242)	Data  0.001 ( 0.001)	Loss 4.1531e-01 (2.9088e-01)	Acc@1  87.50 ( 91.54)	Acc@5  98.75 ( 99.41)
## e[31] optimizer.zero_grad (sum) time: 0.49738001823425293
## e[31]       loss.backward (sum) time: 11.104138135910034
## e[31]      optimizer.step (sum) time: 47.858200788497925
## epoch[31] training(only) time: 94.76472449302673
# Switched to evaluate mode...
Test: [  0/100]	Time  0.210 ( 0.210)	Loss 9.2371e-01 (9.2371e-01)	Acc@1  78.00 ( 78.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.1307e+00 (1.0883e+00)	Acc@1  73.00 ( 72.55)	Acc@5  93.00 ( 91.55)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 1.0368e+00 (1.0556e+00)	Acc@1  73.00 ( 73.19)	Acc@5  94.00 ( 92.52)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 1.4223e+00 (1.1012e+00)	Acc@1  63.00 ( 72.00)	Acc@5  93.00 ( 92.13)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.1303e+00 (1.1024e+00)	Acc@1  73.00 ( 71.73)	Acc@5  93.00 ( 92.34)
Test: [ 50/100]	Time  0.078 ( 0.080)	Loss 1.3514e+00 (1.1225e+00)	Acc@1  67.00 ( 71.45)	Acc@5  93.00 ( 92.14)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.2157e+00 (1.1042e+00)	Acc@1  68.00 ( 71.62)	Acc@5  92.00 ( 92.44)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 1.0823e+00 (1.1034e+00)	Acc@1  66.00 ( 71.56)	Acc@5  92.00 ( 92.34)
Test: [ 80/100]	Time  0.078 ( 0.079)	Loss 1.1413e+00 (1.0953e+00)	Acc@1  70.00 ( 71.83)	Acc@5  92.00 ( 92.35)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.2527e+00 (1.0816e+00)	Acc@1  65.00 ( 72.10)	Acc@5  91.00 ( 92.45)
 * Acc@1 72.220 Acc@5 92.510
### epoch[31] execution time: 102.7648389339447
EPOCH 32
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [32][  0/391]	Time  0.396 ( 0.396)	Data  0.154 ( 0.154)	Loss 2.5820e-01 (2.5820e-01)	Acc@1  92.19 ( 92.19)	Acc@5 100.00 (100.00)
Epoch: [32][ 10/391]	Time  0.241 ( 0.255)	Data  0.001 ( 0.015)	Loss 2.1517e-01 (2.7001e-01)	Acc@1  93.75 ( 91.83)	Acc@5 100.00 ( 99.79)
Epoch: [32][ 20/391]	Time  0.241 ( 0.248)	Data  0.001 ( 0.008)	Loss 2.4159e-01 (2.6218e-01)	Acc@1  94.53 ( 92.19)	Acc@5 100.00 ( 99.67)
Epoch: [32][ 30/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.006)	Loss 2.4398e-01 (2.5804e-01)	Acc@1  95.31 ( 92.26)	Acc@5  98.44 ( 99.55)
Epoch: [32][ 40/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.005)	Loss 2.6323e-01 (2.5797e-01)	Acc@1  91.41 ( 92.44)	Acc@5  99.22 ( 99.50)
Epoch: [32][ 50/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.004)	Loss 3.2941e-01 (2.5866e-01)	Acc@1  90.62 ( 92.52)	Acc@5  99.22 ( 99.53)
Epoch: [32][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 2.3068e-01 (2.6090e-01)	Acc@1  93.75 ( 92.34)	Acc@5 100.00 ( 99.56)
Epoch: [32][ 70/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.4712e-01 (2.6282e-01)	Acc@1  92.19 ( 92.19)	Acc@5 100.00 ( 99.58)
Epoch: [32][ 80/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.7672e-01 (2.6711e-01)	Acc@1  94.53 ( 92.12)	Acc@5 100.00 ( 99.59)
Epoch: [32][ 90/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.5765e-01 (2.6517e-01)	Acc@1  92.97 ( 92.21)	Acc@5 100.00 ( 99.62)
Epoch: [32][100/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.2245e-01 (2.6202e-01)	Acc@1  90.62 ( 92.36)	Acc@5  99.22 ( 99.60)
Epoch: [32][110/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.6209e-01 (2.6141e-01)	Acc@1  89.84 ( 92.35)	Acc@5  99.22 ( 99.56)
Epoch: [32][120/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.4451e-01 (2.5950e-01)	Acc@1  93.75 ( 92.41)	Acc@5 100.00 ( 99.57)
Epoch: [32][130/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.4717e-01 (2.5964e-01)	Acc@1  94.53 ( 92.42)	Acc@5 100.00 ( 99.58)
Epoch: [32][140/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.3081e-01 (2.5807e-01)	Acc@1  93.75 ( 92.46)	Acc@5 100.00 ( 99.59)
Epoch: [32][150/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.8803e-01 (2.5734e-01)	Acc@1  92.97 ( 92.50)	Acc@5  98.44 ( 99.58)
Epoch: [32][160/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2646e-01 (2.5715e-01)	Acc@1  92.97 ( 92.48)	Acc@5 100.00 ( 99.56)
Epoch: [32][170/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3353e-01 (2.5714e-01)	Acc@1  92.19 ( 92.47)	Acc@5 100.00 ( 99.57)
Epoch: [32][180/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.0101e-01 (2.5639e-01)	Acc@1  90.62 ( 92.49)	Acc@5  99.22 ( 99.57)
Epoch: [32][190/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2086e-01 (2.5639e-01)	Acc@1  92.97 ( 92.49)	Acc@5 100.00 ( 99.57)
Epoch: [32][200/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3922e-01 (2.5564e-01)	Acc@1  95.31 ( 92.53)	Acc@5 100.00 ( 99.57)
Epoch: [32][210/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8349e-01 (2.5449e-01)	Acc@1  92.97 ( 92.55)	Acc@5 100.00 ( 99.58)
Epoch: [32][220/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6341e-01 (2.5323e-01)	Acc@1  93.75 ( 92.56)	Acc@5 100.00 ( 99.59)
Epoch: [32][230/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.4294e-01 (2.5346e-01)	Acc@1  96.09 ( 92.55)	Acc@5 100.00 ( 99.58)
Epoch: [32][240/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0189e-01 (2.5312e-01)	Acc@1  93.75 ( 92.56)	Acc@5 100.00 ( 99.59)
Epoch: [32][250/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6950e-01 (2.5397e-01)	Acc@1  92.19 ( 92.53)	Acc@5  99.22 ( 99.59)
Epoch: [32][260/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3257e-01 (2.5374e-01)	Acc@1  95.31 ( 92.54)	Acc@5 100.00 ( 99.58)
Epoch: [32][270/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.3631e-01 (2.5353e-01)	Acc@1  90.62 ( 92.55)	Acc@5  98.44 ( 99.58)
Epoch: [32][280/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.0474e-01 (2.5290e-01)	Acc@1  91.41 ( 92.59)	Acc@5 100.00 ( 99.58)
Epoch: [32][290/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9149e-01 (2.5185e-01)	Acc@1  92.19 ( 92.60)	Acc@5 100.00 ( 99.58)
Epoch: [32][300/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6151e-01 (2.5151e-01)	Acc@1  92.97 ( 92.60)	Acc@5 100.00 ( 99.58)
Epoch: [32][310/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.5784e-01 (2.5099e-01)	Acc@1  91.41 ( 92.62)	Acc@5  99.22 ( 99.58)
Epoch: [32][320/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9586e-01 (2.5002e-01)	Acc@1  92.97 ( 92.65)	Acc@5 100.00 ( 99.58)
Epoch: [32][330/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2278e-01 (2.4960e-01)	Acc@1  92.19 ( 92.66)	Acc@5 100.00 ( 99.59)
Epoch: [32][340/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8390e-01 (2.4893e-01)	Acc@1  96.09 ( 92.70)	Acc@5 100.00 ( 99.59)
Epoch: [32][350/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.4441e-01 (2.4927e-01)	Acc@1  85.16 ( 92.66)	Acc@5  99.22 ( 99.58)
Epoch: [32][360/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6895e-01 (2.4878e-01)	Acc@1  92.19 ( 92.67)	Acc@5 100.00 ( 99.58)
Epoch: [32][370/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2023e-01 (2.4839e-01)	Acc@1  92.19 ( 92.69)	Acc@5 100.00 ( 99.58)
Epoch: [32][380/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8863e-01 (2.4778e-01)	Acc@1  95.31 ( 92.73)	Acc@5  99.22 ( 99.58)
Epoch: [32][390/391]	Time  0.178 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.7980e-01 (2.4768e-01)	Acc@1  91.25 ( 92.73)	Acc@5  98.75 ( 99.59)
## e[32] optimizer.zero_grad (sum) time: 0.4953742027282715
## e[32]       loss.backward (sum) time: 11.113349199295044
## e[32]      optimizer.step (sum) time: 47.877543687820435
## epoch[32] training(only) time: 94.81587791442871
# Switched to evaluate mode...
Test: [  0/100]	Time  0.208 ( 0.208)	Loss 9.8813e-01 (9.8813e-01)	Acc@1  76.00 ( 76.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.1431e+00 (1.0948e+00)	Acc@1  70.00 ( 73.36)	Acc@5  92.00 ( 91.73)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 1.0298e+00 (1.0557e+00)	Acc@1  74.00 ( 73.90)	Acc@5  92.00 ( 92.43)
Test: [ 30/100]	Time  0.077 ( 0.082)	Loss 1.4659e+00 (1.1018e+00)	Acc@1  59.00 ( 72.84)	Acc@5  93.00 ( 92.00)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.1557e+00 (1.1084e+00)	Acc@1  78.00 ( 72.49)	Acc@5  93.00 ( 92.24)
Test: [ 50/100]	Time  0.078 ( 0.080)	Loss 1.3416e+00 (1.1273e+00)	Acc@1  69.00 ( 72.02)	Acc@5  92.00 ( 92.08)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.2661e+00 (1.1075e+00)	Acc@1  70.00 ( 72.20)	Acc@5  93.00 ( 92.54)
Test: [ 70/100]	Time  0.077 ( 0.080)	Loss 1.1209e+00 (1.1072e+00)	Acc@1  67.00 ( 72.17)	Acc@5  96.00 ( 92.52)
Test: [ 80/100]	Time  0.078 ( 0.079)	Loss 1.1353e+00 (1.0994e+00)	Acc@1  74.00 ( 72.35)	Acc@5  93.00 ( 92.54)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.3511e+00 (1.0870e+00)	Acc@1  64.00 ( 72.49)	Acc@5  90.00 ( 92.63)
 * Acc@1 72.640 Acc@5 92.680
### epoch[32] execution time: 102.8158848285675
EPOCH 33
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [33][  0/391]	Time  0.402 ( 0.402)	Data  0.160 ( 0.160)	Loss 2.4617e-01 (2.4617e-01)	Acc@1  90.62 ( 90.62)	Acc@5  99.22 ( 99.22)
Epoch: [33][ 10/391]	Time  0.241 ( 0.255)	Data  0.001 ( 0.015)	Loss 1.7923e-01 (2.5200e-01)	Acc@1  96.09 ( 91.83)	Acc@5 100.00 ( 99.64)
Epoch: [33][ 20/391]	Time  0.241 ( 0.249)	Data  0.001 ( 0.009)	Loss 2.0797e-01 (2.4351e-01)	Acc@1  93.75 ( 92.37)	Acc@5 100.00 ( 99.81)
Epoch: [33][ 30/391]	Time  0.247 ( 0.246)	Data  0.001 ( 0.006)	Loss 3.0023e-01 (2.3843e-01)	Acc@1  88.28 ( 92.64)	Acc@5 100.00 ( 99.75)
Epoch: [33][ 40/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.005)	Loss 1.9603e-01 (2.3332e-01)	Acc@1  94.53 ( 92.93)	Acc@5 100.00 ( 99.73)
Epoch: [33][ 50/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.7758e-01 (2.3176e-01)	Acc@1  96.09 ( 92.97)	Acc@5 100.00 ( 99.71)
Epoch: [33][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.6886e-01 (2.2457e-01)	Acc@1  96.09 ( 93.34)	Acc@5 100.00 ( 99.68)
Epoch: [33][ 70/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.003)	Loss 2.4346e-01 (2.2340e-01)	Acc@1  92.97 ( 93.36)	Acc@5 100.00 ( 99.69)
Epoch: [33][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.2432e-01 (2.2060e-01)	Acc@1  92.97 ( 93.47)	Acc@5 100.00 ( 99.71)
Epoch: [33][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.8445e-01 (2.2001e-01)	Acc@1  91.41 ( 93.59)	Acc@5 100.00 ( 99.70)
Epoch: [33][100/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.6519e-01 (2.2135e-01)	Acc@1  96.88 ( 93.52)	Acc@5 100.00 ( 99.72)
Epoch: [33][110/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.7500e-01 (2.1950e-01)	Acc@1  95.31 ( 93.59)	Acc@5 100.00 ( 99.72)
Epoch: [33][120/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.9213e-01 (2.1909e-01)	Acc@1  88.28 ( 93.56)	Acc@5 100.00 ( 99.74)
Epoch: [33][130/391]	Time  0.246 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.4715e-01 (2.1804e-01)	Acc@1  95.31 ( 93.64)	Acc@5  99.22 ( 99.73)
Epoch: [33][140/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.7433e-01 (2.2094e-01)	Acc@1  96.88 ( 93.57)	Acc@5 100.00 ( 99.68)
Epoch: [33][150/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.2111e-01 (2.2271e-01)	Acc@1  93.75 ( 93.51)	Acc@5  99.22 ( 99.68)
Epoch: [33][160/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.7630e-01 (2.2174e-01)	Acc@1  93.75 ( 93.54)	Acc@5 100.00 ( 99.69)
Epoch: [33][170/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.4361e-01 (2.2048e-01)	Acc@1  96.88 ( 93.58)	Acc@5 100.00 ( 99.69)
Epoch: [33][180/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.2740e-01 (2.1908e-01)	Acc@1  94.53 ( 93.62)	Acc@5 100.00 ( 99.70)
Epoch: [33][190/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.4851e-01 (2.1920e-01)	Acc@1  92.97 ( 93.62)	Acc@5  98.44 ( 99.69)
Epoch: [33][200/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.4257e-01 (2.1923e-01)	Acc@1  96.09 ( 93.63)	Acc@5 100.00 ( 99.69)
Epoch: [33][210/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.9675e-01 (2.2022e-01)	Acc@1  96.09 ( 93.62)	Acc@5 100.00 ( 99.69)
Epoch: [33][220/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.2955e-01 (2.2140e-01)	Acc@1  92.97 ( 93.58)	Acc@5 100.00 ( 99.69)
Epoch: [33][230/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.8348e-01 (2.2133e-01)	Acc@1  94.53 ( 93.60)	Acc@5 100.00 ( 99.69)
Epoch: [33][240/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.6862e-01 (2.2060e-01)	Acc@1  94.53 ( 93.63)	Acc@5 100.00 ( 99.70)
Epoch: [33][250/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.2572e-01 (2.1886e-01)	Acc@1  92.97 ( 93.70)	Acc@5 100.00 ( 99.70)
Epoch: [33][260/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.1398e-01 (2.1979e-01)	Acc@1  90.62 ( 93.66)	Acc@5  99.22 ( 99.70)
Epoch: [33][270/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.8357e-01 (2.1935e-01)	Acc@1  94.53 ( 93.70)	Acc@5 100.00 ( 99.70)
Epoch: [33][280/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.6445e-01 (2.1892e-01)	Acc@1  92.97 ( 93.70)	Acc@5 100.00 ( 99.69)
Epoch: [33][290/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.8342e-01 (2.1868e-01)	Acc@1  98.44 ( 93.71)	Acc@5 100.00 ( 99.70)
Epoch: [33][300/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.2991e-01 (2.1817e-01)	Acc@1  92.19 ( 93.71)	Acc@5  98.44 ( 99.70)
Epoch: [33][310/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.6273e-01 (2.1739e-01)	Acc@1  93.75 ( 93.72)	Acc@5 100.00 ( 99.70)
Epoch: [33][320/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.9001e-01 (2.1774e-01)	Acc@1  90.62 ( 93.71)	Acc@5  99.22 ( 99.70)
Epoch: [33][330/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.3867e-01 (2.1746e-01)	Acc@1  92.97 ( 93.71)	Acc@5 100.00 ( 99.70)
Epoch: [33][340/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.6577e-01 (2.1870e-01)	Acc@1  92.97 ( 93.68)	Acc@5 100.00 ( 99.70)
Epoch: [33][350/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6845e-01 (2.1831e-01)	Acc@1  95.31 ( 93.69)	Acc@5 100.00 ( 99.70)
Epoch: [33][360/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.9099e-01 (2.1837e-01)	Acc@1  92.19 ( 93.69)	Acc@5 100.00 ( 99.70)
Epoch: [33][370/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.5143e-01 (2.1870e-01)	Acc@1  92.97 ( 93.67)	Acc@5  98.44 ( 99.69)
Epoch: [33][380/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.5032e-02 (2.1880e-01)	Acc@1  98.44 ( 93.68)	Acc@5 100.00 ( 99.70)
Epoch: [33][390/391]	Time  0.178 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3222e-01 (2.1846e-01)	Acc@1  95.00 ( 93.70)	Acc@5  98.75 ( 99.69)
## e[33] optimizer.zero_grad (sum) time: 0.4919602870941162
## e[33]       loss.backward (sum) time: 11.128281593322754
## e[33]      optimizer.step (sum) time: 47.865132093429565
## epoch[33] training(only) time: 94.82843923568726
# Switched to evaluate mode...
Test: [  0/100]	Time  0.209 ( 0.209)	Loss 9.9191e-01 (9.9191e-01)	Acc@1  76.00 ( 76.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.1427e+00 (1.1070e+00)	Acc@1  69.00 ( 72.55)	Acc@5  93.00 ( 91.73)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 1.0527e+00 (1.0695e+00)	Acc@1  74.00 ( 73.10)	Acc@5  94.00 ( 92.62)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 1.4618e+00 (1.1126e+00)	Acc@1  63.00 ( 72.29)	Acc@5  91.00 ( 92.19)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.1641e+00 (1.1209e+00)	Acc@1  76.00 ( 71.90)	Acc@5  94.00 ( 92.41)
Test: [ 50/100]	Time  0.077 ( 0.080)	Loss 1.3597e+00 (1.1434e+00)	Acc@1  70.00 ( 71.73)	Acc@5  92.00 ( 92.24)
Test: [ 60/100]	Time  0.077 ( 0.080)	Loss 1.2661e+00 (1.1209e+00)	Acc@1  70.00 ( 71.90)	Acc@5  92.00 ( 92.69)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 1.0875e+00 (1.1167e+00)	Acc@1  68.00 ( 71.90)	Acc@5  89.00 ( 92.54)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 1.1733e+00 (1.1097e+00)	Acc@1  70.00 ( 72.15)	Acc@5  93.00 ( 92.57)
Test: [ 90/100]	Time  0.077 ( 0.079)	Loss 1.4533e+00 (1.0969e+00)	Acc@1  61.00 ( 72.36)	Acc@5  90.00 ( 92.62)
 * Acc@1 72.490 Acc@5 92.690
### epoch[33] execution time: 102.84609985351562
EPOCH 34
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [34][  0/391]	Time  0.390 ( 0.390)	Data  0.148 ( 0.148)	Loss 1.9271e-01 (1.9271e-01)	Acc@1  95.31 ( 95.31)	Acc@5  99.22 ( 99.22)
Epoch: [34][ 10/391]	Time  0.248 ( 0.255)	Data  0.001 ( 0.014)	Loss 2.4655e-01 (1.9724e-01)	Acc@1  92.97 ( 94.82)	Acc@5 100.00 ( 99.50)
Epoch: [34][ 20/391]	Time  0.241 ( 0.248)	Data  0.001 ( 0.008)	Loss 1.8466e-01 (1.8780e-01)	Acc@1  96.88 ( 94.90)	Acc@5 100.00 ( 99.67)
Epoch: [34][ 30/391]	Time  0.242 ( 0.246)	Data  0.001 ( 0.006)	Loss 1.6426e-01 (1.8688e-01)	Acc@1  95.31 ( 95.01)	Acc@5  99.22 ( 99.62)
Epoch: [34][ 40/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.005)	Loss 2.1034e-01 (1.9179e-01)	Acc@1  93.75 ( 94.80)	Acc@5  99.22 ( 99.60)
Epoch: [34][ 50/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.9111e-01 (1.8859e-01)	Acc@1  92.97 ( 94.85)	Acc@5 100.00 ( 99.65)
Epoch: [34][ 60/391]	Time  0.244 ( 0.244)	Data  0.001 ( 0.004)	Loss 2.2581e-01 (1.9138e-01)	Acc@1  92.97 ( 94.72)	Acc@5  98.44 ( 99.60)
Epoch: [34][ 70/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.6936e-01 (1.9206e-01)	Acc@1  92.19 ( 94.62)	Acc@5 100.00 ( 99.64)
Epoch: [34][ 80/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.4719e-01 (1.8979e-01)	Acc@1  95.31 ( 94.71)	Acc@5 100.00 ( 99.64)
Epoch: [34][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.6701e-01 (1.8940e-01)	Acc@1  93.75 ( 94.75)	Acc@5 100.00 ( 99.67)
Epoch: [34][100/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.2578e-01 (1.8847e-01)	Acc@1  98.44 ( 94.79)	Acc@5 100.00 ( 99.69)
Epoch: [34][110/391]	Time  0.247 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.9721e-01 (1.8971e-01)	Acc@1  92.97 ( 94.73)	Acc@5 100.00 ( 99.71)
Epoch: [34][120/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.2153e-01 (1.9135e-01)	Acc@1  96.88 ( 94.69)	Acc@5 100.00 ( 99.71)
Epoch: [34][130/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.1873e-01 (1.9147e-01)	Acc@1  93.75 ( 94.70)	Acc@5  99.22 ( 99.71)
Epoch: [34][140/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.9704e-01 (1.9208e-01)	Acc@1  93.75 ( 94.66)	Acc@5 100.00 ( 99.70)
Epoch: [34][150/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.5884e-01 (1.9177e-01)	Acc@1  96.09 ( 94.67)	Acc@5 100.00 ( 99.70)
Epoch: [34][160/391]	Time  0.246 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.5639e-01 (1.9138e-01)	Acc@1  96.09 ( 94.66)	Acc@5 100.00 ( 99.70)
Epoch: [34][170/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.1077e-01 (1.9256e-01)	Acc@1  89.84 ( 94.63)	Acc@5  99.22 ( 99.70)
Epoch: [34][180/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.6512e-01 (1.9253e-01)	Acc@1  96.88 ( 94.62)	Acc@5 100.00 ( 99.70)
Epoch: [34][190/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8053e-01 (1.9294e-01)	Acc@1  94.53 ( 94.58)	Acc@5  98.44 ( 99.69)
Epoch: [34][200/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.9591e-01 (1.9390e-01)	Acc@1  94.53 ( 94.54)	Acc@5  98.44 ( 99.68)
Epoch: [34][210/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7252e-01 (1.9334e-01)	Acc@1  93.75 ( 94.56)	Acc@5 100.00 ( 99.68)
Epoch: [34][220/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8144e-01 (1.9241e-01)	Acc@1  93.75 ( 94.56)	Acc@5 100.00 ( 99.69)
Epoch: [34][230/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2902e-01 (1.9267e-01)	Acc@1  92.19 ( 94.53)	Acc@5 100.00 ( 99.70)
Epoch: [34][240/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0406e-01 (1.9293e-01)	Acc@1  92.97 ( 94.54)	Acc@5 100.00 ( 99.71)
Epoch: [34][250/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.5085e-01 (1.9313e-01)	Acc@1  91.41 ( 94.50)	Acc@5  99.22 ( 99.71)
Epoch: [34][260/391]	Time  0.247 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8199e-01 (1.9344e-01)	Acc@1  92.97 ( 94.45)	Acc@5 100.00 ( 99.72)
Epoch: [34][270/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3828e-01 (1.9328e-01)	Acc@1  96.09 ( 94.45)	Acc@5 100.00 ( 99.72)
Epoch: [34][280/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8963e-01 (1.9404e-01)	Acc@1  94.53 ( 94.41)	Acc@5 100.00 ( 99.72)
Epoch: [34][290/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8831e-01 (1.9445e-01)	Acc@1  96.09 ( 94.38)	Acc@5  99.22 ( 99.71)
Epoch: [34][300/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8661e-01 (1.9424e-01)	Acc@1  93.75 ( 94.38)	Acc@5 100.00 ( 99.72)
Epoch: [34][310/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8058e-01 (1.9445e-01)	Acc@1  93.75 ( 94.37)	Acc@5 100.00 ( 99.71)
Epoch: [34][320/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9645e-01 (1.9374e-01)	Acc@1  94.53 ( 94.38)	Acc@5 100.00 ( 99.72)
Epoch: [34][330/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2265e-01 (1.9332e-01)	Acc@1  92.97 ( 94.40)	Acc@5  99.22 ( 99.72)
Epoch: [34][340/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7799e-01 (1.9325e-01)	Acc@1  96.09 ( 94.41)	Acc@5 100.00 ( 99.72)
Epoch: [34][350/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.7847e-01 (1.9370e-01)	Acc@1  94.53 ( 94.40)	Acc@5  98.44 ( 99.71)
Epoch: [34][360/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3237e-01 (1.9294e-01)	Acc@1  96.88 ( 94.43)	Acc@5 100.00 ( 99.72)
Epoch: [34][370/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3529e-01 (1.9293e-01)	Acc@1  96.88 ( 94.43)	Acc@5 100.00 ( 99.72)
Epoch: [34][380/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1569e-01 (1.9296e-01)	Acc@1  96.88 ( 94.43)	Acc@5 100.00 ( 99.72)
Epoch: [34][390/391]	Time  0.179 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1912e-01 (1.9310e-01)	Acc@1  92.50 ( 94.42)	Acc@5 100.00 ( 99.71)
## e[34] optimizer.zero_grad (sum) time: 0.49466919898986816
## e[34]       loss.backward (sum) time: 11.145184755325317
## e[34]      optimizer.step (sum) time: 47.84838032722473
## epoch[34] training(only) time: 94.81413674354553
# Switched to evaluate mode...
Test: [  0/100]	Time  0.205 ( 0.205)	Loss 1.0078e+00 (1.0078e+00)	Acc@1  77.00 ( 77.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.078 ( 0.089)	Loss 1.1340e+00 (1.1366e+00)	Acc@1  71.00 ( 73.00)	Acc@5  93.00 ( 91.91)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 1.0743e+00 (1.0918e+00)	Acc@1  72.00 ( 73.48)	Acc@5  93.00 ( 92.67)
Test: [ 30/100]	Time  0.077 ( 0.082)	Loss 1.3871e+00 (1.1332e+00)	Acc@1  63.00 ( 72.29)	Acc@5  93.00 ( 92.06)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.1515e+00 (1.1362e+00)	Acc@1  77.00 ( 72.10)	Acc@5  95.00 ( 92.59)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 1.4102e+00 (1.1558e+00)	Acc@1  68.00 ( 71.94)	Acc@5  91.00 ( 92.33)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.2876e+00 (1.1323e+00)	Acc@1  71.00 ( 72.34)	Acc@5  92.00 ( 92.69)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 1.1169e+00 (1.1320e+00)	Acc@1  68.00 ( 72.28)	Acc@5  93.00 ( 92.55)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 1.1435e+00 (1.1254e+00)	Acc@1  74.00 ( 72.44)	Acc@5  93.00 ( 92.63)
Test: [ 90/100]	Time  0.079 ( 0.079)	Loss 1.4366e+00 (1.1117e+00)	Acc@1  64.00 ( 72.69)	Acc@5  88.00 ( 92.63)
 * Acc@1 72.810 Acc@5 92.700
### epoch[34] execution time: 102.83315515518188
EPOCH 35
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [35][  0/391]	Time  0.402 ( 0.402)	Data  0.160 ( 0.160)	Loss 1.2342e-01 (1.2342e-01)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [35][ 10/391]	Time  0.240 ( 0.256)	Data  0.001 ( 0.015)	Loss 2.0410e-01 (1.6910e-01)	Acc@1  93.75 ( 95.17)	Acc@5 100.00 ( 99.86)
Epoch: [35][ 20/391]	Time  0.241 ( 0.249)	Data  0.001 ( 0.009)	Loss 2.0261e-01 (1.8281e-01)	Acc@1  92.97 ( 94.61)	Acc@5 100.00 ( 99.85)
Epoch: [35][ 30/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.006)	Loss 1.0932e-01 (1.8330e-01)	Acc@1  97.66 ( 94.91)	Acc@5 100.00 ( 99.75)
Epoch: [35][ 40/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.005)	Loss 1.6569e-01 (1.7817e-01)	Acc@1  96.09 ( 95.20)	Acc@5 100.00 ( 99.77)
Epoch: [35][ 50/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.004)	Loss 3.1510e-01 (1.7997e-01)	Acc@1  89.84 ( 94.94)	Acc@5  99.22 ( 99.79)
Epoch: [35][ 60/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.9529e-01 (1.7658e-01)	Acc@1  95.31 ( 95.08)	Acc@5 100.00 ( 99.80)
Epoch: [35][ 70/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 2.1553e-01 (1.7579e-01)	Acc@1  92.97 ( 95.09)	Acc@5  99.22 ( 99.80)
Epoch: [35][ 80/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.5247e-01 (1.7544e-01)	Acc@1  96.88 ( 95.10)	Acc@5 100.00 ( 99.82)
Epoch: [35][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.8350e-01 (1.7420e-01)	Acc@1  93.75 ( 95.22)	Acc@5 100.00 ( 99.84)
Epoch: [35][100/391]	Time  0.239 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.2637e-01 (1.7216e-01)	Acc@1  97.66 ( 95.30)	Acc@5  99.22 ( 99.84)
Epoch: [35][110/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.4078e-01 (1.7096e-01)	Acc@1  95.31 ( 95.35)	Acc@5 100.00 ( 99.84)
Epoch: [35][120/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.5435e-01 (1.7219e-01)	Acc@1  96.09 ( 95.29)	Acc@5 100.00 ( 99.81)
Epoch: [35][130/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.6324e-01 (1.7290e-01)	Acc@1  94.53 ( 95.23)	Acc@5 100.00 ( 99.82)
Epoch: [35][140/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1395e-01 (1.7142e-01)	Acc@1  98.44 ( 95.30)	Acc@5 100.00 ( 99.82)
Epoch: [35][150/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.2581e-01 (1.7102e-01)	Acc@1  96.88 ( 95.30)	Acc@5  99.22 ( 99.82)
Epoch: [35][160/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.7557e-01 (1.6963e-01)	Acc@1  93.75 ( 95.32)	Acc@5 100.00 ( 99.84)
Epoch: [35][170/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.8782e-01 (1.7012e-01)	Acc@1  96.88 ( 95.31)	Acc@5  99.22 ( 99.83)
Epoch: [35][180/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8831e-01 (1.7058e-01)	Acc@1  93.75 ( 95.26)	Acc@5 100.00 ( 99.83)
Epoch: [35][190/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8732e-01 (1.7154e-01)	Acc@1  96.88 ( 95.26)	Acc@5  99.22 ( 99.83)
Epoch: [35][200/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2028e-01 (1.7100e-01)	Acc@1  93.75 ( 95.26)	Acc@5 100.00 ( 99.83)
Epoch: [35][210/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5016e-01 (1.7103e-01)	Acc@1  97.66 ( 95.22)	Acc@5 100.00 ( 99.83)
Epoch: [35][220/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3804e-01 (1.7130e-01)	Acc@1  96.09 ( 95.22)	Acc@5 100.00 ( 99.82)
Epoch: [35][230/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8047e-01 (1.7089e-01)	Acc@1  92.97 ( 95.24)	Acc@5 100.00 ( 99.82)
Epoch: [35][240/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2214e-01 (1.7083e-01)	Acc@1  92.97 ( 95.23)	Acc@5 100.00 ( 99.82)
Epoch: [35][250/391]	Time  0.254 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.4026e-01 (1.6979e-01)	Acc@1  96.09 ( 95.23)	Acc@5 100.00 ( 99.82)
Epoch: [35][260/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8592e-01 (1.6959e-01)	Acc@1  94.53 ( 95.24)	Acc@5  99.22 ( 99.82)
Epoch: [35][270/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.4754e-01 (1.6905e-01)	Acc@1  96.88 ( 95.25)	Acc@5 100.00 ( 99.82)
Epoch: [35][280/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1524e-01 (1.6952e-01)	Acc@1  92.97 ( 95.23)	Acc@5 100.00 ( 99.82)
Epoch: [35][290/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8208e-01 (1.6887e-01)	Acc@1  92.19 ( 95.23)	Acc@5  97.66 ( 99.81)
Epoch: [35][300/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1611e-01 (1.6864e-01)	Acc@1  93.75 ( 95.23)	Acc@5  99.22 ( 99.81)
Epoch: [35][310/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6420e-01 (1.6832e-01)	Acc@1  95.31 ( 95.23)	Acc@5  99.22 ( 99.81)
Epoch: [35][320/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0791e-01 (1.6863e-01)	Acc@1  92.97 ( 95.22)	Acc@5 100.00 ( 99.81)
Epoch: [35][330/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.4168e-01 (1.6966e-01)	Acc@1  96.09 ( 95.19)	Acc@5 100.00 ( 99.80)
Epoch: [35][340/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5730e-01 (1.7014e-01)	Acc@1  94.53 ( 95.16)	Acc@5 100.00 ( 99.80)
Epoch: [35][350/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6528e-01 (1.7042e-01)	Acc@1  94.53 ( 95.15)	Acc@5  99.22 ( 99.80)
Epoch: [35][360/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8550e-01 (1.7128e-01)	Acc@1  92.97 ( 95.11)	Acc@5  99.22 ( 99.80)
Epoch: [35][370/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9061e-01 (1.7201e-01)	Acc@1  93.75 ( 95.10)	Acc@5 100.00 ( 99.80)
Epoch: [35][380/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8517e-01 (1.7163e-01)	Acc@1  94.53 ( 95.12)	Acc@5 100.00 ( 99.80)
Epoch: [35][390/391]	Time  0.178 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.5726e-01 (1.7180e-01)	Acc@1  90.00 ( 95.11)	Acc@5  98.75 ( 99.80)
## e[35] optimizer.zero_grad (sum) time: 0.49375271797180176
## e[35]       loss.backward (sum) time: 11.145015478134155
## e[35]      optimizer.step (sum) time: 47.819480895996094
## epoch[35] training(only) time: 94.759938955307
# Switched to evaluate mode...
Test: [  0/100]	Time  0.207 ( 0.207)	Loss 1.0739e+00 (1.0739e+00)	Acc@1  74.00 ( 74.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.077 ( 0.090)	Loss 1.1454e+00 (1.1557e+00)	Acc@1  71.00 ( 73.45)	Acc@5  94.00 ( 91.73)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 1.0998e+00 (1.1116e+00)	Acc@1  71.00 ( 73.43)	Acc@5  95.00 ( 92.67)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 1.4880e+00 (1.1655e+00)	Acc@1  61.00 ( 72.32)	Acc@5  92.00 ( 92.19)
Test: [ 40/100]	Time  0.079 ( 0.081)	Loss 1.1990e+00 (1.1635e+00)	Acc@1  75.00 ( 72.22)	Acc@5  94.00 ( 92.39)
Test: [ 50/100]	Time  0.077 ( 0.081)	Loss 1.5212e+00 (1.1814e+00)	Acc@1  67.00 ( 72.00)	Acc@5  90.00 ( 92.12)
Test: [ 60/100]	Time  0.077 ( 0.080)	Loss 1.3236e+00 (1.1566e+00)	Acc@1  68.00 ( 72.36)	Acc@5  92.00 ( 92.51)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 1.1502e+00 (1.1544e+00)	Acc@1  68.00 ( 72.31)	Acc@5  91.00 ( 92.32)
Test: [ 80/100]	Time  0.077 ( 0.080)	Loss 1.1885e+00 (1.1483e+00)	Acc@1  71.00 ( 72.35)	Acc@5  91.00 ( 92.33)
Test: [ 90/100]	Time  0.077 ( 0.079)	Loss 1.5172e+00 (1.1336e+00)	Acc@1  64.00 ( 72.53)	Acc@5  91.00 ( 92.45)
 * Acc@1 72.630 Acc@5 92.510
### epoch[35] execution time: 102.77085542678833
EPOCH 36
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [36][  0/391]	Time  0.386 ( 0.386)	Data  0.136 ( 0.136)	Loss 1.9038e-01 (1.9038e-01)	Acc@1  93.75 ( 93.75)	Acc@5  99.22 ( 99.22)
Epoch: [36][ 10/391]	Time  0.240 ( 0.254)	Data  0.001 ( 0.013)	Loss 1.1378e-01 (1.5153e-01)	Acc@1  98.44 ( 96.09)	Acc@5 100.00 ( 99.86)
Epoch: [36][ 20/391]	Time  0.241 ( 0.248)	Data  0.001 ( 0.008)	Loss 1.4077e-01 (1.5096e-01)	Acc@1  96.09 ( 96.06)	Acc@5 100.00 ( 99.85)
Epoch: [36][ 30/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.006)	Loss 1.6694e-01 (1.5496e-01)	Acc@1  95.31 ( 95.79)	Acc@5 100.00 ( 99.87)
Epoch: [36][ 40/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.004)	Loss 1.6279e-01 (1.5212e-01)	Acc@1  94.53 ( 95.73)	Acc@5 100.00 ( 99.85)
Epoch: [36][ 50/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.5106e-01 (1.5369e-01)	Acc@1  95.31 ( 95.57)	Acc@5 100.00 ( 99.88)
Epoch: [36][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 1.7374e-01 (1.5719e-01)	Acc@1  95.31 ( 95.53)	Acc@5 100.00 ( 99.86)
Epoch: [36][ 70/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.4753e-01 (1.5683e-01)	Acc@1  95.31 ( 95.52)	Acc@5 100.00 ( 99.85)
Epoch: [36][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.7645e-01 (1.5364e-01)	Acc@1  95.31 ( 95.66)	Acc@5  99.22 ( 99.84)
Epoch: [36][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.3440e-01 (1.5113e-01)	Acc@1  95.31 ( 95.83)	Acc@5 100.00 ( 99.85)
Epoch: [36][100/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.003)	Loss 8.9881e-02 (1.5304e-01)	Acc@1  98.44 ( 95.72)	Acc@5 100.00 ( 99.83)
Epoch: [36][110/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.6177e-01 (1.5194e-01)	Acc@1  94.53 ( 95.77)	Acc@5 100.00 ( 99.83)
Epoch: [36][120/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.7501e-01 (1.5221e-01)	Acc@1  94.53 ( 95.72)	Acc@5 100.00 ( 99.83)
Epoch: [36][130/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.8132e-02 (1.5138e-01)	Acc@1  98.44 ( 95.77)	Acc@5 100.00 ( 99.83)
Epoch: [36][140/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8073e-01 (1.5242e-01)	Acc@1  93.75 ( 95.73)	Acc@5 100.00 ( 99.82)
Epoch: [36][150/391]	Time  0.248 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3730e-01 (1.5166e-01)	Acc@1  96.88 ( 95.76)	Acc@5 100.00 ( 99.83)
Epoch: [36][160/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3822e-01 (1.5227e-01)	Acc@1  92.19 ( 95.72)	Acc@5 100.00 ( 99.84)
Epoch: [36][170/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1696e-01 (1.5362e-01)	Acc@1  96.88 ( 95.66)	Acc@5 100.00 ( 99.83)
Epoch: [36][180/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2127e-01 (1.5477e-01)	Acc@1  97.66 ( 95.61)	Acc@5 100.00 ( 99.83)
Epoch: [36][190/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0284e-01 (1.5512e-01)	Acc@1  97.66 ( 95.64)	Acc@5 100.00 ( 99.84)
Epoch: [36][200/391]	Time  0.246 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6875e-01 (1.5514e-01)	Acc@1  93.75 ( 95.63)	Acc@5 100.00 ( 99.84)
Epoch: [36][210/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.2150e-02 (1.5501e-01)	Acc@1  97.66 ( 95.62)	Acc@5 100.00 ( 99.84)
Epoch: [36][220/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3231e-01 (1.5478e-01)	Acc@1  94.53 ( 95.61)	Acc@5 100.00 ( 99.85)
Epoch: [36][230/391]	Time  0.250 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.6021e-02 (1.5521e-01)	Acc@1  99.22 ( 95.62)	Acc@5 100.00 ( 99.84)
Epoch: [36][240/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2688e-01 (1.5502e-01)	Acc@1  96.88 ( 95.65)	Acc@5 100.00 ( 99.84)
Epoch: [36][250/391]	Time  0.247 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3093e-01 (1.5480e-01)	Acc@1  96.88 ( 95.66)	Acc@5 100.00 ( 99.84)
Epoch: [36][260/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5218e-01 (1.5527e-01)	Acc@1  94.53 ( 95.66)	Acc@5 100.00 ( 99.84)
Epoch: [36][270/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7235e-01 (1.5498e-01)	Acc@1  92.19 ( 95.67)	Acc@5 100.00 ( 99.84)
Epoch: [36][280/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9549e-01 (1.5481e-01)	Acc@1  96.88 ( 95.70)	Acc@5  99.22 ( 99.84)
Epoch: [36][290/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1272e-01 (1.5517e-01)	Acc@1  94.53 ( 95.68)	Acc@5 100.00 ( 99.85)
Epoch: [36][300/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3951e-01 (1.5538e-01)	Acc@1  96.09 ( 95.65)	Acc@5 100.00 ( 99.85)
Epoch: [36][310/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1347e-01 (1.5559e-01)	Acc@1  96.88 ( 95.64)	Acc@5 100.00 ( 99.85)
Epoch: [36][320/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6977e-01 (1.5520e-01)	Acc@1  94.53 ( 95.64)	Acc@5 100.00 ( 99.85)
Epoch: [36][330/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0070e-01 (1.5585e-01)	Acc@1  92.97 ( 95.62)	Acc@5 100.00 ( 99.84)
Epoch: [36][340/391]	Time  0.253 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3278e-01 (1.5590e-01)	Acc@1  96.09 ( 95.62)	Acc@5  99.22 ( 99.84)
Epoch: [36][350/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.4040e-02 (1.5548e-01)	Acc@1  99.22 ( 95.63)	Acc@5 100.00 ( 99.84)
Epoch: [36][360/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3642e-01 (1.5609e-01)	Acc@1  96.09 ( 95.63)	Acc@5  99.22 ( 99.84)
Epoch: [36][370/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5164e-01 (1.5554e-01)	Acc@1  96.09 ( 95.65)	Acc@5 100.00 ( 99.84)
Epoch: [36][380/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5692e-01 (1.5581e-01)	Acc@1  97.66 ( 95.64)	Acc@5 100.00 ( 99.84)
Epoch: [36][390/391]	Time  0.177 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7948e-01 (1.5580e-01)	Acc@1  96.25 ( 95.63)	Acc@5 100.00 ( 99.84)
## e[36] optimizer.zero_grad (sum) time: 0.49323201179504395
## e[36]       loss.backward (sum) time: 11.128333806991577
## e[36]      optimizer.step (sum) time: 47.841238260269165
## epoch[36] training(only) time: 94.78743577003479
# Switched to evaluate mode...
Test: [  0/100]	Time  0.206 ( 0.206)	Loss 1.0904e+00 (1.0904e+00)	Acc@1  75.00 ( 75.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.078 ( 0.089)	Loss 1.1936e+00 (1.1579e+00)	Acc@1  68.00 ( 73.73)	Acc@5  92.00 ( 91.45)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 1.0431e+00 (1.1172e+00)	Acc@1  73.00 ( 73.48)	Acc@5  94.00 ( 92.33)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 1.4842e+00 (1.1670e+00)	Acc@1  63.00 ( 72.45)	Acc@5  92.00 ( 92.00)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.2270e+00 (1.1729e+00)	Acc@1  76.00 ( 72.07)	Acc@5  93.00 ( 92.27)
Test: [ 50/100]	Time  0.078 ( 0.080)	Loss 1.4599e+00 (1.1921e+00)	Acc@1  66.00 ( 71.86)	Acc@5  89.00 ( 92.10)
Test: [ 60/100]	Time  0.079 ( 0.080)	Loss 1.3479e+00 (1.1697e+00)	Acc@1  69.00 ( 72.23)	Acc@5  92.00 ( 92.52)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 1.2231e+00 (1.1705e+00)	Acc@1  68.00 ( 72.21)	Acc@5  90.00 ( 92.37)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 1.2144e+00 (1.1642e+00)	Acc@1  73.00 ( 72.31)	Acc@5  93.00 ( 92.35)
Test: [ 90/100]	Time  0.077 ( 0.079)	Loss 1.5086e+00 (1.1486e+00)	Acc@1  65.00 ( 72.64)	Acc@5  88.00 ( 92.43)
 * Acc@1 72.770 Acc@5 92.520
### epoch[36] execution time: 102.79819130897522
EPOCH 37
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [37][  0/391]	Time  0.398 ( 0.398)	Data  0.147 ( 0.147)	Loss 1.4649e-01 (1.4649e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [37][ 10/391]	Time  0.241 ( 0.255)	Data  0.001 ( 0.014)	Loss 1.6427e-01 (1.4023e-01)	Acc@1  96.09 ( 96.24)	Acc@5 100.00 ( 99.86)
Epoch: [37][ 20/391]	Time  0.241 ( 0.249)	Data  0.001 ( 0.008)	Loss 2.5290e-01 (1.5201e-01)	Acc@1  91.41 ( 95.50)	Acc@5 100.00 ( 99.89)
Epoch: [37][ 30/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.006)	Loss 1.1994e-01 (1.4481e-01)	Acc@1  97.66 ( 95.89)	Acc@5 100.00 ( 99.87)
Epoch: [37][ 40/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.005)	Loss 1.0599e-01 (1.3924e-01)	Acc@1  96.88 ( 96.17)	Acc@5 100.00 ( 99.89)
Epoch: [37][ 50/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.7799e-01 (1.4228e-01)	Acc@1  93.75 ( 96.02)	Acc@5 100.00 ( 99.88)
Epoch: [37][ 60/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.2190e-01 (1.4360e-01)	Acc@1  96.88 ( 95.97)	Acc@5 100.00 ( 99.85)
Epoch: [37][ 70/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.003)	Loss 1.3087e-01 (1.4108e-01)	Acc@1  96.88 ( 96.13)	Acc@5 100.00 ( 99.85)
Epoch: [37][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.3839e-01 (1.4049e-01)	Acc@1  96.88 ( 96.13)	Acc@5 100.00 ( 99.84)
Epoch: [37][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.4367e-01 (1.4059e-01)	Acc@1  96.09 ( 96.15)	Acc@5 100.00 ( 99.83)
Epoch: [37][100/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.7465e-01 (1.4045e-01)	Acc@1  94.53 ( 96.13)	Acc@5 100.00 ( 99.85)
Epoch: [37][110/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.1456e-01 (1.3962e-01)	Acc@1  99.22 ( 96.14)	Acc@5 100.00 ( 99.86)
Epoch: [37][120/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.3546e-01 (1.4039e-01)	Acc@1  92.97 ( 96.16)	Acc@5  98.44 ( 99.85)
Epoch: [37][130/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1838e-01 (1.3858e-01)	Acc@1  94.53 ( 96.16)	Acc@5 100.00 ( 99.86)
Epoch: [37][140/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.3002e-01 (1.4035e-01)	Acc@1  96.88 ( 96.07)	Acc@5 100.00 ( 99.87)
Epoch: [37][150/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.4443e-01 (1.4035e-01)	Acc@1  95.31 ( 96.08)	Acc@5 100.00 ( 99.87)
Epoch: [37][160/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.4939e-01 (1.4037e-01)	Acc@1  96.88 ( 96.08)	Acc@5 100.00 ( 99.88)
Epoch: [37][170/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.5507e-01 (1.4016e-01)	Acc@1  95.31 ( 96.10)	Acc@5 100.00 ( 99.89)
Epoch: [37][180/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.0432e-01 (1.3935e-01)	Acc@1  98.44 ( 96.13)	Acc@5 100.00 ( 99.89)
Epoch: [37][190/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.4892e-01 (1.4026e-01)	Acc@1  94.53 ( 96.09)	Acc@5 100.00 ( 99.89)
Epoch: [37][200/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3032e-01 (1.4041e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 ( 99.89)
Epoch: [37][210/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.4902e-01 (1.4061e-01)	Acc@1  94.53 ( 96.05)	Acc@5 100.00 ( 99.89)
Epoch: [37][220/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3366e-01 (1.4048e-01)	Acc@1  96.09 ( 96.06)	Acc@5 100.00 ( 99.89)
Epoch: [37][230/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3494e-01 (1.4055e-01)	Acc@1  97.66 ( 96.06)	Acc@5 100.00 ( 99.89)
Epoch: [37][240/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0922e-01 (1.4129e-01)	Acc@1  98.44 ( 96.04)	Acc@5  99.22 ( 99.88)
Epoch: [37][250/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1556e-01 (1.4218e-01)	Acc@1  93.75 ( 96.02)	Acc@5 100.00 ( 99.89)
Epoch: [37][260/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2918e-01 (1.4147e-01)	Acc@1  95.31 ( 96.02)	Acc@5 100.00 ( 99.89)
Epoch: [37][270/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3363e-01 (1.4142e-01)	Acc@1  96.09 ( 96.04)	Acc@5 100.00 ( 99.89)
Epoch: [37][280/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.7400e-02 (1.4127e-01)	Acc@1  98.44 ( 96.04)	Acc@5 100.00 ( 99.89)
Epoch: [37][290/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.0296e-02 (1.4137e-01)	Acc@1  99.22 ( 96.05)	Acc@5 100.00 ( 99.89)
Epoch: [37][300/391]	Time  0.246 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0507e-01 (1.4104e-01)	Acc@1  97.66 ( 96.05)	Acc@5 100.00 ( 99.89)
Epoch: [37][310/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.4749e-01 (1.4162e-01)	Acc@1  96.88 ( 96.04)	Acc@5 100.00 ( 99.89)
Epoch: [37][320/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7357e-01 (1.4171e-01)	Acc@1  94.53 ( 96.03)	Acc@5 100.00 ( 99.89)
Epoch: [37][330/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1757e-01 (1.4124e-01)	Acc@1  95.31 ( 96.04)	Acc@5 100.00 ( 99.89)
Epoch: [37][340/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6255e-01 (1.4169e-01)	Acc@1  94.53 ( 96.03)	Acc@5  99.22 ( 99.89)
Epoch: [37][350/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2559e-01 (1.4166e-01)	Acc@1  95.31 ( 96.03)	Acc@5 100.00 ( 99.89)
Epoch: [37][360/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2402e-01 (1.4140e-01)	Acc@1  96.88 ( 96.06)	Acc@5 100.00 ( 99.89)
Epoch: [37][370/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5753e-01 (1.4148e-01)	Acc@1  94.53 ( 96.06)	Acc@5 100.00 ( 99.89)
Epoch: [37][380/391]	Time  0.251 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.4887e-01 (1.4122e-01)	Acc@1  93.75 ( 96.07)	Acc@5 100.00 ( 99.89)
Epoch: [37][390/391]	Time  0.178 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8365e-01 (1.4161e-01)	Acc@1  96.25 ( 96.07)	Acc@5 100.00 ( 99.89)
## e[37] optimizer.zero_grad (sum) time: 0.4929697513580322
## e[37]       loss.backward (sum) time: 11.122138261795044
## e[37]      optimizer.step (sum) time: 47.840001821517944
## epoch[37] training(only) time: 94.82499861717224
# Switched to evaluate mode...
Test: [  0/100]	Time  0.204 ( 0.204)	Loss 1.0541e+00 (1.0541e+00)	Acc@1  76.00 ( 76.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.2392e+00 (1.1809e+00)	Acc@1  71.00 ( 72.64)	Acc@5  92.00 ( 91.82)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 1.1004e+00 (1.1361e+00)	Acc@1  71.00 ( 73.10)	Acc@5  94.00 ( 92.38)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 1.4461e+00 (1.1827e+00)	Acc@1  62.00 ( 72.35)	Acc@5  92.00 ( 91.94)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.2797e+00 (1.1864e+00)	Acc@1  75.00 ( 72.05)	Acc@5  93.00 ( 92.24)
Test: [ 50/100]	Time  0.079 ( 0.081)	Loss 1.5028e+00 (1.2053e+00)	Acc@1  69.00 ( 71.86)	Acc@5  92.00 ( 92.12)
Test: [ 60/100]	Time  0.077 ( 0.080)	Loss 1.4266e+00 (1.1814e+00)	Acc@1  66.00 ( 72.15)	Acc@5  93.00 ( 92.54)
Test: [ 70/100]	Time  0.079 ( 0.080)	Loss 1.1560e+00 (1.1805e+00)	Acc@1  67.00 ( 72.20)	Acc@5  93.00 ( 92.41)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 1.2272e+00 (1.1746e+00)	Acc@1  73.00 ( 72.36)	Acc@5  92.00 ( 92.41)
Test: [ 90/100]	Time  0.080 ( 0.079)	Loss 1.5346e+00 (1.1603e+00)	Acc@1  63.00 ( 72.60)	Acc@5  89.00 ( 92.47)
 * Acc@1 72.690 Acc@5 92.500
### epoch[37] execution time: 102.84622836112976
EPOCH 38
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [38][  0/391]	Time  0.422 ( 0.422)	Data  0.180 ( 0.180)	Loss 8.7126e-02 (8.7126e-02)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [38][ 10/391]	Time  0.241 ( 0.258)	Data  0.001 ( 0.017)	Loss 7.6539e-02 (1.0572e-01)	Acc@1  99.22 ( 96.73)	Acc@5 100.00 (100.00)
Epoch: [38][ 20/391]	Time  0.241 ( 0.250)	Data  0.001 ( 0.010)	Loss 7.5281e-02 (1.1282e-01)	Acc@1 100.00 ( 96.76)	Acc@5 100.00 ( 99.96)
Epoch: [38][ 30/391]	Time  0.240 ( 0.247)	Data  0.001 ( 0.007)	Loss 1.2073e-01 (1.2013e-01)	Acc@1  96.09 ( 96.72)	Acc@5 100.00 ( 99.95)
Epoch: [38][ 40/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.005)	Loss 8.5863e-02 (1.2013e-01)	Acc@1  98.44 ( 96.72)	Acc@5 100.00 ( 99.94)
Epoch: [38][ 50/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.005)	Loss 1.0876e-01 (1.2235e-01)	Acc@1  99.22 ( 96.68)	Acc@5 100.00 ( 99.94)
Epoch: [38][ 60/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.004)	Loss 8.8197e-02 (1.2251e-01)	Acc@1  98.44 ( 96.70)	Acc@5 100.00 ( 99.94)
Epoch: [38][ 70/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.1191e-01 (1.2269e-01)	Acc@1  96.88 ( 96.64)	Acc@5 100.00 ( 99.93)
Epoch: [38][ 80/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 9.1330e-02 (1.2199e-01)	Acc@1  98.44 ( 96.69)	Acc@5 100.00 ( 99.92)
Epoch: [38][ 90/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 1.4401e-01 (1.2379e-01)	Acc@1  95.31 ( 96.58)	Acc@5 100.00 ( 99.92)
Epoch: [38][100/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.3703e-01 (1.2664e-01)	Acc@1  96.09 ( 96.52)	Acc@5 100.00 ( 99.92)
Epoch: [38][110/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 8.9301e-02 (1.2669e-01)	Acc@1  99.22 ( 96.52)	Acc@5 100.00 ( 99.93)
Epoch: [38][120/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.0623e-01 (1.2716e-01)	Acc@1  96.09 ( 96.51)	Acc@5 100.00 ( 99.92)
Epoch: [38][130/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.2022e-01 (1.2775e-01)	Acc@1  96.88 ( 96.54)	Acc@5 100.00 ( 99.92)
Epoch: [38][140/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.4811e-01 (1.2816e-01)	Acc@1  94.53 ( 96.54)	Acc@5 100.00 ( 99.92)
Epoch: [38][150/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.4234e-01 (1.2884e-01)	Acc@1  96.09 ( 96.53)	Acc@5  99.22 ( 99.92)
Epoch: [38][160/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.0150e-01 (1.2864e-01)	Acc@1  92.19 ( 96.53)	Acc@5 100.00 ( 99.91)
Epoch: [38][170/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.0775e-01 (1.2765e-01)	Acc@1  99.22 ( 96.56)	Acc@5 100.00 ( 99.91)
Epoch: [38][180/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.3867e-01 (1.2819e-01)	Acc@1  95.31 ( 96.54)	Acc@5  99.22 ( 99.91)
Epoch: [38][190/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 8.8171e-02 (1.2860e-01)	Acc@1  98.44 ( 96.52)	Acc@5 100.00 ( 99.91)
Epoch: [38][200/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.0278e-01 (1.2881e-01)	Acc@1  94.53 ( 96.49)	Acc@5 100.00 ( 99.90)
Epoch: [38][210/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.6039e-01 (1.2826e-01)	Acc@1  95.31 ( 96.52)	Acc@5 100.00 ( 99.90)
Epoch: [38][220/391]	Time  0.251 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.5839e-01 (1.2871e-01)	Acc@1  94.53 ( 96.48)	Acc@5 100.00 ( 99.90)
Epoch: [38][230/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.2934e-01 (1.2959e-01)	Acc@1  97.66 ( 96.46)	Acc@5 100.00 ( 99.90)
Epoch: [38][240/391]	Time  0.248 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.5582e-01 (1.2984e-01)	Acc@1  96.88 ( 96.45)	Acc@5  99.22 ( 99.90)
Epoch: [38][250/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.0312e-01 (1.2891e-01)	Acc@1  97.66 ( 96.47)	Acc@5 100.00 ( 99.90)
Epoch: [38][260/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1038e-01 (1.2857e-01)	Acc@1  96.88 ( 96.48)	Acc@5 100.00 ( 99.90)
Epoch: [38][270/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1665e-01 (1.2855e-01)	Acc@1  97.66 ( 96.48)	Acc@5 100.00 ( 99.90)
Epoch: [38][280/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.7365e-01 (1.2870e-01)	Acc@1  96.88 ( 96.47)	Acc@5  99.22 ( 99.90)
Epoch: [38][290/391]	Time  0.246 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.9773e-01 (1.2865e-01)	Acc@1  93.75 ( 96.47)	Acc@5 100.00 ( 99.90)
Epoch: [38][300/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.0963e-01 (1.2892e-01)	Acc@1  95.31 ( 96.45)	Acc@5 100.00 ( 99.90)
Epoch: [38][310/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.2567e-01 (1.2913e-01)	Acc@1  95.31 ( 96.45)	Acc@5 100.00 ( 99.90)
Epoch: [38][320/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1470e-01 (1.2883e-01)	Acc@1  97.66 ( 96.46)	Acc@5 100.00 ( 99.90)
Epoch: [38][330/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.3777e-01 (1.2916e-01)	Acc@1  96.88 ( 96.44)	Acc@5  99.22 ( 99.90)
Epoch: [38][340/391]	Time  0.247 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1346e-01 (1.2962e-01)	Acc@1  96.88 ( 96.43)	Acc@5 100.00 ( 99.90)
Epoch: [38][350/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.3400e-01 (1.2996e-01)	Acc@1  94.53 ( 96.41)	Acc@5 100.00 ( 99.90)
Epoch: [38][360/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 8.5994e-02 (1.2992e-01)	Acc@1  97.66 ( 96.41)	Acc@5 100.00 ( 99.90)
Epoch: [38][370/391]	Time  0.246 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.0979e-01 (1.2977e-01)	Acc@1  98.44 ( 96.43)	Acc@5  99.22 ( 99.90)
Epoch: [38][380/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.6309e-01 (1.2994e-01)	Acc@1  96.09 ( 96.43)	Acc@5 100.00 ( 99.90)
Epoch: [38][390/391]	Time  0.177 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6957e-01 (1.3019e-01)	Acc@1  96.25 ( 96.42)	Acc@5 100.00 ( 99.89)
## e[38] optimizer.zero_grad (sum) time: 0.4943380355834961
## e[38]       loss.backward (sum) time: 11.086041927337646
## e[38]      optimizer.step (sum) time: 47.8734130859375
## epoch[38] training(only) time: 94.86249113082886
# Switched to evaluate mode...
Test: [  0/100]	Time  0.202 ( 0.202)	Loss 1.1246e+00 (1.1246e+00)	Acc@1  75.00 ( 75.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.078 ( 0.089)	Loss 1.2068e+00 (1.1909e+00)	Acc@1  71.00 ( 73.73)	Acc@5  93.00 ( 91.91)
Test: [ 20/100]	Time  0.080 ( 0.084)	Loss 1.0362e+00 (1.1443e+00)	Acc@1  71.00 ( 73.67)	Acc@5  94.00 ( 92.57)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 1.5913e+00 (1.1938e+00)	Acc@1  59.00 ( 72.39)	Acc@5  93.00 ( 92.42)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.3152e+00 (1.2013e+00)	Acc@1  75.00 ( 72.20)	Acc@5  93.00 ( 92.63)
Test: [ 50/100]	Time  0.079 ( 0.080)	Loss 1.4832e+00 (1.2214e+00)	Acc@1  67.00 ( 72.02)	Acc@5  89.00 ( 92.29)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.3613e+00 (1.1959e+00)	Acc@1  69.00 ( 72.33)	Acc@5  93.00 ( 92.66)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 1.1776e+00 (1.1970e+00)	Acc@1  71.00 ( 72.41)	Acc@5  92.00 ( 92.46)
Test: [ 80/100]	Time  0.077 ( 0.080)	Loss 1.2927e+00 (1.1901e+00)	Acc@1  72.00 ( 72.53)	Acc@5  92.00 ( 92.48)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.5109e+00 (1.1750e+00)	Acc@1  64.00 ( 72.74)	Acc@5  89.00 ( 92.58)
 * Acc@1 72.840 Acc@5 92.570
### epoch[38] execution time: 102.87390398979187
EPOCH 39
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [39][  0/391]	Time  0.394 ( 0.394)	Data  0.147 ( 0.147)	Loss 1.3573e-01 (1.3573e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [39][ 10/391]	Time  0.241 ( 0.255)	Data  0.001 ( 0.014)	Loss 1.6827e-01 (1.1749e-01)	Acc@1  93.75 ( 96.95)	Acc@5 100.00 ( 99.86)
Epoch: [39][ 20/391]	Time  0.245 ( 0.249)	Data  0.001 ( 0.008)	Loss 8.6798e-02 (1.1717e-01)	Acc@1  99.22 ( 96.84)	Acc@5 100.00 ( 99.93)
Epoch: [39][ 30/391]	Time  0.240 ( 0.246)	Data  0.001 ( 0.006)	Loss 1.0925e-01 (1.1876e-01)	Acc@1  97.66 ( 96.67)	Acc@5 100.00 ( 99.92)
Epoch: [39][ 40/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.005)	Loss 1.2621e-01 (1.1886e-01)	Acc@1  96.88 ( 96.72)	Acc@5 100.00 ( 99.92)
Epoch: [39][ 50/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 6.8039e-02 (1.1714e-01)	Acc@1  98.44 ( 96.80)	Acc@5 100.00 ( 99.92)
Epoch: [39][ 60/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.004)	Loss 5.1195e-02 (1.1490e-01)	Acc@1  99.22 ( 96.81)	Acc@5 100.00 ( 99.94)
Epoch: [39][ 70/391]	Time  0.246 ( 0.244)	Data  0.001 ( 0.003)	Loss 7.5071e-02 (1.1813e-01)	Acc@1  98.44 ( 96.65)	Acc@5 100.00 ( 99.93)
Epoch: [39][ 80/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.5589e-01 (1.1784e-01)	Acc@1  95.31 ( 96.62)	Acc@5 100.00 ( 99.92)
Epoch: [39][ 90/391]	Time  0.252 ( 0.243)	Data  0.001 ( 0.003)	Loss 6.6346e-02 (1.1565e-01)	Acc@1  99.22 ( 96.75)	Acc@5 100.00 ( 99.93)
Epoch: [39][100/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.1495e-01 (1.1731e-01)	Acc@1  96.88 ( 96.70)	Acc@5 100.00 ( 99.94)
Epoch: [39][110/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1954e-01 (1.1830e-01)	Acc@1  98.44 ( 96.70)	Acc@5 100.00 ( 99.94)
Epoch: [39][120/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.3596e-01 (1.1862e-01)	Acc@1  95.31 ( 96.66)	Acc@5 100.00 ( 99.94)
Epoch: [39][130/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 8.8173e-02 (1.1730e-01)	Acc@1  97.66 ( 96.71)	Acc@5 100.00 ( 99.94)
Epoch: [39][140/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.5373e-01 (1.1665e-01)	Acc@1  96.09 ( 96.71)	Acc@5 100.00 ( 99.94)
Epoch: [39][150/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 7.8430e-02 (1.1682e-01)	Acc@1  99.22 ( 96.68)	Acc@5 100.00 ( 99.94)
Epoch: [39][160/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 9.6716e-02 (1.1605e-01)	Acc@1  99.22 ( 96.73)	Acc@5 100.00 ( 99.95)
Epoch: [39][170/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.3123e-01 (1.1633e-01)	Acc@1  96.09 ( 96.74)	Acc@5 100.00 ( 99.95)
Epoch: [39][180/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 9.9745e-02 (1.1600e-01)	Acc@1  98.44 ( 96.81)	Acc@5 100.00 ( 99.95)
Epoch: [39][190/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.3878e-01 (1.1618e-01)	Acc@1  94.53 ( 96.81)	Acc@5 100.00 ( 99.95)
Epoch: [39][200/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.3246e-01 (1.1618e-01)	Acc@1  96.88 ( 96.87)	Acc@5  99.22 ( 99.95)
Epoch: [39][210/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.0333e-01 (1.1649e-01)	Acc@1  96.88 ( 96.85)	Acc@5 100.00 ( 99.94)
Epoch: [39][220/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 8.6777e-02 (1.1631e-01)	Acc@1  98.44 ( 96.85)	Acc@5 100.00 ( 99.95)
Epoch: [39][230/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1006e-01 (1.1700e-01)	Acc@1  97.66 ( 96.81)	Acc@5 100.00 ( 99.94)
Epoch: [39][240/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1438e-01 (1.1739e-01)	Acc@1  95.31 ( 96.78)	Acc@5 100.00 ( 99.94)
Epoch: [39][250/391]	Time  0.250 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.0031e-01 (1.1654e-01)	Acc@1  97.66 ( 96.81)	Acc@5 100.00 ( 99.94)
Epoch: [39][260/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1125e-01 (1.1676e-01)	Acc@1  97.66 ( 96.81)	Acc@5 100.00 ( 99.94)
Epoch: [39][270/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1015e-01 (1.1608e-01)	Acc@1  96.88 ( 96.84)	Acc@5 100.00 ( 99.94)
Epoch: [39][280/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.9687e-02 (1.1598e-01)	Acc@1  97.66 ( 96.85)	Acc@5 100.00 ( 99.94)
Epoch: [39][290/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7967e-01 (1.1643e-01)	Acc@1  93.75 ( 96.83)	Acc@5 100.00 ( 99.94)
Epoch: [39][300/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0012e-01 (1.1657e-01)	Acc@1  95.31 ( 96.83)	Acc@5 100.00 ( 99.94)
Epoch: [39][310/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2704e-01 (1.1692e-01)	Acc@1  96.88 ( 96.82)	Acc@5 100.00 ( 99.94)
Epoch: [39][320/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.6846e-02 (1.1707e-01)	Acc@1  97.66 ( 96.80)	Acc@5 100.00 ( 99.94)
Epoch: [39][330/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0317e-01 (1.1720e-01)	Acc@1  96.88 ( 96.79)	Acc@5 100.00 ( 99.94)
Epoch: [39][340/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.4064e-02 (1.1751e-01)	Acc@1  96.88 ( 96.77)	Acc@5 100.00 ( 99.94)
Epoch: [39][350/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1364e-01 (1.1788e-01)	Acc@1  93.75 ( 96.76)	Acc@5 100.00 ( 99.94)
Epoch: [39][360/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5716e-01 (1.1801e-01)	Acc@1  93.75 ( 96.75)	Acc@5 100.00 ( 99.94)
Epoch: [39][370/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1750e-01 (1.1787e-01)	Acc@1  98.44 ( 96.75)	Acc@5 100.00 ( 99.94)
Epoch: [39][380/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7811e-01 (1.1786e-01)	Acc@1  92.97 ( 96.74)	Acc@5 100.00 ( 99.94)
Epoch: [39][390/391]	Time  0.178 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0332e-01 (1.1764e-01)	Acc@1 100.00 ( 96.75)	Acc@5 100.00 ( 99.94)
## e[39] optimizer.zero_grad (sum) time: 0.49407315254211426
## e[39]       loss.backward (sum) time: 11.087544441223145
## e[39]      optimizer.step (sum) time: 47.82659959793091
## epoch[39] training(only) time: 94.83378672599792
# Switched to evaluate mode...
Test: [  0/100]	Time  0.210 ( 0.210)	Loss 1.1870e+00 (1.1870e+00)	Acc@1  72.00 ( 72.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.2128e+00 (1.2254e+00)	Acc@1  71.00 ( 73.18)	Acc@5  93.00 ( 91.82)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 1.0676e+00 (1.1667e+00)	Acc@1  71.00 ( 73.33)	Acc@5  93.00 ( 92.48)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 1.6314e+00 (1.2095e+00)	Acc@1  63.00 ( 72.23)	Acc@5  91.00 ( 92.13)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.3759e+00 (1.2189e+00)	Acc@1  76.00 ( 71.98)	Acc@5  93.00 ( 92.46)
Test: [ 50/100]	Time  0.078 ( 0.080)	Loss 1.5320e+00 (1.2372e+00)	Acc@1  68.00 ( 71.82)	Acc@5  88.00 ( 92.12)
Test: [ 60/100]	Time  0.077 ( 0.080)	Loss 1.3665e+00 (1.2123e+00)	Acc@1  67.00 ( 72.08)	Acc@5  95.00 ( 92.48)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 1.2767e+00 (1.2151e+00)	Acc@1  69.00 ( 72.06)	Acc@5  91.00 ( 92.35)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 1.2982e+00 (1.2053e+00)	Acc@1  73.00 ( 72.28)	Acc@5  93.00 ( 92.46)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.5442e+00 (1.1884e+00)	Acc@1  65.00 ( 72.65)	Acc@5  90.00 ( 92.57)
 * Acc@1 72.690 Acc@5 92.590
### epoch[39] execution time: 102.85582685470581
EPOCH 40
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [40][  0/391]	Time  0.400 ( 0.400)	Data  0.158 ( 0.158)	Loss 9.2447e-02 (9.2447e-02)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [40][ 10/391]	Time  0.240 ( 0.255)	Data  0.001 ( 0.015)	Loss 1.0938e-01 (1.0930e-01)	Acc@1  95.31 ( 97.37)	Acc@5 100.00 ( 99.93)
Epoch: [40][ 20/391]	Time  0.241 ( 0.249)	Data  0.001 ( 0.009)	Loss 1.1672e-01 (1.0851e-01)	Acc@1  96.09 ( 97.40)	Acc@5 100.00 ( 99.96)
Epoch: [40][ 30/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.006)	Loss 1.2670e-01 (1.0759e-01)	Acc@1  97.66 ( 97.33)	Acc@5 100.00 ( 99.97)
Epoch: [40][ 40/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.005)	Loss 1.7884e-01 (1.0925e-01)	Acc@1  94.53 ( 97.20)	Acc@5 100.00 ( 99.96)
Epoch: [40][ 50/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 7.0744e-02 (1.1068e-01)	Acc@1  99.22 ( 97.04)	Acc@5 100.00 ( 99.97)
Epoch: [40][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 9.5745e-02 (1.0885e-01)	Acc@1  96.88 ( 97.07)	Acc@5 100.00 ( 99.95)
Epoch: [40][ 70/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 8.9918e-02 (1.0968e-01)	Acc@1  98.44 ( 97.01)	Acc@5 100.00 ( 99.94)
Epoch: [40][ 80/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.2334e-01 (1.0926e-01)	Acc@1  98.44 ( 97.08)	Acc@5 100.00 ( 99.93)
Epoch: [40][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.0400e-01 (1.0977e-01)	Acc@1  98.44 ( 97.08)	Acc@5 100.00 ( 99.92)
Epoch: [40][100/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.0051e-01 (1.1018e-01)	Acc@1  96.88 ( 97.05)	Acc@5 100.00 ( 99.93)
Epoch: [40][110/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.0814e-01 (1.0979e-01)	Acc@1  96.88 ( 97.04)	Acc@5 100.00 ( 99.94)
Epoch: [40][120/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.0586e-01 (1.0926e-01)	Acc@1  96.88 ( 97.04)	Acc@5 100.00 ( 99.94)
Epoch: [40][130/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1560e-01 (1.1053e-01)	Acc@1  97.66 ( 97.01)	Acc@5 100.00 ( 99.93)
Epoch: [40][140/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.6148e-02 (1.1002e-01)	Acc@1  97.66 ( 97.00)	Acc@5 100.00 ( 99.94)
Epoch: [40][150/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.4864e-02 (1.1065e-01)	Acc@1  96.09 ( 96.96)	Acc@5 100.00 ( 99.93)
Epoch: [40][160/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8728e-01 (1.1099e-01)	Acc@1  93.75 ( 96.92)	Acc@5  99.22 ( 99.93)
Epoch: [40][170/391]	Time  0.250 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.9548e-02 (1.1091e-01)	Acc@1  97.66 ( 96.93)	Acc@5 100.00 ( 99.94)
Epoch: [40][180/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1266e-01 (1.1052e-01)	Acc@1  96.88 ( 96.94)	Acc@5 100.00 ( 99.94)
Epoch: [40][190/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0781e-01 (1.1067e-01)	Acc@1  95.31 ( 96.94)	Acc@5 100.00 ( 99.94)
Epoch: [40][200/391]	Time  0.248 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0712e-01 (1.1087e-01)	Acc@1  97.66 ( 96.93)	Acc@5 100.00 ( 99.95)
Epoch: [40][210/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1336e-01 (1.1013e-01)	Acc@1  96.88 ( 96.96)	Acc@5 100.00 ( 99.95)
Epoch: [40][220/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5484e-01 (1.1011e-01)	Acc@1  94.53 ( 96.95)	Acc@5 100.00 ( 99.95)
Epoch: [40][230/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1532e-01 (1.1012e-01)	Acc@1  97.66 ( 96.95)	Acc@5 100.00 ( 99.95)
Epoch: [40][240/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0538e-01 (1.1000e-01)	Acc@1  96.09 ( 96.98)	Acc@5  99.22 ( 99.94)
Epoch: [40][250/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0835e-01 (1.0938e-01)	Acc@1  97.66 ( 97.00)	Acc@5 100.00 ( 99.95)
Epoch: [40][260/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.6477e-02 (1.0933e-01)	Acc@1  98.44 ( 97.01)	Acc@5 100.00 ( 99.95)
Epoch: [40][270/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.0170e-02 (1.0916e-01)	Acc@1  97.66 ( 97.02)	Acc@5 100.00 ( 99.95)
Epoch: [40][280/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.2723e-02 (1.0969e-01)	Acc@1  96.88 ( 97.01)	Acc@5 100.00 ( 99.94)
Epoch: [40][290/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.4293e-02 (1.1016e-01)	Acc@1  97.66 ( 96.98)	Acc@5 100.00 ( 99.94)
Epoch: [40][300/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.4887e-01 (1.1066e-01)	Acc@1  96.09 ( 96.96)	Acc@5 100.00 ( 99.94)
Epoch: [40][310/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5655e-01 (1.1069e-01)	Acc@1  94.53 ( 96.97)	Acc@5 100.00 ( 99.94)
Epoch: [40][320/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.4638e-01 (1.1069e-01)	Acc@1  97.66 ( 96.99)	Acc@5 100.00 ( 99.94)
Epoch: [40][330/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.4678e-01 (1.1074e-01)	Acc@1  96.09 ( 96.98)	Acc@5 100.00 ( 99.94)
Epoch: [40][340/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.8176e-02 (1.1069e-01)	Acc@1  99.22 ( 96.99)	Acc@5 100.00 ( 99.94)
Epoch: [40][350/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3386e-01 (1.1042e-01)	Acc@1  96.09 ( 96.99)	Acc@5 100.00 ( 99.94)
Epoch: [40][360/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2253e-01 (1.1030e-01)	Acc@1  96.88 ( 97.00)	Acc@5 100.00 ( 99.94)
Epoch: [40][370/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.9943e-02 (1.1041e-01)	Acc@1  97.66 ( 96.99)	Acc@5 100.00 ( 99.94)
Epoch: [40][380/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.6454e-02 (1.1038e-01)	Acc@1  96.88 ( 96.98)	Acc@5 100.00 ( 99.94)
Epoch: [40][390/391]	Time  0.179 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5848e-01 (1.1031e-01)	Acc@1  95.00 ( 96.98)	Acc@5 100.00 ( 99.94)
## e[40] optimizer.zero_grad (sum) time: 0.495694637298584
## e[40]       loss.backward (sum) time: 11.086724996566772
## e[40]      optimizer.step (sum) time: 47.83492851257324
## epoch[40] training(only) time: 94.70242214202881
# Switched to evaluate mode...
Test: [  0/100]	Time  0.217 ( 0.217)	Loss 1.2397e+00 (1.2397e+00)	Acc@1  72.00 ( 72.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.077 ( 0.090)	Loss 1.3214e+00 (1.2365e+00)	Acc@1  71.00 ( 73.55)	Acc@5  92.00 ( 91.82)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 1.0558e+00 (1.1955e+00)	Acc@1  71.00 ( 73.19)	Acc@5  93.00 ( 92.29)
Test: [ 30/100]	Time  0.077 ( 0.082)	Loss 1.5319e+00 (1.2449e+00)	Acc@1  66.00 ( 72.10)	Acc@5  93.00 ( 92.00)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.3895e+00 (1.2565e+00)	Acc@1  75.00 ( 71.83)	Acc@5  94.00 ( 92.27)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 1.5014e+00 (1.2705e+00)	Acc@1  70.00 ( 71.75)	Acc@5  90.00 ( 92.08)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.4458e+00 (1.2448e+00)	Acc@1  69.00 ( 72.16)	Acc@5  92.00 ( 92.41)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 1.2176e+00 (1.2449e+00)	Acc@1  66.00 ( 72.14)	Acc@5  90.00 ( 92.25)
Test: [ 80/100]	Time  0.077 ( 0.080)	Loss 1.2290e+00 (1.2374e+00)	Acc@1  75.00 ( 72.37)	Acc@5  92.00 ( 92.30)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.6092e+00 (1.2205e+00)	Acc@1  65.00 ( 72.54)	Acc@5  91.00 ( 92.43)
 * Acc@1 72.650 Acc@5 92.390
### epoch[40] execution time: 102.70969367027283
EPOCH 41
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [41][  0/391]	Time  0.391 ( 0.391)	Data  0.143 ( 0.143)	Loss 7.7451e-02 (7.7451e-02)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [41][ 10/391]	Time  0.241 ( 0.255)	Data  0.001 ( 0.014)	Loss 9.2924e-02 (1.0180e-01)	Acc@1  96.09 ( 96.80)	Acc@5 100.00 (100.00)
Epoch: [41][ 20/391]	Time  0.242 ( 0.249)	Data  0.001 ( 0.008)	Loss 1.2469e-01 (9.2648e-02)	Acc@1  96.09 ( 97.58)	Acc@5 100.00 (100.00)
Epoch: [41][ 30/391]	Time  0.242 ( 0.246)	Data  0.001 ( 0.006)	Loss 6.1792e-02 (8.8203e-02)	Acc@1  98.44 ( 97.76)	Acc@5 100.00 (100.00)
Epoch: [41][ 40/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.005)	Loss 1.0145e-01 (8.8539e-02)	Acc@1  96.88 ( 97.81)	Acc@5 100.00 (100.00)
Epoch: [41][ 50/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 9.4509e-02 (9.0310e-02)	Acc@1  98.44 ( 97.76)	Acc@5 100.00 ( 99.97)
Epoch: [41][ 60/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.003)	Loss 8.2863e-02 (9.1921e-02)	Acc@1  97.66 ( 97.73)	Acc@5 100.00 ( 99.97)
Epoch: [41][ 70/391]	Time  0.243 ( 0.244)	Data  0.001 ( 0.003)	Loss 9.4677e-02 (9.1021e-02)	Acc@1  98.44 ( 97.82)	Acc@5 100.00 ( 99.98)
Epoch: [41][ 80/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 5.6491e-02 (9.0879e-02)	Acc@1  99.22 ( 97.84)	Acc@5 100.00 ( 99.98)
Epoch: [41][ 90/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.0335e-01 (9.2149e-02)	Acc@1  98.44 ( 97.78)	Acc@5 100.00 ( 99.97)
Epoch: [41][100/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.1382e-01 (9.2622e-02)	Acc@1  96.88 ( 97.78)	Acc@5 100.00 ( 99.97)
Epoch: [41][110/391]	Time  0.247 ( 0.243)	Data  0.001 ( 0.002)	Loss 6.9736e-02 (9.3586e-02)	Acc@1  99.22 ( 97.71)	Acc@5 100.00 ( 99.97)
Epoch: [41][120/391]	Time  0.252 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.2354e-01 (9.4484e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 ( 99.97)
Epoch: [41][130/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 7.6910e-02 (9.4809e-02)	Acc@1  98.44 ( 97.64)	Acc@5 100.00 ( 99.97)
Epoch: [41][140/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 7.5790e-02 (9.5309e-02)	Acc@1  98.44 ( 97.61)	Acc@5 100.00 ( 99.97)
Epoch: [41][150/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 9.4285e-02 (9.6864e-02)	Acc@1  96.88 ( 97.52)	Acc@5 100.00 ( 99.97)
Epoch: [41][160/391]	Time  0.247 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1079e-01 (9.7071e-02)	Acc@1  98.44 ( 97.50)	Acc@5 100.00 ( 99.97)
Epoch: [41][170/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 9.4824e-02 (9.6463e-02)	Acc@1  96.09 ( 97.51)	Acc@5 100.00 ( 99.97)
Epoch: [41][180/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 9.1140e-02 (9.7252e-02)	Acc@1  98.44 ( 97.50)	Acc@5 100.00 ( 99.97)
Epoch: [41][190/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 8.4567e-02 (9.7270e-02)	Acc@1  96.88 ( 97.50)	Acc@5 100.00 ( 99.97)
Epoch: [41][200/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1325e-01 (9.7463e-02)	Acc@1  96.88 ( 97.47)	Acc@5 100.00 ( 99.97)
Epoch: [41][210/391]	Time  0.246 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.1693e-02 (9.7091e-02)	Acc@1  96.88 ( 97.50)	Acc@5 100.00 ( 99.97)
Epoch: [41][220/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2736e-01 (9.7844e-02)	Acc@1  95.31 ( 97.47)	Acc@5 100.00 ( 99.97)
Epoch: [41][230/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1873e-01 (9.8239e-02)	Acc@1  96.88 ( 97.46)	Acc@5 100.00 ( 99.97)
Epoch: [41][240/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.1318e-02 (9.7966e-02)	Acc@1  97.66 ( 97.46)	Acc@5 100.00 ( 99.96)
Epoch: [41][250/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 9.1407e-02 (9.8289e-02)	Acc@1  98.44 ( 97.46)	Acc@5  99.22 ( 99.96)
Epoch: [41][260/391]	Time  0.249 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.2453e-01 (9.8398e-02)	Acc@1  96.88 ( 97.46)	Acc@5 100.00 ( 99.96)
Epoch: [41][270/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.2278e-01 (9.8614e-02)	Acc@1  96.88 ( 97.45)	Acc@5 100.00 ( 99.96)
Epoch: [41][280/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.3710e-01 (9.9207e-02)	Acc@1  96.09 ( 97.45)	Acc@5 100.00 ( 99.96)
Epoch: [41][290/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0039e-01 (9.9446e-02)	Acc@1  97.66 ( 97.44)	Acc@5 100.00 ( 99.96)
Epoch: [41][300/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0080e-01 (1.0012e-01)	Acc@1  96.09 ( 97.41)	Acc@5 100.00 ( 99.96)
Epoch: [41][310/391]	Time  0.250 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0575e-01 (1.0080e-01)	Acc@1  97.66 ( 97.40)	Acc@5  99.22 ( 99.96)
Epoch: [41][320/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 8.4838e-02 (1.0068e-01)	Acc@1  97.66 ( 97.39)	Acc@5 100.00 ( 99.96)
Epoch: [41][330/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 5.4983e-02 (1.0071e-01)	Acc@1 100.00 ( 97.38)	Acc@5 100.00 ( 99.96)
Epoch: [41][340/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 7.1877e-02 (1.0091e-01)	Acc@1  98.44 ( 97.38)	Acc@5 100.00 ( 99.96)
Epoch: [41][350/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7800e-01 (1.0154e-01)	Acc@1  96.88 ( 97.37)	Acc@5  99.22 ( 99.95)
Epoch: [41][360/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0244e-01 (1.0174e-01)	Acc@1  95.31 ( 97.35)	Acc@5 100.00 ( 99.95)
Epoch: [41][370/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0547e-01 (1.0226e-01)	Acc@1  95.31 ( 97.31)	Acc@5 100.00 ( 99.95)
Epoch: [41][380/391]	Time  0.250 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.0613e-01 (1.0247e-01)	Acc@1  96.88 ( 97.30)	Acc@5 100.00 ( 99.95)
Epoch: [41][390/391]	Time  0.179 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2549e-01 (1.0262e-01)	Acc@1  95.00 ( 97.29)	Acc@5 100.00 ( 99.95)
## e[41] optimizer.zero_grad (sum) time: 0.49487972259521484
## e[41]       loss.backward (sum) time: 11.15260100364685
## e[41]      optimizer.step (sum) time: 47.792298316955566
## epoch[41] training(only) time: 94.85387086868286
# Switched to evaluate mode...
Test: [  0/100]	Time  0.209 ( 0.209)	Loss 1.2207e+00 (1.2207e+00)	Acc@1  73.00 ( 73.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.3273e+00 (1.2443e+00)	Acc@1  70.00 ( 73.18)	Acc@5  92.00 ( 91.55)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 1.0818e+00 (1.2001e+00)	Acc@1  72.00 ( 73.48)	Acc@5  94.00 ( 92.24)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 1.4964e+00 (1.2418e+00)	Acc@1  66.00 ( 72.39)	Acc@5  93.00 ( 92.03)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.3787e+00 (1.2469e+00)	Acc@1  77.00 ( 71.98)	Acc@5  93.00 ( 92.39)
Test: [ 50/100]	Time  0.078 ( 0.080)	Loss 1.4826e+00 (1.2611e+00)	Acc@1  70.00 ( 71.82)	Acc@5  90.00 ( 92.02)
Test: [ 60/100]	Time  0.077 ( 0.080)	Loss 1.3243e+00 (1.2355e+00)	Acc@1  69.00 ( 72.15)	Acc@5  94.00 ( 92.41)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 1.2991e+00 (1.2347e+00)	Acc@1  67.00 ( 72.17)	Acc@5  92.00 ( 92.30)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 1.3100e+00 (1.2281e+00)	Acc@1  72.00 ( 72.33)	Acc@5  93.00 ( 92.36)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.5920e+00 (1.2140e+00)	Acc@1  64.00 ( 72.57)	Acc@5  89.00 ( 92.42)
 * Acc@1 72.730 Acc@5 92.440
### epoch[41] execution time: 102.86639356613159
EPOCH 42
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [42][  0/391]	Time  0.395 ( 0.395)	Data  0.152 ( 0.152)	Loss 8.4885e-02 (8.4885e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [42][ 10/391]	Time  0.241 ( 0.256)	Data  0.001 ( 0.015)	Loss 9.7807e-02 (8.7794e-02)	Acc@1  98.44 ( 97.51)	Acc@5 100.00 (100.00)
Epoch: [42][ 20/391]	Time  0.242 ( 0.249)	Data  0.001 ( 0.008)	Loss 1.3519e-01 (9.8449e-02)	Acc@1  96.88 ( 97.36)	Acc@5 100.00 ( 99.96)
Epoch: [42][ 30/391]	Time  0.247 ( 0.247)	Data  0.001 ( 0.006)	Loss 8.1593e-02 (1.0122e-01)	Acc@1  98.44 ( 97.30)	Acc@5 100.00 ( 99.92)
Epoch: [42][ 40/391]	Time  0.242 ( 0.246)	Data  0.001 ( 0.005)	Loss 6.9612e-02 (1.0029e-01)	Acc@1  96.88 ( 97.31)	Acc@5 100.00 ( 99.94)
Epoch: [42][ 50/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.004)	Loss 6.6105e-02 (9.7000e-02)	Acc@1  99.22 ( 97.44)	Acc@5 100.00 ( 99.95)
Epoch: [42][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 4.1509e-02 (9.3819e-02)	Acc@1 100.00 ( 97.50)	Acc@5 100.00 ( 99.95)
Epoch: [42][ 70/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 8.6845e-02 (9.4476e-02)	Acc@1  98.44 ( 97.52)	Acc@5 100.00 ( 99.96)
Epoch: [42][ 80/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 1.2556e-01 (9.3900e-02)	Acc@1  94.53 ( 97.50)	Acc@5 100.00 ( 99.96)
Epoch: [42][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 5.9940e-02 (9.2174e-02)	Acc@1  99.22 ( 97.57)	Acc@5 100.00 ( 99.95)
Epoch: [42][100/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 6.4286e-02 (9.1972e-02)	Acc@1  99.22 ( 97.60)	Acc@5 100.00 ( 99.95)
Epoch: [42][110/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1127e-01 (9.0962e-02)	Acc@1  96.88 ( 97.66)	Acc@5 100.00 ( 99.96)
Epoch: [42][120/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 7.1863e-02 (9.0807e-02)	Acc@1  97.66 ( 97.64)	Acc@5 100.00 ( 99.96)
Epoch: [42][130/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 8.3570e-02 (9.0696e-02)	Acc@1  96.88 ( 97.62)	Acc@5 100.00 ( 99.96)
Epoch: [42][140/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 5.8870e-02 (9.1872e-02)	Acc@1  99.22 ( 97.57)	Acc@5 100.00 ( 99.96)
Epoch: [42][150/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.0397e-01 (9.2527e-02)	Acc@1  98.44 ( 97.54)	Acc@5 100.00 ( 99.95)
Epoch: [42][160/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 9.4374e-02 (9.2813e-02)	Acc@1  96.88 ( 97.54)	Acc@5 100.00 ( 99.96)
Epoch: [42][170/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 8.4065e-02 (9.2254e-02)	Acc@1  99.22 ( 97.56)	Acc@5 100.00 ( 99.96)
Epoch: [42][180/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 5.6117e-02 (9.2393e-02)	Acc@1  99.22 ( 97.56)	Acc@5 100.00 ( 99.96)
Epoch: [42][190/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 6.5466e-02 (9.1808e-02)	Acc@1 100.00 ( 97.58)	Acc@5 100.00 ( 99.96)
Epoch: [42][200/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0525e-01 (9.2312e-02)	Acc@1  97.66 ( 97.55)	Acc@5 100.00 ( 99.96)
Epoch: [42][210/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.8534e-02 (9.1851e-02)	Acc@1  96.88 ( 97.56)	Acc@5 100.00 ( 99.96)
Epoch: [42][220/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.8411e-02 (9.1509e-02)	Acc@1 100.00 ( 97.57)	Acc@5 100.00 ( 99.96)
Epoch: [42][230/391]	Time  0.247 ( 0.243)	Data  0.001 ( 0.002)	Loss 5.7402e-02 (9.1847e-02)	Acc@1 100.00 ( 97.58)	Acc@5 100.00 ( 99.97)
Epoch: [42][240/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0080e-01 (9.2317e-02)	Acc@1  96.88 ( 97.56)	Acc@5 100.00 ( 99.97)
Epoch: [42][250/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.6523e-02 (9.2127e-02)	Acc@1  99.22 ( 97.59)	Acc@5 100.00 ( 99.97)
Epoch: [42][260/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.3481e-02 (9.2176e-02)	Acc@1  96.88 ( 97.57)	Acc@5 100.00 ( 99.97)
Epoch: [42][270/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.6475e-02 (9.2324e-02)	Acc@1  98.44 ( 97.57)	Acc@5 100.00 ( 99.97)
Epoch: [42][280/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.3147e-02 (9.2280e-02)	Acc@1  96.09 ( 97.57)	Acc@5 100.00 ( 99.97)
Epoch: [42][290/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1641e-01 (9.2096e-02)	Acc@1  95.31 ( 97.57)	Acc@5 100.00 ( 99.97)
Epoch: [42][300/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.3230e-01 (9.2679e-02)	Acc@1  94.53 ( 97.56)	Acc@5  99.22 ( 99.97)
Epoch: [42][310/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.3065e-02 (9.2619e-02)	Acc@1  98.44 ( 97.56)	Acc@5 100.00 ( 99.96)
Epoch: [42][320/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.3887e-02 (9.2506e-02)	Acc@1  97.66 ( 97.57)	Acc@5 100.00 ( 99.96)
Epoch: [42][330/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 5.9767e-02 (9.2007e-02)	Acc@1  98.44 ( 97.59)	Acc@5 100.00 ( 99.96)
Epoch: [42][340/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.6231e-02 (9.2314e-02)	Acc@1  99.22 ( 97.58)	Acc@5 100.00 ( 99.96)
Epoch: [42][350/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.8754e-02 (9.2361e-02)	Acc@1  98.44 ( 97.56)	Acc@5 100.00 ( 99.96)
Epoch: [42][360/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2166e-01 (9.2686e-02)	Acc@1  96.88 ( 97.55)	Acc@5 100.00 ( 99.96)
Epoch: [42][370/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.4183e-02 (9.2531e-02)	Acc@1  98.44 ( 97.56)	Acc@5 100.00 ( 99.96)
Epoch: [42][380/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.0896e-02 (9.2417e-02)	Acc@1  99.22 ( 97.57)	Acc@5 100.00 ( 99.96)
Epoch: [42][390/391]	Time  0.178 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8391e-01 (9.2681e-02)	Acc@1  93.75 ( 97.57)	Acc@5 100.00 ( 99.96)
## e[42] optimizer.zero_grad (sum) time: 0.49304866790771484
## e[42]       loss.backward (sum) time: 11.129477262496948
## e[42]      optimizer.step (sum) time: 47.80847668647766
## epoch[42] training(only) time: 94.81088376045227
# Switched to evaluate mode...
Test: [  0/100]	Time  0.204 ( 0.204)	Loss 1.1631e+00 (1.1631e+00)	Acc@1  76.00 ( 76.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.078 ( 0.089)	Loss 1.2539e+00 (1.2252e+00)	Acc@1  71.00 ( 73.45)	Acc@5  93.00 ( 91.82)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 1.0958e+00 (1.1984e+00)	Acc@1  72.00 ( 73.52)	Acc@5  93.00 ( 92.24)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 1.5019e+00 (1.2324e+00)	Acc@1  67.00 ( 72.45)	Acc@5  93.00 ( 91.94)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.3267e+00 (1.2441e+00)	Acc@1  75.00 ( 71.80)	Acc@5  94.00 ( 92.20)
Test: [ 50/100]	Time  0.077 ( 0.080)	Loss 1.5454e+00 (1.2637e+00)	Acc@1  71.00 ( 71.71)	Acc@5  90.00 ( 92.02)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.4443e+00 (1.2395e+00)	Acc@1  67.00 ( 71.92)	Acc@5  92.00 ( 92.39)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 1.2724e+00 (1.2413e+00)	Acc@1  67.00 ( 71.90)	Acc@5  91.00 ( 92.20)
Test: [ 80/100]	Time  0.078 ( 0.079)	Loss 1.2785e+00 (1.2345e+00)	Acc@1  74.00 ( 72.14)	Acc@5  92.00 ( 92.23)
Test: [ 90/100]	Time  0.079 ( 0.079)	Loss 1.7059e+00 (1.2186e+00)	Acc@1  62.00 ( 72.35)	Acc@5  89.00 ( 92.27)
 * Acc@1 72.490 Acc@5 92.240
### epoch[42] execution time: 102.82517170906067
EPOCH 43
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [43][  0/391]	Time  0.407 ( 0.407)	Data  0.146 ( 0.146)	Loss 7.2700e-02 (7.2700e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [43][ 10/391]	Time  0.240 ( 0.256)	Data  0.001 ( 0.014)	Loss 6.4611e-02 (6.8509e-02)	Acc@1  99.22 ( 98.22)	Acc@5 100.00 (100.00)
Epoch: [43][ 20/391]	Time  0.242 ( 0.249)	Data  0.001 ( 0.008)	Loss 8.9063e-02 (7.2389e-02)	Acc@1  97.66 ( 98.21)	Acc@5 100.00 ( 99.93)
Epoch: [43][ 30/391]	Time  0.242 ( 0.247)	Data  0.001 ( 0.006)	Loss 9.1512e-02 (7.1773e-02)	Acc@1  98.44 ( 98.41)	Acc@5 100.00 ( 99.95)
Epoch: [43][ 40/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.005)	Loss 8.6103e-02 (7.2636e-02)	Acc@1  97.66 ( 98.34)	Acc@5 100.00 ( 99.94)
Epoch: [43][ 50/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.004)	Loss 5.0947e-02 (7.3826e-02)	Acc@1  98.44 ( 98.25)	Acc@5 100.00 ( 99.95)
Epoch: [43][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 7.2352e-02 (7.5995e-02)	Acc@1  97.66 ( 98.22)	Acc@5 100.00 ( 99.95)
Epoch: [43][ 70/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 7.2398e-02 (7.7084e-02)	Acc@1  98.44 ( 98.16)	Acc@5 100.00 ( 99.96)
Epoch: [43][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 7.6655e-02 (7.8604e-02)	Acc@1  96.88 ( 98.05)	Acc@5 100.00 ( 99.95)
Epoch: [43][ 90/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 6.4949e-02 (7.8155e-02)	Acc@1  99.22 ( 98.09)	Acc@5 100.00 ( 99.96)
Epoch: [43][100/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 6.6819e-02 (7.8295e-02)	Acc@1  98.44 ( 98.04)	Acc@5 100.00 ( 99.95)
Epoch: [43][110/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 9.2246e-02 (7.8395e-02)	Acc@1  96.88 ( 98.05)	Acc@5 100.00 ( 99.96)
Epoch: [43][120/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.0329e-01 (7.8299e-02)	Acc@1  96.88 ( 98.06)	Acc@5 100.00 ( 99.96)
Epoch: [43][130/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.8085e-02 (7.8577e-02)	Acc@1 100.00 ( 98.07)	Acc@5 100.00 ( 99.96)
Epoch: [43][140/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 8.1358e-02 (7.9455e-02)	Acc@1  98.44 ( 98.04)	Acc@5 100.00 ( 99.96)
Epoch: [43][150/391]	Time  0.251 ( 0.243)	Data  0.001 ( 0.002)	Loss 9.6934e-02 (8.0427e-02)	Acc@1  97.66 ( 98.01)	Acc@5 100.00 ( 99.96)
Epoch: [43][160/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1364e-01 (8.0080e-02)	Acc@1  97.66 ( 98.03)	Acc@5 100.00 ( 99.97)
Epoch: [43][170/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 9.5770e-02 (8.1512e-02)	Acc@1  99.22 ( 97.97)	Acc@5 100.00 ( 99.96)
Epoch: [43][180/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 8.1749e-02 (8.2266e-02)	Acc@1  98.44 ( 97.94)	Acc@5 100.00 ( 99.96)
Epoch: [43][190/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 5.5882e-02 (8.2420e-02)	Acc@1  98.44 ( 97.95)	Acc@5 100.00 ( 99.96)
Epoch: [43][200/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.5186e-01 (8.2668e-02)	Acc@1  96.09 ( 97.95)	Acc@5 100.00 ( 99.96)
Epoch: [43][210/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.0003e-01 (8.3170e-02)	Acc@1  98.44 ( 97.93)	Acc@5 100.00 ( 99.96)
Epoch: [43][220/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 6.5096e-02 (8.3202e-02)	Acc@1  97.66 ( 97.94)	Acc@5 100.00 ( 99.96)
Epoch: [43][230/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.0071e-02 (8.3355e-02)	Acc@1  99.22 ( 97.92)	Acc@5 100.00 ( 99.96)
Epoch: [43][240/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.4820e-02 (8.3244e-02)	Acc@1  99.22 ( 97.90)	Acc@5 100.00 ( 99.96)
Epoch: [43][250/391]	Time  0.246 ( 0.243)	Data  0.001 ( 0.002)	Loss 7.9331e-02 (8.2903e-02)	Acc@1  97.66 ( 97.93)	Acc@5 100.00 ( 99.97)
Epoch: [43][260/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.4769e-01 (8.3011e-02)	Acc@1  94.53 ( 97.92)	Acc@5 100.00 ( 99.97)
Epoch: [43][270/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.3897e-02 (8.3201e-02)	Acc@1  98.44 ( 97.90)	Acc@5 100.00 ( 99.97)
Epoch: [43][280/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.9018e-02 (8.3260e-02)	Acc@1  98.44 ( 97.91)	Acc@5 100.00 ( 99.97)
Epoch: [43][290/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.8426e-02 (8.3946e-02)	Acc@1  99.22 ( 97.89)	Acc@5  99.22 ( 99.97)
Epoch: [43][300/391]	Time  0.246 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.8532e-02 (8.3824e-02)	Acc@1  98.44 ( 97.90)	Acc@5 100.00 ( 99.97)
Epoch: [43][310/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0482e-01 (8.3763e-02)	Acc@1  98.44 ( 97.92)	Acc@5 100.00 ( 99.97)
Epoch: [43][320/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.4916e-02 (8.3748e-02)	Acc@1  98.44 ( 97.91)	Acc@5 100.00 ( 99.97)
Epoch: [43][330/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0186e-01 (8.3610e-02)	Acc@1  97.66 ( 97.91)	Acc@5 100.00 ( 99.97)
Epoch: [43][340/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.7598e-02 (8.3388e-02)	Acc@1 100.00 ( 97.92)	Acc@5 100.00 ( 99.97)
Epoch: [43][350/391]	Time  0.246 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.7951e-02 (8.3516e-02)	Acc@1  96.88 ( 97.91)	Acc@5 100.00 ( 99.97)
Epoch: [43][360/391]	Time  0.251 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.9537e-02 (8.3687e-02)	Acc@1  99.22 ( 97.91)	Acc@5 100.00 ( 99.97)
Epoch: [43][370/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.4842e-02 (8.3474e-02)	Acc@1  98.44 ( 97.92)	Acc@5 100.00 ( 99.97)
Epoch: [43][380/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.5942e-02 (8.3480e-02)	Acc@1  98.44 ( 97.92)	Acc@5 100.00 ( 99.97)
Epoch: [43][390/391]	Time  0.182 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1050e-01 (8.3574e-02)	Acc@1  95.00 ( 97.91)	Acc@5 100.00 ( 99.97)
## e[43] optimizer.zero_grad (sum) time: 0.49422788619995117
## e[43]       loss.backward (sum) time: 11.10710334777832
## e[43]      optimizer.step (sum) time: 47.85205268859863
## epoch[43] training(only) time: 94.8290946483612
# Switched to evaluate mode...
Test: [  0/100]	Time  0.204 ( 0.204)	Loss 1.2173e+00 (1.2173e+00)	Acc@1  74.00 ( 74.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.077 ( 0.089)	Loss 1.3362e+00 (1.2541e+00)	Acc@1  72.00 ( 73.45)	Acc@5  94.00 ( 91.64)
Test: [ 20/100]	Time  0.079 ( 0.084)	Loss 1.1085e+00 (1.2173e+00)	Acc@1  72.00 ( 73.19)	Acc@5  91.00 ( 91.90)
Test: [ 30/100]	Time  0.077 ( 0.082)	Loss 1.5327e+00 (1.2622e+00)	Acc@1  66.00 ( 72.16)	Acc@5  92.00 ( 91.65)
Test: [ 40/100]	Time  0.077 ( 0.081)	Loss 1.3804e+00 (1.2743e+00)	Acc@1  78.00 ( 71.83)	Acc@5  93.00 ( 91.88)
Test: [ 50/100]	Time  0.078 ( 0.080)	Loss 1.5272e+00 (1.2907e+00)	Acc@1  72.00 ( 71.76)	Acc@5  88.00 ( 91.63)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.3969e+00 (1.2649e+00)	Acc@1  71.00 ( 72.02)	Acc@5  92.00 ( 92.05)
Test: [ 70/100]	Time  0.077 ( 0.080)	Loss 1.3905e+00 (1.2668e+00)	Acc@1  68.00 ( 72.03)	Acc@5  92.00 ( 91.93)
Test: [ 80/100]	Time  0.078 ( 0.079)	Loss 1.3189e+00 (1.2588e+00)	Acc@1  74.00 ( 72.22)	Acc@5  93.00 ( 92.02)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.6213e+00 (1.2435e+00)	Acc@1  62.00 ( 72.45)	Acc@5  91.00 ( 92.12)
 * Acc@1 72.600 Acc@5 92.190
### epoch[43] execution time: 102.85121202468872
EPOCH 44
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [44][  0/391]	Time  0.403 ( 0.403)	Data  0.148 ( 0.148)	Loss 1.4171e-01 (1.4171e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [44][ 10/391]	Time  0.241 ( 0.256)	Data  0.001 ( 0.015)	Loss 3.8705e-02 (7.4308e-02)	Acc@1  99.22 ( 98.22)	Acc@5 100.00 (100.00)
Epoch: [44][ 20/391]	Time  0.240 ( 0.249)	Data  0.001 ( 0.008)	Loss 3.9814e-02 (7.0600e-02)	Acc@1  99.22 ( 98.47)	Acc@5 100.00 ( 99.96)
Epoch: [44][ 30/391]	Time  0.246 ( 0.246)	Data  0.001 ( 0.006)	Loss 6.6760e-02 (6.7168e-02)	Acc@1  98.44 ( 98.46)	Acc@5 100.00 ( 99.97)
Epoch: [44][ 40/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.005)	Loss 7.5107e-02 (6.7561e-02)	Acc@1  99.22 ( 98.51)	Acc@5 100.00 ( 99.98)
Epoch: [44][ 50/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.004)	Loss 6.1751e-02 (6.8377e-02)	Acc@1  99.22 ( 98.51)	Acc@5 100.00 ( 99.98)
Epoch: [44][ 60/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.4753e-01 (7.0551e-02)	Acc@1  96.88 ( 98.40)	Acc@5  99.22 ( 99.97)
Epoch: [44][ 70/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 5.6318e-02 (7.1014e-02)	Acc@1  99.22 ( 98.36)	Acc@5 100.00 ( 99.98)
Epoch: [44][ 80/391]	Time  0.246 ( 0.243)	Data  0.001 ( 0.003)	Loss 7.8678e-02 (7.3159e-02)	Acc@1  97.66 ( 98.29)	Acc@5 100.00 ( 99.97)
Epoch: [44][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 9.4825e-02 (7.4130e-02)	Acc@1  98.44 ( 98.29)	Acc@5 100.00 ( 99.97)
Epoch: [44][100/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 4.8868e-02 (7.3522e-02)	Acc@1  99.22 ( 98.31)	Acc@5 100.00 ( 99.98)
Epoch: [44][110/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 5.7097e-02 (7.3062e-02)	Acc@1  99.22 ( 98.31)	Acc@5 100.00 ( 99.98)
Epoch: [44][120/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 9.6665e-02 (7.3970e-02)	Acc@1  96.88 ( 98.31)	Acc@5 100.00 ( 99.98)
Epoch: [44][130/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.002)	Loss 6.5003e-02 (7.4105e-02)	Acc@1  98.44 ( 98.28)	Acc@5 100.00 ( 99.98)
Epoch: [44][140/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.4613e-01 (7.5646e-02)	Acc@1  96.09 ( 98.22)	Acc@5 100.00 ( 99.98)
Epoch: [44][150/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.1410e-02 (7.5840e-02)	Acc@1  98.44 ( 98.20)	Acc@5 100.00 ( 99.98)
Epoch: [44][160/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.1558e-02 (7.6102e-02)	Acc@1  99.22 ( 98.20)	Acc@5 100.00 ( 99.98)
Epoch: [44][170/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.8122e-02 (7.5804e-02)	Acc@1  99.22 ( 98.20)	Acc@5 100.00 ( 99.98)
Epoch: [44][180/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.0039e-02 (7.6556e-02)	Acc@1  98.44 ( 98.19)	Acc@5 100.00 ( 99.97)
Epoch: [44][190/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.0514e-02 (7.6922e-02)	Acc@1  99.22 ( 98.18)	Acc@5 100.00 ( 99.97)
Epoch: [44][200/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.8790e-02 (7.7031e-02)	Acc@1  96.88 ( 98.18)	Acc@5 100.00 ( 99.97)
Epoch: [44][210/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0828e-01 (7.7544e-02)	Acc@1  96.88 ( 98.16)	Acc@5 100.00 ( 99.97)
Epoch: [44][220/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0620e-01 (7.8270e-02)	Acc@1  96.09 ( 98.11)	Acc@5 100.00 ( 99.97)
Epoch: [44][230/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.0640e-02 (7.8081e-02)	Acc@1  98.44 ( 98.12)	Acc@5 100.00 ( 99.97)
Epoch: [44][240/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.1578e-02 (7.8086e-02)	Acc@1  98.44 ( 98.13)	Acc@5 100.00 ( 99.97)
Epoch: [44][250/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.1992e-02 (7.8204e-02)	Acc@1  98.44 ( 98.13)	Acc@5 100.00 ( 99.98)
Epoch: [44][260/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2898e-01 (7.8940e-02)	Acc@1  96.88 ( 98.11)	Acc@5 100.00 ( 99.97)
Epoch: [44][270/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.5869e-02 (7.8830e-02)	Acc@1  99.22 ( 98.13)	Acc@5 100.00 ( 99.97)
Epoch: [44][280/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0213e-01 (7.8873e-02)	Acc@1  96.09 ( 98.14)	Acc@5 100.00 ( 99.97)
Epoch: [44][290/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.0346e-02 (7.8787e-02)	Acc@1 100.00 ( 98.14)	Acc@5 100.00 ( 99.97)
Epoch: [44][300/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.9939e-02 (7.8580e-02)	Acc@1  96.88 ( 98.13)	Acc@5 100.00 ( 99.97)
Epoch: [44][310/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.9470e-02 (7.8678e-02)	Acc@1  97.66 ( 98.13)	Acc@5 100.00 ( 99.97)
Epoch: [44][320/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.0222e-02 (7.8428e-02)	Acc@1  99.22 ( 98.14)	Acc@5 100.00 ( 99.97)
Epoch: [44][330/391]	Time  0.252 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.7171e-02 (7.8902e-02)	Acc@1  97.66 ( 98.13)	Acc@5 100.00 ( 99.97)
Epoch: [44][340/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.4052e-02 (7.8492e-02)	Acc@1  98.44 ( 98.14)	Acc@5 100.00 ( 99.97)
Epoch: [44][350/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2685e-01 (7.8951e-02)	Acc@1  96.09 ( 98.11)	Acc@5 100.00 ( 99.98)
Epoch: [44][360/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.3218e-02 (7.8780e-02)	Acc@1  97.66 ( 98.12)	Acc@5 100.00 ( 99.98)
Epoch: [44][370/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.2069e-02 (7.8881e-02)	Acc@1 100.00 ( 98.11)	Acc@5 100.00 ( 99.98)
Epoch: [44][380/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1430e-01 (7.8861e-02)	Acc@1  97.66 ( 98.11)	Acc@5 100.00 ( 99.98)
Epoch: [44][390/391]	Time  0.179 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.7462e-02 (7.9072e-02)	Acc@1  98.75 ( 98.11)	Acc@5 100.00 ( 99.98)
## e[44] optimizer.zero_grad (sum) time: 0.49136900901794434
## e[44]       loss.backward (sum) time: 11.096687078475952
## e[44]      optimizer.step (sum) time: 47.839221239089966
## epoch[44] training(only) time: 94.68890810012817
# Switched to evaluate mode...
Test: [  0/100]	Time  0.212 ( 0.212)	Loss 1.2419e+00 (1.2419e+00)	Acc@1  73.00 ( 73.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.2996e+00 (1.2575e+00)	Acc@1  70.00 ( 73.36)	Acc@5  94.00 ( 91.27)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 1.0795e+00 (1.2181e+00)	Acc@1  70.00 ( 73.14)	Acc@5  95.00 ( 92.00)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 1.5535e+00 (1.2664e+00)	Acc@1  64.00 ( 71.74)	Acc@5  92.00 ( 91.71)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.4065e+00 (1.2770e+00)	Acc@1  76.00 ( 71.56)	Acc@5  93.00 ( 92.07)
Test: [ 50/100]	Time  0.077 ( 0.081)	Loss 1.5510e+00 (1.2985e+00)	Acc@1  71.00 ( 71.55)	Acc@5  88.00 ( 91.73)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.5462e+00 (1.2763e+00)	Acc@1  70.00 ( 71.84)	Acc@5  92.00 ( 92.11)
Test: [ 70/100]	Time  0.077 ( 0.080)	Loss 1.3961e+00 (1.2809e+00)	Acc@1  68.00 ( 71.80)	Acc@5  90.00 ( 92.10)
Test: [ 80/100]	Time  0.077 ( 0.079)	Loss 1.3110e+00 (1.2742e+00)	Acc@1  74.00 ( 72.02)	Acc@5  91.00 ( 92.17)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.6902e+00 (1.2592e+00)	Acc@1  63.00 ( 72.26)	Acc@5  90.00 ( 92.20)
 * Acc@1 72.440 Acc@5 92.260
### epoch[44] execution time: 102.69024300575256
EPOCH 45
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [45][  0/391]	Time  0.399 ( 0.399)	Data  0.153 ( 0.153)	Loss 8.3814e-02 (8.3814e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [45][ 10/391]	Time  0.241 ( 0.256)	Data  0.001 ( 0.015)	Loss 1.0219e-01 (6.9636e-02)	Acc@1  96.88 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [45][ 20/391]	Time  0.241 ( 0.249)	Data  0.001 ( 0.008)	Loss 5.3141e-02 (7.4488e-02)	Acc@1  98.44 ( 98.47)	Acc@5 100.00 ( 99.96)
Epoch: [45][ 30/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.006)	Loss 7.9326e-02 (6.9745e-02)	Acc@1  97.66 ( 98.61)	Acc@5 100.00 ( 99.97)
Epoch: [45][ 40/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.005)	Loss 7.7752e-02 (7.1673e-02)	Acc@1  98.44 ( 98.51)	Acc@5 100.00 ( 99.98)
Epoch: [45][ 50/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.004)	Loss 8.3964e-02 (7.3193e-02)	Acc@1  96.88 ( 98.42)	Acc@5 100.00 ( 99.95)
Epoch: [45][ 60/391]	Time  0.253 ( 0.244)	Data  0.001 ( 0.004)	Loss 4.5738e-02 (7.1864e-02)	Acc@1  99.22 ( 98.42)	Acc@5 100.00 ( 99.95)
Epoch: [45][ 70/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 8.0779e-02 (7.4135e-02)	Acc@1  98.44 ( 98.34)	Acc@5 100.00 ( 99.93)
Epoch: [45][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 6.5988e-02 (7.3682e-02)	Acc@1  99.22 ( 98.40)	Acc@5 100.00 ( 99.94)
Epoch: [45][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.9849e-02 (7.1922e-02)	Acc@1 100.00 ( 98.45)	Acc@5 100.00 ( 99.94)
Epoch: [45][100/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 8.4265e-02 (7.1950e-02)	Acc@1  96.09 ( 98.44)	Acc@5 100.00 ( 99.95)
Epoch: [45][110/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 5.8494e-02 (7.1181e-02)	Acc@1  99.22 ( 98.47)	Acc@5 100.00 ( 99.95)
Epoch: [45][120/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 6.8805e-02 (7.0913e-02)	Acc@1  99.22 ( 98.48)	Acc@5 100.00 ( 99.95)
Epoch: [45][130/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 7.9892e-02 (7.1560e-02)	Acc@1  96.88 ( 98.44)	Acc@5 100.00 ( 99.95)
Epoch: [45][140/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 5.5225e-02 (7.0428e-02)	Acc@1  99.22 ( 98.47)	Acc@5 100.00 ( 99.96)
Epoch: [45][150/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 9.7211e-02 (7.1032e-02)	Acc@1  97.66 ( 98.45)	Acc@5 100.00 ( 99.96)
Epoch: [45][160/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 5.4273e-02 (7.1032e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 ( 99.96)
Epoch: [45][170/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.9099e-02 (7.0793e-02)	Acc@1  99.22 ( 98.43)	Acc@5 100.00 ( 99.95)
Epoch: [45][180/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.1593e-02 (7.0122e-02)	Acc@1  99.22 ( 98.45)	Acc@5 100.00 ( 99.96)
Epoch: [45][190/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.9422e-02 (7.0880e-02)	Acc@1  96.09 ( 98.42)	Acc@5 100.00 ( 99.96)
Epoch: [45][200/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.9434e-02 (7.0828e-02)	Acc@1 100.00 ( 98.41)	Acc@5 100.00 ( 99.96)
Epoch: [45][210/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.3501e-02 (7.1205e-02)	Acc@1  99.22 ( 98.39)	Acc@5 100.00 ( 99.96)
Epoch: [45][220/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.8699e-02 (7.1646e-02)	Acc@1  96.88 ( 98.36)	Acc@5 100.00 ( 99.96)
Epoch: [45][230/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.9102e-02 (7.1928e-02)	Acc@1  99.22 ( 98.36)	Acc@5 100.00 ( 99.96)
Epoch: [45][240/391]	Time  0.255 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.9665e-02 (7.1984e-02)	Acc@1  98.44 ( 98.35)	Acc@5 100.00 ( 99.96)
Epoch: [45][250/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 7.5769e-02 (7.2295e-02)	Acc@1  97.66 ( 98.33)	Acc@5 100.00 ( 99.97)
Epoch: [45][260/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.3666e-02 (7.2270e-02)	Acc@1 100.00 ( 98.32)	Acc@5 100.00 ( 99.97)
Epoch: [45][270/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.0605e-01 (7.2439e-02)	Acc@1  96.88 ( 98.31)	Acc@5 100.00 ( 99.97)
Epoch: [45][280/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 8.5781e-02 (7.2598e-02)	Acc@1  97.66 ( 98.29)	Acc@5 100.00 ( 99.97)
Epoch: [45][290/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.2214e-02 (7.2647e-02)	Acc@1  99.22 ( 98.31)	Acc@5 100.00 ( 99.97)
Epoch: [45][300/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 7.9489e-02 (7.2624e-02)	Acc@1  96.88 ( 98.29)	Acc@5 100.00 ( 99.97)
Epoch: [45][310/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.2916e-02 (7.2582e-02)	Acc@1  98.44 ( 98.29)	Acc@5 100.00 ( 99.97)
Epoch: [45][320/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.8467e-02 (7.2880e-02)	Acc@1  98.44 ( 98.29)	Acc@5 100.00 ( 99.97)
Epoch: [45][330/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.6971e-02 (7.3168e-02)	Acc@1  98.44 ( 98.29)	Acc@5 100.00 ( 99.97)
Epoch: [45][340/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.7437e-02 (7.3037e-02)	Acc@1  97.66 ( 98.29)	Acc@5 100.00 ( 99.97)
Epoch: [45][350/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.4769e-02 (7.3347e-02)	Acc@1  99.22 ( 98.28)	Acc@5 100.00 ( 99.97)
Epoch: [45][360/391]	Time  0.254 ( 0.243)	Data  0.001 ( 0.002)	Loss 5.6979e-02 (7.3543e-02)	Acc@1  99.22 ( 98.27)	Acc@5 100.00 ( 99.97)
Epoch: [45][370/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1052e-01 (7.3566e-02)	Acc@1  96.09 ( 98.26)	Acc@5 100.00 ( 99.97)
Epoch: [45][380/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 6.2336e-02 (7.3609e-02)	Acc@1 100.00 ( 98.26)	Acc@5 100.00 ( 99.97)
Epoch: [45][390/391]	Time  0.178 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.6698e-02 (7.3426e-02)	Acc@1  98.75 ( 98.26)	Acc@5 100.00 ( 99.97)
## e[45] optimizer.zero_grad (sum) time: 0.4938995838165283
## e[45]       loss.backward (sum) time: 11.11280369758606
## e[45]      optimizer.step (sum) time: 47.83599925041199
## epoch[45] training(only) time: 94.85534715652466
# Switched to evaluate mode...
Test: [  0/100]	Time  0.218 ( 0.218)	Loss 1.2067e+00 (1.2067e+00)	Acc@1  74.00 ( 74.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.078 ( 0.091)	Loss 1.3235e+00 (1.2908e+00)	Acc@1  72.00 ( 73.09)	Acc@5  94.00 ( 91.09)
Test: [ 20/100]	Time  0.077 ( 0.084)	Loss 1.0812e+00 (1.2393e+00)	Acc@1  68.00 ( 73.33)	Acc@5  94.00 ( 92.19)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 1.6040e+00 (1.2900e+00)	Acc@1  63.00 ( 71.74)	Acc@5  93.00 ( 92.00)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.3565e+00 (1.2958e+00)	Acc@1  74.00 ( 71.32)	Acc@5  93.00 ( 92.29)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 1.5505e+00 (1.3170e+00)	Acc@1  73.00 ( 71.27)	Acc@5  89.00 ( 92.00)
Test: [ 60/100]	Time  0.077 ( 0.080)	Loss 1.4203e+00 (1.2916e+00)	Acc@1  70.00 ( 71.67)	Acc@5  94.00 ( 92.43)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 1.4335e+00 (1.2905e+00)	Acc@1  69.00 ( 71.79)	Acc@5  89.00 ( 92.28)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 1.3147e+00 (1.2794e+00)	Acc@1  72.00 ( 71.93)	Acc@5  93.00 ( 92.35)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.6679e+00 (1.2649e+00)	Acc@1  63.00 ( 72.13)	Acc@5  89.00 ( 92.31)
 * Acc@1 72.220 Acc@5 92.290
### epoch[45] execution time: 102.86391735076904
EPOCH 46
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [46][  0/391]	Time  0.393 ( 0.393)	Data  0.151 ( 0.151)	Loss 5.8911e-02 (5.8911e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [46][ 10/391]	Time  0.241 ( 0.255)	Data  0.001 ( 0.015)	Loss 5.5175e-02 (6.2966e-02)	Acc@1  99.22 ( 98.65)	Acc@5 100.00 (100.00)
Epoch: [46][ 20/391]	Time  0.245 ( 0.248)	Data  0.001 ( 0.008)	Loss 5.1692e-02 (6.3529e-02)	Acc@1  98.44 ( 98.59)	Acc@5 100.00 (100.00)
Epoch: [46][ 30/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.006)	Loss 4.4864e-02 (6.2952e-02)	Acc@1 100.00 ( 98.66)	Acc@5 100.00 (100.00)
Epoch: [46][ 40/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.005)	Loss 6.9677e-02 (6.6800e-02)	Acc@1  98.44 ( 98.55)	Acc@5 100.00 (100.00)
Epoch: [46][ 50/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 7.4400e-02 (6.6005e-02)	Acc@1  98.44 ( 98.61)	Acc@5 100.00 (100.00)
Epoch: [46][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 7.6498e-02 (6.5564e-02)	Acc@1  98.44 ( 98.54)	Acc@5 100.00 (100.00)
Epoch: [46][ 70/391]	Time  0.251 ( 0.243)	Data  0.001 ( 0.003)	Loss 5.3934e-02 (6.8385e-02)	Acc@1  98.44 ( 98.40)	Acc@5 100.00 (100.00)
Epoch: [46][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 6.3819e-02 (6.8210e-02)	Acc@1  97.66 ( 98.40)	Acc@5 100.00 (100.00)
Epoch: [46][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 7.4774e-02 (6.7530e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [46][100/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 7.7918e-02 (6.8494e-02)	Acc@1  98.44 ( 98.40)	Acc@5 100.00 (100.00)
Epoch: [46][110/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.6206e-02 (6.7398e-02)	Acc@1  99.22 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [46][120/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.002)	Loss 7.4186e-02 (6.6927e-02)	Acc@1  98.44 ( 98.48)	Acc@5 100.00 (100.00)
Epoch: [46][130/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 6.3288e-02 (6.7390e-02)	Acc@1  98.44 ( 98.46)	Acc@5 100.00 (100.00)
Epoch: [46][140/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 5.9974e-02 (6.7751e-02)	Acc@1 100.00 ( 98.46)	Acc@5 100.00 (100.00)
Epoch: [46][150/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.9706e-02 (6.7674e-02)	Acc@1  96.88 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [46][160/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.1763e-02 (6.7764e-02)	Acc@1  97.66 ( 98.43)	Acc@5  99.22 (100.00)
Epoch: [46][170/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.9983e-02 (6.7721e-02)	Acc@1  99.22 ( 98.45)	Acc@5 100.00 (100.00)
Epoch: [46][180/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.4244e-02 (6.8036e-02)	Acc@1  96.88 ( 98.42)	Acc@5 100.00 (100.00)
Epoch: [46][190/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.8190e-02 (6.7478e-02)	Acc@1  96.88 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [46][200/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.3064e-02 (6.7066e-02)	Acc@1  98.44 ( 98.45)	Acc@5 100.00 (100.00)
Epoch: [46][210/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.4918e-02 (6.6883e-02)	Acc@1 100.00 ( 98.46)	Acc@5 100.00 (100.00)
Epoch: [46][220/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.1516e-02 (6.6917e-02)	Acc@1  99.22 ( 98.47)	Acc@5 100.00 ( 99.99)
Epoch: [46][230/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.3634e-02 (6.6709e-02)	Acc@1  98.44 ( 98.45)	Acc@5 100.00 ( 99.99)
Epoch: [46][240/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.0419e-02 (6.6491e-02)	Acc@1 100.00 ( 98.46)	Acc@5 100.00 ( 99.99)
Epoch: [46][250/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.6120e-02 (6.6356e-02)	Acc@1  98.44 ( 98.47)	Acc@5 100.00 ( 99.99)
Epoch: [46][260/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.7127e-02 (6.5779e-02)	Acc@1  98.44 ( 98.49)	Acc@5 100.00 ( 99.99)
Epoch: [46][270/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.0152e-02 (6.5752e-02)	Acc@1  99.22 ( 98.49)	Acc@5 100.00 ( 99.99)
Epoch: [46][280/391]	Time  0.239 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.9115e-02 (6.6150e-02)	Acc@1  99.22 ( 98.49)	Acc@5 100.00 ( 99.99)
Epoch: [46][290/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.8987e-02 (6.6159e-02)	Acc@1  98.44 ( 98.50)	Acc@5 100.00 ( 99.99)
Epoch: [46][300/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.7449e-02 (6.6326e-02)	Acc@1  99.22 ( 98.49)	Acc@5 100.00 ( 99.99)
Epoch: [46][310/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5248e-01 (6.7249e-02)	Acc@1  96.09 ( 98.45)	Acc@5 100.00 ( 99.99)
Epoch: [46][320/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.2431e-02 (6.7095e-02)	Acc@1 100.00 ( 98.44)	Acc@5 100.00 ( 99.99)
Epoch: [46][330/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.6159e-02 (6.7336e-02)	Acc@1  97.66 ( 98.44)	Acc@5 100.00 ( 99.99)
Epoch: [46][340/391]	Time  0.254 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.8458e-02 (6.7338e-02)	Acc@1  96.88 ( 98.44)	Acc@5 100.00 ( 99.99)
Epoch: [46][350/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0791e-01 (6.7299e-02)	Acc@1  96.09 ( 98.44)	Acc@5  99.22 ( 99.99)
Epoch: [46][360/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.1019e-02 (6.7521e-02)	Acc@1 100.00 ( 98.43)	Acc@5 100.00 ( 99.99)
Epoch: [46][370/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.3705e-02 (6.7595e-02)	Acc@1  98.44 ( 98.41)	Acc@5 100.00 ( 99.99)
Epoch: [46][380/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.3648e-02 (6.7922e-02)	Acc@1  97.66 ( 98.40)	Acc@5 100.00 ( 99.99)
Epoch: [46][390/391]	Time  0.178 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.9554e-02 (6.7981e-02)	Acc@1  98.75 ( 98.40)	Acc@5 100.00 ( 99.99)
## e[46] optimizer.zero_grad (sum) time: 0.4923555850982666
## e[46]       loss.backward (sum) time: 11.156259536743164
## e[46]      optimizer.step (sum) time: 47.810617446899414
## epoch[46] training(only) time: 94.71564388275146
# Switched to evaluate mode...
Test: [  0/100]	Time  0.213 ( 0.213)	Loss 1.2521e+00 (1.2521e+00)	Acc@1  75.00 ( 75.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.2754e+00 (1.2957e+00)	Acc@1  72.00 ( 72.64)	Acc@5  92.00 ( 91.18)
Test: [ 20/100]	Time  0.077 ( 0.085)	Loss 1.0634e+00 (1.2528e+00)	Acc@1  73.00 ( 73.00)	Acc@5  94.00 ( 92.00)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 1.6217e+00 (1.3104e+00)	Acc@1  64.00 ( 71.84)	Acc@5  91.00 ( 91.58)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.3817e+00 (1.3146e+00)	Acc@1  78.00 ( 71.73)	Acc@5  93.00 ( 91.93)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 1.6428e+00 (1.3360e+00)	Acc@1  71.00 ( 71.59)	Acc@5  87.00 ( 91.67)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.4112e+00 (1.3076e+00)	Acc@1  67.00 ( 71.79)	Acc@5  95.00 ( 92.10)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 1.3780e+00 (1.3034e+00)	Acc@1  72.00 ( 71.85)	Acc@5  90.00 ( 92.07)
Test: [ 80/100]	Time  0.079 ( 0.080)	Loss 1.3145e+00 (1.2935e+00)	Acc@1  73.00 ( 72.02)	Acc@5  93.00 ( 92.14)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.6738e+00 (1.2757e+00)	Acc@1  67.00 ( 72.29)	Acc@5  89.00 ( 92.25)
 * Acc@1 72.340 Acc@5 92.300
### epoch[46] execution time: 102.73729014396667
EPOCH 47
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [47][  0/391]	Time  0.396 ( 0.396)	Data  0.153 ( 0.153)	Loss 6.7121e-02 (6.7121e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [47][ 10/391]	Time  0.240 ( 0.255)	Data  0.001 ( 0.015)	Loss 4.6952e-02 (5.9022e-02)	Acc@1  98.44 ( 98.79)	Acc@5 100.00 (100.00)
Epoch: [47][ 20/391]	Time  0.241 ( 0.248)	Data  0.001 ( 0.008)	Loss 8.2185e-02 (6.6470e-02)	Acc@1  97.66 ( 98.51)	Acc@5 100.00 (100.00)
Epoch: [47][ 30/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.006)	Loss 4.8433e-02 (6.1923e-02)	Acc@1  99.22 ( 98.69)	Acc@5 100.00 (100.00)
Epoch: [47][ 40/391]	Time  0.242 ( 0.245)	Data  0.001 ( 0.005)	Loss 5.3958e-02 (5.8929e-02)	Acc@1  98.44 ( 98.80)	Acc@5 100.00 (100.00)
Epoch: [47][ 50/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 4.4396e-02 (6.0761e-02)	Acc@1  99.22 ( 98.61)	Acc@5 100.00 (100.00)
Epoch: [47][ 60/391]	Time  0.252 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.3621e-01 (6.3061e-02)	Acc@1  94.53 ( 98.45)	Acc@5 100.00 (100.00)
Epoch: [47][ 70/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 5.2523e-02 (6.4013e-02)	Acc@1  98.44 ( 98.46)	Acc@5 100.00 ( 99.99)
Epoch: [47][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 7.5547e-02 (6.2909e-02)	Acc@1  96.88 ( 98.52)	Acc@5 100.00 ( 99.98)
Epoch: [47][ 90/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 4.1716e-02 (6.3922e-02)	Acc@1  99.22 ( 98.50)	Acc@5 100.00 ( 99.98)
Epoch: [47][100/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 8.7269e-02 (6.3292e-02)	Acc@1  98.44 ( 98.53)	Acc@5 100.00 ( 99.98)
Epoch: [47][110/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 6.0678e-02 (6.3687e-02)	Acc@1  99.22 ( 98.51)	Acc@5 100.00 ( 99.98)
Epoch: [47][120/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.8448e-02 (6.2931e-02)	Acc@1 100.00 ( 98.51)	Acc@5 100.00 ( 99.98)
Epoch: [47][130/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 5.2467e-02 (6.2493e-02)	Acc@1  99.22 ( 98.53)	Acc@5 100.00 ( 99.98)
Epoch: [47][140/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 8.2040e-02 (6.2301e-02)	Acc@1  96.88 ( 98.53)	Acc@5 100.00 ( 99.98)
Epoch: [47][150/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 5.9897e-02 (6.1967e-02)	Acc@1  98.44 ( 98.54)	Acc@5 100.00 ( 99.98)
Epoch: [47][160/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 6.3249e-02 (6.2125e-02)	Acc@1  99.22 ( 98.55)	Acc@5 100.00 ( 99.98)
Epoch: [47][170/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.2088e-02 (6.2420e-02)	Acc@1  99.22 ( 98.53)	Acc@5 100.00 ( 99.98)
Epoch: [47][180/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.1024e-02 (6.2649e-02)	Acc@1  97.66 ( 98.52)	Acc@5 100.00 ( 99.98)
Epoch: [47][190/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 9.8505e-02 (6.2400e-02)	Acc@1  96.88 ( 98.54)	Acc@5 100.00 ( 99.98)
Epoch: [47][200/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.8450e-02 (6.2184e-02)	Acc@1  98.44 ( 98.57)	Acc@5 100.00 ( 99.98)
Epoch: [47][210/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.5173e-02 (6.2484e-02)	Acc@1  98.44 ( 98.56)	Acc@5 100.00 ( 99.98)
Epoch: [47][220/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.3005e-02 (6.2393e-02)	Acc@1  98.44 ( 98.56)	Acc@5 100.00 ( 99.98)
Epoch: [47][230/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.8396e-02 (6.2308e-02)	Acc@1  97.66 ( 98.55)	Acc@5 100.00 ( 99.98)
Epoch: [47][240/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.1959e-02 (6.2563e-02)	Acc@1  98.44 ( 98.54)	Acc@5 100.00 ( 99.98)
Epoch: [47][250/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0556e-01 (6.3134e-02)	Acc@1  95.31 ( 98.52)	Acc@5 100.00 ( 99.98)
Epoch: [47][260/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.7574e-02 (6.3134e-02)	Acc@1  96.88 ( 98.51)	Acc@5 100.00 ( 99.99)
Epoch: [47][270/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3104e-01 (6.3626e-02)	Acc@1  94.53 ( 98.49)	Acc@5 100.00 ( 99.98)
Epoch: [47][280/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.4333e-02 (6.3554e-02)	Acc@1  97.66 ( 98.48)	Acc@5 100.00 ( 99.98)
Epoch: [47][290/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.1098e-02 (6.3694e-02)	Acc@1 100.00 ( 98.48)	Acc@5 100.00 ( 99.98)
Epoch: [47][300/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.2812e-02 (6.3535e-02)	Acc@1  98.44 ( 98.49)	Acc@5 100.00 ( 99.98)
Epoch: [47][310/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0615e-01 (6.3660e-02)	Acc@1  97.66 ( 98.48)	Acc@5  99.22 ( 99.98)
Epoch: [47][320/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.7469e-02 (6.3819e-02)	Acc@1  98.44 ( 98.48)	Acc@5 100.00 ( 99.98)
Epoch: [47][330/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.0615e-02 (6.3744e-02)	Acc@1  96.88 ( 98.48)	Acc@5 100.00 ( 99.98)
Epoch: [47][340/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.9839e-02 (6.3762e-02)	Acc@1  99.22 ( 98.48)	Acc@5 100.00 ( 99.98)
Epoch: [47][350/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.7295e-02 (6.3680e-02)	Acc@1  97.66 ( 98.49)	Acc@5 100.00 ( 99.98)
Epoch: [47][360/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.9740e-02 (6.3634e-02)	Acc@1  97.66 ( 98.49)	Acc@5 100.00 ( 99.98)
Epoch: [47][370/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.5809e-02 (6.3671e-02)	Acc@1  99.22 ( 98.49)	Acc@5 100.00 ( 99.99)
Epoch: [47][380/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.2587e-02 (6.3470e-02)	Acc@1 100.00 ( 98.51)	Acc@5 100.00 ( 99.99)
Epoch: [47][390/391]	Time  0.180 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.9108e-02 (6.3600e-02)	Acc@1  98.75 ( 98.49)	Acc@5 100.00 ( 99.99)
## e[47] optimizer.zero_grad (sum) time: 0.4935770034790039
## e[47]       loss.backward (sum) time: 11.117797374725342
## e[47]      optimizer.step (sum) time: 47.848273038864136
## epoch[47] training(only) time: 94.75078892707825
# Switched to evaluate mode...
Test: [  0/100]	Time  0.207 ( 0.207)	Loss 1.2644e+00 (1.2644e+00)	Acc@1  73.00 ( 73.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.4071e+00 (1.3237e+00)	Acc@1  70.00 ( 72.82)	Acc@5  93.00 ( 91.09)
Test: [ 20/100]	Time  0.077 ( 0.084)	Loss 1.1797e+00 (1.2830e+00)	Acc@1  69.00 ( 72.67)	Acc@5  93.00 ( 91.81)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 1.6000e+00 (1.3296e+00)	Acc@1  67.00 ( 71.74)	Acc@5  92.00 ( 91.71)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.4323e+00 (1.3294e+00)	Acc@1  76.00 ( 71.49)	Acc@5  93.00 ( 92.05)
Test: [ 50/100]	Time  0.078 ( 0.080)	Loss 1.6395e+00 (1.3470e+00)	Acc@1  70.00 ( 71.37)	Acc@5  88.00 ( 91.65)
Test: [ 60/100]	Time  0.077 ( 0.080)	Loss 1.5016e+00 (1.3166e+00)	Acc@1  69.00 ( 71.85)	Acc@5  92.00 ( 92.03)
Test: [ 70/100]	Time  0.079 ( 0.080)	Loss 1.3247e+00 (1.3163e+00)	Acc@1  69.00 ( 71.90)	Acc@5  90.00 ( 92.01)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 1.4330e+00 (1.3071e+00)	Acc@1  70.00 ( 72.05)	Acc@5  93.00 ( 92.11)
Test: [ 90/100]	Time  0.079 ( 0.079)	Loss 1.7704e+00 (1.2898e+00)	Acc@1  62.00 ( 72.24)	Acc@5  91.00 ( 92.26)
 * Acc@1 72.370 Acc@5 92.220
### epoch[47] execution time: 102.765132188797
EPOCH 48
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [48][  0/391]	Time  0.398 ( 0.398)	Data  0.147 ( 0.147)	Loss 9.2072e-02 (9.2072e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [48][ 10/391]	Time  0.242 ( 0.255)	Data  0.001 ( 0.014)	Loss 5.5967e-02 (6.8109e-02)	Acc@1  98.44 ( 98.22)	Acc@5 100.00 (100.00)
Epoch: [48][ 20/391]	Time  0.242 ( 0.249)	Data  0.001 ( 0.008)	Loss 4.6976e-02 (5.9913e-02)	Acc@1  99.22 ( 98.66)	Acc@5 100.00 (100.00)
Epoch: [48][ 30/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.006)	Loss 6.2917e-02 (5.9289e-02)	Acc@1  98.44 ( 98.71)	Acc@5 100.00 (100.00)
Epoch: [48][ 40/391]	Time  0.248 ( 0.245)	Data  0.001 ( 0.005)	Loss 2.9707e-02 (5.7193e-02)	Acc@1 100.00 ( 98.78)	Acc@5 100.00 (100.00)
Epoch: [48][ 50/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.004)	Loss 5.9136e-02 (5.7519e-02)	Acc@1  99.22 ( 98.79)	Acc@5 100.00 (100.00)
Epoch: [48][ 60/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.004)	Loss 4.9951e-02 (5.7032e-02)	Acc@1  99.22 ( 98.87)	Acc@5 100.00 (100.00)
Epoch: [48][ 70/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 5.9272e-02 (5.6772e-02)	Acc@1  98.44 ( 98.81)	Acc@5 100.00 (100.00)
Epoch: [48][ 80/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.003)	Loss 5.5655e-02 (5.7858e-02)	Acc@1 100.00 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [48][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 7.1479e-02 (5.6876e-02)	Acc@1  97.66 ( 98.82)	Acc@5 100.00 (100.00)
Epoch: [48][100/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.2477e-01 (5.6753e-02)	Acc@1  95.31 ( 98.82)	Acc@5 100.00 (100.00)
Epoch: [48][110/391]	Time  0.251 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.5273e-02 (5.7608e-02)	Acc@1 100.00 ( 98.78)	Acc@5 100.00 (100.00)
Epoch: [48][120/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.2930e-02 (5.7669e-02)	Acc@1 100.00 ( 98.81)	Acc@5 100.00 (100.00)
Epoch: [48][130/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.9276e-02 (5.6700e-02)	Acc@1  99.22 ( 98.86)	Acc@5 100.00 (100.00)
Epoch: [48][140/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 5.7387e-02 (5.6432e-02)	Acc@1  98.44 ( 98.88)	Acc@5 100.00 (100.00)
Epoch: [48][150/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.7893e-02 (5.6150e-02)	Acc@1  99.22 ( 98.88)	Acc@5 100.00 ( 99.99)
Epoch: [48][160/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 5.9320e-02 (5.6455e-02)	Acc@1  99.22 ( 98.85)	Acc@5 100.00 (100.00)
Epoch: [48][170/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.4479e-02 (5.7293e-02)	Acc@1 100.00 ( 98.83)	Acc@5 100.00 (100.00)
Epoch: [48][180/391]	Time  0.248 ( 0.243)	Data  0.001 ( 0.002)	Loss 5.8400e-02 (5.7666e-02)	Acc@1  98.44 ( 98.79)	Acc@5 100.00 (100.00)
Epoch: [48][190/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 6.5210e-02 (5.7382e-02)	Acc@1  98.44 ( 98.80)	Acc@5 100.00 (100.00)
Epoch: [48][200/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 6.0501e-02 (5.7578e-02)	Acc@1  98.44 ( 98.78)	Acc@5 100.00 (100.00)
Epoch: [48][210/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.0051e-01 (5.7910e-02)	Acc@1  96.09 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [48][220/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 5.1407e-02 (5.8000e-02)	Acc@1  99.22 ( 98.78)	Acc@5 100.00 ( 99.99)
Epoch: [48][230/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 7.2299e-02 (5.8223e-02)	Acc@1  99.22 ( 98.76)	Acc@5 100.00 ( 99.99)
Epoch: [48][240/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.7012e-02 (5.8139e-02)	Acc@1 100.00 ( 98.75)	Acc@5 100.00 ( 99.99)
Epoch: [48][250/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 9.2683e-02 (5.8376e-02)	Acc@1  96.09 ( 98.73)	Acc@5 100.00 ( 99.99)
Epoch: [48][260/391]	Time  0.246 ( 0.243)	Data  0.001 ( 0.002)	Loss 6.8831e-02 (5.8247e-02)	Acc@1  96.88 ( 98.73)	Acc@5 100.00 ( 99.99)
Epoch: [48][270/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 5.1185e-02 (5.8124e-02)	Acc@1  98.44 ( 98.74)	Acc@5 100.00 ( 99.99)
Epoch: [48][280/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 6.3122e-02 (5.8160e-02)	Acc@1  99.22 ( 98.74)	Acc@5 100.00 ( 99.99)
Epoch: [48][290/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 5.2827e-02 (5.8458e-02)	Acc@1  99.22 ( 98.72)	Acc@5 100.00 ( 99.99)
Epoch: [48][300/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.3551e-02 (5.8618e-02)	Acc@1 100.00 ( 98.72)	Acc@5 100.00 ( 99.98)
Epoch: [48][310/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 6.0055e-02 (5.8470e-02)	Acc@1  98.44 ( 98.72)	Acc@5 100.00 ( 99.98)
Epoch: [48][320/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.5130e-02 (5.8323e-02)	Acc@1 100.00 ( 98.74)	Acc@5 100.00 ( 99.98)
Epoch: [48][330/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 6.4336e-02 (5.8557e-02)	Acc@1  98.44 ( 98.73)	Acc@5 100.00 ( 99.98)
Epoch: [48][340/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 9.5495e-02 (5.8663e-02)	Acc@1  97.66 ( 98.71)	Acc@5 100.00 ( 99.98)
Epoch: [48][350/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 7.2300e-02 (5.8928e-02)	Acc@1  98.44 ( 98.70)	Acc@5 100.00 ( 99.98)
Epoch: [48][360/391]	Time  0.254 ( 0.243)	Data  0.001 ( 0.002)	Loss 5.1053e-02 (5.8993e-02)	Acc@1 100.00 ( 98.69)	Acc@5 100.00 ( 99.98)
Epoch: [48][370/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.3704e-02 (5.8993e-02)	Acc@1  99.22 ( 98.69)	Acc@5 100.00 ( 99.98)
Epoch: [48][380/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 5.6772e-02 (5.9144e-02)	Acc@1  99.22 ( 98.69)	Acc@5 100.00 ( 99.98)
Epoch: [48][390/391]	Time  0.178 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.5970e-02 (5.9318e-02)	Acc@1 100.00 ( 98.69)	Acc@5 100.00 ( 99.98)
## e[48] optimizer.zero_grad (sum) time: 0.49059462547302246
## e[48]       loss.backward (sum) time: 11.140756368637085
## e[48]      optimizer.step (sum) time: 47.788331031799316
## epoch[48] training(only) time: 94.87158441543579
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 1.2673e+00 (1.2673e+00)	Acc@1  76.00 ( 76.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.077 ( 0.089)	Loss 1.3972e+00 (1.3254e+00)	Acc@1  69.00 ( 72.45)	Acc@5  93.00 ( 91.18)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 1.2147e+00 (1.2799e+00)	Acc@1  67.00 ( 72.81)	Acc@5  93.00 ( 91.81)
Test: [ 30/100]	Time  0.079 ( 0.082)	Loss 1.5806e+00 (1.3294e+00)	Acc@1  65.00 ( 71.74)	Acc@5  92.00 ( 91.52)
Test: [ 40/100]	Time  0.077 ( 0.081)	Loss 1.3728e+00 (1.3369e+00)	Acc@1  75.00 ( 71.44)	Acc@5  92.00 ( 91.88)
Test: [ 50/100]	Time  0.078 ( 0.080)	Loss 1.7116e+00 (1.3488e+00)	Acc@1  69.00 ( 71.39)	Acc@5  90.00 ( 91.65)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.5213e+00 (1.3198e+00)	Acc@1  66.00 ( 71.69)	Acc@5  91.00 ( 92.03)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 1.4105e+00 (1.3226e+00)	Acc@1  68.00 ( 71.51)	Acc@5  91.00 ( 91.87)
Test: [ 80/100]	Time  0.078 ( 0.079)	Loss 1.3470e+00 (1.3125e+00)	Acc@1  71.00 ( 71.68)	Acc@5  91.00 ( 91.94)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.6747e+00 (1.2949e+00)	Acc@1  65.00 ( 72.02)	Acc@5  90.00 ( 92.02)
 * Acc@1 72.080 Acc@5 92.040
### epoch[48] execution time: 102.87541890144348
EPOCH 49
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [49][  0/391]	Time  0.392 ( 0.392)	Data  0.142 ( 0.142)	Loss 4.9557e-02 (4.9557e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [49][ 10/391]	Time  0.241 ( 0.255)	Data  0.001 ( 0.014)	Loss 4.1106e-02 (6.2088e-02)	Acc@1 100.00 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [49][ 20/391]	Time  0.241 ( 0.248)	Data  0.001 ( 0.008)	Loss 5.6857e-02 (5.4447e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [49][ 30/391]	Time  0.244 ( 0.246)	Data  0.001 ( 0.006)	Loss 3.8501e-02 (5.4897e-02)	Acc@1 100.00 ( 98.92)	Acc@5 100.00 (100.00)
Epoch: [49][ 40/391]	Time  0.253 ( 0.245)	Data  0.001 ( 0.005)	Loss 3.7867e-02 (5.5735e-02)	Acc@1  99.22 ( 98.91)	Acc@5 100.00 (100.00)
Epoch: [49][ 50/391]	Time  0.242 ( 0.245)	Data  0.001 ( 0.004)	Loss 4.1150e-02 (5.6085e-02)	Acc@1 100.00 ( 98.82)	Acc@5 100.00 (100.00)
Epoch: [49][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 2.9861e-02 (5.5267e-02)	Acc@1  99.22 ( 98.80)	Acc@5 100.00 (100.00)
Epoch: [49][ 70/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 3.5516e-02 (5.4059e-02)	Acc@1  99.22 ( 98.81)	Acc@5 100.00 (100.00)
Epoch: [49][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 6.4324e-02 (5.5240e-02)	Acc@1  98.44 ( 98.78)	Acc@5 100.00 (100.00)
Epoch: [49][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 6.3814e-02 (5.4649e-02)	Acc@1  98.44 ( 98.82)	Acc@5 100.00 (100.00)
Epoch: [49][100/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 5.3935e-02 (5.4762e-02)	Acc@1  98.44 ( 98.81)	Acc@5 100.00 (100.00)
Epoch: [49][110/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 5.4828e-02 (5.5252e-02)	Acc@1  99.22 ( 98.78)	Acc@5 100.00 (100.00)
Epoch: [49][120/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 6.0130e-02 (5.5033e-02)	Acc@1  99.22 ( 98.80)	Acc@5 100.00 (100.00)
Epoch: [49][130/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.3946e-02 (5.4559e-02)	Acc@1 100.00 ( 98.83)	Acc@5 100.00 (100.00)
Epoch: [49][140/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.1605e-02 (5.4212e-02)	Acc@1  99.22 ( 98.84)	Acc@5 100.00 (100.00)
Epoch: [49][150/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.7415e-02 (5.4481e-02)	Acc@1  98.44 ( 98.84)	Acc@5 100.00 (100.00)
Epoch: [49][160/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.4734e-02 (5.4153e-02)	Acc@1  96.88 ( 98.82)	Acc@5 100.00 (100.00)
Epoch: [49][170/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.0998e-02 (5.3737e-02)	Acc@1 100.00 ( 98.83)	Acc@5 100.00 (100.00)
Epoch: [49][180/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.6425e-02 (5.3579e-02)	Acc@1  99.22 ( 98.83)	Acc@5 100.00 (100.00)
Epoch: [49][190/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.5582e-02 (5.3687e-02)	Acc@1  99.22 ( 98.83)	Acc@5 100.00 (100.00)
Epoch: [49][200/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.6634e-02 (5.3702e-02)	Acc@1 100.00 ( 98.82)	Acc@5 100.00 (100.00)
Epoch: [49][210/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.1819e-02 (5.3410e-02)	Acc@1  99.22 ( 98.83)	Acc@5 100.00 (100.00)
Epoch: [49][220/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.7093e-02 (5.3496e-02)	Acc@1  98.44 ( 98.81)	Acc@5 100.00 (100.00)
Epoch: [49][230/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.8271e-02 (5.3841e-02)	Acc@1  98.44 ( 98.79)	Acc@5 100.00 (100.00)
Epoch: [49][240/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8460e-02 (5.3624e-02)	Acc@1  99.22 ( 98.79)	Acc@5 100.00 (100.00)
Epoch: [49][250/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.9033e-02 (5.3445e-02)	Acc@1 100.00 ( 98.81)	Acc@5 100.00 (100.00)
Epoch: [49][260/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.8558e-02 (5.3718e-02)	Acc@1  99.22 ( 98.79)	Acc@5 100.00 (100.00)
Epoch: [49][270/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.4008e-02 (5.3750e-02)	Acc@1 100.00 ( 98.80)	Acc@5 100.00 (100.00)
Epoch: [49][280/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.9607e-02 (5.3972e-02)	Acc@1  99.22 ( 98.80)	Acc@5 100.00 (100.00)
Epoch: [49][290/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.5553e-02 (5.4200e-02)	Acc@1  97.66 ( 98.79)	Acc@5 100.00 (100.00)
Epoch: [49][300/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.4333e-02 (5.4333e-02)	Acc@1  97.66 ( 98.79)	Acc@5 100.00 (100.00)
Epoch: [49][310/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.4037e-02 (5.4530e-02)	Acc@1  95.31 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [49][320/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.8269e-02 (5.4787e-02)	Acc@1  97.66 ( 98.76)	Acc@5 100.00 (100.00)
Epoch: [49][330/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.6110e-02 (5.4755e-02)	Acc@1  97.66 ( 98.76)	Acc@5 100.00 (100.00)
Epoch: [49][340/391]	Time  0.246 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.0892e-02 (5.4580e-02)	Acc@1  99.22 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [49][350/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.9069e-02 (5.4348e-02)	Acc@1  99.22 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [49][360/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.7275e-02 (5.4727e-02)	Acc@1  98.44 ( 98.75)	Acc@5 100.00 (100.00)
Epoch: [49][370/391]	Time  0.255 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.3658e-02 (5.5039e-02)	Acc@1  98.44 ( 98.75)	Acc@5 100.00 (100.00)
Epoch: [49][380/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.4304e-02 (5.5009e-02)	Acc@1  98.44 ( 98.75)	Acc@5 100.00 (100.00)
Epoch: [49][390/391]	Time  0.178 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1330e-01 (5.5352e-02)	Acc@1  98.75 ( 98.74)	Acc@5 100.00 (100.00)
## e[49] optimizer.zero_grad (sum) time: 0.49510741233825684
## e[49]       loss.backward (sum) time: 11.070939302444458
## e[49]      optimizer.step (sum) time: 47.862788677215576
## epoch[49] training(only) time: 94.76484870910645
# Switched to evaluate mode...
Test: [  0/100]	Time  0.206 ( 0.206)	Loss 1.3543e+00 (1.3543e+00)	Acc@1  73.00 ( 73.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.079 ( 0.089)	Loss 1.3047e+00 (1.3037e+00)	Acc@1  70.00 ( 73.45)	Acc@5  94.00 ( 91.45)
Test: [ 20/100]	Time  0.077 ( 0.084)	Loss 1.2232e+00 (1.2670e+00)	Acc@1  69.00 ( 73.52)	Acc@5  93.00 ( 91.95)
Test: [ 30/100]	Time  0.077 ( 0.082)	Loss 1.5919e+00 (1.3136e+00)	Acc@1  63.00 ( 72.45)	Acc@5  94.00 ( 91.74)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.4630e+00 (1.3203e+00)	Acc@1  74.00 ( 71.98)	Acc@5  92.00 ( 92.02)
Test: [ 50/100]	Time  0.078 ( 0.080)	Loss 1.6346e+00 (1.3355e+00)	Acc@1  72.00 ( 72.04)	Acc@5  91.00 ( 91.80)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.5849e+00 (1.3144e+00)	Acc@1  67.00 ( 72.36)	Acc@5  91.00 ( 92.13)
Test: [ 70/100]	Time  0.077 ( 0.080)	Loss 1.3449e+00 (1.3146e+00)	Acc@1  68.00 ( 72.17)	Acc@5  91.00 ( 92.06)
Test: [ 80/100]	Time  0.078 ( 0.079)	Loss 1.3749e+00 (1.3069e+00)	Acc@1  69.00 ( 72.26)	Acc@5  93.00 ( 92.04)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.6114e+00 (1.2884e+00)	Acc@1  64.00 ( 72.49)	Acc@5  91.00 ( 92.13)
 * Acc@1 72.560 Acc@5 92.200
### epoch[49] execution time: 102.78170442581177
EPOCH 50
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [50][  0/391]	Time  0.384 ( 0.384)	Data  0.142 ( 0.142)	Loss 8.2591e-02 (8.2591e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [50][ 10/391]	Time  0.241 ( 0.255)	Data  0.001 ( 0.014)	Loss 4.6980e-02 (5.9937e-02)	Acc@1  99.22 ( 98.86)	Acc@5 100.00 (100.00)
Epoch: [50][ 20/391]	Time  0.241 ( 0.248)	Data  0.001 ( 0.008)	Loss 4.6973e-02 (5.7077e-02)	Acc@1  98.44 ( 98.85)	Acc@5 100.00 (100.00)
Epoch: [50][ 30/391]	Time  0.240 ( 0.246)	Data  0.001 ( 0.006)	Loss 5.5745e-02 (5.1998e-02)	Acc@1  99.22 ( 98.94)	Acc@5 100.00 (100.00)
Epoch: [50][ 40/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.005)	Loss 4.8575e-02 (5.0628e-02)	Acc@1  99.22 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [50][ 50/391]	Time  0.248 ( 0.244)	Data  0.001 ( 0.004)	Loss 5.2624e-02 (5.0898e-02)	Acc@1  99.22 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [50][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 6.4240e-02 (5.1103e-02)	Acc@1  99.22 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [50][ 70/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 5.7885e-02 (5.1774e-02)	Acc@1  98.44 ( 98.93)	Acc@5 100.00 ( 99.99)
Epoch: [50][ 80/391]	Time  0.252 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.2960e-02 (5.1401e-02)	Acc@1 100.00 ( 98.94)	Acc@5 100.00 ( 99.99)
Epoch: [50][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.6943e-02 (5.0509e-02)	Acc@1  99.22 ( 98.96)	Acc@5 100.00 ( 99.99)
Epoch: [50][100/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.6419e-02 (5.0549e-02)	Acc@1 100.00 ( 98.95)	Acc@5 100.00 ( 99.99)
Epoch: [50][110/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.2029e-02 (5.0485e-02)	Acc@1 100.00 ( 98.94)	Acc@5 100.00 ( 99.99)
Epoch: [50][120/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.2612e-02 (5.0170e-02)	Acc@1  99.22 ( 98.95)	Acc@5 100.00 ( 99.99)
Epoch: [50][130/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.3272e-02 (4.9788e-02)	Acc@1  99.22 ( 98.97)	Acc@5 100.00 ( 99.99)
Epoch: [50][140/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.6737e-02 (4.9799e-02)	Acc@1 100.00 ( 98.97)	Acc@5 100.00 ( 99.99)
Epoch: [50][150/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4429e-02 (4.9469e-02)	Acc@1 100.00 ( 98.99)	Acc@5 100.00 ( 99.99)
Epoch: [50][160/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.5791e-02 (4.9229e-02)	Acc@1  98.44 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [50][170/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.7337e-02 (4.9858e-02)	Acc@1 100.00 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [50][180/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.5091e-02 (5.0067e-02)	Acc@1  99.22 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [50][190/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6282e-02 (4.9867e-02)	Acc@1 100.00 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [50][200/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.1743e-02 (5.0243e-02)	Acc@1  99.22 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [50][210/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.5551e-02 (5.0185e-02)	Acc@1  99.22 ( 98.94)	Acc@5 100.00 (100.00)
Epoch: [50][220/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.6717e-02 (5.0335e-02)	Acc@1 100.00 ( 98.94)	Acc@5 100.00 (100.00)
Epoch: [50][230/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.7053e-02 (4.9907e-02)	Acc@1  98.44 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [50][240/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.4279e-02 (5.0020e-02)	Acc@1  96.09 ( 98.94)	Acc@5 100.00 (100.00)
Epoch: [50][250/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.4152e-02 (5.0215e-02)	Acc@1  99.22 ( 98.94)	Acc@5 100.00 (100.00)
Epoch: [50][260/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.4110e-02 (5.0289e-02)	Acc@1 100.00 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [50][270/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.4950e-02 (5.0260e-02)	Acc@1  99.22 ( 98.95)	Acc@5 100.00 ( 99.99)
Epoch: [50][280/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.9688e-02 (5.0210e-02)	Acc@1  98.44 ( 98.95)	Acc@5 100.00 ( 99.99)
Epoch: [50][290/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.0703e-02 (5.0148e-02)	Acc@1 100.00 ( 98.96)	Acc@5 100.00 ( 99.99)
Epoch: [50][300/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.1763e-02 (5.0019e-02)	Acc@1  99.22 ( 98.97)	Acc@5 100.00 ( 99.99)
Epoch: [50][310/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.0194e-02 (4.9950e-02)	Acc@1  99.22 ( 98.97)	Acc@5 100.00 ( 99.99)
Epoch: [50][320/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.4539e-02 (5.0240e-02)	Acc@1  99.22 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [50][330/391]	Time  0.253 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0138e-01 (5.0632e-02)	Acc@1  96.09 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [50][340/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.5727e-02 (5.0650e-02)	Acc@1  99.22 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [50][350/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.5668e-02 (5.0966e-02)	Acc@1  99.22 ( 98.92)	Acc@5 100.00 (100.00)
Epoch: [50][360/391]	Time  0.253 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.7920e-02 (5.0994e-02)	Acc@1  97.66 ( 98.91)	Acc@5 100.00 (100.00)
Epoch: [50][370/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.8846e-02 (5.1095e-02)	Acc@1 100.00 ( 98.91)	Acc@5 100.00 (100.00)
Epoch: [50][380/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.9922e-02 (5.1028e-02)	Acc@1  96.88 ( 98.92)	Acc@5 100.00 (100.00)
Epoch: [50][390/391]	Time  0.182 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2024e-02 (5.0879e-02)	Acc@1 100.00 ( 98.92)	Acc@5 100.00 (100.00)
## e[50] optimizer.zero_grad (sum) time: 0.4944946765899658
## e[50]       loss.backward (sum) time: 11.075852870941162
## e[50]      optimizer.step (sum) time: 47.83236742019653
## epoch[50] training(only) time: 94.77076387405396
# Switched to evaluate mode...
Test: [  0/100]	Time  0.206 ( 0.206)	Loss 1.3938e+00 (1.3938e+00)	Acc@1  74.00 ( 74.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.3778e+00 (1.3159e+00)	Acc@1  69.00 ( 73.00)	Acc@5  92.00 ( 91.27)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 1.2014e+00 (1.2706e+00)	Acc@1  70.00 ( 73.33)	Acc@5  94.00 ( 92.14)
Test: [ 30/100]	Time  0.079 ( 0.082)	Loss 1.6296e+00 (1.3242e+00)	Acc@1  63.00 ( 72.00)	Acc@5  93.00 ( 91.97)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.3974e+00 (1.3317e+00)	Acc@1  75.00 ( 71.85)	Acc@5  92.00 ( 92.17)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 1.6678e+00 (1.3492e+00)	Acc@1  70.00 ( 71.73)	Acc@5  91.00 ( 91.94)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.5201e+00 (1.3239e+00)	Acc@1  69.00 ( 72.08)	Acc@5  91.00 ( 92.28)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 1.3587e+00 (1.3231e+00)	Acc@1  70.00 ( 72.06)	Acc@5  92.00 ( 92.20)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 1.3525e+00 (1.3182e+00)	Acc@1  71.00 ( 72.17)	Acc@5  91.00 ( 92.19)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.7135e+00 (1.2999e+00)	Acc@1  63.00 ( 72.34)	Acc@5  89.00 ( 92.26)
 * Acc@1 72.430 Acc@5 92.310
### epoch[50] execution time: 102.78568077087402
EPOCH 51
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [51][  0/391]	Time  0.391 ( 0.391)	Data  0.148 ( 0.148)	Loss 5.1762e-02 (5.1762e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [51][ 10/391]	Time  0.241 ( 0.254)	Data  0.001 ( 0.015)	Loss 3.8720e-02 (4.8166e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [51][ 20/391]	Time  0.240 ( 0.248)	Data  0.001 ( 0.008)	Loss 5.7493e-02 (4.9244e-02)	Acc@1 100.00 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [51][ 30/391]	Time  0.246 ( 0.246)	Data  0.001 ( 0.006)	Loss 4.1825e-02 (4.6156e-02)	Acc@1  99.22 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [51][ 40/391]	Time  0.243 ( 0.245)	Data  0.001 ( 0.005)	Loss 6.6709e-02 (4.5990e-02)	Acc@1  97.66 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [51][ 50/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.004)	Loss 5.8510e-02 (4.7986e-02)	Acc@1  99.22 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [51][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 4.2068e-02 (4.6111e-02)	Acc@1  98.44 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [51][ 70/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 6.5022e-02 (4.6410e-02)	Acc@1  98.44 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [51][ 80/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.003)	Loss 4.3786e-02 (4.6172e-02)	Acc@1  98.44 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [51][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 6.0833e-02 (4.6637e-02)	Acc@1  97.66 ( 99.05)	Acc@5 100.00 ( 99.99)
Epoch: [51][100/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.7101e-02 (4.5952e-02)	Acc@1  99.22 ( 99.07)	Acc@5 100.00 ( 99.99)
Epoch: [51][110/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 4.6792e-02 (4.6513e-02)	Acc@1  99.22 ( 99.02)	Acc@5 100.00 ( 99.99)
Epoch: [51][120/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 5.8314e-02 (4.7072e-02)	Acc@1  97.66 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [51][130/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.002)	Loss 5.1147e-02 (4.7774e-02)	Acc@1  98.44 ( 98.99)	Acc@5 100.00 ( 99.99)
Epoch: [51][140/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 5.5444e-02 (4.8038e-02)	Acc@1  97.66 ( 98.98)	Acc@5 100.00 ( 99.99)
Epoch: [51][150/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.1846e-02 (4.8395e-02)	Acc@1  98.44 ( 98.97)	Acc@5  99.22 ( 99.99)
Epoch: [51][160/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.8362e-02 (4.7957e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [51][170/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.5138e-02 (4.8340e-02)	Acc@1  96.88 ( 98.97)	Acc@5 100.00 ( 99.99)
Epoch: [51][180/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.0142e-02 (4.8590e-02)	Acc@1  99.22 ( 98.96)	Acc@5 100.00 ( 99.99)
Epoch: [51][190/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.3194e-02 (4.8617e-02)	Acc@1  99.22 ( 98.97)	Acc@5 100.00 ( 99.99)
Epoch: [51][200/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.5754e-02 (4.8739e-02)	Acc@1  99.22 ( 98.97)	Acc@5 100.00 ( 99.98)
Epoch: [51][210/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.9054e-02 (4.8606e-02)	Acc@1 100.00 ( 98.98)	Acc@5 100.00 ( 99.98)
Epoch: [51][220/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.2112e-02 (4.9265e-02)	Acc@1  97.66 ( 98.94)	Acc@5 100.00 ( 99.98)
Epoch: [51][230/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.6948e-02 (4.8877e-02)	Acc@1  97.66 ( 98.94)	Acc@5 100.00 ( 99.98)
Epoch: [51][240/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.3039e-02 (4.8818e-02)	Acc@1  99.22 ( 98.94)	Acc@5 100.00 ( 99.98)
Epoch: [51][250/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.3918e-02 (4.8732e-02)	Acc@1 100.00 ( 98.96)	Acc@5 100.00 ( 99.98)
Epoch: [51][260/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.4646e-02 (4.8459e-02)	Acc@1  97.66 ( 98.96)	Acc@5 100.00 ( 99.99)
Epoch: [51][270/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.1132e-02 (4.8547e-02)	Acc@1  99.22 ( 98.96)	Acc@5  99.22 ( 99.98)
Epoch: [51][280/391]	Time  0.255 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.6708e-02 (4.8777e-02)	Acc@1  96.88 ( 98.95)	Acc@5 100.00 ( 99.98)
Epoch: [51][290/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.7175e-02 (4.8857e-02)	Acc@1  98.44 ( 98.94)	Acc@5 100.00 ( 99.98)
Epoch: [51][300/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.4781e-02 (4.9154e-02)	Acc@1  98.44 ( 98.93)	Acc@5 100.00 ( 99.98)
Epoch: [51][310/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.4990e-02 (4.9555e-02)	Acc@1  98.44 ( 98.91)	Acc@5 100.00 ( 99.98)
Epoch: [51][320/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.2135e-02 (4.9649e-02)	Acc@1  98.44 ( 98.91)	Acc@5 100.00 ( 99.99)
Epoch: [51][330/391]	Time  0.248 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.1239e-02 (4.9631e-02)	Acc@1  99.22 ( 98.91)	Acc@5 100.00 ( 99.99)
Epoch: [51][340/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.7793e-02 (4.9656e-02)	Acc@1  99.22 ( 98.90)	Acc@5 100.00 ( 99.99)
Epoch: [51][350/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.3226e-02 (4.9658e-02)	Acc@1  98.44 ( 98.90)	Acc@5 100.00 ( 99.99)
Epoch: [51][360/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.5492e-02 (4.9760e-02)	Acc@1  99.22 ( 98.90)	Acc@5 100.00 ( 99.99)
Epoch: [51][370/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.5235e-02 (4.9737e-02)	Acc@1 100.00 ( 98.90)	Acc@5 100.00 ( 99.99)
Epoch: [51][380/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.0969e-02 (4.9665e-02)	Acc@1 100.00 ( 98.90)	Acc@5 100.00 ( 99.99)
Epoch: [51][390/391]	Time  0.181 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.1573e-02 (4.9968e-02)	Acc@1  98.75 ( 98.88)	Acc@5 100.00 ( 99.99)
## e[51] optimizer.zero_grad (sum) time: 0.4934122562408447
## e[51]       loss.backward (sum) time: 11.10610556602478
## e[51]      optimizer.step (sum) time: 47.8637490272522
## epoch[51] training(only) time: 94.79470300674438
# Switched to evaluate mode...
Test: [  0/100]	Time  0.212 ( 0.212)	Loss 1.2957e+00 (1.2957e+00)	Acc@1  74.00 ( 74.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.4176e+00 (1.3369e+00)	Acc@1  71.00 ( 73.36)	Acc@5  93.00 ( 91.45)
Test: [ 20/100]	Time  0.077 ( 0.084)	Loss 1.2256e+00 (1.2867e+00)	Acc@1  69.00 ( 73.10)	Acc@5  94.00 ( 92.33)
Test: [ 30/100]	Time  0.077 ( 0.082)	Loss 1.5482e+00 (1.3371e+00)	Acc@1  65.00 ( 71.94)	Acc@5  93.00 ( 91.90)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.4844e+00 (1.3440e+00)	Acc@1  75.00 ( 71.56)	Acc@5  94.00 ( 92.15)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 1.6998e+00 (1.3638e+00)	Acc@1  69.00 ( 71.69)	Acc@5  89.00 ( 91.90)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.4821e+00 (1.3374e+00)	Acc@1  68.00 ( 72.08)	Acc@5  91.00 ( 92.25)
Test: [ 70/100]	Time  0.077 ( 0.080)	Loss 1.3628e+00 (1.3365e+00)	Acc@1  70.00 ( 72.10)	Acc@5  92.00 ( 92.14)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 1.3422e+00 (1.3284e+00)	Acc@1  71.00 ( 72.23)	Acc@5  91.00 ( 92.14)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.7153e+00 (1.3096e+00)	Acc@1  66.00 ( 72.36)	Acc@5  89.00 ( 92.20)
 * Acc@1 72.470 Acc@5 92.240
### epoch[51] execution time: 102.80970120429993
EPOCH 52
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [52][  0/391]	Time  0.386 ( 0.386)	Data  0.143 ( 0.143)	Loss 4.6770e-02 (4.6770e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [52][ 10/391]	Time  0.242 ( 0.255)	Data  0.001 ( 0.014)	Loss 2.3777e-02 (4.4148e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [52][ 20/391]	Time  0.241 ( 0.248)	Data  0.001 ( 0.008)	Loss 7.0869e-02 (4.8539e-02)	Acc@1  97.66 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [52][ 30/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.006)	Loss 6.3068e-02 (4.6487e-02)	Acc@1  96.88 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [52][ 40/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.005)	Loss 8.0151e-02 (4.7113e-02)	Acc@1  97.66 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [52][ 50/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 2.2985e-02 (4.9411e-02)	Acc@1 100.00 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [52][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 3.6895e-02 (4.9301e-02)	Acc@1 100.00 ( 98.92)	Acc@5 100.00 (100.00)
Epoch: [52][ 70/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.9457e-02 (4.8835e-02)	Acc@1 100.00 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [52][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.3479e-02 (4.9018e-02)	Acc@1  99.22 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [52][ 90/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.4290e-02 (4.8385e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [52][100/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 5.5599e-02 (4.8286e-02)	Acc@1  97.66 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [52][110/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.5555e-02 (4.7373e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [52][120/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.7297e-02 (4.7609e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [52][130/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 6.2177e-02 (4.8353e-02)	Acc@1  98.44 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [52][140/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.7583e-02 (4.8800e-02)	Acc@1  98.44 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [52][150/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.6886e-02 (4.8763e-02)	Acc@1  97.66 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [52][160/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 8.5503e-02 (4.8444e-02)	Acc@1  97.66 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [52][170/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.9832e-02 (4.8489e-02)	Acc@1  99.22 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [52][180/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.0161e-02 (4.8221e-02)	Acc@1 100.00 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [52][190/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.8751e-02 (4.8851e-02)	Acc@1  99.22 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [52][200/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.5036e-02 (4.8926e-02)	Acc@1 100.00 ( 98.95)	Acc@5 100.00 ( 99.99)
Epoch: [52][210/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.2303e-02 (4.9157e-02)	Acc@1  96.88 ( 98.92)	Acc@5 100.00 ( 99.99)
Epoch: [52][220/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.8021e-02 (4.9109e-02)	Acc@1 100.00 ( 98.94)	Acc@5 100.00 ( 99.99)
Epoch: [52][230/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.5668e-02 (4.9257e-02)	Acc@1  99.22 ( 98.92)	Acc@5 100.00 ( 99.99)
Epoch: [52][240/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.3148e-02 (4.9335e-02)	Acc@1  97.66 ( 98.92)	Acc@5 100.00 ( 99.99)
Epoch: [52][250/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.7353e-02 (4.9155e-02)	Acc@1  98.44 ( 98.93)	Acc@5 100.00 ( 99.99)
Epoch: [52][260/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.2658e-02 (4.8867e-02)	Acc@1  98.44 ( 98.94)	Acc@5 100.00 ( 99.99)
Epoch: [52][270/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.1564e-02 (4.8935e-02)	Acc@1  98.44 ( 98.94)	Acc@5 100.00 ( 99.99)
Epoch: [52][280/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.2483e-02 (4.8807e-02)	Acc@1 100.00 ( 98.94)	Acc@5 100.00 ( 99.99)
Epoch: [52][290/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4020e-02 (4.8764e-02)	Acc@1 100.00 ( 98.94)	Acc@5 100.00 ( 99.99)
Epoch: [52][300/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.7788e-02 (4.8425e-02)	Acc@1  98.44 ( 98.95)	Acc@5 100.00 ( 99.99)
Epoch: [52][310/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.6901e-02 (4.8380e-02)	Acc@1  99.22 ( 98.96)	Acc@5 100.00 ( 99.99)
Epoch: [52][320/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.2445e-02 (4.8331e-02)	Acc@1  99.22 ( 98.95)	Acc@5 100.00 ( 99.99)
Epoch: [52][330/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.5118e-02 (4.8324e-02)	Acc@1  98.44 ( 98.96)	Acc@5 100.00 ( 99.99)
Epoch: [52][340/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.4093e-02 (4.8681e-02)	Acc@1  98.44 ( 98.95)	Acc@5 100.00 ( 99.99)
Epoch: [52][350/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.2405e-02 (4.8672e-02)	Acc@1  99.22 ( 98.95)	Acc@5 100.00 ( 99.99)
Epoch: [52][360/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.0848e-02 (4.9161e-02)	Acc@1  99.22 ( 98.94)	Acc@5 100.00 ( 99.99)
Epoch: [52][370/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.4018e-02 (4.8986e-02)	Acc@1 100.00 ( 98.95)	Acc@5 100.00 ( 99.99)
Epoch: [52][380/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.2994e-02 (4.9174e-02)	Acc@1  98.44 ( 98.94)	Acc@5 100.00 ( 99.99)
Epoch: [52][390/391]	Time  0.179 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.6120e-02 (4.9108e-02)	Acc@1  98.75 ( 98.94)	Acc@5 100.00 ( 99.99)
## e[52] optimizer.zero_grad (sum) time: 0.49410057067871094
## e[52]       loss.backward (sum) time: 11.165247440338135
## e[52]      optimizer.step (sum) time: 47.77627086639404
## epoch[52] training(only) time: 94.75204539299011
# Switched to evaluate mode...
Test: [  0/100]	Time  0.215 ( 0.215)	Loss 1.3616e+00 (1.3616e+00)	Acc@1  73.00 ( 73.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.4505e+00 (1.3499e+00)	Acc@1  69.00 ( 72.82)	Acc@5  93.00 ( 91.55)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 1.2948e+00 (1.3011e+00)	Acc@1  67.00 ( 73.00)	Acc@5  94.00 ( 92.14)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 1.5861e+00 (1.3405e+00)	Acc@1  66.00 ( 72.42)	Acc@5  91.00 ( 91.84)
Test: [ 40/100]	Time  0.077 ( 0.081)	Loss 1.4818e+00 (1.3511e+00)	Acc@1  73.00 ( 71.90)	Acc@5  94.00 ( 92.15)
Test: [ 50/100]	Time  0.078 ( 0.080)	Loss 1.6487e+00 (1.3666e+00)	Acc@1  72.00 ( 71.80)	Acc@5  90.00 ( 91.80)
Test: [ 60/100]	Time  0.079 ( 0.080)	Loss 1.5321e+00 (1.3400e+00)	Acc@1  68.00 ( 72.11)	Acc@5  91.00 ( 92.15)
Test: [ 70/100]	Time  0.079 ( 0.080)	Loss 1.4185e+00 (1.3445e+00)	Acc@1  66.00 ( 71.93)	Acc@5  88.00 ( 91.96)
Test: [ 80/100]	Time  0.079 ( 0.080)	Loss 1.3394e+00 (1.3358e+00)	Acc@1  71.00 ( 72.12)	Acc@5  92.00 ( 92.00)
Test: [ 90/100]	Time  0.077 ( 0.079)	Loss 1.7363e+00 (1.3181e+00)	Acc@1  65.00 ( 72.37)	Acc@5  90.00 ( 92.10)
 * Acc@1 72.550 Acc@5 92.150
### epoch[52] execution time: 102.76957869529724
EPOCH 53
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [53][  0/391]	Time  0.396 ( 0.396)	Data  0.154 ( 0.154)	Loss 2.8770e-02 (2.8770e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [53][ 10/391]	Time  0.240 ( 0.255)	Data  0.001 ( 0.015)	Loss 1.0853e-01 (5.1244e-02)	Acc@1  98.44 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [53][ 20/391]	Time  0.243 ( 0.249)	Data  0.001 ( 0.008)	Loss 6.2234e-02 (4.8113e-02)	Acc@1  97.66 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [53][ 30/391]	Time  0.251 ( 0.246)	Data  0.001 ( 0.006)	Loss 3.1487e-02 (4.7559e-02)	Acc@1  99.22 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [53][ 40/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.005)	Loss 6.9060e-02 (4.6737e-02)	Acc@1  98.44 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [53][ 50/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 3.2498e-02 (4.5578e-02)	Acc@1  98.44 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [53][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 4.3171e-02 (4.5698e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [53][ 70/391]	Time  0.252 ( 0.244)	Data  0.001 ( 0.003)	Loss 3.0753e-02 (4.6487e-02)	Acc@1  98.44 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [53][ 80/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.003)	Loss 3.8786e-02 (4.7122e-02)	Acc@1  99.22 ( 98.94)	Acc@5 100.00 ( 99.99)
Epoch: [53][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.2027e-02 (4.6427e-02)	Acc@1 100.00 ( 98.98)	Acc@5 100.00 ( 99.99)
Epoch: [53][100/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.7333e-02 (4.5834e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 ( 99.99)
Epoch: [53][110/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.6749e-02 (4.4981e-02)	Acc@1 100.00 ( 99.03)	Acc@5 100.00 ( 99.99)
Epoch: [53][120/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.6445e-02 (4.5415e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 ( 99.99)
Epoch: [53][130/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.2232e-02 (4.4706e-02)	Acc@1 100.00 ( 99.03)	Acc@5 100.00 ( 99.99)
Epoch: [53][140/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 7.1993e-02 (4.4553e-02)	Acc@1  98.44 ( 99.03)	Acc@5 100.00 ( 99.99)
Epoch: [53][150/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.6891e-02 (4.4702e-02)	Acc@1  99.22 ( 99.02)	Acc@5 100.00 ( 99.99)
Epoch: [53][160/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.8637e-02 (4.4725e-02)	Acc@1  99.22 ( 99.02)	Acc@5 100.00 ( 99.99)
Epoch: [53][170/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.2107e-02 (4.4646e-02)	Acc@1  99.22 ( 99.02)	Acc@5 100.00 ( 99.99)
Epoch: [53][180/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.7534e-02 (4.4512e-02)	Acc@1 100.00 ( 99.03)	Acc@5 100.00 ( 99.99)
Epoch: [53][190/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 6.0216e-02 (4.4490e-02)	Acc@1  98.44 ( 99.04)	Acc@5 100.00 ( 99.99)
Epoch: [53][200/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.6376e-02 (4.4345e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 ( 99.99)
Epoch: [53][210/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.6268e-02 (4.4358e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 ( 99.99)
Epoch: [53][220/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.9550e-02 (4.4275e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 ( 99.99)
Epoch: [53][230/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.3405e-02 (4.4127e-02)	Acc@1  98.44 ( 99.05)	Acc@5 100.00 ( 99.99)
Epoch: [53][240/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.5458e-02 (4.4048e-02)	Acc@1 100.00 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [53][250/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.8512e-02 (4.4014e-02)	Acc@1  98.44 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [53][260/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.9072e-02 (4.4214e-02)	Acc@1 100.00 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [53][270/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.9278e-02 (4.4074e-02)	Acc@1  99.22 ( 99.07)	Acc@5 100.00 ( 99.99)
Epoch: [53][280/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.9669e-02 (4.4164e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 ( 99.99)
Epoch: [53][290/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.1927e-02 (4.4623e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 ( 99.99)
Epoch: [53][300/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.3062e-02 (4.4728e-02)	Acc@1 100.00 ( 99.04)	Acc@5 100.00 ( 99.99)
Epoch: [53][310/391]	Time  0.251 ( 0.243)	Data  0.001 ( 0.002)	Loss 6.1539e-02 (4.4593e-02)	Acc@1  98.44 ( 99.05)	Acc@5 100.00 ( 99.99)
Epoch: [53][320/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.0344e-02 (4.4842e-02)	Acc@1  98.44 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [53][330/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.7737e-02 (4.4942e-02)	Acc@1  98.44 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [53][340/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.5714e-02 (4.4741e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [53][350/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 7.9905e-02 (4.4969e-02)	Acc@1  96.88 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [53][360/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.6975e-02 (4.5123e-02)	Acc@1  98.44 ( 99.02)	Acc@5 100.00 ( 99.99)
Epoch: [53][370/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.5686e-02 (4.5151e-02)	Acc@1  99.22 ( 99.02)	Acc@5 100.00 ( 99.99)
Epoch: [53][380/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.3892e-02 (4.5392e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 ( 99.99)
Epoch: [53][390/391]	Time  0.178 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.3380e-02 (4.5420e-02)	Acc@1  98.75 ( 99.00)	Acc@5 100.00 ( 99.99)
## e[53] optimizer.zero_grad (sum) time: 0.49358272552490234
## e[53]       loss.backward (sum) time: 11.129858493804932
## e[53]      optimizer.step (sum) time: 47.82296895980835
## epoch[53] training(only) time: 94.9119815826416
# Switched to evaluate mode...
Test: [  0/100]	Time  0.208 ( 0.208)	Loss 1.3747e+00 (1.3747e+00)	Acc@1  73.00 ( 73.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.077 ( 0.090)	Loss 1.4439e+00 (1.3702e+00)	Acc@1  70.00 ( 73.00)	Acc@5  91.00 ( 91.09)
Test: [ 20/100]	Time  0.079 ( 0.084)	Loss 1.2263e+00 (1.3042e+00)	Acc@1  69.00 ( 72.76)	Acc@5  93.00 ( 92.19)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 1.6264e+00 (1.3499e+00)	Acc@1  66.00 ( 71.74)	Acc@5  93.00 ( 91.90)
Test: [ 40/100]	Time  0.079 ( 0.081)	Loss 1.5451e+00 (1.3658e+00)	Acc@1  70.00 ( 71.10)	Acc@5  93.00 ( 92.00)
Test: [ 50/100]	Time  0.078 ( 0.080)	Loss 1.6488e+00 (1.3888e+00)	Acc@1  69.00 ( 71.12)	Acc@5  90.00 ( 91.61)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.5765e+00 (1.3623e+00)	Acc@1  67.00 ( 71.56)	Acc@5  89.00 ( 91.98)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 1.4165e+00 (1.3634e+00)	Acc@1  69.00 ( 71.62)	Acc@5  88.00 ( 91.92)
Test: [ 80/100]	Time  0.079 ( 0.080)	Loss 1.3937e+00 (1.3575e+00)	Acc@1  70.00 ( 71.67)	Acc@5  91.00 ( 91.93)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.7347e+00 (1.3398e+00)	Acc@1  64.00 ( 71.89)	Acc@5  90.00 ( 91.97)
 * Acc@1 71.980 Acc@5 91.970
### epoch[53] execution time: 102.92989540100098
EPOCH 54
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [54][  0/391]	Time  0.391 ( 0.391)	Data  0.148 ( 0.148)	Loss 3.6347e-02 (3.6347e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [54][ 10/391]	Time  0.240 ( 0.254)	Data  0.001 ( 0.015)	Loss 5.5528e-02 (4.1840e-02)	Acc@1  97.66 ( 99.29)	Acc@5 100.00 ( 99.93)
Epoch: [54][ 20/391]	Time  0.241 ( 0.248)	Data  0.001 ( 0.008)	Loss 1.8774e-02 (4.0537e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 ( 99.96)
Epoch: [54][ 30/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.006)	Loss 3.3658e-02 (4.1478e-02)	Acc@1  98.44 ( 99.24)	Acc@5 100.00 ( 99.97)
Epoch: [54][ 40/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.005)	Loss 8.3999e-02 (4.2664e-02)	Acc@1  97.66 ( 99.10)	Acc@5 100.00 ( 99.98)
Epoch: [54][ 50/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 3.0259e-02 (4.2520e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 ( 99.98)
Epoch: [54][ 60/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.004)	Loss 2.1510e-02 (4.1600e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [54][ 70/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.9340e-02 (4.0585e-02)	Acc@1  99.22 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [54][ 80/391]	Time  0.239 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.1270e-02 (4.0975e-02)	Acc@1 100.00 ( 99.12)	Acc@5 100.00 ( 99.99)
Epoch: [54][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.8997e-02 (4.0809e-02)	Acc@1 100.00 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [54][100/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.0396e-02 (4.0865e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [54][110/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.8552e-02 (4.0321e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 ( 99.99)
Epoch: [54][120/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.5986e-02 (4.0511e-02)	Acc@1  98.44 ( 99.18)	Acc@5 100.00 ( 99.99)
Epoch: [54][130/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.8198e-02 (4.0430e-02)	Acc@1  98.44 ( 99.17)	Acc@5 100.00 ( 99.99)
Epoch: [54][140/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4538e-02 (4.0261e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 ( 99.99)
Epoch: [54][150/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.0543e-02 (4.0505e-02)	Acc@1  97.66 ( 99.16)	Acc@5 100.00 ( 99.99)
Epoch: [54][160/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.1460e-02 (4.0402e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [54][170/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.0805e-02 (3.9652e-02)	Acc@1 100.00 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [54][180/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.7387e-02 (3.9949e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [54][190/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.7268e-02 (4.0221e-02)	Acc@1  99.22 ( 99.21)	Acc@5 100.00 ( 99.99)
Epoch: [54][200/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.9224e-02 (4.0172e-02)	Acc@1  99.22 ( 99.21)	Acc@5 100.00 ( 99.99)
Epoch: [54][210/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.2339e-02 (4.0214e-02)	Acc@1  97.66 ( 99.21)	Acc@5 100.00 ( 99.99)
Epoch: [54][220/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.3541e-02 (4.0060e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 ( 99.99)
Epoch: [54][230/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.8777e-02 (4.0173e-02)	Acc@1  98.44 ( 99.22)	Acc@5 100.00 ( 99.99)
Epoch: [54][240/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.2826e-02 (4.0670e-02)	Acc@1  97.66 ( 99.20)	Acc@5 100.00 ( 99.99)
Epoch: [54][250/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.7365e-02 (4.0945e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 ( 99.99)
Epoch: [54][260/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.8360e-02 (4.0863e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 ( 99.99)
Epoch: [54][270/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9368e-02 (4.0689e-02)	Acc@1 100.00 ( 99.21)	Acc@5 100.00 ( 99.99)
Epoch: [54][280/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.4863e-02 (4.0826e-02)	Acc@1  98.44 ( 99.19)	Acc@5 100.00 ( 99.99)
Epoch: [54][290/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.8246e-02 (4.0851e-02)	Acc@1  98.44 ( 99.19)	Acc@5 100.00 ( 99.99)
Epoch: [54][300/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.0501e-02 (4.0464e-02)	Acc@1  99.22 ( 99.21)	Acc@5 100.00 ( 99.99)
Epoch: [54][310/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.0857e-02 (4.0561e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 ( 99.99)
Epoch: [54][320/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.7393e-02 (4.0482e-02)	Acc@1  99.22 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [54][330/391]	Time  0.246 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.2065e-02 (4.0202e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [54][340/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.3885e-02 (4.0159e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [54][350/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.6228e-02 (4.0190e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [54][360/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.2517e-02 (4.0326e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [54][370/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.3362e-02 (4.0318e-02)	Acc@1  99.22 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [54][380/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.1124e-02 (4.0177e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [54][390/391]	Time  0.178 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8751e-02 (4.0125e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
## e[54] optimizer.zero_grad (sum) time: 0.4939107894897461
## e[54]       loss.backward (sum) time: 11.109463214874268
## e[54]      optimizer.step (sum) time: 47.83227467536926
## epoch[54] training(only) time: 94.74390745162964
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 1.3958e+00 (1.3958e+00)	Acc@1  73.00 ( 73.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.078 ( 0.088)	Loss 1.4035e+00 (1.3519e+00)	Acc@1  70.00 ( 72.91)	Acc@5  93.00 ( 91.64)
Test: [ 20/100]	Time  0.077 ( 0.084)	Loss 1.2143e+00 (1.3037e+00)	Acc@1  68.00 ( 72.95)	Acc@5  96.00 ( 92.33)
Test: [ 30/100]	Time  0.079 ( 0.082)	Loss 1.5425e+00 (1.3419e+00)	Acc@1  64.00 ( 72.06)	Acc@5  90.00 ( 92.00)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.5033e+00 (1.3525e+00)	Acc@1  75.00 ( 71.78)	Acc@5  91.00 ( 92.07)
Test: [ 50/100]	Time  0.077 ( 0.080)	Loss 1.6748e+00 (1.3781e+00)	Acc@1  70.00 ( 71.59)	Acc@5  91.00 ( 91.71)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.5451e+00 (1.3506e+00)	Acc@1  67.00 ( 72.00)	Acc@5  91.00 ( 92.02)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 1.3602e+00 (1.3527e+00)	Acc@1  69.00 ( 72.08)	Acc@5  89.00 ( 91.99)
Test: [ 80/100]	Time  0.078 ( 0.079)	Loss 1.3568e+00 (1.3481e+00)	Acc@1  73.00 ( 72.19)	Acc@5  92.00 ( 91.96)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.6696e+00 (1.3290e+00)	Acc@1  65.00 ( 72.37)	Acc@5  91.00 ( 92.03)
 * Acc@1 72.420 Acc@5 92.050
### epoch[54] execution time: 102.76350259780884
EPOCH 55
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [55][  0/391]	Time  0.405 ( 0.405)	Data  0.158 ( 0.158)	Loss 5.2445e-02 (5.2445e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [55][ 10/391]	Time  0.241 ( 0.256)	Data  0.001 ( 0.015)	Loss 4.1812e-02 (3.9916e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [55][ 20/391]	Time  0.241 ( 0.250)	Data  0.001 ( 0.009)	Loss 4.0893e-02 (3.9399e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [55][ 30/391]	Time  0.242 ( 0.247)	Data  0.001 ( 0.006)	Loss 2.8235e-02 (3.8714e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [55][ 40/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.005)	Loss 7.3477e-02 (3.7860e-02)	Acc@1  96.88 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [55][ 50/391]	Time  0.242 ( 0.245)	Data  0.001 ( 0.004)	Loss 5.9573e-02 (3.9674e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [55][ 60/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.004)	Loss 4.0053e-02 (4.0227e-02)	Acc@1  98.44 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [55][ 70/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 2.6425e-02 (3.9468e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [55][ 80/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 3.5983e-02 (3.9369e-02)	Acc@1  99.22 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [55][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.0735e-02 (3.7921e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [55][100/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 4.2135e-02 (3.7770e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [55][110/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 5.2155e-02 (3.8029e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [55][120/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.9919e-02 (3.8791e-02)	Acc@1  98.44 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [55][130/391]	Time  0.250 ( 0.243)	Data  0.001 ( 0.002)	Loss 7.0414e-02 (3.9295e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [55][140/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.2636e-02 (3.9749e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [55][150/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.8704e-02 (4.0233e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 ( 99.99)
Epoch: [55][160/391]	Time  0.247 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.4026e-02 (3.9833e-02)	Acc@1  98.44 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [55][170/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.2453e-02 (4.0266e-02)	Acc@1  98.44 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [55][180/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.0935e-02 (4.0019e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [55][190/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.8088e-02 (4.0006e-02)	Acc@1  98.44 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [55][200/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.4782e-02 (3.9995e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [55][210/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.5549e-02 (3.9901e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [55][220/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.4879e-02 (3.9665e-02)	Acc@1  98.44 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [55][230/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 5.8716e-02 (3.9831e-02)	Acc@1  98.44 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [55][240/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.6652e-02 (3.9688e-02)	Acc@1  98.44 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [55][250/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.6821e-02 (3.9805e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [55][260/391]	Time  0.249 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.2674e-02 (3.9712e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [55][270/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.3357e-02 (4.0162e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 ( 99.99)
Epoch: [55][280/391]	Time  0.251 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.6487e-02 (4.0102e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 ( 99.99)
Epoch: [55][290/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.8134e-02 (4.0056e-02)	Acc@1  98.44 ( 99.17)	Acc@5 100.00 ( 99.99)
Epoch: [55][300/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.5277e-02 (3.9962e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 ( 99.99)
Epoch: [55][310/391]	Time  0.246 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.2468e-02 (3.9868e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 ( 99.99)
Epoch: [55][320/391]	Time  0.251 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.3576e-02 (3.9963e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [55][330/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.1499e-02 (4.0049e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [55][340/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.8691e-02 (4.0067e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [55][350/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.8100e-02 (4.0055e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [55][360/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.6216e-02 (4.0167e-02)	Acc@1  98.44 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [55][370/391]	Time  0.251 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.8276e-02 (4.0339e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [55][380/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.8857e-02 (4.0371e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [55][390/391]	Time  0.178 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.2292e-02 (4.0667e-02)	Acc@1  98.75 ( 99.17)	Acc@5 100.00 (100.00)
## e[55] optimizer.zero_grad (sum) time: 0.4947073459625244
## e[55]       loss.backward (sum) time: 11.081039428710938
## e[55]      optimizer.step (sum) time: 47.868361473083496
## epoch[55] training(only) time: 94.88291215896606
# Switched to evaluate mode...
Test: [  0/100]	Time  0.210 ( 0.210)	Loss 1.3749e+00 (1.3749e+00)	Acc@1  74.00 ( 74.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.3669e+00 (1.3649e+00)	Acc@1  70.00 ( 73.00)	Acc@5  92.00 ( 91.45)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 1.1474e+00 (1.3295e+00)	Acc@1  71.00 ( 72.90)	Acc@5  95.00 ( 92.05)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 1.6307e+00 (1.3748e+00)	Acc@1  65.00 ( 71.81)	Acc@5  91.00 ( 91.65)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.4674e+00 (1.3848e+00)	Acc@1  73.00 ( 71.37)	Acc@5  93.00 ( 91.85)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 1.7305e+00 (1.4047e+00)	Acc@1  70.00 ( 71.33)	Acc@5  91.00 ( 91.43)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.6113e+00 (1.3801e+00)	Acc@1  66.00 ( 71.57)	Acc@5  94.00 ( 91.80)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 1.3917e+00 (1.3829e+00)	Acc@1  69.00 ( 71.55)	Acc@5  89.00 ( 91.76)
Test: [ 80/100]	Time  0.079 ( 0.080)	Loss 1.3092e+00 (1.3760e+00)	Acc@1  74.00 ( 71.75)	Acc@5  92.00 ( 91.75)
Test: [ 90/100]	Time  0.078 ( 0.080)	Loss 1.6551e+00 (1.3525e+00)	Acc@1  64.00 ( 72.00)	Acc@5  92.00 ( 91.87)
 * Acc@1 72.000 Acc@5 91.930
### epoch[55] execution time: 102.90996646881104
EPOCH 56
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [56][  0/391]	Time  0.404 ( 0.404)	Data  0.162 ( 0.162)	Loss 3.7690e-02 (3.7690e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [56][ 10/391]	Time  0.240 ( 0.256)	Data  0.001 ( 0.016)	Loss 2.8874e-02 (4.1877e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [56][ 20/391]	Time  0.240 ( 0.249)	Data  0.001 ( 0.009)	Loss 3.1592e-02 (4.4875e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 ( 99.96)
Epoch: [56][ 30/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.006)	Loss 3.5745e-02 (4.2065e-02)	Acc@1  98.44 ( 99.22)	Acc@5 100.00 ( 99.97)
Epoch: [56][ 40/391]	Time  0.242 ( 0.245)	Data  0.001 ( 0.005)	Loss 2.1957e-02 (3.9415e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 ( 99.98)
Epoch: [56][ 50/391]	Time  0.251 ( 0.245)	Data  0.001 ( 0.004)	Loss 3.1409e-02 (3.9548e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 ( 99.98)
Epoch: [56][ 60/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.004)	Loss 2.7204e-02 (3.9264e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 ( 99.99)
Epoch: [56][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.5011e-02 (3.9098e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 ( 99.99)
Epoch: [56][ 80/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.9771e-02 (3.8190e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 ( 99.99)
Epoch: [56][ 90/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 4.9332e-02 (3.9213e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 ( 99.99)
Epoch: [56][100/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.5353e-02 (3.9282e-02)	Acc@1  98.44 ( 99.25)	Acc@5 100.00 ( 99.99)
Epoch: [56][110/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.8530e-02 (3.8993e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 ( 99.99)
Epoch: [56][120/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.8507e-02 (3.9180e-02)	Acc@1  99.22 ( 99.24)	Acc@5 100.00 ( 99.99)
Epoch: [56][130/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.2330e-02 (3.8329e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 ( 99.99)
Epoch: [56][140/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.4462e-02 (3.7865e-02)	Acc@1  98.44 ( 99.26)	Acc@5 100.00 ( 99.99)
Epoch: [56][150/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.5008e-02 (3.8224e-02)	Acc@1  98.44 ( 99.22)	Acc@5 100.00 ( 99.99)
Epoch: [56][160/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.4522e-02 (3.8146e-02)	Acc@1  99.22 ( 99.21)	Acc@5 100.00 ( 99.99)
Epoch: [56][170/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.0659e-02 (3.8246e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 ( 99.99)
Epoch: [56][180/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.9259e-02 (3.8382e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 ( 99.99)
Epoch: [56][190/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.8191e-02 (3.8692e-02)	Acc@1  96.88 ( 99.20)	Acc@5 100.00 ( 99.99)
Epoch: [56][200/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.3425e-02 (3.8691e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 ( 99.99)
Epoch: [56][210/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.9752e-02 (3.8806e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 ( 99.99)
Epoch: [56][220/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8188e-02 (3.8638e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 ( 99.99)
Epoch: [56][230/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3983e-02 (3.8694e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 ( 99.99)
Epoch: [56][240/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.3788e-02 (3.8855e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 ( 99.99)
Epoch: [56][250/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8613e-02 (3.8715e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 ( 99.99)
Epoch: [56][260/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.6437e-02 (3.8892e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 ( 99.99)
Epoch: [56][270/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3748e-02 (3.8765e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 ( 99.99)
Epoch: [56][280/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.0366e-02 (3.8997e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 ( 99.99)
Epoch: [56][290/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.7209e-02 (3.9086e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 ( 99.99)
Epoch: [56][300/391]	Time  0.250 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.6715e-02 (3.9180e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 ( 99.99)
Epoch: [56][310/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.2268e-02 (3.9205e-02)	Acc@1  97.66 ( 99.18)	Acc@5 100.00 ( 99.99)
Epoch: [56][320/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.4388e-02 (3.9415e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [56][330/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.4361e-02 (3.9398e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [56][340/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2635e-02 (3.9379e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [56][350/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7545e-02 (3.9195e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [56][360/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.4413e-02 (3.9204e-02)	Acc@1  96.88 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [56][370/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.1579e-02 (3.9243e-02)	Acc@1  98.44 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [56][380/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.6289e-02 (3.9058e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [56][390/391]	Time  0.178 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6064e-02 (3.8978e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
## e[56] optimizer.zero_grad (sum) time: 0.4941220283508301
## e[56]       loss.backward (sum) time: 11.110583543777466
## e[56]      optimizer.step (sum) time: 47.8151113986969
## epoch[56] training(only) time: 94.76100468635559
# Switched to evaluate mode...
Test: [  0/100]	Time  0.211 ( 0.211)	Loss 1.4125e+00 (1.4125e+00)	Acc@1  75.00 ( 75.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.077 ( 0.090)	Loss 1.4419e+00 (1.3748e+00)	Acc@1  67.00 ( 72.73)	Acc@5  92.00 ( 91.45)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 1.2373e+00 (1.3358e+00)	Acc@1  69.00 ( 72.86)	Acc@5  93.00 ( 92.24)
Test: [ 30/100]	Time  0.077 ( 0.082)	Loss 1.6249e+00 (1.3799e+00)	Acc@1  63.00 ( 71.84)	Acc@5  92.00 ( 91.81)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.4534e+00 (1.3873e+00)	Acc@1  74.00 ( 71.41)	Acc@5  93.00 ( 92.02)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 1.7719e+00 (1.4031e+00)	Acc@1  70.00 ( 71.45)	Acc@5  89.00 ( 91.63)
Test: [ 60/100]	Time  0.079 ( 0.080)	Loss 1.5673e+00 (1.3831e+00)	Acc@1  67.00 ( 71.61)	Acc@5  90.00 ( 91.93)
Test: [ 70/100]	Time  0.079 ( 0.080)	Loss 1.4032e+00 (1.3852e+00)	Acc@1  71.00 ( 71.68)	Acc@5  90.00 ( 91.93)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 1.3051e+00 (1.3793e+00)	Acc@1  73.00 ( 71.90)	Acc@5  92.00 ( 91.96)
Test: [ 90/100]	Time  0.079 ( 0.079)	Loss 1.6575e+00 (1.3595e+00)	Acc@1  67.00 ( 72.18)	Acc@5  90.00 ( 92.04)
 * Acc@1 72.380 Acc@5 92.100
### epoch[56] execution time: 102.775390625
EPOCH 57
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [57][  0/391]	Time  0.405 ( 0.405)	Data  0.151 ( 0.151)	Loss 1.4528e-02 (1.4528e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [57][ 10/391]	Time  0.240 ( 0.256)	Data  0.001 ( 0.015)	Loss 3.8888e-02 (3.3031e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [57][ 20/391]	Time  0.241 ( 0.249)	Data  0.001 ( 0.008)	Loss 3.3111e-02 (3.4664e-02)	Acc@1  98.44 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [57][ 30/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.006)	Loss 1.9940e-02 (3.4076e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [57][ 40/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.005)	Loss 2.0418e-02 (3.3319e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [57][ 50/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 5.1481e-02 (3.4081e-02)	Acc@1  98.44 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [57][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 3.6105e-02 (3.3883e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [57][ 70/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 5.8910e-02 (3.4843e-02)	Acc@1  98.44 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [57][ 80/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.5505e-02 (3.5232e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [57][ 90/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.2856e-02 (3.5295e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [57][100/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.7614e-02 (3.5375e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [57][110/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.8866e-02 (3.5289e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [57][120/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 6.2732e-02 (3.5336e-02)	Acc@1  97.66 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [57][130/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.2626e-02 (3.6297e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [57][140/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 5.7893e-02 (3.5859e-02)	Acc@1  98.44 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [57][150/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8361e-02 (3.6172e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [57][160/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8668e-02 (3.5794e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [57][170/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.8271e-02 (3.5782e-02)	Acc@1  98.44 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [57][180/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.1548e-02 (3.5567e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [57][190/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.5692e-02 (3.5696e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [57][200/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9481e-02 (3.5818e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [57][210/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0310e-02 (3.5818e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [57][220/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.7469e-02 (3.5800e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [57][230/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.1504e-02 (3.5677e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [57][240/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.3574e-02 (3.5958e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [57][250/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.3641e-02 (3.5846e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [57][260/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3297e-02 (3.5968e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [57][270/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.6125e-02 (3.5874e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [57][280/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.7189e-02 (3.5971e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [57][290/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.8238e-02 (3.5990e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [57][300/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.7956e-02 (3.5984e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [57][310/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.6127e-02 (3.5937e-02)	Acc@1  97.66 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [57][320/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8570e-02 (3.5740e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [57][330/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.9501e-02 (3.5876e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [57][340/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.6270e-02 (3.5795e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [57][350/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.2289e-02 (3.5855e-02)	Acc@1  98.44 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [57][360/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.4210e-02 (3.5894e-02)	Acc@1  98.44 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [57][370/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.5075e-02 (3.5921e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [57][380/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7437e-02 (3.6142e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [57][390/391]	Time  0.183 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.1248e-02 (3.6158e-02)	Acc@1  97.50 ( 99.32)	Acc@5 100.00 (100.00)
## e[57] optimizer.zero_grad (sum) time: 0.49416637420654297
## e[57]       loss.backward (sum) time: 11.092756509780884
## e[57]      optimizer.step (sum) time: 47.87000846862793
## epoch[57] training(only) time: 94.71943807601929
# Switched to evaluate mode...
Test: [  0/100]	Time  0.240 ( 0.240)	Loss 1.3839e+00 (1.3839e+00)	Acc@1  75.00 ( 75.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.078 ( 0.092)	Loss 1.4266e+00 (1.3913e+00)	Acc@1  70.00 ( 72.64)	Acc@5  92.00 ( 91.45)
Test: [ 20/100]	Time  0.077 ( 0.086)	Loss 1.2347e+00 (1.3276e+00)	Acc@1  70.00 ( 72.95)	Acc@5  95.00 ( 92.10)
Test: [ 30/100]	Time  0.078 ( 0.083)	Loss 1.6203e+00 (1.3791e+00)	Acc@1  64.00 ( 71.90)	Acc@5  92.00 ( 91.87)
Test: [ 40/100]	Time  0.079 ( 0.082)	Loss 1.4659e+00 (1.3888e+00)	Acc@1  74.00 ( 71.41)	Acc@5  93.00 ( 92.00)
Test: [ 50/100]	Time  0.079 ( 0.081)	Loss 1.7418e+00 (1.4090e+00)	Acc@1  72.00 ( 71.47)	Acc@5  90.00 ( 91.63)
Test: [ 60/100]	Time  0.077 ( 0.081)	Loss 1.5496e+00 (1.3843e+00)	Acc@1  69.00 ( 71.69)	Acc@5  93.00 ( 92.05)
Test: [ 70/100]	Time  0.079 ( 0.080)	Loss 1.4242e+00 (1.3863e+00)	Acc@1  69.00 ( 71.70)	Acc@5  88.00 ( 91.97)
Test: [ 80/100]	Time  0.077 ( 0.080)	Loss 1.3277e+00 (1.3774e+00)	Acc@1  70.00 ( 71.83)	Acc@5  93.00 ( 92.02)
Test: [ 90/100]	Time  0.078 ( 0.080)	Loss 1.6367e+00 (1.3527e+00)	Acc@1  67.00 ( 72.20)	Acc@5  88.00 ( 92.09)
 * Acc@1 72.290 Acc@5 92.130
### epoch[57] execution time: 102.7672827243805
EPOCH 58
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [58][  0/391]	Time  0.396 ( 0.396)	Data  0.132 ( 0.132)	Loss 2.8673e-02 (2.8673e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [58][ 10/391]	Time  0.241 ( 0.254)	Data  0.001 ( 0.013)	Loss 4.4519e-02 (3.3575e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [58][ 20/391]	Time  0.241 ( 0.248)	Data  0.001 ( 0.007)	Loss 2.5509e-02 (3.4155e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [58][ 30/391]	Time  0.246 ( 0.246)	Data  0.001 ( 0.005)	Loss 3.9305e-02 (3.2924e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [58][ 40/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.004)	Loss 2.9361e-02 (3.2905e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [58][ 50/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.4509e-02 (3.2395e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [58][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 1.9841e-02 (3.2507e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [58][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.5686e-02 (3.2298e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [58][ 80/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.4363e-02 (3.3054e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 ( 99.98)
Epoch: [58][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 5.0902e-02 (3.3052e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 ( 99.98)
Epoch: [58][100/391]	Time  0.247 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.6307e-02 (3.2647e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 ( 99.98)
Epoch: [58][110/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.9450e-02 (3.2786e-02)	Acc@1  98.44 ( 99.37)	Acc@5 100.00 ( 99.99)
Epoch: [58][120/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.5807e-02 (3.2416e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 ( 99.99)
Epoch: [58][130/391]	Time  0.250 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.4411e-02 (3.1899e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 ( 99.99)
Epoch: [58][140/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.9959e-02 (3.1920e-02)	Acc@1  98.44 ( 99.42)	Acc@5 100.00 ( 99.99)
Epoch: [58][150/391]	Time  0.248 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.6504e-02 (3.1991e-02)	Acc@1  98.44 ( 99.43)	Acc@5 100.00 ( 99.99)
Epoch: [58][160/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.5102e-02 (3.2695e-02)	Acc@1  98.44 ( 99.38)	Acc@5 100.00 ( 99.99)
Epoch: [58][170/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4760e-02 (3.2150e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 ( 99.99)
Epoch: [58][180/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.7774e-02 (3.2273e-02)	Acc@1  98.44 ( 99.37)	Acc@5 100.00 ( 99.99)
Epoch: [58][190/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.1324e-02 (3.2341e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 ( 99.99)
Epoch: [58][200/391]	Time  0.250 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.4161e-03 (3.2788e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 ( 99.99)
Epoch: [58][210/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.4498e-02 (3.2725e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 ( 99.99)
Epoch: [58][220/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5271e-02 (3.2671e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 ( 99.99)
Epoch: [58][230/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.4609e-02 (3.2712e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 ( 99.99)
Epoch: [58][240/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0991e-02 (3.2801e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 ( 99.99)
Epoch: [58][250/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.5123e-02 (3.2923e-02)	Acc@1  98.44 ( 99.37)	Acc@5 100.00 ( 99.99)
Epoch: [58][260/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5403e-02 (3.2896e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 ( 99.99)
Epoch: [58][270/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.4648e-02 (3.2833e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 ( 99.99)
Epoch: [58][280/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.5779e-02 (3.2789e-02)	Acc@1  98.44 ( 99.39)	Acc@5 100.00 ( 99.99)
Epoch: [58][290/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6390e-02 (3.2955e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 ( 99.99)
Epoch: [58][300/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.4040e-02 (3.2992e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 ( 99.99)
Epoch: [58][310/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8418e-02 (3.2984e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 ( 99.99)
Epoch: [58][320/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.6831e-02 (3.3182e-02)	Acc@1  98.44 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [58][330/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.5524e-02 (3.3059e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [58][340/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.001)	Loss 1.3117e-02 (3.3018e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [58][350/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.001)	Loss 3.5527e-02 (3.3172e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [58][360/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.001)	Loss 4.2750e-02 (3.3349e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [58][370/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.001)	Loss 3.4678e-02 (3.3278e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [58][380/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.001)	Loss 3.9021e-02 (3.3354e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [58][390/391]	Time  0.178 ( 0.242)	Data  0.001 ( 0.001)	Loss 5.2754e-02 (3.3549e-02)	Acc@1  98.75 ( 99.37)	Acc@5 100.00 (100.00)
## e[58] optimizer.zero_grad (sum) time: 0.49608469009399414
## e[58]       loss.backward (sum) time: 11.138578653335571
## e[58]      optimizer.step (sum) time: 47.833258628845215
## epoch[58] training(only) time: 94.80464625358582
# Switched to evaluate mode...
Test: [  0/100]	Time  0.212 ( 0.212)	Loss 1.4088e+00 (1.4088e+00)	Acc@1  74.00 ( 74.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.3881e+00 (1.3701e+00)	Acc@1  69.00 ( 72.55)	Acc@5  94.00 ( 91.27)
Test: [ 20/100]	Time  0.077 ( 0.084)	Loss 1.3731e+00 (1.3176e+00)	Acc@1  67.00 ( 72.86)	Acc@5  93.00 ( 92.24)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 1.6452e+00 (1.3767e+00)	Acc@1  63.00 ( 71.48)	Acc@5  94.00 ( 91.90)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.4226e+00 (1.3878e+00)	Acc@1  80.00 ( 71.27)	Acc@5  93.00 ( 92.00)
Test: [ 50/100]	Time  0.077 ( 0.081)	Loss 1.6654e+00 (1.4058e+00)	Acc@1  71.00 ( 71.27)	Acc@5  92.00 ( 91.80)
Test: [ 60/100]	Time  0.077 ( 0.080)	Loss 1.5418e+00 (1.3768e+00)	Acc@1  67.00 ( 71.72)	Acc@5  92.00 ( 92.11)
Test: [ 70/100]	Time  0.079 ( 0.080)	Loss 1.3716e+00 (1.3813e+00)	Acc@1  70.00 ( 71.68)	Acc@5  90.00 ( 92.03)
Test: [ 80/100]	Time  0.079 ( 0.080)	Loss 1.3632e+00 (1.3740e+00)	Acc@1  72.00 ( 71.94)	Acc@5  93.00 ( 91.99)
Test: [ 90/100]	Time  0.078 ( 0.080)	Loss 1.7173e+00 (1.3514e+00)	Acc@1  64.00 ( 72.30)	Acc@5  89.00 ( 92.10)
 * Acc@1 72.500 Acc@5 92.120
### epoch[58] execution time: 102.82925868034363
EPOCH 59
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [59][  0/391]	Time  0.396 ( 0.396)	Data  0.151 ( 0.151)	Loss 2.6495e-02 (2.6495e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [59][ 10/391]	Time  0.242 ( 0.256)	Data  0.001 ( 0.015)	Loss 2.0003e-02 (3.7140e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [59][ 20/391]	Time  0.241 ( 0.249)	Data  0.001 ( 0.008)	Loss 2.9893e-02 (3.3517e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [59][ 30/391]	Time  0.240 ( 0.246)	Data  0.001 ( 0.006)	Loss 3.3212e-02 (3.2895e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [59][ 40/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.005)	Loss 4.8527e-02 (3.1147e-02)	Acc@1  98.44 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [59][ 50/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 2.6386e-02 (3.2800e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [59][ 60/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.8225e-02 (3.2444e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [59][ 70/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 3.5540e-02 (3.2812e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [59][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.1555e-02 (3.2258e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [59][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.7454e-02 (3.2138e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [59][100/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.0546e-02 (3.2205e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [59][110/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.6826e-02 (3.1971e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [59][120/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 5.2912e-02 (3.2584e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [59][130/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.1941e-02 (3.2205e-02)	Acc@1  98.44 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [59][140/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.6321e-02 (3.1960e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [59][150/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.7954e-02 (3.1791e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [59][160/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.0016e-02 (3.1771e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [59][170/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.1633e-02 (3.1676e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [59][180/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.002)	Loss 5.2494e-02 (3.2024e-02)	Acc@1  98.44 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [59][190/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.3804e-02 (3.2146e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [59][200/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.0537e-02 (3.2685e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [59][210/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.2040e-02 (3.2491e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [59][220/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.5273e-02 (3.2602e-02)	Acc@1  98.44 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [59][230/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.9033e-02 (3.2873e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [59][240/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.8097e-02 (3.2885e-02)	Acc@1  99.22 ( 99.39)	Acc@5  99.22 (100.00)
Epoch: [59][250/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.9296e-02 (3.2841e-02)	Acc@1  98.44 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [59][260/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.4126e-02 (3.2718e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [59][270/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.5523e-02 (3.2423e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [59][280/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.4383e-02 (3.2259e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [59][290/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.5971e-02 (3.2373e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [59][300/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.7286e-02 (3.2472e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [59][310/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.5072e-02 (3.2314e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [59][320/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.8718e-02 (3.2306e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [59][330/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.5890e-02 (3.2430e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [59][340/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.1567e-02 (3.2617e-02)	Acc@1  98.44 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [59][350/391]	Time  0.255 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.0719e-02 (3.2620e-02)	Acc@1  97.66 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [59][360/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 5.3697e-02 (3.2848e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [59][370/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 6.0240e-02 (3.3110e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [59][380/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.7051e-02 (3.3195e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [59][390/391]	Time  0.178 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.7795e-02 (3.3144e-02)	Acc@1  98.75 ( 99.40)	Acc@5 100.00 (100.00)
## e[59] optimizer.zero_grad (sum) time: 0.49448060989379883
## e[59]       loss.backward (sum) time: 11.167344093322754
## e[59]      optimizer.step (sum) time: 47.83034563064575
## epoch[59] training(only) time: 94.8395357131958
# Switched to evaluate mode...
Test: [  0/100]	Time  0.206 ( 0.206)	Loss 1.4078e+00 (1.4078e+00)	Acc@1  75.00 ( 75.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.3458e+00 (1.3721e+00)	Acc@1  67.00 ( 72.82)	Acc@5  93.00 ( 91.36)
Test: [ 20/100]	Time  0.077 ( 0.084)	Loss 1.3554e+00 (1.3400e+00)	Acc@1  66.00 ( 72.76)	Acc@5  91.00 ( 91.95)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 1.5509e+00 (1.3888e+00)	Acc@1  65.00 ( 71.90)	Acc@5  92.00 ( 91.68)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.5027e+00 (1.3991e+00)	Acc@1  73.00 ( 71.46)	Acc@5  93.00 ( 91.76)
Test: [ 50/100]	Time  0.078 ( 0.080)	Loss 1.8346e+00 (1.4225e+00)	Acc@1  69.00 ( 71.41)	Acc@5  89.00 ( 91.53)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.5032e+00 (1.3906e+00)	Acc@1  68.00 ( 71.77)	Acc@5  93.00 ( 91.93)
Test: [ 70/100]	Time  0.080 ( 0.080)	Loss 1.4810e+00 (1.3954e+00)	Acc@1  70.00 ( 71.83)	Acc@5  88.00 ( 91.82)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 1.3881e+00 (1.3877e+00)	Acc@1  70.00 ( 72.01)	Acc@5  93.00 ( 91.89)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.6922e+00 (1.3673e+00)	Acc@1  64.00 ( 72.27)	Acc@5  90.00 ( 92.01)
 * Acc@1 72.470 Acc@5 92.010
### epoch[59] execution time: 102.85007071495056
EPOCH 60
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [60][  0/391]	Time  0.398 ( 0.398)	Data  0.154 ( 0.154)	Loss 2.3981e-02 (2.3981e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [60][ 10/391]	Time  0.241 ( 0.255)	Data  0.001 ( 0.015)	Loss 1.9196e-02 (3.0310e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [60][ 20/391]	Time  0.248 ( 0.249)	Data  0.001 ( 0.008)	Loss 2.7264e-02 (2.7456e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [60][ 30/391]	Time  0.240 ( 0.247)	Data  0.001 ( 0.006)	Loss 3.1426e-02 (2.6867e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [60][ 40/391]	Time  0.242 ( 0.245)	Data  0.001 ( 0.005)	Loss 2.2164e-02 (2.7156e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [60][ 50/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.004)	Loss 3.7559e-02 (2.7386e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [60][ 60/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.004)	Loss 3.4382e-02 (2.7840e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [60][ 70/391]	Time  0.245 ( 0.244)	Data  0.001 ( 0.003)	Loss 2.4731e-02 (2.7895e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [60][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 4.8857e-02 (2.8506e-02)	Acc@1  98.44 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [60][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.4012e-02 (2.9057e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [60][100/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.8999e-02 (2.8758e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [60][110/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.9874e-02 (2.9408e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [60][120/391]	Time  0.246 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.2520e-02 (2.9103e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [60][130/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.2582e-02 (2.8803e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [60][140/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.1464e-02 (2.9477e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [60][150/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.5649e-02 (2.9429e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [60][160/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3284e-02 (2.9492e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [60][170/391]	Time  0.249 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8759e-02 (2.9406e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [60][180/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1353e-02 (2.9275e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [60][190/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.6944e-02 (2.9058e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [60][200/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6070e-02 (2.9105e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [60][210/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.8541e-02 (2.9406e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [60][220/391]	Time  0.248 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2252e-02 (2.9378e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [60][230/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.4701e-02 (2.9569e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [60][240/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5242e-02 (3.0066e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [60][250/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.0787e-02 (3.0150e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [60][260/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9858e-02 (2.9955e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [60][270/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2712e-02 (2.9759e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [60][280/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.2899e-02 (2.9727e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [60][290/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6987e-02 (2.9565e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [60][300/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.2351e-02 (2.9705e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [60][310/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.6778e-02 (2.9661e-02)	Acc@1  98.44 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [60][320/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.9125e-02 (2.9580e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [60][330/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4250e-02 (2.9545e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [60][340/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2836e-02 (2.9482e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [60][350/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6388e-02 (2.9422e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [60][360/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1292e-02 (2.9558e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [60][370/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.2471e-02 (2.9617e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [60][380/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.4350e-02 (2.9576e-02)	Acc@1  98.44 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [60][390/391]	Time  0.180 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.6454e-02 (2.9603e-02)	Acc@1  98.75 ( 99.45)	Acc@5 100.00 (100.00)
## e[60] optimizer.zero_grad (sum) time: 0.4930119514465332
## e[60]       loss.backward (sum) time: 11.10720944404602
## e[60]      optimizer.step (sum) time: 47.86013579368591
## epoch[60] training(only) time: 94.72207593917847
# Switched to evaluate mode...
Test: [  0/100]	Time  0.206 ( 0.206)	Loss 1.3622e+00 (1.3622e+00)	Acc@1  75.00 ( 75.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.077 ( 0.090)	Loss 1.3304e+00 (1.3546e+00)	Acc@1  69.00 ( 73.00)	Acc@5  93.00 ( 90.82)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 1.3445e+00 (1.3202e+00)	Acc@1  67.00 ( 72.76)	Acc@5  92.00 ( 91.86)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 1.5770e+00 (1.3742e+00)	Acc@1  62.00 ( 71.84)	Acc@5  92.00 ( 91.55)
Test: [ 40/100]	Time  0.079 ( 0.081)	Loss 1.5014e+00 (1.3842e+00)	Acc@1  75.00 ( 71.44)	Acc@5  93.00 ( 91.54)
Test: [ 50/100]	Time  0.078 ( 0.080)	Loss 1.8192e+00 (1.4094e+00)	Acc@1  69.00 ( 71.37)	Acc@5  89.00 ( 91.31)
Test: [ 60/100]	Time  0.077 ( 0.080)	Loss 1.4964e+00 (1.3809e+00)	Acc@1  68.00 ( 71.64)	Acc@5  95.00 ( 91.80)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 1.4944e+00 (1.3880e+00)	Acc@1  70.00 ( 71.80)	Acc@5  90.00 ( 91.72)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 1.3794e+00 (1.3813e+00)	Acc@1  71.00 ( 71.95)	Acc@5  92.00 ( 91.78)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.6528e+00 (1.3610e+00)	Acc@1  64.00 ( 72.26)	Acc@5  90.00 ( 91.86)
 * Acc@1 72.410 Acc@5 91.890
### epoch[60] execution time: 102.73050212860107
EPOCH 61
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [61][  0/391]	Time  0.415 ( 0.415)	Data  0.173 ( 0.173)	Loss 1.7532e-02 (1.7532e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [61][ 10/391]	Time  0.241 ( 0.257)	Data  0.001 ( 0.017)	Loss 2.1115e-02 (2.7257e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [61][ 20/391]	Time  0.241 ( 0.250)	Data  0.001 ( 0.009)	Loss 1.9961e-02 (2.5310e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [61][ 30/391]	Time  0.240 ( 0.247)	Data  0.001 ( 0.007)	Loss 3.9849e-02 (2.6969e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [61][ 40/391]	Time  0.240 ( 0.246)	Data  0.001 ( 0.005)	Loss 1.5566e-02 (2.8114e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [61][ 50/391]	Time  0.242 ( 0.245)	Data  0.001 ( 0.005)	Loss 2.5376e-02 (2.7961e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [61][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 2.2439e-02 (2.8923e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [61][ 70/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 2.6168e-02 (2.8844e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [61][ 80/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.003)	Loss 3.8181e-02 (2.8723e-02)	Acc@1  98.44 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [61][ 90/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.7059e-02 (2.9127e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [61][100/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.3816e-02 (2.9398e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [61][110/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.2166e-02 (2.8695e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [61][120/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.3882e-02 (2.8487e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [61][130/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.3109e-02 (2.8563e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [61][140/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.7090e-02 (2.8637e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [61][150/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.9398e-02 (2.9045e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [61][160/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.4307e-02 (2.8767e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [61][170/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.8239e-02 (2.9001e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [61][180/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0776e-02 (2.8649e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [61][190/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0323e-02 (2.8483e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [61][200/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.5096e-02 (2.8587e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [61][210/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.9638e-02 (2.8675e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [61][220/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5761e-02 (2.8913e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [61][230/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.4363e-02 (2.9195e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [61][240/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3678e-02 (2.9056e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [61][250/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.2939e-02 (2.9018e-02)	Acc@1  98.44 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [61][260/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1569e-02 (2.9027e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [61][270/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6019e-02 (2.8982e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [61][280/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7405e-02 (2.8818e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [61][290/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7385e-02 (2.8785e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [61][300/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2360e-02 (2.8789e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [61][310/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8945e-02 (2.8704e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [61][320/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1547e-02 (2.8397e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [61][330/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.8348e-02 (2.8378e-02)	Acc@1  98.44 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [61][340/391]	Time  0.254 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9636e-02 (2.8459e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [61][350/391]	Time  0.247 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2599e-02 (2.8406e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [61][360/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.5567e-02 (2.8622e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [61][370/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3994e-02 (2.8477e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [61][380/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.3528e-02 (2.8496e-02)	Acc@1  98.44 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [61][390/391]	Time  0.178 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.9447e-02 (2.8549e-02)	Acc@1  97.50 ( 99.50)	Acc@5 100.00 (100.00)
## e[61] optimizer.zero_grad (sum) time: 0.49245667457580566
## e[61]       loss.backward (sum) time: 11.123407363891602
## e[61]      optimizer.step (sum) time: 47.80904030799866
## epoch[61] training(only) time: 94.79019713401794
# Switched to evaluate mode...
Test: [  0/100]	Time  0.213 ( 0.213)	Loss 1.3745e+00 (1.3745e+00)	Acc@1  74.00 ( 74.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.3521e+00 (1.3533e+00)	Acc@1  69.00 ( 73.18)	Acc@5  93.00 ( 91.73)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 1.3080e+00 (1.3176e+00)	Acc@1  67.00 ( 73.19)	Acc@5  92.00 ( 92.33)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 1.5610e+00 (1.3683e+00)	Acc@1  66.00 ( 72.23)	Acc@5  93.00 ( 91.90)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.4808e+00 (1.3778e+00)	Acc@1  74.00 ( 71.88)	Acc@5  93.00 ( 92.00)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 1.7995e+00 (1.4006e+00)	Acc@1  70.00 ( 71.76)	Acc@5  89.00 ( 91.65)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.4894e+00 (1.3707e+00)	Acc@1  69.00 ( 72.03)	Acc@5  96.00 ( 92.10)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 1.4215e+00 (1.3772e+00)	Acc@1  70.00 ( 72.17)	Acc@5  90.00 ( 91.93)
Test: [ 80/100]	Time  0.077 ( 0.080)	Loss 1.3300e+00 (1.3678e+00)	Acc@1  71.00 ( 72.28)	Acc@5  93.00 ( 91.99)
Test: [ 90/100]	Time  0.079 ( 0.080)	Loss 1.6636e+00 (1.3469e+00)	Acc@1  64.00 ( 72.52)	Acc@5  89.00 ( 92.10)
 * Acc@1 72.690 Acc@5 92.130
### epoch[61] execution time: 102.82142567634583
EPOCH 62
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [62][  0/391]	Time  0.404 ( 0.404)	Data  0.153 ( 0.153)	Loss 2.2321e-02 (2.2321e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [62][ 10/391]	Time  0.241 ( 0.256)	Data  0.001 ( 0.015)	Loss 2.8592e-02 (3.1979e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [62][ 20/391]	Time  0.246 ( 0.249)	Data  0.001 ( 0.008)	Loss 3.7674e-02 (2.8593e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [62][ 30/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.006)	Loss 1.5791e-02 (2.6640e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [62][ 40/391]	Time  0.242 ( 0.245)	Data  0.001 ( 0.005)	Loss 2.0563e-02 (2.5976e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [62][ 50/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.004)	Loss 2.2265e-02 (2.5471e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [62][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 3.2520e-02 (2.5020e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [62][ 70/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 2.2499e-02 (2.5308e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [62][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.9043e-02 (2.5591e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [62][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 4.7999e-02 (2.6114e-02)	Acc@1  98.44 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [62][100/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.3681e-02 (2.5989e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [62][110/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.4031e-02 (2.6760e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [62][120/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.8576e-02 (2.6899e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [62][130/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.6872e-02 (2.7054e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [62][140/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.1346e-02 (2.6911e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [62][150/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5674e-02 (2.6835e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [62][160/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2849e-02 (2.6996e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [62][170/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.3054e-02 (2.6897e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [62][180/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.3640e-02 (2.6947e-02)	Acc@1  98.44 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [62][190/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4318e-02 (2.7065e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [62][200/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4570e-02 (2.6856e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [62][210/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9193e-02 (2.6882e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [62][220/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6462e-02 (2.6897e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [62][230/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0034e-02 (2.7072e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [62][240/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2379e-02 (2.6947e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [62][250/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5015e-02 (2.6742e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [62][260/391]	Time  0.247 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7823e-02 (2.6676e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [62][270/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.2549e-02 (2.6772e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [62][280/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.2785e-02 (2.6857e-02)	Acc@1  98.44 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [62][290/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.0674e-02 (2.6844e-02)	Acc@1  97.66 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [62][300/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2391e-02 (2.6947e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [62][310/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.3400e-02 (2.6911e-02)	Acc@1  97.66 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [62][320/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.2007e-02 (2.6906e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [62][330/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6598e-02 (2.7016e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [62][340/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5563e-02 (2.6827e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [62][350/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2465e-02 (2.6766e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [62][360/391]	Time  0.246 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3204e-02 (2.6676e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [62][370/391]	Time  0.251 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.7753e-02 (2.6612e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [62][380/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3462e-02 (2.6565e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [62][390/391]	Time  0.178 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2284e-02 (2.6440e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
## e[62] optimizer.zero_grad (sum) time: 0.4933445453643799
## e[62]       loss.backward (sum) time: 11.17821455001831
## e[62]      optimizer.step (sum) time: 47.773125410079956
## epoch[62] training(only) time: 94.71001195907593
# Switched to evaluate mode...
Test: [  0/100]	Time  0.208 ( 0.208)	Loss 1.3648e+00 (1.3648e+00)	Acc@1  76.00 ( 76.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.3747e+00 (1.3429e+00)	Acc@1  67.00 ( 73.36)	Acc@5  92.00 ( 90.82)
Test: [ 20/100]	Time  0.077 ( 0.084)	Loss 1.3417e+00 (1.3113e+00)	Acc@1  68.00 ( 73.10)	Acc@5  92.00 ( 91.95)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 1.5414e+00 (1.3638e+00)	Acc@1  65.00 ( 72.03)	Acc@5  94.00 ( 91.74)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.4346e+00 (1.3724e+00)	Acc@1  73.00 ( 71.66)	Acc@5  93.00 ( 91.90)
Test: [ 50/100]	Time  0.078 ( 0.080)	Loss 1.7674e+00 (1.3942e+00)	Acc@1  70.00 ( 71.61)	Acc@5  89.00 ( 91.63)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.5043e+00 (1.3675e+00)	Acc@1  67.00 ( 71.80)	Acc@5  93.00 ( 91.98)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 1.4676e+00 (1.3714e+00)	Acc@1  69.00 ( 71.89)	Acc@5  88.00 ( 91.89)
Test: [ 80/100]	Time  0.077 ( 0.079)	Loss 1.3301e+00 (1.3636e+00)	Acc@1  71.00 ( 72.02)	Acc@5  92.00 ( 91.91)
Test: [ 90/100]	Time  0.077 ( 0.079)	Loss 1.6692e+00 (1.3432e+00)	Acc@1  65.00 ( 72.31)	Acc@5  89.00 ( 92.01)
 * Acc@1 72.450 Acc@5 92.080
### epoch[62] execution time: 102.73810529708862
EPOCH 63
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [63][  0/391]	Time  0.403 ( 0.403)	Data  0.152 ( 0.152)	Loss 1.9655e-02 (1.9655e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [63][ 10/391]	Time  0.240 ( 0.256)	Data  0.001 ( 0.015)	Loss 1.8109e-02 (2.9660e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [63][ 20/391]	Time  0.240 ( 0.249)	Data  0.001 ( 0.008)	Loss 4.8392e-02 (2.7826e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [63][ 30/391]	Time  0.240 ( 0.247)	Data  0.001 ( 0.006)	Loss 1.7924e-02 (2.9918e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [63][ 40/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.005)	Loss 3.6618e-02 (2.9914e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [63][ 50/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.7930e-02 (2.8935e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [63][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 2.1883e-02 (2.9337e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [63][ 70/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 3.1181e-02 (2.9443e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [63][ 80/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.4823e-02 (2.9341e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [63][ 90/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.3615e-02 (3.0061e-02)	Acc@1  98.44 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [63][100/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.0260e-02 (2.9549e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [63][110/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.0398e-02 (2.9108e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [63][120/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.8716e-02 (2.8846e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [63][130/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.4987e-02 (2.9123e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [63][140/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.1126e-02 (2.9681e-02)	Acc@1  97.66 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [63][150/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.2007e-02 (2.9766e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [63][160/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.5998e-02 (2.9922e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [63][170/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.5420e-02 (3.0080e-02)	Acc@1  97.66 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [63][180/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2289e-02 (2.9915e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [63][190/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8244e-02 (2.9735e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [63][200/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4476e-02 (2.9853e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [63][210/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8805e-02 (2.9929e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [63][220/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4025e-02 (2.9910e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [63][230/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.5495e-02 (2.9866e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [63][240/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.0681e-02 (2.9675e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [63][250/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9125e-02 (2.9440e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [63][260/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2193e-02 (2.9325e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [63][270/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.0681e-02 (2.9264e-02)	Acc@1  98.44 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [63][280/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.5415e-02 (2.9474e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [63][290/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6872e-02 (2.9465e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [63][300/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1865e-02 (2.9240e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [63][310/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.6614e-02 (2.9144e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [63][320/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6807e-02 (2.8856e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [63][330/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6920e-02 (2.8870e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [63][340/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.5157e-02 (2.8926e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [63][350/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.2850e-02 (2.8816e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [63][360/391]	Time  0.244 ( 0.242)	Data  0.002 ( 0.002)	Loss 2.4269e-02 (2.8649e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [63][370/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.6890e-02 (2.8670e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [63][380/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0342e-02 (2.8613e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [63][390/391]	Time  0.177 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.1674e-02 (2.8668e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
## e[63] optimizer.zero_grad (sum) time: 0.4924013614654541
## e[63]       loss.backward (sum) time: 11.184420824050903
## e[63]      optimizer.step (sum) time: 47.80858063697815
## epoch[63] training(only) time: 94.82131218910217
# Switched to evaluate mode...
Test: [  0/100]	Time  0.210 ( 0.210)	Loss 1.3690e+00 (1.3690e+00)	Acc@1  74.00 ( 74.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.3745e+00 (1.3567e+00)	Acc@1  68.00 ( 72.91)	Acc@5  92.00 ( 91.00)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 1.2625e+00 (1.3165e+00)	Acc@1  68.00 ( 73.19)	Acc@5  92.00 ( 92.00)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 1.5841e+00 (1.3728e+00)	Acc@1  63.00 ( 72.16)	Acc@5  92.00 ( 91.74)
Test: [ 40/100]	Time  0.077 ( 0.081)	Loss 1.5105e+00 (1.3808e+00)	Acc@1  73.00 ( 71.85)	Acc@5  94.00 ( 91.90)
Test: [ 50/100]	Time  0.079 ( 0.081)	Loss 1.7669e+00 (1.4018e+00)	Acc@1  68.00 ( 71.76)	Acc@5  88.00 ( 91.59)
Test: [ 60/100]	Time  0.079 ( 0.080)	Loss 1.5493e+00 (1.3744e+00)	Acc@1  67.00 ( 72.00)	Acc@5  92.00 ( 91.98)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 1.4699e+00 (1.3815e+00)	Acc@1  70.00 ( 72.06)	Acc@5  88.00 ( 91.89)
Test: [ 80/100]	Time  0.079 ( 0.080)	Loss 1.3470e+00 (1.3729e+00)	Acc@1  70.00 ( 72.14)	Acc@5  92.00 ( 91.91)
Test: [ 90/100]	Time  0.078 ( 0.080)	Loss 1.6644e+00 (1.3521e+00)	Acc@1  64.00 ( 72.43)	Acc@5  89.00 ( 91.98)
 * Acc@1 72.630 Acc@5 92.020
### epoch[63] execution time: 102.87494564056396
EPOCH 64
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [64][  0/391]	Time  0.392 ( 0.392)	Data  0.150 ( 0.150)	Loss 2.6952e-02 (2.6952e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [64][ 10/391]	Time  0.241 ( 0.255)	Data  0.001 ( 0.015)	Loss 1.4864e-02 (2.6310e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [64][ 20/391]	Time  0.241 ( 0.249)	Data  0.001 ( 0.008)	Loss 2.5529e-02 (2.5052e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [64][ 30/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.006)	Loss 2.6238e-02 (2.6213e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [64][ 40/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.005)	Loss 1.8380e-02 (2.6708e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [64][ 50/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 2.2460e-02 (2.6018e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [64][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.8369e-02 (2.6120e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [64][ 70/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.1753e-02 (2.6595e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [64][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.5155e-02 (2.6828e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [64][ 90/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.6492e-02 (2.6685e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [64][100/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.6660e-02 (2.6332e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [64][110/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.2687e-02 (2.6499e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [64][120/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.0918e-02 (2.6846e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [64][130/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.5349e-02 (2.6764e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [64][140/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.5657e-02 (2.7185e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [64][150/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.9608e-02 (2.6811e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [64][160/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.1959e-02 (2.6881e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [64][170/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4840e-02 (2.6615e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [64][180/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8758e-02 (2.6633e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [64][190/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6230e-02 (2.6801e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [64][200/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4979e-02 (2.6838e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [64][210/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3451e-02 (2.6870e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [64][220/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1935e-02 (2.6676e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [64][230/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8225e-02 (2.6453e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [64][240/391]	Time  0.253 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8830e-02 (2.6592e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [64][250/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.7779e-02 (2.6512e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [64][260/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.8113e-02 (2.6304e-02)	Acc@1  98.44 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [64][270/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5131e-02 (2.6416e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [64][280/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9126e-02 (2.6243e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [64][290/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2245e-02 (2.6196e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [64][300/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1298e-02 (2.6302e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [64][310/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.7221e-02 (2.6319e-02)	Acc@1  98.44 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [64][320/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1669e-02 (2.6294e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [64][330/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.0330e-02 (2.6426e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [64][340/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5846e-02 (2.6439e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [64][350/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2614e-02 (2.6522e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [64][360/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0688e-02 (2.6418e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [64][370/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6635e-02 (2.6302e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [64][380/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.4299e-02 (2.6394e-02)	Acc@1  98.44 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [64][390/391]	Time  0.178 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.5737e-02 (2.6549e-02)	Acc@1  98.75 ( 99.56)	Acc@5 100.00 (100.00)
## e[64] optimizer.zero_grad (sum) time: 0.4956681728363037
## e[64]       loss.backward (sum) time: 11.167317152023315
## e[64]      optimizer.step (sum) time: 47.80669569969177
## epoch[64] training(only) time: 94.7846269607544
# Switched to evaluate mode...
Test: [  0/100]	Time  0.208 ( 0.208)	Loss 1.3822e+00 (1.3822e+00)	Acc@1  75.00 ( 75.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.3611e+00 (1.3593e+00)	Acc@1  69.00 ( 73.36)	Acc@5  93.00 ( 91.27)
Test: [ 20/100]	Time  0.077 ( 0.084)	Loss 1.2790e+00 (1.3131e+00)	Acc@1  67.00 ( 73.38)	Acc@5  93.00 ( 92.00)
Test: [ 30/100]	Time  0.077 ( 0.082)	Loss 1.5929e+00 (1.3659e+00)	Acc@1  62.00 ( 72.16)	Acc@5  93.00 ( 91.77)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.4614e+00 (1.3744e+00)	Acc@1  76.00 ( 71.85)	Acc@5  92.00 ( 91.88)
Test: [ 50/100]	Time  0.078 ( 0.080)	Loss 1.7520e+00 (1.3968e+00)	Acc@1  71.00 ( 71.65)	Acc@5  88.00 ( 91.53)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.5359e+00 (1.3704e+00)	Acc@1  67.00 ( 71.87)	Acc@5  93.00 ( 91.95)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 1.4705e+00 (1.3757e+00)	Acc@1  71.00 ( 71.94)	Acc@5  88.00 ( 91.86)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 1.3345e+00 (1.3685e+00)	Acc@1  71.00 ( 72.06)	Acc@5  92.00 ( 91.88)
Test: [ 90/100]	Time  0.077 ( 0.079)	Loss 1.6469e+00 (1.3471e+00)	Acc@1  66.00 ( 72.34)	Acc@5  89.00 ( 91.97)
 * Acc@1 72.500 Acc@5 92.000
### epoch[64] execution time: 102.80189275741577
EPOCH 65
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [65][  0/391]	Time  0.398 ( 0.398)	Data  0.156 ( 0.156)	Loss 1.5201e-02 (1.5201e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [65][ 10/391]	Time  0.241 ( 0.255)	Data  0.001 ( 0.015)	Loss 1.7888e-02 (2.6679e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [65][ 20/391]	Time  0.239 ( 0.249)	Data  0.001 ( 0.009)	Loss 2.6626e-02 (2.6577e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [65][ 30/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.006)	Loss 2.5826e-02 (2.7020e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [65][ 40/391]	Time  0.242 ( 0.246)	Data  0.001 ( 0.005)	Loss 1.6173e-02 (2.6903e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [65][ 50/391]	Time  0.242 ( 0.245)	Data  0.001 ( 0.004)	Loss 5.0937e-02 (2.7193e-02)	Acc@1  98.44 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [65][ 60/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 6.1404e-02 (2.7799e-02)	Acc@1  98.44 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [65][ 70/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 2.4353e-02 (2.7416e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [65][ 80/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.003)	Loss 2.6735e-02 (2.7549e-02)	Acc@1  98.44 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [65][ 90/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.003)	Loss 1.1391e-02 (2.7601e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [65][100/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 4.8348e-02 (2.7340e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [65][110/391]	Time  0.251 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.6189e-02 (2.7145e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [65][120/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.1192e-02 (2.6790e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [65][130/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.7116e-02 (2.6903e-02)	Acc@1  98.44 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [65][140/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.4772e-02 (2.6468e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [65][150/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.4891e-02 (2.6372e-02)	Acc@1  98.44 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [65][160/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.2195e-02 (2.6310e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [65][170/391]	Time  0.247 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.2642e-02 (2.6325e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [65][180/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.3910e-02 (2.6498e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [65][190/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.5137e-02 (2.6474e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [65][200/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.2531e-02 (2.6279e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [65][210/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.6408e-02 (2.6505e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [65][220/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.0594e-02 (2.6310e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [65][230/391]	Time  0.250 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.3685e-02 (2.6501e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [65][240/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.3435e-02 (2.6250e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [65][250/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.9231e-02 (2.6152e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [65][260/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 9.2484e-03 (2.5925e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [65][270/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.6570e-02 (2.5647e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [65][280/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.6494e-02 (2.5643e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [65][290/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.9853e-02 (2.5571e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [65][300/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.3913e-02 (2.5568e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [65][310/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.4630e-02 (2.5494e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [65][320/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.7682e-02 (2.5579e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [65][330/391]	Time  0.251 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.4406e-02 (2.5681e-02)	Acc@1  98.44 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [65][340/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.6088e-02 (2.5616e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [65][350/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.7559e-02 (2.5610e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [65][360/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1673e-02 (2.5628e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [65][370/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.0659e-02 (2.5580e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [65][380/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.2733e-02 (2.5662e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [65][390/391]	Time  0.178 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.7920e-02 (2.5597e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
## e[65] optimizer.zero_grad (sum) time: 0.49228382110595703
## e[65]       loss.backward (sum) time: 11.177498817443848
## e[65]      optimizer.step (sum) time: 47.79327964782715
## epoch[65] training(only) time: 94.86923694610596
# Switched to evaluate mode...
Test: [  0/100]	Time  0.208 ( 0.208)	Loss 1.3647e+00 (1.3647e+00)	Acc@1  74.00 ( 74.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.3848e+00 (1.3472e+00)	Acc@1  68.00 ( 72.36)	Acc@5  92.00 ( 91.00)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 1.2832e+00 (1.3119e+00)	Acc@1  67.00 ( 73.14)	Acc@5  94.00 ( 91.95)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 1.5673e+00 (1.3673e+00)	Acc@1  64.00 ( 72.16)	Acc@5  93.00 ( 91.74)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.4767e+00 (1.3751e+00)	Acc@1  76.00 ( 71.80)	Acc@5  94.00 ( 91.90)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 1.7728e+00 (1.3951e+00)	Acc@1  69.00 ( 71.67)	Acc@5  88.00 ( 91.53)
Test: [ 60/100]	Time  0.077 ( 0.080)	Loss 1.5480e+00 (1.3690e+00)	Acc@1  68.00 ( 71.89)	Acc@5  93.00 ( 92.00)
Test: [ 70/100]	Time  0.079 ( 0.080)	Loss 1.4654e+00 (1.3752e+00)	Acc@1  71.00 ( 71.96)	Acc@5  90.00 ( 91.96)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 1.3421e+00 (1.3674e+00)	Acc@1  69.00 ( 72.10)	Acc@5  93.00 ( 92.00)
Test: [ 90/100]	Time  0.079 ( 0.080)	Loss 1.6835e+00 (1.3460e+00)	Acc@1  66.00 ( 72.34)	Acc@5  89.00 ( 92.08)
 * Acc@1 72.520 Acc@5 92.160
### epoch[65] execution time: 102.9053168296814
EPOCH 66
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [66][  0/391]	Time  0.389 ( 0.389)	Data  0.148 ( 0.148)	Loss 2.4990e-02 (2.4990e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [66][ 10/391]	Time  0.249 ( 0.256)	Data  0.001 ( 0.014)	Loss 2.8017e-02 (2.1647e-02)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [66][ 20/391]	Time  0.241 ( 0.249)	Data  0.001 ( 0.008)	Loss 7.1151e-03 (2.1341e-02)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [66][ 30/391]	Time  0.242 ( 0.247)	Data  0.001 ( 0.006)	Loss 3.8290e-02 (2.3251e-02)	Acc@1  98.44 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [66][ 40/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.005)	Loss 1.1759e-02 (2.4562e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [66][ 50/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.004)	Loss 2.8480e-02 (2.5010e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [66][ 60/391]	Time  0.247 ( 0.244)	Data  0.001 ( 0.003)	Loss 2.0001e-02 (2.5545e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 ( 99.99)
Epoch: [66][ 70/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 2.9864e-02 (2.5526e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 ( 99.99)
Epoch: [66][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.3631e-02 (2.4952e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 ( 99.99)
Epoch: [66][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.9708e-02 (2.5282e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 ( 99.99)
Epoch: [66][100/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 4.1560e-02 (2.5201e-02)	Acc@1  98.44 ( 99.65)	Acc@5 100.00 ( 99.99)
Epoch: [66][110/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.2308e-02 (2.5375e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 ( 99.99)
Epoch: [66][120/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.0646e-02 (2.5347e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 ( 99.99)
Epoch: [66][130/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.1210e-02 (2.5561e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 ( 99.99)
Epoch: [66][140/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.9313e-02 (2.5319e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 ( 99.99)
Epoch: [66][150/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.3752e-02 (2.5110e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 ( 99.99)
Epoch: [66][160/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.0739e-02 (2.5425e-02)	Acc@1  98.44 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [66][170/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.0749e-02 (2.5304e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [66][180/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.3510e-02 (2.5215e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [66][190/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.4463e-02 (2.5191e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [66][200/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.3434e-02 (2.5443e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [66][210/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.2716e-02 (2.5630e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [66][220/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.1687e-02 (2.5942e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [66][230/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.1000e-02 (2.5974e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [66][240/391]	Time  0.246 ( 0.243)	Data  0.001 ( 0.002)	Loss 9.2426e-03 (2.5674e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [66][250/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.5254e-02 (2.5734e-02)	Acc@1  98.44 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [66][260/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1687e-02 (2.5449e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [66][270/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.6635e-02 (2.5325e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [66][280/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.2768e-02 (2.5398e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [66][290/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.2710e-02 (2.5342e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [66][300/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.4008e-02 (2.5293e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [66][310/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.7383e-02 (2.5376e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [66][320/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 9.9621e-03 (2.5321e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [66][330/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.1790e-02 (2.5271e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [66][340/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 6.0951e-02 (2.5286e-02)	Acc@1  98.44 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [66][350/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.3935e-02 (2.5203e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [66][360/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.7876e-02 (2.5297e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [66][370/391]	Time  0.246 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.6296e-02 (2.5348e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [66][380/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.7808e-02 (2.5306e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [66][390/391]	Time  0.178 ( 0.242)	Data  0.001 ( 0.001)	Loss 5.9963e-02 (2.5485e-02)	Acc@1  98.75 ( 99.58)	Acc@5 100.00 (100.00)
## e[66] optimizer.zero_grad (sum) time: 0.49483561515808105
## e[66]       loss.backward (sum) time: 11.16386890411377
## e[66]      optimizer.step (sum) time: 47.8292760848999
## epoch[66] training(only) time: 94.89683794975281
# Switched to evaluate mode...
Test: [  0/100]	Time  0.208 ( 0.208)	Loss 1.3887e+00 (1.3887e+00)	Acc@1  74.00 ( 74.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.3699e+00 (1.3506e+00)	Acc@1  69.00 ( 73.73)	Acc@5  93.00 ( 91.27)
Test: [ 20/100]	Time  0.077 ( 0.084)	Loss 1.2788e+00 (1.3069e+00)	Acc@1  67.00 ( 73.62)	Acc@5  94.00 ( 92.19)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 1.5558e+00 (1.3577e+00)	Acc@1  66.00 ( 72.58)	Acc@5  92.00 ( 91.87)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.4593e+00 (1.3656e+00)	Acc@1  75.00 ( 72.17)	Acc@5  93.00 ( 91.98)
Test: [ 50/100]	Time  0.077 ( 0.080)	Loss 1.7331e+00 (1.3873e+00)	Acc@1  70.00 ( 72.02)	Acc@5  88.00 ( 91.55)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.5571e+00 (1.3618e+00)	Acc@1  68.00 ( 72.18)	Acc@5  93.00 ( 92.03)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 1.4622e+00 (1.3676e+00)	Acc@1  71.00 ( 72.23)	Acc@5  89.00 ( 91.90)
Test: [ 80/100]	Time  0.078 ( 0.079)	Loss 1.3480e+00 (1.3601e+00)	Acc@1  71.00 ( 72.40)	Acc@5  93.00 ( 91.93)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.6538e+00 (1.3394e+00)	Acc@1  64.00 ( 72.56)	Acc@5  90.00 ( 92.01)
 * Acc@1 72.670 Acc@5 92.080
### epoch[66] execution time: 102.9001042842865
EPOCH 67
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [67][  0/391]	Time  0.400 ( 0.400)	Data  0.157 ( 0.157)	Loss 1.7202e-02 (1.7202e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [67][ 10/391]	Time  0.241 ( 0.257)	Data  0.001 ( 0.015)	Loss 1.8947e-02 (3.4468e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [67][ 20/391]	Time  0.241 ( 0.249)	Data  0.001 ( 0.009)	Loss 1.4811e-02 (2.9722e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [67][ 30/391]	Time  0.242 ( 0.247)	Data  0.001 ( 0.006)	Loss 2.3325e-02 (2.7075e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [67][ 40/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.005)	Loss 9.6903e-03 (2.6270e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [67][ 50/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.004)	Loss 1.0855e-02 (2.5289e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [67][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.3416e-02 (2.6278e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [67][ 70/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.003)	Loss 2.8444e-02 (2.6254e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [67][ 80/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 4.5289e-02 (2.5887e-02)	Acc@1  97.66 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [67][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.3875e-02 (2.5521e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [67][100/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.3748e-02 (2.5094e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [67][110/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.2520e-02 (2.4682e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [67][120/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.2307e-02 (2.4693e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [67][130/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.5320e-02 (2.4231e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [67][140/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.2170e-02 (2.4936e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [67][150/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.1049e-02 (2.4734e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [67][160/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.4712e-02 (2.4785e-02)	Acc@1  98.44 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [67][170/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.4391e-02 (2.4823e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [67][180/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.3276e-02 (2.4681e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [67][190/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.3981e-02 (2.4793e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [67][200/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8577e-02 (2.4776e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [67][210/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.0542e-02 (2.4993e-02)	Acc@1  98.44 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [67][220/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.2212e-02 (2.5150e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [67][230/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.4496e-02 (2.4925e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [67][240/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6673e-02 (2.4815e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [67][250/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6570e-02 (2.4846e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [67][260/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3593e-02 (2.4856e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [67][270/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6542e-02 (2.4969e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [67][280/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5428e-02 (2.5050e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [67][290/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9077e-02 (2.5273e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [67][300/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6582e-02 (2.5308e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [67][310/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.1551e-02 (2.5444e-02)	Acc@1  98.44 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [67][320/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.6857e-02 (2.5475e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [67][330/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.5432e-02 (2.5409e-02)	Acc@1  98.44 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [67][340/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.5886e-02 (2.5305e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [67][350/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.0199e-02 (2.5312e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [67][360/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.3478e-02 (2.5351e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [67][370/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0805e-02 (2.5327e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [67][380/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.1871e-02 (2.5369e-02)	Acc@1  98.44 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [67][390/391]	Time  0.178 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.2349e-02 (2.5353e-02)	Acc@1  98.75 ( 99.61)	Acc@5 100.00 (100.00)
## e[67] optimizer.zero_grad (sum) time: 0.493457555770874
## e[67]       loss.backward (sum) time: 11.166171550750732
## e[67]      optimizer.step (sum) time: 47.812639236450195
## epoch[67] training(only) time: 94.82080698013306
# Switched to evaluate mode...
Test: [  0/100]	Time  0.212 ( 0.212)	Loss 1.3894e+00 (1.3894e+00)	Acc@1  74.00 ( 74.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.077 ( 0.090)	Loss 1.4224e+00 (1.3591e+00)	Acc@1  69.00 ( 73.45)	Acc@5  93.00 ( 91.18)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 1.3460e+00 (1.3167e+00)	Acc@1  66.00 ( 73.24)	Acc@5  91.00 ( 91.86)
Test: [ 30/100]	Time  0.079 ( 0.082)	Loss 1.6138e+00 (1.3678e+00)	Acc@1  65.00 ( 72.29)	Acc@5  93.00 ( 91.65)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.4842e+00 (1.3736e+00)	Acc@1  74.00 ( 72.15)	Acc@5  93.00 ( 91.85)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 1.7138e+00 (1.3922e+00)	Acc@1  69.00 ( 71.98)	Acc@5  88.00 ( 91.51)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.5192e+00 (1.3654e+00)	Acc@1  68.00 ( 72.28)	Acc@5  93.00 ( 91.92)
Test: [ 70/100]	Time  0.079 ( 0.080)	Loss 1.4858e+00 (1.3704e+00)	Acc@1  68.00 ( 72.30)	Acc@5  88.00 ( 91.80)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 1.3511e+00 (1.3634e+00)	Acc@1  70.00 ( 72.43)	Acc@5  92.00 ( 91.88)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.6643e+00 (1.3427e+00)	Acc@1  64.00 ( 72.68)	Acc@5  89.00 ( 91.92)
 * Acc@1 72.840 Acc@5 92.030
### epoch[67] execution time: 102.8402943611145
EPOCH 68
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [68][  0/391]	Time  0.401 ( 0.401)	Data  0.153 ( 0.153)	Loss 4.3843e-02 (4.3843e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [68][ 10/391]	Time  0.241 ( 0.255)	Data  0.001 ( 0.015)	Loss 1.0868e-02 (2.4108e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [68][ 20/391]	Time  0.243 ( 0.249)	Data  0.001 ( 0.008)	Loss 1.9572e-02 (2.5671e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [68][ 30/391]	Time  0.240 ( 0.247)	Data  0.001 ( 0.006)	Loss 3.1370e-02 (2.6951e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [68][ 40/391]	Time  0.240 ( 0.246)	Data  0.001 ( 0.005)	Loss 2.3647e-02 (2.6066e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [68][ 50/391]	Time  0.242 ( 0.245)	Data  0.001 ( 0.004)	Loss 1.5066e-02 (2.5484e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [68][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.2931e-02 (2.5100e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [68][ 70/391]	Time  0.246 ( 0.244)	Data  0.001 ( 0.003)	Loss 2.3930e-02 (2.4757e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [68][ 80/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.003)	Loss 1.7763e-02 (2.4333e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [68][ 90/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 1.8729e-02 (2.3990e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [68][100/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.003)	Loss 9.2151e-03 (2.3621e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [68][110/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.1276e-02 (2.3856e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [68][120/391]	Time  0.247 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.6532e-02 (2.3889e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [68][130/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.0332e-02 (2.3822e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [68][140/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 5.2897e-02 (2.4060e-02)	Acc@1  98.44 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [68][150/391]	Time  0.239 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.1605e-02 (2.4514e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [68][160/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.8810e-02 (2.4555e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [68][170/391]	Time  0.249 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.8253e-02 (2.4432e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [68][180/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.1184e-02 (2.4384e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [68][190/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.7166e-02 (2.4404e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [68][200/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.3578e-02 (2.4648e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [68][210/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.0357e-02 (2.4324e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [68][220/391]	Time  0.246 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.1964e-02 (2.4395e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [68][230/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.4334e-02 (2.4721e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [68][240/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.6322e-02 (2.4913e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [68][250/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.6835e-02 (2.5210e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [68][260/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.6462e-02 (2.5025e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [68][270/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.4639e-02 (2.5043e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [68][280/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.5493e-02 (2.5095e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [68][290/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.8158e-02 (2.4930e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [68][300/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.8329e-02 (2.4878e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [68][310/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.4490e-02 (2.4761e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [68][320/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.0922e-02 (2.4944e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [68][330/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.8068e-02 (2.4775e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [68][340/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.5401e-02 (2.4733e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [68][350/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.5763e-02 (2.4702e-02)	Acc@1  98.44 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [68][360/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.2273e-02 (2.4789e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [68][370/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.7411e-02 (2.4727e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [68][380/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 5.3723e-02 (2.4794e-02)	Acc@1  97.66 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [68][390/391]	Time  0.177 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.6978e-02 (2.4898e-02)	Acc@1  98.75 ( 99.58)	Acc@5 100.00 (100.00)
## e[68] optimizer.zero_grad (sum) time: 0.4934418201446533
## e[68]       loss.backward (sum) time: 11.171442747116089
## e[68]      optimizer.step (sum) time: 47.855305433273315
## epoch[68] training(only) time: 94.94409418106079
# Switched to evaluate mode...
Test: [  0/100]	Time  0.216 ( 0.216)	Loss 1.3889e+00 (1.3889e+00)	Acc@1  73.00 ( 73.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.3380e+00 (1.3429e+00)	Acc@1  72.00 ( 73.00)	Acc@5  92.00 ( 91.45)
Test: [ 20/100]	Time  0.078 ( 0.085)	Loss 1.2858e+00 (1.3104e+00)	Acc@1  69.00 ( 73.14)	Acc@5  94.00 ( 92.19)
Test: [ 30/100]	Time  0.079 ( 0.083)	Loss 1.5777e+00 (1.3591e+00)	Acc@1  63.00 ( 72.16)	Acc@5  93.00 ( 91.84)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.4631e+00 (1.3668e+00)	Acc@1  75.00 ( 71.88)	Acc@5  93.00 ( 91.88)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 1.7249e+00 (1.3877e+00)	Acc@1  69.00 ( 71.76)	Acc@5  90.00 ( 91.55)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.5544e+00 (1.3588e+00)	Acc@1  66.00 ( 72.05)	Acc@5  94.00 ( 91.98)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 1.4677e+00 (1.3652e+00)	Acc@1  69.00 ( 72.10)	Acc@5  89.00 ( 91.94)
Test: [ 80/100]	Time  0.077 ( 0.080)	Loss 1.3786e+00 (1.3590e+00)	Acc@1  71.00 ( 72.27)	Acc@5  93.00 ( 92.02)
Test: [ 90/100]	Time  0.079 ( 0.080)	Loss 1.6723e+00 (1.3393e+00)	Acc@1  66.00 ( 72.56)	Acc@5  89.00 ( 92.08)
 * Acc@1 72.710 Acc@5 92.120
### epoch[68] execution time: 102.99989604949951
EPOCH 69
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [69][  0/391]	Time  0.400 ( 0.400)	Data  0.158 ( 0.158)	Loss 2.2896e-02 (2.2896e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [69][ 10/391]	Time  0.240 ( 0.255)	Data  0.001 ( 0.015)	Loss 3.0184e-02 (2.4377e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [69][ 20/391]	Time  0.240 ( 0.249)	Data  0.001 ( 0.008)	Loss 1.4955e-02 (2.2991e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [69][ 30/391]	Time  0.242 ( 0.246)	Data  0.001 ( 0.006)	Loss 1.4112e-02 (2.2752e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [69][ 40/391]	Time  0.242 ( 0.245)	Data  0.001 ( 0.005)	Loss 2.0258e-02 (2.2667e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [69][ 50/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.6971e-02 (2.2626e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [69][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 7.3809e-02 (2.2986e-02)	Acc@1  98.44 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [69][ 70/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.003)	Loss 1.0055e-02 (2.3446e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [69][ 80/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.3854e-02 (2.3555e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [69][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.0650e-02 (2.4437e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [69][100/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.6260e-02 (2.4596e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [69][110/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 4.1828e-02 (2.4991e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [69][120/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.0011e-02 (2.4968e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [69][130/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.6775e-02 (2.4775e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [69][140/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.9059e-02 (2.4574e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [69][150/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.0365e-02 (2.4520e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [69][160/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.9258e-02 (2.4700e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [69][170/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9944e-02 (2.4732e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [69][180/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.6650e-02 (2.4679e-02)	Acc@1  98.44 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [69][190/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5199e-02 (2.4730e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [69][200/391]	Time  0.250 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5147e-02 (2.4927e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [69][210/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.5634e-02 (2.5049e-02)	Acc@1  98.44 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [69][220/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1703e-02 (2.4839e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [69][230/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1453e-02 (2.4793e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [69][240/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.0942e-02 (2.4987e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [69][250/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8995e-02 (2.5241e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [69][260/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3915e-02 (2.5146e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [69][270/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.3686e-02 (2.5157e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [69][280/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.9810e-02 (2.5228e-02)	Acc@1  97.66 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [69][290/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1995e-02 (2.5093e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [69][300/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5255e-02 (2.5230e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [69][310/391]	Time  0.246 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.5478e-02 (2.5285e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [69][320/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6612e-02 (2.5274e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [69][330/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7455e-02 (2.5190e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [69][340/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.7768e-02 (2.5023e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [69][350/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.2399e-02 (2.5159e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [69][360/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.9113e-02 (2.5117e-02)	Acc@1  98.44 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [69][370/391]	Time  0.247 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.4902e-02 (2.5178e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [69][380/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.9960e-02 (2.5119e-02)	Acc@1  98.44 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [69][390/391]	Time  0.178 ( 0.242)	Data  0.001 ( 0.001)	Loss 2.9691e-02 (2.5046e-02)	Acc@1  98.75 ( 99.64)	Acc@5 100.00 (100.00)
## e[69] optimizer.zero_grad (sum) time: 0.49441981315612793
## e[69]       loss.backward (sum) time: 11.158015251159668
## e[69]      optimizer.step (sum) time: 47.79941916465759
## epoch[69] training(only) time: 94.78001308441162
# Switched to evaluate mode...
Test: [  0/100]	Time  0.215 ( 0.215)	Loss 1.3791e+00 (1.3791e+00)	Acc@1  75.00 ( 75.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.077 ( 0.091)	Loss 1.3855e+00 (1.3512e+00)	Acc@1  68.00 ( 73.36)	Acc@5  94.00 ( 91.55)
Test: [ 20/100]	Time  0.078 ( 0.085)	Loss 1.3427e+00 (1.3163e+00)	Acc@1  67.00 ( 73.10)	Acc@5  91.00 ( 92.19)
Test: [ 30/100]	Time  0.079 ( 0.083)	Loss 1.5689e+00 (1.3674e+00)	Acc@1  65.00 ( 72.35)	Acc@5  93.00 ( 91.90)
Test: [ 40/100]	Time  0.077 ( 0.081)	Loss 1.4598e+00 (1.3741e+00)	Acc@1  75.00 ( 72.00)	Acc@5  93.00 ( 92.12)
Test: [ 50/100]	Time  0.077 ( 0.081)	Loss 1.7359e+00 (1.3971e+00)	Acc@1  69.00 ( 71.82)	Acc@5  89.00 ( 91.78)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.5777e+00 (1.3708e+00)	Acc@1  66.00 ( 72.02)	Acc@5  91.00 ( 92.11)
Test: [ 70/100]	Time  0.077 ( 0.080)	Loss 1.4715e+00 (1.3742e+00)	Acc@1  71.00 ( 72.11)	Acc@5  88.00 ( 92.06)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 1.3674e+00 (1.3683e+00)	Acc@1  72.00 ( 72.28)	Acc@5  92.00 ( 92.07)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.7056e+00 (1.3481e+00)	Acc@1  65.00 ( 72.51)	Acc@5  90.00 ( 92.15)
 * Acc@1 72.640 Acc@5 92.210
### epoch[69] execution time: 102.8001639842987
EPOCH 70
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [70][  0/391]	Time  0.404 ( 0.404)	Data  0.163 ( 0.163)	Loss 2.8182e-02 (2.8182e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [70][ 10/391]	Time  0.241 ( 0.255)	Data  0.001 ( 0.016)	Loss 2.2706e-02 (2.4667e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [70][ 20/391]	Time  0.241 ( 0.249)	Data  0.001 ( 0.009)	Loss 1.2213e-02 (2.3550e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [70][ 30/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.006)	Loss 2.9166e-02 (2.5194e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [70][ 40/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.005)	Loss 1.9150e-02 (2.5698e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [70][ 50/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.004)	Loss 2.1325e-02 (2.5780e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [70][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 2.6214e-02 (2.4645e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [70][ 70/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.003)	Loss 2.1465e-02 (2.4726e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [70][ 80/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 1.6340e-02 (2.4667e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [70][ 90/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.8704e-02 (2.4682e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [70][100/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.1205e-02 (2.4670e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [70][110/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 9.1577e-03 (2.4347e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [70][120/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.4489e-02 (2.4365e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [70][130/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.0932e-02 (2.4254e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [70][140/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.7634e-02 (2.4362e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [70][150/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.5550e-02 (2.4286e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [70][160/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.5050e-02 (2.4498e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [70][170/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 5.2532e-02 (2.4494e-02)	Acc@1  96.88 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [70][180/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 6.0767e-02 (2.4994e-02)	Acc@1  97.66 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [70][190/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.7710e-02 (2.5223e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [70][200/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.2467e-02 (2.5021e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [70][210/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.8073e-02 (2.5027e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [70][220/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.2070e-02 (2.5072e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [70][230/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.3895e-02 (2.5188e-02)	Acc@1  98.44 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [70][240/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.5331e-02 (2.5529e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [70][250/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.5955e-02 (2.5393e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [70][260/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1727e-02 (2.5373e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [70][270/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.9948e-02 (2.5410e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [70][280/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0003e-02 (2.5302e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [70][290/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.3755e-02 (2.5272e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [70][300/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2053e-02 (2.5112e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [70][310/391]	Time  0.248 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4465e-02 (2.5017e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [70][320/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.7026e-02 (2.5041e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [70][330/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1404e-02 (2.5004e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [70][340/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1529e-02 (2.4986e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [70][350/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.1919e-02 (2.4929e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [70][360/391]	Time  0.246 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4086e-02 (2.4855e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [70][370/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6224e-02 (2.4836e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [70][380/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6870e-02 (2.4780e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [70][390/391]	Time  0.177 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.3265e-02 (2.4843e-02)	Acc@1  98.75 ( 99.61)	Acc@5 100.00 (100.00)
## e[70] optimizer.zero_grad (sum) time: 0.4975869655609131
## e[70]       loss.backward (sum) time: 11.16573715209961
## e[70]      optimizer.step (sum) time: 47.79251432418823
## epoch[70] training(only) time: 94.81310987472534
# Switched to evaluate mode...
Test: [  0/100]	Time  0.208 ( 0.208)	Loss 1.3631e+00 (1.3631e+00)	Acc@1  75.00 ( 75.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.3613e+00 (1.3511e+00)	Acc@1  71.00 ( 73.55)	Acc@5  94.00 ( 91.64)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 1.2790e+00 (1.3105e+00)	Acc@1  67.00 ( 73.33)	Acc@5  94.00 ( 92.24)
Test: [ 30/100]	Time  0.079 ( 0.082)	Loss 1.5998e+00 (1.3611e+00)	Acc@1  67.00 ( 72.45)	Acc@5  94.00 ( 91.90)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.4545e+00 (1.3672e+00)	Acc@1  75.00 ( 71.98)	Acc@5  93.00 ( 92.02)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 1.6880e+00 (1.3882e+00)	Acc@1  69.00 ( 71.78)	Acc@5  89.00 ( 91.65)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.5600e+00 (1.3618e+00)	Acc@1  66.00 ( 72.02)	Acc@5  92.00 ( 92.05)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 1.4621e+00 (1.3667e+00)	Acc@1  71.00 ( 72.20)	Acc@5  89.00 ( 91.96)
Test: [ 80/100]	Time  0.079 ( 0.080)	Loss 1.3578e+00 (1.3605e+00)	Acc@1  71.00 ( 72.33)	Acc@5  93.00 ( 91.96)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.7182e+00 (1.3405e+00)	Acc@1  65.00 ( 72.56)	Acc@5  89.00 ( 92.05)
 * Acc@1 72.760 Acc@5 92.140
### epoch[70] execution time: 102.82948994636536
EPOCH 71
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [71][  0/391]	Time  0.405 ( 0.405)	Data  0.142 ( 0.142)	Loss 1.2741e-02 (1.2741e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [71][ 10/391]	Time  0.241 ( 0.256)	Data  0.001 ( 0.014)	Loss 5.5284e-02 (3.0173e-02)	Acc@1  98.44 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [71][ 20/391]	Time  0.240 ( 0.249)	Data  0.001 ( 0.008)	Loss 1.5029e-02 (2.5637e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [71][ 30/391]	Time  0.241 ( 0.247)	Data  0.001 ( 0.006)	Loss 1.3993e-02 (2.5388e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [71][ 40/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.004)	Loss 3.7859e-02 (2.4295e-02)	Acc@1  98.44 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [71][ 50/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.004)	Loss 2.3246e-02 (2.3321e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [71][ 60/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.003)	Loss 2.1585e-02 (2.3558e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [71][ 70/391]	Time  0.247 ( 0.244)	Data  0.001 ( 0.003)	Loss 1.5650e-02 (2.3690e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [71][ 80/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.003)	Loss 1.9256e-02 (2.4022e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [71][ 90/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.003)	Loss 1.7527e-02 (2.3743e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [71][100/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.002)	Loss 2.9931e-02 (2.3752e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [71][110/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.5238e-02 (2.3589e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [71][120/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.4432e-02 (2.3768e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [71][130/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1068e-02 (2.3564e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [71][140/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1694e-02 (2.3264e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [71][150/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 5.6814e-02 (2.3389e-02)	Acc@1  97.66 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [71][160/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.3159e-02 (2.3203e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [71][170/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 8.9748e-02 (2.3656e-02)	Acc@1  98.44 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [71][180/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.6300e-02 (2.3693e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [71][190/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.7003e-02 (2.3805e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [71][200/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.7486e-02 (2.3729e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [71][210/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.5915e-02 (2.3812e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [71][220/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.4000e-02 (2.3554e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [71][230/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.7078e-02 (2.3717e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [71][240/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.8722e-02 (2.3762e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [71][250/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1263e-02 (2.3592e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [71][260/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.5462e-02 (2.3637e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [71][270/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 9.6947e-03 (2.3557e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [71][280/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.0866e-02 (2.3425e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [71][290/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.7672e-02 (2.3338e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [71][300/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.4487e-02 (2.3524e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [71][310/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.2325e-02 (2.3604e-02)	Acc@1  97.66 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [71][320/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.1138e-02 (2.3638e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [71][330/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.001)	Loss 2.4781e-02 (2.3587e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [71][340/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.001)	Loss 1.8544e-02 (2.3545e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [71][350/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.001)	Loss 2.3530e-02 (2.3501e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [71][360/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.001)	Loss 1.8631e-02 (2.3395e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [71][370/391]	Time  0.247 ( 0.243)	Data  0.001 ( 0.001)	Loss 3.5170e-02 (2.3547e-02)	Acc@1  98.44 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [71][380/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.001)	Loss 6.0208e-02 (2.3608e-02)	Acc@1  97.66 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [71][390/391]	Time  0.181 ( 0.242)	Data  0.001 ( 0.001)	Loss 4.1720e-02 (2.3638e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
## e[71] optimizer.zero_grad (sum) time: 0.49332690238952637
## e[71]       loss.backward (sum) time: 11.161300897598267
## e[71]      optimizer.step (sum) time: 47.821117877960205
## epoch[71] training(only) time: 94.85580158233643
# Switched to evaluate mode...
Test: [  0/100]	Time  0.235 ( 0.235)	Loss 1.3619e+00 (1.3619e+00)	Acc@1  75.00 ( 75.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.078 ( 0.092)	Loss 1.4058e+00 (1.3584e+00)	Acc@1  70.00 ( 73.36)	Acc@5  93.00 ( 91.09)
Test: [ 20/100]	Time  0.078 ( 0.085)	Loss 1.2864e+00 (1.3150e+00)	Acc@1  68.00 ( 73.14)	Acc@5  93.00 ( 91.95)
Test: [ 30/100]	Time  0.079 ( 0.083)	Loss 1.5804e+00 (1.3669e+00)	Acc@1  65.00 ( 72.19)	Acc@5  93.00 ( 91.61)
Test: [ 40/100]	Time  0.080 ( 0.082)	Loss 1.4634e+00 (1.3714e+00)	Acc@1  75.00 ( 71.88)	Acc@5  92.00 ( 91.80)
Test: [ 50/100]	Time  0.077 ( 0.081)	Loss 1.7105e+00 (1.3908e+00)	Acc@1  70.00 ( 71.82)	Acc@5  88.00 ( 91.43)
Test: [ 60/100]	Time  0.078 ( 0.081)	Loss 1.5272e+00 (1.3650e+00)	Acc@1  68.00 ( 72.05)	Acc@5  93.00 ( 91.89)
Test: [ 70/100]	Time  0.079 ( 0.080)	Loss 1.4953e+00 (1.3703e+00)	Acc@1  70.00 ( 72.11)	Acc@5  88.00 ( 91.83)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 1.3691e+00 (1.3630e+00)	Acc@1  70.00 ( 72.23)	Acc@5  93.00 ( 91.88)
Test: [ 90/100]	Time  0.080 ( 0.080)	Loss 1.6591e+00 (1.3421e+00)	Acc@1  66.00 ( 72.49)	Acc@5  90.00 ( 91.97)
 * Acc@1 72.600 Acc@5 92.040
### epoch[71] execution time: 102.9155433177948
EPOCH 72
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [72][  0/391]	Time  0.395 ( 0.395)	Data  0.148 ( 0.148)	Loss 3.4058e-02 (3.4058e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [72][ 10/391]	Time  0.242 ( 0.255)	Data  0.001 ( 0.015)	Loss 4.2009e-02 (2.2864e-02)	Acc@1  98.44 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [72][ 20/391]	Time  0.240 ( 0.249)	Data  0.001 ( 0.008)	Loss 3.2219e-02 (2.1914e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [72][ 30/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.006)	Loss 2.0753e-02 (2.1704e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [72][ 40/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.005)	Loss 9.1317e-03 (2.2458e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [72][ 50/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 4.0626e-02 (2.5056e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [72][ 60/391]	Time  0.248 ( 0.244)	Data  0.001 ( 0.004)	Loss 2.0273e-02 (2.6348e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [72][ 70/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 2.5677e-02 (2.5866e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [72][ 80/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 1.4461e-02 (2.6048e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [72][ 90/391]	Time  0.251 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.2128e-02 (2.6078e-02)	Acc@1  98.44 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [72][100/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.4399e-02 (2.5768e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [72][110/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.3174e-02 (2.5597e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [72][120/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.8270e-02 (2.5416e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [72][130/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.6687e-02 (2.5443e-02)	Acc@1  98.44 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [72][140/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.5894e-02 (2.5207e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [72][150/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.4365e-02 (2.5643e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [72][160/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.4358e-02 (2.5232e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [72][170/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.9686e-02 (2.5123e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [72][180/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.7594e-02 (2.4717e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [72][190/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.0800e-02 (2.4647e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [72][200/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7280e-02 (2.4355e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [72][210/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0899e-02 (2.4351e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [72][220/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1791e-02 (2.4036e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [72][230/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 8.9928e-03 (2.3646e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [72][240/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.5058e-02 (2.3663e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [72][250/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7772e-02 (2.3589e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [72][260/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.7554e-02 (2.3461e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [72][270/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9819e-02 (2.3325e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [72][280/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8900e-02 (2.3343e-02)	Acc@1  98.44 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [72][290/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8567e-02 (2.3389e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [72][300/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1856e-02 (2.3334e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [72][310/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9950e-02 (2.3458e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [72][320/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0201e-02 (2.3351e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [72][330/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0731e-02 (2.3420e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [72][340/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.2849e-02 (2.3510e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [72][350/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3999e-02 (2.3548e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [72][360/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.5225e-02 (2.3599e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [72][370/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.001)	Loss 4.1273e-02 (2.3597e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [72][380/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.001)	Loss 1.5626e-02 (2.3487e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [72][390/391]	Time  0.177 ( 0.242)	Data  0.001 ( 0.001)	Loss 9.9404e-02 (2.3481e-02)	Acc@1  98.75 ( 99.67)	Acc@5 100.00 (100.00)
## e[72] optimizer.zero_grad (sum) time: 0.49618029594421387
## e[72]       loss.backward (sum) time: 11.157484769821167
## e[72]      optimizer.step (sum) time: 47.785582065582275
## epoch[72] training(only) time: 94.76920413970947
# Switched to evaluate mode...
Test: [  0/100]	Time  0.207 ( 0.207)	Loss 1.4006e+00 (1.4006e+00)	Acc@1  73.00 ( 73.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.3817e+00 (1.3605e+00)	Acc@1  70.00 ( 72.91)	Acc@5  92.00 ( 91.18)
Test: [ 20/100]	Time  0.079 ( 0.084)	Loss 1.2642e+00 (1.3159e+00)	Acc@1  67.00 ( 73.05)	Acc@5  95.00 ( 92.14)
Test: [ 30/100]	Time  0.077 ( 0.082)	Loss 1.5811e+00 (1.3678e+00)	Acc@1  62.00 ( 72.13)	Acc@5  93.00 ( 91.84)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.4874e+00 (1.3760e+00)	Acc@1  74.00 ( 71.76)	Acc@5  93.00 ( 91.98)
Test: [ 50/100]	Time  0.078 ( 0.080)	Loss 1.7197e+00 (1.3935e+00)	Acc@1  71.00 ( 71.76)	Acc@5  89.00 ( 91.69)
Test: [ 60/100]	Time  0.077 ( 0.080)	Loss 1.5740e+00 (1.3678e+00)	Acc@1  68.00 ( 72.08)	Acc@5  92.00 ( 92.08)
Test: [ 70/100]	Time  0.079 ( 0.080)	Loss 1.4827e+00 (1.3739e+00)	Acc@1  71.00 ( 72.15)	Acc@5  89.00 ( 91.97)
Test: [ 80/100]	Time  0.078 ( 0.079)	Loss 1.3401e+00 (1.3661e+00)	Acc@1  71.00 ( 72.35)	Acc@5  93.00 ( 92.05)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.6848e+00 (1.3453e+00)	Acc@1  65.00 ( 72.58)	Acc@5  89.00 ( 92.10)
 * Acc@1 72.760 Acc@5 92.200
### epoch[72] execution time: 102.76408982276917
EPOCH 73
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [73][  0/391]	Time  0.428 ( 0.428)	Data  0.177 ( 0.177)	Loss 1.2793e-02 (1.2793e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [73][ 10/391]	Time  0.241 ( 0.258)	Data  0.001 ( 0.017)	Loss 1.8581e-02 (2.1873e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [73][ 20/391]	Time  0.242 ( 0.250)	Data  0.001 ( 0.010)	Loss 3.1236e-02 (2.4658e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [73][ 30/391]	Time  0.244 ( 0.247)	Data  0.001 ( 0.007)	Loss 2.0897e-02 (2.5332e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [73][ 40/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.005)	Loss 1.7051e-02 (2.5238e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [73][ 50/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.005)	Loss 2.3934e-02 (2.4582e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [73][ 60/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 3.2617e-02 (2.5969e-02)	Acc@1  98.44 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [73][ 70/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 2.4040e-02 (2.5844e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [73][ 80/391]	Time  0.247 ( 0.244)	Data  0.001 ( 0.003)	Loss 2.3236e-02 (2.5131e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [73][ 90/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.2700e-02 (2.4709e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [73][100/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.7380e-02 (2.4636e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [73][110/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.9674e-02 (2.4600e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [73][120/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.9554e-02 (2.4187e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [73][130/391]	Time  0.247 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.7597e-02 (2.4196e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [73][140/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.7924e-02 (2.4165e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [73][150/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.9675e-02 (2.3834e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [73][160/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.5833e-02 (2.3838e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [73][170/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.3733e-02 (2.3705e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [73][180/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1967e-02 (2.3826e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [73][190/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.1729e-02 (2.4132e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [73][200/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2761e-02 (2.4195e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [73][210/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8168e-02 (2.3941e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [73][220/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8717e-02 (2.3743e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [73][230/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6355e-02 (2.3716e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [73][240/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8819e-02 (2.3717e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [73][250/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6896e-02 (2.3622e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [73][260/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1475e-02 (2.3509e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [73][270/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0600e-02 (2.3405e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [73][280/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6170e-02 (2.3288e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [73][290/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1319e-02 (2.3387e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [73][300/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.4117e-02 (2.3463e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [73][310/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.1894e-02 (2.3389e-02)	Acc@1  98.44 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [73][320/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9174e-02 (2.3511e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [73][330/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1896e-02 (2.3547e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [73][340/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.5294e-02 (2.3566e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [73][350/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2671e-02 (2.3484e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [73][360/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7778e-02 (2.3464e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [73][370/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6779e-02 (2.3424e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [73][380/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.5516e-02 (2.3433e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [73][390/391]	Time  0.178 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.0776e-02 (2.3412e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
## e[73] optimizer.zero_grad (sum) time: 0.49434828758239746
## e[73]       loss.backward (sum) time: 11.143819332122803
## e[73]      optimizer.step (sum) time: 47.81101608276367
## epoch[73] training(only) time: 94.76911163330078
# Switched to evaluate mode...
Test: [  0/100]	Time  0.208 ( 0.208)	Loss 1.3678e+00 (1.3678e+00)	Acc@1  75.00 ( 75.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.077 ( 0.090)	Loss 1.4024e+00 (1.3617e+00)	Acc@1  70.00 ( 72.82)	Acc@5  92.00 ( 91.36)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 1.3037e+00 (1.3086e+00)	Acc@1  68.00 ( 73.29)	Acc@5  93.00 ( 92.10)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 1.5947e+00 (1.3611e+00)	Acc@1  64.00 ( 72.35)	Acc@5  93.00 ( 91.90)
Test: [ 40/100]	Time  0.077 ( 0.081)	Loss 1.4829e+00 (1.3677e+00)	Acc@1  75.00 ( 71.90)	Acc@5  93.00 ( 92.02)
Test: [ 50/100]	Time  0.078 ( 0.080)	Loss 1.7057e+00 (1.3874e+00)	Acc@1  73.00 ( 71.86)	Acc@5  89.00 ( 91.71)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.5179e+00 (1.3608e+00)	Acc@1  67.00 ( 72.10)	Acc@5  92.00 ( 92.08)
Test: [ 70/100]	Time  0.079 ( 0.080)	Loss 1.4451e+00 (1.3657e+00)	Acc@1  71.00 ( 72.18)	Acc@5  88.00 ( 91.99)
Test: [ 80/100]	Time  0.078 ( 0.079)	Loss 1.3533e+00 (1.3585e+00)	Acc@1  71.00 ( 72.32)	Acc@5  93.00 ( 92.05)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.6669e+00 (1.3381e+00)	Acc@1  65.00 ( 72.56)	Acc@5  89.00 ( 92.10)
 * Acc@1 72.690 Acc@5 92.170
### epoch[73] execution time: 102.7807686328888
EPOCH 74
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [74][  0/391]	Time  0.394 ( 0.394)	Data  0.143 ( 0.143)	Loss 1.7577e-02 (1.7577e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [74][ 10/391]	Time  0.241 ( 0.255)	Data  0.001 ( 0.014)	Loss 2.5005e-02 (2.0698e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [74][ 20/391]	Time  0.241 ( 0.248)	Data  0.001 ( 0.008)	Loss 1.7836e-02 (2.0859e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [74][ 30/391]	Time  0.240 ( 0.246)	Data  0.001 ( 0.006)	Loss 1.9529e-02 (2.0393e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [74][ 40/391]	Time  0.242 ( 0.245)	Data  0.001 ( 0.005)	Loss 1.6717e-02 (2.1882e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [74][ 50/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 7.7464e-02 (2.3606e-02)	Acc@1  98.44 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [74][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 1.8643e-02 (2.3041e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [74][ 70/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.6530e-02 (2.2496e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [74][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 4.5712e-02 (2.3105e-02)	Acc@1  98.44 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [74][ 90/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.8675e-02 (2.2819e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [74][100/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.5539e-02 (2.2641e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [74][110/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.6894e-02 (2.2944e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [74][120/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.7716e-02 (2.2854e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [74][130/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0834e-02 (2.2770e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [74][140/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1869e-02 (2.2789e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [74][150/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3616e-02 (2.2552e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [74][160/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0349e-02 (2.2911e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [74][170/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4631e-02 (2.3206e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [74][180/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.3299e-02 (2.2962e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [74][190/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3244e-02 (2.2885e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [74][200/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1876e-02 (2.3057e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [74][210/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.4348e-02 (2.3095e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [74][220/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.1663e-02 (2.3026e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [74][230/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.7429e-02 (2.3076e-02)	Acc@1  98.44 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [74][240/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.1130e-02 (2.3089e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [74][250/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1635e-02 (2.3298e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [74][260/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.5533e-02 (2.3124e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [74][270/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.4680e-02 (2.3134e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [74][280/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4626e-02 (2.3124e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [74][290/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4692e-02 (2.3158e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [74][300/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3507e-02 (2.3179e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [74][310/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 5.9485e-02 (2.3189e-02)	Acc@1  98.44 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [74][320/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.7389e-02 (2.3489e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [74][330/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.001)	Loss 2.0216e-02 (2.3359e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [74][340/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.001)	Loss 2.3281e-02 (2.3271e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [74][350/391]	Time  0.252 ( 0.242)	Data  0.001 ( 0.001)	Loss 5.2425e-02 (2.3275e-02)	Acc@1  98.44 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [74][360/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.001)	Loss 2.4812e-02 (2.3197e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [74][370/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.001)	Loss 1.5635e-02 (2.3273e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [74][380/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.001)	Loss 2.4363e-02 (2.3278e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [74][390/391]	Time  0.178 ( 0.242)	Data  0.001 ( 0.001)	Loss 9.8060e-02 (2.3479e-02)	Acc@1  96.25 ( 99.65)	Acc@5 100.00 (100.00)
## e[74] optimizer.zero_grad (sum) time: 0.4946403503417969
## e[74]       loss.backward (sum) time: 11.181435585021973
## e[74]      optimizer.step (sum) time: 47.778878927230835
## epoch[74] training(only) time: 94.68882298469543
# Switched to evaluate mode...
Test: [  0/100]	Time  0.217 ( 0.217)	Loss 1.3733e+00 (1.3733e+00)	Acc@1  75.00 ( 75.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.078 ( 0.091)	Loss 1.3826e+00 (1.3547e+00)	Acc@1  70.00 ( 72.91)	Acc@5  93.00 ( 91.27)
Test: [ 20/100]	Time  0.078 ( 0.085)	Loss 1.3011e+00 (1.3185e+00)	Acc@1  67.00 ( 73.14)	Acc@5  94.00 ( 92.14)
Test: [ 30/100]	Time  0.079 ( 0.082)	Loss 1.5693e+00 (1.3723e+00)	Acc@1  67.00 ( 72.35)	Acc@5  93.00 ( 91.90)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.4817e+00 (1.3803e+00)	Acc@1  74.00 ( 71.95)	Acc@5  94.00 ( 92.05)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 1.7308e+00 (1.3984e+00)	Acc@1  71.00 ( 71.84)	Acc@5  89.00 ( 91.75)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.5695e+00 (1.3719e+00)	Acc@1  68.00 ( 72.11)	Acc@5  92.00 ( 92.08)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 1.4662e+00 (1.3764e+00)	Acc@1  71.00 ( 72.27)	Acc@5  89.00 ( 91.97)
Test: [ 80/100]	Time  0.079 ( 0.080)	Loss 1.3525e+00 (1.3698e+00)	Acc@1  71.00 ( 72.38)	Acc@5  93.00 ( 92.02)
Test: [ 90/100]	Time  0.078 ( 0.080)	Loss 1.6645e+00 (1.3483e+00)	Acc@1  66.00 ( 72.66)	Acc@5  89.00 ( 92.09)
 * Acc@1 72.810 Acc@5 92.130
### epoch[74] execution time: 102.72200274467468
EPOCH 75
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [75][  0/391]	Time  0.402 ( 0.402)	Data  0.155 ( 0.155)	Loss 2.3823e-02 (2.3823e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [75][ 10/391]	Time  0.241 ( 0.256)	Data  0.001 ( 0.015)	Loss 2.7328e-02 (2.3779e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [75][ 20/391]	Time  0.241 ( 0.249)	Data  0.001 ( 0.009)	Loss 2.2283e-02 (2.4026e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [75][ 30/391]	Time  0.240 ( 0.247)	Data  0.001 ( 0.006)	Loss 3.4550e-02 (2.3322e-02)	Acc@1  98.44 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [75][ 40/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.005)	Loss 3.4229e-02 (2.3373e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [75][ 50/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.004)	Loss 2.9813e-02 (2.5026e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [75][ 60/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.004)	Loss 1.5225e-02 (2.4341e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [75][ 70/391]	Time  0.248 ( 0.244)	Data  0.001 ( 0.003)	Loss 2.3071e-02 (2.3706e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [75][ 80/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 3.0049e-02 (2.3641e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [75][ 90/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 2.1140e-02 (2.3314e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [75][100/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 2.1740e-02 (2.3845e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [75][110/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.5168e-02 (2.4281e-02)	Acc@1  98.44 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [75][120/391]	Time  0.246 ( 0.243)	Data  0.001 ( 0.002)	Loss 6.3444e-02 (2.4546e-02)	Acc@1  98.44 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [75][130/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.8689e-02 (2.4698e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [75][140/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1666e-02 (2.4443e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 ( 99.99)
Epoch: [75][150/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 9.6979e-03 (2.4108e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 ( 99.99)
Epoch: [75][160/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.7760e-02 (2.4121e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [75][170/391]	Time  0.254 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.9877e-02 (2.4045e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [75][180/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.6550e-02 (2.3992e-02)	Acc@1  97.66 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [75][190/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.3851e-02 (2.4254e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [75][200/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.0324e-02 (2.4196e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [75][210/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 5.6108e-02 (2.4336e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [75][220/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.4047e-02 (2.4133e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [75][230/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.3072e-02 (2.4068e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [75][240/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.7593e-02 (2.4031e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [75][250/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.2312e-02 (2.3832e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [75][260/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.9953e-02 (2.4025e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [75][270/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.0743e-02 (2.3862e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [75][280/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1010e-02 (2.3862e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [75][290/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.2035e-02 (2.3805e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [75][300/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.2914e-02 (2.3720e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [75][310/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 5.2442e-02 (2.3840e-02)	Acc@1  98.44 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [75][320/391]	Time  0.255 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.6989e-02 (2.3856e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [75][330/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 8.6622e-03 (2.3891e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [75][340/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.0484e-02 (2.3846e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [75][350/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.1904e-02 (2.3960e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [75][360/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.1005e-02 (2.4002e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [75][370/391]	Time  0.247 ( 0.243)	Data  0.001 ( 0.002)	Loss 6.5272e-02 (2.3989e-02)	Acc@1  97.66 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [75][380/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.2969e-02 (2.4137e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [75][390/391]	Time  0.181 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.5112e-02 (2.4202e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
## e[75] optimizer.zero_grad (sum) time: 0.49044108390808105
## e[75]       loss.backward (sum) time: 11.202614545822144
## e[75]      optimizer.step (sum) time: 47.82462501525879
## epoch[75] training(only) time: 94.92405939102173
# Switched to evaluate mode...
Test: [  0/100]	Time  0.209 ( 0.209)	Loss 1.3876e+00 (1.3876e+00)	Acc@1  73.00 ( 73.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.3569e+00 (1.3594e+00)	Acc@1  71.00 ( 72.91)	Acc@5  93.00 ( 91.36)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 1.3116e+00 (1.3159e+00)	Acc@1  68.00 ( 72.86)	Acc@5  91.00 ( 92.14)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 1.5901e+00 (1.3675e+00)	Acc@1  64.00 ( 71.94)	Acc@5  93.00 ( 91.84)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.4807e+00 (1.3733e+00)	Acc@1  76.00 ( 71.78)	Acc@5  92.00 ( 91.98)
Test: [ 50/100]	Time  0.078 ( 0.080)	Loss 1.7232e+00 (1.3950e+00)	Acc@1  69.00 ( 71.63)	Acc@5  90.00 ( 91.57)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.5683e+00 (1.3679e+00)	Acc@1  66.00 ( 71.97)	Acc@5  92.00 ( 91.93)
Test: [ 70/100]	Time  0.077 ( 0.080)	Loss 1.4468e+00 (1.3716e+00)	Acc@1  71.00 ( 72.11)	Acc@5  88.00 ( 91.87)
Test: [ 80/100]	Time  0.078 ( 0.079)	Loss 1.3436e+00 (1.3650e+00)	Acc@1  73.00 ( 72.22)	Acc@5  92.00 ( 91.89)
Test: [ 90/100]	Time  0.077 ( 0.079)	Loss 1.6952e+00 (1.3443e+00)	Acc@1  65.00 ( 72.52)	Acc@5  89.00 ( 91.95)
 * Acc@1 72.690 Acc@5 92.000
### epoch[75] execution time: 102.94101619720459
EPOCH 76
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [76][  0/391]	Time  0.396 ( 0.396)	Data  0.153 ( 0.153)	Loss 4.3727e-02 (4.3727e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [76][ 10/391]	Time  0.240 ( 0.254)	Data  0.001 ( 0.015)	Loss 2.0414e-02 (2.4858e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [76][ 20/391]	Time  0.241 ( 0.248)	Data  0.001 ( 0.008)	Loss 2.2641e-02 (2.3298e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [76][ 30/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.006)	Loss 1.7112e-02 (2.2592e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [76][ 40/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.005)	Loss 2.2729e-02 (2.1906e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [76][ 50/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.6284e-02 (2.2254e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [76][ 60/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.8411e-02 (2.2086e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [76][ 70/391]	Time  0.243 ( 0.244)	Data  0.001 ( 0.003)	Loss 3.3308e-02 (2.3541e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [76][ 80/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.003)	Loss 2.1142e-02 (2.3430e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [76][ 90/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.7724e-02 (2.3139e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [76][100/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.8552e-02 (2.2854e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [76][110/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.1064e-02 (2.3005e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [76][120/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.0162e-02 (2.3240e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [76][130/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.0947e-02 (2.2992e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [76][140/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.3851e-02 (2.2903e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [76][150/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.5925e-02 (2.2717e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [76][160/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.4258e-02 (2.2549e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [76][170/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.2915e-02 (2.3191e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [76][180/391]	Time  0.247 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.0109e-02 (2.3034e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [76][190/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.6466e-02 (2.2872e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [76][200/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.1982e-02 (2.2713e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [76][210/391]	Time  0.251 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.9654e-02 (2.2642e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [76][220/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.3593e-02 (2.2587e-02)	Acc@1  98.44 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [76][230/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.8621e-02 (2.2645e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [76][240/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1994e-02 (2.2652e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [76][250/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.6838e-02 (2.2602e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [76][260/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.7102e-02 (2.2953e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [76][270/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.6429e-02 (2.3057e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [76][280/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 9.6932e-03 (2.2908e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [76][290/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.4138e-02 (2.3076e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [76][300/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1135e-02 (2.2852e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [76][310/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.5575e-02 (2.2774e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [76][320/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.7359e-02 (2.2743e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [76][330/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.7281e-02 (2.2753e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [76][340/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.6454e-02 (2.2743e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [76][350/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.7644e-02 (2.2704e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [76][360/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.7823e-02 (2.2673e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [76][370/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.3261e-02 (2.2591e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [76][380/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.6335e-02 (2.2595e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [76][390/391]	Time  0.179 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.4405e-02 (2.2474e-02)	Acc@1  98.75 ( 99.70)	Acc@5 100.00 (100.00)
## e[76] optimizer.zero_grad (sum) time: 0.49574899673461914
## e[76]       loss.backward (sum) time: 11.143172025680542
## e[76]      optimizer.step (sum) time: 47.84133243560791
## epoch[76] training(only) time: 94.88927412033081
# Switched to evaluate mode...
Test: [  0/100]	Time  0.207 ( 0.207)	Loss 1.4100e+00 (1.4100e+00)	Acc@1  74.00 ( 74.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.079 ( 0.090)	Loss 1.3761e+00 (1.3580e+00)	Acc@1  70.00 ( 73.00)	Acc@5  93.00 ( 91.64)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 1.3070e+00 (1.3185e+00)	Acc@1  68.00 ( 73.38)	Acc@5  93.00 ( 92.38)
Test: [ 30/100]	Time  0.079 ( 0.082)	Loss 1.5839e+00 (1.3690e+00)	Acc@1  66.00 ( 72.39)	Acc@5  92.00 ( 92.06)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.4964e+00 (1.3797e+00)	Acc@1  77.00 ( 71.98)	Acc@5  93.00 ( 92.24)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 1.7497e+00 (1.3989e+00)	Acc@1  70.00 ( 71.82)	Acc@5  89.00 ( 91.76)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.5553e+00 (1.3749e+00)	Acc@1  68.00 ( 71.98)	Acc@5  92.00 ( 92.10)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 1.4959e+00 (1.3792e+00)	Acc@1  70.00 ( 72.07)	Acc@5  89.00 ( 91.99)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 1.3705e+00 (1.3731e+00)	Acc@1  72.00 ( 72.21)	Acc@5  93.00 ( 91.99)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.7201e+00 (1.3524e+00)	Acc@1  64.00 ( 72.45)	Acc@5  90.00 ( 92.05)
 * Acc@1 72.550 Acc@5 92.140
### epoch[76] execution time: 102.91233992576599
EPOCH 77
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [77][  0/391]	Time  0.394 ( 0.394)	Data  0.152 ( 0.152)	Loss 3.1461e-02 (3.1461e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [77][ 10/391]	Time  0.240 ( 0.254)	Data  0.001 ( 0.015)	Loss 1.5135e-02 (2.6761e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 ( 99.93)
Epoch: [77][ 20/391]	Time  0.241 ( 0.248)	Data  0.001 ( 0.008)	Loss 3.9118e-02 (2.7373e-02)	Acc@1  98.44 ( 99.40)	Acc@5 100.00 ( 99.96)
Epoch: [77][ 30/391]	Time  0.242 ( 0.246)	Data  0.001 ( 0.006)	Loss 1.9376e-02 (2.5809e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 ( 99.97)
Epoch: [77][ 40/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.005)	Loss 3.1583e-02 (2.5637e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 ( 99.98)
Epoch: [77][ 50/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.004)	Loss 2.0687e-02 (2.4908e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 ( 99.98)
Epoch: [77][ 60/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.004)	Loss 2.4661e-02 (2.4283e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 ( 99.99)
Epoch: [77][ 70/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 1.4347e-02 (2.4555e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 ( 99.99)
Epoch: [77][ 80/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 2.1756e-02 (2.3471e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 ( 99.99)
Epoch: [77][ 90/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.7575e-02 (2.4623e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 ( 99.98)
Epoch: [77][100/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.3444e-02 (2.4029e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 ( 99.98)
Epoch: [77][110/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.6451e-02 (2.4111e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 ( 99.99)
Epoch: [77][120/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.1773e-02 (2.3545e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 ( 99.99)
Epoch: [77][130/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.7292e-02 (2.3414e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 ( 99.99)
Epoch: [77][140/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.8015e-02 (2.3300e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 ( 99.99)
Epoch: [77][150/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.2363e-02 (2.3077e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 ( 99.99)
Epoch: [77][160/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.6111e-02 (2.3005e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 ( 99.99)
Epoch: [77][170/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.5612e-02 (2.2850e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 ( 99.99)
Epoch: [77][180/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.3697e-02 (2.3005e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 ( 99.99)
Epoch: [77][190/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.1760e-02 (2.3334e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 ( 99.99)
Epoch: [77][200/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.2175e-02 (2.3395e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 ( 99.99)
Epoch: [77][210/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.6789e-02 (2.3251e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 ( 99.99)
Epoch: [77][220/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.5281e-02 (2.3003e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 ( 99.99)
Epoch: [77][230/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.2806e-02 (2.2895e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 ( 99.99)
Epoch: [77][240/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.2405e-02 (2.2705e-02)	Acc@1  98.44 ( 99.67)	Acc@5 100.00 ( 99.99)
Epoch: [77][250/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1890e-02 (2.2490e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 ( 99.99)
Epoch: [77][260/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.0874e-02 (2.2495e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 ( 99.99)
Epoch: [77][270/391]	Time  0.242 ( 0.243)	Data  0.002 ( 0.002)	Loss 2.0958e-02 (2.2408e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 ( 99.99)
Epoch: [77][280/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.1959e-02 (2.2581e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 ( 99.99)
Epoch: [77][290/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.0660e-02 (2.2775e-02)	Acc@1  98.44 ( 99.66)	Acc@5 100.00 ( 99.99)
Epoch: [77][300/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.2160e-02 (2.2809e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 ( 99.99)
Epoch: [77][310/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7466e-02 (2.2784e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 ( 99.99)
Epoch: [77][320/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.3848e-02 (2.2698e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [77][330/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.0558e-02 (2.2713e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [77][340/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.9096e-02 (2.2693e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [77][350/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.1614e-02 (2.2590e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [77][360/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.0640e-02 (2.2583e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [77][370/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.4002e-02 (2.2609e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [77][380/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.2305e-02 (2.2552e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [77][390/391]	Time  0.178 ( 0.242)	Data  0.001 ( 0.001)	Loss 9.7966e-03 (2.2608e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
## e[77] optimizer.zero_grad (sum) time: 0.49308323860168457
## e[77]       loss.backward (sum) time: 11.148174285888672
## e[77]      optimizer.step (sum) time: 47.81274366378784
## epoch[77] training(only) time: 94.84958910942078
# Switched to evaluate mode...
Test: [  0/100]	Time  0.215 ( 0.215)	Loss 1.3586e+00 (1.3586e+00)	Acc@1  74.00 ( 74.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.3642e+00 (1.3632e+00)	Acc@1  69.00 ( 72.73)	Acc@5  93.00 ( 91.36)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 1.3107e+00 (1.3227e+00)	Acc@1  68.00 ( 73.19)	Acc@5  92.00 ( 92.29)
Test: [ 30/100]	Time  0.077 ( 0.082)	Loss 1.5597e+00 (1.3710e+00)	Acc@1  66.00 ( 72.26)	Acc@5  93.00 ( 91.94)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.4729e+00 (1.3792e+00)	Acc@1  75.00 ( 71.98)	Acc@5  93.00 ( 92.05)
Test: [ 50/100]	Time  0.077 ( 0.080)	Loss 1.7524e+00 (1.4003e+00)	Acc@1  72.00 ( 71.86)	Acc@5  88.00 ( 91.69)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.5559e+00 (1.3720e+00)	Acc@1  67.00 ( 72.15)	Acc@5  92.00 ( 92.15)
Test: [ 70/100]	Time  0.079 ( 0.080)	Loss 1.4412e+00 (1.3761e+00)	Acc@1  71.00 ( 72.20)	Acc@5  89.00 ( 92.03)
Test: [ 80/100]	Time  0.077 ( 0.080)	Loss 1.3757e+00 (1.3692e+00)	Acc@1  71.00 ( 72.33)	Acc@5  93.00 ( 92.02)
Test: [ 90/100]	Time  0.079 ( 0.079)	Loss 1.6694e+00 (1.3486e+00)	Acc@1  66.00 ( 72.57)	Acc@5  90.00 ( 92.13)
 * Acc@1 72.720 Acc@5 92.170
### epoch[77] execution time: 102.88420486450195
EPOCH 78
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [78][  0/391]	Time  0.388 ( 0.388)	Data  0.147 ( 0.147)	Loss 1.7332e-02 (1.7332e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [78][ 10/391]	Time  0.249 ( 0.255)	Data  0.001 ( 0.014)	Loss 4.1022e-02 (2.7726e-02)	Acc@1  98.44 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [78][ 20/391]	Time  0.241 ( 0.248)	Data  0.001 ( 0.008)	Loss 3.4008e-02 (2.6112e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [78][ 30/391]	Time  0.242 ( 0.246)	Data  0.001 ( 0.006)	Loss 2.4375e-02 (2.6160e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [78][ 40/391]	Time  0.243 ( 0.245)	Data  0.001 ( 0.005)	Loss 3.1187e-02 (2.4821e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [78][ 50/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.004)	Loss 2.4372e-02 (2.3697e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [78][ 60/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.5782e-02 (2.3589e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [78][ 70/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 4.8753e-02 (2.4714e-02)	Acc@1  98.44 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [78][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.7243e-02 (2.4927e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [78][ 90/391]	Time  0.246 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.0919e-02 (2.4565e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [78][100/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.5273e-02 (2.4528e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [78][110/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.7222e-02 (2.4308e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [78][120/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.8653e-02 (2.3899e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [78][130/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.8250e-02 (2.3942e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [78][140/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.3641e-02 (2.3778e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [78][150/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1004e-02 (2.3872e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [78][160/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1305e-02 (2.3578e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [78][170/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.4389e-02 (2.3723e-02)	Acc@1  98.44 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [78][180/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2109e-02 (2.3524e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [78][190/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3303e-02 (2.3447e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [78][200/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.5576e-02 (2.3221e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [78][210/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.4338e-02 (2.3587e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [78][220/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5676e-02 (2.3544e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [78][230/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9885e-02 (2.3642e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [78][240/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6047e-02 (2.3708e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [78][250/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9059e-02 (2.3667e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [78][260/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5726e-02 (2.3537e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [78][270/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9762e-02 (2.3443e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [78][280/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2936e-02 (2.3516e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [78][290/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7802e-02 (2.3364e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [78][300/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4141e-02 (2.3349e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [78][310/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.8034e-02 (2.3319e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [78][320/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2290e-02 (2.3381e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [78][330/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.4805e-02 (2.3184e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [78][340/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9152e-02 (2.3025e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [78][350/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3755e-02 (2.2916e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [78][360/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.001)	Loss 1.5554e-02 (2.2992e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [78][370/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.001)	Loss 3.2789e-02 (2.2962e-02)	Acc@1  98.44 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [78][380/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.001)	Loss 2.6470e-02 (2.3013e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [78][390/391]	Time  0.177 ( 0.242)	Data  0.001 ( 0.001)	Loss 4.5439e-02 (2.3005e-02)	Acc@1  98.75 ( 99.63)	Acc@5 100.00 (100.00)
## e[78] optimizer.zero_grad (sum) time: 0.49191927909851074
## e[78]       loss.backward (sum) time: 11.128499031066895
## e[78]      optimizer.step (sum) time: 47.78797197341919
## epoch[78] training(only) time: 94.75882291793823
# Switched to evaluate mode...
Test: [  0/100]	Time  0.218 ( 0.218)	Loss 1.4412e+00 (1.4412e+00)	Acc@1  74.00 ( 74.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.079 ( 0.091)	Loss 1.3923e+00 (1.3681e+00)	Acc@1  70.00 ( 72.55)	Acc@5  93.00 ( 91.27)
Test: [ 20/100]	Time  0.078 ( 0.085)	Loss 1.3527e+00 (1.3199e+00)	Acc@1  65.00 ( 72.62)	Acc@5  92.00 ( 92.00)
Test: [ 30/100]	Time  0.079 ( 0.083)	Loss 1.5740e+00 (1.3740e+00)	Acc@1  64.00 ( 71.81)	Acc@5  93.00 ( 91.81)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.4953e+00 (1.3818e+00)	Acc@1  75.00 ( 71.54)	Acc@5  93.00 ( 91.93)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 1.6920e+00 (1.4006e+00)	Acc@1  70.00 ( 71.51)	Acc@5  90.00 ( 91.65)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.5738e+00 (1.3758e+00)	Acc@1  67.00 ( 71.85)	Acc@5  90.00 ( 92.00)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 1.4859e+00 (1.3776e+00)	Acc@1  70.00 ( 71.93)	Acc@5  88.00 ( 91.93)
Test: [ 80/100]	Time  0.077 ( 0.080)	Loss 1.3844e+00 (1.3711e+00)	Acc@1  71.00 ( 72.11)	Acc@5  92.00 ( 91.89)
Test: [ 90/100]	Time  0.078 ( 0.080)	Loss 1.6873e+00 (1.3499e+00)	Acc@1  65.00 ( 72.41)	Acc@5  90.00 ( 91.97)
 * Acc@1 72.520 Acc@5 92.000
### epoch[78] execution time: 102.79145932197571
EPOCH 79
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [79][  0/391]	Time  0.398 ( 0.398)	Data  0.154 ( 0.154)	Loss 3.6511e-02 (3.6511e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [79][ 10/391]	Time  0.242 ( 0.255)	Data  0.001 ( 0.015)	Loss 2.6037e-02 (2.0129e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [79][ 20/391]	Time  0.241 ( 0.249)	Data  0.001 ( 0.008)	Loss 3.4009e-02 (2.2910e-02)	Acc@1  98.44 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [79][ 30/391]	Time  0.241 ( 0.247)	Data  0.001 ( 0.006)	Loss 2.3137e-02 (2.4093e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [79][ 40/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.005)	Loss 1.8901e-02 (2.3020e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [79][ 50/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.1687e-02 (2.2796e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [79][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 3.1780e-02 (2.3199e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [79][ 70/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.003)	Loss 2.9866e-02 (2.3043e-02)	Acc@1  98.44 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [79][ 80/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.8980e-02 (2.2913e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [79][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.3820e-02 (2.2644e-02)	Acc@1  98.44 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [79][100/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.8354e-02 (2.2702e-02)	Acc@1  98.44 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [79][110/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.6029e-02 (2.2444e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [79][120/391]	Time  0.250 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.5688e-02 (2.2418e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [79][130/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.0378e-02 (2.2503e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [79][140/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.5984e-02 (2.2374e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [79][150/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.7645e-02 (2.2715e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [79][160/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.5734e-02 (2.2642e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [79][170/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6850e-02 (2.2497e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [79][180/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5622e-02 (2.2628e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [79][190/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7381e-02 (2.2705e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [79][200/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5181e-02 (2.2737e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [79][210/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1274e-02 (2.2622e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [79][220/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.9160e-02 (2.2372e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [79][230/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3665e-02 (2.2630e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [79][240/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3356e-02 (2.2551e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [79][250/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7228e-02 (2.2610e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [79][260/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2101e-02 (2.2704e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [79][270/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.0056e-02 (2.3007e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [79][280/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.3776e-02 (2.2898e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [79][290/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.3912e-02 (2.2793e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [79][300/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.6668e-02 (2.2853e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [79][310/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1065e-02 (2.2833e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [79][320/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0414e-02 (2.2870e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [79][330/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5611e-02 (2.2817e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [79][340/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0457e-02 (2.2842e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [79][350/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8307e-02 (2.2819e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [79][360/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1545e-02 (2.2728e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [79][370/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5103e-02 (2.2706e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [79][380/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0341e-02 (2.2650e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [79][390/391]	Time  0.179 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.7635e-02 (2.2796e-02)	Acc@1  98.75 ( 99.65)	Acc@5 100.00 (100.00)
## e[79] optimizer.zero_grad (sum) time: 0.4927940368652344
## e[79]       loss.backward (sum) time: 11.12751841545105
## e[79]      optimizer.step (sum) time: 47.807130336761475
## epoch[79] training(only) time: 94.76968932151794
# Switched to evaluate mode...
Test: [  0/100]	Time  0.209 ( 0.209)	Loss 1.3986e+00 (1.3986e+00)	Acc@1  73.00 ( 73.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.079 ( 0.090)	Loss 1.3577e+00 (1.3561e+00)	Acc@1  71.00 ( 72.82)	Acc@5  92.00 ( 91.09)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 1.3069e+00 (1.3129e+00)	Acc@1  67.00 ( 73.00)	Acc@5  93.00 ( 91.90)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 1.5520e+00 (1.3636e+00)	Acc@1  65.00 ( 72.19)	Acc@5  93.00 ( 91.74)
Test: [ 40/100]	Time  0.077 ( 0.081)	Loss 1.4681e+00 (1.3729e+00)	Acc@1  75.00 ( 71.88)	Acc@5  93.00 ( 91.85)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 1.7262e+00 (1.3930e+00)	Acc@1  69.00 ( 71.76)	Acc@5  89.00 ( 91.53)
Test: [ 60/100]	Time  0.079 ( 0.080)	Loss 1.5687e+00 (1.3678e+00)	Acc@1  67.00 ( 72.07)	Acc@5  91.00 ( 91.90)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 1.4568e+00 (1.3713e+00)	Acc@1  71.00 ( 72.17)	Acc@5  88.00 ( 91.89)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 1.3784e+00 (1.3652e+00)	Acc@1  71.00 ( 72.33)	Acc@5  93.00 ( 91.90)
Test: [ 90/100]	Time  0.078 ( 0.080)	Loss 1.6611e+00 (1.3444e+00)	Acc@1  66.00 ( 72.59)	Acc@5  90.00 ( 91.97)
 * Acc@1 72.740 Acc@5 92.040
### epoch[79] execution time: 102.79383826255798
EPOCH 80
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [80][  0/391]	Time  0.389 ( 0.389)	Data  0.144 ( 0.144)	Loss 2.0765e-02 (2.0765e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [80][ 10/391]	Time  0.241 ( 0.254)	Data  0.001 ( 0.014)	Loss 3.0624e-02 (2.2659e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [80][ 20/391]	Time  0.241 ( 0.248)	Data  0.001 ( 0.008)	Loss 1.7526e-02 (2.1406e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [80][ 30/391]	Time  0.240 ( 0.246)	Data  0.001 ( 0.006)	Loss 2.2042e-02 (2.3312e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 ( 99.97)
Epoch: [80][ 40/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.005)	Loss 1.3669e-02 (2.2854e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 ( 99.98)
Epoch: [80][ 50/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.004)	Loss 1.3121e-02 (2.2383e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 ( 99.98)
Epoch: [80][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 2.3569e-02 (2.1914e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 ( 99.99)
Epoch: [80][ 70/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 2.7435e-02 (2.2441e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 ( 99.99)
Epoch: [80][ 80/391]	Time  0.249 ( 0.244)	Data  0.001 ( 0.003)	Loss 1.4583e-02 (2.2120e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 ( 99.99)
Epoch: [80][ 90/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.5521e-02 (2.1952e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 ( 99.99)
Epoch: [80][100/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.1994e-02 (2.2435e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 ( 99.99)
Epoch: [80][110/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.2354e-02 (2.2037e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 ( 99.99)
Epoch: [80][120/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.1757e-02 (2.2135e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 ( 99.99)
Epoch: [80][130/391]	Time  0.246 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.9382e-02 (2.2127e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 ( 99.99)
Epoch: [80][140/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1688e-02 (2.1927e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 ( 99.99)
Epoch: [80][150/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1312e-02 (2.1979e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 ( 99.99)
Epoch: [80][160/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.0109e-02 (2.2023e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [80][170/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.7635e-02 (2.2022e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [80][180/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0608e-02 (2.1954e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [80][190/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.5052e-02 (2.2147e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [80][200/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1927e-02 (2.2093e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [80][210/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1623e-02 (2.2290e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [80][220/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7227e-02 (2.2207e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [80][230/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5307e-02 (2.2190e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [80][240/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.8479e-03 (2.2781e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [80][250/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2680e-02 (2.2509e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [80][260/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1705e-02 (2.2449e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [80][270/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9906e-02 (2.2449e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [80][280/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5465e-02 (2.2353e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [80][290/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7391e-02 (2.2349e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [80][300/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6330e-02 (2.2269e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [80][310/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.1551e-02 (2.2516e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [80][320/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.7394e-02 (2.2585e-02)	Acc@1  97.66 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [80][330/391]	Time  0.246 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7687e-02 (2.2522e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [80][340/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3514e-02 (2.2518e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [80][350/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6251e-02 (2.2472e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [80][360/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6243e-02 (2.2461e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [80][370/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3298e-02 (2.2602e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [80][380/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.9287e-02 (2.2571e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [80][390/391]	Time  0.178 ( 0.242)	Data  0.001 ( 0.001)	Loss 1.6551e-02 (2.2453e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
## e[80] optimizer.zero_grad (sum) time: 0.49136781692504883
## e[80]       loss.backward (sum) time: 11.13985538482666
## e[80]      optimizer.step (sum) time: 47.81684160232544
## epoch[80] training(only) time: 94.76218700408936
# Switched to evaluate mode...
Test: [  0/100]	Time  0.209 ( 0.209)	Loss 1.3892e+00 (1.3892e+00)	Acc@1  74.00 ( 74.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.3988e+00 (1.3669e+00)	Acc@1  71.00 ( 72.91)	Acc@5  93.00 ( 91.45)
Test: [ 20/100]	Time  0.077 ( 0.084)	Loss 1.3169e+00 (1.3255e+00)	Acc@1  65.00 ( 72.67)	Acc@5  92.00 ( 92.10)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 1.5595e+00 (1.3786e+00)	Acc@1  67.00 ( 71.87)	Acc@5  92.00 ( 91.87)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.4926e+00 (1.3859e+00)	Acc@1  74.00 ( 71.56)	Acc@5  93.00 ( 92.00)
Test: [ 50/100]	Time  0.078 ( 0.080)	Loss 1.6955e+00 (1.4018e+00)	Acc@1  69.00 ( 71.41)	Acc@5  88.00 ( 91.67)
Test: [ 60/100]	Time  0.077 ( 0.080)	Loss 1.5508e+00 (1.3768e+00)	Acc@1  67.00 ( 71.67)	Acc@5  93.00 ( 92.13)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 1.5325e+00 (1.3807e+00)	Acc@1  68.00 ( 71.82)	Acc@5  88.00 ( 92.08)
Test: [ 80/100]	Time  0.077 ( 0.079)	Loss 1.3760e+00 (1.3744e+00)	Acc@1  71.00 ( 71.95)	Acc@5  92.00 ( 92.06)
Test: [ 90/100]	Time  0.079 ( 0.079)	Loss 1.6929e+00 (1.3524e+00)	Acc@1  65.00 ( 72.26)	Acc@5  89.00 ( 92.14)
 * Acc@1 72.420 Acc@5 92.190
### epoch[80] execution time: 102.77907466888428
EPOCH 81
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [81][  0/391]	Time  0.402 ( 0.402)	Data  0.150 ( 0.150)	Loss 2.6387e-02 (2.6387e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [81][ 10/391]	Time  0.241 ( 0.256)	Data  0.001 ( 0.015)	Loss 2.1119e-02 (2.0670e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [81][ 20/391]	Time  0.240 ( 0.249)	Data  0.001 ( 0.008)	Loss 2.0735e-02 (2.4571e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [81][ 30/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.006)	Loss 2.6447e-02 (2.3412e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [81][ 40/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.005)	Loss 3.4214e-02 (2.2651e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [81][ 50/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.004)	Loss 9.3161e-03 (2.2256e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [81][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.2181e-02 (2.2350e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [81][ 70/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.5103e-02 (2.1797e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [81][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.8398e-02 (2.2022e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [81][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.0013e-02 (2.2116e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [81][100/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.5173e-02 (2.2208e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [81][110/391]	Time  0.250 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.3857e-02 (2.2320e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [81][120/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.6705e-02 (2.2154e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [81][130/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.9496e-02 (2.2377e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [81][140/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.3605e-02 (2.2192e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [81][150/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.2212e-02 (2.2416e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [81][160/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.1214e-02 (2.2291e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [81][170/391]	Time  0.246 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.4059e-02 (2.2333e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [81][180/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.2878e-02 (2.2359e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [81][190/391]	Time  0.251 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.8587e-02 (2.2431e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [81][200/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.6843e-02 (2.2540e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [81][210/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.2449e-02 (2.2663e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [81][220/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.5848e-02 (2.2560e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [81][230/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.3879e-02 (2.2470e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [81][240/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.0086e-02 (2.2414e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [81][250/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.8832e-02 (2.2174e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [81][260/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.1035e-02 (2.2169e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [81][270/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.2253e-02 (2.2142e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [81][280/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.4984e-02 (2.2204e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [81][290/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 8.2540e-03 (2.2250e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [81][300/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.0808e-02 (2.2292e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [81][310/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.8692e-02 (2.2226e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [81][320/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.5203e-02 (2.2063e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [81][330/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.0050e-02 (2.1975e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [81][340/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.8338e-02 (2.1967e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [81][350/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.6084e-02 (2.2000e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [81][360/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.001)	Loss 1.6493e-02 (2.1929e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [81][370/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.001)	Loss 1.3590e-02 (2.1883e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [81][380/391]	Time  0.253 ( 0.243)	Data  0.001 ( 0.001)	Loss 2.6320e-02 (2.1953e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [81][390/391]	Time  0.178 ( 0.242)	Data  0.001 ( 0.001)	Loss 1.9224e-02 (2.2067e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
## e[81] optimizer.zero_grad (sum) time: 0.495042085647583
## e[81]       loss.backward (sum) time: 11.14630651473999
## e[81]      optimizer.step (sum) time: 47.825316190719604
## epoch[81] training(only) time: 94.89020347595215
# Switched to evaluate mode...
Test: [  0/100]	Time  0.217 ( 0.217)	Loss 1.4160e+00 (1.4160e+00)	Acc@1  74.00 ( 74.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.077 ( 0.091)	Loss 1.3796e+00 (1.3627e+00)	Acc@1  70.00 ( 73.09)	Acc@5  92.00 ( 91.27)
Test: [ 20/100]	Time  0.078 ( 0.085)	Loss 1.3268e+00 (1.3249e+00)	Acc@1  66.00 ( 73.24)	Acc@5  92.00 ( 91.90)
Test: [ 30/100]	Time  0.077 ( 0.082)	Loss 1.5655e+00 (1.3757e+00)	Acc@1  64.00 ( 72.10)	Acc@5  93.00 ( 91.71)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.5095e+00 (1.3847e+00)	Acc@1  75.00 ( 71.71)	Acc@5  93.00 ( 91.85)
Test: [ 50/100]	Time  0.077 ( 0.081)	Loss 1.7167e+00 (1.4006e+00)	Acc@1  71.00 ( 71.63)	Acc@5  88.00 ( 91.59)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.5807e+00 (1.3753e+00)	Acc@1  69.00 ( 72.03)	Acc@5  94.00 ( 91.97)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 1.5077e+00 (1.3793e+00)	Acc@1  71.00 ( 72.13)	Acc@5  88.00 ( 91.87)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 1.3858e+00 (1.3729e+00)	Acc@1  71.00 ( 72.23)	Acc@5  91.00 ( 91.88)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.6605e+00 (1.3517e+00)	Acc@1  65.00 ( 72.49)	Acc@5  90.00 ( 91.96)
 * Acc@1 72.690 Acc@5 92.000
### epoch[81] execution time: 102.91072130203247
EPOCH 82
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [82][  0/391]	Time  0.385 ( 0.385)	Data  0.143 ( 0.143)	Loss 1.5798e-02 (1.5798e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [82][ 10/391]	Time  0.242 ( 0.255)	Data  0.001 ( 0.014)	Loss 1.7017e-02 (2.1071e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [82][ 20/391]	Time  0.241 ( 0.248)	Data  0.001 ( 0.008)	Loss 1.8198e-02 (2.1358e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [82][ 30/391]	Time  0.242 ( 0.246)	Data  0.001 ( 0.006)	Loss 2.7345e-02 (2.0963e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [82][ 40/391]	Time  0.240 ( 0.245)	Data  0.001 ( 0.004)	Loss 1.6041e-02 (2.1672e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [82][ 50/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 2.5307e-02 (2.2804e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [82][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 1.2316e-02 (2.2705e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [82][ 70/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.9696e-02 (2.2709e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [82][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.3937e-02 (2.2402e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [82][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.4045e-02 (2.2665e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [82][100/391]	Time  0.252 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.6355e-02 (2.3020e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [82][110/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.5762e-02 (2.2953e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [82][120/391]	Time  0.246 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.2589e-02 (2.3026e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [82][130/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.6103e-02 (2.2927e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [82][140/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1052e-02 (2.3017e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [82][150/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.4789e-02 (2.2971e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [82][160/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.9867e-02 (2.2958e-02)	Acc@1  98.44 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [82][170/391]	Time  0.246 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.7272e-02 (2.2855e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [82][180/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.4735e-02 (2.2915e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [82][190/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.2795e-02 (2.2628e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [82][200/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1595e-02 (2.2741e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [82][210/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.5511e-02 (2.2609e-02)	Acc@1  98.44 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [82][220/391]	Time  0.247 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9837e-02 (2.2597e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [82][230/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9445e-02 (2.2630e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [82][240/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.6016e-02 (2.2521e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [82][250/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9098e-02 (2.2365e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [82][260/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.0910e-02 (2.2357e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [82][270/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0852e-02 (2.2431e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [82][280/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.3931e-02 (2.2339e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [82][290/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2215e-02 (2.2169e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [82][300/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9859e-02 (2.2191e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [82][310/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5075e-02 (2.2072e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [82][320/391]	Time  0.247 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3804e-02 (2.2055e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [82][330/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.4851e-02 (2.2234e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [82][340/391]	Time  0.250 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3354e-02 (2.2371e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [82][350/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.4618e-02 (2.2340e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [82][360/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.2287e-02 (2.2317e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [82][370/391]	Time  0.246 ( 0.242)	Data  0.001 ( 0.001)	Loss 1.9742e-02 (2.2321e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [82][380/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.001)	Loss 2.2273e-02 (2.2283e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [82][390/391]	Time  0.177 ( 0.242)	Data  0.001 ( 0.001)	Loss 3.9375e-02 (2.2261e-02)	Acc@1  97.50 ( 99.68)	Acc@5 100.00 (100.00)
## e[82] optimizer.zero_grad (sum) time: 0.4924955368041992
## e[82]       loss.backward (sum) time: 11.139071226119995
## e[82]      optimizer.step (sum) time: 47.81505370140076
## epoch[82] training(only) time: 94.79782462120056
# Switched to evaluate mode...
Test: [  0/100]	Time  0.208 ( 0.208)	Loss 1.4160e+00 (1.4160e+00)	Acc@1  74.00 ( 74.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.3690e+00 (1.3596e+00)	Acc@1  72.00 ( 72.91)	Acc@5  92.00 ( 91.45)
Test: [ 20/100]	Time  0.079 ( 0.084)	Loss 1.2929e+00 (1.3197e+00)	Acc@1  68.00 ( 73.14)	Acc@5  94.00 ( 92.14)
Test: [ 30/100]	Time  0.077 ( 0.082)	Loss 1.5692e+00 (1.3725e+00)	Acc@1  66.00 ( 72.10)	Acc@5  93.00 ( 91.90)
Test: [ 40/100]	Time  0.079 ( 0.081)	Loss 1.4566e+00 (1.3838e+00)	Acc@1  77.00 ( 71.73)	Acc@5  93.00 ( 92.02)
Test: [ 50/100]	Time  0.079 ( 0.080)	Loss 1.6940e+00 (1.4019e+00)	Acc@1  72.00 ( 71.65)	Acc@5  89.00 ( 91.69)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.5874e+00 (1.3764e+00)	Acc@1  68.00 ( 71.98)	Acc@5  92.00 ( 92.05)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 1.4978e+00 (1.3780e+00)	Acc@1  71.00 ( 72.14)	Acc@5  88.00 ( 91.96)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 1.3836e+00 (1.3709e+00)	Acc@1  71.00 ( 72.32)	Acc@5  92.00 ( 91.96)
Test: [ 90/100]	Time  0.077 ( 0.079)	Loss 1.7116e+00 (1.3497e+00)	Acc@1  65.00 ( 72.59)	Acc@5  89.00 ( 92.01)
 * Acc@1 72.760 Acc@5 92.040
### epoch[82] execution time: 102.80944442749023
EPOCH 83
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [83][  0/391]	Time  0.413 ( 0.413)	Data  0.158 ( 0.158)	Loss 2.2539e-02 (2.2539e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [83][ 10/391]	Time  0.241 ( 0.256)	Data  0.001 ( 0.015)	Loss 1.1850e-02 (2.4987e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [83][ 20/391]	Time  0.240 ( 0.249)	Data  0.001 ( 0.009)	Loss 1.9216e-02 (2.1854e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [83][ 30/391]	Time  0.240 ( 0.246)	Data  0.001 ( 0.006)	Loss 1.8533e-02 (2.2845e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [83][ 40/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.005)	Loss 3.0277e-02 (2.2512e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [83][ 50/391]	Time  0.249 ( 0.245)	Data  0.001 ( 0.004)	Loss 2.6810e-02 (2.2542e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [83][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 2.7781e-02 (2.3849e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [83][ 70/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.003)	Loss 1.5873e-02 (2.3177e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [83][ 80/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.003)	Loss 2.0817e-02 (2.2424e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [83][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.5980e-02 (2.2420e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [83][100/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.7265e-02 (2.2650e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [83][110/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.2036e-02 (2.2281e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [83][120/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.5023e-02 (2.2442e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [83][130/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.3894e-02 (2.2304e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [83][140/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.2534e-02 (2.2516e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [83][150/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.8441e-02 (2.2454e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [83][160/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1345e-02 (2.2514e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [83][170/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.8365e-02 (2.2443e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [83][180/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.4748e-02 (2.2545e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [83][190/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.6787e-02 (2.2508e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [83][200/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.2899e-02 (2.2491e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [83][210/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.1575e-02 (2.2262e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [83][220/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.9296e-02 (2.2149e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [83][230/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4827e-02 (2.2161e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [83][240/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.4495e-02 (2.2136e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [83][250/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.3548e-02 (2.2160e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [83][260/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.9803e-02 (2.2130e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [83][270/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.8634e-02 (2.2126e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [83][280/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7357e-02 (2.2128e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [83][290/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0676e-02 (2.2150e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [83][300/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.0552e-02 (2.2301e-02)	Acc@1  98.44 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [83][310/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.1963e-02 (2.2293e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [83][320/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.0237e-02 (2.2376e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [83][330/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.4091e-02 (2.2319e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [83][340/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8000e-02 (2.2444e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [83][350/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9785e-02 (2.2258e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [83][360/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.5502e-02 (2.2217e-02)	Acc@1  98.44 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [83][370/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.4084e-02 (2.2191e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [83][380/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.8705e-02 (2.2133e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [83][390/391]	Time  0.180 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4738e-02 (2.2018e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
## e[83] optimizer.zero_grad (sum) time: 0.48984217643737793
## e[83]       loss.backward (sum) time: 11.175878286361694
## e[83]      optimizer.step (sum) time: 47.78791546821594
## epoch[83] training(only) time: 94.84336161613464
# Switched to evaluate mode...
Test: [  0/100]	Time  0.209 ( 0.209)	Loss 1.4243e+00 (1.4243e+00)	Acc@1  74.00 ( 74.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.079 ( 0.090)	Loss 1.3826e+00 (1.3655e+00)	Acc@1  71.00 ( 72.82)	Acc@5  93.00 ( 91.55)
Test: [ 20/100]	Time  0.079 ( 0.084)	Loss 1.3147e+00 (1.3236e+00)	Acc@1  66.00 ( 73.14)	Acc@5  93.00 ( 92.10)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 1.5956e+00 (1.3759e+00)	Acc@1  66.00 ( 72.19)	Acc@5  92.00 ( 91.87)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.5223e+00 (1.3842e+00)	Acc@1  74.00 ( 71.93)	Acc@5  93.00 ( 92.02)
Test: [ 50/100]	Time  0.078 ( 0.081)	Loss 1.6505e+00 (1.4008e+00)	Acc@1  71.00 ( 71.76)	Acc@5  91.00 ( 91.67)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.5863e+00 (1.3763e+00)	Acc@1  68.00 ( 72.05)	Acc@5  93.00 ( 92.08)
Test: [ 70/100]	Time  0.079 ( 0.080)	Loss 1.4823e+00 (1.3794e+00)	Acc@1  71.00 ( 72.17)	Acc@5  88.00 ( 92.01)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 1.3594e+00 (1.3718e+00)	Acc@1  70.00 ( 72.32)	Acc@5  93.00 ( 92.01)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.6743e+00 (1.3504e+00)	Acc@1  65.00 ( 72.56)	Acc@5  89.00 ( 92.09)
 * Acc@1 72.680 Acc@5 92.130
### epoch[83] execution time: 102.8665223121643
EPOCH 84
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [84][  0/391]	Time  0.399 ( 0.399)	Data  0.155 ( 0.155)	Loss 2.2599e-02 (2.2599e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [84][ 10/391]	Time  0.241 ( 0.255)	Data  0.001 ( 0.015)	Loss 2.1794e-02 (2.1406e-02)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [84][ 20/391]	Time  0.240 ( 0.249)	Data  0.001 ( 0.008)	Loss 2.3748e-02 (1.9501e-02)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [84][ 30/391]	Time  0.240 ( 0.247)	Data  0.001 ( 0.006)	Loss 1.2609e-02 (2.0974e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [84][ 40/391]	Time  0.242 ( 0.245)	Data  0.001 ( 0.005)	Loss 1.9482e-02 (2.1069e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [84][ 50/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.004)	Loss 1.6311e-02 (2.0720e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [84][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 2.3678e-02 (2.1144e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [84][ 70/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.003)	Loss 3.1030e-02 (2.1289e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [84][ 80/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 1.8118e-02 (2.0843e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [84][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.9020e-02 (2.0924e-02)	Acc@1  98.44 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [84][100/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.1292e-02 (2.0964e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [84][110/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 4.0052e-02 (2.1499e-02)	Acc@1  98.44 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [84][120/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.3589e-02 (2.2126e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [84][130/391]	Time  0.250 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.8766e-02 (2.2149e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [84][140/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.5099e-02 (2.2354e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [84][150/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.8040e-02 (2.2565e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [84][160/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.0554e-02 (2.2889e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [84][170/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.1564e-02 (2.2804e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [84][180/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.8413e-02 (2.2739e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [84][190/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.8243e-02 (2.2601e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [84][200/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.5627e-02 (2.2761e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [84][210/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.7759e-02 (2.2612e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [84][220/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.4572e-02 (2.2656e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [84][230/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.5548e-02 (2.2499e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [84][240/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.7587e-02 (2.2503e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [84][250/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.0079e-02 (2.2467e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [84][260/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.8056e-02 (2.2310e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [84][270/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.2276e-02 (2.2316e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [84][280/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.7607e-02 (2.2356e-02)	Acc@1  98.44 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [84][290/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.9267e-02 (2.2385e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [84][300/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3434e-02 (2.2282e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [84][310/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8779e-02 (2.2344e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [84][320/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.1597e-02 (2.2314e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [84][330/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9880e-02 (2.2272e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [84][340/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4325e-02 (2.2318e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [84][350/391]	Time  0.254 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3492e-02 (2.2295e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [84][360/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6991e-02 (2.2384e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [84][370/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.8058e-02 (2.2381e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [84][380/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.9459e-02 (2.2394e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [84][390/391]	Time  0.178 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6782e-02 (2.2406e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
## e[84] optimizer.zero_grad (sum) time: 0.49436450004577637
## e[84]       loss.backward (sum) time: 11.195486068725586
## e[84]      optimizer.step (sum) time: 47.78899669647217
## epoch[84] training(only) time: 94.81509184837341
# Switched to evaluate mode...
Test: [  0/100]	Time  0.210 ( 0.210)	Loss 1.3898e+00 (1.3898e+00)	Acc@1  75.00 ( 75.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.079 ( 0.090)	Loss 1.3752e+00 (1.3644e+00)	Acc@1  70.00 ( 73.27)	Acc@5  92.00 ( 90.82)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 1.2864e+00 (1.3218e+00)	Acc@1  68.00 ( 73.33)	Acc@5  93.00 ( 91.81)
Test: [ 30/100]	Time  0.077 ( 0.082)	Loss 1.5828e+00 (1.3713e+00)	Acc@1  68.00 ( 72.32)	Acc@5  92.00 ( 91.68)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.4966e+00 (1.3830e+00)	Acc@1  73.00 ( 71.80)	Acc@5  93.00 ( 91.85)
Test: [ 50/100]	Time  0.077 ( 0.081)	Loss 1.6897e+00 (1.4010e+00)	Acc@1  72.00 ( 71.78)	Acc@5  89.00 ( 91.49)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.5664e+00 (1.3754e+00)	Acc@1  68.00 ( 71.97)	Acc@5  93.00 ( 91.92)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 1.5336e+00 (1.3798e+00)	Acc@1  71.00 ( 72.10)	Acc@5  88.00 ( 91.92)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 1.3619e+00 (1.3727e+00)	Acc@1  71.00 ( 72.26)	Acc@5  91.00 ( 91.93)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.6732e+00 (1.3508e+00)	Acc@1  66.00 ( 72.48)	Acc@5  90.00 ( 92.04)
 * Acc@1 72.650 Acc@5 92.110
### epoch[84] execution time: 102.83231711387634
EPOCH 85
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [85][  0/391]	Time  0.402 ( 0.402)	Data  0.150 ( 0.150)	Loss 1.2331e-02 (1.2331e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [85][ 10/391]	Time  0.240 ( 0.256)	Data  0.001 ( 0.015)	Loss 5.9798e-02 (2.6980e-02)	Acc@1  97.66 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [85][ 20/391]	Time  0.241 ( 0.249)	Data  0.001 ( 0.008)	Loss 1.9742e-02 (2.6185e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [85][ 30/391]	Time  0.241 ( 0.247)	Data  0.001 ( 0.006)	Loss 2.4283e-02 (2.4262e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [85][ 40/391]	Time  0.243 ( 0.245)	Data  0.001 ( 0.005)	Loss 2.5145e-02 (2.4318e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [85][ 50/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 2.1618e-02 (2.3788e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [85][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 2.2599e-02 (2.3266e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [85][ 70/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.003)	Loss 1.5496e-02 (2.2543e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [85][ 80/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.6594e-02 (2.3335e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [85][ 90/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.003)	Loss 4.4176e-02 (2.3483e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [85][100/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.3432e-02 (2.3277e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [85][110/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.7869e-02 (2.2937e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [85][120/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.5870e-02 (2.2931e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [85][130/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.8904e-02 (2.2899e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [85][140/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.3181e-02 (2.2775e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [85][150/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.4224e-02 (2.2711e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [85][160/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.7087e-02 (2.2594e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [85][170/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.6092e-02 (2.2365e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [85][180/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.7025e-02 (2.2775e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [85][190/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.3265e-02 (2.2612e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [85][200/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.4062e-02 (2.2690e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [85][210/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.7860e-02 (2.2579e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [85][220/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7048e-02 (2.2590e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [85][230/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5323e-02 (2.2468e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [85][240/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.3189e-02 (2.2327e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [85][250/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2868e-02 (2.2358e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [85][260/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6371e-02 (2.2579e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [85][270/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4258e-02 (2.2720e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [85][280/391]	Time  0.246 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6097e-02 (2.2662e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [85][290/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5636e-02 (2.2449e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [85][300/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9569e-02 (2.2339e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [85][310/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3468e-02 (2.2283e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [85][320/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5306e-02 (2.2189e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [85][330/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.0943e-02 (2.2077e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [85][340/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4362e-02 (2.2210e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [85][350/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9977e-02 (2.2152e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [85][360/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3229e-02 (2.2223e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [85][370/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.5025e-02 (2.2143e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [85][380/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.2880e-02 (2.2190e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [85][390/391]	Time  0.178 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.8251e-02 (2.2163e-02)	Acc@1  98.75 ( 99.65)	Acc@5 100.00 (100.00)
## e[85] optimizer.zero_grad (sum) time: 0.493450403213501
## e[85]       loss.backward (sum) time: 11.11319351196289
## e[85]      optimizer.step (sum) time: 47.831353425979614
## epoch[85] training(only) time: 94.80521488189697
# Switched to evaluate mode...
Test: [  0/100]	Time  0.209 ( 0.209)	Loss 1.4360e+00 (1.4360e+00)	Acc@1  75.00 ( 75.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.3679e+00 (1.3726e+00)	Acc@1  71.00 ( 73.00)	Acc@5  93.00 ( 91.36)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 1.2485e+00 (1.3212e+00)	Acc@1  66.00 ( 73.29)	Acc@5  94.00 ( 92.05)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 1.6011e+00 (1.3772e+00)	Acc@1  65.00 ( 72.42)	Acc@5  94.00 ( 91.94)
Test: [ 40/100]	Time  0.077 ( 0.081)	Loss 1.4789e+00 (1.3858e+00)	Acc@1  76.00 ( 72.07)	Acc@5  93.00 ( 92.00)
Test: [ 50/100]	Time  0.078 ( 0.080)	Loss 1.6488e+00 (1.4044e+00)	Acc@1  72.00 ( 71.94)	Acc@5  90.00 ( 91.63)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.6367e+00 (1.3805e+00)	Acc@1  66.00 ( 72.11)	Acc@5  92.00 ( 92.02)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 1.5333e+00 (1.3839e+00)	Acc@1  71.00 ( 72.18)	Acc@5  89.00 ( 92.03)
Test: [ 80/100]	Time  0.077 ( 0.080)	Loss 1.4064e+00 (1.3785e+00)	Acc@1  70.00 ( 72.37)	Acc@5  91.00 ( 91.99)
Test: [ 90/100]	Time  0.079 ( 0.079)	Loss 1.7120e+00 (1.3567e+00)	Acc@1  65.00 ( 72.63)	Acc@5  89.00 ( 92.02)
 * Acc@1 72.680 Acc@5 92.070
### epoch[85] execution time: 102.81379532814026
EPOCH 86
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [86][  0/391]	Time  0.396 ( 0.396)	Data  0.153 ( 0.153)	Loss 1.9060e-02 (1.9060e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [86][ 10/391]	Time  0.240 ( 0.255)	Data  0.001 ( 0.015)	Loss 4.2699e-02 (2.8966e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [86][ 20/391]	Time  0.245 ( 0.248)	Data  0.001 ( 0.008)	Loss 1.9212e-02 (2.7215e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [86][ 30/391]	Time  0.242 ( 0.246)	Data  0.001 ( 0.006)	Loss 1.0730e-02 (2.5583e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [86][ 40/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.005)	Loss 1.5868e-02 (2.5590e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [86][ 50/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.8565e-02 (2.4188e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [86][ 60/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.004)	Loss 1.6643e-02 (2.3467e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [86][ 70/391]	Time  0.246 ( 0.244)	Data  0.001 ( 0.003)	Loss 2.0197e-02 (2.2989e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [86][ 80/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.0889e-02 (2.2569e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [86][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.5255e-02 (2.2554e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [86][100/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.9755e-02 (2.2526e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [86][110/391]	Time  0.252 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.1763e-02 (2.1805e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [86][120/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.2959e-02 (2.1932e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [86][130/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.7335e-02 (2.1519e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [86][140/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.1136e-02 (2.1404e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [86][150/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.8732e-02 (2.1413e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [86][160/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.7076e-02 (2.1524e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [86][170/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 7.8683e-03 (2.1277e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [86][180/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.4372e-02 (2.1258e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [86][190/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.7940e-02 (2.1203e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [86][200/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.4825e-02 (2.1419e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [86][210/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.0099e-02 (2.1485e-02)	Acc@1  97.66 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [86][220/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3753e-02 (2.1689e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [86][230/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3026e-02 (2.1716e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [86][240/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0599e-02 (2.1630e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [86][250/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.0582e-02 (2.1516e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [86][260/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.2162e-02 (2.1489e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [86][270/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.0749e-02 (2.1473e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [86][280/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.9871e-02 (2.1670e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [86][290/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.3061e-02 (2.1636e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [86][300/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.5870e-02 (2.1634e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [86][310/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.3094e-02 (2.1591e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [86][320/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 1.6923e-02 (2.1549e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [86][330/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3916e-02 (2.1578e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [86][340/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 9.5331e-03 (2.1611e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [86][350/391]	Time  0.241 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4664e-02 (2.1633e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [86][360/391]	Time  0.243 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.8261e-02 (2.1498e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [86][370/391]	Time  0.240 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.1874e-02 (2.1513e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [86][380/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.2284e-02 (2.1431e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [86][390/391]	Time  0.179 ( 0.242)	Data  0.001 ( 0.001)	Loss 2.0927e-02 (2.1393e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
## e[86] optimizer.zero_grad (sum) time: 0.4955329895019531
## e[86]       loss.backward (sum) time: 11.113877296447754
## e[86]      optimizer.step (sum) time: 47.819040060043335
## epoch[86] training(only) time: 94.73902773857117
# Switched to evaluate mode...
Test: [  0/100]	Time  0.236 ( 0.236)	Loss 1.4264e+00 (1.4264e+00)	Acc@1  73.00 ( 73.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.079 ( 0.093)	Loss 1.4019e+00 (1.3726e+00)	Acc@1  69.00 ( 72.64)	Acc@5  93.00 ( 91.82)
Test: [ 20/100]	Time  0.078 ( 0.086)	Loss 1.3732e+00 (1.3292e+00)	Acc@1  68.00 ( 72.95)	Acc@5  91.00 ( 92.38)
Test: [ 30/100]	Time  0.078 ( 0.083)	Loss 1.5960e+00 (1.3810e+00)	Acc@1  68.00 ( 72.16)	Acc@5  93.00 ( 92.00)
Test: [ 40/100]	Time  0.078 ( 0.082)	Loss 1.4802e+00 (1.3906e+00)	Acc@1  76.00 ( 71.93)	Acc@5  93.00 ( 92.02)
Test: [ 50/100]	Time  0.081 ( 0.081)	Loss 1.7054e+00 (1.4088e+00)	Acc@1  69.00 ( 71.75)	Acc@5  90.00 ( 91.75)
Test: [ 60/100]	Time  0.078 ( 0.081)	Loss 1.5452e+00 (1.3818e+00)	Acc@1  68.00 ( 72.02)	Acc@5  92.00 ( 92.07)
Test: [ 70/100]	Time  0.079 ( 0.080)	Loss 1.4694e+00 (1.3832e+00)	Acc@1  69.00 ( 72.04)	Acc@5  89.00 ( 91.96)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 1.3479e+00 (1.3773e+00)	Acc@1  71.00 ( 72.12)	Acc@5  93.00 ( 91.95)
Test: [ 90/100]	Time  0.077 ( 0.080)	Loss 1.7202e+00 (1.3547e+00)	Acc@1  64.00 ( 72.42)	Acc@5  89.00 ( 92.01)
 * Acc@1 72.580 Acc@5 92.020
### epoch[86] execution time: 102.79770588874817
EPOCH 87
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [87][  0/391]	Time  0.444 ( 0.444)	Data  0.202 ( 0.202)	Loss 1.4807e-02 (1.4807e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [87][ 10/391]	Time  0.241 ( 0.259)	Data  0.001 ( 0.019)	Loss 2.6340e-02 (2.0570e-02)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [87][ 20/391]	Time  0.241 ( 0.251)	Data  0.001 ( 0.011)	Loss 3.4957e-02 (2.5535e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [87][ 30/391]	Time  0.241 ( 0.248)	Data  0.001 ( 0.008)	Loss 1.6672e-02 (2.2747e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [87][ 40/391]	Time  0.242 ( 0.246)	Data  0.001 ( 0.006)	Loss 1.5839e-02 (2.3488e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [87][ 50/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.005)	Loss 1.3479e-02 (2.1743e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [87][ 60/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.005)	Loss 2.4868e-02 (2.2271e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [87][ 70/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.004)	Loss 2.0168e-02 (2.2151e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [87][ 80/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.004)	Loss 2.2904e-02 (2.1718e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [87][ 90/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.003)	Loss 1.6259e-02 (2.1124e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [87][100/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.7822e-02 (2.1006e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [87][110/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.9389e-02 (2.0700e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [87][120/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.5910e-02 (2.0725e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [87][130/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 3.1343e-02 (2.0561e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [87][140/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 2.1922e-02 (2.1074e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [87][150/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.7562e-02 (2.1028e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [87][160/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.3911e-02 (2.1194e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [87][170/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.8061e-02 (2.1448e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [87][180/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 8.6569e-03 (2.1468e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [87][190/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.0148e-02 (2.1488e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [87][200/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.6710e-02 (2.1479e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [87][210/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.6034e-02 (2.1703e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [87][220/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 8.4130e-03 (2.1845e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [87][230/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.3994e-02 (2.2093e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [87][240/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.5247e-02 (2.1966e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [87][250/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.6554e-02 (2.1892e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [87][260/391]	Time  0.245 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.0741e-02 (2.1868e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [87][270/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.6635e-02 (2.2014e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [87][280/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.7873e-02 (2.2048e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [87][290/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.0964e-02 (2.2110e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [87][300/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.2387e-02 (2.2142e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [87][310/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.3573e-02 (2.2309e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [87][320/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.7952e-02 (2.2180e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [87][330/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.5139e-02 (2.2254e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [87][340/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.3165e-02 (2.2108e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [87][350/391]	Time  0.244 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.4409e-02 (2.2108e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [87][360/391]	Time  0.242 ( 0.242)	Data  0.001 ( 0.002)	Loss 2.3965e-02 (2.2094e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [87][370/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.2942e-02 (2.1993e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [87][380/391]	Time  0.245 ( 0.242)	Data  0.001 ( 0.002)	Loss 3.4952e-02 (2.1889e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [87][390/391]	Time  0.178 ( 0.242)	Data  0.001 ( 0.002)	Loss 6.3006e-02 (2.1886e-02)	Acc@1  98.75 ( 99.66)	Acc@5 100.00 (100.00)
## e[87] optimizer.zero_grad (sum) time: 0.4952874183654785
## e[87]       loss.backward (sum) time: 11.135745525360107
## e[87]      optimizer.step (sum) time: 47.825700759887695
## epoch[87] training(only) time: 94.8416395187378
# Switched to evaluate mode...
Test: [  0/100]	Time  0.207 ( 0.207)	Loss 1.3990e+00 (1.3990e+00)	Acc@1  73.00 ( 73.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.079 ( 0.090)	Loss 1.3821e+00 (1.3654e+00)	Acc@1  69.00 ( 72.91)	Acc@5  92.00 ( 91.18)
Test: [ 20/100]	Time  0.078 ( 0.084)	Loss 1.3410e+00 (1.3256e+00)	Acc@1  68.00 ( 73.24)	Acc@5  93.00 ( 91.90)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 1.5334e+00 (1.3746e+00)	Acc@1  66.00 ( 72.23)	Acc@5  93.00 ( 91.68)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.4559e+00 (1.3825e+00)	Acc@1  74.00 ( 71.76)	Acc@5  92.00 ( 91.78)
Test: [ 50/100]	Time  0.079 ( 0.081)	Loss 1.7317e+00 (1.4024e+00)	Acc@1  70.00 ( 71.65)	Acc@5  90.00 ( 91.43)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.5552e+00 (1.3769e+00)	Acc@1  67.00 ( 72.07)	Acc@5  92.00 ( 91.84)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 1.5205e+00 (1.3798e+00)	Acc@1  71.00 ( 72.10)	Acc@5  88.00 ( 91.80)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 1.4066e+00 (1.3740e+00)	Acc@1  72.00 ( 72.21)	Acc@5  91.00 ( 91.79)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.6873e+00 (1.3514e+00)	Acc@1  66.00 ( 72.52)	Acc@5  90.00 ( 91.88)
 * Acc@1 72.630 Acc@5 91.970
### epoch[87] execution time: 102.85494184494019
EPOCH 88
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [88][  0/391]	Time  0.393 ( 0.393)	Data  0.150 ( 0.150)	Loss 2.4673e-02 (2.4673e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [88][ 10/391]	Time  0.240 ( 0.254)	Data  0.001 ( 0.014)	Loss 1.8916e-02 (2.4520e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [88][ 20/391]	Time  0.241 ( 0.249)	Data  0.001 ( 0.008)	Loss 2.6220e-02 (2.0296e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [88][ 30/391]	Time  0.240 ( 0.247)	Data  0.001 ( 0.006)	Loss 3.2255e-02 (2.1310e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [88][ 40/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.005)	Loss 8.6352e-03 (1.9944e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [88][ 50/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.004)	Loss 2.4436e-02 (1.9941e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [88][ 60/391]	Time  0.241 ( 0.245)	Data  0.001 ( 0.004)	Loss 2.1952e-02 (2.0875e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [88][ 70/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 1.2780e-02 (2.0828e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [88][ 80/391]	Time  0.242 ( 0.244)	Data  0.001 ( 0.003)	Loss 2.1967e-02 (2.0590e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [88][ 90/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 1.3246e-02 (2.0501e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [88][100/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 2.9514e-02 (2.1118e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [88][110/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.002)	Loss 6.8330e-03 (2.0879e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [88][120/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.2356e-02 (2.1265e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [88][130/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.9804e-02 (2.1528e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [88][140/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.0318e-02 (2.1719e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [88][150/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.7516e-02 (2.2003e-02)	Acc@1  98.44 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [88][160/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1617e-02 (2.1691e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [88][170/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.7576e-02 (2.1430e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [88][180/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.5389e-02 (2.1352e-02)	Acc@1  98.44 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [88][190/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.9120e-02 (2.1377e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [88][200/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.5606e-02 (2.1427e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [88][210/391]	Time  0.248 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.6707e-02 (2.1295e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [88][220/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.0732e-02 (2.1422e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [88][230/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.2627e-02 (2.1302e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [88][240/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.2168e-02 (2.1377e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [88][250/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.2337e-02 (2.1261e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [88][260/391]	Time  0.251 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.3637e-02 (2.1373e-02)	Acc@1  98.44 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [88][270/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.2038e-02 (2.1338e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [88][280/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.6548e-02 (2.1265e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [88][290/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.6339e-02 (2.1326e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [88][300/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.6812e-02 (2.1170e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [88][310/391]	Time  0.247 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.4494e-02 (2.1198e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [88][320/391]	Time  0.240 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.5797e-02 (2.1281e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [88][330/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.8451e-02 (2.1327e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [88][340/391]	Time  0.251 ( 0.243)	Data  0.001 ( 0.002)	Loss 4.1658e-02 (2.1358e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [88][350/391]	Time  0.251 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.7749e-02 (2.1468e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [88][360/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.0422e-02 (2.1375e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [88][370/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.4170e-02 (2.1363e-02)	Acc@1  98.44 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [88][380/391]	Time  0.243 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.4316e-02 (2.1211e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [88][390/391]	Time  0.178 ( 0.242)	Data  0.001 ( 0.002)	Loss 4.5672e-02 (2.1236e-02)	Acc@1  98.75 ( 99.70)	Acc@5 100.00 (100.00)
## e[88] optimizer.zero_grad (sum) time: 0.4942638874053955
## e[88]       loss.backward (sum) time: 11.154998064041138
## e[88]      optimizer.step (sum) time: 47.80259442329407
## epoch[88] training(only) time: 94.90089416503906
# Switched to evaluate mode...
Test: [  0/100]	Time  0.208 ( 0.208)	Loss 1.3686e+00 (1.3686e+00)	Acc@1  73.00 ( 73.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.3812e+00 (1.3615e+00)	Acc@1  70.00 ( 73.09)	Acc@5  92.00 ( 91.55)
Test: [ 20/100]	Time  0.077 ( 0.084)	Loss 1.3005e+00 (1.3116e+00)	Acc@1  67.00 ( 73.10)	Acc@5  92.00 ( 92.29)
Test: [ 30/100]	Time  0.078 ( 0.082)	Loss 1.5571e+00 (1.3623e+00)	Acc@1  65.00 ( 72.23)	Acc@5  92.00 ( 91.90)
Test: [ 40/100]	Time  0.078 ( 0.081)	Loss 1.4845e+00 (1.3728e+00)	Acc@1  75.00 ( 71.95)	Acc@5  92.00 ( 92.05)
Test: [ 50/100]	Time  0.078 ( 0.080)	Loss 1.7326e+00 (1.3921e+00)	Acc@1  69.00 ( 71.78)	Acc@5  89.00 ( 91.65)
Test: [ 60/100]	Time  0.077 ( 0.080)	Loss 1.5484e+00 (1.3666e+00)	Acc@1  68.00 ( 72.11)	Acc@5  91.00 ( 91.98)
Test: [ 70/100]	Time  0.078 ( 0.080)	Loss 1.4602e+00 (1.3695e+00)	Acc@1  71.00 ( 72.21)	Acc@5  88.00 ( 91.92)
Test: [ 80/100]	Time  0.078 ( 0.080)	Loss 1.3700e+00 (1.3629e+00)	Acc@1  72.00 ( 72.35)	Acc@5  93.00 ( 91.94)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.6853e+00 (1.3421e+00)	Acc@1  64.00 ( 72.58)	Acc@5  89.00 ( 91.97)
 * Acc@1 72.730 Acc@5 92.030
### epoch[88] execution time: 102.93311953544617
EPOCH 89
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [89][  0/391]	Time  0.385 ( 0.385)	Data  0.143 ( 0.143)	Loss 2.1467e-02 (2.1467e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [89][ 10/391]	Time  0.240 ( 0.254)	Data  0.001 ( 0.014)	Loss 1.6605e-02 (1.9852e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [89][ 20/391]	Time  0.240 ( 0.248)	Data  0.001 ( 0.008)	Loss 2.6774e-02 (1.9588e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [89][ 30/391]	Time  0.241 ( 0.246)	Data  0.001 ( 0.006)	Loss 2.6025e-02 (2.0790e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [89][ 40/391]	Time  0.250 ( 0.245)	Data  0.001 ( 0.005)	Loss 2.5204e-02 (2.1514e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [89][ 50/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.004)	Loss 3.8383e-02 (2.1508e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [89][ 60/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 1.1314e-02 (2.1917e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [89][ 70/391]	Time  0.241 ( 0.244)	Data  0.001 ( 0.003)	Loss 1.2016e-02 (2.2054e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [89][ 80/391]	Time  0.240 ( 0.244)	Data  0.001 ( 0.003)	Loss 1.9940e-02 (2.2678e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [89][ 90/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.003)	Loss 1.9533e-02 (2.2472e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [89][100/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.8557e-02 (2.2474e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [89][110/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.8278e-02 (2.2399e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [89][120/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.8141e-02 (2.2193e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [89][130/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.4207e-02 (2.2281e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [89][140/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1337e-02 (2.2173e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [89][150/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.4526e-02 (2.1919e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [89][160/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.0153e-02 (2.2079e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [89][170/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.7059e-02 (2.1890e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [89][180/391]	Time  0.248 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.7407e-02 (2.1748e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [89][190/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.3410e-02 (2.2086e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [89][200/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.2706e-02 (2.2091e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [89][210/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.6682e-02 (2.2035e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [89][220/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.2838e-02 (2.1995e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [89][230/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 9.9797e-03 (2.1839e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [89][240/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.1590e-02 (2.1907e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [89][250/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 3.2686e-02 (2.1809e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [89][260/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.0582e-02 (2.1584e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [89][270/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.1703e-02 (2.1767e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [89][280/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.9332e-02 (2.1670e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [89][290/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.2445e-02 (2.1705e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [89][300/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.002)	Loss 2.1283e-02 (2.1660e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [89][310/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.002)	Loss 1.3972e-02 (2.1484e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [89][320/391]	Time  0.251 ( 0.243)	Data  0.001 ( 0.001)	Loss 2.3939e-02 (2.1436e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [89][330/391]	Time  0.248 ( 0.243)	Data  0.001 ( 0.001)	Loss 9.1227e-03 (2.1385e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [89][340/391]	Time  0.244 ( 0.243)	Data  0.001 ( 0.001)	Loss 1.8872e-02 (2.1267e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [89][350/391]	Time  0.254 ( 0.243)	Data  0.001 ( 0.001)	Loss 2.2344e-02 (2.1186e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [89][360/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.001)	Loss 2.4900e-02 (2.1277e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [89][370/391]	Time  0.241 ( 0.243)	Data  0.001 ( 0.001)	Loss 1.2358e-02 (2.1154e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [89][380/391]	Time  0.242 ( 0.243)	Data  0.001 ( 0.001)	Loss 3.5621e-02 (2.1236e-02)	Acc@1  98.44 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [89][390/391]	Time  0.179 ( 0.242)	Data  0.001 ( 0.001)	Loss 6.6038e-02 (2.1374e-02)	Acc@1  97.50 ( 99.67)	Acc@5 100.00 (100.00)
## e[89] optimizer.zero_grad (sum) time: 0.4944610595703125
## e[89]       loss.backward (sum) time: 11.17316460609436
## e[89]      optimizer.step (sum) time: 47.79944181442261
## epoch[89] training(only) time: 94.89843130111694
# Switched to evaluate mode...
Test: [  0/100]	Time  0.208 ( 0.208)	Loss 1.3551e+00 (1.3551e+00)	Acc@1  76.00 ( 76.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.078 ( 0.090)	Loss 1.3523e+00 (1.3554e+00)	Acc@1  71.00 ( 73.36)	Acc@5  92.00 ( 91.91)
Test: [ 20/100]	Time  0.077 ( 0.084)	Loss 1.3001e+00 (1.3155e+00)	Acc@1  67.00 ( 73.24)	Acc@5  92.00 ( 92.29)
Test: [ 30/100]	Time  0.077 ( 0.082)	Loss 1.5441e+00 (1.3665e+00)	Acc@1  66.00 ( 72.29)	Acc@5  92.00 ( 91.87)
Test: [ 40/100]	Time  0.077 ( 0.081)	Loss 1.4641e+00 (1.3755e+00)	Acc@1  76.00 ( 72.00)	Acc@5  92.00 ( 91.85)
Test: [ 50/100]	Time  0.079 ( 0.080)	Loss 1.6989e+00 (1.3947e+00)	Acc@1  72.00 ( 71.86)	Acc@5  90.00 ( 91.59)
Test: [ 60/100]	Time  0.078 ( 0.080)	Loss 1.5654e+00 (1.3718e+00)	Acc@1  68.00 ( 72.08)	Acc@5  91.00 ( 91.98)
Test: [ 70/100]	Time  0.079 ( 0.080)	Loss 1.4805e+00 (1.3769e+00)	Acc@1  70.00 ( 72.10)	Acc@5  88.00 ( 91.96)
Test: [ 80/100]	Time  0.077 ( 0.080)	Loss 1.3715e+00 (1.3711e+00)	Acc@1  72.00 ( 72.19)	Acc@5  91.00 ( 91.98)
Test: [ 90/100]	Time  0.078 ( 0.079)	Loss 1.6519e+00 (1.3480e+00)	Acc@1  67.00 ( 72.45)	Acc@5  89.00 ( 92.03)
 * Acc@1 72.560 Acc@5 92.100
### epoch[89] execution time: 102.91011238098145
### Training complete:
#### total training(only) time: 8537.805794000626
##### Total run time: 9264.409636497498
